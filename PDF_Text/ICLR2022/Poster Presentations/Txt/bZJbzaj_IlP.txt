Published as a conference paper at ICLR 2022
A Non-parametric Regression viewpoint : Gen-
eralization of Overparametrized Deep ReLU
Network under Noisy Observations
Namjoon Suh, Hyunouk Ko, Xiaoming Huo
H.Milton Stewart School of Industrial and Systems Engineering
Georgia Institute of Technology
Atlanta, GA, USA
{namjsuh,hko39,huo}@gatech.edu
Ab stract
We study the generalization properties of the overparameterized deep neural net-
work (DNN) with Rectified Linear Unit (ReLU) activations. Under the non-
parametric regression framework, it is assumed that the ground-truth function is
from a reproducing kernel Hilbert space (RKHS) induced by a neural tangent
kernel (NTK) of ReLU DNN, and a dataset is given with the noises. Without
a delicate adoption of early stopping, we prove that the overparametrized DNN
trained by vanilla gradient descent does not recover the ground-truth function. It
turns out that the estimated DNN’s L2 prediction error is bounded away from 0.
As a complement of the above result, We show that the '2-regularized gradient
descent enables the overparametrized DNN to achieve the minimax optimal con-
vergence rate of the L2 prediction error, without early stopping. Notably, the rate
we obtained is faster than O(n-1/2) known in the literature.
1	Introduction
Over the past few years, Neural Tangent Kernel (NTK) [Arora et al., 2019b; Jacot et al., 2018; Lee
et al., 2018; Chizat & Bach, 2018] has been one of the most seminal discoveries in the theory of
neural network. The underpinning idea of the NTK-type theory comes from the observation that
in a wide-enough neural net, model parameters updated by gradient descent (GD) stay close to
their initializations during the training, so that the dynamics of the networks can be approximated
by the first-order Taylor expansion with respect to its parameters at initialization. The linearization
of learning dynamics on neural networks has been helpful in showing the linear convergence of
the training error on both overparametrized shallow [Li & Liang, 2018; Du et al., 2018] and deep
neural networks [Allen-Zhu et al., 2018; Zou et al., 2018; 2020], as well as the characterizations of
generalization error on both models [Arora et al., 2019a; Cao & Gu, 2019]. These findings clearly
lead to the equivalence between learning dynamics of neural networks and the kernel methods
in reproducing kernel Hilbert spaces (RKHS) associated with NTK. 1 Specifically, Arora et al.
[2019a] provided the O(n-1/2) generalization bound of shallow neural network, where n denotes
the training sample size.
Recently, in the context of nonparametric regression, two papers, Nitanda & Suzuki [2020]
and Hu et al. [2021], showed that neural network can obtain the convergence rate faster than
O(n-1/2) by specifying the complexities of target function and hypothesis space. Specifically, Ni-
tanda & Suzuki [2020] showed that the shallow neural network with smoothly approximated ReLU
(swish, see Ramachandran et al. [2017]) activation trained via '2-regularized averaged stochastic
gradient descent (SGD) can recover the target function from RKHSs induced from NTK with swish
activation. Similarly, Hu et al. [2021] showed that a shallow neural network with ReLU activation
trained via '2-regularized GD can generalize well, when the target function (i.e., fρ?) is from H1NTK.
1Henceforth, we denote HNTK and HLTK as RKHSS induced from NTK of shallow L = 1 and deep neural
networks L ≥ 2 with ReLU activations, respecitvely.
1
Published as a conference paper at ICLR 2022
Notably, the rate that the papers Nitanda & Suzuki [2020] and Hu et al. [2021] obtained is minimax
optimal, meaning that no estimators perform substantially better than the '2-regularized GD or
averaged SGD algorithms for recovering functions from respective function spaces. Nevertheless,
these results are restricted to shallow neural networks, and cannot explain the generalization abilities
of deep neural network (DNN). Similarly with Arora et al. [2019a], Cao & Gu [2019] obtained
the O(n-1/2) generalization bound, showing that the SGD generalize well for fρ? ∈ HLNTK, when
fρ? has a bounded RKHS norm. However, the rate they obtained is slower than the minimax rate
we can actually achieve. Furthermore, their results become vacuous under the presence of additive
noises on the data set. Motivated from these observations, the fundamental question in this study is
as follows:
When the noisy dataset is generated from a function from HNLTK, does the overparametrized
DNN obtained via ('2-regularized) GDProvably generalize well the unseen data?
We consider a neural network that has L ≥ 2 hidden layers with width m n. (i.e., over-
parametrized deep neural network.) We focus on the least-squares loss and assume that the acti-
vation function is ReLU. A positivity assumption of NTK from ReLU DNN is imposed, meaning
that λ∞ > 0, where λ∞ denotes the minimum eigenvalue of the NTK. We give a more formal math-
ematical definition of ReLU DNN in the following Subsection 2.2. Under these settings, we provide
an affirmative answer to the above question by investigating the behavior of L2-prediction error of
the obtained neural network with respect to GD iterations.
1.1	Contributions
Our derivations of algorithm-dependent prediction risk bound require the analysis on training dy-
namics of the estimated neural network through (regularized) GD algorithm. We include these
results as the contributions of our paper, which can be of independent interests as well.
•	In an unregulaized case, under the assumption λ∞ > 0, we show that the training loss con-
verges to 0 at a linear rate. As will be detailed in subsection 3.3, this is the different result
from the seminal work of Allen-Zhu et al. [2018], where they also prove a linear conver-
gence of training loss of ReLU DNN, but under different data distribution assumption.
•	We show that the DNN updated via vanilla GD does not recover the ground truth function
fρ? ∈ HLNTK under noisy observations, if the DNN is trained for either too short or too long:
that is, the prediction error is bounded away from 0 by some constant as n goes to infinity.
•	In regularized case, we prove the mean-squared error (MSE) of DNN is upper bounded
by some positive constant. Additionally, we proved the dynamics of the estimated neural
network get close to the solution of kernel ridge regression associated with NTK from
ReLU DNN.
•	We show that the '2-regularization can be helpful in achieving the minimax optimal rate
of the prediction risk for recovering fρ? ∈ HNLTK under the noisy data. Specifically, it is
shown that after some iterations of '2-regularized GD, the minimax optimal rate (which is
O (n- 2d-1), where d is a feature dimension.) can be achieved.
Note that our paper is an extension of Hu et al. [2021] to DNN model, showing that the '2 -regularized
DNN can achieve a minimax optimal rate of prediction error for recovering fρ? ∈ HNLTK . However,
we would like to emphasize that our work is not a trivial application of their work from at least two
technical aspects. These aspects are more detailed in the following subsection.
1.2	Technical Comparisons with Hu et al. [2021]
Firstly, in the analysis of training loss of regularized shallow neural-net, Hu et al. [2021] begin the
proof by decomposing the difference between two individual predictions into two terms: one that
is related with the gram matrix evaluated at each iteration of the algorithm and the perturbation
term. Henceforth, we name this decompostion as “Gram+Pert” decomposition. This decomposition
can be checked with the equality (E.2) in the supplementary PDF of Hu et al. [2021]. The key
ingredients for the decomposition are (i) the simple gradient structure of the shallow neural net, and
(ii) the partitioning of the nodes in the hidden-layer into two sets: a set of nodes whose activation
2
Published as a conference paper at ICLR 2022
patterns change from their initializations during training, and the complement of the set. This
construction of the sets peels off the ReLU activation in the difference so that the GD algorithm can
be involved in the analysis. However, because of the compositional structure of the network, the
same nodes partitioning technique cannot be applied for obtaining the decomposition in the DNN
setting with ReLU activation. To avoid this difficulty, we employ a specially designed diagonal
matrix Σ and this matrix can peel off the ReLU function for each layer of the network. (See
the definition of Σ in the proof of Theorem 3.5 in the Appendix.) Recursive applications of this
diagonal matrix across the entire hidden layers enable the Gram+Pert decomposition in our setting.
It should be noted that the diagnoal matrix Σ had been employed in Zou et al. [2020], which
analyzed the behavior of training loss of classification problem via ReLU DNN under logistic loss.
However, since their result is dependent on different data distribution assumption under the different
loss function from ours, they didn’t employ the Gram+Pert decomposition. Thus their technical
approaches are different from ours.
Secondly, Hu et al. [2021] directly penalized the weight parameter W by adding kWk2 3F to
the objective function. The '2-regularization solely on the W has an effect of pushing the weight
towards the origin. This makes kW(k) - W(0) k2 ≤ O 1 2, allowing most activation patterns of
the nodes in the hidden layer can change during the training, even in overparametrized setting.
Here, W(k) denotes the updated weight parameter at kth itertaion of algorithm, and ∣∣ ∙ ∣∣2 denotes
the spectral norm of the matrix. Nonetheless, this doesn’t affect the analysis on obtaining the
upper-bound of MSE in shallow neural net, since the network has only a single hidden layer. In
contrast, in the DNN setting, we allow the non-convex interactions of parameters across the hidden
layers. To the best of our knowledge, a technique for controlling the size of '2-norm of network
gradient has not been developed under this setting, yet. We circumvent this difficulty by regularizing
the distance between the updated and the initialized parameter, instead by directly regularizing
the updated parameter. This ensures that the updated parameter by '2-regularized GD stays in
a close neighborhood to its initialization, so that with heavy over-parametrization, the dynamics
of network becomes linearized in parameter and we can ignore the non-convex interactions of
parameters across the hidden layers. Specifically, under suitable model parameter setting, we prove
that ∣w'k) - W(0)∣∣2 ≤ Oep(√m)3 over all' ∈ {1,...,L}. Here, Oep(∙) hides the dependencies on
the model parameters; L, ω, and n. This result allows us to adopt the so-called “Forward Stability”
argument developed by Allen-Zhu et al. [2018], and eventually leads to the control of network
gradient under '2 sense.
1.3	Additional Related works
There has been another line of work trying to characterize the generalizabilities of DNN under noisy
observation settings. Specifically, it has been shown that the neural network model can achieve
minimax style optimal convergence rates of L2-prediction risk both in regression [Bauer & Kohler,
2019; Liu et al., 2019; Schmidt-Hieber, 2020] and classification [Kim et al., 2021] problems.
Nonetheless, a limitation of the aforementioned papers is that they assume an adequate minimizer
of the empirical risk can be obtained. In other words, the mathematical proofs of their theorems do
not correspond to implementable algortihms.
Recently, several papers, which study the generalization properties of neural network with
algorithmic guarantees, appear online. Specifically, Kohler & Krzyzak [2019] showed that the data
interpolants obtained through DNN by vanilla GD is inconsistent. This result is consistent with our
result, but they consider the overparametrized DNN that is a linear combination of Ω(n10d2) smaller
neural network, and the activation function they consider is sigmoid function, which is smooth and
differentiable. Along this line of research, Kuzborskij & Szepesvari [2021] (regression) and Ji et al.
[2021] (classification) showed that when training overparametrized shallow neural network, early
stopping of vanilla GD enables us to obtain consistent estimators.
2This was empirically shown to be true in paper Wei et al. [2019]. See Figure 3 in their paper. We provide
a brief mathematical explanation on why this result is hard to be shown in Appendix C.
3Readers can find the proof of this result in Appendix G.
3
Published as a conference paper at ICLR 2022
Notation. We use the following notation for asymptotics: For sufficiently large n, we write f (n) =
O(g(n)), if there exists a constant K > 0 such that f (n) ≤ Kg(n), and f(n) = Ω(g(n)) if f(n) ≥
K0g(n) for some constant K0 > 0. The notation f(n) = Θ(g(n)) means that f(n) = O(g(n)) and
f(n) = Ω(g(n)). Let(A, BiTr= Tr(A>B) for the two matrices A, B ∈ Rd1 ×d2. We adopt the
shorthand notation denoting [n] := {1, 2, . . . , n} for n ∈ N.
2	Problem Formulation
2.1	Non-parametric Regression
Let X ⊂ Rd and Y ⊂ R be the measureable feature space and output space. We denote ρ as a
joint probability measure on the product space X × Y, and let ρX be the marginal distribution of
the feature space X. We assume that the noisy data-set D := {(xi, yi)}in=1 are generated from the
non-parametric regression model yi = fρ? (xi) + εi, where εi i削. N(0,12) for i = 1, . . . , n. Let
fw (k) (∙) be the value of neural network evaluated With the parameters W at the k-th iterations of GD
update rule. At k = 0, we randomly initialize the weight parameters in the model following He ini-
tialization [He et al., 2015] with a slight modification. Then, the L2 prediction risk is defined as the
difference between two expected risks (i.e., excess risk) R(fw(k)) := Eρ~(χ,y)[(y — fw(k)(x))2]
and R(f?) := Ep~(x,y)[(y — f?(x))2], where f?(x) := E[y∣x]. Then, We can easily show the
prediction risk has a following form:
Rfkf := R(fw (k)) - R(f?) = Eρχ,ε (fw (k)(x) - f?(x))2 .	(1)
Note that the expectation is taken over the marginal probability measure of feature space, ρx , and
the noise of the data, ε. However, the (1) is still a random quantity due to the randomness of the
initialized parameters (w'0))£=i 工.
2.2	Deep Neural Network with ReLU activation
Following the setting introduced in Allen-Zhu et al. [2018], we consider a fully-connected deep
neural networks with L hidden layers and m network width. For L ≥ 2, the output of the network
fw(∙) ∈ R with input data X ∈ X can be formally written as follows:
fw(x) = √m ∙ vTσ(WLσ(WL-i …σ(Wιx)…))，	⑵
where Sd-1 is a unit sphere in d-dimensional euclidean space, σ(∙) is an entry-wise activation
function, W1 ∈ Rm×d, W2, . . . , WL ∈ Rm×m denote the weight matrices for hidden layers and
v ∈ Rm×1 denote the weight vector for the output layer. Following the existing literature, we will
consider ReLU activation function σ(x) = max(x, 0), which is the most commonly used activation
function by practitioners.
Random Initialization. Each entries of weight matrices in hidden layers are assumed to be
generated from (Wij)`=1 L 〜 N(0, m2), and entries of the output layer are drawn from
Vj 〜 N(0, ω). This initialization scheme helps the forward propagation neither explode nor vanish
at the initialization, seeing Allen-Zhu et al. [2018]; Zou et al. [2018; 2020]. Note that we initialize
the parameters in the last layer with variance ω, where ω ≤ 1 is a model parameter to be chosen
later for technical convenience.
Unregularized GD update rule. We solve a following '2 -loss function with the given dataset
D
1n
LS(W) = 2∑ (yi- fw(xi))2.
i=1
(3)
Let W(10), . . . , WL(0) be the initialized weight matrices introduced above, and we consider a follow-
ing gradient descent update rule:
Wk= WL- ηVw∕Ls(W'k-1))),
` ∈ [L],	k ≥ 1,
(4)
4
Published as a conference paper at ICLR 2022
where ^w` (LS(∙)) is a partial gradient of the loss function LS(∙) with respect to the '-th layer
parameters W', and η > 0 is the learning rate of the gradient descent.
'2 -regularized GD update rule.	The estimator is obtained by minimizing a '2-regularized
function;
L2
Φd(W) ：= LS(WD) + 2 X ∣∣Wd,' - W(0)'∣∣F .	(5)
乙'=1
Naturally, We update the model parameters { Wd,' '==1	L via modified GD update rule:
WDi = (1 - η2μ)WD-1)- ηιVwJLS(WDkT))] + η2μW(0)',	∀' ∈ [L], ∀k ≥ L ⑹
The notations ηι, η are step sizes, and μ > 0 is a tuning parameter on regularization. We adopt
the different step sizes for the partial gradient and regularized term for the theoretical conveniences.
Furthermore, we add the additional subscript D to the update rule (6) to denote the variables are
under the regularized GD update rule. Recall that the W(O)` are initialized parameters same with
the unregularized case. For simplicity, we fix the output layer, and train L hidden layers for both
unregularized and regularized cases.
3	Main Theory
First, we describe the neural tangent kernel (NTK) matrix of (2), which is first proposed by Jacot
et al. [2018] and further studied by Arora et al. [2019b]; Du et al. [2019]; Lee et al. [2018]; Yang
[2019]. NTK matrix of DNN is a L-times recursively defined n × n kernel matrix, whose entries
are the infinite-width limit of the gram matrix. Let vw` [fw(0)(∙)] be the gradient of the ReLU
DNN (2) with respect to the weight matrix in the 'th hidden layer at random initialization. Note that
when ' = 1, VwJfw(O)S] ∈ Rm×d and when ' ∈ {2,...,L}, VwJfw(O)S] ∈ Rm×m. Then,
as m → ∞,
H(O) = (j1^X<vw' [fw(0) (Xi)], vw'fw(0)(Xj )DTr)	→ H∞,	⑺
∖m '=1	n n×n
where H∞ := {Ker(xi, Xj)}：j=「Here, Ker(∙, ∙) denotes a NTK function of (2) to be defined as
follows:
Definition 3.1. (NTK function of (2)). For any X, X0 ∈ X and ' ∈ [L], define
Φ(O) (X, X0) = hX, X0i,
θ(') (X XO)= ( φ('T)(X, X)	φ('T)(x, x0) ) ∈ R2×2
θ (X, X )=(Φ(J)(X0, x) Φ(J)(X0, x0)J ∈ R ,
Φ(')(X, X0) = 2 ∙	E	[σ(u)	∙	σ(v)],	and
(u,v)〜N(0,Θ(') )
Φ(')(x, X0) = 2 ∙	E	[σ(u)	∙	σ(v)],
(u,v)〜N (0,Θ('))
where σ(u) = 1 (U ≥ 0). Then, we can derive the final expression OfNTKfunction of(2) asfollows:
Ker(x, x0) = 2 ∙ X (φ('-1)(x, x0) ∙ Y Φ('0)(x, x0)).	(8)
'=1、	'0 ='	)
The expression in (8) is adapted from Cao & Gu [2019]. As remarked in Cao & Gu [2019], a coef-
ficient 2 in Φ(') and Φ(')remove the exponential dependence on the network depth L in the NTK
function. However, when compared with the NTK formula in Cao & Gu [2019], (8) is different from
two aspects: (i) An additional factor ω in (8)) comes from the difference in initialization settings
of the output layer, in which Cao & Gu [2019] considers Vj 〜N(0,*),whereas we consider
Vj 〜 N(0, m). (ii) Φ(L) is not added in the final expression of (8)), whereas it is added in the
5
Published as a conference paper at ICLR 2022
definition provided in Cao & Gu [2019]. This is because we only train the L hidden layers but fix the
output layer, while Cao & Gu [2019] train the entire layers of the network including the output layer.
As already been pointed by several papers, Cho & Saul [2009] and Jacot et al. [2018], it can
be proved that the NTK function (8) is a positive semi-definite kernel function. Furthermore, Cho &
Saul [2009] prove that the expectations in Φ and Φ have closed form solutions, when the covariance
matrices have the form ( 1t 1t ) with |t| ≤ 1:
E	l^σ(u) ∙ σ(v)]=——(t ∙ (π — arccos(t)) +，1 — t2
(u,v)〜N(0,Θ('))	2∏ \
(9)
E	[σ(u) ∙ σ(v)] =	(∏ - arccos(t) .
(u,v)〜N (0,Θ('))	2π
Clearly, (8) is symmetric and continuous on the product space X × X, from which it can be implied
that Ker(∙, ∙) is a Mercer kernel inducing an unique RKHS. Following Ghorbani et al. [2020], We
define the RKHS induced by (8) as:
Definition 3.2. (NTK induced RKHS). For some integer P ∈ N, set of points {xj }p=ι ⊂ X, and
weight vector α := {α1, . . . , αp} ∈ Rp, define a complete vector space of functions, f : X → R,
HNTK:= cl ({f (∙) = XX αjKer(∙,Xj)}),	(10)
where cl(∙) denotes closure.
In the remaining of our work, we assume the regression function f? (x) := E[y∣x] belongs to HNTK
3.1	Assumptions.
In this subsection, we state the assumptions imposed on the data distribution with some remarks.
(A1) ρX is an uniform distribution on Sd-1 := {x ∈ Rd | kxk2 = 1}, and noisy observations
are assumed to be bounded. (i.e., Px 〜Unif Sd-1 ,yi = O(1), ∀i ∈ [n].)
(A2) Drawn i.i.d. samples {xi,f?(xi)}n=i from the joint measure ρ. Then, with probability at
least 1 - δ, we have λmin HL∞ = λ∞ > 0.
Remark 3.3.
•	When the feature space is restricted on the unit sphere, the NTK function in (8) becomes
rotationally invariant zonal kernel. This setting allows to adopt the results of spectral decay
of (8) in the basis of spherical harmonic polynomials for measuring the complexity of
hypothesis space, HLNTK . See the subsection 3.2 and references therein.
•	Assumption (A2) is commonly employed in NTK related literature for proving global con-
vergence of training error and generalization error of both deep and shallow neural net-
work, Du et al. [2018; 2019]; Arora et al. [2019a]. Note that the (A2) holds as long as no
two xi and xj are parallel to each other, which is true for most of the real-world distribu-
tions. See the proof of this claim in Du et al. [2019].
3.2	MINIMAX RATE FOR RECOVERING fρ? ∈ HLNTK
The obtainable minimax rate of L2-prediction error is directly related with the complexity of func-
tion space of interest. In our setting, the complexity of RKHS HNLTK can be characterized by the
eigen-decay rate of the NTK function. Since Ker(x, x0) is defined on the sphere, the decomposition
can be given in the basis of spherical harmonics as follows:
∞	N (d,k)
Ker(x, xO)= £〃k X	Yk,j (x)Yk,j (x ),
k=0 j=1
6
Published as a conference paper at ICLR 2022
where Kj,j = 1,..., N(d, k) are spherical harmonic polynomials of degree k and {μk}∞=0
are non-negative eigenvalues. Recently, several researchers, both empirically [Basri et al., 2020]
and theoretically [Chen & Xu, 2020; Geifman et al., 2020; Bietti & Bach, 2021], showed that,
for large enough harmonic function frequency k, the decay rate of the eigenvalues μk is in the
order of Θ(k-d) 4. Given this result and the fact N(d, k) = 2"-3 (k+--3) grows as kd-2 for
large k, it can be easily shown λj = Θ(j-d-1), when Ker(x, x0)= Pj∞=1 λjφj(x)φj (x0), for
eigen-values λι ≥ λ2 ≥ ∙∙∙ ≥ 0 and orthonormal basis {φj}∞=ι∙ Furthermore, it is a well known
fact that if the eigenvalues decay at the rate λj = Θ(j -2ν), then the corresponding minimax rate for
estimating function in RKHS is O(n- 2ν+1), [Raskutti et al., 2014; Yuan & Zhou, 2016; HU et al.,
2021]. By setting 2ν = d-ɪ, we can see the minimax rate for recovering f? ∈ HLTK is O (n-2d-1).
Remark 3.4. We defer all the technical proofs of the Theorems in subsections 3.3 and 3.4 in the Ap-
pendix for conciseness of the paper. We also provide numerical experiments which can corroborate
our theoretical findings in the Appendix A.
3.3	Analysis of Unregularized DNN
In this subsection, we provide the results on the training loss of DNN estimator obtained via mini-
mizing unregularized'2-loss (3) and on the corresponding estimator,s L2-predictionriskR(fk, f?).
Theorem 3.5. (Optimization) For some δ ∈ [0,1], set the width of the network as ^gmm) ≥
Ω( 3；^8£), and set the step-size of gradient descent as η = O( nλ∞ m).Then, with probability at
least 1 一 δ over the randomness of initialized parameters W(O) := {w'0) }L+1 with W(+ι = V,
we have for k = 0, 1, 2, . . . ,
LS(W(k)) ≤ (1 - ηmλ∞) LS(W⑼).	(11)
In other words, the training loss drops to 0 at a linear rate.
We acknowledge a series of past works Allen-Zhu et al. [2018]; Du et al. [2019] have similar spirits
with those in Theorem 3.5. However, it is worth noting that their results are not applicable in our
problem settings and data assumptions. Specifically, the result of Du et al. [2019] is based on the
smooth and differentiable activation function, whereas the Theorem 3.5 is about the training error of
ReLU activation function, which is not differentiable at0. Furthermore, the result of Allen-Zhu et al.
[2018] relies on φ-separateness assumption stating that the every pair of feature vectors xi, xj in6=j
is apart from each other by some constant φ > 0 in a Euclidean norm. In our work, the positivity
assumption on the minimum eigenvalue of the NTK is imposed (i.e., λ∞ > 0).
Remark 3.6. Reducing the order of network width is definitely another line of interesting research
direction. We are aware of some works in literature, but we chose not to adopt the techniques since
this can make the analysis overly complicated. To the best of our knowledge, the paper that most
neatly summarizes this line of literature is Zou & Gu [2019]. See the table in page 3 in their paper.
The order ofwidth they obtained is Ω(nL ), where they impose φ-separateness assumption.
Remark 3.7. There has been an attempt to make a connection between the positivity and φ-
SeParateneSS assumptions. Recently, Zou & Gu [2019] proved the relation λ∞ = Ω (φn-2) 4 5 * in
a shallow-neural net setting. See Proposition 3.6. of their work. However, it is still an open ques-
tion on whether this relation holds in DNN setting as well. The results in Theorem 3.5 suggest a
positive conjecture on this question. Indeed, plugging the relation λ∞ = Ω (φn-2) in (11) and in
the η = O( nλ∞m)yield the discount factor(1 - Ω( ηmφ ))k and SteP-Size η = O(标？2m),which
4In shallow neural network with ReLU activation without bias terms, it is shown that μk satisfy μo, μι > 0,
μk = 0 if k = 2j + 1 with j ≥ 1, and otherwise μk = Θ(k-d). See Bietti & Mairal [2019]. However, in
ReLU DNN, it is shown that these parity constraints can be removed even without bias terms and μk achieves
Θ k-d decay rate for large enough k. Readers can refer Bietti & Bach [2021] for this result.
5We conjecture that this is not the tightest lower bound on λ∞. Recently, Bartlett et al. [2021] proves that
λ∞ & d/n in shallow neural net setting. See Lemma 5.3 in their paper.
7
Published as a conference paper at ICLR 2022
are exactly the same orders as presented in Allen-Zhu et al. [2018]. See Theorem 1 of their ArXiv
version paper for the clear comparison. We leave the proof of this conjecture as a future work.
Theorem 3.8. (Generalization) Let fρ? ∈ HNLTK. Fix a failure probability δ ∈ [0, 1]. Set the width
of the network as bgmm) ≥ Ω(	),the step-size of gradient descent as η = O(九杀团),
and the variance parameter ω ≤ O((λ∞δ)2/3). Then, if the GD iteration k ≥ Ω(；臂鼠))or
k ≤ O (ηm1ωL), with probability at least 1 一 δ over the randomness OfinitiaIizedparameters W(O),
we have
RGk ,f?) =Ω(1).
This theorem states that if the network is trained for too long or too short, the L2-prediction error of
fW(k) is bounded away from 0 by some constant factor. Specifically, the former scenario indicates
that the overfitting can be harmful for recovering fρ? ∈ HLNTK given the noisy observations.
Remark 3.9. Readers should note that the Theorem 3.8 does not consider if the GD algorithm
can achieve low prediction risk Rf ,f?) over the range of iterations (ηmωL)-1 . k .
(ηmλ∞)-1 log(n). In the numerical experiment to be followed in Appendix A, we observe that
for some algorithm iterations k*, the risk indeed decreases to the same minimum as low as the
`2 -regularized algorithm can achieve, and increases again. This observation implies that the unreg-
ularized algorithm can achieve the minimax rate of prediction risk. However, analytically deriving
a data-dependent stopping time k* in our scenario requires further studies, since we need a sharp
characterization of eigen-distribution of NTK matrix of ReLU DNN, denoted as HL∞ in this paper.
Readers can refer the Theorem 4.2. ofHu et al. [2021] in shallow-neural network and equation (6)
in Raskutti et al. [2014] in kernel regression context on how to compute k? with the given eigen-
values of the associated kernel matrices.
3.4	ANALYSIS OF `2 -REGULARIZED DNN
In this subsection, We study the training dynamics of '2-regularized DNN and the effects of the reg-
ularization for obtaining the minimax optimal convergence rate of L2-prediction risk. In the results
to be followed, we set the orders of model parameters μ, η1, η2 in (6), and a variance parameter of
output layer, ω as folloWs:
ω=O
(12)
Theorem 3.10. (Optimization) Suppose we minimize '2-regularized objective function (5) via mod-
ified GD (6). Set the network width 匕目3(m)≥ Ω( L20n24) and model parameters as in (12). Then,
with probability at least 1 一 δ, the mean-squared error follows
LS(W(O))/n + Op(1),
(13)
for k ≥ 0. Additionally, after k ≥ Ω((η2μL)-1 log(n3∕2)) iterations of (6), for some constant
C > 0, we have
Ud (k) — H∞	∙I + H∞)	y
≤ OP
2
(14)
where we denote uD(k)
>
[fW(Dk)(x1), . . .,fWD(k)(xn)] .
Several comments are in sequel. Theorem 3.10 is, to our knowledge, the first result that rigorously
shows the training dynamics of '2-regularized ReLU DNN in overparametrized setting. Observe
that the first term on the right-hand side of the inequality (13) converges linearly to 0, and the
second term is some positive constant that is bounded away from 0. This implies that the MSE of
regularized DNN is upper-bounded by some positive constant. Note that we only provide the upper
8
Published as a conference paper at ICLR 2022
bound, but the results of our numerical experiments indicate that the MSE is lower-bounded by
OP(1) as well. We leave the proof of this conjecture for the future work.
The inequality (14) states that the trained dynamics of the regularized neural network can
approximate the optimal solution (denoted as g；) of the following kernel ridge regression problem:
f ∈⅛ {2 XeLf (Xi))2+c2μ kf kHLTK},	(15)
i=1
where ∣∣ ∙ ∣∣hntk denotes a NTK-induced RKHS norm. Note that the optimization problem in
(15) is not normalized by sample size n. The inequality (14) states that after approximately
(η2μL)-1 iterations of (6), the error rate becomes Op(1). The approximation error is computed
n
at the training data points under `2 norm. This should be compared with the Theorem 5.1 of Hu
et al. [2021], where they showed that the similar approximation holds “within” a certain range of
algorithm in shallow neural network setting. In contrast, we show that the approximation holds
“after” k ≥ Ω((η2μL)-1 log(n3/2)) in deep neural network. It should be noted that the difference
of results comes from the regularization scheme, where We penalize the PL=I ∣∣W' — w'0)kF,
whereas Hu et al. [2021] regularized the term ∣W1∣2F.
As another important comparison, Hu et al. [2019] showed the equivalence of a solution of
kernel ridge regression associated with NTK and the first order Taylor expansion of the regularized
neural network dynamics; note, however, that the uD(k) in (14) is a full neural network dynamics.
Let R(fW(k) , fρ?) be the L2-prediction risk of the regularized estimator fW(k) via modified GD (6).
DD
Next theorem states the result of generalization ability of f (k) .
WD
Theorem 3.11. (Generalization) Let f? ∈ HLNTK. Suppose the network width 3学团)≥ Ω(L20n24)
and model parameters are set as suggested in (12). Then, with probability tending to 1, we have
RGWD ,f?)
The resulting convergence rate is O(n- 2d-1) with respect to the training sample size n. Note that
the rate is always faster than O n-1/2 and turns out to be the minimax optimal [Caponnetto &
De Vito, 2007; Blanchard & Mucke, 2018] for recovering f? ∈ HNTK in the following sense:
lim lim inf inf sup P R(f,f?) > rn- 2d-1 =1,	(16)
r→0 n→∞ fb ρ	ρ
where ρ is a data distribution class satisfying the Assumptions (A1), (A2) and fρ? ∈ HNLTK, and
infimum is taken over all estimators D → fb. It is worth noting that the minimax rate in (16) is
same with the minimax rate for recovering fρ? ∈ H1NTK. (i.e., Hu et al. [2021]) This result can be
derived from the recent discovery of the equivalence between two function spaces , H1NTK = HLNTK .
See Geifman et al. [2020] and Chen & Xu [2020].
/ d — 1 ∖
Remark 3.12. A particular choice of μ = Θ (n2d-1) in (12) is for obtaining an optimal minimax
rate for prediction error in Theorem 3.11. Specifically, the order of μ determines the L2 distance
between the f? and the kernel regressor g∖. That is, ∣∣f? 一 g?∣∣2 = Op(n). With the result HNTK =
HNLTK, the same proof of Lemma D.2. in Hu et al. [2021] can be applied for proving this result.
4 Conclusion
We analyze the convergence rate of L2-prediction error of both the unregularized and the regular-
ized gradient descent for overparameterized DNN with ReLU activation for a regression problem.
Under a positivity assumption of NTK, we show that without the adoption of early stopping, the
L2-prediction error of the estimated DNN via vanilla GD is bounded away from 0 (Theorem 3.5),
whereas the prediction error of the DNN via '2-regularized GD achieves the optimal minimax rate
(Theorem 3.11). The minimax rate O(n- 2dd-1) is faster than the O(n-1/2) by specifying the com-
plexities of target function and hypothesis space.
9
Published as a conference paper at ICLR 2022
Acknowledgments
This project is partially supported by the Transdisciplinary Research Institute for Advancing Data
Science (TRIAD), http://triad.gatech.edu, which is a part of the TRIPODS program at NSF and lo-
cates at Georgia Tech, enabled by the NSF grant CCF-1740776. Authors are also partially supported
by NSF grant DMS-2015363.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. arXiv e-prints, pp. arXiv-1811, 2018.
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparame-
terized neural networks, going beyond two layers. Advances in Neural Information Processing
Systems, 32:6158-6169, 2019.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of op-
timization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pp. 322-332. PMLR, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. Advances in Neural Information Processing
Systems, 32:8141-8150, 2019b.
Peter L. Bartlett, Andrea Montanari, and Alexander Rakhlin. Deep learning: a statistical viewpoint.
Acta Numerica, 30:87-201, 2021. doi: 10.1017/S0962492921000027.
Ronen Basri, Meirav Galun, Amnon Geifman, David Jacobs, Yoni Kasten, and Shira Kritchman.
Frequency bias in neural networks for input of non-uniform density. In International Conference
on Machine Learning, pp. 685-694. PMLR, 2020.
Benedikt Bauer and Michael Kohler. On deep learning as a remedy for the curse of dimensionality
in nonparametric regression. Annals of Statistics, 47(4):2261-2285, 2019.
Alberto Bietti and Francis Bach. Deep equals shallow for ReLU networks in kernel regimes. In
ICLR 2021-International Conference on Learning Representations, pp. 1-22, 2021.
Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. In NeurIPS
2019-Thirty-third Conference on Neural Information Processing Systems, volume 32, pp. 12873-
12884, 2019.
Gilles Blanchard and Nicole MUcke. Optimal rates for regularization of statistical inverse learning
problems. Foundations of Computational Mathematics, 18(4):971-1013, 2018.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and
deep neural networks. Advances in Neural Information Processing Systems, 32:10836-10846,
2019.
Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm.
Foundations of Computational Mathematics, 7(3):331-368, 2007.
Lin Chen and Sheng Xu. Deep neural tangent kernel and Laplace kernel have the same RKHS. In
International Conference on Learning Representations, 2020.
Lenalc Chizat and Francis Bach. On the global convergence of gradient descent for over-
parameterized models using optimal transport. In Proceedings of the 32nd International Con-
ference on Neural Information Processing Systems, pp. 3040-3050, 2018.
Youngmin Cho and Lawrence Saul. Kernel methods for deep learning. Advances in Neural Infor-
mation Processing Systems, 22:342-350, 2009.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-
1685. PMLR, 2019.
10
Published as a conference paper at ICLR 2022
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2018.
Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, and Ronen Basri. On
the similarity between the Laplace and neural tangent kernels. arXiv preprint arXiv:2007.01580,
2020.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural
networks outperform kernel methods? arXiv preprint arXiv:2006.13409, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on Imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Tianyang Hu, Wenjia Wang, Cong Lin, and Guang Cheng. Regularization matters: A nonparamet-
ric perspective on overparametrized neural network. In International Conference on Artificial
Intelligence and Statistics, pp. 829-837. PMLR, 2021.
Wei Hu, Zhiyuan Li, and Dingli Yu. Simple and effective regularization methods for training on
noisily labeled data with generalization guarantee. In International Conference on Learning Rep-
resentations, 2019.
Arthur Jacot, Clement Hongler, and Franck Gabriel. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In NeurIPS, 2018.
Ziwei Ji, Justin Li, and Matus Telgarsky. Early-stopped neural networks are consistent. Advances
in Neural Information Processing Systems, 34, 2021.
Yongdai Kim, Ilsang Ohn, and Dongha Kim. Fast convergence rates of deep neural networks for
classification. Neural Networks, 138:179-197, 2021.
Michael Kohler and Adam Krzyzak. Over-parametrized deep neural networks do not generalize
well. arXiv preprint arXiv:1912.03925, 2019.
Ilja KUzborskij and Csaba Szepesvari. Nonparametric regression with shallow overparameterized
neural networks trained by GD with early stopping. In Conference on Learning Theory, pp.
2853-2890. PMLR, 2021.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. Deep neural networks as Gaussian processes. In International Conference on
Learning Representations, 2018.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In NeurIPS, 2018.
Ruiqi Liu, Ben Boukai, and Zuofeng Shang. Optimal nonparametric inference via deep neural
network. arXiv preprint arXiv:1902.01687, 2019.
Atsushi Nitanda and Taiji Suzuki. Optimal rates for averaged stochastic gradient descent under
neural tangent kernel regime. In International Conference on Learning Representations, 2020.
Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv
preprint arXiv:1710.05941, 2017.
Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Early stopping and non-parametric regression:
an optimal data-dependent stopping rule. The Journal of Machine Learning Research, 15(1):
335-366, 2014.
Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU acti-
vation function. Annals of Statistics, 48(4):1875-1897, 2020.
Richard S Varga. Gersgorin-type theorems for partitioned matrices. In Gersgorin and His Circles,
pp. 155-187. Springer, 2004.
11
Published as a conference paper at ICLR 2022
Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018.
Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and
optimization of neural nets vs their induced kernel. Advances in Neural Information Processing
Systems, 32, 2019.
Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,
2019.
Ming Yuan and Ding-Xuan Zhou. Minimax optimal rates of estimation in high dimensional additive
models. AnnalsofStatistics, 44(6):2564-2593, 2016.
Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural
networks. Advances in neural information processing systems, 2019.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep ReLU networks. arxiv e-prints, art. arXiv preprint arXiv:1811.08888,
2018.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-
parameterized deep ReLU networks. Machine Learning, 109(3):467-492, 2020.
12
Published as a conference paper at ICLR 2022
A Numerical illustrations
(a) Optimization
JOXIW Peenbs cπωE
0.5-
⅛≡ uon--pəjd
∏=1OO, Unreg
π=100, Reg
(b) Generalization Error
0.6-I—■------------------------------
0.18
0.16
0.14
» 0.12
,∣o,1o
- 0.08
2 _ _
æ 0.06
0.04
0.02
(c) Generalization Error
0	200	400	600	800	1000	0	200	400	600	800	1000	0	1000	2000	3000	4000	5000
Epochs	Epochs	Sample Size
Figure 1: Results on synthetic data.
In this section, we use synthetic data to corroborate our theoretical findings. We use the He
initialization [He et al., 2015] and employ ('2-regularized) GD as introduced in subsection 2.2. For
the experiments, we run 1000 epochs of GD and use a fixed step size, setting η1 = η2 = 0.001. We
uniformly generate n feature data Xitram from Sd-1 with d = 2 and generate Ni from f? (Xiram) with
εi ~ N(0,1). To create a function f? ∈ HLTK, We use the definition in (10) with α ∈ Unif(SPT)
and with P fixed points {Xj}p=ι ⊂ Unif(Sd-1), where P is simply set as 1. Note that Ker(∙, ∙)
in (10) can be calculated via the formulas (8) and (9) with specified network depth L. We consider a
scenario where we have a network with depth L = 8 and width m = 2000. The variance parameter
of the output layer (ω) is set as 1 for unregularized and 0.001 for regularized cases.
In Fig 1.(a), we record the training errors of regularized networks over the GD epochs k ≤ 1000,
where we have n ∈ {100, 300, 500, 1000, 5000} training samples. This aims to verify the inequal-
ity (13) that the MSE of regularized network is bounded away from 0 by some constant. In Fig 1.(b),
the prediction risks of both unregularized and regularized networks are displayed. We approximate
the risk with 忐 P：=； fk (XjeSt) - f?(XjeSt))2 with a new test data set {xjest, f?(XjeSt)}5=0l over
k ≤ 1000 for both unregularized and regularized cases. In both cases, they reach the same minimal
risks, but the risk of unregularized network increase after it hits the minimal point, whereas the
risk of regularized network stays stable. Theorem 3.6 tells us that for the iteration less than the
order O (nm^L), the prediction error is bounded away from 0. In the experiment for unregularized
case, we set η = 0.01, m = 2000, L = 8, and ω = 1. Plugging in these parameters in the bound
says that the minimum can be achieved within a very few iterations. Note that the optimal risk is
non-zero as long as we have finite sample sizes n, but converges to 0 at the rate O (n- 2d-1). In
Fig 1.(c), we verify that the more training sample sizes we have, the closer the risks of the regu-
larized networks get to 0. The risk is evaluated at the sample sizes n = {100, 300, 500, 1000, 5000}.
We have to acknowledge that there is a discrepancy between our experiment setting and the-
ory. Specifically, due to the limited computing power, we could not run the experiment under the
7 8L18
regime of width 味可一)≥ Ω(ωλ⅞ L ). But the prediction risk behaves similarly as expected
by our theorems, which can be a p∞artial evidence that the statement in theorems still holds in the
narrower width of the network.
B	Preliminary Notations
Before presenting the formal proofs of Lemmas and main results, we introduce several notations
used frequently throughout the proofs. First, we denote X',i the output of the 'th hidden layer with
the input data Xi after applying entry-wise ReLU activation function.
X',i = σ(Wgσ(W'-i …σ(WιX,…)).
Denote fW(k)(x) a value of neural network (2) evaluated at the collection of network parameters
W(k) := {w'k)}. 1 L and w'k) denotes the 'th hidden layer parameter updated by kth GD
= ,...,
13
Published as a conference paper at ICLR 2022
iterations.
Partial gradient of fW(k) (x). We employ the following matrix product notation which was
used in several other papers [Zou et al., 2018; Cao & Gu, 2019]:
`2
Y Ar :
r='ι
A'2 -1 …A'ι if 'l ≤ '2,
otherwise.
(17)
Then, the partial gradient of fw(k)(x) with respect to W(k) for 1 ≤ ' ≤ L has a following form:
for i ∈ {1, . . . , n},
vw' [fw(k)(χi)]=√m Jχ'-∖VT (γγ ∑rk?Wrk)) ∑'ki)l>,	` ∈ [l],
-	×r='+1	)	」
Where ς'? ：= Diag(I(hw'kι), χ'-)ι,ii ≥ O),..., l(hw'km, χ'-)ι,ii ≥ 0)) ∈ Rm×m and w'k)
(k)
denotes j th column of the matrix W' ∖
Gram matrix H(k). Each entries of empirical gram matrix evaluated at the kth GD update
are defined as follows:
1L
Hij (k) = m E〈VW' fW(k) (Xi)] , vW' fW(k) (Xj )DTr .
"'=1
Note that H(0) → HL∞ as m → ∞ which is proved in Jacot et al. [2018]; Yang [2019]; Lee et al.
[2018]; Arora et al. [2019b].
Perturbation region of weight matrices. Consider a collection of weight matrices
W = {W'}'=1	L such that
W ∈B(W⑼,τ) := {W` : kW` - W'0)k2 ≤ τ, ∀' ∈ [L]}.	(18)
For all i ∈ {1,...,n} and ' = 1,...,L, we denote X'i and X',i as the outputs of the '-th layer of
the neural network with weight matrices W(O) and W, and Σ',i and Σ',i are diagonal matrices with
(Σ',i)jj = l(hw'0), X'-ι,ii ≥ 0) and (Σ',i)jj = 1(〈W`j, e`-i,ii ≥ 0), respectively.
C	WHY IS IT HARD TO PROVE ∣∣W(k), — WD)'k2 ≤ O⑴？
In this subsection, we provide a heuristic argument on why it is hard to prove ∣∣WDk)' - W(0)'∣2 ≤
O⑴，where WM is the model parameter of 'th layer in kth iteration of algorithm. Here, we
regularize solely on the model parameter, instead on the relative to the initialization. In this case, we
can write the update rule as follows :
WDk,' = (1-η2 μ)WD-1)-ηιVw'[Ls(WDk-D)], ∀1 ≤ ' ≤ L and ∀k ≥ 1.	(19)
By recursively applying above equation (4.3), we can write Wl(k)' with respect to W(0)' as follows:
k-1
WD)` = (1 - η2μ)k W(0)' - ηι X(1 - η2μ)'Vw' [Ls(W(k-'-1))].
'=0
Then, we can control the bound as follows:
kWD)' - W乳∣2 ≤ (1 - (1 - η2μ)k) IIW黑Il2 + 念` max』VW' [Ls(W(r'T)] ∣∣2.
2 2 2	2 2 2	/∕2 μ =U,...,ΓV∙LII	2
14
Published as a conference paper at ICLR 2022
We know under the initialization setting in our paper, IIWDk)'∣∣2 ≤ O⑴ with high-probability
(See Vershynin [2018]), and as long as we can prove the '2-norm of gradient is bounded, then we
can conclude IlWDk' -W(0'∣∣2 ≤ O ⑴.However, we are not aware of works in which they control
the size of ∣Vw' [Ls (WDk-'-1))] ∣∣2 where the non-convex interactions between model parameters
across the hidden layers are allowed. To the best of our knowledge, we know the work Allen-Zhu
et al. [2019] deals with the three layer case under this setting. But we need further investigations
on whether the techniques employed in their paper can be generalized to arbitrary L-hidden layer
setting.
D Useful Lemmas
A simple fact. Suppose Vj 对 N (0, mω) for j ∈ [m]. Then, with probability at least 1 一
exp[-Ω(m)], kv∣2 ≤ O(ω).
Proof. Since ∣∣v2∣∣ψ ≤ O(m) for j ∈ [m], where ∣∣ ∙ ∣∣ψι denotes a sub-exponential norm, Bern-
stein’s inequality for i.i.d. centered sub-exponential random variables can be employed : For any
t ≥ 0,
t2
≥ t ≤ 2 exp - C min ---------L,-----π-^-r∣一
- V	IPj=IMl∣Ψ1' maxj∏‰ι
(20)
t
where c > 0 is an absolute constant. Note that we used the fact centering does not hurt the sub-
exponentiality of random variable. Choosing t = O (ω) concludes the proof.	□
Lemma 4.1 (Lemma 7.1. Allen-Zhu et al. [2018]). With probability at least 1 — O(nL) ∙
exp]—Ω(m∕L)], 3/4 ≤ kx'0"2 ≤ 5/4 for all i ∈ {1,..., n} and ' ∈ {1,..., L}.
Lemma 4.2 (Lemma B.1. Cao & Gu [2019]). If T ≤ O(L-9/2 [log(m)]-3), then with probability
at least 1 — O(nL) ∙ exp[-Ω(mτ2/3L)], 1/2 ≤ ke`,ikz ≤ 3/2 for all W ∈ B(W⑼，τ), i ∈
{1, . . . , n} and` ∈ {1, . . . , L}.
Lemma 4.3 ( Allen-Zhu et al. [2018]). Uniformly over i ∈ {1, . . . , n} and 1 ≤ `1 ≤ `2 ≤ L, the
following results hold:
1.	(Lemma.7.3, Allen-Zhu et al.[2018]) Suppose m ≥ Ω(nL log(nL)) ,then with probability
at least, 1 — O(nL2) ∙ exp[-Ω(mτ2/3L)],
'2
Y ∑ro>r0)	≤o(√l).
r='ι	2
2.	(Lemma.7.4, Allen-Zhu et al.[2018]) Suppose m ≥ Ω(nL log(nL)), then with probability
at least, 1 — O(nL) ∙ exp[—C(m/L)],
v>(Y ∑r0iM0))∣ ≤o(√w).
3.	(Lemma.8.2, Allen-Zhu et al. [2018]) Suppose T ≤ O(L-9/2[log(m)]-3). For all W ∈
B(W⑼,τ), with probability at least, 1 — O(nL2) ∙ exp[—Ω(mτ2/3L)],
∣∣e'ι,i-χ'1,i∣∣2 ≤ O(TL5/2Piog(m)).
4.	(CoroUary.8.4, Allen-Zhu et al. [2018]) Suppose T ≤ O(L-9/2[log(m)]-3), then with
probability at least, 1 — O(nL2) ∙ exp[-Ω(mτ2/3L)],
∣ς'ι,i- 4?i|0 ≤O(mτ2/3L).
15
Published as a conference paper at ICLR 2022
5.	(Lemma.8.7, Allen-Zhu et al.[2018]) Forall' ∈ [L], let Σ') ∈ [-3, 3]m×m be the diag-
onal matrices with at most s = O(mτ 2/3L) non-zero entries. For all W ∈ B W(0) , τ ,
where T = O (L15), with probability at least 1 一 O (nL) ∙ exp[-Ω(s log(m))],
V 彳 YY (Σ0,i + 琛？ )Wr,)(∑1,i + 琛i)) - v，( Y 琛？wr0?)琛i
'r='ι + 1	'r='ι + 1	)	2
≤ O(T 1/3L2P“log(m)).
Lemma 4.4 (Lemma B.3. Cao & Gu [2019]). There exists an absolute constant κ such that, with
probability at least 1 —O(nL2) ∙ exp[-Ω(mτ 2/3L)], i ∈ 1,...,n and ' ∈ 1,...,L and for all
W ∈ B(W(0),τ), with τ ≤ κL-6[log(m)]-3, it holds uniformly that
∣∣Vw'[fw(xi)]∣∣2 ≤O(√ωm).
Lemma 4.5. Suppose W ∈ B(W(0),τ) and T ≤ O(L-9/2[log(m)]-3). For all U ∈ Rm with a
CardinaUty ∣∣uko ≤ S ,for any 1 ≤ ' ≤ L and i ∈ {1,..., n}, with probability at least 1 - O(nL) ∙
exp ( — Ω(s log(m))) — O(nL) ∙ exp ( — Ω(mτ2%L)),
V>(Y ς r,f r,)u ≤ r ωs lm(m) ∙O(ku∣2).
'r=2	/
Proof. Recall Lemma 4.2. For any fixed vector U ∈ Rm, with probability at least 1 - O(nL) ∙
exp[-Ω(mτ2∕3L)] for T ≤ O(L-9/2[log(m)]-3), for any 1 ≤ ' ≤ L and i ∈ {1,..., n}, We have
the event T,
≤ 3 ∣U∣2 .
2
(21)
Conditioned on this event happens, it is easy to see the random variable v> (QL=a ∑r,iWr,i)u ~
SG(9ω l∣u∣2). Based on this observation, we have the probability,
矶 V>( 口 ς r,fr,i)u ≥ r ωs lm(m) ∙O(∏2))
≤ P(卜>(Y⅛ ςr,f r,i)u ≥ r ωs lm(m) ∙O(ku∣2) |T)+P(Tc)
≤ O(nL) ∙ exp ( — Ω(s log(m))) + O(nL) ∙ exp ( — Ω(mτ2/3L)),
where in the last inequality, union bounds over the indices ` and i, and over the vector U ∈ Rm with
IlUIlO ≤ s are taken.	口
Lemma 4.6. Suppose T ≤ cL9/2[；g(m)]3 for some constant C > 0. Then, for all i ∈ [n] and
' ∈ [L], with probability at least 1 —O(nL) ∙ exp[-Ω(mT2/3L)], we have
∣∣Vw' [fw(k)(xi)] -VW' [fw(0)(xi)] ∣∣2 ≤ O (T1/3L2 Pωm log(m)
16
Published as a conference paper at ICLR 2022
Proof. By using the results from Lemma 4.3, we can control the term :
∣∣VW' [fw(k)(xi)] -VW' [fW(0)(χi)] k2
=√m ∙ χ'-)1vτ( γγ ∑^Wrk)) ∑'k)-x'o)1vτ( γγ ∑rO)WrO))∑(0)
'r='+1	)	r= r='+1	)
2
≤ √m ∙
where, in the last inequality, we used the condition on τ ≤
CL9∕2[log(m)]3
< 1.
1
□
Remark 4.7. Note that the results in Lemmas 6.3 (second and fifth items), 6.4, 6.5, 6.6 are in the
setting of Vj 〜N(0, mω) for j ∈ [m]∙
For the notational convenience, in following Lemmas we denote fW(k) (xi) as ui(k) and let u(k) :=
[u1(k), . . . , un(k)]> for k ≥ 0.
Lemma 4.8. For some δ ∈ [0,1] ,if m ≥ Ω (L log(nL∕δ)), then with probability at least 1 一 δ,
∣∣u(k)∣∣2 ≤ o(√nω) for any k ≥ 0∙
Proof∙ Recall the Lemma 4.2 stating that x(Lk,)i = O(1) for any input data xi for i ∈ [n]. Also
recall that Vj 〜N(0, m) for j ∈ [m], xl,% ∈ Rm and g(k) = √mv>XL,i 〜N(0, O(ω)). Then,
we have a following via simple Markov inequality: for any t ≥ 0,
P ∣u(k)∣2 ≥ t
≤
E[ku(k)∣2] <，E[ku(k)k2] < o(√nω)
t ≤ t ≤ t
□
Lemma 4.9. For some δ ∈ [0,1], if m ≥ Ω (L log(nL∕δ)), then with probability at least 1 — δ, we
have
(K)
∣u(0) 一 y∣2 ≤ O
Proof∙ By Markov’s inequality, for any t ≥ 0,
P(ku(0) -y∣2 ≥ t)≤ Eε,W(0),vh[u(0)-yk2i.	(22)
Note that the expectation in the nominator of (22) is taken over the random noise ε and initialized
parameter W(0), v. We can expand the nominator as follows:
Eε,W(O),v h ∣u(0) - y∣22 i = EW(O),v∣u(0)∣22 + Eε∣y∣22 - 2Eε,W(O),v hy>u(0)i.	(23)
For the convenience of notation, let y* := [fp(xι),..., f?(xn)]> and ε := [ει,..., εn]>. Recall
that We have y = y* + ε, and ∣∣y*∣2 = O(n). Also note that by Lemma 4.1, with probability at
least 1 - O(nL) ∙ exp[-Ω(m∕L)], for any i = 1,...,n, 1x^12 = O(1). Then, we have a random
17
Published as a conference paper at ICLR 2022
variable u^(0) = √mvτxLi 〜N(0, O(ω)). Now, We are ready to derive the orders of three terms
onthe RHS of(23).	'
Ew(0),v ku(0)∣∣2 = O(n),
Eε∣∣y∣∣2 = E∕∣∣y*∣∣2 + 忖2 - 2yτε] = O(n),
Eε,W(0),v [yτu(0)] = Eε,w(0),v[(y* + ε)>u(0)] = 0.
Combining the above three equalities, we conclude the proof.	□
Lemma 4.10. Suppose T = O( Zn). For some δ ∈ [0,1] such that δ ≥ O(nL) ∙
Vmδλ∞
exp[一Ω(mτ2/3L)], then with probability at least 1 — δ, we have
但㈤-卬叽 ≤°(j"/3L3Sgm).
Proof. By the definition of gram matrix Hi,j (k) for any k ≥ 0, we have
∣Hi,j(k) - Hi,j(0)|
=—X (Vwg [fw(k) (Xi)], Vwg [fw(k)(Xj )] /	- (Vwg [fw(0)(Xi)], Vwg [fw(0)(Xj )]
m '=1 ∖	/ Tr ∖
Tr
≤ । X { |(Vwg [fw(k)(Xi)], vwg [fw(k)(Xj)] - Vwg [fw(0)(Xj)]
Tr
+ K Vwg [fW(0) (Xj)] , Vwg fW(k) (Xi)] - Vw' [∕w(0)(xi)]
Tr
≤ — X [ gw` [fw(k)(Xi)] II2 ∙ gw` [fw(k)(Xj)] - Vwg [fw(0)(Xj)] II2
m '=1 IS--------------{----------' S--------------------{---------------------'
≤O (√ωm)
≤O (τ1 /3L2 √ωm log(m))
+ IWwg [fw(0) (Xj )] I I 2 ∙ IWwg [fw(k)(Xi)] - Vw^ [∕w(0) (Xi)] I I 2
I
, I
{^^^^^^^^^^^^^^
≤O (√ωm)
≤o(j∕6n1∕3L3 61 l0g3⅛)).
∖	V mδλ∞ /
,
{^^^^^^^^^^^^
≤θ(τ1 /3L2 √ ωm log(m))
In the second inequality, Lemmas 4.4 and 4.6 are used, and in the last inequality, T = O( ^n^ )
is plugged in. With this, using the fact that Frobenius norm of a matrix is bigger than the operator
norm, we bound the term ∣∣H(k) - H(0)∣∣2 as follows:
∣∣H(k) - H(0)∣∣2 ≤ ∣∣H(k) - H(0)∣∣f ≤ O L7/6n4/3L3^lIm
□
Lemma 4.11. For some δ ∈ [0, 1], with probability at least 1 - δ,
∣∣H∞ - H(0)∣∣2 ≤ θ(ωnL5/2 R^n^)
Proof. For some δ' ∈ [0,1], set ε
For any fixed i,j ∈ [n], we have
L3/2 ʌ/ log(m/')from Theorem 3.1. of Arora et al. [2019b].
P [∣H∞ - Hi,j(0)∣ ≤ O (ωL5/2 √lo⅛^)] ≥ 1 - δ'.
18
Published as a conference paper at ICLR 2022
0
After applying the union bound over all i,j ∈ [n], setting δ =	, and using the fact that FrobeniUs
norm of a matrix is bigger than the operator norm, we conclude the proof.	□
For two positive semi-definite matrices A and B, if we write A B, then it means A - B is
positive semi-definite matrix. Similarly, ifwe write A B, then it means A - B is positive definite
matrix. With these notations, we introduce a following Lemma.
Lemma 4.12 (Lemma D.6. Hu et al. [2021]). For two positive semi-definite matrices A and B,
1.	Suppose A is non-singular, then A 占 B ^⇒ λmαx(BAT) ≤ 1 and A * B ^⇒
λmax(BA-1) < 1, where λmax(∙) denotes the maximum eigenvalue ofthe input matrix.
2.	Suppose A, B and Q are positive definite matrices, A and B are exchangeable, then
A B =⇒ AQA	BQB.
E Proof of Theorem 3.5
For the convenience of notation, denote ui(k) = fW(k) (xi) and let u(k) =
u1(k), u2(k), . . . , un (k)>. In order to achieve linear convergence rate of the training error,
ku(k) - yk22, we decompose the term as follows:
ku(k +1)- yk2 = ∣∣u(k)- y + (U(k +1) - u(k)) 112
=ku(k) - yk2 - 2(u(k) - y)>(u(k + 1) - u(k)) + ku(k + 1) - u(k)k2.
Equipped with this decomposition, the proof consists of the following steps:
1.	Similarly with Du et al. [2019], a term U(k + 1) - U(k) is decomposed into two terms,
where we denote them as I(1k) and I(2k), respectively. We note that the first term I(1k) is
related with a gram matrix H(k) and a second term I(2k) can be controlled small enough in
`2 sense with proper choices of the step size and the width of network.
2.	A term ∣∣u(k +1) 一 u(k)k2 needs to be controlled small enough to ensure 2(u(k) —
y)>(u(k + 1) 一 u(k)) > ∣∣u(k + 1) 一 u(k)∣2 so that the loss decreases.
3.	It is shown that the distance between the gram matrix H(k) and the NTK matrix HL∞ is
close enough in terms of operator norm.
4.	Lastly, we inductively show that the weights generated from gradient descent stay within a
perturbation region B W(0), τ , irrespective with the number of iterations of algorithm,
We start the proof by analyzing the term U(k + 1) 一 U(k).
Step 1. Control on u(k + 1) 一 u(k).	Recall (∑k)).. = l(hw'k),x'-[3≥ 0) and We
introduce a diagonal matrix Σ'?, whose jth entry is defined as follows:
(ς `ki))jj = (e`k+1)
—
*)	hw`k+1), x'-+?)
',i'jj ∙(w'k+1), x'-+ι)i - hw'j), x'-)ι,i)
With this notation, the difference x(Lk,+i 1) 一 x(Lk,)i can be rewritten via the recursive applications of
vXk+I) _ Y(k) — (y*(k) I e(k))(^W(k+I)Y(k+I) _ ʌv(k)γ(k)	)
xL,i	一 xL,i = ΣL,i + ΣL,i WL	xL-1,i 一 WL xL-1,i
=(∑L5 + ∑ LS)WLk+1) (XL-ι1,i - χLk-ι,i) + (∑Lki + ∑ LS)(WLk+1) - WLk))XL-巧
LL
=X(Y (∑rki)+∑rki))wrk+1))(∑'?+∑'k))(w'k+1)-w'k))χ'-∖
'=1 ×r='+1	,
(24)
19
Published as a conference paper at ICLR 2022
Then, we introduce following notations :
d`? = (YY ∑rki)wrk))∑'ki),
×r='+1	,
D'k) = (YY H?+∑rki))wrk+1))⑸？+∑给.
×r='+1	,
Now, We can write ui(k + 1) - ui(k) by noting that ui(k) = √m ∙ VTXLki
ui(k + 1) - ui(k) = √m ∙ vt(xf+I)-XL,i)
=√m ∙ vt X D 'ki)(w'k+1) - w'k)χ'-∖
L
=-η√m ∙ vt X D'ki) VwJLs (W(k))] x'-∖
、-------------------------------------}
(25)
{	-
琰
-η√m ∙ VTXX (d'ki) - d',Vw'[Ls(wM)]x'-∖
、--------------------------------}
z
Here, I(1k,i) can be rewritten as follows:
Ln
IIki) = -η√m •VTX D'?X(Uj (k) -yj)vW' fW(k)(Xj)]χ'-)ι,i
'=1 j=1
nL
-η∙X(uj(k) -yj) ∙ (√mXVTD'ki)vW'fW(k)(Xj)]χ'-)ι,i
j=1	\	'=1
n	1L
-mη ,二(Uj(k) -yj) ∙ m	(vW'fW(k)(Xi)], vW'fW(k)(Xj)]/
n
-mη ∙ E (uj(k) - yj) ∙ Hi,j(k).
j=1
For I(2k,i) , we need a more careful control. First, we pay our attention on bounding the term
kv>(D'? - D'k))∣∣2 as follows: By triangle inequality, We have
—
≤
2
+
(26)
2
We control the first term of the right-hand side (R.H.S) in (26). By the fourth item of the Lemma 4.3,
we know ∣∣∑r？ - ∑r0i)ko ≤ O(mτ2/3L) and |(£、? - ∑roi))jj| ≤ 1 for j ∈ [m]. Then, we can
plug Σ0r0,i = Σ(rk,i) - Σ(r0,i) in the inequality of the fifth item of Lemma 4.3. So, the first term of the
R.H.S in (26) can be bounded by O (τ1/3L2 ,ω log(m)).
The second term of the R.H.S in (26) can be similarly controlled as the first term. Observe that
|区？ + 七；？)“ ≤ 1, then we have |区？+ ∑ ：？ - ∑r0))jj | ≤ 2 for all j ∈ [m]. Notethatbythe
definition of Σe (rk,i), we have kΣe (rk,i)k0 = kΣ(rk,i+1)-Σ(rk,i)k0 ≤ kΣ(rk,i+1)-Σ(r0,i)k0+kΣ(rk,i)-Σ(r0,i)k0 ≤
O(mτ2/3L). Thus, by the triangle inequality, we have ∣∣∑rk) + Σ(Tk - ∑r0i)ko ≤ O(mτ2/3L).
These observations enable us to plug Σ0r0,i = Σ(rk,i) + Σe (rk,i) - Σ(r0,i) in the inequality of the fifth item
of Lemma 4.3, and give the bound on the second term as O (T 1/3L2 ,ω log(m)).
20
Published as a conference paper at ICLR 2022
We have IlVT(DH - d'?)||2 ≤ O(τ1/3L2p∕ωlog(m)). Now, We control the '2-norm of
the ∑2k) as follows:
n
W∣2 ≤X 陶
n
L
vw∕LS(W(”| ∣ 2 ∙	a：i∣∣2
、 V ，
≤O(1) : Lemma 4.2
≤O(L2τ 1∕3√ω log(m))
≤ θ(ηnL2τ1/3,ωmlog(m)) X ∣∣ Vw‘ [Ls (W(k))] |1
/	∖ lʒ
2
F
‹ O
ηnL5/2τ1/3 Pωm log(m))tEI∣Vw'[LS(W(k))]||
'=1
n
L
≤ O (nnL5/2T^3pωmlog(m)) t
X (Uj (k) - y)2 X ∣∣Vw' [∕w(k)(χ7-)] IlF
j=i
2=1
<θ( ηnL3τ^3ωmplog(m) ) ∣∣u(k) — y∣2 .
(27)
Step 2. Control on ∣∣u(k + 1)-u(k)∣2. Recall that by (25), XILTI)-XLCi can be written as follows:
It is worth noting that,
VwJLS(W(k))]| ∣ 2
L
X(k+1)-X强=X D(k)
L , i	L , i / /	', i
2=1
I)- W'k)
i
L
-η ∙χ Wvw'[Ls(wE)]x'3,i.
2=1
n
E(Uj(k) - yj)Vwg [fw(k)(Xj)]
j=1
n
≤ X(Uj(k) - yj)2 XI∣Vwg [fw(k)(Xj)]∣∣2
j=i
< O(nmω) ∣∣u(k) — y∣∣
(28)
Also, observe that | (∑∖? + Σ!?)力 | ≤ 1 for all j ∈ [m], so by Lemma A.3 of Zou et al. [2020], we
know ∣∣D'k)∣2 ≤ O(√L). Combining all the facts, we can conclude:
n2
ι∣u(k+1)- u(k)k2 = m ∙ X k TXLk+I)-VTXLk i)
i=1 '	)
n2
≤ m •同2 X W+I)-XLki||2
i=1
n
≤ η2m ∙
m2X XHDfi)%∙∣∣
i=1
2=1
vw∕LS(W(k))]| ∣ 2 ∙∣∣χ'-)1,i∣∣2
L
2
2
2
n
j
1
2
2 .
< θ(η2n2L2m2ω2) ∣u(k) — y∣2
≤ θ(η2n2L2m2) ∣∣u(k) - y∣2 ,
(29)
21
Published as a conference paper at ICLR 2022
where in the third inequality, we additionally used the fact kvk22 = O(ω) with probability at least
1 - exp(-Ω(m)), and the inequality (28). In the last inequality, We used the assumption ω ≤ 1.
Step 3. λmin (H(k)) ≥ λ∞ with sufficiently large m. Denote P(A) as a SPrectral radius of a
matrix A. Then, we have
kH(k)- H∞k2 ≥ ρ(H(k)- H∞)
≥ -λmin(H(k) - H∞)
≥ λmin(H∞) - λmin (H(k))
≥ λ∞ - λmin(H(k)),	(30)
where, in the second inequality, we used a triangle inequality, λmin H(k) - HL∞ + λmin HL∞ ≤
λmin(H(k)). By Lemmas 4.10 and 4.11, setting m ≥ Ω(ω7n8L18l°λ⅛)"d O(≤
ω ≤ 1, we have
kH(k) - HL∞k2 ≤ kH(k) - H(0)k2 + kH(0) - HL∞k2
≤
θ(ω7"3L3 dl0⅞m)
mδλ2∞
+ O	严!>
。『/6川哈「
≤
λ∞
≤ -
一2
(31)
Thus, combining (30) and (31) yields that λmin(H(k)) ≥ λ∞.
Step4. Concluding the proof. Recall that IIk) = -mη ∙ H(k)(u(k) - y). Then observe
that
(u(k) - y)>IIk) = -ηm ∙ (u(k) - y)>H(k)(u(k) - y)
≤ -ηm ∙ λmin(H(k)) ∣∣u(k) - yk2
≤ -ηm ∙ λ∞ l∣u(k) - y∣2.	(32)
We set the step size η, radius of perturbation region τ and network width m as follows,
η = ω( 2^∞	),
n2L2m
T = o( -n√^ ),
V√mδλ∞√
m ≥ ω"8l18 log3⅞2).
λ8∞δ
With the above settings, we can control the ∣u(k + 1) - y∣22 by combining (27), (29) and (32) as
follows,
I∣u(k + 1)- y∣2 = ∣∣u(k)- y + (U(k +1)- u(k)) ∣∣2
=l∣u(k) - y∣2 - 2ηm ∙ (u(k) - y)>H(k)(u(k) - y)
-(u(k) - y)>I2k) + ∣∣u(k + 1) - u(k)k2
≤ (l — ηmλ∞ + θ(ηnL3τ 1/3m9plog(m)) + O(η2n2L2m2)) ∣u(k) — y∣2
≤ (ι - ηmλ∞)∣u(k)-y∣2.
22
Published as a conference paper at ICLR 2022
So far, we have shown from Step 1 to Step 4 that given the radius of perturbation region τ has the
order O( *
mδλ∞
then we can show the training error drops linearly to 0 with the discount factor
(1 - ηmλ∞ ) along with the proper choices of η and m. It remains Us to prove the iterates w'k)
for all ` ∈ [L] generated by GD algorithm indeed stay in the perturbation region B W(0) , τ over
k ≥ 0 with T = O( n√ω
mδλ∞
Step 5. The order of the radius of perturbation region. We employ the induction process
for the proof. The induction hypothesis is: ∀s ∈ [k + 1],
(33)
First, it is easy to see it holds for s = 0. Now, suppose it holds for s = 0, . . . , k, we consider
s=k+1.
∣W'k+1)- w'k)∣∣2=卜 W'[Ls(w(k)川 ∣2
n
η ∙ E (Uj(k) - yj)Vw' [fw(k)(xj)]
n
X(Uj (k) - yj)2
j=1
≤ η ∙
∖
n
X∣∣Vw' [fw(k)(xj )]∣∣2λ
j=1
(34)
where in the second inequality, we used Lemmas 4.4. Note that since it is assumed that
w'k) ∈ B(W(O),τ), the Lemma is applicable with m ≥ Ω (ω7n8L18 logg(m)). Simi-
larly, since it is assumed that the induction hypothesis holds for s = 0, . . . , k, we can see
∣∣u(k) - yk2 ≤ (1 - ηmλ∞ )kku(0) - yk2. This inequality is plugged in the last inequality with
Lemma 4.9.
By combining the inequalities (33) for s ∈ [k] and (34), and triangle inequality, we conclude the
proof:
+1)
-w'0)∣∣2
Proposition 5.1. For some δ ∈ [0,1], Set the width of the network as m ≥ Ω(ω7n8L18 lθg8 (£)),
and the step-size of gradient descent as η = O Q/Lim). Then, with probability at least 1 一 δ over
the randomness of initialized parameters W(0), we have for k = 0, 1, 2, . . . ,
u(k) - y = (I - ηmH∞)k(u(0) -y) + ξ(k),
where
kξ(k)∣2 =《-ηmλ∞[1o 卜m SSm^! ∣y - u(0)∣2.
Proof. Define ui(k) := fw(k) (xi), then we have
u(k + 1) — u(k) = -ηm ∙ H(k)(u(k) — y) + IIk)
=-ηm ∙ H∞ (u(k) - y) - ηm ∙ (H(k) - H∞)(u(k) - y) + Ilk)
=-ηm ∙ H∞ (u(k) - y) + e(k).
23
Published as a conference paper at ICLR 2022
By recursively applying the above equality, we can easily derive a following for any k ≥ 0,
k-1
u(k) - y = (I- ηmH∞)k(u(0) - y) + X(I- ηmH∞)te(k - 1 - t).	(35)
t=0
X---------------V-
=ξ(k)
}
Now, we want to show ξ(k) can be controlled in arbitrarily small number. First, e(k) needs to be
bounded in an `2 norm:
ke(k)k2 ≤ ηm kHL∞ - H(k)k2 ku(k) - yk2 + I(2k)
≤ ηm ∙O (s7/6n4/3L3 6∣ log⅛m)
mλ2∞δ
ku(k) -yk2,
where, in the second inequality, τ = O
is plugged in (27). Equipped with the bound on
ke(k)k2, we can easily bound the kξ(k)k2 as follows:
k-1
X(I- ηmH∞)te(k - 1 - t)
t=0	2
1 - t)k2
≤
k-1
X kI - ηmHL∞kt2 ke(k -
ηm
ηm
•『/6"4/3L3 6) logλ∞δ)! ku(k -1 - t)- yk2
ML 二!(1-ηmλ∞ Γ-tku(0)-yk2
k(1 - ηmλ∞)k-1o(ηm•3/L! ku(0) -yk2.	(36)
Note that in the third inequality, we used the result from Theorem 1.
□
F Proof of Theorem 3.8
We begin the proof by decomposing the error fw(k)(x) - f * (x) for any fixed X ∈ Unif(Sd-1) into
two terms as follows:
fW(k) (X)- f * (X) = (fW(k) (X)- g* (X)) + (g* (X)- f * (X)) .	(37)
X------V--------} X-----V------}
∆1	∆2
Here, we denote the solution of kernel regression with kernel HL∞ as g* (X), which is a mini-
mum RKHS norm interpolant of the noise-free data set {xi,f?(Xi)}n=ι. To avoid the ConfU-
sion of the notation, we write Ker(X, X) = HL∞(X, x1), . . . , HL∞(X, xn ) in=1 ∈ Rn and let
y * = [f? (xι) ,...,f? (xn)]>. Then, We have a following closed form solution g* (x) as,
g*(X) := Ker(X, X)(H∞)-1y*.
24
Published as a conference paper at ICLR 2022
With the decomposition (37), the proof sketch of Theorem 3.8 is as follows.
1.	Note that for any ` ∈ [L], we have fbW(k) (X) = hvee(vw` [fw(k)(x)]), vec(w'k))i∙ We
can write the term vec(w'k)) with respect to vec(w'0)), H∞ and the residual term via
recursive applications of GD update rule and the result from proposition 2.1. Readers can
refer (38). Using the equality (38), we can further decompose ∆1 into three terms. That
is, ∆ι = ∆11 + ∆12 + ∆13. Then, using the boundedness of '2-norm of network gradient
and the fact that the size of kξ(k)k2 can be controlled with wide enough network, we can
control the size of k∆12 k2 and k∆13k2 aribtarily small.
2.	In the term ∆2, the g? is an interpolant based on noiseless data. For large enough data
points, g? converges fastly to f ? at the rate Op(√1n).
3.	Lastly, the ∆11 is the only term that is involved with random error ε, and we show that
k∆11 k2 is bounded away from 0 for small and large GD iteration index k.
Step 1. Control on ∆ι. For n data points (xι,..., Xn) and for the kth updated parameter W(k),
denote:
vW'[fW(k) (X)] = Vec 卜 W' [fW(k) (XI)]), …，Vec 卜 W' [fW(k) (Xn)]).
Note that when ' = 1, vw` [fw(k)(X)] ∈ Rmd×n and when ' = 2,...,L, vw` [fw(k)(X)] ∈
2
Rm ×n. With this notation, we can rewrite the Gradient Descent update rule as
vec(w'k+1)) = vec(w'k)) — ηVw, [fw(k)(X)](u(k) - y), k ≥ 0.
Applying Proposition 3.8, we can get :
Vec(w'k)) -vec(w'0))
k-1
=X kec(w'j+1)) -Vec(W'j)))
j=0
k-1
-η ∙ X vW' [fw(j)(X)](u(j) - y)
j=0
k-1	k-1
η ∙ X vW' [fW(j) (X)](I - ηmH∞)j (y - U(O)) -η ∙ X vW' [fW(k) (X)]ξj )
j=0	j=0
k-1	k-1
η ∙ X vW' [fW(0) (X)](I - ηmH∞)j (y - U(O)) - η ∙ X vW' [fW(k) (X)]ξj)
j=0	j=0
+ η ∙ X ( vW' [fWj)(X)] - vW' [fW(o)(X)] (I- ηmH∞)j(y - U(O)))
j=0
k-1
=η ∙ X vw` [fW(0)(X)](I - ηmH∞)j (y - u(0)) + ξ0(k).	(38)
j=0
First, We control '2-norm of the first term of ξ0(k) as follows: Note that ∣∣vW' [fW(j)(X)] IIF ≤
O(√nmω) by Lemma 4.4 for 0 ≤ j ≤ k 一 1. Then, we have
k-1
η ∙ X vW' [fW(j)(X)]ξj)
j=0
k-1
≤XO
j=0
2
o(j(i- ηmλ∞
O(ηm 〜“沱3 j⅛∞)! ky -U(O)k2
/ n11∕6L3ω5∕3	,_、
≤θ (mF的% Pogm))…叽.
(39)
25
Published as a conference paper at ICLR 2022
In the second inequality, P∞ι j(1 - ηm^∞ Y = O()is used. Then, We control '2-norm
of the second term of ξ (k) as follows:
k-1 ∣-	-∣
η ∙ X Vwg [fw(j)(X)] - VW' [∕w(0)(x)] (I - ηmH∞)" (y - u(0))
j=o L	-
2
k-1
≤ Xn ∣∣i-ηmH∞∣∣2 Ily - U(O)Il2
j=0	∖
n
X ∣Wwg [fw(j)(Xi)] - Vwg [fw(0)(Xi)] ∣∣2
i=1
k-1	. /n1∕3m1∕3r2()2/3 ___、
≤ ∑^n(1 - nmλ∞Yo( —W而—PIOg(m))O(√n) ∣∣y - U(O)Il2
/ n5/6/232/3	∖
≤θ (m2∕3λfδ1∕6 √⅛) |y - u(0)12，	(4O)
where in the second inequality, we used Lemmas 4.6 with T
Now, we are ready to control ∆1 term. By using the equality (38), we can decompose the term ∆1
as follows: Let us denote Gk = Pk-II nm(l - ηmH∞)j. Note that for any ' ∈ [L], fw(k) (x)=
(vec"wg [fw(k)(x)]), VeC(W'k))〉and recall that y = y* + ε. Then, for any fixed ' ∈ [L], we
have:
∆1 = (vec(Vw/ [fw(k)(x)]), vec(w'k)))- Ker(x, X)(H∞)-1y*
+ Ker(x, X)Gky - Ker(x, X)Gky
=Ker(x, X) [Gk - (H∞)-1]y* + Ker(x, X)Gkε
'----------------------V---------------------}
= ∆11
1 X—Z-	-	, 「,>_	L _	__r	, 一 一
+ m ^XVeC(VW` [fw(k)(X)]) vwg [fw(0)(X)]- Ker(X'X) Gky
-m '=1	-
-m X VeC(VW`[fw(k)(x)])>vwg[∕w(0)(x)]Gky
':'='
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^™
= ∆12
+ (vec(Vw/ [fw(k)(x)]), VeC(W'0)))+ VeC(VW`, [fw(k)(x)])>ξ'(k)
-mVeC(VW`o [fw(k)(x)])>VW'0 [fw(0)(χ)]GkU(O).
..------------------------------V--------------------------------}
= ∆13
(41)
Our goal in this step is to control I∆12 I2 and I∆13I2. Then, in the third step, we will show I∆11 I2
is the term, which governs the behavior of the prediction risk with respect to algorithm iteration k.
26
Published as a conference paper at ICLR 2022
First, We bound the '2 norm of the first term in ∆12 as:
L
m X vee(vw` [∕w(k)(x)])TVw' [∕w(0)(x)] - Ker(x, X) Gky
'=1
2
1L * * *
≤ —亍 E IlVeC(VW` [fW(k)(X)]) -VeC(Vw` [fW(0)(X)]) ∣∣2 ||VW' [fW(0)(X)] ∣∣F IlGkyk
'=1 '---------1---------{--------------------'、--1----Y--------/
Lemma 4.4
2
≤O (T 1∕3L2 √ωm log(m)) : Lemma 4.6
>z-
≤O(√ωnm):
1
+L ∖
n	/ 1	L	、2
X ( m X〈Vwg [fw(0)(X)], Vwg [fw(0) (Xi)DTr- Ker(X, Xi) )	IlGk y∣∣2
i=1 'm '=1	)
≤{o( m5⅛⅛ pl0g^)+oS/2L3/2 qRL^)} IGk ∣2 ky∣2
≤o( I …∙ky∣2)+o(「「∙kyk2)
(42)
where, in the second inequality, we plugged T = O( √n^ ) in the result of Lemma 4.6 and used
V mδλ∞
Lemma 4.11. In the last inequality, we used IlGkk2 ≤ O (J). Similarly, we can control the '2
norm of the second term in ∆12 as follows:
—X VeC(VW` [fw(k)(X)]) Vwg [fw(0)(X)] Gky
m J,	'	-	` , 一
`:'='0
2
≤ - X IlVeC(VW'[fw(k)(x)])k2 ∙∣∣Vwc [fw(0)(X)] kF ∙ kGk|卜同
m	、.—	、.一	k.y}
W	≤θ(√ωm)	-/ 1 ,
≤O (√ωmn)
≤O
λ∞
≤ O
■ kyk2.
(43)
We turn our attention to controlling ∣∣∆13∣∣2.
The first
∣∣vec(Vw^o [∕w(k)(x)])∣∣2	≤ O(√mω) by Lemma 4.4.
VeC(VW` [fw(k)(x)])>vec(w'0)) is simply a N(0, O(ω)) for 1
term in ∆13; Recall that
Then, the random variable
≤ ' ≤ L. A straightforward
application of Chernoff bound for normal random variable and taking union bound over the layer
1 ≤ ' ≤ L yield that: with probability at least 1 一 δ,
2
1
卜ec(Vw∕ [fw(k)(x)])>vec(w'0))∣ ≤ θ( jωlog (IL)).
(44)
The '2 norm of the third term in ∆13 can be bounded as follows:
WVeC(VW`o [fw(k)(X)])'vw∕ [fw(0)(X)]GkU(O)
2
≤ m M
(Vw∕ [fw(k)(x)])∣L ∣∣Vw∕ [fw(0)(X)] LkGkU(0)∣∣2 ≤ O
≤O (√mω)
≤O (√ωmn)
Z----{z—
≤o( √nω
∖ λ∞>δ
nω3δ2).	(45)
λ∞δ j
/k
27
Published as a conference paper at ICLR 2022
In the last inequality, we used the Lemma 4.8 and ∣∣Gk∣∣2 ≤ O(六).	By combin-
ing (39), (40), (44), (45) with ∣∣Vw^ 0 [fw(o)(x)] L ≤ O(√mω), we have a following :
IIδ13 l∣2 ≤ ∣∣DVeC(VW`o [fW(k)(x)]), VeC(W'0))>∣∣2 + ∣∣(VeC(VW'0 [fW(0)(χ)]))>ξ0(k)∣∣2
+
m (vec(vW'0 [fw(o)(χ)]) τ^W'0 [fw(o)(χ)]Gk u(0)∣∣
+O
+O
1/6X^S1/6
,ωω/2! !
λ∞δ ).
n5/6L2s7/6 ∣∣y — u(0)∣∣2
n11/6L3ω13/6 ∣∣y — u(0)∣2
n11/6L3"13/6 ∣∣y — u(0)∣2
m^6λ∞3δν6
m1/6xM3S1/6
+O
+O nω/2
λ∞δ
n5/6 L2 ω 7/6 ∣y — u(0)∣2
m1/6xM3S1/6
(46)
Step 2. Control on ∆2. First, note that there is a recent finding that the reproducing kernel Hilbert
spaces of NTKs with any number of layers (i.e., L ≥ 1) have the same set of functions, if kernels
are defined on Sd-1. See Chen & Xu [2020]. Along with this result, we can apply the proof used in
Lemma.D.2. in Hu et al. [2021] for proving a following :
∣∆2∣2 = OP
(47)
Step 3. The behavior of L2 risk is characterized by the term △“. Recall the decompositions (37)
and (41), then We have:
^ ，、…，、 ， / , , , 、 ， „
fw(k) (x) — f (x) = △11 + △12 + △13 + △2 := △11 + Θ.
(48)
Our goal in this step is mainly two-folded: (i) Control Eε ∣Θ∣22 arbitrarily small with proper choices
of step-size of GD η and width of the network m. (ii) Show that how Eε ∣∆11 ∣22 affect the behavior
of prediction risk over the GD iterations k. First, note that we have
Eε ky∣2 = Eε ky* + ε∣2 ≤ 2(y*)>y* + 2E, ∣ε∣2 = O(n).
(49)
Second, recall Lemma 4.9 and note that over the random initialization, with probability at least 1 -δ,
Eεky — u(0)k2 ≤O(n
Now, by combining the bounds (42), (46) and (47), we have
Eε ∣Θ∣2 ≤ 3Eε( ∣∆12∣2 + ∣∆13∣2 + ∣^2∣2 )
(50)
口	/ n5∕3L4ω7∕3	1	. . ll ll9∖ /ω2nL3 /log(nL∕δ) ll ll9ʌ
≤ Eε	[O m^∞^	log(m)	∙ ky k2)	+ O S ；	∙kyk2)
+ O (n11/3L6ω13∕3 ky - u(0)k2 log(m)! + O /n5/3L%7/3 ky — u(0)k2
m1/3xg3S1/3
log(m)
+O
O
m
≤O
ω2n2L3 rlog(nL∕δ)
^^∞V^
+O
+ O (-
n
m
^∕n14∕3L6ω13∕3∣ ( ∖ 八	n8∕3L4ω7∕3 1 ,、
+ O ------8/3---log(m) + O --------14/3--log(m)
1m1∕3λ∞ δ4/3	)	1m1∕3λ∞/ δ4/3
(51)
28
Published as a conference paper at ICLR 2022
where in the third inequality, we used (49) and (50).
CaSe 1. When k is large, the L2 risk is bounded away from zero by some constant.
Now we control Eε ∣∣∆111∣2. Recall the definitions kf∣∣2 := fx∈sd-ι ∖f (x)∣2dx and
Gk = Pk-CJ ηm(l - ηmH∞)". Let us denote S = y*y*τ. Then, we have
Ee ∣∆ιι∣2 = /	Ker(x, X)
Jx∈Sd-1
-(H∞)-1)y*y*T(Gk-(H∞)-1) + Gk Ker(X,x)dx
I	Ker(x, X)(H∞)
Jx∈Sd-1
IMk(H∞)-1Ker(X,x)dx
where
Mk = (I - ηmH∞)kS(I - ηmH∞)k + (I - (I - ηmH∞)k)2
=[(I - ηmH∞)k - (S +1)-1](S +1)[(I - ηmH∞)k - (S +1)-1] +1- (S +1)-1.
For the algorithm iterations k ≥ (黑n^ )Co with some constant Co > 1, we have
(Z — ηmH∞)k W (1 — ηmλ∞)k ∙ I W exp(-ηmλ∞k) ∙ I W exp(-Co log(n)) =	∙ Z.
Since 1 + ∣∣y∣2 ≤ gn for some constant Ci, we have
λmax(ɪ ∙ (S + I)) = 1 + Cyk2 ≤ -ɪ < 1.	(52)
∖nc°	) nC0 nC0-i
Using the first item of Lemma (4.12) with the inequality (52), we have
(I- ηmH∞)k W ɪ ∙Iγ (S +1 )-i.	(53)
nC0
The above inequality (53) lead to a following result :
(s+i)-i - (i-ηmH∞)k 占(s + i)-i - nCo ∙i.	(54)
It is obvious that both (S +1)	- (I - ηmH∞) and (S +1)	- nCy ∙I are positive definite
matrices due to (54), and it is also easy to see that they are exchangeable. By using the second item
of Lemma (4.12), we have
Mk = [(I - ηmH∞)k - (S +1 )-1](S +1 )[(I - ηmH∞)k - (S +1 )-i] +1- (S +1 )-i
上[(s + i)-i - nCy ∙i](s + i)[(s + i)-i - nCy ∙i] + I-(S+i)-i
=n2C0 S + (1 nc0 ) ^ Z.
Then, we have
n⅛0A + (1- nCo) BZ coB,
Ee ∣∆11∣2 上
where c0 ∈ (0,1) is a constant and
A = [	[Ker(x, X)(H∞)-1y
x∈Sd-1
,*]2dx,	and B = /	[Ker(x, X)(H∞)-1]2dx.
x∈Sd-1
(55)
By triangle inequality with the decomposition (48) and the bound on Ee ∣∣Θ∣2 in (51), we have:
EjfW(k) - f*∣∣2 = Ee ∣∆11 +Θ∣2
1
≥ 2 Ee ∣∆11∣2- Ee ∣Θ∣2
≥coB-O(I)- O (⅞∞⅛)- O (焉poly Qn,L, λ∞, δ)).(56)
29
Published as a conference paper at ICLR 2022
For the third term in (56), We can choose ω ≤ C2 (λ∞∞δ )2/3 for some constant C2 > 0 such that
the term can be bounded by c0 ∣∣Ker(∙, X) (H∞) 1∣∣ . Similarly, the width m can be chosen large
enough such that the fourth term in (56) is upper-bounded by C∣∣Ker(∙, X)(H∞) 1k2. Using the
above choices of k, ω, and m, we can further bound (56):
Eε ∣∣fw(k) — f*∣∣2 ≥ c40 ∣∣Ker(∙,X)(H∞)-1∣∣2-O(1).	(57)
Note that Eεkf∞ — g*∣∣2 = ∣∣Ker(∙,X)(H∞) 1∣2 where g* := 0 and f∞ denotes the noise
interpolator. Then, by Theorem 4.2. of Hu et al. [2021], we know that Eεkf∞ - g* ∣∣2 ≥ ci for some
constant ci > 0. Then, we can take n large enough such that the term O(ɪ) is upper-bounded by
c08C1, and finish the proof.
Case 2 When k is small, the L2 risk is bounded away from zero by some constant.
Recall the definition of ∆ii in the decomposition (41),
∆11 := Ker(x, X)Gk [y* + ε] - Ker(x, X)H∞y*
:=△"- Ker(χ, X)H∞y*.	(58)
We denote the eigen-decomposition of the matrix HL∞ := Pin=i λivivi>, then we can easily see a
following:
k-i
Gk = ηm
j=0
k-i n
ηm	vivi>
j=0 i=i
Y ηmk ∙ I.
By using the above inequality, we have
Eε ∣∣∆^ιk2 = /	Ker(x,X)Gk(S +1)GkKer(X,x)dx
x∈S d-1
≤ η2m2 k2 ( /	[Ker(x, X)y*]2dx + ∣∣Ker(∙, X)∣2) = O (η2m2k2ω2n2I2).
Recall the decompositions (37) and (41), then we have:
Eε
-f *∣∣2
Eε ∣∆" + Θ - Ker(∙, X)H∞y*∣∣2
2 ∣Ker(∙,X)H∞y*k2 — Eε ∣∆" + Θ∣2
1 ∣Ker(∙,X)H∞y*k2 - 2Eε ∣∆"k2 — 2E, ∣Θ∣2
2 ∣Ker(∙, X)H∞y*k2 - O (η12m2}22ωj2n2I22} — O
-O
-O (m4/3poly (ω,n,L
(59)
For some constant Ci0 > 0, let k ≤ Ci (ηmnωL) such that the second term in the bound (59)
can be bounded by 1 ∣∣Ker(∙,X)(H∞) 1y*∣∣2. For the fourth term in (59), we can choose
ω ≤ C2(λ∞δ)2/3 for some constant C2 > 0 such that the term can be bounded by
1 ∣∣Ker(∙, X)(H∞)-1y*∣∣ . Similarly, the width m can be chosen large enough such that the fifth
term in (59) is upper-bounded by 1 ∣∣Ker(∙, X)(H∞) 1y*∣∣2. Using the above choices of k, ω, and
m, we can further bound (59):
Eε ∣∣fw(k) - f*∣∣2 ≥ 4 ∣∣Ker(∙,X)(H∞)-1y*∣∣2 - O
≥ C30 ∣∣fρ? ∣∣22 - O
(60)
≥
≥
≥
1
n
In the second inequality, we used (47) with triangle inequality. In the third inequality, we can
i	C0 ∣	∣2
take n large enough such that the term O (ɪ) is upper-bounded by -3- ∣∣fj∣∣2. Lastly, by using the
assumption that fρ? is a square-integrable function, we finish the proof.
30
Published as a conference paper at ICLR 2022
G Proof of Theorem 3. 1 0-Training error
For the convenience of notation, we denote uD,i (k)
fW(k) (xi) and let uD(k)
[ui,d(k),...,Un,D(k)]>. In order to analyze the training error of '2-regularized estimator,
kuD(k) - yk22, we decompose the term as follows:
I∣ud(k + 1) - y∣2 = l∣UD(k +1) - (1 - η2μL)uD(k)k2 + k(1 - η2μL)uD(k) - y∣2
-2(y - (1 - η2μL)UD(k)) (UD(k + 1) - (1 - η2μL)UD(k))
(61)
Equipped with this decomposition, the proof consists of the following steps:
1.
2.
3.
We decompose the decayed prediction difference UD (k + 1) - (1 - ημL)uD (k) into two
terms. We note that the first term is related with a gram matrix HD(k) and denote a second
term as I(Dk) .
The term I(Dk) can be further decomposed into three terms, where we denote them as I(2k,D) ,
I3kD and I5kD. The crux for controlling the '2 -norm of the above three terms is to utilize
the results from the Appendix A.4. The applications of Lemmas in the Appendix A.4 is
possible, since we can inductively guarantee that k WDk' - WD0'k2 is sufficiently small
enough for large enough m.
Given the decomposition (61), we further decompose it into four terms as follows:
(61) = k(1 - η2μL)uD(k) - yk2 + ∣∣uD(k + 1)-(1 - η2μL)uD(k)∣∣2
I
} I
}
™{^™
:=T1
™{^™
:=T2
+ 2mηι(y -(1 - η2μL)uD(k))>H0(k)(uD(k) - y)
|
{Z
:=T3
-2(y -(I- η2μL)UD(k)) ID).
}
(62)
|
{z^^^
:=T4
4.
5.
}
In this step, we obtain the upper-bound of kTik2 for i = 1, 2, 3, 4 obtained in Step 4.
We combine the upper-bounds of kTik2 for i = 1, 2, 3, 4 in step 3 and obtain the bound on
kuD(k + 1) - yk22 with respect to kuD(k) - yk22 and kyk2.
Lastly, we inductively show that the weights generated from regularized gradient descent
stay within a perturbation region B W(0), τ , irrespective with the number of iterations of
algorithm.
We start the proof by analyzing the term u(k + 1) - (1 - η2μL)u(k).
Step 1. Dynamics of UD(k + l)-(l-η2μL)∙UD(k). Recall HDkMi)jj = MHDMj, XD)'-ι,ii ≥
0)and we introduce a diagonal matrix ΣD)` v whose jth entry is defined as follows:
hw(k+1) X(k+1) i
(∑(k) ∖	_ ∕∑(k+1) _ ∑(k) ∖	___________hwD,',j，xD,'-1,ii
VDD,i,i)jj = ∖ςd,'C	LD£i〃j	/	(k+1) y(k+1) ∖ Mk)	y(k)	∖
hwD,',j，xD,'-1,ii - hwD,',j，xD,'-1,ii
With this notation, the difference x(Dk,+L1,i) - x(Dk,)L,i can be rewritten via the recursive applications of
ΣDk)' i： Then, We introduce following notations:
DDkei= ( YY ∑D‰W(kr)∑D‰,	D Dk'i= ( YY (∑Dri + ∑ Dk)ri)W(k + 1)(∑D‰ + ∑ ^i)
,,	,r, ,r ,,	,,	,r,	,r,	,r	,,	,,
'r='+1	)	'r='+1	)
31
Published as a conference paper at ICLR 2022
Now, we can write uD,i(k + 1) - uD,i(k) by noting that uD,i(k) = √m ∙ VTXD)L i:
UD,i(k + 1) — UD,i (k)
—/—	T( (k+D	⑹ ∖
=vm ∙ v ∖xD,L,i - xD,L,i)
=√m ∙ vt X D Dli(WD+1)- wD⅛) XDL,i
'=1	×	)
√m ∙ vt X DDii ( - ηιVW' [Ls (W削]-η2μwD⅛ + η2μwD1力 XDL,i
'=1	'	'
L
-ηι√m ∙ VT X D队VWjLS(WQXD‰,i
-ηι√m ∙ VTX (d Dli- DDli)VWjLS(WQ XDL,i
-wD'	XDL,i
,
L
-η2μ√m ∙ vT X DDZiWD
2=1
|
,
"^^^^^^^^^"^^^^^^^^^^{^^^^^^^^^^^^^^^^^^^^""^
T(k)
T4,D,i
L
+η2μ√m ∙ vT X DDZiWD)WL"
2=1
(63)
I
,
τ(k)
I5,D,i
where in the second equality, we used the recursive relation (24), and in the third equality, modified
GD update rule (6) is applied.
Furthermore, IsD i Can be rewritten as follows:
L	n
ISD,i = -ηι√m ∙VT X DDZiX (UDj (k) - y3)vW' %MXj)RD3i
'=1	j = 1
n	L L	、
=-ηι £ (uD,j(k) - yj) ∙ (√mE VTDDZiVWJfWMXj)]XDL,i)
j=1	∖	'=1	'
n	1 L /	∖
=-mηi ∙	(uD,j (k) -yj) ∙ m∑S (vWg fW(k) (Xi)] , vW'[fW(k)(Xj )]/
n
=-mηι ∙E(UD,j(k) -yj) ∙ HD,i,j (k).
j = 1
WithI4? = (-η2μL) ∙ uD,i(k) and (64), we can rewrite (63) as follows:
(64)
n
uD,i(k + I)-(I- η2μL)UD,i(k) = -mηi ∙ X (uD,j (k) - yj) ∙HD,i,j (k) + 12kD ,i + 13kD ,i + 15kD ,i∙
j = 1
(65)
32
Published as a conference paper at ICLR 2022
Step 2. Control of the size
2
Let I(D) = [I(2,D) ,1 + I(3,D) ,1 + I(5,D) ,1, . . . , I(2,D) ,n + I(3,D) ,n + I(5,D) ,1]>. Now, we control the
bound on the I(Dk) . Recall that in Eq. (27), we have
2 ≤ O (η1nL3τ 1l3ωmSog(m) ) ∣∣ud(k) - y∣b .
(66)
Similarly,
n
can be bounded:
2
n
L
{^^^^^^^^^^^
≤θ(L2τ 1∕3√ω log(m))
-wM ∙ IlXDLJ
-------} 1---V---}
≤τ	≤O(1)
(67)
Lastly I(5k,D) 2 can be bounded:
n
L
n
L
≤ X η2μ√m ∙ vT X DD)',iWD)eχDL,i
i=1
'=1
≤ η2μL ∙	lui,D(k)1 + η2μ√m •£ E
i=1
i=1
'=1
≤ O ( η2μnLp log(L∕δ) ) + O ( η2μnL3∕2τ
i=1
'=1
X Iη2μ√m ∙ VTX D 队
-W(0)' xD,'-ι,i
-WD)'II2
-- /
^z
≤τ
llxD)`-ι,iL
'----{z-----}
≤O(1)
(68)
+
n
n
L
性∙ IiDDy 2
≤O(√ω)	F√lT |
where in the last inequality, we employed the same logic used in (44) with the Lemma 4.2 to obtain
the upper-bound on the U,D(k)|. We set the orders of the parameters μ, ηι,小,T, and ω as follows:
1
Ln
μ = Θ (n2d-1 ) ,	η1 = θ( ɪn
T = o(√√ωn
mδ
(69)
Plugging the choices of parameters (69) with sufficiently large m in (66), (67) and (68) yields
≤O
2
12 n- 12d-86 ^mɪ )∙∣UD (k)-y∣2 + Op( 3
m1∕6δ1∕3 )	n n2
(70)
Step 3. Upper-bound of IlTik 2 on i = 1, 2,3,4.
First, we work on getting the upper-bound on λmax HD(k) . By the Gershgorin’s circle the-
orem [Varga, 2004], we know the maximum eigenvalue of symmetric positive semi-definite matrix
is upper-bounded by the maximum absolute column sum of the matrix. Using this fact, we can
33
Published as a conference paper at ICLR 2022
bound the λmax HD (k) as :
n
λmaχ(HD (k)) ≤ maxn	|HD,i,j(k)|
,..., j=1
n
≤
max
i=1,...,n
Σ
j=1
'=1
Tr
n 1 L
≤ i=m,axnX m X 卜 w' fWDk)(Xi)] IUlVW` [fwMχj)] IIF
j = 1	'=1'-------{z--------'、-------{--------}
≤O(√mω)	≤O(√mω)
≤ θ(nLω).
(71)
Recall the decomposition (62). Our goal is to obtain the upper-bound on Ti for i = 1, 2, 3, 4.
Control on Ti. By using the inequality 2η2μL(1- η2μL)y>(y - UD (k)) ≤ n2μL ∣∣y∣∣2 + η2μL(l -
η2μL)2 ∣∣y - UD(k)k2, we have
ky - (1 - η2μL)UD(k)k2 = 1(1 - η2μL)(y - UD(k)) + η2μLy∣l2
= (1 - η2μL)2 ∣y - UD(k)∣22 +η22μ2L2 ∣y∣22
+ 2η2μL(1 - η2μL)y> y - UD (k)
≤ (η2μL + η2μ2L2) kyk2+ (1 + η2μL)(1 - η2μL)2 ky - UD(k)∣∣2 ∙
(72)
Control on T2. Recall the equality (65). Then, through applications of the Young’s inequality
ka + bk22 ≤ 2 kak22 + 2 kbk22 for a, b ∈ Rn, we have
kUD (k + 1)-(1 - η2μL)uD (k)∣∣2 = ∣∣-mηι ∙Hd (k)(uD (k) - y) + IDk)(
≤ 2m2η2λmaχ(HD(k))2 ky - u0(k)∣∣2 + 2 ∣∣瑁)||： . (73)
Similarly with T1 and T2, we can control T3 and T4 as follows:
Control on T3.	Recall HD (k) is a Gram matrix by definition. Then, by using the fact
λmin HD (k) ≥ 0 and Cauchy-Schwarz inequality, we have
2mηι(y - (1 - η2μL)uD(k))>H0(k)(uD(k) - y)
=-2mηι(1 - η2μL)(y - UD(k))>H0(k)(y - UD(k)) + (2mη1η2μL) ∙ y>H0(k)(uD(k) - y)
≤ (2mη1η2μL) ∙ λmaχ(HD(k)) ky - UD(k)k2 + (2mηιη2μL ∙ (λmaχ(HD(k)) ∣∣yk2 Ily - UD(k)k2 )
-2mηιλmi∩(HD(k)) Ily — UD(k)k2
=(4mηιη2μL) ∙ λmaχ(HD(k)) Ily - UD(k)k2 + (4mηιη2μL) ∙ λmaχ(HD(k)) kyk2 . (74)
Control on T4. By a simple Cauchy-Schwarz and Young’s inequality, we have
-2(y -(I- η2μL)uD(k)) ID)
=-2(1 - η2μL)(y - UD(k))>IDk) + 2η2μL ∙ y>ID(k)
≤ 2(1- η2μL) ky - UD(k)∣∣2 ∣∣ιDk)( + η2μL kyk2 + η2μL ∣∣IDk)∣∣2	(75)
34
Published as a conference paper at ICLR 2022
Step 4. Upper-bound of the decomposition on training error (62).
Before getting the upper bound of the decomposition (62), we first work on obtaining the
bound of (76). Set K = O(n⅛) and notice η2μL = O(1) by (69), then We have
2 忡］∣2 + 2(1- η2μL) ky -UD(k)k2∣lDk)∣2 + η2μL 忡)∣∣2	(76)
=(2 + η2μL^ ∣∣IDk)∣∣2 + 2κ(1 - η2μL) l∣y -UD(k)k2 ∙ K ∣∣IDk)∣∣2
≤ (2 + η2μL + 薜)∣∣IDk) ∣L + κ2 (1 - η2μL)2 ky - uD(k)k2
=κ12 ∙ ∣∣IDk)∣∣2 + κ2(1 - η2μL)2 ky — UD(k)k2
≤ (K12 ∙O(L37∕6n-9d-8mgm3) + κ2(1 - η2μL)2) ∙ ky - UD(k)k2 + K12 ∙OP(n14)
≤ (η2μL)4(1 - η2μL)2 ∙ ky — UD(k)k2 + 也μL ∙ kyk2,	(77)
where in the second inequality, the Eq. (70) is used with (a + b)2 ≤ 2a2 + 2b2 for a, b ∈ R, and
in the last inequality, We used kyk22 = O(n) and the sufficiently large mto control the order of the
coefficient terms of ky 一 UD(k)k2∙ Specifically, we choose m ≥ Ω(L19n20logδ(m)).
Now, by combining the inequalities (72), (73), (74), (75), (71) and (77), we obtain the upper-bound
on the decomposition (62);
kUD(k+1)-yk22
≤ (2η2μL + η2μ2L2 + 4mηιη2μL . λmax(H0(k))) Tlyk2
+ ((1 + η2μL)(l ― η2μL)2 + 2m2η2λmax(HD(k))2 + 4mηιη2μL • λmax(H0(k)))
• ky - uD(k)k2 + (2 ∣∣IDk)∣∣2 + 2(1 - η2μL) ky - uD(k)k2 ∣∣1D)∣∣2 + η2μL ∣∣ID)∣∣2 )
≤ {3η2μL + η2μ2L2 + O^ωmnη1η2μL2) j ∙ kyk2
+ {(1 + η2μL + η4μ4L4)(l — η2μL) + O
• ky -UD (k)k2
：=A∙kyk2 + (1 -B) ∙ky - UD (k)k2.
ω2m2n2η2L2) + O (ωmnηιη2μL2) }
(78)
With the order choices of μ, ηι and η2 as in (69), it is easy to see the leading terms of both A and
B are same as η2μL = o(n). Then, by recursively applying the inequality (78), we can get the
upper-bound on the training error.
ky - UD(k + 1)k2 ≤A∙kyk2 + (1 - B) ∙ky - UD(k)k2
≤ Akyk22 • Xk (1-B)j)+(1-B)k+1 • ky - UD(0)k22
≤ A∙kyk2 + (1 -B)k+1 ∙ky-Ud(0)k2
≤O(n) + (1 - η2μL)k+1 ∙ ky - Ud(0)k2.	(79)
In the last inequality, we used A = o(1), B ≥ η2μL and kyk2 = O(n).
35
Published as a conference paper at ICLR 2022
Step 5. The order of the radius of perturbation region. It remains Us to prove the radius of
perturbation region T has the order OP (L√√ωn 2d-1). First, recall that the '2-regularized GD
update rule is as:
W噌' =(1- η2μ)wDk-1) 2- ηιVw∕Ls(WDkT))] + η2μwD', ∀1 ≤ ` ≤ L and ∀k ≥ L
(80)
Similarly with the proof in the Theorem 3.5, we employ the induction process for the proof. The
induction hypothesis is
M⅛-W^2 ≤o(
∀s ∈ [k + 1].	(81)
It is easy to see it holds for s = 0, and suppose it holds for s = 0, 1, . . . , k, we consider k+ 1. Using
the update rule (80), we have
W(K I)- W(SII2 ≤
η2μ IIwDS -W 乳 ∣∣2+m ∣∣ vW' [LS(WDk))] ∣∣2
≤
O
≤
O
η1n	mω
O
≤
√δ
ηin√mω
+O
+O
η2μ∣∣W黑-W乳
vW' [fWD(k)(Xi)]
τ、k 八	n
-η2μL)2 O( δ δ
2
In the first inequality, we use the induction hypothesis for s = k, and Lemma 4.4. In the second
inequality, since the induction hypothesis holds for s = 0, 1, . . . , k, we employ ky - uD (k)k2 ≤
θ(√n) + (1 — η2μL) 2 ∣∣y 一 UD(0)∣∣2 with the Lemma4.9. In the last inequality, We use ημ < 1.
By triangle inequality, the induction holds for s = k + 1. Plugging the proper choices ofη1, η2 and
μ as suggested in (69) to O(η√gω) yields IW^ - W(⅛∣2 ≤ OP (Lmn击)∙
H Proof of Theorem 3. 1 0-Kernel ridge regressor approximation
We present a following proof sketch on the approximation of regularized DNN estimator to kernel
ridge regressor.
1. The key idea for proving the second result in Theorem 3.8 is to write the distance between
ui,D (k) (where D is to denote the prediction is obtained from regularized GD rule) and
kernel regressor B := H∞(Cμ ∙ I + H∞)-1y in terms of NTK matrix H∞, which is as
follows:
UD(k) - B = ((1 - η2μL) ∙ I - mηιH∞) (UD(0) - B) + e∏(k).
Above equality describes how the regularized estimator evolves to fit the kernel regressor
as iteration of algorithm goes by.
2. We can bound the '2-norm of residual term e°(k) as O(1∕n), and show that the '2 norm
of the first term on the RHS of equation (4.3) decays at the rate θ(√n(1 一 η2μL)k). Here
the √n comes from the bound ∣∣B∣∣2 ≤ Ο(√n), since we know ∣∣u(0)∣2 has O(√nω) with
small ω ≤ 1. This yields the claim.
36
Published as a conference paper at ICLR 2022
Recall the equality (65). Then, we have
UD(k +1) - (1 - η2μL)u0(k)
=-mηι ∙ HD (k)(uD(k) - y) + I2kD + I3kD + I5kD
=-mηι ∙ H∞(uD (k) - y)
-mηι ∙ (HD(k) - H∞)(ud (k) - y) + EkD + UkD + KkD
=-mηι∙ H∞(ud(k) - y) + ξD(k).	(82)
With τ = O√mωn n 2d-1) , similarly with Lemma 4.10 and a direct employment of the result from
Lemma 4.11, we can control the distance from HD(k) to HL∞ under operator norm as follows:
kHD(k)-HL∞k2 ≤ kHD(k) - H(0)k2 + kH(0) - HL∞k2
≤O s7/6L10/3
≤ O L19/12n
≤ O L19/12n
log3(m)
mδ2
log3(m) ) + O QMndlog(nL0
mδ2	m
嗓3(叫+O L Ln-nd-6 y⅛Lzu
mδ2	m
(83)
,	1	5d-2.
where in the third inequality, ω = O (^3/2 n-2d-1) is plugged-in. The last inequality holds with
d ≥ 2 with large enough n and the condition on width m ≥ Ω (L1
of ξD (k) can be bounded as:
9汽20 log3(m)
^δ2
. Then, the `2 norm
kξD(k)k2 ≤ mηι ∙ kH∞ - HD(k)k2 kuD(k) - yk2 + IbDoIL
≤O L19/12n--匕
• IIUD(k) - yk2 +OP
、-----V------}
≤o(√n∕δ)
≤O
L19∕12n-粤-6 Mog(M)
m1∕6δ5/6
+ OP
(84)
where in the second inequality, we used (83) with η1
to control the first term and
employed Eq. (70) to control the second term. In the last equality, we used m ≥ Ω(L19n201ogδ2m^).
Now, by setting B := ^mI + Η∞) H L∞y, we can easily convert the equality (82) as follows:
for k ≥ 1,
UD(k) - B = ((1 - η2μL) - I - mηιΗ∞) (up(k - 1) - B) + ξp(k - 1).	(85)
The recursive applications of the equality (85) yields
k
UD(k) - B =1 1 - η2μL) ∙I- mηιH∞) (UD(0)- B)
+ X ((1 - η2μL) ∙I- mηιH∞) ξD(k - j - 1)
j=0
k
=((1 - η2μL) ∙I- mηιH∞) (uD(0) - B) + eD(k).	(86)
37
Published as a conference paper at ICLR 2022
Now, we bound the `2 norm of eD (k) in (86):
keD(k)k2
kj
E((I- ημL) ∙I- mηιH∞) ξD(k - j - I)
j=0
k
≤ XMI- n2〃L)∙ I - mηιH∞∣ j kξD(k - j - I)k2
j=0
k
≤ X (1 - η2μL)j kξD(k - j - I) k2
(87)
in the last inequality, We used η2μL = O 信)and Eq.(84). Now, We control the '2-norm of the first
term in (86) as:
k
(1 - η2μL) ∙I-mηιH∞) (ud(O) - B)
≤ (1 -小小刀)k IIuD(O)- Bk2
2
≤ O(√n(1 -η2μL)k),
(88)
where in the second inequality, we used ||uD(0)∣2 ≤ O(√nω∕δ) and the fact that
• kyk2 ≤ O(√n).
2
2
By combining (87) and (88) and using a fact (1 - η2μL)k ≤ exp(-ημLk), we conclude that after
k ≥ Ω((η2μL)-1 log(n3/2)), the error ∣∣uD(k) - B∣∣2 decays at the rate O(n).
I Proof of Theorem 3.11
We begin the proof by decomposing the error fW(k)(x) - f * (x) for any fixed X ∈ Unif(Sd-1) into
two terms as follows:
fw(k) (X)- f (X)= (fw(k) (X)- gμ (X)) + (gμ(X)- f (X)) .	(89)
D	'____D {z__________} S--------{-------'
Δd,i	δd,2
Here, we devise a solution of kernel ridge regression gμ (x) in the decomposition (89):
g*(x) := Ker(X,X)(Cμ ∙I + H∞)-1y,
for some constant C > O. Specifically, in the proof to follow, we choose η1 and η2 such that
C = ηη2mL for the theoretical convenience. Our goal is to show that all the terms ∣∣Δd,i∣∣2, and
∣∣Δd,2∣∣2 have the order either equal to or smaller than O(n-2d-1) with the proper choices on
m, μ, ηι and η2. Since the high-level proof idea is similar with that of Theorem 3.8, we omit
the step-by-step proof sketch of Theorem 3.11. The most notable difference between the proof
strategies of the two theorems is that the regularized DNN approximate the kernel ridge regressor
of noisy data, whereas in Theorem 3.8, unregularized DNN approximate the interpolant based on
noiseless data.
Step 1. Control on Δd,2. First, note that there is a recent finding that the reproducing ker-
nel HiIbert spaces induced from NTKs with any number of layers (i.e., L ≥ 1) have the same set
of functions, if kernels are defined on Sd-1. See Chen & Xu [2020]. Along with this result, under
the choice of model parameters as suggested in (69), we can apply exactly the same proof used in
Theorem.3.2 in Hu et al. [2021] for proving a following :
∣∆d,2∣2 ：= ∖∖gμ - f *∖∖2 =OP
∖∖g*∖∖H =OP (1).
(90)
38
Published as a conference paper at ICLR 2022
Step 2, Control on ∆p,1. For n data points (x1,..., Xn) and for the kth updated parameter WDɔ,
denote:
▽wg fw(k) (X)] = vec 卜 Wg[∕w(k) (XI)]}…，vec 卜 Wg[∕w(k) (Xn)]) J
Note that when ' = 1, Vw£ [/w(fc) (X)] ∈ Rmd×n and when ' = 2,...,L, Vw£ [/w(fc)(X)] ∈
_	2 一	D	d
Rm ×n
With this notation, We can write the vectorized version of the update rule (80) as:
k-1	/
VeC(WM)= VeC(WD)`) - ηι X (1 - η2μjvW' [fWD (I-I)(X)] ( UD (k — j — I) — y
j=0	∖
∀1 ≤ ' ≤ L and ∀k ≥ 1. Using the equality, we can get the decomposition :
k-1	/
VeC(W累)=VeC(WD)`) -ηιvWg[fWD(0)(X)] X (1 - η2μ)j (UD(k - j - 1) - y
	|	V	'	j = 0	'	' :=EI	|	V	' :=E2
k-1
-ηι E (1 - η2μ)j vwg [∕wnd-I)(X)] - vwg [∕wn(0)(X)] (UD(k - j -1) - y L
j=0	L	」'	，
'-----------------------------------V-----------------------------------}
=E3
(91)
Let zD,k(x) ：= VeC(VW` [fWgo (x)]), and note that fw(k)(x) = (zD,k(x), VeC(W黑)〉.Then, by
the definition of Δd,i and the decomposition (91), we have
∆D,1 = L X hzD,k(x), E1 + E2 + E3)一 Ker(x, X)(喘Z + H∞) y
1	L	1 L
=L Σ hzD,k (x), E1) + L ∑ hzD,k (x), E3)
+ L ^X hzD,k(x), E2i - Ker(X' X) (m Z + H∞) y	(92)
1 '-1-----------------------V---------------------------/
:=C
First, we focus on controlling the '2 bound on the first two terms in (92). Observe that the first term
can be bounded as:
1	L	2	1 L
L ΣS hzD,k(X) EI) ≤ L ɪs lhzD,k(x), E1 i∣2 ∙	(93)
Recall that ∣∣z0,k(x)∣∣2 ≤ O(√mω) by Lemma 4.4. Then, the random variable
zD,k(X)TVeC(WI(O)`) ∣ zD,k(x) is simply a N(0, O(ω)) for 1 ≤ ' ≤ L. A straightforward ap-
plication of Chernoff bound for normal random variable and taking union bound over the layer
1 ≤ ' ≤ L yield that: with probability at least 1 - δ,
1L
L '=1
,k (X)TVeC
≤ O ( ω log
(94)
The '2 norm of the second term in (92) can be similarly bounded as (93) in addition with the Cauchy-
Schwarz inequality:
1L	2	1L	1L
LfhzD,k(X), E3)≤ L∑∣hzD,k(x), E3)∣2 ≤ L 斗zD,k(X)k2 kE3k2 .	(95)
39
Published as a conference paper at ICLR 2022
The ∣∣E3∣∣2 is bounded as:
k-1	Γ	"I z
∣∣E3∣∣2 =	ηι X	(1	-	η2μ}j	Vw£	[∕wn(k-j-i)(x)]	-	Vw£	[∕wd(0)(X)]	(UD(k	- j - 1) - y
j=0	L	」'	∙
k-1
≤ η1 X (1 - η2μ)j ∙ IIVWg [fWD (k-j-I)(X)] - vW' [fWD (0)(X)] ∣∣2 IIuD (k - j - 1) - yk2
j=0
k-1
≤ ηi X (1 - η2μ)j ∙ IIvWg [fWD (k-j-I)(X)] - vWg [fWD (0)(X)] ∣∣F lluD(k - j - 1) - yk2
j=0
k-1
ηι £ (1 - η2μ)3 ∙
j=0
k-1
≤ η1 £ (1 - η2μ)3 ∙
j=0
∖
n
EIIVWg [fWD(k-j-1)(Xi)] -VWg[fW0(0)(Xi)] 1 1 F IIuD(k - j - I)-
i=1
yk2
∖
n
2	X ||VWg [fWD(k-j-1)(Xi)] - vWg [fWD(0)(Xi)] ll2 kuD(k - j - 1)-
yk2
η	/	___________∖	//10/331/6	______∖
≤ ——∙ O ( τ 1/3L2 ,ωmn log(m) I ∙ O(√n) ≤ O I —^TTTTn6d-3 ,log(m)).
η2μ	∖	)	∖ m2∕3j1/3	)
(96)
In the first, second and third inequalities, We used a simple fact that for the matrix A ∈ Rd1 ×d2 with
rank r, then ∣∣A∣2 ≤ IlAkF ≤ √r∣A∣2. Recall that the rank of the matrix VWg [Wdd (k-j-1)(x)] 一
VWg [∕wd(0) (x)] is at most 2. In the second to the last inequality, we use the result of Lemma 4.6
and the ∣∣ud(i) - y∣∣2 ≤ O(√n) for any i ≥ 1. In the last inequality, we plug the correct orders
as set in (69) to T, η1, η2 and μ. Back to the inequality (95), using the ∣∣zD,k(x)∣∣2 ≤ O(√mω)
and (96), we can get
1
L
L
X ∣zD,k (χ)∣2 ∣E3∣2 ≤Op
2=1
(L20∕3ω4∕3
∖	m1/3
(97)
Before controlling the '2 norm of C in (92), recall that we set B := (n2^Z + H∞) H∞y and
the dynamics of UD (k) - B can be expressed in terms of H∞ as follows: For any k ≥ 1,
UD(k) - B = ((1- η2μL) ∙Z- mη1 H∞) (UD(0) - B) + ∈D(k),	(98)
with ∣∣θd(k)∣2 ≤ O(1). Using (98), we can further decompose the term E2 in (91) as:
k-1
e2 := -5VWg [fWD(0)(X)]	(1 - η2μ)j ( uD(k - j - 1) - y )
j=0	'	)
k—1	k—j — 1
=mVwg [fWD(0)(X)] X (1 - η2μ)j ((1 - η2μL) ∙ I - mη1H∞) B
j=0	'	)
k-1	k-j -1
-	mVwg [fwD(0)(X)] X (1 - η2μj ((1 - η2μL) ∙ I - mη1H∞)	ud(0)
j=0	'	)
k-1
-	%VWg [fWD (0)(X)] X (1 - η2μj eD (k - j - 1)
j=0
k-1
-	mVWg [fWD(0)(X)] E (1 - η2μj (B - y∖
j=0	'	)
=e2,1 + e2,2 + e2,3 + e2,4∙	(99)
40
Published as a conference paper at ICLR 2022
Then, We can re-write the error term C in (92) as:
一 1 上， ，、1 上， ，、1 上，
C=串 hzD,k (x), E2,Q +lςhzD,k (X), E2,2〉 +lςhzD,k (X) ,E2,3〉
L X hzD,k (x), E2,4〉- Ker(x, X)(篙I + H∞)；
(100)
{z^
:=D
Our goal is to control the '2 norm of each summand in the equality (100). For the first three terms
in (100), a simple Cauchy-Schwarz inequality can be applied: for i = 1, 2,3:
1L
L EhzD,k(x), E2,i)
L '=1
2 I L	I L
≤ L EKzD,k (χ), E2Q∣2 ≤ L EkzD,k (χ)k2 ∙ kE2,∙d∣2.
L'=1	L'=1
We work on obtaining the bound of PL=I ∣∣E2,11∣2. Let Tk be defined as
k— 1	/	、k-j-1
Tk ：= X(1 -η2μ)j^(1 -η2μL) ∙I-mη1H∞)
Then, we have
X ∣∣e2,1∣∣2 = η2 X (BTTTVW' [∕wn(0)(χ)]τVw' [∕wd(0)(X)]五B
'=1
'=1 '
=mη2B>T> H(0)无 B
=mη2B>T>(H(0) - H∞) TkB + mη2B>T>H∞TkB
≤ mη2 kH(0) - H∞∣∣2 ∙ B>T2B + mη2B>T>H∞TkB.
(101)
To obtain the upper-bound on (101), we need to control the terms TkH∞T and Bτ7^B. Let us
denote H∞ = P2ι λivivτ be the eigen-decomposition of H∞. Using 1 - η2μL ≤ 1 - η2μ, note
that
k—1	k	k k—j—1
Tk = X(1 - η2μ) (\-饵L)IT(E - 1 -ɪLH∞)
k— 1 /	X i
-Q-仙尸 X (I-告h h∞)
(1-η2")kT X (⅛i^bVT W
j = 0、	1-42* 八j	/
mηιλ∞
k
—∙I.
(102)
+
L
L
(1 - 3〃)
A similar logic can be applied to bound TkH∞Tk:
,T 二 /1 _(1 _ λ)k∖2
T> H∞ Tk - (1 - η2μ)k-1 X ( -( m；"'	j ) N Vj VT
j=0、	1-n2μλj	)
(1 —小小产 (Hg)-I
-m2η2	YHL).
(103)
41
Published as a conference paper at ICLR 2022
Recall the definition of the notation B := H∞ (η2μLI + H∞)	y. Then, We can bound the term
BTTkT H∞Tk B:
(1 —小〃产
BTTTH∞TB ≤
m2ηi
(1 一小〃)
m2η2
. bt(h∞)-1b
2k	、-1	、-1
一∙ yτ	I + H∞	H∞	I + H∞ ) y
O((1 — 3〃广
1	m2η2
(104)
where in the last equality, we used IIgjlH =OlP (1) in (90). Now we turn our attention to bound
the term BtT2B,
BTTi2B ≤
⅛⅛2k y> (箸 i+h∞Γ (Hj2 (篙 i+H∞)-1y
O((I - η2μ)2k n、
I	m2η2λ∞	)
(105)
where we used ∣∣y∣∣2 = O(n) in the last inequality. Combining the bounds (104), (105) and the
result from Lemma 4.11, we can further bound (101) and have:
X kE2,ι∣∣2 ≤θ(ω ⅛∣μΓ …q^
+ (1 — η24)2kʌ ≤ O ((1 — η24)2k
m )
(106)
m
where in the second inequality, we used m ≥ Ω (L19n20logδ2m)). Similarly, we can bound
p3kE2,2∣ι2:
L
E kE2,2∣∣2
2=1
=η2 E (UD(O)TTTVW' [∕wd(O)(X)]>Vwg [∕wd(O)(X)]TkUD(O))
'=1 ∖	)
=mη2UD (0)TTTH(O)Tk UD (0)
=mη1uD(0)TTT (H(O)- H∞)TkUD(0) + mη2uD(0)TTTH∞7ku0(0)
≤ mη2 ∣∣H(0) - H∞∣∣2 ∙ Ud(0)TTi2Ud(0) + mη2UD(0)TTTH∞TkUD(0)
≤ mη2 Qm-黑)。"/ q0⅛F)kUD(0)k2
+mη2 (1 mηημ) UD (O)T(H∞)- ιud (0)
≤ O
OP
(1 - η2μ)2k n%2L5/2 q log(nL∕δ) + nω(1 - η2μ)2k
mλ∞δ2	V —m — + —mλ∞δ2一
∕nω(1 - η2μ)2k λ
mλ∞
(107)
Here, in the second inequality, we used the inequalities (102) and (103) and Lemma 4.11. In the
third inequality, WeUSedtheLemma 4.8, ∣∣u(0)∣∣2 = O(ʌɪ) with probability at least 1 - δ. In the
last equality, we used m ≥ Ω(L19n20 log；(m)
42
Published as a conference paper at ICLR 2022
Next, we bound PL=IkE2,3∣∣2 as:
L	/ k-1
X ∣∣E2,3∣∣2 = mηl ∙ ( X (1 — η2μ)3 (efc-j-i) ) HD(0)( X (1 —小〃)j (efc-j-i)
'=1	∖j=0	)	∖j=0
22
≤ 叫∙ λmax(H0(k)) ∙ kek-j-ik2 ≤ 叫∙ O(ωnL) ∙ O
η2 μ2	唯 μ2
L 3 L3 — 4d-3
O ——ω ∙ n 2d-1
∖ m
(108)
Now, we focus on obtaining the '2 norm bound on D in (100). Recall the definition of the notation
B := H∞ ( η2μLI + H∞ )	y. A simple calculation yields that
B — y = H∞	I + H∞) iy — y =—皿(皿I + H∞) iy.
mη1 η1 m
Then, We can re-write the expression of the D as:
D :
1 L	k-1	、-1
∙ ηι L X〈zD,k(X), Vw' [∕wd(0)(X)DX(I 一 η2μj	i + h∞ ) y
L '=1	j=0
—Ker(x, X)	I + H∞) y
1L
—〈ZD,k (x), VWg [∕wd(0) (X)D — Ker(X, X)
m '=1
1L
—(1 一 η2μ) m ΣS〈ZD,k (X), VWgfWD (0) (X)D
2=1
1L
m∑S<z0,k (χ), vw` fWD (0) (X)D - Ker(X, X)
I + H∞)	y
I + H∞) I
I+H∞)	y
∖	2=1
—(1 — η2μ)k —X(z0,k (x), vWgfWD (0) (X)D	i + h∞) y
m U	)
+ (1 — (1 — η2μ)k ) (m X〈ZD,k (X) — zD,0(x), VWgfWD (0)(X)D)(η2μm i + ]
'=1	(109)
where in the second equality, Pk=(I(I - η2μ)j = I-(I-产)is used. The '2 norm of first term in
the (109) can be bounded as:
-1
y,
I( m X〈zD,k(X), VW' [∕wd(0)(x)D — Ker(X, X)
≤ i (m X〈zD,k(x), VWgfWD (0)(X)D—Ker(X, χ)
I + H∞ )	y
∖ '=1
n / 1 L
ΣS (m E〈z0(x)，Vwg fWD(0)
i=1 ' '=1
2
(Xi)D — Ker(x, Xi))
η2 μL
η↑m
2
I+H∞)	y
2
I + H∞)	y
T
2
≤ O (ω√nL5/2 q∕log(l) ∙ O
=O (ωηlmnL5/2 qlog(nL∕6) ) = O (ωL5∕2nɪ qlog(nL∕6)
(110)
43
Published as a conference paper at ICLR 2022
where, in the second inequality, we used Lemma 4.11, and also we used
∣(篙 Z+H∞)T] ≤ RiFZHTy ≤ S 将 M=。(盟 √n)∙
(111)
The '2 norm of the second term in (109) can be easily bounded as:
(1 - η2μ)k -m X〈z0(x)，NW[/Wo(O)(X)]〉(喘I+h∞)	y
≤
<
2=1	、 〃	/
1L
(1 - η2μ) ( — E(z0(x), VW' [∕wd(0)(x)D - Ker(X' X)
'm 2=1
2
+ (1 - η2μ)kKer(x, X)
I + H∞)	y
1 L
(1 - η2μ)	m E(z0(x)，Vw' [∕wd(0)
2=1
+ (1 - η2 μ)k Ker(x, X)
2
(X)D - Ker(x, X)	∙
2
η2匹 I + H∞) 1y
η∖m	)
I + H∞)	y
L
2
2
<
(1 - η2μ)k ∙ O 卜«l3/2 qMm^) ∙O (黑√n) + O ((1 - k
<
(1 - η2μ)k ∙ O (ω
2d-1 勺 ιog(nL∕δ)
+ O (1-η2μ)k .
(112)
Lastly, the '2 norm of the third term in (109) is bounded as:
/	/	、k、(1 A /	，、	，、一	「. 一
(1-(1-η2μ)) m ∑<zD,k (χ)-zD,0(x), Vw' [∕wd(0)(x)D
∖m 2=1
1-
1 L
(1 - η2μ)k) ∙ m X〈zD,k (x)-
'=1
zD,0(x), Vw' [∕wd(0)(X)])∣ ∙
2
I + H∞) I
1-
1-
1-
(1 -η2μ)k) ∙ (-1X ιιzD,k (X) - zD,0 (X)Il F ∣ ∣ Vwg [fWD (0) (X)] ∣ ∣ f
' m '=1
(1 — η2μ)k) ∙ (—O(τ1/3L2pωmlog(m)) ∙ O(√ωmn)) ∙ O( ηi-
(1-η2N) ∙o( s7/6L10/3n M Plθg(m) ) <o( ω7∕6L10∕3n 汨
∖	m1/6δ1/3 /	∖
2
I + H∞)	y
√log(m))
m1∕6δ1/3 J
(113)
喘I+碎3
L
<
<
<
<
2
where in the fourth inequality, T = OP (L√ωn小)is plugged in. Combining the inequali-
ties (110), (112) and (113), we get the bound on ∣∣D∣∣2 in (109):
∣D∣2 < θ(ω2L5n 2⅛ ʌ/ "以丁)) + (1 -η2”)2k o(w2L3π 2⅛ / '。叫LL/δ) + O((1 - η2μ)2k )
+ O«/3L20/3n 咨 m⅛⅛ )
TR院品X) + O((1 - η2〃产).	(114)
44
Published as a conference paper at ICLR 2022
Step 3. Combining all pieces. Recall ∣∣z0,k(x)k2 ≤ O(√mω). With this fact, combining the
bounds (94), (97), (106), (107), (108) and (114), We can bound the ∣∣∆D,ιk2 Via the decomposi-
tion (92) as follows:
∣Δd,i∣2 ≤ 1 XX∣zD,k(x)>vec(wD')∣2 +
L '=1
1L
+ L EkzD,k(χ)∣2 kE2,1∣∣2 +
1L
L EkzD,k (χ)∣2 kE3∣∣2
1L	1L
L Σ kzD,k(x)k2 kE2,2∣2 + L E kzD,k(x)k2 kE2.3∣2
1 XX hzD,k(x), E2,4i - Ker(χ, X)	I + H∞) y
L '=1
≤ O ω log
'=1
(L20∕3ω4/3
+ OP (	mi/3	n
nω2(1 - η2μ)2k
Lλ∞
'=1
2
+ OP
ω(1 一 η2μ)2k
L
+ OP
L2
+OP ——ω ∙ n
m
_ 4d-3
-2d-1
+
2
+ O ( ω7/3L20/3n6d-3 ^m^ I + OP((I- η2μ)2k
∖	m1∕3δ2/3 /	∖
≤ OP
45