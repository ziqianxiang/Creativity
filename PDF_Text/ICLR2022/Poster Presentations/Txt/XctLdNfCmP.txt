Published as a conference paper at ICLR 2022
Predicting physics in mesh-reduced space
WITH TEMPORAL ATTENTION
Xu Han *
Tufts University
Xu.Han@tufts.edu
Han Gao*
University of Notre Dame
hgao1@nd.edu
Tobias Pffaf*
DeepMind
tob.pfaff@gmail.com
Jian-Xun Wang
University of Notre Dame
jwang33@nd.edu
Li-Ping Liu
Tufts University
Liping.Liu@tufts.edu
Ab stract
Graph-based next-step prediction models have recently been very successful in
modeling complex high-dimensional physical systems on irregular meshes. How-
ever, due to their short temporal attention span, these models suffer from error
accumulation and drift. In this paper, we propose a new method that captures
long-term dependencies through a transformer-style temporal attention model. We
introduce an encoder-decoder structure to summarize features and create a com-
pact mesh representation of the system state, to allow the temporal model to op-
erate on a low-dimensional mesh representations in a memory efficient manner.
Our method outperforms a competitive GNN baseline on several complex fluid
dynamics prediction tasks, from sonic shocks to vascular flow. We demonstrate
stable rollouts without the need for training noise and show perfectly phase-stable
predictions even for very long sequences. More broadly, we believe our approach
paves the way to bringing the benefits of attention-based sequence models to solv-
ing high-dimensional complex physics tasks.
1	Introduction
There has been an increasing interest in many scientific disciplines, from computational fluid dy-
namics [3, 40] over graphics [43, 41] to quantum mechanics [21, 1], to accelerate numerical simu-
lation using learned models. In particular, methods based on Graph Neural Networks (GNN) have
shown to be powerful and flexible. These methods can directly work with unstructured simulation
meshes, simulate systems with complex domain boundaries, and adaptively allocate computation to
the spatial regions where it is needed [8, 39, 35, 52].
Most models for complex physics prediction tasks, in particular those on unstructured meshes, are
next-step prediction models; that is, they predict the next state u(t+ 1, x) of a physical system from
the current state u(t, x). As next-step models suffer from error accumulation, mitigation strategies
such as training noise [39] or augmented training data using a solver-in-the-loop [42] have to be used
to keep rollouts stable. These remedies are not without drawbacks- training noise can be hard to
tune, and ultimately place a bound on the achievable model accuracy. And worse, next-step models
also tend to show drift, which is not as easily mitigated. Failure examples include failure to conserve
volume or energy, shift in phase, or loss of shape information (see e.g., the failure case example in
[39]).
On the other hand, auto-regressive sequence models such as Recurrent neural networks (RNNs), or
more recently transformers, have been hugely successful in predicting sequences in NLP and image
applications [36, 31]. They can capture stochastic dynamics and work with partial observations
[50]. Furthermore, their long attention span allows them to better preserve phase and conserved
quantities [18]. However, as memory cost for full-sequence transformer models scales with both
* Equal contribution.
1
Published as a conference paper at ICLR 2022
sequence length and spatial extents, it is hard to directly extend such models to predict physical
systems defined on large unstructured meshes.
This paper combines powerful GNNs and a transformer to model high-dimensional physical systems
on meshes. The key idea is creating a locally coarsened yet more expressive graph, to limit memory
consumption of the sequence model, and allow effective training. We first use a GNN to aggregate
local information of solution fields of a dynamic system into node representations by performing
several rounds of message passing, and then coarsen the output graph to a small set of pivotal nodes.
The pivotal nodes’ representations form a latent that encodes the system state in a low-dimensional
space. We apply a transformer model on this latent, with attention over the whole sequence, and
predict the latent for the next step. We then use a second GNN to recover the full-sized graph by
up-sampling and message passing. This procedure of solving on a coarse scale, upsampling, and
performing updates on a fine-scale is related to a V-cycle in multigrid methods [4].
We show that this approach can outperform the state-of-the-art MeshGraphNets[35] baseline on
accuracy over a set of challenging fluid dynamics tasks. We obtain stable rollouts without the need
to inject training noise, and unlike the baseline, do not observe strong error accumulation or drift in
the phase of vortex shedding.
2	Related Work
Developing and running simulations of complex, high-dimensional systems can be very time-
intensive. Particular for computational fluid dynamics, there is considerable interest in using neural
networks for accelerating simulations of aerodynamics [3] or turbulent flows [22, 46]. Fast predic-
tions and differentiable learned models can be useful for tasks from airfoil design [40] over weather
prediction [37] to visualizations for graphics [43, 41, 7]. While many of these methods use convo-
lutional networks on 2D grids, recently GNN-based learned simulators, in particular, have shown to
be a very flexible approach, which can model a wide range of systems, from articulated dynamics
[38] to dynamics of particle systems [24, 39]. In particular, GNNs naturally enable simulating on
meshes with irregular or adaptive resolution [35, 8].
These methods are either steady-state or next-step prediction models. There are, however, strong
benefits of training on the whole predicted sequence: next-step models tend to drift and accumu-
late error, while sequence models can use their long temporal range to detect drift and propagate
gradients through time to prevent error accumulation. Models based on RNNs have been success-
fully applied to 1D time series and small n-body systems [6, 54] or small 2D systems [50]. More
recently, transformer models [44], which have been hugely successful for NLP tasks [36], are be-
ing applied to low-dimensional physics prediction tasks [18]. However, applying sequence mod-
els to predict high-dimensional systems remains a challenge due to their high memory overhead.
Dimensionality reduction techniques, such as CNN autoencoders [34, 33, 27, 23, 30, 17, 12, 28],
POD [45, 49, 5, 32, 19, 9, 48, 11], or Koopman operators [25, 10, 15] can be used to construct a low-
dimensional latent space. The auto-regressive sequence model then operates on these linear (POD
modes) or nonlinear (CNNs) latents. However, these methods cannot directly handle unstructured
data with moving or varying-size meshes, and many of them do not consider parameter variations.
For example, POD cannot operate on state vectors with different lengths (e.g., variable mesh sizes
data in Fig.2 bottom). On the other hand, CNN auto-encoders can only be applied to rectangular
domains and uniform grids; and rasterization of complex simulation domains to uniform grids are
known to be inefficient and are linked to many drawbacks as discussed in [13].
In contrast, our method reduces the input space locally by aggregating information on a coarsened
graph. This plays to the strength of GNNs to learn local universal rules and is very closely related
to multi-level methods [51, 16] and graph coarsening [14, 53, 2].
3	Methodology
3.1	Problem Definition and Overview
We are interested in predicting spatiotemporal dynamics of complex physical systems (e.g., fluid dy-
namics), usually governed by a set of nonlinear, coupled, parameterized partial differential equations
2
Published as a conference paper at ICLR 2022
%1
μ
Figure 1: The diagram of the proposed model, GMR-Transformer-GMUS. We first represent the
domain as a graph and then select pivotal nodes (red/green/yellow) to encode information over the
entire graph. The encoder GMR runs Message passing along graph edges so that the pivotal nodes
collect information from nearby nodes. The latent vector zt summarizes information at the pivotal
nodes, and represents the whole domain at the current step. The transformer will predict zt+1 based
on all previous state latent vectors. Finally, we decode zt+1 through message passing to obtain the
next-step prediction Yt+1.
(PDEs) as shown in its general form,
du∂X,t) = F[u(x,t); μ],	x,t ∈ Ω X [0,Te],
(1)
where u(x, t) ∈ Rd represents the state variables (e.g., velocity, pressure, or temperature) and
F is a partial differential operator parameterized by μ. Given initial and boundary conditions,
unique spatiotemporal solutions of the system can be obtained. One popular method of solving these
PDE systems is the finite volume method (FVM) [29], which discretizes the simulation domain Ω
into an unstructured mesh consisting of N cells (Ci : i = 1, . . . , N). At time t, the discrete
state field Yt = {ui,t : i = 1, . . . , N } can thus be defined by the state value ui,t at each cell
center. Traditionally, solving this discretized system involves sophisticated numerical integration
over time and space. This process is often computationally expensive, making it infeasible for real-
time predictions or applications requiring multiple model queries, e.g., optimization and control.
Therefore, our goal is to learn a simulator, which, given the initial state Y0 and system parameters
μ, can rapidly produce a rollout trajectory of states Y1...YT.
As mentioned above, solving these systems with high spatial resolution by traditional numerical
solvers can be quite expensive. In particular, propagating local updates over the fine grid, such
as pressure updates in incompressible flow, can require many solver iterations. Commonly used
technique to improve the efficiency include multigrid methods [47], which perform local updates
on both the fine grid, as well as one or multiple coarsened grids with fewer nodes, to accelerate the
propagation of information. One building block of multigrid methods is the V-cycle, which consists
of down-sampling the fine to a coarser grid, performing a solver update, up-sampling back on the
fine grid, and performing an update on the fine grid.
Noting that GNNs excel at local updates while the attention mechanism over temporal sequences
allows long-term dependency (see remark A.2), we devise an algorithm inspired by the multigrid
V-cycle. For each time step, we use a GNN to locally summarize neighborhood information on our
fine simulation mesh into pivotal nodes, which form a coarse mesh (section 3.2). We use temporal
attention over the entire sequence in this lower-dimensional space (section 3.3), upsample back onto
the fine simulation, and perform local updates using a mesh recovery GNN (section 3.2). This com-
bination allows us to efficiently make use of long-term temporal attention for stable, high-fidelity
dynamics predictions.
3.2	Graph Representation and Mesh Reduction
Graph Representation. We construct a graph G = (V, E) to represent a snapshot of a dynamic
system at time step t. Here each node i ∈ V corresponds the mesh cell Ci , so the graph size is
|V | = N . The set of edges E are derived from neighboring relations of cells: if two cells Ci and Cj
3
Published as a conference paper at ICLR 2022
are neighbors, then two directional edges (i, j) and (j, i) are both in E. We fix this graph for all time
steps. At each step t, each node i ∈ V uses the local state vector ui,t as its attribute. Therefore,
(G, (Y0, . . . , YT )) forms a temporal graph sequence. The goal of the learning model is to predict
(Y1,...,YT)givenGandY0.
Mesh Reduction. After representing the system as a graph, we use a GNN to summarize and extract
a low-dimensional representation zt from Yt for each step t. In this part of discussion, we omit the
subscript t for notational simplicity. We refer to the encoder as Graph Mesh Reducer (GMR) since
its role is coarsening the mesh graph. GMR first selects a small set S ⊆ V of pivotal graph nodes and
locally encodes the information of the entire graph into representations at these nodes. By operating
on rich, summarized node representations, the dynamics of the entire system is well-approximated
even on this coarser graph.
There are a few considerations for selection of pivotal nodes, for example, the spatial spread and the
centrality of nodes in the selection. We generally use uniform sampling to select S from V . This
effectively preserves the density of graph nodes over the simulation domain, i.e. pivotal nodes are
more concentrated in important regions of the simulation domain. More details and visualizations
on the node selection process can be found in section A.6.
GMR is implemented as a Encode-Process-Decode (EPD) GraphNet [39]. GMR first extracts node
features and edge features from the system state using the node and edge Multi-Layer Perceptrons
(MLPs).
vi0 = mlpv(Y [i]),	ei0j = mlpe(p(i) -p(j)).	(2)
Here Y [i] is the i-th row of Y , i.e. the state vector at each node, and p(i) is the spatial position of
cell Ci .
Then GMR uses L GraphNet processer blocks [38] to further refine node representations through
message passing. In this process a node can receive and aggregate information from all neighboring
nodes within graph distance L. Each processor updates the node and edge representations as
ej = mlpe(e'-1,v'-1,v'-1),	v' = mlpv (v'-1, X e'-11, ' = 1,...,L.⑶
j∈Ni
Here vi0, ei0j are the outputs of Equation 2, and Ni denotes all neighbors of node i. The two functions
mlpe(∙) and mlpV (∙) respectively concatenate their arguments as vectors and then apply MLPs. The
calculation in equation 2 and equation 3 computes a new set of node representations V = (viL : i ∈
V) for the graph G.
Finally GMR applies an MLP to representations of the pivotal nodes in S only to “summarize” the
entire graph onto a coarse graph:
hi = mlpr (viL), i ∈ S	(4)
We concatenate these vectors into a single latent z = concat(hi : i ∈ S) as reduced vector represen-
tation of the entire graph. We collectively denote these three computation steps as z = GMR(G, Y ).
The latents z can be computed independently for each time step t and will be used as the represen-
tation for the attention-based simulator.
Mesh Recovery To recover the system state from the coarse vector representation z, we define a
Graph Mesh Up-Sampling (GMUS) network. The key step of GMUS is to restore information on
the full graph from representations of pivotal nodes. This procedure is the inverse of operation of
GMR. We first set the representations at pivotal nodes by splitting z, that is, ri = hi , i ∈ S . We
then compute representations of non-pivotal nodes by spatial interpolation [2]: for a non-pivotal
node j, we choose a set Nj0 of k nearest pivotal nodes in terms of spatial distance and compute the
its representations rj by
rj =
i∈Nj0
Wijhi
i∈Nj0 wij
1
Wij = E
(5)
Here d(j, i) is the spatial distance between cells Cj and Ci . Then every node i ∈ V has a represen-
tation ri, and all nodes’ representations are collectively denoted as R = (ri : i ∈ V).
4
Published as a conference paper at ICLR 2022
Similar to GMR, GMUS applies EPD GraphNet to the initial node representation R to restore Y
on the full graph G. We denote the chain of operations so far as Y = GMUS(G, z). Details about
information flows in GMR and GMUS can be found in section A.1.
We train GMR and GMUS as an auto-encoder over all time steps and sequences. For each time step
t, we compute Y⅛ = GMUS(G, GMR(G, Yt)) and minimize the reconstruction loss
T
Lgraph =	X kK — Ek2 .	(6)
n=1
With these two modules we can encode system states (Y1, . . . , YT) to latents (z1, . . . , zT) as a low-
dimensional representation of system dynamics. In the next section, we train a transformer model
to predict the sequences of latents.
3.3	Attention-based Simulator
As a dynamics model, we learn a simulator which can predict the sequence (z1, . . . , zT) autore-
gressively based on an initial state zo = GMR(G, Y0) and the system parameters μ. The system
parameters include conditions such as different Reynolds numbers, initial temperatures, or/and pa-
rameters of the spatial domain (see Table 3 for details). The model is conditioned on μ, to be able
to predict flows with arbitrary system parameters at test time.We encode μ into a special parameter
token z-1 of the same length as the state representation vectors zt
z-1 = mlPp(μ).	(7)
Then, the transformer model [44], trans(∙) predicts the subsequent latent vectors in an autoregres-
sive manner.
;^1 = trans(z-ι, zo),	Zt = trans(z-ι, zo, Zι,..., Zt-ι)
(8)
Specifically, we use a single layer of multi-head attention in our transformer model, which we found
sufficiently powerful for the environments we studied. We first describe the computation of a single
attention head: At step t, the prediction z^t-ι from the previous step issues a query to compute
attention a(t-i) over latents (z-ι, zo, j^i,..∙, Zt-i).
a(t-1)
softmax
(Z3WJW2
I	√d0
z-1, zo, z1,. ..,zt-1,
(9)
Here W1 and W2 are learnable parameters, and d0 denotes the length of the vector zt-1. This
equation corresponds the popular inner-product attention model [44].
Next, the attention head computes the prediction vector
gt = W3 z-1,Zo, Zi,..., Z(t-1)] at-1,
with the learned parameter W3 .
(10)
Our multi-head attention model uses K parallel attention heads with separate parameters to compute
K vectors (gt1, . . . , gtK) as described above. These vectors are concatenated and fed into an MLP to
predict the residual of Zt over Zt-∖.
Zt = Zt-i +mlPm (Concat(g1,..., gK))	(11)
We train the autoregressive model on entire sequences by minimizing the prediction loss
T
Latt = X Ilzt - ztk2.
t=i
(12)
Compared to next-step models such as MeshGraphNet [35], this model can propagate gradients
through the entire sequence, and can use information from all previous steps to produce stable
predictions with minimal drift and error accumulation.
The proposed model also contains next-step models as special cases: ifwe select all nodes as pivotal
nodes and set the temporal attention model such that it predicts the next step with only physical
parameters and the current step, then the proposed model becomes a next-step model. The proposed
model shows advantage when it reduces the dimension of representation by using pivotal nodes and
expands the input range when predicting the next step. A.2 shows how this method reduces the
amount of computation and memory usage.
5
Published as a conference paper at ICLR 2022
3.4	Model training and testing
We train the mesh reduction and recovery modules GMR, GMUS by minimizing equation 6 and
the temporal attention model by minimizing equation 12. We obtain best results by training those
components separately, as this provides stable targets for the autoregressive predictive module. A
second reason is memory and computation efficiency; we can train the temporal attention method
on full sequences in reduced space, without needing to jointly train the more memory-intensive
GMR/GMUS modules.
The transformer model is trained by incrementally including loss terms computed from different
time steps. For every time step t, we train the transformer by minimizing the objective Ptt0=1 kzt0 -
Zto||2，Until the objective reaches a threshold, We add the next loss term ∣∣zt+ι - Zt+ι∣∣ to the
objective and continue to train the model We found that this incremental approach leads to much
more stable training than directly minimizing the overall objective.
As the transformer model can directly attend to all previous steps via equation 10, We don’t suffer as
much from vanishing gradients, compared to e.g. LSTM sequence models. By predicting in mesh-
reduced space, the memory footprint is limited, and the model can be trained Without approximate
gradient calculations such as teacher forcing or limiting the attention history. Details for training
procedure can be found in section A.3.
Once We have trained our models, We can make predictions for a neW problem. We first represent
system parameters as a token z-1 by equation 7 and encode the initial state into a token z0 =
GMR(G, Y0), then we predict the sequence zι,..., ZT by equation 8, and finally we decode the
system state Y by
Yt = GMUS(Zt), t = 1,...,T.	(13)
4	results
Datasets. We tested the proposed method to three datasets of fluid dynamic systems: (1) flow
over a cylinder with varying Reynolds number; (2) high-speed flow over a moving wedge with
different initial temperatures; (3) flow in different vascular geometries. These datasets correspond to
applications with fixed, moving, and varying unstructured mesh, respectively. A detailed description
of the datasets can be found in section A.4.
Methods. The proposed methods is compared to the state-of-the-art MeshGraphNet method [35],
which is a next-step model and has been shown to outperform a list of previous methods. Two
versions of MeshGraphNet are considered here, with and without Noise Injection (NI). We also study
variants of our model with LSTM and GRU (GMR-LSTM and GMR-GRU) instead of temporal
attention. These variants also operate in mesh-reduced space, and use the same encoder (GMR) and
decoder (GMUS) as our main method. Model details can be found in section A.5. And section A.6
shows pivotal nodes for each dataset.
Stable and visually accurate model predictions. We tested the proposed method on three fluid
systems. Figure 2 compares our model’s predictions for velocity against the ground truth (CFD)
on six test set trajectories. Our model shows stable predictions even without noise injection, and
model rollouts are visually very closely to the CFD reference. On the other hand, without mitiga-
tion strategies, next-step models often struggle to predict such long sequences stably and accurately.
For cylinder flow, our model can successfully learn the varying dynamics under different Reynolds
numbers, and accurately predict their corresponding transition behaviors from laminar to vortex
shedding. In supersonic flow over a moving wedge, our model can accurately capture the shock
bouncing back and forth on a moving mesh. In particular, the small discontinuities at the leeward
side of the edge are discernible in our model predictions. Lastly, in vascular flow with a variable-size
circular thrombus, and thus different mesh sizes, the model is able to predict the flow accurately. In
both cylinder flow and vascular flow, both frequency and phase of vortex shedding is accurately cap-
tured. The excellent agreement between model predictions and reference simulation demonstrates
the capability of our model when predicting long sequences with varying-size meshes. Results for
pressure can be found in section A.9.
6
Published as a conference paper at ICLR 2022
Re = 307 (Cylinder Flow)
t=0	t=85 t= 215	t=298
Re = 993 (Cylinder Flow)
t=0	t=85 t= 215	t=298
61UISjnO
,,
T(0) = 201 (Sonic Flow)
T(0) = 299 (Sonic Flow)
=IlUI Sjno
R = 0.31 (VaScUlarfloW) R = 0.49 (VaScUlar flow)
t=40	t=80^^	t=I60^^^^t=250	t=40	t=80^^	t= 160 ^^t=250
FigUre 2: ContoUrS of the velocity field, aS predicted by oUr model verSUS the groUnd trUth (CFD).
OUr model accUrately predictS long rolloUt SeqUenceS Under varying SyStem parameterS.
Table 1: The average relative rolloUt error of three SyStemS, with Unit of ×10-3. We compare
MeShGraphNet with or withoUt noiSe injection (NI), to three variantS of the oUr model (LSTM,
GRU, or TranSformer), which all Shared the Same GMR/GMUS meSh redUction modelS. OUr model
Significantly oUtperform MeShGraphNet on dataSetS with long rolloUtS.
DataSet-rollout Step Variable		Cylinder flow-400			Sonic flow-40				VaScular flow-250		
		u	v	P	u	v	p	T	u	v	p
MeShGraphNet	NI	25	778	136	1.71	3.67	0.4	0.027	57	133	55
	without NI	98	2036	673	4.12	6.13 0.24 0.020 3117				1771	601
	GRU	114	1491	1340	1.34	4.59	0.59	0.37	8.2	11.2	23.6
OurS	LSTM	124	1537	1574	1.57	5.8	0.69	0.45	8.4	11.1	23.3
	TranSformer 4.9		89	38	0.95	2.8	0.43	0.39	7.3	10	22
Error behavior under long rollouts To qUantitatively aSSeSS the performanceS of oUr
modelS and baSelineS, we compUte the relative mean SqUare error (RMSE), defined aS
RMSE(UPrediction, ^truth) = P(UP	edic-Ui	) , over the full rollout trajectory. Table 1 com-
( (Ui	)
pareS average prediction errorS on all three domainS. Our attention-baSed model outperformS baSe-
lineS and model variantS in moSt ScenarioS. For Sonic flow, which haS 40 time StepS, our model
performS better than MeShGraphNet when predicting velocity valueS (u and v). The quantitieS p and
T are relatively Simple to predict, and all modelS Show very low relative error rateS (< 10-3), though
our model haS Slightly worSe performance. The exampleS with long rollout SequenceS (predicting
cylinder flow and vaScular flow) highlight the Superior performance of our attention-baSed method
compared with baSeline next-Step modelS, and the RMSE of our method iS Significantly lower than
baSelineS.
Figure 3 ShowS how error accumulateS in different modelS. Our model haS higher error for the firSt
few iterationS, which can be explained by the information bottleneck of meSh reduction. However,
the error only increaSeS very Slowly over time. MeShGraphNet on the other hand SufferS from Strong
error accumulation, rendering it leSS accurate for long-Span predictionS. NoiSe injection partially
addreSSeS the iSSue, but error accumulation iS Still conSiderably higher compared to our model.
We can attribute thiS better long-Span error behavior of our Sequence model, to being able to paSS
gradientS to all previouS StepS, which iS not poSSible in next-Step modelS. We alSo note that the
7
Published as a conference paper at ICLR 2022
Time Step	Time Step
Figure 3: Averaged error over all state variables on cylinder flow (left), sonic flow (middle) and vas-
CUlar flow (right), for the models MeshGraphNets(MGN)C —), MGN-NI( ),OUrs-GRU ( ),
Ours-LSTM (■ —), Ours-Transformer (-). Our model, particularly the transformer, show much
less error accUmUlation compared to the next-step model.
Figure 4: Predictions of the next-step MeshGraphNet model and our model, compared to ground
truth. The next-step model fails to keep the shedding frequency and show drifts on cylinder flow.
On vascular flow, we notice the left inflow diminishing over the time, while our model remains close
to the reference simulation.
transformer model performs much better compared to the GRU and LSTM model variants. We
believe its ability to directly attend to long-term history helps to stabilize gradients in training, and
also to identify recurrent features of the flow, as discussed later in this section.
In Figure 4 we illustrate the difference in the models’ abilities to predict long rollouts. For cylinder
flow, our model is able to retain phase information. MeshGraphNet on the other hand drifts to an
opposite vortex shedding phase at t = 250, though its prediction still looks plausible. For vascular
flow, our model gives an accurate prediction at step t = 228, while the prediction of MeshGraphNet
is significantly different from the ground-truth at this step: the left inflow diminishes, making the
prediction less physically plausible.
Diagnosis of state latents. The low-dimensional latent vectors, computed by the encoder-decoder
(GMR and GMUS), can be viewed as lossy compression of high-dimensional physical data on ir-
regular unstructured meshes. In our experiment, we observe the compression ratio is ≤ 4% while
the loss of the information measured by RMSE is below 1.5%. More details are in section A.7.
We also visualize state latents z-s from the cylinder flow dataset and show that the model can distin-
guish rollouts from different Reynolds numbers. We first use Principal Component Analysis (PCA)
to reduce the dimension of z-s to two and plot them in Figure 5 (left). Rollouts with two different
Reynolds numbers have two clearly distinctive trajectories. They eventually enters their respective
periodic phase. We also apply PCA directly to the system states (Y) and get 2-d latent vectors di-
rectly from the original data. We plot these vectors in Figure 5 (right). Latent vectors from PCA start
in the center and form a circle. The two trajectories from two different Reynolds numbers are hard
to distinguish, which makes it challenging for the learned model to capture parameter variations. Al-
though CNN-based embedding methods [15, 51, 30] are also nonlinear, they cannot handle irregular
geometries with unstructured meshes due to the limitation of classic convolution operations. Using
pivotal nodes combined with GNN learning, the proposed model is both flexible in dealing data with
irregular meshes and effective in capturing state transitions in the system.
Diagnosis of attention weights. The attention vector at in equation 9 plays an important role in
predicting the state of the dynamic system. For example, if the system enters a perfect oscillating
8
Published as a conference paper at ICLR 2022
C
5
.
0
-1
-1	-0.95	-0.9	-0.85	-0.8	-1	-0.5	0	0.5
C0	C0
987
...
000
noitnettA nekoT retemara
0.6
0	100	200	300	400
Time Step
150
.
0
edutingaM
Figure 5: 2-D principle subspace of the latent vectors from
GMR (left) and PCA (middle) for flow past cylinder system:
Re = 307 (——>),Re = 993 (——>)start from —and —>—
respectively.
Figure 6: Attention values of
the parameter tokens versus
time, Re = 307 (------), and
Re = 993 (----).
gaM xaM ta qerF
0
Frequency	Frequency	Reynolds Number
Figure 7: Attention (—), CFD data (—•—). (left) Re versus frequency for attention and CFD data.
Fourier transform for both attention and CFD data at (middle) Re = 307 and (right) Re = 993.
stage, the model could directly look up and re-use the encoding from the last cycle as the prediction,
to make predictions with little deterioration or drift.
We can observe the way our model makes predictions via the attention magnitude over time. Figure 6
shows attention weights for the initial parameter token z-1, which encodes the system parameters
μ, over one cylinder flow trajectory. In the initial transition phase, we see high attention to the
parameter token, as the initial velocity fields of different Reynold numbers are hard to distinguish.
As soon as vortex shedding begins around t = 120, and the flow enters an oscillatory phase, and
attention shift away from the parameter token, and locks onto these cycles. This can be seen in
Figure 7 (left): we perform a Fourier analysis of the attention weights, and notice that the peak
of the attention frequency (blue) coincides with the vortex shedding frequency in the data (purple).
That is, our dynamics model learned to attend to previous cycles in vortex shedding, and can use this
information to make accurate predictions that keep both frequency and phase stable. Figure 7 (right)
shows that this observation holds for the whole range of Reynolds numbers, and their corresponding
vortex shedding frequencies.
For sonic flow, the system state during the first few steps is much easier to discern even without
knowing the system parameters μ, and hence, the attention to z-ι quickly decays. This shows that
the transformer can adjust the attention distribution to the most physically useful signals, whether
that is parameters or system state. Analysis on this, as well as the full attention weights maps can be
found in section A.10.
5 Conclusion
In this paper we introduced a graph-based mesh reduction method, together with a temporal at-
tention module, to efficiently perform autoregressive prediction of complex physical dynamics. By
attending to whole simulation trajectories, our method can more easily identify conserved properties
or fundamental frequencies of the system, allowing prediction with higher accuracy and less drift
compared to next-step methods.
9
Published as a conference paper at ICLR 2022
Acknowledgement
Han Gao and Jianxun Wang are supported by the National Science Foundation under award numbers
CMMI-1934300 and OAC-2047127. Li-Ping Liu are supported by NSF 1908617. Xu Han was also
supported by NSF 1934553.
References
[1]	MS Albergo, G Kanwar, and PE Shanahan. Flow-based generative models for markov chain
monte carlo in lattice field theory. Physical Review D, 100(3):034515, 2019.
[2]	Ferran Alet, Adarsh Keshav Jeewajee, Maria Bauza Villalonga, Alberto Rodriguez, Tomas
Lozano-Perez, and Leslie Kaelbling. Graph element networks: adaptive, structured compu-
tation and memory. In International Conference on Machine Learning, pp. 212-222. PmLr,
2019.
[3]	Saakaar Bhatnagar, Yaser Afshar, Shaowu Pan, Karthik Duraisamy, and Shailendra Kaushik.
Prediction of aerodynamic flow fields using convolutional neural networks. Computational
Mechanics, 64(2):525-545, 2019.
[4]	Dietrich Braess and Wolfgang Hackbusch. A new convergence proof for the multigrid method
including the v-cycle. SIAM journal on numerical analysis, 20(5):967-975, 1983.
[5]	Ashesh Chattopadhyay, Pedram Hassanzadeh, and Devika Subramanian. Data-driven predic-
tions of a multiscale lorenz 96 chaotic system using machine-learning methods: reservoir com-
puting, artificial neural network, and long short-term memory network. Nonlinear Processes
in Geophysics, 27(3):373-389, 2020.
[6]	Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary
differential equations. arXiv preprint arXiv:1806.07366, 2018.
[7]	Xiaohui Chen, Xu Han, Jiajing Hu, Francisco JR Ruiz, and Liping Liu. Order matters: Prob-
abilistic modeling of node sequence for graph generation. arXiv preprint arXiv:2106.06189,
2021.
[8]	Filipe de Avila Belbute-Peres, Kevin Smith, Kelsey Allen, Josh Tenenbaum, and J Zico Kolter.
End-to-end differentiable physics for learning and control. In Advances in Neural Information
Processing Systems, pp. 7178-7189, 2018.
[9]	Hamidreza Eivazi, Hadi Veisi, Mohammad Hossein Naderi, and Vahid Esfahanian. Deep neu-
ral networks for nonlinear model order reduction of unsteady flows. Physics of Fluids, 32(10):
105104, 2020.
[10]	Hamidreza Eivazi, Luca Guastoni, Philipp Schlatter, Hossein Azizpour, and Ricardo Vinuesa.
Recurrent neural networks and koopman-based frameworks for temporal predictions in a low-
order model of turbulence. International Journal of Heat and Fluid Flow, 90:108816, 2021.
[11]	Stefania Fresca and Andrea Manzoni. Pod-dl-rom: enhancing deep learning-based reduced
order models for nonlinear parametrized pdes by proper orthogonal decomposition. Computer
Methods in Applied Mechanics and Engineering, 388:114181, 2022.
[12]	Kai Fukami, Koji Fukagata, and Kunihiko Taira. Assessment of supervised machine learn-
ing methods for fluid flows. Theoretical and Computational Fluid Dynamics, 34(4):497-519,
2020.
[13]	Han Gao, Luning Sun, and Jian-Xun Wang. Phygeonet: physics-informed geometry-adaptive
convolutional neural networks for solving parameterized steady-state pdes on irregular domain.
Journal of Computational Physics, 428:110079, 2021.
[14]	Hongyang Gao and Shuiwang Ji. Graph u-nets. In international conference on machine learn-
ing, pp. 2083-2092. PMLR, 2019.
10
Published as a conference paper at ICLR 2022
[15]	Nicholas Geneva and Nicholas Zabaras. Transformers for modeling physical systems. arXiv
preprint arXiv:2010.03957, 2020.
[16]	Dirk Hartmann, Christian Lessig, Nils Margenberg, and Thomas Richter. A neural network
multigrid solver for the navier-stokes equations. arXiv preprint arXiv:2008.11520, 2020.
[17]	Kazuto Hasegawa, Kai Fukami, Takaaki Murata, and Koji Fukagata. Machine-learning-based
reduced-order modeling for unsteady flows around bluff bodies of various shapes. Theoretical
and Computational Fluid Dynamics, 34(4):367-383, 2020.
[18]	Michael J Hutchinson, Charline Le Lan, Sheheryar Zaidi, Emilien Dupont, Yee Whye Teh,
and Hyunjik Kim. Lietransformer: equivariant self-attention for lie groups. In International
Conference on Machine Learning, pp. 4533-4543. PMLR, 2021.
[19]	Pierre JacqUier, Azzedine Abdedou, Vincent Delmas, and Azzeddine Soulaimani. Non-
intrusive reduced-order modeling using uncertainty-aware deep neural networks and proper
orthogonal decomposition: Application to flood modeling. Journal of Computational Physics,
424:109854, 2021.
[20]	Hrvoje Jasak, Aleksandar Jemcov, Zeljko Tukovic, et al. Openfoam: A c++ library for complex
physics simulations. In International workshop on coupled methods in numerical dynamics,
volume 1000, pp. 1-20. IUC Dubrovnik Croatia, 2007.
[21]	Gurtej Kanwar, Michael S. Albergo, Denis Boyda, Kyle Cranmer, Daniel C. Hackett, Sebastien
Racaniere, Danilo Jimenez Rezende, and Phiala E. Shanahan. Equivariant flow-based sampling
for lattice gauge theory. Phys. Rev. Lett., 125:121601, Sep 2020. doi: 10.1103/PhysRevLett.
125.121601.
[22]	Dmitrii Kochkov, Jamie A. Smith, Ayya Alieva, Qing Wang, Michael P. Brenner, and Stephan
Hoyer. Machine learning-accelerated computational fluid dynamics. Proceedings of the Na-
tional Academy of Sciences, 118(21), 2021.
[23]	Angran Li, Ruijia Chen, Amir Barati Farimani, and Yongjie Jessica Zhang. Reaction diffusion
system prediction based on convolutional neural network. Scientific reports, 10(1):1-9, 2020.
[24]	Yunzhu Li, Jiajun Wu, Russ Tedrake, Joshua B. Tenenbaum, and Antonio Torralba. Learning
particle dynamics for manipulating rigid bodies, deformable objects, and fluids. In Interna-
tional Conference on Learning Representations, 2019.
[25]	Bethany Lusch, J Nathan Kutz, and Steven L Brunton. Deep learning for universal linear
embeddings of nonlinear dynamics. Nature communications, 9(1):1-10, 2018.
[26]	Luis F Gutierrez Marcantoni, JoSe P Tamagno, and Sergio A Elaskar. High speed flow simula-
tion using openfoam. Mecanica COmPutaciOnal, 31(16):2939-2959, 2012.
[27]	Romit Maulik, Bethany Lusch, and Prasanna Balaprakash. Reduced-order modeling of
advection-dominated systems with recurrent neural networks and convolutional autoencoders.
Physics of Fluids, 33(3):037106, 2021.
[28]	Masaki Morimoto, Kai Fukami, Kai Zhang, Aditya G Nair, and Koji Fukagata. Convo-
lutional neural networks for fluid flow analysis: toward effective metamodeling and low-
dimensionalization. arXiv preprint arXiv:2101.02535, 2021.
[29]	Fadl Moukalled, L Mangani, Marwan Darwish, et al. The finite volume method in computa-
tional fluid dynamics, volume 113. Springer, 2016.
[30]	Takaaki Murata, Kai Fukami, and Koji Fukagata. Nonlinear mode decomposition with convo-
lutional neural networks for fluid dynamics. Journal of Fluid Mechanics, 882, 2020.
[31]	Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku,
and Dustin Tran. Image transformer. In International Conference on Machine Learning, pp.
4055-4064. PMLR, 2018.
11
Published as a conference paper at ICLR 2022
[32]	Suraj Pawar, SM Rahman, H Vaddireddy, Omer San, Adil Rasheed, and Prakash Vedula. A
deep learning enabler for nonintrusive reduced order modeling of fluid flows. Physics of Fluids,
31(8):085101, 2019.
[33]	Jiang-Zhou Peng, Siheng Chen, Nadine Aubry, Zhi-Hua Chen, and Wei-Tao Wu. Time-variant
prediction of flow over an airfoil using deep neural network. Physics of Fluids, 32(12):123602,
2020.
[34]	Jiang-Zhou Peng, Siheng Chen, Nadine Aubry, Zhihua Chen, and Wei-Tao Wu. Unsteady
reduced-order model of flow over cylinders based on convolutional and deconvolutional neural
network structure. Physics of Fluids, 32(12):123609, 2020.
[35]	Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter W Battaglia. Learning
mesh-based simulation with graph networks. In International Conference on Learning Repre-
sentations, 2021.
[36]	Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Lan-
guage models are unsupervised multitask learners. 2019.
[37]	Stephan Rasp and Nils Thuerey. Data-driven medium-range weather prediction with a resnet
pretrained on climate simulations: A new model for weatherbench. Journal of Advances in
Modeling Earth Systems, 13(2):e2020MS002405, 2021.
[38]	Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Tobias Springenberg, Josh Merel, Martin Ried-
miller, Raia Hadsell, and Peter Battaglia. Graph networks as learnable physics engines for
inference and control. In International Conference on Machine Learning, pp. 4470-4479.
PMLR, 2018.
[39]	Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Pe-
ter Battaglia. Learning to simulate complex physics with graph networks. In International
Conference on Machine Learning, pp. 8459-8468. PMLR, 2020.
[40]	Nils Thuerey, Konstantin Weiβenow, LUkas PrantL and XiangyU Hu. Deep learning methods
for reynolds-averaged navier-stokes simulations of airfoil flows. AIAA Journal, 58(1):25-36,
2020.
[41]	Kiwon Um, Xiangyu Hu, and Nils Thuerey. Liquid splash modeling with neural networks. In
Computer Graphics Forum, volume 37, pp. 171-182. Wiley Online Library, 2018.
[42]	Kiwon Um, Robert Brand, Philipp Holl, Nils Thuerey, et al. Solver-in-the-loop: Learning from
differentiable physics to interact with iterative pde-solvers. arXiv preprint arXiv:2007.00016,
2020.
[43]	Benjamin Ummenhofer, Lukas Prantl, Nils ThUrey, and Vladlen Koltun. Lagrangian fluid
simulation with continuous convolutions. In International Conference on Learning Represen-
tations, 2020.
[44]	Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural informa-
tion processing systems, pp. 5998-6008, 2017.
[45]	Pantelis R Vlachas, Wonmin Byeon, Zhong Y Wan, Themistoklis P Sapsis, and Petros
Koumoutsakos. Data-driven forecasting of high-dimensional chaotic systems with long short-
term memory networks. Proceedings of the Royal Society A: Mathematical, Physical and
Engineering Sciences, 474(2213):20170844, 2018.
[46]	Rui Wang, Karthik Kashinath, Mustafa Mustafa, Adrian Albert, and Rose Yu. Towards
physics-informed deep learning for turbulent flow prediction, 2020.
[47]	Pieter Wesseling. Introduction to multigrid methods. Technical report, INSTITUTE FOR
COMPUTER APPLICATIONS IN SCIENCE AND ENGINEERING HAMPTON VA, 1995.
12
Published as a conference paper at ICLR 2022
[48]	Pin Wu, Junwu Sun, Xuting Chang, Wenjie Zhang, Rossella Arcucci, Yike Guo, and Christo-
pher C Pain. Data-driven reduced order model with temporal convolutional neural network.
Computer Methods in Applied Mechanics and Engineering, 360:112766, 2020.
[49]	Xuping Xie, Guannan Zhang, and Clayton G Webster. Non-intrusive inference reduced order
model for fluids using deep multistep neural network. Mathematics, 7(8):757, 2019.
[50]	SHI Xingjian, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun
Woo. Convolutional lstm network: A machine learning approach for precipitation nowcasting.
In Advances in neural information processing systems, pp. 802-810, 2015.
[51]	Jiayang Xu and Karthik Duraisamy. Multi-level convolutional autoencoder networks for para-
metric prediction of spatio-temporal dynamics. Computer Methods in Applied Mechanics and
Engineering, 372:113379, 2020.
[52]	Jiayang Xu, Aniruddhe Pradhan, and Karthik Duraisamy. Conditionally parameterized,
discretization-aware neural networks for mesh-based modeling of physical systems. arXiv
preprint arXiv:2109.09510, 2021.
[53]	Rex Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L Hamilton, and Jure
Leskovec. Hierarchical graph representation learning with differentiable pooling. arXiv
preprint arXiv:1806.08804, 2018.
[54]	Ruiyang Zhang, Yang Liu, and Hao Sun. Physics-informed multi-lstm networks for metamod-
eling of nonlinear structures. Computer Methods in Applied Mechanics and Engineering, 369:
113226, 2020.
13
Published as a conference paper at ICLR 2022
A Appendix
A. 1 Information flow in GMR and GMUS
Graph Mesh Reducer
Graph Mesh Up-Sampling
Figure 8: Schematic of information flow in the Graph Mesh Reducer (GMR) and Graph Mesh
Up-Sampling (GMUS). Red and blue nodes are pivotal nodes. The node size increase represents
information aggregation from surroundings.
Figure 8 shows how the information flows in Graph Mesh Reducer (GMR) and Graph Mesh Up-
Sampling (GMUS). This is analogous to a V-cycle in multigrid methods: Consider a graph with
seven nodes (|V | = 7) to be reduced and recovered by GMR and GMSU with two message-passing
layers. The increased size of pivotal nodes after a message-passing layer represents the information
aggregation from surrounding nodes.
A.2 Computational complexity
The computation of the proposed model is from the three components: GMR, GMUS, and the
transformer. The graph G contains N nodes and also has O(N) edges because of its special structure.
Then GMR and GMUS as L-layer graph neural networks have computation time O(NLd*), with
d being the maximum number of hidden units.
The transformer takes time O(T ∙ (d0)2) to make a prediction: a prediction needs to consider O(T)
previous steps, and the computation over a time step is O((d0)2). Note that d0 is the length of the
latent vector at each step.
In our actual calculation, the transformer takes much less time than GMR or GMUS since the latent
dimension d0 is small.
However, it would be very expensive if the model did not use GMR and directly applied a trans-
former to the original system states. In this case, d0 would be Nd, and the total complexity would
be O(TN2d2), which would not be feasible for typical computation servers.
14
Published as a conference paper at ICLR 2022
(a) Cylinder Flow
Figure 9: Overview of datasets of (a) cylinder flow, (b) sonic flow, and (c) vascular flow.
A.3 Model Training
The GMR/GMUS and temporal attention model are trained separately. We first train GMR/GMUS
by minimizing the reconstruction loss Lgraph defined by Eqn. 6. Then, the parameters of the trained
encoder-decoder are frozen, and only the parameters of the temporal attention model will be trained
by minimizing the prediction loss Latt (Eqn. 12) with early stopping. The parameter token encoder
mlpp is trained jointly with temporal attention. The training algorithm is detailed in Algorithm 1.
Algorithm 1 Training Process
Input: Domain graph G, Node features over time [Y0, . . . , YT], GMRθ, GMUSφ,Attentionω,
RMSE threshold R, MLPψ, condition parameter μ
Output: Learned parameters θ , φ, ω and ψ
repeat
for Yt ∈ [Y0,...,YT]do
Yt = GMUSφ (GMRθ (Yt, G))
end for
ʌ ʌ
Compute %θ,φ J 5,φLgraph(θ φ, [K,…，YT], [K,…，YT])
Update φ, θ using the gradients Vφ, Vθ
until convergence of the parameters (θ , φ)
[Z0, Z1,…，ZT] = GMRθ([Y,…,Yt]), z-i = MLPψ(μ)
fort ∈ [1,2,...,T] do
repeat
[Zι,…，Zt] = Attention”([z—i, zo])
COmPUte V ω,ψ j- ▽ ω,ψ Latt (ω, ψ, [z1, ∙ ∙∙ , zt], [z1, ∙ ∙∙ , zt])
Update ω, ψ using the gradients Vω, Vψ
until RMSE([zι,..., z/ [∙^ι,..., Zt]) < R0
end for
A.4 Dataset details
Three flow datasets, cylinder flow, sonic flow and vascular flow, are used in our numerical ex-
periments, and the schematics of flow configurations are shown in Figure 9. Cylinder and vascu-
lar flow are governed by incompressible Navier-Stokes (NS) equations, whereas compressible NS
equations govern sonic flow. The ground truth datasets are generated by solving the incompress-
ible/compressible NS equations based on the finite volume method (FVM). We use the open-source
FVM library OpenFOAM [20] to conduct all CFD simulations. All flow domains are discretized by
triangle (tri) mesh cells and the mesh resolutions are adaptive to the richness of flow features. The
case setup details can be found in Table 2. Both cylinder flow and sonic flow datasets consist of 51
training and 50 test trajectories, while vascular flow consists of 11 training and 10 test trajectories.
15
Published as a conference paper at ICLR 2022
Table 2: Simulation details of datasets
Dataset	PDES	Cell type	Meshing	#nodes	# steps	δt (sec)
Cylinder flow	Incompr. NS	tri	Fixed 一	1699	400	05
Sonic flow	Compr. NS	Ei	Moving	1900	-40-	5 X 10-7
Vascular flow	Incompr. NS	tri	Small-varying	7561 (avg.)	250	0.5
Table 3: Input-output information of our networks: u, v, p, T, ρ, m denote X-axis velocity, Y-axis
velocity, pressure, temperature, density and cell volume, respectively. x denote the cell coordinates.
T,0 , Re, r denote the initial temperature (sonic flow), Reynolds number (cylinder flow) and radius
of the thrombus (vascular flow).
Dataset	GMR node input	GMR edge input	GMUS node output	Parameter token model input	Nodal embed dim	# selected node
Cylinder flow	Ui,Vi,pi,mi,Re	Xi - Xj, |xi- xj|	Ui,Vi,pi	Re	4	256
Sonic flow	Ui,Vi,pi,Ti ,Pi mi,T,0	xi - Xj, |xi- xj|	Ui,Vi,pi Ti, Pi	T,0	4	256
Vascular flow	Ui,vi,pi,mi,r	xi - Xj, |xi - Xj |	Ui,Vi,pi	r	2	400
Here we summarize all input and outputs of the GMR/GMUS models, as well as the attention simu-
lator for each dataset. GMR encodes the high-dimensional system state onto the node presentations
on pivotal nodes, while GMR decodes latents back to the high-dimensional physical state (Figure
8). The parameter token encoder takes the physical parameter μ as input, and outputs a parameter
token. The attention model takes the parameter token, together with the latent vectors of states at all
previous time steps z, to predict the latents of the next time step. All input and output variables for
each dataset are detailed in Table 3.
The governing equations for all fluid dynamic cases can be summarized as follows,
∂ + ▽ ∙ (Pv) = 0,
竽 + v∙ (Pvv) = v∙ (μVv)-Vp + V∙ (μ(Vv)T) - 2V(μV∙ V),
(14)
∂
∂t (PCpT) + v∙ (PCpvT)
v∙ (kVT)+PTDDP+Dp
-3μψ + μφ,
where v is the velocity vector, μ is the viscosity, k is the thermal conductivity, Cp is the specific
heat of the fluid and the definitions of the stream scalar Ψ, the potential scalar Φ are referred to the
chapter 3 in [29]. Only the sonic flow (compressible flow) involves solving the third equation.
For cylinder flow, the simulation mesh topology remains the same for all trajectories, and the
Reynolds number varies. In sonic flow, topology also remains fixed; however, the mesh node coor-
dinates change over the course of the simulation to simulate the moving ramp. Here, we vary the
initial temperature. Finally, for vascular flow, we vary the radius of the thrombus, which means that
each trajectory will have a different mesh topology, and the simulation model should be able to work
with variable-sized inputs in this case.
A.5 Model details
Node and edge representations in our GraphNets are vectors of width 128. The node and edge
functions (mlpv, mlpe, mlpr) are MLPs with two hidden layers of size 128, and ReLU activation.
The input dimension of mlpv is based on the number of input features in each node (see details in
table 3), and the input dimension of mlpe is three due to the one-layer neighboring edge. We use
L = 3 GraphNet blocks for both GMR and GMUS. The node and edge functions mlp`e and mlp`v
used in the GraphNet blocks are 3-Layer, ReLU-activated MLPs with hidden size of 128. The output
16
Published as a conference paper at ICLR 2022
(d)	(e)
(f)
Figure 11: Sonic flow system: (a-c) all cells in the FV mesh for 3 different time steps; (d-f) pivotal
nodes for 3 different time steps.
size is 128 for the all MLPs, except for the final layer, which is of size 4 for Cylinder, Sonic and 2
for Vascular.
We use a single layer and four attention heads in our transformer model. The embedding sizes of z
for each dataset (cylinder flow, sonic flow, and vascular flow) are 1024, 1024, and 800, respectively.
This is calculated by #pivotal nodes × nout , in which nout is the output size of the last GraphNet
block. The mlpp used to encode the system parameter μ is a MLP With 2 hidden layers of size 100,
and with output dimensions that match z, i.e. 1024, 1024, and 800 for Cylinder Flow, Sonic Flow,
and Vascular FloW, respectively.
Table 4: Number of parameters for each model
DataSet	MeshGraphNet	GRU	LSTM	Transformer	GMR-GMU
Cylinder flow	2.2M	9.4M	11.5M	14.1M	1.2M
Sonic flow	2.2M	9.4M	11.5M	14.1M	1.2M
Vascular flow	2.2M	5.8M	7.0M	8.5M	1.2M 一
A.6 Pivotal points selection
In general, pivotal nodes are selected by uniformly sampling from the entire mesh, as it preserves
the mesh density distribution, Which is designed based on the floW physics observed in training data.
For our datasets, We select 256 pivotal nodes out of 1699 cells for cylinder floW, 256 pivotal nodes
out of 1900 cells for sonic floW, and 400 pivotal nodes out of 7561 cells for vascular floW. The
pivotal nodes in vascular floW are manually reduced in the aneurysm region, Where floW features
are not rich. The pivotal nodes distributions for the three floW systems are shoWn in Figures 10, 11,
and 12, respectively.
17
Published as a conference paper at ICLR 2022
(b)
(a)
(d)
(c)
(e)	(f)
Figure 12: Vascular flow system: all cells in the FV mesh (left), pivotal nodes (right), r = 0.3 (a-b),
r = 0.4 (c-d), r = 0.5 (e-f).
18
Published as a conference paper at ICLR 2022
Table 5: Report of data compressing: QoI data represent original data of the variables of interest;
Embedding represents the saved embedding vectors from the GMR (encoder); Decoder is the saved
GMUS model which is used to decode the embeddings; Compression Ratio is the ratio of QoI data
size to the sum of the embedding and decoder sizes. The RMSE is the relative mean square error of
the reconstructed data from embedding.
Dataset	QoI Data	Embedding	Decoder	Compression Ratio	RMSE(X 10-3)
Cylinder flow	3570 MB	-82.1MB-	12.8MB	37	143
Sonic flow	651MB	-8.5MB-	18 MB	25	Tn
Vascular flow	1951MB	8.5MB —	49.3 MB	33	10
A.7 Data Compression
The Graph Mesh Reducer (GMR) and Graph Mesh Up-Sampling (GMUS) can be treated as an
encoder and decoder for compressing high-dimensional physical data on irregular unstructured
meshes. For any given mesh data (G, Y ), we can encode it into a latent vector z = GMR(G, Y )
through GMR. Therefore, a high-dimensional spatiotemporal trajectory (G, Y0, . . . , YT ) can be
compressed into a low-dimensional representation (G, z0, . . . , zT), which significantly reduces
memory for storage. Similar to other compressed sensing techniques, the original data can be re-
stored by GMUS from latent vectors. Table 5 shows that the decoder can significantly reduce the
size of the original data to about 3% with a slight loss of accuracy after being restored by the de-
coder. Note that only the GMR/GMUS are included in the data compressing process, the transformer
dynamics model is not involved.
A.8 Inference time
Our ground truth solver uses the finite volume method to solve the governing PDEs. Specifically,
the ”pressure implicit with splitting of operators” (PISO) algorithm and ”semi-implicit method for
pressure linked equations” (SIMPLE) [29] are used to simulate the unsteady cylinder and vascular
flows, and a PISO-based compressible solver [26] is used to simulate the high-speed sonic flow. All
these traditional numerical methods involve a large number of iterations and time integration steps.
In contrast, the trained neural network model is able to make predictions without the need of sub-
timestepping. Hence, these predictions can be are significantly faster than running the ground-truth
simulator. The online evaluation of our learned model at inference time only involves two steps,
Step 1: Zi = trans(z-ι, z0), Zt = trans(z-ι, zo, Zι,..., Zt-ι)
Step 2: Yt = GMUS(Zt), t =1,...,T
(15)
which are completely decoupled. We compare the evaluation cost of the learned model with the
FV-based numerical models in Table 6, and observe significant speedups for all three datasets.
Table 6: Wall-clock time of finite volume (FV) model and attention-GMUS to simulate a trajectory
for three flow cases.
Dataset	GMUS	Transformer	GT Model	Speed-up
Cylinder flow (for 400 time steps)	2sec	0.4758sec	1688.59sec	-682-
Sonic flow (for 40 time steps)	0.2Sec	0.047sec —	25.026sec	100
Vascular flow (for 250 time steps)	2.63sec	0.31sec	2451sec	800
A.9 Other variable contour
Figure 13 shows the pressure contours.
19
Published as a conference paper at ICLR 2022
Re = 307 (Cylinder Flow)
Re = 993 (Cylinder Flow)
t=0	t=85	t=715	t=298	t=0	t=85	t=7T5	t=298
61UISJnO
T(0) = 201 (Sonic Flow)
T(0) = 299 (Sonic Flow)
SSH Sjno
R = 0.31 (Vascular flow)
t=40	t=80 t= 160	t=250
R = 0.49 (Vascular flow)
t=40	t=80 t= 160	t=250
-s
n
Figure 13: The pressure contours of rollouts predicted by our model versus the ground truth (CFD).
With different condition settings, the model accurately predict long sequences of rollouts.

A.10 Further analysis of latent encodings and attention
Figure 14 analyzes the latent encoding and attention for the sonic flows in a similar vein to section 4.
Different initial temperatures (as the system parameter) lead to fast diverging of state dynamics, and
thus the parameter tokens overlap for different parameters. However, the attentions to z-1 also
rapidly decay, demonstrating that the transformer can adjust the attention distribution to capture
most physically useful signals.
-1	-0.5	0
C0
(a)
-0.5	0
1
5
.
0
0
0	20 40
C0	Time Step
(b)	(c)
Figure 14: 2-D principle subspace of the embedded vectors from GMR (a) and PCA (b) for the high-
speed sonic flow system: T(0) = 201 trajectory (——>),T(0) = 299 trajectory (——>)start from their
parameter tokens —<— and —•—, respectively. (c) Attention values of the parameter tokens versus
time, T(0) = 201 (—), and T(0) = 299 (---).
Figure 15 shows the overview of attention value distribution in a time-time diagram for all three fluid
systems. The attentions of the state to that at different time step is plotted as a log-scale contour.
Note that the sum of attention values in the same row equals to 1 due to the softmax layer (Eqn. 9).
20
Published as a conference paper at ICLR 2022
Figure 15: Examples of attention value (log value contour) for (a) flow past cylinder, (b) sonic flow
and (c) vascular flow.
Table 7: The average relative rollout error for the cylinder flow, with unit of ×10-3. We compare
MeshGraphNet with noise injection (NI) to our model with transformer. We train the model on the
training trajectories with 400 steps, and test it on the unseen trajectories with 800 steps.
rollout step	800
Variable	Uvp
MeshGraphNet-NI 43 1274 258^
Ours-Transformer 6^^158^^48-
A.11 Longer rollouts
We perform an additional experiment to test how the model performs when rolling out of the training
range. We train a model for the cylinder flow setup (n=400 steps), and evaluate it for lengths of 2n.
To limit the memory footprint for very long rollouts, we use a sliding window of n for the attention,
i.e., we always attend to the last n time steps. This way allow us to retain the same complexity for
the training. Fig. 16 illustrates this procedure: We always use the parameter embedding token z-ι
as the first entry of the attention sequence, followed by a moving window of the n last timesteps.
Table 7 compares the performance of the proposed model with MeshGraphNet on a longer rollout
length (2n = 2 X 400 steps). We find that the proposed model still outperforms the baseline for all
u, v and p trajectories.
z^-1
Zt—n+1
1~∙√
z t-3
Zt -2
1~∙√
Zt -1
I
Figure 16: Sliding window for attention mechanism on latent representations. The parameter em-
bedding token z-ι is always included in the first entry.
A.12 Prediction on convection-diffusion flow
Many of the systems we study in the main paper have oscillatory behaviors, which are hard to
capture by next-step models stably. To further demonstrate the capability of the model on predicting
non-oscillatory dynamics, we add an additional example: a convection-diffusion flow, governed by
dC + ▽• (Vc)= V2( P| 力	(16)
where c is the concentration of the transport scalar, v = [1, 1] is the convection velocity, andPe is the
Peclet number, which controls the ratio of the advective transport rate to its diffusive transport rate.
21
Published as a conference paper at ICLR 2022
As shown in Figure 17, the model can accurately predict the non-oscillatory sequences of rollouts
Quantitatively, our model (RMSE = 1.03 × 10-3) still outperforms MeshGraphNet (RMSE =
2.79 × 10-3).
A.13 Cardiac flow prediction
We test the proposed model on cardiac flows (pulsatile flows) with varying viscosity in multiple
cardiac cycles, which is more realistic for cardiovascular systems. An idealized spatial-temporal
inflow condition is defined as
0.6(x - xmin)(x - xmax)	2πt 2
v(x,t) =------7------------r2	(sin( )) +0.15	(17)
(xmax - xmin )	1.8
to simulate the pulsating inflow from the heart. The model can accurately predict 10 cardiac cycles
(Figure 18). The comparison with MeshGraphNet is listed in Table 8, showing significantly better
performance for our model.
Table 8: The average relative rollout error for cardiac flow, with unit of ×10-3.
rollout step	300
Variable	Uvp
MeshGraPhNet-NI 194 71W^
Ours-Transformer 4.7 2.3 106
Figure 18: The velocity contours of rollouts predicted by our model versus the ground truth (CFD)
for a cardiac flow.
22