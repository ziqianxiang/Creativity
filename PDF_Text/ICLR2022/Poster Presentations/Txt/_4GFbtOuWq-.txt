Published as a conference paper at ICLR 2022
Capacity of Group-invariant Linear Readouts
from Equivariant Representations: How Many
Objects can be Linearly Classified Under All
Possible Views ?
Matthew Farrell* ^, Blake Bordelon* ^, Shubhendu Trivedit, & Cengiz Pehlevan^
^ Harvard University
{msfarrell,blake_bordelon,Cpehlevan} @seas.harvard.edu
t Massachusetts Institute of Technology
shubhendu@csail.mit.edu
Ab stract
Equivariance has emerged as a desirable property of representations of objects
subject to identity-preserving transformations that constitute a group, such as
translations and rotations. However, the expressivity of a representation con-
strained by group equivariance is still not fully understood. We address this gap
by providing a generalization of Cover’s Function Counting Theorem that quanti-
fies the number of linearly separable and group-invariant binary dichotomies that
can be assigned to equivariant representations of objects. We find that the frac-
tion of separable dichotomies is determined by the dimension of the space that is
fixed by the group action. We show how this relation extends to operations such
as convolutions, element-wise nonlinearities, and global and local pooling. While
other operations do not change the fraction of separable dichotomies, local pool-
ing decreases the fraction, despite being a highly nonlinear operation. Finally, we
test our theory on intermediate representations of randomly initialized and fully
trained convolutional neural networks and find perfect agreement.
1 Introduction
The ability to robustly categorize objects under conditions and transformations that preserve the
object categories is essential to animal intelligence, and to pursuits of practical importance such as
improving computer vision systems. However, for general-purpose understanding and geometric
reasoning, invariant representations of these objects in sensory processing circuits are not enough.
Perceptual representations must also accurately encode their transformation properties.
One such property is that of exhibiting equivariance to transformations of the object. When such
transformations are restricted to be an algebraic group, the resulting equivariant representations have
found significant success in machine learning starting with classical convolutional neural networks
(CNNs) (Denker et al., 1989; LeCun et al., 1989) and recently being generalized by the influen-
tial work of Cohen & Welling (2016). Such representations have elicited burgeoning interest as
they capture many transformations of practical interest such as translations, permutations, rotations,
and reflections. Furthermore, equivariance to these transformations can be easily “hard-coded” into
neural networks. Indeed, a new breed of CNN architectures that explicitly account for such transfor-
mations are seeing diverse and rapidly growing applications (Townshend et al., 2021; Baek et al.,
2021; Satorras et al., 2021; Anderson et al., 2019; Bogatskiy et al., 2020; Klicpera et al., 2020;
Winkels & Cohen, 2019; Gordon et al., 2020; Sosnovik et al., 2021; Eismann et al., 2020). In ad-
dition, equivariant CNNs have been shown to capture response properties of neurons in the primary
visual cortex beyond classical Gabor filter models (Ecker et al., 2θl8).
* These authors contributed equally.
1
Published as a conference paper at ICLR 2022
While itis clear that equivariance imposes a strong constraint on the geometry of representations and
thus of perceptual manifolds (Seung & Lee, 2000; DiCarlo & Cox, 2007) that are carved out by these
representations as the objects transform, the implications of such constraints on their expressivity
are not well understood. In this work we take a step towards addressing this gap. Our starting
point is the classical notion of the perceptron capacity (sometimes also known as the fractional
memory/Storage capacity) - a quantity fundamental to the task of object categorization and closely
related to VC dimension (Vapnik & Chervonenkis, 1968). Defined as the maximum number of
points for which all (or 1-δ fraction of) possible binary label assignments (i.e. dichotomies) afford
a hyperplane that separates points with one label from the points with the other, it can be seen to
offer a quantification of the expressivity of a representation.
Classical work on perceptron capacity focused on points in general position (Wendel, 1962; Cover,
1965; Schlafli, 1950; Gardner, 1987; 1988). However, understanding the perceptron capacity when
the inputs are not merely points, but are endowed with richer structure, has only recently started to
attract attention. For instance, work by Chung et al. (2018); Pastore et al. (2020); Rotondo et al.
(2020); Cohen et al. (2020) considered general perceptual manifolds and examined the role of their
geometry to obtain extensions to the perceptron capacity results. However, such work crucially
relied on the assumption that each manifold is oriented randomly, a condition which is strongly
violated by equivariant representations.
With these motivations, our particular contributions in this paper are the following:
•	We extend Cover’s function counting theorem and VC dimension to equivariant representations,
finding that both scale with the dimension of the subspace fixed by the group action.
•	We demonstrate the applicability of our result to G-convolutional network layers, including pool-
ing layers, through theory and verify through simulation.
1.1	Related works
Work most related to ours falls along two major axes. The first follows the classical perceptron
capacity result on the linear separability of points (Schlafli, 1950; Wendel, 1962; Cover, 1965;
Gardner, 1987; 1988). This result initiated a long history of investigation in theoretical neuroscience,
e.g. (Brunel et al., 2004; Chapeton et al., 2012; Rigotti et al., 2013; Brunel, 2016; Rubin et al.,
2017; Pehlevan & Sengupta, 2017), where it is used to understand the memory capacity of neuronal
architectures. Similarly, in machine learning, the perceptron capacity and its variants, including
notions for multilayer perceptrons, have been fundamental to a fruitful line of study in the context of
finite sample expressivity and generalization (Baum, 1988; Kowalczyk, 1997; Sontag, 1997; Huang,
2003; Yun et al., 2019; Vershynin, 2020). Work closest in spirit to ours comes from theoretical
neuroscience and statistical physics (Chung et al., 2018; Pastore et al., 2020; Rotondo et al., 2020;
Cohen et al., 2020), which considered general perceptual manifolds, albeit oriented randomly, and
examined the role of their geometry to obtain extensions to the perceptron capacity result.
The second line of relevant literature is that on group-equivariant convolutional neural networks
(GCNNs). The main inspiration for such networks comes from the spectacular success of classical
CNNs (LeCun et al., 1989) which directly built in translational symmetry into the network architec-
ture. In particular, the internal representations of a CNN are approximately1 translation equivariant:
if the input image is translated by an amount t, the feature map of each internal layer is translated
by the same amount. Furthermore, an invariant read-out on top ensures that a CNN is translation
invariant. Cohen & Welling (2016) observed that a viable approach to generalize CNNs to other
data types could involve considering equivariance to more general transformation groups. This idea
has been used to construct networks equivariant to a wide variety of transformations such as pla-
nar rotations (Worrall et al., 2017; Weiler et al., 2018b; Bekkers et al., 2018; Veeling et al., 2018;
Smets et al., 2020), 3D rotations (Cohen et al., 2018; Esteves et al., 2018; Worrall & Brostow, 2018;
Weiler et al., 2018a; Kondor et al., 2018a; Perraudin et al., 2019), permutations (Zaheer et al., 2017;
Hartford et al., 2018; Kondor et al., 2018b; Maron et al., 2019a; 2020), general Euclidean isome-
tries (Weiler et al., 2018a; Weiler & Cesa, 2019; Finzi et al., 2020), scaling (Marcos et al., 2018;
Worrall & Welling, 2019; Sosnovik et al., 2020) and more exotic symmetries (Bogatskiy et al., 2020;
Shutty & Wierzynski, 2020; Finzi et al., 2021) etc.
1Some operations such as max pooling and boundary effects of the convolutions technically break strict
equivariance, as well as the final densely connected layers.
2
Published as a conference paper at ICLR 2022
A quite general theory of equivariant/invariant networks has also emerged. Kondor & Trivedi (2018)
gave a complete description of GCNNs for scalar fields on homogeneous spaces of compact groups.
This was generalized further to cover the steerable case in (Cohen et al., 2019b) and to general gauge
fields in (Cohen et al., 2019a; Weiler et al., 2021). This theory also includes universal approxima-
tion results (Yarotsky, 2018; Keriven & Peyre, 2019; Sannai et al., 2019b; Maron et al., 2019b; Segol
& Lipman, 2020; Ravanbakhsh, 2020). Nevertheless, while benefits of equivariance/invariance in
terms of improved sample complexity and ease of training are quoted frequently, a firm theoretical
understanding is still largely missing. Some results however do exist, going back to (Shawe-Taylor,
1991). Abu-Mostafa (1993) made the argument that restricting a classifier to be invariant can not
increase its VC dimension. Sokolic et al. (2017) extend this idea to derive generalization bounds
for invariant classifiers, while Sannai et al. (2019a) do so specifically working with the permutation
group. Elesedy & Zaidi (2021) show a strict generalization benefit for equivariant linear models,
showing that the generalization gap between a least squares model and its equivariant version de-
pends on the dimension of the space of anti-symmetric linear maps. Some benefits of related ideas
such as data augmentation and invariant averaging are formally shown in (Lyle et al., 2020; Chen
et al., 2020). Here we focus on the limits to expressivity enforced by equivariance.
2	Problem formulation
Suppose x abstractly represents an object and let r(x) ∈ RN be some feature map of x to an
N -dimensional space (such as an intermediate layer of a deep neural network). We consider trans-
formations of this object, such that they form a group in the algebraic sense of the word. We denote
the abstract transformation of x by element g ∈ G as gx. Groups G may be represented by invert-
ible matrices, which act on a vector space V (which themselves form the group GL(V ) of invertible
linear transformations on V ). We are interested in feature maps r which satisfy the following group
equivariance condition:
r(gx) = π(g)r(x),
where π : G → GL(RN ) is a linear representation of G which acts on feature map r(x). Note
that many representations of G are possible, including the trivial representation: π(g) = I for all g.
We are interested in perceptual object manifolds generated by the actions of G. Each of the P
manifolds can be written as a set of points {∏(g)rμ : g ∈ G} where μ ∈ [P] ≡ {1, 2,..., P}; that
is, these manifolds are orbits of the point rμ ≡ r(xμ) under the action of ∏. We will refer to such
manifolds as π-manifolds.2
Each of these π-manifolds represents a single object under the transformation encoded by π; hence,
each of the points in a π-manifold is assigned the same class label. A perceptron endowed with a
set of linear readout weights w will attempt to determine the correct class of every point in every
manifold. The condition for realizing (i.e. linearly separating) the dichotomy {yμ}μ can be written
as yμw>π(g)rμ > 0 for all g ∈ G and μ ∈ [P], where yμ = +1 if the μth manifold belongs to the
first class and yμ = -1 if the μth manifold belongs to the second class. The perceptron capacity is
the fraction of dichotomies that can be linearly separated; that is, separated by a hyperplane.
For concreteness, one might imagine that each of the rμ is the neural representation for an image of
a dog (if yμ = +1) or of a cat (if yμ = -1). The action ∏(g) could, for instance, correspond to the
image shifting to the left or right, where the size of the shift is given by g . Different representations
of even the same group can have different coding properties, an important point for investigating
biological circuits and one that we leverage to construct a new GCNN architecture in Section 5.
3	Perceptron Capacity of Group- Generated Manifolds
Here we first state and prove our results in the general case of representations of compact groups over
RN. The reader is encouraged to read the proofs, as they are relatively simple and provide valuable
intuition. We consider applications to specific group representations and GCNNs in Sections 4 and
5 that follow.
2Note that for a finite group G, each manifold will consist of a finite set of points. This violates the technical
mathematical definition of “manifold”, but we abuse the definition here for the sake of consistency with related
work (Chung et al., 2018; Cohen et al., 2019b); to be more mathematically precise, one could instead refer to
these “manifolds” as π-orbits.
3
Published as a conference paper at ICLR 2022
3.1	SEPARABILITY OF π-MANIFOLDS
We begin with a lemma which states that classifying the P π-manifolds can be reduced to the
problem of classifying their P centroids. For the rest of Section 3, we let π : G → GL(RN ) be
an arbitrary linear representation of a compact group G.3 We denote the average of π over G with
respect to the Haar measure by h∏(g)ig∈G; for finite G this is simply ∣G∣ Pg∈G ∏(g) where |G| is the
order (i.e. number of elements) of G. For ease of notation we will generally write hπi ≡ hπ(g)ig∈G
when the group G being averaged over is clear.
Lemma 1. A dataset {(∏(g~)rμ ,yμ)} g∈G,μ∈[p ] consisting of P ∏-manifolds with labels yμ is lin-
early separable if and only if the dataset {(h∏)rμ ,yμ)}μ∈[p ] consisting of the P centroids h∏)rμ
with the same labels is linearly separable. Formally,
∃w ∀g ∈ G,μ ∈ [P] : yμw>π(g)rμ > 0 ^⇒ ∃w ∀μ ∈ [P] : yμw> <π>r* > 0.
Proof. The forward implication is obvious: if there exists a w which linearly separates the P mani-
folds according to an assignment of labels yμ, that same W must necessarily separate the centroids
of these manifolds. This can be seen by averaging each of the quantities yμw>∏(g)rμ over g ∈ G.
Since each of these quantities is positive, the average must be positive.
For the reverse implication, suppose yμw> (∏)rμ > 0, and define W = h∏i>w. We will show that
W separates the P ∏-manfolds since
yμW>∏(g)rμ = yμw> h∏i∏(g)r* (Definition of W)
=yμw> h∏(g0)∏(g)igo∈Grμ (Definition of g and linearity of ∏(g))
=yμw> h∏irμ (Invariance of the Haar Measure μ(Sg) = μ(S) for set S)
> 0 (Assumption that W separates centroids)
Thus, all that is required to show that W separates the ∏-orbits are basic properties of a group
representation and invariance of the Haar measure to G-transformations.	□
3.2	Relationship with Cover’s Theorem and VC dimension
The fraction f of linearly separable dichotomies on a dataset of size P for datapoints in general
position 4 in N dimensions was computed by Cover (1965) and takes the form
f(P,N)=21-PNX-1Pk-1	(1)
where we take mn = 0 for m > n. Taking α = P/N, f is a nonincreasing sigmoidal func-
tion of α that takes the value 1 when α ≤ 1, 1/2 when α = 2, and approaches 0 as α → ∞.
In the thermodynamic limit of P, N → ∞ and α = O(1), f(α) converges to a step function
f(α) = limp,n→∞,α=p∕N f (P,N) = Θ(2 - α) (Gardner, 1987; 1988; Shcherbina & Tirozzi,
2003), indicating that in this limit all dichotomies are realized if α < 2 and no dichotomies can be
realized if α > 2, making αc = 2 a critical point in this limit.
The VC dimension (Vapnik & Chervonenkis, 1968) of the perceptron trained on points in dimension
N is defined to be the largest number of points P such that all dichotomies are linearly separable
for some choice of the P points. These points can be taken to be in general position.5 Because
f(P, N) = 1 precisely when P ≤ N, the VC dimension is always N = P for any finite N and
P (see Abu-Mostafa et al. (2012)), even while the asymptotics of f reveal that f(P, N) approaches
1 at any N > P/2 as N becomes large. In this sense the perceptron capacity yields strictly more
information than the VC dimension, and becomes comparatively more descriptive of the expressivity
as N becomes large.
In our G-invariant storage problem, the relevant scale is α = P/N0 where N0 is the dimension of
the fixed point subspace (defined below). We are now ready to state and prove our main theorem.
3 Note that our results extend to more general vector spaces than RN, under the condition that the group be
semi-simple.
4A set of P points is in general position in N -space if every subset of N or fewer points is linearly
independent. This says that the points are “generic” in the sense that there aren’t any prescribed special linear
relationships between them beyond lying in an N -dimensional space. Points drawn from a Gaussian distribution
with full-rank covariance are in general position with probability one.
5This is because linear dependencies decrease the number of separable dichotomies (see Hertz et al. (2018)).
4
Published as a conference paper at ICLR 2022
Theorem 1. Suppose the points h∏)rμ for μ ∈ [P] lie in general position in the subspace Vo =
range(hπi) = {hπix : x ∈ V}. Then V0 is the fixed point subspace of π, and the fraction of
linearly separable dichotomies on the P π-manifolds {π(g)rμ : g ∈ G} is f (P, No), where No 二
dim V0. Equivalently, N0 is the number of trivial irreducible representations that appear in the
decomposition of π into irreducible representations.
The fixed point subspace is the subspace W = {w ∈ V|gw = w ,∀g ∈ G}. The theorem and its
proof use the notion of irreducible representations (irreps), which are in essence the fundamental
building blocks for general representations. Concretely, any representation of a compact group over
a real vector space decomposes into a direct sum of irreps (Naimark & Stern, 1982; Bump, 2004).
A representation π : G → GL(V) is irreducible if V is not 0 and if no vector subspace of V is
stable under G, other than 0 and V (which are always stable under G). A subspace W being stable
(or invariant) under G means that π(g)w ∈ W for all w ∈ W and g ∈ G. The condition that the
points (∏)rμ be in general position essentially means that there is no prescribed special relationship
between the rμ and between the rμ andhn). Taking the rμ to be drawn from a full-rank Gaussian
distribution is sufficient to satisfy this condition.
Proof. By the theorem of complete reducibility (see Fulton & Harris (2004)), π admits a decomposi-
tion into a direct sum of irreps ∏ = ∏% ㊉ ∏k2㊉...㊉∏kM acting on vector space V = V1 ㊉ V2 ㊉...VM,
where = denotes equality UP to similarity transformation. The indices kj indicate the type of irrep
corresponding to invariant subspace Vj . The fixed point subspace Vo is the direct sum of subspaces
where trivial kj = 0 irreps act: Vo = Ln:k =o Vn. By the Grand Orthogonality Theorem of irreps
(see Liboff (2004)) all non-trivial irreps average to zero h∏k,j(g))g∈G 8 δk,oδij. Then, the matrix
hπi simply projects the data to Vo . By Lemma 1 the fraction of separable dichotomies on the π-
manifolds is the same as that of their centroids h∏)rμ. Since, by assumption, the P points h∏)rμ lie
in general position in V0, the fraction of separable dichotomies is f (P, dim V0) by Equation 1.	□
Remark: A main idea used in the proof is that only nontrivial irreps average to zero. This will be
illustrated with examples in Section 4 below.
Remark: For finite groups, No can be easily computed by averaging the trace of π, also known as
the character, over G: No = hTr(π(g))ig∈G = Tr(hπi) (see Serre (2014)).
Remark: If the perceptron readout has a bias term b i.e. the output of the perceptron is w>π(g)r+b,
then this can be thought of as adding an invariant dimension. This is because the output can be
written W>π(g)r where W = (w, b), r = (r, 1), and ∏(g) = π(g)㊉ 1 is π with an extra row
and column added with a 1 in the last position and zeros everywhere else. Hence the fraction of
separable dichotomies is f(P, No + 1).
These results extend immediately to a notion of VC-dimension of group-invariant perceptrons.
Corollary 1. Let the VC dimension for G-invariant perceptrons with representation π, NVπC,
denote the maximum number P, such that there exist P anchor points {rμ}p=1 so that
{(∏(g)rμ, yμ)}μ∈[p ],g∈G is separablefor all possible binary dichotomies {yμ}μ∈[p ]. Then NVC =
dim(Vo).
Proof. By theorem 1 and Equation 1, all possible dichotomies are realizable for P ≤ dim(Vo)
provided the points h∏) rμ are in general position.	□
Our theory also extends immediately to subgroups. For example, strided convolutions are equivari-
ant to subgroups of the regular representation.
Corollary 2. A solution to the the G-invariant classification problem necessarily solves the G0 -
invariant problem where G0 is a subgroup of G.
Proof. Assume that yμw>r(gxμ) > 0 for g ∈ G. G0 ⊆ G =⇒ yμw>r(gxμ) > 0 ∀g ∈ G.	□
Consequently, the G0-invariant capacity is always higher than the capacity for the G-invariant clas-
sification problem.
5
Published as a conference paper at ICLR 2022
Figure 1: π-manifolds for different π, illustrating that only the fixed point subspace contributes to
capacity. In each panel two manifolds are plotted, with color denoting class label. Stars indicate the
random points rμ for μ ∈ {1, 2} where the orbits begin, and closed circles denote the other points in
the π-manifold. For (a) and (b) the group being represented is G = Z4 and for (c) G = Z3. (a) Here
∏(g) is the 2 X 2 rotation matrix R(2∏g∕4). The open blue circle denotes the fixed point subspace
{0}. (b) Here ∏(g) is the 3 × 3 block-diagonal matrix with the first 2 × 2 block being R(2∏g∕4)
and second 1 × 1 block being 1. The blue line denotes the fixed point subspace span{(0, 0, 1)}. (c)
Here π(g) is the 3 × 3 matrix that cyclically shifts entries of length-3 vectors by g places. The blue
line denotes the fixed point subspace span{(1, 1, 1)}.
4	EXAMPLE APPLICATION: THE CYCLIC GROUP Zm
In this section we illustrate the theory in the case of the cyclic group G = Zm on m elements. This
group is isomorphic to the group of integers {0, 1, ..., m - 1} under addition modulo m, and this
is the form of the group that we will consider. An example of this group acting on an object is an
image that is shifted pixel-wise to the left and right, with periodic boundaries. In Appendix A.3 we
show an application of our theory to the non-abelian Lie group SO(3).
4.1	Rotation matrices
The 2 × 2 discrete rotation matrices R(θg) ≡
cos(θg)
sin(θg)
- sin(θg)
cos(θg)
g ∈ Zm, are one possible representation of Zm; in this case V
where θg = 2πg∕m and
R2 . This representation is
irreducible and nontrivial, which implies that the dimension of the fixed point subspace is 0 (only
the origin is mapped to itself by R for all g). Hence the fraction of linearly separable dichotomies of
the π-manifolds by Theorem 1 is f(P, 0). This result can be intuitively seen by plotting the orbits,
as in Figure 1a for m = 4. Here it is apparent that it is impossible to linearly separate two or more
manifolds with different class labels, and that the nontrivial irrep R averages to the zero matrix.
The representation can be augmented by appending trivial irreps, defining π : G → GL(RN) by
π(g) = R(θg)㊉ I ≡ R(θg) I where I is an (N 一 2) × (N — 2)-dimensional identity matrix.
The number of trivial irreps is N - 2, so that the capacity is f (P, N - 2). This is illustrated in
Figure 1b for the case N = 3. Here we can also see that the trivial irrep, which acts on the subspace
span{(0, 0, 1)}, is the only irrep in the decomposition ofπ that does not average to zero. This figure
also makes intuitive the result of Lemma 1 that dichotomies are realizable on the π-manifolds if and
only if the dichotomies are realizable on the centroids of the manifolds.
4.2	THE REGULAR REPRESENTATION OF Zm
Suppose π : Zm → GL(V ) is the representation of Zm consisting of the cyclic shift permutation
matrices (this is called the regular representation of Zm). In this case V = Rm and π(g) is the
matrix that cyclically shifts the entries of a length-m vector g places. For instance, if m = 3 and
v = (1, 2, 3) then π(2)v = (2, 3, 1).
In Appendix A.2 we derive the irreps of this representation, which consist of rotation matrices of
different frequencies. There is one copy of the trivial irrep π0(g) ≡ 1 corresponding with the fixed
point subspace span{1m} where 1m is the length-m all-ones vector. Hence the fraction of separable
6
Published as a conference paper at ICLR 2022
dichotomies is f(P, 1). This is illustrated in Figure 1c in the case where m = 3. The average of the
regular representation matrix is h∏i =由lmlɪ,, indicating that h∏i projects data along 1m.
4.3	Direct sums of Regular Representations
For our last example We define a representation using the isomorphism Zm = Zm^ ㊉ Zm? for
m = m1m2 and m1 and m2 coprime6 7. Let π(1) : Zm1 → GL(Rm1 ) and π(2) : Zm2 → GL(Rm2)
be the cyclic shift representations (i.e. the regular representations) of Zm1 and Zm2, respectively.
Consider the representation ∏(1) ㊉ n(2) : Zm → GL(Rm1+m2) defined by (∏ ⑴㊉ ∏(2))(g) ≡ ∏ ⑴(g
mod mi)㊉∏(2)(g mod m2), the block-diagonal matrix with ∏(1)(g mod mi) being the first and
π(2) (g mod m2) the second block.
There are two copies of the trivial representation in the decomposition of ∏(1) ㊉ ∏(2), correspond-
ing to the one-dimensional subspaces span{(1m1 , 0m2 )} and span{(0m1 , 1m2 )}, where 0m1 is the
length-k vector of all zeros. Hence the fraction of separable dichotomies is f (P, 2). This reasoning
extends simply to direct sums of arbitrary length ', yielding a fraction of f (P, ').7 These representa-
tions are used to build and test a novel G-convolutional layer architecture with higher capacity than
standard CNN layers in Section 5.
These representations are analogous to certain formulations of grid cell representations as found in
entorhinal cortex of rats (Hafting et al., 2005), which have desirable qualities in comparison to place
cell representations (Sreenivasan & Fiete, 2011).8 Precisely, a collection {Zmk × Zmk}k of grid cell
modules encodes a large 2-dimensional spatial domain Zm × Zm where m = Qk mk .
5	G-Equivariant Neural Networks
The proposed theory can shed light on the feature spaces induced by G-CNNs. Consider a single
convolutional layer feature map for a finite9 group G with the following activation
ai,k(x) = φ(wi>gk-ix) , gk ∈ G , i ∈ {1, ..., N}	(2)
for some nonlinear function φ. For each filter i, and under certain choices of π , the feature map
ai(x) ∈ R|G| exhibits the equivariance property ai(gkx) = π(gk)ai(x). We will let a(x) ∈ R|G|N
denote a flattened vector for this feature map.
In a traditional periodic convolutional layer applied to inputs of width W and length L, the feature
map of a single filter ai(x) ∈ R|G| is equivariant with the regular representation of the group
G = ZW ×ZL (the representation that cyclically shifts the entries ofW ×L matrices). Here the order
of the group is |G| = WL. Crucially, our theory shows that this representation contributes exactly
one trivial irrep per filter (see Appendix A.4.1). Since the dimension of the entire collection of N
feature maps a(χ) is N|G| for finite groups G, one might naively expect capacity tobe P 〜2N|G|
for large N from Gardner (1987). However, Theorem 1 shows that for G-invariant classification,
only the trivial irreps contribute to the classifier capacity. Since the number of trivial irreps present
in the representation is equal to the number of filters N, we have P 〜2N.
We show in Figure 2 that our prediction for f(P, N) matches that empirically measured by training
logistic regression linear classifiers on the representation. We perform this experiment on both
(a) a random convolutional network and (b) VGG-11 (Simonyan & Zisserman, 2015) pretrained
on CIFAR-10 (Krizhevsky, 2009) by (liukuang, 2017). For these models we vary α by fixing the
number of input samples and varying the number of output channels by simply removing channels
from the output tensor. The convolutions in these networks are modified to have periodic boundary
conditions while keeping the actual filters the same - see Appendix A.4.1 and Figure A.1 for more
information and the result of using non-periodic convolutions, which impact the capacity but not the
overall scaling with N0 .
6Two numbers are coprime if they have no common prime factor.
7Our results do not actually require that the mk be coprime, but rather that none of the mk divide one of
the others. To see this, take fm and fmk, to be m and the mk after being divided by all divisors common among
them. Then Zm =㊉k=1Zm卜 and provided none of the mk are 1, one still gets a fraction of f (P,').
8Place cells are analogous to standard convolutional layers.
9We consider finite groups here for simplicity, but the theory extends to compact Lie groups.
7
Published as a conference paper at ICLR 2022
Other GCNN architectures can have different capacities. For instance, a convolutional layer equiv-
ariant to the direct sum representation of Section 4.3 has double the capacity with P 〜4N (Fig-
ure 2c), since each output channel contributes two trivial irreps. See Appendix A.4.4 for an explicit
construction of such a convolutional layer and a derivation of the capacity.
5.1	Pooling Operations
In convolutional networks, local pooling is typically applied to the feature maps which result from
each convolution layer. In this section, we describe how our theory can be adapted for codes which
contain such pooling operations. We will first assume that π is an N -dimensional representation of
G. Let P(r) : RN → RN/k be a pooling operation which reduces the dimension of the feature map.
The condition that a given dichotomy {yμ} is linearly separable on a pooled code is
∃w ∈ RN/k ∀μ ∈ [P],g ∈ G : yμw>P(π(g)rμ) > 0	(3)
We will first analyze the capacity when P(∙) is a linear function (average pooling) before studying
the more general case of local non-linear pooling on one and two-dimensional signals.
5.1.1	Local Average Pooling
In the case of average pooling, the pooling function P(∙) is a linear map, represented with matrix
P which averages a collection of feature maps over local windows. Using an argument similar to
Lemma 1 (Lemma 2 and Theorem 2 in Appendix A.4.2), we prove that the capacity of a standard
CNN is not changed by local average pooling: for a network with N filters, local average pooling
preserves the one trivial dimension for each of the N filters. Consequently the fraction of separable
dichotomies is f(P, N).
5.1.2	Local Nonlinear Pooling
Often, nonlinear pooling operations are applied to downsample feature maps. For concreteness, we
will focus on one-dimensional signals in this section and relegate the proofs for two-dimensional
signals (images) to Appendix A.4.2. Let r(x) ∈ RN×D represent a feature map with N filters and
length-D signals. Consider a pooling operation P(∙) which maps the D pixels in each feature map
into new vectors of size D/k for some integer k. Note that the pooled code is equivariant to the
subgroup H = ZD/k, in the sense that P (π(h)r) = ρ(h)P(r) for any h ∈ H. The representation
ρ is the regular representation of the subgroup H. We thus decompose G into cosets of size D/k:
g = jh, where j ∈ Zk and h ∈ ZD/k. The condition that a vector w separates the dataset is
∀μ ∈ [P ],j ∈ Zk ,h ∈ H ： yμw>ρ(h)P (π(j )rμ) > 0.	(4)
We see that there are effectively k points belonging to each of the P orbits in the pooled code. Since
P (∙) is nonlinear, the averaging trick utilized in Lemma 1 is no longer available. However, We
can obtain a lower bound on the capacity, f(P, bN/kc), from a simple extension of Cover’s original
proof technique as we show in Appendix A.4.3. Alternatively, an upper bound f ≤ f(P, N) persists
since a W which satisfies Equation 4 must separate hρ(h)ih∈H P(rμ). This upper bound is tight
when all k points hρ(h),h∈H P(∏(j)rμ) coincide for each μ, giving capacity f (P, N). This is what
occurs in the average pooling case where the upper bound f ≤ f(P, N) is tight. Further, if we
are only interested in the H -invariant capacity problem, the fraction of separable dichotomies is
f(P, N), since ρ is a regular representation of H as we show in 4.
5.1.3	Global pooling
Lemma 1 shows that linear separability of π-manifolds is equivalent to linear separability after
global average pooling (i.e. averaging the representation over the group). This is relevant to CNNs,
which typically perform global average pooling following the final convolutional layer, which is
typically justified by a desire to achieve translation invariance. This strategy is efficient: Lemma 1
implies that it allows the learning of optimal linear readout weights following the pooling operation,
given only a single point from each π-manifold (i.e. no need for data augmentation). Global average
pooling reduces the problem of linearly classifying |G|P points in a space of dimension N to that
of linearly classifying P points in a space of dimension dim V0. Note that by an argument similar to
Section 5.1.2, global max pooling may in contrast reduce capacity.
8
Published as a conference paper at ICLR 2022
Figure 2: Capacity of GCNN representations. Solid lines denote the empirically measured fraction
f(α) of 100 random dichotomies for which a logistic regression classifier finds a separating hyper-
plane, where α = P/N0 . Dotted lines denote theoretical predictions. Shaded regions depict 95%
confidence intervals over random choice of inputs, as well as network weights in (a) and (c). (a)
f(α) ofa random periodic convolutional layer after ReLU (blue line) and followed by 2x2 max pool
(orange line), with P = 40 and N0 = # output channels. Max pooling reduces capacity by a factor
between 1/4 and 1 as predicted by our theory. (b) f(α) of VGG-11 pretrained on CIFAR-10 after a
periodic convolution, batchnorm, and ReLU (blue line), followed by a 2x2 maxpool (orange line),
and then another set of convolution, batchnorm, and ReLU (green line), with P = 20 and N0 = #
output channels. Max pooling reduces capacity as predicted. (c) f(α) after a random convolutional
layer equivariant to the direct sum representation of Z10 ㊉ Zg as defined in Section 4.3, with P = 16
and N0 = 2 (# output channels).
5.2	Induced representations
Induced representations are a fundamental ingredient in the construction of general equivariant neu-
ral network architectures (Cohen et al., 2019b). Here we state our result, and relegate a formal
definition of induced representations and the proof of the result to Appendix A.4.5.
Proposition 1. Let π be a representation of a finite group induced from ρ. Then the fraction of
separable dichotomies of π-manifolds is equal to that of the ρ-manifolds.
6	Discussion and Conclusion
Equivariance has emerged as a powerful framework to build and understand representations that re-
flect the structure of the world in useful ways. In this work we take the natural step of quantifying
the expressivity of these representations through the well-established formalism of perceptron ca-
pacity. We find that the number of “degrees of freedom” available for solving the classification task
is the dimension of the space that is fixed by the group action. This has the immediate implication
that capacity scales with the number of output channels in standard CNN layers, a fact we illustrate
in simulations. However, our results are also very general, extending to virtually any equivariant
representation of practical interest - in particular, they are immediately applicable to GCNNs built
around more general equivariance relations, as we illustrate with an example ofa GCNN equivariant
to direct sum representations. We also calculate the capacity of induced representations, a standard
tool in building GCNNs, and show how local and global pooling operations influence capacity. The
measures of expressivity explored here could prove valuable for ensuring that models with general
equivariance relations have high enough capacity to support the tasks being learned, either by using
more filters or by using representations with intrinsically higher capacity. We leave this to future
work.
While the concept of perceptron capacity has played an essential role in the development of machine
learning systems (Scholkopf & Smola, 2002; Cohen et al., 2020) and the understanding of biolog-
ical circuit computations (Brunel et al., 2004; Chapeton et al., 2012; Rigotti et al., 2013; Brunel,
2016; Rubin et al., 2017; Pehlevan & Sengupta, 2017; Lanore et al., 2021; Froudarakis et al., 2020),
more work is wanting in linking it to other computational attributes of interest such as generaliza-
tion. A comprehensive picture of the computational attributes of equivariant or otherwise structured
representations of artificial and biological learning systems will likely combine multiple measures,
including perceptron capacity. There is much opportunity, and indeed already significant recent
work (e.g. Sokolic et al. (2017)).
9
Published as a conference paper at ICLR 2022
Reproducibility S tatement
Code for the simulations can be found at https://github.com/msf235/
group- invariant-perceptron-capacity. This code includes an environment.yml
file that can be used to create a python environment identical to the one used by the authors. The
code generates all of the plots in the paper.
While the main text is self-contained with the essential proofs, proofs of additional results and
further discussion can be found in the Appendices below. These include
Appendix A.1 a glossary of definitions and notation.
Appendix A.2 a derivation of the irreps for the regular representation of the cyclic group Zm .
Appendix A.3 a description of the irreps for the group SO(3) and the resulting capacity.
Appendix A.4 a description of the construction of the GCNNs used in the paper, the methods for
empirically measuring the fraction of linearly separable dichotomies, a description of local
pooling, and complete formal proofs of the fraction of linearly separable dichotomies for
these network representations (including with local pooling). This appendix also includes
more description of the induced representation and a formal proof of the fraction of linearly
separable dichotomies.
Appendix A.4.1 also contains an additional figure, Figure A.1.
Acknowledgements
MF and CP are supported by the Harvard Data Science Initiative. MF thanks the Swartz Foundation
for support. BB acknowledges the support of the NSF-Simons Center for Mathematical and Statis-
tical Analysis of Biology at Harvard (award #1764269) and the Harvard Q-Bio Initiative. ST was
partially supported by the NSF under grant No. DMS-1439786.
10
Published as a conference paper at ICLR 2022
References
Yaser S. Abu-Mostafa. Hints and the VC Dimension. Neural Computation, 5(2):278-288, 03 1993.
ISSN 0899-7667. doi: 10.1162/neco.1993.5.2.278. URL https://doi.org/10.1162/
neco.1993.5.2.278.
Yaser S Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin. Learning from data, volume 4.
AMLBook New York, NY, USA:, 2012.
Brandon Anderson, Truong Son Hy, and Risi Kondor. Cormorant: Covariant molecular neural
networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlche-Buc, E. Fox, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.,
2019.
Minkyung Baek, Frank Dimaio, Ivan V. Anishchenko, Justas Dauparas, Sergey Ovchinnikov,
Gyu Rie Lee, JUe Wang, Qian Cong, Lisa N. Kinch, R. Dustin Schaeffer, Claudia Millan, Hahn-
beom Park, Carson Adams, Caleb R. Glassman, Andy M. DeGiovanni, Jose H. Pereira, An-
dria V. Rodrigues, Alberdina Aike van Dijk, Ana C Ebrecht, Diederik Johannes Opperman, Theo
Sagmeister, Christoph Buhlheller, Tea Pavkov-Keller, Manoj K. Rathinaswamy, Udit Dalwadi,
Calvin K. Yip, John E. Burke, K. Christopher Garcia, Nick V. Grishin, Paul D. Adams, Randy J.
Read, and David Baker. Accurate prediction of protein structures and interactions using a three-
track neural network. Science, 373:871 - 876, 2021.
Eric B. Baum. On the capabilities of multilayer perceptrons. J. Complex., 4:193-215, 1988.
Erik J. Bekkers, Maxime W. Lafarge, Mitko Veta, Koen A. J. Eppenhof, Josien P. W. Pluim, and
Remco Duits. Roto-translation covariant convolutional networks for medical image analysis.
ArXiv, abs/1804.03393, 2018.
Alexander Bogatskiy, Brandon Anderson, Marwah Roussi, David Miller, and Risi Kondor. Lorentz
group equivariant neural network for particle physics. In International Conference on Machine
Learning, pp. 992-1002. PMLR, 2020.
Nicolas Brunel, Vincent Hakim, Philippe Isope, Jean-Pierre Nadal, and Boris Barbour. Optimal
information storage and the distribution of synaptic weights: perceptron versus purkinje cell.
Neuron, 43(5):745-757, 2004.
Nicolas J.-B. Brunel. Is cortical connectivity optimized for storing information? Nature Neuro-
science, 19:749-755, 2016.
Daniel Bump. Lie groups, volume 8. Springer, New York, 2004. ISBN 9781475740943.
Julio Chapeton, Tarec Fares, Darin LaSota, and Armen Stepanyants. Efficient associative memory
storage in cortical circuits of inhibitory and excitatory neurons. Proceedings of the National
Academy of Sciences, 109:E3614 - E3622, 2012.
Shuxiao Chen, Edgar Dobriban, and Jane Lee. A group-theoretic framework for data augmen-
tation. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 21321-21333. Curran As-
sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
f4573fc71c731d5c362f0d7860945b88-Paper.pdf.
SueYeon Chung, Daniel D. Lee, and Haim Sompolinsky. Classification and Geometry of General
Perceptual Manifolds. Physical Review X, 8(3):031003, July 2018. doi: 10.1103/PhysRevX.8.
031003.
Taco Cohen and Max Welling. Group equivariant convolutional networks. In ICML, 2016.
Taco Cohen, Mario Geiger, Jonas Kohler, and Max Welling. Spherical cnns. ArXiv, abs/1801.10130,
2018.
Taco Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max Welling. Gauge equivariant convolu-
tional networks and the icosahedral cnn. In ICML, 2019a.
11
Published as a conference paper at ICLR 2022
Taco S Cohen, Mario Geiger, and Maurice Weiler. A general theory of equivariant CNNs on ho-
mogeneous spaces. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlche-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso-
ciates, Inc., 2019b.
Uri Cohen, SueYeon Chung, Daniel D. Lee, and Haim Sompolinsky. Separability and geometry of
object manifolds in deep neural networks. Nature Communications, 11(1):746, February 2020.
ISSN 2041-1723. doi: 10.1038/s41467-020-14578-5.
T. Cover. Geometrical and statistical properties of systems of linear inequalities with applications in
pattern recognition. IEEE Trans. Electron. ComPut, 14:326-334, 1965.
John Denker, W. Gardner, Hans Graf, Donnie Henderson, R. Howard, W. Hubbard, L. D. Jackel,
Henry Baird, and Isabelle Guyon. Neural network recognizer for hand-written zip code digits.
In D. Touretzky (ed.), Advances in Neural Information Processing Systems, volume 1. Morgan-
Kaufmann, 1989. URL https://proceedings.neurips.cc/paper/1988/file/
a97da629b098b75c294dffdc3e463904- Paper.pdf.
James J DiCarlo and David D Cox. Untangling invariant object recognition. Trends in cognitive
sciences, 11(8):333-341, 2007.
Alexander S Ecker, Fabian H Sinz, Emmanouil Froudarakis, Paul G Fahey, Santiago A Cadena,
Edgar Y Walker, Erick Cobos, Jacob Reimer, Andreas S Tolias, and Matthias Bethge. A
rotation-equivariant convolutional neural network model of primary visual cortex. arXiv PrePrint
arXiv:1809.10504, 2018.
Stephan Eismann, Raphael J. L. Townshend, Nathaniel Thomas, Milind Jagota, Bowen Jing, and
Ron O. Dror. Hierarchical, rotation-equivariant neural networks to select structural models of
protein complexes. Proteins: Structure, 89:493 - 501, 2020.
Bryn Elesedy and Sheheryar Zaidi. Provably strict generalisation benefit for equivariant models.
In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on
Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of
Machine Learning Research, pp. 2959-2969. PMLR, 2021. URL http://proceedings.
mlr.press/v139/elesedy21a.html.
Carlos Esteves, Christine Allen-Blanchette, Ameesh Makadia, and Kostas Daniilidis. Learning so(3)
equivariant representations with spherical cnns. In ECCV, 2018.
Marc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Gordon Wilson. Generalizing convo-
lutional neural networks for equivariance to lie groups on arbitrary continuous data. In ICML,
2020.
Marc Finzi, Max Welling, and Andrew Gordon Wilson. A practical method for constructing equiv-
ariant multilayer perceptrons for arbitrary matrix groups. ArXiv, abs/2104.09459, 2021.
Emmanouil Froudarakis, Uri Cohen, Maria Diamantaki, Edgar Y Walker, Jacob Reimer, Philipp
Berens, Haim Sompolinsky, and Andreas S Tolias. Object manifold geometry across the mouse
cortical visual hierarchy. bioRxiv, 2020.
William Fulton and Joe Harris. RePresentation Theory: A First Course. Readings in Mathematics.
Springer-Verlag New York, New York, 2004. ISBN 9780387974958.
E. Gardner. Maximum storage capacity in neural networks. EPL, 4:481-485, 1987.
E. Gardner. The space of interactions in neural network models. Journal of Physics A, 21:257-270,
1988.
Jonathan Gordon, David Lopez-Paz, Marco Baroni, and Diane Bouchacourt. Permutation equivari-
ant models for compositional generalization in language. In ICLR, 2020.
Torkel Hafting, Marianne Fyhn, Sturla Molden, May-Britt Moser, and Edvard I. Moser. Microstruc-
ture of a spatial map in the entorhinal cortex. Nature, 436(7052):801-806, August 2005. ISSN
0028-0836, 1476-4687. doi: 10.1038/nature03721.
12
Published as a conference paper at ICLR 2022
Jason S. Hartford, Devon R. Graham, Kevin Leyton-Brown, and Siamak Ravanbakhsh. Deep models
of interactions across sets. In ICML, 2018.
John Hertz, Anders Krogh, and Richard G Palmer. Introduction to the theory of neural computation.
CRC Press, 2018.
Guangbin Huang. Learning capability and storage capacity of two-hidden-layer feedforward net-
works. IEEE transactions on neural networks,14 2:274-81, 2003.
Nicolas Keriven and Gabriel Peyre. Universal invariant and equivariant graph neural net-
works. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso-
ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
ea9268cb43f55d1d12380fb6ea5bf572- Paper.pdf.
Johannes Klicpera, Janek Groβ, and Stephan Gunnemann. Directional message passing for molec-
ular graphs. ArXiv, abs/2003.03123, 2020.
Risi Kondor and Shubhendu Trivedi. On the generalization of equivariance and convolution in neural
networks to the action of compact groups. In ICML, 2018.
Risi Kondor, Zhen Lin, and Shubhendu Trivedi. Clebsch-gordan nets: a fully fourier space spherical
convolutional neural network. In NeurIPS, 2018a.
Risi Kondor, Hy Truong Son, Horace Pan, Brandon M. Anderson, and Shubhendu Trivedi. Covariant
compositional networks for learning graphs. ArXiv, abs/1801.02144, 2018b.
A. Kowalczyk. Estimates of storage capacity of multilayer perceptron with threshold logic hidden
units. Neural networks : the official journal of the International Neural Network Society, 10 8:
1417-1433, 1997.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
Frederic Lanore, N. Alex Cayco-Gajic, Harsha Gurnani, Diccon Coyle, and R. Angus Silver. Cere-
bellar granule cell axons support high-dimensional representations. Nature Neuroscience, June
2021. ISSN 1097-6256, 1546-1726. doi: 10.1038/s41593-021-00873-x.
Yann Andre LeCun, Bernhard E. Boser, John S. Denker, Donnie Henderson, Richard E. Howard,
Wayne E. Hubbard, and Lawrence D. Jackel. Backpropagation applied to handwritten zip code
recognition. Neural Computation, 1:541-551, 1989.
Richard L. Liboff. Primer for Point and Space Groups. Undergraduate Texts in Contemporary
Physics. Springer, New York, 2004. ISBN 9780387402482.
liukuang. pytorch-cifar. https://github.com/kuangliu/pytorch-cifar, 2017.
Clare Lyle, Mark van der Wilk, Marta Z. Kwiatkowska, Yarin Gal, and Benjamin Bloem-Reddy. On
the benefits of invariance in neural networks. ArXiv, abs/2005.00178, 2020.
George W. Mackey. Induced Representations of Locally Compact Groups and Applications,
pp. 132-166. Springer Berlin Heidelberg, Berlin, Heidelberg, 1970. ISBN 978-3-642-
48272-4. doi: 10.1007/978-3-642-48272-4_6.	URL https://doi.org/10.1007/
978-3-642-48272-4_6.
Diego Marcos, Benjamin Kellenberger, Sylvain Lobry, and Devis Tuia. Scale equivariance in cnns
with vector fields. ArXiv, abs/1807.11783, 2018.
Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph
networks. ArXiv, abs/1812.09902, 2019a.
Haggai Maron, Ethan Fetaya, Nimrod Segol, and Yaron Lipman. On the universality of invari-
ant networks. In Proceedings of the 36th International Conference on Machine Learning, vol-
ume 97, pp. 4363-4371. PMLR, 2019b. URL https://proceedings.mlr.press/v97/
maron19a.html.
13
Published as a conference paper at ICLR 2022
Haggai Maron, Or Litany, Gal Chechik, and Ethan Fetaya. On learning sets of symmetric elements.
ArXiv, abs/2002.08599, 2020.
Mark Aronovich Naimark and Aleksandr Isaakovich Stern. Theory of group representations, volume
246. Springer, New York, 1982. ISBN 9781461381440.
Mauro Pastore, Pietro Rotondo, Vittorio Erba, and Marco Gherardi. Statistical learning theory of
structured data. Physical Review E: Statistical Physics, Plasmas, Fluids, and Related Interdisci-
plinary Topics, 102(3):032119, September 2020. doi: 10.1103/PhysRevE.102.032119.
Cengiz Pehlevan and Anirvan M. Sengupta. Resource-efficient perceptron has sparse synaptic
weight distribution. 2017 25th Signal Processing and Communications Applications Conference
(SIU),pp.1-4, 2017.
Nathanael Perraudin, Michael Defferrard, Tomasz Kacprzak, and Raphael Sgier. Deepsphere: Effi-
cient spherical convolutional neural network with healpix sampling for cosmological applications.
Astron ComPut, 27:130-146, 2019.
Siamak Ravanbakhsh. Universal equivariant multilayer perceptrons. In Hal DaUme In and Aarti
Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume
119 of Proceedings of Machine Learning Research, pp. 7996-8006. PMLR, 13-18 Jul 2020.
URL https://proceedings.mlr.press/v119/ravanbakhsh20a.html.
Mattia Rigotti, Omri Barak, Melissa R Warden, Xiao-Jing Wang, Nathaniel D Daw, Earl K Miller,
and Stefano Fusi. The importance of mixed selectivity in complex cognitive tasks. Nature, 497
(7451):585-590, 2013.
Pietro Rotondo, Marco Cosentino Lagomarsino, and Marco Gherardi. Counting the learnable func-
tions of geometrically structured data. Physical Review Research, 2(2):023169, May 2020. doi:
10.1103/PhysRevResearch.2.023169.
Ran Rubin, L. F. Abbott, and Haim Sompolinsky. Balanced excitation and inhibition are required
for high-capacity, noise-robust neuronal selectivity. Proceedings of the National Academy of
Sciences, 114:E9366 - E9375, 2017.
Akiyoshi Sannai, M. Imaizumi, and M. Kawano. Improved generalization bounds of group invariant
/ equivariant deep networks via quotient feature spaces. 2019a.
Akiyoshi Sannai, Yuuki Takai, and Matthieu Cordonnier. Universal approximations of permutation
invariant/equivariant functions by deep neural networks. ArXiv, abs/1903.01939, 2019b.
Victor Garcia Satorras, E. Hoogeboom, F. Fuchs, I. Posner, and M. Welling. E(n) equivariant nor-
malizing flows for molecule generation in 3d. ArXiv, abs/2105.09016, 2021.
LUdWig Schlafli. Theorie der vielfachen Kontinuitat, pp. 167-387. Springer Basel, Basel, 1950.
ISBN 978-3-0348-4118-4. doi: 10.1007/978-3-0348-4118-4_13. URL https://doi.org/
10.1007/978-3-0348-4118-4_13.
Bernhard Scholkopf and Alexander J. Smola. Learning with Kernels: Support Vector Machines,
Regularization, OPtimization, and Beyond. Adaptive Computation and Machine Learning. MIT
Press, 2002. ISBN 978-0-262-19475-4.
Nimrod Segol and Yaron Lipman. On universal equivariant set netWorks. ArXiv, abs/1910.02421,
2020.
Jean-Pierre Serre. Linear Representations of Finite Groups. Graduate Texts in Mathematics.
Springer, NeW York, 2014. ISBN 978-1-4684-9460-0 978-1-4684-9458-7.
H Sebastian Seung and Daniel D Lee. The manifold Ways of perception. science, 290(5500):2268-
2269, 2000.
John ShaWe-Taylor. Threshold netWork learning in the presence of equivalences. In NIPS, 1991.
14
Published as a conference paper at ICLR 2022
Mariya Shcherbina and Brunello Tirozzi. Rigorous Solution of the Gardner Problem. Com-
munications in Mathematical Physics, 234(3):383-422, March 2003. ISSN 0010-3616, 1432-
0916. doi: 10.1007/s00220-002-0783-3. URL http://link.springer.com/10.1007/
s00220-002-0783-3.
N. Shutty and Casimir Wierzynski. Learning irreducible representations of noncommutative lie
groups. ArXiv, abs/2006.00724, 2020.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. CoRR, abs/1409.1556, 2015.
Bart Smets, Jim Portegies, Erik J. Bekkers, and Remco Duits. Pde-based group equivariant convo-
lutional neural networks. ArXiv, abs/2001.09046, 2020.
Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel Rodrigues. Generalization Error of In-
variant Classifiers. In Aarti Singh and Jerry Zhu (eds.), Proceedings of the 20th International
Conference on Artificial Intelligence and Statistics, volume 54 of Proceedings of Machine Learn-
ing Research, pp. 1094-1103. PMLR, 20-22 Apr 2017. URL https://proceedings.mlr.
press/v54/sokolic17a.html.
Eduardo Sontag. Shattering all sets of k points in general position requires (k 1)/2 parameters.
Neural Computation, 9:337-348, 1997.
I.	Sosnovik, A. Moskalev, and A. Smeulders. Scale equivariance improves siamese tracking. 2021
IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 2764-2773, 2021.
Ivan Sosnovik, Michal Szmaja, and Arnold W. M. Smeulders. Scale-equivariant steerable networks.
ArXiv, abs/1910.11093, 2020.
Sameet Sreenivasan and Ila Fiete. Grid cells generate an analog error-correcting code for singularly
precise neural computation. Nature Neuroscience, 14(10):1330-1337, October 2011. ISSN 1097-
6256, 1546-1726. doi: 10.1038/nn.2901.
Raphael J. L. Townshend, Stephan Eismann, Andrew M. Watkins, Ramya Rangan, Maria Karelina,
Rhiju Das, and Ron O. Dror. Geometric deep learning of rna structure. Science, 373:1047 - 1051,
2021.
Vladimir N. Vapnik and Alexey Y. Chervonenkis. On the uniform convergence of relative frequen-
cies of events to their probabilities. Dokl. Akad. Nauk., 181(4), 1968.
Bastiaan S. Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equiv-
ariant cnns for digital pathology. ArXiv, abs/1806.03962, 2018.
Roman Vershynin. Memory capacity of neural networks with threshold and rectified linear unit
activations. SIAM J. Math. Data Sci., 2:1004-1033, 2020.
Maurice Weiler and Gabriele Cesa. General e(2)-equivariant steerable cnns. ArXiv, abs/1911.08251,
2019.
Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco Cohen. 3d steerable cnns:
Learning rotationally equivariant features in volumetric data. In NeurIPS, 2018a.
Maurice Weiler, Fred A. Hamprecht, and Martin Storath. Learning steerable filters for rotation
equivariant cnns. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
849-858, 2018b.
Maurice Weiler, Patrick Forre, Erik P. Verlinde, and Max Welling. Coordinate independent ConvolU-
tional networks - isometry and gauge equivariant convolutions on riemannian manifolds. ArXiv,
abs/2106.06020, 2021.
J.	G. Wendel. A problem in geometric probability. Mathematica Scandinavica, 11:109-112, 1962.
Eugene Wigner. Gruppentheorie Und Ihre Anwendung Auf Die Quantenmechanik Der Atomspek-
tren. Vieweg+Teubner Verlag, Wiesbaden, first edition, 1931. ISBN 978-3-663-00642-8.
15
Published as a conference paper at ICLR 2022
Marysia Winkels and Taco Cohen. Pulmonary nodule detection in ct scans with equivariant cnns.
Medical image analysis, 55:15-26, 2019.
Daniel E. Worrall and Gabriel J. Brostow. Cubenet: Equivariance to 3d rotation and translation. In
ECCV, 2018.
Daniel E. Worrall and Max Welling. Deep scale-spaces: Equivariance over scale. ArXiv,
abs/1905.11697, 2019.
Daniel E. Worrall, Stephan J. Garbin, Daniyar Turmukhambetov, and Gabriel J. Brostow. Harmonic
networks: Deep translation and rotation equivariance. 2017 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 7168-7177, 2017.
Dmitry Yarotsky. Universal approximations of invariant maps by neural networks. ArXiv,
abs/1804.10306, 2018.
Chulhee Yun, S. Sra, and A. Jadbabaie. Small relu networks are powerful memorizers: a tight
analysis of memorization capacity. In NeurIPS, 2019.
Manzil Zaheer, SatWik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, and
Alex Smola. Deep sets. In NIPS, 2017.
16
Published as a conference paper at ICLR 2022
A	Appendix
A. 1 Notation and Glossary
•	x: an abstract notation for an input object
•	r(x): a feature map of the input to an N dimensional vector space.
•	π: N dimensional linear representation of group G. For each g ∈ G, π(g) ∈ GL(RN ) is
an N × N invertible real matrix.
•	Equivariance property: r(gx) = π(g)r(x) for all g ∈ G and all x.
•	Invariant measure: a measure μ : G → R+ on G with μ(gS) = μ(S) = μ(Sg). For
finite groups, the uniform distribution. For locally compact topological groups, the Haar
measure.
•	〈%三3：an average over the invariant measure of G
•	Irreducible representation (irrep): an irreducible representation ρ on vector space V satis-
fies ρ(g)v ∈ V for all v ∈ V, g ∈ G.
•	Character χ(g): the trace of the representation χ(g) = Tr π(g).
•	Fixed point subspace: the subspace V0 for which π(g)v ∈ V0 for all v ∈ V0.
•	General position: a collection of P points in general position in an N dimensional vector
space have the property that any subset of k ≤ N points are linearly independent. These
points are generic in the sense that they satisfy no more linear relationships than they must.
•	Dichotomy: a particular binary labeling {yμ} of P points {xμ}.
•	f(P, N): fraction of linearly separable dichotomies given by Cover’s function counting
theorem (Cover, 1965).
•	VC dimension: the largest possible integer P such that there exist P points where all
possible dichotomies {yμ} can be realized by the model Abu-Mostafa et al. (2012).
•	Capacity: the largest possible ratio αc = P/N where P points in general position can be
linearly separated by a N dimensional perceptron with probability 1 in an asymptotic limit
where P, N → ∞ with P/N = ON,P (1). The classical result is αc = 2 (Gardner, 1987;
Shcherbina & Tirozzi, 2003).
•	P(∙): a local pooling operation.
A.2 Irreps for the cyclic group
Here we compute the irreps of representations π : Zm → GL(V ) of the cyclic group Zm over
a real vector space V (see (Serre, 2014) for a derivation of the irreps when V is a complex vec-
tor space). To find the irreps, one can use the form for the eigenvalues and eigenvectors for cir-
culant matrices, since all the π(g) are circulant. This results in the simultaneous diagonalization
π(g) = V (1㊉ R(2πg∕m)㊉ R(4πg∕m)㊉.…㊉ R((m — 1)πg∕m)) V> where V is the real-
valued version of the discrete Fourier transform matrix (the columns are proportional to cosines and
sines of varying frequencies, along with a column proportional to 1m).
Note that the 2x2 rotation matrices R(2πgk∕m) are irreps for k = m/2 and k = 0, since there
is no one-dimensional subspace of R2 that is invariant to R(2∏gk∕m) for all g. The exception
for k = m/2 if m is even, gives R(2πgk∕m) = ( —1)gI, which corresponds to rotation of 180
degrees. The subspace span{(1,0)} is invariant to this action, so that R(2πgk∕m) is not an irrep.
This representation can thus be reduced to an action on a one-dimensional subspace, represented by
(—1)g. The case k = 0 gives the trivial representation.
A.3	SO(3): A Non-Abelian Lie Group
The special orthogonal group SO(3) on 3 dimensions (rotation group), the 3 × 3 orthogonal ma-
trices with determinant +1, can also be analyzed within our theory. G-convolutional neural net-
works that are equivariant to SO(3) rotations have become of high interest in the physical sciences
and computer vision where the objects of interest often respect these symmetries (Anderson et al.,
17
Published as a conference paper at ICLR 2022
2019; Cohen et al., 2018; Esteves et al., 2018; Kondor et al., 2018a) The irreducible representations
have the form Bkm where Bkm are (2km + 1) × (2km + 1) block matrices, known as Wigner D-
matrices (Wigner, 1931). The trivial irreps correspond to the one-dimensional irreps with k = 0.
Thus, the SO(3)-invariant classification capacity merely counts the number of trivial irreps which
have N0 = Pm δkm,0. The capacity is again f (P, N0).
A.4 G-equivariant convolutional layers
A.4. 1 Standard convolutional layers
A convolutional layer consists of a set of N k × k0 filters Fi that are convolved (technically cross-
correlated) with a stack of M W × L input tensors. Here M is the number of input channels and
N the number of output channels. The convolution runs each filter (i.e. takes the dot product at
all possible positions) over each of the W × L input tensors, and the result is averaged across the
M input channels to produce the output of one output channel. In the positions where the filters
approach the edges of the input tensor, different choices can be made about how to handle these
edge conditions. The standard choice is to pad the edges with some number of zeros depending on
the desired shape of the output tensor and run the convolution out to the end of the padded image.
Another possible choice is to loop the edges of the input tensor together, so that the filter is applied to
the other side of the input tensor as it runs off the edge. This periodic boundary condition allows us
to write the convolution formally in terms of group actions, and to apply our theory directly. When
convolutions are not periodic, the resulting capacity increases somewhat but still follows the P/N0
scaling of the periodic convolutions (Figure A.1).
For the random convolutional layers of Figure 2a, the input tensors are size 10 × 10 and the number
of input channels are M = 3, as for standard color images. Each entry of these tensors is normally
distributed with mean 0. The filters are also of size 10 × 10 with periodic boundary conditions,
and are initialized according to a normal Xavier distribution with parameters that are the default for
Pytorch 1.9. The convolution is implemented via the Pytorch 1.9 implementation of Conv2d with
Padding_mode="circular" and Padding=0 in the case of periodic boundary conditions. The bias term
of the convolution is set to zero and the convolution is followed by a ReLU nonlinearity (blue line).
The resulting figures do not change aPPreciably for different choices of inPut tensor size, number of
inPut channels, or size of filters (though note that the nonlinearity is essential for satisfying the gen-
eral Position condition of Theorem 1; otherwise, the caPacity would be determined by the number
of inPut channels rather than number of outPut channels). The outPut of this convolution is then fed
through a 2 × 2 max Pooling layer (orange line in Figure 2a), Provided by Pytorch 1.9’s MaxPool2d.
The Pretrained VGG-11 layers used in Figure 2b and Figure A.1 are taken from liukuang (2017).
The first convolutional block (blue line) consists of 3 × 3 Pretrained filters aPPlied to CIFAR-10
image tensors randomly selected from the validation set and normalized in the same way they are
normalized during training (see liukuang (2017) for details). These images are of size 32 × 32 and
have M = 3 inPut channels, followed by a batch normalization layer in evaluation mode (fixed
Parameters), and then followed by a ReLU nonlinearity. The boundary conditions of these convolu-
tions are set to be Periodic in Figure 2b and nonPeriodic with a zero Padding sizes of 1 in Figure A.1,
and the bias term is set to zero. The batch normalization is an element-wise oPeration and so equiv-
ariant to the representations We consider - thus this operation is not expected and is not observed
to affect the PercePtron caPacity. This convolutional block is then followed by a 2 × 2 max Pooling
layer (orange line). Finally, another convolutional block of 3 × 3 filters, batch normalization, and
ReLU nonlinearity are applied (green line).
The fraction of linearly separable dichotomies is measured empirically by using the scikit-learn
LogisticRegression implementation of logistic regression, With a tolerance value of tol=1e-18 and
an inverse regularization value of C=1e8. The maximum number of iterations is set to 500. An
intercept (i.e. bias) is not used for this fit.
To formally prove that the fraction of separable dichotomies is f (P, N) for standard periodic con-
volutional layers, first note that the convolution is equivariant With respect to cyclic permutation of
the inputs and of the outputs. The representation for cyclically permutating the output tensor can
be Written LkN=1 π Where π is the representation that cyclically permutes the entries of W × L
matrices. Since each copy of π contains one trivial irrep in its decomposition into a direct sum of
18
Published as a conference paper at ICLR 2022
Figure A.1: The fraction of realizable dichotomies of non-periodic convolutional layers is higher
than periodic convolutional layers, but still obeys the same scaling. Details are exactly as in Fig-
ure 2b, but using non-periodic convolutions with a zero padding of size 1. Here the theory line refers
to the theory for periodic convolutions.
irreps, the direct sum LkN=1 π contains N trivial irreps in its decomposition. The final step to use
Theorem 1 is to argue that the centroids of the manifolds are in general position. Since a nonlinearity
(ReLU) is applied to the output of the convolution, and since there is no particular structure in the
convolutional filters beyond possibly sparsity, we can generically expect this to be the case.
A.4.2 Local Pooling
First we prove an extension of Lemma 1 to equivariant linear maps. This will be used to show that
average pooling does not affect the capacity of the regular representation of Zm .
Lemma 2. Let π be a representation of the group G and suppose the matrix P is equivariant with
respect to the restriction ofπ to a subgroup H ⊆ G, so that for all h ∈ H P π(h) = ρ(h)P for some
representation ρ of H. Let R denote a set of representatives of G/H. Then we have the following
equivalence.
∃w ∀μ ∈ [P], g ∈ G : yμw>Pπ(g)rμ > 0
^⇒ ∃w ∀μ ∈ [P] ∀g0 ∈ R : yμw>Ph∏(h)ih∈Hπ(g0)rμ > 0.
Proof. For the forward implication, we write the coset decomposition g = hg0 of g and average
over H to find
∀g ∈ G : yμw>Pπ(g)r* > 0 ^⇒ ∀h ∈ H,g0 ∈ R : yμw>Pπ(h)π(g0)rμ > 0
=⇒ ∀g0 ∈ R : yιwτP(∏(h))h∈Hπ(g0)r" > 0.
For the backward implication, suppose yμw>Phπ(K))h∈H∏(g0)rμ > 0 for all representatives g0 ∈
R, and define W = hρ(h))~>∈Hw. For any g ∈ G, take a coset decomposition g = hg0 for h ∈ H
and g 0 ∈ R. We then have
yμW>Pπ(g)rμ = yμW>Pπ(h)π(g0)rμ (Coset decomposition)
=yμW>ρ(h)Pπ(g0)rμ (P is H-equivariant)
=yμw> hρ(h0)iho∈H ρ(h)P∏(g0)rμ (Definition of W)
=yμw> hρ(h)ih∈H P ∏(g0)rμ (Invariance of measure)
= yμW>P hπ(h)ih∈H π(g0)rμ (P is linear and H-equivariant)
> 0 (By assumption).	(5)
The implication follows.
□
19
Published as a conference paper at ICLR 2022
Lemma 3. For the regular representation of G = ZD, a local average pooling over windows of
size k generates a matrix P which is equivariant with respect to the subgroup H = ZD/k with the
property that
P hπ(h)ih∈H = aP hπ(g)ig∈G	(6)
where a > 0 is a positive constant.
Proof. First, note that the new pooled code is the regular representation of H since shifts of size
mk in the original feature map corresponds to shifts of length m in the pooled code. Thus P is
equivariant to H = ZD/k. Next we note the following two facts
P hπ(h)ih∈H = a01D/k 1D	(7)
P hπ(h)ig∈G = a001D/k 1>D	(8)
a0 and a00 are positive constants and 1D and 1D/k are the D and D/k dimensional vector of all ones,
respectively. Thus, We have that Ph∏(h)ih∈H = aPh∏(g)ig∈G where a is a positive constant. □
Theorem 2. The fraction of linearly separable dichotomies of a CNN pooling layer with N filters
after average pooling from feature maps with the size of the input image W × L to pooled feature
maps of size W/k × L/k is f(P, N), i.e. no capacity is lost due to local average pooling.
Proof. The CNN layer before pooling is a regular representation π of the full group G = ZW × ZL,
applied to each of the M input channels via a direct a sum LjM=1 π. The layer after pooling is a
regular representation ρ of the subgroup H = ZW/k × ZL/k, also applied to the output channels via
a direct sum LjN=1 ρ. Let R be a set of representatives of G/H. Since P is equivariant to π and ρ
over H we have by the previous two lemmas that
∀g ∈ G : yμw>Pπ(g)rμ > 0 ^⇒ ∀g0 ∈ R : yμw>P<π(h)>h∈Hπ(g0)rμ > 0
^⇒ ∀g0 ∈ R : yμw>Ph∏(g)ig∈G∏(g0)rμ > 0
^⇒ yμw>Ph∏(g)ig∈Grμ > 0
^⇒ yμw>Ph∏(h)ih∈Hrμ > 0
^⇒ yμw>hρ(h)ih∈HPrN > 0
Thus the capacity is determined by the rank of hρ(h),h∈H, assuming the hρ(h)ih∈HPrμ are
in general position. Since each ρ is a copy of the regular representation for H, the rank of
hLjN=1 ρ(h)ih∈H is merely N. Thus the fraction of linearly separable dichotomies is f(P, N),
the same as the capacity before pooling.	□
Now we prove local pooling operations in a standard CNN preserve a regular representation of a
subgroup of the cyclic group.
Lemma 4. Suppose P is a local pooling operation on two-dimensional signals (CNN feature maps),
and that π is the regular representation of a group G = ZW × ZL on code a(x). A pooled feature
map r = P(a) which acts on k × k windows of a is a regular representation of the subgroup
H = ZW/k × ZL/k.
Proof. Suppose an equivariant feature map a(x) ∈ RW ×L×N has corresponding regular represen-
tation of the group G = ZW × ZL for each of the N filters. Consider any local pooling operation
P (∙) (such as average or maximum) which acts on k X k patches where k divides both W and L.
rij,h(X)= P ({aio,jo,h(x) | i0 ∈ [ik, (i + 1)k],j0 ∈ [jk, (j + 1)k]})	(9)
Note that for k > 1, r(x) is no longer equivariant to G since the representation does not satisfy
the homomorphism property for shifts with length ` not divisible by k. However, the new code is
equivariant to a subgroup H = ZW/k × ZL/k, namely vertical and horizontal shifts with length
divisible by k. Let πnxk,mk represent a vertical shift of the image x by nk pixels and horizontal shift
20
Published as a conference paper at ICLR 2022
by mk pixels. Note that aij,h (πnxk,mkx) = ai+nk,j+mk (x) since a(x) is equivariant. Then, the
h-th pooled feature map transforms as
rij,h(πxk,mkX) = P [{ai0,j0,h(πχk,mk x) | i0 ∈ [ik, (i + 1)k],j 0 ∈ [jk, (j + 1)k]})
=P ({aio+nk,jo+mk,h(x) | i0 ∈ [ik,(i+1)k],j0 ∈ [jk, (j + 1)k]})
(a is Equivariant to πx)
= P ({ai0,j0,h(x) | i0 ∈ [(n + i)k, (n+ i+ 1)k], j0 ∈ [(m+j)k, (m+j + 1)k]}) ,
(k divides W, H)
= ri+n,j+m,h(x) , (Definition ofr)
We thus find that the code is a regular representation of the subgroup of the cyclic translations
H = {(nk, mk)}n∈[H∕k],m∈[w∕k]. This new group G0 has dimension |H| = 吉|G|.	□
A.4.3 Lower B ound and Upper Bound on Capacity for Nonlinear Pooling
Theorem 3. Suppose a code which is equivariant to a finite group G is pooled to a new code which
is equivariant to a finite subgroup H ⊆ G. Suppose the number of trivial dimensions in the original
G-equivariant code is N0. Then the fraction of linearly separable dichotomies on the G-invariant
problem for the pooled code is at least f(P, bN0/kc) where k = |G/H |. Similarly the fraction is at
most f(P, N0).
Proof. The pooled code, by assumption, has the property P(π(hg0)r) = ρ(h)P (π(g0)r) for any
h ∈ H and g0 ∈ R, where R is a set of representatives of G/H. The G-invariant separability
condition amounts to the proposition
∃w∀μ ∈ [P] ∀h ∈ H,g0 ∈ R : yμw>ρ(h)P(π(g0)rμ) > 0	(10)
^⇒ ∃w∀μ ∈ [P] ∀g0 ∈ R: y"w> hρ(h)ih∈H P(∏(g0)rμ) > 0;	(11)
in other words, a solution on the right hand side affords a solution over all of the manifolds generated
in the input space. We see that, this requires considering if this particular dichotomy is linearly
separble on the Pk anchor points hρ(h)ih∈H P(∏(g0)rμ). The simplest strategy to obtain an upper
bound is to consider what happens when a single new manifold is added. We see that when a
single new base point r is added it corresponds to k new points in the orbit hρi P(π(g0)r) for all
g0 ∈ R. Suppose that hρ(h)ih∈H has rank N0. Let C(P, N0) represent the number of linearly
separable dichotomies for P G-orbits in N0 trivial dimensions. Upon the addition of the k new
points (P → P + 1), we find that some of the pre-existing separable dichotomies give a new
separable dichotomy. This can be guaranteed to occur when a w separates the old dichotomy and
has w> hp ∙ P(π(g0)r) = 0 for the new anchor point r (but this condition is not necessary for a
new dichotomy to be separable). This condition means that the original dichotomy is separable in
the No - k dimensional subspace {w : W ∙ P(∏(g0)r) = 0}. By making infinitesimal adjustment
to this w the correct label on this new orbit can be achieved without altering the labels on any other
dichotomy. Since this argument gives a sufficient but not necessary condition to generate a new
dichotomy, we obtain the following inequality
C(P+1,N0) ≥ C(P, N0) + C(P, N0 -k).	(12)
Solving this recursion gives the capacity fP ooled(P, N0) ≥ fCover (P, bN0/kc). The greatest ca-
pacity occurs in the special case where hρi P(π(g0)r) = hρi P(r). In this case, the usual counting
theorem applies giving a fraction of separable dichotomies of f (P, N0). This is achieved, for in-
stance in average pooling as We showed in Theorem 2.	□
A.4.4 Direct sum equivariant convolutional layers
Here we describe how to build a convolutional layer architecture that is equivariant with respect
to the regular representation in the input space and the direct sum representations introduced in
Section 4.3 in the output space. For the following we assume that m1 and m2 are coprime, though
see the footnote in Section 4.3 for a discussion of how to loosen this requirement.
The input data is a W × L × M tensor where M is the number of input channels. The first step
is to simply take the output of a standard convolution (in our simulations we also apply a ReLU
21
Published as a conference paper at ICLR 2022
nonlinearity) applied to this input with periodic boundary conditions, resulting in a W ×L×N tensor
where N is the number of output channels. The next step is to, for each of the output channels, take
an average (or maximum) between entries spaced m1 entries apart horizontally or vertically in the
matrix, resulting in an m1 × m1 matrix. In our simulations we took averages rather than maximums.
This is repeated for the other number m2, resulting in an m2 × m2 matrix. This is repeated for every
output channel, resulting in N matrices of size m1 × m1 and N matrices of size m2 × m2 . Finally,
the resulting matrices are flattened and appended into an (m21 + m22) × N matrix, and the result is
passed through a nonlinearity (ReLU).
As the input tensor is cyclically permuted according to a regular representation π of Zm1m2, the
output of this equivariant convolutional layer permutes according to the representation ∏(1) ㊉ ∏(2)
where π(1) is the regular representation of Zm1 and π(2) is the regular representation of Zm2.
The proof that the fraction of separable dichotomies is given by f(P, 2N) follows the same proof as
for the standard periodic convolutions in Appendix A.4.1. Instead ofa direct sum LkN=1 π we get a
direct sum LN=1(∏(1) ㊉n(2)). Each of the π(1) ㊉ n(2) contain two trivial irreps in its decomposition,
so that the final fraction is f (P, 2N).
A.4.5 Induced representations
First we state the definition of an induced representation. Let H be a subgroup of a finite group
G and let ρ : H → GL(W) be a representation of H. Let V be the vector space of functions
f : G → W such that f (gh) = ρ(h)f (g) for all h ∈ H and g ∈ G. We now define the induced
representation π : G → GL(V ) to be the representation which satisfies (π(g0)f)(g) = f (gg0).
For intuition, note that every element of g can be written g = rh where r is a representative for a
coset in G/H and h ∈ H . This is because the cosets G/H partition G and the action of H stays
within a coset; hence r selects out the coset, and h goes to the desired element of the coset: g = rh.
With this decomposition, the action of π is then π(rh)f (g) = ρ(h)f (gr). Hence the r component
under π has the effect of permuting to the new coset that gr belongs in, and the h component under
π then has the effect ρ(h) on the resulting vector f(gr) that we originally specified. This is the most
natural way to get a representation ofG from a representation ofH. In the case of finite groups, one
can think of the r component as permuting a set of isomorphic copies ofV , each copy corresponding
to a different coset.
To compute the capacity of induced representations, and so prove Proposition 1, we use Frobenius
reciprocity of characters. Recall that the character θ ofa representation π : G → GL(V ) is the map
θ : G → C induced by the trace: θ(g) = Tr(π(g)). Now let θ be the character of ρ : H → GL(V )
and let θG be the character of the induced representation. Then hθG(g)ig∈G = hθ(h)ih∈H by
Frobenius reciprocity of characters (Mackey, 1970). The average of the character is the number
of trivial representations contained in the decomposition of the representation (see Serre (2014)).
Hence the capacity of the induced representation is equal to the capacity of ρ. The existence of
extensions beyond finite groups is not clear to the authors, but we welcome information if such
exists.
22