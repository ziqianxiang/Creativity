Published as a conference paper at ICLR 2022
On the Convergence of the Monte Carlo Ex-
ploring Starts Algorithm for Reinforcement
Learning
Che Wang1,2	Shuhan Yuan1	Kai Shao1	Keith Ross1 *
1	New York University Shanghai
2	New York University
Ab stract
A simple and natural algorithm for reinforcement learning (RL) is Monte Carlo
Exploring Starts (MCES), where the Q-function is estimated by averaging the
Monte Carlo returns, and the policy is improved by choosing actions that maxi-
mize the current estimate of the Q-function. Exploration is performed by “explor-
ing starts”, that is, each episode begins with a randomly chosen state and action,
and then follows the current policy to the terminal state. In the classic book on RL
by Sutton & Barto (2018), it is stated that establishing convergence for the MCES
algorithm is one of the most important remaining open theoretical problems in
RL. However, the convergence question for MCES turns out to be quite nuanced.
Bertsekas & Tsitsiklis (1996) provide a counter-example showing that the MCES
algorithm does not necessarily converge. Tsitsiklis (2002) further shows that if the
original MCES algorithm is modified so that the Q-function estimates are updated
at the same rate for all state-action pairs, and the discount factor is strictly less than
one, then the MCES algorithm converges. In this paper we make headway with
the original and more efficient MCES algorithm given in Sutton & Barto (1998),
establishing almost sure convergence for Optimal Policy Feed-Forward MDPs,
which are MDPs whose states are not revisited within any episode when using
an optimal policy. Such MDPs include a large class of environments such as all
deterministic environments and all episodic environments with a timestep or any
monotonically changing values as part of the state. Different from the previous
proofs using stochastic approximations, we introduce a novel inductive approach,
which is very simple and only makes use of the strong law of large numbers.
1	Introduction
Perhaps the most famous algorithm in tabular reinforcement learning is the so-called Q-learning
algorithm. Under very general conditions, it is well known that the Q-learning converges to the
optimal Q-function with probability one (Tsitsiklis, 1994; Jaakkola et al., 1994). Importantly, in
order to guarantee convergence for Q-learning, it is only required that every state-action pair be
visited infinitely often. Furthermore, as discussed in the related work, Q-learning converges for the
infinite-horizon discounted problem as well as for the non-discounted terminal-state problem (also
known as the stochastic shortest-path problem).
The Q-learning algorithm is inspired by dynamic programming and uses back-ups to update the
estimates of the optimal Q-function. An alternative methodological approach, which does not use
back-ups, is to use the Monte Carlo episodic returns to estimate the values of the Q-function. In
order for such an algorithm to succeed at finding an optimal policy, the algorithm must include some
form of exploration. A simple form of exploration is “exploring starts,” where at the beginning of
each episode, a random state-action pair is chosen. In the classic book on reinforcement learning
by Sutton & Barto (2018), the authors describe such an algorithm, namely, Monte Carlo Exploring
* Correspondence to: KeithRoss <keithwross@nyu.edu>.
1
Published as a conference paper at ICLR 2022
Starts (MCES). In MCES, after a (random-length) episode, the Q-function estimate is updated with
the Monte Carlo return for each state-action pair along the episode, and the policy is improved in the
usual fashion by setting itto the argmax of the current Q-function estimate. Exploration is performed
by exploring starts, where the initial state-action pairs may be chosen with any distribution.
We briefly remark here that AlphaZero is a Monte Carlo algorithm in that it runs episodes to comple-
tion and uses the returns from those episodes for the targets in the loss function (Silver et al., 2018).
AlphaZero additionally uses function approximators and planning (Monte Carlo Tree Search), and
is thus much more complicated than MCES. But AlphaZero is nonetheless fundamentally a Monte
Carlo algorithm rather than a Q-learning-based algorithm. We mention AlphaZero here in order to
emphasize that Monte Carlo algorithms are indeed used in practice, and it is therefore important
to gain a deep understanding of their underlying theoretical properties. Additional discussion is
provided in Appendix C.
Since Q-learning converges under very general conditions, a natural question is: does MCES con-
verge under equally general conditions? In the 1996 book, Bertsekas & Tsitsiklis (1996) provide
a counter-example showing that the MCES algorithm does not necessarily converge. See also Liu
(2020) for numerical results in this direction. Thus, we see that the MCES convergence problem is
fundamentally trickier than the Q-learning convergence problem. Instead of establishing a very gen-
eral result as in Q-learning, we can at best establish convergence for a broad class of special-cases.
Sutton and Barto write at the end of Section 5.3: “In our opinion, this is one of the most fundamen-
tal open theoretical questions in reinforcement learning”. This paper is focused on this fundamental
question. Although other questions, such as rates of convergence and regret bounds, are also impor-
tant, in this paper our goal is to address the fundamental question of convergence.
Tsitsiklis (2002) made significant progress with the MCES convergence problem, showing that al-
most sure convergence is guaranteed if the following three conditions hold: (i) the discount factor is
strictly less than one; (ii) the MCES algorithm is modified so that after an episode, the Q-function
estimate is updated with the Monte Carlo return only for the initial state-action pair of the episode;
and (iii) the algorithm is further modified so that the initial state-action pair in an episode is chosen
with a uniform distribution. As in the proof of Q-learning, Tsitsiklis’s proof is based on stochastic
approximations. The conditions (ii) and (iii) combined ensure that the Q function estimates are
updated at the same average rate for all state-action pairs, and both conditions appear to be crucial
for establishing convergence in the proof in Tsitsiklis (2002). However, these two conditions have
the following drawbacks:
•	Perhaps most importantly, condition (ii) results in a substantially less efficient algorithm,
since only one Q-function value is updated per episode. The original Sutton and Barto
version is more efficient since after each episode, many Q-values are typically updated
rather than just one. (We also note as an aside that AlphaZero will also collect and use
Monte Carlo return for all states along the episode, not just for the first state in the episode,
as discussed on page 2 of Silver et al. (2017), also see discussion in Appendix.)
•	Similar to the idea of importance sampling, one may want to use a non-uniform distribution
for the starting state-action pairs to accelerate convergence.
•	In some cases, we may not have access to a simulator to generate uniform exploring starts.
Instead, we may run episodes by interacting directly with the real environment. Such natu-
ral interactions may lead to starting from every state, but not uniformly. An example would
be playing blackjack at a casino rather than training with a simulator.
In this paper we provide new convergence results and a new proof methodology for MCES. Unlike
the result in Tsitsiklis (2002), the results reported here do not modify the original MCES algorithm
and do not require any of the conditions (i) - (iii). Hence, our results do not have the three draw-
backs listed above, and also allow for no discounting (as in the stochastic shortest path problem).
However, our proofs require restrictions on the dynamics of the underlying MDP. Specifically, we
require that under the optimal policy, a state is never revisited. This class of MDPs includes stochas-
tic feed-forward environments such as Blackjack (Sutton & Barto, 2018) and also all deterministic
MDPs, such as gridworlds (Sutton & Barto, 2018), Go and Chess (when played against a fixed op-
ponent policy), and the MuJoCo environments (Todorov et al., 2012) (Episodic MuJoCo tasks fall
into the category of OPFF MDPs because the MuJoCo simulation is deterministic). More examples
2
Published as a conference paper at ICLR 2022
are provided in appendix C. Moreover, if the trajectory horizon is instead fixed and deterministic,
we show that the original MCES algorithm always converges (to a time-dependent) optimal policy,
without any conditions on the dynamics, initial state-action distribution or the discount factor.
Importantly, we also provide a new proof methodology. Our proof is very simple, making use of
only the Strong Law of Large Numbers (SLLN) and a simple inductive argument. The proof does
not use stochastic approximations, contraction mappings, or martingales, and can be done in an
undergraduate course in machine learning. We believe that this new proof methodology provides
new insights for episodic RL problems.
In addition to the theoretical results, we present numerical experiments that show the original MCES
can be much more efficient than the modified MCES, further highlighting the importance of improv-
ing our understanding on the convergence properties of the original MCES algorithm.
2	Related Work
Some authors refer to an MDP with a finite horizon H as an episodic MDP. For finite horizon
MDPs, the optimal Q-function and optimal policy are in general non-stationary and depend on time.
Here, following Sutton & Barto (2018), we instead reserve the term episodic MDPs for MDPs that
terminate when the terminal state is reached, and thus the episode length is not fixed at H and
may have a random length. Moreover, for such terminal-state episodic MDPs, under very general
conditions, the optimal Q-function and policy are stationary and do not depend on time (as in infinite-
horizon discounted MDPs). When the dynamics are known and the discount factor equals 1, the
episodic optimization problem considered here is equivalent to the stochastic shortest path problem
(SSPP) (see Bertsekas & Tsitsiklis (1991) and references therein; also see Chapter 2 of Bertsekas
(2012)). Under very general conditions, value iteration converges to the optimal value function,
from which an optimal stationary policy can be constructed.
Convergence theory for RL algorithms has a long history. For the infinite-horizon discounted cri-
terion, by showing that Q-learning is a form of stochastic approximations, Tsitsiklis (1994) and
Jaakkola et al. (1994) showed that Q-learning converges almost surely to the optimal Q-function
under very general conditions. There are also convergence results for Q-learning applied to episodic
MDPs as defined in this paper with discount factor equal to 1. Tsitsiklis [8, Theorems 2 and 4(c)]
proved that if the sequence of Q-learning iterates is bounded, then Q-learning converges to the op-
timal Q values almost surely. Yu & Bertsekas (2013) prove that the sequence of Q-learning iterates
is bounded for episodic MDPs with or without non-negativity assumptions, fully establishing the
convergence of Q-learning for terminal-state episodic RL problems.
This paper is primarily concerned with the convergence of the MCES algorithm. In the Introduction
we reviewed the important work of Sutton & Barto (1998), Bertsekas & Tsitsiklis (1996), and Tsit-
siklis (2002). Importantly, unlike Q-learning, the MCES algorithm is not guaranteed to converge
for all types of MDPs. Indeed, in Section 5.4 of Bertsekas & Tsitsiklis (1996), Example 5.12 shows
that MCES is not guaranteed to converge for a continuing task MDP. However, if the algorithm is
modified, as described in the Introduction, then convergence is guaranteed (Tsitsiklis, 2002). Re-
cently, Chen (2018) extended the convergence result in Tsitsiklis (2002) to the undiscounted case,
under the assumption that all policies are proper, that is, regardless of the initial state, all policies
will lead to a terminal state in finite time with probability one. More recently, Liu (2020) relaxed
the all policies being proper condition. As in Tsitsiklis (2002), both Chen (2018) and Liu (2020)
assume conditions (ii) - (iii) stated in the introduction, and their proofs employ the stochastic ap-
proximations methodology in Tsitsiklis (1994). The results we develop here are complementary to
the results in Tsitsiklis (2002), Chen (2018), and Liu (2020), in that they do not require the strong
algorithmic assumptions (ii) - (iii) described in the Introduction, and they use an entirely different
proof methodology.
In this work we focus on the question of convergence of the MCES problem. We briefly mention,
there is also a large body of (mostly orthogonal) work on rates of convergence and regret analysis
for Q-learning (e.g. see Jin et al. (2018)) and also for Monte Carlo approaches (e.g., see Kocsis &
Szepesvari (2006) Azar et al. (2017)). To the best of our knowledge, these regret bounds assume
finite-horizon MDPs (for which the optimal policy is time-dependent) rather than the terminal-state
episodic MDPs considered here.
3
Published as a conference paper at ICLR 2022
3	MDP Formulation
Following the notation of Sutton & Barto (2018), a finite Markov decision process is defined by a
finite state space S and a finite action space A, reward function r(s, a) mapping S × A to the reals,
and a dynamics functionp(∙∣s, a), which for every S ∈ S and a ∈ A gives a probability distribution
over the state space S.
A (deterministic and stationary) policy π is a mapping from the state space S to the action space A.
We denote π(s) for the action selected under policy π when in state s. Denote Stπ for the state at
time t under policy π. Given any policy π, the state evolution becomes a well-defined Markov chain
with transition probabilities
P(Sπ+ι = SlSn = S) =P(S0ls,π(S))
3.1	Return for Random-Length Episodes
As indicated in Chapters 4 and 5 of Sutton & Barto (2018), for RL algorithms based on MC meth-
ods, we assume the task is episodic, that is “experience is divided into episodes, and all episodes
eventually terminate no matter what actions are selected.” Examples of episodic tasks include “plays
of a game, trips through a maze, or any sort of repeated interaction”. Chapter 4 of Sutton & Barto
(2018) further states: “Each episode ends in a special state called the terminal state, followed by a
reset to a standard starting state or to a sample from a standard distribution of starting states”.
The “Cliff Walking” example in Sutton & Barto (2018) is an example of an “episodic MDP”. Here
the terminal state is the union of the goal state and the cliff state. Although the terminal state will not
be reached by all policies due to possible cycling, it will clearly be reached by the optimal policy.
Another example of an episodic MDP, discussed at length in Sutton and Barto, is “Blackjack”.
Here we can create a terminal state which is entered whenever the player sticks or goes bust. For
Blackjack, the terminal state will be reached by all policies. Let S denote the terminal state. (If there
are multiple terminal states, without loss in generality they can be lumped into one state.)
When using policy π to generate an episode, let
Tπ = min{ t : Stπ = SS}	(1)
be the time when the episode ends. The expected total reward when starting in state S is
Vn(S) = E [XX r(S∏ ,∏(S∏))∣S∏ = s]	(2)
t=0
Maximizing (2) corresponds to the stochastic shortest path problem (as defined in Bertsekas &
Tsitsiklis (1991); Bertsekas (2012)), for which there exists an optimal policy that is both stationary
and deterministic (for example, see Proposition 2.2 of Bertsekas (2012)).
At the end of this paper we will also consider the finite horizon problem of maximizing:
H
VH (s) = E[X r(S∏ ,π(S∏ ,t))∣S∏ = s]	(3)
t=0
where H is a fixed and given horizon. For this criterion, it is well-known that optimal policy π(S) =
(π(S, 1), . . . , π (S, H )) is non-stationary. For the finite-horizon problem, it is not required that the
MDP have a terminal state.
3.2 Classes of MDPs
We will prove convergence results for important classes of MDPs. For any MDP, define the MDP
graph as follows: the nodes of the graph are the MDP states; there is a directed link from state S to
S0 if there exists an action a such that p(S0|S, a) > 0.
We say an environment is Stochastic Feed-Forward (SFF) if a state cannot be revisited within any
episode. More precisely, the MDP is SFF if its MDP graph has no cycles. Note that transitions
are permitted to be stochastic. SFF environments occur naturally in practice. For example, the
4
Published as a conference paper at ICLR 2022
Blackjack environment, studied in detail in Sutton & Barto (2018), is SFF. We say an environment
is Optimal Policy Feed-Forward (OPFF) if under any optimal policy a state is never re-visited.
More precisely, construct a sub-graph of the MDP graph as follows: each state s is a node in the
graph, and there is a directed edge from node S to s0 if p(s0∣s, a*) > 0 for some optimal action a*.
The MDP is OPFF if this sub-graph is acyclic.
An environment is said to be a deterministic environment if for any state s and chosen action a,
the reward and subsequent state s0 are given by two (unknown) deterministic functions r = r(s, a)
and s0 = g(s, a). Many natural environments are deterministic. For example, in Sutton & Barto
(2018), environments Tic-Tac-Toe, Gridworld, Golf, Windy Gridworld, and Cliff Walking are all
deterministic. Moreover, many natural environments with continuous state and action spaces are
deterministic, such as the MuJoCo robotic locomotion environments (Todorov et al., 2012). It is
easily seen that all SFF MDPs are OPFF, and all deterministic MDPs for which the optimal policy
terminates w.p.1 are OPFF.
4 Monte Carlo with Exploring Starts
The MCES algorithm is given in Algorithm 1. The MCES algorithm is a very natural and simple
algorithm, which uses only Monte Carlo returns and no backups for updating the Q-function and
training the policy. A natural question is: does it converge to the optimal policy? As mentioned in
the Introduction, unlike for Q-learning, it does not converge for general MDPs. We instead establish
convergence for important classes of MDPs.
Algorithm 1 is consistent with the MCES algorithm in Sutton & Barto (2018) and has the following
important features:
•	Q(St, At) is updated for every St, At pair along the episode, and not just for S0, A0 as
required in (Tsitsiklis, 2002).
•	The initial state and action can be chosen arbitrarily (but infinitely often), and does not have
to be chosen according to a uniform distribution as required in (Tsitsiklis, 2002).
•	For simplicity, we present the algorithm with no discounting (i.e., γ = 1). However, the
subsequent proofs go through for any discount factor γ ≤ 1. (The proof in Tsitsiklis (2002)
requires γ < 1.)
We emphasize that although the results in Tsitsiklis (2002) require restrictive assumptions on the
algorithm, the results in Tsitsiklis (2002) hold for general MDPs. Our results do not make restrictive
algorithmic assumptions, but only hold for a sub-class of MDPs.
Algorithm 1 MCES
1:	Initialize: π(s) ∈ A, Q(s, a) ∈ R, for all s ∈ S, a ∈ A, arbitrarily;
Returns(s, a) J empty list, for all S ∈ S, a ∈ A.
2:	while True do
3:	Choose S0 ∈ S, A0 ∈ A s.t. all pairs are chosen infinitely often.
4:	Generate an episode following π: S0, A0, S1, A1, . . . , ST-1, AT-1, ST .
5:	G J 0
6:	for t = T - 1, T - 2, . . . , 0 do
7:	G J G + r(St , At )
8:	Append G to ReturnS(St, At)
9:	Q(St, At) J average(ReturnS(St, At))
10:	π(St) J arg maxa Q(St, a)
We begin with the following lemma, whose proof can be found in Appendix A.
Lemma 1. Let X1, X2, . . . be a sequence of random variables. Let T be a random variable taking
values in the positive integers, and suppose P(T < ∞) = 1. Suppose that for every positive integer
n, Xn, Xn+1, . . . are i.i.d. with finite mean x* and finite variance when conditioned on T = n.
Then P(limN→∞ N PN=1 Xi= x*) = L
5
Published as a conference paper at ICLR 2022
Say that the MCES algorithm begins iteration u at the uth time that an episode runs, and ends the
iteration after Q and π have been updated (basically an iteration in the while loop in Algorithm 1).
Denote by Qu(s, a) and πu(s) for the values of Q(s, a) and π(s) at the end of the uth iteration.
For simplicity, assume that the MDP has a unique optimal policy π*, and denote q*(s, a) for the
corresponding action-value function. Note that the proof extends easily to the case of multiple
optimal policies.
We now state and prove the convergence result for SFF MDPs. We provide the full proof here in
order to highlight how simple it is. Afterwards we will state the more general result for OPFF MDPs,
for which the proof is a little more complicated,
Theorem 1. Suppose the MDP is SFF Then Qu(s, a) converges to q*(s, a) and ∏u(s) converges to
π*(s) for all S ∈ S and all a ∈ A w.p.1.
Proof. Because the MDP is SFF, its MDP graph is a Directed Acyclic Graph (DAG). So we can
re-order the N states such that from state sk and selecting any action, we can only transition to a
state in {sk+1, . . . , sN}. Note state sN is the terminal state, from state sN-1, all actions lead to sN.
The proof is by backward induction. The result is trivially true for s = sN. Suppose it is true for all
states in {sk+1, . . . , sN} and all actions a ∈ A. We now show it is true for sk and all actions a ∈ A.
We first establish Qu(Sk, a) converges to q*(sk,a) for all a ∈ A w.p.1. Let T be the iteration
U when πu(s) has converged to π*(s) for all S ∈ {sk+ι,..., SN}. By the inductive assumption,
P(T < ∞) = 1. Let a ∈ Abe any action. Now consider any episode after time T in which we visit
state Sk and choose action a. Because the MDP is SFF, the next state will be in {Sk+1, . . . , SN}, and
because of the inductive assumption, the subsequent actions in the episode will follow the optimal
policy ∏* until the episode terminates. By the definition of q*(sk, a), the expected return for such
an episode is equal to q*(sk, a). Let Gn denote the return for (Sk, a) for the nth episode in which
(Sk, a) appears. After time T, these returns are i.i.d. with mean q*(Sk, a). Therefore, by Lemma 1,
1	1	1	*/
lim Qu(Sk,a) = lim vy TGn = q (Sk, a) w.p.1	(4)
u→∞	N→∞ N
n=1
It remains to show that ∏u(Sk) converges to ∏*(Sk) w.p.1. Define a* = ∏*(Sk). Since ∏* is the
unique optimal policy, we have:
q*(Sk,a*) ≥ q*(Sk,a) + 0	(5)
for some 0 > 0 for all a 6= a*.
Let Ω be the underlying sample space, and let Λ be the set of all ω ∈ Ω such that Qu(Sk, a)(ω)
converges to q* (Sk, a) for all a ∈ A. By (4), P(Λ) = 1. Thus for any ω ∈ Λ and any > 0, there
exists a u0 (depending on ω) such that u ≥ u0 implies
∣q*(Sk, a) - Qu(Sk ,a)(ω)∣ ≤ E for all a ∈ A	(6)
Let be any number satisfying 0 < < 0/2, let ω ∈ Λ, and u0 be such that (6) is satisfied for all
u ≥ u0. It follows from (5) and (6) that for any u ≥ u0 we have
Qu(Sk, a*)(ω) ≥q*(Sk,a*)-E	(7)
≥ q*(Sk, a) + E0 - E	(8)
≥ Qu(Sk, a)(ω) + E0 - 2E	(9)
> Qu(Sk, a)(ω)	(10)
for all a 6= a* . Let u be any iteration after u0 such that state Sk is visited in the corresponding
episode. From the MCES algorithm, πu(Sk)(ω) = arg maxa Qu(Sk, a)(ω). Thus the above in-
equality implies πu(Sk)(ω) = a*; furthermore, πu(Sk)(ω) will be unchanged in any subsequent
iteration. Thus, for every ω ∈ Λ, πu(Sk)(ω) converges to a*. Since P (Λ) = 1, it follows πu(Sk)
converges to a* w.p.1., completing the proof.
□
We make the following additional observations: (1) It is not necessary to assume the MDP has
a unique optimal policy. The theorem statement and proof go through with minor modification
without this assumption. (2) Also we can allow for random reward, with distribution depending on
the current state and action. The proof goes through with minor changes.
6
Published as a conference paper at ICLR 2022
5 OPFF MDPS
For the case of OPFF, we first need to modify the algorithm slightly to address the issue that an
episode might never reach the terminal state for some policies (for example, due to cycling). To this
end, let M be some upper bound on the number of states in our MDP. We assume that the algorithm
designer has access to such an upper bound (which may be very loose). We modify the algorithm
so that if an episode does not terminate within M steps, the episode will be forced to terminate and
the returns will simply not be used. We also initialize all Q values to be -∞, so that the policy will
always prefer an action that has led to at least one valid return over actions that never yield a valid
return. The modified algorithm is given in Algorithm 2. Finally, as in Sutton & Barto (2018), we use
first-visit returns for calculating the average return. (This mechanism is not needed for SFF MDPs
since under all policies states are never revisited.)
Algorithm 2 First-visit MCES for OPFF MDPs	
1	: Initialize: π(S) ∈ A, Q(S, a) = -∞, for all S ∈ S, a ∈ A, arbitrarily; Returns(s, a) J empty list, for all S ∈ S, a ∈ A.
2	: while True do
3	: Choose S0 ∈ S, A0 ∈ A s.t. all pairs are chosen infinitely often.
4	:	Generate an episode following π: S0, A0, S1, A1, . . . , ST-1, AT-1, ST .
5	: if the episode does not end in less than M time steps then
6	:	terminate the episode at time step M
7	: else
8	:	GJ0
9	:	for t = T - 1, T - 2, . . . , 0 do
10	:	G J G + r(St, At)
11	:	if St, At does not appears in S0, A0, S1, A1 . . . , St-1, At-1 then
12	:	Append G to ReturnS(St, At)
13	:	Q(St, At) J average(ReturnS(St, At))
14	:	π(St) J arg maxa Q(St, a)
Theorem 2. Suppose the MDP is OPFF. Then Qu(s,a) converges to q*(s, a) and ∏u(s) converges
to π*(s) for all S ∈ S and all a ∈ A w.p.1.
The proof can be found in the appendix. As with SFF MDPs, it is not necessary to assume that the
MDP has a unique optimal policy; furthermore, the reward can be random (with distribution depend-
ing on current state and action). Note for OPFF MDPs, our proof has to take a more sophisticated
approach since we now can transition to arbitrary state by taking any non-optimal action.
6 Finite-Horizon MDPs
In this section, we extend our results to finite-horizon MDPs. In this case, we will be able to establish
convergence for all MDPs (i.e., not just for OPFF MDPs). A finite-horizon MDP is defined by a
finite horizon set H = {0, 1, . . . , H}, a finite state space S, a finite action space A, a reward function
r(s,t, a) mapping S×H×A tothe reals, and a dynamics function p( ∙∣ s,t,a), which for every S ∈ S,
a ∈ A and time step t, gives a probability distribution over the state space S. The horizon H is the
fixed time at which the episode terminates. Note in this setting, the optimal policy ∏*(s, t) will also
be time-dependent, even if p(∙∣s, t, a) and r(s, t, a) do not depend on t.
The MCES algorithm for this setting is given in Algorithm 3. Note that in this version of the
algorithm, during exploring starts we need to choose an S0 ∈ S, A0 ∈ A, and an h ∈ [0, H],.
Corollary 1. Suppose we are using the finite-horizon optimization criterion. Then Qu (S, t, a) con-
verges to q* (s, t, a) and ∏u(s, t) converges to π*(s,t) forall S ∈ S, t ∈ H and all a ∈ A w.p.1.
Proof. A finite-horizon MDP is equivalent to a standard MDP where the time step is treated as part
of the state. Since the number of time steps monotonically increases, this MDP is SFF. Thus the
convergence OfMCES for the finite-horizon MDP setting follows directly from Theorem 1.	□
7
Published as a conference paper at ICLR 2022
Algorithm 3 MCES for Finite-Horizon MDPs
1:	Initialize: π(s, t) ∈ A, Q(s, t, a) ∈ R, for all s ∈ S, t ∈ [0, H], a ∈ A, arbitrarily;
Returns(s,t a) — empty list, for all S ∈ S,t ∈ [0, H],a ∈ A.
2:	while True do
3:	Choose S0 ∈ S, A0 ∈ A, h ∈ [0, H], s.t. all triples are chosen infinitely often.
4:	Generate an episode following π: Sh, Ah, Sh+1, Ah+1, . . . , SH-1, AH-1, SH.
5:	G — 0
6:	fort = H - 1,H - 2, .. . ,hdo
7:	G - G + r(St, t, At)
8:	Append G to Returns(St , t, At)
9:	Q(St, t, At) — average(Returns(S", At))
10:	π(St, t) — arg max。Q(St,t, a)
7 Experimental results
In addition to the theoretical results, we also provide experimental results to compare the conver-
gence rate of the original MCES algorithm and the modified MCES algorithm in Tsitsiklis (2002),
where the Q-function estimate is updated with Monte Carlo return only for the initial state-action
pair of each episode. We will call the original MCES the “multi-update” variant, and the modified
MCES the “first-update” variant. We consider two classical environments: blackjack and stochas-
tic cliffwalking. We will briefly discuss the settings and summarize the results. A more detailed
discussion, including additional results on Q-Learning is provided in appendix D and E.
For blackjack, we use the same problem setting as discussed in Sutton & Barto (1998). Blackjack
is an episodic task where for each episode, the player is given 2 initial cards, and can request more
cards (hits) or stops (sticks). The goal is to obtain cards whose numerical values sum to a number as
great as possible without exceeding 21. Along with the two MCES variants, we also compare two
initialization schemes: standard-init, where we initialize by first drawing two random cards from a
deck of 52 cards; and uniform-init, where the initial state for an episode is uniformly sampled from
the state space. We now compare their rate of convergence in terms of performance and the absolute
Q update error, averaged across state-action pairs visited in each episode. Figure 1 shows uniform
initialization converges faster than standard initialization, and the multi-update variant is faster than
the first-update variant for both initialization schemes.
(a) Performance
(b) Average absolute Q update error
Figure 1: (a) Performance and (b) average of absolute Q update error (in log scale) for blackjack.
Multi-update variant has better performance and lower Q error.
In cliff walking, an agent starts from the bottom left corner of a gridworld, and tries to move to the
bottom right corner, without falling into a cliff (an area covering the bottom row of the gridworld).
The agent gets a -1 reward for each step it takes, and a -100 for falling into the cliff. We consider
two variants: SFF cliff walking and OPFF cliff walking tasks. For the SFF variant, the current time
step of the episode is part of the state, so the MDP is a SFF MDP, and the agent tries to learn a time-
dependent optimal policy. In this setting we add more stochasticity by having wind move the agent
towards one of four directions with some probability. For the OPFF variant, the time step is not part
of the state, and the wind only affect the agent when the agent moves to the right, and can only blow
the agent one step upwards or downwards, making an OPFF MDP. We now compare the two MCES
8
Published as a conference paper at ICLR 2022
variants and measure the convergence rate in terms of the performance and the average L1 distance
between the current Q estimate and the optimal Q values. The results are summarized in Figure 2,
the training curves are averaged across a number of different gridworld sizes and wind probabilities.
Results show that multi-update MCES consistently learns faster than first-update MCES.
(b) OPFF Cliff Walking,
average L1 distance to
optimal Q values
(d) SFF Cliff Walking,
average L1 distance to
optimal Q values
(c) SFF Cliff Walking,
Performance
(a) OPFF Cliff Walking,
Performance
Figure 2: Performance and the average L1 distance to optimal Q values, on OPFF and SFF cliff
walking, the curves are averaged over different gridworld sizes and wind probability settings.
Our experimental results show that the multi-update MCES converges much faster than the first-
update MCES. These results further emphasize the importance of gaining a better understanding of
the convergence properties of the original multi-update MCES variant, which is much more efficient.
8 Conclusion
Theorem 2 of this paper shows that as long as the episodic MDP is OPFF, then the MCES algo-
rithm converges to the optimal policy. As discussed in Section 3.2, many environments of practical
interest are OPFF. Our proof does not require that the Q-values be updated at the same rate for all
state-action pairs, thereby allowing more flexibility in the algorithm design. Our proof methodol-
ogy is also novel, and can potentially be applied to other classes of RL problems. Moreover, our
methodology also allows us to establish convergence results for finite-horizon MDPs as a simple
corollary of Theorem 2. Combining the results of Bertsekas & Tsitsiklis (1996), Tsitsiklis (2002),
Chen (2018), Liu (2020), and the results here gives Figure 3, which summarizes what is now known
about convergence of the MCES algorithm. In appendix F, we also provide a new counterexample
where we prove that MCES may not converge in a non-OPFF episodic environment.
Figure 3: MCES has been studied for two classes of algorithms: Q-values updated at the same rate
Tsitsiklis (2002); Chen (2018); Liu (2020) and the original, more flexible algorithm, which does not
require the conditions (ii) and (iii) stated in the Introduction. We partition the episodic MDP space
into two classes: OPFF MDPs and non-OPFF MDPs. As shown in the figure, this leads to four
algorithmic/MDP regions. Convergence is now established for the three blue shaded regions, for all
discount factors γ ≤ 1. In the other region, it is known that at least for some non-OPFF MDPs,
convergence does not occur (shown as the orange oval region). A new counterexample in episodic
MDP and additional discussion are provided in appendix F.
The results in this paper along with other previous works (Tsitsiklis, 2002; Chen, 2018; Liu, 2020)
make significant progress in establishing the convergence of the MCES algorithm. Many cases of
practical interest are covered by the conditions in these papers. It still remains an open problem
whether there exist conditions on MDPs that are weaker than the OPFF, and can still guarantee the
convergence of the original MCES algorithm.
9
Published as a conference paper at ICLR 2022
References
Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforce-
ment learning. In International Conference on Machine Learning, pp. 263-272. PMLR, 2017.
Dimitri P Bertsekas. Dynamic programming and optimal control, volume 2. Athena scientific
Belmont, MA, 4 edition, 2012.
Dimitri P Bertsekas and John N Tsitsiklis. An analysis of stochastic shortest path problems. Math-
ematics of Operations Research, 16(3):580-595, 1991.
Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic programming, volume 5. Athena Scien-
tific Belmont, MA, 1996.
Xinyue Chen, Zijian Zhou, Zheng Wang, Che Wang, Yanqiu Wu, and Keith Ross. Bail: Best-action
imitation learning for batch deep reinforcement learning. In Advances in Neural Information
Processing Systems, volume 33, pp. 18353-18363, 2020.
Yuanlong Chen. On the convergence of optimistic policy iteration for stochastic shortest path prob-
lem. arXiv preprint arXiv:1808.08763, 2018.
Tommi Jaakkola, Michael I Jordan, and Satinder P Singh. Convergence of stochastic iterative dy-
namic programming algorithms. In Advances in neural information processing systems, pp. 703-
710, 1994.
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably effi-
cient? In Advances in Neural Information Processing Systems, volume 31. Curran Associates,
Inc., 2018.
Levente Kocsis and Csaba Szepesvari. Bandit based monte-carlo planning. In European conference
on machine learning, pp. 282-293. Springer, 2006.
Jun Liu. On the convergence of reinforcement learning with monte carlo exploring starts. arXiv
preprint arXiv:2007.10916, 2020.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. nature, 550(7676):354-359, 2017.
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement
learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140-
1144, 2018.
Richard S Sutton and Andrew G Barto. Introduction to reinforcement learning, volume 2. MIT
press Cambridge, 1998.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026-
5033. IEEE, 2012.
John N Tsitsiklis. Asynchronous stochastic approximation and q-learning. Machine learning, 16
(3):185-202, 1994.
John N Tsitsiklis. On the convergence of optimistic policy iteration. Journal of Machine Learning
Research, 3(Jul):59-72, 2002.
Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous con-
trol: Improved data-augmented reinforcement learning. arXiv preprint arXiv:2107.09645, 2021.
Huizhen Yu and Dimitri P Bertsekas. On boundedness of q-learning iterates for stochastic shortest
path problems. Mathematics of Operations Research, 38(2):209-227, 2013.
10
Published as a conference paper at ICLR 2022
A Proof of Lemma 1
Proof. We have
1N
P( lim — VXi = x*)
N→m∞ N	i x
i=1
∞N
=X P (Nl→m∞ N X Xi = x* IT = n)P (T = n)
n=1	i=1
∞	n-1	N
=X P(Nim∞(N X Xi + N X Xi) = x*IT =喻P(T = n)
n=1	i=1	i=n
∞N
=X P(Nim∞ N X Xi = x* IT = n)P(T = n) = 1
n=1	i=n
The first equality follows from P(T < ∞) = 1. The last equality follows from the conditional i.i.d.
assumption and the Strong Law of Large Numbers. Note we can apply the Strong Law of Large
Numbers here because as stated in Lemma 1, we have the assumption that Xn, Xn+1, . . . are i.i.d.
with finite mean and finite variance.
□
B Proof of theorem 2
Proof. Because the MDP is OPFF, its optimal policy MDP graph is a DAG, so we can re-order the
states such that from state sk and selecting the optimal action a*k, we can only transition to a state in
{sk+1, . . . , sN}. Note that state sN is the terminal state, and note that from state sN-1 the optimal
action only leads to sN .
We first show that for all k = 1, . . . , N, Qu(sk, a*k) converges to q* (sk, a*k) and that πu(sk) con-
verges to π* (sk), w.p.1. Once we establish this convergence result for all k = 1, . . . , N, we will
then complete the proof by establishing convergence of Qu(sk, a) for arbitrary actions a. Note that
the organization of this proof is slightly more complicated than that of Theorem 1 due to the weaker
OPFF assumption.
The result is trivially true for s = sN . Suppose now Qu(sj, aj*) converges to q* (sj, aj*) and that
πu(sj) converges to π*(sj), w.p.1 for all j = k + 1, . . . , N. We now show Qu(sk, a*k) converges to
q*(sk, a*k) and that πu(sk) converges to π*(sk), w.p.1. Denote a* = a*k.
Let T be the iteration u when πu(s) has converged to πu* (s) for all s ∈ {sk+1, . . . , sN}. By the
inductive assumption, P(T < ∞) = 1. Now consider any episode after time T that for some
timestep in this episode, we arrive in state sk and chooses the optimal action a*. Because the MDP
is OPFF, the next state will be in {sk+1, . . . , sN }, and because of the inductive assumption the
subsequent actions in the episode will follow the optimal policy π* until the episode terminates
within a finite number of steps. By the definition of q*(sk, a*), the expected return for (sk, a*) in
this episode is equal to q*(sk, a*). Let Gn denote the return for nth episode in which we visit sk
and chooses optimal action a*. Note that after time T, these returns are i.i.d. with mean q* (sk, a*).
Therefore, by Lemma 1,
1N
lim Qu(Sk,a*) = lim — S^Gn = q*(sk,a*) w.p.1	(11)
u→∞	N →∞ N
n=1
Next we show that πu(sk) converges to π* (sk) = a* w.p.1. Since π* is the unique optimal policy,
we have:
q	*(sk, a*) ≥q	*(sk,a)+0	(12)
for some 0 > 0 for all a 6= a*.
The proof at this stage is different from the proof at the corresponding stage in the proof of Theorem
1 since we can now only use (11) for a = a* .
11
Published as a conference paper at ICLR 2022
Consider state sk and an arbitrary action a ∈ A. From the MCES algorithm, we have Qu(sk , a) =
L1~ PLUI Gι, where Lu is the total number of first-visit returns used to compute Qu(Sk, a) UP
through the uth iteration, and Gl is the return value for the lth such iteration. Let Π denote the
(finite) set of all deterministic policies, Lπu denote the number of first-visit returns used to compute
Qu(sk, a) up through the uth iteration when using policy π, and Gιπ, 1 ≤ l ≤ Lπu, denote the lth
return value when policy π is used. We have
(13)
(14)
By the law of large numbers, we know that for any policy π such that Luπ → ∞ we have w.p.1
1 Lπu
lim π- EGn = qπ(sk,a) ≤ q*(sk,a)
u→∞ Lπ
u ι=1
(15)
where qπ(sk, a) is the action-value function for policy π. The inequality in (15) follows from the
definition of q*(s, a). It follows from (14) and (15) that w.p.1
limsup Qu(Sk, a) ≤ q*(sk, a) for all a	(16)
u→∞
Note that in the OPFF setting, in the special case that no valid return has been obtained for (Sk, a),
then Qu(Sk, a) = -∞ ≤ q*(Sk, a), so the above inequality still holds.
Let Ω be the underlying sample space, and let Λ ⊂ Ω be the set over which (16) holds. Note that
P(Λ) = 1. Thus for any ω ∈ Λ and any > 0, there exists a u0 such that u ≥ u0 implies
Qu(Sk, a)(ω) ≤ q*(Sk, a) + for all a ∈ A	(17)
Let be any number satisfying 0 < < 0/2, let ω ∈ Λ, and u0 be such that (17) is satisfied for all
u ≥ u0 . It follows from (11), (12) and (17) that for any u ≥ u0 we have
Qu(Sk, a*)(ω) ≥q*(Sk,a*)-	(18)
≥ q*(Sk, a) + 0 -	(19)
≥ Qu(Sk, a)(ω) + 0 - 2	(20)
> Qu(Sk, a)(ω)	(21)
for all a 6= a* . Let u be any iteration after u0 such that the episode includes state Sk. From
the MCES algorithm, πu(Sk)(ω) = arg maxa Qu(Sk, a)(ω). Thus the above inequality implies
πu(Sk)(ω) = a*; furthermore, πu(Sk)(ω) will be unchanged in any subsequent iteration. Thus, for
every ω ∈ Λ, πu(Sk)(ω) converges to a*. Since P(Λ) = 1, it follows πu(Sk) converges to a* w.p.1.
We have now shown that for all k = 1, . . . , N, Qu(Sk, a*k) converges to q*(Sk, a*k) and that πu(Sk)
converges to π*(Sk), w.p.1. It remains to show convergence of Qu(S, a) to q*(S, a) for arbitrary
state S and action a. Let u0 be such that u ≥ u0, πu(S) = π* (S) for all S ∈ S. Consider an episode
after iteration u0 in which we visit state S and take action a. After taking action a, the policy follows
the optimal policy π*. Thus the expected return for (S, a) in this episode is q* (S, a). We can thus
once again apply Lemma 1 to show Qu(S, a) converges to q* (S, a) w.p.1, thereby completing the
proof.	□
12
Published as a conference paper at ICLR 2022
C Additional discussion on practical OPFF tasks and MC
ALGORITHMS
C.1 OPFF as a practical setting
OPFF MDP is a large and important family of MDPs. There are many natural examples of OPFF
MDPs, for example, Blackjack, windy gridworlds and MuJoCo robotic locomotion as discussed in
this work. Also note that if a task involves any monotonically changing value as part of the state,
then it is also OPFF. For example, when time is added to the state to handle finite horizon criteria,
then the MDP becomes OPFF whether or not the original MDP is OPFF. Here we give an extended
list of real-world practical problems that fall into OPFF MDPs:
•	Operating a robot or datacenter with a power budget;
•	Driving a car with a given amount of fuel or to reach a target within a time limit;
•	Manufacturing a product with limited resources;
•	Doing online ads bidding with a fixed budget;
•	Running a recommendation systems with a limited amount of recommendation attempts;
•	Trading to maximize profit within a time period, and more;
For the MuJoCo environment, note if we treat it as an episodic MDP (for example, if the task is
to run towards a goal, and the episode terminates when the goal is reached) instead of an infinite-
horizon task (for example, if the task is to keep running indefinitely), then it falls into the category
of OPFF because the simulation is deterministic. As discussed in the main paper, all deterministic
episodic MDPs are OPFF.
C.2 How AlphaZero relates to Monte Carlo methods
The AlphaZero learning process can be roughly seen as a nested loop: there is the outer loop where
the agent starts the game from an empty board and plays to the end of the game, and then for each
state s on this trajectory, there is an inner loop of Monte Carlo Tree Search (MCTS), which is a
planning phase that starts from state s. After the planning finishes, a single physical move is made
(in the outer loop). When we consider all algorithmic components of AlphaZero, it is clear that
AlphaZero is very different from MCES (even the MCTS algorithm alone is very different from
MCES). Although MCES and AlphaZero do not do the same thing, they share some important
similarities on a high level.
In appendix E, we further provide a discussion on the use of Monte Carlo methods in recent liter-
ature, together with an additional experimental comparison between the MCES algorithm and the
Q-Learning algorithm.
13
Published as a conference paper at ICLR 2022
D	Additional experimental details
D.1 B lackjack environment
Here we give a more in-depth discussion on our blackjack experiments. The code for the experiments
is provided 1.
We use the same problem setting as discussed in Sutton & Barto (1998). We consider only a single
player against a dealer. The goal of the blackjack game is to request a number of cards so that the
sum of the numerical values of your cards is as great as possible without exceeding 21. All face
cards will be counted as 10 and an ace can be counted as either 1 or 11. If the player holds an ace
that can be counted as 11 without causing the total value to exceed 21, then the ace is said to be
usable.
For each round of blackjack, both the player and the dealer will get two cards respectively at the
beginning. One of the dealer’s cards is face up and another is face down. For each time step in
the episode, the player has two possible actions: either to request an additional card (hits), or stops
(sticks), if the player’s sum exceeds 21 (goes bust), then the player loses. After the player sticks,
The dealer will hit or stick according to a fixed policy: he sticks on any sum of 17 or greater and hits
otherwise, if the dealer goes bust then the player wins. In the end, if both sides do not go bust, their
sums are compared, and the player wins if the player’s sum is greater, and loses if the dealer’s sum
is greater, and the game is a draw if both sides have the same sum. When the episode terminates,
the reward is 1 if the player wins, -1 if the player loses, and 0 if the game is a draw. Note that in an
episode, the player cannot revisit a previously visited state (if the player has a usable ace, the player
can move into a state with a unusable ace, but not vice-versa). Therefore blackjack is a stochastic
feed-forward (SFF) environment.
For blackjack, the player’s state consists of three components: the player’s current sum, the card
that the dealer is showing, and whether the player has a usable ace. There are some states where the
optimal policy is trivial. For example, when the player has a very small sum, the player can always
hit and the sum value will increase for sure without the danger of going bust. If we ignore such
special states, then we have a total of 200 states that are interesting to the player (it is 200 states
considering the player’s sum (12-21), dealer’s showing card (ace-10) and whether the player has a
usable ace (Sutton & Barto, 1998)). For the uniform initialization scheme, the initial state of the
player is uniformly sampled from these 200 states.
For the standard initialization, we instead simply follow the rules of blackjack. We randomly draw
cards for the player and the dealer, each of these cards can be one of the 13 cards (ace-10, J, Q and
K) with equal probability. (The cards are dealt from an infinite deck, i.e., with replacement. )
When we compare the convergence rate of the multi-update MCES and the first-update MCES, we
use performance and the average absolute Q update error. The Q update error for a state-action
pair is simply the difference between the Q values before and after a Q update. For each episode,
the average absolute Q update error is averaged over all state-action pairs that are updated in this
episode. In all figures, the solid curve shows the mean value over 5 seeds, and the shaded area
indicates the confidence interval.
In Figure 1, we see that the first-update MCES variant (which only updates the Q value for the initial
state-action pair in each episode) works in blackjack even when we sample according to the standard
rule of blackjack and not uniformly. This might be due to fact that for blackjack, although the initial
state distribution under standard initialization (where the player gets 2 random initial cards from
the dealer) is not the same as uniform initialization, it still sufficiently covers the state space, so
the agent is able to update its value estimates for all state-action pairs and gradually learn a good
policy. However, this is not the case for cliffwalking, where if we initialize an episode according
to the standard rule, then the initial state for the agent is always at the the bottom left corner of
the gridworld, making it impossible to update the value function for other states. In many practical
problems, we might not have access to an initial state distribution that sufficiently covers the entire
state space, so it is very important to gain a better understanding of the convergence properties of
the multi-update MCES variant.
1https://github.com/Hanananah/On-the-Convergence-of-the-MCES-Algorithm-for-RL
14
Published as a conference paper at ICLR 2022
D.2 Stochastic Cliff Walking environment
Here we give a more in-depth discussion on our cliff walking experiments.
As illustrated in Figure 4, in the cliff walking environment, the agent starts from the bottom left
corner of a gridworld (marked with the letter S), and tries to move to the goal state at the bottom
right corner (marked with the letter G), without falling into the cliff area (shaded area at the bottom
row of the gridworld).
In our setting, the agent can move in four directions (right, up, left, down), and gets a -1 reward for
each step it takes, a -100 for falling into the cliff, and a 0 for reaching the goal state. If the agent
reaches the goal state or falls off the cliff, the episode will immediately terminate. If the agent moves
out of the boundary of the gridworld, the agent will be “bounced” back and will not be able to go
outside the boundaries.
Here we consider two variants of the cliffwalking environment: SFF cliff walking and OPFF cliff
walking. For the SFF variant, the current time step of the episode is part of the state. Since the time
step can only increase, the MDP is now a SFF MDP, and the agent tries to learn a time-dependent
optimal policy. In this setting we add stochasticity by having wind move the agent one step towards
one of the four directions with some probability. For example, when the wind probability is 50%,
there is 12.5% chance that the agent will take an additional random step for each direction due to the
wind. In Figure 4 this is illustrated with the arrows near the position P2 . If the agent is in the SFF
cliff walking environment and is currently at position P2, then if the agent moves one step upward
(indicated by the solid arrow), there is some chance that the agent will also take an additional step
towards one of the four directions, with equal probability (indicated by dashed arrows). Note that
the agent’s state space now includes the x, y positions of the agent, as well as the current time step.
For the OPFF variant, the time step is not part of the state. To make sure the environment is OPFF,
we only allow the wind to affect the agent when it moves to the right, and the wind can only blow
the agent one step upwards, or downwards. This is shown in Figure 4 by the arrows near the position
P1, if the agent is in the OPFF cliff walking environment and is current in position P1, then if the
agent moves one step to the right, the wind has some probability of making it take an additional step
upwards or downwards, with equal probability. In this setting, if the agent takes any action other
than moving one step right, then it will not be affected. Under such a setting, if the agent is following
an optimal policy, then in an episode it will never revisit a previously visited state, thus the MDP is
OPFF.
Special care is required when running MCES in the OPFF environment, since it is possible the agent
can run into infinite loops (e.g. keep bumping into the boundary), depending on the initial policy.
One simple method to tackle this issue is to have an artificial time limit, and if during one episode
the agent reaches this time limit, the episode is terminated and the agent receives a large, negative
reward. This allows deterministic agents to train effectively in these OPFF environments.
D.2.1 Cliff walking experimental results
We use performance and the average L1 distance to the optimal Q value as metrics for the con-
vergence rates of the multi-update and first-update MCES variants. Here the optimal Q values are
computed using value iteration. The full experimental results are shown in Figures 5, 6, 7 and 8. We
present results on gridworld sizes of 8 × 6, 12 × 9, 16 × 12 and wind probability = 0.1, 0.3, 0.5.
In Fig 5 and Fig 6, we consider the OPFF cliff walking environment, where time is not included in
the state space and the wind only affects the action to the right.
In Fig 7 and Fig 8, we consider the SFF cliff walking environment, where the current time step is
included as part of the state and the wind can affect the agent in all four directions.
Results in both cliff walking environments, and across all tested gridworld size and wind probabil-
ity consistently show that multi-update MCES learns faster than first-update MCES. These results
show there is indeed a significant performance difference between these two variants of the MCES
algorithm. We also observe a general trend that the performance gap tends to increase when the state
space becomes larger (a larger gridworld) and when we have more stochasticity in the environment
(a higher wind probability). In Figure 6 and 8, each row shows performance for a gridworld size and
each column shows a wind probability, if we go from the top left figure (figure (a)) to the bottom
15
Published as a conference paper at ICLR 2022
Figure 4: Cliff Walking Environment Illustration: The grid world contains of a start state, a goal
state and a cliff area between them. In the OPFF cliff walking setting with wind probability Pw,
when the agent is at P1 and moves one step to the right, it will take an additional step upwards or
downwards, each With probability Pw and not take this additional step With probability 1 - Pw. In
the SFF cliff walking environment with wind probability Pw, when the agent is at P2 and takes one
step up, it will take an additional step towards one of the four directions, each with probability Pw,
and take no additional step with probability 1 - Pw .
right figure (figure (i)), we see that the performance gap tends to become bigger. This result shows
that taking more updates to the value estimate can be important especially in complex tasks with
more randomness.
16
Published as a conference paper at ICLR 2022
(g) 16x12 grid, 10% wind	(h) 16x12 grid, 30% wind	(i) 16x12 grid, 50% wind
Figure 5: OPFF Cliff Walking: average L1 distance to optimal Q values, with different grid world
sizes and wind probabilities. Y-axis is in log scale.
17
Published as a conference paper at ICLR 2022
(g) 16x12 grid, 10% wind
(h) 16x12 grid, 30% wind
(i) 16x12 grid, 50% wind
Figure 6: OPFF Cliff Walking: Performance, with different grid world sizes and wind probabilities.
18
Published as a conference paper at ICLR 2022
(a) 8x6 grid, 10% wind
(b) 8x6 grid, 30% wind
O -EE⅛0 04->ta SqV
(e) 12x9 grid, 30% Wind
0	20000 40000 60000 80000100000
(c) 8x6 grid, 50% wind
(d) 12x9 grid, 10% wind
2.2 × IO1
2 × IO1
1.8 × IO1
1.6 × IO1
1.4 × IO1
(g) 16x12 grid, 10% wind
Ondo 03tQSqq
20000 40000 60000 80000100000
0 -raE⅛o 03tQSqq
⑴ 12x9 grid, 50% Wind
2.4 × IO1
2.2 × IO1
2 × IO1
1.8 × IO1
1.6 × IO1
(h)	16x12 grid, 30% wind
(i)	16x12 grid, 50% wind
Figure 7: SFF Cliff Walking: average L1 distance to optimal Q values, with different grid world
sizes and wind probabilities, y axis is in log scale.
19
Published as a conference paper at ICLR 2022
(g) 16x12 grid, 10% wind
(h) 16x12 grid, 30% wind
(i) 16x12 grid, 50% wind
Figure 8: SFF Cliff Walking: Performance, with different grid world sizes and wind probabilities.
20
Published as a conference paper at ICLR 2022
E Additional Experiments on Q-Learning
qIn this section, we additionally compare the multi-update MCES variant to the Q-learning algo-
rithm with different learning rates and discount factors. The results are summarized in Figure 9, the
training curves are averaged across a number of different gridworld sizes and wind probabilities.
Results show that in the CliffWalking environment, the optimal hyperparameters for Q-Learning are
different in each environment. With the right set of hyperparameters, Q-Learning can significantly
outperform the multi-update variant of MCES. This observation is consistent with the strong sample
efficiency of a large number of Q-Learning-based methods in recent deep reinforcement learning lit-
erature. Note that some of the most effective deep reinforcement learning methods, such as DrQv2
(Yarats et al., 2021), use a variant of the n-step TD method, which can be seen as a generalization
of both MC and one-step TD methods, as discussed in Chapter 7 of Sutton & Barto (2018). Popu-
lar on-policy deep reinforcement learning methods such as PPO (Schulman et al., 2017) also use a
technique called generalized advantage estimation, which can also be seen as a mix of MC and TD
methods. In offline deep reinforcement learning, some recent methods use Monte Carlo returns to
select a number of “best actions” and perform imitation learning, effectively avoiding some of the
unique issues in offline reinforcement learning (Chen et al., 2020).
These results show that it is important to improve our understanding of both MC and TD methods,
in order to develop new algorithms that are efficient and performant. Detailed results for each setting
are shown in Figure 10, Figure 11, Figure 12 and Figure 13.
(a) OPFF Cliff Walking,
Performance
(b) OPFF Cliff Walking,
average L1 distance to
optimal Q values
(c) SFF Cliff Walking,
Performance
(d) SFF Cliff Walking,
average L1 distance to
optimal Q values
Figure 9:	Performance and the average L1 distance to optimal Q values, on OPFF and SFF cliff
walking, for MCES with multi-update variant and Q-Learning with different learning rate (a) and
discount factor (g), the curves are averaged over different gridworld sizes and wind probability
settings.
21
Published as a conference paper at ICLR 2022
50000 IOOOOO 150000	200000	250000	300000
(a) 8x6 grid, 10% wind
(b) 8x6 grid, 30% wind	(c) 8x6 grid, 50% wind
O 50000 IOOOOO 150000	200000	250000	300000
(e)	12x9 grid, 30% wind
O 50000 IOOOOO 150000	200000	250000	300000
(f)	12x9 grid, 50% wind
O 50000 IOOOOO 150000	200000	250000	300000
(d)	12x9 grid, 10% wind


O -EE⅛0 04->ta SqV
(h) 16x12 grid, 30% wind
(g) 16x12 grid, 10% wind
——MCES-Multi
—Q-a0.001-g0.99
——Q-a0.01-g0.99
---Q-a0.l-g0.99
---Q-a0.001-g0.9
---Q-a0.01-g0.9
---Q-a0.1-g0.9
—




(i) 16x12 grid, 50% wind
Figure 10:	OPFF Cliff Walking: average L1 distance to optimal Q values, with different grid world
sizes and wind probabilities. Y-axis is in log scale.
22
Published as a conference paper at ICLR 2022
(b) 8x6 grid, 30% wind
(a) 8x6 grid, 10% wind
50000	100000	150000	200000	250000	300000
50000	100000	150000	200000	250000	300000
(c) 8x6 grid, 50% wind
50000	100000	150000	200000	250000	300000
(d) 12x9 grid, 10% wind
(g) 16x12 grid, 10% wind
(e) 12x9 grid, 30% wind
50000	100000	150000	200000	250000	300000
(h) 16x12 grid, 30% wind
(f) 12x9 grid, 50% wind
(i) 16x12 grid, 50% wind
Figure 11:	OPFF Cliff Walking: Performance, with different grid world sizes and wind probabilities.
23
Published as a conference paper at ICLR 2022
(c) 8x6 grid, 50% wind
(a) 8x6 grid, 10% wind
(b) 8x6 grid, 30% wind
(d) 12x9 grid, 10% wind
0 -EE⅛0 04->ta SqV
(g) 16x12 grid, 10% wind
(e) 12x9 grid, 30% wind
Xlo1loɪ
O-EEAdO 04->ta SqV
(h) 16x12 grid, 30% wind
Xlo1loɪ
O-EEAdO 04->ta SqV
(f) 12x9 grid, 50% wind
MCES-MuIti
——Q-a0.001-g0.99
——Q-a0.01-g0.99
----Q-a0.l-g0.99
---Q-a0.001-g0.9
---Q-a0.01-g0.9
---Q-a0.1-g0.9
50000 IOOOOO
(i) 16x12 grid, 50% wind
Figure 12:	SFF Cliff Walking: average L1 distance to optimal Q values, with different grid world
sizes and wind probabilities. Y-axis is in log scale.
24
Published as a conference paper at ICLR 2022
(a) 8x6 grid, 10% wind
(b) 8x6 grid, 30% wind
(c) 8x6 grid, 50% wind
(d) 12x9 grid, 10% wind
(e) 12x9 grid, 30% wind
(f) 12x9 grid, 50% wind
(g) 16x12 grid, 10% wind
50000	100000	150000	200000	250000	300000
(h) 16x12 grid, 30% wind
50000	100000	150000	200000	250000	300000
(i) 16x12 grid, 50% wind
Figure 13:	SFF Cliff Walking: Performance, with different grid world sizes and wind probabilities.
25
Published as a conference paper at ICLR 2022
F Counterexample
In this section, we present a counterexample in which the MCES algorithm (4) fails to converge with
a specific choice of exploring starts. The example is motivated by the counterexample in Bertsekas
& Tsitsiklis (1996), where the authors considered estimating the value function in a deterministic
continuing-task environment. In Bertsekas & Tsitsiklis (1996), the estimated value function cycles
in a fixed region during the iteration with a specific update rule and does not converge. However,
that example is not for an episodic task. Our example is concerned with estimating the Q-function
in an episodic MDP setting using the MCES algorithm. We can not directly apply the approach in
Bertsekas & Tsitsiklis (1996) to our problem without substantial modification. Moreover, in our
example, each generated episode and return are stochastic. So it is more difficult to confine the Q-
values in a specific region than the deterministic case. These features make our example substantially
different from the one in Bertsekas & Tsitsiklis (1996). Next, we formulate the episodic MDP and
describe the choice of exploring starts; then we show that the MCES algorithm (4) does not converge
with such a choice of exploring starts.
F.1 MDP Formulation
Let the state space be S = {1, 2, 3} where state 3 represents the terminal state, and action space
A = {m = move, s = stay}. At each state i with i = 1, 2, there are two actions, move to the
other state or stay. We assume that after taking each action, the system will transit to the terminal
state 3 with the same probability > 0, where is a small number. Therefore we have the following
transition probability:
p(2|1, m) = 1 - ,	p(1|1, s) = 1 -
p(1|2, m) = 1 - ,	p(2|2, s) = 1 -
p(3|i, m) = ,	p(3|i, s) = for i = 1, 2
We also assume the reward function r(i, a) to be:
r(i, m) = 0 for i = 1, 2
r(i, s) = -1 for i = 1,2
(22)
(23)
and the return G is the sum of the reward with the discounted factor 0 < γ < 1.
Figure 14: The MDP problem
From the above setting, we can easily see that there are four possible stationary policies:
(a)	At each state, always choose to move.
(b)	At state 1, move to state 2. At state 2, stay.
(c)	At state 1, stay. At state 2, move to state 1.
(d)	At each state, always choose to stay.
26
Published as a conference paper at ICLR 2022
We denote these four policies by πa , πb , πc, and πd, respectively. It is easy to see that πa is the
optimal policy, and the corresponding action-value function qπa is:
qπa (1, m) = 0, qπa (1, s) = -1
qπa (2, m) = 0, qπa (2, s) = -1
(24)
F.2 MECS algorithm with a specific exploring starts
We now apply the MCES algorithm4 to this MDP problem to estimate the optimal policy ∏*(s) and
the corresponding action-value function q*(s,a). Note that we have modified the algorithm a little
so that for each episode the Q-function is updated only for the initial state-action pair (S0, A0).
Algorithm 4 MCES
1:	Initialize: π(s) ∈ A, Q(s, a) ∈ R, for all s ∈ S, a ∈ A, arbitrarily;
Returns(s, a) J empty list, for all S ∈ S, a ∈ A.
2:	while True do
3:	Choose S0 ∈ S, A0 ∈ A s.t. all pairs are chosen infinitely often.
4:	Generate an episode following π: S0, A0, S1, A1, . . . , ST-1, AT-1, ST .
5:	GJ PtT=0 γtr(St, At)
6:	Append G to Returns(S0, A0)
7:	Q(S0, A0) J average(R(S0, A0))
8:	π(S0) J arg maxa Q(S0, a)
The estimates for ∏*(s) and q*(s, a) at the end of uth iteration are denoted by ∏u(s) and Qu(s, a),
respectively. For simplicity, we also denote Qu(1, m), Qu(1, s), Qu(2, m), and Qu(2, s) by Q1um,
Q1us, Q2um, and Q2us, respectively. Moreover, the vector (Qium, Qius) is denoted by
Qiu = (Qium,Qius), where i = 1,2
(25)
The policy π(i) is uniquely determined at each state i by comparing the paired value (Qium, Qius).
For example, when Qis > Qim, π(i) = stay. When Qis < Qim, π(i) = move. Note that once the
policy π is determined, the returns of the generated episodes are i.i.d. random variables. There are
total four possible cases by comparing paired Q-values (Q1s, Q1m) and (Q2s, Q2m). The trajectory
and return of each case generated by MCES algorithm 4 are listed in Table 1 and 2.
State 1	State 2	Initial start	Episode trajectory	Return
Q(l,s)>Q(l,m)	Q(2,s)>Q(2,m)	(LS)	l,s,l,s,l,s...	-l-γ-γ2...
		(Lm)	l,ml2,s,2,s..	0-γ-γ2...
		(2,s)	2.s,2,s,2,s..	-l-γ-γ2...
		(2,m)	2,InrLS,Ls …	Q-γ-γ 2...
Q(l,s)>Q(l,m)	Q(2,s)<Q(2.m)	(LS)	l.s,l,s,l,s...	-l-γ-γ2...
		(Im)	l,m,2,m,l,s,l,s...	Q-O-Y2-Y &...
		(2,s)	2,s.2,m,l,s,l,s...	-l-Q-γ2-γ &...
		(2,m)	2,InrLS,Ls …	Q-γ-γ2...
Table 1: Episode and return with different initial starts generated by the MCES algorithm when
Q(1, s) > Q(1, m)
27
Published as a conference paper at ICLR 2022
State 1	State 2	Initial start	Episode trajectory	Return
Q(LS)VQ(Lm)	Q(2,s)>Q(2,m)	(LS)	l,s,l,m,2ls,2ls...	-1-0-y 2-γ 3 ...
		(Im)	l,rn∣2,s,2ls...	o-r-y2-
		(2ts)	2,si2,s∣2,s...	-l-γ-γ 2 ...
		(2.m)	2,Γ∏∣1,∏∏,21S121S.1.	0-0-y 2-γ 3...
Q(LS)VQ(Lm)	Q(2ls)<Q(2,m)	(LS)	1,s,1,γπ,2iπi,1iiti...	-1
		(Lm)	l,rnl2,m,l∣m∣l,m...	0
		(2ts)	2,S12,∏∏,11∏∩,2,∏∩...	-1
		(2.m)	2,mll,nπ,2,m...	0
Table 2: Episode and return with different initial starts generated by the MCES algorithm when
Q(1, s) < Q(1, m)
Here we give the expected returns of several cases related to our counterexample.
Lemma 2. Consider an episode generated by the MCES algorithm 4:
(i)	When Q(1, s) < Q(1, m), Q(2, s) > Q(2, m), and the initial start is (1, m), or when Q(1, s) >
Q(1, m), Q(2, s) > Q(2, m), and the initial start is (2, m), the expected returns qπb (1, m) and
qπd (2, m) are the same and are equal to
-μι ：= E[G] = -MT)	(26)
1 - γ + γ
(ii) When Q(1, s) > Q(1, m), Q(2, s) < Q(2, m), and initial start is (2, s), or when Q(1, s) <
Q(1, m), Q(2, s) > Q(2, m), and the initial start is (1, s), the expected returns qπc (2, s) and
qπb (1, s) are the same and are equal to
-1
-μ2 ：= E[G] = I——+ Y(1 -E)	(27)
1	- γ + γ
(iii)	When Q(1, s) > Q(1, m) and the initial start is (1, s), or when Q(2, s) > Q(2, m) and the
initial start is (2, s), the expected return is
-1
-μ3 ：= E[G] =	-	(28)
1 - γ + γE
Moreover, we have μι < μ2 < μ3, and the variance of the return in each Case is finite.
Proof. Let AN be the event that a generated episode terminates after taking N + 1 actions. From
the formulation of our MDP problem, we have
P (AN) = E(1 - E)N, N ≥ 0	(29)
We first prove (i), in this case, the episode is of the form:
1, m, 2, s, 2, s, ... or 2, m, 1, s, 1, s, ...
the return of an episode that terminates after taking N + 1 actions is
so
∞
E[G] = X -
N=0
Y(I-YN) e(1-e)N
1-γ
γ(1-γ N)
—
1 - Y
N≥0
Eγ
(1 -γ)(1 -γ+γE)
-Y(I - e)
1 - Y + YE
(30)
(31)
—
士+
28
Published as a conference paper at ICLR 2022
(ii) In this case the episode is of the form
1, s, 1, m, 2, s, 2, s, ... or 2, s, 2, m, 1, s, 1, s...
the probability density of the return is
P(G= -1) = P(A0∪A1) = +(1 -)
2	N+1
P(G = -1 - Y -'	==P(AN) = e(1 - e)N ,for N ≥ 2
the expected return is
∞	2 N+1
E[G] = - - E(I - E) + £(-1------1--- )e(1 - E)N
N=2	γ
∞
-X
N=0
1 γN+1	∞
1 E(I - ON + YE X (1 - E)N
γ	N=1
- L +(1-Y )(1-Y + YE) + Y(I-E)
-1
+---------+ Y(I - E)
1 - Y + YE
(iii) In this case the possible episode is
1, s, 1, s, 1, s, ... or 2, s, 2, s, 2, s, ...
so the probability density of the return is
1 N+1
P(G = --1—) = P(AN) = e(1 -E)N, N ≥ 0
1-Y
and the expectation is
∞	1 N+1
E[G] = X —F—E(I-E)N
1-Y
N=0
=-ɪ + 7~U------------.
1 -Y (1 -Y)(1 -Y+YE)
_	-1
1 - Y + YE
Moreover, we have
Y(I - E) < ——1-γ(1 - e) < ——1——
1 - Y + YE 1 - Y + YE	1 - Y + YE
(32)
(33)
(34)
(35)
(36)
By a similar calculation, we can see that the second moment E[G2] of each case is finite. Therefore,
the variances are also finite.	□
Next, we describe specific regions in Q1m-Q1s and Q2m-Q2s planes. The two planes and the
corresponding regions are represented in Fig. 15a and Fig. 15b, respectively. In each figure, Qim -
axis and Qis-axis represent the value of Qium and Qius after uth iteration given the state i, where
i = 1, 2. Since π(i) = arg maxa Q(i, a), we can see that the line li
li : Qim = Qis	(37)
is the boundary for different policies π(i). For the vector (Qim, Qis) above the line li, we have
π(i) = stay. Similarly, for those below li, we have π(i) = move. In the Q1m-Q1s plane, we define
Rl = {(QUm, Qus) : QUm > Qus ,QUs < -μ2 + δ, Qum < -bl}	(38)
R2 = {(QUm, Qus) : Qus < -μ2 + δ, -bl < Qum < -1}	(39)
29
Published as a conference paper at ICLR 2022
R3={(Q1um,Q1us) : Q1um <Q1us,-b1 < Q1um < -1, Q1us <-1}
and in the Q2m-Q2s plane, we define
T = {(QUm, Qus) :-1 < Qum < 0, Qus < -μι + δ}
(40)
(41)
T2={(Q2um,Q2us) : -1 < Q2um <0,-1-δ<Q2us < -1}	(42)
T3 = {(Qum, Qus) : -μi - δ < Qum < -μ1 + δ, Qus > Qum, Qus < -1}	(43)
T4 = {(Qum, Qus) : -μi - δ < Qum < -μ1 + δ, Qus < Qum}	(44)
Here the parameters δ and b1 satisfy
0 <δ< min{	ʒ ,Ml , μ - 1 -	}----}, 2 < bi < min{μι, 2μ2 - 2δ ―	—} (45)
2	2	2(1 - γ)	1 - γ
The regions Ri and Ti are nonempty if γ and satisfy the following condition:
Condition 3.	1>2	(46)
and 1 1 - Y + Ye	-2(1 - Y) - Y(I - e) -1 >0	(47)
Remark 4. We claim that there exist γ and that satisfy Condition 3. Here we give one way to find
suitable Y and e. Let the ratio 1-γ = 10, then e = 1-0γ and
Y(I - e) = Y(9 + Y)
1 — γ + γe 10 — γ(9 + γ)
as γ approaches 1, 才二屋?)approaches infinity. So we can choose Y close to 1 (then e is close to
0) such that I-(Y+I > 2. Particularly, we can choose 0.7 < Y < 1 such that (46) holds. For the
second inequality, similarly
1	Y _ y2 + 10y - 20
1 - Y + Ye - 2(1 - Y) = 2(y2 +9y - 10)
it approaches to infinity as Y approaches 1. So we see that Condition 3 will hold for all 0.7 < Y < 1
when e = (1 - Y)/10.
(a) Q1m-Q1s
Figure 15: Ri and Ti are blue regions in Q1m-Q1s plane and Q2m-Q2s plane
(b) Q2m-Q2s
Recall that Q1u = (Q1um, Q1us) and Q2u = (Q2um, Q2us ). Assuming that Q1u and Q2u are initialized in
R1 and T1, respectively, we now describe the specific choice of exploring starts.
30
Published as a conference paper at ICLR 2022
Exploring starts 5. We initialize Q1u and Q2u to be in R1 and T1, respectively. We then repeat the
following 8 steps over and over again:
1.	Keep choosing (1, m) as the initial state-action pair and update Q(1, m) until Q1u enters
R2.
2.	Keep choosing (2, s) as the initial state-action pair and update Q(2, s) until Q2u enters T2.
3.	Keep choosing (1, s) as the initial state-action pair and update Q(1, s) until Q1u enters R3.
4.	Keep choosing (2, m) as the initial state-action pair and update Q(2, m) until Q2u enters
T3.
5.	Keep choosing (1, s) as the initial state-action pair and update Q(1, s) until Q1u enters R2.
6.	Keep choosing (1, m) as the initial state-action pair and update Q(1, m) until Q1u returns
to R1 .
7.	Keep choosing (2, s) as the initial state-action pair and update Q(2, s) until Q2u enters T4.
8.	Keep choosing (2, m) as the initial state-action pair and update Q(2, m) until Q2u returns
to T1.
Note that in each of 8 steps, only one of the values Q(1, m), Q(1, s), Q(2, m), and Q(2, s) changes.
All others remain constant.
The trajectories of Q1u and Q2u during the iteration are visualized in Fig. 16a and Fig. 16b, respec-
tively.
(a) The trajectory of Q1u
(b) The trajectory of Q2u
Figure 16: Trajectories of Q-values during the iteration
Next, we show that the MCES algorithm 4 does not converge with the exploring starts (5). Precisely,
we have the following result
Theorem 6. Given γ and satisfy the condition (3), suppose that Q1u and Q2u are initialized in R1
and T1, respectively. Following the MCES algorithm (4) with the exploring starts 5, the trajectory
of Q1u forms a cycle according to R1 → R2 → R3 → R2 → R1 almost surely, and the trajectory
of Q2u forms a cycle according to T1 → T2 → T3 → T4 → T1 almost surely. Thus, Qu (s, a) does
not converge with probability 1.
Proof. Step 1. Start with (1, m)
When Q1u ∈ R1 and Q2u ∈ T1, we have Q(1, s) < Q(1, m) and Q(2, s) < Q(2, m), the generated
episode is just
1,	m, 2, m, 1, m...
31
Published as a conference paper at ICLR 2022
so the return is always 0. During the iteration, Q1um increases and approaches zero according to
QumI = (I- -)QUm + 1∙ 0	(48)
m	n mn
Here n denote the number of stored Q1m-values after the (u + -)-th iteration. Since when Q1u ∈ R1,
Q1um < -b1 < -2, and n ≥ 2, we have
QumI = (I- n)Qum < (1 - n)(-2) < -1	(49)
So when Q1u ∈ R1, we have Q1um+1 < --, and Q1u will stay in R1 or R2 during the iteration. Since
Q1um approaches zero, after several updates, Q1u enters the region R2 .
Step 2. Start with (2, s)
When Q1u ∈ R2 and Q2u ∈ T1, the return with initial start (2, s) is always -1. So during the
iteration, Q2us will increase and approach -1. Eventually, Q2u will enter T2.
Step 3. Start with (1, s)
Similar to step 2, the return is always -1, and Q1us will approach -1. Since in R2 we have Q1um <
-1, Q1u will cross the line l1 and enter R3 after several updates.
Step 4. Start with (2, m)
In this case, Q1u ∈ R3 and Q2u ∈ T2, so every episode is generated by the same policy and is of the
form:
2,	m, 1, s, 1, s, ...
So the returns are i.i.d. random variables, and from lemma (2), we have
E[G] = -γ(1-e) = -μι	(50)
1 - γ + γ
Let n be the number of episodes generated in this step and Gi be their returns, then
Sn = G1 + G2 + ... + Gn
By the strong law of large numbers, we have
— —→ 一μι, a.s. (as n —→ +∞)	(51)
n
Assume that we have had t iterations before we start step 4, and there are L stored Return(2, m),
then
Qt+n = LQIm + Sn = LQm + Sn
Q2m L + n L + n + L + n
since Land Qt2m are finite numbers and independent of n, we have
lim Q2+n = -μι a.s.
n→∞
(52)
(53)
Therefore, when we keep choosing (2, m) as the initial start and updating Q(2, m), we will have
Q2+n < -μι + δ for some n. Thus, Qu will enter the region T3 almost surely after several iterations.
Note that even though the policy πu (2) changes after Q2u crosses the line l2, it does not affect the
return of the generated episode with the initial state-action pair (2, m) since only the first state of
the generated episode is state 2.
Step 5. Start with (1, s)
In this case, at first we have
Q1us > Q1um and Q2us > Q2um	(54)
therefore πu(1) = stay and πu(2) = stay. The generated episode with initial (1, s) is of the form:
1,s,1,s,1,s,...	(55)
and the returns are i.i.d. random variables with the expectation:
-1
E[G] = ;------------ = -μ3	(56)
1 - γ + γ
32
Published as a conference paper at ICLR 2022
Similar to the argument in step 4, when we keep updating Q1us by choosing (1, s) as the initial start,
Qus can not stay in R3 forever. It will approach -μ3. So Qu will cross the line l1 a.s.
After crossing the line l1 , Q1u will enter R2 or I, where I is the region
I = {(Qum, Qus): -bl < Qum < -1, Qum > Qus, Qus > -μ2 + δ}	(57)
If Q1u enters R2, we end this step and move to the next. Otherwise, we still keep updating Q1us. At
this moment, we have
Qlus < Qlum and Q2us > Q2um	(58)
the generated episode then becomes
1,s,1,m,2,s,2,s,2,s...	(59)
and the returns are i.i.d. random variables from another distribution with the expectation:
-1
E[G] = -------------+ Y(I - E) = -μ2	(6O)
1	- γ + γ
So if Qu stays in I during the iteration, Qus will approach -μ2. On the other hand, since We can
successively get returns equal to -1, Qlu might return to R3 . Then the generated episodes are of
type (55), Qlu will again cross the line ll after several updates. We claim that Qlu can not oscillate
between R3 and I forever. If so, we will get infinitely many sample returns with the expectation
either -μ2 or -μ3, the sample mean, Qus will be close to a convex combination of -μ2 and -μ3.
And We will have Qus < -μ2 + δ for some u. Therefore, Qu enters R2 after several updates
eventually.
Step 6. Start with (1, m)
The possible episode in this case is
1,	m, 2, s, 2, s, 2, s...
therefore the expectation of the return is -μι. During the iteration, Qum will converge to -μι. So
Qlum will decrease, and Qlu will return to Rl. Note that the sample return satisfies
G > - ɪ	(61)
1-γ
When Qu ∈ R2, the value of QumI is at least - ɪ (bi + ɪ-ɪY). By condition (3) and our assumptions
(45) on bl and δ, we have
QumI > -μ2 + δ	(62)
Therefore, when Qlu ∈ R2, Qlu+l will not cross the line ll during the iteration, and the policy πu
does not change. This ensures that we can get returns from the same distribution so that Qlu will
return to Rl .
Step 7. Start with (2, s)
In this case, we have Qlu ∈ Rl , and Q2u ∈ T3, so the possible episode is:
2,	s,2, s, 2, s...
The expected return is -μ3. Similar to the previous discussion, after several updates, Qu will cross
the line l2 and enter T4, eventually.
Step 8. Start with (2, m)
When Qlu ∈ Rl and Q2u ∈ T4, the return of an episode starts with (2, m) is always 0. So when we
keep updating Q2um, it will approach 0, and Q2u will return to Tl.
After Q2u returns to Tl, we go back to step 1 and follow the eight-step exploring starts (5) again.
We can see that during this process, all the state-action pairs can be chosen infinitely often, and the
Q-values will continue to alternate between these regions and not converge. Also, the policy does
not converge. Therefore, Algorithm (4) following the exploring starts (5) does not converge with
probability 1.	□
33