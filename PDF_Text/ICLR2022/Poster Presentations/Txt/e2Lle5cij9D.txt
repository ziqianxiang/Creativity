Published as a conference paper at ICLR 2022
Hidden Convexity of Wasserstein GANs:
Interpretable Generative Models
with Closed-Form S olutions
Arda Sahiner； Tolga Ergen； Batu Ozturkler, Burak Bartan, John Pauly, Morteza Mardani & Mert Pilanci
Department of Electrical Engineering
Stanford University
Stanford, CA 94305, USA
{sahiner,ergen,ozt,bbartan,pauly,morteza,pilanci}@stanford.edu
Ab stract
Generative Adversarial Networks (GANs) are commonly used for modeling com-
plex distributions of data. Both the generators and discriminators of GANs are
often modeled by neural networks, posing a non-transparent optimization prob-
lem which is non-convex and non-concave over the generator and discrimina-
tor, respectively. Such networks are often heuristically optimized with gradi-
ent descent-ascent (GDA), but it is unclear whether the optimization problem
contains any saddle points, or whether heuristic methods can find them in prac-
tice. In this work, we analyze the training of Wasserstein GANs with two-layer
neural network discriminators through the lens of convex duality, and for a va-
riety of generators expose the conditions under which Wasserstein GANs can
be solved exactly with convex optimization approaches, or can be represented
as convex-concave games. Using this convex duality interpretation, we further
demonstrate the impact of different activation functions of the discriminator. Our
observations are verified with numerical results demonstrating the power of the
convex interpretation, with applications in progressive training of convex archi-
tectures corresponding to linear generators and quadratic-activation discrimina-
tors for CelebA image generation. The code for our experiments is available at
https://github.com/ardasahiner/ProCoGAN.
1 Introduction
Generative Adversarial Networks (GANs) have delivered tremendous success in learning to generate
samples from high-dimensional distributions (Goodfellow et al., 2014; Cao et al., 2018; Jabbar et al.,
2021). In the GAN framework, two models are trained simultaneously: a generator G which attempts
to generate data from the desired distribution, and a discriminator D which learns to distinguish
between real data samples and the fake samples generated by generator. This problem is typically
posed as a zero-sum game for which the generator and discriminator compete to optimize objective f
p； = min max f (G, D).
The ultimate goal of the GAN training problem is thus to find a saddle point (also called a Nash
equilibrium) of the above optimization problem over various classes of (G, D). By allowing the
generator and discriminator to be represented by neural networks, great advances have been made in
generative modeling and signal/image reconstruction (Isola et al., 2017; Karras et al., 2019; Radford
et al., 2015; Wang et al., 2018; Yang et al., 2017). However, GANs are notoriously difficult to train,
for which a variety of solutions have been proposed; see e.g., (Nowozin et al., 2016; Mescheder et al.,
2018; Metz et al., 2016; Gulrajani et al., 2017).
One such approach pertains to leveraging Wasserstein GANs (WGANs) (Arjovsky et al., 2017),
which utilize the Wasserstein distance with the `1 metric to motivate a particular objective f . In
particular, assuming that true data is drawn from distribution px , and the input to the generator is
* Equal Contribution
1
Published as a conference paper at ICLR 2022
Table 1: Convex landscape and interpretation of WGAN with two-layer discriminator under different dis-
criminator activation functions and generator architectures. Note that adding a linear skip connection to the
discriminator imposes an additional mean matching constraint when using quadratic activation.
Discriminator Generator	Linear Activation	Quadratic Activation	ReLU Activation
Linear 2-layer (polynomial) 	2-layer (ReLU)	convex convex convex	convex, closed form convex, closed form convex	convex-concave convex-concave convex-concave
Interpretation	mean matching	covariance matching	piecewise mean matching
drawn from distribution pz , we represent the generator and discriminator with parameters θg and θd
respectively, to obtain the WGAN objective
P* =minmax Eχ~pχ[Dθd (X)] - Ez~pz[Dθd (Gθg (Z))].	(I)
θg θd
When G and D are neural networks, neither the inner max, nor, the outer min problems are convex,
which implies that min and max are not necessarily interchangeable. As a result, first, there is no
guarantees if the saddle points exists. Second, it is unclear to what extent heuristic methods such
as Gradient Descent-Ascent (GDA) for solving WGANs can approach saddle points. This lack of
transparency about the loss landscape of WGANs and their convergence is of paramount importance
for their utility in sensitive domains such as medical imaging. For instance, WGANs are commonly
used for magnetic resonance image (MRI) reconstruction (Mardani et al., 2018; Han et al., 2018),
where they can potentially hallucinate pixels and alter diagnostic decisions. Despite their prevalent
utilization, GANs are not well understood.
To shed light on explaining WGANs, in this work, we analyze WGANs with two-layer neural network
discriminators through the lens of convex duality and affirm that many such WGANs provably have
optimal solutions which can be found with convex optimization, or can be equivalently expressed as
convex-concave games, which are well studied in the literature (Zakovic & Rustem, 2003; Zakovic
et al., 2000; Tsoukalas et al., 2009; Tsaknakis et al., 2021). We further provide interpretation into the
effect of various activation functions of the discriminator on the conditions imposed on generated
data, and provide convex formulations for a variety of generator-discriminator combinations (see
Table 1). We further note that such shallow neural network architectures can be trained in a greedy
fashion to build deeper GANs which achieve state-of-the art for image generation tasks (Karras et al.,
2017). Thus, our analysis can be extended deep GANs as they are used in practice, and motivates
further work into new convex optimization-based algorithms for more stable training.
Contributions. All in all, the main contributions of this paper are summarized as follows:
•	For the first time, we show that WGAN can provably be expressed as a convex problem (or a
convex-concave game) with polynomial-time complexity for two-layer discriminators and two-layer
generators under various activation functions (see Table 1).
•	We uncover the effects of discriminator activation on data generation through moment matching,
where quadratic activation matches the covariance, while ReLU activation amounts to piecewise
mean matching.
•	For linear generators and quadratic discriminators, we find closed-form solutions for WGAN
training as singular value thresholding, which provides interpretability.
•	Our experiments demonstrate the interpretability and effectiveness of progressive convex GAN
training for generation of CelebA faces.
1.1	Related Work
The last few years have witnessed ample research in GAN optimization. While several divergence
measures (Nowozin et al., 2016; Mao et al., 2017) and optimization algorithms (Miyato et al., 2018;
Gulrajani et al., 2017) have been devised, GANs have not been well interpreted and the existence of
saddle points is still under question. In one of the early attempts to interpret GANs, (Feizi et al., 2020)
shows that for linear generators with Gaussian latent code and the 2nd order Wasserstein distance
2
Published as a conference paper at ICLR 2022
objective, GANs coincide with PCA. Others have modified the GAN objective to implicitly enforce
matching infinite-order of moments of the ground truth distribution (Li et al., 2017; Genevay et al.,
2018). Further explorations have yielded specialized generators with layer-wise subspaces, which
automatically discover latent “eigen-dimensions" of the data (He et al., 2021). Others have proposed
explicit mean and covariance matching GAN objectives for stable training (Mroueh et al., 2017).
Regarding convergence of Wasserstein GANs, under the fairly simplistic scenario of linear discrimi-
nator and a two-layer ReLU-activation generator with sufficiently large width, saddle points exist
and are achieved by GDA (Balaji et al., 2021). Indeed, linear discriminators are not realistic as
then simply match the mean of distributions. Moreover, the over-parameterization is of high-order
polynomial compared with the ambient dimension. For more realistic discriminators, (Farnia &
Ozdaglar, 2020) identifies that GANs may not converge to saddle points, and for linear generators
with Gaussian latent code, and continuous discriminators, certain GANs provably lack saddle points
(e.g., WGANs with scalar data and Lipschitz discriminators). The findings of (Farnia & Ozdaglar,
2020) raises serious doubt about the existence of optimal solutions for GANs, though finite parameter
discriminators as of neural networks are not directly addressed.
Convexity has been seldomly exploited for GANs aside from (Farnia & Tse, 2018), which studies
convex duality of divergence measures, where the insights motivate regularizing the discriminator’s
Lipschitz constant for improved GAN performance. For supervised two-layer networks, a recent
of line of work has established zero-duality gap and thus equivalent convex networks with ReLU
activation that can be solved in polynomial time for global optimality (Pilanci & Ergen, 2020; Sahiner
et al., 2020a; Ergen & Pilanci, 2021d; Sahiner et al., 2020b; Bartan & Pilanci, 2021; Ergen et al.,
2021). These works focus on single-player networks for supervised learning. However, extending
those works to the two-player GAN scenario for unsupervised learning is a significantly harder
problem, and demands a unique treatment, which is the subject of this paper.
1.2	Preliminaries
Throughout the paper, we denote matrices and vectors as uppercase and lowercase bold letters,
respectively. We use 0 (or 1) to denote a vector and matrix of zeros (or ones), where the sizes are
appropriately chosen depending on the context. We also use In to denote the identity matrix of size n.
For matrices, we represent the spectral, Frobenius, and nuclear norms as k • l∣2, k ∙ l∣F, and k • k*,
respectively. Lastly, we denote the element-wise 0-1 valued indicator function and ReLU activation
as l[x ≥ 0] and (x)+ = max{χ, 0}, respectively.
In this paper, we consider the WGAN training problem as expressed in equation 1. We consider
the case of a finite real training dataset X ∈ Rnr ×dr which represents the ground truth data from
the distribution we would like to generate data. We also consider using finite noise Z ∈ Rnf ×df
as the input to the generator as fake training inputs. The generator is given as some function
Gθg : Rdf → Rdr which maps noise from the latent space to attempt to generate realistic samples
using parameters θg, while the discriminator is given by Dθd : Rdr → R which assigns values
depending on how realistically a particular input models the desired distribution, using parameters θd .
Then, the primary objective of the WGAN training procedure is given as
P* = minmax 1>Dθd(X)- 1>Dθd (Ge, (Z)) + Rg(θg) - Rd(θd),	(2)
θg θd
where Rg and Rd are regularizers for generator and discriminator, respectively. We will analyze
realizations of discriminators and generators for the saddle point problem via convex duality. One
such architecture is that of the two-layer network with md neurons and activation σ, given by
md
Dθd(X) = Xσ(Xuj)vj1.
j=1
Two activation functions that we will analyze in this work include polynomial activation σ(t) =
at2 + bt + c (of which quadratic and linear activations are special cases where (a, b, c) = (1, 0, 0)
and (a, b, c) = (0, 1, 0) respectively), and ReLU activation σ(t) = (t)+ . As a crucial part of our
convex analysis, we first need to obtain a convex representation for the ReLU activation. Therefore,
we introduce the notion of hyperplane arrangements similar to (Pilanci & Ergen, 2020).
1In the case of networks with bias, one can write Dθd (X) = Pjm=1 σ(Xuj + 1bj)vj .
3
Published as a conference paper at ICLR 2022
Hyperplane arrangements. We define the set of hyperplane arrangements as Hx := {diag(l[Xu ≥
0]) : u ∈ Rdr }, where each diagonal matrix Hx ∈ Hx encodes whether the ReLU activation is
active for each data point for a particular hidden layer weight u. Therefore, for a neuron u, the output
of the ReLU activation can be expressed as (Xu)+ = HxXu, with the additional constraint that
(2Hx - Inr) Xu ≥ 0. Further, the set of hyperplane arrangements is finite, i.e. |Hx| ≤ O(r(nr/r)r),
where r := rank(X) ≤ min(nr, dr) (Stanley et al., 2004; Ojha, 2000). Thus, we can enumerate all
possible hyperplane arrangements and denote them as Hx = {H(xi)}|iH=1x|. Similarly, one can consider
the set of hyperplane arrangements from the generated data as {H(gi)}|iH=1g|, or of the noise inputs to
the generator: {H(zi)}|iH=1z|. With these notions established, we now present the main results2.
2	Overview of Main Results
As a discriminator, we consider a two-layer neural network with appropriate regularization, md
neurons, and arbitrary activation function σ . We begin with the regularized problem
md	md
p* = min max X 1>σ(XUj) - 1>σ(Gθg (Z)Uj) vj + Rg(θg) - βd X |vj |	⑶
θg vj,kujk2≤1j=1	j=1
with regularization parameter βd > 0. This problem represents choice ofRd corresponding to weight-
decay regularization in the case of linear or ReLU activation, and cubic regularization in the case of
quadratic activation (see Appendix) (Neyshabur et al., 2014; Pilanci & Ergen, 2020; Bartan & Pilanci,
2021). Under this model, our main result is to show that with two-layer ReLU-activation generators,
the solution to the WGAN problem can be reduced to convex optimization or a convex-concave game.
Theorem 2.1. Consider a two-layer ReLU-activation generator of the form Gθg (Z) = (ZW1)+W2
with mg ≥ nfdr + 1 neurons, where W1 ∈ Rdf ×mg and W2 ∈ Rmg ×dr. Then, for appropriate
choice of regularizer Rg = kGθg (Z)k2F, for any two-layer discriminator with linear or quadratic
activations, the WGAN problem equation 3 is equivalent to the solution of two successive convex
optimization problems, which can be solved in polynomial time in all dimensions for noise inputs
Z of a fixed rank. Further, for a two-layer ReLU-activation discriminator, the WGAN problem is
equivalent to a convex-concave game with coupled constraints.
In practice, GANs are often solved with low-dimensional noise inputs Z, limiting rank(Z) and
enabling polynomial-time trainability. A particular example of the convex formulation of the WGAN
problem in the case of a quadratic-activation discriminator can be written as
G* =argmin ∣∣G∣∣F s.t. ∣∣X>X - G>G∣∣2 ≤ βd	(4)
G
W；, W =argmin ∣∣W1∣∣F + ∣∣W2∣∣F s.t. G* = (ZW1)+W2,	(5)
W1 ,W2
where the solution G* to equation 4 can be found in polynomial-time via singular value thresholding,
formulated exactly as G* = L(Σ2 一 βdI)y2V> for any orthogonal matrix L, where X = UΣV>
is the SVD of X. While equation 5 does not appear convex, it has been shown that its solution is
equivalent to a convex program (Ergen & Pilanci, 2021a; Sahiner et al., 2020a), which for the norm
kSkKi,* := mint≥0 t s.t. S ∈ tconv{Z = hgT : (2H(zi) - Inf)ZU ≥ 0, kZk* ≤ 1} is expressed as
|Hz|	|Hz|
{Vi*}|iH=1z| = arg min X kVikKi,* s.t. G* = XH(zi)ZVi,	(6)
Vi i=1	i=1
The optimal solution to equation 6 can be found in polynomial-time in all problem dimensions when
Z is fixed-rank, and can construct the optimal generator weights W1*, W2* (see Appendix C.1). This
WGAN problem can thus be solved in two steps: first, it solves for the optimal generator output; and
second, it parameterizes the generator with ReLU weights to achieve the desired generator output.
In the case of ReLU generators and ReLU discriminators, we find equivalence to a convex-concave
game with coupled constraints, which we discuss further in the Appendix (Zakovic & Rustem, 2003).
For certain simple cases, this setting still reduces to convex optimization.
2All the proofs and some extensions are presented in Appendix.
4
Published as a conference paper at ICLR 2022
Theorem 2.2. In the case of 1-dimensional (dr = 1) data {xi}in=1 where nr = nf = n, a two-layer
ReLU-activation generator, and a two-layer ReLU-activation discriminator with bias, with arbitrary
choice of convex regularizer Rg (w), the WGAN problem can be solved by first solving the following
convex optimization problem
w* = arg min Rg (W) s.t.
w∈Rn
2n	j
X Si(Xi- Xj) ≤ βd, X Si(Xj- Xi) ≤ βd,∀j ∈ [2n]
i=j	i=1
(7)
and then the parameters of the two-layer ReLU-activation generator can be found via
|Hz|	|Hz|
{(ui*,vi*)}|iH=1z| = arg min X kui k2 + kvi k2 s.t. w* = X H(zi)Z(ui - vi),
ui,vi ∈Ci i=1	i=1
where
〜	(xb i+1 c,	if i is odd	++1, if i is odd	w. rn 1
Xi =(	“一 , si = V 1 √■—	, ∀i ∈ [2n]
Iw i, If i is even	1—1, if i is even
forconvexsets Ci, given that the generator has mg ≥ n +1 neurons and βd ≤ minij∈[n]"=j∙ ∣Xi -Xj |.
This demonstrates that even the highly non-convex and non-concave WGAN problem with ReLU-
activation networks can be solved using convex optimization in polynomial time when Z is fixed-rank.
In the sequel, we provide further intuition about the forms of the convex optimization problems found
above, and extend the results to various combinations of discriminators and generators. In the cases
that the WGAN problem is equivalent to a convex problem, if the constraints of the convex problem
are strictly feasible, the Slater’s condition implies Lagrangian of the convex problem provably has a
saddle point. We thus confirm the existence of equivalent saddle point problems for many WGANs.
3	Two-Layer Discriminator Duality
Below, we provide novel interpretations into two-layer discriminator networks through convex duality.
Lemma 3.1. The two-layer WGAN problem equation 3 is equivalent to the following optimization
problem
p* = minRg(θg) s.t. max ∣1>σ(Xu) — 1>σ(Gθ (Z)u)∣ ≤ βd.	(8)
θg	kuk2≤1	g
One can enumerate the implications of this result for different discriminator activation functions.
3.1	Linear-activation Discriminators Match Means
In the case of linear-activation discriminators, the expression in equation 8 can be greatly simplified.
Corollary 3.1. The two-layer WGAN problem equation 3 with linear activation function σ(t) = t is
equivalent to the following optimization problem
p* =minRg(θg) s.t. k1>X - 1> Gθg (Z)k2 ≤ βd.	(9)
Linear-activation discriminators seek to merely match the means of the generated data Gθg (Z) and
the true data X, where parameter βd controls how strictly the two must match. However, the exact
form of the generated data depends on the parameterization of the generator and the regularization.
3.2	Quadratic-activation Discriminators Match Covariances
For a quadratic-activation network, we have the following simplification.
Corollary 3.2. The two-layer WGAN problem equation 3 with quadratic activation function σ(t) =
t2 is equivalent to the following optimization problem
p* =minRg(θg)s.t. kX>X-Gθg(Z)>Gθg(Z)k2 ≤βd.	(10)
5
Published as a conference paper at ICLR 2022
In this case, rather than an Euclidean norm constraint, the quadratic-activation network enforces
fidelity to the ground truth distribution with a spectral norm constraint, which effectively matches
the empirical covariance matrices of the generated data and the ground truth data. To combine
the effect of the mean-matching of linear-activation discriminators and covariance-matching of
quadratic-activation discriminators, one can consider a combination of the two.
Corollary 3.3. The two-layer WGAN problem equation 3 with quadratic activation function σ(t) =
t2 with an additional unregularized linear skip connection is equivalent to the following problem
p* = min Rg(θg) s.t.
θg	g g
kX>X-Gθg(Z)>Gθg(Z)k2≤βd
1>X=1>Gθg(Z)
(11)
This network thus forces the empirical means of the generated and true distribution to match exactly,
while keeping the empirical covariance matrices sufficiently close. Skip connections therefore provide
additional utility in WGANs, even in the two-layer discriminator setting.
3.3 ReLU-activation Discriminators Match Piecewise Means
In the case of the ReLU activation function, we have the following scenario.
Corollary 3.4. The two-layer WGAN problem equation 3 with ReLU activation function σ(t) = (t)+
is equivalent to the following optimization problem
p*=min Rg (θg )s.t.	ιmaχι	∣(l>Hj1)X - 1>Hgj2)Gθg (Z))U∣ ≤ βd, ∀ji,j2∙
θg	kuk2 ≤1
(2H(xj1)-Inr)Xu≥0
(2H(gj2) -Inf)Gθg (Z)u≥0
(12)
The interpretation of the ReLU-activation discriminator relies on the concept of hyperplane arrange-
ments. In particular, for each possible way of separating the generated and ground truth data with a
hyperplane U (which is encoded in the patterns specified by Hx and Hg), the discriminator ensures
that the means of the selected ground truth data and selected generated data are sufficiently close
as determined by βd . Thus, we can characterize the impact of the ReLU-activation discriminator
as piecewise mean matching. Thus, unlike linear- or quadratic-activation discriminators, two-layer
ReLU-activation discriminators can enforce matching of multi-modal distributions.
4	Generator Parameterization and Convexity
Beyond understanding the effect of various discriminators on the generated data distribution, we can
also precisely characterize the WGAN objective for multiple generator architectures aside from the
two-layer ReLU generators discussed in Theorem 2.1, such as for linear generators.
Theorem 4.1.	Consider a linear generator of the form Gθg (Z) = ZW. Then, for arbitrary choice
of convex regularizer Rg (W), the WGAN problem for two-layer discriminators can be expressed as
a convex optimization problem in the case of linear activation, as well as in the case of quadratic
activation provided rank(Z) is sufficiently large and Rg = βg ∣∣Gθg (Z) k F. In the case ofa two-layer
discriminator with ReLU activation, the WGAN problem with arbitrary choice of convex regularizer
Rg (W) is equivalent to a convex-concave game with coupled constraints.
We can then discuss specific instances of the specific problem at hand. In particular, in the case of a
linear-activation discriminator, the WGAN problem with weight decay on both discriminator and
generator is equivalent to the following convex program
p* =min βIWkF s.t. ∣1>X - 1>ZW∣2 ≤ βd.	(13)
W2
In contrast, for a quadratic-activation discriminator with regularized generator outputs,
p* ≥ d* =min β∣G∣F s.t. ∣X>X - G>G∣2 ≤ βd,	(14)
G2
6
Published as a conference paper at ICLR 2022
where G = ZW, with p* = d* under the condition that rank(Z) is sufficiently large. In particular,
allowing the SVD of X = UΣV>, we define k = maxk02≥βd k, and note that if rank(Z) ≥ k,
equality holds in (14) and a closed-form solution for the optimal generator weights exists, given by
W* = (Z>Z)-2(Σ2 - βdI) + V>.
(15)
Lastly, for arbitrary convex regularizer Rg, the linear generator, ReLU-activation discriminator
problem can be written as the following convex-concave game
p* =mn	maχ	Rg(W) - βd E(IIrjij21∣2 + krj∙ιj21∣2)
W rj1 ,j2,r0j1j2	j1,j2
(16)
+ X	1>H(xji)X - 1>H(gj2)ZW	(rjij2 -r0jij2)
ji,j2
(2H(xji) -In)Xrjij2 ≥0, (2H(gj2) - In)ZWrjij2 ≥0
s.t.	,
(2H(xji) -In)Xr0jij2 ≥0, (2H(gj2) - In)ZWr0jij2 ≥ 0
∀jι ∈ [|Hx|],∀j2 ∈ [|Hg|],
where we see there are bi-linear constraints which depend on both the inner maximization and the
outer minimization decision variables. We now move to a more complex form of generator, which is
modeled by a two-layer neural network with general polynomial activation function.
Theorem 4.2.	Consider a two-layer polynomial-activation generator of the form Gθg (Z) =
σ(ZW1)W2 for activation function σ(t) = at2 + bt + c with fixed a, b, c ∈ R. Define
Zi = [vec(ziz>)> bz> c] as the lifted noise data points, in which case Gθg (Z) = ZW.
Then, for arbitrary choice of convex regularizer Rg (W), the WGAN problem for two-layer discrimi-
nators can be expressed as a convex optimization problem in the case of linear activation, as well as
in the case of quadratic activation provided rank(Z) is sufficiently large and Rg = ∣∣Gθg (Z)IlF. In
the case of a two-layer discriminator with ReLU activation, the WGAN problem with arbitrary choice
of convex regularizer Rg (W) is equivalent to a convex-concave game with coupled constraints.
Under the parameterization of lifted noise features, a two-layer polynomial-activation generator
behaves entirely the same as a linear generator. The effect of a polynomial-activation generator is thus
to provide more heavy-tailed noise as input to the generator, which provides a higher dimensional
input and thus more degrees of freedom to the generator for modeling more complex data distributions.
5 Numerical Examples
5.1	ReLU-activation Discriminators
0.5
-01
0
.5	-1	。5	0	0.5	1	1.5
(a) βd = 0.1
0.5
(b) Generator space
0
-0.5
-1.5	-1	。5	0	0.5	1	1.5
(c) βd = 1
(d) Generator space
Figure 1: Numerical illustration of Theorem 2.2 for ReLU generator/discriminator with 1D data
x = [-1, 1]T and Rg(w) = ∣w∣22. For βd = 0.1, we observe that the constraint set of the convex
program in equation 17 is a convex polyhedron shown in (b) and the optimal generator output is the
vertex w1 = (-1 + βd) and w2 = 1 - βd. In contrast, for βd = 1, the constraint set in (d) is the
larger scaled polyhedra and includes the origin. Therefore, the optimal generator output becomes
w1 = w2 = 0, which corresponds to the overlapping points in (c) and demonstrates mode collapse.
We first verify Theorem 2.2 to elucidate the power of the convex formulation of two-layer ReLU
discriminators and two-layer ReLU generators in a simple setting. Let us consider a toy dataset with
7
Published as a conference paper at ICLR 2022
Figure 2: A modified architecture for progressive training of convex GANs (ProCoGAN). At each stage i, a
linear generator Wi is used to model images at a given resolution Xi , attempting to fool quadratic-activation
discriminator Di, for which the optimal solution can be found in closed-form via equation 15. Once stage i is
trained, the input to stage i + 1 is given as the output of the previous stage with learned weights Wl, which is
then used to model higher-resolution images Xi+1. The procedure continues until high-resolution images can
be generated from successive application of linear generators.
the data samples x = [-1, 1]T3. Then, the convex program can be written as
min Rg (w) s.t.
w∈R2
4
X Si (Xi - Xj)
i=j
j
≤ βd, X Si(Xj- Xi) ≤ βd,∀j ∈ [4].
i=1
Substituting the data samples, the simplified convex problem becomes
min Rg (w) s.t.	|w1	+	w2|	≤ βd,	|w2	- 1| ≤ βd,	|w1 + 1| ≤	βd.	(17)
w∈R2
As long as Rg (w) is convex in w, this is a convex optimization problem. We can numerically solve
this problem with various convex regularization functions, such as Rg (w) = kwkpp for p ≥ 1.
We visualize the results in Figure 1. Here, we observe that when βd = 0.1, the constraint set is
a convex polyhedron and the optimal generator outputs are at the boundary of the constraint set,
i.e., w1 = (-1 + βd) and w2 = 1 - βd. However, selecting βd = 1 enlarges the constraint set such
that the origin becomes a feasible point. Thus, due to having Rg (w) = kwk22 in the objective, both
outputs get the same value w1 = w2 = 0, which demonstrates the mode collapse issue.
5.2	Progressive Training of Linear Generators and Quadratic Discriminators
Here, we demonstrate a proof-of-concept example for the simple covariance-matching performed by a
quadratic-activation discriminator for modeling complex data distributions. In particular, we consider
the task of generating images from the CelebFaces Attributes Dataset (CelebA) (Liu et al., 2015),
using only a linear generator and quadratic-activation discriminator. We compare the generated
faces from our convex closed-form solution in equation 15 with the ones generated using the original
non-convex and non-concave formulation. GDA is used for solving the non-convex problem.
We proceed by progressively training the generators layers. This is typically used for training
GANs for high-resolution image generation (Karras et al., 2017). The training operates in stages
of successively increasing the resolution. In the first stage, we start with the Gaussian latent code
Z ∈ Rnf ×df and locally match the generator weight W1 to produce samples from downsampled
distribution of images X1 . The second stage then starts with latent code Z2 , which is the upsampled
version of the network output from the previous stage ZW：. The generator weight W2 is then trained
to match higher resolution X2 . The procedure repeats until full-resolution images are obtained. Our
approach is illustrated in Figure 2. The optimal solution for each stage can be found in closed-form
using equation 15; we compare using this closed-form solution, which we call Progressive Convex
GAN (ProCoGAN), to training the non-convex counterpart with Progressive GDA.
3See Appendix for derivation, where we also provide an example with the data samples x = [-1, 0, 1]T.
8
Published as a conference paper at ICLR 2022
(a) ProCoGAN (Ours). Top: (βd(4), βd(5)) = (7.2×103, 1.0×104)
Bottom: (βd(4),βd(5))=(1.9×104,3.3×104)
(b) Progressive GDA (Baseline)
Figure 3: Representative generated faces from ProCoGAN and Progressive GDA with stagewise training of
linear generators and quadratic-activation discriminators on CelebA (Figure 2). ProCoGAN only employs the
closed-form expression equation 15, where βd controls the variation and smoothness in the generated images.
In practice, the first stage begins with 4×4 resolution RGB images, i.e. X1 ∈ Rnr ×48 , and at each
successive stage we increase the resolution by a factor of two, until obtaining the final stage of
64 × 64 resolution. For ProCoGAN, at each stage i, we use a fixed penalty βd(i) for the discriminator,
while GDA is trained with a standard Gradient Penalty (Gulrajani et al., 2017). At each stage, GDA
is trained with a sufficiently wide network with m(di) = (192, 192, 768, 3092, 3092) neurons at each
stage, with fixed minibatches of size 16 for 15000 iterations per stage. As a final post-processing
step to visualize images, because the linear generator does not explicitly enforce pixel values to be
feasible, for both ProCoGAN and the baseline, we apply histogram matching between the generated
images and the ground truth dataset (Shen, 2007). For both ProCoGAN and the baseline trained on
GPU, we evaluate the wall-clock time for three runs. While ProCoGAN trains for only 153 ± 3
seconds, the baseline using Progressive GDA takes 11696 ± 81 seconds to train. ProCoGAN is
much faster than the baseline, which demonstrates the power of the equivalent convex formulation.
We also visualize representative freshly generated samples from the generators learned by both
approaches in Figure 3. We keep (βd(1), βd(2), βd(3)) fixed, and visualize the result of training two
different sets of values of (βd(4) , βd(5)) for ProCoGAN. We observe that ProCoGAN can generate
reasonably realistic looking and diverse images. The trade off between diversity and image quality can
be tweaked with the regularization parameter βd . Larger βd generate images with higher fidelity but
with less degree of diversity, and vice versa (see more examples in Appendix B.2). Note that we are
using a simple linear generator, which by no means compete with state-of-the-art deep face generation
models. The interpretation of singular value thresholding per generator layer however is insightful to
control the features playing role in face generation. Further evidence and more quantitative evaluation
is provided in Appendix B.2. We note that the progressive closed-form approach of ProCoGAN may
also provide benefits in initializing deep non-convex GAN architectures for improved convergence
speed, which has precedence in the greedy layerwise learning literature (Bengio et al., 2007).
6	Conclusions
We studied WGAN training problem under the setting of a two-layer neural network discriminator,
and found that for a variety of activation functions and generator parameterizations, the solution can
be found via either a convex program or as the solution to a convex-concave game. Our findings
indicate that the discriminator activation directly impacts the generator objective, whether it be
mean matching, covariance matching, or piecewise mean matching. Furthermore, for the more
complicated setting of ReLU activation in both two-layer generators and discriminators, we establish
convex equivalents for one-dimensional data. To the best of our knowledge, this is the first work
providing theoretically solid convex interpretations for non-trivial WGAN training problems, and
even achieving closed-form solutions in certain relevant cases. In the light of our results and existing
convex duality analysis for deeper networks, e.g., Ergen & Pilanci (2021b;c); Wang et al. (2021), we
conjecture that a similar analysis can also be applied to deeper networks and other GANs.
9
Published as a conference paper at ICLR 2022
7	Acknowledgements
This work was partially supported by the National Science Foundation under grants ECCS-2037304,
DMS-2134248, the Army Research Office, and the National Institutes of Health under grants
R01EB009690 and U01EB029427.
8	Ethics and Reproducibility S tatements
This paper aims to provide a complete theoretical characterization for the training of Wasser-
stein GANs using convex duality. Therefore, we believe that there aren’t any ethical con-
cerns regarding our paper. For the sake of reproducibility, we provide all the experimental
details (including preprocessing, hyperparameter optimization, extensive ablation studies, hard-
ware requirements, and all other implementation details) in Appendix B as well as the source
(https://github.com/ardasahiner/ProCoGAN) to reproduce the experiments in the pa-
per. Similarly, all the proofs and explanations regarding our theoretical analysis and additional
supplemental analyses can be found in Appendices C, D, E, and F.
References
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In International conference on machine learning, pp. 214-223. PMLR, 2017.
Yogesh Balaji, Mohammadmahdi Sajedi, Neha MUkUnd Kalibhat, Mucong Ding, Dominik Stoger,
Mahdi Soltanolkotabi, and Soheil Feizi. Understanding overparameterization in generative adver-
sarial networks. arXiv preprint arXiv:2104.05605, 2021.
Burak Bartan and Mert Pilanci. Neural spectrahedra and semidefinite lifts: Global convex op-
timization of polynomial activation neural networks in fully polynomial-time. arXiv preprint
arXiv:2101.02429, 2021.
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training of
deep networks. In Advances in neural information processing systems, pp. 153-160, 2007.
Yang-Jie Cao, Li-Li Jia, Yong-Xia Chen, Nan Lin, Cong Yang, Bo Zhang, Zhi Liu, Xue-Xiang Li,
and Hong-Hua Dai. Recent advances of generative adversarial networks in computer vision. IEEE
Access, 7:14985-15006, 2018.
Antonin Chambolle and Thomas Pock. A first-order primal-dual algorithm for convex problems with
applications to imaging. Journal of mathematical imaging and vision, 40(1):120-145, 2011.
Tolga Ergen and Mert Pilanci. Convex geometry of two-layer relu networks: Implicit autoencoding
and interpretable models. In Silvia Chiappa and Roberto Calandra (eds.), Proceedings of the
Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of
Proceedings of Machine Learning Research, pp. 4024-4033. PMLR, 26-28 Aug 2020. URL
https://proceedings.mlr.press/v108/ergen20a.html.
Tolga Ergen and Mert Pilanci. Convex geometry and duality of over-parameterized neural networks.
Journal of Machine Learning Research, 22(212):1-63, 2021a. URL http://jmlr.org/
papers/v22/20-1447.html.
Tolga Ergen and Mert Pilanci. Global optimality beyond two layers: Training deep relu networks via
convex programs. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International
Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp.
2993-3003. PMLR, 18-24 Jul 2021b. URL https://proceedings.mlr.press/v139/
ergen21a.html.
Tolga Ergen and Mert Pilanci. Path regularization: A convexity and sparsity inducing regularization
for parallel relu networks. CoRR, abs/2110.09548, 2021c. URL https://arxiv.org/abs/
2110.09548.
10
Published as a conference paper at ICLR 2022
Tolga Ergen and Mert Pilanci. Implicit convex regularizers of cnn archi-tectures: Convex optimization
of two-and three-layer networks in polynomial time. In International Conference on Learning
Representations, 2021d.
Tolga Ergen and Mert Pilanci. Revealing the structure of deep neural networks via convex duality. In
Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine
Learning, volume 139 of Proceedings of Machine Learning Research, pp. 3004-3014. PMLR,
18-24 Jul 2021e. URL https://Proceedings .mlr.press∕v139∕ergen21b.html.
Tolga Ergen, Arda Sahiner, Batu Ozturkler, John Pauly, Morteza Mardani, and Mert Pilanci. Demys-
tifying batch normalization in relu networks: Equivalent convex optimization models and implicit
regularization. arXiv preprint arXiv:2103.01499, 2021.
Farzan Farnia and Asuman Ozdaglar. Do gans always have nash equilibria? In International
Conference on Machine Learning, pp. 3029-3039. PMLR, 2020.
Farzan Farnia and David Tse. A convex duality framework for gans. arXiv preprint arXiv:1810.11740,
2018.
Soheil Feizi, Farzan Farnia, Tony Ginart, and David Tse. Understanding gans in the lqg setting:
Formulation, generalization and stability. IEEE Journal on Selected Areas in Information Theory,
1(1):304-311, 2020.
Aude Genevay, Gabriel Peyra and Marco CUtUrL Learning generative models with sinkhorn di-
vergences. In International Conference on Artificial Intelligence and Statistics, pp. 1608-1617.
PMLR, 2018.
Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint
arXiv:1406.2661, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved
training of wasserstein gans. arXiv preprint arXiv:1704.00028, 2017.
Changhee Han, Hideaki Hayashi, Leonardo Rundo, Ryosuke Araki, Wataru Shimoda, Shinichi
Muramatsu, Yujiro Furukawa, Giancarlo Mauri, and Hideki Nakayama. Gan-based synthetic brain
mr image generation. In 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI
2018), pp. 734-738. IEEE, 2018.
Zhenliang He, Meina Kan, and Shiguang Shan. Eigengan: Layer-wise eigen-learning for gans. arXiv
preprint arXiv:2104.12476, 2021.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. arXiv preprint
arXiv:1706.08500, 2017.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with
conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 1125-1134, 2017.
Abdul Jabbar, Xi Li, and Bourahla Omar. A survey on generative adversarial networks: Variants,
applications, and training. ACM Computing Surveys (CSUR), 54(8):1-49, 2021.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for
improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 4401-4410, 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
11
Published as a conference paper at ICLR 2022
ChUn-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabds P6czos. Mmd gan:
Towards deeper understanding of moment matching network. arXiv preprint arXiv:1705.08584,
2017.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV), December 2015.
Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least
squares generative adversarial networks. In Proceedings of the IEEE international conference on
computer vision, pp. 2794-2802, 2017.
Morteza Mardani, Enhao Gong, Joseph Y Cheng, Shreyas S Vasanawala, Greg Zaharchuk, Lei Xing,
and John M Pauly. Deep generative adversarial neural networks for compressive sensing mri. IEEE
transactions on medical imaging, 38(1):167-179, 2018.
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do
actually converge? In International conference on machine learning, pp. 3481-3490. PMLR,
2018.
Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial
networks. arXiv preprint arXiv:1611.02163, 2016.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Youssef Mroueh, Tom Sercu, and Vaibhava Goel. Mcgan: Mean and covariance feature matching
gan. In International conference on machine learning, pp. 2527-2535. PMLR, 2017.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. arXiv preprint arXiv:1606.00709, 2016.
Piyush C Ojha. Enumeration of linear threshold functions from the lattice of hyperplane intersections.
IEEE Transactions on Neural Networks, 11(4):839-850, 2000.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. arXiv preprint arXiv:1912.01703, 2019.
Mert Pilanci and Tolga Ergen. Neural networks are convex regularizers: Exact polynomial-time
convex optimization formulations for two-layer networks. In International Conference on Machine
Learning, pp. 7695-7705. PMLR, 2020.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Arda Sahiner, Tolga Ergen, John Pauly, and Mert Pilanci. Vector-output relu neural network problems
are copositive programs: Convex analysis of two layer networks and polynomial-time algorithms.
arXiv preprint arXiv:2012.13329, 2020a.
Arda Sahiner, Morteza Mardani, Batu Ozturkler, Mert Pilanci, and John Pauly. Convex regularization
behind neural reconstruction. arXiv preprint arXiv:2012.05169, 2020b.
Alexander Shapiro. Semi-infinite programming, duality, discretization and optimality conditions.
Optimization, 58(2):133-161, 2009.
Dinggang Shen. Image registration by local histogram matching. Pattern Recognition, 40(4):
1161-1172, 2007.
Richard P Stanley et al. An introduction to hyperplane arrangements. Geometric combinatorics, 13:
389-496, 2004.
12
Published as a conference paper at ICLR 2022
Ioannis Tsaknakis, Mingyi Hong, and Shuzhong Zhang. Minimax problems with coupled lin-
ear constraints: Computational complexity, duality and solution methods. arXiv preprint
arXiv:2110.11210, 2021.
Angelos Tsoukalas, Berg Rustem, and Efstratios N Pistikopoulos. A global optimization algorithm
for generalized semi-infinite, continuous minimax with coupled constraints and bi-level problems.
Journal of Global Optimization, 44(2):235-250, 2009.
Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy.
Esrgan: Enhanced super-resolution generative adversarial networks. In Proceedings of the Euro-
pean Conference on Computer Vision (ECCV) Workshops, pp. 0-0, 2018.
Yifei Wang, Tolga Ergen, and Mert Pilanci. Parallel deep neural networks have zero duality gap.
CoRR, abs/2110.06482, 2021. URL https://arxiv.org/abs/2110.06482.
Guang Yang, Simiao Yu, Hao Dong, Greg Slabaugh, Pier Luigi Dragotti, Xujiong Ye, Fangde Liu,
Simon Arridge, Jennifer Keegan, Yike Guo, et al. Dagan: Deep de-aliasing generative adversarial
networks for fast compressed sensing mri reconstruction. IEEE transactions on medical imaging,
37(6):1310-1321, 2017.
Stanislav Zakovic and Berc Rustem. Semi-infinite programming and applications to minimax
problems. Annals of Operations Research, 124(1):81-110, 2003.
Stanislav Zakovic, Costas Pantelides, and Berc Rustem. An interior point algorithm for computing
saddle points of constrained continuous minimax. Annals of Operations Research, 99(1):59-77,
2000.
13
Published as a conference paper at ICLR 2022
Appendix
Table of Contents
A Notation Summary	15
B Experimental Details and Additional Numerical Examples	15
B.1	ReLU-activation Discriminators ......................................... 15
B.2	Progressive Training of Linear Generators and Quadratic Discriminators . 17
C Additional Theoretical Results	21
C.1 Convexity and Polynomial-Time Trainability of Two-Layer ReLU Generators . .	21
C.2 Norm-Constrained Discriminator Duality .................................. 23
C.3 Generator Parameterization for Norm-Constrained Discriminators .......... 25
D Overview of Main Results	26
D.1 Derivation of the Form in equation 3 .................................... 26
D.2 Proof of Theorem 2.1 .................................................... 27
D.3 Note on Convex-Concave Games with Coupled Constraints ................... 29
D.4 Proof of Theorem 2.2 .................................................... 30
E Two-Layer Discriminator Duality	31
E.1 Proof of Lemma 3.1 ...................................................... 31
E.2	Proof of Corollary 3.1 .................................................. 31
E.3	Proof of Corollary 3.2 .................................................. 31
E.4	Proof of Corollary 3.3 .................................................. 32
E.5	Proof of Corollary 3.4 .................................................. 32
F Generator Parameterization and Convexity	32
F.1	Proof of Theorem 4.1 .................................................. 32
F.2	Proof of Theorem 4.2 .................................................. 34
14
Published as a conference paper at ICLR 2022
A Notation S ummary
Here, we provide a table summarizing the notation used in this paper for clarity.
Table 2: Notation used throughout the paper.
Symbol	Meaning
~θg-	Generator parameters
θd	Discriminator parameters
Gθg	Generator function
Dθd	Discriminator function
X	Ground truth data
Z	Noise inputs to generator
Rg	Generator regularization function
Rd	Discriminator regularization function
βg	Generator regularization parameter
βd	Discriminator regularization parameter
nr	Number of samples from X
dr	Dimension of samples from X
nf	Number of samples from Z
df	Dimension of samples from Z
Hx	Hyperplane arrangements of X
Hz	Hyperplane arrangements of Z
r	rank(X)
rz	rank(Z)
mg	Generator hidden layer neurons
md	Discriminator hidden layer neurons
Uj	First-layer weight of discriminator neuron j
Vj	Second-layer weight of discriminator neuron j
Wi	First-layer generator weight matrix
W2	Second-layer generator weight matrix
B Experimental Details and Additional Numerical Examples
B.1 ReLU-activation Discriminators
We first provide some non-convex experimental results to support our claims in Theorem 2.2. For this
case, we use a WGAN with two-layer ReLU network generator and discriminator with the parameters
(mg, md, βd, μ) = (150,150,10-3,4e - 6). We then train this architecture on the same dataset in
Figure 1. As illustrated in Figure 4, depending on the initialization seed, the training performance for
the non-convex architecture might significantly change. However, whenever the non-convex approach
achieves a stable training performance its results match with our theoretical predictions in Theorem
2.2.
In order to illustrate how the constraints in Theorem 2.2 change depending on the number of data
samples, below, we analyze a case with three data samples.
Let us consider a toy dataset with the data samples x = [-1, 0, 1]T. Then, the convex program can
be written as
min Rg (w) s.t.
w∈R3
6	j
Esi(Xi - Xj) ≤ βd, Esi(Xj - Xi) ≤ βd,∀j ∈ [6].
i=j	i=1
(18)
Substituting the data samples, the simplified convex problem admits
min Rg (w) s.t.
w∈R3
|w1 + w2 + w3 | ≤ βd,
|1 - (w2 + w3)| ≤ βd, |w1 + w2 + 1| ≤ βd,	(19)
|w3 - 1| ≤ βd, |w1 + 1| ≤ βd
15
Published as a conference paper at ICLR 2022
+ Real data
°Q4-	X Generator output
0.02-
0.00- X X +	+
-0.02 -
-0.04-
-2.5 -2.0 -1.5 -1.0 -0.5 0.0 0.5 1.0
(a) Trial#1 - 1D illustration
+ Real data
°Q4	X Generator output
0.02
OQO米	米
-0.02 -
-0.04 -
-1.0	-0.5	0.0	0.5	1.0
050505050
NLL0.0.SLL2.
- - - -
① n-(u> ①>yΛqo
(b) Trial#1 - Loss curves
5 0 5 0 5 0
Qo.SLLZ
- - - -
Bn-S 9 >-t;①(qo
(d) Trial#2 - Loss curves
(c) Trial#2 - 1D illustration
Figure 4: Non-convex architecture trained on the dataset in Figure 1 using the Adam optimizer with
(mg, md, βd, μ) = (150,150,10-3,4e - 6). Unlike our stable convex approach, the non-convex
training is unstable and leads to undamped oscillations depending on the initialization. In particular,
for Trial#1 ((a) and (b)), we obtain unstable training so that the generator is unable to capture the
trend in the real data. However, in Trial#2 ((c) and (d)), the non-convex architecture is able to match
the real data as predicted our theory in Theorem 2.2.
which exhibits similar trends (compared to the case with two samples in Figure 1) as illustrated in
Figure 5.
Proof. To derive the convex form, we begin with equation 18 and simplify to:
j j	1 2	| - (w1 + 1) + 1 - (w2 + 1) + 2 - (w3 + 1)| ≤ βd	0 ≤ βd | -	w1	- (w2	-	w1)	+ (1 -	w1)	-	(w3	-	w1)|	≤	βd	|w1 + 1| ≤	βd	
j	3	| - w2 + 1 - w3 | ≤ βd	|1 + w1| ≤ βd
j	4	|(1 - w2) - (w3 - w2)| ≤ βd	|w2 - (w2 - w1) + (w2 + 1)| ≤ βd
j	5	|w3 - 1| ≤ βd	|2 - (1 - w1) + 1 - (1 - w2)| ≤ βd
j=6 0≤βd Simplifying the constraints above yield j = 1 |w1 + w2 + w3| ≤ βd j = 2	|1 - (w2	+ w3)|	≤	βd j = 3	|1 - (w2	+ w3)|	≤	βd j = 4	|w3 - 1|	≤ βd j = 5	|w3 - 1|	≤ βd j =6 0 ≤βd			|(w3 + 1) - (w3 - w1) + w3 - (w3 - w2 ) + (w3 - 1)| ≤ βd . 0≤βd |w1 + 1| ≤ βd |w1 + 1| ≤ βd |w1 + w2 + 1| ≤ βd |w1 + w2 + 1| ≤ βd |w1 + w2 + w3 | ≤ βd .
which can further be simplified to the expression in equation 19.
□
16
Published as a conference paper at ICLR 2022
0.4
0.2
0
米
-0.2
-0.4
■+*
+ real
× fake
(b) Generator space
-1.5	-1	-0.5	0	0.5	1	1.5
0.5
(a) βd = 0.1
0
(c) βd = 1
-0.5
-1.5	-1	-0.5	0	0.5	1	1.5
(d) Generator space
Figure 5: Numerical illustration of Theorem 2.2 for ReLU generator/discriminator with 1D data
x = [-1, 0, 1]T andRg(w) = kwk22.
B.2 Progressive Training of Linear Generators and Quadratic Discriminators
The CelebA dataset is large-scale face attributes dataset with 202599 RGB images of resolution
218 × 178, which is allowed for non-commercial research purposes only. For this work, we take
the first 50000 images from this dataset, and re-scale images to be square at size 64 × 64 as the
high-resolution baseline X5 ∈ R50000×12288. All images are represented in the range [0, 1]. In order
to generate more realistic looking images, we subtract the mean from the ground truth samples prior
to training and re-add it in visualization. The inputs to the generator network Z ∈ R50000×48 are
sampled from i.i.d. standard Gaussian distribution.
For the Progressive GDA baseline, we train the networks using Adam (Kingma & Ba, 2014),
with α = 1e - 3, β1 = 0, β2 = 0.99 and = 10-8, as is done in (Karras et al., 2017).
Also following (Karras et al., 2017), we use WGAN-GP loss with parameter λ = 10 and an
additional penalty edriftEx〜px[D(x)2], where edrift = 10-3. Also following (Karras et al., 2017),
for visualizing the generator output, we use an exponential running average for the weights of the
generator with decay 0.999. For progressive GDA, similar to the ProCoGAN formulation, we
penalize the outputs of the generator G with penalty βg kGk2F for some regularization parameter
βg. For the results in the main paper, we let βg(i) = 100/d(ri) where d(ri) is the dimension
of the real data at each stage i. At each stage of the progressive process, the weights of the
previous stages are held constant and not fine-tuned, so as to match the architecture of ProCo-
GAN. We plot the loss curves of the final stage of the baseline in Figure 6 to demonstrate convergence.
We emphasize that the results of Progressive GDA as shown in this paper are not identical
to the original progressive training formulation of (Karras et al., 2017), with many key differences
which prevent our particular architecture from generating state-of-the-art images on par with
(Karras et al., 2017). Many key aspects of (Karras et al., 2017) are not captured by the architecture
studied in this work, including: using higher-resolution ground truth images (up to 1024 × 1024),
17
Published as a conference paper at ICLR 2022
Figure 6: Loss curves of the final 64 × 64 stage of training of the non-convex generator and non-
concave discriminator as trained with the baseline Progressive GDA method as used in the main
paper, for images shown in Figure 3. Discriminator fake loss corresponds to the total network output
over the fake images, while real loss corresponds to the negative of the total network output over the
real images, output penalty corresponds to the e"iftEx〜px [D(x)2] penalty, gradient penalty refers
to the GP loss with λ = 10, discriminator loss is the sum over all of the discriminator losses, and
generator loss corresponds to the negative of the discriminator fake loss.
18
Published as a conference paper at ICLR 2022
Table 3: FID results of progressive training of linear generators and two-layer quadratic-activation
discriminators using both the convex approach and the non-convex baseline. Results are reported
over three runs.
Method
FID
Progressive GDA (Baseline)	194.1 ±4.5
ProCoGAN(Ours): (βd4),βd5)) = (7.2×103,1.0×104)	128.4 ± 0.4
ProCoGAN(Ours): (βd4),βd5)) = (1.9×104, 3.3×104厂	147.1 ±2.4
'JΓ
(b) Progressive GDA (Baseline)
Figure 7:	Representative generated faces at 4 × 4 resolution from ProCoGAN and Progressive GDA with
stagewise training of linear generators and quadratic-activation discriminators on CelebA (Figure 2).
progressively growing the discriminator as well as the generator, using convolutional layers rather
than fully-connected layers, using leaky-ReLU activation rather than linear or quadratic-activation,
fusing the outputs of different resolutions, and fine-tuning the weights of previous stages when a new
stage is being trained. The objective of this experiment is not to replicate (Karras et al., 2017) exactly
with a convex algorithm, but rather to simply demonstrate a proof-of-concept for the effectiveness
of our equivalent convex program as an alternative to standard GDA applied to the non-concave
and non-convex original optimization problem, when both approaches are applied to the same
architecture of a linear generator and quadratic-activation two-layer discriminator.
For ProCoGAN, for both of the sets of faces visualized in the main paper, we arbitrarily choose
(βd(1), βd(2), βd(3)) = (206, 1.6 × 103, 5.9 × 103). βd(i) are in general chosen to truncate ki singular
values of Xi = Ui ΣiVi, where ki can be varied.
Both methods are trained with Pytorch (Paszke et al., 2019), where ProCoGAN is trained
with a single 12 GB NVIDIA Titan Xp GPU, while progressive GDA is trained with two of them.
For numerical results, We use Frechet Inception Distance (FID) as a metric (HeUsel et al., 2017),
generated from 1000 generated images from each model compared to the 50000 ground-truth images
used for training, reported over three runs. We display our results in Table 3. We find that loW values
of βd seem to improve the FID metric for ProCoGAN, and these greatly outperform the baseline
in terms of FID in both cases. In addition, to shoW further the progression of the greedy training,
for both ProCoGAN and Progressive GDA in the settings described in the main paper, We shoW
representative outputs of each trained generator at each stage of training in Figures 7, 8, 9, and 10.
Further, We ablate the values of βd(i) for ProCoGAN to shoW in an even more extreme case the tradeoff
betWeen smoothness and diversity, and ablate βg(i) in the case of ProgressiveGDA, Which provides a
similar tradeoff, as We shoW in Figure 11.
19
Published as a conference paper at ICLR 2022
XjHMΩi:G 球 A
(a) ProCoGAN (Ours)
(b) Progressive GDA (Baseline)
Figure 8:	Representative generated faces at 8 × 8 resolution from ProCoGAN and Progressive GDA with
stagewise training of linear generators and quadratic-activation discriminators on CelebA (Figure 2).
(a) ProCoGAN (Ours)
(b) Progressive GDA (Baseline)
Figure 9: Representative generated faces at 16 × 16 resolution from ProCoGAN and Progressive GDA with
stagewise training of linear generators and quadratic-activation discriminators on CelebA (Figure 2).
(a) ProCoGAN (Ours). Top: βd(4) = 7.2×103
Bottom: βd(4) = 1.9×104
(b) Progressive GDA (Baseline)
Figure 10: Representative generated faces at 32 × 32 resolution from ProCoGAN and Progressive GDA with
stagewise training of linear generators and quadratic-activation discriminators on CelebA (Figure 2). ProCoGAN
only employs the closed-form expression equation 15, where βd controls the variation and smoothness in the
generated images.
20
Published as a conference paper at ICLR 2022
(a) ProCoGAN (Ours). Top: βd(i) = (1.3 × 103,2.7 × 103,9.0 × 103,2.6 × 104,6.4 × 104)
Bottom: βd(i) = (51, 557, 2.9 × 103,5.3 × 103, 6.2 × 103)
(b) Progressive GDA (Baseline). Top: βg(i) = 10/d(ri)
Bottom: βg(i) = 1000/d(ri)
Figure 11: Effect of βd(i) on generated faces from ProCoGAN and effect of βg(i) on generated faces from
Progressive GDA with stagewise training of linear generators and quadratic-activation discriminators on CelebA
(Figure 2). ProCoGAN only employs the closed-form expression equation 15, where βd controls the variation
and smoothness in the generated images, which can clearly be seen in the extreme example here. We also see
that βg has a similar effect for Progressive GDA, where high values of βg make output images less noisy but
also less diverse.
C Additional Theoretical Results
C.1 Convexity and Polynomial-Time Trainability of Two-Layer ReLU
Generators
In this section, we re-iterate the results of (Sahiner et al., 2020a) for demonstrating an equivalent
convex formulation to the generator problem equation 5:
W；, W2 =argmin ∣∣Wι∣∣F + ∣∣W2∣∣F s.t. G* = (ZWι)+W
W1,W2
In the case of ReLU-activation generators, this form appears in many of our results and proofs. Thus,
we establish the following Lemma.
Lemma C.1. The non-convex problem equation 5 is equivalent to the following convex optimization
problem
|Hz|	|Hz|
{Vi*}|iH=1z| = arg min kVikKi,* s.t. G* =	H(zi)ZVi
Vi i=1	i=1
for kVikKi,* := mint≥0 t s.t. Vi ∈ tconv{Z = hgT : (2H(zi) -Inf)Zu ≥ 0, kZk* ≤ 1}, provided
that the number OfneUrOns mg ≥ nf dr + 1. Further, this problem has complexity O(nrf (nf )3rf),
where rf := rank(Z).
Proof. We begin by re-writing equation 5 in terms of individual neurons:
mg	mg
min X kujk22 + kvjk22 s.t.G* = X(Zuj)+vj>.
uj ,vj
j=1	j=1
Then, we can restate the problem equivalently as (see D.1):
mg	mg
min X kvjk2 s.t.G* = X(Zuj)+vj>.
kujk2≤1,vjj=1	j=1
(20)
(21)
21
Published as a conference paper at ICLR 2022
Then, we take the dual of this problem as in (Ergen & Pilanci, 2021a; 2020; Sahiner et al., 2020a).
First, form the Lagrangian
min
kuj k2 ≤1,vj
mg	mg
mRxX kvjk2 +tr(R>G*) - Xtr(RT(ZUj)+v>).
j=1	j=1
(22)
Then, by Sion’s minimax theorem, we can exchange the minimum over v and maximum over R, to
obtain
mg	mg
min 1 maχmvin∑S kvj k2+tr(R>G*)- Etr(R>(zuj )+v>).
kujk2≤1	j j=1	j=1
(23)
Minimizing this over v, we obtain the equivalent problem
min maxtr(R>G*) s.t.kR>(Zuj) + k2 ≤ 1 ∀j ∈ [mg].	(24)
kujk2≤1 R
Under the condition mg ≥ nfdr + 1, we obtain the equivalent semi-infinite strong dual problem
maxtr(R>G*)s.t.kR>(Zu)+k2 ≤ 1 ∀ku∣∣2 ≤ 1.	(25)
R
This over-parameterization requirement arises from the argument that this semi-infinite dual constraint
can be supported by at most mg ≤ nf d + 1 neurons u, and thus if mg ≥ mg, strong duality holds
(see Lemma 5 of (Sahiner et al., 2020a), Section 3 of (Shapiro, 2009)). This problem can further be
re-written as
max tr(R>G*) s.t. max kR>(ZU)+k2 ≤ 1.	(26)
R	kuk2≤1
Using the concept of dual norm, we introduce the variable w to obtain the equivalent problem
maxtr(R>G*) s.t. max w>R>(ZU)+ ≤ 1.	(27)
R	kuk2≤1	+
kwk2≤1
Then, we enumerate over all potential sign patterns to obtain
maxtr(R>G*) s.t.	max	w>R>H(i)ZU ≤ 1,	(28)
R	kuk2≤1	z
kwk2≤1
i∈[∣Hi)∣]
(2H(zi) -Inf)Zu≥0
which we can equivalently write as
maxtr(R>G*) s.t. max	(R, H(i)Zuw>i ≤ 1,	(29)
R	kuk2≤1	z
kwk2≤1
i∈[∣HZi) ∣]
(2H(zi)-Inf)Zu≥0
which can further be simplified as
maxtr(R>G*) s.t.	max	(R, Hzi)ZVii ≤ 1 ∀i ∈ [|Hz|].	(30)
R	kVikKi,*≤1	一
We note that unlike the set of rank-one matrices satisfying the affine constraint when parameterized by
Uw>, the matrices Vi ∈ Ki are not necessarily rank-one, and can in fact be full-rank. In particular,
Ki is the convex hull of rank-one matrices for which the left factors satisfy the ith affine ReLU
constraint. We then take the Lagrangian problem
maxmintr(R>G*) + X %(1 — max	(R, H(i)ZVii),
R λ≥0	i=1	kVikKi,*≤1	z
or equivalently
|Hz|
maxmin min	tr(R>G*) + X % — %(R, Hzi)ZV排
R λ≥0 kVikKi,*≤1	i=1
(31)
(32)
22
Published as a conference paper at ICLR 2022
By Sion’s minimax theorem, we can change the order of the maximum and minimum. Then,
maximizing over R leads to
min min
λ≥0 ∣Mkκi,*≤1
|Hz|
Xλi
i=1
s.t. G*
|Hz|
X λiH(zi)ZVi.
i=1
(33)
Lastly, we note that this is equivalent to
|Hz|	|Hz|
argmin X 12屁,* s.t. G* = X Hzi)ZVi
Vi i=1	i=1
(34)
as desired. The computational complexity of this problem is given as O(nrf (nf )3rf), where
rf := rank(Z) (see Table 1 of (Sahiner et al., 2020a)). The general intuition behind this complexity
is that this problem can be solved with a Frank-Wolfe algorithm, each step of which requires
the solution to |Hz| ≤ O(rf (nrf /df)rf) subproblems, each of which has complexity O(nrf ).
To obtain the weights to the original problem equation 5, we factor Vi* = Pjd=r 1 hi*j gi*j where
(2H(zi) - Inf)Zhi*j ≥ 0 and kgi*j k2 = 1, and then form
(w1*ij , w2*ij )
(qfe，gj E
,i∈ [|Hz|], j ∈ [dr]
as the ijth row of W1* and ijth column ofW2*, respectively. Re-substituting these into equation 5
obtains a feasible point with the same objective as the equivalent convex program equation 6.	□
C.2 Norm-Constrained Discriminator Duality
In this section, we consider the discriminator duality results in light of weight norm constraints, rather
than regularization, and find that many of the same conclusions hold. In order to model a 1-Lipschitz
constraint, we can use the constraint {Pj|vj| ≤ 1, kujk2 ≤ 1}. Then, for a linear-activation
discriminator, for any data samples a, b, we have
mm	m
|Xa>ujvj- X b>ujvj| = | X a>uj - b> ujvj|
j=1	j=1	j=1
≤ max	a>uj - b>uj
kuj k2 ≤1
= ka - bk2.
Thus, { j|vj| ≤ 1, kujk2 ≤ 1} implies 1-Lipschitz for linear-activation discriminators. For
discriminators with other activation functions, we use the same set of constraints as well.
Lemma C.2. A WGAN problem with norm-constrained two-layer discriminator, of the form
*
p*
min max
θg Pj |vj l≤1,kuj k2≤1
m
X 1>σ(Xuj) - 1>σ(Gθg(Z)uj) vj+Rg(θg)
j=1
with arbitrary non-linearity σ, can be expressed as the following:
*
p*
min max
θg kuk2≤1
1>σ(Xu)-1>σ(Gθg(Zu))+Rg(θg)
Proof. We first note that by the definition of the dual norm, we have
m
max	cjvj= max cTv = kck∞ = max |cj |.
Pj∣vj∣≤1 j=ι	kvkι≤1	j∈[m]
Using this observation, we can simply maximize with respect to vjto obtain
p* = min	max	1>σ(Xuj) - 1>σ(Gθg (Z)uj) +Rg(θg)
θg j∈[m],kuj k2≤1
23
Published as a conference paper at ICLR 2022
which we can then re-write as
*
P
min max
θg kuk2≤1
1>σ(Xu)-1>σ(Gθg(Z)u)+Rg(θg)
as desired.
□
Corollary C.1. A WGAN problem with norm-constrained two-layer discriminator with linear activa-
tions σ(t) = t can be expressed as the following:
p* =mink1>X- 1>Gθg(Z)k2+Rg(θg).
θg
Proof. Start with the following
p* = min max 1>Xu - 1>Gθg(Z)u +Rg(θg).
θg kuk2≤1
Solving over the maximization with respect to u obtains the desired result:
p* =mink1>X- 1>Gθg(Z)k2+Rg(θg).
θg
□
Corollary C.2. A WGAN problem with norm-constrained two-layer discriminator with quadratic
activations σ(t) = t2 can be expressed as the following:
p* =minkX>X-Gθg(Z)>Gθg(Z)k2+Rg(θg).
θg
Proof. Start with the following
p* = min max 1>(Xu)2 - 1>(Gθg(Z)u)2 +Rg(θg),
θg kuk2≤1
which we can re-write as
p* = min max
θg kuk2≤
X-Gθg(Z)>Gθg(Z)iu+Rg(θg).
Solving the maximization over u obtains the desired result
p* =mθinkX>X-Gθg(Z)>Gθg(Z)k2+Rg(θg).
□
Corollary C.3. A WGAN problem with norm-constrained two-layer discriminator with ReLU activa-
tions σ(t) = (t)+ can be expressed as the following:
*
p*
min	max
θg	j1∈[lHxl]
jw ∈[lHgU
kuk2≤1
(2H(xj1)-Inr)Xu≥0
(2H(gj2) -Inf)Gθg (Z)u≥0
1>H(xj1)Xu-1>H(gj2)Gθg(Z)u+Rg(θg).
Proof. We start with
*
p*
mθgin kumka2≤x1 1>(Xu)+ - 1>(Gθg (Z)u)+
+ Rg(θg).
Now, introducing sign patterns of the real data and generated data, we have
*
p*
min	max
θg	j1 HHx|]
j2E[1Hg1]
kuk2≤1
(2H(xj1)-Inr)Xu≥0
(2H(gj2)-Inf)Gθg(Z)u≥0
1>H(xj1)Xu-1>H(gj2)Gθg(Z)u+Rg(θg)
as desired.
□
24
Published as a conference paper at ICLR 2022
C.3 Generator Parameterization for Norm-Constrained Discriminators
Throughout this section, we utilize the norm constrained discriminators detailed in Section C.2.
C.3.1 LINEAR GENERATOR (σ(t) = t)
Linear-activation discriminator. For a linear generator and linear-activation norm-constrained
discriminator (see Corollary C.1 for details), we have
p* = min maxl(1>X - 1>ZW) U + Rg(W)
= mink1>X- 1>ZWk2 + Rg(W).
For arbitrary choice of convex regularizer Rg (W), this problem is convex.
Quadratic-activation discriminator (σ(t) = t2). For a linear generator and quadratic-activation
norm-constrained discriminator (see Corollary C.2 for details), we have
p* = min ∣∣X>X - (ZW)>ZW∣∣2 + Rg(W).	(35)
W
If rank(Z) ≥ rank(X), with appropriate choice of Rg(W) = βg ∣ZW∣2F, we can write this as
p* =min∣X>X-G∣2+βg∣G∣*,	(36)
which is convex. With a symmetric solution G* to the above, we can factor it into G* = H>H,
and solve the system H = ZW* to find the optimal original generator weight W*, which when
substituted into the original objective in equation 35 will obtain the same objective value. We
note that if rank(X) > rank(Z), a valid solution W* is not guaranteed because the linear system
H = ZW* has no solutions if rank(H) > rank(Z). However, since rank(H) ≤ rank(X), as long
as rank(Z) ≥ rank(X), we will be able to exactly find original weights W* from G*, and the two
problems are equivalent.
ReLU-activation discriminator (σ(t) = (t)+).For a linear generator and ReLU-activation norm-
constrained discriminator (see Corollary C.3 for details), we have
p* = min	max	1>H(j1)XU - 1>H(j2)ZWU + Rg(W)|
W	jι∈[∣Hχ∣] I x	g	g
jw ∈[lHg |]
kuk2≤1
(2H(xj1)-Inr)Xu≥0
(2H(gj2) -Inf )ZWu≥0
For arbitrary choice of convex regularizer Rg(W), this is a convex-concave problem with coupled
constraints, as in the weight-decay penalized case.
C.3.2 Polynomial-activation Generator
All of the results of the linear generator section hold, with lifted features (see proof of Theorem 4.2).
C.3.3 ReLU-activation Generator
Linear-activation discriminator (σ(t) = t). With standard weight decay, we have
p* = min ∣1>X - I>(ZW1)+W2k2 + β (∣∣W1kF + IW2kF)∙
W1 ,W2	2
We can write this as a convex program as follows. For the output of the network (ZW1)+W2, the
fitting term is a convex loss function. From (Sahiner et al., 2020a), we know that this is equivalent to
the following convex optimization problem
|Hz|	|Hz|
p*=mVin∣1>X-1>XH(zi)ZVi∣2+βgX∣Vi∣Ki,*
i	i=1	i=1
25
Published as a conference paper at ICLR 2022
where Ki := conv{ug> : (2Hzi)- Inf )Zu ≥ 0, ∣∣g∣∣2 ≤ 1} and ∣∣Vi∣∣Ki,* := mint≥0 t s.t. Vi ∈
tKi.
Quadratic-activation discriminator (σ(t) = t2). We have
P* = min kX>X - ((ZW1)+W2)>(ZW1 )+W2k2 + Rg(Wι, W2).
W1,W2
For appropriate choice of regularize] Rg (Wι, W2) = βg ∣∣(ZW1)+W2kF and mg ≥ nf dr + 1,
we can write this as
G* = argmin ∣X>X - G>G∣2 + 工∣G∣F
W1,W2	2
W1*,W2* =argmin∣W1∣2F+∣W2∣2Fs.t. G* = (ZW1)+W2.
W1,W2
The latter of which we can re-write in convex form as shown in Lemma C.1:
|Hz|	|Hz|
{Vi*}|iH=1z| = arg min X ∣Vi∣Ki,* s.t. G* = XH(zi)ZVi
Vi i=1	i=1
for convex sets	Ki	:= conv{ug>	:	(2H(zi)	-	Inf)Zu ≥	0, ∣g∣2 ≤	1},	and ∣Vi∣Ki,*	:=
mint≥0 t s.t. Vi ∈ tKi . Thus, the quadratic-activation discriminator, ReLU-activation generator
problem in the case of a norm-constrained discriminator can be written as two convex optimization
problems, with polynomial time trainability for Z of a fixed rank.
ReLU-activation discriminator (σ(t) = (t)+). In this case, we have
arg min	max	1>H(xj1)Xu - 1>H(gj2)(ZW1)+W2u +Rg(W1,W2).
Wi ,W2	jι∈[∣Hχ∣]	I	I
jw∈[lHg |]
kuk2≤1
(2H(xj1)-Inr)Xu≥0
(2H(gj2)-Inf)(ZW1)+W2u≥0
Then, for appropriate choice of Rg(Wι, W2) = βg∣∣(ZW1)+W2kF, assuming mg ≥ nf dr + 1,
this is equivalent to
G* = argmin max	∣1>H(jI)XU - 1>H(j2)Gu| + βgIIGkF
G	jι∈[∣Hχ∣]	1 X	g	1	2	"F
jw ∈[lHg |]
kuk2≤1
(2H(xj1)-Inr)Xu≥0
(2H(gj2)-Inf)Gu≥0
W1*,W2* =argmin∣W1∣2F+ ∣W2∣2F s.t. G* = (ZW1)+W2.
W1,W2
The latter of which we can re-write in convex form as shown in Lemma C.1:
|Hz|	|Hz|
{Vi*}|iH=1z| = arg min X ∣Vi∣Ki,* s.t. G* = XH(zi)ZVi
Vi	i=1	i=1
for convex sets Ki := conv{Ug> : (2H(zi) - Inf)ZU ≥ 0, ∣g∣2 ≤ 1} and norm ∣Vi∣Ki,* :=
mint≥0 t s.t. Vi ∈ tKi . Thus, the ReLU-activation discriminator, ReLU-activation generator prob-
lem in the case of a norm-constrained discriminator can be written as a convex-concave game in
sequence with a convex optimization problem.
D Overview of Main Results
D. 1 Derivation of the Form in equation 3
Let us consider a positively homogeneous activation function of degree one, i.e., σ (tx) =
tσ (x) , ∀t ∈ R+. Note that commonly used activation functions such as linear and ReLU sat-
isfy this assumption. Then, weight decay regularized training problem can be written as
*
p*
mm
minmaχ X (1> σ(XUj)- 1>σ(Gθg (Z)Uj)) vj + Rg (θg)- βd X(M I∣2 + Vf).
g d j=1	j=1
26
Published as a conference paper at ICLR 2022
Then, We first note scaling the discriminator parameters as Uj = αjUj and Vj = Vj/αj does not
change the output of the networks as shown below
mm
Eσ(XU j )vj = Eσ(Xajuj
j=1	j=1
m
)Oj=x σ(Xuj)vj
mm	m
X σ(Gθg (Z)Uj)vj=x σ(Gθg (ZIajuj)Oj=xσ(Gθg (Z)Uj )vj∙
j=1	j=1	j	j=1
Moreover, We have the folloWing AM-GM inequality for the Weight decay regularization
mm
X(Mk2 + v2) ≥2X(Mk2∣vj∣),
j=1	j=1
where the equality is achieved when the scaling factor is chosen as αj∙= (3])/ . Since the
scaling operation does not change the right-hand side of the inequality, We can set kUj k2 = 1, ∀j.
Thus, the right-hand side becomes kvk1 = Pjm=1 |vj |.
We also note that this result was previously derived for linear (Ergen & Pilanci, 2021e) and ReLU
(Pilanci & Ergen, 2020; Ergen & Pilanci, 2021d). Similarly, the extensions to polynomial and
quadratic activations were presented in (Bartan & Pilanci, 2021).
D.2 Proof of Theorem 2.1
Linear-activation discriminator (σ(t) = t). The regularized training problem for two-layer ReLU
networks for the generator can be formulated as follows
p* = min Rg (Wι, W2)s.t. max ∣1>σ(XU) - l>σ((ZW1)+W2)U)∣≤ βd
W1,W2 g	kuk2≤1
σ=)=t P* = min Rg(W1, W2)s.t. ∣∣1>X - I>(ZW1)+W2)k2 ≤ βd.
W1 ,W2
Assume that the network is sufficiently over-parameterized (which we will precisely define below).
Then, we can write the problem
p* = min kGk2F s.t. k1>X-1>Gk2 ≤βd,
where the solution G* is given by a convex program. Then, to find the optimal generator weights,
one can solve
min kW1k2F + kW2k2F s.t. G* = (ZW1)+W2,	(37)
W1 ,W2
which can be solved as a convex optimization problem in polynomial time for Z of a fixed rank, as
shown in Lemma C.1, given by
|Hz|	|Hz|
{Vi*}|iH=1z| = arg min X kVikKi,* s.t. G* = XH(zi)ZVi
Vi i=1	i=1
for convex sets Ki := conv{Ug> : (2H(zi) - Inf )ZU ≥ 0, kgk2 ≤ 1} and norm
kVi kKi ,* := mint≥0 t s.t. Vi ∈ tKi, provided that the generator has mg ≥ nfdr + 1 neu-
rons, and we can further find the original optimal generator weights W1* , W2* from this problem.
Quadratic-activation discriminator (σ(t) = t2). Based on the derivations in Section E.3, we start
with the problem
p* = min Rg(W1,W2)s.t. kX>X - ((ZW1)+W2)>(ZW1)+W2)k2 ≤βd.
W1 ,W2
Assume that the network is sufficiently over-parameterized (which we will precisely define below).
Then, we can write the problem
p* = min kGk2F s.t. kX>X - G>Gk2 ≤βd,
27
Published as a conference paper at ICLR 2022
where the solution G* is given by G = L(Σ2 - βdI)+/2 V> for any orthogonal matrix L. Then, to
find the optimal generator weights, one can solve
min kWιkF + ∣∣W2kF s.t. G* = (ZWI)+W2,	(38)
W1,W2
which can be solved as a convex optimization problem in polynomial time for Z of a fixed rank, as
shown in Lemma C.1, given by
|Hz|	|Hz|
{Vi*}|iH=1z| = arg min X kVikKi,* s.t. G* = XH(zi)ZVi
Vi i=1	i=1
for convex sets Ki := conv{ug> : (2H(zi) - Inf )Zu ≥ 0, kgk2 ≤ 1} and norm
kVi kKi ,* := mint≥0 t s.t. Vi ∈ tKi, provided that the generator has mg ≥ nfdr + 1 neu-
rons, and we can further find the original optimal generator weights W1* , W2* from this problem.
ReLU-activation discriminator (σ(t) = (t)+ ). We start with the following problem, where the
ReLU activations are replaced by their equivalent representations based on hyperplane arrangements
(see Section E.5),
p* = min Rg(W1,W2)
W1 ,W2
s.t.	max	1>H(xj1)X-1>H(gj2)(ZW1)+W2u ≤βd
j1∈[lHxl]	.
j2∈[lHg |]
(2H(xj1)-Inr)Xu≥0
(2H(gj2) -Inf )(ZW1)+W2u≥0
Assume that the generator network is sufficiently over-parameterized, with mg ≥ nfdr + 1 neurons.
Then, we can write the problem as
G* = arg min kGk2F
G
s.t. max	1>H(xj1)X-1>H(gj2)Gu ≤βd
j1∈[lHxl]
j2∈[lHg 1]
(2H(xj1)-Inr)Xu≥0
(2H(gj2)-Inf)Gu≥0
and
min kW1k2F+kW2k2Fs.t.G*=(ZW1)+W2
W1 ,W2
the latter of which can be solved as a convex optimization problem in polynomial time for Z of a
fixed rank, as shown in Lemma C.1, given by
|Hz|	|Hz|
{Vi*}|iH=1z| = arg min X kVikKi,* s.t. G* = XH(zi)ZVi
Vi i=1	i=1
for convex sets Ki := conv{ug> : (2H(zi) - Inf )Zu ≥ 0, kgk2 ≤ 1} and norm
kVi kKi ,* := mint≥0 t s.t. Vi ∈ tKi, provided that the generator has mg ≥ nfdr + 1 neu-
rons, and we can further find the original optimal the generator weights W1* , W2* from this problem.
The former problem is a convex-concave problem. We begin with by forming the Lagrangian of the
constraints:
p* = mGin kGk2F
s.t.	αmi≥n0	1>H(xj1)X-1>H(gj2)G	+ αj>1	2H(xj1)	-InrX+γj>2	2H(gj2)	-InfG ≤βd
γj21 ≥0
XjUHxUjzHHg |]
min	-1>H(xj1)X-1>H(gj2)G+α0j1>	2H(xj1)-InrX+γj02>	2H(gj2)	-	Inf	G ≤βd
α0j ≥0	x	g	1	x	r	2	g	f	2
γj02 ≥0
XjUHxUjzHHg |]
28
Published as a conference paper at ICLR 2022
Then, forming the Lagrangian, we have
p* = min max IIGkF
G	λ,λ0 ≥0
αj1 ≥0, α0j1 ≥0,
γj2 ≥0, γj02 ≥0,
∀j1 ∈[lHxl],j2∈[lHg |]
-Xλj1j2βd-	1>H(xj1)X-1>H(gj2)G	+ αj>1	2H(xj1)	- Inr	X + γj>2	2H(gj2)	- Inf	G2
j1 j2
-Xλ0j1j2βd- -1>H(xj1)X-1>H(gj2)G	+α0j1>2H(xj1)-InrX+γj02>2H(gj2)-InfG2
j1 j2
We can then re-write this as
p* =min	max	IIGkF
G krj1j2 k2≤1, kr0j1j2k2≤1
λ,λ0≥0
αj1 ≥0, α0j1 ≥0,
γj2 ≥0, γj02 ≥0,
XjUHxUjzHHg |]
-Xλj1j2 βd-	1>H(xj1)X-1>H(gj2)G	+ αj>1
j1 j2
2H(xj1) -Inr	X+γj>2 2H(gj2) -Inf G rj1j2
-Xλ0j1j2	βd-	-	1>H(xj1)X-1>H(gj2)G	+α0j1>	2H(xj1) -Inr	X+γj02>	2H(gj2)	-Inf	G	r0j1j2
j1 j2
maximizing over α, α0 ,γ, γ0, we have
*
P
min max
G krj1j2 k2≤1, kr0j1j2 k2≤1
λ,λ0≥0
kGk2F-βdX(λj1j2+λ0j1j2)+X	1>H(xj1)X-1>H(gj2)G	(λj1j2rj1j2	-λ0j1j2r0j1j2)
j1 j2
j1 j2
s.t.(2H(xj1) - In)Xrj1j2 ≥0, (2H(gj2) - In)Grj1j2 ≥0, (2H(xj1) - In)Xr0j1j2 ≥0, (2H(gj2) - In)Gr0j1j2 ≥0
We can then re-parameterize this problem by letting rj1j2 = λj1j2rj1j2 and r0j j = λ0j j r0j j to
obtain the final form:
p*=min max0	kGk2F-βdX(krj1j2k2+kr0j1j2k2)+X1>H(xj1)X-1>H(gj2)G (rj1j2 -r0j1j2)
G rj1j2,rj1j2	j1j2	j1j2
s.t.(2HxjI)- In)Xrj j2	≥	0,	(2Hgj2)	-	In)Grj京 ≥ 0, (2HxjI)- In)Xrj≥ 0,	(2Hgj2)-	InQjIj2	≥ 0
which is a convex-concave game with coupled constraints, as desired.	□
D.3 Note on Convex-Concave Games with Coupled Constraints
We consider the following convex-concave game with coupled constraints:
p* = mGin max0	kGk2F-βdX(krj1j2k2+kr0j1j2k2)+X1>H(xj1)X-1>H(gj2)G (rj1j2 -r0j1j2)
rj1 j2,rj1 j2	j1j2	j1j2
s.t.(2H(xj1)-In)Xrj1j2 ≥0, (2H(gj2)-In)Grj1j2 ≥0, (2H(xj1)-In)Xr0j1j2 ≥0, (2H(gj2)-In)Gr0j1j2 ≥0
Here, we say the problem has “coupled constraints" because some of the constraints jointly depend
on G and rj1 j2 , r0j j . The existence of saddle points for this problem, since the constraint set is not
jointly convex in all problem variables, is not known (Zakovic & Rustem, 2003).
However, if all the constraints are strictly feasible, then by Slater’s condition, we know the
Lagrangian of the inner maximum has a saddle point. Therefore, in the case of strict feasibility, we
can write the problem as
*
p*
min	max0	mi0n	kGk2F	- βdX(krj1j2k2	+ kr0j1j2k2) + X	1>H(xj1)X - 1>H(gj2)G	(rj1j2	-r0j1j2)
G rj1j2,r0j1j2 λj1j2,λ0j1j2≥0	j1j2	j1j2
+Xλj>1j2(2H(gj2)-In)Grj1j2+Xλ0j1j2>(2H(gj2)-In)Gr0j1j2
j1 j2	j1 j2
s.t.(2H(xj1)-In)Xrj1j2 ≥0, (2H(xj1)-In)Xr0j1j2 ≥0
29
Published as a conference paper at ICLR 2022
which by Slater’s condition is further identical to
p* = min min max	kGk2F	-βdX(krj1j2k2+kr0jj	k2) + X	1>H(xj1)X - 1>H(j2)G	(rj1j2	-r0jj	)
p λ , λ0 ≥0 G r	,r0	F d	j1j2 2	j1j2 2	x	g	j1j2	j1j2
+Xλj>1j2(2H(gj2) -In)Grj1j2+Xλ0j1j2>(2H(gj2) -In)Gr0j1j2
j1 j2	j1 j2
s.t.(2H(xj1) - In)Xrj1j2 ≥0, (2H(xj1) - In)Xr0j1j2 ≥0
For a fixed outer values of λj1j2 , λ0j j , the inner min-max problem no longer has coupled constraints,
and has a convex-concave objective with convex constraints on the inner maximization problem.
A solution for the inner min-max problem can provably be found with a primal-dual algorithm
(Chambolle & Pock, 2011), and we can tune λj1j2 , λ0j j as hyper-parameters to minimize the
solution of the primal-dual algorithm, to find the global objective p*.
D.4 Proof of Theorem 2.2
Let us first write the training problem explicitly as
md	md
4min	max	1T X ((Xuj + bj )+ - (Gθg (Z)Uj + bj)+) vj + β X(U2 + v2) 十 Rg (θg ).
θg ∈Cg uj ,bj ,vj ∈R
j=1	j=1
After scaling, the problem above can be equivalently written as
min Rg(θg) s.t. max IIT(Xu + b)+ 一 1T (Gθ (z)u + b) I ≤ βd.
θg ∈Cg	∣u∣≤1,b I	+	g	+l
By the overparameterization assumption, we have Gθg (z)u + b + = (wu + b)+. Hence, the
problem reduces to
min Rg (w)	s.t. max II1T	(Xu	+ b)+ 一	1T (wu +	b)+ II ≤	βd.	(39)
Now, let us focus on the dual constraint and particularly consider the following case
max
b
(xi + b) 一	(wj + b)
i∈S1	j∈S2
≤β st (xi+b) ≥0, ∀i ∈S1, (xl +b) ≤0, ∀l ∈S1c
d, .. (wj + b) ≥ 0, ∀j ∈ S2 , (wk + b) ≤ 0, ∀k ∈ S2c,
(40)
where we assume u = 1 and S1 and S2 are a particular set of indices of the data samples with active
ReLUs for the data and noise samples, respectively. Also note that S1c and S2c are the corresponding
complementary sets, i.e., S1c = [n]\S1 and S2c = [n]\S2 . Thus, the problem reduces to finding the
optimal bias value b. We first note that the constraint can be compactly written as
min < min -xι, min -Wk
l∈S1c	k∈S2c
≥ b ≥ max
-xi , max -wj
j∈S2
Since the objective is linear with respect to b, the maximum value is achieved when bias takes the
value of either the upper-bound or lower-bound of the constraint above. Therefore, depending on the
selected indices in the sets S1 and S2 , the bias parameter will be either -xk or - wk for a certain
index k. Since the similar analysis also holds for u = -1 and the other set of indices, a set of optimal
solution in general can be defined as
(u*, b*) = (±1, ±xk/wk).
Now, due to the assumption βd ≤ mini,j∙∈[n]"=j∙忠一xj-1, We can assume that xι ≤ wι ≤ x2 ≤
. . . ≤ xn ≤ wn without loss of generality. Note that equation 39 will be infeasible otherwise. Then,
based on this observation above, the problem in equation 39 can be equivalently written as
w* = arg min Rg (w) s.t.
w∈Rn
2n	I
X Si(Xi- Xj) ≤ βd,
i=j	II
jI
Esi(Xj - Xi) ≤ βd,∀j ∈ [2n]
i=1	I
(41)
30
Published as a conference paper at ICLR 2022
where
Xi = Jxb i+1 c,
2	[wi，
if i is odd	+1, if i is odd
, si =
if i is even	-1, if i is even
, ∀i ∈ [2n].
After solving the convex optimization problem above for w, we need to find a two-layer ReLU
network generator to model the optimal solution w* as its output. Therefore, We can directly use the
equivalent convex formulations for two-layer ReLU networks introduced in (Pilanci & Ergen, 2020).
In particular, to obtain the network parameters, we solve the following convex optimization problem
|Hz|	|Hz|
{(ui*,vi*)}|iH=1z| = arg min X kuik2 + kvik2 s.t. w* = X H(zi)Z(ui - vi),
ui,vi ∈Ci i=1	i=1
where Ci = {u ∈ Rdf : (2HZi) - In)ZU ≥ 0} and we assume that mg ≥ n + 1.	□
E Two-Layer Discriminator Duality
E.1 Proof of Lemma 3.1
We start with the expression from equation 3
*
p*
m
min max	1>σ(Xuj ) -
θg vj,kujk2≤1j=1
m
1>σ(Gθg(Z)Uj)] vj + Rg(θg) - βd X |vj|.
j=1
We now solve the inner maximization problem with respect to vj , which is equivalent to the mini-
mization of an affine objective with `1 penalty:
p* = minRg(θg) s.t. max ∣1>σ(Xu) — 1>σ(Gθ (Z)u)∣ ≤ βd.
θg	g g	kuk2≤1	g
□
E.2 Proof of Corollary 3.1
We simply plug in σ(t) = t into the expression of equation 8:
p* = min Rg (θg ) s.t.
θg g g
max
kuk2≤1
| 1>X-1>Gθg(Z) U| ≤βd.
Then, one can solve the maximization problem in the constraint, to obtain
p* =minRg(θg)s.t. k1>X- 1>Gθg(Z)k2 ≤βd
θg
as desired.
□
E.3 Proof of Corollary 3.2
We note that for rows of X given by {xi}in=r1,
nr	nr
1>(XU)2 = X(xi>U)2 = XU>xixi>U = U>X>XU
i=1	i=1
Then, substituting into equation 8, we have:
p* =minRg(θg) s.t. max |U> X>X - Gθ (Z)>Gθ (Z) U| ≤ βd.
θg	kuk2≤1	g	g
Then, solving the inner maximization problem over U, we obtain
p* = minRg(θg) s.t. kX>X-Gθg(Z)>Gθg(Z)k2 ≤βd
as desired.
□
31
Published as a conference paper at ICLR 2022
E.4 Proof of Corollary 3.3
When there is a linear skip connection, we can write the problem as
*
P
mm
X 1>σ(Xuj)-1>σ(Gθg (Z)uj) vj+ 1>X-1>Gθg (Z) w+Rg(θg)-βd X |vj|,
j=1	j=1
min max
θg vj,w,kujk2≤1
where σ(t) = t2. Solving over w yields the constraint that 1>X = 1>Gθg (Z). Then, following
through the minimization over vj as in Lemma 3.1 and substitution of the non-linearity as in 3.3, we
obtain the desired result.	□
E.5 Proof of Corollary 3.4
We start with the problem equation 8, and substitute the ReLU non-linearity
p* = min Rg(θg) s.t. max |1> (Xu)+ - 1>(Gθ (Z)u)+ | ≤ βd.
θg	kuk2≤1	g
Then, we can introduce hyper-plane arrangements as described in Section 1.2 over both X and
Gθg (Z) to obtain the desired result.
p* = min Rg (θg)
s.t.	max
kuk2≤1
j1∈[lHxl]
j2∈[lHg 1]
(2H(xj1)-Inr)Xu≥0
(2H(gj2)-Inf)Gθg(Z)u≥0
□
1>H(xj1)X-1>H(gj2)Gθg(Z)u ≤βd
F Generator Parameterization and Convexity
F.1 Proof of Theorem 4.1
We will analyze individual cases of various discriminators in the case of a linear generator.
Linear-activation discriminator (σ(t) = t). We start from the dual problem (see Section E.2 for
details):
p* = min Rg (W) s.t. max 1>σ(Xu) - 1>σ(ZWu) ≤ βd
W	g	kuk2≤1
= min Rg (W) s.t. max (1>X - 1>ZW)u ≤ βd
W	g	kuk2≤1
=minRg(W) s.t. k1>X- 1>ZW)k2 ≤ βd.
X ʌ /
Clearly, the objective and constraints are convex, so the solution can be found via convex optimization.
Slater’s condition states that a saddle point of the Lagrangian exists, and only under the condition
that the constraint is strictly feasible. Given βd > 0, as long as 1>Z 6= 0, we can choose a W such
that 1>X = 1> ZW, and a saddle point exists. The Lagrangian is given by
p* =minmaxRg(W) + λ(k1>X - 1>ZWk2 - βd).
Introducing additional variable r, we have also
p* =min max Rg(W) + λ(1>X - 1>ZW)r - βd.
W λ≥0
krk2≤1
Now, v = λr, where λ = kvk2
p* =minmaxRg(W) + (1>X - 1>ZW)v - βdkvk2.
32
Published as a conference paper at ICLR 2022
From Slater’s condition, we can change the order of min and max without changing the objective,
which proves there is a saddle point:
p* =maxminRg(W) + (1>X - 1>ZW)v - βd∣∣v∣∣2.
The inner problem is convex and depending on choice of Rg (W) can be solved for W* in closed
form, and subsequently the outer maximization is convex as well. Thus, for a linear generator
and linear-activation discriminator, a saddle point provably exists and can be found via convex
optimization.
Quadratic-activation discriminator (σ(t) = t2). We start from the following dual problem (see
Section E.3 for details)
p* =minRg(W) s.t. kX>X - (ZW)>(ZW)k2 ≤ βd.
Wg
This can be lower bounded as follows:
P* ≥ d* =min 勺∣∣G∣∣F s.t. ∣∣X>X - G>G∣∣2 ≤ βd.	(42)
G2
Which can further be written as:
d* =min &∣∣Gk* s.t. ∣∣X>X - Gk2 ≤ βd.
G 2
This is a convex optimization problem, with a closed-form solution. In particular, if we let X>X =
VΣ2V> be the eigenvalue decomposition of the covariance matrix, then the solution to equation 42
is found via singular value thresholding:
G* = V(Σ2 - βdI)+V> .
This lower bound is achievable if∃W : (ZW)> (ZW) = G*. A solution is achieved by allowing
W = (Z>Z)-V2(Σ2 - βdI)Y2V>, where computing (Z>Z)-1/2 requires inverting only the first
k eigenvalue directions4, where k := maxk02≥βd k. Thus given that rank(Z) ≥ k, the solution of
the linear generator, quadratic-activation discriminator can be achieved in closed-form.
In the case that rank(Z) ≥ k + 1, strict feasibility is obtained, and by Slater’s condition a saddle
point of the Lagrangian exists. One can form the Lagrangian as follows:
p* = min maxβg∣∣Gk* +tr(RX>X) - tr(RG) - βdtr(R).
G R0 2
This is a convex-concave game, and from Slater’s condition we can exchange the order of the
minimum and maximum without changing the objective:
p* =maxmin βg∣∣G∣* +tr(RX>X) - tr(RG) - βdtr(R).
R0 G 2
ReLU-activation discriminator (σ(t) = (t)+ ). We again start from the dual problem (see Section
E.5 for details)
p* = min Rg (W)
s.t. max	1>H(xj1)X - 1>H(gj2)ZWu ≤ βd
j1∈[lHx |]	.
j2 ∈[lHg 1]
(2H(xj1)-Inr)Xu≥0
(2H(gj2)-Inf)ZWu≥0
We can follow identical steps of the proof of Theorem 2.1 (see Section D.2), with ZW instead of G,
obtain
p* =	mWin	max0	Rg(W)-βdX(∣rj1j2∣2+∣r0j1j2∣2)+X1>H(xj1)X-1>H(gj2)ZW(rj1j2-r0j1j2)
W rj1j2,rj1j2	j1j2	j1j2
s.t.(2H(xj1) - In)Xrj1j2 ≥0, (2H(gj2) - In)ZWrj1j2 ≥0, (2H(xj1)-In)Xr0j1j2 ≥0, (2H(gj2)-In)ZWr0j1j2 ≥0
as desired. Thus, as long as Rg is convex in W, we have a convex-concave game with coupled
constraints.	□
4For instance, letting Z = QΛQ>, we can use (Z>Z)-1/2 = Q[：k]A-：], where ]：周 indicates taking the
first k columns/diagonal entries respectively.
33
Published as a conference paper at ICLR 2022
F.2 Proof of Theorem 4.2
We note that for a polynomial-activation generator with m neurons and corresponding weights w(j1),
wj(2), for samples {zi}in=f1:
m>
Gθg(zi) = Xσ(zi>wj )wj
j=1
m
X a(zi>w(j ))2 +b(zi>wj()) + c)w(j )
j=1
j=1
m
X
j=1
m
ahzizi> , wj(1)w(j1)
>i + b(zi>wj(1)) + c)wj(2)>
avec(zizi>)
bzi
c
>
vec
1)>	>
wj(1)wj(2)
(2)>
wj
m
X z>wj
j=1
z1
>
z2
W
rr 1
znf
〜
Z W
for Zi :
avec(zizi>)
bzi
c
as the lifted features of the inputs, and a re-parameterized weight ma-
trix wj :
vec(w(1)wj1) )Wj)
W⑴W⑵丁
wj wj
(2)>
wj
(Bartan & Pilanci, 2021). Thus, any two-layer polynomial-
activation generator can be re-parameterized as a linear generator, and thus after substituting Z as Z
for Theorem 4.1, we can obtain the desired results.
□
34