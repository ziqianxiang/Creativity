Published as a conference paper at ICLR 2022
Adaptive Generalization and Optimization of
Three-Layer Neural Networks
Khashayar Gatmiry
MIT
gatmiry@mit.edu
Stefanie Jegelka
MIT
stefje@mit.edu
Jonathan Kelner
MIT
kelner@mit.edu
Ab stract
While there has been substantial recent work studying generalization of neural
networks, the ability of deep networks in automating the process of feature extrac-
tion still evades a thorough mathematical understanding. As a step toward this
goal, we analyze learning and generalization of a three-layer neural network with
ReLU activations in a regime that goes beyond the linear approximation of the
network and is hence not captured by the common Neural Tangent Kernel. We
show that despite nonconvexity of the empirical loss, a variant of SGD converges
in polynomially many iterations to a good solution that generalizes. In particular,
our generalization bounds are adaptive: they automatically optimize over a family
of kernels that includes the Neural Tangent Kernel to provide the tightest bound.
1 Introduction
The ability of overparameterized neural networks trained by (stochastic) gradient descent to gener-
alize well on test data (Krizhevsky et al., 2012; Silver et al., 2016; Hinton et al., 2012), even if they
perfectly fit the the training data, has intrigued theoretical researchers and led to many approaches
for generalization bounds (Neyshabur et al., 2015; Bartlett et al., 2017; Neyshabur et al., 2018; Dzi-
ugaite & Roy, 2017; Wei et al., 2019; Golowich et al., 2018; Arora et al., 2018b; Zhou et al., 2018;
Konstantinos et al., 2017). This generalization ability is tied to the optimization procedure, i.e., the
trajectory of the training algorithm in a non-convex loss landscape, and the structure of the data.
Hence, several recent works study the training of neural networks. For instance, Safran & Shamir
(2018) address the role of overparametrization in avoiding bad local minima, and Zhang et al. (2016)
empirically show that overparametrized networks trained by SGD can even perfectly fit to random
labels. Within the popular framework of the Neural Tangent Kernel (NTK) (Jacot et al., 2018), which
uses a linear approximation of the network at initialization, several works analyze the optimization
trajectory and show global convergence of (S)GD to a global optimum of the empirical loss (Allen-
Zhu et al., 2019; Li & Liang, 2018; Zou et al., 2018; Du et al., 2018). Extending the viewpoint
to generalization, Arora et al. (2019a;b) exploit the kernel-like behaviour of two-layer networks
close to their initialization to prove generalization for the final network, showing that two-layer
neural networks generalize as well as Kernel Ridgeless Regression (KRLR) with the NTK. Cao
& Gu (2019) show a tighter bound with a Neural Tangent Random Feature Model. The kernel
approach, however, has two main limitations: First, while KRLR can generalize well in specific
high dimensional regimes (Liang et al., 2020), there is theoretical and empirical evidence that it
can be inconsistent with noise (Rakhlin & Zhai, 2019). Is there an approach for analyzing neural
networks that shows they perform at least as well as KRLR, but is also robust to noise?
Second, importantly, neural networks are known to outperform traditional statistical methods in
many regimes as they are able to automate the process of feature extraction from data, as opposed
to kernel methods that work with a fixed feature representation. This poses the question of other,
adaptive, regimes beyond the linear network approximation. In this realm, Wu et al. (2018) show
generalization bounds that, instead of the NTK norm, scale with respect to another functional norm.
This norm corresponds to the minimum RKHS norm of the function among a family of kernels,
i.e., their method in a sense picks the best kernel in this family. However, this result ignores the
computational aspect of the problem. Are there particular nonlinear regimes beyond NTK for
which a gradient-type polynomial-time algorithm, in a way, adaptively chooses a suitable kernel?
1
Published as a conference paper at ICLR 2022
Going beyond the NTK view, a line of work convexifies the optimization problem via an approxi-
mation of SGD dynamics with a continuous time gradient flow in the space of probability measures
on the hidden units of the network, equipped with the Wasserstein metric (Mei et al., 2018; Chizat
& Bach, 2018; Mei et al., 2019; Wei et al., 2018; Sirignano & Spiliopoulos, 2020; Javanmard et al.,
2019; Lu et al., 2020). Taking another perspective, Allen-Zhu et al. (2018) consider a three-layer
network model that is not captured by the NTK approximation, and learn an underlying concept
class by exploiting saddle-point escape theory for nonconvex SGD (Ge et al., 2015a). However,
evaluating the complexity measure of Allen-Zhu et al. (2018) is rather involved, and only aligns
well with functions that are described by a particular network. Whether one can recover the NTK
bound (e.g. the NTK norm) from these results is not clear. For the NTK setting, in contrast, Arora
et al. (2019a) develop a purely data dependent generalization bound. Going beyond two layers, is
it possible to prove a data-dependent complexity measure beyond the NTK regime that recovers the
NTK result (Arora et al., 2019a) as a special case?
In this work, we address the above questions:
•	We consider a regime for 3-layer neural networks that is not captured by the NTK approximation
and show that, despite nonconvexity, a variant of projected SGD finds a good solution, as measured
by the regularized empirical loss, importantly, after polynomially many iterations.
•	We introduce a new function norm k.kζ as the minimum RKHS norm with respect to a family
of kernels K, which is upper bounded by the NTK norm up to constants. We show that for an
arbitrary function f, the generalization gap of the trained network scales by kfkζ. This makes our
generalization bound adaptive, in the sense that it scales with the best kernel in K. As a byproduct,
our bounds are comparable with kernel regression bounds simultaneously with all kernels in K.
We hope that our techniques motivate researchers to prove such adaptive generalization bounds
for deeper networks, which can potentially result in stronger depth separation.
•	We show generalization bounds with a new data-dependent complexity measure that generalizes
the NTK-based complexity in (Arora et al., 2019a). Up to logarithmic factors, our bounds are
upper bounded by those NTK-based bounds and hence improve over them (if one substitutes their
LiPschitz loss with a smooth one) - see Appendix A.1 for a simple explicit example. Importantly,
our bound can also handle noisy distributions as opposed to (Arora et al., 2019a).
Further Related work. While the idea of a learning algorithm that combines multiple kernels has
been employed for a while in the community (Sonnenburg et al., 2006; Rakotomamonjy et al., 2007;
Duan et al., 2012), our understanding of the connections between deep learning and multiple kernel
learning is yet in its infancy. Recently, Dou & Liang (2020) define a time-varying kernel based on the
network weights and show that the limit of the gradient flow converges to a suitable dynamic kernel,
in the sense that the residual of the link function onto its RKHS could be in a smaller ranked space
compared to the orthogonal complement of the RKHS. Ghorbani et al. (2019) analyze the difference
between training a two layer ReLU network and its NTK or random feature simplifications, for a
mixture of Gaussians input distribution and quadratic target functions. Ignoring the computational
hardness imposed by nonconvexity, Bach (2017) prove a dimension dependent generalization bound
beyond NTK. In another line of work, Chizat & Bach (2020) study gradient flow on losses with
exponential tail and its relation to the max margin solution. Wei et al. (2019) show an interesting
separation between the learning power of two layer ReLU networks and their NTK approximation,
by showing a sample complexity gap for an artificially constructed distribution.
With a different approach, Allen-Zhu & Li (2020) analyze multi-layer networks with quadratic ac-
tivations, and prove generalization bounds polynomial in the dimension and precision by assuming
an underlying teacher network, which shows a remarkable algorithmic depth separation. The prob-
lem of depth separation for neural networks and more generally their expressive power has been
investigated by several researchers before (Raghu et al., 2017; Daniely, 2017; Barron, 1994; Funa-
hashi, 1989; Safran & Shamir, 2016; Safran et al., 2019). The assumption of an underlying teacher
network that one seeks to recover is common, too (Li & Yuan, 2017; Zhong et al., 2017; Brutzkus
& Globerson, 2017). Other works focus mainly on the algorithm and use other techniques, such as
tensor factorization, to find a global optimum (Tian, 2016; Bakshi et al., 2019; Janzamin et al., 2015;
Zhong et al., 2017). Finally, many authors study the loss landscape under various assumptions (Free-
man & Bruna, 2016; Nguyen & Hein, 2017; Soudry & Carmon, 2016; Soltanolkotabi et al., 2018;
Ge et al., 2017), some of them consider the simplified case of deep linear networks (Arora et al.,
2018a; Saxe et al., 2013; Bartlett et al., 2018; Kawaguchi, 2016).
2
Published as a conference paper at ICLR 2022
2 Setup and approximation by kernels
We analyze a 3-layer ReLu neural network from inputs X ∈ Rd to outputs y ∈ R of the form
fv0,W0 (X) = √m2aτσ ((V⑼ + V0) Ws √m1 σ((W(0)+ W0)X)),
(1)
where a ∈ Rm2 is a vector of random signs, V (0) ∈ Rm2 ×m3 and W(0) ∈ Rm1 ×d are ran-
dom weight initializations with i.i.d Gaussian entries V(0) 〜 N(0,κ22),Wj(,0k) 〜 N (0, κ12), and
W s ∈ Rm3 ×m1 is a random sign matrix, which is roughly a random projection and change of coor-
dinates into a lower dimensional space. We refer to Ws √^σ((W⑼ + W0)x) as the first layer and
√⅛ aT σ ((V ⑼ + V 0)(.)) as the second layer. The algorithm trains weight matrices V 0 and W0, and
Ws, a are fixed. We assume that the outputs are a.s. bounded by a constant, |yi| ≤ B, and kxi k = 1.
As loss `(., .), we use the squared loss. We denote the training (empirical) loss ofa function f on our
data {xi}in=1 , {yi}in=1 and the expected loss with respect to the data distribution (population loss) by
n
Rn (f ) = 1 £『/( f (Xi) ,yi), and R (f )= E 以 f (X) ,y),
respectively. Sometimes, we refer to the vector of labels (yi)in=1 by y. Finally, HK is the space of
functions with bounded RKHS-norm of kernel K, and the notation O hides log factors.
2.1 Kernel approximations, decomposition and adaptivity
Kernel approximations of neural networks play an important role in our analysis. First, a common
approximation is the NTK. The Neural Tangent kernel for a 2-layer ReLu network is
H∞(x 1 ,x2) = hx 1 ,x2i ∙ F2(hx 1 ,x2i/(∣∣x 1 IIlIX21∣)),	for F2(X) = 4 + arcsin(X)/(2∏). (2)
To introduce adaptivity, a key part of our analysis is to approximate the second layer in the 3-layer
network by a product kernel K∞ G that decomposes into a “fixed” part K∞ and an “adaptive”
part G. To define these kernels, for every i ∈ [n], let φ(0)(Xi) be the output of the first layer of the
network at initialization, φ⑼(Xi) = √=^ Wsσ(W⑼Xi), and φ(0)(Xi) + φ0(Xi) be that output for
weights W(0) + W0. The adaptive kernel G captures the dot product between the learned weights:
G(Xi, Xj) = hφ0(Xi), φ0(Xj)i.	(3)
This form of G motivates the complexity measure we define in the next section, if one thinks of
the entries of φ0 as bounded NTK-norm functions of the input. Next, we consider the second layer,
where the part K∞ arises from roughly stable activations. To formalize this stability, let Sgn(V X)
be the diagonal matrix whose diagonal contains the coordinate-wise signs of the vector VX. If we
assume that Sgn((V⑼ + V0)(φ(0)(Xi) + φ0(Xi))) ≈ Sgn(V⑼φ(0)(Xi)) 一 We prove a rigorous
statement in Appendix A.12 - then
fw0,v0 (Xi) = √⅛aτ(V⑼ + V0)(φ(0)(Xi) + Φ(Xi))	(4)
m2
≈ DV⑼ + V0, √mm2aτSgn (V⑼φ⑼(Xi))(φ⑼(Xi) + φ0(Xi))TE.	(5)
Focusing on the adaptive part φ0(X) of the first layer, we write
DV⑼+ V0, √mm2aτSgn(V⑼φ⑼(Xi))φ'(Xi)TE := D⑼ + V0, Υ(Xi))	(6)
and can then view the second layer as a function in the RKHS of the product kernel
hΥ(Xi), Υ(Xj)i =: K∞(Xi,Xj)G(Xi,Xj). This defines the new kernel K∞, which we simplify
into the kernel K∞ that is independent of initialization (K∞ is defined in Equation (8)). To do so,
∙-v
we first observe that K∞ concentrates around
E W 〜N (0. 21 )1 {wτ φ ⑼(Xi)} 1 {wτ φ(O) (Xj)} = F2( ∣∣hφ00)( Xii) j£ 00( 7) i ).
2	∣φ(0)(Xi)∣∣φ(0)(Xj)∣
3
Published as a conference paper at ICLR 2022
Moreover, the Gaussian initialization and assumption kxi k = 1 imply that hφ(0) (xi), φ(0) (xj)i
concentrates around m3F3(hχi,xj), for F3 : [一 1, 1] → [0,1 ] defined as
F3(X)=a"∖ - X2 + 4X + 21∏x arcsin(X),	(7)
so KK∞(Xi,Xj) ≈ F2(2F3(hχi,Xji)) =： K∞(Xi,Xj).	(8)
For general X1, X2 not necessarily unit norm, We define K∞(X1 ,x2) = K∞(X1 /∣∣x 1 ∣∣,x2/∣∣x2 k).
It is easy to check that the coefficients in the Taylor series ofF2 and F3 are nonnegative. Combining
this With Schur’s Product Theorem implies K∞ is PSD (Appendix A.4). We also denote the data
kernel matrix on (Xi)in=1 by K∞ and H∞. Like Arora et al. (2019a), We assume the data distribution
is (λ0, δ, n)-non-degenerate With respect to H∞ and K∞, i.e., With probability at least 1 一 δ, the
smallest eigenvalues of H∞ and K∞ are at least λ0 > 0.
3 Data dependent complexity measure and generalization
The emergence of G K∞ above gives rise to an adaptive kernel-like complexity measure that
Will determine generalization bounds. Intuitively, this complexity measure reflects the tWo layers.
Here, We vieW G as the Gram matrix of some “ideal” first-layer feature functions gk. We measure
the complexity of the prediction function via the RKHS of A := G K∞, and alloW a flexible
choice of the features gk . The gk may be vieWed as feature representation of φ0 in Equation (3):
G(Xi, Xj) := P gk(Xi)gk(Xj). To regularize this choice, We penalize the complexity of the features
gk via the NTK norm. Alternatively, the features gk are flexible but have bounded NTK norm.
For a labeling f * ∈ Rn of the n data points and fixed G, this leads to the complexity
Q(f *,G) = f *TAT f * ∙hH∞-1 ,Gi = f *TAT f * Xm31 kgk kH∞； a = G Θ K∞,	(9)
k=1
Where m3 is the number of intermediate features. The choice ofm3 is discussed in Appendix A.13.
Our data-dependent complexity measure implicitly selects the G (or equivalently the feature vectors
gk ) that leads to the tightest bound, trading off data fit and function complexity:
===((Xi) n=1, (yi) n=1) ：= fm就{2 nR (f *)+$ 氏n ζ (f *,g )},	(io)
where we use a log factor $ = O(log(n)3 + log(1 ∕λ0)).
To make the relation to adaptive kernel spaces even more explicit, assume that the gk are bounded
as Pk kgkk2H∞ ≤ 1. Then we define a family K of corresponding kernels of the form
K{g}(X1, X2) = K∞(X1,X2)	g∈{g} g(X1)g(X2) ,	(11)
i.e., K ：= {K{g} | {g} finite, Pg∈{g} kgk2H∞ ≤ 1}. With this notation, the complexity measure is
=((Xi), (yi)) = fmn“ {2nRn(f *) + $ KiK f *TK-1 f * O.	(12)
Hence, this measure may be understood as searching for the most efficient and effective feature
representation within a family of RKHSs.
We may also relate this complexity measure to the NTK-based complexity measure yTH∞-1y
∙-v
(Arora et al., 2019a). For any labeling f*, let f* ∈ HH∞ be the function with minimum NTK norm
that maps Xi's to f*'s, so ∣∣jf*kH∞ = f *TH∞-1 f *. If we set {g} = {f*∕kf*k}, then one can
show (Appendix A.5)
f*TK-f*"k}f* ≤ 4f*T(f*f*T)-1 f* × kf*k2 =4kf*k2 =4f*TH∞-1 f*,
(13)
which implies
=≤ fmid2 nR(f *)+(4 $)f *t H ∞-1 f}
One can further set f * = y above and obtain
= ≤ (4$1)yT H∞-1y.
(14)
4
Published as a conference paper at ICLR 2022
3.1	Generalization
With the complexity = in hand, we can now state our generalization result. It assumes optimization
by a Projected Stochastic Gradient Descent (PSGD), described in detail in Section 4.
Theorem 1 Suppose we run Projected Stochastic Gradient Descent (PSGD) on the regularized em-
pirical risk with parameters as in Section 4, and |yi | ≤ B a.s.. Then, with high probability (e.g.
0.99) over the randomness of data, initialization and noise of the gradient steps, PSGD converges in
poly (B ∨ 1 /B, 1 /λ0, n) iterations to a solution (Wp>sgd, VPSGD) With population risk bounded as
R(fWPSGD ,VPSGD ) ≤
B((Xi)n=ι, (y)n=ι) + B2B
nn
(15)
As a side remark, the factor 2 in front of Rn (f *) in the definition of B in (12) is not special and
a similar generalization bound can be obtained for any γ > 1. Substituting the upper bound on
the complexity in Equation (14), one recovers an NTK-based generalization bound that scales with
yτH∞-1 y/n UP to log factors, which is roughly the square of the generalization bound presented
in (Arora et al., 2019a). The reason for the faster squared rate here is that we are considering smooth
losses, while they work with a bounded Lipschitz loss. Indeed, itis not hard to apply a more rigorous
uniform convergence analysis from (Srebro et al., 2010) to also obtain a faster squared rate for the
approach used in (Arora et al., 2019a).
Since Equation (14) is an upper bound on our complexity, our result generalizes and tightens the
NTK bound (Arora et al., 2019a). To illustrate the flexibility of our complexity measure, we show
in Appendix A.1 a simple explicit example of functions represented as polynomial series where our
bound improves upon the NTK bound. Notably, we only substitute low-rank matrices G in our
complexity measure for this construction. We leave further investigation of our complexity measure
for arbitrary G’s to future work.
3.2	Underlying Concept class
Instead of data dependent generalization bounds, one may study the generalization gap with respect
to some concept class. The complexity measure B implicitly uses the following adaptive norm on
the space of functions from Rd , the infimum of the RKHS norms for the family of kernels K:
kfkζ =K{ign}f∈KkfkK{g}.
(16)
It is not hard to check that k.kζ is in fact a norm, and that the inf is achieved by a particular set {g}.
Similar to the derivation of the upper bound on the complexity measure in Equation (14), by setting
{g} = {f*/kf* kH∞ }, we obtain the following NTK upper bound:
kfkζ≤4kfkH∞.	(17)
This leads to a function-dependent generalization bound which bounds the risk of the learned net-
work against an arbitrary function f with kf kζ < ∞.
Theorem 2 For any measurable function f : Rd → R, in the same setting as Theorem 1, the
population risk of the trained network can be bounded as
R( fWpsGD,Vpsgd ) ≤ 2R( f) +。(B) ∙	(18)
As in the data-dependent case, the factor 2 on R(f) can be reduced to any constant γ > 1.
3.3	Interaction of layers beyond the linear approximation
Here, we give a high level intuition on how the adaptivity is achieved in our regime compared to
NTK. In the NTK approach, for every input x, the neural net fW,V (x) is approximated by its linear
approximation at (W⑼,V(0)) (the initialized network), fw,v(x) = hνw,vfw(o),v(0)(X),(W —
W(0), V - V(0))i∙ The NTK approximation works as long as (W, V) are close enough to their
5
Published as a conference paper at ICLR 2022
initialization that the linear approximation remains accurate and the interaction of weights be-
tween layers is negligible. Specifically, the features φ0(xi) behave almost linearly with respect
to W — W(0) as Il W - W⑼ Il is taken to be small and the sign pattern Sgn((W⑼ + W0)Xi)
is proven not to change much compared to Sgn W(0)xi . Additionally, the NTK-type analysis
needs the following two conditions to be satisfied: (1) the sign pattern of φ(0) (xi) + φ0(xi) with
respect to V (0) + V 0 remains almost the same as the sign pattern of φ(0)(x) with respect to V (0),
and (2) the weight changes W0 and V 0 should not interact, which means the “interaction” term,
√1m2 aτ Sgn (V(0)φ (0)( Xi)) V 0φ0 (Xi) ≈ 0, should be negligible. Therefore, the non-negligible terms
for the NTK are:⑴ √=2ατSgn(V(0)φ(0) (Xi)) V(0)φ0(Xi), which is almost linear in W0 (recall
that φ0(Xi) depends on W0), and ⑵√^aτSgn(V(0)φ(0)(Xi))V0φ(0)(Xi) which is linear in V0.
This approach has two important implications: (1) it convexifies the optimization (for convex loss),
as the approximation is now linear in W; and (2) it simplifies proving generalization, as it works
with the class of functions in the RKHS space of some fixed kernel. However, this simplification
leaves no room for the ability of the neural network to learn intermediate feature representations.
In our regime, in contrast, we enforce the condition ∀j, i : Vj0 ⊥ φ(0)(Xi) (?), which implies the sec-
ond (2) above is zero, while the interaction term is not negligible any more and the network behaves
similar to a quadratic function with respect to (W0, V0) (for fixed Xi). Condition (?) is critical both
in proving the convergence of the algorithm as well as bounding the Rademacher complexity of the
class of networks with bounded weights. Rather than working with a fixed kernel, the interaction
term enables us to use the first layer for representing the input in a suitable feature space, which can
be interpreted as picking a suitable kernel, then use the second layer to describe the output based on
those features. This is also indirectly encoded in our complexity measure. In addition to enforcing
the orthogonality condition (?) (in the SGD variant), conditions for entering our regime are that the
overparameterization m1, m2 , m3 and κ1, κ2 are within a specific range with respect to each other.
We listed these relations in Appendix A.3.
To illustrate the benefit of going to this more involved regime, denote the class of neural networks
with bounded Frobenius norms IW0I ≤ γ1, IV0I ≤ γ2 by Gγ1,γ2 (and a bit more structure which
we elaborate upon in the proofs); it turns out that Gγ1,γ2 roughly includes HK(O(γ1γ2)) for every
kernel K ∈ K, in the sense that each f ∈ HK (O(γ1γ2)) is well-approximated within Gγ1 ,γ2 to
arbitrarily small error on fixed input (the error goes down with the size of the network). On the other
hand, we show that the Rademacher Complexity (RC) of Gγ1 ,γ2 behaves similar to the RC of the
NTK class HH∞ (O(γ1γ2))! As our algorithm guarantees finding a network with sufficiently small
empirical risk within Gγ1,γ2, this phenomenon underlies our adaptive generalization bounds.
Compared to previous work that provides an adaptive kernel analysis still fora two layer model (Dou
& Liang, 2020) (although their analysis is for the gradient flow and non-algorithmic), our model
requires an additional layer so it can, in a sense, “simulate” the process of feature extraction in one
layer to be used in the next layer.
3.4	Comparison with Kernel fitting
We compare our generalization bounds with some kernel fitting rates. Given a kernel K with
K(X, X) ≤ 1 for every X : IXI ≤ 1, suppose we want to fit a function from HK(B0), i.e. having K-
RKHS norm bounded by B0. In the realizable setting, when there is an underlying f ** ∈ HK(B0)
with zero risk, Empirical Risk Minimization (ERM) enjoys a fast rate using the smoothness of the
loss (Srebro et al., 2010). The Rademacher Complexity bound R(HK(B0)) ≤ O(√Bn) then implies
R(f erm) ≤ O(B02/n)	(19)
for the squared loss, which is minimax optimal up to log factors. To compare to the neural network,
We substitute f ** into Theorem 2. To relate the B in our bound to B0, assume for simplicity of
exposition that IIfIk = IlflIk* for some K * ∈ K (otherwise We can use a convergent sequence).
Observing that K{g}(x,x) ≤ 1 for every kernel K{g} ∈ K, we obtain that ∣f **(X)| ≤ ∣∣f**∣∣κ* =
If** Iζ (Appendix A.6). Combining this fact with the realizability assumption, we can then upper
6
Published as a conference paper at ICLR 2022
bound the parameter B in Theorem2 by ∣∣∕**kζ, and obtain
R ( fwpPSGD ,Vpsgd ) = O( kf『k 2/n ) .	(20)
If We further take K to be in K, then Equation (20) combined with ∣∣f ** 限 ≤ ∣∣f ^^∣k ≤ B0 implies:
R(fWpSGD ,VpSGD)=O( B0 2/n),
that is, for every kernel K ∈ K, our deep learning approach almost achieves the conventional kernel
bound in Equation (19).
Repeating the uniform risk bound stated in Theorem 1 in (Srebro et al., 2010) for HK(B0) where
B0 is set to all powers of two, followed by a union bound, one can easily obtain a fast rate of
R (fKRLR) ≤ O ( Uy K " + B),	(21)
for the solution of KRLR in the general case (not realizable) for the squared loss. On the other hand,
for a B-bounded Lipschitz loss, we instead get a slow rate for KRLR:
R(严 LR) ≤ O(「+ √),
where B is an a.s. bound on |y| as before. This bound is similar to Arora et al. (2019a). Note that our
data dependent generalization bound in Theorem 1 already achieves the fast rate for KRLR in (21)
for any K ∈ K. Finally, in the non-realizable case, we still have the following fast rate for ERM
regarding the hypothesis class HK (B0) (Srebro et al., 2010):
R(f ERM) ≤ O(R(f **) +
B02 + B2 )
n ),
where now f ** := argminf ∈Hk(p,)R(f), while Theorem 2 also implies (again for every K ∈ K):
R ( fWPSGD ,Vpsgd ) ≤ OR ( f ** ) + f-kn2^-)=OR ( f ** ) + B⅛B
4 Algorithm: Projected Stochastic Gradient Descent
In this section, we describe our algorithm PSGD, presented as pseudocode in Figure 1, which is
roughly Stochastic Gradient Descent modified to project out a low-dimensional random subspace
from the second-layer weights. PSGD approximately runs SGD on a smoothed version of the fol-
lowing loss function (ψ1, ψ2 are defined in Appendix A.2)
L1(W0,V0) =Rn(fW0,V0)+ψ1∣W0∣2+ψ2∣V0∣2.
Compared to standard SGD, our algorithm makes two modifications: (1) it uses randomized smooth-
ing to alleviate the non-smoothness of the ReLUs, (2) it ensures that the weights in the second layer
are orthogonal to the data features φ(0) (x) computed by the first layer at initialization. This helps to
control layer interactions as pointed out in Section 3.3. For smoothing, we add Gaussian smoothing
matrices Wρ and Vρ to the weights with i.i.d. entries drawn from N(0, β2/m 1) and N(0, β2/m2)
respectively, for β2 = Op((K1 √m 1)-1(K2√m2))-223), β 1 = Op(m3K2√m2(K1 √m1)-1). To sim-
plify the exposition, Op(.) is hiding the dependencies on the basic parameters B,n, 1 /λo and log
factors. Our convergence proof uses the loss with respect to this smoothed network.
For the projection, let Φ⊥ ⊂ Rm2 ×m3 be the subspace of weights of the second layer whose rows
are orthogonal to the first-layer data representations φ(0) (xi)’s ∀i ∈ [n] at initialization:
V0 ∈ Φɪ 什 ∀j ∈ [m2], ∀ ∈ [n]: Vjφ(0)(Xi) = 0.	(22)
In summary, at point (W0, V0), the algorithm samples a random (xi, yi) from the data, as well as
smoothing matrices Wρ,1, Vρ,1, Wρ,2, Vρ,2. It then computes an unbiased estimate for the gradient
ʌ ʌ
(Vw, Vy), adds additional normalized Gaussian noise matrices Ξ1, Ξ2 and moves in this direction
with step size η = 1 /poly(n, B ∨ 1 /B, 1 /λ0):
(W0,V0) J (W0,V0) + η(VW + Ξ1/(√m 1kΞ1 k), Projφɪ(VV + Ξ2∕∣∣Ξ21)).	(23)
7
Published as a conference paper at ICLR 2022
Parameters. Our results apply to the overparameterized regime, when the size of the network,
i.e. parameters m 1, m2,m3 are polynomially large in n, B ∨ 1 /B, 1 /λ0. This guarantees that the
network has suitable function representation capacity, and PSGD is able to find a good local direction
at every iteration. The regularization coefficients ψ1 , ψ2 can be set with respect to any candidate
(f ^, G) for our complexity measure (9). In Appendix A.2, we introduce a simple doubling trick
that handles the case when we do not have access to an optimal candidate solution. With such
an f *, as We describe in Remark 1, define V := max{Rrb(f)/2, B2/n}, and set ψ 1 = ν/4, ψ2 =
ν/ (4 ¢( f*,G)), where f* is the projection of f * along the span of eigenvectors of A with eigenvalue
as large as Ω(1 /n2). We list the suitable regime for overparameterization in Appendix A.3.
Algorithm 1 PSGD(Projected Stochastic Gradient Descent)
Input: network architecture m 1 ,m2,m3, initialization parameters K1 ,κ2, smoothing parameters
β1, β2, training set (xi, yi)ir=1, label parameter B, (f*, G) from the complexity measure
1:	Gaussian initialization W(0) J N(0, K1), Vj? J N(0, K2)
2:	Define parameters ψ1, ψ2, ν, η, subspace Φ⊥, and objective L1 as described in Section 4
3:	whileL1(W0,V0) > Rr(f*) +2νdo
4:	Gaussian matrices W^1 ,W^2 J N(0,祭),VjPk 1,j2 J N(O 条)
5:	Sample data (xi, yi) uniformly at random
6:	Compute gradient estimates
7:	J VW = '(fw0+wρ, 1Y0+Vρ, 1 (Xi),yi)Vw fw0+Wρ,2γ0+Vρ,2 (Xi) + 2ψ 1W0,
[▽v = '(fw0 + Wρ, 1V0 + Vρ, 1 (Xi), yi)VVfw0+Wρ,2,V0 + Vρ,2 (Xi) + 2ψ2V0
8:	Move as (W 0,V0) J (W 0,V0) + η( V W +ΞJ( √m 1∣∣ Ξ11∣), Projφ ɪ (V V +Ξ2/k Ξ2 k))
9:	Return (W0, V0)
5 High Level Idea of the PSGD Analysis
The reason for considering a Frobenius norm regularizer in PSGD is that we want the weights to
remain close to their initialization so the final network is in the class Gγ1,γ2 for suitably chosen
γ1, γ2; while still reducing the nonconvex empirical loss Rr (fW0,V0 ). We prove convergence for
PSGD by building on ideas from Allen-Zhu et al. (2018), with a framework based on the classic
result that SGD can escape saddle points for nonconvex functions. Compared to them, we take a
different approach driven by our purely data-dependent complexity measure. We augment this by a
careful Rademacher complexity analysis of the class Gγ1,γ2 in Appendix A.11.
Construction of a good Network To study the loss landscape, similar to (Allen-Zhu et al., 2018),
we show the existence of a good local update at reasonable points (W0, V0), using the ideal pair
(W*, V*) that we carefully construct from our complexity measure. Here, we sketch our proof for
constructing (W*, V*). Let (W0, V0) be the current weights of the algorithm. Fix a sample i ∈ [n].
In Appendix A.12, we use G to construct W* for the first layer weights with decomposition W* =
Pm=31 W* and O(1) bounded norm, such that φ* (Xi)k := √=ιWsSgn ((W⑼ + W0)Xi^ W*xi..
This decomposition ensures for every k, k0 ∈ [m3], negating Wk* only negates φ* (Xi)k0 when k0 = k
and has no effect on φ* (Xi)k0 for k0 6= k. This way, we can easily generate any arbitrary sign flip of
the entries of φ*(Xi). We use this property to generate a suitable random descent direction.
Next, we construct a suitable weight matrix V* for the second layer which maps the features φ* (Xi)
into fi* (recall the definition of the complexity measure). The key here is that we consider a regime
where the norm of φ(0)(Xi) is typically larger than that of φ0(Xi) and φ* (Xi), so it is very likely
that the sign pattern in the second layer is determined by φ(0)(Xi) in most rows. In such a scenario,
the condition Vj0 ⊥ φ(0)(Xi) becomes vital as the interaction of V0 with φ(0)(Xi) is problematic for
both generalization and optimization. From the standpoint of generalization, without excluding this
interaction, one can exploit the large size of φ(0)(Xi) and build a network within the class Gγ1,γ2
corresponding to a complex function that overfits the data. Indeed, we utilize the large magnitude of
φ(0) and its orthogonality to the rows of V0 in the RC bound. On the other hand, since the weights
8
Published as a conference paper at ICLR 2022
of the first layer does not affect φ(0)(xi), the interaction of V 0 and φ(0) (xi) is problematic for the
algorithm’s convergence, particularly in proving the existence of a local descent direction. This is
the rationale behind our orthogonality constraint (22).
Finally, the φ* (Xi )’s, the above control on the signs, and the fact that hφ⑼(Xi J, φ⑼(Xi2 )〉concen-
trates around m3EW〜N(0/11)[σ(WTXi Jσ(wτXi2)[ which recovers the structure of the kernel K∞
(Section 2), give rise to the kernel G Θ K ∞ in the second layer. Using this structure, We construct V *
that maps φ* (Xi )’s to f* 's, which has additional good properties, including O(f *T (GΘ K∞) —1 f * )-
bounded norm, and rows that are orthogonal to φ(0)(Xi)'s. For more details, see Appendix A.12.
Nonexistence of Bad Saddle Points Next, we want to exploit (W*, V*) to prove the existence
of a good direction along which the objective decreases locally. Moving along (W*, V*) is the
first idea, which fails as the cross terms created between W0, V* and V0, W* cannot be bounded
effectively. Instead, we randomly perturb W * and V* in a coupled way and prove a reduction
in expectation. We elaborate more on this suitable random direction. Multiplying random signs
Σk onto Wk*, we define the sum WΣ* = Pkm=31 ΣkWk*. We also multiply the same signs to the
columns of V* and project it back onto Φ⊥ to obtain VΣ*. Then, we move in the random direction
(√ηW∑* — ηW/2, √ηV* — ηV/2); this update creates additional cross terms in the objective that
we must bound to prove a local reduction argument. A key point here is that we prove with high
probability the norm of the weights is always bounded. This norm restriction enables us to substitute
terms that we do not have control over by their worst-case supremum. We refer to Appendix A.13
for similar techniques.
Convergence of PSGD Finally, we use the fact that SGD escapes good saddle points (Ge et al.,
2015b). For proving the existence ofa good random direction to escape saddle points above, we use
that the norm of weights is uniformly bounded along all iterations; this bound, in fact, is looser than
the bound that we show for the final weights of the network. Yet, this additional restriction cannot be
addressed by the classical nonconvex theory of SGD. Consequently, we refine and adapt the proof
of (Ge et al., 2015b) to incorporate this additional constraint. At a high level, Ge et al. (2015b) work
with a supermartingale based on the loss value. To guarantee the additional norm restriction, it is
initially tempting to apply Azuma-Hoeffding concentration to bound the upward deviations of this
process. However, this fails as the process has a two-fold behavior, depending on how large the
gradient is. At the core of our refinement proof here, we instead directly bound the MGF of the
martingale using Doobs maximal inequality. We refer to Section A.16 for more details.
Acknowledgments
The work was supported in part by NSF awards SCALE MoDL 2134108, CCF-2112665 (TILOS AI
Research Institute), CCF-1955217, CCF-1565235, and DMS-2022448. We would like to also thank
Sasha Rakhlin and Elchanan Mossel for fruitful discussions.
References
Zeyuan Allen-Zhu and Yuanzhi Li. Backward feature correction: How deep learning performs deep
learning. arXiv preprint arXiv:2001.04413, 2020.
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameter-
ized neural networks, going beyond two layers. arXiv preprint arXiv:1811.04918, 2018.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252. PMLR, 2019.
Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient
descent for deep linear neural networks. arXiv preprint arXiv:1810.02281, 2018a.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. In International Conference on Machine Learning, pp.
254-263. PMLR, 2018b.
9
Published as a conference paper at ICLR 2022
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of op-
timization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pp. 322-332. PMLR, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. arXiv preprint arXiv:1904.11955, 2019b.
Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal of
Machine Learning Research, 18(1):629-681, 2017.
Ainesh Bakshi, Rajesh Jayaram, and David P Woodruff. Learning two layer rectified neural networks
in polynomial time. In Conference on Learning Theory, pp. 195-268. PMLR, 2019.
Andrew R Barron. Approximation and estimation bounds for artificial neural networks. Machine
learning, 14(1):115-133, 1994.
Peter Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural
networks. arXiv preprint arXiv:1706.08498, 2017.
Peter Bartlett, Dave Helmbold, and Philip Long. Gradient descent with identity initialization effi-
ciently learns positive definite linear transformations by deep residual networks. In International
conference on machine learning, pp. 521-530. PMLR, 2018.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian
inputs. In International conference on machine learning, pp. 605-614. PMLR, 2017.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and
deep neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2019.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-
parameterized models using optimal transport. arXiv preprint arXiv:1805.09545, 2018.
Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks
trained with the logistic loss. In Conference on Learning Theory, pp. 1305-1338. PMLR, 2020.
Amit Daniely. Depth separation for neural networks. In Conference on Learning Theory, pp. 690-
696. PMLR, 2017.
Xialiang Dou and Tengyuan Liang. Training neural networks as learning data-adaptive kernels:
Provable representation and approximation benefits. Journal of the American Statistical Associa-
tion, pp. 1-14, 2020.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018.
Lixin Duan, Ivor W Tsang, and Dong Xu. Domain transfer multiple kernel learning. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 34(3):465-479, 2012.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. arXiv preprint
arXiv:1703.11008, 2017.
C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectified network optimization.
arXiv preprint arXiv:1611.01540, 2016.
Ken-Ichi Funahashi. On the approximate realization of continuous mappings by neural networks.
Neural networks, 2(3):183-192, 1989.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle pointsonline stochastic
gradient for tensor decomposition. In Conference on learning theory, pp. 797-842. PMLR, 2015a.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points — online
stochastic gradient for tensor decomposition. In Peter Grunwald, Elad Hazan, and Satyen Kale
(eds.), Proceedings of The 28th Conference on Learning Theory, volume 40 of Proceedings
of Machine Learning Research, pp. 797-842, Paris, France, 03-06 Jul 2015b. PMLR. URL
http://proceedings.mlr.press/v40/Ge15.html.
10
Published as a conference paper at ICLR 2022
Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape
design. arXiv preprint arXiv:1711.00501, 2017.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Limitations of lazy
training of two-layers neural networks. arXiv preprint arXiv:1906.08899, 2019.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. In Conference On Learning Theory,pp. 297-299. PMLR, 2θl8.
Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,
Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks
for acoustic modeling in speech recognition: The shared views of four research groups. IEEE
Signal processing magazine, 29(6):82-97, 2012.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gener-
alization in neural networks. arXiv preprint arXiv:1806.07572, 2018.
Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guar-
anteed training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473, 2015.
A. Javanmard, M. Mondelli, and A. Montanari. Analysis of a two-layer neural network via displace-
ment convexity. ArXiv, abs/1901.01375, 2019.
Kenji Kawaguchi. Deep learning without poor local minima. In nips, 2016.
Pitas Konstantinos, Mike Davies, and Pierre Vandergheynst. Pac-bayesian margin bounds for con-
volutional neural networks-technical report. arXiv preprint arXiv:1801.00171, 2017.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep con-
volutional neural networks. Advances in neural information processing systems, 25:1097-1105,
2012.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. arXiv preprint arXiv:1808.01204, 2018.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation.
arXiv preprint arXiv:1705.09886, 2017.
Tengyuan Liang, Alexander Rakhlin, et al. Just interpolate: Kernel ridgeless regression can general-
ize. Annals of Statistics, 48(3):1329-1347, 2020.
Yiping Lu, Chao Ma, Yulong Lu, Jianfeng Lu, , and Lexing Ying. A mean-field analysis of deep
resnet and beyond: Towards provable optimization via overparameterization from depth. In icml,
2020.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-
layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665-E7671,
2018.
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural
networks: dimension-free bounds and kernel limit. In Conference on Learning Theory, pp. 2388-
2464. PMLR, 2019.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pp. 1376-1401. PMLR, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to
spectrally-normalized margin bounds for neural networks. In International Conference on Learn-
ing Representations, 2018.
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In Interna-
tional conference on machine learning, pp. 2603-2612. PMLR, 2017.
11
Published as a conference paper at ICLR 2022
Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the
expressive power of deep neural networks. In Doina Precup and Yee Whye Teh (eds.), Pro-
ceedings of the 34th International Conference on Machine Learning, volume 70 of Proceed-
ings of Machine Learning Research, pp. 2847-2854. PMLR, 06-11 Aug 2017. URL http:
//proceedings.mlr.press/v70/raghu17a.html.
Alexander Rakhlin and Xiyu Zhai. Consistency of interpolation with laplace kernels is a high-
dimensional phenomenon. In Conference on Learning Theory, pp. 2595-2623. PMLR, 2019.
Alain Rakotomamonjy, Francis Bach, StePhane Canu, and Yves Grandvalet. More efficiency in mul-
tiple kernel learning. In Proceedings of the 24th international conference on Machine learning,
pp. 775-782, 2007.
Itay Safran and Ohad Shamir. Depth separation in relu networks for approximating smooth non-
linear functions. arXiv preprint arXiv:1610.09887, 14, 2016.
Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks.
In International Conference on Machine Learning, pp. 4433-4441. PMLR, 2018.
Itay Safran, Ronen Eldan, and Ohad Shamir. Depth separations in neural networks: what is actually
being separated? In Conference on Learning Theory, pp. 2664-2666. PMLR, 2019.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484-489, 2016.
Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A central
limit theorem. Stochastic Processes and their Applications, 130(3):1820-1852, 2020.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. IEEE Transactions on Information
Theory, 65(2):742-769, 2018.
Soren Sonnenburg, Gunnar Ratsch, Christin Schafer, and Bernhard Scholkopf. Large scale multiple
kernel learning. The Journal of Machine Learning Research, 7:1531-1565, 2006.
Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees
for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.
Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. Smoothness, low noise and
fast rates. In J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta
(eds.), Advances in Neural Information Processing Systems, volume 23. Curran Asso-
ciates, Inc., 2010. URL https://proceedings.neurips.cc/paper/2010/file/
76cf99d3614e23eabab16fb27e944bf9-Paper.pdf.
Yuandong Tian. Symmetry-breaking convergence analysis of certain two-layered neural networks
with relu nonlinearity. Network, 100(1):1-1, 2016.
Colin Wei, Jason Lee, Qiang Liu, and Tengyu Ma. On the margin theory of feedforward neural
networks. 2018.
Colin Wei, Jason Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and
optimization of neural nets vs their induced kernel. 2019.
Lei Wu, Chao Ma, and Weinan E. A priori estimates of the generalization error for two-layer neural
networks. 2018.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
12
Published as a conference paper at ICLR 2022
Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees
for one-hidden-layer neural networks. In International conference on machine learning, pp. 4140-
4149. PMLR, 2017.
Wenda Zhou, Victor Veitch, Morgane Austern, Ryan P Adams, and Peter Orbanz. Non-vacuous gen-
eralization bounds at the imagenet scale: a pac-bayesian compression approach. arXiv preprint
arXiv:1804.05862, 2018.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes over-
parameterized deep relu networks. arxiv e-prints, art. arXiv preprint arXiv:1811.08888, 2018.
13
Published as a conference paper at ICLR 2022
A Appendix
This appendix contains the different main sections of the proof. Lower-level lemmas may be found
in Appendix B.
Contents
A.1 Stronger Generalization bounds for polynomials ............................. 15
A.2 The Doubling Trick ......................................................... 16
A.3 Amount of Overparameterization	............................ 17
A.4 PSD property of K∞ ......................................................... 18
A.5 Complexity upper bound ..................................................... 18
A.6 Complexity measure and the ζ-norm .......................................... 18
A.7 Core Generalization Result ................................................. 20
A.8 Structure of the proof, setting m3,	and further definitions ................ 22
A.9 Proof of Theorem 2 ......................................................... 24
A.10 Optimization .............................................................. 25
A.11 Rademacher Complexity ..................................................... 28
A.12 Constructing W^,V*......................................................... 34
A.12.1 First Layer, Construction of W *.................................... 34
A.12.2 Second Layer, Construction of V* ................................... 39
A.12.3 Construction of V* ................................................. 45
A.13 Existence ofa good direction .............................................. 48
A.14 Existence ofa good direction Helper Lemmas ................................ 53
A.15 Bounding the worst-case Senario ........................................... 65
A.16 Convergence ............................................................... 73
A.17 Process from a higher view: definition of the (X) sequence ................ 77
A.18 Bounding the MGF ofXi’s ................................................... 79
A.19 Proof of Theorem 7 ........................................................ 80
A.20 Gaussian Smoothing ........................................................ 81
A.20.1 Setting β1 and β2 .................................................. 86
A.21 Basic Tools ............................................................... 87
A.21.1 Defining the rare events Ej ........................................ 91
A.21.2 Bounding the value of f0 ........................................... 93
A.21.3 Bounding the difference between Original and Smoothed Functions .... 94
14
Published as a conference paper at ICLR 2022
A.1 Stronger Generalization bounds for polynomials
In this section, we prove an explicit generalization bound for functions represented as a polynomial
sum. Note that the bounds in Arora et al. (2019a) for polynomials assume the monomials with degree
larger than one to have even powers, while here we do not impose this restriction. In addition,
different from Arora et al. (2019a), our bounds remain meaningful in the noisy case (recall our
Theorem 2).
More specifically, we bound the ζ norm of such functions. Consider the target function s with the
following power series formula:
∞
y = s(x) = Xap(wpTx)p,	(24)
p=1
where ap ∈ R and wp ∈ Rd . We can write
d
s(x) = g1 (x) +	xkg2k(x),	(25)
k=1
where xk denotes the kth entry of vector x here and
g1 (x) =	ap(wpTx)p,
p∈A1 :={p=1 or p even}
and for all k ∈ [d]:
g2k(x) =	wpkap(wpTx)p-1.
p∈A2：={p>2, P odd}
Then, using the Taylor series of x(4 + arc2FX)) = P∞=ι Ypxp for ∣x∣ ≤ 1, the RKHS H(H∞) of
the NTK can be identified by square-summable sequences of reals (ap0 )p∞0 =1 with dot product
∞
h(ap0 )p∞0 =1, (bp0 )p∞0=1i =	γλ(p0) ap0 bp0 ,
p0 =1
where λ(p0) : Z≥0 → Z≥0 such that it maps zero to zero, the first d positive integers are mapped to
one, the next d2 ones are mapped to 2, etc. Moreover, the RKHS mapping Ψ : Rd → H(H∞ ) from
the Euclidean space is:
Ψ(x) = kxk x1, ..., xd, (xk1 xk2 )k1,k2∈[d] , . . . , (xk1 xk2 . . . xkp)k1,...,kp ∈[d] , . . . ,
where x0 = x/kxk and in the notation above we are presenting a sequence of sequences, by which
we mean the inner sequences simply unfold. Using this identification, one can see using the linear
representations of g1 , g2k in H:
kg1k2H∞ = X γpa2pkwpk22p,	(26)
p∈A1
kg2kk2H∞ = X γpwp2ka2pkwpk22(p-1).	(27)
p∈A2
Summing above and noting the linear representation ofg:
d∞
kg1 k 2H∞ +	kg2kk2H∞ =	γpa2pkwpk22p+
γp a2p kwp k22p = Xγpap2kwpk22p = kgk2H∞.
k=1	p∈A1	p∈A2	p=1
(28)
Now for {g} := {gk0 }dk+=11 := {g1} ∪ {g2k}dk=1, we consider the kernel K{g}. Expanding the tai-
lor series of F2(2F3)(x) = P∞=o μpxp, We find the identification (hko)k∈[d +1],p0=0,…,∞ with dot
product
∞	d+1
∑>λ( po )∑ hp0 qp0,
p0 =0	k=1
15
Published as a conference paper at ICLR 2022
with RKHS map
Ψ2 (x) = g10 (x), . . . ,gd0+1(x),x01g10 (x), . . . , x01gd0 +1(x), . . . ,x0dg10(x), . . . ,
xdgd+1 (x), (xk1 xk2 gk (x))k1 ,k2∈[d],k∈[d+1] , . . . , (xk1 xk2 . . . xkp gk (x))k1 ,...,kp∈[d],k∈[d+1] , ... .
Now, we compute the norm of function s with respect to K{g}, combining the above representation
and dot product with Equation (25) and the fact that we work with unit norm x, so x0 = x:
I∣s^ K{g} = μ o + (d +I) μ ι.	(29)
Plugging the above and Equation (28) into the definition of k.kζ in (16), we conclude
d∞
同2 ≤ 同K{g) (Ig 11H∞ +£ Ig21H∞) ≤ (μ0 + (d +1)μι) E IpaPlwpIl2p	(30)
k=1	p=1
Note that if the odd exponents (except possibly one) in the definition of s in (24) are zero, then
we could consider only the function g1 and kernel Kg1 , which would have implied a bound of
μ o P ∞=ι Ypa plwpl 2 P.
A.2 The Doubling Trick
For the SGD optimization, we set the regularization coefficients in the loss L1 as
ψ 1 = Vl4,	ψ 2 = Vl (4 C (f*,G)),	(31)
with v := max{Rn (f *)/2,B2/n. This assumes we know the f * and G that minimize the adapta-
tion within the complexity measure (12). To achieve generalization bound in Theorem 1, here, we
explain how to use a simple doubling trick to get over the fact that we might not know these optimal
solutions f* and G. The proof here is based on the generalization result in Theorem 3.
Theorem 1 Without explicitly knowing the exact value of the complexity measure, i.e., the optimal
solution of Equation (12), one can still achieve the generalization bound in Theorem 1.
Proof of Theorem 1
Our core generalization result is in Theorem 3. The proof of Theorem 1 is simply adding a
doubling trick on top of the argument of Theorem 3. We also prove Theorem 2 as a consequence
of Theorem 1 in Appendix 2. In the rest of the proofs, for simplicity, we refer to (WPSGD, VPSGD)
by (W0, V0). Let f**, G* be the optimal solution to (10). With a simple rescaling of G*, we can
assume hH∞-1, G*i = 1. (Note that the complexity does not change by such rescaling). Now one
can exploit the condition Iyi I∞ ≤ B, and consider the setting f* = 0 to get the following trivial
upper bound on the complexity measure:
=((xi), (yi)) ≤ 2nB2 .
Therefore,
2nRn(f**) + f**A-1f**(c0$) ≤ 2nB2.	(32)
Using Equation (32) and the optimality of (f**, G*):
Rn(f**) ≤ B2,
C=C(f**,G*) ≤2nB2.	(33)
Combining the first equation above with the definition of V in Equation (41), we get
B2/n ≤ v ≤ B2.	(34)
To initialize ψ1 and ψ2, we use Equations (31) for any f* and G, and as a result we get a general-
ization bound as in Equation (45). However, to achieve the best possible rate characterized by our
complexity measure in Theorem 1 without explicitly computing the answer of (12), we use a simple
16
Published as a conference paper at ICLR 2022
doubling trick; for every pair (ζ0, ν0) such that ζ0 is a power of 2 between B2 and 2nB2, and ν0 is a
power of two between B2/n and B2, we initialize ψ1, ψ2 as in Equation (31) and run the algorithm,
then return the network which minimizes the empirical loss after the required polynomial number of
steps. This is to make sure that the value of the loss will go at some point below the PSGDstopping
threshold on the loss, since the stopping threshold depends on Rn(f **) which We are not aware
of. Another way to resolve this issue, to have an early stop when the value of the loss pass the
threshold is to again run a doubling trick on the value of Rn(f **) for every fixed value of V and
ζ, and run PSGD with stop threshold Rn(f **) + 2V (here, Rn(f **) is set using the doubling trick
variable). This approach works because our final upper bound on the risk ignores the constants (note
that the doubling trick introduce additional constants). Moreover, since V ≥ B2/n by definition, we
don,t need to run Rn(f **) over values smaller than Ω(B2/n), since it does not change the order
of Rn(f **) + 2v. Particularly, combining this with the upper bound on Rn(f **), we only need to
run the doubling trick for Rn(广)in the interval (Ω(B2/n), O(B2)). Now let v0 be the power of 2
within v(G*,f **)/2 ≤ v0 < V(G*, f **). If we are in the case
f**TA-1 f** < B2,	(35)
then for ζ0 equal to the smallest power of two larger than B2, when we run PSGD with pair (v0, ζ0),
by Theorem 3:
R(fw0,V0) ≤ 2Rn(f *) + C0$2B2 + B2 ≤ =((S)n=11 ((y)n=1) + C —.	(36)
n	nn
Because we return the minimum upper bound on the risk (the tighter lower bound of Equa-
tion (44)) among all such powers of two, we certainly achieve the above rate in (36). Otherwise,
if f**TA-1f** ≥ B2, let ζ0 be the power of two within f*TA-1f* ≤ ζ0 ≤ 2f*TA-1f*, then
again it is easy to check that conditions of Theorem 3 are satisfied, hence we get the following
generalization bound:
R(fw0,V0) ≤ 2Rn(f **) + C00$ (Z0 + B2) ≤ 2Rn(f **) + C00$2Z(f**,G*) + B2
nn
≤ =((Xi) n=ι,(yi) n=ι) + C B 2 $
nn
which proves the bound of Theorem 1.
(37)
(38)
A.3 Amount of Overparameterization
In this section, to provide high-level insight, we indicate the right order of magnitude that our over-
parameterization should be in, with respect to one another. Note that the exact coefficients in these
inequalities would depend on the basic parameters B, 1 /λo,n, which we have avoided here for sake
of simplicity. We refer the reader to our main proof (mostly Appendix parts A.12, A.13) for more
details.
κ1κ2m3 << 1,
K2√m2 >> 1,
Kι√m3 >> κ2√m2,
4
m1 >> m3,
κι√m1 >> m3/2,
κ2 << 1 /√m3
√m3K2 << 1 /√m3
√m2 >> m3/2K1 κ2
m33(K2m2) << K1m1
mι,m2,m3, 1 /κι, 1 /κ2 = poly(n, B ∨ 1 /B, 1 /λ0).
In addition, we set the smoothing parameters as
β2 := θp ((K1 √m3)- 1(√m2κ2)-21
β 1 ：= Θp(m 3 √m3 / (K1 √m 1)),
17
Published as a conference paper at ICLR 2022
where Θp only shows polynomial dependencies on the overparameterization.
A.4 PSD PROPERTY OF K∞
The Schur product theorem states that for PSD matrices A and B, A B is also PSD. Now given
an analytic function F whose tailor series coefficients are all nonnegative, Suppose we apply F on
some PSD matrix A entrywise, denoted by F (A), under the condition that the entries of A are in
the radius of convergence of F, then using Schur product theorem, it is straightforward that F(A) is
also PSD.
Using the above property, one can then check that the tailor series of the defined functions F2 and F3
are nonnegative, hence, the application of the function F2(2F3(x)) on the gram matrix of (xi)in=1 is
a PSD matrix, ( note that hxi, xji ≤ 1 is in the convergence radius of F2(2F3(x)).) thus K∞ is
indeed a kernel.
A.5 Complexity upper bound
First we mention a simple fact that hadamard product respects matrix orderings. Given PSD matrices
A, B, C such that A B, the fact that AC B C is an easy consequence of the Schur Product
Theorem; indeed, B - A is PSD by definition, so (B - A) C = B C - A C is also PSD.
Next, it is easy to check that the tailor series of arcsin(x) has all nonnegative coefficients. Therefore,
for a PSD matrix X, as we discussed in Appendix A.4, applying arcsin entrywise on X, namely
arcsin X, is also PSD. Setting X equal to the entrywise application of 2F3 to the gram matrix of
datapoints (xi)in=1, we realize the matrix arcsin 2F3 hxi, xji 1≤ij≤n is also PSD. Noting
the definition of K∞ in Equation (8), we conclude that for the data kernel matrix K∞ we have
K∞ ≥ 11IT,
4
where 1 is the all ones n-dimensional vector.
Combining the two mentioned facts, we can lower bound the matrix K = K∞ G for any matrix
G as
K = K∞ Θ G ≥ 1IIT Θ G = 1 G.
44
Substituting the rank one matrix f 寸^T for the n-dimensional vector f * in Equation (13):
Kf./kf.k} = K ∞ Θ f *f *T/kf* k 2 ≥ 4 f *f *t/kf*k2.	(39)
The inequality used in (13) then follows from Equation (39).
A.6 COMPLEXITY MEASURE AND THE ζ-NORM
This is a brief section regarding some basic properties of = and k.kζ.
First, note that the two versions of the complexity measure in Equations (10) and (11) are equivalent,
as for any finite set of functions {g}, we can define the gram matrix with respect to the feature
vectors of these functions on data, and for an arbitrary nonzero PSD G we can consider a Cholesky
factorization for G as G = XTX, then define the functions {gk} as the minimum-NTK norm
functions which map the input to the features corresponding to X. This observation further implies
We can suppose the factor matrix X is in Rn×n, and there is a set of at most n functions {gk}n=1
which corresponds to this G.
Next, we show that for an arbitrary function f, its sup norm over the unit ball is bounded by its ζ
norm:
sup |f (x)| ≤ kfkζ.	(40)
kxk=1
18
Published as a conference paper at ICLR 2022
Note that for a kernel K which satisfies K(x, x) ≤ 1, using Cauchy Schwarz we simply obtain
f(x) ≤ kfkκKχX)≤ ≤ kfkκ,
where recall that k.kK is the norm corresponding to the RKHS space ofK . Hence, to show (40), it
suffice to show that for all kernels K ∈ K and unit norm x we have K (x, x) ≤ 1. To see this fact,
note that the norm of each x ∈ Rd in the NTK-space is H∞ (x, x) = 2. Therefore, for each function
g with bounded-NTK norm, again using Cauchy Schwarz:
∣g (X) l≤ 1 kgkH∞.
As a result, for a family of functions {g} with Pg∈{g} kgk2H∞ ≤ 1, we have on every unit norm x:
g∈{g}
g(x)2 ≤ 1.
On the other hand, it is easy to check that for every unit norm x, We have K∞ (x, x) ≤ 1, so for
every such {g}, we have by definition
K{g}(x) ≤ 1,
Which completes the proof of Equation (40).
19
Published as a conference paper at ICLR 2022
A.7 Core Generalization Result
In this section, we prove our core generalization result for the trained network, Theorem 3, which
underlies our generalization bounds in Theorems 1 and 2. Recall that in the rest of the proofs, we
refer to the solution (WPSGD, VPSGD) returned by PSGD simply by (W0, V0).
Theorem 3 Suppose we have a good candidate pair (f * ,G) regarding our CompIexity measure
in (10) that satisfies〈H∞-1 ,Gi ≤ 1, f *TA-1 f * ≤ Z (recall A = G Θ H∞), and that f * has zero
projection onto the directions of eigenvectors ofA whose eigenvalues are smaller than O(1/n2) (the
last condition can be relaxed, see the next remark). Then, for
ν = max{Rn(f*)/4, B2/n},	(41)
ifwe are given ν/2 ≤ V0 ≤ V, and we set
ν0
ψ ι = 4	(42)
ψ 2 = 4Vζ,	(43)
then for the solution (W0, V 0) returned by PSGD we have the following generalization bound:
R(fw-V0) ≤ %Rn(fw0y0) + C00B*'n—)	(44)
≤ 5 Rn ( f * )+ C0$ ( Z + B) ,	(45)
3n
for constants C0, C00 and log factor $ = log(n)3 + log(1 /λo).
Remark 1 Given a pair (f*, G) satisfying hH∞-1, Gi ≤ 1, f*TA-1f* ≤ Z, one can project out
the directions that are along the eigenvectors of A with eigenvalues SmaUer than Ω(1 /n2) to obtain
f*, then use the pair (f*, G) in Theorem 3. This way, the third condition mentioned in Theorem 3
also becomes true. As we show in Lemma 42, by switching f * to f* the quantity f *t A-1 f * does
not increase, and the quantity Rn(f*) is multiplied by a constant c> 1 arbitrarily close to one, then
adds up with O(B2/n). This means that the bounds in Theorem 3 for the pair (f*,G) translates
into similar bounds for (f*, G) albeit with a bit worse contants. It is straightforward to see that with
small enough choice ofcand careful AM-GM inequality that we apply inthe proof of Theorem 3, one
can end up with the same constants regarding the pair (f*, G) as declared in Theorem 3. Fora more
careful discussion on this, we refer the reader to the proof of Lemma 10.
Proof of Theorem 3
Almost all of our proofs in the rest are in the aim of proving Theorem 3. Crucially, to prove
this Theorem, we need to establish two big results:
1.	We need to show that the final network has small training loss, and is within the class
Gγ1,γ2 for some suitable γ1, γ2. This is handled by Theorem 4 in Appendix A.10. We
define the class Gγ1,γ2 roughly as the class of networks with norm bounds kW - W(0) k ≤
γ1, kV - V (0) k ≤ γ2 where the rows of V - V (0) are orthogonal to the subspace Φ, plus an
additional structure defined in Appendix A.11. This task, on its own, has three main steps
in our proof:
(a)	we construct a “good” underlying network, Appendix A.12
(b)	we find a “good” random direction and study the landscape of the objective, Ap-
pendix A.13
(c)	we prove the convergence, Appendix A.16
2.	The Rademacher Complexity of the class Gγ1,γ2 needs to be suitably bounded. This is
handled by Theorem 5 in Appendix A.11.
With access to these results, here we show how Theorem 3 follows by a simple application of the
generalization bound in (Srebro et al., 2010). Specifically, for fixed constants z1, z3 and every integer
20
Published as a conference paper at ICLR 2022
i ≥ 0, we use Theorem 1 of (Srebro et al., 2010) for the class Gz1,γi, γi = 2i × B/z3 with confidence
probability 1 - 2-iδ3, which, with a union bound, implies that with probability at least 1 - δ3, for
every i and fW0,V0 ∈ Gz1,γi :
R ( fw 0,V 0 ) ≤ Rn ( fw 0,V 0 ) + K Iq Rn ( fw，,V，)( ^logS )1 5 R G ι,∙J + j ”^11 n2-iδ3)L )
(46)
+ log( n )3RG1 ,Yi )2 + “θg(1 In -δ3))),	(47)
where '(fwo,v，(x),y) is a.s. bounded by b for function within the class Gz 1 Yi, and K is a universal
constant. In the following, we aim to further bound the Rademacher complexity R and parameter b.
Applying the AM-GM inequality with respect to ratio z4 > 0 for the second term:
R(fw0,V0) ≤ (1 + z4)Rn(fw0,V0) + K2 /z4 log(n)1.5R(Gz1,γi) +

b log(1 /(2 F 3))
n
2
+ lθg(n)3R(Gz 1 ,Yi )2 + b lOg(11n-3))
≤ (1 + Z4)Rn(fw0,V0) + K2/z4(log(n)1 5RG1 ,Yi) + rblθg(1 In3D)2
+ log( n )3 R (Gz 1 ,Yi )2 + blog(11n3))
≤ (1 + Z4)Rn(fw0,V0) + (2K2/z4 + 1)log(n)3R(Gz 1 ,%)2 + (2K2/z4 + 1)blog(1 In -3))
(48)
Now let γ* be the smallest number of the form 2iB/z3 (for some i) which is not smaller than Z2 √ζ.
This definition implies
Y* ≤ max{2Z2pζ,B/z3}■	(49)
Now Theorem 5 in Appendix A.11 bounds the Rademacher complexity:
R(Gz 1 ,γ*) ≤ √n.	(50)
On the other hand, from Theorem 4 by setting Zι,z2 = √40, We get fw，,v，∈ Gz 1 ,γ*.
Moreover, from Lemma 34, for fw，,v0 ∈ Gz 1 ,γ*, We have for every ∣x∣∣ ≤ 1:
|fw0,V0(x)| ≤ 2Z1γ*,	(51)
so the loss `(fw 0,V 0 (x), y) can be bounded by (B + 2Z1γ*)2 using the 1 smoothness property.
Therefore, for the class Gz 1 ,γ^ We can set b = (B + 2Z1 γ*)2. Combining this with Equation (50)
and plugging into Equation (48):
上 + (2 K 2/z 4 + 1)( B + 2 Z11 * )2 IOg(I/(2-iδ3))
R(fw0,V0) ≤ (1 + Z4)Rn(fw0,V0 ) + (2K2/Z4 + 1) log(n)3
n
n
Furthermore, by definition of γ* , we have 2i ≤ 2Z2 Z3 VZ/B:
_ , .	,	. _ , .	, __n ,	. n , . 0	_	,	Γ~___4Z2 Y *2
R(fw0,V0) ≤ (1 + Z4)Rn(fw0,V0) + (2K2/z4 + 1)4Z2(log(n)3 + 2log(2Z2Z3\fZ/B)) —n—
(52)
+ (2K2/z4 + 1)2B2 log(2Z2Z3).	(53)
n
Now applying the upper bound on Y * :
R(fw0,V0) ≤ (1 + Z4)Rn(fw0,V0) + (2K2/Z4 + 1)4Z2(log(n)3 + 2log(2Z2Z3P/B))4Z2(2Z2R+ JBZ3)2
(54)
+ (2K2/z4 + 1)2B2 l0g(2Z2Z3)
n
(55)
21
Published as a conference paper at ICLR 2022
If Z > B2, in the third term above We substitute B by √. Finally, similar to the bound We stated in
Equation (14), note that we have the following trivial bound for ζ :
Z ≤ yTH∞-1y ≤ 4nB2/λ0,	(56)
i.e. there is no point in considering larger Z's, which implies log(2Z2Z3√ζ∕B) = O(log(n) +
log(1 /λ0)). Plugging this above and picking Z4 = 1 /3 show the proof of Equation (44). Further-
more, applying Equation (68) in Theorem 4 to the Rn(fW0,V0) term in Equation (44) further gives
the second Equation (45).
Remark 2 In the Same setting ofTheorem 3, ifwe have ν/2 ≤ V0 but not generally upper bounded
by ν, then PSGD leads to the following generalization bound:
R ( fw 0,V 0 ) ≤ Rn ( f * ) + V0 + C000$ ( Z + B 2) ,
n
using a similar argument as we did for Theorem 3.
A.8 S TRUCTURE OF THE PROOF, SETTING m3, AND FURTHER DEFINITIONS
Throughout the proof, (W0, V 0) represents the pair of matrices of the current iteration of PSGD,
(W*, V *) are the “ideal” matrices that we construct in Appendix A.12, (Wρ, V ρ) and refers to the
gaussian smoothing matrices. Importantly, note that our squared loss `(f, y) is zero at f = y. We
have tried to make the lower level proofs into sub-lemmas and create a manageable hierarchy as
much as we could, to make the document more clear and readable.
Similar to the conditions in Theorem 3, through out most of the proofs we assume that we are given
a pair (f*, G) with a slightly more general setting of Theorem 3:
f*TA-1f* ≤ Z2, forA = GK∞,
hG, H∞i ≤ Z1.
Particularly, Z1, Z2 appear in Appendix A.13. Because we are allowed to rescale G, we do not really
gain much by assuming this more general setting, though we pick to work with the general setting
as the abstraction makes the proof more straightforward to understand.
We refer to the parameters B, 1 /λ0,n as the “basic parameters”, m 1 ,m2,m3, 1 /κ 1, κ2 as the “over-
parameterization”, and β1 , β2 as the “smoothing parameters.” By the phrase “having enough overpa-
rameterization” we mean it suffices to pick the overparameterization m 1 ,m2, m3, 1 /κ 1, 1 /κ2 only
polynomially large in the basic parameters.
Throughout the proof, we denote the change in the output of the first layer at W (0) + W0 + Wρ
compared to the initialization value by φ(2) (xi), i.e.
φ⑵(Xi) = √mWsσ((W⑼ + W0 + Wρ)Xi) - φ⑼(Xi),
while recall that φ0(Xi) has a similar definition except without the smoothing matrix Wρ. Although
our model is a three layer network, throughout the proof, we refer to the parts Ws√1m-σ((W⑼ +
W0)x) and √m-aτσ ((V(O) + V0)(.)) as the “first layer” and “second layer,” respectively.
Also, we sometimes refer to the binary sign pattern of vector X multiplied to matrix W by DW,x
(DW,x := Sgn(W X)), i.e. the jth diagonal entry of DW,x is one if WjT, X ≥ 0, and is zero otherwise.
To refer to the jth row of W as a vector, we sometimes drop the comma in Wj, and write it as Wj .
For brevity, we denote the Frobenius norm kW kF of matrix by kW k, and the Euclidean norm of
a vector X by kXk. For matrices W1 , W2 we denote their natural dot product by hW1 , W2 i :=
tr(W1T W2). We refer to the smallest eigenvalue of a matrix by λmin(.). We write R(.) for the
Rademacher complexity of a function class. We refer to the smoothed version of the network by
fW0 0,V0 (X), defined by
fW0 0,V0 (X) = EWρ,VρfW0+Wρ,V0+Vρ (X).
22
Published as a conference paper at ICLR 2022
In the proof, we mainly work with the loss over the smoothed network f0, defined as
L(W0,V0) = Rn(fW0 0,V0)+ψ1kW0k2+ψ2kV0k2.	(57)
Our algorithm, PSGD can be regarded roughly as an SGD over L.
Similar to what we discussed in section 3, let the functions {gk }km=31 be some feature representation
whose gram matrix is equal to G and hH∞, Gi = Pkm=31 kgkk2H∞. In such setting, it is not hard
to check that we can assume each gk is the minimum norm NTK function which maps (xi)in=1 to
(gk(xi))in=1’s. Indeed, if this is not the case for some gk, we can project the RKHS representation of
gk onto the span of the representations of (xi)in=1, which can only decrease the complexity measure.
Hence we can represent gk ∈ HH∞ as a linear combination of basic functions H∞(xi, .) on data
points:
n
∀k ∈ [m3], gk(x) := X Vk,iH∞(xi, x).	(58)
i=1
Here, the sum of squared-H ∞ norms of Vk is bounded as
Xk kVkk2H∞ =Xk kgkk2H∞ =hH∞,Gi ≤ζ1.	(59)
For each i ∈ [n], We refer to the feature representation vector (gk(Xi))建[on Xi as Xi. Note that We
have the relationship
X i = (H∞ Vk) m^ι,	(60)
Where Hi∞, is the ith roW ofH∞. In the analysis, We Work With a bound ξ on the quantity maxk kVk k
Which should be bounded polynomially by other basic parameters; in particular, it is defined in
Lemma 10 and is used to bound a cross term in Lemma 14. HoWever, maxk kVk k might not be
effectively bounded for an arbitrary feature representation. Fortunately, we can remedy this by a
simple trick; for every natural number s, one can substitute every gk by S copies of gk∕√s, without
changing the gram matrix G. Therefore, for any δ, one can increase the multiset of functions (gk)
to a bigger set (gk), by adding at most O(Q/δ) functions, making sure of the following for the new
functions:
∀k : VTH∞Vk = IgkkH∞ ≤ δ.	(61)
(This is because Pk ∣gk ∣∣H∞ ≤ 1). Furthermore, observe that for each gram matrix G, we have an
n-dimensional feature representation (gk)kn=1 for G according to the Cholesky factorization. Com-
bining these facts, we conclude, to guarantee Equation (61), in the worst case, we need m3 to be as
large as n + O(Q∖∕δ).
Finally, observing the following inequality
kVkk2 ≤ kVkkH∞/λ0 ≤ ∣gkkH∞/λ0.	(62)
in order to guarantee maxk ∣∣Vk k ≤ ξ we need to take m3 as large as n + O(Z1 /(ξ2λ0)), which is
indeed bounded polynomially by the basic parameters because of the same condition for 1 /ξ. This
computation also brings into sight an important point:
“Although each gram matrix G is representable by n features, in order for the algorithm to be able
to find a suitable network, m3 might need to be larger than n.”
Moreover, for every 1 ≤ k, i ≤ n, we define the matrices Zik ∈ Rmd as
Zk = 1 /√m (WSj l{Wj(0) t Xi}χi) j=1,	(63)
where in the above notation, j is enumerating the columns of the matrix. We also define the following
matrices which we use in our construction later:
n
Wk+T = Wk+T =	Vk,iZki,
i=1
(64)
and W+ as
m3
W+ =	Wk+.
k=1
23
Published as a conference paper at ICLR 2022
Finally, to avoid unnecessary complication, we often argue high probability bounds without an
explicit representation of their dependency on the chance of failure (which is a negligible logarithmic
factor). We also ignore all constants and log factors, and mainly work with the notation . which
ignores constants; we write a . b ± c as a short form for b - O(c) ≤ a ≤ b + O(c). As there
are several hierarchies of new parameters that are defined based on lower-level ones, we rename
the new parameters and continue viewing them as black-box. This makes the proofs more readable,
since we also do not care about the exact dependency of the underlying parameters most of the time,
rather we are interested in their orders of magnitude, for example that a given parameter goes to zero
polynomially fast with respect to the overparameterization, etc. Due to the large number of symbols
that we have to work with, we might use a symbol more than once, of course when it is clear from
the context which one we are refering to.
A.9 Proof of Theorem 2
In this section, we prove Theorem 2, stated below.
Theorem 2 For any function f : Rd → R, in the same setting as Theorem 1, the population risk of
the trained network (W0, V 0) can be bounded as
R (fw 0,V 0) ≤ 2 R (f) + O (a$ "*21 B ).	(65)
Proof of Theorem 2
Theorem 2 is a simple consequence of Theorem 1; for the given function f, we apply Theo-
rem 2 with the smaller coefficient Y = 3 for Rn (f *), regarding the complexity upper bound, by
setting f * := (f (Xi))n⅛
4tf 4 ^4	. f *TKT f * ɪ B2a$
R(fwo,vo) ≤ WRn(f) + (a$) min------1----
3	K∈K n n
=4 Rn (f) + (a$ )min f*TK-f + X
3 K∈K n n
On the other hand, because f*T K-1f* is the minimum-RKHS norm of a function with respect to
kernel K which maps xi’s to f*i and f is one such function, we have f*T K-1f* ≤ kfkK. This
inequality implies
Km∈inKf*TK-1f* ≤ kfkζ,
so we obtain
4	kf kζ2 + B2
R( fW0,V0) ≤ 3Rn ( f ) + (a$)-.	(66)
3n
Therefore, it remains to bound Rn(f) by R(f).
As we showed in Appendix A.6, for every input x we have f(x) ≤ kfkζ, so for every data (x, y),
by the fact that |y| ≤ B a.s. and α smoothness of the loss, we have `(f (x), y) ≤ α(kf kζ + B)2.
Moreover, note that the random variable `(f (x), y) has mean R(f). It is easy to check that in this
setting, the variance of `(f (x), y) is at most R(f)α(kf kζ + B)2. Therefore, an application of the
Bernstein inequality, we have with high probability over the dataset
Rn (f) ≤ R (f) + O r r r (f)α (kf kζ+ B )2 + αfΛBl! ≤ 3 R (f) + O (αf∆Bl).
n n2 n
Plugging this back to Equation (66) completes the proof. As a result, the learned network can
compete with any function that has reasonably small kf kζ :
R(fw0,V0) ≤ Wn {2R(f) + O(a$"^: B )}.
24
Published as a conference paper at ICLR 2022
A.10 Optimization
In this section, we glue together
•	the existence of a good random direction that we prove in Appendix A.13
•	the convergence analysis of PSGD that we do based on the work Ge et al. (2015b) in Ap-
pendix A.16.
Theorem 4 In the same setting of Theorem 3, assume the network (W0, V 0) returned by PSGD, has
sufficient polynomially large “overparameterization”. Then, for the solution (W0, V0) returned by
PSGD we have
L (W J') ≤ Rn (f)+ ν,	(67)
which further implies
Rn ( fW-V A ≤ Rn ( f ) + 2 V	(68)
kW0k2 ≤40, kV0k2 ≤ 40ζ.	(69)
Moreover, for every i ∈ [n], j ∈ [m1], j ∈/ P for P defined in 1, we have that sign((Wj(0) +Wj0)Txi)
and sign(Wj(0) xi ) are the same.
Proof of Theorem 4
Let Υ ∈ Rm2 (m3 -n)×m2m3 be a matrix whose rows are an orthonormal basis for the space
of matrices whose rows are orthogonal to span({φ(0) (xi)}in=1), i.e. Φ⊥, as defined in (22). Then,
we consider a linear change of coordinates for the subspace Φ⊥, regarding the second layer weights,
as v0 = Υvec(V 0) where vec(.) splits out the vectorized version of a matrix. For consistent notation,
we also denote w0 = W0, so we now have a new coordinate system (w0, v0) ∈ Rm2(m3-n)×m1d for
pairs of weights (W0, V 0) such that V0 ∈ Φ⊥. We also define the loss function
LΠ(w := (w0, v0)) =L(W0,V0),
with respect to the change of coordinate.
Now it is easy to see that running PSGD on L in the normal coordinates is equivalent to running
stochastic gradient descent on LΠ with respect to (w0, v0). Moreover, because multiplying to matrix
Υ is an orthonormal change of coordinates for Φ⊥ and because V 0 is already in φ⊥ at each step of
PSGD, then kv0k = kV 0k, so the conditions kW0k ≤ C1, kV0k ≤ C2 are equivalent to kw0k ≤
C1,	≤ C2. Furthermore, by our construction, the random matrix V∑ is in the subspace Φɪ, so
the norm bounds ∣∣W*k ≤ Zι, kV*k ≤ Z2 are equivalent to ∣∣w*k ≤ Zι, ∣W*k ≤ Z2 for w^ := W∑∑
and v* := Υvec( V *).
Now we apply the result of Theorem 6 on LΠ with parameter νset as ν0 (recall the definition of ν0
from Theorem 3), Z2 := Z and Z1 := 1, and ∆ := Rn(f*), as defined in Theorem 3. More specifi-
cally, based on our arguments above regarding the natural isometry in the change of coordinate, any
pair (w0, v0) in the domain ∣w0∣ ≤ C1, ∣v0∣ ≤ C2, LΠ (w0, v0) ≥ Rn(f*) + ν0 translates into a
pair (W0,V0) in the domain ∣W0∣ ≤ C1, ∣V0∣ ≤ C2, L(W0, V0) ≥ Rn(f*) + ν0, for which by
Theorem 6 there exists (WΣ*, VΣ*) such that
E∑ L (W/ — η/2W/ + √ηw*∑, V0 - η/2V0 + √ηv∑*) ≤ L (W∖ V0) - nV /4.	(70)
Translating back to the change of coordinates:
E∑Lπ(w0 — n/2w0 + √ηw∑, v0 — n/2v0 + √ηv∑) ≤ L(w0, v0) — nV/4.	(71)
Now we apply Lemma 44 to translate this into an argument about the landscape of LΠ . As a result,
applying the bounds in Equations (106) and (126), we obtain that for (w0, v0) such that
LΠ(w0, v0) ≥Rn(f*)+ν0,
25
Published as a conference paper at ICLR 2022
we should either have
∣∣VLπ(w',v')k ≥ —,	v/	:
4 V^wFTFT
_	Vl 4
= 4√P0FTFF
ν
=16√C2 + C2 2
or
λmin (VVLπ(w0,v0)} ≤ -—―IV、42 , U *口2、
\	2	2mm∑( ∣∣w*∣∣2 + ∣∣v*∣∣2)
ν
-----------：-------：-
2min∑("∑ k2 + M k 2)
ν
≤------------
—16( C1 + Z2)
ν
=- 16(1 + Z).
Next, we want to apply Theorem 7 by setting
ν
Y = 16(1 + Z) 2
^` = Rn (f * ) + V,
and Lipschitz parameters ρ1, ρ2, ρ3 = poly(B, C1, C2, m1, m2, m3) set as described in Ap-
pendix B.1, Theorem 9. Also, note that as prescribed by Theorem 7, we set
C1
N + 41
ψ 1
C2 := 3,
ψ2
(72)
where l = O(1) depends on our desired chance of success for the algorithm, specified in Theorem 7.
Finally, note that Theorem 7 needs to work with a bounded noise on the gradient whose covariance
matrix is bounded between two multipliers of identity. The point of injecting extra noise to SGD
in PSGD is in fact because of this covariance condition that we need in Theorem 7. On the other
hand, note that in general, because of the gaussian smoothing that we use, the noise vector is not
supported on a bounded domain, which makes it a bit harder to apply Hoeffding type concentration.
To remedy this, we introduce a coupling between our unbounded noise vector for L(W0, V 0) and
another noise random variable whose support is bounded, which with high probability is equal to
the real noise, along all iterations. In Corollary, we further translate this coupling for the objective
LΠ after change of cooridnates, and write down the exact dependencies of the parameters Q, σ1 and
σ2 , which are all polynomial in the basic parameters and the overparameterization.
Hence, the conditions of Theorem 7 are satisfied, so we conclude that after at most
poly(P1, P2,P3, Q, N,C 1, C2, 1 /γ, log(σ 1 /σ2))	= poly(B, mι,m2, m3,C 1 ,C2,Z1,Z2)=
poly(n, B ∨ 1 /B, 1 /γ0) number of iterations, PSGD reach a point Wt in some iteration t with
Lπ(Wt) ≤ N.
Translating back this wt = (wt0 , vt0) by multiplying the vt0 part to ΥT , we get a pair (Wt0, Vt0) with
objective value bounded as
L(Wt0,Vt0) ≤Rn(f*)+ ν0.	(73)
But note that we obviously have the condition kW0k ≤ C1, kV 0k ≤ C2 through the whole iterations,
for the choice of C1 , C2 in Equation (72). Therefore, using Lemma 34, for every i ∈ [n]:
|fW0 0,V0(xi)| =O(C1,C2),	(74)
|fW0,V0(xi)| =O(C1,C2)	(75)
26
Published as a conference paper at ICLR 2022
From Equations (75), as also stated in Theorem 9, we know that for all i ∈ [n], `(., yi) is
O(C1C2) + B2-Lipschitz at points fW0,V0 (xi) and fW0 0,V0(xi ), so we can bound the difference
I'(fWo,vo(Xi),yi) - '(fwo,vo(Xi),yi)| by (O(C1C2) + B2)IfW01v，(Xi),yi) - fw0^v，(Xi)I, which
in turn can become arbitrarily small having enough overparameterization using Lemma 35, in partic-
ular, We force it to be smaller than O(V/(B2 + C1C2)) (recall V ≥ ν/2 ≥ B2/(2n)). As a result,
We get ∣'(fWo,v0 (Xi),yi) - '(fw0,v，(Xj,yi) I = O(V) for every i ∈ [n], which in turn implies
IL(W0, V0) - L1 (W0, V0)I ≤ ν by picking small constants, where recall that the objective L1 is the
same as L but without the smoothing. Now applying this bound to Equation (73), we get
L 1(W0,Vt0) ≤ Rn(广)+2V.
Therefore, as PSGD check the values of L1 in the loop, it terminates at such pair (Wt, Vt ). From this
point onward, we refer to the returned (Wt0, Vt0) as just (W0, V0).
Opening the definition of L1 (W0, V0), we clearly get
Rn(fw0,V0) ≤ L 1(W0, V0) ≤ Rn(f *) + 2V ≤ Rn(f *) + 2V.	(76)
Furthermore, noting the setting of ψ 1, ψ2 in Theorem 3 and the fact that ν0 ≥ ν/2 ≥ Rn (f *) /8, we
get
∣∣W0II2 ≤ 4(Rn(fV) + 2ν ≤ 40,	(77)
IlV0∣2 ≤ 式(Rrn(f)+ 2V0) ≤ 40ζ,	(78)
which completes the proof. The fact that for every i ∈ [n], j ∈ [m1], j ∈/ P we have that
sign((Wj(0) + Wj0)TXi) and sign(Wj(0) Xi) are the same follows from Lemma 1.
27
Published as a conference paper at ICLR 2022
A.11 Rademacher Complexity
In this section we show the proof for our Rademacher Complexity bound, which is used in Theo-
rem 3.
Theorem 5 Let Gγ1,γ2 be the class of neural nets with weights (W, V ) in our three layer setting,
such that kW - W(0) k ≤ γ1, kV - V (0) k ≤ γ2, where for every j ∈ [m2], i ∈ [n]: Vj0 ⊥
φ(0)(xi), and for every i ∈ [n],j ∈ [m1], j ∈/ P for P ⊆ [m1] defined in Lemma 1, it satisfies
sign((Wj(0) + Wj0)xi) = sign(W(0)xi). Then, for large enough overparameterization, we have the
following bound on the Rademacher complexity:
R(GYι ,γ2) ≤ 2√γ2.
Proof of Theorem 5
Here, we do not have the smoothing matrices Wρ, V ρ anymore. In this section, unlike the
optimization section that we used {x0i}in=1 to denote the output of the first layer by incorporating
also the smoothing matrices, here we define it without them:
Xi = √m W sσ ((W ⑼ + W 0) Xi).
Now define the matrices
Zi = 1 /√m2 Qj l{Vj(0)xi ≥ 0}χ0i)j=1,
Zi+ = 1 /√m2 ( aj (1 {Vj,xi ≥ 0 } - K Vj⑼ Xi ≥ 0 D xi ) m = 1 .
To bound the Zi0+ part, note that substituting C1 by Y1 in lemma 6 and assuming conditions
m 1 = Ω( m 4),
2C 3/2	3 3
『(nm3 )1 / 4 ≤ Y ι,
κ1 m1 λ0
(we can use this result because we do not have the smoothing matrix Wρ here), we get with high
probability over the initialization for every i ∈ [n]:
kφ0(xi)k = kx0i - φ(0)(xi)k .γ1.
(79)
Therefore, we can write
I trace( VZi+) I = √m IXX aj M sign( Vj, xi) = sign( V⑼ xi) }Vj,xi∖
≤ √m XX M sign( Vj,xi.)=sign( V⑼ xi) }∖Vjxi∖
≤ ； X i{∣VwxiI ≤ I(Vj,- Vj(o))xi∣} (I(Vj,- Vj(0))xiI + IV(0)xiI)
m2 j
≤√= X i{IVw xiI ≤ I (Vj,- Vj °)) xiI}(21 (Vj,- Vj( Cl)) xiI).
m2
j
≤-1= X i(IVw xiI, I (Vj, - V⑼) xiI ≤ Y2 / 3( K )1 / 3 min(γ 1, HxiH)} QI (V, - v( 0)) xiI)
m2	m2
j
+ 3 X 1{I(V； - V⑼)xiI≥ Y2/3(κ)1 /3 min(γι,HxiH)}(21(Vj, - Vj°))xiI).
m2 j	m2
28
Published as a conference paper at ICLR 2022
Now using the fact that Vj - Vj(0) is orthogonal to φ(0) (xi)’s:
LHS ≤
2 Y2 2 3( m )113γ 1
√m2
E1{∣V(。)χi∣≤ Y223( m )1 1 3 同}
2
+ √m2 X M偌,—v(0)Mxi - Φ(0)(Xi)H ≥ Y223(m)113Y 1}倍,-V⑼Mxi - Φ⑼(Xi)H∙
Next, using the upper bound on MX0i - φ(0) (Xi)M:
2 〜213( κ2 )1 2 3M	…	… —
LHS .	√H	X(MIV(O)χi∣ ≤ Y213(K)113怩H}∙
√m2	j j	m 2
+ √Y= X M倍，—V⑼H & YY13(m)113}偌，—Vj0)H
2/13( K2. )11
.	√m2	XX(I{∣V(°XiI ≤ YY13(mm2)113HXiH}
+ -Ymi yXXijjF^^F3O^ jVTjF
≤ ^√mP1 (XMIV⑼χi∣ ≤ Y213(m)113HXiH}) + -⅛ × (m)1131.
Then, applying the first argument of Lemma 29, we have with high probability over the randomness
of (0):
LHS ≤
Y213 (m )113
Yi毕产)113Y213)+ YYYI × (m)113 1
K 2 m 2	√m2	K 2	Y Y13
√m2
29
Published as a conference paper at ICLR 2022
Therefore, we can write:
R(GY 1,γ 2)ι (X)，(yi)=n 以湍PS
n
ifV,W (xi)
t=1
n
X Ci aτ σ (1 /√m2 VW sσ (1 / √m1 W Xi))
i=1
LE e sup
n V,W ∈S
1	n
—Ee sup	Ci aT σ (1 /√m2 Vxii)
n V,W∈S i=1
LE e sup sup
W∈SV∈S
n
n
X itrace(V (Zi0 + Zi0+))
i=1
.1 Ee SUP
n W,V ∈S
itrace(VZi0) +
i=1
n
1	n	4/3
≤ —Ee sup sup trace(V(X	)) +	，Y二∖ /%
n	W∈S V ∈S	i=1	(κ2 m2)1/3
1n
=—Ee sup sup trace((V - V(0))(T CZi))
n	W∈SV∈S	i=1
1	n	4/3
+ -Ee sup trace(V(O)(X CiZi)) +	YYJ"3 ∙
n	W∈S	i=1	(κ2 m2)1/3
(80)
For the first term, for every j ∈ [m2], define Hj to be the set of i’s in [n] where the jth column of
Zi0 is non-zero, i.e.
Hj = {i ∈ [n] : Vj(0)x0i ≥ 0}.
Here, we use the crucial assumption that (V - V(0))j Tφ(0) (xi) = 0, so we can drop the φ(0) (xi)
term when x0i is multiplied to V - V(0). Using this trick and applying Cauchy Schwarz, we bound
the first term as:
LEe sup trace((V -
n	W,V ∈S
≤ n EjV -V ⑼ k IXSt
i=1
m2
m ∑k∑Ciφ(2)(xi)k2.
m2 j=1 i∈Hj
Further using Jensen’s inequality:
LEe sup trace((V -
n W,V ∈S
≤ kV - V(0) k
_ n t
i=1
m2
E e ~∑ sup k	Ciφ(2) (xi)k2.
m2 j=1W∈S i∈Hj
(81)
Using Equation (110) of Lemma 6 (note that we do not have the smoothing matrix Wρ here, so we
are allowed to use this result), we obtain
kW-W(0) Z k.φ (%) k. √12 (mm3)”,
where Zik ’s are defined in Equation (317).
Plugging this back in (81):
30
Published as a conference paper at ICLR 2022
E sup k	iφ0(xi)k2
W∈S i∈Hj
≤ 吼 sup (|| X eihW - W⑼,Zk川 + 2^2(n3m3 )1 /4)2
W∈S' i∈Hj	r 1 m 1 λ0	)
.Ee SUp Il X eiW - W(O),Zk)k2 + C3(nm3)1 /2
W∈S i∈Hj	i κ1 m1λ0
=吼 SUp X (trace((W - W(O))( X	)))2 + C3(n3m3)1 /2.
W∈Sk=1	i∈Hj	κ1 m1 0
(82)
Now for every fixed dataset, with high probability over the randomness of Ws, for every k1 6= k2:
hXeiZik1, XeiZik2i ≤ X	hZik11,Zik22i
i∈Hj	i∈Hj	i1 ,i2∈Hj
1	m1
=m~ £ IEWsIjWs2 ,jhxx 1，Xi 2 iHW(ff) Xi 1 ≥ 0 }1{W(,0)Xi 2 ≥ 0 }|
1 i1 ,i2∈Hj j=1
But note that because (xi 1 ,Xi2i ≤ 1, the variables W^γ,jW^2 jx% 1 ,χ%2il{W(-0Xi 1 ≥
0}1{W,(0)Xi2 ≥ 0} are SUbgaUssian with parameter one with respect to the randomness of Ws.
Hence, with high probability over the randomness of Ws, we get
I(X eizk1, X eizi2i∣. m X √m ≤√m.	(83)
i∈Hj	i∈Hj	1 i1 ,i2∈Hj	1
Therefore, with high probability over the randomness of W(0) and W0 and the dataset, we get
Equation (83). In order to get rid of the high probability argument on the dataset, we use the stronger
Equation (318) in Lemma 46 which uniformly bounds (Zk1 (X), Zk2 (X0)i by log(m1)d/m1 for any
X, X0 , which in turn gives
I(XeiZik1, XeiZik2iI ≤ X I(Zik11,Zik22iI .
i∈Hj	i∈Hj	i1 ,i2∈Hj
n2 d log(m1 )
√m 1
with high probability, independent of the choice of dataset. This bounds is slightly worse comapred
to (83), but still efficient for our purpose.
Furthermore, a similar bound to Equation (83) can be obtained in a more adversarial situation when
we also take maximum against the choice of the dataset.
Note that the entries of Pi∈Hj eiZik for 1 ≤ k ≤ m3 can differ only in a sign. Therefore, their
norms are all equal. Now suppose that Cj is the random variable of the norm of these variables:
Cj := k	eiZikk.
i∈Hj
Then, by substituting rk = Cj P记鼻㈡2由 in Lemma 40, We get
X (trace((W - W⑼)(X eiZk)))2 ≤ C2(1 + m2O(nm))kW - W⑼kF (84)
k=1	i∈Hj	m1 j
= (Cj + n2m*gmI))kW - W(0)kF.	(85)
Now recall from Equation (79), we have
kφ0(Xi)k ≤ γ1.	(86)
31
Published as a conference paper at ICLR 2022
Hence, we can apply Corollary 5.1 with φ(2)(xi) and C1 substituted by φ0(xi) and γ1 respectively,
to argue with high probability over the initialization, there exists a set Pi such that for every i / [n]
and j / Pi, sign of VjTχi is the same as V⑼ φ(0) (Xi), and moreover,
∖Pi∖ . (rC⅛)113m2.
(m3 κ21 )
Now let
Hj = {i / [n] : V⑼φ⑼(Xi) ≥ 0}.
Note that for j / P = U分 Pi, We have Hj = Hj. Now note that the norm of each Pi∈πj EiZk is at
most one. for each index 1 ≤ ' ≤ m 1 d, as the random variables P记后,Ei (Zk)` are P记后,(Zk)2 ≤
Pi∈[n] (Zk)2 subgaussian, we have with probability at least 1 一 1 over the randomness of Ei's, for
every 1 ≤ ` ≤ m1d and every 1 ≤ j ≤ m2:
I x Ei (Zk)`i ≤ SX(Zk)2 log( m 1 dm 2 n),
i∈]Hj	y i∈ [n]
which implies for every j / [m2]:
IXEiZi I ≤ X X (Zi )` log(m1 d) ≤ n log(m1 dm2 n).
i∈HHj
Name this event B, so
` i∈[n]
P(B) ≤ 1.
n
Note that although Hj might depend on the randomness of Ei's, Hj does not, and if j / P, we
obtain
Cj = Il E EiZkI ≤ √nlog(m 1 dm2n).
i∈H j
Moreover, note that we also have the following worse case bound:
Cj = I X Ei Zik I ≤ X IZik I ≤ n.
i∈Hj
记Hj
Applying the last two inequalities into Equations (297) and (85):
m2
Ee - SUP sup Il E Eiφ0 (Xi ) Il2
m2 j=1 W∈S i∈Hj
C3 n3 m3	1 m2	m3	2
≤ — (—Γ3)112 + —XEe sup X (trace((W - W⑼)(X EiZk)))
κ1 m1λ0	m2 j=1	W∈Sk=1	i∈Hj
≤ C3(nm3)112 + ɪEel{B} X(Cj + n2m2d2g(mI))W 一 W(0)IF
κ1 m1λ0	m2	j	m1	F
j∈P	1
+ ɪ Eel{B} X(C + n 2 m 2『I)) W 一 W (0) I F
m2	j	m1
j∈P	1
+ ɪEel{Bc} X(Cj + n2m3d粤TnI)) ∣∣W 一 W⑼ IlF
m2	j
j=1
√m 1
√m 1
√m 1
≤ C3 (n 3 m 3 )112
κ1 m1λ0
+ IlW — W⑼ IlF h 1P-1 (n2 +
m2
n2m32dlog(m1)
√m 1
+2 n+
n2m32dlog(m1)
√m 1
)]
≤ C3(n-m3)112 + Y2[n3(rC2百产3 + (
κ1 m1λ0	(m3κ12 )
C12	1/3 n3 m23 d log(m1 )
(m 3 K1)
√m 1
+2
n2m32d log(m1)
√m 1
+ 2n .
(87)
32
Published as a conference paper at ICLR 2022
Next, We analyze the term 1 Ee SuPW∈strace(V(O)(P2ι eiZ0))∙ Noting that kφ(0)(Xi)∣∣ .
Kι √m3 with high probability and using Equation (86):
n nn
k ∑^iZi∣F ≤ E ∣Z0kF ≤ E kxik ≤ ∑(kφ0(Xi)k + kφ⑼(Xi)k) . n(√m3K1 + Y1). (88)
i=1	i=1	i=1	i
Hence
1	n	1n
—Ee SuP trace (V(0) (X
eiZi)) = —Ee sup X itrace(V(0)Zi0)
n	W∈S	i=1	i n W∈S i=1	i
n m2
=n 以黑 X ɛi (√m2X *⑼XiI {V(0)X ≥0 ∙
i=1	2 j=1
1 n1	1
=—Ee sup G   aTσ (V(0) Xi) ≤ sup - aτσ (V(0) Xi)
n W∈S i=1	m2	W∈S m2
But using Lemma 30:
LHS
Applying a similar bound as we did in Equation (88) on ∣X0i ∣:
IIXiIl ≤ kφ(0)(Xi)k + kφ0(Xi)k . K1 √m3 + Y1 ∙
Substituting above, we get
1	n
-Ee sup trace(V(0)(T tiZi)) ≤ K2√m33(K1 √m33 + Y1).
n	W∈S	i=1	i
(89)
Finally, Substituting Equations (87) into (81), then combining it with (89) into (80), we obtain a
bound on Rademacher complexity which holds w.h.p over both the randomness of the initialization
and the dataset:
RG ι,γ 2) ∣χ.,. ∖/C3 (n3m3 )1 /2
K1 m1λ0
(90)
C 2
(m 3 k 1)
1/3
( C2 " / 3 n3 m 3 d log( m 1)
+(( m 3 κ2)	√m11
+ 2 n 2 m 3 d log( m I) + 2 n
√m 1
(91)
+ κ 2 √m 3( κ 1 √m3 + γ 1) +
Having enough overparameterization, we have for every dataset (X, y) (i.e. worst-case Rademacher
complexity):
R(GY 1 ,γ2 ) ∣χ,y ≤ 2Y1Yγ2/√n∙
(92)
Note that for the bound (92) to hold, the overparameterization should be picked poly large in Y1, Y2,
as well as in other basic parameters. However, noting Equations (49) and (56) in the proof of
Theorem 3, we set Y1 = 1 ,y2 ≥ Ω(B, n, 1 /y0) in Theorem 3, so Y1Y2 is at most poly in the basic
parameters. Therefore, again the overparameterization can be picked polynomially large in the basic
only parameters (i.e. independent of Y1, Y2 or ζ).
33
Published as a conference paper at ICLR 2022
A.12 Constructing W *, V *
This section consists of two subsections; First, we prove a structural result for the first layer weights
(W0 , V0 ) that the algorithm visits, then construct a weight matrix W * for the first layer with some
good properties. Second, we do the same thing for the second layer (however, the structure of the
first and second layers are completely different). Through out this section, we assume we have the
norm bounds kW0k ≤ C1, kV0k ≤ C2.
Notably, we rely on a number of basic Lemmas more related to the representation power of the
network, which we defer their proof into a later Appendix B.2 and refer to them here as needed.
A.12.1 FIRST LAYER, CONSTRUCTION OF W*
Lemma 1 Supposem ι ≥ 16 n2 m 2/λ 0 .Let Pi = {j ∈ [ m ι] ∣ ∖wj°^ Xi∣ ≤ C 2 / √m J and P = ∪Pi.
During SGD iterations, suppose we have kW0kF ≤ C1. Then, for a value c2 satisfying
2C1 √nm2/Vλ0 ≤ c2 ≤ K1 λ0√m1 /(2n2),
with high probability ∀i:
∣Pi∣ . C2√m 1 /k 1,
and for j ∈/ P, during the whole algorithm we have
kWjk ≤ '√mc + C2/(4√mI) ≤ C2(2√mI),
m1λ0
c2/√m2 ≤ ∣W(0)χi∣.
So the signs of neurons outside P never changes. In particular, we can set c2 as small as c2 二
C1 √nm2/∖∕λ0. In the rest ofthe proof (i.e. other sections), we set C2 to this value.
Proof of Lemma 1
Define the matrix
〜.
Zk
(WSjXi" ： WwTxi ≥ c2/√m 1})n=1.
Let Pi be the set of indices j such that 1{W(0)Txi ≥ C2/√m 1} is zero. First of all, note that by
Bernstein inequality:
∣Pi∣
≤ c 2 √m/κ 1 + o( JC 2 √m /κ 1
+ 1) . C2 √m 1 /κ 1.
Now suppose that until the current iteration of the algorithm the assumption has been true, i.e. the
signs of the neurons outside of P have never changed. As a result, due to the specific update of the
SGD for both of the terms EZ'(fvo,w0 (x), y) and ∣∣ W0∣∣F, if We define W0∣p to be the restriction
of W0 to indices that are not in P (i.e. the columns in P are equal to zero), then we can write
m3 n
W 0∣pt = ∑ £»Z k.
k=1 i=1
(93)
An issue here is that We also have some injected noise by PSGD into W0 Which violates Equation (93).
To handle the injected noise as Well, We define the subspace Φ0 of Rm1 ×d matrices to be the set of
vectors With arbitrary roWs for j ∈ [m1] With j ∈ P, While restricted to the other roWs j ∈/ P in
should be in the span of (Zi)i,k. Then, we decompose W0 into subspaces Φ0 and Φ0⊥ respectively
as W0 = W0(1) + W0(2), Where W0(1) ∈ Φ0,W0(2) ∈ Φ0⊥. Here, We Want to prove ∣W0(j1)∣ ≤
C2/(4√m 1). We handle the ∣∣ W0j2) ∣∣ part in Appendix 45. So instead of W0∣p in Equation (93) we
consider W0(1)∣P:
T	m3 n
W0 (1) ∣p = XX ɑk,iZ k.
k=1 i=1
(94)
34
Published as a conference paper at ICLR 2022
We handle the other part W 0 (2) in Appendix 45. Now exactly similar to the drivation in Lemma 38,
we can state with high probability
C12 ≥ kW0k2 ≥ kW0(1) k2
m3	n
≥ M'⑴同2 ≥ E k ∑jak,iZk kk - O(nm3/√m 1) E ∣∣αk∣∣2.	(95)
k=1 i=1	k
Note that we are exploiting the fact that the norm kM0 k remains bounded by C1. Now using a
Hoeffding bound for matrix H∞0 defined below, we write:
H∞i 1 ,i2 := EW:N(0,Rd)l{∀i : WTgI ≥ c2/√m 1 }xTXi2(l{wτXi 1 ≥ 0}1{wTXi2 ≥ 0})
=Ew: N (0, R d )( 1{WTXi 1 ≥ 0 }1{WTXi 2 ≥ 0 D χTι χi 2
± O(El{3i : ∣wτXi∣ ≤ C2/√m J( 1{wtXi 1 ≥ 0}1{wtXi2 ≥ 0}))XTIXi2
=Hi∞,i2 ± O(nc2/(√m 1K1) llXiIkkXi2 k )
=H∞,i2 ± O(nc2/(√m 1K1)).	(96)
NoW opening Equation (95) and using the property C2 ≤ k 1 λ0√m”(2n2), We get
LHS = XX ɑk,i 1 αk,i 2 hZ k1k2 i - O (nm 3/√m 1 X llɑk k 2)
k i1 ,i2	k
=XX ɑk,i 1 αk,i 2 (H ∞i 1 ,i 2 ± O (1 /√m 1)) - O ( nm 3/√m 1 X llak Il 2)
k i1 ,i2	k
≥	αk,i 1 αk,i2Hi∞2 ± Ilak∣∣2O(nc2/√m 1K1) - O(nm3/√m 1 X ∣∣ɑk∣∣2)
k i1 ,i2	k
≥ X akH∞ak - O(nc2/√m 1K1) X ∣∣ak∣∣2 一 O(nm3/√m 1 X ∣∣ak∣∣2)
k	kk
≥ X aTH∞ak - O(c2n2/√m 1K1) X ∣∣ak ∣∣2 - O(nm3/√m 1 X ∣∣ak ∣∣2)
k	kk
=(λ0 一 O(nm3/√m 1) - O(C2n2/√m 1K1)) E ∣∣ak∣2
k
≥λ 0 / 2 X ∣ak 12.
k
For the last line to hold, We need enough overparameterization. This implies
X∣ak12 . C2/入0.
k
NoW again, exactly similar to the derivation in Lemma 38, for j ∈/ P We have
∣M0(j1)
Il ≤ Jnm3/√m1
Which completes most of the proof. For the rest, We are left to shoW that for the other partM0(2),
We have ∣M0(j2) ∣ ≤ C2/(4√m 1), which we do in Appendix 45.
Lemma 2 Under condition m3n/ √m 1 ≤ λ0/4, there exist matrices {Wk}m=1 ∈ Rm 1 ×d s.t. for
every k 6= k0 ∈ [m3] and i ∈ [n]:
Wk,zk 0 i = 0,
∣Wk∙- W+1 . Pm3∣Vk∣H∞,
λ0 m1
IMk,zki-hw+ ZiI . λ√= ∣Vk∣h∞.
35
Published as a conference paper at ICLR 2022
Furthermore, for k1 6= k2:
lhwkι,Wk2il ≤ λ2√m1 (1+ √m3)MjH8M21由8∙	(97)
Proof of Lemma 2
Let
Wk+ =	Vk,iZki .
i
we want to compute the norm of the projection P (Wk+) of Wk+ onto the subspace spanned by all
Zki 0 for k0 6= k and i ∈ [n]:
kP(Wk+)k2= (hWk+, Zki0i)kT06=k,i∈[n]hZki11, Zki22i-1	(hWk+, Zki0 i)k06=k,i∈[n],
1	2	(k1,i1),(k2,i2)∈[m3]-{k}×[n]
(98)
where the first and third terms are vectors and the middle term is a matrix. Now note that for each
k0 , k1, k2 6= k, by Hoeffding inequality:
必 1 Z2i)..「= H ∞ + ( ± 1 /√m) i l,i 2 ∈ [g	(99)
i1	,i2∈[n]
hwk^,Zk0i = hXVk,iZk,Zk0i. 3 X Wkil
m1
i 1 i
≤√√n kVk k
m1
≤ -7= kVkkH8.	(100)
m1λ0
Therefore,
k ( hW+,zk0 i ) !T0 = k,i∈[ n ] k ≤ n r m 1 λo kVk l∣H8 .	(IOI)
Now Equation (99) implies for small enough m1
λmn((hZk 1 Zkii	「J ≥ λ0/2,	(102)
i1,i2∈[n]
as long as λ0 ≥ 2n/m 1. Moreover, define A to be the block version of
A0= (hZki11,Zki22i-1	,
1	2	(k1,i1),(k2,i2)∈[m3]-{k}×[n]
i	.e. for k 1 = k2 they are the same but for k 1 = k2 A is zero. Then
∙-v
λmin ( A) ≥ λ0/2,
because the eigenvalues of each block is at least λ0/2 using Equation (102). But note that
∣A0 - Jl k 2 ≤ ∣A0 - AkF ≤ m 3 n√m.
So as long as m3n/√m 1 ≤ λ0/4, We have λmin (A) ≥ λ0/4. Combining this fact with Equa-
tion (101) and plugging it into Equation (98), we obtain
2
kPW+) k2.	kVk k H
m1 λ0
8.
36
Published as a conference paper at ICLR 2022
Now define W = Wl+ — P (Wl+). Then
WX — W+ Il = IIP(W+)k < :nmkVkkH∞,
λ 0 √m 1
K WX — W+Z)| ≤ IIP(W+) IIkZi k < ^m ∣Vk∣H∞.
λ 0 √m 1
Furthermore, note that WX2 is orthogonal to W+ for kι = k2, so
KWX1 ,WX2 )1 = KW + — P (W +) ,WX2 )1
=∖(P (W +) ,W+ — P (W +) )|
≤kP(W +) IlkW+ — P (W +) k
≤kP(W +) I( IlW+1 + IlP(W +) I).	(103)
But note that
IW+1 = IX Vk,iZiI ≤ X |Vk^I ≤ X |Vk,i| ≤ √nIvkI2. ≤ PIVkIH∞.
Therefore, we can bound Equation (103) as:
∣wχχι ,w'χ2)∣ ≤ 导 √m (ι+√m)IV k/H∞IV 2 2 ih∞ .
Lemma 3 There exists a matrix W+2 such that for every j ∈ P, W+2 j = 0, and
|trace (W +2 Z2) — x i, 21 ≤ C1 √m3n2 / (λ 0 K1 √m1) ∣∣V2∣H ∞.
Proof of Lemma 3
Define W +2 to be equal to W + for j ∈/ P and equal to zero vector otherwise. Then, by
Lemma 38: (note that |马 | ≤ c 1 √nm √m / (√x 0 K1))
|trace(W+Z2) — trace(W+2Zli)| ≤ 1 √ 1 E |W+jxi|
jeP
≤ ɪ IW+1
√m 1
≤√nm / (m 1") ∣p |IvkIH ∞
≤ C1 m3n2/((λ0κ 1 √m) IV2IH∞ .
Combining this with Lemma 37, the desired result follows.
Lemma 4 Under condition m3n/√m 1 ≤ λ0 /4, there exist matrix WX's exactly satisfying the same
conditions in Lemma 2 but with respect to W+2 instead of W+, and moreover, for j ∈ P we have
WkXj = 0.
Proof of Lemma 4
We can repeat the exact same procedure of Lemma 2 for W +2 . Using the bound in Equa-
tion (96), we have
(Z 1、Zk2))..「= H 0∞ + θ ( ±1 /√m) i 1 ,i 2 ∈ [ n ]
'	i i 1 ,i 2 ∈ [ n ]
=H ∞ + ( ±nc 2/√mK I) i 1 ,i 2 ∈ [ n ] + θ ( ±1 / √m ) i 1 ,i 2 ∈ [ n ]
=H ∞ + ( ±nc 2/√mK I) i 1 ,i 2 ∈ [ n ],
37
Published as a conference paper at ICLR 2022
so as long as
n2c2/√m 1K1 = n2Cι√nm3/(K1 √m 1 λ0) ≤ λ0/2,
with similar argument as in Lemma 2, we get
λmin (
ZZ k 2i i l,i 2 ∈ [ n]) ≥ 0 / 2 ∙
Moreover,
M+2 4 k 0 i = h∑yz,iZ k ,Zk J
i
√^- X Wk,i∖
√m 1 V
≤
kVk k
≤
√mnλo M kH∞
Thus, using the same argument as before the proof is complete.
Lemma 5 Suppose
m 1 ≥ n 7 m 3/λ 0.
During SGD, suppose we are currently at (V 0, M0 ) withM0 ≤ C1 . For any matrix M1 , we denote
the signs of the first layer imposed byM1 by DW1 ,xi. Then with high probability, there exists
W * = ∑ke [ m 3] Wj^ such that Wj^ ,s is orthogonal to all other Zk o's for k0 = k, and for ^very
i ∈ [n], we have:
k√1m1W sDW ⑼+WgW F-X ik . √mλ h1 + nC i kVkkH ∞ :=沆kVj kH∞ -
Moreover, we have
nWj*k ≤ √nm3 / (P m 1λ 0) yXx kVk k H∞+
n⅛(? MS ):=。
m 3
m1
(104)
Particularly, for any diagonal sign matrix Σ ∈ Rm3 ×m3, we have
kWς k F ≤ (nλ2n √m3 (ι+√m3)+(1+Ornlλ 0 √m 1)+λm))) X kVjk H
(105)
which, by having enough overparameterization, implies
kWΣ* kF ≤
(106)
where
m3
WΣ* :=	ΣjWj*.
j=1
(107)
Moreover, we have
√m w sDW (0)+WomW* Xi=ς √m w sDW (0)+W E w *xi∙
(108)
Proof of Lemma 5
From Lemma 3, we have
∖Xi,k - trace(W+2Zj)∖ ≤ C1 m3n2/(λ0K1 √m1) kVj∣∣h∞.
y2XVi∞
区,
38
Published as a conference paper at ICLR 2022
Combining this with the result of Lemma 4, we get:
同Ltrace(%Z收+ J k惘小
√mλ0 [1 + 守 kVkkH∞ .
(109)
On the other hand, based on the property that W^ j = 0 for j ∈ P and its orthogonal property from
Lemma 4, for j ∈ P we get
√— WsDW (0)+ W ,χ W ^xi = √— WsDW (0) X W ^xi
m1	,xi	m1	,xi
=trace( W %) = trace( W^Zk)
=√— WsDW (0) X Wkxi,
m1	, i
which combined with Equation (109) completes the proof. From the above, Equation (108) is also
clear. Finally, note that by Lemma 38 we have
IIW+2 j Il ≤ √nm3/(∖Jm 1 λ0)
IVk I2H
k
which Combined with Lemma 4 implies
kW；k ≤ √m / (pmλ)yχ而晨+
"3 (? kVkkH∞ ):=。
m 3
m1
while the other claims follows from Lemma 39 and Lemma 4, combined with Equation (97):
kWςkF ≤EkWktk2 + E IhWki,Wli∣
k	k1 6=k2
≤ X "罚 2 + ⅞ √3 (1 + √3 )(X kVkkH∞ )2
≤ X 叫 k2 + ⅞ √3 (1 + √3) √ (X IVk kH∞)
≤X kWk*k2 + 手 √3 (1 + √3 J
≤ f√1 (1 + 得)Z1 + X kW+2k2 + n≡ X IVkkH∞
≤ W端(1 + *)Z1 + X kW+ k2 + λ≡ X kVkkH∞
nʌ/n ʌ/m3
(1 +
挚)Z1 + (1 + 0( n/ (λ 0 √m 1) + nm ))Z1.
√mι	λ 0 m 1
Next, We move on to construct V * for the second layer.
A.12.2 Second Layer, Construction of V *
In this section, We present a couple of lemmas that step by step lead to the construction of V*.
We remind the reader that φ(0) (xi) is the output of the first layer at initialization Weights, φ0(xi)
and φ(2) (xi) are the changes in the output of the first layer When W0 and W0 + Wρ are added,
39
Published as a conference paper at ICLR 2022
respectively, and finally φ* (Xi) is the optimal features that are generated by the matrix W * but with
the sign pattern of W(0) + W0, i.e.
φ* ( Xi ) = √m W s Dw (0)+ W 0 ,Xi W *xi.
We also define x0i as
Xi = φ ⑼(Xi) + φ ⑵(Xi) = √1mι W sσ ((W ⑼ + W0 + WP) Xi).
To begin, we state a lemma to bound the magnitude of kφ0(Xi)k, given that the norm of W0 is
bounded by C1 and the sign pattern Sgn (W (0) + W0)Xi satisfies condition stated for the set of
indices P in Lemma 1. Later on, we exploit this Lemma in Lemma 33 to state bounds for kφ(2)(Xi)k.
Lemma 6 Let the matrix W0 with norm bound kW0k ≤ C1, such that the signs of (Wj(0) + Wj0)Xi
and Wj(0)Xi can be different only for j ∈ P, for P defined in Lemma 1. (Note that for W0 at every
step of the algorithm, this is automatically satisfied by Lemma 1) Then
2 C 3 / 2 n3 m 3 …	C
kφ ( Xi ) k ≤ I- (-----O + (I + O ( m 3 / √m 1)) C1 .
κ1 m1λ0
Particularly for large enough m1 compared to n, m3, λ0, κ1,C1, we have
kφ0(Xi)k .C1.
Proof of Lemma 6
We write
lφk (Xi ) - hW 0,Zk i∖ ≤ 2/√m1 X \WjXil ≤ 2/√m1 X kW0I
j∈P	j∈P
--- 9Λ73/2 <n3m∙-> .,.
≤ 2P∖P∖/√mkW 1|尸 ≤ — (T)1 /4,	(110)
κ1 m1λ0
where the last line follows from the bound on ∖P∖ from Lemma 1.
On the other hand, because by Hoeffding We know that hZk, Zk，). 1 / √mι by Lemma 40, We get
m3
EhW0,zkY ≤ (1 + O(m2/√m 1))kW0kF ≤ (1 + O(m3/√mι))C2.
k=1
Combining this with Equation (110), we get
kφ0(Xi)k ≤
k ∖φ(k2)
(Xi) - hW0,Zkii∖2+
^XWZi
≤ √/2(nm)1 /4 + (1 + o(m3/√mι))cι.
κ1 m1λ0
(111)
Next, we prove a structural lemma regarding the sign pattern in the second layer when we feed in X0i
to it, with the important message that the dominance of sign patterns are specified by φ(0)(Xi).
Lemma 7 Suppose we have m3 K1 & C2, K2 √m3 ≥ C2, and m 1 satisfies the condition on
Lemma 6. If we have the condition kφ(2) (Xi)k . C1, which happens under the high probability
event Ec defined in Lemma 33, then for every i ∈ [n], there exist a subset Pi which might depend on
W(0),V(0),W0,V0, such that
∖Pi∖ .
((m⅛ )1 /3
+ (c3 +
22
C1	)(_C2_
c23m3K21	K22m
m2.
40
Published as a conference paper at ICLR 2022
∙-v
Moreover, for every i ∈ [n], for j ∈/ Pi,:
2|V；(0)φ⑼(Xi)I ≥ |Vj(0)φ⑵(Xi)I + M(φ⑼(Xi) + φ⑵(Xi))I,
IV(0)φ(0)(X)I & (m)1 /3C223c3∣φ⑼(X)II,
IV(0)φ⑼(Xi)I & (m)123C223c3∣χi∣∙
Proof of Lemma 7
By assumption, we know that during the algorithm, we have IV0 I ≤ C2 . Also, we know by
Lemma 33 that under Ec :
Iφ(2)(Xi)I ≤ 2C1∙
Define the set
Pi = {jI IV(0)φ(0)(Xi)I ≤ C3(m)123C223IΦ(0)(Xi)∣}	(112)
and P0 = ∪Pi0 . We have
P(IVj(0)φ⑼(Xi)I ≤ c3(K)123C223IΦ(0)(Xi)H) ≤ c3(K)123C223/κ2,
j	m2	m2
so by Bernstein, with high probability:
IP0I ≤ m2C223c3(K)123/κ2 + m Im2C223c3(K)113∕κ2 + 1 . C3m2C223(K)123/κ2,
m2	m2	m2
so with high prob.
IP0I . C3C223(m2)223∙	(113)
2	κ2
On the other hand, Note that
m1
φ k°)( Xi) = E 1 ∕√m 1 WSj σ (W⑼ Xi)	(114)
j=1
is subGaussian with parameter σ2 = O(1∕m1 Pj σ(Wj(0)Xi)2). Furthermore, note that if we com-
pute the variance of φ(k0)(Xi) with respect to the randomness of Ws:
m1
Eφk"(Xi)2 = 1 /m 1 X σ(W(a)Xi)2 := N
j=1
which itself concentrates around 1 /2K2 IIXill2 = 1 /2K2 by another Bernstein, i.e. N = 1 /2K 1(1 ±
O(1 /√m 1)). Therefore, by concentration of subexponential variables (Bernstein), it is not hard
to see that the squared norm of the vector φ(0) (Xi) is (m3K41, K2)-subexponential and concentrates
around m3N, i.e.
∣∣φ(0)(Xi)H2 = m3N ± O(κ2√m3) = m3κ2[/2 ± O(m3κ2//√m 1) ± O(κ2√√mt3),	(115)
with high probability. Combining this with the fact that Iφ(2) (Xi)I . C1 implies with high proba-
bility:
hφ(0)(Xi) H > √m3K 1	z116
PWiI &	∙	(I)
Now define Pi0 = {j I {VjX,iI ≥ [Vj0^ φ (0)( Xi) I/3} .If j ∈ Pi0 - Pi, then by Equation (116), with
high probability
HVjiHHφ(2)(Xi)H ≥ IVjiφ(2)(Xi)I =IVji(φ(0)(Xi)+φ(2)(Xi))I= IVjiXiiI
41
Published as a conference paper at ICLR 2022
≥ |V^(0)φ⑼(Xi)∣/3 & c3(m)113C2/3I∣φ⑼(Xi)II,
or	2
∣%∣∣2 & C2(κ2)223C443手.
j m2	C12
But note that IV 0 I2F ≤ C22 by our assumption, which implies
∣P00 - PiI. CC/[C2(存223C4/3噜]=CO(詈)223.	(117)
m2 C1	c3 m3 κ1 κ2
Now combining Equations (113) and (117), we finally obtain
∣P0'I = ∣P0' - PiI + ∣P0I . (C3 +	)C223(m)23.
C32m3 κ21	κ2
Now define the set
Pi0 = {jI |V；(0)φ(2)(Xi)I ≥ |Vj(0)φ(0)(Xi)∣/3}.	(118)
Note that for every j ∈ [m2], Vj(0)φ(0)(Xi) is gaussian with variance Iφ(0)(Xi)I over the randomness
of Vj(0), so
P(Vj(0)φ(0) (Xi) ≤ ακ2 Iφ(0) (Xi)I) . α.
Therefore, if we define the set
Qi = {j ∈ [m2]I IVj(0)φ(0)(Xi)I ≤ ακ2Iφ(0)(Xi)I},
then for large enough m2, by Bernstein with high prob.:
IQiI . αm2.	(119)
Now note that φ(0) (Xi) is fixed during the algorithm. On the other hand, by random matrix the-
ory, We know that with high probability, the eigenvalues of the matrix V(0) are in (K2(√m2 —
√m3), κ2(√m22 + √m3)). Therefore, even if the vector φ⑵(Xi) is picked adversarialy (because
it keeps changing during the algorithm), we get that with high probability over the randomness of
V(0):
∣V(0)Φ(2)(Xi)Il2 ≤ κ2(√m2 + √m3)2IIΦ(2)(Xi)Il2 . K2m2∣∣Φ(2)(Xi)∣∣2.	(120)
Moreover, because Iφ(2)(Xi)I . C1 and from Equation (115), with high probability over the ran-
domness of W(0) :
lφ(0)(Xi) H & √m3K1
∣φ(2)(Xi)I &	Cι .
This means that for j ∈ Pi000 - Qi , combining these inequalities we conclude with high probability
IV(0)φ(2)(Xi)I ≥ IV(0)φ(0)(Xi)I/3 & ακ2∣φ(0)(Xi)I ≥ ακ2^CK ∣φ⑵(Xi)∣,
which combined with (120) implies
IPi000 I .
m 2 C2
m3 K21 α2
Balancing this term with the one in Equation (119), we set
α :=
C1223
12 3 2 2 3 ,
m3 K1
which implies
∣P0"I . W- QiI + IQiI ≤ (r⅛)123m2.
i i	(m3 K21 )
∙-v
Defining Pi = Pi0 ∪ Pi0, we finally get
∙-v
IP,i∣.
fc2) )12 3 + (C3 + C2 : 1κ2 )C2 2 3(	)12 3) m 2.
m3K1)	C3m3K1	K2m2
Clearly by the definition of Pi00 and Pi000 the proof is complete.
42
Published as a conference paper at ICLR 2022
Corollary 5.1 Under the condition kφ(2) (xi)k . C1 (which happens under the event Ec defined in
C2/3	κ2m
Lemma 33), setting C3 :=	1 /3 2/3 (KCm )113 in the previous Lemma, we obtain ∀i ∈ [n]:
∣Pi∣ . ()113m2∙
(m3 κ21 )
∙-v
Also forj ∈/ Pi, the conditions in (112) and (118) becomes the same as
C213
|W7(O) Φ (0)( Xi) l≤ K 2	1/3 2 / 3 M (0)( Xi) H∙
m3 κ1
Hence, for every i ∈ [n] andfor j / Pi, with high probability:
3∖w((0')Φ(0)(Xi)| ≥ |Wj(0)Φ(2)(Xi)| + ∣W0(Φ(0)(Xi) + Φ(2)(Xi))|,	(121)
|Wj(0)Φ(0)(Xi)I & K2-Cζ73kΦ(0)(Xi)k & K2(√m3K 1 C2)113,	(122)
m3 κ1
C2/3
|Wj(0)φ(0)(Xi)I &	ι/3 2/3 kXik∙	(123)
m3 K2
Next, we state concentration result for the gram matrix of φ(0) (Xi)’s.
Lemma 8 For every i1, i2 ∈ [n], with high probability over the randomness of W(0) and V (0) we
have
hφ(0)(Xi 1),φ(0)(Xi2)i = m3Eσ(W(O)Xi 1)σ(W(O)Xi?) ± O(m3K1 /√m 1 + √m3K2)∙
Proof of Lemma 8
First, we compute the expectation:
EhΦ(0)(Xi 1),Φ(0)(Xi2)i = 1 /m 1	X X EWS,j1WS,j2σ(W(0)Xi 1)σ(W(0Xi2)
j1 ,j2 ∈[m1] k∈[m3]
= 1/m1 X E X Wks,j1Wks,j2σ(Wj(10)Xi1)σ(Wj(20)Xi2) + m3/m1 X σ(Wj(0)Xi1)σ(Wj(0)Xi2)∙
j1 6=j2 k∈[m3]	j∈[m1]
But σ(Wj(10)Xi1 )σ(Wj(20)Xi2) is (m1K41, K21)-sub-exponential, so
X σ(W(^)Xi 1)σ(W(^)Xi?) = m 1Eσ(W(^)Xi Jσ(Wj(O)Xi?) ± O(√m11 K1),
j∈[m1]
which means with high probability:
Ehφ(0)(Xi 1),φ(0)(Xi2)i = m3Eσ(W,(0)Xi Jσ(W(O)Xi?) ± O(m3K2/√m 1)∙
On the other side, we know that φ(k0)(Xi1 ) is subgaussian with parameters σ2	=
1 /m 1 Pj (W(O)Xi 1 )2 := N 1 and σ2 = 1 /m 1 Pj (W(O)Xi? )2 := N2 respectively. On the other
hand, we know that by Bernstein w.h.p
N 1 = 1 /2k2(1 ± O(1 /√m 1)),
N2 = 1 /2κ2(1 ± O(1 /√m 1))∙
Hence, φF)(Xi Jφk2 (Xi2) is (N 1 N2, √N1 1 N2)-subexponential, and so hφ(0) (Xi J, φ(0)(Xi2 )〉is
(m3N 1 N2, √N 1 N2)-subexponential. Therefore, applying another Bernstein on the top, We get
hφ(O) (Xi 1 ), φ(O) (Xi2 )i = Ehφ(O) (Xi 1 ), φ(O) (Xi2 )i ± O( √m3 PN 1 N2 )
=m3Eσ(Wj(O)Xi Jσ(Wj(O)Xi2) ± O(m3K2/√m 1) ± y^m3K1 (1 ± O(1 /√m 1))
2
=m3Eσ(W(O)Xi 1)σ(W(O)Xi2) ± O(m3K1 /√m 1 + √m33K2)∙
43
Published as a conference paper at ICLR 2022
Now we define the matrix Li ∈ Rm3×m2, with its jth column Li,j equal to
√mj21{匕⑼ φ(0)( Xi)≥ 0 W( Xi).
First, we state the following lemma which characterize a concentration result for the gram matrix of
(Li)in=1.
Lemma 9 With high probability, we have the following approximation:
hLi 1 ,Li2 = hφ(Xiι), φ*(Xi2)i hF2(2F3(hxi 1 ,Xi2)) ± O(m-1 /4 + m-1 /4 + m- 1/4)].
Proof of Lemma 9
By Hoeffding:
hLi i ,Li 2 i = 11m 2 X Φ* (Xi I)T Φ* (Xi 2) 1{“⑼ Φ (1)( Xi i) ≥ 0 }l(Vj(0) Φ (1)( Xi 2) ≥ 0 }
j ∈m2
=Φ* (Xi I)T Φ* (Xi 2 )(El{Vj⑼ Φ (1)( Xi 1) ≥ 0 }l(Vj(0) Φ (1)( Xi 2) ≥ 0 }± O (1 /√m 2))
=Φ*(XiI)TΦ*(Xi2)(F2(hΦ(1)(Xi 1),Φ⑴(Xi2)i/(∣∣Φ(1)(Xi 1)∣∣∣∣Φ⑴(Xi2)Il)) ±O(1 /√m)),
where recall
F2( X) = 1 / 4 + arcsin( X) / 2 π,
measures the angle between two unit vectors based on their dot product. Now notice that according
to Lemma 8, with high probability:
hLi 1 ,Li 2 MhΦ* (Xi 1) ,Φ* (Xi 2) i
m m	m 3 Eσ ( W(0)Xi I) σ ( W(0)Xi 2 ) ± O (( m 3/√m 1 + √m3) K 2)
=F2(/	± O (1 Nm 2)
(I (m3Eσ(W⑼Xi 1 )2 ± O((m3/√m 1 + √m3)K 11))(m3Eσ(W⑼Xi2)2 ± O(...)
=F2 (F3% ,Xii R ± √11√m T± O (1 /√m 2)),
1/2 ± O(1/ m1 + 1/ m3)
where recall F3 : [-1, +1] → [-1/2, 1/2] is defined as:
F3(X) :
1— - X2	X X arcsin X
+4+	2 π
It is easy to see F3 has the property that for unit vectors X1 , X2 and w sampled as standard normal:
F3(hX1, X2i) = Eσ(wT X1)σ(wT X2).
But because |F3(.)| = O(1), we have
hLi 1 ,Li2 Mhφ* (Xi 1 ) ,φ* (Xi2 )i = F2(2F3( hXi 1 ,Xi2 i ) ± O(1 /√m 1 + 1 /√m2 + 1 /√m3)).
NoW notice that the derivative of F2, i.e. 1 /2π√1 - x2 is increasing in the interval (0,1), so for a
fixed δ, the maximum of ∣F↑2(X) - F2(X - δ)| happens at X = 1. On the other hand, by writing the
first order approximation of arcsin(1 - t2) around t = 0 and upper bounding its derivative in the
interval [0, 1], we get that for 0 ≤ δ ≤ 1:
arcsin(1 一 δ) ≥ arcsin(1) — 2Vδ.
Therefore, F2(X ± δ) = F2(X) ± O(√). Hence:
hLi 1 ,Li 2 i∕hΦ* (Xi 1) ,Φ* (Xi 2)) = F2(2 FAhXi 1 ,Xi 2 i))
± O(J1 /√m 1 + 1 /√m2 + 1 /√m3)
F2 2F3(hXi1,Xi2i) ± O(m1-1/4 + m2-1/4 + m3-1/4),
which completes the proof.
Finally, we are ready to construct the weights V * for the second layer.
44
Published as a conference paper at ICLR 2022
A.12.3 Construction OF V *
Lemma 10 Let
< =√⅛ hi + ?∙
Suppose we have the condition that for every k ∈ [m3]:
maxkVkk ≤ξ,	(124)
k
where recall the definition of Vk in Equation (58). We assume enough overparameterization to make
sure < < 1. Recall for the matrix A defined by
A = ( hx i 1 ,x: i 2 iF22(2F3( hxi 1 ,xi 2 i )))	,	(125)
1≤i1 ,i2≤n
we have
(f*(xi))in=1TA-1(f*(xi))in=1≤ζ2.
Then, there exists weight matrix V* which only depends on the random initializations W (0) , V(0)
(e.g. not on V0 and W0) for the second layer, such that having enough overparameterization
kV*k2F ≤ 2ζ2,	(126)
and for every j ∈ [m2 ]:
V*k ≤ (1 + <2 n√nζ ξ := % 3 ξ∕√m 2,	(127)
m2
∣∣V*∣∣2 ≤√m[ n(1 + <)]l^^pk^^ := %2/√m2,	(128)
and further under the high probability event Ec defined in Lemma 33:
l√1m^aTDV(0)+V0,χV*Φ*(xi) — f *(xi)I . (√m1κi产3(1 + <)y<2 XX kVkkH∞ ：= <3.
(129)
Proof of Lemma 10
Let
n
V* =	Vi*Li,
i=1
be the minimum norm vector which maps Li’s to f* (xi)’s. As a result, for the matrix
L = (hLi1, Li2 i)
i1,i2
it is easy to see
kV* k2F = (f*(xi))in=1TL-1(f*(xi))in=1.
Now combining Lemmas 5 and 41, we get
kφ*(xi)h ≤ (1 + <∖ξ
∣Φ* (xi) ∣∣≤ (i+<) yχX∣rV<∞,
(130)
(131)
and
hl^* ( xi 1 ) ,φ* ( xi 2 ∖)-hx i 1 ,x i 2 iI ≤ (2 < + < 2)E kVk k H∞ .
k
45
Published as a conference paper at ICLR 2022
Now by Lemma 9:
lhLi 1 ,Li 2 i - Ai 1 ,i 2 1 . (2 沆 + 沆 2)(X MlIH "W2(2 F3(hxi 1 ,xi 2 i ))| + hx i 1 ,x i 2 i ( m - 1 /4 + m - 1/4 + m - 1/4 )
k
+ other cross term.
ByaPPlying hx i 1 ,x i 2〉≤ ∣∣x i JIx i 21∣ we get
LHS≤	(XlVkl2H∞)(2<+	<2)|||F22F3(hxi1, xi2i)|||	+ (m1-1/4 +	m2-1/4 +	m3-1/4)
.(X MlH" (< + m-1 /4 + m-1 /4 + m-") .
k
Therefore,
∣A - hLi1, Li2 ii1,i2 ∣2 ≤ ∣A - hLi1, Li2 ii1,i2 ∣F
≤ n(X ∣Vk IH∞)(< + m- 1 /4 + m- 1 /4 + m1 1 /4) ：= <2 ,
k
Note that <2 naturally goes to zero (with Poly dePendence) as <→ 0 and m1 , m2 , m3 are large
enough. Now if all of the eigenvalues of the matrix A are Ω(1 /n2), then if We overparameterize
enough such that <2 = O(1/n2) with small enough constant so that <2 is less than half of the
smallest eigenvalue ofA, then for the ith eigenvalue λi ofA and L we can write
λi (L) ≥ λi (A) -<2 ≥ λi (A”2,
so
λi(L-1) ≤ 2λi(A-1),
which imPlies the ProPerty
俨*∣F ≤ 2Z.	(132)
However, A might have very small eigenvalues. To remedie this, we use Lemma 42; we can substi-
tute f * with some f such that
B2
Rn ( f* ) ≤ 2 Rn ( f * ) + -n,	(133)
f*τATf* ≤ f*TATf*,	(134)
where f* is on the subspace of eigenvectors of A whose eigenvalues are larger than Ω(1 /n 2JBut
it is easy to check that in the context of Theorem 3, such substitution results in a f*τA-1 f* ≤
f *τ A-1 f * ≤ ζ and v( f*) parameter (as defined in (41)) with respect to f* which satisfies V / 2 ≤ V.
Note that the algorithm is with respect to the setting ν, however we want to exploit generalization
bound with respect to fV* whose parameter is νV as it enables us to use our analysis in this Lemma.
Furthermore, note that using Equation (133) we can further upper bound the empirical risk of fV*
with that of f* , which makes it straightforward to derive a similar generalization bound as in (45)
with respect to f*, of course with a change of constants. Note that fV* is just the sum of A-eigenbasis
directions in f * whose eigenvalues are larger than Ω(1 /n2) Jence, given a pair (f *, G), as we also
point out in remark 1, we can construct the suitable pair (fV*, G) algorithmically and then use that
pair to initialize the parameters of the algorithm (namely ζand ν). Otherwise, ifwe are not explicitly
given a pair (f*, G) and instead want to run the doubling trick described in Theorem 1, we do not
even have any additional computation; since using Theorem 1, within the framework of the doubling
trick, the risk of the final network is competitive with respect to any choice of (fV*, G). Note that as
we mentioned in Lemma 42, the constant 2 is arbitrary and can be reduced to any number less than
2, and it is easy to see that one can pick choice of constants along the way such that we end up with
a factor two behind the risk (first) term in the definition of our complexity measure.
Therefore, without loss of generality we can use substitute f* by fV* and still obtain Equation (132).
46
Published as a conference paper at ICLR 2022
On the other hand, the definition of V * implies
√⅛2 aT DV (0) / *φ* xi ) = f * (xi))
But note that by Corollary 5.1, under the high probability event Ec defined in Lemma 33, DV(0),xi
and Dv(0)+Vo,χ can only be different in the index set Pi and
∖Pi∖ . ()1 /3m2,
(m3 κ21 )
Therefore, for all i ∈ [n]:
∖ √— aT DV (0)+ V * V *φ* ( xi )-√=^ aT DV (0) X V *φ* ( xi ) ∖
m2	m2
≤ 1 /√m 2 X ∖V*Φ* (xi) ∖
j∈P
≤ 1 /√m 2 X kVmΦ* (xi) k
j∈P
≤√Pkv *∣(1+< )SXX∣Vi∞
.
C1
√m33 K ι
)1/3p2 (1 + 沆)jχ MI陷 ∞,
which proves the first claim. On the other hand, we get:
2ζ2 ≥ kV*k2F ≥ VTLV)
But because λmin(L) & 1/n2, we get
VTLV & kVk22/n2,
which implies
M2 . nVZ2 ∙
But now using Equation (130), we can write
n1
∖Vj,kI ≤∑,∖Vi∖∖Lij,k| ≤ Fkφ*(xi)IViI
m2
i=1	2	i
.(√≡ X ∖vi∖≤ (1 + 沆)ξ√n∣v∣∕√m 2
(1 + <) n√nζ2
.----产-----ξ
m2
which proves the other part. Moreover,
kV*k2 ≤ IVk23(XM(xi)k2) ≤ 3n(1 + <),Z2XkVk∞r0∙∙
m2 i	m2	k
(135)
47
Published as a conference paper at ICLR 2022
A.13 Existence of a good direction
Our aim in this section is to show that if the objective value is above certain threshold, there exists a
good random direction which reduces the objective in expectation. Particularly our aim is to prove
the following theorem (informal):
Theorem 6 For a given pair (f * ,G) with
hH∞,Gi ≤ζ1,
f*T(K∞G)-1f* ≤ζ2,
Rn(f*) ≤ ∆,
recall the ideal random matrices (WΣ*, VΣ*) constructed in Appendix A.12, where Σ is a random
diagonal sign matrix. Specifically, WΣ* is defined in Equation (107), and VΣ* is the projection of the
rows of matrix V * onto the orthogonal subspace spanned by (φ(0) (xi))in=1.
Using the parameter setting for i = 1, 2
ν
ψi = 4Zi,，	(136)
with respect to an arbitrary parameter ν > 0, then for every pair (W0, V0) such that kW0k ≤
C1, kV0k ≤ C2 and
L(W0,V0) ≥ ∆+ν,	(137)
for parameters m 1, m 2 ,m 3,1 / κ 1,1 / κ 2 polynomially large enough in B, 1 /λ 0, n, C1, C2 and small
enough step size η, we have
E∑L (卬/ — η/2卬/ + √ηw∑∑, V0 - η/2V0 + √V∑) ≤ L (W∖ V0) - ην/4.	(138)
In order to prove the above theorem, we first state and prove the following lemma which is the core
of Theorem 6.
Lemma 11 For matrices (W*, V*) constructed in Appendix A.12, specifically for their random
coupling (WΣ*, VΣ*) as denoted above, we have:
e∑'(fQ-η/2)W，+√ηw*,(1 -η/2)v0+√ηv*(Xi),y) ≤ (I- η)'(fWo,v‘(Xi),y) + η'(f *(Xi),y) ± η3
where 夕 goes to zero with polynomially large overparameterization (the exact dependence is re-
vealed via the proof).
Proof of Lemma 11
For brevity, we use the notation D0 ,ρ here to refer to the diagonal binary sign matrix when
the input is multiplied by the sum of weight and smoothing matrices. It will be clear in the context
of the equation that what the “input” and the “weight” matrices are. This notation is also defined and
used in Lemma 28). Here, we bound multiple cross terms that are created as a result of moving in
the random direction. To simplify the presentation and avoid confusing recursions in the proof, we
have made a sublemma for each of these cross terms and has deferred its proof to Appendix A.14.
We use difference sub-indices of the symbol < to illustrate terms that go to zero by growing the
overparameterization in our architecture.
48
Published as a conference paper at ICLR 2022
We start by using Lemma 28,
EΣ ' (于01 -η/2) W / + √ηW∑∑, (1 -η/2) Vz + √ηV∑ ( Xi ), yi )
=E∑ Q (E W p,V P f(1 -η/2) W' + √ηW£ + W P, (1 -η/2) V' + √ηV萱 + V P ( xi ) ,yi )
=E∑ Q (E W p,v p aτ D,,ρ (V(0) + (1 — η/2) V0 + VP + √V∑) W sD0,ρ (W(0) + (1 — η/2) W0 + WP + √ηW∑∑) Xi
+ 沆 8 η, yj.
=E∑Q(EWP,VP [aτD,,ρ(V(0) + (1 — η/2)/+ VP)WsD,,ρ(W(0) + (1 — η/2)W0 + Wρ)Xi
+ ηaτDo,^V^WsD,,ρW∑^Xi] + √ηEWp,vp [aτD”(V(0) + (1 — η/2)V0 + VP)WsD,,ρW∑^Xi
+ aτ D，,ρ VS W sD, ,ρ (W(0) + (1 — η/2) W0 + W P) ”]
+ 沆 8 η, y)∙
Now using the notation introduced in Lemma 18, We have
W sD，,ρ (W(0) + (1 — η/2) W0 + W ρ) Xi = φ(0) (Xi) + (1 — η/2) φ (2)( Xi) + 2 φ ⑵0 (Xi).
By Lemma 18, we have the following bound for φ(2)0(Xi):
E W ρ,v p I √m aτDV(0)+ V P + V，x(V(0) + V P + (1 — η/2) V /) φ ⑵0 (Xi )∣
.(K2√m2m3 + √m3β2 + C2)<5∙
Therefore, Combining this with Lemma 20, we get
=E∑Q(EWp,vρ [aTD,,ρ(V(0) + (1 — η/2)V0 + VP)Ws(φ(0)(Xi) + (1 — η/2)φ(2)(Xi))
+ ηaτD，,^V^WsD，,ρW∑SXi] + √ηEWp,vp [aTD，,ρ(V(0) + (1 — η/2)V0 + VP)WsD，,ρW∑SXi
+ aτ D，,ρ VS W sD，,ρ (W(0) + (1 — η/2) W0 + W P) Xi ]
± O((κ2√m2m3 + √m3β2 + C2)<5η) ± O(<8η), yj
=E∑Q(EWp,vρ [(1 — η)aτD，,ρ(V(0) + V0 + VP)Ws(φ(0)(Xi) + φ(2)(Xi))
+ ηaτD，,^V^WsD，,ρW∑Sg] + √nEWp,vp [aτD，,ρ(V(0) + (1 — η/2)V0 + VP)WsD，,ρW∑SXi
+ aτ D，,ρ VS W sD，,ρ (W(0) + (1 — η/2) W0 + W P) Xi ]
± O(η(窿 + 沆4 + (√m,3κ2 + β2)(C1 + √m,3βι))) ± O((κ2√m2m3 + √m,3β2 + C2)%η) ± O(沆8η), y).
=E∑ Q ((1 — η) fW，v，( Xi)+ ηE W p,v P aτ D，ρVS Ws D，ιρW∙S Xi
+ √ηEWp,vρ [aτD，,ρ(V(0) + (1 - η/2)V0 + VP)WsD，,ρW∑SXi
+ aτD，,PVSWsD，,ρ(W(0) + (1 - η/2)W/ + WP)X^
± O(η(员6 + <4 + (√m3K2 + β2)(C1 + √m3β 1))) ± O((K2√m2m3 + √m,3β2 + C2)五5η) ± O(五8η), yj .
49
Published as a conference paper at ICLR 2022
Moreover, using the notation φ*"(Xi) introduced in Lemma 15 and the bound in Lemma 17, We can
rewrite the second term as:
LHS = E∑'((1 一 η) fW0,V0(Xi) + ηEWP,VPaτDo,ρV∑(Φ*(Xi) + φ"(Xi))
+ √ηEWρ,vρ haTD，,p(V⑼ + (1 - η/2)V0 + VP)WsDo,ρW∑∑Xi
+ aτDo,ρVSWsDo,ρ(W⑼ + (1 一 η/2)W0 + WP)Xii
± O(η(员6 + <4 + (√m3K2 + β2)(C1 + √m3β 1))) ÷ O((K2√mm + √m3β2 + C2)<5η) ± O(<8η), yi
=E∑ ' ((1 一 η) fW 0,V 0 (Xi) + η E W P,V P aτ Do ,ρV∑ φ* (Xi)
+ √ηEWp,vPhaTD，,p(V⑼ + (1 - η/2)V0 + VP)WsDo,ρW∑∑Xi
+ aτDo,ρVSWsDo,p(W⑼ + (1 一 η/2)W0 + WP)Xii
± O(η<10) ± O(η(<6 + <4 + (√m3K2 + β2)(C1 + √m3β 1))) ± O((K2√m2m3 + √m3β2 + C2)<5η)±
O(<8η), yi.	(139)
NoW We Write the gradient-lipshitz inequality for ` at point
P∑1) := (1 - η) fWo,v0 (Xi) + ηEWP,VPaτDo,ρV∑Sφs (Xi) ± η‹ρ 1,
and regarding the following vector, where 夕 1 is the sum of all the noise terms above and goes to
zero by over parameterization:
P∑) := √ηEWP,VP haτD，,p(V⑼ + (1- η/2)V0 + VP)WsDoW^∑Xi
+ aτDo,pVSWsDo,P(W⑼ + (1 - η/2)W0 + WP)Xii.
Hence, using the 1 smoothness of `(., yi):
LHS ≤ E∑'(P∑0) + E∑'(P)√ηp∑) + 2ηE∑(P∑2))2.	(140)
But note that
E∑ '(P) √ηP ∑2) = '(P) √ηE ς P ∑2) = 0.	(141)
On the other hand, using the notation of Lemma 15 and the result of Lemma 16:
E∑ (EWp,vpaτDo,p(V⑼ + (1 - η/2)V0 + VP)WsDoW∑xJ 2	(142)
=E∑ (EWp,vPaτDo,p(V⑼ + (1 - η/2)V0 + VP)(Σφ*(Xi) + φ*∑(Xi)))2	(143)
≤ 4E∑ (EWp,vpaτDo,p(V⑼ + (1 - η/2)V0 + VP)Σφ*(Xi))2	(144)
+ 4E∑(ewp,vpaτDo,p(V⑼ + (1 - η/2)V0 + VP)φ*∑(Xi))2	(145)
. <212 +<121.	(146)
Moreover, using again the result on φ(2)0(Xi) from Lemma 18 and the fact that φ(0)(Xi) is orthogonal
to the rows of VΣS :
EwP,VPaτDo,pV∑ WsDo,P(W⑼ + (1 - η/2)W0 + WP)Xi
=aτDo,pV∑ (φ⑼(Xi) + (1 - η/2)φ⑵(Xi))
+ 2 aτ Do,p VS φ ⑵0 (Xi)
=(1 - 2)aτDo,PV∑Φ(2)(Xi)
+ 2 aτ Do,p VS W sDo,pΦ ⑵0 (Xi)
.(1 - 2)aτDo,PVSΦ(2)(Xi) ±<5.
50
Published as a conference paper at ICLR 2022
Combining the last Equation with Lemma 14:
E∑ (E W ." aτ Do,ρV∑ W sD,,ρ (W ⑼ + (1 — η/2) W' + W P) x，2
.(1- 2)2aτDo,^V∑WsDo,ρφ(2)(Xi) + 曜
. <02 + <52.	(147)
Combining Equations (146) and (147):
E∑(PΣ2))2 . <2ι + <22 + <2 + <5 =Q2.	(148)
Combining Equations (141) and (148), plugging into (140), and reopening the definition of p(Σ1):
LHS . E∑'((1 - η)fWo,vo(Xi) + ηEw.,VPaτDo,ρV∑iΦ^(Xi) ± η电I,yj + ημ2夕2.	(149)
NoW note that We can easily bound the magnitude of the term ηEWP,vPaτD,,ρV∑Φ* (Xi) as:
IE w P,V P aT D0,ρV∑^φ^ ( Xi ) I ≤ E W P,V P laτ D0,ρv∑φ ( Xi ) |
≤ M*kΦ*(Xi)k ≤ kV*kΦ*(Xi)k ≤ √2ζ2(1 + <)
kVkk2H
k
While using Lemma 34:
lfW0,V0 (Xi ) 1 ≤ (K2 √m33 +
K1 + c 1 + √m 3 β 1
+ C2(C1 + √m33 β 1),
Which is O(C1C2) for enough overparameterization and smoothing parameters β1, β2 as defined
in A.20.1. Furthermore, from Equations (131) and (126), We easily see that
Ewp,vPaτD0,ρV∑^Φ¥(Xi) ≤ p2ζ2(1 + <)
kVkk2H
k
Now taking η small enough so that the bound η√2ζ2 (1 + <)，P 卜 M k H∞ and η电ι both also
be bounded of order O(C1C2), We observe that the term inside the argument of `(., yi) Equa-
tion in (149) is O(C1C2). Hence, We can use the Lipschitz parameter of ` in the interval
[-O(C1C2), O(C1C2)], given by Lemma 9 to take out the noise term:
LHS . E∑ ' ((1 - η) fW o,v 0 (Xi)+ η E w P,VP aτ Do,ρV∑ φ* (Xi) ,y) ± η^ 1 + ηO(C1C2 + B) P 2.
(150)
NoW by applying Lemma 19 and Writing the Lipchitz property of ` at point (1 - η)fW0 o,Vo (Xi)
O(C1C2):
LHS . e∑'((1- η)fWo,v0(Xi) + ηf*(Xi) ±η<9,yj ±ηρ 1 + ηθ(C1C2 + B)P2
=E∑'((1 - η)fWo,v0 (Xi) + ηf * (Xi),y) ± η<9 ± ηp 1 + ηθ(C1C2 + B)P2
= `((1 - η)fW0 0,V0 (Xi) +ηf*(Xi),yi ±ηP,
Where the last line is just definition. NoW Convexity of` finishes the proof.
Next, using Lemma 11 We prove Theorem 6.
Restating Theorem 6 In the same setting as Theorem 6 and having enough overparameterization
such that ρ ≤ V (P defined in Lemma 11) and polynomially small enough step size η, we have
E∑L(W0 - ηW0 + √ηiW-∑, V0 - ηV0 + √V∑) ≤ L(W; V0) - ην/4.
51
Published as a conference paper at ICLR 2022
Proof of Theorem 6
First, note that taking expectation w.r.t Σ:
m 3	m 3
E∑H(1 — η/2)WW + √ηW∑∑∣∣2 = E∑(1 - η/2)2"，||2 + 2(1 — η/2)√n(W0, E ΣfcWn + ηk E Σ&W||2
k = 1	k =1
=(1 - η/2)2kW0k2 + ηX kW：k2,
k
which by orthogonality of W： 's:
LHS = (1 - η/2)2∣∣W 0∣∣2 + η∣∣W *∣∣2 = (1 - η) ∣∣W 0∣∣2 + η∣∣W *∣∣2 + η2 ∣∣W 0∣∣2.
Similarly for V0:
E∑ k (1 -η/2) V0+√V k = (1 -η/2)2∣∣V'k2 + η E∑ ∣∣V * Σ ∣∣2 = (1 -η) ∣∣V 0∣∣2 + η∣∣V *∣∣2+ η 2∣∣V'k2.
Now using Lemma 11:
E∑ L (W0 - η/2W0 + √ηW*, V，-η/2 V0 + √≠∑*)
≤ (1 - η)EZ以fWo,vo(x),y) + ηeZ以f *(x),y)
+ (1 - η)(ψιkW0k2 + ψ2kV0k2) + η(ψιkW*k2 + ψ2∣∣V*k2) + η(夕 + η(∣∣W0∣∣2 + IlV0∣∣2))
≤ L(ww, v0) - η(L(wW, v0) - △ - ψ 1Q- ψ2ζ2) + ηG + η(IIWil2 + IIv0II2)}
which by the choice of 也's is equal to
LHS ≤ L (W 0,V0) - η (L (W ∖ VW)- △ - ν/2)+ η (p + η (∣∣W 0∣∣2 + IVk 2))
LHS ≤ L(Ww,Vw) - ην/2 + η(伊 + η(k Ww∣∣2 + k Vw∣∣2)).
Moreover, using the condition
p ≤ Vl8,
and picking η as small as
η(kWWk2 + kvwk2) ≤ η(C2 + C2) ≤ ν/8,
We finally get
LHS ≤ L(Ww,Vw) - ην/4.
52
Published as a conference paper at ICLR 2022
A.14 Existence of a good direction Helper Lemmas
In this section, we state and prove the core lemmas that are used in the proof of Lemma 11. Notably,
through all of this section, we assume the norm bounds kW0k ≤ C1, kV 0k ≤ C2 and that as our
usual assumption, the rows of V0 are orthogonal to φ⑼(Xi)’s for all i ∈ [n]. A notation that We use
throughout the proofs is W which refers to the projectiono of V * Σ onto the orthogonal subspace to
(φ(0)(xi))in=1.
Lemma 12 Let P(.) be the projection operator onto the subspace spanned by (φ(0)(xi))in=1. Also,
we denote the projection of rows of V*Σ onto the orthogonal subspace to (φ(0) (xi))in=1 by VΣ*j.
Then
E∑ HVS j-V* Σ) Il2 ≤ % 2 ξ2 n/m 2,
with high probability
Mj- V*ς)k . %√7=n,
m2
Proof of Lemma 12
By Equation (128), we have ∣∣V*∣∣g ≤ %3ξ∕√m2. Now suppose that U1, ...,un are an
orthonormal basis for the subspace span(φ(0)(xi))in=1. Then
EΣ	kVΣ*j	-	Vj*Σ)k2 =	EΣ	kP(Vj*Σ)k2=XXVj*2kui2k≤	kVj*	k2∞n≤%23ξ	2n/m2.
ik
Also, by Hoeffding, with high probability:
m3
kP(Vj*Σ)k2 =	(	Vj*kuikΣk)2.nkVj* k2∞,
i k=1
which implies the second part.
Lemma 13 The first cross term goes away because of the definition of VΣ*. (inside the expectations
is zero almost surely)
0.
Lemma 14 Second cross term:
EΣ EV ρ,W P	aT DV(0) + V ρ+V 0,Xi VΣ*φ(2)( Xi )])
.ξ 2((1+<)2nζ2+%23n)(C12+m3β12) = <02.
(151)
(152)
Proof of Lemma 14
53
Published as a conference paper at ICLR 2022
This time we use Equation (128) in Lemma 10 and Lemma 12:
EΣ EV ρ,w P	aT DV ⑼ + V ρ+V，x V∑ φ (2)( Xi )])
≤ e∑(ev ρ,w P1 /√m 2 X ∣v∑ j Φ ⑵(Xi) |)2
j
≤ m-EΣ,Vp,wP (X 唱 j Φ ⑵(Xi) 1)2
2j
≤ E V P ,W P E Σ E ∣v∑^ j φ (2)( xi) 12
j
.EV P W P E ∑ XI (VS j-V- ∑) φ (2)( X) 12 + XIV* ∑ φ (2)( X) 12
jj
.EVP,WPEΣ X kVΣ*j - Vj*Σ)k2kφ(2)(Xi)k2 + X kVj*k2∞kφ(2)(Xi)k22
jj
. ((1 + <)2nζ2ξ2 + %23ξ2n)EVP,WPkφ(2)(Xi)k22.
Now according to Lemma 33, we have
EVP,WPkφ(2)(Xi)k2 . C12 + m3β12,
which completes the proof.
Lemma 15 We get an additional term φ*0(Xi) as a result of smoothing which we define as
φ*0( Xi ) = √m W sDW (0)+ Wz+W P,Xi WΣ Xi - φ* Σ( Xi ) .
(153)
Then
P(φ*(Xi) =0) ≤ m 1 exp{-c2/(8β2)}.
Moreover, we have the following inequality almost surely (over the randomness of W ρ):
M, (Xi) k . W 惘降.
Proof of Lemma 15
According to Lemma 1, for j ∈/ P, for every i ∈ [n] we have
I (W* + W) XiI ≥ C 2 / 2 √m i.
Now note that as long as the sign patterns forj ∈/ P does not change, φ*0(Xi) will be zero. Therefore
by union bound
m1
P(φ*0(Xi) 6= 0) ≤ X P(sign change in j) ≤ m1P(I(Wj(0) +Wj0)XiI ≤ IWjρXiI)
j=1
≤ mιP(IWpXiI ≥ c2/(2√mι)).
BUt (Wp) Xi is Gaussian with variance β2/m ι. Hence
LHS . m 1 exp {-c2/(β2)},
which proves the first part. For the second part, according to Equation (105) in Lemma 5, for every
k ∈ [m3]:
lφ*k ( Xi ) 1 ≤ 1 √m WsDW (0)+Wz + W PX W *XiI + lφ* k ( Xi ) 1
≤ 2/√m 1X kWj*k ≤ 2kW** . XkVkk2∞,
jk
(154)
(155)
which implies the second part.
54
Published as a conference paper at ICLR 2022
Lemma 16 Fourth Extra term:
EW ρ yP [ √- aDV(0)+Vρ+V0,xi(V ⑼ + V P + (1 - η/2)V / )φ"( X)]
.(K2√m2m3 + C2 + √m3β2) mι exp {-c2/(8β2)} jX M ∣∣H∞ := < 11 ∙
Proof of Lemma 16
Note that with high probability over the randomness of V⑼,We have ∣∣ V(O) ∣∣f . √m2m3K2. Now
according to Lemma 15 and using the fact that kV0 kF ≤ C2 :
≤ EWPyP√m2kα∣∣V⑼ + Vρ + (1 - η)V'k2∣φ*0(Xi)k
=EWPVP∣V⑼ + VP + (1- η/2)V0∣F∣φ*0(Xi)k
≤ √EVPlV(0) + α-η∕2)Vi+lVp∣i) m 1 exp{-c2/(β2)}^X∖∖vk∣H∞
.√r(0)ιi+lVi-+m3βI m 1 eχp {-c 2 / (8 β 2)} yXXιvιi∞
.(κ2√m2m3 + C2 + √m3β2) m 1 exp {-c2/(β2)} jχ MkH∞.
Lemma 17 Fifth extra term:
E W p,V P [ √m a DV(0) + V P+V 0 ,xiVΣφf'(xi)]| . P2m 1 exP {-c2/(8β2)} yXX kVk kH∞ := < 10.
Proof of Lemma 17
Similar to the previous Lemma, the inner expectation can be bounded as:
≤ E√^-HMkFkφ*0(Xi)k ≤ EkV*∣Fkφ*'(Xi)k . P2m 1 exp j2/(β2)},∕XIVli∞∙
m2	k
Lemma 18 We have another extra term as a product ofthe movement -η/2 W0 in the first layer:
2
φ ( Xi ) = -[ W Dw (0) + W P+W 0 ( W + W P + (1-η∕ 2) W ) xi-φ ' ( Xi ) -Q-η/2) φ ( Xi )].
η m1
Then
E W P,V P √— a DV(0)+ V P + V 0 xi v∑^ φ (2) 0( Xi )
m2
.yζ2mw+√mCPmɪp-∕8β^Ci :=< 5.
E W p,V p 1 √m a Dv (0)+ V P + V0 ,xi ( V ⑼ + v p +(1 - n/2) v 0) φ ⑵ 0( Xi )|
.(K2 √m2m3 + √m3β2 + C2)<5.
Proof of Lemma 18
First we prove the following approximation argument (for all k ∈ [m3]):
EWP lφ(2) (Xi)k 12 ≤ ^- + 2- 1-+ m 1 exp {-c2/(8β2)}C2 .
m1	m1K1
(156)
(157)
(158)
55
Published as a conference paper at ICLR 2022
We have
LHS = EWρ
. EWρ
- Ws DW (0) + W P+W，,Xi ( W (0) + W ρ ) Xi- √m WsDW ⑼,Xi W (0) Xi 1
WsDW (0)+ W P,Xi ( W (0) + W P ) Xi- √m WsDW ⑼,Xi W (0) Xi|
+EWρ
1 2
Wk DW (0)+ W P,Xi ( W	+ W ρ ) Xi- √m Wk DW (0)+ W ρ+W ∖xi ( W	+ W ρ ) xi I
By the independence of Wp,s, the first term can be upper bounded as
1Xm12
W(°)+ Wρ)Xi 1{(W(°)+ Wρ)Xi ≥ 0} - W⑼Xil{Wj(°)Xi ≥ 0})
j=1
1 m1	1	β2
≤—XEW P ∖Wpxi∖ = m X—
j=1
β 1
m1
For the second term, note that for every j ∈/ P, the jth entries of DW(0) +WP,xi and
DW(0) +WP+W0,xi are different only if Wjρ can make a sign change in the jth row, i.e. ∖(Wj(0) +
∙-v
W0) Xi∖ ≤ ∖WpXi∖ should happen. We denote this event for every j ∈ P by Ej. Furthermore, if this
happens for some j, then the value of (W (0)+ Wρ)jXi is upper bounded by ∖Wj0Xi∖. Now similar
to our discussion in Lemma 15 and using the result of Lemma 1:
m1
P( ∪j/ p Ej) = P (sign change in some j ∈ P) ≤ E P(Sign change in j)
j∈P
≤ m 1P(∖(W⑼+ W)Xi∖ ≤ ∖WpXi∖) ≤ m 1P(∖WpXi∖ ≥ c2/(2√mJ)∙
But note that (WP)Xi is Gaussian with variance β2/m 1. Hence
LHS . m 1 exp {-c2/(8β2)h
So finally we can write
β2	1	1
.----+ E W P-(X lWj xi∖) + EW P-( ^-{∪j∕P Ej} X ∖Wj xi∖)
—1	—1	—1
1	1 j∈P	1	j∈P
.―+m∣kW，k 2+p( ∪j∈p Ej) kW zk 2
≤ βL + Cc2C1 + m 1 exp{-c2/(8β2)}C2.
—1	—1 κ1
which completes the proof for Equation (158). This immediately implies
E W P kΦ ⑵,(Xi) k ≤ qEWPkφ20X^W ≤ ^∕m3(mm2→√C=CC∣7+m1θxP{-c2∕(8β2)C) ∙
Now we first prove Equation (156):
IE W p,V P [ √— a Dv (0) + V p+V，，Xi V∑ φ ⑵ 0 ( Xi )]I ≤ E W P √— IalllIDV (0) + V p+V，，Xi V∑J1kF kφ ⑵ 0 ( Xi )] ∖k
—2	—2
≤ M kF E W P kφ ⑵) (Xi)] ∖k ≤ kV IF E W P kφ ⑵ 0 (Xi)] ∖k
.ζ∣ ζ2m3(^I +----√- 1---+ m 1 exp {-c2/(8β2)}C2)∙
—1	—1 κ1	2	1 1
56
Published as a conference paper at ICLR 2022
To prove Equation (157):
EWρ
yρ I √m aTDV(0)+Vρ+V0,xi(V ⑼ + V P + (1 — η/2) V 0) φ ⑵ 0( Xi )∣
.E W PY P √m IlaIIII。V (0) + V ρ + (1 -η/2)V <Xi ( V (0) + V ρ + (1 - n/2)V 0) IIF ∣∣φ ⑵ 0 ( χi ) Il
.E W PV P √m2 IaIV ⑼ + V ρ + (1-η/2) VIIF Iφ ⑵)(Xi) I
.√m-IaI 5而0F+币-QVI3EWP Iφ⑵0(Xi)I
.((K2 m2 m3 + m3 β2 + C2 %.(K2 √m2 m3 + `√m3 β2 + C2 网.
Lemma 19 Closeness condition:
E∑ 忸wPVP [ L aTDV (0)+V P+V 'χ量 φ* ∑( Xi )] - f * (Xi )∣. <9,
where
痛：=%3ξ√n(1 + 沆)jχ MIH∞ + <3
+ m2 (exp { — (m2KC4)113/(2β2)} + m 1 exp{-C2/(8m3β1)}
(159)
∖p2 (1 + <) yXX IVk I H∞.
(160)
Proof of Lemma 19
Note that by Corollary 5.1 and according to the proof of Equation 129 in Lemma 10, if for
every j ∈ P we don,t have a sign change in DV(0)+VP+V0河足 V*Φ* (Xi), then get
I /— aT dv (0)+ V P+V 0 Xi V *φ* ( Xi ) - f * ( Xi ) I ≤ < 3.
m2	,xi
Also, note that we need the event Ec (defined in Lemma 33) to happen in order to be able to use
Corrolary 5.1. Hence, given a W ρ for which Ec happens, we upper bound the probability of sign
change with respect to the randomness of V ρ . We define the following event with respect to the
randomness of Vρ when conditioned on a W ρ for which Ec happens (Pi’s are defined in Lemma 7):
SC := {∃j ∈ Pi s.t.IVjPXiI & (m)113C223IXiI}∙
Now from the result in Corollary 5.1 we have ≤ 1{sign change in j ∈ Pi} ≤ 1{SC}. Therefore,
Εl{sign change} ≤ 1{SC} ≤ X P(∣VpXi∣ & (K)123C223∣XiI)
j m2
j∈2 Pi	2
≤ m2P(∣VjρXi∣ & (m)123C223IXiI).
But note that (Vjρ)X0i is Gaussian with variance β22 IX0iI2/m2. Hence
LHS . m2 exp { — (m2K2C4)123/(2β2)}.	(161)
Now let D be a sign matrix random variable such that if Ec and SCc both happens, then it is equal
to the valid sign matrix DV(0)+VP+V0,xi, and otherwise it is equal to an arbitrary valid sign matrix in
the case when both Ec and SCc happen. Now using Equation (116) we have with high probability
57
Published as a conference paper at ICLR 2022
over the initialization:
EΣ∣ E W ρ,v P [ /— aT Dv(°)+ V ρ+V 'χ vΣ* φ* ∑( Xi )] — f * ( Xi ) I
1	Vm2	1
≤ E∑ ∣ Ewρ,vρ [√m aTDV(°)+ V P + V，x(VS — V * Σ) φ* ∑( Xi)]
+ E∑ ∣ E W P,v P ar DV (°)+v ρ+v,gV * Σ φ* ∑( Xi)] — f * (Xi)
≤ EWρ,vPE∑
aT DV ⑼+ V P + V！ x ( vΣ* — V * ς)0* Σ( Xi ) I
+ E∑ ∣ E W ρ,v ρ
aT DV (°)+v P+v !,Xi V *φ* (Xi)] — f * (Xi)
≤ E w ρ,vP E∑ m X ι∣v∑ j— vj ς iiii。* ∑( χi)Ii
2 j
ar ^V ^φ^ (Xi) — f * (Xi)
+ 1{SC U E}
+ E∑∣E W ρ,v ρ
r DV(°)+ V -gV *φ* ( xi ) - D )]∣
vPE∑m X Mj — V*∑Iφ*∑(Xi)I
2	j
ar DV *φ* (Xi) — f * (Xi ))∣
+ E∑∣E W ρ,v P
+ E∑Ewρ,vρ 11{SC U E}
aT Dv (°)+ V ,,xi V *φ* ( Xi ) - √m aT DV * φ* ( Xi ))] I
≤ Ewρ,vP √m X qE∑kv∑^j — Vj ςk2ι∣φ*(χi)Ii
V 2 j
+ 员3 + 2P( SC U E) maX
Or D V ^φ^ (Xi )1.
≤ EWρ,
Now note that for any sign matrix D0, We have the following bound:
ar D0V *φ* (Xi) ∣ ≤ √m- HkV ** ∣φ* (Xi) I < P (1 + 沆)jX 恒哈8.
Also, applying a union bound and using Lemmas 33
P( SC U E) ≤ P( SC)+ P( E)
< exp{ — (m2κ2C4)1 /3/(2β2)} + m 1 exp{-C2/(8m3β2)}.
Hence, also applying Lemma 40, we further write
LHS < %3ξ√n(1 + 沆)jχ M 哈∞ + 沆3
+ m2 (exp { — (m2κ2C4)1 /3/(2β2)} + m 1 exp{—C2/(8m3β2)}) p2(1 + 沆)jχ IIVk∣H∞ .
Lemma 20 Suppose we have m3 R2 ≥ C2. Then, for the following basic term we have:
E W P ,vP [ √— aTDV(°)+ V P+V' X(V (0) + V P + (1 — η/2) V0)(φ (0)( Xi) + (1 — η/2) φ(2)( Xi))
< (I — η )E W P,V P [ √— aT Dv(°)+ V P + V! X ( V ⑼ + V ρ + V 0)( φ ⑼(Xi ) + φ(2)( Xi ))
58
Published as a conference paper at ICLR 2022
±η (五6 + <4 + (√m3K2 + β2)(C1 + √m3β 1)) ,
where
<4 ：= C2(C1 + √m33β 1)m2 exp{-C4/3(√m22κ2)223/8β2} +
C21/3
(√m 2 κ 2)1 / 3
C2( C1 + m3β3 β 1),
<6 ：= m 1 exp{-C2/(8m3β2)}√m3K 1(^√m2 + β2) + <6,
and <6 is defined in Lemma 22.
Proof of Lemma 20
First, note that by orthogonality of φ(0) (xi) to the rows of V0:
LHS - η/2EWiP [√maτDV⑺十丫.十丫，咫 (V⑼ + VP)φ⑼ (Xi)
=LHS - η/2EWPVP [√maτDV⑺十丫.十广m (V⑼ + VP + (1 - η/2) V,)φ⑼ (Xi)
=(1 - η/2)Ewp,vp [√m aDV(0)+Vp+V，,xi(V⑼+ VP + (1 - η/2)V/)(φ⑼(Xi) + φ⑵(Xi))]
=(1 - η/2)2EVp [√——aτDV(0)+VP+V，,xi (V(0) + VP + V0)(φ(0) (Xi) + φ(2) (Xi))]
+ (1 - η/2)(η/2)EVp[√- aτDV(0)+VP+V，,xi (V (0) + VP)(φ(0) (Xi) + φ(2) (Xi))]
But note that for the second term:
EWρ,VP [ √- aDV⑼ + Vp + V工Xi(V(0)+VP)(φ(0)(Xi)+φ(2)(Xi))]
.Ewp,vP[√-2OTDV(0)+VP,Xi(V⑼ + VP)(φ⑼(Xi) + φ⑵(Xi))]
±√m	X	∣Vj(φ⑼(Xi)+φ⑵(Xi))।
2 j: sign change
=E V p [ -L- aT DV(0)+ V p, Xi (V ⑼ + VP)(φ ⑼ (Xi) + φ ⑵ (Xi))]
-2
± -1^	X	∖v0φ (2)( Xi) |.
V-2 j ： SiWange
(162)
(163)
Now conditioned on X0i, by the result of Lemma 31 we know there exists a set of indices O ⊆ [-2],
C2/3
s.t. ∣O∣ ≤ (√m2K2)2/3 - 2 and for j ∈ O we have
呼)Xil ≥ CC23(√2K2)123M
-2
and
\V；Xil ≤ CC3(2√⅞KNI3IIXM
2 -2
Now for j ∈ [-2], define the event
Rj = {|WjPX0i| ≥
C22 3( √- 2κ 2)113
2 ʌ/m 2
IX0i I},
and R = ∪jRj. First, note that using Gaussian tail bound, R is a rare event:
P(R) ≤ XP(Rj) ≤ m2 exp{-C423(√-2κ2)223/8β.
j
59
Published as a conference paper at ICLR 2022
Now forj ∈/ O and under Rc, clearly we have that the signs of (Vj(0) +Vjρ)x0i and (Vj(0)+Vjρ+Vj0)x0i
are the same. Therefore, applying Lemma 33, we can argue under Rc :
E W P,V P /—	X	lVjφ (2)( xi ) 1 ≤ E W P /— X lVjφ (2)( xi ) |
m2	m2
2 j: sign change	2 j∈O
≤ m2~V IlV 0kEW P kφ (2)( xi ) Il ≤ ( √m2κ 2)1 / 3 C2( CI + √m33 β1.
Hence, overall, using Cauchy-Shwartz
1	c1 / 3
E W P ,V P √m	V0φ	lVj φ (2)( xi ) l ≤ IIV ZkE W P kφ (2)( xi ) Il P( R)+( √m K )1 / 3 C2( C1+ √m3 3 β 1)
2 j: sign change	2 2
≤ C2(C1 + √m33β 1)m2 exp{-C443(√m22κ2)23/8β2} +
C12 3
(5:K2)123c2(c 1 + √m3β 1) ：= <4.
(164)
On the other hand, using Lemma 30, we have with high probability over the randomness of initial-
ization
7	— a DV (0) ,gV (0)φ ⑵ ( Xi ) ≤ √m 3 K 2 ∣∣φ (2)( Xi ) ∣∣∙
√m 2
Hence:
E V P [ √m a DV(0)+ V P,Xi ( V (0) + VP ) φ (2)( Xi )]
≤	E W P,V P [ √m a DV ⑼ XiV (0)φ (2)( Xi ) + √m X lVpφ (2)( Xi ) | ]
≤	E W P √m a DV (0) ,XiV (0)φ (2)( xi ) + β 2E W P kφ (2)( xi ) Il
.	( √m3 K 2 + β 2)E W P∣∣φ (2)( Xi ) I
.(√m3K2 + β2)(C1 + √m3β 1).	(165)
Combining Equations (164) and (165) into Equation (163):
IE V P [ √m a DV(0) + V P+V 'Xi( V (0) + V p ) φ (2)( xi )] I . < 4 + ( √m3 K 2 + β 2)( C1 + √m 3 β 1) .
2	(166)
Moreover, for the first term in (162), using Equation (165) and Lemmas 33 and Lemma 30 we have
IE V P [ √— a DV(0) + V P+V ∖χi ( V (0) + VP + VZ) φ (2)( xi )] |
m2	,Xi
.IEWp,vp √— aTDv(0)V(0)Φ(2)(Xi)| + EWp,vP -L- X ∖Vpφ⑵ (Xi)| + -L- X ∣VjΦ(2)(Xi)|
m2	m2 j	m2 j
.K 2 √m 3 (C1 + β 1 √m 3) + (C2 + β 2)E W P iiφ (2)( Xi) Ii
.K2√m3(C1 + β 1 √m3) + (C2 + β2)(C1 + √m3β 1).
(167)
60
Published as a conference paper at ICLR 2022
Substituting Equations (166) and (167) into Equation (162), we finally get
LHS - η/2E W ρ,v P [ √— a DV ⑼+ V ρ + V Ix ( V (0) + V p ) φ (0)( Xi )
.Qirn 2)2EVP [ √m2 a DV ⑼+Vρ+V E (V (0)+ Vρ+VV)φ (2)(X)]
± 2 (员4 + (√m3K2 + β2)(C1 + √m3β 1)
.(1 - η )EV P [ √m2 a DV (0)+Vρ+V $ V (0)+ Vρ+Vv)φ ⑵ (X)]
± r42∣E V P [ √ma DV (0)+ V P+V,,Xi( V ⑼ + Vρ + V') φ ⑵(Xx )]|
± 2 (员4 + (√m3K2 + β2)(C1 + √m3β 1)^
.(1 - η )E V P [ √m a DV (0)+ V P + V，x ( V (0) + VP + VZ) φ ⑵ ( xi )]
± η 2( K 2 √m33( C1 + β 1 √m33) + ( C2 + β 2)( C1 + √m33 β 1))
± η (五4 + (√m33K2 + β2)(C1 + √m33β 1)) .
(168)
Now by picking η small enough so that the second term is dominated by the third term we get:
LHS - η/2E W P,V P [ √m a Dv (0)+ V P + V ∣,Xi ( V (0) + VP ) φ (0) ( xi )
.(1 - η )E V P [ √m a DV (o)+VP+VI,xi (V (0) + VP + V Z)φ(2) (Xi)]
± η (<4 + (√m3K2 + β2)(C1 + √m3β1))∙
(169)
(170)
(171)
NoW We aim to bound the term EWp,vp [√120τDv(0)+VP+V∣χi (V(0) + VP)φ(0) (Xi). First assume
that we are in the event Ec defined in Lemma 33, i.e. we have kφ(2) (Xi)k . C1. Conditioned on
such WP, We noW Work With the randomness of the initialization and VP. Note that the random
matrix V(0) + VP jointly over the randomness of VP and the initialization is also Gaussian, and its
variance is
K 2 ≤ K2 + β2 ≤ 2 K2,
2	2	m2	2
(172)
where the inequality follows from the fact that K2 ≥ -‰ and β2 ≤ 1. Therefore, applying
m2
Lemma 22 for the random matrix V(0) in the Lemma as V(0) + VP here, the bound does not change
up to constants because of the inequality (172). Hence, with high probability, lets say with prob.
1 - δ1 this time over both the randomness of initialization and VP :
L = ∣ √m aDv(0)+vp+v<xi(V (0) + V P)φ(0)(Xi)∣ ≤<6
(173)
This means that with probability at least 1 一 √J 1 over the random initialization, then we have (173)
with prob. at least 1 一 √δ 1 over the randomenss of Vρ. We name the latter high probability statement
as (?). Moreover, note that by Lemma 32 and assuming m3 log(m2) < m2, we have the following
61
Published as a conference paper at ICLR 2022
almost surely bound (also note that Vjρφ⑼(Xi) is Gaussian with Std √m ∣∣φ(0)(Xi) ∣∣):
EVJ √m aTDv(0)+ VP+V"(V (0) +Vρ)φ(0)(Xi)	(174)
=E V ρ I √m aT Dv (0)+v P+V 'Mi (V (0) + Vρ)φ(0)(Xi)	(175)
1	m2	1	m2
≤ √=- Ε |Vj(0)φ (0)( Xi) I + E V P L ^φ0)φ (0)( Xi) I	(176)
m2 j=1	m2 j=1
1	m2	1	m2 β
.M(0)(Xi)H sup k ZIV(O)x'I +ΑΕ√^M⑼(Xi)H	(177)
kx' k=1 m2 j=1 j	m2 j=1	m2
.HΦ(0)(Xi)H(√m2 + β2)∙	(178)
Furthermore, note because each variable ∣VfΦ(0)(Xi)I is √2-∣∣φ(0)(Xi)∣∣ subGaussian. Therefore,
L is subGaussian with parameter Hφ(0)(Xi)Hβ2 with respect to the randomness of Vρ. Now the
point is that the high probability argument in (?) is much stronger than what one can get from
the subGaussian ineqaulity with parameter ∣φ(0) (Xi)∣β2 (with the corresponding expectation term
∣∣φ(0)(Xi)H(√m2 + β2)). However, the disadvantage of (?) is that it only works for a fixed δ 1.
In other words, at least it is not obvious from this argument that why for a fixed W(0) in a high
probaiblity region of the random initialization, whether we can send δ1 to zero by growing the
constant behind <6 with logarithmic rate log(1 /δ). This makes our job hard for bounding the
expectation with respect to Vρ if we only wish to rely on (?). Therefore, we combine it with the
inequality that we get from the subGaussian parameter that we introudced above. More rigorously,
we define the thresholding parameter
f := HΦ(0)(Xi)H(√m2 + β2) + HΦ(0)(Xi)Hβ2 iog(HΦ(0)(Xi)H(√m2 + β2)∕<6)
=θ(hφ (0)( Xi) H (√m 2 + β 2 iog( HΦ (0)( Xi) H (√m 2 + β 2)∕< 6))),
for which we have
E LI f ≤ L P(f ≤ L) . <6∙
we divide the range of values for L into three parts:
E[L] = E LI L ≤ < P(L ≤ <6)
+ E LI <6 ≤ L ≤ f P <6 ≤ L ≤ f
+ E LI f ≤ L P(f ≤ L)
≤ E LI L ≤ <6 + P(<6 ≤ L ≤ f) +<6
.<6 + √δif ∙
Now by choosing δ1 . 1∕f, we conclude with high probability over initialization and conditioned
on Wρ's such that Ec happens We have
E V ρ∣√m- aT DV (0)+ V P+V' ,xi (V(0) + V P) φ (0)( Xi )∣ = E[ L ] . < 6 ∙
Finally, we integrate also with respect to Wρ . To control the random variable when E happens, we
use the bound in (178) and the fact that E is a rare event due to Lemma 33:
EWP ,v p ∣ √m aτDV(0)+ V P + V <Xi ( V (0)+ V ρ ) φ (0)( Xi )| . P( E ) ∣φ (0)( Xi ) H ( √m2 2 + β 2)+ P( Ec ) < 6
≤ mι exp{-C1 /(8m3βl)}√m3Ki(^√m2 + β2) + <6 ：= <6∙
Substituting this into (171) the proof is finally complete.
62
Published as a conference paper at ICLR 2022
Lemma 21 Third cross term: with high probability over initialization, we have
EΣE
WJVP [ √m a DV(0)+ VP+V∖xi (V ⑼ + V P + (1 — η) v 0 )∑ φ^ (Xi )])2
. ξ2 (m1 exp{-C12 /(8m3β12 )}(κ22 m2 m3 + β12 m3) + <72 C22 ) := <122 .
Proof of Lemma 21
Note that the way We defined the matrix W * and as a result φ* (Xi) only depends on the ran-
domness of W(0), not on W0 or the randomness of V(0). Now using Equation (130) and Jensen
inequality we can write (for vector v, the notation v2 is another vector with each entry as the
second power of the corresponding entry in v):
2
aDV(0)+VP+V0,xi(V(0)+VP+(1-η)V0)Σφ*(Xi)]
EΣ EWP,
≤ EΣEWP,VP
aT DV (0)+V P+V 0,xi(V (0) + V P + (1 - η)V 0)Σφ*(Xi))2
E w p,v ρ E∑ (h√m
≤ EWP,VP
EWP,VP
a
2
2 aDV(0) +VP+V0,xi(V(0)+VP+(1-η)V0), Σφ*(Xi)i
TDV(0)+VP+V0,xi(V(0)+VP+(1-η)V0))2, φ*(Xi)2E
aTDV(0)+VP+V0,xi (V (0) + VP + (1 -
≤ ξ2 EWP,VP
≤ 2ξ2EWP,VP
1
√m 2,
llɪ
Ii ʌ/m,
aT DV (0)+V P+V 0,xi (V (0) + VP +
φ (χi)||
2 aTDV(0) +VP+V0,xi(V (0) + VP)l2 + 2ξ2 EWP,VP
aTDV(0)+VP+V0,xi (1 - η)V0 I
2
∞
2
2
≤ξ 2E WP V Pl √m
≤ ξ2 EWP,VP
aT DV (0)+V P+V 0,xi(V (0) + V P)lll2 + ξ2(1 - η)2kV 0k2F
22
aT DV (0)+V P+V 0,xi(V (0) + V P)ll22 + ξ2(1 - η)2C22.
Now under the event Ec defined in Lemma 33 we get that kφ(2)(Xi)k . C1, so we can bound the
above as
≤ ξ2 EVP	sup
kV0k≤C2,V0⊥φ(0)(xi),kx0k≤C1
+ ξ2(1 - η)2C22.
Now defining
L2 :
(179)
(180)
to bound the first term, we want to apply Lemma 23 using the same trick that we did in the proof
of Lemma 20. Note that L2 is the same term as Γ2x0,V0 in Lemma 23 except that it is defined with
respect to V(0) + VP instead of V(0). On the other hand, note that V(0) + VP has Gaussian entries
with variance K2 + 焉 and we know K2 ≤ K2 + 焉 ≤ 2K2, which means the argument ofLemma 23
holds true here up to constants:
sup	L2 . <27 .
kV0k≤C2,V0⊥φ(0)(xi),kx0k≤C1
This holds with probability say 1 一 δ2 over the randomness of both V(0) and Vp. Therefore, with
probability 1 一 √δ2 over the initialization, then with probability at least 1 一 √δ2 over the randomness
63
Published as a conference paper at ICLR 2022
of V ρ we have the above. Moreover, with a simple Cauchy-Swuartz we get the following almost
surely bound:
L2 . kV(0)k2F+kVρk2F.
(181)
Now the variable kV ρ k2 is subexponential with parameter (β14m23 , β12m3). Furthermore, with high
probability we have kV (0) k2F . m2m3κ22 . Therefore, taking
f 2 := Θ (κ2m2m3 + β1 m3 log ((Km2m3 + β2m3)/<7))
then one can easily see by the subexponential tail:
E[L2 | L2 ≥ f2] = Θ f2 ,
P(L2 ≥ f2) ≤ <27/f2 .
Hence, we can apply the same trick as Lemma 20 as
E[L2] = EhL2| L2 ≤ <27iP(L2 ≤ <72)
+EhL2| <27 ≤L2 ≤ f2iP<72 ≤L2 ≤ f2
+EhL2| f2 ≤ L2iP(f2 ≤L2)
.EhL2| L2 ≤ <72i + P(<27 ≤L2 ≤ f2) + <27
.< 7 + √ 2f2.
Now taking δ2 . <7/f 2, We finally get that conditioned on Wρ 's where E happens, then
EVρL2 ≤ <72.
On the other hand, to handle the case when E happens, we can use the bound in (181) as it does not
depend on the occurrence of E as well:
EWρ,VρL2 ≤P(E)EVρ(kV(0)k2+ kVρk2)+P(Ec)<27
.m 1 exp{-C2/(8m3β2)}(Km2m3 + β2m3) + <7.
Plugging this back into (180) we finally get
LHS . ξ2(m 1 exp{-C2/(8m3β2)}(Km2m3 + β2m3) + <7) + ξ2C2.
64
Published as a conference paper at ICLR 2022
A.15 B ounding the worst-case Senario
Lemma 22 Suppose m3 ≥ log(m2) and √m33K1 & C1. We define the sign matrices。，⑼+Vz Xi
and DVx0(0) ,xi with respect to the multiplications
(V (0) + V 0)(φ(0)(xi) + x0),
and
V(0)(φ(0)(xi)+x0).
Then, with high probability:
kxok. C1JV SU^1^C 2 3 (0) √2 a DV(0)+ V 0Xi V ⑼ φ (0)( Xi )
.(	(CIC2)4/3	+ (CIC2)223m3/3(K1K2)113plog(m2) λ∕1 + log(m ) C；/3(K2√m2)223 λ
~ %√m2κ2)1 /3(√m3K 1)1 /3	m 1 /3	八	2 C223(κ 1 √m3)1 /3)
m 3 / 2 K 1 K 2 r---- Z-----------------------------------
+----产—√log(m2)(log(m3) + log(log(m2))) + K 1 K*\m3 log(m2) :=沆6.
m2
Proof of Lemma 22
Consider a cover for the euclidean ball of radius C1 in Rm3 with precision , i.e. BC1 ().
So for every χ0 ∈ Rm3, there exists an x ∈ B5 (C) such that kX — x0k ≤ g and ∣Bc 1 (C)∣ . (ɪ)m3.
Now fix X0 and X. We have
1	0	1 m2
ΓX0,V0 ：= -=raTDV(0) + v0,χiV⑼φ(0)(Xi) = -=r j{( 1{(V⑼+ V)(φ(0)(Xi)+ X0) ≥ 0}v(ff)φ(0)(Xi).
m2 m2j =1
Now by a union bound, because each variable Vj(0)φ(0) (Xi) is Gaussian with parameter
K2 kφ(0)(Xi)k and using Equation (115), with high probability we have for every j ∈ [m2]:
V⑼φ(0)(Xi) . K2 kφ(0)(Xi) k plog(m2) . K 1 K2Pm3 log(m2).	(182)
Therefore, by Hoeffding over the randomness of the Bernoulli variables aj , for a fixed X0 with high
probability:
m	m 2	___________
Γx； := √m- X ajl{V0∙°(φφ(0)(Xi) + X0) ≥ 0}V(^)φ(0)(Xi) . K 1 K2pm3 log(m2).
2j =1
On the other hand, We know that the VC-dimension of the class of binary functions with respect to
halfspaces in Rm3 is m3 + 1. Therefore, the set of different sign patterns in matrices DVx0(0) ,xi is
bounded by m2m3+1, i.e. for
D = {DV(0),XiI X0 ∈ Rm3},
we have
|D| .m2m3+1.
Therefore, by taking a union bound over all sign matrices in D, we get with high probability
SUPrχ0 . K 1 K2√m3 log(m2) √log(m2m3+1) = K 1 K2m3 log(m2).	(183)
X0
Now for a threshold r which satisfies
r ≥ 2√m3k2c,	(184)
we define
Jx,r = {j ∈ [ m 2] I IV⑼(φ(0) (Xi)+ X) I ≤ r}.
Now by Equation (115)and the assumption of the Lemma -m3 K 1 & C 1, We have
kφ(0)(Xi) + Xk ≤ kφ(0)(Xi)k + IXk . √m3K 1 + C 1.	(185)
65
Published as a conference paper at ICLR 2022
kφ(0)( Xi ) + Xk ≥ kφ(0)( Xi ) Il - kxk & √m3 K1 - C1 & √m 3K1 .	(186)
Hence, “("(φ(0)(Xi) + X) is Gaussian with standard deviation at least Ω(K2√m33K1). Therefore,
P(|V(O)(0(O)(Xi)+ X) | ≤ r) .——.
j	m3K1K2
This implies
r
E[∣Jx,rI] . —/=---m2.	(187)
m3K1 K2
On the other hand, note that |Jx,r | is the sum ofm2 Bernoulli random variables, so it is subGaussian
with parameter m2 . Therefore, with high probability
r
lJx,r 1 . -J=---m2 + √m2.
m3K1K2
Now taking maximum over all X ∈ BC1 () and exploiting the subGaussian tail of the random
variables, we get with high probability
x∈BC1 ()J.". √mK- m 2 + pm2ɪ°g(^≡ . √mKκ;m 2 + P m 2 m 3 log(1 /) ∙
(188)
Moreover, consider a threshold 1 < θ, such that e~θθ28 ≤ m2/m3, and define the following set of
indices
Jx(02, )θ := {j ∈ [m2]| |Vj(0)X0| ≥ θK2C1}.
Then, using Lemma 29 and noting the fact that the standard deviation of Gaussians in V(0) is K2 and
that kφ(2)(Xi)k ≤ C1, with high probability:
sup	∣J(2θI ≤ m3(log(m3) +log(log(m2)))∙	(189)
x0: kx0k=1
Now note that for each j ∈ [m2], kVj(0) k2 is subexponential with parameters (m3K42, K22), which
means that with high probability:
mjax ||Vj(0)k2 . m3K2 + √m3κ2 plog(m2) + K2 log(m2).
But with condition m3 ≥ log(m2), we can further upper bound it as
mjax kVj(0)k2 . m3K22.
Now for fixed X, X0, for j ∈ Jx,r we have
IVj(0)(φ(0)(Xi) + X0)I ≤ IVj(0)(φ(0)(Xi)+X)I+IVj(0)(X0 -X)I
≤ IVj(0)(φ(0)(Xi) + X)I + kVj(0)kkX0 - Xk
.r + √m3k2e.
On the other hand, for j ∈ JX2θ:
IVj(0)X0I ≤ θK2C1.	(190)
Therefore, for j ∈ Jx,r - Jx(02, )θ :
|Vj(0) φ(O) (Xi) I ≤ IV⑼(φ(O)(Xi) + X) I + |Vj(0)X0I . r + √m3K2e + θκ2C1.	(191)
In a similar fashion, ifj ∈/ Jx,r, then using assumption (184):
IVjt0^(φ(0)(Xi) + X)I ≥ IVj(°^(φ(0)(Xi) + X)I - IVjt0^(X - X)I & r - √m3K2E ≥ r/2.	(192)
66
Published as a conference paper at ICLR 2022
Hence, using the fact that φ⑼(Xi) is orthogonal to V-:
11{ (V⑼ + Vj)(φ ⑼(Xi) + x') ≥ 0 }- 1{V⑼(φ (0)( Xi) + x') ≥ 0} I
≤ !ji∖j^j'(φ(0)(Xi)+ x,)I > ∖vj^0∖φ(0)(Xi)+ x')∣?
≤ ljXo∣'∖ > IV⑼(φ(0)(Xi)+ x1)∖?
≤MHV'MM > IV(0)(Φ(0)(Xi) + x')∖?
≤MHV'必 > W(o)(Φ(0)(Xi) + x')∖?
≤ Rj >
呼)(。(0) (Xi)+ x') ∖	r
薮 + 4C1}.
(193)
Now by triangle inequality and Equations (193), (191), (190) and the fact that kV'∣∣f ≤ C2, We can
write:
∖γX- ΓXY/ ∖
≤ √m	X ∣1{ (V⑼+ Vj)(φ (0)( Xi)+ x') ≥ 0 }-l(Vj(0)( φ (0)( Xi)+ x') ≥ 0 }∣∖V ⑼ φ (0)( Xi) ∖
F j∈Jx,r-JX2)θ
+ √m	X	∣ 1{ (V⑼ + Vj)(φ (0)( Xi)+ x') ≥ 0 }-l(Vj(0)( φ(0) (Xi)+ x') ≥ 0 }卜啖 φ (0)( Xi) ∖
2 ji ( Jx,r∪j( 2)θ )
+ -L- X I 1{(V⑼ + Vj)(φ(0)(Xi) + x') ≥ 0}-l{Vf)(φ(0)(Xi) + x') ≥ 0} ∣∖V∙(°)φ(0)(Xi)∖
2 ”另
≤
√⅛ ∖%,L娼1jJ aJ 2JV⑼ φ (0)( Xi )∖
+ 崇 X RMk >
2 j∈ ( Jx ,r∪J( 2)θ )
∖ Vf)(φ (0)( Xi ) + x') ∖
2C1
+ 4⅛1 ∖ V(0)φ (0)( Xi) ∖
+-m2 ∖ % 1m 箫 ∖ v(0) φ (0)( xi)∖
≤√m2 JLJe∖j^x J2)θ ∖ V(0)φ(0)(X)∖
+ 3 X	MkV'k > ∖ """；Yi) + x) + ɪ>(∖ V(0)(φ(0)(Xi) + x') ∖ + θκ2C1)
√m0	z—z	J	2 C1	4 C1	J
2 jt ( JX ,r∪j( 2)θ )
+ ~√m2 ∖ % ∖j∈ 箫 ∖ Vj0)φ (0)( Xi ) ∖
.√m JL娼 ∖jj J 2)θ ∖ v(o)φ (0)( X)∖
1	r
+ √m	E	a{kvJ k > ci}(c 1 kVjk + θκ2CI)
+ -⅛ ∖ % j 箫 ∖ V(o) φ (0)( xi) ∖
.3 ∖ Jr- J2θ ∖ (r + -m3K2e + θκ2C1) + -CL ,j~J&C∑VkFζ
ʌ/m^ 2	ʌ/m 2 V `	C1 '
67
Published as a conference paper at ICLR 2022
+ √m2#Q: IMn & SθKCι + √m2JX2θjm2 V⑼小?X)∖
1 ,一	L 一…C K C2
√—— ∖Jx,r - Jχ0 θ ∖ (r + √m3 κK C + θκK C1) + √=-
mK	mKr
C 3 C2
ʌ/m 2 r K
+
θκ K + √m, ∖Jχ^θ ∖ m m2 1Vj(0) φ (0)( xi) ∖-
Now using Equations (207), (189), (182), and (184), and the bound on ∖Jx(0K,)θ∖ from Lemma 29, we
write
1 . _ 一一 1 ,小，	，_：—:~~■	C K CK	C 3 CK -
l—— ∖Jx,r ∖ ( r + θκ K C1) + ~j^^∖θ∖χl θ ∖κ 1IK K λ/ m 3 log( m K) + J=---------------1 J=~K θκ K
mK	mK x ,	mK r	mK rK
1
√m K
( Lr-----mK + PmKm3 log(1 Ie) ) (r + θκKC1)
m3 κ1 κK
1 / /	/ 、	/	/	、、、\	C--―；--	C K CK	C3 CK
+ √m (m3(log(m3) +log(log(mK)))) K1KK Jm3 log(mK) + r√m + √m 样 θκK.
≤ 1
一 √m K
r------mK + PmKm3 log(1 /e) ) (r + θκKC1)
3K1KK
+m√mκ Pogm›ogm3)+log(log(m K)))+Cm+√C θ K.	(194)
Now setting
r* = (C1C2)KK3 m3”(κ11/KK)"3 .
mK
By this choice, from (194) we obtain
∖γ _r ∖ ≤ (	(CIck)4/3	, (CICK)KK3m3/3(K1KK)IK3plog(1 1) λ∕1+ θC1K3(κK√mk)KK3、
∖ x0- x"∖ 一 ((√m K K k)1 / 3( √m3 K1)1 /3 +	m1 k 3	八+ C2 / 3( K1 √m 3)1 K 3
3KK
+z√f" P^m>°g(m 3)+log(log(m K))).
Now we set
θ*:= 3 log(mK),
which also satisfies the condition of Lemma 29 and combining with Equation (183), we get that with
high probability
∖Γx0,V0 ∖ ≤ ∖Γx0,V0 - Γx0∖ + ∖Γx0 ∖
.(	(CICk)4/3	I (CICK)KK3m2K3(K1KK)IK3Plog(1 1)\( 1l (	)C1K3(KK√mK)K/3\
.1(√mKKk)1 /3(√m3K1)1 /3 +	m1/3	A + g(mK)C2K3(K1 √m3)1 /3)
3 / K	_________ ____________________________________________
+——3T=- vzlog( m κ)(log( m 3) + log(log( m K))) + K1K K √m 3 log( m K),
mK
where nx0n	.	CK	and nV0nF	≤	CK,	∀j:	Vj0 φ(0)(xi) =	0.	We also need to satisfy condition (184),
which regarding this choice for θ = θ* becomes
r* = (C1Ck)KK3m^(TKKK)IK3 ≥ 2Cm3Kκe,
mK
for which it suffices to set
1K3
e* = (C1Ck)KK3--------KI , KK3 ,
2(mK m3)1K3KK
(195)
(196)
Substituting this choice of e above and picking the overparameterization large enough to dominate
the magnitude of C1, CK so that log(1Ie*) . log(mK), the proof is complete.
68
Published as a conference paper at ICLR 2022
Lemma 23 Under the following condition
(√m2κ2)1 /3(√m3Kι)2/3 ≥ log-7/6(m2)(C1C2)1 /3,
with high probability we have
sup	3 H X aj M (V⑼ + Vj)(φ ⑼(Xi)+ X) ≥ 0 }Vj(0) k
kx0k.C1,kV0kF≤C2,V0⊥φ(0) m2	j
.√m3κ2 log(m2) + ((√m22)2∕3 (C1C2)223 log1 /6(m2) = <7.
(√m 3 κ 1)213
Proof of Lemma 23
Similar to Lemma 22, define the helper functions Γx0 and Γx0,V0 as
Γ“v，= ɪk X aj 1{(V⑼ + Vj)(φ⑼(Xi) + X) ≥ 0}Vj(ff) k,	(197)
m2	j
ΓX = √mk Xajl{vjθ0(φ⑼(Xi) + X) ≥ 0}Vj(0)k.	(198)
2j
First we bound supx0 Γx0. To this end, note that because Vj(0) ∈ Rm3 and the VC-dimension of
half-planes is m3 + 1, then by Sauer’s Lemma, the set
D = {Dv(0),XiI Xy Rm3,kx0k . C1}
of all sign pattern matrices has cardinality at most
|D| ≤ m2m3+1.
Now note that with high probability, the entries of the matrix V⑼ are all less than
O(κ2,log(m2m3)). On the other hand, for each fixed sign pattern DV(0)χi, we have for the
sum with respect to this sign pattern:
∣∣4 Xaj 1{V(O)(φ⑼(Xi) + X0) ≥ 0}V(0)Il2	(199)
m2 j
is (m3κ24 log2 (m2m3), κ22 log(m2m3)) sub-exponential with respect to the randomness of a, be-
cause each entry of the vector √=2 Pj∙ aj l{Vj^0^ (φ⑼(Xi) + X) ≥ 0}Vj^0^ is (K2plog(m2m3))-
subGaussian. Therefore, with high probability we have
k√m X aj l{Vj⑼(φ (0)( Xi) + X0) ≥ 0 }V(ff) k2	(200)
2j
≤ E a [∣∣ ɪ X aj !{vj00( φ(O)(Xi)+ x0 ) ≥ 0 }V(0) ∣∣2]+ deviation	(201)
m2 j
.m3κ2 log(m2m3) + √m3K2 log(m2m3) + κ2 log(m2m3).	(202)
Similarly, ifwe take a union bound over all sign matrices in D and using the fact that m2 > m3:
suprX，= sup kj Xajl{Vj⑼(φ(0)(Xi) + X) ≥ 0}Vj(0)k2	(203)
x0	x0∈Rm3	m2 j
.m3κ2 log(m2m3) + √m3κ2 log(m2m3) Jlog(m尸+1) + κ2 log(m2m3)log(m尸+1) (204)
. m3K22 log2 (m2 ),	(205)
which implies
sup Γ x，. √m 3 κ 2 log( m 2).	(206)
x，
69
Published as a conference paper at ICLR 2022
Moreover, defining Jv,x similar to Lemma 22 and using the similar approach we get with high
probability
max |Jx,r | .
x∈BC1 ()
r
√m 3 K1K 2
m2 + √m2 log( ∣Bcι (e) |)
-ʒ=----m2 + VZm2m3 log(1 /t).
m3 K1 K2
(207)
Now for simplifying the analysis, we assume that for indices j ∈ Jx,r we can change the sign
pattern with no cost on V0, i.e. we can pick any subset of them. Therefore, we first compute a high
probability upper bound on the following quantity:
√⅛ S"SU M signs" X ±Vj(0)"∙
(208)
Ifwe form the matrix V(0) (Jx,r) be the matrix which only keeps the rows with indices in Jx,r, then
the above quantity can be computed as
占 sup " X ±j " = -L-	sup	"vT V ⑼(Jx,r ) "	(209)
Vm 2 S⊂Jχ,r ,± signs j∈s	mm 2 v∈{ 0,1 — 1 }lJχ,r 1
≤ /— λ max(V(0)( Jx,r )) sup Ilvll ≤ /— λ max(V(0)( Jx,r )) lJx,r l,	(210)
m2	m2
where λmax is the maximum singular value of the matrix. Now by random matrix theory, we know
for a fixed x and arbitrary t ≥ 0, the following argument holds:
P(λmaχ( V⑼(Jx,r ))/κ2 & 乒 + 而]+ t) ≤ 2e-°'2 .
Therefore, as |D| ≤m2m3+1, we get with high probability
max λmax(V⑼(Jx,r)) . κ2(√m3 + q∣Jχ,rI + ʌ/log(mm3+1))
x∈BC1 ()
≤ K2Plog(m2)m3 + K2φjχ,r∣.
Therefore, with high probability
sup	sup Il X V⑼ Il ≤ ； (qlθg(m2)m3 lJx,r 1 + lJx,r |)∙
x∈BC1() m2 S⊂Jx,r j∈S j	m2
(211)
(212)
(213)
(214)
On the other hand, as in Equation (192) in the proof of Lemma 22, for j ∈/ Jx,r we have:
|Vj(0)(φ(0)(Xi) +	x0)I	≥	∣Vj(0'l(φ(0)(Xi) + X)I -	∣Vj(°^	(X	- x0) I & r -	-m3K2e.	(215)
Picking
r
c^ :=----------,
2√m3K2 ,
we get for j ∈/ Jx,r
IVj(0)(φ(0)(Xi)+X0)I &r.
Now similar to the derivation in (193) we have
∣1{ (V⑼+ V)(φ (0)( Xi)+ X0) ≥ 0 }-l{V ⑼(φ ⑼(Xi)+ X0) ≥ 0 }∣	(216)
≤ MIV0IG & IV(0)(φ(0)(Xi) + X0)∣}	(217)
r
≤ {{Vj^"I & 冤}.	(218)
Hence, because ∣∣V0∣∣f ≤ C2, the number of indices for which 1{(V⑼ + V)(φ(0)(Xi) + x0) ≥
0} = 1{V⑼(φ(0)(Xi) + X0) ≥ 0} is at most l = (C1C2) . Therefore, we bound the following
quantity to use in the analysis:
sup I X ±Vj(0) I.	(219)
S⊂ [ m 2] & ∣S∣≤∕,± signs j∈s
70
Published as a conference paper at ICLR 2022
But if we define for m2 < j ≤ 2m2,
then
sup k	±Vj(0)k ≤ sup k	Vj(0)k.
S⊂ [ m 2] & ∣S∣≤∕,± signs j∈s	S⊂ [2 m 2] & ∣S∣≤1 j∈s
Now note that each entry of Pj∈s V⑼ is ^√lκ2 subGaussian. Hence, the quantity ∣∣ Pj∈s V⑼ ∣∣2
is (m3l2κ42, lκ22) subexponential. Therefore, we have with high probability
sup ∣ X Vj⑼ ∣2. E ∣ X Vj⑼ ∣2 + √m 3IK 2∖ jog (2 m 2) + IK 2 log (2 7)
∣S∣≤l j∈S	j∈S	l	l
.m3lκ2 + √m3lκ2 jog ,m2)+ lκ2 log rm[
.m3lκ2 + √m31/2κ2 plog(m2) + l2K2 log(m2).
Hence
Now using Equation , we can write
IΓx0 - Γx0,V 0 I
≤ —m-1 .X (1{(V⑼ + Vj)(φ⑼(Xi) + X) ≥ 0}-i{Vj(0)(φ⑼(Xi) + X0) ≥ 0})V⑼∣
1
+	~j=^
m2
1
∣ E (1{ (V⑼ + Vj )(φ ⑼(Xi)+ X0 ≥ 0 } — 1{V ⑼(φ(0) (Xi)+ x0) ≥ 0 })V⑼ ∣
j∈/ Jx,r
≤	__ SuP
m2 S⊂Jx,r,±signs
1
∣	±Vj(0) ∣
j∈S
+ ―^	SuP
Mm 2 S⊂ [ m 2] & ∣S∣≤ (ClrC2 )2 ,± signs
∣	±Vj(0) ∣
j∈S
≤ √m (qlog(m2)m3 lJx,r 1 + lJx,r |) + √m (—m33√1k2 + IK2 Plog(m2)
K2
≤ √m:
L (	Trn 2---+ Im 2 m3 log(
2	m3K1K2
√mK2 )) / (Plog(m2)m3 + q∣Jx,rI)
1
√m 2
.
〜√m 2
1
√m 2
√m3( C1C2 /r) κ 2 + (C1C2/r )2 K 2 √log( m 2)
(—m~ + ∖Im2m3 log(包运))
m3K1K2	r
(—m3(C1C2/r)K2 + (C1C2/r)2K2Plog(m2)).
Combining this with (206):
IΓχ//1 . √m3K2 log(m2)
(220)
TyPm 2
√m3 κ 1
+ K2∖ m3 log( (^mK2 ) + -ɪ (—m3(C1C2/r)K2 + (C1C2/r)2K2Plog(m2)).
r	m2
(221)
Now setting
r* = m¾≠!(C1C2)223 log1 /6(m2),
m2
+
r
71
Published as a conference paper at ICLR 2022
we get
LHS . √m3κ2 log(m2) + (√h^ (CC2产3 log1 /6(m2)
+ K2m112 log112(m2) +
m 313 κ 213( C ι C2)113
m 116κ113 log116(m2)
.√m3 κ 2 log(m 2)+(√m32κ 2 )213 (CIC2 )213log116m 2)
+
m1312κ2(C1C2)113
(√m 2 K 2)113( √m 3 κ 1)113 log116( m 2)
(222)
(223)
(224)
(225)
Now under the condition
(√m 2 K 2)113( √m 3 K1)2 2 3 ≥ log - 716( m 2)( C1C2)113,
The final term is dominated by the first term, which finally completes the proof.
72
Published as a conference paper at ICLR 2022
A.16 Convergence
The goal of this section is to prove Theorem 7.
Theorem 7 Letting N = 4B2, by Corollary 8.1, we have Lπ(0) ≤ N. We define the domain
Dl := {|”|| ≤ Cι := ^+4, kv0k ≤ C2 := ^+24l}. For a large enough constant l = O(1) and
function LΠ(w := (w0, v0)) : RN → R. Moreover, suppose LΠ is ρ1 Lipschitz, ρ2 gradient Lipschitz,
and ρ3 hessian Lipschitz in the domain Dl (ρ1, ρ2, ρ3 ≥ 1), in the sense that their first, second,
and third directional derivatives in an arbitrary unit direction is bounded by the corresponding
parameters. Suppose we have access to the gradient of LΠ at each point in Dl plus a zero mean
noise vector £ such that σ12I ≤ E££T ≤ σ22I and k£k ≤ Q almost surely. Also, suppose for a
threshold n` ≤ N, if Lπ(W) ≥ n` and W ∈ Dl, then we have at least one OfthefoUowing conditions
holds:
⑴ kvLπ(W) k≥ WC⅛,
⑵ λmin ( V2LRW)) ≤ -γ.
(226)
(227)
Then starting from W0 = 0, with probability at least 0.999 after at most
poly(Pι,ρ2,ρ3, Q, N, Cι, C2, 1 /γ, log(σ 1 /σ2)) number of iterations, We reach a point Wt
such that Lπ(Wt) ≤ n`,
Proof Our proof here is a refined version of that in Ge et al. (2015a). As We mentioned in section 5,
the key fact that We are using in the other parts of our proof is a uniform upper bound kW0 k ≤ C1 ,
kv0 k ≤ C2 Which is unjustified by only naively using Ge et al, (2015a), Here, first We restate a
refined version of Lemmas 14 and 16 in Ge et al, (2015a) in Lemmas 24 and 26 respectively, and
then use them to also bound the upWard deviations of LΠ , Moreover, to avoid Writing repeated
proofs and overWhelm the reader, We mostly treat the arguments in Lemma 16 of Ge et al, (2015a)
as blackbox and use them for our purpose here, A point to mention before We start, unlike Lemma
14 of Ge et al, (2015a) Where the dependency on other parameters than the step size η is more
explicit, Lemma 16 hides the dependencies on all the other parameters (Which is polynomial), Here,
We folloW the same style,
We refer to the trajectory of the steps of algorithm by (Wt)t≥0, In Lemmas of this section, To avoid
introducing neW notation and complicating things, We refer to the current point of the algorithm by
W0 , While for the next point of the algorithm We use W1 (in Lemma 24), and WT (in Lemma 26)
∙-v	∙-v
respectively. Also, similar to Ge et al. (2015a), O and Ω below means we are looking at the depen-
dency on η ,
Lemma 24 Suppose LΠ(W0) ≤ N+2l, and consider a parameter χ > 1 which can be set arbitrarily.
For every point W0 such that Lπ(W0) ≤ N + 21, ∣∣VLπ(W0)∣∣ ≥ 2 Jn(Q2 + σ2N)P2P2(2X + ɪ),
then for W1 := W0 - η(vLΠ (W0) +£) and random variable R1 (depending on W0) defined as
ELΠ(W1) - LΠ(W0) = -η2R12,	(228)
∙-v
we have Ri = Ω(1) a.s., and almost surely:
ILπ(W1) — Lπ(Wo)∣ ≤ nRi/√P2X.
(the expectation is over the randomness of £).
Proof This lemma is a tuned version of Lemma 14in Ge et al. (2015a). First, note that the condition
LΠ(W0) ≤ N + 2l assures the smoothness coefficients P1, P2 and P3 for LΠ by Corollary ??. We
follow similar to Ge et al. (2015a) (picking η< 1/(2P2)):
ELπ(wi) — Lπ(w0) ≤ — 2∣VLπ(w0)∣2 + n2σ2p2N
≤ — 4IIvlπ(W0)Il2 — n2(σ2N + Q2)ρ2P2(2X + 2)+ n σ^2202N
≤ — 4 ∣VL π( W 0) k2 — 2 n2 Q 2 P 2 P1 χ.	(229)
73
Published as a conference paper at ICLR 2022
where we used the fact that ρ1 ≥ 1. On ther other hand, LΠ is ρ1 Lipcshitz, so we have almost
surely
Lπ(W1) - Lπ(W0)| ≤ P1 η(∣NLπ(W0)+ £k) ≤ P1 η(∣NLπ(W0)k + |£k)
≤ P1 η(∣∣VLπ(Wo)k + Q).	(230)
(To be completely precise, we should justify that we can write the Lipschitz inequality at point W0,
we also need to make sure that W1 remains in the domain that we have the Lipschitz parameter in,
i.e. Dl . To see why this is true, see the next Corollary).
Therefore
(P2χ)LΠ(W1) - LΠ(W0) ≤ 2η2P21P2χkVLΠ(W0)k2 + 2P2χη2P12Q2.	(231)
Taking
η≤ (8P21P2χ)-1,	(232)
we get from Equation (229):
ELΠ(W1) - LΠ(W0) ≤ -2η2P21P2χkVLΠ(W0)k2 - 2P2χη2.P12Q2.	(233)
Combining Equations (231) and (233), we see that
(P2χ)LΠ(W1) - LΠ(W0)2 ≤ -(ELΠ(W1) - LΠ(W0)).
Hence, if we define
ELΠ(W1) - LΠ(W0) := -η2R12,
we get
ILπ(Wι) - Lπ(Wo)∣ ≤ ηRi/√P2X,	(234)
and from Equation (233), that
R2 ≥ 2P2P2χkVLπ(W0)k2 + 2P2χρ 1Q2 ≥ 2ρ2χρ 1Q2 = Ω(1).
Moreover, because the function is P1-Lipshitz at the domain point W0, we get from Equation (230):
-η2R2 = ELπ(WIiπ(W0) ≥ -nP 1(kVLπ(W0)k + Q) ≥ -η 1(p 1 + Q) ≥ -P1χ,
by taking
η ≤ (P1 (P1 + Q)P2χ)-1,
which implies
η R1 ≤	1 .
√p2X
This, combined with Equation (234) and triangle inequality implies:
ILπ(w 1) - ELπ(w 1)∣ ≤ ηR1 /√P2χ + η2R1 ≤ 2ηtR∖/√2χ.
(235)
Lemma 25 As long as the value of the function at some W is bounded by N + 21 (Lπ(W) ≤
N + 21), then η can be picked small enough (polynomially in other parameters), namely η ≤
l/(∣∣VLπ(w0) k + Q), so that the Change OfthefUnction by a SteP is bounded by 1.
Proof Let ψ = min{ψ1, ψ2}. First, note that as the function is bounded by N + 2l, we have the
Lipschitz parameter P1, hence kVLΠ(W)k ≤ P1. Therefore, the change in W in a step is bounded as
kVLπ(w) + £k ≤ Q + P1.
So by picking
N + 31	N + 21
η ≤ W ~ψ~ -V ~ιp(, (Q+PI),
74
Published as a conference paper at ICLR 2022
We guarantee that the value of w after a step remains in the ball of radius J ^+ψ3l, hence We still have
the smoothing parameters even after one step. Therefore, now we can use the Lipcshitz parameter ρ1
to bound the value of the function after one step as it is Written in Equation Equation (230). Using
this Equation, it is enough to pick η as small as:
η ≤ l/(kyLπ(Wo)k + Q),	(236)
so that the change in the function Would be at most l as desired.
Lemma 26 For a fixed point W o s.t. L π( W 0) ≤ N + 21, suppose we pick η small enough such that
§(η) := 2∖∕η(Q2 + σ2N)ρ2ρ2(2χ +1) < “晨 ”2 ∙
2	16 C12 + C22
Then, note thatfor ∣∣VLπ(Wo)∣∣ ≤ §(η), condition 227 implies:
λmin (V2LRW0)) ≤ γ∙
Then, using the notation ET for the high probability event corresponding to Equations (36) and (44)
in Ge et al. (2015a), for small enough η (polynomially small w.r.t other parameters), for R2 defined
as
E[Lπ(WT) - Lπ(w0)] 1{ET} = — R2η2,	(237)
we have almost surely
∣[Lπ(WT) - Lπ(W0)] 1{Eτ}∣ ≤ R2η∕√ρ2χ∙	(238)
Note that in the expectations above W0 is assumed fixed. Furthermore, we can assume P(ET) ≥
1 - O( η 5).
Proof This Lemma is a tuned version of Lemma 16 in Ge et al. (2015a). We change a couple of
things here. First, We consider an implicit coupling that if Wt exits Dl We do not move it anymore,
i.e. Wt0 = Wt, ∀t0 ≥ t, Which means the noise vectors also becomes zero, i.e. £t0 = 0, ∀t0 ≥ t.
This Way, the sequence of noise vectors remain bounded by Q, because if Wt is inside Dl, then by
assumption k£tk ≤ Q, while otherwise £ = 0. We denote the event that the sequence W 0 ,∙∙∙wt
remain in Dl by ET, Where T is defined in Lemma 16 ofGe et al. (2015a).
Note that we also have the smoothing parameters ρ1, ρ2, ρ3 for all (Wt) because of this coupling. In
fact, we will use a more strict coupling; we consider the event ET to be the high probability event
corresponding to the bounds in Equations (44) and (36) of Ge et al. (2015a) holding for all t ≤ T;
We will see that ET ⊆ ET at the end of this proof, but for now we assume it is true. An important
point to note here is that in Ge et al. (2015a), P(ET) is bounded by O(η2). However, the exponent
dependency of η in this bound comes from Azuma-Hoeffding type inequalities, particularly used in
Equations (60) and (42) in Ge et al. (2015a), in which by considering larger constants one can easily
get higher exponents. For our analysis, a bit stronger dependence of η5 is required.
Also, because the distribution of our noise depends on the point W, our sequence of noise vectors
(£t ) is a martingale instead of being i.i.d, so we apply Azuma-Hoeffding inequality instead of the
simple Hoeffdings in Lemma 16 of Ge et al. (2015a). (because we are also sampling a random
(xi , yi ) to compute the estimate of the gradient, this could be simplified to the case where we com-
pute the actual gradient and then inject an i.i.d noise vector in each step, but it is an overhead to
compute the actual gradient, so here we choose to analyze the more complicated case.)
∙-v
Next, notice the definition of Λ and Λ right after Equation (66) in Ge et al. (2015a), which in our
notation translates to
Λ := VLπ(w0)TE+1 δHδ, Λ = VLπ(w0)Tδ + 1 δτHδ + δτHδ + P∣∣δ + δ∣3∙	(239)
2	26
where
∙-v
δ = Wτ — w0, δ = WT — Wτ,
75
Published as a conference paper at ICLR 2022
for (Wt) which is a coupled sequence with (Wt) as defined in Ge et al. (2015a). Note that We apply
the coupling for the sequence Wt as well, i.e. if Wt+1 = Wt, we also set Wt+1 = Wt.
To show Equation (237), we want to use Equation (67) in Ge et al. (2015a), though we only use the
expansion for the first term which is under 1{£7}, i.e.
E[Lπ(WT) - Lπ(wo)]IET = EΛIET + EAlET.	(240)
First of all, as it is mentioned in Lemma 16 of (Ge et al., 2015a), in the case where the noise vector
σ12I ≤ E££T ≤ σ22I instead of having E££T = σ2I fora fixed σ, in order to still get a negative
term of order η in Equation (68) of (Ge et al., 2015a), we just need the size of Tmax to be as large
as O(* (log d + log σ1)), and it does not change the order of η in any other part of Lemma 16.
Now similar to Equation (68) of (Ge et al., 2015a), if w.l.o.g we assume the smallest eigenvalue γ0
corresponds to i = 1:
N T -1
EAIE T ≤ 2 fλf {,i<.} 0} (1 - ηλi )2 τ η2 σ 2P(E T)+
i=1	τ=0
N	T-1
2∑ %£{-λ^≥0}0} (1 - ηλi)2τ η2σ22
i=1	τ=0
2	T-1	2
≤ η hσ 2--------γ oσ 2P(E t ) X(1 + ηγ o)2 Ti ≤-等.
n	i—∩
(241)
(242)
(243)
where in the last line we use the fact that P(ET) ≤ 1 /2 plus the additional log(σ2/σ 1) factor.
Second, note that our threshold § (η) for the size of gradient in Lemmas 24 and 26 has the same
order of η compared to that of Lemmas 14 and 16 in Ge et al. (2015a). Therefore, the arguments in
Lemma 16 that considers the order of η and treat the other parameters as constants is true here as
well. Hence, we still have Equation (69) ofGe et al. (2015a) which is under the event ET. Applying
it to Equation (240),
Hence, finally by a similar derivation of Equation (67) in Ge et al. (2015a):
E[Lπ(WT) - Lπ(W0)] 1{Et} ≤ -Ω(η).
(244)
Next, we turn to prove the second bound (238). Combining Equations (36) and (44) in (Ge et al.,
2015a), we get with high probability (we use the final high probability parameter of Lemma 16
which is the result of a union bound over all the high probability arguments which is equivalent to
the occurrence of ET), i.e. when ET happens,
IIWT - W0 Il ≤ O(η1 log1).	(245)
η
Picking η small enough such that for the bound above we have
O (η1 log1) ≤
η
we get for every W in the line connecting W0 to WT :
which implies that LΠ has the smoothing parameters ρ1, ρ2, ρ3 along W0 to WT. Therefore, by the
ρ2 -gradient smoothness property of LΠ :
∣∣VLπ(W)- NLπ(W0)Il ≤ P2∣∣w0 - WIl ≤ O(P2η1 log 1).
η
Combining the assumption of the Lemma ∣∣VLπ(w0) ∣∣ ≤ O(η 1), we get
IVL π( W) II =O(η112 log(i/η)).
76
Published as a conference paper at ICLR 2022
∙-v
(The last O also hides the dependency on ρ2). Now integrating over the derivative along the direction
from w0 to wT :
L π( wτ) = L π( w o) + / NL π( tw o + (1 — t )wτ) T (wτ — w o) dt.
Therefore, using (245) one more time, under the event ET :
∣Lπ(wt) — Lπ(Wo)| ≤ J ∣NLπ(two + (1 — t)wτ)T(wτ — W0)∣dt
≤ Z INLΠ (tw0 + (1 — t)wT IIwT — w0 Idt
0
≤
O( η112 Iogi/η) IIwT — W 0 Il = O( η log21 /η).
Hence
[LΠ (wT ) -
Lπ(W0)] 1{Eτ}∣ ≤。(ηlog21 /η),
(246)
which
Now comparing Equations (244) and (246), it is clear that one can pick η small enough (again
polynomially small in the other parameters) such that for some random variable R2, which also
depends on η, so that equations (237) and (238) hold.
It remains to show ET ⊆ ET . This is desirable as up until now we have only proved (237) and (238)
for the coupled sequence (which does not move outside the ball Dl ), but we know that under the
event ET , the coupled sequence and the original sequence are the same, which automatically implies
the conclusion for the original sequence. Notice that the bound in (246) is an a.s. upper bound on
the change of the function value under the event ET for every 1 ≤ t ≤ T . Therefore, by picking η
small enough (polynomially) s.t. the quantity O(η log(1 /η)2) in Equation (246) is bounded by l, We
again make sure that the value of function during these steps changes by at most l compared to w0,
i.e. for every 1 ≤ t ≤ T :
∣[Lπ(wt) — Lπ(Wo)] 1{Et}∣≤ l,	(247)
hence, remains bounded by N + 31. This implies ET ⊆ ET as promised.
A.17 PROCESS FROM A HIGHER VIEW: DEFINITION OF THE (X) SEQUENCE
The goal here is to find a w^ with Lπ(w^) ≤ n` using Lemmas 24 and 26 (recall the definition of
n` from Theorem 7). The main result of this section is Lemma 27. For this purpose we define a
useful coupling: to begin, as done in Ge et al. (2015a), define a sequence of times τi inductively in
the following way: To define τi+1 based on τi, if the condition
N' ≤ Lπ(wτi+ι) ≤N + 21	(248)
does not hold, then just set τi+1 = τi ?(1). Otherwise, using the conditions (227), we are either in
the situation of Lemma 24 or Lemma 26 by setting the value of w0 in these Lemmas as w0 = wτi .
In the first case, define τi+1 = τi + 1 ?(2). In the latter case, Let ET be the same high probability
event that we consider in Lemma (26), which happens when the aggregate behavior of the noise
vectors is normal, as a result of which w remains close to the starting point w0 . Note that from
Lemma 26, we know P(ET) ≥ 1 — O(η5). Now if the event ET happens, define τi+1 := τi + T
?(3), for T also from Lemma 26 and defined originally in Lemma 16 of Ge et al. (2015a), while
otherwise, define Ti+ι = Ti ?(4). Moreover, if ET does not happen, define the rest of Ss equal
to τi: τi0 = τi for every i0 ≥ i. At the same time, we define the monotone increasing events {Gi},
where Gi happens in the case ?(4), and Gi+1 happens in case ?(4). Also, Gi happens if any of the
previous Gi0’s happen for i0 < i; in other words, Gi is included in Gi+1. We use these events to
bound the probability that the process remains above N'. Moreover, define the sequence of random
variables (Xi) as Xi := LΠ (wτi ). Note that by Lemma 25 and Equation (247) in Lemma 26, we
have N' — 1 ≤ Xi ≤ N + 31. The key idea behind defining Xi ’s is that we want to bound the MGF
of LΠ(wt), without worrying about falling out of the assumptions of Lemmas 24 and 26. With the
definition of (Xi ) and Gi, we are ready to state the theorem which roughly says the sequence Ti will
most likely stop after a number of steps.
77
Published as a conference paper at ICLR 2022
Lemma 27 Let QR := U∞Lr ({ Ti ≤ R} ∩ G∣i). Then, for some
R = O(log(1 /1))( N + 3 L)
(249)
we have P(QR) ≤ δ1. In other words, after R iterations of PSGD, the defined sequence (Xi) above
has either been in situation ?(1) or ?(4). Here, θ depends polynomially in the other parameters.
Proof By Equations (228) and (244) in Lemmas 24 and 26, there exist a constant θ depending
polynomially on all parameters except η such that
E[Xi +1 — Xi1 Gi] ≤ -θ(Ti +1 - Ti 加2 ∙	(25O)
Now for some constant C that We specify later, define the random time ι as the largest i where
Ti ≤ C/η2. Using the fact that Gi-1 ⊆ Gi, for every i we have a.s.:
Xi +ι l{<7i} — XiI {<7i-1} = l{Gi — Gi-1} (—Xi) + (Xi+ι — Xi) 1{Gi}∙
Now summing this for i = 1 to ι, taking expectation from both sides and using (250):
∞
EXi+ι {.{} — Xo = X El{Gi — Gi-1}(-Xi)l{ι ≥ i}
i=1
∞
+ X(Xi +1 — Xi) 1{G?i ∩ {1 ≥ i}}
i=1
∞
≤ XE(l{Gi} — {{(-i-1})(-Xi)
i=1
∞
+ X E(Xi+1 — Xi I Gi ∩ {1 ≥ i})P(GJi ∩ {1 ≥ i})
i=1
≤ sup sup |Xi |
i
∞
+ θ £ E(—(Ti+1 — Ti)η2 I Gi ∩{1 ≥ i})P(Gi ∩{1 ≥ i})
i=1
∞
=SUPsUP ∣Xi∣ — η2θ X E(Ti +1 — Ti) 1{Gi ∩ {1 ≥ i}}∙
Now using Lemma 25, we know that in except when ET happens (in which we stop the time se-
quence Ti), the increments ofXi are at most L. Therefore, the value ofXi’s always remain bounded
by N + 3L, hence:
∞
LHS ≤ N + 3L — θn2 XE(Ti +1 — Ti) 1{Gi ∩ {ι ≥ i}}∙
i=1
Also, by restricting the integration of the second term to the part Ui∞=1 GJi ∩ {Ti ≥ 2C/n2} of the
sample space, we know that under the event {ι ≥ i}, Gi automatically happens when Ti+1 = Ti (it
is easy to check). Therefore:
∞∞
LHS ≤N + 31 — θη 2 El { J Qi∩ {Ti ≥ C/n 2})} £( Ti +1 — Ti) l{Gi∩{1 ≥ i}}
i=1	i=1
∞∞
=N + 31 — θn2EI{U 仅i ∩ {Ti ≥ C/n2})} X(Ti +1 — Ti) 1{1 ≥ i}
i=1	i=1
∞i
N + 31 — θn2EI{U 仅i ∩ {Ti ≥ C/n2})} £(Ti+1 — Ti)
i=1	i=1
∞
N + 31 — θn2El{U 仅i ∩{Ti ≥ C/n2 })}Ti+1∙
i=1
78
Published as a conference paper at ICLR 2022
Now by the definition of ι, τ +ι ≥ C/n2. Hence, We can write
∞
LHS ≤^ + 3i-θn2E1{U ®∩{τi ≥ C/n2})}(C/n2)
i=1
∞
N + 31 - cθp( U 仅i ∩ {τi ≥ C/n2})y
i=1
But note that Xi ’s are a.s. bounded between 0 and N + 3l, which implies the LHS above is at least
-(N + 3l). Therefore, we finally get:
∞
P(U 仅i ∩{τi ≥ C/n2})) ≤
i=1
2 N + 61
Cθ
Picking C = C * := 2(2 N + 61)/θ:
∞
P(U 仅i ∩{τi ≥ C*/n2})) ≤ 2.
i=1
(251)
∙-v
Note that the differences between Ti飞 is at most TmaX = O(1 /n) Ge et al. (2015a). Hence, again
for n polynomially small in other parameters, (251) implies that for R = 2C* /n2, there exists
R = poly(.) such that after R iterations on the main sequence (Wt), the corresponding sequence (Ti)
has either been in ?(1) or ?(4) with chance at least 1 /2. Repeating this argument log(1 /δι) times
(using the markov property of the process) we conclude the proof.
A.18 B OUNDING THE MGF OF Xi’S
Next, we want to exploit Xi’s to bound the upward deviation of LΠ(wt). For a fix θ the goal
here is to bound E[exp{θXi}] (this is a different θ!). More precisely, let Ft be the sub-sigma field
generated by variables wt from time zero to t, and Fi := Fτi be the sigma field of the stop time Ti.
Then, obviously, Xi is measurable w.r.t Fi . We prove the following theorem:
Theorem 8 For any θ > 0, the sequence (Eeθ(Xi-X0))i∞=1 is a supermartingale with respect to the
filteration (Fi),
Proof We proceed inductively by jointly conditioning on the previous Xi and whether Gi has hap-
pened or not, and whether we are in situation ?(2) or ?(3). We have
E[exp{θ(Xi+1 - X0)}| Fi]
=E[exp{θ(Xi +ι — Xi + Xi- Xo)}l{Gi}∣Fi]
+ E[exp{θ(Xi+ι - Xi + Xi- Xo)}1{Gi ∩ ?(2)}∣Fi]
+ E[exp{θ(Xi+ι - Xi + Xi- Xo)}1{Gi ∩ ?(3)}∣Fi].
Now by the a.s. bounds of Lemmas 24 and 26:
E[ Xi +1 - Xil Gi) wTi s.t. ? (2)] = - R2 n 2,
E[Xi +ι — Xi| Gi, WτiSt ? (3)] = — R2n2,
(Xi +1 - Xi)^^-{Qi} = 0∙ (a∙s∙)
Where R1 and R2 are r.v. defined in Lemmas 24 and 26 and are clearly Fi measurable. This implies
E[( Xi +ι - Xi) 1{Gi, wτ st?⑵}∣Fi ] = - R2 n 2 l{θiw wτi s∙t.?⑵},
E[(Xi +ι - Xi) 1{Gi, Wτis.t.?⑶}∣ Fi] = -R2n2{{i,iw Wτis∙t.?⑶}.
Now we mention the following fact:
Fact For a σ subGaussian random variable X we have E[exp{θX}] ≤ exp{θ2σ2}.
Using the a.s. bounds of Lemmas 24 and 26, we get that conditioned on {<√i, WTi st ? (2)},
79
Published as a conference paper at ICLR 2022
Xi +ι 一 EXi +ι is a.s. bounded by 2ηθRi/(ρ2X), and conditioned on {GGi, w^ s.t. ? (3)},
Xi +1 - EXi+ι is bounded by 2ηθR2/(P2X). Therefore, using the above fact
EhexP{θ( Xi+1 — E( Xi+11 G∕i, WTi st? (2))) }∣ G WTi s上.? (2)] ≤ exp { 4 η2 θ 2Ri / (P 2 X)},
(252)
EhexP{θ(Xi+1 — E(Xi+11 Gii, WTi st.? (3)))∣ G WTi st.?⑶}] ≤ exp{4η2θ2R2/(P2X)},
(253)
which implies in the notation of conditional expectation on sigma field:
E[exp{θ(Xi+1 — E(Xi +1 ∣Fi))}l{(ji,	?(2)}	Fii	≤ exP{4η的2火2人P2X)}1{Gi，?(2)},	(254)
EhexP{θ(Xi+1 — E(Xi +1 ∣Fi))}l{(√i,	?(3)}	Fii	≤ exP{4η2"2R2人P2X)}1{Gi，?(3)}.	(255)
Now we write:
LHS ≤ E[exP{θ(Xi+1 — X0)}l{Gi}∣ Fi]
+E[exP {θ (Xi+1 —	E[ Xi+11 Fi ])} exP {θ (E[ Xi +11	Fi ]	—	Xi)} exP {θ (Xi- X 0) }l{Gi ∩ ? (2) }| Fi ]
+E[exP{θ(Xi+1 —	E[Xi+11 Fi])} exP{θ(E[Xi +11	Fi]	-	Xi)} exP{θ(Xi- X0)}l{^Gi ∩ ?(3)}∣Fi]
≤ exP{θ(Xi — X0)}l{Gi}
+ exP{θ(Xi — X0)}E[exP{θ2R2η2/(P2X)} exP{—"(Riη2)}l{Qi ∩ ?(2)}| Fi]
+ exP{θ(Xi — X0)}E[exP{θ2R2η2/(p2X)} exP{—"(R2η2)}l{G∕i ∩ ?(3)}∣ Fi]
≤ exP{θ(Xi — X0)}E[(l{Gi}
+ exP {θ 2r2 η2 / (P 2 X) — θ (R2 η2) }i{Gi∩ ? (2)}
+ exP{θ2r2η2/(P2X) — θ(R2η2)}1{Gi ∩ ?(3)}) I Fii ∙
Now setting θ := 1 and picking X ≥ 1/P2 :
LHS ≤ exP{(Xi — X0)}E[l{Gi} + 1{Gi ∩ ?(2)} + 1{屐 ∩ ?(3)}∣ 司.
= exP{Xi — X0}.
Now by hypothesis of Induction we have
E[exP{Xi+1 — X0}] = E[E[exP{Xi+1 — X0}| Fi]] ≤ E[exP{Xi — X0}] ≤ 1,
which finishes the proof of step of induction.
Now using Doob’s Maximal inequality for positive supermartingales and R defined in (249):
P( suP (Xi — X0) ≥ z)
1≤i≤R
= P( suP exP{Xi — X0} ≥ exP{z}) ≤ E[exP{XR — X0}]/ exP{z} ≤ e-z.	(256)
1≤i≤R
A.19 Proof of Theorem 7
Finally with the developed tools, we are ready to prove Theorem 7.
Proof of Theorem 7 Starting from w0 = 0 with Lπ (w0) ≤ N, We use Equation (256) to get
P(suP1 ≤i≤R exP{Xi — X0} ≥ Ω(log(1 /δ 1))) ≤ δ 1. Therefore, setting l = Θ(log(1 /δ 1)) and a
union bound implies with probability at least 1 — 2δ1 we should have gotten into situation ?(1) or
?(4) without the value of Xi exceeding N + 2l. On the other hand, using Lemma 26 we know that
Eτ happens with probability at least 1 — O(η5) for every 1 ≤ t ≤ R which is equal to Ti for some i
80
Published as a conference paper at ICLR 2022
and when we are in the situation of Lemma 26. As a result, the chance that even one of ET’s happen
along R iterations is at most
Rθ(η 5)= η 3 O(Iog(I /I))(N + 3 3).
θ
But picking η small enough with respect to log(δ1) and other parameters, we conclude that with
probability at least 1 -3δ, after R rounds, we should have gotten into situation ?(1) and not ?(4) and
not exceeding N + 23, which means that Xi = Lπ (w%) has gotten under the threshold n` . Note that
as soon as that happens, we terminate the algorithm. We elaborate on this more in Appendix A.10.
A.20 Gaussian Smoothing
In this section, we describe our smoothing scheme and the approximation that it provides which
enables us to keep the signs from the case η = 0. Recall that we use Gaussian smoothing matrices
Vj)卜〜N(0, β1∕mι) and Wρ k 〜N(0, β2/m2). Here, We will particularly specify lower bounds
for β1 and β2 in order for our sign approximation to be precise. On the other hand, we normally
prefer the smoothing noise to be as low as possible so the primary and smoothed functions are close,
so we set β1, β2 equal to their lower bounds, and use this setting in the other parts.
To begin fix one of the inputs xi . In order to reduce and simplify the amount of notations, we refer
to the sign pattern matrix (diagonal sign matrix) of both the first and second layers by D with the
appropriate indices. More specifically, for the first layer, we refer to Sgn(W (0) + W0 + Wρ)xi by
Do,ρ and Sgn(W⑼ + (1 - η/2)W0 + Wρ + √ηW*)Xi by Do,p. Similarly, for the second layer,
of course depending on the input vector, we refer to the sign matrix with respect to the matrices
V⑼ + V0 + Vp and V⑼ + (1 - η/2)V0 + Vp + √ηV* by by DoP and Do,p,n, respectively. We
introduce two new notations as well for the output of the first layer with respect to different matrix
and sign patterns:
X(1) := WsDop(W⑼ + (1 — η)W0 + WP + √ηV*)Xi,	(257)
x0(2) := WsDo,p,n(W⑼ + (1 - η)W0 + Wp + √ηV*)Xi.	(258)
For further brevity, we sometimes refer to X0(2) by X0.
Now we are ready to mention our approximation theorem regarding the smoothing and the sign
changes.
Lemma 28 Under the conditions K1 √m 3 & C1 + β 1 √m3 and m 2 ≥ m 3 log( m 2), then for ^very
i ∈ [n]:
∣EWρ,vPaDo,p,n(V⑼ + (1- η)V0 + VP + √V*)Ws(W⑼ + (1 - η)W0 + Wp + √ηV*)Xi
-aτDo,p(V⑼ + (1 - η)V0 + VP + √V*)WsDo,p(W⑼ + (1 - η)W0 + WP + √V*)xj
≤ η%2β-1 [(C1 + √m3β 1)2/(κ 1 √m3) + √βn3m 1 β 1 + C^ exp{-C2/(8m3β2)”
× [exP {-C4 / 3( √m 2 κ 2)2 / 3/(8 β 2) } + ( √mCκ 2)2 / 3 ]
+ AK2√m2+C1)(exp{-c2/(32β2)}+ κ√1) %2mβ√m3 := η<8.
(259)
Proof of Lemma 28
81
Published as a conference paper at ICLR 2022
We can bound the Left hand side above as
LHS ≤
∣E W jv P aτ Do,ρ,η (V ⑼ + (1 — η)/+ VP + √ηV *) WsD,,ρ,n (W(O) + (1 — η) W' + Wρ + √ηW *) Xi
-aτDo,ρ(V⑼ + (1 - η)V' + VP + √ηV*)WsDo,ρ,n(W⑼ + (1 - η)W0 + WP + √ηW*)Xi∣
+ ∣EwPVPaTDo,ρ(V⑼ + (1 - η)V0 + VP + √ηV*) WsDo,ρ,n(W(O) + (1 - η)W0 + WP + √ηW*)Xi
-ατDo,p(V(O) + (1 - η)V0 + VP + √V*)WsDo^(W⑼ + (1 - η)W0 + WP + √ηW*)Xi∣.
:= A1 + A2.	(260)
We bound A1 and A2 separately. First, we start with A1.
Let Pi be the set of indices j for which 1{∣(V⑼ + V0)x0∖ ≤ R*κ2kχ0k} happens. Then, from
Lemma 31, we have ∣Pi ∣ . R*m2. Now for j ∈ [m2], we write
1{ sign change in thejth neuron} × ∣ amount of change ∣
≤ {{jPfx, ∈ (-Vj(o)X- Vjx + ηVjx0 - √ηv*x0, -Vj(0)X- Vjx0)} ×
(261)
√m (∣ηVjx0∣ + ∣√ηV*x0∣)
2	(262)
Moreover, note that
∣Vj0x0∣ = ∣Vj0(x0 - φ(O)(xi))∣,
∣Vj*x0∣ = ∣Vj*(x0-φ(O)(xi))∣.
Also, because kVj0k ≤ kV0k ≤ 2C2 plus using Equation (128), we can further upper bound the
above indicator as:
≤ {{jPXx/ ∈ (-Vj((00 X0-Vjx0 - (ηkVjk + √ηkV*k)min {∣∣χ0k, ∣∣χ0-φ(O)(Xi) ∣∣},-V(o)χ0-V0χ0)}
×4 ηkV0k + √ηkv*k) kx0 - Φ (0)( Xi) k
m2
≤ {{^^Xx! ∈ (-VjQ°X0-Vjx0-(2ηC2+√η%2/√m2)min{kx0k, kx0-φ(0)(Xi)∣∣}, -VjQ°X-Vjx0)}
× -τ^(2 ηC2 + √η% 2/√m 2) kx0 — φ (O)(xi) k.
m2	2
Taking √η ≤ %/(2C2√m2), We can further upper bound as
.{{jPXxf ∈ (-VjQ0 x0 - V 0x0 - 2 √η% 2 min {kx0 - φ (O)( Xi) k, kx0k}∕√m 2, -Vj(°^ x0 - V 0x0)}
× (√η%2/m2)kx0 - Φ(O)(Xi)k.
Therefore, conditioned on x0 :
Evρ [1 {sign change in thejth neuron} × ∣amountofchange ∣ ∣ x0] ≤
P( Vj x0 ∈ (-Vj-0 x0 - VjX - 2 √η∣% 2 min {kx0 - φ (O)( Xi) k, kx0k}∕√m 2, -j x0 - VjX))
× (√η%2/m2)kx0 - Φ(O)(Xi)k.
Now notice that for j ∈Pi, we have
∣ -Vj(O)x0 - Vj0x0∣ ≥ R*κ2kx0k.
Also, note that the variable VPXOiS gaussian with variance kx0kβ2∕√m2 Therefore, conditioned on
x0, for j ∈Pi, we have (note that x0 does not depend on the randomness of VP):
P(VjX0 ∈ (-V(O)X0 - VjX，- 2√η%2 min{kx0 - φ(O)(Xi)k, kx0k}∕√m2, -V⑼x0 - Vjx'))
82
Published as a conference paper at ICLR 2022
.exp{min{∣ - V⑼x0 - Vjx0 - 2√η%2kx0k∕√m21, | - V⑼x0 - Vθχ0∣}/(√2kx0kβ2/√m2)}2
× (l∣x0kβ2/√m2)- 1 × (√η%2 min{k^ - Φ(0)(Xi)II, kx0k}∕√m2).
This equation follows from the fact that
P( a ≤N≤ b) . la-bl e min2 {a，b}/σ2.	(263)
σ
On the other hand, note that for √η . C2-(√;K2) / We have:
Rκ 2 kχ0k/2= CC / 3√m2K 2)1/3 ∣χ0k ≥ 2 √η% 2 ∣χok∕√m 2
2 m2
Which implies
.exp {R*κ2 kx0k/(2√kx0kβ2/√m2)}2(√η%2/β2)
≤ exp{-CC43(√m2K2)2/3/(8β2)}(√η%2/β2).
On the other side, for j ∈ Pi, we can write
P(Vρx0 ∈ (-Vj0°X- Vjx- 2√η%2 min{∣X - φ⑼(Xi)k, ∣x0∣}∕√m2 , -Vj0°X- V!χ0))
.(kx0kβ2/√m2)- 1(√η%2 min{∣X - φ(O)(Xi)k, ∣x0∣}∕√m2) = min{∣X - φ(0)(Xi)k∕∣x0k, 1 }√η%2/β2.
Therefore, overall using the fact that ∣Vj^k ≤ %2/√m2, we can write
A1 . X exp {-CC,443(√m2κ2)223/(8β2)}(√η%2/β2) min{∣X - φ(0)(Xi)k2/k^k, kx0 - φ(O)(Xi)k}
j∈P
+ X(√η%2/β2) min{kX - φ(0)(Xi)k∕∣X0k, 1} × (√η%2/m2) ∣X' - φ(0)(Xi) k
j∈P
.hm2 × exp {-C423(√m2κ2)223/(8β2)}(√η%2/β2) × (√η%2/m2)
+ η%2β-1 ]^mi^i min{kX - φ(O)(Xi)k2/kXk, kX0 - Φ(O)(Xi)k}
≤ η%2 β - 1 min {k^- φ (O) (Xi)k 2/kXk, kX0- φ (O)(Xi) k} h exp {-c42 3( √m 2κ 2)2 2 3/(8β C)}+(JmCK 2)2 2 3 i.
(264)	2
Next, we bound A 2. First we bound E W p ∣x0 ⑴-x0 (2) k. Recalling the setting C 2 = 2 √nm 3 C 1 /√λ0
and the definition of in P from Lemma 1, we obtain that for j ∈/ P , we have for all i ∈ [n]:
∣w(0) Xi | ≥ c 2/√m i ,
∣wjχil ≤ c2/(2√m 1),
which means for j ∈/ P :
I(W⑼ + 啊)Xil ≥ C2/(2√m1).	(265)
Also, we have
∣PI ≤ C2√m 1 /κ 1.	(266)
Now using Equation (104) in Lemma 5, we can write for every i ∈ [n]:
valj := 1{ sign change in thejth neuron} × ∣ amount of change ∣
≤ lWjρXχi ∈ (-Wj(o)X - W0Xi + ηWjXi - √ηW*xi, -Wj(0)Xi- WjXi)}
× -^~√I√ηWjXi + ηWjXi∣).
m1 j j
83
Published as a conference paper at ICLR 2022
USingthefaCtthatkWk ≤ ∣∣WIlF ≤ C 1, and Equation (104) (IlW*∣∣ ≤ %,m3) and picking
√η ≤ Cm ,weobtain
≤ l{Wj⅛ ∈ (-W^Xi - W0Xi - η∣Wjk - √ηkW-I ^wj00Xi- WXi)}
×√m (√η∣W jtk + η∣W0k)
≤ 1{WPXi ∈ (-W(0)Xi - Wjxi- 2√ηρ^m3, -W(0)Xi - Wjxi)}
J	JJ	ʌ/m ι	J	J
× √2h √η% Jm3) ∙
√m ι V m ι
NOW for j ∈ P,because WpXiis Gaussian with std √⅛:
EWP [Valj] ≤ P(WpXi	∈	(-Wj0Xx-	- W0Xi - 2√η%3, -W^Xi	-	W0Xi))	×	.——	√η%3
J	JJ	mm 1	j	j	mm 1	mm 1
.exp-{min{∣-W(O)Xi- WJXi- 2√η%等∣, ∣ -%⑼”-W0x0∖}/(√2β 1 √ 1)}2
× (β 11 √m1 1厂 1 × (√η% √-3) × √——√η% √-3 ∙
m1	m1	m1
Now from Equation (265) and by picking √η . √m so that
2 √η% √m3 ≤c 2/(4 √ 1),
then
2
lhs . eχp{-c 2/(32 β 2) }ηβm1
On the other hand, for j ∈ P we have
EValj ≤ P(WpXi ∈ (-Wj：0)x- - W0x- - 2√η%√m3-,-Wj：0)x- - W0x-))
1 ∕q / r- ∖- 1 L √m3	L √m3	%2m3
.(β 1 Nm 1) Vη%-7=3 × 5% —3 = η-3—∙
m1	m1	β1 m1
(267)
1
× r-
m1
(268)
Now define the following random variable with respect to the randomness of Wp:
m 1
Val := E 1{ sign change in thej th neuron} × ∣ amount of change ∣
j=1
then for every k ∈ [m3], we have
IXk⑴-Xk⑵ ∣ ≤ VaI
which implies
kx0(1) - x'(2) k ≤ √m3Val.
But Combining Equations (267) and (268):
EVal ≤ 卜XP{-c2/(32β2)} + m)η%β 1 3
≤ (exp{-c2/(32β2)} + ɪ)η%m,
κ	K 1 √rn 1/	β 1
which implies
Ewρ kx0(1) - x0(2) k ≤ (exp{-c2/(32β2)} +——√=-)η% m3√m3 ∙	(269)
'	K 1 ʌ/m 1	β 1
84
Published as a conference paper at ICLR 2022
Now we can write
I aτ Do,ρ (V ⑼ + (1 — η) V 0 + V P + √ηv *) X ⑵—aτ Do,ρ (V ⑼ + (1 — η) V 0 + V P + √V *) X ⑴ I
1	m2
≤k EI(V(o) + (1 - η)Vj + Vj + √Vj)(X0(2) - d⑴)I
m2 j=1
m2
≤ .  ^X ∣Vj^0^(X⑵—X(1))I + (1 — η)∣Vj(X0(2) — X0⑴)I + ∣Vj(X⑵—X(1))∣ + √η∣v*(X(2) — X⑴)∣
m2 j=1
m2	m2
=√m E ∣^( X0 ⑵-X0 ⑴)∣ + ∣Vρ (X0 ⑵-X0 ⑴)∣ + √m E((I- η) HVlI + √ηkV*k) IX0 ⑵-X0 ⑴ k
2	j=1	2 j=1
1	m2
≤ L E ∣V⑼(X0⑵一X0⑴)∣ + ∣Vjl(X0⑵一X0⑴)∣ + ((1 - η)kV0" + √ηkV**) IX0⑵-X0⑴ k，
m2
2	j=1
Now by Equation (126) in Lemma 10 (i.e. ∣∣ V*∣f . √2) and the fact that ∣∣ V0∣f ≤ C2, and by
taking
L	C2
√η ≤√Z2.'
we have
((1 -η)∣V0kF + √η∣V*∣F) . C1,	(270)
so we can bound the above as
m2
LHS . √=^ X (V?x0⑵-x0⑴)∣ + ∖Vρ(x0⑵-X0⑴)∣) + C1 ∣∣x0⑵-X0⑴||,
m2 j=1
Furthermore, using Lemma 32 and noting the fact that the entries of V(0) are normal with standard
deviation κ2, we get with high probability over the randomness of V(0):
m m 2	_________________________
.√m X VP (X0 (2) -X0 ⑴)∣ + (K 2 √m2+K 2p m 3(log( m 3) + log(log( m 2)))+C ə ∣x0 (2) -X(1) ||,
2 j=1
NoWnotethat Vj (X(2) -X⑴)is normal with standard deviation √β22 ∣x0(2) -X⑴ ∣∣. Hence, taking
expectation with respect to VP :
Ej ∣aτDo,ρ,η(V⑼ + (1 - η) V0 + VP + √V*)X(2) - aτDo,ρ,η(V⑼ + (1 - η) V0 + VP + √V*)X(1) |
.卜 2 √m2 + κ 2 P m 3(log( m 3) + log(log( m 2))) + C ι) ∣x0 ⑵一X(1) ||，
(271)
Finally, Combining Equation (264) and (271) and applying it to Equation (260) implies with high
probability over the random initialization:
∣E W .,V Pa Do,ρ,η (V ⑼ + (1 - η) V0 + VP + √V *) Ws X(2)
-aτDo,p(V⑼ + (1 - η)V0 + VP + √V*)Wsx0(1) |
≤ A1 + A2
.Ewρη%2ββ-1 min{∣X - φ⑼(Xi)||2/∣X∣, ∣x0 - φ⑼(Xi)∣∣} hexp {-C4/3(√m2κ2)223/(8β2)} + / ACl2 、2/3]
( m2K2)
+ EwP (K2√m + K2 Pm3(log(m3) + log(log(m2))) + Cι) ∣x0(2) - X0⑴ ||
85
Published as a conference paper at ICLR 2022
Now notice that under Ec, using the assumption K1 √m33 & C1 + √m3β 1 and Lemma 33 We have
Iml ≥ 11。⑼(Xi)IITx’ — φ ⑼(Xi)Ii
≥ K1 √m3 - (C1 + √m3β 1)
& K1 √m3,
and
∣x0 - Φ(0)(Xi)12 ≤ (C1 + √m3β 1)2,	(272)
which implies:
EWP min{∣x0 — Φ(0)(Xi)∣∣2/IX0II,1X0 — Φ(0)(Xi)∣∣?
≤ EWP 1{EC}∣X0 - φ(0)(Xi)12/1X011 + 1{E}∣X0 - φ(0)(Xi)I
.(C1 + √m3β 1)2/(K1 √m3) + EWP {-^E}Xxf - φ(0)(Xi)Il
.(C1 + √m3β 1)2/(K1 √m3) + h√m3m 1 β 1 + C 1i exp{-C1 /(m3β2)}.
Substituting this above and further applying the result of Lemma 33 and Equation (269) and the
assumption that m2 ≥ m3 log(m2):
A1 + A2 . η%2β-1 [(C1 + √m3β 1)2/(K1 √m3) + √m33m 1 β 1 + C1] exp{-C1 /(8m3β2)}]
× [eχp{-C443(√m2k2)2/3/(8β2)} + (Vk2)2/3]
+ η (K2√m + C 1)( exp{-c2/(32β2)} + -ɪ) %2m3√m3
K1 m1	β1
which completes the proof.
A.20. 1 SETTING β1 AND β2
As we mentioned, to minimize the amount of deviation of the smoothed function compared to the
original one, we prefer to choose β1, β2 as small as possible. (The benefit of such choice, indeed, can
be observed more explicitly in other parts of the proof, e.g. Appendix A.14.) Observing the bound in
Equation (259) and noting that we can easily make the exponential terms orders of magnitude smaller
than the poly terms, it is easy to find the following optimal setting for the smoothing parameters:
β 2 := θ P (( K1 √m 3) -1( √m 2 κ 2) - 2),
β 1 ：= θ p (m 3 √m3 / (κ 1 √m 1)).
Using this setting, we still can make <8 arbitrarily small. Here, we remind the reader that Op only
cares about the non-logarithmic dependencies on the overparameterization, i.e. m1, m2, m3, K1, K2.
86
Published as a conference paper at ICLR 2022
A.21 Basic Tools
In this section, we introduce and prove some lemmas that we use in our analysis as basic tools.
Lemma 29 Suppose V (0) ∈ Rm2×m3 has standard normal entries and a is a random sign vector.
Suppose theta > 1, R < 1 are given thresholds, such that
m2R & m3(log(1/R) + log(m3) + log(log(m2)),
e-2/8 . m3Im2.
Then, for the following quantities:
NR1 (x) = #j ∈ [m] : |Vj(0)x| ≤ R
Nθ2(x) = #j ∈ [m] : |Vj(0)x| ≥ θ,
with high probability we have
sup NR1 (x0) . m2R,
kx0k=1
sup Nθ2 (x0) . m3 (log(m3) + log(log(m2))).
kx0k=1
Proof of Lemma 29
Suppose B1 () is a cover for the Euclidean ball in Rm3 with precision . We know
|B1()| . (1/)m3.
Now for a fixed kxk = 1, we have
P(Wj(0)x ≤ 2R) . R.
Therefore, using Bernstein, with high probability we have
#(j ∈ [m2] : I"⑼x∣ ≤ 2R) . m2R + Pm2R + 1.
Hence, using union bound, we have with high probability
sup #(j ∈ [m]： W"x∣≤ 2R) . m2R + Plog 出ι(C)|Pm2R + log ∣B 1(C)|
x∈B1()
=m2R + Jm2Rm3 log(1 /c) + m3 log(1 /c).
By picking
C . R/(√m3 log(m2m3)),
The assumption implies m2R ≥ m3 log(1/C), which implies
LHS . m2R.
On the other hand, note that with high probability we have
sup	∣vj-i0) I ≤ plog(m2m3).	(273)
j∈[m2],k∈[m3]
Now for kx0k = 1 which is not in the cover, if x is the closest point to it in the cover, i.e. x ∈ B1(C)
and kx - x0 k ≤ C, then for every j ∈ [m2] we have
||Vj(0)x| TU⑼XzII ≤ ||Vj(0)IlIlx — xzk ≤ Pm3log(m2m3)C ≤ R,
by picking a small enough constant. Therefore, for aj that I"j(0)xI ≥ 2R, then
I"j(0)xzI ≥ 2R— R = R.
87
Published as a conference paper at ICLR 2022
Therefore, we get that with high probability, for every kx0 k = 1:
sup #j ∈ [m2] : |Vj(0)x0| ≤ R . m2R.
kx0k=1
For the second part, note that for kxk = 1, by the tail bound for normal vars:
P(W⑼x ≥ θ/2) . e--θ2/8.
Hence, again using Bernstein, we have with high probability
Sup #j ∈ [m2]: ∣V⑼x∣≥ θ/2). m2e~θ228 + Plog 出ι(E)|Pm2e-θ/8 + log ∣B 1(E)|
x∈B1()
.m2e~θ228 + Pm3 log(1 /e) Pm2e-2/8 + m3 log(1 /e).
By picking
E . 1 /(√m3 log(m2m3)),
and using the assumption m2e~θ228 . m3, all terms are dominated by the third term so We can
bound the above as
sup #j ∈ [m2] : |Vj(0)x| ≥ θ/2 . m3(log(m3) + log(log(m2))).
x∈B1()
NoW for kx0k = 1 not in the cover, for the neW E We can Write
||Vj^0^x∣ - ∖vj-0')X0∣∣ ≤ IIV⑼IlllX - x0k ≤ Pm3log(m2m3)E ≤ 1 /2 ≤ θ/2.
Hence, With high prob.
sup # j ∈ [m2] : |Vj(0)x| ≥ θ . m3(log(m3) + log(log(m2))).
kx0k=1
Lemma 30 For x ∈ Rd and W(0) ∈ Rm×d which has standard normal entries (and a is a random
sign vector), we have with high probability:
Sup f (x) := -^=aτσ (W⑼ x) ≤ √d.
kxk=1	m
Proof of Lemma 30
For the first part, We first compute an upper bound on
E Sup
kxk=1
-T= (W σ (W ⑼ x).
m
To do so, We use Dudley’s chaining. Note that the for x1, x2 ∈ Rd, the variable σ(Wj(0)x1) -
σ(Wj(0)x2) is subGaussian With parameter Ix1 - x2 I, so the variable f(x1) - f(x2) is also sub-
Gaussian With parameter Ix1 - x2 I. Hence, by Dudley’s integral:
E SUp √maτσ(W⑼X) ≤ / ʌ/log(N(Bd,E)) . √d.
NoW for a fixed x, note that
1	1	1m
~i=(W σ ( w1χ)-产 a σ (W2 x) ≤ -J=	kW1 j - W2 j k ≤ kW1 - W2 Il^ ∙
m	m	m j=1
Hence, the function f(x) is 1-lipchitz With respect to W and l2 norm, so is the function Sup f (x).
Hence, by Gaussian concentration, Sup f(x) is 1-subGaussian around its mean, so We finally get
With high probability
Sup f (x) . √d + 1 . √d.
88
Published as a conference paper at ICLR 2022
Lemma 31 For
C2/3
R^ := -^C2-K,
(√m 2 κ 2)2 2 3
we have with high probability over the randomness of V (0):
sup #j∈ [ m 2] : I (V(°) + Vj) "∣≤ R*κ 2 IHl) . R*m 2.
x0,V0: kV0k≤C2
Proof of Lemma 31
Note that obviously the condition of Lemma 29 is satisfied with this choice of R = R*.
Therefore, with high probability we have for an arbitrary x0 :
#(|Vj(0)x'l ≤ 2R*κ2∣X1∣) ≤ m2R*.
On the other hand, note that for j ∈ [m2] such that IVj0x0I ≥ Rκ2 Ix0I, we have
IVj0IIx0I ≥ IVj0x0I≥Rκ2Ix0I,
which implies
IVj0 I ≥ Rκ2 .
C2	C2
Therefore, there are at most r^ ∙ Therefore, setting aside m2 R + r^ of j s, for the rest We have
I(Vj(0)+Vj0)x0I ≥ IVj(0)x0I- IVj0x0I ≥ 2Rκ2Ix0I - Rκ2Ix0I =Rκ2Ix0I.
Setting R* as defined above balances the terms m2 R and RK2, which completes the proof.
Lemma 32 If V(0) ∈ Rm2 ×m3 is a matrix with standard normal entries, then with high probability
1	m 2
sup T IV^0^XI . √m + Jm3(log(m3) +log(log(m2))).
kx0 k=1 m2 j=1
Proof of Lemma 32
Let B 1(E) be a cover for the unit Euclidean ball with precision e, for which we have ∣B 1(E) ∣ . (ɪ)m3.
Now for a fixed x ∈ B1 (), note that because Vj(0)x is a standard normal variable, the random
variable ∣Vj^0^χ0∣ — E∣Vj^0^χ0∣ is O(1)-SUbGaUSsian, which means √=- P/(∣Vj^0^χ0∣ — E∣Vj^0^χ0∣)
is also O(1)-subGaussian. Now from the tail of maximum of subGaussian variables:
1 m C	小	，----------- ，--------------
sup T E(IV⑼x∣- E∣V⑼x∣) . √log(∣B 1(E) ∣) = √m3 log(1 /e).
x∈B1() m2 j=1	j	j
On the other hand, note that E∣Vj(0)x0∣) = O(1), which implies w.h.p:
m	m 2	___________
sup X ∣ V⑼x∣ . √m + Pm3 log(1 /).
x∈B1() m2 j=1 j
Moreover, note that again by the tail of subGaussian variables, we have w.h.p:
r maxr JV(O) ∣ . √log(m2m3),
j∈[m2],k∈[m3]	,
which implies with high prob for every j ∈ [m2]:
IIV⑼ Il . Pm3 log(m2m3).
89
Published as a conference paper at ICLR 2022
Now by picking
e ：二 (Pm3 log( m2 m3))
we get with high probability
1	m 2
sup T WN . √m2 + Jm3(log(m3) +log(log(m2))).
x∈B1() m2j=1j
(274)
On the other hand, for an arbitrary x0 with kx0 k = 1, if x ∈ B1 () is the representative of x0, we
have by definition kx0 - xk ≤ , which combined with (A.21) implies
. Pm3 log(m2m3) (Pm3 log(m2m3))	≤ 1.
Therefore
m2
X |Vj(0)x0 | -
j=1
≤ √m 2 .
(275)
Combining Equations (274) and (275), we conclude the result.
90
Published as a conference paper at ICLR 2022
A.21.1 DEFINING THE RARE EVENTS Ej
Lemma 33 For x0(2) defined in Equation (258) we have
EW∕∣"⑵—φ(0)(Xi)∣∣, EW∕φ(2)(Xi)Il ≤ C1 + √m3βι,
EW ρ kφ(2) (xi)k2 . C12 + m3 β12.
Moreover, for the events
Ej = {∖Wj Xi∖ ≥ C1 /√m3m 1}, E = ∪jEj,
we have under Ec:
IX0(2) - φ(0) (Xi)I, Iφ(2) (Xi)I . C1.
Furthermore, E happens rarely:
P(E) . m 1 exp{—C2/(8m3β2)},
EWP {^}}φ(2(2)(Xi)Il ≤ h√m3m 1 β 1 + g] exp{-C2 /(8m3β2)}.
EWP l{E}∣∣x0(2) - φ(0)(Xi)I ≤ h√m3m 1 β 1 + CIieXP{-C2/(8m3β2)}.
Finally, we have the following almost surely bound:
小	一二 1^
∣∣φ (xi) Il ≤ C1 + √m3 X √——∖Wj xi∖.
m1 j
j=1	1
Proof of Lemma 33
We start by writing
∖X0k(2)
- √m Wkσ (W ⑼
m1	1
+ (1 - η)W0 + √ηW*)Xi∖ ≤ E ^=W∖WpXi∖.
j=1 m1 j
(276)
∙-v
Now notice that by Lemma 1, we know for every j ∈/ P :
∖Wjji)Xi∖ ≥ c2/√m2,
∖(1 - η)W00Xi∖ ≤ C2/(2√m 1).
In addition, by Equations in (104) from Lemma 5, for every j ∈ [m1]:
IWj* I ≤ %1
m 3
m1
(277)
(278)
so by picking
we obtain
η ≤ c2/(4%vm3)
η\w*χi\ ≤ 4⅛;.
(279)
Combining this with Equations in (278), We see that the signs of (W(ID) + (1 一 η) W00 + √ηW*)Xi
and W(" Xi are the same for j ∈ P.
Moreover, the matrix (1 一 η) W0 + √ηW* satisfies
I(1 - η)W0 + √ηW*∣∣ ≤ (1 - η)C1 + √√2ζ2 ≤ C1,
by picking √η ≤ C1 /√Z2. Hence, the conditions of Lemma 6 are satisfied and we get:
II*W，Q(W⑼ + (1 - η)W0 + √ηW*)Xi- φ(0)(Xi)∣∣ ≤ C1.
(280)
91
Published as a conference paper at ICLR 2022
Combining Equations (276) and (280):
m1	1
kx0(2) - φ⑼(Xi)k ≤ Cι + √m3 E ^=W∣WpXi∣.	(281)
m1	j
j=1	1
In exactly similar fashion, one can derive
m1
kφ (2)( Xi ) Il ≤ C1 + √m 3 ^X ~∕^^ρxii∖∙	(282)
M Vm 1	j
Now first of all, note
m1	1
EWP X /— ∖Wj xi∖ ≤ β 1,
j =1 Vm 1
which proves the first part of the claims. For the second part, note that by the Gaussian tail bound
P(∖WpXi∖ ≥ C1 /√mm) . exp{-C2/(8m3β1)}.
Therefore,
P(E) ≤ XP(Ej) ≤ m 1 exp{-C2/(8m3β2)}.
j
Moreover
m1
EWP 1{Ε} X
j=1
∖jρXi ∖≤ E W P √—— XX
l{Εj2}∖WpXi∖ + √=- XEwPl{Εj}∖WpXi∖,
m1 j2 j6=j2	m1 j
=h √m~ ∑∑ E W P ∖WjpXi∖ + √m X EW P [ ∖W7ρXi∖∣ Ej]]P( Ej)
m1 j2 j6=j2	m1 j
.hm 1 β 1 + C1 /√m3] exp{-C2/(8m3β2)}.
Plugging this into (281) finishes the proof. Also, under Ec by Equation (281) we have
IX0(2) - φ(0) (Xi)I, Iφ(2) (Xi)I . C1.
Finally, exploiting Equation (282):
EWP Iφ(2)(Xi)I2
.C2 + m3ɪEWP(X ∖wpχi∖)2 ≤ C2 + m3EWP X ∖wpXi∖2
m1	j	j
≤ C12 + m3β12 .
92
Published as a conference paper at ICLR 2022
A.21.2 BOUNDING THE VALUE OF f0
The following Lemma provides a reasonable bound on the value of the smoothed function.
Lemma 34 We have the following general bound on the values of the smoothed function: With high
probability over the initialization, for kW0k ≤ C1, kV0k ≤ C2 and ∀i ∈ [n] (having small enough
choices of β1, β2 described in Appendix A.20):
IfW 0,V 0 ( Xi ) | . ( K 2 √m3 + β 2)√m 3 K1 + C1 + √m 3 β l) + C2( C1 + √m3 β 1),
which is O(C1C2) for large enough overparameterization as described in AppendixA.3. Moreover,
we have the following almost surely bound (with respect to the randomness of Wρ and V ρ):
|fW 0+W ρ,V 0+V ρ (xi)|
.(K 2 √m3 + IY ρkF ) (√m3 K1 + C1 + √m3 ( √m ^X l^pxi I)) + C2( C1 + √m3 ( ~^~ ^X l^∕xi I)) •
1j	1j
Notably, with slightly higher overparameterization, the high probability bound in (34) holds even if
we take supremum over x.
Proof of Lemma 34
Using Lemmas 30 and 33 and using the fact that φ(0) (xi) is orthogonal to the rows of Y0
(recall x0i = φ(0) (xi) + φ(2) (xi)):
IfW 0+W ρ,V 0+V ρ (xi)I
≤ ∣- aσ (V(0) xi) + /— X |(Vj + Vj0)xil
m2	m2 j j
≤ (K 2 √m3) Ilxik + √m X lvfxil + C2kφ(2)( xi ) k
工 ( K 2 √m3 + kV PkF ) kxik+ C2 kφ ⑵(xi ) k
.(K 2 √m3 + kV ρkF ) (√m3 K1	+ C1 +	√m3 (	√—	^X	l^pxiD)	+	C2( C1	+ √m3 (	√—	^X1^/x/)) -
m1 j	m1 j
(283)
Note that above, if we apply the stronger worst-case norm bound of the first layer’s output presented
in Lemma 46, we would get supx,kxk=1 IfW0+Wρ,V0+Vρ (x)I is bounded by the RHS, which in turn
proves a stronger uniform bound on f0 .
Similarly, this time by taking expectation with respect to Wρ and Vρ:
IfW0 0,V0 (xi)I = IEWρ,Vρ fW0,V0 (xi)I
≤ EWρ,Vρ IfW0,V0 (xi)I
=E W ρ( K 2 √m 3 + β 2) kxik + C2 kxi — φ (0)( xi ) k
.(k 2 √m3 + β 2) √m 3 k 1 + C1 + √m 3 β 1) + C2( C1 + √m3 β 1) ∙
Corollary 8.1 If we set C1 = C2 = 0 above, we get
lf0,0 ( xi ) 1 ≤ ( K 2 √m3 + β 2)(√m3 K1 + √m33 β 1),
the point being these terms go to zero by an order of O((√m2K2) —2). Therefore, taking
(√m2κ2厂3 << B, we make sure that ∣f0,01 < B, so by the 1 smoothness of ' and B bound-
edness of the labels we get `(f00,0 (xi), yi) < 4B2.
93
Published as a conference paper at ICLR 2022
A.21.3 Bounding the difference between Original and Smoothed Functions
The following Lemma bounds the difference between the smoothed function and original function
of the network.
Lemma 35 Bound on the smoothing change under the assumption m2 ≥ m3 log(m2): with high
probability over the initialization, for any (W0, V 0) with kW0k ≤ C1, kV 0k ≤ C2:
|fW0,V0 (xi) - fW0 0,V0(xi)|
≤ β2(K1 √m33 + C1 + √m33βι) + (C2 + K2√m2)√m33β 1.
Proof of Lemma 35
We write
|fW0,V0(xi) - fW0 0,V0 (xi)| = |fW 0,V 0 (xi) - EWρ,VρfW0+Wρ,V0+Vρ (xi)|
= EWρ,V ρ fW0,V0(xi) - fW0+Wρ,V0+Vρ(xi)
≤ EWρ,Vρ fW0,V0(xi) - fW0+Wρ,V0+Vρ (xi).
In the following, σ means we apply Relu activation to the vector in front of it (entrywise):
LHS ≤ EWP VPl -ɪaτσ(V⑼ + V0 + VP)-1^-Wsσ(W⑼ + W0 + WP)xi
m2	m1
-	√m aτσ (V⑼ + V0) √m Ws σ (W⑼ + W0 + WP)XJ
+EWP1VPI √maτσ(V⑼ + V0) √m Wsσ(W⑼ + W0 + WP)Xi
-	√maτσ(V⑼ + V0)√m~Wsσ(W⑼ + W0)Xi1
Now for the first term above, using the previous notation of X0i representing the output of the first
layer and using Lemma 33:
EWP VPI -ɪaτσ(V⑼ + V0 + VP)-1^-Wsσ(W⑼ + W0 + WP)xi
m2	m1
-	√maτσ(V⑼ + V0) √mWsσ(W⑼ + W0 + WP)xj
≤ EWp'vP短 XX 1V^xil
(284)
. β2EW P kX0ik
≤ β2EWp,vP (||。⑼(Xi) Il + M⑵(Xi) II)
.β2(K1 √m3 + CI + √m3β 1).
For the second term, by starting off with a simple triangle inequality:
≤ C2EW PIxi- Φ (0)( Xi) - Φ ⑵ P (Xi) Il + E W P √m XXI V(0)( x[- Φ (0)( Xi) - Φ ⑵ P (Xi ))|.
j=1
94
Published as a conference paper at ICLR 2022
Now using Lemma 32:
.(C2 + K2√m2)EWP kxi - φ(0)(Xi) - φ(2)p(Xi) k
.(C2 + K2√m2) √m3EWP〉： √m I^PXil
j1
.(C2 + K2√m ) √m3β 1.
(285)
Combining Equations (284) and (285) we conclude the proof.
95
Published as a conference paper at ICLR 2022
B Appendix
Contents
B.1	Smoothness coefficients ................................................ 96
B.1.1	Computing the Lipschitz Coefficients of fW0 0,V0 ................. 98
B.2	Representation Lemmas ................................................. 104
B.2.1	Representation Toolbox .......................................... 104
B.2.2	Some Linear Algebra ............................................. 105
B.2.3	Bound on the norm of xi's........................................ 106
B.3	Coupling for VW, VV ................................................... 108
B.4	Handling the Injected Noise by PSGD ................................... 113
B.4.1	Bounding the Norm of the first Layer’s Output in the Worst Case . 114
B.1	Smoothness coefficients
Recall that for a function f ∈ C3 on Rd, We say it is μι Lipschitz, μ2 gradient Lipschitz, and μ3
hessian Lipschitz at point x if for every unit direction V, ∣ 余 f (X + λv) ∣ ≤ μ 1, ∣ 念 f (X + λv) ∣ ≤ μ2,
and ∣~dλf (x + λv)∣ ≤ μ3.
The aim of this section is to bound the Lipschitz coefficients of the loss `(, y) and objective
L(W0, V0) in a bounded domain kW0k ≤ C1, kV0k ≤ C2. The folloWing is our main Theorem
in this regard:
Theorem 9 For given values C1, C2 > 0, in the domain kW0k ≤ C1, kV 0k ≤ C2, for any
label ∣y∣ ≤ B, the loss function `(., y) is O((C1C2 + B2))-Lipschitz (having enough overpa-
rameterization) and 1 gradient-Lipschitz x = fW0 0,V0. Moreover, the loss function L(W 0, V0) is
(O(C1C2) + B)Ψ1 + 2(C1 + C2) Lipschitz, Ψ12 + (O(C1C2) + B)Ψ2 + 4 gradient Lipschitz, and
3Ψ2Ψ1 + (O(C1C2) + B)Ψ3 hessian Lipschitz, where Ψ1, Ψ2, Ψ3 are defined in Lemma 36.
Proof of Lemma 9
As in the proof of Lemma 36, let (WW, VV) be a unit direction, i.e. ∣∣lW∣∣2 + ∣∣V∣∣2 = 1.
Then, using Lemma 34, We knoW that for every i ∈ [n]:	∣fW0 0,V0 (xi)∣ = O(C1C2), so by
1-smoothness of the loss and B-boundedness of the labels, We get that `(., y) is (O(C1C2) + B)
lipshcitz at point fW0 0,V0. The gradient smoothness parameter of the square loss ` is bounded by 1
and its third derivative is zero. NoW using these coefficients, We can easily compute the coefficients
for L as Well by simple differentiation:
∣dλ'(fW0+λWγ0+λV(Xi),yi)∣ =1'(f0,yi)dλf01 ≤ (O(C1C2) + B)ψι,
d	d	d2
∣ dλ2 ' ( fW 0 + λVK Y 0 + λV (Xi ) ,yi ) ∣ =	'( f	,yi )( dλf	) + '( f ,yi ) d^2 f	∣ ≤ ψ1	+ (O(CIC2) +	B )ψ2 ∙
d	... d	d2 d	d3
∣ dλ3 ' ( fW 0 + λVK V 0 + λ∖T (xi )，yi ) ∣ = ∣ ' ( f，yi )( ~^f ) + 3'( f，yi ) dλ2 f dλf + '( f，yi ) dλ3 f
≤ Ψ13 + 3Ψ2Ψ1 + (O(C1C2) + B)Ψ3,
Moreover, note that
-d-kW0 + λWWk2 = 2M0 + λWW,JVi∣	= 2M0,JVi ≤ 2∣W0k = 2C1,
dλ	λ=0
d2
户 kW0 + λW k2 = M M i = 2,
dλ2
d3
灰 M0 + λWk2 = 0,
96
Published as a conference paper at ICLR 2022
∙-v
and similarly for kV 0 + λV k2 . Combining these results finishes the proof.
Above, we used parameters Ψ1, Ψ2, Ψ3, the Lipschitz coefficients of f0 in domain kW0k ≤
C1 , kV 0 k ≤ C2, which we bound in Lemma 36 below.
97
Published as a conference paper at ICLR 2022
B.1.1 COMPUTING THE LIPSCHITZ COEFFICIENTS OF fW0 0,V0
In this section, we bound the Lipschitz coefficients of fW0 0,V0 in the domain kW0k≤C1,kV0k≤C2
by poly (m1 , m2, m3, β1, β2) functions.
Lemma 36 For every point (W0, V0) in the domain kW0k ≤ C1, kV 0k ≤ C2, we have the following
bounds on the Lipschitz coefficients of fW0 0,V0 ((WW, V) is a unit direction with ∣∣lW∣∣2 + ∣∣V∣∣2 = 1):
I dλfW 0+λ]W ,V 0+^V (Xi )1 λ=ol
m2
〜β 2
+m( V
m=^- Vm 3 ( K1 + C1 + β 1) (K 2 Vm 3 + C2)+ Vm Vm 3 (K1 + C1 + β1))
dλ fW 0+^W
京 κ11+Cj +
；V 0+λV (xi )l λ=0
Ψ1,
.(β1 IIWWIl2 + β IIV^k2) Jm3 ((K2 Vm3 + C2)2 + β2)[(K1 + C 1)2 + β2i := ψ2
Idλ3 fW 0+λ↑W ,V 0+λV (xi )λ=0l
.(β2 IlWW k 2 + β2 IIV^ k2)	rm 3 (( k 2 Vm 3 + C2)2 + β 2)h( k 1 + C1 )2 + β2i :=
Ψ3
(286)
Proof of Lemma 36
Let
ρ(W ρ,V ρ) :
__________________1__________________ f IWPI2 _ IV叩｝
(V2∏)m2m3+m 1d(β 1 /Vm 1)m 1d(β2/Vm2)m2m3	2β2/m 1 2β2/m2 ʃ
be the density function of the law of W P and VP which is a joint Gaussian. Then to compute the
derivative and second derivative of the function in the unit direction (W , V), s.t. IW I2F+IVI2F ,
we can write the value of the smoothed function as an integration with density ρ, change variable,
and then take derivatives:
dλfW 0+^w ^v 0+λVr (xi )λ=0
=dλEWρ,VP fW0 + λ^W+WP,V0 + λV+VP (xi)
d
dλ
d
dλ
fW0+λWW+Wp,v0+λV>+Vp (xi)P(Wp, vp)d(W J vp)
fW + ^V + (Xi)P(W + _ (W/ + λW), V + _ (V/ + λV))d(W +, V+)
■"W
But one can easily see that for fixed V0 and V, the set of functions fW+,V+ (xi)ρ(W + _ (W 0 +
λW), V+ _ (V/ +λV)) for a small neighborhood ofλ can simultaneously be upper bounded by an
integrable function. Hence, the Leibnitz rule holds here because of dominated convergence theorem,
98
Published as a conference paper at ICLR 2022
and We can change the order of integration and derivation:
=f fw + V + (X)&ρ(W + - (W0 + XW) W + -(/+ M))d(W + W +)
J	d入
=-/ fw + ,v + (X){(IW, R), ((m| - (W+ - (W0 + λ1W)), (m - (V+ -(『+ XV^))))
P (W + - (W / + λ!W), V + - (V0 + X∖f)) d (W +, V +)
J fW+,V +(xi)D(W, VV), (β2
E W P,V P
E w p,v P
W J m2 VP) ∖ρ(Wρ VP) d(W J VP)
β 2	/
-V P〉) fW0 + λWw+W P,V0 + λV+V P (Xi)( xi )1 λ _0
■\V, V p/)Wo^ 0 + W P,V0 + V P (Xi)( xi ) ∙
(287)
W P) + m
W P) + m
Similarly We can compute the second derivative:
dX2 fW 0+^w ^v 0+^v (Xi )1 λ _o
=ddχ I fW+^v+(Xi )((W,V), ((mm2- )T( W + - (W 0+λiv
ρ(W + - (W0 + XW), V + - (V0 + XV))d(W +, V +)
=/ fW+,V + (Xi)[<(W,V), ((m)T(W+ - (W0 + XW)),
m，
	
m∙
D(W, V), ((β2)TW, (β2)TV))]ρ(W + - (W0 + XW),v + - (V0 + XV))d(W+,v+)
∖	m ι	m 2	/」
=/ fW+^V + (Xi) [<(W,V), (m W P m VP N + D(W, V), (m WW,m V)〉] P (W P VP) d (WP ,v P)
=Ewρ,vρ [(mhW^,WPP+m2hv,vp))-(m∣∣τWkk+m2kV"k2)]fW,+“w+w。,丫，+ʌv+VP(Xi)(Xi)∣
L \ β 1	β 2	β	β β 1	β 2	/」	I 入一0
=Ewp,vP [(今hWW,wPP + 写hy,vp))-(皆IlWII2 + 写IlVIi2)]fw0+wp,v0+VP(Xi)(Xi)•
八β 1	β 2	β	β P1	β 2	/」
(288)
Similarly for the third derivative:
dX3 fw 0+λw，V0+λv (Xi )∣λ _o
dX / fw+^v+(Xi)[《w, V), ((m) -1 (w+ - (w 0+xw)) , (m
	
D(W, V),((P-厂1 W,(β2厂1 V)〉]P(W + - (W0 + XW),v+ - (V0 + xV))d(W+,v+)
m1	m2
/ fw + ,V + (Xi) [D(W,V), ((m)T (W+ - (W0 + XTW
m
-3《他 V), ((m 尸(W+- ( w 0+XW)), (m 尸(V+-(V 0+XV)))X(W, V),(( m 尸 W, (m 尸 V)〉]
P (W + - (W 0 + XW), V + - (V 0 + XV)) d (W+ ,V +)
=Ewp,vp [(mh<w,wPP + my,yPP) - 3(mhw^,WPP + mhy,yPP)(m~kwIl2 + mIIVIl2)
β1	β2	β1	β2	β1	β2
fW 0 + W P ,V 0 + V P (Xi)
99
Published as a conference paper at ICLR 2022
Now for first derivative, exactly similar to the derivation in (283), we can write
≤ EWρ,
≤ EWρ,
E w PY P
〉
W P
W P
+W Pvz+V P ( Xi)( xi )∣
Kz + W P,V0 + V P (Xi) ( xi )∣
V P ∣k 2 √m3 腐⑵ H + 口叫尸区⑵ H + EV P √=2 XX ∖ypχ^ I)
≤ EWP,VP (∣?DIW,wρE∣ + 愕DV,vP)|)卜2√m3Hxi(2)H + HV'*Hxi(2)H + 焉 E IVPxi⑵I)
m，
VP) ∣ EwPHxi(2)H (K2√m3 + HV'Hf) + EWPEVP
β 2 ' I '	/ I	'	7
+ m2 ((E W P Hxi(2) H ∣ D W, WP E ∣)( K 2 √m 3 + HV /Hf ) + EW P
√m21 (v，v p〉∣XX^^0
+ E w P E v P
+ m ((e w P Hxi(2)H ∣ D W, W p E ∣)( K 2 √m 3 + C2)+ E W P ∣ D W, W p E ∣ E V P + E Vjfx∕(^^ I),
P1	mm2 j
where the last line follows because HV/H . C2. But notice that because ∣∣t^Hf ≤ 1, HWHF ≤ 1,
then(V, VP) and(W, WP) are Gaussian variables with variances at most β2/m2 and β1 /m 1.
Hence
EVP ∣〈VWPE ∣ . β2/√m2,	(289)
Ewp ∣ DW^,wPE ∣ . β 1 /√m 1.	(290)
Similarly, using the same derivation as in (33), one can also get the following a.s. bound (over the
randomness of WP):
Hxi⑵H . √m3(K1 + C1 + -L-E WrP^xn∖),	(291)
∖	√m 1	j
therefore
ewp llxi^2^ Il. -m3 (K 1+ c 1+ ewp √m E w^jχi∣ i)	(292)
.-m3(K1 + C1 + β 1).	(293)
Moreover, for every j ∈ [m2]:
Evp ∣ DV, VpE ∣ VjPxxi2) I ≤ "P ∣ (VWPE ∣2 JEZVx2F
=M 率 Hxi(2) H = β Hxi(2) H,
m2	m2	m2
Ewp ∣ DWW,WρE ∣ Wrjxii∖ ≤ m1 .	(294)
Similarly, using Equation (291) we bound
E w P Hxi(2) H ∣ D ɪw ,w P E ∣. -m3 (e w P ∣ D W, wP E∣( K1 + c 1) + E w P √m ElD ɪw ,w P E ∣ wrjχn
(295)
100
Published as a conference paper at ICLR 2022
Now applying these bounds (290), (293), (294), and (295) to (289) and using the fact that
如P √=- XXI吟X⑵∣≤ β 2㈣⑵n，
we get
LHS .詈
+m
^m	√m3 (K1 + C1 + β 1) CK2 √m3 + C2)+ EWP √m	llx：⑵ Il)
κ ι + C1) +
κ 2 √m3 + C2) + E W P
I (ɪ^ ,w PI β 2 同⑵ Ii)
K1 + C1 + βι))
Vm 2
〜离
+m
β2 √m3(K1 + °1+βW2 √m 3 + 02)+ √⅛ √m 3C
κ 1 + C1) +
κ2√m3 + C2) + β2√m3 C m~ (κ 1 + C1) +
To make it easier for handling the second and third derivatives, we first bound the expectations of
fWo+wρ V0+Vp(沙)(Xi) which enables us to use Cauchy-schwartz. Again using similar derivation
as in (283) and Equation (291):
E W JV P fw0 + W PjV0+V P (Xi)( Xi )
≤ Ewp,vP (K2√m3llχi^2^Il+ C2llχi^2^Il+ √m E |V7xi⑵i)
.(K 2 √m3 + C2)2E W PllXi ⑵ Il2 + EW P EV P^- (E IVPXi ⑵ 1)
2 j
.(K 2 √m3+c2)2EWP IlXi ⑵ Ii2+EWP m^ ^X EVjP ιVjχ(y) i 2+EWP m^	^X	EVP	iVji Xii22	田嚓	ivj2 Xii2' i
j	j 1= j 2
.((K2√m3+c2)2+β外 EWpm3 (K 1 + c 1 + √m1
.(K 2 √m3 + c2)2EW PllXi ⑵ Il2 + EW P — IlXi ⑵ Il2 + EW P β 2 IlXi ⑵ Il2 m'm 2 ”
m2	m2
.((K 2 √m 3 + C2 )2 + β 2)e W P IlXi ⑵ Il2
E ∣WρXi∣ )2
j
((K 2 √m 3 + C2 )2 + β 2) m 3 [( K1 + CI)2 + m~ ^X E W P |Wj?Xi| 2 + m~ ^X E WPi lWρι XiIE WjIIWj2 x^∣ ]
j	j i =j 2
((K2√m3 + C2)2 + β2) m3 h(K1 + C1)2 + 邑 + β2 m(m - 1) ]
m1	m2
m3 ((K2√m3 + C2)2 + β2)[(K1 + CI)2 + βl] ∙
(296)
101
Published as a conference paper at ICLR 2022
Now for the second derivative, we can proceed by applying Cauchy-Swartz:
dλ2 fW0+λTVY0 + XV(Xi)|
λ=0
≤ EWP,VP |(-1 hWW,wρi + β2(忆 Vρi) — (β IWk2 + β2 IIV^k2) | | fW0+WP,V0+Vρ(Xi)(χi) |
22
≤ EWPVP| 哥 W,WP2 - 11IWI2 + 备①,VPY- mIVk2 + ⅛/W,WPNzP|
I β1	β2	β2	β2	β1 β2	1
fW0 + W P,V0 + V P (Xi)( xi ) |
≤ ʌ IEWP UPI 砧〈W Wρ∖2 -m kWk2 + 喧〈V Vρ∖2 -普 ∣Vk2 + 2m ιm2(W WP∖b VP∖ |2
≤VEWPYPIβ4w,w i β2kWk + β4(V,V i β2kVk + β2β2 w,w i〈V,V i |
yEWp,VP|fW0+WpV + Vp(xi)(Xi)| ∙
Now note that the cross terms have expectation zero, so we get
iZEWZmWWT-mWkTɪmhWi^^mk^T^^pEWZWW^W^
V ∖ β4	β2 β ββ2	β2 β β 1 β2
yEWP,VP|fW0 + WP,V0 + VP(xi)(Xi)|
.\ 景 kWW k 4 + ^d2 kVr k 4 + ^ -®2、2 2 kWW k 2 kVr k 2∖ /EWP,VP|fW0+WPV+VP(xi)(Xi)r
β1	β2	β1 β2
β2 IIW7 k 2 + β kV k 2) yEWPVP|fW0+WPV+VP(xi)(Xi)| ∙
Now applying Cauchy-shwartz and Equation (296) to above and combining it with Equations (288):
dλ2 fW0+IW V0+IV (Xi) | X=0
mkWk2
β1
m3 ((K2√m3 + C2)2+ β2)卜K1 + C1)2 + β2].
Similarly for the third derivative:
| dλ3 fW0+λiW v0+XV (Xi) | λ=0 |
=EWp,vP |(mhWW,Wρi + 货hv,vPi) -3(m1 hw,Wρi + m(v,vρi)(mIIW7Il2 + mI∣vλIl2)|
β1	β2	β1	β2	β1	β2
| fW0 + W P,V0 + V P ( Xi ) |
≤ s/EW^z^h(mWWiɪmVVρiy^-3(m1WWPi+mhVVPiy(mk^kɪɪmk^ɪɪ
β1	β2	β1	β2	β1	β2
yEWP,VP|fW0+WPV0+VP(Xi)| .
But note that
EWPVp [(与 W，WPi + mhv，vPi) - 3(姿 W，WPP + 罢hv,vP
β1	β2	β1	β2
≤ 2EWp,vP (詈(TV, Wρi + 贷(V,Vρi)6
β1	β2
+18(m IWk2 + 空 kV k2 )2e W p,v P (m W, W ρi + 空(V, V ρi )2
β1	β2	β1	β2
(297)
Mm W k2+爸 kv k 2)『
102
Published as a conference paper at ICLR 2022
∙-v
∙-v
Now note that m (I4λ, WP) + m2 hVr, VP) is a normal variable with variance
Therefore, by the bound on the moments of normal random variables:
LHS.( m Wn2+mIV i 2)3∙
Plugging this into Equation (297) and also using Equation (296):
∙∙w	∙∙w
m IlW Il2 + mHY K
∙-v
∙-v
(298)
I dλ3 fW 0+^W V 0+入V (Q)L=o I
.(-1'IIwwIl2 + 击IIV^Il2)	Jm3 ((K2√m3 + C2)2 + β2)[(Kι + CI)2 + β2i ∙
β1	β2
103
Published as a conference paper at ICLR 2022
B.2 Representation Lemmas
In this section, we prove lemmas mostly related to the representation power of the network, which
we mainly use in Appendix A.12.
B.2. 1 Representation Toolbox
Lemma 37 Recall the definitions of Wk+, and Xi,k from Equations (64), and (60). Forall k,i ∈ [n]:
∣xi,k - trace(W+ Zk)I ≤ pn/(m 1 λ0)IIVk∣∣ff∞.
Proof of Lemma 37
nn
trace(Wk+Zki) = trace(Zki	Vk,jZkj)=	Vk,i0 hZki, Zki0 i.	(299)
j=1	i0=1
But note
m1
hZi,Zk i = 1 /m 1 X Wk,j2l{Wj⑼TXi}l{wj00Txi}hxi,Xii
j=1
=hxi',xii X 1(Wj(o)tXi}l{Wj(0)tXiO}.
Now note that hXi>, xii ≤ 1, and 1{W⑼TXi}l{wj^°tXi>} is a Bernoulli with
E 1{W(" t Xi}l{Wij^0 t XiO} = 1 / 4 + arcsin( hx,yi) / 2 π.
Therefore, by Hoeffding inequality we get
..:	T.	_ _ .	.	.	__
IhZk ,Zki- H∞01 = O (1 /√m1).
Hence, because obviously ∣H∞ ∣2 ≤ 1, we get
nn	n
trace(W+ Zk) = E Vkil,H∞ + O(1 /√m 1) E Vky = Xi,k + O(1 /√m 1) E Vky,
iO=1	iO=1	iO=1
which implies
Itrace(W+Zk) - Xi,k I ≤ O(1 /√m 1)√n^∣Vk Il2
.O (√n/ (√m 1 √λ 0)) ∣∣Vk ∣∣h∞
≤ n/(/(m 1 λ0)∣∣Vk∣∣h∞.
Lemma 38 (Bounding the rows norm) For every 1 ≤ j ≤ m1, we have
∣∣Wj+ Il ≤ √nm3/(Vm1λ0)
Xk
∣Vk ∣2H
Furthermore, for every k ∈ [m3], we have
IIWk +∣≤ K]^λm1 IlVk∣H8.
(300)
For the ease of notation, because here we want to work with row sub indices of the matrix Wk+ , we
refer to it by Wk+ . Proof of Lemma 38
104
Published as a conference paper at ICLR 2022
For a fixed 1 ≤ j ≤ m1 we have with high probability over the randomness of the sign
matrix W s :
m3	m3	n
肛+ H = HEWk+n = HEWsjI/√mEVk,lχlι{Wj(0)Xi ≥0}∣∣
k=1	k=1	i=1
m3 n
≤ √m3 / √m、	H	Vk,iXi 1{W⑼Xi ≥ 0}H2
k=1 i=1
≤ √m3 /√mι yXχ(X ∖Vk,iι)
≤ √m3 / √m ^nXHVkH
≤ √m3n/(Pm 1 λ0) ^/X HVk HH∞. ■
Furthermore, for every k ∈ [m3], we have
HWk+H ≤ 1 /√mX ∣Vk,i∣ ≤ (√n∕√m)HVkH2 ≤ (√n/P>0m1)HVkH∏∞.
i
Lemma 39 With high probability over the initialization, we have
HWk + H F ≤ (1 ± Otnl60√m))) HVk HH∞ ■
Proof of Lemma 39
Recall from the definition of Wk+ in Equation (64):
n	nn
HWk+H2F= H	Vk,iZkiH2F=	Vk,iVk,i0hZki,Zki0i
i=1	i=1 i0=1
nn
=	Vk,iVk,io (H∞o ± o(1 /√m 1)). VT H ∞ Vk ± o(( HVk H1)2/√m)
i=1 i0=1
= HVkHH∞ ± HVkHH∞o(n/(λ。√m)) = (1 ± o(n/(λ。√m)))HVkHH
B.2.2 Some Linear Algebra
Lemma 40 For n ≤ s, let r1, ■ ■ ■ , rn be s-dimensional vectors that are approximately normalized
and orthogonal to one another, i.e. given some δ > 0, for every 1 ≤ i 6= j ≤ n:
-δ 工hri, rji ≤ δ, HriH2 ≤ 1 + δ.
Then, for any vector v we have
n
Xhv,rii2 ≤ (1 + δ + n(n - 1)δ(1 + δ)2)∣∣vH2.
i=1
Proof of Lemma 40
Define
n
V1 = y^lhv,riiri, V2 = V - v 1 .
i=1
105
Published as a conference paper at ICLR 2022
First, note that
nn
Xhv, rii2 ≤ (1 + δ) Xhv, rii2krik2
i=1	i=1
= (1 +δ)k Xhv,riirik2 - 2(1 + δ)	X hv, riihv, riihri, rji
i	1≤i6=j≤n
= (1 + δ)kv1k2 - 2(1 +δ)	X hv, riihv, riihri, rji.
1≤i6=j ≤n
Next, we write
n	nn
hv1,v -v1i = hv,	hv, riirii - h	hv,riiri,	hv, riirii
i=1	i=1	i=1
=	i hv, rii2 - i hv, rii2 - 2 i6=j hv, riihv, rjihri, rji
=	-2	hv, riihv, rjihri, rji.
i6=j
Therefore
n
hv,rii2≤(1+δ)(kv1k2+kv-v1k2)-2(1+δ)	hv, riihv, riihri, rj i.
i=1	1≤i6=j≤n
≤ (1 + δ)(kv1k2 + kv - v1k2 + 2hv1,v - v1i)
+ 2(1 + δ) X hv, riihv, riihri, rji
1≤i6=j ≤n
≤ (1+δ)kvk2+2(1+δ)	X	kvk2krikkrjkδ
1≤i6=j≤n
≤(1+δ)kvk2+2(1+δ)	X	kvk2(1 + δ)δ
1≤i6=j≤n
= (1 + δ)kvk2 + n(n - 1)δ(1 + δ)2kvk2
= (1+δ+n(n-1)δ(1+δ)2)kvk2,
which completes the proof.
In the following lemma, We state a trivial bound on the norm of Xi based on Q.
B.2.3 BOUND ON THE NORM OF XjS
Lemma 41 For every i ∈ [n], we have
Ixill≤ jχ MIlH∞ = P^∙
Proof of Lemma 41
By definition:
x T = ( h∞ V )二
Now consider the Cholskey factorization H∞ = KKT. Because of the assumption IXi I = 1, we
know that the diagonal of H∞ is all 1/2. Hence, for the ith row of K we have IKiI = 1/2. Now
by Cauchy-Swartz, we have
Xi21	= h	Vk,iKi,	Ki1 i2	≤ I	Vk,iKiI2 IKi1	I2 = 1/2IVk	I2H∞ .
i
i
Summing over i and noting Equation (59) completes the proof.
106
Published as a conference paper at ICLR 2022
Lemma 42 In the context of Lemma 10, for Q≤ 2 nB2, one can substitute f * by f such that
B2
Rn ( f* ) ≤ 2 Rn ( f * ) + n
f*Ta- 1 f* ≤ f*Ta- 1 f*
and furthermore, f* is in the subspace of eigenvectors of A with eigenvaue larger than Ω( n2).
Moreover, the constant 2 is arbitrary and can be changed to any constant more than one, with the
cost of an additional constant behind the second term.
Proof of Lemma 42
For an arbitrary i ∈ [ n ] and some given vector f* (We will specify later), We define
S = If* - f*l,
and suppose the slope of `(., yi) at point fi* is equal to c. Then, using the convexity, the fact that
'(yi, yi)=0, and the 1-smoothness of '(.,yi), it is not hard to see the following Poincare inequality
betWeen the value and derivative of `(., yi ) at point fi* :
C ≤ ,2'(fi,yi) := 2'.	(301)
where from now on, for brevity, we refer to `(fi* , yi ) by `. Also, from the definition of S and again
using 1 smoothness property, it is easy to see that
'(f"yi) ≤ (C + S)S + '(fi,yi) = (C + S州 + '	(302)
Plugging Equation (301) into (302) and using AM-GM inequality:
' (f*,yi) ≤ S2 + cS + ' ≤ S2 + √2S + '
≤ S2 + ` + S2/2 + `
≤ 2' + 3 S2 / 2.
Summing above for i ∈ [n], we obtain
Rn(f*) ≤ 2Rn(f*) + 3kf* - f*k2/2.	(303)
Now we write an eigendecomposition for A as A = Pin=1 λiuiuiT for orthonormal basis {ui}, and
let f* = Pi γiui be the representation of f* in this basis. Then, from our assumption, for arbitrary
ω>0
X γi2λi-1 = f*TA-1f* ≤ 4nB2,
i
which implies
ω-1 X γi2 ≤ 4nB2 ,
i: λi ≤ω
or equivalently
X γi2 ≤ 4nB2ω,	(304)
i: λi ≤ω
where notice that Pi: λi≤ω γi2 is the squared norm of the projection off* onto the directions whose
eigenvalue is at most ω. Now taking ω = ι2n2 and defining f* by keeping only the directions in the
expansion of f * in the eigenbasis of A, for which λi > ω , completes the proof.
107
Published as a conference paper at ICLR 2022
B.3 Coupling for 弋W,弋V
In general, because the gaussian smoothing matrices (Wρ, V ρ) can become unbounded, the gradi-
ent estimates (qW, q v) = Vw,v ' (fw0+Wρ,V0+Vρ (xi), yi) also become unbounded. However, in
analyzing the stochastic behavior of SGD and showing that it can escape saddle points, it is conve-
nient to assume the gradient’s noise vector is almost surely bounded. The goal of this section is to
introduce a coupling between (Wρ, V ρ) and another random variable that is a.s. bounded polynomi-
ally in other parameters. As that the coupled random variables take different values is exponentially
small while the number of iterations in our algorithm is only polynomially large, without any con-
cern we instead work with this new random varaible, and with an overload of notation we also denote
itby(Wρ,Vρ).
Lemma 43 For an arbitrary parameter χ >> 1, On any pair for (W0, V0) with kW0k ≤ C1,
IlV Il ≤ C2 ,there exist a mean zero random vector A with respect to the randomness ofthe uniformly
picked data point (xi, yi) and the smoothing matrices Wρ,1, Wρ,2, Vρ,1, and Vρ,2 which define
ʌ
Vw -v 0 (meaning it is a function of those variables), such that with probability at least
1 - 2 exp{-(χ2 - 1)dm1/4} - 2 exp{-(χ2 - 1)m3m2/4} := 1 - δ1,
we have
VW0,v0 = Vw0,v0L(W0, V0) + A,	(305)
and finally A is a.s. polynomially bounded, i.e. almost Surely we have
kAk ≤ poly(mι,m2, m3,C1,C2, B, X)∙
Proof of Lemma 43
Remember that x0i was the output of the first layer (by considering the smoothing
Wρ). Now with high probability over the initialization,
IVW0fW0+Wρ,v0+vρ (xi)IF
llvχ< fw0+w p,v0+v ρ( Xi) T ：W；，k
llvχ< fw0+w ρ,v0+v ρ( Xi) Ilk ^dW k
1	1 m3
k √— a DV0+vP(V ⑼+v 0+v P) kk√m∑diag(Wks)Dw 0 +w ρ,xi XiT kF
√m 2	√m 1 k=1ι
≤
≤
≤
(kV⑼ kF + kV0kF + kVPkF)(√m E kdiag(WS)Dw0+wρχixTkF
1k
(K 2 √m 2 m 3 + C2 + kV PkF ) ( √m X k diag(Ws ) DW0+W P,xi xT kF )
(K 2 √m 2 m 3 + C2 + kV PkF ).
on the other hand, using the final bound in Lemma 33:
kVv0fw0+wρ,v0+Vρ(Xi)kF = k ,— diag(a)DV0+VPXFkF ≤ kxik
m2	i	i
≤ K1 √m3 + C1 + √m3 ^X ~J^^W^p χii∖
j √m 1 j
≤ κ 1 √m3 + C1 + √m3kwpkF.
matrix
(306)
(307)
*	∙∙w
Denoting '(fw0+wρ,ι,v0+vρ,ι ,yi)Vw0,v0'(fw0+wρ,2,v0+vρ,2 (Xi),y%) by Vw0,v0, then Combin-
ing Equations (306) and (307) and using the 1 gradient lipschitzness property of the square loss,
∙-v
kVw0,v0kF
d (' (f,y))
df
∖	kVw0fw0+wP,2,v0+vP,2 (Xi)k2F + kVv0 fw0+wP,2,v0+vP,2 (Xi)k2F
≤ (fWv0+W P1 ,V0 + V P1 ( Xi ) ∖ + ∖B∖^ k^ 1 √m 3 + C1 + √m3 kw p, 2 kF + K 2 √m 2 m 3 + C2 + kV P 2 kF
(308)
108
PUbHShed as a COnferenCe PaPer at ICLR 2022
-∙na=y" applying CaUChy—Swartz to the SeCOnd as bound5'Lemma 34 We have
一 AV、+VKW+v-
≤ (K2Λ∕^3 + =yF) (Λ∕^3κl + cl + Vm3 = W) ÷ C2 (Cl + <m3 = w一 ).
COmbillg this With (308y
=VVK」-一 F
≤ F +?务3 二 V?-一 F) (≠⅞κl +p+ √7⅛=w?-一) + C2(P+ √^i=wj)一
× 亍17^3 + Cl + √^W=F + 3√m2m3 + C2 + = v'2=f) •
TherefOrusing the LiPSChitZ boundTheOrem B
= CWVi VWV、Fw义(jV、(皂 W一F
≤ =CW VF + 一一Fy〜MVVWy⅛'+vκw+v皂 7左一一 F
≤ F 二3务3 + =V?一 F) (≠⅞κl ++ √7⅛=w?一) + C2(+ √7⅛=w?一)一
×√⅝ + c1 + √^3=wm+k2√^⅛+c2 + =w2+(O(C1C2)+E)W1∙
(309)
NI We define the f-lling events
m 二工=WP=F≥ χv⅜l < =WJF ≥ χv⅜I 丁
m2 一 = 士一 Vp=F ≥ x√^‰2 < =yp=FIvx√^‰2 丁
Where recall We assume X Vv L τhe-aas We kill the Variable =W-示 has mean dβ^ and is
SUbeXPOIlential With ParameterS (dβ^τn 广-/ml). HeIIC 尸 by a UlIion bound and Bemste(NOte
that W? JWp)2 arm independents
)
≤ 2p( = WP = F ≥ XF √⅛
U 2p( = w≈示 ≥ χ2 玄 N)
≤ 4max (eXPT(X2 — 1)2注⅛∖(4⅛f∖ml)丁 expτ(x2 — 1)女茬(4m∕ml 二)
=4max (eXP-{1X2 — 1)2JmI/4 b 2exp∙{lx2 — lκml∕4)) ≤ 2exp∙{l(x2 — lκml∕4)∙
Similarly for 一一 V一九
P(E2) = P( = W = F ≥ x∕¾√⅝5) ≤ 4expτ(x2 — l33m2∕4 丁
MOreoVeLbeCaUSe Ofthe SUbeXPOllential tails Of=W示 and =V一for each OfWPlorW
WJOrW
E(=w≈宜 ml) m X 自r E(=w≈亚 ml) m X2⅛"
E(=y≈Ξm2) m x√⅝∕¾∙ E(=y≈史 m2) m X2m⅛∙
Nl DefilIilIg 口 uu 口2 and COmbillg the above equationE
E(=wp=lmy) ≤ E=WP=(Im3 + lm2y) H E(=w≡ml)p(ml) + E(=w=p(m2)
m X√⅜12expτ(x2 — l)dml∕4y + √⅜-2exp T(X2 — l33m2∕4y
=2√⅛l(xexpτ(x2 — lκml∕4y +expτ(x2 — l33m2∕4?
109
Published as a conference paper at ICLR 2022
and
EllW321{ΞJ = E(∣∣W321 Ξ1)P(Ξ1)
≤ 2dβ12χ2 exp{-(χ2 - 1)dm1/4}.
Similarly
E(∣γρ∣∣l{Ξ}) ≤ 2√m3β2(exp{-(X2 - 1)dm∖/4} + Xexp{-(X2 - 1)m3m2/4}),
and
E(∣∣Vρ∣∣21{Ξ2}) ≤ 2m3β2χ2exp{-(X2 - 1)m3m2/4}.
Applying these equations to (309) with Cauchy-Schwartz to get the upper bounds
E 1{Ξι}[∣WPjIlkWP2Il ≤ E 1{ΞJ∣∣Wρ∣∣2 and (EWPllWρ∣∣)2 ≤ EWPllWρ∣∣2 (forterms withonly
one Wρ,i or V ρ,i, we simply write them as Wρ and V ρ):
∙-v
E W P Y PIlVW"V' - ^W -V * E( xi,yi)〜Z ' ( fW -V ' ( Xi ) *i ) ∣∣F {{ ξ }
≤ E W P,V P, (Xi,yi) h B + ( K 2 √m 3 + kVρ, 1 l∣F ) (√m 3 K1 + C1 + √m3 IlW Pk) + C2( C1 + √m3 IlW P ∣∣)]
× (K ι √mf3 + C ι + √m 31W pIf + κ 2 √m 2 m 3 + C2 + IIV pIf ) + α (O (C1C2) + B )Ψ1
=h B + (κ 2 √m 3)( √m 3 κ 1 + C1) + C1C2 i
× (K1 √m33 + C1 + K2√m2m3 + C2 + `m3EWP 1{Ξ}∣Wp∣f + EVP 1{Ξ}∣Vρ∣F)
+ (B + (√m33 K1 + C1) √m33 + (K 2 m 3 + C2 √m 3) + √m33( K1 √m33 + C1 + K 2 √m 2 m 3 + C2))
× (E W P 1{ Ξ1}∣∣W PkF E V P kV PkF + E W PkW PkF E V P 1{ Ξ2}kV PkF)
+ (B + √m3 K1 + C 1)( EV P 1{ Ξ2}kV ρk2 + P(Ξ1 )E kV ρk2)
+ C2 m 3(E w P 1{ ΞJ∣∣W ρk2 + P(Ξ2)E∣∣W ρk2)
+ m 3(E w P 1{ Ξ1}∣∣W ρk 2E V P kV ρk + E w P ∣W ρ∣ 2E V P 1{ Ξ2}kV ρk)
+ √m 3(E V P 1{ Ξ2}kV P k 2E w PkW ρk + E V P kV ρk2 E w P 1{ Ξ1 }∣∣W Pk)
+ (O(C1C2) + B)Ψ1P(Ξ)
≤ (exp{-(X2 - 1)dm1/4} + X exp{-(X2 - 1)m3m2/4})poly(m1, m2, m3) = negligible.
(310)
But note that
V W-Vz = q w-V，+ Vw z ,v z (ψ 1 ∣W 1∣2 + ψ 2 kV 1∣2),
▽w-V ； L (W 0, V 0) = VW -V ； E( Xi,yi gZ ' ( fW-V，( Xi ), yi ) + VW -V ； ( ψ 1 ||W 0∣∣ 2 + ψ 2 ∣∣V 0∣∣ 2) ∙
Applying this to Equation (310), we get that ifwe define
Λ := Vw-VI- Vw-VzL(W0, V0),
then
EkΛl{Ξ}∣ ≤ (exp{-(X2 - 1)dm 1 /4} + Xexp{-(X2 — 1)m3m2/4})poly(m 1,m2,m3).
On the other hand, note that using again Equation (309), we have the following a.s. bound:
kΛl{Ξc}∣ = poly(m 1,m2, m3,C 1, C2,X).
Defining
Λ1 =Λ1 { Ξ c},
Λ2 = 1{ Ξ } E(Λ IΞ),
Λ = Λ1 +Λ2,
110
Published as a conference paper at ICLR 2022
we get that with probability at least 1 - P(Ξ):
jw0,v0 = ^w0,v0L(Ww, V0) + Λ
and also note that
EΛ = EΛ = 0.
Finally by the a.s. bound for Λ1, we have a.s.:
H Λ Il ≤ Ewρ,Vρ,(xi,yi) Il Λl { Ξ }∣∣ + I Λl { Ξ。}||
≤ (exp{-(χ2 - 1)dm1/4} + χ exp{-(χ2 - 1)m3m2/4})poly(m1, m2,m3) + poly(m1, m2,m3)
= poly(m1, m2, m3, C1, C2, B, χ),
which completes the proof.
Corollary 9.1 It is easy to check that running PSGD with unbiased gradient estimate VWo,v，
is equivalent to running SGD after our change of coordinates, with unbiased gradient estimate
Vw0,v0 ：= YVwo,v0, where Y is the matrix for our Change of coordinate, which is equal to Y
defined in Appendix A.10 for the coordinates in V0 and simply identity for the coordinates in W0.
Therefore, projecting both sides in Equation (311) of Lemma 43 onto Φ⊥ by multiplying Y implies
that with high probability for all iterations of the algorithm
Vw0,v0 = YVw0,V0Lπ (W； V0) + YΛ
=R w，,v0 L π( w0,v0) + YΛ,	(311)
where £ := YΛ (using the properties of Λ in Lemma 43) is a mean zero noise vector with almost
surely bounded norm, i.e. ||£∣∣ ≤ Q0 for some Q0 = poly(mι,m2,m3,C 1 ,C2). (we dropped the χ
parameters by considering constant high probability argument).
Finally, note that injecting noise (Ξi/∣Ξι∣∣, Ξ2/(√m11∣Ξ21∣)) by PSGD results in adding an extra
zero mean noise (ξ 1, Ξ2) := (YΞι/∣Ξi ∣∣, YΞ2/(√m11∣Ξ21∣)) to the gradient R⑪,vLπ(w0, v0).
Therefore, overall running SGD on LΠ (which is equivalent to PSGD on L) observe an unbiased
noise vector defined as £ := £
and Ξ2 are σ121 and σ2 21 for
∙-v	∙-v	∙-v
+ (Ξ1, Ξ2). Now it is easy to check that the moment matrix ofΞ1
σ102:
1
m21 d
σ202:
m2 (m3 - n)
(i-y) ^m
m2 3,
(312)
(313)
which implies the moment matrix of £ is upper bounded by
σ22I := (Q0/(m2m3 + m1d) + max{σ10 2, σ202})I,
and lower bounded by
σ22I := min{σ10 2, σ20 2}I,
i.e.
σ12 I E££T ≤ σ22 I .
(Note that we look at the new coordinates (w0, v0) as a vectors, so the term E££T makes sense.)
∙-v
Moreover H Ξ1H
bound for £:
∙∙w
1 / y∕m 1, ∣ Ξ2 H ≤ 1 almost surely, which implies the following almost SUreIy
M∣≤ Q := Q0 + 1 + 1 /vm 1.
111
Published as a conference paper at ICLR 2022
Lemma 44 Let g(x) be a second order differentiable function over RN such that at point x, there
exist a random direction y and deterministic direction z and fixed positive real r with:
E y = α
Ey g (N + ηz + √ηy) ≤ g (N) 一 ηr.
Then, for the gradient and Hessian at point x, we have either
r
Wg ( n ) n≥ 湎
or
λmin (V2g(n)) ≤ - 2kyyk2 .
Proof of Lemma 44
We write the second order tailor approximation of g around N:
g ( N + W ) = g ( N ) + Vg ( N ) T W + 2 WT V 2 g ( N ) W + θ(∣∣W∣∣2).
NoW substituting w with ηz + √ηy and taking expectation with respect to y, as We send η → 0 and
using the fact that Ey = 0:
E y g (N + ηz + √ηy) = g (N) + E y Vg (N) T (ηz + √ηy) + 2( ηz + √ηy) T V 2 g (N)(ηz + √ηy) + O(Inz + √ηyk2)
=E y g (N) + ηVg (N) T Z + 1 η 2 ZT V 2 g (N) Z + η 2 yT V 2 g (N) y + o (η∣∣y∣∣2)
=E y g (N) + ηVg (N) T Z + η 2 yT V 2 g (N) y + o (η).
Combining the assumption with the above Equation, we get that for small enough η, we have
ηVg(N)Tz + η2EyyTV2g(N)y ≤ -ηr/2,
i.e.
Vg(N)Tz + 2EyyTV2g(N)y ≤ -r/2,
which means we should either have
Vg(N)TZ ≤ -r/4,
which implies
or
which implies
r
kVg(N)k ≥ 4∣Zkk
EyyT V2g(N)y ≤ -r/2,
λmin (V2g(N)) ≤ - 2maxy∈s;ort(y)IIyk2 .
112
Published as a conference paper at ICLR 2022
B.4 Handling the Injected Noise by PSGD
In this section, we prove that having SGD injecting noise into our gradient estimates mostly does not
change the sign pattern of the first layer, namely among the set of rows in P defined in Lemma 1.
Lemma 45 Having enough overparameterization, with high probability, at every iteration of the
PSGD for W0(2) defined in the proof of Lemma 1, we have for every j ∈ [m1]:
"W j Ii ≤ C 2 / (4 √m 1).
Proof of Lemma 45
Let ΦW be the subspace of the first layer weight matrices which is zero in rows j ∈ P (P is
defined in Lemma 1), while in other rows it is the span of Zi 's, i.e. using our notation Zk
introduced in the proof of Lemma 1, we can write ΦW is span(Zik)i,k.
Recall from Lemma 1 that we decompose the first layer weight WW as WW(1) + WW(2), namely
the parts in the subspace ΦW and subspace Φw⊥ respectively. Moreover, let Ξι/(√m∕∣Ξι ∣∣) =
Ξ(1) + Ξ(2) be the decomposition of the injected noise at some iteration of PSGD into subspaces ΦW
and ΦW⊥ respectively.
Now recall that the current	WW is the value of the previous iteration moved by the gradient plus the
injected noise:
WW = WW - η(qW0 + Ξ⑴ + Ξ⑵)
=WW - η NW0 + 2M WWTI) + 2M WL-⑵ + Ξ⑴ + Ξ⑵)
where WW is the weight of the previous iteration and WW-,(1), WW-,(2) are again its decomposition to
Φw and Φw⊥ , where ▽ Wo V 0 is defined in Lemma 43. Applying Lemma 24 for the previous iteration
of the algorithm, we get ▽ W，∈ ΦW since the bad events E defined in Lemma 33 occurs only with
probability exponentially small (hence union bound across all the iterations rules it out). Hence, the
decomposition for the current iteration becomes
WW⑴=WL-⑴-η(qW0 + 2ψ 1WL-⑴ + Ξ(1)),
WW(2) = (1 - 2ηM1)WW--(2) + ηΞ(2).
(314)
(315)
We handle the WW(1) part in Lemma 1 and prove that as long as ∣WW(1) ∣2 ≤ ∣WW∣2 remains
bounded by C12, then the sign pattern of the first layer, when only considering the WW(1) part, is
specified by the initialization except within set P; here we handle the WW(2) part as well.
Note that for every row j ∈	[mι], the variable ∣∣(Ξι∕(√m11∣Ξι∣∣)) ,∣∣2 is
(O(1/(m14d)), O(1/(m12d)))-subexponential with mean 1/m1. Therefore, with probability
that is exponentially small in m ι, ||仕 / (√m11∣ ΞJ∣)) j ∣ is bounded by O (1 /m ι). It is not hard to
see the same argument holds for the projection of Ξι/(√m11∣Ξι ∣∣) onto Φ⊥, i.e. Ξ(2). Applying a
union bound for all iterations, again using the fact that we run PSGD for poly iterations while the
chance of error is exponentially small in m1 , we can then argue that with high probability over the
noise of gradients, at every iteration and for every j ∈ [m1]:
IΞj2) ∣ = C)(1 /mι).
(316)
But applying trinagle inequality to Equation (315) and writing it in a telescope form, particularly
for the jth row, and further using the assumption in 316, we get that ∣WW(j2) ∣ grows at most to
O(1/(m1M1)); as we set 1/M1 = O(poly(n)), assuming polynomially large enough m1 concludes
the claim.
113
Published as a conference paper at ICLR 2022
B.4.1	B ounding the Norm of the first Layer’s Output in the Worst Case
Lemma 46 Suppose W0 satisfies the assumption of Lemma 1, i.e. kW0k ≤ C1, and kWj0k ≤
C2/(2√m J except possible for indices in P, also defined in 1. Then, with high probability over
initialization
C3 / 2 n 3 m
sup kΦ0(x)k . (1 + O(m2d2 log(mι)2/mι))Cι + √m33√=-(—— )1 /4,
x,kxk=1	κ1 m1 λ0
which is O(C1 )for large enough overparameterization.
Proof of Lemma 46
Note the because the VC-dimension of the class of binary functions with respect to halfs-
paces in Rd isd + 1, the number of different sign patterns DW (0) ,x for different x can be at most
m1d+1 . Now similar to Equation (317), for k ∈ [m3] define
Zk (X) = 1 /√m (Ws,j 1{W^O)tχ}χ) j=1.	(317)
Then, for k1 6= k2, as kxk = 1:
1 m1
hZk 1 (X),Zk2(X)i = m jC WsIjWs2 j1{Wj Tx}
1	m1
≤ — sup X DW(0) ,xj,j Ws1 ,j Ws2 j.
m1	x
j=1
But for each fixed DW(0),x, using Hoeffding bound, we have with probability 1 - δ:
√mi X DW ⑼,xjjWkijWs ,j. Somδ ■
Applying the above for all possible sign patterns with δ < O(1/m1 d+1 ) and a union bound, we have
with high probability
，_,、_,、、	1	ml _	.一 ，	、，J—
SuP hZk 1 (x) ,Zk2 (x) i ≤ ——SuPΣ DW(0) ,xj,j Wsι ,jWs2 j . d log(m 1)/√m1 .
x,kxk=1	m1	x j=1
We can even state the following stronger bound with respect to two adversarially picked vectors
x, x :
1	m 1
SUP	hZk 1 (X),Zk2(χ0)i ≤ 一 SUPE DW(0),χjjDW(0),χ0j,jWsi,jWs2j . dlog(m 1)/√m1,
kxk=1,kx0 k=1	m1 x j=1	1	2
(318)
because each DW (0),x0 j,j has at most m1d+1 cases as we discussed above, then DW (0) ,xj,jDW(0) x0j,j
has at most m1d+1 possible cases, and applying a similar Hoeffding bound for each of them and a
union bound as we did will imply (318). We will use this generalized version in another section.
Now combining Equation (318) with the fact that kW0k ≤ C1 and applying Lemma 40:
m3
SuP XhW0, Zk(x)i2 ≤ (1 + O(m23d2 log(m1)2/m1))C12.	(319)
x,kxk=1 k=1
On the other hand, setting m2 = m 1, m3 = d, and R = C2/(2√m11K1) in Lemma 29, We get with
high probability
#(j ∈ [m] ： |Vj(0)x∣ ≤ c2/(2√m)) ≤ m 1 c2/(2√m1) = √m1 C2/(2K1).
114
Published as a conference paper at ICLR 2022
Noting that kWjk ≤ C2/(2√m11) for j ∈ P, We conclude that with high probability, for any x,
{{(W(0) + W0) T x ≥ 0 } and 1{W,(0) T x ≥ 0} can be different in at most √mι C 2 / (2 K1) of the j 飞
outside of [m 1] \ P. Therefore, as we have also ∣P∣ . nc2√m11 /κ 1 from Lemma 1, we conclude
that with high probability, for any x, there are at most O (nc2 √m [κ 1) sign changes by adding W0
to W(0). This further implies:
iφk (x) - hw 0, Zk (x) iι ≤2/√mι	E	ιWj xι
j: Sgn(Wj(0)T x)6=Sgn((W (0)+W 0)jT x)
≤ kW0k2y∣{j∣ Sgn(Wj(0)tx) = Sgn((W(0) + W0)tx)}∣/√m
.C1 ∖Jnc2 √m, 1 /κ 1 /√m 1
=C3_ ( n 3 m 3 )1 / 4
√K 1 rnn 1 λ 0	.
Combining this with (319), we conclude with high probability:
C3/2 n3.
sup kΦ0(x)k . (1 + O(m2d2 log(m 1)2/m 1))C1 + √m3^~(一 ʌ
x,kxk=1	κ1 m1λ0
which completes the proof.
m3
)1/4,
115