Published as a conference paper at ICLR 2022
Latent Image Animator:
Learning to animate images via latent space
NAVIGATION
Yaohui Wang, Di Yang, Francois Bremond & Antitza Dantcheva
Inria, Universite Cote d'Azur
{yaohui.wang,di.yang,francois.bremond,antitza.dantcheva}@inria.fr
Ab stract
Due to the remarkable progress of deep generative models, animating images has
become increasingly efficient, whereas associated results have become increas-
ingly realistic. Current animation-approaches commonly exploit structure repre-
sentation extracted from driving videos. Such structure representation is instru-
mental in transferring motion from driving videos to still images. However, such
approaches fail in case the source image and driving video encompass large ap-
pearance variation. Moreover, the extraction of structure information requires ad-
ditional modules that endow the animation-model with increased complexity. De-
viating from such models, we here introduce the Latent Image Animator (LIA),
a self-supervised autoencoder that evades need for structure representation. LIA
is streamlined to animate images by linear navigation in the latent space. Specifi-
cally, motion in generated video is constructed by linear displacement of codes in
the latent space. Towards this, we learn a set of orthogonal motion directions si-
multaneously, and use their linear combination, in order to represent any displace-
ment in the latent space. Extensive quantitative and qualitative analysis suggests
that our model systematically and significantly outperforms state-of-art methods
on VoxCeleb, Taichi and TED-talk datasets w.r.t. generated quality. Source code
and pre-trained models are publicly available1.
Figure 1: LIA animation examples. The two images of Marilyn Monroe and Emmanuel Macron
are animated by LIA, which transfers motion of a driving video (smaller images on the top) from
VoxCeleb dataset (Chung et al., 2018) onto the still images. LIA is able to successfully animate
these two images without relying on any explicit structure representations, such as landmarks and
region representations.
1https://wyhsirius.github.io/LIA-project/
1
Published as a conference paper at ICLR 2022
1	Introduction
In the series of science fiction books Harry Potter (Rowling et al., 2016; Rowling, 2019), wizards
and witches were able to magically enchant portraits, bringing them to life. Remarkable progress
of deep generative models has recently turned this vision into reality. This work examines the
scenario where a framework animates a source image by motion representations learned from a
driving video. Existing approaches for image animation are classically related to computer graphics
(Cao et al., 2014; Thies et al., 2016; 2019; Zhao et al., 2018) or exploit motion labels (Wang et al.,
2020b) and structure representations such as semantic maps (Pan et al., 2019; Wang et al., 2018;
2019), human keypoints (Jang et al., 2018; Yang et al., 2018; Walker et al., 2017; Chan et al., 2019;
Zakharov et al., 2019; Wang et al., 2019; Siarohin et al., 2019), 3D meshes (Liu et al., 2019; Chen
et al., 2021), and optical flows (Li et al., 2018; Ohnishi et al., 2018). We note that the ground truth
of such structure representations has been computed a-priori for the purpose of supervised training,
which poses constraints on applications, where such representations of unseen testing images might
be fragmentary or difficult to access.
Self-supervised motion transfer approaches (Wiles et al., 2018; Siarohin et al., 2019; 2021) accept
raw videos as input and learn to reconstruct driving images by warping source image with pre-
dicted dense optical flow fields. While the need for domain knowledge or labeled ground truth data
has been obviated, which improves performance on in-the-wild testing images, such methods entail
necessity of explicit structure representations as motion guidance. Prior information such as key-
points (Siarohin et al., 2019; Wang et al., 2021a) or regions (Siarohin et al., 2021) are learned in an
end-to-end training manner by additional networks as intermediate features, in order to predict target
flow fields. Although online prediction of such representations is less tedious than the acquisition of
ground truth labels, it still strains the complexity of networks.
Deviating from such approaches, we here aim to fully eliminate the need of explicit structure rep-
resentations by directly manipulating the latent space of a deep generative model. To the best of
our knowledge, this constitutes a new direction in the context of image animation. Our work is
motivated by interpretation of GANs (Shen et al., 2020; Goetschalckx et al., 2019; Jahanian et al.,
2020; Voynov & Babenko, 2020), showcasing that latent spaces of StyleGAN (Karras et al., 2019;
2020b) and BigGAN (Brock et al., 2019) contain rich semantically meaningful directions. Given
that walking along such directions, basic visual transformations such as zooming and rotation can
be induced in generated results. As in image animation, we have that motion between source and
driving images can be considered as higher-level transformation, a natural question here arises: can
we discover a set of directions in the latent space that induces high-level motion transformations
collaboratively?
Towards answering this question, we introduce LIA, a novel Latent Image Animator constituting of
an autoencoder for animating still images via latent space navigation. LIA seeks to animate a source
image via linearly navigating associated source latent code along a learned path to reach a target
latent code, which represents the high-level transformation for animating the source image. We
introduce a Linear Motion Decomposition (LMD) approach aiming to represent a latent path via a
linear combination of a set of learned motion directions and associated magnitudes. Specifically, we
constrain the set as an orthogonal basis, where each vector indicates a basic visual transformation.
By describing the whole motion space using such learned basis, LIA eliminates the requirement of
explicit structure representations.
In addition, we design LIA to disentangle motion and appearance within a single encoder-generator
architecture. Deviating from existing methods using separate networks to learn disentangled fea-
tures, LIA integrates both, latent motion code, as well as appearance features in a single encoder,
which highly reduces the model complexity and simplifies training.
We provide evaluation on multiple datasets including VoxCeleb (Chung et al., 2018),
TaichiHD (Siarohin et al., 2019) and TED-talk (Siarohin et al., 2021). In addition, we show that
LIA outperforms the state-of-the-art in preserving the facial structure in generated videos in the
setting of one-shot image animation on unseen datasets such as FFHQ (Karras et al., 2019) and
GermanPublicTV (Thies et al., 2020).
2	Related work
Video generation GAN-based video generation is aimed at mapping Gaussian noise to video, di-
rectly and in the absence of prior information (Vondrick et al., 2016; Saito et al., 2017; Tulyakov
et al., 2018; Wang et al., 2020a; Wang, 2021). Approaches based on deep probabilistic models (Den-
2
Published as a conference paper at ICLR 2022
Figure 2: General pipeline. Our objective is to transfer motion via latent space navigation. The
entire training pipeline consists of two steps. Firstly, we encode a source image xs into a latent code
zs→r . By linearly navigating zs→r along a path wr→d, we reach a target latent code zs→d. The
latent paths are represented by a linear combination between a set of learned motion directions (e.g.,
d1 and d2), which is an orthogonal basis, and associated magnitudes. In the second step, we decode
zs→d to a target dense optical flow field φs→d, which is used to warp xs into the driving image xd .
While we train our model using images from the same video sequence, in the testing phase, xs and
xd generally pertain to different identities.
ton & Birodkar, 2017; Li & Mandt, 2018; Bhagat et al., 2020; Xie et al., 2020) were also proposed
to tackle this problem, however only show results on toy datasets with low resolution. Recently,
with the progress of GANs in photo-realistic image generation (Brock et al., 2019; Karras et al.,
2019; 2020a), a series of works (Clark et al., 2019; Wang et al., 2021c) explored production of high-
resolution videos by incorporating the architecture of an image generator into video GANs, trained
jointly with RNNs. Tian et al. (2021) directly leveraged the knowledge of a pre-trained StyleGAN
to produce videos of resolution up to 1024 × 1024. Unlike these approaches, which generate random
videos based on noise vectors in an unconditional manner, in this paper, we focus on conditionally
creating novel videos by transferring motion from driving videos to input images.
Latent space editing In an effort to control generated images, recent works explored the discovery
of semantically meaningful directions in the latent space of pre-trained GANs, where linear navi-
gation corresponds to desired image manipulation. Supervised (Shen et al., 2020; Jahanian et al.,
2020; Goetschalckx et al., 2019) and unsupervised (Voynov & Babenko, 2020; Peebles et al., 2020;
Shen & Zhou, 2021) approaches were proposed to edit semantics such as facial attributes, colors and
basic visual transformations (e.g., rotation and zooming) in generated or inverted real images (Zhu
et al., 2020; Abdal et al., 2020). In this work, as opposed to finding directions corresponding to
individual visual transformations, we seek to learn a set of directions that cooperatively allows for
high-level visual transformations that can be beneficial in image animation.
Image animation Related approaches (Chan et al., 2019; Wang et al., 2018; Zakharov et al., 2019;
Wang et al., 2019; Yang et al., 2020) in image animation required strong prior structure labels as mo-
tion guidance. In particular, Chan et al. (2019), Yang et al. (2020) and Wang et al. (2018) proposed
to map representations such as human keypoints and facial landmarks to videos in the setting of
image-to-image translation proposed by Isola et al. (2017). However, such approaches were only
able to learn an individual model for a single identity. Transferring motion on new appearances
requires retraining the entire model from scratch by using videos of target identities. Several recent
works (Zakharov et al., 2019; Wang et al., 2019) explored meta learning in fine-tuning models on
target identities. While only few images of target identities were required during inference time,
it was still compulsory to input pre-computed structure representations in those approaches, which
usually are hard to access in many real-world scenarios. Towards addressing this issue, very recent
works (Siarohin et al., 2019; 2021; Wang et al., 2021b; Wiles et al., 2018) proposed to learn im-
age animation in self-supervised manner, only relying on RGB videos for both, training and testing
without any priors. They firstly predicted dense flow fields from input images, which were then uti-
lized to warp source images, in order to obtain final generated results. Inference only required one
image of a target identity without any fine-tuning step on pre-trained models. While no priors were
required, state-of-the-art methods still followed the idea of using explicit structure representations.
FOMM (Siarohin et al., 2019) proposed a first order motion approach to predict keypoints and local
transformations online to generate flow fields. Siarohin et al. (2021) developed this idea to model
articulated objects by replacing a keypoints predictor by a PCA-based region prediction module.
Wang et al. (2021b) extended FOMM by predicting 3D keypoints for view-free generation. We
3
Published as a conference paper at ICLR 2022
Figure 3: Overview of LIA. LIA is an autoencoder consisting of two networks, an encoder E and a
generator G. In the latent space, we apply Linear Motion Decomposition (LMD) towards learning
a motion dictionary Dm , which is an orthogonal basis where each vector represents a basic visual
transformation. LIA takes two frames sampled from the same video sequence as source image xs
and driving image xd respectively during training. Firstly, it encodes xs into a source latent code
zs→r and xd into a magnitude vector Ar→d . Then, it linearly combines Ar→d and a trainable Dm
using LMD to obtain a latent path wr→d, which is used to navigate zs→r to a target code zs→d.
Finally, G decodes zs→d into a target dense flow field and warps xs to an output image xs→d . The
training objective is to reconstruct xd using xs→d .
note though that in all approaches, given that keypoints or regions are inadequately predicted, the
quality of generated images drastically decreases. In contrast to such approaches, our method does
not require any explicit structure representations. We dive into the latent space of the generator and
self-learn to navigate motion codes in certain directions with the goal to reach target codes, which
are then decoded to flow fields for warping.
3	Method
Self-supervised image animation aims at learning to transfer motion from a subject of a driving video
to a subject in a source image based on training with a large video dataset. In this work, we propose
to model such motion transformation via latent space navigation. The general pipeline is illustrated
in Fig. 2. Specifically, for training, our model takes in a pair of source and driving images, randomly
sampled from one video sequence. These two images are encoded into a latent code which is used
to represent motion transformation in the image space. The training objective is to reconstruct the
driving image by combining source image with learned motion transformation. For testing, frames
of a driving video are sequentially processed with the source image to animate the source subject.
We provide an overview of the proposed model in Fig. 3. Our model is an autoencoder, consisting
of two main networks, an encoder E and a generator G. In general, our model requires two steps to
transfer motion. In the first step, E encodes source and driving images Xs ,xd 〜 X ∈ R3×H×W into
latent codes in the latent space. The source code is then navigated into a target code, which is used
to represent target motion transformation, along a learned latent path. Based on proposed Linear
Motion Decomposition (LMD), we represent such a path as a linear combination of a set of learned
motion directions and associated magnitudes, which are learned from xd . In the second step, once
the target latent code is obtained, G decodes it as a dense flow field φs→d 〜 Φ ∈ R2×H×W and
uses φs→d to warp xs and then to obtain the output image. In the following, we proceed to discuss
the two steps in detail.
3.1	Latent motion representation
Given a source image xs and a driving image xd , our first step constitutes of learning a latent code
zs→d 〜Z ∈ RN to represent the motion transformation from Xs to xd. Due to the uncertainty
of two images, directly learning zs→d puts forward a high requirement on the model to capture a
complex distribution of motion. Mathematically, it requires modeling directions and norms of the
vector zs→d simultaneously, which is challenging. Therefore, instead of modeling motion transfor-
mation Xs → Xd , we assume there exists a reference image Xr and motion transfer can be modeled
4
Published as a conference paper at ICLR 2022
as xs → xr → xd, where zs→d is learned in an indirect manner. We model zs→d as a target point
in the latent space, which can be reached by taking linear walks from a starting point zs→r along a
linear path wr→d (see Fig. 2), given by
zs→d = zs→r + wr→d,	(1)
where zs→r and wr→d indicate the transformation xs → xr and xr → xd respectively. Both zs→r
and wr→d are learned independently and zs→r is obtained by passing xs through E.
We learn wr→d via Linear Motion Decomposition (LMD). Our idea is to learn a set of motion
directions Dm = {d1, ..., dM} to represent any path in the latent space. We constrain Dm as an
orthogonal basis, where each vector indicates a motion direction di . We then combine each vector
in the basis with a vector Ar→d = {a1, ..., aM}, where ai represents the magnitude of di. Hence,
any linear path in the latent space can be represented using a linear combination
M
wr→d =	aidi,	(2)
i=1
where di ∈ RN and ai ∈ R for all i ∈ {1, ..., M}. Semantically, each di should represent a basic
visual transformation and ai indicates the required steps to walk in di towards achieving wr→d . Due
to Dm entailing an orthogonal basis, any two directions di , dj follow the constrain
<di,dj >= 10 ii =6= jj.	(3)
We implement Dm as a learnable matrix and apply the Gram-Schmidt process during each forward
pass, in order to meet the requirement of orthogonality. Ar→d is obtained by mapping zd→r, which
is the output of xd after E, through a 5-layer MLP. The final formulation of latent motion represen-
tation for each xs and xd is thus given as
M
zs→d = zs→r +	aidi.	(4)
i=1
3.2	Latent code driven image animation
Once we obtain zs→d, in our second step, we use G to decode a flow field φs→d and warp xs . Our
G consists of two components, a flow field generator Gf and a refinement network Gr (we provide
details in App. A).
Towards learning multi-scale features, G is designed as a residual network containing N models
to produce a pyramid of flow fields φs→d = {φi }1N in different layers of Gf . Multi-scale source
features xesnc = {xienc}1N are obtained from E and are warped in Gf.
However, as pointed out by Siarohin et al. (2019), only relying on φs→d to warp source features is
insufficient to precisely reconstruct driving images due to the existing occlusions in some positions
of xs . In order to predict pixels in those positions, the network is required to inpaint the warped
feature maps. Therefore, we predict multi-scale masks {mi}1N along with {φi}1N in Gf to mask out
the regions required to be inpainted. In each residual module, we have
x0i = T (φi, xienc)	mi,	(5)
where denotes the Hadamard product and T denotes warping operation, whereas x0i signifies the
masked features. We generate both dense flow fields, as well as masks by letting each residual
module output a 3-channel feature map in which the first two channels represent φi and the last
channel mi. Based on an inpainted feature map f(x0i), as well as an upsampled image g(xi-1)
provided by the previous module in Gr , the RGB image from each module is given by
oi = f(x0i) + g(oi-1),	(6)
where f and g denote the inpainting and upsampling layers, respectively. The output image oN from
the last module constitutes the final generated image xs→d = oN .
3.3	Learning
We train LIA in a self-supervised manner to reconstruct xd using three losses, i.e., a reconstruction
loss Lrecon, a perceptual loss Lvgg (Johnson et al., 2016) and an adversarial loss Ladv. We use
Lrecon to minimize the pixel-wise L1 distance between xd and xs→d, calculated as
Lrecon (xs→d, xd) = E[kxd - xs→d k1].	(7)
5
Published as a conference paper at ICLR 2022
Towards minimizing the perceptual distance, we apply a VGG19-based Lvgg on multi-scale feature
maps between real and generated images, written as
Lvgg (xs→d, xd) = E[	kFn(xd) - Fn(xs→d)k1],	(8)
n
where Fn denotes the nth layer in a pre-trained VGG19 (Simonyan & Zisserman, 2015). In prac-
tice, towards penalizing real and generated images in multi-scale images, we use a pyramid of four
resolutions, namely 256 × 256, 128 × 128, 64 × 64 and 32 × 32 as inputs of VGG19. The final
perceptual loss is the addition of perceptual losses in four resolutions.
Further, towards generating photo-realistic results, we incorporate a non-saturating adversarial loss
Ladv on xs→d, which is calculated as
Ladv (Xs→d) = Eχs→d~prec[-lθg(D(Xs→d)],	(9)
where D is a discriminator, aimed at distinguishing reconstructed images from the original ones.
Our full loss function is the combination of three losses with λ as a balanced hyperparameter
L(xs→d, xd)
Lrecon (xs→d, xd) + λLvgg (xs→d, xd) + Ladv (xs→d).
(10)
3.4	Inference
In inference stage, given a driving video sequence Vd = {xt}1T , we aim to transfer motion from Vd
to xs, in order to generate a novel video Vd→s = {xt→s}1T. If Vd and xs stem from the same video
sequence, i.e., xs = x1, our task comprises of reconstructing the entire original video sequence.
Therefore, we construct the latent motion representation of each frame using absolute transfer,
which follows the training process, given as
zs→t = zs→r + wr→t, t ∈ {1, ..., T}.	(11)
However, in real world applications, interest is rather placed on the scenario, where motion transfer
between xs and Vd, the latter stemming from different identities, i.e., xs 6= x1. Taking a talking
head video as an example, in this setting, beyond identity, x1 and xs might also differ in pose and
expression. Therefore, we propose relative transfer to eliminate the motion impact of wr→1 and
involve motion of wr→s in the full generated video sequence. Owing to a linear representation of
the latent path, we can easily represent zs→t for each frame as
zs→t = (zs→r + wr→s ) + (wr→t - wr→1)
= zs→s + (wr→t - wr→1), t ∈ {1, ..., T}.
(12)
The first term in Eq. (12), zs→s indicates the reconstruction of xs, while the second term
(wr→t - wr→1) represents the motion from x1 to xt. This equation indicates that the original
pose is preserved in xs , at the same time motion is transferred from Vd. We note that in order to
completely replicate the position and pose in Vd, it requires xs and x1 to contain similar poses in
relative motion transfer.
4	Experiments
In this section, we firstly describe our experimental setup including implementation details and
datasets. Secondly, we qualitatively demonstrate generated results based on testing datasets. Then,
we provide quantitative evaluation w.r.t. image quality on (a) same-identity reconstruction, (b) cross-
video motion transfer, presenting (c) a user study. Next, we conduct an ablation study that demon-
strates (d) the effectiveness of our proposed motion dictionary, as well as (e) associated size. Finally,
we provide an in-depth analysis of our (f) latent codes and (g) motion dictionary to interpret their
semantic meanings.
Datasets Our model is trained on the datasets VoxCeleb, TaichiHD and TED-talk. We follow
the pre-processing method in (Siarohin et al., 2019) to crop frames into 256 × 256 resolution for
quantitative evaluation.
Implementation details Our model is implemented in PyTorch (Paszke et al., 2019). All models
are trained on four 16G NVIDIA V100 GPUs. The total batch size is 32 with 8 images per GPU.
We use a learning rate of 0.002 to train our model with the Adam optimizer (Kingma & Ba, 2014).
The dimension of all latent codes, as well as directions in Dm is set to be 512. In our loss function,
we use λ = 10 in order to penalize more on the perceptual loss. It takes around 150 hours to fully
train our framework.
6
Published as a conference paper at ICLR 2022
Figure 4: Qualitative results. Examples for same-dataset absolute motion transfer on TaichiHD
(top-right) and TED-talk (bottom-right). On VoxCeleb (left), we demonstrate cross-dataset relative
motion transfer. We successfully transfer motion between x1 and xt from videos in VoxCeleb to xs
from FFHQ, the latter not being used for training.
Evaluation metrics We evaluate our model w.r.t. (i) reconstruction faithfulness using L1, LPIPS,
(ii) generated video quality using video FID, as well as (iii) semantic consistency using average
keypoint distance (AKD), missing keypoint rate (MKR) and average euclidean distance (AED).
Details are available in App. B.2.
4.1	Qualitative Results
Firstly, we evaluate the ability of LIA to generate realistic videos and compare related results with
four state-of-the-art methods. For TaichiHD and TED-talk datasets, we conduct an experiment re-
lated to cross-video generation. Corresponding results (see Fig. 4) confirm that our method is able
to correctly transfer motion on articulated human bodies, in the absence of explicit structure repre-
sentations. For the VoxCeleb dataset, we conduct a cross-dataset generation-experiment, where we
transfer motion from VoxCeleb to images of the FFHQ dataset. We observe that our method out-
performs FOMM and MRAA w.r.t. image quality, as both approaches visibly deform the shape of
the original faces. This is specifically notable in the case that source and driving images entail large
pose variations. At the same time, LIA is able to successfully tackle this challenge and no similar
deformations are visible.
4.2	Comparison with State-of-the-art Methods
We quantitatively compare our method with the state-of-the-art approaches X2Face, Monkey-Net,
FOMM and MRAA on two tasks, namely (a) same-identity reconstruction and (b) cross-video mo-
tion transfer. Additionally, we conduct a (c) user study.
(a)	Same-identity reconstruction We here evaluate the reconstruction ability of our method.
Specifically, we reconstruct each testing video by using the first frame as xs and the remain-
ing frames as xd. Results on three datasets are reported in Table 1. Focusing on foreground-
reconstruction, our method outperforms the other approaches w.r.t. all metrics. More results are
presented in App. B.3, discussing background-reconstruction.
(b)	Cross-video motion transfer Next, we conduct experiments, where source images and driv-
ing videos stem from different video sequences. In this context, we mainly focus on evaluating
talking head videos and explore two different cases. In the first case, we generate videos using the
VoxCeleb testing set to conduct motion transfer. In the second case, source images are from an
unseen dataset, namely the GermanPublicTV dataset, as we conduct cross-dataset motion transfer.
In both experiments, we randomly construct source and driving pairs and transfer motion from driv-
ing videos to source images to generate a novel manipulated dataset. Since ground truth data for
our generated videos is not available, we use video FID (as initialized by Wang et al. (2020a)) to
7
Published as a conference paper at ICLR 2022
Table 1: Same-identity reconstruction. Comparison with state-of-the-art methods on three datasets
for same-identity reconstruction.
VoXCeleb	TaichiHD	TED-talks
Method	Li	AKD	AED	LPIPS	Li	(AKD, MKR)	AED	LPIPS	Li	(AKD, MKR)	AED	LPIPS
X2Face	0.078	7.687	0.405	-	0.080	(17.654, 0.109)	-	-	-	-	-	-
Monkey-Net	0.049	1.878	0.199	-	0.077	(10.798, 0.059)	-	-	-	-	-	-
FOMM	0.046	1.395	0.141	0.136	0.063	(6.472, 0.032)	0.495	0.191	0.030	(3.759, 0.0090)	0.428	0.13
MRAA w/o bg	0.043	1.307	0.140	0.127	0.063	(5.626, 0.025)	0.460	0.189	0.029	(3.126, 0.0092)	0.396	0.12
Ours	0.041	1.353	0.138	0.123	0.057	(4.823, 0.020)	0.431	0.180	0.027	(3.141, 0.0095)	0.399	0.11
Table 2: Cross-video generation. We report
video FID for both inner- and cross-dataset
tasks on VoXCeleb and GermanPublicTV.
	VoxCeleb	GermanPublicTV
FOMM	―0323-	0.456
MRAA	0.308	0.454
Ours	0.161	0.406
Table 3: User study. We ask 20 human raters
to conduct a subjective video quality evalua-
tion.
	VoxCeleb(%)	TaichiHD(%)	TED-talk(%)
Ours/FOMM	92.9/7.1	-64.5/35.5^^	71.4/28.6
Ours/MRAA	89.7/10.3	60.7/39.9	54.8/45.2
compute the distance between generated and real data distributions. As shown in Tab. 2, our method
outperforms all other approaches w.r.t. video FID, indicating the best generated video quality.
(c)	User study We conduct a user study to evaluate video quality. Towards this, we displayed
paired videos and asked 20 human raters ‘which clip is more realistic?’. Each video-pair contains
a generated video from our method, as well as a video generated from FOMM or MRAA. Results
suggest that our results are more realistic in comparison to FOMM and MRAA across all three
datasets (see Tab. 3). Hence, the obtained human preference is in accordance with our quantitative
evaluation.
Table 4: Ablation study on motion dictionary.
We conduct eXperiments on three datasets with
and without Dm and show reconstruction results.
VoxCeleb	TaichiHD^^TED-talks
Method Li LPIPS Li LPIPS Li LPIPS
w/o Dm 0.049 0.165 0.062 0.186 0.031 0.12
Full 0.041 0.123 0.057 0.180 0.028 0.11
Table 5: Ablation study on Dm size. We con-
duct experiments on three datasets with 5 different
Dm size and show reconstruction results.
M	VoxCeleb		TaichiHD		TED-talks	
	Li	LPIPS	Li	LPIPS	Li	LPIPS
5	0.051	0.15	0.070	0.22	0.037	0.15
10	0.043	0.13	0.065	0.20	0.036	0.13
20	0.041	0.12	0.057	0.18	0.028	0.11
40	0.042	0.12	0.060	0.19	0.030	0.12
100	0.041	0.12	0.058	0.18	0.028	0.11
4.3	Ablation study
We here analyze our proposed motion dictionary and focus on answering following two questions.
(d)	Is the motion dictionary Dm beneficial? We here explore the impact of proposed Dm, by
training our model without Dm . Specifically, we output wr→d directly from MLP, without using
LMD to learn an orthogonal basis. From the evaluation results reported in Tab. 4 and qualitative
results in App. B.5, we observe that in the absence of Dm, model fails to generate high-quality
images, which proves the effectiveness of Dm , consistently on all datasets.
(e)	How many directions are required in Dm? Towards finding an effective size of Dm, we
empirically test three different M, viz. 5, 10, 20, 40 and 100. Quantitative results in Tab. 5 show
that when using 20 directions, the model achieves the best reconstruction results.
4.4 Further analysis
(f)	Latent code analysis. While our method successfully transfers motion via latent space navi-
gation, we here aim at answering the question — what does xr represent? Towards answering this
question, we proceed to visualize xr . We firstly decode zs→r into a dense flow field φs→r, which
is then used to warp xs (we show details in App. B.4). Fig. 5 shows examples of xs and xr. Inter-
estingly, we observe that xr represents the canonical pose of xs, regardless of original poses of the
subjects. And for all datasets, reference images resemble each other w.r.t. pose and scale. As such
8
Published as a conference paper at ICLR 2022
Figure 5: Visualization of reference images. Example source (top) and reference images (down)
from VoxCeleb, TaichiHD and TED-talk datasets. Our network learns reference images of a consis-
tently frontal pose, systematically for all input images of each dataset.
Figure 6: Linear manipulation of four motion directions on the painting of Mona Lisa. Manip-
ulated results indicate that d6 represents eye movement, d8 represents head nodding, whereas d19
and d7 represent facial expressions.
reference images can be considered as a normalized form of xs , learning transformations between
xs and xd using xs → xr → xd is considerably more efficient than xs → xd , once xr is fixed.
Noteworthy, we found the similar idea of learning a ‘reference image’ has also been explored
by Siarohin et al. (2019) (FOMM) and Wiles et al. (2018) (X2Face). However, deviating from
our visualized ‘reference image’, the ’reference image ’in FOMM refers to a non-visualized and ab-
stract concept. In addition, LIA only requires a latent code zs→r, rather than the ’reference image’
for both, training and testing, which is contrast to X2Face.
(g)	Motion dictionary interpretation. Towards further interpretation of directions in Dm , we
conduct linear manipulations on each di . Images pertained to manipulating four motion directions
are depicted in Fig. 6. The results suggest that the directions in Dm are semantically meaningful,
as they represent basic visual transformations such as head nodding (d8), eye movement (d6) and
facial expressions (d19 and d7). More results can be found on our project webpage2.
5 Conclusions
In this paper, we presented a novel self-supervised autoencoder LIA, aimed at animating images
via latent space navigation. By the proposed Linear Motion Decomposition (LMD), we were able
to formulate the task of transferring motion from driving videos to source images as learning lin-
ear transformations in the latent space. We evaluated proposed method on real-world videos and
demonstrated that our approach is able to successfully animate still images, while eliminating the
necessity of explicit structure representations. In addition, we showed that the incorporated motion
dictionary is interpretable and contains directions pertaining to basic visual transformations. Both
quantitative and qualitative evaluations showed that LIA outperforms state-of-art algorithms on all
benchmarks. We postulate that LIA opens a new door in design of interpretable generative models
for video generation.
2https://wyhsirius.github.io/LIA-project/
9
Published as a conference paper at ICLR 2022
Ethic Statement
In this work, we aim to synthesize high-quality videos by transferring motion on still images. Our
approach can be used for movie production, making video games, online education, generating
synthetic data for other computer vision tasks, etc. We note that our framework mainly focuses on
learning how to model motion distribution rather than directly model appearance, therefore it is not
biased towards any specific gender, race, region, or social class. It works equally well irrespective
of the difference in subjects.
Reproducibility S tatement
We assure that all the results shown in the paper and supplemental materials can be reproduced. We
intend to open-source our code, as well as trained models.
Acknowledgements
This work was granted access to the HPC resources of IDRIS under the allocation AD011011627R1.
It was supported by the French Government, by the National Research Agency (ANR) under Grant
ANR-18-CE92-0024, project RESPECT and through the 3IA Cote d,Azur Investments in the Future
project managed by the National Research Agency (ANR) with the reference number ANR-19-
P3IA-0002.
References
Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan++: How to edit the embedded im-
ages? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), June 2020.
Brandon Amos, Bartosz Ludwiczuk, and Mahadev Satyanarayanan. Openface: A general-purpose
face recognition library with mobile applications. Technical report, CMU-CS-16-118, CMU
School of Computer Science, 2016.
Sarthak Bhagat, Shagun Uppal, Zhuyun Yin, and Nengli Lim. Disentangling multiple features in
video sequences using gaussian processes in variational autoencoders. In ECCV, 2020.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity
natural image synthesis. In ICLR, 2019.
Adrian Bulat and Georgios Tzimiropoulos. How far are we from solving the 2d & 3d face alignment
problem? (and a dataset of 230,000 3d facial landmarks). In ICCV, 2017.
Chen Cao, Qiming Hou, and Kun Zhou. Displaced dynamic expression regression for real-time
facial tracking and animation. ACM Transactions on graphics (TOG), 33(4):1-10, 2014.
Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A. Sheikh. Openpose: Realtime multi-person
2d pose estimation using part affinity fields. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 2019.
Caroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A Efros. Everybody dance now. In ICCV,
2019.
Hao Chen, Yaohui Wang, Benoit Lagadec, Antitza Dantcheva, and Francois Bremond. Joint gener-
ative and contrastive learning for unsupervised person re-identification. In CVPR, 2021.
J. S. Chung, A. Nagrani, and A. Zisserman. Voxceleb2: Deep speaker recognition. In INTER-
SPEECH, 2018.
Aidan Clark, Jeff Donahue, and Karen Simonyan. Adversarial video generation on complex datasets.
arXiv preprint arXiv:1907.06571, 2019.
10
Published as a conference paper at ICLR 2022
Emily L Denton and vighnesh Birodkar. Unsupervised learning of disentangled representations
from video. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett (eds.), NeurIPS, 2017.
Lore Goetschalckx, Alex Andonian, Aude Oliva, and Phillip Isola. Ganalyze: Toward visual defini-
tions of cognitive image properties. In ICCV,, pp. 5744-5753, 2019.
Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can spatiotemporal 3d cnns retrace the history
of 2d cnns and imagenet? In CVPR, 2018.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In NIPS, 2017.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-Image Translation with
Conditional Adversarial Networks. In CVPR, 2017.
Ali Jahanian, Lucy Chai, and Phillip Isola. On the ”steerability” of generative adversarial networks.
In ICLR, 2020.
Yunseok Jang, Gunhee Kim, and Yale Song. Video Prediction with Appearance and Motion Condi-
tions. In ICML, 2018.
Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and
super-resolution. In European conference on computer vision, pp. 694-711. Springer, 2016.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In CVPR, 2019.
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyz-
ing and improving the image quality of StyleGAN. In CVPR, 2020a.
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyz-
ing and improving the image quality of stylegan. In CVPR, 2020b.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.),
Advances in Neural Information Processing Systems, 2012.
Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, and Ming-Hsuan Yang. Flow-grounded
spatial-temporal video prediction from still images. In ECCV, 2018.
Yingzhen Li and Stephan Mandt. Disentangled sequential autoencoder. ICML, 2018.
Wen Liu, Zhixin Piao, Jie Min, Wenhan Luo, Lin Ma, and Shenghua Gao. Liquid warping gan: A
unified framework for human motion imitation, appearance transfer and novel view synthesis. In
CVPR, 2019.
Arsha Nagrani, Joon Son Chung, Weidi Xie, and Andrew Zisserman. Voxceleb: Large-scale speaker
verification in the wild. Computer Science and Language, 2019.
Katsunori Ohnishi, Shohei Yamamoto, Yoshitaka Ushiku, and Tatsuya Harada. Hierarchical video
generation from orthogonal information: Optical flow and texture. In AAAI, 2018.
Junting Pan, Chengyu Wang, Xu Jia, Jing Shao, Lu Sheng, Junjie Yan, and Xiaogang Wang. Video
generation from single semantic label map. arXiv preprint arXiv:1903.04480, 2019.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems. 2019.
William Peebles, John Peebles, Jun-Yan Zhu, Alexei A. Efros, and Antonio Torralba. The hessian
penalty: A weak prior for unsupervised disentanglement. In Proceedings of European Conference
on Computer Vision (ECCV), 2020.
11
Published as a conference paper at ICLR 2022
JK Rowling. Harry potter. The 100 Greatest Literary Characters, pp. 183, 2019.
Joanne Kathleen Rowling, John Tiffany, and Jack Thorne. Harry Potter and the Cursed Child-Parts
One and Two (Special Rehearsal Edition). Pottermore Publishing, 2016.
Masaki Saito, Eiichi Matsumoto, and Shunta Saito. Temporal generative adversarial nets with sin-
gular value clipping. In ICCV, 2017.
Yujun Shen and Bolei Zhou. Closed-form factorization of latent semantics in gans. In CVPR, 2021.
Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. Interpreting the latent space of gans for
semantic face editing. In CVPR, 2020.
Aliaksandr Siarohin, StePhane Lathuiliere, Sergey Tulyakov, Elisa Ricci, and NicU Sebe. First order
motion model for image animation. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-
Buc, E. Fox, and R. Garnett (eds.), NeurIPS. Curran Associates, Inc., 2019.
Aliaksandr Siarohin, Oliver Woodford, Jian Ren, Menglei Chai, and Sergey Tulyakov. Motion
rePresentations for articulated animation. In CVPR, 2021.
Karen Simonyan and Andrew Zisserman. Very deeP convolutional networks for large-scale image
recognition. In International Conference on Learning Representations, 2015.
Justus Thies, Michael Zollhofer, Marc Stamminger, Christian Theobalt, and Matthias Nieβner.
Face2face: Real-time face caPture and reenactment of RGB videos. In CVPR, 2016.
Justus Thies, Michael Zollhofer, and Matthias Nieβner. Deferred neural rendering: Image synthesis
using neural textures. ACM Transactions on Graphics (TOG), 38(4):1-12, 2019.
Justus Thies, Mohamed Elgharib, Ayush Tewari, Christian Theobalt, and Matthias Nieβner. Neural
voice PuPPetry: Audio-driven facial reenactment. ECCV, 2020.
Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N. Metaxas, and Sergey
Tulyakov. A good image generator is what you need for high-resolution video synthesis. In
ICLR, 2021.
Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. MoCoGAN: DecomPosing motion
and content for video generation. In CVPR, 2018.
Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics.
In NIPS, 2016.
Andrey Voynov and Artem Babenko. UnsuPervised discovery of interPretable directions in the gan
latent sPace. arXiv preprint arXiv:2002.03754, 2020.
Jacob Walker, Kenneth Marino, Abhinav GuPta, and Martial Hebert. The Pose knows: Video fore-
casting by generating Pose futures. In ICCV, 2017.
Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, and Bryan Catan-
zaro. Video-to-video synthesis. In NeurIPS, 2018.
Ting-Chun Wang, Ming-Yu Liu, Andrew Tao, Guilin Liu, Jan Kautz, and Bryan Catanzaro. Few-
shot video-to-video synthesis. In NeurIPS, 2019.
Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot free-view neural talking-head synthesis
for video conferencing. In CVPR, 2021a.
Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot free-view neural talking-head synthesis
for video conferencing. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2021b.
Yaohui Wang. Learning to Generate Human Videos. Theses, Inria - Sophia Antipolis ; Universite
Cote d’Azur, SePtember 2021.
Yaohui Wang, Piotr Bilinski, Francois Bremond, and Antitza Dantcheva. G3AN: Disentangling
appearance and motion for video generation. In CVPR, 2020a.
Yaohui Wang, Piotr Bilinski, Francois F Bremond, and Antitza Dantcheva. ImaGINator: Conditional
Spatio-Temporal GAN for Video Generation. In WACV, 2020b.
12
Published as a conference paper at ICLR 2022
Yaohui Wang, Francois Bremond, and Antitza Dantcheva. Inmodegan: Interpretable motion decom-
position generative adversarial network for video generation. arXiv preprint arXiv:2101.03049,
2021c.
Olivia Wiles, A Koepke, and Andrew Zisserman. X2face: A network for controlling face generation
using images, audio, and pose codes. In Proceedings of the European conference on computer
vision (ECCV),pp. 670-686, 2018.
Jianwen Xie, Ruiqi Gao, Zilong Zheng, Song-Chun Zhu, and Ying Nian Wu. Motion-based gener-
ator model: Unsupervised disentanglement of appearance, trackable and intrackable motions in
dynamic patterns. In AAAI, 2020.
Ceyuan Yang, Zhe Wang, Xinge Zhu, Chen Huang, Jianping Shi, and Dahua Lin. Pose guided
human video generation. In ECCV, 2018.
Zhuoqian Yang, Wentao Zhu, Wayne Wu, Chen Qian, Qiang Zhou, Bolei Zhou, and Chen Change
Loy. Transmomo: Invariance-driven unsupervised video motion retargeting. In CVPR, 2020.
Egor Zakharov, Aliaksandra Shysheya, Egor Burkov, and Victor Lempitsky. Few-shot adversarial
learning of realistic neural talking head models. In ICCV, 2019.
Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable
effectiveness of deep features as a perceptual metric. In CVPR, 2018.
Long Zhao, Xi Peng, Yu Tian, Mubbasir Kapadia, and Dimitris Metaxas. Learning to forecast and
refine residual motion for image-to-video generation. In ECCV, 2018.
Zhedong Zheng, Tao Ruan, Yunchao Wei, Yi Yang, and Tao Mei. Vehiclenet: Learning robust visual
representation for vehicle re-identification. IEEE Transaction on Multimedia (TMM), 2020.
Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-domain gan inversion for real image
editing. In Proceedings of European Conference on Computer Vision (ECCV), 2020.
13
Published as a conference paper at ICLR 2022
A Details of model architecture
We proceed to describe the model architecture in this section. Fig. 7 shows details of our E. In
each ResBlock in E, spatial size of input feature maps are downsampled. We take feature maps of
spatial sizes from 8 × 8 to 256 × 256 as our appearance features xienc. We use a 5-layer MLP to
predict a magnitude vector Ad→r from zd→r . Fig. 8 (a) shows the general architecture of our G,
which consists of two components, a flow field generator Gf and a refinement network Gr. We
apply StyleConv (Upsample + Conv3 × 3), which is proposed by StyleGAN2, in Gf . StyleConv
takes latent representation zs→t as style code and generates flow field φi and corresponding mask
mi . Gr uses UpConv (Conv1 × 1 + Upsample) to upsample and refine inpainted feature maps to
target resolution. We show details pertaining to G block in Fig. 8 (b). Each G block is used to
upsample ×2 the previous resolution. We stack 6 blocks towards producing 256 resolution images.
d→r
(a) Encoder
(b) ResBlock
Figure 7: Encoder architecture. We show details of architecture of E in (a) and ResBlock in (b).
B Experiments
We proceed to introduce details of datasets and evaluation metrics used in our experiments.
B.1	Datasets
VoxCeleb (Nagrani et al., 2019) consists of a large amount of interview videos of different celebri-
ties. Following the process of FOMM (Siarohin et al., 2019), we extract frames and crop them into
256 × 256 resolution. In total, VoxCeleb contains a training set of 17928 videos and a test set of 495
videos.
14
Published as a conference paper at ICLR 2022
Upsample
Figure 8: Generator architecture. We show details about architecture of G in (a) and G block in
(b).
TaiChiHD (Siarohin et al., 2019) consists of videos of full human bodies performing Tai Chi actions.
We follow the original pre-processing of FOMM (Siarohin et al., 2019) and utilize its 256 × 256
version. TaiChiHD contains 1096 training videos and 115 testing videos.
TED-talk is a new dataset proposed in MRAA (Siarohin et al., 2021). It comprises a number of
TED-talk videos, where the main subjects have been cropped out. We resize the original version
into 256 × 256 resolution to train our model. This dataset includes 1124 training videos and 130
testing videos.
B.2	Evaluation metrics
We use five different metrics to evaluate our experimental results, namely L1, LPIPS, AKD, MKR
and AED that quantify the reconstructed results. In addition, we compute video FID to evaluate
video quality in motion transferring tasks.
L1 represents the mean absolute pixel difference between reconstructed and real videos.
LPIPS (Zhang et al., 2018) aims at measuring the perceptual similarity between reconstructed and
real images by leveraging the deep features from AlexNet (Krizhevsky et al., 2012).
Video FID is a modified version of the original FID (Heusel et al., 2017). We here follow the
same implementation as Wang et al. (2020a) and utilize a pre-trained ResNext101 (Hara et al.,
2018) to extract spatio-temporal features to compute the distance between real and generated videos
distributions. We take the first 100 frames of each video as input of the feature-extractor to compute
the final scores.
Average keypoint distance (AKD) and missing keypoint rate (MKR) evaluate the difference be-
tween keypoints of reconstructed and ground truth videos. We extract landmarks using the face
alignment approach of (Bulat & Tzimiropoulos, 2017) and extract body poses for both TaiChiHD
and TED-talks using OpenPose (Cao et al., 2019). AKD is computed as the average distance be-
tween corresponding keypoints, whereas MKR is the proportion of keypoints present in the ground-
truth that are missing in a reconstructed video.
Average Euclidean distance (AED) measures the ability of preserving identity in reconstructed
video. We use a person re-identification pretrained model (Zheng et al., 2020) for measuring human
bodies (TaichiHD and TED-talk) and OpenFace (Amos et al., 2016) for faces to extract identity em-
beddings from reconstructed and ground truth frame pairs, then we compute MSE of their difference
for all pairs.
15
Published as a conference paper at ICLR 2022
B.3 Comparison with full MRAA
We show quantitative evaluation results with the full MRAA model in Tab. 6. We observe that
our method achieves competitive results in reconstruction and keypoint evaluation. While we do
not explicitly predict keypoints, w.r.t. the TaichiHD dataset, interestingly we outperform MRAA in
both, AKD and MKR. Such results showcase the effectiveness of our proposed method on modeling
articulated human structures. However, reconstruction evaluation cannot provide a completely fair
comparison on how well the main subjects (e.g., faces and human bodies) are generated in videos.
This is in particular the case for TaichiHD and TED-talk, where backgrounds have large contribu-
tions to the final scores.
Table 6: Comparison with full MRAA.
VoxCeleb	TaiChiHD	TED-talks
Method	Li	AKD	AED	LPIPS	Li	(AKD,MKR)	AED	LPIPS	Li	(AKD,MKR)	AED LPIPS
MRAA	0.041	1.303	0.135	0.124	0.045	(5.551,0.025)	0.431	0.178	0.027	(3.107,0.0093)	0.379~0.11
Ours	0.041	1.353	0.138	0.123	0.057	(4.823, 0.020)	0.431	0.180	0.027	(3.141, 0.0095)	0.399 0.11
B.4	Reference image generation.
To produCe xr , we use G to deCode zs→r into the flow field φs→r . ReferenCe image xr is obtained
by warping xs using φs→r . The entire proCess is shown in Fig. 9.
Figure 9: Reference image generation.
B.5	Qualitative results on effectivenes s of using motion dictionary
Fig	. 10 illustrates the generated results on transferring motion from VoxCeleb to GermanPubliCTV
with and without motion diCtionary. We observe that without the motion diCtionary, appearanCe
information is undesirably transferred from driving videos to generated videos.
B.6 Limitations
For human body, one limitation of our method is dealing with body oCClusion. We observe in
Fig. 11 that in taiChi videos, in Case of oCClusion Cause by legs and arms, motion is not transferred
suCCessfully. In addition, in TED-talks, transferring hand motion is Challenging, as hands are of
small size, artiCulated and sometimes oCCluded by human bodies.
16
Published as a conference paper at ICLR 2022
Figure 10: Generated results with and without Dm . We observe that the disentanglement of
appearance and motion is much better by using Dm .
TaichiHD
TED-talks
Driving
Generated results
Figure 11: Failure cases. We observed that it is still challenging for LIA to handle arm-leg occlusion
(Taichi) and hand motion (TED-talk).
17