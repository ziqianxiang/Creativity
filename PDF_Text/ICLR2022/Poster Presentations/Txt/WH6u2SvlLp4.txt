Published as a conference paper at ICLR 2022
Learning Prototype-oriented Set Representa-
tions for Meta-Learning
Dandan Guo 1, Long Tian2, Minghe Zhang3, Mingyuan Zhou4, Hongyuan Zha1
1 The Chinese University of Hong Kong, Shenzhen 2Xidian University
3 Georgia Institute of Technology 4The University of Texas at Austin
guodandan@cuhk.edu.cn tianlong@xidian.edu.cn mzhang388@gatech.edu
migyuan.zhou@mccombs.utexas.edu zhahy@cuhk.edu.cn
Ab stract
Learning from set-structured data is a fundamental problem that has recently at-
tracted increasing attention, where a series of summary networks are introduced
to deal with the set input. In fact, many meta-learning problems can be treated
as set-input tasks. Most existing summary networks aim to design different ar-
chitectures for the input set in order to enforce permutation invariance. However,
scant attention has been paid to the common cases where different sets in a meta-
distribution are closely related and share certain statistical properties. Viewing
each set as a distribution over a set of global prototypes, this paper provides a
novel prototype-oriented optimal transport (POT) framework to improve exist-
ing summary networks. To learn the distribution over the global prototypes, we
minimize its regularized optimal transport distance to the set empirical distribu-
tion over data points, providing a natural unsupervised way to improve the sum-
mary network. Since our plug-and-play framework can be applied to many meta-
learning problems, we further instantiate it to the cases of few-shot classification
and implicit meta generative modeling. Extensive experiments demonstrate that
our framework significantly improves the existing summary networks on learning
more powerful summary statistics from sets and can be successfully integrated
into metric-based few-shot classification and generative modeling applications,
providing a promising tool for addressing set-input and meta-learning problems.
1	Introduction
Machine learning models, such as convolutional neural networks for images (He et al. 2016) and
recurrent neural networks for sequential data (Sutskever et al. 2014), have achieved great success in
taking advantage of the structure in the input space (Maron et al. 2020). However, extending them to
handle unstructured input in the form of sets, where a set can be defined as an unordered collections
of elements, is not trivial and has recently attracted increasing attention (Jurewicz & Str0mberg-
Derczynski, 2021). Set-input is relevant to a range of problems, such as understanding a scene
formed ofa set of objects (Eslami et al. 2016), classifying an object composed ofa set of 3D points
(Qi et al. 2017), summarizing a document consisting ofa set of words (Blei et al., 2003; Zhou et al.,
2016), and estimating summary statistics from a set of data points for implicit generative models
(Chen et al. 2021). Moreover, many meta-learning problems, which process different but related
tasks, may also be viewed as set-input tasks (Lee et al., 2019), where an input set corresponds to
the training dataset of a single task. Therefore, we broaden the scope of set-related applications by
including traditional set-structured input problems and most meta-learning problems. Both of them
aim to improve the quick adaptation ability for unseen sets, even though the latter is more difficult
because of limited samples or the occurrence of new categories for classification problems.
For a set-input, the output of the model must not change if the elements of the input set are reordered,
which entails permutation invariance of the model. To enforce this property, multiple researchers
have recently focused on designing different network architectures, which can be referred to as a
summary network for compressing the set-structured data into a fixed-size output. For example,
the prominent works of Zaheer et al. (2017) and Edwards & Storkey (2017) combined the standard
1
Published as a conference paper at ICLR 2022
feed-forward neural networks with a set-pooling layer, which have been proven to be universal ap-
proximators of continuous permutation invariant functions. Lee et al. (2019) further introduced Set
Transformer to encode and aggregate the features within the set using multi-head attention. Maron
et al. (2020) designed deep models and presented a principled approach to learn sets of symmetric
elements. Despite the effectiveness and recent popularity of these works in set-input problems, there
are several shortcomings for existing summary networks, which could hinder their applicability and
further extensions: 1) The parameters of the summary network are typically optimized by a task-
specific loss function, which could limit the models’ flexibility. 2) A desideratum of a summary
network is to extract set features, which have enough ability to represent the summary statistics of
the input set and thus benefit the corresponding set-specific task; but for many existing summary
networks, there is no clear evidence or constraint that the outputs of the summary network could de-
scribe the set’s summary statistics well. These limits still remain even with the recent more carefully
designed summary networks, while sets with limited samples further exacerbate the problem.
To address the above shortcomings, we present a novel and generic approach to improve the sum-
mary networks for set-structured data and adapt them to meta-learning problems. Motivated by
meta-learning that aims to extract transferable patterns useful for all related tasks, we assume that
there are K global prototypes (i.e., centers) among the collection of related sets, and each proto-
type or center is encouraged to capture the statistical information shared by those sets, similar to the
“topic” in topic modeling (Blei et al., 2003; Zhou et al., 2016) or “dictionary atom” in dictionary
learning (Aharon et al., 2006; Zhou et al., 2009). Specifically, for the jth set, we consider it as a
discrete distribution Pj over all the samples within the set (in data or feature space). At the same
time, we also represent this set with another distribution Qj (in the same space with Pj ), supported
on K global prototypes with a K-dimensional set representation hj . Since hj measures the impor-
tance of global prototypes for setj, it can be treated as the prototype proportion for summarizing the
salient characteristics of set j . Moreover, the existing summary networks can be adopted to encode
set j as hj for their desired property of permutation invariance. In this way, we can formulate the
learning of summary networks as the process of learning a Pj to be as close to Qj as possible, a pro-
cess facilitated by leveraging the optimal transport (OT) distance (Peyre & CUtUri 2019). Therefore,
the global prototypes and summary network can be learned by jointly optimizing the task-specific
loss and OT distance between Pj and Qj in an end-to-end manner. We can refer to this method
as prototype-oriented OT (POT) framework for meta-learning, which is applicable to a range of
UnsUpervised and sUpervised tasks, sUch as set-inpUt problems solved by sUmmary networks, meta
generation (Hong et al., 2020c; AntonioU et al., 2017), metric-based few-shot classification (Snell
et al., 2017), and learning statistics for approximate Bayesian compUtation (Chen et al., 2021). We
note oUr constrUction has drawn inspirations from previoUs works that Utilize a transport based loss
between a set of objects and a set of prototypes (TanwisUth et al., 2021; Wang et al., 2022). These
works mainly follow the bidirectional conditional transport framework of Zheng & ZhoU (2021),
instead of the Undirectional OT framework, and focUs on different applications.
Since oUr plUg-and-play framework can be applied to many meta-learning problems, this paper fUr-
ther instantiates it to the cases of metric-based few-shot classification and implicit meta generative
modeling. We sUmmarize oUr contribUtions as follows: (1) We formUlate the learning of sUmmary
network as the distribUtion approximation problem by minimizing the distance between the distribU-
tion over data points and another one over global prototypes. (2) We leverage the POT to measUre the
difference between the distribUtions for Use in a joint learning algorithm. (3) We apply oUr method
to metric-based few-shot classification and constrUct implicit meta generative models, where a sUm-
mary network is Used to extract the sUmmary statistics from set and optimized by the POT loss.
Experiments on several meta-learning tasks demonstrate that introdUcing the POT loss into existing
sUmmary networks can extract more effective set representations for the corresponding tasks, which
can also be integrated into existing few-shot classification and GAN frameworks, prodUcing a new
way to learn the set’ sUmmary statistics applicable to many applications.
2	Background
2.1	Summary networks for set-structured input
To deal with the set-strUctUred inpUt Dj = {xj,1:Nj } and satisfy the permUtation invariance in set,
a remarkably simple bUt effective sUmmary network is to perform pooling over embedding vectors
extracted from the elements of a set. More formally,
Sφ (Dj) = gφ2 (pool ({fφι (xji) ,...,fφι (XjNj)})) ,	(1)
2
Published as a conference paper at ICLR 2022
where f©、(∙) acts on each element of a set and gΦ? (pool(∙)) aggregates these encoded features and
produces desired output, and φ = {φ1, φ2} denotes the parameters of the summary network. Most
network architectures for set-structured data follow this structure; see more details from previous
works (Lee et al., 2019; Zaheer et al., 2017; Edwards & Storkey, 2017; Maron et al., 2020).
2.2	Optimal Transport
Although OT has a rich theory, we limit our discussion to OT for discrete distributions and refer the
reader to Peyre & Cuturi (2019) for more details. Let us consider P and q as two discrete probability
distributions on the arbitrary space X ⊆ Rd , which can be formulated as p = Pin=1 ai δxi and
q = Pjm=1 bj δyj . In this case, a ∈ Σn and b ∈ Σm, where Σn denotes the probability simplex
of Rn . The OT distance between a and b is defined as
OT(a, b) = min hT, Ci,	(2)
T∈U(a,b)
where〈•，•〉means the Frobenius dot-product; C ∈ R≥×m is the transport cost function with element
Cij = C(xi, yj); T ∈ Rn>×0 m denotes the doubly stochastic transport probability matrix such that
U(a, b) := {T | Pin Tij = bj, Pjm Tij = ai}. To relax the time-consuming problem when opti-
mising the OT distance, Cuturi (2013) introduced the entropic regularization, H = - Pij Tij lnTij,
leading to the widely-used Sinkhorn algorithm for discrete OT problems.
3	Proposed framework
In meta-learning, given a meta-distribution pM of tasks, the marginal distribution pj of task j is sam-
pled from pM for j ∈ J , where J denotes a finite set of indices. E.g., we can sample pj from pM
with probability jJ when PM is uniform over a finite number of marginals. During meta-training,
direct access to the distribution of interest pj is usually not available. Instead, we will observe a set
of data points Dj = {xji}iN=j1, which consists ofNj i.i.d. samples from Pj overRd. We can roughly
treat the meta-learning problems as the set-input tasks, where dataset Dj from Pj corresponds to
an input set. To learn more representative features from related but unseen sets in meta-learning
problems, we adopt the summary network as the encoder to extract set representations and improve
it by introducing the OT loss and global prototypes, providing many applications. Besides, we also
provide the applications to metric-based few-shot classification and implicit generative framework
by assimilating the summary statistics. Below we describe our model in detail.
3.1	Learning global prototypes and set representation via OT
Given J sets from meta-distribution PM, we can represent each set Dj from meta-distribution PM
as an empirical distribution over Nj samples on the original data space, formulated as
Pj = Xi=1 Nδxji,Xji ∈ Rd.	(3)
Since all sets (distributions) drawn from meta-distribution PM are closely related, it is reasonable
to assume that these sets share some statistical information. Motivated by dictionary learning, topic
modeling, and two recent prototype-oriented algorithms (Tanwisuth et al., 2021; Wang et al., 2022),
we define the shared information as the learnable global prototype matrix B = {βk} ∈ Rd×K,
where K represents the number of global prototypes and βk denotes the distributed representation
of the k-th prototype in the same space of the observed data points (e.g., “topic” in topic modeling).
Given the prototype matrix B, each set can be represented with a K-dimensional weight vector
hj ∈ Σk (e.g., “topic proportion” in topic modeling), where hjk means the weight of the prototype
βk for set j. Hence, we can represent set Dj with another distribution Qj on prototypes β±K:
Qj =XK hjkδβk,βk ∈Rd,	(4)
k=1
where hj is a set representation for describing set j . Since set j can be represented as Qj and Pj ,
we can learn set-specific representation hj and prototype matrix B by pushing Qj towards Pj :
Nj K
OT(Pj,Qj) = minhT,Ci d=ef. XXCikTik,
B,hj
ik
(5)
3
Published as a conference paper at ICLR 2022
Set 1
Set j
Set J
Summary Network
Global centers
Figure 1: An overview of our proposed framework, where “pool” operation including mean, sum, max or
similar. We compute the representation hj for set j using the summary network in Equation 1, which is the
weight vector of the global prototypes (i.e., centers) βi:K in the corresponding set.
Nj ×K
where C ∈ R≥0 is the transport cost matrix. In this paper, to measure the distance between
data point Xji in set j and prototype βk, unless specified otherwise, we construct C as Cik =
1 -cos (xji, βk), which provides an upper-bounded positive similarity metric. Besides, the transport
probability matrix T ∈ RN0×K should satisfy Π(a, b) := {T | T1k = a, Tτ1N = b} with
Tik = T(xji, βk), where a = [N1-] ∈ ΣNj and b = [hjk] ∈ Σk denote the respective probability
vectors for distribution Pj in Equation 3 and Qj in Equation 4.
Since hj should be invariant to permutations of the samples in set j, we adopt a summary network
to encode the set of Nj points. For unsupervised tasks, taking the summary network in Equation 1
as the example, we can directly add a Softmax activation function into Sφ to enforce the simplex
constraint in set representation hj, denoted as hj = Softmax(Sφ(Dj)). As shown in Fig. 1, given
J sets, to learn the global prototype matrix B and summary network parameterized by φ, we adopt
the entropic constraint (Cuturi, 2013) and define the average OT loss for all training sets as
ɪ J (Nj κ	Nj κ	∖	ɪ J
LPOT=m∣n J X(XXCikTik - eXX -TikInTik I =mi∏ J X (OTe(Pj, Qj)),
j=1	i k	i k	j=1
(6)
where is a hyper-parameter for entropic constraint. Algorithm 1 describes the workflow of the
POT loss for improving summary network under unsupervised tasks. For supervised tasks, set j is
denoted as Dj = {xj,1:Nj , yj}, where yj is the ground-truth output determined by specific tasks. As
hj is a normalized weight vector, directly using it to realize the corresponding task may be undesired.
Denoting zj = pool fφ1 (xj 1) , . . . , fφ1 xjNj , we project it to the following vectors:
hj = fe(Zj), yj = fλ (Zj ),	⑺
where hj and yj∙ are responsible for the POT and task-specific losses, respectively. Now the sum-
mary network parameters φ= {e, λ, φ1} and global prototypes B are learned by jointly optimizing
the task-specific loss (computed by yj∙ and yj∙) and OT loss in Equation 6. In summary, minimiz-
ing the POT loss defined by the prototype distribution Qj and empirical distribution Pj provides a
principled and unsupervised way to encourage the summary network to capture the set’s summary
statistics. Therefore, our plug-and-play framework can integrate a suite of summary networks and
realize efficient learning from new sets for both unsupervised and supervised tasks.
3.2	Application to Metric-based Few-shot Classification
As a challenging meta-learning problem, few-shot classification has recently attracted increasing at-
tention, where one representative method is metric-based few-shot classification algorithms. Taking
the ProtoNet (Snell et al., 2017) as an example, we provide a simple but effective method to im-
prove its classification performance with the help of POT and summary network, where we refer the
reader to ProtoNet for more details. ProtoNet represents each class by computing an M -dimensional
representation zj ∈ RM, with an embedding function fφ1 : Rd → RM, where we adopt the same
φ1 as the learnable parameters, following the feature extractor in summary network in Equation 1
for simplicity. Formally, ProtoNet adopts the average pooling to aggregate the embedded features
of the support points belonging to its class into vector Zj = / P(X洒 yji)∈sj fφι (Xji). ProtoNet
4
Published as a conference paper at ICLR 2022
Algorithm 1 The workflow of POT on minimizing the OT distance between Pj and Qj .
Require: Datasets D1:J, batch size m, learning rate α, initial summary network parameters φ, initial global
prototype matrix B, cost function C and hyper-parameter .
while B, φ has not converged do
Randomly choose j from 1, 2, .., J
Sample the real data {xji }im=1 from set j, which is denoted as empirical distribution Pj in Equation 3;
Compute the set representation hj = Softmax(Sφ ({xj i}im=1)) with summary network in Equation 1;
Represent the Qj with global prototype matrix B and statistics hj in Equation 4;
Compute the loss OT (Pj , Qj ) between Pj and Qj with Sinkhorn algorithm in Equation 6;
B — B + αgB, where gB — Vb [OTe(Pj,Qj)]; φ — φ + αgφ, where gφ — Vφ [OTe(Pj, Qj)];
end while
then compares the distance between a query point fφ1 (x) to the zj in the same embedding space.
Motivated by the summary network, we further introduce a feed-forward network gφ2 to map the
zj into the hj used to define Qj distribution over B. Therefore, the functions gφ2 and fφ1 can be
jointly optimized by minimizing the POT loss and the original classification loss in ProtoNet,
J Nj
min V (φ1, φ2,B) = LPOT + XX
CLS(yji,yji)	⑻
φ1,φ2,B
j=1 i=1
where hj is used for computing the POT loss, Nj the number of samples in class j, yji the pre-
dicted label for sample xji, conditioned on z1:J and fφ1 (xji), and CLS the classification loss. Only
introducing matrix B and gφ2 , whose parameters are usually negligible compared to fφ1 , our pro-
posed method can benefit the metric-based few-shot classification by enforcing the fφ1 to learn more
powerful representation zj of each class and feature fφ1 (x) of the query sample.
3.3	Application to Implicit Meta Generative models
Considering implicit meta generative modeling is still a challenging but important task in meta-
learning, we further present how to construct the model by introducing set representation hj as
summary statistics, where We consider GAN-based implicit models. Specifically, given Pj 〜pm，
we aim to construct a parametrized pushforward (i.e., generator) of reference Gaussian distribution
ρ, denoted as Tθ(∙,pj)]ρ, to approximate the marginal distribution Pj, where θ summarizes the
parameters of pushforward. Since it is unaccessible to the distribution of interest pj , we replace pj
with Pj and use the summary network to encode set Dj into hj as discussed above, which is further
fed into the generator serving as the conditional information, denoted as T⅛(z; hj), Z 〜 ρ. To
enforce the pushforward Tθ (z; hj) to fit the real distribution Pj as well as possible, we introduce a
discriminator fw following the standard GAN (Goodfellow et al., 2014). Generally, our GAN-based
model consists of three components. Summary network Sφ(∙) focuses on learning the summary
statistics hj by minimizing OT (Pj, Qj ). The pushforward aims to push the combination of a
random noise vector z and statistics hj to generate samples that resemble the ones from Pj , where
we simply adopt a concatenation for hj and z although other choices are also available. Besides,
fw tries to distinguish the “fake” samples from the “real” samples in set Dj . Therefore, we optimize
the implicit meta generative model by defining the objective function as:
min max V (B, φ, θ, w) = LPOT +
B,φ,θ w
1J
彳 £ Ex〜Pj [log fw(x)]+ Ez〜ρ[log(1 - fw(Tθ(z; hj))] (9)
J j=1
In addition to the standard GAN loss, we can also adopt the Wasserstein GAN (WGAN) of Arjovsky
et al. (2017) to approximate Pj . It is also flexibly to decide the input fed into fw . For example,
following the conditional GAN (CGAN) of Mirza & Osindero (2014), we can combine hj and data
points (generated or real), where the critic can be denoted as fw (x, hj) and fw (Tθ(z; hj), hj),
respectively. Since we focus on fitting the meta-distribution with the help of the summary network
and POT loss, we leave the problem-specific design of the generator, critic, and summary network as
future work for considerable flexibility in architectures. In Appendix A, we provide the illustration
of our proposed model in Fig. 3 and detailed algorithm in Algorithm 2.
4	Related Work
Learning Summary Representation of Set-input. There are two lines for learning the set rep-
resentation. The first line aims to design more powerful summary networks, which are reviewed in
5
Published as a conference paper at ICLR 2022
Introduction and Section 2.1 and omitted here due to the limited space. The another line assumes
a/some to-be-learned reference set(s), and optimizes the distance between the original sets (or fea-
tures of the observed data points) and the reference set(s) with OT or other distance measures, to
learn set representation. For example, RepSet (Skianis et al., 2020) computes some comparison
costs between the input sets and some to-be-learned reference sets with a network flow algorithm,
such as bipartite matching. These costs are then used as set representation in a subsequent neural
network. However, unlike our framework, RepSet does not allow unsupervised learning and mainly
focuses on classification tasks. The Optimal Transport Kernel Embedding (OTKE) (Mialon et al.,
2021) marries ideas from OT and kernel methods (ScholkoPf et al., 2002), and aligns features of a
given set to a trainable reference distribution. Wasserstein Embedding for Graph Learning (WEGL)
(Kolouri et al., 2021) also uses a similar idea to the linear Wasserstein embedding as a Pooling oPer-
ator for learning from sets of features. To the best of our knowledge, both of them view the reference
distribution as the barycenter and comPute the set-sPecific rePresentation by aggregating the features
(embedded with kernel methods) in a given set with adaPtive weight, defined by the transPort Plan
between the given set and the reference. Different from them, we assume J Probability distribu-
tions (rather than one reference distribution with an uniform measure) over these shared PrototyPes
by taking set rePresentations h1:J as the measures, to aPProximate the corresPonding J emPirical
distributions, resPectively. Then we naturally use the summary network as the encoder to comPute
h1:J, which can be jointly oPtimized with the shared PrototyPes by minimizing the POT loss in
an unsuPervised way. For a given set, we can directly comPute its rePresentation with summary
network, avoiding iteratively oPtimizing the transPort Plan between the given set and learned refer-
ence like OTKE and WEGL. These differences between the barycenter Problem and ours, which are
further described in APPendix B, lead to different views of set rePresentation learning and different
frameworks as well. To learn comPact rePresentations for sequential data, Cherian & Aeron (2020)
blend contrastive learning, adversarial learning, OT, and Riemannian geometry into one framework.
However, our work directly minimises the POT cost between emPirical distribution and the to-be-
learned distribution, Providing a laconic but effective way to learn set rePresentation.
Metric-based few-shot classification methods. Our method has a close connection with metric-
based few-shot classification algorithms. For examPle, MatchingNet (Vinyals et al., 2016) and Pro-
toNet (Snell et al., 2017) learned to classify samPles by comPuting distances to rePresentatives of
each class. Using an attention mechanism over a learned embedding of the suPPort set to Predict
classes for the query set, MatchingNet (Vinyals et al., 2016) can be viewed as a weighted nearest-
neighbor classifier aPPlied within an embedding sPace. ProtoNet (Snell et al., 2017) takes a class’s
PrototyPe to be the mean of its suPPort set in the learned embedding sPace, which further Performs
classification for an embedded query Point by finding the nearest class PrototyPe. ImPortantly, the
global PrototyPes in our PaPer are shared among all sets, which is different from the sPecific Proto-
tyPe for each class in ProtoNet but suitable for our case. Due to the flexibility of our method, we can
Project the average aggregated feature vector derived from the encoder in ProtoNet or MatchingNet,
into hj by introducing a simPle neural network, which can be jointly oPtimized with the encoder by
minimizing the classification loss and POT loss. Our novelty is that the POT loss can be naturally
used to imProve the learning of encoder while largely maintaining existing model architectures or
algorithms. Another recent work for learning multiPle centers is infinite mixture PrototyPes (IMP)
(Allen et al., 2019), which rePresents each class by a set of clusters and infers the number of clus-
ters with Bayesian nonParametrics. However, in our work, the centers are shared for all classes and
set-sPecific feature extracted from the summary network serves as the ProPortion of centers, where
the centers and summary network can be jointly learned with the POT loss.
Meta GAN-based Models. As discussed by Hong et al. (2020a), meta GAN-based models can be
roughly divided into optimization-based, fusion-based, and transformation-based methods. CloUatre
& Demers (2019) and Liang et al. (2020) integrated GANs with meta-learning algorithms to real-
ize the optimization-based methods, including model-agnostic meta-learning (MAML) (Finn et al.,
2017) and Reptile (Nichol et al., 2018). Hong et al. (2020b;c) fused multiple conditional images by
combining matching procedure with GANs, providing fusion-based methods. For transformation
based methods, Antoniou et al. (2017) and Hong et al. (2020a) combine only one image and the ran-
dom noise into the generator to produce a slightly different image from the same category, without
using the multiple images from same category. Besides, a recent work that connects existing sum-
mary network with GAN is MetaGAN (Zhang et al. 2018), which feeds the output of the summary
network into the generator and focuses on few-shot classification using MAML. The key differences
6
Published as a conference paper at ICLR 2022
of these models from ours is that we develop POT to capture each sets’ summary statistics, where
we can flexibly choose the summary network, generator, and discriminator for specific tasks.
5	Experiments
We conduct extensive experiments to evaluate the performance of our proposed POT in improving
summary networks, few-shot generation, and few-shot classification. Unless specified otherwise, we
set the weight of entropic constraint as =0.1, the maximum iteration number in Sinkhorn algorithm
as 200, and adopt the Adam optimizer (Kingma & Ba, 2015) with learning rate 0.001. We repeat all
experiments 5 times and report the mean and standard deviation on corresponding test datasets.
5.1	Experiments about POT loss in S ummary Network
To evaluate the effectiveness of POT in improving the summary network, we conduct three tasks
on two classical architectures: DeepSets (Zaheer et al. 2017) and Set Transformer (Lee et al.
2019), where the former uses standard feed-forward neural networks and the latter adopts the
attention-based network architecture. For Set Transformer and DeepSets, the summary network
is defined in Equation 1 and optimized by the task-specific loss; for Set Transformer(+POT) and
DeepSets(+POT), the summary network, defined as in Equations 1 and 7, is optimized by both the
POT loss and task-specific loss. More experimental details are provided in Appendix C.
Amortized Clustering with Mixture of Gaussians (MoGs): We consider the task of maximum
likelihood of MoGs with C components, denoted as P(x; θ) = Pc=I ∏cN (X | μc, diag (σ2)).
Given the dataset X = {x1:n} generated from the MoG, the goal is to train a neural network,
which takes X as input set and outputs parameters θ = {∏c, μc,σc}ι,c. Each dataset contains
n ∈ [100, 500] points on a 2D plane, each of which is sampled from one of C Gaussians. Table 1
reports the test average likelihood of different models with varying C∈ {4, 8}, where we set K=50
prototypes for all C. We observe that Set Transformer outperforms DeepSets largely, validating
the effectiveness of attention mechanisms in this task. We note both Set Transformer(+POT) and
DeepSets(+POT) improve their baselines, showing that the POT loss can encourage the summary
networks to learn more efficient summary statistics.
Point Cloud Classification: Here, we evaluate our method on the task of point cloud classification
using the ModelNet40 (Chang et al., 2015) dataset 1, which consists of3D objects from 40 different
categories. By treating each object as a point cloud, we represent it as a set of N vectors in R3 (x;
y; z-coordinates). Table 1 reports the classification accuracy, where we perform the experiments
with varying N ∈ {64, 1024}, set K = 40 prototypes. Clearly, both DeepSets and Set Transformer
can be improved by adding the POT loss. Notably, fewer points would lead to lower performance,
where the POT loss plays a more important role. Taking this task as the example, we further study
our model’s sensitivity to hyper-parameter in Fig. 4 of Appendix C.5.
Sum of Digits: Following Zaheer et al. (2017), we aim to
compute the sum of a given set of digits, where we consider
MNIST8m (Loosli et al., 2007), consisting of8 million instances
of 28 × 28 grey-scale stamps of digits in {0, ..., 9}. By ran-
domly sampling a subset of maximum M = 10 images from
MNIST8m, we build N = 100k “sets” of training, where we
denote the sum of digits in that set as the set-label. We construct
100k sets of test MNIST digits, where we vary the M starting
from 10 all the way up to 100. The output of the summary net-
work is a scalar, predicting the sum of M digits. In this case,
we adopt L1 as the task-specific loss and set K = 10 proto-
types. We show the accuracy of digit summation for different
algorithms in Fig. 2 and find that the POT loss can enhance the
summary networks to achieve better generalization. In this task,
we also explore the convergence rate and the learned transport
plan matrix of Sinkhorn algorithm in Fig. 5 of the Appendix C.6.
Figure 2: Accuracy of digit sum-
mation with image inputs, where all
models are trained on tasks of length
10 at most and tested on examples of
length up to 100.
5.2	Experiments on Few-shot Classification
To explore whether our proposed method can improve the metric-based few-shot classification, we
consider two commonly-used algorithms as the baselines, including ProtoNet (Snell et al., 2017)
1We adopt the point-cloud dataset directly from the authors of Zaheer et al. 2017
7
Published as a conference paper at ICLR 2022
Table 1: Test performance of different methods, where left table denotes the likelihood for MoG with varying
C (number of components), oracle is the likelihood of true parameters for the test data, and right denotes the
test accuracy for point cloud classification task With varying N (number of points).
Task	Test likelihood for MoG					Test accuracy for the point cloud classification				
Algorithm	C=4	C=5	C=6	C=7	C=8	N=64	N=128	N=256	N=512	N=1024
Oracle	-1.473	-1.660	-1.820	-1.946	-2.058	-	-	-	-	-
DeepSets	-1.809	-1.812	-1.897	-2.115	-2.261	79.14	82.51	84.62	85.74	-86.83-
	±0.015	±0.016	±0.017	±0.016	±0.014	±0.035	±0.028	±0.037	±0.045	±0.042
DeepSets(+POT)	-1.723	-1.743	-1.861	-2.078	-2.214	79.91	83.65	85.22	86.25	86.93
	±0.015	±0.017	±0.012	±0.018	±0.015	±0.050	±0.060	±0.055	±0.067	±0.075
Set Transformer	-1.501	-1.721	-1.859	-2.003	-2.106	79.01	82.31	84.46	85.82	-86.34-
	士0.006	±0.006	±0.007	±0.007	±0.007	±0.103	±0.117	±0.125	±0.114	±0.122
Set Transformer(+POT)	-1.486	-1.676	-1.828	-1.967	-2.084	80.00	83.32	85.64	86.51	86.84
	± 0.007	± 0.007	± 0.006	±0.007	±0.007	±0.111	±0.130	±0.121	±0.124	±0.115
and MatchNet (Vinyals et al., 2016). Denoting the feature extractor in each algorithm as fφ1, we
consider several popular backbones, including ResNet10 and ResNet34 (He et al., 2016). Recalling
the discussions in Section 3.2, to enforce fφ1 to learn more poWerful image features, We additionally
introduce matrix B and net gφ2 and learn the model by minimizing the POT loss and classification
errors. We perform the experiments on the CUB (Welinder et al., 2010) and miniImageNet (Ravi &
Larochelle, 2016). As a fine-grained feW-shot classification benchmark, CUB contains 200 different
classes of birds With a total of 11, 788 images of size 84 × 84 × 3, Where We split the dataset into 100
base classes, 50 validation classes, and 50 novel classes folloWing Chen et al. (2019). miniImageNet
is derived from ILSVRC-12 dataset (Russakovsky et al., 2015), consisting of 84 × 84 × 3 images
from 100 classes With 600 random samples in each class. We folloW the splits used in previous Work
(Ravi & Larochelle, 2016), Which splits the dataset into 64 base classes, 16 validation classes, and
20 novel classes. Table 2 reports the 5Way5shot and 5Way10shot classification results of different
methods on miniImageNet and CUB. We see that introducing the POT loss and summary netWork
can consistently improve over baseline classifiers, and the performance gain gradually increases
With the development of number of netWork layers. This suggests that our proposed plug-and-play
frameWork can be flexibly used to enhance the metric-based feW-shot classification, Without the
requirement of designing complicated models on purpose.
Table 2: 5Way5shot and 5Way10shot classification accuracy (%) on CUB and miniImageNet, respectively,
based on 1000 random trials. Here, (∙) is the P-Value computed with two-sample t-test, and P-Value with blue
(red) color means the increase (reduce) of performance when introducing POT loss.
Datasets	CUB		miniImageNet	
ProtoNet(resnet10) ProtoNet(+OT) (resnet10)	84.32 ± 0.51 84.44 ±0.51 (1e-7)	87.41 ± 0.49 87.69 ±0.53 (1e-33)	72.74±0.63 72.94 ±0.66 (1e-12)	78.14 ± 0.56 78.76 ± 0.49 (1e-146)
ProtoNet(resnet34) ProtoNet(+OT) (resnet34)	87.33 ± 0.48 88.34± 0.46 (0.0)	91.75 ± 0.47 92.17 ± 0.48 (1e-79)	73.99 ± 0.64 75.15± 0.63(1e-265)	78.64 ± 0.56 79.05 ± 0.52 (1e-60)
MatchNet (resnet10) MatchNet(+OT) (resnet10)	82.98± 0.56 83.64 ± 0.58 (1e-127)	85.97± 0.53 86.02± 0.56 (0.04)	68.82 ± 0.65~'一 68.95 ± 0.62( 1e-6)	72.06 ± 0.54~ 71.94 ± 0.56 (1e-6)
MatchNet (resnet34) MatchNet(+OT) (resnet34)	84.66± 0.55 85.50 ± 0.66 (1e-172)	86.32 ± 0.56 86.75 ± 0.61 (1e-57)	68.32 ± 0.66 68.51 ± 0.64 (1e-11)	72.41 ± 0.63 71.98 ± 0.59 (1e-53)
5.3	Experiments on Few- shot Generation
Here, we consider few-shot generation task to inVestigate the effectiVeness of our proposed implicit
meta generatiVe framework, where we consider CGAN (Mirza & Osindero, 2014) and DAGAN
(Antoniou et al., 2017) as baselines for their ability to generate conditional samples. For CGAN-
based models, we adopt summary network as the encoder to extract the feature Vector hj from a
giVen set Dj, which is further fed into the generator and discriminator as the conditional information.
Since the original DAGAN only assimilates one image into the generator, in our framework, we
replace the encoder in DAGAN with summary network to learn the set representation. For DAGAN-
based models, we adopt the same way with the original DAGAN to construct the real/samples for
the critic and explain here for clarity: we sample two sets (D1j, D2j ) from the same distribution or
category; then we represent the real samples as the combination of D1j and D2j and the fake ones
as the combination of D1j and D1j from the generator (conditioned on D1j). Different from our
framework that separately optimizes the summary network using the OT loss, all other models for
comparison in this paper jointly optimize the encoder (e.g., summary network) with the generator
by the generator loss. Besides, for a fair comparison, we also consider introducing the additional
reconstruction loss (mean square error, MSE) to optimize the generator and encoder in baselines. We
8
Published as a conference paper at ICLR 2022
consider the DeepSets as the summary network for its simple architecture. For 3D natural images,
we adopt the pretrained densenet (Iandola et al., 2014) to extract features from each data pints, and
take the features as the input to summary network. To evaluate the quality of generated samples,
We adopt commonly used metric Frechet Inception Distance (FID) (HeUsel et al., 2017), where We
only report the FID score (Heusel et al., 2017) considering the notable performance gap between our
model and the compared ones. We provide several examples on toy datasets to shoW the efficiency
of our proposed model in Appendix E.
Table 3: FID ] of images generated by different methods with varying unseen angles A on MNIST.
Algorithms	A=-160	A=-120	A=-80	A=-40	A=0	A=40	A=80	A=120	A=160
CGAN	227.94	203.59	211.74	231.63	228.35	222.87	195.30	202.69	202.35
	± 2.56	±2.21	±1.88	± 2.11	±1.94	± 2.02	±1.58	±1.75	±1.16
CGAN+MSE	225.56	201.09	209.11	222.54	223.61	220.18	193.75	200.12	201.13
	± 2.05	± 1.62	±0.91	± 2.13	±1.55	±1.47	±1.39	±1.08	± 2.33
CGAN+POT	213.30	193.42	196.56	217.28	210.27	206.44	181.05	190.19	191.22
	± 1.35	± 1.25	± 1.61	± 1.58	± 2.04	±1.78	± 0.99	±1.22	±1.37
DAGAN	169.85	201.58	157.95	204.23	218.42	196.70	160.35	198.24	164.64
	± 1.56	±1.98	±1.45	±2.31	±1.96	±1.75	±1.57	±2.14	±1.85
DAGAN+MSE	169.77	200.12	155.87	202.16	215.12	194.61	159.08	197.48	163.23
	±1.51	±2.10	±1.41	±1.89	±1.97	±2.10	±1.74	±2.05	±1.84
DAGAN+ POT	159.51	174.07	135.89	177.64	197.09	186.17	142.40	174.75	146.69
	士 1.57	士1.63	士1.17	±1.88	士1.46	±1.90	±1.33	±1.59	±1.47
Rotated MNIST: FolloWing Wu et al. (2020), We artificially transform each image in MNIST
dataset (LeCun, 1998) with 18 rotations (-180 to 180 by 20 degrees), leading to 18 distributions
characterized by angle A. We choose 9 interleaved distributions for training and the rest as unseen
distributions for testing. We consider the CGAN-based and DAGAN-based models, respectively.
During the test stage, for each unseen distribution, we randomly sample 1000 real images and gen-
erate 20 fake images based on every 20 real images and repeat this process 50 times (i.e., 1000/20),
resulting in 1000 generated samples for each method. We summarize the test performance in Table 3
with varying A. We can find that our proposed framework allows for better generalization to related
but unseen distributions at test time, indicating the POT loss can enforce the summary network to
capture more salient characteristics.
Natural Images: We further consider few-shot image generation on Flowers (Nilsback & Zisser-
man, 2008) and Animal Faces (Deng et al., 2009), where we follow seen/unseen split provided in
Liu et al. (2019). Flowers dataset contains 8189 images of 102 categories, which are divided into 85
training seen and 17 testing unseen categories; Animal Faces dataset contains 117, 574 animal faces
collected from 149 carnivorous animal categories, which are split into 119 training seen and 30 test-
ing unseen categories. We present the example images generated by DAGAN and DAGAN(+POT)
and network architectures in Appendix F for the limited space, where we also compute the FID
scores with the similar way in Rotated MNIST experiment. We find that our method achieves the
lowest FID and has the ability to generate more realistic natural images compared with baselines.
This indicates the summary network in our proposed framework can successfully capture the impor-
tant summary statistics within the set, beneficial for the few-shot image generation.
6	Conclusion
In this paper, we present a novel method to improve existing summary networks designed for set-
structured input based on optimal transport, where a set is endowed with two distributions: one
is the empirical distribution over the data points, and another is the distribution over the learnable
global prototypes. Moreover, we use the summary network to encode input set as the prototype
proportion (i.e., set representation) for global centers in corresponding set. To learn the distribution
over global prototypes and summary network, we minimize the prototype-oriented OT loss between
two distributions in terms of the defined cost function. Only additionally introducing the acceptable
parameters, our proposed model provides a natural and unsupervised way to improve the summary
network. In addition to the set-input problems, our plug-and-play framework has shown appealing
properties that can be applied to many meta-learning tasks, where we consider the cases of metric-
based few-shot classification and implicit meta generative modeling. Extensive experiments have
been conducted, showing that our proposed framework achieves state-of-the-art performance on
both improving existing summary networks and meta-learning models for set-input problems. Due
to the flexibility and simplicity of our proposed framework, there are still some exciting extensions.
For example, an interesting future work would be to apply our method into approximate Bayesian
computation for posterior inference.
9
Published as a conference paper at ICLR 2022
References
Michal Aharon, Michael Elad, and Alfred Bruckstein. K-SVD: An algorithm for designing over-
complete dictionaries for sparse representation. IEEE Transactions on signal processing, 54(11):
4311-4322, 2006.
Kelsey Allen, Evan Shelhamer, Hanul Shin, and Joshua Tenenbaum. Infinite mixture prototypes for
few-shot learning. In International Conference on Machine Learning, pp. 232-241. PMLR, 2019.
Jason M. Altschuler and Enric Boix-Adsera. Wasserstein barycenters can be computed in Polyno-
mial time in fixed dimension. J. Mach. Learn. Res., 22:44:1-44:19, 2021.
Antreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation generative adversarial
networks. arXiv preprint arXiv:1711.04340, 2017.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In International conference on machine learning, pp. 214-223. PMLR, 2017.
David M Blei, Andrew Y Ng, and Michael I Jordan. Latent Dirichlet allocation. Journal of machine
Learning research, 3(Jan):993-1022, 2003.
Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer
look at few-shot classification. In 7th International Conference on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.
Yanzhi Chen, Dinghuai Zhang, Michael U. Gutmann, Aaron C. Courville, and Zhanxing Zhu. Neu-
ral approximate sufficient statistics for implicit models. In 9th International Conference on Learn-
ing Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021.
Anoop Cherian and Shuchin Aeron. Representation learning via adversarially-contrastive optimal
transport. In International Conference on Machine Learning, volume 119, pp. 1820-1830, 2020.
Louis Clouatre and Marc Demers. Figr: Few-shot image generation with reptile. arXiv preprint
arXiv:1901.02199, 2019.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural
information processing systems, 26:2292-2300, 2013.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE Computer Society Conference on Computer Vision
and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA, pp. 248-255.
IEEE Computer Society, 2009.
Harrison Edwards and Amos J. Storkey. Towards a neural statistician. In 5th International Confer-
ence on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference
Track Proceedings, 2017.
SM Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Geoffrey E Hinton,
et al. Attend, infer, repeat: Fast scene understanding with generative models. Advances in Neural
Information Processing Systems, 29:3225-3233, 2016.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In International Conference on Machine Learning, pp. 1126-1135. PMLR,
2017.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. volume 27, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in
neural information processing systems, 30, 2017.
10
Published as a conference paper at ICLR 2022
Yan Hong, Li Niu, Jianfu Zhang, Jing Liang, and Liqing Zhang. Deltagan: Towards diverse few-shot
image generation with sample-specific delta. arXiv preprint arXiv:2009.08753, 2020a.
Yan Hong, Li Niu, Jianfu Zhang, and Liqing Zhang. Matchinggan: Matching-based few-shot image
generation. In 2020 IEEE International Conference on Multimedia and Expo (ICME), pp. 1-6.
IEEE, 2020b.
Yan Hong, Li Niu, Jianfu Zhang, Weijie Zhao, Chen Fu, and Liqing Zhang. F2gan: Fusing-and-
filling gan for few-shot image generation. In Proceedings of the 28th ACM International Confer-
ence on Multimedia, pp. 2535-2543, 2020c.
Gao Huang, Shichen Liu, Laurens Van der Maaten, and Kilian Q Weinberger. Condensenet: An
efficient densenet using learned group convolutions. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 2752-2761, 2018.
Forrest Iandola, Matt Moskewicz, Sergey Karayev, Ross Girshick, Trevor Darrell, and Kurt Keutzer.
Densenet: Implementing efficient convnet descriptor pyramids. arXiv preprint arXiv:1404.1869,
2014.
Mateusz Jurewicz and Leon StrOmberg-Derczynski. Set-to-sequence methods in machine learning:
a review. arXiv preprint arXiv:2103.09656, 2021.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd Interna-
tional Conference on Learning Representations, 2015.
Soheil Kolouri, Navid Naderializadeh, Gustavo K. Rohde, and Heiko Hoffmann. Wasserstein em-
bedding for graph learning. In 9th International Conference on Learning Representations, ICLR
2021, Virtual Event, Austria, May 3-7, 2021, 2021.
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set
transformer: A framework for attention-based permutation-invariant neural networks. In Interna-
tional Conference on Machine Learning, pp. 3744-3753. PMLR, 2019.
Weixin Liang, Zixuan Liu, and Can Liu. Dawson: A domain adaptive few shot generation frame-
work. arXiv preprint arXiv:2001.00576, 2020.
Ming-Yu Liu, Xun Huang, Arun Mallya, Tero Karras, Timo Aila, Jaakko Lehtinen, and Jan Kautz.
Few-shot unsupervised image-to-image translation. In 2019 IEEE/CVF International Conference
on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pp.
10550-10559. IEEE, 2019.
Gaelle Loosli, StePhane Canu, and Leon Bottou. Training invariant support vector machines using
selective sampling. Large scale kernel machines, 2, 2007.
Haggai Maron, Or Litany, Gal Chechik, and Ethan Fetaya. On learning sets of symmetric elements.
In International Conference on Machine Learning, pp. 6734-6744. PMLR, 2020.
Gregoire Mialon, Dexiong Chen, Alexandre d,Aspremont, and Julien Mairal. A trainable optimal
transport embedding for feature aggregation and its relationship to attention. In ICLR 2021-The
Ninth International Conference on Learning Representations, 2021.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. CoRR, abs/1411.1784,
2014.
Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv
preprint arXiv:1803.02999, 2018.
Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large num-
ber of classes. In Sixth Indian Conference on Computer Vision, Graphics & Image Processing,
ICVGIP 2008, Bhubaneswar, India, 16-19 December 2008, pp. 722-729. IEEE Computer Soci-
ety, 2008.
11
Published as a conference paper at ICLR 2022
Gabriel Peyre and Marco Cuturi. Computational optimal transport. Found. Trends Mach. Learn., 11
(5-6):355-607, 2019.
Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets
for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 652-660, 2017.
Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. 2016.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li Fei-
Fei. Imagenet large scale visual recognition challenge. Int. J. Comput. Vis., 115(3):211-252,
2015.
Bernhard Scholkopf, Alexander J Smola, Francis Bach, et al. Learning with kernels: SuPPort vector
machines, regularization, optimization, and beyond. MIT press, 2002.
Konstantinos Skianis, Giannis Nikolentzos, Stratis Limnios, and Michalis Vazirgiannis. Rep the
set: Neural networks for learning set representations. In International conference on artificial
intelligence and statistics, pp. 1410-1420. PMLR, 2020.
Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical networks for few-shot learning. In
Advances in Neural Information Processing Systems, pp. 4077-4087, 2017.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In Advances in neural information Processing systems, pp. 3104-3112, 2014.
Korawat Tanwisuth, Xinjie Fan, Huangjie Zheng, Shujian Zhang, Hao Zhang, Bo Chen, and
Mingyuan Zhou. A prototype-oriented framework for unsupervised domain adaptation. Advances
in Neural Information Processing Systems, 34, 2021.
Oriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Match-
ing networks for one shot learning. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg,
Isabelle Guyon, and Roman Garnett (eds.), Advances in Neural Information Processing Systems
29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016,
Barcelona, SPain, pp. 3630-3638, 2016.
Dongsheng Wang, Dandan Guo, He Zhao, Huangjie Zheng, Korawat Tanwisuth, Bo Chen, and
Mingyuan Zhou. Representing mixtures of word embeddings with mixtures of topic embeddings.
arXiv PrePrint arXiv:2203.01570, 2022.
Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, and
Pietro Perona. Caltech-ucsd birds 200. 2010.
Mike Wu, Kristy Choi, Noah Goodman, and Stefano Ermon. Meta-amortized variational inference
and learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp.
6404-6412, 2020.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, and
Alexander J. Smola. Deep sets. In NeurIPS, pp. 3391-3401, 2017.
Ruixiang Zhang, Tong Che, Zoubin Ghahramani, Yoshua Bengio, and Yangqiu Song. Metagan: An
adversarial approach to few-shot learning. NeurIPS, 2:8, 2018.
Huangjie Zheng and Mingyuan Zhou. Exploiting chain rule and Bayes’ theorem to compare proba-
bility distributions. Advances in Neural Information Processing Systems, 34, 2021.
Mingyuan Zhou, Haojun Chen, Lu Ren, Guillermo Sapiro, Lawrence Carin, and John Paisley. Non-
parametric Bayesian dictionary learning for sparse image representations. Advances in neural
information Processing systems, 22, 2009.
Mingyuan Zhou, Yulai Cong, and Bo Chen. Augmentable gamma belief networks. The Journal of
Machine Learning Research, 17(1):5656-5699, 2016.
12
Published as a conference paper at ICLR 2022
IQj=g 吸
Ar= 1
Figure 3: The overview of our proposed implicit meta generative framework, where We sample the
j-th distribution from training sets, feed the data points into the summary network, generate the fake
samples with the random noise and summary output as the input, discriminate the real/fake samples.
Algorithm 2 The workflow of our proposed implicit meta generative framework.
Require: Datasets Di： J, initial discriminator parameters w, initial generator parameters θ, initial
summary network parameters φ, initial matrix B, the cost function C, the number of critic itera-
tions per generator iteration ηcritic, the batch size m, learning rate α and the hyper-parameter .
while w, θ,B , φ has not converged do
Randomly choose j from 1, 2, .., J
fort = 1,∙∙∙ ,ηcritic do
Sample the real data set Dj = {xji }im=1 from j-th empirical distribution Pj ;
Embed the observed batch data into the statistics hj = Softmax(Sφ (Dj));
Sample a batch of prior samples {zi}im=1 from p(z);
gw 一 -Vwmm Pi=i[log fw(Xj) +log(1 - fw(Tθ(zi, hj)))];
W J W + agw
end for;
Represent the Qj with global prototype matrix B and set representation/statistics hj
Compute the loss OT(Pj, Qj) between Pj and Qj with Sinkhorn algorithm in Equation 6
gB J VB [OT(Pj , Qj)];
B J B + αgB ;
gφ J Vφ[OT(Pj,Qj)];
φ J φ + αgφ; see Algorithm 1 for more details;
Sample a batch of latent variables {z(i) }r=1 〜p(z);
gθ J vθ [m1 Pm=IIOg(I- fw (Tθ(Zi,hj)))];
θ J θ + αgθ ;
end while
A Algorithms and illustration of our proposed model
The pseudo code for the implicit meta generative modeling is provided in Algorithm 2.
13
Published as a conference paper at ICLR 2022
B The difference between our model and barycenter problem
In this section, we clarify the difference between Wasserstein barycenter and our method. Specif-
ically, for j-th distribution, we denoted Pj as its empirical distribution consisting of Nj samples,
expressed as Pj = PNjI Nkδχ^, Xji ∈ Rd. Notably, aj = [N^] ∈ ΣNj represents the probability
measure for distribution Pj .
For another thing, we can represent Pj with another to-be-learned distribution Qj , defined as
Qj = PkK=1 hjk δβk, βk ∈ Rd. Here bj = [hjk] ∈ ΣK is the probability measure for distribution
Qj, which can be computed using summary network Sφ (xi,1:Nj) and serves as the representation
for set j . And βk is the k-th prototype in the same space of the observed data points, which is the
k-th column of B ∈ RK×d, a learnable global prototype matrix. To optimize the β±K and the
summary network Sφ for computing hj , we minimize the average OT loss (between Qj and Pj ) for
all training sets. We rewrite the Equation (6) here for convenience:
J	Nj K	Nj K	J
Lot = m- J X XXCik-XX 储kι□储k I - Biφ J ΣS (θ^(ʃj, Qj)).
,	j=1	i k	i k	,	j=1
Usually, this equation can also be represented:
1J
mi 2 3n J E(OTKaj, bj)).
B,φ J
j=1
(10)
In terms of Wasserstein barycenter, we adopt the same notations for consistency. Following
(Altschuler & Boix-Adsera, 2021), given J empirical distributions Pi： J and their respective proba-
bility measures [a1, . . . , aJ] supported onRd and a vector λ ∈ ΣJ, their corresponding Wasserstein
barycenter can be viewed as another distribution Q, i.e., Q = PkK=1 mkδβk, βk ∈ Rd, where βk is
the k-th column of B, m = [mk] ∈ ΣK is the probability measure for distribution Q. Then we can
learn the barycenter (i.e., B and m) by minimizing
1J	1J
mm J £% W (Pj ,Q) = mm J EλjW (aj, m)	(II)
,m	j=1	,m	j=1
where above W(∙, ∙) denotes the squared 2-Wasserstein distance. By comparing the equation 11 and
equation 10, we can find that our model learns a Qj to approximate Pj for each distribution j but
“barycenter” problem learns a shared Q as the barycenter for all P1:J. Therefore, after minimizing
the average loss for all training sets with Equation (6), we can use the probability measure bj =
[hjk] (i.e., hj) to represent the empirical distribution Pj. Especially, we can directly map the test
set to its representation by using the summary network. However, since the probability measure
m in “barycenter” is shared by all distributions, it can not represent a specific set. Therefore, to
achieve the set representation, it might need to first compute the transport plan between test set and
the barycenter Q and then aggregate the data points (or features) within the test set by taking the
transport plan as the weight. Therefore, our model produces a more intuitive solution to learn the
set representation, which can take full advantage of the existing summary networks and provides a
promising tool for addressing set-input and meta-learning problems.
C Experimental settings ab out introducing POT loss into the
S ummary Networks
C.1	Details for amortized clustering with mixtures of Gaussians
We generate the 2D toy datasets following the Lee et al. (2019), where we additionally vary the C
(the number of components) from 4 to 8. Below, we present the detailed generation process about
the toy datasets:
1. Specify the number of components C for 2D toy dataset.
2. Generate the number of data points, n 〜Uniform(100,500).
3. Sample the mean vector for C components.
μc,d 〜Uniform(—4, 4), C =1,...,C, d = 1, 2
14
Published as a conference paper at ICLR 2022
4.	Sample the cluster labels.
π 〜Dir(1>) , Zi 〜Categorical(∏), i = 1,..., n, z% = 1,..., C
5.	Generate data from spherical Gaussian.
Xi 〜N (μzi, (0.3)2I), i = 1,...,n
C.2 Details about Set-Transformer-based and DeepSets-based architectures
used in MoGs experiments
DeepSets In terms of the DeepSets, the fφ1 in summary network contains 3 permutation-equivariant
layers with 256 channels followed by mean-pooling over the set structure. Then the resulting vector
representation zj of the set is then fed to a fully connected layer with 512 units followed by a linear
layer 512 X C(1 + 2 * 2), where C denotes the number of components. We use ELU activation
at all layers. To introduce the POT loss into the DeepSets, we further feed the zj into a fully con-
nected layer with 512 units followed by a 50-way softmax unit and also introduce the global matrix
B ∈ R2×50.
Set Transformer To perform the MoGs experiments, we adopt the same architecture for Set Trans-
former following Lee et al. (2019), whose parameters are reported in Table 4. To introduce the POT
loss, we also add the two fully connected layers with 256 units on the resulting vector zj followed
by a 50-way softmax unit, and a global prototype matrix B ∈ R2×50 .
C.3 Details about Set-Transformer-based and DeepSets-based architectures
USED IN SUM OF DIGITS
DeepSets Following the official code in Zaheer et al. (2017), we adopt the default architecture
to implement the DeepSets, where we first project the image into a 128-dimensional vector with
three convolutional layers and apply summary network on the 128-dimensional vectors. To build
DeepSets(+POT), we take the set representation zj after sum-pooling in summary network as the
input and introduce fully connected layer with 128 units followed by a 10-way softmax unit, and a
global prototype matrix B ∈ R128×10 .
Set Transformer For Set Transformer, we follow the similar structure used in MoGs experiments,
where we also project the image with three convolutional layers and output a scalar. To build Set
Transformer(+POT), we take the representation zj after sum-pooling in summary network as the
input and introduce fully connected layer with 128 units followed by a 10-way softmax unit, and a
center matrix B ∈ R128×10.
C.4 Details about Set-Transformer-based and DeepSets-based architectures
USED IN POINT CLOUD CLASSIFICATION
DeepSets For original DeepSets, we adopt the same architecture with Zaheer et al. (2017). In a
specific, the fφ1 in summary network contains 3 permutation-equivariant layers with 256 channels
followed by max-pooling over the set structure. Then the resulting vector representation zj of the
set is then fed to a fully connected layer with 256 units followed by a 40-way softmax unit. We
use Tanh activation at all layers and dropout on the layers after set-max-pooling (i.e., two dropout
operations) with 50% dropout rate. To introduce the POT loss into the DeepSets, we further feed the
zj into a fully connected layer with 256 units followed by a 40-way softmax unit, with 70% dropout
rate. Besides, we additionally introduce the center matrix B ∈ R3×40 .
Set Transformer We also adopt the same architecture to implement the Set Transformer, where we
summarize the parameters in Table 5, following Lee et al. (2019). To improve the Set Transformer
with POT loss, we also introduce a fully connected layer with 256 units followed by a 40-way
softmax unit, with 90% dropout rate, and a center matrix B ∈ R3×40 .
C.5 Parameter sensitivity
In the previous experiments, we fix the value of as 0.1, controlling the weight of the entropic
regularisation in the Sinkhorn algorithm. Notably, unless specified otherwise, we specify the con-
struction of C as Cik = 1 - cos (xji, βk). Therefore, the cost function provides an upper-bounded
15
Published as a conference paper at ICLR 2022
Table 4: Detailed architectures of Set Transformer used in the MoGs experiments, cited from Lee et al. (2019),
where C denotes the number of components.
Encoder			Decoder	
rFF	SAB	ISAB	Pooling	PMA
FC(128, ReLU)	SAB(128, 4)	ISABm(128, 4)	mean	PMA4(128, 4)
FC(128, ReLU)	SAB(128, 4广	ISABm(128"	FC(128, ReLU)	SAB(128, 4)
FC(128, ReLU)	-	-	FC(128, ReLU)	FC(C ∙ (1 + 2 ∙ 2),)
FC(128, ReLU「	-	-	FC(128, ReLU)	FC(C ∙ (1 + 2 ∙ 2),)
-	-	-	FC(C ∙ (1 + 2 ∙ 2),)	-
Table 5: Detailed architectures of Set Transformer used in the point cloud classification experiments, cited
from Lee et al. (2019).
Encoder		Decoder	
rFF	ISAB	Pooling	PMA
FC(256, ReLU)	ISAB(256, 4)	max	Dropout(0.5)
FC(256, ReLU)	ISAB(256, 4厂	-DroPoUt(0.5)-	PMA1(256, 4)
FC(256, ReLU)	-	FC(256, ReLU厂	DroPoUt(0.5)-
FC(256, )	-	DroPoUt(0.5)	FC(40,)一
-	-	FC(40,)一	-
(％)κK3JJJnv
----DeePSetS(+POT)
----DeepSets
70
68
66
64
62
60........................... ..........
0.01	0.05 0.10	0.501.00	10.00
Hyper-parameter in Sinkhom algorithm
Figure 4: Parameter sensitivity of DeepSets(+POT) on point cloud classification task, with varying , where
each object is represented as a set of N = 20 vectors.
positive similarity metric, making the has the corresponding reasonable range as a prior knowledge.
Here, we study our DeepSets(+POT)’s sensitivity to . We consider the point cloud classification
task and each object is represented as a set of N = 20 vectors. As shown in Fig. 4, we report the
performance of DeepSets(+POT) on point cloud classification task with varying , where DeepSets
serves as the baseline. It can be seen that our model is robust to the . Besides, all the results of
DeeepSets(+POT) with different are superior than that of DeeepSets, indicating the effectiveness
of our method. By fine-tuning for each dataset in each task, we might obtain better results than
those reported in our experiments. However, we aim to validate our method instead of exhaustively
tuning this hyper-parameter and thus we set = 0.1, which can achieve the acceptable result.
C.6 Convergence rate of Sinkhorn algorithm
In this paper, we set the maximum iteration number as Itermax = 200 in Sinkhorn algorithm for
all experiments. As shown in Fig. 5, we visualize the convergence rate of Sinkhorn algorithm,
where we consider the task about “sum of digits” (DeepSets+POT). The upper figure shows the
convergence rate of Sinkhorn algorithm. The bottom figure visualizes the transport plan matrix
with varying iterations. We find that the 200 iterations are typically enough for Sinkhorn algorithm
and we can learn a sparse transport plan matrix T when the algorithm converge. Notably, the
transport plan matrix needs to satisfy two marginal constraints, defined by the probability measures
of two distributions, respectively. Recall that the empirical distribution has an unchanged uniform
probability measure, so the learned transport plan matrix is dense for the Nj observed samples. In
terms of another distribution, its probability measure is the set-specific representation, weighting the
16
Published as a conference paper at ICLR 2022
Figure 5: Top: the convergence rate of Sinkhorn for c(x, y) = 1 - Cosine(x, y), and e = 0.1, as measured in
term of marginal constraint violation J PJ PNj ∣ul+1 -UijI, where l is the iteration index and U is the scaling
variable; please see page 67 in Peyre & Cuturi (2019) for more details. Bottom: evolution of the transport plan
matrix T = diag u(`) K diag v(`) computed at iteration of Sinkhorn’s iterations.
importance of K shared centers for corresponding set. Therefore, it is reasonable that transport plan
matrix is sparse for K centers.
D Experimental settings about few-shot classification
Denote the prototype for set j (computed by fφ1 ) in few-shot classification as cj . We consider two
backbones for fφ1, including ResNet10 and ResNet34, which produce the 512-dimensional cj. To
improve the metric-based few-shot classification with our framework, taking the cj as input, we
further construct the gφ2 . Specifically, we introduce a fully connected network with architecture
as 512 → 256 units with ReLU function followed by a X-way (X=64 for CUB, and X=128 for
miniImageNet) Softmax function and a center matrix B ∈ R512×X. We conduct 10000 tasks of the
training set Dtr to train the model while 1000 tasks of the test set Dte to evaluate the learned model.
And Dtr ∩ Dte = 0. We run 60 epochs to train the model on CUB and miniImageNet. The model
is trained using Adam optimizer with default settings (learning rate 1e - 3, β = (0.9, 0.999), and
= 1e - 8) on one Nvidia Geforce RTX3090 GPU.
E	Additional experimental results on few-shot generation
ABOUT TOY DATASETS
We test our algorithm through a series of synthetic data sets and realistic data sets. For synthetic
datasets, we set Tθ , fw and Sφ as fully connected neural networks, where Tθ , fw have 4 hidden
layers and fφ1 and gφ2 (we adopt DeepSets) have 3 hidden layers. Each layer has 200 nodes, and
the activation function is chosen as RELU, where we adopt the softmax in the final layer.
Normal distribution on 2D toy data: We first consider the 2D normal case, where the training
data contains 10K sets and each set contains 100 data points from N(μ, Σ). We sample the mean,
variance, and covariance from U [-5, 5], U [1, 2], and U [-0.5, 0.5], respectively. Fig. 6 shows
the real (gray points) and generated samples (red points) by different models given unseen test sets,
where we only consider CGAN-based methods for the simple toy data. We find that our model (third
column) can improve the resistance to mode collapse compared with CGAN+MSE (second column)
and better fit the unseen test distributions than CGAN (first column). This result indicates the POT
loss can spur the summary network to capture more desired statistics for unseen distributions.
17
Published as a conference paper at ICLR 2022
Figure 6: Examples of few-shot generation for two dimensional GaUssian distributions, where We visualize the
real samples from the true distributions (gray points) and generated samples (red points) by different models
(from first to third column: CGAN, CGAN+MSE and our proposed cGan+POT), where we also plot the
contour of each Gaussian distribution.
One-dimensional Gaussian distributions: In this case, we generate another collection of synthetic
1 - D datasets based on Gaussian parametric family, where the means and variances are sampled
from U[-1,1] and U[0.5, 2] respectively. The training data contains 10K sets each containing 50
samples. We also visualize the pdfs of gaussian distributions with randomly means and variance
in Fig 7, which are used to sample test data, and show the 500 data points generated by the push-
forward. For this experiment, we set the dimension of Z and summary vector S as 2.
Multi-family distribution on 1D toy data: To validate if our proposed model can capture many
types of distributional families simultaneously, we construct a collection of synthetic 1-D datasets
each containing 100 samples from either an Exponential, Gaussian or Laplacian distribution with
equal probability. For Gaussian and Laplacian distributions, means and variances are sampled from
U [-1, 1] and U [0.5, 2] respectively; for Exponential distributions, rates are sampled from U [0.5, 2].
Fig. 8 visualizes the pdfs of six one-dimensional test distributions with different means and variances
and the generated data points. It is interesting to observe that the generated data points can fit the
corresponding pdf well, indicating our model can generalize to different distributions with varying
parameters. Besides, our model performs slightly worse on the Exponential distributions, perhaps
attributing to the fact that it is the only non-symmetric distribution.
F Details about natural image generations and the results
We use denseNet proposed by Huang et al. (2018) as the backbone of summary network, then
a pooling operation is conducted as Zaheer et al. (2017) does. And a 2-layer fully connected
network with ReLU activation function is finally employed to embed the 4096-D visual features
into the corresponding 512-D set representations hj. As for conditional generator (conditioned
on set representations as well as Gaussian noise), we introduce a 2-layer embedding network
[100→600→100] with LeakyReLU activation function to embed the input noise n. Besides, we
use a 5-layer deconvolution network [ConvTranspose2d(X + 100, 512, 4, 1, 0)→ ConvTrans-
pose2d(512, 256, 4, 2, 1)→ConvTranspose2d(512, 128, 4, 2, 1)→ConvTranspose2d(128, 64, 4,
2, 1)→ConvTranspose2d(64, 3, 4, 2, 1)], where X = 64, 128 for oxford and animal face datasets
respectively with BatchNorm along channels and ReLU activation function to deconvolute the con-
catenated noise embeddings and set representations cat([n, hj]) as fake output images xf ake :
RN×3×64×64. Finally, a 5-layer discriminator network [Conv2d(2*3, 64, 4, 2, 1)→Conv2d(64,
128, 4, 2, 1))→Conv2d(128, 256, 4, 2, 1)→Conv2d(256, 512, 4, 2, 1)→Conv2d(512, 1, 4, 1, 0)]
with BatchNorm as well as LeakyReLU activation function at the first fourth deconvolutional layers
and the last layer without BatchNorm while with sigmoid activation funtion to distinguish the true
or fake generated images. We present the generated results in Figure 9.
18
Published as a conference paper at ICLR 2022
Figure 7: Examples of few-shot generation for one dimensional Gaussian distributions, where We
visualize the generated samples by our GAN+POT (conditioned on the test samples from the unseen
distribution) and the PDF of the unseen true distribution.
Figure 8: Examples of few-shot generation for multi-distributions, where we visualize the generated
samples (green) by our GAN+POT (conditioned on the test samples from the unseen distribution)
and the PDF (red) of the unseen true distribution.
19
Published as a conference paper at ICLR 2022
(a) Real images
Above: Animal Face
Below: Oxford flower
(b) Fake images generated by DAGAN
Above: Animal Face
Below: Oxford flower
■■■› I
谕∙
(C) Fake images generated by DAGAN+OT
Above: Animal Face
Below: Oxford flower
Figure 9: Examples of few-shot generation for natural images, where the images are generated by DAGAN
(second column) and DAGAN+POT (third column) conditioned on 3 different categories on Oxford(flower)
and animal face datasets. The FID ] scores of DAGAN and DAGAN+POT on Oxford are 97.25 and 91.78,
respectively. The FIDs of DAGAN and DAGAN+POT on animal face are 139.14 and 131.51.
20