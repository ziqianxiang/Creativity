Published as a conference paper at ICLR 2022
The Geometry of Memoryless Stochastic Pol-
icy Optimization in Infinite-Horizon POMDPs
Johannes Muller
Max Planck Institute for Mathematics in the Sciences, Leipzig, Germany
jmueller@mis.mpg.de
Guido Montufar
Department of Mathematics and Department of Statistics, UCLA, CA, USA
Max Planck Institute for Mathematics in the Sciences, Leipzig, Germany
montufar@math.ucla.edu
Ab stract
We consider the problem of finding the best memoryless stochastic policy for an
infinite-horizon partially observable Markov decision process (POMDP) with fi-
nite state and action spaces with respect to either the discounted or mean reward
criterion. We show that the (discounted) state-action frequencies and the expected
cumulative reward are rational functions of the policy, whereby the degree is de-
termined by the degree of partial observability. We then describe the optimization
problem as a linear optimization problem in the space of feasible state-action fre-
quencies subject to polynomial constraints that we characterize explicitly. This
allows us to address the combinatorial and geometric complexity of the optimiza-
tion problem using recent tools from polynomial optimization. In particular, we
estimate the number of critical points and use the polynomial programming de-
scription of reward maximization to solve a navigation problem in a grid world.
1 Introduction
Markov decision processes (MDPs) were introduced by Bellman (1957) as a model for sequential
decision making and optimal planning (see, e.g., Howard, 1960; Derman, 1970; Puterman, 2014).
Many algorithms in reinforcement learning rely on the ideas and methods developed in the context of
MDPs (see, e.g., Sutton & Barto, 2018). Often in practice, the decisions need to be made based only
on incomplete information of the state of the system. This setting is modeled by partially observable
Markov decision processes (POMDPs) introduced by Astrom (1965) (for a historical discussion see
Monahan, 1982), which have become an important model for planning under uncertainty. In this
work we pursue a geometric characterization of the policy optimization problem in POMDPs over
the class of stochastic memoryless policies and its dependence on the degree of partial observability.
It is well known that acting optimally in POMDPs may require memory (AStrom,1965). A POMDP
with unlimited memory policies can be modeled as a belief state MDP, where the states are replaced
by probability distributions that serve as sufficient statistics for the previous observations (Kaelbling
et al., 1998; Murphy, 2000). Finding an optimal policy in this class is PSPACE-complete for finite
horizons (Papadimitriou & Tsitsiklis, 1987) and undecidable for infinite horizons (Madani et al.,
2003; Chatterjee et al., 2016). Therefore, it is of interest to consider POMDPs with constrained
policy classes. A natural class to consider are memoryless policies, also known as reactive or Markov
policies, which select actions based solely on the current observations. In this case, it is useful to
allow the actions to be selected stochastically, which not only allows for better solutions but also
provides a continuous optimization domain (Singh et al., 1994).
Although they are more restrictive than policies with memory, memoryless policies are attractive as
they are easier to optimize and are versatile enough for certain applications (Tesauro, 1995; Loch &
Singh, 1998; Williams & Singh, 1999; Kober et al., 2013). In fact, finite-memory policies can be
modeled in terms of memoryless policies by supplementing the state of the system with an external
memory (Littman, 1993; Peshkin et al., 1999; Icarte et al., 2021). Hence theoretical advances on
memoryless policy optimization are also of interest to finite-memory policy optimization. Theoret-
1
Published as a conference paper at ICLR 2022
ical aspects and optimization strategies over the class of memoryless policies have been studied in
numerous works (see, e.g., Littman, 1994; Singh et al., 1994; Jaakkola et al., 1995; Loch & Singh,
1998; Williams & Singh, 1999; Baxter et al., 2000; Baxter & Bartlett, 2001; Li et al., 2011; Az-
izzadenesheli et al., 2018). However, finding exact or approximate optimal memoryless stochastic
policies for POMDPs is still considered an open problem (Azizzadenesheli et al., 2016), which is
NP-hard in general (Vlassis et al., 2012). One reason for the difficulties in optimizing POMDPs
is that, even in a tabular setting, the problem is non-convex and can exhibit suboptimal strict local
optima (Bhandari & Russo, 2019). For memoryless policies the expected cumulative reward is a
linear function of the corresponding (discounted) state-action frequencies. In the case of MDPs the
feasible set of state-action frequencies is known to form a polytope, so that the optimization problem
can be reduced to a linear program (Manne, 1960; De Ghellinck, 1960; d’Epenoux, 1963; Hordijk
& Kallenberg, 1981). On the other hand, to the best of our knowledge, for POMDPs the specific
structure of the feasibility constraints and the optimization problem have not been studied, at least
not in the same level of detail (see related works below).
Related works In MDPs, the (discounted) state-action frequencies form a polytope resp. a com-
pact convex set in the finite resp. countable state-action cases, whereby the extreme points are
given by the state-action frequencies of deterministic stationary policies (Derman, 1970; Altman
& Shwartz, 1991). Further, Dadashi et al. (2019) showed that the set of state value functions in finite
state-action MDPs is a finite union of polytopes. The set of stationary state-action distributions of
POMDPs has been studied by Montufar et al. (2015) highlighting a decomposition into infinitely
many convex subsets whose dimensions depend on the degree of observability. Although this de-
composition can be used to localize optimal policies to some extent, a description of the pieces in
combination is still missing, needed to capture the properties of the optimization problem. We will
obtain a detailed description in terms of finitely many polynomial constraints with closed form ex-
pressions and bound their degrees in terms of the observation mechanism. This yields a polynomial
programming formulation of POMDPs generalizing the linear programming formulation of MDPs.
This is different to the formulation as a quadratically constrained problem by Amato et al. (2006),
where the number and degree of constraints do not depend on the observability. Finally, Cohen &
Parmentier (2018) described finite horizon POMDPs as a mixed integer linear program.
Grinberg & Precup (2013) showed that the expected mean reward is a rational function and obtained
bounds on the degree of this function. We generalize this to the setting of discounted rewards and
refine the result by relating the rational degree to the degree of observability. For both MDPs and
POMDPs the expected cumulative reward is known to be a non-convex function of the policy even
for tabular policy models (Bhandari & Russo, 2019). Nonetheless, for MDPs critical points can be
shown to be global maxima under mild conditions. In contrast, for POMDPs or MDPs with linearly
restricted policy models, it is known that non-global local optimizers can exist (Baxter et al., 2000;
Poupart et al., 2011; Bhandari & Russo, 2019). However, nothing is known about the number of local
optimizers. We will present bounds on the number of critical points building on our computation of
the rational degree and feasibility constraints.
The structure of the expected cumulative reward has been studied in terms of the location of the
global optimizers and the existence of local optimizers. Most notably, itis well known that in MDPs
there always exist optimal policies which are memoryless and deterministic (see Puterman, 2014). In
the case of POMDPs, optimal memoryless policies may need to be stochastic (see Singh et al., 1994).
Montufar et al. (2015); Montufar & RaUh (2017); Montufar et al. (2019) obtained upper bounds on
the number of actions that need to be randomized by these policies, which in the worst case is equal
to the number of states that are compatible with the observation. Although we do not improve these
results (which are indeed tight in some cases), our description of the expected cumulative reward
function leads to a simpler proof of the bounds obtained by Montufar et al. (2015).
Neyman (2003) considered stochastic games as semialgebraic problems showing that the minmax
and maxmin payoffs in an n-player game are semialgebraic functions of the discount factor. Al-
though this is not directly related to our work, we take a similar philosophy. We pursue a semi-
algebraic description of the feasible set of discounted state-action frequencies in POMDPs, which
is closely related to the general spirit of semialgebraic statistics, where this is usually referred to
as the implitization problem (Zwiernik, 2016). Based on this we characterize the properties of the
optimization problem by its algebraic degree, a concept that has been advanced in recent works on
polynomial optimization (Bajaj,1988; Nie & Ranestad, 2009; OzlUm Cehk et al., 2021).
2
Published as a conference paper at ICLR 2022
Contributions We obtain results for infinite-horizon POMDPs with memoryless stochastic poli-
cies under the mean or discounted reward criteria which can be summarized as follows.
1.	We show that the state-action frequencies and the expected cumulative reward can be written as
fractions of determinantal polynomials in the entries of the stochastic policy matrix. We show that
the degree of these polynomials is directly related to the degree of observability (see Theorem 4).
2.	We describe the set of feasible state-action frequencies as a basic semialgebraic set, i.e., as the
solution set to a system of polynomial equations and inequalities, for which we also derive closed
form expressions (see Theorem 16 and Remark 18).
3.	We reformulate the expected cumulative reward optimization problem as the optimization of
a linear function subject to polynomial constraints (see Remark 19), which we use to solve a
navigation problem in a grid world (see Appendix F). This is a POMDP generalization of the
dual linear programming formulation of MDPs (Kallenberg, 1994; Puterman, 2014).
4.	We present two methods for computing the number of critical points, which rely, respectively, on
the rational degree of the expected cumulative reward function and the geometric description of
the feasible set of state-action frequencies (see Theorem 20, Proposition 21 and Appendix D).
2 Preliminaries
We denote the simplex of probability distributions over a finite set X by ∆χ. An element μ ∈ ∆χ
is a vector with non-negative entries μχ = μ(χ), X ∈ X adding to one. We denote the set of Markov
kernels from a finite set X to another finite set Y by ∆YX. An element Q ∈ ∆YX is a |X| × |Y| row
stochastic matrix with entries Qxy = Q(y|x), x ∈ X, y ∈ Y. Given Q(1) ∈ ∆YX and Q(2) ∈ ∆YZ
we denote their composition into a kernel from X to Z by Q(2) ◦ Q(1) ∈ ∆XZ . Given p ∈ ∆X
and Q ∈ ∆χ We denote their composition into a joint probability distribution by P * Q ∈ ∆χ×γ,
(p * Q)(x, y) ：= p(x)Q(y∣x). The support of V ∈ RX is the set SuPP(V) = {x ∈ X: vχ = 0}.
A partially observable Markov decision process or shortly POMDP is a tuple (S, O, A, α, β, r). We
assume that S, O and A are finite sets which we call state, observation and action space respectively.
We fix a Markov kernel α ∈ ∆SS×A which we call transition mechanism and a kernel β ∈ ∆SO which
we call observation mechanism. Further, we consider an instantaneous reward vector r ∈ RS×A .
We call the system fully observable if β = id1, in which case the POMDP simplifies to a Markov
decision process or shortly MDP.
As policies we consider elements π ∈ ∆OA and call the Markov kernel τ = π ◦ β ∈ ∆SA its
corresponding effective policy. A policy induces transition kernels Pπ ∈ ∆SS××AA and pπ ∈ ∆SS by
Pn(s0,a0∣s,a)	:=	α(s0∣s,	a)(π ◦ β)(a0∣s0)	and	p∏(s0∣s)	：= ɪ2 (∏	◦ β)(a∣s)α(s0∣s,	a).
a∈A
For any initial state distribution μ ∈ ∆s, a policy π ∈ △? defines a Markov process on SXA with
transition kernel Pn which we denote by Pπ,μ. For a discount rate Y ∈ (0,1) and Y = 1 we define
∞
Rμ(∏) ：=Ep∏,μ (1 - Y) X Ytr(st, at)
t=0
and Rμ(∏) ：= lim Epπ,μ
1	T→∞
1 T-1
T t=0
st , at ) ,
called the expected discounted reward and the expected mean reward, respectively. The goal is to
maximize this function over the policy polytope △AO . For a policy π we define the value function
Vγn ∈ RS via Vγn(s) ：= Rδγs (π), s ∈ S, where δs is the Dirac distribution concentrated at s. A
short calculation shows that Rμ(π) = Es ° r(s, a)n；,*(s, a) = hr, ηγ,μi
where
η∏,μ(s,a)
f(1 - Y) p∞=0 γtPn,μ(st = s,at = a),
IlimT→∞ T PT—1 Pn,μ(st = s,at = a),
S×A (Zahavy et al., 2021),
ifY∈(0,1)	(1)
ifY= 1.	(1)
Here, ηγ,μ is an element of Δs×a called expected (discounted) state-action frequency (Derman,
1970), (discounted) visitation/occupancy measure or on-policy distribution (Sutton & Barto, 2018).
Denoting the state marginal of ηYf,μ by PnN ∈ ∆s we have ηγ,μ(s, a) = PnF(S)(π ◦ β)(a∣s). We
recall the following well-known facts.
1More generally, the system is fully observable if the supports of {β(∙∣s)}s∈s are disjoint subsets of O.
3
Published as a conference paper at ICLR 2022
Proposition 1 (Existence of state-action frequencies and rewards). Let (S, O, A, α, β, r) be a
POMDP Y ∈ (0,1] and μ ∈ ∆S. Then η(,μ, ρπμ and RY (π) exist for every π ∈ ∆A and μ ∈ ∆S
and are continuous in Y ∈ (0,1] for fixed π and μ.
For γ = 1 we work under the following standard assumption in the (PO)MDP literature2 .
Assumption 2 (Uniqueness of stationary disitributions). If Y = 1, we assume that for any policy
π ∈ ∆OA there exists a unique stationary distribution η ∈ ∆S×A of Pπ .
The following proposition shows in particular that for any initial distribution μ, the infinite time
horizon state-action frequency η<,μ is the unique discounted stationary distribution of Pn.
Proposition 3 (State-action frequencies are discounted stationary). Let (S, O, A, α, β, r) be a
POMDP γ ∈ (0,1] and μ ∈ ∆s. Then η(,μ is the unique element in Δs×a satisfying the dis-
counted Stationarity equation η(,μ = YPnηγ>μ + (1 - Y)(μ * (π ◦ β)). Further, Pnμ is the unique
element in ∆s satisfying Pnμ = YpTPnμ + (1 — Y)μ.
We denote the set of all state-action frequencies in the fully and in the partially observable case by
NY := {η∏," ∈ ∆s×A | π ∈ ∆A}	and Nμ,β := {η∏,μ ∈ ∆s×A | π ∈ ∆O}.
We have seen that the expected cumulative reward function RY: △，→ R factorises according to
△A —→ ∆A --→ Nγμ,β → R, ∏ → ∏ ◦ β → η∏,μ → hr, η∏,μis×A.
This is illustrated in Figure 1. We make use of this decomposition in two different ways. First,
in Section 3 We study the algebraic properties of the parametrization Ψμ of the set of state-action
frequencies NY$. In Section 4 We derive a description of NYμ,β via polynomial inequalities.
Figure 1: Geometry of a POMDP with two states, two actions and two observations. The top shows
the observation policy polytope △OA ; the associated state policy polytope △SA (yellow) along with
its subset of effective policies ∆S,β (blue); and the corresponding sets of discounted state-action
frequencies in the simplex Δs×a (a tetrahedron in this case). The bottom shows the graph of the
expected cumulative discounted reward R as a function of the observation policy π ; the state policy
τ ; and the discounted state-action frequencies η . We characterize the parametrization and geometry
of these domains and the structure of the expected cumulative reward function.
2Assumption 2 is weaker than ergodicity, for which well known criteria exist. For γ < 1 the assumption is
not required, since the discounted stationary distributions are always unique.
4
Published as a conference paper at ICLR 2022
3 The Parametrization of Discounted State-Action Frequencies
In this section we show that the discounted state-action frequencies, the value function and the
expected cumulative reward of POMDPs are rational functions and relate their rational degree, which
can be interpreted as a measure of their complexity, to the degree of observability. Here, we say that
a function is a rational function of degree at most k if it is the fraction of two polynomials of degree
at most k. By Cramer’s rule (see Appendix B.2), it holds that
η∏,“(s, a) = (π ◦ β)(a∣s)ρ∏,μ(s) = (π ◦ β)(a∣s) ∙ (1 -Y) ∙ KIIlYpT)j ,
where (I - YpT)μ denotes the matrix obtained by replacing the s-row of I - YpT with μ. Since p∏
depends linearly on π and the determinant is a polynomial, this is a rational function in the entries
of π. For the degree, we show the following result in Appendix B.1.
Theorem 4 (Degree of POMDPs). Let (S, O, A, α, β, r) be a POMDP μ ∈ ∆s be an initial dis-
tribution and γ ∈ (0,1) a discount factor The state-action frequencies ηj,μ and ργ,μ, the value
function Vn and the expected cumulative reward Rμ(π) are rational functions with common de-
nominator in the entries of the policy π. Further, if they are restricted to the subset Π ⊆ ∆AO of
policies which agree with a fixed policy π0 on all states outside of O ⊆ O, they have degree at most
I {s ∈ S | β(o∣s) > 0 for some o ∈ O}∣ .
Hence, the number of states that are compatible with o determines the algebraic complexity of the
discounted state-action frequencies, the value function and the reward function. Various refinements
of the theorem are presented in Appendix B.1. For the mean reward case and under an ergodicity
assumption, Grinberg & Precup (2013) showed that the stationary distributions are a rational func-
tion of degree of most |S | of the policy. From Theorem 4 we can derive multiple implications (see
also Appendix D.5.2 for implications on the optimization landscape):
Corollary 5 (Feasible state-action frequencies and value functions form semialgebraic sets). Con-
sider a POMDP (S, O, A, α, β, r) and let μ ∈ ∆s be an initial distribution and Y ∈ (0,1) a
discount factor. The set of discounted state-action frequencies and the set of value functions are
semialgebraic sets3.
Proof. By Theorem 4, both sets possess a rational and thus a semialgebraic parametrization and are
Semialgebraic by the Tarski-Seidenberg theorem (Neyman, 2003).	□
We compute the defining linear and polynomial (in)equalities of the set of feasible state-action fre-
quencies in Section 4 for MDPs and POMDPs respectively, which shows in particular that also
in the mean case the state-action frequencies form a semialgebraic set. The special properties of
degree-one rational functions, which we elaborate in the Appendix B.3, imply the following results.
The first one is a refinement of Dadashi et al. (2019, Lemma 4), stating that linear interpolation
between two policies that differ on a single state leads to a linear interpolation of the corresponding
value functions. We generalize this to state-action frequencies, explicitly compute the interpolation
speed and describe the curves obtained by interpolation between arbitrary policies. Further, our
formulation extends to the mean reward case (see Remark 43).
Proposition 6. Let (S, A, α, r) be an MDP and Y ∈ (0, 1). Further, let π0, π1 ∈ ∆SA be two
policies that differ on at most k states. For any λ ∈ [0,1] let Vλ ∈ RS and ηf ∈ Δs×a denote the
value function and state-action frequency belonging to the policy π0 + λ(π1 - π0) with respect to
the discount factor y, the initial distribution μ and the instantaneous reward r. Then the rational
degrees of λ → Vλ and λ → ηλ are at most k. Ifthey differ on at most one state s ∈ S then
Vλ = V0 + c(λ) ∙ (V1 - V0) and ημ = ημ + c(λ) ∙ (ημ - ημ) forall λ ∈ [0,1],
where
c(λ)
det(I - Yp1 )λ
det(I - Ypλ)
det(I - Yp1 )λ
(det(I - Ypι) - det(I - Yp0))λ + det(I - Yp0)
3A semialgebraic set is a set defined by a number of polynomial inequalities or a finite union of such sets;
for details see Appendix A.2.
5
Published as a conference paper at ICLR 2022
In particular, for a blind controller with two actions the set of feasible value functions and the set of
feasible state-action frequencies are pieces of curves with rational parametrization of degree at most
k = |S |. By Theorem 4, the cumulative reward of (PO)MDPs is a degree-one rational function in
every row of the (effective) policy. Since degree-one rational functions attain their maximum in a
vertex (Corollary 39), we immediately obtain the existence of an optimal policy which is determin-
istic on every observation from which the state can be reconstructed, which has been shown using
other methods by Montufar et al. (2015).
Proposition 7 (Determinism of optimal policies). Let (S, O, A, α, β, r) be a POMDP μ ∈ ∆s be
an initial distribution and γ ∈ (0, 1) a discount factor and let π ∈ ∆AO be an arbitrary policy and
denote the set of observations o such that |{s ∈ S | β (o|s) > 0}| ≤ 1 by O. Then there is a policy
π, which is deterministic on every o ∈ O such that RY(∏) ≥ Rμ(π).
Proof. For o ∈ O, the reward function restricted to the o-component of the policy is a rational
function of degree at most one. By Corollary 39 (see Appendix B.3.2), there is a policy π, which is
deterministic on o and satisfies Rμ(∏) ≥ Rμ(∏). Iterating over o ∈ O yields the result.	口
On observations which can be made from more than one state, bounds on the required stochasticity
were established by Montufar & Rauh (2017); Montufar et al. (2019).
4	The Set of Feasible Discounted State-Action Frequencies
In Corollary 5, we have seen that the state-action frequencies form a semialgebraic set. Now we
aim to describe its defining polynomial inequalities. In the case of full observability, the feasible
state-action freqencies are known to form a polytope (Derman, 1970; Altman & Shwartz, 1991)
which is closely linked to the dual linear programming formulation of MDPs (Hordijk & Kallen-
berg, 1981), see also Figure 1. We first describe the combinatorial properties of this polytope (see
Appendix C.1.2) and extend the result to the partially observable case, for which we obtain explicit
polynomial inequalities induced by the partial observability under a mild assumption. Most proofs
are postponed to Appendix C. In Section 5 we discuss how the degree of these defining polynomials
allows us to upper bound the number of critical points of the optimization problem. We use the fol-
lowing explicit version of the classic characterization of the state-action frequencies as a polytope
(see Appendix C.1).
Proposition 8 (Characterization of Nμ). Let (S, A, α, r) be an MDP, μ ∈ ∆s be an initial distri-
bution and γ ∈ (0, 1]. It holds that
NY = ∆s×A ∩ {η ∈ Rs×a | hwγs , η iS×A = (1 - Y)μs for S ∈ S}	(2)
where WY := δs 0 1/ — γα(s∣∙, ∙). For Y ∈ (0,1), Δs×a can be replaced by [0, ∞)s×a in ⑵.
Now we turn towards the partially observable case and introduce the following notation.
Definition 9 (Effective policy polytope). We call the set of effective policies τ = π ◦ β ∈ ∆SA the
effective policy polytope and denote it by ∆SA,β .
Note that ∆SA,β is indeed a polytope since it is the image of the polytope ∆OA under the linear
mapping π 7→ π ◦ β = βπ. Hence, we can write it as an intersection ∆SA,β = ∆SA ∩ U ∩ C,
where U, C ⊆ RS×A are an affine subspace and a polyhedral cone and describe a finite set of linear
equalities and a finite set of linear inequalities respectively.
Defining linear inequalities of the effective policy polytope Obtaining inequality descriptions of
the images of polytopes under linear maps is a fundamental problem that is non-trivial in general. It
can be approached algorithmically, e.g., by Fourier-Motzkin elimination, block elimination, vertex
approaches, and equality set projection (Jones et al., 2004). In the special case where the linear map
is injective, one can give the defining inequalities in closed form as we show in Appendix C.2.1.
Hence, for the purpose of obtaining closed-formulas for the effective policy polytope we make the
following assumption. However, our subsequent analysis in Section 4 can handle any inequalities.
Assumption 10. The matrix β ∈ ∆SO ⊆ RS×O has linearly independent columns.
6
Published as a conference paper at ICLR 2022
Remark 11. The assumption above does not imply that the system is fully observable. Recall
that if β has linearly independent columns, the Moore-Penrose takes the form β+ = (βTβ)-1βT .
An interesting special case is when β is deterministic but may map several states to the same
observation (this is the partially observed setting considered in numerous works). In this case,
β+ = diag(n-1,..., n-1∣)βT, where n denotes the number of states with observation o. In this
case, β+ agrees with the conditional distribution β(s∣o) with respect to a uniform prior over the
states; however, this is not in general the case since β+ can have negative entries.
Theorem 12 (H -description of the effective policy polytope). Let (S, O, A, α, β, r) be a POMDP
and let Assumption 10 hold. Then it holds that
∆SA,β = ∆SA ∩U ∩ C = U ∩ C ∩ D,	(3)
where U = {π ◦ β | π ∈ RS×O } = ker(βT)⊥ is a subspace, C = {τ ∈ RS×A | β+τ ≥ 0}
is a pointed polyhedral cone and D = {τ ∈ RS×A | a(β+τ)oa = 1 for all o ∈ O} an affine
subspace. Further, the face lattices of ∆OA and ∆SA,β are isomorphic.
Defining polynomial inequalities of the feasible state-action frequencies In order to transfer
inequalities in ∆A to inequalities in the set of state-action frequencies Nμ, We use that the inverse
of π 7→ ηπ is given through conditioning (see Proposition 46) under the following assumption.
Assumption 13 (Positivity). Let PYF > 0 hold entrywise for all policies π ∈ ∆A.
This assumption holds in particular, if either α > 0 and γ > 0 or γ < 1 and μ > 0 entrywise
(see Appendix C.1). Assumption 13 is standard in linear programming approaches and necessary
for the convergence of policy gradient methods in MDPs (Kallenberg, 1994; Mei et al., 2020). By
conditioning, we can translate linear inequalities in ∆A into polynomial inequalities in Nμ.
Proposition 14 (Correspondence of inequalities). Let (S, A, α, r) be an MDP, τ ∈ ∆SA and let
η ∈ Δs×a denote its corresponding discounted state-action frequency for some μ ∈ ∆s and
γ ∈ (0, 1]. Let c ∈ R, b ∈ RS×A and set S := {s ∈ S | bsa 6= 0for some a ∈ A}. Then
bsaτsa ≥ c implies	bsaηsa	ηs0a0 - c	ηs0a0 ≥ 0,
s,a	s∈S a	s0∈S∖{s} a0	s0∈S a0
where the right is a multi-homogeneous polynomial4 in the blocks (ηsa)a∈A ∈ RA with multi-degree
1S ∈ NS. If further Assumption 13 holds, the inverse implication also holds.
The preceding proposition shows that the state-action frequencies of a linearly constrained policy
model, where the constraints only address the policy in individual states form a polytope. However,
the effective policy polytope is almost never of this box type (see Remark 55).
Example 15 (Blind controller). For a blind controller the linear equalities defining the effective
policy polytope in ∆A are Tsia - 丁5?& 二 T(a|si) - T(a∣s2) = 0 for all a ∈ A, si, s2 ∈ S. They
translate into the polynomial equalities ηs1aρs2 - ηs2aρs1 = 0 for all a ∈ A, s1, s2 ∈ S. In the case
that A = {a1, a2}, we obtain
0 = ηs1 a1 (ηs2 a1 + ηs2a1 ) - ηs2 a1 (ηs1 a1 + ηs1 a1 ) = ηs1 a1 ηs2a2 - ηs1 a2 ηs2a1 for all s1 , s2 ∈ S,
which is precisely the condition that all 2 × 2 minors of η vanish. Hence, in this case the set of
state-action frequencies NYμ,β is given as the intersection of NY of state-action frequencies of the
associated MDP and the determinantal variety of rank one matrices.
The following result describes the geometry of the set of feasible state-action frequencies.
Theorem 16. Let (S, O, A, α, β, r) be a POMDP μ ∈ ∆s and Y ∈ (0,1] and assume that As-
sumption 13 holds. Then we have NYF = NY ∩ V ∩ B, where V is a variety described by
multi-homogeneous polynomial equations and B is a basic semialgebraic set described by multi-
homogeneous polynomial inequalities. Further, theface lattices of ∆A,β and NYF are isomorphic.
4Apolynomial P: Rn1 ×∙∙∙×Rnk → R is called multi-homogeneous with multi-degree (dι,...,dk) ∈ Nk,
if it is homogeneous of degree dj in the j-th block of variables for j = 1, . . . , k.
7
Published as a conference paper at ICLR 2022
Remark 17. The variety V corresponds to the subspace U and the basic semialgebraic set B to the
cone C from (3). Further, closed form expressions for the defining polynomials can be computed
using Proposition 14 (see also Remark 18). The statement about isomorphic face lattices is in the
sense that ∆A,β and Nμ,β have the same number of surfaces of a given dimension with the same
neighboring properties. This can be seen in Figure 1, where the effective policy polytope and the set
of state-action frequencies both have four vertices, four edges, and one two-dimensional face.
Remark 18. By Theorem 12 and Proposition 14, the defining polynomials of the basic semialge-
braic set B from Theorem 16 are indexed by a ∈ A, o ∈ O and are given by
pao (η) := X βo+sηsa	Y	X ηs0 a0 = X	X	βo+s0	Y ηsf(s) ≥ 0, (4)
s∈So'	s0∈So∖{s} a0 f f: So→A 's0∈f-1({a})	, s∈So
where So := {s ∈ S | β+s = 0}. The polynomials depend only on β and not on γ, μ nor α, and
have |So||A||So|-1 monomials of degree |So| of the form Qs∈S ηsf(s) for some f : So → A. In
particular, we can read of the multi-degree of pao with respect to the blocks (ηsa)a∈A which is given
by 1so (see also Proposition 14). A complete description of the set Nμ,β via (in)equalities follows
from the description of NY via linear (in)equalities given in (2). In Section 5 We discuss how the
degree of these polynomials controls the complexity of the optimization problem.
Remark 19 (Planning in POMDPs as a polynomial optimization problem). The semialgebraic de-
scription of the set NYμ,β of feasible state-action distributions allows US to reformulate the reward
maximization as a polynomially constrained optimization problem with linear objective (see also
Remark 59 and Algorithm 1). This reformulation allows the use of constrained optimization algo-
rithms, which we demonstrate in Appendix F on the toy example of Figure 1 and a grid world. Note
that this polynomial program is different to the quadratic program obtained by Amato et al. (2006).
5	Number and Location of Critical Points
Although the reward function of MDPs is non convex, it still exhibits desirable properties from a
standpoint of optimization. For example, without any assumptions, every policy can be continuously
connected to an optimal policy by a path along which the reward is monotone (see Appendix D.5).
Under mild conditions, all policies which are critical points of the reward function are globally
optimal (Bhandari & Russo, 2019). In partially observable systems, the situation is fundamentally
different. In this case, suboptimal local optima of the reward function can exist as can be seen
in Figure 1 (see also Poupart et al., 2011; Bhandari & Russo, 2019). In the following we use the
geometric description of the discounted state-action frequencies to study the number and location of
critical points. These are important properties of the optimization problem and have implications on
the required stochasticity of optimal policies. In Appendix D we discuss the mean reward case and
an example and describe the sublevelsets as semialgebraic sets.
We regard the reward as a linear function po over the set of feasible state-action frequencies Nμ,β.
Under Assumption 13 π 7→ ηπ is injective and has a full-rank Jacobian everywhere (see Ap-
pendix C.1.1). Hence, the critical points in the policy polytope ∆AO correspond to the critical points
of po on Nμ,β (see Trager et al., 2019). In general, critical points of this linear function can occur on
every face of the semialgebraic set Nμ,β. The optimization problem thus has a combinatorial and a
geometric component, corresponding to the number of faces of each dimension and the number of
critical points in the relative interior of any given face. We have discussed the combinatorial part in
Theorem 16 and focus now on the geometric part. Writing Nμ,β = {η ∈ Rs×a | pi(η) ≤ 0,i ∈ I},
we are interested in the number of critical points on the interior of a face,
int(Fj) = {η ∈ NYμ,β | Pj(η) = 0 for j ∈ J,pi(n) > 0 for i ∈ I \ J}.
Note that a point η is critical on int(FJ), if and only if it is a critical point on the variety VJ :=
{η ∈ RS×A | pj (η) = 0 forj ∈ J}. For the sake of notation we write J = {1, . . . , m}. We can
bound the number of critical points in the interior of the face by the number of critical points of the
polynomial optimization problem of optimizing po(η) subject to Pι(η) = •… = Pm(η) = 0. This
number is upper bounded by the algebraic degree of the problem which controls also the (algebraic)
complexity of optimal policies (see Appendix D.1 for details). Using Theorem 12, Proposition 14
and an upper bound on the algebraic degree of polynomial optimization by Nie & Ranestad (2009)
yields the following result.
8
Published as a conference paper at ICLR 2022
Theorem 20. Consider a POMDP (S, O, A, α, β, r), γ ∈ (0, 1), assume that r is generic, that
β ∈ RS×O is invertible, and that Assumption 13 holds. For any given I ⊆ A × O consider the
following set of policies, which is the relative interior of a face of the policy polytope:
int(F) = {π ∈ ∆A | π(a∣o) = 0 if and only if (a, o) ∈ I}.
Let O := {o ∈ O | (a, o) ∈ Ifor some a} and set ko := |{a | (a, o) ∈ I}| as well as do := |{s |
βo-s1 6= 0}|. Then, the number of critical points of the reward function on int(F) is at most
Y dk) ∙ X	Y (do- i)io,	(5)
o∈O	Po∈O io=l o∈O
where l = ∣S∣(∣A∣ 一 1) 一 |I|. If a and μ are generic, this bound can be refined by computing the
polar degrees of multi-homogeneous varieties (see Proposition 21 for a special case). The same
bound holds in the mean reward case γ = 1 for l given in Remark 61.
By results from Montufar & RaUh (2017) a POMDP has optimal memoryless stochastic policies with
| SuPP∏(∙∣o)∣ ≤ lo, where lo = ∣suppβ(o∣∙)∣ ≥ 1. Hence, we may restrict attention to optimization
over NYμ,β with k = Po∈o ko active inequalities (zeros in the policy), where ko = max{∣A∣-lo, 0}.
Over these faces of the feasible set, the algebraic degree of the reward maximization problem is
UPPerbOUndedby Qo∈o dko Piι+…+io = |S|(|A|-1)-k Qo∈O (do - 1)io due to Theorem 20.
In the special case of MDPs the bound shows that for MDPs only deterministic policies can be
critical points of the reward function (see Corollary 62). Setting I := 0 shows that there are no
critical points in the interior of the policy polytope ∆OA . This requires the assumption that β is
invertible (see Appendix D.4). The bound in Theorem 20 neglects the specific algebraic structure of
the problem, and can be refined by considering polar degrees of determinantal varieties. This yields
the following tighter upper bound for a blind controller with two actions (see Appendix D.3).
Proposition 21 (Number of critical points in a blind controller). Let (S, O, A, α, β, r) be a POMDP
describing a blind controller with two actions, i.e., O = {o} and A = {a1,a2} and let r, α and μ
be generic and let Y ∈ (0,1). Then the reward function RY has at most ∣S∣ critical points in the
interior int(∆?) = (0,1) of the policy polytope and hence at most ∣S∣ + 2 critical points.
In Appendix D.4 we provide examples of blind controllers which have several critical points in
the interior (0,1) = int(∆A) and strict maxima at the two endpoints of the interval [0,1] = ∆A
respectively. Such points are called smooth and non-smooth critical points respectively.
6	Conclusion
We described geometric and algebraic properties of POMDPs and related the rational degree of the
discounted state-action frequencies and the expected cumulative reward function to the degree of
observability. We described the set of feasible state-action frequencies as a basic semialgebraic
set and computed explicit expressions for the defining polynomials. In particular, this yields a
polynomial programming formulation of POMDPs extending the linear programming formulation
of MDPs. Based on this we use polynomial optimization theory to bound the number of critical
points of the reward function over the polytope of memoryless stochastic policies. Our analysis
also yields insights into the optimization landscape, such as the number of connected components
of superlevel sets of the expected reward. Finally, we use a navigation problem in a grid world
to demonstrate that the polynomial programming formulation can offer a computationally feasible
approach to the reward maximization problem.
Our analysis focuses on infinite-horizon problems and memoryless policies with finite state, obser-
vation, and action spaces. Continuous spaces are interesting avenues, since they occur in real world
application like robotics. The general bound on the number of critical points in Theorem 20 does
not exploit the special multi-homogeneous structure of the problem, which could allow for tighter
bounds as illustrated in Proposition 21 for blind controllers. Computing polar degrees is a challeng-
ing problem that remains to be studied using more sophisticated algebraic tools. Possible extensions
of our work include the generalization to policies with finite memories as sketched in Appendix E.1.
Further, we believe that it is interesting to explore to what extent our results can be used to identify
policy classes guaranteed to contain maximizers of the reward in POMDPs.
9
Published as a conference paper at ICLR 2022
Acknowledgments
The authors thank Alex Tong Lin and Thomas Merkh for valuable discussions on POMDPs, Bernd
Sturmfels for sharing his expertise on algebraic degrees and Mareike Dressler, Marina Garrote-
LoPez and Kemal Rose for their discussions on polynomial optimization. The authors acknowledge
support by the ERC under the European Union’s Horizon 2020 research and innovation programme
(grant agreement no 757983). JM received support from the International Max Planck Research
School for Mathematics in the Sciences and the Evangelisches Studienwerk Villigst e.V..
References
Eitan Altman and Adam Shwartz. Markov decision problems and state-action frequencies. SIAM
journal on control and optimization, 29(4):786-809,1991.
Christopher Amato, Daniel S Bernstein, and Shlomo Zilberstein. Solving pomdps using quadrat-
ically constrained linear programs. In Proceedings of the fifth international joint conference on
Autonomous agents and multiagent systems, pp. 341-343, 2006.
Miguel F Anjos and Jean B Lasserre. Handbook on semidefinite, conic and polynomial optimization,
volume 166. Springer Science & Business Media, 2011.
Kamyar Azizzadenesheli, Alessandro Lazaric, and Animashree Anandkumar. Open problem: Ap-
proximate planning of POMDPs in the class of memoryless policies. In Conference on Learning
Theory, pp. 1639-1642. PMLR, 2016.
Kamyar Azizzadenesheli, Yisong Yue, and Animashree Anandkumar. Policy Gradient in Partially
Observable Environments: Approximation and Convergence. arXiv:1810.07900, 2018.
Chanderjit Bajaj. The Algebraic Degree of Geometric Optimization Problems. Discrete & Compu-
tational Geometry, 3(2):177-191, 1988.
Serguei Barannikov, Alexander Korotin, Dmitry Oganesyan, Daniil Emtsev, and Evgeny Burnaev.
Barcodes as summary of loss function’s topology. arXiv:1912.00043, 2019.
Saugata Basu. Different Bounds on the Different Betti Numbers of Semi-Algebraic Sets. Discrete
and Computational Geometry, 30(1):65-85, 2003.
Saugata Basu. Algorithms in Real Algebraic Geometry: A Survey. arXiv:1409.1534, 2014.
Saugata Basu, Richard Pollack, and Marie-FrancoiSe Roy. Algorithms in RealAlgebraic Geometry
(Algorithms and Computation in Mathematics). Springer-Verlag, Berlin, Heidelberg, 2006.
Jonathan Baxter and Peter L Bartlett. Infinite-Horizon Policy-Gradient Estimation. Journal of
Artificial Intelligence Research, 15:319-350, 2001.
Jonathan Baxter, Peter L Bartlett, et al. Reinforcement Learning in POMDP’s via Direct Gradient
Ascent. In ICML, pp. 41-48. Citeseer, 2000.
Richard Bellman. A Markovian decision process. Journal of mathematics and mechanics, 6(5):
679-684, 1957.
Jalaj Bhandari and Daniel Russo. Global Optimality Guarantees For Policy Gradient Methods.
arXiv:1906.01786, 2019.
Jacek Bochnak, Michel Coste, and Marie-FranCoiSe Roy. Real Algebraic Geometry, volume 36.
Springer Science & Business Media, 2013.
Paul Breiding, Turku Ozlum Celik, Tlmothy Duff, Alexander Heaton, Aida Maraj, Anna-Laura
Sattelberger, Lorenzo Venturello, and Oguzhan Yuruk. Nonlinear Algebra and Applications.
arXiv:2103.16300, 2021.
Michael J Catanzaro, Justin M Curry, Brittany Terese Fasy, JaniS Lazovskis, Greg Malen, Hans
Riess, Bei Wang, and Matthew Zabka. Moduli spaces of morse functions for persistence. Journal
of Applied and Computational Topology, 4(3):353-385, 2020.
10
Published as a conference paper at ICLR 2022
Krishnendu Chatterjee, Martin Chmel´k, and MathieU TracoL What is decidable about partially
observable Markov decision processes withω-regularobjectives. Journal of Computer and System
Sciences, 82(5):878-911, 2016. URL https://www.sciencedirect.com/science/
article/pii/S0022000016000246.
Victor Cohen and Axel Parmentier. Linear Programming for Decision Processes with Partial Infor-
mation. arXiv:1811.08880, 2018.
Robert Dadashi, Adrien Ali Taiga, Nicolas Le Roux, Dale Schuurmans, and Marc G Bellemare.
The value function polytope in reinforcement learning. In International Conference on Machine
Learning, pp. 1486-1495. PMLR, 2019.
Guy De Ghellinck. Les problemes de decisions sequentielles. Cahiers du Centre d’Etudes de
Recherche Operationnelle, 2(2):161-179, 1960.
Francois d’Epenoux. A Probabilistic Production and Inventory Problem. Management Science, 10
(1):98-108, 1963.
Cyrus Derman. Finite state Markovian decision processes. Academic Press, 1970.
Joseph Leo Doob. Stochastic processes, volume 10. New York Wiley, 1953.
Jan Draisma, Emil Horobet, Giorgio Ottaviani, Bernd Sturmfels, and Rekha R. Thomas. The EU-
clidean distance degree of an algebraic variety. Foundations of Computational Mathematics, 16
(1):99-149, 2016. URL https://doi.org/10.1007/s10208-014-9240-x.
Dean Gillette. 9. stochastic games with zero stop probabilities. In Contributions to the Theory of
Games (AM-39), Volume III, pp. 179-188. Princeton University Press, 1958.
D Yu Grigor’ev and NN Vorobjov. Counting connected components ofa semialgebraic set in subex-
ponential time. Computational Complexity, 2(2):133-186, 1992.
Yuri Grinberg and Doina Precup. Average Reward Optimization Objective In Partially Observable
Domains. In International Conference on Machine Learning, pp. 320-328. PMLR, 2013.
J William Helton and Victor Vinnikov. Linear matrix inequality representation of sets. Communica-
tions on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathemat-
ical Sciences, 60(5):654-674, 2007.
A Hordijk and LCM Kallenberg. Linear Programming Methods for Solving Finite Markovian De-
cision Problems . In DGOR, pp. 468-482. Springer, 1981.
Ronald A Howard. Dynamic programming and Markov processes. MIT Press, 1960.
Jeffrey J. Hunter. Chapter 2 - generating functions. In Jeffrey J. Hunter (ed.), Mathematical
Techniques of Applied Probability, pp. 24-67. Academic Press, 1983. URL https://www.
sciencedirect.com/science/article/pii/B9780123618016500083.
Rodrigo Toro Icarte, Richard Valenzano, Toryn Q. Klassen, Phillip Christoffersen, Amir mas-
soud Farahmand, and Sheila A. McIlraith. The act of remembering: A study in partially ob-
servable reinforcement learning, 2021. URL https://openreview.net/forum?id=
uFkGzn9RId8.
Tommi Jaakkola, Satinder Singh, and Michael Jordan. Reinforcement Learning Algo-
rithm for Partially Observable Markov Decision Problems. In G. Tesauro, D. Touret-
zky, and T. Leen (eds.), Advances in Neural Information Processing Systems, volume 7.
MIT Press, 1995. URL https://proceedings.neurips.cc/paper/1994/file/
1c1d4df596d01da60385f0bb17a4a9e0- Paper.pdf.
Colin Jones, E. C. Kerrigan, and Jan Maciejowski. Equality Set Projection: A new algorithm for
the projection of polytopes in halfspace representation. Technical report, Cambridge, 2004. URL
http://publications.eng.cam.ac.uk/327023/.
Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in
partially observable stochastic domains. Artificial intelligence, 101(1-2):99-134, 1998.
11
Published as a conference paper at ICLR 2022
Lodewijk CM Kallenberg. Survey of linear programming for standard and nonstandard Markovian
control problems. Part I: Theory. ZeitschriftfUr Operations Research, 40(1):1-42,1994.
Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The
International Journal of Robotics Research, 32(11):1238-1274, 2013.
HT Kung. The computational complexity of algebraic numbers. In Proceedings of the fifth annual
ACM symposium on Theory of computing, pp. 152-159, 1973.
Amy N. Langville and William J. Stewart. The Kronecker product and stochastic automata networks.
Journal of Computational and Applied Mathematics, 167(2):429-447, 2004. URL https://
www.sciencedirect.com/science/article/pii/S0377042703009312.
Jean Bernard Lasserre. An introduction to polynomial and semi-algebraic optimization, volume 52.
Cambridge University Press, 2015.
Yanjie Li, Baoqun Yin, and Hongsheng Xi. Finding optimal memoryless policies of POMDPs
under the expected average reward criterion. European Journal of Operational Research, 211(3):
556-567, 2011. URL https://www.sciencedirect.com/science/article/pii/
S0377221710008805.
Michael L. Littman. An optimization-based categorization of reinforcement learning environ-
ments. In Jean-Arcady Meyer, Herbert L. Roitblat, and Stewart W. Wilson (eds.), From Ani-
mals to Animats 2, pp. 262-270. MIT Press, 1993. URL http://www.cs.rutgers.edu/
~mlittman/Papers/sab92.giveout.ps.
Michael L. Littman. Memoryless policies: Theoretical limitations and practical results. In Proceed-
ings of the Third International Conference on Simulation of Adaptive Behavior: From Animals to
Animats 3: From Animals to Animats 3, SAB94, pp. 238-245. MIT Press, 1994.
John Loch and Satinder P. Singh. Using Eligibility Traces to Find the Best Memoryless Policy in
Partially Observable Markov Decision Processes. In Proceedings of the Fifteenth International
Conference on Machine Learning, ICML ’98, pp. 323-331, San Francisco, CA, USA, 1998.
Morgan Kaufmann Publishers Inc.
Omid Madani, Steve Hanks, and Anne Condon. On the undecidability of probabilis-
tic planning and related stochastic optimization problems. Artificial Intelligence, 147(1):
5-34, 2003. URL https://www.sciencedirect.com/science/article/pii/
S0004370202003788. Planning with Uncertainty and Incomplete Information.
Alan S Manne. Linear Programming and Sequential Decisions. Management Science, 6(3):259-267,
1960.
Peter McMullen and Egon Schulte. Abstract regular polytopes, volume 92. Cambridge University
Press, 2002.
Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence
rates of softmax policy gradient methods. In International Conference on Machine Learning, pp.
6820-6829. PMLR, 2020.
George E. Monahan. A survey of partially observable Markov decision processes: Theory, models,
and algorithms. Management Science, 28(1):1-16, 1982. URL http://www.jstor.org/
stable/2631070.
GUido Montufar and Johannes Rauh. Geometry of Policy Improvement. In International Conference
on Geometric Science of Information, pp. 282-290. Springer, 2017.
Guido Montufar, Keyan Ghazi-Zahedi, and Nihat Ay. Geometry and Determinism of Optimal Sta-
tionary Control in Partially Observable Markov Decision Processes. arXiv:1503.07206, 2015.
Guido Montufar, Johannes Rauh, and Nihat Ay. Task-agnostic constraining in average reward
POMDPs. In Task-agnostic reinforcement learning Workshop at ICLR 2019. 2019. URL https:
//tarl2019.github.io/assets/papers/montufar2019taskagnostic.pdf.
12
Published as a conference paper at ICLR 2022
Kevin P. Murphy. A Survey of POMDP Solution Techniques. Environment, 2, 10 2000.
Tim Netzer and Andreas Thom. Polynomials with and without determinantal representations. Linear
algebra and its applications, 437(7):1579-1595, 2012.
Abraham Neyman. Real Algebraic Tools in Stochastic Games. In Stochastic games and applica-
tions, pp. 57-75. Springer, 2003.
Jiawang Nie and Kristian Ranestad. Algebraic Degree of Polynomial Optimization. SIAM Journal
on Optimization, 20(1):485-502, 2009. URL https://doi.org/10.1137/080716670.
Christos H Papadimitriou and John N Tsitsiklis. The complexity of Markov decision processes.
Mathematics of operations research, 12(3):441-450, 1987.
Leonid Peshkin, Nicolas Meuleau, and Leslie Pack Kaelbling. Learning Policies with External
Memory. In Proceedings of the 16th International Conference on Machine Learning, pp. 307-
314. Morgan Kaufmann, 1999.
Pascal Poupart, Tobias Lang, and Marc Toussaint. Analyzing and escaping local optima in plan-
ning as inference for partially observable domains. In Joint European Conference on Machine
Learning and Knowledge Discovery in Databases, pp. 613-628. Springer, 2011.
Martin L Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John
Wiley & Sons, 2014.
Johannes Rauh, Nihat Ay, and GUido Montufar. A continuity result for optimal memoryless planning
in pomdps. 2021.
Jesus M Ruiz. Semialgebraic and semianalytic sets. Cahiers du se´ minaire d’histoire des
mathematiques, 1:59-70, 1991.
Satinder P Singh, Tommi Jaakkola, and Michael I Jordan. Learning without state-estimation in
partially observable Markovian decision processes. In Machine Learning Proceedings 1994, pp.
284-292. Elsevier, 1994.
Lucca Sodomaco. The Distance Function from the Variety of partially symmetric rank-one Tensors.
PhD thesis, University of Florence, Department of Mathematics and Computer Science, 2020.
Pierre-Jean Spaenlehauer. Solving multi-homogeneous and determinantal systems: algorithms, com-
Plexity, applications. PhD thesis, Universite Pierre et Marie Curie (Univ. Paris 6), 2012.
Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT press, 2018.
Richard S Sutton, David A McAllester, Satinder P Singh, Yishay Mansour, et al. Policy Gradient
Methods for Reinforcement Learning with Function Approximation. In NIPs, volume 99, pp.
1057-1063. Citeseer, 1999.
Gerald Tesauro. Temporal Difference Learning and TD-Gammon. Commun. ACM, 38(3):58-68,
March 1995. URL https://doi.org/10.1145/203330.203343.
Sascha Timme. Numerical Nonlinear Algebra. PhD thesis, Technische Universitat Berlin (Ger-
many), 2021.
Matthew Trager, Kathlen Kohn, and Joan Bruna. Pure and Spurious Critical Points: a Geometric
Study of Linear Networks. In International Conference on Learning Representations, 2019.
Nikos Vlassis, Michael L Littman, and David Barber. On the Computational Complexity of Stochas-
tic Controller Optimization in POMDPs. ACM Transactions on Computation Theory (TOCT), 4
(4):1-8, 2012.
Robert Vrabel. A note on the matrix determinant lemma. International Journal of Pure and Applied
Mathematics, 111(4):643-646, 2016.
Stephan Wilhelm Weis. Exponential Families with Incompatible Statistics and Their Entropy Dis-
tance. Friedrich-Alexander-UniversitatErlangen-Nurnberg (Germany), 2010.
13
Published as a conference paper at ICLR 2022
John Williams and Satinder Singh. Experimental Results on Learning Stochastic Memory-
less Policies for Partially Observable Markov Decision Processes. In M. Kearns, S. Solla,
and D. Cohn (eds.), Advances in Neural Information Processing Systems, volume 11.
MIT Press, 1999. URL https://proceedings.neurips.cc/paper/1998/file/
1cd3882394520876dc88d1472aa2a93f- Paper.pdf.
Tom Zahavy, Brendan O’Donoghue, Guillaume Desjardins, and Satinder Singh. Reward is enough
for convex MDPs. arXiv:2106.00661, 2021.
Gunter M Ziegler. Lectures on Polytopes, volume 152. Springer Science & Business Media, 2012.
Piotr Zwiernik. Semialgebraic statistics and latent tree models. Monographs on Statistics and
Applied Probability, 146:146, 2016.
Karl Johan Astrom. Optimal Control ofMarkov Processes with Incomplete State Information. Jour-
nal OfMathematicalAnalysis andApplications, 10:174-205, 1965. URL https://lup.lub.
lu.se/search/ws/files/5323668/8867085.pdf.
Turku Ozlum Celik, Asgar Jamneshan, Guido Montufar, Bernd Sturmfels, and Lorenzo Ven-
turello. Wasserstein distance to independence models. Journal of Symbolic Computation, 104:
855-873, 2021. URL https://www.sciencedirect.com/science/article/pii/
S0747717120301152.
14
Published as a conference paper at ICLR 2022
Appendix
The Sections A-D of the Appendix correspond to the Sections 2-5 of the main body. We present the
postponed proofs and elaborate various remarks in more detail. In Appendix E we discuss possible
extensions of our results to memory policies and polynomial POMDPs. In Appendix F we provide
details on the example plotted in Figure 1 and provide a plot of a three dimensional state-action
frequency set.
A Details on the Preliminaries	15
A.1 Partially observable Markov decision processes ................................ 15
A.2 Semialgebraic sets and their face lattices .................................... 17
B Details on the Parametrizaton of Discounted State-Action Frequencies	17
B.1	The degree of determinantal polynomials ...................................... 17
B.2	The degree of POMDPs ......................................................... 19
B.3 Properties of degree-one rational functions ................................... 20
C Details on the Geometry of State-Action Frequencies	22
C.1 The fully observable case ..................................................... 23
C.2 The partially observable case ................................................. 26
D Details on the Optimization	30
D.1 Introduction to algebraic degrees ............................................. 31
D.2 General upper bound on the number of critical points .......................... 32
D.3 Number of critical points in a two-action blind controller .................... 34
D.4 Examples with multiple smooth and non-smooth critical points .................. 36
D.5 (Super)level sets of (PO)MDPs ................................................. 37
E Possible Extensions	38
E.1 Application to finite memory policies ......................................... 38
E.2 Polynomial POMDPs ............................................................. 38
F	Examples	39
F.1 Toy example of Figure 1 ....................................................... 39
F.2 Navigation in a grid world .................................................... 42
F.3 A three dimensional example ................................................... 44
A Details on the Preliminaries
We elaborate the proofs that where ommited or only sketched in the main body.
A.1 Partially observable Markov decision processes
The statement of Proposition 1 can found in the work by Howard (1960) and we quickly sketch
the proof therein. In order to show that the expected state-action frequencies exist without any
15
Published as a conference paper at ICLR 2022
assumptions, We recall that for a (row or column) stochastic matrix P, the Cesaro mean is defined
by
1 T-1
P * := lim — Vp t
T→∞ T
t=0
and exists without any assumptions. Further, P* is the projection onto the subspace of stationary
distribution (Doob, 1953). For γ ∈ (0, 1), the matrix
∞
Pγ* := (1-γ)Xγt Pt = (1-γ)(I-γP)-1
t=0
is known as the Abel mean of P, where we used the Neumann series. By the Tauberian theorem, it
holds that Pγ* → P* for γ % 1 (Gillette, 1958; Hunter, 1983).
Proposition 1 (Existence of state-action frequencies and rewards). Let (S, O, A, α, β, r) be a
POMDP Y ∈ (0,1] and μ ∈ ∆s. Then ηγ,μ, ργ,μ and Rμ(π) exist for ^very π ∈ ∆g and μ ∈ ∆s
and are continuous in Y ∈ (0,1] for fixed π and μ.
Proof. The existence of the state-action frequencies as well as the continuity with respect to the
discount parameter follows directly from the general theory since
η∏,μ = (Pn )γ (μ * (∏ ◦ β))
for γ ∈ (0,1) and η∏,μ = (Pn)*(μ * (∏ ◦ β)). With an analogue argument, the statement follows
for the state frequencies and for the reward.	□
For Y = 1 we work under the following standard assumption in the (PO)MDP literature5.
Assumption 2 (Uniqueness of stationary disitributions). IfY = 1, we assume that for any policy
π ∈ ∆AO there exists a unique stationary distribution η ∈ ∆S×A ofPπ.
The following proposition shows in particular that for any initial distribution μ, the infinite time
horizon state-action frequency η∏,μ is the unique stationary distribution of Pn.
Proposition 3 (State-action frequencies are discounted stationary). Let (S, O, A, α, β, r) be a
POMDP γ ∈ (0,1] and μ ∈ ∆s. Then ηγ,μ is the unique element in Δs×a satisfying the dis-
counted stationarity equation η∖,μ = YPnηγ,μ + (1 — γ)(μ * (π ◦ β)). Further, ρ^,μ is the unique
element in ∆s satisfying ρ^,μ = Ypnρ^,μ + (1 — Y)μ.
Proof. By the general theory of Cesaro means, (Pn)* projects onto the space of stationary distribu-
tions and hence the η∏,μ = (Pn)*(μ * (∏ ◦ β)) is stationary. Hence, by Assumption 2, η∏,μ is the
unique stationary distribution. For Y ∈ (0, 1) we have
η∏,μ = (Pn )； (μ * (∏ ◦ β)) = (I — yP∏ )-1(μ * (∏ ◦ β)),
which yields the claim. For the state distributions ργ,μ the claim follows analogously or by marginal-
isation.	□
Since the state-action frequencies satisfy this generalized stationarity condition, we sometimes refer
to them as discounted stationary distributions.
5Assumption 2 is weaker than ergodicity and is satisfied whenever the Markov chain with transition kernel
Pπ is irreducible and aperiodic for every policy π, e.g., when the transition kernel satisfies α > 0. For
γ ∈ (0, 1) the assumption is not required, since the discounted stationary distributions are always unique since
I - γPπ is invertible because the spectral norm of Pπ is one.
16
Published as a conference paper at ICLR 2022
A.2 Semialgebraic sets and their face lattices
We recall the definition of semialgebraic sets, which are fundamental objects in real algebraic ge-
ometry (Bochnak et al., 2013). A basic (closed) semialgebraic set A is a subset of Rd defined by
finitely many polynomial inequalities as
A = {x ∈ Rd | pi (x) ≥ 0 for i ∈ I},
where I is a finite index set and the pi are polynomials. A semialgebraic set is a finite union
of basic (not necessarily closed) semialgebraic sets, and a function is called semialgebraic if its
graph is semialgebraic. By the Tarski-Seidenberg theorem the range of a semialgebraic function
is semialgebraic. A simple algebraic set has a lattice associated to it, induced by the set of active
inequalities. More precisely, for a subset J ⊆ I we set FJ := {x ∈ A | pj(x) = 0 for j ∈ J} and
endow the set F := {FJ | J ⊆ I} with the partial order of inclusion. We call the elements of F
the faces of A. The faces described above are a generalization of the faces of a classical polytope
and a special case of the faces of an abstract polytope and we refer to McMullen & Schulte (2002)
for more details. Next, we want to endow this partially ordered set with more structure. A lattice F
carries two operations, the join ∧ and the meet ∨, which satisfy the absortion laws F ∨ (F ∧G) = F
and F ∧ (F ∨ G) = F for all F, G ∈ F . In the face lattice of a basic semialgebraic set, the join and
meet are given by
F ∧ G := F ∩ G and F ∨ G :=	\ H.
H∈F: F,G⊆H
A morphism between two lattices F and G is a mapping 夕：F → G that respects the join and the
meet, i.e., such that 夕(F ∧ G)二夕(F) ∧ 夕(G) and 夕(F ∨ G)二夕(F) ∨ 夕(G) for all F,G ∈ F.
A lattice isomorphism is a bijective lattice morphism where the inverse is also a morphism. We say
that two basic semialgebraic sets with isomorphic face lattice are combinatorially equivalent.
B Details on the Parametrizaton of Discounted State-Action
Frequencies
B.1	The degree of determinantal polynomials
Determinantal representation of polynomials play an important role in convex geometry (see for
example Helton & Vinnikov, 2007; Netzer & Thom, 2012), but often the emphasis is put on sym-
metric matrices. We adapt those arguments to the general case and present them here. We call p a
determinantal polynomial if it admits a representation
p(x) 二 det A0 + X xiAi	for all x ∈ Rm ,	(6)
for some A0 , . . . , Am ∈ Rn×n . Let us use the notations
mm
A(x) :二 A0 +	xiAi and B(x) :二	xiAi.
i=1	i=1
Proposition 22 (Degree of monic univariate determinantal polynomials). Let A, B ∈ Rn×n and A
be invertible and let λ1, . . . , λn ∈ C denote the eigenvalues of A-1B if repeated according to their
algebraic multiplicity. Then,
p : R → R, t 7→ det(A + tB)
is a polynomial of degree
deg(p)二 ∣{j ∈{1,...,n}∣ λj = 0} ∣ ≤ rank(B).
The roots of p are given by {-λj-1 | j ∈ J} ⊆ C. If further A-1B is symmetric, then we have
deg(p) 二 rank(B).
17
Published as a conference paper at ICLR 2022
Proof. Let J ⊆ {1, . . . , n} denote the set of indices j such that λj 6= 0. For x 6= 0 we have6
p(t) = det(A) det(I + tA-1B) = xn det(A) det(A-1B + t-1I) = xn det(A)χA-1 B (-t-1)
n
=tn Y(TT- %) = (-1)nTJLY (f) ∙ ∏(t + λ-1),
which is a polynomial of degree |J|. Note that |J| is upper bounded by the complex rank of A-1B.
Since the rank over C and R agree for a real matrix, we have deg(p) ≤ rank(A-1B) = rank(B).
Assume now that A-1B is symmetric, then the rank of A-1B coincides with the number |J| of non
zero eigenvalues. Further, the rank of B and ATB is the same.	□
Remark 23. Note that the degree ofp can be lower than rank(B), for example if
A = I andB = 11 --11 = 11 (1 -1) .
Then we have rank(B) = 1, but
p(λ) =det 1+λλ 1--λλ = (1+λ)(1-λ)+λ2 =1
and therefore deg(p) = 0. Note that in this case A-1B = B has no non-zero eigenvalues.
Now we show that the degree ofp is still bounded by rank(B) even ifA is not invertible. However,
we loose an explicit description of the degree in this case.
Proposition 24 (Degree of univariate determinantal polynomials). Let A, B ∈ Rn×n and consider
the polynomial
p : R → R, t 7→ det(A + tB).
Then either p = 0 or if p(t0) 6= 0 and λ1, . . . , λn ∈ C denote the eigenvalues of (A + t0B)-1B
repeated according to their algebraic multiplicity, then p has degree
deg(p) = ∣{j ∈ {1,... ,n} | λj = 0}∣ ≤ rank(B).
In particular, it always holds that deg(p) ≤ rank(B).
Proof. Let without loss of generality p(t0) 6= 0, then C = A+t0B is invertible. By Proposition 22,
the degree of q(t) = det(C + tB) is precisely |{j | λj 6= 0}|. Noting that p(t) = q(t - t0) yields
the claim.	□
The following result generalizes Proposition 24 to multivariate determinantal polynomials.
Proposition 25 (Degree of determinantal polynomials). Let p : Rm → R be a determinantal poly-
nomial with the representation (6). Then
deg(p) ≤ max { rank(B(x)) | x ∈ Rm}.
Proof. Let us fix x ∈ Rm and for t ∈ R set f(t) := p(tx) = det(A0 + tB(x)). By the next
proposition it suffices to show that deg(f) ≤ rank(B(x)). However, this is precisely the statement
of Proposition 24.	□
Proposition 26 (Degree of polynomials). Let p : Rn → R be a polynomial. Then there is a direction
x ∈ Rn such that t 7→ p(tx) is a polynomial of degree deg(p). Moreover, for any x ∈ Rn, the
univariate polynomial t 7→ p(tx) has degree at most deg(p).
Proof. Let without loss of generality p be non trivial. Decompose p into its leading and lower order
terms p = p1 + p2 and choose x ∈ Rn such that p1(x) 6= 0. Let k := deg(p), then we have
Pi(tx) = tkpι(χ) for all μ ∈ R. Since the degree of t → p(tx) is at most k - 1, the degree of
t → p(tx) = pi (tx) + p2 (tx) is k.	□
6Here, χC (λ) = det(C - λI) denotes the characteriztic polynomial of a matrix C.
18
Published as a conference paper at ICLR 2022
Remark 27. Analogue to the univariate case, it is possible to give a precise statement on the degree,
which is the following. If p is not vanishing and x0 ∈ Rm is such that p(x0) 6= 0, then A(x0) ∈
Rn×n is invertible. Writing λ1(x), . . . , λn(x) ∈ C for the eigenvalues of A(x0)-1B(x) and J(x)
for the indices j such that λj (x) 6= 0 we obtain
deg(p) = max {∣ J(x)| | X ∈ Rm}.
B.2	The degree of POMDPs
The general bounds on the degree of determinantal polynomials directly implies Theorem 4, which
we state again for the sake of convenience here.
Theorem 4 (Degree of POMDPs). Let (S, O, A, α, β, r) be a POMDP μ ∈ ∆s be an initial dis-
tribution and Y ∈ (0,1) a discount factor The state-action frequencies η'μ and PnK the value
function Vn and the expected cumulative reward Rμ(π) are rational functions with common de-
nominator in the entries of the policy π. Further, if they are restricted to the subset Π ⊆ ∆AO of
policies which agree with a fixed policy π0 on all states outside of O ⊆ O, they have degree at most
I {s ∈ S | β(o∣s) > 0 for some o ∈ O}∣ .
In fact, the results from the preceding pararaph imply the following sharper versions.
Theorem 28 (Degree of discounted state-action frequencies of POMDPs). Let (S, O, A, α, β, r) be
a POMDP μ ∈ ∆s be an initial distribution and Y ∈ (0,1) a discount factor Then the discounted
state-action distributions can be expressed as
ηπ,μ(s, a) = Usa： for every π ∈ △£ and S ∈ S,	(7)
where
qs,α(∏) := (∏ ◦ β)(a∣s) ∙ (1 - Y) ∙det(I - YpT)μ and q(π) := det(I - YpT)
are polynomials in the entries of the policy. Further, if qs,a and q are restricted to the subset Π ⊆
△OA of policies which agree with a fixed policy π0 on all states outside of O ⊆ O and if we set
S := {s ∈ S | β(o∣s) > 0 for some o ∈ O}, then they have degree at most
deg(qs,a) ≤ max rank(pTπ -pπT0)s0 +1S(s) ≤ |S|	(8)
π∈Π
and
deg(q) ≤ max rank(pπT -pπT0) ≤ |S| .	(9)
Proof. Recall that we have
η∏,“(s,a) = (∏ ◦ β)(a∣s)ρ∏,μ(s).
Further, by Proposition 3, the state distribution is given by PnN
Cramer’s rule yields
(I - Y)(I - YpT)-1μ. APPlying
det(I - YpT)μ
det(I - YpT)
P∏,μ(s)
where (I - YpT)μ denotes the matrix obtained by replacing the s-th row of I - YpT with μ, which
shows (7). For the estimates on the degree, we note that
deg(qs,a) ≤ deg(I - YpT)μ + 1s(s).
Further, we can use Proposition 25 to estimate the degree over a subset Π ⊆ △SA of policies which
agree with a fixed policy π0 on all states outside of O ⊆ O. We obtain
deg(I - YpT)μ ≤ maxrank((I - YpnO)μ - (I - YpT)μ) = maxrank Y(PT - PlTO)0,
which shows the first estimate in (8). To see the second inequality, we first assume that s ∈ S. Then
(pπT - pπTO)s0 has at most |S| - 1 non zero columns and hence rank at most |S| - 1. If s ∈/ S, then
with the same argument, the rank of (pπT - pπT )s0 =pπT - pπT is at most |S| and the second estimate
in (8) holds in both cases. The estimates in (9) follow with completely analoguous arguments. □
19
Published as a conference paper at ICLR 2022
Remark 29. The polynomial q is independent of the initial distribution μ, whereas the polynomi-
als qs,a and therefore also their degrees depend on μ. Further, Proposition 24 contains an exact
expressions for the degree of the polynomials qs,a and q depending on the eigenvalues of certain
matrices.
Corollary 30 (Degree of the reward and value function). Theorem 28 also yields the rational degree
of the reward and value function. Indeed, it holds that7
Ps,a r(s,a)qs,α(π)	.1	. det(I - γp∏ + Tn 乳 μ) 一
一而一二 (I-Y) . 一det(I-γp∏)	- 1 + Y，
where we used the matrix determinant lemma (Vrabel, 2016), and
V∏( 、 det(I - YPn + r∏ ㊈ δs)
VY (S) = -det(I-γp∏)-------------1+ γ.
The degree of their denominator is bounded by (9). The degree of the numerator of Vγn (s) is bounded
by (8). Finally, the degree ofthe numerator ofthe reward RY is bounded by the maximum degree of
the numerators of Vn (S) over the SuPPortof μ. An explicit formula for the rational degrees can be
deduced from Remark 27.
Corollary 31 (Degree of curves). Let (S, A, α, r) be an MDP, μ ∈ ∆s be an initial distribution,
Y ∈ (0, 1) a discount factor and r ∈ RS×A. Further, let π0, π1 ∈ ∆SA be two Policies that disagree
on k states. Let ηλ and Vλ denote the discounted state-action frequencies and the value function
belonging to the Policy πλ := π0 + λ(π1 - π0). Then both λ 7→ ηλ and λ 7→ V λ are rational
functions of degree at most k.
B.3 Properties of degree-one rational functions
B.3.1	A line theorem for degree-one rational functions
First, we notice that certain degree-one rational functions map lines to lines which implies that they
map polytopes to polytopes. Further, the extreme points of the range lie in the image of the extreme
points which implies that degree-one rational functions are maximized in extreme points -just like
linear functions.
Definition 32. We say that a function f: Ω → Rm is a rational function of degree at most k with
common denominator if it admits a representation of the form fi = pi /q for polynomials pi and q
of degree at most k .
Remark 33. We have seen that the state-action frequencies, the reward function and the value
function of POMDPs are rational functions of degree at most |S | with common denominator. In
the case of MDPs and if a policy is fixed on all but k states, it is a rational function with common
denominator of degree at most k.
Proposition 34. Let Ω ⊆ Rd be convex and f: Ω → Rm be a rational function of degree at most
one with common denominator and the rePresentation fi(x) = pi(x)/q(x) for affine linear functions
Pi, q. Then, f maps lines to lines. More precisely, if xo, xι ∈ Ω, then
c: [0,1]→[0,1],
λ 7→
q(χι)λ
q(xλ)
_________q(χι)λ________
(q(χι) 一 q(χo))λ + q(χo)
is strictly increasing and satisfies
f((1 - λ)x0 + λx1) = (1 - c(λ))f(x0) + c(λ)f(x1) = f(x0) + c(λ)(f(x1) - f(x0)).	(10)
Further, c is strictly convex if |q(x1)| < |q(x0)|, strictly concave if |q(x1)| > |q(x0)| and linear if
|q(x0)| = |q(x1)|.
Proof. We set xλ := (1 - λ)x0 + λx1 and by explicit computation we obtain
f(xλ)
p(χλ)
q(xλ)
(1 ― λ)p(xo) + λp(xι) = (1 ― λ)q(xo) f(	λq(xι)
q(χλ)	q(χλ)	"0	q(χλ)
7Here, U Z V := UvT denotes the Kronecker product.
20
Published as a conference paper at ICLR 2022
Noting that
λq(XI) =λq(XI)= c(λ)
q(xλ)	(1 - λ)q(xo) + λq(xι)
and
(1 - λ)q(xo) + λq(xι) = (1 - λ)q(x0) + λq(xι)=]
q(χλ)	q(χλ)	q(χλ)
yields (10). Finally, we differentiate and obtain
c0(λ)
q(xo)q(xi)
q(χλ)2
(11)
Since q has no root in Ω it follows that q(χ0) and q(χι) have the same sign and hence c0(λ) > 0.
Differentiating a second time yields
c00(λ) = -2q(xo)q(xι)(q(xι) - q(xo)) ∙ q(xλ)-3.
Using that sgn(q(xλ)) = sgn(q(xo)) = sgn(q(xι)) yields the assertion.	口
Remark 35. The formula (10) holds for all λ ∈ R for which Xλ = λxo + (1 — λ)xι ∈ Ω.
Proposition 36 (Level sets of degree one rational functions). Let Ω ⊆ Rd be convex and f: Ω → R
be a rational function of degree at most one. Then, La ：= {x ∈ Ω | f (x) = ɑ} is the intersection
of an affine space with Ω.
Proof. For χ,y ∈ La the ray {x + t(y - x) 11 ∈ R}∩ Ω is contained in La by the line theorem. 口
B.3.2	Extreme points of degree-one rational functions
It is well known that linear functions obtain their maxima on extreme points. We show that this is
also the case for rational functions of degree at most one.
Definition 37. Let Ω ⊆ Rd. Then We call X ∈ Ω an extreme point of Ω if X is not the strict convex
combination of two other points in Ω, i.e., if X = (1 - λ)xo + λxι for xo, xι ∈ Ω and λ ∈ (0,1)
implies x° = xι = x. We denote the set of extreme points of Ω by extr(Ω).
Proposition 38. Let Ω ⊆ Rd be convex and f: Ω → Rm be a rational function of degree at most
one with common denominator. Then f (Ω) is convex and we have extr(f (Ω)) ⊆ f (extr(Ω)).
Proof. Let yo = f (x0),y1 = f (xι) ∈ f (Ω). Then by the line theorem, the line connecting yo and
y1 agrees with the image of the line connecting x0 and x1 under f, in particular, it is contained in
f (Ω) which shows the convexity of f (Ω). Pick now an extreme point y = f (x) ∈ extr(f (Ω)). If
x ∈ extr(Ω), there is nothing to show, so let X ∈ extr(Ω). Then by the Caratheodory theorem we
can write x as a strict convex combination Pin=1 λixi, λi > 0, n ≥ 2 for some extreme points xi ∈
extr(Ω). In particular, it is possible to write X as the strict convex combination X = (1 - λ)x0 + λxι
by setting x0 := Pin=2 λixi . Now, by the line theorem we have
y = (1 - c(λ))f(x0) + c(λ)f(x1),
where c(λ) ∈ (0, 1). Since y is an extreme point, this implies f(x0) = f(x1) = y. In particular,
this shows that y = f(xι) ∈ f (extr(Ω)).	口
Corollary 39 (Maximizers of degree-one rational functions). Let Ω ⊆ Rd be a convex and compact
set and let f: Ω → R be a rational function ofdegree at most one with common denominator. Then
f is maximized in at least one extreme point of Ω. In particular, if Ω is a polytope, f is maximized
in at least one vertex.
Proof. Since Ω is compact and f is continuous, f (Ω) is a compact interval, lets say f (Ω) = [α, β].
By the preceding proposition we have {α, β} = extr(f (Ω)) ⊆ f (extr(Ω)), which shows that f is
maximized in at least one extreme point.	口
Corollary 40. Let P ⊆ Rd be a polytope and f: Ω → Rm be a rationalfunction ofdegree at most
one with common denominator. Then f(P) is a polytope and we have vert(f (P)) ⊆ f (vert(P)).
21
Published as a conference paper at ICLR 2022
Proof. By the preceding proposition, f(P) is convex. Further, f(P) has finitely many extreme
points since extr(f (P)) ⊆ f (extr(P)) = f (Vert(P)), which implies the assertion.	口
Proposition 41. Let f: P → Rm be defined on the Cartesian product P = Pi X …X Pk of
polytopes, which is a degree-one rational function with common denominator whenever all but one
components are fixed. Then f(P) has finitely many extreme points and it holds that
extr(f (P)) ⊆ f (vert(P)) = f (vert(Pi) X …X Vert(Pk)).
In particular, if m = 1 this shows that f is maximized in at least one vertex of P.
Proof. Let now X = (x(1),..., x(k)) ∈ Pi × ∙∙∙× Pk be such that f (x) ∈ extr(f(P)). If x(i) ∈
Vert(Pi), there is nothing to show. Hence, we assume that x(i) ∈/ Vert(Pi). Let us denote the
restriction of f onto Pi by g, where we keep the other components fixed to be x(j). Then we have
g(x(i)) ∈ extr(g(Pi)) and hence by Proposition 38 there is X(i) ∈ Vert(Pi) SUch that g(X(i))=
g(χ(i)) = f (x). Replacing x(i)by X(i)and iterating over i yields the claim.	口
Remark 42. We have seen that both the value function as well as the discounted state-action fre-
quencies are degree-one rational functions in the rows of the policy in the case of full observability.
Hence, the extreme points of the set of all value functions and of the set of discounted state-action
frequencies are described by the proposition above. In fact we will see later that the discounted
state-action frequencies form a polytope; further, one can show that the set of value functions is a
finite union of polytopes (see Dadashi et al., 2019).
B.3.3	Implications for POMDPs
Proposition 6. Let (S, A, α, r) be an MDP and γ ∈ (0, 1). Further, let π0, πi ∈ ∆SA be two
policies that differ on at most k states. For any λ ∈ [0,1] let Vλ ∈ RS and ηf ∈ Δs×a denote the
value function and state-action frequency belonging to the policy π0 + λ(πi - π0) with respect to
the discount factor γ, the initial distribution μ and the instantaneous reward r. Then the rational
degrees of λ → Vλ and λ → ηλ are at most k. Ifthey differ on at most one state S ∈ S then
Vλ = V0 + c(λ) ∙ (Vi - V0) and ημ = ημ + c(λ) ∙ (ημ — ημ) forall λ ∈ [0,1],
where
c(λ)
det(I - γpi)λ
det(I — γpλ)
det(I — γpι)λ	_、ρμ (S)
—-----------；--------------------------------——λ • —TΓ-.-~.
(det(I — γpι) — det(I — γpo))λ + det(I — γpo)	ρμ (S)
Proof. This is a direct consequence of Proposition 34 and Theorem 4.
□
Remark 43. The proposition above describes the interpolation speed λ ∙ Pμ(s)∕ρμ(s) in terms of
the discounted state distribution in S. This expressions extends to the case of mean rewards - note
that the determinants vanish - and the theorem can be shown to hold in this case as well, if We set
0/0 := 0. Note that the interpolation speed does not depend on the initial condition μ.
Remark 44. Refinements on the upper bound of the rational degree of λ 7→ Vλ and λ 7→ ηλ can
be obtained using Proposition 24. Indeed, if We write ηλ(s, a) = qsa(λ)∕q(λ) like in Theorem 28
those degrees can be upper bounded by
deg(qsa) ≤ rank(pi — po)so + 1S(s) ≤ rank(pi — po) and deg(q) ≤ rank(pi — po),
where S ⊆ S is the set of states on which the two policies differ; see also the proof of Theorem 28
for more details on an analogue argument. Hence, the degree of the two curves λ 7→ Vλ and λ 7→ ηλ
is upper bounded by rank(pi — po).
C Details on the Geometry of State-Action Frequencies
The set of all state-action frequencies is known tobe a polytope in the fully observable case (Derman,
1970) and we show that it is combinatorially equivalent to the conditional probability polytope ∆SA .
We show that in the partially observable case the set of feasible state-action frequencies is cut out
from this polytope by a finite set of polynomial inequalities. We discuss the special structure of
those polynomials and give closed form expressions for them.
22
Published as a conference paper at ICLR 2022
C.1 The fully observable case
Let VY串 ∈ ∆s×s denote the expected number of transitions from S to s0 given by
∞	T-1
(1 - Y) X YtPπ,μ(St = S,st+1 = s0) and lim 不 X Pπ,μ(st = s,st+ι = s0)
T→∞ T
t=0	t=0
respectively. Note that we have
ν∏,”(s,s0)= E η∏,μ(s,a)α(s0∣s,a),	(12)
a∈A
hence VTnF is the image of ηγ,μ under the linear transformation
fα : ∆s×A → ∆s×s,	η → (X η(s, a)α(s0∣s,a))	.	(13)
Therefore, We can hope to obtain a characterization of NY using this mapping. In order to do so, We
would like to understand the structural properties of νγ,μ. For Y = 1 those distributions have equal
marginals since We can compute
X ν∏,“(s, s0) - X ν∏,“(s0, s)=3lim0 T (Pπ,μ(so = s) - Pπ,μ(sτ +1 = S))= 0.	(14)
s0∈S	s0∈S	→∞ T
In the discounted case, We compute similarly
X ν∏,μ(s,s0) - γ X ν∏,μ(s0,s) = (1 - γ)
s0∈S	s0∈S
X∞∞
γtPπ,“(st = s) - X γt+1Pπ,μ(st+ι = s)
t=0
=(1 - γ )μ(s)∙
If we perceive νγ,μ ∈ ∆s×s as a matrix, we have shown that
(ν∏,μ)τ1s = Y(V∏,μ)1s + (1- γ)μ,
which motivates the following definition.
We will see that the set of state-action frequencies is the pre-image of the following polytope under
a linear map.
Definition 45 (Discounted Kirchhoff polytopes). For a distribution μ ∈ ∆s and Y ∈ (0,1] we
define the discounted Kirchhoff polytope (this is a generalization of a definition by Weis 2010)
Ξμ := {ν ∈ ∆s×s ⊆ Rs×s | VTIS = YV 1s + (1 - Y)μ},
where 1S ∈ RS is the all one vector.
So far, we have observed that fα(ηγ,μ) ∈ Ξμ, i.e., that
Ψμ :∆A → Δs×a, π→ ηγ,μ
maps to f-1(Ξμ). In order to see that this mapping is surjective on f-1(Ξμ) we show that its right
inverse is given through conditioning. The following proposition uses the ergodicity assumption.
Proposition 46. Let Y ∈ (0,1] and η ∈ f-1(Ξμ) and let P ∈ ∆s denote the state marginal of η.
Set
π(∙∣s) := η η("s) = η(s, ∙"ρ(s)	if P(S) > 0
arbitrary element in ∆A if ρ(s) = 0,
then we have ηγ,μ = η.
Proof. We calculate
Y(Pπ)Tη(s, a) =Y ɪ2 α(s∣s0, a0)π(a∣s)η(s0, a0) = Y∏(a∣s) ɪ2 α(s∣s0, a0)η(s0, a0)
s0,a0	s0,a0
=Yn(a|s) EV(S0,s) = n(a|s) EV(S,s') - (1 - Y)μ(s)
s0	s0
=π(a∣s)ρ(s) — (1 — Y)π(a∣s)μ(s) = η(s, a) — (1 — Y)(μ * π)(s, a).
The unique characterization from Proposition 3 of ηγ,μ yields the assertion.
□
23
Published as a conference paper at ICLR 2022
The proposition states that we can reconstruct the policy from the state-action frequencies by con-
ditioning and is well known in the context of the dual linear programming formulation of MDPs
(Kallenberg, 1994). Hence, it will be convenient later to work under the following assumption in
which ensures that policies in ∆SA are one-to-one with state-action frequencies.
Assumption 13 (Positivity). Let PYF > 0 hold entrywise for all policies π ∈ ∆A.
Note that this positivity assumption holds in particular, if either α > 0 and γ > 0 or γ < 1 and
μ > 0 or entrywise. Indeed, if α > 0, then the transition kernel p∏ is strictly positive for any policy
since
p∏(s0∣s) = £(n ◦ β)(a∣s)α(s0∣s,a) > 0,
a
since (π ◦ β)(a∣s) > 0 for some a ∈ A. Since ργ,μ is discounted stationary with respect to p∏
(Proposition 3), it holds that
P∏,μ(s) = YXP∏,μ(s0)p∏(s∣s0) + (1 - γ)μ(s) > 0
s0
since PYF(S) > 0 for some s0 ∈ S. If μ > 0 and γ < 1, then ργ,μ(S) ≥ (1 - Y)μ(s) > 0. As a
consequence of Proposition 46, We obtain the following characterization of Nμ.
Proposition 8 (Characterization of Nμ). Let (S, A, α, r) be an MDP, μ ∈ ∆s be an initial distri-
bution and Y ∈ (0, 1]. It holds that
NY = ∆s×a ∩ {η ∈ rs×a | hwγs , η iS×A =(1 - Y)μs for S ∈S}	(2)
where WY := δs 0 1/ — Ya(Sh ∙)∙ For Y ∈ (0,1), Δs×a Can be replaced by [0, ∞)s×a in ⑵.
Instead of proving this proposition directly, we first present the following version of it.
Proposition 47. Let (A, S, a, r) be an MDP and Y ∈ (0,1] .It holds Ihat NY = f—1 (Ξμ).
Proof. By (13) and (14), it holds that fα(Nμ) ⊆ Ξμ and thus NY ⊆ f—1(Ξμ). However, by
Proposition46, for every η ∈ fa(Ξμ) there is a policy π ∈ ∆/ such that ηγ,μ = η and hence it
holds that f-1(Ξμ) ⊆ f-1(Ξμ).	□
Proofof Proposition 8. By the preceding proposition η ∈ NY is equivalent to η ∈ ∆s×/ and
V := fα(η) ∈ Ξμ. Using the definition of Ξμ this equivalent to
X ν(S, S0) =YXν(S0,S)+(1-Y)μ(s)
for all S ∈ S. Plugging in the definition of fα we see that the term on the left hand side is equivalent
to
EEn(S,a)α(s1s,a) = En(S,a) = hδs0 1a ,ηis×/.
The first term of the right hand side is precisely
YEEn(S0,a)a(S|S0,a) = hYa(Sh ∙),ηis×/.
s0 a
Hence, We have seen that fa(η) ∈ Ξμ is equivalent to the condition
hwY,ηis×A = (1 - y)m(s) forall S ∈S.	(15)
Note that
{η | hw,Y, ηis×A = (1 - Y)μ(S) for all S ∈S} = ηo + {wγ | S ∈ S}⊥
for an arbitrary element η0 satisfying (15). This shows the first equation in (2). The second equation
follows from the observation that Ps wYs = (1 - Y)1S. Hence, for Y < 1 it holds that
(ηo + {wγ | S ∈ S}⊥) ∩ ∆s×a = (ηo + ({w； | S ∈s}∪{1s)⊥) ∩ [0, ∞)s×a
=(ηo + {wγ | S ∈S}⊥) ∩ [0, ∞)s×a
□
24
Published as a conference paper at ICLR 2022
C.1.1 Derivative of the discounted state-action frequencies
In this section we discuss the Jacobian of the parametrization π 7→ ηπ of the discounted state-action
frequencies. One motivation for this is that this Jacobian plays an important role in the relation
of critical points in the policy space and the space of discounted state-action frequencies. Note that
Ψγ(∏) = (1-γ)(1-γP∏T)-1(μ*∏) is well defined, whenever ∣∣P∏ ∣∣2 < γ-1. Hence, we can extend
Ψγ onto the neighborhood {∏ ∈ RS×A | ∣P∏∣∣2 < γ-1} of ∆A, which enables Us to compute the
Jacobian of Ψμ.
Proposition 48 (Jacobian of Ψμ). For any policy π ∈ ∆A and S ∈ S,a ∈ A it holds that
∂(s,a)ψY(∏) = (I-YPnT)-1(ρ∏,μ * ∂(s,a)∏) = P…(I-YPn)-1(δs 氧 δa),	(16)
where
(Pnμ * ∂(s,a)∏)(s0, a0) = ρ∏,μ(s0)∂(s,a)∏(α0∣s0) = ρ∏,μ(s)(δs 0 δa)(s0, a0).
Hence, ∂(s,a)Ψμ(π) is identical to the (s, a)-th column of (I 一 YPn)-1 up to the Scalingfactor of
ργ,μ(s) .In particular, if ργ,μ(s) > 0 forall S ∈ S, the Jacobian DΨμ hasfull rank.
Proof. Recall that for invertible matrices A(t), it holds that ∂tA(t)-1 = -A(t)-1(∂tA(t))A(t)-1.
We compute
(1 - Y)-1∂(s,a)ψμ(∏) = ∂(s,a)(I-YPf )-1(μ * ∏)
=(d(s,a)(I-YPnT厂I)(μ * π) + (I - YPT厂1d(s,a)(μ * π)
=-(1-Y)T(I-YPn)-1∂(s,a)(I - YPT)η∏,“
+ (I-YPT)-1(μ * d(s,a)π)
= (I-YPn)-1 ((I-Y)-1Y(d(s,a)PΠT)η∏,μ + μ * ⅜3,a)π).
Further, direct computation shows
((∂(s,a)PT)η∏,μ)(s, a) = ∂(s,a)∏(α∣s) X α(s∣s0, a0)π(a0∣s0)ργ,“(s0)
s0,a0
=(PTρ∏,μ * ∂(s,a)∏)(s, a).
Using the fact that 吟F is the discounted stationary distribution, yields
(I-Y)TY(d(s,a)PT)η∏,μ + μ * d(s,a)π = ((I-Y)TYpTPY,μ + μ) * d(s,a)π
=(I-Y)TPY," * d(s,a)π,
which shows (16). Note that (I - YPnT)-1(δs 0 δa) is precisely the (S0, a0)-th column of the
matrix (I - YPnT)-1. Those columns are linearly independent, and so are the partial derivatives
∂(s,a)Ψμ(∏), given that the discounted stationary distribution ργ,μ vanishes nowhere.	□
Corollary 49 (Dimension of Nμ). Assume that ρ'μ > 0 entrywise for some policy π ∈ int(∆A).
Then we have
dim(Nμ) = dim(∆A) = ∣S∣(∣A∣- 1).
Proof. By Proposition 48 the mapping Ψμ is full rank in a neighborhood of ∏ and hence, We have
dim(Nμ) = dim(Ψμ(∆A)) = dim(∆A).
□
Let us consider a parametrized policy model ΠΘ = {πθ | θ ∈ Θ} with differentiable parametrization
θ 7→ πθ .
Proposition 50 (Parameter derivatives of discounted state-action frequencies). It holds that
∂θiη∏θ,μ = (I - YPnθ)-1(ρYθ,μ * ∂θi∏θ),	where (ργθ,μ * ∂θi∏)(s, a) = ρ∏θ,μ(s)∂θi∏θ(a|s).
25
Published as a conference paper at ICLR 2022
Proof. This follows directly from the application of the chain rule and (16).	□
Using this expression, we can compute the parameter gradient with respect to the discounted reward
F(θ) := Rμ(∏θ) and recover the well known policy gradient theorem, see Sutton et al. (1999).
Definition 51 (state-action value function). We call Qπ := (I - γPπ)-1r ∈ RS×A the state-action
value function or the Q-value function of the policy π.
Corollary 52 (Policy gradient theorem). It holds that
∂θiF(θ) = XρY∏θ,“(s) X∂θi∏θ(a∣s)Qπθ(s,a) = Xη∏θ,μ(s,a)∂θτ log(∏θ(a∣s))Qπθ(s,a).
s	a	s,a
Proof. Using the preceding proposition, we compute
∂θiF(θ) = hρ∏∏θ,μ * ∂θi∏θ,Qπθis×A = Xρ∏∏θ,“(s) X ∂θi∏θ(a∣s)Qπθ(s,a)
sa
=Xη∏θ,“(s, a)∂θi log(∏θ(a∣s))Qπθ(s, a).
s,a
□
Remark 53 (POMDPs as parametrized policy models). The case of partial observability can some-
times be regarded as a special case of parametrized policies. In fact the observation mechanism β
induces a linear map π 7→ π ◦ β . This interpretation together with the preceding proposition can be
used to calculate policy gradients in partially observable systems.
C.1.2 The face lattice in the fully observable case
So far, we have seen that the set of state-action frequencies form a polytope in the fully observable
case. However, not all polytopes are equally complex and thus we aim to describe the face lattice of
NYμ, which describes the combinatorial properties of a polytope, see Ziegler (2012).
Theorem 54 (Combinatorial equivalence of Nμ and ∆A). Let (A, S, α, r) be an MDP and Y ∈
(0,1]. Then π → ηπ,μ induces an order preserving surjective morphism between theface lattices of
△A and NY, such thatfor every I ⊆ S ×A it holds that
{π ∈ ∆A | π(a∣s) = 0 for all (s, a) ∈ I} 一 {η ∈ NYμ | η(s, a) = 0 for all (s, a) ∈ I}.
If additionally Assumption 13 holds, this is an isomorphism and preserves the dimension of the faces.
Proof. First, We note that the faces of both △ A and NY have the structure of the left and right hand
side of (54) respectively, which follows from (2). Denote now the left and right hand side in (54) by
F and G respectively, then We need to show that Ψμ(F) = G. For ∏ ∈ F it holds that
η∏,μ(s, a) = ρY∏,μ(s)π(a∣s) = 0 for all (s, a) ∈ I
and hence ηπ,μ ∈ G. On the other hand for η ∈ G We can set ∏(∙∣s) := η(∙∣s) whenever defined and
any other element such that π(a∣s) = 0 for all (s, a) ∈ I otherwise. Then we surely have π ∈ F
and by Proposition 46 also ηπ,μ = η. In the case that ργ,μ > 0 holds entrywise for all policies
∏ ∈ int(∆A), the mapping η → η(∙∣∙) defines an inverse to Ψμ, which shows that the mapping
defined in (54) is injective. The assertion on the dimension follows from basic dimension counting,
from the fact that the rank is preserved by a lattice isomorphism or by virtue of Proposition 48. □
C.2 The partially observable case
In Corollary 5, we have seen that the discounted state-action frequencies form a semialgebraic set.
Now we aim to describe its defining polynomial inequalities. In Section 5 we will discuss how the
degree of these polynomials allows us to upper bound the number of critical points of the optimiza-
tion problem.
26
Published as a conference paper at ICLR 2022
Definition 9 (Effective policy polytope). We call the set of effective policies τ = π ◦ β ∈ ∆SA the
effective policy polytope and denote it by ∆SA,β .
Note that ∆SA,β is indeed a polytope since it is the image of the polytope ∆OA under the linear
mapping π 7→ π ◦ β. Hence, we can write it as an intersection
∆SA,β = ∆SA ∩U ∩C,	(17)
where U , C ⊆ RS×A are an affine subspace and a polyhedral cone and describe a finite set of linear
equalities and a finite set of linear inequalities respectively. In the following we will compute those
sets explicitely under mild conditions and see that they do not carry an affine part.
C.2. 1 Defining linear inequalities of the effective policy polytope
Obtaining inequality descriptions of the images of polytopes under linear maps is a fundamental
problem that is non-trivial in general. It can be approached algorithmically, e.g., by Fourier-Motzkin
elimination, block elimination, vertex approaches, and equality set projection (Jones et al., 2004).
We discuss the special case where the linear map is injective, corresponding to the case where the
associated matrix B has linearly independent columns. As a polytope is a finite intersection of
closed half spaces H+ = {x | nTx ≥ α}, it suffices to characterize the image BH+. It holds that
BH+ = {y ∈ range B | nT B+y ≥ α} = {y | ((B+)Tn)Ty ≥ α} ∩ ker(BT)⊥,	(18)
where B+ is a pseudoinverse and where we have used that B+y consists of at most one element by
the injectivity of B . Let us now come back to the mapping π 7→ π ◦ β = βπ . By the “vec-trick”,
this map corresponds to vec(β∏I) = (IT 0 β) vec(∏). Hence the linear map is represented by the
matrix B = 10 β. We observe that (10 β)+ = I 0 β+ (see Langville & Stewart, 2004, Section
2.6.3). Notice that B = I 0 β has linearly independent columns if and only if β does. By the
above discussion, if β has linearly independent columns, then an inequality hπ, ni ≥ 0 in the policy
polytope ∆AO corresponds to an inequality hτ, (β+)Tni ≥ 0 in the polytope ∆SA.
Assumption 10. The matrix β ∈ ∆SO ⊆ RS×O has linearly independent columns.
Remark 11. The assumption above does not imply that the system is fully observable. Recall
that if β has linearly independent columns, the Moore-Penrose takes the form β+ = (βTβ)-1βT.
An interesting special case is when β is deterministic but may map several states to the same
observation (this is the partially observed setting considered in numerous works). In this case,
β+ = diag(n-1,..., n-1∣)βT, where n denotes the number of states with observation o. In this
case, βsθ agrees with the conditional distribution β(s∣o) with respect to a uniform prior over the
states; however, this is not in general the case since β+ can have negative entries.
Theorem 12 (H -description of the effective policy polytope). Let (S, O, A, α, β, r) be a POMDP
and let Assumption 10 hold. Then it holds that
∆SA,β = ∆SA ∩U ∩ C = U ∩ C ∩ D,	(3)
where U = {π ◦ β | π ∈ RS×O } = ker(βT)⊥ is a subspace, C = {τ ∈ RS×A | β+τ ≥ 0}
is a pointed polyhedral cone and D = {τ ∈ RS×A | a(β+τ)oa = 1 for all o ∈ O} an affine
subspace. Further, the face lattices of ∆OA and ∆SA,β are isomorphic.
Proof. First, we recall the defining linear (in)equalities of the policy polytope ∆AO , which are given
by
π(a∣o) = hδo 0 δa, ∏io×A ≥ 0 for all a ∈ A,o ∈ O and
£n(a|o) = hδo 0 1∕,∏>o×A = 1 for all o ∈ O.
a
Hence, by the general discussion from above, namely by (18), it holds that
∆SA,β = ker(βT)⊥ ∩ {τ | β+τ ≥ 0} ∩ nτ | X(β+τ)oa = 1 for all o ∈ Oo.
a
27
Published as a conference paper at ICLR 2022
Note that the linear inequalities Pa (β+τ)oa = 1 are redundant in ∆SA. To see this, we note that
β+1S = 1O by the injectivity of β and β1O = 1S. Now we can check that
X(β+τ)oa=XXβo+sτsa=Xβo+sXτsa=Xβo+s=1.
This together with β (∆OA ) ⊆ ∆SA shows that
∆SA,β = ∆SA ∩ ker(βT )⊥ ∩ {τ | β+τ ≥ 0}.
The reformulation of the sets C and D for deterministic observation mechanisms β follows from the
preceding remark.	口
C.2.2 Defining polynomial inequalities of the feasible state-action
FREQUENCIES
Using that the inverse ofπ 7→ ηπ is given through conditioning (see Proposition 46), we can translate
linear inequalities in ∆A into polynomial inequalities in Nμ. More precisely, We have the following
result, which can easily be extended to more general inequalities.
Proposition 14 (Correspondence of inequalities). Let (S, A, α, r) be an MDP, τ ∈ ∆SA and let
η ∈ Δs×a denote its corresponding discounted state-action frequency for some μ ∈ ∆s and
γ ∈ (0, 1]. Let c ∈ R, b ∈ RS×A and set S := {s ∈ S | bsa 6= 0for some a ∈ A}. Then
bsaτsa ≥ c implies	bsaηsa	ηs0a0 - c	ηs0a0 ≥ 0,
s,a	s∈S a	s0∈S∖{s} a0	s0∈S a0
where the right is a multi-homogeneous polynomial8 in the blocks (ηsa)a∈A ∈ RA with multi-degree
1S ∈ NS. If further Assumption 13 holds, the inverse implication also holds.
Proof. Let τ ∈ ∆SA and let η denote its corresponding discounted stationary distribution and ρ the
state marginal. Assuming that the left hand side holds, we compute
bsaηsa	ηs0 a0 = ΣΣbsa τsa ρs	ρs0
s∈S a	s0∈S∖{s} a0	s∈S a	s0∈S∖{s}
⅛bsaTsa ∙ ∏ρs0 ≥ Cn ρs,
s0∈S	s0∈S
which shows the first implication. If further Assumption 13 holds, the product over the marginals is
strictly positive, which shows the other implication.	口
Remark 55. According to the preceding proposition, a linear inequality in the state policy polytope
∆SA involving actions of k different states yields a polynomial inequality of degree k in the set
of state-action frequencies Nμ. In particular, for a linearly constrained policy model Π ⊆ ∆A,
where every constraint only addresses a single state, the set of state-action frequencies induced
by these policies will still form a polytope. This shows that this type of box constraints are well
aligned with the algebraic geometric structure of the problem. The linear constraints arising from
partial observability never exhibit this box type structure - unless the system is equivalent to its
fully observable version. This is because the projection of the effective policy polytope ∆SA,β onto
a single state always gives the entire probability simplex ∆A, which is never the case, if there is a
non trivial linear constraint concerning only this state.
Theorem 16. Let (S, O, A, α, β, r) be a POMDP μ ∈ ∆s and Y ∈ (0,1] and assume that As-
sumption 13 holds. Then we have NYF = NY ∩ V ∩ B, where V is a variety described by
multi-homogeneous polynomial equations and B is a basic semialgebraic set described by multi-
homogeneous polynomial inequalities. Further, theface lattices of ∆A,β and Nμ,β are isomorphic.
8Apolynomial P: Rn1 ×∙∙∙×Rnk → R is called multi-homogeneous with multi-degree (dι,...,dk) ∈ Nk,
if it is homogeneous of degree dj in the j-th block of variables for j = 1, . . . , k.
28
Published as a conference paper at ICLR 2022
	(In)equalities of state policies	(In)equalities of state-action frequencies
MDPS	△A is described by	NY is described by
	T(a|s) ≥ 0 Row normalization: Pa T(a|s) - 1 = 0 — —	η(s,a) ≥ 0 — Discounted stationarity: hwY,ηi - (1 - γ)μ(s) = 0 For γ = 1: Ps,a ηsa - 1 =0
POMDPs	△A，e is described in ∆A by	Nμ,β is described in NY by
	Linear (in)equalities See Section 4 Closed form under Assumption 10: See Theorem 12 Closed form for deterministic observ.: See Remark 57	Polynomial (in)equalities See Section 4, Proposition 14 Closed form under Assumption 10: See Remark 18 for inequalities See Remark 56 for equalities Closed form for deterministic observ.: See Remark 57
Table 1: Correspondence of the defining linear and polynomial inequalities of the (effective) state
policies and the (feasible) state-action frequencies for MDPs and POMDPs respectively.
Proof. The equation NYμ,β = NY ∩V∩B is a direct consequence of (3) and Proposition 14. Further,
it is clear from Proposition 14 that the mapping Ψ: ∆A → Nμ, ∏ → ηγ,μ induces a bijection of the
face lattices of ∆A,β and Nμ,β. In order to see that thejoin and meet are respected, We note that for
F,G ∈ F(∆SA,β) it holds that Ψ(F ∧ G) = Ψ(F ∩ G) = Ψ(F) ∩ Ψ(G) = Ψ(F) ∧ Ψ(G). Further,
Ψ(F ∨ G) is a face of Nμ,β containing Ψ(F) and Ψ(G) and hence by definition Ψ(F) ∨ Ψ(G) ⊆
Ψ(F ∨ G). Further, for any face I of NYF containing Ψ(F) and Ψ(G) it holds that Ψ-1(I) is a
face of ∆A,β containing F and G and hence Ψ-1(I) ⊇ F ∨ G or equivalently Ψ(F ∨ G) ⊆ I. 口
Comparing (17) and Theorem 16 We see that the linear space U corresponds to the variety V, Where
the cone C corresponds to the basic semialgebraic set B. In general, every linear (in)equality cutting
out the effective policy polytope ∆SA,β from the state policy polytope ∆SA of the associated MDP
corresponds to a polyomial (in)equality cutting out the feasible state-action frequencies NYμ,β from
all state-action frequencies NY of the corresponding MDP, see also Table 1. This correspondence
arises by relating state-action frequencies to state policies via conditioning. Hence, the problem
of computing the defining polynomial inequalities of the feasible state-action frequencies reduces
to computing the defining linear inequalities of the effective policy polytope. This can be done in
closed form if β has linearly independent columns or if it deterinistic, see Remark 18, 56 and 57.
Remark 18. By Theorem 12 and Proposition 14, the defining polynomials of the basic semialge-
braic set B from Theorem 16 are indexed by a ∈ A, o ∈ O and are given by
pao(η)	:= X	βo+sηsa	Y	X ηs0a0	= X	X	βo+s0	Y	ηsf(s)	≥ 0, (4)
s∈So ∖	s0∈So∖{s} a0 f	f: So→A ' s0∈f-1({a})	, s∈So
where So := {s ∈ S | β+s = 0}. The polynomials depend only on β and not on γ, μ nor α, and
have |So||A||So|-1 monomials of degree |So| of the form Qs∈S ηsf(s) for some f : So → A. In
particular, we can read of the multi-degree of pao with respect to the blocks (ηsa)a∈A which is given
by 1so (see also Proposition 14). A complete description of the set NYμ,β via (in)equalities follows
from the description of NY via linear (in)equalities given in (2). In Section 5 we discuss how the
degree of these polynomials controls the complexity of the optimization problem.
29
Published as a conference paper at ICLR 2022
Remark 56 (Defining polynomial equalities). Analogously to the defining inequalities, we can
compute the defining polynomial equalities in the following way. First, we need to compute a
basis {bj}j∈J of {βπ | π ∈ RO×A}⊥ = ker(βT) ⊆ RS×A, which can easily be done using the
Gram-Schmidt process. Note that the defining linear equalities of the effective policy polytope (in
the policy polytope) are given by hbj, τiS×A = 0. Hence, by Proposition 14 the corresponding
polynomial equality is given by
qj (η) := X X bjsaηsa	Y X ηs0a0 = 0,	(19)
s∈Sj a∈A	s0 ∈Sj \{s} a0∈A
where Sj := {s ∈ S | bjsa 6= 0 for some a ∈ A}.
Remark 57 (Polynomial constraints for deterministic observations). In the case, where β corre-
sponds to a determinstic mapping we can compute all polynomial constraints in closed form. Let us
assume that β(o∣s) = 60g(s)for some mapping g: S → O and write So := g-1({o}) ⊆ S, then
τ ∈ ∆SA belongs to the effective policy polytope ∆SA,β if and only if
τ (a|s1) = τ (a|s2) for all s1, s2 ∈ So, a ∈ A, o ∈ O.	(20)
Note that this can be encoded in Po|A|(|So| - 1) = |A|(|S| - |O|) linear equations; indeed if we
fix so ∈ So, then (20) is equivalent to
τ (a|s) - τ (a|so) = 0 for all s ∈ So \ {so}, a ∈ A, o ∈ O.	(21)
Another way to derive these linear equalities is by noticing that esa - esoa form a basis ofker(βT),
compare also Remark 56. By Proposition 14 for η ∈ NY it is equivalent to lie in Nμ,β or to satisfy
ηsa	ηsoa0	-	ηsoa	ηsa0	= 0 for all s ∈	So	\ {so}, a ∈ A, o ∈	O.	(22)
Note that in this case, there are no polynomial inequalities; this can also be seen from Remark 11
and Remark 18. Indeed, it holds that β+ = βT diag(n1, . . . , n|O|) ≥ 0, where no := |So|. Hence,
the polynomial inequalities pao(η) ≥ 0 are redundant on the cone [0, ∞)S×A.
Remark 58. In the fully observable case we have |So| = 1 for each o. Hence, each of the polynomial
inequalities has a single term of degree 1. Indeed, in this case the inequalities are simply ηsa ≥ 0,
for each a, for each s. In the case of a deterministic β, we have βo+s = 1s∈So /|So |. For each o, a,
there is an inequality Pf: S →A |f-1(a)| Qs∈S ηsf(s) ≥ 0 of degree |So| equal to the number of
states that are compatible with o.
Remark 59 (Reformulation of reward maximization as a polynomial program). By the theorem
above and Proposition 14, reward maximization is equivalent to the maximization of a linear function
subject to polynomial constraints. This enables the use of any (approximate) solution technique of
polynomial optimization problems in order to solve POMDPs. Such methods have been developed
for a long time and have been applied to a variety of problems (Anjos & Lasserre, 2011; Lasserre,
2015). As meta algorithm, this is presented in Algorithm 1. Once, a solution η* is obtained, the cor-
responding state policy T* ∈ ∆A can be computed by conditioning, i.e. T(a|s) := ηsa∕(Pao ηsa').
Then, every ∏* ∈ △? with β∏* = T* is an optimal policy. Such a policy can be computed by solv-
ing a system of linear equations, which are βπ = T and π ∈ ∆OA , which is standard. In particular,
if β has linearly independent columns, it holds that π* := β+T. We demonstrate that this offers a
computationally feasible approach to planning of POMPDs in Section F on the toy example used for
Figure 1 and a grid world.
D Details on the Optimization
Let us quickly recall how we can reformulate the reward maximization problem as a polynomial
optimization problem, which then leads us to the mighty tools of algebraic degrees. We perceive
the reward maximization problem again as the maximization of a linear function p0 over the set of
feasible state-action frequencies Nμ,β. Since under Assumption 13 the parametrization ∏ → ηπ
is injective and has a full-rank Jacobian everywhere (see Appendix C.1.1), the critical points in the
policy polytope △? correspond to the critical points of po on NYμ,β (Trager et al., 2019). In general,
30
Published as a conference paper at ICLR 2022
Algorithm 1 Polynomial programming for POMDPs
Require: α,β,γ,μ
for s ∈ S do
Ws — δs 0 1a - γα(s∣∙, ∙)
end for
for a ∈ A, o ∈ O do
Define pao according to Equation (4)
end for
Compute a basis {bj}j∈J of {βπ | π ∈ RO×A}⊥ ⊆ RS×A
for j ∈ J do
Define qj according to Equation (19)
end for
η* — arg max hr, ηi Sbj to η ≥ 0, hws,n) = (1 — γ)μs, h1s×A ,ηi = 1, Pao(η) ≥ 0, qj (η) = 0
R* J hr,η*i
τ* J η*(∙∣∙) ∈ ∆A
π* J solution of βπ = T*
return maximizer η*, optimal value R*, optimal policy π*
critical points of this linear function can occur on every face of the Semialgebraic set Nμ,β. The
optimization problem thus has a combinatorial and a geometric component, corresponding to the
number of faces of each dimension and the number of critical points in the interior of any given
face. We have discussed the combinatorial part in Theorem 16 and focus now on the geometric part.
Writing Nμ,β = {η ∈ Rs×a : pi(η) ≤ 0,i ∈ I}, We are interested in the number of critical points
on the interior of a face
int(Fj) = {η ∈ NYμ,β | Pj(η) = 0 for j ∈ J,Pi(η) > 0 for i ∈ I \ J}.
Note that a point η ∈ int(FJ) is critical, if and only ifit is a critical point on the variety
VJ := {η ∈ RS×A | pj(η) = 0 forj ∈ J}.
For the sake of notation, let us assume that J = {1, . . . , m} from noW on. We can upper bound the
number of critical points in the interior of the face by the number of critical points of the polynomial
optimization problem
maximize p0(η) subject to pj (η) = 0 forj = 1, . . . , m,	(23)
Where the polynomials have n variables. The number of critical points of this problems is upper
bounded by the algebraic degree of the problem as We discuss noW.
D.1 Introduction to algebraic degrees
We try to present the results from the mighty theory of algebraic degrees that We use here and refer
the interested reader to the excellent loW level introduction by Breiding et al. (2021) and to the
references therein. Let us consider the polynomial optimization problem (23), Where We do not
require p0 to be linear. Further, denote the number of variables by n (in the case of state-action
frequencies n = |S ||A|) and denote the degrees of p0, . . . , pm by d0 , . . . , dm. We call a point
critical, if it satisfies the KKT conditions (Vpo(x) + Pm=I λiVpi(x) = 0, pι(x)=…=Pm(X)=
0), Which can be phrased as a system of polynomial equations (see Nie & Ranestad, 2009). The
number of complex solutions to those criticality equations, When finite, is called the algebraic degree
of the problem. The algebraic degree is determined by the nature of the polynomials P0 , . . . , Pm and
captures the computational complexity of the optimization problem (Kung, 1973; Bajaj, 1988).9
A special case of (23) is When m = n and the polynomials P1, . . . , Pm are generic. Then by
Bezout's theorem there are exactly di …dn isolated points satisfying the polynomial constraints
and all of them are critical and hence the algebraic degree is precisely di … dn (Timme, 2021). If
the polynomials P0 , . . . , Pm define a complete intersection, i.e., the co-dimension of their induced
9The coordinates of critical points can be shoWn to be roots of some univariate polynomials Whose degree
equals the algebraic degree and Whose coefficients are rational functions of the coefficients of p0 , . . . , pm .
31
Published as a conference paper at ICLR 2022
variety is m + 1, the algebraic degree of (23) is upper bounded by
d1 …dm	X	(do - 1)i0 …(dm - I)im,	(24)
io +--+im=n-m
and this bound is attained for generic polynomials (Nie & Ranestad, 2009; Breiding et al., 2021).
For non-complete intersections, the expression (24) does not need to yield an upper bound if some
constraints are redundant. However, we can modify the expression to obtain a valid upper bound.
Indeed, if l and c = n - l denote the dimension and co-dimension of
V := {x | P1 (x)=…=Pm(X) = 0}
and if po is generic and if the degrees are ordered, i.e., di ≥ ∙∙∙ ≥ dm, then the algebraic degree is
upper bounded by
di ∙∙∙ dc X	(do - 1)i0 …(dc - 1)ic.	(25)
i0+--+ic = l
To see this, fix a subset J ⊆ {1, . . . , m} of cardinality c, such that V = {x | pj (x) = 0 for j ∈ J}.
Then we can apply the bound from (24) and evaluate it to be
Y dj	X	(do - 1)i0∙Y (dj- 1)ij,
j∈J	i0+Pj∈J ij =n-c	j∈J
which is clearly upper bounded by (25). If po is linear, then do = 1 and the expression simplifies to
di …dc X	(di - 1)i0 …(dc - 1)ic.
iι+--+ic = l
If further di = 1 for i ≥ k for some k ≤ c, then we obtain
di …dk	X	(di	-	I)i1	…(dk	-	I)ik.	(26)
i1 +-+ik = l
If pk+i, . . . ,pm are affine linear (and in general position relative topi, . . . ,pk, the algebraic degree
of (23) is given by the (m - k)-th polar degree δm-k(V) of the variety
V := {η | Pk+i(η)=…=Pm(η) = 0},
see DraIsma et al. (2016); OzlUm Celik et al. (2021). This relation is particularly useful, since
for state-action frequencies there are always active linear equations as described in (2). The po-
lar degrees of certain interesting cases (Segre-Veronese varieties) have been recently computed by
Sodomaco (2020, Section 5) and our proof of Proposition 21 builds on those formulas and their
presentation by Ozlum Celik et al. (2021).
Remark 60 (Genericity assumptions). In the case, where the polynomials Po , . . . , Pm are not
generic, there might be infinitely many critical points. Indeed, even for a linear program, i.e., when
all polynomials are linear, there might be infinitely many and even a non-trivial face of global op-
tima. This is however not the case if Po is generic. Hence, the genericity assumptions on the reward
vector r and also other elements of the POMDP are not surprising. For example, they prevent the
reward vector to be identical to zero or to be perpendicular on all vectors δs 0 1/ - γα(s∣∙, ∙) in
which cases the reward function would be constant and every policy would be a global optimum.
D.2 General upper bound on the number of critical points
Theorem 20. Consider a POMDP (S, O, A, α, β, r), γ ∈ (0, 1), assume that r is generic, that
β ∈ RS×O is invertible, and that Assumption 13 holds. For any given I ⊆ A × O consider the
following set of policies, which is the relative interior of a face of the policy polytope:
int(F) = {π ∈ ∆/ | π(a∣o) = 0 if and only if (a, o) ∈ I}.
Let O := {o ∈ O | (a, o) ∈ Ifor some a} and set ko := |{a | (a, o) ∈ I}| as well as do := |{s |
βo-si 6= 0}|. Then, the number of critical points of the reward function on int(F) is at most
(Y dk) ∙ X	Y (do - i)io,	(5)
o∈O	Po∈O io=l o∈O
where l = ∣S∣(∣A∣ — 1) 一 |I|. If a and μ are generic, this bound can be refined by computing the
polar degrees of multi-homogeneous varieties (see Proposition 21 for a special case). The same
bound holds in the mean reward case γ = 1 for l given in Remark 61.
32
Published as a conference paper at ICLR 2022
Proof. The face G of the effective policy polytope corresponding to F is given by
int(G) = nτ ∈ ∆SA,β | (β+τ)oa = 0 ⇔ (a, o) ∈ Io .
In order to describe the corresponding set of discounted state-action frequencies, we use the notation
Pao(η) := E I β+sηsa	Π	Eηs0a0 ),
s∈So ∖	s0∈So∖{s} a0	)
then it holds that
Nμβ = {η ∈NYμ | pao(η) ≥ 0 for all a ∈ A, o ∈ O}.
Then, F and G correspond to the face
int(H) = {η ∈ NYμ,β | Pao(η) = 0 ⇔ (a, o) ∈ i}
={η ∈ NY | Pao(η) ≥ 0 and equality if and only if (a, o) ∈ I}.
In order to use the explicit description of NY given in (2), We remind the reader that w： := δs 0
1a 一 γα(s∣∙, ∙). Then, it holds that
int(H) = η ∈ [0, ∞)S×A | pao(η) ≥ 0 and equality if and only if (a, o) ∈ I
hw：,ηis×A = (1 ― γ)μs for S ∈ S},
Where We used Proposition 8. Since the discounted state distributions are all positive by assumption,
for η ∈ int(H) it holds 小& = 0 if and only if T(a|s) := η(a∣s) = 0. Note that T = π ◦ β for some
π ∈ ∆OA by assumption and thus for η ∈ int(H) it holds that ηsa = 0 if and only if
0 = τ (a|s) = ɪ2 β(o∣s)∏(a∣o),
o
which holds if and only if (a, o) ∈ I for every o ∈ O with β(o∣s) > 0. Hence, if we write
J := {(s, a) | (a, o) ∈ I for all o ∈ O with β(o∣s) > 0}, we obtain
int(H) = η ∈ RS×A | ηsa ≥ 0 and equality if and only if (s, a) ∈ J,
hw：, ηis×A = (1 一 γ)μs for s ∈ S,
pao (η) ≥ 0 and equality if and only if (a, o) ∈ I .
The number of critical points over this surface is upper bounded by the number of critical points
over
V = {η ∈ RS×A | ηsa = 0 for (s, a) ∈ J,
hw：, ηis×A = (1 一 γ)μs for S ∈ S,Pao(η) = 0 for (a, o) ∈ I}.
Now we want to apply (26) and note that the objective p0 = r is generic. Further, we see that
there are |I | non-linear constraints and hence in the notation of (26) have k = |I |. Further, we can
calculate to dimension and co-dimension of V as follows. Note that F → V, π 7→ ηπ is a local
parametrization of V (meaning it parametrizes a full dimensional subset of V), which is injective
and has full rank Jacobian everywhere. Hence, we have
l = dim(V) = dim(F) = |S|(|A| 一 1) 一 |I| = |S||A| 一 |S| 一 |I|.
The co-dimension ofV is given by |S||A| 一 dim(V) = |S| + |I| and with the notation from (26), we
have c = |S| + |I| ≥ k. Further, it holds that deg(pao) ≤ do and using (26) yields an upper bound
of
Y	do	∙ X Y	(do	- l)jao	= Y	dko	∙ X	Y (do	- l)io.
(s,o)∈I	P(a,o)∈I jao=m (a,o)∈I	o∈O	Po∈O io=m o∈O
□
33
Published as a conference paper at ICLR 2022
Remark 61 (The mean reward case). Theorem 20 can be generalized to the mean reward case, i.e.,
to the case of γ = 1 with some adjustments. Indeed, the proof can be carried out analogously,
however, the characterization of Nμ has the extra linear condition that Psa 小& = 1, see also
Proposition 8. Indeed, in the mean reward case we have with the notation from the proof above
int(H) = η ∈ RS×A | ηsa ≥ 0 and equality if and only if (s, a) ∈ J,
hwγ , ηiS ×A = 0 for s ∈ S ,	ηsa = 1,
sa
pao (η) ≥ 0 and equality if and only if (a, o) ∈ I .
Hence, the upper bound in (5) remains valid if we set
l := dim nη ∈ RS×A | ηsa = 0 for (s, a) ∈ J, hwγs, ηiS×A = 0 for s ∈ S,
X ηsa = 1, pao (η) = 0 for (a, o) ∈ I .
sa
(27)
In the discounted case we obtained an explicit formulation for l. In the mean case the value obeys
a case distinction depending, in particular, on whether the all ones vector 1S lies in the span of
the vectors wγs . However, the value can be computed from the above expression (27) in any given
specific case.
Corollary 62 (Critical points of MDPs). Consider an MDP (S, A, α, r), γ ∈ (0, 1), assume that r
is generic, that β ∈ RS×O is invertible, and that Assumption 13 holds. Then, every critical point
π ∈ ∆OA of the discounted expected reward function is deterministic.
Proof. We evaluate the bound of Equation (5). If the face is not a vertex, then the corresponding
index set I ⊆ A × O satisfies |I| < |O|(|A| - 1) and thus in the notation from Theorem 20 it holds
that l > 0. Note that do = 1 for every o ∈ O and hence there is at least one factor in the product
in (5) that vanishes and so does the whole expression in (5).	□
Remark 63 (Geometry around the critical points). The key argument in the proof of Theorem 20
is that a critical point π ∈ ∆AO of the reward function corresponds to a critical point η of a linear
function over a multi-homogeneous variety V, where the defining polynomials can be computed by
the means of Proposition 8 and Proposition 14. A closer study of this variety would shed light into
the geometry of the loss landscape around the critical points, which has important implications for
gradient based methods.
Remark 64 (Efficient design of observation mechanisms). The bound (5) could be used to design
observation mechanisms in such a way that the reward function has the least critical points, which
would potentially make the system more approachable for gradient based methods. Rauh et al.
(2021) showed that planning in POMDPs is stable under perturbations of the observation kernel β .
More precisely, consider two observation kernels β,β0 ∈ ∆O satisfying kβ(∙∣s) - β0(∙∣s)kτv =
Po∣β(o∣s) - β0(o∣s)∣∕2 ≤ ε for every S ∈ S. Then if π ∈ ∆g is an optimal policy of
(S, A, O,α,β0,r), then it is a 2εγ∣∣rk∞∕(1 - γ)-optimal policy of (S, A, O,α,β,r). Hence, if
β does not fulfill the invertability assumption made in Theorem 20 an arbitrary small perturbation
of it does (given that β is a square matrix) and hence Theorem 20 provides an upper bound on the
number of critical points of an approximate problem. Further, note that the faces, which are guar-
anteed to contain an optimal policy by Montufar & Rauh (2017) might be considerably fewer for
the POMDP (S, A, O, α, β0, r). The bound (5) could be used to identify the best perturbations of a
given magnitude to obtain a problem with a minimal number of critical points.
Remark 65 (Design of policy models). Knowledge about the location of critical points of the reward
function can be used to design policy models, which provably include those critical points and
therefore also the optimal policy.
D.3 Number of critical points in a two-action blind controller
This subsection is devoted to the proof of Proposition 21 that we restate here for convenience.
34
Published as a conference paper at ICLR 2022
Proposition 21 (Number of critical points in a blind controller). Let (S , O, A, α, β, r) be a POMDP
describing a blind controller with two actions, i.e., O = {o} and A = {a1,a2} and let r, α and μ
be generic and let Y ∈ (0,1). Then the reward function RY has at most |S| critical points in the
interior int(∆A) = (0,1) of the policy polytope and hence at most |S| + 2 critical points.
Before we present the proof of this result, we discuss how the bound on the rational degree of the
reward function leads to am upper bound on the number of critical points. We consider a blind
controller and restrict ourselves to the discounted case γ ∈ (0, 1). We associate the policy polytope
∆AO with [0, 1] and for p ∈ [0, 1] we write πp and ηp for the associated policy and the state-action
frequency. From Theorem 4 we know that the reward function R = f/g : [0, 1] → R is a rational
function of degree at most k := |S |, which is well known to possess at most 2k - 2 critical points.
Hence, there are at most this many critical points in the interior (0, 1) if the reward function is not
constant. Now we use the geometric description of the set of state-action frequencies and yields a
refined bound.
Proofof Proposition 21. First, We note that since μ is generic and γ < 1 Assumption 13 is Sat-
isfied. In this case, the combinatorial part is simple, since there are only two zero-dimensional
faces of the state-action frequencies (corresponding to the endpoints of the unit interval) and one
one-dimensional face (corresponding to the interior of the unit interval). Let us set
U = {η ∈ Rs×a | hwγ, ηis×A = (1 - γ)μs for all s ∈ S},
where w； := δs 0 1/ - γα(s∣∙, ∙). By Proposition 8 and Example 15 the set of discounted state-
action frequencies is given by
NYμ,β = NY ∩D1 = [0, ∞)s×a ∩U∩D1.
Like above, we associate the policy polytope ∆OA with [0, 1] and for p ∈ [0, 1] we write πp and ηp for
the associated policy and the state-action frequency. We aim to bound the number of critical points
of the reward function over (0, 1) or equivalently the number of critical over {ηp | p ∈ (0, 1)} where
we used that Assumption 13 holds. Further, recall that ηp(a∣s) = ηPa∕ρP, we have that
{ηp | P ∈ (0,1)} = {η ∈ NYμ,β | η(a∣s) > 0 for all s ∈ S,a ∈ A}
={η ∈ NYμ,β | ηsa > 0 for all s ∈ S,a ∈ A}
= (0,∞)S×A∩U∩D1.
Thus the number of critical points over {ηp | p ∈ (0, 1)} are upper bounded by the number of critical
points on U∩Dι. Note that if a and μ are generic, the subspace U is in general position. Further, its
dimension is |S ||A| - |S | = |S |, where we used |A| = 2. Hence, the number of complex solutions
to the KKT conditions over U ∩ D1 are given by the k-th polar degree δk(D1), where k := |S|,
where we also used the genericity of the reward vector. We can compute the polar degree using the
formula presented by Ozlum CeIik et al. (2021) to obtain
k-2k+k+2	k	1 δk(D1)=	Xs=0	(-1)s2kk--(sk++11) =X(-1)s(k --+1)(k-“ We calculate the three individual terms to be (k+1)k! (X _CD	j	 Ik - 1)! l+=0 (k - 1-i)! (2 - 1 - j)! and Y /k - 1)！ ((k¾ + (⅛!=	(k - s)! ( X ―也	j—∖ '	)∖+=s (k - 1 - i)! (2 - 1 - j)!) ‘X	C)	j ∖ ∖i+⅛(k - 1-i)]	(2- 1 - j)!). ∖ _	(k + I)k	k!	(O)	(O)	_	(k + I)k2 )=-2	! ∙ (k - 1)!	∙ Tr	=	-2-, -k] (遹-⅛ + (⅛⅛) = -k2(k-1)- 2k,
35
Published as a conference paper at ICLR 2022
and
kk--
11 (k-2)!
(1)(2) + J2l_
(k - 2)! + (k - 3)!
(k-2)!
2 2k	k(k - 1)、
l(k - 2)! + 2(k - 3)!)
2k +
k(k - 1)(k - 2)
2
Adding those three summands we obtain
δk(D1)
k3 + k2 + k3 - 3k2 + 2k
- k3 + k2 - 2k + 2k = k.
2
Note that there is also a more structural argument to obtain this polar degree. In fact, the polar
degree δι(Di) = 0 for l > dim(D；) - 1, where D； denotes the dual variety of Di (OOzlum Celik
et al., 2021). Note that in the case of k X 2 matrices D； = Di (Draisma et al., 2016) and hence
it holds that δl(Di) = 0 for l > dim(Di) - 1 = k (Spaenlehauer, 2012). The largest non-zero
polar degree is equal to the degree of the dual variety (Draisma et al., 2016) and hence we obtain
δk (Di) = degree(D；) = degree(Di) = k (Spaenlehauer, 2012).	口
Note that this bound is not necessarily sharp, since it is exactly the number of complex solutions of
the criticality equations over U ∩Di. Overall, we have seen that the study of the algebraic properties
of the reward function provided an upper bound on the number of critical points of the problem,
which can be improved using the description of the state-action frequencies as a basic semialgebraic
set and employing tools from algebraic geometry.
D.4 Examples with multiple smooth and non-smooth critical points
It is the goal of this example to demonstrate that for a blind controller multiple critical points can
occur in the interior (0,1) = int(∆A) as well as at the two endpoints of [0,1] = ∆A of the policy
polytope. We refer to such points as smooth and non-smooth critical points. We consider a blind
controller with one observation, two actions ai , a2 and three states si , s2 , s3 and a deterministic
transition kernel α and reward described by the graph shown in Figure 2.
Figure 2: Graph describing the deterministic transition kernel α and the associated instantaneous
rewards.
We make the usual identification [0,1] = △2, where We associateP with ∏(ajo). In Figure 3, the
reward function is plotted on the left for the three initial conditions μ = δsι ,δs2 ,δs3. Itis apparent
that the reward has two critical points in the interior of the policy polytope ∆O = [0,1] for the
two initial conditions μ = 6§i = δs3. For μ = δs2, there are two strict local maxima on the two
endpoints of the interval. In this example, the bound from Proposition 21 ensures that there are at
most |S | = 3 critical points in the interior and at most |S | + 2 = 5 critical points in the whole policy
polytope. We see that those bounds are not sharp in this specific setting. Note that this example is
stable under small perturbations of the transition kernel and reward vector and hence can occur for
36
Published as a conference paper at ICLR 2022
generic α and r. The right hand side of Figure 3 shows a three dimensional random projection of the
set of feasible discounted state-action frequencies. By Theorem 4 they are a curve in RS×A = R6
with an injective rational parametrization of degree at most |S| = 3.
PJeMdJ Pwungs5
0.0	0.2	0.4	0.6	0.8
Probability of selecting first action
Figure 3: Plot of the reward function for initial distributions δs on the left and of a three-dimensional
random projection of the set of feasible discounted state-action frequencies on the right.
D.5 (Super)level sets of (PO)MDPs
D.5.1 Connectedness of superlevel sets in MDPs
Theorem 66 (Existence of improvement paths in MDPs). For every policy π ∈ ∆SA, there is a
continuous path connecting π to an optimal policy along which the reward is monotone. If further
π 7→ ηπ is injective, the reward is strictly monotone along this path, ifπ is suboptimal. In particular,
the superlevel sets of MDPs are connected.
Proof. Let us fix π ∈ ∆SA and set η0 := ηπ and η1 be a global optimum and ηt be the linear
interpolation and ρt be the corresponding state marginal. Note that for s ∈ S it holds that either
Pt(S) > 0 for all t ∈ (0,1) or Pt(S) = 0 for all t ∈ [0,1]. In the latter case, We can set ∏t(∙∣s) to
be an arbitrary element in ∆A. For the other states and t ∈ (0, 1) we can define the policy through
conditioning by ∏t(a∣s) := ηt(s, a)∕ρt(s) and will continuously extend the definition to t ∈ {0,1}
in the following. If P0(S) > 0 or P1 (S) > 0, then the definition extends naturally. Suppose that
P0(S) = 0, then we now that P1(S) > 0 since otherwise Pt(S) = 0 for all t ∈ [0, 1]. Now for t > 0
it holds that
πt(S, a) =
ηt(s,a)
Pt(S)
(1 - t)ηo(s,a) + tηι(s,a) _ tηι(s,a)
(1 - t)ρo(s) + tρι(s)	tρι(s)
ηι(S,a)
PI(S)'
which extends continuously to t = 0. If ρi(s) = 0, then like before, ∏t(∙∣S) does not depend on t
and we can extend it to t = 1. Now we have constructed a continuous path πt, such that ηπt = ηt
and thus
R(∏t) = hr, ηti = (1 - t)hr,ηoi + thr,ηιi = R(∏o) + t(R* - R(∏o)),
which is strictly increasing if π0 is suboptimal. It remains to construct a continuous path between
π0 and π. Note that if ρ0(S) > 0, the policies π0 and π agree on the state S and so does the
linear interpolation between the two policies. Now, by Proposition 46 we see that every linear
interpolation between π0 and π has the state-action distribution η0 . Gluing the two paths, we obtain
a path that first leaves the state-action distribution unchanged and then increases the reward strictly
up to optimality.	□
D.5.2 The semialgebraic structure of level and superlevel sets for POMDPs
Consider a POMDP (S, A, O, α, β, r) and fix a discount rate γ ∈ (0, 1) as well as an initial condition
μ ∈ ∆s. The levelset
La := {∏ ∈ ∆A | Rμ(π) = a}
37
Published as a conference paper at ICLR 2022
of the reward function is the intersection of a variety generated by one determinantal polynomial
of degree at most |S| with the policy polytope △A. Indeed, by Theorem 4 the reward function RY
is the fraction f/g of two determinantal polynomials f and g of degree at most |S|. The level set
consists of all policies, such that f(π) = ag(π). Thus, the levelset is given by
La = ∆A ∩ {x ∈ RO×A | f (X)- ag(x) = 0}.
Analogously, a superlevel set is the intersection
△OA ∩ x ∈ RO×A | f(x) - ag(x) ≥ 0}
of a basic semialgebraic generated by one determinantal polynomial of degree at most |S| with the
policy polytope △OA . In particular, both the levelset and superlevel sets of POMDPs are semialge-
braic sets defined by linear inequalities and equations (corresponding to the conditional probability
polytope △OA) and a determinantal (in)equality of degree at most |S |. This description can be used
to bounds the number of connected components, which captures important properties of the loss
landscape of an optimization problem (Barannikov et al., 2019; Catanzaro et al., 2020). By a the-
orem due to Eojasiewicz, level and superlevel sets possess finitely many connected (Semialgebraic)
components (Ruiz, 1991; Basu et al., 2006) and there exist algorithmic approaches to computing the
number of connected components (Grigor’ev & Vorobjov, 1992) as well as explicits upper bounds,
which involve the dimension, the number of defining polynomials as well as their degrees (Basu,
2003; 2014). Those results are generalizations of the classic result due to Milnor and Thom which
bounds the sum of all Betti numbers of a variety. If we apply the Milnor-Thom theorem to the vari-
ety V we obtain that there are at most |S|(2|S| - 1)|O||A|-1 many connected components ofV. This
bound neglects the determinantal nature of the defining polynomial and might therefore be coarse.
Using an analogue approach, we can also study the level and superlevel sets of the reward function
in the space of feasible state-action frequencies. Indeed, they are the intersections of the hyperplane
{η ∈ RS×A | hr, ηiS×A = a} and halfspace {η ∈ RS×A | hr, ηiS×A ≥ a} with the semialgebraic
set Nμ,β of state-action frequencies.
E Possible Extensions
E.1 Application to finite memory policies
In general, it is possible to reduce POMDPs with finite memory policies to a POMDP with mem-
oryless policies by augmenting the state and observation space with the memory. Say we consider
policies with a memory that stores the last k observations that were made. Then we could set
S := SX OkT and O := Ok. If the first state is so and the first observation that is being made is
oo, then We will associate it with so := (so, oo,...,oo) ∈ S and oo := (oo, ...,oo) ∈ O respec-
tively. If after t steps, the current state is st = (st, ot-k,..., 0t-1) and the next observation is ot,
then we set st := (ot-k,..., ot). An analogue strategy can be taken when the memory does consist
of more than the history of observations and for example includes the history of decision. It remains
open to explore the implications of the translation of our results to policies with internal memory
with this identification.
E.2 Polynomial POMDPs
Zahavy et al. (2021) consider MDPs, where the objective is a convex function of the state-action fre-
quency, i.e., where Rμ(∏) = f (ηγ,μ) for some convex function f: Rs×a → R and coin the name
of convex MDPs. In analogy, we refer to the case where f is a polyomial function as polynomial
(PO)MDPs. In polynomial POMDPs, the problem of reward maximization is by definition an op-
timization problem of a polynomial function over the set Nγμ,β of feasible state-action frequencies.
Since the feasible state-action frequencies form a basic semialgebraic set, the problem of reward
maximization in polynomials is a polynomial optimization problem. Hence, the method of bound-
ing the number of critical points as discussed in Section 5 generalizes to the case of polynomial
reward criteria. If f is a polynomial of degree d, the upper bound (5) from Theorem 20 takes the
form
∏dko ∙ E	(d - 1)i∏(do- 1)io.
o∈O	i+Po∈O io=m	o∈O
38
Published as a conference paper at ICLR 2022
The use of polar degrees does not extend in general to the case of polynomial POMDPs, since they
require a linear objective function, but can still be related to the algebraic degree for a quadratic
objective as it is the case for the Euclidean distance function (Draisma et al., 2016).
F Examples
Here, we provide examples, which illustrate our findings. In particular, we compute the defining
polynomial inequalities of the set of feasible state-action frequencies for the example from Figure 1
and a navigation problem in a grid world. We use an interior point method to solve the constrained
optimization problem corresponding to the polynomial programming formulation of the respective
POMPDs and see that in this offers a computationally feasible approach to the reward maximization
problem.
F.1 Toy example of Figure 1
We discuss in detail a toy POMDP which we used to generate the plots in Figure 1. We consider
state, observation, and action spaces with two elements each, as well as following deterministic
transition mechanism α, observation mechanism β , and instantaneous reward r:
S= {s1, s2},
O = {o1,o2},
A = {a1,a2},
α(si |sj , ak) = δik ,
r(s, a) = δs1 ,sδa1 ,a + δs2 ,sδa2,a,
γ = 0.5.
The transitions, instantaneous rewards, and observations are shown in Figure 4. As an initial distri-
bution We take the uniform distribution μ = (δsι + δs2 )/2 over the states.
Figure 4: The left shows the transition graph of the toy example. The right shows the observation
mechanism; the numbers on the edges indicate the observation probabilities.
Polynomial programming formulation To illustrate Theorem 16 (and Proposition 14), We de-
rive step-by-step the explicit polynomial program for the reWard maximization in this toy example.
For this, We first compute the defining inequalities of the set of feasible state-action frequencies.
We begin with the linear constraints that define the set NY of state-action frequencies of the asso-
ciated MDP, given in general form in Proposition 8. In the remainder, We denote the state-action
frequencies as matrices
η= η(s1, a1) η(s1, a2)
η	η(s2, a1) η(s2, a2)
η11
η21
η12
η22
∈ RS×A .
Following Proposition 8, the linear inequalities are ηij ≥ 0 for all i, j ∈ {1, 2}, and the linear
equations arehw；, ηis×A = (1 - γ)μi = 1/4 for i ∈ {1,2}, whereby here
w1 = δι 0 1a - γα(1∣∙,
and
w2 = δ2 0 1a — γα(2∣∙, ■
Thus the two linear equations are
(1	Λ	- 1(1	0∖1(1	A
[0	0J	2	11	0厂	2	1-1 0J
仅	0)-1	(0	1) =	1	(0 -0
V	1)	2	10	1厂	2	12	1 . ∙
2η11 + 4η12 - 2η21 = 1
-2η12 + 4η21 + 2η22 = 1∙
(28)
39
Published as a conference paper at ICLR 2022
It remains to compute the polynomial inequalities, which can be done using Remark 18. We invert
the matrix β and obtain
β+ = β-1 = -11 20 ∈ RS×O.
Using the notation from Remark 18 we have So1 = {s1} and So2 = {s1, s2}, and thus the polyno-
mial inequalities are
η11 ≥ 0
η12 ≥ 0
-η11(η21 + η22) + 2η21(η11 + η12) ≥ 0
-η12 (η21 + η22) + 2η22 (η11 + η12) ≥ 0.
The first two inequalities can be seen to be redundant and can be discarded. Finally, note that the
objective function is given by
hr,ηiS×A = η11 + η22.
Hence, we have obtained the following explicit formulation of the reward maximization problem as
a polynomial optimization problem:
{2ηιι + 4加2 - 2小1 - 1 =0
-2η12 + 4η21 + 2η22 - 1 = 0
η11, η12, η21, η22 ≥ 0	(29)
η11η21 + 2η21η12 - η11η22 ≥ 0
η12η22 + 2η11η22 - η12η21 ≥ 0.
Solution with constrained and polynomial optimization tools The formulation (29) allows us
to use polynomial optimization algorithms, semi-definite programming (SDP) solvers, or relaxation
hierarchies such as the popular Sum Of Squares (SOS). Using the modeling language JuMP and the
interior point solver Ipopt we directly obtained the globally optimal10 solution to problem (29)
(rounded to three digits)
*
η
The corresponding optimal state policy τ
0.667	0
0.167 0.167 .
* is obtained simply by conditioning on states, and any
pre-image under the observation kernel is an optimal observation policy, in this case simply π*
β-1τ*,
π*=	01	10	∈∆OA
This policy achieves a reward of Rμ(∏*) = hr,η*)s×A = 0.833 (rounded to three digits). The
computations took 0.01s (on a 2 GHz Quad-Core Intel Core i5 processor). The command in JuMP
to call the optimizer Ipopt is simply:
model = Model(optimizer_with_attributes(Ipopt.Optimizer)
@variable(model, \eta[1:2, 1:2]>=0)
@constraint(model, 2\eta[1, 1] + 4\eta[1, 2] - 2\eta[2, 1] == 1)
@constraint(model, -2\eta[1, 2] + 4\eta[2, 1] + 2\eta[2, 2] == 1)
@constraint(model, \eta[1,
- \eta[1, 1]\eta[2, 2]
@constraint(model, \eta[1,
- \eta[1, 2]\eta[2, 1]
1]\eta[2, 1] +
>= 0)
2]\eta[2, 2] +
>= 0)
2\eta[2, 1]\eta[1,
2\eta[1, 1]\eta[2,
2]
2]
@NLobjective(model, Max, \eta[1, 1] + \eta[2, 2])
optimize!(model)
For completeness, we also provide the command to solve a relaxation in Python SumOfSquares,
which is the following, although we found this to run a bit slower depending on the selected de-
gree. Here we negate the objective in order to obtain a minimization problem and square the search
variables (which are required to be non-negative) in order to obtain polynomials of even degree:
10The SOS relaxation provides a certificate for global optimality in this case.
40
Published as a conference paper at ICLR 2022
e11, e12, e21, e22 = sp.symbols(’e11 e12 e21 e22’)
prob = poly_opt_prob([e11, e12, e21, e22], - e11**2 - e22**2,
eqs=[+ 2 * e11**2 + 4 * e12**2 - 2 * e21**2 - 1,
- 2 * e12**2 + 4 * e21**2 + 2 * e22**2 - 1,
+ e11**2 + e12**2 + e21**2 + e22**2 - 1],
ineqs=[e11**2 * e21**2 + 2 * e21**2 * e12**2 - e11**2 * e22**2,
e12**2 * e22**2 + 2 * e11**2 * e22**2 - e12**2 * e21**2], deg=2)
prob.solve()
print(prob.value)
Policy gradient methods may not find a global optimum We want to demonstrate an important
problem of policy gradient methods, which is the well known possibility to get stuck in local optima,
in the case of the toy example. For this, we used a tabular softmax policy model to represent the
interior of the policy polytope ∆AO, i.e. used the following parametric policy model
∏θ(a|o):=
exp(θoa)
PaO eχp(θoa0)
for θ ∈ RO×A .
We computed 15 policy gradient trajectories, where we used the policy gradient theorem (see Corol-
lary 52) to compute the update directions. The starting positions where generated randomly, such
that the initial conditions in the policy polytope ∆AO are uniformly random. The trajectories in the
policy polytope ∆A = [0,1]2 are shown in Figure 5, which also shows a heat map of the reward
function. We observe that 5 of the trajectories converge to a suboptimal strict local minimum. Note
that this is not artefact of the parametrization, but of the fact that there is a strict local minimum
and hence every naive local optimization method will suffer from this problem. The reward of the
suboptimal local minimum
π= 11 00 ∈∆AO
is 0.747 if rounded to 3 digits.
R
I- 0.7848
-0.7308
-0.6768
-0.6228
-0.5688
-0.5148
-0.4608
-0.4068
-0.3528
-0.2988
Figure 5: Policy gradient optimization trajectories (shown as black curves) in the observation policy
polytope. As expected, since the problem is non-convex and has several distinct local optimizers,
the trajectories converge to different local optimizers depending on the initial policy.
Number of critical points We evaluate the bound of Theorem 20 for this toy problem. First note
that in this example the observation matrix β is invertible with β-1 = -11 02 . Further, Assump-
tion 13 is satisfied for initial distributions μ with full support. Hence, We can apply Theorem 20.
Here, we have |S | = |A| = |O| = 2 and in the notation of Theorem 20 we have do1 = 1 and
do2 = 2. As discussed in the main body, the bound evaluates to zero if we consider the inte-
rior of the policy polytope, which corresponds to I = 0. This means that there are no critical
41
Published as a conference paper at ICLR 2022
points in the interior of the policy polytope, in other words, all optimal policies lie at the boundary
and hence have one or more zero entries. The one-dimensional faces correspond to the index sets
{(a1, o1)}, {(a2, o1)}, {(a1,o2)}, {(a2, o2)}. The choices I = {(a1, o1)}, {(a2, o1)} correspond to
the two edges on the left and right of the policy polytope ∆OA as shown in the top left corner of
Figure 1 or alternatively to the two straight faces of the set Nμ,β of state-action distributions shown
in the top right corner. The bound (5) evaluates to zero for those choices. This can also be seen in
the bottom row in Figure 1, where it is apparent that there are no critical points on the respective
faces. For the choices I = {(a1, o2)}, {(a2, o2)} the bound (5) evaluates to two. Indeed, these faces
contain critical points. The bound is not sharp in this case since the actual number of critical points
in any of the two faces of the policy polytope ∆AO, which correspond to the two non-linear faces of
Nμ,β is one. Nonetheless, this illustrates how the theorem allows us to discard most faces of the
polytope and focus the search for an optimal policy on just two faces.
F.2 Navigation in a grid world
Figure 6: Depiction of a grid world; the reward of R is obtained in state 1, the actions are
{R, L, U, D} corresponding to movements to the right, left, up and down; observed are the pos-
sible directions that the agent can move in. Once the agent transitions to state 1, she transfers to
state 7 and 13 uniformly.
We consider the grid world depicted in Figure 6 with 13 states and 7 observations, where it
is the goal to reach state 1. The four actions are {R, L, U, D} corresponding to the directions
right, left, up and down on the grid. The transitions are deterministic and lead to the cell right,
left, above or below the current cell, if this cell is admissible; from the goal state 1 one tran-
sitions uniformly to the states 7 and 13 independently of the chosen action. Further, we con-
sider deterministic observations, which correspond to the agent being able to observe its imme-
diate four neighboring positions. This observation mechanism partitions the state space into the
seven subsets {1, 7}, {2, 4, 9, 10}, {3, 8}, {5}, {6, 12}, {11}, {13}, which lead to the observations
o1, o2, o3, o4, o5, o6 and o7 respectively. Hence, by Remark 57 the polynomial constraints are given
by
η7aρ1 - η1aρ7 = 0 for all a ∈ A
η4aρ2 - η2aρ4 = 0 for all a ∈ A
η8aρ2 - η2aρ8 = 0 for all a ∈ A
η10aρ2 - η2aρ10 = 0 for all a ∈ A
η9aρ3 - η3aρ9 = 0 for all a ∈ A
η12aρ6 - η6aρ12 = 0 for all a ∈ A,
42
Published as a conference paper at ICLR 2022
where ρs =	a ηsa. The linear constraints apart from η ≥ 0 can be computed to be
ρι	-	γ(η12 + η13	+ ηi4 +	η22)	= μι
ρ2	-	γ(η11 + η23	+ η24 +	η32)	= μ2
P3 — γ(η21 + η33 + n42)= μ3
P4 - γ(η31 + η43 + η44 + η52) = μ4
P5 - γ(η41 + η53 + η54) = μ5
p6 - γ(η34 + η6i + η62 + η11,3) = μ6
P7 - γ(ρi∕2	+ η72	+ η73	+ η74	+	η82)	= μ7
P8 -	γ(η71	+ η83	+ η84	+	η92)	= μ8
P9 - γ(η81 + η93 + ηi0,2) = μ9
ρ10 - γ(η91 + η10,3 + η10,4 + η11,2) = μ10
ρ11 - γ(η10,1 + η11,1 + η11,4) = μ11
ρ12 - γ(η94 + η12,1 + η12,2 + η13,3) = μ12
P13 - γ(ρi∕2 + ηi2,4 + ηi3,l + ηi3,2 + m3,4)= μi3∙
Further, the objective function is given by
hr, ηiS×A = η11 + η12 + η13 + η14.
Let Us now consider the uniform distribution μs = 1/13 for S ∈ S as an initial distribution and
γ = 0.999 as a discount factor. Like for the toy problem we used the interior point method Ipopt
implemented in the Julia packages JuMP and Ipopt to solve this polynomial optimization problem.
The solver took around 0.03s consistently (on a 2 GHz Quad-Core Intel Core i5 processor). The
found solution is (rounded to three digits)
	RLUD	
1	0.033 0.000 0.000 0.000	
2	0.044 0.033 0.000 0.000	
3	0.049 0.077 0.000 0.000	
4	0.066 0.049 0.000 0.000	
5	0.000 0.066 0.000 0.000	
6	0.000 0.000 0.033 0.000	
*	!-7 η = 7	0.133 0.000 0.000 0.000	∈ Δs×a ⊆ RS×A
8	0.075 0.117 0.000 0.000	
9	0.057 0.042 0.000 0.000	
10	0.033 0.024 0.000 0.000	
11	0.000 0.000 0.033 0.000	
12	0.000 0.000 0.017 0.000	
13	0.000 0.000 0.016 0.000	
and has the objective value hr, η*iS×A = 0.033. The corresponding optimal state policy τ* is		
obtained simply by conditioning on states, and any pre-image under the observation kernel is an
optimal observation policy, in this case simply π*			= β+τ* which is (rounded to two digits)		
	R	L	U	D	
o1	1.00	0.00	0.00	0.00	
o2	0.53	0.47	0.00	0.00	
03	0.48	0.52	0.00	0.00	
* ∏ = 04	0.00	1.00	0.00	0.00	∈ ∆A.
O5	0.00	0.00	0.99	0.00	
o6	0.00	0.00	1.00	0.00	
07	0.00	0.00	0.99	0.00	
This policy
1.	moves right on observation 1 corresponding to the states 1 and 7,
2.	moves right and left with probability close to 1∕2 on observation 2 corresponding to states
2, 4, 9 and 10,
43
Published as a conference paper at ICLR 2022
3.	moves right and left with probability close to 1/2 on observation 3 corresponding to the
states 3 and 8,
4.	moves left on observation 4 corresponding to the state 5,
5.	moves up on observation 5 corresponding to the states 6 and 12,
6.	moves up on observation 6 corresponding to the states 11,
7.	moves up on observation 7 corresponding to the state 13.
The action choices of the policy ∏ are also shown in Figure 7. Note that the policy ∏ selects
the best action in the states 1, 5, 6, 11, 12 and 13. Those are the states that are either identifiable
from its observation (this is the case for 5, 11 and 13) or where the optimal actions of all states
leading to the same observation agree (this is the case for the pairs {1, 7} and {6, 12}). In the other
states, where the corresponding observation is ambiguous, the policy randomizes among the two
actions, which are optimal for the compatible states. This is for example the case for the states
2, 4, 9 and 10, which all lead to observation 2. The optimal MDP policy would move left in state
2 and 4 and move right in the states 9 and 10. The POMDP policy has to randomize between
moving left and right, since otherwise the agent could never reach the goal state if starting in 7 or
13. The same consideration applies to the states 3 and 8, which both lead to observation 3. The Julia
Figure 7: Depiction of the policy π, which is found using the polynomial programming formulation
of the POMDP and applying the interior point method Ipopt implemented in the Julia libraries JuMP
and Ipopt; an arrow into two direction indicates that the agent moves into those two directions with
probability close to 1/2.
F.3 A three dimensional example
Let us now discuss an example where the set of discounted state-action distributions is three-
dimensional and not two-dimensional as before. For this, we consider a generalization of the previ-
ous example where |S | = 3, |A| = 2 and |O| = 3 such that
dim(∆A) = dim(∆A) = 3 ∙ (2 — 1) = 3.
The observation mechanism used is
100
β = 1/2 1/2	0	.
1/3 1/3 1/3
Further, the action mechanism α and the initial distribution μ are sampled randomly and the used
discount factor is 0.5. Since the initial distribution is generic and β is invertible, the set of state-
action frequencies NY and the set of feasible state-action frequencies Nμ,β are three-dimensional
and are in fact combinatorially equivalent to the three-dimensional cube ∆A = △? = [0,1]3 (see
Theorem 16). In Figure 8 we plot a random three-dimensional projection of the sets. More precisely,
we plot their one-dimnesional faces dashed and solid for the MDP and POMDP respectively. The
combinatorial equivalence to the three-dimensional cube can be see in this plot.
44
Published as a conference paper at ICLR 2022
Figure 8: A random three-dimensional projection of the set of discounted state-action frequencies of
the MDP is the polytope defined by the dashed straight edges. The same random three-dimensional
projection of the set of feasible discounted state-action frequencies is the basic semialgebraic set
where the edges are shown by the solid lines. Both of those sets are combinatorially equivalent to a
three dimensional cube.
45