Published as a conference paper at ICLR 2022
Learning Graphon Mean Field Games
and Approximate Nash Equilibria
Kai Cui & Heinz Koeppl
Department of Electrical Engineering, Technische Universitat Darmstadt, Germany
{kai.cui,heinz.koeppl}@bcs.tu-darmstadt.de
Ab stract
Recent advances at the intersection of dense large graph limits and mean field
games have begun to enable the scalable analysis of a broad class of dynamical
sequential games with large numbers of agents. So far, results have been largely
limited to graphon mean field systems with continuous-time diffusive or jump
dynamics, typically without control and with little focus on computational meth-
ods. We propose a novel discrete-time formulation for graphon mean field games
as the limit of non-linear dense graph Markov games with weak interaction. On
the theoretical side, we give extensive and rigorous existence and approximation
properties of the graphon mean field solution in sufficiently large systems. On the
practical side, we provide general learning schemes for graphon mean field equi-
libria by either introducing agent equivalence classes or reformulating the graphon
mean field system as a classical mean field system. By repeatedly finding a reg-
ularized optimal control solution and its generated mean field, we successfully
obtain plausible approximate Nash equilibria in otherwise infeasible large dense
graph games with many agents. Empirically, we are able to demonstrate on a
number of examples that the finite-agent behavior comes increasingly close to the
mean field behavior for our computed equilibria as the graph or system size grows,
verifying our theory. More generally, we successfully apply policy gradient rein-
forcement learning in conjunction with sequential Monte Carlo methods.
1	Introduction
Today, reinforcement learning (RL) finds application in various application areas such as robotics
(Kober et al., 2013), autonomous driving (Kiran et al., 2021) or navigation of stratospheric balloons
(Bellemare et al., 2020) as a method to realize effective sequential decision-making in complex
problems. RL remains a very active research area, and there remain many challenges in multi-agent
reinforcement learning (MARL) as a generalization of RL such as learning goals, non-stationarity
and scalability of algorithms (Zhang et al., 2021). Nonetheless, potential applications for MARL
are manifold and include e.g. teams of unmanned aerial vehicles (Tozicka et al., 2018; Pham et al.,
2018) or video games (Berner et al., 2019; Vinyals et al., 2017). While the domain of MARL is
somewhat empirically successful, problems quickly become intractable as the number of agents
grows, and methods in MARL typically miss a theoretical foundation. A recent tractable approach
to handling the scalability problem in MARL are competitive mean field games and cooperative
mean field control (Gu et al., 2021). Instead of considering generic multi agent Markov games, one
considers many agents under the weak interaction principle, i.e. each agent alone has a negligible
influence on all other agents. This class of models naturally contains a large number of real world
scenarios and can find application e.g. in analysis of power network resilience (Bagagiolo & Bauso,
2014), smart heating (Kizilkale & Malhame, 2014), edge computing (Banez et al., 2019) or flocking
(Perrin et al., 2021b). See also Djehiche et al. (2017) for a review of other engineering applications.
Mean field games. Mean field games (MFGs) were first popularized in the independent seminal
works of Huang et al. (2006) and Lasry & Lions (2007) for the setting of differential games with
diffusion-type dynamics given by stochastic differential equations. See also Gueant et al. (2011);
Bensoussan et al. (2013) for a review. Since then, extensions have been manifold and include e.g.
discrete-time (Saldi et al., 2018), partial observability (Saldi et al., 2019), major-minor formulations
1
Published as a conference paper at ICLR 2022
(Nourian & Caines, 2013) and many more. In the learning community, there has been immense
recent interest in finding and analyzing solution methods for mean field equilibria (Cardaliaguet &
Hadikhanloo, 2017; Mguni et al., 2018; Guo et al., 2019; Subramanian & Mahajan, 2019; Elie et al.,
2020; Cui & Koeppl, 2021; Pasztor et al., 2021; Perolat et al., 2021; Perrin et al., 2021a), solving
the inverse reinforcement learning problem (Yang et al., 2018a) or applying related approximations
directly to MARL (Yang et al., 2018b). In contrast to our work, Yang et al. (2018a) consider states
instead of agents on a graph, while Yang et al. (2018b) requires restrictive assumptions and only
considers averages instead of distributions of the neighbors. Recently, focus increased also on the
cooperative case of mean field control (Carmona et al., 2019b; Mondal et al., 2021), for which
dynamic programming holds on an enlarged state space, resulting in a high-dimensional Markov
decision process (Pham & Wei, 2018; Motte & Pham, 2019; Gu et al., 2020; Cui et al., 2021).
Mean field systems on graphs. For mean field systems on dense graphs, prior work mostly con-
siders mean field systems without control (Vizuete et al., 2020) or time-dynamics, i.e. the static case
(Parise & Ozdaglar, 2019; Carmona et al., 2019a). To the best of our knowledge, Gao & Caines
(2017) and Caines & Huang (2019) are the first to consider general continuous-time diffusion-type
graphon mean field systems with control, the latter proposing many clusters of agents as well as
proving an approximate Nash property as the number of clusters and agents grows. There have since
been efforts to control cooperative graphon mean field systems with diffusive linear dynamics using
spectral methods (Gao & Caines, 2019a;b). On the other hand, Bayraktar et al. (2020); Bet et al.
(2020) consider large non-clustered systems in a continuous-time diffusion-type setting without con-
trol, while Aurell et al. (2021b) and Aurell et al. (2021a) consider continuous-time linear-quadratic
systems and continuous-time jump processes respectively. To the best of our knowledge, only Vasal
et al. (2021) have considered solving and formulating a graphon mean field game in discrete time,
though requiring analytic computation of an infinite-dimensional value function defined over all
mean fields and thus being inapplicable to arbitrary problems in a black-box, learning manner. In
contrast, we give a general learning scheme and also provide extensive theoretical analysis of our
algorithms and (slightly different) model. Finally, for sparse graphs there exist preliminary results
(Gkogkas & Kuehn, 2020; Lacker & Soret, 2020), though the setting remains to be developed.
Our contribution. In this work, we propose a dense graph limit extension of MFGs in discrete
time, combining graphon mean field systems with mean field games. More specifically, we consider
limits of many-agent systems with discrete-time graph-based dynamics and weak neighbor interac-
tions. In contrast to prior works, we consider one of the first general discrete-time formulations as
well as its controlled case, which is a natural setting for many problems that are inherently discrete
in time or to be controlled digitally at discrete decision times. Our contribution can be summarized
as: (i) formulating one of the first general discrete-time graphon MFG frameworks for approximat-
ing otherwise intractable large dense graph games; (ii) providing an extensive theoretical analysis of
existence and approximation properties in such systems; (iii) providing general learning schemes for
finding graphon mean field equilibria, and (iv) empirically evaluating our proposed approach with
verification of theoretical results in the finite N -agent graph system, finding plausible approximate
Nash equilibria for otherwise infeasible large dense graph games with many agents.
2	Dense graph mean field games
In the following, we will give a dense graph N -agent model as well as its corresponding mean field
system, where agents are affected only by the overall state distribution of all neighbors, as visualized
in Figure 1. Asa result of the law of large numbers, this distribution will become deterministic - the
mean field -as N → ∞. We begin with graph-theoretical preliminaries, see also LovgSz (2012) for
a review. The study of dense large graph limits deals with the limiting representation of adjacency
matrices called graphons. Define I := [0, 1] and W0 as the space of all bounded, symmetric and
measurable functions (graphons) W ∈ W0, W : I × I → R bounded by 0 ≤ W ≤ 1. For any
simple graph G = ({1, . . . , N}, E), we define its step-graphon a.e. uniquely by
WGEy)=	E	l(i,j)∈E ∙ lχ∈( iN1 , N ]∙ly∈( j-1 , N ],	(1)
i,j∈{1,...,N}
2
Published as a conference paper at ICLR 2022
Figure 1: Graphical model visualization. (a): A graph with 5 nodes; (b): The associated step
graphon of the graph in (a) as a continuous domain version of its adjacency matrix; (c): A visualiza-
tion of the dynamics, i.e. the center agent is affected only by its neighbors (grey).
see e.g. Figure 1. We equip W0 with the cut (Semi-)norm ∣∙∣∣□ and cut (Pseudo-)metric δ□
kWk	:= sSu,Tp
W(x,y)dxdy ,
δ□ KW O)=infkW- W k□,
(2)
for graphons W, W0 ∈ W0 and W[p(x, y) := W0(夕(x),g(y)), where the supremum is over all
measurable subsets S, T ⊆ I and the infimum is over measure-preserving bijections φ: I → I.
To provide motivation, note that convergence in δ□ is equivalent to e.g. convergence of probabil-
ities of locally encountering any fixed subgraph by randomly sampling a subset of nodes. Many
such properties of graph sequences (GN )N∈N converging to some graphon W ∈ W0 can then be
described by W, and We point to Lovasz (2012) for details. In this work, We Win primarily use the
analytical fact that for converging graphon sequences kWGN - W k□ → 0, we equivalently have
kWGN - WkL∞→L1
sup
kgk∞≤1 I I
(WGN (α, β) -W(α,β))g(β)dβ
dα → 0
(3)
under the operator norm of operators L∞ → L1, see e.g. Lovasz (2012), Lemma 8.11.
By Lovasz (2012), Theorem 11.59, the above is equivalent to convergence in the cut metric
δ□(WGN, W) → 0 up to relabeling. In the following, we will therefore assume sequences of simple
graphs GN = (VN , EN ) with vertices VN = {1, . . . , N}, edge sets EN, edge indicator variables
ξiN,j := 1(i,j)∈EN for all nodes i, j ∈ VN, and associated step graphons WN converging in cut norm.
Assumption 1. The SequenCe OfsteP-graphons (WN)N∈N converges in cut norm ∣∣∙∣∣□ or equiva-
lently in operator norm ∣∣∙∣∣Loo→L1 as N → ∞ to some graphon W ∈ W0, i.e.
kWN-Wk□→0,	kWN-WkL∞→L1 →0.	(4)
Next, we define W -random graphs to consist of vertices VN := {1, . . . , N} with adjacency
matrices ξN generated by sampling graphon indices αi uniformly from I and edges ξNj 〜
Bernoulli(W(αi, αj)) for all vertices i, j ∈ VN. For experiments, by Lovasz (2012), Lemma 10.16,
we can thereby generate a.s. converging graph sequences by sampling W -random graphs for any
fixed graphon W ∈ W0 . In principle, one could also consider arbitrary graph generating processes
whenever a valid relabeling function φ is known.
In our work, the usage of graphons enables us to find mean field systems on dense graphs and
to extend the expressiveness of classical MFGs. As examples, we will use the limiting graphons
of uniform attachment, ranked attachment and P-ErdoS-R6nyi (ER) random graphs given by
Wunif(x,y)= 1 - max(x, y), Wrank(x,y)= 1 - xy and Wer(x,y)= p respectively (Borgs et al.,
2011; Lovasz, 2012), each of which exhibits different node connectivities as shown in Figure 2.
Figure 2: Three graphons used in our experiments. (a): Uniform attachment graphon; (b): Ranked
attachment graphon; (c): Erdos-Renyi (ER) graphon with edge probability 0.5.
3
Published as a conference paper at ICLR 2022
Finite agent graph game. For simplicity of analysis, we consider finite state and action spaces
X,U as well as times T := {0, 1, . . . , T - 1}. On a metric space A, define the spaces of all Borel
probability measures P(A) and all Borel measures B1 (A) bounded by 1, equipped with the L1
norm. For simplified notation, we denote both a measure ν and its probability mass function by
ν(∙). Define the space of policies Π := P(U)T×X, i.e. agents apply Markovian feedback policies
πi = (πti)t∈T ∈ Π that act on local state information. This allows for the definition of weakly
interacting agent state and action random variables
X0 〜μo,	Ui	〜∏i(∙∣	Xi),	Xi+1	〜P(∙	I Xt,Ut, Gt),	∀t ∈T, ∀i ∈	Vn	(5)
under some transition kernel P : X ×U × B1(X) → P(X), where the empirical neighborhood mean
field Git of agent i is defined as the B1(X)-valued (unnormalized) neighborhood state distribution
Gt ：= NN X ξij δχj,	(6)
j∈VN
where δ is the Dirac measure, i.e. each agent affects each other at most negligibly with factor 1/N .
Finally, for each agent i we define separate, competitive objectives
T-1
JiN(π1,...,πN) :=E	r(Xti,Uti,Git)	(7)
t=0
to be maximized over πi, where r : X × U × B1(X) → R is an arbitrary reward function.
Remark 1. We can also consider infinite horizons, JN(π1,..., πN) ≡ E [P∞=0 Ytr(Xi, Uti, Gt)]
with all results but Theorem 4 holding. One may also extend to state-action distributions, heteroge-
neous starting conditions and time-dependent r, P, though we avoid this for expositional simplicity.
Remark 2. Note that it is straightforward to extend to heterogeneous agents by modelling agent
types as part of the agent state, see also e.g. Mondal et al. (2021). It is only required to model agent
states in a unified manner, which does not imply that there can be no heterogeneity.
With this, we can give a typical notion of Nash equilibria as found e.g. in Saldi et al. (2018). How-
ever, under graph convergence in Assumption 1, it is always possible for a finite number of nodes
to have an arbitrary neighborhood differing from the graphon as N → ∞. Thus, it is impossible
to show approximate optimality for all nodes and only possible to show for an increasingly large
fraction 1- p → 1of nodes. For this reason, we slightly weaken the notion of Nash equilibria by
restricting to a fraction 1- p of agents, as e.g. considered in (Carmona, 2004; Elie et al., 2020).
Definition 1. An (ε, p)-Markov-Nash equilibrium (almost Markov-Nash equilibrium) for ε, p > 0
is defined as a tuple of policies (π1, . . . , πN) ∈ ΠN such that for any i ∈ WN, we have
JiN (π1, . . . , πN) ≥ sup JiN (π1, . . . , πi-1, π, πi+1, . . . , πN) - ε,	(8)
π∈Π
for some set WN ⊆ VN containing at least b(1- p)N c agents, i.e. |WN| ≥ b(1- p)N c.
The minimal such ε > 0 for any fixed policy tuple (and typically p = 0) is also called its exploitabil-
ity. Whilst we ordain ε-optimality only for a fraction 1- p of agents, if the fraction p is negligible,
it will have negligible impact on other agents as a result of the weak interaction property. Thus, the
solution will remain approximately optimal for almost all agents for sufficiently small p regardless
of the behavior of that fraction p of agents. In the following, we will give a limiting system that shall
provide (ε, p)-Markov-Nash equilibria with ε, p → 0 as N → ∞.
Graphon mean field game. The formal N → ∞ limit of the N -agent game constitutes its
graphon mean field game (GMFG), which shall be rigorously justified in Section 3. We define the
I
space of measurable state marginal ensembles Mt := P(X ) and measurable mean field ensembles
M := P(X)t×i, in the sense that α → μα(x) is measurable for any μ ∈ M, t ∈ T, X ∈ X.
Similarly, we define the space of measurable policy ensembles Π ⊆ ΠI, i.e. with measurable
α 7→ πtα (u | x) for any π ∈ Π, t ∈ T, x ∈ X , u ∈ U.
In the GMFG, we will consider infinitely many agents α ∈ I instead of the finitely many i ∈ VN .
As a result, We will have infinitely many policies ∏ɑ ∈ Π 一 one for each agent α 一 through some
measurable policy ensemble π ∈ Π. We again define state and action random variables
Xa〜μo,	Ua〜∏α(∙ιXF)	χα+ι〜P(∙ιχα,uα,Ga),	YCtt∈i×t ⑼
4
Published as a conference paper at ICLR 2022
where we introduce the (now deterministic) B1(X)-valued neighborhood mean field of agents α as
Gtα :=
[w (a,β)μβ dβ
(10)
for some deterministic μ ∈ M. Under fixed π ∈ Π, μa should be understood as the law of Xa,
μα ≡ L(Xa). Finally, define the maximization objective of agent a over πa for fixed μ ∈ M as
T-1
jμ(∏α)≡E Er(XlUaGa).	(“)
t=0
To formulate the limiting version of Nash equilibria, we define a map Ψ : Π → M mapping from a
policy ensemble π ∈ Π to the corresponding generated mean field ensemble μ = Ψ(π) ∈ M by
μ0 ≡ μo,	μa+ι(χ0)	≡ X	μt^(χ)	X ∏f(u |	χ)P(χ01	χ,u,g;)，∀α	∈ I (12)
x∈X	u∈U
where integrability in (10) holds by induction, and note how then μ? = L(Xa).
Similarly, let Φ: M → 2π map from a mean field ensemble μ to the set of optimal policy ensem-
bles π characterized by πa ∈ argmax∏∈∏ Jμ(π) for all a ∈ I, which is particularly fulfilled if
∏a(u | x) > 0 =⇒ U ∈ arg maXuθ∈u Qμ(t, x, u0) for all α ∈ I, t ∈ T, X ∈ X, U ∈ U, where Qμ
is the optimal action value function under fixed μ ∈ M following the Bellman equation
Qμ(t,x,u) = r(x,u, Ga) + X' P (x0 | x,u, G；)argmax Qg(t + 1,x0,u0)	(13)
x0∈X	u0∈U
with Qμ(T, x,u) ≡ 0 and generally time-dependent, see Puterman (2014) for a review.
We can now define the GMFG version of Nash equilibria as policy ensembles π generating mean
field ensembles μ under which they are optimal, as μ = L(Xa) if all agents a ∈ I follow ∏α.
Definition 2. A Graphon Mean Field Equilibrium (GMFE) is a pair (π, μ) ∈ Π X M such that
π ∈ Φ(μ) and μ = Ψ(π).
3 Theoretical analysis
To obtain meaningful optimality results beyond empirical mean field convergence, we will need
a Lipschitz assumption as in the uncontrolled, continuous-time case (cf. Bayraktar et al. (2020),
Condition 2.3) and typical in mean field theory (Huang et al., 2006).
Assumption 2. Let r, P, w be Lipschitz continuous with Lipschitz constants Lr, LP , LW > 0.
Note that all proofs but Theorem 1 also hold for only block-wise Lipschitz continuous w, see
Appendix A.1. Since X × U × B1(X) is compact, r is bounded by the extreme value theorem.
Proposition 1. Under Assumption 2, r will be bounded by |r| ≤ Mr for some constant Mr > 0.
We then obtain existence of a GMFE by reformulating the GMFG as a classical MFG and applying
existing results from Saldi et al. (2018). More precisely, we consider the equivalent MFG with
extended state space X ×I, action space U, policy ∏ ∈ P (U )t×x×i , mean field μ ∈ P (X × I )t ,
reward function r((x, α), u, μ) := r(x,u, JIW(αt,β)μt(∙,β) dβ) and transition dynamics such
∙-v	∙-v
that the states (Xt, at) follow (X0, α0)〜μ0 := μ0 0 Unif([0,1]) and
Ut 〜∏t(∙ | Xt,at),
∙-v
X^t+1
〜P(∙ I Xt, Ut, L
W(αt,β)μt(∙,β) dβ),
αt+1 = αt .
(14)
Theorem 1.	Under Assumption 2, there exists a GMFE (π, μ) ∈ Π × M.
Meanwhile, in finite games, even showing the existence of Nash equilibria in local feedback policies
is problematic (Saldi et al., 2018). Note however, that while this reformulation will be useful for
learning and existence, it does not allow us to conclude that the finite graph game is well approx-
imated, as classical MFG approximation theorems e.g. in Saldi et al. (2018) do not consider the
graph structure and directly use the limiting graphon W in the dynamics (14).
5
Published as a conference paper at ICLR 2022
As our next main result, we shall therefore show rigorously that the GMFE can provide increasingly
good approximations of the N -agent finite graph game as N → ∞. As mentioned, the following
also holds for only block-wise Lipschitz continuous W instead of fully Lipschitz continuous W .
Complete mathematical proofs together with additional theoretical supplements can be found in
Appendix A.1 and A.2. To obtain joint N -agent policies as approximate Nash equilibria from a
GMFE (π, μ), We define the map Γn(∏) ：= (π1,π2,..., πN) ∈ Πn, where
πti(u | x) := πtαi (u | x),	∀(α, t, x, u) ∈ I × T × X × U	(15)
with ai = N, as by Assumption 1 the agents are correctly labeled such that they match up with their
limiting graphon indices αi ∈ I. In our experiments, we use the αi generated during the generation
process of the W -random graphs, though for arbitrary finite systems one would have to first identify
the graphon as well as an appropriate assignment of agents to graphon indices αi ∈ I, which is a
separate, non-trivial problem requiring at least graphon estimation, e.g. Xu (2018).
For theoretical analysis, we propose to lift the empirical distributions and policy tuples to the con-
tinuous domain I, i.e. under an N-agentpolicy tuple (π1, . . . , πN) ∈ ΠN, we define the step policy
ensemble πn ∈ Π and the random empirical step measure ensemble μN ∈ M by
∏N,α ：= X 1α∈(导,NlF μN,α ：= X 1α∈(导,知∙ δXj,	∀(α,t) ∈I×T.	(16)
i∈VN	i∈VN
In the following, we consider deviations of the i-th agent from (π1, π2, . . . , πN) = ΓN (π) ∈ ΠN
to (∏1,..., ∏i-1, ∏, ∏i+1,..., ∏N) ∈ Πn, i.e. the i-th agent deviates by instead applying ∏ ∈ Π.
Note that this includes the special case of no agent deviations. For any f ： X × I → R and state
marginal ensemble μt ∈ Mt, define μt(f) ：= I Px∈X f (x, α)μα(x) dα. We are now ready to
state our first result of convergence of empirical state distributions to the mean field, potentially at
the classical rate O(1∕√N) and consistent with results in uncontrolled, continuous-time diffusive
graphon mean field systems (cf. Bayraktar et al. (2020), Theorem 3.2).
Theorem 2.	Consider Lipschitz continuous π ∈ Π up to a finite number of discontinuities Dπ,
with associated mean field ensemble μ = Ψ(π). Under Assumption 1 and the N-agent policy
(π1,..., πi-1,π, πi+1,..., πn) ∈ Πn with (π1, π2,..., πN) = ΓN(∏) ∈ ∏n, ∏ ∈ ∏, t ∈ T, we
have for all measurable functions f ： X × I → R uniformly bounded by some Mf > 0, that
E [∣μN(f) - μt(f)∣] → 0	(O)
uniformly over all possible deviations π ∈ Π,i ∈ VN. Furthermore, if the graphon Convergence in
Assumption 1 is at rate O(1∕√N), then this rate ofconvergence is also O(1∕√N).
In particular, the technical Lipschitz requirement of π typically holds for neural-network-based poli-
cies (Mondal et al., 2021; Pasztor et al., 2021) and includes also the case of finitely many optimality
regimes over all graphon indices α ∈ I, which is sufficient to achieve arbitrarily good approximate
Nash equilibria through our algorithms as shown in Section 4. We would like to remark that the
above result generalizes convergence of state histograms to the mean field solution, since the state
marginals of agents are additionally close to each of their graphon mean field equivalents. The above
will be necessary to show convergence of the dynamics of a deviating agent to
八 i	i ɪ	ʌ ɪ	入W	八 i 八 i	i
XN	TTN ʌ / ∖ V N ∖ V N	T~>f ∖VN TTNCN ∖	V-∕√- 1- ZT-	/1 O∖
0 〜μo,	UtN	〜πt(∙ |	XtN),	χt+ι 〜P(∙	| XtN,utN, GtN),	∀t	∈ T	(18)
for almost all agents i, i.e. the dynamics are approximated by using the limiting deterministic
neighborhood mean field G GN, see Appendix A.1. This will imply the approximate Nash property:
Theorem 3. Consider a GMFE (π, μ) with Lipschitz continuous π up to a finite number ofdiscon-
tinuities Dπ. Under Assumptions 1 and 2, for any ε,p > 0 there exists N0 such that for all N > N0,
the policy (π1, . . . , πN) = ΓN (π) ∈ ΠN is an (ε, p)-Markov Nash equilibrium, i.e.
JiN (π1, . . . , πN) ≥ max JiN (π1, . . . , πi-1, π, πi+1, . . . , πN) - ε	(19)
i	π∈Π i
for all i ∈ WN and some WN ⊆ VN ： |WN| ≥ b(1 - p)N c.
In general, Nash equilibria are highly intractable (Daskalakis et al., 2009). Therefore, solving the
GMFG allows obtaining approximate Nash equilibria in the N-agent system for sufficiently large N,
since ε, p → 0 as N → ∞. As a side result, we also obtain first results for the uncontrolled discrete-
time case by considering trivial action spaces with |U| = 1, see Corollary A.2 in the Appendix.
6
Published as a conference paper at ICLR 2022
4 Learning graphon mean field equilibria
By learning GMFE, one may potentially solve otherwise intractable large N -agent games. For learn-
ing, we can apply any existing techniques for classical MFGs (e.g. Mguni et al. (2018); Subramanian
& Mahajan (2019); Guo et al. (2019)), since by (14) we have reformulated the GMFG as a classical
MFG with extended state space. Nonetheless, it may make sense to treat the graphon index α ∈ I
separately, e.g. when treating special cases such as block graphons, or by grouping graphically sim-
ilar agents. We repeatedly apply two functions Φ, Ψ by beginning with the mean field μ0 = Ψ(π0)
generated by the uniformly random policy π0, and computing πn+1 = Φ(μn), μn+1 = Ψ(πn+1)
for n = 0, 1, . . . until convergence using one of the following two approaches:
•	Equivalence classes method. We introduce agent equivalence classes, or discretization,
of I for the otherwise uncountably many agents α ∈ I by partitioning I into M subsets.
For example, in the special case of block graphons (block-wise constant W), one can solve
separately for each block equivalence class (type) of agents, since all agents in the class
share the same dynamics. Note that in contrast to multi-class MFGs (Huang et al., 2006),
GMFGs are rigorously connected to finite graph games and can handle an uncountable
number of classes α. To deal with general graphons, we choose equidistant representatives
∙-v
αi ∈ I, i = 1, . . . , M covering the whole interval I, and approximate each agent α ∈ Ii
∙-v
by the nearest αi for the intervals Ii ⊆ I of points closest to that αi to obtain M approxi-
mate equivalence classes. Formally, we approximate mean fields Ψ(π) = PMI 1a∈τiμαi
recursively computed over all times for any fixed policy ensemble π, and similarly policies
Φ(μ) = ∑Mι 1α∈li∏ai where πai is the optimal policy of αi for fixed μ. We solve
the optimal control problem for each equivalence class using backwards induction (alter-
natively, one may use reinforcement learning), and solve the evolution equation for the
representatives αi of the equivalence classes recursively. For space reasons, the details are
found in Appendix A.3. Note that this does not mean that we consider the N -agent problem
with N = M, but instead we approximate the limiting problem with the limiting graphon
W , and the solution will be near-optimal for all sufficiently large finite systems at once.
•	Direct reinforcement learning. We directly apply RL as Φ. The central idea is to consider
the GMFG as a classical MFG with extended state space X × I, i.e. for fixed mean fields,
we solve the MDP defined by (14). Agents condition their policy not only on their own
state, but also their node index α ∈ I and the current time t ∈ T, since the mean fields
are non-stationary in general and require time-dependent policies for optimality. Here, we
assume that we can sample from a simulator of (9) for a given fixed mean field as commonly
assumed in MFG learning literature (Guo et al., 2019; Subramanian & Mahajan, 2019). For
application to arbitrary finite systems, one could apply a model-based RL approach coupled
with graphon estimation, though this remains outside the scope of this work. For solving
the mean field evolution equation (12), we can again use any applicable numerical method
and choose a conventional sequential Monte Carlo method for Ψ. While it is possible to
exactly solve optimal control problems for each agent equivalence class with finite state-
action spaces, this is generally not the case for e.g. continuous state-action spaces. Here,
a general reinforcement learning solution can solve otherwise intractable problems in an
elegant manner, since the graphon index α simply becomes part of a continuous state space.
For convergence, we begin by stating the classical feedback regularity condition (Huang et al., 2006;
Guo et al., 2019) after equipping Π, M e.g. with the supremum metric.
ʌ ʌ
Proposition 2. Assume that the maps Ψ, Φ are LiPschitz with constants c1,c2 and ci ∙ c2 < 1. Then
the fixed point iteration μn+i = Ψ(Φ(μn)) converges.
Feedback regularity is not assured, and thus there is no general convergence guarantee. Whilst one
could attempt to apply fictitious play (Mguni et al., 2018), additional assumptions will be needed
for convergence. Instead, whenever necessary for convergence, we regularize by introducing Boltz-
mann policies ∏α(u | x) a exp(η!Qμ(t,x,u)) with temperature η, provably converging to an
approximation for sufficiently high temperatures (Cui & Koeppl, 2021).
Theorem 4.	Under Assumptions 1 and 2, the equivalence classes algorithm with Boltzmann policies
Φ(μ)α(u | x) a exp(1 Qμ(t,x,u)) ConvergeSforSufftCIentIyhIgh temperatures η > 0.
7
Published as a conference paper at ICLR 2022
1.0
0.5
0.0
(a)	Wunif	α
0	20	40
1.0
0.5
0.0
0.50
0.25
0.00
0	20	40
1.0
0.5
0.0
ttt
Figure 3: Achieved equilibrium via M = 100 approximate equivalence classes in SIS-Graphon,
plotted for each agent α ∈ I. Top: Probability of taking precautions when healthy. Bottom: Proba-
bility of being infected. It can be observed that agents with less connections (higher α) will take less
precautions. (a): Uniform attachment graphon; (b): Ranked attachment graphon; (c): ER graphon.
Importantly, even an exact solution of the GMFG only constitutes an approximate Nash equilibrium
in the finite-graph system. Furthermore, even the existence of exact finite-system Nash equilibria
in local feedback policies is not guaranteed, see the discussion in Saldi et al. (2018) and references
therein. Therefore, little is lost by introducing slight additional approximations for the sake of a
tractable solution, if at all needed (e.g. the Investment-Graphon problem in the following converges
without introducing Boltzmann policies), since near optimality holds for small temperatures (Cui &
Koeppl, 2021). Indeed, we find that we can show optimality of the equivalence classes approach for
sufficiently fine partitions of I, giving us a theoretical foundation for our proposed algorithms.
ʌ ʌ
Theorem 5.	Under Assumptions 1 and 2, for a solution (π, μ) ∈ Π X M, π ∈ Φ(μ), μ = Ψ(π)
of the M equivalence classes method and for any ε, p > 0 there exists N0 , M ∈ N such that for all
N > N0, the policy (π1, . . . , πN) = ΓN (π) ∈ ΠN is an (ε, p)-Markov Nash equilibrium.
A theoretically rigorous analysis of the elegant direct reinforcement learning approach is beyond our
scope and deferred to future works, though we empirically find that both methods agree.
5	Experiments
In this section, we will give an empirical verification of our theoretical results. As we are unaware
of any prior discrete-time GMFGs (except for the example in Vasal et al. (2021), which is similar
to the first problem in the following), we propose two problems adapted from existing non-graph-
based works on the three graphons in Figure 2. For space reasons, we defer detailed descriptions of
problems and algorithms, plots as well as further analysis, including exploitability and a verification
of stability of our solution with respect to the number of equivalence classes - to Appendix A.3.
The SIS-Graphon problem was considered in Cui & Koeppl (2021) as a classical discrete-time
MFG. We impose an epidemics scenario where people (agents) are infected with probability propor-
tional to the number of infected neighbors and recover with fixed probability. People may choose to
take precautions (e.g. social distancing), avoiding potential costly infection periods at a fixed cost.
In the Investment-Graphon problem - an adaptation of a problem studied by Chen et al. (2021),
where it was in turn adapted from Weintraub et al. (2010) - we consider many firms maximizing
profits, where profits are proportional to product quality and decrease with total neighborhood prod-
uct quality, i.e. the graph models overlap in e.g. product audience or functionality. Firms can invest
to improve quality, though it becomes more unlikely to improve quality as their quality rises.
Learned equilibrium behavior. For the SIS-Graphon problem, we apply softmax policies for
each approximate equivalence class to achieve convergence, see Appendix A.3 for details on tem-
perature choice and influence. In Figure 3, the learned behavior can be observed for various α. As
expected, in the ER graphon case, behavior is identical over all α. Otherwise, we find that agents
take more precautions with many connections (low α) than with few connections (high α). For the
uniform attachment graphon, we observe no precautions in case of negligible connectivity (α → 1),
while for the ranked attachment graphon there is no such α ∈ I (cf. Figure 2). Further, the fraction
of infected agents at stationarity rises as α falls. A similar analysis holds for Investment-Graphon
without need for regularization, see Appendix A.3.
8
Published as a conference paper at ICLR 2022
(b) SIS-Graphon, Wrank	(c) SIS-Graphon, Wer
SIS-Graphon, Wunif
Q- C
岛—豆需
p 0i	I ,
0	25	50	75	100
N
Investment-Graphon, Wunif
5
- NJ|ix
- NJ|ix
- N J| ixam
p 0i	I ,
0	25	50	75	100
N
Investment-Graphon, Wrank
5 O
αJ- N J| ixam
- N J| ixam
0	25	50	75	100
N
Investment-Graphon, Wer
S
Figure 4: Decreasing maximum deviation between average N -agent objective and mean field objec-
tive over all agents for the GMFE policy and 5 W -random graph sequences. (a): Uniform attachment
graphon; (b): Ranked attachment graphon; (c): ER graphon.
Note that the specific method of solution is not of central importance here, as in general any RL
and filtering method can be substituted to handle 1. otherwise intractable or 2. inherently sample-
based settings. Indeed, we achieve similar results using PPO (Schulman et al., 2017) in Investment-
Graphon, enabling a general RL-based methodology for GMFGs. In Appendix A.3, we find that
PPO achieves qualitatively and quantitatively similar behavior to the equivalence classes method,
with slight deviations due to the approximations from PPO and Monte Carlo. In particular, the
PPO exploitability ε ≈ 2 remains low compared to ε > 30 for the uniform random policy, see
Appendix A.3. In Appendix A.3, we also show how, due to the non-stationarity of the environment,
a naive application of MARL (Yu et al., 2021) fails to converge, while existing mean field MARL
techniques (Yang et al., 2018b) remain incomparable as agents must observe the average actions of
all neighbors. On SIS-Graphon, we require softmax policies to achieve convergence, which is not
possible with PPO as no Q-function is learned. In general, one could use entropy regularized poli-
cies, e.g. SAC (Haarnoja et al., 2018), or alternatively use any value-based reinforcement learning
method, though an investigation of the best approach is outside of our scope.
Quantitative verification of the mean field approximation. To verify the rigorously established
accuracy of our mean field system empirically, we will generate W -random graphs. Note that there
are considerable difficulties associated with an empirical verification of (19), since 1. for any N
one must check the Nash property for (almost) all N agents, 2. finding optimal ∏ is intractable,
as no dynamic programming principle holds on the non-Markovian local agent state, while acting
on the full state fails by the curse of dimensionality, and 3. the inaccuracy from estimating all
JiN, i = 1, . . . , N at once increases with N due to variance, i.e. cost scales fast with N for fixed
variance. Instead, we verify (26) in Appendix A.1 using the GMFE policy on systems of up to
N = 100 agents, i.e. ∏ = ∏αi for the closest α% and comparing for all agents at once (p = 0).
Shown in Figure 4, for W -random graph sequences, at each N we performed 10000 runs to estimate
maxi |JiN - Jαi |. We find that the maximum deviation between achieved returns and mean field
return decreases as N → ∞, verifying that we obtain an increasingly good approximation of the
finite N -agent graph system. The oscillations in Figure 4 stem from the randomly sampled graphs.
6	Conclusion
In this work, we have formulated a new framework for dense graph-based dynamical games with the
weak interaction property. On the theoretical side, we have given one of the first general discrete-
time GMFG formulations with existence conditions and approximate Nash property of the finite
graph system, thus extending classical MFGs and allowing for a tractable, theoretically well-founded
solution of competitive large-scale graph-based games on large dense graphs. On the practical side,
we have proposed a number of computational methods to tractably compute GMFE and experi-
mentally verified the plausibility of our methodology on a number of examples. Venues for further
extensions are manifold and include extensions of theory to e.g. continuous spaces, partial ob-
servability or common noise. So far, graphons assume dense graphs and cannot properly describe
sparse graphs (W = 0), which remain an active frontier of research. Finally, real-world applica-
tion scenarios may be of interest, where estimation of agent graphon indices becomes important for
model-based MARL. We hope that our work inspires further applications and research into scalable
MARL using graphical dynamical systems based on graph limit theory and mean field theory.
9
Published as a conference paper at ICLR 2022
Acknowledgments
This work has been funded by the LOEWE initiative (Hesse, Germany) within the emergenCITY
center. The authors also acknowledge support by the German Research Foundation (DFG) via the
Collaborative Research Center (CRC) 1θ53 - MAKL
Ethics S tatement
Existing mean field methodologies, including ours, currently require manual modeling and have not
been applied in a model-based reinforcement learning setting for given finite agent systems. As a
result, we do not foresee any immediate ethical issues stemming from this work.
Reproducibility Statement
For reproducibility, in the supplement we provide all code required to reproduce all results in this
work. This includes but is not limited to our models and problems, algorithms as well as all plotting
scripts for all of the figures found in this work.
References
Alexander Aurell, Rene Carmona, Gokce Dayanikli, and Mathieu Lauriere. Finite state graphon
games with applications to epidemics. arXiv preprint arXiv:2106.07859, 2021a.
Alexander Aurell, Rene Carmona, and Mathieu Lauriere. Stochastic graphon games: Ii. the linear-
quadratic case. arXiv preprint arXiv:2105.12320, 2021b.
Fabio Bagagiolo and Dario Bauso. Mean-field games and dynamic demand management in power
grids. Dynamic Games and Applications, 4(2):155-176, 2014.
Reginald A Banez, Lixin Li, Chungang Yang, Lingyang Song, and Zhu Han. A mean-field-type
game approach to computation offloading in mobile edge computing networks. In ICC 2019-
2019 IEEE International Conference on Communications (ICC), pp. 1-6. IEEE, 2019.
Erhan Bayraktar, Suman Chakraborty, and Ruoyu Wu. Graphon mean field systems. arXiv preprint
arXiv:2003.13180, 2020.
Marc G Bellemare, Salvatore Candido, Pablo Samuel Castro, Jun Gong, Marlos C Machado, Sub-
hodeep Moitra, Sameera S Ponda, and Ziyu Wang. Autonomous navigation of stratospheric bal-
loons using reinforcement learning. Nature, 588(7836):77-82, 2020.
Alain Bensoussan, Jens Frehse, and Phillip Yam. Mean field games and mean field type control
theory, volume 101. Springer, 2013.
Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, PrzemysIaW Debiak, Christy
Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large
scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.
Gianmarco Bet, Fabio Coppini, and Francesca R Nardi. Weakly interacting oscillators on dense
random graphs. arXiv preprint arXiv:2006.07670, 2020.
Christian Borgs, Jennifer Chayes, Lgszl6 Lovdsz, Vera S6s, and Katalin Vesztergombi. Limits of
randomly groWn graph sequences. European Journal of Combinatorics, 32(7):985-999, 2011.
Peter E Caines and Minyi Huang. Graphon mean field games and the gmfg equations: ε-nash
equilibria. In 2019 IEEE 58th Conference on Decision and Control (CDC), pp. 286-292. IEEE,
2019.
Pierre Cardaliaguet and Saeed Hadikhanloo. Learning in mean field games: the fictitious play.
ESAIM: Control, Optimisation and Calculus of Variations, 23(2):569-591, 2017.
Guilherme Carmona. Nash equilibria of games With a continuum of players, 2004.
10
Published as a conference paper at ICLR 2022
Rene Carmona, Daniel Cooney, Christy Graves, and Mathieu Lauriere. Stochastic graphon games:
I. the static case. arXiv preprint arXiv:1911.10664, 2019a.
Rene Carmona, Mathieu Lauriere, and Zongjun Tan. Model-free mean-field reinforcement learning:
mean-field mdp and mean-field q-learning. arXiv preprint arXiv:1910.12802, 2019b.
Yang Chen, Jiamou Liu, and Bakhadyr Khoussainov. Agent-level maximum entropy inverse rein-
forcement learning for mean field games. arXiv preprint arXiv:2104.14654, 2021.
Kai Cui and Heinz Koeppl. Approximately solving mean field games via entropy-regularized deep
reinforcement learning. In International Conference on Artificial Intelligence and Statistics, pp.
1909-1917. PMLR, 2021.
Kai Cui, Anam Tahir, Mark Sinzger, and Heinz Koeppl. Discrete-time mean field control with
environment states. arXiv preprint arXiv:2104.14900, 2021.
Constantinos Daskalakis, Paul W Goldberg, and Christos H Papadimitriou. The complexity of
computing a nash equilibrium. SIAM Journal on Computing, 39(1):195-259, 2009.
Boualem Djehiche, Alain Tcheukam, and Hamidou Tembine. Mean-field-type games in engineering
[j]. AIMS Electronics and Electrical Engineering, 1(1):18-73, 2017.
Romuald Elie, Julien Perolat, Mathieu Lauriere, Matthieu Geist, and Olivier Pietquin. On the con-
vergence of model free learning in mean field games. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 34, pp. 7143-7150, 2020.
Shuang Gao and Peter E Caines. The control of arbitrary size networks of linear systems via graphon
limits: An initial investigation. In 2017 IEEE 56th Annual Conference on Decision and Control
(CDC), pp. 1052-1057. IEEE, 2017.
Shuang Gao and Peter E Caines. Graphon control of large-scale networks of linear systems. IEEE
Transactions on Automatic Control, 65(10):4090-4105, 2019a.
Shuang Gao and Peter E Caines. Spectral representations of graphons in very large network systems
control. In 2019 IEEE 58th Conference on Decision and Control (CDC), pp. 5068-5075. IEEE,
2019b.
Marios-Antonios Gkogkas and Christian Kuehn. Graphop mean-field limits for kuramoto-type mod-
els. arXiv preprint arXiv:2007.02868, 2020.
Haotian Gu, Xin Guo, Xiaoli Wei, and Renyuan Xu. Mean-field controls with q-learning for coop-
erative marl: Convergence and complexity analysis. arXiv preprint arXiv:2002.04131, 2020.
Haotian Gu, Xin Guo, Xiaoli Wei, and Renyuan Xu. Mean-field multi-agent reinforcement learning:
A decentralized network approach. arXiv preprint arXiv:2108.02731, 2021.
Olivier Gueant, Jean-Michel Lasry, and Pierre-Louis Lions. Mean field games and applications. In
Paris-Princeton lectures on mathematical finance 2010, pp. 205-266. Springer, 2011.
Xin Guo, Anran Hu, Renyuan Xu, and Junzi Zhang. Learning mean-field games. In Advances in
Neural Information Processing Systems, pp. 4966-4976, 2019.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International confer-
ence on machine learning, pp. 1861-1870. PMLR, 2018.
Minyi Huang, Roland P Malhame, and Peter E Caines. Large population stochastic dynamic games:
closed-loop mckean-vlasov systems and the nash certainty equivalence principle. Communica-
tions in Information & Systems, 6(3):221-252, 2006.
B Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A Al Sallab, Senthil Yoga-
mani, and Patrick Perez. Deep reinforcement learning for autonomous driving: A survey. IEEE
Transactions on Intelligent Transportation Systems, 2021.
11
Published as a conference paper at ICLR 2022
Arman C Kizilkale and Roland P Malhame. Collective target tracking mean field control for electric
space heaters. In 22nd Mediterranean Conference on Control and Automation, pp. 829-834.
IEEE, 2014.
Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The
International Journal of Robotics Research, 32(11):1238-1274, 2013.
Daniel Lacker and Agathe Soret. A case study on stochastic games on large graphs in mean field
and sparse regimes. arXiv preprint arXiv:2005.14102, 2020.
Jean-Michel Lasry and Pierre-Louis Lions. Mean field games. Japanese journal of mathematics, 2
(1):229-260, 2007.
Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg, Joseph Gon-
zalez, Michael Jordan, and Ion Stoica. Rllib: Abstractions for distributed reinforcement learning.
In International Conference on Machine Learning, pp. 3053-3062. PMLR, 2018.
Lgszl6 Lovdsz. Large networks and graph limits, volume 60. American Mathematical Soc., 2012.
David Mguni, Joel Jennings, and Enrique Munoz de Cote. Decentralised learning in systems with
many, many strategic agents. Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
Washim Uddin Mondal, Mridul Agarwal, Vaneet Aggarwal, and Satish V Ukkusuri. On the approx-
imation of cooperative heterogeneous multi-agent reinforcement learning (marl) using mean field
control (mfc). arXiv preprint arXiv:2109.04024, 2021.
MederiC Motte and HUyen Pham. Mean-field markov decision processes with common noise and
open-loop controls. arXiv preprint arXiv:1912.07883, 2019.
Mojtaba Nourian and Peter E Caines. -nash mean field game theory for nonlinear stochastic dy-
namical systems with major and minor agents. SIAM Journal on Control and Optimization, 51
(4):3302-3331, 2013.
Francesca Parise and Asuman Ozdaglar. Graphon games. In Proceedings of the 2019 ACM Confer-
ence on Economics and Computation, pp. 457-458, 2019.
Barna Pasztor, Ilija Bogunovic, and Andreas Krause. Efficient model-based multi-agent mean-field
reinforcement learning. arXiv preprint arXiv:2107.04050, 2021.
Julien Perolat, Sarah Perrin, Romuald Elie, Mathieu Lauriere, Georgios Piliouras, Matthieu Geist,
Karl Tuyls, and Olivier Pietquin. Scaling up mean field games with online mirror descent. arXiv
preprint arXiv:2103.00623, 2021.
Sarah Perrin, Mathieu Lauriere, Julien Perolat, Romuald Elie, Matthieu Geist, and Olivier Pietquin.
Generalization in mean field games by learning master policies. arXiv preprint arXiv:2109.09717,
2021a.
Sarah Perrin, Mathieu Lauriere, Julien Perolat, Matthieu Geist, Romuald Elie, and Olivier Pietquin.
Mean field games flock! the reinforcement learning way. arXiv preprint arXiv:2105.07933,
2021b.
Huy Xuan Pham, Hung Manh La, David Feil-Seifer, and Aria Nefian. Cooperative and distributed
reinforcement learning of drones for field coverage. arXiv preprint arXiv:1803.07250, 2018.
Huyen Pham and Xiaoli Wei. Bellman equation and viscosity solutions for mean-field stochas-
tic control problem. ESAIM: Control, Optimisation and Calculus of Variations, 24(1):437-461,
2018.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John
Wiley & Sons, 2014.
Naci Saldi, Tamer Basar, and Maxim Raginsky. Markov-nash equilibria in mean-field games with
discounted cost. SIAM Journal on Control and Optimization, 56(6):4256-4287, 2018.
12
Published as a conference paper at ICLR 2022
Naci Saldi, Tamer Bayar, and Maxim Raginsky. Approximate nash equilibria in partially observed
stochastic games with mean-field interactions. Mathematics ofOperations Research, 44(3):1006—
1033, 2019.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Jayakumar Subramanian and Aditya Mahajan. Reinforcement learning in stationary mean-field
games. In Proceedings of the 18th International Conference on Autonomous Agents and MultiA-
gent Systems,pp. 251-259, 2019.
Jan Tozicka, Benedek Szulyovszky, Guillaume de Chambrier, Varun Sarwal, Umar Wani, and Man-
tas Gribulis. Application of deep reinforcement learning to uav fleet control. In Proceedings of
SAI Intelligent Systems Conference, pp. 1169-1177. Springer, 2018.
Deepanshu Vasal, Rajesh Mishra, and Sriram Vishwanath. Sequential decomposition of graphon
mean field games. In 2021 American Control Conference (ACC), pp. 730-736. IEEE, 2021.
Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets,
Michelle Yeo, Alireza Makhzani, Heinrich Kuttler, John Agapiou, Julian Schrittwieser, et al.
Starcraft ii: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782, 2017.
Renato Vizuete, Paolo Frasca, and Federica Garin. Graphon-based sensitivity analysis of sis epi-
demics. IEEE Control Systems Letters, 4(3):542-547, 2020.
Gabriel Y Weintraub, C Lanier Benkard, and Benjamin Van Roy. Computational methods for obliv-
ious equilibrium. Operations research, 58(4-part-2):1247-1265, 2010.
Jiaming Xu. Rates of convergence of spectral methods for graphon estimation. In International
Conference on Machine Learning, pp. 5433-5442. PMLR, 2018.
Jiachen Yang, Xiaojing Ye, Rakshit Trivedi, Huan Xu, and Hongyuan Zha. Learning deep mean
field games for modeling large population behavior. In International Conference on Learning
Representations, 2018a.
Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang. Mean field multi-
agent reinforcement learning. In International Conference on Machine Learning, pp. 5571-5580.
PMLR, 2018b.
James J Yeh. Real Analysis: Theory Of Measure And Integration. World Scientific Publishing
Company, 2014.
Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising
effectiveness of mappo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955,
2021.
Kaiqing Zhang, Zhuoran Yang, and Tamer Basyar. Multi-agent reinforcement learning: A selective
overview of theories and algorithms. Handbook of Reinforcement Learning and Control, pp.
321-384, 2021.
13
Published as a conference paper at ICLR 2022
A Appendix
A. 1 Theoretical details
In this section, we will give all intermediate results required to prove the results in the main text, as
well as additional results, e.g. for the uncontrolled case. For convenience, we first state all obtained
theoretical result. Proofs for each of the theorems and corollaries can be found in their own sections
further below.
Note that except for Theorem 1, as mentioned in the main text we can also slightly weaken As-
sumption 2 to block-wise Lipschitz continuous W, i.e. there exist LW > 0 and disjoint intervals
{I1 , . . . , IQ}, ∪iIi = I s.t. ∀i, j ∈ {1, . . . , Q},
|W(χ,y) - W(x,y)∣ ≤ LW(|x - x| + |y - y|), ∀(χ,y), (x,y) ∈ ‰ XIj	(20)
which is fulfilled e.g. for block-wise Lipschitz-continuous or block-wise constant graphons.
For α ∈ I, define the α-neighborhood maps Gα : Mt → P(X) and empirical α-neighborhood
maps GαN : Mt → P(X ) as
Gα(μt)
:= [w (α,β)μβ dβ,
GN(μt) := [wn(α,β)μβdβ
(21)
and note how We naturally have Ga = Gα(μt) in the mean field system and Gi = GN (μN) in
the finite system. Finally, for ν, ν0 ∈ Mt , π ∈ Π and graphon W, define the ensemble transition
kernel operator Ptπ,ν,W : Mt → Mt via
(VPnVOW )ɑ ≡ XV a(x) X∏a(u | x)P (∙ | x,u,yW(α,β)ν0β dβ)	(22)
x∈X	u∈U	I
and note how we have μt+ι = μtP^π,μt,w in the mean field system.
After showing Theorem 2, we continue by showing convergence of the law of deviating agent state
Xti to the law of the corresponding auxiliary mean field systems given by
八 i	i ɪ	ʌ ɪ	入W	八 i 八 i	i
XN	TTN d/ Iv N' V N	T~>f ∖VN TTNCN ∖	V-∕√- 1- ZT-	∕CO∖
0 〜μ0,	UtN	〜πt(∙	|	XtN),	χt+ι	〜P(∙	|	XtN,UtN, GtN),	∀t	∈ T (23)
for almost all agents i as N → ∞.
Lemma A.1. Consider Lipschitz continuous π ∈ Π up to a finite number of discontinuities Dπ,
with associated mean field ensemble μ = Ψ(π). Under Assumptions 1 and 2 and the N-agent
policy (π1,..., πi-1,∏, πi+1,..., πN) ∈ Πn where (π1, π2,..., πN) = Γn (π) ∈ Πn, π ∈ Π
arbitrary, for any uniformly bounded family of functions G from X to R and any ε, p > 0, t ∈ T,
there exists N0 ∈ N such that for all N > N0 we have
SupIE [g(Xi)] - E [g(Xti)i| <ε	(24)
uniformly over π ∈ Π,i ∈ WN for some WN ⊆ VN with |Wn| ≥ b(1 — P)NC.
Similarly, for any uniformly Lipschitz, uniformly bounded family of measurable functions H from
X × B1 (X ) to R and any ε, p > 0, t ∈ T, there exists N0 ∈ N such that for all N > N0 we have
sup
h∈H
|e [h(Xi, GN (μN))] - E [h(X^, GN (μt))]∣
<ε
(25)
uniformly over π ∈ Π,i ∈ WN for some WN ⊆ VN with |Wn| ≥ b(1 — P)NC.
As a direct implication of the above results, the objective functions of almost all agents converge
uniformly to the mean field objectives.
Corollary A.1. Consider Lipschitz continuous π ∈ Π up to a finite number of discontinuities Dπ,
with associated mean field ensemble μ = Ψ(π). Under Assumptions 1 and 2 and the N-agent
policy (π1,..., πi-1,∏, πi+1,..., πn) ∈ Πn where (π1, π2,..., πn) = Γn (π) ∈ Πn, π ∈ Π
arbitrary, for any ε, P > 0, there exists N0 ∈ N such that for all N > N0 we have
∣ Jn (π1,...,πiτ,∏,πi+1,...,πN) - Jμ (π)∣ < ε	(26)
uniformly over π ∈ Π,i ∈ WN for some WN ⊆ VN with |Wn| ≥ b(1 — P)NC.
14
Published as a conference paper at ICLR 2022
The approximate Nash property (Theorem 3) of a GMFE (π, μ) then follows immediately from the
definition of a GMFE, since π is by definition optimal under μ.
As a corollary, we also obtain results for the uncontrolled case without actions, which is equivalent
to the case where |U| = 1, i.e. there being only one trivial policy that is always optimal.
Corollary A.2. Under Assumption 1 and |U| = 1, we have for all measurable functions f : X ×I →
R uniformly bounded by |f | ≤ Mf and all t ∈ T that
E [∣μN(f) - μt(f)∣] → 0.	QT)
Furthermore, if the convergence in Assumption 1 is at rate O(1∕√N), the rate of convergence is
also at O(1∕√N).
If further Assumption 2 holds, then for any uniformly bounded family of functions G from X to R
and any ε, p > 0, t ∈ T, there exists N0 ∈ N such that for all N > N0 we have
sup∣E [g(Xi)] - E [g(Xti )i∣<ε	(28)
uniformly over i ∈	WN	for some	WN	⊆	VN	with |WN|	≥	b(1	- p)N c,	and similarly for any
uniformly Lipschitz, uniformly bounded family of measurable functions H from X × B1 (X) to R
and any ε, p > 0, t ∈ T, there exists N0 ∈ N such that for all N > N0 we have
sup
h∈H
∣E [h(Xi, GN (〃N))] — E [h(X户,GN (μt))]∣
<ε
(29)
uniformly over i ∈ WN for some WN ⊆ VN with |WN| ≥ b(1 - p)N c.
A.2 Proofs
In this section, we will give full proofs to the statements made in the main text and in Appendix A.1.
A.2. 1 Proof of Theorem 1
Proof. First, we will verify Saldi et al. (2018), Assumption 1 for the MFG with dynamics given by
(14). For this purpose, as in Saldi et al. (2018) let us metrize the product space with the sup-metric,
and equip the space P(X × I) with the weak topology. Note that the results hold for both the finite
and infinite horizon setting, see Saldi et al. (2018), Remark 6.
(a)	The reward function r((x,α),u,μ) := r(x,u, JT W(αt,β)μt(∙,β) dβ) is continuous,
since for ((Xn, an), un, μn) → ((x, α), u, μ) We have
W W(αn, β)μn(∙, β)dβ → W W(α, β)μ(∙, β) dβ
by Lipschitz continuity of W and weak convergence of μn and therefore
r(xn,Un J W(αn,β)μn(∙,β)dβ) → r(x,u,/W(α,β)μ(∙,β)dβ)
by Assumption 2.
(b)	The action space is compact and the state space is locally compact.
(c)	Consider the moment function w(x, α) ≡ 2. In this case, we can choose ζ = 1 (we use ζ
instead of α in Saldi et al. (2018)).
∙-v	∙-v	∙-v	∙-v	∙-v
(d)	The stochastic kernel P that fulfills (14) such that (Xt+1,αt+1)〜P(∙ | (Xt, αt), Ut, μt)
is weakly continuous, since for ((xn, αn),un μn → ((x, α), u, μ) we again have
W W(αn, β)μn(∙, β)dβ → W W(α, β)μ(∙, β) dβ
15
Published as a conference paper at ICLR 2022
and therefore for any continuous bounded f : X × I → R,
f	f dP( ∙ I(Xn, an ), Un, μn )
=/f f dPα(∙∣ (xn,α),Un,"n) 6仪九(da)
=	f dP(∙ | xn, un,[w (αn,β)μn(∙,β)dβ)
→ f f dP(∙ | x,u, [ W(α,β)μ(∙,β)dβ)
XI
=[f dP(∙ | (x,α),u,μ)
X×I
∙-v
by disintegration of P and (14).
(e)	By boundedness of r, we trivially have v(x) ≡ 1 ≤ ∞.
(f)	By boundedness of r, we can trivially choose β = 1 (we flip the usage of β and γ).
(g)	As a result of the above choices, ζγβ = γ < 1 trivially.
By Saldi et al. (2018), Theorem 3.3 We have the existence of a mean field equilibrium (∏, μ) with
∙-v
some Markovian feedback policy ∏ acting on the state (Xt, at). By defining the mean field and
policy ensembles π, μ via ∏α(u ∣ x) = ∏t(u ∣ x, α), μ = "t(∙, α), we obtain existence of the
a-a.e. optimal policy ensemble π, since at any time t ∈ T, thejoint state-action distribution μt 0 ∏
puts mass 1 on optimal state-action pairs (see Saldi et al. (2018), Theorem 3.6), implying that for
a.e. a the policy must be optimal, as otherwise there exists a non-null set I。⊆ I such that for all
a ∈ lɔ,there is some suboptimality ε > 0, which directly contradicts the prequel.
For the remaining suboptimal α ∈ I0 in the null set I0 ⊆ I, we redefine π optimally for those α
(always possible in our case, see e.g. Puterman (2014)). This policy ensemble generates μ = Ψ(π)
α-a.e. uniquely, and we need only consider its α-a.e. unique equivalence class for optimality,
implying π ∈ Φ(μ). Furthermore, μ is always measurable by definition, whereas π is measurable
because ∏t is by definition a Markov kernel, and thus ∏t(u ∣ ∙, ∙) = ∏t({u} ∣ ∙, ∙) for Borel set {u} is
a measurable function, which implies measurability of ∏t(u ∣ x, ∙) (see e.g. Yeh (2014), Appendix
E). Therefore, we have proven existence of the GMFE (π, μ).	□
A.2.2 Proof of Theorem 2
Proof. The proof is by induction as follows.
Initial case. For t = 0, we trivially have for all measurable functions f : X × I → R uniformly
bounded by |f | ≤ Mf a law of large numbers result
e [∣μN (f) - μo(f )∣]
E
X X μN,α(x) f(x,α)- X μα(x) f (x, α)dα
I x∈X	x∈X
E
N x i ∕i_1 i f(Xo,α)dα-E
i∈VN ∖ J(ɪ, N]
f(X0i,α)dα
J(iN1, N]	.
∣∣∣
≤ (El(N i%(/宗，NJM,α)dα - E
1
2
f(X0i,α)dα
J (iN1, N]	.

N XE
i∈VN
f(X0i,α)dα -E
(iN1, N]
f(X0i,α)dα
J (iN1, N]	_
1
2
2Mf
≤ --j=
N	√N
by definition of μN, independence of {X0}i∈yw and X0 〜 μo = μα for all i ∈ VN, α ∈ I, where
the second equality follows from Fubini’s theorem.
16
Published as a conference paper at ICLR 2022
Induction step. Assume that the induction assumption holds at t. Then by definition of μN, for
all bounded function f : X ×I → R with ∖f | ≤ Mf,
E [陷ι(f) - μt+ι(f)∣] ≤ E [∣μN+1 (f) - μNP%,Wn(f) I ]
+ E [ μN Ptπ,μN ,Wn (f ) - μN Pt,μN ,W (f ) | ]
+ E [ μNP腺,w (f )-μNp∏μN,w (f) ∣ ]
+ E [ μNp晨 n,w (f )-μNΡ0w (f) ∣ ]
+ E [ I μNp∏μt,w(f) - μt+ι(f) ∣ ].
First term. We have by definition of μN
En μN+ι(f )-μNpt% ,Wn (f) ∣ ]
E
X X μt+,α (x) f (X,α)dα
JZ x∈X
	
/ X μN,α(x) X ∏N,α(u I x) X P 卜 | x,u, ∕wn(α,β)μN,β dβ) f(x0, α)dα
E
N X ([i_1 i f(χi+ι,α)dα- E
i∈VN ∖J(ɪ, N]
I . f(Xi+ι,α)dα Xt
J(iN1, N ]	.
))1
≤H (±%(L导,JX"a" - E
I . f(Xi+ι,α)dα Xt
J(iN1, N ]	.

1
2
=IN XE
∖	i∈VN
2Mf
≤ -Tf
_ √>
1
2
/	f (Xi+1,α)dα - E
(iN1, N ]
[.f(Xi+ι,α)dα Xt
1( iN1, N ]	.
where the last equality follows from conditional independence of {Xi+1 }i∈VN given Xt ≡
{Xi}i∈VN and the law of total expectation.
Second term. We have
E [ ∣ μNPt* ,Wn (f)iNPt* ,W (f) ∣ ]
E
/ X μN,α(x) X πNo(U I x) X P (x0 | x,u,/ WN(α,β)μN,β dβ) f(x0,α)dα
-∕ix μN,α(x) X ∏N,α(u | x) X P卜 | x,u∕w (α,β)μN,β dβ) f (x0, α) dα
≤ |X|Mf Lp E
/ LWN (α,β)μN,β dβ -/W(α,β)μN,β dβ
dα
I
I
≤ |X|2Mf Lp sup E
x∈X
I I
Wn(α,β)μN,β(x) - W(α,β)μ},β(x) d1β dα → 0
I
by Assumption 1 and μNe(x) trivially being bounded by 1. If the convergence in Assumption 1 is
at rate O(1/√N), then this convergence is also at rate O(1/√N).
17
Published as a conference paper at ICLR 2022
Third term. We have
E [ I μN 喷N ,W (f) —μNp晨 N,W (f) I ]
E
[X 〃"(x) X πN,ɑ(u ∖ x) X P 卜 ∖ x,u∕w(α,β)μNβ dβ) f (x0, Q) dα
—/ X 〃N,α(x) X 呜(U ∖ x) X P 卜0 ∖ x,uJιw (α, β')μt,β dβ)于 (x',a)da
≤ ∖X∖∖U∖Mf E
/bN,α(u ∖ x) — ∏α(u ∖ x) I dα
d	「Na]
∖X∖∖U∖Mf E	E L… 1 ∏t N (u ∖ x)—碟(U ∖ x)
j∈VN∖{i} JI2-1,N]
dα
+ ∖X∖∖U∖Mf E
/	∖∏t(u ∖ x) — πjα(u ∖ x)∖ da
J(i-1, N ]	.
≤ ∖X∖∖U∖Mf ∙ N + ∖X∖∖U∖Mf ∙ 2∖N∖ + ∖X∖∖U∖Mf ∙ N
by assumption of Lipschitz continuous π up to a finite number of discontinuities D∏ as well as the
deviating agent i’s error term, for which the integrands are bounded by 2.
Fourth term. We have
E [ I *PmN ,w (f) —μ"t"w (f) I ]
E
X X 4N,α (X) X 碟(U ∖ x) X P (x0 ∖ x,uJ[w (a,β )μN,β dβ) f(x0,a)da
-/ X μv,α(X) X 琮(U ∖ x) X P (x0 ∖ x, u, ( W (a, β)μf dβ) f (x0, a) da
≤ Mf ∖X∖ E sup [ P(x[x,u, [ W(a,β)μ>β dβ )
|_x,u Ji	∖	JI	J
—P (x0 ∖ x, u,/ W(a,β)〃β dβ) i da
≤ Mf ∖X∖Lp
XE
“M Ui J工
x0∈X
W (a, β )μN,β (x0)dβ — / W (a,β)μf (x0)dβ da
I
≤ Mf ∖X∖2Lp ∙ *
in the case of rate O(1∕√N), or uniformly to zero otherwise, from Lipschitz P by defining the
functions f z α(x,β) = W (a,β) ∙1x=x∕ for any (x0,a) ∈ XxZ and using the induction assumption
on fX,，a to obtain
E
W [w(a,β)μNβ(x'')dβ — ^
W(α, β)μf (x0) dβ da
I
I
I
IE
/ W(a,β)μN,β(x0)dβ — / W(a,β)μ:(x0)dβ da
=Z E [ I μN (fX 0,α) — μt(fX 0,α) I ] da ≤ C
JI	√N
for some C0(1) > 0 uniformly over all f bounded by 1 if the convergence in Assumption 1 is at
rate O(1/√N), or uniformly to zero otherwise.
18
Published as a conference paper at ICLR 2022
Fifth term. We have
E [∣μNPtπμt,w(f) - μtp鼠t,w(f)∣]
E
E
X X μN,α(X) X πtα(u | X) X P 卜1 x,u∕W(α, β)μβ dβ) f (XIa) dα
- [χ 〃:(x) X 嗤(U | x) X P 卜 | x,u, ∕w (a,β)μβ dβ) f(X0, α) dα
X X μN,α(χ)fO(X, a)da- L X μt(x)fO(X, a)da
=E[∣μN(f0) - μt(f0)∣] ≤ C√Mf) .
N
in the case of rate O(1∕√N), or uniformly to zero otherwise, again by induction assumption applied
to the function
f 0(X,α) = X ∏α(u | x) X P (xo | x,u, f W(a,β)μβdβ) f(X0,a)
u∈U	x0∈X	I
bounded by Mf . This completes the proof by induction.
□
A.2.3 Proof of Lemma A.1
Proof. First, we will show that (24) implies (25).
Proof of (24) =⇒ (25). We consider a uniformly Lipschitz, uniformly bounded family of mea-
surable functions H from X × B1(X) to R. Let Mh be the uniform bound of functions in H and Lh
be the uniform Lipschitz constant. Then, for arbitrary h ∈ H we have
∣E [h(xi, GN(μN))i - E [h(X文 G卡(μt))i∣
=∣E [h(xi, GN(μN))i - E [h(xi, GN (μt))i∣
+ ∣E [h(Xi, GN (μt))i - E [h(Xi, G亮(μt))i∣
+ ∣E [h(Xi, G卡(μt))i - E [h(XtNr, G亮(μt))i∣
which we will analyze in the following.
First term. We have
∣E [h(xi, GN(μN))i - E [h(xi,GN(μt))i∣
≤ E hE h∣h(xi,GN(μNY)- "GN(μt))∣ ∣ Xiii
≤ Lh E h∣∣Gji (μN) - GN (μt)∣∣i
=Lh X E ∣iWn(",8n炉(X)dβ - [wn(得,β)μβ(X)dβ ≤ CN
by Theorem 2 applied to the functions fN,i,χ(X0, β) = WN(N, β) ∙ 1χ=χ, uniformly bounded by 1.
Second term. Similarly, we have
∣E [h(Xi, GN (μt))i - E [h(Xi, G号(μt))i∣
i	i
≤ LhIlGN (μt) - GN (μt)∣∣ι
19
Published as a conference paper at ICLR 2022
≤ Lh
x∈X
(N⑼-W (N，e» μβ (χ)dβ
≤ Lh X∖[ WNN (NF,e)— N [	W (α,β )dα)μβ (x)dβ
χ∈x I'1 '	，(iN1，卡]	)
+ LhE	W(α,β)dα — W(-i-,β) ) μ?(x) dβ
X∈X	得，亮〕	N t
where the latter term can be bounded as
Lh
x L (n /一
χ∈X I J(，
x∈X
W(α, β)dα — W(盘,β) ) μ?(x) dβ
⅛]	N)
≤ Lh X [ NL 一 (W(α,β) — W(H,β))便(x)da dβ
x∈X ∖ JI J(iN1，N] '	)
LrIyIM 1 LW	LW LhIXI
≤ LhIXIN ∙ N ∙ ɪ = N
by Assumption 2.
Alternatively, if we assumed the weaker block-wise Lipschitz condition on W in (20), we can obtain
the same result for almost all i ∈ VN, i.e. for any p0 > 0 there exists N0 ∈ N such that for any
N > N0, there exists a set WN0 , |WN0 | ≥ b(1 — p0)Nc such that for all i ∈ WN0 the above is true:
Since by (20) there exist only a finite number Q of intervals and therefore jumps, there can be only
Q many i for which the above fails, while for all other i we again have
UN /宇,京
β) — W(dNαe,β)) μβ(x)dαdβ
≤ E	[3一(W(α,β) — W(dNαe,β)) μβ(x)da dβ
j∈{i,…,Q}	(ɪ,N] '	)
VN ɪ LW — LWLh∣X∣
-∙ N ∙ ɪ = -N-
by (20), as (iN1, N] X Ij ⊆I X Ij for some k ∈ {1,..., Q}.
For the former term we observe that
Lh X∖[ WNN (NF,e)- N ʌɪ	W (α,β)dα)μβ (x)dβ
χ∈X I'1 \	，( ^N，得]	)
≤ Lh
x∈X
(WN(α,β) - W(α,β)) μβ(x)dβ da
and by defining for any x ∈ X the terms IiN (x) via
IiN(x) :=N
J (Nr, N ]
and noticing that we have
1N
N X IiN (χ)
i=1
I ∖I
/ (WN(α,β) — W(α,β)) μβ(x)dβ da
(WN(α,β) — W(a,β)) μβ(x)dβ dα → 0
by Assumption 1, we can conclude that for any ε1,p1 > 0 there exists N0 ∈ N such that for any
N > N0, there exists a set WN1 , IWN1 I ≥ b(1 — p1)Nc such that for all i ∈ WN1 we have
IiN (x) < ε1,
JN /午,
I
20
Published as a conference paper at ICLR 2022
since by the above We can choose N0 ∈ N such that for any N > N0 We have N PN=I IN (x) <
ε1p1, and from IN(x) ≥ 0 it would otherwise follow that N PN=I IN(x) ≥ N ∙ dpιN] ε1 ≥ ε1p1
Which Would be a direct contradiction. Therefore, for all i ∈ WN1 , We have uniformly
LFN / iN1, N
(WN(α, β)
— W(α,β)) μtβ(x) dβ
dα = Lh	IiN (x) → 0 .
x∈X
Third term. By (24), for any ε2, p2 > 0 there exists a set WN2 , |WN2 | ≥ b(1 - p2)Nc such that
for all i ∈ WN2 we have
∣E [h(Xi, GN (〃川 — E [h(X^, GN (μt))]∣ <ε2
independent of ∏ ∈ Π.
The intersection of WN0 , WN1, WN2 has at least N — dp0Ne — dp1Ne — dp2Ne agents fulfilling (25),
which completes the proof of (24) =⇒ (25) for almost all agents by choosing ε1, ε2 sufficiently
small andp0,p1,p2 < P such that N - dpoN]-dpiNe — dp2N] ≥ b(1 - P)NC, which is equivalent
to 1 — dp0Ne — dpNNl — dpNNl ≥ b(1-N)NC and is true for sufficiently large N, since in the limit,
1	dP0Ne	dPιNe	dP2Ne	v 1 TJC n 1 no and b(1-P)NC	› 1 n MNTM
1-----N--------N----------N---→ 1 - PO - Pl - P2 and -----N------→ 1 - P as N T ∞.
Proof of (24). All that remains is to show (24), which will automatically imply (25) at all times
t ∈ T by the prequel. We will show (24) by induction.
_	一__	.	一 一， 一:、	一，八三、一 一 一..	一	..一一
Initial case. At t = 0, L(Xt) = μo = L(XtN) by definition. Thus, trivially
∣E[g(X0)] - E [g(X∕ )i∣ = 0 <ε.
Induction step. For any uniformly bounded family of functions G from X to R with bound Mg,
we will show that for any ε, P > 0, there exists N0 ∈ N such that for all N > N0 we have
∣E [g(Xt+ι)] - E [g(Xt+1)]∣ <ε
uniformly over ∏ ∈ Π,i ∈ WN for some WN ⊆ VN with |WN | ≥ b(1 - p)NC. Observe that
∣E [g(Xt+ι)] - E [g(Xt+ ι)i∣ = ∣E [lN,t(Xt, GN (〃N))i - E 卜N,t(Xti, G卡(〃川 ∣
where we defined the uniformly bounded, uniformly Lipschitz functions
lN,t(x, V) ≡ E ∏t(u | x) E P(x0 | x, u, V)g(x0)
u∈U	x0 ∈X
with Lipschitz constant |X |MgLP and uniform bound Mg. By the induction assumption and (24)
=⇒ (25) from the prequel, there exists N0 ∈ N such that for all N > N0 we have
∣E [lN,t(Xt, GN (μN ))i - E 卜 N,t(Xti, G N (μt))]∣ <ε
uniformly over ∏ ∈ Π,i ∈ WN for some WN ⊆ VN with |WN| ≥ [(1 - p)NC, which completes
the proof by induction.	□
A.2.4 Proof of Corollary A.1
Proof. Define the uniformly bounded, uniformly Lipschitz functions
r∏(x,ν) ≡ Er(x,u, V)∏t(u | S)
u∈U
with Lipschitz constant |U |Lr and uniform bound Mr such that by Lemma A.1 and Fubini’s theo-
rem, there exists N0 ∈ N such that for all N > N0 we have
JN (π1,...,πiτ,∏,πi+1,...,∏)-Jμ (π)∣
21
Published as a conference paper at ICLR 2022
T-1
≤ ∑∣E [r∏t (Xi, G 亮(μt))] — E [r∏t (XtN, G N (μt))]∣ <ε.
t=0
uniformly over π ∈ Π,i ∈ WN for some WN ⊆ VN with |Wn | ≥ [(1 - P)NC by choosing the
maximum over all N0 at each finite time step from Lemma A.1.
In case of the infinite horizon discounted objective, we instead first cut off at a time T >
such that trivially
log
ε(I-^Y
4Mr
log Y
T-1
EYtIE [r∏t (Xi, G代(μt))] - E [r∏t (XtN, G卡(μt))] ∣
t=0
∞
+ YT E Yt-T ∣E [r∏t (Xi, G贵(μt))] - E [r∏t (XtN, G备(μt))] ∣
t=T
T-1
<∑Yt ∣E [r∏t (Xi, G卡(μt))] - E [r∏t (XtN, G齐(μt))] ∣ + %
t=0	2
and then handle the remaining term analogously to the finite horizon case.	□
A.2.5 Proof of Theorem 3
Proof. By Corollary A.1, for any ε > 0 there exists N0 ∈ N such that for all N > N0 we have
max (JN(π1,..., πi-1, π, πi+1,..., πN) — JN(π1,..., πN))
≤ max (JN (π1,..., πi-1, π, πi+1,..., πN) — Jμ (π))
π∈Π	N
+ max(Jμ (∏) - Jμ (∏NN))
Π∈Π ∖ N	N	)
+ J(∏ n )-JN(∏1,...,∏N))
< ε + 0 + ε =ε
uniformly over i ∈	WN	for some WN	⊆	VN	With |Wn|	≥	[(1	-P)NC,	since πNN ∈
arg max∏ JC (π) by definition of a GMFE. Reordering completes the proof.	□
N
A.2.6 Proof of Corollary A.2
Proof. The proof follows immediately from Theorem 2 and Lemma A.1 by considering the trivial
policy π that always chooses the only action available together with its generated mean field μ =
Ψ(π).	□
A.2.7 Proof of Proposition 2
Proof. The set Π is a complete metric space, since existence of limits follows from completeness
of R, pointwise limits of measurable functions are measurable, and policies will remain normalized.
Banach,s fixed point theorem applied to Φ ◦ Ψ gives US the desired result.	□
A.2.8 Proof of Theorem 4
Proof. Formally, we approximate mean fields by Ψ(π) = PM=I 1a∈τiμαi for any fixed policy
ensemble π, and similarly policies Φ(μ) = PM=I 1θi∈fi∏ɑi where πai is the softmax policy of αi
for fixed μ, i.e.	%
exp (Qμ(ηx,U)
πtα(u | x)
P PXn (QM(t,x,U))
乙U∈U exp I η J
(30)
22
Published as a conference paper at ICLR 2022
By the Bellman equation (13), Qμ(t,x,u) is LiPschitz in μ for all (t,x,u) ∈ T XX XU Un-
der Assumption 2. Since the Lipschitz constants are shared over all α, by Cui & Koeppl (2021),
Lemma B.7.5, (30) is therefore Lipschitz with Lipschitz constant proportional to 1∕η, which imme-
ʌ
diately implies that Φ is also Lipschitz with Lipschitz constant cι∕η. By its recursive definition as
compositions of Lipschitz functions, Ψ is Lipschitz as well with some constant c2. Therefore, the
composition of both functions Ψ ◦ Φ is Lipschitz with constants c1c2∕η, which will be less than 1
for sufficiently large η. By Proposition 2, the equivalence classes algorithm Ψ ◦ Φ converges to a
fixed point.
□
A.2.9 Proof of Theorem 5
Proof. First, note that under the equivalence classes method, the distance between any α and its
representant α% uniformly shrinks to zero as M → ∞, i.e. maxi=ι,…,m suPα∈i- ∣α 一 α/ → 0.
We begin by showing that a solution of the M equivalence classes method (π, μ) ∈ Π X M,
ʌ ʌ
π ∈ Φ(μ), μ = Ψ(π) following (31), (32) fulfills approximate optimality, i.e. for any ε > 0 there
exists M0 s.t. for all M > M0
sup max Ja(∏) 一 Ja (∏α)) <ε,
α∈I π∈Π
where we introduced the true, exact mean field ensemble μ = Ψ(π) following (12) generated by
the block-wise solution policy PM=I 1θi∈fi∏ɑi of the M equivalence classes method, as well as the
true mean field system under μ and any policy ∏ ∈ Π
Xa〜μo,	Ua〜∏t(∙ιχα),	χα+ι〜P(∙ιχα,uα,Gα), NC埒∈工XT
with B1(X)-valued Gα := RI W(α, β)μβ dβ and Ja(∏) ≡ E IPT-II r(X1呼,G讨,while sys-
tem (9) is to be understood as the system under the approximate mean field ensemble μ.
To see this, we will analyze
SUpmax (Jμ(π) - Ja(π°)) ≤ . max SUp max (Jμ(π) - Jμ(π))
a∈I π∈π	i=1,…,m a∈I π∈π
+. maχf sup max (Jμ(π) - Jai(n))
i=1,...,Ma∈Ii π∈π
+ -max, sup max (Jμi(∏) 一 Jμi(∏αi))
i=1,…,m a∈Ii π∈π
+ i=maxM sup Jai (∏ai) - Jμ(∏ai))
,…，a a∈Ii
+ ,_maxJvf sup (Jμ(∏a) - Ja (∏a)).
i=1,…,M a∈Ii
First term. For any π ∈ Π, define the uniformly bounded, uniformly Lipschitz functions
rπ(x, ν) ≡	r(x, u, ν)πt(u | s)
u∈U
with Lipschitz constant |U |Lr and uniform bound Mr such that for the first term, we have
Ja (∏) - Jμ(∏)) ≤ Jμ (∏) - Jμ(∏)∣
T-1
≤ E ∣e 卜 ∏(χa, G a)] - E%(χa, Ga]∣
t=0
T-1	T-1
≤ E ∣E%(χa, Ga) -r∏(χa, Ga]∣ + E ∣E%(χa, Ga] - E%(χa, Ga]∣
t=0	t=0
T-1
T-1
旺 ∖u∖L JI
t=0
W(α,β)(μβ - μβ)dβ)
T-1
+ E∣e %(χa, Ga] - e [r∏ (Xa Ga]∣.
t=0
23
Published as a conference paper at ICLR 2022
For the former term, note that
WW(α(α, β)(μlt- μltβ)dβ) = XjH W(α,β)(μβ - μα)dβ)
≤ i=maxM：UpiXlk"α-"αik
and We will show by induction over t = 0,1,...,T that suPα∈ι. kμα - M?i k → 0 over all a
uniformly over all equivalence classes I At t = 0, we have trivially μo = μo. Assume that
suPα∈ι. kμα - μαi k → O. Then for t +1,we have
sup ∣∣μα+ι- μ0+ Ji
α∈Ii
suP
α∈Ii
≤ suP
a∈Ii
E μα(χ) E ∏α⅜	∣ x)p(∙	∣ x, U Ga)- E μαi(χ)	E ∏αi (U	ι χ)p (∙	ι χ, U Gai)
x∈X	u∈U	x∈X	u∈U
E μa(χ) E ∏a⅜	∣ x)p(∙	∣ x, u G a)	- E μai ⑺	E ∏ai(u	ι χ)p (∙	ι χ, U G ai)
x∈X	u∈U	x∈X	u∈U
+ ∑>ai(χ) £ ∏ai(u i χ)p(∙ i χ,u G ai) - £〃ai(x) £ ∏ai(u ι χ)p (∙ ι χ,u G ai)
∣x∈X	u∈U	x∈X	u∈U
+ ∑>ai (X)E ∏ai (u i χ)p(∙ i χ,u, G ai) - £〃ai (X)E ∏ai (U ι χ)p(∙ ι χ,u, Gai)
∣x∈X	u∈U	x∈X	u∈U
≤ sup	£ μa(X) Ena(U |	x)P(∙	|χ,u, Ga) - E μa(x) Enai(U	| x)P(∙	|χ,u, Gai)
a∈Ii	x∈X	u∈U	x∈X	u∈U
+ IXl2 kμai - μai k + IXl2∣u∣Lp kμai - μai k → 0
as M → ∞, since the first term is uniformly Lipschitz inα by (12) as a recursive composition, finite
multiplication and addition of Lipschitz functions, whereas the other terms tend to zero by induction
hypothesis. Since the Lipschitz constants do not depend on Ii , the convergence is uniform.
To bound the latter term, we first note that r∏ (∙, Ga) is always bounded by Mr regardless of t, α, n,
i.e. it again suffices to show that for any family of functions G from X to R uniformly bounded by
Mr , we have
sup∣e [g(χa)] - E [g(ɪa)]l → o.
g∈G
The proof is by induction. At t = 0, we trivially have L(Xa) = μo = L(Xa). Assuming that the
induction hypothesis holds at t, then at t + 1 we have
sup ∣e [g(χa+ι)] - E[g(xa+ι)]∣ =sup∣E[it(xa, Ga))] - e [it(χa, Ga)]∣ → 0
g∈G	g∈G
by the induction hypothesis, where we defined the uniformly bounded functions
it(X, ν) ≡	πt(U l X)	P(X0 l X, U, ν)g(X0)
u∈U	x0∈X
with uniform bound Mr. Therefore, ∣ Jμ(π) - Jμ(π)∣ → 0 uniformly over all α, π.
Second term. For the second term, we analogously have
jμ(∏)- Jai (∏)) ≤ ∣jμ(∏)- Jai (∏)∣
T-1
≤ lUlLr
t=0
J (W(α,β) - W(ai,β))μβ dβ)
T-1
+	lE[rπ(Xta,Gtai]-E[rπ(Xtai,Gtai]l
t=0
24
Published as a conference paper at ICLR 2022
where the former term uniformly tends to zero as M → ∞ over all α by Lipschitz W from As-
∙-v
sumption 2 and increasingly fine partition intervals Ii , while for the latter term we again show that
for any family of functions G from X to R uniformly bounded by Mr, we have
SUp ∣e [g(xa)] - E[g(χαi )]∣→ o.
g∈G
The proof is by induction. At t = 0, We trivially have L(Xa) = μo = L(Xai). Assuming that the
induction hypothesis holds at t, then at t + 1 we have
SUp ∣e [g(χα+ι)] - E [g(χα+ι)] I = sup ∣e [it(χa, Gai))] - e [it(χai, Gai )]∣→ 0
g∈G	g∈G
by the induction hypothesis, Where We defined the uniformly bounded functions
lt(x, ν) ≡	πt(u | x)	P (x0 | x, u, ν)g(x0)
u∈U	x0∈X
with uniform bound Mr. Therefore, ∣ Jμ(π) - Jμ (π)∣ → 0 uniformly over all α, π.
ʌ
Third term. By definition, we have optimality of π ∈ Φ(μ) under the approximate mean field μ
at each representative a%. Therefore, the term max∏∈∏ (J^ (∏) - Jμi (∏ɑi)) is upper bounded by
0, as there is no policy π that improves over πai .
Fourth and fifth term. The results follow from the first and second term by inserting πa for π.
Variations on the setting. The infinite horizon discounted case is handled as in the proof of Corol-
lary A.1, i.e. repeating the above up to some chosen time horizon T and trivially bounding all terms
with t ≥ T . The block-wise Lipschitz graphon case (20) is handled by choosing the equivalence
classes I ⊆ Ij such that they are part of at most one block Tj of the graphon.
Proof of Theorem 5. Now fix any ε, p > 0. As a result of the prequel, we have that there exists
M0 s.t. for all M > M0
SUpmax ∣Jμ(πα) - Ja(∏)∣ < ε∙
a∈I π∈Π	3
Pick any such M > M0. By Corollary A.1 (for the first and third term, since π is constant with at
most M discontinuities) and the prequel (for the second term), there exists N0 ∈ N such that for all
N > N0 we have
max (JN(π1,..., πi-1, π, πi+1,..., πN) — JN(π1,..., πN))
π∈Π i	i
≤ max (JN (π1,..., πi-1, π, πi+1,..., πN) — Jμ (π))
+ max (J々n) - J/t (∏卡)
π∈Π ∖ N	N
<ε + ε + ε = ε
3	3	3
which holds uniformly over i ∈ WN for some WN ⊆ VN with |Wn| ≥ b(1 - P)NC, since πN ∈
arg max∏ Jμ (π) by definition of a GMFE. Reordering completes the proof.	□
N
A.3 Experimental details
In this section, we will give a full description of all the algorithms and hyperparameters we used
during our experiments. For reinforcement learning, we use PPO (Schulman et al., 2017).
For the approximate equivalence classes, we shall consider grids (αm ∈ [0, 1])m=1,...,M with asso-
ciated policies (∏αm ∈ Π)m=ι,…,m and mean fields (μam ∈ P(X)t)m=ι,…,m. For the grid, we
25
Published as a conference paper at ICLR 2022
Algorithm 1 Fixed point iteration
1:	Initialize μ0 as the mean field induced by the uniformly random policy q.
2:	for k = 0, 1, . . . do
3:	Compute πk ∈ Π either directly by PPO on (14), or by computing Qμ via Algorithm 2 and
using (38) to obtain a softmax policy.
4:	Compute μk+1 induced by πk using Algorithm 3, or for RL the neighborhood mean fields
Gtα directly using Algorithm 4.
5:	end for
Algorithm 2 Backwards induction
1:	Input: Grid (αm ∈ [0,1])m=ι,…,m, mean field μ ∈ M.
2:	for m = 1, . . . , M do
3:	Initialize terminal condition Qμ(T,x,u) ≡ 0 for all (x, u) ∈ X XU.
4:	for t = T - 1, . . . , 0 do
5:	for (x, u) ∈ X × U do
6:	Qam (t,x,u) - r(x,u, Gam) + pχo∈χ P(x0 | x,u, Gam)maxu，∈u Qam (t +1,x0,u0).
7:	end for
8:	end for
9:	end for
10:	Return (Qμm)m=ι,...,M
choose the points ɑm, = ImO with m = 0,..., 100. Here, an agent α shall use the policy πam with
the closest αm .
ʌ
To be precise, for the approximate mean field μ = Ψ(π) We define μɑ ≡ μam for the αm closest to
α, i.e. formally, we thus have
M
ψ(∏) = E 1a∈im μam	(31)
m=1
for any fixed policy ensemble π, with μ defined through the recursive equation
μαm ≡	μo,	μamι(χ0)	≡ X	μam(X)	X πtam(U	|	X)P(X0	| x,u, Gam),	m =	1,...,M (32)
x∈X	u∈U
where under the assumption of equivalence classes Im ≡ [am, bm] of size (bm 一 am), we obtain
neighborhood mean fields via
M
Ga =£(bm 一 am)W(α,αm)^am .	(33)
m=1
Note that in our algorithms, we shall assume equisized partitions and use (bm-am) = M. Similarly,
the policy ensemble is approximated by
M
Φ(μ) = ∑1a∈Ii ∏ai	(34)
i=1
where ∏ai is the optimal policy of ai for any fixed μ, i.e. the optimal policy and mean field of each
α is approximated by the optimal solution and mean field of the closest αi , which is an increasingly
good approximation for sufficiently fine grids under the standing Lipschitz assumptions. In the case
of block-wise Lipschitz continuous graphons via (20), a similar justification holds as long as each
equivalence class remains constrained to one of the blocks of the graphon, see Theorem 5.
In Algorithm 1, the learning scheme is described on a high level. In our experiments, we either use
approximate equivalence classes via Algorithms 2 and 3, or reinforcement learning in the form of
PPO together with sequential Monte Carlo in Algorithm 4, though in principle one can mix arbitrary
methods.
26
Published as a conference paper at ICLR 2022
Algorithm 3 Forward simulation
1:	Input: Grid (αm ∈ [0,1])m=ι,…,m,policy π ∈ Π.
2:	Initialize starting condition μfm ≡ μo for all m = 1,...,M.
3:	for t = 0, . . . , T - 2 do
4:	for m = 1, . . . , M do
5:	μα+mι 一 Px∈x μαm(χ) pm∈u ∏αm(u ι χ)p(∙ ι B M PM=I W(。碓,。排片).
6:	end for
7:	end for
8:	Return (μαm)m=ι,...,M
Algorithm 4 Sequential Monte Carlo
1:	Input: Number of trajectories K = 5, number of particles L = 200, policy π ∈ Π.
2:	for k = 1, . . . , K do
3:	Initialize particles αm 〜Unif([0,1]), xm,k 〜μo for all m = 1,...,L.
4:	for t = 1, . . . , T - 1 do
5:	for m = 1, . . . , L do
6:	Sample action U 〜∏αm(∙ | Xm,k).
7:	Sample new particle state Xm,k 〜P(∙ | Xmk,u, L PL=I W(αm,αn)δχn,k).
8:	end for
9:	end for
10:	end for
11:	return neighborhood mean fields Gtα ≈ KK ∑K=1 L Em=I W(α,αm)δχm,k.
We ran each trial of our experiments on a single conventional CPU core, with typical wall-clock
times reaching up to at most a few days. We estimate the required compute to approximately 6500
core hours. We did not use any GPUs or TPUs. More specifically, the training of our approximate
equivalence class approach took on average approximately 24 hours for 250 iterations in SIS and
50 iterations in Investment. As a result, Figure 5 for the selection of appropriate temperatures took
around 2500 core hours. The PPO experiments took approximately 3 days for each configuration,
resulting in approximately 200 core hours for Figure 7. Finally, for the N -agent evaluations in
Figure 4, each run up to 100 agents takes up to 4 core hours. Adding on top of that around 250 core
hours for the rest of the experiments results in a total of approximately 4000 core hours.
For PPO, we used the RLlib implementation by Liang et al. (2018) (version 1.2.0, Apache-2.0
license). To allow for time-dependent policies, we append the current time to the network inputs.
Further, discrete-valued observations are one-hot encoded. Any other parameter configurations are
given in Algorithms 1, 2, 3 and 4, as well as in Table 2.
As for the specific configurations used in the PPO experiments, we give the hyperparameters in
Table 1 and used with a feedforward neural network policy consisting of two hidden layers with 256
nodes and tanh activations, outputting a softmax policy over all actions.
A.3.1 Problem definitions
For each possible problem setting, we list the applied temperature setting in Table 2. In the follow-
ing, let G ∈ P(X ).
SIS-Graphon. In the SIS-Graphon game as described in the main text, we have X = {S, I},
U = {U,D}, μo(I) = 0.5, r(x, u, G) = —2 ∙ 1{i}(x) — 0.5 ∙ 1{d}(U) and T = {0,..., 49}.
Similar parameters produce similar results, and we set the transition probabilities as
P(S | I, ∙, ∙)=0.2,
P(I | S, U, G) =0.8 ∙ G(I),
P(I | S,D, ∙)=0.
Investment-Graphon. Similarly, in the Investment-Graphon game we have X = {0, 1, . . . 9},
U = {I,O}, μo(0) = 1, r(x, U, G) = .P,〉鼻口吟—2 ∙ 1{i}(u) and T = {0,..., 49}. We set
27
Published as a conference paper at ICLR 2022
Table 1: PPO Hyperparameters		
Symbol	Function	Value
lr	Learning rate	0.00005
γ	Discount rate	1
λ	GAE lambda	0.99
cKL	KL coefficient	0.2
β	KL target	0.006
cent	Entropy coefficient	0.01
	Clip parameter	0.2
B	Training batch size	4000
Bm	Mini batch size	128
ISGD	SGD iterations per training batch	30
Table 2: Temperature configurations
Experiment	η for approximate equivalence classes
SIS-Graphon, Wunif	0.101
SIS-Graphon, Wrank	0.3
SIS-Graphon, Wer	0.101
Investment-Graphon, Wunif	0
Investment-Graphon, Wrank	0
Investment-Graphon, Wer	0.05
the transition probabilities for x = 0, 1, . . . , 8 as
P(X + 1 | x, I, ∙)
P(X | x, I, ∙)
P(X | x, O, ∙)
while for x = 9 the next state is always x = 9.
9 — x
10
1 + X
10
1,
A.3.2 Exploitability and temperature choice
In the following, we will explain our choice of temperatures in Table 2 by approximately evaluating
the average exploitability of GMFE candidates (π, μ) - as it is intractable to approximately evaluate
the maximum exploitability over all α ∈ I - defined by
∆J (∏, μ)
S sup Jμ(π*) — Jμ(πα)dα.
JI π*∈Π
(35)
More specifically, when using approximate equivalence classes, we compute the exploitability of
some policy π by computing the optimal policy π* obtained via Algorithm 2, under the fixed mean
field μ generated by π via Algorithm 3, inserting ∏*,α into (35) and then approximating by
J Jα (π)dα ≈ Mm X X μo(X) X πo(U | X)Qam (O,x,U).	(36)
I	m=1,...,M x∈X	u∈U
Here, We defined for any policy ∏ ∈ Π and α ∈ I the policy evaluation functions Qμ,π as usual via
Qμ,π(t, x,u) = r(x, U, Ga) + E P(x0 I X, U Ga) E ∏o(u0 I x)Qμ,π(t + 1, x0, u0)	(37)
x0∈X	u0∈U
with terminal condition Qμ,π (T, x, U) ≡ 0, which can be computed as in Algorithm 2, see also
Puterman (2014) for a revieW.
To achieve convergence of fixed point iterations to approximate equilibria, for previous μn ∈ M
we compute the action value function Qμn via Algorithm 2 using approximate equivalence classes
28
Published as a conference paper at ICLR 2022
η
Figure 5: Final approximate exploitability mean and its minimum / maximum (shaded region) over
the last 10 iterations for various temperatures η. We can see convergence for sufficiently high
temperatures and choose the lowest temperature such that we still have convergence with low ex-
ploitability. Furthermore, compared to the uniformly random policy, our approximate exploitability
is significantly lower, indicating a good approximate GMFE. For the Investment-Graphon problem,
the approximate exploitability of the uniform policy is not shown, as it is above 30. (a): SIS-
Graphon; (b): Investment-Graphon.
η
and then define the next policy πn+1 = Φ(μn) for every α ∈ I via the Softmax function
∏n+1,ɑi (U । x)
exp
n
Qai (t,χ,u)
η
Pu∈u exP (Qμ⅜u2)
(38)
for the closest αi with some temperature η > 0 chosen minimally for convergence.
For choosing the temperature, we evaluate the approximate final exploitability at various temper-
atures. The results can be seen in Figure 5, where we plot the average, minimum and maximum
exploitability over the last 10 iterations of the fixed point learning scheme. The reasoning behind
choosing our temperatures as in Table 2 is that we can see no fluctuations (indicating convergence of
our learning scheme) together with a low approximate exploitability at the indicated temperatures.
A.3.3 Additional experiments
In Figure 6, we plot investment behavior at quality x = 0 as well as expected quality for each α
of the approximate equivalence class solution, and similarly in Figure 7 for the PPO solution with
sequential Monte Carlo. Here, for each α we averaged quality over all particles within a distance of
0.05 to α. We can see that PPO achieves qualitatively and quantitatively similar behavior, deviating
slightly due to the approximate optimality of the PPO algorithm. To be precise, when evaluating
exploitability via either solution, we find that the learned policy exploitability remains around ε ≈ 2,
compared to ε > 30 for the uniform random policy.
In Figure 8 the equilibrium behavior is shown for the Investment-Graphon problem without softmax
policy regularization (except for the ER graphon case), as we find that the problem already converges
to a very good equilibrium with low approximate exploitability, see Figure 5. In this problem, we
find that the resulting (deterministic without regularization) policy will let agents invest up to a
certain quality, after which any further investment is avoided. The agents with higher connectivity
will invest up to a lower quality, as they are in competition with more products.
In Figure 9 and 10, we have performed ablations over the number of equivalence classes for the SIS-
Graphon problem. As can be observed, the solution obtained by approximate equivalence classes
remains stable regardless of the particular number of equivalence classes, showing the stability of
discretization approach and supporting Theorem 5.
Finally, in Figure 11 we exemplarily show training results of applying state-of-the-art multi-agent
reinforcement learning methods such as multi-agent PPO (MAPPO, Yu et al. (2021)) on the finite-
agent system with observed, randomized-per-episode graphon indices and W -random graphs. Here,
we use the same hyperparameters as shown in Table 1. As can be seen, due to the non-stationarity
of the other agents, a naive application of MARL techniques fails to converge at all.
29
Published as a conference paper at ICLR 2022
Figure 6: The M = 100 approximate equivalence classes solution of Investment-Graphon. We
plot the probability of investing at state x = 0 (top) together with the evolution of average quality
(bottom). (a): Uniform attachment graphon; (b): Ranked attachment graphon; (c): ER graphon.
1.0
05
0.0
0
40
20
t
Figure 7: The probability of investing at state x = 0 (top) together with the evolution of average
quality (bottom) for PPO. The solution is similar to Figure 6, though slightly different due to the
approximations stemming from PPO and sequential Monte Carlo. (a): Uniform attachment graphon;
(b): Ranked attachment graphon; (c): ER graphon.
t
1.0
0.5
0.0
t
1.0
0.5
0.0
⑻	Wumf	X
0	20	40
1.0
0.5
0.0
(b)	Wrank	X Q
「1-
0	20	40
1.0
0.5
0.0
(C)	Wer_X Q
「I-
0	20	40
9
4
-1
t
t
t
Figure 8: Achieved equilibrium via M = 100 approximate equivalence classes in Investment-
Graphon. Top: Maximum quality X UP to which agents will invest (∏α(I | x) > 0.5), shown for
each α ∈ I , t ∈ T. Bottom: Expected quality versus time of each agent α ∈ I. It can be observed
that agents with less connections (higher α) will invest more. (a): Uniform attachment graphon; (b):
Ranked attachment graphon; (c): ER graphon.
30
Published as a conference paper at ICLR 2022
Figure 9: Achieved equilibrium via approximate equivalence classes in SIS-Graphon for the uniform
attachment graphon, plotted for each representative αi ∈ I . Top: Probability of taking precautions
when healthy. Bottom: Probability of being infected. (a): M = 10; (b): M = 30; (c): M = 50.
ttt
Figure 10: Achieved equilibrium via approximate equivalence classes in SIS-Graphon for the ranked
attachment graphon, plotted for each representative αi ∈ I . Top: Probability of taking precautions
when healthy. Bottom: Probability of being infected. (a): M = 10; (b): M = 20; (c): M = 30.
O 50 IOO 150	200	O 10	20	30	40
Iterations	t
Figure 11: Learning curve and results for an exemplary straightforward application of multi-agent
PPO (MAPPO, Yu et al. (2021)). Left: Sum of expected agent objectives over learning iterations;
Right: Final policy probability of taking precautions when healthy.
31