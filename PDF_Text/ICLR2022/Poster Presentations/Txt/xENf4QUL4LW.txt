Published as a conference paper at ICLR 2022
Sample Selection with Uncertainty of Losses
for Learning with Noisy Labels
Xiaobo Xia1 Tongliang Liu11 Bo Han2
Mingming Gong3 Jun Yu4 Gang Niu5 Masashi Sugiyama5,6
1TML Lab, The University of Sydney 2Hong Kong Baptist University
3The University of Melbourne 4University of Science and Technology of China
5RIKENAIP 6The University of Tokyo
Ab stract
In learning with noisy labels, the sample selection approach is very popular, which
regards small-loss data as correctly labeled during training. However, losses are
generated on-the-fly based on the model being trained with noisy labels, and
thus large-loss data are likely but not certain to be incorrect. There are actually
two possibilities of a large-loss data point: (a) it is mislabeled, and then its loss
decreases slower than other data, since deep neural networks “learn patterns first”;
(b) it belongs to an underrepresented group of data and has not been selected yet. In
this paper, we incorporate the uncertainty of losses by adopting interval estimation
instead of point estimation of losses, where lower bounds of the confidence intervals
of losses derived from distribution-free concentration inequalities, but not losses
themselves, are used for sample selection. In this way, we also give large-loss but
less selected data a try; then, we can better distinguish between the cases (a) and
(b) by seeing if the losses effectively decrease with the uncertainty after the try. As
a result, we can better explore underrepresented data that are correctly labeled but
seem to be mislabeled at first glance. Experiments demonstrate that the proposed
method is superior to baselines and robust to a broad range of label noise types.
1	Introduction
Learning with noisy labels is one of the most challenging problems in weakly-supervised learning,
since noisy labels are ubiquitous in the real world (Mirzasoleiman et al., 2020; Yu et al., 2019; Nishi
et al., 2021; Arazo et al., 2019; Yang et al., 2021a; Bai & Liu, 2021). For instance, both crowdsourcing
and web crawling yield large numbers of noisy labels everyday (Han et al., 2018). Noisy labels can
severely impair the performance of deep neural networks with strong memorization capacities (Zhang
et al., 2017; Zhang & Sabuncu, 2018; Pleiss et al., 2020; Lukasik et al., 2020; Chen et al., 2022).
To reduce the influence of noisy labels, a lot of approaches have been recently proposed (Natarajan
et al., 2013; Liu & Tao, 2016; Ma et al., 2018; Yang et al., 2021b; Zheng et al., 2020; Xia et al., 2019;
2020; Tanaka et al., 2018; Malach & Shalev-Shwartz, 2017; Li et al., 2020b; Menon et al., 2018;
Thekumparampil et al., 2018; Xu et al., 2019; Kim et al., 2019; Jiang et al., 2020; Harutyunyan et al.,
2020). They can be generally divided into two main categories. The first one is to estimate the noise
transition matrix (Patrini et al., 2017; Shu et al., 2020; Hendrycks et al., 2018; Yang et al., 2021c; Wu
et al., 2022), which denotes the probabilities that clean labels flip into noisy labels. However, the
noise transition matrix is hard to be estimated accurately, especially when the number of classes is
large (Yu et al., 2019). The second approach is sample selection, which is our focus in this paper. This
approach is based on selecting possibly clean examples from a mini-batch for training (Han et al.,
2018; Wang et al., 2018; Yao et al., 2020a; Wang et al., 2019; Yu et al., 2019; Lee et al., 2019; Wang
et al., 2019; Yao et al., 2022). Intuitively, if we can exploit less noisy data for network parameter
updates, the network will be more robust.
A major question in sample selection is what criteria can be used to select possibly clean examples.
At the present stage, the selection based on the small-loss criteria is the most common method, and
has been verified to be effective in many circumstances (Han et al., 2018; Jiang et al., 2018; Yu et al.,
,Corresponding author
1
Published as a conference paper at ICLR 2022
Epoch
Figure 1: Illustrations of uncertainty of losses. Experiments are conducted on the imbalanced noisy MNIST
dataset. Left: uncertainty of small-loss examples. At the beginning of training (Epochs 1 and 2), due to the
instability of the current prediction, the network gives a larger loss to the clean example and does not select it
for updates. If we consider the mean of training losses at different epochs, the clean example can be equipped
with a smaller loss and then selected for updates. Right: uncertainty of large-loss examples. Since the deep
network learns easy examples at the beginning of training, it gives a large loss to clean imbalanced data with
non-dominant labels, which causes such data unable to be selected and severely influence generalization.
2019; Wei et al., 2020; Yao et al., 2020a). Specifically, since deep networks learn patterns first (Arpit
et al., 2017), they would first memorize training data of clean labels and then those of noisy labels
with the assumption that clean labels are of the majority in a noisy class. Small-loss examples can
thus be regarded as clean examples with high probability. Therefore, in each iteration, prior methods
(Han et al., 2018; Wei et al., 2020) select the small-loss examples based on the predictions of the
current network for robust training.
However, such a selection procedure is debatable, since it arguably does not consider uncertainty
in selection. The uncertainty comes from two aspects. First, this procedure has uncertainty about
small-loss examples. Specifically, the procedure uses limited time intervals and only exploits the
losses provided by the current predictions. For this reason, the estimation for the noisy class posterior
is unstable (Yao et al., 2020b), which causes the network predictions to be equally unstable. It
thus takes huge risks to only use losses provided by the current predictions (Figure 1, left). Once
wrong selection is made, the inferiority of accumulated errors will arise (Yu et al., 2019). Second,
this procedure has uncertainty about large-loss examples. To be specific, deep networks learn easy
examples at the beginning of training, but ignore some clean examples with large losses. Nevertheless,
such examples are always critical for generalization. For instance, when learning with imbalanced
data, distinguishing the examples with non-dominant labels are more pivotal during training (Menon
et al., 2020; Wei et al., 2021). Deep networks often give large losses to such examples (Figure 1,
right). Therefore, when learning under the realistic scenes, e.g., learning with noisy imbalanced data,
prior sample selection methods cannot address such an issue well.
To relieve the above issues, we study the uncertainty of losses in the sample selection procedure to
combat noisy labels. To reduce the uncertainty of small-loss examples, we extend time intervals and
utilize the mean of training losses at different training iterations. In consideration of the bad influence
of mislabeled data on training losses, we build two robust mean estimators from the perspectives of
soft truncation and hard truncation w.r.t. the truncation level, respectively. Soft truncation makes
the mean estimation more robust by holistically changing the behavior of losses. Hard truncation
makes the mean estimation more robust by locally removing outliers from losses. To reduce the
uncertainty of large-loss examples, we encourage networks to pick the sample that has not been
selected in a conservative way. Furthermore, to address the two issues simultaneously, we derive
concentration inequalities (Boucheron et al., 2013) for robust mean estimation and further employ
statistical confidence bounds (Auer, 2002) to consider the number of times an example was selected
during training.
The study of uncertainty of losses in learning with noisy labels can be justified as follows. In statistical
learning, it is known that uncertainty is related to the quality of data (Vapnik, 2013). Philosophically,
we need variety decrease for selected data and variety search for unselected data, which share a
common objective, i.e., reduce the uncertainty of data to improve generalization (Moore, 1990). This
is our original intention, since noisy labels could bring more uncertainty because of the low quality
of noisy data. Nevertheless, due to the harm of noisy labels for generalization, we need to strike a
good balance between variety decrease and search. Technically, our method is specially designed for
2
Published as a conference paper at ICLR 2022
handling noisy labels, which robustly uses network predictions and conservatively seeks less selected
examples meanwhile to reduce the uncertainty of losses and then generalize well.
Before delving into details, we clearly emphasize our contributions in two folds. First, we reveal prior
sample selection criteria in learning with noisy labels have some potential weaknesses and discuss
them in detail. The new selection criteria are then proposed with detailed theoretical analyses. Second,
we experimentally validate the proposed method on both synthetic noisy balanced/imbalanced datasets
and real-world noisy datasets, on which it achieves superior robustness compared with the state-
of-the-art methods in learning with noisy labels. The rest of the paper is organized as follows. In
Section 2, we propose our robust learning paradigm step by step. Experimental results are discussed
in Section 3. The conclusion is given in Section 4.
2	Method
In this section, we first introduce the problem setting and some background (Section 2.1). Then we
discuss how to exploit training losses at different iterations (Section 2.2). Finally, we introduce the
proposed method, which exploits training losses at different iterations more robustly and encourages
networks to pick the sample that is less selected but could be correctly labeled (Section 2.3).
2.1	Preliminaries
Let X and Y be the input and output spaces. Consider a k-class classification problem, i.e., Y = [k],
where [k] = {1, . . . , k}. In learning with noisy labels, the training data are all sampled from a
corrupted distribution on XXY. We are given a sample With noisy labels, i.e., S = {(x, y)}, where
y is the noisy label. The aim is to learn a robust classifier that could assign clean labels to test data by
only exploiting a training sample with noisy labels.
Let f : X → Rk be the classifier with learnable parameters w. At the i-th iteration during training,
the parameters of the classifier f can be denoted as Wi. Let' : Rk ×Y → Rbea surrogate loss
function for k-class classification. We exploit the softmax cross entropy loss in this paper. Given an
arbitrary training example (x, y), at the i-th iteration, we can obtain a loss 'i, i.e., ' = '(f (wi； x),y).
Hence, until the t-th iteration, we can obtain a training loss set Lt about the example (x, y), i.e.,
Lt = {'ι,...,'t}.
In this paper, we assume that the training losses in Lt conform to a Markov process, which is to
represent a changing system under the assumption that future states only depend on the current
state (the Markov property). More specifically, at the i-th iteration, if we exploit an optimization
algorithm for parameter updates (e.g., the stochastic gradient descent algorithm (Bottou, 2012)) and
omit other dependencies (e.g., S), we will have P(wi|wi-1, . . . , w0) = P(wi|wi-1), which means
that the future state of the classifier f only depends on the current state. Furthermore, given a training
example and the parameters of the classifier f, we can determine the loss of the training example as
discussed. Therefore, the training losses in Lt will also conform to a Markov process.
2.2	Extended Time Intervals
As limited time interval cannot address the instability issue of the estimation for the noisy class
posterior well (Pleiss et al., 2020), we extend time intervals and exploit the training losses at different
training iterations for sample selection. One straightforward idea is to use the mean of training losses
at different training iterations. Hence, the selection criterion could be
1t
μ =	'i.	(I)
i=1
It is intuitive and reasonable to use such a selection criterion for sample selection, since the operation
of averaging can mitigate the risks caused by the unstable estimation for the noisy class posterior,
following better generalization. Nevertheless, such a method could arguably achieve suboptimal
classification performance for learning with noisy labels. The main reason is that, due to the great
harm of mislabeled data, part of training losses are with too large uncertainty and could be seen as
outliers. Therefore, it could be biased to use the mean of training losses consisting of such outliers
(Diakonikolas et al., 2020), which further influences sample selection. More evaluations for our
claims are provided in Section 3.
3
Published as a conference paper at ICLR 2022
2.3 Robust Mean Estimation and Conservative Search
We extend time intervals and meanwhile exploit the training losses at different training iterations
more robustly. Specifically, we build two robust mean estimators from the perspectives of soft
truncation and hard truncation (Catoni, 2012). Note that for specific tasks, it is feasible to decide
the types of robust mean estimation with statistical tests based on some assumptions (Chakrabarty
& Samorodnitsky, 2012). We leave the analysis as future work. Two distribution-free robust mean
estimators are introduced as follows.
Soft truncation. We extend a classical M-estimator from (Catoni, 2012) and exploit the widest
possible choice of the influence function. More specifically, give a random variable X , let us consider
a non-decreasing influence function ψ : R → R such that
ψ(X) = log(1 + X + X2/2), X ≥ 0.
(2)
The choice of ψ is inspired by the Taylor expansion of the exponential function, which can make the
estimation results more robust by reducing the side effect of extremum holistically. The illustration
for this influence function is provided in Appendix A.1. For our task, given the observations on
training losses, i.e., Lt = {'ι,..., 't}, We estimate the mean robustly as follows:
μs
1t
t∑ψ('i).
(3)
i=1
We term the above robust mean estimator (3) the soft estimator.
Hard truncation. We propose a new robust mean estimator based on hard truncation. Specifically,
given the observations on training losses Lt, we first exploit the K-nearest neighbor (KNN) algorithm
(Liao & Vemuri, 2002) to remove some underlying outliers in Lt . The number of outliers is denoted
by to(to < t), which can be adaptively determined as discussed in (Zhao et al., 2019). Note that we
can also employ other algorithms, e.g., principal component analysis (Shyu et al., 2003) and the local
outlier factor (Breunig et al., 2000), to identify underlying outliers in Lt . The main reason we employ
KNN is because of its relatively low computation costs (Zhao et al., 2019).
The truncated loss observations on training losses are denoted by Lt-to. We then utilize Lt-to for
the mean estimation. As the potential outliers are removed with high probability, the robustness of
the estimation results will be enhanced. We denote such an estimated mean as μh. We have
μh=占 Jt-J
(4)
The corresponding estimator (4) is termed the hard estimator.
We derive concentration inequalities for the soft and hard estimators respectively. The search strategy
for less selected examples and overall selection criterion are then provided. Note that we do not need
to explicitly quantify the mean of training losses. We only need to sort the training examples based
on the proposed selection criterion and then use the selected examples for robust training.
Theorem 1 Let Zn = {zι, ∙∙∙ , zn} be an observation set with mean μz and variance σ2. By
exploiting the non-decreasing influence function ψ(z) = log(1 + z + z2/2). For any > 0, we have
1 n
~y2ψ(zi) 一μz
n
i=1
with probability at least 1 一 2.
Proof can be found in Appendix A.1.
≤ σ2(n + σ⅛T))
n 一 σ2
(5)
Theorem 2 Let Zn = {z1, . . . , zn} be a (not necessarily time homogeneous) Markov chain with
mean μz, taking values in a Polish state space Λι X ... X An, and with a minimal mixing time 〜由.
The truncated set with hard truncation is denoted by Zno, with no < n. If |zi| is upper bounded by Z.
For any 1 > 0 and 2 > 0, we have
n - no
z
Σ
⅛∈Zn∖Zno
-μz
1	(5 2	■ ^^2	2Zno Γ	■ ^^2n、	…
≤ F (々"log 7 + 丁 ilog J	⑹
1
with probability at least 1 - 1 - 2.
4
Published as a conference paper at ICLR 2022
Proof can be found in Appendix A.2. For our task, let the training loss be upper-bounded by L. The
value of L can be determined easily by training networks on noisy datasets and observing the loss
distribution (Arazo et al., 2019).
Conservative search and selection criteria. In this paper, we will use the concentration inequalities
(5) and (6) to present conservative search and the overall sample selection criterion. Specifically,
we exploit their lower bounds and consider the selected number of examples during training. The
selection of the examples that are less selected is encouraged.
Denote the number of times one example was selected by nt(nt ≤ t). Let e = 4.For the
circumstance with soft truncation, the selection criterion is
σ2(t + σ IogRt))
'? = μs- n」
(7)
Let e1 = e2 =，, for the situation with hard truncation, by rewriting (6), the selection criterion is
'h = μh -
2√2τminL(t + √2to )
(t - to)√t
∕log(4t)
n—
(8)
Note that we directly replace t with nt. If an example is rarely selected during training, nt will be far
less than n, which causes the lower bounds to change drastically. Hence, we do not use the mean of
all training losses, but use the mean of training losses in fixed-length time intervals. More details
about this can be checked in Section 3.
For the selection criteria (7) and (8), we can see that they consist of two terms and have one term
with a minus sign. The first term in Eq. (7) (or Eq. (8)) is to reduce the uncertainty of small-loss
examples, where we use robust mean estimation on training losses. The second term, i.e., the
statistical confidence bound, is to encourage the network to choose the less selected examples (with
a small nt). The two terms are constraining and balanced with σ2 or τmin. To avoid introducing
strong assumptions on the underlying distribution of losses (Chakrabarty & Samorodnitsky, 2012),
we tune σ and τmin with a noisy validation set. For the mislabeled data, although the model has high
uncertainties on them (i.e., a small nt) and tends to pick them, the overfitting to the mislabeled data
is harmful. Also, the mislabeled data and clean data are rather hard to distinguish in some cases as
discussed. Thus, we should search underlying clean data in a conservative way. In this paper, we
initialize σ and τmin with small values. This way can reduce the adverse effects of mislabeled data
and meanwhile select the clean examples with large losses, which helps generalize. More evaluations
will be presented in Section 3.
The overall procedure of the proposed
method, which combats noisy labels
by concerning uncertainty (CNLCU),
is provided in Algorithm 1. CNLCU
works in a mini-batch manner since
all deep learning training methods are
based on stochastic gradient descent.
Following (Han et al., 2018), we ex-
ploit two networks with parameters θ1
and θ2 respectively to teach each other.
Specifically, when a mini-batch S is
formed (Step 3), we let two networks
select a small proportion of examples
in this mini-batch with Eq. (7) or (8)
(Step 4 and Step 5). The number of
instances is controlled by the function
R(T ), and two networks only select
R(T) percentage of examples out of
the mini-batch. The value of R(T)
should be larger at the beginning of
Algorithm 1 CNLCU Algorithm.
1:	Input θι and θ2, learning rate η, fixed T, epoch Tk and
Tmax , iteration tmax ;
for T = 1, 2, . . . , Tmax do
2:	Shuffle training dataset S ;
for t = 1, . . . , tmax do
3:	Fetch mini-batch S from S ;
4:	Obtain Si = argmins,so∣≥R(τ)∣s∣ '*(θι, S0);
// calculated with Eq. (7) or Eq. (8)
5:	Obtain S2 = argminso：|S ∣≥R(T)|S| '?(θ2, S0);
// calculated with Eq. (7) or Eq. (8)
6:	Update θi = θi - η^'(θι, S2);
7:	Update θ2 = θ2 -甲证*八;
end
8:	Update R(T) = 1 - min { Tτ,τ};
end
9:	Output θi and θ2.
training, and be smaller when the number of epochs goes large, which can make better use of memo-
rization effects of deep networks (Han et al., 2018) for sample selection. Then, the selected instances
are fed into its peer network for parameter updates (Step 6 and Step 7).
5
Published as a conference paper at ICLR 2022
3	Experiments
In this section, we evaluate the robustness of our proposed method to noisy labels with comprehensive
experiments on the synthetic balanced noisy datasets (Section 3.1), synthetic imbalanced noisy
datasets (Section 3.2), and real-world noisy dataset (Section 3.3).
3.1	Experiments on Synthetic Balanced Noisy Datasets
Datasets. We verify the effectiveness of our method on the manually corrupted version of the follow-
ing datasets: MNIST (LeCun et al.), F-MNIST (Xiao et al., 2017), CIFAR-10 (Krizhevsky, 2009),
and CIFAR-100 (Krizhevsky, 2009), because these datasets are popularly used for the evaluation of
learning with noisy labels in the literature (Han et al., 2018; Yu et al., 2019; Wu et al., 2021; Lee
et al., 2019). The four datasets are class-balanced. The important statistics of the used synthetic
datasets are summarized in Appendix B.1.
Generating noisy labels. We consider broad types of label noise: (1). Symmetric noise (abbreviated
as Sym.) (Wu et al., 2020; Ma et al., 2018). (2) Asymmetric noise (abbreviated as Asym.) (Ma et al.,
2020; Xia et al., 2021; Wei et al., 2020). (3) Pairflip noise (abbreviated as Pair.) (Han et al., 2018; Yu
et al., 2019; Zheng et al., 2020). (4). Tridiagonal noise (abbreviated as Trid.) (Zhang et al., 2021). (5).
Instance noise (abbreviated as Ins.) (Cheng et al., 2020; Xia et al., 2020). The noise rate is set to 20%
and 40% to ensure clean labels are diagonally dominant (Ma et al., 2020). More details about above
noise are provided in Appendix B.1. We leave out 10% of noisy training examples as a validation set.
Baselines. We compare the proposed method (Algorithm 1) with following methods which focus on
sample selection, and implement all methods with default parameters by PyTorch, and conduct all
the experiments on NVIDIA Titan Xp GPUs. (1). S2E (Yao et al., 2020a), which properly controls
the sample selection process so that deep networks can better benefit from the memorization effects.
(2). MentorNet (Jiang et al., 2018), which learns a curriculum to filter out noisy data. We use
self-paced MentorNet in this paper. (3). Co-teaching (Han et al., 2018), which trains two networks
simultaneously and cross-updates parameters of peer networks. (4). SIGUA (Han et al., 2020),
which exploits stochastic integrated gradient underweighted ascent to handle noisy labels. We use
self-teaching SIGUA in this paper. (5). JoCor (Wei et al., 2020), which reduces the diversity of
networks to improve robustness. To avoid too dense tables, we provide results of other sample
selection methods and other types of baselines such as adding regularization. All results are presented
in Appendix B.2. Here, we term our methods with soft truncation and hard truncation as CNLCU-S
and CNLCU-H respectively.
Network structure and optimizer. For MNIST, F-MNIST, and CIFAR-10, we use a 9-layer CNN
structure from (Han et al., 2018). Due to the limited space, the experimental details on CIFAR-100
are provided in Appendix B.3. All network structures we used here are standard test beds for weakly-
supervised learning. For all experiments, the Adam optimizer (Kingma & Ba, 2014) (momentum=0.9)
is used with an initial learning rate of 0.001, and the batch size is set to 128 and we run 200 epochs.
We linearly decay learning rate to zero from 80 to 200 epochs as did in (Han et al., 2018). We take
two networks with the same architecture but different initializations as two classifiers as did in (Han
et al., 2018; Yu et al., 2019; Wei et al., 2020), since even with the same network and optimization
method, different initializations can lead to different local optimal (Han et al., 2018). The details of
network structures can be checked in Appendix C.
For the hyper-parameters σ2 and τmin, we determine them in the range {10-1, 10-2, 10-3, 10-4}
with a noisy validation set. Note that the use of hyperparameters aims to reduce the dependency
on strong assumptions and thus make our methods perform well in practice. We provide more
details about this in Appendix D. Here, we assume the noise level τ is known and set R(T) =
1 - min{ Tτ, T} with Tk=10. If T is not known in advanced, it can be inferred using validation sets
(Liu & Tao, 2016; Yu et al., 2018). As for performance measurement, we use test accuracy, i.e., test
accuracy = (# of correct prediction) / (# of testing). All experiments are repeated five times. We
report the mean and standard deviation of experimental results.
Experimental results. The experimental results about test accuracy are provided in Tables 1, 2, and
3. Specifically, for MNIST, as can be seen, our proposed methods, i.e., CNLCU-S and CNLCU-H,
produce the best results in the vast majority of cases. In some cases such as asymmetric noise, the
baseline S2E outperforms ours, which benefits the accurate estimation for the number of selected
6
Published as a conference paper at ICLR 2022
Noise type	Sym.		Asym.		Pair.		Trid.		Ins.	
Method/Noise	20%	40%	20%	40%	20%	40%	20%	40%	20%	40%
S2E	98.46	95.62	99.05	98.45	98.56	94.22	99.02	97.23	97.93	94.02
	±0.06	±0.91	±0.02	±0.26	±0.32	±0.79	±0.09	±1.26	±1.26	±2.39
MentorNet	95.04	92.08	96.32	90.86	93.19	90.93	96.42	93.28	94.65	90.11
	±0.03	±0.42	±0.17	±0.97	±0.17	±1.54	±0.09	±1.37	±0.73	±1.26
Co-teaching	97.53	95.62	98.25	95.08	96.05	94.16	98.05	96.18	97.96	95.02
	±0.12	±0.30	±0.08	±0.43	±0.96	±1.37	±0.06	±0.85	±0.09	±0.39
SIGUA	92.31	91.88	93.96	62.59	93.77	86.22	94.92	83.46	92.90	86.34
	±1.10	±0.92	±0.82	±0.15	±1.40	±1.75	±0.83	±2.98	±1.82	±3.51
JoCor	98.42	98.04	98.05	94.55	98.01	96.85	98.45	96.98	98.62	96.07
	±0.14	±0.07	±0.37	±1.08	±0.19	±0.43	±0.17	±0.25	±0.06	±0.31
CNLCU-S	98.82	98.31	98.93	97.67	98.86	97.71	99.09	98.02	98.77	97.78
	±0.03	±0.05	±0.06	±0.22	±0.06	±0.64	±0.04	±0.17	±0.08	±0.25
CNLCU-H	一 98170	98.24 -	-99.01	98:01 ^	-98.44^	-97.37'	-98.89 一	-97.92	98.74 一	-97.42
	±0.06	±0.06	±0.04	±0.03	±0.19	±0.32	±0.15	±0.05	±0.16	±0.39
Table 1: Test accuracy (%) on MNIST over the last ten epochs. The best two results are in bold.
Noise type	Sym.		Asym.		Pair.		Trid.		Ins.	
Method/Noise	20%	40%	20%	40%	20%	40%	20%	40%	20%	40%
S2E	89.99	75.32	89.00	81.03	88.66	67.09	89.53	77.29	88.65	79.35
	±2.07	±5.84	±0.95	±1.93	±1.32	±4.03	±2.63	±3.97	±2.12	±3.04
MentorNet	90.37	86.53	89.69	67.21	87.92	83.70	88.74	85.63	87.52	83.27
	±0.17	±0.65	±0.19	±2.94	±1.08	±0.49	±0.33	±0.59	±0.15	±1.42
Co-teaching	91.48	88.80	91.03	68.07	90.77	86.91	91.24	89.18	90.60	87.90
	±0.10	±0.29	±0.14	±4.58	±0.23	±0.71	±0.11	±0.36	±0.12	±0.45
SIGUA	87.64	87.23	76.97	45.96	69.59	68.93	79.97	76.14	76.92	74.89
	±1.29	±0.72	±2.59	±3.40	±5.75	±2.80	±3.23	±4.24	±5.09	±4.84
JoCor	91.97	89.96	90.95	79.79	91.52	87.40	92.01	89.42	91.43	87.59
	±0.13	±0.19	±0.21	±2.39	±0.24	±0.58	±0.17	±0.33	±0.71	±0.94
CNLCU-S	92.37	91.45	92.57	83.14	92.04	88.20	92.24	90.08	91.69	89.02
	±0.15	±0.28	±0.15	±1.77	±0.26	±0.44	±0.17	±0.34	±0.10	±1.02
CNLCU-H	-92。	91.60 -	-92.60	82:69 -	^9I.70^	87.70 '	一92.33一	-90.22^	一91.50一	-88.79'
	±0.21	±0.19	±0.18	±0.43	±0.18	±0.69	±0.26	±0.71	±0.21	±1.22
Table 2: Test accuracy on F-MNIST over the last ten epochs. The best two results are in bold.
small-loss examples. For F-MNIST, the training data becomes complicated. S2E cannot achieve the
accurate estimation in such situation and thus has no great performance like it got on MNIST. Our
methods achieve varying degrees of lead over baselines. For CIFAR-10, our methods once again
outperforms all the baseline methods. Although some baseline, e.g., Co-teaching, can work well
in some cases, experimental results show that it cannot handle various noise types. In contrast, the
proposed methods achieve superior robustness against broad noise types. The results mean that our
methods can be better applied to actual scenarios, where the noise is diversiform.
Ablation study. We first conduct the ablation study to analyze the sensitivity of the length of time
intervals. In order to avoid too dense figures, we exploit MNIST and F-MNIST with the mentioned
noise settings as representative examples. For CNLCU-S, the length of time intervals is chosen in
the range from 3 to 8. For CNLCU-H, the length of time intervals is chosen in the range from 10 to
15. Note that the reason for their different lengths is that their different mechanisms. Specifically,
CNLCU-S holistically changes the behavior of losses, but does not remove any loss from the loss set.
We thus do not need too long length of time intervals. As a comparison, CNLCU-H needs to remove
some outliers from the loss set as discussed. The length should be longer to guarantee the number of
examples available for robust mean estimation. The experimental results are provided in Appendix
B.4, which show the proposed CNLCU-S and CNLCU-H are robust to the choices of the length of
time intervals. Such robustness to hyperparameters means our methods can be applied in practice and
does not need too much effect to tune the hyperparameters.
Furthermore, since our methods concern uncertainty from two aspects, i.e., the uncertainty from both
small-loss and large-loss examples, we conduct experiments to analyze each part of our methods.
Also, as mentioned, we compare robust mean estimation with non-robust mean estimation when
learning with noisy labels. More details are provided in Appendix B.4.
7
Published as a conference paper at ICLR 2022
Noise type	Sym.		Asym.		Pair.		Trid.		Ins.	
Method/Noise	20%	40%	20%	40%	20%	40%	20%	40%	20%	40%
S2E	80.78	69.72	84.03	75.04	81.72	61.50	81.44	64.39	79.89	62.42
	±0.88	±3.94	±1.01	±1.24	±0.93	±4.63	±0.59	±2.82	±0.26	±3.11
MentorNet	80.92	74.67	80.37	71.69	77.98	69.39	78.02	71.56	77.02	68.17
	±0.48	±1.17	±0.26	±1.06	±0.31	±1.73	±0.29	±0.93	±0.71	±2.52
Co-teaching	82.35	77.96	83.87	73.43	80.94	72.81	81.17	74.37	79.92	73.29
	±0.16	±0.39	±0.24	±0.62	±0.46	±0.92	±0.60	±0.64	±0.57	±1.62
SIGUA	78.19	77.67	75.14	52.76	74.41	61.91	75.75	74.05	74.34	67.98
	±0.22	±0.41	±0.36	±0.68	±0.81	±5.27	±0.53	±0.41	±0.39	±1.34
JoCor	80.96	76.65	81.39	69.92	80.33	71.62	79.03	74.33	78.21	71.46
	±0.25	±0.43	±0.74	±1.63	±0.20	±1.05	±0.13	±1.09	±0.34	±1.27
CNLCU-S	83.03	78.25	85.06	75.34	83.16	73.19	82.77	74.37	82.03	73.67
	±0.21	±0.70	±0.17	±0.32	±0.25	±1.25	±0.32	±1.37	±0.37	±1.09
CNLCU-H	^ 83:03^	一78.33 一	-84.95	75:29 -	-83.39	73:40	一 82.52 一	-74.79	81.93 一	-73.58 '
	±0.47	±0.50	±0.27	±0.80	±0.68	±1.53	±0.71	±1.13	±0.25	±1.39
Table 3: Test accuracy (%) on CIFAR-10 over the last ten epochs. The best two results are in bold.
3.2	Experiments on Synthetic Imbalanced Noisy Datasets
Experimental setup. We exploit MNIST and F-MNIST. For these two datasets, we reduce the number
of training examples along with the labels from “0” to “4” to 1% of previous numbers. We term such
synthetic imbalanced noisy datasets as IM-MNIST and IM-F-MNIST respectively. This setting aims
to simulate the extremely imbalanced circumstance, which is common in practice. Moreover, we
exploit asymmetric noise, since these types of noise can produce more imbalanced case (Patrini et al.,
2017; Ma et al., 2020). Other settings such as the network structure and optimizer are the same as
those in experiments on synthetic balanced noisy datasets.
As for performance measurements, we use test accuracy. In addition, we exploit the selected ratio of
training examples with the imbalanced classes, i.e., selected ratio=(# of selected imbalanced labels /
# of all selected labels). Intuitively, a higher selected ratio means the proposed method can make
better use of training examples with the imbalanced classes, following better generalization (Kang
et al., 2020).
Experimental results. The test accuracy achieved on IM-MNIST and IM-F-MNIST is presented in
Figure 2. Recall the experimental results in Tables 1 and 2, we can see that the imbalanced issue is
catastrophic to the sample selection approach when learning with noisy labels. For IM-MNIST, as
can be seen, all the baselines have serious overfitting in the early stages of training. The curves of test
accuracy drop dramatically. As a comparison, the proposed CNLCU-S and CNLCU-H can give a
try to large-loss but less selected data which are possible to be clean but equipped with imbalanced
labels. Therefore, our methods always outperform baselines clearly. In the case of Asym. 10%, our
methods achieve nearly 30% lead over baselines. For IM-F-MNIST, we can also see that our methods
perform well and always achieve about 5% lead over all the baselines. Note that due to the huge
challenge of this task, some baseline, e.g., S2E, has a large error bar. In addition, the baseline SIGUA
performs badly. It is because SIGUA exploits stochastic integrated gradient underweighted ascent on
large-loss examples, which makes the examples with imbalanced classes more difficult to be selected
than them in other sample selection methods. Due to the limited space, the selected ratio achieved on
IM-MNIST and IM-F-MNIST is presented in Appendix B.5, which explain well why our methods
perform better than multiple baselines.
3.3	Experiments on Real-world Noisy Datasets
Experimental setup. To verify the efficacy of our methods in the real-world scenario, we conduct
experiments on the noisy dataset Clothing1M (Xiao et al., 2015). Specifically, for experiments
on Clothing1M, we use the 1M images with noisy labels for training and 10k clean data for test
respectively. Note that we do not use the 50k clean training data in all the experiments. For
preprocessing, we resize the image to 256×256, crop the middle 224×224 as input, and perform
normalization. The experiments on Clothing1M are performed once due to the huge computational
cost. We leave 10% noisy training data as a validation set for model selection. Note that we do not
exploit the resampling trick during training (Li et al., 2020a). Here, Best denotes the test accuracy of
the epoch where the validation accuracy was optimal. Last denotes test accuracy of the last epoch. For
8
Published as a conference paper at ICLR 2022
S2E ——MentorNet ----- Co-teaching ——SIGUA ——JoCor ——CNLCU-S ——CNLCU-H
一Asym. 10% —	- Asym. 20% —	- Asym. 30% —	- Asym. 40% -
I IsINW，WI I IIBNI
Figure 2: Test accuracy vs. number of epochs on IM-MNIST and IM-F-MNIST. The error bar for
standard deviation in each figure has been shaded.
the experiments on Clothing1M, we use ResNet-18 and ResNet-50 which are pretrained on ImageNet.
We also use the Adam optimizer and set the batch size to 64. During the training stage, we run 15
epochs in total and set the learning rate 8 × 10-4, 5 × 10-4, and 5 × 10-5 for 5 epochs each.
Experimental results. The results on Clothing1M are provided in Table 4. Specifically, the
proposed methods get better results than state-of-the-art methods on Best. With ResNet-18, we
achieve improvements of +1.28% and +0.99%. With ResNet-50, we achieve improvements of
+2.51% and +2.16%. Likewise, the proposed methods outperform all the baselines on Last. We
achieve improvements of +1.01% and +0.54% with ResNet-18, and improvements of +2.47% and
+2.05% with ResNet-50. All these results verify the effectiveness of the proposed methods.
Methods	S2E	MentorNet	Co-teaching	SIGUA	JoCor	CNLCU-S	CNLCU-H
Best (R-18)	67.34	68.36	69.37	62.89	70.09	71.37	71.08
Last (R-18)	65.90	67.42	68:62	58.73	69.75	70.76	70.29
Best (R-50)	68.03	67.25	67.94	65.37	69.06	71.57	71.22
LaSi (R-50)一	66.25	66.59 一	67.05 —	60.77	68.41	70.88	70.46	-
Table 4: Test accuracy (%) on Clothing1M. “R-18” (resp. “R-50”) means that we exploit ResNet-18
(resp. ResNet-50). The best two results are in bold.
Combining with semi-supervised
learning. For combating noisy labels
in real-world noisy datasets, the state-
of-the-art methods, e.g., DivideMix
(Li et al., 2020a), always employ the
semi-supervised learning technology.
As our methods mainly focus on sam-
ple selection, to make the comparison
fair, we combine our methods with
Methods	Food-101	WebVision (Mini)	ClothingIM
DivideMix	86.73	7732	74.76
DivideMix-S	86.92	77:53	74.90
DivideMix-H	86.88	77.48	74.82	—
Table 5: The test accuracy (%) on three real-world datasets.
DivideMix-S (resp. DivideMix-H) means that our CNLCU-S
(resp. CNLCU-H) is combined with the advanced techniques
in DivideMix. The best two results are in bold.
semi-supervised learning. The sample selection procedure in DivideMix is replaced by our methods.
Other settings are kept the same. Following prior works (Ma et al., 2020), the experiments are
conducted on three real-world noisy datasets, i.e., Food-101 (Bossard et al., 2014), WebVision (Mini)
(Li et al., 2017), and Clothing1M (Xiao et al., 2015). The results are provided in Table 5. As can be
seen, the proposed methods are superior and can be used to improve the cutting edge performance.
4	Conclusion
In this paper, we focus on promoting the prior sample selection in learning with noisy labels, which
starts from concerning the uncertainty of losses during training. We robustly use the training losses at
different iterations to reduce the uncertainty of small-loss examples, and adopt confidence interval
estimation to reduce the uncertainty of large-loss examples. Experiments are conducted on benchmark
datasets, demonstrating the effectiveness of our method. We believe that this paper opens up new
possibilities in the topics of using sample selection to handle noisy labels, especially in improving
the robustness of models on imbalanced noisy datasets.
9
Published as a conference paper at ICLR 2022
5	Ethics Statement
This paper doesn’t raise any ethics concerns. This study doesn’t involve any human subjects, practices
to data set releases, potentially harmful insights, methodologies and applications, potential conflicts
of interest and sponsorship, discrimination/bias/fairness concerns, privacy and security issues, legal
compliance, and research integrity issues.
Acknowledgment
XBX is supported by Australian Research Council Projects DE-190101473. TLL is partially supported
by Australian Research Council Projects DE-190101473, IC-190100031, and DP-220102121. BH
was supported by the RGC Early Career Scheme No. 22200720 and NSFC Young Scientists Fund
No. 62006202. MMG is supported by Australian Research Council Projects DE-210101624. JY
is sponsored by CAAI-Huawei MindSpore Open Fund (CAAIXSJLJJ-2021-016B). GN and MS
are supported by JST AIP Acceleration Research Grant Number JPMJCR20U3, Japan. MS is also
supported by the Institute for AI and Beyond, UTokyo.
References
Eric Arazo, Diego Ortego, Paul Albert, Noel O’Connor, and Kevin McGuinness. Unsupervised label
noise modeling and loss correction. In ICML, pp. 312-321, 2019.
Devansh Arpit, StanislaW Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S
Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at
memorization in deep netWorks. In ICML, pp. 233-242, 2017.
Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine
Learning Research, 3(Nov):397-422, 2002.
Yingbin Bai and Tongliang Liu. Me-momentum: Extracting hard confident examples from noisily
labeled data. In ICCV, pp. 9312-9321, 2021.
Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101-mining discriminative compo-
nents With random forests. In ECCV, pp. 446-461, 2014.
Leon Bottou. Stochastic gradient descent tricks. In Neural networks: Tricks ofthe trade, pp. 421-436.
Springer, 2012.
Stephane Boucheron, Gabor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymptotic
theory of independence. Oxford university press, 2013.
Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and Jorg Sander. Lof: identifying density-
based local outliers. In SIGMOD, pp. 93-104, 2000.
Olivier Catoni. Challenging the empirical mean and empirical variance: a deviation study. In Annales
de l'IHP Probabilites et StatiStiqueS, volume 48, pp. 1148-1185, 2012.
Arijit Chakrabarty and Gennady Samorodnitsky. Understanding heavy tails in a bounded World or, is
a truncated heavy tail heavy or not? Stochastic models, 28(1):109-143, 2012.
HaW-Shiuan Chang, Erik Learned-Miller, and AndreW McCallum. Active bias: Training more
accurate neural netWorks by emphasizing high variance samples. In NeurIPS, pp. 1002-1012,
2017.
Ling-Hao Chen, He Li, and Wenhao Yang. Anomman: Detect anomaly on multi-vieW attributed
netWorks. arXiv preprint arXiv:2201.02822, 2022.
Peng Chen, Xinghu Jin, Xiang Li, and Lihu Xu. A generalized catoni’s m-estimator under finite α-th
moment assumption With α ∈ (1, 2). arXiv preprint arXiv:2010.05008, 2020.
Pengfei Chen, Ben Ben Liao, Guangyong Chen, and Shengyu Zhang. Understanding and utilizing
deep neural netWorks trained With noisy labels. In ICML, pp. 1062-1070, 2019.
10
Published as a conference paper at ICLR 2022
Jiacheng Cheng, Tongliang Liu, Kotagiri Ramamohanarao, and Dacheng Tao. Learning with bounded
instance-and label-dependent label noise. In ICML, 2020.
Ilias Diakonikolas and Daniel M Kane. Recent advances in algorithmic high-dimensional robust
statistics. arXiv preprint arXiv:1911.05911, 2019.
Ilias Diakonikolas, Daniel M Kane, and Ankit Pensia. Outlier robust mean estimation with subgaus-
sian rates via stability. arXiv preprint arXiv:2007.15618, 2020.
Evarist Gin6, RafaI LataIa, and Joel Zinn. Exponential and moment inequalities for U-Statistics. In
High Dimensional Probability II,pp.13-38. Springer, 2000.
Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi
Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In
NeurIPS, pp. 8527-8537, 2018.
Bo Han, Gang Niu, Xingrui Yu, Quanming Yao, Miao Xu, Ivor Tsang, and Masashi Sugiyama. Sigua:
Forgetting may make learning with noisy labels more robust. In ICML, pp. 4006-4016, 2020.
Hrayr Harutyunyan, Kyle Reing, Greg Ver Steeg, and Aram Galstyan. Improving generalization by
controlling label-noise information in neural network weights. In ICML, pp. 4071-4081, 2020.
Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel. Using trusted data to train
deep networks on labels corrupted by severe noise. In NeurIPS, 2018.
Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. MentorNet: Learning data-
driven curriculum for very deep neural networks on corrupted labels. In ICML, pp. 2309-2318,
2018.
Lu Jiang, Di Huang, Mason Liu, and Weilong Yang. Beyond synthetic noise: Deep learning on
controlled noisy labels. In ICML, pp. 4804-4815, 2020.
Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis
Kalantidis. Decoupling representation and classifier for long-tailed recognition. In ICLR, 2020.
Youngdong Kim, Junho Yim, Juseung Yun, and Junmo Kim. Nlnl: Negative learning for noisy labels.
In ICCV, pp. 101-110, 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
Yann LeCun, Corinna Cortes, and Christopher J.C. Burges. The MNIST database of handwritten
digits. http://yann.lecun.com/exdb/mnist/.
Kimin Lee, Sukmin Yun, Kibok Lee, Honglak Lee, Bo Li, and Jinwoo Shin. Robust inference via
generative classifiers for handling noisy labels. In ICML, pp. 3763-3772, 2019.
Junnan Li, Richard Socher, and Steven C.H. Hoi. Dividemix: Learning with noisy labels as semi-
supervised learning. In ICLR, 2020a.
Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is
provably robust to label noise for overparameterized neural networks. In AISTATS, 2020b.
Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, and Luc Van Gool. Webvision database: Visual
learning and understanding from web data. arXiv preprint arXiv:1708.02862, 2017.
Yihua Liao and V Rao Vemuri. Use of k-nearest neighbor classifier for intrusion detection. Computers
& security, 21(5):439-448, 2002.
Liu Liu, Tianyang Li, and Constantine Caramanis. High dimensional robust m-estimation: Arbitrary
corruption and heavy tails. arXiv preprint arXiv:1901.08237, 2019.
11
Published as a conference paper at ICLR 2022
Tongliang Liu and Dacheng Tao. Classification with noisy labels by importance reweighting. IEEE
Transactions on pattern analysis and machine intelligence, 38(3):447-461, 2016.
Eyal Lubetzky and Allan Sly. Cutoff for the ising model on the lattice. Inventiones mathematicae,
191(3):719-755, 2013.
Michal Lukasik, Srinadh Bhojanapalli, Aditya Menon, and Sanjiv Kumar. Does label smoothing
mitigate label noise? In ICML, pp. 6448-6458, 2020.
Yueming Lyu and Ivor W Tsang. Curriculum loss: Robust learning and generalization against label
corruption. In ICLR, 2020.
Xingjun Ma, Yisen Wang, Michael E Houle, Shuo Zhou, Sarah M Erfani, Shu-Tao Xia, Sudanthi
Wijewickrema, and James Bailey. Dimensionality-driven learning with noisy labels. In ICML, pp.
3361-3370, 2018.
Xingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Erfani, and James Bailey. Nor-
malized loss functions for deep learning with noisy labels. In ICML, pp. 6543-6553, 2020.
Eran Malach and Shai Shalev-Shwartz. Decoupling" when to update" from" how to update". In
NeurIPS, pp. 960-970, 2017.
Aditya Krishna Menon, Brendan Van Rooyen, and Nagarajan Natarajan. Learning from binary labels
with instance-dependent noise. Machine Learning, 107(8-10):1561-1595, 2018.
Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and
Sanjiv Kumar. Long-tail learning via logit adjustment. arXiv preprint arXiv:2007.07314, 2020.
Baharan Mirzasoleiman, Kaidi Cao, and Jure Leskovec. Coresets for robust training of neural
networks against noisy labels. In NeurIPS, 2020.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning.
MIT Press, 2018.
David S Moore. Uncertainty. On the shoulders of giants: New approaches to numeracy, pp. 95-137,
1990.
Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with
noisy labels. In NeurIPS, pp. 1196-1204, 2013.
Kento Nishi, Yi Ding, Alex Rich, and Tobias Hollerer. Augmentation strategies for learning with
noisy labels. arXiv preprint arXiv:2103.02130, 2021.
Laura Niss and Ambuj Tewari. What you see may not be what you get: Ucb bandit algorithms robust
to -contamination. In UAI, pp. 450-459, 2020.
Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making
deep neural networks robust to label noise: A loss correction approach. In CVPR, pp. 1944-1952,
2017.
Daniel Paulin et al. Concentration inequalities for markov chains by marton couplings and spectral
methods. Electronic Journal of Probability, 20, 2015.
Geoff Pleiss, Tianyi Zhang, Ethan R Elenberg, and Kilian Q Weinberger. Identifying mislabeled data
using the area under the margin ranking. In NeurIPS, 2020.
Gareth O Roberts, Jeffrey S Rosenthal, et al. General state space markov chains and mcmc algorithms.
Probability surveys, 1:20-71, 2004.
Jun Shu, Qian Zhao, Zengben Xu, and Deyu Meng. Meta transition adaptation for robust deep
learning with noisy labels. arXiv preprint arXiv:2006.05697, 2020.
Mei-Ling Shyu, Shu-Ching Chen, Kanoksri Sarinnapakorn, and LiWu Chang. A novel anomaly
detection scheme based on principal component classifier. Technical report, 2003.
12
Published as a conference paper at ICLR 2022
Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from noisy
labels with deep neural networks: A survey. arXiv preprint arXiv:2007.08199, 2020.
Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa. Joint optimization framework
for learning with noisy labels. In CVPR, 2018.
Kiran K Thekumparampil, Ashish Khetan, Zinan Lin, and Sewoong Oh. Robustness of conditional
gans to noisy labels. In NeurIPS, pp.10271-10282, 2018.
Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media, 2013.
Xiaobo Wang, Shuo Wang, Jun Wang, Hailin Shi, and Tao Mei. Co-mining: Deep face recognition
with noisy labels. In ICCV, pp. 9358-9367, 2019.
Yisen Wang, Weiyang Liu, Xingjun Ma, James Bailey, Hongyuan Zha, Le Song, and Shu-Tao Xia.
Iterative learning with open-set noisy labels. In CVPR, pp. 8688-8696, 2018.
Hongxin Wei, Lei Feng, Xiangyu Chen, and Bo An. Combating noisy labels by agreement: A joint
training method with co-regularization. In CVPR, pp. 13726-13735, 2020.
Tong Wei, Jiang-Xin Shi, Wei-Wei Tu, and Yu-Feng Li. Robust long-tailed learning under label noise.
arXiv preprint arXiv:2108.11569, 2021.
Kirk M Wolter. Introduction to variance estimation, volume 53. Springer, 2007.
Pengxiang Wu, Songzhu Zheng, Mayank Goswami, Dimitris Metaxas, and Chao Chen. A topological
filter for learning with label noise. In NeurIPS, 2020.
Songhua Wu, Xiaobo Xia, Tongliang Liu, Bo Han, Mingming Gong, Nannan Wang, Haifeng Liu,
and Gang Niu. Class2simi: A noise reduction perspective on learning with noisy labels. In ICML,
2021.
Songhua Wu, Mingming Gong, Bo Han, Yang Liu, and Tongliang Liu. Fair classification with
instance-dependent label noise. In Clear, 2022.
Xiaobo Xia, Tongliang Liu, Nannan Wang, Bo Han, Chen Gong, Gang Niu, and Masashi Sugiyama.
Are anchor points really indispensable in label-noise learning? In NeurIPS, pp. 6835-6846, 2019.
Xiaobo Xia, Tongliang Liu, Bo Han, Nannan Wang, Mingming Gong, Haifeng Liu, Gang Niu,
Dacheng Tao, and Masashi Sugiyama. Part-dependent label noise: Towards instance-dependent
label noise. In NeurIPS, 2020.
Xiaobo Xia, Tongliang Liu, Bo Han, Chen Gong, Nannan Wang, Zongyuan Ge, and Yi Chang.
Robust early-learning: Hindering the memorization of noisy labels. In ICLR, 2021.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive noisy
labeled data for image classification. In CVPR, pp. 2691-2699, 2015.
Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectified activations in
convolutional network. arXiv preprint arXiv:1505.00853, 2015.
Yilun Xu, Peng Cao, Yuqing Kong, and Yizhou Wang. L_dmi: A novel information-theoretic loss
function for training deep nets robust to label noise. In NeurIPS, pp. 6222-6233, 2019.
Shuo Yang, Lu Liu, and Min Xu. Free lunch for few-shot learning: Distribution calibration. In ICLR,
2021a.
Shuo Yang, Songhua Wu, Tongliang Liu, and Min Xu. Bridging the gap between few-shot and many-
shot learning via distribution calibration. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 2021b.
13
Published as a conference paper at ICLR 2022
Shuo Yang, Erkun Yang, Bo Han, Yang Liu, Min Xu, Gang Niu, and Tongliang Liu. Estimating
instance-dependent label-noise transition matrix using dnns. arXiv preprint arXiv:2105.13001,
2021c.
Quanming Yao, Hansi Yang, Bo Han, Gang Niu, and James Tin-Yau Kwok. Searching to exploit
memorization effect in learning with noisy labels. In ICML, pp.10789-10798, 2020a.
Yu Yao, Tongliang Liu, Bo Han, Mingming Gong, Jiankang Deng, Gang Niu, and Masashi Sugiyama.
Dual t: Reducing estimation error for transition matrix in label-noise learning. In NeurIPS, 2020b.
Yu Yao, Tongliang Liu, Mingming Gong, Bo Han, Gang Niu, and Kun Zhang. Instance-dependent
label-noise learning under a structural causal model. In NeurIPS, 2021.
Yu Yao, Tongliang Liu, Bo Han, Mingming Gong, Gang Niu, Masashi Sugiyama, and Dacheng Tao.
Rethinking class-prior estimation for positive-unlabeled learning. In ICLR, 2022.
Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor W Tsang, and Masashi Sugiyama. How does
disagreement benefit co-teaching? In ICML, 2019.
Xiyu Yu, Tongliang Liu, Mingming Gong, Kayhan Batmanghelich, and Dacheng Tao. An efficient
and provable approach for mixture proportion estimation using linear independence assumption.
In CVPR, pp. 4480-4489, 2018.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In ICLR, 2017.
Yivan Zhang, Gang Niu, and Masashi Sugiyama. Learning noise transition matrix from only noisy
labels via total variation regularization. In ICML, 2021.
Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks
with noisy labels. In NeurIPS, pp. 8778-8788, 2018.
Yue Zhao, Zain Nasrullah, and Zheng Li. Pyod: A python toolbox for scalable outlier detection.
Journal of Machine Learning Research, 20(96):1-7, 2019.
Songzhu Zheng, Pengxiang Wu, Aman Goswami, Mayank Goswami, Dimitris Metaxas, and Chao
Chen. Error-bounded correction of noisy labels. In ICML, pp. 11447-11457, 2020.
14
Published as a conference paper at ICLR 2022
Appendix
The appendices are organized as follows. Section A details the proofs for our theorems. Section B
presents more experimental results. Section C shows the details of used network structures. Section D
gives more justifications for our theoretical analyses. Section E provides more discussions on learning
by considering uncertainty. Section F provides more discussions on sample selection methods.
A	Proof of Theoretical Results
A.1 Proof of Theorem 1
For the circumstance with soft truncation, μz = η Pn=1 ψ(zi). As suggested in (Catoni, 2012), We
can exploit μ- and μ+ such that
μ- ≤ μz ≤ μ+,	(9)
to derive a bound for 瓦.For some positive real parameter a, we define
n
r(μz) = X ψ [α(zi - μz)] = 0∙	(10)
i=1
Let us introduce the quantity
1n
r(θ) =	Ψ [α(zi - θ)].	(11)
αn
i=1
With the exponential moment inequality (Gine et al., 2000) and the Cr inequality (Mohri et al., 2018),
we have
exp{anr(θ)} ≤ {1 + α(μ - θ) + α2[σ2 + (μ - θ)2]}n
≤ exp{nα(μz — θ) + nɑ2[σ2 + (μz — θ)2]}.
(12)
In the same way,
exp{-αnr(θ)} ≤ exp{-na(μz — θ) + nɑ2[σ2 + (μz — θ)2]}.	(13)
If We define for any μs ∈ R the bounds
B-(O) = μz - θ - α[σ2 + (μz - O)2]------)	(14)
αn
and
B+(θ) = μ — θ + α[σ2 + (μz — θ)2] +-----L	(15)
αn
From (Chen et al., 2020) (Lemma 2.2), we obtain that
P (r(O) > B-(O)) ≥ 1 - and P (r(O) < B+(O)) ≥ 1 - .	(16)
Let μ- be the largest solution of the quadratic equation B- (θ) and μ+ be the smallest solution of the
quadratic equation B+(O). Also, to guarantee the solution of the quadratic equation, we assume
4α2σ2 +
4log(L1)
≤ 1.
(17)
n
From (Chen et al., 2020) (Theorem 2.6), we then have
ασ2 + log曰
μ- ≥ μ— -0-f^
and
ασ2 + log曰
μ+ ≤ μ + α - ιαn
(18)
(19)
15
Published as a conference paper at ICLR 2022
With probability at least 1-2e, We have μ- ≤ μz ≤ μ+. We can choose α = f. Then We have
lμz - μz |
≤ σ2(n + σ2^-1))
n - σ2
(20)
Which holds With probability at least 1-2e.
We exploit the lower bound and let e =*.Then we have
σ2(t + σ2 l°f(2t))
`? = r.______________________________________________× ________t2___L
S μs	2	，
s	nt - σ2
where nt denotes the number of times that the example was selected in the time intervals.
(21)
Figure 3: The illustration of the influence function for the soft estimator.
Here, we provide the graph of the used influence function for the soft estimator, which explains the
mechanism of the function y = log(1 + x + x2/2) more clearly. The illustration is presented in
Figure 3. As can be seen, when x is large and may be an outlier, the influence function can reduce its
negative impact for mean estimation. Therefore, we exploit such an influence function for robust
mean estimation, which brings better classification performance.
A.2 Proof of Theorem 2
Lemma 1 ((Paulin et al., 2015)) Let Zn = {z1 , . . . , zn} be a (not necessarily time homogeneous)
Markov chain with mean μz, taking values in a Polish state space Λι X ... X An, with a mixing time
τ(υ) (for 0 ≤ υ ≤ 1). Let
Tmin = Jnf /(υ) ∙ 22_U) .	(22)
0≤υ<1	1 - υ
For some η ∈ R+, suppose that f : A → R satisfies thefollowing inequality:
n
f (a) — f(b) ≤ Xηl[ai = bi],	(23)
i=1
for every a, b ∈ A. Then for any e ≥ 0, we have
P(∣f(Zn)- Ef (Zn)I ≥ e) ≤ 2exp(--2e^∖	(24)
η2τmin
The detailed definition of the mixing time for the Markov chain can be found in (Paulin et al., 2015;
Roberts et al., 2004). Let f be the mean function. Following the prior work on mean estimation (Liu
et al., 2019; Diakonikolas et al., 2020; Diakonikolas & Kane, 2019; Niss & Tewari, 2020), without
loss of generality, we assume μz = 0 for the underlying true distribution, and |zi| is upper bounded
by Z. Then we can set η to 4Z/n for Eq. (23). Combining the above analyses, we can revise Eq. (24)
as follows:
(25)
16
Published as a conference paper at ICLR 2022
and
P max |zi |
i∈[n] i
2n
2Tmin log ——
2
≤ 2,
(26)
for 1 > 0 and 2 > 0. If we remove the potential outliers Zno from Zn . Therefore, we have
1
n - no
Σ
zi ∈Zn \Zn
1
n - no
zi∈Zn-zi∈Zn
≤
≤
+ x I)
zi ∈Zno
+ no max |zi |
i∈[n]
(27)
1
≤--------
n - no
匚	^~~2	2Zno
(2Z y 2τmin log   +------n-
T^n
V 2τmin log ^―
which holds with probability at least 1 - 1 - 2 .
For our task, We exploit the concentration inequality. Let eι = e? = J, and the losses be bounded by
L. Next we can obtain
∣μh - μ∣≤ t 2Lt (p2τmin lθg(4t) + to P4τmin
2√2TminL(t + √2to)
(t - to)t
(28)
VZIog(W
with the probability at least 1 - 1. In practice, it is easy to identify the value of L. For example, we
can training deep netWorks on noisy datasets to observe the loss distributions. Then, We exploit the
lower bound such that
'h = μh -
2√2TminL(t + √2to)
(t - tο)√t
∕log(4t)
n—
(29)
for sample selection.
B Complementary Experimental Analyses
	# of training	# of testing	# of class	size
MNIST	60,000	-10,000-	10	28×28×1
F-MNIST	60,000	-10000-	10	28×28×1
CIFAR-10	50,000	-10000-	10	32×32×3
CIFAR-100	50,000	10,000	100	32×32×3
Table 6: Summary of synthetic datasets used in the experiments.
B.1	The Details of Datasets and Generating Noisy Labels
For the details of datasets, the important statistics of the used datasets are summarized in Table 6.
For the details of generating noisy labels, we exploit both class-dependent and instance-dependent
label noise which include five types of synthetic label noise to verify the effectiveness of the proposed
method. Here, we describe the details of the noise setting as follows:
(1)	. Class-dependent label noise:
•	Symmetric noise: this kind of label noise is generated by flipping labels in each class uniformly to
incorrect labels of other classes.
17
Published as a conference paper at ICLR 2022
Figure 4: Synthetic class-dependent transition matrices used in our experiments on MNIST. The noise
rate is set to 20%.
Noise type	Sym.		Asym.		Pair.		Trid.		Ins.	
Method/Noise	20%	40%	20%	40%	20%	40%	20%	40%	20%	40%
NPCL	98.66	98.21	98.89	96.14	98.06	97.50	98.84	97.62	97.25	95.75
	±0.03	±0.11	±0.02	±1.21	±0.24	±0.18	±0.05	±0.29	±0.63	±1.04
INCV	98.58	98.37	98.89	97.67	97.98	97.21	98.74	97.65	96.83	94.97
	±0.14	±0.06	±0.06	±0.26	±0.21	±0.39	±0.17	±0.32	±1.04	±1.35
APL	98.76	94.92	98.63	88.65	98.66	68.44	98.93	76.44	97.63	87.90
	±0.06	±0.31	±0.05	±1.72	±0.10	±2.95	±0.04	±3.04	±0.73	±1.94
CDR	94.77	92.16	96.73	91.05	93.25	71.02	94.06	70.28	93.17	77.45
	±0.17	±0.73	±0.19	±0.76	±0.90	±3.89	±0.92	±4.01	±0.96	±3.04
Table 7: Test accuracy (%) on MNIST over the last ten epochs.
•	Asymmetic noise : this kind of label noise is generated by flipping labels within a set of
similar classes. In this paper, for MNIST, flipping 2→7, 3—8, 566. For F-MNIST, flip-
ping TSHIRT→SHIRT, PULLOVER→COAT, SANDALS→SNEAKER. For CIFAR-10, flipping
TRUCK→AUTOMOBILE, BIRD→AIRPLANE, DEER→HORSE, CAT6DOG. For CIFAR-100,
the 100 classes are grouped into 20 super-classes, and each has 5 sub-classes. Each class is then
flipped into the next within the same super-class.
•	Pairflip noise: the noise flips each class to its adjacent class.
•	Tridiagonal noise: the noise corresponds to a spectral of classes where adjacent classes are easier
to be mutually mislabeled, unlike the unidirectional pair flipping. It can be implemented by two
consecutive pair flipping transformations in the opposite direction.
(2)	. Instance-dependent label noise:
• Instance noise: the noise is quite realistic, where the probability that an instance is mislabeled
depends on its features. We generate this type of label noise to validate the effectiveness of the
proposed method as did in (Xia et al., 2020; Yao et al., 2021).
We use synthetic noisy MNIST as an example and plot the noise transition matrices in Figure 4. The
noise rate is set to 20%.
B.2	Comparison with Other Baselines
We focus on the sample selection approach in learning with noisy labels. In the main paper (Section
3.1), we fairly compare our methods with the baselines which also focus on sample selection. We
add two other sample-selection baselines here. As suggested by the related survey (Song et al., 2020),
the sample selection methods can be divided into three categories: (a) “Multi-network Learning”, (b)
“Multi-round Learning”, and (c) “Hybrid Approach”. As our method belongs to (a), we exploit INCV
(Chen et al., 2019) which belongs to (b), and NPCL (Lyu & Tsang, 2020) which belongs to (c), to
make the comparison more comprehensive.
Here, we also evaluate other types of baselines. We exploit APL (Ma et al., 2020) and CDR (Xia
et al., 2021), which add implicit regularization from different perspectives. The experiments are
conducted on MNIST and F-MNIST. Other experimental settings are the same as those in the main
paper. The experimental results are provided in Tables 7 and 8, which show that the proposed methods
are superior to these baselines with respect to classification performance.
18
Published as a conference paper at ICLR 2022
Noise type	Sym.		Asym.		Pair.		Trid.		Ins.	
Method/Noise	20%	40%	20%	40%	20%	40%	20%	40%	20%	40%
NPCL	91.65	86.72	91.20	70.65	91.07	85.03	90.99	88.79	90.77	86.34
	±0.37	±2.30	±0.06	±2.83	±0.22	±1.94	±0.15	±1.04	±0.85	±2.78
INCV	91.66	85.39	91.35	65.82	90.26	85.73	91.76	88.42	89.54	86.21
	±0.31	±1.14	±0.70	±4.32	±0.38	±1.92	±0.17	±1.55	±1.79	±2.86
APL	91.73	89.06	90.13	80.34	90.22	78.54	90.84	86.53	90.96	85.55
	±0.20	±0.41	±0.17	±0.63	±0.80	±4.33	±0.22	±0.76	±0.77	±2.86
CDR	85.62	71.83	89.78	79.05	85.72	69.07	86.75	73.63	85.92	73.14
	±0.96	±1.37	±0.41	±1.39	±0.65	±2.31	±1.19	±2.82	±1.43	±3.12
Table 8: Test accuracy on F-MNIST over the last ten epochs.
B.3	EXPERIMENTS ON SYNTHETIC CIFAR-100
For CIFAR-100, we use a 7-layer CNN structure from (Yu et al., 2019; Yao et al., 2020a). Other
experimental settings are the same as those in the experiments on MNIST, F-MNIST, and CIFAR-10.
The results are provided in Table 9. We can see the proposed method outperforms all the baselines.
Noise type	Sym.		Asym.		Pair.		Trid.		Ins.	
Method/Noise	20%	40%	20%	40%	20%	40%	20%	40%	20%	40%
S2E	44.59	25.78	42.18	26.81	42.99	26.96	43.16	27.72	43.13	27.12
	±0.32	±5.44	±1.73	±2.25	±1.54	±2.48	±0.93	±3.56	±0.67	±3.86
MentorNet	43.15	37.62	41.03	28.27	40.06	27.17	42.20	31.74	40.54	33.09
	±0.42	±0.89	±0.22	±0.41	±0.37	±0.92	±0.30	±0.88	±0.69	±1.53
Co-teaching	45.17	40.95	42.76	30.27	42.50	30.07	44.41	34.96	42.23	35.87
	±0.25	±0.52	±0.34	±0.33	±0.39	±0.17	±0.41	±0.35	±0.52	±1.47
SIGUA	42.03	40.53	36.67	26.71	36.48	26.73	39.21	32.69	39.19	33.51
	±0.33	±0.49	±0.25	±0.42	±0.37	±0.33	±0.40	±0.36	±0.32	±0.43
JoCor	45.93	41.56	42.89	29.19	42.12	30.12	44.98	34.23	44.28	35.60
	±0.21	±0.57	±0.37	±1.42	±0.35	±0.65	±0.27	±1.13	±0.59	±0.99
CNLCU-S	46.09	42.11	43.06	30.47	43.08	30.33	45.19	35.49	44.80	36.23
	±0.29	±0.70	±0.28	±0.37	±0.92	±0.74	±0.90	±1.30	±0.70	±0.49
CNLCU-H	^ 46:27^	42.05 -	-43.21	30:55 -	-43.25-	30:79	一 45.02 一	-35.24^	45.02 一	-36.17
	±0.38	±0.87	±0.93	±0.72	±0.75	±0.86	±1.06	±0.93	±1.07	±1.54
Table 9: Test accuracy (%) on CIFAR-100 over the last ten epochs. The best two results are in bold.
B.4	Experiments for Ablation Study
We conduct the ablation study to analyze the sensitivity of the length of time intervals. The results
are shown in Figure 5 and 6. As we can seen, the proposed method, i.e., CNLCU-S and CNLCU-H
are robust to the choices of hyperparameters.
11-NWI I jas=w,κ I
- sym. -
• 30¾WtW
w¾wtw
S ，	，	*	，	《
IkiwiMefVQl
→- ao¾wtw
<fl¾wtw
S ，	，	*	，	《
Tfcwlntavel
Asym.
2∙
Fq
•■,■; "∖'-λ'
I ，.，
-Trid.-
，，，,，♦
1⅛⅛ WetVBl
，，，,，♦
- ins. -
，	，	，	,	， t






Figure 5: Illustrations of the hyperparameter sensitivity for the proposed CNLCU-S. The error bar
for standard deviation in each figure has been shaded.
Note that in this paper, we concern uncertainty from two aspects, i.e., the uncertainty about small-loss
examples and the uncertainty about large-loss examples. Here, we conduct ablation study to show the
19
Published as a conference paper at ICLR 2022
一Pair.-
—Sym.-
11-NWI
—Asym.-
- trid. -
—Ins.-
6⅛00
• 30¾>χ4M
IJSWVH I
11	33	”	14	"
TfcwIrtfHvwI
二L二
Figure 6: Illustrations of the hyperparameter sensitivity for the proposed CNLCU-H. The error bar
for standard deviation in each figure has been shaded.
effect of removing different components to provide insights into what makes the proposed methods
successful. The experiments are conducted on MNIST and F-MNIST. Other experimental settings
are the same as those in the main paper (Section 3.1). Note that we employ two networks to teach
each other following (Han et al., 2018). Therefore, when we do not consider uncertainty in sample
selection, the proposed methods will reduce to the baseline Co-teaching (Han et al., 2018).
To study the effect of concerning uncertainty about small-loss examples, we remove the concerns
about large-loss examples, i.e., the network is not encouraged to choose the less selected examples for
updates. We express such a setting as “without concerning about large-loss examples” (abbreviated
as w/o cl). To study the effect of concerning uncertainty about large-loss examples, we remove the
concerns about small-loss examples, i.e., we only exploit the predictions of the current network. We
express such a setting as “without concerning about small-loss examples” (abbreviated as w/o cs).
Besides, we express the setting which directly uses non-robust mean as Co-teaching-M.
The experimental results of ablation study are provided in Tables 10 and 11. As can be seen, both
aspects of uncertainty concerns can improve the robustness of models. Therefore, combining two
uncertainty concerns, we can better combat noisy labels. In addition, robust mean estimation is
superior to the non-robust mean in learning with noisy labels.
Noise type	Sym.		Asym.		Pair.		Trid.		Ins.	
Method/Noise	20%	40%	20%	40%	20%	40%	20%	40%	20%	40%
CNLCU-S	98.82	98.31	98.93	97.67	98.86	97.71	99.09	98.02	98.77	97.78
	±0.03	±0.05	±0.06	±0.22	±0.06	±0.64	±0.04	±0.17	±0.08	±0.25
CNLCU-S w/o cl	98.02	96.83	98.50	96.25	98.22	96.08	98.64	97.25	98.17	97.13
	±0.08	±0.29	±0.04	±0.13	±0.13	±0.75	±0.31	±0.24	±0.20	±0.40
CNLCU-S w/o cs	98.15	97.12	98.36	96.39	98.04	96.12	98.74	97.30	98.11	97.32
	±0.20	±0.22	±0.07	±0.48	±0.24	±0.68	±0.05	±0.52	±0.15	±0.43
CNLCU-H	98.70	98.24	99.01	98.01	98.44	97.37	98.89	97.92	98.74	97.42
	±0.06	±0.06	±0.04	±0.03	±0.19	±0.32	±0.15	±0.05	±0.16	±0.39
CNLCU-H w/o cl	98.06	96.92	98.39	96.51	97.04	95.62	98.33	97.41	98.01	96.15
	±0.13	±0.23	±0.04	±0.57	±0.87	±0.93	±0.47	±0.92	±0.20	±0.28
CNLCU-H w/o cs	98.19	97.05	98.76	97.17	97.26	96.31	98.29	97.65	98.34	96.49
	±0.22	±0.49	±0.59	±0.60	±1.19	±0.25	±0.17	±0.92	±0.36	±0.48
Co-teaching-M	97.72	97.78	98.27	95.42	96.22	95.01	97.92	96.64	98.02	96.03
	±0.08	±0.32	±0.03	±0.42	±0.10	±0.65	±0.14	±0.77	±0.04	±0.57
Co-teaching	97.53	95.62	98.25	95.08	96.05	94.16	98.05	96.18	97.96	95.02
	±0.12	±0.30	±0.08	±0.43	±0.96	±1.37	±0.06	±0.85	±0.09	±0.39
Table 10: Test accuracy (%) on MNIST over last ten epochs.
B.5	Results on Synthetic Imbalanced Noisy Datasets
In the main paper, we have provided the test accuracy on synthetic imbalanced noisy datasets in
Figure 2. Due to the limited space, we provide the selected ratio achieved on IM-MNIST and IM-F-
MNIST in Table 12. The results explain well why our methods perform better on synthetic imbalanced
noisy datasets, i.e., our methods can make better use of training examples with the imbalanced classes.
Note that since we give a try to large-loss but less selected data in a conservative way, the selected
20
Published as a conference paper at ICLR 2022
Noise type	Sym.		Asym.		Pair.		Trid.		Ins.	
Method/Noise	20%	40%	20%	40%	20%	40%	20%	40%	20%	40%
CNLCU-S	92.37	91.45	92.57	83.14	92.04	88.20	92.24	90.08	91.69	89.02
	±0.15	±0.28	±0.15	±1.77	±0.26	±0.44	±0.17	±0.34	±0.10	±1.02
CNLCU-S w/o cl	91.77	89.40	91.25	72.93	91.53	87.31	91.31	89.50	91.09	88.45
	±0.35	±0.26	±0.30	±2.63	±0.17	±0.59	±0.52	±0.32	±0.13	±0.57
CNLCU-S w/o cs	91.85	90.76	91.94	80.99	91.28	87.31	91.39	89.29	90.98	88.73
	±0.33	±0.28	±0.09	±2.74	±0.20	±0.72	±0.07	±0.51	±0.43	±0.62
CNLCU-H	92.42	91.60	92.60	82.69	91.70	87.70	92.33	90.22	91.50	88.79
	±0.21	±0.19	±0.18	±0.43	±0.18	±0.69	±0.26	±0.71	±0.21	±1.22
CNLCU-H w/o cl	91.70	90.05	91.08	71.35	91.03	87.22	91.59	90.01	90.80	88.31
	±0.04	±0.31	±0.06	±2.30	±0.29	±0.72	±0.07	±0.24	±0.27	±1.09
CNLCU-H w/o cs	91.82	90.92	92.45	80.73	91.21	87.49	92.08	89.72	91.21	88.62
	±0.13	±0.42	±0.25	±1.63	±0.17	±0.32	±0.13	±0.24	±0.38	±0.73
Co-teaching-M	91.33	89.05	91.14	71.03	90.85	86.95	91.50	89.18	90.74	88.25
	±0.18	±0.73	±0.90	±3.73	±0.61	±0.19	±0.46	±0.44	±1.06	±0.92
Co-teaching	91.48	88.80	91.03	68.07	90.77	86.91	91.24	89.18	90.60	87.90
	±0.10	±0.29	±0.14	±4.58	±0.23	±0.71	±0.11	±0.36	±0.12	±0.45
Table 11: Test accuracy (%) on F-MNIST over last ten epochs.
Dataset		IM-MNIST						IM-F-MNIST				
Method/Noise	10%	20%	30%	40%	10%	20%	30%	40%
S2E	0.13	0.11	0.09	0.05	0.13	0.17	0.16	0.12
	±0.12	±0.05	±0.02	±0.01	±0.04	±0.03	±0.02	±0.04
MentorNet	0.10	0.15	0.12	0.13	0.12	0.15	0.09	0.14
	±0.02	±0.02	±0.03	±0.02	±0.01	±0.03	±0.01	±0.02
Co-teaching	0.09	0.07	0.05	0.12	0.17	0.04	0.13	0.07
	±0.03	±0.02	±0.01	±0.01	±0.05	±0.00	±0.04	±0.01
SIGUA	0.04	0.04	0.01	0.02	0.03	0.02	0.04	0.00
	±0.00	±0.00	±0.00	±0.00	±0.00	±0.00	±0.00	±0.00
JoCor	0.11	0.08	0.07	0.06	0.05	0.13	0.13	0.07
	±0.04	±0.01	±0.03	±0.02	±0.01	±0.04	±0.03	±0.02
CNLCU-S	0.60	0.37	0.39	0.38	0.35	0.39	0.36	0.30
	±0.11	±0.09	±0.04	±0.06	±0.03	±0.04	±0.03	±0.02
CNLCU-H	-0.57--	-0.32-	^ 0.37 -	^ 0.32 一	-0.34-	0:35—	-0：32 -	^^0.28^'
	±0.13	±0.01	±0.07	±0.05	±0.02	±0.06	±0.04	±0.03
Table 12: Selected ratio (%) on IM-MNIST and IM-F-MNIST. The best two results are in bold.
ratio is still far away from the class prior probability on the test set, i.e., 10%. However, a little
improvement of the selection ratio can bring a considerable improvement of test accuracy. These
results tell us that, in the sample selection approach when learning with noisy labels, improving
the selected ratio of training examples with the imbalanced classes is challenging but promising for
generalization. This practical problem deserves to be studied in depth.
B.6	Experiments with High Noise Levels
In the main paper, we set the noise rates to be smaller than 50%, which ensures that clean labels in
noisy classes are diagonally dominant in all label noise settings. To demonstrate the effectiveness
of our method with high noise levels, we conduct experiments on MNIST and F-MNIST with 50%,
60%, and 70% symmetric noise. The experimental results are presented in Table 13. As can be seen,
the proposed methods can achieve the best performance in almost all cases.
C Complementary Explanation for Network Structures
Table 14 describes the 9-layer CNN (Han et al., 2018) used on MNIST, F-MNIST, and CIFAR-10.
Table 15 describes the 7-layer CNN (Yu et al., 2019) used on CIFAR-100. Here, LReLU stands for
Leaky ReLU (Xu et al., 2015). The slopes of all LReLU functions in the networks are set to 0.01.
Note that that the 7/9-layer CNN is a standard and common practice in weakly supervised learning.
We decided to use these CNNs, since then the experimental results are directly comparable with
previous approaches in the same area, i.e., learning with noisy labels.
21
Published as a conference paper at ICLR 2022
DataSet		MNIST					F-MNIST			
Method/Noise	50%	60%	70%	50%	60%	70%
APL	84.97±2.97	75.68±1.22	70.11±0.52	76.80±3.21	72.77±4.37	68.39±7.17
CDR	76.85±2.46	57.22±1.92	54.22±0.94	53.41±1.81	45.82±2.77	41.33±3.69
INCV	96.14±0.33	94.12±1.13	92.13±0.75	84.92±0.60	82.79±1.64	80.12±0.26
NPCL	97.05±0.14	94.77±0.73	93.58±1.26	86.07±1.33	83.29±2.65	79.39±1.58
S2E	87.76±2.39	79.24±4.79	50.30±7.82	62.20±3.94	59.88±1.27	45.88±7.96
MentorNet	91.14±0.17	90.11±0.37	88.72±0.46	86.51±0.11	85.91±0.44	83.27±0.55
Co-teaching	95.60±0.38	95.44±0.30	94.11±0.38	88.72±0.14	87.92±0.34	85.92±0.72
SIGUA	91.35±2.62	88.62±1.93	86.08±6.04	83.39±3.29	79.36±4.54	72.14±4.28
JoCor	97.14±0.10	96.47± 0.46	95.01±0.29	89.16±0.27	87.93±0.61	86.99±0.92
CNLCU-S	97.78±0.07	96.73±0.31	96.02±0.38	89.56±0.25	88.56±0.59	87.54±0.42
--CNLCU-H …	-97：64±0.25一	%45±0.55-	--95∙30±0j6	一89.77±0»	-瓯07±0.62一	-87.40±1.05一
Table 13: Test accuracy on MNIST and F-MNIST with high noise levels over the last ten epochs. The
best two results are in bold.
CNN on MNIST	CNN on F-MNIST	CNN on CIFAR-10
28×28 Gray Image	28×28 Gray Image	32×32RGB Image
3×3 conv, 128 LReLU
3×3 conv, 128 LReLU
3×3 conv, 128 LReLU
2×2 max-pool
dropout, p = 0.25
3×3 conv, 256 LReLU
3×3 conv, 256 LReLU
3×3 conv, 256 LReLU
2×2 max-pool
dropout, p = 0.25
3×3 conv, 512 LReLU
3×3 conv, 256 LReLU
3×3 conv, 128 LReLU
avg-pool
dense 128→10	∣ dense l28→10	∣ dense 128→10
CNN on CIFAR-100
32×32 RGB Image
3×3conv, 64 ReLU
3 × 3 Conv, 64 ReLU
2×2 max-pool
3×3 conv, 128 ReLU
3×3 conv, 128 ReLU
2×2 max-pool
3×3 conv, 196 ReLU
3×3 conv, 196 ReLU
2×2 max-pool
dense 256→100
Table 15: The CNN on CIFAR-100.
Table 14: The CNN on MNIST, F-MNIST, and CIFAR-
10.
D Needed Assumptions without Noisy Validation Sets
In the main paper, a noisy validation set is exploited to determine σ2 and τmin . We discuss, if we do
not use the noisy validation set, what assumptions we will need.
•	In statistical learning, it is an important problem for estimating the variance of a data
distribution. However, it is hard to obtain an accurate estimation from a finite sample. To
reduce the estimation errors, some kinds of assumptions are often needed, e.g., random
sampling assumptions, replacement assumptions, and strict distribution assumptions. We
suggest the readers to refer (Wolter, 2007) for more details.
•	For the accurate estimation of τmin , we have to assume that the Markov chain can exhibit a
cutoff, which means that the total variation distance decreases very rapidly in a small interval
(Paulin et al., 2015). Readers can check the Figure 1 of (Lubetzky & Sly, 2013) for better
understanding.
E Further Discussion on Learning by Uncertainty
We discuss a related work named ActiveBias (Chang et al., 2017) on learning with uncertainty,
although it studies the predicted probability, rather than the loss. In general, the differences between
our work and ActiveBias lie in the different problems, different focuses, and different technical
implementation. In more detail, we summarize the main differences between the two works in the
following:
•	ActiveBias mainly focuses on mining hard examples to help generalization. Although it
mentions the label-noise problem, it does not consider comprehensive noise settings. Also,
22
Published as a conference paper at ICLR 2022
the baselines are SGD, ADAM, and other optimization methods, but not the methods are
specially designed for learning with noisy labels. In contrast, our work mainly focuses on
learning with noisy labels and has more comprehensive experiments, which explore the
performance of different label-noise methods under noisy supervision.
•	ActiveBias focuses on using the hard examples, but not too difficult examples, which is
stated in the original paper. In the imbalanced setting of this paper, clean imbalanced data
are too difficult to distinguish with the predictions of models (Figure 1, right) and therefore
are too difficult examples. Our methods aim to make use of such examples. Experiments
show that such examples are of great importance for generalization.
•	The introduction of uncertainty in ActiveBias relies on the estimation of the prediction
variance but does not consider the bad influence of mislabeled data/outliers on this estimation.
Our methods use the loss distribution and the side effect of mislabeled data is considered by
robust mean estimators.
F Related Work on Sample Selection
In this paper, we focus on the procedure of using the small-loss criterion for sample selection, which
is very commonly used in learning with noisy labels and shared by the state-of-the-art methods. We
review prior effects in this line. Existing methods (Jiang et al., 2018; Han et al., 2018; Yu et al.,
2019; Wei et al., 2020; Lyu & Tsang, 2020) focus on the class-balanced problem. Based on the
memorization effect of deep networks, they select examples with small losses for network updates
and drop examples with large losses. The main reason is that clean examples are more likely to
have small losses, but mislabeled examples are more likely to have large losses in class-balanced
cases. Distinctly, these methods ignore the clean examples with large losses, following suboptimal
generalization. When the training dataset is noisy and extremely imbalanced, prior methods are weak.
It is because since they cannot select clean hard examples within non-dominant classes effectively
based on losses, which is shown in the main paper. Unfortunately, such examples are critical for
generalization. We are the first one to focus on this important problem and tackle it by considering
the uncertainty of losses.
23