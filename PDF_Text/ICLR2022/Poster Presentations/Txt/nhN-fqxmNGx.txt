Published as a conference paper at ICLR 2022
A Comparison of Hamming Errors of Represen-
tative Variable Selection Methods
Zheng Tracy Ke
Department of Statistics
Harvard University
Cambridge, MA 02138, USA
zke@fas.harvard.edu
Longlin Wang
Department of Statistics
Harvard University
Cambridge, MA 02138, USA
lwang2@fas.harvard.edu
Ab stract
Lasso is a celebrated method for variable selection in linear models, but it faces
challenges when the variables are moderately or strongly correlated. This motivates
alternative approaches such as using a non-convex penalty, adding a ridge regular-
ization, or conducting a post-Lasso thresholding. In this paper, we compare Lasso
with 5 other methods: Elastic net, SCAD, forward selection, thresholded Lasso,
and forward backward selection. We measure their performances theoretically
by the expected Hamming error, assuming that the regression coefficients are iid
drawn from a two-point mixture and that the Gram matrix is block-wise diagonal.
By deriving the rates of convergence of Hamming errors and the phase diagrams,
we obtain useful conclusions about the pros and cons of different methods.
1	Introduction
Variable selection is one of the core problems in high-dimensional data analysis. Consider a linear
regression, where the response y ∈ Rn and the design matrix X = [x1, . . . , xp] ∈ Rn×p satisfy that
y = Xe + z,	∣∣χjk = 1, Z 〜N(0,σ2In).
(1)
The goal is estimating the support of β (Supp(β)). Lasso (Tibshirani, 1996) is a popular method:
βlasso = argmi/{∣∣y - Xβ∣2∕2 + λ∣β∣∣J.
(2)
Lasso has good rates of convergence on the Lq-estimation error or prediction error (Bickel et al.,
2009). However, it can be unsatisfactory for variable selection, especially when the columns in the
design matrix are moderately or strongly correlated. Zhao & Yu (2006) showed that an irrepresentable
condition on X is necessary for Lasso to recover Supp(β) with high probability, and such a condition
is restrictive when p is large (Fan & Lv, 2010). Ji & Jin (2012) studied the Hamming error of Lasso
and revealed Lasso’s non-optimality by lower-bounding its Hamming error rate. Many alternative
strategies were proposed for variable selection, such as using non-convex penalties (Fan & Li, 2001;
Zhang, 2010; Shen et al., 2012), adding a ridge regularization (Zou & Hastie, 2005), post-processing
on the Lasso estimator (Zou, 2006; Zhou, 2009), and iterative algorithms (Zhang, 2011; Donoho
et al., 2012). In this paper, our main interest is to theoretically compare these different strategies.
Existing theoretical studies focused on ‘model selection consistency’ (e.g., Fan & Li (2001); Zhao &
Yu (2006); Zou (2006); MeinshaUsen & Buhlmann (2010); Loh & WainWright (2017)), which uses
P(Supp(β) = Supp(β)) to measure the performance of variable selection. However, for many real
applications, the study of the Hamming error (i.e., total number of false positives and false negatives)
is in urgent need. For example, in genome-wide association studies (GWAS) or Genetic Regulatory
Network, the goal is to identify the genes or SNPs that are truly associated with a given phenotype,
and we hope to find a multiple testing procedure that simultaneously controls the FDR and maximizes
the power (for multiple testing). This problem can be re-cast as minimizing the Hamming error in a
special regression setting (Efron, 2004; Jin, 2012; Sun & Cai, 2007). This motivates us to study the
Hamming errors of variable selection methods, which were rarely considered in the literature.
We adopt the rare and weak signal model (Donoho & Jin, 2004; Arias-Castro et al., 2011; Jin & Ke,
2016), which is often used in theoretical analysis of sparse linear models. Let p be the asymptotic
1
Published as a conference paper at ICLR 2022
parameter. Given constants H ∈ (0,1) and r > 0, we assume that βj 's are iid generated such that
β = ∫τp, WithPrObability ep,	where e = - T = p2rl ( )	⑶
βj = 0, with probability 1 - p, where p=p , τp= 2r log(p). (3)
As P → ∞, kβko ≈ p1-", and a nonzero βj is at the critical order ,log(p). 1 The two parameters
(H, r) capture the sparsity level and signal strength, respectively. We may generalize (3) to let nonzero
βj's take different values in [τp, ∞), but the current form is more convenient for presentation.
The blockwise covariance structure is frequently observed in real applications. In genetic data, there
may exist strong correlations between nearby genetic markers, but the long-range dependence is
usually negligible; as a result, the sample covariance matrix is approximately blockwise diagonal
(Dehman et al., 2015). In financial data, the sample covariance matrix of stock returns (after common
factors are removed) is also approximately blockwise diagonal, where each block corresponds to an
industry group (Fan et al., 2015). Motivated by these examples, we consider an idealized setting,
where the Gram matrix G = X0X is block-wise diagonal consisting of 2 × 2 blocks:
G=diag(B,B,...,B,B0),	where B= 1	ρ	and B0	= B,	if p is even,	(4)
ρ	1	1,	if p is odd.
This is an idealization of the blockwise covariance structures in real applications. We may generalize
(4) to allow unequal block sizes and unequal off-diagonal entries, but we keep the current form for
convenience of presentation. Model (4) is also closely connected to the random designs in compressed
sensing (Donoho, 2006). Write X = [X1, X2, . . . , Xn]0. Suppose X1, X2, . . . , Xn are iid generated
from N 0, n-1Σ), where Σ has the same form as G in (4). In a high-dimensional sparse setting, we
have kβ k0 n p. Then, G = X0X ≈ Σ, and due to the blessing of sparsity of β, Gβ ≈ Σβ. As
a result, X0y (sufficient statistic of β) satisfies that X0y = Gβ +N(0, G) ≈ Σβ +N(0, Σ), and the
right hand side reduces to Model (4) (Genovese et al., 2012). In Section 3.5, we formally show that
this random design setting is asymptotically equivalent to Model (4).
Now, under model (3) and model (4), we have three parameters (H, r, ρ). They capture the sparsity
level, signal strength and design correlations, respectively. Our main results are the explicit con-
vergence rates of Hamming error, as a function of (H, r, ρ), for different methods. We will study
six methods: (i) Lasso as in (2); (ii) Elastic net (Zou & Hastie, 2005), which adds an additional L2-
penalty to (2), (iii) smoothly clipped absolute deviation (SCAD) (Fan & Li, 2001), which replaces the
L1 -penalty by a non-convex penalty, (iv) thresholded Lasso (Zhou, 2009), which further thresholds
the Lasso solution, and two iterative algorithms, (v) forward selection and (vi) forward backward
selection (Huang et al., 2016); see Section 3 for a precise description of each method. To our best
knowledge, our results are the first that directly compare Hamming errors of these methods.
2	A preview of main results and some discussions
For any β, its Hamming error is H(β, β) = Pj= 1{βj = 0, βj = 0} + Pj=ι 1{βj = 0, βj = 0}.
As we shall show, for any of the six methods studied here, there exists a function h(H, r, ρ) ∈ [0, 1]
such that E[H(β, β)] = LpP1-h(^,r,ρ), where Lp is a multiAog(P) term. (A multi-log(p) term is
such that Lp ∙ Pe → ∞ and Lp ∙ ρ-e → 0 for any E > 0.) Since the expected number of true relevant
variables is PI『we are interested in three cases:
•	Exact recovery: h(H, r, ρ) > 1. In this case, the expected Hamming error is o(1) as P → ∞.
It follows that model selection consistency holds.
•	Almost full recovery: H < h(H, r, ρ) < 1. In this case, the expected Hamming error does
not vanish as P → ∞, but it is much smaller than the total number of true relevant variables.
Variable selection is still satisfactory (although model selection consistency no longer holds).
•	No recovery: h(H, r, ρ) ≤ H. In this case, the expected Hamming error is comparable with
or much larger than the total number of true relevant variables. Variable selection fails.
1In (1), we assume that each column of X is standardized to have a unit '2-norm and that the order of nonzero
βj is ʌ/log(n). Alternatively, many works assume that each column of X is standardized to have an '2-norm of
√n and that the order for nonzero βj is n-1/2 ʌ/log(p). These are two equivalent parameterizations.
2
Published as a conference paper at ICLR 2022
p = 0.5
18
8o
Figure 1: Phase diagrams of six variable selection methods for a block-wise diagonal design. The
parameters (H, r, P) characterize the sparsity, signal strength, and design correlations, respectively.
For each method, We plot the curve r = U ⑻ which separates Region of Almost Full Recovery and
Region of Exact Recovery (the lower this curve, the better). Explicit forms of U(H) are in Section 3.
On the left panel, the curves for Lasso and SCAD overlap and are displayed as a dashed line. How to
interpret these phase curves are discussed in Section 2.
p= - 0.4
16
14
12
10
8
6
4
2
一■ Elastic net (μ = 0.25)
----Lasso
SCAD
Thresholded Lasso
Forward selection
Forward backward selection
I I Region of no recovery
0.2
0.8
0.4	0.6
ə
1.0
We call the two-dimensional space (H, r) the phase space. For each fixed P, the phase space is divided
into three regions: Region of Exact Recovery (ER), which is the subset {(H, r) : h(H, r, P) > 1}, and
Region of Almost Full Recovery (AFR) and Region of No Recovery (NR) defined similarly. This gives
rise to a phase diagram for each method. We denote the curve separating ER region and AFR region
by r = U(H) and the curve separating AFR region and NR region by r = L(H); they are called the
upper and lower phase curves, respectively. The phase diagram and phase curves are convenient
ways to visualize the convergence rates of the Hamming error.
Figure 1 shows the phase curves for the six methods (with explicit expressions in the theorems in
Section 3). These phase curves depend on the correlation parameter P. Under our model, for each
diagonal block (j, j + 1), it holds that E[χjy∣β] = βj + ρβj+ι, where βj,βj+ι ∈ {0, τp}. Therefore,
a positive P boosts the signal at each individual site (i.e., E[χj y∣β] ≥ βj), while a negative P leads
to potential ‘signal cancellation' (i.e., E[χjy∣β] ≤ βj). This is why the phase curves have different
shapes for positive and negative P. In Figure 1, we plot the phase curves for P = 0.5 and P = -0.4.
For other positive/negative value of P, the patterns are similar.
Discussion of SCAD. SCAD is a representative of non-convex penalization methods. There have
been inspiring works that demonstrate the advantages of using a non-convex penalty (e.g., Fan
& Peng (2004); Loh & Wainwright (2017)). Our results support their insights from a different
angle: The phase curve of SCAD is strictly better than that of Lasso, when H < 0.5 and P < 0.
Furthermore, our results illustrate where the advantage of SCAD comes from — compared with
Lasso, it handles ‘signal cancellation’ better. To see this, we recall that ‘signal cancellation’ only
happens for P < 0. Moreover, under our model (3), the expected number of signal pairs (a signal pair
is a diagonal block {j, j + 1} where both βj and βj+ι are nonzero) is X Pep = p1-2^. Therefore,
‘signal cancellation’ becomes problematic only when H < 0.5 and P < 0 both hold. This explains
why the phase curves of SCAD and Lasso are the same for the other values of H and P. We note that
in the previous studies (e.g., Loh & Wainwright (2017)), the advantages of a non-convex penalty in
handling ‘signal cancellation’ are reflected in the weaker conditions of (X, β) for achieving model
selection consistency. Our results support the advantage of using a non-convex penalty by directly
studying the Hamming errors and phase diagrams.
The performance of SCAD can be further improved by adding an entry-wise thresholding on β. We
believe that the phase diagrams of thresholded SCAD are better than those of SCAD itself, although
the extremely tedious analysis impedes us from specific results for now. Also, we are cautious about
what to conclude from comparing SCAD and thresholded Lasso. In our settings, Lasso has no model
selection consistency mainly because the signals are too weak (i.e., r is not sufficiently large). In
such settings, thresholded Lasso outperforms SCAD in terms of the Hamming error. However, there
are cases where Lasso has no model selection consistency no matter how large the signal strength is
3
Published as a conference paper at ICLR 2022
(Zhao & Yu, 2006). For those cases, it is possible that SCAD is better than thresholded Lasso (see
Wainwright (2009) for a related study).
Discussion of Elastic net. The phase curve of Elastic net is worse than that of Lasso. As we will
explain in Section 3.1, Elastic net is a ‘bridge’ between Lasso and marginal regression in our case.
Since the phase curve of marginal regression is always worse than that of Lasso for the blockwise
diagonal design, we do not benefit from using Elastic net in the current setting. We must note that
Elastic net is motivated by genetic applications where several correlated variables are competing
as predictors, and where it is implicitly assumed that groups of correlated variables tend to be all
relevant or all irrelevant (Zou & Hastie, 2005). This is not captured by our model (3). Therefore,
our results do not go against the benefits of Elastic net known in the literature, but rather our results
support that the advantages of Elastic net come from ‘group’ appearance of signal variables.
Discussion of thresholded Lasso. Thresholded Lasso is a representative of improving Lasso by
post-processing. There have been inspiring works that demonstrate the advantages of such a post-
processing (van de Geer et al., 2011; Wang et al., 2020; Weinstein et al., 2020). Our results support
these insights from a different angle. It is surprising (and very encouraging) that the improvement
by post-Lasso thresholding is so significant. We note that Lasso is a 1-stage method, which solves
a single optimization to obtain β. By comparison, thresholded Lasso is a 2-stage method. Lasso
has only one algorithm parameter λ, while thresholded Lasso has two algorithm parameters λ and
t (the threshold). In Lasso, we control false positives and false negatives with the same algorithm
parameter λ, and it is sometimes hard to find a value of λ that simultaneously controls the two types
of errors well. In thresholded Lasso, the two types of errors can be controlled separately by two
algorithm parameters. This explains why thresholded Lasso enjoys such a big improvement upon
Lasso. It inspires us to modify other 1-stage methods, such as SCAD, by adding a post-processing
step of thresholding. For example, we conjecture that thresholded SCAD also has a strictly better
phase diagram than that of Lasso, even for a positive ρ. On the other hand, thresholding is no free
lunch. It leaves one more tuning parameter to be decided in practice. Our theoretical results are based
on ideal tuning parameters. How to properly choose these tuning parameters in a data-driven way
is an interesting question. Weinstein et al. (2020) proposes a promising approach, where they use
cross-validation to select λ and FDR control by knockoff to select t. We leave it for future work to
study the phase diagrams with data-driven tuning parameters.
Discussion of the two iterative algorithms. We consider two iterative algorithms, forward selection
(‘Forward’) and forward backward selection (‘FB’). The FB algorithm we analyze is a simplified
version in Huang et al. (2016), which has only one backward step (after all the forward steps have
finished) by thresholding the refitted least-squares solution. Our results show that both methods
outperform Lasso, and between these two methods, FB is strictly better than Forward. In the literature,
there are very interesting theoretical works showing the advantages of iterative algorithms for variable
selection (Donoho et al., 2012; Zhang, 2011). Our results support their insights from a different angle.
We discover that, for a wide range of ρ, FB has the best phase diagram among all the six methods.
This is a very encouraging result. Of course, it is as important to note that the performance of an
iterative algorithm tends to be more sensitive to the form of the design, due to its sequential nature.
3	Main Results
Consider model (1), (3), and (4), where We set σ2 = 1 without loss of generality. Let E[H(β, β)] be
the expected Hamming error, where the expectation is with respect to the randomness of β and z . Let
Lp denote a generic multi-log(p) term such that Lpp → ∞ and Lpp- → 0 for any > 0.
Theorem 1. Under Models (1), (3), and (4), for each of the methods considered in this paper
(Lasso, Elastic net, SCAD, thresholded Lasso, forward selection, forward backward selection, as
well as marginal regression in Section 3.1), there exists afunction h(% r, P) such that E[H(β, β)]=
LpP1-h(",r,ρ). The explicit expressions of h(£, r, P), which may depend on the tuning parameters of
a method, are given in Theorems B.1, C.1, D.1-D.3, F.1, G.1, H.1-H.4 of the supplement.
In the main article, to save space, we only present the expressions of the upper phase curve U(H)=
U (H; ρ) and the lower phase curve L(Iff) = L(H; ρ) for each method, which are defined as follows:
U(H; P) = inf {r > 0 : h(H, r, P) > 1},	L(H; P) = inf {r > 0 : h(H, r, P) > H}.	(5)
4
Published as a conference paper at ICLR 2022
16
p = 0.5
14
12
6
4
2
10
r
8
0.2	0.4	0.6	0.8	1.0
(9
12
8o
p = -0.2
10
8
6
4
2
56
4S
40
^O2	04	06	08 LO ¾.0
(9
8o-
64
μ→∞ (Marg. Reg.)
μ = 2
匚二]μ = 0.5
I I μ = 0 (Lasso)
P = - 0.4
32
24
16
8
Figure 2: The phase diagrams of Elastic net and its comparison with Lasso (notation: η = ρ∕(1+ μ)).
These two curves describe the phase diagram: The upper phase curve U (H) separates the ER region
and AFR region, and the lower phase curve L(H) separates the AFR region and NR region.
3.1	Elastic net and Lasso
The Elastic net (Zou & Hastie, 2005) is a method that estimates β by
βEN = argminβ{∣∣y - Xβ『/2 + λ∣∣β∣k + (μ∕2)kβ『卜
(6)
Compared with Lasso, it adds an additional L2-penalty to the objective function. Below, we fix μ > 0
and re-parametrize λ =，2q log(p), for some constant q > 0. The choice of q affects the exponent,
1 - h(H, r, ρ), in the expression of E[H(β, β)]. We choose the ideal q that minimizes 1 - h(H, r, ρ).
The next theorem is proved in the supplement.
Theorem 2 (Elastic Net). Under Models (1), (3), and (4), let βEN be the Elastic net estimator in (6).
Fix μ and write η = ρ∕(1 + μ). Let λ =，2q log(p) with an ideal choice of q that minimizes the
exponent of E[H(β, β )]. The phase curves are given by L(H) = H, and
U(H) = max
max
{h1(H),h2(H)},
when ρ ≥ 0,
{h1(H), h2(H),h3(H),h4(H)} , when ρ < 0,
where h1(H) = (1 + √1 - H)2, h2 (H) = (1-p| +，1+-|-2ρη )2(1 - H), h3(H) = (i-1ρ∣)2 (1 +
√++-pη √τ-lH )2 ,and h4(H) = (⅜⅜¾+ (√T-^ + ⅛S √T-2h )2.
Lasso is a special case with μ = 0. By setting μ = 0 (equivalently, η = ρ) in Theorem 2, we obtain
the phase curves for Lasso. They agree with the results in Ji & Jin (2012) (but Ji & Jin (2012) does
not cover Elastic net).
To see the effect of the L2-penalty, we consider an extreme case where μ → ∞. Some elementary
algebra shows that (1 + μ)βEN converges to the soft-thresholding of X0y at the threshold λ. In other
words, as μ → ∞, Elastic net converges to marginal screening (i.e., select variables by thresholding
the marginal regression coefficients). At the same time, when μ = 0, (1 + μ)βEN equals the Lasso
estimate. Hence, Elastic net serves as a bridge between Lasso and marginal regression. In the setting
here, the phase diagram of marginal regression is inferior to that of Lasso, and so the phase diagram
of Elastic net is also inferior to that of Lasso. See the proposition below and Figure 2:
Proposition 1. In Theorem 2, for each fixed H ∈ (0, 1), as μ → 0, U(H) is monotone decreasing
and converges to ULasso(H), which is the upper phase curve of Lasso; as μ → ∞, U(H) is mono-
tone increasing and converges to UMR(H), which is the upper phase curve of marginal regression.
Furthermore, when P ≤ 一2, Umr(Θ) = ∞ for all 0 < H ≤ 2 (i.e. exact recovery is impossible to
achieve no matter how large r is).
3.2	Smoothly clipped absolute deviation penalty (SCAD)
SCAD (Fan & Li, 2001) is a non-convex penalization method. For any a > 2, it defines a penalty
function qλ(θ) on (0, ∞) by qλ(θ) = Rθ qλ(t)dt, where q](θ) = λ{l(θ ≤ λ) + 宵-队 I(θ > λ)}.
5
Published as a conference paper at ICLR 2022
Figure 3: Left: Phase curves of SCAD. Middle and Right: Comparison of SCAD and Lasso.
The resulting penalty function qλ(∙) coincides with the L1 -penalty in (0, λ] and becomes a constant
in [aλ, ∞). Let Qλ(β) = Pj=I qλ(∖βj |). Then, SCAD estimates β by
βSCAD = ammine{∣∣y - χβk2∕2 + Qλ(β)}.	(7)
The following theorem is proved in the supplemental material (see Figure 3, left panel):
Theorem 3 (SCAD). Under Models (1), (3), and (4),let βsCAD be the SCAD estimator in (7). FiX
a ∈ (2, ι-2∣ρ∣). Let λ =，2q log(p) with an ideal choice of q that minimizes the rates Ofconvergence
of the expected Hamming error. The phase curves are given by L(H) = H, and
U (H)
max{h1(H), h2(H),h3(H)} ,
max{h1(H),h2(H),h4(H),h5(H)},
when ρ ≥ 0,
when ρ < 0,
where h1 (H) = (I + √1 - H )2, and h2 (H) = (1	+	JI-IP )	(I - H),	h4 (H)=	( J 11-p3	+	ι-∣ρ∣ ),
h 〈Hi —( 3-p q 1-p√ H ι /2(1-2∙)	a-.))2 d
h3 (H) = 2(1-ρ2) V 1-ρV 1 - H + 2 V ^+P-(Γ-P)2	,	and
1” )(1 -H),	if /W ≥ 中* /S⅛,
h5(H) = I
(i-1ρ∣)2 (尸√1-h+尸√f)2，	if JW ≤ T⅛≡,
[he(H)	other wise，
with
h6(H)
1≡⅛ 尸+√iW+
l-ɪ](1 _H) - 1-2.
1+p」(I H)	(1+∣P∣)2
(1Tρ∣)[( U )2 + E
2
Note that the phase curves of Lasso are given in Theorem 2 by setting η = ρ. We compare SCAD
with Lasso. When ρ < 0, the upper phase curve in Theorem 3 is strictly lower than that of Lasso (see
Figure 3, middle and right panels). When ρ ≥ 0, the upper phase curve in Theorem 3 is sometimes
higher than that of Lasso. Note that we restrict a < 1⅛ in Theorem 3. In fact, a larger a may be
preferred for ρ ≥ 0. The next proposition is about using an optimal a.
Proposition 2. In the SCAD estimator, we choose a = a* and λ =，2q* log(p) such that (a*, q*)=
(a* (H, r, ρ), q* (H, r, P)) minimize the rates ofconvergence of the expected Hamming error among all
choices of (a, q). Let U* (H) be the resulting upper phase curve for SCAD. Then, U(H) = ULasso(H)
when P ≥ 0, and U(H) < ULasso(H) when P < 0.
The phase curves of SCAD are insensitive to the choice of a. When a < 0, the optimal a* can be any
value in (2, ɪ-/). When P ≥ 0, there exists a constant C = c(H, ρ) such that the optimal a* is any
value in (c, ∞). As a → ∞, the SCAD penalty reduces to the L1-penalty. This explains why the
phase curve of SCAD is the same as that of Lasso when P ≥ 0.
6
Published as a conference paper at ICLR 2022
Figure 4: Comparison of the phase diagrams of thresholded Lasso and Lasso.
3.3	Thresholded Lasso
Let βLasso be the Lasso estimator in (2). The thresholded Lasso estimator §TL is obtained by applying
coordinate-wise hard-thresholding to the Lasso estimator:
βTL = βLasso ∙ ι{∣βLasso∣ >t},	1 ≤ j ≤ p.	(8)
Theorem 4 (Thresholded Lasso). UnderModels (1), (3), and (4), let βτL be the thresholded Lasso
estimator in (8). Let λ =，2q log(p) and t =，2W log(p) with the ideal (q,w) that minimize the
exponent ofthe expected Hamming error The phase curves are given by L(H) = H, and
U (H)
max{h1(H), h2(H)} ,
max{h1(H),h2(H),h3(H)} ,
when ρ ≥ 0,
when ρ < 0,
where hi(H)=(I+√1 -H)2, h2(H) = 4(-ρ2), and h3(H) = a+ι+ιpι q 1-p2+1 j qi-pt)
See Figure 4 for a comparison with Lasso (a special case oft = 0). With the flexibility of using an
optimal t, the phase diagram of thresholded Lasso is always better than that of Lasso.
Theorem 4 also gives other interesting facts about thresholded Lasso. First, the shape of phase curves
is much less affected by the sign of ρ. This differs from Lasso, Elastic net, and SCAD, for which the
shape of phase curves is significantly different for positive and negative ρ. Second, the optimal λ in
thresholded Lasso is considerably smaller than the optimal λ in Lasso (it can be seen from the proofs
of Theorem 4 and Theorem 2). This is because the λ in thresholded Lasso only serves to control false
negatives, but the λ in Lasso is used to simultaneously control false positives and false negatives,
hence, cannot be too small. We observe the same phenomenon in simulations; see Section 4.
3.4	Forward selection and forward backward selection
Forward selection is a classical textbook method for variable selection. Write X = [xi, x2, . . . , xp],
where xi ∈ Rn for 1 ≤ i ≤ p. For any subset A ⊂ {1, 2, . . . ,p}, let PA⊥ be the projection onto the
orthogonal complement of the linear space spanned by {xi : i ∈ A}. Given a threshold t > 0, the
forward selection algorithm initializes with S° = 0 and ro = y. At the kth iteration, compute
i* = argmaχi∈Sk-ι lxirk-11,	δ = |xi*rk-1|/kPSk-Ixi*k.
If δ > t, compute Sk = Sk-ι ∪ {i*} and rk = P⊥y; otherwise, output βfοrward as the least-squares
estimator restricted to Sk-i. The stopping rule of δ ≤ t is equivalent to measuring the decrease of
the residual sum of squares. The following theorem is proved in the supplemental material:
Theorem 5 (Forward Selection). UnderModels (1), (3), and (4), let yβforward be the estimatorfrom
forward selection. Let t =，2q log(p) with the ideal q that minimizes the exponent ofthe expected
Hamming error. The phase curves are given by L(H) = H, and
U(H)
max{hi(H), h2(H), h3(H)} ,
max{hi(H), h2(H), h3(H), h4(H)} ,
when ρ ≥ 0,
when ρ < 0,
with hi(H) = (1 + √TR)2, h2(H)=翠-第 h3(H) = *1*2, h4(H) = (J⅞≡着 + ⅛)2.
7
Published as a conference paper at ICLR 2022
P = 0.7
Figure 5: The phase diagrams of forward selection and forward backward selection.
Forward backward selection (FB) modifies forward selection by allowing to drop variables. We use
the FB algorithm in Huang et al. (2016), where the backward step is conducted after all the forward
steps are finished. For a threshold v > 0, it applies entry-wise thresholding on βforward:
βFB = βjorward ∙ 1{∣βjorward∣ >v},	1 ≤ j ≤ p.	(9)
Theorem 6 (Forward Backward Selection). Under Models (1), (3), and (4) ,let βFB be the estimator
from forward selection. Let t =，2q log(p) and V =，2u log(p) with the ideal (q, U) that minimize
the exponent ofthe expected Hamming error When P ≥ 0, the phase curves are given by L(H) = H,
and
U(H) = max {hι(H), h2(H),他(H)},
where hι(H) and h2(H) are the same as in Theorem 5 and hɜ (H) = (V'1-"+'1-2刃. When ρ < 0,
U(H) ≤ max{g1(H),g2(H),g3(H),g4(H)} ,
where g1(H) = (Vmin(H) + √1 - H)2, g2(H)=笔记, g3 (H) = (J 1-22 + Vmin
(q 1-2"~ + tmin⑻)2 V . (H) = max,1 ' I 1-1 1 and t . (H) — max, √2 一
∖,(J 2(1-∣ρ∣) + 1-∣ρ∣ , Vmm(H) = max[1, V 1-ρ2 ʃ, and tmm(H) = max[ 2 ,
(H)2,g4(H)
Vmin(")	1
1-ρ2
Theorem 6 gives U(H) for P ≥ 0 and an upper bound of it for P < 0. Combining it with Theorems 2
and 5, we conclude that the upper phase curve of FB is always better than those of Lasso and forward
selection (for P < 0, the upper bound here is already better than U(H) for the other two methods).
We remark that we did obtain the exact phase curve for P < 0 in the proof of Theorem 6. It is just too
complicated and space-consuming to present it in the main text. However, given specific values of
(H, P), we can always plot the exact phase curve using the (complicated) formulas in the supplement.
In Figures 1 and 5, the phase curves of FB are indeed the exact ones.
3.5	Connection to the random design model
Consider the random design as mentioned in Section 1. The minimax Hamming error is H* (H, r, ρ)=
infβ E[H(β,β)], where the infimum is taken over all methods β and the expectation is with respect
to the randomness of (X, β, z). We can define H*(H, r, ρ) in the same way for our current model
(4). The minimax Hamming error is related to the statistical limit of the model setting, but not any
specific method. The next theorem shows that, when n sp = p1-" (we allow both p ≤ n and
p > n), the convergence rate of the minimax Hamming error is the same under two models.
Theorem 7. Under Models (1) and (3), suppose X is independent of (β, z) and its rows are iid
generated from N(0, n-1Σ), with Σ having the same form as G in (4). Suppose n = pω, with
ω > 1 - H (note: this allows ω < 1, which corresponds to n p). There exists a number h** (H, r, ρ)
such that the minimax Hamming error satisfies that H*(H, r, ρ) = Lpp1-h**(",r,P). Furthermore, if
we instead have X0X = Σ (i.e., model (4)), then it also holds that H*(H, r, ρ) = Lpp1~h'^^((",r,ρ). 4
4 Simulations
In Experiments 1-3, (n, p) = (1000, 300). In Experiment 4, (n, p) = (500, 1000).
8
Published as a conference paper at ICLR 2022
Experiment 1 (block-wise diagonal designs). We generate (X, β) as in (3)-(4). For each method,
we select the ideal tuning parameters that minimize the average Hamming error over 50 repetitions.
The averaged Hamming errors and its standard deviations under the ideal tuning parameters over 500
repetitions are reported below. The results are consistent with the theoretical phase diagrams (see
Figure 1). E.g., thresholded Lasso and forward backward selection are the two methods that perform
the best; Lasso is more unsatisfactory when ρ < 0; SCAD improves Lasso when ρ < 0.
P	犷	r
0.5	0.1	1.5
0.5	0.1	4
-0.5	0.1	1.5
Lasso	ThresLasso	ElastiCNet	SCAD	Forward	FoBaCkWard
11.57 (3.59)~10.48 (3.34)~11.57 (3.31)~11.72 (3.33)~14.88(4.12)~13.35 (3.90)
1.00 (1.00)	0.42 (0.65)	1.03 (1.00)	1.00	(0.96)	0.66 (0.84)	0.51 (0.73)
35.62 (5.09)	15.62 (4.06)	35.48 (5.64)	25.87	(5.04)	19.48 (4.61)	14.82 (3.82)
Table 1:	Experiment 1 (bloCk-diagonal designs). (n, p) = (1000, 300).
Experiment 2 (general designs). In the Toeplitz design, we let (X0X)i,j = 0.7|i-j| and set
(色 r) = (0.1,2.5). In the factor model design, we let X0X = BB0 - diag(BB0) + Ip, where entries
of B ∈ Rp×2 are iid from Unif(0,0.6), and set (∂, r) = (0.1,1.5). Same as in Experiment 1, we use
the ideal tuning parameters. The averaged Hamming errors and its standard deviations are reported
below. The Toeplitz design is a setting where eaCh variable is only highly Correlated with a few other
variables. The faCtor model design is a setting where a variable is (weakly) Correlated with all the
other variables. The results are quite similar to those in Experiment 1. This Confirms that the insight
gained in the study of the bloCk-wise diagonal design Continues to apply to more general designs.
design
Toeplitz
Factor model
Lasso ThresLasso ElasticNet SCAD Forward FoBackward
47.15 (6.32)~22.02 (5.31)~47.40 (6.41)~24.61 (5.70)~30.77 (6.18)~22.93 (5.44)
21.14 (4.52)	15.90 (3.87)	21.20 (4.45)	19.68 (4.23)	20.04 (4.34)	16.13 (3.76)
Table 2:	Experiment 2 (general designs). (n, p) = (1000, 300).
Experiment 3 (tuning parameters). Fix (ð,r) = (0.1,1.5) and P ∈ {±0.5} in the block-wise
diagonal design. We study the effect of tuning parameters in Lasso, thresholded Lasso (ThreshLasso),
forward selection (ForwardSelect), and forward backward selection (FB). In (a)-(b), we show the
heatmap of averaged Hamming error (over 50 repetitions) of ThreshLasso for a grid of (t, λ); when
t = 0, it reduces to Lasso. In (c)-(d), we show the Hamming error of FB for a grid of (v, t); when
v = 0, it reduces to ForwardSelect. Cyan points are theoretically optimal tuning parameters (formulas
are in proofs of theorems). Red points are empirically optimal tuning parameters that minimize the
averaged Hamming error. The theoretical tuning parameter values are quite close to the empirically
optimal ones. Moreover, the optimal λ in ThreshLasso is smaller than the optimal λ in Lasso.
(a) Lasso and ThreshLasso
(ρ = 0.5)
160
140
120
IOO
80
60
40
20
(b) Lasso and ThreshLasso
(ρ = -0.5)
120
100
80
60
40
20
(c) ForwardSelect and FB
(ρ = 0.5)
120
100
80
60
40
20
(d) ForwardSelect and FB
(ρ = -0.5)
Figure 6: Experiment 3 (effects of tuning parameters). In all plots, cyan points are computed from the
formulas in our theory, and red points are the empirically best tuning parameters (they minimize the
average Hamming error over 500 repetitions). In (a)-(b), the cyan/red points with t = 0 correspond
to Lasso, and the other two are for thresholded Lasso. In (c)-(d), the cyan/red points with t = 0
correspond to forward selection, and the other two are for forward backward selection.
Experiment 4 (p > n and random designs). Fix (n,p, 8, r) = (500,1000,0.5,1.5). We simulate
data from the random design setting in Theorem 7. We study the average Hamming error over 500
repetitions (tuning parameters are set in the same way as in Experiment 1). See Table 3. We have
some similar observations as before: e.g., ThreshLasso and FoBackward are still the best two,
9
Published as a conference paper at ICLR 2022
References
Ery Arias-Castro, Emmanuel J Candis, and Yaniv Plan. Global testing under sparse alternatives:
ANOVA, multiple comparisons and the higher criticism. The Annals of Statistics, pp. 2533-2556,
2011.
Peter J Bickel, Ya’acov Ritov, and Alexandre B Tsybakov. Simultaneous analysis of lasso and dantzig
selector. The Annals of Statistics, 37(4):1705-1732, 2009.
Alia Dehman, Christophe Ambroise, and Pierre Neuvial. Performance of a blockwise approach in
variable selection using linkage disequilibrium information. BMC Bioinformatics, 16(1):1-14,
2015.
David Donoho and Jiashun Jin. Higher criticism for detecting sparse heterogeneous mixtures. The
Annals of Statistics, 32(3):962-994, 2004.
David L Donoho. Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289-1306,
2006.
David L Donoho, Yaakov Tsaig, Iddo Drori, and Jean-Luc Starck. Sparse solution of underdetermined
systems of linear equations by stagewise orthogonal matching pursuit. IEEE Transactions on
Information Theory, 58(2):1094-1121, 2012.
Bradley Efron. Large-scale simultaneous hypothesis testing: the choice of a null hypothesis. Journal
of the American Statistical Association, 99(465):96-104, 2004.
Jianqing Fan and Runze Li. Variable selection via nonconcave penalized likelihood and its oracle
properties. Journal of the American Statistical Association, 96(456):1348-1360, 2001.
Jianqing Fan and Jinchi Lv. A selective overview of variable selection in high dimensional feature
space. Statistica Sinica, 20(1):101, 2010.
Jianqing Fan and Heng Peng. Nonconcave penalized likelihood with a diverging number of parameters.
The Annals of Statistics, 32(3):928-961, 2004.
Jianqing Fan, Yuan Liao, and Xiaofeng Shi. Risks of large portfolios. Journal of Econometrics, 186
(2):367-387, 2015.
Christopher R Genovese, Jiashun Jin, Larry Wasserman, and Zhigang Yao. A comparison of the lasso
and marginal regression. The Journal of Machine Learning Research, 13:2107-2143, 2012.
Shiqiong Huang, Jiashun Jin, and Zhigang Yao. Partial correlation screening for estimating large
precision matrices, with applications to classification. The Annals of Statistics, 44(5):2018-2057,
2016.
Pengsheng Ji and Jiashun Jin. UPS delivers optimal phase diagram in high-dimensional variable
selection. The Annals of Statistics, 40(1):73-103, 2012.
Jiashun Jin. Comment on “Estimating false discovery proportion under arbitrary covariance de-
pendence" by Fan et al. Journal of the American Statistical Association, 107(499):1042-1045,
2012.
Jiashun Jin and Zheng Tracy Ke. Rare and weak effects in large-scale inference: methods and phase
diagrams. Statistica Sinica, pp. 1-34, 2016.
Po-Ling Loh and Martin J Wainwright. Support recovery without incoherence: A case for nonconvex
regularization. The Annals of Statistics, 45(6):2455-2482, 2017.
Nicolai Meinshausen and Peter Buhlmann. Stability selection. Journal of the Royal Statistical
Society: Series B (Statistical Methodology), 72(4):417-473, 2010.
ρ	g	r	Lasso	ThresLasso	ElasticNet	SCAD	Forward	FoBackward
0.5	0.5	1.5	16.02 (5.52)	9.83 (4.08)	13.92 (5.12)	15.98 (6.28)	11.74 (5.55)	9.84 (4.93)
-0.5	0.5	1.5	18.49 (6.03)	10.50 (4.23)	15.18 (5.64)	18.12 (6.00)	12.00 (5.67)	10.41 (5.03)
Table 3: Experiment 4 (p > n and random designs).
10
Published as a conference paper at ICLR 2022
Xiaotong Shen, Wei Pan, and Yunzhang Zhu. Likelihood-based selection and sharp parameter
estimation. Journal ofthe American Statistical Association,107(497):223-232, 20l2.
Wenguang Sun and T Tony Cai. Oracle and adaptive compound decision rules for false discovery
rate control. Journal of the American Statistical Association, 102(479):901-912, 2007.
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society: Series B (Statistical Methodology), 58(1):267-288, 1996.
Sara van de Geer, Peter Buhlmann, and ShUheng Zhou. The adaptive and the thresholded lasso for
potentially misspecified models (and a lower bound for the lasso). Electronic Journal of Statistics,
5:688-749, 2011.
Martin J Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using
L1-constrained quadratic programming (lasso). IEEE Transactions on Information Theory, 55(5):
2183-2202, 2009.
Shuaiwen Wang, Haolei Weng, and Arian Maleki. Which bridge estimator is optimal for variable
selection? The Annals of Statistics, 48(5), 2020.
Asaf Weinstein, Weijie Su, MaIgOrzata Bogdan, Rina F Barber, and Emmanuel J Candis. A
power analysis for knockoffs with the lasso coefficient-difference statistic. arXiv preprint
arXiv:2007.15346, 2020.
Cun-Hui Zhang. Nearly unbiased variable selection under minimax concave penalty. The Annals of
Statistics, 38(2):894-942, 2010.
Tong Zhang. Adaptive forward-backward greedy algorithm for learning sparse representations. IEEE
Transactions on Information Theory, 57(7):4689-4708, 2011.
Peng Zhao and Bin Yu. On model selection consistency of lasso. The Journal of Machine Learning
Research, 7:2541-2563, 2006.
Shuheng Zhou. Thresholding procedures for high dimensional variable selection and statistical
estimation. Advances in Neural Information Processing Systems, 22:2304-2312, 2009.
Hui Zou. The adaptive lasso and its oracle properties. Journal of the American Statistical Association,
101(476):1418-1429, 2006.
Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal of the
Royal Statistical Society: Series B (Statistical Methodology), 67(2):301-320, 2005.
11