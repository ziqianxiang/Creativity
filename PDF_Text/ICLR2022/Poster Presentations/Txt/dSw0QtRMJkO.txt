Published as a conference paper at ICLR 2022
High Probability Bounds for a Class of Non-
convex Algorithms with AdaGrad Stepsize
Ali Kavis
EPFL (LIONS)
ali.kavis
@epfl.ch
Kfir Y. Levy *
Technion
kfirylevy
@technion.ac.il
Volkan Cevher
EPFL (LIONS)
volkan.cevher
@epfl.ch
Ab stract
In this paper, we propose a new, simplified high probability analysis of AdaGrad
for smooth, non-convex problems. More specifically, we focus on a particular
accelerated gradient (AGD) template (Lan, 2020), through which we recover
the original AdaGrad and its variant with averaging, and prove a convergence
rate of O(1∕√T) with high probability without the knowledge of smoothness
and variance. We use a particular version of Freedman’s concentration bound
for martingale difference sequences (Kakade & Tewari, 2008) which enables us
to achieve the best-known dependence of log(1∕δ) on the probability margin δ.
We present our analysis in a modular way and obtain a complementary O(1/T)
convergence rate in the deterministic setting. To the best of our knowledge, this
is the first high probability result for AdaGrad with a truly adaptive scheme, i.e.,
completely oblivious to the knowledge of smoothness and uniform variance bound,
which simultaneously has best-known dependence of log(1∕δ). We further prove
noise adaptation property of AdaGrad under additional noise assumptions.
1	Introduction
Adaptive gradient methods are a staple of machine learning (ML) in solving core problems such as
min f (x) := Ez~d [f (x; z)],	(P)
x∈Rd
where the objective f(x) is possibly non-convex, and D is a probability distribution from which
the random vector z is drawn. Problem (P) captures for instance empirical risk minimization or
finite-sum minimization (Shalev-Shwartz & Ben-David, 2014) problems, where z represents the
mini-batches and D corresponds to the distribution governing the data or its sampling strategy.
Within the context of large-scale problems, including streaming data, computing full gradients is
extremely costly, if not impossible. Hence, stochastic iterative methods are the main optimizer
choice in these scenarios. The so-called adaptive methods such as AdaGrad (Duchi et al., 2011),
Adam (Kingma & Ba, 2014) and AmsGrad (Reddi et al., 2018) have witnessed a surge of interest
both theoretically and practically due to their off-the-shelf performance. For instance, adaptive
optimization methods are known to show superior performance in various learning tasks such as
machine translation (Zhang et al., 2020; Vaswani et al., 2017).
From a theoretical point of view, existing literature provides a quite comprehensive understanding
regarding the expected behaviour of existing adaptive learning methods. Nevertheless, these results
do not capture the behaviour of adaptive methods within a single/few runs, which is related to the
probabilistic nature of these methods. While there exists high probability analysis of vanilla SGD for
non-convex problems (Ghadimi & Lan, 2013), adaptive methods have received limited attention in
this context.
Our main goal in this paper is to understand the probabilistic convergence properties of adaptive
algorithms, specifically AdaGrad, while focusing on their problem parameter adaptation capabilities
in the non-convex setting. In this manuscript, adaptivity refers to the ability of an algorithm to ensure
* A Viterbi fellow
1
Published as a conference paper at ICLR 2022
convergence without requiring the knowledge of quantities such as smoothness modulus or variance
of noise. Studies along this direction largely exist for the convex objectives; for instance, Levy et al.
(2018) shows that AdaGrad can (implicitly) exploit smoothness and adapt to the magnitude of noise
in the gradients when f(x) is convex in (P).
This alternative perspective to adaptivity is crucial because most existing analysis, both for classical
and adaptive methods, assume to have access to smoothness constant, bound on gradients (Reddi et al.,
2018) and even noise variance (Ghadimi & Lan, 2013). In practice, it is difficult, if not impossible,
to compute or even estimate such quantities. For this purpose, in the setting of (P) we study a class
of adaptive gradient methods that enable us to handle noisy gradient feedback without requiring the
knowledge of the objective’s smoothness modulus, noise variance or a bound on gradient norms.
We summarize our contributions as follows:
1.	We provide a modular, simple high probability analysis for AdaGrad-type adaptive methods.
2.	We present the first optimal high probability convergence result of the original AdaGrad
algorithm for non-convex smooth problems. Concretely,
(a) we analyze a fully adaptive step-size, oblivious to Lipschitz constant and noise variance,
(b) We obtain the best known dependence of log(1∕δ) on the probability margin δ.
(C) we show that under SUb-GaUSSian noise model, AdaGrad adapts to noise level with
high probability, i.e, as variance σ → 0, convergence rate improves, 1∕√T → 1/T.
3.	We present new extensions of AdaGrad that inclUde averaging and momentUm primitives,
and prove similar high probability boUnds for these methods, as well. Concretely, we stUdy
a general adaptive template which individUally recovers AdaGrad, AdaGrad with averaging
and adaptive RSAG (Ghadimi & Lan, 2016) for different parameter choices.
In the next section, we will provide a broad overview of related work with an emphasis on the recent
developments. Section 3 formalizes the problem setting and states oUr blanket assUmptions. Section 4
introdUces the bUilding blocks of oUr proposed proof techniqUe while proving convergence resUlts
for AdaGrad. We generalize the convergence resUlts of AdaGrad for a class of nonconvex, adaptive
algorithms in Section 5. Finally, we present conclUding remarks in the last section.
2	Related Work
Adaptive methods for stochastic optimization As an extended version of the online (projected)
GD (Zinkevich, 2003), AdaGrad (DUchi et al., 2011) is the pioneering work behind most of the
contemporary adaptive optimization algorithms Adam, AmsGrad and RmsProp (Tieleman & Hinton,
2012) to name a few. Simply pUt, sUch AdaGrad-type methods compUte step-sizes on-the-fly by
accUmUlating gradient information and achieve adaptive regret boUnds as a fUnction of gradient
history (see also (Tran & Phong, 2019; AlacaoglU et al., 2020b; LUo et al., 2019; HUang et al., 2019)).
Universality, adaptive methods and acceleration We call an algorithm universal if it achieves
optimal rates under different settings, without any modifications. For convex minimization problems,
Levy et al. (2018) showed that AdaGrad attains arate of O(1∕T + σ∕√T) by implicitly adapting to
smoothness and noise levels; here T is the number of oracle queries and σ is the noise variance. They
also proposed an accelerated AdaGrad variant with scalar step-size. The latter result was extended
for compactly constrained problems via accelerated Mirror-Prox algorithm (Kavis et al., 2019), and
for composite objectives (Joulani et al., 2020). Recently, Ene et al. (2021) have further generalized
the latter results by designing a novel adaptive, accelerated algorithm with per-coordinate step-sizes.
Convergence properties of such algorithms under smooth, non-convex losses are unknown to date.
Adaptive methods for nonconvex optimization Following the popularity of neural networks,
adaptive methods have attracted massive attention due to their favorable performance in training
and their ease of tuning. The literature is quite vast, which is impossible to cover exhaustively here.
Within the representative subset (Chen et al., 2019; Zaheer et al., 2018; Li & Orabona, 2019; Zou
et al., 2019; Defossez et al., 2020; Alacaoglu et al., 2020a; Chen et al., 2020; Levy et al., 2021). The
majority of the existing results on adaptive methods for nonconvex problems focus on in expectation
performance.
2
Published as a conference paper at ICLR 2022
High probability results Ghadimi & Lan (2013) are the first to analyze SGD in the non-convex
regime and provide tight convergence bounds. Nevertheless, their method requires a prior knowledge
of the smoothness modulus and noise variance. In the context of adaptive methods, Li & Orabona
(2020) considers delayed AdaGrad (with lag-one-behind step-size) for smooth, non-convex losses
under SUbgaUSSian noise and proved O(σ,log(T∕δ)∕√Γ) rate. Under similar conditions, Zhou
et al. (2018) proves convergence of order O((σ2 log(1∕δ))∕T + 1∕√T) for AdaGrad. However,
both works require the knowledge of smoothness to set the step-size. Moreover, Ward et al. (2019)
guarantees that AdaGrad with scalar step-size convergences at O((1∕δ) log(T)/√T) rate with high
probability. Although their framework is oblivious to smoothness constant, their dependence of
probability margin is far from optimal. More recently, under heavy-tailed noise having bounded pth
moment forp ∈ (1, 2), Cutkosky & Mehta (2021) proves a rate of O(log(T ∕δ)∕T (p-1)/(3p-2)) for
clipped normalized SGD with momentum; nevertheless their method requires the knowledge of (a
bound on) the behavior of the heavy tails.
3	Setup and preliminaries
As we stated in the introduction, we consider the unconstrained minimization setting
min f (x) := Ez〜D [f (x; z)],
x∈Rd
where the differentiable function f : Rd → R is a smooth and (possibly) nonconvex function.
We are interested in finding a first-order e-stationary point satisfying ∣∣Vf (Xt)II2 ≤ e, where ∣∣∙k de-
notes the Euclidean norm for the sake of simplicity. As the standard measure of convergence, we will
quantify the performance of algorithms with respect to average gradient norm, 1 PT=IllVf (Xt)Il2.
It immediately implies convergence in minimum gradient norm, mint∈[T] ∣Vf(xt)∣2. Moreover,
note that if we are able to bound T PT=I ∣∣Vf (Xt)II2, then by choosing to output a solution XT
which is chosen uniformly at random from the set of query points {X1, . . . , XT}, then we ensure that
EkVf(XT)∣2 ：= τ PT=I ∣∣Vf (xt)k2 is bounded.
A function is called G-Lipschitz continuous if it satisfies
|f (X) - f(y)| ≤ G∣X - y∣, ∀X,y ∈ dom(f),	(1)
which immediately implies that
∣Vf(X)∣ ≤G, ∀X∈dom(f).	(2)
A differentiable function is called L-smooth if it has L-Lipschitz gradient
∣Vf (X) - Vf(y)∣ ≤ L∣X - y∣, ∀X,y ∈ dom(Vf).	(3)
An equivalent characterization is referred to as the “descent lemma” (Ward et al., 2019; Beck, 2017),
|f (X) - f(y) - hVf(y),X-yi| ≤ (L∕2)∣X-y∣2.	(4)
Assumptions on oracle model: We denote stochastic gradients with Vf (X) = Vf (X; z), for some
random vector drawn from distribution D. Since our template embraces single-call algorithms, we
use this shorthand notation for simplicity. An oracle is called (conditionally) unbiased if
EVe f (X)|X = Vf (X), ∀X ∈ dom(Vf).	(5)
Gradient estimates generated by a first-order oracle are said to have bounded variance if they satisfy
E∣Ve f (X) - Vf (X)∣2 |X ≤ σ2, ∀ ∈ dom(Vf).	(6)
Finally, we assume that the stochastic gradient are bounded almost surely, i.e.,
..~ .... ~
kVf(X)k≤ G, VX ∈ dom(Vf).	(7)
Remark 1. Bounded variance assumption (6) is standard in the analysis of stochastic methods (Lan,
2020). Similarly, for the analysis of adaptive methods in the nonconvex realm, it is very common to
assume bounded stochastic gradients (see (Zaheer et al., 2018; Zhou et al., 2018; Chen et al., 2019;
Li & Orabona, 2020) and references therein).
3
Published as a conference paper at ICLR 2022
4 Proposed analysis and Adagrad
This section introduces our proposed proof technique as well as our main theoretical results for
AdaGrad with proof sketches and discussions on the key elements of our theoretical findings. We will
present a high-level overview of our simplified, modular proof strategy while proving a complementary
convergence result for AdaGrad under deterministic oracles. In the sequel, we refer to the name
AdaGrad as the scalar step-size version (also known as AdaGrad-Norm) as presented in Algorithm 1.
Algorithm 1 AdaGrad
Input: time horizon T , x1 ∈ Rd, step-size {ηt}t∈[T] , G0 > 0
1:	for t = 1, ..., T do
2:	Generate gt = V f (Xt)
3:	ηt = ，	1
t	√G0 + Pk=ι kgkk2
4:	xt+1 = xt - ηtgt
5:	end for
Before moving forward with analysis, let us first establish the notation we will use to simplify
the presentation. In the sequel, we use [T] as a shorthand expression for the set {1, 2, ..., T}.
We will use ∆t = f(xt) - minx∈Rd f(x) as a concise notation for objective suboptimality and
∆max = maxt∈[T +1] ∆t will denote the maximum over ∆t. In the rest of Section 4, we denote
stochastic gradient of f at xt by gt = Vf(xt) = Vf(xt; zt) and its deterministic equivalent as
gt := Vf (xt). We also use the following notation for the noise vector ξt := gt 一 gt.
Notice that AdaGrad (Alg. 1) does not require any prior knowledge regarding the smoothness modulus
nor the noise variance. The main results in this section is Theorem 4.2, where we show that with high
probability AdaGrad obtains an optimal rate O(log(1∕δ)∕√T) for finding an approximate stationary
point. Moreover, Theorem 4.1 shows that in the deterministic case AdaGrad achieves an optimal rate
of O(1/T), thus establishing its universality.
4.1 Technical Lemmas
We make use of a few technical lemmas while proving our main results, which we refer to in our proof
sketches. We present them all at once before the main theorems for completeness. First, Lemmas 4.1
and 4.2 are well-known results from online learning, essential for handling adaptive stepsizes.
Lemma 4.1. Let a1, ..., an be a sequence of non-negative real numbers. Then, it holds that
∖
n
X ai ≤
i=1
n
X
i=1
ai
qpk= ak
un
≤ 2tuX ai
Lemma 4.2. Let a1, ..., an be a sequence of non-negative real numbers. Then, it holds that
nn
XI ≤ 1+log(1+X ai)
The next lemma is the key for achieving the high probability bounds.
Lemma 4.3 (Lemma 3 in Kakade & Tewari (2008)). Let Xt be a martingale difference sequence
such that |Xt | ≤ b. Let us also define
Vart-1(Xt) =Var(Xt | X1,...,Xt-1) =E Xt2 | X1,...,Xt-1 ,
and VT = PtT=1 Vart-1(Xt) as the sum of variances. For δ < 1/e andT ≥ 3, it holds that
P (^X Xt > max n 2 p/VT, 3b,log(1∕δ)} /log(1∕δ)) ≤ 4log(T)δ	(8)
4
Published as a conference paper at ICLR 2022
4.2	Overview of proposed analysis
We will start by presenting the individual steps of our proof and provide insight into its advantages.
In the rest of this section, we solely focus on AdaGrad, however, the same intuition applies to the
more general Algorithm 2 as we will make clear in the sequel. Using the shorthand notations of
gt = Vf (Xt) and gt = Vf (xt； Zt) = Ut + ξt, the classical analysis begins with,
Lη2
f(Xt+1) - f(xt) ≤-ηt∣∣gt∣∣ - ηthgt,ξti+ 2llgtk .
which is due to the smoothness property in Eq. (4). Re-arranging and summing over t ∈ [T] yields,
T	TT
Xntk@k2 ≤ f(χι) - f(χ*) + X -ηthgt,ξti + 2 Xη2kgtk2.
t=1
t=1
t=1
The main issue in this expression is the ηthgt, ξti term, which creates measurability problems due to
the fact hat ηt and ξt are dependent random variables. On the left hand side, the mismatch between
ηt and kgt∣2 prohibits the use of technicals lemmas as we accumulate stochastic gradients for ηt.
Moreover, we cannot make use of Holder-type inequalities as we deal with high probability results.
Instead, we divide both sides by ηt , then sum over t and re-arrange to obtain a bound of the form,
Xkgtk2 ≤ δ+X (ηt- !)
T	LT
∆t + X -hgt,ξti + 2 X ntkgtk2
∆ T	LT
≤ 丁+X-hgt ,ξti+2 X ηtkgtk2
(9)
This modification solves the two aforementioned problems, but we now need to ensure boundedness
of function values, specifically the maximum distance to the optimum, ∆max. In fact, neural networks
with bounded activations (e.g. sigmoid function) in the last layer and some objective functions in
robust non-convex optimization (e.g. Welsch loss (Barron, 2019)) satisfy bounded function values.
However, this is a restrictive assumption to make for general smooth problems and we will prove that
it is bounded or at least it grows no faster than O(log(T)). As a key element of our approach, we
show that it is the case for Alg. 1 & 2. Now, we are at a position to state an overview of our proof:
1.	Show that ∆max ≤ O(log(T)) with high probability or ∆max ≤ O(1) (deterministic).
2.	Prove that PT=I -hgt, ξti ≤ O(√T) with high probability using Lemma 4.3.
3.	Show that LL PT=I ηt∣gtk2 ≤ O(√T) by using Lemma 4.1.
For completeness, we will propose a simple proof for AdaGrad in the deterministic setting. This will
showcase advantages of our approach, while providing some insight into it. We provide a sketch of
the proof, whose full version will be accessible in the appendix.
Theorem 4.1. Let Xt be generated by Algorithm 1 with G0 = 0 for simplicity. Then, it holds that
T X kVf (Xt)k2 ≤ O ( °1 T LL
t=1
ProofSketch (Theorem 4.1). Setting gt = Vf (xt), deterministic counterpart ofEq. (9) becomes
T
X kgtk2 ≤
t=1
LT	uT
肃 + 2Xηtkgtk2 ≤ (∆max + L)tX kgtk2,
where we obtain the final inequality using Lemma 4.1. Now, we show that ∆T+1 is bounded for any
T. Using descent lemma and summing over t ∈ [T],
f(xT +1) - f (x*) ≤ f (xl) - f (x*)+ X (-t^ - 1) ηtkgt k2.
5
Published as a conference paper at ICLR 2022
Now, define t0 = max {t ∈ [T] | ηt > L }, SUCh that (Lnt - 1)≤ 0 for any t > t0. Then,
f (XT+ι) - f (x*) ≤ δi + χ(ʒnt - 1) ηtkgtk2 + X (-ηtr - 1) ηtkgtk2
L t0	L
≤ δi + 2 X% k@k ≤ δi + 2 (1 + log (1 + L /4)),
t=1
where We use the definition of to and Lemma 4.2 for the last inequality. Since this is true for any T,
the bound holds for ∆max, as well. Defining X = JPT=I Ilgtll2, the original expression reduces to
X 2 ≤ (∆max + L) X. Solving for X, plugging in the bound for ∆max and dividing by T results in
1T
T EkVf(Xt)k2 ≤
t=1
(∆1 + L (3 + log(L2∕4)))2
T
□
Remark 2. To our knowledge, the most relevant analysis was provided by Ward et al. (2019), which
achieves O(log(T)∕T) convergence rate. Our new approach enables us to remove log(T) factor.
4.3	High probability convergence under stochastic oracle
Having introduced the building blocks, we will now present the high probability convergence bound
for AdaGrad (Algorithm 1). Let us begin by the departure point of our proof, which is Eq. (9)
X Iigt k2 ≤ —max+X-hgt,ξti+ X X η Iigtk2.
匕	∣ηo t=ι	2 匕
^"{z'
a)	W)	W*)
(10)
We can readily bound expression (* * *) using Lemma 4.1. Hence, what remains is to argue about
high probability bounds for expressions (*) and (**), which we do in the following propositions.
Proposition 4.1. Using Lemma 4.3, with probability 1 - 4 log(T)δ and δ < 1∕e, we have
T
X -hgt,ξti
t=1
≤ 2σ Vzlog(1∕δ)
T
X kgtk2 +3(G2 + GG)log(1∕δ).
t=1
The last ingredient of the analysis is the bound on ∆t . The following proposition ensures a high
probability bound of order O(log(t)) on ∆t under Algorithm 1.
Proposition 4.2. Let Xt be generated by AdaGrad for G0 > 0. With probability at least 1 - 4 log(t)δ,
∆t+ι ≤ ∆ι + 2L(1 + log (max{1, G0} + G2t)) + G0 1(Mι + σ2) log(1∕δ) + M2,
where Mi = 3(G2 + G(G) and M2 = G-1(2G2 + GG).
As an immediate corollary, since the statement of Proposition 4.2 holds for any t, it holds for ∆max
by definition. Hence, we have that maxt∈[T] ∆t = ∆max ≤ O ∆1 + L log(T) + σ 2 log(1∕δ) with
high probability for any time horizon T. In the light of the above results, we are now able to present
our high probability bound for AdaGrad.
Theorem 4.2. Let Xt be the sequence of iterates generated by AdaGrad. Under Assumptions 2, 6, 7,
for ∆max ≤ O (∆ι + L log(T) + σ2 log(1∕δ)), with probability at least 1 — 8log(T)δ,
1T
T Ekgtk2 ≤
t=1
..	.	. C	~ .	.	.	.	,	.	_ .	~.	. .	∕Ξ-；-. ,,
(∆max + L) GO +3(G + GG)log(1∕6 (△ max + L) G + 2Gσplog(V6
T	+	√T
6
Published as a conference paper at ICLR 2022
ProofSketch (Theorem 4.2). Define gt = Pf(Xt) and gt = Vf (xt； Zt) = gt + ξt. By Eq. (10),
XXX kgtk2 ≤ 也 + XxX -hgt,ξti + L XxX ηtkgtk2.
t=1	ηT	t=1	2 t=1
Invoking Lemma 4.1 on the last sum and using Proposition 4.1 for the second expression, we have
with probability at least 1 - 4 log(T )δ
T
X kgtk2 ≤ (∆max + L)
t=1
T
G2 + X kgtk2 + 2σpl0g(1∕δ)
t=1
T
X kgtk2 +3(G2 + GG)log(1∕δ)
t=1
Finally, We use the bounds on the gradient norms ∣∣gtk ≤ G and ∣∣gtk ≤ G, re-arrange the terms and
divide both sides by T. Due to Proposition 4.2, with probability at least 1 - 8 log(T)δ,
1T
T ∑∣gtk2 ≤
t=1
(∆max + L) Go + 3(G2 + GG)log(1∕δ)	(∆max + L) G + 2Gσ/log(1∕δ)
T	+	√T
where ∆max ≤ O (∆ι + L log(T) + σ2 log( ɪ). We keep ∆max in the bound due to lack of space. □
4.4 Noise adaptation under sub-Gaussian noise model
To our knowledge, under the standard setting we consider (unbiased stochastic gradients with bounded
variance), noise adaptation is not achieved for high probability convergence to first-order stationary
points, specifically for AdaGrad-type adaptive methods. We call an algorithm noise adaptive if the
convergence rate improves 1∕√T → 1/T as variance σ → 0. Following the technical results and
approach proposed by Li & Orabona (2020), we will prove that high probability convergence of
AdaGrad (Algorithm 1) exhibits adaptation to noise under sub-Gaussian noise model. First, we will
introduce the additional assumption on the noise. We assume that the tails of the noise behaves as
sub-Gaussian if,
E exp(∣Vf (x, z) - Vf (x)∣2) | x ≤ exp(σ2).	(11)
This last assumption on the noise is more restrictive than standard assumption of bounded variance.
Indeed, Eq. (11) implies bounded variance (Eq. (6)). Finally, we conclude with the main theorems.
We first establish a high probability bound on ∆max and then present noise-adaptive rates for AdaGrad.
Theorem 4.3. Let xt be generated by AdaGrad and define ∆t = f(xt) - minx∈Rd f (x). Under
sub-Gaussian noise assumption as in Eq. (11), with probability at least 1 - 3δ,
△t+1 ≤ A + 3 * 5 * *G-1G2 + 2G-1σ2log S) + 4⅛σ2 log(10
+ E (1 + log (max {1, G2 } + 2G2t + 2σ2t log
Theorem 4.4. Let xt be generated by AdaGrad and define ∆t = f(xt) - minx∈Rd f (x). Under
sub-Gaussian noise assumption as in Eq. (11) and considering high probability boundedness of ∆max
due to Theorem 4.3, with probability at least 1 - 5δ,
1T
τEkutk ≤
t=1
32 (∆ max + L)) + 8 (∆ max + L)(GO + σ p2 log(1/6)) + 8σ2 log(1∕δ)	8√2 (∆ max + L) σ
T	+	√T
Remark 3. By introducing the sub-Gaussian noise model, we manage to achieve a high probability
convergence bound that is adaptive to noise, while removing the dependence on a bound on stochastic
gradients in the final result.
5	Generalization to AGD template
Having proven the high probability convergence for AdaGrad, we will now present an extension
of our analysis to the more general accelerated gradient (AGD) template, which corresponds to a
specific reformulation of Nesterov’s acceleration (Ghadimi & Lan, 2016).
7
Published as a conference paper at ICLR 2022
Algorithm 2 Generic AGD Template
Input: Horizon T , X1 = x1 ∈ Rd, at ∈ (0,1], step-sizes {ηt}t∈[τ], {γt}t∈[τ]
1:	for t = 1, ..., T do
2:	Xt = atxt + (1 - αt)Xt
-. 一 〜，、 =〜，、 一 〜， 、
3：	Set gt = Nf(Xt) or gt = ▽ f(xt) = Vf (xt； Zt)
4:	xt+1 = xt - ηtgt
5：	Xt+1 = Xt - Ytgt
6: end for
This particular reformulation was recently referred to as linear coupling (Allen-Zhu & Orecchia,
2016), which is an intricate combination of mirror descent (MD), gradient descent (GD) and averaging.
In the sequel, we focus on two aspects of the algorithm; averaging parameter αt and selection of
(adaptive) step-sizes ηt and γt . We could recover some well-known algorithms from this generic
scheme depending on parameter choices, which we display in Table 1.
Our reason behind choosing this generic algorithm is two-fold. First, it helps us demonstrate flexibility
of our simple, modular proof technique by extending it to a generalized algorithmic template. Second,
as an integral element of this scheme, we want to study the notion of averaging (equivalently
momentum (Defazio, 2021)), which is an important primitive for machine learning and optimization
problems. For instance, it is necessary for achieving accelerated convergence in convex optimization
(Nesterov, 1983; Kavis et al., 2019), while it helps improve performance in neural network training
and stabilizes the effect of noise (Defazio, 2021; Sutskever et al., 2013; Liu et al., 2020).
Next, we will briefly introduce instances of Algorithm 2, their properties and the corresponding
parameter choices. We identify algorithms regarding the choice of averaging parameter αt and step
sizes ηt and γt. The averaging parameter αt has two possible forms: αt = 2/(t + 1) for weighted
averaging and αt = 1/t for uniform averaging. We take αt = 2/(t + 1) by default in our analysis, as
it is a key element in achieving acceleration in the convex setting. Let us define the AdaGrad step
size once more, which we use to define ηt and γt,
ηet = (G20 + Xtk=1 kgkk2)-1/2,	G0>0.	(12)
The first instance of Algorithm 2 is the AdaGrad, i.e. Xt+ι = Xt - ηtgt. Since Xi = xi by
initialization, We have Xi = Xi = xi. The fact that ηt = Yt = ηtj implies the equivalence Xt = Xt for
any t ∈ [T], which ignores averaging step. The second instance we obtain is AdaGrad with averaging,
Xt = atxt + (1 - αt)Xt-i
Xt+i = Xt - αtηtgt ,
(13)
where ηt = ηt and Yt = 0. For the initialization xi = Xi, we can obtain by induction that Xt = Xt-i,
hence the scheme above. The final scheme We Will analyze is RSAG algorithm proposed by Ghadimi
& Lan (2016). It selects a step size pair that satisfies Yt ≈ (1 + αt)ηet and ηt = ηet, generating a
3-sequence algorithm as in the original form of Algorithm 2.
Table 1: Example methods covered by the generic AGD template.
Algorithm	Weights (at)	Step-size (ηt,γt)
AdaGrad	N/A	~ ηt = ηt,	Yt = ηt	
AdaGrad w/ Averaging	at = t⅛ or t	ηt = at%,	Yt = 0	
Adaptive RSAG(Ghadimi & Lan, 2016)	一	2 at = t+r	ηt = e	Yt = (1 + at)ηt
AcceleGrad(LeVy et al., 2018)	一	2 at =中	ηt ≈ Otηt,	Yt ≈ ηt	
Before moving on to convergence results, we have an interesting observation concerning time-scale
difference between step sizes for AdaGrad, AdaGrad with averaging and RSAG. Precisely, Yt is
always only a constant factor away from ηt . This phenomenon has an immediate connection to
acceleration in the convex realm, we will independently discuss at the end of this section.
8
Published as a conference paper at ICLR 2022
Having defined instances of Algorithm 2, we will present high probability convergence rates for
them. Similar to Eq. (10) for AdaGrad, we first define a departure point of similar structure in
Proposition 5.1, then apply Proposition 4.1 and 4.2 in the same spirit as before to finalize the bound.
Let Us define gt = Vf (Xt) and gt = Vf (Xt； Zt) = Ut + ξt. Following the notation in (Ghadimi &
Lan, 2016), let Γt = (1 - αt)Γt-1 with Γ1 = 1. Now, we can begin with the departure point of the
proof, which is dUe to Ghadimi & Lan (2016).
Proposition 5.1. Let Xt be generated by Algorithm 2 where gt = Vf (XUt; zt) = gUt + ξt and
gUt = Vf (XUt). Then, it holds that
T
XkgUtk2≤
t=1
ηT
∆max + 2L
αt (η--2γtf kgtk2 + XXX-hgt,ξti.
Γt	αt	t=1
}	{z}
(**)
Next, We will deliver the complementary bound on term (*) in Proposition 5.1.
Proposition 5.2. Using the recursive definition of Γ, we have
1 - ɑk)Γk# at ≤ (2	if αt = t+1 ；
_ rt Uog(T + 1) if at = 1.
Finally, we present the high probability convergence rates for AdaGrad with averaging and RSAG.
Theorem 5.1. Let Xt be the sequence generated by AdaGrad with averaging or adaptive RSAG. Under
Assumptions 2, 6, 7, for ∆m办 ≤ O (∆ι + L log(T) + σ2 log(1∕δ)), with probability 1 — 8log(T)δ,
1T
T Ekgtk2 ≤
t=1
..	.	一	C~	.c	~ .	..
Go(∆max + 3L + Llog(max{1, G0 } + G T)) + 3(G + GG)log(1∕δ))
T
+
G(∆max + 3L + Llog(max{1,G0 2} + G2T)) + 2GσPlog(1∕δ)
A discussion on acceleration and nonconvex analysis: AGD and its variants are able to converge
at the fast rate of O(1∕T2 ) (Nesterov, 2003) for smooth, convex objectives, while matching the
convergence rate of GD in the nonconvex landscapes. In fact, the mechanism that allows them to
converge faster could even restrict their performance when convexity assumption is lifted. We will
conclude this section with a brief discussion on this phenomenon.
As we mentioned previously, step-sizes for AdaGrad, its averaging variant and RSAG have the
same time scale up to a constant factor. However, AcceleGrad, an accelerated algorithm, has a
time-scale difference of O(t) between ηt and γt, and it runs with a modified step-size of ηt =
(1 + Ptk=1 ak-2 kgtk2)-1/2. This scale difference is not possible to handle with standard approaches
or our proposed analysis. If we look at second term in Proposition 5.1, it roughly evaluates to
kgtk2,
where each summand is O(t4) orders of magnitude larger compared to other methods. A factor of
at-2 = t2 is absorbed by the modified step-size, but this term still grows faster than we can manage.
We aim to understand it further and improve upon in our future attempts.
2Lτ X α4ι
6	Conclusions
We propose a simple and modular high probability analysis for a class of AdaGrad-type algorithms.
Bringing AdaGrad into the focus, we show that our new analysis techniques goes beyond and
generalizes to the accelerated gradient template (Algorithm 2) which individually recovers AdaGrad,
AdaGrad with averaging and adaptive version of RSAG (Ghadimi & Lan, 2016). By proposing
a modification over standard analysis and relying on concentration bounds for martingales, we
achieve high probability convergence bounds for the aforementioned algorithms without requiring
the knowledge of smoothness L and variance σ while having best-known dependence of log(1∕δ) on
δ. To our knowledge, this is the first such result for adaptive methods, including AdaGrad.
9
Published as a conference paper at ICLR 2022
Acknowledgments
This project has received funding from the European Research Council (ERC) under the European
Union’s Horizon 2020 research and innovation programme (grant agreement no 725594 - time-data)
K.Y. Levy acknowledges support from the Israel Science Foundation (grant No. 447/20).
References
Ahmet Alacaoglu, Yura Malitsky, and Volkan Cevher. Convergence of adaptive algorithms for weakly
convex constrained optimization, 2020a.
Ahmet Alacaoglu, Yura Malitsky, Panayotis Mertikopoulos, and Volkan Cevher. A new regret
analysis for Adam-type algorithms. In Hal Daum III and Aarti Singh (eds.), Proceedings of the
37th International Conference on Machine Learning, volume 119 of Proceedings of Machine
Learning Research, pp. 202-210. PMLR, 13-18 JUl 2020b. URL http://Proceedings.
mlr.press/v119/alacaoglu20b.html.
ZeyUan Allen-ZhU and Lorenzo Orecchia. Linear coUpling: An Ultimate Unification of gradient and
mirror descent, 2016.
Jonathan T. Barron. A general and adaptive robUst loss fUnction. In 2019 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 4326-4334, 2019. doi: 10.1109/CVPR.
2019.00446.
Amir Beck. First-Order Methods in Optimization. SIAM-Society for IndUstrial and Applied
Mathematics, Philadelphia, PA, USA, 2017. ISBN 1611974984.
JinghUi Chen, DongrUo ZhoU, Yiqi Tang, Ziyan Yang, YUan Cao, and QUanqUan GU. Closing the
generalization gap of adaptive gradient methods in training deep neUral networks, 2020.
Xiangyi Chen, Sijia LiU, RUoyU SUn, and Mingyi Hong. On the convergence of a class of adam-type
algorithms for non-convex optimization. In International Conference on Learning Representations,
2019. URL https://openreview.net/forum?id=H1x-x309tm.
Ashok CUtkosky and Harsh Mehta. High-probability boUnds for non-convex stochastic optimization
with heavy tails, 2021.
Aaron Defazio. MomentUm via primal averaging: Theoretical insights and learning rate schedUles
for non-convex optimization, 2021.
Alexandre Defossez, Leon BottoU, Francis Bach, and Nicolas UsUnier. On the convergence of adam
and adagrad, 03 2020.
John DUchi, Elad Hazan, and Yoram Singer. Adaptive sUbgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(JUl):2121-2159, 2011.
Alina Ene, HUy L. NgUyen, and Adrian VladU. Adaptive gradient methods for constrained convex
optimization and variational ineqUalities, 2021.
Saeed Ghadimi and GUanghUi Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic
programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Saeed Ghadimi and GUanghUi Lan. Accelerated gradient methods for nonconvex nonlinear and
stochastic programming. Math. Program., 156(12):5999, March 2016. ISSN 0025-5610. doi: 10.
1007/s10107-015-0871-8. URL https://doi.org/10.1007/s10107-015-0871-8.
Haiwen HUang, Chang Wang, and Bin Dong. Nostalgic adam: Weighting more of the past gradients
when designing the adaptive learning rate. In Proceedings of the Twenty-Eighth International Joint
Conference on Artificial Intelligence, IJCAI-19, pp. 2556-2562. International Joint Conferences
on Artificial Intelligence Organization, 7 2019. doi: 10.24963/ijcai.2019/355. URL https:
//doi.org/10.24963/ijcai.2019/355.
10
Published as a conference paper at ICLR 2022
Pooria Joulani, Anant Raj, Andras Gyorgy, and Csaba Szepesvari. A simpler approach to ac-
celerated optimization: iterative averaging meets optimism. In Hal Daum III and Aarti Singh
(eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of
Proceedings of Machine Learning Research, pp. 4984-4993. PMLR, 13-18 JUl 2020. URL
https://proceedings.mlr.press/v119/joulani20a.html.
Sham M. Kakade and Ambuj TeWarL On the generalization ability of online 泣Stronglyj/i% convex
programming algorithms. In Proceedings of the 21st International Conference on Neural Informa-
tion Processing Systems, NIPS’08, pp. 801808, Red Hook, NY, USA, 2008. Curran Associates Inc.
ISBN 9781605609492.
Ali Kavis, Kfir Y. Levy, Francis Bach, and Volkan Cevher. Unixgrad: A universal, adaptive algorithm
With optimal guarantees for constrained optimization. InH. Wallach, H. Larochelle, A. Beygelzimer,
F. d’Alche Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems
32, pp. 6260-6269. Curran Associates, Inc., 2019.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Guanghui Lan. First-order and Stochastic Optimization Methods for Machine Learning. Springer,
2020.
Kfir Y Levy, Alp Yurtsever, and Volkan Cevher. Online adaptive methods, universality and accelera-
tion. In Neural and Information Processing Systems (NeurIPS), December 2018.
Kfir Yehuda Levy, Ali Kavis, and Volkan Cevher. STORM+: Fully adaptive SGD With recursive
momentum for nonconvex optimization. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman
Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://
openreview.net/forum?id=ytke6qKpxtr.
Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent With
adaptive stepsizes. In Kamalika Chaudhuri and Masashi Sugiyama (eds.), Proceedings of the
Twenty-Second International Conference on Artificial Intelligence and Statistics, volume 89
of Proceedings of Machine Learning Research, pp. 983-992. PMLR, 16-18 Apr 2019. URL
http://proceedings.mlr.press/v89/li19c.html.
Xiaoyu Li and Francesco Orabona. A high probability analysis of adaptive sgd With momentum,
2020.
Yanli Liu, Yuan Gao, and Wotao Yin. An improved analysis of stochastic gradient descent With
momentum, 2020.
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods With dynamic
bound of learning rate, 2019.
H Brendan McMahan and MattheW Streeter. Adaptive bound optimization for online convex opti-
mization. COLT 2010, pp. 244, 2010.
Yurii Nesterov. A method for solving the convex programming problem With convergence rate
o(1/k2). Dokl. Akad. Nauk SSSR, 269:543-547, 1983.
Yurii Nesterov. Introductory lectures on convex optimization. 2004, 2003.
Sashank Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
International Conference on Learning Representations, 2018.
Shai Shalev-ShWartz and Shai Ben-David. Understanding Machine Learning: From Theory to
Algorithms. Cambridge University Press, USA, 2014. ISBN 1107057132.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization
and momentum in deep learning. In Sanjoy Dasgupta and David McAllester (eds.), Proceedings of
the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine
Learning Research, pp. 1139-1147, Atlanta, Georgia, USA, 17-19 Jun 2013. PMLR. URL
https://proceedings.mlr.press/v28/sutskever13.html.
11
Published as a conference paper at ICLR 2022
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2), 2012.
Phuong T. Tran and L. T. Phong. On the convergence proof of amsgrad and a new version. IEEE
ACCeSS,7:61706-61716, 2019.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, L Ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(eds.), AdvanCeS in Neural Information ProCeSSing SyStemS, volume 30. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
3f5ee243547dee91fbd053c1c4a845aa- Paper.pdf.
Rachel Ward, Xiaoxia Wu, and Leon Bottou. AdaGrad stepsizes: Sharp convergence over nonconvex
landscapes. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), ProCeedingS of the 36th
International ConferenCe on MaChine Learning, volume 97 of ProCeedingS of MaChine Learn-
ing ReSearCh, pp. 6677-6686. PMLR, 09-15 Jun 2019. URL http://proceedings.mlr.
press/v97/ward19a.html.
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive
methods for nonconvex optimization. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), AdvanCeS in Neural Information ProCeSSing SyStemS,
volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/
paper/2018/file/90365351ccc7437a1309dc64e4db32a3-Paper.pdf.
Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank J Reddi, Sanjiv
Kumar, and Suvrit Sra. Why are adaptive methods good for attention models?, 2020.
Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu. On the convergence of adaptive
gradient methods for nonconvex optimization. ArXiv, abs/1808.05671, 2018.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In
ProCeedingS of the Twentieth International ConferenCe on International ConferenCe on MaChine
Learning, ICML’03, pp. 928935. AAAI Press, 2003. ISBN 1577351894.
Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, and Wei Liu. A sufficient condition for conver-
gences of adam and rmsprop. In ProCeedingS of the IEEE/CVF ConferenCe on Computer ViSion
and Pattern ReCognition (CVPR), June 2019.
12
Published as a conference paper at ICLR 2022
A Appendix
A.1 Proof of technical lemmas in Section 4.1
Lemma 4.1. Let a1, ..., an be a sequence of non-negative real numbers. Then, it holds that
∖
n
X ai ≤
i=1
n
X
i=1
ai
JPk=I ak
≤2
≤ 2t
n
X ai
i=1
Proof. For the proof of first iniequality, please check Appendix A.4 of Levy et al. (2018), while that
of the second one can be found at Appendix B of McMahan & Streeter (2010), which corresponds to
their Lemma 5.	□
Lemma 4.2. Let a1, ..., an be a sequence of non-negative real numbers. Then, it holds that
nn
X p⅛- ≤ 1+log(1+X ai)
Proof. We will follow the proof steps of Levy et al. (2018) with a slight modification. The proof is
due to induction.
For the base case of n = 1:
a1
a1
1 ≤ 1+log(1+a1)
Assume that the statement holds up to and including n - 1 > 1. Then, for n:
n	n-1	?	n
XP⅛ ≤ 1 + iog(1 + X ai) + Pna ≤ 1 + iog(1 + X ai
We want to show that for any an , the second inequality with the question mark (?) holds. Let us
define x
Pn-I-. Focusing on the second inequality and re arranging the terms We get,
i=1 ai
an
n^n
i=1 ai
≤ log
1 + Pn=1 ai
ι+Pn-IIai
log
1+
an
ι+pn-ι1ai
≤ log
1+
an
n— 1
i=1 ai
Notice that
n-1
an	i=1 ai
an
an
- -------------------
∑n=ι ai	pn—ι1 ai ∑n=ι a	pn—1 ai
--- ----- . ,
Pf愤	Pi=1 ai O+
an
Pn=II a
1
1
1
=X------
1+x
Combining both expressions,
x
ι+χ ≤ Iog(I + X)
Which always holds Whenever X ≥ 0.
□
13
Published as a conference paper at ICLR 2022
Lemma 4.3 (Lemma 3 in Kakade & Tewari (2008)). Let Xt be a martingale difference sequence
such that |Xt | ≤ b. Let us also define
Vart-1(Xt) = Var (Xt | X1,...,Xt-1) =E Xt2 | X1,...,Xt-1 ,
and define VT = PtT=1 Vart-1(Xt) as the sum of variances. For δ < 1/e and T ≥ 3, it holds that
P (XXXt > max {2PVT, 3bPlog(1∕δ)}Plog(1∕δ)) ≤ 4log(T)δ	(14)
Proof. The proof of this lemma could be found at the beginning of the Appendix section of Kakade
& Tewari (2008), which is their Lemma 3 in the main text.
□
A.2 Proofs in Section 4.3
Theorem 4.1.	Let xt be generated by Algorithm 1 with G0 = 0 for simplicity. Then, it holds that
T XX kVf (χt)k2 ≤ O ( A T LL
t=1
Proof(Theorem 4.1). Setting gt = Vf (xt), deterministic counterpart of Eq. (9) becomes
T	∆	LT	uT
X kgtk2 ≤ 育 + 2 X ηtkgtk2 ≤ (∆max + L)UX kgtk2,
where we obtain the final inequality using Lemma 4.1. Now, we show that ∆T+1 is bounded for any
T. Using descent lemma and the update rule for xt,
f (xt+l) - f (xt) ≤ hgt, xt+1 - Xti + 2 kxt+1 - xtk2
≤ -ηtkgtk2 +	2ηt-Ilgtk2
Summing over t ∈ [T], telescoping function values and re-arranging right-hand side,
f (xT+1) - f(xt) ≤ X (-2^ - 1) ηtkgt k2
f(xτ+ι)- f (x*) ≤ f(xι)- f (x*)+X (-2ηt- 1) ηtkgtk2
where x* = argminχ ^Rd f (x). Now, define to = max {t ∈ [T] | ηt > L }, such that (空 一 1)≤
0 for any t > t0 . Then,
f(χτ+ι)-f(x*) ≤ δi + X(* 一 1) ηtkgtk2 + X (4 一 1) ηtkgtk2
t0
≤ ∆1 + L2 Xη2k*k2
t=1
L
≤ δi + 2
1+log (1+X kgtk2))
≤ δi + 2(1+log(1 + ηt0
L	L2
≤ ∆ι + 2 (1 + log (1 + ɪ
(Lemma 4.2)
(Definition of ηt)
(Definition of t0)
14
Published as a conference paper at ICLR 2022
where We use the definition of to and Lemma 4.2 for the last inequality. Since this is true for any T ,the
bound holds for ∆max such that ∆max ≤ ∆ι + L(1 + log(L2/4)). Now, define X = JPT=I Ilgtll2,
then the original expression reduces to X 2 ≤ (∆max + L) X. Solving for X trivially yields
T
X ≤ (∆max + L) =⇒ X2 = X IiUtIl2 ≤ (∆max + L)).
t=1
Plugging in the bound for ∆max and dividing by T gives,
1T
T EkVf(Xt)k2 ≤
t=1
+ ⅜
3+log 1+
T
□
Theorem 4.2.	Let xt be the sequence of iterates generated by AdaGrad. Under Assumptions 2, 6, 7,
for ∆maχ ≤ O (△] + L log(T) + σ2 log(1∕δ)), with probability at least 1 一 8log(T)δ,
1T
T ∑kUtk2 ≤
t=1
..	.	.c	~ .	.	.	.	, .	_ .	≈.	. .	∕Ξ--；-. ,,
(△ max + L) Go + 3(G + GG)lθg(1∕δ) (△ max + L) G + 2GσPlog(I那)
T	+	√T
Proof (Theorem 4.2). Define gUt = Vf(xt) and gt = Vf(xt; zt) =gUt + ξt. By Eq. (10),
T	△	T	LT
X kUtk2 ≤ △max + X -hgt, ξti + 2 XηtIgtI2
t=1	ηT t=1	2 t=1
Invoking Lemma 4.1 and plugging the bound for the term (**) from Proposition 4.1 we achieve with
probability 1 一 4 log(T)δ,
T
XIgUtI2 ≤ (△max + L)
t=1
T
G2 + X kgtk2 + 2σPI0g(1∕δ)
t=1
T
X kgtk2 +3(G2 + GG)log(1∕δ)
t=1
≤ (∆max + L)西 + G2T + 2GσPlog(1∕δ)√T +3(G2 + GG)log(1∕δ)
≤ (△max + L) G0 + 3(G2 + GG)log(1∕δ) + h(^max + 2L) G + 2GσPlog(I那)]√T
Dividing both sides by T, we achieve the bound,
r^Γ	. .	.	∙c	~.	...	..
T X kgt k2 ≤ (△ max + L) Go + 3(G2 + GG)log(1∕δ) + (△max
T t=1	T
Now, we will incorporate the high probability bound for △max to complete the convergence proof.
Essentially, we are interested in scenarios in which both the statement of Proposition 4.1 and the
statement of Proposition 4.2 holds, simultaneously, with high probability. Formally, let the statement
of Proposition 4.1 be denoted as event A and the statement of Proposition 4.2 as event B. We have
already proven that
P(A) ≥ 1 一 4 log(T)δ	& P(B) ≥ 1 一 4 log(T)δ
What we want to obtain is a lower bound to P (A ∩ B), which is
P (A ∩ B) = P (A) + P (B) 一 P (A ∪ B)
≥ 1 一 4 log(T)δ + 1 一 4 log(T)δ 一 P (A ∪ B)
≥ 2 一 8 log(T)δ 一 1 = 1 一 8 log(T)δ,
which is the best we could do due to the unknown extent of dependence between events A and B .
Hence, integrating the results of Proposition 4.2, with probability at least 1 一 8 log(T)δ,
1T
T Ekgt k2 ≤
t=1
(△max + L) GO + 3(G2 + GG)log(1∕δ) + (△max + L) G + 2GσPlog(I那)
T
15
Published as a conference paper at ICLR 2022
where
△max ≤ ∆ι + 2L(1+ log (max{1, G?} + G2T)) + G-1(3(G2 + GG) + σ2)log(1∕δ) + G-1(2G2 + GG)
□
Proposition 4.1. Using Lemma 4.3, with probability 1 - 4 log(T)δ with δ < 1/e, we have
T
T
X -hgt,ξti
t=1
≤ 2σ√log(1∕δ)t E kgtk2 + 3(G2 + GG)log(1∕δ).
t=1
Proof. We have to show that the random variable -hgt,ξti is a martingale difference sequence and
satisfies the conditions in Lemma 4.3. Let us define Ft = σ(ξt, ..., ξ1) as the σ-algebra generated by
randomness up to, and including ξt. Notice that Ft is the natural filtration of -<gt,ξti∙ Then, we
need to show that
1.	-hgt,ξti is integrable,
2.	martingale (difference) property holds, E [-(gt,ξti∣Ft-ι ] = 0.
First off, we show that -hgt, ξti is integrable:
E[∖hgt,ξti|] ≤ E[kgtkkξtk]
=E [k@k2 + kξtk2 ]
≤ G2+E[E[kξtk∖Ft-1]]
≤ G + σ < +∞,
where the second inequality is due to towering property of expectation. Then, the maritngale property:
E [-hgt,ξti∖Ft-1 ] = -hgt, E[ ξt∖Ft-1 D
=-hgt, 0i = 0
Before applying Lemma 4.3, we need to verify that ∖-hgt, ξti∣ is bounded:
∖-hgt,ξti∣ = ∖-hgt,gt - Util = IIIUtll2 -hgt,gtiI ≤ IIUtll2 + ∖-hgt,gti∣ ≤ Ilgtll2 + IIgtllIlgtIl ≤ G2 + GG,
where we used G-Lipschitzness of f and almost sure boundedness of stochastic gradients gt. Now,
we are able make the high probability statement. By Lemma 4.3, with probability 1 - 4 log(T)δ for
δ < 1∕e, we have
T
-hgUt , ξti ≤ max 2t
X E [ hgt,ξti2∣xt ], 3(G2 + GG)Plog(1∕δ) 1 Plog(1∕δ)
t=1	
(1)	,_______
≤ √ιog(i∕δ) ∣2t
(2)	,_______
≤ √ιog(i∕δ) ∣2t
T
X E | ||九||2||&|[2山]+ 3(G2 + GG)Plog(1∕δ)
t=1
σ2 X IgtI2 + 3(G2 + GG)Plog(1∕δ))
t=1
T
≤ 2σPlog(1∕δ)t X IIgtIl2 + 3(G2 + GG)log(1∕δ)
t=1
where we used Cauchy-Schwarz inequality for the inner product to obtain inequality (1) and bounded
variance assumption to obtain (2).	□
16
Published as a conference paper at ICLR 2022
Proposition 4.2. Let xt be generated by AdaGrad for G0 > 0. With probability at least 1 - 4 log(t)δ,
△t+1 ≤ ∆ι + 2L(1 + log(G2 + G2t)) + Go 1(M1 + σ2)log(1∕δ) + M2,
where Mi = 3(G2 + GGG) and M2 = G-1(2G2 + GG).
Proof. We will handle this bound in two cases. First, we show the bound for AdaGrad, and then for
the remaining two algorithms. Indeed, the bounds for the two cases differ by a factor of constants.,
hence we will use the larger bound for all three algorithms.
Case 1 (AdaGrad)
Let gt = Vf (Xt) and gt = Vf (xt； zt), such that gt = gt + ξt. Then, by smoothness
Lη2
f (χt+ι) - f (xt) ≤ -ηthgt, gti +—2llgtk
Lη2
=ftkUtk - ηthgt,ξti + 2Ilgtk
Defining x* = minχ∈Rd f (x) as the global minimizer of f and summing over t ∈ [T],
T	LT	T
f(xτ+ι)- f(x*) ≤f(xι)-f(x*) + X-ηt∣∣gt∣ι2 + 2Xη2kgtk2+X-ηthgt,ξti	(15)
t=1	t=1	t=1
-----------------} V------------------} '--------------------}
(A)	(B)	(C)
Term (A) At this point, we will keep this term as it will be coupled with the sum-of-conditional-
variances term which will be obtained through martingale concentration.
Term (B)
LT
2 X ηt kgt k
t=1
L X kgtk2
2 t=1 G0 + Pt=i kgik2
L
≤——
-2
1 +log max{1,G02} +X kgtk2
(Lemma 4.2)
(Bounded gradients)
≤ 2 (1 + log (max{1,G0} + G2T))
Bounding term (C)
TT
T
£一ηthgt,ξti ≤^2-ηt-ιhgt,ξti + £(nt-i — ηt)hgt,ξti
t=1
t=1	t=1
`-----------{----------} `---------------V--------------}
(C.1)	(C.2)
We will make use of Lemma 4.3 to achieve high probability bounds on term (C.1). To do so, we
need to prove that Xt = -ηt-ιhgt, ξt i is a martingale difference sequence and validate some of its
properties:
1.	一ηt-ιhgt, ξti is absolutely integrable:
E[|—ηt-ihgt,ξti∣] ≤ G-1E[∣hgt,ξti∣]
≤ G-1E[kgtk2 + kξtk2]
≤ G0-1(G2 + σ2) < +∞
2. 一ηt-ιhgt, ξti is adapted to its natural filtration Ft = σ(ξι,…,ξt)
17
Published as a conference paper at ICLR 2022
3.	It satisfies the martingale (difference) property:
E [-ηt-ιhgt,ξti | Ft-ι] = -ηt-ιhgt,E [ξt | Ft-ι D = 0
4.	Xt = -ηt-ιhgt,ξti is bounded：
-ηt-ιhgt,ξti ≤ G-1 lhgt,ξtil ≤ G-I(Ilgtk2 + kUtkkgtII) ≤ G-I(G2 + GG)
5.	Conditional variance of Xt = -ηt-ιhgt,ξ).
Vart-I(Xt)= E [(ηt-ιhgt,ξti)2 l Ft-I ]
≤ G-2E [ (hgt,ξti)2 ∣Ft-ι ]
≤ G-2Igtk2E [kξtk2 ∣Ft-ι]
≤ G-2σ2kgtk2
Term (C.1) Now, we are at a position to apply Lemma 4.3 on term (C.1). With probability
1 - 4 log(T)δ,
T
E-ηt-ιhgt,ξti
t=1
≤ max 2
-I ∖
XXE[(ηt-ιh5t,ξti)2 ∣Ft-ι], 3G-1(G2 + GG)Plog(1∕δJ /log(1∕δ)
t=1	
≤ max1 2t XX σ2 η2-i∣gtk2,3G-1(G2 + GG)Plog(1∕δ) } ,log(1∕δ)
T
≤ 2σPlog(1∕δ)t Xη2-ikgtk2 + 3G-1(G2 + GG)log(1∕δ)
t=1
X-------------{------------}
(D)
Term (C.2):
TT
X(ηt-ι - ηt)hgt,ξti ≤ X(ηt-ι - η) lhgt,ξtil
t=1	t=1
T
≤ (G2 + GG) X(ηt-ι - nt)
t=1
≤ (G2 + GG)no
Terms (A) + (D): All the underbraced term but expression (D) either grows as O (log(T)), or is
upper bounded by a constant. The worst-case growth of term (D) is O(√T), which We will keep
under control via term (A).
T	T
Xn2-ιkgtk2 - Xntkgtk2
t=1	t=1
(A) + (D) ≤ 2σ√log(1∕δ)t
T
T
≤ 2σVZlOg(10t En2-ιkgtk2 - GO£n2kgtk2
t=1
T
t=1
T
≤ 2σVZlOg(10t En2-ιkgtk2 - Go£n2-ikgtk2 + Go£(n2-i -n2)kgtk2
t=1
t=1
t=1
T
≤ 2σ√log(1∕δ)t
TT
Xn2-ιkgtk2 - Go Xn2-ιkgtk2 + GoG2n0
t=1	t=1
18
Published as a conference paper at ICLR 2022
In order to characterize the growth of this expression, let Us define f (x) = 2σ/log(1∕δ)√Ξ - G0x,
which is a concave function as its second derivative is non-positive. Now, looking at derivative of f ,
f=σ^√≡
dx	x
- G0 ,
which is0at X = G-2σ2 log(1∕δ). This is indeed the point at which the function attains its maximum.
For the final step of the proof, we define ZT = PL η2-1kgtk2. Then,
(A) + (D) ≤ f(ZT) + G0G2η02
≤ f(G-2σ2 log(1∕δ))+ GoG2η2
=G-1σ2 log(1∕δ) + GoG2η0
Final bound Plugging all the expression together and setting η0 = η1 , with probability at least
1 - 4 log(T)δ,
f (xT +1)- f (x*) ≤ f (xI)- f (x*) + 2 (1+ log (max{1, G2} + G2T))
+ Go 1(3(G2 + GG) + σ2) log(1∕δ)
+ G-1(2G2 + GG)
Since this result holds for any T, to make it consistent with the statement of the proposition, we
re-state the bound with t,
f(xt+ι) — f(x*) ≤ f(xι) — f(x*) + L(1 + log (max{1,G2} + G2t))
+ Go 1(3(G2 + GG) + σ2) log(1∕δ)
+ G-1(2G2 + GG)
Case 2 (AdaGrad Wl Averaging & RSAG) Let us define stochastic gradient at Xt as gt =
Vf (Xt； Zt) = Ut + ξt where Ut = Pf(Xt) Then, by smoothness,
f(xt+l) - f (Xt) ≤ hvf (Xt),xt+1 - xti + — kxt+1 - xtk2
Lηt2	2
=-ηthgt, gti - ηthvf (Xt) - gt, gti +——— IlgtIl	(CaUChy-SChWarz)
Lη2
=-ηt∣∣Vf(Xt) - ηthgt,ξti + Vf(Xt)IlIlgtk + -—tIlgtIl2	(Smoothness)
Lη2
=-ηt∣∣gt Il - ηthgt, ξti + LntIlXt - XtlIIlgtll +——— Ilgtll	(YoUng S ineq.)
=-nt∣lgtIl2 - nthgt,ξti + —忸-xM2 + 乙褚也||2
Using recursive expansion of IXUt - Xt I2 and summing over t ∈ [T],
∆t+ι ≤ ∆ι + LXX [(i-αt)ΓtXOkSk-F)2 IgtI2] + XXXLn2IgtI2 -ntIgtI2 -nthgt,ξti
2 t=1	k=1 Γk	α2k	t=1
≤ δi + — X ]X(1 - αk)rk Ot (nk α27k)的讨2 + XLn2IgtIl2 - ntMtll2 - nthut,ξti
19
Published as a conference paper at ICLR 2022
First, We plug in at = 2/( + 1) and invoke Proposition 5.2. Recognizing that ∣γt - ηt∣ = αtη for
both RSAG and AdaGrad with averaging,
T
T
T
∆T+1 ≤ ∆1 +∑-ηtkgtk2 + 2L∑η2kgtk2 + ∑-ηthgt,ξti
t=1	t=1	t=1
------------} V------------} '-------------}
(A)	(B)	(C)
Observe that this expression is the same as Eq. (15) UP to replacing L in term (B) of AdaGrad with
2L. Hence, the same bounds hold up to incorporating the aforementioned change. With probability
1 - 4 log(T)δ,
f (XT +ι) - f (x*) ≤ f (xι) - f (x*) + 2L(1 + log (max{1, -2} + G2T))
+ G-1(3(G2 + GG) + σ2)log(1∕δ)
+ G-1(2G2 + GG)
Similarly, since this holds for any T, we re-state the results with t for consistency,
f(xt+ι) - f(x*) ≤ f(xι) - f(x*) + 2L(1 + log (max{1,G2} + G2t))
+ G- 1 (3(G2 + GG) + σ2) log(1∕δ)
+ G-1(2G2 + GG)
□
A.3 Proofs in Section 5
Proposition 5.1. Let Xt be generated by Algorithm 2 where gt = Vf (Xt； Zt) = Ut + ξt and
gt = Vf(Xt). Then, it holds that
T
XkgUtk2≤
t=1
ηT
∆max + 2L
Γt
.}
αt (ηt - γt)2
T
kgtk2+X-hgUt,ξti.
t=1
'----{-----}
(**)
Proof. This result is due to Ghadimi & Lan (2016); Lan (2020) up to introducing adaptive step-sizes.
We follow their derivations in the deterministic setting and incorporate it with our high probability
analysis. Then,
f(χt+1) - f (Xt) ≤ QVf(Xt)xt+1 - xti + 2kχt+1 - xtk2
Lη2
≤ -ηthVf (Xt), Vf (Xt) + ξti + 詈kgtk2
Lη2
=-ηtkgtk2 - ηthVf(Xt),ξti- ηthVf(Xt) -Vf(Xt),gti + Tkgtk2
≤ -ηtkgtk2 -ηthgt,ξti + 2IIXt - Xtk2 + Lη2kgtk2
where we used descent lemma (Eq. (4)) in the first inequality, and update rule for Xt+1 in Algorithm 2,
line 4 in the second inequality. For the last line, we use Cauchy-Schwarz, apply smoothnness
definition in Eq. (3) and finally use Young’s inequality. Let us define ∆t = f(Xt) - minx∈Rd f(X)
and ∆max = maxt∈[T] ∆t. Dividing both sides by ηt, rearranging, and summing over t = 1, ..., T we
obtain,
TT	T	T	T
X kgtk2 ≤ X - A- 4t+1) + 2 X — kXt-Xtk2 + LXηtkgtk2 + X-hgUt , ξt i
t=1	t=1 ηt	2 t=1 ηt	t=1	t=1
20
Published as a conference paper at ICLR 2022
Now, We express the term Xt — Xt recursively, as a function of gradient norms.
Xt 一 Xt = (1 一 Qt) [Xt 一 Xt]
=(I 一 Qt)Ixt-1 — Xt-1 + (nt-1 一 Yt-1)gt-1]
=(I 一 Qt) [(1 — Qt-I)(Xt-1 — Xt-1) + (nt-1 一 Yt-l)gt-1]
t-1 / t-1	\
=(1-Qt)E ∏ (1 一 Qj) (ηk 一 γk)gk
k=1 j=k+1
t-1 Γ
(I 一 Qt) E Γ- (nk - Yk)gk
k=1	k
t-1
(1 - αt)Γt-1 X
k=1
αk (ηk - γk)
gk	gk,
Γk	Qk
Hence, by convexity of squared norm and (absolute) homogeneity of vector norms,
t-1
kxt - xtk2 = Il(I- Qt)rt-i X
k=1
t-1
≤ (1 - αt)2Γt-1 X
k=1
αk (ηk - γk)
Γk	αk
gkk2
αk (ηk - γk)
rk	Qk
2
kgkk2
t
≤ (1 - Qt)ΓtX
k=1
Finally, we plug this in the original expression,
αk (ηk - γk)2
rk	Qk
kgkk2
TT
X kgtk2 ≤ X Qt - ∆t+ι) +
t=1	t=1 ηt
2 X (i)”
t
αk (ηk - γk)2
t=1
+ LEntkgtk2 + £ -hgt,ξti
t=1
t=1
T1	T
≤ δ + X(ɪ 一 -1)∆t+ι + L X
η1 t=1 ηt+1 ηt	2 t=1
T
+LX
t=1
kgtk2
,G0 + Pk= kgk k
k=1
Γk αt (ηt - γt)
1 一 Qk)——
T
=+ X -hgt,ξti
2 t=1
∆	T-1	1	1 L T
≤ 中』哈 (.一 nt)+2 X
rk	Q2
ηk
Γt
kgkk2
2
kgtk2
Γk	αt (ηt - γt)
ηk
Γt
2
kgtk2
+ 2Lt G2 + £ kgtk2 + E -hgt,ξti
t=1
≤ ^max + 2L +
-	nτ
2ηT t=1
|
t=1
T
1 - αk)Γk
{z
(*)
αt (ηt - γt)2
Γt
.}
T
kgtk2 + X 一〈况,&i.
t=1
'------{------}
(**)
T
t
T
T
T
T
We rearranged the summations to obtain the second inequality, used the assumption that ∆t ≤ ∆max
for any t together with Lemma 4.1, and we telescope the first summation on the right hand side to
obtain the result.
□
Next, We provide the proof for term (*) in Proposition 5.1.
21
Published as a conference paper at ICLR 2022
Proposition 5.2. We have
1 - ak)Γk# Γt ≤ (2	if αt = 1+1;
_ rt Uog(T + 1) if at = t.
Proof. First, we begin with the weighted averaging setting, i.e., αt = 2/(t + 1). Using the recursive
definition of Γ, one could easily show that for any αt ∈ (0, 1),
ak _ 1
f^Γk = Γ
k=1
t
Γt X a = L
t⅛1 rk
Defining At = Pt=1 i = t(t+1) and A° = 1,we have that at = A^ and
tt
Γt = Y(1 - ai) = Y(1
i=1	i=1
i	Ai-1	1
Ai )= Ar = At
i=1
Hence, We can express term (*) as
1 - ak )Γk
a ≤
Γt 一
T1
X黑
k=t	k
T
2
k=t
1
k(k + 1)
t
t
T1
2 X 1 —
k
k=t
1
k + 1
t
1
t
—
2
τ+l)t
≤2
For the uniform averaging setting with at = t, for t > 1,
k - 1 1
rt =	(1 - αi ,丁 = k
i=1	i=2
Hence, again for t > 1,
1 - ak )Γk
where the last inequality is due to that fact that integral of f(x) = 1/x over the range [1, k] upper
bounds the summation above.
□
Finally, we conclude with the high probability convergence theorem for AdaGrad with averaging and
RSAG,
Theorem 5.1. Let xt be the sequence of iterates generated by AdaGrad with averaging or adaptive
RSAG. Under Assumptions 2, 6, 7, for ∆max ≤ O (∆ι + L log(T) + σ2 log(1∕δ)), with probability
22
Published as a conference paper at ICLR 2022
1 - 8 log(T)δ,
1T
T X k@k ≤
t=1
Go (△ max + 3L + Llog (max{1, G0 2} + G2T)) + 3(G2 + GG)log(1∕δ))
T
G (△ max + 3L + L log (max{1,G0 2} + G2T)) + 2GσPlog(1∕δ)
+	√T
Proof. Again by Proposition 5.1,
X kgtk2 ≤ 2±9+白 X
t=1	ητ	2ητ t=1
X
1 - αk)Γk
αt (ηt - γt)2
{z
(*)
Γt
.}
T
kgtk2+X-hgUt,ξti.
t=1
`—{—/
(**)
Both for AdaGrad with averaging and adaptive RSAG, we use weighted averaging. Moreover, due to
the particular step-size choices, ηt - γt = αtηt for both methods. Combining this observation with
the previous expression we get
ττ
X kgtk2 ≤ 2+竺 + 二 X
t=1	ητ	2ητ t=1
、
1-αk)rk	αtη2kgtk2+X-hgt,ξti.
t=1
{z
(*)
X—{—'
(**)
}
T
We introduce the bounds in Proposition 5.2 and Proposition 4.1 for the respective marked term,
T
T
XkgUtk2≤
t=1
△max + 2L + L P= η2∣∣gtk2 +
ηT
2σ√log(1∕δ)t E kgtk2 + 3(G2 + GG)log(1∕δ)
t=1
△max + 3L + L log (max{1, Go-2} + PtT=1 kgt k2)
一	ητ
+ 2GσPlog(1∕δ)√T + 3(G2 + GG)Iog(1∕δ)
≤ △max + 3L + L log max{1, Go-2} + X kgtk2	ut
T
G20 +X kgtk2
t=1
+ 2Gσ √log(1∕δ) VT + 3(G2 + GG)log(1∕δ)
≤ (G (∆max + 3L + Llog (max{1, G-2} + G2T)) + 2Gσ,log(1∕δ)) √T
+ Go (∆max + 3L + Llog (max{1, G-2} + G2T)) + 3(G2 + GG)Cg(1∕δ)
where We used Lemma 4.2 in the second inequality, while boundedness of Ut and almost sure
boundedness of gt in the last line. Dividing both sides by T, and using the same argument as in the
proof of Theorem 4.2, with probability at least 1 - 8 log(T)δ,
1T
TX 僚|| ≤
t=1
Go (∆max + 3L + Llog (max{1, Go 2} + G2T)) + 3(G2 + GG)log(1∕δ))
T
G (∆max + 3L + L log (max{1,G0 2} + G2T)) + 2GσPlog(1∕δ)
+	√T
23
Published as a conference paper at ICLR 2022
where
△max ≤ ∆ι + 2L(1 + log (max{1, G0} + G2T)) + G0 1(3(G2 + GG) + σ2)log(1∕δ) + G0 1(2G2 + GG)
□
B	Noise adaptation under sub-Gaussian noise
In this section of the appendix, we present the proof of Theorems 4.3 and 4.4 along with the Lemmas
that we will require in the proofs. We first prove a bound on △max and then show noise-adaptive rates
for AdaGrad.
B.1 HIGH PROBABILITY B OUNDS ON △MAX
We will argue about high probability bounds on objective sub-optimality under sub-Gaussian assump-
tion. First, we will present Lemma 1 from Li & Orabona (2020), as well as our modified version of it
that we use in our derivations.
Lemma B.1 (Lemma 1 from Li & Orabona (2020)). Let Zι,…，Zt be a martingale difference
sequence (MDS) with respect to random vectors ξι, …，ξτ and Yt be a sequence ofrandom variables
which is σ(ξι, ∙∙∙ , ξt-ι)-measurable. Given that E [exp(Z2∕Yt2) | ξι,…ξt-ι] ≤ exp(1), for any
λ > 0 and δ ∈ (0, 1) with probability at least 1 - δ,
TT
X Zt ≤ 3λ X Y2 + 1log(1∕δ)
4λ
t=1	t=1
Next, we present a slightly modified version of the above lemma. Its proof follows the same lines
with Lemma B.1 up to replacing Yt with a deterministic quantity, selecting a particular choice of λ
and dealing with the MDS Zt itself rather than its square, Zt2 .
Lemma B.2. Let Zι, ∙∙∙ ,Zt be a martingale difference sequence (MDS) with respect to random
vectors ξι,…,ξτ and σ2 ∈ R such that E [exp(Zt∕σ2) | ξι,…ξt-ι ] ≤ 1. Then, with probability
as least 1 - δ,
T
X Zt ≤ σ2 log(1∕δ)
t=1
We will also make use of another relevant result (Lemma 5 in Li & Orabona (2020)) regarding the
probabilistic behavior of maximum over norms of noise vectors.
Lemma B.3 (Lemma 5 in Li & Orabona (2020)). Under assumptions as in Eq. (5) and (11), let
ξt = Nf(Xt,zt) 一 ▽/(Xt). For δ ∈ (0,1), with probability at least 1 一 δ,
Imgr kξt k2 ≤ 冉隼(eT)
Theorem 4.3. Let Xt be generated by AdaGrad and define △t = f(Xt) 一 minx∈Rd f (X). Under
sub-Gaussian noise assumption as in Eq. (11), with probability at least 1 一 3δ,
△t+1 ≤ δ1 + 3Go1G2 + 2G0 1σ2 log (k)+ -j-^-σ2 log(10
δ	4G0
+ L(1 + log (max {1, G0} + 2G2t + 2σ2tlog (et)))
Proof. Using the initial steps of the proof in the original derivation,
TT	T
△t +ι ≤ δi + X —nJ®/2 + X-ηthgt,ξti + 2Xη2kgtk2
t=1	t=1	t=1
×-------{-------} X----------{--------} X---------{---------}
(A)	(B)	(C)
24
Published as a conference paper at ICLR 2022
Term (A) + (B): In order to deal with measurability issues, we will divide term (B) into two parts:
T
T
T
Σ -ηthgt, ξti = Σ -ηt-ιhgt, ξti+y^(ηt-ι - ηt)hgt, ξti
t=1
t=1
(≤1) XT
t=1
T
t=1
T
-nt-ιhgt,ξti + 2(G2 + ιmtaTll&k2)X(ηt-ι - 罩)
≤ ≤	t=1
≤ y^-ηt-ιhgt,ξti+2(G2+ImaxT ∣∣ξt『加0
t=1
where we used Cauchy-Schwarz together with Young’s inequality to obtain inequality (1) and
telescoped in the last line. Moreover, we pick η0 ≥ η1 to make sure monotonicity. Without loss of
generality, a natural choice would be η0 = G0-1, which aligns with the definition in Algorithm 1. By
Lemma B.3, with probability at least 1 - δ,
TT
X-ηthgt,ξti ≤ X-ηt-ιhgt,ξti + 2G-I
t=1	t=1
G2 + σ2 log
Now, We will invoke Lemma B.1 on the term PT=I -ηt-ι(gt, ξti by setting Zt = -ηt-ιhgt, ξti,
Yt2 = η2-ι ∣∣Ut∣∣2σ2, with probability at least 1 - δ,
TT
T	3T	1
X -ηt-ιhgt,ξt i ≤ λX X Yt +∖log(I©
4λ
t=1	t=1
T
=彳 λσ2 X η2-ιkgtk2 + = bg(10
4λ
t=1
Now, summing UP the expression above with term (A) and leaving ɪ log(1∕δ) aside for now,
TT	T	T
4λσ2 Xη2-ιkgtk2- Xηtkgtk2 ≤ 3λσ2Xη2-Mk2- g°X褚|加|2
t=1	t=1	t=1	t=1
TTT
≤ 4λσ2Xη2-Mk2-GoXη2-ιMtk2 + GoX(η2-ι-η2) kgtk2
t=1	t=1	t=1
≤ (4λσ2 - GO)X ηt-1kgt∣2 + GoG2η2
where we used Goη2 ≤ η in the first inequality and added/subtracted PT=I η2-ιkgtk2 in the second
inequality. Since we have a free variable to choose, λ, we could set it to λ = ^^ to obtain,
TT
4λσ2 Xη2-ιkgtk2- Xηtkgtk2 ≤ G-1G2
Hence, summing up all the expressions together, with probability at least 1 - 2δ,
(A) +(B) ≤ 3G-1G2 + 2G-1σ2 log (~δ~) + 4G^σ2Iog(10
25
Published as a conference paper at ICLR 2022
Term (C): This term is easy to prove using online learning lemmas as we did previously, but we
introduce a slight change in order to avoid bounded stochastic gradient assumption.
LT
2 X η2kgtk2
t=1
L
≤——
-2
1 + log
T
max {1,Go} + X ι∣gtk
t=1
))
L
≤——
-2
1 + log
max
T
{1,G0} + 2X ∣gtk2 + 2
t=1
Xt=T1ιξtιo
≤ 2 (1 + log (maχ {1,G°} + 2G2T + 2 (ImtaxT kξtk2) T
We invoked Lemma 4.2 to obtain the first inequality. Once again via Lemma B.3, with probability at
least 1 - δ,
LT
2 Xη2kgtk2 ≤
t=1
1 + log
max {1, G02 } + 2G2T + 2σ2 T log
Finally, merging all the expression, with probability at least 1 - 3δ,
δt+ι ≤ δi + 3G-1G2 + 2G-1σ2 log (τξ^) + ~T7Γσl Iog(10
δ	4G0
+ 2 (1 + log (max{1,Gο} + 2G2T + 2σ2Tlog (~δ~)))
=O (∆ι + σ2 log (eT) + σ2 log(1∕δ) + Llog (T + σ2Tlog (/)))
□
B.2 High Probability Convergence Rate
Now, we are at a position to prove noise-adaptive bounds.
Theorem 4.4. Let xt be generated by AdaGrad and define ∆t = f (xt) - minx∈Rd f (x). Under
sub-Gaussian noise assumption as in Eq. (11) and considering high probability boundedness of ∆max
due to Theorem 4.3, with probability at least 1 - 5δ,
1T
T Ekgtk2 ≤
t=1
32 (∆max + L)) + 8 (∆max + L) (G0 + σ p2 log(1/6)) + 8。2 log(1∕δ)
T
8√2 (∆max + L) σ
+	√T
Proof. We take off from the same step of the original analysis by defining ξt = Vf (xt, Zt) — Vf (xt),
T
Xkggtk2≤
t=1
TT
+ X -hgt,ξti + 2 X ηtkgtk2
uT	T
≤ (∆max + L) utG20 + X kgtk2 + X -hggt, ξti
t=1	t=1
(∆max+L)tG20+2	(kggtk2 + kξtk2 +
(∆
max + L) G0 + t
t=1
T
2Xkggtk2 +
t=1
T
σ2 - σ2) + X -hggt,ξti
t=1
T
max 0, 2X(kξtk2
t=1
'-------------
{z^
(*)
1/2	T
-σ2) })	+	+ X-hgt,ξti
-----}	||=1{-------}
(**)
≤
≤
T
26
Published as a conference paper at ICLR 2022
We already showed that -hgt, ξti is a MDS. Similarly, We could show that martingale property holds
for kξtk2 - σ2,
E [ kξtk2 — σ2 | ξι,…，ξt-i ] = E [ kξtk2 | ξι,…，ξt-i ] - σ2 ≤ σ2 - σ2 = 0.
Lemma B.2 immediately implies for term (*) that with probability at least 1 - δ,
T
X(kξtk2 - σ2) ≤ σ2 IOg(10
t=1
For term (**), we apply Lemma B.1 with Y2 = σ2∣∣gtk2 and λ = 1∕σ2 to obtain with probability at
least 1 - δ,
T	3T
X -hgt,ξti ≤ 4 X Ilgtk2 + σ2 Iog(10
t=1	t=1
Plugging these values in and re-arranging,
1 τ	τ
4 ^X Ilgtk2 ≤ √2 Omax + L) t ^X Ilgtk2 + Omax + L) (GO + σP2log(V6 + σ√2T) + σ2 log(1/^
4 t=1	t=1
We will conclude our proof by treating the above inequality as a quadratic inequality with respect to
X = qPT=1 k[tk2∙ Defining C = (∆max + L) (G0 + σp2 log(1∕δ) + σ√2T) + σ2 log(1∕δ),
X2 — 4√2 (∆maχ + L) x — 4c ≤ 0,
where the roots of the inequality are
X
4√2 Omax + L) ± 332 (∆max + L)2 + 16 (∆max + L) (G0 + σP2 log(1/δ) + σ√2T) + 16σ2 log(1/δ)
2
Since X > 0 by default, we will take into account the positive root above, which yields,
^X kθtk2 ≤ 4 (√2(∆max + L) + r(∆max + L) (G0 + 2 (∆max + L) + σP2 log(1/δ) + σ√2T) + σ2 log(1/δ))
1T
TX Bk ≤
t=1
32 Omax + L)) + 8 Omax + L) (GO + σP2 log(1/6)) + 8σ2 log(1∕δ)
T
8√2 (∆max + L) σ
+	√T
□
27