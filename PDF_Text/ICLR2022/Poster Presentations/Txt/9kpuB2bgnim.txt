Published as a conference paper at ICLR 2022
Huber Additive Models for Non-stationary
Time Series Analysis
Yingjie Wang1,2, Xianrui Zhong3, Fengxiang He2#, Hong Chen4, * & Dacheng Tao2
1 College of Informatics, Huazhong Agricultural University, China
2JD Explore Academy, JD.com Inc, China
3Department of Computer Science, University of Illinoist at Urbana-Champaign, USA
4College of Science, Huazhong Agricultural University, China
yingjiewang1201@gmail.com, xzhong23@illinois.edu,
fengxiang.f.he@gmail.com, chenh@mail.hzau.edu.cn,
dacheng.tao@gmail.com
Ab stract
Sparse additive models have shown promising flexibility and interpretability in pro-
cessing time series data. However, existing methods usually assume the time series
data to be stationary and the innovation is sampled from a Gaussian distribution.
Both assumptions are too stringent for heavy-tailed and non-stationary time series
data that frequently arise in practice, such as finance and medical fields. To address
these problems, we propose an adaptive sparse Huber additive model for robust
forecasting in both non-Gaussian data and (non)stationary data. In theory, the
generalization bounds of our estimator are established for both stationary and non-
stationary time series data, which are independent of the widely used mixing condi-
tions in learning theory of dependent observations. Moreover, the error bound for
non-stationary time series contains a discrepancy measure for the shifts of the data
distributions over time. Such a discrepancy measure can be estimated empirically
and used as a penalty in our method. Experimental results on both synthetic and
real-world benchmark datasets validate the effectiveness of the proposed method.
The code is available at https://github.com/xianruizhong/SpHAM.
1	Introduction
Additive model has become one of the most powerful tools for time series analysis due to the
exemplary monograph (Stone, 1985; Hastie & Tibshirani, 1990) and companion software (Chambers
& Hastie, 1992). For the past two decades, the growing importance of algorithmic flexibility
and interpretability motivates the development of various additive models along with theoretical
explorations (Huang & Yang, 2004; Wang & Yang, 2007; Chu & Glymour, 2008; Song & Yang, 2010;
Yang et al., 2018; Chen et al., 2017; Liu et al., 2020; Chen et al., 2021a) and practical applications
(Dominici et al., 2002; Wang & Brown, 2011; Ravindra et al., 2019; Bussmann et al., 2020; Wang
et al., 2020). Although these aforementioned works have shown promising behaviours, the proposed
methods in these works require some stringent assumptions on the stochastic process, e.g., various
mixing conditions (Doukhan, 1994), stationary distribution and Gaussian innovation.
A number of attempts have been made to relax such stringent assumptions. For the purpose of dealing
with non-Gaussian innovation, Qiu et al. (2015) develop an elliptical vector autoregressive model
for estimating heavy-tailed stationary processes with parametric convergence analysis that reduces
the influence of non-Gaussian innovation. Moreover, under stationarity and decaying β-mixing
condition, Wong et al. (2020) derive nonasymptotic estimation error of lasso without assuming
special parametric form of the data generating process. Stationarity and various mixing conditions
are commonly adopted in many previous studies, see, e.g., (Mohri & Rostamizadeh, 2009; 2010;
Kock & Callot, 2015; Wong et al., 2020). In practice, the mixing and stationary conditions are too
stringent and not always valid (Baillie, 1996; Kuznetsov & Mohri, 2020a). To relax the mixing and
stationary conditions, Adams & Nobel (2010) prove asymptotic guarantees for stationary ergodic
* Corresponding authors.
1
Published as a conference paper at ICLR 2022
sequences. Agarwal & Duchi (2013) establish generalization bounds for asymptotically stationary
(mixing) processes in the case of stable on-line learning algorithms. Kuznetsov & Mohri (2014)
establish learning guarantees for fully non-stationary and mixing processes. Recently, Kuznetsov &
Mohri (2020a) provide data-dependent generalization risk bounds for non-stationary and non-mixing
stochastic processes.
However, the exploration of additive models for both robust and non-stationary time series forecasting
is still very limited. In this paper, we propose a class of sparse Huber additive models with theoretical
guarantees. Our main contributions are summarized as follows:
•	Algorithm design and theoretical guarantees: In Section 3.1, we first propose a novel sparse
HUber additive model (SPHAM) with HUber loss, sparsity-inducing '2,1-norm regularize]
and additive data-dependent hypothesis space. This proposed method works for stationary
time series and can achieve robust forecasting and satisfactory inference (e.g., Granger
causal discovery) simultaneously. In theory, Section 3.2 establishes the upper bound of the
function approximation error of SpHAM by developing error decomposition technique (Wu
et al., 2006; Chen et al., 2021b) and employing sequential Rademacher complexity (Rakhlin
et al., 2010; 2015; Kuznetsov & Mohri, 2020a). With properly selected scale parameters, the
theoretical findings indicate that: a) For stationary time series, the function approximation
consistency with convergence rate O(T- 2) can be pursued, even if the innovation is non-
Gaussian distribution (see Theorem 1 and Corollary 1 for more details). Moreover, this
consistency analysis appears to be novel because no explicit data dependence assumptions
are not imposed here, e.g., various mixing conditions used in (Doukhan, 1994; Mohri &
Rostamizadeh, 2010; Zou et al., 2009; Wong et al., 2020); b) For non-stationary time series,
the function approximation error is bounded by a discrepancy measure, which characterizes
the drifts of the data distributions along with time (see Theorem 2 for more details). By
penalizing such a discrepancy measure, Section 3.3 further proposes an adaptive SpHAM
for non-stationary time series and provides its theoretical upper bound correspondingly (see
Theorem 3 for more details).
•	Optimization and empirical evaluations: The proposed SpHAM and adaptive SpHAM can
be implemented efficiently by Fast Iterative Shrinkage-Thresholding Algorithm (FISTA)
(Beck & Teboulle, 2009). Experimental results on both synthetic and real-world benchmark
CauseMe (Runge et al., 2019) validate the effectiveness of the proposed method.
Related works: There exist some interesting studies towards sparse additive model for time series
analysis from both theoretical and practical viewpoints; see, e.g., Chu & Glymour (2008); Song &
Yang (2010); Yang et al. (2018). Although they show promising interpretability and modeling capacity,
all of them require Gaussian innovation, stationarity and various mixing conditions. Moreover, these
assumptions are not always valid in heavy-tailed and non-stationary time series data. In contrast to
these previous additive models, we are seeking to formulate our method and investigate its asymptotic
properties without resorting to such strict assumptions.
Robust forecasting is a vibrant area of research in time series analysis (Qiu et al., 2015; Wong et al.,
2020). As one of the triumphs and milestones of robust statistics, the success stories of Huber
algorithms are mainly on robust prediction tasks with i.i.d datasets (Huber, 1964; Huber & Ronchetti,
2009; Loh, 2017; Feng & Wu, 2020), and their extensions to time series prediction are fairly sparse.
To our best knowledge, this is the first work that considers Huber additive models for non-stationary
and dependent time series data.
Our theoretical analyses are inspired from the successful usage of sequential Rademacher complexity
in Kuznetsov & Mohri (2020b;a). The generalization risk bounds are established in their works
for a general scenario of non-stationary and non-mixing stochastic processes. However, it should
be mentioned that we are chiefly concerned about the function approximation analysis, which is
essentially different from theirs and is very crucial for Huber algorithms, since the convergence
of generalization risk cannot imply the convergence of function approximation (Sun et al., 2019;
Feng & Wu, 2020; He & Tao, 2020). Moreover, the development of analysis techniques (e.g., error
decomposition and sequential Rademacher complexity) for assessing our method may shed light on
other robust models for nonstationary time series analysis.
To better highlight the novelty of our method, Table 1 summarizes the algorithmic properties of our
method and other related works, e.g., Sparse additive models for time series (TS-SpAM) (Yang et al.,
2
Published as a conference paper at ICLR 2022
Table 1: The properties of related methods
	TS-SPAM一	DBF	R-Dantzig	Ours
Hypothesis space	Additive Spline-based	Kernel-based	Linear	Additive Kernel-based
Loss	Squared	Squared	Quantile-based	Huber
Mixing condition	Yes	No	Yes	No
Stationarity	Yes	No	Yes	No
Robustness	No	No	Yes	Yes
Sparsity	YeS 一	No 一	Yes	Yes
2018), Discrepancy-based Forecasting (DBF) (Kuznetsov & Mohri, 2020a) and Robust Dantzig-
selector-type estimator (R-Dantzig) (Qiu et al., 2015).
The remainder of this paper is organized as follows. Section 2 recalls the background of additive
models. Section 3 mainly provides our methods and theoretical guarantees. We provide empirical
evaluations in Section 4. Finally, Section 5 concludes this paper. The source code package is available
at https://github.com/xianruizhong/SpHAM.
2	Preliminary
Let {Zt}t∞=-∞ be a stochastic time series with time index t, where variable Zt = (Xt, Y t) takes
values in the compact input space X ⊂ Rp and the output space Y ⊂ R. We consider a common
nonparametric model
Yt = f*(Xt)+ εt, E(εt)=0,	(1)
where f *(∙) is the ground truth function, and the innovation εt is i.i.d. across time t ∈ Z. For the
sake of simplicity, we denote ρt and ρtX as the jointed distribution of (Xt, Yt) and the corresponding
marginal distribution with respect to Xt, respectively. This setup actually covers a large number of
scenarios commonly used in practice. For instance, the case Xt that contains p lagged values of Yt
(e.g., Xt = Y(t-p) ×∙∙∙× Y(t-I)) corresponds to the p-order autoregressive models. Moreover, this
case can be viewed as a vector autoregressive model in the sense that input X t includes the historical
information of multiple variables.
Although such a nonparametric model (1) makes very few assumptions on data generation, the related
nonparametric algorithms suffer so-called “curse of dimensionality”, see Fan & Gijbels (1996) for
further discussion. An effective strategy for solving this problem is additive model. Usually, the
additive structure is obtained by decomposing the input space X ∈ Rp into X = X1 × ... × Xp.
Under the assumption that the ground truth admits an additive structure f * = Pp=I fj, the additive
model can be defined by
Yt = f*(Xl) + …+ fp (Xp)+ εt,	⑵
where each component fj* : Xj → R is a smooth function. In linear time series analysis, a weak
stationarity condition (i.e., the first two moments of time series are time invariant) is preferred (Han
et al., 2015; Qiu et al., 2015). In contrast, strict stationarity is primarily used to analyse the nonlinear
time series (Fan & Yao, 2005).
Definition 1. A stochastic process {Z t}t∞=-∞ is strictly stationary if (Z1, ..., Zt) and
(Z1+k, ..., Zt+k) have the same joint distributions for any t ∈ Z and k ∈ Z.
Note that, if not otherwise stated, the stationarity in this paper refers to strict stationarity. Suppose
that we are given T size time series data {(xt, yt)}tT=1 ∈ ZT which are drawn from an additive
data-generating model (2). Under stationarity condition and zero-mean Gaussian innovation with
finite variance, the widely used methods that learn the ground truth f * usually integrate squared loss
and a smoothness- or sparsity-inducing regularizer Ω(∙) into a structural risk minimization scheme:
Tp
miH £(yt- E fj (Xj))2 + ω(∕ ),
t=1
j=1
3
Published as a conference paper at ICLR 2022
where H := {f1 + ... + fp : fj ∈ Hj, j = 1, ..., p} is an additive hypothesis space. Commonly, each
subspace Hj could be reproducing kernel Hilbert space (Chen et al., 2017; Kandasamy & Yu, 2016;
Raskutti et al., 2012), the orthogonal basis inducing space (Ravikumar et al., 2009; Meier et al., 2009;
Yang et al., 2018), or the composite function space with the neural network as a typical (Agarwal
et al., 2020; Bussmann et al., 2020).
However, when facing heavy-tailed innovation, these methods may have degraded performance due
to the amplification of the squared loss to large residuals. As one commonly used statistic in robust
learning community (Peng et al., 2019; Wang et al., 2017b; Feng & Wu, 2020), Huber loss is defined
as
` (f(xt) - yt) =	(f(xt)-yt)2,	if |f(xt)-yt| <σ
σ	[2σ∣f (xt) - yt∣ - σ2, if |f(xt)-yt∣≥ σ,
(3)
where σ is a positive hyper-parameter. Note that in the previous studies (Huber & Ronchetti, 2009;
Loh, 2017), the hyper-parameter σ is set to be fixed according to the 95% asymptotic efficiency rule.
However, Huber regression with a fixed scale parameter may not be able to learn the ground truth
when the noise is asymmetric, as argued recently in Feng & Wu (2020); Sun et al. (2019). In this
paper, we choose the scale parameter σ by relating it to the moment condition of the noise distribution
and the sample size so that the resulting regression estimator can asymptotically converge to the
ground truth function.
3	Method
3.1	Sparse Huber additive models
In this paper, we choose reproducing kernel Hilbert space (RKHS) HKj , j = 1, .., p, to form the
the additive hypothesis space H, where each HKj is associated with a symmetric and positive
semi-definite Mercer kernel Kj : Xj × Xj → R. An additive RKHS is defined as
HK = {f1 + ... + fp : fj ∈ HKj , j = 1, ..., p}	(4)
with kernel norm kf k2K = inf {kf k2K + ... + kf k2K }. By integrating the Huber loss (3), additive
RKHS and kernel norm inducing regularizer into a Tikhonov regularization scheme, the regularized
Huber additive model with kernel-norm can be formulated as
p	Tp	p
fη = X fη,j = argmin	{X'σ(yt - X fj(Xj)) + ηXTjkfjIlKj},	⑸
j=1	f=Pj=1 fj ,fj ∈HKj t=1	j=1	j=1
where η is positive regularization parameter and τj is the weight for j-th kernel norm. The representer
theorem (Wahba, 1990) ensures that fη can be represented as
pT
fη = XX ɑtjKj(Xj, ∙), αtj ∈ R.
j=1 t=1
To offer the method with sparsity, we consider the following sparsity-inducing penalty
p	pT
Ω(f )：= inf {£ Tj Ilaj ∣∣2 : f = E Eatj Kj (xj, ∙)}.
j=1	j=1 t=1
Let HZ be an additive data dependent hypothesis space defined by
pT
Hz = {f = XX atj Kj (Xj, ∙∙) ： atj ∈ R}.
j=1 t=1
Then the SpHAM can be formulated as
1T	p
f = argmin{- X 'σ (y — X fj (Xj)) + λa(f)}.
f∈HZ T t=1	j=1
4
Published as a conference paper at ICLR 2022
Denote	Ktj	= (Kj (xj1, xtj), ...,	Kj (xjT, xtj))0	∈	RT,	α =	(α01, ..., α0p)0	∈	RTp	and	αj	=
(α1j, ..., αTj)0 ∈ RT, where α0j here refers to the transpose of αj for avoiding the confusion.
The SpHAM can be represented as
pT
f= XX αλj Kj (Xj ,∙)	(6)
j =1 t=1
with
1T	p	p
αλ =	argmin	{τE'σ(yt - E(Kjyaj) + 入£引电ι∣2}.	⑺
αj ∈RT ,j=1,...,p T t=1	j=1	j=1
The optimization problem (7) can be solved by Fast Iterative Shrinkage-Thresholding Algorithm
(FISTA) (Beck & Teboulle, 2009). We provide the detailed optimization procedure in Appendix E.
Remark 1. Inspired by the studies on group additive models (Yin et al., 2012; Pan & Zhu, 2017;
Chen et al., 2017), the modeling capacity of our proposed method can be improved by embedding the
(Hierarchical) group structure. For instance, group SpHAM can be easily formulated by replacing
the direct decomposition {Xj}jp=1, Xj ∈ R with the subgroups decomposition {Xd}dD=1, where each
Xd ⊂ X may cover more than one variable.
3.2 Asymptotic theory analysis
In this section, we provide the theoretical analyses of SpHAM, including the following:
•	The function approximation errors of SpHAM for stationary time series data (Theorem 1;
Section 3.2.1) and non-stationary time series data (Theorem 2; Section 3.2.2);
•	An adaptive SpHAM (inspired by above theoretical findings) and its function approximation
error for non-stationary time series (See Theorem 3; Section 3.2.3)
Due to space limitation, the high-level outline and detailed proofs are provided in the Appendix A-D.
3.2.1	Function approximation analysis for stationary time series
Through this paper, the marginal distribution w.r.t Yt is assumed to be almost everywhere supported
on [-M, M] for some M ≥ 0.
Assumption 1. Assume that |Yt|, ∀t ∈ Z, is bounded and there exists a constant c > 0 such that
E|Y t |1+c < ∞, ∀t ∈ Z.
The moment condition in Assumption 1 is rather weak in the sense that the response variable Y t
possesses infinite variance. The same condition also applies to the distributions of the innovation εt,
implying that heavy-tailed innovation is allowed.
Assumption 2. Let K = supχ∈χ，Kj(x,x) < ∞, ∀j = 1,…,p.
Assumption 2 only requires that the kernel is bounded under compact X, which holds for all Mercer
kernels (e.g., Gaussian kernel) and has been used in many learning theory literatures; see, e.g., (Wu
et al., 2006; Steinwart & Christmann, 2008; Wu & Zhou, 2008).
The following definitions are needed for Assumption 3. For any j = 1, ..., p, we define a kernel
integral operator LKj ,T+1 : L2(ρTX+1) → L2(ρTX+1) associated with the kernel Kj by
LKj,T+1 (f)(xjT+1) =	Kj (xjT+1
Xj
uj )f (uj )dρXj (uj ).
Note that LKj,T +1 is a compact and positive operator on L2(ρTX+1). According to Mercer theorem, we
can find the corresponding normalized eigenpairs {(ζij, ψij)}i≥1 such that {ψij}i≥1 is an orthonormal
basis of L2(ρTX+1) and ζij → 0 as i → ∞. Then for given r > 0, we defined the r-th power LrKj,T +1
by
LrKj,T+1(Xβijψij)=Xβij(ζij)rψij.
i≥1	i≥1
5
Published as a conference paper at ICLR 2022
Assumption 3. We assume that fj : Xj → R, ∀j = 1,...,p is a function of the form fj =
LKj,t +ι(gj) VT ∈ (0, 2] with some gj ∈ L?(ρX+1), Vh ∈ Z.
Assumption 3 is a natural extension from i.i.d setting (Assumption 1 in (Wu et al., 2006; Christmann
& Zhou, 2016; Chen & Wang, 2018)) to non-i.i.d time series. Indeed, this assumption stands in
most practical cases. For instance, if T = 0.5, the ground truth function fj needs to be a real-valued
function in the RKHS HK . The RKHS admits a large class of bounded and continuous real-valued
functions that are in Hilbert space. Moreover, this assumption has been widely used in learning
theory; please refer to (Smale & Zhou, 2003; Cucker & Zhou, 2007) for more discussions.
Theorem 1. Suppose that the process {Z t}t∞=-∞ is stationary. Let Assumptions 1-3 be true. By
taking σ = Tm, η = T- 4r and λ = T- 4r -m, we have for any 0 < δ < 1,
kf-f*kL2(ρX+1) ≤ Ce log(1∕δ)Tψ(m,c,r)
with confidence at least 1 - δ, where C is a positive constant independently of T, λ, η, δ and σ and
ψ(m GT) =	∫max{-2，m -	1，-Cm + 41r	+ m - 1},	if m ≤ 1 -	41r
,，	[max{ - 2, m -	1, -Cm + 2r	+ 2m - 2},	if m > 1 —2.
Remark 2. In the stationary case, our bound appears to be novel for the following reasons: a) the
result is completely independent of the various mixing conditions which are widely used in the theory
analysis of non-i.i.d dependent time series (Mohri & Rostamizadeh, 2009; Yang et al., 2018; Mohri
& Rostamizadeh, 2010; Guo & Shi, 2011; Qiu et al., 2015); b) compared with Yang et al. (2018), the
innovation assumption is rather weak in the sense that the innovation possesses infinite variance and
thus admits a heavy-tailed distribution.
Corollary 1. Suppose that the process {Zt}t∞=-∞ is stationary. Let all the conditions in Theorem 1
be true. We then have for any 0 < δ < 1
kf - fj kL2(ρT +ι)≤ Ceiog(1∕δ)τψ(m,c)
with confidence at least 1 - δ, where
Ψ(m,c) = [max{-1 ,-cm + m - 1J
max{m - 1, -Cm + +2m - 1},
if m ≤ 2
if m > 2.
Figure 1 summaries the convergence
rates in Corollary 1 by taking differ-
ent σ and C. If the innovation is Gaus-
sian distribution with finite variance
(i.e., Assumption 1 holds for any
C > 1), one can arbitrarily select a
σ = Tm (0 < m < 1) to obtain con-
vergence rates O(T-2). Moreover,
we can see that the convergence rate
will decrease as m increases. Com-
bined with the conclusion in Lemma
1 (i.e., the equivalence relation be-
tween Huber loss based empirical
risk and MSE as σ → ∞), it in-
dicates that σ indeed plays a trade-
off role between algorithmic robust-
ness and variance-reduction. For the
weak moment condition (e.g., 0 < Figure 1: The convergence rates under different σ and C.
C < 1), one may get slower conver-
gence rates (e.g., O(T(I-C)m-2) or
O (T(2-c)m-1)), which also coincides with our intuitive understanding that small σ may be conducive
to robust forecasting. Note that our method will not converge when σ and C are both located in the
white area in Figure 1.
6
Published as a conference paper at ICLR 2022
As a comparison, due to the non-robustness of the squared loss, the most existing convergence
rates are established under the assumption that the innovation is Gaussian distribution with finite
variance (see, e.g., Yang et al. (2018); Han et al. (2015); Wang & Yang (2007); Kock & Callot
(2015)). However, from Theorem 1 and Corollary 1, the asymptotic convergence of SpHAM can
be obtained under weaker moment condition, which verifies the robustness of our method. Recall
-2d
the learning rate O(T2d+1) derived in Yang et al. (2018), where d is the order of smoothness of
the component function fj,j = 1,…,p. Relatively slow convergence rate O(T-1) We obtained
indicates the sacrifice for the absence of mixing condition.
3.2.2	Function approximation analysis for non- stationary time series
In nonstationary time series setting, different Zt s may follow different distributions. Thus c. For
given weights {st}T=ι, we define the estimator fs = PP=ι fj as the minimizer of the following
weighted objective
Eλ (产)：=min {XX st'σ (yt- XX fj (Xj))+λΩ(f)}.
f∈ Z t=1	j=1
(8)
Next, we introduce a discrepancy measure to characterize the discrepancy between the target distribu-
tion and the distributions of observations (Kuznetsov & Mohri, 2020a).
Definition 2. For any f ∈ HZ, the discrepancy measure with respect to Huber loss is defined as
T
disc(s):= ʃsup ∣E'σ(f(xτ +1) - yτ+1) - X stE'σ(f (xt) - yt)}.
The discrepancy measures the non-stationarity of the stochastic process {Zt}t∞=-∞ with respect to
both the loss function 'σ and the hypothesis set HZ.
Theorem 2. Let Assumptions 1-3 be true. We assume that fj ∈ Hj ∀j = 1,...,p. By taking
σ = T 21c, λ = T-1 and η = T- 2, we have for any 0 < δ < 1
11
kf	f*kL2(ρT +1) ≤ disc(s) + Eλ(fs) + C1ksk2T2c + C log(1∕δ)T-1
with confidence at least 1 - δ, where Ce1, Ce2 are two positive constants independently of T, λ, η, δ
and σ.
Note that there are several existing studies towards analyzing nonstationary and non-mixing time
series, see, e.g., Kuznetsov & Mohri (2020a). Different from them, the error bound we derived is
with respect to the function approximation rather than generalization risk, which is very crucial for
Huber regression problem, since the convergence of generalization risk cannot imply the convergence
of function approximation (Sun et al., 2019; Feng & Wu, 2020).
3.2.3 Adaptive sparse Huber additive model for nonstationary time series
Theorem 2 illustrates that we shall minimize the following optimization problem for non-stationary
time series forecasting:
Tp
mHn {X st'σ(yt- X fj (Xj)) + disc(S) + λ1Cf) + λ2T蚩 ksk2},
f∈ Z t=1	j=1
where λ1 and λ2 are two positive regularization parameters, and c is a positive constant introduced
in Assumption 1. Although the discrepancy measure disc(s) is crucial for such an optimization
problem, we cannot obtain its exact value since we do not have access to the distributions of Zt , t ∈ Z.
Hence, we need to estimate the approximated discrepancy from given data. Inspired by Kuznetsov &
Mohri (2020a), one natural and necessary assumption is that there exists an underlying representation
relationship between distribution ρT +1 and distributions ρt, t = 1, ..., T.
7
Published as a conference paper at ICLR 2022
Assumption 4. Denote by a probability set q* = {qJ= }T=ι with PT=I qt = 1 .We assume that the
following term is sufficiently small:
T
disc(q*) ：= sup [E'σ(f(XT +1) - yτ +1) - Xq⅛E'σ(f(χt) -yt)].
f∈HZ	t=1
Note that the priori q* can be any distribution, which shall be given empirically according to the
trend of the time series. For instance, in a particular scenario of the distribution ρτ+1 does not change
drastically compared with the distributions ρt, t = 1, ..., T, Kuznetsov & Mohri (2020a) have proven
that the Assumption 4 holds if the s* is an uniform distribution over last l > 0 observations, where l
is a hyper-parameter that can be tuned in practical applications.
Theorem 3. Let Assumption 4 and the conditions in Theorem 2 be true. We have for any 0 < δ < 1
τ
kfs - f*kLgT+1) ≤ sup X(q*-st)'σ(f(χt)-yt)+ Eλ(fs)
2 X	f∈HZ t=1
+Cι log(1∕δ)(kq* - sk2 + ksk2T泰)+ C log(1∕δ)T-1
with confidence at least 1 - δ, where Ce1, Ce2 are two constants independently of T, λ, η, δ and σ.
Finally, the optimization problem of adaptive SpHAM can be formulated as following two stages:
Step A: finding the weight S:
τ
S = argmin{	SUp X(q*	-St)'σ(f(Xt)- yt)	+	λι∣∣q*	-	s∣∣2	+	λ2T2c∣∣sk2}	(9)
s f∈HZ t=1
Step B: forecasting:
pτ
fs(XT+1)=XX ɑj K (Xj,xτ+I),
j=1 t=1
where
Tp	p
αs =	arg min	{£ ^t'σ (yt - E(Kjyaj) + λ^τj ∣αj ∣2}.	(10)
αj∈RT,j=1,...,p t=1	j=1	j=1
The optimization problems (9) can be solved by standard gradient descent method. Similarly to
the strategy for optimization problem (7), we use Fast Iterative Shrinkage-Thresholding Algorithm
(FISTA) (Beck & Teboulle, 2009) for Step B. We provide the detailed procedure in Appendix E.
4	Experiment
This section validates the effectiveness of SpHAM and adaptive SpHAM. In all experiments, the
GaUssian kernel Kj (u, V) = exp(- ku-vk2), where j = 1,...,p and bandwidth d > 0, is employed
for constructing the additive data dependent hypothesis space. Due to limited space, the evaluations
on real-world data are provided in Appendix F. We consider two synthetic examples as below:
Example A:	Inspired by (KUznetsov & Mohri, 2020a), a stationary time series is generated according
to the non-linear additive aUtoregressive model:
Yt = 3 sin(2Yt-2) - sin(2Yt-3) + 2εt,
where the innovation εts are i.i.d. drawn from GaUssian distribUtion N(0, 0.5) and StUdent distribUtion
with degree of freedom 2, respectively.
Example B:	Inspired by (KUznetsov & Mohri, 2020a), a time series with smooth drift is generated by
Yt = -ɪ Sin(Yt-1) + 1εt,
400	1	/	2 ,
8
Published as a conference paper at ICLR 2022
where the distributions of noise εt are the same as above.
Hyper-parameter selection and evaluation criterions: Recall that SpHAM algorithm requires three
hyper-parameters: regularization parameter λ, bandwidth of kernel d and Huber parameter σ. We set
these parameters according to the suggestions in our Theorems. Based on the suggestion in Theorem
1-3, the selection of HUber parameter σ is σ = T418 and the regularization parameter is λ = T-1.
Moreover, we set the bandwidth d = 0.5 and tune l ∈ {100, 150, 200, ..., 350}. The evaluation
criterions for forecasting used here contains Average Sample Error(ASE)=
N qp=fι-yy
and True Deviation (TD)=
N √Pf?-f *(χt))2, where N is the number of test samples.
For each example, we generate time series with 4000 sample points. The samples at time t =
{1500, 1501, ..., 1899} are used as a training set, and the samples at next time t = {1900, ..., 1999}
are considered as the test data. The competitors include Simple Exponential Smoothing (SES) ,
TS-SpAM (Yang et al., 2018) and Vanilla Long Short-Term Memory (LSTM). The tuning parameters
of other competing methods such as LSTM and SES, are chosen according to their original python
packages. All the evaluations are repeated for 50 times. The average results for Examples A-B are
presented in Table 2.
From Table 2, the results on Example A verify that our SpHAM is competitive with other approaches
based on square loss under Gaussian noise, and performs better in presence of heavy-tailed t noise.
Moreover, the results on Example B shows the promising performance of our adaptive SpHAM for
non-stationary time series forecasting.
Table 2: The results on synthetic data.
Gaussian noise	Student noise
	Methods	ASE (Std)	TD (Std)	ASE (Std)	TD (Std)
	SES	O.114(士.008)	O.O98(±.005)	O.493(±.313)	O.1O5(±.009)
	LSTM	0.063(±.005)	0.022(±.005)	O.538(±.342)	O.219(±.089)
Example A	TS-SpAM	0.070(±.007)	O.O35(±.008)	O.5O5(±.332)	O.131(±.042)
	SpHAM (ours)	O.O76(土.005)	O.O42(±.008)	0.492(±.333)	0.095(±.006)
	SES	O.435(士.015)	O.424(±.015)	O.4O2(±.080)	O.366(±.017)
	LSTM	0.120(±.125)	O.114(±.127)	O.247(±.125)	O.181(±.088)
Example B	TS-SpAM	O.114(±.022)	O.1O5(±.021)	O.173(±.125)	O.1O4(±.051)
	SpHAM (ours)	O.115(±.028)	O.112(±.029)	O.179(±.116)	O.O96(±.026)
	Adaptive SpHAM (ours)	0.102(士.026)	0.096(±.026)	0.172(±.118)	0.085(±.024)
5 Conclusion
We propose an adaptive sparse Huber additive model by integrating Huber loss and '2,1 -norm
regularizer into an additive data dependent hypothesis space. We theoretically explore the asymptotic
properties of our method for both non-Gaussian and (non)stationary time series. Experimental results
on both synthetic and real-world data validate the effectiveness of the proposed method.
Acknowledgments
This work is supported by the Major Science and Technology Innovation 2030 “New Generation Ar-
tificial Intelligence” key project (No. 2021ZD0111700) and the National Natural Science Foundation
of China under Grant No. 12071166. We sincerely appreciate the anonymous ICLR reviewers for
their helpful comments.
References
Terrence M. Adams and Andrew B. Nobel. Uniform convergence of vapnik-chervonenkis classes
under ergodic sampling. TheAnnaIsofProbability, 38(4):1345-1367, 2010.
Alekh Agarwal and John C. Duchi. The generalization ability of online algorithms for dependent
data. IEEE Transactions on Information Theory, 59(1):573-587, 2013.
9
Published as a conference paper at ICLR 2022
Rishabh Agarwal, Nicholas Frosst, Xuezhou Zhang, Rich Caruana, and Geoffrey E. Hinton. Neural
additive models: Interpretable machine learning with neural nets. arXiv:2004.13912v1, 2020.
Richard T. Baillie. Long memory processes and fractional integration in econometrics. Journal of
Econometrics ,73(1):5-59,1996.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM Journal on Imaging Sciences, 2(1):183-202, 2009.
Bart Bussmann, Jannes Nys, and Steven Latre. Neural additive vector autoregression models for
causal discovery in time series data. arXiv:2010.09429v1, 2020.
John M. Chambers and Trevor Hastie. Statistical Models in S. Wadsworth & Brooks/Cole Advanced
Books & Software, 1992.
Hong Chen and Yulong Wang. Kernel-based sparse regression with the correntropy-induced loss.
Applied and Computational Harmonic Analysis, 44(1):144-164, 2018.
Hong Chen, Xiaoqian Wang, Cheng Deng, and Heng Huang. Group sparse additive machine. In
Advances in Neural Information Processing Systems (NIPS), pp. 198-208. 2017.
Hong Chen, Changying Guo, Yingjie Wang, and Huijuan Xiong. Sparse additive machine with ramp
loss. Analysis and Applications, 19(3):509 - 528, 2021a.
Hong Chen, Yingjie Wang, Feng Zheng, Cheng Deng, and Heng Huang. Sparse modal additive
model. IEEE Transactions on Neural Networks and Learning Systems, 32(6):2373 - 2387, 2021b.
Andreas Christmann and Ding-Xuan Zhou. Learning rates for the risk of kernel based quantile
regression estimators in additive models. Analysis and Applications, 14(3):449-477, 2016.
Tianjiao Chu and Clark Glymour. Search for additive nonlinear time series causal models. Journal of
Machine Learning Research, 9:967-991, 2008.
Benoit Colson, Patrice Marcotte, and Gilles Savard. An overview of bilevel optimization. Annals of
Operations Research, 153(1):235-256, 2007.
Felipe Cucker and Ding-Xuan Zhou. Learning Theory: An Approximation Theory Viewpoint.
Cambridge University Press, 2007.
Francesca Dominici, Aidan McDermott, Scott L. Zeger, and Jonathan M. Samet. On the use of
generalized additive models in time-series studies of air pollution and health. American Journal of
Epidemiology, 156(3):193-203, 2002.
Paul Doukhan. Mixing: Properties and Examples. Lecture Notes in Statistics. Springer, 1994.
Jianqing Fan and Irene Gijbels. Local Polynomial Modelling and Its Applications. Chapman and
Hall, 1996.
Jianqing Fan and Qiwei Yao. Nonlinear time series: Nonparametric and parametric methods.
Springer, 2005.
Yunlong Feng and Qiang Wu. A statistical learning assessment of huber regression.
arXiv:2009.12755v1, 2020.
Zheng-Chu Guo and Lei Shi. Classification with non-i.i.d. sampling. Mathematical and Computer
Modelling, 54(5):1347-1364, 2011.
Fang Han, Huanran Lu, and Han Liu. A direct estimation of high dimensional stationary vector
autoregressions. Journal of Machine Learning Research, 16:3115-3150, 2015.
Trevor J. Hastie and Robert J. Tibshirani. Generalized additive models. London: Chapman and Hall,
1990.
Fengxiang He and Dacheng Tao. Recent advances in deep learning theory. arXiv preprint
arXiv:2012.10931, 2020.
10
Published as a conference paper at ICLR 2022
Jianhua Z. Huang and Lijian Yang. Identification of non-linear additive autoregressive models.
Journal of the Royal Statistical Society. Series B (Statistical Methodology), 66(2):463-477, 2004.
Peter J. Huber. Robust estimation of a location parameter. The Annals of Mathematical Statistics, 35
(1):73-101, 1964.
Peter J. Huber and Elvezio M. Ronchetti. Robust Statistics. Wiley, 2009.
Egide Kalisa, Sulaiman Fadlallah, Mabano Amani, Lamek Nahayo, and Gabriel Habiyaremye.
Temperature and air pollution relationship during heatwaves in birmingham, uk. Sustainable Cities
and Society, 43:111-120, 2018.
Kirthevasan Kandasamy and Yaoliang Yu. Additive approximations in high dimensional nonpara-
metric regression via the SALSA. In International Conference on Machine Learning (ICML), pp.
69-78, 2016.
Anders Bredahl Kock and Laurent Callot. Oracle inequalities for high dimensional vector autoregres-
sions. Journal of Econometrics, 186(2):325-344, 2015.
Vitaly Kuznetsov and Mehryar Mohri. Generalization bounds for time series prediction with non-
stationary processes. In International Conference on Algorithmic Learning Theory, 2014.
Vitaly Kuznetsov and Mehryar Mohri. Discrepancy-based theory and algorithms for forecasting
non-stationary time series. Annals of Mathematics and Artificial Intelligence, 88:367-399, 2020a.
Vitaly Kuznetsov and Mehryar Mohri. Learning theory and algorithms for forecasting non-stationary
time series. In Advances in Neural Information Processing Systems (NIPS), 2020b.
Guodong Liu, Hong Chen, and Heng Huang. Sparse shrunk additive models. In International
Conference on Machine Learning (ICML), 2020.
Jiajia Liu, Yudong Ye, Chenglong Shen, Yuming Wang, and R Erdelyi. A new tool for cme arrival
time prediction using machine learning algorithms: Cat-puma. The Astrophysical Journal, 855(2):
109-118, 2018.
Po-Ling Loh. Statistical consistency and asymptotic normality for high-dimensional robust mestima-
tors. The Annals of Statistics, 45(2):866-896, 2017.
Lukas Meier, Sara Van De Geer, and Peter Buhlmann. High-dimensional additive modeling. The
Annals of Statistics, 37(6B):3779-3821, 2009.
Mehryar Mohri and Afshin Rostamizadeh. Rademacher complexity bounds for non-i.i.d. processes.
In Advances in Neural Information Processing Systems (NIPS), 2009.
Mehryar Mohri and Afshin Rostamizadeh. Stability bounds for stationary φ-mixing and β-mixing
processes. In Advances in Neural Information Processing Systems (NIPS), 2010.
George Ruchathi Mwaniki, Chelsea Rosenkrance, H. Will Wallace, B. Tom Jobson, Mathew H.
Erickson, Brian K. Lamb, Rick J. Hardy, Rasa Zalakeviciute, and Timothy M. VanReken. Factors
contributing to elevated concentrations of pm2.5 during wintertime near boise, idaho. Atmospheric
Pollution Research, 5(1):96-103, 2014.
Chao Pan and Michael Zhu. Group additive structure identification for kernel nonparametric regres-
sion. In Advances in Neural Information Processing Systems (NIPS), pp. 4907-4916. 2017.
Jiangtao Peng, Luoqing Li, and Yuan Yan Tang. Maximum likelihood estimation-based joint sparse
representation for the classification of hyperspectral remote sensing images. IEEE Transactions on
Neural Networks and Learning Systems, 30(6):1790-1802, 2019.
Huitong Qiu, Sheng Xu, Fang Han, Han Liu, and Brian Caffo. Robust estimation of transition
matrices in high dimensional heavy-tailed vector autoregressive processes. In Proceedings of the
32nd International Conference on Machine Learning, volume 37, pp. 1843-1851, 2015.
11
Published as a conference paper at ICLR 2022
Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning: random averages,
combinatorial parameters, and learnability. In Advances in Neural Information Processing Systems
(NIPS),pp.1984-1992, 2010.
Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Sequential complexities and uniform
martingale laws of large numbers. Probability Theory and Related Fields, 161(1):111-153, 2015.
Garvesh Raskutti, Martin J. Wainwright, and B Yu. Minimax-optimal rates for sparse additive models
over kernel classes via convex programming. Journal of Machine Learning Research, 13(2):
389-427, 2012.
Pradeep Ravikumar, Han Liu, John Lafferty, and Larry Wasserman. SpAM: sparse additive models.
Journal of the Royal Statistical Society: Series B, 71:1009-1030, 2009.
Khaiwal Ravindra, Preety Rattan, Suman Mor, and Ashutosh Nath Aggarwal. Generalized additive
models: Building evidence of air pollution, climate change and human health. Environment
International, 132:104987, 2019.
Meir Ron. Nonparametric time series prediction through adaptive model selection. Machine Learning,
39(1):5-34, 2000.
Jakob Runge, Sebastian Bathiany, Erik M. Bollt, Gustau Camps-Valls, Dim Coumou, Ethan R Deyle,
Clark Glymour, Marlene Kretschmer, MigUel D. Mahecha, Jordi Munoz-Mari, Egbert H. van
Nes, Jonas Peters, Rick Quax, Markus Reichstein, Marten Scheffer, Bernhard Scholkopf, Peter L.
Spirtes, George Sugihara, Jie Sun, Kun Zhang, and Jakob Zscheischler. Inferring causation from
time series in earth system sciences. Nature Communications, 10, 2019.
Stephen Smale and Ding-Xuan Zhou. Estimating the approximation error in learning theory. Analysis
and Applications, 01:17-41, 2003.
Qiongxia Song and Lijian Yang. Oracally efficient spline smoothing of nonlinear additive autore-
gression models with simultaneous confidence band. Journal of Multivariate Analysis, 101(9):
2008-2025, 2010.
Ingo Steinwart and Andreas Christmann. Support Vector Machines. Springer Science and Business
Media, 2008.
Charles J. Stone. Additive regression and other nonparametric models. The Annals of Statistics, 13
(2):689-705, 1985.
Qiang Sun, Wen-Xin Zhou, and Jianqing Fan. Adaptive huber regression. Journal of the American
Statistical Association, 115(529):254-265, 2019.
Pham Dinh Tao and Le Thi Hoai An. A d.c. optimization algorithm for solving the trust-region
subproblem. SIAM Journal on Optimization, 8:476-505, 1998.
Grace Wahba. Spline models for observational data. Society for Industrial and Applied Mathematics,
1990.
Li Wang and Lijian Yang. Spline-backfitted kernel smoothing of nonlinear additive autoregression
model. The Annals of Statistics, 35(6):2474 - 2503, 2007.
Xiaofeng Wang and Donald E. Brown. The spatio-temporal generalized additive model for criminal
incidents. In Proceedings of 2011 IEEE International Conference on Intelligence and Security
Informatics, pp. 42-47, 2011.
Xiaoqian Wang, Hong Chen, Weidong Cai, Dinggang Shen, and Heng Huang. Regularized modal
regression with applications in cognitive impairment prediction. In Advances in Neural Information
Processing Systems (NIPS), 2017a.
Yimin Wang, Jiajia Liu, Ye Jiang, and Robert Erdelyi. Cme arrival time prediction using convolutional
neural network. The Astrophysical Journal, 881(11):15, 2019.
12
Published as a conference paper at ICLR 2022
Yingjie Wang, Hong Chen, Feng Zheng, Chen Xu, Tieliang Gong, and Yanhong Chen. Multi-task
additive models for robust estimation and automatic structure discovery. In Advances in Neural
Information Processing Systems (NIPS), 2020.
Yulong Wang, Yuan Yan Tang, and Luoqing Li. Correntropy matching pursuit with application to
robust digit and face recognition. IEEE Transactions on Cybernetics, 47(6):1354-1366, 2017b.
Kam Chung Wong, Zifan Li, and Ambuj Tewari. Lasso guarantees for β-mixing heavy-tailed time
series. The Annals of Statistics, 48(2):1124-1142, 2020.
Qiang Wu and Ding Xuan Zhou. Learning with sample dependent hypothesis spaces. Computers and
Mathematics with Applications, 56(11):2896-2907, 2008.
Qiang Wu, Yiming Ying, and Ding-Xuan Zhou. Learning rates of least-square regularized regression.
Foundations of Computational Mathematics, 6(2):171-192, 2006.
Qiang Wu, Yiming Yin, and Ding-Xuan Zhou. Multi-kernel regularized classfiers. Biometrika, 23:
108-134, 2007.
Wenjun Yan, Marcus A. Worsley, Thang Pham, Alex Zettl, Carlo Carraro, and Roya Maboudian.
Effects of ambient humidity and temperature on the no2 sensing characteristics of ws2/graphene
aerogel. Applied Surface Science, 450:372-379, 2018.
Yingxiang Yang, Adams Wei Yu, Zhaoran Wang, and Tuo Zhao. Detecting nonlinear causality in
multivariate time series with sparse additive models. arXiv:1803.03919v2, 2018.
Junming Yin, Xi Chen, and Eric P Xing. Group sparse additive models. In International Conference
on Machine Learning (ICML), pp. 871-878, 2012.
Bin Zou, Luoqing Li, and Zongben Xu. The generalization performance of erm algorithm with
strongly mixing observations. Machine Learning, 75(3):275-295, 2009.
13
Published as a conference paper at ICLR 2022
A Proof sketch
This section sketches the proof focusing on the conceptual aspects. The full proofs are provided in
Appendix B-D.
Figure 2: The important ingredients for Theorem 1.
Figure 2 summaries the important ingredients for the proof of Theorem 1. Lemma 1 describes the
properties off and Lemma 2 illustrates the relation between huber-based risk and MSE. Following
these two lemmas and previous works (Wang et al., 2017a; Feng & Wu, 2020), We then decompose
the function error into three important parts: sample error, hypothesis error and approximation error.
To bound the sample error, We need to employ the sequential Rademacher complexity (Kuznetsov &
Mohri, 2020a), which is the key to measuring the capacity of the data-dependent hypothesis space
for non-i.i.d data. Lemmas 3-4 give an important concentration inequality and the upper bound
of sequential Rademacher complexity, respectively. Finally, Theorem 1 is obtained by combining
Propositions 2-4. The proof of Theorems 2 is similar to the above process, except that we develop a
different error decomposition in Proposition 5. On the basis of Theorem 2, by applying an assumption
on data distribution, we derive Theorem 3.
The full proofs of Theorems 1-3 are provided in Sections C-E, respectively.
B Proof of Theorem 1
-c-rτ r` , ∙11	,	1	, ∙	a %
We first illustrate two key properties of f .
Lemma 1. Let Assumptions 1-2 be true. From the definition of f in Eq. (6), there hold
kfk∞ ≤ Mf= λT mjL T
and
≤ κ-2Mf
2M 2
λT minj=1,...,p T
where M is a positive constant such that |Y t | ≤ M, ∀t ∈ Z.
Proof. Denote by ET (f ):= PT=I 'σ (f (X⑴)一y⑴)for notational convenience. From the definition
of f, we know that
ET(f)+ λΩ(f) ≤ETσ(0).
It means that
p
Ω(f) = X T kɑλk2 ≤ λ-1 ET (0).
j=1
14
Published as a conference paper at ICLR 2022
From the definition of Huber loss (3), We obtain
1工 小 C
ET(0) = T E'σ(y㈤)≤ σ2, if |y⑴∣≤ σ,∀t = 1,...,T,
t=1
and
1 T
ET (0) = T 寸 σ (y⑴)= 2σ∣y㈤ ∣- σ2 ≤ 2σM - σ2, if ∣y ㈤ ∣ > σ, ∀t =1,...,T.
Therefore,
JL	2 M 2
X TjH 2 ≤	.
According to the property of RKHS, We conclude that
P	P
k∕k∞ ≤ KkfK ≤ KtX kfj 员 ≤ κ2 X |喏口2 ≤
2κ2M 2
λτ minj=ι,...,P Tj
This completes the proof by denoting Mf
2κ2M 2
λT minj = 1,...,p Tj
□
Let HZ be a data-dependent hypothesis space constrained by sparsity-inducing regularizer Ω(∙), i.e.,
_	p t	p	2 M 2
HZ = {f(X)=X X a Kj (xj ,xj)： X Tjk %k2 ≤ λT minj=ι,..,P Tj }.
For any h ∈ Z, denote by E,T+1(∕) := E'σ (f(xτ +1)-yτ +1) and ET +1(∕) := E(f(xτ +1)-yτ +1)2,
respectively. In the following, we describe the relationship between ET+1(f) and ET +ι(f), ∀f ∈ HZ.
Note that a similar proof is given in (Feng & Wu, 2020) for the i.i.d case.
Lemma 2. Let Assumptions 1-2 be true. For any h ∈ Z and f ∈ HZ ,there holds
∣[ET+1(f) -ET+ι(f *)] - [Et +ι(f) -Et +ι(f *)]∣ ≤ M,
where both c and MC = 23+c(M∕* + M^ + 1)2E(∣YT+1∣1+c) are positive constants.
Proof. For any σ > max{M + M ^, 1}, we denote two events Iγ and IIγ as follows
Iγτ+ι = {YT +1 : ∣YT +1∣ ≥ σ∕2}
and
IIγτ+ι = {YT +1 : ∣YT +1∣ < σ∕2}.
Then, for any f ∈ HZ and ∣∣f*k∞ = Mf* < ∞, we have
∣[ET+1(f) -ET+ι(f*)] - [Et+1(f) - ET+ι(f*)]∣
=∣ [〃,(f(χr +1) - yT +1)dρτ+1 - 口。(f *(χτ +1) - yτ +1)dρτ+1] - f - f* ||；2(成 +i)∣
=∖[fχfy 'σ (f(xT+1) - yτ+1) - 'σ (f *(xT+1) - yτ+1)dρT+1dρX+1] - kf - f *kL2(ρT+1) ∣
≤ ∣ / /	'σ(f(χτ+1) - yτ+1) - 'σ(f*(χτ+1) - yτ+1)dρT+1dρX+11
I JX JIYT +1 ∪IIγT+1	I
+∖[ Z	(f (χτ+1) - yτ+1)2 - (f *(χτ+1) - yτ+1)2dρT+1dρX+1 ∣
JX JIYT +1 UIIYT + 1
≤ ∖J'J'ι	'σ (f (xT+1) - yτ+1) - 'σ (f *(χτ+1) - yτ+1)dρT+1dρX+11
+∖fjι	(f(χτ+1) - yτ+1)2 - (f *(χτ+1) - yτ+1)2 dρT+XdρT+1 .
15
Published as a conference paper at ICLR 2022
The last inequality is based on the fact that
|yT +1 - f (xτ +1)I < ∣yτ +11 + kfk∞ < σ, ∀(xτ +1,yτ +1) ∈ X × IIyT+1
and hence
0σ(f(χτ+1) - yτ+1) = (f(χτ +1) - yτ +1)2.
Recalling that the Huber loss is 2σ-Lipschitz continuous, then We get
∖ jχ jι	0σ (f (χτ+1) - yτ+1) -0σ (f *(XT +1) - yτ +1)dρT+1dρX+1 ∣
≤2σ 11 I	∣f(X)- f乂叫时"网+1 ∣
JX JIYT +1
≤ 2σkf - f*k∞Prob{Iγ}.
According to Markov’s inequality, for any constant c > 0, we further have
Prob{IγT+1} = Prob{∣YT +1∣ ≥ σ∕2} = Prob{∣YT +1∣1+c ≥ (σ∕2)1+c} ≤ "十。E(IYT +1∣1+c)
σ1+c
Then we have
IL /	/σ (f (Xτ+1) - yτ+1) - 'σ (f *(XT +1) - yτ +1)dρT+1dρX+1
≤ 22+ckf - f *k∞E(∣yT +1∣1+c)
.
σc
Moreover, the second term in the right-hand side can be bounded by
I / /	(f(χτ +1) - yτ +1)2 - (f*(XT+1) - yτ+1)2dρT+1dρX+1
JX JIYT+1
≤ f - f *k∞
≤ kf - f *k∞
LL	∣2yτ +1 - f(xT +1) - f*(XT+1)∣dρTl+⅛X+1
h /	2∣yτ +1∣dρT+1 + (kfk∞ + kf*k∞)Prob(IyT+1：
JIYT +1
According to Holder inequality, We have
/	∣yT+1∣dρT+1 ≤ (Prob(IYT+1))⅛(E|YT +1∣1+c)⅛ ≤
IY T +1
Then We can deduce that
)]∙
2c E(∣Y T +1∣1+c)
σc
kf - f*k∞ [ /	2∣yτ +1∣dρT+1 + (kf k∞ + kf*k∞)Prob(IγT+1)]
IY T +1
≤
(22+c + 21+c)f - f *∣∣∞E(∣Yt +1∣1+c) + 21+c(f k∞ + f *∣∣∞)2E(IyT +1 ∣1+c)
σc
σ1+c
Finally, there holds
I[(00(f(Xτ+1) -yτ+1) - JzQ(f*(XT+1) -yτ+1)] - kf -f*kL2(ρT+1) I < M
With
Mc = 23+c(Mf* + M^ + 1)2E(∣yT +1 ∣1+c).
This completes the proof.
□
Define a stepping-stone function with respect to the distribution PT+1:
p	p
fη,τ+1 =Efn,τ +1,j = arg min E0σ(f (xT +1) - YT +1) + n£TjkfjIlK3-.
j=1	f ∈Hκ	j=1
To establish the bound of function approximation for stationary time series setting, we make the
following error decomposition.
16
Published as a conference paper at ICLR 2022
Proposition 1. Let Assumptions 1-2 be true. For any f ∈ HZ ,there holds
kf - f*kL2(ρX+1) ≤ E1 + E2 + E3 + 2Mcc,
where
E1 = {ΕT+1 (f)-ET(f)} + {ET(fη,τ +1) -ET+ι(fη,τ +1)}
p
E2 = ET +ι(fη,τ +1) -ET +1 (f*)+ η∑>jk裁T +ι,jkKj
j=1
and
p
E3 = {ET (f) + λΩ(f) -ET (fη) -ηj>j kfη,jkKj}.
j=1
Proof. According to Lemma 2, for any f ∈ HZ, We can make following error decomposition
kf - f*kL2(ρτ +1) = ET+1 ⑺-ET +1(f *) ≤ ET+1(f) -ET+1(f *) + σ
={Eτ+ι(f)—ET (f)}+{ET (fη,τ+1)—ETσ+ι(fη,τ+ι)}+{ET (f)—ET (fη,τ+ι)}
V-------------------------{z---------------------}
E1
+ET+ι(fη,T+1) - ET+ι(f *) + σ
p
≤ EI + ET+1(fτ*,T +1) - ET +1(f *) + η X TjkfnZ,T+1,jkKj +{ET (f) + λα(f) — ET (fη*,T +1)
j=1
S---------------------V---------------------}
E2
一 η X Tjkf*,T +1,jkKj} + -Ccr
j=1
p	2M
≤ EI + E2 + {ET (f) + λα(f) - et (fn) — η X Tjkfn,jkKj} +^^σC~
、---------------------------------V------------------}
E3
2Mc
≤ E1 + E2 + E3 + —C .
σc
□
In statistical machine learning community, we call E1, E2 and E3 sample error, approximation
error and hypothesis error, respectively. The sample error E1 describes the divergence between
the empirical risk ETσ (f) and the expected risk Eσ (f). The hypothesis error E2 characterizes the
difference between the empirical regularized risks with HK and HZ . The approximation error
measures the approximation ability of RKHS HK to H.
B.1 Sample Error
In this section, we focus on providing the bound of sample error E1. Unfortunately, the traditional
tools for complexity analysis such as covering number (Ron, 2000; Guo & Shi, 2011), Rademacher
complexity (Mohri & Rostamizadeh, 2009)) and concentration inequalities ((Wu et al., 2007)), cannot
be applied into this non-i.i.d. setting directly. To solve this problem, we employ the sequential
Rademacher complexity developed in (Rakhlin et al., 2010; Kuznetsov & Mohri, 2020a). We define a
function-based random variable as
ξf (Z)= 'σ (f (x) - y) - 'σ fη,T +1(x) - y),f ∈ HZ .
Now, we turn to establish the bound of
1T
Eξf (ZT+1) - T Eξf(zt), ∀f ∈Hz.
t=1
17
Published as a conference paper at ICLR 2022
A necessary ingredient needed for our analysis is data-dependent sequential Rademacher complexity
Rakhlin et al. (2010), which we review in the following. We adopt the following definition of a
complete binary tree: a Z-valued complete binary tree v is a sequence (v1, ..., vT) of T mappings
vt : {±1}t-1 → Z, t ∈ [1, T]. A path in the tree is γ = (γ1, ..., γT-1) ∈ {±1}T-1. To simplify the
notation, we will write vt(γ) instead vt(γ1, ..., γt-1), even though vt depends only on the first t - 1
elements of γ . The following definition generalizes the classical notion of Rademacher complexity
to sequential setting.
Definition 3. The sequential Rademacher complexity RT (G) of a function class G is defined by
1T
RT(G) =SUpE[sup TNYtg(Vt(γ))],
where the supremum is taken over all complete binary trees of depth T with values in Z and γ is a
sequence of Rademacher random variables.
Based on the definition of sequential Rademacher complexity, we have the following concentration
inequality:
Lemma 3. Suppose that the time series {Z t}t∞=-∞ is strictly stationary. For any δ > 0, with
probability at least 1 一 δ, the following inequality holds for all f ∈ HZ and all a > 0
T
E-	1 L ɪ	1	,----
Ef (Z + ) ≤ 亍£&(Z ) + √T +6M'σ Vz 4π log TRT (G) +
t=1	T
where G := {'σ (f,z) + 'σ (f, z) ： f ∈ H Z} and M'σ is a positive constant such that 'σ (f (x(t))—
y(t)) ≤ M'σ for any f ∈ HZ and t ∈ Z.
According to the property of sequential Rademacher complexity (see Proposition 14 in (Rakhlin et al.,
2015)), we have
RT(G) = RT('σ oHz,Z) ≤ 16b(1 + 4V2log3/2(eT2))Rτ(HZ)	(11)
with the 2σ-Lipschitz constant of 'σ(∙).
Lemma 4. Under Assumptions 1-2, there holds
G (C / 32b[1 + 4V2log3/2(eT2)]M2p2K
RT(G) ≤---------------不---------------.
Proof. In fact, to every kernel Kj , we can associate a feature map φ with inner product <
φ(xj), φ(x0j) >= K(xj, x0j) for any xj, x0j ∈ Xj and j = 1, ..., p. Then the constrained data-
dependent hypothesis space HZ can be rewritten as
pT	p
H Z = {f (x)= XX
αijφ(xj(t))φ(xj) : Xτjkαjk2 ≤
2M2
λTminj=1,..,p Tj
18
Published as a conference paper at ICLR 2022
By direct computation, we have
RT (H Z )
T pT
sup Eσ [ sup -X σt(Xχ ɑij φ(xji))φ(xjt)(σ)) - y(t))]
{≈t,yt}T=ι	α∈Rτp T t=ι j = ι i=ι
1	T pT	1	T
— sup Eσ sup X σt XX aijφ(xji))φ(xjt)(σ)) - - sup Eσ X σty(t)(σ)
T mt)}T=ι α∈RTp t=ι j = ι i=ι	T {y⑺}=1 t=1
1	T pT
— sup Eσ supPX σt Xx αij φ(χji))φ(Xjt) (σ))
1	T pT
斤 sup Eσ sup Xσt XXα∙ijK(Xji),X(t)(σ))
T {X(t)}T=l	α∈RTp M j=1 M
1T
于 sup Eσ sup α V"σtKt(σ), Kt(σ) = (K(x11),x1t)(σ)),...,K(XpT),xPt)(σ))0∈ RTp
T {x(ty}T=ι	α∈Rτp	t=ι
≤
T SUp ka0k sup
{xt}T=ι
T
Eσk XσtKt(σ)k2
t=1
≤
≤
2M2
~TΓ sup
T	{比⑴}T=1
T
Eσk X σtKt(σ)k2
t=1
2M2
~Tf2~ SUP ʌ
T {比⑴}T=ι ∖
T
Eσ [ X σtσsK0t(σ)Ks (σ)]
s,t=1
2M2
~TΓ sup ∖
T {比⑴}T=1 ∖
T
Eσ[X K0t(σ)Kt(σ)]
t=1
≤
2M 2p 2 K
T
Thus, combining above result with Eq. (11). We get the desirable result.
□
Combining the results in Lemma 4 with the inequality in Lemma 3, we then obtain the bound of
sample error E1 for the strictly stationary time series setting.
Proposition 2. Under Assumptions 1-2, for any δ > 0,thefollowing inequality holdsfor all f ∈ HZ
and all a > 0	______
1	Z-------- M'σ √81og 1
Ei ≤ √T + 6M', √4∏ log TR + —√T—
with probability at least 1 - δ, where
r_ 32σ[1+4V21og3/2(eT2)]M2p2K
B.2	Approximation Error
Since the output function f T+1is the optimal estimator in RKHS HK for time T + 1, the learning
rates of the learning algorithm indeed depend on the approximation ability of the hypothesis space HK
with respect to the optimal risk E(f *) measured by the approximation error E2. For any j = 1,…,p
and h ∈ Z, we first define the kernel integral operator LKj,T +1 : L2(ρTX+1) → L2(ρTX+1) associated
with the kernel Kj by
LKj,T+1(f)(XjT+1) =	Kj(XjT+1
Xj
uj)f(uj)dρTXj+1(uj),∀h∈Z.
19
Published as a conference paper at ICLR 2022
Note that LKj,T +1 is a compact and positive operator on L2(ρTX+1). According to Mercer theorem,
we can find the corresponding normalized eigenpairs {(ζhj,i, ψhj ,i)}i≥1 such that {ψhj ,i}i≥1 is an
orthonormal basis of L2(ρTX+1) and ζhj,i → 0 as i → ∞. Then for given r > 0, we defined the r-th
power LrKj ,T+1 by
LrKj,T +1 (X βhj,iψhj,i) = Xβhj,i(ζhj,i)rψhj,i.
i≥1	i≥1
We introduce an intermediate function as follows:
-	-τ	--lτ	上*
fη,T +1,j = (LKj,T +1 + ητjI) LKj,T+1fj , ∀j = 1, ..., p.
Lemma 5. Under Assumption 3,for the intermediate function frι,τ +ι,j defined above, there holds
Ekfη,τ +ι,j(XT +1) - fj(xT +1)k + ητjkfη,τ +ι,j(XT +1)kKj ≤ 2(g产5焉① +f*k2∙
Proof. Under Assumption 3, for any h ∈ Z, we know that fj = LrK h(gj∖) for some g工 j =
Pi≥1βh,iψhj,i ∈ L2(ρTXj+1). Then we have
fj
=LrKj,T+1(	βh,iψhj,i)=	(ζhj,i)rβh,iψhj,i
i≥1	i≥1
The case r = 2 means each fj lies in the RKHS H,κj. Then we have
f	j _ L JfnT I]-1L 产 f* _ X nTj (Zh,i)rβh,iψh,i
fη,h,j - fj = (LKj,h + ητj I ) LKj ,hfj - fj = ʌ,	Tj^	.
i≥1	ζi + ητj
Then we have
kfη,τ +ι,j- f；k2	=
≤
Similarly, we also have
(X ητjj βh,i )2 = (ητ )2r X( jητ- )2-2r (7rζhΓ- )2r (βh,i)2
i≥1 ζh,i + ητj	i≥1 ζh,i + ητj	ζh,i + ητj
(ητj)2r X(βhj,i)2 = (ητj)2rkL-Krj,T+1fjjk22.
i≥1
ητ kfη,τ+I,jk2	= (ητ )2 X(j+r )2(βh,i)2 = (ητ )2r X( 7j^~ )2+2r (7jj )2-2r j
i≥1	ζh,i	+ ητj	i≥1	ζh,i	+	ητj	ζh,i	+	ητj
≤	(ητj)2rkL-Krj,T+1fjjk22.
□
Proposition 3. Under Assumptions 3, there holds
pp
E2=E(fηj,T+1)-E(fj)+ηXτjkfηj,T+1,jk2K ≤ (p + 1) X(ητj)2rkL-Krj,T+1fjjk22.
j=1	j=1
Proof. According to Lemma 1, we have
p
ET +1 (fηj,T +1) - ET+1(fj) + η X τj kfηj,T +1,j k2K
j=1
pM
≤ ET+ι(fη,τ +1) - ETσ+ι(f *) + ηXTjkfj,τ +ι,jkK + U
j=1	σ
pM
≤ ET+ι(fη,τ +1)-ET+i(f*) + ηEτjkfη,τ +ι,jIlK + ^σf
j=1	σ
p	2M
≤ Eτ +ι(fη,τ +1) - Eτ +ι(f ) + ηEτjkfη,τ +ι,jIlK +
j=1	σ
20
Published as a conference paper at ICLR 2022
Then we have
p
ET +1(fη,T +1)- ET +1(f *) + η∑2τj kfη,T +1,j IlK =
j=1
≤
Combing above results, we get the desired result.
≤
≤
p
kfη,T +1 - f*∣∣2 + η XTj kfη,τ +ι,j kK
j=1
pp
kX[fη,h,j-fj*]k2+ η X Tj kfη,h,j kK
j=1	j=1
pp
X Pkfη,T +1,j - fj*k2 + n X Tjkfη,T +1,j IlK
j=1	j=1
p
(P +1) X(nτj )2rkLκr,τ+ιfj k2.
j=1
□
B.3	Hypothesis Error
This section focuses on bounding the hypothesis error
p
E3 = {ET (f) + λΩ(f) -ET (fη ) - n£ kfη,j kK}.
j=1
We first give the properties of f and then use them to bridge f and fη .
Lemma 6. For all j = 1, ..., P, there holds
1	uX	工	"-	2σ
Tjkaηk2 =/t X('σ(yt -XKjtan))2 ≤ nT2
Proof. Recall the represent theorem which ensures that
pT
fη = XX aj Kj (XjQ, a2j ∈ R.
j=1 t=1
For notation simplicity, denote ajη = (a1ηj, ..., aηTj)0 ∈ RT and αη = ((a1η)0, ..., (apη)0)0 ∈ RTp.
From the definition of Eq. (7), we deduce that
1T	p	p
αη = argmin{亍X 'σ (yt- X Kjtaj)+ η X Tj (ajη)0Kjaj },
α∈RTp T t=1	j=1	j=1
where Kj = {K(xjs, xij)}in,s=1 ∈ RT×T. we then have
1T	p
T X 'σ(yt - X Kjtaj)Kji = nτjKjan.
t=1	j=1
It is easy to deduce that
1p	p
a = -1T ('σ (yι - X KjIan),../a- X Kj T a ))0.
ηTjT	j=1	j=1
Then we obtain that, for any j = 1, ..., P,
1	uɪ	工	"-	2σ
Tjkank2 = nTt X('σ(yt - X Kjtan))2 ≤ ηT2
This completes the proof.	□
21
Published as a conference paper at ICLR 2022
Proposition 4. Under Assumptions 3, there holds
p
E3 ≤ M(fn) = λfτ kαj l∣2
j=1
V 2λpσ
ηT 2
Proof.
p
E3 = {ET(f) + λΩ(f) - ET(fη)-n£ ∣fη,j∣K}
j=1
p
=ET(f) + λΩ(f) -ET(fη) - λΩ(f) - n£ ||九∣K + λΩ(f)
j=1
._ , O .
≤ λΩ(fη)
Combining the above inequality with Lemma 6, we get that
p
E3 ≤ λ3fη) = λEτj忖k2 ≤ 二 1 -
j=1	nT2
□
Proof of Theorem 1: Combining Propositions 1-4, for Tη ≤ 1, we have with confidence 1 - δ
kf -f*%ST +1) ≤ Clog(1∕δ)(T-2 + T-1σ + n2r + λn-1T-2σ + σ-cλ-2T-2+ σ-cλ-1T-1),
β
where C is a positive constant independently of T, λ, η, δ, σ . By taking σ = Tm , η = Tβ and
λ = Tγ, we then have with confidence 1 - δ
kf - f *%(PT +1) ≤ Celog(1∕δ)Tw(m，e，Y，c，r),
where
Ψ(m, β, γ, c, r) = max{-ɪ, m — 1, 2rβ, Y - ɪ - β + m, -Cm — 2γ — 2, -Cm — Y _ 1}.
By taking σ = *, β
Ψ(m, c, r)
一 4r and Y =-金-蚩 Direct computation shows that
_ (max{-2, 2c - 1},	if m ≤ 1 - 4r
[max{ - 2, m — 1, —Cm + 2r + 2m _ 2}, if m > 1 — 4r.
This completes the proof.
C Proof of Theorem 2
Denote by s = {st}tT=1 a probability set with PtT=1 st = 1. We define following functions associated
with the probability set s:
p
fT+1 = X fT +1,j = arg fmn E'σ (f (XT +1) - yT +1),
j=1	f K
pT
fT,s = X fTj = arg min X st'σ (J(XD-yt)
j=1	f∈HKt=1
fη=Xfη,j = argmin	{χst'σ(yt - Xfj(Xj))+ηxTjkfjkκj,
j=1	f =Pjp=1 fj ,fj ∈HKj t=1	j=1	j=1
and
fs = X J = arg min	{X St'σ (yt - X fj (xj )) + λΩ(f)}.
j=1	f =Pjp=1 fj ,fj ∈HKj t=1	j=1
Correspondingly, We denote by ETs(f) = PT=I st'σ(f (xt) - yt).
22
Published as a conference paper at ICLR 2022
Proposition 5. Let Assumptions 1-2 be true. For any f ∈ HZ ,there holds
kf -f *kL2(ρτ+1)≤ EI+E2+E3+ET (fT)+η X TjkfTjk Kj + ^σcc,
X	j=1	σ
where
Ei= ET+ι(fs)-ET,s(fs)
p
E2=ET,s(fs)+λΩ(fs) -ET,s (fη) - η x Tj kfη,jkκj
j=1
and
p
E3 = ET +ι(f*,T+1) -ET+ι(f *)+η x Tj kfη,T+ι,j ι∣Kj.
j=1
Proof. According to Lemma 2, for any f ∈ HZ, We can make following error decomposition
kf - f*kL2(ρX+1)
=ET +1(f ) - ET +1(f *)
≤ Eτ+ι(f) — Eτ+ι(f*) + ^σc
≤ {Eτ+ι(f) -Eτ,s(f)} +{Eτ,s(f) -Eτ,s(fτ,s)}+Eτ,s(fτ,s)+Eτ+ι(fη,τ+1)-Eτ+ι(f*)
V------V--------}
E1
十η X Tjkfn∖τ+ι,jk κj∙} + ~~∑^
pp
≤ Ei + {ET,s(f) + λΩ(f) -ETe(fT,s) - η X TjkfTjkKj} + ETe(fT,s) + η X TjkfTj kKj
j=1	j=1
+ET+ι(fn,T+1) — ETσ+ι(f *)+η X Tjkf;,丁+ι,j kκj} + ~σc
j=1	σ
pp
≤ Ei + {ET,s(f) + λΩ(f)-ET,s(fs)-ηXTjkfn,jkKj} +ET,s(fT,s) + ηXTjkfTjkKj
j=1	j=1
S------------------V---------------}
E2
+ETσ+ι(fni,T+1)—ET+ι(f *)+η X Tj kfn,T+ι,j kKj}+σ^c-
j=i	σ
p	p	2M
≤ E1 + E2 + ET+l(fj,T+l) - ET +1(f *)+ η X Tj kfn,T+1 ,jkKj +ET,s(fT,s)+ η X TjkfTjkKj + ~σ^
j=1	j=1	σ
V----------------V-----------------}
E3
≤	E1 + E2 + E3 + ET,s(fT,s)+ η X TjkfTjk Kj + ~~~
j=1	σ
p	2M
≤	E1	+	E2	+	E3	+ fmHn	{ET,	(f)	+ λQ(f)} + η X TjkfTjkKj	+ ~σ∑~.
Z	j=1
□
Following the generalization analysis in (Kuznetsov & Mohri, 2020a), we need to discrepancy
measure to measure the discrepancy of the non-stationarity of the stochastic process {Zt}t∞=-∞ with
respect to both the loss function 'σ and the hypothesis set HZ.
23
Published as a conference paper at ICLR 2022
Definition 4. The discrepancy describes the discrepancy between target distribution and the distribu-
tion of the sample. For any f ∈ HHZ, the discrepancy measure with respect to Huber loss is defined
as
T
disc(s):= sup ∣E'σ (f (xT +1) - yτ+1) - X Est'σ (f (xt)
f∈HZ	t=1
- yt) .
Lemma 7. For any δ > 0, with probability at least 1 - δ, the following inequality holds for all
f ∈ THz and all α > 0
T
E'σ (f(xT +1)-yT +1) ≤ X St'σ (f(xt)-yt)+disc(s) + ks∣∣2 + 6M'σ p4∏ log TRT(G )+M'σl∣S∣∣2
t=1
where G := {'σ (f,z) : f ∈ THZ} and M'σ is a positive constant such that 'σ (f(xt) — yt) ≤ M'σ
for any f ∈ THZ and t ∈ Z.
Proof of Theorem 2 The proofs of bounding errors E1, E2 and E3 proceeds similarly to the proof of
Theorem 1 and are omitted for brevity.
According to Lemma 3, we have
E1 ≤ disc(s) + IlsIl2 + 6M'σ p4∏ logTR + M'σl∣s∣∣2 y8log 1,
where
R = ks∣∣2[32σ[1 + 4√2log3S(eT2)]M 2p 2 K].
Moreover, from Proposition 4, we can get
p
E3 ≤ (p + 1)X(ητj)2rkLκr,τ +1fjkl
j=1
Similarly, to bound the hypothesis error
p
E2 = {ET,s(fs) + λΩ(fs I-ETS (否―ηf kfη,jkK },
j=1
we first give the properties of f and then use them to bridge f and fη .
Lemma 8. For all j = 1, ..., p, there holds
E2 ≤ 2λPσks∣∣2
一η .
Proof. Recall the represent theorem which ensures that
pT
fη = XXαjsKj(xj,∙), αjs∈ R.
j=1 t=1
For notation simplicity, denote αjη,s = (α1ηj,s, ..., αηT,js)0 ∈ RT and αη,s = ((αη1,s)0, ..., (αpη,s)0)0 ∈
RTp . We then deduce that
αη,s = argmin{X st'σ(yt - X Kjtaj) + ηXTj(aj)0Kjaj},
α∈RT p t=1	j=1	j=1
where Kj = {K(xjs, xij)}in,s=1 ∈ RT×T. We get
Tp
X st'σ(yt - XKjtj)Kji = ητjKjaj,s.
t=1	j=1
24
Published as a conference paper at ICLR 2022
It is easy to deduce that
1p	p
αη,s =-L(sι'σ(y1- X KjIan),…,ST'σ (y -X Kj0Tαjη))0.
ητj	j=1	j=1
Then we obtain that, for any j = 1, ..., p,
Tjkank2 = 1 t Xs2('σ(yt -XKjtaj))2 ≤ 2σkS2
η t=1	j=1	η
This completes the proof.	口
Under Assumptions 1-2, there holds
p
E2 = {et (f)+λΩ(fs) -ET (fn)- ηf kfn,jkK}
j=1
p
=ET (fs)+λΩ(fs) -ET (fn) - λΩfn) - n£ ∣fn,jkK+λΩf
j=1
p
≤ λΩ(fns) = λfτ kan,sk2
j=1
≤	2λpσksk2
一 η .
By combining the above results, we have with confidence 1 - δ
k产-f*%(pX+1)
≤ disc(s) + min {ET,s(f) + λΩ(f)}
f∈HZ
+C log(1∕δ)(ksk2 + ksk2σ + η2r + λη-1ksk2σ + σ-cλ-2T-2 + σ-cλ-1T-1 + η),
1	1	1 . 1	. . 1 1	1 .1	1	.	1-
where C is a positive constant independently of T, λ, η, δ, σ and p. By taking σ = T 2c, λ = T 1
and η = T-1, we completes the proof.
D Proof of Theorem 3
According to the definition of disc(T + 1), we have
disc(s)
SUP ⅛'σ (f(xT +1)
f∈HZ
T
-yT+1) - XstE'σ(f(χt)-yt)}
t=1
≤
TT
sup [X q⅛E'σ(f(xt) -yt) - X stE'σ(f(xt) -yt)] + disc(q*).
f∈HZ t=1	t=1
Furthermore, it is easy to deduce that
TT
sup X(q⅛ - st)E'σ(f(χt) - yt) - sup X(qt -St)'σ(f (χt) - yt)
f∈HZ t=1	f∈HZ t=1
T
≤ sup X(q⅛ - st)[E'σ(f (χt) - yt) - 'σ(f (χt) - yt)].
f∈HZ t=1
Proof of Theorem 3 According to the proof of Theorem 1 in Kuznetsov & Mohri (2020a), we have
Z .一 ɪ ɪ •一―1	..... .........1	1
SUP 5T(qt-st)[E'σ(f(X )-y )-'σ(f(x )-y )] ≤ —≡+6M'σVZ4πlogTRT(G)+M'σkq -sk2∖∕8log^
f∈HZ t=1	T σ	σ	δ
25
Published as a conference paper at ICLR 2022
By combining above results, we have with confidence 1 - δ
kfs - f *M+1)
T
≤ disc(q*) + SUp £3 - st)'σ(f (xt) — yt) + min {Eys(f) + λΩ(f)}
f∈HZt=1	f∈HZ
+Celog(1∕δ)(∣∣q* - s∣∣2 + ks∣∣2 + ∣∣s∣∣2σ + η2r + λη-lksk2σ + σ-cλ-2T-2 + σ-cλ-1TT + η).
By taking σ = T*,λ = TT and η = T- 1, we complete the proof.
Algorithm 1: Optimization procedure for adaptive SpHAM
Input: Data {(xt , yt )}tT=1, Max-Iter Z ∈ Z, Mercer kernel Kj, j = 1, ..., p with bandwidth d,
Weights τι,l = 1,…,p, q*.
Initialization: Lipschitz constant L, s0 .
Step A: Computing weights s:
for z = 1, ..., Z do
1. Compute AzT via DC-Programming (or gradient descent method);
_ 2. Update Sz via (12).
Output: S = sZ.
Step B: Computing fs:
for z = 1, ..., Z do
1): Compute
2): mz+1 =
αz = pL(βz) via (14);
1+√1=4mZ .
3): βz+1 = αz +
2	;
I (αz - αz-1).
Output： αs = αz;
Prediction function: fs = Pp=ι PT=I OjKj(xj, ∙);
Variable selection: {j : kαjs k2 ≥ v, j = 1, ..., p}.
E Experiment Optimization
The optimization problem (10) reduces to problem 7 when taking s* = TIT. Recall the optimization
problem in Step A:
T	λ	λ1
S = argmin{ SUp Egt- St)'σ(f(Xt) - y )+ 年kq - sk2 + VkSk2T2c}.
s f∈HZt=1 t	2	2	2	2
The above optimization problem can be equivalently rewritten as a common type of bilevel optimiza-
tion problem (Colson et al., 2007), i.e,
Outer problem: mins PT=1(q* - st)'σ(fs(xt) — yt) + λ"∣q* -s∣∣2 + λ2T1/2CkSk2,
Inner problem: fs = arg maXf ∈h pT=1(q* - st)'σ (f(xt) — yt),
where the outer (min) problem parameterized by S, is nested within the inner (max) problem. The
outer problem can be solved by standard gradient descent, where in each step, we need to optimize
the inner (max) problem with last updated S.
We denote by k the iteration time. Then for k + 1-th iteration, we have the following gradient update
rule
Sk+1 = Sk - γ(Ak - λι(q* - Sk) + λ2T泰Sk),	(12)
where γ is learning rate,
Tp	Tp	0
Ak = -('σ (XX αsj Kj (Xj,χ1) -y* 1 *),∙∙∙,'σ (XX αsj Kj (Xj,xT ) - yT )) ∈ RT
t=1 j=1	t=1 j=1
26
Published as a conference paper at ICLR 2022
and
αsk = (αs1k1,...,αsTk1,...,αs1kp,...,αsTkp)0 ∈RTp
is obtained by the following weighted optimization problem
T	Tp
αsfc = arg max X⑷一Sk)'σ (X X αmjKj (x7,xj) -yt).
t=1	m=1 j=1
Note that the inner problem is subjected to f ∈ HK, i.e., kαk2 = Pjp=1 τj kαj k2 ≤
2M 2
λT minj=1,...,p Tj
A widely-used method for solving this constrained inner problem is DC programming (Tao & An,
1998). For simplicity, We here transform this inner problem into a regularized problem with '2-norm
regularizer, and solve this regularized problem by standard gradient method. After obtaining the
solution S, we turn to solve the following weighted optimization problem in Step B:
Tp	p
as =	arg min	{£ St'σ (yt - E(Kjyaj) + 人£ Tj ∣∣αj ∣∣2}.	(13)
αj∈RT,j=1,...,p t=1	j=1	j=1
We can see that this problem contains non-smooth function λ Pjp=1 τj ||aj ||2 which makes the
standard gradient descent method inapplicable. To conquer this challenge, we leverage fast iterative
shrinkage-thresholding algorithm (FISTA)(Beck & Teboulle, 2009). Our optimization problem
becomes
where
ak = pL(ak-1) :
t=1
))
(Kt)0ak-1)||2
(14)
—
and
2((Kt)0a-yt)(Kjt)0	|(Kt)0a - yt | <σ
2σ
-2σ
(Kt )0a - yt ≥ σ
(Kt )0a - yt ≤ -σ
and L = max T PT=1 k(Kt)0Ktk is the lipschitz constant T PT=I V'σ(yt - (Kt)0ak-1). Denote
Proj(s) as a projection of s and v > 0 as the threshold value. To focus on the weights of samples
which are really useful for forecasting, we consider Proj(s) as a box projection such that St >
qt, ∀t = 1,…,p). Finally, the optimization procedure for adaptive SPHAM can be summarized in
Algorithm 1. Note that if we only run Step B with ^ = T1, ∀t = 1,…，T, we can further obtain the
optimization procedure for SpHAM.
Remark 3. The computational complexity of Algorithm 1 depends on the optimization strategy for
DC programming, the training size T, dimension p and the iteration times Z. We denote by the
computational complexity of DC programming O(DC(T, p)). Then, the computational complexity of
Step A is O(ZDC(T,p) + ZT) and the computational complexity of Step B is O(ZT3p3). Thus, the
total computational complexity of Algorithm 1 is O(Z O(DC) + ZT3p3). For large scale data, we
can further speed up Algorithm 1 by random Fourier features technique [Rahimi and Recht 2007],
which is leaved for future work. This has been carefully discussed in the revised manuscript.
F	Additional Experiment
F.1 Evaluation on benchmark data
We test our algorithm on nonlinear dataset from CauseMe. The hyper-parameter selection is the same
as the one in Section 4. The results in Table 3 verify the effectiveness of our method.
27
Published as a conference paper at ICLR 2022
Table 3: The ASE on CaUseMe data (P refers to the dimension of features).
Methods	(P = 3,T = 300)	(p=5,T=300)
LSTM	0.7681	0.9230
TS-SpAM	0.7782	0.9485
SpHAM	0.7548	0.9484
F.2 Experiments on Air Quality dataset
We use the Air Quality dataset obtained from UCI Machine Learning Repository (https://
archive.ics.uci.edu/ml/datasets/Air+quality) to test our model’s ability to detect
the Granger causality. This dataset includes 9358 hourly air quality data in an Italian city, collected
from March 2004 to February 2005. The details of the dataset can be obtained on the UCI website.
The Granger causal network we detect is shown in Figure 3. Given the network, we observe that
the temperature (T) influences ozone (O3) and nitrogen dioxide (NO2) which is validated in Kalisa
et al. (2018). Mwaniki et al. (2014) confirm the relationship between relative humidity (RH) and
nitrogen dioxide (NO2). Yan et al. (2018) verify the relationship between humidity and nitrogen
dioxide (NO2).
F.3 Experiments on Coronal Mass Ejections dataset
Coronal Mass Ejections (CMEs) are the most violent eruptions in the Solar System. Despite
machine learning approaches have been applied to these tasks recently Wang et al. (2019); Liu
et al. (2018), there is no any work for interpretable prediction with Granger causal network. CMEs
data are provided in The Richardson and Cane List (http://www.srl.caltech.edu/ACE/
ASC/DATA/level3/icmetable2.htm). From this link, we collect 152 ICMEs observations
from 1996 to 2016. The features of CMEs are provided in SOHO LASCO CME Catalog (https:
//cdaw.gsfc.nasa.gov/CME_list/). In-situ solar wind parameters can be downloaded
from OMNIWeb Plus (https://omniweb.gsfc.nasa.gov/). A total of9 features are chosen
as input, including: (1) Central PA (CPA), (2) Angular Width, (3) three approximated speeds ( Linear
Speed, 2nd-order Speed at final height,and 2nd-order Speed at 20 Rs), (4) Mass, (5) Kinetic Energy,
(6) MPA and (7)CMEs arrival time. Figure 4 shows that Granger causal network when the output
is CMEs Arrival time. Some interesting findings are concluded from this Granger causal network.
For instance, Speed and Mass, as the significant variables causing CMEs arrival time, have been also
screened out in Liu et al. (2018). Morover, the CMEs Angular Width does not cause the CMEs arrival
time forecasting, while Liu et al. (2018) state that they have a significant correlation. This indicates
that the CMEs arrival time is affected by the Angular Width at current time, but not by the historical
Angular Width.
28
Published as a conference paper at ICLR 2022
Figure 3: The Granger causal network on Air Quality dataset.
29
Published as a conference paper at ICLR 2022
Figure 4: The Granger causal network on CMEs dataset.
30