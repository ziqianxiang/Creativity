Published as a conference paper at ICLR 2022
NASI: Label- and Data-agnostic Neural
Architecture Search at Initialization
Yao Shu, Shaofeng Cai, Zhongxiang Dai, Beng Chin Ooi & Bryan Kian Hsiang Low
Department of Computer Science, National University of Singapore
{shuyao,shaofeng,daizhongxiang,ooibc,lowkh}@comp.nus.edu.sg
Ab stract
Recent years have witnessed a surging interest in Neural Architecture Search (NAS).
Various algorithms have been proposed to improve the search efficiency and the
search effectiveness of NAS, i.e., to reduce the search cost and improve the general-
ization performance of the selected architectures, respectively. However, the search
efficiency of these algorithms is severely limited by the need for model training
during the search process. To overcome this limitation, we propose a novel NAS al-
gorithm called NAS at Initialization (NASI) that exploits the capability of a Neural
Tangent Kernel in being able to characterize the performance of candidate archi-
tectures at initialization, hence allowing model training to be completely avoided
to boost the search efficiency. Besides the improved search efficiency, NASI also
achieves competitive search effectiveness on various datasets like CIFAR-10/100
and ImageNet. Further, NASI is shown to be label- and data-agnostic under mild
conditions, which guarantees the transferability of the architectures selected by our
NASI over different datasets.
1	Introduction
The past decade has witnessed the wide success of deep neural networks (DNNs) in computer
vision and natural language processing. These DNNs, e.g., VGG (Simonyan & Zisserman, 2015),
ResNet (He et al., 2016), and MobileNet (Howard et al., 2017), are typically handcrafted by human
experts with considerable trials and errors. The human efforts devoting to the design of these DNNs
are, however, not affordable nor scalable due to an increasing demand of customizing DNNs for
different tasks. To reduce such human efforts, Neural Architecture Search (NAS) (Zoph & Le, 2017)
has recently been introduced to automate the design of DNNs. As summarized in (Elsken et al., 2019),
NAS conventionally consists of a search space, a search algorithm, and a performance evaluation.
Specifically, the search algorithm aims to select the best-performing neural architecture from the
search space based on its evaluated performance via the performance evaluation. In the literature,
various search algorithms (Luo et al., 2018; Zoph et al., 2018; Real et al., 2019) have been proposed
to search for architectures with comparable or even better performance than the handcrafted ones.
However, these NAS algorithms are inefficient due to the requirement of model training for numerous
candidate architectures during the search process. To improve the search inefficiency, one-shot NAS
algorithms (Pham et al., 2018; Dong & Yang, 2019; Liu et al., 2019; Xie et al., 2019) have trained
a single one-shot architecture and then evaluated the performance of candidate architectures with
model parameters inherited from this fine-tuned one-shot architecture. So, these algorithms can
considerably reduce the cost of model training in NAS, but still require the training of the one-shot
architecture. This naturally begs the question whether NAS is realizable at initialization such that
model training can be completely avoided during the search process? To the best of our knowledge,
only a few empirical efforts to date have been devoted to developing NAS algorithms without model
training (Mellor et al., 2021; Park et al., 2020; Abdelfattah et al., 2021; Chen et al., 2021).
This paper presents a novel NAS algorithm called NAS at Initialization (NASI) that can completely
avoid model training to boost search efficiency. To achieve this, NASI exploits the capability of
a Neural Tangent Kernel (NTK) (Jacot et al., 2018; Lee et al., 2019a) in being able to formally
characterize the performance of infinite-wide DNNs at initialization, hence allowing the performance
of candidate architectures to be estimated and realizing NAS at initialization. Specifically, given
1
Published as a conference paper at ICLR 2022
the estimated performance of candidate architectures by NTK, NAS can be reformulated into an
optimization problem without model training (Sec. 3.1). However, NTK is prohibitively costly to
evaluate. Fortunately, we can approximate it1 using a similar form to gradient flow (Wang et al.,
2020) (Sec. 3.2). This results in a reformulated NAS problem that can be solved efficiently by a
gradient-based algorithm via additional relaxation using Gumbel-Softmax (Jang et al., 2017) (Sec.
3.3). Interestingly, NASI is even shown to be label- and data-agnostic under mild conditions, which
thus implies the transferability of the architectures selected by NASI over different datasets (Sec. 4).
We will firstly empirically demonstrate the improved search efficiency and the competitive search
effectiveness achieved by NASI in NAS-Bench-1Shot1 (Zela et al., 2020b) (Sec. 5.1). Compared
with other NAS algorithms, NASI incurs the smallest search cost while preserving the competitive
performance of its selected architectures. Meanwhile, the architectures selected by NASI from the
DARTS (Liu et al., 2019) search space over CIFAR-10 consistently enjoy the competitive or even
outperformed performance when evaluated on different benchmark datasets, e.g., CIFAR-10/100
and ImageNet (Sec. 5.2), indicating the guaranteed transferability of architectures selected by our
NASI. In Sec. 5.3, NASI is further demonstrated to be able to select well-performing architectures
on CIFAR-10 even with randomly generated labels or data, which strongly supports the label- and
data-agnostic search and therefore the guaranteed transferability achieved by our NASI.
2	Related Works and Background
2.1	Neural Architecture Search
A growing body of NAS algorithms have been proposed in the literature (Zoph & Le, 2017; Liu
et al., 2018; Luo et al., 2018; Zoph et al., 2018; Real et al., 2019) to automate the design of
neural architectures. However, scaling existing NAS algorithms to large datasets is notoriously hard.
Recently, attention has thus been shifted to improving the search efficiency of NAS without sacrificing
the generalization performance of its selected architectures. In particular, a one-shot architecture is
introduced by Pham et al. (2018) to share model parameters among candidate architectures, thereby
reducing the cost of model training substantially. Recent works (Chen et al., 2019; Dong & Yang,
2019; Liu et al., 2019; Xie et al., 2019; Chen & Hsieh, 2020; Chu et al., 2020) along this line have
further formulated NAS as a continuous and differentiable optimization problem to yield efficient
gradient-based solutions. These one-shot NAS algorithms have achieved considerable improvement
in search efficiency. However, the model training of the one-shot architecture is still needed.
A number of algorithms are recently proposed to estimate the performance of candidate architectures
without model training. For example, Mellor et al. (2021) have explored the correlation between the
divergence of linear maps induced by data points at initialization and the performance of candidate
architectures heuristically. Meanwhile, Park et al. (2020) have approximated the performance of
candidate architectures by the performance of their corresponding Neural Network Gaussian Process
(NNGP) with only initialized model parameters, which is yet computationally costly. Abdelfattah et al.
(2021) have investigated several training-free proxies to rank candidate architectures in the search
space, while Chen et al. (2021) intuitively adopt theoretical aspects in deep networks (e.g., NTK (Jacot
et al., 2018) and linear regions of deep networks (Raghu et al., 2017)) to select architectures with a
good trade-off between its trainability and expressivity. Our NASI significantly advances this line
of work in (a) providing theoretically grounded performance estimation by NTK (compared with
(Mellor et al., 2021; Abdelfattah et al., 2021; Chen et al., 2021)), (b) guaranteeing the transferability
of its selected architectures with its provable label- and data-agnostic search under mild conditions
(compared with (Mellor et al., 2021; Park et al., 2020; Abdelfattah et al., 2021; Chen et al., 2021))) and
(c) achieving SOTA performance in a large search space over various benchmark datasets (compared
with (Mellor et al., 2021; Park et al., 2020; Abdelfattah et al., 2021)). Based on our results, Shu et al.
(2022) recently have provided further improvement on existing training-free NAS algorithms.
2.2	Neural Tangent Kernel (NTK)
Let a dataset (X, Y) denote a pair comprising a set X of m n0-dimensional vectors of input features
and a vector Y ∈ Rmn×1 concatenating the m n-dimensional vectors of output values, respectively.
1More precisely, we approximate the trace norm of NTK.
2
Published as a conference paper at ICLR 2022
Let a DNN be parameterized by θt ∈ Rp at time t and output a vector f(X; θt) ∈ Rmn×1 (ab-
breviated to ft) of the predicted values of Y. Jacot et al. (2018) have revealed that the training
dynamics of DNNs with gradient descent can be characterized by an NTK. Formally, define the NTK
Θt(X,X) ∈ Rmn×mn (abbreviated to Θt) as
Θt(X, X) , Vθtf(X; θt) Vθtf(X; θt)> .	(1)
Given a loss function Lt at time t and a learning rate η, the training dynamics of the DNN can then
be characterized as
Vtft = -ηΘt(X,X) VftLt,	VtLt = -ηVftLt> Θt(X,X)VftLt .	(2)
Interestingly, as proven in (Jacot et al., 2018), the NTK stays asymptotically constant during the
course of training as the width of DNNs goes to infinity. NTK at initialization (i.e., Θ0) can thus
characterize the training dynamics and also the performance of infinite-width DNNs.
Lee et al. (2019a) have further revealed that, for DNNs with over-parameterization, the aforemen-
tioned training dynamics can be governed by their first-order Taylor expansion (or linearization) at
initialization. In particular, define
flin(x; θt) , f (x; θo) + Vθ0 f (x; θo)>(θt - θo)	(3)
for all x ∈ X. Then, f(x; θt) and flin(x; θt) share similar training dynamics over time, as described
formally in Appendix A.2. Besides, following the definition of NTK in (1), this linearization flin
achieves a constant NTK over time.
Given the mean squared error (MSE) loss defined as Lt , m-1 kY - f(X; θt)k22 and the constant
NTK Θt = Θ0, the loss dynamics in (2) above can be analyzed in a closed form while applying
gradient descent with learning rate η (Arora et al., 2019):
Lt = m-1 Pim=n1(1 - ηλi)2t(ui>Y)2 ,	(4)
where Θ0 = Pim=n1 λi(Θ0)uiui>, and λi(Θ0) and ui denote the i-th largest eigenvalue and the
corresponding eigenvector of Θ0, respectively.
3	Neural Architecture Search AT Initialization
3.1	Reformulating NAS via NTK
Given a loss function L and the model parameters θ(A) of architecture A, we denote the training
and validation loss as Ltrain and Lval, respectively. NAS is conventionally formulated as a bi-level
optimization problem (Liu et al., 2019):
min∕ Lvai(θ* (A); A)
s.t. θ*(A) , argminθ(A) Ltrain(θ(A); A).
(5)
Notably, model training is required to evaluate the validation performance of each candidate architec-
ture in (5). The search efficiency of NAS algorithms (Real et al., 2019; Zoph et al., 2018) based on (5)
is thus severely limited by the cost of model training for each candidate architecture. Though recent
works (Pham et al., 2018) have considerably reduced this training cost by introducing a one-shot
architecture for model parameter sharing, such a one-shot architecture still requires model training
and hence incurs training cost.
To completely avoid this training cost, we exploit the capability of a NTK in characterizing the per-
formance of DNNs at initialization. Specifically, Sec. 2.2 has revealed that the training dynamics of
an over-parameterized DNN can be governed by its linearization at initialization. With the MSE loss,
the training dynamics of such linearization are further determined by its constant NTK. Therefore,
the training dynamics and hence the performance of a DNN can be characterized by the constant
NTK of its linearization. However, this constant NTK is computationally costly to evaluate. To this
end, we instead characterize the training dynamics (i.e., MSE) of DNNs in Proposition 1 using the
trace norm of NTK at initialization, which fortunately can be efficiently approximated. For simplicity,
we use this MSE loss in our analysis. Other widely adopted loss functions (e.g., cross entropy with
softmax) can also be applied, as supported in our experiments. Note that throughout this paper, the
parameterization and initialization of DNNs follow that of Jacot et al. (2018). For a L-layer DNN,
We denote the output dimension of its hidden layers and the last layer as nι = •… = nL-ι = k and
nL = n, respectively.
3
Published as a conference paper at ICLR 2022
Proposition 1. Suppose that kxk2 ≤ 1 for all x ∈ X and Y ∈ [0, 1]mn fora given dataset (X, Y) of
size |X | = m, a given L-layer neural architecture A outputs ft ∈ [0, 1]mn as predicted labels of Y
with the corresponding MSE loss Lt, λmin(Θ0) > 0 for the given NTK Θ0 w.r.t. ft at initialization,
and gradient descent (or gradient flow) is applied with learning rate η < λ-m1ax (Θ0). Then, for any
t ≥ 0, there exists a constant c0 > 0 such that as k → ∞,
Lt ≤ mn2(1 - ηλ(Θo))q + E	(6)
with probability arbitrarily close to 1 where q is set to 2t if t < 0.5, and 1 otherwise, λ(Θo)，
(mn)-1 Pm=n1 λi(Θo), and E，2copn∕(mk)(1 + c0Pl∕k).
Its proof is in Appendix A.3. Proposition 1 implies that NAS can be realizable at initialization.
Specifically, given a fixed and sufficiently large training budget t, in order to select the best-performing
architecture, we can simply minimize the upper bound of Lt in (6) over all the candidate architectures
in the search space. Since both theoretical (Mohri et al., 2018) and empirical (Hardt et al., 2016)
justifications in the literature have shown that training and validation loss are generally highly related,
we simply use Lt to approximate Lval. Hence, (5) can be reformulated as
min∕ mn2(1 - ηλ(Θ°(A))) + E s.t. λ(Θ0(A)) < η-1.	(7)
Note that the constraint in (7) is derived from the condition η < λ-m1ax(Θ0(A)) in Proposition 1, and
η and E are typically constants2 during the search process. Following the definition of trace norm, (7)
can be further reduced into
maxA kΘ0(A)ktr	s.t. kΘ0(A)ktr < mnη-1 .	(8)
Notably, Θ0(A) only relies on the initialization of A. So, no model training is required in optimizing
(8), which achieves our objective of realizing NAS at initialization.
Furthermore, (8) suggests an interesting interpretation of NAS: NAS intends to select architectures
with a good trade-off between their model complexity and the optimization behavior in their model
training. Particularly, architectures containing more model parameters will usually achieve a larger
kΘ0 (A)ktr according to the definition in (1), which hence provides an alternative to measuring the
complexity of architectures. So, maximizing kΘ0(A)ktr leads to architectures with large complexity
and therefore strong representation power. On the other hand, the complexity of the selected architec-
tures is limited by the constraint in (8) to ensure a well-behaved optimization with a large learning
rate η in their model training. By combining these two effects, the optimization of (8) naturally trades
off between the complexity of the selected architectures and the optimization behavior in their model
training for the best performance. Appendix C.1 will validate such trade-off. Interestingly, Chen et al.
(2021) have revealed a similar insight of NAS to us.
3.2	Approximating the Trace Norm of NTK
The optimization of our reformulated NAS in (8) requires the evaluation of kΘ0(A)ktr for each
architecture A in the search space, which can be obtained by
kΘo(A)ktr = Pχ∈χ ∣∣Vθ0(A)f(x,θo(A))∣∣F ,	(9)
where ∣∣ ∙ ∣∣f denotes the Frobenius norm. However, the Frobenius norm of the Jacobian matrix in
(9) is costly to evaluate. So, we propose to approximate this term. Specifically, given a γ-Lipschitz
continuous loss function3 Lx (i.e., ∣Vf Lx ∣2 ≤ γ for all x ∈ X),
γ-1 Vθ0(A)Lx2 =γ-1 VfLx>Vθ0(A)f(x,θ0(A))2 ≤ Vθ0(A)f(x,θ0(A))F .	(10)
The Frobenius norm Vθ0(A)f(x, θ0(A))F can thus be approximated efficiently using its lower
bound in (10) through automatic differentiation (Baydin et al., 2017).
Meanwhile, the evaluation of ∣Θ0(A)∣tr in (9) requires iterating over the entire dataset of size m,
which thus incurs O(m) time. Fortunately, this incurred time can be reduced by parallelization over
2The same learning rate is shared among all candidate architectures for their model training during the search
process in conventional NAS algorithms such as (Liu et al., 2019).
3Notably, the γ-Lipschitz continuity is well-satisfied by widely adopted loss functions (see Appendix A.3).
4
Published as a conference paper at ICLR 2022
mini-batches. Let the set Xj denote the input feature vectors of the j -th randomly sampled mini-batch
of size |Xj| = b. By combining (9) and (10),
kΘo(A)ktr ≥ YT Pχ∈χ ∣∣Vθ0(A)Lχ∣∣2 ≥ bγ-1 pm=bι ∣b-1 Pχ∈Xj Vθ°(A)Lχ∣∣2 ,	(11)
where the last inequality follows from Jensen’s inequality. Note that (11) provides an approximation
of kΘ0(A)ktr incurring O(m/b) time because the gradients within a mini-batch can be evaluated
in parallel. In practice, we further approximate the summation over m/b mini-batches in (11) by
using only one single uniformly randomly sampled mini-batch Xj. Formally, under the definition of
kΘe0(A)ktr , kb-1 Px∈Xj Vθ0(A)Lxk22 , our final approximation of kΘ0(A)ktr becomes
kΘ0(A)ktr ≈mγ-1kΘe0(A)ktr .	(12)
This final approximation incurs only O(1) time and can also characterize the performance of neural
architectures effectively, as demonstrated in our experiments. Interestingly, a similar form to (12),
called gradient flow (Wang et al., 2020), has also been applied in network pruning at initialization.
3.3 Optimization and Search Algorithm
The approximation of kΘ0(A)ktr in Sec. 3.2 engenders an efficient optimization of our reformulated
NAS in (8): Firstly, we apply a penalty method to transform (8) into an unconstrained optimization
problem. Given a penalty coefficient μ and an exterior penalty function F(x)，max(0, x) with a
pre-defined constant ν , γnη-1, and a randomly sampled mini-batch Xj, by replacing kΘ0(A)ktr
with the approximation in (12), our reformulated NAS problem (8) can be transformed into
maxA [∣∣Θo(A)ktr - μF(∣∣Θo(A)ktr - V)] .	(13)
Interestingly, (13) implies that the complexity of the final selected architectures is limited by not only
the constraint V (discussed in Sec. 3.1) but also the penalty coefficient μ: For a fixed constant V, a
larger μ imposes a stricter limitation on the complexity of architectures (i.e., k Θ0(A) ||廿 < V) in the
optimization of (13).
The optimization of (13) in the discrete search space, however, is intractable. So, we apply certain
optimization tricks to simplify it: Following that of Liu et al. (2019); Xie et al. (2019), we represent
the search space as a one-shot architecture such that candidate architectures in the search space can
be represented as the sub-graphs of this one-shot architecture. Next, instead of optimizing (13), we
introduce a distribution pα(A) (parameterized by α) over the candidate architectures in this search
space like that in (Zoph & Le, 2017; Pham et al., 2018; Xie et al., 2019) to optimize the expected
performance of architectures sampled from pα(A):
―	,	. . Λ .. ~	. 一 , .. ~	..... 、	，一
maxαEA-pa(A) [R(A)]	s.t.R(A)，∣∣Θo(A)ktr — μF(k©o(A)ktr — V) .	(14)
Then, we apply Gumbel-Softmax (Jang et al., 2017; Maddison et al., 2017) to relax the optimization
of (14) to be continuous and differentiable using the reparameterization trick. Specifically, for a given
α, to sample an architecture A, we simply need to sample g from p(g) = Gumbel(0, 1) and then
determine A using α and g (more details in Appendix B.3). Consequently, (14) can be transformed
into
maxα Eg〜p(g) [R(A(α, g))] .	(15)
After that, we approximate (15) based on its first-order Taylor expansion at initialization such that it
can be optimized efficiently through a gradient-based algorithm. In particular, given the first-order
approximation within the ξ-neighborhood of initialization α0 (i.e., k∆k2 ≤ ξ):
Eg〜p(g) [R(A(ao + △,g))] ≈ Eg〜p(g)[R(A(αo, g)) + Vα°R(A(αQ, g))>∆] ,	(16)
the maximum of (16) is achieved when
△* = argmaxEg〜p(g) %。R(A(αo,g))>N = ξEg〜p(g) [∣∣	VaOR(A川I .
k∆∣∣2≤ξ	IIEg 〜p(g)[vao R(A(α0, g))]∣∣2
(17)
5
Published as a conference paper at ICLR 2022
Algorithm 1 NAS at Initialization (NASI)
1:	Input: dataset D，(X, Y), batch size b, steps T, penalty coefficient μ, constraint constant V, ini-
tialized model parameters θ0 for one-shot architecture and distribution pα0 (A) with initialization
α0 = 0, set ξ = 1
2:	for step t = 1, . . . , T do
3:	Sample data Dt 〜D of size b
4:	Sample gt 〜p(g) = Gumbel(0,1) and determine sampled architecture At based on α0, gt
5:	Evaluate gradient Gt = Vα0R(At) with data Dt
6:	end for
7:	Estimate △* with (18) and get α* = α0 + △*
8:	Select architecture A* = arg maxA pα* (A)
1e3 Search Space 1
3 2 1
SPqeI UJOPUe⅞ Xoda
ιe3 Search Space 2
8
6-
4-
2-
ie5 Search Space 3
4-
2-
ie3 Search Space 1
ie3 Search Space 2
6-
4-
2-
ie5 Search Space 3
4-
3-
2-
1 一
APProx w/ True Labels	Approx w/ True Data
(a) Label-agnostic	(b) Data-agnostic
Figure 1: Comparison of the approximated kΘ0(A)ktr following (12) using the three search spaces
of NAS-Bench-1Shot1 on CIFAR-10 (a) between random vs. true labels, and (b) between random
vs. true data. Each pair (x, y) in the plots denotes the approximation for one candidate architecture in
the search space with true vs. random labels (or data), respectively. The trends of these approximations
are further illustrated by the lines in orange. In addition, Pearson correlation coefficient ρ of the
approximations with random vs. true labels (or data) is given in the corner.
The closed-form solution in (17) follows from the definition of dual norm and requires only a one-step
optimization, i.e., without the iterative update of ∆. Similar one-step optimizations have also been
adopted in (Goodfellow et al., 2015; Wang et al., 2020).
Unfortunately, the expectation in (17) makes the evaluation of △* intractable. Monte Carlo sampling
is thus applied to estimate △* efficiently: Given T sequentially sampled g (i.e., gι,..., gτ) and let
Gi，VaoR(A(ao, gi)), ∆ can be approximated as
△*
T
ξ IX_________Gt__________
T ~=1 maxdlGil、,..., IIGtll2)
(18)
Note that inspired by AMSGrad (Reddi et al., 2018), the expectation IlEg〜p(g) Vɑ°R(A(α0, g))] ∖
in (17) has been approximated using max(IG1 I2, . . . , IGtI2) in (18) based on a sample ofg at time
t. Interestingly, this approximation is non-decreasing in t and therefore achieves a similar effect of
learning rate decay, which may lead to a better-behaved optimization of △*. With the optimal △*
and α* = α0 + △*, the final architecture can then be selected as A*，arg max/Pα* (A), which
finally completes our NAS at Initialization (NASI) algorithm detailed in Algorithm 1. Interestingly,
this simple and efficient solution in (18) can already allow us to select architectures with competitive
performances, as shown in our experiments (Sec. 5).
4	Label- and Data-agnostic Search of NASI
Besides the improved search efficiency by completely avoiding model training during search, NASI
can even guarantee the transferability of its selected architectures with its provable label- and data-
agnostic search under mild conditions in Sec. 4.1 and Sec. 4.2, respectively. That is, under this
provable label- and data-agnostic search, the final selected architectures on a proxy dataset using
NASI are also likely to be selected and hence guaranteed to perform well on the target datasets.
So, the transferability of the architectures selected via such label- and data-agnostic search can be
naturally guaranteed, which will also be validated in Sec. 5 empirically.
6
Published as a conference paper at ICLR 2022
4.1	Label-Agnostic Search
Our reformulated NAS problem (8) has explicitly revealed that it can be optimized without the need
of the labels from a dataset. Though our approximation of kΘ0(A)ktr in (12) seemingly depends
on the labels of a dataset, (12) can, however, be derived using random labels. This is because the
Lipschitz continuity assumption on the loss function required by (10), which is necessary for the
derivation of (12), remains satisfied when random labels are used. So, the approximation in (12)
(and hence our optimization objective (13) that is based on this approximation) is label-agnostic,
which hence justifies the label-agnostic nature of NASI. Interestingly, NAS algorithm achieving a
similar label-agnostic search has also been developed in (Liu et al., 2020), which further implies the
reasonableness of such label-agnostic search.
The label-agnostic approximation of kΘ0(A)ktr is demonstrated in Fig. 1a using the three search
spaces of NAS-Bench-1Shot1 with randomly selected labels. According to Fig. 1a, the large Pearson
correlation coefficient (i.e., ρ ≈ 1) implies a strong correlation between the approximations with
random vs. true labels, which consequently validates the label-agnostic approximation of kΘ0(A)ktr.
Overall, these empirical observations have verified that the approximation of kΘ0(A)ktr and hence
NASI based on the optimization over this approximation are label-agnostic, which will be further
validated empirically in Sec. 5.3.
4.2	Data-Agnostic Search
Besides being label-agnostic, NASI is also guaranteed to be data-agnostic. To justify this, we prove
in Proposition 2 (following from the notations in Sec. 3.1) below that kΘ0(A)ktr is data-agnostic
under mild conditions.
Proposition 2. Suppose that x ∈ Rn0 and kxk2 ≤ 1 for all x ∈ X given a dataset (X , Y) of
size |X | = m, a given L-layer neural architecture A is randomly initialized, and the γ -Lipschitz
continuous nonlinearity σ satisfies ∣σ(x)∣ ≤ |x|. Then, for any two data distributions P(x) and
Q(x), denote Z , kP (x) - Q(x)k dx, as n1, . . . , nL-1 → ∞ sequentially,
(mn)-1 kΘ0(A; P)ktr - kΘ0(A; Q)ktr ≤ n0-1ZD(γ)
with probability arbitrarily close to 1. D(Y) is set to L if Y = 1, and (1 一 γ2L)∕(1 — γ2) otherwise.
Its proof is in Appendix A.4. Proposition 2 reveals that for any neural architecture A, kΘ0(A)ktr is
data-agnostic if either one of the following conditions is satisfied: (a) Different datasets achieve a
small Z or (b) the input dimension n0 is large. Interestingly, these two conditions required by the
data-agnostic kΘ0(A)ktr can be well-satisfied in practice. Firstly, we always have Z < 2 according
to the property of probability distributions. Moreover, many real-world datasets are indeed of high
dimensions such as 〜103 for CIFAR-10 (Krizhevsky et al., 2009) and 〜105 for COCO (Lin et al.,
2014). Since kΘ0(A)ktr under such mild conditions is data-agnostic, NASI using kΘ0(A)ktr as the
optimization objective in (8) is also data-agnostic.
While kΘ0(A)ktr is costly to evaluate, we demonstrate in Fig. 1b that the approximated kΘ0(A)ktr
in (12) is also data-agnostic by using random data generated with the standard Gaussian distribution.
Similar to the results using true vs. random labels in Fig. 1a, the approximated kΘ0 (A)ktr using
random vs. true data are also highly correlated by achieving a large Pearson correlation coefficient
(i.e., ρ > 0.9). Interestingly, the correlation coefficient here is slightly smaller than the one of
label-agnostic approximations in Fig. 1a, which implies that our approximated kΘ0(A)ktr is more
agnostic to the labels than data. Based on these results, the approximated kΘ0(A)ktr is guaranteed to
be data-agnostic. So, NASI based on the optimization over such a data-agnostic approximation is
also data-agnostic, which will be further validated empirically in Sec. 5.3.
5	Experiments
5.1	Search in NAS-Bench-1Shot1
We firstly validate the search efficiency and effectiveness of our NASI in the three search spaces of
NAS-Bench-1Shot1 (Zela et al., 2020b) on CIFAR-10. As these three search spaces are relatively
7
Published as a conference paper at ICLR 2022
I Random .PC-DARTS * ENAS D- DARTS *GDAS * NASl]
Figure 2: Comparison of search efficiency (search budget in x-axis) and effectiveness (test error
evaluated on CIFAR-10 in y-axis) between NASI and other NAS algorithms in the three search spaces
of NAS-Bench-1Shot1. The test error for each algorithm is reported with the mean and standard error
after ten independent searches.
Search Space 3
Table 1: Performance comparison among state-of-the-art (SOTA) image classifiers on CIFAR-10/100.
The performance of the final architectures selected by NASI is reported with the mean and standard
deviation of five independent evaluations. The search costs are evaluated on a single NVidia 1080Ti.
Architecture	Test Error (%)		Params (M)		Search Cost (GPU Hours)	Search Method
	C10	C100	C10	C100		
DenseNet-BC (Huang et al., 2017)	3.46*	17.18*	25.6	25.6	-	manual
NASNet-A (Zoph et al., 2018)	2.65	-	3.3	-	48000	RL
AmoebaNet-A (Real et al., 2019)	3.34±0.06	18.931	3.2	3.1	75600	evolution
PNAS (Liu et al., 2018)	3.41±0.09	19.53*	3.2	3.2	5400	SMBO
ENAS (Pham et al., 2018)	2.89	19.43*	4.6	4.6	12	RL
NAONet (Luo et al., 2018)	3.53	-	3.1	-	9.6	NAO
DARTS (2nd) (Liu et al., 2019)	2.76±0.09	17.54t	3.3	3.4	24	gradient
GDAS (Dong & Yang, 2019)	2.93	18.38	3.4	3.4	7.2	gradient
NASP (Yao et al., 2020)	2.83±0.09	-	3.3	-	2.4	gradient
P-DARTS (Chen et al., 2019)	2.50	-	3.4	-	7.2	gradient
DARTS- (avg) (Chu et al., 2020)	2.59±0.08	17.51±0.25	3.5	3.3	9.6	gradient
SDARTS-ADV (Chen & Hsieh, 2020)	2.61±0.02	-	3.3	-	31.2	gradient
R-DARTS (L2) (Zela et al., 2020a)	2.95±0.21	18.01±0.26	-	-	38.4	gradient
TE-NAS] (Chen et al., 2021)	2.83±0.06	17.42±0.56	3.8	3.9	1.2	training-free
NASI-FIX	2.79±0.07	16.12±0.38	3.9	4.0	0.24	training-free
NASI-ADA	2.90±0.13	16.84±0.40	3.7	3.8	0.24	training-free
, Reported by Dong & Yang (2019) with their experimental settings.
* Obtained by training corresponding architectures without cutout (Devries & Taylor, 2017) augmentation.
] Evaluated using our experimental settings in Appendix B.4.
small, a lower penalty coefficient μ and a larger constraint V (i.e., μ=1 and V =1000) are adopted
to encourage the selection of high-complexity architectures in the optimization of (13). Here, ν is
determined adaptively as shown in Appendix B.1.
Figure 2 shows the results comparing the efficiency and effectiveness between NASI with a one-
epoch search budget and other NAS algorithms with a maximum search budget of 20 epochs to allow
sufficient model training during their search process. Figure 2 reveals that among all these three
search spaces, NASI consistently selects architectures of better generalization performance than other
NAS algorithms with a search budget of only one epoch. Impressively, the architectures selected by
our one-epoch NASI can also achieve performances that are comparable to the best-performing NAS
algorithms with 19× more search budget. Above all, NASI guarantees its benefits of improving the
search efficiency of NAS algorithms considerably without sacrificing the generalization performance
of its selected architectures.
5.2	Search in the DARTS Search Space
We then compare NASI with other NAS algorithms in a more complex search space than NAS-Bench-
1Shot1, i.e., the DARTS (Liu et al., 2019) search space (detailed in Appendix B.2). Here, NASI
8
Published as a conference paper at ICLR 2022
Table 2: Performance comparison of the architectures selected by NASI with random vs. true
labels/data on CIFAR-10. The standard method denotes the search with the true labels and data of
CIFAR-10 and each test error in the table is reported with the mean and standard deviation of five
independent searches.
Method	NAS-Bench-IShotI			DARTS
	S1	S2	S3	
Standard	7.3±1.1	7.2±0.4	7.2±0.6	2.95±0.13
Random Label	6.8±0.3	7.0±0.4	7.5±1.4	2.90±0.12
Random Data	6.6±0.2	7.5±0.7	7.3±0.9	2.97±0.10
selects the architecture with a search budget of T =100, batch size of b=64 and μ=2. Besides, two
different methods are applied to determine the constraint ν during the search process: the adaptive
determination with an initial value of 500 and the fixed determination with a value of 100. The final
selected architectures with adaptive and fixed ν are, respectively, called NASI-ADA and NASI-FIX
(visualized in Appendix C.4), which are then evaluated on CIFAR-10/100 (Krizhevsky et al., 2009)
and ImageNet (Deng et al., 2009) following Appendix B.4.
Table 1 summarizes the generalization performance of the final architectures selected by various
NAS algorithms on CIFAR-10/100. Compared with popular training-based NAS algorithms, NASI
achieves a substantial improvement in search efficiency and maintains a competitive generalization
performance. Even when compared with other training-free NAS algorithm (i.e., TE-NAS), NASI is
also able to select competitive or even outperformed architectures with a smaller search cost. Besides,
NASI-FIX achieves the smallest test error on CIFAR-100, which also demonstrates the transferability
of the architectures selected by NASI over different datasets. Consistent results on ImageNet can be
found in Appendix C.5.
5.3	Label- and Data-Agnostic Search
To further validate the label- and data-agnostic search achieved by our NASI as discussed in Sec. 4,
we compare the generalization performance of the final architectures selected by NASI using random
labels and data on CIFAR-10. The random labels are randomly selected from all possible categories
while the random data is i.i.d. sampled from the standard Gaussian distribution. Both NAS-Bench-
1Shot1 and the DARTS search space are applied in this performance comparison where the same
search and training settings in Sec. 5.1 and Sec. 5.2 are adopted.
Table 2 summarizes the performance comparison. Interestingly, among all these four search spaces,
comparable generalization performances are obtained on CIFAR-10 for both the architectures selected
with random labels (or data) and the ones selected with true labels and data. These results hence
confirm the label- and data-agnostic search achieved by NASI, which therefore also further validates
the transferability of the architectures selected by NASI over different datasets.
6	Conclusion
This paper describes a novel NAS algorithm called NASI that exploits the capability of NTK for
estimating the performance of candidate architectures at initialization. Consequently, NASI can
completely avoid model training during the search process to achieve higher search efficiency than
existing NAS algorithms. NASI can also achieve competitive generalization performance across
different search spaces and benchmark datasets. Interestingly, NASI is guaranteed to be label- and
data-agnostic under mild conditions, which therefore implies the transferability of the final selected
architectures by NASI over different datasets. With all these advantages, NASI can thus be adopted to
select well-performing architectures for unsupervised tasks and larger-scale datasets efficiently, which
to date remains challenging to other training-based NAS algorithms. Furthermore, NASI can also be
integrated into other training-based one-shot NAS algorithms to improve their search efficiency while
preserving the search effectiveness of these training-based algorithms.
9
Published as a conference paper at ICLR 2022
Reproducibility S tatement
To guarantee the reproducibility of the theoretical analysis in this paper, we have provided complete
proof of our propositions and also the justification of certain assumptions in Appendix A. Moreover,
we have conducted adequate ablation studies to further investigate the impacts of these assumptions as
well as the approximations used in our method on the final search results in Appendix C.6. Meanwhile,
to guarantee the reproducibility of the empirical results in this paper, we have provided our codes in
the supplementary materials and detailed experimental settings in Appendix B.
Acknowledgments
This research is supported by A*STAR under its RIE2020 Advanced Manufacturing and Engineering
(AME) Programmatic Funds (Award A20H6b0151).
References
Mohamed S. Abdelfattah, Abhinav Mehrotra, Lukasz Dudziak, and Nicholas D. Lane. Zero-cost
proxies for lightweight NAS. In Proc. ICLR, 2021.
Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In Proc. ICML,
pp. 322-332, 2019.
Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind.
Automatic differentiation in machine learning: A survey. Journal of Machine Learning Research,
18:153:1-153:43, 2017.
Han Cai, Ligeng Zhu, and Song Han. ProxylessNAS: Direct neural architecture search on target task
and hardware. In Proc. ICLR, 2019.
Wuyang Chen, Xinyu Gong, and Zhangyang Wang. Neural architecture search on imagenet in four
gpu hours: A theoretically inspired perspective. In Proc. ICLR, 2021.
Xiangning Chen and Cho-Jui Hsieh. Stabilizing differentiable architecture search via perturbation-
based regularization. In Proc. ICML, pp. 1554-1565, 2020.
Xin Chen, Lingxi Xie, Jun Wu, and Qi Tian. Progressive differentiable architecture search: Bridging
the depth gap between search and evaluation. In Proc. ICCV, pp. 1294-1303, 2019.
Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. A downsampled variant of imagenet as an
alternative to the CIFAR datasets. arXiv:1707.08819, 2017.
Xiangxiang Chu, Xiaoxing Wang, Bo Zhang, Shun Lu, Xiaolin Wei, and Junchi Yan. DARTS-:
Robustly stepping out of performance collapse without indicators. arXiv:2009.01027, 2020.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A large-scale hierarchical
image database. In Proc. CVPR, 2009.
Terrance Devries and Graham W. Taylor. Improved regularization of convolutional neural networks
with cutout. arXiv:1708.04552, 2017.
Xuanyi Dong and Yi Yang. Searching for a robust neural architecture in four GPU hours. In Proc.
CVPR, pp. 1761-1770, 2019.
Xuanyi Dong and Yi Yang. Nas-bench-201: Extending the scope of reproducible neural architecture
search. In Proc. ICLR, 2020.
Simon S. Du, XiyU Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent Provably optimizes
over-parameterized neural networks. In Proc. ICLR, 2019.
Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. Journal
of Machine Learning Research, 20:55:1-55:21, 2019.
10
Published as a conference paper at ICLR 2022
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In Proc. ICLR, 2015.
Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In Proc. ICML ,pp.1225-1234, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on ImageNet classification. In Proc. ICCV, pp. 1026-1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proc. CVPR, pp. 770-778, 2016.
Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. MobileNets: Efficient convolutional neural networks for
mobile vision applications. arXiv:1704.04861, 2017.
Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected
convolutional networks. In Proc. CVPR, pp. 2261-2269, 2017.
Arthur Jacot, Clement Hongler, and Franck Gabriel. Neural Tangent Kernel: Convergence and
generalization in neural networks. In Proc. NeurIPS, pp. 8580-8589, 2018.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with Gumbel-Softmax. In
Proc. ICLR, 2017.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Proc. NeurIPS, pp. 8570-8581, 2019a.
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip H. S. Torr. Snip: Single-shot network pruning
based on connection sensitivity. In Proc. ICLR, 2019b.
Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dollar, and C. Lawrence Zitnick. Microsoft COCO: Common objects in context. In Proc. ECCV.,
pp. 740-755, 2014.
Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan L.
Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. In Proc. ECCV,
pp. 19-35, 2018.
Chenxi Liu, Piotr Dollar, Kaiming He, Ross B. Girshick, Alan L. Yuille, and Saining Xie. Are labels
necessary for neural architecture search? In Proc. ECCV, pp. 798-813, 2020.
Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Differentiable architecture search. In
Proc. ICLR, 2019.
Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. Neural architecture optimization.
In Proc. NeurIPS, pp. 7827-7838, 2018.
Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. ShuffleNet V2: Practical guidelines for
efficient CNN architecture design. In Proc. ECCV, pp. 122-138, 2018.
Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural network
acoustic models. In Proc. ICML, pp. 3, 2013.
Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The Concrete distribution: A continuous
relaxation of discrete random variables. In Proc. ICLR, 2017.
Joseph Mellor, Jack Turner, Amos J. Storkey, and Elliot J. Crowley. Neural architecture search
without training. In Proc. ICML, 2021.
11
Published as a conference paper at ICLR 2022
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT
press, 2018.
Daniel S. Park, Jaehoon Lee, Daiyi Peng, Yuan Cao, and Jascha Sohl-Dickstein. Towards NNGP-
guided neural architecture search. arXiv:2011.06006, 2020.
Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, and Jeff Dean. Efficient neural architecture
search via parameter sharing. In Proc. ICML,pp. 4092-4101, 2018.
Maithra Raghu, Ben Poole, Jon M. Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the
expressive power of deep neural networks. In Proc. ICML, pp. 2847-2854, 2017.
Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V. Le. Regularized evolution for image
classifier architecture search. In Proc. AAAI, pp. 4780-4789, 2019.
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of Adam and beyond. In
Proc. ICLR, 2018.
Yao Shu, Wei Wang, and Shaofeng Cai. Understanding architectures learnt by cell-based neural
architecture search. In Proc. ICLR, 2020.
Yao Shu, Zhongxiang Dai, Zhaoxuan Wu, and Bryan Kian Hsiang Low. Unifying and boosting
gradient-based training-free neural architecture search. arXiv:2201.09785, 2022.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In Proc. ICLR, 2015.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov,
Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proc. CVPR, pp. 1-9, 2015.
Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and
Quoc V. Le. MnasNet: Platform-aware neural architecture search for mobile. In Proc. CVPR, pp.
2820-2828, 2019.
Hidenori Tanaka, Daniel Kunin, Daniel L. Yamins, and Surya Ganguli. Pruning neural networks
without any data by iteratively conserving synaptic flow. In Proc. NeurIPS, 2020.
Chaoqi Wang, Guodong Zhang, and Roger B. Grosse. Picking winning tickets before training by
preserving gradient flow. In Proc. ICLR, 2020.
Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. SNAS: Stochastic neural architecture search.
In Proc. ICLR, 2019.
Jingjing Xu, Liang Zhao, Junyang Lin, Rundong Gao, Xu Sun, and Hongxia Yang. KNAS: green
neural architecture search. In Proc. ICML, pp. 11613-11625, 2021.
Quanming Yao, Ju Xu, Wei-Wei Tu, and Zhanxing Zhu. Efficient neural architecture search via
proximal iterations. In Proc. AAAI, pp. 6664-6671, 2020.
Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, and Frank Hutter.
Understanding and robustifying differentiable architecture search. In Proc. ICLR, 2020a.
Arber Zela, Julien Siems, and Frank Hutter. NAS-Bench-1Shot1: Benchmarking and dissecting
one-shot neural architecture search. In Proc. ICLR, 2020b.
Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. In Proc. ICLR,
2017.
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V. Le. Learning transferable architectures
for scalable image recognition. In Proc. CVPR, pp. 8697-8710, 2018.
12
Published as a conference paper at ICLR 2022
Appendix A Theorems and Proofs
We firstly introduce the theorems proposed by Jacot et al. (2018); Lee et al. (2019a), which reveal
the capability of NTK in characterizing the training dynamics of infinite-width DNNs. Note that the
these theorems follow the parameterization and initialization of DNNs in (Jacot et al., 2018). Our
analysis based on these theorems thus also needs to follow such parameterization and initialization of
DNNs.
A. 1 Neural Tangent Kernel at Initialization
Jacot et al. (2018) have validated that the outputs of infinite-width DNNs at initialization tends to
Gaussian process (Theorem 1) and have further revealed the deterministic limit of NTK at initialization
(Theorem 2). We denote InL as a nL × nL matrix with all elements being 1.
Theorem 1 (Jacot et al. (2018)). Fora network of depth L at initialization, with a Lipschitz nonlinear-
ity σ, and in the limit as nι, ∙∙∙ ,nL-ι → ∞ sequentially, the outputfunctions fθ,i for i = 1,…，nL,
tend (in law) to iid centered Gaussian processes of covariance Σ(L), where Σ(L) is defined recursively
by
Σ(1)(x, x0) = -xγX + β2 ,
n0
ς(L)(X, XO) = Eg〜N(0,∑(l-i))[σ(g(x))σ(g(χ0))] + β2
such that the expectation is with respect to a centered Gaussian process f with covariance Σ(L-1).
Theorem 2 (Jacot et al. (2018)). For a network of depth L at initialization, with a Lipschitz nonlin-
earity σ, and in the limit as nι, ∙∙∙ ,nL-ι → ∞ sequentially, the NTK Θ(L) converges in probability
to a deterministic limiting kernel:
Θ(L) → Θ∞L) 乳 InL .
Kernel Θ(∞L) : Rn0×n0 → R is defined recursively by
Θ(∞1) (x, x0) = Σ(1) (x, x0) ,
Θ∞L)(x, X0) = Θ∞L-1)(x, x0)Σ(L) (x, x0) + ∑(L)(x, x0),
where
ς(L)(X,XO) = Eg〜N(0,∑(L-1)) [6(g(X))6(g(XO))]
such that the expectation is with respect to a centered Gaussian process f with covariance Σ(L-1)
and σ denotes the derivative of σ.
A.2 Training Dynamics of Infinite-Width Neural Networks
Given λmin(Θ) as the minimal eigenvalue of NTK Θ and define ηcritical , 2(λmin(Θ)+λmax(Θ))-1,
Lee et al. (2019a) have characterized the training dynamics of infinite-wide neural networks as below.
Theorem 3 (Lee et al. (2019a)). Let nι =…=m = k and assume λmin(Θ) > 0. Applying
gradient descent with learning rate η < ηcritical (or gradient flow), for every x ∈ Rn0 with kxk ≤ 1,
with probability arbitrarily close to 1 over random initialization,
sup ∣∣ft - ftin∣∣2 = O(k-1) as k → ∞ .
Remark. For the case of L = 2, Du et al. (2019) have revealed that if any two input vectors
of a dataset are not parallel, then λmin (Θ) > 0 holds, which fortunately can be well-satisfied for
most real-world datasets. Though the training dynamics of DNNs only tend to be governed by their
linearization at initialization when the infinite width is satisfied as revealed in Theorem 3, empirical
results in (Lee et al., 2019a) suggest that such linearization can also govern the training dynamics of
practical over-parameterized DNNs.
13
Published as a conference paper at ICLR 2022
A.3 Proof of Proposition 1
With aforementioned theorems, especially Theorem 3, our Proposition 1 can be proved as below with
an introduced lemma (Lemma 1).
Lemma 1. Let loss function L (f(X; θt), Y), abbreviated to L(ft), be γ -Lipschitz continuous within
the domain V. Under the condition in Theorem 3, there exists a constant c0 > 0 such that as k → ∞,
∖∖L(ft) -L(ftin儿 ≤ √γ
with probability arbitrarily close to 1.
Proof. Since L(ft) is γ-Lipschitz continuous, for any v, u ∈ V, we have
kL(v) - L(u)k2 ≤γkv - uk2	(19)
following the definition of Lipschitz continuity. Besides, under the condition in Theorem 3, Theorem 3
reveals that there exists a constant c0 such that as k → ∞,
lift- ftinl∣2 ≤ √k	(20)
with probability arbitrarily close to 1. Combining (19) and (20), we hence can finish the proof by
∣lLft)-L(Ein)∣∣2 ≤ Y l∣ft- ftin∣∣2 ≤ √γ ask → ∞.	(21)
□
Remark. The Y-LiPschitz continuity based on ∣∣ ∙ k2 is commonly satisfied for widely adopted loss
functions. For example, given 1-dimensional MSE L = m-1 Pim(xi -yi)2, let xi, yi ∈ [0, 1] denote
the prediction and label respectively, the Lipschitz of MSE with respect to X，(xι, ∙∙∙ , Xm)> is 2.
Meanwhile, given the n-class Cross Entropy with Softmax L = -m-1 Pim Pjn yi,j log(pi,j) as the
loss function, let pi,j ∈ (0, 1) and yi,j ∈ {0, 1} denote the prediction and label correspondingly, with
Pj yi,j = 1 and pi,j , exp(xi,j)/ Pj exp(xi,j) for input xi,j ∈ R, the Lipschitz of Cross Entropy
with Softmax with respect to X，(x1,1, •…，Xi,j, •…，Xm,n)> is then 1.
Proof of Proposition 1. Note that the linearization flin(x; θt) achieves a constant NTK Θt = Θ0
because its gradient with respect to θt (i.e., Vθo f(x; θ0)) stays constant over time. Given MSE
as the loss function, according to the loss decomposition (4) in Sec. 2.2, the training dynamics of
flin(X; θt) can then be analyzed in a closed form:
mn
L(ftin) = 一 X(1-ηλi)2t(u>Y)2 ,	(22)
m
i=1
where Θ0 = Pim=n1 λi(Θ0)uiui>, and λi(Θ0) and ui denote the i-th largest eigenvalue and the
corresponding eigenvector of Θ0, respectively. With η < λmax(Θ0)-1 and λmin(Θ0) > 0, ηλi(Θ0)
is then under the constraint that
0 < ηλi(Θ0) < :max(普0) ⇒ 0 < ηλi(Θ0) < 1 .
λmax(Θ0 )
(23)
Hence, for the case of t ≥ 0.5, with 0 < 1 一 ηλi(Θ0) < 1 and λ(Θ0)，(mn)-1 PmL λi(Θ0),
Xmn(1 - ηλi(Θ0))2t ≤ Xmn(1 - ηλi(Θ0))
i=L	i=L
= mn(1 - ηλ(Θ0)) .
(24)
Further, given 0 ≤ t < 0.5, the scalar function y = x2t is concave for any x ∈ R≥0. Following from
the Jensen’s inequality on this concave function,
mn
X(1 - ηλi (Θ0))2t ≤ mn
i=L
mn
一X(1- ηλi(θ0))
mn
i=L
2t
(25)
mn(1 — ηλ(Θ0))2t .
14
Published as a conference paper at ICLR 2022
With bounded labels Y ∈ [0, 1]mn and the unit norm of eigenvectors kuik2 = 1,
(ui>Y)2 ≤ kuik22kYk22 ≤ kYk22 ≤mn.	(26)
By introducing (24), (25) and (26) into (22), the training loss L(ftlin) can then be bounded by
L(ftin) ≤ (mn) ∙ 1 X(1 — %(Θo))2t
m
i=1
(27)
≤ mn2(1 — ηλ(Θo))q ,
where q is set to be 2t if 0 ≤ t < 0.5, and 1 otherwise.
Following from Theorem 3, while aPPlying gradient descent (or gradient flow) with learning rate
η < λ-m1ax < ηcritical , 2(λmin + λmax)-1, for all x ∈ X with kxk ≤ 1, there exists a constant c0
such that as k→ ∞,
^ft - ft^∣2 ≤√√k
(28)
. r	____ _______________-∣ mn
with Probability arbitrarily close to 1. Hence, ftlin ∈ —c0 1/k, 1 + c0 1/k with ft ∈
[0,1]mn. Within the extended domain ft ∈ [-copl/k, 1 + co/1 /k] for the MSE loss function,
kVft l∣∣2=m ∣∣γ-ftk2
≤ 2,n/m(1 + cop/k)
(29)
Hence, the MSE within this extended domain is 2,n/m(1 + c0,1/k) -LiPschitz continuous.
Combining with Lemma 1,
L(ft) ≤ L(ftin) + 2co/n/(mk)(1 + cop/k)
≤ mn2(1 — ηλ)q + 2copn/(mk)(1 + c0p1/k)
(30)
which concludes the proof by denoting Lt , L(ft).
Remark. kxk2 ≤ 1 can be well-satisfied for the normalized dataset, which is conventionally adopted
as data pre-processing in practice.
A.4 Proof of Proposition 2
According to Theorem 2, Θo ，Θ(L) is determined by both Σ(L)(x, x) and Σ(L)(x, x) of all
X ∈ X.We hence introduce Lemma 2 below regarding Σ(L)(x, x) and Σ(L)(x, x) to ease the proof
of ProPosition 2. Particularly, we set β = 0 for the β in Theorem 1 and Theorem 2 throughout our
analysis. Note that this condition is usually satisfied by training DNNs without bias.
Lemma 2. For a network of depth L at initialization, with the γ-Lipschitz continuous nonlinearity σ
satisfying ∣σ(x)∣ ≤ |x| forall X ∈ R, g^ven the input features X ∈ X,
Σ(L)(x, x) ≤ n-1x>x , Σ(L)(x, x) ≤ γ2 ,
where Σ(L)(x, x) and Σ(L)(x, x) are defined in Theorem 1 and Theorem 2, respectively, and β = 0.
Proof. Following from the definition in Theorem 1,
£(L)(X, x) = Eg〜N(0,∑(L-1)) [b(g(X))σ(g(X))]
(a)
≤ Eg〜N(o,∑(L-1)) [g2(x)]	(31)
(=b) Σ(L-1)(x, x),
15
Published as a conference paper at ICLR 2022
in which (a) follows from ∣σ(x) | ≤ |x| and (b) results from the variance of g 〜N(0, Σ(L-1)). By
following (31) from layer L to 1, we get
Σ(L) (x, x) ≤ Σ(1) (x, x) (=a) n0-1x>x ,	(32)
where (a) is defined in Theorem 1.
Similarly, following from the definition in Theorem 2,
ς (L)(X, X) = Eg 〜N (OHL-% [σ(g(χ))^σ(g(χ)y]
(a)
≤ Eg〜N(0,∑(L-1)) [γ2]	(33)
(=b) γ2,
in which (a) results from the γ-Lipschitz continuity of nonlinearity and (b) follows from the expecta-
tion of a constant.	□
Proof of Proposition 2. Notably, Theorem 2 reveals that in the limit as nι,…,nL-ι → ∞
sequentially, Θ(L) → Θ∞L) 0 InL with probability arbitrarily close to 1 over random initializations.
We therefore only need to focus on this deterministic limiting kernel to simplify our analysis, i.e., let
Θ0 = Θ(∞L) 0 InL. Particularly, given m input vectors X 〜P(x) with covariance matrix ΣP and a
L-layer neural architecture of n-dimensional output, For γ 6= 1, we have
(mn)-1kΘ0ktr = (mn)-1kΘ(∞L) 0 InL ktr
(=a) (mn)-1kΘ(∞L)ktrkInLktr
(=b) m-1kΘ(∞L)ktr
=) Ex〜p(x) [θ∞L)(x, x)i
=) Ex〜P(x) [θ∞Lτ)(x, x)Σ(L)(x, x) + Σ(L)(x, x)i
(≤, Ex〜P(x) [γ2θ∞LT)(X, X) + n0-1 X>Xi
=Ex〜P(x) [γ Θ(∞L-1)(X,X)i +n0-1kΣPktr ,
(34)
in which (a) derives from the property of Kronecker product, (b) follows from the notation nL = n
and (c) results from the property of expectation and trace norm. In addition, (d) follows from the
definition of Θ(∞L) (X, X) in Theorem 2 and (e) is derived by introducing Lemma 2 into (d). Finally,
(f) follows from the expectation and variance of P (X). By following (c-f) from layer L to 1, we can
get
(mn)-1kΘoktr ≤ y2(lT)Ex〜P(x) [∑(I)(x, x)] +
n-1k∑P ktr (-Y2(LT)) (-Y2)-1	(35)
=n-1k∑Pktr (1-Y2L)(I-Y2)-1 .
Notably, (35) is derived under the condition that Y 6= 1. For the case of Y = 1, similarly, we can get
(mn)-1kΘ0ktr ≤ n0-1LkΣPktr .
(36)
Note that with kxk2 ≤ 1, we have
k∑P ktr = Ex 〜P(x) [x>x] — Ex 〜P(x) [x]> Ex 〜P (x) [x]
≤ Ex〜P(x) [x>x]	(37)
≤1.
16
Published as a conference paper at ICLR 2022
Given any two different distributions P(x) and Q(x), define Z = kP (x) - Q(x)k dx,4 we can
construct a special probability density function P(x) = Z-1 kP (x) - Q(x)k to further employee
the bounds in (35) and (36). Specifically, for γ 6= 1, by combining (35) and (37), we can get
kP (X) - Q(X)kΘ(∞L)(X,X)dX
(38)
(mn)-1∣kΘo(P)ktr -kΘo(Q)ktr∣ =) Eχ~p⑻[θ∞L)(x, x)]-
Ex~Q(x) [θ∞) (X, X)]
(b)
≤
(c)
≤ n-1Zk∑pk廿(1-Y2L)(1-Y2)-1
(d)
≤ n-1Z (I-Y2L)(I-Y2)T ,
in which (a) follows from (c) in (34), (b) results from Cauchy Schwarz inequality and (c) is obtained
by introducing inequality (35) and distribution Pe(X). Finally, (d) is based on (37).
Similarly, in the case of Y = 1, we can get
(mn)-1∣∣∣ kΘ0(P)ktr - kΘ0(Q)ktr ∣∣∣ ≤n0-1ZL,	(39)
which concludes the proof.
Remark. Note that ReLU is widely adopted as the nonlinearity in neural networks, which satisfies
the LiPschitz continuity with Y = 1 and the inequality ∣σ(x)∣ ≤ |x|. Notably, many other ReLU-type
nonlinearities (e.g., Leaky ReLU (Maas et al., 2013) and PReLU (He et al., 2015)) can also satisfy
these two conditions.
Appendix B	Experimental Settings
B.1	Determination of Constraint V and Penalty Coefficient μ
As demonstrated in Sec. 3.1, constraint ν (derived from kΘ0(A)ktr < mnη-1 in (8)) introduces a
trade-off between the complexity of final selected architectures and the optimization behavior in their
model training. This constraint ν hence is of great importance to our NASi algorithm. Though ν
has already been pre-defined as ν , Ynη-1 in Sec. 3.3, we still tend to take it as a hyper-parameter
to be determined for the selection of best-performing architectures in practice. Specifically, in this
paper, two methods are adopted to determine ν during the search progress: The fixed and adaptive
method shown as below. Notably, the final architectures selected by NASi via the fixed and adaptive
method are called NASi-FiX and NASi-ADA, respectively. Note that our experiments in the main
text suggest that both methods can select architectures with competitive generalization performance
over different tasks.
The fixed determination. We initialize and fix ν with ν0 during the whole search process. Hence,
ν0 is required to provide a good trade-off between the complexity of architectures and their optimiza-
tion behavior in the search space. intuitively, the expectation of architecture complexity in the search
space can help to select architectures with medium complexity and hence implicitly achieve a good
trade-off between the complexity of architectures and their optimization behavior. Specifically, we
randomly sample N = 50 architectures in the search space (i.e., Ai,…，AN), and then determine
ν0 before the search process by
N
ν=ν0 =N-1XkΘe0(Ai)ktr .	(40)
Note that we can further enlarge ν0 in practice to encourage the selection of architectures with larger
complexity.
4We abuse this integration to ease notations.
17
Published as a conference paper at ICLR 2022
The adaptive determination. We initialize ν with a relatively large ν0 and then adaptively update
it with the expected kΘ0(A)ktr of sampled architectures during the search process. Specifically, with
sampled architectures Ai, •…，At in the history, V at time t (i.e., Vt) during the search process is
given by
Vt =t-1 V0 + Xt-1 kΘe0(Aτ)ktr	.	(41)
We apply a relatively large V0 to ensure a loose constraint on the complexity of architectures in
the first several steps of the search process. Note that the adaptive determination provides a more
accurate approximation of the expected complexity of architectures in the search space than the fixed
determination method if more architectures are sampled to update Vt, i.e., t > N .
Note that (13) in Sec. 3.3 further reveals the limitation on the complexity of final selected architectures
by the penalty coefficient μ. Particularly, μ=0 indicates no limitation on the complexity of architec-
tures. Following from the introduced trade-off between the complexity of final selected architectures
and the optimization behavior in their model training by the constraint V, for a search space with
relatively Iarger-Complexity architectures, a larger penalty coefficient μ (i.e., μ = 2) is preferred to
search for architectures with relatively smaller complexity to ensure a well-behaved optimization
with a larger learning rate η. On the contrary, for a search space with relatively smaller-complexity
architectures, a lower penalty coefficient μ (i.e., μ=1) is adopted to ensure the complexity and hence
the representation power of the final selected architectures. Appendix C.6 provides further ablation
study on the constraint V and the penalty coefficient μ.
B.2	THE DARTS SEARCH SPACE
Following from the DARTS (Liu et al., 2019) search space, candidate architecture in our search space
comprise a stack of L cells, and each cell can be represented as a directed acyclic graph (DAG) of
N nodes denoted by {x0, x1, . . . , xN-1}. To select the best-performing architectures, we instead
need to select their corresponding cells, including the normal and reduction cell. Specifically, x0
and x1 denote the input nodes, which are the output of two preceding cells, and the output xN of a
cell is the concatenation of all intermediate nodes from x2 to xN-1. Following that of SNAS (Xie
et al., 2019), by introducing the distribution of architecture in the search space (i.e., pα(A) in (14)),
each intermediate nodes xi (2 ≤ i ≤ N - 1) denotes the output of a single sampled operation
oi 〜Pi(o) with Po∈o Pi(o) = 1 given a single sampled input Xj 〜Pi(x) with j ∈ 0,…，i - 1 and
Pij-=10pi(xj) = 1, where O is a predefined operation set. After the search process, only operations
achieving the top-2 largest Pi(o) and inputs achieving the top-2 largest Pi (x) for node xi are retained.
Each intermediate node xj hence connects to two preceding nodes with the corresponding selected
operations.
Following from DARTS, the candidate operation set O includes following operations: 3 × 3 max
pooling, 3 × 3 avg pooling, identity, 3 × 3 separable conv, 5 × 5 separable conv, 3 × 3 dilated separable
conv, 5 × 5 dilated separable conv. Note that our search space has been modified slightly based on the
standard DARTS (Liu et al., 2019) search space: (a) Operation zero is removed from the candidate
operation set in our search space since it can never been selected in the standard DARTS search
space, and (b) the inputs of each intermediate node are selected independently from the selection
of operations, while DARTS attempts to select the inputs of intermediate nodes by selecting their
coupling operations with the largest weights. Notably, following from DARTS, we need to search
for two different cells: A normal cell and a reduction cell. Besides, a max-pooling operation in
between normal and reduction cell is applied to down-sampling intermediate features during the
search process inspired by NAS-Bench-1Shot1 (Zela et al., 2020b).
B.3	Sampling Architecture with Gumbel-Softmax
Notably, aforementioned Pi (o) and Pi (x) for the node xi follow the categorical distributions. To
relax the optimization of (14) with such categorical distributions to be continuous and differentiable,
Gumbel-Softmax (Jang et al., 2017; Maddison et al., 2017) is applied. Specifically, supposing Pi(o)
and Pi(x) are parameterized by αio,j and αix,k respectively, with Straight-Through (ST) Gumbel
Estimator (Jang et al., 2017), we can sample the single operation and input for the node xi (2 ≤ i ≤
18
Published as a conference paper at ICLR 2022
N - 1) during the search process by
j * = arg max
j
eχp((Mj + goj"T)
k* = arg max
k
jex eχp((αo,j + goj"T)
exP((ax,k + gχ,Q∕τ)
(42)
k exp((αix,k + gix,k)∕T)
where gio,j and gix,k are sampled from Gumbel(0, 1) and T denotes the softmax temperature, which
is conventionally set to be 1 in our experiments. Note that in (42), j ∈ [0, ∙∙∙ , |O|- 1] and
k ∈ [0, ∙∙∙ ,i - 1], which correspond to the |O| operations in the operation set O and the i candidate
inputs for the node xi, respectively. Notably, the gradient through ST can be approximated by
its continuous counterpart as suggested in (Jang et al., 2017), thus allowing the continuous and
differentiable optimization of (14). By sampling the discrete operation and input with (42) for each
nodes in the cells, the final sampled architecture can hence be determined.
B.4	Evaluation on CIFAR-10/100 and ImageNet
Evaluation on CIFAR-10/100. Following DARTS (Liu et al., 2019), the final selected architectures
consist of20 searched cells: 18 of them are identical normal cell and 2 of them are identical reduction
cell. An auxiliary tower with weight 0.4 is located at 13-th cell of the final selected architectures
and the number of initial channels is set to be 36. The final selected architecture are then trained via
stochastic gradient descent (SGD) of 600 epochs with a learning rate cosine scheduled from 0.025
to 0, momentum 0.9, weight decay 3 × 10-4 and batch size 96 on a single Nvidia 2080Ti GPU.
Cutout (Devries & Taylor, 2017), and ScheduledDropPath linearly increased from 0 to 0.2 are also
employed for regularization purpose.
Evaluation on ImageNet. We evaluate the transferability of the selected architectures from CIFAR-
10 to ImageNet. The architecture comprises of 14 cells (12 normal cells and 2 reduction cells). To
evaluate in the mobile setting (under 600M multiply-add operations), the number of initial channels
is set to 46. We adopt conventional training enhancements (Liu et al., 2019; Chen et al., 2019;
Chen & Hsieh, 2020) include an auxiliary tower loss of weight 0.4 and label smoothing. Following
P-DARTS (Chen et al., 2019) and SDARTS-ADV (Chen & Hsieh, 2020), we train the model from
scratch for 250 epochs with a batch size of 1024 on 8 Nvidia 2080Ti GPUs. The learning rate
is warmed up to 0.5 for the first 5 epoch and then decreased to zero linearly. We adopt the SGD
optimizer with 0.9 momentum and a weight decay of 3 × 10-5 for the model training on ImageNet.
Appendix C	More Results
C.1 Trade-off B etween Model Complexity and Optimization Behaviour
In this section, we empirically validate the existence of trade-off between model complexity of
selected architectures and the optimization behavior in their model training for our reformulated NAS
detailed in Sec. 3.1.
The trade-off in NAS-Bench-1Shot1. Specifically, Figure 3 illustrates the relation between test
error and approximated kΘ0(A)ktr of candidate architectures in the three search spaces of NAS-
Bench-1Shot1. Note that with the increasing of the approximated kΘ0(A)ktr, the test error decreases
rapidly to a minimum and then increase gradually, which implies that architectures only with certain
kΘ0(A)ktr (or with desirable complexity instead of the largest complexity) can achieve the best
generalization performance. These results hence validate the existence of trade-off between the
model complexity and the generalization performance of selected architectures. Note that such
trade-off usually results from different optimization behavior in the model training of those selected
architectures as demonstrated in the following experiments.
The trade-off in the DARTS search space. We then illustrate the optimization behavior of the final
selected architecture from the DARTS search space with different constraint and penalty coefficient in
Figure 4 to further confirm the existence of such trade-off in our reformulated NAS (8). Specifically,
19
Published as a conference paper at ICLR 2022
2 0 1e-1 Search Space 1
0.5
0
4
1e2
0.5
20 1e-1 Search Space 2
0
4
1e2
Approximated Trace Norm of NTK
20 1e-1 Search Space 3
0.5
2	4
1e2
1
0
Figure 3: The relation between test error and approximated kΘ0(A)ktr of candidate architectures
in the three search spaces of NAS-Bench-1Shot1 over CIFAR-10. Note that x-axis denotes the
approximated kΘ0(A)ktr, which is averaged over the architectures grouped in the same bin based on
their approximated kΘ0(A)ktr. Correspondingly, y-axis denotes the averaged test error with standard
deviation (scaled by 0.5) of these grouped architectures. In addition, the red lines demonstrate the
smallest test errors achieved by candidate architectures in these three search spaces.
0 5 0 5 0 5
5 2 0 7 5 2
(％)」。」」山一səl
0 5 0 5 0 5
5 2 0 7 5 2
(％)」。」」山一səl
O
I O
6
4
2
I O
Epoch	Epoch
(a) Constraint V	(b) Penalty coefficient μ
Figure 4:	The optimization behavior (test error on CIFAR-10 in model training) of the final selected
architectures with different constraint V and penalty coefficient μ. The model parameter (MB) denoted
by p of each architecture is given in the top-right corner.
we apply NASI with the fixed determination of V to select architectures in the the DARTS search
space over CIFAR-10, where V0 in the fixed determination method introduced in Appendix B.1 is
modified manually for the comparison. Besides, a search budget ofT = 100 and batch size of b = 64
are adopted. These final selected architectures are then trained on CIFAR-10 following Appendix 5.2.
Note that, in Figure 4(a), with the increasing of V, the final selected architectures by NASI contain
more parameters and hence achieve larger complexity. Meanwhile, the final selected architecture with
V=10 enjoys a faster convergence rate in the first 50 epochs, but a poorer generalization performance
than the one with V= 200. Interestingly, the final selected architecture with V=100 realizes the
fastest convergence and the best generalization performance by achieving a proper complexity of final
selected architectures. These results validate the existence and also the importance of the trade-off
between the complexity of final selected architectures and the optimization behavior in their model
training. However, the trade-off introduced by the penalty coefficient μ shown in Figure 4(b) is hard to
be observed, which indicates that the constraint V is of greater importance than the penalty coefficient
μ in terms of the trade-off between the complexity of architectures and their optimization behavior.
Interestingly, Figure 4(b) can still reveal the slower convergence rate and poorer generalization
performance caused by the selection of architecture with relatively larger complexity, i.e., μ=1.
C.2 Comparison to Other Training-Free Metrics
We compare our methods with other training-free NAS methods using both the Spearman correlation
and the Kendall’s tau between the training-free metrics and the test accuracy on CIFAR-10 in the
three search spaces of NAS-Bench-1Shot1. We adopt one uniformly randomly sampled mini-batch
data to evaluate these two correlations and apply the same implementations of these training-free
20
Published as a conference paper at ICLR 2022
Table 3: Comparison of the Spearman correlation and the Kendall’s tau for various training-free metrics in the three search spaces of NAS-Bench-1Shot1 on CIFAR-10.						
Methods	Spearman Correlation			Kendall’s Tau		
	S1	S2	S3	S1	S2	S3
SNIP (Lee et al., 2019b)	-0.49	-0.62	-0.79	-0.39	-0.49	-0.63
GraSP (Wang et al., 2020)	0.41	0.54	0.17	0.33	0.42	0.15
SynFlow (Tanaka et al., 2020)	-0.52	-0.45	-0.53	-0.42	-0.40	-0.47
NASWOT (Mellor et al., 2021)	0.21	0.32	0.54	0.16	0.24	0.44
NASI (conditioned)	0.62	^^0.74	0.76	0.44	0.53	0.53
Table 4: The comparison among state-of-the-art (SOTA) NAS algorithms on NAS-Bench-201. The performance of the final architectures selected by NASI is reported with the mean and standard deviation of four independent trials. The search costs are evaluated on a single Nvidia 1080Ti.					
Architecture	Test Accuracy (%)			Search Cost (GPU Sec.)	Search Method
	C10	C100	IN-16-120		
ResNet (He et al., 2016)	93.97	70.86	43.63	-	-
ENAS (Pham et al., 2018)	54.30	15.61	16.32	13315	RL
DARTS (1st) (Liu et al., 2019)	54.30	15.61	16.32	10890	gradient
DARTS (2nd) (Liu et al., 2019)	54.30	15.61	16.32	29902	gradient
GDAS (Dong & Yang, 2019)	93.61±0.09	70.70±0.30	41.84±0.90	28926	gradient
NASWOT (N=10) (Mellor et al., 2021)	92.44±1.13	68.62±2.04	41.31±4.11	3	training-free
NASWOT (N=100) (Mellor et al., 2021)	92.81±0.99	69.48±1.70	43.10±3.16	30	training-free
NASWOT (N=1000) (Mellor et al., 2021)	92.96±0.81	69.98±1.22	44.44±2.10	306	training-free
TE-NAS (Chen et al., 2021)	93.90±0.47	71.24±0.56	42.38±0.46	1558	training-free
KNAS (Xu et al., 2021)	93.05	68.91	34.11	4200	training-free
NASI (T)	93.08±0.24	69.51±0.59	40.87±0.85	30	training-free
NASI (4T)	93.55±0.10	71.20±0.14	44.84±1.41	120	training-free
NAS methods in (Abdelfattah et al., 2021) for the comparison. Table 3 summarizes the comparison,
where the results of our metric are reported under the constraint in (8). Interestingly, our metric
generally achieves a higher positive correlation than other training-free metrics, which confirms the
reasonableness and also the effectiveness of our training-free metric.
C.3 Search in NAS-Bench-201
To further justify the improved search efficiency and competitive search effectiveness of our NASI
algorithm, we also compare it with other state-of-the-art NAS algorithms in NAS-Bench-201 (Dong &
Yang, 2020) on CIFAR-10/100 (C10/100) and ImageNet-16-200 (IN-16-200) 5. Table 4 summarizes
the comparison. Note that the baselines in Table 4 are obtained from TE-NAS (Chen et al., 2021)
paper. Notably, compared with training-based NAS algorithms, our NASI algorithm can achieve
significantly improved search efficiency while maintaining a competitive or even outperforming
test performance. Furthermore, our NASI algorithm is shown to be able to enjoy both improved
search efficiency and effectiveness when compared with most other training-free baselines. Although
TE-NAS, as the best-performing training-free NAS algorithm on both CIFAR-10/100, achieves a
relatively improved test accuracy than our NASI (T), our NASI with a search budget of T = 30s is
50× more efficient than TE-NAS and is able to achieve compelling test performance on all the three
datasets. Moreover, by providing a larger search budget, our NASI algorithm (i.e., NASI (4T)) can in
fact achieve comparable (on CIFAR-10/100) or even better (on ImageNet-16-120) search results with
12× lower search cost compared with TE-NAS.
C.4 Architectures Selected by NASI
The final selected cells in the DARTS search space (i.e., NASI-FIX and NASI-ADA used in Sec. 5.2)
are illustrated in Figure 5, where different operations are denoted with different colors for clarification.
5ImageNet-16-200 is a down-sampled variant of ImageNet (ImageNet16×16) (Chrabaszcz et al., 2017)
21
Published as a conference paper at ICLR 2022
Sep-5
(a) NASI-FIX (Normal)
Sep-5
(b) NASI-FIX (Reduction)
(c) NASI-ADA (Normal)
(d) NASI-ADA (Reduction)
Figure 5:	The final selected normal and reduction cells of NASI-FIX and NASI-ADA in the reduced
DARTS search space on CIFAR-10. Note that x0 , x1 denote the input nodes, x2 , x3 , x4 , x5 denote
intermediate nodes and x6 denotes the output node as introduced in Appendix B.2.
22
Published as a conference paper at ICLR 2022
Table 5: Performance comparison among SOTA image classifiers on ImageNet. Note that architec-
tures followed by C10 are transferred from the CIFAR-10 dataset, while architectures followed by
ImageNet are directly selected on ImageNet.
Architecture	Test Error (%)		Params (M)	+× (M)	Search Cost (GPU Days)
	Top-1	Top-5			
Inception-v1 (Szegedy et al., 2015)	30.1	10.1	6.6	1448	-
MobileNet (Howard et al., 2017)	29.4	10.5	4.2	569	-
ShuffleNet 2×(v2) (Ma et al., 2018)	25.1	7.6	7.4	591	-
NASNet-A (Zoph et al., 2018)	26.0	8.4	5.3	564	2000
AmoebaNet-A (Real et al., 2019)	25.5	8.0	5.1	555	3150
PNAS (Liu et al., 2018)	25.8	8.1	5.1	588	225
MnasNet-92 (Tan et al., 2019)	25.2	8.0	4.4	388	-
DARTS (Liu et al., 2019)	26.7	8.7	4.7	574	4.0
SNAS (mild) (Xie et al., 2019)	27.3	9.2	4.3	522	1.5
GDAS (Dong & Yang, 2019)	26.0	8.5	5.3	581	0.21
ProxylessNAS (Cai et al., 2019)	24.9	7.5	7.1	465	8.3
P-DARTS (Chen et al., 2019)	24.4	7.4	4.9	557	0.3
DARTS- (Chu et al., 2020)	23.8	7.0	4.5	467	4.5
SDARTS-ADV (Chen & Hsieh, 2020)	25.2	7.8	5.4	594	1.3
TE-NAS (C10) (Chen et al., 2021)	26.2	8.3	5.0	-	0.05
TE-NAS (ImageNet) (Chen et al., 2021)	24.5	7.5	5.4	-	0.17
NASI-FIX (C10)	24.3	7.3	5.2	585	0.01
NASI-FIX (ImageNet)	24.4	7.4	5.5	615	0.01
NASI-ADA (C10)	25.0	7.8	4.9	559	0.01
NASI-ADA (ImageNet)	24.8	7.5	5.2	585	0.01
Interestingly, according to the definitions and findings in (Shu et al., 2020), these final selected cells by
NASI are relatively deeper and shallower than the ones selected by other NAS algorithms and hence
may achieve worse generalization performance. Nonetheless, in our experiments, the architectures
constructed with these cells (i.e., NASI-FIX and NASI-ADA) achieve competitive or even better
generalization performance than the ones selected by other NAS algorithms as shown in Table 1 and
Table 5. A possible explanation is that our NASI algorithm provides a good trade-off between the
complexity of architectures and the optimization in their model training, while other NAS algorithms
implicitly prefer architectures with smaller complexity and faster convergence rate as revealed in (Shu
et al., 2020). Note that the final selected cells in NASI-FIX and NASI-ADA are of great similarity
to each other, which hence implies that the adaptive determination and the fixed determination of
constraint ν share similar effects on the selection of final architectures.
C.5 Evaluation on ImageNet
We also evaluate the performance of the final architectures selected by NASI on ImageNet and
summarize the results in Table 5. Notably, NASI-FIX and NASI-ADA outperform the expert-
designed architecture ShuffleNet 2×(v2), NAS-based architecture MnasNet-92 and DARTS by a
large margin, and are even competitive with best-performing one-shot NAS algorithm DARTS-.
Notably, while achieving better generalization performance than TE-NAS (ImageNet), NASI (C10)
is even shown to be more efficient by directly transferring the architectures selected on CIFAR-10
to ImageNet based on its provable transferability in Sec. 4. Meanwhile, by directly searching on
ImageNet, NASI-ADA (ImageNet) is able to achieve further improved performance over NASI-ADA
(C10) while the performance of NASI-FIX (ImageNet) and NASI-FIX (C10) are quite similar.6
Above all, these results on ImageNet further confirm the good transferability of the architectures
selected by NASI to larger-scale datasets.
6In order to maintain the same initial channels between NASI-FIX (ImageNet) and NASI-FIX (C10), the
multiply-add operations of NASI-FIX (ImageNet) has to be larger than 600M by a small margin.
23
Published as a conference paper at ICLR 2022
Table 6: Search with varying architecture widths N in the three search spaces OfNAS-Bench-IShotI.
Spaces	N=2	N=4	N = 8	N=16	N = 32
S1	7.0±0.6	8.3±1.8	8.0±2.2	7.3±1.5	6.5±0.2
S2	7.3±0.7	8.0±1.5	7.3±0.6	7.3±0.4	7.0±0.2
S3	7.6±0.8	7.7±1.1	6.8±0.1	6.8±0.3	6.4±0.2
Table 7: Pearson correlation between our NTK trace norm approximation and the exact NTK trace
norm in the three search spaces of NAS-Bench-1Shot1.
Spaces	Mini-batch						Sum
	b=4	b=8	b=16	b=32	b=64	b=128	
S1	0.74	0.80	0.84	0.83	0.85	0.86	0.88
S2	0.82	0.87	0.90	0.89	0.91	0.91	0.92
S3	0.66	0.74	0.81	0.79	0.82	0.84	0.87
C.6 Ablation Study and Analysis
The impacts of architecture width. As our theoretically grounded performance estimation of
neural architectures relies on an infinite width assumption (i.e., n → ∞ in Proposition 1), we
investigate the impacts of varying architecture width on the generalization performance of final
selected architectures by our NASI. We adopt the same search settings in Sec. 5.1 but with a varying
architecture width (i.e., a varying number of initial channels) on CIFAR-10 for the three search spaces
of NAS-Bench-1Shot1. Table 6 shows results of the effectiveness of our NASI in NAS-Bench-1Shot1
with varying architecture widths N: NASI consistently selects well-performing architectures and a
larger width (N = 32) enjoys better search results. Hence, the infinite width assumption for NTK
does not cause any empirical issues for our NASI.
The effectiveness of NTK trace norm approximations. Since the trace norm of NTK is costly
to evaluate, we have provided our approximation to it in Sec. 3.2. In this section, we empirically
validate the effectiveness of our NTK trace norm approximation. Specifically, we evaluate the Pearson
correlation between our approximations (including the approximations using the sum of sample
gradient norm in the first inequality of (11) and the approximations using mini-batch gradient norm
in (12)) and the exact NTK trace norm under varying batch size in the three search spaces of NAS-
Bench-1Shot1. The results are summarized in Table 7. The results confirm that our approximation
is reasonably good to estimate the exact NTK trace norm of different architectures by achieving a
high Pearson correlation between our approximations and the exact NTK trace norm. Interestingly, a
larger batch size of mini-batch gradient norm generally achieves a better approximation, and the sum
of sample gradient norm achieves the best approximation, which can be explained by the possible
approximation errors we introduced when deriving our (11) and (12).
The impacts of batch size. Since we only adopt a mini-batch to approximate the NTK trace norm
shown in Sec. 3.2, we further examine the impacts of varying batch size on the search results in the
three search spaces of NAS-Bench-1Shot1 in this section. Table 8 summarizes the search results.
Interestingly, the results show that our approximation under varying batch sizes achieves comparable
search results, further confirming the effectiveness of our approximations in select well-performing
architectures.
The impacts of constraint V and penalty coefficient μ. Based on the analysis in Sec. 3.3 and the
results in Appendix C.1, the choice of constraint V and penalty coefficient μ is thus non-trivial to
select best-performing architectures since they trade off the complexity of final selected architectures
and the optimization in their model training. In this section, we demonstrate their impacts on the
generalization performance of the finally selected architectures and also the effectiveness of our fixed
determination on the constraint V in detail. Notably, we adopt the same settings (including the search
and training settings) as those in Appendix C.1 on the DARTS search space, where V0 in the fixed
determination method introduced in Appendix B.1 is modified manually for the comparison.
24
Published as a conference paper at ICLR 2022
Table 8: Search with varying batch sizes b in the three search spaces of NAS-Bench-1Shot1.
Spaces	b=8	b=16	b = 32	b=64	b= 128
S1	6.8±0.3	6.7±0.2	6.7±0.1	6.0±0.3	7.0±0.3
S2	6.9±0.2	7.3±0.4	7.1±0.2	7.2±0.3	6.8±0.2
S3	7.0±0.3	6.6±0.2	6.7±0.2	6.6±0.1	7.1±0.2
10.11
3.23
O
O I
2
IMl
1
沙—
1
50,
3.27
4.07
4.07
4.07
3.16
3.16
3.23
2.70
2.78
2.91
3.09
3.25
3.19
2.94
3.39
3.48
3.20
密)」O」LUISəj.
'O
'4
Test Error (%)
I-,50
4
，90
3
30
S
，70
2
，2盛
1
2 3 4
5
3
0 4 3
11
(a) Joint impacts	(b) Individual impacts
Figure 6:	The impacts of constraint V and penalty coefficient μ on the generalization performance of
the final selected architectures by NASI: (a) their joint impacts, and (b) their individual impacts.
Figure 6 summarizes their impacts into two parts: the joint impacts in Figure 6(a) and the individual
impacts in Figure 6(b). Notably, in both plots, with the increasing of V or μ, the test error of the final
selected architecture by NASI gradually decreases to a minimal and then increase steadily, hence
further validating the existence of the trade-off introduced by V and μ. Moreover, the final selected
architectures with V=100 significantly outperform other selected architectures. This result thus
supports the effectiveness of our fixed determination method for V in NASI. Interestingly, Figure 6(b)
reveals a less obvious decreasing trend of test error for μ compared with V, which hence further
implies the greater importance of constraint V than penalty coefficient μ in terms of the generalization
performance of the final selected architectures. Therefore, in our experiments, we conventionally set
penalty coefficient μ=1 for small-complexity search spaces and μ=2 for large-complexity search
spaces as introduced in Appendix B.1.
25