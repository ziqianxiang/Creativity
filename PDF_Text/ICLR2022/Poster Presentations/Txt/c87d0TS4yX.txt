Published as a conference paper at ICLR 2022
Orchestrated Value Mapping
for Reinforcement Learning
Mehdi Fatemi
Microsoft Research
Montreal, Canada
mehdi.fatemi@microsoft.com
Arash Tavakoli
Max Planck Institute for Intelligent Systems
Tubingen, Germany
arash.tavakoli@tuebingen.mpg.de
Ab stract
We present a general convergent class of reinforcement learning algorithms that
is founded on two distinct principles: (1) mapping value estimates to a different
space using arbitrary functions from a broad class, and (2) linearly decomposing
the reward signal into multiple channels. The first principle enables incorporat-
ing specific properties into the value estimator that can enhance learning. The
second principle, on the other hand, allows for the value function to be repre-
sented as a composition of multiple utility functions. This can be leveraged for
various purposes, e.g. dealing with highly varying reward scales, incorporating a
priori knowledge about the sources of reward, and ensemble learning. Combin-
ing the two principles yields a general blueprint for instantiating convergent al-
gorithms by orchestrating diverse mapping functions over multiple reward chan-
nels. This blueprint generalizes and subsumes algorithms such as Q-Learning,
Log Q-Learning, and Q-Decomposition. In addition, our convergence proof for
this general class relaxes certain required assumptions in some of these algorithms.
Based on our theory, we discuss several interesting configurations as special cases.
Finally, to illustrate the potential of the design space that our theory opens up, we
instantiate a particular algorithm and evaluate its performance on the Atari suite.
1	Introduction
The chief goal of reinforcement learning (RL) algorithms is to maximize the expected return (or
the value function) from each state (Szepesvari, 2010; SUtton & Barto, 2018). For decades, many
algorithms have been proposed to compute target value functions either as their main goal (critic-
only algorithms) or as a means to help the policy search process (actor-critic algorithms). However,
when the environment features certain characteristics, learning the underlying value function can
become very challenging. Examples include environments where rewards are dense in some parts
of the state space but very sparse in other parts, or where the scale of rewards varies drastically. In
the Atari 2600 game of Ms. Pac-Man, for instance, the reward can vary from 10 (for small pellets)
to as large as 5000 (for moving bananas). In other games such as Tennis, acting randomly leads to
frequent negative rewards and losing the game. Then, once the agent learns to capture the ball, it can
avoid incurring such penalties. However, it may still take a very long time before the agent scores
a point and experiences a positive reward. Such learning scenarios, for one reason or another, have
proved challenging for the conventional RL algorithms.
One issue that can arise due to such environmental challenges is having highly non-uniform action
gaps across the state space.1 In a recent study, van Seijen et al. (2019) showed promising results
by simply mapping the value estimates to a logarithmic space and adding important algorithmic
components to guarantee convergence under standard conditions. While this construction addresses
the problem of non-uniform action gaps and enables using lower discount factors, it further opens
a new direction for improving the learning performance: estimate the value function in a different
space that admits better properties compared to the original space. This interesting view naturally
raises theoretical questions about the required properties of the mapping functions, and whether the
guarantees of convergence would carry over from the basis algorithm under this new construction.
1Action gap refers to the value difference between optimal and second best actions (Farahmand, 2011).
1
Published as a conference paper at ICLR 2022
One loosely related topic is that of nonlinear Bellman equations. In the canonical formulation of
Bellman equations (Bellman, 1954; 1957), they are limited in their modeling power to cumulative
rewards that are discounted exponentially. However, one may go beyond this basis and redefine the
Bellman equations in a general nonlinear manner. In particular, van Hasselt et al. (2019) showed
that many such Bellman operators are still contraction mappings and thus the resulting algorithms
are reasonable and inherit many beneficial properties of their linear counterparts. Nevertheless, the
application of such algorithms is still unclear since the fixed point does not have a direct connection
to the concept of return. In this paper we do not consider nonlinear Bellman equations.
Continuing with the first line of thought, a natural extension is to employ multiple mapping functions
concurrently in an ensemble, allowing each to contribute their own benefits. This can be viewed as a
form of separation of concerns (van Seijen et al., 2016). Ideally, we may want to dynamically modify
the influence of different mappings as the learning advances. For example, the agent could start with
mappings that facilitate learning on sparse rewards. Then, as it learns to collect more rewards, the
mapping function can be gradually adapted to better support learning on denser rewards. Moreover,
there may be several sources of reward with specific characteristics (e.g. sparse positive rewards
but dense negative ones), in which case using a different mapping to deal with each reward channel
could prove beneficial.
Building upon these ideas, this paper presents a general class of algorithms based on the combination
of two distinct principles: value mapping and linear reward decomposition. Specifically, we present
a broad class of mapping functions that inherit the convergence properties of the basis algorithm. We
further show that such mappings can be orchestrated through linear reward decomposition, proving
convergence for the complete class of resulting algorithms. The outcome is a blueprint for building
new convergent algorithms as instances. We conceptually discuss several interesting configurations,
and experimentally validate one particular instance on the Atari 2600 suite.
2	Value Mapping
We consider the standard reinforcement learning problem which is commonly modeled as a Markov
decision process (MDP; Puterman (1994)) M = (S, A, P, R, P0, γ), where S and A are the discrete
sets of states and actions, P(s0|s, a) =. P[st+1 =s0 | st =s, at =a] is the state-transition distribution,
R(r|s, a, s0) =. P[rt = r | st = s, at = a, st+1 = s0] (where we assume r ∈ [rmin, rmax]) is the
reward distribution, P0(s) =. P[s0 = s] is the initial-state distribution, and γ ∈ [0, 1] is the discount
factor. A policy π(a∣s) = P[at = a | St = s] defines how an action is selected in a given state.
Thus, selecting actions according to a stationary policy generally results in a stochastic trajectory.
The discounted sum of rewards over the trajectory induces a random variable called the return. We
assume that all returns are finite and bounded. The state-action value function Qπ(s, a) evaluates
the expected return of taking action a at state s and following policy π thereafter. The optimal value
function is defined as Q*(s, a) = max∏ Qn(s, a), which gives the maximum expected return of
all trajectories starting from the state-action pair (s, a). Similarly, an optimal policy is defined as
π*(a∣s) ∈ argmax∏ Qn(s, a). The optimal value function is unique (Bertsekas & Tsitsiklis, 1996)
and can be found, e.g., as the fixed point of the Q-Learning algorithm (Watkins, 1989; Watkins &
Dayan, 1992) which assumes the following update:
Qt+1(St, at) — (I - at)Qt(st, at) + αt(rt + YmaxQt(St+1, aO)),	(I)
a0
where αt is a positive learning rate at time t. Our goal is to map Q to a different space and perform
the update in that space instead, so that the learning process can benefit from the properties of the
mapping space.
We define a function f that maps the value function to some new space. In particular, we consider
the following assumptions:
Assumption 1 The function f(x) is a bijection (either strictly increasing or strictly decreasing) for
all x in the given domain D = [c1, c2] ⊆ R.
Assumption 2 The function f(x) holds the following properties for all x in the given domain D =
[c1,c2] ⊆ R:
1. f is continuous on [c1, c2] and differentiable on (c1, c2);
2
Published as a conference paper at ICLR 2022
2.	|f0(x)| ∈ [δ1, δ2] for x ∈ (c1, c2), with 0 < δ1 < δ2 < ∞;
3.	f is either of semi-convex or semi-concave.
We next use f to map the value function, Q(s, a), to its transformed version, namely
QGa) = f(QGa)).	⑵
Assumption 1 implies that f is invertible and, as such, Q(s, a) is uniquely computable from Q(s, a)
by means of the inverse function f-1. Of note, this assumption also implies that f preserves the
ordering in x; however, it inverts the ordering direction if f is decreasing. Assumption 2 imposes
further restrictions on f, but still leaves a broad class of mapping functions to consider. Throughout
the paper, we use tilde to denote a “mapped” function or variable, while the mapping f is under-
standable from the context (otherwise it is explicitly said).
2.1 Base Algorithm
If mapped value estimates were naively placed in a Q-Learning style algorithm, the algorithm would
fail to converge to the optimal values in stochastic environments. More formally, in the tabular case,
an update of the form (cf. Equation 1)
Qet+ι(st, at) —(I - αt)Qet(st, at) + αf ot+YmaXf I(Qt(St+ι,aO)))	⑶
converges2 to the fixed point Qe(s, a) that satisfies
Qo(S, a)= EsO 〜P (∙∣s,a),r 〜R(∙∣s,a,s0) [f 1 + Y m^ X f- 1 (Q0(S0, a0))J .	(4)
Let Us define the notation Q0(s, a) = f-1 (Q0(s, a)). If f is a semi-convex bijection, f-1 will be
semi-concave and Equation 4 deduces
Q0(s,a) = f-1(Q0(s,a))
=f-1(EsO 〜P (∙∣s,a),r 〜R(∙∣s,a,s0) [f 卜 + Y ma X Qθ(s0, a0))])
≥ Es0〜P(∙∣s,a),r〜R(∙∣s,a,s0)
f-1 f r +Yma0XQ0(S0, a0)
EsO〜P(∙∣s,α), r〜R(∙∣s,a,s0) [r + Y maX QIɔ (S , a )],
(5)
where the third line follows Jensen’s ineqUality. Comparing EqUation 5 with the Bellman optimality
equation in the regular space, i.e. Q*(s, a) = Es0,r〜p,r [r + Ymax。，Q*(s0, a0)], We conclude that
the valUe fUnction to which the Update rUle (3) converges overestimates Bellman’s backUp. Similarly,
if f is a semi-concave function, then Q0(S, a) underestimates Bellman’s backup. Either way, it
follows that the learned value function deviates from the optimal one. Furthermore, the Jensen’s
gap at a given state S — the difference between the left-hand and right-hand sides of Equation 5 —
depends on the action a because the expectation operator depends on a. That is, at a given state S,
the deviation of Q0(s, a) from Q*(s, a) is not a fixed-value shift and can vary for various actions.
Hence, the greedy policy w.r.t. (with respect to) Q0(s, ∙) may not preserve ordering and it may not
be an optimal policy either.
In an effort to address this problem in the spacial case of f being a logarithmic function, van Seijen
et al. (2019) observed that in the algorithm described by Equation 3, the learning rate αt gener-
ally conflates two forms of averaging: (i) averaging of stochastic update targets due to environment
stochasticity (happens in the regular space), and (ii) averaging over different states and actions (hap-
pens in the f’s mapping space). To this end, they proposed to algorithmically disentangle the two
and showed that such a separation will lift the Jensen’s gap if the learning rate for averaging in the
regular space decays to zero fast enough.
Building from Log Q-Learning (van Seijen et al., 2019), we define the base algorithm as follows:
at each time t, the algorithm receives Qt(S, a) and a transition quadruple (S, a,r, S0), and outputs
.1
Qt+1(S, a), which then yields Qt+1(S, a) = f-1 Qt+1(S, a) . The steps are listed below:
2The convergence follows from stochastic approximation theory with the additional steps to show by induc-
tion that Q remains bounded and then the corresponding operator is a contraction mapping.
3
Published as a conference paper at ICLR 2022
Here, the mapping f is any function that satisfies Assumptions 1 and 2. Remark that similarly to
the Log Q-Learning algorithm, Equations 9 and 10 have decoupled averaging of stochastic update
targets from that over different states and actions.
3 Reward Decomposition
Reward decomposition can be seen as a generic way to facilitate (i) systematic use of environmental
inductive biases in terms of known reward sources, and (ii) action selection as well as value-function
updates in terms of communication between an arbitrator and several subagents, thus assembling
several subagents to collectively solve a task. Both directions provide broad avenues for research
and have been visited in various contexts. Russell & Zimdars (2003) introduced an algorithm called
Q-Decomposition with the goal of extending beyond the “monolithic” view of RL. They studied the
case of additive reward channels, where the reward signal can be written as the sum of several reward
channels. They observed, however, that using Q-Learning to learn the corresponding Q function of
each channel will lead to a non-optimal policy (they showed it through a counterexample). Hence,
they used a Sarsa-like update w.r.t. the action that maximizes the arbitrator’s value. Laroche et al.
(2017) provided a formal analysis of the problem, called the attractor phenomenon, and studied a
number of variations to Q-Decomposition. On a related topic but with a different goal, Sutton et al.
(2011) introduced the Horde architecture, which consists of a large number of “demons” that learn
in parallel via off-policy learning. Each demon estimates a separate value function based on its own
target policy and (pseudo) reward function, which can be seen as a decomposition of the original
reward in addition to auxiliary ones. van Seijen et al. (2017) built on these ideas and presented hybrid
reward architecture (HRA) to decompose the reward and learn their corresponding value functions
in parallel, under mean bootstrapping. They further illustrated significant results on domains with
many independent sources of reward, such as the Atari 2600 game of Ms. Pac-Man.
Besides utilizing distinct environmental reward sources, reward decomposition can also be used as a
technically-sound algorithmic machinery. For example, reward decomposition can enable utilization
of a specific mapping that has a limited domain. In the Log Q-Learning algorithm, for example, the
log(∙) function cannot be directly used on non-positive values. Thus, the reward is decomposed such
+
that two utility functions Q+ and Q- are learned for when the reward is non-negative or negative,
respectively. Then the value is given by Q(s, a) = exp Qe+(s, a) - exp Qe-(s, a) . The learning
C 1 Γ∙ Z-∖-l-	1 P∖——F , ,	,	1	.∖	-i'	1	,	, J	,
process of each of Q+ and Q- bootstraps towards their corresponding value estimate at the next
state with an action that is the arg max
+
of the actual Q, rather than that of Q+ and Q- individually.
We generalize this idea to incorporate arbitrary decompositions, beyond only two channels. To be
specific, we are interested in linear decompositions of the reward function into L separate channels
r(j), forj = 1 . . . L, in the following way:
L
r := X λjr(j),	(11)
j=1
with λj ∈ R. The channel functions r(j) map the original reward into some new space in such a way
that their weighted sum recovers the original reward. Clearly, the case of L = 1 and λ1 = 1 would
retrieve the standard scenario with no decomposition. In order to provide the update, expanding
from Log Q-Learning, we define Qe(j) forj = 1 . . . L, corresponding to the above reward channels,
4
Published as a conference paper at ICLR 2022
and construct the actual value function Q using the following:
L
Qt(s, a) := X λj fj-1 Qe(tj)(s,a) .	(12)
j=1
We explicitly allow the mapping functions, fj, to be different for each channel. That is, each reward
channel can have a different mapping and each Qe(j) is learned separately under its own mapping.
Before discussing how the algorithm is updated with the new channels, we present a number of
interesting examples of how Equation 11 can be deployed.
As the first example, we can recover the original Log Q-Learning reward decomposition by consid-
ering L = 2, λ1 = +1, λ2 = -1, and the following channels:
r(1) := rt ifrt ≥ 0
t 0 otherwise
; r(2) := |rt| ifrt < 0
t 0 otherwise
(13)
Notice that the original reward is retrieved via rt = rt(1) - rt(2). This decomposition allows for using
a mapping with only positive domain, such as the logarithmic function. This is an example of using
reward decomposition to ensure that values do not cross the domain of mapping function f.
In the second example, we consider different magnifications for different sources of reward in the
environment so as to make the channels scale similarly. The Atari 2600 game of Ms. Pac-Man is an
example which includes rewards with three orders of magnitude difference in size. We may therefore
use distinct channels according to the size-range of rewards. To be concrete, let r ∈ [0, 100] and
consider the following two configurations for decomposition (can also be extended to other ranges).
Configuration 1:		λ =	10, λ3 = 100	Configuration 2:			=9,	λ3 = 90
λι 二	二 1,			λι 二	二 1,	入2		
r(1) r t	:= {	rt if rt ∈ [0,1] 0 rt > 1		r(1) r t	:=<	frt U	if rt ∈ [0,1] rt > 1	
r(2) r t	I	'0	if rt ≤ 1	r(2) r t		∣0		if rt ≤ 1
	:= ∖	0.1rt	if rt ∈ (1,10]		:=<	(Tt	— 1)/9	if rt ∈ (1,10]
	I	、0	rt > 10			11		rt > 10
r(3) r t	:= {	0 0.01rt	if rt ≤ 10 if rt ∈ (10,100]	r(3) r t	:=<	「。 l(rt	if rt ≤ 10 -10)/90 if rt ∈ (10,100]	
Each of the above configurations presents certain characteristics. Configuration 1 gives a scheme
where, at each time step, at most one channel is non-zero. Remark, however, that each channel will
be non-zero with less frequency compared to the original reward signal, since rewards get assigned to
different channels depending on their size. On the other hands, Configuration 2 keeps each channel
to act as if there is a reward clipping at its upper bound, while each channel does not see rewards
below its lower bound. As a result, Configuration 2 fully preserves the reward density at the first
channel and presents a better density for higher channels compared to Configuration 1. However,
the number of active channels depends on the reward size and can be larger than one. Importantly,
the magnitude of reward for all channels always remains in [0, 1] in both configurations, which could
be a desirable property. The final point to be careful about in using these configurations is that the
large weight of higher channels significantly amplifies their corresponding value estimates. Hence,
even a small estimation error at higher channels can overshadow the lower ones.
Over and above the cases we have presented so far, reward decomposition enables an algorithmic
machinery in order to utilize various mappings concurrently in an ensemble. In the simplest case,
we note that in Equation 11 by construction we can always write
L
r(j) := r and X λj := 1.
j=1
(14)
5
Published as a conference paper at ICLR 2022
That is, the channels are merely the original reward with arbitrary weights that should sum to one.
We can then use arbitrary functions fj for different channels and build the value function as pre-
sented in Equation 12. This construction directly induces an ensemble of arbitrary mappings with
different weights, all learning on the same reward signal. More broadly, this can potentially be com-
bined with any other decomposition scheme, such as the ones we discussed above. For example, in
the case of separating negative and positive rewards, one may also deploy two (or more) different
mappings for each of the negative and positive reward channels. This certain case results in four
channels, two negative and two positive, with proper weights that sum to one.
4 Orchestration of Value-Mappings using Decomposed Rewards
4.1	Algorithm
To have a full orchestration, we next combine value mapping and reward decomposition. We follow
the previous steps in Equations 6-10, but now also accounting for the reward decomposition. The
core idea here is to replace Equation 6 with 12 and then compute Qe(j) for each reward channel
in parallel. In practice, these can be implemented as separate Q tables, separate Q networks, or
different network heads with a shared torso. At each time t, the algorithm receives all channel
outputs Qet(j), for j = 1 . . . L, and updates them in accordance with the observed transition. The
complete steps are presented in Algorithm 1.
A few points are apropos to remark. Firstly, the steps in the for-loop can be computed in parallel
for all L channels. Secondly, as mentioned previously, the mapping function fj may be different
for each channel; however, the discount factor γ and both learning rates βf,t and βreg,t are shared
among all the channels and must be the same. Finally, note also that the action <⅛+ι, from which all
the channels bootstrap, comes from arg maxa0 Qt(st+1, a0) and not the local value of each channel.
This directly implies that each channel-level value Q(j) = f-1(Qej) does not solve a channel-level
Bellman equation by itself. In other words, Q(j) does not represent any specific semantics such as
expected return corresponding to the rewards of that channel. They only become meaningful when
they compose back together and rebuild the original value function.
4.2	Convergence
We establish convergence of Algorithm 1 by the following theorem.
Algorithm 1: Orchestrated Value Mapping.
Input: (at time t)
Qe(tj ) for j = 1 . . . L
st, at, rt, and st+1
Output: Qet(+j )1 for j = 1 . . . L
Compute r(j for j = 1 ...L
begin
1	Qt(st, at) := Pj* L=1 λj fj-1 Qe(tj)(st, at)
2	at+ι := argmaxao (Qt(St+ι,a0))
for j = 1 to L do
3	Ut(j) := rt(j) +γfj-1 Qet(j)(st+1,aat+1)
4	UV := f-1 (Q(j)(st,at)) + βreg,t (Utj)- f-1 (Qejj)(st,at)))
5	Qt+ι(st,at) := Qtj)(St,at) + βf,t (fj (Utj))- QY)(St, at)
end
end
6
Published as a conference paper at ICLR 2022
Theorem 1 Let the reward admit a decomposition as defined by Equation 11, Qt(st, at) be defined
by Equation 12, and all Qe(tj) (st, at) updated according to the steps of Algorithm 1. Assume further
that the following hold:
1.	All fj ’s satisfy Assumptions 1 and 2;
2.	TD error in the regular space (second term in line 4 of Algorithm 1) is bounded for all j;
3.	Pt=0 βf,t ∙ βreg,t = ∞;
4.	P∞=0(βf,t ∙ βreg,t)2 < ∞；
5.	βf,t ∙ ∣βreg,t → 0 as t → ∞.
Then, Qt(s,a) converges to Q↑ (s, a) with probability onefor all state-action pairs (s,a).
The proof follows basic results from stochastic approximation theory (Jaakkola et al., 1994; Singh
et al., 2000) with important additional steps to show that those results hold under the assumptions of
Theorem 1. The full proof is fairly technical and is presented in Appendix A.
We further remark that Theorem 1 only requires the product βf,t ∙ βreg,t to go to zero. As this product
resembles the conventional learning rate in Q-Learning, this assumption is no particular limitation
compared to traditional algorithms. We contrast this assumption with the one in the previous proof
of Log Q-Learning which separately requires βreg to go to zero fast enough. We note that in the
case of using function approximation, as in a DQN-like algorithm (Mnih et al., 2015), the update
in line 5 of Algorithm 1 should naturally be managed by the used optimizer, while line 4 may be
handled manually. This has proved challenging as the convergence properties can be significantly
sensitive to learning rates. To get around this problem, van Seijen et al. (2019) decided to keep βreg,t
at a fixed value in their deep RL experiments, contrary to the theory. Our new condition, however,
formally allows βreg,t to be set to a constant value as long as βf,t properly decays to zero.
A somewhat hidden step in the original proof of Log Q-Learning is that the TD error in the regular
space (second term in Equation 9) must always remain bounded. We will make this condition
explicit. In practice, with bounds of the reward being known, one can easily find bounds of return
in regular as well as fj spaces, and ensure boundness of Ut(j) - fj-1(Qet(j)) by proper clipping.
Notably, clipping of Bellman target is also used in the literature to mitigate the value overflow issue
(Fatemi et al., 2019). The scenarios covered by Assumption 2, with the new convergence proof due
to Theorem 1, may be favorable in many practical cases. Moreover, several prior algorithms such as
Q-Learning, Log Q-Learning, and Q-Decomposition can be derived by appropriate construction
from Algorithm 1.
4.3	Remarks
Time-dependent Channels
The proof of Theorem 1 does not directly involve the channel weights λj . However, changing them
will impact Qt, which changes the action )⅛+ι in line 2 of Algorithm 1. In the case that λj's vary
with time, if they all converge to their final fixed value soon enough before the learning rates become
too small, and if additionally all state-action pairs are still visited frequently enough after λj ’s are
settled to their final values, then the algorithm should still converge to optimality. Of course, this
analysis is far from formal; nevertheless, we can still strongly conjecture that an adaptive case where
the channel weights vary with time should be possible to design.
Slope of Mappings
Assumption 2 asserts that the derivative of fj must be bounded from both below and above. While
this condition is sufficient for the proof of Theorem 1, we can probe its impact further. The proof
basically demonstrates a bounded error term, which ultimately converges to zero under the condi-
tions of Theorem 1. However, the bound on this error term (see Lemma 2 in Appendix A) is scaled
by δmax = maxj δ(j), with δ(j) being defined as
δ(j) = δ2(j)/ δ1(j) - 1,	(15)
7
Published as a conference paper at ICLR 2022
where δ1(j) and δ2(j) are defined according to Assumption 2 (0 < δ1(j) ≤ |fj0(x)| ≤ δ2(j)). In the
case of fj being a straight line δ(j) = 0, thus no error is incurred and the algorithm shrinks to
Q-Learning. An important extreme case is when δ1(j) is too small while δ2(j) is not close to δ1(j) .
It then follows from Equation 15 that the error can be significantly large and the algorithm may
need a long time to converge. This can also be examined by observing that if the return is near the
areas where fj0 is very small, the return may be too compressed when mapped. Consequently, the
agent becomes insensitive to the change of return in such areas. This problem can be even more
significant in deep RL due to more complex optimization processes and nonlinear approximations.
The bottom-line is that the mapping functions should be carefully selected in light of Equation 15 to
avoid extremely large errors while still having desired slopes to magnify or suppress the returns when
needed. This analysis also explains Why logarithmic mappings of the form f (x) = C ∙ log(χ + d)
(as investigated in the context of Log Q-Learning by van Seijen et al. (2019)) present unfavorable
results in dense reWard scenarios; e.g. in the Atari 2600 game of Skiing Where there is a reWard at
every step. In this expression c is a mapping hyperparameter that scales values in the logarithmic
space and d is a small positive scalar to ensure bounded derivatives, Where the functional form of
the derivative is given by f0(χ) = χ^. Hence, δ2 = d, whereas δι can be very close to zero
depending on the maximum return. As a result, When learning on a task Which often faces large
returns, Log Q-Learning operates mostly on areas of f where the slope is small and, as such, it can
incur significant error compared to standard Q-Learning. See Appendix B for a detailed illustration
of this issue, and Appendix D for the full list of reward density variations across a suite of 55 Atari
2600 games.
5 Experimental Results
In this section, we illustrate the simplicity and utility of instantiating new learning methods based on
our theory. Since our framework provides a very broad algorithm class with numerous possibilities,
deep and meaningful investigations of specific instances go far beyond the scope of this paper (or
any single conference paper). Nevertheless, as an authentic illustration, we consider the LogDQN
algorithm (van Seijen et al., 2019) and propose an altered mapping function. As discussed above, the
logarithmic mapping in LogDQN suffers from a too-small slope when encountering large returns.
We lift this undesirable property while keeping the desired magnification property around zero.
Specifically, we substitute the logarithmic mapping with a piecewise function that at the break-point
x = 1 - d switches from a logarithmic mapping to a straight line with slope c (i.e. the same slope
as c ∙ log(x + d) at X = 1 一 d):
f(x) :
(c ∙ log(x + d)
[c ∙ (X — 1 + d)
if x ≤ 1 一 d
if X > 1 一 d
We call the resulting method LogLinDQN, or Logarithmic-Linear DQN. Remark that choosing
x = 1 一 d as the break-point has the benefit of using only a single hyperparameter C to determine both
the scaling of the logarithmic function and the slope of the linear function, which otherwise would
require an additional hyperparameter. Also note that the new mapping satisfies Assumptions 1 and
2. We then use two reward channels for non-negative and negative rewards, as discussed in the first
example of Section 3 (see Equation 13), and use the same mapping function for both channels.
Our implementation of LogLinDQN is based on Dopamine (Castro et al., 2018) and closely matches
that of LogDQN, with the only difference being in the mapping function specification. Notably, our
LogLin mapping hyperparameters are realized using the same values as those of LogDQN; i.e.
c = 0.5 and d ≈ 0.02. We test this method in the Atari 2600 games of the Arcade Learning Envi-
ronment (ALE) (Bellemare et al., 2013) and compare its performance primarily against LogDQN
and DQN (Mnih et al., 2015), denoted by “Lin” or “(Lin)DQN” to highlight that it corresponds
to a linear mapping function with slope one. We also include two other major baselines for refer-
ence: C51 (Bellemare et al., 2017) and Rainbow (Hessel et al., 2018). Our tests are conducted on
a stochastic version of Atari 2600 using sticky actions (Machado et al., 2018) and follow a unified
evaluation protocol and codebase via the Dopamine framework (Castro et al., 2018).
Figure 1 shows the relative human-normalized score of LogLinDQN w.r.t. the worst and best of
LogDQN and DQN for each game. These results suggest that LogLinDQN reasonably unifies the
good properties of linear and logarithmic mappings (i.e. handling dense or sparse reward distribu-
tions respectively), thereby enabling it to improve upon the per-game worst of LogDQN and DQN
8
Published as a conference paper at ICLR 2022
LogLin vs. min{Log, Lin}
_____________________.-BHBaBlIIllI
-200%
400% -
200% -
0% -
LogLin vs. max{Log, Lin}
-200%
Figure 1: Difference in human-normalized score for 55 Atari 2600 games, LogLinDQN versus
the worst (top) and best (bottom) of LogDQN and (Lin)DQN. Positive % means LogLinDQN
outperforms the per-game respective baseline.
∂JOOmPB∙aIBΠUON,m UmH
Rainbow
C51
=(Lin)DQN
LogDQN
LogLinDQN
Figure 2: Human-normalized mean (left) and median (right) scores across 55 Atari 2600 games.
(top panel) and perform competitively against the per-game best of the two (bottom panel) across a
large set of games. Figure 2 shows median and mean human-normalized scores across a suite of 55
Atari 2600 games. Our LogLinDQN agent demonstrates a significant improvement over most base-
lines and is competitive with Rainbow in terms of mean performance. This is somewhat remarkable
provided the relative simplicity of LogLinDQN, especially, w.r.t. Rainbow which combines several
other advances including distributional learning, prioritized experience replay, and n-step learning.
6 Conclusion
In this paper we introduced a convergent class of algorithms based on the composition of two distinct
foundations: (1) mapping value estimates to a different space using arbitrary functions from a broad
class, and (2) linearly decomposing the reward signal into multiple channels. Together, this new
family of algorithms enables learning the value function in a collection of different spaces where
the learning process can potentially be easier or more efficient than the original return space. Ad-
ditionally, the introduced methodology incorporates various versions of ensemble learning in terms
of linear decomposition of the reward. We presented a generic proof, which also relaxes certain
limitations in previous proofs. We also remark that several known algorithms in classic and recent
literature can be seen as special cases of the present algorithm class. Finally, we contemplate re-
search on numerous special instances as future work, following our theoretical foundation. Also, we
believe that studying the combination of our general value mapping ideas with value decomposition
(Tavakoli et al., 2021), instead of the the reward decomposition paradigm studied in this paper, could
prove to be a fruitful direction for future research.
9
Published as a conference paper at ICLR 2022
Reproducibility Statement
We release a generic codebase, built upon the Dopamine framework (Castro et al., 2018), with the
option of using arbitrary compositions of mapping functions and reward decomposition schemes as
easy-to-code modules. This enables the community to easily explore the design space that our theory
opens up and investigate new convergent families of algorithms. This also allows to reproduce the
results of this paper through an accompanying configuration script. The source code can be accessed
at: https://github.com/microsoft/orchestrated- value- mapping.
References
Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The Arcade Learning Envi-
ronment: An evaluation platform for general agents. Journal of Artificial Intelligence Research,
47:253-279, 2013.
Marc G. Bellemare, Will Dabney, and Remi Munos. A distributional perspective on reinforcement
learning. In Proceedings of the International Conference on Machine Learning, pp. 449-458,
2017.
Richard E. Bellman. The theory of dynamic programming. Technical Report P-550, The RAND
Corporation, 1954.
Richard E. Bellman. Dynamic Programming. Princeton University Press, 1957.
Dimitri P. Bertsekas and John N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific, 1996.
Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G. Bellemare.
Dopamine: A research framework for deep reinforcement learning. arXiv:1812.06110, 2018.
Amir-massoud Farahmand. Action-gap phenomenon in reinforcement learning. In Advances in
Neural Information Processing Systems, 2011.
Mehdi Fatemi, Shikhar Sharma, Harm van Seijen, and Samira Ebrahimi Kahou. Dead-ends and
secure exploration in reinforcement learning. In Proceedings of the International Conference on
Machine Learning, pp. 1873-1881, 2019.
Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan
Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in
deep reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence,
pp. 3215-3222, 2018.
Tommi Jaakkola, Michael I. Jordan, and Satinder P. Singh. On the convergence of stochastic iterative
dynamic programming algorithms. Neural Computation, 6(6):1185-1201, 1994.
Romain Laroche, Mehdi Fatemi, Joshua Romoff, and Harm van Seijen. Multi-advisor reinforcement
learning. arXiv:1704.00756, 2017.
Marlos C. Machado, Marc G. Bellemare, Erik Talvitie, Joel Veness, Matthew J. Hausknecht, and
Michael Bowling. Revisiting the Arcade Learning Environment: Evaluation protocols and open
problems for general agents. Journal of Artificial Intelligence Research, 61:523-562, 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529-533, 2015.
Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John
Wiley & Sons, 1994.
Stuart Russell and Andrew L. Zimdars. Q-Decomposition for reinforcement learning agents. In
Proceedings of the International Conference on Machine Learning, pp. 656-663, 2003.
10
Published as a conference paper at ICLR 2022
Satinder P. Singh, Tommi Jaakkola, Michael L. Littman, and Csaba Szepesvari. Convergence results
for single-step on-policy reinforcement-learning algorithms. Machine Learning, 38(3):287-308,
2000.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press, 2nd
edition edition, 2018.
Richard S. Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M. Pilarski, Adam White,
and Doina Precup. Horde: A scalable real-time architecture for learning knowledge from unsuper-
vised sensorimotor interaction. In Proceedings of the International Conference on Autonomous
Agents and Multiagent Systems, pp. 761-768, 2011.
Csaba Szepesvari. Algorithmsfor Reinforcement Learning. Morgan & Claypool, 2010.
Arash Tavakoli, Mehdi Fatemi, and Petar Kormushev. Learning to represent action values as a
hypergraph on the action vertices. In Proceedings of the International Conference on Learning
Representations, 2021.
Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with Double Q-
Learning. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 2094-2100,
2016.
Hado van Hasselt, John Quan, Matteo Hessel, Zhongwen Xu, Diana Borsa, and Andre Barreto.
General non-linear Bellman equations. arXiv:1907.03687, 2019.
Harm van Seijen, Mehdi Fatemi, Joshua Romoff, and Romain Laroche. Separation of concerns in
reinforcement learning. arXiv:1612.05159, 2016.
Harm van Seijen, Mehdi Fatemi, Joshua Romoff, Romain Laroche, Tavian Barnes, and Jeffrey
Tsang. Hybrid reward architecture for reinforcement learning. In Advances in Neural Information
Processing Systems, 2017.
Harm van Seijen, Mehdi Fatemi, and Arash Tavakoli. Using a logarithmic mapping to enable lower
discount factors in reinforcement learning. In Advances in Neural Information Processing Sys-
tems, 2019.
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando de Freitas.
Dueling network architectures for deep reinforcement learning. In Proceedings of the Interna-
tional Conference on Machine Learning, pp. 1995-2003, 2016.
Christopher J. C. H. Watkins. Learning from Delayed Rewards. PhD thesis, University of Cam-
bridge, 1989.
Christopher J. C. H. Watkins and Peter Dayan. Q-Learning. Machine Learning, 8(3):279-292, 1992.
11
Published as a conference paper at ICLR 2022
A	Proof of Theorem 1
We use a basic convergence result from stochastic approximation theory. In particular, we invoke
the following lemma, which has appeared and proved in various classic texts; see, e.g., Theorem 1
in Jaakkola et al. (1994) or Lemma 1 in Singh et al. (2000).
Lemma 1 Consider an algorithm of the following form:
△t+1 (x) := (1 - αt)∆t(x) + αtψt(x),	(16)
with X being the state variable (or vector of variables), and at and ψt denoting respectively
the learning rate and the update at time t. Then, △ converges to zero w.p. (with probability)
one as t → ∞ under thefollowing assumptions:
1.	The state space is finite;
2.	Et αt = ∞ and Et α2 < ∞;
3.	∣∣E{ψt(x) | Ft}∣∣w ≤ ξ ∣∣∆t(x)∣∣w, with ξ ∈ (0,1) and || ∙ ||w denoting a weighted
max norm;
4.	Var {ψt(x) | Ft} ≤ C (1 + ∣∣∆t(x)∣∣w )2, for some constant C;
where Ft is the history ofthe algorithm until time t.
Remark that in applying Lemma 1, the △t process generally represents the difference between a
stochastic process of interest and its optimal value (that is Qt and Qt), and X represents a proper Con-
catenation of states and actions. In particular, it has been shown that Lemma 1 applies to Q-Learning
as the TD update of Q-Learning satisfies the lemma’s assumptions 3 and 4 (Jaakkola et al., 1994).
We define Q(tj) (s, a) := f-1 Qet(j)(s, a) , for j = 1 . . . L. Hence,
LL
Qt(s, a) = X λj fj-1 Qet(j)(s,a) = XλjQt(j)(s,a).	(17)
j=1	j=1
We next establish the following key result, which is core to the proof of Theorem 1. The proof is
given in the next section.
Lemma 2 Following Algorithm 1,for each ChanneI j ∈ {1,...,L} we have
Q(+ι(st, at) = Qj)(St, at) + βreg,t ∙ βf,t (Utj)- Qj)(St, at) + ej)) ,	(18)
with the error term satisfying thefollowings:
1.	Bounded by TD error in the regular space with decaying coefficient
Iej)| ≤ βreg,t∙ βf,t ∙ δj)∣Utj)- Qj)(St,a/,	(19)
where δj) = δj/ δ(j) — 1 is a positive constant;
2.	For a given f, ej does not change Signfor all t (it is either always non-positive or
always non-negative);
3.	ej is fully measurable given the variables defined at time t.
From Lemma 2, it follows that for each channel:
Qj)l(st, at) = Qj(St, at) + βreg,t ∙ βf,t (Utj)- Qj(St, at) + e j)) ,	(20)
12
Published as a conference paper at ICLR 2022
with e(tj) converging to zero w.p. one under condition 4 of the theorem, and Ut(j) defined as:
Uyj=r(j)+YQj)(St+d
Multiplying both sides of Equation 20 by λj and taking the summation, we write:
LL	L
X λjQ(+)ι(St, at)=x λj Qyj(S t, at)+βreg,t ∙ ft x λj (Pyj- Qyj(S t, at)+e(j)).
j=1	j=1	j=1
Hence, using Equation 17 we have:
L
Qt+1(st, at) = Qt(St, at) + Breg,t ∙ ∣βf,t SX λj (UF) - Qyj(St, at) + ej))
y=1
L
=Qt(St,at) + βreg,t ∙ βf,t X λj (rjj + YQyj(St+1 *t+1 ) - QY)(St,at) + ej)
y=1
=Qt(St, at) + βreg,t ∙ ∣β f,t I rt + Y Qt(St+1, 々t+1)- Qt(St, at) + ɪ2 λj eY' I ∙
y=1
(21)
Definition of at+ι deduces that
Qt(St+ι,at+ι) = Qt ( St+1, arg max Qt(St+ι ,a0) ) = max Qt(St+ι ,a0).
a0	a0
By defining et := Py=1 λye(tyj, we rewire Equation 21 as the following:
Qt+1(St, at) = Qt(St, at) + βreg,t ∙ ∣βf,t 卜 t + Y m?X Qt(St+1, a) - Qt(St, at) + et) .	(22)
This is a noisy Q-Learning algorithm with the noise term decaying to zero at a quadratic rate w.r.t.
the learning rate's decay; more precisely, in the form of (βreg,t ∙ βf,t)2.
Lemma 1 requires the entire update to be properly bounded (as stated in its assumptions 3 and 4). It
has been known from the proof of Q-Learning (Jaakkola et al., 1994) that TD error satisfies these
conditions, i.e. rt + Y maxa0 Qt(St+1, a0) - Qt(St, at) satisfies assumptions 3 and 4 of Lemma 1.
To prove convergence of mapped Q-Learning, we therefore require to show that |et | also satisfies a
similar property; namely, not only it disappears in the limit, but also it does not interfere intractably
with the learning process during training. To this end, we next show that as the learning continues,
|et | is indeed bounded by a value that can be arbitrarily smaller than the TD error. Consequently, as
TD error satisfies assumptions 3 and 4 of Lemma 1, so does |et|, and so does their sum.
Let δmax = maxy δ(yj, with δ(yj defined in Lemma 2. Multiplying both sides of Equation 19 by λy
and taking the summation over j , it yields:
L L
ιeti = xλje(jj ≤ Xkjeyjl
y=1	y=1
L
≤ X lλy | ∙ βf,t ∙ βreg,t ∙ δyj IUtyj-Qyj(St, at)|
y=1
L
≤ βf,t ∙ Breg,t ∙ δmax X |%卜 |Utyj- Qy)(St, at)|
y=1
L
=βf,t ∙ Breg,t ∙ δmax ^^ |%「 |『(" + Y Q^ (St+1, f⅛+1) - Qy)(S t, at)
y=1
(23)
13
Published as a conference paper at ICLR 2022
The second line follows from Lemma 2.
IfTD error in the regular space is bounded, then ∣r(j) + Y Qj(st+ι,at+ι)- Qt(j)(st,at) ≤ K(j)
for some K(j) ≥ 0. Hence, Equation 23 induces:
L
|et| ≤ βf,t ∙ βreg,t ∙ δmax X A』'K(j)
j=1
=βf,t ∙ ∣βreg,t ∙ δmax ∙ K,	(24)
with K = pL=ι ∣λj | ∙ K(j) ≥ 0; thus, |et| is also bounded for all t. As (by assumption) βreg,t ∙ βf,t
converges to zero, we conclude that there exists T ≥ 0 such that for all t ≥ T we have
|et| ≤ ξ ∣∣rt + γ ma0xQt(st+1,a0) - Qt(st,at)∣∣ ,	(25)
for any given ξ ∈ (0, 1].
Hence, not only |et | goes to zero w.p. one as t → ∞, but also its magnitude always remains upper-
bounded below the size ofTD update with any arbitrary margin ξ. Since TD update already satisfies
assumptions 3 and4 of Lemma 1, we conclude that with the presence ofet those assumptions remain
satisfied, at least after reaching some time T where Equation 25 holds.
Finally, Lemma 2 also asserts that et is measurable given information at time t, as required by
Lemma 1. Invoking Lemma 1, we can now conclude that the iterative process defined by Algorithm 1
converges to Qt w.p. one.
Proof of Lemma 2
Part 1
Our proof partially builds upon the proof presented by van Seijen et al. (2019). To simplify the
notation, we drop j in fj , while we keep j in other places for clarity.
By definition we have Qet(j)(s, a) = f Qt(j)(s, a) . Hence, we rewrite Equations 3, 4, and 5 of
Algorithm 1 in terms of Qt(j):
Utj)= r(j) + γQ(j)(st+ι, at+ι),	(26)
Utj) = Qyj(St ,at)+ βreg,t (Utj)- Q^ (st,αt)),	(27)
f (Q(",αt)) = f "(st,ɑt)) + βf,t ("U.) -f(Q?)(St,at))).	(28)
The first two equations yield:
Utj)= Qj(St, at) + βreg,t (r(j) + YQtj)(St+1, &t+i) - Qj)(St,aj}	(29)
By applying f-1 to both sides of Equation 28, we get:
Q(+ι(st,at)= f-1 (f (Qj)(St,at)) + βf,t ("U?) - f(Q?)(st,at)))),	(30)
which can be rewritten as:
QIt+ι(St, at) = Qj)(St, at) + βf,t (Utj)- Qj)(St, at)) + etj),	(31)
where e(tj ) is the error due to averaging in the mapping space instead of in the regular space:
e(j) := f-1 (f (Qj)(St, at)) + βf,t (f(Uttj)) - f(Qtj)(St, at))))
-Qtj)(St, at) - βf,t (Utj)- Qtj)(St, at)). (32)
14
Published as a conference paper at ICLR 2022
Table 1: Ordering of v and w for various cases of f.
f	SEMI-CONVEX	SEMI-CONCAVE
MONOTONICALLY INCREASING	f(w) ≥ f(v) ; w ≥ v f(w) ≤ f(v) ; w ≤ v
MONOTONICALLY DECREASING f(w) ≥ f(v) ; v ≥ w f(w) ≤ f(v) ; v ≤ w
We next analyze the behavior of e(tj) under the Theorem 1’s assumptions. To simplify, let us intro-
duce the following substitutions:
a→Qt(j) (st, at)
b → Utej)
v →	(1 - βf,t) a + βf,t b
W → (I- βf,t)f (a) + βf,tf (b)
W → f T(W)
The error eetj ) can be written as
eej) = f-1((I- βf,t)f (a) + βf,tf (b)) -((I- βf,t)a + βf,tb)
=f T(W) - V
= W - v.
We remark that both v and W lie between a and b. Notably, etej) has a particular structure which we
can use to bound W - v . See Table 1 for the ordering of v and W for different possibilities of f.
We define three lines g0(x), g1(x), and g2(x) such that they all pass through the point (a, f (a)).
As for their slopes, g0 (x) has the derivative f0(a), and g2 (x) has the derivative f0(b). The function
g1(x) passes through point (b, f (b)) as well, giving it derivative (f (a) - f (b))/(a - b). See Figure 3
for all the possible cases. We can see that no matter if f is semi-convex or semi-concave and if
it is increasing or decreasing these three lines will sandwich f over the interval [a, b] if b ≥ a, or
similarly over [b, a] if a ≥ b. Additionally, it is easy to prove that for all x in the interval of a and b,
either of the following holds:
g0(x) ≥ f(x) ≥ g1(x) ≥ g2(x)	(33)
or
g0(x) ≤ f(x) ≤ g1(x) ≤ g2(x).	(34)
The first one is equivalent to
g0-1(y) ≤ f-1(y) ≤ g1-1(y) ≤ g2-1(y),	(35)
while the second one is equivalent to
g0-1(y) ≥ f-1(y) ≥ g1-1(y) ≥ g2-1(y).	(36)
From the definition of g1 it follow that in all the mentioned possibilities of f combined with either
of a ≥ b or b ≥ a,we always have gι(v) = W and g-1(W) = v. Hence, plugging W in Equation 35
and Equation 36 (and noting that f T(W) = W) deduces
g-1(W) ≤ W ≤ V ≤ g-1(W)	(37)
or
g-1(W) ≥ W ≥ V ≥ g-1(W).	(38)
Either way, regardless of various possibilities for f as well as a and b, we conclude that
|e(j)| = |v - W| ≤ |g-1(W)-g-1(W)|.	(39)
15
Published as a conference paper at ICLR 2022
Figure 3: The lines go, g1, and g? for all the different possibilities of fj. Cases A-D correspond to
a ≤ b, and cases E-H are the same functions but for b ≤ a. We use the notations w0 = g-1(W) and
V = g-1(滴).
16
Published as a conference paper at ICLR 2022
From definition of the lines g0 and g2 , we write the line equations as follows:
g0(x) - f(a) = f0 (a)(x - a),
g2 (x) - f (a) = f0 (b)(x - a).
Applying these equations on the points (g-1(W), W) and (g-1(W),W), respectively, it yields:
W - f(a) = f 0(a)(g-1(W) - a),
W - f(a) = f0(b)(g-1(W)- a),
which deduce
-1	W - f (a),
g0 (W) = fp+ a
Plugging the above in Equation 39, it follows:
(40)
(41)
|et(j) | = |v
,	-1 W - f (a) .
；	g2 (W) = —f 0(b)	+ a .
We next invoke the mean value theorem, which states that if f is a continuous function on the closed
interval [a, b] and differentiable on the open interval (a, b), then there exists a point c ∈ (a, b) such
that f(b) - f(a) = f0(c)(b - a). Remark that based on Assumption 2, c would satisfy fj0 (c) ≤ δ2(j),
also that f0(b) - f0(a) ≤
st, at
(42)
From Equation 27, it follows that
Utj)- Qj)(St, at) = βreg,t (Utj)- Qyj(St ,at)) ∙
We therefore can write
Ietj)I ≤ βf,t (甲-砂)δ2jj (U(j) -Qtj)(St,at))
=βf,t (J - ^jy) δ2j) ∙ βreg,t (jj - Qtj)(St,at))
=βf,t ∙ βreg,t∙ 6j)Ktj)- Q(j)(st,at)| ,
where δ(j ) = δ2(j)
the lemma.
—
is a positive constant. This completes the proof for the first part of
17
Published as a conference paper at ICLR 2022
Part 2
For this part, it can be directly seen from Figure 3 that for a given fj , the order of w0 , w, v, and v0 is
fixed, regardless of whether a ≥ b or b ≥ a (in Figure 3, compare each plot A, B, C, and D with their
counterparts at the bottom). Hence, the sign of e(tj ) = w - v will not change for a fixed mapping.
Part 3
Finally, we note that by its definition, et(j) comprises quantities that are all defined at time t. Hence,
it is fully measurable at time t, and this completes the proof.
B Discussion on Log versus LogLin
△z
SEməɑ:PeddEW
Figure 4: The functions Log and LogLin are illustrated. Remark the compression property of Log
as compared to LogLin for returns larger than one.
As discussed in Section 4.3 (Slope of Mappings), the logarithmic (Log) function suffers from a too-
low slope when the return is even moderately large. Figure 4 visualizes the impact more vividly. The
logarithmic-linear (LogLin) function lifts this disadvantage by switching to a linear (Lin) function
for such returns. For example, if the return changes by a unit of reward from 19 to 20, then the
change will be seen as 0.05 in the Log space (i.e. log(20) - log(19)) versus 1.0 in the LogLin
space; that is, Log compresses the change by 95% for a return of around 20. As, in general, learning
subtle changes is more difficult and requires more training iterations, in such scenarios normal DQN
(i.e. Lin function) is expected to outperform LogDQN. On the other hand, when the return is small
(such as in sparse reward tasks), LogDQN is expected to outperform DQN. Since LogLin exposes
the best of the two worlds of logarithmic and linear spaces (when the return lies in the respective
regions), we should expect it to work best if it is to be used as a generic mapping for various games.
18
Published as a conference paper at ICLR 2022
400% η
LogLin vs. Log
-200%
400% -
200% -
0% .
LogLin vs. Lin
-200%
Figure 5: Difference in human-normalized score for 55 Atari 2600 games, LogLinDQN versus
LogDQN (top) and (Lin)DQN (bottom). Positive % means LogLinDQN outperforms the respective
baseline.
C Experimental Details
The human-normalized scores reported in our Atari 2600 experiments are given by the formula
(similarly to van Hasselt et al. (2016)):
scoreagent — ScorerandOm
,
scorehuman - scorerandom
where scoreagent, scorehuman, and scorerandom are the per-game scores (undiscounted returns) for the
given agent, a reference human player, and random agent baseline. We use Table 2 from Wang et al.
(2016) to retrieve the human player and random agent scores. The relative human-normalized score
of LogLinDQN versus a baseline in each game is given by (similarly to Wang et al. (2016)):
ScoreLOgLinDQN - scorebaseline
max(scorebaseline , scorehuman ) - scorerandom
where scoreLogLinDQN and scorebaseline are computed by averaging over the last 10% of each learning
curve (i.e. last 20 iterations).
The reported results are based on three independent trials for LogLinDQN and LogDQN, and five
independent trials for DQN.
D Additional Results
Figure 5 shows the relative human-normalized score of LogLinDQN versus LogDQN (top panel)
and versus (Lin)DQN (bottom panel) for each game, across a suite of 55 Atari 2600 games.
LogLinDQN significantly outperforms both LogDQN and (Lin)DQN on several games, and is
otherwise on par with them (i.e. when LogLinDQN is outperformed by either of LogDQN or
(Lin)DQN, the difference is not by a large margin).
Figure 6 shows the raw (i.e. without human-normalization) learning curves across a suite of 55 Atari
2600 games.
Figures 7, 8, and 9 illustrate the change of reward density (measured for positive and negative
rewards separately) at three different training points (before training begins, after iteration 5, and
after iteration 49) across a suite of 55 Atari 2600 games.
19
Published as a conference paper at ICLR 2022

DQN
C51
Rainbow
LogDQN
LogLinDQN
Alien
Amidar
Assault
Asterix
Asteroids
“ ɪf-r*JJrnTGF-iFW∙"r*j
150000 -
100000
S 50000
Iteration
Iteration
Iteration
Iteration
IceHockey
Jamesbond
Kangaroo
Iteration
Iteration
Iteration
MontezumaReveiige
MsPacman
PrivateEye
Pong
-20
Robotank
Seaquest
ι:N
Iteration
StarGunner
Tennis
TimePilot
Tutankham
Venture
VideoPinbaU
WizardOfWor
Atlantis
20000 -
Bowling
Iteration
Iteration
KungFuMaster
10000 -
20000 -
Solaris
SpaceIiivaders
-100f!Q -
-30000 -
10000 -
Iteration
500000 -
25000 -
Iteration
Iteration
Iteration
Iteration
Iteration
50000
Iteration
Iteration
Iteration
Iteration
Iteration
Iteration
CrazyClimber
Iteration
Freeway
Iteration
20 ^ .AL
ɑ Z>z'-v
20 -I______
50
Iteration
50
Iteration
500On -
Iteration
BankHeist
BattleZone
BeamRider
40∞0 -I
SOOO -
Iteration
Iteration
Iteration
Iteration
Iteration
DemonAttack
Iteration
Frostbite
Iteration
Iteration
Iteration
Breakout
Centipede
ChopperConimand
Iteration
Iteration
Iteration
DoubleDunk
Iteration
Gopher
NameThisGame
Iteration
Iteration
Skiing
Iteration
Iteration
50000 -
Endnro
Iteration
Gravitar
Knill
Iteration
Phoenix
Iteration
Riverraid
IteratiOD
Iteration
Iteration
YarsRevenge
FishingDerby
Iteration
Iteration
RoadRunner
Iteration
Iteration
UpNDown


工⅛⅛e*"F
Pitfall
Figure 6:	Learning curves in all 55 Atari 2600 games for (Lin)DQN, C51, Rainbow, LogDQN,
and LogLinDQN. Shaded regions indicate standard deviation.
20
Published as a conference paper at ICLR 2022
Alien	Amidar
UOle」©-
0.25 -
0.00 -	~τ
0.25 -
0.00 -	∙∣
0.25 J M
。.。。世斗d
0	50	100
Assault
0.2 -
0.0 -∣Γ⅝nn∣ I ⅛
0.2 -I
o.o⅛φ^
0.2 -
OoaJa
O 50 IOO
Astenx
Uole-Ia-
Uoqe-Ia-uo=e-3-
Breakout
Centipede
0.5 -|
o.l -
lo⅛¾ar1⅛⅛ι∣-.
O.。
0.1 -I
n jl⅛⅛⅛⅛‰,ι
0	50	100
uole」8-
CrazyCIimber
0.2 -∣
0.0
0.2
0.0
0.2
σ⅛
0.0
0	50	100
DemonAttack
0.2 -
0.0 -I K*1.1.... I
0.2 1
o.o -IpV■"惴⅝∣
0.2 1
o.o J∣ffS⅛f∙τr*∙⅞tr
0	50	100
DoubIeDunk
o.2 -
o.o - 标,,”怵&怖，
-	0.2 J1--1----r
0.2 -
o.o
-	0.2 J1--1——r
0.2 -
o.o -再■媾“梆M
-	0-2 ----1----r
0	50	100
0	50	100
Freeway
0.1 -
O
°-0 -∣l ■ ■
0.1 -
In(J小 WT
0.1 -I
liJlll⅛llM∣i
0.0 -IJMMeMI]
0	50	100
pos Positive
Frostbite
1 -∣
0 ⅜
1 -∣
。—∣
0	50	100
Negative
Gopher
0.25 -
ooo-l1.............
0.25 -
0.00 L
0.25 -
0.00 l∣
0	50	100
Steps (xlθ)

Figure 7:	(Part 1/3) Reward density in the Atari suite. The rewards of each game are measured with
three behavioral policies: (i) fully random (iteration 0: the first training iteration), (ii) an ε-greedy
policy using ε = 0.1, with state-action values estimated by a (Lin)DQN agent after completing 5
iterations of training, and (iii) similar to (ii) but after 49 training iterations. All experiments are
averaged over 20 independent trials (shaded areas depict standard deviation) and for 1000 steps with
a moving average window of 10 steps.
21
Published as a conference paper at ICLR 2022
IceHockey
I Γ
■聊牺蝌M
0	50	100
Kangaroo
0.1 -I
o.o -4
KungFuMaster
0.25 -
0.00 - ,*∙
0.25 -
0.00」| 「	1.....I
0.25 -
0.00」i A+ftyι⅝w⅜
0	50	100
UOAe-Ia-
MontezumaRevenge
0.01 -
0.00---------------
-	0.01 -I1----1-----Γ
0.01 -
0.00---------------
-	0.01 -I1----1-----Γ
0.01 -1
0.00---------------
-	0.01 -ɪɪ---1------r
0	50	100
MsPacman
-1 ^li---1----r
0 1---，口 ”何惭
-1 ^li---1----r
0 -1——,一 I，，
-1 ^li-------1---------r
0	50	100
Pitfall
0 -I-，rr-n
0.0」|・毗・中，、
0.2 -
CC LL如3⅛W a
O.o -111 -If 1 -∣rp r-m ∏i
0.2 -
■ I .
0.0 -IIM-一明
0	50	100
Steps (XlO)
0.25 -
0.00-lh*Vwri
0	50	100
RoadRunner
0.5 -
0-0 A-----1-----r
0.5 -
0.0 -Il---1-----r
0.5 -
o.o ll⅜⅜**y⅛⅛r
0	50	100
Skiing	Solaris
TimePiIot
0.2 -I
Tutankham
0.25 -
0.00 -1--β≡7-,m ,"∣
0.25 -
0.00 I— ¥一■、
0.25 -
o.oo φ..
0	50	100
UpNDown
Figure 8:	(Part 2/3) Reward density in the Atari suite. The rewards of each game are measured with
three behavioral policies: (i) fully random (iteration 0: the first training iteration), (ii) an ε-greedy
policy using ε = 0.1, with state-action values estimated by a (Lin)DQN agent after completing 5
iterations of training, and (iii) similar to (ii) but after 49 training iterations. All experiments are
averaged over 20 independent trials (shaded areas depict standard deviation) and for 1000 steps with
a moving average window of 10 steps.
22
Published as a conference paper at ICLR 2022
S 6寸
UO一芭 ©-
Venture
o.oi -∣
o.oo----------------
-	0∙01 -ɪɪ-----ɪ------r
0.01 -∣
0.00----------------
-	0.01 4-------1------Γ
0.01 -
0.00----------------
-	O.oi -I1---1--------Γ
O	50	IOO
VideoPinbaII
0.25 -
0.00 IL
0.25 -
0.00 - WtUiA
0.25 -
-wκ***⅛√M*
0.0。IL [-------r
0	50	100
Steps (xlθ)
Y⅛rsRevenge
0.5 -
0.0
0.5 -
0.0 Mjl--------r
0.5 -
o.o⅛∆sr_r
0	50	100
posit Positive
Negative
Figure 9:	(Part 3/3) Reward density in the Atari suite. The rewards of each game are measured with
three behavioral policies: (i) fully random (iteration 0: the first training iteration), (ii) an ε-greedy
policy using ε = 0.1, with state-action values estimated by a (Lin)DQN agent after completing 5
iterations of training, and (iii) similar to (ii) but after 49 training iterations. All experiments are
averaged over 20 independent trials (shaded areas depict standard deviation) and for 1000 steps with
a moving average window of 10 steps.
23