Published as a conference paper at ICLR 2022
Graphon based Clustering and Testing of Net-
works: Algorithms and Theory
Mahalakshmi Sabanayagam
Technical University of Munich
maha.sabanayagam@tum.de
Leena C. Vankadara
University of Tubingen
leena.chennuru-vankadara@uni-tuebingen.de
Debarghya Ghoshdastidar
Technical University of Munich
Munich Data Science Institute
ghoshdas@in.tum.de
Ab stract
Network-valued data are encountered in a wide range of applications, and pose
challenges in learning due to their complex structure and absence of vertex corre-
spondence. Typical examples of such problems include classification or grouping
of protein structures and social networks. Various methods, ranging from graph
kernels to graph neural networks, have been proposed that achieve some success
in graph classification problems. However, most methods have limited theoreti-
cal justification, and their applicability beyond classification remains unexplored.
In this work, we propose methods for clustering multiple graphs, without vertex
correspondence, that are inspired by the recent literature on estimating graphons—
symmetric functions corresponding to infinite vertex limit of graphs. We propose
a novel graph distance based on sorting-and-smoothing graphon estimators. Using
the proposed graph distance, we present two clustering algorithms and show that
they achieve state-of-the-art results. We prove the statistical consistency of both
algorithms under Lipschitz assumptions on the graph degrees. We further study
the applicability of the proposed distance for graph two-sample testing problems.
1	Introduction
Machine learning on graphs has evolved considerably over the past two decades. The traditional
view towards network analysis is limited to modelling interactions among entities of interest, for in-
stance social networks or world wide web, and learning algorithms based on graph theory have been
commonly used to solve these problems (Von Luxburg, 2007; Yan et al., 2006). However, recent ap-
plications in bioinformatics and other disciplines require a different perspective, where the networks
are the quantities of interest. For instance, it is of practical interest to classify protein structures
as enzyme or non-enzyme (Dobson & Doig, 2003) or detect topological changes in brain networks
caused by Alzheimer’s disease (Stam et al., 2007). In this paper, learning from network-valued data
refers to clustering where each network is treated as an entity, as opposed to the traditional network
analysis problems that involve a single network of interactions (Newman, 2003).
Machine learning on network-valued data has been an active area of research in recent years, al-
though most works focus on the network classification problem. The generic approach is to convert
the network-valued data into a standard representation. Graph neural networks are commonly used
for network embedding, that is, finding Euclidean representations of each network that can be fur-
ther used in standard machine learning models (Narayanan et al., 2017; Xu et al., 2019). In contrast,
graph kernels capture similarities between pairs of networks that can be used in kernel based learning
algorithms (Shervashidze et al., 2011; Kondor & Pan, 2016; Togninalli et al., 2019). In particular,
the graph neural tangent kernel defines a graph kernel that corresponds to infinitely wide graph neu-
ral networks, and typically outperforms neural networks in classification tasks (Du et al., 2019). A
more classical equivalent for graph kernels is to define metrics that characterise the distances be-
1
Published as a conference paper at ICLR 2022
tween pairs of graphs (Bunke & Shearer, 1998), but there has been limited research on designing
efficient graph distances and developing algorithms for clustering network-valued data.
The motivation for this paper stems from two shortcomings in the literature on network-valued data
analysis: first, the efficacy of existing kernels or embeddings have not been studied beyond net-
work classification, and second is the lack of theoretical analysis of these methods, particularly in
the small sample setting. Generalisation error bounds for graph kernel based learning exist (Du
et al., 2019), but these bounds, based on learning theory, are meaningful only when many networks
are available. However, in many applications, one needs to learn from a small population of large
networks and, in such cases, an informative statistical analysis should consider the small sample,
large graph regime. To address this issue, we take inspiration from the recent statistics literature
on graph two-sample testing—given two (populations of) large graphs, the goal is to decide if they
are from same statistical model or not. Although most theoretical studies in graph two-sample test-
ing focus on graph with vertex correspondence (Tang et al., 2017a; Ghoshdastidar & von Luxburg,
2018), some works address the problem of testing graphs on different vertex sets either by defining
distances between graphs (Tang et al., 2017b; Agterberg et al., 2020) or by representing networks
in terms of pre-specified network statistics (Ghoshdastidar et al., 2017). The use of network statis-
tics for clustering network-valued data is studied in Mukherjee et al. (2017). Another fundamental
approach for dealing with graphs of different sizes is graph matching, where the objective is to
determine the vertex correspondence. Graph matching is often solved by formulating it as an opti-
mization problem (Zaslavskiy et al., 2008; Guo et al., 2019) or defining graph edit distance between
the graphs (Riesen & Bunke, 2009; Gao et al., 2010). Although there is extensive research on graph
matching, the efficacy of these methods in clustering network-valued data remains unexplored.
Contribution and organisation. In this work, we follow the approach of defining meaningful graph
distances based on statistical models, and use the proposed graph distance in the context of learning
from networks without vertex correspondence. In particular, we propose graph distances based on
graphons. Graphons are symmetric bivariate functions that represent the limiting structure for a
sequence of graphs with increasing number of nodes (Lovasz & Szegedy, 2006), but can be also
viewed as a nonparametric statistical model for exchangeable random graphs (Diaconis & Janson,
2007; Bickel & Chen, 2009). The latter perspective is useful for the purpose of machine learning
since it allows us to view the multiple graphs as random samples drawn from one or more graphon
models. This perspective forms the basis of our contributions, which are listed below:
1)	In Section 2, we propose a distance between two networks, that do not have vertex correspondence
and could have different number of vertices. We view the networks as random samples from (un-
known) graphons, and propose a graph distance that estimates the L2 -distance between the graphons.
The distance is inspired by the sorting-and-smoothing graphon estimator (Chan & Airoldi, 2014).
2)	In Section 3, we present two algorithms for clustering network-valued data based on the proposed
graph distance: a distance-based spectral clustering algorithm, and a similarity based semi-definite
programming (SDP) approach. We derive performance guarantees for both algorithms under the
assumption that the networks are sampled from graphons satisfying certain smoothness conditions.
3)	We empirically compare the performance of our algorithms with other clustering strategies based
on graph kernels, graph matching, network statistics etc. and show that, on both simulated and
real data, our graph distance-based spectral clustering algorithm outperforms others while the SDP
approach also shows reasonable performance, and they also scale to large networks (Section 3.3).
4)	Inspired by the success of the proposed graph distance in clustering, we use the distance for graph
two-sample testing. In Section 4, we show that the proposed two-sample test is statistically consis-
tent for large graphs, and also demonstrate the efficacy of the test through numerical simulation.
We provide further discussion in Section 5 and present the proofs of theoretical results in Appendix.
2	Graph Distance based on Graphons
Clustering or testing of multiple networks requires a notion of distance between the networks. In
this section, we present a transformation that converts graphs of different sizes into a fixed size
representation, and subsequently, propose a graph distance inspired by the theory of graphons. We
first provide some background on graphons and graphon estimation. Graphon has been studied in
2
Published as a conference paper at ICLR 2022
the literature from two perspectives: as limiting structure for infinite sequence of growing graphs
(Lovasz & Szegedy, 2006), or as exchangeable random graph model. In this paper, We follow the
latter perspective. A random graph is said to be exchangeable if its distribution is invariant under
permutation of nodes. Diaconis & Janson (2007) showed that any statistical model that generates
exchangeable random graphs can be characterised by graphons, as introduced by Lovasz & Szegedy
(2006). Formally, a graphon is a symmetric measurable continuous function w : [0, 1]2 → [0, 1]
where w(x, y) can be interpreted as the link probability between two nodes of the graph that are as-
signed values x and y, respectively. This interpretation propounds the following two stage sampling
procedure for graphons. To sample a random graph G with n nodes from a graphon w, in the first
stage, one samples n variables U1, . . . , Un uniformly from [0, 1] and constructs a latent mapping be-
tween the sampled points and the node labels. In the second stage, edges between any two nodes i, j
are randomly added based on the link probability w(Ui , Uj ). Mathematically, if we abuse notation
to denote the adjacency matrix by G ∈ {0, 1}n×n, we have
U1,...,Un 蚓 Uniform [0,1] and	Gij∖Ui,Uj 〜Bernoulli (W(Ui,Uj)) for all i < j.
We consider problems involving multiple networks sampled independently from the same (or dif-
ferent) graphons. We make the following smoothness assumptions on the graphons.
Assumption 1 (Lipschitz continuous) A graphon w is Lipschitz continuous with constant L if
|w(u, v) - w(u0, v0)| ≤ L (u - u0)2 + (v - v0)2	for every u, v, u0, v0 ∈ [0, 1].
Assumption 2 (Two-sided Lipschitz degree) A graphon w has two-sided Lipschitz degree with
constants λ1, λ2 > 0 if its expected degree function g, defined by g(u) = R01 w(u, v)dv, satisfies
λ2 |u - u0| ≤ |g(u) - g(u0)| ≤ λ1 |u - u0| for every u, u0 ∈ [0, 1].
One of the challenges in graphon estimation is due to the issue of non-identifiability, that is, differ-
ent graphon functions w can generate the same random graph model. In particular, two graphons w
and w0 generate the same random graph model if they are weakly isomorphic—there exist two mea-
sure preserving transformations φ, φ0 : [0, 1] → [0, 1] such that w(φ(u), φ(v)) = w0(φ0(u), φ0(v)).
Moreover, the converse also holds meaning that such transformations are known to be the only
source of non-identifiability (Diaconis & Janson, 2007). This weak isomorphism induces equiva-
lence classes on the space of graphons. Since our goal is only to cluster graphs belonging to random
graph models, we simply make the following assumption on our graphons.
Assumption 3 (Equivalence classes) Any reference to K graphons, w1 , . . . , wK, assumes that, for
every i, j, either wi = wj or wi and wj belong to different equivalence classes. Furthermore, with-
out loss of generality, we assume that every graphon wi is represented such that the corresponding
degree function gi is non-decreasing.
Remark on the necessity of Assumptions 1-3. Assumption 1 is standard in graphon estimation
literature (Klopp et al., 2017) since it avoids graphons corresponding to inhomogeneous random
graph models. It is known that two graphs from widely separated inhomogeneous models (in L2 -
distance) are statistically indistinguishable (Ghoshdastidar et al., 2020), and hence, it is essential to
ignore such models to derive meaningful guarantees. Assumption 2 ensures that, under a measure-
preserving transformation, the graphon has strictly increasing degree function, which is a canonical
representation of an equivalence class of graphons (Bickel & Chen, 2009). Assumption 3 is needed
since graphons can only be estimated up to measure-preserving transformation. As noted above, it
is inconsequential for all practical purposes but simplifies the theoretical exposition.
Graph transformation. In order to deal with multiple graphs and measure distances among pairs
of graphs, we require a transformation that maps all graphs into a common metric space—the space
of all n0 × n0 symmetric matrices for some integer n0 . While the graphon estimation literature
provides several consistent estimators (Klopp et al., 2017; Zhang et al., 2017), only the histogram
based sorting-and-smoothing graphon estimator of Chan & Airoldi (2014) can be adapted to meet
the above requirement. We use the following graph transformation, inspired by Chan & Airoldi
(2014). The adjacency matrix G of size n × n is first reordered based on a non-unique permutation
σ, such that the empirical degree based on this permutation is monotonically increasing. The degree
3
Published as a conference paper at ICLR 2022
sorted adjacency matrix is denoted by Gσ. It is then transformed to a ‘histogram’ A ∈ Rn0×n0 as
hh
1n
Aij = h2∑∑G(i-i)h+i1,C7-i)h+j1，where h = 一 and LC is the floor function. (1)
h	n0
i1 =1 j1 =1
Proposed graph distance. Given two directed or undirected graphs G1 and G2 with n1 and n2
nodes, respectively, we apply the transformation (1) to both the graphs with n0 ≤ min{n1,n2}. We
propose to use the graph distance
d(G1, G2) = ~IlAI- A2kF,	⑵
n0
where Ai and A2 denote the transformed matrices and k ∙∣∣f denotes the matrix FrobeniUs norm.
Proposition 1 shows that, if G1 and G2 are sampled from two graphons, then the graph dis-
tance (2) consistently estimates the L2-distance between the two graphons, which is defined as
Iw1 - w2I2L2 = R01 R01 (w1(x, y) - w2(x, y))2 dx dy.
Proposition 1 (Graph distance is consistent) Let wi and w2 satisfy Assumptions 1-3. Let Gi and
G2 be random graphs with at least n nodes sampled from the graphons w1 and w2, respectively. If
n → ∞ and n° is chosen such that n0 )g n → 0, then with high probability (w.h.p.),
lk wi - w2 ∣∣L2 - d(G1, G2)∣ = O (n10).	⑶
Proof sketch. We define a novel technique for approximating the graphon. The proof in Ap-
pendix A.1 first establishes that the approximation error is bounded using Assumption 1. Conse-
quently, a relation between approximated graphons and transformed graphs is derived using lemmas
from Chan & Airoldi (2014). Proposition 1 is subsequently proved using the above two results.
Remark on Proposition 1 for sparse graphs. The defined sampling procedure for graphon gen-
erates dense graphs, deviating from the real world sparse networks. To adapt it to sparse graphs,
one may modify the sampling procedure to Gij |Ui, Uj 〜Bernoulli(ρw(Ui, Uj)) where P depends
only on n (Olhede & Wolfe, 2014). Under this process, the consistency result in Proposition 1 re-
mains unchanged for P = Ω(dlog n/n) (proof in Appendix A.2). This bound cannot be improved
to the expected real world measure where P = Ω(log n/n), because of the degree sorting step in (1).
Nevertheless, our analysis allows for relatively sparser graphs with strong consistency result.
Notation. For ease of exposition, Proposition 1 as well as main results are stated asymptotically
using the standard O(∙) and Ω(∙) notations, which subsume absolute and Lipschitz constants. We
use “with high probability” (w.h.p.) to state that the probability of an event converges to 1 as n → ∞. 3
3 Graph Clustering
We now present the first application of the proposed graph distance (2) in the context of clustering
network-valued data. We are particularly interested in the setting where one needs to cluster a small
population of large graphs, that is, minimum graph size n grows faster than the sample size m. This
scenario is relevant in practice as bioinformatics or neuroscience application often deals with very
few graphs (see real datasets in Section 3.3). Theoretically, this perspective complements guarantees
for (graph) kernels that are applicable only in supervised setting and large sample regime, m → ∞.
In contrast, our guarantees are more conclusive for bounded m and large graph size, n → ∞.
3.1 Distance Based Spectral Clustering (DSC)
Given m graphs with adjacency matrices Gi , ..., Gm, we propose a distance based clustering al-
gorithm where we apply spectral clustering to an estimated distance matrix. The distance matrix
D ∈ Rm×m is computed on all pairs of graphs using the defined estimator function (2), that is
Dbij = d(Gi, Gj). Unlike the standard Laplacian based spectral clustering, which is applicable
for adjacency or similarity matrices, we use the method suggested by Mukherjee et al. (2017) that
4
Published as a conference paper at ICLR 2022
computes the K leading eigenvectors of Db (corresponding to the K smallest eigenvalues in magni-
tude) and applies k-means clustering to the rows of the eigenvector matrix resulting in K number
of clusters. We refer to this distance based clustering algorithm as DSC, described in Algorithm 1
of Appendix. To derive the statistical consistency of DSC, we consider the problem of clustering m
random graphs of potentially different sizes, each sampled from one ofK graphons. We establish the
consistency in Theorem 1 by proving that the number of misclustered graphs → 0 asymptotically.
Theorem 1 (Consistency of DSC) Consider K graphons satisfying Assumptions 1-3, and m ran-
dom graphs G1 , . . . , Gm, each sampled from one of the K graphons (assume there is at least one
graph from each graphon). Define the distance matrix D ∈ Rm×m such that Dij = kwi - wj kL2
where wi and wj are the graphons from which Gi and Gj are generated. Let n be the size of the
smallest graph, and γ be the K-th smallest eigenvalue value of D in magnitude. As n → ∞, if n0 is
m2n2 log n	3
ChoSen such that----n―^-----→ 0, then DSC misclusters at most O ( γm2 ) graphs w.h.p.
Proof sketch. The proof, given in Appendix A.3, uses the Davis-Kahan spectral perturbation theorem
to bound the error in terms of kDb - DkF, which is further bounded using Proposition 1.
While the number of misclustered graphs seem to depend on m3 , we note that there is an inverse
dependence on γ2 which has dependence on m (see Corollary 1 that illustrates it for a specific case).
Moreover, our focus is on the setting where m = O(1) and n, n0 → ∞, in which case, the error
asymptotically vanishes. It is natural to wonder whether the dependence on m and n0 is tight in
the above bounds. Currently, we do not know the optimal rates, but deriving this would be difficult
due to the strong dependency of entries in Db and slow rate of convergence of the graph distance
in Proposition 1. The presence of γ in the above clustering error bound makes Theorem 1 less
interpretable. Hence, we also consider the specific case of K = 2 (two graphons) in the following
result, along with the assumption that equal number of graphs are generated from both graphons.
Corollary 1 Let w 6= w0 be two graphons satisfying Assumptions 1-3, and m is a bounded even
number. Assume that equal number of graphs are generated from w and w0. For any n0 and large
m2n2 log n
enough constant C such that kw 一 w0∣∣L2 ≥ Cm and---------Qn------→ 0 as n → ∞, the number of
graphs misclustered by Algorithm 1 goes to zero w.h.p.
The corollary implies that given the observed graphs are large enough, and if the choice of no
is relatively small, no《 ,n/ logn, and the graphons are Ω(n1) apart in L2-distance, then the
clustering is consistent. Intuitively, it can be understood that ifwe condense large graphs to a small
representation (small no), then the clusters can be identified only if the models are quite dissimilar.
3.2	Similarity Based Semi-Definite Programming (SSDP)
We propose another algorithm for clustering m graphs based on similarity between pairs of graphs.
The pairwise similarity matrix S ∈ Rm×m is computed by applying Gaussian kernel on the distance
between the graphs, that is Sij = exp (— d(Gi；Gj)), where σ1,...,σn are parameters. For theo-
retical analysis, we assume σ1 = . . . = σn is fixed, but in experiments, the parameters are chosen
adaptively. We use the following semi-definite program (SDP) (Yan et al., 2018; Perrot et al., 2020)
to find membership of the observed graphs. Let X ∈ Rm×m be the normalised clustering matrix,
that is Xij = 1/|C| if i andj belong to cluster C, and 0 otherwise. Then, SDP for estimating X is:
,^__ __ __ __. _____________________________________ —
max trace(SbX)	s.t. X ≥ 0, X 0, X1 = 1, trace(X) = K,	(4)
X
where X ≥ 0, X 0 ensure that X is a non-negative, positive semi-definite matrix, and 1 denotes
the vector of all ones. We denote the optimal X from the SDP as
X . Once we have X , we apply
standard spectral clustering on X to obtain a clustering of the graphs. We refer to this algorithm as
SSDP, described in Algorithm 2 of Appendix. We present strong consistency result for SSDP below.
Theorem 2 (Consistency of SSDP) Consider K graphons, w1, . . . , wK, satisfying Assumptions 1-
3, and m random graphs, each sampled from one of the K graphons. Let n be the size of the smallest
graph. As n → ∞, if no is chosen such that m nnlog n → 0 and min ∣∣wι — w" ∣∣l2 = Ω (nm), then
the number of graphs misclustered by SSDP is zero w.h.p.
5
Published as a conference paper at ICLR 2022
Proof sketch. The proof in Appendix A.4 adapts Perrot et al. (2020, Proposition 1) to the present
setting and combines it with Proposition 1 to derive the stated condition for zero error.
Theorem 2 is slightly stronger than Theorem 1, or Corollary 1, since SSDP achieve a zero clustering
error for large enough graphs. This theoretical merit of SDP over spectral clustering is known in the
statistics literature. Similar to Corollary 1, the choice of n0 is important such that it does not violate
the minimum L2-distance condition in the theorem to ensure consistency.
Remark on the knowledge of K. Above discussions assume that the number of clusters K is
known, which is not necessarily the case in practice. To tackle this issue, one can estimate K using
Elbow method (Thorndike, 1953) or approach from Perrot et al. (2020),and then use it as input in
our algorithms, DSC and SSDP. One can modify the SDP (4) and Theorem 2 to the case where K
is adaptively estimated. However, we found the corresponding algorithm, adapted from Perrot et al.
(2020), to be empirically unstable in the present context. Hence, the knowledge of K is assumed
in the following experiments, which also allows the efficacy of the proposed algorithms and graph
distance to be evaluated without the error induced by incorrect estimation of K.
3.3	Experimental Analysis
In this section, we evaluate the performance of our algorithms DSC and SSDP, both in terms of
accuracy and computational efficacy. We measure the performance of the algorithms in terms of
error rate, that is, the fraction of misclustered graphs by using the source of the graphs as labels.
Since clustering provides labels up to permutation, we use the Hungarian method (Kuhn, 1955) to
match the labels. The performance can also be measured in terms of Adjusted Rand Index (results
in Appendix C.3). We use simulated and real datasets for evaluation and obtain all our experimental
results using Tesla K80 GPU instance with 12GB memory from Google. Code available in Github.
Simulated data. We generate graphs of varied sizes from four graphons, W1 (u, v) = uv,
W2(u,v) = exp { — max(u,v)0.75}, W3(u,v) = exp {—0.5 * (min(u, v) + u0.5 + v0∙5)} and
W4(u, v) = |u - v|. The simulated graphs are dense and the graph sizes are controlled to study
how algorithms scale. Their corresponding L2 distances between pairs of graphons is shown later
in Figure 3 and the heatmap of the graphons are visualised in Figure 4 in Appendix.
Real data. We use datasets from two contrasting domains: small molecule datasets from Bioin-
formatics and large network datasets from Social Networks. We use Proteins (Borgwardt et al.,
2005), KKI (Pan et al., 2016), OHSU (Pan et al., 2016) and Peking」(Pan et al., 2016) datasets from
Bioinformatics, and Facebookd (Oettershagen et al., 2020), GithubStargazers (Rozemberczki
et al., 2020), Deezer-Ego-Nets (Rozemberczki et al., 2020) and Reddit-Binary (Yanardag & Vish-
wanathan, 2015) datasets from Social Networks. We sub-sample a few graphs from each dataset by
setting a minimum number of nodes to validate clustering small number of large graphs (small m,
large n). The number (#graphs) and size (#nodes) of the graphs are listed in Figure 1 tables. We
consider all combinations of the datasets for three and four clusters in both the domains separately.
Choice of no and σi. As noted in our algorithms DSC and SSDP, no is an input parameter. Theo-
rems 1 and 2 show the choice of no = O (ʌ/n/log n). Hence, We set no = ʌ/n/ log n where n is the
minimum number of nodes. In Appendix C.2, we use simulated data to show that the above choice
of no is reasonable (if not the best) for both DSC and SSDP. Furthermore, the similarity matrix S
in SSDP is computed using parameters σi and we set σi = d(Gi, G5nn) where G5nn is the fifth
nearest neighbour of Gi. Hence, apart from knowledge of K, our algorithms are parameter-free.
Performance comparison with existing methods. We compare our algorithms with a range of
approaches for measuring similarity or distance among multiple networks. Most methods discussed
below provide a kernel or distance matrix to which we apply spectral clustering to obtain the clusters:
1)	Network Clustering based on Log-Moments (NCLM) is an embedding based clustering strategy
for different sized graphs (Mukherjee et al., 2017) that is based on network statistics called log
moments. Log moments for a graph with adjacency matrix A and number of nodes n is obtained by
(log (m1(A)) , log (m2(A)) , . . . , log (mJ (A))) where mi(A) = trace(A/n)i and J is a parameter. 2 * *
2) Wasserstein Weisfeiler-Lehman Graph Kernels (WWLGK) is a recent graph kernel that is based
on the Wasserstein distance between the node feature vector distributions of two graphs proposed
by Togninalli et al. (2019).
6
Published as a conference paper at ICLR 2022
0.6 -
⅛
S 0.4 -
81,82,83	81,83,84	81,82,84	82,83,84	81,82,83,84
DSC (ours)	WWLGK
SSDP (ours) - GNTK
NCLM	NCGMM
NCMMD TLE Time Limit Exceeded
Bioinformatics #graphs # nodes
Si Proteins	7	[273,620]
B2	KKI	7	[62. 90]
B3	OHSU	9	[140,	171]
84	Peking l	7	[86, 134]
七 0.2 -
0.0 —
Sl > Si, Sn	SLSAS3	51, $3, $4	$2, $3, $4	$1，$2, $3, $4
Social Networks #graphs #nodes
Si Facebook-Ctl	10	[100, 100]
Sz GithUb_Stargazers 8	[951,957]
S3 DeeZe^Ego_Nets	10	[208,363]
S4 ReddiLBinary 12	[3219,3782]
Figure 1: Evaluation of DSC and SSDP with other methods. (row 1) Results on simulated data.
(rows 2 and 3) Results on real data from Bioinformatics and Social Networks, respectively. DSC
outperforms in majority of the cases. Tables in rows 2 and 3 show details of the considered datasets.
3)	Graph Neural Tangent Kernel (GNTK) is another graph kernel that describes infinitely wide graph
neural networks derived by Du et al. (2019). Both WWLGK and GNTK provide state-of-the-art
performance in graph classification with GNTK outperforming most graph neural networks.
4)	Network Clustering algorithm based on Maximum Mean Discrepancy (NCMMD) considers a
graph metric (MMD) to cluster the graphs. MMD distance between random graphs is proposed
as an efficient test statistic for random dot product graphs (Agterberg et al., 2020). We compute
MMD between the graphs that are represented by latent finite dimensional embedding called spectral
adjacency embedding with the dimension r as a parameter.
5)	In Network Clustering algorithm based on Graph Matching Metric (NCGMM), we match two
graphs of different sizes by appending null nodes to the small graph (Guo et al., 2019) and compute
Frobenius norm between the matched graphs as their distance. Although the graph metrics (MMD
and graph matching) are for different purposes, we evaluate their efficacy in the context of clustering.
The parameters in the algorithms are n0 and σi in DSC and SSDP, J in NCLM, number of iterations
(#itr) to perform in WWLGK, number of layers (#layer) in graph neural networks for GNTK, r
in NCMMD and none in NCGMM. We fix n0 in our algorithms using the theoretical bound and σi
is set adaptively as discussed, whereas use grid search to tune the parameters for other algorithms.
Evaluation on simulated data. We sample 10 graphs of varied sizes between 50 and 100 nodes
from each of the four graphons in Figure 4, and perform the experiments by considering all com-
binations of three and four clusters of the graphons. Based on the theoretical bound, n0 is fixed
to 5 as n = 50. We report the performance for J = 8, r = 3, #itr = 1 and #layer = 2 as
these produce the best results. The first row of Figure 1 shows the average performance of the al-
gorithms computed over 5 independent runs. We observe that our algorithm DSC outperforms all
the other algorithms, achieving nearly zero error in all cases, and SSDP also performs competitively
by standing second or third best. The graph kernels, WWLGK and GNTK, and the graph metric
based method NCGMM typically do not perform well. NCMMD either performs very well or quite
poorly. We sample small graphs since otherwise GNTK cannot run due to memory requirement for
dense large graphs and NCGMM has high computation time. Appendix C.4 includes evaluation of
the algorithms except GNTK and NCGMM on larger graphs, where we observe similar behaviour.
Evaluation on real data. We consider all combinations of three and four clusters of both Bioinfor-
matics and Social Networks separately. The second and third rows of Figure 1 show the performance
with n0 = 30, J = 8, r = 3, #itr = 1 and #layer = 2, and the upper limit of 7200 seconds (2
hours) as running time of algorithms. We observe DSC outperforms other algorithms by a large mar-
gin in majority of the combinations, while in the other combinations like {Proteins,KKI,Peking_ 1},
DSC performs well with a very small margin to the best performing one. Although NCLM and
GNTK compare favorably in Social Networks datasets, they typically have high error rate in Bioin-
formatics or simulated datasets, suggesting that they could be well suited for large networks, whereas
7
Published as a conference paper at ICLR 2022
DSC is more versatile and suitable for all networks. SSDP performs moderately on real data, but it
achieves the smallest error in some cases, implying that it is suited for certain types of networks.
Computation time comparison. Figure 2 shows the time (measured in seconds) taken by each al-
gorithm for four clusters case, plotted in log scale. Appendix C.5 illustrates similar behavior in three
clusters case as well. Our algorithms, DSC and SSDP, perform competitively with respect to time as
well. In addition, it scales effectively for large graphs unlike other algorithms. It is worth noting that
although NCLM takes lesser time than DSC and SSDP for small
graphs, it takes longer for large social networks datasets, thus
favoring our methods in terms of both accuracy and scalability.
Graph matching based algorithm, NCGMM, has severe scalabil-
ity issue showing the inapplicability of such methods to learning
problems. We also evaluate the scalability of all algorithms by
measuring the time for clustering different sets of varied sized
graphs from graphons W1 , W2 , W3 and W4. Detailed discussion
on high scalability of DSC and SSDP is given in Appendix C.6.
Figure 2: Computation time
4 Graph Two-Sample Testing
Inspired by the remarkable performance of our graph distance (2) in clustering, we analyse its appli-
cability in graph two-sample testing. Two-sample testing is usually studied in the large sample case
m → ∞, and several nonparametric tests are known that could also be applied to graphs. However,
it is also relevant to study the small sample setting in graphs, particularly m = 2, that is, whether
two large graphs are statistically identical or not (Ghoshdastidar et al., 2020; Agterberg et al., 2020).
We consider the following formulation of the graph two-sample problem, stated under the assump-
tion that the graphs are sampled from graphons. Given two random graphs, G1 sampled from some
model (here, graphon w1), and G2 from another model w2, the goal is to determine which of the
following hypothesis is true:	H0	:	{w1	= w2 } or Ha	: w1	6=	w2	: kw1 -	w2 kL	≥ φ for some
φ > 0. Existing works consider alternative random graph model, such as inhomogeneous ErdOs-
Renyi models or random dot product graph models, which are more restrictive. The condition φ > 0
is necessary if one only has access to finitely many independent samples (Ghoshdastidar et al., 2020).
A two-sample test T is a binary function of the given samples such that T = 1 denotes that the test
rejects the null hypothesis H0 and T = 0 implies that the test rejects the alternate hypothesis Ha .
The goodness of a two-sample test is measured in terms of the Type-I and Type-II errors, which
denote the probabilities of incorrectly rejecting the null and alternate hypotheses, respectively. The
goal of this section is to show that one can construct a test T that has arbitrarily small Type-I and
Type-II errors. For this purpose, we consider the test
T :I{d(G1,G2) ≥ξ}	(5)
for some ξ > 0, where I{∙} is the indicator function and d(G1,G2) is the proposed graph distance
for some choice of integer n0. We state the following theoretical guarantee for the test T.
Theorem 3 Assume that the graphons wι, w2 satisfy Assumptions 1-3, and let the graphs Gi 〜wι
and G2 〜 w2 have at least n nodes. As n → ∞, there is a choice of ξ such that the Type-I and
Type-II errors of the test T in (5) goto0if n0 )g n → 0 and φ ≥ nC, where the constant C depends
only on the Lipschitz constants.
Theorem 3 shows that the test T in (5) can distinguish between any pair of graphons that have
separation ∣∣wι - w2∣∣L2 = Ω(1∕n0) with arbitrarily small error, if the graphs are large enough.
Empirical analysis. We validate the consistency result in Theorem 3 by computing power of the
proposed test T, which measures the probability of rejecting the null hypothesis H0 . Intuitively,
power of the test for graphs sampled from same graphons should be small (close to a pre-specified
significance level) since H0 must not be rejected, whereas, it should be close to 1 for graphs sampled
from different graphons. As known in the testing literature, theoretical threshold, ξ in (5), is typi-
cally conservative in practice and the rejection/acceptance is decided based on p-values, computed
using bootstrap samples. To this end, we follow the bootstrapping strategy in Ghoshdastidar & von
Luxburg (2018, Boot-ASE algorithm). We also compare the test T by replacing d(G1, G2) in 5
8
Published as a conference paper at ICLR 2022
o.oo	0.25	0.22	0.38
0.25	0.00	0.10	0.34
0.22	0.10	0.00	0.26
0.38	0.34	0.26	0.00
W1 W2 W3 W4
L2 distance
0.06	1.00	1.00	1.00
1.00	0.06	0.97	1.00
1.00	0.97	0.03	1.00
1.00	1.00	1.00	0.13
I w1	I W2	I w3	w4
Proposed Distance
0.04	0.99	0.90	0.05
0.99	0.03	0.26	1.00
0.90	0.26	0.05	1.00
0.05	1.00	1.00	0.03
w1 w2 w3 w4
Log Moments
1.00
1.00
1.00 1.00 1.00
0.57
w2 -
w3 -
w4 -
1.00 1.00 1.00
0.64 0.47
0.47 0.51
1.00
1.00
0.77
W1 W2 W3 W4
MMD
Figure 3: (left) L2 distance between the graphons W1, W2, W3 and W4. (other plots) Average
power of the test (5) for a graph pair of sizes 100 and 200, sampled from every pair of graphons.
with two other statistics, log moments (Mukherjee et al., 2017) and MMD, an efficient test statistics
for random dot product graphs (Agterberg et al., 2020). We perform the experiment by sampling
graphs Gi 〜wι and G?〜w2 of size n and 2n, respectively, where wι and w2 are chosen from
graphons W1, W2, W3, W4. The power ofT is computed for n = 100 (thus n0 = 10 from the theo-
retical bound) and the significance level 0.05, averaged over 500 trials of bootstrapping 100 samples
generated from all pairs of graphons. The plots in Figure 3 show the average power of T with our
proposed distance, log moments and MMD as d(G1, G2), respectively. From the results, it is clear
that T using our distance can distinguish between graphons that are quite close too (smallest L2
distance for W2 and W3), whereas, other test statistics are weak as log moments statistic accepts H0
even when it is wrong (see W1 and W4) and MMD based test rejects it strongly almost always (see
diagonal). Appendix C shows similar result for small and large n, and evaluation on real datasets.
5	Conclusion
There has been significant progress in learning on complex data, including network-valued data.
However, much of the theoretical and algorithmic development have been in large sample problems,
where one has access to m → ∞ independent samples. Practical applications of network-valued
data analysis often leads to small sample, large graph problems—a setting where the machine learn-
ing literature is quite limited. Inspired by graph limits and high-dimensional statistics, this paper
proposes a simple graph distance (2) based on non-parametric graph models (graphons).
Sections 3-4 demonstrate that the proposed graph distance leads to provable and practically effective
algorithms for clustering (DSC and SSDP) as well as two-sample testing (5). Extensive empirical
studies on simulated and real data show that the clustering based on the graph distance (2) out-
performs methods based on more complex graph similarities or metrics, both in terms of accuracy
and scalability. Figures 1-2 show that DSC achieves best performance for both small dense graphs
(simulated graphons) as well as large sparse graphs (social networks). On the other hand, popular
machine learning approaches—graph kernels or graph matching—can be computationally expensive
in large graphs and their performance may not improve as n → ∞, see WWLGK in Figure 7.
Statistical approaches, such as the proposed clustering algorithms and two-sample test, show better
performance on large graphs (Figures 1, 3 and Appendix C.4). Theorems 1-3 theoretically support
this observation by showing consistency of the clustering and testing methods in the limit ofn → ∞.
The theoretical results, however, hinge on Assumptions 1-3. We remark that such smoothness and
equivalence assumptions could be necessary for meaningful non-parametric approaches, which is
also supported by the graph testing and graphon estimation literature. Further insights about the
necessity of smoothness assumptions would aid in theoretical and algorithmic development.
The poor performance of graph kernels and graph matching in clustering and small sample problems
calls for further studies on these methods, which have shown success in network classification.
Fundamental research, combining graphon based approaches and kernels, could lead to improved
techniques. Instead of sorting-and-smoothing graphon estimator, other histogram based methods
that reduce graphons to a fixed size by identifying block structures and averaging (Olhede & Wolfe,
2014) can be explored. However, the distance defined on such reduced graphons require alignment
of the blocks between the graphons. While such methods do not require Assumption 2 and allow
sparse graphs, the computationally inefficient alignment step pose a practical challenge. Algorithmic
modifications, such as estimation of K, would be also useful in practice.
9
Published as a conference paper at ICLR 2022
6	Acknowledgements
This work has been supported by the German Research Foundation (Research Training Group GRK
2428) and the Baden-WUrttemberg StiftUng (Eliteprogram for Postdocs project “Clustering large
evolving networks”). The authors thank the International Max Planck Research School for Intelli-
gent Systems (IMPRS-IS) for sUpporting Leena ChennUrU Vankadara.
7	Ethics Statement
The proposed clUstering algorithms for network-valUed data can natUrally be evalUated on real-
world data, bUt no social interpretation aboUt the resUlts will be drawn. Moreover, development of
fair clUstering algorithms is not the focUs of this project.
8	Reproducibility Statement
The assumptions for the theory are stated clearly in Assumptions 1-3 and all the theoretical re-
sUlts, Proposition 1, Theorems 1-3, Corollary 1, are proved in detail in Appendix A. The imple-
mentation of the considered algorithms with steps to reproduce the results is available in Github -
https://tinyurl.com/3ycmmj4u. Datasets used in the experiments are public and the links are pro-
vided in download-datasets function of Utils .py. The experimental results can be repro-
duced by following graph_clustering.ipynb.
References
Joshua Agterberg, Minh Tang, and Carey Priebe. Nonparametric two-sample hypothesis testing for
random graphs with negative and repeated eigenvalues. arXiv preprint arXiv:2012.09828, 2020.
Peter J Bickel and Aiyou Chen. A nonparametric view of network models and newman-girvan and
other modularities. Proceedings of the National Academy of Sciences, 106(50):21068-21073,
2009.
Karsten M Borgwardt, Cheng Soon Ong, Stefan Schonauer, SVN Vishwanathan, Alex J Smola, and
Hans-Peter Kriegel. Protein function prediction via graph kernels. Bioinformatics, 21(Suppl 1):
i47-i56, 2005.
Horst Bunke and Kim Shearer. A graph distance metric based on the maximal common subgraph.
Pattern recognition letters, 19(3-4):255-259, 1998.
Stanley Chan and Edoardo Airoldi. A consistent histogram estimator for exchangeable graph mod-
els. In International Conference on Machine Learning, pp. 208-216, 2014.
Persi Diaconis and Svante Janson. Graph limits and exchangeable random graphs. arXiv preprint
arXiv:0712.2749, 2007.
Paul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes without
alignments. Journal of molecular biology, 330(4):771-783, 2003.
Simon S Du, Kangcheng Hou, Russ R Salakhutdinov, Barnabas Poczos, Ruosong Wang, and Keyulu
Xu. Graph neural tangent kernel: Fusing graph neural networks with graph kernels. In Advances
in Neural Information Processing Systems, pp. 5724-5734, 2019.
Xinbo Gao, Bing Xiao, Dacheng Tao, and Xuelong Li. A survey of graph edit distance. Pattern
Analysis and applications, 13(1):113-129, 2010.
Debarghya Ghoshdastidar and Ulrike von Luxburg. Practical methods for graph two-sample testing.
In Advances in Neural Information Processing Systems, pp. 3019-3028, 2018.
Debarghya Ghoshdastidar, Maurilio Gutzeit, Alexandra Carpentier, and Ulrike von Luxburg. Two-
sample tests for large random graphs using network statistics. In Conference on Learning Theory,
pp. 954-977, 2017.
10
Published as a conference paper at ICLR 2022
Debarghya Ghoshdastidar, Maurilio Gutzeit, Alexandra Carpentier, Ulrike von Luxburg, et al. Two-
sample hypothesis testing for inhomogeneous random graphs. Annals of Statistics, 48(4):2208-
2229, 2020.
Xiaoyang Guo, Anuj Srivastava, and Sudeep Sarkar. A quotient space formulation for generative
statistical analysis of graphical data. Journal of Mathematical Imaging and Vision, pp. 735-752,
2019.
Olga Klopp, Alexandre B Tsybakov, Nicolas Verzelen, et al. Oracle inequalities for network models
and sparse graphon estimation. The Annals of Statistics, 45(1):316-354, 2017.
Risi Kondor and Horace Pan. The multiscale laplacian graph kernel. In Advances in Neural Infor-
mation Processing Systems, pp. 2990-2998, 2016.
Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics
quarterly, 2(1-2):83-97, 1955.
Laszlo Lovasz and BalazS Szegedy. Limits of dense graph sequences. Journal of Combinatorial
Theory, Series B, 96(6):933-957, 2006.
Soumendu Sundar Mukherjee, Purnamrita Sarkar, and Lizhen Lin. On clustering network-valued
data. In Advances in neural information processing systems, pp. 7071-7081, 2017.
Annamalai Narayanan, Mahinthan Chandramohan, Rajasekar Venkatesan, Lihui Chen, Yang Liu,
and Shantanu Jaiswal. graph2vec: Learning distributed representations of graphs. arXiv preprint
arXiv:1707.05005, 2017.
Mark EJ Newman. The structure and function of complex networks. SIAM review, 45(2):167-256,
2003.
Lutz Oettershagen, Nils M Kriege, Christopher Morris, and Petra Mutzel. Temporal graph kernels
for classifying dissemination processes. In Proceedings of the 2020 SIAM International Confer-
ence on Data Mining, pp. 496-504, 2020.
Sofia C Olhede and Patrick J Wolfe. Network histograms and universality of blockmodel approxi-
mation. Proceedings of the National Academy of Sciences, 111(41):14722-14727, 2014.
Shirui Pan, Jia Wu, Xingquan Zhu, Guodong Long, and Chengqi Zhang. Task sensitive feature
exploration and learning for multitask graph classification. IEEE transactions on cybernetics, 47
(3):744-758, 2016.
Michael Perrot, Pascal Esser, and Debarghya Ghoshdastidar. Near-optimal comparison based clus-
tering. In Advances in Neural Information Processing Systems, pp. 19388-19399, 2020.
Kaspar Riesen and Horst Bunke. Approximate graph edit distance computation by means of bipartite
graph matching. Image and Vision computing, 27(7):950-959, 2009.
Benedek Rozemberczki, Oliver Kiss, and Rik Sarkar. Karate club: An api oriented open-source
python framework for unsupervised learning on graphs. In Proceedings of the 29th ACM Inter-
national Conference on Information & Knowledge Management, pp. 3125-3132, 2020.
Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn, and Karsten M
Borgwardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(9):
2539-2561, 2011.
Cornelis J Stam, BF Jones, G Nolte, M Breakspear, and Ph Scheltens. Small-world networks and
functional connectivity in alzheimer’s disease. Cerebral cortex, 17(1):92-99, 2007.
Minh Tang, Avanti Athreya, Daniel L Sussman, Vince Lyzinski, Youngser Park, and Carey E Priebe.
A semiparametric two-sample hypothesis testing problem for random graphs. Journal of Compu-
tational and Graphical Statistics, 26(2):344-354, 2017a.
Minh Tang, Avanti Athreya, Daniel L Sussman, Vince Lyzinski, and Carey E Priebe. A nonpara-
metric two-sample hypothesis testing problem for random graphs. Bernoulli, 23(3):1599-1630,
2017b.
11
Published as a conference paper at ICLR 2022
Robert L Thorndike. Who belongs in the family? Psychometrika,18(4):267-276,1953.
Matteo Togninalli, Elisabetta Ghisu, FeliPe Llinares-Lopez, Bastian Rieck, and Karsten Borgwardt.
Wasserstein weisfeiler-lehman graph kernels. In Advances in Neural Information Processing
Systems, pp. 6439-6449, 2019.
Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17(4):395-416,
2007.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In International Conference on Learning Representations, 2019.
Bowei Yan, Purnamrita Sarkar, and Xiuyuan Cheng. Provable estimation of the number of blocks
in block models. In International Conference on Artificial Intelligence and Statistics, pp. 1185-
1194, 2018.
Shuicheng Yan, Dong Xu, Benyu Zhang, Hong-Jiang Zhang, Qiang Yang, and Stephen Lin. Graph
embedding and extensions: A general framework for dimensionality reduction. IEEE transactions
on pattern analysis and machine intelligence, 29(1):40-51, 2006.
Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In ACM SIGKDD international
conference on knowledge discovery and data mining, pp. 1365-1374, 2015.
Mikhail Zaslavskiy, Francis Bach, and Jean-Philippe Vert. A path following algorithm for the graph
matching problem. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(12):
2227-2242, 2008.
Yuan Zhang, Elizaveta Levina, and Ji Zhu. Estimating network edge probabilities by neighbourhood
smoothing. Biometrika, 104(4):771-783, 2017.
12
Published as a conference paper at ICLR 2022
A Proofs of theoretical results
We discuss the proofs of Proposition 1 and Theorems 1-3 with supporting lemmas in this section.
A.1 Proposition 1
The distance function defined in (2) estimates the L2-distancebetween graphons that are continuous.
To prove this, we introduce a method to discretize the continuous graphons in the following so that
it is comparable with the transformed graphs described in the graph distance estimator (2).
Graphon discretization. We discretize the graphon by applying piece-wise constant function ap-
proximation that is inspired from Chan & Airoldi (2014), similar to the graph transformation. More
precisely, any continuous graphon w is discretized to a matrix W of size n0 × n0 with
Wij
11	,	,
w(x +-, y +-)dx dy
n0	n0
(6)
We make Assumptions 1-3 to derive the concentration bound stated in Proposition 1. The proof
structure is as follows:
1.	We bound the point-wise deviation of graphon with its discretized correspondence using
Lipschitzness assumption (Lemma 1).
2.	We derive the error bound between the L2-distance and Frobenius norm of the discretized
graphons using Lemma 1 (Lemma 2).
3.	We establish a relation between Frobenius norm of the histograms of graphons and graphs
(Lemma 3).
4.	Finally, we prove Proposition 1 by combining Lemmas 2 and 3.
Lemma 1 (Lipschitz condition) For any graphon W and corresponding discretization W, define a
piecewise constant function w(x, y) = Wij where X ∈ [太,ɪ + ] and y ∈ [ j, j + n^ ]. Using
the Lipschitz continuous assumption, we have
Iw(X,y) - w(x,y)l ≤ ------,	(7)
n0
where L is the Lipschitz constant in Assumption 1.
Proof of Lemma 1 (Lipschitz condition). We use Assumption 1 on Lipschitzness to prove this
lemma. The following holds for a graphon w with Lipschitz constant L,

w(x + —,y + j) -w(χ,y)
n0	n0
≤L
与十j ≤ Wl
n20	n20	n0
(8)
We prove Lemma 1 using (8) and the definition of w(x, y) = Wij where X ∈ [W, W + *]and
y ∈ [njo, njo+ no]，
l—7------ l -7---------------- i	j
lw(χ,y) - w(x,y)1 = w(χ,y) - w(x,y) ± w(x +-------,y + ―)
l	n0	n0
≤ w(x,y) - w(x + -,y + j) + w(x + —,y + j) - w(x,y)
n0 n0
n0 n0
2√⅞L 口
no
i
j
Lemma 2 (Error bound of discretization) For two graphons w1 and w2, the error bound between
the L2 -distance and the Frobenius norm of the corresponding discretized graphons W1 and W2
satisfies
k wl - w2 ∣∣L2-----kW1 - W2∣∣F ≤ ' .	(9)
l	2 n0	l n0
13
Published as a conference paper at ICLR 2022
Proof of Lemma 2 (Error bound of the approximation). Lemma 1 is used to prove this lemma.
Let L1 and L2 be the Lipschitz constants of w1 and w2 .
I w1 一 w2 I2L2
(w1 (x, y) - w2(x, y))2 dxdy
±w](x, y) and ±w2(x, y) within the square, expand and apply Lipschitzness condition from Lemma 1
8L]2	8L22	16L]L2	n0 n0 1	2
≤ 葛+菊++X X no ((WI)kl -(W2)kl)
+ 4√2L0≡ X X "g)kl-w)kll
0	k=0 l=0 0
≤ (2v/2(LI + L2))2 +』W] - W2kF +
n0	n02	F
≤ (2√2(LI + L2)+ ɪ∣W] - W2∣F)2
n0
n0
4√2(Li + L2)
kW1 - W2k1
(10)
(a)： ∣∣x∣∣ι ≤ √n IlxIIF
Similarly, by ±w](x,y) and ±w2(x,y) to ɪ∣W] 一 吨∣∣F and applying LiPschitzness condition
n20
from Lemma 1 we get,
"kW1 一 W2kF ≤ (S(L10+L2) + kw1 一 w2kL2)2
(11)
Combining (10) and (11), we prove
k w1-w2 kL2- nokW] - W2kF ≤
2√2(L1 + L2 (≤ 4V2L,	(b)： L = max"],乙?”
n0
n0
We derive the following relation between histogram of graphs and graphons by adapting lemmas
from Chan & Airoldi (2014) for our problem and the error bound of discretization (9).
Lemma 3 Let G]〜wι and G2 〜w2 have respective graph transformations A] and A2. Let W]
and W2 be the corresponding discretized graphons of w1 and w2, respectively. As n → ∞ and
n20 logn
----------→ 0, then for any e > 0,
n
|I A1 - A2 IF - I W1 - W2 IF| ≤ 4
with probability converging to 1.
(12)
Proof of Lemma 3. The proof of this lemma is inspired from Chan & Airoldi (2014). Fori = {1, 2},
let matrix Wi of size n0 × n0 be the discretized graphon wi and let matrix Abi of size n0 × n0 be
another transformation of graph Gi based on the true permutation σbi, that is, σbi denotes the ordering
of the graph Gi based on the corresponding graphon wi. In other words, the discretized graphon Wi
is the expectation of Ai . The reordered graph is denoted by Giσ and then Ai is obtained by,
hh
1σ	n
(Ai) kl = QT T(G i )kh+kιih+iι where h b C and [∙[ is the floor function.
h	n0
k1=0l1=0	0
ɪɪ T 1	ill Λ	Λ Il	♦	个 个 TTT-	1 TTT-
We bound IA1 一 A2 IF using A1, A2, W1 and W2 as,
^ ..	^
∣Aι - A2∣f ≤ ∣Aι - Ai∣f + kA2 一 A2∣f +
—
≤ 2 max
i={1,2}
≤ 2 max
i={1,2}
IAi 一 AbiIF + 2 max IAbi 一 Wi IF + IW1 一 W2 IF (13)
i={1,2}
14
Published as a conference paper at ICLR 2022
We have the following for all i using Assumption 2 on Lipschitzness of the expected degree function
g(u)i of graphon wi and Lemma 3 of Chan & Airoldi (2014),
E[kAi- AikF ] ≤ n2 (2+4c2l2
≤ 2 U +
log n
n
max kAi - Abi k2F
i={1,2}	F
; Ci depends on Lipschitz constants of g(u)i
(14)
Applying Markov’s inequality to bound the probability P
using (14),
(a)
P max kAi - Abi k2F ≥ 12 ≤
i={1,2}	i={1,2}
E UAi- AikF]
72
1
; (a) : Union bound
(15)
Thus asymptotically, as n → ∞ and
probability converging to 1.
n20 log n
n
→ 0, for all i and any 1 > 0, kAi - Abi kF < 1 with
(=)O ( n log n )
—k e2n )
From Lemma 4 of Chan & Airoldi (2014), we have the following for all i,
E[kAi-Wi kF ] ≤ n0 ≤ n2
(16)
Applying Markov’s inequality to bound the probability P max kAbi - Wik2F ≥ 22 using (16),
i={1,2}
2	2 (a)	E hkAbi -Wik2Fi (16) 2n04
P max kAi - WikF ≥ e2	≤ E - -------------2-------- ≤ ɪ-2- ; (a): Union bound
i={1,2}	i={1,2}	2	2n
(17)
Again asymptotically as n → ∞, for all i and any 2 > 0, kAbi - WikF < 2 with probability
converging to 1. Lets assume 2 = 1 = . Substituting (15) and (17) in (13),
kA1 - A2 kF ≤ 4 + kW1 - W2 kF	(18)
n2 log n
With probability converging to 1 as n → ∞ and -----------> 0.
n
The lower bound can similarly be obtained,
kA1 - A2 kF ≥ kW1 - W2 kF - 2 max kAbi - Wi kF - 2 max kAbi - Ai kF
i={1,2}	i={1,2}
≥ kW1 - W2kF - 4
(19)
n2 log n
with probability converging to 1 as n → ∞ and--------------> 0.
n
Thus, |kA1 - A2kF - kW1 - W2 kF| ≤ 4 satisfy for any > 0 with probability converging to 1
n2 log n
as n → ∞ and-------------> 0 from equations (18) and (19).
n

Proof of Proposition 1 (Graph distance is consistent). Proposition 1 immediately follows from
Lemmas 2 and 3 after a simple decomposition step as shown below.
|k w1 - w2 kL2 - d(G1, G2)| =
≤
2,3
≤
k w1 - w2 ∣∣L2 - d(G1, G2) 土 一IlWI- W2kF
2	n0
k wι- W2 kL2 - ɪkWι- W2kF + ɪkWι- W2kF - d(G1,G2)
2	n0	n0
4√2L	4e c / 1、1	C	In log n	Cl_，
-------1-= O - holds for any e > 0 as n → ∞ and —°---> 0 .□
n0	n0	n0	n
15
Published as a conference paper at ICLR 2022
A.2 Proposition 1 For Sparse Graphs
To adapt the dense graph model, graphon, to sparse graphs, the sampling procedure is modified
to Gij ∣Ui,Uj-〜 Bernoulli (PW(Ui,Uj)) where P depends only on n. With this modification,
equation 14 will be changed to
E[kAi - AikF] ≤ 2n0 + %C2L2p2皿 + n24C2L2ρ皿=O
n2	n2	n	n
n
(20)
and the proof involves modification to Lemma 2 of Chan & Airoldi (2014). We state the modified
lemma and briefly sketch the proof below.
Modified Lemma 2 of Chan & Airoldi (2014). Let σ(i) be the oracle permutation such that
Uσ(1) < Uσ(2) < . .
.<Uσ(n).Then,if 手-σj
1 log n
< 6L17V~
log n
n
(21)
with probability at least 1 - 8exp ( — —ɪɑgn ). Conversely, if (21) holds with probability at
18L21 P2
1	log n
least 1 - 8exp -Te -then
18L21 P2
σ(j)
log n (ɪ 1	1
n V3L1 + 3LiL2 + L2
(22)
1 log n
with probability at least 1 - 40 exp — ——2 —
18L21 P2
Changes to Proof in Chan & Airoldi (2014). Suppose ∣ σ(i - σj ∣ < P for δ > 0 and δ < ρ.
Then, P (∣Uσ(i) - Uσ(j)1 > 3P) ≤ 4 exp (-2nP2). Consequently,
P (∣g(Uσ(i)) - g(Uσ(j))∣ > 3Llδ) ≤ P
i) - Uσ(j) I > 3 P) ≤ 4exp J2""*).
Following their proof, we have
P ∣dσ(i) - dσ(j)∣ > 6L1δ ∣ Uσ(i), Uσ(j)
≤ 4 exp (-^nL2δ2) + 4exp (-2n≤ 8 exp (-2ng),
(23)
when ρ > 击.Note that there is a small mistake in their proof when Hoeffding's inequality is
applied, where a factor of n2 is written in place of n. Putting δ = 6L^ Jlonn and considering
P = Ω
P
since ρ depends on n and δ < ρ, we get
i) - dσ(j)l > ʌ/ —g— I Uσ(i), Uσ(j) ) ≤ 8exp
1 log n
18L1 ~2^~
Converse can similarly be proved.
(20) can be derived by substituting the above changed lemma in their proof. Consequently, Propo-
sition 1 still holds under this formulation, with slight change to the condition as n → ∞ and
n20P log n
------------→ 0 for ρ = Ω
n
our algorithms in the next sections.
. We will not consider sparsity ρ for analysing consistency of
—
16
Published as a conference paper at ICLR 2022
A.3 Distance Based Spectral Clustering (DSC)
We make Assumptions 1-3 on the K graphons to analyse the Algorithm 1. We establish the Consis-
tency of this algorithm by deriving the number of misclustered graphs |M| through the following
steps.
1.	We establish deviation bound between the estimated distance matrix Db and the ideal dis-
tance matrix D (Lemma 4).
2.	We formulate Davis-Kahan theorem in terms of the deviation bound using the result from
Mukherjee et al. (2017) (Lemma 5).
3.	We derive the number of misclustered graphs from Lemma 5.
As stated previously, Db in Algorithm 1 is an estimate of D ∈ Rm×m , where we define Dij =
kwi - wjkL2. Note that D is a block matrix with rank K, since Dij = 0 for all Gi, Gj generated
from same graphon, and equals the distance between the graphons i and j otherwise.
We derive the deviation bound for the distance matrix using Lemma 3 and the result is as follows.
n2 log n
Lemma 4 (Distance deviation bound) As n → ∞ and----------------> 0, we establish
n
m
with probability converging to 1.
Db-DF=O
n0
(24)
Proof of Lemma 4 (Distance deviation bound). From Proposition 1 and the definitions of Dij and
Dij, it is easy to see that ∣Dj 一 DijI = O ) with probability converging to 1 as n → ∞ and
n2 log n → 0
n.
Using Lemmas 2 and 3, and the definitions of Dij and Dij , we have
—kAi - Aj∣∣f - Ilwi - Wj∣∣L2
n0
—kAi - AjkF ± —IlWi - Wj kF - kwi - wj∣∣L2
n0	n0
≤ n lkAi- Aj kF -kWi - WjkF |+ kwi - wj kL2 - n10 kWi - Wj kF
n0
(12) 4e	4√2L
≤-------+-------
n0 n0
with prob. → 1 asymptotically
Hence, kDb - DkF = O
Thus asymptotically,
Dijl = O f ɪ) with probability converging to 1.
m2n2 logn
with probability converging to 1 as n → ∞ and-----------> 0.
n
A variant of Davis-Kahan theorem (Mukherjee et al., 2017) and the derived deviation bound (24)
are used to prove the following lemma.
ɪ _____ k /C ♦一 一……_________.ʌ ʃ ,TΛ j√> T ,1	r/ ,	.	1	1	j
Lemma 5 (Davis-Kahan theorem) Let V and V be the m×K matrices whose columns correspond
to the leading K eigenvectors of D and Db, respectively. Letγ be the K-th smallest eigenvalue value
n20 log n
of D in magnitude. As n → ∞ and----------> 0, there exists an orthogonal matrix O such that,
n
VbOb-VF=O
(25)
with probability converging to 1.
—
1
17
Published as a conference paper at ICLR 2022
Proof of Lemma 5 (Davis-Kahan theorem). A variant of Davis Kahan theorem from Proposition
A.2 of Mukherjee et al. (2017) states the following for matrix D of rank K. Let Vb and V be m * K
matrices whose columns correspond to the leading K eigenvectors of Db and D, respectively, and γ
be the K-th smallest eignenvalue of D in magnitude, then there exists an orthogonal matrix Ob of
size K * K such that,
V O - VL ≤ 4ID - DlF a o
m2n02 log n
as n → ∞ and--------------→ 0.□
n
The number of misclustered graphs is |M| ≤ 8mT kVbOb- V k2F where mT is the maximum number
of graphs generated from a single graphon (Mukherjee et al., 2017). Since mT = O(m), |M| =
3
m3
O
by substituting (25) in |M|. Hence proving Theorem 1.
Proof of Theorem 1. The number of misclustered graphs |M| ≤ 8mT Vb Ob - V from
Mukherjee et al. (2017). Thus, we prove the theorem using Lemma 5. That is, as n → ∞ and
m2n02 log n
-----"-------→ 0,
n
|M| ≤ 8mT Vb Ob - V 2 =5 O
3
m3

Proof of Corollary 1. This corollary deals with a special case where K = 2 and equal number of
graphs are generated from the two graphons w and w0. Therefore, mT in the number of misclustered
graphs |M| is m/2. The ideal distance matrix D will be of size m × m with 0 and kw - w0 kL2 as
entries depending on whether the samples are generated from the same graphon or not. For such a
block matrix D, the two non zero eigenvalues are ± — ∣∣w 一 w0kl? . Therefore, Y is — ∣∣w 一 w0kl2.
Corollary 1 can be derived by substituting the derived γ in the number of misclustered graphs |M|
in Theorem 1 as shown below.

|M| = O
kw 一 w0kL2 n0
Let us assume ∣w 一 w0 ∣L2
|M| →0.
≥ Cm where C is a large constant, then as n → ∞,
n0
—2n02 log n
n
→ 0,
□
A.4 Similarity Based Semi-Definite Programming (SSDP)
We make Assumptions 1-3 on the K graphons to study the recovery of clusters from Algorithm 2.
The proof structure for cluster recovery stated in Theorem 2 is as follows:
1.	We establish deviation bound between the estimated similarity matrix S and the ideal sim-
ilarity matrix S (Lemma 6).
2.	We derive the recoverability condition by adapting Proposition 1 of Perrot et al. (2020) and
the obtained deviation bound (Lemma 7).
The ideal similarity matrix S ∈ Rm×m is symmetric with K × K block structure, and S = ZΣZT
where Z ∈ {0, 1}m×K be the clustering membership matrix and Σ ∈ RK×K such that Σll0 repre-
sents ideal pairwise similarity between graphs from clusters Cl and Cl0. From the definition of Sij,
∑ιιo = exp I -∣wl——wlkL2 ) where Wl and w“ are graphons corresponding to clusters Cl and C,
σlσl0
respectively. Sbis the estimated similarity matrix of S as mentioned earlier. Since Xb ∈ RK×K is the
normalised clustering matrix, X = ZN-1ZT where N is a diagonal matrix with -^,..., -；∣.
|C1 |	|CK |
We derive the deviation bound for the similarity matrices using Lemma 3 and the result is as follows.
18
Published as a conference paper at ICLR 2022
Lemma 6 (Similarity deviation bound) As n → ∞,
→ 0, we establish
n20 log n
n
(26)
with probability converging to 1. Hence, from the result kS - SkF
converging to 1.
with probability
Proof of Lemma 6 (Similarity deviation bound). We derive the bound using Lemmas 2 and 3, and
the definitions of Sij and Sij .
Sbij
exp -
IlAi - AjIIF
n0σiσj
exp
kWi- WjkF +4e
noσ2
exp (-kwi-WjkL2 ) eχp
4e + 4√2L
noσ2
Consider σi = σj = σ
with probability → 1 asymptotically
exp(-x) ≥ 1 - 2x for x > 0
Sij - Sij
8(e + √2L)
noσ2
8(e + √2L)
noσ2
8(e + √2L)
noσ2
Sij ∈ [0, 1]
(27)
^
Sij
exp -
kAi - Aj kF
n0σ2
exp -
kWi- WjkF - 4e
n0σ2
≤ exp ( kwi - wjkL2 ʌ exp 44e + 4√2L
≤ exp	2 J exp	2
σ2	n0σ2
≤ Sij(1 + 注善!
Sij + Sij-
8(E + √2L)
Sij +	O
n0σ2
with probability → 1 asymptotically
exp(x) ≤ 1 + 2x for x > 0
Sij ∈ [0, 1]
(28)
3
≥
2
≥
≥
≥
≥
3
≤
≤
≤
—
Thus, from (27) and (28), we get |Sbij - Sij | ≤
for any E, with probability
..r	∣n2logn -	8m	8m	8 8m(E + √2L)
converging to 1 as n → ∞ and----------> 0. Hence, kS — S∣∣f ≤ -------T)---
n	n0σ2
m2n2 logn
With probability converging to 1 as n → ∞ and---------> 0.
n
The condition for exact recovery of clusters is derived by adapting Proposition 1 of Perrot et al.
(2020). The proposition states the recoverability condition for such an SDP defined in (4) in terms of
the similarity deviation bound. Thus, We use the derived bound in Lemma 6 and establish condition
on the L2 -distance to satisfy the proposition from Perrot et al. (2020). First, We state the adapted
proposition.
19
Published as a conference paper at ICLR 2022
We define ∆1 and ∆2 as,
ʌ	1	ʌ	IC	f^1 I
∆1 = min(1 - Σll0) and ∆2= max |Sij - Sij |.
Then, the following should be satisfied for Xb to be the unique optimal solution of the SDP in (4):
..^ . _ .
kSb - S kF ≤ min |Cl | min
, ∆1 - 6∆2 .
The minimum cluster size min |Cl | in our case is 1. Consequently, the recoverability condition is
derived and is as follows.
Lemma 7 (Recoverability of clusters) As n → ∞,
m2n2 log n → 0,the min
n
be Ω (m) So that X is the unique optimal solution ofthe SDP (4).
l6=l0
kwl - wl0 kL2 should
Proof of Lemma 7 (Recoverability of clusters). We derive the condition to satisfy the stated
proposition.
ʌ	1	∖ '	t ʌ	∙ I r⅛	<-i	∖ EI	♦	♦	ι .	∙	∙	ι 1
∆1 = 1 - maxl6=l0 Σll0 and ∆2= min |Sij - Sij |. The minimum cluster size in our case can be 1.
ij
The analyses of the two cases of the Proposition is as follows.
Case 1.	Let Us assume ∆2 ≤ ^1, then min { δ21 , ∆1 - 6∆?} will be 号.Therefore,
8m(c + √2L)
n0σ2
kwl -wl0kL2
exp — min----------------2
l6=l0	σ2
kwl - wl0 kL2
min-------------
l6=l0	σ2
ml6=iln0 kwl - wl0kL2
min kwl - wl0 kL2
1- ImaXexp (-
kwl - wl0 kL2
σ2
16m(e + √2L)
n0σ2
(	16m(e + √2L)
-log 1-----------2----
n0σ2
(_	、 k
16m(e + √2L) ʌ 1
k k
(29)
≤
≤
≥
Case 2.	Let US assume ∆2 > ^1, then min { ∆21, ∆ι - 6∆2} will be ∆ι - 6∆2. Therefore,
8m(e + √2L)
n0σ2
kwl - wl0 kL2
min
l6=l0	σ2
ml6=iln0 kwl - wl0 kL2
min kwl - wl0 kL2
≤1
- max exp
l6=l0
kwl - wl0kL2
σ2
6 * 8(e + √2L)
n0σ2
with probability → 1 aymptotically
≥ - log 1 -
≥ σ2
Σ
k=1
8(m + 6)(e + √2L)
n0σ2
8(m + 6)(e + √2L)、k 1
n0σ2	k
(30)
∞
—
Thus, from (29) and (30), We must satisfy mini=" ∣∣wι - w* ∣∣L2 = Ω
hold. Consequently, Theorem 2 is the direct reflection of this lemma.
for the Proposition to
20
Published as a conference paper at ICLR 2022
A.5 Graph Two-Sample Testing
Theorem 3 of two-sample testing is proved by deriving the probability of Type-1 and Type-2 errors.
We make Assumptions 1-3 for this case. Let Wi and W2 be the no X no discretized graphons of wι
and w2 , respectively, obtained using (6). Then, the alternate hypothesis Ha can be rewritten using
Lemma 2 in the following way,
1	4√2L (9)
—kWi- W2kF + ^^ ≥ φ
no	F	no
kWi- W2kF ≥ noφ - 4√2L = P	(31)
We derive the probability of the errors using Lemma 3 and is stated in the following lemmas.
Lemma 8 (Probability of Type-1 error) The probability of Type-1 error, i.e. rejecting the null hy-
pothesis when it is actually true, is
P(T = 1|Ho : True) ≤ C logn
ξ2 n
(32)
where C depends only on the Lipschitz constants.
Proof of Lemma 8 (Probability of Type-1 error). The Type-1 error is rejecting Ho when it is
true. Therefore, in this scenario, kWi - W2 kF = 0. Thus, from Lemma 3, (15) and (17), we have
C n2 log n
||Ai 一 A2∣∣F ≤ 4e with 1 ——q--------probability. Therefore, the probability of Type-1 error is,
P(T = 1|Ho : T rue) = P(d(Gi, G2) ≥ ξ)
= P(kAi - A2kF ≥ noξ)	err only when noξ ≤ 4
(≤) C n0 log n ∏
一ξ2 n0	n
Lemma 9 (Probability of Type-2 error) The probability of Type-2 error, i.e. accepting the null
hypothesis when the alternate hypothesis is actually true, is
P(T = 0Ha: True) ≤ G-M 等	(33)
where Ci and C2 depend only on the Lipschitz constants.
Proof of Lemma 9 (Probability of Type-2 error). The Type-2 error is evaluating to null hypothesis
when the alternate hypothesis is true. Therefore, from (31) kWi - W2kF ≥ ρ. From Lemma 3, (15)
and (17),
||Ai - A2k F ≥ k Wi - W2∣∣F - 4e	with probability 1 - 1-n n0 10g n
F	2 n
≥ ρ-4
The probability of Type-2 error is,
P(T = 0|Ha : T rue) = P(d(Gi, G2) < ξ)
= P(kAi - A2kF < noξ) err only when noξ ≤ ρ - 4; let noξ = 4
≤ P(kAi- A2kF < P)
≤ C1 no log n
P2	n
We get the probability by substituting -P- = φ 一 from (31) in the above equation. Theorem
no	no
3 can be proved by asymptotic analysis of Lemmas 8 and 9.	∏
21
Published as a conference paper at ICLR 2022
B DSC and SSDP Algorithms
The proposed algorithms DSC and SSDP are described as follows:
Algorithm 1: Distance based Spectral Clus- tering (DSC)	Algorithm 2: Similarity based Spectral Clus- tering (SSDP)
input : Adjacency matrices Gi,..., Gm, histogram size no output: K clusters C1, ..., CK	input : Adjacency matrices Gi,..., Gm,, histogram size no output: K clusters Ci, ..., CK
Construct distance matrix Compute Db ∈ Rm×m, whereDbij = d(Gi,Gj)	Construct similarity matrix Compute Sb ∈ Rm×m, where Sbij = exp (-dGGL)) σiσj
	with σi = . . . = σn
Clustering Apply spectral clustering to Db with K number of clusters resulting in Ci,...,Ck	Clustering Find Xb using (4) and apply standard spectral clustering to Xb resulting in Ci, ..., CK.
C	Experimental Details
In this section, we present experimental details and additional experiments.
C.1 Simulated Data - Heatmap of Graphons
Figure 4 shows the heatmap of the considered four graphons W1, W2, W3 and W4. We sample
graphs from these graphons for the experiments.
W1(u, v)	W2(u, v)	VlZ3(u, v)	W∕4(u, v)
Figure 4: Heatmaps of graphons W1 , W2 , W3 and W4.
C.2 CHOICE OFn0
We validate the theoretically deduced bound for no = O(ʌ/n/ log n) by sampling 5 graphs with
a fixed number of nodes n from each of the four graphons, in total 20 graphs, and measuring the
performance of DSC and SSDP for different n0 = {5, 10, 15, 20, 25, 30}. We perform three simu-
lations with n = {50, 100, 500} and fix neighbourhood of one in SSDP. Figure 5 shows the average
error of both the algorithms over 5 independent trials. Based on the theoretical considerations for
no (《，n/ log n), We evaluate no = {5,7,15} for n = {50,100,500}, respectively. The experi-
mental results show that the derived bound for n0 serves as a reasonable choice (if not the best) for
both DSC and SSDP irrespective of n. Hence, the choice of no can be deterministic and adaptive
with respect to n, thus making our algorithms parameter-free.
C.3 Experimental results using Adjusted Rand Index (ARI)
In this section, we provide the results for evaluation of algorithms on simulated and real data under
the same setting as described in Section 3.3. Figure 6 shows the evaluation of all the discussed
algorithms using ARI, where the observations made from error rate hold.
22
Published as a conference paper at ICLR 2022
Figure 5: Validation of the bound for n0 . The plot shows the average error rate (percentage of
misclustered graphs) of the proposed algorithms DSC and SSDP for different n = {50, 100, 500}.
DSC (ours)
SSDP (ours)
1.0 -
0.8 -
≡ 0-6-
< 0.4 -
0.2 -
0.0
1.0 -
0.8 -
7 0.6 -
<0.4-
0.2 -
0.0
・ DSC (ours)
SSDP (ours)
NCLM
NCMMD
WWLGK
GNTK
NCGMM
TLE Time Limit Exceeded
81,82,83	81,83,84 Bι,B2,B4 82,83,84	B1,B2,B3,B4
Figure 6: Evaluation of DSC and SSDP with other methods. (row 1) Results on simulated data using
ARI. (rows 2 and 3) Results on real data from Bioinformatics and Social Networks, respectively.
C.4 Evaluation on large simulated data
As mentioned in Section 3.3, we evaluate algorithms except GNTK and NCGMM on large graphs
sampled from the four graphons W1 , W2, W3 and W4 with nodes between 100 and 1000. Figure 7
shows the results measured using average error rate and average ARI, respectively. The proposed
algorithm DSC outperforms the others in all the case and SSDP stands second or third best, as
observed in simulated data with small graphs.
DSC (ours)	SSDP (ours)	NCLM	NCMMD WWLGK
Figure 7: Evaluation of all algorithms except GNTK and NCGMM using average error rate and
average ARI for large simulated data.
23
Published as a conference paper at ICLR 2022
C.5 Computation Time of Algorithms
Table 1 shows the time (measured in seconds) taken for the algorithms on the considered dataset
combinations. Three clusters also show similar behavior to four clusters.
Table 1: Time measured in seconds for algorithms on different datasets combinations
Dataset	DSC	SSDP	NCLM	NCMMD	WWLGK	GNTK	NCGMP
W1,W2,W3	0.13	0.22	0.17	11.66	0.66	225.46	15.43
W2,W3,W4	0.14	0.23	0.18	12.98	0.70	199.98	16.27
W1,W3,W4	0.13	0.25	0.19	12.93	0.71	200.85	16.98
W1,W2,W4	0.13	0.24	0.18	12.55	0.68	198.38	16.54
W1,W2,W3,W4	0.20	0.38	0.28	22.05	1.19	390.07	30.18
B1 , B2, B3	1.10	1.25	0.17	20.72	2.54	28.21	219.58
B1 , B3, B4	0.99	1.18	0.16	21.51	2.85	32.22	225.02
B1,B2,B4	1.01	1.08	0.14	15.61	1.92	19.49	174.25
B2, B3, B4	1.04	1.13	0.13	11.99	0.78	13.38	21.47
B1 , B2, B3, B4	1.33	1.46	0.183	28.66	3.19	40.18	278.80
S1,S2,S4	1.50	1.65	8.21	1125.71	454.19	1609.78	TLE
S1,S2,S3	1.25	1.34	0.36	77.67	15.32	294.53	TLE
S1,S3,S4	1.52	1.64	8.07	1001.52	348.25	1485.90	TLE
S2,S3,S4	1.48	1.69	8.88	1035.98	440.87	1757.06	TLE
S1,S2,S3,S4	1.97	2.22	9.47	1069.28	437.21	2060.68	TLE
C.6 Scalability experiment
We evaluate the scalability of the considered algorithms using simulated data by measuring the time
taken for clustering 40 random graphs, 10 sampled from each of the graphons W1 , W2, W3 and
W4. We did 7 experiments in which the size of the sampled graphs are varied as [50, max_size]
where max-size = {100,200,300,400, 500,600,700}. Figure 8 shows the experimental results
which illustrates high scalability of DSC, SSDP and NCLM over other algorithms. Note that the
experiment shows NCLM as scalable as DSC and SSDP since the sampled graphs are small.
DSC (ours) *NCLM * WWLGK* NCGMP
-*-SSDP (ours)-*- NCMMD*GNTK ɪ[ɪTime Limit Exceeded
8 6 4 2
(S)ωEF 60-
0 -
100	200	300	400	500	600	700
Max graph size
Figure 8: Computation time of algorithms on different sets of simulated data for four clusters case
demonstrating the scalability of each algorithm. Computation time is plotted in log scale.
C.7 Two-Sample Testing
In this section, we evaluate the efficacy of the proposed test T with different d(G1, G2) by varying
the graph sizes n. We consider n = {50, 100, 150} and fix n0 = 10 from the theoretical bound
for evaluating the test T . The power is computed using the test T for the significance level 0.05,
and the plots in Figure 9 show the average power computed over 500 trials of bootstrapping 100
samples generated from all pairs of graphons for d(G1, G2) as our proposed distance, log moments
24
Published as a conference paper at ICLR 2022
graph sizes (150, 300)
ωuujssδpωsodQJd sauωEO≡6o~∣
graph sizes (50,100)
graph sizes (100, 200)
WI H	0.07	1.00	1.00	1.00	WI H	0.06	1.00	1.00	1.00	WI H
W2 -	1.00	0.06	0.39	1.00	W2 -	1.00	0.06	0.97	1.00	W2 -
皿3 -	1.00	0.39	0.05	0.91	皿3 -	1.00	0.97	0.03	1.00	% -
W4 -	1.00	1.00	0.91	0.17		1.00	1.00	1.00	0.13 I	W4 -
1.00
1.00
1.00
0.04
1.00 1.00 1.00
0.08
1.00
1.00 1.00
0.05
1.00
1.00 1.00
0.13
w2 w3 w∕4
w1 -
W2
W4 -
w1 w2 w3 w∕4
0.02	0.69	0.03
0.85	0.04 0.12	1.00
0.69	0.12 0.04	0.96
0.03		0.03
W2
1.00 1.00
0.93
0.52
w1 w2 w3 w∕4
W1
w1 -
W4 -
W2 W3 VU4
0.04	0.99	0.90	0.05
0.99	0.03	0.26	1.00
0.90	0.26	0.05	1.00
0.05	1.00	1.00	0.03
w1 -∖
1.00 1.00 1.00
0.57
w2 - 1.00
w1 w2 w3 w∕4
0.05
0.99
0.05
w1 -∖
w1 -∖
w1 -∖
1.00 0.99
0.03
0.46
0.05
0.46
0.04
1.00 1.00
1.00
1.00
0.04
W1 W2 W3 VU4
0.70
1.00 1.00 1.00
w2 一
1.00
0.45 0.54
0.96
w2 - 1.00
0.64
0.47
1.00
w2 - 1.00
0.87
0.85
1.00
W4 -
1.00
0.54 0.51
0.87
1.00
0.47
0.51
1.00
1.00
0.85
0.98
1.00
0.93
0.96
0.87
0.61
W4 - 1.00 1.00 1.00
0.77
1.00 1.00 1.00 0.99
w1
W1 W2 W3 VU4
w1 w2 w3 w∕4
W1 W2 W3 VU4
Figure 9: Illustration of two-sample testing with the proposed distance vs log moments and MMD
on varying n in graph pairs of size (n, 2n). The plots show the average power of the test T. Test
based on the proposed distance is consistent for sufficiently large graphs and efficient compared to
other methods in distinguishing even closer graphons.
and MMD, respectively. From the result for graph sizes (50, 100), we observe that the graphon
pair (W2 , W3) is not easily distinguishable (low H0 rejection probability), which can be explained
by their respective L2 distance that is shown in the left plot of Figure 3. This issue does not arise
in testing larger graphs as the result shows for graph sizes (100, 200) and (150, 300). Therefore,
test T with the proposed distance can distinguish between pairs of graphons that are quite close
provided that the observed graphs are sufficiently large, thus proving to be consistent. On the other
hand, log moments and MMD based tests show weakness in distinguishing the graphons, where
log moments based test T accepts the null hypothesis in most cases even when the graphons are
different for all graph sizes. For instance, the result for graphon pair W1 and W4 is indistinguishable
using log moments statistic for any graph size. On the contrary, MMD based test T rejects the null
hypothesis almost always for larger graphs (diagonal values in all graph cases). Thus, we conclude
that the proposed test T in 5 is consistent and this experiment illustrates the efficiency of the test T
compared to other plausible test statistics.
Subsequently, We evaluate the efficacy of the above tests on the discussed real datasets - Bioinfor-
matics and Social Networks. We consider graphs from a dataset to belong to a population and hence
the objective of the test statistic is to distinguish graphs from different populations, that is, graphs
from tWo different datasets. Since the populations are not knoWn and the real graphs are treated as
representatives of the population, We compute p-value of the test instead of poWer to measure the
efficacy. The p-value is the evidence for rejecting the null hypothesis Which implies that the smaller
the p-value, the stronger the evidence that the null hypothesis should be rejected. Therefore, the p-
value should be high (greater than the significance level) for graphs from same population and loW
(' 0) for graphs from different populations. Figure 10 shoWs the result for both the dataset cases
and different tests. From the results, it is clear that the log moments and MMD based tests are poor
25
Published as a conference paper at ICLR 2022
Log Moments
sui3euj.lo',-u-o∞EOM*jωN -e-ɔos
Proposed Distance
Figure 10: Illustration of two-sample testing with the proposed distance vs log moments and MMD
on real datasets - Bioinformatics and Social Networks. The plots show the p-value of the test T .
Test based on the proposed distance is better than the other two tests.
空
Ei工台必£1
and inefficient on real datasets as log moments based test has high acceptance of null hypothesis for
almost all the pair of graphs from any population and MMD based test rejects the null hypothesis
always except when the graphs are the same. Whereas, the test using our proposed distance perform
well on large graphs from Social Networks datasets, for instance, S4 with other datasets and within
itself. This test performs well even for small graphs when compared to the other two tests.
26