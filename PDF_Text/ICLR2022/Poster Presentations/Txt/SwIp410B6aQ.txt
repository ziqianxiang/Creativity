Published as a conference paper at ICLR 2022
On the Role of Neural Collapse in Transfer
Learning
Tomer Galanti	Andras Gyorgy	Marcus Hutter
MIT	DeepMind	DeepMind
galanti@mit.edu	agyorgy@deepmind.com	mhutter@deepmind.com
Ab stract
We study the ability of foundation models to learn representations for classifi-
cation that are transferable to new, unseen classes. Recent results in the liter-
ature show that representations learned by a single classifier over many classes
are competitive on few-shot learning problems with representations learned by
special-purpose algorithms designed for such problems. In this paper, we pro-
vide an explanation for this behavior based on the recently observed phenomenon
that the features learned by overparameterized classification networks show an
interesting clustering property, called neural collapse. We demonstrate both theo-
retically and empirically that neural collapse generalizes to new samples from the
training classes, and - more importantly - to new classes as well, allowing foun-
dation models to provide feature maps that work well in transfer learning and,
specifically, in the few-shot setting.
1	Introduction
In a variety of machine learning applications, we have access to a limited amount of data from the
task that we would like to solve, as labeled data is oftentimes scarce and/or expensive. In such
scenarios, training directly on the available data is unlikely to produce a hypothesis that generalizes
well to new, unseen test samples. A prominent solution to this problem is to apply transfer learning
(see, e.g., Caruana, 1995; Bengio, 2012; Yosinski et al., 2014). In transfer learning, we are typically
given a large-scale source task (e.g., ImageNet ILSVRC, Russakovsky et al., 2015) and a target
task from which we encounter only a limited amount of data. While there are multiple approaches
to transfer knowledge between tasks, a popular approach suggests to train a large neural network
on a source classification task with a wide range of classes (such as ResNet50, He et al., 2016,
MobileNet, Howard et al., 2017 or the VGG network, Simonyan & Zisserman, 2014), and then to
train a relatively smaller network (e.g., a linear classifier or a shallow MLP) on top of the penultimate
layer of the pretrained network, using the data available in the target task.
Due to the effectiveness of this approach, transfer learning has become a central element in the
machine learning toolbox. For instance, using pretrained feature maps is common practice in a
variety of applications, including fine-grained classification (Chen et al., 2019a; Huang & Li, 2020;
Yang et al., 2018), object detection, (Redmon et al., 2016; Ren et al., 2015; He et al., 2017), semantic
segmentation (Long et al., 2015; Chen et al., 2018), or medical imaging (Chen et al., 2019b). In fact,
due to the cumulative success of transfer learning, large pretrained models that can be effectively
adapted to a wide variety of tasks (Brown et al., 2020; Ramesh et al., 2021) have recently been
characterized as foundation models (Bommasani et al., 2021), emphasizing their central role in
solving various learning tasks.
Typically, a foundation model is pretrained on a source task at a time when the concrete description
of the target task (or target tasks) is not - or only partially - available to the practitioner. Therefore,
the ability to precondition or design the training regime of the foundation model to match the target
task is limited. As a result, there might be a domain shift between the source and target tasks, such
as a different amount of target classes, or a different number of available samples. Hence, intuitively,
a foundation model should be fairly generic and applicable in a wide range of problems.
On the other hand, when some specifics of the target tasks are known, often special-purpose algo-
rithms are designed to utilize this information. Such an example is the problem of few-shot learning,
1
Published as a conference paper at ICLR 2022
when it is known in advance that the target problems come with a very small training set (Vinyals
et al., 2016; Ravi & Larochelle, 2017; Finn et al., 2017; Lee et al., 2019). While these specialized
algorithms have significantly improved the state of the art, the recent work of Tian et al. (2020);
Dhillon et al. (2020) demonstrated that predictors trained on top of foundation models can also
achieve state-of-the-art performance on few-shot learning benchmarks.
Despite the wide range of applications and success of transfer learning and foundation models in
particular, only relatively little is known theoretically why transfer learning is possible between two
tasks in the setting mentioned above.
Contributions. In this paper we present a new perspective on transfer learning with foundation
models, based on the phenomenon of neural collapse (Papyan et al., 2020). Informally, neural
collapse identifies training dynamics of deep networks for standard classification tasks, where the
features (the output of the penultimate layer) associated with training samples belonging to the same
class concentrate around their class feature mean. We demonstrate that this property generalizes to
new data points and new classes (e.g., the target classes), when the model is trained on a large set
of classes with many samples per class. In addition, we show that in the presence of neural col-
lapse, training a linear classifier on top of the learned penultimate layer can be done using only few
samples. We verify these findings both theoretically and via experiments. In particular, our results
provide a compelling explanation for the empirical success of foundation models, as observed, e.g.,
by Tian et al. (2020); Dhillon et al. (2020). In Appendix B.2, we show evidence for neural collapse
for feature maps obtained from lower layers of the network, as well, although to a lesser extent.
The rest of the paper is organized as follows: Additional related work is discussed in Section 1.1.
The problem setting, in particular, foundation models, are introduced in Section 2. Neural collapse is
described in Section 3. Our theoretical analysis is presented in Section 4: generalization to unseen
examples is considered in Section 4.1, to new classes in Section 4.2, while the effect of neural
collapse on the classification performance is discussed in Section 4.3. Experiments are presented in
Section 5, and conclusions are drawn in Section 6. Proofs and additional experiments are relegated
to the appendix.
1.1	Other Related Work
Various publications, such as Baxter (2000); Maurer et al. (2016); Pentina & Lampert (2014);
Galanti et al. (2016); Khodak et al. (2019); Du et al. (2021), suggested different frameworks to
theoretically study various multi-task learning settings. However, all of these works consider learn-
ing settings in which the learning algorithm is provided with a sequence of learning problems, and
therefore, are unable to characterize the case where only one source task is available.
In contrast, we propose a theoretical framework in which the learning algorithm is provided with one
classification task and is required to learn a representation of the data that can be adapted to solve
new unseen tasks using few samples. This kind of modeling is aligned with the common practice of
transfer learning using deep neural networks.
While in unsupervised domain adaptation (DA) (Ben-David et al., 2006; Mansour et al., 2009) one
typically has access to a single source and a single target task, one does not have access to labels from
the former, making the problem ill-posed by nature and shaping the algorithms to behave differently
than those that are provided with labeled samples from the target task. In particular, although in
transfer learning we typically train the feature map on the source dataset, independently of the target
task, in DA this would significantly limit the algorithm’s ability to solve the target task.
2	Problem Setup
We consider the problem of training foundation models, that is, general feature representations
which are useful on a wide range of learning tasks, and are trained on some auxiliary task. To
model this problem, we assume that the final target task we want to solve is a k-class classification
problem T (the target problem), coming from an unknown distribution D over such problems, and
the auxiliary task where the feature representation is learned on an l-class classification problem,
called the source problem. Formally, the target task is defined by a distribution P over samples
(x, y) ∈ X × Yk , where X ⊂ Rd is the instance space, and Yk is a label space with cardinality k .
To simplify the presentation, we use one-hot encoding for the label space, that is, the labels are rep-
resented by the unit vectors in Rk, and Yk = {ei : i = 1, . . . , k} where ei ∈ Rk is the ith standard
2
Published as a conference paper at ICLR 2022
unit vector in Rk ; with a slight abuse of notation, sometimes we will also write y = i instead of
y = ei. For a pair (x, y) with distribution P, we denote by Pi the class conditional distribution of x
given y = i (i.e., Pi(∙) = P[x ∈ ∙ | y = i]).
A classifier h : X → Rk assigns a soft label to an input point x ∈ X, and its performance on the
target task T is measured by the risk
LT(A) = E(χ,y)~p['(h(X), y)] ,	(I)
where ` : Rk × Yk → [0, ∞) is a non-negative loss function (e.g., zero-one or cross-entropy losses).
Our goal is to learn a classifier h from some training data S = {(xi, yi)}in=1 of n independent and
identically distributed (i.i.d.) samples drawn from P . However, when n is small and the classi-
fication problem is complicated, this might not be an easy problem. To facilitate finding a good
solution, we aim to find a classifier of the form h = g ◦ f, where f : Rd → Rp is a feature map
from a family of functions F ⊂ {f0 : Rd → Rp} and g ∈ G = {g0 : Rp → Rk } is a classifier used
on the feature space Rp . The idea is that the feature map f is learned on some other problem where
more data is available, and then g is trained to solve the hopefully simpler classification problem of
finding y based on f (x), instead of x. That is, g is actually a function of the modified training data
{(f (xi), yi)}in=1; to emphasize this dependence, we denote the trained classifier by gf,S.
We assume that the auxiliary (source) task helping to find f is an l-class classification problem over
the same sample space X, given by a distribution P, and here we are interested in finding a classifier
h : X → Rl of the form h = g ◦ f, where g ∈ G ⊂ {g0 : Rp → Rl} is a classifier over the feature
space f (X) = {f (x) : X ∈ X}. Given a training dataset S = {(χi,yi)}m=ι, both components of
the trained classifier, denoted by fg and gg are trained on S, With the goal of minimizing the risk in
the source task, given by
_ , ~	― - 一 ， , . .
LS(hS) = E(x,y)~P ['(9s(fS(x)),y)]
(note that with a slight abuse of notation we used the same loss function as in (1), although they
operate over spaces of different dimensions). A standard assumption is that S is drawn i.i.d. form
P; however, in this work instead we will use hierarchical schemes where first classes are sampled
according to their marginals in P , and then for any selected class c, training samples Sc are chosen
from the corresponding class-conditional distribution (or just class conditional, for short) Pc； the
specific assumptions on the distribution of S will be given when needed. Since f, g and h always
depend on the source data only, to simplify the notation we will often drop the S subscript whenever
this does not cause any confusion.
In a typical setting h is a deep neural network, f is the representation in the last internal layer of the
network (i.e., the penultimate embedding layer), and gg, the last layer of the network, is a linear map;
similarly g in the target problem is often taken to be linear. The learned feature map f is called a
foundation model (Bommasani et al., 2021) when it can be effectively used in a wide range of tasks.
In particular, its effectiveness can be measure by its expected performance over target tasks:
LD (f) = ET ~D Es~P n [Lτ(gS ◦ f )]
(recall that P is the distribution associated with task T).
Notice that while the feature map f is evaluated on the distribution of target tasks determined by D,
the training of f in a foundation model, as described above, is fully agnostic of this target. In con-
trast, several transfer learning methods (such as few-shot learning algorithms) have been developed
which optimize f not only as a function of its training data S, but also based on some properties of
D, such as the number of classes and the number of samples per class in S.
Perhaps surprisingly, recent studies (Tian et al., 2020; Dhillon et al., 2020) demonstrate that the
target-agnostic training of foundation models (or some slight variation of them, such as transductive
learning) are competitve with such special purpose algorithm. In the rest of the paper we analyze
this phenomenon, and provide an explanation through the recent concept of neural collapse.
Notation. For an integer k ≥ 1, [k] = {1,...,k}. k ∙ k stands for the Euclidean norm. For a set
A = {ai}in=1 ⊂ B and a function u ∈ U ⊂ {u0 : B → R}, we define u(A) = {u(a1), . . . , u(an)}
and U(A) = {u(A) : u ∈ U}. For a distribution Q over X ⊂ Rd and u : X → Rp, we denote by
3
Published as a conference paper at ICLR 2022
μu(Q) = Ex~q[u(x)] and by Varu(Q) = Ex~q[∣∣u(x) - μu(Q)k2] the mean and variance of u(x)
for X 〜 Q. For A = {ai}n=ι ⊂ R, We denote Avgn=Ja/ = ɪ Pn=ι ai. For a finite set A, We
denote by U[A] the uniform distribution over A.
3	Neural Collapse
Neural collapse (NC) is a recently discovered phenomenon in deep learning (Papyan et al., 2020;
Han et al., 2021): it has been observed that during the training of deep netWorks for standard clas-
sification tasks, the features (the output of the penultimate, a.k.a. embedding layer) associated With
training samples belonging to the same class concentrate around the mean feature value for the
same class. More concretely, Papyan et al. (2020) observed essentially that the ratio of the Within-
class variances and the distances betWeen the class means converge to zero. They also noticed that
asymptotically the class-means (centered at their global mean) are not only linearly separable, but
are actually maximally distant and located on a sphere centered at the origin up to scaling (they form
a simplex equiangular tight frame), and furthermore, that the behavior of the last-layer classifier (op-
erating on the features) converges to that of the nearest-class-mean decision rule.
On the theoretical side, already Papyan et al. (2020) shoWed the emergence of neural collapse for
a Gaussian mixture model and linear score-based classifiers. Poggio & Liao (2020b;a); Mixon
et al. (2020) theoretically investigated Whether neural collapse occurs When training neural netWorks
under MSE-loss minimization With different settings of regularization. Furthermore, Zhu et al.
(2021) shoWed that in certain cases, any global optimum of the classical cross-entropy loss With
Weight decay in the unconstrained features model satisfies neural collapse.
Papyan et al. (2020) defined neural collapse via identifying that the Within-class variance normalized
by the intra-class-covariance tends to zero toWards the end of the training (see Appendix B.4 for
details). In this paper, We Work With a slightly different variation of Within-class variation collapse,
Which Will be later connected to the clusterability of the sample feature vectors. For a feature map f
and tWo (class-conditional) distributions Q1, Q2 over X, We define their class-distance normalized
variance (CDNV) to be
Vf(Q1,Q2)
Varf(Q1) + Varf(Q2)
2∣∣μf(Qι)- μf(Q2)k2 .
For finite sets S1, S2 ⊂ X We define Vf(S1, S2) = Vf(U[S1], U [S2]). Our version of neural
collapse (at training) asserts that limt→∞ Avgi=j∈[t] [Vft (Si, Sj)] = 0. Intuitively, when decreasing
the variance of the features of a given class in comparison to its distance from another class, We
expect to be able to classify the features into the classes With a higher accuracy. This definition is
essentially the same as that of Papyan et al. (2020), making their empirical observations about neural
collapse valid for our definition (as also demonstrated in our experiments), but our neW definition
simplifies the theoretical analysis. Furthermore, the theoretical results of Zhu et al. (2021); Mixon
et al. (2020); Poggio & Liao (2020a;b) also identify settings in Which the feature map f induced by
critical points of the training loss satisfies Avgi6=j Vf(Si, Sj) = 0 (for large enough sample sizes).
4	Neural Collapse on Unseen Data
In this section We theoretically analyze neural collapse in the setting of Section 2, With particular
attention of variance collapse on data not used in the training of the classifier. First, in Section 4.1
We shoW that if neural collapse happens on the training set S, then it generalizes to unseen samples
from the same task under mild, natural assumptions (given the sample size is large enough). Next
We shoW, in Section 4.2, that one can also expect neural collapse to happen over neW classes When
the classes in the source and target tasks are selected randomly from the same class distributions
(given many source classes). The significance of these results is that they shoW that the feature
representations learned during training are immediately useful in other tasks; We shoW in Section 4.3
hoW this leads to loW classification error in feW-shot learning.
4.1	Generalization to New Samples from the Same Class
In this section, We provide a generalization bound on the CDNV, Vf (Pi, Pj), betWeen tWo source
class-conditional distributions Pi and Pj in terms of its empirical counterpart, Vf(Si, Sj), and cer-
4
Published as a conference paper at ICLR 2022
tain generalization gap terms bounding the difference between expectations and empirical averages
for f and its variants, where f is the output of the learning algorithm with access to S. Assume that
for all c ∈ [l], Sc is a set ofmc i.i.d. samples drawn from Pc, and for any δ ∈ (0, 1), let 1(Pc, mc, δ)
and 2(Pc, mc, δ) be the smallest positive values such that with probability at least 1 -δ, the learning
algorithm returns a function f ∈ F that satisfies
IIEx 〜Pcf(X)] - Avgχ∈Sc [f (x)]∣∣ ≤ eι(Pc,mc,δ) =: 6⑷；
IEx〜Pjkf(x)k2] — Avgx∈Sc[kf(x)k2]∣ ≤ e2(Pc,mc,δ) =: e2(δ).
Typically, these quantities can be upper bounded using Rademacher complexities (Bartlett &
Mendelson, 2002) related to F, scaling usually as O(vzlog(1∕δ)∕mc) (for a fixed c), as We show in
Proposition 3 (in Appendix E) for ReLU neural networks with bounded weights. Next, we present
our bound on the CDNV of the source distributions; the proof is relegated to Appendix C.
Proposition 1. Fix two source classes, i and j with distributions Pi and Pj, and let δ ∈ (0, 1). Let
Sc 〜Pm for c ∈ {i, j} .Let
e1(δ∕4) + e1(δ∕4)	Avgc∈{i,j} ∣⅛0∕4"2kμf(Pc)卜咨0/4)+ 前仞明
a = ----------------- and B =----------------------------------------------------.
kμf (Pi) - μf (Pj )k	kμf (Si) - μf (Sj )k2
2
Then, with probability at least 1 - δ over S, we have Vf (Pi , Pj) ≤ (Vf (Si , Sj) + B) (1 + A) .
Writing out the bracket, the bound in the proposition has several terms. The first one is the empirical
CDNV Vf (Si, Sj) between the two datasets Si 〜Pmi and Sj 〜Pmj. This term is assumed to be
small by the neural collapse phenomenon during training. The rest of the terms are proportional to
the generalization gaps ec(δ∕4) and ec(δ∕4) — as discussed above, typically we expect these terms
to scale as O(,log(1∕δ)∕mc). In addition, according to the theoretical analyses of neural collapse
available in the literature (Papyan et al., 2020; Mixon et al., 2020; Zhu et al., 2021; Rangamani et al.,
2021), under certain conditions the function f converges to a solution for which {μf (Si)}i=ι form a
simplex equiangular tight frame (ETF), that is, after centering their global mean, μf (Si) are of equal
length, and ∣∣μf (Si) - μf(Sj)k are also equal and maximized for all i = j. This implies that if f is
properly normalized, the distance ∣∣μf(Si) - μf(Sj)k is lower bounded by a constant, and hence A
and B indeed are small (in Appendix B.3 we demonstrate empirically that the minimum distances
of the class means are indeed not too small in the scenarios we consider). Finally, we also note that
with probability 1 - δ, we have ∣μf(Pi) - μf(Pj)∣∣ ≥ ∣μf(Si) - μf(Sj)∣∣- 4(δ∕2) - 4(δ∕2).
Hence, if mi and mj are large enough, ∣μf(Pi) - μf(Pj-)∣ is larger than a constant with high
probability. Therefore, assuming mi, mj are large, ifVf(Si, Sj) is small, then we expect Vf(Pi, Pj)
to also be small. That is, if neural collapse emerges in the training data of two source classes, we
should also expect it to emerge in unseen samples of the same classes.
4.2	Neural Collapse Generalizes to New Classes
Previously, we showed that if the CDNV is minimized by the learning algorithm on the training data,
we expect it to be small for unseen source samples. As a next step, we show that if neural collapse
emerges in the set of source classes, we can also expect it to emerge in new, unseen target classes.
To analyze this scenario, we essentially treat class-conditional distributions as data points on which
the feature map f is trained (in a noisy manner, depending on the actual samples), and apply stan-
dard techniques to derive generalization bounds to new data points, which, in this case, are class-
conditional distributions. Accordingly, we assume that both the source and the target classes come
from a distribution over DC over a set of classes C. Each class is represented by its class-conditional
distribution, and hence, with a slight abuse of notation, we say that the source class-conditional dis-
tributions P = {Pi}li=1 are selected according to DC(P1, . . . , Pl | Pi 6= Pj for all i 6= j ∈ [l]).
Below we show that if neural collapse emerges in the source classes, it also emerges in unseen target
classes C = c0 with class-conditional distributions Pc, P co 〜DC (Pc, Pc，| Pc = P co) in the sense
that Vf(Pc, Pc0) is expected to be small. Our bound below applies a uniform convergence argument
based on Rademacher complexities. The Rademacher complexity of a set Y ⊂ Rn is defined as
5
Published as a conference paper at ICLR 2022
R(Y) = Ee supy∈γ [he, yi], where C = (6ι,..., en)〜 U[{±1}n]. For a given feature map f ∈ F,
We also define a mapping Hf : C → Rp+1 as Hf(Pc) = (μf (Pc), Varf(Pc)), and for a class of
functions F*, we define Hf* (P) = {(Hf(Pc))c=ι : f ∈ F*}.
Proposition 2. Let F* ⊂ F be any finite set of functions with ∆(F*)=
inff∈F* inf Pc 6=Pc0 kμf (Pc) - μf (Pc0 )k > 0. Then, with probability at least 1 - δ over the
selection of source class distributions P,
/	16 SuP	Varf(P 0)∖	,------ 〜
EPc=Pc,[Vf (Pc,Pc0)] ≤ Avgi=hvf(Pi,Pj )i + ∣ 8+	f f∈*∆F.)——Pfgf∆HL
L	」〈	ʌʌ(ʃ ) y	(lg 1) ʌʌ(ʃ )
Zl	II" M∣λ 2PlOg(I那)∙ SuP Varf(PO)
4suPx∈X, f ∈F* kf (X)k ʌ _____f ∈F*,P0∈C_____
∆(F*)	J	√ ∙ ∆(F*)2	.
The above proposition, proved in Appendix D (based on Corollary 3 of Maurer & Pontil, 2019),
provides an upper bound on the discrepancy between the expected value of EPc 6=Pc0 [Vf (Pc, Pc0 )]
and the averaged value Avgi6=j[Vf (Pi, Pj)] across a set of source distributions P1, . . . , Pl that were
sampled from DC . In addition, we treat F* as a set of candidate functions from which the learning
algorithm selects its candidates. Similarly to Corollary 3 of Maurer & Pontil (2019), this set is
assumed to be finite only for technical (measurability) reasons.
The proposition shows that if Avgi6=j [Vf (Pi, Pj)] is small, we expect EPc 6=Pc0 [Vf (Pc, Pc0 )] to be
small, if the source data is representative enough, as discussed below. First note that by Proposi-
tion 1, each term Vf (Pi , Pj ) is expected to be small in the presence of neural collapse (for large
enough training sets Sc), so their average Avgi6=j [Vf (Pi, Pj)] is also small. According to Proposi-
tion 4 in Appendix E,the Rademacher complexity in the bound scales as O(p√7) for ReLU networks
with bounded weights, so the corresponding term in the bound is O(p∕(√7∆(F*)2)), which is of
similar order as the last term (neglecting the supremum terms and the dependence on p for now).
Hence, the bound tends to zero as the number of source classes l increases as long as ∆(F*) is
not too small (in particular, is not zero). While there is no way to guarantee this (e.g., if two class-
conditional distributions are too close, ∆(F*) can be very small), we can say that if the feature maps
f ∈ F* keep the classes apart, the bound may become reasonably small. If the feature dimension
p is large and f ∈ F* are similar to random maps, this assumption typically holds (e.g., if f is
not a constant function, which can be expected with a proper training method). Even when the
number of classes is very large, the bound can be reasonable: if the corresponding feature means
are more-or-less uniformly distributed in a p-dimensional unit cube for all f ∈ F*, ∆(F*) =
Ω(√p ∙ |C|-2/p) (see Lemma 2 in Appendix F), and hence combining with the dependence of the
Rademacher complexity on l andp, and the fact that Varf(P0) ≤ P and supχ,f Ilf (x)k ≤ √p, the
bound is O(ʌ/p/l ∙ |C|6/p) which tends to zero as l increases.
4.3 CDNV and Classification Error
Next, we study the relation between the CDNV and the classification performance it induces.
Consider a balanced k-class classification problem with class distributions P1 , . . . , Pk, datasets
Sc 〜Pnc, C ∈ [k] and nι = ... = n. Let S = ∪c∈[k]Sc denote all the samples and consider
the ‘nearest empirical mean’ classifier hS,f (x) = arg minc∈[k] If (x) - μf (Sc)I (note that hS,f is a
classifier with convex polytope cells on top of f, and it is a linear classifier when k = 2). Then, as it
is shown in Proposition 5 in Appendix G the expected classification error of hS,f is upper bounded
by 16(k - 1) Avgi6=j Vf(Pi, Pj)(1 + 1∕nc) and iff ◦ P1 and f ◦ P2 are spherically symmetric, then
the classification error is upper bounded by 16(k - 1) Avgi6=j Vf(Pi, Pj)(1∕p + 1∕nc). Therefore,
in case of neural collapse (i.e., small Avgi6=j Vf (Pi, Pj)), the error probability is small, explaining
the success of foundation models in the low-data regime (such as in few-shot learning problems) in
the presence of neural collapse. Putting together Propositions 1 and 2, and Proposition 5, it follows
that, with high probability (over the selection of the source, as well as the source samples), the ex-
pected error (over the target classes and target samples) of a nearest class-mean classifier over the
target classes can be bounded by the average neural collapse over the source training samples plus
6
Published as a conference paper at ICLR 2022
Method	Architecture	Mini-ImageNet		CIFAR-FS		FC-100	
		1-shot	5-shot	1-shot	5-shot	1-shot	5-shot
Matching Networks (Vinyals et al., 2016)	64-64-64-64	43.56 ± 0.84	55.31 ± 0.73				
LSTM Meta-Learner (Ravi & Larochelle, 2017)	64-64-64-64	43.44 ± 0.77	60.60 ± 0.71				
MAML (Finn et al., 2017)	32-32-32-32	48.70 ± 1.84	63.11 ± 0.92	58.9 ± 1.9	71.5 ± 1.0		
Prototypical Networks (Snell et al., 2017)	64-64-64-64	49.42 ± 0.78t	68.20 ± 0.66t	55.5 ± 0.7	72.0 ± 0.6	35.3 ± 0.6	48.6 ± 0.6
Relation Networks (Sung et al., 2018)	64-96-128-256	50.44 ± 0.82	65.32 ± 0.7	55.0 ± 1.0	69.3 ± 0.8		
SNAIL (Mishra et al., 2018)	ResNet-12	55.71 ± 0.99	68.88 ± 0.92				
TADAM (Oreshkin et al., 2018)	ResNet-12	58.50 ± 0.30	76.7 ± 0.3			40.1 ± 0.4	56.1 ± 0.4
AdaResNet (Munkhdalai et al., 2018)	ResNet-12	56.88 ± 0.62	71.94 ± 0.57				
Dynamics Few-Shot (Gidaris & Komodakis, 2018)	64-64-128-128	56.20 ± 0.86	73.0 ± 0.64				
Activation to Parameter (Qiao et al., 2018)	WRN-28-10	59.60 ± 0.4lt	73.74 ± 0.19t				
R2D2 (Bertinetto et al., 2019)	96-192-384-512	51.2 ± 0.6	68.8 ± 0.1	65.3 ± 0.2	79.4 ± 0.1		
Shot-Free (Ravichandran et al., 2019)	ResNet-12	59.04 ± n/a	77.64 ± n/a	69.2 ± n/a	84.7 ± n/a		
TEWAM Qiao et al. (2019)	ResNet-12	60.07 ± n/a	75.90 ± n/a	70.4 ± n/a	81.3 ± n/a		
TPN (Liu et al., 2019)	ResNet-12	55.51 ± 0.86	75.64 ± n/a				
LEO (Rusu et al., 2019)	WRN-28-10	61.76 ± 0.08t	77.59 ± 0.12t				
MTL (Sun et al., 2019)	ResNet-12	61.20 ± 1.80	75.50 ± 0.80				
OptNet-RR (Lee et al., 2019)	ResNet-12	61.41 ± 0.61	77.88 ± 0.46	72.6 ± 0.7	84.3 ± 0.5	40.5 ± 0.6	57.6 ± 0.9
MetaOptNet (Lee et al., 2019)	ResNet-12	62.64 ± 0.61	78.63 ± 0.46	72.0 ± 0.7	84.2 ± 0.5	41.1 ± 0.6	55.3 ± 0.6
Transductive Fine-Tuning (Dhillon et al., 2020)	WRN-28-10	65.73 ± 0.68	78.40 ± 0.52	76.58 ± 0.68	85.79 ± 0.5	43.16 ± 0.59	57.57 ± 0.55
Distill-simple (Tian et al., 2020)	ResNet-12	62.02 ± 0.63	79.64 ± 0.44	71.5 ± 0.8	86.0 ± 0.5	42.6 ± 0.7	59.1 ± 0.6
Distill (Tian et al., 2020)	ResNet-12	64.82 ± 0.60	82.14 ± 0.43	73.9 ± 0.8	86.9 ± 0.5	44.6 ± 0.7	60.9 ± 0.6
Ours (simple)	WRN-28-4	58.12 ± 1.19	72.0 ± 0.99	68.81 ± 1.20	81.49 ± 0.98	44.96 ± 1.14	57.21 ± 10.89
Ours (lr scheduling)	WRN-28-4	60.37 ± 1.25	72.35 ± 0.99	70.0 ± 1.29	81.39 ± 0.96	43.42 ± 1.0	54.14 ± 1.1
Ours (lr scheduling + model selection)	WRN-28-4	61.27 ± 1.14	74.74 ± 0.76	72.37 ± 1.12	82.94 ± 0.89	45.81 ± 1.27	56.85 ± 1.30
Table 1: Comparison to prior work on Mini-ImageNet, CIFAR-FS, and FC-100 on 1-shot and
5-shot 5-class classification. Reported numbers are test target classification accuracy of various
methods for various data sets. The notation a-b-c-d denotes a 4-layer convolutional network with a,
b, c, and d filters in each layer. *The algorithm was trained on both train and validation classes. We
took the data from Dhillon et al. (2020) and Tian et al. (2020).
terms which converge to zero as the number of source training samples mc, as well as the number
of source classes l increase.
5	Experiments
In this section we experimentally analyze the neural collapse phenomenon and how it generalizes to
new data points and new classes. We use reasonably good classifiers to demonstrate that, in addition
to the neural collapse observed in training time by Papyan et al. (2020), it is also observable on test
data from the same classes, as well as on data from new classes, as predicted by our theoretical re-
sults. We also show that, as expected intuitively, neural collapse is strongly correlated with accuracy
in few-shot learning scenarios. The experiments are conducted over multiple datasets and multiple
architectures, providing strong empirical evidence that neural collapse provides a compelling ex-
planation for the good performance of foundation models in few-shot learning tasks. Experimental
results are reported averaged over 20 random initialization together with 95% confidence intervals.
5.1	Setup
Method. Following our theoretical setup, we first train a neural network classifier h = g ◦ f
on a source task. Then we evaluate the few-shot performance of f on target classification tasks
by training a new classifier h = g ◦ f, by minimizing the cross-entropy loss between its logits
and the one-hot encodings of the labels. The training is conducted using SGD with learning rate
η and momentum 0.9 with batch size 64. Here, g is the top linear layer of the neural network and
f is the mapping implemented by all other layers. At the second stage, given a target few-shot
classification task with training data S = {(xi, yi)}Z=∖, We train a new top layer g as a solution
of ridge regression acting on the dataset {(f (Xi), yi)}n=ι with regularization λn = α√n. Thus, g
is a linear transformation with the weight matrix wS,f = (f(X)>f(X) + λnI)-1f(X)>Y , where
f(X) is then× d data matrix for the ridge regression problem containing the feature vectors {f(xi)}
(i.e., f(X)> = [f (x1), . . . , f(xn )]) and Y is then × k label matrix (i.e., Y > = [y1, . . . , yn]), where
X ∈ Rn×d, Y ∈ Rn×k and fθ(X) ∈ Rn×p. We did not apply any form of fine-tuning for f at the
second stage. In the experiments we sample 5-class classification tasks randomly from the target
dataset, with nc training samples for each class (thus, altogether n = 5nc above), and measure the
performance on 100 random test samples from each class. We report the resulting accuracy rates
averaged over 100 randomly chosen tasks.
Architectures and hyperparameters. We experimented with two types of architectures for h:
wide ResNets (Zagoruyko & Komodakis, 2016) and vanilla convolutional networks of the same
structure without the residual connections, denoted by WRN-N -M and Conv-N -M, where N is the
7
Published as a conference paper at ICLR 2022
5 '
10
20
30
40
50
60
Iteizitlon
WRN-28-4 on CIFAR-FS
>z□υ
2"仆
4«	41 4» 4» 4« 4 4' 4, 4β 4，4™ 户
ItEratIon
>z□υ
—5
——10
—20
—30
——40
—50
SO
Iteration
一5
—10
—20
—30
——40
—50
60
0.70
0.65
产
0.55
0.5β
0.45
一5
一10
—20
—30
——40
—50
«0
Conv-28-4 on CIFAR-FS
4。	41 4» 4» 4« 4» 4∙ V 4» 4» 4" 4"
IteratiQn



WRN-28-4 on Mini-ImageNet
—5
—10
—15
一20
一25
一30
—35
4。	41 4z 45 4∙ 45 4' 4, 4β 4, 4u 4=	40 4x e 4' 4, 45 4, 4, 4β 4, 4™ 4= 4° 4x e 4' 4∙ 45 4, 4, 4β 4, 4m 4" 4" 41 # 4' 4∙ 45 4, 47 4" 4' 4u 4"
nerMk>π	IterMIon	Iteration	Ke ration
Conv-16-2on EMNIST
(a) train CDNV	(b) test CDNV	(c) target CDNV	(d) target accuracy
Figure 1: Within-class variation collapse. (a) CDNV on the source training data; (b) CDNV
over the source test data; (c) CDNV over the target classes, all plotted in log-log scale. (d) Target
accuracy rate (lin-log scale). In each experiment we trained the model on a different number of
source classes l ∈ {5, 10, 20, 30, 40, 50, 60} (as indicated in the legend).
depth and M is the width factor. We used the following default hyperparameters: η = 2-4, batch
size 64 and α = 1. In Figs. 2-3 of the appendix we provide experiments showing that the results are
consistent for different η. We also provide experiments with a standard learning-rate schedule. See
Appendix A for a complete description of the architectures.
Datasets. We consider four different datasets: (i) Mini-ImageNet (Vinyals et al., 2016); (ii)
CIFAR-FS (Bertinetto et al., 2019); (iii) FC-100 (Oreshkin et al., 2018); and (iv) EMNIST (bal-
anced) (Cohen et al., 2017). For a complete description of the datasets, see Appendix A.
5.2	Results
While the ridge regression method to train the few-shot classifier g may seem simplistic, it provides
reasonably good performance, and hence it is suitable for studying the success of recent trans-
fer/transductive learning methods (Dhillon et al., 2020; Tian et al., 2020) for few-shot learning. To
demonstrate this, we compared the 1 and 5-shot performance of our simplistic method to several
few-shot learning algorithms on Mini-ImageNet, CIFAR-FS and FC-100, summarized in Table 1.
On each dataset, we report the average performance of our method on epochs between 90 and 100.
As can be seen, the method we study in this paper is competitive with the rest of the literature on the
8
Published as a conference paper at ICLR 2022
three benchmarks, especially in the 1-shot case (even achieving the state of the art on FC-100).1 To
improve the performance of our method a bit, we employed a standard learning rate scheduling with
initial learning rate η = 0.05, decayed twice by a factor 0.1 after 30 epochs each (accuracy rates
are reported averaging over epochs 90-100, as before). Since the performance of these networks
plateaued slightly after the first learning rate decay on the source test data, we also applied model
selection based on this information, and used the network from the first 60 epochs (to avoid over-
fitting to the source data and classes happening with the smallest learning rate) with the best source
test performance. The combination of these two variations typically resulted in a small improvement
of a few percentage points in the problems considered, see the last line of Table 1.
As our main experiment, we validate the theoretical assessments we made in Section 4. We argued
that in the presence of neural collapse on the training classes, the trained feature map can be used for
training a classifier with a small number of samples on new unseen classes. The argument asserts that
if we observe neural collapse on a large set of source classes, then we expect to have neural collapse
on new unseen classes as well, when assuming that the classes are selected in an i.i.d. manner. In
this section we demonstrate that neural collapse generalizes to new samples from the same classes,
and also to new classes, and we show that it is correlated with good few-shot performance.
To validate the above, we trained classifiers h with a varying number of l randomly selected source
classes. For each run, we plot in Figure 1 the CDNV as a function of the epoch for the training and
test datasets of the source classes and over the test samples of the target classes. In addition, we plot
the 5-shot accuracy of ridge regression using the learned feature map f . Similar experiments with
different numbers of target samples are reported in Figures 4 and 5 in the appendix.
As it can be seen in Figure 1, the CDNV decreases over the course of training on both training
and test datasets of the source classes, showing that neural collapse generalizes to new samples
from the training classes. Since the classification tasks with fewer number of source classes are
easier to learn, the CDNV tends to be larger when training with a wider set of source classes. In
contrast, we observe that when increasing the number of source classes, the presence of neural
collapse in the target classes strengthens. This is in alignment with our theoretical expectations (the
more “training” classes, the better generalization to new classes), and the few-shot performance also
consistently improves when the overall number of source classes is increased. Note that in practice
the CDNV does not decrease to zero (due to the limited sample sizes), and plateaus at a value above
1 on the target classes since we use a relatively small number of source classes (at most 60 here and
64 in the appendix). To validate the generality of our results, this phenomenon is demonstrated in
several settings, e.g., using different network architectures and datasets in Figure 1. As can be seen,
the values of the CDNV on the target classes are relatively large compared to those on the source
classes, except for the results on EMNIST. However, these values still indicate a reasonable few-
shot learning performance, as demonstrated in the experiments. These results consistently validate
our theoretical findings, that is, that neural collapse generalizes to new source samples, it emerges
for new classes, and its presence immediately facilitates good performance in few-shot learning.
In Appendix B.2, we also demonstrate that similar phenomena can be observed for feature maps
obtained from lower layers of the network, as well, although to a lesser extent.
6	Conclusions
Employing foundation models for transfer learning is a successful approach for dealing with over-
fitting in the low-data regime. However, the reasons for this success are not clear. In this paper we
presented a new perspective on this problem by connecting it to the newly discovered phenomenon
of neural collapse. We showed that the within-class variance collapse tends to emerge in the test data
associated with the classes encountered at train time and, more importantly, in new unseen classes
when the new classes are drawn from the same distribution as the training classes. In addition, we
showed that when neural collapse emerges in the new classes, then it requires very few samples to
train a linear classifier on top of the learned feature representation that accurately predicts the new
classes. These results provide a justification to the recent successes of transfer learning in few-shot
tasks, as observed by Tian et al. (2020) and Dhillon et al. (2020).
1The inferior performance of our method compared to Distill-simple (Tian et al., 2020), is most likely due
to the choice of the final few-shot classifier: our ridge regression classifier is inferior to their logistic regression
solution (as shown in Table 1), while it is superior compared to their nearest neighbor classifier (see Table 5 in
their paper).
9
Published as a conference paper at ICLR 2022
Acknowledgements
During this work, Tomer Galanti was a research scientist intern at DeepMind. The authors would like
to thank Csaba Szepesvari and Razvan Pascanu for illuminating discussions during the preparation
of this manuscript, and Miruna P^slar for her priceless technical support.
References
Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. J. Mach. Learn. Res., 3:463-482, 2002.
Peter L. Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Proceedings of the 31st International Conference on Neural Information
Processing Systems, NIPS’17, pp. 6241-6250, Red Hook, NY, USA, 2017. Curran Associates
Inc. ISBN 9781510860964.
Jonathan Baxter. A model of inductive bias learning. J. Artif. Int. Res., 12(1):149-198, March 2000.
Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations
for domain adaptation. In Advances in Neural Information Processing Systems, pp. 137-144,
2006.
Yoshua Bengio. Deep learning of representations for unsupervised and transfer learning. In Pro-
ceedings of ICML Workshop on Unsupervised and Transfer Learning, volume 27 of Proceedings
of Machine Learning Research, pp. 17-36, Bellevue, Washington, USA, 02 Jul 2012. PMLR.
Luca Bertinetto, Joao F. Henriques, Philip Torr, and Andrea Vedaldi. Meta-learning with differen-
tiable closed-form solvers. In International Conference on Learning Representations, 2019.
Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,
Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson,
Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie S. Chen, Kathleen Creel,
Jared Quincy Davis, Dorottya Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Ste-
fano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren
Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto,
Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard,
Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte
Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, and
et al. On the opportunities and risks of foundation models. CoRR, abs/2108.07258, 2021.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information
Processing Systems, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020.
Rich Caruana. Learning many related tasks at the same time with backpropagation. In Advances in
Neural Information Processing Systems, volume 7. MIT Press, 1995.
Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and Jonathan K Su. This looks
like that: Deep learning for interpretable image recognition. In Advances in Neural Information
Processing Systems, volume 32. Curran Associates, Inc., 2019a.
Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-
decoder with atrous separable convolution for semantic image segmentation. In ECCV (7), volume
11211 of Lecture Notes in Computer Science, pp. 833-851. Springer, 2018.
Sihong Chen, Kai Ma, and Yefeng Zheng. Med3d: Transfer learning for 3d medical image analysis.
CoRR, abs/1904.00625, 2019b.
10
Published as a conference paper at ICLR 2022
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre van Schaik. Emnist: an extension of
mnist to handwritten letters. arXiv preprint arXiv:1702.05373, 2017.
Guneet Singh Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A baseline for
few-shot image classification. In International Conference on Learning Representations, 2020.
Simon Shaolei Du, Wei Hu, Sham M. Kakade, Jason D. Lee, and Qi Lei. Few-shot learning via
learning the representation, provably. In International Conference on Learning Representations,
2021.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In Proceedings of the 34th International Conference on Machine Learning,
volume 70 of Proceedings of Machine Learning Research, pp. 1126-1135. PMLR, 06-11 Aug
2017.
Tomer Galanti, Lior Wolf, and Tamir Hazan. A theoretical framework for deep transfer learning.
Information and Inference: A Journal of the IMA, 5:159-209, 2016.
Spyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In
Proceedings ofthe IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. Information and Inference: A Journal of the IMA, 9, 12 2017. doi: 10.1093/
imaiai/iaz007.
X. Y. Han, Vardan Papyan, and David L. Donoho. Neural collapse under mse loss: Proximity to and
dynamics on the central path, 2021.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
770-778, 2016. doi: 10.1109/CVPR.2016.90.
Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In Proceedings of the
IEEE International Conference on Computer Vision (ICCV), Oct 2017.
Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for
mobile vision applications, 2017.
Zixuan Huang and Yin Li. Interpretable and accurate fine-grained recognition via region grouping.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
8662-8672, 2020.
Sham Kakade and Ambuj Tewari. Learning theory, 2008.
Ekatherina A. Karatsuba. On the asymptotic representation of the euler gamma function by ramanu-
jan. J. Comput. Appl. Math., 135(2):225-240, October 2001.
Mikhail Khodak, Maria-Florina F Balcan, and Ameet S Talwalkar. Adaptive gradient-based meta-
learning methods. In Advances in Neural Information Processing Systems, volume 32. Curran
Associates, Inc., 2019.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University
of Toronto, 05 2012.
Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with
differentiable convex optimization. In CVPR, 2019.
Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sungju Hwang, and Yi Yang.
Learning to propagate labels: Transductive propagation network for few-shot learning. In Inter-
national Conference on Learning Representations, 2019.
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic
segmentation. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 3431-3440, 2015. doi: 10.1109/CVPR.2015.7298965.
11
Published as a conference paper at ICLR 2022
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds
and algorithms. In Proceedings of The 22nd Annual Conference on Learning Theory (COLT
2009), Montreal, Canada, 2009.
Andreas Maurer and M. Pontil. Uniform concentration and symmetrization for weak interactions.
In COLT, 2019.
Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The benefit of multitask
representation learning. J. Mach. Learn. Res., 17(1):2853-2884, January 2016.
Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-
learner. In International Conference on Learning Representations, 2018.
Dustin G. Mixon, Hans Parshall, and Jianzong Pi. Neural collapse with unconstrained features,
2020.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning.
Adaptive Computation and Machine Learning. MIT Press, Cambridge, MA, 2nd edition, 2018.
Tsendsuren Munkhdalai, Xingdi Yuan, Soroush Mehri, and Adam Trischler. Rapid adaptation with
conditionally shifted neurons. In Proceedings of the 35th International Conference on Machine
Learning, volume 80 of Proceedings of Machine Learning Research, pp. 3664-3673. PMLR,
2018.
Boris Oreshkin, Pau Rodrlguez Lopez, and Alexandre Lacoste. Tadam: Task dependent adaptive
metric for improved few-shot learning. In Advances in Neural Information Processing Systems,
volume 31. Curran Associates, Inc., 2018.
Vardan Papyan, X. Y. Han, and David L. Donoho. Prevalence of neural collapse during the terminal
phase of deep learning training. Proceedings of the National Academy of Sciences, 117(40):
24652-24663, 2020.
Anastasia Pentina and Christoph Lampert. A pac-bayesian bound for lifelong learning. In Proceed-
ings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of
Machine Learning Research, pp. 99l-999, Bejing, China, 22-24 Jun 2014. PMLR.
T. Poggio and Q. Liao. Implicit dynamic regularization in deep networks. Technical report, Center
for Brains, Minds and Machines (CBMM), 2020a.
Tomaso Poggio and Qianli Liao. Explicit regularization and implicit bias in deep network classifiers
trained with the square loss, 2020b.
Tomaso Poggio, Andrzej Banburski, and Qianli Liao. Theoretical issues in deep networks. Proceed-
ings of the National Academy of Sciences, 117(48):30039-30045, 2020.
Limeng Qiao, Yemin Shi, Jia Li, Yaowei Wang, Tiejun Huang, and Yonghong Tian. Transductive
episodic-wise adaptive metric for few-shot learning. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision (ICCV), October 2019.
Siyuan Qiao, Chenxi Liu, Wei Shen, and Alan L. Yuille. Few-shot image recognition by predicting
parameters from activations. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2018.
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,
and Ilya Sutskever. Zero-shot text-to-image generation. CoRR, abs/2102.12092, 2021.
Akshay Rangamani, Mengjia Xu, Andrzej Banburski, Qianli Liao, and Tomaso Poggio. Dynamics
and neural collapse in deep classifiers trained with the square loss. Technical report, Center for
Brains, Minds and Machines (CBMM), 2021.
S. Ravi and H. Larochelle. Optimization as a model for few-shot learning. In International Confer-
ence on Learning Representations, 2017.
12
Published as a conference paper at ICLR 2022
Avinash Ravichandran, Rahul Bhotika, and Stefano Soatto. Few-shot learning with embedded class
models and shot-free meta training. 2019 IEEE/CVF International Conference on Computer
Vision (ICCV),pp. 331-339, 2019.
Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified,
real-time object detection. In Proceedings of the IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), June 2016.
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object
detection with region proposal networks. In Advances in Neural Information Processing Systems,
volume 28. Curran Associates, Inc., 2015.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV), 115(3):211-252, 2015.
Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero,
and Raia Hadsell. Meta-learning with latent embedding optimization. In International Conference
on Learning Representations, 2019.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. CoRR, abs/1409.1556, 2014.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.
Qianru Sun, Yaoyao Liu, Tat-Seng Chua, and Bernt Schiele. Meta-transfer learning for few-shot
learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.
Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H.S. Torr, and Timothy M. Hospedales.
Learning to compare: Relation network for few-shot learning. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B. Tenenbaum, and Phillip Isola. Rethinking few-
shot image classification: A good embedding is all you need? In Proceedings of the European
Conference on Computer Vision (ECCV), volume 12359 of Lecture Notes in Computer Science,
pp. 266-282. Springer, 2020.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, koray kavukcuoglu, and Daan Wierstra. Match-
ing networks for one shot learning. In Advances in Neural Information Processing Systems,
volume 29. Curran Associates, Inc., 2016.
Ze Yang, Tiange Luo, Dong Wang, Zhiqiang Hu, Jun Gao, and Liwei Wang. Learning to navigate
for fine-grained classification. In Proceedings of the European Conference on Computer Vision
(ECCV), September 2018.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep
neural networks? In Advances in Neural Information Processing Systems, volume 27. Curran
Associates, Inc., 2014.
Sergey Zagoruyko and N. Komodakis. Wide residual networks. ArXiv, abs/1605.07146, 2016.
Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu. A
geometric analysis of neural collapse with unconstrained features, 2021.
13
Published as a conference paper at ICLR 2022
A	Experimental Details
Datasets. Throughout the experiments, we consider four different datasets: (i) Mini-
ImageNet (Vinyals et al., 2016) and (ii) CIFAR-FS (Bertinetto et al., 2019), (iii) FC-100 (Oreshkin
et al., 2018) and EMNIST (balanced) (Cohen et al., 2017). Each dataset is split into meta-train, meta-
validation and meta-test classes; we select the data for the source classes from the meta-training, and
use similarly the meta-test data for the target tasks (we do not use the meta-validation classes). Each
one of the class splits is also partitioned into train and test samples; we use these for training and
evaluating our models. The Mini-ImageNet dataset contains 100 classes randomly chosen from Im-
ageNet ILSVRC-2012 (Russakovsky et al., 2015) with 600 images of size 84 × 84 pixels per class.
It is split into 64 meta-training classes, 16 meta-validation classes and 20 classes for meta-testing.
CIFAR-FS and FC-100 are two derivatives of the CIFAR-100 dataset (Krizhevsky, 2012). CIFAR-
FS consists of a random split of the CIFAR-100 classes into 64 classes for meta-training, 14 for
meta-validation and 20 for meta-testing. FC-100 contains 100 classes which are grouped into 20 su-
perclasses. These classes are partitioned into 60 meta-train classes from 12 superclasses, 20 classes
from 4 superclasses for meta-validation, and 20 meta-test classes from 4 superclasses. We also con-
sider the EMNIST dataset, which is an extension of the original MNIST dataset. We randomly split
its classes into 35 source classes and 12 target classes (which are: 2, 10, 11, 12, 13, 16, 18, 22, 25,
33, 34, 44). We use random cropping augmentations at training time across all of the experiments.
Architectures. We experimented with two types of architectures for h: wide ResNets (Zagoruyko
& Komodakis, 2016) and vanilla convolutional networks. Wide ResNets start with a convolutional
layer (with kernels 3 × 3 and 16 output channels), followed by three groups of layers. Each group
of layers includes a convolutional layer, followed by a sequence of N residual layers. Each residual
layer in the i’th group contains two convolutional layers using 3 × 3 kernels and output 2i+3M
channels. Each convolutional layer is followed by a ReLU activations and batch normalization post
activations. The network’s penultimate layer is a mean pooling activation with kernels of size 4 × 4,
returning an output of dimension 256 and followed by a linear layer. The vanilla convolutional net-
works have the same architectures as the wide ResNets, except that we omit the residual connections.
The networks are denoted by WRN-N -M and Conv-N -M , respectively.
B Additional Experiments
In this section we report additional experiments to provide further insights.
B.1	Varying Hyperparameters
We start by presenting experimental results to validate the consistency of our findings when varying
different hyperparameters.
Varying learning rates. We repeated the experiments in rows 1 and 4 of Figure 1 with learning
rates η = 2-2i-2 fori = 1, 2, 3, 4. We also report the train and test accuracy rates on the source data.
The results are reported in Figure 2 and Figure 3. The results are consistent with the observations we
had in Figure 1. Interestingly, when using smaller learning rates, the CDNV over the source training
and test data tends to be smaller, while the CDNV on the target classes tends to be larger. We
attribute this observation to overfitting to the source classes and indeed we observe that the few-shot
performance is generally worse in those cases.
Learning rate scheduling and varying number of target samples. To further demonstrate the
relationship between neural collapse and generalization to new classes, we experimented with the
default learning rate and standard learning rate scheduling (also used by Tian et al. (2020)), with
varying number of target samples. In this experiment, we trained WRN-28-4 using SGD with the
default learning rate and learning rate scheduling, starting with learning rate η = 0.05 with decay
factor 0.1 which is applied every 30 epochs twice. For the default training setting, in Figure 4(a-c)
we report the dynamics of the CDNV on the source training data, the source test data (i.e., unseen
test samples from the source classes), and the target classes (resp.) and in Figure 5(d-g) we plot the
1, 5, 10 and 20-shot accuracy rates of the network during training time. The results of learning rate
scheduling are provided in Figure 5.
14
Published as a conference paper at ICLR 2022
Similarly to Figure 1, as expected, we can observe that when increasing the number of source classes,
the few-shot performance improves, while the CDNV on the target classes tends to decrease. Also,
in line with our theory, the CDNV on the target classes is negatively correlated with the few-shot
performance, that is, better neural collapse yields better performance. For example, in Figure 5(d-g)
it is evident that the peak performance on all few-shot experiments for the case of 64 source classes
is achieved around the minimal value of the CDNV on the target classes in Figure 5(c). This is
achieved around iteration 16, 000 for CIFAR-FS and around iteration 20, 000 for Mini-ImageNet,
a little bit after the first learning rate decay (at 15, 000 and 18, 000 steps, respectively). As can
be seen, the performance slightly decreases after the peak iteration, while the CDNV on the target
classes slightly increases, when the training starts to overfit to the source data and the source classes.
This effect can be mitigated by selecting the final network based on the performance on the source
test data, as we show in Table 1.
B.2	Neural Collapse and Lower Layers
We conducted a comparison between the behaviour of the penultimate layer (the top embedding
layer) and a lower layer in the network, as the feature layer. We trained a WRN-28-4 on CIFAR-
FS using the default hyperparameters as described in Section 2, but in this experiment we used
the second-to-last embedding layer of the network as the feature layer for few-shot learning. A
comparison of the behavior of this layer and the top embedding layer (that we use everywhere else
in the paper) is shown in Figure 6 in terms of the CDNV on the source training data, the source test
data, and the new classes (the target data), as well as for the 5-shot 5-class classification accuracy.
As can be seen, the few-shot performance of the top embedding layer is superior to the performance
of the lower layer. This is in agreement with the evidently smaller values of the CDNV given by
the top embedding layer in comparison to the second-to-last embedding layer in all three cases (i.e.,
source train and test data and the target data). Nevertheless, we can see that the neural collapse
phenomenon and the associated good few-shot performance is preserved in this lower layer of the
network, however, less pronounced.
B.3	Dynamics of the Class-Embedding Distances
In Section 4.1 we argued that the generalization bound in Proposition 1 is meaningful when
the minimal distances mι□i=j kμf(Si) - μf(Sj)k2 (between the empirical class means) and
mini=j kμf(Pi) - μf(Pj)∣∣2 (between the true class means) are not too small. Therefore, We
empirically investigated their dynamics during training in our standard setting (WRN-28-4 with
the default hyperparameters, see Section 2) on CIFAR-FS, considering a varying number l ∈
{5, 10, 20, 30, 40, 50, 60} of source classes and learning rates η ∈ {2-2i-2}i4=1. As can be seen
in Figure 7, the values of mιni=j kμf (Si) - μf (Sj)k2 and mini=j ∣∣μ∕(Pi) - μf (Pj-)k2 tend to in-
crease during training. For completeness, we also plotted the values of mi∏i=j kμf (Pi) - μf (Pj )k2
for the target classes {Pc}k=ι (k = 20 for CIFAR-FS). Interestingly, mini=j kμf (Pi) - μf (Pjjk2
tends to grow with the number of source classes, showing improved generalization to new classes.
B.4	Class-Covariance Normalized Variance
In this section we consider the formal definition of Papyan et al. (2020) for neural collapse (their
first definition), which uses a normalized variance definition somewhat different from CDNV we
used in our analysis. For completeness, we present some experiments which demonstrate that our
findings based on CDNV also apply to their normalization.
To give the formal definition, consider a training set S = Uc=1 Sc = Uc=1{(xcj ,c)}mc 1 for an
l-class classification problem, where Sc is a collection of mc samples from class c. Assume that a
neural network is trained to classify the samples in S, where after t steps of the training we obtain
the classifier h = gt ◦ ft, where, as before, gt denotes the linear map of the last layer of the network,
and ft is the feature map.
Class-Covariance Normalized Variance. Papyan et al. (2020) define the class-covariance nor-
malized variance (CCNV) to define neural collapse: For a given feature map f, let μf = μf (Sc)
denote the mean feature value of class c, and let μG = Avgc=Jμf ] denote the global fea-
15
Published as a conference paper at ICLR 2022
厚0.6
2-z
1.0
0.8
0.4
0.4
0.4
0.2
0.2
Iteration
IteetlQn
0.0
4，	4» 411	l
——3
—10
—20
—30
——40
—50
60
——3
——10
—20
—30
——40
—50
8
——3
—10
—20
—30
—40
—50
8
—
4， 4u 4"
-5
—ICI
--2C
—30
--40
一50
«0
2-z
0.8
0.6
0.4
0.2
=W
—20
—30
——40
—50
60
一5
—10
'-20
一30
-40
——50
60
2-z
2-
4, 4m 4u
Iteration
1.0
1.0
IX)
0.8
0.8
03
厚0.6
Iteration
(a) Source classes, train data
«， dw du
0.8
0.8
9.6
0.6
45 4« 45 4∙ 4, 4∙ 4» 4" 4"
Iteration
4» 4j0 3
3.4
Iteration
4* 4> 4« 4» 4∙ 4, 4β 4» 4" 4"
Iteration
一5
10
一20
—30
-40
——50
——60
—5
——10
—20
—30
——40
—50
εo
—5
—ICI
—20
—30
——40
—50
8
2-z
4， 4jo 4u
a， a"小
——3
——10
—20
—30
——Aa
—50
8
——3
—10
—20
—30
—4«
—50
60
--S
—U
--2C
——30
--4Q
一50
60
=W
—20
—30
——40
—50
SO
一5
—10
—20
—30
——40
—50
60
03
03
一5
—10
—20
—30
——40
—50
60

(b)	Source classes, test data
#■ 4« 4u
Iteration
--i
—U
——2C
一30
--40
一50
60
——5
—10
—20
—30
——40
—50
——¢0
—5
10
—20
—30
——40
—50
——«0
4' 4, 4" 4, 4j0 4u
Iteration
一5
——10
一20
一30
——40
——50
60
OΛO
0.75
0.70
3 045
0«0
055
03β
4，4M 4"
Iteration
二%
一20
—30
——40
—50
60
0.75
0.70
,0⅛5
W MO
055
030
Iteration
0Λ0
U
2。
Iteizitlon
tcratk∣π
0.75
0.70
0.β5
go-βo
0.55
0.50
0.45
4， 4M
Iterathn
Iteration
0.75
0.70
§0«5
0«0
055
03β
ɑɑ
--i
—U
——2C
—30
--40
—50
60
—5
——10
—2。
—30
——40
—50
60
SSM

(c) Target classes, test data
Figure 2: Averaged class variance and accuracy rates when varying the number of source
classes on CIFAR-FS. In (a) we plot the CDNV and accuracy rates on the source training data
(resp.), in (b) on the source test data and in (c) on the target test data. In each experiment we trained
a WRN-28-4 with SGD on a set of l ∈ {5, 10, 20, 30, 40, 50, 60} source classes (as indicated in the
legend). The i’th column stands for learning rate η = 2-2i-2.
ture mean. The CCNV is defined to be Tr (∑W ∑B ɔ, where ∑W and ∑B are the intra- and
inter-class covariance matrices ΣfW = Avg c∈[l] [(f (xcj ) - μf ) ∙ (f (Xcj ) - μf )>] and ςB =
j∈[mc]
Avgc∈[i] [(μf - μG) ∙ (μf - μG)>, where A+ stands for the Moore-Penrose inverse of a (square)
16
Published as a conference paper at ICLR 2022
=W
—15
—20
—25
—30
—35
=?o
--15
—20
一25
——30
—35
41 # 4， 4∙ " 4" 4, 4β 4，4皿 4=
ItEratIon
41 41 4， 4« "	4" 4>	4»	4， 4皿 4"
Iteration
HeetiQn
一5
—10
一U
—20
一25
一30
一 35
XVB8⅛
40	41 4» 4» 4« 4 4∙ 4> 4' 4» 4" 4"
ItEratIon
一：。
一15
—20
一25
—30
——35
4»	41 4» 4» 4« 4 4' 4> 4β 4» 4”户
Iteration
—35
一 %
—U
—20
一25
一30
4» 4« 型 4∙ 4, 4∙ 4» 4" 4"
ItEratIon
(a) Source classes, train data
4«	41 4» 4» 4« 4 4' 4, 4β 4，4™ 4u
IterMIon
HeetiQn
41 4» 4» 4« 4» 4' 4> 4β 4，4" 4"
ltrπt⅛>π
(b) Source classes, test data
41 4z 49 4* 4$ 4β 4' 4β 49 4u 411	40	41 4z 49 4* 4$ 4β 4' 4β 49 4u 411	40	41 4z 49 44 4$ 4β 4' 4b 4' 4" 4"
IterMIon	Iteration	Iteration
一5
—10
—15
—20
—25
—30
——35
=W
—15
—20
—25
—30
——35
40	41 4* 4» 4« 型 4∙ 4,型 4» 4" 4"
Iteration
41 4* 4» 4« 4» 4∙ 4, 4β 4» 4”
Iteration

(c)	Target classes, train data
Figure 3: Averaged class variance and accuracy rates when varying the number of source
classes on EMNIST. In (a) we plot the CDNV and accuracy rates on the source training data (resp.),
in (b) on the source test data and in (c) on the target test data. In each experiment we trained a WRN-
28-4 with SGD on a set of l ∈ {5, 10, 20, 30, 40, 50, 60} source classes (as indicated in the legend).
The i’th column stands for the results when training with learning rate η = 2-2i-2 .
matrix A. According to Papyan et al. (2020), (their first version of) neural collapse happens if
limt→∞ Tr (∑W∑Bt) =0.
In Figures 10 and 11 we plot the CCNV as a function of training iterations on the source training
data, source test data and target classes. As can be seen, the value of the CCNV decreases on the
17
Published as a conference paper at ICLR 2022
train and test data of the source classes. In addition, the CCNV on the target classes decreases when
training with a larger set of source classes, which is due to better generalization. In contrast, the
CCNV on the source classes increases when training with a larger set of source classes, since it
increases the complexity of the optimization problem. These results are qualitatively very similar to
the once presented for CDNV, showing that both definitions of neural collapse behaves essentially
the same way. The corresponding train/test/few-shot accuracy rates are provided in Figures 2-3. In
Figures 8-9 we provide the CDNV, CCNV and accuracy rates of training WRN-28-4 and Conv-28-4
on Mini-ImageNet and CIFAR-FS (resp.) with a varying number of source classes.
Iteration
Iteration
22
Sd
⅛VS8⅛
一5
—10
—20
—30
——40
—50
64
0.85
0.80
0.75
l∙0.70
0.C5
o.βo
0.5S
(a) CDNV, train	(b) CDNV, test
0.70
0.C5
o.βo
&0.5S
E
I 0.50
5
10
20
30
40
50
64
XVB8U≤
Iteration
(d)	1-shot accuracy
Iteizitlon	Iteizitlon
(e)	5-shot accuracy (f) 10-shot acc
CIFAR-FS
——5
—10
---20
——30
——40
——50
64
2-3
—10
—20
——30
——40
一50
64
Iteration
Itenition
(d) 1-shot acc
(b) CDNV, test
(c) CDNV, target
0.75
0.70
>0«
i÷
IMO
0.5S
0.50
0.45
Iteizitlon
(g) 20-shot acc
(e) 5-shot acc	(f) 10-shot acc
Mini-ImageNet

Figure 4: Within-class variation and few-shot performance with learning rate η = 2-4. We
trained WRN-28-4 using SGD with learning rate scheduling on l ∈ {5, 10, 20, 30, 40, 50, 64} source
classes (as indicated in the legend). For each dataset, in (a-c) we plot the CDNV on the train and
test data and the target classes (resp.). In (d-g) we plot the 1,5,10 and 20-shot accuracy rates (resp.).
18
Published as a conference paper at ICLR 2022
——5
—10
--20
——30
——40
——50
64
2j
ΛN8
0.80
0.75
&0TO
W 0.C5
0.C0
0.5S
0.50
(b) CDNV, test
(c) CDNV, target
Iteizitlon
(d)	1-shot accuracy (e) 5-shot accuracy (f) 10-shot acc
CIFAR-FS
——5
—10
——20
——30
——40
——50
64
—5
—10
—20
一30
——40
—50
S3
(a) CDNV, train
o.βo
—5
—10
—20
—30
——40
—50
64
—5
10
—20
—30
——40
—50
64
(b) CDNV, test
t t
(C) CDNY target
—5
—10
—20
一30
——40
一50
I
(d) 1-shot acc
64
(g) 20-shot acc
(e)	5-shot acc	(f) 10-shot acc
Mini-ImageNet

Figure 5: Within-class variation and few-shot performance with learning rate scheduling. We
trained WRN-28-4 using SGD with learning rate scheduling on l ∈ {5, 10, 20, 30, 40, 50, 64} source
classes (as indicated in the legend). For each dataset, in (a-c) we plot the CDNV on the train and
test data and the target classes (resp.). In (d-g) we plot the 1,5,10 and 20-shot accuracy rates (resp.).
Ss
S3
-----top layer
mkMte ∣a⅛βr
0.80
0.75
0.70
2。.婚
⅛ 0.60
Iteration
(a) train
Itemtipn	Iteretipn	Iteration
(b)	test	(c) target	(d) accuracy
Figure 6: Within-class variation collapse of the second-to-last embedding layer. In each ex-
periment we trained a WRN-28-4 using SGD with η = 2-4 on a set of l = 64 source classes
on CIFAR-FS. We compare the CDNVs and 5-shot accuracy rates of the second-to-last embedding
layer and the top embedding layer of the network. In (a) we compare the CDNV on the source train
data, in (b) the CDNV on the source test data, (c) the CDNV on the target classes and in (d) the
5-shot accuracy rates.
19
Published as a conference paper at ICLR 2022
—5
—10
—20
—30
——40
—50
«0
、8U5J9P
4， 4 皿 4"
Iteration
一5
—10
——20
一30
--40
——50
60
Iteration
IteratIon
(a)	Source classes, train data
41 4j 4， 4∙ " 4"	4, 4» 4，4皿 4"
Ite ration
—5
—10
—20
—30
——40
—50
60
40	41 4» 4» 4« 4S 4∙ 4, 4β 4» 4” 4u
Iteration
Z8UJJJ5P
4， 4皿 4"
4， 4皿 4"
(b)	Source classes, test data
、8U3P
hutIqP
40	41	42 4' 44 4, 4β	4, 4β 49 430 4"	40	41 4z 45 4* 4, 4β	4, 4β 4' 4ω 4" A0 41 42 4, 4* 4, 4β	4, 4β 4' 4皿 4"	40	41	42 4' 4* 45 4β	4, 4，	4' 4皿 4"
IterMIon	Iteration	Iteration	KeraHon
(c)	Target classes, test data
Figure 7: Dynamics of minimal class-means distance. In (a) We plot mmi=∙ ∣∣μ∕ (Si) - μf (Sj )k,
in (b) We plot mi%=j ∣∣μf (Pi)-μf (Pj- )k and in (c) We plot mi%=j ∣∣μf (Pi)-μf (Pj- )k as a function
of the number of training iterations. In each experiment We trained a WRN-28-4 using SGD on a set
of l ∈ {5, 10, 20, 30, 40, 50, 60} source classes on CIFAR-FS (as indicated in the legend). The i’th
column corresponds to the results of training With SGD With η = 2-2i-2 .
4«	41 4» 4» 4∙ 4» 4∙ 4, 4β 4» 4™ 4u
Iteration
4， 4皿 4"
一5
—10
—20
—30
——AO
—50
60
4。	43 4j 4， 4∙ 45 4' 4,	4≈	4» 4" 4u
Iteration
0.70
0.β5
产
%55
一5
—10
—20
—30
--40
—50
——60
Iteration
41 4j 4， 4* "	4' 47 4β 4，4皿
IteratIon
(a) train
(c) target
4。	41 4» 4» 4« 4» 4« 47 4» 4» 4u> 4"
Iteratgn
(b) test

Figure 8: Results of WRN-28-4 on Mini-ImageNet. (row 1) CDNV, (row 2) CCNV, and (row 3)
accuracy on the source train, test, and the target data (columns a,b,c, resp.). Each model Was trained
using SGD With η = 2-4 on a set of l ∈ {5, 10, 20, 30, 40, 50, 60} source classes (as indicated in
the legend).
20
Published as a conference paper at ICLR 2022
一5
—10
一20
一30
--40
一50
60
(a) train
(b) test
(c) target
4。	41 42 4, 4« 4» 4" 47 4» 4’ 4ω 4"
Iteratgn
Figure 9: Results of Conv-28-4 on CIFAR-FS. (row 1) CDNV, (row 2) CCNV, and (row 3) ac-
curacy on the source train, test, and the target data (columns a,b,c, resp.). Each model was trained
using SGD with η = 2-4 on a set of l ∈ {5, 10, 20, 30, 40, 50, 60} source classes (as indicated in
the legend).
・■
4»	41 42 4, 4» 4» 4' A1 4β 取 4m 4≈
Iteration
(a) Source classes, train data
(b) Source classes, test data
4»	41 42 4, 4» 4S 4' 4, 4a 49 4m 4≈
Iteration
4»	43 42 4, 4» 4S 4' 4, A" 49 4m 4u
Iteizitlon
4u

(c) Target classes, test data
Figure 10: Within-class variation collapse on CIFAR-FS. In (a) We plot the CCNV on the source
training data, in (b) on the source test data and in (c) on the target test data. In each experiment
we trained a WRN-28-4 using SGD on a set of l ∈ {5, 10, 20, 30, 40, 50, 60} source classes (as
indicated in the legend). The i’th column stands for the results when training with learning rate
η = 2-2i-2.
21
Published as a conference paper at ICLR 2022
4« " 4"	4> 4» 4，4" 4"	4"	41 # 4，	4* " 4'
IterMIon	Iteration
(a) Source classes, train data
一∖
—W
—20
一25
一30
—35
?o
一5
—10
--15
一20
一25
一30
—35
4 皿 4"	4"	41 # 45 44 "	4' 47 4，	4， 4皿 4"
Iteration
2-z
2-，
4»	41 4l 4» 4« 4 4' 4, 4β 4» 4”户
Iteration
41 A2 4, 4* 4$ 4β 4' 4β 49 4u 411	40	41 A2 43 44 4$ 4β 47 4β 4' 4u 411	40	41 4z 49 44 4$ 48 A1 4b A9 4" 4"
IterMIon	Iteration	Iteration
(b) Source classes, test data
40	41 4l 4» 4, 45 4' 4, 4β 4» 4" 4"
Iteration
4。	41 4» 4» 44 45 4' 4, 4β 4，4™ 4"	4«	41 4» 4» 4, 45 4∙ 4, 4β 4» 4"
IterMIon	Iteration
(c) Target classes, test data
Figure 11: Within-class variation collapse on EMNIST. In (a) We plot the CCNV on the source
training data, in (b) on the source test data and in (c) on the target test data. We trained a WRN-28-4
with SGD on a set of l ∈ {5, 10, 15, 20, 25, 30, 35} source classes (as indicated in the legend). The
i’th column shows results for learning rate η = 2-2i-2 .
22
Published as a conference paper at ICLR 2022
C Proof of Proposition 1
m
Lemma 1. Let Pc be a class distribution, δ ∈ (0,1) and Sc = {xς7-匕圈 〜PmC be a dataset of fm
samples. Let f be the output of the learning algorithm. If
∣∣Ex~Pc[f(x)]- AVgx∈Sc[f(x)]∣∣ ≤ ec(S);
∣Eχ~pJkf(x)k2] — Avgx∈Sc[kf(x)k2]∣ ≤ ec(δ),	()
then we have
,~ , ~ _ , . , ~ . .. _ , . <-> ...
Varf(Pc) ≤ Varf (Sc)+ e2(δ) + 2∣∣μ∕(Pc)k ∙ ec(δ) + ef(δ)2 .	⑶
Proof. Recall that
Varf(Pc) = Eχ~pjkf(x)k2] -kEx~pJf(x)]k2 .	(4)
WeWoUld like to upper bound Ex~pjkf (χ)k2] in terms of Avgx∈sjkf (χ)k2] and to lower bound
kEx~Pc[f(X)]k2 Using k AVgx∈scf (χ)]k2. ByQ),
Ex~Pc[kf(x)k2] ≤ Avgm=Cι[kf(Xcj)k2] + e2(δ) .	(5)
By the triangle inequality,
kEx~Pc[f(x)]k + ax~pjf(x)] — AVg旌Jf(Xcj)“ ≥ k Avgm⅛[f(Xcj)]k .
By (2),
胆x~Pc[f(x)]- Avgj=Cι[f(Xcj)]∣∣ ≤ ef(δ).
Hence,
kEx~Pc[f(X)]k + 6(δ) ≥ k AvgmCι[f(Xcj)]k ,
which implies
IlAVg 旌 ι[f(Xcj )]『≤ ∣∣Ex~Pc[f(X)]∣∣2 + 2ec(δ) ∙kEx~pJf (x)]∣∣ + ec(δ)2 .	(6)
Combining (5) and (6), we obtain
Varf (Pc) ≤ Varf (Sc) + e2(δ)+2 ∣∣Ex~p0[f (x)]∣∣ ∙ ef (δ) + ec(δ)2
=Varf (Sc) + e2(δ) + 2||〃f (Pc)k ∙前⑷ + ec(δ)2 ,
completing the proof.	口
Proposition 1. Fix two source classes, i andj with distributions Pi and Pj, and let δ ∈ (0, 1). Let
Sc 〜PmC for c ∈ {i, j} .Let
ei(δ∕4) + e1(δ∕4)夕以 b _ Avgc∈{i,j}[奇("4)+21间(Pc)卜百(8/4)+ 宙仞明
=kμf (Pi)- μf (Pj )k	a =	fiFfW	.
2
Then, with probability at least 1 - δ over S, we have Vf (Pi , Pj) ≤ (Vf (Si , Sj) + B) (1 + A)2.
Proof. By definition and the union bound, with probability at least 1 — δ, the following inequalities
hold simultaneously for all c ∈ {i, j}:
∣∣Ex~Pc[f(X)]- Avgx∈Sc[f (x)]∣ ≤ ec(δ∕4);
∣Ex~pJkf(X)k2] — Avgx∈Sc[kf(X)k2]∣ ≤ ec(δ∕4) .	()
In the rest of the proof, we assume that the above inequalities hold. Let ∆ = ∣kμf (Pi) 一 μf (Pj) k2 一
kμf(Si) — μf(Sj )k2∣. By simple algebraic manipulations,
,~	.	.~... C	, ~	.	.~... C
1	1	kμf(Pi)-μf(Pj)k -kμf(Si)-μf(Sj)k
------------------ -----------------------------------------------------------------
..	.~ .	.~...c	..	, ~ .	.~...c	..	, ~ .	.~...c ..	, ~ .	.~...c
kμf (Pi)-μf (Pj)k2	kμf (Si)-μf (Sj)k2	kμf (Pi)-μf (Pj)k2 ∙ kμf (Si)-μf (Sj)k2	(8)
1	∆	L
~	~	~ I	~	~ Z	~	~ Z-.
一kμf(Si)-μf(Sj)k2	kμf(Pi)-μf(Pj)k2 ∙ kμf(Si)-μf(Sj)k2
23
Published as a conference paper at ICLR 2022
By (7) and Lemma 1 We have
∀c ∈ {i, j} : Varf(PC) ≤ Varf(Sc) + 62(δ∕4) + 2|向(P0)∣∣ ∙ ef(δ∕4) + ef(δ∕4)2 * .	(9)
Multiplying both sides of (8) by Varf(P3), and using (9), combining the above inequality with (8),
We obtain
,~ , , ~ . , ~ .
Varf(PC)	/	Varf (SC)	Varf(SC) ∙ ∆
~ 〜 ʌ ~ ~ ~ ~
(Pi) - μf (Pj)k2 - kμf (Si) - μf (Sj)∣∣2	||〃f(P)-〃f(P)∣∣2 ∙∣M(Si)-μf(Sj)k2
(≡C(δ∕4) + 2∣μf (PC)k ∙ ef(δ∕4) + ef(δ∕4)2) ∙ ∆
~ ~ - ~ ~ -
(Pi) - μf (Pj)k2 ∙∣M(Si) - μf (Sj)k2
,eC(δ∕4) + 2∣∣μf(PC)∣∣∙ 6(δ∕4)+ ef(δ∕4)2
.
∣μf (S) - μf (¾)∣2
(10)
Averaging (10) over C ∈ {i, j} gives
Vf (Pi,Pj) ≤ Vf (Si,Sj)
1 H-----H-------H---
||〃f(Pi)-〃f(P )k2
AvgC∈{i,j}卜2(δ∕4) + 2∣μf (PC)k ∙ ef(δ∕4) + ef(δ∕4)2] ∙ ∆
(Pi) - μf (Pj)k2 ∙∣M(Si) - μf (Sj)∣∣2
,AvgC∈{i,j}卜2(δ∕4) + 2||〃f(PC)k ∙ ef(δ∕4) + ef(δ∕4)2]
I	.
I∣μf (Si)-〃f (Sj)12
(11)
By the triangle inequality and (7),
,~ , ~ . .. , ~ . , ~ . .. , ~ , ~ . .. , ~ ~ ...
kμf (Si)- μf(Sj	)k ≤	kμf (Pi)- μf(Pj )k + kμf (Si)- μf (Pi)k + kμf	(Sj )	- μf(Pj )k
.~ . . ~ ... .- . √
≤ k〃f(Pi) - μf(Pj)k + 6(s∕4) + 4(δ∕4),
and by a symmetric argument,
∣kμf (Pi )- μf (Pj )k-kμf(Si)- μf(Sj )k I ≤ E1(δ∕4) + 4(δ∕4).
This and (7) imply
∆ = |||〃f (Pi) - μf (Pj )∣2- ∣μf(Si)- μf(Sj )∣2∣
=1 kμf (Pi)- μf(Pj )k - kμf (Si)- μf(Sj )k I θlμf (Pi ) - μf (Pj )k + kμf (Si)- μf(Sj )k)
≤ (≡1(δ∕4) + e1(δ∕4)) ∙(2||〃f (Pi) - μf(Pj )∣ + e1(δ∕4) + j(δ∕4)).
Plugging the above bound into (11) shows that, with probability at least 1 - δ,
Vf (Pi, Pj) ≤ Vf (Si, Sj)
(e1(δ∕4)+ e1(δ∕4)
I ɪ -1-	~	~
∖	||〃f(Pi)-〃f(Pj )∣
e1(δ∕4) + j(δ∕4)
,~ , , ~ ...
kμf (Pi)- μf (Pj )k
,2AvgC∈{i,jJeC(δ∕4) + 2∣∣μf(PC)∣∣∙ ef(δ∕4)+ ef(δ∕4)2] ∙ g(δ∕4) + j(δ∕4))
1
kμf (Pi)- μf (Pj)∣∣ ∙ kμf (Si)- μf (Sj)∣∣2
AvgC∈{i,j}卜2(δ∕4) + 2||〃f(Pc)I ∙ ef(δ∕4) + ≡C(δ∕4)2] ∙ g(δ∕4) + 4(δ∕4))2
IM(Pi)-μf (P)k2 ∙ IM(Si)-μf(Sj)k2
ι Avgc∈{i,j}卜2(δ∕4) + 2∣μf(Pc)I ∙ ef(δ∕4) + ef(δ∕4)2]
^+
l∣μf (Si)-〃f (Sj)12
≤ (Vf (Si, Sj) + B)(1+ A)2,
completing the proof.
□
24
Published as a conference paper at ICLR 2022
D Proof of Propositions 2
Proposition 2. Let F * ⊂ F be any finite set of functions with ∆(F *)=
inff∈f* infPc=Pc0 kμf(Pc) — μf(Pco)k > 0. Then, with probability at least 1 一 δ over the
selection of source class distributions P,
EPc 6=Pc0[Vf (Pc, Pc0)]
≤ Avgi=jhVf(Pi,Pj)i +
I
8 +
∖
16 SuP	Varf(P0)∖
f ∈F *,P0∈C
∆(F *)
√2π log(l)E[R(HF* (P))]
—(l - 1) ∙ ∆(F*)2-
z	.	∣∣" x∣∣λ 2√log(I⑷∙ SuP Varf(PO)
Λ	4supx∈X, f∈F* kf(x)k)	f∈F*,P0∈C
+ ∖ +	∆^	)	√ ∙ ∆(F*)2
To prove this proposition, we apply Theorem 2 of Maurer & Pontil (2019). In order to do so,
we need the following definitions (their Definition 1): Let U be any set and A : Ul → R. For
z = (z1, . . . , zl), j ∈ {1, . . . , l} and zj0 ∈ U, we define the j-th partial difference operator on A as
Dz0 A(z) = A(. . . , zj-1 , zj, zj+1 . . . ) — A(. . . , zj-1 , zj, zj+1, . . . ) .
In addition, we denote
M(A)
max sup
j∈[l] z∈Ul
zj0 ∈U: zj0 6=zj
Dzj0 A(z)
k j-zjk ,
and
J(A)
Dzrr0 Dzjj0 A(z)
l ∙ max SUP --------------------j---
r6=j∈[l]	z∈Ul	kzj- zjk
zj0 ∈U: zj0 6=zj
zr0 ∈U: zr0 6=zr
and the maximal difference
K (A) = max
j∈[l]
suP
z∈Ul
zj0 ∈U : zj0 6=zj
Dzj0 A(z).
Now we are ready to proceed with the proof.
Proof. Let U = HF* (C), z = (ui, vi)li=1 ∈ Ul, Ia6=b = I[a 6= b] and
vi + vj
A(Z) = AVgi=j [IUi=Uj ∙ M-Uj ∣∣2
Furthermore, assume that P =	{P1,...,P1}〜DC (Pi,	...,Pi	|	∀i	= j ∈	[l]	:	Pi	=	Pj)	is
an independent and identical copy of P. Then, by Corollary 3 of Maurer & PontiI (2019) and the
fact that the Gaussian complexity of any set A ⊂ Rl is at most 2,log(l) times its Rademacher
complexity, with probability at least 1 - δ over the selection of P,
max [Ep [A(Hf(P))] - A(Hf(P))]
≤ 2,2πlog(l) (2M(A) + J(A)) ∙ E[R(Hf* (P))] + K(A) ∙ Pllog(1∕δ).
(12)
Since P1, . . . , Pl are sampled such that Pi 6= Pj (for all i 6= j ∈ [l]) and ∆(F*) > 0, we have
--	,~	,	,二、r
I[μf (Pi) = μf(Pj)] = 1 for all i = j, and so
A((Hf(P)) = Avgi=j[l[μf(Pi)= μf(Pj)]∙ Vf (Pi,Pj)] = Av&=j[VfRPj)]
Similarly, we have A((Hf (P)) = Avgi6=j [Vf (Pi, Pj)], and by the linearity of expectation,
EP [A(Hf (P))] = EPc6=Pc0 [Vf(Pc, Pc0)]. By substituting these into (12), we obtain that for any
25
Published as a conference paper at ICLR 2022
f ∈ F *, We have
- . - , ~ ~ .
EPc=Pc,[Vf (Pc,Pc0)] ≤ Av&=j Vf (Pi,Pj)
+ 2,2π log(l)(2M(A) + J(A)) ∙ E[R(Hf*(P))]	(13)
+ K(A) ∙ Pl log(1∕δ).
To finish the proof, We bound M (A), J(A) and K(A). Letj ∈ [l] be some index and z0 = (zi)li=1 =
(u0i, vi0)li=1 ∈ Ul be a vector, such that, zi0 = zi for all i 6= j and zj0 6= zj. Then,
Dzj0A(z) = A(z) - A(z0)
=1 X	II ， V + V _I ， o vi + Vj 1	(14)
l(l- l)i∈[⅜1=j[ jkUi-Ujk2	Ui=Ujkui-ujk1 ,	( )
since only pairs involving j are non-zero in the average defining A. To simplify notation, let a =
kui - uj k and b = kui - u0j k and c = kzj - zj0 k = k(uj, vj) - (u0j, vj0 )k. Next We bound the terms
in the sum in (14) individually. We divide the analysis into three cases:
Case 1.	Assume ui = uj = u0j . In this case We simply have
I ，	Vi + Vj	-I ， ,	vi+ Vj	ʌ =0
IUi=Ujkui-Ujk2	Iui=Ujkui-uj∣"	0 .
Case 2.	Assume ui 6= uj and ui = u0j (a bound can be obtained similarly for ui = uj and
ui 6= u0j). In this case, since zi, zj ∈ U, We have Vi, Vj ≤ sup	Varf(P0), implying
f ∈F *,P0∈C
I	Vi+ Vj	_I	Vi + Vj
Iui=Uj kui-uj k2 - Iui=Uj kui-Ujk2
2 sup	Varf (P0)
Vi + Vj	f ∈F*,P0∈C
≤ kUi- Ujk2 ≤	∆(F*)2
(15)
In addition, since c ≥ kui - ujk ≥ ∆(F*), We have
-1 I	Vi + Vj	_I	Vi + Vj
Iui=Uj kui-Uj k2 - Iui=Uj kui-Ujk2
2 sup	Varf(P0)
f ∈F *,P0∈C
∆(F *)3
(16)
Case 3.	Assume ui 6= uj and ui 6= u0j . By simple algebraic manipulations, We have
Vi + Vj I	Vi + Vj
UU kUi-ujk2 - Ui=Uj kUi-Ujk2
Vi + Vj	Vi + Vj0
---------	.........-
kUi-Ujk2.............kUi - Ujk2
(v+V .)(b - a)(b+a) + Vj-Vj
(Vi + Vj)	a2b2	+	b2
≤
≤
(Vi + Vj) lb- a12；(? + a) + ⅛j^
a2 b2	b2
/	, ʌ kuj - UjkYb + a) , |Vj - Vj|
(Vi+Vj)-------a2b-------+
26
Published as a conference paper at ICLR 2022
where the last inequality follows from the triangle inequality. Since ∣∣u7- - Ujll ≤ ∣∣u7∙∣∣ + IlujI∣ ≤
2supχ6χ f∈f* If(x)∣ andv%,vj ≤ sup	Varf(P0),wehave
,	f∈F*, P0∈C
I	Vi+ Vj	_I	Vi + Vj
Iui=uj IUi-UjI2 - Iui=uj IlUi-UjI2
≤ 4 sup
χ∈X, f∈F
8supχ∈X 一
≤
If(X)i∙	SUP	Varf(P) ∙
'*	f ∈F*, P0∈C
f∈F* kf(x)k ∙	SUP
f ∈F*, P0∈C
∆(F*)3
+
2 sup	Varf(P0)
f ∈F *,P 0∈C
∆(F*)2
Varf(P0)	2 sup	Varf(P0)
f ∈F *,P 0∈C
∆(F*)2
We notice that ∣uj — uj∣ ≤ C and |vj - vj| ≤ c. Therefore, we obtain
c-1
4 sup	Varf(P0)
f∈F*, P0∈C
∆(F*)3
Therefore, in each case we have
I	Vi + Vj	_I	Vi + Vj
Iui=Uj IlUi-Ujk2 - Iui=Uj IlUi-Ujk2
4supχ∈χ, f∈F* kf(x)k ∙	SUP
f∈F*, P0∈C
Vi + vj
LUi=Uj IUi- Uj∣2
0 + 1
+ ∆(F*)2 '
Varf(P0)	2 sup	Varf(P0)
f ∈F *,P 0∈C
Iui=uj M⅛⅛ - I
≤ (Vi+Vj)∙ (+ + a⅛
≤
+
1
≤
+
and also
∆(F*)3
∆(F*)2
c-1
I	Vi+ Vj	_I	Vi + Vj
Iui=uj IUi-UjI2 - Iui=uj IUi-UjI2
4 sup	Varf(P0)
f ∈F *,P 0∈C
∆(F*)3
+ ∆(F*)2 '
≤
1
Hence,
IDjjA(Z) I
and
8supχ∈X, f∈F* If(x)I ∙ SUP
f∈F*, P0∈C
l ∙ ∆(F*)3
Varf(P0)	2 sup	Varf(P0)
__________f ∈F *,p0∈c__________
+	l ∙ ∆(F*)2
,(17)
% A(Z) I	4 f ∈Fs*u %/arf(P)
Izj-ZjI ≤ l ∙ △(尸)3
+ l∙ ∆(F* )2 '
(18)
Therefore, K(A) and M(A) are also bounded by the right hand sides of (17) and (18). Next, we
upper bound J(A). Let r = j and Zr ∈ U. We have
DQjj A(Z)
v	vr+vj _ π	Vr +vj
1	Ur =uj kur-Ujk2 Ur =uj ∣∣ur 一ujk2
Ilzj-Zjll	- l(l-1)
c
0 ,	0 I 0
-∏-	Vr + Vj	-∏-	Vr +Vj
IUr=Uj kUr-Ujk2 - IUr=Uj ∣mr-Ujk2
I(I-I)
≤ 1(1-1)
I	Vr +vj _I	Vr +vj
止 Ur = UjkUr-Uj ∣∣2	IIUr =UjkUr-Ujk2
ι 1
+ l(l- 1)
0 ,	0 I 0
-∏-	Vr +Vj	-∏-	vr +vj
IUr=Uj ∣mr-Ujk2 - IUr=Uj imr-Ujk2
≤
—
1
1
c
c
c
1
27
Published as a conference paper at ICLR 2022
Hence, by (18) we have
J(A) ≤ l-1
8 sup	Varf(P0)
f ∈F*,P0∈C	ι 2
∆(F * )3	+ ∆(F *)2
Substituting the bounds of M(A), K(A) and J(A) into (13) proves the proposition.	口
E Analysis for Sections 4. 1 -4.2
In this section we analyze the asymptotic behaviour of c1(δ) and c2(δ) (see Section 4.1), as well as
of EP [R(Hf* (P))] (see Proposition 2) for ReLU neural networks. A ReLU neural feature map2 is
a function of the form f(x) = W qσ(W q-1 . . . σ(W1x)) : Rd → Rp, where σ is the element-wise
ReLU function σ(x) = max(0, x), and Wi ∈ Rdi+1 ×di for i ∈ [q], where d1 = d and dq+1 = p.
Throughout this section, we use F to denote the set of ReLU neural feature maps (with the depth
q and the dimensions d1, . . . , dq+1 of the layers fixed). The spectral complexity of a network f is
defined as C(f) = maxj∈[p] kWjq ∣∣∙Qq-1 IlWr k WherefOrvectors, ∣∣∙k denotes the Euclidean norm,
while for matrices, it is the spectral (L2-induced) norm. This quantity upper bounds the Lipschitz
constant of f and is similar in fashion to other (slightly different) notions of spectral complexity for
neural networks (Golowich et al., 2017; Bartlett et al., 2017).
Throughout the section, for a given function g : A → Bk andj ∈ [k], we denote the j’th coordinate
of g(χ) by g(χ)j and for a class G ⊂ {g : A → Bk}, we define Gj = {g(∙)j : g ∈ G}.
Before presenting the main claims of this section, following Poggio et al. (2020), we start with
describing a canonical representation of ReLU neural networks (the proofs of our statements are
given for this representation). Since max(0, ax) = a max(0, x), for all x ∈ R and a ≥ 0, any
neural network f = W qσ(W q-1 . . . σ(W1x)) ∈ F can be represented as a modified network
f0(x) = Vq σ(V qT...σ(V 1x)),where Vq = Wq ∙ Qr=I ∣∣W r∣∣ and ∀r ≤ q - 1 : Vr = k^.
In particular, ∣Vr∣ = 1 for all r ≤ q - 1 and, since Vq = Wjq ∙ Qqq= 11∣Wr∣, maxj∈p ∣∣Vq∣∣ =
maxj∈p IIWjqk ∙ Qq=1 ∣∣Wr∣ = C(f). Thus, for any f ∈ F, there exists an equivalent neural
network f0 which belongs to the set
FM =	Wqσ(Wq-1 . .	. σ(W1x))	∈ F |	∀r	≤ q - 1 : ∣Wr	∣ = 1 and max∣Wjq∣	≤ M
j∈[p]
for any M ≥ C(f). Therefore, F = SM∞=1 FM (FM ⊂ F is trivial by definition).
Next we present the first claim of the section. The following proposition shows that the first and
second moments of ReLU neural feature maps with bounded spectral norms concentrate around
their means. In particular, if a learning algorithm (in Section 4.1) is guaranteed to return a neural
network f with C(f) ≤ M, then ∈C(δ) and €C(δ) are bounded by the right hand sides of (19) and
(20), respectively. Note that both of these terms scale as O(∙∖∕log(1∕δ)∕mc). The analysis is based
on Theorem 1 of Golowich et al. (2017) and the proof of Theorem 1.1 of Bartlett et al. (2017).
Proposition 3. Let P = {Pc}C=1 be a set of class-conditional distributions over a bounded set
X ⊂ Rd and let m1, . . . , ml ∈ N. Let F be the class of ReLU neural feature maps as defined above.
mm
Then, with probability at least 1 — δ over the selection of S1 〜P1 1,..., Sl 〜Pl l ,for all C ∈ [l]
and f ∈ F, we have
快~Pc[f(x)] — Avgχ∈Sc [f (x)]∣∣
≤ p(C(f) + 1"x∈X kxk(3√q +2+ rl°g≡δ) + plog(C(f) + 1)
√mC	∖	V 2
(19)
2ReLU neural networks for classification typically have an additional soft-max layer on top their feature
map.
28
Published as a conference paper at ICLR 2022
and
IEx〜Pc [kf(X)k2] - Avgχ∈⅛ [kf(X)k2]∣
≤ PM2s√∈X kxk2 " +4 + 3产等 + 3Plog(C(f) + 1)
(20)
Proof. We prove that, for any fixed c ∈ [l] and M ∈ N, (19) and, respectively, (20) hold for all
f ∈ FM simultaneously with probability at least 1 - 2，m(M十1).Then taking the union bound over
c and M proves the proposition (since F = SM∞=1 FM).
Fix c ∈ [l] and M ∈ N. By the triangle inequality, for any f ∈ F, we have
p
但"Pc [f (X)] - AVg 江 Sc [f (x)] Il ≤ X ∣Eχ 〜Pc [f (X)j ] - Avgx∈Sc [f (x)j] ∣.	(21)
j=1
By Theorem 3.3 of Mohri et al. (2018), with probability at least 1 - 2piM(δM+[)over the selection
of Sc 〜PmC, for any f ∈ FM, We have
∣E"P∕(X)j] - Avgx∈Sc [f (X)j] ∣
≤ 2R(FM(Sc)) +3 sup	f'(x)小 Slog(4PlM(M + 1)/S) .	(22)
mc	x∈X, f0∈FM	2mc
The first term on the right hand side can be bounded using Theorem 1 of Golowich et al. (2017),3
stating that
R(FM(Sc)) ≤ √mc (p2log(2)q +1) M sup ∣∣x∣∣≤√mC (1.5√q + 1) M sup ∣∣x∣∣ .	(23)
x∈X	x∈X
Moreover, for any f0 ∈ FM we have |f0(χ)j| ≤ M ∙ supx∈χ ∣∣xk. Substituting these inequalities
into (22) implies
∣Ex〜Pc [f (X)j] - Avgx∈Sc [f (X)j] ∣
(24)
≤ 罡√ +2)M ∙ supx∈χkχk
√m∑
+ 3M ∙ sup I∣x∣∣∙ J：
x∈X
log(4plM (M + 1)∕δ)
2mc
By the union bound, (24) holds for all j ∈ [p] simultaneously with probability at least 1 - 21M(M+1).
Combining this with (21), we obtain that for any fixed c ∈ [l] and M ∈ N, with probability at least
1 - 21M(M +1),for all f ∈ FM \ FM T we have
忸〜Pc[f(X)]- Avgx∈Sc [f (X)] Il
≤ PM supx∈χ Ilxll
一	√mc
(3√q +2 + 3 产PT三
≤ PMs√mXkxk 卜√q +2 + 3yi0g≡δ) + 3pl0g(M+I))
≤ P(C(f)+ √mpx∈Xkxk (3√q + 2 + 3严4叵 + 3plog(C(f)+2)
(25)
3The original statement makes use of the Frobenius norms of the matrices, however, the proof is exactly the
same if we replace the Frobenius norm with the spectral norm.
29
Published as a conference paper at ICLR 2022
where the last inequality follows from the fact that M ≤ C(f) + 1, since C(f) ∈ [M - 1, M]. Next,
we prove the second inequality for fixed c and M. As in (22), by Theorem 3.3 of Mohri et al. (2018),
With probability at least 1 - ?m(M十1)over the selection of Sc, for all f ∈ FM, We have
∣Ex~Pc[kf(x)k2] -AVgx∈sjkf(x)k2]∣
≤
2R(Gs(Sc))
mc
+ 3 "f(x)k2 JR
(26)
where GS	=	{kf(∙)k2	| f ∈	FM}.	By definition, with E =	(6ι,..., ∈mc)	denoting i.i.d.
Rademacher random variables (i.e., random variables taking values ±1 With probability 1/2 each),
,_ , ~ ..
R(G S(Sc))= Ee
p	mc
≤ X EJf ∈Fm(X eif (xci)j)∖
mc	p
X XEif (Xci)2
(27)
Note that the jth terms (in the square bracket) at the right-hand side above is the Rademacher
complexity of the composite function class g ◦ FjM = {g ◦ f0|f0 ∈ FjM} for g(y) =
y2. Since supx∈X |f0(x)j|	≤ M supx∈X kxk and g is 2M supx∈X kxk-Lipschitz on
[-M supx∈X kxk, M supx∈X kxk], by Lemma 1.1 in Lecture 17 of Kakade & Tewari (2008), the
Rademacher complexity of this composite function class can be bounded as
R(g ◦ FM(Sc)) ≤ 2M sup kxk ∙ R(FM(Sc)).	(28)
x∈X
Combining with (27) and (23) yields
pp
R(GS(Sc)) ≤	fR(g	◦FM)	≤ 2M sup	kxk ER(FM)	≤ pM2 sup	∣∣xk2√mZ(3√q	+ 2).
j =1	x∈X j =1	x∈X
(29)
Substituting into (26) and using supx∈X, f0∈FM kf 0 (x)k2 ≤ pM2 supx∈X kxk2 imply, similarly
to (25), that with probability at least 1 - 2M(M +1)over the selection of Sc, for all f ∈ FM \ FM-1
simultaneously we have
∣Ex~Pc [kf (X)k2] - AVgx∈Sc [kf (X)k2] I
≤ PM 2 s√∈Xkxk2 卜 √q + 4 + 3j Iog(I≡ + 3,log(C(f ) + 2)! ,
finishing the proof.	口
Next we show that the Rademacher complexity in Proposition 2 scales as O(√7) for ReLU neural
networks with bounded spectral complexities.
Proposition 4. Let F* = {f ∈ F|C(f) ≤ M} be a class of ReLU neural networks with. Then,
EP[R(Hf* (P))] ≤ √l(1.5√q + 1)M sup ∣∣x∣ ( 1 + 4pM sup ∣∣x∣
x∈X	x∈X
Proof. As discussed at the beginning of the section, F* and FM define the same function class
through different representations. Thus it suffices to consider FM in place of F* (as trivially
EP [R(Hf*(P))] = EP [R(Hf m (P))]).
Fix P = (P1, . . . , Pl). With E = (Ecj)c∈[l],j∈{0}∪[p] denoting i.i.d. Rademacher random variables,
by the definition of HFM and because supa∈A[f(a) + g(a)] ≤ supa∈A f(a) + supa∈A g(a), we
30
Published as a conference paper at ICLR 2022
have
R(HFM (P)) = Ee	sup
f ∈F M
ι p
ΣΣCcj μf (PC) j + CcO Varf (PC)
P
≤ Σ>
j=i
SUP 72 C
f∈FM ∖c=1
Cjμf(Pc)j) [ + Ee sup
f ∈F M
(X CCOVarf
(30)
~
~
We bound the two terms in (30) separately. The terms in the first summation can be bounded as
∕sup (X CCjμf (PC)) = ʃsup (X CCj%c~pjf (XC) j]
≤ %1 ~P1,...,Xl~PlEe
『up (X CCjf (XC) J]
(31)
%1 ~□ι,...,Xι~□ι R (FM HxJC=1))
≤ √7(1.5√q + 1)M sup ∣∣x∣∣ ,
x∈X
where the last inequality holds because of (23) (with l instead of mC samples). The elements of the
second summation (30) can be bounded as
Ee sup 2v CCOVarf
_f ∈FM ∖c=1
/SUP (X CcO (E 吼 y [kf(Xc)k2]-此。~Α。[f(Xc)]k2))]
≤ Ee
Ee
+ Ee
))
))
sup
f ∈F M
sup
f ∈F M
(-X CCθkEχjJf(XC)]k2)]
(XCCOkExcy[f(Xc)]∣∣2)], (32)
where the last equation follows from the fact that Cco are distributed symmetrically around 0. We
can bound the first term in (32) above using (29) (again, with l instead of m，c samples) as
sup
f ∈F M
≤ ExI〜户 1,...商ι ~R Ee
sup (X CCOkf (Xc)k2) ≤ pM2 sup ∣∣xk2√7 (3√q + 2)
f∈FM ∖C=1	力	x∈X
(33)
The second term in (32) can be bounded similarly, but because now the expectation is inside the
squared norm, we need to provide a few more steps:
∕sup (X cco∣%c~qc [f (XC)] Ii2)]
(l p
EECCO (ExC 〜户jf(χc)j])2
(34)
p
≤ X Ee
j=i
∕sup (X CCO (%c~p⅛[f(xc)j])2)]
31
Published as a conference paper at ICLR 2022
Now similarly to (28),
m∈ax spp lEχ-P [f (x)j ]l≤ 熠 SPp *~p f ㈤川 ≤ M ∙ SuX kxk
implies (via Lemma 1.1 in Lecture 17 of Kakade & Tewari (2008))
E Sup
f∈FM
2
≤ 2M SuP ∣∣xk ∙ Ee
x∈X
ʃsup (X Ec0EXc~Pcf (Xc)j])[
≤ 2M
= 2M
SuP kxk ∙ %c~PcEe
x∈X
SuP
f∈FM
(35)
SuP kxk∙ EP%c~pc[R(FM({xc}c=I))]
≤ M2 Sup kxk2√7(3√q + 2),
x∈X
where in the last inequality we used again Theorem 1 of Golowich et al. (2017) adapted to the
spectral norm, as in (23). Combining (34) and (35) gives
Ee /up (X ecokEχc~Pcf (xc)]k2) ] ≤ PM 2 SuP kxk2 √ (3√q + 2)
Substituting this and (33) into (32) gives
Ee SuP	c0Varf
f∈FM c=1
≤ 2pM2 SuP kxk2√l(3√q + 2)
x∈X
Plugging in this and (31) into (30) gives
R(HFM (P)) ≤ √l(1.5√q + 1)M Sup kxk (1 + 4pM SuP kxk ),
x∈X
x∈X
giving the desired bound.
□
F Analysis for Section 4.2
Lemma 2. Let X1, . . . , Xn be a set of i.i.d. uniformly distributed points in the p-dimensional
unit cube. There is a positive constant C > 0 (independent of n and p), such that,
E Imini=j∈[n] kXi - Xj k] ≥ C ∙ n-2∕p√p.
Proof. For any pair i 6= j ∈ [n],
πp∕2
Pr[kXi — Xjk ≥ D] ≥ 1 — — --Dp ,
[k j k ≥ ]≥	Γ(p∕2+1),
p/2
since the volume of the p-dimensional ball of radius D is Γ(P∕2+1) DP and the volume of the unit
cube is 1. Hence, by the union bound over all pairs i 6= j ∈ [n]:
Pr min kXi - Xj k ≥ D ≥ max 0, 1 -
i6=j
n(n - 1)	nP/2	Dp∖ = h(n D)
2	Γ(p∕2+1)D 厂 h(n,D) .
32
Published as a conference paper at ICLR 2022
Since h(n, D) > 0 if and only if D < D* = M-1/p, where M = n(n-1「(£//+1), We obtain
∞
E min kXi - Xjk
i6=j
=0
≥Z0
Pr min kXi -Xjk ≥ D dD
i6=j
,d*	(D*)p+1
h(l, D) dD = D* - M ；/])
D* 1 - M
d* C-+)
2	2 y/p Γ(p∕2 + 1)1∕p
n(n - 1)
≥ C ∙ n-2∕p√p
∏1∕2
where the last inequality follows from Ramanujan’s approximations of the Gamma function (Karat-
suba, 2001) for some C > 0 independent of n, p.	口
G Analysis for Section 4.3
In the following proposition we provide a formal statement of the analysis provided in Sec-
tion 4.3. We consider a balanced k-class classification problem and a given feature map f (e.g.,
pretrained). We upper bound the classification error of the nearest-mean classifier hf,S(x) =
argmi□c∈[k] ∣∣f (x) - μf (Sc)k in terms of the averaged CDNY that is, Avgi= [Vf (Pi, Pj)]. There-
fore, if the averaged CDNV is small, then, we expect the nearest-mean classifier to have a small
classification error.
Proposition 5. Let T be a balanced k-class classification problem with distribution P along with
Class-Conditional distributions P = {PC}k=ι. Let S = Uk=ι Sc be a dataset, such that Sc 〜
Pcnc (n =…=nk = nc ∈ N). Let f : Rd → Rp be any feature map and let hf,s (x)=
argminc∈[k] ∣∣f (x) — μf (Sc)∣ be the nearest-mean classifier Then, we have
E[Err] ：= ESE(x,y)~PI[h(x) = y] ≤ 16(k - 1) [SfPy + nc] Avgi=j [Vf(Pi,Pj)],
where s(f,P) =pif{f ◦Pc}ck=1 are spherically symmetric and s(f, P) = 1 otherwise.
Proof. First note that
E[Err] = ES E(χ,y)~p I[hf,S (X)= y]
1k
=k EPr[hf,s (Xi) = i]
i=1
=k X Pr[hf,s (Xi)=j].
i6=j
(36)
In the following we will fix i and j 6= i and bound Pr[hf,S (Xi) = j], which is the probability that
Xi 〜Pi is (wrongly) classified as j = i, for all three cases (general, symmetric, Gaussian). Let C
be either i or j. Let μc := μf (Pc) and μc := μf (Sc) and Ui = f (xi). Let Xc := ∣∣∕^c - Uik and
Yc := ∣∣Ui - μc∣ and Zc := ∣∣μc - μc∣ for C = i,j. Let aj = ∣∣μi - μj ∣∣ (or ɑ for short since i and
j are fixed) and σc2 := Varf(Pc) ≡ E[Yc2] = ncE[Zc2]. Note that Vf(Pi, Pj) = (σi2 +σj2)∕2αi2j.
33
Published as a conference paper at ICLR 2022
General case: With this, and by triangle inequalities, Yj ≤ Xj + Zj and Xi ≤ Yi + Zi and
Yi + Yj ≥ α, we get
Pr[hf,S (xi) = j] = Pr[Xj ≤ Xi]
≤ Pr[Yj ≤ Yi + Zi + Zj]
≤ Pr[Yj ≤ 3α ∨ Yi + Zi + Zj ≥ 3α]
≤ Pr[Yi ≥ a ∨ Yi ≥ α ∨ Zi + Zj ≥ 2 ]
≤ Pr[Yi ≥ α] + Pr[Zi + Zj ≥ 2]
≤ Pr[Yi2 ≥ Ο2] + Pr[Z2+Z2 ≥ α2].
Now by Markov’s inequality,
Pr[Z2+Z2 ≥ α2] ≤ E[Z2+有/(α2) = 8σ2 + 8σj,
ncαij
and similarly Pr[Y^2 ≥ α2] ≤ 16σ2∕α2j∙. Plugging this into (36), by symmetrization, leads to the
desired inequality
1	16σ2	8σi2 + 8σ2
E[Err]	≤ k	X →	+	; 02	j	= 16(k	- 1)(1 + n⅛) Avgi=j [Vf U)].
k i6=j αij	ncαij
Spherically symmetric case: In this case
Pr[h(Xi)= j] = PrMui- μjll≤llui- Ri||]
= Pr[ui ∈ H] = Pr[d(ui, H) = 0], where
H ：= {χ ∈ Rp ： ||X - μj∙∣∣≤∣∣χ - μi∣∣}
≡	{x	∈	Rp	：	hμi	—	μj-,	2x —	^i	—	μj	≥ 0}
≡ {X ∈ Rp ： d(X, H) = 0}
is the half-space of all ui that are wrongly classified, and d(X, H) is the distance ofX from separating
hyperplane ∂H if x ∈ H and 0 if X ∈ H. The distance D of the closest point of H to μi is
D = d(μi,H) = max n0, D ∣∣μi - μj ii , μi- μi+μj Eo
For fixed displacement magnitudes Zi and Zj , this distance is smallest if both displacements are
colinear in direction of μ% - μj∙, in which case
D ≥ Dmin
α	Zi + Zj
2	2-
A formal proof of this claim is as follows: W.l.g. we can choose (a coordinate system such that)
μi = 0:
2D
hμi -	μj,	μi	+	μj i	_	llμj ||2 -	llμill2	_ f∖∖ ʌ H	IIllJAj k + kμi	k
MiIl = ^lW-lj∣T = (kμjk-kμik) kμ, -μjk
≥	Mj k-kμik	≥	kμj k-kμj	- μj k-kμik = α -	Zj-	Zi	= 2Dmin ,
where both inequalities are simple applications of the triangle inequality.
Now, for fixed μ^i and μj∙ i.e. fixed D and by rotational invariance, the probability that ui ∈ H is the
same as the probability that (e.g.) the first coordinate of ui - μi is larger than D. With Yi1 := u1 -μ1
and γ ∈ (0, 1), this implies
Pr[ui ∈ H] = Pr[Yi1 ≥ D]
= Pr[Yi1 ≥ D ∧ Zi + Zj ≤ γα] + Pr[Yi1 ≥ D ∧ Zi + Zj > γα]
≤ Pr[Yi1 ≥ 0.5(1 - γ)α] + Pr[Zi + Zj > γα]
≤ Pr[(Yi1)2 ≥ 0.25(1 - γ)2α2] + Pr[Zi2 +Zj2 > 0.5γ2α2]	(37)
≤	E[(Yi1)2]	+ E[Z2 + 曷]
—0.25(1 — Y )2a2	Y 2a2
= σ2∕P	+ E + σ2"nc	(38)
=0.25(1 - γ)2α2 +	γ2α2	^	()
34
Published as a conference paper at ICLR 2022
Plugging this into (36) with γ = 0.5, by symmetrization, leads to
E[Err] ≤
1 x
i6=j
8σi2 + 8σj2
α2j
1 + ɪ
p nc
16(k -I) ^ + ^- AVgi=j [Vf (Pi, Pj )]
p	nc
□
35