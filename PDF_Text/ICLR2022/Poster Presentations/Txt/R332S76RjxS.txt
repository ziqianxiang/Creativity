A Global Convergence Theory for Deep ReLU
Implicit Networks via Over-parameterization
Tianxiang Gao
Department of Computer Science
Iowa State University
gaotx@iastate.edu
Jia Liu
Dept. of Electrical and Computer Engineering
The Ohio State University
liu@ece.osu.edu
Hailiang Liu
Department of Mathematics
Iowa State University
hliu@iastate.edu
Hridesh Rajan
Department of Computer Science
Iowa State University
hridesh@iastate.edu
Hongyang Gao
Department of Computer Science
Iowa State University
hygao@iastate.edu
Ab stract
Implicit deep learning has received increasing attention recently, since it gener-
alizes the recursive prediction rules of many commonly used neural network ar-
chitectures. Its prediction rule is provided implicitly based on the solution of an
equilibrium equation. Although many recent studies have experimentally demon-
strates its superior performances, the theoretical understanding of implicit neural
networks is limited. In general, the equilibrium equation may not be well-posed
during the training. As a result, there is no guarantee that a vanilla (stochastic)
gradient descent (SGD) training nonlinear implicit neural networks can converge.
This paper fills the gap by analyzing the gradient flow of Rectified Linear Unit
(ReLU) activated implicit neural networks. For an m-width implicit neural net-
work with ReLU activation and n training samples, we show that a randomly
initialized gradient descent converges to a global minimum at a linear rate for the
square loss function if the implicit neural network is over-parameterized. It is
worth noting that, unlike existing works on the convergence of (S)GD on finite-
layer over-parameterized neural networks, our convergence results hold for im-
plicit neural networks, where the number of layers is infinite.
1	Introduction
1)	Background and Motivation: In the last decade, implicit deep learning (El Ghaoui et al., 2019)
have attracted more and more attention. Its popularity is mainly because it generalizes the recursive
rules of many widely used neural network architectures. A line of recent works (Bai et al., 2019;
El Ghaoui et al., 2019; Bai et al., 2020) have shown that the implicit neural network architecture
is a wider class that includes most current neural network architectures as special cases, such as
feed-forward neural networks, convolution neural networks, residual networks, and recurrent neu-
ral networks. Moreover, implicit deep learning is also well known for its competitive performance
compared to other regular deep neural networks but using significantly fewer computational re-
sources (Dabre & Fujita, 2019; Dehghani et al., 2018; Bai et al., 2018).
Although a line of literature has been shown the superior performance of implicit neural networks
experimentally, the theoretical understanding is still limited. To date, it is still unknown if a simple
first-order optimization method such as (stochastic) gradient descent can converge on an implicit
neural network activated by a nonlinear function. Unlike a regular deep neural network, an implicit
neural network could have infinitely many layers, resulting in the possibility of divergence of the
1
forward propagation (El Ghaoui et al., 2019; Kawaguchi, 2021). The main challenge in establishing
the convergence of implicit neural network training lies in the fact that, in general, the equilibrium
equation of implicit neural networks cannot be solved in closed-form. What exacerbates the problem
is the well-posedness of the forward propagation. In other words, the equilibrium equation may
have zero or multiple solutions. A line of recent studies have suggested a number of strategies to
handle this well-posedness challenge, but they all involved reformulation or solving subproblems
in each iteration. For example, El Ghaoui et al. (2019) suggested to reformulate the training as
the Fenchel divergence formulation and solve the reformulated optimization problem by projected
gradient descent method. However, this requires solving a projection subproblem in each iteration
and convergence was only demonstrated numerically. By using an extra softmax layer, Kawaguchi
(2021) established global convergence result of gradient descent for a linear implicit neural network.
Unfortunately, their result cannot be extended to nonlinear activations, which are critical to the
learnability of deep neural networks.
This paper proposes a global convergence theory of gradient descent for implicit neural networks
activated by nonlinear Rectified Linear Unit (ReLU) activation function by using overparameteriza-
tion. Specifically, we show that random initialized gradient descent with fixed stepsize converges
to a global minimum of a ReLU implicit neural network at a linear rate as long as the implicit
neural network is overparameterized. Recently, over-parameterization has been shown to be effec-
tive in optimizing finite-depth neural networks (Zou et al., 2020; Nguyen & Mondelli, 2020; Arora
et al., 2019; Oymak & Soltanolkotabi, 2020). Although the objective function in the training is non-
smooth and non-convex, it can be shown that GD or SGD converge to a global minimum linearly
if the width m of each layer is a polynomial of the number of training sample n and the number of
layers h, i.e., m = poly(n, h). However, these results cannot be directly applied to implicit neural
networks, since implicit neural networks have infinitely many hidden layers, i.e., h → ∞, and the
well-posedness problem surfaces during the training process. In fact, Chen et al. (2018); Bai et al.
(2019; 2020) have all observed that the time and number of iterations spent on forward propagation
are gradually increased with the the training epochs. Thus, we have to ensure the unique equilibrium
point always exists throughout the training given that the width m is only polynomial of n.
2)	Preliminaries of Implicit Deep Learning: In this work, we consider an implicit neural network
with the transition at the `-th layer in the following form (El Ghaoui et al., 2019; Bai et al., 2019):
z' = σ (√YmAz'-1 + φ(x)) ,	(1)
where φ : Rd → Rm is a feature mapping function that transforms an input vector x ∈ Rd to a
desired feature vector φ , φ(x), z` ∈ Rm is the output of the `-th layer, A ∈ Rm×m is a trainable
weight matrix, σ(u) = max{0, u} is the ReLU activation function, andγ ∈ (0, 1) is a fixed scalar to
scale A. As will be shown later in Section 2.1, γ plays the role of ensuring the existence of the limit
z* = lim'→∞ z'. In general, the feature mapping function φ is a nonlinear function, which extracts
features from the low-dimensional input vector x. In this paper, we consider a simple nonlinear
feature mapping function φ given by
Φ(x)，-‰σ(Wx),	⑵
m
where W ∈ Rm×d is a trainable parameter matrix. As ` → ∞, an implicit neural network can be
considered an infinitely deep neural network. Consequently, z* is not only the limit of the sequence
{z' }∞=o with zo = 0, but it is also the equilibrium point (or fixed point) of the equilibrium equation:
z* = σ(γAz* + φ),	(3)
where 7 ，γ∕√m. In implicit neural networks, the prediction y for the input vector X is the
combination of the fixed point z* and the feature vector φ, i.e.,
y = UT z* + VT φ,	(4)
where u, v ∈ Rm are trainable weight vectors. For simplicity, we use θ , vec (A, W, u, v) to
group all training parameters. Given a training data set {(xi, yi)}in=1, we want to minimize
n1
L(θ) = ɪs 2(yi - yi)2
i=1
2ky - yk2,
(5)
2
where y and y are the vectors formed by stacking all the prediction and labels.
3)	Main Results: Our results are based on the following observations. We first analyze the for-
ward propagation and find that the unique equilibrium point always exists if the scaled matrix YA in
Eq. (3) has an operator norm less than one. Thus, the well-posedness problem is reduced to finding
a sequence of scalars {γk }k∞=1 such that Yk A(k) is appropriately scaled. To achieve this goal, We
show that the operator norm A(k) is uniformly upper bounded by a constant over all iterations.
Consequently, a fixed scalar Y is enough to ensure the well-posedness of Eq. (3). Our second ob-
servation is from the analysis of the gradient descent method with infinitesimal step-size (gradient
flow). By applying the chain rule with the gradient flow, We derive the dynamics of prediction y(t)
which is governed by the spectral property of a Gram matrix. In particular, if the smallest eigen-
value of the Gram matrix is lower bounded throughout the training, the gradient descent method
enjoys a linear convergence rate. Along with some basic functional analysis results, it can be shown
that the smallest eigenvalue of the Gram matrix at initialization is lower bounded if no two data
samples are parallel. Although the Gram matrix varies in each iteration, the spectral property is
preserved if the Gram matrix is close to its initialization. Thus, the convergence problem is reduced
to showing the Gram matrix in latter iterations is close to its initialization. Our third observation is
that we find random initialization, over-parameterization, and linear convergence jointly enforce the
(operator) norms of parameters upper bounded by some constants and close to their initialization.
Accordingly, we can use this property to show that the operator norm ofA is upper bounded and the
spectral property of the Gram matrix is preserved throughout the training. Combining all these in-
sights together, we can conclude that the random initialized gradient descent method with a constant
step-size converges to a global minimum of the implicit neural network with ReLU activation.
The main contributions of this paper are summarized as follows:
(i)	By scaling the weight matrix A with a fixed scalar Y, we show that the unique equilibrium
point z* for each X always exists during the training if the parameters are randomly initialized,
even for the nonlinear ReLU activation function.
(ii)	We analyze the gradient flow of implicit neural networks. Despite the non-smooth and non-
convexity of the objective function, the convergence to a global minimum at a linear rate is
guaranteed if the implicit neural network is over-parameterized and the data is non-degenerate.
(iii)	Since gradient descent is discretized version of gradient flow, we can show gradient descent
with fixed stepsize converges to a global minimum of implicit neural networks at a linear rate
under the same assumptions made by the gradient flow analysis, as long as the stepsize is
chosen small enough.
Notation: For a vector x, kxk is the Euclidean norm of x. For a matrix A, kAk is the operator
norm of A. If A is a square matrix, then λmin (A) and λmax(A) denote the smallest and largest
eigenvalue of A, respectively, and λmaχ(A) ≤ k A∣∣. We denote [n]，{1, 2, ∙∙∙ , n}.
2	Well-Posedness and Gradient Computation
In this section, we provide a simple condition for the equilibrium equation (3) tobe well-posed in the
sense that the unique equilibrium point exists. Instead of backpropagating through all the intermedi-
ate iterations ofa forward pass, we derive the gradients of trainable parameters by using the implicit
function theorem. In this work, we make the following assumption on parameter initialization.
Assumption 1 (Random Initialization). The entries Aij and Wij are randomly initialized by the
standard Gaussian distribution N(0, 1), and ui and vi are randomly initialized by the symmetric
Bernoulli or Rademacher distribution.
Remark 2.1. This initialization is similar to the approaches widely used in practice (Glorot &
Bengio, 2010; He et al., 2015). The result obtained in this work can be easily extended to the case
where the distributions for Aij , Wij , ui, and vi are replaced by sub-Gaussian random variables.
2.1	Forward propagation and well-posedness
In a general implicit neural network, Eq. (3) is not necessarily well-posed, since it may admit zero
or multiple solutions. In this work, we show that scaling the matrix A with 7 = γ∕√m guarantees
3
the existence and uniqueness of the equilibrium point z* with random initialization. This follows
from a foundational result in random matrix theory as restated in the following lemma.
Lemma 2.1 (Vershynin (2018), Theorem 4.4.5). Let A be an m × n random matrix whose entries
Aij are independent, zero-mean, and sub-Gaussian random variables. Then, for any t > 0, we have
k Ak ≤ CK(√m + √n +1) with probability at least 1 - 2e-t . Here C > 0 is a fixed constant,
and K = maxi,j kAij kψ2.
Under Assumption 1, Lemma 2.1 implies that, with exponentially high probability, kAk ≤ c√m
for some constant c > 0. By scaling A bya positive scalar Y We show that the transition Eq. (1) is a
contraction mapping. Thus, the unique equilibrium point exists with detailed proof in Appendix A.1.
Lemma 2.2. If kAk ≤ c√m for some C > 0, then for any γ° ∈ (0,1),the scalar Y，min{γ0,γ0∕c}
uniquely determines the existence of the equilibrium z* for every x, and ∣∣z'k ≤ 匚宗 ∣∣φk for all'.
Lemma 2.2 indicates that equilibria always exist if We can maintain the operator norm of the scaled
matrix (γ∕√m)A less than 1 during the training. However, the operator norms of matrix A are
changed by the update of the gradient descent. It is hard to use a fixed scalar γto scale the matrix
A over all iterations, unless the operator norm of A is bounded. In Section 3, we will show ∣∣ A∣ ≤
2c√m always holds throughout the training, provided that ∣ A(0)∣ ≤ c√m at initialization and the
width m is sufficiently large. Thus, by using the scalar γ= min{γ0, γ0∕(2c)} for any γ0 ∈ (0, 1),
equilibria always exist and the equilibrium equation Eq. (3) is well-posed.
2.2	Backward Gradient Computing
For a regular network with finite layers, one needs to store all intermediate parameters and apply
backpropagation to compute the gradients. In contrast, for an implicit neural network, we can derive
the formula of the gradients by using the implicit function theorem. Here, the equilibrium point
z* is a root of the function f given by f(z, A, W) , z - σ (γYAz + φ). The essential challenge
is to ensure the partial derivative ∂f∕∂z at z * is always invertible throughout the training. The
following lemma shows that the partial derivative ∂f∕∂z at z* always exists with the scalar γ,
and the gradient derived in the lemma always exists. The gradient derivations for each trainable
parameters are provided in the following lemma and the proof is provided in Appendix A.2.
Lemma 2.3 (Gradients of an Implicit Neural Network). Define the scalar Y，min{γ0, γo∕c}. If
IlAIl ≤ c√m for some constant c > 0, then for any γo ∈ (0,1), the following results hold:
ð/ðɪw(94
(i)	The partial derivatives of f with respect to z, A, and W are
[Im - YY diag(σ0(YYAz + φ))A]T ,	(6)
-Y [zτ 0 diag(σ0(γAz + φ))]T ,	(7)
1T
-√m [xT 0 diag(σ0(Wx))] diag(σ0(γAz + φ)) ,	(8)
where Y，γ∕√m.
(ii)	For any vector v, the following inequality holds
λmin {Im - YY diag(σ0(v))A} > 1 - Y0 > 0,	(9)
which further implies ∂f ∕∂z is invertible at the equilibrium point z*.
(iii) The gradient of the objective function L with respect to A and W are given by
▽uL =	n :X(yi-	n yi)zi,	VvL =	-yi)φi,	(10)
	i=1	i=1	
VaL =	n =X γ(yi i=1	-yi)DiT[Im-YYDiA]-TuziT,	(11)
Vw L 二	n1 =X , 2-√ ʌ/m	Kyi- yi)ET {DT [Im - γDiA]-T U + v} xT,	(12)
4
where Zi is the equilibrium point for the training data (Xi, yi), Di，diag(σ0(γAzi + φi)),
and Ei , diag(σ0(Wxi)).
3	Global Convergence of the Gradient Descent Method
In this section, we establish the global convergence results of the gradient descent method in im-
plicit neural networks. In Section 3.1, we first study the dynamics of the prediction induced by the
gradient flow, that is, the gradient descent with infinitesimal step-size. We show that the dynamics
of the prediction is controlled by a Gram matrix whose smallest eigenvalue is strictly positive over
iterations with high probability. Based on the findings in gradient flow, we will show that the ran-
dom initialized gradient descent method with a constant step-size converges to a global minimum at
a linear rate in Section 3.2.
3.1	Continuous time analysis
The gradient flow is equivalent to the gradient descent method with an infinitesimal step-size. Thus,
the analysis of gradient flow can serve as a stepping stone towards understanding discrete-time
gradient-based algorithms. Following previous works on gradient flow of different machine learning
models (Saxe et al., 2013; Du et al., 2019; Arora et al., 2018; Kawaguchi, 2021) the gradient flow of
implicit neural networks is given by the following ordinary differential equations:
dvec (A)	∂L(t)	dvec (W)	∂L(t)	du	∂L(t)	dv	∂L(t)
dt	∂A ， dt	∂ W， dt	∂U ， dt	∂v
where L(t) represents the value of the objective function L(θ) at time t.
(13)
Our results relies on the analysis of the dynamics of prediction y^(t). In particular, Lemma 3.1 shows
that the dynamics of prediction y(t) is governed by the spectral property of a Gram matrix H (t).
Lemma 3.1 (Dynamics of Prediction y(t)). Suppose IlA(S)Il ≤ c√m for all 0 ≤ S ≤ t. Let
X ∈ Rn×d, Φ(s) ∈ Rn×m, and Z(S) ∈ Rn×m be the matrices whose rows are the training data xi,
feature vectors φi, and equilibrium points zi at time S, respectively. With scalar Y , min{Y0, Y0/c}
for any γo ∈ (0,1), the dynamics of prediction τ^(t) is given by
d^ = - [(γ2M(t) + In) ◦ Z(t)Z(t)T + Q(t) ◦ XXT + Φ(t)Φ(t)T] (y - y),
,-H(t)(y - y),
(14)
where ◦ is the Hadamard product (i.e., element-wise product), and matrices M(t) ∈ Rn×n and
Q(t) ∈ Rn×n are defined as follows:
M(t)ij ,UT(Im - YDiA)TDiDT(Im - YDj A)-Tu,
Q(t)ij ,(DT(Im - YDiA)TU + V)T EiET (DT(Im - YDj A)-TU + V).
(15)
(16)
Note that the matrix H(t) is clearly positive semidefinite since it is the sum of three positive semidef-
inite matrices. If there exists a constant λ > 0 such that λmin{H(t)} ≥ λ > 0 for all t, i.e., H(t)
is positive definite, then the dynamics of the loss function L(t) satisfies the following inequality
L(t) ≤ exp{-λt}L(0),
which immediately indicates that the objective value L(t) is consistently decreasing to zero at a
geometric rate. With random initialization, we will show that H(t) is positive definite as long as
the number of parameters m is sufficiently large and no two data points are parallel to each other. In
particular, by using the nonlinearity of the feature map φ, we will show that the smallest eigenvalue
of the Gram matrix G(t) , Φ(t)Φ(t)T is strictly positive over all time t with high probability. As
a result, the smallest eigenvalue of H(t) is always strictly positive.
Clearly, G(t) is a time-varying matrix. We first analyze its spectral property at its initialization.
When t = 0, it follows from the definition of the feature vector φ in Eq. (2) that
1	1m
G(0) = Φ(0)Φ(0)T = — σ(XW (0)T )σ(XW (0)T )T = — ^σ(Xwr (0))σ(Xwr (0))T.
r=1
5
By Assumption 1, each vector wr(0) follows the standard multivariate normal distribution, i.e.,
Wr(0)〜N(0, Id). By letting m → ∞,we obtain the covariance matrix G∞ ∈ Rn×n as follows:
G∞，Ew~N(o,id)[σ(Xw)σ(Xw)T].	(17)
Here G∞ is a Gram matrix induced by the ReLU activation function and the random initialization.
The following lemma shows that the smallest eigenvalue of G∞ is strictly positive as long as no
two data points are parallel. Moreover, later in Lemmas 3.3 and 3.4, we conclude that the spectral
property ofG∞ is preserved in the Gram matrices G(0) and G(t) during the training, as long as the
number of parameter m is sufficiently large.
Lemma 3.2. Assume kxik = 1 for all i ∈ [n]. If xi 6k xj for all i 6= j, then λ0 , λmin{G∞} > 0.
Proof. The proof follows from the Hermite Expansions of the matrix G∞ and the complete proof is
provided in Appendix A.4.	口
Assumption 2 (Training Data). Without loss of generality, we can assume that each xi is normalized
to have a unit norm, i.e., kxik = 1, for all i ∈ [n]. Moreover, we assume xi 6k xj for all i 6= j.
In most real-world datasets, it is extremely rare that two data samples are parallel. If this happens,
by adding some random noise perturbation to the data samples, Assumption 2 can still be satisfied.
Next, we show that at the initialization, the spectral property of G∞ is preserved in G(0) if m
is sufficiently large. Specifically, the following lemma shows that if m = ΩΩ(n2), then G(0) has
a strictly positive smallest eigenvalue with high probability. The proof follows from the standard
concentration bound for Gaussian random variables, and we relegate the proof to Appendix A.5.
Lemma 3.3. Let λo = λmin(G∞) > 0. If m = Ω ^n2 log (δ)), then with probability of at least
1 - δ, it holds that ∣∣G(0) - G∞k2 ≤ λ0, and hence λmin(G(0)) ≥ 3λ°.
During training, G(t) is time-varying, but it can be shown to be close to G(0) and preserve the
spectral property of G∞, if the matrices W (t) has a bounded operator norm and it is not far away
from W (0). This result is formally stated below and its proof is provided in Appendix A.6.
Lemma 3.4. Suppose ∣∣ W(0)k ≤ c√m, and λma{G(0)} ≥ 3λ°. For any matrix W ∈ Rm×d
that satisfies ∣∣W∣∣ ≤ 2c√m and ∣∣W - W(0)∣ ≤ 1√CmXk∣2 ，R, the matrix defined by G，
春σ(XWT)σ(XWT)T satisfies IIG - G(0)∣∣ ≤ 与 and λmin(G) ≥ λ0.
The next lemma shows three facts: (1) The smallest eigenvalue of G(t) is strictly positive for all
t ≥ 0; (2) The objective value L(t) converges to zero at a linear convergence rate; (3) The (operator)
norms of all trainable parameters are upper bounded by some constants, which further implies that
unique equilibrium points in matrices Z(t) always exist. We prove this lemma by induction, which
is provided in Appendix A.7.
Lemma 3.5. Suppose that ∣u(0)k = √m, ∣v(0)k = √m, k W(0)∣ ≤ c√m, k A(0)k ≤ c√m, and
λmin{G(0)} ≥ 4λo > 0. If m = Ω (c2nkχk2 ky(0) - y∣2) and 0 < γ ≤ min{1, 41c}, then for
any t ≥ 0, the following results hold:
⑴ λmin(G(t)) ≥ λ20,
(ii)	ku(t)k≤ 当/ky(0) - yk,
(iii)	kv(t)k≤ 8c√√nky(0) - yk,
(iv)	∣∣W(t)k ≤ 2c√m,
(v)	∣A(t)∣ ≤ 2c√m,
(vi)	l∣y(t) - y∣2 ≤ eχp{-λot}∣y(0) - y∣2.
By using simple union bounds in Lemma 2.1 and 3.5, we immediately obtain the global convergence
result for the gradent flow as follows.
6
Theorem 3.1 (Convergence Rate of Gradient Flow). Suppose that Assumptions 1 and 2 hold. If
2
we set the number of parameter m
0
and choose 0 < γ ≤ min{ 1, 4C }, then with
probability at least 1 - δ over the initialization, we have
ky(t) - yk2 ≤ eχp{-λ0t}ky(0) - yk2, ∀t ≥ 0.
This theorem establishes the global convergence of the gradient flow. Despite the nonconvexity of
the objective function L(θ), Theorem 3.1 shows that if m is sufficiently large, then the objective
value is consistently decreasing to zero at a geometric rate. In particular, Theorem 3.1 requires
m = Ω(n2), which is similar or even better than recent results for the neural network With finite
layers (Nguyen & Mondelli, 2020; Oymak & Soltanolkotabi, 2020; Allen-Zhu et al., 2019; Zou &
Gu, 2019; Du et al., 2019). In particular, previous results showed that m is a polynomial of the
number of training sample n and the number of layers h, i.e., m = Ω(nαhβ) with a ≥ 2 and
β ≥ 12 (Nguyen & Mondelli, 2020, Table 1). These results do not apply in our case since we have
infinitely many layers. By taking advantage of the nonlinear feature mapping function, we establish
the global convergence for the gradient flow with m independent of depth h.
3.2 Discrete time analysis
In this section, we show that the randomly initialized gradient descent method with a fixed step-
size converges to a global minimum at a linear rate. With similar argument used in the analysis
of gradient flow, we can show the (operator) norms of the training parameters are upper bounded
by some constants. It is worth noting that, unlike the analysis of gradient flow, we do not have an
explicit formula for the dynamics of prediction y(t) in the discrete time analysis. Instead, We have to
show the difference between the equilibrium points Z(k) in two consecutive iterations are bounded.
Based on this, we can further bound the changes in the predictions y(k) between two consecutive
iterations. Another challenge is to show the objective value consistently decreases over iterations.
A general strategy is to show the objective function is (semi-)smooth with respect to parameter θ,
and apply the descent lemma to the objective function (Nguyen & Mondelli, 2020; Allen-Zhu et al.,
2019; Zou & Gu, 2019). In this section, we take advantage of the nonlinear feature mapping function
in Eq. (4). Consequently, we are able to obtain a Polyak±OjaSieWiCz-Iike condition (Karimi et al.,
2016; Nguyen, 2021), which allows us to provide a much simpler proof.
The following lemma establishes the global convergence for the gradient descent method with a
fixed step-size when the operator norms of A(0) and W (0) are bounded and λmin(G(0)) > 0. The
proof is proved in Appendix A.8.
Lemma 3.6 (Gradient Descent Convergence Rate). Suppose ∣∣u(0)k = √√m, ∣∣v(0)k = √√m,
∣∣W(0)k ≤ c√m, ∣A(0)k ≤ c√m, and λmin{G(0)} ≥ ∣λo > 0. If 0 < γ ≤ min{ 1, -1c},
m = Ω (C nkχk ∣∣y(0) — y∣∣2), and stepsize α = O (λ0∕n2), then for any k ≥ 0, we have
(i)	λmin(G(k)) ≥ 乎,
(ii)	ku(k)k≤ 32C卢ky(0) - yk,
(iii)	kv(k)k≤ 竺争ky(0) - yk,
(iv)	kW(k)k ≤ 2c√m,
(v)	∣∣A(k)k ≤ 2c√m,
(vi)	ky(k) - yk2 ≤ (1 - αλo∕2)k∣∣y(0) - y∣∣2.
By using simple union bounds to combine Lemma 2.1 and 3.6, we obtain the global convergence
result for the gradient descent.
Theorem 3.2 (Convergence Rate of Gradient Descent). Suppose that Assumption 1 and 2 hold. If
we set m = Ω log (δ)), 0 < Y ≤ min{2, -1c}, and choose step-size α = O (λ0∕n2), then
with probability at least 1 - δ over the initialization, we have
ky(k) - yk2 ≤ (1 - αλo∕2)kky(0) - y∣2,	∀k ≥ 0.
7
train loss
——WIdtħ 58
---width 1000
——Wldtħ 28。
——width 4000
-1.0 -
test loss
——wrtdth500
---WMthjK)8
——WldthZOOO
——width 4000
(a) Training loss
operator norm of layer
,---width 500
width 1000
——width 2000
——width 4000
0.1992
0.1990
0.19βB
0.19β6
0.1SB4
0.1SB2
0.19β0
£。U-OIe-9d。
(b) Test loss	(c) Operator norms
(d) Training loss
CsSOnEcl
(e) Test loss
0	200	400	«0 SOO lβ∞
epoch
(f) Operator norms
CsSOnEcl
(g) Training loss
test loss
-BO=WO-
0.1992
0.1990
C O∙lSβB -
t
⅞ 0.1Sβ6
0.1SB4
0.1Sβ2
0.1S80
operator norm of layer
‘「僧 ΛJ"ΠI■邢WlUWrrHiMM(WR l”pf∣⅛WVιfl _ WHm IoOO
——Wldtħ 208
0	200	400	«0 SOO lβ∞
epoch
(h) Test loss	(i) Operator norms
train loss
0	200	400 SOO 800	1000
epoch
(j) Training loss
test loss
0	200	400	«0 SOO IOOO
epoch
O 200	400	«0 SOO lβ∞
epoch
(k) Test loss	(l) Operator norms


Figure 1: Results on MNIST, FashionMNIST, CIFAR10, and SVHN. We evaluate the impact of the
width m on the training loss, test loss, and operator norm of the scaled matrix (γ∕√m)A(k) on four
real datasets.
4	Experimental Results
In this section, we use real-world datasets MNST, FashionMNST, CIFAR10, and SVHN to evalu-
ate our theoretical findings. We initialize the entries of parameters A, W, u, and v by standard
Gaussian or symmetric Bernoulli distribution independently as suggested in Assumption 1. For
each dataset, we only use classes 0 and 1, and 500 samples are randomly drawn from each class
to generate the training dataset with n = 1000 samples. All data samples are converted to gray
scale and resized into a 28 × 28 pixel image. We also normalize each data to have unit norm. If
two parallel samples are observed, we add a random Gaussian noise perturbation to one of them.
Thus, Assumption 2 is also satisfied. We run 1000 epochs of gradient descent with a fixed step-size.
We test three metrics with different width m. We first test how the extent of over-parameterization
8
affects the convergence rates. Then, we test the relation between the extent of over-parameterization
and the operator norms between matrix A(k) and its initialization. Note that the “operator norm” in
the plots denotes the operator norm of the scaled matrix (γ∕√m)k A(k)k. Third, We test the extent
of over-parameterization and the performance of the trained neural network on the unseen test data.
Similar to the training dataset, We randomly select 500 samples from each class as the test dataset.
From Figure 1, the figures in the first column show that as m becomes larger, we have better conver-
gence rates. The figures in the second column show that as m becomes larger, the neural networks
achieve lower test loss. The figures in the third column show that the operator norms are slightly
larger for larger m but overall the operator norms are approximately equal to its initialization. The
bell curve from the classical bias-variance trade-off does not appear. This opens the door to anew re-
search direction in implicit neural networks in the analyses of generalization error and bias-variance
trade-off.
5	Related works
Implicit models has been explored explored by the deep learning community for decades. For ex-
ample, Pineda (1987) and ALMEIDA (1987) studied implicit differentiation techniques for training
recurrent dynamics, also known as recurrent back-propagation (Liao et al., 2018). Recently, there
has been renewed interested in the implicit models in the deep learning community (El Ghaoui
et al., 2019; Gould et al., 2019). For example, Bai et al. (2019) introduces an implicit neural net-
work called deep equilibrium model for the for the task of sequence modeling. By using implicit
ODE solvers, Chen et al. (2018) proposed neural ordinary differential equation as an implicit resid-
ual network with continuous-depth. Other instantiations of implicit modeling include optimization
layers (Djolonga & Krause, 2017; Amos & Kolter, 2017), differentiable physics engines (de Avila
Belbute-Peres et al., 2018; Qiao et al., 2020), logical structure learning (Wang et al., 2019), and
continuous generative models (Grathwohl et al., 2019).
The theoretical study of training finite-layer neural networks via over-parameterization has been an
active research area. Jacot et al. (2018) showed the trajectory of the gradient descent method can
be characterized by a kernel called neural tangent kernel for smooth activation and infinitely width
neural networks. For a finite-width neural network with smooth or ReLU activation, Arora et al.
(2019); Du et al. (2019); Li & Liang (2018) showed that the dynamics of the neural network is gov-
erned by a Gram matrix. Consequently, Zou et al. (2020); Du et al. (2019); Allen-Zhu et al. (2019);
Nguyen & Mondelli (2020); Arora et al. (2019); Zou & Gu (2019); Oymak & Soltanolkotabi (2020)
showed that (stochastic) gradient descent can attain global convergence for training a finite-layer
neural network when the width m is a polynomial of the sample size n and the depth h. However,
their results cannot be applied directly to implicit neural networks since implicit neural networks
have infinite layers and the equilibrium equation may not be well-posed. Our work establishes the
well-posedness of the equilibrium equation even if the width m is only square of the sample size n.
6	Conclusion and Future Work
In this paper, we provided a convergence theory for implicit neural networks with ReLU activation
in the over-parameterization regime. We showed that the random initialized gradient descent method
with fixed step-size converges to a global minimum of the loss function at a linear rate if the width
m = Ω(n2). In particular, by using a fixed scalar Y ∈ (0,1) to scale the random initialized weight
matrix A, we proved that the equilibrium equation is always well-posed throughout the training. By
analyzing the gradient flow, we observe that the dynamics of the prediction vector is controlled by a
Gram matrix whose smallest eigenvalue is lower bounded by a strictly positive constant as long as
m = Ω(n2). We envision several potential future directions based on the observations made in the
experiments. First, we believe that our analysis can be generalized to implicit neural networks with
other scaling techniques and initialization. Here, we use a scalar γ∕√m to ensure the existence of
the equilibrium point z* with random initialization. With an appropriate normalization, the global
convergence for identity initialization can be obtained. Second, we believe that the width m not
only improves the convergence rates but also the generalization performance. In particular, our
experimental results showed that with a larger m value, the test loss is reduced while the classical
bell curve of bias-variance trade-off is not observed.
9
Acknowledgments
This work has been supported in part by National Science Foundation grants III-2104797,
DMS1812666, CAREER CNS-2110259, CNS-2112471, CNS-2102233, CCF-2110252, CNS-21-
20448, CCF-19-34884 and a Google Faculty Research Award.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252. PMLR, 2019.
LB ALMEIDA. A learning rule for asynchronous perceptrons with feedback in a combinatorial
environment. In Proceedings, 1st First International Conference on Neural Networks, volume 2,
pp. 609-618. IEEE, 1987.
Brandon Amos and J Zico Kolter. Optnet: Differentiable optimization as a layer in neural networks.
In International Conference on Machine Learning, pp. 136-145. PMLR, 2017.
Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient
descent for deep linear neural networks. arXiv preprint arXiv:1810.02281, 2018.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. arXiv preprint arXiv:1904.11955, 2019.
Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Trellis networks for sequence modeling. arXiv
preprint arXiv:1810.06682, 2018.
Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. arXiv preprint
arXiv:1909.01377, 2019.
Shaojie Bai, Vladlen Koltun, and J Zico Kolter. Multiscale deep equilibrium models. arXiv preprint
arXiv:2006.08656, 2020.
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differ-
ential equations. arXiv preprint arXiv:1806.07366, 2018.
Raj Dabre and Atsushi Fujita. Recurrent stacking of layers for compact neural machine translation
models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 6292-
6299, 2019.
Filipe de Avila Belbute-Peres, Kevin Smith, Kelsey Allen, Josh Tenenbaum, and J Zico Kolter. End-
to-end differentiable physics for learning and control. Advances in neural information processing
systems, 31:7178-7189, 2018.
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Eukasz Kaiser. Universal
transformers. arXiv preprint arXiv:1807.03819, 2018.
Josip Djolonga and Andreas Krause. Differentiable learning of submodular models. Advances in
Neural Information Processing Systems, 30:1013-1023, 2017.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-
1685. PMLR, 2019.
Laurent El Ghaoui, Fangda Gu, Bertrand Travacca, Armin Askari, and Alicia Y Tsai. Implicit deep
learning. arXiv preprint arXiv:1908.06315, 2, 2019.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256. JMLR Workshop and Conference Proceedings, 2010.
Stephen Gould, Richard Hartley, and Dylan Campbell. Deep declarative networks: A new hope.
arXiv preprint arXiv:1909.04866, 2019.
10
Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. Ffjord:
Free-form continuous dynamics for scalable reversible generative models. International Confer-
ence on Learning Representations, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. arXiv preprint arXiv:1806.07572, 2018.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the Polyak-IojasieWicz condition. In Joint European Conference on Ma-
chine Learning and Knowledge Discovery in Databases, pp. 795-811. Springer, 2016.
Kenji KaWaguchi. On the theory of implicit deep learning: Global convergence With implicit layers.
arXiv preprint arXiv:2102.07346, 2021.
ErWin Kreyszig. Introductory functional analysis with applications, volume 1. Wiley NeW York,
1978.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural netWorks via stochastic gradient
descent on structured data. arXiv preprint arXiv:1808.01204, 2018.
Renjie Liao, YuWen Xiong, Ethan Fetaya, Lisa Zhang, KiJung Yoon, Xaq PitkoW, Raquel Urta-
sun, and Richard Zemel. Reviving and improving recurrent back-propagation. In International
Conference on Machine Learning, pp. 3082-3091. PMLR, 2018.
Barbara MacCluer. Elementary functional analysis, volume 253. Springer Science & Business
Media, 2008.
Quynh Nguyen. On the proof of global convergence of gradient descent for deep relu netWorks With
linear Widths. arXiv preprint arXiv:2101.09612, 2021.
Quynh Nguyen and Marco Mondelli. Global convergence of deep netWorks With one Wide layer
folloWed by pyramidal topology. arXiv preprint arXiv:2002.07867, 2020.
Samet Oymak and Mahdi Soltanolkotabi. ToWard moderate overparameterization: Global con-
vergence guarantees for training shalloW neural netWorks. IEEE Journal on Selected Areas in
Information Theory, 1(1):84-105, 2020.
Fernando Pineda. Generalization of back propagation to recurrent and higher order neural netWorks.
In Neural information processing systems, pp. 602-611, 1987.
Yi-Ling Qiao, Junbang Liang, Vladlen Koltun, and Ming C Lin. Scalable differentiable physics for
learning and control. arXiv preprint arXiv:2007.02168, 2020.
AndreW M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural netWorks. arXiv preprint arXiv:1312.6120, 2013.
Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Sci-
ence. Number 47 in Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge
University Press, 2018. ISBN 978-1-108-41519-4.
Po-Wei Wang, Priya Donti, Bryan Wilder, and Zico Kolter. Satnet: Bridging deep learning and log-
ical reasoning using a differentiable satisfiability solver. In International Conference on Machine
Learning, pp. 6545-6554. PMLR, 2019.
Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural
netWorks. arXiv preprint arXiv:1906.04688, 2019.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-
parameterized deep relu netWorks. Machine Learning, 109(3):467-492, 2020.
11
A Appendix
A.1 proof of Lemma 2.2
Proof. Let γo ∈ (0,1). Set Y，min{γ0, γ0∕c}. Denote Y = γ∕√m. Then
I∣z'+1 - z' I I = I I σ (YAN' + φ) - σ (YANgT + φ川
≤Y I I Azg - Azg-1 11 , σ is I-LiPschitz continuous
=YIIa(n' - z'-1)II
≤Y∣∣Akkzg-zgτ∣∣,
≤γc√mkzg - ZgTk,	∣∣A∣∣ ≤ c√m
=γ0kzg-zgτ∣∣.
Applying the above argument' times, We obtain
l∣z'+1 - z'k ≤ γgl∣z1 - z0k = YgkZIIl = Y0kσ(φ)k ≤ Y0llφk,
where we use the fact z0 = 0. For any positive integers p, q with P ≤ q, we have
忖-Zqk ≤kzp- zp+1 k + …+ 忖T- Zqk
≤Yokφk + ∙∙∙ + Yo llφk
≤Yokφk (1 + Y0 + Y2 +--)
=4阚.
1 - Y0
Since Y0 ∈ (0,1), we have ∣∣zp - Zqk → 0 as P → ∞. Hence, {z0}* is a Cauchy sequence.
Since Rm is complete, the equilibrium point z* is the limit of the sequence {zg}∞=ι, so that Z exists
and is unique. Moreover, let q → ∞, then we obtain ∣∣zp - z* ∣ ≤ ɪɪ^∣∣φ∣, so that the fixed-point
iteration converges to z linearly.
Letp = 0 and q = ', then we obtain ∣∣zg∣ ≤ ɪ-1^-∣∣φ∣.
□
A.2 Proof of Lemma 2.3
Proof. (i) To simplify the notations, we denote D ，diag(σz(5Az + φ)), and E
diag(σ0(Wx)). The differential of f is given by
df =d(z — Yσ(Y Az + φ))
=dz — Dd(YAz + φ)
=[Im — 5DA] dz — 7D(dA)z — Ddφ.
Taking vectorization on both sides yields
vec (df) = [Im - 7DA] vec (dz) - vec (YDdAz) - Dvec (dφ)
=[Im — γDA] vec (dz) — γ[zτ 0 D]vec (dA) — Dvec (dφ).
Therefore, the partial derivative of f with respect to z, A, and φ are given by
I 1-ɪ
A] D
,D0
-7K r∙
[Im -YY -D
===
12
It follows from the definition of the feature vector φ in Eq. (2) that
dφ = ɪdσ(Wx)	= ɪE(dW)x	= ɪ	[xT	乳 E]	vec (dW).
-/ ʌm	- / ʌm	- / ʌm	L	」
Thus, the partial derivative of φ with respect to W is given by
∂φ	1	「T	-yT
∂W = √m [x 乳 © .
(18)
By using the chain rule, we obtain the partial derivative of f with respect to W as follows
∂f _ ∂φ ∂f
---=----:-
∂W ∂W ∂φ
√m [xT ㊈ MT DT
—
(ii)	Let v be an arbitrary vector, and u be an arbitrary unit vector. The reverse triangle inequality
implies that
Il(Im - 7diag(σ0(v))A) Uk ≥∣∣u∣∣ -叶diag(σ0(v))Au∣∣
≥kuk - 7kdiag(σ'(v))kkAkkUk
(a)
≥ (1 - Y0)kuk
=1 - Y0 > 0,
where (a) is due to ∣σ0(v)∣ ≤ 1 and kAkop ≤ c√m. Therefore, taking infimum on the
left-hand side over all unit vector u yields the desired result.
(iii)	Since f (z*, A, W) = 0, taking implicit differentiation of f with respect to A at z* gives us
∂z
∂A
∂f
∂ Z
*) + (⅛f
z=z*	∖
z=z*
0.
The results in part (i)-(ii) imply the smallest eigenvalue of ∂f 1* is strictly positive, so that it
is invertible. Therefore, we have
∂z*
∂A
—
z=z*
-1
=Y [zT ㊈ D]t [Im - γDA]-τ .	(19)
Similarly, we obtain the partial derivative of z* with respect to W as follows
∂ Z*
∂W
—
z=z*
∂f
∂z
一 ι 1
=ɪ [xT 乳 E]t Dt [Im - YDA]-t . (20)
To further simplify the notation, we denote z to be the equilibrium point z * by omitting the
superscribe, i.e., Z = z*. Let y = uTZ + vTφ be the prediction for the training data (x, y).
The differential of y is given by
dy = d (UTZ + vTφ) = uTdz + zdu + vTdφ + φTdv.
The partial derivative of y with respect to u, v, z, and φ are given by
rʌ ʌ	rʌ ʌ	rʌ ʌ	rʌ ʌ
∂y _	∂y	_	∂y	_	∂y	_
瑟=u,	∂u	= z,而=φ,	∂φ	= v.
Let' = 1 (y - y)2. Then ∂`∕∂y = (y - y). By chain rule, we have
(21)
∂'
∂ u
∂'
∂φ
∂y ∂'
∂u ∂y
∂y ∂'
∂v ∂y
z(y - y),
(22)
(23)
φ(y - y).
By using (19)-(20) and chain rule, we obtain
∂'	∂z ∂'
——-=——:—:—
∂A ∂A ∂z
∂z ∂y 班	、「T.n]T「r	~∏4i-τ
=∂a ∂z 诙=Y(y- y)1z 乳 DJ [Im- YDAi	u，
(24)
13
and
	d` _ ∂z ∂y d` ∂φ ∂y d` 	=	——-- +	——-- ∂W	∂W ∂z ∂y + ∂W ∂φ ∂y = ^=(y - y)[xτ 0 E]τ [Dτ(Im - γDA)-τU + v] .	(25)
Since L = Pn=I' with ' = '(yi, %), we have dL = Pn=I d' and ∂L∕∂' = 1. Therefore,
we obtain
∂L 	二 ∂A	nn =X W = X Y(yi - yi) [zτ 0 Di]τ [Im - YDiA]-τ U,	(26) i=1	i=1
∂L ∂W	二 ∂八	J.L 1	„	„	一 =X	∂W = X √m (yi-yi)[xi	0	Eii	[Di(Im	-	YDiA)-	U + v] ,	(27) i=1	i=1
∂L --二 ∂u	n ∂'∙	n ：X ∂U = X(yi-yi)zi,	(28) i=1 U	i=1
∂L ——= ∂v	二 ∂九 ʌ X ∂v = X(yi-yi)φi.	(29) i=1	i=1
□
A.3 PROOF OF LEMMA 3.1
Proof. Let Zi denote the i-th equilibrium point for the i-the data sample Xi. By using (19), (20),
(26) and (27), we obtain the dynamics of the equilibrium point Zi as follows
dzi ---= dt	(∂zi λτ dvec (A)	( ∂ Zi ∖τ dvec (W) =∖Aa) -dt- + ∖Wv) -dt- JdZi ∖τ( ∂L∖	∂ ∂zi∖τ f	∂L∖ - 1∂aJ I-∂A) + IdWJ I-∂W) n =-Y2 X(yj - yj)[Im - YDiA]-1 [zT 0 Di] [zT 0 Dj]τ [Im - YDjA]-τ U j=1
1n
---£(% - yj) [Im - yDiA]-1 Di [xT 0 Ei] [xT 0 Ej]t [Dt(Im - YDj A)-Tu + v
m j=ι
n
-Y2 X(yj - yj)[Im - YDiA]-1 DiDT [Im - YDj A]-t UZTZj
j=i
1n
--]Γ(yj - yj)[Im - YDiA]-1 DiEiET [DT(Im - YDjA)-tU + v] xTXj.
m j=ι
By using (18) and 27, we obtain the dynamics of the feature vector φi
j=1
—YDj A)-t u + v]xτ Xj.
m
14
By chain rule, the dynamics of the prediction yi is given by
dy	=( ∂^ AT dzi	+ (∂y^L ∖T	dΦi	+ (∂y^i ∖t du	+ (∂a ∖t dv
dt I ∂zi) dt ∖ ∂φi)	dt ∖ ∂u) dt IdvJ dt
n
=-Y2 X(yj - Vj ) [UT (Im - ^DiA)TDiDT(Im - YDj A)-T u](ZT Zj )
j=1
1n	T
—	-X(Vj	—	Vj)	[(DT(Im	- YDiA)IU + v)	EiET(DT(Im	- YDjA)	TU + v)]	(xTXj)
m j=1
n
-	X(yj - Vj)(ZTZj)
j=1
n
-	X(Vj -Vj )(φT φj).
j=1
Define the matrices M(t) ∈ Rn×n and Q(t) ∈ Rn×n as follows
M(t)ij ,mUT(Im - YDiA)-1 DiDj(Im - γDjA)-Tu,
Q(t)ij ,m (DT(Im - YDiA)TU + V)T EiET (DT(Im - YDjA)-TU + V).
Let X ∈ Rn×d, Φ(t) ∈ Rn×m, and Z(t) ∈ Rn×m be the matrices whose rows are the training
data xi , feature vectors φi , and equilibrium points zi at time t, respectively. The dynamics of the
prediction vector y^ is given by
dt = - [(γ2M(t) + In) ◦ Z(t)Z(t)T + Q(t) ◦ XXT + Φ(t)Φ(t)T] (y(t) - y).
□
A.4 Proof of Lemma 3.2
A.4. 1 Review of Hermite Expansions
To make the paper self-contained, we review the necessary background about the Hermite polyno-
mials in this section. One can find each result in this section from any standard textbooks about
functional analysis such as MacCluer (2008); Kreyszig (1978), or most recent literature (Nguyen &
Mondelli, 2020, Appendix D) and (Oymak & Soltanolkotabi, 2020, Appendix H).
We consider an L2-space defined by L2(R, dP), where dP is the Gaussian measure, that is,
_x2
:	2
dP = p(x)dx, where p(x) =	_e
2π
Thus, L2(R, dP) is a collection of functions f for which
|f (x)|2 dP (x) =	|f(x)|2p(x)dx
=Ex〜N(0,1) If(X)I < ∞.
-∞	-∞
Lemma A.1. The ReLU activation σ ∈ L2 (R, dP ).
Proof. Note that
Z∞∞
∣σ(x)∣2p(x)dx ≤ /	∣x∣2p(x)dx = Ex〜N(0,1)|x|2
∞	-∞
Var(x) = 1.
□
15
For any functions f, g ∈ L2(R, dP), we define an inner product
hf, gi :
Z∞
∞
f (x)g(x)dP (x)
Z∞
∞
f (χ)g(χ)p(χ)dχ = Ex 〜N (0,ι)[f (X)g(X)].
Furthermore, the induced norm k ∙ k is given by
kfk2=hf,fi=	∞ |f (X)|2 dP (X)
=Ex〜N(0,1) |f (X)| .
-∞
This L2 space has an orthonormal basis with respect to the inner product defined above, called
normalized probabilist’s Hermite polynomials {hn(X)}n∞=0 that are given by
hn(x)=二(-1)nex2/2Dn(e-x2/2),	where Dn(e-x2/2) = dn- e-x2/2.
n!	dXn
Lemma A.2. The normalized probabilist’s Hermite polynomials is an orthonormal basis of
L (R, dP): hhm, hni = δmn.
Proof. Note that Dn(e-x2/2) = e-x2/2Pn(X) for a polynomial with degree of n and leading term
is (-1)nxn. Thus, we can consider hn(x) = √= (-1)nPn(x).
Assume m < n
hhn,hm i =Ex 〜N (0,1)[hn(x)hm(x)]
=I	hn (x) hm (X)-/	e / dx,
-∞	2π
1
ʌ/2nʌ/n!
Z∞
∞
rewrite hn(X) by its definition
Z∞
∞
k 二 Γ-∖ (-1)2n+m ZX e-x2/2Dn[Pm(x)]dx,
2π n!	m!	-∞
rewrite hm by the polynomial form
integration by parts n times
1
∖p2∏∖pn ʌ/m!
There is no boundary terms because the super exponential decay of e-x2/2 at infinity. Since m<n,
then Dn(Pm) = 0 so that hhm, hni=0. If m=n, then Dn(Pm) = (-1)nn!. Thus, hhn, hni =
1.	□
Remark: Since {hn} is an orthonormal basis, for every f ∈ L2(R, dP), we have
∞
f(X) = X hf,hni hn(X)
n=0
in the sense that
2
lim
N→∞
N
f(X) - X hf, hni hn(X)
n=0
Nim∞ Ex~N (0,1)
N
f(X) - X hf, hni hn(X)
n=0
2
=0
Lemma A.3. f ∈ L2(R, dP ) if and only if Pn∞=0 |hf, hni|2 < ∞.
16
Proof. Note that
hf, f = [ If(x)l2 dP(x)
J -∞
Z ∞ / ∞	∖ ( ∞	ʌ
Ehf,Mhi(χ)	Ehfhjihj(x)dp(x)
-∞ V=0	) ∖j=o	)
∞	f∞
=E hf,hi>hf,hj /	hi(x)hj(x)dP(x)
i,j=0	'-∞
∞
X lhf,hii∣2.
i=1
□
Lemma A.4. Consider a Hilbert space H with inner product《，•》. If ∣∣ fn - f ∣∣ → 0 and ∣∣gn - g∣∣ →
0, then hf,gi = Iimn→∞ hfn, gni .
Proof. Observe that
lhf, gi - hfn, gnil ≤ lhf, gi - hfn, 9)∖ + "n, gi - hfn, 9ni∖
≤kf k∣g - gnk + kfnkkg-gn∣.
Let n → ∞, then the continuity of ∣∙ ∣∣ implies the desired result.
□
Lemma A.5. Let {hn(x)} be the normalized probabilist,s Hermite polynomials. For any fixed
number t, we have
9	∞ tn
X X -¼hn(x).
n=0 ^n!
(30)
Proof. First, we show f (x) = ext-t2/2 ∈ H，L2(R, dP).
hf,fi =Eχ~N(0,1) lf(x)l2
=「 e2xt-t2 ɪe-x2∕2dx
J-∞	V 2π
t2 ∕∞	1
e L∞ √2πexp
e" < ∞.
—
一} dx,
X ~ N(2t, 1)
Thus f (x) ∈ H. Then f (x) = P∞0 hf, hQ hn(x). Note that
hf, %〉=Eχ~N(0,1)[f (x)hn(x)]
=/∞ ex』2 ∙ √L(-1)neχ2∕2Dn(e-χ2/2) ∙ √Ue
-x2/2dx
1	1	∕,∞ 一 上2 ∕C	_2 ∕c
√== (-1)n√= J	ext t /2 ∙ Dn(e x /2)dx, integration by parts n times
-1产
∙∞
-∞
In 1	/∞ eχt-t2∕2tn ∙ e-χ2∕2dx
V 2π J-∞
——_e-(X-t)2∕2dx,	X ~ N(t, 1)
□
17
Lemma A.6. Let a, b ∈ Rd with kak = kbk = 1, then
Ew〜N(0,Id) [hn(ha, wi)hm(hb, Wi)] = ha, bi δmn∙
Proof. Given fixed numbers s and t, we define two functions f (W) = eha,wit-t2/2 and g(W)
ehb,wis-
s2/2 . Let x	= ha, Wi and y	= hb, wi. Then we have
f(W) =	eha,wit-t2/2 = e	∞n	∞ …=X ∖hn(x) = X n=0 n!	n=0
g(W) =	ehb,wis-s2/2 = e	∞n	∞ ys-s2/2 = X √n %(y) = X n=0 n	n=0
sn
√n! hn(hb, W〉).
tn
—=hn(ha, Wi),
n!
Define a Hilbert space Hd = L2 (Rd, dP ), where dP is the multivariate Gaussian measure,
equipped with inner product hf,gi ，Ew〜N(o,Id)[f (W)g(W)]. Clearly, f,g ∈ Hd. Definese-
quences {fN } and {gN} as follows
N tn	N sn
fN(W) = E -ɜhn(ha, Wi) and gN(W) = E √≡hn(hb, w〉).
n=0 n!	n=0 n!
Since kf - fNk → 0 and kg - gNk → 0, we have
Ew 〜N (0,id)[f (w)g(w)] = hf,gi
= lim hfN, gNi
N→∞
=lim Ew〜N(0,Id)[fN(w)gN(w)]
N→∞
N
= lim X
N→∞
n,m=0
Note that the LHS is also given by
tnsm
√n! √m!
Ew 〜N (0,Id)[hn (ha, wi)gn(hb, w〉)]
EsN (0,id) [f (W)g(W)] =e-t2 IJs 2/EsN (0,Id) [eha,wit+hb,wis]
=e-t2/2-s2/2Ew〜N(o,Id) [ePd=ι wGit+biS)]
d
=e-t2/2-s2/2 Y Ewi〜N(0,1) [ewi(ait+bis)]
i=1
d
=e-t2/2-s2/2YMwi(ait+bis)
i=1
eha,bist
∞
X
ha, bin (st)n
n!
n=0
Since s and t are arbitrary numbers, matching the coefficients yields
Ew 〜N (0,Id) [hn(ha, Wi)hm(hb, Wi)] = ha, bi δmn .
□
A.4.2 LOWER BOUND THE SMALLEST EIGENVALUES OF G∞
The result in this subsection is similar to the results in (Nguyen & Mondelli, 2020, Appendix D) and
(Oymak & Soltanolkotabi, 2020, Appendix H). The key difference is the assumptions made on the
training data. In particular, Oymak & Soltanolkotabi (2020) assumes the training data is δ-separable,
i.e., min{kxi - xj k, kxi + xj k} ≥ δ > 0 for all i 6= j, and Nguyen & Mondelli (2020) assumes
the data xi follows some sub-Gaussian random variable, while we assume no two data are parallel
to each other, i.e., xi 6k xj for all i 6= j.
18
Lemma A.7. Given an activation function σ, if σ ∈ L2(R, dP) and kxi k = 1 for all i ∈ [n], then
∞
G∞ = X lhσ, hki∣2 (.
k=0	|
XX T ◦•••◦ XX T),
'	_ 一 一	J
(31)
^^{^^™
k times
where ◦ is elementwise product.
Proof. Observe
G∞ =Ew~N(O,Id) [σ(hw, Xii)σ(hw, Xj〉)]
∞
=E hσ, hki hσ, h`i E®~n(0,3 [hk(hw, XiY)h`(hw, Xji)]
k,'=0
∞
=E hσ, hki hσ, h`i ∙ hxi, Xjik δke
k,'=0
∞
=X hσ,hki2 hXi,Xjik
k=0
□
Note that the tensor product of Xi and Xi is Xi 0 Xi ∈ R* ×1, so that
hXi, Xjik
(Xi 0∙∙∙0 Xi, Xj 0∙∙∙0 Xj)
' k times	k times
Here We introduce the (row-wise) Khatri-RaO product of two matrices A ∈ Rk×m, B ∈ Rk×n.
Then
Aι* 0 Bι*
A * B =	.	∈ Rk×mn
.
Ak* 0 Bk*
where Ai* indicates the i-th row of matrix A. Therefore, the i-th row of X * ∙∙∙ * X，X*n is
Xi 0∙∙∙0 Xi. As a result, we obtain a more compact form of (31) as follows
∞
G∞ = X ∣hσ,hki∣2 (X*k)(X*k)T.	(32)
k=0
Lemma A.8. If σ(x) is a nonlinear function and ∣σ(x) | ≤ |x| and, then
sup{n : hσ, hni > 0} = ∞.
Proof. It is equivalent to show σ(x) is not a finite linear combination of polynomials. We prove
by contradiction. Suppose σ(x) = a0 + aιx + •… + anxn. Since σ(0) = 0 = a0, then σ(x)=
aιX +-----+ anxn. Observe that
∣σ(x)∣	∣aι x +---+ anXn∣
lim	= lim -------------------
x→∞ |x| x→∞	|x|
=lim lai + …+ anXn-11 ,
x→∞
∞
which contradicts 誓 ≤1 for allx = 0.	口
Lemma A.9. If Xi 6k Xj for all i 6= j, then there exists k0 > 0 such that λmin (X*k)(X*k)T > 0
for all k ≥ k0. Therefore, λmin(G∞) > 0.
19
Proof. To simplify the notation, denote K = (X*k)T ∈ Rkd×n. Since Xi kXj and 此/| = 1, then
let δ，max{∣hxi, Xji∣} = max{∣cos θj∣} and δ ∈ (0,1), where θj is the angle between Xi and
xj . For any unit vector v ∈ Rn , we have
n 2
VT (X *k)(X *k )t V =kKvk2 = X Vi K*i
i=1
nn
=E Evivjhκ*i, Kj i
i=1 j=1
nn
=	vivj hXi, Xji
i=1 j=1
n
=Xvi2kXik2k +Xvivj hXi,Xjik
i=1	i6=j
=1 +	vivj hXi, Xjik ,
i6=j
where the last equality is because kXik = 1 and kVk= 1. Note that
vivj hXi,Xjik ≤	|vi| |vj| |hXi,Xji|k
i6=j	i6=j
≤δkX |vi| |vj|, by lhχi,Xji| ≤ δ
i6=j
≤δk Xn |vi|!
≤nδk , by Cauchy-Schwart’s inequlity.
By inverse triangle inequality, we have
kKVk2 ≥ 1 - nδk.
Choose ko ≥ logn/ log(1∕δ), then λmin{(X*k)(X*k)T} > 0 for all k ≥ k0.
□
A.5 Proof of Lemma 3.3
Proof. Since ∣∣Xi∣∣ = 1 and Wr (0)〜N(0, Id), We have XTWr (0)〜N(0,1) for all i ∈ [n]. Let
Xir，σ [xTWr (0)] and Z 〜N(0,1), then for any ∣λ∣ ≤ 1∕√2, we have
Eexp{X2,λ2} = Eexp{σ [xtWr(0)] 2 λ2} ≤ Eexp{Z2λ2} = 1/p1 - 2λ2 ≤ e2tt,
where the first inequality is due tot ∣σ(x) | ≤ |x|, and the last inequality is by using the numerical
inequality 1/(1 - x) ≤ e2x. Choose λ ≤ (log √2),2, we obtain E{X2rλ2} ≤ 2. By using
Markov’s inequality, we have for any t ≥ 0
P{∣Xir | ≥ t} = P n |Xir |2 /log √2 ≥ t2/ log √2} ≤ 2exp {-12 log √2} ≤ 2exp {—12∕4}.
Therefore Xir is a sub-Gaussian random variable with sub-Gaussian norm kXi kψ2 ≤ 2 (Vershynin,
2018, Proposition 2.5.2). Then XirXjr is a sub-exponential random variable with sub-exponential
norm kXirXjrkψ1 ≤ 4 (Vershynin, 2018, Lemma 2.7.7). Observe that
1m
Gij(0) = Φi(0)TΦj(0) = 一 Xσ [xtWr (0)] σ [xtWr (0)]
m
r=1
1m
—〉JXir Xjr.
m
r=1
20
Since Gi∞j = E [Gij (0)], (Vershynin, 2018, Exercise 2.7.10) implies that Gij (0) - Gi∞j is also a
zero-mean sub-exponential random variable. It follows from the Bernstein’s inequality that
P {kG(0)- G∞k2 ≥ λ40} ≤P {kG(0)- G∞kF ≥ λ40
=P kG(0) -G∞k2F
=P ∖ XX IGij(0)-
{i,j=1
≤ Xn P	IIGij(0) -
i,j=1
Gi∞j	≥
Gi∞j 2 ≥
=XXP {∣Gij(0)- G∞∣≥ λn
i,j=1
≤n2 ∙ 2exp {—cλ0m∕n2}
≤δ,
where c > 0 is some constant, and we use the facts kX k2 ≤ kX kF, and P{Pin=1 xi ≥ ε} ≤
Pi=ι P{xi ≥ ε∕n}.	一	□
A.6 Proof of Lemma 3.4
Proof. By using the 1-Lipschitz continuity of σ(x), we have
∣∣G — G(0)k =— ∣∣σ(XW T )σ(XW T )T — σ(XW (0)T )σ(XW (0)T )T ∣∣
m
≤ -1 kσ(XWT )σ(XWT )t — σ(XWT )σ(XW (0)T )t k
m
+ -1 kσ(XWT )σ(XW (0)T )t — σ(XW (0)T )σ(XW (0)T )t k
m
=—kσ(XWT )kkσ(XWT) — σ(XW (0)t )k
m
+ -1 ∣σ(XWT) — σ(XW (0)t )∣∣σ(XW (0)t )∣
m
≤ 二 ∣X k∣W k∣X k∣W — W (0)k + 二 ∣X k∣W — W (0)kkX k∣W (0)k
m
4c
≤√^m ∣X k2∣w — W (0)k
≤?.
m
□
A.7 Proof of Lemma 3.5
Proof. It suffices to show the result holds for γ = min{γ0, γ0∕2c}, where γ0 = 1∕2. Note that
Lemma 2.1 still holds if one chooses a larger c. Thus, We choose C % √λ0∕∣X∣∣. We prove by the
induction. Suppose that for 0 ≤ s ≤ t, the followings hold
⑴ λmin(G(S)) ≥ λ0,
(ii)	ku(s)k≤ Wky(0) — y∣,
(iii)	Ms)∣ ≤ 8⅜√n∣y(0) — y∣,
21
(iv)	∣∣W(s)k ≤ 2c√m,
(V)	IlA(S)Il ≤ 2c√m,
(Vi)	ky(s) 一 yk2 ≤ eχp{-λos}∣∣y(0) — y∣2,
Since λmin(G(s)) ≥ λ0, we have
dk ky⑴-yk2 = - 2(y⑴一y)τ H ⑴(y⑴-y)
dt
≤ - λoky⑴一yk2
SolVing the ordinary differential equation yields
ky(t) — yk2 ≤ exp{—λot}∣∣y(0) — y∣∣2.
By using the inductive hypothesis ∣∣ W(s)∣ ≤ 2c√m, We have
kφi(s)k = √= σ(W(s)Xi) ≤√= ∣W (s)k∣Xik ≤ 2c.
m m
It follows from Lemma 2.2 with γ0 = 1/2 that
∣z:(s)k≤ 2∣φi(s)k ≤ 4c.
Note that
n
NL(s)∣≤ X ∣yi(s) — yi∣∣φi(s)k
i=1
n
≤2c X Iyi(S) — yi|
i=1
≤2c√n∣y(s) — y∣
≤2c√n exp{—λos∕2}∣y(0) — y∣
and so
kv(t) — v(0)k≤ t' IVvL(s)∣ds
0
≤2c√n∣y(0) — y∣ Z exp{—λQs∕2}ds
0
≤ 乎 ky(0) — yk,
λ0
Since Vi(0) follows symmetric Bernoulli distribution, then ∣∣v(0)∣ = √m and we obtain
kv(t)k ≤ kv(t) — v(0)k + kv(0)k ≤ 8c√nky(0) — yk,
λ0
where the last inequality is due to m = Ω (C nkχk ∣∣τ^(0) — y∣2) and C % √λ0∕∣X∣∣.
Similarly, we have
n
∣VuL(s)∣ ≤X∣yi(s) — yi∣∣z*∣
i=1
≤4c√n∣y(s) — y∣
≤4c√n exp{—λos∕2}∣y(0) — y∣,
22
so that
ku(t) - U(O)k ≤ Zt kVuL(s)kds ≤ 8c√nky(0) - yk.
0	λ0
Since Ui(O) follows symmetric Bernoulli distribution, then ∣∣u(0)k = √m and We obtain
ku⑴k≤ku⑴-U(O)k + ku(0)k ≤ 看ky(0) -yk.
Note that
n1
kVw L(s)k ≤E√m Iyi(S)- yi∣kEi(s)k (kUi(s)-1u(s)k + kv(s)k) kgk
i=1
≤64λ√nky(O) - yk.X Iyi(S) -y|
0	i=1
64cn
≤丁ky(O) - yk∙ky(s) - yk
λ0
64cn
≤r-ky(O) - yk2 ∙ eχp{-λ0“2},
λ0
so that
kW(t)-W(O)k ≤ ZtkVWL(S)kdS
0
128cn
≤ 苏标 ky(O)- yk2
λo√m
≤ 16ckX k2
≤R.
Therefore, we obtain
kW(t)k ≤ kW(t) - W(0)k + kW(0)k ≤ 2c√m,
Moreover, it follows from Lemma 3.4 that λmin{G(t)} ≥ λ0.
Note that
n
kVAL(s)k ≤ X √γm ∣yi(s) - yi∣kDikkUi(s)-1kku(s)kkz*k
i=1
V	32c√n
一λo√m
V	32cn
一λo√m
V	32cn
一λo√m
n
ky(O) - yk ∙ X Iyi(S) -y∕
i=1
ky(O)- yk ∙ ky(S)- yk
l∣y∕(0) - yk2 ∙ eχp{-λos∕2},
so that
kA(t) - A(O)k ≤ Zt kVAL(S)kdS
0
64cn
≤λ0√mky(O) - yk .
Then
∣A(t)k≤kA(t) - A(O)k + kA(0)k≤ 2c√m.
□
23
A.8 Proof of Lemma 3.6
In this section, We prove the result for discrete time analysis or result for gradient descent. Assume
∣∣A(0)k ≤ c√m and IlW(0)k ≤ c√m. Further, we assume λmin(G(0)) ≥ 3λo and we assume
m = Ω (C nkXk ∣∣y(0) - y∣∣2) and choose 0 < γ ≤ min{1∕2,1∕4c}. Moreover, we assume the
stepsize α = O (λ0∕n2). We make the inductive hypothesis as follows for all 0 ≤ S ≤ k
⑴ λmin(G(S)) ≥ λ0,
(ii)	ku(s)k≤ 3¾^ky(0) - yk,
(iii)	kv(s)k ≤ 16λ√nky(0) - yk,
(iv)	∣∣w(S)Il ≤ 2c√m,
(V) ∣∣A(s)∣ ≤ 2c√m,
(vi) ky(s) - yk2 ≤ (1 - αλo∕2)sky(0) - y∣2.
Proof. Note that Lemma 2.1 still holds if one chooses a larger c. Thus, we choose C % √λ0∕∣X∣∣.
By using the inductive hypothesis, we have for any 0 ≤ S ≤ k
kφi(s)k = k√mσ(W(s)Xi)k ≤ √m∣W(s)k ≤ 2c
and
∣Φ(s)k ≤ ∣Φ(s)∣f = (XX kΦi(s)k2!	≤ 2c√n.	(33)
By using Lemma 2.2, we obtain the upper bound for the equilibrium point zi(S) for any 0 ≤ S ≤ k
as follows
kzi(s)k ≤ ɪkΦi(s)k =2∣φi(s)k≤ 4c,
1 - γ0
where the last inequality is because we choose γ0 = 1/2, and
∣Z(s)k ≤ ∣Z(s)∣f = (XL ∣Zi(s)k2!	= 4c√n.	(34)
By using the upper bound of φi(S), we obtain for any 0 ≤ S ≤ k
n
∣VvL(s)∣≤ X ∣yi(s) - yi∣∣φi(s)∣
i=1
n
≤2c X Iyi(S) -yi|
i=1
≤2c√n∣y(s) - y∣
≤2c√n(1 - aXo/2)s/2ky(O)- yk.
Let β，∙∖∕1 - αλo∕2. Then the upper bound OfkVvL(s)∣ can be written as
IlVvL(S)∣∣≤ 2c√nβs∣y(0) - y∣,	(35)
24
and
kk
kv(k + 1) - v(0)k ≤ X kv(s + 1) - v(s)k = αX ∣∣VvL(s)k
k
≤α ∙ 2c√n∣∣y(0) - yk ∙ Xβs
s=0
=『∙ 2c…-yk J
λ0	1 - β
≤8c√nky(0) - yk,
λ0
where the last inequality we use the facts β < 1. By triangle inequality, we obtain
kv(k + I)k ≤ kv(k + 1) - v(0)k + kv(0)k ≤ 16c√nky(0) - yk,
λ0
which proves the result (iii). Similarly, we can upper bound the gradient of u
n
kvuL(S)k ≤ X Iyi(S) -yi| kzik ≤ 4c√nky(S)- yk ≤ 4c√nβsky(O)- yk
i=1
so that
ku(k +1) - U(O)k ≤ 16c√nky - yk,
λ0
and
ku(k)k ≤ ku(k) - U(O)k + ku(0)k ≤ 32c√nky(0) - yk.
λ0
The result (ii) is also obtained.
By using the inductive hypothesis, we can upper bound the gradient of W as follows
n1
kVWL(s)k ≤E√m ∖yi(si∣kEi(s)k (kUi(s)-1u(s)k + kv(s)k) kx,k
i=1
≤ 128√√n ky(O) - yk X Iyi(S) - yi|
0	i=1
128cn
≤τ-√= ky(0) - yk ∙ ky(s) - yk
λ0 m
≤^√cnky(0) - yk2 ∙ βs,
λ0 m
(36)
(37)
so that
k
kW(k+1)-W(O)k≤αXkVWL(S)k
s=0
128cn
λ2√m
≤α ∙
k
ky(0) - yk2 ∙ XβS
s=0
512cn
≤ λ2√而 ky(O)- yk2
√mλ0
≤ 16ckΧ k2
≤R,
25
where the third inequality holds is because m is large, i.e., m = Θ (C nkχk2ky(O)-yk2)∙ TO
simplify the notation, we assume
Cc2nIXI2
m =—个31Iy(O) - yI2
(38)
for some large number C > 0. Moreover, we obtain
IlW(k + 1)k≤ IlW(k + 1) - W(0)k + IlW(0)k ≤ 2c√m,
Therefore, it follows from Lemma 3.4 that λmin{G(k + 1)} ≥ λ0. Thus, the results (i) and (iv) are
established.
By using similar argument, we can upper bound the gradient of A as follows Note that
n
∣VaL(s)∣ ≤ X √m ∣yi(s)-yi∣kDikkUi(s)-1kku(s)kkz罚
i=1
≤64c√n ky(O) — yk ∙ X Iyi(S) — yi|
λ0 √m	i=1
64cn
≤v√= ky(0) - yk ∙ ky(s) - yk
λ0 m
≤ 64√tl ky(0) - yk2 ∙ βs,
λ0 m
so that
k
IA(k + 1) - A(O)I ≤αXIVAL(S)I
s=0
k
≤α ∙r予博。-yH2 ∙ XβS
λ0 √m	s=0
256cn
≤ 中Iy(O)- y『.
Since m = Cc RXk ∣∣τ^(0) 一 y∣2 and c,C > O are large enough, We have
∣∣A(k +1)∣ ≤ ∣∣A(k + 1)- A(O)II + IlA(O)k≤ 2c√m.
Therefore, the result (v) is obtained and the equilibrium points zi(k + 1) exists for all i ∈ [n].
To establish the result (vi), we need to derive the bounds between equilibrium points Z(k) and
feature vectors Φ(k). We firstly bound the difference between equilibrium points zi(k + 1) and
zi(k). For any ` ≥ 1, we have
Iz'+1 (k + 1) - z'+1(k)I =Iσ [5A(k + 1)z'(k +1) + Φi(k +1)] - σ [YA(k)z'(k) + φi(k)] I
≤IYA(k + 1)z'(k + 1) + Φi(k + 1) - YA(k)z'(k) - Φi(k)I
≤YIA(k + 1)z'(k + 1) - A(k)z'(k)I + IIΦi(k + 1) - Φi(k)I,
where the first term can be bounded as follows
Y∣∣A(k +1)z'(k +1) - A(k)z'(k)I
≤Y∣∣A(k +1) - A(k)IIz'(k +1)I + 包A(k)IIz'(k +1) - z'(k)I
≤Yα∣∣VAL(k)∣∣(4c)+ 力||4仿训2'(k + 1) - z'(k)I
64αcn
≤ r— Iy(O) - yI2βk + (1∕2)l∣z'(k + 1) - z'(k)I,
λ0m
26
and the second term is bounded as follows
kφi(k + 1) - φi(k)k
√1m∣∣σ[W(k + 1)xi] - σ[W(k)xi]k
≤√1m∣W(k +1) - W(k)kkXik
α
≤√m Ww乙网
128αcn
≤------
λ0m
ky(0) - yk2 ∙ βk.
Thus, we obtain
kz'+1(k+1) - z'+1⑻k ≤(I/2)kz'(k+1) - z'⑻k + 250mn ky(O)- yk2 ∙β k
≤(1∕2)'∣z1(k +1) - zi(k)k +
≤(1∕2)'kz1(k +1) - zi(k)k +
"∣y(0)-yk2 ∙ βk ∙ XX 2-j
λ0m
j=0
512αcnky(0)-yk2 ∙ βk.
λ0m
By letting ` → ∞, we obtain
512αcn
∣∣Zi(k + 1) - Zi(k)k ≤ F----
λ0m
By using the Cauchy-Schwartz’s inequality, we have
ky(0) - yk2 ∙ βk.
kZ(k+1)-Z(k)k ≤kZ(k+1)-Z(k)kF
512αcn3∕2 …、 ll9 L
≤	ky(O)- yk2 ∙ βk.	(39)
In addition, we will also bound the difference in φi(k + 1) and φi(k). Note that
M(k + 1)-阿初=√⅛ kσ[W (k + 1)xi]- σ[W (k)χi]k ≤ T ky(O)- yk2 ∙β k，
so that
∣Φ(k + 1) - Φ(k)∣ ≤∣Φ(k+1)-Φ(k)∣F ≤
128αcn3/2
λ0m
∣y(0) - yk2 ∙ βk
(40)
Now, we are ready to establish the result (vi). Note that
ky(k + 1) - yk2 =ky(k + 1) - y(k) + y(k) - yk2
=ky(k + 1) - y(k)k2 + 2 hy(k + 1) - y(k), y(k) - y + ∣y(k) - y∣2.
In the rest of this proof, we will bound each term in the above inequality. By the prediction rule of
y, We can bound the difference between y(k + 1) and y(k) as follows
∣y(k + 1) - y(k)k =∣∣Z(k + 1)u(k + 1) + Φ(k + 1)v(k + 1) - Z(k)u(k) - Φ(k)v(k)∣
≤∣Z(k + 1)u(k + 1) - Z(k)u(k)∣ +∣Φ(k+1)v(k+1) - Φ(k)v(k)∣,
where the first term can be bounded as follows by using (34), 36, 38, 39, hypothesis (ii), and a large
constant C0 > O
∣Z(k + 1)∣∣u(k + 1) - u(k)∣ +∣Z(k+1)-Z(k)∣∣u(k)∣
=α∣Z(k +1)k∣VuL(k)k + ∣Z(k +1) - Z(k)k∣u(k)k
≤αC0c2nky(O) - y∣ ∙ βk,
27
and the second term is bounded as follows by using (33), 35, 40, 38, hypothesis (iii), and a large
constant C0 > 0
kΦ(k + 1)kkv(k + 1) - v(k)k + kΦ(k + 1) - Φ(k)kkv(k)k
=α∣∣Φ(k +1)kkVvL(k)k + ∣∣Φ(k +1) - Φ(k)kkv(k)k
≤αCoc2nky(0) - yk ∙ βk.
Therefore, we have
ky(k + 1) - y(k)k ≤ αCoc2nky(0) - yk ∙ βk,	(41)
where the scalar 2 is absorbed in C0 and the constant C0 is difference from C.
Let g , Z (k)u(k + 1) + Φ(k)v(k + 1). Then we have
hy(k +1) - y(k), y(k) - yi = hy^(k +1) - g, y(k) - yi + hg - y(k), y(k) - y.
Let us bound each term individually. By using the Cauchy-Schwartz inequality, we have
hy^(k +1) - g, y(k) - yi
=h(Z(k + 1) - Z(k))u(k + 1), y(k) - yi + h(Φ(k + 1) - Φ(k))v(k + 1), y(k) - y
≤ (kZ(k + 1) - Z(k)kku(k +1)k + kΦ(k + 1) - Φ(k)kkv(k +1)k) kθ(k) - yk
≤αCoc2nky(0) - yk ∙ βkky(k) - yk, by (34), 36, 38, 39
≤αCoc2n ∙ β2kky(0) - y∣∣2.	(42)
By using VuL(k) = Z(k)T(y(k) - y), VvL(k) = Φ(k)T(y(k) - y) and λmm(G(k)) ≥ λο∕2,
we get
hg - y(k), y(k) - yi = -α(y(k) - y)T [Z(k)Z(k)T + Φ(k)Φ(k)T] (yf(k) - y)
≤-αλ0ky(k) - yk2.	(43)
By combining the inequalities (41), 42, 43, we obtain
ky(k + 1) - yk2 ≤ (1 - α [λo - Coc2n - aC2c4n2]) β2k∣∣y(0) - y∣∣2
≤1
1
αλ0) ∙ β2kky(0) - yk2
αλ0∖ k+1 Il	∣∣2
F ) ky(O) - yk ,
where the second inequality is by a = O 您).This proves the result (vi) and complete the proof.
□
28