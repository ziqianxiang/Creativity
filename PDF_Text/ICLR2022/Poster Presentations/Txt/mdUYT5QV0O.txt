Training Structured Neural Networks
Through Manifold Identification and Vari-
ance Reduction
Zih-Syuan Huang
Academia Sinica
zihsyuan@stat.sinica.edu.tw
Ching-pei Lee
Academia Sinica
leechingpei@gmail.com
Abstract
This paper proposes an algorithm, RMDA, for training neural networks
(NNs) with a regularization term for promoting desired structures. RMDA
does not incur computation additional to proximal SGD with momentum,
and achieves variance reduction without requiring the objective function
to be of the finite-sum form. Through the tool of manifold identification
from nonlinear optimization, we prove that after a finite number of iter-
ations, all iterates of RMDA possess a desired structure identical to that
induced by the regularizer at the stationary point of asymptotic conver-
gence, even in the presence of engineering tricks like data augmentation
that complicate the training process. Experiments on training NNs with
structured sparsity confirm that variance reduction is necessary for such an
identification, and show that RMDA thus significantly outperforms existing
methods for this task. For unstructured sparsity, RMDA also outperforms
a state-of-the-art pruning method, validating the benefits of training struc-
tured NNs through regularization. Implementation of RMDA is available at
https://www.github.com/zihsyuan1214/rmda.
1 Introduction
Training neural networks (NNs) with regularization to obtain a certain desired structure
such as structured sparsity or discrete-valued parameters is a problem of increasing interest.
Existing approaches either use stochastic subgradients of the regularized objective (Wen
et al., 2016; 2018) or combine popular stochastic gradient algorithms for NNs, like SGD
with momentum (MSGD) or Adam (Kingma & Ba, 2015), with the proximal operator
associated with the regularizer to conduct proximal stochastic gradient updates to obtain a
model with preferred structures (Bai et al., 2019; Yang et al., 2019; Yun et al., 2021; Deleu
& Bengio, 2021). Such methods come with proven convergence for certain measures of
first-order optimality and have shown some empirical success in applications. However, we
notice that an essential theoretical support lacking in existing methods is the guarantee for
the output iterate to possess the same structure as that at the point of convergence. More
specifically, often the imposed regularization is only known to induce a desired structure
exactly at optimal or stationary points of the underlying optimization problem (see for
example, Zhao & Yu, 2006), but training algorithms are only able to generate iterates
asymptotically converging to a stationary point. Without further theoretical guarantees, it
is unknown whether the output iterate, which is just an approximation of the stationary
point, still has the same structure. For example, let us assume that sparsity is desired,
the point of convergence is x* = (1, 0, 0), and two algorithms respectively produce iterates
{yt = (1, t-1, t-1)} and {zt = (1 + t-1 , 0, 0)}. Clearly, both iterate sequences converge to
x* , but only zt has the same desired structure as its limit point x* , while yt is not useful
for sparsity despite that the point of convergence is. This work aims at filling this gap to
propose an algorithm for training structured NNs that can provably make all its iterates after
1
a finite number of iterations possess the desired structure of the stationary point to which
the iterates converge. We term the structure at a stationary point a stationary structure,
and it should be understood that for multiple stationary points, each might correspond
to a different stationary structure, and we aim at identifying the one at the limit point
of the iterates of an algorithm, instead of selecting the optimal one among all stationary
structures. Although finding the structure at an inferior stationary point might seem not
very meaningful, another reason for studying this identification property is that for the same
point of convergence, the structure at the limit point is the most preferable one. Consider
the same example above, We note that for any sequence {xt} converging to x*, x∖ = 0 for
all t large enough, for otherwise xt does not converge to x*. Therefore, xt cannot be sparser
than x* if xt → x*.1 Identifying the structure of the point of convergence thus also amounts
to finding the locally most ideal structure under the same convergence premise.
It is well-known in the literature of nonlinear optimization that generating iterates consis-
tently possessing the structure at the stationary point of convergence is possible if all points
with the same structure near the stationary point can be presented locally as a manifold
along which the regularizer is smooth. This manifold is often termed as the active manifold
(relative to the given stationary point), and the task of generating iterates staying in the
active manifold relative to the point of convergence after finite iterations is called manifold
identification (Lewis, 2002; Hare & Lewis, 2004; Lewis & Zhang, 2013). To identify the ac-
tive manifold of a stationary point, we need the regularizer to be partly smooth (Lewis, 2002;
Hare & Lewis, 2004) at that point, roughly meaning that the regularizer is smooth along the
active manifold around the point, while the change in its value is drastic along directions
leaving the manifold. A more technical definition will be given in Section 3. Fortunately,
most regularizers used in machine learning are partly smooth, so stationary structure iden-
tification is possible, and various deterministic algorithms are known to achieve so (Hare
& Lewis, 2007; Hare, 2011; Wright, 2012; Liang et al., 2017a;b; Li et al., 2020; Lee, 2020;
Bareilles et al., 2020).
On the other hand, for stochastic gradient-related methods to identify a stationary struc-
ture, existing theory suggests that the variance of the gradient estimation needs to vanish
as the iterates approach a stationary point (Poon et al., 2018), and indeed, it is observed
empirically that proximal stochastic gradient descent (SGD) is incapable of manifold iden-
tification due to the presence of the variance in the gradient estimation (Lee & Wright,
2012; Sun et al., 2019).2 Poon et al. (2018) showed that variance-reduction methods such
as SVRG (Johnson & Zhang, 2013; Xiao & Zhang, 2014) and SAGA (Defazio et al., 2014)
that utilize the finite-sum structure of empirical risk minimization to drive the variance of
their gradient estimators to zero are suitable for this task. Unfortunately, with the standard
practice of data augmentation in deep learning, training of deep learning models with a reg-
ularizer should be treated as the following stochastic optimization problem that minimizes
the expected loss over a distribution, instead of the commonly seen finite-sum form:
W∈nE	F (W):= Eg~D [fξ (W)] + ψ (W),	⑴
where E is a Euclidean space with inner product《,•〉and the associated norm ∣∣∙∣∣, D is a
distribution over a space Ω, fξ is differentiable almost everywhere for all ξ ∈ Ω, and ψ(W)
is a regularizer that might be nondifferentiable. We will also use the notation f(W) :=
Eξ~D[fξ(W)]. Without a finite-sum structure in (1), Defazio & Bottou (2019) pointed out
that classical variance-reduction methods are ineffective for deep learning, and one major
reason is the necessity of periodically evaluating V f (W) (or at least using a large batch from
D to get a precise approximation of it) in variance-reduction methods is intractable, hence
manifold identification and therefore finding the stationary structure becomes an extremely
tough task for deep learning. Although recently there are efforts in developing variance-
reduction methods for (1) inspired by online problems (Wang et al., 2019; Nguyen et al.,
2021; Pham et al., 2020; Cutkosky & Orabona, 2019; Tran-Dinh et al., 2019), these methods
all have multiple hyperparameters to tune and incur computational cost at least twice or
1See a more detailed discussion in Appendix B.1.
2 An exception is the interpolation case, in which the variance of plain SGD vanishes asymptot-
ically. But data augmentation often fails this interpolation condition.
2
thrice to that of (proximal) SGD. As the training of deep learning models is time- and
resource-consuming, these drawbacks make such methods less ideal for deep learning.
To tackle these difficulties, we extend the recently proposed modernized dual averaging
framework (Jelassi & Defazio, 2020) to the regularized setting by incorporating proximal
operations, and obtain a new algorithm RMDA (Regularized Modernized Dual Averaging)
for (1). The proposed algorithm provably achieves variance reduction beyond finite-sum
problems without any cost or hard-to-tune hyperparameters additional to those of proximal
momentum SGD (proxMSGD), and we provide theoretical guarantees for its convergence
and ability for manifold identification. The key difference between RMDA and the original
regularized dual averaging (RDA) of Xiao (2010) is that RMDA incorporates momentum
and can achieve better performance for deep learning in terms of the generalization ability,
and the new algorithm requires nontrivial proofs for its guarantees. We further conduct
experiments on training deep learning models with a regularizer for structured-sparsity to
demonstrate the ability of RMDA to identify the stationary structure without sacrificing the
prediction accuracy.
When the desired structure is (unstructured) sparsity, a popular approach is pruning that
trims a given dense model to a specified level, and works like (Gale et al., 2019; Blalock et al.,
2020; Evci et al., 2020; Verma & Pesquet, 2021) have shown promising results. However, as a
post-processing approach, pruning is essentially different from structured training considered
in this work, because pruning is mainly used when a model is available, while structured
training combines training and structure inducing in one procedure to potentially reduce the
computational cost and memory footprint when resources are scarce. We will also show in
our experiment that RMDA can achieve better performance than a state-of-the-art pruning
method, suggesting that structured training indeed has its merits for obtaining sparse NNs.
The main contributions of this work are summarized as follows.
•	Principled analysis: We use the theory of manifold identification from nonlinear opti-
mization to provide a unified way towards better understanding of algorithms for training
structured neural networks.
•	Variance reduction beyond finite-sum with low cost: RMDA achieves variance re-
duction for problems that consist of an infinite-sum term plus a regularizer (see Lemma 2)
while incorporating momentum to improve the generalization performance. Its spatial
and computational cost is almost the same as proxMSGD, and there is no additional
hyperparameters to tune, making RMDA suitable for large-scale deep learning.
•	Structure identification: With the help of variance reduction, our theory shows that
under suitable conditions, after a finite number of iterations, iterates of RMDA stay in
the active manifold of its limit point.
•	Superior empirical performance: Experiments on neural networks with structured
sparsity exemplify that RMDA can identify a stationary structure without reducing the
validation accuracy, thus outperforming existing methods by achieving higher group spar-
sity. Another experiment on unstructured sparsity also shows RMDA outperforms a
state-of-the-art pruning method.
After this work is finished, we found a very recent paper Kungurtsev & Shikhman (2021) that
proposed the same algorithm (with slightly differences in the parameters setting in Line 5
of Algorithm 1) and analyzed the expected convergence of (1) under a specific scheduling
of ct = st+1 αt-+11 when both terms are convex. In contrast, our work focuses on nonconvex
deep learning problems, and especially on the manifold identification aspect.
2 Algorithm
Details of the proposed RMDA are in Algorithm 1. At the t-th iteration with the iter-
ate Wt-1, We draw an independent sample ξt 〜D to compute the stochastic gradient
Vfξt (Wt-1), decide a learning rate η, and update the weighted sum Vt of previous stochas-
tic gradients using η and the scaling factor βt := t/t:
V0 :=0, Vt :=工tk=1ηkβkVfξk (Wk-1) = Vt-1 + ηtβtVf&(Wt-1), ∀t> 0.
3
Algorithm 1: RMDA (W0,T,η(∙), c(∙))
input : Initial point W0, learning rate schedule η(∙), momentum schedule c(∙),
number of epochs T
ι V04—0,	α 04—0
2	for t = 1, ...,T do
3	βt J ¢, St J η (t) βt, at J at-1 + St
4	Sample ξt 〜D and compute Vt J Vt-* 1 + St V fξt (Wt-1)
5	Wt J arg min W (Vt, W)+ βt∖∖ W — W 0∣∣2 3 4 + atψ (W)	//(2)
6	Wt J (1 — c(t))Wt-1 + c(t)Wt
output: The final model WT
The tentative iterate Wt is then obtained by the proximal operation associated with ψ:
W t = Prox ɑtβ- ψ (W 0 - β- V t) , at =工 t= =1 Bk ηk,	⑵
where for any function g, Prox g (x) := arg min § ∣∣ y - x ∣∣2 / 2 + g (y) is its proximal operator.
The iterate is then updated along the direction Wt - Wt-1 with a factor of ct ∈ [0, 1]:
Wt = (1 - Ct) Wt-1 + CtIWt = Wt-1 + Ct (Wt - Wt-1) .	(3)
When ψ ≡ 0, RMDA reduces to the modernized dual averaging algorithm of Jelassi &
Defazio (2020), in which case it has been shown that mixing Wt-1 and Wt in (3) equals
to introducing momentum (Jelassi & Defazio, 2020; Tao et al., 2018). We found that this
introduction of momentum greatly improves the performance of RMDA and is therefore
essential for applying it on deep learning problems.
3 Analysis
We provide theoretical analysis of the proposed RMDA in this section. Our analysis shows
variance reduction in RMDA and stationarity of the limit point of its iterates, but all of
them revolves around our main purpose of identification of a stationary structure within a
finite number of iterations. The key tools for this end are partial smoothness and mani-
fold identification (Hare & Lewis, 2004; Lewis, 2002). Our result is the currently missing
cornerstone for those proximal algorithms applied to deep learning problems for identifying
desired structures. In fact, it is actually well-known in convex optimization that those algo-
rithms based on plain proximal stochastic gradient without variance reduction are unable to
identify the active manifold, and the structure of the iterates oscillates due to the variance
in the gradient estimation; see, for example, experiments and discussions in Lee & Wright
(2012); Sun et al. (2019). Our work is therefore the first one to provide justification for
solving the regularized optimization problem in deep learning to really identify a desired
structure induced by the regularizer. Throughout, Vfξ denotes the gradient of fξ , ∂ψ is the
(regular) subdifferential of ψ, and relint(C) means the relative interior of the set C.
We start from introducing the notion of partial smoothness.
Definition 1. A function ψ is partly smooth at a point W * relative to a set M W * ə W * if
1. Around W *, M W * is a C2-manifold and ψ |m W * is C2.
2. ψ is regular (finite with the Fr6chet subdifferential coincides with the limiting Fr6chet
subdifferential) at all points W ∈ M W * around W * with ∂ψ (W) = 0.
3. The affine span of ∂ψ(W *) is a translate of the normal space to MW* at W* .
4. ∂ψ is continuous at W* relative to MW* .
We often call MW* the active manifold at W* . Another concept required for manifold
identification is prox-regularity (Poliquin & Rockafellar, 1996).
Definition 2. A function ψ is prox-regular at W* for V* ∈ ∂ψ(W *) if ψ is finite at
W*, local ly lower semi-continuous around W*, and there is ρ > 0 such that ψ(W1) ≥
ψ (W2) + (V, Wι — W2〉— P Il Wι — W21∣2 whenever Wι, W2 are close to W * with ψ (W2) near
ψ(W*) and V ∈ ∂ψ(W2) near V* . ψ is prox-regular at W* if it is so for all V ∈ ∂ψ(W *).
4
To broaden the applicable range, a function ψ prox-regular at some W * is often also assumed
to be subdifferentially continuous (Poliquin & Rockafellar, 1996) there, meaning that if
Wt → W*, ψ(W t) → ψ(W *) holds when there are V * ∈ ∂ψ(W *) and a sequence {Vt}
such that V t ∈ ∂ψ(W t) and V t → V * . Notably, all convex and weakly-convex (Nurminskii,
1973) functions are regular, prox-regular, and subdifferentially continuous in their domain.
3.1 Theoretical Results
When the problem is convex, convergence guarantees for Algorithm 1 under two specific
specific schemes are known. First, when ct ≡ 1, RMDA reduces to the classical RDA,
and convergence to a global optimum (of Wt = Wt in this case) on convex problems has
been proven by Lee & Wright (2012); Duchi & Ruan (2021), with convergence rates of the
expected objective or the regret given by Xiao (2010); Lee & Wright (2012). Second, when
ct = st+1αt-+11 and (βt, αt) in Line 5 of Algorithm 1 are replaced by (βt+1, αt+1), convergence
is recently analyzed by Kungurtsev & Shikhman (2021). In our analysis below, we do not
assume convexity of either term.
We show that if {Wt} converges to a point W * (which could be a non-stationary one), {Wt}
also converges to W* .
Lemma 1. Consider Algorithm 1 with {Ct} satisfying E Ct = ∞. If {Wt} converges to a
point W*, {Wt} also converges to W* .
We then show that if {Wt} converges to a point, almost surely this point of convergence
is stationary. This requires the following lemma for variance reduction of RMDA, meaning
that the variance of using V to estimate Vf (Wt-1) reduces to zero, as α-i V converges to
Vf (Wt-1) almost surely, and this result could be of its own interest. The first claim below
uses a classical result in stochastic optimization that can be found at, for example, (Gupal,
1979, Theorem 4.1, Chapter 2.4), but the second one is, to our knowledge, new.
Lemma 2. Consider Algorithm 1. Assume for any ξ 〜 D, fξ is L-LipSchitz-ContinuouSly-
differentiable almost surely for some L, so f is also L-Lipschitz-continuously-differentiable,
and there is C ≥ 0 Such that Eξt〜DllVfξt (Wt-1) ∣∣2 ≤ C for all t. If {ηt} satisfies
工βtηtα- = ∞,工(βtηtαj1)2 < ∞,	∣∣Wt +1 - Wt∣∣ (βtηtɑj1)-1 -→ 0,⑷
then αt-1Vt -→ Vf(Wt-1) with probabilityone. Moreover, if {Wt} lies in a bounded set,
We get E∣∣α-1 Vt - Vf (Wtτ)∣∣2 → 0 even if the second condition in (4) is replaced by a
weaker condition of βtηtαt-1 → 0.
In general, the last condition in (4) requires some regularity conditions in F to control the
change speed of Wt . One possibility is when ψ is the indicator function of a convex set,
βtηt Y tp for t ∈ (1 /2, 1) will satisfy this condition. However, in other settings for η, even
when F and ψ are both convex, existing analyses for the classical RDA such that Ct ≡ 1
in Algorithm 1 still need an additional local error bound assumption to control the change
of Wt+1 - Wt . Hence, to stay focused on our main message, we take this assumption for
granted, and leave finding suitable sufficient conditions for it as future work.
With the help of Lemmas 1 and 2, we can now show the stationarity result for the limit
point of the iterates. The assumption of βt αt-1 approaching 0 below is classical in analyses
of dual averaging in order to gradually remove the influence of the term ∣∣ W — W0∣∣ .
Theorem 1. Consider Algorithm 1 with the conditions in Lemmas 1 and 2 hold, and
assume the set of stationary points Z := {W | 0 ∈ ∂F (W)} is nonempty and βtαt-1 → 0.
For any given W0, consider the event that {Wt} converges to a point W * (each event
corresponds to a different W*), then if ∂ψ is outer semicontinuous at W*, and this event
has a nonzero probability, W* ∈ Z, or equivalently, W* is a stationary point, with probability
one conditional on this event.
Finally, with Lemmas 1 and 2 and Theorem 1, we prove the main result that the active
manifold of the limit point is identified in finite iterations of RMDA under nondegeneracy.
5
Theorem 2. Consider Algorithm 1 with the conditions in Theorem 1 satisfied. Consider
the event of {Wt} converging to a certain point W * as in Theorem 1, if the probability of
this event is nonzero; ψ is prox-regular and Subdifferentially continuous at W * and partly
smooth at W* relative to the active C2 manifold M; ∂ψ is outer semicontinuous at W*; and
the nondegeneracy condition
-Vf (W*) ∈ relint ∂ψ (W*)	(5)
holds at W*, then conditional on this event, almost surely there is T0 ≥ 0 such that
Wt ∈ M, ∀t ≥ T0.	(6)
In other words, the active manifold at W* is identified by the iterates of Algorithm 1 after
a finite number of iterations almost surely.
As mentioned in Section 1, an important reason for studying manifold identification is to get
the lowest-dimensional manifold representing the structure of the limit point, which often
corresponds to a preferred property for the application, like the highest sparsity, lowest rank,
or lowest VC dimension locally. See an illustrated example in Appendix B.1.
4	Applications in Deep Learning
We discuss two popular schemes of training structured deep learning models achieved
through regularization to demonstrate the applications of RMDA. More technical details
for applying our theory to the regularizers in these applications are in Appendix B.
4.1	Structured Sparsity
As modern deep NN models are often gigantic, it is sometimes desirable to trim the model to
a smaller one when only limited resources are available. In this case, zeroing out redundant
parameters during training at the group level is shown to be useful (Zhou et al., 2016),
and one can utilize regularizers promoting structured sparsity for this purpose. The most
famous regularizer of this kind is the group-LASSO norm (Yuan & Lin, 2006; Friedman
et al., 2010). Given λ ≥ 0 and a collection G of index sets {Ig} of the variable W, this
convex regularizer is defined as
ψ (W )= λ 工gL wg∣∣ WIJl,	⑺
with Wg > 0 being the pre-specified weight for Ig. For any W*, let GW* ⊆ G be the index
set such that W*, = 0 for all j ∈ G W *, the group-LASSO norm is partly smooth around W *
relative to the manifold MW* := {W | WIi = 0, ∀i ∈ GW* }, so our theory applies.
In order to promote structured sparsity, we need to carefully design the grouping. For-
tunately, in NNs, the parameters can be grouped naturally (Wen et al., 2016). For any
fully-connected layer, let W ∈ Rout×in be the matrix representation of the associated pa-
rameters, where out is the number of output neurons and in is that of input neurons, we
can consider the column-wise groups, defined as W:,j for all j , and the row-wise groups
of the form Wi,:. For a convolutional layer with W ∈ Rfilter×channel×height×width being the
tensor form of the corresponding parameters, we can consider channel-wise, filter-wise, and
kernel-wise groups, defined respectively as W:,j,:,:, Wi,:,:,: and Wi,j,:,: .
4.2	Binary/Discrete Neural Networks
Making the parameters of an NN binary integers is another way to obtain a more compact
mo del during training and deployment (Hubara et al., 2016), but discrete optimization is
hard to scale-up. Using a vector representation W ∈ Rm of the variables, Hou et al. (2017)
thus proposed to use the indicator function of W | WIi = αibIi , αi > 0, bIi ∈ {±1}|Ii| to
induce the entries of W to be binary without resorting to discrete optimization tools, where
each Ii enumerates all parameters in the i-th layer. Yang et al. (2019) later proposed to use
minα∈[0,1]m	im=1 αi(Wi + 1)2 + (1 - αi)(Wi - 1)2 as the regularizer and to include α as
6
a variable to train. At any α* with 10 := {i | α* = 0} and 11 := {i | α* = 1}, the objective
is partly smooth relative to the manifold {(W, α) | αI0 = 0, αI1 = 1}. Extension to discrete
NNs beyond the binary ones is possible, and Bai et al. (2019) have proposed regularizers
with closed-form proximal operators for it.
5	Experiments
We use the structured sparsity application in Section 4.1 to empirically exemplify the ability
of RMDA to find desired structures in the trained NNs. RMDA and the following methods
for structured sparsity in deep learning are compared using PyTorch (Paszke et al., 2019).
•	ProxSGD (Yang et al., 2019): A simple proxMSGD algorithm. To obtain group sparsity,
we skip the interpolating step in Yang et al. (2019).
•	ProxSSI (Deleu & Bengio, 2021): This is a special case of the adaptive proximal SGD
framework of Yun et al. (2021) that uses the Newton-Raphson algorithm to approximately
solve the subproblem. We directly use the package released by the authors.
We exclude the algorithm of Wen et al. (2016) because their method is shown to be worse
than ProxSSI by Deleu & Bengio (2021).
To compare these algorithms, we examine both the validation accuracy and the group spar-
sity level of their trained models. We compute the group sparsity as the percentage of
groups whose elements are all zero, so the reported group sparsity is zero when there is no
group with a zero norm, and is one when the whole model is zero. For all methods above,
we use (7) with column-wise and channel-wise groupings in the regularization for training,
but adopt the kernel-wise grouping in their group sparsity evaluation. Throughout the ex-
periments, we always use multi-step learning rate scheduling that decays the learning rate
by a constant factor every time the epoch count reaches a pre-specified threshold. For all
methods, we conduct grid searches to find the best hyperparameters. All results shown in
tables in Sections 5.1 and 5.2 are the mean and standard deviation of three independent
runs with the same hyperparameters, while figures use one representative run for better
visualization.
In convex optimization, a popular way to improve the practical convergence behavior for
momentum-based methods is restarting that periodically reset the momentum to zero
(O’donoghue & Candes, 2015). Following this idea, we introduce a restart heuristic to
RMDA. At each round, we use the output of Algorithm 1 from the previous round as the
new input to the same algorithm, and continue using the scheduling η and c without reset-
ting them. For ψ ≡ 0, Jelassi & Defazio (2020) suggested to increase ct proportional to the
decrease of ηt until reaching ct = 1. We adopt the same setting for ct and ηt and restart
RMDA whenever η changes. As shown in Section 3 that Wt finds the active manifold,
increasing ct to 1 also accords with our interest in identifying the stationary structure.
5.1 Correctness of Identified Structure Using Synthetic Data
ssentcerroC ytisrap
(a) Logistic regression
rorrE gniniarT
* RMDA (sparsity correctness)
. PrOXSSI (sparsity correctness)
■	ProxSGD (sparsity correctness)
■	一 ■ RMDA (training error)
ProxSSI (training error)
ProxSGD (training error)
100%
75%
50%
25%
0%
ssentcerroC ytisrap
100%
75%
100
Epochs
」0」」山
gniniarT
0%
200
(b) Legend
(c) Convolutional network
Figure 1: Group sparsity pattern correctness and training error rates on synthetic data.
Our first step is to numerically verify that RMDA can indeed identify the stationary structure
desired. To exactly find a stationary point and its structure a priori, we consider synthetic
problems. We first decide a ground truth model W that is structured sparse, generate
random data points that can be well separated by W , and then decide their labels using
W . The generated data are then taken as our training data. We consider a linear logistic
7
regression model and a small NN that has one fully-connected layer and one convolutional
layer. To ensure convergence to the ground truth, for logistic regression we generate more
data points than the problem dimension to ensure the problem is strongly convex so that
there is only one stationary/optimal point, and for the small NN, we initialize all algorithms
close enough to the ground truth. We report in Fig. 1 training error rates (as an indicator
for the proximity to the ground truth) and percentages of the optimal group sparsity pattern
of the ground truth identified. Clearly, although all methods converge to the ground truth,
only RMDA identifies the correct structure of it, and other methods without guarantees for
manifold identification fail.
5.2 Neural Networks with Real Data
Table 1: Group sparsity and validation accuracy of different methods. We report mean and
standard deviation of three independent runs (except that for the linear convex model, only
one run is conducted as we are guaranteed to find the global optima). MSGD is the baseline
with no sparsity-inducing regularizer.
Algorithm	Validation accuracy	Group sparsity	Validation accuracy	Group sparsity
	Logistic regression/MNIST				FUlly-connected NN/FashionMNIST	
ProxSGD ProxSSI RMDA	91.31 % 91.31 % 91.34 %	38.78 % 39.54 % 56.51 %	88.72 ± 0.05% 88.44 ± 0.41% 88.09 ± 0.04%	31.42 ± 0.36% 35.25 ± 1.56% 42.89 ± 0.66%
	LeNet5/MNIST				LeNet5/FashionMNIST	
MSGD	99.36 ± 0.06%	-	91.96 ± 0.01%	-
ProxSGD ProxSSI RMDA	99.13 ± 0.02% 99.07 ± 0.03% 99.10 ± 0.06%	76.57 ± 2.33% 77.82 ± 1.56% 79.81 ± 1.56%	90.99 ± 0.17% 90.93 ± 0.02% 91.41 ± 0.10%	50.50 ± 2.66% 60.49 ± 1.05% 66.15 ± 1.68%
	VGG19/CIFAR10					VGG19/CIFAR100		
MSGD	94.03 ± 0.11%	-	74.62 ± 0.22%	-
ProxSGD ProxSSI RMDA	92.38 ± 0.31% 92.51 ± 0.03% 93.62 ± 0.15%	72.57 ± 6.04% 81.05 ± 0.16% 86.37 ± 0.25%	71.91 ± 0.08% 66.20 ± 0.38% 72.23 ± 0.20%	08.63 ± 4.88% 46.41 ± 1.42% 58.86 ± 0.41%
	ReSNet50/CIFAR10					ReSNet50/CIFAR100		
MSGD	95.65 ± 0.03%	-	79.13 ± 0.19%	-
ProxSGD ProxSSI RMDA	92.36 ± 0.14% 94.09 ± 0.08% 94.25 ± 0.02%	76.82 ± 4.09% 74.81 ± 1.28% 83.01 ± 0.50%	75.53 ± 0.49% 74.52 ± 0.29% 76.12 ± 0.46%	51.83 ± 0.34% 32.79 ± 2.53% 57.67 ± 3.76%
We turn to real-world data used in modern computer vision problems. We consider two
rather simple models and six more complicated modern CNN cases. The two simpler models
are linear logistic regression with the MNIST dataset (LeCun et al., 1998), and training a
small NN with seven fully-connected layers on the FashionMNIST dataset (Xiao et al.,
2017). The six more complicated cases are:
1.	A version of LeNet5 with the MNIST dataset,
2.	The same version of LeNet5 with the FashionMNIST dataset,
3.	A modified VGG19 (Simonyan & Zisserman, 2015) with the CIFAR10 dataset
(Krizhevsky, 2009),
4.	The same modified VGG19 with the CIFAR100 dataset (Krizhevsky, 2009),
5.	ResNet50 (He et al., 2016) with the CIFAR10 dataset, and
6.	ResNet50 with the CIFAR100 dataset.
For these six more complicated tasks, we include a dense baseline of MSGD with no sparsity-
inducing regularizer in our comparison. For all training algorithms on VGG19 and ResNet50,
we follow the standard practice in modern vision tasks to apply data augmentation through
random cropping and horizontal flipping so that the training problem is no longer a finite-
sum one. From Fig. 2, we see that similar to the previous experiment, the group sparsity
level of RMDA is stable in the last epochs, while that of ProxSGD and ProxSSI oscillates
below. This suggests that RMDA is the only method that, as proven in Section 3, identifies
the structured sparsity at its limit point, and other methods with no variance reduction
fail. Moreover, Table 1 shows that manifold identification of RMDA is achieved with no
8
S0-8K
As,leds
Logistic Regression on MNIST
LeNet5 on MNIST
-B- ProxSGD
-∙- ProxSSI
-⅛- RMDA
PQ」n:>n」S
⅛58γ
As,leds PQ∙lnnS
PrOxSGD
-∙- ProxSSI
-*- RMDA
400	420	440	460	480	500
VGG19 on CIFAR10
0.9
Q.0.8
0.7
0.6
VGG19 On CIFAR100
900	920	940	960	980	1000
Epochs
Epochs
ResNet50 on CIFAR10
ResNet50 on CIFAR100
420	440	460	480	500
Epochs
A-s,leds PQ∙lnsntS
5 0 5 0 5”
6 6 5 5 4
As,leds PQ∙lnnS
LeNet5 on FashiOnMNIST
-B- PrOxSGD
-∙- ProxSSI
-⅛- RMDA
)0	420	440 460	480	500
Epochs
As,leds PQ∙lnnS
≈0
-B- PrOxSGD
-∙- ProxSSI
-⅛- RMDA
j≡
A-s,leds PQ∙lnsntS
T PrOxSGD
-∙- ProxSSI
-⅛- RMDA
0	960	980	1000
Epochs
0.3
I4θ0 1420 1440 1460 1480 1500
Epochs
Figure 2: Group Sparsity v.s epochs of different algorithms on NNs of a single run.
sacrifice of the validation accuracy, so RMDA beats ProxSGD and ProxSSI in both criteria,
and its accuracy is close to that of the dense baseline of MSGD. Moreover, for VGG19
and ResNet50, RMDA succeeds in finding the optimal structured sparsity pattern despite
the presence of data augmentation, showing that RMDA can indeed overcome the difficulty
from the infinite-sum setting of modern deep learning tasks.
We also report that in the ResNet50/CIFAR100 task, on our NVIDIA RTX 8000 GPU,
MSGD, ProxSGD, and RMDA have similar per-epoch cost of 68, 77, and 91 seconds respec-
tively, while ProxSSI needs 674 seconds per epoch. RMDA is thus also more suitable for
large-scale structured deep learning in terms of practical efficiency.
5.3 Comparison with Pruning
We compare RMDA with a state-of-the-art pruning method RigL (Evci et al., 2020). As
pruning focuses on unstructured sparsity, We use RMDA with ψ(W) = λ∣∣ W∣∣ 1 to have a fair
comparison, and tune λ to achieve a pre-specified sparsity level. We run RigL with 1000
epochs, as its performance at the default 500 epochs was unstable, and let RMDA use the
same number of epochs. Results of 98% sparsity in Table 2 show that RMDA consistently
outdoes RigL, indicating regularized training could be a promising alternative to pruning.
Table 2: Comparison between RMDA and RigL with 1000 epochs for unstructured sparsity
in a single run.
ResNet50 with CIFAR10			ReSNet50 With CIFAR100	
Algorithm	Sparsity	AccUracy	SParSity	Accuracy
Dense baseline		94.81%^	74.61%	
RMDA	98.36%	93.78%	98.32%	74.32%
RigL	98.00%	93.41%	98.00%	70.88%
6 Conclusions
In this work, we proposed and analyzed a new algorithm, RMDA, for efficiently training
structured neural networks with state-of-the-art performance. Even in the presence of data
augmentation, RMDA can still achieve variance reduction and provably identify the desired
structure at a stationary point using the tools of manifold identification. Experiments show
that existing algorithms for the same purpose fail to find a stable stationary structure, while
RMDA achieves so with no accuracy drop nor additional time cost.
9
Acknowledgements
This work was supported in part by MOST of R.O.C. grant 109-2222-E-001-003-MY3, and
the AWS Cloud Credits for Research program of Amazon Inc.
References
Yu Bai, Yu-Xiang Wang, and Edo Liberty. ProxQuant: Quantized neural networks via
proximal operators. In International Conference on Learning Representations, 2019. 1, 7
Gilles Bareilles, Franck Iutzeler, and Jer^me Malick. NeWton acceleration on manifolds
identified by proximal-gradient methods. Technical report, 2020. arXiv:2012.12936. 2
Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag. What is
the state of neural netWork pruning? In Machine Learning and Systems, 2020. 3
Patrick Breheny and Jian Huang. Penalized methods for bi-level variable selection. Statistics
and its interface, 2(3):369, 2009. 22
James V. Burke and Michael C. Ferris. Weak sharp minima in mathematical programming.
SIAM Journal on Control and Optimization, 31(5):1340—1359, 1993. 19
Ashok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-
convex SGD. In Advances in Neural Information Processing Systems, pp. 15236—15245,
2019. 2
Aaron Defazio and Leon Bottou. On the ineffectiveness of variance reduced optimization
for deep learning. In Advances in Neural Information Processing Systems, pp. 1755-1765,
2019. 2
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient
method With support for non-strongly convex composite objectives. In Advances in neural
information processing systems, pp. 1646-1654, 2014. 2
Tristan Deleu and Yoshua Bengio. Structured sparsity inducing adaptive optimizers for deep
learning. Technical report, 2021. arXiv:2102.03869. 1, 7, 20, 22
John C. Duchi and Feng Ruan. Asymptotic optimality in stochastic optimization. The
Annals of Statistics, 49(1):21-48, 2021. 5
Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the
lottery: Making all tickets Winners. In International Conference on Machine Learning,
pp. 2943-2952, 2020. 3, 9
Jerome Friedman, Trevor Hastie, and Robert Tibshirani. A note on the group lasso and a
sparse group lasso. Technical report, 2010. 6
Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural netWorks.
Technical report, 2019. arXiv:1902.09574. 3
A.M. Gupal. Stochastic methods for solving nonsmooth extremal problems. Naukova Dumka,
1979. 5, 14
Warren L. Hare. Identifying active manifolds in regularization problems. In Fixed-Point
Algorithms for Inverse Problems in Science and Engineering, pp. 261-271. Springer, 2011.
2
Warren L. Hare and Adrian S. LeWis. Identifying active constraints via partial smoothness
and prox-regularity. Journal of Convex Analysis, 11(2):251-266, 2004. 2, 4
Warren L. Hare and Adrian S. LeWis. Identifying active manifolds. Algorithmic Operations
Research, 2(2):75-75, 2007. 2
10
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In IEEE conference on computer vision and pattern recognition, pp. 770-778,
2016. 8, 20
Lu Hou, Quanming Yao, and James T Kwok. Loss-aware binarization of deep networks. In
International Conference on Learning Representations, 2017. 6
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q. Weinberger. Densely
connected convolutional networks. In IEEE conference on computer vision and pattern
recognition, pp. 4700-4708, 2017. 20
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio.
Binarized neural networks. Advances in neural information processing systems, 2016. 6
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In International conference on machine learning, pp.
448-456, 2015. 20
Samy Jelassi and Aaron Defazio. Dual averaging is surprisingly effective for deep learning
optimization. Technical report, 2020. arXiv:2010.10502. 3, 4, 7
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive
variance reduction. Advances in neural information processing systems, pp. 315-323,
2013. 2
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In
International Conference on Learning Representations, 2015. 1
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report,
2009. 8
Vyacheslav Kungurtsev and Vladimir Shikhman. Regularized quasi-monotone method for
stochastic optimization. Technical report, 2021. arXiv:2107.03779. 3, 5
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning
applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998. 8
Ching-pei Lee. Accelerating inexact successive quadratic approximation for regularized
optimization through manifold identification. Technical report, 2020. arXiv:2012.02522.
2, 17
Sangkyun Lee and Stephen J. Wright. Manifold identification in dual averaging for regu-
larized stochastic online learning. Journal of Machine Learning Research, 13:1705-1744,
2012. 2, 4, 5
Adrian S. Lewis. Active sets, nonsmoothness, and sensitivity. SIAM Journal on Optimiza-
tion, 13(3):702-725, 2002. 2, 4
Adrian S. Lewis and Shanshan Zhang. Partial smoothness, tilt stability, and generalized
hessians. 23(1):74-94, 2013. 2
Yu-Sheng Li, Wei-Lin Chiang, and Ching-pei Lee. Manifold identification for ultimately
communication-efficient distributed optimization. In International Conference on Ma-
chine Learning, 2020. 2
Jingwei Liang, Jalal Fadili, and Gabriel Peyre. Activity identification and local linear con-
vergence of forward-backward-type methods. 27(1):408-437, 2017a. 2
Jingwei Liang, Jalal Fadili, and Gabriel Peyre. Local convergence properties of douglas-
rachford and alternating direction method of multipliers. Journal of Optimization Theory
and Applications, 172(3):874-913, 2017b. 2
Lam M. Nguyen, Katya Scheinberg, and Martin Takac. Inexact SARAH algorithm for
stochastic optimization. Optimization Methods and Software, 36(1):237-258, 2021. 2
11
Evgeni Alekseevich Nurminskii. The quasigradient method for the solving of the nonlinear
programming problems. Cybernetics, 9(1):145-150, 1973. 5
Brendan O’donoghue and Emmanuel Candes. Adaptive restart for accelerated gradient
schemes. Foundations of computational mathematics, 15(3):715-732, 2015. 7
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-
perative style, high-performance deep learning library. Advances in neural information
processing systems, pp. 8026-8037, 2019. 7
Nhan H. Pham, Lam M. Nguyen, Dzung T. Phan, and Quoc Tran-Dinh. ProxSARAH: An
efficient algorithmic framework for stochastic composite nonconvex optimization. Journal
of Machine Learning Research, 21:1-48, 2020. 2
Rene Poliquin and R Rockafellar. Prox-regular functions in variational analysis. Transac-
tions of the American Mathematical Society, 348(5):1805-1838, 1996. 4, 5
Clarice Poon, Jingwei Liang, and Carola-Bibiane Schonlieb. Local convergence properties of
SAGA/prox-SVRG and acceleration. In International Conference on Machine Learning,
2018. 2
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale
image recognition. 2015. 8
Yifan Sun, Halyun Jeong, Julie Nutini, and Mark Schmidt. Are we there yet? manifold iden-
tification of gradient-related proximal methods. In International Conference on Artificial
Intel ligence and Statistics, pp. 1110-1119, 2019. 2, 4
Varun Sundar and Rajat Vadiraj Dwaraknath. [reproducibility report] rigging the lottery:
Making all tickets winners. In ML Reproducibility Chal lenge 2020, 2021. 20, 26
Wei Tao, Zhisong Pan, Gaowei Wu, and Qing Tao. Primal averaging: A new gradient evalu-
ation step to attain the optimal individual convergence. IEEE transactions on cybernetics,
50(2):835-845, 2018. 4
Quoc Tran-Dinh, Nhan H. Pham, Dzung T. Phan, and Lam M. Nguyen. Hybrid stochastic
gradient descent algorithms for stochastic nonconvex optimization. Technical report, 2019.
2
Sagar Verma and Jean-Christophe Pesquet. Sparsifying networks via subdifferential inclu-
sion. In International Conference on Machine Learning, pp. 10542-10552, 2021. 3
Zhe Wang, Kaiyi Ji, Yi Zhou, Yingbin Liang, and Vahid Tarokh. Spiderboost and momen-
tum: Faster variance reduction algorithms. In Advances in Neural Information Processing
Systems, pp. 2406-2416, 2019. 2
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured
sparsity in deep neural networks. Advances in neural information processing systems, pp.
2074-2082, 2016. 1, 6, 7
Wei Wen, Yuxiong He, Samyam Rajbhandari, Minjia Zhang, Wenhan Wang, Fang Liu, Bin
Hu, Yiran Chen, and Hai Li. Learning intrinsic sparse structures within long short-term
memory. In International Conference on Learning Representations, 2018. 1
Stephen J. Wright. Accelerated block-coordinate relaxation for regularized optimization.
SIAM Journal on Optimization, 22(1):159-186, 2012. 2
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for
benchmarking machine learning algorithms. Technical report, 2017. arXiv:1708.07747. 8
Lin Xiao. Dual averaging methods for regularized stochastic learning and online optimiza-
tion. Journal of Machine Learning Research, 11(88):2543-2596, 2010. 3, 5
12
Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance
reduction. SIAM Journal on Optimization, 24(4):2057-2075, 2014. 2
Yang Yang, Yaxiong Yuan, Avraam Chatzimichailidis, Ruud JG van Sloun, Lei Lei, and
Symeon Chatzinotas. ProxSGD: Training structured neural networks under regularization
and constraints. In International Conference on Learning Representations, 2019. 1, 6, 7
Ming Yuan and Yi Lin. Model selection and estimation in regression with grouped variables.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68(1):49-67,
2006. 6
Jihun Yun, Aurelie C Lozano, and Eunho Yang. Adaptive proximal gradient methods for
structured neural networks. Advances in Neural Information Processing Systems, 2021.
1, 7
Cun-Hui Zhang. Nearly unbiased variable selection under minimax concave penalty. The
Annals of statistics, 38(2):894-942, 2010. 21
Peng Zhao and Bin Yu. On model selection consistency of Lasso. The Journal of Machine
Learning Research, 7:2541-2563, 2006. 1
Hao Zhou, Jose M. Alvarez, and Fatih Porikli. Less is more: Towards compact CNNs. In
European Conference on Computer Vision, pp. 662-677. Springer, 2016. 6
Appendices
Table of Contents
A Proofs	13
A.1 Proof of Lemma 1
A.2 Proof of Lemma 2
13
14
A.3 Proof of Theorem 1 .......................................
16
A.4 Proof of Theorem 2 .......................................
17
B	Additional Discussions on Applications	18
B.1 Structured Sparsity ......................................... 18
B.2 Binary Neural Networks ...................................... 20
C	Experiment Setting Details	20
D	More Results from the Experiments	20
E Other Regularizers for Possibly Better Group Sparsity and General-
ization	21
A Proofs
A.1 Proof of Lemma 1
Proof. Using (3), the distance between Wt and W * can be upper bounded through the
triangle inequality:
IlWt - W*∣∣ = ∣∣(1 - Ct)(Wt-1 - W*) + Ct (Wt - W*)∣∣
≤ Ct∣∣IWt - W*∣∣ + (1 - Ct) ∣∣WtT- W*∣∣.	(8)
13
For any event such that Wt → W*, for any t > 0, we can find Te ≥ 0 such that
IlWt - W*∣∣ ≤ g	∀t ≥ Te.
Let δt := IlWt — W*∣∣, we see from the above inequality and (8) that
δt ≤ (I - ct) δt-1 + ct3 ∀t ≥ Te.
By deducting E from both sides, we get that
(δt - e ) ≤ (I - ct)(δt -1 - E) , ∀t ≥ Te.
Since	ct = ∞, we further deduce that
∞
lim (δt - E) ≤ H (1 - ct) (δτe-1 - e )
t →∞	ɪ ɪ
t=T
∞
≤ Π exP(- ct )(δTe-1 - E)
t=T
exp
Ht)
(δTT - E) = 0,
where in the first inequality we used the fact that exp(x) ≥ 1 + x for all real number x. The
result ab ove then implies that
lim δt ≤ E.
t →∞
As E is arbitrary and δt ≥ 0 from the definition, we conclude that limt→∞ δt = 0, which is
equivalent to that Wt → W*.	□
A.2 Proof of Lemma 2
Proof. We observe that
α-1 Vt =古 ηkβk	(WkT)
H αt
此 α-11 Vt-1 +:"(W t-1)
(1 -也)°二1 Vt-1 + M ▽%(Wt-1).
∖ at J	at
From that f is L-Lipschitz-continuously differentiable, we have that
∣∣Eξt+s [Vfξt+1 (Wt)] - Eξt〜D [vfξt (Wt-1)]∣∣ = IRf (Wt) - f (Wt-1)∣∣
≤ l∣∣ Wt - Wt-1∣∣.
Therefore, (4) and (9) imply that
∣∣Eξt +1~D [v fξt+1 ( W t)] - Eξt -D [v fξt (WtT)]∣∣ <∕∣ W t - WtTn	4
0 ≤	-1≤ L	-1→
一	βtηta—	-	βtηta-
which together with the sandwich lemma shows that
∣∣Eξt +1~D [Vfξt+1 (Wt)] - Eξt〜D [Vfξt (Wt-1)] ∣∣	-→	0
βtηta-i
(9)
0,
(10)
Therefore, the first two conditions of (4) together with (10) and the bounded variance
assumption satisfy the requirements of (Gupal, 1979, Chapter 2.4, Theorem 4.1), so the
conclusion of almost sure convergence hold.
14
For the convergence in L? part, we first define mt := α-1 Vt and Tt := βtηtα-r for notational
ease. Consider ∣∣ mt +1 — V F (Wt )∣∣ , we have from the update rule in Algorithm 1 that
Il mt+1 — V F (Wt )∣∣2
=∣∣(1 — τt) mt + τt V fξt +ι (Wt) — V F (Wt) ∣∣2
=∣∣(1 — τt)(mt — V F (Wt)) + Tt(VfQ (Wt) — V F (Wt)) ∣∣2
=(1 — Tt )2∣∣ mt — V F (Wt )∣∣2 + Tt21∣ V fξt+1 (Wt) — V F (Wt )∣∣2
+ 2 τt (1 — τt M mt — V F (Wt), V fξt +1 (Wt) — V F (Wt ))
=(1 — τt )2∣∣ (mt — V F (Wt-1)) + (V F (Wt -1) — V F (Wt ))∣∣2	(11)
+ τt2∣∣V fξt+1 (Wt) — V F (Wt )∣∣2 + 2 τt (1 — τt M mt — V F (Wt), V fξt+ι (Wt) — V F (Wt )).
Let {Ft}t≥0 denote the natural filtration of {(mt, Wt)}t≥0. Namely, Ft records the informa-
tion of W0, { Ci} i-0, {ηi} t = 0, and { ξi} t =1. By defining Ut := ∣∣ mt — V F (Wt-1) ∣2 and taking
expectation over (11) conditional on Ft, We obtain from E [Vfξt +1 (Wt) | Ft = VF(Wt)
that
E [ Ut+ι ∣Ft ] = (1 — τt )2∣∣ (mt — V F (WtT)) + (V F (Wt-1) — V F (Wt ))∣∣2
+ τt2E [∣∣V fξt (Wt) —V F (Wt )∣∣2 ∣Ft ] .	(12)
From the last condition in (4) and the Lipschitz continuity of V F, there are random variables
{et} and {ut} such that ∣∣ Ut ∣∣ = 1, Et ≥ 0, and V F (Wt-1) — V F (Wt) = TtttUtfOr all t > 0,
with Ct J 0 almost surely. We thus obtain that
∣∣ mt — V F (Wt-1) + V F (Wt-1) — V F (Wt )∣∣2
=∣∣ mt — V F (Wt-1)+ TtetUt ∣∣2
[	2
= (1+ Tt)2 -- (mt — VF (Wt-1)) + [ T etUt∣∣
1 + Tt、	1 + Tt
≤ (1 + τt)2 (τ+τtUt + 1⅛et2),	(13)
where we used Jensen,s inequality and the convexity of ∣∣∙∣2 in the last inequality. By
substituting (13) back into (12), we obtain
E [ Ut+ι∣Ft ]
≤ (1 — Tt )2(1 + Tt) Ut + (1 — Tt )2(1 + Tt) Ttet2 + Tt 2E [∣∣V f1(Wt) — V F (Wt )∣∣2 ∣ Ft ]
≤ (1 — τt)(Ut + τtet2) + τt 2E [∣∣V fξt (Wt) — V F (Wt )∣∣2 ∣Ft ]
≤ (1 — τt) Ut + τt et2 + τt 2E [∣∣V fξt (Wt) — V F (Wt )∣∣2 ∣Ft ] .	(14)
For the last term in (14), we notice that
E [∣∣V fξt (Wt) — V F (Wt )∣∣2 ∣Ft ] ≤ 2 (E [∣∣Vfξt (Wt )∣∣2] + ∣∣V F (Wt )∣∣2)
≤ 2 (C +∣∣VF(Wt)∣∣2) ,	(15)
where the last inequality is from the bounded variance assumption. Since by assumption
the {Wt} lies in a bounded set K, we have that for any point W * ∈ K, Wt — W * is upper
bounded, and thus ∣∣V F (Wt) — V F (W *)∣ is also bounded, implying that ∣∣V F (Wt )∣2 ≤ C2
for some C2 ≥ 0. Therefore, (15) further leads to
E [∣∣Vfξt(Wt) — VF(Wt)∣∣2 ∣Ft] ≤ C3	(16)
15
for some C3 ≥ 0.
Now we further take expectation on (14) and apply (16) to obtain
E Ut +1 ≤ (I - Tt )E Ut + Ttet2 + τt2 C3 = (I - Tt) E Ut + Tt (62 + TtC3)∙	(17)
Note that the third implies et J 0, so this together with the second condition that Tt J 0
means e2 + TtC3 J 0 as well, and thus for any δ > 0, we can find Tδ ≥ 0 such that et + TtC3 ≤ δ
for all t ≥ Tδ. Thus, (17) further leads to
EUt+1 - δ ≤ (1 - Tt)EUt + Ttδ - δ = (1 - Tt) (EUt - δ) , ∀t ≥ Tδ .	(18)
This implies that (EUt - δ) becomes a decreasing sequence starting from t ≥ Tδ , and since
Ut ≥ 0, this sequence is lower bounded by -δ, and hence it converges to a certain value. By
recursion of (18), we have that
t
EUt- δ ≤ U (1 - Ti )(EUTδ - δ),
i=Tδ
and from the well-known inequality (1 + x) ≤ expx for all x ∈ R, the above result leads to
EUt 一 δ ≤ exp (一 ^^ i = Tδt‰^ (EUt$ 一 δ).
By letting t approach infinity and noting that the first condition of (4) indicates
∞
Tt = ∞
t=k
for any k ≥ 0, we see that
-δ ≤ lim EUt - δ ≤ exp
t→∞
卜 ^ Ti)
i=Tδ
(EUTδ - δ ) = 0.
(19)
As δ is arbitrary, by taking δ J 0 in (19) and noting the nonnegativity of Ut , we conclude
that lim EUt = 0, as desired. This proves the last result in Lemma 2.	口
A.3 Proof of Theorem 1
Proof. Using Lemma 2, we can view a-1 Vt as Vf (Wt) plus some noise that asymptotically
decreases to zero with probability one:
α-1 Vt = Vf (Wt)+ et,	Ilet∣∣ 一→→ 0.	(20)
We use (20) to rewrite the optimality condition of (2) as (also see Line 5 of Algorithm 1)
一 (Vf (Wt) + et + βtα- (IWt - W0)) ∈ ∂ψ (IWt) .	(21)
Now we consider ∂F (Wt). Clearly from (21), we have that
Vf (Wt) - Vf (Wt) - et - βtα- (Wt - W0) ∈ ∂Vf (Wt) + ψ (Wt) = ∂F (Wt) . (22)
Now we consider the said event that Wt → W * for a certain W *, and let us define this
event as A ⊆ Ω. From Lemma 1, we know that Wt → W * as well under A. Let us define
B ⊆ Ω as the event of et → 0, then we know that since P(A) > 0 and P(B) = 1, where P is
the probability function for events in Ω, P (A ∩ B) = P (A). Therefore, conditional on the
event of A, we have that et -→→ 0 still holds. Now we consider any realization of A∩B. For
the right-hand side of (22), as Wt is convergent and βtα-1 decreases to zero, by letting t
approach infinity, we have that
^lim et + βtα-r (Wt - W0) =0 + 0 (W * - W0) = 0.
By the Lipschitz continuity of Vf, we have from (3) and (4) that
0 ≤ IlVf (Wt) - Vf (Wt) Il ≤ LllWt - Wtll.
16
As {Wt} and {Wt} converge to the same point, We see that ∣∣ Wt - W[∣ → 0, so Vf (Wt)-
Vf (Wt) also approaches	zero.	Hence,	the limit of	the right-hand side	of (22)	is
JIim Vf	(Wt)	- (Vf	(Wt) + G +	βtα-1 (Wt - W0))	= 0.	(23)
On the other hand, for the left-hand side of (22), the outer semicontinuity of ∂ψ at W * and
the continuity of Vf shoW that
lim Vf (Wt) + ∂ψ(Wt) ⊆ ∂Vf (W*) + ψ (W*) = ∂F(W*).	(24)
t→∞
Substituting (23) and (24) back into (22) then proves that 0 ∈ ∂F(W*) and thus W* ∈
Z.	□
A.4 Proof of Theorem 2
Proof. Our discussion in this proof are all under the event that Wt → W* . From the argu-
ment in Appendix A.3, We can vieW αt-1V t as Vf(Wt) plus some noise that asymptotically
decreases to zero With probability one as shoWn in (20). From Lemma 1, We knoW that
Wt → W*. From (21), there is Ut ∈ ∂ψ (Wt) such that
Ut = -α-1 Vt + a-1 βt (Wt - W0) .	(25)
Moreover, We define
Yt := Wt - Wt.	(26)
By combining (25)-(26) with (20), we obtain
min	HYll
y ∈ ∂f (Wt)
≤ ∣∣Vf(Wt) + Ut∣∣
=∣∣Vf (IWt) - Vf (Wt) - q - α-1 βt(Wt - W0) ∣∣
≤∣∣Vf (IWt) - Vf (Wt) ∣∣ +1ql + α-1 βt∣∣Wt- W0∣∣
≤LHYtH + HQH + α-1 βt (∣∣W* - Wt∣∣ + ∣∣W0 - W*∣∣) ,	(27)
where we used the Lipschitz continuity of Vf and the triangle inequality in the last inequal-
ity.
We now separately bound the terms in (27). From that Wt → W * and Wt → W *, it is
straightforward that ∣∣Yt∣∣ → 0. The second term decreases to zero almost surely according
to (20) and the argument in Appendix A.3. For the last term, since αt-1 βt → 0, and
∣∣ Wt — W * ∣∣ → 0, we know that
α-1 βt∣∣W0 - W*∣∣ → 0, α-1 βt∣∣Wt- W*∣∣ → 0.
Therefore, we conclude from the above argument and (27) that
min	H Y ∣∣ -→ 0.
Y ∈ ∂F (Wt)
As f is smooth with probability one, we know that if ψ is partly smooth at W* relative to M,
then so is F = f + ψ with probability one. Moreover, Lipschitz-continuously differentiable
functions are always prox-regular, and the sum of two prox-regular functions is still prox-
regular, so F is also prox-regular at W* with probability one. Following the argument
identical to that in Appendix A.3, we know that these probability one events are still
probability one conditional on the event of Wt → W * as this event has a nonzero probability.
As Wt → W * and V f (Wt) + Ut -→ 0 ∈ ∂F (W *) (the inclusion is from (5)), we have from
the subdifferential continuity of ψ and the smoothness of f that F(Wt) -→ F(W*). Since
we also have Wt → W * and min Y ∈ ∂f( W t) H Y H -→ 0, clearly
(Wt,F (Wt) ,	min
∖	Y ∈ ∂F ( Wt)
HYH)
-a-.→s. (W*,F(W*),0) .
(28)
Therefore, (28) and (5) together with the assumptions on ψ at W* imply that with proba-
bility one, all conditions of Lemma 1 of Lee (2020) are satisfied, so from it, (6) holds almost
surely, conditional on the event of Wt → W*.	□
17
B Additional Discussions on Applications
We now discuss in more technical details the applications in Section 4.1, especially regarding
how the regularizers satisfy the properties required by our theory.
B.1	Structured Sparsity
We start our discussion with the simple £ 1 norm as the warm-up for the group-LASSO norm.
It is clear that ∣∣ W h is a convex function that is finite everywhere, so it is prox-regular,
subdifferentially continuous, and regular everywhere, hence we just need to discuss about
the remaining parts in Definition 1. Consider a problem with dimension n > 0. Note that
n
Il 2 Ili =工 | xi |,
i=1
and the absolute value is smooth everywhere except the point of origin. Therefore, it is
clear that ∣∣xh is locally smooth if xi = 0 for all i. For any point x*, when there is an index
set I such that x* = 0 for all i ∈ I and x* = 0 for i / I, We see that the part of the norm
corresponds to IC (the complement of I):
E i x"
i∈IC
is locally smooth around x*. Without loss of generality, We assume that I = {1, 2,...,k}
for some k ≥ 0, then the subdifferential of ∣∣x∣∣ i at x* is the set
{sgn(xi)} × …× {sgn(xk)} × [-1, 1]n-k,	(29)
and clearly if we move from x* along any direction y := (yi, . . . , yk, 0, . . . , 0) with a small
step, the function value changes smoothly as it is a linear function, satisfying the first
condition of Definition 1. Along the same direction y with a small enough step, the set
of subdifferential remains the same, so the continuity of subdifferential requirement holds.
We can also observe from the above argument that the manifold should be M方* = {x ∣
xi = 0, ∀i ∈ I}, and clearly it is a subspace of Rn with its normal space at x* being
N := {y ∣〈 x *, y〉= 0} = {y ∣ yi = 0, ∀ i / IC}, which is clearly the affine span of (29) with
the translation being (sgn( x i) ×…× sgn( xk), 0,..., 0). Moreover, indeed the manifolds are
low dimensional ones, and for iterates approaching x* , staying in this active manifold means
that the (unstructured) sparsity of the iterates is the same as the limit point x* . We also
provide a graphical illustration of ∣∣xh with n = 2 in Fig. 3. We can observe that for any
x with xi = 0 and x2 = 0, the function is smooth locally around any point, meaning that
IlxIl i is partly smooth relative to the whole space at x (so actually smooth locally around
x). For x with xi = 0, the function value corresponds to the sharp valley in the graph, and
we can see that the function is smooth along the valley, and this valley corresponds to the
one-dimensional manifold {x | xi = 0} for partial smoothness.
Next, we use the same graph to illustrate the importance of manifold identification. Consider
that the red point x* = (0, 1.5) is the limit point of the iterates of a certain algorithm, and
the yellow points and black points are two sequences that both converge to x* . If the iterates
of the algorithm are the black points, then clearly except for the limit point itself, all iterates
are nonsparse, and thus the final output of the algorithm is also nonsparse unless we can get
to exactly the limit point within finite iterations (which is usually impossible for iterative
methods). On the other hand, if the iterates are the yellow points, this is the case that
the manifold is identified, because all points sit in the valley and enjoy the same sparsity
pattern as the limit point x* . This is why we concern about manifold identification when
we solve regularized optimization problems.
From this example, we can also see an explanation for why our algorithm with the property
of manifold identification performs better than other methods without such a property.
Consider a Euclidean space any point x* with an index set I such that xI* = 0 and |I | > 0.
This means that x* has at least one coordinate being zero, namely x* contains sparsity.
Now let
Co := min ∣ x*|,
0 i∈IC i
18
Figure 3: An illustration of partial smoothness of the £ 1 norm.
then from the definition of I, e0 > 0. Fro any sequence {xt} converging to x*, for any
E ∈ (0, E0), We can find Te ≥ 0 such that
Ilxt — x*∣∣2 ≤ e, ∀t ≥ Te.
Therefore, for any i / I, we must have that xt = 0 for all t ≥ Te. Otherwise/xt — x* |卜 ≥ Eo,
but eo > e ≥ Ilxt — x*∣∣2, leading to a contradiction. On the other hand, for any i ∈ I, we can
have xit = 0 for all t without violating the convergence. That being said, for any sequence
converging to x* , eventually the iterates cannot be sparser than x* , so the sparsity level
of x* , or of its active manifold, is the local upper bound for the sparsity level of points
converging to x* . Therefore, if iterates of two algorithms converge to the same limit point,
the one with a proven manifold identification ability clearly will produce a higher sparsity
level.
Similar to our example here, in applications other than sparsity, iterates converging to a
limit point dwell on super-manifolds of the active manifold, and the active manifold is the
minimum one that locally describes points with the same structure as the limit point, and
thus identifying this manifold is equivalent to finding the locally most ideal structure of the
application.
Now back to the sparsity case. One possible concern is the case that the limit point is (0, 0)
in the two-dimension example. In this case, the manifold is the 0-dimensional subspace {0}.
If this is the case and manifold identification can be ensured, it means that limit point itself
can be found within finite iterations. This case is known as the weak sharp minima (Burke
& Ferris, 1993) in nonlinear optimization, and its associated finite termination property is
also well-studied.
For this example, We also see that ∣∣xh is partly smooth at any point x*, but the manifold
differs with x* . This is a specific benign example, and in other cases, partial smoothness
might happen only locally at some points of interest instead of everywhere.
Next, we further extend our argument above to the case of (7). This can be viewed as
the £ι norm for each group and we can easily obtain similar results. Again, since the
group-LASSO norm is also convex and finite everywhere, prox-regularity, regularity, and
subdifferential continuity are not issues at all. For the other properties, we consider one
group first, then the group-LASSO norm reduces to the £2 norm. Clearly, ∣∣x∣∣2 is smooth
locally if x = 0, with the gradient being x/1∣x∣∣2, but it is nonsmooth at the point x = 0,
where the subdifferential is the unit ball. This is very similar to the absolute value, whose
subdifferential at 0 is the interval [—1, 1]. Thus, we can directly apply similar arguments
above, and conclude that for any W*, (7) is partly smooth at W* with respect to the
manifold M W* = {W | WIg = 0, ∀g : W* = 0}, which is again a lower-dimensional
subspace. Therefore, the manifold of defining the partial smoothness for the group-LASSO
norm exactly corresponds to its structured sparsity pattern.
19
B.2	Binary Neural Networks
We continue to consider the binary neural network problem. For easier description, for
the Euclidean space E we consider, we will use a vectorized representation for W, A ∈ E
such that the elements are enumerated as W1 , . . . , Wn and α1 , . . . , αn . The corresponding
optimization problem can therefore be written as
min
W,A∈E
n
Eξ~D [ fξ (W)] + λ ^^ ^i (Wi + 1)2 + (I- αi)(wi - 1)2 + δ [0,1] (ai )^ ,
i=1
(30)
where given any set C , δC is the indicator function of C , defined as
0 if x ∈ C,
δC(x) = ∞	else.
We see that except for the indicator function part, the objective is smooth, so the real partly
smooth term that we treat as the regularizer is
n
Φ(α) :=	δ[0,1] (αi).
i=1
We note that for αi ∈ (0, 1), the value of δ[0,1] (αi) remains a constant zero in a neighbor-
hood of αi, and for αi ∈/ [0, 1], the indicator function is also constantly infinite within a
neighborhood. Thus, the point of nonsmoothness, happens only atαi ∈ {0, 1}, and similar
to our discussion in the previous subsection, Φ is partly smooth along directions that we fix
those αi at the boundary (namely, being either 0 or 1) unchanged. The identified manifold
therefore corresponds to the entries ofαi that are fixed at 0 or 1, and this can serve as the
indicator for the desired binary pattern in this task.
C Experiment Setting Details
For the weights wg of each group in (7), for all experiments in Section 5, we follow Deleu &
Bengio (2021) to set wg = ,|Ig |. All ProXSSI parameter settings, excluding the regulariza-
tion weight and the learning rate schedule, follow the default values in their package.
Tables 3 to 13 provide detailed settings of Section 5.2. For the modified VGG19 model,
we follow Deleu & Bengio (2021) to eliminate all fully-connected layers except the output
layer, and add one batch-norm layer (Ioffe & Szegedy, 2015) after each convolutional layer
to simulate modern CNNs like those proposed in He et al. (2016); Huang et al. (2017). For
ResNet50 in the structured sparsity experiment in Section 5.2, our version of ResNet50 is
the one constructed by the publicly available script at https://github.com/weiaicunzai/
pytorch-cifar100.
In the unstructured sparsity experiment presented in Section 5.3, for better comparison
with existing works in the literature of pruning, we adopt the version of ResNet50 used by
Sundar & Dwaraknath (2021).3 Table 14 provides detailed settings of Section 5.3. For RigL,
we use the PyTorch implementation of Sundar & Dwaraknath (2021).
D More Results from the Experiments
In this section, we provide more details of the results of the experiments we conducted in the
main text. In particular, in Fig. 4, we present the change of validation accuracies and group
sparsity levels with epochs for the group sparsity tasks in Section 5.2. We then present in
Fig. 5 validation accuracies and unstructured sparsity level versus epochs for the task in
Section 5.3. We note that although it takes more epochs for RMDA to fully stabilize in
terms of manifold identification, the sparsity level usually only changes in a very limited
range once (sometimes even before) the validation accuracy becomes steady, meaning that
we do not need to run the algorithm for an unreasonably long time to obtain satisfactory
results.
3https://github.com/varun19299/rigl-reproducibility.
20
Table 3: Details of the experimental settings of logistic regression on MNIST in Section 5.2.
Parameter	Value
Data set Model Loss function Regularization function Regularization weight Total epochs	MNIST Logistic regression Cross entropy Group LASSO 10-3 500	
ProXSGD	
Learning rate schedule Momentum	η (epoch) = 10TTePoch/50」 10-1	
ProxSSI	
Learning rate schedule	η (epoch) = 10-3TePoch/50」
	RMDA		
Restart epochs Learning rate schedule Momentum schedule	50, 100,150, 200 η (epoch) = max(10-5, 10-1-Lepoch/50」) c (epoch) = min(1, 10-2+Lepoch/50」)
Table 4: Details of the experimental settings of the multi-layer fully-connected NN on
FashionMNIST in Section 5.2.
Parameter	Value
Data set Model Loss function Regularization function Total epochs	FaShiOnMNIST Fully-connected NN (Table 11) Cross entropy Group LASSO 500	
ProxSGD	
Regularization weight Learning rate schedule Momentum	10-4 η (epoch) = 10-1-LePoch h 50J 10-1	
ProxSSI	
Regularization weight Learning rate schedule	4 × 10-6 η (epoch) = 10-3-LePoch h 50J	
	RMDA		
Regularization weight Restart epochs Learning rate schedule Momentum schedule	7 × 10-5 50, 100, 150, 200 η (epoch) = max(10-5, 10-1-LePoch/50J) c (epoch) = min(1, 10-2+LePoch/50J)
E Other Regularizers for Possibly Better Group Sparsity
and Generalization
A downside of (7) is that it pushes all groups toward zeros and thus introduces bias in the
final model. For its remedy, minimax concave penalty (MCP, Zhang, 2010) is then proposed
to penalize only the groups whose norm is smaller than a user-specified threshold. More
precisely, given hyperparameters λ ≥ 0, ω ≥ 1, the one-dimensional MCP is defined by
MCP(w; λ, ω) :
if|w| < ωλ,
if|w| ≥ ωλ.
One can then apply the above formulation to the norm of a vector to achieve the effect of
inducing group-sparsity. In our case, given an index set Ig that represents a group, the
21
Table 5: Details of the experimental settings of LeNet5 on MNIST in Section 5.2
Parameter	Value
Data set Model Loss function Regularization function Total epochs	MNIST LeNet5 (Table 12) Cross entropy Group LASSO 500	
ProXSGD	
Regularization weight Learning rate schedule Momentum	1.2 × 10-4 η (epoch) = 10TTePOch / 50J 10-1	
ProxSSI	
Regularization weight Learning rate schedule	9 × 10-5 η (epoch) = 10-3TePOch /50J
	RMDA		
Regularization weight Restart epochs Learning rate schedule Momentum schedule	10-4 50,100,150, 200 η(epoch) = max(10-4, 10-LePOch/50J) c (epoch) = min(1, 10-2+Lepoch /50J)
Table 6: Details of the experimental settings of LeNet5 on FashionMNIST in Section 5.2
Parameter	Value
Data set Model Loss function Regularization function Total epochs	FashionMNIST LeNet5 (Table 12) Cross entropy Group LASSO 500	
ProxSGD	
Regularization weight Learning rate schedule Momentum	1.2 × 10-4 η (epoch) = 10-1-LePOch /50J 10-1	
ProxSSI	
Regularization weight Learning rate schedule	6 × 10-5 η (epoch) = 10-3-LePOch /50J
	RMDA		
Regularization weight Restart epochs Learning rate schedule Momentum schedule	10-4 50,100, 150, 200 η(epoch) = max(10-4, 10-LePOch/50J) c (epoch) = min(1, 10-2+LePOch /50J)
MCP for this group is then computed as (Breheny & Huang, 2009)
MCP (WIg; λg,ωg) := ∙	f 讨 WIJ -⅛^ ifU WIJ“ λ, ∖ ωgλg2	if∣l WIJ ≥ ωgλg.
We then consider	|G|
ψ(W)	=E MCP (Wrg; λg ,ωg) .	(31) g=1
It is shown in Deleu & Bengio (2021) that group MCP regularization may simultaneously
provide higher group sparsity and better validation accuracy than the group LASSO norm
in vision and language tasks. Another possibility to enhance sparsity is to add another £i-
norm or entry-wise MCP regularization to the group-level regularizer. The major drawback
22
Table 7: Details of the experimental settings of the modified VGG19 on CIFAR10 in Sec-
tion 5.2.
Parameter	Value
Data set Model Loss function Regularization function Total epochs	CIFAR10 VGG19 (Table 13) Cross entropy Group LASSO 1000	
ProXSGD	
Regularization weight Learning rate schedule Momentum	5 × 10-5 η (epoch) = 10TTePOch / 100J 10-1	
ProxSSI	
Regularization weight Learning rate schedule	3 × 10-7 η (epoch) = 10-3TePOch / 100j
	RMDA		
Regularization weight Restart epochs Learning rate schedule Momentum schedule	10-4 100, 200, 300,400, 500 η (epoch) = max(10-6, 10T-LePOch /100J) c (epoch) = min(1, 10-2+Lepoch /100J)
Table 8: Details of the experimental settings of the modified VGG19 on CIFAR100 in
Section 5.2.
Parameter	Value
Data set Model Loss function Regularization function Total epochs	CIFAR100 VGG19 (Table 13) Cross entropy Group LASSO 1000	
ProxSGD	
Regularization weight Learning rate schedule Momentum	3 × 10-5 η (epoch) = 10T-LePOch / 100j 10-1	
ProxSSI	
Regularization weight Learning rate schedule	10-7 η (epoch) = 10-3-LePOch / 100j
	RMDA		
Regularization weight Restart epochs Learning rate schedule Momentum schedule	10-4 100, 200, 300, 400, 500 η (epoch) = max(10-6, 10T-LePOch /100J) c (epoch) = min(1, 10-2+LePOch /100J)
of these approaches is the requirement of additional hyperparameters, and we prefer simpler
approaches over those with more hyperparameters, as hyperparameter tuning in the latter
can be troublesome for users with limited computational resources, and using a simpler
setting can also help us to focus on the comparison of the algorithms themselves. The
experiment in this subsection is therefore only for illustrating that these more complicated
regularizers can be combined with RMDA if the user wishes, and such regularizers might
lead to better results. Therefore, we train a version of LeNet5, which is slightly simpler
than the one we used in previous experiments, on the MNIST dataset with such regularizers
using RMDA and display the respective performance of various regularization schemes in
Fig. 6. For the weights wg of each group in (7), in this experiment we consider the following
setting. Let Li be the collection of all index sets that belong to the i-th layer in the network,
23
Table 9: Details of the experimental settings of ResNet50 on CIFAR10 in Section 5.2.
We use the ResNet50 model from the public script https://github.com/weiaicunzai/
pytorch-cifar100.
Parameter	Value
Data set Model Loss function Regularization function Total epochs	CIFAR10 ResNet50 Cross entropy Group LASSO 1500	
ProXSGD	
Regularization weight Learning rate schedule Momentum	5 × 10-5 η (epoch) = 10TTePOch / 150J 10-1	
ProxSSI	
Regularization weight Learning rate schedule	3 × 10-7 η (epoch) = 10-3TePOch/150j	
	RMDA		
Regularization weight Restart epochs Learning rate schedule Momentum schedule	IQ-3 150,300,450, 600 η (epoch) = max(10-4, 10-Lepoch /150j) c (epoch) = min(1, 10-2+Lepoch /150j)
Table 10: Details of the experimental settings of ResNet50 on CIFAR100 in Section 5.2.
We use the ResNet50 model from the public script https://github.com/weiaicunzai/
pytorch-cifar100.
Parameter	Value
Data set Model Loss function Regularization function Total epochs	CIFAR100 ResNet50 Cross entropy Group LASSO 1500	
ProxSGD	
Regularization weight Learning rate schedule Momentum	4 × 10-5 η (epoch) = 10T-LePOch /150j 10-1	
ProxSSI	
Regularization weight Learning rate schedule	3x10-7 η (epoch) = 10-3-LePOch /150j
	RMDA		
Regularization weight Restart epochs Learning rate schedule Momentum schedule	10-3 150,300,450, 600 η (epoch) = max(10-4, 10-LePOch /150j) c (epoch) = min(1, 10-2+LePOch /150j)
and denote
NLi :=	|Ij|
Ij ∈Li
the number of parameters in this layer, for all i, We set Wg = ʌ/NLi for all g Such that
Ig ∈ Li. Given two constants λ > 0 and ,ω > 1, The values of λg and ωg in (31) are then
assigned as λg = λwg and ωg = ωwg .
In this figure, group LASSO is abbreviated as GLASSO; £i-norm plus a group LASSO
norm, L1GLASSO; group MCP, GMCP; element-Wise MCP plus group MCP, L1GMCP.
Our results exemplify that different regularization schemes might have different benefits on
24
Table 11: Details of the multi-layer fully-connected NN. https://github.com/
zihsyuan1214/rmda/blob/master/Experiments/Models/mlp.py.
Parameter	Value
Type of layers Number of layers Number of output neurons each layer: 1, 2, 3, 4, 5, 6, 7 Activation function for ConvolUtion/output layer	fully-connected layer 7 512, 256,128, 64, 32, 16,10 relu/softmax
Table 12: Details of the modified LeNet5 for experiments in Section 5.2. https://github.
com/zihsyuan1214/rmda/blob/master/Experiments/Models/lenet5_large.py.
Parameter	Value
Number of layers Number of convolutional layers Number of fully-connected layers Size of convolutional kernels Number of output filters 1, 2 Number of output neurons 3, 4 Kernel size, stride, padding of maxing pooling Operations after convolutional layers Activation function for convolution/output layer	^4 2 2 3 × 3 20, 50 500, 10 2 × 2, none, invalid max pooling relu/softmax
one of the criteria with proper hyperparameter tuning. The detailed numbers are reported
in Table 15 and the experiment settings can be found in Tables 16 and 17.
Table 13: Details of the modified VGG19. https://github.com/zihsyuan1214/rmda/
blob/master/Experiments/Models/vgg19.py.
Parameter	Value
Number of layers Number of convolutional layers Number of fully-connected layers Size of convolutional kernels Number of output filters 1-2, 3-4, 5-8, 9-16 Kernel size, stride, padding of maxing pooling Operations after convolutional layers Activation function for convolution/output layer	17 16 1 3 X 3 64, 128, 256, 512 2 × 2, 2, invalid max pooling, batchnorm relu/softmax
25
Table 14: Details of experimental settings of ResNet50 on CIFAR10 and CIFAR100 for
unstructured sparsity. In this experiment, we adopt the version of ResNet50 in Sundar &
Dwaraknath (2021).
Parameter	Value
Model Loss function	ReSNet50 Cross entropy
	RMDA		
Data Total epochs L1 weight Restart epochs Learning rate schedule Momentum schedule	CIFAR10 1000 10-5 150,300, 450 η(epoch) = max(10-3, 10-Lepoch/ 150J) c(epoch) = min(1, 10-2+Lepoch/150J)
Data Total epochs L1 weight Restart epochs Learning rate schedule Momentum schedule	CIFAR100 1000 3 X 10-5 150,300, 450 η(epoch) = max(10-3, 10-LePoch/ 150J) c(epoch) = min(1, 10-2+Lepoch/150J)
	RgL		
Data Total epochs Sparse initialization Density Prune rate Decay schedule Apply when Interval End when Learning rate Momentum Weight decay Label smoothing Decay frequency Warmup steps Decay factor	CIFAR10 1000 erdos-renyi-kernel 0.02 0.3 cosine step end 100 65918 0.1 0.9 10-4 0.1 20000 1760 0.2
Data Total epochs Sparse initialization Density Prune rate Decay schedule Apply when Interval End when Learning rate Momentum Weight decay Label smoothing Decay frequency Warmup steps Decay factor	CIFAR100 1000 erdos-renyi-kernel 0.02 0.3 cosine step end 100 65918 0.1 0.9 10-4 0.1 20000 1760 0.2	
26
0.25
^ra
>
0.4
0.0
(a) Logistic regression on MNIST
(c)	LeNet5 on MNIST
0.0
>
>>
0.50
(e) VGG19 on CIFAR10
Aue」n。。4 uocep--e> A-SJeds
(g) ResNet50 on CIFAR10
FUlly-Connected NN on FaShionMNIST
0.75
<
0.50
-A~~~A~~⅛÷∙⅛÷⅛~~A~~~Λ-
-B- ProxSGD
→- ProxSSI
-⅛- RMDA
100	200	300	400	500
0.2
100
EpochS
200	300
EpochS
400
500
(b) Fully-connected NN on FashionMNIST
LeNet5 on FashionMNIST
0.90
<
0.85
ra
卫
>
>>
0.0
0.00
0.00
MSg MSGD
-B- PrOXSGD
→- ProxSSI
-⅛- RMDA
100	200	300	400	500
Epochs
0.5
P
100	200	300	400	500
Epochs
(d)	LeNet5 on FashionMNIST
VGG19 on CIFAR100
0.5
0
200
MSGD
-B- PrOXSGD
→- ProxSSI
RMD RMDA
400	600
Epochs
800	1000
0.25
0
200
800	1000
400	600
Epochs
(f) VGG19 on CIFAR100
0.75
0.50
0.25
ResNet50 on CIFAR100
' MSGD
-B- PrOXSGD
→- ProxSSI
→- RMDA
0	200	400	600	800	1000 1200 1400
Epochs
0.50
0.25
0	200	400	600	800	1000 1200 1400
Epochs
(h) ResNet50 on CIFAR100
∕⅜"iz⅜ιzA，一 2.上 J 4:
0
0
* ∙a∙ * *ɪ* ɪ
Figure 4: Group Sparsity and validation accuracy v.s epochs of different algorithms on
various models with the group-LASSO regularization of a single run.
27
(a) ResNet50 on CIFAR10
1.0
0.5
d
ω
0.0
0	200	400	600	800	1000
0.7
<
0.6
lŋ
0. 0.5
>	0	200	400	600	800	1000
Epochs
(b) ResNet50 on CIFAR100
ycaruccA noitadila
Figure 5: Unstructured Sparsity and validation accuracy v.s epochs of RMDA on ResNet50
of a single run.
Epochs
Figure 6: Comparison between group LASSO, L1+group LASSO, Group MCP, L1+Group
MCP
Table 15: Results of training LeNet5 on MNIST using RMDA with different regularizers.
We report mean and standard deviation of three independent runs.
Regularizers	Validation accuracy	Group sparsity
GLASSO	99.11 ± 0.06%	45.33 ± 0.99%
L1GLASSO	99.02 ± 0.01%	58.92 ± 1.30%
GMCP	99.25 ± 0.08%	32.81 ± 0.96%
L1GMCP	99.21 ± 0.03%	32.91 ± 0.35%
Table 16: Details of the modified simpler LeNet5 for the experiment in Appendix E.
https://github.com/zihsyuan1214/rmda/blob/master/Experiments/Models/lenet5_
small.py.
Parameter	Value
Number of layers Number of convolutional layers Number of fully-connected layers Size of convolutional kernels Number of output filters 1, 2 Number of output neurons 3, 4, 5 Kernel size, stride, padding of maxing pooling Operations after convolutional layers Activation function for convolution/output layer	-5 3 2 5 × 5 6,16 120, 84, 10 2 × 2, none, invalid max pooling relu/softmax
28
Table 17: Details of the experimental settings for comparing different regularizers in Ap-
pendix E
Parameter	Value
Data set	MNIST
Model	LeNet5 (Table 16)
Loss function	Cross entropy
Algorithms	RMDA
Total epochs	300
Restart epochs	30, 60, 90, 120
Learning rate schedule	η(epoch) = max(10-5, 10-1-Lepoch/30J)
Momentum schedule	c (epoch) = min(1, 10-2+Lepoch /30J)
	GLASSO		
Group LASSO weight	10-5
	LIGLASSO		
L1 weight	10-4
Group LASSO weight	10-5	
	GMCP		
Group MCP weight	10-5
Y		64	
	LIGMCP		
L1 weight	10-4
Group MCP weight	10-5
Y		64	
29