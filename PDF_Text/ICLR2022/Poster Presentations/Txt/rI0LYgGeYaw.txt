Published as a conference paper at ICLR 2022
Understanding approximate and unrolled
DICTIONARY LEARNING FOR PATTERN RECOVERY
Beno^t Malezieux
Universite Paris-Saclay, Inria, CEA
L2S, Universite Paris-Saclay-CNRS-CentraleSUPelec
benoit.malezieux@inria.fr
Thomas Moreau
Universite Paris-Saclay, Inria, CEA
PalaiseaU, 91120, France
thomas.moreau@inria.fr
Matthieu Kowalski
L2S, Universite Paris-Saclay-CNRS-CentraleSupelec
Gif-sUr-Yvette, 91190, France
matthieu.kowalski@universite-paris-saclay.fr
Ab stract
Dictionary learning consists of finding a sparse representation from noisy data and
is a common way to encode data-driven prior knowledge on signals. Alternating
minimization (AM) is standard for the underlying optimization, where gradient
descent steps alternate with sparse coding procedures. The major drawback of
this method is its prohibitive computational cost, making it unpractical on large
real-world data sets. This work studies an approximate formulation of dictionary
learning based on unrolling and compares it to alternating minimization to find
the best trade-off between speed and precision. We analyze the asymptotic behav-
ior and convergence rate of gradients estimates in both methods. We show that
unrolling performs better on the support of the inner problem solution and during
the first iterations. Finally, we apply unrolling on pattern learning in magnetoen-
cephalography (MEG) with the help of a stochastic algorithm and compare the
performance to a state-of-the-art method.
1	Introduction
Pattern learning provides insightful information on the data in various biomedical applications. Typ-
ical examples include the study of magnetoencephalography (MEG) recordings, where one aims to
analyze the electrical activity in the brain from measurements of the magnetic field around the scalp
of the patient (Dupre la Tour et al., 2018). One may also mention neural oscillations study in the
local field potential (Cole & Voytek, 2017) or QRS complex detection in electrocardiograms (Xiang
et al., 2018) among others.
Dictionary learning (Olshausen & Field, 1997; Aharon et al., 2006; Mairal et al., 2009) is particu-
larly efficient on pattern learning tasks, such as blood cells detection (Yellin et al., 2017) and MEG
signals analysis (Dupre la Tour et al., 2018). This framework assumes that the signal can be de-
composed into a sparse representation in a redundant basis of patterns - also called atoms. In other
words, the goal is to recover a sparse code Z ∈ Rn×T and a dictionary D ∈ Rm×n from noisy mea-
surements Y ∈ Rm×T which are obtained as the linear transformation DZ, corrupted with noise
B ∈ Rm×T: Y = DZ + B. Theoretical elements on identifiability and local convergence have
been proven in several studies (Gribonval et al., 2015; Haeffele & Vidal, 2015; Agarwal et al., 2016;
Sun et al., 2016). Sparsity-based optimization problems related to dictionary learning generally rely
on the usage of the `0 or `1 regularizations. In this paper, we study Lasso-based (Tibshirani, 1996)
dictionary learning where the dictionary D is learned in a set of constraints C by solving
1
Z∈RminD∈C F(Z，D) , 2 kDZ - Yk2 + λ kZk1
(1)
1
Published as a conference paper at ICLR 2022
Dictionary learning can be written as a bi-level optimization problem to minimize the cost function
with respect to the dictionary only, as mentioned in Mairal et al. (2009),
min G(D)，F(Z* (D), D)	with	Z*(D) = arg min F(Z, D) .	(2)
D∈C	Z∈Rn×T
Computing the data representation Z* (D) is often referred to as the inner problem, while the global
minimization is the outer problem. Classical constraint sets include the unit norm, where each
atom is normalized to avoid scale-invariant issues, and normalized convolutional kernels to perform
Convolutional Dictionary Learning (Grosse et al., 2007).
Classical dictionary learning methods solve this bi-convex optimization problem through Alternat-
ing Minimization (AM) (Mairal et al., 2009). It consists in minimizing the cost function F over Z
with a fixed dictionary D and then performing projected gradient descent to optimize the dictionary
with a fixed Z. While AM provides a simple strategy to perform dictionary learning, it can be ineffi-
cient on large-scale data sets due to the need to resolve the inner problems precisely for all samples.
In recent years, many studies have focused on algorithm unrolling (Tolooshams et al., 2020; Scetbon
et al., 2021) to overcome this issue. The core idea consists of unrolling the algorithm, which solves
the inner problem, and then computing the gradient with respect to the dictionary with the help of
back-propagation through the iterates of this algorithm. Gregor & LeCun (2010) popularized this
method and first proposed to unroll ISTA (Daubechies et al., 2004) - a proximal gradient descent
algorithm designed for the Lasso - to speed UP the computation of Z*(D). The N + 1-th layer
of this network 一 called LISTA - is obtained as ZN +ι = STλ (W 1Y + WClZN), with ST be-
ing the soft-thresholding operator. This work has led to many contributions aiming at improving this
method and providing theoretical justifications in a supervised (Chen et al., 2018; Liu & Chen, 2019)
or unsupervised (Moreau & Bruna, 2017; Ablin et al., 2019) setting. For such unrolled algorithms,
the weights W1 and W2 can be re-parameterized as functions of D - as illustrated in Figure A in
appendix - such that the output ZN (D) matches the result of N iterations of ISIA, i.e.
WD = 1 D>
L
and
WD2
I-
ɪ D> D
L
where L = kD k2 .
(3)
Then, the dictionary can be learned by minimizing the loss F(ZN (D), D) over D with back-
propagation. This approach is generally referred to as Deep Dictionary Learning (DDL). DDL and
variants with different kinds of regularization (Tolooshams et al., 2020; Lecouat et al., 2020; Scetbon
et al., 2021), image processing based on metric learning (Tang et al., 2020), and classification tasks
with scattering (Zarka et al., 2019) have been proposed in the literature, among others. While these
techniques have achieved good performance levels on several signal processing tasks, the reasons
they speed up the learning process are still unclear.
In this work, we study unrolling in Lasso-based dictionary learning as an approximate bi-level opti-
mization problem. What makes this work different from Bertrand et al. (2020), Ablin et al. (2020)
and Tolooshams & Ba (2021) is that we study the instability of non-smooth bi-level optimization
and unrolled sparse coding out of the support, which is of major interest in practice with a small
number of layers. In Section 2, we analyze the convergence of the Jacobian computed with auto-
matic differentiation and find out that its stability is guaranteed on the support of the sparse codes
only. De facto, numerical instabilities in its estimation make unrolling inefficient after a few dozen
iterations. In Section 3, we empirically show that unrolling leads to better results than AM only with
a small number of iterations of sparse coding, making it possible to learn a good dictionary in this
setting. Then we adapt a stochastic approach to make this method usable on large data sets, and we
apply it to pattern learning in magnetoencephalography (MEG) in Section 4. We do so by adapting
unrolling to rank one convolutional dictionary learning on multivariate time series (Dupre la Tour
et al., 2018). We show that there is no need to unroll more than a few dozen iterations to obtain
satisfying results, leading to a significant gain of time compared to a state-of-the-art algorithm.
2	B i-level optimization for approximate dictionary learning
As Z* (D) does not have a closed-form expression, G cannot be computed directly. A solution is to
replace the inner problem Z* (D) by an approximation ZN (D) obtained through N iterations of a
numerical optimization algorithm or its unrolled version. This reduces the problem to minimizing
GN(D) , F(ZN (D), D). The first question is how sub-optimal global solutions of GN are
2
Published as a conference paper at ICLR 2022
compared to the ones of G. Proposition 2.1 shows that the global minima of GN converge as fast as
the numerical approximation ZN in function value.
Proposition 2.1 Let D* = argmi□D∈c G(D) and DN = argmi□D∈c GN(D), where N is the
number ofunrolled iterations. We denote by K(D*) a Constant depending on D*, and by C(N) the
convergence speed of the algorithm, which approximates the inner problem solution. We have
GN (DN* ) - G(D*) ≤ K(D*)C(N) .
The proofs of all theoretical results are deferred to Appendix C. Proposition 2.1 implies that when
ZN is computed with FISTA (Beck & Teboulle, 2009), the function value for global minima of
GN converges with speed C(N) = N towards the value of the global minima of F. Therefore,
solving the inner problem approximately leads to suitable solutions for equation 2, given that the
optimization procedure is efficient enough to find a proper minimum of GN . As the computational
cost of zN increases with N, the choice of N results in a trade-off between the precision of the
solution and the computational efficiency, which is critical for processing large data sets.
Moreover, learning the dictionary and computing the sparse codes are two different tasks. The loss
GN takes into account the dictionary and the corresponding approximation ZN(D) to evaluate the
quality of the solution. However, the dictionary evaluation should reflect its ability to generate
the same signals as the ground truth data and not consider an approximate sparse code that can be
recomputed afterward. Therefore, we should distinguish the ability of the algorithm to recover a
good dictionary from its ability to learn the dictionary and the sparse codes at the same time. In
this work, we use the metric proposed in Moreau & Gramfort (2020) for convolutions to evaluate
the quality of the dictionary. We compare the atoms using their correlation and denote as C the cost
matrix whose entry i, j compare the atom i of the first dictionary and j of the second. We define a
sign and permutation invariant metric S (C) = maxσ∈sn n PNi ∣Cσ(i),i∣, where Sn is the group
of permutations of [1, n]. This metric corresponds to the best linear sum assignment on the cost
matrix C, and it can be computed with the Hungarian algorithm. Note that doing so has several
limitations and that evaluating the dictionary is still an open problem. Without loss of generality, let
T = 1 and thus z ∈ Rn in the rest of this section.
Gradient estimation in dictionary learning. Approximate dictionary learning is a non-convex
problem, meaning that good or poor local minima of GN may be reached depending on the initial-
ization, the optimization path, and the structure of the problem. Therefore, a gradient descent on GN
has no guarantee to find an adequate minimizer of G. While complete theoretical analysis of these
problems is arduous, we propose to study the correlation between the gradient obtained with GN
and the actual gradient of G, as a way to ensure that the optimization dynamics are similar. Once
z*(D) is known, Danskin (1967, Thm 1) states that g*(D) = VG(D) is equal to "F(z*(D), D),
where V2 indicates that the gradient is computed relatively to the second variable in F. Even though
the inner problem is non-smooth, this result holds as long as the solution z* (D) is unique. In the
following, we will assume that D>D is invertible on the support of z* (D), which implies the
uniqueness of z* (D). This occurs with probability one ifD is sampled from a continuous distribu-
tion (Tibshirani, 2013). AM and DDL differ in how they estimate the gradient of G. AM relies on
the analytical formula of g* and uses an approximation zN of z*, leading to the approximate gra-
dient gN1 (D) = V2F (zN (D), D). We evaluate how well gN1 approximates g* in Proposition 2.2.
Proposition 2.2 Let D ∈ Rm×n . Then, there exists a constant L1 > 0 such that for every number
of iterations N
gN1 - g* ≤ L1 kzN (D) - z*(D)k .
Proposition 2.2 shows that gN1 converges as fast as the iterates of ISTA converge. DDL computes
the gradient automatically through zN (D). As opposed to AM, this directly minimizes the loss
GN(D). Automatic differentiation yields a sub-gradient gN2 (D) such that
gN2 (D) ∈V2F(zN(D),D)+JN+∂1F(zN(D),D) ,	(4)
where JN : Rm×n → Rn is the weak Jacobian of zN (D) with respect to D and J+N denotes its
adjoint. The product between J+N and ∂1F (zN (D), D) is computed via automatic differentiation.
3
Published as a conference paper at ICLR 2022
Proposition 2.3 Let D ∈ Rm×n. Let S* be the support of z*(D), SN be the support of ZN and
2
SN = SN ∪ S*. Let f(z, D) = 2 ∣∣Dz 一 y∣b be the data-fitting term in F. Let R(J, S)=
J+(V2,ιf(z*, D) Θ 1e) + V2,f(z*, D) Θ 1s. Then there exists a constant L2 > 0 and a sub-
sequence of (F)ISTA iterates zφ(N) such that for all N ∈ N:
∃ gφ(N) ∈ V2f (Zφ(N), D) + J+(N)① lf(Zφ(N ),D) + λ∂k∙kι (Zφ(N))) s.t.：
∣∣gΦ(N)-g*∣∣ ≤ IlR(Jφ(n),sφ(n))|| i∣zφ(n)-z*ιι + ^22 i∣zφ(n)-z*ιι2 ∙
This sub-sequence zφ(N) corresponds to iterates on the support ofz*.
Proposition 2.3 shows that gN2 may converge faster than gN1 once the support is reached.
Ablin et al. (2020) and Tolooshams & Ba (2021) have studied the behavior of strongly convex
functions, as it is the case on the support, and found similar results. This allowed Tolooshams &
Ba (2021) to focus on support identification and show that automatic differentiation leads to a better
gradient estimation in dictionary learning on the support under minor assumptions.
However, we are also interested in characterizing the behavior outside of the support, where the
gradient estimation is difficult because of the sub-differential. In practice, automatic differenti-
ation uses the sign operator as a sub-gradient of ∣∣∙∣ι. The convergence behavior of gN is also
driven by R(JN, SN) and thus by the weak Jacobian computed via back-propagation. We first com-
pute a closed-form expression of the weak Jacobian of z* (D) and zN (D). We then show that
**
R(JN, SN) ≤ L ∣JN - J* ∣ and we analyze the convergence ofJN towards J*.
Study of the Jacobian. The computation of the Jacobian can be done by differentiating through
ISTA. In Theorem 2.4, we show that JN+1 depends on JN and the past iterate zN , and converges
towards a fixed point. This formula can be used to compute the Jacobian during the forward pass,
avoiding the computational cost of back-propagation and saving memory.
Theorem 2.4 At iteration N + 1 of ISTA, the weak Jacobian ofzN+1 relatively to Dl, where Dl is
the l-th row of D, is given by induction:
d (ZN +1)-	θ
∂Dl = 1lzN + 1|>0 θ
(⅞N) - L (Dlz> + (DJZN TlILn + d dd∂zN)))
"∂DNl) will be denoted by JN. It converges towards the weak Jacobian J* of z* relatively to Di,
whose values are
Jl S* = -(D>S* D:,S* ) 1(Dlz*> + (D>z* - yl)In)s* ,
on the support S* of z*, and 0 elsewhere. Moreover, R(J*, S*) = 0.
This result is similar to Bertrand et al. (2020) where the Jacobian ofz is computed over λ to perform
hyper-parameter optimization in Lasso-type models. Using R(J*, S*) = 0, we can write
∣∣∣R(JN, SeN)∣∣∣ ≤ ∣∣∣R(JN,SeN)-R(J*,S*)∣∣∣ ≤L∣JN-J*∣ ,	(5)
as ∣V12,1f(z*, D)∣2 = L. If the back-propagation were to output an accurate estimate JN of the
weak Jacobian J*, ∣∣R(JN, SfN)∣∣ would be 0, and the convergence rate of gN2 could be twice as
fast as the one of gN1 . To quantify this, we now analyze the convergence of JN towards J*. In
Proposition 2.5, we compute an upper bound of ∣JlN - Jl* ∣ with possible usage of truncated back-
propagation (Shaban et al., 2019). Truncated back-propagation of depth K corresponds to an initial
estimate of the Jacobian JN-K = 0 and iterating the induction in Theorem 2.4.
Proposition 2.5 Let N be the number of iterations and K be the back-propagation depth. We as-
sume that ∀n ≥ N - K, S * ⊂ Sn. Let EN = Sn \ S*, let L bethe largest eigenvalue of D>s* D：,s*,
and let μnlι be the smallest eigenvalue of D'>,snD：,sn-i. Let Bn = ∣PtEn - D>e D∙'>* Ps* ∣∣, Where
PS is the projection on Rs and Dt is the pseudo-inverse of D. We have
K	K-1 k
∣∣JN - 川∣ ≤ Y(I- μL-k) kJl*k+L ∣Dlk X Y(1-μL-i)(∣∣zN-k - z*∣∣+Bn-k kz；k)
k=1	k=0 i=1
4
Published as a conference paper at ICLR 2022
~0
IO0 ιo2 ιo4
Iterations N
一IEN7*n -IISN — s* Ilo
-20
-10
-0
IO0 IO2 IO4
Iterations N
Max BP depth
—full -200 — 50 —20
Iterations N Iterations N
Figure 1: Average convergence of JlN towards Jlj for two samples from the same data set, gener-
ated with a random Gaussian matrix. Jlj - JlN converges linearly on the support in both cases.
However, for sample 2, full back-propagation makes the convergence unstable, and truncated back-
propagation improves its behavior, as described in Proposition 2.5. The proportion of stable and
unstable samples in this particular example is displayed in Figure 2.
Proposition 2.5 reveals multiple stages in the Jacobian estimation. First, one can see that if all
iterates used for the back-propagation lie on the support Sj, the Jacobian estimate has a quasi-linear
convergence, as shown in the following corollary.
Corollary 2.6 Let μ > 0 be the smallest eigenvalue of D>s* D：,s*. Let K ≤ N be the back-
propagation depth and let Δn = F(ZN, D) 一 F(zj,D) + L ∣∣zn 一 z*k. Suppose that ∀n ∈
[N - K, N]; Sn ⊂ Sj. Then, we have
Il Jj - JNll ≤(1 一 L)K kJjk + K(1 一 L)KTkDlk 4∆N-K
Once the support is reached, ISTA also converges with the same linear rate (1 一 L). Thus the
gradient estimate gN2 converges almost twice as fast as gN1 in the best case - with optimal sub-
gradient -as O(K (1 ― μ )2K). This is similar to Ablin et al. (2020, Proposition.5) and Tolooshams
& Ba (2021). Second, Proposition 2.5 shows that lJlj 一 JlN l may increase when the support is not
well-estimated, leading to a deterioration of the gradient estimate. This is due to an accumulation
of errors materialized by the sum in the right-hand side of the inequality, as the term BN kzj k may
not vanish to 0 as long as SN 6⊂ Sj . Interestingly, once the support is reached at iteration S < N,
the errors converge linearly towards 0, and we recover the fast estimation of gj with g2 . Therefore,
Lasso-based DDL should either be used with a low number of steps or truncated back-propagation
to ensure stability. These results apply for all linear dictionaries, including convolutions.
Numerical illustrations. We now illustrate these theoretical re-
sults depending on the number N of unrolled iterations. The data
are generated from a random Gaussian dictionary D of size 30 × 50,
with Bernoulli-Gaussian sparse codes z (sparsity 0.3, σz2 = 1), and
Gaussian noise (σ?0% = 0.1) - more details in Appendix A.
Figure 1 confirms the linear convergence of JlN once the support
is reached. However, the convergence might be unstable when the
number of iteration grows, leading to exploding gradient, as illus-
trated in the second case. When this happens, using a small number
of iterations or truncated back-propagation becomes necessary to
prevent accumulating errors. It is also of interest to look at the pro-
portion of unstable Jacobians (see Figure 2). We recover behaviors
observed in the first and second case in Figure 1. 40% samples suf-
fer from numerical instabilities in this example. This has a negative
impact on the gradient estimation outside of the support.
Ooo
3 2 1
= ⅛l⅜ =
0 -
IO1 IO3
Iterations N
Figure 2: Average conver-
gence of JN towards Jj for
50 samples. In this example,
40% of the Jacobians are un-
stable (red curves).
We display the convergence behavior of the gradients estimated by
AM and by DDL with different back-propagation depths (20, 50, full) for simulated data and images
in Figure 3. We unroll FISTA instead of ISTA to make the convergence faster. We observed similar
5
Published as a conference paper at ICLR 2022
BP depth
Figure 3: Gradient convergence in angle for 1000 synthetic samples (left) and patches from a
noisy image (center). The image is normalized, decomposed into patches of dimension 10 × 10 and
with additive Gaussian noise (σ2 = 0.1). The dictionary for which the gradients are computed is
composed of 128 patches from the image. (right) Relative difference between angles from DDL and
AM. Convergence is faster with DDL in early iterations, and becomes unstable with too many steps.
behaviors for both algorithms in early iterations but using ISTA required too much memory to reach
full convergence. As we optimize using a line search algorithm, we are mainly interested in the abil-
ity of the estimate to provide an adequate descent direction. Therefore, we display the convergence
in angle defined as the cosine similarity hg, g*)= ,；(%?..The angle provides a good metric to
assert that the two gradients are correlated and thus will lead to similar optimization paths. We also
provide the convergence in norm in appendix. We compare gN1 and gN2 with the relative difference
of their angles with g*, defined as hgNIghgrhgN；g i. When its value is positive, DDL provides the
best descent direction. Generally, when the back-propagation goes too deep, the performance of gN2
decreases compared to gN1, and we observe large numerical instabilities. This behavior is coherent
with the Jacobian convergence patterns studied in Proposition 2.5. Once on the support, gN2 reaches
back the performance of gN1 as anticipated. In the case of a real image, unrolling beats AM by up
to 20% in terms of gradient direction estimation when the number of iterations does not exceed 50,
especially with small back-propagation depth. This highlights that the principal interest of unrolled
algorithms is to use them with a small number of layers 一 i.e., a small number of iterations.
3 Approximate dictionary learning in practice
This section introduces practical guidelines on Lasso-based approximate dictionary learning with
unit norm constraint, and we provide empirical justifications for its ability to recover the dictionary.
We also propose a strategy to scale DDL with a stochastic optimization method. We provide a full
description of all our experiments in Appendix A. We optimize with projected gradient descent com-
bined to a line search to compute high-quality steps sizes. The computations have been performed
on a GPU NVIDIA Tesla V100-DGXS 32GB using PyTorch (Paszke et al., 2019).1
Improvement of precision. As stated before, a low number of iterations allows for efficient and
stable computations, but this makes the sparse code less precise. One can learn the steps sizes of
(F)ISTA to speed up convergence and compensate for imprecise representations, as proposed by
Ablin et al. (2019) for LISTA. To avoid poor results due to large degrees of freedom in unsuper-
vised learning, we propose a method in two steps to refine the initialization of the dictionary before
relaxing the constraints on the steps sizes:
1.	We learn the dictionary with fixed steps sizes equal to + where L = ∣∣D∣∣2, given by convergence
conditions. Lipschitz constants or upper bounds are computed at each gradient step with norms,
or the FFT for convolutions, outside the scope of the network graph.
2.	Then, once convergence is reached, we jointly learn the step sizes and the dictionary. Both are
still updated using gradient descent with line search to ensure stable optimization.
1Code is available at https://github.com/bmalezieux/unrolled_dl.
6
Published as a conference paper at ICLR 2022
----AM ---------- DDL -------- DDL + steps
Gradient steps
JOqUInN
Figure 4:	(left) Number of gradient steps performed by the line search before convergence, (center)
distance to the optimal loss, and (right) distance to the optimal dictionary recovery score depending
on the number of unrolled iterations. The data are generated as in Figure 1. We display the mean and
the 10% and 90% quantiles over 50 random experiments. DDL needs less gradient steps to converge
in early iterations, and unrolling obtains high recovery scores with only a few dozens of iterations.
5 O
2 2
HNSd
Denoismg
---AM
DDL
---DDL steps
—1 DL-Oracle
Min. distribution
5 1
2 2
XNSd
-0.7
O
CDL minima

IO0 IO1 IO2	10	18	-2	0	2
Iterations N	SNR (dB)	Normalized distance
Figure 5:	We consider a normalized image degraded by Gaussian noise. (left) PSNR depending on
the number of unrolled iterations for σn2oise = 0.1, i.e. PSNR = 10 dB. DL-Oracle stands for full
AM dictionary learning (103 iterations of FISTA). There is no need to unroll too many iterations to
obtain satisfying results. (center) PSNR and average recovery score between dictionaries depending
on the SNR for 50 random initializations in CDL. (right) 10 loss landscapes in 1D for σn2oise = 0.1.
DDL is robust to random initialization when there is not too much noise.
The use of LISTA-like algorithms with no ground truth generally aims at improving the speed of
sparse coding when high precision is not required. When it is the case, the final sparse codes can be
computed separately with FISTA (Beck & Teboulle, 2009) or coordinate descent (Wu et al., 2008)
to improve the quality of the representation.
3.1	Optimization dynamics in approximate dictionary learning
In this part, we study empirical properties of approximate dictionary learning related to global opti-
mization dynamics to put our results on gradient estimation in a broader context.
Unrolling v. AM. In Figure 4, we show the number of gradient steps before reaching convergence,
the behavior of the loss FN, and the recovery score defined at the beginning of the section for syn-
thetic data generated by a Gaussian dictionary. As a reminder, S(C) = maxσ∈sn n PNi ∣Cσ(i),il
where C is the correlation matrix between the columns of the true dictionary and the estimate. The
number of iterations corresponds to N in the estimate zN(D). First, DDL leads to fewer gradient
steps than AM in the first iterations. This suggests that automatic differentiation better estimates
the directions of the gradients for small depths. However, computing the gradient requires back-
propagating through the algorithm, and DDL takes 1.5 times longer to perform one gradient step
than AM on average for the same number of iterations N. When looking at the loss and the recovery
score, we notice that the advantage of DDL for the minimization of FN is minor without learning
the steps sizes, but there is an increase of performance concerning the recovery score. DDL bet-
ter estimates the dictionary for small depths, inferior to 50. When unrolling more iterations, AM
performs as well as DDL on the approximate problem and is faster.
Approximate DL. Figure 4 shows that high-quality dictionaries are obtained before the conver-
gence of FN, either with AM or DDL. 40 iterations are sufficient to reach a reasonable solution
7
Published as a conference paper at ICLR 2022
concerning the recovery score, even though the loss is still very far from the optimum. This suggests
that computing optimal sparse codes at each gradient step is unnecessary to recover the dictionary.
Figure 5 illustrates that by showing the PSNR of a noisy image reconstruction depending on the
number of iterations, compared to full AM dictionary learning with 103 iterations. As for synthetic
data, optimal performance is reached very fast. In this particular case, the model converges after
80 seconds with approximate DL unrolled for 20 iterations of FISTA compared to 600 seconds in
the case of standard DL. Note that the speed rate highly depends on the value of λ. Higher values
of λ tend to make FISTA converge faster, and unrolling becomes unnecessary in this case. On the
contrary, unrolling is more efficient than AM for lower values of λ.
Loss landscape. The ability of gradient descent to find adequate local minima strongly depends on
the structure of the problem. To quantify this, we evaluate the variation of PSNR depending on the
Signal to Noise Ratio (SNR) (10 log 10 (。2/。2) where σb is the variance of the noise) for 50 random
initializations in the context of convolutional dictionary learning on a task of image denoising, with
20 unrolled iterations. Figure 5 shows that approximate CDL is robust to random initialization when
the level of noise is not too high. In this case, all local minima are similar in terms of reconstruction
quality. We provide a visualization of the loss landscape with the help of ideas presented in Li
et al. (2018). The algorithm computes a minimum, and we chose two properly rescaled vectors to
create a plan from this minimum. The 3D landscape is displayed on this plan in Figure B using the
Python library K3D-Jupyter2. We also compare in Figure 5 (right) the shapes of local minima in 1D
by computing the values of the loss along a line between two local minima. These visualizations
confirm that dictionary learning locally behaves like a convex function with similar local minima.
3.2	Stochastic DDL
In order to apply DDL in realistic settings, it
is tempting to adapt Stochastic Gradient Descent
(SGD), commonly used for neural networks. The
major advantage is that the sparse coding is not per-
formed on all data at each forward pass, leading to
significant time and memory savings. The issue is
that the choice of gradient steps is critical to the op-
timization process in dictionary learning, and SGD
methods based on simple heuristics like rate decay
are difficult to tune in this context. We propose to
leverage a new optimization scheme introduced in
Vaswani et al. (2019), which consists of performing
a stochastic line search. The algorithm computes a
good step size at each epoch, after which a heuristic
decreases the maximal step. Figure 6 displays the
recovery score function of the time for various mini-
Minibatch size
——100	——2000	——Full batch
Figure 6: Recovery score vs. time for 10
random Gaussian matrices and 105 samples.
Initialization with random dictionaries. In-
termediate batch sizes offer a good trade-off
between speed and memory usage.
batch sizes on a problem with 105 samples. The data were generated as in Figure 1 but with a
larger dictionary (50 × 100). The algorithm achieves good performance with small mini-batches and
thus limited memory usage. We also compare this method with Online dictionary learning (Mairal
et al., 2009) in Figure E. It shows that our method speeds up the dictionary recovery, especially
for lower values of λ. This strategy can be adapted very easily for convolutional models by taking
sub-windows of the full signal and performing a stochastic line search, as demonstrated in Section 4.
See Tolooshams et al. (2020) for another unrolled stochastic CDL algorithm applied to medical data.
4 Application to pattern learning in MEG signals
In magnetoencephalography (MEG), the measurements over the scalp consist of hundreds of simul-
taneous recordings, which provide information on the neural activity during a large period. Convo-
lutional dictionary learning makes it possible to learn cognitive patterns corresponding to physiolog-
ical activities (Dupre la Tour et al., 2018). As the electromagnetic waves propagate through the brain
at the speed of light, every sensor measures the same waveform simultaneously but not at the same
2Package available at https://github.com/K3D-tools/K3D-jupyter.
8
Published as a conference paper at ICLR 2022
Spatial pattern 2
Temporal pattern 2
Figure 7:	Stochastic Deep
CDL on 6 minutes of MEG
data (204 channels, sampling
rate of 150Hz). The algo-
rithm uses 40 atoms, 30 un-
rolled iterations and 100 iter-
ations with batch size 20. We
recover heartbeat (0), blink-
ing (1) artifacts, and an au-
ditory evoked response (2)
among others.
Minibatch	Time window	Steps learning	Corr. u	Corr. V	Mean corr.	Time
5	20^s	True	0.85 ± 0.02	0.84 ± 0.06	-0.845	110s
5	20 s	False	0.88 ± 0.02	0.78 ± 0.06	0.83	57 s
5	10 s	True	0.83 ± 0.01	0.82 ± 0.09	0.825	56 s
20	10 s	True	0.85 ± 0.01	0.75 ± 0.09	0.80	163 S
Table 1: Stochastic Deep CDL on MEG data (as in Figure 7). We compare u and v to 12 important
atoms output by alphacsc (correlation averaged on 5 runs), depending on several hyperparame-
ters, with 30 layers, 10 epochs and 10 iterations per epochs. λrescaled = 0.3λmax, λmax = DT y∞.
The best setups achieve 8θ% - 90% average correlation with alphacsc in around 100 sec. Com-
pared to around 1400 sec. Our method is also faster than convolutional K-SVD (Yellin et al., 2017).
intensity. The authors propose to rely on multivariate convolutional sparse coding (CSC) with rank-
1 constraint to leverage this physical property and learn prototypical patterns. In this case, space and
time patterns are disjoint in each atom: Dk = ukvkT where u gathers the spatial activations on each
channel and v corresponds to the temporal pattern. This leads to the model
1
min
zk∈RT,uk∈RS,vk∈Rt 2
n
E(Ukv>) * Zk - y
k=1
+λ	kzkk1
2 k=1
(6)
where n is the number of atoms, T is the total recording time, t is the kernel size, and S is the number
of sensors. We propose to learn u and v with Stochastic Deep CDL unrolled for a few iterations to
speed up the computations of the atoms. Figure 7 reproduces the multivariate CSC experiments of
alphacsc3 (Dupre la ToUr et al., 2018) on the dataset sample of MNE (Gramfort et al., 2013) - 6
minutes of recordings with 204 channels sampled at 150Hz with visual and audio stimuli.
The algorithm recovers the main waveforms and spatial patterns with approximate sparse codes and
without performing the sparse coding on the whole data set at each gradient iteration, which leads
to a significant gain of time. We are able to distinguish several meaningful patterns as heartbeat
and blinking artifacts or auditive evoked response. As this problem is unsupervised, it is difficult
to provide robust quantitative quality measurements. Therefore, we compare our patterns to 12
important patterns recovered by alpahcsc in terms of correlation in Table 1. Good setups achieve
between 80% and 90% average correlation ten times faster.
5 Conclusion
Dictionary learning is an efficient technique to learn patterns in a signal but is challenging to ap-
ply to large real-world problems. This work showed that approximate dictionary learning, which
consists in replacing the optimal solution of the Lasso with a time-efficient approximation, offers
a valuable trade-off between computational cost and quality of the solution compared to complete
Alternating Minimization. This method, combined with a well-suited stochastic gradient descent
algorithm, scales up to large data sets, as demonstrated on a MEG pattern learning problem. This
work provided a theoretical study of the asymptotic behavior of unrolling in approximate dictionary
learning. In particular, we showed that numerical instabilities make DDL usage inefficient when too
many iterations are unrolled. However, the super-efficiency of DDL in the first iterations remains
unexplained, and our first findings would benefit from theoretical support.
3Package and experiments available at https://alphacsc.github.io
9
Published as a conference paper at ICLR 2022
Ethics S tatement
The MEG data conform to ethic guidelines (no individual names, collected under individual’s con-
sent, . . . ).
Reproducibility Statement
Code is available at https://github.com/bmalezieux/unrolled_dl. We provide a
full description of all our experiments in Appendix A, and the proofs of our theoretical results in
Appendix C.
Acknowledgments
This work was supported by grants from Digiteo France.
References
Pierre Ablin, Thomas Moreau, Mathurin Massias, and Alexandre Gramfort. Learning step sizes
for unfolded sparse coding. In Advances in Neural Information Processing Systems, pp. 13100-
13110, 2019.
Pierre Ablin, Gabriel Peyre, and Thomas Moreau. SUPer-efficiency of automatic differentiation for
functions defined as a minimum. In Proceedings of the 37th International Conference on Machine
Learning, pp. 32-41, 2020.
Alekh Agarwal, Animashree Anandkumar, Prateek Jain, and Praneeth Netrapalli. Learning sparsely
used overcomplete dictionaries via alternating minimization. SIAM Journal on Optimization, 26
(4):2775-2799, 2016.
Michal Aharon, Michael Elad, and Alfred Bruckstein. K-svd: An algorithm for designing overcom-
plete dictionaries for sparse representation. IEEE Transactions on Signal Processing, 54:4311 -
4322, 2006.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM J. Imaging Sciences, 2:183-202, 2009.
Quentin Bertrand, Quentin Klopfenstein, Mathieu Blondel, Samuel Vaiter, Alexandre Gramfort, and
Joseph Salmon. Implicit differentiation of lasso-type models for hyperparameter optimization. In
International Conference on Machine Learning, pp. 810-821. PMLR, 2020.
Xiaohan Chen, Jialin Liu, Zhangyang Wang, and Wotao Yin. Theoretical linear convergence of
unfolded ista and its practical weights and thresholds. Advances in Neural Information Processing
Systems, 2018.
Scott R Cole and Bradley Voytek. Brain oscillations and the importance of waveform shape. Trends
in cognitive sciences, 21(2):137-149, 2017.
John M. Danskin. Theory of Max-Min and Its Application to Weapons Allocation Problems. Springer
Berlin Heidelberg, Berlin/Heidelberg, 1967.
Ingrid Daubechies, Michel Defrise, and Christine Mol. An iterative thresholding algorithm for linear
inverse problems with a sparsity constrains. Communications on Pure and Applied Mathematics,
57, 2004.
Charles-Alban Deledalle, Samuel Vaiter, Jalal Fadili, and Gabriel Peyre. Stein unbiased gradient
estimator of the risk (sugar) for multiple parameter selection. SIAM Journal on Imaging Sciences,
7(4):2448-2487, 2014.
Tom DUPre la Tour, Thomas Moreau, Mainak Jas, and Alexandre Gramfort. Multivariate convolu-
tional sparse coding for electromagnetic brain signals. Advances in Neural Information Process-
ing Systems, 31:3292-3302, 2018.
10
Published as a conference paper at ICLR 2022
Alexandre Gramfort, Martin Luessi, Eric Larson, Denis A Engemann, Daniel Strohmeier, Christian
Brodbeck, Roman Goj, Mainak Jas, Teon Brooks, Lauri Parkkonen, et al. Meg and eeg data
analysis with mne-python. Frontiers in neuroscience, 7:267, 2013.
Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. International con-
ference on machine learning, pp. 399-406, 201θ.
Remi Gribonval, RodolPhe Jenatton, and Francis Bach. Sparse and spurious: dictionary learning
with noise and outliers. IEEE Transactions on Information Theory, 61(11):6298-6319, 2015.
Roger Grosse, Rajat Raina, Helen Kwong, and Andrew Y. Ng. Shift-Invariant Sparse Coding for
Audio Classification. Cortex, 8:9, 2007.
Benjamin D Haeffele and Rene Vidal. Global optimality in tensor factorization, deep learning, and
beyond. arXiv preprint arXiv:1506.07540, 2015.
Bruno Lecouat, Jean Ponce, and Julien Mairal. A flexible framework for designing trainable priors
with adaptive smoothing and game encoding. In Advances in neural information processing
systems, 2020.
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss land-
scape of neural nets. In Advances in neural information processing systems, pp. 6389-6399,
2018.
Jialin Liu and Xiaohan Chen. Alista: Analytic weights are as good as learned weights in lista. In
International Conference on Learning Representations, 2019.
Julien Mairal, Francis Bach, J. Ponce, and Guillermo Sapiro. Online learning for matrix factorization
and sparse coding. Journal of Machine Learning Research, 11, 2009.
Thomas Moreau and Joan Bruna. Understanding neural sparse coding with matrix factorization. In
International Conference on Learning Representation, 2017.
Thomas Moreau and Alexandre Gramfort. Dicodile: Distributed convolutional dictionary learning.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.
Bruno A. Olshausen and David J Field. Sparse coding with an incomplete basis set: A strategy
employed by \protect{V1}. Vision Research, 37(23):3311-3325, 1997.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-
performance deep learning library. In Advances in neural information processing systems, pp.
8026-8037, 2019.
Meyer Scetbon, Michael Elad, and Peyman Milanfar. Deep k-svd denoising. IEEE Transactions on
Image Processing, 30:5944-5955, 2021.
Amirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots. Truncated back-propagation
for bilevel optimization. In International Conference on Artificial Intelligence and Statistics, pp.
1723-1732. PMLR, 2019.
Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere i: Overview and
the geometric picture. IEEE Transactions on Information Theory, 63(2):853-884, 2016.
Wen Tang, Emilie Chouzenoux, Jean-Christophe Pesquet, and Hamid Krim. Deep transform and
metric learning network: Wedding deep dictionary learning and neural networks. arXiv preprint
arXiv:2002.07898, 2020.
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society Series B, 58:267-288, 1996.
Ryan J. Tibshirani. The lasso problem and uniqueness. Electronic Journal of Statistics, 7(1):1456-
1490, 2013.
11
Published as a conference paper at ICLR 2022
Bahareh Tolooshams and Demba Ba. Pudle: Implicit acceleration of dictionary learning by back-
propagation. arXiv preprint, 2021.
Bahareh Tolooshams, Sourav Dey, and Demba Ba. Deep residual autoencoders for expectation
maximization-inspired dictionary learning. IEEE Transactions on Neural Networks and Learning
Systems, PP:1-15, 2020.
Sharan Vaswani, Aaron Mishkin, Issam Laradji, Mark Schmidt, Gauthier Gidel, and Simon Lacoste-
Julien. Painless stochastic gradient: Interpolation, line-search, and convergence rates. Advances
in neural information processing systems, 32:3732-3745, 2019.
Tong Tong Wu, Kenneth Lange, et al. Coordinate descent algorithms for lasso penalized regression.
Annals of Applied Statistics, 2(1):224-244, 2008.
Yande Xiang, Zhitao Lin, and Jianyi Meng. Automatic qrs complex detection using two-level con-
volutional neural network. Biomedical engineering online, 17(1):1-17, 2018.
Florence Yellin, Benjamin D Haeffele, and Rene Vidal. Blood cell detection and counting in holo-
graphic lens-free imaging by convolutional sparse dictionary learning and coding. In International
Symposium on Biomedical Imaging, pp. 650-653. IEEE, 2017.
John Zarka, Louis Thiry, Tomas Angles, and Stephane Mallat. Deep network classification by
scattering and homotopy dictionary learning. In International Conference on Learning Represen-
tations, 2019.
A	Full description of the experiments
This section provides complementary information on the experiments presented in the paper.
A.1 Convergence of the Jacobians - Figure 1 and Figure 2
We generate a normalized random Gaussian dictionary D of dimension 30 × 50, and sparse codes
z from a Bernoulli Gaussian distribution of sparsity 0.3 and σ2 = 1. The signal to process is y =
Dz + b where b is an additive Gaussian noise with σn2oise = 0.1. The Jacobians are computed for a
random perturbation D+bD ofD where bD is a Gaussian noise of scale 0.5σD2 . JlN corresponds to
the approximate Jacobian with N iterations of ISTA with λ = 0.1. Jl corresponds the true Jacobian
computed with sparse codes obtained after 104 iterations of ISTA with λ = 0.1.
In Figure 2, the norm JlN - Jll is computed for 50 samples.
A.2 Convergence of the gradient estimates - Figure 3
Synthetic data. We generate a normalized random Gaussian dictionary D of dimension 30 × 50,
and 1000 sparse codes z from a Bernoulli Gaussian distribution of sparsity 0.3 and σ2 = 1. The
signal to process is y = Dz + b where b is an additive Gaussian noise with σn2oise = 0.1. The
gradients are computed for a random perturbation D + bD of D where bD is a Gaussian noise of
scale 0.5σD2 .
Noisy image. A 128 × 128 black-and-white image is degraded by a Gaussian noise with σn2oise =
0.1 and normalized. We processed 1000 patches of dimension 10 × 10 from the image, and we
computed the gradients for a dictionary composed of 128 random patches.
gN corresponds to the gradient for N iterations of FISTA with λ = 0.1. gl corresponds to the true
gradient computed with a sparse code obtained after 104 iterations of FISTA.
A.3 Optimization dynamics on synthetic data - Figure 4
We generate a normalized random Gaussian dictionary D of dimension 30 × 50, and sparse codes
z from a Bernoulli Gaussian distribution of sparsity 0.3 and σ2 = 1. The signal to process is
y = Dz + b where b is an additive Gaussian noise with σn2oise = 0.1. The initial dictionary
12
Published as a conference paper at ICLR 2022
is taken as a random perturbation D + bD of D where bD is a Gaussian noise of scale 0.5σD2 .
N corresponds to the number of unrolled iterations of FISTA. F * is the value of the loss for 103
iterations minus 10-3. S * is the score obtained after 103 iterations plus 10-3. The optimization is
done with λ = 0.1. We compare the number of gradient steps (left), the loss values (center), and the
recovery scores (right) for 50 different dictionaries. DDL with steps sizes learning is evaluated on
100 iterations only due to memory and optimization time issues.
A.4 Optimization dynamics and loss landscapes on images - Figure 5
A 128 × 128 black-and-white image is degraded by a Gaussian noise and normalized.
Left. In this experiment, σn2oise = 0.1. We learn a dictionary composed of 128 atoms on 10 × 10
patches with FISTA and λ = 0.1 in all cases. The PSNR is obtained with sparse codes output by
the network. The results are compared to the truth with the Peak Signal to Noise Ratio. Dictionary
learning denoising with 1000 iterations of FISTA is taken as a baseline.
Center. We learn 50 dictionaries from 50 random initializations in convolutional dictionary learn-
ing with 50 kernels of size 8 × 8 with 20 unrolled iterations of FISTA and λ = 0.1. The PSNR is
obtained with sparse codes output by the network. We compare the average, minimal and maximal
PSNR, and recovery scores with all other dictionaries to study the robustness to random initialization
depending on the level of noise (SNR).
Right. In this experiment, σn2oise = 0.1. We learn 2 dictionaries from 2 random initializations in
convolutional dictionary learning with 50 kernels of size 8 × 8 with 20 unrolled iterations of FISTA
and λ = 0.1. We display the loss values on the line between these two dictionaries. The experiment
is repeated on 10 different random initializations.
A.5 Stochastic DDL on synthetic data - Figure 6
We generate a normalized random Gaussian dictionary D of dimension 50 × 100, and 105 sparse
codes z from a Bernoulli Gaussian distribution of sparsity 0.3 and σ2 = 1. The signal to process
is y = Dz + b where b is an additive Gaussian noise with σn2oise = 0.1. The initial dictionary is
taken as a random gaussian dictionary. We compare stochastic and full-batch line search projected
gradient descent with 30 unrolled iterations of FISTA and λ = 0.1, without steps sizes learning.
Stochastic DDL is run for 10 epochs with a maximum of 100 iterations for each epoch.
A.6 Pattern learning in MEG - Figure 7
Stochastic Deep CDL on 6 minutes of recordings of MEG data with 204 channels and a sampling
rate of 150Hz. We remove the powerline artifacts and high-pass filter the signal to remove the
drift which can impact the CSC technique. The signal is also resampled to 150 Hz to reduce the
computational burden. This preprocessing procedure is presented in alphacsc, and available in
the code in the supplementary materials. The algorithm learns 40 atoms of 1 second on mini batches
of 10 seconds, with 30 unrolled iterations of FISTA, λscaled = 0.3, and 10 epochs with 10 iterations
per epoch. The number of mini-batches per iteration is 20, with possible overlap.
B	Extra figures and experimental results
LISTA - Figure A. Illustration of LISTA for Dictionary Learning with initialization Z0 = 0 for
N = 3. WD = L(D)>, WD = (I - L(D)>D), where L = ∣∣Dk2. The result ZN(D) output
by the network is an approximation of the solution of the LASSO.
Loss landscape in 2D - Figure B. We provide a visualization of the loss landscape with the help of
ideas presented in Li et al. (2018). The algorithm computes a minimum, and we chose two properly
rescaled vectors to create a plan from this minimum. The 3D landscape is displayed on this plan in
the appendix using the Python library K3D-Jupyter. This visualization and the visualization in 1D
confirm that (approximate) dictionary learning locally behaves like a convex function with smooth
local minima.
13
Published as a conference paper at ICLR 2022
y I	I	~ I
RDJ	[wD]	[wD]
→FFHWO⅛⅜ΓWO■ Zn (D)
Figure A: LISTA
Figure B:	Loss landscape in approximate CDL
Gradient convergence in norm - Figure C. Gradient estimates convergence in norm for synthetic
data (left) and patches from a noisy image (right). The setup is similar to Figure 3. Both gradient
estimates converge smoothly in early iterations. When the back-propagation goes too deep, the
performance of gN2 decreases compared to gN1 , and we observe large numerical instabilities. This
behavior is coherent with the Jacobian convergence patterns studied in Proposition 2.5. Once on the
support, gN2 reaches back the performance of gN1 .
Figure C: Gradient estimates convergence in norm for synthetic data (left) and patches from a noisy
image (right). Both gradient estimates converge smoothly in early iterations, after what DDL gradi-
ent becomes unstable. The behavior returns to normal once the algorithm reaches the support.
Computation time to reach 0.95 recovery score - Figure D. The setup is similar to Figure 6. A
random Gaussian dictionary of size 50 × 100 generates the data from 105 sparse codes with sparsity
0.3. The approximate sparse coding is solved with λ = 0.1 and 30 unrolled iterations of FISTA.
The algorithm achieves good performances with small mini-batches and thus limited memory usage.
Stochastic DDL can process large amounts of data and recovers good quality dictionaries faster than
full batch DDL.
Sto DDL vs. Online DL - Figure E. We compare the time Online DL from spams4 (Mairal
et al., 2009) and Stochastic DDL need to reach a recovery score of 0.95 with a batch size of 2000.
Online DL is run with 10 threads. We repeat the experiment 10 times for different values of λ from
4package available at http://thoth.inrialpes.fr/people/mairal/spams/
14
Published as a conference paper at ICLR 2022
(S) ΦUIII
-l-ɔnoə 日 IN
Time to reach a score of 0.95
50
40
30
20
io
0
100	500	2000	10000 DDL Oracle DL
Minibatch size
Figure D:	Time to reach a recovery score of 0.95. Intermediate batch sizes offer a good trade-off
between speed and memory usage compared to full-batch DDL.
0.1 to 1.0. The setup is similar to Figure D, and we initialize both methods randomly. Stochastic
DDL is more efficient for smaller values of λ, due to the fact that sparse coding is slower in this
case. For higher values of λ, both methods are equivalent. Another advantage of Stochastic DDL
is its modularity. It works on various kinds of dictionary parameterization thanks to automatic
differentiation, as illustrated on 1-rank multivariate convolutional dictionary learning in Figure 7.
Time to reach a score of 0.95
2 1
O O
1 1
(S) əuie
0.2	0.4	0.6	0.8	1.0
Λ
Figure E:	Comparison between Online DL and Stochastic DDL. Stochastic DDL is more efficient
for smaller values of λ, due to the fact that sparse coding is slower in this case.
C Proofs of theoretical results
This section gives the proofs for the various theoretical results in the paper.
C.1 Proof of Proposition 2.1.
Proposition 2.1 Let D* = argmi□D∈c G(D) and DN = argmi□D∈c GN(D), where N is the
number ofunrolled iterations. We denote by K(D*) a constant depending on D*, and by C(N) the
convergence speed of the algorithm, which approximates the inner problem solution. We have
GN(DN) — G(D*) ≤ KP)C(N).
Let	G(D)	，	F(Z*(D),D)	and	GN(D)	，	F(ZN(D),D)	where	Z*(D)=
argminz∈Rn×τ F(Z, D) and ZN(D) = FISTA(D,N). Let D* = argmi□D∈c G(D) and
DN = argmi□D∈c GN(D). We have
GN(DN* ) -G(D*) =GN(DN* ) -GN(D*)+GN(D*) -G(D*)	(7)
=F(ZN(DN),DN)-F(ZN(D*),D*)	(8)
+F(ZN(D*),D*) - F (Z(D*), D*)	(9)
15
Published as a conference paper at ICLR 2022
By definition of DN
F(ZN(DN), DN) - F(ZN(D) D*) ≤ 0	(10)
The convergence rate of FISTA in function value for a fixed dictionary D is
F (Zn (D), D) — F (Zn (D), D) ≤ KD	(11)
N2
Therefore
F (Zn (D*), D*) — F (Z (D*), D*) ≤	(12)
Hence
Gn(DN) — G(D*) ≤ KNP	(13)
C.2 Proof of Proposition 2.2
Proposition 2.2 Let D ∈ Rm×n. Then, there exists a constant L1 > 0 such that for every number
of iterations N
IIgN — g*∣∣ ≤ Li kzN(D)-z*(D)k .
We have
F(z,D) = 2 kDz — yk2 + λ kzki	(14)
V2F (z, D) = (Dz — y)z>	(15)
zo(D) = 0 and the iterates (zN(D))N∈n converge towards z*(D). Hence, they are contained in
a closed ball around z*(D).As V2F(∙, D) is continuously differentiable, it is locally LiPschitz on
this closed ball, and there exists a constant L1(D) depending on D such that
IIgN — g*∣∣ = kV2F (zN (D), D) —V2F (z*(D),D)k	(16)
≤ Li(D) kzN(D) — z*(D)k	(17)
C.3 Proof of Proposition 2.3.
Proposition 2.3 Let D ∈ Rm×n. Let S* be the support of z*(D), SN be the Support of zN and
SN = SN ∪ S*. Let f (z, D) = 1 ∣∣Dz 一 y∣∣2 be the data-fitting term in F. Let R(J,S)=
j+(v2,ιf(z*, d) θ ie) + V22,if(z*, D)	1Se. Then there exists a constant L2 > 0 and a sub-
sequence of (F)ISTA iterates zφ(N) such that for all N ∈ N:
∃ gφ(N) ∈ V2f(zφ(N), D) + j+(N) (Vif(zφ(N),D) + λ∂k∙kι (zφ(N))) S.t.:
llgΦ(N) - g*II ≤ IR(JΦ(N), Sφ(N))11 l∣zφ(N) — z*∣∣ + 号 ∣lzΦ(N) — z*∣∣2 .
This sub-sequence zφ(N) corresponds to iterates on the support ofz*.
We have
gN(D) ∈ V2f(zN(D),D) + JN (Vif(zN(D),D) + λ∂k∙kι(zN))	(18)
We adaPt equation (6) in Ablin et al. (2020)
gN2 = g* + R(JN, SfN)(zN — z* ) + RND,z + J+N RzN,z	(19)
where
R(J, S = J+ "2,if (z*, D) θ 1s) + V2,if (z*, D) θ 1e	(20)
RND,z =V2f(zN,D)—V2f(z*,D) —V22,if(z*,D)(zN—z*)	(21)
RNz ∈ VIf(ZN, D) + λdk∙kι (ZN) — V2,if (z*, D)(ZN — z*)	(22)
16
Published as a conference paper at ICLR 2022
A	I *	k
As ZN and z* are on SN
V2,J(Z*,D)(ZN - Z*)= (v2jf(z*,D) Θ ISN)(zn - z*)	(23)
J+(Vljf(z*,D)(zn - z*)) = J+ (v2jf(z*, D) Θ ISN(ZN - z*))	(24)
As stated in Proposition 2.2, V2f(∙, D) is locally Lipschitz, and RDDZ is the Taylor rest of
V2f (∙, D). Therefore, there exists a constant Ld,z such that
VN ∈ N,∣∣RD,z∣∣ ≤ 弩 kZN(D) - z*(D)k2	(25)
We know that 0 ∈ Vif (z*, D) + 19卜卜(z*). In other words, ∃u* ∈ X9.卜(z*) s.t. Vif (z*, D) +
u* = 0. Therefore we have:
RzN,z ∈ Vif(zN,D)-Vif(z*,D)-V2i,if(z*,x)(zN-z*)+λ∂kzNki -u*	(26)
Let Lz,z be the Lipschitz constant of Vif (∙,D). (F)ISTA outputs a sequence such that there
exists a sub-sequence (zφ(N) )N∈N which has the same support as z*. For this sub-sequence,
u* ∈ λ∂k∙kι (zφ(N)). Therefore, there exists Rφ(N)such that
1.	RΦ(N) ∈ VIf(Zφ(N), D) + λdk∙kι (Zφ(N)) - V1,1f (Z*, x)(zφ(N) - z )
2.	∣∣RΦ(N)∣∣ ≤ ⅛z i∣zφ(n)-z*i∣2
For this sub-sequence, we can adapt Proposition 2 from Ablin et al. (2020). Let L2 = LD,z + Lz,z,
we have
∃ gφ(N) ∈ P2f(zφ(N), D) + jφ(N )31f(zφ(N), D) + λd ∣∣zφ(N) ∣∣ι ), s.t. :	(27)
∣∣gφ(N)- g*∣∣ ≤ ∣∣R(Jφ(n ),^N ))∣∣ i∣zφ(n ) - z*ii + ^22 i∣zφ(n )- z*『	(28)
C.4 Proof of Theorem 2.4.
Theorem 2.4 At iteration N + 1 of ISTA, the weak Jacobian of zN+i relatively to Dl, where Dl is
the l-th row of D, is given by induction:
d (ZN +1)— 1	θ
∂Dl	= 1|zN+ι|>0 θ
(⅛) - T (DIzN + (D>ZN - yι)In + DτDd(ZN)))
∂Dl L	∂Dl
d∂zN) will be denoted by JN. It converges towards the weak Jacobian J* of z* relatively to Dl,
whose values are
Jl S* = -(DTS* D:,s* )-1(Dlz*τ + (DTz* - Iyl)In)S* ,
on the support S* of z*, and 0 elsewhere. Moreover, R(J*, S*) = 0.
We start by recalling a Lemma from Deledalle et al. (2014).
Lemma C.1 The soft-thresholding STμ defined by STμ(z) = Sgn(Z) Θ (|z| - μ)+ is weakly dif-
ferentiable with weak derivative dSTZ(Z) = 1∣z∣>μ.
Coordinate-wise, ISTA corresponds to the following equality:
ZN+ι = STμ((I - 1 D>D)zn + 1 Dτy)	(29)
LL
1 m n	1 n
(ZN +1)i = STμ((ZN )i - L ^X(^X DjiDjp)(ZN )p + L ^X Djiyj )	(30)
p=i j=i	j=i
17
Published as a conference paper at ICLR 2022
The Jacobian is computed coordinate wise with the chain rule:
d (ZN +1 )i	f∂ (ZN 、	I	- I		1 a ∕v-∕v	1 Jd	n —n.n	
∂Dik — l(zN+1)il>0 Last term:		I ∂D,	ik	L∂Dik'W ji jp' N'p、L∂Dik 乙二 ji为〃 p=1 j=1	j = 1 (31) Vi		
			诉 E Djiyj =觎yi j=1		(32)
Second term:					
^	m n 忌旺 DjiDjP(ZN)p ,			m n m n : n n c/ (ZN )p	∂DrDjP =A ZDjiDjP	+ Z 工 -∂DΓ P=1 j=1	P=1 j=1	(zN )p	(33)
	∂DjiDjP _ ∂Dik		(2Dik	if j	=	l	and i =	P	= k I	DiP	if j	=	l	and i =	k	and p	=	k I	Dii	if j	=	l	and i =	k	and P	=	k [0	else		(34)
Therefore:					
XX XX dDjiDjp P= j=	dDik		(ZN)p :	m =)：(2DikδiPδik + DiiδPk 1i=k + DIPδikIk=P)(ZN)P P=1		(35)
			m =2Dik (ZN ) k δik + Dii(ZN ) k 1i=k +)： DiP(ZN )Pδik P=1 p=k		(36)
			m =Dii(ZN)k + δik ɪ2 Dip (ZN)P P=1		(37)
Hence:					
∂(ZN +1)i _ ∂Dik	,1(zN+1	)i∣>0 ∙	(IZN V - I(Dii(ZN )k + ∂Dlk	L m	m n δik(EDip(zn)p)+ EE 率DjiDjP- ∂Dlk P=1	P=1 j=1	-δikyi))	(38)
This leads to the following vector formulation:
d(zN+1)	d(zN) 1 D › D>~	IT I DT 力 d(zN)
~D)dΓ = 1zN+ι>0 θ (~∂DΓ — L(DZN + (D ZN 一如Tm + d d-∂dΓ
(39)
On the support of z*, denoted by S*, this quantity converges towards the fixed point:
Ji = -(DTS*DF )-1(D芦*T + (D>z* - y)Tm)s*	(40)
Elsewhere, J* is equal to 0. To prove that R(J*,S*) = 0, We use the expression given by equa-
tion 39
J*
J* - 1s* Θ J*
1s* Θ
(j* - L (V1,1∕(z*, D)T + V1,f(z*,D)TJ*))
1 1s* θ V2 if(z*,Dl)τ + 1s* Θ V2 if(z*,D)tJ*
L	，	，
0 = J*+(V2,if(z*,D) Θ 1s*) + V2,ιf(z*,D) Θ 1s;
0 = R(J*,S*)
(41)
(42)
(43)
(44)
18
Published as a conference paper at ICLR 2022
C.5 Proof OF Proposition 2.5 and Corollary 2.6
Proposition 2.5 Let N be the number of iterations and K be the back-propagation depth. We as-
sume that ∀n ≥ N — K, S * U Sn. Let EN = Sn \ S*, let L bethe largest eigenvalue of Dτs^ D：,s*,
and let μn be the SmalleSt eigenvalue of D>sηD：,sn-ɪ. Let Bn = IPEiT 一 DTE D'>*Ps* ∣∣, where
PS is the projection on RS and Dt is the pseudo-inverse of D. We have
K
∣∣Jn -川 ∣≤ Y(I -
K-1 k
*) kJ*k + L kD，k X ∏(1-T)(∣∣zN-k - z*∣∣+Bn-k 悯*11).
k=0 i=1
We denote by G the matrix (I — LDtD). For ZN with support SN and n* with support S*, We
have with the induction in Theorem 2.4
尤SN = (G JN-1 + uN T)SN	(45)
Jι*s* = (G J*+u*)S*	(46)
where UN = -L (D(ZT + (DTZN — yι)l) and the other terms on SN and S* are 0.
We can thus decompose their difference as the sum of two terms, one on the support S* and one on
this complement EN = SN \ S*
ji - JN = ( ji - JN)s* + ( ji - JN)En .
Recall that we assume S * U SN. Let,s study the terms separately on S * and EN = SN \ S *. These
two terms can be decompose again to constitute a double recursion system,
(JN - J* )s* = GS* (JN-1 - jl*) + (UNT- u* )s*	(47)
=gS*,S* (JF-1 - J；)s* + gs*,En-i (JF-1 - J*)En-i + (UNT- u*)s* ,
(48)
(JιN - J)EN = (JN)En = Gen(JιN-1 - J) + Gen,s* J* + (UNT)EN	(49)
=GEn,S*(JF-1 - Jl )s* + GEn,En-i (JFT- JIL-i	(50)
+ (UNT- UDEN + ((Ui)EN - D:END:,s*(D>S*D ：,S*)T(Ui)S*).
We define as PSN EN the operator which projects a vector from EN on (SN, En) with zeros on
SN. As S * U E N = SN, we get by combining these two expressions,
(jN - Jl	)sn	=GSN,Sn-1 (JN	1 -	Jl	)sn-i	+	(UN	1 - u* )sn	(51)
+ PSn,En ((UoEN- D:END：，S*(D>S*D ：,S* )-1(u*)s*)
Taking the norm yields to the following inequality,
∣∣JιN - Jι*∣∣ ≤ ∣∣Gsn,sn-i∣∣∣∣JiN-1 - Ji*∣∣ +1∣UNT- U∣∣	(52)
+ I(Ui)EN - D:END： ,s* ID>s*D： ,s* )-1 (Ui)S* ∣∣ .
Denoting by μN the smallest eigenvalue of D :SND： ,Sn-i, then ∣ ∣ GSN,Sn-i ∣ ∣ = (1 - μN) and we
get that
K
∣∣JN - J*∣∣ ≤ ∏(1 - *) ∣∣JN-k - Jι*∣∣	(53)
k = 1
+X ∏(1 - μN-)( ∣∣UN-k - u*∣∣+Ii (u* 人-%- DlEN-k DIT*(Ui)S* ∣∣).
k=0 i=1
19
Published as a conference paper at ICLR 2022
The back-propagation is initialized as JN—K = 0. Therefore ∣∣ JN—K - JjIl = IIJjl∣. More-
OVerIluNi-uj∣∣ ≤ LιιDιIi IIzN-k-zj∣∣.Finally, ||(u；)ENi-DlEN-kDt,>*(uj)s*∣∣Can
be rewritten with projection matrices PeN_k and P§* to obtain
Mj)ENi- DlENiDt,>* (uj)s* ∣∣ ≤ ∣∣PENi遍-DlENiDt,>*pS*uj∣∣	(54)
≤∣∣pENi-D⅛Ni Dt,>* ps*∣∣kuj∣	(55)
≤∣∣pEN-k - D-,EN-k Dt,>* pS*∣∣2 IDl Ikzjk .	(56)
Let Bn-k = ∣∣pEn-M-DlENiDt,>*pS* ∣∣. We have
K	K-1 k
∣∣Jln - Jij∣∣≤ ∏(1-T)kJjk+2IDlk XY(I-T)(∣∣zN-k -zj∣∣+BN- kzjk).
k=1	k=0 i=1
(57)
We now suppose that the support is reached at iteration N - s, with s ≥ K. Therefore, ∀n ∈
[N - s, N] Sn = Sj. Let An = F(zn, D) - F(zj, D) + L ∣∣zn - z*∣. On the support, F is a
μ-strongly convex function and the convergence rate of (ZN) is
kz*-ZNk ≤ (1- μ)s竺N二	(58)
LJ	LJ
Thus, we obtain
∣∣JN - Jj∣∣ ≤ Y(1 - μN-) kJjk	(59)
k=1
K-1 k
+ J kDk X ∏(1 - μN-i)(IIzN-k - zi∣∣+ bn-k kujk)
k=0 i=1
≤ Y(1 - *) kJjk	(60)
k=1
2	s-1
+ J kDk £(1 -
k=0
μ k( ( ∣ ∣ ^N-k
L) ( ∣ ∣ zl
K-1	k
+JkDik(I- L)s XY(1
k=s-1i=s-1
≤Y (1 - μN-) k Jj k
k=1
zj∣∣)
-T)(∣∣zN-k - zj∣∣+ Bn-k k(uj)k)
(61)
+ ∣ kDik χ(1 - μ )k(1
L	L
k=0
μ ʌ s-1-k 2AN-s
L1	-L~
K-1 k
+ l kDik (1 - L)s X Y (1 -
k=s-1 i=s-1
T )(∣∣zN-k-zj∣∣+ Bn-k k(uj)k)
≤Y (1 - μN-) k Jj k
k=1
+kDi k (1 - μ )s-1s 4An二
LL
K-1 k
+ ykDik(I- μ)s X Y(1 -
LL
k=s-1 i=s-1
(62)
T )(∣∣zN-k-zj∣∣+ Bn-k k(uj)k)
(63)
20
Published as a conference paper at ICLR 2022
Corollary 2.6 Let μ > 0 be the smallest eigenvalue of D>s* D：,s*. Let K ≤ N be the back-
propagation depth and let Δn = F(ZN,D) — F(z*,D) + L ∣∣zn 一 z*∣∣. Suppose that ∀n ∈
[N — K, N]; Sn ⊂ S*. Then, we have
J*- J/I ≤ (1 - L )KkJTk + K(1 - L )K TkDlk 4δN-k
Theterm L ∣Dlk (1 - L )s PK=S-IQk=s-ι(1 - μN-i )(∣IzN- -z*∣∣ + BN-k k (u*) k ) vanishes
when the algorithm is initialized on the support. Otherwise, it goes to 0 as s, K → N and N → ∞
because ∀n > N - s, μn = μ < 1.
D Iterative algorithms for sparse coding resolution.
ISTA. Algorithm to solve minz 2 ∣∣y - Dz∣∣2 + λ ∣z∣∣ι
Algorithm 1 ISTA
y, D, λ, N
z0 = 0, n = 0
Compute the Lipschitz constant L of D>D
while n < N do
un+1 - zN - LD>(Dzn - y)
zn+1 - STλ (Un+1)
n - n + 1
end while
FISTA. Algorithm to solve minz 2 ky - Dzk2 + λ ∣∣z∣∣ι
Algorithm 2 FISTA
y, D, λ, N
z0 = x0 = 0, n = 0, t0 = 1
Compute the Lipschitz constant L of D>D
while n < N do
un+1 - Zn - LD> (Dzn - y)
xn+1 V- STλ (un+1)
+ ι ι+√ι+4tn
tn+1 v-----2----
zn+1 V xn+1 + t⅛(Xn+1 - Xn)
nV n+1
end while
21