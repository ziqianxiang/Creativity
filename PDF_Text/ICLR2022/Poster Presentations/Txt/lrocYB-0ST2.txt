Published as a conference paper at ICLR 2022
Approximation and Learning with Deep
Convolutional Models: a Kernel Perspective
Alberto Bietti
Center for Data Science, New York University
alberto.bietti@nyu.edu
Ab stract
The empirical success of deep convolutional networks on tasks involving high-
dimensional data such as images or audio suggests that they can efficiently ap-
proximate certain functions that are well-suited for such tasks. In this paper, we
study this through the lens of kernel methods, by considering simple hierarchical
kernels with two or three convolution and pooling layers, inspired by convolutional
kernel networks. These achieve good empirical performance on standard vision
datasets, while providing a precise description of their functional space that yields
new insights on their inductive bias. We show that the RKHS consists of additive
models of interaction terms between patches, and that its norm encourages spatial
similarities between these terms through pooling layers. We then provide general-
ization bounds which illustrate how pooling and patches yield improved sample
complexity guarantees when the target function presents such regularities.
1	Introduction
Deep convolutional models have been at the heart of the recent successes of deep learning in problems
where the data consists of high-dimensional signals, such as image classification or speech recognition.
Convolution and pooling operations have notably contributed to the practical success of these models,
yet our theoretical understanding of how they enable efficient learning is still limited.
One key difficulty for understanding such models is the curse of dimensionality: due to the high-
dimensionality of the input data, it is hopeless to learn arbitrary functions from samples. For instance,
classical non-parametric regression techniques for learning generic target functions typically require
either low dimension or very high degrees of smoothness in order to obtain good generalization (e.g.,
Wainwright, 2019), which makes them impractical for dealing with high-dimensional signals. Thus,
further assumptions on the target function are needed to make the problem tractable, and we seek
assumptions that make convolutions a useful modeling tool. Various works have studied approxima-
tion benefits of depth with models that resemble deep convolutional architectures (Cohen & Shashua,
2017; Mhaskar & Poggio, 2016; Schmidt-Hieber et al., 2020). Nevertheless, while such function
classes may provide improved statistical efficiency in theory, it is unclear if there exist efficient
algorithms to learn such models, and hence, whether they might correspond to what convolutional
networks learn in practice. To overcome this issue, we consider instead function classes based on
kernel methods (ScholkoPf & Smola, 2001; Wahba, 1990), which are known to be learnable with
efficient (polynomial-time) algorithms, such as kernel ridge regression or gradient descent.
We consider “deeP” structured kernels known as convolutional kernels, which yield good emPirical
Performance on standard comPuter vision benchmarks (Li et al., 2019; Mairal, 2016; Mairal et al.,
2014; Shankar et al., 2020), and are related to over-Parameterized convolutional networks (CNNs) in
so-called “kernel regimes” (Arora et al., 2019; Bietti & Mairal, 2019b; Daniely et al., 2016; Garriga-
Alonso et al., 2019; Jacot et al., 2018; Novak et al., 2019; Yang, 2019). Such regimes may be seen as
Providing a first-order descriPtion of what common deeP models trained with gradient methods may
learn. Studying the corresPonding function sPaces (reProducing kernel Hilbert sPaces, or RKHS)
may then Provide insight into the benefits of various architectural choices. For fully-connected
architectures, such kernels are rotation-invariant, and the corresPonding RKHSs are well understood
in terms of regularity ProPerties on the sPhere (Bach, 2017a; Smola et al., 2001), but do not show
any major differences between deeP and shallow kernels (Bietti & Bach, 2021; Chen & Xu, 2021;
1
Published as a conference paper at ICLR 2022
Geifman et al., 2020). In contrast, in this work we show that even in the kernel setting, multiple layers
of convolution and pooling operations can be crucial for efficient learning of functions with specific
structures that are well-suited for natural signals. Our work paves the way for further studies of the
inductive bias of optimization algorithms on deep convolutional networks beyond kernel regimes,
for instance by incorporating adaptivity to low-dimensional structure (Bach, 2017a; Chizat & Bach,
2020; Wei et al., 2019) or hierarchical learning (Allen-Zhu & Li, 2020).
We make the following contributions:
•	We revisit convolutional kernel networks (Mairal, 2016), finding that simple two or three layers
models with Gaussian pooling and polynomial kernels of degree 2-4 at higher layers provide
competitive performance with state-of-the-art convolutional kernels such as Myrtle kernels (Shankar
et al., 2020) on Cifar10.
•	For such kernels, we provide an exact description of the RKHS functions and their norm, illustrating
representation benefits of multiple convolutional and pooling layers for capturing additive and
interaction models on patches with certain spatial regularities among interaction terms.
•	We provide generalization bounds that illustrate the benefits of architectural choices such as pooling
and patches for learning additive interaction models with spatial invariance in the interaction terms,
namely, improvements in sample complexity by polynomial factors in the size of the input signal.
Related work. Convolutional kernel networks were introduced by Mairal et al. (2014); Mairal
(2016). Empirically, they used kernel approximations to improve computational efficiency, while we
evaluate the exact kernels in order to assess their best performance, as in (Arora et al., 2019; Li et al.,
2019; Shankar et al., 2020). Bietti & Mairal (2019a;b) show invariance and stability properties of
its RKHS functions, and provide upper bounds on the RKHS norm for some specific functions (see
also Zhang et al., 2017); in contrast, we provide exact characterizations of the RKHS norm, and
study generalization benefits of certain architectures. Scetbon & Harchaoui (2020) study statistical
properties of simple convolutional kernels without pooling, while we focus on the role of architecture
choices with an emphasis on pooling. Cohen & Shashua (2016; 2017); Mhaskar & Poggio (2016);
Poggio et al. (2017) study expressivity and approximation with models that resemble CNNs, showing
benefits thanks to hierarchy or local interactions, but such models are not known to be learnable with
tractable algorithms, while we focus on (tractable) kernels. Regularization properties of convolutional
models were also considered in (Gunasekar et al., 2018; Heckel & Soltanolkotabi, 2020), but in
different regimes or architectures than ours. Li et al. (2021); Malach & Shalev-Shwartz (2021) study
benefits of convolutional networks with efficient algorithms, but do not study the gains of pooling.
Du et al. (2018) study sample complexity of learning CNNs, focusing on parametric rather than non-
parametric models. Mei et al. (2021) study statistical benefits of global pooling for learning invariant
functions, but only consider one layer with full-size patches. Concurrently to our work, Favero et al.
(2021); Misiakiewicz & Mei (2021) study benefits of local patches, but focus on one-layer models.
2	Deep Convolutional Kernels
In this section, we recall the construction of multi-layer convolutional kernels on discrete signals,
following most closely the convolutional kernel network (CKN) architectures studied by Mairal
(2016); Bietti & Mairal (2019a). These architectures rely crucially on pooling layers, typically with
Gaussian filters, which make them empirically effective even with just two convolutional layers. These
kernels define function spaces that will be the main focus of our theoretical study of approximation
and generalization in the next sections. In particular, when learning a target function of the form
f * (x) = Pi f (x), We will show that they are able to efficiently exploit two useful properties of f *:
(locality) each fi may depend on only one or a few small localized patches of the signal; (invariance)
many different terms fi may involve the same function applied to different input patches. We provide
further background and motivation in Appendix A.
For simplicity, we will focus on discrete 1D input signals, though one may easily extend our results
to 2D or higher-dimensional signals. We will assume periodic signals in order to avoid difficulties
with border effects, or alternatively, a cyclic domain Ω = Z∕∣Ω∣Z. A convolutional kernel of depth L
may then be defined for input signals χ,χ0 ∈ L2(Ω, Rp) by Kl(x,x0) = hΨ(χ), Ψ(χ0)i, through the
explicit feature map
Ψ(x) = ALMLPL …AιMιPιx.	(1)
2
Published as a conference paper at ICLR 2022
Here, P', m` and a` are linear or non-linear operators corresponding to patch extraction, kernel
mapping and pooling, respectively, and are described below. They operate on feature maps in L2(Ω')
(with Ωo = Ω) with values in different Hilbert spaces, starting from Ho := Rp, and are defined below.
An illustration of this construction is given in Figure 1.
Patch extraction. Given a patch shape s` ⊂ Ω'-ι, such
as s` = [-1,0,1] for one-dimensional patches of size 3,
the operator P' is defined for X ∈ L2(Ω'-ι, H'-ι) by1
P'x[u] = (x[u + v])v∈S' ∈ H^1.
Kernel mapping. The operators m` perform a non-
linear embedding of patches into a new Hilbert space
using dot-product kernels. We consider homogeneous
|S |
dot-product kernels given for z, z0 ∈ H'-1 by
k'(z,z0) = kzkkz0kκ' (k⅛i⅛)
Figure 1: Convolutional kernel.
hφe(z),φe(z'0))H',
(2)
is`i
where 夕' :H'_1 → H' is a feature map for the kernel. The kernel functions take the form k`(u)=
Pj≥0 bjuj with bj ≥ 0. This includes the exponential kernel κ(u) = eα(u-1) (i.e., Gaussian kernel
on the sphere) and the arc-cosine kernel arising from random ReLU features (Cho & Saul, 2009), for
which our construction is equivalent to that of the conjugate or NNGP kernel for an infinite-width
random ReLU network with the same architecture. The operator M' is then defined pointwise by
M'x[u]=夕'(x[u]).
(3)
At the first layer on image patches, these kernels lead to functional spaces consisting of homogeneous
functions with varying degrees of smoothness on the sphere, depending on the properties of the
kernel (Bach, 2017a; Smola et al., 2001). At higher layers, our theoretical analysis will also consider
simple polynomial kernels such as k'(z, z0) = (hz, z0i)r, in which case the feature map may be
explicitly written in terms of tensor products. For instance, r = 2 gives 夕'(z) = Z 0 Z and H' =
(H's'1 )02 = (H'-ι 0 H'-ι)lS'l×lS'1. See Appendix A.2 for more background on dot-product
kernels, their tensor products, and their regularization properties.
Pooling. Finally, the pooling operators A' perform local averaging through convolution with a
filter h` [u], which we may consider to be symmetric (h` [-u] =: h` [u] = h` [u]). In practice, the pooling
operation is often followed by downsampling by a factor s' , in which case the new signal A'x is
defined on a new domain Ω' with ∣Ω'∣ = ∣Ω'-ι∣∕s', and we may write for X ∈ L2(Ω'-ι) and U ∈ Ω',
A'x[u] =	h'[s'u - v]x[v].	(4)
v∈Ω'-1
Our experiments consider Gaussian pooling filters with a size and bandwidth proportional to the
downsampling factor s`, following Mairal (2016), namely, size 2s' + 1 and bandwidth √2s'. In
Section 3, we will often assume no downsampling for simplicity, in which case we may see the filter
bandwidth as increasing with the layers.
Links with other convolutional kernels. We note that our construction closely resembles kernels
derived from infinitely wide convolutional networks, known as conjugate or NNGP kernels (Garriga-
Alonso et al., 2019; Novak et al., 2019), and is also related to convolutional neural tangent ker-
nels (Arora et al., 2019; Bietti & Mairal, 2019b; Yang, 2019). The Myrtle family of kernels (Shankar
et al., 2020) also resembles our models, but they use small average pooling filters instead of Gaussian
filters, which leads to deeper architectures due to smaller receptive fields.
1L2(Ω, H) denotes the space of H-valued signals X such that IlxkL29 H) ：= Pu∈ω l∣x[u]kH < ∞.
3
Published as a conference paper at ICLR 2022
3	Approximation with (Deep) Convolutional Kernels
In this section, we present our main results on the approximation properties of convolutional kernels,
by characterizing functions in the RKHS as well as their norms. We begin with the one-layer case,
which does not capture interactions between patches but highlights the role of pooling, before moving
multiple layers, where interaction terms play an important role. Proofs are given in Appendix E.
3.1	The One-Layer Case
We begin by considering the case of a single convolutional layer, which can already help us illustrate
the role of patches and pooling. Here, the kernel is given by
K1(x,x0) = hAφ(x), Aφ(x0)iL2(Ω,H),
with Φ(χ)[u]=夕(xu), where We use the shorthand Xu = Pχ[u] for the patch at position u. We now
characterize the RKHS of K1 , showing that it consists of additive models of functions in H defined
on patches, with spatial regularities among the terms, induced by the pooling operator A. (Notation:
A* and At denote the adjoint and pseudo-inverse of an operator A, respectively.)
Proposition 1 (RKHS for 1-layer CKN.). The RKHS of K1 consists of functions f(x) =
hG, φ(X)iL2(Ω,H) =Eu∈ω G[u](xu), with G ∈ Range(A*), and with RKHS norm
kfkHκ1 =GrinΩ H kAt*GkL2(Ω,H) S 工	f (X) = X G[u](Xu)	⑸
∈	( , )	u∈Ω
Note that if A* is not invertible (for instance in the presence of downsampling), the constraint G ∈
Range(A*) is active and At* is its pseudo-inverse. In the extreme case of global average pooling, we
have A = (l,..., 1) 0 Id : L2(Ω, H) → H, so that G ∈ Range(A*) is equivalent to G[u] = g for
all u, for some fixed g ∈ H. In this case, the penalty in (5) is simply the squared RKHS norm kgk2H .
In order to understand the norm (5) for general pooling, recall that A is a convolution operator with
filter h1, hence its inverse (which we now assume exists for simplicity) may be easily computed in
the Fourier basis. In particular, for a patch z ∈ Rp|S1|, defining the scalar signal gz[u] = G[u](z), we
may write the following using the reproducing property and linearity:
At*G[u](z) = (AT)>gz[u] = FT diag(F h ι)-1 FgzH
where hi [u] := hi [-u] arises from transposition, F is the discrete Fourier transform, and both F
and A are viewed here as ∣Ω∣×∣Ω∣ matrices. From this expression, we see that by penalizing the
RKHS norm of f, we are implicitly penalizing the high frequencies of the signals gz [u] for any z,
and this regularization is stronger when the pooling filter hi has a fast spectral decay. For instance,
as the spatial bandwidth of hi increases (approaching a global pooling operation), Fhi decreases
more rapidly, which encourages gz [u] to be more smooth as a function of u, and thus prevents f
from relying too much on the location of patches. If instead hi is very localized in space (e.g., a
Dirac filter, which corresponds to no pooling), gz[u] may vary much more rapidly as a function of u,
which then allows f to discriminate differently depending on the spatial location. This provides a
different perspective on the invariance properties induced by pooling. If we denote G[u] = At*G[u],
the penalty writes
kGkL2(Ω,H)= X kG[u]kH.
u∈Ω
∙-v
Here, the RKHS norm ∣∣ ∙ ∣∣h also controls smoothness, but this time for functions G[u](∙) defined on
input patches. For homogeneous dot-product kernels of the form (2), the norm takes the form kgkH =
∣∣T-1 g∣L2(sd-i), where the regularization operator T-1 is the self-adjoint inverse square root of
the integral operator for the patch kernel restricted to L2(Sd-i). For instance, when the eigenvalues
of T decay polynomially, as for arc-cosine kernels, T-1 behaves like a power of the spherical
Laplacian (see Bach, 2017a, and Appendix A.2). Then we may write
kGkL2(Ω,H) = k ((A 1)> 0 T 2 )GkL2(Q)RL2(SdT),
which highlights that the norm applies two regularization operators (A-1)> and T-1 independently
on the spatial variable and the patch variable of (u, z) 7→ G[u](z), viewed here as an element
of L2(Ω) 0 L2(Sd-i).
4
Published as a conference paper at ICLR 2022
Table 1: Cifar10 test accuracy with 2-layer convolutional kernels with 3x3 patches and pool-
ing/downsampling sizes [2,5], with different choices of patch kernels κ1 and κ2 . The last model is
similar to a 1-layer convolutional kernel. See Section 5 for experimental details.
Ki-K2	Exp-Exp	ExP-Poly3	Exp-Poly2	Poly2-Exp	Poly2-Poly2	Exp-Lin
Test acc.	87.9%	87.7%	86.9%	85.1%	82.2% 一	80.9%
3.2	The Multi-Layer Case
We now study the case of convolutional kernels with more than one convolutional layer. While the
patch kernels used at higher layers are typically similar to the ones from the first layer, we show
empirically on Cifar10 that they may be replaced by simple polynomial kernels with little loss in
accuracy. We then proceed by studying the RKHS of such simplified models, highlighting the role of
depth for capturing interactions between different patches via kernel tensor products.
An empirical study. Table 1 shows the performance of a given 2-layer convolutional kernel
architecture, with different choices of patch kernels κ1 and κ2. The reference model uses exponential
kernels in both layers, following the construction in Mairal (2016). We find that replacing the second
layer kernel by a simple polynomial kernel of degree 3, κ2 (u) = u3, leads to roughly the same test
accuracy. By changing κ2 to κ2(u) = u2, the test accuracy is only about 1% lower, while doing
the same for the first layer decreases it by about 3%. The shallow kernel with a single non-linear
convolutional layer (shown in the last line of Table 1) performs significantly worse. This suggests
that the approximation properties described in Section 3.1 may not be sufficient for this task, while
even a simple polynomial kernel of order 2 at the second layer may substantially improve things by
capturing interactions, in a way that we describe below.
Two-layers with a quadratic kernel. Motivated by the above experiments, we now study the
RKHS of a two-layer kernel K2(x,x0) = hΨ(x), Ψ(xz))l2(Ω2,H2) with Ψ as in (1) with L = 2,
where the second-layer uses a quadratic kernel2 on patches k2(z, z0) = (hz, z0i)2. An explicit
feature map for k2 is given by 夕2(z) = Z 0 z. Denoting by H the RKHS of k`, the patches Z lie
in H1S2I thus We may view 夕2 as a feature map into a Hilbert space H = (H 0 H)lS2l×lS2| (by
isomorphism to H|S2| 0 H|S2|). The following result characterizes the RKHS of such a 2-layer
convolutional kernel, showing that it consists of additive models of interaction functions in H 0 H
on pairs of patches, with different spatial regularities on the interaction terms induced by the two
pooling operations. (Notation: We use the notations diag(M)[u] = M[u,u] for M ∈ L2(Ω2),
diag(x)[u, v] = l{u = v}x[u] for X ∈ L2(Ω), and Lc is the translation operator Lcx[u] = x[u 一 c].)
Proposition 2 (RKHS of 2-layer CKN with quadratic k2). Let Φ(x)=(夕ι(xu) 0 夕ι(xv))u,v∈Ω ∈
L2(Ω2, H0H). TheRKHSof K when k2(z, z0) = ({z, z0i)2 consists of functions oftheform
f(x) =	hGpq, Φ(x)i =	Gpq[u, v](xu, xv),
p,q∈S2	P,q∈S2 u,v∈Ω
where Gpq ∈ L2(Ω2,H 0 H) obeys the constraints Gpq ∈ Range(Epq) and diag((LpA1 0
LqAι)^'^Gpq) ∈ Range(Ag). Here, Epq : L2(Ωι) → L2(Ω2) is a linear operator given by
Epqx = (LpAι 0 LqAι)* diag(x).	(6)
The squared RKHS norm kf k2HK is then equal to the minimum over such decompositions of the
quantity
X IIA2* diag((LpAI 0 LqA1)t*Gpq)kL2(Ω2,H0H)∙	⑺
p,q∈S2
As discussed in the one-layer case, the inverses should be replaced by pseudo-inverses if needed, e.g.,
when using downsampling. In particular, if A2g is singular, the second constraint plays a similar role
to the one-layer case. In order to understand the first constraint, we show in Figure 2 the outputs
2For simplicity we study the quadratic kernel instead of the homogeneous version used in the experiments,
noting that it still performs well (78.0% instead of 79.4% on 10k examples).
5
Published as a conference paper at ICLR 2022
Figure 2: Display of the 2D response EpqX ∈ L2(Ω2) of the operator in (6) for various 1D inputs X ∈
L2(Ω). (left/Center) Dirac inputs X = δu centered at two different locations u; (right) Constant
input X = 1. The responses are localized on the p - q diagonal, corresponding to interactions between
two patches at distance around P - q. Here, we took (p,q) = (4,0), with a signal size ∣Ω∣ = 20.
of EpqX for Dirac delta signals X[v] = δu [v]. We can see that if the pooling filter h1 has a small
support of size m, then Gpq[u - p, v - q] must be zero when |u - v| > m, which highlights that the
functions in Gpq may only capture interactions between pairs of patches where the (signed) distance
between the first and the second is close to p - q.
The penalty then involves operators LpAi 0 LqAi, which may be seen as separable 2D convolutions
on the “images” Gpq[u, v]. Then, if z, z0 ∈ Rp|S1| are two fixed patches, defining gz,z0 [u, v] =
Gpq[u, v](z, z0), we have, assuming Ai is invertible and symmetric,
(LpAi 0 LqAi ) Gpq [u, v](z, z ) = (Ai 0 Ai )	gz,z0 [u - p, v - q]
= F2-i diag(F2(hi 0 hi))-iF2gz,z0[u - p, v - q],
where F2 = F 0 F is the 2D discrete Fourier transform. Thus, this penalizes the variations of gz,z0
in both dimensions, encouraging the interaction functions to not rely too strongly on the specific
positions of the two patches. This regularization is stronger when the spatial bandwidth of hi is
large, since this leads to a more localized filter in the frequency domain, with stronger penalties on
high frequencies. In addition to this 2D smoothness, the penalty in Proposition 2 also encourages
smoothness along thep - q diagonal of this resulting 2D image using the pooling operator A2. This
has a similar behavior to the one-layer case, where the penalty prevents the functions from relying
too much on the absolute position of the patches. Since A2 typically has a larger bandwidth than Ai ,
interaction functions Gpq[u, u+r] are allowed to vary with r more rapidly than with u. The regularity
of the resulting “smoothed” interaction terms as a function of the input patches is controlled by the
RKHS norm of the tensor product kernel ki 0 ki as described in Appendix A.2.
Extensions. When using a polynomial kernel k2 (z, z0) = (hz, z0i)α with α > 2, we obtain a
similar picture as above, with higher-order interaction terms. For example, if α = 3, the RKHS
contains functions with interaction terms of the form Gpqr[u, v, w](Xu, Xv, Xw), with a penalty
^X kA2* diag((AIp 0 A1q 0 AIr 户*Gpqr ) k L2(Ω2,H®3),
p,q,r∈S2
where Aic = LcAi. Similarly to the quadratic case, the first-layer pooling operator encourages
smoothness with respect to relative positions between patches, while the second-layer pooling
penalizes dependence on the global location. One may extend this further to higher orders to capture
more complex interactions, and our experiments suggest that a two-layer kernel of this form with a
degree-4 polynomial at the second layer may achieve state-of-the-art accuracy for kernel methods
on Cifar10 (see Table 2). We note that such fixed-order choices for κ2 lead to convolutional kernels
that lower-bound richer kernels with, e.g., an exponential kernel at the second layer, in the Loewner
order on positive-definite kernels. This imples in particular that the RKHS of these “richer” kernels
also contains the functions described above. For more than two layers with polynomial kernels,
one similarly obtains higher-order interactions, but with different regularization properties (see
Appendix D).
4	Generalization Properties
In this section, we study generalization properties of the convolutional kernels studied in Section 3,
and show improved sample complexity guarantees for architectures with pooling and small patches
when the problem exhibits certain invariance properties.
6
Published as a conference paper at ICLR 2022
Learning setting. We consider a non-parametric regression setting with data distribution ρ
over (x, y), where the goal is to minimize R(f) = E(x,y)〜ρ[(y - f (x))2]. We denote by f * =
Eρ[y∣x] = arg minf R(f) the regression function, and assume f * ∈ H for some RKHS H with
kernel K . Without any further assumptions on the kernel, we have the following generalization bound
on the excess risk for the kernel ridge regression (KRR) estimator, denoted f (see Proposition 7 in
Appendix E):
E[R(fn) - R(f*)] ≤ Ckf*kHyEx~ρX[K(x'x',	(8)
n
where ρX is the marginal distribution of ρ on inputs x, C is an absolute constant, and τρ2 is an upper
bound on the conditional noise variance Var[y∣x]. We note that this 1∕√n rate is optimal if no further
assumptions are made on the kernel (Caponnetto & De Vito, 2007). The quantity Ex〜PX [K(x, x)]
corresponds to the trace of the covariance operator, and thus provides a global control of eigenvalues
through their sum, which will already highlight the gains that pooling can achieve. Faster rates can be
achieved, e.g., when assuming certain eigenvalue decays on the covariance operator, or when further
restricting f*. We discuss in Appendix F.2 how similar gains to those described in this section can
extend to fast rate settings under specific scenarios.
One-layer CKN with invariance. As discussed in Section 3, the RKHS of 1-layer CKNs consists
of sums of functions that are localized on patches, each belonging to the RKHS H of the patch
kernel k1 . The next result illustrates the benefits of pooling when f * is translation invariant.
Proposition 3 (Generalization for 1-layer CKN.). Assume f*(x) = Pu∈ω g(xu) With g ∈ H of
minimal norm, and assume Ex〜PX [kι(xu, Xv)] ≤ σU-v for some (σ2)r∈Ω. ForaI-layer CKNKi
with any pooling filter h ≥ 0 with ∣∣hkι = 1, we have ∣∣f *kHKl = ∙∖∕∣Ω∣kgkH, and KRR satisfies
ʌ
E R(fn)- R(f* )
¥ J Xh,Lrhiσ2.
(9)
The quantities σr2 can be interpreted as auto-correlations between patches at distance r from each
other. Note that if h is a Dirac filter, then(h, Lrh = l{r = 0} (recall Lrh[u] = h[u — r]), thus
only σ2 plays a role in the bound, while if h is an average pooling filter, we have (h, Lr h = 1∕∣Ω∣,
so that σ0 is replaced by the average σ2 := Pr σ"∣Ω∣. Natural signals commonly display a decay
in their auto-correlation functions, suggesting that a similar decay may be present in σr2 as a function
of r. In this case, σ2 may be much smaller than σ2, which in turn yields an improved sample
complexity guarantee for learning such an f * with global pooling, by a factor up to ∣Ω∣ in the extreme
case where σr vanishes for r ≥ 1 (since σ2 = σ0∕∣Ω∣ in this case). In Appendix F.1, we provide
simple models where this can be quantified. For more general filters, such as local averaging or
Gaussian filters, and assuming σr2 ≈ 0 for r 6= 0, the bound interpolates between no pooling and
global pooling through the quantity ∣h∣22. While this yields a worse bound than global pooling on
invariant functions, such filters enable learning functions that are not fully invariant, but exhibit some
smoothness along the translation group, more efficiently than with no pooling. It should also be noted
that the requirement that g belongs to an RKHS H is much weaker when the patches are small, as
this typically implies that g admits more than p|S |/2 derivatives, a condition which becomes much
stronger as the patch size grows. In the fast rate setting that we study in Appendix F.2, this also
leads to better rates that only depend on the dimension of the patch instead of the full dimension (see
Theorem 8 in Appendix F.2).
Two layers. When using two layers with polynomial kernels at the second layer, we saw in Section 3
that the RKHS of CKNs consists of additive models of interaction terms of the order of the polynomial
kernel used. The next proposition illustrates how pooling filters and patch sizes at the second layer
may affect generalization on a simple target function consisting of order-2 interactions.
Proposition 4 (Generalization for 2-layer CKN.). Consider a 2-layer CKN K2 with quadratic k2,
as in Proposition 2, and pooling filters h1, h2 with ∣h1 ∣1 = ∣h2 ∣1 = 1. Assume that ρX satis-
fies Ex〜PX [kι(xu, Xu0)kι(xv, Xv，)] ≤ E if U = u0 or v = v0, and ≤ 1 otherwise. We have
Ex〜PX[K2(x,x)] ≤ ∣S2∣2∣Ω∣ (&(h2,Lvh2ihh1,Lvhii2 + E
(10)
7
Published as a conference paper at ICLR 2022
As an example, consider f *(x) = Euv g(xu, Xv) for g ∈ H 的 H ofminimal norm. Thefollowing
ʌ
table illustrates the obtained generalization bounds R(fn) — R(f *) forKRR with various two-layer
architectures (δ: Dirac filter; 1: global average pooling):
hi	h2	IS2I	kf *kK2	Ex〜PX [K2(x,x)]	Bound (e = 0,τp = 1)
I-	-δ-	"ΓΩT	∣Ω∣kg∣	~∣Ω∣3 + e∣Ω∣3-	-≡ΩFT√n-
δ	1	∣Ω∣	∣Ω∣kgk	∣Ω∣2 + e∣Ω∣3	kgk∣Ω∣2∕√n
1	1	∣Ω∣	p⅛k	∣Ω∣ + e∣Ω∣3	kgk∣Ω∣∕√n
1	δ or 1	1	p⅜	∣Ω∣-1 + e∣Ω∣	kgk∕√n
The above result shows that the two-layer model allows for a much wider range of behav-
iors than the one-layer case, between approximation (through the norm ∣∣f *||此)and estimation
(through Ex〜PX [K2(χ, x)]), depending on the choice of architecture. Choosing the right architecture
may lead to large improvements in sample complexity when the target functions has a specific
structure, for instance here by a factor up to ∣Ω∣2∙5. In Appendix F.1, we discuss simple possible
models where we may have a small . Note that choosing filters that are less localized than Dirac
impulses, but more than global average pooling, will again lead to different “variance” terms (10),
while providing more flexibility in terms of approximation compared to global pooling. This result
may be easily extended to higher-order polynomials at the second layer, by increasing the exponents
on |S2 | and hh1, Lvh1i to the degree of the polynomial. Other than the gains in sample complexity
due to pooling, the bound also presents large gains compared to a “fully-connected” architecture, as
in the one-layer case, since it only grows with the norm of a local interaction function in HgH that
depends on two patches, which may then be small even when this function has low smoothness.
5	Numerical Experiments
In this section, we provide additional experiments illustrating numerical properties of the con-
volutional kernels considered in this paper. We focus here on the Cifar10 dataset, and on CKN
architectures based on the exponential kernel. Additional results are given in Appendix B.
Experimental setup on Cifar10. We consider classification on Cifar10 dataset, which consists of
50k training images and 10k test images with 10 different output categories. We pre-process the
images using a whitening/ZCA step at the patch level, which is commonly used for such kernels
on images (Mairal, 2016; Shankar et al., 2020; Thiry et al., 2021). This may help reduce the
effective dimensionality of patches, and better align the dominant eigen directions to the target
function, a property which may help kernel methods (Ghorbani et al., 2020). Our convolutional kernel
evaluation code is written in C++ and leverages the Eigen library for hardware-accelerated numerical
computations. The computation of kernel matrices is distributed on up to 1000 cores on a cluster
consisting of Intel Xeon processors. Computing the full Cifar10 kernel matrix typically takes around
10 hours when running on all 1000 cores. Our results use kernel ridge regression in a one-versus-all
approach, where each class uses labels 0.9 for the correct label and -0.1 for the other labels. We report
the test accuracy for a fixed regularization parameter λ = 10-8 (we note that the performance typically
remains the same for smaller values of λ). The exponential kernel always refers to K(U) = eσ12(UT)
with σ = 0.6. Code is available at https://github.com/albietz/ckn_kernel.
Varying the kernel architecture. Table 2 shows test accuracies for different architectures com-
pared to Table 1, including 3-layer models and 2-layer models with larger patches. In both cases,
the full models with exponential kernels outperform the 2-layer architecture of Table 1, and provide
comparable accuracy to the Myrtle10 kernel of Shankar et al. (2020), with an arguably simpler
architecture. We also see that using degree-3 or 4 polynomial kernels at the second second layer of
the two-layer model essentially provides the same performance to the exponential kernel, and that
degree-2 at the second and third layer of the 3-layer model only results in a 0.3% accuracy drop. The
two-layer model with degree-2 at the second layer loses about 1% accuracy, suggesting that certain
Cifar10 images may require capturing interactions between at least 3 different patches in the image
for good classification, though even with only second-order interactions, these models significantly
outperform single-layer models. While these results are encouraging, computing such kernels is
prohibitively costly, and we found that applying the Nystrom approach of Mairal (2016) to these
8
Published as a conference paper at ICLR 2022
Table 2: Cifar10 test accuracy for two-layer architectures with larger second-layer patches, or three
layer architectures. κ denote the patch kernels used at each layer, ‘conv’ the patch sizes, and ‘pool’
the downsampling factors for Gaussian pooling filters. We include the Myrtle10 convolutional
kernel Shankar et al. (2020), which consists of 10 layers including Exp kernels on 3x3 patches and
2x2 average pooling.
K	ConV	pool	Testacc. (10k)	Test acc. (50k)
(Exp,Exp)	(3,5)	(2,5)	8T7T%	883%
(Exp,Poly4)	(3,5)	(2,5)	81.3%	88.3%
(Exp,Poly3)	(3,5)	(2,5)	81.1%	88.2%
	(Exp,Poly2)		(3,5)	(2,5)	80.1%	87.4%
(Exp,Exp,Exp)	(3,3,3)	(2,2,2)	807%	882%
(Exp,Poly2,Poly2)	(3,3,3)	(2,2,2)	80.5%	87.9%
Myrtle10 Shankar et aL(2020~	-	-	-	88.2% 一
6 XIO-2 -
4 × IO-2 -
spectral decay, 1/2/3 layers
- 3-layer (3,3.3) (2,2,2)
-2-layer (3,3) (2,5)
I-Iayer (3) (5)
I-Iayer (5) (8)
gaussιan vs strιded pooling
----gausslan
IO1 '=	---- Strlded
average MSE
(υSE
Figure 3: (left) Mean squared error of kernel ridge regression on Cifar10 for different kernels with
3x3 patches as a function of sample size (averaged over the 10 classes). (center) Eigenvalue decays of
kernel matrices (with Exp kernels) on 1000 Cifar images, for different depths, patch sizes (3 or 5) and
pooling sizes. (right) decays for 2-layer architecture, Gaussian pooling filter vs strided convolutions
(i.e., no pooling). The plots illustrate that pooling is essential for reducing effective dimensionality.
kernels with more layers or larger patches requires larger models than for the architecture of Table 1
for a similar accuracy. Figure 3(left) shows learning curves for different architectures, with slightly
better convergence rates for more expressive models involving higher-order kernels or more layers;
this suggests that their approximation properties may be better suited for these datasets.
Role of pooling. Figure 3 shows the spectral decays of the empirical kernel matrix on 1000
Cifar images, which may help assess the “effective dimensionality” of the data, and are related to
generalization properties (Caponnetto & De Vito, 2007). While multi-layer architectures with pooling
seem to provide comparable decays for various depths, removing pooling leads to significantly
slower decays, and hence much larger RKHSs. In particular, the “strided pooling” architecture (i.e.,
with Dirac pooling filters and downsampling) shown in Figure 3(right), which resembles the kernel
considered in (Scetbon & Harchaoui, 2020), obtains less than 40% accuracy on 10k examples. This
suggests that the regularization properties induced by pooling, studied in Section 3, are crucial for
efficient learning on these problems, as shown in Section 4. Appendix B provides more empirics on
different pooling configurations.
6	Discussion and Concluding Remarks
In this paper, we studied approximation and generalization properties of convolutional kernels,
showing how multi-layer models with convolutional architectures may effectively break the curse of
dimensionality on problems where the input consists of high-dimensional natural signals, by modeling
localized functions on patches and interactions thereof. We also show how pooling induces additional
smoothness constraints on how interaction terms may or may not vary with global and relative spatial
locations. An important question for future work is how optimization of deep convolutional networks
may further improve approximation properties compared to what is captured by the kernel regime
presented here, for instance by selecting well-chosen convolution filters at the first layer, or interaction
patterns in subsequent layers, perhaps in a hierarchical manner.
9
Published as a conference paper at ICLR 2022
Acknowledgments
The author would like to thank Francis Bach, Alessandro Rudi, Joan Bruna, and Julien Mairal for
helpful discussions.
References
Zeyuan Allen-Zhu and Yuanzhi Li. Backward feature correction: How deep learning performs deep
learning. arXiv preprint arXiv:2001.04413, 2020.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. In Advances in Neural Information Processing
Systems (NeurIPS), 2019.
Francis Bach. Breaking the curse of dimensionality with convex neural networks. Journal of Machine
LearningResearch (JMLR),18(1):629-681, 2017a.
Francis Bach. On the equivalence between kernel quadrature rules and random feature expansions.
Journal of Machine Learning Research (JMLR), 18(1):714-751, 2017b.
Francis Bach. Learning Theory from First Principles (draft). 2021. URL https://www.di.ens.
fr/~fbach/ltfp_book.pdf.
Gregory Beylkin and Martin J Mohlenkamp. Numerical operator calculus in higher dimensions.
Proceedings of the National Academy of Sciences, 99(16):10246-10251, 2002.
Alberto Bietti and Francis Bach. Deep equals shallow for ReLU networks in kernel regimes. In
Proceedings of the International Conference on Learning Representations (ICLR), 2021.
Alberto Bietti and Julien Mairal. Group invariance, stability to deformations, and complexity of
deep convolutional representations. Journal of Machine Learning Research (JMLR), 20(25):1-49,
2019a.
Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. In Advances in
Neural Information Processing Systems (NeurIPS), 2019b.
Joan Bruna and StePhane Mallat. Invariant scattering convolution networks. IEEE Transactions on
Pattern Analysis and Machine Intelligence (PAMI), 35(8):1872-1886, 2013.
Andrea CaPonnetto and Ernesto De Vito. OPtimal rates for the regularized least-squares algorithm.
Foundations of Computational Mathematics, 7(3):331-368, 2007.
Lin Chen and Sheng Xu. DeeP neural tangent kernel and laPlace kernel have the same rkhs. In
Proceedings of the International Conference on Learning Representations (ICLR), 2021.
Minshuo Chen, Yu Bai, Jason D Lee, Tuo Zhao, Huan Wang, Caiming Xiong, and Richard Socher.
Towards understanding hierarchical learning: Benefits of neural rePresentations. In Advances in
Neural Information Processing Systems (NeurIPS), 2020.
Lenaic Chizat and Francis Bach. ImPlicit bias of gradient descent for wide two-layer neural networks
trained with the logistic loss. In Conference on Learning Theory, 2020.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable Programming.
In Advances in Neural Information Processing Systems (NeurIPS), 2019.
Youngmin Cho and Lawrence K Saul. Kernel methods for deeP learning. In Advances in Neural
Information Processing Systems (NIPS), 2009.
Carlo Ciliberto, Francis Bach, and Alessandro Rudi. Localized structured Prediction. In Advances in
Neural Information Processing Systems (NeurIPS), 2019.
Nadav Cohen and Amnon Shashua. Convolutional rectifier networks as generalized tensor de-
comPositions. In Proceedings of the International Conference on Machine Learning (ICML),
2016.
10
Published as a conference paper at ICLR 2022
Nadav Cohen and Amnon Shashua. Inductive bias of deep convolutional networks through pooling
geometry. In Proceedings of the International Conference on Learning Representations (ICLR),
2017.
Felipe Cucker and Steve Smale. On the mathematical foundations of learning. Bulletin of the
American mathematical society, 39(1):1-49, 2002.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks:
The power of initialization and a dual view on expressivity. In Advances in Neural Information
Processing Systems (NIPS), 2016.
Simon S Du, Yining Wang, Xiyu Zhai, Sivaraman Balakrishnan, Ruslan Salakhutdinov, and Aarti
Singh. How many samples are needed to estimate a convolutional neural network? In Advances in
Neural Information Processing Systems (NeurIPS), 2018.
Costas Efthimiou and Christopher Frye. Spherical harmonics in p dimensions. World Scientific,
2014.
Alessandro Favero, Francesco Cagnetta, and Matthieu Wyart. Locality defeats the curse of dimen-
sionality in convolutional teacher-student scenarios. In Advances in Neural Information Processing
Systems (NeurIPS), 2021.
Adri鱼 Garriga-Alonso, Laurence Aitchison, and Carl Edward Rasmussen. Deep convolutional
networks as shallow gaussian processes. In Proceedings of the International Conference on
Learning Representations (ICLR), 2019.
Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, and Ronen Basri. On
the similarity between the laplace and neural tangent kernels. In Advances in Neural Information
Processing Systems (NeurIPS), 2020.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural
networks outperform kernel methods? In Advances in Neural Information Processing Systems
(NeurIPS), 2020.
Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent on
linear convolutional networks. In Advances in Neural Information Processing Systems (NeurIPS),
2018.
Wolfgang HackbUSch and Stefan Kuhn. A new scheme for the tensor representation. Journal of
Fourier analysis and applications, 15(5):706-722, 2009.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2016.
Reinhard Heckel and Mahdi Soltanolkotabi. Denoising and regularization via exploiting the structural
bias of convolutional generators. In Proceedings of the International Conference on Learning
Representations (ICLR), 2020.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in Neural Information Processing Systems (NIPS),
2018.
Herve J6gou, Florent Perronnin, Matthijs Douze, Jorge Sdnchez, Patrick P6rez, and Cordelia Schmid.
Aggregating local image descriptors into compact codes. IEEE Transactions on Pattern Analysis
and Machine Intelligence (PAMI), 34(9):1704-1716, 2011.
Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, and
Jascha Sohl-Dickstein. Finite versus infinite neural networks: an empirical study. In Advances in
Neural Information Processing Systems (NeurIPS), 2020.
Zhiyuan Li, Ruosong Wang, Dingli Yu, Simon S Du, Wei Hu, Ruslan Salakhutdinov, and Sanjeev
Arora. Enhanced convolutional neural tangent kernels. arXiv preprint arXiv:1911.00809, 2019.
11
Published as a conference paper at ICLR 2022
Zhiyuan Li, Yi Zhang, and Sanjeev Arora. Why are convolutional nets more sample-efficient than
fully-connected nets? In Proceedings of the International Conference on Learning Representations
(ICLR), 2021.
Yi Lin. Tensor product space anova models. Annals of Statistics, 28(3):734-755, 2000.
David G Lowe. Object recognition from local scale-invariant features. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 1999.
Julien Mairal. End-to-End Kernel Learning with Supervised Convolutional Kernel Networks. In
Advances in Neural Information Processing Systems (NIPS), 2016.
Julien Mairal, Piotr Koniusz, Zaid Harchaoui, and Cordelia Schmid. Convolutional kernel networks.
In Advances in Neural Information Processing Systems (NIPS), 2014.
Eran Malach and Shai Shalev-Shwartz. Computational separation between convolutional and fully-
connected networks. In Proceedings of the International Conference on Learning Representations
(ICLR), 2021.
Stephane Mallat. Group invariant scattering. Communications on Pure and Applied Mathematics, 65
(10):1331-1398, 2012.
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Learning with invariances in random
features and kernel models. In Conference on Learning Theory (COLT), 2021.
Hrushikesh N Mhaskar and Tomaso Poggio. Deep vs. shallow networks: An approximation theory
perspective. Analysis and Applications, 14(06):829-848, 2016.
Ha Quang Minh, Partha Niyogi, and Yuan Yao. Mercer’s theorem, feature maps, and smoothing. In
Conference on Learning Theory (COLT), 2006.
Theodor Misiakiewicz and Song Mei. Learning with convolution and pooling operations in kernel
methods. arXiv preprint arXiv:2111.08308, 2021.
Roman Novak, Lechao Xiao, Yasaman Bahri, Jaehoon Lee, Greg Yang, Jiri Hron, Daniel A Abolafia,
Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with many
channels are gaussian processes. In Proceedings of the International Conference on Learning
Representations (ICLR), 2019.
Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. Why and
when can deep-but not shallow-networks avoid the curse of dimensionality: a review. International
Journal of Automation and Computing, 14(5):503-519, 2017.
Saburou Saitoh. Integral transforms, reproducing kernels and their applications, volume 369. CRC
Press, 1997.
Jorge Sdnchez, Florent Perronnin, Thomas Mensink, and Jakob Verbeek. Image classification with
the fisher vector: Theory and practice. International Journal of Computer Vision (IJCV), 105(3):
222-245, 2013.
Meyer Scetbon and Zaid Harchaoui. Harmonic decompositions of convolutional networks. In
Proceedings of the International Conference on Machine Learning (ICML), 2020.
Johannes Schmidt-Hieber et al. Nonparametric regression using deep neural networks with relu
activation function. Annals of Statistics, 48(4):1875-1897, 2020.
Bernhard Scholkopf and Alexander J Smola. Learning with kernels: support vector machines,
regularization, optimization, and beyond. 2001.
Vaishaal Shankar, Alex Fang, Wenshuo Guo, Sara Fridovich-Keil, Jonathan Ragan-Kelley, Ludwig
Schmidt, and Benjamin Recht. Neural kernels without tangents. In Proceedings of the International
Conference on Machine Learning (ICML), 2020.
12
Published as a conference paper at ICLR 2022
Winfried Sickel and Tino Ullrich. Tensor products of Sobolev-besov spaces and applications to
approximation from the hyperbolic cross. Journal of Approximation Theory, 161(2):748-786,
2009.
Alex J Smola, Zoltan L Ovari, and Robert C Williamson. Regularization with dot-product kernels. In
Advances in Neural Information Processing Systems (NIPS), 2001.
Louis Thiry, Michael Arbel, Eugene Belilovsky, and Edouard Oyallon. The unreasonable effectiveness
of patches in deep convolutional kernels methods. In Proceedings of the International Conference
on Learning Representations (ICLR), 2021.
Ulrike von Luxburg and Olivier Bousquet. Distance-based classification with lipschitz functions.
Journal of Machine Learning Research (JMLR), 5(Jun):669-695, 2004.
Grace Wahba. Spline models for observational data, volume 59. Siam, 1990.
Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-
bridge University Press, 2019.
Colin Wei, Jason Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and
optimization of neural nets vs their induced kernel. In Advances in Neural Information Processing
Systems (NeurIPS), 2019.
Thomas Wiatowski and Helmut Bolcskei. A mathematical theory of deep convolutional neural
networks for feature extraction. IEEE Transactions on Information Theory, 64(3):1845-1866,
2018.
Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,
2019.
Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
Proceedings of the European Conference on Computer Vision (ECCV), 2014.
Y. Zhang, P. Liang, and M. J. Wainwright. Convexified convolutional neural networks. In International
Conference on Machine Learning (ICML), 2017.
A	Further Background
This section provides further background on the problem of approximation of functions defined on
signals, as well as on the kernels considered in the paper. We begin by introducing and motivating
the problem of learning functions defined on signals such as images, which captures tasks such as
image classification where deep convolutional networks are predominant. We then recall properties
of dot-product kernels and kernel tensor products, which are key to our study of approximation.
A.1 Natural Signals and Curse of Dimensionality
We consider learning problems consisting of labeled examples (x, y)〜P from a data distribution ρ,
where X is a discrete signal x[u] with U ∈ Ω denoting the position (e.g., pixel location in an image) in
a domain Ω, x[u] ∈ Rp (e.g., P = 3 for RGB pixels), and y ∈ R is a target label. In a non-parametric
setup, statistical learning may be framed as trying to approximate the regression function
f * (X) = Eρ[y∣χ]
using samples from the data distribution ρ. If f * is only assumed to be Lipschitz, learning requires a
number of samples that scales exponentially in the dimension (see, e.g., von Luxburg & Bousquet
(2004); Wainwright (2019)), a phenomenon known as the curse of dimensionality. In the case of
natural signals, the dimension d = p∣Ω∣ scales with the size of the domain ∣Ω∣ (e.g., the number of
pixels), which is typically very large and thus makes this intractable. One common way to alleviate
this is to assume that f * is smooth, however the order of smoothness typically needs to be of the order
of the dimension in order for the problem to become tractable, which is a very strong assumption
here when d is very large. This highlights the need for more structured assumptions on f * which
may help overcome the curse of dimensionality.
13
Published as a conference paper at ICLR 2022
Insufficiency of invariance and stability. Two geometric properties that have been successful for
studying the benefits of convolutional architectures are (near-)translation invariance and stability to
deformations. Various works have shown that certain convolutional models f yield good invariance
and stability Mallat (2012); Bruna & Mallat (2013); Bietti & Mairal (2019a), in the sense that
when X is a translation or a small deformation of x, then |f (X) - f (x)| is small. Nevertheless, one
can show that for band-limited signals (such as discrete signals), kx - χ∣∣2 can be controlled in a
similar way (though with worse constants, see (Wiatowski & Bolcskei, 2018, Proposition 5)), so that
Lipschitz functions on such signals obey such stability properties. Thus, deformation stability is not
a much stronger assumption than Lipschitzness, and is insufficient by itself to escape the curse of
dimensionality.
Spatial localization. One successful strategy for learning image recognition models which predates
deep learning is to rely on simple aggregations of local features. These may be extracted using
hand-crafted procedures (Lowe, 1999; Sdnchez et al., 2013; J6gou et al., 2011), or using learned
feature extractors, either through learned filters in the early layers of a CNN (Zeiler & Fergus, 2014),
or other procedures (e.g., Thiry et al. (2021)). One simplified example that encodes such a prior
is if the target function f * only depends on the input image through a localized part of the input
such as a patch xu = (x[u + v])v∈S ∈ Rp|S|, where S is a small box centered around 0, that is,
f* (x) = g*(xu). Then, if g* is assumed to be Lipschitz, we would like a sample complexity that
only scales exponentially in the dimension of a patch p|S |, which is much smaller than dimension of
the entire image p∣Ω∣. This is indeed the case if we use a kernel defined on such patches, such as
K(x, x0) =	k(xu, x0u),
where k is a “simple” kernel such as a dot-product kernel, as discussed in Appendix C. In contrast,
if K is a dot-product kernel on the entire image, corresponding to an infinite-width limit of a fully-
connected network, then approximation is more difficult and is generally cursed by the full dimension
(see Appendix C). While some models of wide fully-connected networks provide some adaptivity to
low-dimensional structures such as the variables in a patch (Bach, 2017a), no tractable algorithms are
currently known to achieve such behavior provably, and it is reasonable to instead encode such prior
information in a convolutional architecture.
Modeling interactions. Modeling interactions between elements of a system at different scales,
possibly hierarchically, is important in physics and complex systems, in order to efficiently handle
systems with large numbers of variables (Beylkin & Mohlenkamp, 2002; Hackbusch & Kuhn, 2009).
As an example, one may consider target functions f* (x) that consist of interaction functions of
the form g(xp, xq), where p, q denote locations of the corresponding patches, and higher-order
interactions may also be considered. In the context of image recognition, while functions of a single
patch may capture local texture information such as edges or color, such an interaction function
may also respond to specific spatial configurations of relevant patches, which could perhaps help
identify properties related to the “shape” of an object, for instance. If such functions g are too general,
then the curse of dimensionality may kick in again when one considers more than a handful of
patches. Certain idealized models of approximation may model such interactions more efficiently
through hierarchical compositions (e.g., Poggio et al. (2017)) or tensor decompositions (Cohen &
Shashua, 2016; 2017), though no tractable algorithms are known to find such models. In this work,
we tackle this in a tractable way using multi-layer convolutional kernels. We show that they can
model interactions through kernel tensor products, which define functional spaces that are typically
much smaller and more structured than for a generic kernel on the full vector (xp, xq).
A.2 Dot-Product Kernels and their Tensor Products
In this section, we review some properties of dot-product kernels, their induced RKHS and regular-
ization properties. We then recall the notion of tensor product of kernels, which allows us to describe
the RKHS of products of kernels in terms of that of individual kernels.
Dot-product kernels. The rotation-invariance of dot-product kernels provides a natural description
of their RKHS in terms of harmonic decompositions of functions on the sphere using spherical
harmonics (Smola et al., 2001; Bach, 2017a). This leads to natural connections with regularity
14
Published as a conference paper at ICLR 2022
properties of functions defined on the sphere. For instance, if the kernel integral operator on L2(Sd-1)
has a polynomially decaying spectral decay, as is the case for kernels arising from the ReLU
activation (Bach, 2017a; Bietti & Mairal, 2019b), then the RKHS contains functions g ∈ L2(Sd-1)
with an RKHS norm equivalent to
心：/-1 gkL2(Sd-1),	(II)
for some β that depends on the decay exponent and must be larger than (d - 1)/2, with ∆Sd-1
the Laplace-Beltrami operator on the sphere. This resembles a Sobolev norm of order β, and the
RKHS contains functions with bounded derivatives up to order β. When d is small (e.g., at the first
layer with small images patches), the space contains functions that need not be too regular, and may
thus be quite discriminative, while for large d (e.g., for a fully-connected network), the functions
must be highly smooth in order to be in the RKHS, and large norms are necessary to approach
non-smooth functions. For kernels with decays faster than polynomial, such as the Gaussian kernel,
the RKHS contains smooth functions, but may still provide good approximation to non-smooth
functions, particularly with small d and when using small bandwidth parameters. The homogeneous
case (2) leads to functions f (x) = Ilxkg(奇)with g defined on the sphere, with a norm given by the
same penalty (11) on the function g (Bietti & Mairal, 2019b).
Kernel tensor products. For more than one layer, the convolutional kernels we study in Section 3
can be expressed in terms of products of kernels on patches, of the form
m
K((x1, . . . , xm), (x1 , . . . , x0m)) =	k(xj, x0j),	(12)
j=1
where x1 , . . . , xm , x01 , . . . , x0m ∈ Rd are patches which may come from different signal locations.
If φ : Rd → H is the feature map into the RKHS H of k, then
ψ(xi,...,Xm) = φ(xi) 0∙∙∙0 2(Xm)
is a feature map for K, and the corresponding RKHS, denoted H0m = H0∙∙∙0H, contains all
functions
n
f(x1, . . . , xm ) =	gi,1 (x1 ) . . . gi,m (xm),
i=1
for some n, with gi,m ∈ H for i ∈ [n] and j ∈ [m] (see,e.g., (Wainwright, 2019, Section 12.4.2)
for a precise construction). The resulting RKHS is often much smaller than for a more generic
kernel on Rd×m; for instance, if H is a Sobolev space of order β in dimension d, then H0m is
much smaller than the Sobolev space of order β in d × m dimensions, and corresponds to stronger,
mixed regularity conditions (see, e.g., Bach, 2017b; Sickel & Ullrich, 2009, for the d = 1 case).
This can yield improved generalization properties if the target function has such a structure (Lin,
2000). Kernels of the form (12) and sums of such kernels have been useful tools for avoiding the
curse of dimensionality by encoding interactions between variables that are relevant to the problem
at hand (Wahba, 1990, Chapter 10). In what follows, we show how patch extraction and pooling
operations shape the properties of such interactions between patches in convolutional kernels through
additional spatial regularities.
B Additional Experiments
In this section, we provide additional experiments to those presented in Section 5, using different
patch kernels, patch sizes, pooling filters, preprocessings, and datasets.
Three-layer architectures with different patch kernels. Table 3 provides more results on 3-layer
architectures compared to Table 2, including different changes in the degrees of polynomial kernels
at the second and third layer. In particular we see that the architecture with degree-2 kernels at both
layers, which captures interactions of order 4, also outperforms the simpler ones using degree-4
kernels at either layer, suggesting that a deeper architecture may better model relevant interactions
terms on this problem.
15
Published as a conference paper at ICLR 2022
Table 3: Cifar10 test accuracy with 3-layer convolutional kernels with 3x3 patches and pool-
ing/downsampling sizes [2,2,2], with different choices of patch kernels κ1, κ2 and κ3. The last
model is similar to a 1-layer convolutional kernel. Due to high computational cost, we use 10k
training images instead of the full training set (50k images) in most cases.
K	K	K3	Test ac. (10k)	Test ac. (50k)
ExP	ExP	Exp	807^	882%
ExP	Poly2	Poly2	80.5%	87.9%
Exp	Poly4	Lin	80.2%	-
Exp	Lin	Poly4	79.2%	-
ExP	Lin	Lin	74.1%	-
Table 4: Cifar10 test accuracy on 10k examples with 2-layer convolutional kernels with 3x3 patches
at the first layer, pooling/downsampling sizes [2,5] and patch kernels [Exp,Poly2], with different
patch sizes at the second layer.
|S2|	1x1	3x3	5x5	7x7	9x9	11x11
Testacc. (10k)	76.3%	79.4%	80.1%	80.1%	80.1%	79.9%
Varying the second layer patch size. Table 4 shows the variations in test performance when
changing the size of the second patches at the second layer. We see that intermediate sizes between
3x3 and 9x9 work best, but that performance degrades when using patches that are too large or too
small. For very large patches, this may be due to the large variance in (10), or perhaps instability (Bietti
& Mairal, 2019a). For |S2| =1x1, note that while pooling after the first layer allows even 1x1 patches
to capture interactions across different input image patches, these may be limited to short range
interactions when the pooling filter is localized (see Proposition 2), which may limit the expressivity
of the model.
Table 5: Cifar10 test accuracy with patch kernels that are either arc-cosine kernels (denoted ReLU) or
polynomial kernels. The 2-layer architectures use 3x3 patches and [2,5] downsampling/pooling as in
Table 1. “ReLU-NTK” indicates that we consider the neural tangent kernel for a ReLU network with
similar architecture, instead of the conjugate kernel.
κι	K2	Testac. (10k)	Test ac. (50k)
ReLU-	ReLU	785%	866%
ReLU-NTK	ReLU-NTK	79.2%	87.2%
ReLU	Poly2	77.2%	-
ReLU	Lin	71.5%	-
Arc-cosine kernel. In Table 5, we consider 2-layer convolutional kernels with a similar architecture
to those considered in Table 1, but where we use arc-cosine kernels arising from ReLU activations
instead of the exponential kernel used in Section 5, given by
K(U) = — (U ∙ (π — arccos(u)) + pl — u2).
The obtained convolutional kernel then corresponds to the conjugate kernel or NNGP kernel arising
from an infinite-width convolutional network with the ReLU activation (Daniely et al., 2016; Garriga-
Alonso et al., 2019; Novak et al., 2019). We may also consider the neural tangent kernel (NTK) for
the same architecture, which additionally involves arc-cosine kernels of degree 0, which correspond
to random feature kernels for step activations u → l{u ≥ 0}. We find that the NTK performs slightly
better than the conjugate kernel, but both kernels achieve lower accuracy compared to the Exponential
kernel shown in Table 1. Nevertheless, we observe a similar pattern regarding the use of polynomial
kernels at the second layer, namely, the drop in accuracy is much smaller when using a quadratic
kernel compared to a linear kernel, suggesting that non-linear kernels on top of the first layer, and the
interactions they may capture, are crucial on this dataset for good accuracy.
16
Published as a conference paper at ICLR 2022
Table 6: Cifar10 test accuracy for one-layer architectures with larger patches of size 6x6, exponential
kernels, and different downsampling/pooling sizes (using Gaussian pooling filters with bandwidth
and size of filters proportional to the downsampling factor). The results are for 10k training samples.
Pooling	2	4	6	8	10
Test acc. (10k)	67.6%	73.3%	75.5%	75.8%	75.5%
One-layer architectures and larger initial patches. Table 6 shows the accuracy for one-layer
convolutional kernels with 6x6 patches3 and various pooling sizes, with a highest accuracy of 75.8%
for a pooling size of 8. While this improves on the accuracy obtained with 3x3 patches (slightly
above 74% for the architectures in Tables 1 and 3 with a single non-linear kernel at the first layer),
these accuracies remain much lower than those achieved by two-layer architectures with even
quadratic kernels at the second layer. While using larger patches may allow capturing patterns that
are less localized compared to small 3x3 patches, the neighborhoods that they model need to remain
small in order to avoid the curse of dimensionality when using dot-product kernels, as discussed in
Section 2. Instead, the multi-layer architecture may model information at larger scales with a much
milder dependence on the size of the neighborhood, thanks to the structure imposed by tensor product
kernels (see Section A.2) and the additional regularities induced by pooling.
We also found that larger patches at the first layer may hurt performance in multi-layer models: when
considering the architecture of Table 1 with exponential kernels, using 5x5 patches instead of 3x3 at
the first layer yields an accuracy of 79.6% instead of 80.5% on Cifar10 when training on the same
10k images. This again reflects the benefits of using small patches at the first layer for allowing
better approximation on small neighborhoods, while modeling larger scales using interaction models
according to the structure of the architecture. We note nevertheless that for standard deep networks,
larger patches are often used at the first layer (e.g., He et al., 2016), as the feature selection capabilities
of SGD may alleviate the dependence on dimension, e.g., by finding Gabor-like filters.
Gaussian vs average pooling. Table 7 shows the differences in performance between two or three
layer architectures considered in Table 2, when Gaussian pooling filters are replaced by average
pooling filters. For both architectures considered, average pooling leads to a significant performance
drop. This suggests that one may need deeper architectures in order for such average pooling filters to
work well, as in (Shankar et al., 2020), either with multiple 3x3 convolutional layers before applying
pooling, or by applying multiple average pooling layers in a row as in certain Myrtle kernels. Note
that iterating multiple average pooling layers in a row is equivalent to using a larger and more smooth
pooling filter (with one more order of smoothness at each layer), which may then be more comparable
to our Gaussian pooling filters.
Table 7: Gaussian vs average pooling for two models from Table 2.
	Model		Gaussian	Average
κ: (Exp,Exp), conv: (3,5), pool: (2,5) κ: (Exp,Exp,Exp), conv: (3,3,3), pool: (2,2,2)	88.3% 88.2%	75.9% 72.4%
Table 8: SVHN test accuracy for a two-layer convolutional kernel network with NystrGm approXima-
tion (Mairal, 2016) with patch size 3x3, pooling sizes [2,5], and filters [256, 4096].
K	K	Test acc. (full with Nystrom)
EXP	EXP	895%
Exp	Poly3	89.3%
Exp	Poly2	88.6%
Poly2	Exp	87.1%
Poly2	Poly2	86.6%
EXP	Lin		78.5%	
3 Note that in this case the ZCA/whitening step is applied on these larger 6X6 patches.
17
Published as a conference paper at ICLR 2022
SVHN dataset. We now consider the SVHN dataset, which consists of 32x32 images of digits from
Google Street View images, 73 257 for training and 26 032 for testing. Due to the larger dataset size,
We only consider the kernel approximation approach of Mairal (2016)based on the NystrGm method,
which projects the patch kernel feature maps at each layer to finite-dimensional subspaces generated
by a set of anchor points (playing the role of convolutional filters), themselves computed via a
K-means clustering of patches.4 We train one-versus-all classifiers on the resulting finite-dimensional
representations using regularized ERM With the squared hinge loss, and simply report the best test
accuracy over a logarithmic grid of choices for the regularization parameter, ignoring model selection
issues in order to assess approximation properties. We use the same ZCA preprocessing as on Cifar10
and the same architecture as in Table 1, With a relatively small number of filters (256 at the first layer,
4096 at the second layer, leading to representations of dimension 65 536), noting that the accuracy can
further improve When increasing this number. Our observations are similar to those for the Cifar10
dataset: using a degree-3 polynomial kernel at the second layer reaches very similar accuracy to the
exponential kernel; using a degree-2 polynomial leads to a slight drop, but a smaller drop than When
making this same change at the first layer; using a linear kernel at the second layer leads to a much
larger drop. This again highlights the importance of using non-linear kernels on top of the first layer
in order to capture interactions at larger scales than the scale of a single patch.
Local versus global whitening. Recall that our pre-processing is based on a patch-level Whitening
or ZCA on each image, folloWing Mairal (2016). In practice, this is achieved by Whitening extracted
patches from each image, and reconstructing the image from Whitened patches via averaging. In
contrast, other approaches use global Whitening of the entire image Lee et al. (2020); Shankar et al.
(2020). For the 2-layer model shoWn in Table 2 With 5x5 patches at the second layer, We found global
ZCA to provide significantly Worse performance, With a drop from 88.3% to about 80%.
Finite networks and comparison to Shankar et al. (2020). The Work Shankar et al. (2020)
introduces Myrtle kernels but also consider similar architectures for usual CNNs With finite-Width,
trained With stochastic gradient descent. Obtaining competitive architectures for the finite-Width
case is not the goal of our Work, Which focuses on good architectures for the kernel setup, yet
it remains interesting to consider this question. In the case of Shankar et al. (2020), training the
finite-Width netWorks yields better accuracy compared to their “infinite-Width” kernel counterparts, a
commonly observed phenomenon Which may be due to better “adaptivity” of optimization algorithms
compared to kernel methods, Which have a fixed representation and thus may not learn representations
adapted to the data (see, e.g., Allen-Zhu & Li, 2020; Bach, 2017a; Chizat et al., 2019). Nevertheless,
We found that for the tWo-layer architecture considered in Table 1, Which has many feWer layers
compared to the Myrtle architectures of Shankar et al. (2020), using a finite-Width ReLU netWork
yields poorer performance compared to the kernel (around 83% at best, compared to 87.9%). This
may suggest that for convolutional netWorks, deeper netWorks may have additional advantages When
using optimization algorithms, in terms of adapting to possibly relevant structure of the problem,
such as hierarchical representations (see, e.g., Allen-Zhu & Li (2020); Chen et al. (2020); Poggio
et al. (2017) for theoretical justifications of the benefits of depth in non-kernel regimes).
C Complexity of Spatially Localized Functions
In this section, We briefly elaborate on our discussion in Section A.1 on hoW simple convolutional
structure may improve complexity when target functions are spatially localized. We assume f *(x)=
g*(xu) with Xu = (x[u + v])v∈s ∈ Rp|S| a patch of size |S|, where g* is a Lipschitz function.
If we define the kernel Ku(x, x0) = k(xu, x0u), where k is a dot-product kernel arising from a
one-hidden layer network with positively-homogeneous activation such as the ReLU, and further
assume patches to be bounded and g* to be bounded, then the uniform approximation error bound
of Bach (2017a, Proposition 6) together with a simple O(1/√n) Rademacher complexity bound on
estimation error shows that we may achieve a generalization bound with a rate that only depends on
the patch dimensionp|S| rather thanp∣Ω∣ in this setup (i.e., a sample complexity that is exponential
inp|S|, which is much smaller thanp∣Ω∣).
4We use the PyTorch implementation available at https://github.com/claying/
CKN-Pytorch-image.
18
Published as a conference paper at ICLR 2022
If We consider the kernel K(x, x0) = 5Zu∈ω k(xu, xU), the RKHS contains all functions in the
RKHS of Ku for all U ∈ Ω, with the same norm (this may be seen as an application of Theorem 6
With a feature map given by concatenating the kernel maps of each Ku), so that We may achieve the
same approximation error as above, and thus a similar generalization bound that is not cursed by
dimension. This kernel also allows us to obtain similar generalization guarantees when f * consists of
linear combinations of such spatially localized functions on different patches within the image.
In contrast, when using a similar dot-product kernel on the full signal, corresponding to using a
fully-connected network in a kernel regime, one may construct functions f* (x) = g* (xu) with g*
Lipschitz where an RKHS norm that is exponentially large in the (full) dimension p∣Ω∣ is needed for
a small approximation error (see Bach, 2017a, Appendix D.5).
Related to this, Malach & Shalev-Shwartz (2021) show a separation in the different setting of learning
certain parity functions on the hypercube using gradient methods; their upper bound for convolutional
networks is based on a similar kernel regime as above. We note that kernels that exploit such a
localized structure have also been considered in the context of structured prediction for improved
statistical guarantees (Ciliberto et al., 2019).
D	Extensions to More Layers
In this section, we study the RKHS for convolutional kernels with more than 2 convolutional layers,
by considering the simple example of a 3-layer convolutional kernel K3 defined by the feature map
Ψ(x) = A3M3P3A2M2P2A1M1P1x,
with quadratic kernels at the second and third layer, i.e., k2(z, z0) = (hz, z0i)2 and k3(z, z0) =
(hz, z0i)2. By isomorphism, we may consider the sequence of Hilbert spaces H' to be Hi = H,
H2 = (H 0 H)ls2l×lS2l, and H3 = (H04)(ls3l×ls2l×ls2l)2. For some domain Ω, we define the
operators diag2 : L2(Ω4) → L2(Ω2) and its adjoint diag2 : L2(Ω2) → L2(Ω4) by
diag2(M)[u, v] = M[u, u, v, v]	for M ∈ L2(Ω4)
diag2(M)[ui, U2,U3, U4] = l{uι = U2} l{u3 = U4}M[ui, U3]	for M ∈ L2(Ω2).
We may then describe the RKHS as follows.
Proposition 5 (RKHS of 3-layer CKN with quadratic k2/3). The RKHS of K3 when k2 and k3 are
quadratic kernels (〈•, ∙))2 consists of functions oftheform
f(x) =	Σ	Σ	Gα[u1, u2, u3, u4](xu1, xu2 , xu3 , xu4 ),	(13)
α∈(S3 ×S2×S2 )2 uι ,u2,u3,uv ∈Ω
where Ga ∈ L2(Ω4, H04) obeys the constraint
Gα ∈ Range(Eα),	(14)
where the linear operator Ea : L2(Ω3) → L2(Ω4) for α = (p,q,r,p0,q0,r0) (with p,p0 ∈ S3
and q, r, q0, r0 ∈ S2) is defined by
Eax = A1*,a diag2(A*2,a diag(A3*x)).
The operators A1,a and A2,a denote:
A1,a = LqA1 0 LrA1 0 Lq0A1 0 Lr0 A1
A2,a = LpA2 0Lp0A2.
The squared RKHS norm kf k2HK is then equal to the minimum over decompositions (13) of the
quantity
X kA3* diag(A2*a diag2(Al*aGa))kL2(Ω3).	(15)
a
The constraint (14) and penalty (15) resemble the corresponding constraint/penalty in the two-
layer case for an order-4 polynomial kernel at the second layer, but provide more structure on the
19
Published as a conference paper at ICLR 2022
interactions, using a multi-scale structure that may model interactions between certain pairs of patches
((xu1 , xu2) and (xu3 , xu4) in (13)) more strongly than those between all four patches. In addition to
localizing the interactions Gα around certain diagonals, the kernel also promotes spatial regularities:
assuming that the spatial bandwidths of a` increase with ', the functions (u, v, wι, w2) → Gα[u, U +
w1, u +v, u + v + w2] may vary quickly with w1 or w2 (distances between patches in each of the two
pairs), but should vary more slowly with v (distance between the two pairs) and even more slowly
with u (a global position).
E	Proofs
We recall the following result about reproducing kernel Hilbert spaces, which characterizes the RKHS
of kernels defined by explicit Hilbert space features maps (see, e.g., Saitoh, 1997, §2.1).
Theorem 6 (RKHS from explicit feature map). Let H be some Hilbert space, ψ : X → H a
feature map, and K(x, x0) = hψ(x), ψ(x0)iH a kernel on X. The RKHS H of K consists of
functions f = hg,ψ(∙))H, with norm
kfkH = inf{kg0kH : g ∈ H St f = hg0,Ψ(∙)iH}	(16)
We also state here the generalization bound for kernel ridge regression used in Section 4, adapted
from Bach (2021, Proposition 7.1).
Proposition 7 (Generalization bound for kernel ridge regression). Denote f *(x) = Eρ[y∣x]. As-
sume f * ∈ H, Varρ[y∣x] ≤ σ2, K(x, x) ≤ 1 a.s., and define
1n
fλ := argmin — £3 - f(χi))2 + λkfkH,	(17)
f∈H n i=1
for i.i.d. data (xi, yi)〜P, i = 1,...,n. Let
n ≥ max {5 ,σ⅛ι ,σ⅞⅛Kr}.
For λ = pσ2 EPX [K(x, x)]∕n∣∣f *kH, we have
E[R(fλ) - R(f *)] ≤ Ckf*k%rσ2 EPXB(Xxɪ,	(18)
where C is an absolute constant.
Proof. Under the conditions of the theorem, we may apply (Bach, 2021, Proposition 7.1), which
states that for λ ≤ 1 and n ≥ 5 (1 + log(1∕λ)), we have
E[R(fλ) - R(f *)] ≤ 16σ2 Tr((∑ + λI)-1Σ) + 16λhf*, (Σ + λI)-1Σf*iH + 2∣kf*k∞,
n	n2	∞
where Σ = EPX [K(x, ∙) 0 K(x, ∙)] is the covariance operator. We conclude by using the inequalities
Tr((Σ + λI)-1 Σ) ≤ Tr^ = EPX [K(X,x)]
λλ
hf*, (∑ + λi)-1∑f*iH ≤kf*kH,
and optimizing for λ.	□
E.1 Proof of Proposition 1 (RKHS of One-Layer Convolutional Kernel)
Proof. From Theorem 6, the RKHS contains functions of the form
f(x) = hF,AΦ(x)iL2 (Ω1,H),
with RKHS norm equal to the minimum of ∣∣F∣∣l2(Ωi,h) over such decompositions.
We may alternatively write f (x) =(G, Φ(x)il2(ω,h) with G = A*F. The mapping from F to G is
one-to-one if G ∈ Range(A*). Then, we obtain that equivalently, the RKHS contains functions of
this form, with G ∈ Range(A*), and with RKHS norm equal to the minimum of ∣∣A^*G∣l2(ΩiH)
over such decompositions.	□
20
Published as a conference paper at ICLR 2022
E.2 PROOF OF PROPOSITION 2 (RKHS OF 2-LAYER CKN WITH QUADRATIC k2)
Proof. From Theorem 6, the RKHS contains functions of the form
f(x) = hF,A2M2P2 A1Φ1(x)iL2(Ω2,H2),
with RKHS norm equal to the minimum of ∣∣F∣∣L2(Ω2,H2) over SUch decompositions. Here, Φι(x) ∈
L2(Ω,H) is given by Φι(x)[u]=夕ι(xu), so that Φ(x) in the statement is given by Φ(x) =
Φι(χ) 0 Φι(χ). We also have that H = (HxH)ls2l×ls2l, so that we may write F = (FPq)p,q∈S2
with Fpq ∈ L2(Ω2, H 0 H).
For p, q ∈ S2 , denoting by Lc the translation operator Lcx[u] = x[u - c], we have
(M2P2A1Φ1(x)[u])pq = LpA1Φ1(x)[u] 0 LqA1Φ1(x)[u]
= diag(LpA1Φ1(x) 0 LqA1Φ1(x))[u]
= diag((LpA1 0 LqA1)Φ(x))[u].
Then, we have
hFpq, (A2M2P2A1φ1(X))Pq iL2(Ω2,H0H) = hFPq, A2 diag((LpAI 0 Lq AI)φ(X))i L2 (Ω2 ,H蜜H)
=hAAFpq, diag((LpAI 0 Lq Al)φ(X))iL2(Ωι,He)H)
=hdiag(A2Fpq), (Lpai 0 LqAi)φ(X)iL2(Ωl,HeH)
=h(LpA1 0 Lq AI) diag(A2 Fpq ), φ(x)iL2(Ω2 ,H0H) ∙
We may then write this ashGpq, Φ(χ))L2(Ω2,ΗeΗ) with
Gpq = (LpAι 0 LqAι)* diag(A;FPq),
and the mapping between Fpq ∈ L2(Ω2,H 0 H) and Gpq ∈ L2(Ω2,H 0 H) is one-to-one
if Gpq ∈ Range((LpAI 0 LqAi)*), and diag((LpA1 0 LqAι)^^Gpq) ∈ Range(A2). We may
then equivalently write the RKHS norm as the minimum over Gpq satisfying such constraints for
all p, q ∈ S2 , of the quantity
X kFpq k2 = X ∣A2* diag((LpAi 0 Lq Ai)t*Gpq )kL2(Ω2,H0H)∙
p,q∈S2	p,q∈S2
□
E.3 PROOF OF PROPOSITION 5 (RKHS OF 3 - LAYER CKN WITH QUADRATIC k2/3)
Proof. Let Φ(x)=(夕i(Xu))U ∈ L2(Ω, H), so that we may write
Σ
G[u1,u2,u3,u4](Xu1
Xu2 , Xu3 , Xu4 ) = hG, φ(X)	iL2(Ω4,H馋4),
u1,u2,u3,uv ∈Ω
for some G ∈ L2(Ω4, He4).
From Theorem 6, the RKHS contains functions of the form
f (x) = hF,A3M3P3 A2Φ2(X)iL2(Ω3,H3),	(19)
with RKHS norm equal to the minimum of ∣∣F∣∣L2(Ω3,H3) over such decompositions. Here, Φa(x) ∈
L2 (Ω 1, H2) = L2 (Ω 1, (H0H)|S2 l×lS2|) is given as in the proof of Proposition 2, by
Φ2,qr(X)[u] = diag((LqA1 0 LrA1)(Φ(X) 0 Φ(X)))[u],
for q, r ∈ S2. A patch P3A2Φ2(X)[u] is then given by
P3A2Φ2(X)[u] = (LpA2Φ2,qr(X)[u])p∈S3,q,r∈S2 ∈ (H0H)lS3l×lS2l×lS2l∙
Applying the quadratic feature map given by ^3(z) = Z 0 Z ∈ (H04)(lS3l×lS2l×lS21)2 for Z ∈
(H0H)lS3l×lS2l×lS2l, we obtain for α = (p, q, r,p0, q0, r0) ∈ (S3 X S2 X S2)2,
(M3P3A2Φ2 (X)[u])α = LpA2Φ2,qr (X)[u] 0 Lp0 A2Φ2,q0r0 (X)[u]
= diag(A2,α (Φ2,qr (X) 0 Φ2,q0r0 (X)))[u],
21
Published as a conference paper at ICLR 2022
where
A2,α = LpAz 0 Lp0 A2∙
Now, one can check that we have the following relation:
Φ2,qr(x) 0 Φ2,q0r0 (x) = diag((LqA1 0 LrA1)(Φ(x) 0 Φ(x))) 0 diag((Lq0 A1 0 Lr0 A1)(Φ(x) 0 Φ(x)))
=diag2(A1,αΦ(x 严)，
with
A1,α = LqA1 0 LrA1 0 Lq0A1 0 Lr0A1.
Since H = (Hi»4)(匿31*匿2»匿21)2, We may write F = (Fα)α∈(s3×s2×s2)2, With each Fa ∈
L2(Ω3, H14). Wethenhave
hFa,(A3M3P3A2Φ2(x))a) L2(Ω3)
=hFa, A3 diag(A2,α diag2(A1,aΦ(x严))iL2(Ω3)
=hdiag(A3Fa), A2,a diag2(A1,aΦ(x严)il2(ω2)
=hdiag2(A2,a diag(A3Fa)),Al,aΦ(x产iL2(Ω4)
=hA1,a diag2(A2,a diag(A3Fa)), Φ(x产iL2(Ω4).
We may write this as(Ga, Φ(x)14)l2(ω4,h馋4), with
Ga = A13,a diag2(A32,a diag(A33Fa)).
The mapping from Fa to Ga is bijective if Ga is constrained to the lie in the range of the operator Ea .
If Ga satisfies this constraint, we may write
Fa = a3* diag(A23a diag2(Al3aGa)).
Then, the resulting penalty on Ga is as desired.
□
E.4 Proof of Proposition 3 (generalization for one-layer CKN)
Proof. Note that we have
Ex[K1(x, x)] =	h[u - v]h[u - v - r] Ex[k1(xv, xv-r)]
≤ Xhh, Lrhiσr2
v,r
=∣Ω∣ Xhh, Lrh)σT.
r
It remains to verify that kf 3∣∣Hk = ∙∖∕iΩ^kgkH. Notethatif wedenote G = (g)u∈Ω ∈ L2(Ω, H),
then we have A3G = G, since Pv h[v-u]G[v] = (Pv h[v-u])g = g. This implies that A3*G = G,
regardless of which pooling filter is used. Then we have, by (5) that ∣∣f 3k2 ≤ ∣Ω∣kg∣∣2. Further,
since g is of minimal norm, no other G ∈ L2(Ω, H) may lead to a smaller norm, so that we can
conclude ∣∣f3∣∣2 = ∣Ω∣∣∣g∣∣2.	□
E.5 Proof of Proposition 4 (generalization for two-layer CKN with
QUADRATIC k2)
Proof. We begin by studying the “variance” quantity Ex [K2 (x, x)]. By expanding the construction
of the kernel K2 , we may write
K2(x, x) = Σ Σ h2[u-v]h2[u-v0]×
p,q∈S2 u,v,v0
h1[v-w1]h1[v-w2]h1[v0-w10]h1[v0-w20]k1(xw1-p, xw10 -p)k1(xw2-q, xw20-q).
w1 ,w2
00
w10 ,w20
22
Published as a conference paper at ICLR 2022
Upper bounding the quantity E[k1(xw1-p, xw0 -p)k1(xw2-q, xw0 -q)] by 1 when w1 = w10 and w2
w20 , and by otherwise, the sum of the coefficients in front of 1 can be bounded as follows:
Σ Σ
p,q∈S2 u,v,v0,w1,w2
h2[u-v]h2[u-v0]h1[v-w1]h1[v-w2]h1[v0-w1]h1[v0-w2]
|S2|2	h2[u-v]h2[u-v]hLvh1, Lv0h1i2
u,v,v0 v
|S2|2XhLvh2, Lv0h2ihLvh1, Lv0h1i2
v,v0
∣Ω∣∣S2∣2 Xhh2,Lv h2)hh1,Lv hi)2.
v
while the sum of coefficients bounded by E is upper bounded by ∣Ω∣∣S2∣2. Overall, this yields
Eχ[K2(x,x)] ≤ ∣S2∣2∣Ω∣ (工hh2,Lvh2ihh1,Lvhi)2 + E
(20)
The bounds obtained for the example function f *(x) = EuV g(χu,χv) rely on plugging in
the values of the Dirac filter h[u] = δ(u = 0) or average pooling filters h[u] = 1∕∣Ω∣ in
the expression of Eχ[K2(χ, x)], and on computing the norm ∣∣f*||此 for different architectures
using Eq. (7) in Proposition 2. Computing Ex [K2 (x, x)] is immediate using the expression
above. Bounding ∣∣f *||/ is more involved, and requires finding appropriate decompositions of
the form f *(x) = Pp q Pu V Gpq[u, v](χu, XV) in order to leverage Proposition 2:
•	When using a Dirac filter at the first layer, we need ∣S2∣ = ∣Ω∣ in order to capture log-
range interaction terms, and we represent f * as a sum of Gp that are non-zero and equal
tog only on the p-th diagonal, i.e., Gp [u, v] =g when v = u + p, and zero otherwise.
We then verify Pp Pu,v Gp[u, v](xu, xv) = PpPug	(xu,xu+p) = f*(x). Then, the
expression (7) on this decomposition yields ∣S2∣∣Ω∣kgkH蜜H = ∣Ω∣2∣gkH0H, for any choice
of pooling h2 such that ∣h2 ∣i = 1 (using similar arguments to the proof of Proposition 3).
This is then equal to the squared norm of f * due to the minimality of ∣∣g∣H0H.
•	When using average pooling at the first layer and ∣S2∣ = ∣Ω∣, we may use a single
term G[u, v] with all entries equal tog, i.e., a decomposition f* (x) = Pu,v G[u, v](xu, xv).
Using (7), we obtain an upper bound ∣Ω∣∣g∣2 on the squarednorm. The same decomposition
can be used when |S2| = 1, leading to the same bound.
□
F	Generalization Gains under Specific Data Models
In this section, we consider simple models of architectures and data distribution where we may
quantify more precisely the improvements in sample complexity guarantees thanks to pooling.
We consider architectures with non-overlapping patches, and a data distribution where the patches are
independent and uniformly distributed on the sphere Sd-i sphere in d := p|Si | dimensions. Further,
we consider a dot-product kernel on patches of the form ki (z, z0) = κ(hz, z0)) for z, z0 ∈ Sd-i, with
the common normalization κ(1) = 1.
In the construction of Section 2, using non-overlapping patches corresponds to taking a downsampling
factor si = |Si | at the first layer, and a corresponding pooling filter such that hi[u] = 0 for u 6= 0
mod |Si|. Alternatively, we may more simply denote patches by Xu with U ∈ Ω by considering a
modified signal with more channels (xu = x[u] of dimension pe instead ofp, where e is the patch size)
so that extracting patches of size 1 actually corresponds to a patch of size e of the underlying signal.
We then have that the patches Xu in a signal X are i.i.d., uniformly distributed on the sphere Sd-i.
We denote the uniform measure on Sd-i by dτ.
23
Published as a conference paper at ICLR 2022
We note that when patches are in high dimension, overlapping patches may become near-orthogonal,
which could allow extensions of our arguments below to the case with overlap, yet this may require
different tools similar to Mei et al. (2021). We leave these questions to future work.
F.1 QUANTIFYING THE TRACE OF THE COVARIANCE OPERATOR E[K(x, x)]
In this section, we focus on the “variance” term E[K(x, x)] which is used in the generalization results
of Section 4.
One layer. In the one-layer case, we clearly have σ02 := E[κ(hxu, xui)] = κ(1) = 1. For u 6= v,
since patches xu and xv are independent and i.i.d., σu2-v is a constant independent of u, v, which
may be computed by integration on the sphere as:
σu-v = E[κ( hxu , xv i )] = Exu 〜τ[EXv 〜TIK(hxu,xv i ) |xu]]
Z1
κ(t)(1 - t2)-3ddt,
1
(21)
where we used a standard change of variable t = hxu, xvi when integrating xv over Sd-1 (see,
e.g., Efthimiou & Frye, 2014), with ωp-ι = 2πp/2/Γ(p∕2) the surface measure of the sphere Sp-1.
Note that the integral in (21) corresponds to the constant component in the Legendre decomposition
of κ, which is known for common kernels as consider in various works studying spectral properties
of dot-product kernels (Bach, 2017a; Minh et al., 2006). For instance, for the exponential kernel
(or Gaussian on the sphere) κ(hx, y〉) = e -⅛- = eσ12(hx,yi-1), which is used in most of our
experiments with σ = 0.6, we have (Minh et al., 2006, Theorem 2):
ωd-2
ωd-i
Z-11 κ(t)(1
-t2)d-3dt = e-02 (2σ2)3-2"2Id∕2-i(1∕σ2)Γ(d∕2),
where I denotes the modified Bessel function of the first kind. For arc-cosine kernels, it may be
obtained by leveraging the random feature expansion of the kernel (Bach, 2017a). More generally,
we also note that when the patch dimension d is large, we have
ωd-2
ωd-1
Z-11 κ(t)(1
d-3
-t2)Fdt → κ(0),
as d → ∞,
since ωωd-2(1 一 t2)d-3 is a probability density that converges weakly to a Dirac mass at 0. In
particular, for the exponential kernel with σ = 0.6, we have κ(0) = e-1∕σ2 ≈
0.06. When learning
a translation-invariant function, the bound in Prop. 3 then shows that global average pooling yields an
improvement w.r.t. no pooling of order | Ω | / (1+0.061Ω |). Note that removing the constant component
of κ, i.e., using the kernel K(1)-K(0), may further improve this bound, leading to a denominator very
close to 1 when d is large, and hence an improvement in sample complexity of order ∣Ω∣. We also
remark that the dependence on κ(0) may be removed by using a finer generalization analysis beyond
uniform convergence that leverages spectral properties of the kernel (see Section F.2).
Two layers with quadratic k2 . For the two-layer case, we may obtain expressions
of E[kι(χu, XuO )kι(χv, XvO)] as above. Denote E := Ez/，〜τ[kι(z, z0)] where z, z0 are independent,
which is given in (21). We may have the following cases:
•	If u = u0 and v = v0, we have, trivially, EIk1(xu, xuO)k1 (xv, xvO)] = 1.
•	Ifu = u0 and v 6= v0, we have EIk1(xu, xuO)k1 (xv, xvO)] = EIk1 (xv, xvO)] = E, since xv and xvO
are independent. The same holds if u 6= u0 and v = v0 .
•	If |{u, v, u0, v0}| = 4, we have
EIk1(xu, xuO)k1(xv, xvO)] = EIk1(xu,xuO)] EIk1(xv, xvO)] = E2.
•	If u = v and |{u, u0, v0}| = 3, then we have
EIk1 (xu , xuO )k1 (xv , xvO )] = Exu IExuO Ik1 (xu , xuO )|xu] ExOv Ik1 (xu , xvO )|xu]] = E ,
by using Exu0 [kι(xu, Xu，)|xu] = Ez,z，〜T[kι(z, z0)], which holds by rotational invariance.
24
Published as a conference paper at ICLR 2022
•	If U = v0 and u0 = v, We have E[kι(xu, Xu0)kι(xv, Xv0)] = Ezn〜τ[kι(z, z0)2]=：匚 This takes
the same form as (21), but with a different kernel function κ2 instead of κ. Note that in the case
of the Exponential kernel, κ2 is also an exponential kernel With different bandWidth.
Overall, when E and e are small compared to 1, we can see that the quantity E[kι (xu, Xuθ)kι(χv, Xv，)]
is small compared to 1 unless u = u0 and v = v0, thus satisfying the assumptions in Prop. 4. As
described above, we may obtain expressions of E and e in various cases, and in particular these vanish
in high dimension when using a kernel with κ(0) = 0.
F.2 Fast rates
In this section, we derive spectral decompositions of 1-layer CKN architectures with non-overlapping
patches under the product of spheres distribution described in the previous section. This allows
us to derive fast rates that depend on the complexity of the target functions on patches, and shows
similar improvement factors to those derived in Section F.1, without the κ(0) term, which in fact turns
out to only be due to a single eigenspace, namely constant functions. We note that our derivation
extends (Favero et al., 2021) to the case of generic pooling filters, and considers a different data
distribution.
Before studying the 1-layer case, we remark that while it may seem natural to extend such decom-
positions to the 2-layer case using tensor products of spherical harmonics, as done by Scetbon &
Harchaoui (2020) in the case without pooling, it appears that pooling may make it more challenging
to find an eigenbasis since subspaces consisting of tensor products of spherical harmonics with fixed
total degree are no longer left stable by the kernel.5 We thus leave such a study to future work.
We begin by considering the following Mercer decomposition of the patch kernel
∞	N(d,k)
kι(z,z0) = κ(hz,z0i) = Xμk X Yk,j(Z)Ykj(Z0),
k=0	j=1
where Yk,j for k ≥ 0 and j = 1, . . . , N(d, k) are spherical harmonic polynomials of degree k
forming an orthonormal basis of L2(dτ). The μk here are Legendre/Gegenbauer coefficients of the
function κ (see, e.g., Bach, 2017a; Smola et al., 2001).
Note that the 1-layer kernel may be written as
Kh(x,y) = E h ~ h[u - v]κ(hxu,yv))，
u,v∈Ω
where ~ denotes circular convolution and h[u] := h[-u]. We denote by 巨W [u] = exp(2iπwu∕∣Ω∣),
W = 0,..., ∣Ω∣- 1, the DFT basis vectors, and by ew = βw/ a∕∣Ω∣ the normalized DFT basis vectors,
which satisfy(£仅,ew = Pu ew [u]eW [u] = 1, where z* is the complex conjugate of z. Define the
Fourier coefficients
h[w] = hh,≡w i = ɪ2h[u]e-2iπwu/^1,
u
2
and let λw := h ~ h[w] = |h[w]|2. Note that when the filter is normalized s.t. khk1 = 1, we
have λo = h[0] = 1. We will also use the Parseval identity ∣∣hk2 = (Pw λw)∕∣Ω∣. Using the inverse
DFT, it holds
_	ι ∣ω∣-i	ι ∣ω∣-i	∣ω∣-i
h ~ h[u - v]	= 77ΓΓ λ λ λw Gw [u	- u]	= 77ΓΓ λ λ λw Gw [u] Gw Iv]	=):	λw ew [u] ew Iv] ∙
1 1 w=0	1 1 w=0	w=0
Then, we have
∣Ω∣-1	N(d,k) /	∖ (	∖
Kh (x, y) =	XX λwμk	X	X ew [u]Yk,j (Xu)	X ew [u]Yk,j (yu).
w=0 k≥0	j=1	u	u
5For instance, the term k1 (xw, yu)k1 (xw, yv), which may only appear in the presence of pooling, maps the
polynomial Yk(Xu)Yk(Xv) of degree 2k to a polynomial μkYk(Xw)2 which is not necessarily orthogonal to all
spherical harmonics tensor products of degree smaller than 2k.
25
Published as a conference paper at ICLR 2022
Note that when k = 0, we have Y0,1(xu) = 1 for all u, hence
(XXew[u]Y0,ι(Xu)) (XXew[u]Y0,i(yU)) =(XXew[u])(XXew[v]) = |a|i{w = 0},
since e0 = ∣Ω∣-"(1,..., 1) and Pu ew[u] = 0 forw > 0. Then, we may write
∣Ω∣-1	N (d,k)
Kh(χ,y) = ∣Ω∣λ0μ0φ0(x)Φo(y) + E ∑λw μk E φw,k,j (x)φw,k,j (y),
w=0 k≥1	j=1
with φo(χ) = 1, φw,k,j(x) = Pu ew [u]Yk,j(xu), and φ* denotes the complex conjugate of φ.
It is then easy to check that the φo and φw,k,j form an orthonormal basis of L2(dτ 01ωi). Wethus have
obtained a Mercer decomposition of the kernel Kh w.r.t. the data distribution, so that its eigenvalues
are also the eigenvalues of the covariance operator (Caponnetto & De Vito, 2007), and control
generalization performance, typically through the degrees of freedom
N(λ) = Tr((∑ + λI)-1Σ) = X λ∣÷，
λ + ξm
m≥0
where Σ is the covariance operator, and (ξm)m is the collection of its eigenvalues. In particular, ifwe
have a decay ξm N m-α (with a < 1), then we have N(λ) ≤ O(λ-^α), which then leads to a fast
rate of n-a/(a+1) on the excess risk when optimizing for λ in kernel ridge regression (Bach, 2021;
Caponnetto & De Vito, 2007). In our case, the degrees of freedom takes the form
Nh(λ) = λ⅞μ0	+ X1 X N(d, k)!⅛μ^	(22)
λ + M|10仙0	w=0 W λ + λwμk
We make a few remarks:
•	Given that the number of spatial frequencies w is fixed, the asymptotic decay rate of eigen-
values associated to the φw,k,j (that is, λwμk, each with multiplicity N(d, k)) is the same as
that of the eigenvalues associated to φw0 ,k,j for some fixed w0, which in turn corresponds
to the decay for the corresponding dot-product kernel on the sphere. For instance, if κ is
the arc-cosine kernel, we have ξm N m-a with α = d-, and more generally α = d-ɪ
for a kernel resembling a Sobolev space with s bounded derivatives. This then leads to
a rate n2s/(2s+(d-1)), which only depends on the dimension d of patches, rather than the
full dimension d∣Ω∣. One may also add more general assumption on the smoothness of the
localized components of f * (such as g in Proposition 3) in order to get rates that depend
explicitly on the order of smoothness s of such components (as in Caponnetto & De Vito,
2007).
•	The eigenvalue associated to φ0 plays a minor role as by itself as it only contributes at most
τρ2 /n to the excess risk, which is negligible compared to the rest of the eigenvalues which lead
to a slower n-a/(a+1) rate.
•	With no pooling (h is aDirac delta), we have λw = ∣h[w]∣2 = 1 for all w. We then have
Nh(λ ≤ 1 + ∣Ω∣NK(λ),
where we definedNK(X) := Pk≥.1 N(d, k)μk/(λ + μk).
•	With global pooling (h = 1∕∣Ω∣ is constant), we have λ0 = 1, and λw = 0 for w > 0. This
yields
Nh(λ) ≤ 1 +Nκ(λ).
(23)
This then yields an improvement by a factor ∣Ω∣ in sample complexity guarantees compared
to the scenario above with no pooling, namely the dominant term in the excess risk bound will
be C(1/n)a/(a+1) compared to C(|C|/n)a/(a+1).
26
Published as a conference paper at ICLR 2022
•	For more general pooling, one may exploit specific decays of λw to obtain finer bounds. We
may also obtain the following bound by Jensen’s inequality
Nh(λ) ≤ 1 + ∣Ω∣ X N(d,k)
k≥1	λ+λμ
≤ 1 + ∣Ω∣NK
(研)
where we used that X = (Pw λw)∕∣Ω∣ = ∣∣h∣∣22, by Parseval's identity. When NK(λ) ≤
Cκλ-Vα, we get a bound
Nh(λ) ≤ 1 + Cκ∣Ω∣∣∣h∣^αλTα.	(24)
instead of a bound N (λ) ≤ 1 + C ∣Ω∣λ一^α for the case of no pooling, that is, the improvement
is again controlled by IIhIl2, which goes from 1∕∣Ω∣ for global pooling, to 1 for no pooling.
The following result provides an example of a generalization bound for invariant target functions,
which illustrates that there is no curse of dimensionality in the rate if the patch dimension is much
smaller than the full dimension (i.e., d《 d∣Ω∣), as well as the benefits of pooling.
Theorem 8 (Fast rates for one-layer CKN on invariant targets). Consider an invariant target of the
form f * (x) = Eu∈ω g* (Xu), with Ez〜dτ [g* (z)] = 0, and assume:
•	(capacity condition) NK(λ) ≤ Cκλ-1/α,
•	(source condition) g* = Tκrg0 and Ig0 IL2 (dτ) ≤ C*, where Tκ is the integral operator of
the kernel K on L2(dτ), with r > α-α1.
Then, kernel ridge regression with the one-layer CKN kernel Kh with pooling filter h (with IhI1 = 1)
satisfies, for n large enough,
2αr
E[R(fn)] - R(f*) ≤ C∣Ω∣ k≡2^!	+
(25)
where C is independent of ∣Ω∣ and h. For global pooling, the factor Ilhk2/a = |C|-1/a can be
improved to ∣Ω∣-1. In COntrast, with no pooling we have Ilhk2/a = 1, i.e., n needs to be ∣Ω∣ times
larger for the same guarantee. Note that for α → 1 and r = 1∕2, the resulting bound resembles that
of Proposition 3.
We note that if g* is assumed to be S-smooth on the sphere, the source condition with 2ar = d-sɪ
corresponds to a Sobolev condition of order s, and leads to the bound
/	、	2s
E[R(fn)] - R(f *) ≤ C∣Ω∣ (khn^) +
(26)
which highlights that the rate only depends on the patch dimension d instead ofthefull dimension d| Ω |.
Proof. Under the conditions of the theorem, we may apply (Bach, 2021, Proposition 7.2), which
states that for λ ≤ 1 and n ≥ 5 (1 + log(1∕λ)), we have
τ2	24
E[R(fλ)] - R(f*) ≤ 16上Nh(λ) + 16Ah(λ,f*) + F∣f*∣∞,	(27)
n	n2	∞
where the degrees of freedom Nh(λ) is given in (22) and satisfies the upper bound (24), and the
approximation error is given by
Ah(λ,f*) = fmHκ kf - f*kL2(dτ@叫 + λkfkHκh,
where HKh is the RKHS of Kh . Denote by
f = a0φ0 +	aw,k,jφw,k,j,	f* = a0*φ0 +	a*w,k,j φw,k,j
w,k,j	w,k,j
27
Published as a conference paper at ICLR 2022
the decompositions of f and f * in the orthonormal basis defined above. If g* = Ek j gkjYkj is the
spherical harmonic decomposition of g*, then We have
a0* = E[f* (x)] = 0
a*,kj = E[f * (x)φo,k,j (x)] = gkj X e*[u] = PMgkj
u
a*w,k,j = 0 for w 6= 0.
This yields
Ah(λ, f *) = min (a0 - aO)2 + λ ICliO------------
a0,a0,k,j	|叫人040
N(d,k)
+ XX (a0,k,j	- a0*,k,j )2 + λ
k≥1 j=1
a0,kj
λoμk
N(d,k)	b2
min X X 的(bkj-gkj )2 + λl°l Yj
bk,j k≥ι M	λoμk
N(d,k)	b2
∣ω∣ minX X (bkj-gkj)2 +λj
bk,j I、C - -i	μk
k≥0j =1
∣ω∣ min kg - g*kL2(dτ) + λkgkH = |C|AK(X,g*)
g∈H
where the second line uses a* = 0 and considers bkj∙ = ao,kj/ a∕∣Ω∣^, while the third line uses go,1 =
0 and the fact that λ0 = 1 regardless of the choice of pooling filter h. Thus, Ah(λ, f*) does not
depend on h, and corresponds to the approximation error Aκ(λ, g*) of the patch kernel κ on the
sphere (UP to a factor ∣Ω∣). Under the source condition, we then have, by (Cucker & Smale, 2002,
Theorem 3, p.33),
Ah(λ,f*) = ∣Ω∣Aκ(λ,g*) ≤∣Ω∣C2λ2r.
with ao*,k,j = ao*. Combining with (24) and plugging this into (27), we obtain
八	T2	T2Cκ∣Ω∣khk2∕αλ-Va	r k	k f * k2
E[R(fλ)] - R(f *) .上 + P .............  +	∣Ω∣C2λ2r + f∞.
n	n	*	n2
For the choice
α
λ =(TPCκ∣Ω∣khk”! E
n =1	rα∣Ω∣C2n )	,
we have
2αr
ʌ	9	1	(CKT2|Q|khk2/a\ 2ar+1	T2	kf*k2
E[R(fλ)] - R(f *) . (∣Ω∣C2)E ............ + JP + fL
n	n	n2
2αr
= (CK玲½C*号∣ω∣(3! K + % + rɪ.
P	n	n	n2
In the case of global pooling, the term in parentheses scales as (∣Ω∣-1∕α∕n) 2αα+1, but can be
2αr
improved to (1∕∣Ω∣n) 2αr+1 by using the bound (23) on Nh(λ) instead of (24).
When r > O-I, We can choose n large enough so that n ≥ /(1 + log(1∕λn)) is satisfied, and the
higher order terms in n are negligible.	□
28