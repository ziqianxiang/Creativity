Published as a conference paper at ICLR 2022
The Spectral Bias of Polynomial Neural Net-
WORKS
Moulik Choraria
University of Illinois at Urbana-Champaign
moulikc2@illinois.edu
Leello Dadi
EPFL, Switzerland
leello.dadi@epfl.ch
Grigorios G Chrysos
EPFL, Switzerland
grigorios.chrysos@epfl.ch
Julien Mairal
Univ. Grenoble-Alpes, Inria
julien.mairal@inria.fr
Volkan Cevher
EPFL, Switzerland
volkan.cevher@epfl.ch
Ab stract
Polynomial neural networks (PNNs) have been recently shown to be particularly
effective at image generation and face recognition, where high-frequency informa-
tion is critical. Previous studies have revealed that neural networks demonstrate
a spectral bias towards low-frequency functions, which yields faster learning of
low-frequency components during training. Inspired by such studies, we conduct
a spectral analysis of the Neural Tangent Kernel (NTK) of PNNs. We find that the
Π-Net family, i.e., a recently proposed parametrization of PNNs, speeds up the
learning of the higher frequencies. We verify the theoretical bias through exten-
sive experiments. We expect our analysis to provide novel insights into designing
architectures and learning frameworks by incorporating multiplicative interactions
via polynomials.
1	Introduction
Deep neural networks (DNNs) have demonstrated remarkable success in different domains [22, 11].
DNNs can approximate complex functions or even datasets with randomized labels arbitrarily well
[50, 3], which makes their ability to avoid over-fitting on real data surprising, since it seems to
disagree with prior notions of model complexity. This has sparked the interest in investigating
the notion of “implicit bias” in neural network training, which makes them favor low complexity
solutions [43, 21, 26].
The spectral analysis of deep networks offers one perspective on this implicit bias. Deep neural
networks demonstrate a learning bias towards low frequency functions - i.e. functions that vary
globally without local fluctuations are learned faster when training neural networks via gradient
descent [37, 48]. The phenomenon, termed as the spectral bias of neural networks [37], has been
explored from the perspective of the Neural Tangent Kernel (NTK) [24]. The eigenvalues of the
obtained kernel influence important characteristics such as the approximation properties and rate of
learning [10, 39, 36]. For standard two-layer ReLU networks within the NTK regime, the analysis
supports the idea of a spectral bias by showing faster error convergence for information in lower
frequencies [12].
Recently, another class of models, Polynomial Neural Networks (PNNs), that express high order
polynomial expansions, have demonstrated state-of-the-art performance in the challenging tasks of
image generation [28] and face recognition [17]. Both tasks rely on fine-grained details, which
correspond to high-frequency information. More generally, multiplicative interactions have demon-
strated strong empirical performance on image-based applications [44, 47, 4]. Jayakumar et al. [25]
highlight how multiplicative interactions, which construct second degree polynomials, can enlarge
the hypothesis space and lead to faster learning for certain classes of functions.
1
Published as a conference paper at ICLR 2022
To understand this success of PNNs, we conduct a spectral analysis, inspired by the analysis of
DNNs. We focus on one instance of polynomial networks called Π-Nets [16], where the output is
a piece-wise polynomial function of the input obtained via multiplicative layers. The parameters of
a Π-Net can be represented as high-order tensors, while polynomial expansions can offer increased
representation power [17]. Our main contributions can be summarized as follows:
1.	We analyze two-layer polynomial networks in the NTK regime. By studying the spectral proper-
ties of the corresponding kernel, we prove a theoretical speed-up in learning higher frequencies
over standard neural networks and validate the hypothesis in the approximate NTK regime, on
the task of learning spherical harmonics.
2.	Beyond the NTK regime, we demonstrate this enhanced bias of Π-Nets towards higher frequen-
cies in several experimental settings, beginning from synthetic learning tasks and then proceeding
to state-of-art networks and inverse problems with 2D images.
Aside from improving the understanding of polynomials in neural networks, our proposed analysis
also sheds new light on the effect of multiplicative interactions in neural networks, prevalent in
certain domains of machine learning including vision and natural language processing [29, 7].
2	Related Work
Spectral Bias: Motivated by the empirical observations in [3, 37, 48, 46] that deep networks first
learn “simple patterns”, several papers [49, 12, 8, 2] have conducted theoretical analyses to explain
this bias towards lower frequencies. The work of [2], aiming to understand why random labels take
longer to learn than natural labels, showed that alignment of the labels with the eigenvectors of the
NTK Gram matrix determines the learning speed. Extending this result, Cao et al. [12] provide
an explanation for the spectral bias by analyzing the decay rate of eigenvalues of the NTK when
the input data is uniformly distributed on the sphere. Under the same assumption, Basri et al. [8]
study training dynamics in the NTK setting for 2-layer ReLU networks with a fixed outer layer and
an explicitely included linear bias term in the ReLU. [9] further extends this work to account for
non-uniform data distributions. All these findings show that DNNs learn lower frequency functions
faster which prompted Tancik et al. [45] to propose methods to mitigate this bias. Our paper aims to
establish similar results for PNNs, to explain their good performance at learning higher frequencies.
Polynomial neural networks (PNNs): The early papers that explore polynomials in the context of
neural networks are mainly divided into two categories: 1) self-organizing networks with hard coded
features [23], 2) Pi-Sigma networks [41, 34]. In both cases, the constructions did not scale well to
higher dimensional inputs, and were not used for high-dimensional signals, such as images. More
recent papers have used the Hadamard product to capture correlations between different branches
of an architecture [4, 44, 47, 17]. Our goal is to analyze the properties of these polynomial neural
networks that have shown sucess in practice.
Polynomial activation functions (PAFs): It is important to note the distinction between PNNs and
Polynomial activation functions. PAFs expand (element-wise) each feature to an rth degree, i.e.,
they assume a (deep) neural network where the element-wise activation functions are rth degree
polynomials. This is substantially different from capturing higher-order correlations across input
(or feature) elements like PNNs especially in the presence of non-linear activations. Theoretical
work on over-parametrization [18], expressive power [30] and generalization of shallow nets [35]
have emerged for PAFs. The aforementioned papers, however, do not conduct a spectral analysis
and do not exhibit the benefits of PNNs for learning high-frequency information.
3	Analysis of Polynomial Neural Networks in the NTK regime
In this section, we conduct a careful analysis of the kernel approximation of polynomial neural
networks (PNNs) to gain insight on the effect of the multiplicative interactions. PNNs include mul-
tiplicative interactions and express high-degree polynomial expansions. The recent parametrization
of the Π-Net family [16], which we summarize below, is used as a representative PNN. We re-
view the tangent kernel approximation of neural networks and then we derive the tangent kernel of
two-layer Π-Nets to study the spectral bias of Π-Nets.
Notation: We denote by〈•，•〉the standard inner-product on Rd. For two vectors x, y P Rd, X * y
denotes the element-wise or Hadamard product. We use Xm to denote the mode-m vector product1.
1The reader may refer to the appendix for more details on the mode-m product.
2
Published as a conference paper at ICLR 2022
We define the asymptotic notations Ω(∙) and Ω(∙) as follows: Let an and bn be two sequences. We
write an “ Ωpbn if liminf『g ∣an∕bn∣ > 0. We use Ω(∙) to hide the logarithmic factors in Ω(∙).
3.1	Π-NET FORMULATION
A polynomial expansion of the input vector z P Rδ can be used to express the output x P Ro as an
Nth degree polynomial expansion as follows:
x
N	n`1
“ ∑ (Wrns ∏ ^jZ)
n“1	j“2
` β,
(1)
where β P Ro and { Wrns P RoXnm“1 Xmd(nL are the learnable parameters, and Xm is the mode-
m vector product. Since the number of tensor parameters W rns grow exponentially with the degree
of the polynomial, a coupled tensor decomposition with factor sharing can be used. The idea in
Π-Nets is to propose such decompositions that can capture higher order correlations, and can be
implemented in standard deep learning frameworks. One such decomposition uses the recursive
formulation Xn “ (ATnsz) * (STnlxn´i + Biinsbrns) for n = 1,...,N and expresses the out-
put X as x “ CxN + β. The term in the rightmost parenthesis is exactly the recursive form of
a standard neural network (without activations). Therefore, Π-Nets augment standard neural net-
work with multiplicative interactions via the Hadamard product. While Π-Nets can approximate the
target function without activations, they achieve state-of-art performance with activation functions,
wherein the output is a piece-wise polynomial. We include more details in the Appendix.
3.2	The Neural Tangent Kernel
Consider the following two-layer ReLU neural network (without bias parameters) with width m
that assumes the following form (defined as in [10]): fw(x) “ ʌ/m^W2σ(W1x), where Wi P
Rmx(d+1), W2 p R1χm and we assume inputs {x}i“i follow some distribution T on the unit
sphere Sd P Rd'1; σ(∙) denotes the element-wise ReLU operator. As the width of the network m
goes to infinity, if the weights at initialization Wp0) are independent and each follow the standard
normal distribution, the inner product of the network gradient at initialization gives rise to a limiting
kernel, namely the Neural Tangent Kernel (NTK) [24] κ defined as :
κ(x, x1)“ lim XVwfwpoq pxq, Vwfwpoq (x1)〉.	(2)
m→8
This kernel has been used to characterize the behavior of sufficiently wide networks fW during
training. For instance, if the network is trained to minimize the `2 loss, then its training dynamics
closely track those of kernel regression under κ. This holds in a particular training regime, referred
to as “lazy training” [14], where the parameters hardly vary after initialization and the network can
be approximated by its first-order Taylor expansion at initialization as:
fw(x) « fwpoq (x) + XVwfwpoq (x), W ´ Wp0qy.
In practice however, the conditions of lazy training are often violated (notably, the requirement
that the weights do not move) within the first few steps of gradient descent. Nevertheless, the NTK
remains a useful theoretical tool for analyzing the neural network behavior as some of its predictions
have been shown to hold in practice [12, 32].
The NTK for the two-layer ReLU network fw (x) takes the following form [10, 14, 19]
κ(x, x1) “ 2χx, x1〉Ki(x, x1) + 2κ2(x, x1),
where the kernels κ1 and κ2 are defined as follows:
Ki(x, x1) “ Ew„N(0,i)[σ1(Xw, xy)σ1 (〈w, x1〉)],
κ2(x, x1) “ Ew„n(o,i)[σ(Xw, xy)σ(Xw, x1〉)],
(3)
(4)
where σ(∙) denotes the ReLU operator and σ1(∙) denotes the indicator function 1[∙20]. The kernels
κ1, κ2 have been explicitly computed for the ReLU activation in [38, 15, 10] where they were shown
to be positive semi-definite dot-product kernels. The function value depends only on the value
of the dot-product of the arguments or more formally, κ1(x, x1) “ g1(Xx, x1〉) and κ2(x, x1) “
g2(Xx, xιy) for some functions g1,g2 : R → R. These kernels will serve as building blocks for the
tangent kernel of the Π-Net architecture studied in the next section.
3
Published as a conference paper at ICLR 2022
3.3	Π-KERNEL
Following [8, 10, 12] that consider shallow two-layer ReLU networks, we derive the tangent kernel
for a two-layer Π-Net by supplementing the standard network with a multiplicative interaction layer:
fw(x) “ ∖ — W3G(W2x) * σ(Wιx)S,
m
(5)
where Wι, W2 P Rmχpd'1q, W3 P R1'm, * denotes the Hadamard product and We again assume
inputs {x}n“i that follow some distribution T on the unit sphere Sd U Rd'1.
Remark 1 Note that formulation yields a piece-wise quadratic polynomial and that it is important to
consider ReLU activations (or other non-linearities) since otherwise, the network yields a quadratic
polynomial of the input and is no longer a universal approximator, even with infinite width.
Theorem 1 Let κ1 , κ2 as defined in Eq. (4). The limiting kernel for the two-layer Π-Net, called
Π-kernel and denoted by κπ px, x1q, takes the following form:
κπ px, x1q “ 2p2xx, x1 yκ1 px, x1q ` κ2px, x1qqκ2px, x1q.
(6)
The proof follows the standard NTK calculations and is presented in the Appendix. Importantly, the
addition of a single multiplicative interaction induces a product of kernels form.
Remark 2 The required width that ensures that the two-layer Π-Net stays close to initialization
(the corresponding NTK is close to the limiting Kn) is slightly higher than standard networks, by a
C(?log mq factor. We provide a rough sketch of the proof in the Appendix (end of C).
Remark 3 The advantage of using the 2-layer Π-Net formulation is that it allows us to directly
contrast against prior results on standard 2-layer feed-forward network, by gauging the effect of the
extra multiplicative layer. However, unlike feed-forward networks, the theory does not easily extend
to polynomials of higher degree or depth, since even with a fixed degree and depth, the NTK varies
with the placement of the multiplicative connections. In this work, we choose to focus primarily on
the effect of the multiplicative layer and leave the extension to general polynomials for future work.
Since the Π-kernel κπ can be expressed as a sum of products of continuous positive definite dot-
product kernels κ1 and κ2 , it inherits their regularity properties and is itself a Mercer kernel, as a
consequence of the Schur Product theorem.
3.4	Spectral analysis
To properly characterize the approximation properties and the behavior during training of this newly
derived kernel, we study its Mercer decomposition. Indeed, this approach was used in [10] to show
that the 2-layer NTK (Eq. (3)) has better approximation properties than a fixed first layer network,
and in [12] to study the spectral bias of feed forward ReLU networks.
The Mercer decomposition of a kernel κ is derived from the eigenvalues and eigenfunction of the
integral operator associated to the kernel (see [40] Theorem 2.10 and references therein). Taking X
to be some compact set in Rd, recall that for any continuous kernel function K : X X X → R and
Borel measure τ on X, we can define an integral operator Lκ that “convolves” any square integrable
function f P Lτ2 (Xq with κ:
Lκ(fq(xq “
X
κ(x, yqf(yqdτ(yq.
(7)
For a Mercer kernel, this linear operator admits countable, real, non-negative eigenvalues
tμ1,μ2,... U and the associated eigenfunctions form an orthonormal basis of LT(X). If the data
is uniform on the sphere Sd, and κ is a dot-product kernel, then these eigenfunctions of Lκ are the
spherical harmonics ([42], Lemma 4). Consequently, by applying Mercer’s Theorem, we can obtain
the following decomposition:
8	N pd,kq
κ(x, x1) = £ μk £ Ykj (X)Ykj (x1),	(8)
k“0	j“1
where Yk,j for j = 1, 2...N (d, k) represent the spherical harmonics of degree k in d ` 1 variables
(whose explicit formula is given in Appendix D.1 in [5]), and N(d, k) := 2%―1 (k'´´2).
4
Published as a conference paper at ICLR 2022
N pd,kq a2
akj <8
From this, we can characterize the RKHS H associated to κ as follows:
N pd,kq
H “ f f “	∑	∑ ak,jYk,jp∙q subject to kfkΗ “	£
k50,μk‰0 j“1	k50,μk ‰0 j = 1 μk
The benefit of determining the decay rate of the eigenvalues {μι, μ2,...} can be easily deduced
from the previous set as the eigenvalues determine the size of H. Indeed, the slower the decay of
{μ1,μ2,... },themoresequences (ak,j)k,j will verify £中/工 ‰0 XN=Pd,kq * <8. The results of
[12] rely on this decay rate to show that gradient descent on wide networks picks up low-frequency
information first2 (Theorem 4.2, [12]). This decay rate is therefore of central importance. Next, we
briefly refer to the prior results on characterizing this decay rate for the kernel obtained for the two-
layer feed forward ReLU network (Eq. (3)) and we derive the equivalent results for the Π-kernel.
Proposition 1 (Theorem 4.3 in Cao et al. [12], Proposition 5 in Bietti & Mairal [10]) For the
tangent kernel corresponding to two-lαyerfeedforwardReLU network, the eigenvalues (μk)k satisfy
the following:
,μo,μι “ Ω(iq,
μ μk = 0, when k is odd,	(9)
pk = θ(k´d´1 q when k " d.
The decay is exponentially fast in the input dimension d. For the Π-kernel, we show the following
improvement.
Theorem 2 Let {μ∏,1,μ∏,2,...} denote the eigenvalues of the linear operator Lκπ associated to
the kernel κ∏. For k " d (k ‰ 2 mod 4), it holds that μ∏,k “ θ(k´d/´2).
The proof of the theorem is provided in the Appendix. The main idea is to plug in the Mercer decom-
position of each kernel, leading to a product of polynomials form. The expansion of this product
then allows for isolating the dominant terms contributing to the eigenvalues for each frequency.
The key take-away is the much slower rate of decay the kth eigenvalue (an order almost Ω(kpd{2q))
when compared to the standard two-layer NTK. An immediate consequence is a “larger” RKHS,
which lends the Π-kernel superior approximation properties in the higher frequencies. Further, when
combined with the findings of [12], it yields a speed-up in learning higher frequency harmonics.
Remark 4 On the point of a “larger” RKHS, we note the two perspectives at play. First is the idea
that a slower decay implies more functions contained in the RKHS, leading to better approximation
properties. However, from the classical statistical learning point of view, while a larger RKHS makes
the optimization problem easier, it may lead to a sub-optimal prediction performance [6], eventually
relating to the traditional trade-off in machine learning. We note through our experiments that it is
the perspective relating to better approximation and optimization for the Π-kernel that dominates.
Remark 5 The k " d setting may not reflect the case for image-based applications. For instance,
generation tasks wherein the output is an image of resolution d X d, the higher frequencies of interest
roughly correspond to the order of Ω(d). Consequently, the exponential improvement in the decay
rate for the k " d setting, and by extension in the approximation properties of the RKHS as well as
the speed of learning higher frequency information, may not be as dramatic elsewhere. Nevertheless,
our analysis provides sufficient intuition to expect some degree of improvement in realistic settings.
4	Numerical evidence
The analysis in Section 3 reveals that polynomial networks in the NTK regime learn higher fre-
quency information faster. In practice however, neural networks deviate from the near-initialization
NTK conditions within just a few iterations of gradient descent. Therefore, to verify the analysis on
the spectral bias of Π-Nets, we conduct a series of experiments that increasingly deviate from the
NTK regime, including image-based datasets to further verify our theoretical analysis. We initially
consider synthetic data (Section 4.1), then move onto realistic settings. For all experiments, we use
Π-Nets based on the product of polynomials formulation (Appendix), in the same vein as [16].
2We offer a brief characterization of the result in the Appendix.
5
Published as a conference paper at ICLR 2022
4.1	Experiments on synthetic data
Our first experiment relates to learning spherical harmonics in the approximately infinite width NTK
regime [12]. More precisely, the task comprises of learning linear combinations of spherical harmon-
ics with data uniformly distributed on the unit sphere. We consider large-width two-layer networks
to simulate the infinite-width settings and we compare the performance of Π-Nets against standard
neural networks. The optimization method is full batch vanilla gradient descent, to avoid stochastic
effects. The observations align with our theoretical findings, we defer the results to Appendix due
to space constraints. Next, we assess whether the spectral bias manifests beyond the NTK regime.
We consider the task of learning sinusoidal signals with smaller width networks and larger learning
rates, so as to expressly violate the NTK assumptions.
Comparison on Learning Sinusoids
standard-net	∏-net
OOOOInOooo寸 OoOOm Ooooz OOOOI
Suoqeal-
5	10 15 20 25 30 35 40 45 50	5	10 15 20 25 30 35 40 45 50
Frequency [Hz]	Frequency [Hz]
Figure 1: Comparison of learning speeds across different frequencies. The target signal. i.e., sinu-
soids, is transformed in the Fourier domain and the learned components are compared to the true
amplitudes. On the colormap scale, 1 denotes the perfect approximation. We observe that the Π-Net
(right) learns higher frequencies faster, i.e., lower in the y axis, than the standard network (left).
Learning Sinusoids The goal is to learn sinusoidal signals [37]. Given frequencies ki P K, with
amplitudes Ai P α and phases φi P Φ, the target map f * : [0,1] → R is defined as f *(x) “
Ai sin(2πkix' φi). We approximate this map with two methods: i) a fully connected neural network
and ii) a Π-Net. In the first setting, we compare a 256-unit wide, six-layer deep neural network
against a 256-unit wide, six-layer deep Π-Net (which has five multiplicative layers). In each case,
the network fθ (parametrized by θ) regresses over f* (K = p5, 10, ..., 45, 50), φi „ Up0, 2π) and
Ai “ 1 @ i), using N “ 200 evenly spaced input samples over r0, 1s and with a fixed learning
rate (same for both networks); the spectrum of the network fθ pk) is monitored during training by
tracking the magnitude of learned components for each frequency, averaged over 5 runs (Fig. 1).
Remark 6 The experimental setup for training the networks replicates the setup of Rahaman et al.
[37], without any special initialization or hyper-parameter tuning for either network and the Π-Net
is implemented exactly as the product of polynomials formulation specified in the Appendix.
We observe that Π-Nets do speed up training of higher frequencies. To better substantiate our claim,
we consider two additional variants, the results of which are included in the Appendix. In the first,
we compare a deeper nine-layer feedforward network with a six-layer Π-Net with only three multi-
plicative layers, such that the feedforward network includes more parameters (Fig. 10). Since skip
connections (additive) are known to speed-up training, we next compare the Π-Net with a feedfor-
ward network of same depth, but with additive skip connections instead of multiplicative (Fig. 11).
In both cases, we verify that the speed-up in learning higher frequencies due to multiplicative layers
is much more significant.
Discussion: In the task of learning sinusoids, multiplicative interactions can speed up the learning
of higher frequencies and are more effective at doing so than simply increasing the depth. In the
Appendix, we conduct an experiment to evaluate the robustness of the networks in retaining high
frequency information (Fig. 12) and we see that Π-Nets are more robust to perturbations in the higher
frequencies. Remarkably, the Π-Net with more interactions is noticeably more robust than the lower
degree polynomial network (Fig. 13 in the appendix). We hypothesize that Π-Nets enhance the
6
Published as a conference paper at ICLR 2022
representational space for higher frequencies, relating to our observations on the RKHS in the NTK
regime, which makes them less susceptible than standard neural networks. This could also explain
why more multiplicative interactions lead to more robustness.
4.2	Learning Images
To further validate our claim in natural images, we adopt the convolutional layers often used in deep
convolutional neural networks (DCNNs) [31]. DCNNs are ubiquitous in vision, partly due to the
DCNN structure imposing a suitable prior for tasks with natural images, termed as the Deep Image
Prior (DIP) [33]. We precisely assess how this prior changes with multiplicative interactions in
Π-Nets in a denoising setup adapted from the DIP framework.
Experiment - Denoising: We consider an image X P R3χHχw and obtain its noisy version x0 “
X ' δ, perturbed with Gaussian noise. For the input, We sample a random tensor Z P RNXHXW
(N = 32 in our setup). We consider a neural network fθ (making the parametrization by θ explicit),
and optimize the reconstruction loss, min6 ∣∣fθ(Z) 一 χo∣∣2, with respect to the noisy image. We can
expect the DCNN structure to first learn the (“natural”) features corresponding to the true image X
and pick up the noise only in the latter stages, which can then be avoided using early-stopping [33].
Remark 7 Our goal is not to quantify the denoising performance but rather, to validate the experi-
mental observation on sinusoids and assess whether Π-Nets can speed up learning of high frequen-
cies in real-world applications. If the speed up is confirmed, Π-Nets can be early-stopped, i.e.,
require less iterations for achieving the target result.
PSNR with Noisy Image (XO)
Denoising
PSNR with True Image (x)
0	500 IOOO 1500	2000	2500	0	500	1000	1500	2000	2500
Iterations	Iterations
Figure 2: The plots track the progress of the denoising task in the DIP setup via measuring the PSNR
w.r.t. the noisy image (left) and the true image (right). We ideally want to stop the optimization
process when the PSNR w.r.t the true image is maximum. We observe that for Π-Nets, the maximum
PSNR point occurs much earlier, beyond which the PSNR w.r.t the true image starts to decrease as
the network begins to learn the noise. This indicates that the Π-Net shows a reduced impedance
towards high frequency information, compared to standard networks.
As in [37], we experiment with the U-net architecture, adapting it suitably for Π-Nets with an image
of resolution 480 x 600. The implementation details are included in the Appendix. We train both
networks for 2500 iterations, with the same learning rate and identical input tensor Z. We monitor the
Peak Signal-to-Noise Ratio (PSNR) curves while training, in Fig. 2. The peak-PSNR with the true
image for both standard and Π-Nets are roughly the same, indicating similar denoising performance.
However, Π-net achieves this peak PSNR in approximately half the number of iterations. Beyond
this point, the Π-net PSNR with the true image decreases as it starts to pick up the high-frequency
noise. We confirm this in the visual snapshot of the output of the two networks (Fig. 16 in the
appendix). We conclude that Π-Nets do indeed speed up learning in images, by showing a reduced
impedance towards high-frequency information. Next, we check how this bias affects learning high
frequency information pertaining to natural images in the absence of noise.
Experiment - Power Spectrum Analysis: We consider the identical setup as the denoising experi-
ment but without the noise perturbation, setting δ “ 0. Therefore, the task for the network fθ learn
θ* such that fθ* (z) “ x. We allow 600 iterations of gradient descent and we monitor the radial
power spectral density (p.s.d.) of the network output image against the ground truth p.s.d. It allows
us to explicitly track the learning progress for each the frequency magnitude.
7
Published as a conference paper at ICLR 2022
Figure 3: We compare the power spectral density curves at different checkpoints during 600 itera-
tions of optimization, for standard (scale = 3) vs Π networks (scale = 2). The goal for each network
is to match the power spectral density of the target image. We observe that the Π-Net picks up
high-frequency information in the image faster.
We use the same U-net structure and suitably modify it for Π-Nets as before. Since the multiplicative
interactions introduce more parameters, we reduce the depth/scale (by scale, we mean number of
down/up-sampling operations in the U-net) of Π-Net to ensure roughly the same parameters („1.3
million). The learning speeds in different frequencies can be observed in Fig. 3. In the Appendix,
we repeat the same experiment for equal depth networks.
Discussion: From the preceding two experiments, we establish how Π-Nets pick up high frequency
information faster than (larger) DCNNs. In the Appendix, we note the effect of multiplicative inter-
actions on the deep image prior of the network. We also verify that this altered spectral bias remains
relevant for standard learning tasks with natural images. Consequently, for tasks where capturing
the finer details of the image is important (such as the denoising instance above), this potentially
leads to a reduced number of iterations for Π-Nets as compared to standard networks. We expect
the insights from our work to be useful for guiding network design with multiplicative interactions
in large-scale experiments.
4.3	Effect of Label Noise in Classification
Finally, we conduct an experiment in a classification setting, wherein the goal is to quantify the
effect of this spectral bias in presence of label noise. The setup is adapted from Rahaman et al. [37],
a fully connected 6-layer deep 256-unit wide network is trained on a binary classification on MNIST
images (by only considering classes “3” and “8”). The labels are perturbed by label noise of different
frequencies and the network is trained on the noisy labels with mean squared loss. Rahaman et al.
[37] noted for feedforward networks that for a fixed amplitude, low frequency label noise degrades
generalization performance (difference between training and validation losses) to a larger extent
than high frequency noise. While the low frequency noise is learned instantly, the high-frequency
noise is only fit later in the training. As a result, the network learns only true labels at first, and
this corresponds to a “dip” in the true validation loss in the early stages. This “dip” becomes larger
8
Published as a conference paper at ICLR 2022
with increasing frequency of noise, indicating network impedance towards higher frequencies in the
early iterations. Thus, it is only during the latter stages that the true validation loss degrades.
We repeat the experiment with Π-Nets, by supplementing the feedforward network with exactly one
multiplicative layer, and contrast the two networks. Since Π-Nets pick up high frequency variations
faster, we consequently expect a smaller “dip”. We train for 5000 iterations with identical learning
rates and observe the validation loss curves for different label noise frequencies. In Fig. 4, we zoom
in on the first 1000 iterations for better visualization (complete curves included in the Appendix).
Discussion: While the performance for the two networks is identical in absence of label noise
(freq=0), the validation “dip” in Π-Net for higher frequencies (0.3, 0.5, 1) is visibly smaller and is
negligible for lower frequencies (0.1, 0.2), indicating that Π-Nets pick up the high frequency label
noise in the decision boundaries much faster. Additionally, we verify that increasing the number of
multiplicative layers reduces the “dip” even further. The plots are deferred to the Appendix.
O 200	400	600	800 IOOO
Training Iteration
O 200	400	600	800 IOOO
Training Iteration
Figure 4: We compare the validation loss curves for the first 1000 iterations on the binary classifica-
tion task, for Π-Net (left) and standard network (right). For the same frequency, the validation dips
for Π-Net are much smaller, indicating a higher tendency to pick up high-frequency label noise.
5	Discussion
In this work, we focus on the spectral of polynomial neural networks. Our theoretical results in the
NTK regime utilize a two-layer polynomial network and demonstrate a speed-up in the learning of
higher frequencies over standard neural networks. We experimentally verify these properties, even
outside of the NTK training regime. Additionally, the results offer intuition behind the success of
networks that use multiplicative interactions, such as StyleGAN [28], whose connections to polyno-
mials have been noted previously [17].
As a future direction, we aim to design controlled settings to study how polynomial and multi-
plicative interactions affect performance in state-of-the-art conditional generative models for image
generation or deblurring, where high-frequency information is critical for photo-realistic outcomes.
It should be noted that the spectral bias of standard neural networks towards low frequency (com-
plexity) functions is believed to help in generalization. Therefore, another important direction is
to explore whether this enhanced spectral bias of polynomials towards higher complexity functions
translates to differences in their generalization properties. Finally, our results in the classification
task yield a further research direction towards analyzing the smoothness of decision boundaries of
Π-Nets with multiplicative interactions, especially focusing on areas wherein this effect becomes
relevant such as adversarial susceptibility or knowledge distillation.
Acknowledgments
We are thankful to the anonymous reviewers for their constructive feedback. Research was spon-
sored by the Army Research Office and was accomplished under Grant Number W911NF-19-1-
0404. This project has received funding from the European Research Council (ERC) under the
9
Published as a conference paper at ICLR 2022
European Union’s Horizon 2020 research and innovation programme (grant agreement n° 725594 -
time-data). JM was supported by the ERC grant number 714381 (SOLARIS project) and by ANR
3IA MIAI@Grenoble Alpes, (ANR19-P3IA-0003).
References
[1]	Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via
over-parameterization. CoRR, abs/1811.03962, 2018. URL http://arxiv.org/abs/
1811.03962.
[2]	Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In Interna-
tional Conference on Machine Learning, pp. 322-332. PMLR, 2019.
[3]	Devansh Arpit, StanislaW Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio,
Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Si-
mon Lacoste-Julien. A closer look at memorization in deep netWorks. arXiv:1706.05394 [cs,
stat], July 2017. URL http://arxiv.org/abs/1706.05394. arXiv: 1706.05394.
[4]	Francesca Babiloni, Ioannis Marras, Filippos Kokkinos, Jiankang Deng, Grigorios Chrysos,
and Stefanos Zafeiriou. Poly-nl: Linear complexity non-local layers With polynomials. In
International Conference on Computer Vision (ICCV), 2021.
[5]	Francis Bach. Breaking the Curse of Dimensionality With Convex Neural NetWorks.
arXiv:1412.8690 [cs, math, stat], October 2016. URL http://arxiv.org/abs/1412.
8690. arXiv: 1412.8690.
[6]	Francis R. Bach. Sharp analysis of loW-rank kernel matrix approximations.	CoRR,
abs/1208.2015, 2012. URL http://arxiv.org/abs/1208.2015.
[7]	Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural Machine Translation by
Jointly Learning to Align and Translate. arXiv:1409.0473 [cs, stat], May 2016. URL http:
//arxiv.org/abs/1409.0473. arXiv: 1409.0473.
[8]	Ronen Basri, David W. Jacobs, Yoni Kasten, and Shira Kritchman. The convergence rate of
neural netWorks for learned functions of different frequencies. CoRR, abs/1906.00425, 2019.
URL http://arxiv.org/abs/1906.00425.
[9]	Ronen Basri, Meirav Galun, Amnon Geifman, David W. Jacobs, Yoni Kasten, and Shira
Kritchman. Frequency bias in neural netWorks for input of non-uniform density. CoRR,
abs/2003.04560, 2020. URL https://arxiv.org/abs/2003.04560.
[10]	Alberto Bietti and Julien Mairal. On the Inductive Bias of Neural Tangent Kernels.
arXiv:1905.12173 [cs, stat], October 2019. URL http://arxiv.org/abs/1905.
12173. arXiv: 1905.12173.
[11]	Tom B. BroWn, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
Wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-
Wal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, ReWon Child, Aditya Ramesh,
Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,
Mateusz LitWin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan-
dlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are FeW-Shot Learn-
ers. arXiv:2005.14165 [cs], July 2020. URL http://arxiv.org/abs/2005.14165.
arXiv: 2005.14165.
[12]	Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu. ToWards Understand-
ing the Spectral Bias of Deep Learning. arXiv:1912.01198 [cs, stat], October 2020. URL
http://arxiv.org/abs/1912.01198. arXiv: 1912.01198.
[13]	L. Carlitz. The product of tWo ultraspherical polynomials. Glas-
gow Mathematical Journal,	5(2):76-79, July 1961. ISSN 2051-2104,
2040-6185. doi:	10.1017/S204061850003433X. URL https://www.
10
Published as a conference paper at ICLR 2022
cambridge.org/core/journals/glasgow-mathematical-journal/
article/product-of-two-ultraspherical-polynomials/
BF734D19E2D88047AFD196033D20766A. Publisher: Cambridge University Press.
[14]	Lenaic Chizat, Edouard Oyallon, and Francis Bach. On Lazy Training in Differentiable Pro-
gramming. arXiv:1812.07956 [cs, math], January 2020. URL http://arxiv.org/abs/
1812.07956. arXiv: 1812.07956.
[15]	Youngmin Cho and Lawrence Saul. Kernel Methods for Deep Learning. In
Advances in Neural Information Processing Systems, volume 22. Curran Asso-
ciates, Inc., 2009. URL https://papers.nips.cc/paper/2009/hash/
5751ec3e9a4feab575962e78e006250d-Abstract.html.
[16]	Grigorios G. Chrysos, Stylianos Moschoglou, Giorgos Bouritsas, Yannis Panagakis, Jiankang
Deng, and Stefanos Zafeiriou. P-nets: Deep Polynomial Neural Networks. pp. 7325-
7335, 2020. URL https://openaccess.thecvf.com/content_CVPR_2020/
html/Chrysos_P-nets_Deep_Polynomial_Neural_Networks_CVPR_2020_
paper.html.
[17]	Grigorios G. Chrysos, Stylianos Moschoglou, Giorgos Bouritsas, Jiankang Deng, Yannis Pana-
gakis, and Stefanos P Zafeiriou. Deep Polynomial Neural Networks. pp. 1-1, 2021. ISSN
1939-3539. doi: 10.1109/TPAMI.2021.3058891. Conference Name: IEEE Transactions on
Pattern Analysis and Machine Intelligence.
[18]	Simon Du and Jason Lee. On the power of over-parametrization in neural networks with
quadratic activation. In International Conference on Machine Learning (ICML), pp. 1329-
1338, 2018.
[19]	Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient Descent Provably Op-
timizes Over-parameterized Neural Networks. arXiv:1810.02054 [cs, math, stat], February
2019. URL http://arxiv.org/abs/1810.02054. arXiv: 1810.02054.
[20]	Christopher Frye and Costas J Efthimiou. Spherical harmonics in p dimensions. arXiv preprint
arXiv:1205.3548, 2012.
[21]	Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit
Bias of Gradient Descent on Linear Convolutional Networks. In Advances in
Neural Information Processing Systems, volume 31. Curran Associates, Inc.,
2018.	URL https://proceedings.neurips.cc/paper/2018/hash/
0e98aeeb54acf612b9eb4e48a269814c-Abstract.html.
[22]	Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, Hy-
oukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen. GPipe: Efficient
Training of Giant Neural Networks using Pipeline Parallelism. arXiv:1811.06965 [cs], July
2019. URL http://arxiv.org/abs/1811.06965. arXiv: 1811.06965.
[23]	A. G. Ivakhnenko. Polynomial Theory of Complex Systems. IEEE Transactions on Sys-
tems, Man, and Cybernetics, SMC-1(4):364-378, October 1971. ISSN 2168-2909. doi:
10.1109/TSMC.1971.4308320. Conference Name: IEEE Transactions on Systems, Man, and
Cybernetics.
[24]	Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural Tangent Kernel: Convergence and
Generalization in Neural Networks. arXiv:1806.07572 [cs, math, stat], February 2020. URL
http://arxiv.org/abs/1806.07572. arXiv: 1806.07572.
[25]	Siddhant M. Jayakumar, Wojciech M. Czarnecki, Jacob Menick, Jonathan Schwarz, Jack Rae,
Simon Osindero, Yee Whye Teh, Tim Harley, and Razvan Pascanu. Multiplicative interactions
and where to find them. In International Conference on Learning Representations (ICLR),
2020.
[26]	Ziwei Ji and Matus Telgarsky. The implicit bias of gradient descent on nonseparable data.
In Proceedings of the Thirty-Second Conference on Learning Theory, pp. 1772-1798. PMLR,
June 2019. URL https://proceedings.mlr.press/v99/ji19a.html. ISSN:
2640-3498.
11
Published as a conference paper at ICLR 2022
[27]	Ziwei Ji, Justin D. Li, and Matus Telgarsky. Early-stopped neural networks are consistent.
2021. arXiv:2106.05932 [cs.LG].
[28]	Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In Conference on Computer Vision and Pattern Recognition (CVPR),
2019.
[29]	Tero Karras, Samuli Laine, and Timo Aila. A Style-Based Generator Architecture for Gen-
erative Adversarial Networks. arXiv:1812.04948 [cs, stat], March 2019. URL http:
//arxiv.org/abs/1812.04948. arXiv: 1812.04948.
[30]	Joe Kileel, Matthew Trager, and Joan Bruna. On the expressive power of deep polynomial
neural networks. Advances in neural information processing Systems (NeurIPS), 32:10310-
10319, 2019.
[31]	Yann LeCun and Yoshua Bengio. Convolutional networks for images, speech, and time series.
In The handbook of brain theory and neural networks, pp. 255-258. MIT Press, Cambridge,
MA, USA, October 1998. ISBN 978-0-262-51102-5.
[32]	Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. Advances in neural information processing systems, 32:8572-8583,
2019.
[33]	Victor Lempitsky, Andrea Vedaldi, and Dmitry Ulyanov. Deep Image Prior. In 2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 9446-9454, June 2018. doi:
10.1109/CVPR.2018.00984. ISSN: 2575-7075.
[34]	Chien-Kuo Li. A Sigma-Pi-Sigma Neural Network (SPSNN). Neural Processing Letters, 17
(1):1-19, February 2003. ISSN 1573-773X. doi: 10.1023/A:1022967523886. URL https:
//doi.org/10.1023/A:1022967523886.
[35]	Stefano Sarao Mannelli, Eric Vanden-Eijnden, and Lenka Zdeborova. Optimization and gen-
eralization of shallow neural networks with quadratic activation functions. arXiv preprint
arXiv:2006.15459, 2020.
[36]	Atsushi Nitanda and Taiji Suzuki. Optimal rates for averaged stochastic gradient descent un-
der neural tangent kernel regime. In International Conference on Learning Representations
(ICLR), 2021.
[37]	Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred A. Ham-
precht, Yoshua Bengio, and Aaron Courville. On the Spectral Bias of Neural Networks.
arXiv:1806.08734 [cs, stat], May 2019. URL http://arxiv.org/abs/1806.08734.
[38]	Nicolas Le Roux and Yoshua Bengio. Continuous neural networks. In Marina Meila and Xi-
aotong Shen (eds.), Proceedings of the Eleventh International Conference on Artificial Intel-
ligence and Statistics, volume 2 of Proceedings of Machine Learning Research, pp. 404-411,
San Juan, Puerto Rico, 21-24 Mar 2007. PMLR. URL https://proceedings.mlr.
press/v2/leroux07a.html.
[39]	Meyer Scetbon and Zaid Harchaoui. A spectral analysis of dot-product kernels, 2021.
[40]	Bernhard Scholkopf, Alexander J Smola, Francis Bach, et al. Learning with kernels: SuPPort
vector machines, regularization, optimization, and beyond. MIT press, 2002.
[41]	Y. Shin and J. Ghosh. The pi-sigma network: an efficient higher-order neural network for
pattern classification and function approximation. In IJCNN-91-Seattle International Joint
Conference on Neural Networks, volume i, pp. 13-18 vol.1, July 1991. doi: 10.1109/IJCNN.
1991.155142.
[42]	Alex Smola, Zoltan OVari, and Robert C Williamson. Regularization with dot-product kernels.
In T. Leen, T. Dietterich, and V. Tresp (eds.), Advances in Neural Information Processing
Systems, volume 13. MIT Press, 2001. URL https://proceedings.neurips.cc/
paper/2000/file/d25414405eb37dae1c14b18d6a2cac34-Paper.pdf.
12
Published as a conference paper at ICLR 2022
[43]	Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The
Implicit Bias of Gradient Descent on Separable Data. arXiv:1710.10345 [cs, stat], December
2018. URL http://arxiv.org/abs/1710.10345. arXiv: 1710.10345.
[44]	RUPesh KUmar Srivastava, Klaus Greff, and Jurgen Schmidhuber. Highway Networks.
arXiv:1505.00387 [cs], November 2015. URL http://arxiv.org/abs/1505.00387.
arXiv: 1505.00387.
[45]	Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Ragha-
van, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier fea-
tures let networks learn high frequency functions in low dimensional domains. In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances
in Neural Information Processing Systems, volume 33, pp. 7537-7547. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
55053683268957697aa39fba6f231c68-Paper.pdf.
[46]	Guillermo Valle-Perez, Chico Q. Camargo, and Ard A. Louis. Deep learning generalizes be-
cause the parameter-function map is biased towards simple functions. In International Con-
ference on Learning Representations, 2019. URL https://openreview.net/forum?
id=rye4g3AqFm.
[47]	Yan Wang, Lingxi Xie, Chenxi Liu, Siyuan Qiao, Ya Zhang, Wenjun Zhang, Qi Tian, and
Alan Yuille. Sort: Second-order response transform for visual recognition. In International
Conference on Computer Vision (ICCV), pp. 1359-1368, 2017.
[48]	Zhi-Qin John Xu, Yaoyu Zhang, Tao Luo, Yanyang Xiao, and Zheng Ma. Frequency principle:
Fourier analysis sheds light on deep neural networks. CoRR, abs/1901.06523, 2019. URL
http://arxiv.org/abs/1901.06523.
[49]	Greg Yang and Hadi Salman. A fine-grained spectral perspective on neural networks. arXiv
preprint arXiv:1907.10599, 2019.
[50]	Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understand-
ing deep learning requires rethinking generalization. arXiv:1611.03530 [cs], February 2017.
URL http://arxiv.org/abs/1611.03530. arXiv: 1611.03530.
13
Published as a conference paper at ICLR 2022
Appendix
A A PRIMER ON POLYNOMIAL NEURAL NETWORKS (Π-NETS)
Polynomial neural networks (Π-Nets) [16] were recently introduced with the output being a high
degree polynomial expansion of the input. The parameters of Π-Nets can be represented as higher
order tensors. These networks are able to serve as function approximators even without the use of
non-linear activations. When used in conjunction with activations, the models demonstrate better
expressivity and achieve state-of-art results on a variety of learning tasks. We next describe the
construction of Π-Nets.
A.1 Method
Mode-m vector product: Consider an Mth order tensor X, with each of its element addressed by
M indices, i.e., pXqi1,i2,...,iM “ xi1,i2,...,iM . An M th -order real-valued tensor X is defined over
the tensor space Ri X I2 X ... X IM , where Im P Z for m = 1, 2,....M. The mode-m unfolding
of a tensor X P RI1 χI2χ…XIM maps X to a matrix Xpmq P RImXIm With Im “ ∏Mf1 i‰m Ii.
such that the tensor element Xi1 ,i2 ,...,iM is mapped to the matrix element Xim ,j where j “ 1 `
∑MS1,k‰m(ik — 1)Jk, where Jk “ ∏ML1,n‰m In. The mode-m vector product of X with a vector
U P RIm, denoted by X Xm U P RI1 x I2 x …XImTXIm十1…∙IM , results in a tensor of order M — 1:
(X Xm u)iι,…,im´i
∣im' 1 ,...,iM
Im
xi1,i2,...,iMuim.
im “1
Furthermore, we denote X ^ 1 uι ^2 u2.….XM UM “ X ∏ml1 XmUm.
Π-Net learns a function G : Rd → Ro, such that each element of the output Xj can be expressed as
a polynomial of all the input elements zi, with i P r1, ds as follows:
xj “ G(zqj “ βj ` wjr1s z ` zT Wjr2s z`
N	(10)
Wj3s ^1 Z ^2 Z ^3 Z '-----' WjNs 口 ^nZ,
n“1
where βj P R and { Wjns P Rnm=I Xmd(N“i are parameters for approximating the output Xj. The
correlations (of the input elements zi) up to Nth order emerge in Eq. (10). More compactly:
N	n`1
X = G(Z)= X Wrns 口 Xjz) + β,
n“1	j “2
(11)
where β P Ro and { Wrns P Ro×∏m=ι χmd(N“1 are the learnable parameters. This form allows us
to approximate any smooth function as per an extension of the Weierstrass Theorem. To prevent an
exponential number of parameters, the authors propose using coupled tensor decompositions.
A.2 Tensor Decomposition for Single Polynomial
An appropriate tensor decomposition on the parameters in Eq. (11) allows for implementation with
a neural network. Here, we briefly describe one such decomposition:
Model: NCP: Next, we consider a joint hierarchical decomposition on the polynomial parameters.
A Nested coupled CP decomposition (NCP) results in the following recursive relationship for Nth
order approximation:
xn = (ATnsz) * (STnsxn´l + BTnsbrn]),	(12)
14
Published as a conference paper at ICLR 2022
(ATnsz) * (BTnsbrns)
for n “ 2, . . . , N with x1 “
and x “ CxN ` β. The parameters C P
RoXk, Arns P Rdxk, Srns P RkXk, Brns p Rωxk, b[ns p Rω for n = 1,...,N, are learnable.
brns P Rω (nN“1 act as a scaling factor for each parameter tensor, whose role is illustrated in case of
the third order approximation in Eq. (13):
Figure 5: Schematic illustration of the NCP [16].
Gpz) “ β ' Wr1s ^2 b[is ^3 z ' Wr2s ^2 b[2s ^3 z ^4 z ' Wr3s ^2 bps ^3 z ^4 z ^5 z. (13)
Further, the joint factorization of the parameter tensors (in matrix form) for the third order NCP
polynomial may be summarized as follows:
• First order parameters : Wpr11qs “ C pAr3s d Br3s )T.
Second order parametes: Wpr12qs
“C
Ar2s d
• Third order parameters: Wpr13qs
“ C Ar3s d
Ar1s d Br1s
A.3 Product of Polynomials
The second scheme of implementation approximates the target function using a product of poly-
nomials form, wherein the output of first polynomial is fed to the next and so on. The concept is
visually depicted in 6; for instance, if each polynomial is degree two, then stacking N such polyno-
mials results in an overall order of 2N .
Remark: In practice, each matrix operation in the recursive formulation of Π-Nets represents an
affine transform on a vector. Therefore, the implementation in standard libraries may be as simple as
using a convolutional ora fully-connected layer. The other difference arises due to the multiplicative
layers, which may be seen as special ’skip’ connections which are combined with the network output
via the element-wise vector product (Hadamard product), as opposed to addition. Therefore, it is
easy to see why these networks scale as well as standard deep networks.
15
Published as a conference paper at ICLR 2022
Figure 6: Schematic illustration of the product of polynomials model [16].
B Eigenvalues determine the speed of learning
As noted previously, the integral operator with respect to kernel function is defined as follows:
Lκpfqpxq “
X
κpx, yqfpyqdτpyq.
(14)
The key idea in [12] is to establish guarantees on the speed of convergence of gradient descent along
different directions of LK, as defined by its eigenfunctions. Consider {λi }i21 with λ1 ≥ λ2 ≥ ..
be the strictly positive eigenvalues of LK with {φi }i≥1 the respective eigenfunctions and define
Vi “ n-1(φ(x1 q, φ(x2q......φ(xnqq. Since the eigenvalues may not be distinct, define rk as the sum
of multiplicities of the first k distinct eigenvalues of LK. Define Vrk “ pv1..., vrk q. By definition,
ViiWrk are rescaled restrictions of orthonormal functions on the training examples. Therefore, they
form a set of almost orthonormal bases in the vector space Rn .
Finally, denote y = (y1,y2..…yn)t and yptq = PfWptq (xι),..…fwptq (Xn))t as the ground truth
and predictions at time t, respectively, where fW ptq denotes the 2-layer ReLU network. Then the
following result holds:
Theorem 4.2 [12]: Suppose ∣φj(x)| ≤ M for j P [rkS and X P Sd'1. For any 3 δ ≥ 0
and integer k, if n ≥ Ω(l2. max{(λrk — λrk+1 )´2, M4r^}),m ≥ Ω(poly(T,'´1e´1)), then
with probability at least 1 — δ, gradient descent with step size η “ O(m´1) satisfies:
n-1/2 . llVrTPy ´ yptq川2 ≤ 2 . (1 ― λrk)τn-1/2 . ||VrTy||2'3	(15)
which is to say that the convergence rate of ∣∣ VT (y — yptq )∣∣2, or alternatively the projection of the
residual error on the space spanned by the first rk eigenvalues is controlled by the rkth eigenvalue
λrk. With some additional work, this result can be extended to the 2-layer Π-Netby accounting for
the extra width factor of Ω(ʌ/logm), as noted previously.
The key takeaway in that with a wide enough network and large enough sample size, gradient
descent first learns the target function along the eigen-directions with larger eigenvalues. Since
the decay of eigenvalues is slower for the 2-layer Π-Net, it leads to a speed-up in learning higher
frequencies.
C PROOF OF THEOREM 1: DERIVING THE Π-KERNEL
In this section, we prove our first main result for the Π-kernel corresponding to theorem 1. Consider
a two-layer Π-Net fW parametrized as follows :
fw(X) = m2 W W3[σ(W2x) * σ(Wιx)S,	(16)
where the weights W1 PRmXd, W2 P RmXd, W3 P R1xm
are initialized with independent identi-
cally distributed N(0, 1) coordinates.
16
Published as a conference paper at ICLR 2022
Recall that the neural tangent kernel κπ corresponds to the limit of the following inner product
κ∏(x, x1 q “ lim XVwfw(0)(x), Vwfw(o)(x1)〉.	(17)
m→8
We can compute the inner-product Eq. (17) by computing the derivatives with respect to W1, W2,
W3 of fw separately and sum up the inner products to obtain κπ , since the gradient can be split
into three blocks.
We denote by α :“ Wιx, and by β :“ W2x the pre-activation vectors; and by a, β the post-
activation vectors where the element-wise activation σ is applied to α and β respectively.
First consider the derivative w.r.t W1 denoted BW1 :
∏2~ m	.〜
Bwι fw(x) “ ∖ — y W3σ(βi(x))σ1(Gi(x))Bw1 Gi(x).	(18)
m
i“1
Since <⅛ “ eTWιx “〈Wi, e%xτ〉，where ei P Rm is the i-th canonical basis vector of Rm, we
have that
BWI ai(x) = eix .
The contribution to the NTK (Eq. (17)) corresponds to 〈BW1fW(xq, BW1 fw(x1q〉. Note that since
the network is symmetric in tW1, W2u, we need only look at W1 to obtain the contribution for
W2 as well. We find that
2m
<Bwιfw(x), Bwifw(x1qy “ 一 W W3W3[σ(βi(x))σ(βj(x1))S[σ1 (山(多))。1居(x1))SxeixT, ejXT〉
m
i,j“1
2m
“ 一£ W3W3[σ(βi(x))σ(βj(x1qqsrσ1 (α,i(xqqσ(αj(x1qqsxτx1 的
m
i,j“1
2m
“ 一£ W3W3rσ(βi(x))σ(βi(x1))Srσ1 (αi(x))σ1(c¾(x1 ))]阿％].
m
i“1
(19)
AS lim m → 8, the above quantity converges to its expectation by the law of large numbers because
the terms are independent and identically distributed with finite expectation. Consequently,
lim〈Bwifw(xq, Bwifw(x1)〉“ 2〈x, x1)κι(x, x1)κ2(x, x1)	(20)
m→8
since
$Ewi rσ1(c⅛i(x))σ1(c⅛i(x1))S = κι(x, x1)
EW2 rσ(βi(xqqσ(βi(x1qqs	“ κ2(x,x1q
%EW3 rW3i Wi3s	“ 1.
By symmetry, we obtain with a similar term for W2 and their total contribution to Eq. (17) adds up
to twice the term obtained for W1 . Characterizing the kernel w.r.t W3 is more straightforward:
Bw3 fw(x) “ ∖ — σ(α(x)) * σ(β(x)).
m
Therefore, its contribution to the NTK is
2m
xBw3fw(x), Bw3fw(x )〉“ mEiσ(α(x))σ(βi(x))σ(ai(x ))σ(βi(x )).
i
(21)
(22)
Applying the law of large numbers again, as lim m → 8, this quantity tends to:
17
Published as a conference paper at ICLR 2022
Ewι,W2„n(o,i){[σ(XWι, x›))σ(<Wι, x1〉)Sm〈W2, x>))σ(<W2, x1〉)]}
“ EW1rσpxW1,xyqqσpxW1,x1yqsEW2rσpxW2,xyqqσpxW2,x1yqs	(23)
“ 2κ2(x, x1 )κ2 (x, x1).
The theorem follows by summing UP 2 ^ Eq.(20) and Eq.(23).
Width Requirements
As noted previously, two-layer Π-Netneeds an the extra factor C(?log m) in terms of width, to stay
close to initialization The Proof is largely derived from Lemma A.6 in [27], based on ideas first noted
in [1] and therefore, we just provide a rough sketch here. For simplicity, consider the set of weights
(W2, W3) fixed at initialization, i.e. at W2(0q, W3(0q and note the first-order Taylor expansion for
the Π-Net w.r.t W1 as follows:
fw(x) « fW(X) = fwpoq (x) + χvwp0qfwp0q (x), Wi ´ w10qy
Here fW0 denotes the first order Taylor expansion of f at W0 . We can then expand the RHS as:
CCf Σ W30j(σ(xτw20j)σ(xTw10j) + σ(xTW^)σ1(xTW^)xT(Wι,j ´ W(Oj)).
m
j
Consequently, we can bound the approximation error as:
Ifw(x)—fW(x)l ≤ jɪ∑ ∣σ(xTW20j)∣∣w30j∣(σ(xτTWIjH(σ(xTWpj)+σ1 (XTW(Oj)XT(WIj´W(Ojq)].
m
j
Notice that aside from the ∣σ(xτW20j)∣, the error term is exactly the same as that for a standard
network. Also note that P(XTW2j2T) = P (σ(xτW2j) > T) ≤ e´"/2, for τ > 0 by the
sub-gaussian tail bound. To ensure ∣σ(xτW20j)∣ ≤ T @ j with probability 1 一 δ, we use the union
bound to obtain the condition on τ as m.ef2∕2 < δ. Consequently, we have that T 〜 ((?log m),
which essentially becomes an additional constant factor in the error term, over the usual error terms
from the standard feed-forward network.
18
Published as a conference paper at ICLR 2022
D PROOF OF THEOREM 2: CHARACTERIZING THE Π-KERNEL EIGENDECAY
Here, we prove the Π-kernel eigenvalue decay rate stated in theorem 2. We first recall some connec-
tions between spherical harmonics and Gegenbauer polynomials.
Definition 1 For a given α P R, Gegenbauer (or ultraspherical) polynomials denoted Ckα :
[―1,1] → R are a family of orthogonal polynomials With respect to the weight function X →
pi — x2)。—1, i.e,
/]Ca(x)ca(x)(i—x2)a—2 “ O,
for k ‰ `.
Remark: Gegenbauer polynomials are a generalization of Legendre polynomials which can be re-
covered by taking α “ 1.
The following addition formula expresses Gegenbauer polynomials in terms of spherical harmonics.
Lemma 1 (Theorem 4.11 in [20]) (Addition formula) Let tYk,j ujN“p1d,kq denote spherical harmonics
of degree k in d ` 1 variables. It holds that, for any x, x1 P Sd,
N W)	P d´l q
∑ Yk,j Px)Yk,j (x') “ N(d,k)CpFqp<x, x1y).
j“1
By making use of this Lemma, we can rewrite the Mercer decomposition of any admissible dot
product kernel K when the data is uniform on the unit sphere. Indeed, by simplifying the Mercer’s
decomposition in terms of spherical harmonics we have :
8	Npd,q
K (x, x1q “ X μk X Yk,j (X)Kj (χ1)
k“0 j“1
8	d´1
“ £ μkN(d, k)Ck 2	(<x, x1y),
k“0
(24)
where (μk)8to are its corresponding eigenvalues, and the second equality follows from Lemma 1.
Now, since κ1 and κ2 are dot-product Mercer kernels, we can, akin to Eq. (24), obtain their respec-
tive decompositions in terms of Gegenbauer polynomials. There exist (μ1,k)8“o and Wk)"
both sequences of eigenvalues, such that
8	d´1
<X, x1>κι (x, x1)= X μι,k N (d,k)Ck 2 (3 x1〉)
k“0
8	d´1
K2 (x, x1)= X μ2,k N (d,k)Ck 2 (<x, x1〉).
k“0
(25)
The decay rate of the eigenvalues (μι,k)8to and (μ2,k)8“o is known [ , 10, 12] and is stated in the
following Lemma.
Lemma 2 (Appendix D.2 of [5], Lemma 17 of [10]) For k " d, k even, we have that μι,k 〜
A(d)k—d—1 and μ2,k 〜B(d)k—d—2—1{2, where A(d) and B(d) are constants only depending on
the dimension d.
Our goal is to establish the decay rate of the eigenvalues (μ∏,k)8“o of the kernel
κ∏(x, x1) = 2(2<x, x1>κι(x, x1) ' κ2(x, x1))κ2(x, x1).	(26)
By plugging in the representations in Eq. (25) into the Π-kernel expression (Eq. (26)), we find that
19
Published as a conference paper at ICLR 2022
8	d´1	8	d´1
κ∏(x, x1) = 2(X(2μι,k ' μ2,k)N(d,k)ck 2)(〈x, x1〉))(X μ2,kN(d,k)Ck 2)(〈x, x1〉)).
k“0	k“0
(27)
Moreover, since we can also write
8	d´1
κ∏(x, x1) = £ μ∏,kN(d, k)Ck 2)(〈x, x1〉)，
k“0
an appropriate factorisation of the product Eq. (27) will allow us to deduce the order of μ∏,k by
equating the coefficients appearing in front of the Gegenbauer polynomials.
Developing the product in Eq. (27) leads to the appearance of products of Gegenbauer polynomials.
The product of two Gegenbauer polynomials turns out to be a linear combination of other Gegen-
bauer polynomials.
Lemma 3 (Equation 8 in [13]) Let α P R, for any m, n P N, there exists positive coefficients
(λSm,nq)m0pm,nq such that
min(m,nq
Cmaq(X)Cnaq(X)=	∑	λsm,nqem`…「(吟
s“0
(28)
By using this expansion, coefficients for each Gegenbauer polynomial may be identified.
Take k P N to be an even number. We know from plugging Lemma 3 into Eq. (27) that the coefficient
μ∏,2kN(d, 2k) in front of the polynomial c]/q can be lower bounded as follows.
μ∏,2kN(d, 2k)》N(d,k)2(2μι,k ' μ2,k)μ2,kλ0k,kq.
(29)
We obtain this loWer bound by considering the contribution of a single term in Eq. (28) Where
d ´ 1 ʌ
m “ n “ k and S “ 0 to the coefficient in front of。；1 .This contribution is a lower bound
because all coefficients involved in the product are non-negative.
Consequently, in order to establish a decay rate for (μ∏,k) it suffices to study the decay rate of λ0k,kq
This rate is given by the following Lemma.
d 1	(k，kq
Lemma 4 Let ɑ = -´- be an integer, the coefficient λ0，J defined in Lemma 3 admits thefollowing
expression λ0k,kq = (ppa'j⅞k⅞ (α'2∙l)! . Moreover, for k " d, it holds that
双种〜kp-{2q.
See Appendix D.1 for a proof.
We noW dispose of all the necessary results to prove Theorem 2. Starting from Eq. (29), We find that
for k " d, We have that
N Npd, kq2	λ(k,k)
μ∏,2k2N(d .k) p2μ1,k + μ2,k qμ2,k λ0
k k	(k,kq
〜(2k)d (241,k + 〃2，k)〃2，kλ0	(by StirIing)
kd
“2dΩ(k-2"2)kpd/2q	(by Lemma 2 and Lemma 4)
“Q((2k)—d/2—2).
(30)
This concludes the proof that, for k divisible by 4, We have μ∏,k “ θ(k´d^´2). For k “ 1 mod 4,
We conduct the same reasoning taking the coefficient λ02k'1,2kq. For k “ 3 mod 4, the coefficient to
consider is λ02k'1,2k'2q. The equivalence derivation Lemma 4 proceeds in exactly the same manner
for these coefficients.
20
Published as a conference paper at ICLR 2022
D.1 Proof of Lemma 4
Let us denote pαqk :“ αpα ` 1qpα ` 2q.pα ` k ´ 1q and pαq0 :“ 1.
From Equation 8 in [13], we have that
pk,kq
λ0
2k ' α (α)o(α)k(α)k (2a)2k (2k)!
2kΓα.. 0!(k)!(k)! .702T.(2O)2k
(α)k(α)k (2k)!
(k)!(k)! (0)2k
((a ' k ´ 1)!)2 (2k)!(α ´ 1)!
((α ´ 1)!(k)!)2 (α + 2k ´ 1)!
((a ' k ´ 1)!)2	(2k)!
(α ´ 1)!((k)!)2 (α + 2k ´ 1)!.
(31)
We Can apply Stirling,s approximation stating that n!〜 √2πn(m)n to find that:
λ(k,k)〜(α + k ´ 1)(α + k ´ 1)2pα'kτq
0	?α — 1 (α ´ 1)α-1k(k)2k
√2k(2k)2k
?α + 2k — 1 (α + 2k — 1)a'2k—1
(32)
Considering the case when k " α or equivalently, k " d since α “ (d — 1){2, we obtain the
following simplification:
(k,kq
λ0
k(2v+2k —1)
(2k)2k+0.5
k2k'1	. (2k)v'2k-0.5
„( k )ɑT 〜kpd{2q.
P 2)
(33)
21
Published as a conference paper at ICLR 2022
E LEARNING S PHERICAL HARMONICS WITH Π-KERNEL
Following the setup in [12], we perform a similar experiment on learning combinations of spherical
harmonics in the NTK regime. We define and initialize the Π-Net exactly as specified, with a
width of 32768 neurons and using vanilla gradient descent, to approximate the kernel learning in the
infinite width limit. We take n “ 1000 samples, txuin“1 from the uniform distribution on the unit
sphere S10. We define our target function with integral k P K as follows:
f *(χ) = NIKq ∑ AkPk pχχ,Zk yq,
p q kPK
(34)
where the Pk (tq is the Gegenbauer polynomial with degree k, ζk are fixed vectors that are indepen-
dently generated from uniform distribution on unit sphere inR10, and N (Kq is a suitable normalizing
constant to keep the order of magnitude of the target function approximately the same for different
choices of K. As noted previously, f * may be seen as a linear combination of spherical harmonics
and we compare the error residuals during the learning process in standard vs Π-Nets in the NTK
regime, for varying K. We use a moving average of range 20 on these curves to smoothen out the
heavy oscillations in the latter stages.
?QU*O-)II⅛U3- u,2t20」d -enpw3,l
Figure 7
Figure 8: The plots represent a comparison of log-scale convergence curve of error projection
lengths for standard vs Π-kernel for different order harmonics with K “ t1, 2, 4u, indicating a
clear improvement in the rate of convergence of error for higher harmonics
For the first setting, we look at K “ t1, 2, 4u, with corresponding weight ratio as A1 : A2 : A4 “
1 : 1 : 1. For each frequency, we look at the rate of convergence for the two kernels.
We observe (Fig. 8) that the rate of error convergence for the Π-kernel is much faster than the
standard two-layer NTK and especially for higher harmonics (clearest increase for the highest k “
4), which is exactly what is expected from theoretical results.
Next, we consider K “ t1, 3, 4, 5, 8, 12u i.e. we consider higher frequency harmonics and also
introduce odd harmonics, with the respective Ak assigned the equal weights relative to each other.
Recall that the eigenvalues corresponding to odd harmonics vanish for the standard kernel, meaning
that we expect a much slower convergence rate for them. As before, we look at the rate of conver-
gence for individual harmonics for the two kernels. The convergence curves are presented in Fig. 9.
We verify the speed-up in higher frequencies, and an especially notable gap for odd harmonics other
than k “ 1.
Discussion: The empirical results strongly support our hypothesis that the Π-kernel can speed up
learning higher harmonics faster than the standard two-layer kernel, even for settings outside of
k " d.
22
Published as a conference paper at ICLR 2022
= ∑cθ1p,<(x,ωlυ⅛ (1,3,4,5,8,12}
Figure 9: The plots represent a comparison of log-scale convergence curve of error projection
lengths for standard vs Π-kernel for different order harmonics with K “ t1, 3, 4, 5, 8, 12u. We again
see a clear improvement in the rate of convergence of error for higher harmonics, and especially so
for odd harmonics greater than 1.
23
Published as a conference paper at ICLR 2022
F Synthetic Experiments with Sinusoids
F.1 Learning Sinusoids
Depth vs Order of Polynomial: Learning Sinusoids
standard-net (depth = 9)	∏-net (depth = 6, 3 injections)
Figure 10: The heat map denotes a comparison on the effectiveness of increasing depth vs introduc-
ing multiplicative interactions via Π-Nets for learning high-frequency information. The empirical
evidence shows that multiplicative layers are more effective for learning higher frequencies faster.
Additive vs Multiplicative Skip Connections： Learning Sinusoids
standard-net (depth = 6, 3 additive skip connections} _	∏-net (depth = 6, 3 injections)
OooOInOOOo b OoOomo0。OZOoOOl
SU。一⅛E
Figure 11: The heat map denotes a comparison on the effectiveness of multiplicative layers via Π-
Nets vs only using additive skip connections. The additive skip connections do not seem to affect
the spectral bias over standard neural networks in terms of improving speed of learning for high
frequencies.
F.2 Robustness
Robustness to Perturbations: Motivated by the observations of Rahaman et al. [37], we evaluate
the susceptibility of higher frequencies to random perturbations for standard networks and Π-Nets.
The setup relies on the learning task in the previous experiment. More precisely, following conver-
gence of the network to a low-error approximation of target f * (denoted by fθ*), random isotropic
perturbations are introduced to the network parameters: θ “ θ* ` δθ. We monitor the effect of
increasing the perturbation magnitude δ on the frequencies of interest. In the first setting (Fig. 12),
we compare the effects of perturbations on the six-layer standard network to the six-layer Π-Net
(with five multiplicative layers). In the second setting (Fig. 13), we directly compare the two vari-
ants of 6-layer deep Π-Nets i.e. one with five layers and the other with three layers. We observe
that that Π-Nets are more robust to perturbations, especially in terms of retaining high frequency
information, as compared to standard feedforward networks. We also note that the presence of more
multiplicative interactions makes the network more robust.
24
Published as a conference paper at ICLR 2022
Comparison on Robustness
-0.6
-0.4
-0.2
-0.8
Frequency [Hz]	Frequency [Hz]
Figure 12: The heap maps present a comparison on the robustness to random parameter perturba-
tions for six-layer standard vs six-layer Π-Net. The y-axis denotes the norm of the random pertur-
bation. For standard networks, the high-frequency information is lost quickly as the perturbation
norm increases, while Π-Nets are much more effective at retaining higher frequency information,
even under large perturbations.
L 0.0
Figure 13: The heap maps present a comparison on the robustness to random parameter perturbations
for two variants of Π-Nets, one with three multiplicative injection layers and the other, a higher
degree polynomial with five multiplicative layers. We observe a higher degree polynomial leads
to higher robustness, which is consistent with our intuition that multiplicative layers expand the
solution space for learning high-frequency information.
25
Published as a conference paper at ICLR 2022
G Experiments with Images
G. 1 Implementation Details
U-net type hourglass architectures provide the ideal inductive bias for a host of image restoration
tasks in the DIP framework and for our experiments, we use the same architectural design as [33] for
the standard networks. Since we require the output and input to have the same spatial dimensions,
the number of downsampling blocks is equal to the upsampling blocks. We refer to this number as
the ‘scale’ of the U-net. We provide a schematic illustration in Fig. 14.
Figure 14: Illustration for the U-net with scale = 2, i.e the standard network with two upsam-
pling/downsampling operations.
Note that the each down/upsampling block within itself contains convolutions, normalization and
pooling but we abstract those details from the schematic for clarity. For the Π-network architecture,
we consider a product of two polynomials model which modifies the standard network by introduc-
ing multiplicative connections. An illustrative schematic is presented in Fig. 15.
Figure 15: Adapting the 2-scale U-net for the product of two polynomials Π-network, the vertical
dotted line highlights the separation between the polynomials.
Remark: In addition to the architecture, it is important to note another important detail about using
Π-Nets. In practice, we recommend the use of two separate learning rates while training Π-Nets,
wherein the learning rate for multiplicative connection parameters specifically is lower than the
learning rate corresponding the parameters in the feed-forward part of the network. It leads to more
stability while training.
26
Published as a conference paper at ICLR 2022
G.2 Additional Experiments
(a)
(b)
Figure 16: Visual comparison of the denoised image pertaining to the denoising experiment in 4.2.
We compare a snapshot (right) of the respective network outputs after 2500 iterations against the true
image (left) for (a) standard network (b) Π-Net. We visually confirm that Π-network has already
begun to fit the high-frequency noise faster and to a larger extent.
27
PUbliShed as a ConferenCe PaPersICLR 2022

H∣∣ω∣∣)
1¾
IoI
-F=
Io 2
Figure lsWe ViSUaHZe the deep image prior Of different n—Nets at5'itializatδ'n WithrandOm
WeightSfOrthe Same inpuL by IOOkg at the power SPeCtral density of the output，With a fixed
u—net architecturintroducing more multiplicative interactionsIletWOrk ShiftS the IletWOrk spec—
trum towards higher frequenciewhich OfferSintuition towards UlIderStandg Why multiplicative
interactionS SPeed UP learning in high—frequency informatioP
1⅞IOl IO2
-e-
一 terato'n : 225
1⅞
IO6
P(IMl)
1	1
ɔ	O
6	Co
三 era!∑ron ： 75
true -mage
n,netw ork
standard network
1⅞IOl IO2
-ε-
一 teraro'n 一 300
P(IwlI)
IOl 10
一 I
P(IMI)
5	5
Iol 10
-e-
P(IWlI)
S S
P(IMI)
5	5
W
IoO 1°IO2
-ɛ-
-teraΓ-ro,n 一 375
IOI 10
-ɪ-
Figure l0°COmPariSOIl Ofthe PIer SPeCtraI density CUrVeS OVer 600 iterationSfOr Standard VS ∏
IIetWOrkS (Same SCaIe) against the ground truth，We ObSerVe that the n—Net PiCk UP higher frequen—
CieS faster
28
Published as a conference paper at ICLR 2022
H Additional Experiments in Classification
(a) Validation loss curve zoomed in for first 1000 iterations.
(b) Validation loss curve for 5000 iterations.
Figure 19: Validation loss curves corresponding to the classification experiment, presenting a com-
parison between Π-Net with one multiplicative layer and standard feedforward network. The smaller
dip for Π-Net implies a tendency to pick up high frequency label noise faster.
Our results allow us to make a further overarching conclusion that Π-Nets, in addition to picking up
higher frequencies w.r.t inputs faster (as demonstrated in the DIP settings), in classification settings
can also pick up high frequency variations in the decision boundaries faster, thus making our general
claims about the spectral bias of Π-Nets and multiplicative interactions stronger.
29
Published as a conference paper at ICLR 2022
(a) Validation loss curve zoomed in for first 1000 iterations.
1 SSo-I CO=S=S 3SW
SSo-IU,9⅛p≡> 山SW
(b) Validation loss curve for 5000 iterations.
Figure 20: Validation loss curves corresponding to the classification experiment to observe the effect
of increasing multiplicative injections. We compare the Π-Net with one multiplicative layer to Π-
Net with two multiplicative layers. The validation dip reduces even further for the Π-Net with
more multiplicative layers (i.e., a higher degree polynomial) indicating that more multiplicative
interactions improve the network’s ability to learn more complex decision boundaries (introduced
by the high frequency noise).
30