Published as a conference paper at ICLR 2022
Convergent and Efficient Deep Q Network Al-
GORITHM
Zhikang T. Wang & Masahito Ueda*
Department of Physics and Institute for Physics of Intelligence
University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan
{wang,ueda}@cat.phys.s.u-tokyo.ac.jp
Ab stract
Despite the empirical success of the deep Q network (DQN) reinforcement learn-
ing algorithm and its variants, DQN is still not well understood and it does not
guarantee convergence. In this work, we show that DQN can indeed diverge and
cease to operate in realistic settings. Although there exist gradient-based con-
vergent methods, we show that they actually have inherent problems in learning
dynamics which cause them to fail even in simple tasks. To overcome these prob-
lems, we propose a convergent DQN algorithm (C-DQN) that is guaranteed to
converge and can work with large discount factors (〜0.9998). It learns robustly
in difficult settings and can learn several difficult games in the Atari 2600 bench-
mark that DQN fails to solve.
1	Introduction
With the development of deep learning, reinforcement learning (RL) that utilizes deep neural net-
works has demonstrated great success recently, finding applications in various fields including
robotics, games, and scientific research (Levine et al., 2018; Berner et al., 2019; Fosel et al., 2018;
Wang et al., 2020). One of the most efficient RL strategy is Q-learning (Watkins, 1989), and the
combination of Q-learning and deep learning leads to the DQN algorithms (Mnih et al., 2015; Hes-
sel et al., 2018; Riedmiller, 2005), which hold records on many difficult RL tasks (Badia et al.,
2020). However, unlike supervised learning, Q-learning, or more generally temporal difference
(TD) learning, does not guarantee convergence when function approximations such as neural net-
works are used, and as a result, their success is actually empirical, and the performance relies heavily
on hyperparameter tuning and technical details involved. This happens because the agent uses its
own prediction to construct the learning objective, a.k.a. bootstrapping, and as it generalizes, its pre-
dictions over different data interfere with each other, which can make its learning objective unstable
in the course of training and potentially lead to instability and divergence.
This non-convergence problem was pointed out decades ago by the pioneering works of Baird (1995)
and Tsitsiklis & Van Roy (1997), and it has been empirically investigated for DQN by Van Hasselt
et al. (2018). We have also observed the divergence of DQN in our experiments, as in Fig. 6. The
non-convergence problem often shows up as instability in practice and it places significant obstacles
to the application of DQN to complicated tasks. It makes the training with deeper neural networks
more difficult, limits the time horizon for planning, and makes the results sometimes unstable and
sensitive to hyperparameters. This state of affairs is not satisfactory especially for those scientific
applications that require convergence and generality. Although convergent gradient-based methods
have also been proposed (Sutton et al., 2009; Bhatnagar et al., 2009; Feng et al., 2019; Ghiassian
et al., 2020), they cannot easily be used with deep non-linear neural networks as they either re-
quire linearity or involve computationally heavy operations, and they often show worse empirical
performance compared with TD methods.
In this work, we show that the above-mentioned gradient-based methods actually have inherent
problems in learning dynamics which hamper efficient learning, and we propose a convergent DQN
*RIKEN Center for Emergent Matter Science (CEMS), Wako, Saitama 351-0198, Japan
1
Published as a conference paper at ICLR 2022
(C-DQN) algorithm by modifying the loss of DQN. Because an increase of loss upon updating the
target network of DQN is a necessary condition for its divergence, we construct a loss that does not
increase upon the update of the target network, and therefore, the proposed algorithm converges in
the sense that the loss monotonically decreases. In Sec. 2 we present the background. In Sec. 3
we discuss the inefficiency issues in the previous gradient-based methods and demonstrate using
toy problems. In Sec. 4 we propose C-DQN and show its convergence. In Sec. 5 we show the
results of C-DQN on the Atari 2600 benchmark (Bellemare et al., 2013) and in Sec. 6 we present
the conclusion and future prospect. To our knowledge, the proposed C-DQN algorithm is the first
convergent RL method that is sufficiently efficient and scalable to obtain successful results on the
standard Atari 2600 benchmark using deep neural networks, showing its efficacy in dealing with
realistic and complicated problems.
2	Background
Reinforcement learning involves a Markov decision process (MDP), where the state st of an envi-
ronment at time step t makes a transition to the next state st+1 conditioned on the action of the agent
at at time t, producing a reward rt depending on the states. The process can terminate at terminal
states sT, and the transition of states can be either probabilistic or deterministic. The goal is to find
a policy π(s) to determine the actions at+i 〜π(st+i) in order to maximizes the return PT—t rt+i,
i.e., the sum of future rewards. In practice, a discounted return PiT=-0t γirt+i is often used instead,
with the discount factor γ < 1 and γ ≈ 1, so that the expression is convergent for T → ∞ and that
rewards far into the future can be ignored, giving an effective time horizon 匚%.The value function
is defined as the expected return for a state st following a policy π, and the Q function is defined as
the expected return for a state-action pair (st, at):
T -t
Vπ(st) = Eat,{(st+i,at+i)}iT=-1t	X γirt+i
i=0
T -t
Qπ(st, at) = E{(st+i,at+i)}iT=-1t X γirt+i , (1)
i=0
with at+i 〜∏(st+i) in the evaluation of the expectation. When the Q function is maximized by a
policy, We say that the policy is optimal and denote the Q function and the policy by Q* and ∏*,
respectively. The optimality implies that Q* satisfies the Bellman equation (Sutton & Barto, 2018)
Q*(st, at) = rt + γEst+1 ma0x Q* (st+1, a0) .
(2)
The policy π* is greedy with respect to Q*, i.e. π*(s) = arg maxa0 Q*(s, a0). Q-learning uses this
recursive relation to learn Q*. In this work we only consider the deterministic case and drop the
notation Es… [∙] where appropriate.
When the space of state-action pairs is small and finite, we can write down the values ofan arbitrarily
initialized Q function for all state-action pairs into a table, and iterate over the values using
∆Q(st, at) = α rt + γma0xQ(st+1, a0) - Q(st, at) ,	(3)
where α is the learning rate. This is called Q-table learning and it guarantees convergence to Q*.
If the space of (s, a) is large and Q-table learning is impossible, a function approximation is used
instead, representing the Q function as Qθ with learnable parameter θ. The learning rule is
∆θ = αVθQθ(st,at)① + YmaaX Qθ(st+ι,a0) - Qθ(st, at) ,	(4)
which can be interpreted as modifying the value of Qθ (st, at) following the gradient so that
Qθ(st, at) approaches the target value rt + γ maXa0 Qθ(st+1, a0). However, this iteration
may not converge, because the term maXa0 Qθ (st+1, a0) is also θ-dependent and may change
together with Qθ(st,at). Specifically, an exponential divergence occurs if YVQθ(st,at) ∙
Vθ maXa0 Qθ (st+ι,a0) > ∣∣Vθ Qθ (st, at)||2 is always satisfied and the value of max。，Q§ (st+ι,a0)
is not constrained by other means.1 This can be a serious issue for realistic tasks, because the adja-
cent states St and st+ι often have similar representations and VQθ (st, ∙) is close to VQθ (st+ι, ∙).
1This can be shown by checking the Bellman error δt := rt + γ maxa0 Qθ (st+1, a0) - Qθ(st, at), for
which we have ∆δt = αδt (γVθQθ(st,at) ∙ max.，Qθ(st+1,a0) - ∣∣VθQθ(st,at)∣∣2) up to the first order
of ∆θ following Eq. (4). As ∆δt is proportional to δt with the same sign, it can increase exponentially.
2
Published as a conference paper at ICLR 2022
The DQN algorithm uses a deep neural network with parameters θ as Qθ (Mnih et al., 2015),
and to stabilize learning, it introduces a target network with parameters θ, and replace the term
maxao Qθ(st+ι, a0) by max。，Qd(St+1,a0), so that the target value r + Y max。，Qg(st+ι, a0) does
not change simultaneously with Qθ. The target network θ is then updated by copying from θ for ev-
ery few thousand iterations of θ. This technique reduces fluctuations in the target value and dramati-
cally improves the stability of learning, and with the use of offline sampling and adaptive optimizers,
it can learn various tasks such as video games and simulated robotic control (Mnih et al., 2015; Lil-
licrap et al., 2015). Nevertheless, the introduction of the target network θ is not well-principled,
and it does not really preclude the possibility of divergence. As a result, DQN sometimes requires
a significant amount of hyperparameter tuning in order to work well for a new task, and in some
cases, the instability in learning can be hard to diagnose or remove, and usually one cannot use a
discount factor γ that is very close to 1. In an attempt to solve this problem, Durugkar & Stone
(2017) considered only updating θ in a direction that is perpendicular to Vθ max。，Qθ(st+ι, a0);
however, this strategy is not satisfactory in general and can lead to poor performance, as shown in
Pohlen et al. (2018).
One way of approaching this problem is to consider the mean squared Bellman error (MSBE), which
is originally proposed by Baird (1995) and called the residual gradient (RG) algorithm. The Bellman
error, or Bellman residual, TD error, is given by δt(θ) := rt + γ max。， Qθ(st+1, a0) - Qθ(st, at).
Given a dataset S of state-action data, δt is a function of θ, and we can minimize the MSBE loss
LMSBE(θ) := E hlδ(θ)^i = js? X	∣qθ(st, at) - rt - YmaXQθ(st+ι,a0)∣ ,	(5)
(st,at,rt,st+ι)∈S
and in practice the loss is minimized via gradient descent. If LMSBE becomes zero, we have δt ≡ 0
and the Bellman equation is satisfied, implying Qθ = Q*. Given a fixed dataset S, the convergence
of the learning process simply follows the convergence of the optimization of the loss. This strategy
can be used with neural networks straightforwardly. There have also been many improvements on
this strategy including Sutton et al. (2008; 2009); Bhatnagar et al. (2009); Dai et al. (2018); Feng
et al. (2019); Ghiassian et al. (2020); Touati et al. (2018), and they are often referred to as gradient-
TD methods. Many of them have focused on how to evaluate the expectation term in Eq. (2) and
make it converge to the same solution found by TD, or Q-learning. However, most of these methods
often do not work well for difficult problems, and few of them have been successfully demonstrated
on standard RL benchmarks, especially the Atari 2600 benchmark. In the following we refer to the
strategy of simply minimizing LMSBE as the RG algorithm, or RG learning.
3 Inefficiency of minimizing the MSBE
3.1 Ill-conditionness of the loss
In the following we show that minimizing LMSBE may not lead to efficient learning by considering the
case of deterministic tabular problems, for which all the gradient-TD methods mentioned above re-
duce to RG learning. For a tabular problem, the Q function values for different state-action pairs can
be regarded as independent variables. Suppose we have a trajectory of experience {(st, at, rt)}tN=-01
obtained by following the greedy policy with respect to Q, i.e. at = arg max。， Q(st, a0), and sN is
a terminal state. Taking Y = 1, the MSBE loss is given by
LMSBE = N
N-2
(Q(st, at) - rt - Q(st+1, at+1)) + (Q(sN -1, aN-1) - rN-1) .
t=0
(6)
Despite the simple quadratic form, the Hessian matrix of LMSBE with variables {Q(st, at)}tN=-01 is
ill-conditioned, and therefore does not allow efficient gradient descent optimization. The condition
number κ of a Hessian matrix is defined by κ :=
∣λmax∣, where λmax and 1面口 are the largest and
smallest eigenvalues. We have numerically found that κ of the Hessian of LMSBE in Eq. (6) grows as
O(N 2). To find an analytic expression, we add an additional term Q(s0, a0)2 to LMSBE, so that the
3
Published as a conference paper at ICLR 2022
Figure 1: Left: the cliff walking task, where the agent is supposed to go to the lower right corner
as quickly as possible and avoid the grey region. The red arrows show the optimal policy. In this
example the system has the height of 4 and the width of 10. Right: results of learning the cliff
walking task in a tabular setting, using randomly sampled state-action pair data. The upper plots
show the result with width 10 and γ = 0.9, and the lower show the result with width 20 and
Y = 0.95. |Q - Q*|2 is the squared distance between the learned Q function and the optimal Q*.3
Both Q-table learning and RG use the learning rate of 0.5, averaged over 10 repeated runs.
update steps
Hessian matrix becomes
(4	-2	∖
-2	4 J.
(7)
-2
∖
-2
4
N×N
The eigenvectors of this Hessian matrix that have the form of standing waves are given by
(Sin Nk+πι, Sin NNk++1,... Sin Nk1 )T for k ∈ {1, 2,...N}, and the corresponding eigenvalues are given
by 4 - 4 cos NnI. Therefore, We have K
1+coS N∏+1 〜O(N2). See appendix for the details.
With γ < 1, if the states form a cycle, i.e., if sN -1 makes a transition to s0, the loss becomes
LMSBE = N[PN=-2 (Qt - r - γQt+ι)2 3 + (QN-1 - TN-1 - γQ0)2], where Q(st, at) is denoted
by Qt . Then, the Hessian matrix is cyclic and the eigenvectors have the form of periodic waves:
(Sin 2Nπ, sin 4Nπ,…sin l2Nkn)T and (CoS 2Nπ, CoS 警，…CoS 2Nkπ)T for k ∈ {1,2,…N}, with
corresponding eigenvalues given by 2(1 + γ2) - 4γ CoS 警.At the limit of N → ∞, we have
κ = (1-γ)2. Using γ ≈ 1, we have K 〜 O Q-Y)2). By interpreting ι-γ as the effective time
horizon, or the effective size of the problem, we see that κ is quadratic in the size of the problem,
which is the same as its dependence on N in the case of γ = 1 above. In practice, K is usually
104 〜105, and we therefore conclude that the loss is ill-conditioned.
The ill-conditionedness has two important implications. First, as gradient descent converges at a
rate of O(K),2 the required learning time is quadratic in the problem size, i.e. O(N2), for the RG
algorithm. In contrast, Q-learning only requires O(N) steps as it straightforwardly propagates re-
ward information from si+1 to si at each iteration step. Therefore, the RG algorithm is significantly
less computationally efficient than Q-learning. Secondly, the ill-conditionedness implies that a small
LMSBE does not necessarily correspond to a small distance between the learned Q and the optimal
Q*, which may explain why LMSBE is not a useful indicator of performance (Geist et al., 2017).
Cliff walking By way of illustration, we consider a tabular task, the cliff walking problem in
Sutton & Barto (2018), as illustrated in Fig. 1. The agent starts at the lower left corner in the
grid and can move into nearby blocks. If it enters a white block, it receives a reward -1; if it
enters a grey block which is the cliff, it receives a reward -100 and the process terminates; if it
2Although in the deterministic case momentum can be used to accelerate gradient descent to a convergence
rate of Ο(√κ), it cannot straightforwardly be applied to stochastic gradient descent since it requires a large
momentum factor (√K-1 )2 (Polyak, 1964) which results in unacceptably large noise.
3∣Q — Q*|2 is defined by P(S。)∣Q(s,a) — Q*(s, a)∣2, where the sum is taken over all state-action pairs.
4
Published as a conference paper at ICLR 2022
enters the goal, it terminates with a zero reward. To learn this task, we initialize Q for all state-
action pairs to be zero, and we randomly sample a state-action pair as (st , at) and find the next
state st+1 to update Q via Eq. (3), which is the Q-table learning, or to minimize the associated loss
(Qθ (st, at) - rt - γ maxa0 Qθ(st+1, a0))2 following the gradient, which is RG learning. As shown
in Fig. 1, RG learns significantly more slowly than Q-table learning, and as shown in the right plots,
given a fixed value of LMSBE, RG's distance to the optimal solution Q* is also larger than that of
Q-table learning, showing that Q-table learning approaches Q* more efficiently. To investigate the
dependence on the size of the problem, we consider a width of 10 of the system with γ = 0.9, and
its doubled size—a width of 20 with γ = 0.95. The results are presented in the upper and lower
plots in Fig. 1. Notice that the agent learns by random sampling from the state-action space, and
doubling the system size reduces the sampling efficiency by a factor of 2. As the learning time
of Q-learning is linear and RG is quadratic in the deterministic case, their learning time should
respectively become 4 times and 8 times for the double-sized system. We have confirmed that the
experimental results approximately coincide with this prediction and therefore support our analysis
of the scaling property above.
3.2 Tendency of maintaining the average prediction
Besides the issue of the ill-conditionedness, the update rule of RG learning still has a serious problem
which can lead to unsatisfactory learning behaviour. To show this, we first denote Qt ≡ Q(st, at)
and Qt+1 ≡ maxa0 Q(st+1, a0), and given that they initially satisfy Qt = γQt+1, with an observed
transition from st to st+1 with a non-zero reward rt, repeatedly applying the Q-table learning rule
in Eq. (3) leads to ∆Qt = rt and ∆Qt+1 = 0, and thus ∆ (Qt + Qt+1) = rt. On the other hand, in
the case of RG, minimizing LMSBE using the gradient results in the following learning rule
∆Q(st, at) = α rt +γmaxQ(st+1,a0) - Q(st,at) ,	∆ max Q(st+1, a0) = -γ∆Q(st, at),
a0	a0
(8)
and therefore whenever Qt is modified, Qt+1 changes simultaneously, and we have
∆(Qt+Qt+1)=(1-γ)rt.	(9)
Due to the condition γ ≈ 1, ∆ (Qt + Qt+1) can be very small, which is different from Q-learning,
since the sum of the predicted Q, i.e. Pt Qt, almost does not change. This occurs because there is
an additional degree of freedom when one modifies Qt and Qt+1 to satisfy the Bellman equation:
Q-learning tries to keep Qt+ι fixed, while RG follows the gradient and keeps Qt + YQt+ι fixed,
except for the case where st+1 is terminal and Qt+1 is a constant. This has important implications
on the learning behaviour of RG as heuristically explained below.
If the average of Q(s, a) is initialized above that of Q*(s, a) and the transitions among the states
can form loops, the learning time of RG additionally scales as O(ɪ-1Y) due to Eq. (9), regardless of
the finite size of the problem. As shown in Fig. 2, in the cliff walking task with width 10, Q-table
learning has roughly the same learning time for different Y, while RG scales roughly as O(y-1γ) and
does not learn for γ = 1. This is because the policy arg maxa0 Q(s, a0) ofRG prefers non-terminal
states and goes into loops, since the transitions to terminal states are associated with Q values below
what the agent initially expects. Then, Eq. (9) controls the learning dynamics of the sum of all
Q values, i.e. P(s,a) Q(s, a), with the learning target being P(s,a) Q* (s, a), and the learning time
scales as O( ɪ-1Y). For Y = 1, ∆ P(S ɑ)Q(s,a)is always zero and P(S ɑ)Q*(s, a) cannot be learned,
which results in failure of learning.
A more commonly encountered failure mode appears when Q is initialized below Q*, in which case
the agent only learns to obtain a small amount of reward and faces difficulties in consistently improv-
ing its performance. A typical example is shown in Fig. 3, where Q is initialized to be zero and the
agent learns from its observed state transitions in an online manner following the -greedy policy.4
We see that while Q-table learning can solve the problem easily without the help ofa non-zero , RG
cannot find the optimal policy with = 0, and it learns slowly and relies on non-zero values. This
is because when RG finds rewards, Q(s, a) for some states increase while the other Q(s, a) values
4-greedy means that for probability a random action is used; otherwise arg maxa0 Q(s, a0) is used.
5
Published as a conference paper at ICLR 2022
Figure 2: Results of cliff
walking in Fig. 1 with dif-
ferent γ, with system’s width
of 10, averaged over 10 runs.
RG with γ = 1 does not
converge within reasonable
computational budgets.
Figure 3: Left: one-way cliff walking task, where the agent starts at
the lower left corner, and at each step it is allowed to move to the
right to obtain a reward of 2, or move up and terminate with a reward
of -1. It terminates upon reaching the goal. Right: performance
of the learned greedy policy and min Q(s, a) for online Q-table
learning and RG, following the -greedy policy for different values
of, with γ = 1 and a learning rate of 0.5, averaged over 100 trials.
decrease and may become negative, and if the negative values become smaller than the reward asso-
ciated with the termination, i.e. -1, it will choose termination as its best action. Therefore, it relies
on the exploration strategy to correct its behaviour at those states with low Q(s, a) values and to find
the optimal policy. Generally, when an unexpected positive reward rt is found in learning, accord-
ing to the learning rule in Eq. (8), with an increase of rt in Q(st, at), maxa0 Q(st+1 , a0) decreases
simultaneously by rt, and the action at st+1, i.e. arg maxa0 Q(st+1, a0), is perturbed, which may
make the agent choose a worse action that leads to rt less reward, and therefore the performance
may not improve on the whole. In such cases, the performance of RG crucially relies on the explo-
ration strategy so that the appropriate action at st+1 can be rediscovered. In practice, especially for
large-scale problems, efficient exploration is difficult in general and one cannot enhance exploration
easily without compromising the performance, and therefore, RG often faces difficulties in learning
and performs worse than Q-learning for difficult and realistic problems.
Remark Although the above discussion is based on the tabular case, we believe that the situation
is not generally better when function approximations are involved. With the above issues in mind,
it can be understood why most of the currently successful examples of gradient-TD methods have
tunable hyperparameters that can reduce the methods to conventional TD learning, which has a Q-
learning-style update rule. If the methods get closer to conventional TD without divergence, they
typically achieve better efficiency and better quality of the learned policy. When the agent simply
learns from the gradient of the loss without using techniques like target networks, the performance
can be much worse. This probably explains why the performance of the PCL algorithm (Nachum
et al., 2017) sometimes deteriorates when the value and the policy neural networks are combined
into a unified Q network, and it has been reported that the performance of PCL can be improved by
using a target network (Gao et al., 2018). Note that although ill-conditionedness may be resolved
by a second-order optimizer or the Retrace loss (Munos et al., 2016; Badia et al., 2020), the issue in
Sec. 3.2 may not be resolved, because it will likely converge to the same solution as the one found
by gradient descent and thus have the same learning behaviour. A rigorous analysis of the issue in
Sec. 3.2 is definitely desired and is left for future work.
4 Convergent DQN algorithm
4.1	Interpreting DQN as fitted value iteration
As we find that Q-learning and the related conventional TD methods and DQN have learning dynam-
ics that is preferable to RG, we wish to minimally modify DQN so that it can maintain its learning
dynamics while being convergent. To proceed, we first cast DQN into the form of fitted value itera-
tion (FVI) (Ernst et al., 2005; Munos & SzePesv^π, 2008). With initial parameters θ° of the target
network, the DQN loss for a transition (st, at, rt, st+1) and network parameters θ is defined as
2
'DQN(θ; θi)：
St,at) - r - YmXQ氏(st+ι,a'
(10)
6
Published as a conference paper at ICLR 2022
and DQN learns by iterating over the target network
LDQN(θ; θi) := E hlDQN(θ; E)] ,
〜
〜
θi+1 = arg min LDQN(θ; θi),
θ
(11)
and θi is used as the parameter of the trained network for a sufficiently large i. In practice, the
minimum in Eq. (11) is found approximately by stochastic gradient descent, but for simplicity, here
we consider the case where the minimum is exact. When DQN diverges, the loss is supposed to
diverge with iterations, which means we have minθ LDQN(θ; θi+1) > minθ LDQN(θ; θi) for some i.
4.2	Constructing a non-increasing series
Theorem 1.	The minimum of LDQN(θ; θi) with target network θi is upper bounded by LMSBE(θi).
El ♦ i	i	ι ∙	. ι r∙	τ	/ ∕⅛	∕⅛ ∖	τ	/ ∕⅛ ∖	ι ♦ τ	/∕k 六 ∖,
This relation can be derived immediately from LDQN(θi; θi) = LMSBE(θi) and minθ LDQN(θ; θi) ≤
LDQN(θi; θi), giving minθ LDQN(θ; θi) ≤ LMSBE(θi).
-«-» Tl ɪʌ y-v、T 1∙	♦ T / 八 ∕⅛ \ 1 •	• . ) ♦	∙	1.1 Γ∙ T	/彳 \	.
When DQN diverges, minθ LDQN(θ; θi) diverges with increasing i, and therefore LMSBE(θi) must
also diverge. Therefore at each iteration, while the minimizer θi+ι minimizes the i-th DQN loss
LDQN(θ; θi), it can increase the upper bound of the (i+1)-th DQN loss, i.e. LMSBE(θi+1). We want
both LDQN and LMSBE to decrease in learning and we define the convergent DQN (C-DQN) loss as
一 ，一 ^r 、	_
LCDQN(θ; θi) ：= E
[max {'DQN(θ; θ),'MSBE(θ)}],
(12)
where 'MSBE(θ) ：= (Qθ(st, at) - r - Y max。，Qθ(st+ι, a0))2 and LMSBE = E ['msbe].
Theorem 2.	The C-DQN loss satisfies minθ LCDQN(θ; θi+1) ≤ minθ LCDQN(θ; θi), given θi+1
arg minθ LCDQN(θ; θi).
We have
〜
〜

min LCDQN(θ; θi+l) ≤ LCDQN(θi+i; θi+l) = LMSBE(θi+l),
θ
(13)
τ /:	∖,πnl	I λ / X ∕⅛∖ λ /才 ∖ I τ / X : \ ,	∙ τ /八 彳、
LMSBE(θi+1) ≤ E [maχ I'DQN(θi+1; θi),'MSBE(θi+1)∣J = LCDQN(θi+1; θi) ≤ 吗n LCDQN(θ; θi)∙
(14)
Therefore, we obtain the desired non-increasing condition minθ LCDQN(θ; θi+1) ≤
mιnθ LCdqn(θ; θi), which means that the iteration θ — argmm& LCDQN(θ; θ) is convergent,
in the sense that the loss is bounded from below and non-increasing.
C-DQN as defined above is convergent for a given fixed dataset. Although the analysis starts from
the assumption that θi exactly minimizes the loss, in fact, it is not necessary. In practice, at the
moment when the target network θ is updated by θ, the loss immediately becomes equal to LMSBE(θ)
which is bounded from above by the loss LCDQN(θ; θ) before the target network update. Therefore,
as long as the loss is consistently optimized during the optimization process, the non-increasing
property of the loss holds throughout training. We find that it suffices to simply replace the loss used
in DQN by Eq. (12) to implement C-DQN. As we empirically find that LMSBE in RG is always much
smaller than LDQN, we expect C-DQN to put more emphasis on the DQN loss and to have learning
behaviour similar to DQN. C-DQN can also be augmented by various extensions of DQN, such
as double Q-learning, distributional DQN and soft Q-learning (Van Hasselt et al., 2016; Bellemare
et al., 2017; Haarnoja et al., 2017), by modifying the losses 'DQN and 'MSBE accordingly. The mean
squared loss can also be replaced by the Huber loss (smooth `1 loss) as commonly used in DQN
implementations. More discussions on the properties of C-DQN are provided in the appendix.
5	Experiments
5.1	Comparison of C-DQN, DQN and RG
We focus on the Atari 2600 benchmark as in Mnih et al. (2015), and use the dueling network ar-
chitecture and prioritized sampling, with double Q-learning where applicable (Wang et al., 2016;
7
Published as a conference paper at ICLR 2022
PJeMaJ
——C-DQN
DQN
——RG
IOW
0.0 0.2 0.4 0.6 0.8 1.0
frames ×10e
PJeMω,J
Figure 5: Training Perfor-
mance and loss on Space In-
vaders when half of the data
are randomly discarded.
IO6
IO7
——C-DQN
DQN
——RG
10e
frames
PJeMaJ
0	2	4	6
frames ×107
Oooo
Oooo
0 5 0 5
2 11
PJeMaJ
0.0	0.5	1.0	1.5	2.0
frames ×108
IO6 IO7 IO8
frames
Figure 4: Training Performance and training loss on games Pong (left) and Space Invaders (right).
Ooooo
Ooooo
0 8 6 4 2
1 PJeMaJ
Q.Q 0.2 0.4 0.6 0.8 1.0
frames ×10β
0.2 0.4 0.6 0.8 1.0
frames ×10β
Figure 6: Training performance and training loss on Space In-
vaders when the memory adopts a random replacement strategy
(left) and when the memory is smaller and adopts different strate-
gies (middle and right).
Schaul et al., 2015; Van Hasselt et al., 2016). We refer to the combination of the original DQN and
these techniques as DQN, and similarly for C-DQN and RG. Details of exPerimental settings are
given in the aPPendix and our codes are available in the suPPlementary material.
As C-DQN, DQN and RG only differ in their loss functions, we follow the hyPerParameter settings
in Hessel et al. (2018) for all the three algorithms and comPare them on two well-known games,
Pong and Space Invaders, and the learning curves for Performance and loss are shown in Fig. 4. We
see that both C-DQN and DQN can learn the tasks, while RG almost does not learn, desPite that
RG has a much smaller loss. This coincides with our Prediction in Sec. 3, which exPlains why there
are very few examPles of successful aPPlications of RG to realistic Problems. The results show that
C-DQN as a convergent method indeed Performs well in Practice and has Performance comParable
to DQN for standard tasks. Results for a few other games are given in the aPPendix.
5.2	Learning from incomplete trajectories of experience
To give an examPle in which DQN is Prone to diverge, we consider learning from incomPlete tra-
jectories of exPerience, i.e. given a transition (st, at, st+1) in the dataset, the subsequent transition
(st+1, at+1, st+2) may be absent from the dataset. This makes DQN Prone to diverge because while
DQN learns Qθ(st, at) based on maxa0 Qθ(st+1, a0), there is a Possibility that maxa0 Qθ(st+1, a0)
has to be inferred and cannot be directly learned from any data. To create such a setting, we ran-
domly discard half of the transition data collected by the agent and keeP the other exPerimental
settings unchanged, excePt that the mean squared error is used instead of the Huber loss to allow
for divergence in gradient. We find that whether or not DQN diverges or not is task-dePendent in
general, with a larger Probability to diverge for more difficult tasks, and the result for Space Invaders
is shown in Fig. 5. We see that while DQN diverges, C-DQN learns stably and the sPeed of its learn-
ing is only slightly reduced. This confirms that C-DQN is convergent regardless of the structure of
the learned data, and imPlies that C-DQN may be Potentially more suitable for offline learning and
learning from observation when comPared with DQN.
A similar situation arises when one does not use the first-in-first-out (FIFO) strategy to rePlace old
data in the rePlay memory (i.e. the dataset) with new data when the dataset is full, but rePlaces
old data randomly with new data. In Fig. 6, we show that conventional DQN can actually diverge
in this simPle setting. In the existing DQN literature, this rePlacement strategy is often ignored,
while here it can been seen to be an imPortant detail that affects the results, and in Practice, FIFO
is almost always used. However, FIFO makes the memory data less diverse and less informative,
and it increases the Possibility of the oscillation of the co-evolvement of the Policy and the rePlay
memory, and as a result, a large size of the rePlay memory is often necessary for learning. In Fig. 6,
we show that when the size of the rePlay memory is reduced by a factor of 10, C-DQN can benefit
from utilizing the random rePlacement strategy while DQN cannot, and C-DQN can reach a higher
Performance. Note that DQN does not diverge in this case, Probably because the rePlay memory is
8
Published as a conference paper at ICLR 2022
Figure 7: Training performance on several difficult games in Atari 2600, with learning rate 4 × 10-5 .
Each line represents a single run and the shaded regions show the standard deviation. The discount
factors are shown in the titles and all DQN agents have significant instabilities or divergence in loss.
less off-policy when it is small, which alleviates divergence. The result opens up a new possibility
of RL of only storing and learning important data to improve efficiency, which cannot be realized
stably with DQN but is possible with C-DQN.
5.3	Difficult games in Atari 2600
In this section we consider difficult games in Atari 2600. While DQN often becomes unstable
when the discount factor γ gets increasingly close to 1, in principle, C-DQN can work with any γ.
However, we find that a large γ does not always result in better performance in practice, because
a large γ requires the agent to learn to predict rewards that are far in the future, which are often
irrelevant for learning the task. We also notice that when γ is larger than 0.9999, the order of
magnitude of (1 - γ)Qθ gets close to the intrinsic noise caused by the finite learning rate and
learning can stagnate. Therefore, we require γ to satisfy 0.99 ≤ γ ≤ 0.9998, and use a simple
heuristic algorithm to evaluate how frequent reward signals appear so as to determine γ for each
task, which is discussed in detail in the appendix. We also take this opportunity to evaluate the mean
μ of Q and the scale σ of the reward signal using sampled trajectories, and make our agent learn
the normalized value Q-μ instead of the original Q. We do not clip the reward and follow Pohlen
et al. (2018) to make the neural network learn a transformed function which squashes the Q function
approximately by the square root.
With C-DQN and large γ values, several difficult tasks which previously could not be solved by
simple DQN variants can now be solved, as shown in Fig. 7. We find that especially for Skiing,
Private Eye and Venture, the agent significantly benefits from large γ and achieves a higher best
performance in training, even though Private Eye and Venture are partially observable tasks and not
fully learnable, which leads to unstable training performance. Evaluation of the test performance
and details of the settings are given in the appendix. Notably, we find that C-DQN achieves the
state-of-the-art test performance on Skiing despite the simplicity of the algorithm.
6	Conclusion and future perspectives
We have discussed the inefficiency issues regarding RG and gradient-TD methods, and addressed the
long-standing problem of convergence in Q-learning by proposing a convergent DQN algorithm, and
we have demonstrated the effectiveness of C-DQN on the Atari 2600 benchmark. With the stability
of C-DQN, we can now consider the possibility of tuning γ freely without sacrificing stability, and
consider the possibility of learning only important state transitions to improve efficiency. C-DQN
can be applied to difficult tasks for which DQN suffers from instability. It may also be combined
with other strategies that involve target networks and potentially improve their stability.
There are many outstanding issues concerning C-DQN. The loss used in C-DQN is non-smooth,
and it is not clear how this affects the optimization and the learning dynamics. In our experiments
this does not appear to be a problem, but deserves further investigation. When the transitions are
stochastic, the loss LMSBE used in C-DQN does not converge exactly to the solution of the Bellman
equation, and therefore it would be desirable if C-DQN can be improved so that stochastic transitions
can be learned without bias. It would be interesting to investigate how it interplays with DQN
extensions such as distributional DQN and soft Q-learning, and it is not clear whether the target
network in C-DQN can be updated smoothly as in Lillicrap et al. (2015).
9
Published as a conference paper at ICLR 2022
Reproducibility Statement
All our experimental results can be reproduced exactly by our codes provided in the supplementary
material, where the scripts and commands are organised according to the section numbers. We also
present and discuss our implementation details in the appendix so that one can reproduce our results
without referring to the codes.
References
Joshua Achiam, Ethan Knight, and Pieter Abbeel. Towards characterizing divergence in deep Q-
learning. arXiv preprint arXiv:1903.08894, 2019.
Andrgs Antos, Csaba SzePesvðri, and Remi Munos. Learning near-optimal policies with Bellman-
residual minimization based fitted policy iteration and a single sample path. Machine Learning,
71(1):89-129, 2008.
Adria Puigdomenech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi,
Zhaohan Daniel Guo, and Charles Blundell. Agent57: Outperforming the atari human benchmark.
In International Conference on Machine Learning, pp. 507-517. PMLR, 2020.
Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In
Machine Learning Proceedings 1995, pp. 30-37. Elsevier, 1995.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-
ment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:
253-279, 2013.
Marc G Bellemare, Will Dabney, and Remi Munos. A distributional perspective on reinforcement
learning. In International Conference on Machine Learning, pp. 449-458. PMLR, 2017.
Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, PrzemySIaW Debiak, Christy
Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large
scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.
Shalabh Bhatnagar, Doina Precup, David Silver, Richard S Sutton, Hamid Maei, and Csaba
Szepesvgri. Convergent temporal-difference learning with arbitrary smooth function approxi-
mation. Advances in neural information processing systems, 22:1204-1212, 2009.
Aditya Bhatt, Max Argus, Artemij Amiranashvili, and Thomas Brox. Crossnorm: Normalization
for off-policy TD reinforcement learning. arXiv preprint arXiv:1902.05605, 2019.
Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le Song. Sbeed:
Convergent reinforcement learning with nonlinear function approximation. In International Con-
ference on Machine Learning, pp. 1125-1134. PMLR, 2018.
Ishan Durugkar and Peter Stone. TD learning with constrained gradients. In Proceedings of the
Deep Reinforcement Learning Symposium, NIPS 2017, Long Beach, CA, USA, December 2017.
URL http://www.cs.utexas.edu/users/ai-lab?NIPS17-ishand.
Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. First return, then
explore. Nature, 590(7847):580-586, 2021.
Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning.
Journal of Machine Learning Research, 6:503-556, 2005.
Yihao Feng, Lihong Li, and Qiang Liu. A kernel loss for solving the Bellman equation. arXiv
preprint arXiv:1905.10506, 2019.
Thomas Fosel, Petru Tighineanu, Talitha Weiss, and Florian Marquardt. Reinforcement learning
with neural networks for quantum feedback. Physical Review X, 8(3):031084, 2018.
Scott Fujimoto, David Meger, and Doina Precup. An equivalence between loss functions and non-
uniform sampling in experience replay. arXiv preprint arXiv:2007.06049, 2020.
10
Published as a conference paper at ICLR 2022
Yang Gao, Huazhe Xu, Ji Lin, Fisher Yu, Sergey Levine, and Trevor Darrell. Reinforcement learning
from imperfect demonstrations. arXiv preprint arXiv:1802.05313, 2018.
Matthieu Geist, Bilal Piot, and Olivier Pietquin. Is the Bellman residual a bad proxy? In Advances
in Neural Information Processing Systems,pp. 3205-3214, 2017.
Sina Ghiassian, Andrew Patterson, Shivam Garg, Dhawal Gupta, Adam White, and Martha White.
Gradient temporal-difference learning with regularized corrections. In International Conference
on Machine Learning, pp. 3524-3534. PMLR, 2020.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. arXiv preprint arXiv:1702.08165, 2017.
Alexander Hans and Steffen Udluft. Efficient uncertainty propagation for reinforcement learning
with limited data. In International Conference on Artificial Neural Networks, pp. 70-79. Springer,
2009.
Alexander Hans and Steffen Udluft. Ensemble usage for more reliable policy identification in rein-
forcement learning. In ESANN, 2011.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan
Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in
deep reinforcement learning. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
Tobias Johannink, Shikhar Bahl, Ashvin Nair, Jianlan Luo, Avinash Kumar, Matthias Loskyll,
Juan Aparicio Ojea, Eugen Solowjow, and Sergey Levine. Residual reinforcement learning for
robot control. In 2019 International Conference on Robotics and Automation (ICRA), pp. 6023-
6029, 2019. doi: 10.1109/ICRA.2019.8794127.
Seungchan Kim, Kavosh Asadi, Michael Littman, and George Konidaris. Deepmellow: removing
the need for a target network in deep Q-learning. In Proceedings of the Twenty Eighth Interna-
tional Joint Conference on Artificial Intelligence, 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Sergey Levine, Peter Pastor, Alex Krizhevsky, Julian Ibarz, and Deirdre Quillen. Learning hand-
eye coordination for robotic grasping with deep learning and large-scale data collection. The
International Journal of Robotics Research, 37(4-5):421-436, 2018.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and
Michael Bowling. Revisiting the arcade learning environment: Evaluation protocols and open
problems for general agents. Journal of Artificial Intelligence Research, 61:523-562, 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529-533, 2015.
Remi Munos and Csaba Szepesvdri. Finite-time bounds for fitted value iteration. Journal of Ma-
chine Learning Research, 9(27):815-857, 2008. URL http://jmlr.org/papers/v9/
munos08a.html.
Remi Munos, Tom Stepleton, Anna Harutyunyan, and Marc G Bellemare. Safe and efficient off-
policy reinforcement learning. arXiv preprint arXiv:1606.02647, 2016.
11
Published as a conference paper at ICLR 2022
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between
value and policy based reinforcement learning. In Advances in Neural Information Processing
Systems,pp. 2775-2785, 2017.
Shota Ohnishi, Eiji Uchibe, Yotaro Yamaguchi, Kosuke Nakanishi, Yuji Yasui, and Shin Ishii. Con-
strained deep Q-learning gradually approaching ordinary Q-learning. Frontiers in neurorobotics,
13:103, 2019.
Tobias Pohlen, Bilal Piot, Todd Hester, Mohammad Gheshlaghi Azar, Dan Horgan, David Budden,
Gabriel Barth-Maron, Hado Van Hasselt, John Quan, Mel Vecerik, et al. Observe and look further:
Achieving consistent performance on atari. arXiv preprint arXiv:1805.11593, 2018.
Boris T Polyak. Some methods of speeding up the convergence of iteration methods. Ussr compu-
tational mathematics and mathematical physics, 4(5):1-17, 1964.
Martin Riedmiller. Neural fitted q iteration-first experiences with a data efficient neural reinforce-
ment learning method. In European Conference on Machine Learning, pp. 317-328. Springer,
2005.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv
preprint arXiv:1511.05952, 2015.
Ralf Schoknecht and Artur Merke. TD (0) converges provably faster than the residual gradient
algorithm. In Proceedings of the 20th International Conference on Machine Learning (ICML-
03), pp. 680-687, 2003.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Richard S Sutton, Csaba Szepesvdri, and Hamid Reza Maei. A convergent O(N) temporal-difference
algorithm for off-policy learning with linear function approximation. In NIPS, 2008.
Richard S Sutton, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba
Szepesvdri, and Eric Wiewiora. Fast gradient-descent methods for temporal-difference learning
with linear function approximation. In Proceedings of the 26th Annual International Conference
on Machine Learning, pp. 993-1000, 2009.
Ahmed Touati, Pierre-Luc Bacon, Doina Precup, and Pascal Vincent. Convergent tree backup and re-
trace with function approximation. In International Conference on Machine Learning, pp. 4955-
4964. PMLR, 2018.
Volker Tresp. The wet game of chicken. Siemens AG, CT IC 4, Technical Report, 1994.
John N Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with function
approximation. IEEE transactions on automatic control, 42(5):674-690, 1997.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double Q-
learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 30, 2016.
Hado Van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph Mo-
dayil. Deep reinforcement learning and the deadly triad. arXiv preprint arXiv:1812.02648, 2018.
Zhikang T Wang, Yuto Ashida, and Masahito Ueda. Deep reinforcement learning control of quantum
cartpoles. Physical Review Letters, 125(10):100401, 2020.
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling
network architectures for deep reinforcement learning. In International conference on machine
learning, pp. 1995-2003. PMLR, 2016.
Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. 1989.
Shangtong Zhang, Wendelin Boehmer, and Shimon Whiteson. Deep residual reinforcement learn-
ing. In Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent
Systems, AAMAS ’20, pp. 1611-1619, Richland, SC, 2020. International Foundation for Au-
tonomous Agents and Multiagent Systems. ISBN 9781450375184.
Shangtong Zhang, Hengshuai Yao, and Shimon Whiteson. Breaking the deadly triad with a target
network. arXiv preprint arXiv:2101.08862, 2021.
12
Published as a conference paper at ICLR 2022
A Convergence of C-DQN in stochastic settings
When the transition of states is stochastic, it is well-known that the minimum of LMSBE does not
exactly correspond to the solution of the Bellman equation, because we have
LMSBE (θ) = Est+1
st, at) - rt - γ ma0x Qθ (st+1 , a0)
= Qθ(st, at) - rt - γ Est+1
hma0xQθ(st+1,a0)i	+γ2Varst+1(ma0xQθ(st+1,a0)),
(15)
where Var(∙) represents the variance, and it can be seen that only the first term on the last line corre-
sponds to the solution of the Bellman equation. Because C-DQN involves LMSBE, if the underlying
task is stochastic, C-DQN may not converge to the optimal solution due to the bias in LMSBE. In
fact, both the minimum of LMSBE and the solution of the Bellman equation are stationary points for
C-DQN. To show this, we first assume that the minimum of LDQN and that of LMSBE are unique
and different. If parameters θ and 仄 satisfy θ =仄=arg min LDQN(∙;仄)，i.e., if they are at the
converging point of DQN, then we consider an infinitesimal change δθ of θ following the gradient
of LMSBE in an attempt to reduce LMSBE. Because we have LMSBE(θ) = LDQN(θ; θ) = LDQN(θ; θi),
as θ + δθ moves away from the minimum LDQN(θ; θi), we have
_	, _	~ ,	_	-T ,	_	, _ ,	_	, _
LDQN(θ + δθ; θi) > LDQN(θ; θi) = LMSBE(θ) > LMSBE(θ + δθ),	(16)
and therefore for θ + δθ, LDQN is larger than LMSBE, and C-DQN will choose to optimize LDQN
instead of LMSBE and the parameter will return to the minimum θ, which is the converging point
of DQN. On the other hand, given θ =仄=arg min LMSBE(∙), if We change θ by an infinitesimal
amount δθ in an attempt to reduce LDQN(θ; θi), we similarly have
_	, _	_	, _ ,	_	-T ,	_	, _	-T .
LMSBE(θ + δθ) > LMSBE(θ) = LDQN(θ; θi) > LDQN(θ + δθ; θi),	(17)
and therefore, for the same reason C-DQN will choose to optimize LMSBE and the parameter will
return to θ. Therefore, C-DQN can converge to both the converging points of DQN and RG. More
generally, C-DQN may converge somewhere between the converging points of DQN and RG. It
tries to minimize both the loss functions simultaneously, and it stops if this goal cannot be achieved,
i.e., if a decrease of one loss increases the other. Interestingly, this does not seem to be a severe
problem as demonstrated by the successful application of C-DQN to the Atari 2600 benchmark
(see Sec. 5), because the tasks include a large amount of noise-like behaviour and subtitles such as
partially observable states.
A. 1 A case study: the wet-chicken benchmark
To investigate the behaviour of C-DQN more closely, we consider a stochastic toy prob-
lem, which is known as the wet-chicken benchmark (Tresp, 1994; Hans & Udluft, 2009;
2011). In the problem, a canoeist paddles on a river starting at position x = 0, and
there is a waterfall at position x = l = 20, and the goal of the canoeist is to
get as close as possible to the waterfall without reaching it. The canoeist can choose
to paddle back, hold the position, or drift forward, which corresponds to a change of
-1, 0, or +1 in his/her position x, and there is random turbulence Z 〜 Uniform(-2.5, +2.5), a
uniformly distributed random number, that also contributes to the change in x and stochastically
perturbs x at each step. The reward is equal to the position x, and x is reset back to 0 if he/she
reaches the waterfall at x = 20. The task does not involve the end of an episode, and the per-
formance is evaluated as the average reward per step. This task is known to be highly stochastic,
because the effect of the stochastic perturbation is often stronger than the effect of the action of the
agent, and the stochasticity can lead to states that have dramatically different Q function values.
To learn this task, we generate a dataset of 20000 transitions using random actions, and we train a
neural network on this dataset using the DQN, the C-DQN and the RG algorithms to learn the Q
values. The results are shown in Fig. 8. In the left panel of Fig. 8, it can be seen that while RG
significantly underperforms DQN, the performance of C-DQN lies between DQN and RG and is
only slightly worse than DQN. This shows that when the task is highly stochastic, although C-DQN
13
Published as a conference paper at ICLR 2022

15 -
14 -
8 -
Figure 8: Performance on the wet-chicken benchmark training on a dataset generated by the random
policy (left) and the distances among the learned Q functions (right). The experiment is repeated for
10 times, and the standard error of the performance and the standard deviation of the distances are
shown as the shaded regions.
500	1000	1500	2000
epoch
ωUUBtt-p
150 -
125 -
100 -
75 -
50-
25 -
175 -
500	1000	1500	2000
epoch
7 -
0
0 -
0
may not reach the optimal solution as DQN can do, C-DQN still behaves robustly and produces
reasonably satisfactory results, while RG fails dramatically.
To obtain further details of the learned Q functions, we estimate the distance between the
Q functions. The distance |Q1 - Q2 | between two Q functions Q1 and Q2 is estimated as
P(x,a) (Q1 (x, a) - Q2(x, a))2, where the summation on the position x is taken over the dis-
crete set {0, 1, 2, ...19}. In the right panel of Fig. 8, we show the estimated distances among the
learned Q functions of DQN, C-DQN and RG. We see that the distance between QDQN and QCDQN
increases rapidly in the beginning and then slowly decreases, implying that DQN learns quickly at
the beginning, and C-DQN catches up later and reduces its distance to DQN. Notably, we find that
the value |QCDQN - QDQN| + |QCDQN - QRG| - |QDQN - QRG| always converges to zero, indicating
that the solution found by C-DQN, i.e. QCDQN, lies exactly on the line from QDQN to QRG, which is
consistent with our argument that C-DQN converges somewhere between the converging points of
DQN and RG.
Concerning the experimental details, the neural network includes 4 hidden layers, each of which has
128 hidden units and uses the ReLU as the activation function, and the network is optimized using
the Adam optimizer (Kingma & Ba, 2014) with default hyperparameters. We use the batch size of
200, and the target network is updated after each epoch, which makes the DQN algorithm essentially
the same as the neural fitted Q (NFQ) iteration algorithm (Riedmiller, 2005). The training includes
2000 epochs, and the learning rate is reduced by a factor of 10 and 100 at the 1000th and the 1500th
epochs. The discount factor γ is set to be 0.97. The position x and the reward are normalized by
20 before training, and the evaluation of the performance is done every 5 epochs, using 300 time
steps and repeated for 200 trials. The entire experiment is repeated for 10 times including the data
generation process.
B Additional experimental results
In this section we present additional experimental results. In Sec. B.1, we present the results of
applying C-DQN to the problem of measurement feedback cooling of quantum quartic oscillators
in Wang et al. (2020), where we show that the final performances are more consistent regarding
different random seeds and have a smaller variance compared with the results of DQN. Concerning
the Atari 2600 benchmark, in Sec. B.2, results for several other games are presented. In Sec. B.3
we show that C-DQN allows for more flexible update periods of the target network. In Sec. B.4 we
report the test performance of C-DQN on the difficult Atari 2600 games shown in Sec. 5.3, and in
Sec. B.5 we discuss the results of C-DQN on the game Skiing.
14
Published as a conference paper at ICLR 2022
B.1	Results on measurement feedback cooling of a quantum quartic
OSCILLATOR
To show the stability of C-DQN compared with DQN for problems with practical significance, we
reproduce the results in Wang et al. (2020), which trains a RL controller to do measurement feed-
back cooling of a one-dimensional quantum quartic oscillator in numerical simulation. Details of
the problem setting are given in Wang et al. (2020) and in our supplementary codes. The training
performances for C-DQN and DQN are shown in Fig. 9, where each curve represents a different
experiment with a different random seed. As shown in Fig. 9, different C-DQN experiments have
similar learning curves and final performances; however, in sharp contrast, those of the DQN exper-
iment have apparently different fluctuating learning curves and the performances are unstable, and
some of the repetitions cannot reach a final performance that is comparable to the best-performing
ones. The results show that compared with DQN, the outcome of the training procedure of C-DQN
is highly stable and reproducible, which can greatly benefit practical applications.
Figure 9: Training performance of C-DQN and DQN on the task of measurement feedback cooling
of quartic oscillators. The vertical axis shows the energy of the cooled quartic oscillator, and a
smaller energy represents better performance. The horizontal axis shows the simulated time of
the oscillator system that is used to train the agent. Each curve represents a separate trial of the
experiment.
B.2	Experimental results on other games
In addition to the results in Sec. 5.1, we present results on 6 other Atari 2600 games comparing
C-DQN and DQN, which are shown in Fig. 10.
In general, we find that the loss value of C-DQN is almost always smaller than DQN, and for
relatively simple tasks, C-DQN has performance comparable to DQN, but for more difficult tasks,
they show different performances with relatively large variance. Specifically, we find that C-DQN
has better performance for tasks that require more precise learning and control such as Atlantis, and
for tasks that are unstable and irregular such as Video Pinball. However, for tasks that are highly
stochastic and partially observable such as Fishing Derby and Time Pilot, C-DQN may perform less
well compared with DQN, probably because the term LMSBE in LCDQN does not properly account for
stochastic transitions.
B.3	More flexible update periods for the target network
As C-DQN has a better convergence property, it allows for shorter update periods of the target
network. Specifically, convergence of C-DQN is obtained as long as the loss decreases during the
optimization process after an update of the target network. One period of the update of the target
network consists of Nd iterations of gradient descent on θ minimizing LCDQN(θ; θi) or LDQN(θ; θi)
with the target network θi, and then using θ as the next target network θi+1, where Nθd represents
the update period of the target network. In previous works on DQN, Nθd is set to be 2000 or 2500
(Hessel et al., 2018; Mnih et al., 2015), and DQN may experience instability for a too small Nθd.
However, we empirically find that for many tasks in Atari 2600, Nθd can be reduced to 200 or even
20 without instability for C-DQN. Therefore, we find that C-DQN requires less fine tuning on the
15
Published as a conference paper at ICLR 2022
Atlantis
PJeMaJ
Breakout
O O
2 1
PJeMaJ
30
PJeMaJ
Video Pinball
Figure 10:	Training performance and training loss of C-DQN and DQN on several other Atari 2600
games, using the same experimental settings as in Sec. 5.1.
hyperparameter Nd compared with DQN. The experimental results on Space Invaders and Hero are
shown in Fig. 11, using the experimental settings in Sec. 5.1. We see that C-DQN has a generally
higher performance compared to DQN when Nθd becomes small, and the performance of DQN is
sometimes unstable and is sensitive to the value of Nθd.
1250 -
1000 -
Ooo
5 0 5
7 5 2
PjeMaJ
25000-
20000 -
15000-
10000 -
5000 -

Figure 11:	Training performance using different update periods of the target network on games
Space Invaders (left) and Hero (right). In the game Hero, there appears to be a local optimum with
reward 13000 where the learning can fail to make progress, which is also seen in Fig. 15.
B.4	Test performance on difficult Atari games
In this section we report the test performance of the agents in Sec. 5.3 and compare with existing
works. As the learned policy of the agents has large fluctuations in performance due to noise,
local optima, and insufficient learning when the task is difficult, instead of using the agent at the
end of training, we use the best-performing agent during training to evaluate the test performance.
Specifically, we save a checkpoint of the parameter θ of the agent every 104 steps, and we choose the
three best-performing agents by comparing their training performances, computed as the average of
the 40 nearby episodes around each of the checkpoints. After the training, we carry out a separate
validation process using 400 episodes to find the best-performing one among the three agents, and
then, we evaluate the test performance of the validated best agent by another 400 episodes, using a
different random seed. The policy during evaluation is the -greedy policy with = 0.01„ with no-
16
Published as a conference paper at ICLR 2022
Table 1: Test performance on difficult Atari 2600 games, corresponding to the results in Sec. 5.3
in the main text, evaluated using no-op starts and without sticky actions (Machado et al., 2018).
The DQN results are produced using the same experimental settings as the C-DQN experiments
except for the loss function. Human results and results for Agent57 are due to Badia et al. (2020),
and results for Rainbow DQN are due to Hessel et al. (2018). The human results only represent
the performance of an average person, not a human expert, and the human results correspond to
reasonably adequate performance instead of the highest possible performance of human.
Task	C-DQN	DQN	Human	Rainbow DQN	Agent57 (SOTA)
Skiing	-3697 ± 157	-29751 ± 224	-4337	-12958	-4203 ± 608
Tennis	10.9 ± 6.3	-2.6 ± 1.4	-8.3	0.0	23.8 ± 0.1
Private Eye	14730 ± 37	7948 ± 749	69571	4234	79716 ± 29545
Venture	893 ± 51	386 ± 85	1188	5.5	2624 ± 442
Figure 12: A screenshot of the game
Skiing in Atari 2600.
frames	×1O8
Figure 13: Training performance of C-DQN on Skiing
with learning rate 2 × 10-5, following the experimental
procedure in Sec. 5.3. The standard deviation among
the three runs are shown as the shaded region.
op starts5 (Mnih et al., 2015). The average of the test performances of the 3 runs in our experiments
are shown in Table 1 together with the standard error, compared with existing works and the human
performance.
As we have basically followed the conventional way of training DQN on Atari 2600 as in Mnih et al.
(2015) and Hessel et al. (2018), our C-DQN and the Rainbow DQN in Hessel et al. (2018) allow for
a fair comparison because they use the same amount of computational budget and a similar neural
network architecture.6 In Table 1, we see that in these four difficult Atari 2600 games Rainbow
DQN fails to make progress in learning, and C-DQN can achieve performances higher than Rainbow
DQN and show non-trivial learning behaviour. The results of Agent57 is for reference only, which
represents the currently known best performance on Atari 2600 in general and does not allow for
a fair comparison with C-DQN, as it involves considerably more computation, more sophisticated
methods and larger neural networks. We find that our result on the game Skiing is exceptional, which
is discussed in the next section.
B.5	The Atari game Skiing
In Table 1, one exceptional result is that C-DQN achieves a performance higher than Agent57 on
the game Skiing, actually, using an amount of computation that is less than 0.1% of that of Agent57.
We find that this performance is higher than any other known result so far and thus achieves the
state-of-the-art (SOTA) for this specific game. To elucidate the underlying reason, we describe this
game first.
5No-op starts mean that at the start of each episode the no-operation action is executed randomly for 1 to
30 frames.
6In fact, a fair comparison with Rainbow DQN can be made except for the case of Skiing, because reward
clipping adopted by Rainbow DQN does not permit the learning of Skiing. Nevertheless, this does not affect
our conclusion.
17
Published as a conference paper at ICLR 2022
A screenshot of the game Skiing is shown in Fig. 12. This game is similar to a racing game. The
player is required to go downhill and reach the goal as fast as possible, and the time elapsed before
reaching the goal is the minus reward. At each time step, the player receives a small minus reward
which represents the accumulated time, until the goal is reached and the game ends. In addition,
the player is required to pass through the gates along his/her way, which are represented by the two
small flags shown in Fig. 12, and whenever the player fails to pass a gate, a 5-second penalty is
added to the elapsed time when the player reaches the final goal. The number of gates that have
not been passed are shown at the top of the game screen. Using the standard setting in Mnih et al.
(2015), the number of state transitions for an episode of this game is 〜1300 for the random policy,
〜4500 when the player slows down significantly, and 〜500 when the policy is near-optimal.
Since the penalty for not passing a gate is given only at the end of the game, the agent needs to
relate the penalty at the end of the game to the events that happen early in the game, and therefore
the discount factor Y should be at least around 1 -忐 to mk learning effective. However, the
learning may still stagnate if γ is not larger, because when γ is small, the agent prefers taking a
longer time before reaching the goal, so that the penalty at the end is delayed and the Q function for
the states in the early game is increased, which will increase the episode length and make a larger
γ necessary. Therefore, we have chosen to tune our hyperparameter setting so that γ ≈ 1 - 5100 * is
obtained on this game (see Sec. E.3), and we find that our C-DQN agent successfully learns with the
γ and produces a new record on this game.7 The large fluctuations in its training performance shown
in Sec. 5.3 are mostly due to the noise coming from the finite learning rate, which can be confirmed
by repeating the experiments with a smaller learning rate, shown in Fig. 13. However, in this case
the learning easily gets trapped in local optima and the final test performance is worse. Note that in
fact, we cannot fairly compare our result with Agent57, because we have tuned our hyperparameters
so that the obtained γ is in favour of this game, while Agent57 uses a more general bandit algorithm
to adaptively determine γ .
C Related Works
In this section, we present some related works for further references. The slow convergence of RG
compared with TD methods has been shown in Schoknecht & Merke (2003). The O(N 2) scaling
property can also be derived by considering specific examples of Markov chains, such as the “Hall”
problem as pointed out in Baird (1995). The convergence property of RG-like algorithms has been
analysed in Antos et al. (2008), assuming that the underlying Markov process is β-mixing, i.e. it
converges to a stable distribution exponentially fast. However, this assumption is often impractical,
which may have underlain the discrepancy between the theoretical results of RG and the experi-
mental effectiveness. There is an improved version of RG proposed in Zhang et al. (2020), and RG
has been applied to robotics in Johannink et al. (2019). Concerning DQN, there have been many
attempts to stabilize the learning, to remove the target network, and to use a larger γ. Pohlen et al.
(2018) introduces a temporal consistency loss to reduce the difference between Qθ and Qd on st+1,
and the authors showed that the resulting algorithm can learn with γ = 0.999. A variant of it is
proposed in Ohnishi et al. (2019). Kim et al. (2019) and Bhatt et al. (2019) propose extensions for
DQN and show that the resulting DQN variants can sometimes operate without a target network
when properly tuned. Achiam et al. (2019) gives an analysis of the divergence and proposes to use
a method similar to natural gradient descent to stabilize Q-learning; however, it is computationally
heavy as it uses second-order information, and therefore it cannot be used efficiently with large neu-
ral networks. Recently, the strategy of using target networks in DQN has been shown to be useful
for TD learning as well by Zhang et al. (2021). Some other works have been discussed in the main
text and we do not repeat them here.
D Calculation details of the condition number
In this section we provide the calculation details of Sec. 3.1. Given the loss function
1 N-2
L = N ^X (Qt- rt - Qt+1)2 + (QN-1 - rN-I)2 ,	(18)
N t=0
7The single highest performance we observed was around -3350, and the optimal performance in this game
is reported to be -3272 in Badia et al. (2020).
18
Published as a conference paper at ICLR 2022
where Q(st, at) is denoted by Qt, we add an additional term (Q0)2 to it, and the loss function
becomes
1	N-2
L = N [Q0 + ^X (Qt - rt - Qt+1)2 + (QN-1 - rN-I)2]
N	t=0
1	N-2	(19)
N [Q0 +(Qt - 2rtQt + 2rtQt+ι - 2QtQt+ι + rt + Qt+ι)
N	t=0
+ (QN-1 - 2rN-IQN-1 + 2rN - 1)].
To calculate the condition number of the Hessian matrix, We ignore the prefactor NN and only evalu-
ate the second-order derivatives. From Eq. (19), it is straightforward to obtain
∂2L
∂Q2
∂2L
∂Qt∂Qt+1
and therefore the Hessian matrix is
t∈ {0,1,...N-1}
t∈ {0,1,...N-2}
(20)
(4	-2	∖
-2	4 J.
(21)
-2
-2
4
N×N
∖
The eigenvectors of this matrix can be explicitly obtained due to the special structure of the matrix,
kπ	2kπ	Nkπ T
which are given Dy (Sin N+],Sin N+],...Sin N+]) for k ∈ {1, 2,...N }, and the corresponding
eigenvalues are given Dy 4 - 4cos NnI. To show this, we first simplify the Hessian matrix Dy
removing its constant diagonal 4, which only contributes to a constant 4 in the eigenvalues, and then
we multiply the eigenvectors Dy the eigenvalues
kπ
卜in Nn
0	0 ∙ kπ	2kπ
-2 (Sm-------+ Sm-----
C N +1 + N + 1
2kπ
,Sin N + 1 ,... Sin
), (Sin
kπ
Nkπ
N +1
3kπ
NTI + sin n+1
T	kπ
•(-4CoS NTl)
(N - 1)kπ
K.(Sm
(22)
.(N +1)kπ V
+ Sin F^))
(23)
H11P tn oirι	nk∏	CCq kπ	一	1 ∣∙	(n-1)kπ	∣ (^n+ (n+1)kπ ʌ	Aa ∖x∕0	αkc hα∙vα qi∏	0'k∏	一
due to Sin	N +1	CoS N+]	—	2 ISin	—N+1—	+ Sin —N+1— I ∙	As We	also have Sin	N+]	一
Sin (N++1kπ) — 0, the product of the vector and the eigenvalue is exactly equal to the product
of the vector and the Hessian matrix. As the vectors (Sin Nn, Sin Nkn,... Sin NkI)t are linearly
independent for k ∈ {1, 2, ...N}, they are all the eigenvectors of the Hessian matrix, and therefore
4	.	4	4-	4 .	4	.	4	4-4 CoS Nn
the eigenvalues are 4 - 4 cos ^nT. The condition number is then K — 7-j-n+1 . Using the Taylor
IY + 1	4-4 CoS N+ι
series expansion, the denominator in the expression of K is 4 - 4 cos N+1 —(N+：)2 + O(N4), and
therefore for large N, K ≈ 44-4oSosπ - 〜O(N2).
When the state sN-1 makes a transition to s0, with the discount factor γ, the loss is given Dy
1 N-2
L — N ^X (Qt - rt - YQt+1)2 + (QN-1 - rN-1 - YQ0)2
N t=0
(24)
Using the same calculation, the non-zero second-order derivatives are given Dy
∂ 2L
∂Q2
2+2Y2,
/2L	— -2γ,
dQtdQt+1
t∈ {0,1,...N-1}
t∈{0,1,...N-1}, QN—Q0,
(25)
19
Published as a conference paper at ICLR 2022
and the Hessian matrix is cyclic. Assuming that N is even, the eigenvectors are given by
(Sin 2Nπ, sin 4Nπ,…sin 2NNkπ)T and (Cos 2Nπ, cos 警，... cos 2NNkn)T for k ∈ {1,2,...NN}, with
eigenvalues given by 2(1 + Y2) - 4γcos 2kN∏-. The result can be similarly confirmed by notic-
ing the relation sin 2nNπ ∙ (-4γ cos 2N∏)
icity sin
2(N +1)kπ
2(n+1)k-
cos N
N =
) and cos
Sin 2N∏, and similarly
-2γ (sin
cos 2nNπ
2(n-1)kπ
N
∙ (-4γ
+ sin 2(n+1)kπ) and the period-
cos 2N) = -2γ(cos 2⅛μkπ +
2(NN1)kπ = cos 2N∏, which proves that they are indeed the eigenvectors
and eigenvalues. At the limit of N → ∞,we have K = 2(1++^--YcOSSnn
〜2(1+Y2 )+4γ — (1+γ)2
〜2(1+γ2)-4γ = (1-γ)2 .
Using γ 〜1, we obtain K ~ O	-Y)2
E Experimental details on Atari 2600
E.1 General settings
We follow Mnih et al. (2015) to preprocess the frames of the games by taking the maximum of
the recent two frames and changing them to the grey scale. However, instead of downscaling them
to 84×84 images, we downscale exactly by a factor of 2, which results in 80×105 images as in
Ecoffet et al. (2021). This is to preserve the sharpness of the objects in the images and to preserve
translational invariance of the objects. For each step of the agent, the agent stacks the frames seen in
the recent 4 steps as the current observation, i.e. state st, and decides an action at and executes the
action repeatedly for 4 frames in the game and accumulates the rewards during the 4 frames as rt .
Thus, the number of frames is 4 times the number of steps of the agent. One iteration of the gradient
descent is performed for every 4 steps of the agent, and the agent executes the random policy for
50000 steps to collect some initial data before starting the gradient descent. The replay memory
stores 1 million transition data, using the first-in-first-out strategy unless otherwise specified. The
neural network architecture is the same as the one in Wang et al. (2016), except that we use additional
zero padding of 2 pixels at the edges of the input at the first convolutional layer, so that all pixels in
the 80×105 images are connected to the output. Following to Hessel et al. (2018), we set the update
period of the target network to be 8000 steps, i.e. 2000 gradient descent iterations, using the Adam
optimizer (Kingma & Ba, 2014) with a mini-batch size of 32 and default β1 , β2 hyperparameters,
and we make the agent regard the loss of one life in the game as the end ofan episode. The discount
factor γ is set to be 0.99 unless otherwise specified, and the reward clipping to [-1, 1] is applied
except in Sec. 5.3 and E.3.
Gradient of LCDQN upon updating the target network Although we use gradient descent to
minimize Lcdqn, when we update the target network by copying from θ to θ, 'DQN(θ; θi) is exactly
equal to 'MSBE(θ) and the gradient of LCDQN is undefined. In this case, we find that one may simply
use the gradient computed from 'DQN without any problem, and one may rely on the later gradient
descent iterations to reduce the loss LCDQN. In our experiments, we further bypass this issue by
using the parameters θ at the previous gradient descent step instead of the current step to update θ,
so that θ does not become exactly equal to θ. This strategy is valid because the consecutive gradient
descent steps are supposed to give parameters that minimize the loss almost equally well, and the
parameters should have similar values. In our implementation of DQN, we also update the target
network in this manner to have a fair comparison with C-DQN.
Loss As mentioned in Sec. 4.2, either the mean squared error or the Huber loss can be used to
compute the loss functions 'DQN and 'MSBE. In Sec. 5.2 we use one half of the mean squared error,
and the loss functions are given by
Qθ(st+ι,a0»
'DQN(θ; θ) = 1 (Qθ (st,at) - Irt - Y maX
'MSBE(θ) = 2 (Qθ (St, at) - rt - Y maX
Qθ(st+1, a0)	.
(26)
20
Published as a conference paper at ICLR 2022
In Sec. 5.1 we use the Huber loss, and the loss functions are given by
'DQN(θ; θ) =	'Huber	(Qθ(st,	at),	rt	+ Y m，X Qd(St+1, a
'MSBE (θ) =	'Huber	(Qθ (st,	at),	Irt	+ Y 咤 X Qθ (st+1,a'
(27)
'Huber(X, y)
22 (X - y)2, if |x - y| < 1,
[|x - y| - 2,
if |x - y| ≥ 1.
In Sec. 5.3, We let the agents learn a normalized Q function Q ≡ Q-μ, and We use the strategy in
Pohlen et al. (2018) to squash the Q function approximately by the square root before learning. The
relevant transformation function T is defined by
_,, , , ,
T(Q) := sign(Q)
(JIQ | + 1 - 1) + CTQ,
(28)
and its inverse is given by
T-1(f) =sign(f)
2T
(29)
ʌ
ʌ
In our experiments We set T = 0.01 as in Pohlen et al. (2018), and the loss functions are
'DQN(θ; θ)= 'Huber (fθ (st, at), T Vt + YTT (maX fθ(st+1, a0)))) ,
'MSBE (θ) = ' Huber (fθ (st, at), T 卜 t + Y T T (ma X fθ (st+1, a0)))) ,
(30)
Where fθ is the neural netWork, and T-1 (fθ (st, at)) represents the learned normalized Q function
Qθ(st, at), and ^ is the reward that is modified together with the normalization of the Q function,
Which is discussed in Sec. E.3.
When we plot the figures, for consistency, we always report the mean squared errors as the loss
functions, which are given by
'DQN(θ; θ) = (Qθ(st,at) - Irt- γ maX Qd(st+1,a0))
'MSBE(θ) = (Qθ(st, at) - rt - Y maX Qθ(st+1, a0)) ,
or,
'DQN(θ; θ) = (fθ(st, at) -T (rt + YTT (maX fd(st+1,a0))))
'MSBE(θ) = (fθ(st, at) -T (rt + YTT (maX fθ(st+1,a0))))
(31)
(32)
When we use double Q-learning (Van Hasselt et al., 2016) in our experiments, all
maXa0 Qθd(st+1, a0) and maXa0 fθd(st+1, a0) terms in the above equations are actually replaced by
Qθd (st+1, arg maXa0 Qθ (st+1, a0)) and fθd (st+1, arg maXa0 fθ (st+1, a0)). This modification do not
change the non-increasing property of LCDQN, which can be confirmed easily.
Other details We find that the exploration can often be insufficient when the game is difficult, and
we follow Van Hasselt et al. (2016) and use a slightly more complicated schedule for the C parameter
in the C-greedy policy, slowly annealing C from 0.1 to 0.01. We have a total computation budget of
5 × 107 steps for each agent, and at the j-th step of the agent, for j ≤ 50000, we set C = 1 since
the initial policy is random; for 50000 > j ≥ 106, C is exponentially annealed to 0.1 following
E = ej7τ, with T = 106/ln(0.1); for 106 > j ≥ 4 X 107, E is linearly decreased from 0.1 to 0.01;
for j > 4 × 107 we set C = 0.01. This strategy allows C to stay above 0.01 for a fairly long time
and facilitates exploration to mitigate the effects of local optima. We use this E schedule in all of our
experiments.
21
Published as a conference paper at ICLR 2022
We set the learning rate to be 6.25 × 10-5 following Hessel et al. (2018) unless otherwise specified.
We use gradient clipping in the gradient descent iterations, using the maximal `2 norm of 10 in
Sec. 5.1 and 5.2, and 5 in Sec. 5.3. The a hyperparameter for the Adam optimizer follows Hessel
et al. (2018) and is set to be 1.5 × 10-4 in Sec. 5.2, but in Sec. 5.1 it is set to be 1.5 × 10-4 for
DQN, and 5 × 10-5 for C-DQN and 5 × 10-6 for RG, becaue we observe that the sizes of the
gradients are different for DQN, C-DQN and RG. a is set to be 10-6 in Sec. 5.3 and E.3. The
weight parameters in the neural networks are initialized following He et al. (2015), and the bias
parameters are initialized to be zero.
E.2 Prioritized sampling
In our experiments we have slightly modified the original prioritized sampling scheme proposed by
Schaul et al. (2015). In the original proposal, in gradient descent optimization, a transition data
di = (st, at, rt, st+1) ∈ S is sampled with priority pi, which is set to be
Pi = (∣δi∣ + Ep)α,	(33)
where Ep is a small positive number, α is a sampling hyperparameter, and ∣δi∣ is the evaluated
Bellman error when di was sampled last time in gradient descent, which is
∣δi(DQN)∣ = ∣Qθ(st,at) - rt - YmaXQd(St+ι,a0)∣	(34)
for DQN and
∣δi(RG) ∣ = ∣∣Qθ (st, at) - rt - γ ma0x Qθ (st+1, a0)∣∣ ,	(35)
∣δi( C-DQN) ∣ = max {∣δi( DQN )∣,∣δi( RG )∣} ,	(36)
as we have chosen for RG and C-DQN, respectively. The probability for di to be sampled is Pi =
pi
------.To correct the bias that results from prioritized sampling, an importance sampling weight
j pj
wi is multiplied to the loss computed on di , which is given by
Wi = (PNpj ∙ P) ,	(37)
where N is the total number of data and β is an importance sampling hyperparameter. The bias
caused by prioritized sampling is fully corrected when β is set to be.
wi
Schaul et al. (2015) propose using Wi := ----instead of Wi, so that the importance sampling
maxj wj
weight only reduces the size of the gradient. However, we find that this strategy would make the
learning highly dependent on the hyperparameter Ep in Eq. (33), because given a data with vanish-
ingly small ∣δi ∣, its corresponding priority is pi ≈ Epα, and therefore the term maxj Wj becomes
maxj wj ≈ (PNPP ∙ E-α)" H E-aβ. As a result, the gradient in learning is scaled by the term
maxj Wj which is controlled by α, β and Ep, and maxj Wj changes throughout training and typi-
cally increases when the average of ∣δi ∣ becomes large. For a given Ea hyperparameter in the Adam
optimizer, the overall decrease of the gradient caused by maxj Wj is equivalent to an increase of
Ea, which effectively anneals the size of the update steps of the gradient descent. This makes Ep an
important learning hyperparameter, as also noted by Fujimoto et al. (2020), although this hyperpa-
rameter has been ignored in most of the relevant works including the original proposal. The results
on Space Invaders for different values of Ep are plotted in Fig. 14, which use the experimental set-
tings in Sec. 5.1. It can be seen that the performance is strongly dependent on Ep. This issue may
partially explain the difficulties one usually encounters when trying to reproduce published results.
Lower bounded prioritization To remove this subtlety, we use the original importance sampling
weight Wi instead of Wi. As ∣δi∣ is heavily task-dependent, to remove the dependence of pi on EP
P pj
for all the tasks, We make use of the average P := N ' to bound p from below instead of simply
using Ep so as to prevents pi from vanishing. Specifically, we set pi to be
Pi = max[(∣δi∣ + Ep)α Jp-∖ ,	(38)
22
Published as a conference paper at ICLR 2022
frames ×108	frames ×108
Figure 14: Training performance and loss for DQN on Space Invaders, with different hyperparam-
eters p and following the prioritization scheme in Schaul et al. (2015). The loss is calculated by
multiPlying Wi and 'DQN for each sampled data.
where we set e? to be a vanishingly small number 10-10, and Cp > 1 is a prioritization hyperparam-
eter that controls the lower bound relative to the average. In our experiments We set Cp = 10. This
scheme makes sure that regardless of the size of ∣δ∕, a data is always sampled with a probability
that is at least ^N, and Wi is bounded by Ce from above provided that the total priority Pj Pj does
not change too quickly. We adopt this prioritization scheme in all of our experiments except for the
experiments in Fig. 14 above. Compared to Fig. 14, it can be seen that our DQN loss and C-DQN
loss on Space Invaders in Fig. 4 do not change as much during training.
Details of setting When a new data is put into the replay memory, it is assigned a priority that is
equal to the maximum of all priorities pi in the memory that have been calculated using Eq. (38),
and at the beginning of learning when gradient descent has not started, we assign the collected data a
priority equal to 100. We also additionally bound Wi from above by 2cCp, so that Wi does not become
too large even if the total priority Pj pj fluctuates. The hyperparameter β is linearly increased from
0.4 to 1 during the 5 × 107 steps of the agent, following Schaul et al. (2015), and α is 0.9 in Sec. 5.3
and E.3 and is 0.6 otherwise. We did not try other values of α.
In an attempt to improve efficiency, whenever we use ∣δi∣ to update the priority Pi of a transition
(st, at, rt, st+ι), we also use its half 卑 to compute (牛 + ep)α, and use it as a lower bound to
update the priority of the preceding transition (st-1, at-1, rt-1, st). This accelerates learning by
facilitating the propagation of information. We adopt this strategy in all of our experiments except
in Sec. 5.2 and in Fig. 14, in order to make sure that the results are not due to this additional strategy.
E.3 Evaluation of the discount factor and normalization of the learned Q
FUNCTION
Evaluation of the discount factor As discussed in Sec. B.5, some tasks require a large discount
factor γ, while for many other tasks, a large γ slows down the learning significantly and make the
optimization difficult. Therefore, we wish to find a method to automatically determine a suitable
γ for a given task. As an attempt, we propose a heuristic algorithm to approximately estimate the
frequency of the reward signal in an episode, based on which we can determine γ. The algorithm is
described in the following.
Given an episode Ek in the form of an ordered sequence of rewards Ek = (ri)iT=k0-1, for which sTk
is a terminal state,8 we wish to have an estimate of the average number of steps needed to observe
the next non-negligible reward when one starts from i = 0 and moves to i = Tk. Suppose that all
rewards {ri} are either 0 ora constant r(1) 6= 0; then, one may simply count the average number of
steps before encountering the next reward r(1) when one moves from i = 0 to Tk . However, such
a simple strategy does not correctly respect the different kinds of distributions of rewards in time,
such as equispaced rewards and clustered rewards, and this strategy is symmetric with regard to the
time reversal 0 - Tk, which is not satisfactory.9 Therefore, we use a weighted average instead. To
8We consider rt to be the reward that is obtained when moving from state st to st+1.
9This is because if one almost encounters no reward at the beginning but encounters many rewards at the
end of an episode, the time horizon for planning should be large; however, if one encounters many rewards at
the beginning but almost encounters no reward at the end, the time horizon is not necessarily large. Therefore,
23
Published as a conference paper at ICLR 2022
this end we define the return
Tk-1
Ri(Ek) := X rt,
(39)
t=i
which is the sum of the rewards that are to be obtained starting from time step i, and we compute
the average number of steps to see the next reward weighted by this return as
ʌ,.
^(Ek)
Ei=O 尼(*k"i,	li ：= min{t | t ≥ I Irt = r(1)} — i + 1
pT0k=o1 R (Ek)
(40)
where li is the number of time steps to encounter the next reward starting from the time step i. This
strategy does not respect the time reversal symmetry as it involves Ri , and as the sum is taken over
the time steps {0, 1, ...Tk - 1}, it can distinguish between clustered rewards and equispaced rewards,
and it also properly takes the distance between consecutive clusters of rewards into account.
The next problem is how we should deal rewards that have various different magnitudes. As we
can deal with the case of the rewards being either 0 or a constant, we may decompose a trajectory
containing different magnitudes of rewards into a few sub-trajectories, each of which contains re-
wards that are either 0 or a constant, so that we can deal with each of these sub-trajectories using
Eq. (40), and finally use a weighted average of l over those sub-trajectories as our result. With a
set of episodes {Ek}, we treat each of the episodes separately, and again, use a weighted average
over the episodes as our estimate. The algorithm is given by Alg. 1. The weights in line 3 in Alg. 1
ensure that when computing the final result, episodes with a zero return can be ruled out, and that an
episode with a large return does not contribute too much compared with the episodes with smaller
returns. We have intentionally used the inverse in line 10 in Alg. 1 and used the root mean square in
line 12 in order to put more weight on episodes that have frequent observations of rewards. Taking
the absolute values of rewards in line 2 is customary, and in fact, one can also separate the negative
and positive parts of rewards and treat them separately. Note that the output f should be divided
by a factor of 2 to correctly represent the frequency of rewards. We also notice that the above ap-
proach can actually be generalized to the case of continuous variables of reward and time, for which
integration can be used instead of decomposition and summation.
To obtain the discount factor γ, we set the time horizon tobe fγ, with a time-horizon hyperparameter
Cγ, and then we set Y = 1 - f. To make Y close to 0.9998 for the difficult games discussed in
Sec. 5.3, we have set Cγ = 15. However, later we noticed that the agent actually learns more
efficiently with a smaller γ, and Cγ may be set to range from 5 to 15. In Sec. 5.3, Cγ = 15 is used,
but in the following experiments we use Cγ = 10.
We make use of the complete episodes in the initial 50000 transitions collected at the beginning of
training to evaluate Y, and here we also regard the loss ofa life in the game as the end ofan episode.
We clip the obtained Y so that it lies between 0.99 and 0.9998, and Y is set to be 0.9998 ifno reward
is observed.
Normalization of the Q function In addition to using the transformation function T(∙) to squash
the Q function as described in Eq. (28) and (30), we normalize the Q function by the scale of the
reward since the tasks in Atari 2600 have vastly different magnitudes of rewards. For an episode Ek,
the Q function, or the value function, as the discounted return in the episode is given by
Tk-1
Qi;k =	Yt-irt,
t=i
(41)
and We compute its mean μ by taking the average of Qi；k over all states in given sample episodes.
The standard deviation of Qi；k, however, has a dependence on Eγ. If we simply normalize Qi；k by
its standard deviation, the magnitude of the reWard signal after normalization becomes dependent
on the hyperparameter Cγ, which we wish to avoid. To obtain a normalization that is independent of
Cγ, we assume that rewards are i.i.d. variables with mean μr and standard deviation σr. Focusing on
the discount factor should not be evaluated by a method that respects the time reversal. Note that the Fourier
transform also respects the time reversal and thus does not suffice for our purpose.
24
Published as a conference paper at ICLR 2022
Algorithm 1 Estimation of the expected frequency of observing a next reward signal
1:
2:
3:
4:
5:
6:
7:
8:
9:
Input: sample episodes {Ek }
Output: an estimate of the inverse of the number of time steps f
for Ek ∈ {Ek } do
Ek = (ri)T=k-1 J (∣ri∣)l^=-1
Wk J √R画)
j J 0
while rewards Ek
jJj+1
. Taking the absolute value of reward
. For computing a weighted average over different episodes
(ri)iT=k0-1 are not all zero do
10:
11:
12:
13:
14:
15:
16:
Ek(j), Ek JDECOMPOSESEQUENCE(Ek)
end while
Compute lk J PjPR0?(；(l(Ek ) . Weighting the results by the contribution of the rewards
fk J lk
end for	________
Compute f J Jg Wwfk	. We use RMS to have more emphasis on episodes with larger fk
return f
17:
18:
19:
20:
procedure DecomposeSequence(E)
r0 J min{ri}iT=-01 = minE
E0 J (ri00)iT=-01, ri00 :=	00, ifri =0
i i=0 i r0 , otherwise
EJ (ri -ri00)iT=-01
return E0, E
end procedure
the Q function at the initial states, i.e. Q0;k , we obtain the relation
1 - γTk
E [Q0;k] = ~~.	μr,
1-γ
and therefore we estimate μr by μr = N Pk Qo；k 二Yk, with the number of sample episodes NE.
Also, We have
(42)
1 - γTk
Var ( Q0;k - -1	μr
1-Y2Tk 2
2	2 σ r ,
1 - γ2 r
(43)
and therefore σr can be estimated by the variance of {(Q0;k — I--Tk μr) ∙ JI--YTk }.
After obtaining the standard deviation of rewards σr , we need to compute a scale σ to normalize
the learned Q function. To avoid Cγ dependence of σ, we use σr to roughly predict the standard
deviation of Qo；k if γo ≡ 1 一 f is used as the discount factor, and we use the obtained standard
deviation as the normalization factor σ . For simplicity we ignore the variance of Tk and obtain
1 X ∕1-^k
σ = σr∙ NE ∑V Lr
(44)
Finally, the function We let the agent learn is Q ≡ Q-μ.
The normalized function Q also obeys the Bellman equation as Well, but With a slightly modified
reWard. It can be easily shoWn that
A*/	、	rt - (I - Y)μ
Q (st, at)=-----------------
σ
,	A * /	/ ∖ ∙ Γ∙	♦	.	∙ 1
+ γ max Q (st+ι, a0) if st+ι is non-terminal,
a0
(45)
and
Q*(st,at) = rt——μ if st+ι is terminal.
(46)
σ
25
Published as a conference paper at ICLR 2022
Figure 15: Training performance for C-DQN on several games in Atari 2600 compared with the
human performance (Badia et al., 2020) and the double DQN (Hessel et al., 2018), using the same
experimental setting as in Sec. 5.3, except for using Cγ = 10.
Therefore, the effect of normalization amounts to modifying the reward rt to ^ := rt-(σ一γ)μ,
and then assigning an additional terminal reward - γμ. ThiS is easy to implement and We use this
normalization in our experiments in Sec.5.3. Similarly to the evaluation ofγ, we use the episodes in
the initial 50000 transitions to compute μ and σ; however, here We do not regard the loss of a life as
the end of an episode, so that the lengths of the episodes do not become too short. We also do not
take episodes that have a zero return into account.
Results on other Atari games To demonstrate the generality of the above strategy, we report our
results on several games in Atari 2600, using a learning rate of 4 X 10-5 and Cγ = 10. The results
are shown in Fig. 15. We find that for some games, especially Hero, the learning sometimes gets
trapped in a local optimum and learning may stagnate, which deserves further investigation. We did
not do a fine grid search on the learning rate and we simply selected from 6 × 10-5, 4 × 10-5 and
2 × 10-5, and we chose 4 × 10-5, because it produces reasonable results for most of the tasks. We
notice that it is difficult to find a learning rate that works well for all the tasks, as the tasks have
drastically different levels of stochasticity and are associated with different time horizons.
We also notice that there are several cases where our strategy of the normalization and the evaluation
of γ does not give satisfactory results. This is mainly because we have assumed that the time
scales of obtaining rewards are similar for the random policy and for a learned policy. A typical
counterexample is the game Breakout, where the random policy almost obtains no reward in an
episode but a learned policy frequently obtains rewards. Therefore, our strategy above is still not
general enough to deal with all kinds of scenarios, and it cannot replace the bandit algorithm in
Badia et al. (2020) which is used to select γ, and therefore a better strategy is still desired.
F	Experimental details on cliff walking
In the cliff walking experiments, we store all state-action pairs into a table, excluding the states
of goal positions and cliff positions, and excluding the actions that go into the walls. We plot the
data on a log scale of iteration steps by explicitly evaluating the loss over all state-action pairs in
Sec. 3.1, and evaluating the reward of the greedy policy in Sec. 3.2. Because we do evaluations at
equal intervals on a log scale of the x-axis, fewer evaluations are made when the number of iteration
steps is larger, and as a consequence, the scatter plots in the right of Fig. 1 do not have equally many
data points along the curves. The learning rate α is always 0.5, and in the -greedy policy is always
fixed. Specifically for the one-way cliff walking task in Fig. 3, when two actions a1 and a2 have the
same Q function value, i.e. Q(st, a1) = Q(st, a2), the greedy policy randomly chooses a1 or a2 at
state st, and when maxa0 Q(st+1, a0) = Q(st+1 , a1) = Q(st+1, a2), we modify the learning rule
26
Published as a conference paper at ICLR 2022
of RG for transition (st, at, rt, st+1) to be
∆Q(st, at) = α rt + γma0xQ(st+1, a0) - Q(st, at) ,
γ
δQ(St+1, a1) = δQ(St+1, a2) = -2δQ(St, at),
so that the greedy policy at st+1 is not changed after learning from the transition.
(47)
27