Published as a conference paper at ICLR 2022
On the Pitfalls of Heteroscedastic Uncertainty
Estimation with Probabilistic Neural Networks
Maximilian Seitzer1, Arash Tavakoli1, Dimitrije Antic2, Georg Martius1
1	Max Planck Institute for Intelligent Systems, Tubingen, Germany
2	University of Tubingen, Tubingen, Germany
maximilian.seitzer@tuebingen.mpg.de
Ab stract
Capturing aleatoric uncertainty is a critical part of many machine learning systems.
In deep learning, a common approach to this end is to train a neural network to
estimate the parameters of a heteroscedastic Gaussian distribution by maximizing
the logarithm of the likelihood function under the observed data. In this work, we
examine this approach and identify potential hazards associated with the use of
log-likelihood in conjunction with gradient-based optimizers. First, we present a
synthetic example illustrating how this approach can lead to very poor but stable
parameter estimates. Second, we identify the culprit to be the log-likelihood loss,
along with certain conditions that exacerbate the issue. Third, we present an
alternative formulation, termed β-NLL, in which each data point’s contribution
to the loss is weighted by the β-exponentiated variance estimate. We show that
using an appropriate β largely mitigates the issue in our illustrative example.
Fourth, we evaluate this approach on a range of domains and tasks and show that
it achieves considerable improvements and performs more robustly concerning
hyperparameters, both in predictive RMSE and log-likelihood criteria.
1	Introduction
Endowing models with the ability to capture uncertainty is of crucial importance in machine learning.
Uncertainty can be categorized into two main types: epistemic uncertainty and aleatoric uncertainty
(Kiureghian & Ditlevsen, 2009). Epistemic uncertainty accounts for subjective uncertainty in the
model, one that is reducible given sufficient data. By contrast, aleatoric uncertainty captures the
stochasticity inherent in the observations and can itself be subdivided into homoscedastic and
heteroscedastic uncertainty. Homoscedastic uncertainty corresponds to noise that is constant across
the input space, whereas heteroscedastic uncertainty corresponds to noise that varies with the input.
There are well-established benefits for modeling each type of uncertainty. For instance, capturing
epistemic uncertainty enables effective budgeted data collection in active learning (Gal et al., 2017),
allows for efficient exploration in reinforcement learning (Osband et al., 2016), and is indispensable
in cost-sensitive decision making (Amodei et al., 2016). On the other hand, quantifying aleatoric
uncertainty enables learning of dynamics models of stochastic processes (e.g. for model-based or
offline reinforcement learning) (Chua et al., 2018; Yu et al., 2020), improves performance in semantic
segmentation, depth regression and object detection (Kendall & Gal, 2017; Harakeh & Waslander,
2021), and allows for risk-sensitive decision making (Dabney et al., 2018; Vlastelica et al., 2021).
We examine a common approach for quantifying aleatoric uncertainty in neural network regression.
By assuming that the regression targets follow a particular distribution, we can use a neural network
to predict the parameters of that distribution, typically the input-dependent mean and variance when
assuming a heteroscedastic Gaussian distribution. Then, the parameters of the network can be learned
using maximum likelihood estimation (MLE), i.e. by minimizing the negative log-likelihood (NLL)
criterion using stochastic gradient descent. This simple procedure, which is the de-facto standard (Nix
& Weigend, 1994; Lakshminarayanan et al., 2017; Kendall & Gal, 2017; Chua et al., 2018), is known
to be subject to overconfident variance estimates. Whereas strategies have been proposed to alleviate
this specific issue (Detlefsen et al., 2019; Stirn & Knowles, 2020), we argue that an equally important
1
Published as a conference paper at ICLR 2022
.3 .0 .3
0. 0. -.
Y tegraT
I	I	I
0	5	10
Input X
0.3
0.0
0.00
00
ESMR
0.25	0.50	0.75	1.00
Update Steps	×107
Figure 1: Training a probabilistic neural network to fit a simple sinusoidal fails. Left: Learned
predictions (orange line) after 107 updates, with the shaded region showing the predicted standard
deviation. The target function is given by y(x) = 0.4 sin(2πx) + ξ, where ξ is Gaussian noise with a
standard deviation of 0.01. Right: Root mean squared error (RMSE) over training, mean and standard
deviation over 10 random seeds. For comparison, we plot the training curve when using the mean
squared error as the training objective - achieving an optimal mean fit (dashed line) in 105 updates.
This behavior is stable across different optimizers, hyperparameters, and architectures (see Sec. B.2).
issue is that this procedure can additionally lead to subpar mean fits. In this work, we analyze and
propose a simple modification to mitigate this issue.
Summary of contributions We demonstrate a pitfall of optimizing the NLL loss for neural network
regression, one that hinders the training of accurate mean predictors (see Fig. 1 for an illustrative
example). The primary culprit is the high dependence of the gradients on the predictive variance.
While such dependence is generally known to be responsible for instabilities in joint optimization of
mean and variance estimators (Takahashi et al., 2018; Stirn & Knowles, 2020), we identify a fresh
perspective on how this dependence can further be problematic. Namely, we hypothesize that the
issue arises due to the NLL loss scaling down the gradient of poorly-predicted data points relative to
the well-predicted ones, leading to effectively undersampling the poorly-predicted data points.
We then introduce an alternative loss formulation, termed β-NLL, that counteracts this by weighting
the contribution of each data point to the overall loss by its β-exponentiated variance estimate, where
β controls the extent of dependency of gradients on predictive variance. This formulation subsumes
the standard NLL loss for β = 0 and allows to lessen the dependency of gradients on the variance
estimates for 0 < β ≤ 1. Interestingly, using β = 1 completely removes such dependency for
training the mean estimator, yielding the standard mean squared error (MSE) loss - but with the
additional capacity of uncertainty estimation. Finally, we empirically show that our modified loss
formulation largely mitigates the issue of poor fits, achieving considerable improvements on a range
of domains and tasks while exhibiting more robustness to hyperparameter configurations.
2	Preliminaries
Let X, Y be two random variables describing the input and target, following the joint distribution
P(X, Y ). We assume that Y is conditionally independent given X and that it follows some probability
distribution P (Y | X). In the following, we use the common assumption that Y is normally
distributed given X; i.e. P(Y | X) = N(μ(X), σ2(X)), where μ: RM → R and σ2: RM → R+
are respectively the true input-dependent mean and variance functions.1 Equivalently, we can write
Y = μ(X)+e(X), with e(X)〜N(0, σ2(X)); i.e. Y is generated from X by μ(X) plus a zero-mean
Gaussian noise with variance σ2(X). This input-dependent variance quantifies the heteroscedastic
uncertainty or input-dependent aleatoric uncertainty.
To learn estimates μ(X),σ2(X) of the true mean and variance functions, it is common to use a
neural network fθ parameterized by θ. Here, μ(X) and σ2(X) can be outputs of the final layer (Nix
& Weigend, 1994) or use two completely separate networks (Detlefsen et al., 2019). The variance
output is hereby constrained to the positive region using a suitable activation function, e.g. softplus.
1For notational convenience, we focus on univariate regression but point out that the work extends to the
multivariate case as well.
2
Published as a conference paper at ICLR 2022
feature granularity
Early
LUUUUU
Sample weighting
任了	JfactorlO
feature granularity
Later
Sample weighting
Cy amplification
variaπce-weighted gradients P(IE) OC
B = Q (NLL)
/3 = 0.5
t- x	ɪ factor 32 χ一
B = I (MSEfor mean)
P(®) oc 1
匕」
Figure 2: Illustration of the pitfall when training with NLL (negative log-likelihood) versus our
solution. An initial inhomogeneous feature space granularity (see Sec. 3.1) results early on in different
fitting quality. The implicit weighting of the squared error in NLL can be seen as biased data-sampling
with p(x) 8 σ2(X) (See Eq∙ 6)∙ Badly fit parts are increasingly ignored during training. On the right,
the effect of our solution (Eq. 7) on the relative importance of data points is shown.
The optimal parameters θNLL can then be found using maximum likelihood estimation (MLE) by
minimizing the negative log-likelihood (NLL) criterion LNLL under the distribution P(X, Y ):
θNLL = argminLnll(θ) = argmin E 1 logσ2
θ	θ X,Y 2
(X) + (Y2-2^(X)) +const] ∙	(1)
2σ 2 (X )
In contrast, standard regression minimizes the mean squared error (MSE) LMSE :
θM SE=argmin LMSE ⑻=argmin χEY [(Y - μx ))2 ]∙	⑵
In practice, Eq. 1 and Eq. 2 are optimized using stochastic gradient descent (SGD) with mini-batches
of samples drawn from P(X, Y). The gradients of LNLL w.r.t. (with respect to) μ(X), σ2(X) are
given by
丁 LNLL⑻=XEYPXX1
V^2 Lnll (θ) = E
X,Y
σ2(X )-(Y - μ(X ))2
2(σ2(x ))2
(3, 4)
3	Analysis
We now return to the example of trying to fit a sinusoidal function from Sec. 1. Recall from Fig. 1
that using the Gaussian NLL as the objective resulted in a suboptimal fit. In contrast, using MSE as
the objective the model converged to the optimal mean fit in a reasonable time. We now analyze the
reasons behind this surprising result.
From Eq. 3, we see that the true mean μ(X) is the minimizer of the NLL loss. It thus becomes
clear that a) the solution found in Fig. 1 is not the optimal one, and b) the NLL objective should,
in principle, drive μ(X) to the optimal solution μ(X). So, why does the model not converge to the
optimal solution? We identify two main culprits for this behavior of the Gaussian NLL objective:
1.	Initial flatness of the feature space can create an undercomplex but locally stable mean fit.
This fit results from local symmetries and requires a form of symmetry breaking to escape.
2.	The NLL loss scales the gradient of badly-predicted points down relative to well-predicted
points, effectively undersampling those points. This effect worsens as training progresses.
These culprits and their effect on training are illustrated in Fig. 2 (left). If the network cannot fit a
certain region yet because its feature space (spanned by the last hidden layer) is too coarse, it would
then perceive a high effective data variance. This leads to down-weighting the data from such regions,
fueling a vicious cycle of self-amplifying the increasingly imbalanced weighting. In the following,
we analyze these effects and their reasons in more detail.
3.1	S ymmetry and Feature Non-Linearity
It is instructive to see how the model evolves during training as shown in Fig. 3. The network first
learns essentially the best linear fit while adapting the variance to match the residuals. The situation
3
Published as a conference paper at ICLR 2022
(a) After 1 000 updates.
(b) After 10 000 updates.
.4 .0 .4
0. 0. -0.
Y tegraT
0	3	6	9	12
Input X
(c) After 500 000 updates.
.4 .0
00-
Y tegraT
0	3	6	9	12	0	3	6	9	12
Input X	Input X
Figure 3: Model fit using the NLL loss at different stages of training shown in orange with ±σ
uncertainty band. Black dots mark training data. Fitting the function begins from the left and is
visibly slow.
is locally stable. That is, due to the symmetries of errors below and above the mean fit, there is no
incentive to change the situation. Symmetry breaking is required for further progress. One form of
symmetry breaking comes with the inherent stochasticity of mini-batch sampling in SGD, or the
natural asymmetries contained in the dataset due to, e.g., outliers. Moreover, we hypothesize that the
local non-linearity of the feature space plays an important role in creating the necessary non-linear fit.
Let us consider the non-linearity of the feature space. This quantity is not easy to capture. To
approximate it for a dataset D, we compute how much the Jacobian Jf of the features f (x) w.r.t. the
input varies in an L2-ball with radius r around a point x, denoted as the Jacobian variance:2
V(X) = ∣B1-∣	X	(Jf(XO) -	∣B1-∣	X	Jf (XOO))	,	Bx	= {x0 ∈ D: kx -x0k2 ≤ r}.⑸
Figure 4 visualizes the Jacobian variance over the input space as a function of the training progress.
Although initially relatively flat, it becomes more granular in parts of the input space, the parts which
are later well fit. The region with low Jacobian variance remains stuck in this configuration (see
Fig. 1). This provides evidence that the non-linearity of the feature space is important for success or
failure of learning on this dataset. However, why does gradient descent not break out of this situation?
3210
601×spetSetadpU
oOUBμυΛ Uwq0。Ef
.
3210
601×spetSetadpU
ytilibaborP gnilpma
24
-
0	4	8	12
Input X
Figure 4:	Jacobian variance over training time,
using the mean of matrix V(X) (see Eq. 5).
0	4	8	12
Input X
Figure 5:	Probability of sampling a data point at
input X over training time.
3.2	Inverse-Variance Weighting Effectively Undersamples
The answer lies in an imbalanced weighting of data points across the input space. Recall that the
gradient V^LNLL of the NLL w.r.t. the mean scales the error μ(X) - Y by ^2")(Eq. 3). As
symmetry is broken and the true function starts to be fit locally, the variance quickly shrinks in
these areas to match the reduced MSE. If the variance is well-calibrated, the gradient becomes
μ^⅞-Y ≈(左(X)--Y)2 = μ(x1)-γ. Data points With already low error will get their contribution in
the batch gradient scaled up relatively to high error data points - “rich get richer” self-amplification.
Thus, NLL acts contrary to MSE which focuses on high-error samples. If the true variance σ2 on
the well-fit regions is much smaller than the errors on the badly-fit regions, or there are much more
well-fit than badly-fit points, then learning progress is completely hindered on the badly-fit regions.
2This is a form of approximative second-order derivative computed numerically, which also gives non-zero
results for networks with relu activation (in contrast to, for example, the Hessian).
4
Published as a conference paper at ICLR 2022
Another way to view this is to interpret the different weighting of points as changing the training
distribution P(X, Y ) to a modified distribution P(X, Y ) in which points with high error have a
lower probability of getting sampled. This can be shown by defining P(X,Y) = Z-1 P2X(X),
where Z = R PXx) dxdy is a normalizing constant, and recognizing that the gradient of the NLL is
proportional to the gradient of the MSE loss in Eq. 2 under the modified data distribution P (X, Y):
NμLNLL(O) = Z ∙ EX,Y~P(X,Y)[μ(X) - Y] H V^Eχ,γZP(X,Y)
-(Y - μ(x ))2 -
2
(6)
In Fig. 5, we plot P(X, Y) over training time for our sinusoidal example. It can be seen that the
virtual probability of sampling a point from the high-error region drops over time until it is highly
unlikely to sample points from this region (10-5 as opposed to 10-3 for uniform sampling). We
show that this behavior also carries over to a real-world dataset in Sec. B.3.
Sometimes, “inverse-variance weighting” is seen as a feature of the Gaussian NLL (Kendall & Gal,
2017) which introduces a self-regularizing property by allowing the network to “ignore” outlier
points with high error. This can be desirable if the predicted variance corresponds to data-inherent
unpredictability (noise), but it is undesirable if it causes premature convergence and ignorance of hard-
to-fit regions, as shown above. In our method, we enable control over the extent of self-regularization.
4	Method
In this section, we develop a solution method to mitigate these issues with NLL training. Our
approach, which we term β -NLL, allows choosing an arbitrary loss-interpolation between NLL and
MSE while keeping calibrated uncertainty estimates.
4.1	Variance-Weighting the Gradients of the NLL
The problem we want to address is the premature convergence of NLL training to highly suboptimal
mean fits. In Sec. 3, we identified the relative down-weighting of badly-fit data points in the NLL
loss together with its self-amplifying characteristic as the main culprit. Effectively, NLL weights
the mean-squared-error per data point with σ⅛, which can be interpreted as sampling data points
with P(x) H σ2. Consequently, we propose modifying this distribution by introducing a parameter
β allowing to interpolate between NLL’s and a completely uniform data point importance. The
2β
resulting sampling distribution is given by P(x) H % and illustrated in Fig. 2 (right).
How could this weighting be achieved? We simply introduce the variance-weighting term σ2β to the
LNLL loss such that it acts as a factor on the gradient. We denote the resulting loss as β-NLL:
Le-NLL= E [bσ2β(X)C (1 logσ2(X) + (Y - J(X))2 + const)],	⑺
X,Y	2	2σ (X )
where [∙[ denotes the stop gradient operation. By stopping the gradient, the variance-weighting term
acts as an adaptive, input-dependent learning rate. In this way, the gradients of Lβ-NLL are:
VL (θ)= E Γμ(X)- Y] VL	(θ)= E Γσ2(X)-(Y -μ(X))2]
V^Le-NLL (θ) =	XEY	σι2-2β(X)	,	NσLe-NLL (θ)	—	XEY	2σ4-2e(X)	∙	(8,9)
Naturally, for β = 0, we recover the original NLL loss. For β = 1 the gradient w.r.t. μ in Eq. 8 is
equivalent to the one of MSE. However, for the variance, the gradient in Eq. 9 is a new quantity with
2σ2 in the denominator. For values 0 < β < 1, we get different loss interpolations. Particularly
interesting is the case of β = 0.5, where the data points are weighted with σ (inverse standard
deviation instead of inverse variance). In our experiments (Sec. 5), we find that β = 0.5 generally
achieves the best trade-off between accuracy and log-likelihood. A Pytorch implementation of the
loss function is provided in Sec. D.5.
Note that the new loss Le-NLL is not meant for performance evaluation, rather it is designed to result
in meaningful gradients. Due to the weighting term, the loss value does not reflect the model’s quality.
The model performance during training should be monitored with the original negative log-likelihood
objective and, optionally, with RMSE for testing the quality of the mean fit.
5
Published as a conference paper at ICLR 2022
(a) LNLL	(b) LMSE
Figure 6: Distribution of residual prediction errors depending on the loss function for the ObjectSlide
dataset (see Sec. 5.2). Dashed lines show predictive RMSE. (a) The NLL loss (LNLL) yields
multimodal residuals. There is a long tail of difficult data points that are ignored, while easy ones are
fit to high accuracy. (b) The MSE loss (LMSE) results in a log-normal residual distribution. (c) Our
β-NLL loss (Lβ-NLL) yields highly accurate fits on easy data points without ignoring difficult ones.
4.2	Allocation of Function Approximator Capacity
Even though LMSE, LNLL, and Lβ-NLL all have the same optima w.r.t. the mean (and also the
variance in the case of LNLL and Lβ-NLL), optimizing them leads to very different solutions. In
particular, because these losses weight data points differently, they assign the capacity of the function
approximator differently. Whereas the MSE loss gives the same weighting to all data points, the
NLL loss gives high weight to data points with low predicted variance and low weight to those with
high variance. β-NLL interpolates between the two. The behavior of the NLL loss is appropriate
if these variances are caused by true aleatoric uncertainty in the data. However, due to the use of
function approximation, there is also the case where data points cannot be well predicted (maybe
only transiently). This would result in high predicted variance, although the ground truth is corrupted
by little noise. The different loss functions thus vary in how they handle these difficult data points.
An example of how the differences between the losses manifest in practice is illustrated in Fig. 6.
Here we show the distribution of the residuals for a dynamics prediction dataset containing easy
and hard to model areas. The NLL loss essentially ignores a fraction of the data by predicting high
uncertainty. By analyzing those data points, we found that they were actually the most important
data points to model correctly (because they captured non-trivial interactions in the physical world).
How important are the data points with high uncertainty? Are they outliers (i.e. do they stem from
truly noisy regions), to which we would be willing to allocate less of the function approximator’s
capacity? Or are they just difficult samples that are important to fit correctly? The answer is task-
dependent and, as such, there is no one-loss-fits-all solution. Rather, the modeler should choose
which behavior is desired. Our β-NLL loss makes this choice available through the β parameter.
5	Experiments
In our experiments, we ask the following questions and draw the following conclusions:
Sec. 5.1: Does β-NLL fix the pitfall with NLL’s convergence?
Yes, β-NLL converges to good mean and uncertainty estimates across a range of β values.
Sec. 5.2: Does β-NLL improve over NLL in practical settings? How sensitive to hyperparameters
is β-NLL? We investigate a diverse set of real-world domains: regression on the UCI
datasets, dynamics model learning, generative modeling on MNIST and Fashion-MNIST,
and depth-map prediction from natural images.
Yes, β -NLL generally performs better than NLL and is considerably easier to tune.
Sec. 5.3: How does β-NLL compare to other loss functions for distributional regression? We
compare with a range of approaches: learning to match the moments of a Gaussian
(termed “moment matching” (MM); see Sec. A), using a Student’s t-distribution instead of
a Gaussian (Detlefsen et al., 2019), or putting different priors on the variance and using
variational inference (xVAMP, xVAMP*, VBEM, VBEM*) (Stirn & Knowles, 2020).
It depends. Different losses make different trade-offs, which we discuss in Sec. 5.3.
6
Published as a conference paper at ICLR 2022
Figure 7: Convergence properties analyzed on the sinusoidal regression problem. RMSE after 200 000
epochs, averaged over 3 independent trials, is displayed by color codes (lighter is better) as a function
of learning rate and model architecture (see Sec. D.1). The original NLL (β = 0) does not obtain
good RMSE fits for most hyperparameter settings. Figure S3 shows results for the NLL metric.
(a) NLL (b) β = 0.25	(c)β = 0.5	(d) β = 1	(e) MM (f) std. dev.
10-
o-
-10-
0	10
0	10	0	10
0	10	0	10
Figure 8: Fits for the heteroscedastic sine example from Detlefsen et al. (2019) (a-e). Dotted lines
show the ground truth mean and ±2σ, respectively. (f) The predicted standard deviations (with
shaded std. over 10 independent trials) with the same color code. Note that β = 0.5 and β = 1 graphs
lie on top of one another. Inside the training regime, all β-NLL variants (a-d) yield well-calibrated
uncertainty estimates. Moment matching (e) significantly underestimates the variance everywhere.
4 -
0-
⅛ 2
0	10
We refer the reader to Sec. C and Sec. D for a description of datasets and training settings. We
evaluate the quality of predictions quantitatively in terms of the root mean squared error (RMSE) and
the negative log-likelihood (NLL).
5.1	Synthetic Datasets
Sinusoidal without heteroscedastic noise We first perform an extended investigation of our il-
Iustrative example from Fig. 1 - a sine curve with a small additive noise: y = 0.4sin(2πx) + ξ,
with ξ being Gaussian noise with standard deviation σ = 0.01. One would expect that a network
with sufficient capacity can easily learn to fit this function. Figure 7 inspects this over a range of
architectures and learning rates.
We find that for the standard NLL loss (β = 0), the networks do not converge to a reasonable mean
fit. There is a trend that larger networks and learning rates show better results, but when comparing
this to β-NLL with β = 0.5 we see that the networks are indeed able to fit the function without any
issues. As expected, the same holds for the mean squared error loss (MSE) and β-NLL with β = 1.
The quality of the fit w.r.t. NLL is shown in Fig. S3.
Sinusoidal with heteroscedastic noise We sanity-check that β-NLL is still delivering good
uncertainty estimates on the illustrative example from Detlefsen et al. (2019) - a sine curve with
increasing amplitude and noise: y = x sin(x) + xξ1 + ξ2, with ξ1 and ξ2 being Gaussian noise with
standard deviation σ = 0.3. Figure 8 (a-e) displays the predictions of the best models (w.r.t. NLL
validation loss) and (f) compares the predicted uncertainties over 10 independent trials. Fitting the
mean is achieved with all losses. On the training range, β-NLL with β > 0 learns virtually the same
uncertainties as the NLL loss.
5.2	Real-World Datasets
UCI Regression Datasets As a standard real-world benchmark in predictive uncertainty estimation,
we consider the UCI datasets (Hernandez-Lobato & Adams, 2015). Table 1 gives an overview
comparing different loss variants. We refer to Sec. B.4 for the full results on all 12 datasets. The
results are encouraging: β-NLL achieves predictive log-likelihoods on par with or better than the
NLL loss while clearly improving the predictive accuracy on most datasets.
7
Published as a conference paper at ICLR 2022
Table 1: Results for UCI Regression Datasets. We report predictive log-likelihood and RMSE (±
standard deviation). Ties denotes the number of datasets (out of 12) for which the method cannot be
statistically distinguished from the best method (see Sec. B.4). We compare with Student-t (Detlefsen
et al., 2019) and xVAMP/VBEM (Stirn & Knowles, 2020). Section B.4 lists the full results.
LL ↑
RMSE J
Loss β Ties concrete energy naval
yacht
Ties concrete energy naval
yacht
Lβ-NLL	0	3	-3.25	± 0.31	-3.22	± 1.41	12.46	± 1.18	-2.86	±	5.18
Lβ-NLL	0.25	4	-3.31	± 0.51	-2.82	± 0.82	13.78	± 0.33	-1.97	±	1.14
Lβ-NLL	0.5	5	-3.29	± 0.36	-2.41	± 0.72	13.99	± 0.40	-2.47	±	1.68
Lβ-NLL	0.75	5	-3.27	± 0.34	-2.80	± 0.59	13.63	± 0.62	-1.87	±	0.55
Lβ-NLL	1.0	2	-3.23	± 0.33	-3.37	± 0.58	13.59	± 0.30	-2.27	±	1.07
LMM	0	-3.49	± 0.38	-4.26	± 0.50	12.73 ± 0.64	-11.2 ± 31.0
LMSE	—	—	—	—	—
Student-t	10	-3.07	± 0.14	-2.46	± 0.34	12.47 ± 0.48	-1.23 ± 0.55
xVAMP	6	-3.06 ± 0.15	-2.47	± 0.32	12.44 ± 0.60	-0.99 ±	0.33
xVAMP*	7	-3.03 ± 0.13	-2.41	± 0.32	12.80 ± 0.55	-1.04 ±	0.47
VBEM	2	-3.14 ± 0.07	-4.29	± 0.16	8.05 ± 0.13	-2.65 ±	0.10
VBEM*	8	-2.99 ± 0.13	-1.91	± 0.21	13.10 ± 0.47	-0.98 ±	0.24
5	6.08 ±	0.65	2.25	± 0.34	0.0021	± 0.0006	1.22	±	0.47
6	5.79 ±	0.74	1.81	± 0.30	0.0012	± 0.0004	1.73	±	1.00
7	5.61 ±	0.65	1.12	± 0.25	0.0006	± 0.0002	2.35	±	1.44
8	5.67 ±	0.73	1.31	± 0.45	0.0004	± 0.0001	1.97	±	1.03
9	5.55 ±	0.77	1.54	± 0.54	0.0004	± 0.0000	2.08	±	1.13
5	6.28 ±	0.82	2.19	± 0.28	0.0005	± 0.0001	3.02	±	1.38
12	4.96 ±	0.64	0.92	± 0.11	0.0004	± 0.0001	0.78	±	0.25
5	5.82 ±	0.59	2.26	± 0.34	0.0026	± 0.0009	1.34	±	0.63
8	5.44 ±	0.64	1.87	± 0.32	0.0023	± 0.0004	0.99	±	0.43
8	5.35 ±	0.73	2.00	± 0.26	0.0020	± 0.0006	1.13	±	0.66
9	5.21 ±	0.58	1.29	± 0.33	0.0009	± 0.0004	1.66	±	0.84
9	5.17 ±	0.59	1.08	± 0.17	0.0015	± 0.0005	0.65	±	0.20
(a) ObjectSlide
Figure 9: Sensitivity analysis of loss functions to hyperparameters on the dynamics model learning
tasks: ObjectSlide (a) and Fetch-PickAndPlace (b). The distributions over validation RMSE and NLL
are shown as a function of hyperparameters, based on a grid search over different model configurations
(see Sec. D.2). While the NLL loss is highly sensitive when evaluating RMSE, the β-NLL loss
shows much less sensitivity and yields good results regardless of the exact configuration.
(b) Fetch-PickAndPlace
Dynamics models As a major application of uncertainty estimation lies in model-based reinforce-
ment learning (RL), we test the different loss functions on two dynamics predictions tasks of varying
difficulty, ObjectSlide, and Fetch-PickAndPlace. In both tasks, the goal is to predict how an ob-
ject will move from the current state and the agent’s action. Whereas ObjectSlide (Seitzer et al.,
2021) is a simple 1D-environment, Fetch-PickAndPlace (Plappert et al., 2018) is a complex 3D
robotic-manipulation environment. The models are trained on trajectories collected by RL agents.
For both datasets, we perform a grid search over different hyperparameter configurations (see Sec. D.2)
for a sensitivity analysis to hyperparameters settings, presented in Fig. 9. It reveals that NLL is
vulnerable to the choice of hyperparameters, whereas β-NLL achieves good results over a wide
range of configurations. The best performing configurations for each loss are then evaluated on a
hold-out test set (Table 2). One can see that the NLL loss results in poor predictive performance
and also exhibits quite a high variance across random seeds. Our method yields high accuracy and
log-likelihood fits for a range of β values, with β = 0.5 generally achieving the best trade-off.
Generative modeling and depth-map prediction For generative modeling, we train variational
autoencoders (Kingma & Welling, 2014) with probabilistic decoders on MNIST and Fashion-MNIST.
For the task of depth regression, we modify a state-of-the-art method (AdaBins; Bhat et al. (2021))
and test it on the NYUv2 dataset (Silberman et al., 2012) with our loss (Fig. S6). Table 3 presents
selected results for both tasks, yielding similar trends as before. We refer to Sec. B.5 and Sec. B.6 for
more details, including qualitative results.
5.3	Comparison to Other Loss Functions
The previous sections have demonstrated that our β-NLL loss has clear advantages over the NLL
loss. However, the comparison to other loss functions requires a more nuanced discussion. First,
8
Published as a conference paper at ICLR 2022
Table 2: Test results for dynamics models, using best configurations found in a grid search. The
reported standard deviations are over 5 random seeds. We compare with Student-t (Detlefsen et al.,
2019) and xVAMP/VBEM (Stirn & Knowles, 2020).
Loss	β	ID-Slide		Fetch-PickAndPlace	
		RMSE ]	LL ↑	RMSE ]	LL ↑
Lβ-NLL	0	0.0192 ± 0.006	7.97 ± 3.62	0.00163 ± 0.00008	18.72 ± 7.32
Lβ-NLL	0.25	0.0107 ± 0.004	9.03 ± 0.47	0.00102 ± 0.00004	24.43 ± 1.64
Lβ-NLL	0.5	0.0064 ± 0.002	9.28 ± 0.75	0.00096 ± 0.00002	24.68 ± 0.08
Lβ-NLL	0.75	0.0087 ± 0.003	6.61 ± 1.83	0.00098 ± 0.00001	22.77 ± 0.17
Lβ-NLL	1.0	0.0074 ± 0.001	6.58 ± 0.29	0.00102 ± 0.00001	21.32 ± 0.07
LMM		0.0078 ± 0.001	diverges	0.00104 ± 0.00003	19.33 ± 1.31
LMSE		0.0068 ± 0.001	—	0.00103 ± 0.00000	—
Student-t		0.0155 ± 0.006	11.30 ± 0.03	0.00117 ± 0.00001	30.44 ± 0.08
xVAMP		0.0118 ± 0.002	10.58 ± 0.19	0.00128 ± 0.00005	29.02 ± 0.12
xVAMP*		0.0199 ± 0.006	10.89 ± 0.10	0.00128 ± 0.00001	29.19 ± 0.08
VBEM		0.0039 ± 0.000	3.79 ± 0.00	0.00104 ± 0.00003	17.39 ± 0.29
VBEM*		0.0280 ± 0.011	10.13 ± 0.49	0.00118 ± 0.00003	28.62 ± 0.15
Table 3: Selected results for generative modeling and depth-map prediction. Left: Training variational
autoencoders on MNIST and Fashion-MNIST. Right: Depth-map prediction on NYUv2. Full results
can be found in Table S3 and Table S4.
Loss	β	MNIST		Fashion-MNIST		NYUv2			
		RMSE ]	LL ↑	RMSE ]	LL ↑	Loss	β	RMSE ]	LL ↑
Lβ-NLL	0	0.237 ± 0.002	2116 ± 55	0.170 ± 0.001	1940 ± 104	Lβ-NLL	0	0.3854	-4.52
Lβ-NLL	0.5	0.151 ± 0.003	2220 ± 25	0.125 ± 0.003	1639 ± 52	Lβ-NLL	0.5	0.3789	-7.50
Lβ-NLL	1.0	0.152 ± 0.001	1706 ± 30	0.138 ± 0.002	1142 ± 26	Lβ-NLL	1.0	0.3845	-5.10
Student-t		0.273 ± 0.002	4291 ± 103	0.182 ± 0.002	2857 ± 9	LMSE		0.3776	—
xVAMP*		0.225 ± 0.001	3062 ± 215	0.160 ± 0.002	2150 ± 131	L1		0.3850	—
VBEM*		0.176 ± 0.008	3213 ± 238	0.150 ± 0.003	2244 ± 78	SI Loss		0.419	—
we also test an alternative loss function based on matching the moments of the Gaussian (LMM ; see
Sec. A). While this loss results in high accuracy, it is unstable to train and exhibits poor likelihoods.
Second, we test several loss functions based on a Student’s t-distribution, including xVAMP and
VBEM (Stirn & Knowles, 2020). These approaches generally achieve better likelihoods than the
β-NLL; we conjecture this is because their ability to maintain uncertainty about the variance results
in a better fit (in terms of KL divergence) when the variance is wrongly estimated. In terms of
predictive accuracy, β -NLL outperforms the Student’s t-based approaches. Exceptions are some of
the UCI datasets with limited data (e.g. “concrete” and “yacht”) where xVAMP and VBEM are on par
or better than β-NLL. This is likely because these methods can mitigate overfitting by placing a prior
on the variance. However, xVAMP and VBEM are also non-trivial to implement and computationally
heavy: both need MC samples to evaluate the prior; for xVAMP, training time roughly doubles as
evaluating the prior also requires a second forward pass through the network. In contrast, β-NLL is
simple to implement (see Sec. D.5) and introduces no additional computational costs.
6	Conclusion
We highlight a problem frequently occurring when optimizing probabilistic neural networks using
the common NLL loss: training gets stuck in suboptimal function fits. With our analysis, we reveal
the underlying reason: initially badly-fit regions receive increasingly less weight in the loss which
results in premature convergence. We propose a simple solution by introducing a family of loss
functions called β -NLL. Effectively, the gradient of the original NLL loss is scaled by the β-
exponentiated per-sample variance. This allows for a meaningful interpolation between the NLL
and MSE loss functions while providing well-behaved uncertainty estimates. The hyperparameter
β gives practitioners the choice to control the self-regularization strength of NLL: how important
should high-noise regions or difficult-to-predict data points be in the fitting process. In most cases,
β = 0.5 will be a good starting point. We think the problem discussed in this paper is primarily why
practitioners using the Gaussian distribution in regression or generative modeling tasks often opt
for a constant or homoscedastic (global) variance, as opposed to the more general heteroscedastic
(data-dependent) variance. We hope that our simple solution contributes to changing this situation by
improving the usability and performance of modeling data uncertainty with deep neural networks.
9
Published as a conference paper at ICLR 2022
Acknowledgments
The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS)
for supporting Maximilian Seitzer. Georg Martius is a member of the Machine Learning Cluster
of Excellence, EXC number 2064/1 - Project number 390727645. We acknowledge the financial
support from the German Federal Ministry of Education and Research (BMBF) through the Tubingen
AI Center (FKZ: 01IS18039B).
Reproducibility S tatement
All settings are described in detail in Sec. C and Sec. D. We make full code and data available under
https://github.com/martius- lab/beta- nll.
References
Dario Amodei, Chris Olah, JaCob Steinhardt, Paul Christiano, John Schulman, and Dan Mane.
Concrete problems in AI safety. ArXiv, abs/1606.06565, 2016.
Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. AdaBins: Depth estimation using
adaptive bins. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 4008-4017, Los Alamitos, CA, USA, jun 2021. IEEE Computer Society. doi:
10.1109/CVPR46437.2021.00400. URL https://doi.ieeecomputersociety.org/
10.1109/CVPR46437.2021.00400.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. OpenAI Gym. ArXiv, abs/1606.01540, 2016.
Xiaotian Chen, Xuejin Chen, and Zheng-Jun Zha. Structure-aware residual pyramid network for
monocular depth estimation. In Proceedings of the Twenty-Eighth International Joint Conference
on Artificial Intelligence, IJCAI-19, pp. 694-700. International Joint Conferences on Artificial
Intelligence Organization, 7 2019. doi: 10.24963/ijcai.2019/98. URL https://doi.org/10.
24963/ijcai.2019/98.
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep rein-
forcement learning in a handful of trials using probabilistic dynamics models. In
S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 31. Curran Asso-
ciates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
3de568f8597b94bda53149c7d7f5958c- Paper.pdf.
Will Dabney, Georg Ostrovski, David Silver, and Remi Munos. Implicit quantile networks for
distributional reinforcement learning. In Jennifer Dy and Andreas Krause (eds.), Proceedings of
the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pp. 1096-1105. PMLR, 10-15 Jul 2018. URL https://proceedings.
mlr.press/v80/dabney18a.html.
Nicki S. Detlefsen, Martin J0rgensen, and S0ren Hauberg. Reliable training and estimation of
variance networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche—Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran As-
sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
07211688a0869d995947a8fb11b215d6-Paper.pdf.
Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model
uncertainty in deep learning. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings
of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine
Learning Research, pp. 1050-1059, New York, New York, USA, 20-22 Jun 2016. PMLR. URL
https://proceedings.mlr.press/v48/gal16.html.
Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep Bayesian active learning with image data.
In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference
10
Published as a conference paper at ICLR 2022
on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 1183-
1192. PMLR, 06-11 Aug 2017. URL https://Proceedings .mlr.press∕v70∕gal17a.
html.
Ali Harakeh and Steven L. Waslander. Estimating and evaluating regression predictive uncertainty
in deep object detectors. In International Conference on Learning Representations (ICLR), 2021.
URL https://openreview.net/forum?id=YLewtnvKgR7.
Jose Miguel Hemandez-Lobato and Ryan P. Adams. Probabilistic backpropagation for scalable
learning of Bayesian neural networks. In Francis Bach and David Blei (eds.), Proceedings of
the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine
Learning Research, pp. 1861-1869, Lille, France, 07-09 Jul 2015. PMLR. URL https://
proceedings.mlr.press/v37/hernandez-lobatoc15.html.
Alex Kendall and Yarin Gal. What uncertainties do we need in Bayesian deep learning for computer
vision? In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran As-
sociates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
2650d6089a6d640c5e85b2b88265dc2b-Paper.pdf.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations (ICLR), 2015.
Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. In International Conference
on Learning Representations (ICLR), 2014.
Armen Der Kiureghian and Ove Ditlevsen. Aleatory or epistemic? does it matter? Struc-
tural Safety, 31(2):105-112, 2009. ISSN 0167-4730. doi: https://doi.org/10.1016/j.strusafe.
2008.06.020. URL https://www.sciencedirect.com/science/article/pii/
S0167473008000556. Risk Acceptance and Risk Communication.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing
Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.
cc/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf.
Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong Suh. From big to small: Multi-scale
local planar guidance for monocular depth estimation. ArXiv, abs/1907.10326, 2019.
David A. Nix and Andreas S. Weigend. Estimating the mean and variance of the target probability dis-
tribution. In Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN’94),
volume 1, pp. 55-60, 1994. doi: 10.1109/ICNN.1994.374138.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration
via bootstrapped DQN. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 29. Curran Asso-
ciates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/file/
8d8818c8e140c64c743113f563cf750f-Paper.pdf.
Cristina Pinneri, Shambhuraj Sawant, Sebastian Blaes, and Georg Martius. Extracting strong
policies for robotics tasks from zero-order trajectory optimizers. In International Conference on
Learning Representations (ICLR), 2021. URL https://openreview.net/forum?id=
Nc3TJqbcl3.
Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell,
Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, Vikash Kumar, and Wojciech
Zaremba. Multi-goal reinforcement learning: Challenging robotics environments and request for
research. ArXiv, abs/1802.09464, 2018.
Maximilian Seitzer, Bernhard Scholkopf, and Georg Martius. Causal influence detection for im-
proving efficiency in reinforcement learning. In M. Ranzato, A. Beygelzimer, K. Nguyen, P. S.
Liang, J. W. Vaughan, and Y. Dauphin (eds.), Advances in Neural Information Processing Systems,
11
Published as a conference paper at ICLR 2022
volume 34. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/
paper/2021/file/c1722a7941d61aad6e651a35b65a9c3e-Paper.pdf.
Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support
inference from RGBD images. In Andrew Fitzgibbon, Svetlana Lazebnik, Pietro Perona, Yoichi
Sato, and Cordelia Schmid (eds.), Computer Vision - ECCV 2012,pp. 746-760, Berlin, Heidelberg,
2012. Springer Berlin Heidelberg. ISBN 978-3-642-33715-4.
Andrew Stirn and David A. Knowles. Variational variance: Simple and reliable predictive variance
parameterization. ArXiv, abs/2006.04910, 2020.
Hiroshi Takahashi, Tomoharu Iwata, Yuki Yamanaka, Masanori Yamada, and Satoshi Yagi. Student-t
variational autoencoder for robust density estimation. In Proceedings of the Twenty-Seventh
International Joint Conference on Artificial Intelligence, IJCAI-18, pp. 2696-2702. International
Joint Conferences on Artificial Intelligence Organization, 7 2018. doi: 10.24963/ijcai.2018/374.
URL https://doi.org/10.24963/ijcai.2018/374.
Marin Vlastelica, Sebastian Blaes, Cristina Pinneri, and Georg Martius. Risk-averse zero-order trajec-
tory optimization. In Aleksandra Faust, David Hsu, and Gerhard Neumann (eds.), Proceedings of
the 5th Conference on Robot Learning, volume 164 of Proceedings of Machine Learning Research,
pp. 444-454. PMLR, 08-11 Nov 2021. URL https://proceedings.mlr.press/v164/
vlastelica22a.html.
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y. Zou, Sergey Levine,
Chelsea Finn, and Tengyu Ma. MOPO: Model-based offline policy optimization. In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances
in Neural Information Processing Systems, volume 33, pp. 14129-14142. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
a322852ce0df73e204b7e67cbbef0d0a- Paper.pdf.
12
Published as a conference paper at ICLR 2022
Appendix
A	The Moment Matching Loss
We also investigated an alternative loss function designed to counter the imbalanced weighting of
data points by the NLL loss, which we call “moment matching” (MM). It uses standard squared
error losses to estimate the moments of the target distribution, i.e. directly estimating the sufficient
statistics of the target distribution. In our experiments, we found that this loss function generally fixes
the problems with premature convergence when using LNLL , but it also leads to underestimation of
variances and exhibits considerable training instabilities.
To fully describe a Gaussian distribution, only the first two moments μ, σ2 need to be estimated. So,
why not directly define losses based on the moment estimators? Starting from the conditional mean
of the targets, E[Y | X], we can define the squared deviation as a loss and see that the MSE is an
upper bound:
(E[Y ] X] - μ(x))2 = E[(Y - μ(x)) | X]2 ≤ e[(y - μ(x))21 X] ：= LMM. (si)
Notice that LMM is the standard MSE loss. Thus, learning a mean fit μ(χ) can follow the standard
(non-distributional) regression procedures. Interestingly, due to the moment matching viewpoint, we
can analogously define a loss for fitting the variance (or the second central moment). Variance is
defined as E[(Y - μ(X))2 | X]. As such, by analogy, we can define the following loss: LMM ：=
E[((Y - μ(X))2 - σ2(X))2 | x] . To use the same physical unit as LMM, We reformulate it in
terms of the standard deviation as:
LMM := E (J(Y - μ(X))2 - σ(X)) X .	(S2)
Thus, the moment matching loss can be expressed simply by the sum of the two losses: LMM =
LMM + Lmm∙ We found that using LMM instead of LMM makes it easier to balance the losses.
Interestingly, our β-NLL subsumes the moment matching loss if we allow for different values of β
to be used for mean and variance estimation. In particular, using β-NLL with β = 1 for the mean
and β = 2 for the variance results in the same gradients as using 1LMM + 4 LMM as the loss. Note
that we did not investigate this connection further, and also did not perform any experiments with
different values of β for mean and variance.
B	Additional Results
B.1	Further Consequences of Inverse-Variance Weighting
In Sec. 3.2, we interpreted the inverse-variance weighting of the NLL as training under a different
data distribution P(X, Y) in which points with high error have a low probability of being sampled. A
consequence of this is that the distribution P(X, Y) continuously shifts while the model is improving
its fit. For datasets where the noise level is low on parts of the input space and the underlying
function can be modeled to high accuracy, an interesting phenomenon can be observed: even though
the training curve for RMSE looks like it indicates convergence (i.e. it flattens out), the model can
actually still be adapting with the changing training distribution. It is just that the changes happen
on data points that already have such low prediction error relative to the average error that further
improvements are virtually invisible in the average error. The actual training progress can be revealed
using a histogram of the prediction errors on a log-scale, in the manner of Fig. 6. Eventually, the
training distribution stabilizes when for all data points, either the prediction error reaches the noise
level at that point, or the variance can not decrease further at that point because it reaches a manually
set lower bound. If no lower bound on the variance is set, the distribution may never stabilize, leading
to training instabilities when the variance reaches close to zero.
13
Published as a conference paper at ICLR 2022
B.2	Synthetic Dataset
In Fig. S1, we replicate the experiment from Fig. 1 (training the NLL loss on a sinusoidal for 107
updates) on several more random seeds. In order to test the dependence of our reported issue on
the optimizer, we repeat the experiment from Fig. 1 but use different optimizers than Adam with
β1 = 0.9, β2 = 0.999. The results are shown in Fig. S2. We find that none of the configurations
reaches below an RMSE of 0.1 (the optimal value corresponds to an RMSE of 0.01), indicating that
the issue is occurring stably across optimization settings. In Fig. S3, we provide the results for the
NLL metric on the sinusoidal dataset, complementing the results of Fig. 7 (see discussion in Sec. 5.1).
.4 .0
0. 0. -
Y tegraT
.4 .0
00-
Y tegraT
.4 .0
00-
Y tegraT
0	3	6	9	12	0	3	6	9	12	0	3	6	9	12
Input X	Input X	Input X
Figure S1:	Repeating the experiment from Fig. 1, i.e. training with NLL loss for 107 update steps.
The observed behavior is stable across different independent trials.
0.30
0.25
0.20
0.15
0.10
-L0.10
HSnM
Figure S2:	Using different optimizers to train on the sinusoidal from Fig. 1 with the NLL loss. The
color code indicates the mean RMSE over 3 independent trials per optimizer setting. Black indicates
that all trials diverged. Training was done for 2 ∙ 106 update steps and used architecture 2 from
Table S5. The observed behavior is stable across optimization settings.
(a) β = 0 (NLL)
1e-03
⅛
3e-04
α
1e-04
E
ω
3e-05
u
N
(b) β = 0.5
Architecture
(c) β = 1
Figure S3:	Convergence properties analyzed on the sinusoidal toy regression problem. Same as
Fig. 7 but for the negative log-likelihood (NLL) criterion. Due to the bad mean fit, the original NLL
loss (β = 0) is also bad for most hyperparameter settings. With β > 0.5 good fits are obtained for
many settings. Also for β = 1, which corresponds to MSE for fitting the mean, good uncertainty
predictions are obtained with our β-NLL as testified by the low NLL scores.
B.3	Analysis of Sampling Probabilities on Fetch-PickAndPlace
In Fig. S4, we show how the analysis from Sec. 3.2 transfers to a real world dataset, namely Fetch-
PickAndPlace. The figure shows how the distribution of effective sampling probability evolves
during training (over a fixed set of training points) and compares that against a proxy “oracle”: the
distribution of effective sampling probabilities when using the squared residuals from a model trained
14
Published as a conference paper at ICLR 2022
with the MSE loss. The mismatch between the two distributions demonstrates that optimizing the
NLL loss drastically undersamples in comparison to the reference, effectively never sampling some
data points. This further corroborates our analysis in Sec. 3.2.
Initial
Epoch 1
0
0
4
tnuoC
■ LNLL
l≡j Lmse (trained) i
"I ɪ
0
0
4
tnuoC
00
42
tnuoC
sno□
10-8	10-5	10-2
Sampling Probability
600 η----Epoch≡θ
400-
200-
0
10-2
Sampling Probability
10-8	10-5	10-2
Sampling Probability
Epoch 100
600
sno□
600
Epoch 10
Sampling Probability
Epoch 200
0-0-
40 20
tnuoC
0
Figure S4: Undersampling behavior of LNLL on Fetch-PickAndPlace. The plot shows how the
distribution of effective sampling probability evolves over training time, taken over 2000 fixed
training points sampled at the initial epoch. The dashed blue histogram shows the distribution of
effective sampling probabilities when using the squared residuals from a model trained with MSE loss
LMSE . This gives a reference distribution that LNLL should roughly match, taking into account the
relative hardness of prediction on different samples. The LNLL drastically undersamples compared to
the reference (note the log-scale), effectively never sampling some points.
B.4	UCI Datasets
In this section, we include results for predictive log-likelihood (Table S1) and RMSE (Table S2) for
all UCI datasets we evaluated on.
We find that baselines based on the Student’s t-distribution (xVAMP and VBEM (Stirn & Knowles,
2020)) tend to have better predictive log-likelihood than β-NLL, although there is also a dataset
where β -NLL performs better (“naval”) or where there is no statistically significant improvement
(“housing”, “kin8m”, “wine-red”, “wine-white”). For RMSE, LMSE unsurprisingly performs best.
Our β -NLL is often on par with LMSE and on par or better than the other baselines, except for
“yacht”. At the same time, our method is very simple to implement (see Sec. D.5) and computationally
lightweight compared to xVAMP and VBEM, which require the costly evaluation of a prior and
Monte-Carlo sampling at each training step.
B.5	Generative Modeling with Variational Autoencoders
We test different loss functions on the task of generative modeling using variational autoencoders
(VAEs) (Kingma & Welling, 2014). To this end, we parameterize the decoder distribution p(x | z)
with N(μ(z),σ2(z)), where the mean μ(z) and variance σ2(z) are outputs of a neural network. We
train the VAE by maximizing the ELBO Eq(z|x) [log p(x |z)] - DKL(q(z | x) || p(z)), plugging in
different loss functions for log p(x | z ). Following Stirn & Knowles (2020), we evaluate the log-
posterior predictive likelihood log Eq(z|x) [p(x | z)]. We approximate the expectation using a finite
mixture of 20 Monte-Carlo samples from q(z | x). To compute the RMSE, we take the mean of that
mixture. We compare β-NLL against LMM, LNLL with a fixed variance of 1, Student-t (Takahashi
et al., 2018; Detlefsen et al., 2019), xVAMP and VBEM (Stirn & Knowles, 2020).
We evaluate on MNIST and FashionMNIST. Table S3 presents quantitative results. VBEM achieves
the best reconstruction error at the expense of poor log-likelihood. Vice versa, Student-t, xVAMP,
xVAMP*, and VBEM* achieve strong log-likelihoods but worse reconstruction errors. β-NLL with
β > 0 provides a good compromise between log-likelihood and reconstruction error. Figure S5
15
Published as a conference paper at ICLR 2022
Table S1: Results for UCI Regression Datasets. Predictive log-likelihood (higher is better) and standard deviation, together with dataset size, input and output dimensions. Best mean value in bold. Results that are not statistically distinguishable from the best result are marked with f.					
	carbon (10721, 5, 3)		concrete (1030, 8, 1)	energy (768, 8, 2)	housing (506, 13, 1)
Lβ-NLL(β	0)	11.36 ± 2.11	-3.25 ± 0.31	-3.22 ± 1.41	-2.86 ± 0.50
Lβ-NLL(β	0.25)	10.91 ± 2.42	-3.31 ± 0.51	-2.82 ± 0.82	-2.75 ± 0.42
Lβ-NLL(β	0.5)	10.22 ± 4.00	-3.29 ± 0.36	-2.41 ± 0.72	-2.64 ± 0.36*
Lβ-NLL(β	0.75)	10.82 ± 1.37	-3.27 ± 0.34	-2.80 ± 0.59	-2.72 ± 0.42*
Lβ-NLL(β	1.0)	3.31 ± 20.88	-3.23 ± 0.33	-3.37 ± 0.58	-2.85 ± 0.87
LMM		4.72 ± 5.80	-3.49 ± 0.38	-4.26 ± 0.50	-3.42 ± 1.01
LMSE									
Student-t		15.59 ± 0.43	-3.07 ± 0.14t	-2.46 ± 0.34	-2.47 ± 0.24*
xVAMP		13.28 ± 0.19	-3.06 ± 0.15t	-2.47 ± 0.32	-2.43 ± 0.21*
xVAMP*		13.17 ± 0.26	-3.03 ± 0.13t	-2.41 ± 0.32	-2.45 ± 0.22*
VBEM		5.68 ± 0.70	-3.14 ± 0.07	-4.29 ± 0.16	-2.56 ± 0.15
VBEM*		13.23 ± 0.36	-2.99 ± 0.13	-1.91 ± 0.21	-2.42 ± 0.22
		kin8m	naval	power	protein
		(8192, 8, 1)	(11934,16, 2)	(9568, 4, 1)	(45730, 9, 1)
Lβ-NLL(β	0)	1.140 ± 0.039t	12.46 ± 1.18	-2.807 ± 0.057	-2.80 ± 0.05
Lβ-NLL(β	0.25)	1.142 ± 0.026t	13.78 ± 0.33*	-2.801 ± 0.053	-2.79 ± 0.05
Lβ-NLL(β	0.5)	1.141 ± 0.046t	13.99 ± 0.40	-2.805 ± 0.052	-2.78 ± 0.02
Lβ-NLL(β	0.75)	1.137 ± 0.0411	13.63 ± 0.62*	-2.806 ± 0.053	-2.79 ± 0.02
Lβ-NLL(β	1.0)	1.126 ± 0.041	13.59 ± 0.30	-2.810 ± 0.051	-2.80 ± 0.03
LMM		0.999 ± 0.063	12.73 ± 0.64	-2.918 ± 0.092	-2.98 ± 0.06
LMSE									
Student-t		1.155 ± 0.037	12.47 ± 0.48	-2.738 ± 0.026	-2.56 ± 0.02
xVAMP		1.147 ± 0.037t	12.44 ± 0.60	-2.788 ± 0.032	-2.74 ± 0.03
xVAMP*		1.147 ± 0.036t	12.80 ± 0.55	-2.785 ± 0.036	-2.71 ± 0.02
VBEM		1.019 ± 0.054	8.05 ± 0.13	-2.819 ± 0.018	-2.85 ± 0.01
VBEM*		1.138 ± 0.036t	13.10 ± 0.47	-2.783 ± 0.031	-2.73 ± 0.01
		superconductivity	wine-red	wine-white	yacht
		(21263, 81, 1)	(1599,11,1)	(4898, 11, 1)	(308, 6, 1)
Lβ-NLL(β	0)	-3.60 ± 0.23	-1.03 ± 0.24*	-1.059 ± 0.074*	-2.86 ± 5.18
Lβ-NLL(β	0.25)	-3.56 ± 0.14	-0.98 ± 0.12*	-1.041 ± 0.064*	-1.97 ± 1.14
Lβ-NLL(β	0.5)	-3.60 ± 0.10	-0.99 ± 0.16*	-1.036 ± 0.065*	-2.47 ± 1.68
Lβ-NLL(β	0.75)	-3.72 ± 0.11	-1.02 ± 0.22*	-1.039 ± 0.060*	-1.87 ± 0.55
Lβ-NLL(β	1.0)	-3.83 ± 0.09	-0.97 ± 0.10*	-1.040 ± 0.067*	-2.27 ± 1.07
LMM		-4.45 ± 0.39	-1.22 ± 0.43	-1.135 ± 0.093	-11.24 ± 31.03
LMSE									
Student-t		-3.38 ± 0.04	-0.94 ± 0.10*	-1.034 ± 0.062*	-1.23 ± 0.55*
xVAMP		-3.40 ± 0.03	-0.95 ± 0.06*	-1.038 ± 0.050*	-0.99 ± 0.33*
xVAMP*		-3.40 ± 0.04t	-0.94 ± 0.06	-1.029 ± 0.048*	-1.04 ± 0.47*
VBEM		-3.63 ± 0.09	-0.95 ± 0.07*	-1.028 ± 0.048	-2.65 ± 0.10
VBEM*		-3.40 ± 0.04t	-0.94 ± 0.07*	-1.031 ± 0.057*	-0.98 ± 0.24
16
Published as a conference paper at ICLR 2022
Table S2: Results for UCI Regression Datasets. RMSE (lower is better) and standard deviation, together with dataset size, input and output dimensions. Best mean value in bold. Results that are not statistically distinguishable from the best result are marked with f.					
	carbon (10721, 5, 3)		concrete (1030, 8, 1)	energy (768, 8, 2)	housing (506, 13, 1)
Lβ-NLL(β	0)	0.0068 ± 0.0029t	6.08 ± 0.65	2.25 ± 0.34	3.56 ± 1.07t
Lβ-NLL(β	0.25)	0.0069 ± 0.0028t	5.79 ± 0.74	1.81 ± 0.30	3.48 ± 1.15t
Lβ-NLL(β	0.5)	0.0068 ± 0.0029t	5.61 ± 0.65	1.12 ± 0.25	3.42 ± 1.04t
Lβ-NLL(β	0.75)	0.0069 ± 0.0028t	5.67 ± 0.73	1.31 ± 0.45	3.43 ± 1.07t
Lβ-NLL(β	1.0)	0.0073 ± 0.0026t	5.55 ± 0.77t	1.54 ± 0.54	3.50 ± 0.95t
LMM		0.0097 ± 0.0034	6.28 ± 0.82	2.19 ± 0.28	4.02 ± 1.18
LMSE		0.0068 ± 0.0028t	4.96 ± 0.64	0.92 ± 0.11	3.24 ± 1.08t
Student-t		0.0067 ± 0.0029t	5.82 ± 0.59	2.26 ± 0.34	3.48 ± 1.17t
xVAMP		0.0067 ± 0.0029t	5.44 ± 0.64t	1.87 ± 0.32	3.23 ± 1.00t
xVAMP*		0.0067 ± 0.0029t	5.35 ± 0.73t	2.00 ± 0.26	3.38 ± 1.15t
VBEM		0.0074 ± 0.0026t	5.21 ± 0.58t	1.29 ± 0.33	3.32 ± 1.06t
VBEM*		0.0067 ± 0.0029	5.17 ± 0.59t	1.08 ± 0.17	3.19 ± 1.02
		kin8m	naval	power	protein
		(8192, 8, 1)	(11934,16, 2)	(9568, 4, 1)	(45730, 9, 1)
Lβ-NLL(β	0)	0.087 ± 0.004	0.0021 ± 0.0006	4.06 ± 0.18t	4.49 ± 0.11
Lβ-NLL(β	0.25)	0.083 ± 0.003	0.0012 ± 0.0004	4.04 ± 0.18t	4.35 ± 0.05t
Lβ-NLL(β	0.5)	0.082 ± 0.003t	0.0006 ± 0.0002	4.04 ± 0.17t	4.31 ± 0.02t
Lβ-NLL(β	0.75)	0.081 ± 0.004t	0.0004 ± 0.0001t	4.04 ± 0.15t	4.28 ± 0.02t
Lβ-NLL(β	1.0)	0.081 ± 0.003t	0.0004 ± 0.0000	4.06 ± 0.18t	4.31 ± 0.05t
LMM		0.082 ± 0.003t	0.0005 ± 0.0001	4.07 ± 0.16t	4.32 ± 0.07t
LMSE		0.081 ± 0.003	0.0004 ± 0.0001t	4.01 ± 0.19	4.28 ± 0.07
Student-t		0.085 ± 0.005	0.0026 ± 0.0009	4.02 ± 0.16t	4.76 ± 0.24
xVAMP		0.081 ± 0.003t	0.0023 ± 0.0004	4.03 ± 0.17t	4.38 ± 0.05t
xVAMP*		0.082 ± 0.003t	0.0020 ± 0.0006	4.03 ± 0.18t	4.31 ± 0.02t
VBEM		0.082 ± 0.003t	0.0009 ± 0.0004	4.09 ± 0.15t	4.31 ± 0.01t
VBEM*		0.082 ± 0.004t	0.0015 ± 0.0005	4.02 ± 0.18t	4.35 ± 0.09t
		superconductivity	wine-red	wine-white	yacht
		(21263, 81, 1)	(1599,11,1)	(4898, 11, 1)	(308, 6, 1)
Lβ-NLL(β	0)	13.87 ± 0.50	0.636 ± 0.038t	0.691 ± 0.032t	1.22 ± 0.47
Lβ-NLL(β	0.25)	13.50 ± 0.49	0.638 ± 0.036t	0.687 ± 0.039t	1.73 ± 1.00
Lβ-NLL(β	0.5)	13.02 ± 0.47	0.635 ± 0.037t	0.685 ± 0.035t	2.35 ± 1.44
Lβ-NLL(β	0.75)	13.20 ± 0.46	0.638 ± 0.035t	0.689 ± 0.034t	1.97 ± 1.03
Lβ-NLL(β	1.0)	13.42 ± 0.41	0.639 ± 0.035t	0.684 ± 0.031t	2.08 ± 1.13
LMM		13.68 ± 0.79	0.652 ± 0.044t	0.692 ± 0.032t	3.02 ± 1.38
LMSE		12.48 ± 0.40	0.633 ± 0.036t	0.684 ± 0.038	0.78 ± 0.25t
Student-t		13.52 ± 0.60	0.636 ± 0.038t	0.688 ± 0.036t	1.34 ± 0.63
xVAMP		13.33 ± 0.52	0.635 ± 0.035t	0.691 ± 0.032t	0.99 ± 0.43
xVAMP*		13.42 ± 0.59	0.633 ± 0.035t	0.685 ± 0.032t	1.13 ± 0.66
VBEM		12.72 ± 0.57t	0.639 ± 0.041t	0.685 ± 0.035t	1.66 ± 0.84
VBEM*		13.15 ± 0.43	0.633 ± 0.040	0.686 ± 0.036t	0.65 ± 0.20
17
Published as a conference paper at ICLR 2022
shows qualitative examples. Our β-NLL loss with β > 0 allows to learn good reconstructions
and meaningful uncertainties. Moreover, images produced by sampling latents from the prior are
semantically meaningful, indicating that adding our loss function does not break the disentangling
properties of VAEs. Compare that to Student-t, xVAMP(*), and VBEM(*), which do not produce
similarly clear images when sampling from the prior.
Table S3: Results for generative modeling with variational autoencoders on MNIST and Fashion-
MNIST. We report RMSE and posterior predictive log-likelihood (LL) with standard deviation over 5
independent trials.
Loss		MNIST		Fashion-MNIST	
		RMSE (	LL ↑	RMSE (	LL ↑
LNLL (σ2 =	1)	0.153 ± 0.002	-730 ± 0	0.143 ± 0.002	-729 ± 0
Lβ-NLL(β	= 0)	0.237 ± 0.002	2116 ± 55	0.170 ± 0.001	1940 ± 104
Lβ-NLL(β	= 0.25)	0.181 ± 0.004	2511 ± 88	0.140 ± 0.002	2010 ± 39
Lβ-NLL(β	= 0.5)	0.151 ± 0.003	2220 ± 25	0.125 ± 0.003	1639 ± 52
Lβ-NLL(β	= 0.75)	0.142 ± 0.001	1954 ± 50	0.131 ± 0.001	1331 ±32
Lβ-NLL(β	= 1.0)	0.152 ± 0.001	1706 ± 30	0.138 ± 0.002	1142 ± 26
LMM		0.260 ± 0.000	385 ± 11	0.295 ± 0.000	-151 ±4
Student-t		0.273 ± 0.002	4291 ± 103	0.182 ± 0.002	2857 ± 9
xVAMP		0.225 ± 0.002	2989 ± 268	0.161 ± 0.001	2158 ± 100
xVAMP*		0.225 ± 0.001	3062 ± 215	0.160 ± 0.002	2150 ± 131
VBEM		0.114 ± 0.001	719±9	0.108 ± 0.000	660 ± 2
VBEM*		0.176 ± 0.008	3213 ±238	0.150 ± 0.003	2244 ± 78
B.6 Depth Regression
We evaluate β-NLL on the task of depth regression on the NYUv2 dataset (Silberman et al., 2012).
For this purpose, we use a state-of-the-art method for depth regression, AdaBins (Bhat et al., 2021),
and train it with different loss functions. Note that we remove the Mini-ViT Transformer module
from the model, thus our results are not directly comparable with those reported by Bhat et al. (2021).
Table S4 presents quantitative results. β-NLL with β > 0 achieves better RMSE than the NLL
loss. β-NLL with β = 0.5 again provides a good trade-off, achieving similar RMSE as LMSE and
performing better on some of the other metrics. Figure S6 shows qualitative examples. Compared to
LNLL, the depth maps predicted by β-NLL with β > 0 are noticeably sharper.
Table S4: Results for depth regression on the NYUv2 dataset (Silberman et al., 2012). We adapt a state-
of-the-art network for depth regression from AdaBins (Bhat et al., 2021) and train it with different
loss functions. In contrast to AdaBins, our network does not include the Mini-ViT Transformer
module and thus our results are not directly comparable with those originally reported. For reference,
we also report numbers from other recent literature on this task. Notably, the Gaussian NLL in all
variants clearly outperforms the Scale Invariant (SI) loss, despite the latter being a loss function
specifically designed for the task of depth regression. We refer the reader to Bhat et al. (2021) for a
description of the metrics.
Method		δ1 ↑	δ2 ↑	δ3 ↑	REL (	RMSE (	log10 (	LL ↑
Lβ-NLL(β =	0)	0.8855	0.9796	0.9959	0.1094	0.3854	0.0462	-4.52
Lβ-NLL(β =	0.25)	0.8887	0.9812	0.9956	0.1081	0.3818	0.0458	-8.15
Lβ-NLL(β =	0.5)	0.8885	0.9813	0.9956	0.1093	0.3789	0.0458	-7.50
Lβ-NLL(β =	0.75)	0.8902	0.9804	0.9952	0.1095	0.3800	0.0462	-7.35
Lβ-NLL(β =	1.0)	0.8872	0.9813	0.9958	0.1088	0.3845	0.0467	-5.10
LMSE		0.8890	0.9806	0.9960	0.1086	0.3776	0.0461	—
L1		0.8877	0.9798	0.9955	0.1073	0.3850	0.0459	—
SI Loss (Bhat et al., 2021)		0.881	0.980	0.996	0.111	0.419	—	—
AdaBins (Bhat et al., 2021)		0.903	0.984	0.997	0.103	0.364	0.044	—
BTS (Lee et al., 2019)		0.885	0.978	0.994	0.110	0.392	0.047	—
DAV (Chen et al., 2019)		0.882	0.980	0.996	0.108	0.412	—	—
18
Published as a conference paper at ICLR 2022
MNIST
Fashion-MNIST
IMjMFml ■/
dataset
Lβ-NLL(β = 0)
mean
std
〜Posterior
〜N (0,1)
1
q
g ∙≡心
■ ■ JIJf ∣J∣∙AJI
Ei 口 二 J F 二一二
I ^ ʌ,' A , - -' fl * i ■
Lβ-NLL(β = 0.25)
mean
std
〜Posterior
〜N (0,1)
Lβ-NLL(β = 0.5)
mean
std
〜Posterior
〜N (0,1)
Lβ-NLL(β = 0.75)
Lβ-NLL(β = 1.0)
mean
LNLL (σ2 = 1)	std
NLL	〜Posterior
〜N (0,1)
mean
std
〜Posterior
〜N (0,1)
mean
std
〜Posterior
〜N (0,1)
LMM
Student-t
xVAMP
xVAMP*
VBEM
VBEM*
mean
std
〜Posterior
〜N (0,1)
mean
std
〜Posterior
〜N (0,1)
mean
std
〜Posterior
〜N (0,1)
mean
std
〜Posterior
〜N (0,1)
mean
std
〜Posterior
〜N (0,1)
mean
std
〜Posterior
〜N (0,1)
/ 7 3 ? a a / j 9 σ
¥誓露学鎏露，琴祥雄
勺S Wd♦帖片筋5 C
∣7a72Λ∕f93
IΛ*	周科


Figure S5:	Generative modeling with variational autoencoders on MNIST and Fashion-MNIST. The
overall first row shows inPuts from the test set. For each method, we Present Posterior Predictive
means (i.e. reconstructions), the Posterior Predictive standard deviations, samPles from the Posterior
Predictive distribution, and finally samPles using the Prior N (0, 1). LNLL (σ2 = 1) refers to Gaussian
log-likelihood with a fixed variance of 1. Values are cliPPed to the interval [0, 1]. ExamPles are not
cherry-Picked.
19
Published as a conference paper at ICLR 2022
Input
Ground
Truth
Figure S6:	Example results for depth regression on the NYUv2 dataset (Silberman et al., 2012).
First two rows show input image and ground truth depth map, where black values in the depth map
represent missing values. For each method, we present the predicted depth map (pred.), and the
aleatoric uncertainty in form of the predicted standard deviation (std.). Results for Lβ-NLL with
β > 0 are noticeably sharper. The last column shows a magnified view on the previous picture.
20
Published as a conference paper at ICLR 2022
C Datasets and Training Settings
Sinusoidal without heteroscedastic noise This dataset is created by taking 1 000 uniformly spaced
points on the interval [0, 12] as inputs x and applying the function y(x) = 0.4 sin(2πx) + ξ to them
to create the targets y, where ξ is Gaussian noise with a standard deviation of 0.01.
Sinusoidal with heteroscedastic noise We use the synthetic data as introduced in Detlefsen et al.
(2019). From the functional form y = x sin(x) + xξ1 + ξ2, with Gaussian noise with standard
deviation σ = 0.3 for ξ1 and ξ2, we sample 500 points uniformly spaced in the interval [0, 10]. The
model is an MLP with one hidden layer of 50 units and tanh activations (as used in Detlefsen et al.
(2019) and Stirn & Knowles (2020)).
UCI Datasets We use the UCI datasets suite commonly used to benchmark uncertainty estimation,
stemming from the UCI Machine Learning Repository.3 In particular, we use the training-test protocol
from (Hemandez-Lobato & Adams, 2015; Gal & Ghahramani, 2016), and their data splits4, except
for “carbon”, “energy”, “naval”, “superconductivity”, and “wine-white”, where we generate our own
random splits.
Inputs and targets are whitened on the training set. Metrics are reported in the original scale of
the data. Each dataset is divided into 20 randomly sampled train-test splits (80%-20%). For each
split, we further randomly divide the training set into 80% training data and 20% validation data and
search for an optimal learning rate from the set {10-4,3 ∙ 10-4,7 ∙ 10-4,10-3,3 ∙ 10-3,7 ∙ 10-3}
by monitoring log-likelihood on the validation set. We train for a maximum of 20 000 updates, except
for the larger “kin8m”, “power plant”, “protein”, and “naval” datasets where we train for a maximum
of 100 000 updates. We perform early-stopping with a patience of 50 epochs, retrain the model
with the best found learning rate on the full training set, and then evaluate on the test split. The
reported performance and standard deviations are taken as averages over all test splits. Note that
the performance we report is not comparable with other publications, as performance is known to
differ strongly over different data splits. Some other works also perform early-stopping on the test
set, which distorts the results.
We use a single-layer relu hidden network with 50 neurons, except for “protein” where we use 100
neurons. The batch size is 256.
Following Stirn & Knowles (2020), for each method, we report the number of datasets for which the
method is statistically indistinguishable from the respective best method in Table 1 (Ties). For this
purpose, we performed a two-sided Kolmogorov-Smirnov test with ap ≤ 0.05 significance level.
ObjectSlide This environment consists of an agent whose task is to slide an object to a target
location (Seitzer et al., 2021). The continuous state space consists of 4 dimensions: agent and object
positions and velocities, and the continuous action space is a one-dimensional movement command.
The forward prediction task consists of predicting the change in object position in the next state from
the current state and action. The dataset we use consists of 180 000 transitions collected using a
random policy, which we split into training, validation, and testing sets with 60 000 transitions each.
Inputs and targets are whitened on the training set. Metrics are reported in the original scale of the
data. We train for a maximum of 5 000 epochs with a batch size of 256, and evaluate the model with
the best validation log-likelihood on the test set afterwards.
Fetch-PickAndPlace We use the Fetch-PickAndPlace environment (Plappert et al., 2018) from
OpenAI Gym (Brockman et al., 2016) as a challenging real-world scenario. The task of the agent is
to use a position-controlled 7 DoF robotic arm to lift an object to a target location in space. The state
space is 25-dimensional and the action space is 4-dimensional. As in ObjectSlide, the prediction task
is to predict the 3-dimensional change in object position from the current state and action. We use
840 000 transitions collected using the APEX method (Pinneri et al., 2021) as our dataset, which we
split into 70% training, 15% validation, and 15% testing data. Inputs and targets are whitened on the
training set. Metrics are reported in the original scale of the data. We train for a maximum of 500
3https://archive.ics.uci.edu
4available under https://github.com/yaringal/DropoutUncertaintyExps
21
Published as a conference paper at ICLR 2022
epochs with a batch size of 256, and evaluate the model with the best validation log-likelihood on the
test set afterwards.
NYUv2 Depth Regression We use the dataset in the variant provided by Lee et al. (2019).5
Training settings and evaluation protocol were taken from Bhat et al. (2021). We train for 25 epochs
using a batch size of 16 and validate the model every 100 updates. We use the model with the best
“REL” metric on the validation set for testing.
D	Hyperparameter Settings and Implementation Details
For all experiments, we used the Adam optimizer (Kingma & Ba, 2015) with standard settings
β1 = 0.9, β2 = 0.999. We parameterize the Gaussian distribution using two linear layers on top
of shared features produced by an MLP The variance σ2(x) is constrained to the positive region
using the softplus(x) = log(1 + exp(x)) activation function. We additionally add a small constant
of 10-8 to prevent the variance from collapsing to zero and clamp the maximum variance to 1 000.
Some baselines use a Student’s t-distribution (Student’s t, xVAMP(*), VBEM(*)) as their predictive
distribution. This distribution results from integrating out the unknown variance of a Gaussian with a
learned Gamma prior on the inverse variance (Detlefsen et al., 2019; Stirn & Knowles, 2020). We
parametrize the Gamma distribution in terms of data-dependent alpha and beta parameters, i.e. α(x)
and β(x), which are computed using linear layers on top of the shared features. In this case, the
MLP has three outputs: mean, alpha, and beta. Both alpha and beta are constrained to the positive
region using the SoftPlus activation. We add a positive constant of 1.001 for alpha and 10-8 ∙ 0.001
for beta. Alpha is clamped to a maximum value of 1000 and beta to 10-8 ∙ 999. These values
were chosen such that the resulting variance matches the range (10-8, 1000] while ensuring that the
degrees-of-freedom parameter ν of the Student’s t is always greater than 2.
For xVAMP and VBEM, the MLP outputs a fourth term, π(x), representing the logits of a categorical
distribution that specifies the mixture weights of the prior. We initialize the prior parameters exactly
the same as Stirn & Knowles (2020). For these methods, the objective function additionally contains
a KL divergence between a Gamma distribution and a mixture-of-Gamma distributions. Following
Stirn & Knowles (2020), we approximate this KL divergence using 20 Monte-Carlo samples.
D.1 Sinusoidal Regression Problem
The sinusoidal fit in Fig. 1 results from a network of two hidden layers with 128 neurons per layer
and tanh activations, optimized with a learning rate of 5 ∙ 10-4 and a batch size of 100. For the
experiment in Sec. 5.1, we scan over learning rates and architectures with different hidden layers and
units per layer, as detailed in Table S5.
Table S5:	Architectures used for the sinusoidal regression task. Fully-connected feed-forward neural
networks with tanh activations.
Architecture #	0	1	2	3	4
# Hidden Layers	2	2	2	3	3
# Units per Layer	32	64	128	128	256
D.2 ObjectSlide and Fetch-PickAndPlace
For each tested loss function, we performed a grid search on the ObjectSlide and Fetch-PickAndPlace
datasets. We report the parameters we scanned over in Table S6. Table S7 reports the model
configurations with the best validation log-likelihood on the grid search. For the results in Table 2,
we retrained the best model configuration with five different random seeds and evaluated them on the
hold-out test set.
5available under https://github.com/cogaplex-bts/bts
22
Published as a conference paper at ICLR 2022
Table S6:	Hyperparameter settings for our grid search on the ObjectSlide and Fetch-PickAndPlace
datasets. We run 96 configurations per loss function.
Hyperparameter	Set of Values
Learning Rate	{3 ∙ 10-5,10-4, 3 ∙ 10-4, ∙10-3}
# Hidden Layers	{2, 3, 4}
# Units per Layer	{128, 256, 386, 512}
Activation	{tanh, relu}
Table S7:	Best hyperparameters found by grid search on ObjectSlide and Fetch-PickAndPlace
datasets, measured by best log-likelihood on the validation set.
(a) ObjectSlide							(b) Fetch-PickAndPlace						
Method	β	LR	Layers			Act.	Method	β	LR	Layers			Act.
LMSE		10—3	3	×	128	relu	LMSE		10-3	4	×	128	relu
LNLL		10-3	3	×	128	relu	LNLL		3∙10-4	4	×	128	relu
Lβ-NLL	0.25	10-3	3	×	128	relu	Lβ-NLL	0.25	3∙10-4	4	×	128	relu
Lβ-NLL	0.5	10-3	3	×	128	relu	Lβ-NLL	0.5	3∙10-4	4	×	128	relu
Lβ-NLL	0.75	10-3	3	×	128	relu	Lβ-NLL	0.75	10-3	4	×	128	relu
Lβ-NLL	1.0	10-3	3	×	128	relu	Lβ-NLL	1.0	10-3	4	×	128	relu
LMM		10-3	3	×	128	relu	LMM		10-3	4	×	128	relu
Student-t		10-3	2	×	386	relu	Student-t		3∙10-4	3	×	256	relu
xVAMP		10-4	4	×	128	relu	xVAMP		10-4	3	×	386	relu
xVAMP*		10-4	3	×	256	relu	xVAMP*		10-4	3	×	386	relu
VBEM		3∙10-4	2	×	256	tanh	VBEM		10-3	3	×	386	relu
VBEM*		10-3	2	×	386	relu	VBEM*		10-4	3	×	386	relu
D.3 Variational Autoencoders
We largely follow Stirn & Knowles (2020) for their training settings for the VAE experiment. In
particular, we use an encoder with three layers of 512, 256, 128 neurons and a decoder with three
layers of 128, 256, 512 neurons, all with relu activations. The latent space is 10-dimensional for
MNIST and 25-dimensional for FashionMNIST. We train the VAEs for a maximum of 1 000 epochs,
using Adam with a learning rate of 0.0003 and a batch size of 256. Early-stopping with a patience of
50 epochs is performed on the log-likelihood of the validation set. The validation set consists of 20%
of the MNIST/FashionMNIST training set.
D.4 Depth Regression
We use the official implementation of AdaBins (Bhat et al., 2021),6 thereby reproducing their exact
training settings and evaluation protocol. We remove the AdaBins/mini-ViT Transformer from the
model. Instead, the feature map output by the U-Net is reduced to two channels using a 1 × 1
convolution, where we use the first channel as the mean predictor and the second channel as the
variance predictor. In this setting, both mean and variance are constrained to positive numbers by
a softplus activation. On top of that, we add a positive offset to ensure a minimum output value of
10-3 for the mean (the minimum possible depth value) and 10-6 for the variance and clamp both
mean and variance to a maximum value of 10.
6https://github.com/shariqfarooq123/AdaBins
23
Published as a conference paper at ICLR 2022
D.5 Implementation of beta-NLL in Pytorch
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
def beta_nll_loss(mean, variance, target, beta):
"""Compute beta-NLL loss
:param mean: Predicted mean of shape BxD
:param variance: Predicted variance of shape BxD
:param target: Target of shape BxD
:param beta: Parameter from range [0, 1] controlling relative
weighting between data points, where ‘0‘ corresponds to
high weight on low error points and ‘1‘ to an equal weighting.
:returns: Loss per batch element of shape B
loss = 0.5 * ((target - mean) ** 2 / variance + variance.log())
if beta > 0:
loss = loss * variance.detach() ** beta
return loss.sum(axis=-1)
24