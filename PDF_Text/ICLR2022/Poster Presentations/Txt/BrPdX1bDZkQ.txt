Published as a conference paper at ICLR 2022
DemoDICE: Offline Imitation Learning with
Supplementary Imperfect Demonstrations
Geon-Hyeong Kim1, Seokin Seo2, Jongmin Lee1, Wonseok Jeont,3,4, HyeongJoo Hwang2,
Hongseok Yang1,2,5, Kee-Eung Kim1,2
1	School of Computing, KAIST, Daejeon, Republic of Korea
2	Kim Jaechul Graduate School of AI, KAIST, Daejeon, Republic of Korea
3	Mila, Quebec AI Institute
4	School of Computer Science, McGill University
5	Discrete Mathematics Group, Institute for Basic Science (IBS), Daejeon, Republic of Korea
Ab stract
We consider offline imitation learning (IL), which aims to mimic the expert’s be-
havior from its demonstration without further interaction with the environment.
One of the main challenges in offline IL is to deal with the narrow support of the
data distribution exhibited by the expert demonstrations that cover only a small
fraction of the state and the action spaces. As a result, offline IL algorithms that
rely only on expert demonstrations are very unstable since the situation easily de-
viates from those in the expert demonstrations. In this paper, we assume additional
demonstration data of unknown degrees of optimality, which we call imperfect
demonstrations. Compared with the recent IL algorithms that adopt adversarial
minimax training objectives, we substantially stabilize overall learning process by
reducing minimax optimization to a direct convex optimization in a principled
manner. Using extensive tasks, we show that DemoDICE achieves promising re-
sults in the offline IL from expert and imperfect demonstrations.
1	Introduction
Reinforcement learning (RL) (Sutton et al., 1998) aims to learn a intelligent behavioral strategy
based on reward feedback. Although RL has achieved remarkable success in many challenging do-
mains, its practicality and applicability are still limited in two respects. First, we need to specify
the reward function, which may be non-trivial to do so in many real-world problems that require
complex decision making. Second, the standard RL setting assumes online interaction with the en-
vironment during the intermediate stages of learning, which is infeasible for mission-critical tasks.
Imitation learning (IL) (Pomerleau, 1991; Ng & Russell, 2000) addresses the first limitation of
RL, where the agent is trained to mimic the expert from demonstration instead of specifying the
reward function. It is well known that adopting supervised learning for training the imitating agent,
commonly referred to as behavioral cloning (BC), is vulnerable to the distribution drift (Ross et al.,
2011). Thus, most of the successful IL algorithms rely on online experiences collected from the
environment by executing intermediate policies during training. Recent progress made by adversarial
imitation learning (AIL) (Ho & Ermon, 2016; Ke et al., 2019; Kostrikov et al., 2020) achieving state-
of-the-art results on challenging imitation tasks still relies on such an online training paradigm.
Unfortunately, in many realistic tasks such as robotic manipulation and autonomous driving, online
interactions are either costly or dangerous. Offline RL (Fujimoto et al., 2019; Kumar et al., 2019;
2020; Levine et al., 2020; Wang et al., 2020; Lee et al., 2021; Kostrikov et al., 2021) aims to ad-
dress these concerns by training the agent from the pre-collected set of experiences without online
interactions. To prevent an issue caused by the distributional shift, offline RL algorithms mitigate
the phenomenon by constraining the shift or making a conservative evaluation of the policy being
learned.
tThe research that is the basis of this paper was done while the author was at Mila/MCGill University, but
the author is currently employed by Qualcomm Technologies Inc.
1
Published as a conference paper at ICLR 2022
In this paper, we are concerned with offline IL problems. Finding an effective algorithm for these
problems is tricky. For instance, naively extending the offline RL algorithms (which assume a reward
function) to the offline IL setting does not work. In practice, expert demonstrations are scarce due
to the high cost of obtaining them. Thus, they typically cover only a small fraction of the state and
action spaces, which in turn makes the distribution drift issue even more stand out compared with
the standard offline RL setting with a reward function. We mitigate this issue by assuming a large
number of supplementary imperfect demonstrations, without requiring any level of optimality for
these imperfect demonstrations; they may contain expert or near-expert trajectories (Wu et al., 2019;
Wang et al., 2021) as well as non-expert ones all together. This generality covers the situations from
real-world applications, but at the same time, it poses a significant challenge for the design of a
successful offline IL algorithm.
In this paper, we propose DemoDICE, a novel model-free algorithm for offline IL from expert and
imperfect demonstrations. We formulate an offline IL objective which not only mitigates distribution
shift from the demonstration-data distribution but also naturally utilizes imperfect demonstrations.
Our new formulation allows us to compute a closed form solution, which learns a policy in the space
of stationary distributions but suffers from the instability issue in practice. We tackle the issue by
proposing an alternative objective, which leads to a stable algorithm in practice while keeping the
optimal stationary distribution. Finally, we introduce a method to extract the expert policy from a
learned stationary distribution in a simple yet effective way. Our extensive evaluations show that
DemoDICE achieves performance competitive to or better than a state-of-the-art off-policy IL algo-
rithm in the offline-IL tasks with expert and imperfect demonstrations. The code to reproduce our
results is available at our GitHub repository1.
2	Preliminaries
2.1	Markov Decision Process (MDP)
We assume an environment modeled as a Markov Decision Process (MDP), defined by tuple M =
hS, A, T, R, p0, γi, where S is the set of states, A is the set of actions, T : S × A → ∆(S) is the
probability p(st+1 |st, at) of making transition from state st to state st+1 by executing action at at
timestep t, R : S × A → R is the reward function, p0 ∈ ∆(S) is the distribution of the initial state
s0, and γ ∈ [0, 1] is the discount factor. A policy π : S → ∆(A) of MDP M is a mapping from
states of M to distributions over actions. For the given policy π, the stationary distribution dπ is
defined as follows:
∞
d (s,a) = (1 - Y) EY tp(st = s,at = a∣so 〜Po(∙),st 〜T (∙∣St-i,at-i),at 〜∏(∙∣st))
t=0
We assume a precollected dataset DE of (s, a, s0) tuples generated by the expert, and a precollected
imperfect dataset DI (generated by unknown degrees of optimality). More precisely, for the (under-
lying) expert policy’s stationary distribution dE (s, a), we assume that (s, a, s0) ∈ DE is sampled
as (s, a)〜dE, s0 〜 T(∙∣s, a). We define DU = DE ∪ Dl, the union of two datasets, and denote
the corresponding stationary distribution of the dataset DU as dU. We denote the trajectories gen-
erated by expert policy as expert trajectories, and trajectories generated by non-expert policies as
non-expert trajectories. The expert demonstrations consist only of expert trajectories and imperfect
demonstrations consist of a mixture of expert and non-expert trajectories. In this paper, we assume
that the quality of imperfect demonstrations is unknown.
2.2	Imitation Learning
Behavior cloning (BC) is a classical IL approach, which attempts to find a function that maps s to a
via supervised learning. The standard BC finds a policy π by minimizing the negative log-likelihood:
min Jbc(∏) := min-τDD∣	X log∏(a∣s).	(1)
(s,a)∈D
1https://github.com/KAIST-AILab/imitation-dice
2
Published as a conference paper at ICLR 2022
However, it is known to be brittle (Ross et al., 2011) when the interaction with the environment
deviates from the scarce trajectories in DE . In such cases, BC fails to recover expert policies.
One of the notable approaches for IL is to formulate the problem as distribution matching (Ho &
Ermon, 2016; Ke et al., 2019; Kostrikov et al., 2020). When instantiated with the KL divergence
widely used in previous IL works (Ke et al., 2019; Kostrikov et al., 2020), the approach amounts to
finding a policy π by optimizing the following objective:
max -DκL(dπkdE) = E(s,a)〜d∏
log
dE (s, a)
dπ (s, a)	.
(2)
Since we cannot directly access the exact value of dE(s, a) and dπ (s, a), we estimate their ratio
using samples from dE(s, a) and dπ(s, a), given as follows:
max	E(S )〜dE [log c(s,a)] + E(s,a)〜d∏ [log(1 - c(s,a))].	(3)
eS×A→(0,1)	'	'
Here, the optimal discriminator C recovers logc*(s, a) - log(1 - c*(s, a)) = log ；[：：). Based
on this connection between generative adversarial networks (GANs) and IL, AIL algorithms focus
on recovering the expert policy (Ho & Ermon, 2016; Kostrikov et al., 2019). However, the agent
obtain samples from dπ through interaction with environment, which is impossible in offline setting.
Therefore, to tackle the offline IL problems, we should derive an alternative estimation without
on-policy samples.
3	DEMODICE
In this section, we present a novel model-free offline IL algorithm named offline imitation learn-
ing using additional imperfect Demonstrations via stationary DIstribution Correction Estimation
(DemoDICE). Starting from a regularized offline IL objective which accords with offline RL algo-
rithms, we present a formulation that does not require on-policy samples. Such formulation allows us
to construct a nested maximin optimization for offline IL from expert and imperfect demonstrations
(Section 3.1). Then, we derive the closed-form solution to the sub-problem of the aforementioned
optimization and obtain a simple convex optimization objective (Section 3.2). Since the objective
is unstable in practice, we transform the objective to an alternative yet still convex objective (Sec-
tion 3.3). Finally, we show how to extract the policy from the learned correction term (Section 3.4).
3.1	Transform constrained optimization into nested optimization
In the context of offline RL, most works use expected return maximization with some regularization
to overcome the extrapolation error in offline settings (Fujimoto et al., 2019; Kumar et al., 2019;
Nachum et al., 2019b; Kumar et al., 2020; Lee et al., 2021). In this work, we use KL divergence
minimization between dπ and dE with KL-regularization:
π* := argmax —DκL(dπIldE) — αDκL(dπ∣∣dU),	(4)
π
where α ≥ 0 is a hyperparameter that controls the balance between minimizing KL divergence with
dE and preventing deviation of dπ from dU.
Many online AIL algorithms estimate divergence between expert and current policy using on-policy
samples, which is not available in offline scenario. In contrast, to construct a tractable optimization
problem in the offline setting, we consider a problem equivalent to Equation 4 in terms of stationary
distribution d:
max - DκL (dIdE) - αDκL (dIdU)	(5)
d
s.t ɪ2 d(s, a) = (1 — Y)po(s) + γ y^T(s|s, a)d(s, α) ∀s,	(6)
a	s,a
d(s, a) ≥ 0 ∀s, a.	(7)
3
Published as a conference paper at ICLR 2022
The constraints (6-7) are called the Bellman flow constraints. The Lagrangian of the above con-
strained optimization problem is
max min -DκL(d∣∣dE) - αDκL(d∣∣dU) + 汇 ν (s)((1 - γ)po(s) + YEd)(S)-(BM)(S)), (8)
d≥0 ν
s
where V(S) are the Lagrange multipliers, (B*d)(s) := Ea d(s, a) is the marginalization operator,
and (Xd)(s) := PM a T(S恒,a)d(s, a) is the transposed Bellman operator. We introduce following
derivations for the optimization (8) to obtain tractable optimization in the offline setting:
-DκL(dkdE) - αDκL(dkdU) + E V(s)((1 - γ)po(s) + γ(%d)(s) - (B*d)(s))
s
(1 - Y )Es 〜P0 [ν (s)]+ E(s,a)〜d Y(T V )(s,a) - V (s) - log Zd(S,a) - α log Zd(S,a)∖	(9)
0	dE (s, a)	dU (s, a)
(1 - Y)Ep0[V(s)] +Ed
dE (s, a)	d(s, a)
Y(TV)(s, a) - ν(s) + log du⅛∑) -(1 + α) log d⅛⅛
、--------------------------{z------}	、—{z—1
:=r(s,a)	:=w(s,a)
(10)
the equality in Equation 9 holds from the following properties of transpose operators:
EV (s)(B*d)(s) = Ed(s, a)(BV)(s, a) and EV(S)(Xd)(s) = £d(s,a)(T V )(s,a),
where (BV)(s, a) = V(s), (TV)(s, a) = Ps0 T (s0 |s, a)V(s0), with assumption dE (s, a) > 0 when
d(s, a) > 0 (Nachum et al., 2019a). We introduce another log ratio, denoted by r(s, a) in Equa-
tion 10 to avoid using log Cd(Ssaa), which requires on-policy samples to estimate. Unlike log Cd(Ssaa),
we can estimate r(s, a) in the offline setting using dE and dU, as we will discuss in the next section
in detail.
We change the distribution used in the expectation of Equation 10 from d to dU by following the
standard trick of importance sampling as follows:
(I- Y)Es 〜po[V (S)]+ E(s,a)〜d[r(s,a) + Y(T V )(s,a) - (BV)(S,a) -(1 + α) log w(s,a)]
'---------------------------------------------{----------------}
:=AV(s,a)(advantageusing V)
= (I- Y)Es〜P0 [V(S)] + E(s,a)〜du [w(s, a)(Aν(s,a) - (1 + α)log W(S, a))]
=: L(w, V; r).	(11)
As an alternative, one can convert expectation ofd to dE instead of dU in Equation 10 by the similar
application of the trick of importance sampling.
(I - Y)Es 〜pο [v(s)] + E(s,a)〜dE [eXp(-r(s,a))w(s,a)(Aν (s,a) - (1 + α)log w(s,a))].
In practice, due to the small number of demonstrations in dE, there may be a lack of diversity. To
release this practical issue, we prefer to use dU instead of dE .
In summary, DemoDICE solves the following maximin optimization:
max min L(w, V ; r),	(12)
w≥0 ν
where r is trained by using precollected datasets. Note that the solution w* of Equation 12 with
the ground-truth ratio r is the ratio of two distributions, the stationary distribution dπ* of the expert
policy π * and the stationary distribution dU of union of expert and imperfect demonstrations.
3.2	Pretrained stationary distribution ratio and a closed-form solution
To solve the problem (12), we can pretrain r, the log-ratio of dE (s, a) and dU (s, a). To this end, we
train a discriminator c : S × A → [0, 1] using the following maximization objective:
max	Jc(dE, dU) := EdE [log c(s, a)] + EdU [log(1 - c(s, a))],	(13)
eS×A→[0,1]
4
Published as a conference paper at ICLR 2022
whose maximizer is c*(s, a) = dU 表)+%, m.By using c*, r can be also obtained as
r(s, a) = - log
1
c*(s, a)
-1
(14)
Since the optimization (8) is a convex optimization and the strong duality holds, the optimization
(8) is equal to
min max L(w, ν; r).	(15)
ν w≥0
The closed-form solution of inner max optimization of (15) turns out to be w* (s, a) =
exp (A；+/ 一 1)for all (s, a), where AV(s, a) := r(s, a) + Y(Tν)(s, a) 一 V(s). By using w*(s, a),
optimization (15) can be reduced to a simple convex minimization problem as follows:
mιin L(WV, ν； r) = (1 一 Y)Es〜p0 [ν(s)] + (1 + a)E(s,θ)〜〃u exp (A；；：)一 1)	(16)
Although L(wν*, ν; r) in Equation 16 is convex on ν, we observe that L(wν*, ν; r) in Equation 16 is
numerically unstable in practice. Especially, the exponential term in L(wν*, ν; r) is prone to explo-
sion and so is the gradient NVL(WV, ν; r).
3.3	Alternative convex optimization
In order to derive a numerically-stable alternative objective, we describe the following theoretical
result:
Proposition 1. Define the objective L(ν; r) as
/	∖	/1	∖ τττ>	r / ∖ 1 .	/ -1	∖ ι Irn
L(V； r) := (I- Y)Es〜P0 [v(S)] + (1 + α)lOgE(s,a)〜dU
exp
AV(s, a)
1 + ɑ
(17)
Then, the following equality holds:
V
min L(WVV, V; r) = min L(V; r).
VV
(The proof can be found in Appendix A.)
The objective L(WVV, V; r) in Equation 16 has the instability issue since the gradient with respect
to v involves an unbounded easily-exploding function exp( ∙). In contrast, the alternative objective
L(V; r) in Equation 17 does not suffer from the same stability issue because in this case, the gradient
forms a soft-max and is bounded by 1. Furthermore, we observe that the surrogate objective is still
convex:
Proposition 2. The objective L(V; r) is convex with respect to V. (The proof is in Appendix C.)
To summarize, we have presented a training objective L(V; r) which significantly stabilizes the
objective L(WVV, V; r) while having the same optimal value. We converted the minimax optimization
(15) to a single minimization, which is much more stable. Proposition 1 is, however, limited in that
it is not explain how to sample actions from learned V. Thus, We need a method to sample action
from learned V and the next section presents such a method.
3.4	Policy extraction
In this section, we present a method for extracting a policy from the optimal VVV = arg minV L(V; r).
Our method is based on the following optimization, which amounts to the variant of weighted BC:
m∏in -E(s,a)〜d∏* [log∏(a∣s)] = -E(s,。)〜&u [w*(s, a) logπ(a∣s)],	(18)
*
where w*(s, a) = ",；；)). It can be estimated by V* ∈ argmin” L(WV, V; r), but We use L(V; r)
in Equation 17 to train V. Here, from the Proposition 1, we introduce theoretical results to explain
the relation between V * ∈ argmin” L(WV ,V r) and V* ∈ argmin/L(V; r):
5
Published as a conference paper at ICLR 2022
Corollary 1. Let V = argmin” L(ν; r) and V = argmin” L(WV, V； r) be the sets of V. Then the
following equation holds: (We provide the proof in Appendix B.)
~	- . _ _ _,
Ve = {V + c|V ∈ V, c ∈ R}
(19)
We observe that for any VV ∈ V, there are VV ∈ V and C ∈ R such that Vv = VV + c. Based on
this relationship, we can easily derive the following proportion: (see Appendix D for more detailed
derivation)
〜, 、	(Aν*(s,a)∖	dv(s,a)
WEs，a := exp [-τ+α- J(X dUM.
In other words, using VVV ∈ V, we can only obtain an unnormalized stationary distribution ratio
instead of exact ratio. Therefore, we estimate Equation 18 using self-normalized importance sam-
pling (Owen, 2013) as shown below:
.τ —*、	E(s,a)〜dU [wν*(s,a)lθg n(a|s)]
^11 n"—	E(s,a)〜dU [wi>*(s, a)]
(20)
Based on the weighted BC, we simply extract the policy from VVV ∈ V. Our policy extraction is highly
related to variants of weighted BC (Wang et al., 2018; 2020; Siegel et al., 2020) in offline RL. The
common idea of these methods is (1) training an action-value function Q(s, a) and (2) estimating
the corresponding advantage A(s, a).Using an increasing, non-negative function f (∙) (e.g., exp(∙)),
they perform weighted BC on precollected dataset with defining weight f (A(s, a), s, a) as:
arg min E(s,a)〜d。[—f (A(s, a), s, a) log π(a∣s)].
π
Finally, if we regard the advantage of optimal VV in Equation 16 as an advantage function, our policy
extraction matches the offline RL based on weighted BC.
4 Related Works
Learning from imperfect demonstrations Among several recent works (Wu et al., 2019; Brown
et al., 2019; 2020; Brantley et al., 2020; Tangkaratt et al., 2020; Sasaki & Yamashina, 2021; Wang
et al., 2021) that attempted to leverage imperfect demonstrations in IL, two-step importance weight-
ing imitation learning (2IWIL), generative adversarial imitation learning with imperfect demonstra-
tion and confidence (IC-GAIL) (Wu et al., 2019), weighted GAIL (WGAIL) (Wang et al., 2021), and
Behavioral Cloning from Noisy Demonstrations (BCND) (Sasaki & Yamashina, 2021) are closely
related to our work.
2IWIL, IC-GAIL and WGAIL suppose imperfect demonstrations which are composed of expert and
non-expert trajectories and this definition is the same as that of DemoDICE. Assuming that some
of trajectories in the imperfect demonstrations are provided with labels of confidence of optimali-
ties, 2IWIL trains a semi-supervised classifier to predict confidence scores for unlabeled imperfect
demonstrations and train the policy using the score. Instead of such two-step learning, IC-GAIL
trains the policy in an end-to-end fashion.
Inspired from Wu et al. (2019), Wang et al. (2021) suggest a general weighted GAIL objective
function. Unlike previous work, WGAIL predicts the weight for imperfect demonstrations without
information about their quality. However, 2IWIL, IC-GAIL, and WGAIL are online on-policy imi-
tation learning method, whereas our algorithm, DemoDICE, is an offline imitation learning method.
BCND assumes that the expert demonstrations are sampled from a noisy expert policy. BCND learns
ensemble policies with a weighted BC objective, where the weight is policy learned by weighted BC
in the previous internal iteration. As the learning progresses, the mode of the learned policy moves
towards that of the noisy expert policy. Thus, the learned policy tends to converge to a non-expert
policy when the non-expert trajectories comprise the majority of demonstrations.
6
Published as a conference paper at ICLR 2022
Stationary distribution corrections In RL and IL, some prior works have used distribution cor-
rections. In AlgaeDICE (Nachum et al., 2019b), regularization in the space of stationary distribution
is augmented to a policy optimization objective to solve off-policy RL problems. In addition, by us-
ing dual formulation of f-divergence and change of variables (Nachum et al., 2019a), an off-policy
learning objective is derived in AlgaeDICE. ValueDICE (Kostrikov et al., 2020) minimizes the KL
divergence between the agent and the expert stationary distributions to solve IL problems.
Similar to the AlgaeDICE, ValueDICE obtains an off-policy learning objective for distribution
matching by using the dual formulation of KL divergence and change of variables. Both AlgaeDICE
and ValueDICE objectives are optimized by nested optimization, which may suffer from numerical
instability.
In contrast, OptiDICE (Lee et al., 2021) reduces the same optimization used in AlgaeDICE to uncon-
strained convex optimization. Although it shows promising performance in offline RL tasks, directly
applying it to offline IL from expert and imperfect demonstrations is not trivial as we discussed in
Section 3.
Off policy learning from observations (OPOLO) (Zhu et al., 2020) addresses off-policy IL using
expert demonstrations composed only of states. It has similarities to DemoDICE in two aspects.
Firstly, OPOLO adopts a discriminator to estimate a state-transition distribution ratio. Furthermore,
DICE-like technique is applied to the derivation of the objective of OPOLO, which is similar to ours.
Thus, based on the relation between DemoDICE and OPOLO, offline IL using expert demonstrations
containing only states can be considered as a promising direction for future work.
5 Experiments
In this section, we present the empirical performance of DemoDICE and baseline methods on Mu-
JoCo continuous control environments (Todorov et al., 2012) using the OpenAI Gym (Brockman
et al., 2016) framework. We provide experimental results for 4 MuJoCo environments: Hopper,
Walker2d, HalfCheetah, and Ant. We utilize D4RL datasets (Fu et al., 2020) to construct expert
and imperfect demonstrations for our experiments. We construct datasets, baselines, and evaluation
protocol according to the following procedures:
Datasets For each of MuJoCo environments, we utilize three types of D4RL datasets (Fu et al.,
2020), whose name end with "-expert-v2”，“-full_rep山y-v2”，or "-random-v2”. In the remain-
der of this section, we refer them by using corresponding suffixes. We regard the trajectories in
expert-v2 as expert trajectories. The set of expert demonstrations DE consists of the first trajec-
tory in expert-v2.
Baselines We compare our method with three strong baseline methods, BC, BCND (Sasaki &
Yamashina, 2021), and ValueDICE (Kostrikov et al., 2020). To consider the potential benefit of
utilizing DI, we carefully tuned BC with 5 different values of β ∈ {1, 0.75, 0.5, 0.25, 0}, which
controls the balance between minimizing negative log-likelihood of DE and minimizing that of
DU = DE ∪ DI as follows:
mπin JBC(β) (π) := -β
1
|DEI
E	logπ(a∣s)-(1- β) ∙
(s,a)∈DE
1
∣DU∣
log π(a∣s).
(s,a)∈DU
We denote those 5 different settings by BC(β=1), BC(β=0.75), BC(β=0.5), BC(β=0.25), BC(β=0).
For BCND, we use all demonstrations as noisy expert demonstrations and report statistics of the
best performance among 5 internal iterations over 5 seeds. We provide the details for BCND in
Appenedix E.1. Lastly, we made ValueDICE to utilize imperfect datasets by plugging in all the
demonstrations to replay buffer.
Evaluation metric The normalized scores for each environment are measured by
normalized score = 100 X -------------Score-randomjCore-----, where expert and random scores
expert score-random score
are average returns of trajectories in expert-v2 and random-v2, respectively. We compute
the average normalized score and the standard error over five random seeds. In the following
subsections, we provide experimental results on two types of imperfect datasets.
7
Published as a conference paper at ICLR 2022
O.OM 0.2M O.4M 0.6M 0.8M IM O.OM 0.2M O.4M 0.6M 0.8M IM O.OM 0.2M O.4M 0.6M 0.8M IM
O.OM 0.2M O.4M 0.6M 0.8M IM O.OM 0.2M O.4M 0.6M 0.8M IM O.OM 0.2M O.4M 0.6M 0.8M IM
Training Iterations	Training Iterations	Training Iterations
Expert	BCND —— BC {β = O) —— BC (0 = 0.5)	—— BC (β = 1)	——VaIueDICE —— DemoDICE (ours)
Figure 1: Performance of DemoDICE and baseline algorithms on mixed-dataset tasks M1, M2,
and M3. Especially, in the M3 task, which contains a large number of bad trajectories, DemoDICE
maintains performance while ValueDICE, BCND, and BC fail to achieve competitive performance.
We plot the mean and the standard errors (shaded area) of the normalized scores over 5 random
seeds.
O.OM 0.2M O.4M 0.6M 0.8M IM
Training Iterations
5.1	Mixed dataset
DemoDICE mainly aims to overcome distributional drift caused by the lack of expert demonstra-
tions. When DemoDICE successfully learns the optimal unnormalized stationary distribution ratio,
we can expect that DemoDICE distinguishes expert trajectories from non-expert ones to update its
own policy. Based on this intuition, we hypothesize that if the same sufficiently-many expert tra-
jectories are included in imperfect demonstrations, DemoDICE achieves the optimal performance
invariant to the number of sub-expert trajectories in imperfect demonstrations. To see if it is the
case, we use mixed datasets which have the same expert trajectories but have different numbers of
non-expert trajectories in imperfect demonstrations.
Experimental setup Across all environments, we consider 3 tasks, each of which is called one
of M1, M2, or M3 and is provided with expert and imperfect demonstrations. While sharing the
expert demonstrations composed of the same single expert trajectory, imperfect demonstrations in
each task are composed of expert and random trajectories with different ratios such as 1:0.25 (M1),
1:1 (M2), and 1:4 (M3). Specifically, imperfect demonstrations in M1, M2, and M3 are composed of
400 expert trajectories sampled from expert-v2 and 100, 400, and 1600 random trajectories sampled
from random-v2, respectively.
Result We report the result of DemoDICE and comparing methods in Figure 1. For simplicity,
BC(β = 0.25) and BC(β = 0.75) are in Appendix F. Note that imperfect demonstrations in the tasks
M1, M2, and M3 share the same expert trajectories, while having the different number of random
trajectories. Ideally, if an algorithm uses only expert trajectories from imperfect demonstrations,
the algorithm should have the same performance in the tasks M1, M2, and M3, as discussed at
the beginning of this subsection, except convergence speed. While ValueDICE, BCND, and all the
variants of BC catastrophically fail to recover expert policies in task M3 on all the environments
except Walker2d, DemoDICE reaches the expert performance regardless of environments and tasks.
8
Published as a conference paper at ICLR 2022
Expert	BCND —— BC(J3 = O)	—— BC (0 = 0.5)	—— BC (β = 1)	——VaIueDICE —— DemoDICE (ours)
Figure 2: Performance of DemoDICE and baseline algorithms for replay buffer task RB. In RB
tasks, DemoDICE achieves better or competitive performances compared to the other baselines. We
plot the mean and the standard error (shaded area) of the normalized scores over 5 random seeds.
The result strongly implies that DemoDICE succeeds to learn appropriate stationary distribution
correction, which is possible only when the algorithm effectively leverages expert trajectories in im-
perfect demonstrations ignoring the rest. Furthermore, it is remarkable that DemoDICE reaches the
expert performance in HalfCheetah in the end, which can be found in Appendix F.1. To summarize,
we have empirically shown that DemoDICE is robust to any choices of the environments and tasks
and significantly outperforms existing methods on offline IL.
5.2	Replay buffer dataset
When we use mixed datasets as imperfect demonstration sets, ValueDICE poorly performs in all
environments except Walker2d as We observed in Section 5.1. Since full_replay-v2 consists
of the replay buffer’s data during off-policy training, we believe it is one of the most practically-
relevant datasets to fill ValueDICE’s replay buffer. Therefore, We conduct additional experiments
using this dataset as the imperfect demonstrations.
Experimental setup Applying the same expert demonstrations used in Section 5.1, We employ
another task named RB, which uses full_replay-v2 directly as its imperfect dataset.
Result Figure 2 summarizes the result of DemoDICE and baseline methods. We observe that De-
moDICE performs on par with ValueDICE in Walker2d. However, on the other environments, De-
moDICE strictly outperforms ValueDICE. We also observe that DemoDICE performs on par with
BCND in Ant, HalfCheetah environments, and significantly outperforms in Hopper and Walker2d
environments. While showing the performance competitive to the best performing baseline meth-
ods in Walker2d, Ant, and HalfCheetah, DemoDICE outperforms all the methods in Hopper by a
significant margin.
Since the replay buffer dataset is collected by the policy which evolves (in policy training phase), it
can be regarded as a set of imperfect demonstration from a wide range of sources. We emphasize that
DemoDICE performs well not only when bad trajectories make up majority of the given imperfect
demonstrations but also when they are generated from multiple sources.
6	Conclusion
We have presented DemoDICE, an algorithm for offline IL from expert and imperfect demonstra-
tions that achieves state-of-the-art performance on various offline IL tasks. We first introduced a
regularized offline IL objective and reformulated the objective so as to make it natural to compute
closed-form solution. We then tackled the instability coming from the naive application of closed-
form solution by the alternative objective which yields not only the same optimal value but also a
stable convex optimization. Furthermore, we presented the method to extract an optimal policy with
simple weighted BC. Lastly, our extensive empirical evaluations showed that DemoDICE achieves
remarkable performance close to the optimal by exploiting imperfect demonstrations effectively.
9
Published as a conference paper at ICLR 2022
Acknowledgments
This work was supported by the National Research Foundation (NRF) of Korea (NRF-
2019R1A2C1087634, NRF-2021M3I1A1097938) and Institute of Information & communications
Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) (No.2019-
0-00075, No.2020-0-00940, No.2021-0-02068) HY was supported by the Engineering Research
Center Program through the National Research Foundation of Korea (NRF) funded by the Korean
Government MSIT (NRF-2018R1A5A1059921) and also by the Institute for Basic Science (IBS-
R029-C1).
References
Kiante Brantley, Wen Sun, and Mikael Henaff. Disagreement-regularized imitation learning. In
International Conference on Learning Representations (ICLR), 2020.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Daniel Brown, Wonjoon Goo, Prabhat Nagarajan, and Scott Niekum. Extrapolating beyond sub-
optimal demonstrations via inverse reinforcement learning from observations. In International
Conference on Machine Learning (ICML), pp. 783-792. PMLR, 2019.
Daniel S Brown, Wonjoon Goo, and Scott Niekum. Better-than-demonstrator imitation learning via
automatically-ranked demonstrations. In Conference on Robot Learning (CoRL), pp. 330-359.
PMLR, 2020.
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: Datasets for deep
data-driven reinforcement learning, 2020.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning (ICML), pp. 2052-2062. PMLR,
2019.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Im-
proved training of Wasserstein GANs. arXiv preprint arXiv:1704.00028, 2017.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural
Information Processing Systems (NeurIPS), 2016.
Liyiming Ke, Matt Barnes, Wen Sun, Gilwoo Lee, Sanjiban Choudhury, and Siddhartha Srinivasa.
Imitation learning as f -divergence minimization. arXiv preprint arXiv:1905.12888, 2019.
Ilya Kostrikov, Kumar Krishna Agrawal, Debidatta Dwibedi, Sergey Levine, and Jonathan Tomp-
son. Discriminator-actor-critic: Addressing sample inefficiency and reward bias in adversarial
imitation learning. In International Conference on Learning Representations (ICLR), 2019.
Ilya Kostrikov, Ofir Nachum, and Jonathan Tompson. Imitation learning via off-policy distribution
matching. In International Conference on Learning Representations (ICLR), 2020.
Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement learning
with Fisher divergence critic regularization. In International Conference on Machine Learning
(ICML), pp. 5774-5783. PMLR, 2021.
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy Q-
learning via bootstrapping error reduction. In Advances in Neural Information Processing Systems
(NeurIPS), 2019.
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative Q-learning for offline
reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), 2020.
Jongmin Lee, Wonseok Jeon, Byung-Jun Lee, Joelle Pineau, and Kee-Eung Kim. OptiDICE: Offline
policy optimization via stationary distribution correction estimation. In International Conference
on Machine Learning (ICML), 2021.
10
Published as a conference paper at ICLR 2022
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
Ofir Nachum and Bo Dai. Reinforcement learning via fenchel-rockafellar duality. arXiv preprint
arXiv:2001.01866, 2020.
Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. DualDICE: Behavior-agnostic estimation of
discounted stationary distribution corrections. In Advances in Neural Information Processing
Systems (NeurIPS), 2019a.
Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. AlgaeDICE:
Policy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019b.
Andrew Y Ng and Stuart J Russell. Algorithms for inverse reinforcement learning. In Proceedings
of the International Conference on Machine Learning (ICML), 2000.
Art B. Owen. Monte Carlo theory, methods and examples. 2013.
Dean A Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neu-
ral computation, 3(1):88-97,1991.
StePhane Ross, Geoffrey Gordon, and J Andrew Bagnell. A reduction of imitation learning and
structured prediction to no-regret online learning. In Proceedings of the International Conference
on Artificial Intelligence and Statistics (AISTATS), PP. 627-635, 2011.
Fumihiro Sasaki and Ryota Yamashina. Behavioral cloning from noisy demonstrations. In Interna-
tional Conference on Learning Representations (ICLR), 2021.
Noah Siegel, Jost Tobias SPringenberg, Felix BerkenkamP, Abbas Abdolmaleki, Michael Neunert,
Thomas LamPe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. KeeP doing what worked:
Behavior modelling Priors for offline reinforcement learning. In International Conference on
Learning Representations (ICLR), 2020.
Richard S Sutton, Andrew G Barto, et al. Introduction to reinforcement learning, volume 135. MIT
Press Cambridge, 1998.
Voot Tangkaratt, Bo Han, Mohammad Emtiyaz Khan, and Masashi Sugiyama. Variational imitation
learning with diverse-quality demonstrations. In International Conference on Machine Learning
(ICML), PP. 9407-9417. PMLR, 2020.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A Physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, PP. 5026-5033.
IEEE, 2012.
Qing Wang, Jiechao Xiong, Lei Han, Peng Sun, Han Liu, and Tong Zhang. ExPonentially weighted
imitation learning for batched historical data. In Advances in Neural Information Processing
Systems (NeurIPS), PP. 6291-6300, 2018.
Yunke Wang, Chang Xu, Bo Du, and Honglak Lee. Learning to weight imPerfect demonstrations.
In International Conference on Machine Learning (ICML), PP. 10961-10970. PMLR, 2021.
Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh S Merel, Jost Tobias SPringenberg, Scott E
Reed, Bobak Shahriari, Noah Siegel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized
regression. In Advances in Neural Information Processing Systems (NeurIPS), 2020.
Yueh-Hua Wu, Nontawat CharoenPhakdee, Han Bao, Voot Tangkaratt, and Masashi Sugiyama. Imi-
tation learning from imPerfect demonstration. In International Conference on Machine Learning
(ICML), PP. 6818-6827, 2019.
Zhuangdi Zhu, Kaixiang Lin, Bo Dai, and Jiayu Zhou. Off-Policy imitation learning from observa-
tions. Advances in Neural Information Processing Systems (NeurIPS), 33, 2020.
11
Published as a conference paper at ICLR 2022
A Proof of Proposition 1
We provide the detailed proofs based on the previous works (Nachum & Dai, 2020; Lee et al., 2021).
Therefore, we highly recommend to read the works for a better understanding. We first provide a
lemma to prove the proposition:
Lemma 1 (Fenchel duality). Consider a primal problem given by
min JP (x) := g(x) + h(Ax),
x∈Ω
where g,h : Ω → R are convex, lower-semi-continuous and Ais a linear operator. Then, the dual
of this problem is given by
max JD(y) := -g*(-A*y) - h*(y),
y∈Ω*
where A* is a (Hermitian) adjoint operator of A, i.e., hy, Axi =(A*y, Xi for all x, y.
Proof. We borrow the proof steps from Nachum & Dai (2020). Remark that the definition of conju-
gate function of a function f :
f*(y) := maxhx, yi - f (x),
x
where〈•，•〉is inner product. Based on the definition, We can easily derive the following equations:
min g(x) + h(Ax) = min max g(x) + hy, Axi - h* (y)
x	xy
max
y
mxin g(x) + hy, Axi
- h*(y)
max
y
-mxaxh-A*y,xi - g(x) - h* (y)
max
y
-g* (-A*y) - h* (y).
Now, we prove the proposition:
Proposition 1. Define the objective L(ν; r) as
/	∖	/ -1	∖ τττ>	Γ / ∖ 1	.	/ -1	∖ i Irn
L(V； r)：=(1 - Y)Es〜po [ν(s)] + (1 + a)logE(s,。)〜dU
Then, the following equality holds:
*
min L(wν* , ν; r) = min L(ν; r).
νν
(The proof can be found in Appendix A.)
Aν (s, a)
exP
1+α
Proof. We first define two functions:
g(∙) ：= δ{(1-γ)po}(∙),
h(∙) ：= h-r,∙i + (1 + α)DκL(∙kdu),
where
dE (s, a)
…：=log dUM，
δC(x) ：=0 x∈C,
∞ otherwise.
Then, the corresponding conjugate functions are given by
g*(∙) :=(1 - γ)Epo [∙],
h*(∙):= (1 + α) logEdU
• + r
exP 77一
1+α
(17)
□
12
Published as a conference paper at ICLR 2022
Now, we rewrite the optimization problem (5-7) as follows:
max -δ{(1-γ)po} ( -(YT -B)*d) - DKL(d||dE) - αDKL(dkdu)
=-δ{(1-γ)po} ( -(YT - B)*d) - E(s,a)〜d[—r(S, a)] -(I + α)DKL(dkdU)
=-g( -(YT - B)*d) - h(d).
By Lemma 1, we reformulate the optimization problem as:
max -g( — (YT — B)*d) — h(d)
d
≡ ming*(ν) + h*((γT - B)V)
ν
(1 - Y)Ep0 [ν] + (1 + α) log EdU
(1 -
(1 -
(1 -
exp
(YT- B)ν+r
1+α
Y)Es 〜po[ν (s)] + (1 + α)log E(s,a)〜du
Y)Es 〜po[ν (s)] + (1 + α)log E(s,a)〜du
Y)Es 〜po[ν (s)] + (1 + α)log E(s,a)〜du
exp
exp
exp
Y(Tν)(s, a) - (Bν)(s, a) + r(s, a)
1+α
Y(Tν)(s,a) - ν(s)
1+α
Aν (s, a)
1+α
r
It means that min” L(WK V; r) and min” L(ν; r) are the same optimization problems.
We obtain the conjugate function g* using the fact thatha, yiis the conjugate of δa(χ). We also
compute h* using the two facts: b ∙ f*(y-a) is the conjugate of(a, Xi + b ∙ f (x) and log Ep[exp(y)]
is the conjugate of DKL(xkp).
13
Published as a conference paper at ICLR 2022
B Proof of Corollary 1
We remind the objectives in Equation 16 and Equation 17:
L(wV, V； r) := (I-Y)Es〜P0 [V(S)] + (1 + O)E(S,a)〜dU
Aν (s, a)
exp(π+^ -1
L(V； r) ：= (1 - Y)Es〜po [ν(s)] + (1 + α) log E(s,a)〜&u
exp
AV (s, a
1 + α
where
Aν (S, a) := r(S, a) + Y(TV)(S, a) - V(S),
wV(s, a) := exp (AIV(s, ɑɔ - 1).
ν	1+α
Note that for any constant C, the advantage for V + C is:
A(V+C)(S, a) = r(S, a) + Y(T (V + C))(S, a) - (V(S) + C)
=r(s, a) + Y((Tν)(s, a) + C) - (ν(s) + C)
= r(S, a) +Y(TV)(S,a) - V(S) - (1 - Y)C
= AV (S, a) - (1 - Y)C
Lemma 2. For arbitrary function V and constant C, the equality
Le (V ; r) = Le(V + C ; r)
holds.
Proof. From the definition of L(V; r), we derive the following equation:
Le (V + C ; r)
(I- Y)Es〜po [ν(S) + C] + (1 + α)log E(s,a)〜du
exp
A(ν+C)(S,a)
1 + α
(I- Y)Es〜po [ν(S) + C] + (1 + α)log E(s,a)〜du
AV (S, a) - (1 - Y)C
eχp -----------------
1+α
(1 - Y)Es〜po[ν(s) + C] + (1 + α)log exp (-(II /C) E(s,a)〜d。exp C„
(1 - Y)Es〜po[ν(s) + C] + (1 + α) log E(s,“)〜d。exp (A+^) - (II-+?C
(1 - Y)Es〜po [ν(s)] + (1 + α) log E(s,0)〜d。
exp
AV(s, a)
1 + α
= L(V; r)
□
Lemma 3. For any function V, the following inequality always holds:
τ/ *	∖、7r∕. ∖
L(WV ,ν ； r) ≥ L(ν ； r).
Equality holds if and only if
Γ	A Aν (s,a)	M
E(s,a)〜dU [exp ( 1 + α - 1JJ = 1.
Proof. For any y ≥ 0,
y - 1 ≥ logy,
14
Published as a conference paper at ICLR 2022
and equality holds if and only if y = 1. From this inequality, We can derive the inequality directly:
AV (s,α)
-z------- - 1
1 + ɑ
AV(s, a) - 1
1 + ɑ
and equality holds if and only if
E(s,α)〜du exp
AV (s,a)	ι
--------- 1
1 + ɑ
1.
Finally, We obtain the folloWing results:
L(W工 ν; r) = (1 - Y)ES〜po [ν(s)] + (1 + α)E(s,o)^rfu exp
AV (s,a)	ι
- 1
1 + ɑ
≥ (1 - Y)ES〜pc,[ν(s)] + (1 + α) l log E(s,a)〜W exp
Aν(s, a) - 1
≥ (1 - Y)ES〜po [ν(s)] + (1 + α) log E E(s,a)〜W exp
(1 - Y)ES〜po [ν(s)] + (1 + α)log E(s,a)〜W exp
∖ 1 + α
(AV(s, a)
∖ 1 + α
AV(s, a))
1 + α )
+1
-1 exp(1)
=L(ν; r).
The equality holds if and only if
AV(s, a) - 1
1 + ɑ
1
□
Lemma 4. For any optimal solution ν* = arg min” L(V; r), there is a constant C such that ν* + C
is an optimal solution of min“ L(w*, ν; r).
>Λ	∕' T .	~ -J= 1	. ∙	1	1	. ∙	Γ∙	♦	^Γ /	∖	1
Proof. Let ν* be an optimal solution of arg min” L(ν; r) and
C * := 1-Y log E(s,a)〜dU exp
Then, ^ := ν* + C * satisfies
Ap*(s,a) - 1
1 + ɑ
E(s,a)〜dU exp
Aν(s,a)	ι
-Z------- 1
exp
1 + a y _|
*+c*)(s, a)—-
1 + α /_
AP*(s, a) - (1 - Y)C*	1
exp
exp
1 + ɑ
AP*(s,α) - 1 * * * - Y C* - I
exp
1 + ɑ
Ap* (s, a)
-1
exp
1 + ɑ
Ap*(s,a) - 1
1 + ɑ
( 1 - Y
exp (-E
〜F exp
Av* (s,a)
-------------1
1 + ɑ
-1
1 + ɑ
1.
1 -	.1	ʌ ∙	1	. ∙	1	1	. ∙	Γ∙	/	∖ 1 T	C EI	F	1 ∙ .	f . ∙
Furthermore, V is also an optimal solution of min” L(V; r) by Lemma 2. Then, by equality condition
in Lemma 3,


L(WV, V; r) = L(^; r) = min L(ν; r) ≤ min L(WV) V; r).
V
V
It means, V is an optimal solution of min” L(WV, V r)
□
15
Published as a conference paper at ICLR 2022
Lemma 5. An optimal solution VV = arg minν L(wνV, V; r) is also an optimal solution of
. T/ 、
minν L(V; r)
Proof. From the Lemma 3,
min L(wV, V; r) = L(WV* ,ν*; r) ≥ L(V*; r).
ν
T	Λ 1	. 1	.	T /	⅛	∖	1	∙	/	∖ 1	.1	♦♦	1
Lemma 4 show that minν L(wνV, V; r) and minν L(V; r) have the same minimum value,
Le(VV; r) ≤ L(wνV* , VV; r) = min L(wνV, V; r) = min Le(V; r)
ν
ν
<11 τ.	.1	.	7-" /	⅛	∖ ∙	.1	♦♦	1	Γ∙	1 . 1	Γ∙	士.	. ∙	1	1	Γ∙
holds. It means that L(VV; r) is the minimum value of L and therefore, VV is an optimal solution of
mm” L(ν; r).	□
Finally, we ready to prove the corollary:


Corollary 1. Let V = arg minν L(V; r) and V = arg minν L(wνV, V; r) be the sets of V. Then the
following equation holds: (We provide the proof in Appendix B.)
V = {ν + c∣ν ∈ V,c ∈ R}
(19)
Proof. Lemma 4 and Lemma 5 shows that the last equation holds.
□
16
Published as a conference paper at ICLR 2022
C Proof of Proposition 2
Proposition 2. The objective L(ν; r) is convex with respect to ν. (The proof is in Appendix C.)
Proof. For any functions ν, ν0 and constant λ ∈ [0, 1], we can derive the following equality:
A(λν+(1-λ)ν0) (s, a)
= r(s, a) +γ(T(λν+ (1 - λ)ν0))(s, a) - (λν(s) + (1 - λ)ν0(s))
= r(s, a) + γλ(T ν)(s, a) +γ(1 - λ)(Tν0)(s,a) - (λν(s) + (1 - λ)ν0(s))
= λ(r(s, a) +γ(Tν)(s,a) - λν(s)) + (1 - λ)(r(s, a) +γ(Tν0)(s,a) - λν0(s))
= λAν (s, a) + (1 - λ)Aν 0 (s, a).
It means, Aν is the linear function with respect to ν . Furthermore, using the convexity of log-sum-
exp,
logE(s,a)〜dU [exp(A(λν+(i-λ)ν0)(s, a))]
=logE(s,a)〜dU [exp(λAν(s,a) + (1 - X)A“，(s, a))]
≤ λlog E(s,a)〜dU [exp(Aν(s, a))] + (1 - λ) logE(s,0)〜d。[A”，(s, a))]
Therefore, L(V; r) is convex function With respect to V.	□
D Policy extraction
By the definition of Aν,
(A(ν*+C)(S,a)	1
exp _-------------1
1+α
exp AA>*(S,a) - (I - Y)C
∖	1 + α
-1
Therefore,
(X exp (A(V*+C)(S,a) - 1
∖	1 + α
d*(s, a)
dU (S, a)
17
Published as a conference paper at ICLR 2022
E Experimental Details
Implementation detail For fair comparison, we use the same learning rate to train actors of BC
and DemoDICE. In DemoDICE, we applied the same regularization coefficient α = 0.05 in Equa-
tion 17 across all the tasks and dataset configurations. We implement our network architectures for
BC and DemoDICE based on the implementation of OptiDICE2. For ValueDICE, we use its offi-
cial implementation3 * without any modification to network architectures or hyperparameters. Based
on DAC (Kostrikov et al., 2019), we treat terminal states as absorbing states in ValueDICE and
DemoDICE.
For stable discriminator learning, we use gradient penalty regularization on the r(s, a) function,
which was proposed in Gulrajani et al. (2017) to enforce 1-Lipschitz constraint. To stabilize critic
training, we add gradient L2-norm to the critic loss for the regularization. Detailed hyperparameter
configurations used for our main experiments are summarized in Table 1.
Hyperparameters	BC	DemoDICE
γ (discount factor)	0.99	0.99
α (regularization coefficient)	-	0.05
learning rate (actor)	3 × 10-5	3 × 10-5
network size (actor)	[256, 256]	[256, 256]
learning rate (critic)	-	3 × 10-4
network size (critic)	-	[256,256]
learning rate (discriminator)	-	3 × 10-4
network size (discriminator)	-	[256,256]
gradient L2-norm coefficient (critic)	-	1 × 10-4
gradient penalty coefficient (discriminator)	-	10
batch size	256	256
# of expert trajectories	1	1
# of training iterations	1,000,000	1,000,000
Table 1: Configurations of hyperparameters used in our experimental results.
E.1 Experimental Details for BCND
We implement the BCND based on the BCND paper (Sasaki & Yamashina, 2021). We shuffle all
demonstrations and divide them into K disjoint sets {Dk}kK=1. Then, for each iteration, we perform
stochastic gradient ascent to update each policy ∏θk using DK. Finally, We update reward R(s, a)
and η using
1K
7^J∕ ʌ	, ɪ	X^~λ	/ I ʌ	V√∕ 、一 ΓΛ
R(S,a)	- K	ʌ,	πθk (OIS),	∀(s,	a) ∈ d,
K
η <---------------------	.
Es 〜D [Σ k πθk (μθk (S)IS)]
Here, we use following hyperparameters as reported in paper (Sasaki & Yamashina, 2021): We
use M = 5 (number of iteration), K = 5 (number of policy / disjoint dataset), N = 128 (batch
size), L = 500 × IDk I/N for each Dk (k ∈ {1, . . . , K}, in our tasks, L ∈ [3 × 105, 1.1 × 106]),
∏θ(a∣s) = KK PK=I ∏θk (a∣s), Z = 10-4 (shared learning rate), Adam optimizer with learning rate
Z X η for gradient ascent, and Gaussian policy ∏θk = N(μjk (s), σθ2k). For Guassian policy, we use
neural network with size [100,100] for μjk, and a trainable independent vector for σjk, as reported
in the paper.
2
https://github.com/secury/optidice
3
https://github.com/google- research/google- research/tree/master/value_dice
18
Published as a conference paper at ICLR 2022
We measure the performance of algorithm for each internal iteration. In Figure 1 and Figure 2, we
report the best performance and corresponding standard error of the algorithm among the M internal
iterations (we compute the average performance for each iteration and pick the maximum average
performance as the best performance).
Algorithm 1 Behavioral Cloning from Noisy Demonstrations
Require: Noisy expert demonstrations D, policy parameters {θk}kK=1, learning rate ζ
Ensure: Ensemble policy πθ = K PK=I πθk
Set R(S, a) = 1 for ∀(s,a) ∈ D.
Split D into K disjoint sets {D1, D2 , . . . , DK }.
for iteration= 1, . . . , M do
for k = 1, . . . , K do
Initialize parameters θk .
for l = 1, . . . , L do
Sample a minibatch Bk = {(sn, an)}nN=1 from Dk .
Calculate a sampled gradient N PN=I N$k log∏ek(sn,an) ∙ R(Sn,an).
Update θk by gradient ascent with learning rate ζ × η
end for
end for
Update R(S, a) and η
end for
19
Published as a conference paper at ICLR 2022
F Additional Experimental Results
In this section, we report additional experimental results for detailed empirical analysis of our ap-
proach. We focus three experiments as follows: (1) asymptotic performance of our approach on
HalfCheetah environment for the mixed dataset we considered in Section 5.1, (2) behavior cloning
with more diverse β selections which are omitted in Figure 1, (3) ablation study on hyperparameter
α for DemoDICE, which is appeared in Equation 17.
F.1 Asymptotic Performance of DemoDICE on HalfCheetah
To inspect asymptotic behaviors of DemoDICE on our mixed dataset tasks in HalfCheetah environ-
ment, we extend the number of training iterations into 2 million with fixing the other hyperparameter
settings. As Figure 3 shows, DemoDICE converges to the expert performance for all tasks we con-
sidered in Section 5.1.
Figure 3: 1 Learning curves of DemoDICE during 2 million training iterations in HalfCheetah mixed
dataset tasks (M1, M2, M3).
20
Published as a conference paper at ICLR 2022
F.2 Behavior Cloning Performance
In Figure 1, we omitted performance of BC with β = 0.25 and β = 0.75 for simplicity. Figure 4
shows performance of entire BC configurations, i.e., β ∈ {0, 0.25, 0.5, 0.75, 1.0}.
Hopper-M 1(400:100)	…	VVaIker2 d-M1(400:100)
OTE3%tf pez=eUUoN
O.OM 0.2M 0.4M 0.6M 0.8M	IM
Ant-M 1(400:100)
HalfCheetah-Ml(AOOrIOO)
O.OM 0.2M 0.4M 0.6M 0.8M	IM
O.OM 0.2M 0.4M 0.6M 0.8M IM
OTE3%tf pez=eUUoN
H。PPer-M2(400:400)
IOO
80
60
40
20
Walker2d-M2(400:400)
100
80
60
40
20
0.0M 0.2M 0.4M 0.6M 0.8M IM
Ant-M2(400:400)
0.0M 0.2M 0.4M 0.6M 0.8M IM
HalfCheetah-M2(400:400)
0.0M 0.2M 0.4M 0.6M 0.8M IM
, HOPPer-M3(400:1600)
1m80m40m
OTE3%tf pez=eUUoN
WaIker2d-M3(400:1600)
∞ M « M O
O.OM 0.2M 0.4M 0.6M 0.8M IM
Ant-M3(400:1600)
0.0M 0.2M 0.4M 0.6M 0.8M IM
HaIfCheetah-M3(400:1600)
0.0M 0.2M 0.4M 0.6M 0.8M IM
,	Hopper-RB
OTE3%tf pez=eUUoN
0.0M 0.2M 0.4M 0.6M 0.8M	IM
Training iterations
O.OM 0.2M 0.4M 0.6M 0.8M IM
b	Ant-RB
0.0M 0.2M 0.4M 0.6M 0.8M IM
HaIfCheetah-RB
0.0M 0.2M 0.4M 0.6M 0.8M IM
b	Walker2d-RB
0.0M 0.2M 0.4M 0.6M 0.8M IM
Training Iterations
0.0M 0.2M 0.4M 0.6M 0.8M	IM
Training iterations
0.0M 0.2M 0.4M 0.6M 0.8M IM
Training Iterations
mm4°M
-Expert BCND —— BC (β = 0)	—— BC (β = 025)	—— BC (β = 0.5)	—— BC (β = 0.75)	—— BC (β= 1)	—— DemoDICE (ours)
Figure 4:	Performance of variants ofBC with β ∈ {0, 0.25, 0.5, 0.75, 1.0} and DemoDICE both on
the mixed datasets (M1, M2, M3) and the replay buffer datasets. We plot the mean and the standard
errors (shaded area) of normalized scores over 5 random seeds.
21
Published as a conference paper at ICLR 2022
F.3 Different Quality of Imperfect Demonstrations
In this section, we construct new tasks where random trajectories in mixed dataset (see 5.1) are
replaced by medium trajectories, named I1, I2, and I3. Here, all tasks share the expert demonstrations
composed of the single expert trajectory. Imperfect demonstrations in each task are composed of 400
expert trajectories and medium trajectories, and the number of medium trajectories is 100 for the task
I1, 400 for I2, and 1600 for I3. In Figure 5, we observed that DemoDICE performs well in these new
tasks.
120
W 100-
3 80
S.
P 6。-
(υ
N 40-
75
U 20-
z o-
Hopper-11(400:100)
120-
IOO
80
60-
40-
20-
Wa lker2d-ll(400:100)
0.0M 0.2M 0.4M 0.6M 0.8M IM
Training Iterations
O.OM 0.2M 0.4M 0.6M 0.8M IM
Training Iterations
Expert —— Medium —— BC(β = O) —— BC 伊= 0.5)	—— BCo = I)	—— VSIueDICE —— DemoDICE (ours)
Figure 5:	Performance of DemoDICE and baseline algorithms on alternative mixed-dataset tasks
I1, I2, and I3. Bold lines and shaded areas indicate the means and the standard errors of normalized
scores over 5 random seeds respectively.
22
Published as a conference paper at ICLR 2022
F.4 Different Coverage of Expert Demonstrations
In this section, we introduce additional experiments with different coverages of expert demonstra-
tions. Concretely, we constructed expert demonstrations using an increasing number of expert tra-
jectories, namely 1, 2, 5, up to 10. Because DemoDICE already performs well in M1 and M2 tasks,
we compare M3 and RB tasks for this ablation study. In this experiments, DemoDICE is not signif-
icantly better than other baselines when the expert demonstrations cover enough space. We remind
the readers that when the coverage is not high, DemoDICE significantly outperforms the others as
shown in Figure 1 and 2 in the main text.
HoPPer-M3(400:1600)	Walke r2d-M3(400:1600)	HaIfCheetah-M3(400:1600)
Ant-M3(400:1600)
SEn4"≈P"Z-"LLUON
1	2	5	10
2	5
Hopper-RB
2	5
Walker2d-RB
2	5	10
HaIfCheetah-RB
Ant-RB
SE∩4"≈P"Z-"LLUON
Number of Expert Trajectories	Number of Expert Trajectories
Number of Ex pert Traj ecto ries	Numberof Expert Trajectories
Expert —— BC (β = 0)	—— BC (β = 0.25)	—— BC (/3 = 0.5)	—— BC 窗=0.75)	—— BC(j3 = l) —— VaIueDICE —— DemoDlCE (ours)
Figure 6:	We compare DemoDICE with baselines when the number of expert trajectories constitut-
ing expert demonstrations is 1, 2, 5, and 10. Each performance is measured as a normalized score
averaged by the last 5 evaluations during training. Bold lines and shaded areas indicate the means
and the standard errors of converged performances over 5 random seed respectively.
23
Published as a conference paper at ICLR 2022
F.5 ABLATION TEST ON HYPERPARAMETER α
As we can notice in Equation 17, DemoDICE introduces a hyperparameter α, which is a regulariza-
tion coefficient. In this section, we aim to study how does the choice of α effect to training processes
and performance of our algorithm. We select α ∈ {10, 1, 0.5, 0.1, 0.05, 0} and train on M3 datasets
and replay buffer datasets by preserving the other settings same as before. Figure 7 describes the
results of ablation study. We point out that DemoDICE shows almost the same performance in when
α ∈ [0, 0.1].
Expert ------- α= 10	---- α = 1	---- α = 0.5	---- <z = 0.1	---- σ = 0.05	---- α = 0
Figure 7:	Performance of DemoDICE by varying α ∈ {10, 1, 0.5, 0.1, 0.05, 0} on the mixed datasets
M3 and the replay buffer datasets. Bold lines and shaded areas indicate the means and the standard
errors (shaded area) of normalized scores over 5 random seeds respectively.
24
Published as a conference paper at ICLR 2022
F.6 Ablation Test on Pure Offline Imitation Learning
In this section, we compare DemoDICE and BC using only expert demonstrations. In Figure 8,
we observe that DemoDICE and BC show similar performance. In pure offline IL, DemoDICE
essentially tries to find a stationary distribution that minimizes DKL(d||dE) on the MLE MDP con-
StrUcted by DE. Let dE be the empirical stationary distribution of dE. Then, the optimal solution
is trivially given by d* = dE with DKL(d||dE) = 0 since dE itself is a valid stationary distribu-
tion on the MLE MDP. Finally, extracting a policy from dE reduces to a simple behavior cloning:
min∏ -E(s,a)〜dE [log π(a∣s)], which concludes that DemoDICE would not be better than BC in the
pure offline IL setting.
0.2M 0.4M 0.6M 0.8M IM
Walker2d⅛l
0.0M 0.2M 0.4M 0.6M 0.8M IM
Walker2d#2
120 100	HoPPer#5	120 IOO	HoPPer#10
			
80		80	
	ιJ a∣λΛ∕VXΛ ʌʌ.		
60 40		60 40	产 d VM∣yWW,
			f
20		20	
	TZ		匚
0		O	
			
0.0M 0.2M 0.4M 0.6M 0.8M IM
Walker2d#5
SEn4∙H πωN--nE^OZ
120-
gIOO
⅛ 80
P 60
(υ
.H 40
75
E 20
120
120
8 Q
8 Q
6Q
6Q
O.OM 0.2M 0.4M 0.6M 0.8M IM
O.OM 0.2M 0.4M 0.6M 0.8M IM
120
HaIfCheetah#2
100
80
60-
40-
20-
0.0M 0.2M 0.4M 0.6M 0.8M IM
Walker2d≠10
HalfCheetah#1
g 100
I 80
1 60-
(u
.H 40-
a
E 20-
i o∙
O.OM 0.2M 0.4M 0.6M 0.8M IM
Ant#l
O.OM 0.2M 0,4M 0.6M 0.8M IM
Training Iterations
O.OM 0.2M 0.4M 0.6M 0.8M IM
Expert ---- BC ----- DemoDlCE (expert only)
Figure 8:	Performance of DemoDICE and BC with only expert demonstrations. For each environ-
ment, we construct expert demonstrations using an increasing number of expert trajectories, namely
1, 2, 5, up to 10. Bold lines and shaded areas indicate the means and the standard errors of normal-
ized scores over 5 random seeds respectively.
25
Published as a conference paper at ICLR 2022
G Detailed Analysis of Empirical Results
In this section, we provide additional discussions about empirical results.
First, M1 is an easy task where most of the trajectories are from the expert, so some baselines also
succeed to achieve good performance.
Environment	All trajectories	〉 100	> 90	> 80	> 70	> 60	> 50
Hopper	3514 =	0	117	176	217	281	385
Walker2d	1887	0	0	325	564	638	693
Ant	1318	333	479	576	629	672	727
HalfCheetah	999	0	0	0	143	467	689
Table 2: Statistics of trajectories in full_replay-v2 in D4RL datasets.
To discuss about RB tasks, we show the normalized score statistics of trajectories in the replay buffer
in the Table 2. For each environment, we counted the number of trajectories that have a normalized
score greater than R as > R, so the numbers above are cumulative counts.Thus, this table suggests
natural upper bound of performances of imitation learning algorithms, e.g. it would be very hard to
score around 100 for HalfCheetah task.
In the Ant-RB task, we have enough good trajectories, especially > 100, so both DemoDICE and
BC(β = 0) recover expert policy.
In the HalfCheetah-RB task, there is no trajectory with > 80, so DemoDICE eagerly distinguishes
the non-expert trajectories from expert ones, throwing away non-expert demonstrations. Thus it
overall behaves like BC(β = 0).
In the Walker2d-RB task, both ValueDICE and DemoDICE show similar performances at around 80
at the end. Looking at the statistics above, since the best trajectory in RB has its normalized score
less than 90, we believe that ValueDICE and DemoDICE both achieved reasonable performance
levels.
26