Published as a conference paper at ICLR 2022
How Did the Model Change? Efficiently As-
sessing Machine Learning API Shifts
Lingjiao Chen, Matei Zaharia, James Y. Zou
Stanford University
{lingjiao,jamesz}@stanford.edu matei@cs.stanford.edu
Ab stract
ML prediction APIs from providers like Amazon and Google have made it simple
to use ML in applications. A challenge for users is that such APIs continuously
change over time as the providers update models, and changes can happen silently
without users knowing. It is thus important to monitor when and how much the ML
APIs’ performance shifts. To provide detailed change assessment, we model ML
API shifts as confusion matrix differences, and propose a principled algorithmic
framework, MASA, to provably assess these shifts efficiently given a sample
budget constraint. MASA employs an upper-confidence bound based approach to
adaptively determine on which data point to query the ML API to estimate shifts.
Empirically, we observe significant ML API shifts from 2020 to 2021 among 12 out
of 36 applications using commercial APIs from Google, Microsoft, Amazon, and
other providers. These real-world shifts include both improvements and reductions
in accuracy. Extensive experiments show that MASA can estimate such API shifts
more accurately than standard approaches given the same budget.
1	Introduction
Machine learning (ML) prediction APIs have made it dramatically easier to build ML applications.
For example, one can use Microsoft text API (Mic, a) to determine the polarity of a text review
written by a customer, or Google speech API (Goo, b) to recognize users’ spoken commands received
by a smart home device. These APIs have been gaining in popularity (MLa; Chen et al., 2020), as
they eliminate the need for ML users to train their own models.
Monitoring and assessing the performance of these third-party ML APIs over time, however, is
under-explored. ML API providers continuously collect new data or change their model architectures
(Qi et al., 2020) to update their services, which could silently help or harm downstream applications’
performance. For example, as shown in Figure 1 (a) and (b), we observe a 7% overall accuracy drop
of IBM speech API on the AUDIOMNST dataset in March 2021 compared March 2020. In our
systematic study of 36 API and dataset combinations, there are 12 cases where the API’s performance
changed by more than 1% on the same dataset from 2020 to 2021 (sometimes for the worse). Such
performance shifts are of serious concern not only because of potential disruptions to downstream
tasks, but also because consistency is often required for audits and oversight. Therefore, it is important
to precisely assess shifts in an API’s predictions over time. Moreover, in this assessment, it is often
more informative to quantify how the entire confusion matrix of the API has changed rather than
just the overall accuracy. In the IBM case in Figure 1, it is interesting that a major culprit of the
drop in performance is the 2021 model mistaking “four” for “five”. In other settings, changes in the
confusion matrix could still cause issues even if the overall accuracy stays the same.
In this paper, we formalize the problem of assessing API shifts as estimating changes in the confusion
matrix on the same dataset in a sample-efficient manner (i.e., with few calls to the API itself to
minimize dollar cost). The straightforward approach is to compare the API’s prediction on randomly
sampled data. However, this can require a large number of API calls to estimate the confusion matrix,
which is expensive given that each call costs money. To help address this challenge, we propose
MASA, a principled algorithm for ML API shift assessments. MASA efficiently estimates shifts
in the API,s confusion matrix by clustering the dataset and adaptively sampling data from different
clusters to query the API. MASA automates its sampling rate from different data clusters based on
1
Published as a conference paper at ICLR 2022
OVeraIl accuracy:98.3
MASA
Uniform
0.0 0.1 0.0
0.0 0.0 0.0
0.0 0.0 0.0
0.2 0.0 0.0
500 IOOO 1500 2000
Sample size
(C)Shiftestimatorperformance
0.0 0.0 0.0
6 7 8
label
(a) Confusion matrix 2020 (%)
n⅞3 æo o.ogg		o.o	0.0	0.3 0.0 0.0 0.0		0.0	0.1 0.1	W 0.0	0.0	0.7 0.1
0.0	0.0	9.4		0.0	0.0		W	回		oɪ
o.o	0.0	0.1	9.4	0.0	0.0	0.2	0.0	0.1	0.0	0.1
0.1	0.0	0.0	0.0	7.1	2.6	0.0	0.0	0.0	0.0	0.3
Ty	W	而	i∏f∏∏8.9			⅛τ		0.0 0.3		0.6
	W		IiTil	m	FTl	9.7		Γo	■	0.1
o.o	0.1	0.0	0.0	0.0	0.0	0.1	9.7	10.0	0.0	0.1
~0Λ	æo-	硕	y∑	yy	^o	^δ^	0.0	o.o IiW		æ4
o.o	o.o	0.0	0.0	0.0	0.0	o.o	0.0			0.4
o.o	o.o	0.0	0.0	0.0	0.0	o.o	0.0	0.0	0.0	0.0
Predicted label
(b) Confusion matrix 2021 (%)
Figure 1: ML API shift for IBM speech recognition API on AMNIST, a spoken digit dataset. (a) and
(b) give its (normalized) confusion matrix in April 2020 and 2021, respectively. There is an overall
7% accuracy drop. One factor is the 2021 model incorrectly predicting more “four” as “five”. (c)
Given a sample budget, the proposed MASA can assess the API shift with much smaller error in
Frobenius norm compared to standard uniform sampling.
the uncertainty in the confusion matrix estimation. For example, MASA may query the ML API on
more samples with the true label “four” than “one”, if it is less sure about the estimated performance
change on the former. Employing an upper-confidence-bound approach to estimate the uncertainties,
MASA enjoys a low computation and space cost as well as a fast estimation error rate guarantee.
MASA’s adaptive sampling substantially improves the quality of estimation for API shifts. In
extensive experiments on real world ML APIs, MASA’s assessment error is often an order of
magnitude smaller than that of standard uniform sampling with same sample size (e.g., Figure 1 (c)).
To reach the same tolerable target estimation error, MASA can reduce the required sample size by
more than 50%, sometimes up to 90%.
Contributions. In short, our main contributions include:
1.	We demonstrate that commercial Ml APIs can experience significant performance shifts over
time, and formulate ML API shift assessments via confusion matrix difference estimation as
an important practical problem. We will release the dataset consisting of 1,224,278 samples
annotated by ML APIs in different date to stimulate more research on ML API shifts.
2.	We propose MASA, an algorithm to assess the ML API performance shifts efficiently.
MASA adaptively determines querying the ML API on which data points to minimize the
shift estimation error under a sample size constraint. We show that MASA enjoys a low
computation cost and performance guarantees.
3.	We evaluate MASA on real world APIs from Google, Microsoft, Amazon and other providers
for tasks including speech recognition, sentiment analysis, and facial emotion recognition.
MASA leads to estimation errors an order of magnitude smaller than standard uniform
sampling using the same sample size, or over 90% fewer samples to reach the same tolerable
estimation error. Our code and datasets are also released 1.
Related Work. Distribution shifts in ML deployments: Performance shifts in ML systems have
been observed in applications like disease diagnosis (Lipton et al., 2018), facial recognition (Wang
et al., 2020), and Inference of molecular structure (Koh et al., 2020). Most of them are attributed
to distribution shifts, i.e., the distribution of the test and training datasets are different. Distribution
shifts are usually modeled as covariate shifts (Shimodaira, 2000; Sugiyama et al., 2007; Quinonero-
Candela et al., 2009) or label shifts (Lipton et al., 2018; Saerens et al., 2002; Azizzadenesheli et al.,
2019; Zhao et al., 2021). API shifts are orthogonal to distribution shifts: instead of attributing the
performance shifts to data distribution changes, API shifts concern with ML APIs changes which
changes its predictions on the same dataset. The methods for detecting distribution drifts typically
rely on changes in data feature statistics and can not detect changes in the API on the same data. To
the best of our knowledge, MASA is the first work to systematically investigate ML API shifts.
1https://github.com/lchen001/MASA
2
Published as a conference paper at ICLR 2022
YELP IMDB WAIMAI SHOP
Dataset
(a) Sentiment Analysis
FER+ RAFDB EXPW AFNET
Dataset
(b) Facial Emotion Recognition
DIGIT AMNIST CMD FLUENT
Dataset
(c) Speech Recognition
Figure 2: Observed overall accuracy changes. Each row corresponds to an ML API, and each column
represents a dataset. The entry is the overall accuracy difference between evaluation in spring 2020
and spring 2021. In 12 out of 36 cases, the API’s overall accuracy changed by more than 1%; this
includes several cases of substantial drops in performance.
Deploying and monitoring ML APIs: Several issues in deployed ML APIs have been studied.
For example, Buolamwini & Gebru (2018) showed that strong biases toward minority can exist in
commercial APIs and Ribeiro et al. (2020) revealed that several bugs in commercial APIs can be
detected using checklists. Kang et al. (2020) extend program assertions to monitor and improve
deployed ML models. Chen et al. (2020) consider the trade-offs between accuracy performance and
cost via exploiting multiple APIs. On the other hand, the proposed MASA focuses on estimating
(silent) API performance changes cheaply and accurately, which has not been studied before.
Stratified sampling and multi-arm bandits: Stratified sampling has proved to be useful in various
domains, such as approximate query processing (Chaudhuri et al., 2007), population mean estimation
(Carpentier et al., 2011; 2015), and complex integration problems (Lepretre et al., 2017). A common
approach is to model stratified sampling as a multi-arm bandit (MAB) problem: view each data
partition as an arm, and set the regret as the variance of the obtained estimator. Plenty of algorithms
exist for solving standard MAB problems, such as the epsilon greedy algorithm (Slivkins, 2019),
upper-confidence-bound approach (Auer et al., 2002), and Thompson sampling method (Russo et al.,
2018). While those algorithms aim at playing the best arm as often as possible, stratified sampling’s
goal is often to estimate the average of all arms, and thus needs to allocate the number of calls for
all arms aptly based on their variance. In contrast to estimating the population average in standard
stratified sampling, our goal is to simultaneously estimate a matrix whose entries can be correlated.
Thus, we have an objective (uncertainty score) that is different from theirs (variance), which also
leads to an optimal allocation different from that of standard stratified sampling. This difference
requires additional statistical bounds to prove convergence, which we provide in our paper. It also
results in a different upper-confidence-bound term and convergence rate compared to standard bandit
algorithms.
2	The API Shift Problem
Empirical assessment of ML API shifts. We start by making an interesting observation: Commer-
cial ML APIs’ performance can change substantial over time on the same datasets. We investigated
twelve standard datasets across three different tasks, namely, YELP (Dat, c), IMDB (Maas et al.),
WAIMAI (Dat, b), SHOP (Dat, a) for sentiment analysis, FER+ (Goodfellow et al., 2015), RAFDB
(Li et al.), EXPW (Zhang et al.), AFNET (Mollahosseini et al., 2019) for facial emotion recognition,
and DIGIT (Dat, d), AMNIST (Becker et al., 2018), CMD (Warden, 2018), FLUENT (Lugosch et al.),
for speech recognition. For each dataset, we evaluated three commercial ML APIs’ accuracy in April
2020 and April 2021. Figure 2 summarizes the overall accuracy changes.
There are several interesting empirical findings. First, API performance changes are quite common. In
fact, as shown in Figure 2, API performance changes exceeding 1% occurred in about 33% of all (36)
considered ML API-dataset combinations. Since the data distribution remains fixed, such a change is
due to ML APIs’ updates. Second, the API updates can either help or hurt the accuracy performance
depending on the datasets. For example, as shown in Figure 2 (a), the Amazon sentiment analysis
3
Published as a conference paper at ICLR 2022
Figure 3: How MASA works. MASA first partitions the dataset. Then it picks which partition to
sample based on some uncertainty measurement, queries the ML API on the drawn sample, and uses
the API’ prediction to update uncertainty and estimated shifts on this partition. This is repeated until
the ML API has been queried N times. Finally, the estimated shifts on different partitions are aptly
fused to obtain the desired API shifts.
API’s accuracy increases on YELP, WAIMAI, and SHOP, but decreases on IMDB. In addition, the
update of Microsoft facial emotion recognition API only affects performance on the FER+ dataset, as
shown in Figure 2 (b). Another interesting finding is that the magnitude of the performance change
can be quite different. In fact, most of the accuracy differences are between 1-3%, but on DIGIT
dataset, Google’s accuracy change is more than 20%.
Fine-grained assessment of API shift as changes in the confusion matrix. Based on feedback
from practitioners, accuracy change alone is insufficient, and attribution to per class change is often
much more informative (Tsipras et al., 2020; Hou et al., 2019; Luque et al., 2019). Thus, a natural
idea is to quantify an ML API’s performance by its confusion matrix. We assess the change of the
confusion matrix over time as a measure of API shift.
Formally, consider an ML service for a classification task with L labels. For a data point x from
some domain X, let y(x) ∈ [L] denote its predicted label on x, and y(χ) be the true label. For
example, for sentiment analysis, x is a text paragraph, and the task is to predict if the polarity
of X is positive or negative. Here L = 2, and y(x) = 1 implies positive predicted label while
y(x) = 2 indicates negative true label. The confusion matrix is denoted by C ∈ RL×L where
Ci,j，Pr[y(x) = i, y(x) = j]. Given a confusion matrix of the ML API measured previously (say,
a few months ago), Co, the ML API shift is defined as ∆C , C - CO .
Using confusion matrix difference to quantify the ML API shift is informative. E.g, the overall
accuracy change is simply the trace of ∆C. It also explains which label gets harder or easier for
the updated API. Still consider, e.g., sentiment analysis. Given a 2% overall accuracy change,
∆C1,2 = 1% and ∆C2,1 = -3% implies that the change is due to predicting less (-3%) negative
texts as positive, by sacrificing the accuracy on positive texts slightly (1%). This suggests that the
API could have been updated with more negative training texts.
3	MASA: ML API Shift Assessment
Now we present MASA, an algorithmic framework efficiently to assess ML API shifts. Suppose the
old confusion matrix Co and a large labeled dataset D are available. Given a query budget N, our
goal is to generate ∆C, an estimation of the API shifts as accurately as possible by querying the ML
API y(∙) on N samples drawn from D.
MASA achieves its goal via an adaptive sampling approach (Figure 3). It first divides the given
dataset D into several partitions (clusters). Then it adaptively decides which sample to query the ML
API in an iterative manner: at each iteration, it selects one data partition based on some uncertainty
measure (defined below), and queries the ML API on one sample randomly drawn from this partition.
The API,s prediction is obtained to update the uncertainty measure as well as the estimated shift ∆C.
This process is repeated until the ML API has been queried N times or if a stopping rule is reached.
We explain each step in detail next.
3.1	Data Partitioning
A key intuition in MASA is that not all samples are equally informative for estimating API shifts.
Consider, for example, a vision API makes perfect predictions on “dog” images, and guesses randomly
on ”cat“ pictures. The “dog” images are less informative, as even a small sample of “dog” queries
4
Published as a conference paper at ICLR 2022
would tell that there is essentially no confusion for this class. Intuitively, within a sample budget,
an estimator with more samples from “cat” pictures should be more accurate overall than that from
“dog”. Generally, harder images tend to be more informative.
Thus, it is a natural idea to partition all data points based on factors that may correlate with their
informativeness, and sample from those partitions separately. In MASA, we use partitions Di,k
that each contain the points with true label i and difficulty level k. The difficulty level is an integer
indicating how hard it is to predict the data point’s label. It needs not be perfect, and can be simply
the discretized prediction confidence generated by some simple ML models. A total of L labels and
K distinct difficulty labels lead to a total of LK partitions. If the uncertainty or variability of the ML
API’s prediction on each partition is different, then drawing a different number of samples from each
partition may improve the shift assessment performance compared to standard uniform sampling. We
verify this empirically in our evaluation (Section 4).
3.2	Budget Allocation Problem
Given the data partition, two questions arise: (i) how many samples should be drawn from each
partition, and (ii) how to estimate the ML API shifts given available samples. The second question is
relatively straightforward. Note that the API shifts satisfy
LK
∆Ci,j = Pr[y(x) = i, y(x) = j] — C oj = XX
Pr[y(x) = i, y(x) = j,x ∈ Di,k] — Coj
i=1 k=1
KK
=XPr[y(x) = j,x ∈ Di,k] — Co,j = XPr[x ∈ Di,k]Pr[y(x)= j|x ∈ Di,k] — Coj
k=1	k=1
where the first equation is by definition, the second is due to total probability rule, the third uses
the fact that x ∈ Di,k implies y(x) = i, and the last equation applies conditional probability. Here,
Pr[x ∈ Di,k] is simply ratio of size of partition Di,k and entire dataset D, known a prior. To assess
∆Ci,j, We only need to estimate Pr[y(χ) = j |x ∈ D%,k], the predicted label distribution on partition
Di,k. It can be estimated simply via the frequency of predicting label j among all available samples
draWn from Di,k.
NoW We consider the sample allocation problem. For ease of notation, We denote Pr[xi ∈ Di,j]
by pi,k, Pr[y(x) = j|x ∈ D%,k] and its estimation by μ3k,j and μ力必,respectively. Then for
deterministic sample allocations, the squared Frobenius norm error can be Written as
E [k∆C — ∆C kF i = X E (∆Ci,j - ∆C i,j)2 = X E (X Pi,k [μi,k,j — μi,k,j ])
i,j	i,j	k
=〉:pi,,kE ([μi,k,j — μi,k,j])
i,j,k
ThUS we use the loss L(A, N) , Pi,j,® p,kE ([μi,k,j —μi,k,j])2 to measure the performance of any
sample budget allocation algorithm A using N samples. For any fixed N, our goal is to find a sample
budget allocation algorithm A to minimize the loss L(A, N). Notably, we can generalize it for other
scenarios by replacing (∆C — ∆C) with W (∆C — ∆C), where is element-wise multiplication
and W is an L X L weight matrix. Different choices of W can penalize the error of each entry in ∆C
differently and serve for different purposes. For example, if misclassifying label 1 as label 2 is the
only focus, then we can simply set W1,2 = 1 and Wi,j = 0, ∀(i, j) 6= (1, 2). If we use the identity
matrix as the weight W , then it becomes equivalent to estimating the overall accuracy by minimizing
the variance. To minimize the loss for the general scenarios, Algorithm 1 (which is explained in
the rest of this section) is still applicable by simply multiplying the API selection formula (line 3 in
Algorithm 1) with its corresponding weights.
3.3	Uncertainty Score and Optimal Allocation
The optimal sample allocation is directly connected to how informative each data partition is. To see
this, let us first introduce the notation of uncertainty score for each data partition.
5
Published as a conference paper at ICLR 2022
Algorithm 1 MASA’s ML API shift assessment algorithm.
1
2
3
4
5
6
7
8
9
10
Input :ML API y(∙), query budget N, partitions Di,k,P ∈ R* l×k, Co ∈ Rl×l, and a > 0
Output: Estimated ML API Shift ∆C ∈ RL × L
Set N = OL×K ,μ = 0L×κ×L,σ = 0L×κ ,H = 0L×κ×L	. Initialization
for n — 1 to N do
(i, k),	ifNi,k <2
(i*,k*)—《	Pik (八	r——λ	,	. Determine data partition
[argmaχi,k NN⅛ «,k + 4J 危>。/
Sample Xn from Di*,k* and query the ML API to obtain y(xn)
Ni*,k* J Ni*,k* + 1	. Update sample size
"i*,k*,j J μi*,k*,j + Iy(XnNj* μi*,k*,j, ∀j ∈ [L]	. Update predicted label distribution
{2Hi*,k*,y(xn),	ifNi*,k* < 2 *
ι-Hi*,k*,y(Xn) -σ2* fc*	. Update uncertainty score
σ2*,k* +-----i*N*⅛~~—,	o/w
Hi*,k*,y(xn) J Hi*,k*,y(xn) + 1	. Update label frequency
end
Return ∆C ∈ RL×L where ∆Ci,j = PK=Ipik①邛4 - C0,j,∀i,j
. Confusion estimation
Definition 1. σ2 k，(1 一 PL=I Pr2[y(x) = j|x ∈ Di,k]) denotes the uncertainty score ofDi,k.
The uncertainty score quantifies how informative each Di,k is by subtracting from 1 the sum of
the square of each label’s probability mass. The uncertainty score is related to collision entropy
(discussed in Appendix A), and determines the optimal allocation as follows.
Lemma 1. Let A* be the sample allocation algorithm that achieves the smallest expected squared
Frobenius norm error. Then the number ofsamples drawnfrom Di,k by A* is
N *. =	pi,k σi,k— N
i,k	Pi,k 2,k
Lemma 1 shows that the optimal budget allocation depends on the uncertainty score, but in practice,
we do not know the uncertainty score before drawing samples and querying the ML API. Thus, a
natural question is how to estimate the uncertainty score σ2 k Suppose n samples, X1,X2, ∙ ∙ ∙ ,Xn,
are drawn from partition Di,k. Then we can estimate σi2,k by
1 nn
σ2,k 4 1 一 n(n 一 1)X X Iy(Xs)=y(χt)	(3.1)
s=1 t:t=1,t6=s
3.4 An Uncertainty-aware Adaptive Sampling Algorithm
Now we have a chicken-and-egg problem: estimating the uncertainty scores is needed to find the
optimal sample allocation, but sampling from all partitions is needed to estimate their uncertainty
scores. To overcome this issue, we adopt an iterative sampling approach, as shown in Algorithm
1. At each iteration, it alternates between (i) uncertainty score-based new sample selection (line 3
- 4) and (ii) uncertainty score and predicted label distribution update using the new sample (line 5
- 8). After querying the ML API N times, the API shifts are obtained by (iii) fusing the estimated
predicted label distribution on each partition (line 10). We give the details as follows.
Uncertainty score and predicted label distribution update. After obtaining the predicted label for
a sample from partition Di*,k* , we need to update (i) the number of samples already drawn from this
partition, denoted by Ni*,k*, (ii) the estimated predicted label distribution, denoted by μi*,k*,j, ∀j,
and (iii) the estimated uncertainty score, G2* 卜*. For Ni*,k* and μi*,k*,j (line 5-6), we use standard
incremental update approach (Cotton, 1975), which requires constant space and computational cost
per iteration. For G2* 卜*, We additionally maintain the number of label j being predicted among all
samples drawn from Di*,k*, denoted by H i*,k*,j (line 8). This enables a fast incremental update of
G2* k* (line 7).
6
Published as a conference paper at ICLR 2022
Uncertainty score-based new sample selection. To determine on which partition to select a new
sample, we use an upper-confidence-bound approach on the weighted uncertainty score (second case
in line 3), after ensuring two samples have been drawn from each partition (first case in line 3). Two
samples are needed for an initial estimation of each partition’s uncertainty score. Here, we use a
parameter a > 0 to balance between exploiting knowledge of uncertainty score (<^2 卜)and exploring
more partitions (《NI^).
We quantify the performance of MASA v.s. the optimal allocation algorithm A* as follows.
Theorem 2. If a > 2 log L + log K + 4 log N and N > 4LK, then we have
L(MASA, N) - L(A*, N) ≤ O(N-4 log1 N)
Roughly speaking, Theorem 2 shows that the loss gap between the API shift estimated by MASA
and the (unreachable) optimal allocation algorithm ceases in the rate of N-5/4. Note that the loss of
the optimal allocation decays in the rate of N-1. Thus, as N gets larger and larger, the relative gap
becomes more and more negligible.
4	Experiments
We apply MASA to estimate the shifts of several real world ML services for various tasks. Our
goal is three-fold: (i) understand if and why MASA assess the API shifts efficiently, (ii) examine
how much sample cost MASA can reduce compared to standard sampling, and (iii) exploit the
trade-offs between estimation accuracy and query cost achieved by MASA. We also study how the
hyperparameters affect MASA’s performance, left to Appendix C.
Tasks, ML APIs, and datasets. As shown in Section 2, we have observed 12 of 36 cases where
there is a >1% overall accuracy performance change of an ML API. Thus, we focus on evaluating
MASA’s performance on those 12 cases. Except for case study, all experiments were averaged over
1500 runs. In all tasks, we created partitions using difficulty levels induced by a cheap open source
model from GitHub. More details are in Appendix C.
Sentiment analysis: a case study on Amazon API. We start by a case study on Amazon API on
a sentiment analysis dataset, YELP to understand MASA’s performance. We adopt MASA with
sample budget 2000. The dataset is divided into 4 partitions D+,l , D+,h, D-,l , D-,h, depending on
whether the true label is positive (+) or negative (-), and quality score produced by the 2020 version
is lower (l) or higher (h) than the median.
We first note that the API shift gives an interesting explanation to Amazon API’s accuracy change.
In fact, as shown in Figure 4 (a-c), the accuracy increase is mostly because more texts (2.7%) with
negative altitudes are correctly classified. One possible explanation is that the API has been retrained
on a dataset with more negative texts. Next, we observe that MASA produces accurate estimation of
the API shift by comparing Figure 4 (c) and (d). This is primarily due to (i) that the data partitioning
separates more uncertain data from less uncertain ones, and (ii) that adaptive sampling learns the
uncertainty level effectively. Figure 4 (e-f) shows that while the partitions’ size is similar, their
uncertainty scores are different. For example, higher quality score implies a much smaller uncertainty
for positive texts (D+,h and D+,l in Figure 4(f)). As shown in Figure 4 (g), MASA indeed learns to
utilize such imbalanced uncertainty: its sampling allocation for each partition is quite close to the
optimal allocation (dark star point). Finally, it is worth noting that MASA outperforms standard
uniform sampling notably, as shown in Figure 4(h). This is because uniform sampling does not
exploit the uncertainty of each partition.
Budget savings achieved by MASA. In many applications, it suffices to obtain an estimated API
shift close to the true shift, e.g., within a 1% Frobenius norm error. Thus, a natural question arises: to
reach the same tolerable estimation error, how much sampling cost can MASA reduce compared to
standard sampling approaches?
To answer this question, we compare MASA with two natural approaches: (i) uniform sampling and
(ii) stratified sampling by drawing same number of samples for each true label. For each approach,
7
Published as a conference paper at ICLR 2022
0)≥⅛sod
2
48.
1.8
47.5
2.5
0)>4e6φu
38.8
8.6
41.4
-0.6
-2.7
positive
0.6
2.7
-0.5
-2.6
positive
3≥⅛sod 3>0e63u
0.5
2.6
11.2
positive
negative
(d) Estimated Shift
negative
(c) True API shift
negative
(a) CM 2020
positive negative
(b) CM 2021
Figure 4: Case study for Amazon API’s performance shift on dataset YELP. (a) and (b) give its
confusion matrix in spring 2020 and spring 2021, respectively. (c) is their differences, i.e., the API
shift. MASA’s estimated shift using 2000 samples is in (d). The dataset is divided into 4 partitions
based on (i) positive (+) or negative (-) true labels, and (ii) low (l) or high (h) quality score. (e) and
(f) give the size and uncertainty score of each partitions. (g) shows MASA’s sampling decision per
iteration, where the dark dot points represent the (unreachable) optimal sample allocation. (h) reveals
its performance.
3≥⅛sod 3>0e63u
Table 1: Required sample size to reach 1% Frobnius norm error. Here we compare MASA with
uniform (U) sampling and stratified (S) sampling. U and S required very similar sample sizes and
are reported in the same column. The sample size is obtained when a 1% Frobenius norm error is
achieved With Probability 95%.______________________________________________________________________
API;Dataset	Sample size		Save	API;Dataset	Sample size		Save
	MASA	U/S			MASA	U/S	
Amazon;YELP	4.5K	19.7K	77%	IBM;DIGrT	3.6K	17.0K	79%
Amazon;IMDB	10.3K	20.8K	51%	IBM;AMNIST	2.4K	18.5K	87%
Amazon;WAIMAI	7.8K	18.0K	57%	GoogleQIGIT	4.2K	17.0K	75%
Amazon;SHOP	4.8K	20.8K	77%	Google;AMNIST	1.1K	18.5K	94%
MS; FER+	2.6K	19.9K	87%	Google;CMD	1.6K	15.2K	89%
Google; EXPW	4.2K	17.9K	77%	MS;DIGrr	3.3K	17.0K	81%
we measure the number of samples needed to reach 1% Frobenius norm error with probability 95%,
via an uPPer bound on the estimated Frobenius error. The details are left to APPendix C. As shoWn in
Table 1, MASA usually requires more than 70% feWer samPles to reach such tolerable Frobenius
norm error than the uniform and stratified samPling, Primarily due to its shift estimation is more
accurate. Uniform and stratified samPling required the similar number of samPles because the uPPer
bounds on their estimated Frobenius error are similar.
Trade-offs between estimation error and query budget . Next We examine the trade-offs betWeen
API shift estimation error and samPle size achieved by MASA, shoWn in Figure 5. We first note
that, across all 12 observed API shifts, MASA consistently outPerforms standard uniform samPling
for any fixed samPle size. In fact, the achieved estimation error of MASA is usually an order of
magnitude smaller than that of uniform samPling. This verifies that MASA can Provide more accurate
8
Published as a conference paper at ICLR 2022
, . . . P ■
JE∙ uuou,tpejenbs
2000	3000
Sample size
→- MASA
τt-- Uniform
(a) Amazon YELP
Figure 5: API shift estimation performance and sample size trade-offs. We compare the expected
squared Frobenius norm error of MASA with K = 3 partitions versus standard uniform sampling.
For any sample size, MASA consistently leads to an estimation error much smaller than uniform
sampling across different API and dataset combinations.
assessments of API shifts in diverse applications. Second, some API shifts are easier to estimate
than others. For example, for Google API shift on AMNIST, using 1000 samples already gives an
expected squared Frobenius norm error lower than 10-4, while it usually requires 2000 samples for
other shifts. This is probably because the skew in its uncertainties among different partitions is more
severe than other shifts.
5	Conclusion
In this paper, we identify and formulate the problem of characterizing ML API shifts. Our systematic
empirical study has shown that API model updates are frequent, and that some updates can reduce
performance substantially. Quantifying such shifts is an important but understudied problem that
can greatly affect the reliability of applications using ML-as-a-service. To capture fine-grained
change assessment, we model the ML API shifts as confusion matrix differences. Adaptive sampling
methods are typically designed for estimating a single scalar (e.g., adaptive population mean) or
training a model (e.g., active learning), and thus not directly applicable for API shift estimation.
Uniform sampling is natural but requires a large number of samples. Thus, we propose an algorithmic
framework, MASA, to adaptively assess the API shifts using as few samples as possible. Our work
focuses on estimating changes in the confusion matrix because the confusion matrix is often what is
used by practitioners to assess API performance. We acknowledge that confusion matrices are most
applicable for classification tasks, and other measures need to be used for more complex APIs (e.g.
OCR, NLP). While this is a limitation, classification with a small to moderate number of classes of
interest is a common use case for ML APIs, and this is an important starting point since it has not
been studied before. We also release our dataset of 1,224,278 samples annotated by commercial APIs
in different dates as the first dataset and resource for studying ML API performance shifts.
9
Published as a conference paper at ICLR 2022
References
Amazon Comprehend API. https://aws.amazon.com/comprehend. [Accessed March-
2020 and March-2021].
Baidu API. https://ai.baidu.com/. [Accessed March-2020 and March-2021].
SHOP dataset. https://github.com/SophonPlus/ChineseNlpCorpus/tree/
master/datasets/online_shopping_10_cats, a.
WAIMAI dataset. https://github.com/SophonPlus/ChineseNlpCorpus/tree/
master/datasets/waimai_10k, b.
YELP dataset. https://www.kaggle.com/yelp-dataset/yelp-dataset, c.
DIGIT dataset. https://github.com/Jakobovski/free-spoken-digit-dataset,
d.
Face++ Emotion API. https://www.faceplusplus.com/emotion-recognition/.
[Accessed March-2020 and March-2021].
Google NLP API. https://cloud.google.com/natural- language. [Accessed March-
2020 and March-2021].
Google Vision API. https://cloud.google.com/vision, a. [Accessed March-2020 and
March-2021].
Google Speech API. https://cloud.google.com/speech-to-text, b. [Accessed
March-2020 and March-2021].
IBM Speech API. https://cloud.ibm.com/apidocs/speech-to-text. [Accessed
March-2020 and March-2021].
Machine Learning as a Service Market Report .	https:
//www.mordorintelligence.com/industry- reports/
global- machine- learning- as- a- service- mlaas- market.
Microsoft computer vision API. https://azure.microsoft.com/en-us/services/
cognitive-services/computer-vision, a. [Accessed March-2020 and March-2021].
Microsoft speech API. https://azure.microsoft.com/en-us/services/
cognitive-services/speech-to-text, b. [Accessed March-2020 and March-
2021].
K.B. Athreya and S.N. Lahiri. Measure Theory and Probability Theory. Springer, 2006.
Peter Auer, Nicold Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit
problem, 2002. URL https://doi.org/10.1023/A:1013689704352.
Kamyar Azizzadenesheli, et al. Regularized learning for domain adaptation under label shifts. In
ICLR 2019.
Soren Becker, et al. Interpreting and explaining deep neural networks for classification of audio
signals. CoRR, abs/1807.03418, 2018.
Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial
gender classification. In FAT 2018.
A. Carpentier et al. Finite time analysis of stratified sampling for monte carlo. In NIPS 2011.
Alexandra Carpentier, Remi Munos, and AndrgS Antos. Adaptive strategy for stratified monte carlo
sampling. J. Mach. Learn. Res.,16:2231-2271, 2015.
Surajit Chaudhuri, Gautam Das, and Vivek R. Narasayya. Optimized stratified sampling for approxi-
mate query processing. ACM Trans. Database Syst., 32(2):9, 2007.
10
Published as a conference paper at ICLR 2022
Lingjiao Chen, Matei Zaharia, and James Y. Zou. FrugalML: How to use ML prediction apis more
accurately and cheaply. In NeurIPS 2020.
Ira W. Cotton. Remark on stably updating mean and standard deviation of data. Commun. ACM, 18
(8):458, 1975.
Ian J. Goodfellow, et al. Challenges in representation learning: A report on three machine learning
contests. Neural Networks, 64:59-63, 2θ15.
Saihui Hou, et al. Learning a unified classifier incrementally via rebalancing. In CVPR 2019.
Daniel Kang, et al. Model assertions for monitoring and improving ML models. In MLSys 2020.
Pang Wei Koh, et al. WILDS: A benchmark of in-the-wild distribution shifts. CoRR, abs/2012.07421,
2020.
Florian Lepretre, Fabien Teytaud, and Julien Dehos. Multi-armed bandit for stratified sampling:
Application to numerical integration. In TAAI 2017.
Shan Li, Weihong Deng, and JunPing Du. Reliable crowdsourcing and deep locality-preserving
learning for expression recognition in the wild. In CVPR 2017.
Zachary C. Lipton, Yu-Xiang Wang, and Alexander J. Smola. Detecting and correcting for label shift
with black box predictors. In ICML 2018,.
Loren Lugosch, et al. Speech model pre-training for end-to-end spoken language understanding. In
Interspeech 2019.
Amalia Luque, et al. The impact of class imbalance in classification performance metrics based on
the binary confusion matrix. Pattern Recognit., 91:216-231, 2019.
Andrew L. Maas, et al. Learning word vectors for sentiment analysis. In HLT 2011.
Ali Mollahosseini, Behzad Hasani, and Mohammad H. Mahoor. Affectnet: A database for facial
expression, valence, and arousal computing in the wild. IEEE Trans. Affect. Comput., 10(1):18-31,
2019.
Haode Qi, et al. Benchmarking intent detection for task-oriented dialog systems. CoRR,
abs/2012.03929, 2020.
Joaquin Quinonero-Candela, etal. Covariate Shift by Kernel Mean Matching,pp. 131-160. 2009.
M. Ribeiro, et al. Beyond accuracy: Behavioral testing of NLP models with checklist. In ACL 2020.
Daniel Russo, et al. A tutorial on thompson sampling. Found. Trends Mach. Learn., 11(1):1-96,
2018.
Marco Saerens, Patrice Latinne, and Christine Decaestecker. Adjusting the outputs of a classifier to
new a priori probabilities: A simple procedure. Neural Comput., 14(1):21-41, 2002.
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-
likelihood function. In JSPI 2000.
Aleksandrs Slivkins. Introduction to multi-armed bandits. In Found. Trends Mach. Learn. 2019.
Masashi Sugiyama, et al. Direct importance estimation with model selection and its application to
covariate shift adaptation. In NIPS 2007.
Dimitris Tsipras, et al. From imagenet to image classification: Contextualizing progress on bench-
marks. In ICML 2020.
Zhongyuan Wang, et al. Masked face recognition dataset and application. In Arxiv 2020.
Pete Warden. Speech commands: A dataset for limited-vocabulary speech recognition. In Arxiv
2018.
Zhanpeng Zhang, et al. Learning social relation traits from face images. In ICCV 2015.
Eric Zhao, et al. Active learning under label shift. In AISTATS 2021.
11
Published as a conference paper at ICLR 2022
Acknowledgement
This work was supported in part by a Google PhD Fellowship, NSF CCF 1763191, NSF CAREER
1651570 and 1942926, NIH P30AG059307, NIH U01MH098953, grants from the Chan-Zuckerberg
Initiative, Sutherland, and affiliate members and other supporters of the Stanford DAWN project—Ant
Financial, Meta, Google, Infosys, NEC, and VMware—as well as Cisco and SAP. We also thank
anonymous reviewers for helpful discussion and feedback.
Appendix Outline The appendix is organized as follows. Section A provides additional technical
details. All proofs are presented in Section B. We give experimental setups, details of datasets and
ML APIs, and further empirical results in Section C.
A Technical Details
Computation and space cost of MASA. One attractive property of MASA is its low computation
and space cost. In fact, it can be easily verified from Algorithm 1 that, the computation cost is only
linear in the number of samples N. The occupied space is only constant. Therefore, MASA can be
easily applied for large number of samples.
Choice of parameter a. The parameter a is used to balance between exploiting and exploration in
Algorithm 1. Throughout this paper, we set a = 1 as the default value. While theoretically a should
depend on the partition size and sample number, in practice we found that a = 1 works well. An
in-depth analysis for this remains an interesting open problem.
Stopping rule under loss requirements. For MASA, we establish the upper bound on the loss by
(i) computing the upper bound on the estimated uncertainty score for each partition, and (ii) summing
up all those upper bounds weighted by the partition size to form the upper bound on the loss. For
Uniform sampling or stratified sampling, we directly use the upper bound on the Frobenius loss.
Here, We adopt the standard upper bound for Bernoulli variables. That is to say, for any estimator
using n samples, we use as its upper bound, where C is a parameter to control the confidence.
For both methods, We choose c to ensure a 1% error under 95% confidence level.
B Proofs
We present all missing proofs here. For ease of expositions, let us first introduce a few notations. We
let xn denote the nth sample drawn in Algorithm 1, and use In to indicate from which partition the
sample xn is drawn. For example, In = (i, k) indicates that xn is drawn from the partition Di,k.
Let z`,k,t ∈ [L] denote the ML API’s predicted label for the tth sample drawn from the partition
D',k. Abusing the notation a little bit, let N',k,n denote the value of N',k after the n - 1th iteration
and before the nth iteration in Algorithm 1. Similarly, let σ',k,n be the value of a`,k, μ',k,j,n be the
value of μ',k,j, and H',k,,j,n be the value of H',k,j, all after the n - 1th iteration and before the nth
iteration in Algorithm 1. In addition, let ∆',k，P /炉廿 and ∆min = min∆',k. Similarly,
let us denote σmin，mina`,k. By assumption thatp`,k > 0 and a`,k > 0, we must have ∆min > 0
and σ min > 0.
B.1	Useful Lemmas
Let us first give a few useful lemmas. The first gives a high probability bound on our estimated
uncertainty score.
Lemma 3. Let the event A be
A,
1≤k≤K,1≤'≤L
1≤t≤N
1 - t(t-υ
E E IsL4kj-σe,k
i=1 j=1,j6=i
∖
t
t
Then for any δ > 0, we have Pr[A] ≥ 1 - LKNδ.
12
Published as a conference paper at ICLR 2022
Proof. For any fixed t, let us first denote
1 tt
f (z',k,1, z',k,2, ∙∙∙ , z',k,t) , 1 - ~(j~P E 1z',kL',kj
i=1 j=1,j6=i
Its expectation is simply
E[f (z',k,1,z',k2 …，z',k,t)] =E[1 - t(t - 1)
tt
E E	ɪz`,k,i=z',kj]
i=1 j=1,j 6=i
tt
=1-1(⅛
E[∑ JIz'…',kj]
=1 — E[Iz八=z"]
L z',k,i=z',k,j J
where the second equation applies the linearity of expectation, and the third equation uses the fact
that all z`,k,i are identically independent. Note that
E[ W',k,i=Z',kj]
= PrIz',k,i = z',k,j]
L
=E Pr[z',k,i = r] Pr[z',k,j = r]
r=1
L2
= EPr[z`,k,i = r]
r=1
where the first equation uses the definition of indicator function, the second uses the fact that two
sample are independent and there are only L many possible labels, and the last equation uses the fact
that those samples’ distribution is identical. Applying this in the above equation, we get
E[f (z',k,1, z',k,2,…，z',k,t)] =1 - E[lz',k,i=z',kj]
L2
=1- EPr[z`,k,i = r]=σ2,k
r=1
That is to say, its expectation is simply the uncertainty score σ∖ k. On the other hand, We note that,
for any i, we have
f(z',k,1,…，z',k,i-1,z',k,i,z',k,i+1,…,z',k,t) - f(z',k,1,…，z',k,i-1, z',k,i,z',k,i+1,…,z',k,t)
1 XX
t(t - 1) X	ζ',k,i=ζ',kj
j=1,j6=i
；',k,i =z',kj ≤ t(t - 1) ^ (t	1) = t
—
where the inequality is due to the fact that the indicator function can only take values in {0, 1}.
Similarly, we have
f(z',k,1, …,z',k,i-1,z',k,i,z',k,i+1, …，z',k,t) - f(z',k,1,…，z',k,i-1, z',k,i,z',k,i+1,…,z',k,/
1 XX
t(t - 1) X	z',k,i =z',k,j
j=1,j6=i
—
%k,i=z',kj ≥ t(t - 1) ∙	(t	1)
1
—
t
By Mcdiarmid inequality, we have
Pr[∣f (z',k,1,Z',k,2,∙∙∙ ,z`,k,t) - E[f(ζ',k,ι,Z',k,2,…，z`,k,t)]l ≥ H
Set δ = 2e-2t2. This simply becomes, with probability at most δ,
If(Z',k,1,z',k,2, ∙ ∙ ∙ ,z',k,t) - σ22,k |
2e2	ɔ
≤ 2e Pt=I L- = 2e-2t'
=|f (z',k,1,z',k,2,…,z',k,t) - E[f (z',k,1 ,z',k,2,…，z',k,t)]∣ ≥ j jj
13
Published as a conference paper at ICLR 2022
Note that f is positive, we can take square root of both side, and obtain with probability at most δ,
| f(z`
,k,1, z`,k,2,
,z',k,t) - σ',k 1 ≥
Or alternatively, with probability at least 1 - δ,
1 ʌ/f(z',k,1,z',k,2, ∙ ∙ ∙ ,z',k,t) - σ',k 1 ≤
which holds for fixed t, `, k. Taking union bound, we know that with probability 1 - KLNδ,
1 Jf(Z',k,1,z',k,2, ∙ ∙ ∙ ,z',k,t) - σ',k 1 ≤
which holds for all t,', k. Plugging in the form of f completes the proof.	口
The next one is more technical: it gives a connection between stopping time and adaptive sampling.
We omit the proof and refer the interested readers to (Athreya & Lahiri, 2006).
Lemma 4 (Wald’s second inequality). Let {Ft}t=1,...,n be a filtration and {Xt}t=1,...,n be an Ft
adapted sequence ofi.i.d. random variables with finite expectation μ and variance Var. Assume that
Ft and σ({Xs : s ≥ t + 1}) are independent for any t ≤ n, and let T(≤ n) be a stopping time with
respect to Ft. Then
E (XXi - Tμ)2 = E[T] Var.
B.2	Proof of Lemma 1
Proof. Recall that the loss, defined as the expected squared Frobenius norm error, is
E hk∆C - ∆CkFi = X E (∆Ci,j - ∆Ci,j)2 = X E (XPi,k [μi,k,j -	])
i,j	i,j	k
=〉:pi,,kE ([μi,k,j - μi,k,j])
i,j,k
Here we basically apply the definition of each entry. Suppose Ni,k samples are allocated to estimate
μi,k,j . Then we have
21
E (“k,j - μi,k,jD = n— Pr[y(X) = j∣x ∈ Di,k](1 - Pr[y(X) = j∣χ ∈ Di,kD
Ni,k
since μi,k,j is effectively a Bernoulli variable. Then the loss becomes
E [k∆C - ∆CkF] = X或kE (M,k,j -%,k,j])2
i,j,k
=Xp2,kN^pr[y(X) = j∣x ∈ Di,k](1 - Pr[y(X) = j∣x ∈ Di,kD
Ni,k
i,j,k
=XP2,k-V1- Xpr[y(x) = j∣x ∈ Di,k](1 - Pr[y(X)= j∣x ∈ Di,k])
Ni,k
i,k	, j
where the last equation is simply by rearranging the summation. Note that
X Pr[y(X) = j∣X ∈ Di,k] = 1
j
The last summation is simply
2
EPr[y(X) = j∣X ∈ Di,k](1 - Pr[y(X) =j∣X ∈ Di,k]) = 1 - EPr[y(X) = j∣X ∈ Di,k])
jj
=σi2 k
i,k
14
Published as a conference paper at ICLR 2022
Thus, the loss becomes
E ［心。-δc IlF i = X 或k σ2,k N-
i,k	i,k
By Cauchy Schwarz inequality, we have
(X R )(x Nj ≥(x “，k σJ
where the equality holds if and only if
p,k σ2,k = P20,k0σ20,k0
N 2,k	N 20,k0
for any i, i0 , k, k0 . That is to say, there exists some constant c, such that
pi,kσi,k	pi0,k0 σi0,k0	1
N i,k	N i0 ,k0	c
And thus, Ni,k = pi,kσi,kc. Summing over i, k gives
N = Ni,k =	pi,kσi,kc
i,k	i,k
Thus,
N
C —
i,kpi,kσi,k
and
Ni,k — Pi,kσi,kc — Pi,kσi,k •
1
i,k pi,k σ i,k
Pi,kθi,k
i,kpi,kσi,k
which completes the proof.
□
B.3 Proof of Theorem 2
Proof. To prove this theorem, we need a few more lemmas.
Lemma 5. Algorithm 1’s computational cost is O(LKN) and space cost is O(L2K). Furthermore,
for any n > 2LK, after the n - 1th iteration and before the nth iteration, we have
n-1
N',k — N',k,n — E IIi = (',k)
i=1
μ',k,j
1	n-1
—μ',k,j,n — N^ £ 1Ii=(',k) Iy(Xi)=j
`,k,n i=1
1	n-1 n-1
σ',k — σ',k,n — 1 - N	(N	-1)	ɪIi = Ij = (',k) ɪ⅛(xi)=y(χj)
`,k,n	`,k,n	i=1 j=1,j6=i
n-1
H',k,j — H',k,j,n — E lIi = (',k) Iy(Xi)=j
i=1
Proof. The computational and space cost can be easily verified: as shown in Algorithm 1, the
variables σ, μ, H, N take space LK, L2K, L2K, LK. Therefore, the space is bounded by O(L2K).
For the first 2LK iterations (line 3-8) in Algorithm 1, the computation cost is clearly O(LK).
15
Published as a conference paper at ICLR 2022
For the rest iterations (line 10- 16), the most expensive cost is computing In, which requires LK
computations per iteration. Therefore, the total computational cost is O(LKN).
Next we show that the above four equations hold for every n > 2LK. We prove this by induction.
1)	n = 2LK + 1: One can easily verify this by plugging the initial values established in line 3-8 in
Algorithm 1.
2)	Suppose the four equations hold for the case when n = m. Now consider n = m + 1. Now let us
consider two cases.
• Any ', k such that Im+1 = (2, k): There is nothing update,
m— 1	m—1	m—1
N `,k,m+1 = N',k,m = E IIi = (',k)= Σ IIi=(',k) + 0 = Σ IIi = (2,k) + IIm = (',k)
i=1	i=1	i=1
m
=E IIi=(',k)
i=1
Similarly, one can show that
μ2,k,j,m+1
=μ2,k,j,m
1
N ',k,m
m
ElIi=(',k) Iy(Xi)=j
i=1
σ',k,m⅛1 =6',k,m
1	mm
1 - N',k,m(N',k,m - 1) X j=X=i 1Ii=Ij = (',k) Iy(Xi)=y(xj)
m
H `,k,j,m+1 = H',k,j,m =	^^Ii = (',k) ly(≈i)=j
i=1
• For some '*,k* such that Im+1 = ('*,k*).
Let us first consider N`* ,k*. We increment N`* ,k* by one, and thus
m—1	m—1
N '*,k* ,m+1 = N '*,k*,m + 1 = £ 1Ii = ('*,*k) + 1 = £ 1Ii = ('*,k
i=1	i=1
*
)+ IIm = ('*,k* )
m
ElIi=('*,k*)
i=1
Next we consider "`* ,k* ,j. Using a similar argument as above, we have
μ '*,k*,j,m+1 =“'*,k*,j,m +
ɪg(^m)=j - μ'*,k*,j,m
N'*,k*,m+1
N '*,k*,m+1 - 1 ʌ	l
N'*,k* ,m+1	"'*'k* ",'m +
^⅛(Xm)=j
N '*,k*,m+1
N '*,k*,m A	l
^λγ	μ'* ,k * ,j,m +
N '*,k*,m+1
^⅛(Xm)=j
N '*,k*,m+1
N '*,k*,m______1
N '*,k*,m+1 N '*,k*,m
m—1
E !Ii=('*,k*)Iy(Xi)=j +
i=1
^⅛(Xm)=j
N '*,k*,m+1
1m
N '*,k*,m+1 X 1Ii='*,k*)Iy(Xi)=j
16
Published as a conference paper at ICLR 2022
where the first equation is due to the update rule of "g*,k*,j, the second equation is simply
grouping by μt*,k* ,j,m, the third equation is due to the fact that N'*,k*,m+ι = N '*,k*,m + 1,
the forth equation is due to the induction assumption, and the forth equation is simply
algebraic rewriting.
Now let us consider σj* k* m+1. We can write
σ'* ,k* ,m+1
^2	+_______2	_ H'*,k*,y(xm),m
=σ'*,k*,m + "λt	(1 - -ʌf	i"
N '*,k* ,m+1	N '*,k*,m+1 - 1
N'* ,k*,m+1 - 2σ2	+	2 (ι	H'*,k* ,y(xm),m
N '*,k*,m+1	,	，	N `* ,k*,m+1	N'*,k*,m+1 - 1
ArQ	1	m—1 m—1
N 2*,k*,m+1 - 2 门 ________1_________ LLll	书	ʌ
N,* k*	N ,* k*	(N ,* k*	- 1)	Ii=	∕i=∕j = ('*,k*) y(χi)=y(χj))
JV2*,k*,m+1	∕v'*,k*,m(∕v'*,k*,m	ɪ) 2=1 j=1 j=2
2	(1 - H'* ,k* 舟(Xm),m)
N'*,k*,m+1	N `* ,k*,m+1 - 1
m—1 m—1
N 2*,k*,m+1 - 2 门 _________________1_______________ LLll	书	ʌ
N '*,k*,m+1 (I - (N '*,k*,m+1- 2)(N '*,k*,m+1- 1)	j^j=^^ U Iai) = ^(Xj))
2
+ 1<F^--------
N '*,k*,m+1
(1 -
H '*,k*,^(Xm),m
N'* ,k*,m+1 - 1
1	m—1 m—1
=1 - N '*,k*,m+1(N '*,k*,m+1- 1) X jjχ=*=i*,k*%(Xi)=g(Xj)
2H'*
,k* ,^(xm),m____
N'*,k*,m+1(N'*,k*,m+1 - 1)
where the first equation is by the update rule in Algorithm 1, the second equation is simply
rearranging the terms, the third equation uses the induction assumption, the forth one uses
the update rule on N'*,k* and thus N'*,k*,m = N'*,k*,m+1 — 1, and the fifth equation is
also rearranging the terms.
On the other hand, by induction assumption, we have
H '*,k*,^(xm),m
m—1
^^∕i = ('*,k*) ly(xi) = y(xm)
i=1
And thus
m—1 m—1
Σ Σ
ɪʃi =Ij = ('*,k* ) ly(xi) = y(xj) + 2H'* ,k* ,^(xw,),m
i=1 j=1,j=i
mm
E E IIi=('*,k*)l^(Xi)=^(Xm)
i=1 j=1,j=i
Hence, the above equation becomes
σ'*,k* ,m+1 = 1 - Ng
k* ,m+ 1(M* ,k* ,m+1 - 1)
mm
£	£ IIi=Ij-=('*,k*) l^(Xi)=^(Xj-)
i=1 j=1,j=i
1
17
Published as a conference paper at ICLR 2022
Finally, let us consider H`*,k*,j. If j = y(χm), it is clear that
m—1
H`*,k* ,j,m+1 =H `* ,k* ,j,m = E IIi = ('*,k*) 1y(Xi)=j + 0
i=1
m— 1
=〉:l∕i = ('*,k*) l^(g)=j + l∕i = ('*,k*) Iy(Xm)=j
i=1
m
=E IIi = ('*,k*) 1y(xi)=j
i=1
where the first is due to that there is no update for this j, the third equation is due to the fact
that y(χm) = j, and all the other equations are algebraic rewriting.
If j = ^(χm), it is clear that
m—1
H2* ,k* ,j,m+1 =H `*,k*,j,m + 1 = ɪ2 !Ii = ('*,k*) Iy(Xi)=j + 1
i=1
m—1
=〉:l∕i=('*,k*)Iy(Xi)=j + l∕i=('*,k*)Iy(Xm)=j
i=1
m
=E IIi=('*,k*)Iy(Xi)=j
i=1
where the first is due to that there is no update for this j , the third equation is due to the fact
that y(xm) = j, and all the other equations are algebraic rewriting.
That is to say, We have shown that,
m
Ne,k,m+1 = E 1Ii=(e,k)
i=1
1m
Jkj,m+1 = N	ɪ/i:(`,k) ɪ⅛(g)=j
n e,k,m+1 i=1
1
σ',k,m+1
1 —-------------------
N ',k,m+1(N ',k,m+1 - 1)
mm
Σ Σ ɪIi=Ij=(2,k) ɪ⅛(xi)=y(xj)
i=1 j = 1,j=i
m
He,k,j,m+1 = ElIi=(e,k) Iy(Xi)=j
i=1
always hold. By induction, we can say that for any n > 2LK, the original equations hold, which
completes the proof.
Lemma 6. Suppose that the event A holds. Set δ = 2e-a. Thenfor each ', k, we have
□
1
1
而≤ Nk
1 + 4LKN T
log2∕δN - 4
∆min
for any 1 ≤ ' ≤ L, 1 ≤ k ≤ K.
Proof. To show this, let us first establish the following useful lemma.
Lemma 7. Suppose that the event A holds. If Algorithm 1 draws at least one sample from D'0,ko
after the first 2LK iterations, we must have,for every ', k,
N',k ≥ (N'0,k0- 1"'‘k公"k0 + 2/
----------∖ —1
log2∕δ ʌ
2(N'o,ko - '1)
18
Published as a conference paper at ICLR 2022
Proof. Since the event A holds, we have
∖
tt
1 -  ------ X	X	Izgk ∙=s∙
t(t _ 1) / /	/ / z',k,i=z',kj
i=1 j=1,j 6=i
—
for every `, k, t. Since this holds for every fixed t, it should also holds for any random variable t.
Specifically, we must have
1
1 1---------------
∖	N ',k,n(N'
N `,k,n N `,k,n
I) E E	1 …,kj-σ',k
,k,n	i=1 j=1,j6=i
≤ 4八οg2∕δ
-V 2N',k,n
Note that, by definition,
σ ',k,n = t
1	N `,k,n N `,k,n
1 - N',k,n(N…1 X j=X=i1z"k"kj
We can then rewrite the above inequality as
4 /lοg2∕δ
iσ',k,n-σ',k ι≤ Vkn
That is to say,
4 八οg2∕δ	八οg2∕δ
σ',k- V 2N∑n ≤ K ≤ σ',k+ V 2N'∑n
Adding 4J W≡ to both sides,this becomes
不	4 Ibg 2M Vb , 9 4 Ibg 2∕δ
σ',k ≤ σ',kn+V 2N∑n ≤ σ',k+ V 2N'∑n
Multiplying both sides by 滞：,We have
P',k	≤ P',k
N',k,n 气温 ≤ N',k,n
4 /lοg2∕δ
V 2N',k,n
V P',k (C J 2 41MM
≤ W [σ 2^配总
(B.1)
Which holds for any `, k, n. Note that N > 2LK, there must exist some `0 , k0, such that Algorithm
1 draws a sample from the data partition。检也 after the first 2LK iterations. Suppose the last
time a sample is drawn from D'ο,k° is no > 2LK. That is to say, N'。用个 =N- 1, ∀n =
no + 1,…，N. Since Algorithm 1 chooses 'o, k° at iteration no, by line 11in Algorithm 1, we have
P P	p`,k L l 41 log2∕δ
'0,k0 = argmaxk0 (σ',k,n0 + V	)
By definition of arg max, we have
P'0,k0	/ʌ	, 4 / log"δ、、P',k	,ʌ	, 4 / log 2外、
Z	(σ'θ,k0,n0 + ∖ DZ	) - Z	(σ',k,n0	+ ∖l ɔ ʌr	)
N 'o,ko,no	2 2N 'o,ko,ηo	N ',k,n	V 2N ',k,n
Setting n = no in the first half of inequality B.1, we have
p`,k	Λ + 4 log2∕δ ʌ - p`k
N',k,no (σ',k,n0	2∣ 2N',k,no ) - N',k,no ^kn
Combining the above two inequalities gives
P'o,ko	rʌ	, 4/ log”δ、、p`,k	„
Z	(σ'o,k0,no + ∖ ɔ ʌr	) — Z	σ',k
N 'o,ko,no	2 2N 'o,ko,no	N ',k,no
19
Published as a conference paper at ICLR 2022
Noting that by definition, N',k,n ≤ N',k,N = N',k, We can lower bound 1/N',k,n by 1/N',k, and
the above inequality becomes
P'o,ko	rʌ	, 41 log2/J ʌ	p`,k
N`o,ko,no	'0,k0,n0	V 2N'0,k0,n0	- N',k ',k
Now setting n = n0, ` = `0, k = k0 in the second half of inequality B.1, we have
P'0,k0	Λ	+ 4/ log2/S ʌ ≤ P'0,k0	(	+2 4/ log2/S
N '0,k0,n I '0,k0,n + V2N '0,k0 ,no) ≤ N '0,k0,n0 I '0,k0 +	∖∕2N '0,k0 ,no
Combining the above two inequalities, we have
p`o,ko
N'o,ko,no
J l 04/ log"δ、、P',k _
(σ'o,ko + 2∖	------) ≥ v-σ',k
2 2N 'o,ko,no	N ',k
Observe that no is the last time a sample is drawn from partition D'o,ko, we have N`o,ko,no =
N`o,ko,n -1, ∀n = no + 1,…，N. Specifically, N`o,ko,no = N'°,k°,N -1 = N'°,k0-1. Replacing
N'o,ko,no by N`o,ko — 1 in the above inequality, we get
+ 2 4/ log2/。)
+ V 2(N'0,ko-'1))
p`,k
≥ 而σ',k
which holds for every `, k. Rearranging the terms completes the proof.
□
Now we are ready to prove the bound on N',k - NZk.
Let us first consider the lower bound. By definition, we have
LK
XX N ',k = N
'=1k=1
Subtracting 2 from each element, we have
X X(N',k- 2) = N - 2LK = N-NLKN
'=1 k = 1
Note that by definition, N = PL=I PK=I N' 卜.We can now replace the second N in the above
equality, and obtain
LK
XX(N',k- 2)
N - 2LK 2 _ N - 2LK 昌？斯* V £ NZk(N - 2LK)
~N~N = ~N- MNN',k = M 2 N
Now let us consider two cases.
(i) Assume N',k - 2 ≥ N',k(N-2LK). That is to say, N',k ≥ N',k(N-2LK) + 2. Then we have
11
--≤ ——；--：-
N ≤ N淙(N-2LK) + 2
N	+ 2
20
Published as a conference paper at ICLR 2022
subtracting N^ from both sides, We get
11 TT：	 -	TT：	 N ',k	N ',k	-	1	L -N法(N-2LK) + 2	N',k =N J N^K - 2 N ',k∙(N 嬴(N-2LK) +2) N *	N 'k(N-2LK) V N ',k -	N -T∑	/N'k(N-2LK) N*,k ∙ (4^——) 2LKN ',k =	N	 =T7*	zN 'k (N—2LK) N*,k ∙ (——) _	2LK =N*,k (N - 2LK)
where the last inequality is simply by removing the constant 2. Now by assumption, N > 4LK, we
have N - 2LK < ɪ N. The above inequality can be further simplified as
1	1	2LK	4LK
—-----——≤ —- ------------- ≤ —---
N',k	N鼠 - NZk(N - 2LK) - N鼠N
By definition, we have NZk = N∆',k - N∆mi∏. Therefore, we have
1	1 /	2LK	/ 4LK
N^ -而-NZ,k(N - 2LK) - NkN
That is to say,
1	1 Γ	4LK
标-引［+ ɪ
And thus, apparently,
1 1
-----------
Nol -N *
N ',k N ',k
1 + 4LKN—
log2∕δ
ɪmr
N - 4
....	一 一	N3 (N — 2LK) 一.	.... 一	一
(ii) Assume N',k 一 2 < ,k N------------------------------------------------------------. Then there must exists some '0, k0 such that N`θ,kɑ - 2 >
N ； ^ (N — 2LK)	ci.	a 1	1	1 „	C	C
—0, 0 N--------- > 0. That is to say, Algorithm 1 draws at least one sample from D'0,k0 after the first
2LK iterations. By Lemma 7, we must have
N 2,k ≥ (N '0,k0 - I) σ',k :"k (σ'0,k0 + 2 S 2( N og"：	!
p'o,ko ∖	2 2(N'0,k0 - 1) J
N'0,k0 - 2 > N'0,k0(N-2LK) implies
N '0,k0- 1 >N'0,k0-2 >N '0,k0 (N - 2LK)
Therefore, we can use this lower bound on N缸丽一
1 in the above inequality and obtain
21
Published as a conference paper at ICLR 2022
N 〉N 2,k0 (N — 2LK ∖τ	P',k	C Qu
N'，k ≥-----------N---------0'，kP^ [σ'0,k0 + 2t
-1
log2∕δ
2 N No®(N-2LK)
2	N
N；0,ko(N - 2LK) σ',kP',k	( +	2 U log 2∕δ ʌ
N	σ'o,koP" [	σ'o,ko t 2NBo，%。(N-LK )
NZk(N - 2LK)
N
1 +上
σ `o ,ko
log 2∕δ
NB t, (N—2LK
° 'o ,ko'	/
2	N
—1
4
t
where the first equality is by dividing 仃9。at both denominator and numerator, and the second
equality uses the fact that N' 卜 is proportional to p`,kg`h Taking inverse of the above inequality
gives
ɪ ≤_____N_____ ι + ^u _鹿丝_
N',k ≤ NZk(N - 2LK) I + O'。,®。t 2NBo,%。(N-2LK)
Now let us simplify this inequality. Let us first expand all terms and obtain
ɪ ≤ ____________N______	1 + ^4	l°g2∕δ—
N',k ≤ NZk(N - 2LK) I + b'。,®。t 2NB0，%。(N-2LK)
1	2LK
+
NZk+ NZk(N - 2LK)
1	2LK
+
NZk+ NZk(N - 2LK)
N	2 u	log 2∕δ
+4
+ NZk(N - 2LK) o'。,k。t 2NB。,%。(N-2LK)
2
σ '。 ,k。
+
41(	N Y log2∕δ
VlN - 2LKJ 2N熬N；。,®。
For the second term, by assumption, N > 4LK and thus N - 2LK > 1∕2N, we have
2LK
N - 2LK
4LK
≤----
≤ N
Thus the above equation becomes
4LK
——≤ —----+ ---
N',k ≤ Ni,k + NZkN
2
+--------
σ'o ,k。
5	log 2∕δ
N - 2LKJ 2N部N；。,®。
1
1
4
N
For the third term, N > 4LK also implies
N
2LK
N - 2LK = 1 + N - 2LK < 1 +
2LK	=2
4LK - 2LK
Thus the above inequality can be further simplified as
1
1
4
----≤--------+
N',k ≤ Nlk +
4LK	ι
NIkN +
log 2∕δ
σ'o,ko
NrkNlZ
',k	'。 ,k。
4LK
+ ~N~
l 4	4/log2∕δ
小。V N io,ko
1
≤ Nik
4
22
Published as a conference paper at ICLR 2022
Now by definition, σj
above inequality
∙'o,ko ≥ σmin, and N:0,k0 = N∆'c,,kc, ≥ N∆mi∏, we can further simplify the
1
1
1
≤而
1
≤ Nk
——≤ ——
N ',k - N Zk
l°g“δ N - 4
∆min
That is to say,
1
1
——≤ ——
N" - N 嬴
1 + 4LKN—
That is to say, no matter N',k - 2 <
completes the proof.
N Zk(N-2LK)
N
or not, this inequality always holds, which
□
Now we are ready to prove Theorem 2. Let us first note that the loss can be written as
LKL
LN = XXX 成kE[μ',k,j- u`,k,j]2
'=1k=1j=1
LKL	] N ',k
=XXX脸E[%,j -击 X IMk,t=j)2 1a]
'=1 k=1 j=1	',k t=1
LKL	1 N 2,k
+ XXX或kE[(〃e,k,j - N~ X 1z',k,t=j产IAC]
'=1 k=1 j=1	',k t=1
Let us first consider the first term.
LKL
XXX p2,k E[(μ',k,j- μ',k,j )2 IA]
(B.2)
'=1k=1j=1
LKL
XXX %E[("g
'=1k=1j=1
1	N',k
-即 X %k"=j y1 A
(B.3)
LKL
XXX应理
'=1k=1j=1
1	/	N',k
N-	N',kμ',k,j - E 1%,k,t=j
NKk ∖	t=1
2
where We plug in the definition of μ. By Lemma 6, We have the upper bound on 1/N',k
1
1
≤Σ- ≤ TΞ~Ξ-
NL N嬴
Therefore, we can use this inequality to obtain
E
2
≤
4LK
+ w N
1	/	N ',k
N- I N"μ',k,j- E ɪz`,k,t=j
/"k ∖	t=1
N- 4 ]2E
1 + 4LKN-1
l°g“δ N - 4
∆min
N ',k
N ',kμ',k,j- E ιzg,k,t=j
t=1
2
(B.4)
23
Published as a conference paper at ICLR 2022
It is not hard to see that N',k is a stopping time. In fact, for any ', k, and any time n, a new sample is
drawn purely based on estimated uncertainty score G and observed sample number N',k,n-ι UP to
the current iteration, which is part of the history. As N',k < N is bounded, N',k is a stopping time.
Hence, we can apply Lemma 4, and obtain
N	N',k	∖
EUNe,kμe,k,j - X IZg=J Ia ≤ E
≤E[N',k]Pr[z',k,ι = j](1 - Pr[z',k,ι = j])
N ',k
N',kμ',k,j -	ɪz`,k,t='
t=1
where the first inequality uses the fact that square term must be non-negative, and the second inequality
uses the fact that, for Bernoulli distribution with mean a, its variance is a(1 - a). Applying this in
inequality B.4, we have
≤
≤
4LK .I 9
--N-2 +
∆min
乎N
∆min
2
IA
N ',k
N',kμ',k,j - E 1zg,k,t=j
t=1
N - 4]2E
Now applying this in equality B.3, we get
LKL
XXX P2,kE[(“',k,j - μ',k,j)2 IA]
'=1k=1j=1
LKL
XXX p2,kE
'=1k=1j=1
N ',k
N',kμ',k,j -	ɪz`,k,t='
t=1
2
IA
N-4]2E[N',k] Pr[z',k,ι = j](1 - Pr[z',k,ι = j])
I (	N /	Y
NJɪ lNe,kμ',k,j - E 1Z',k,t=j ) IA
≤ XXX成k[N- + ∆-n-2 + ^- 4J ；5 / N-N]2E[N',k] Pr[Z',k,1 = j](1 - Pr[Z',k,1 = j])
'=1 k=1 j=1	N ',k	Amin	OminV Amin
=X X%% [N- + ∆KN-2 + ɪ S∆≡N-5]2E[N',k]
'=1 k=1	N ',k	Amin	Omin V Amin
(B.5)
where the last equation uses the fact that o`,k = 1 - P%ι Pr2[zg,k,ι = j] = P%ι Pr[zg,k,ι
j](1 - Pr[z',k,ι = j]). Applying the inequality 1/(1 + x) ≤ 1 - X
1	1
N `k ≤ N * ɪ,
',k	',k
1 + 4LKN-1
log2∕δ
^∆∑iT
N 4
24
Published as a conference paper at ICLR 2022
Note that
∖k σ2,k
,k )2
=N-2(E PfM 0”，?
e,k0
1 + 4LKN
1 + 4LKN-1
log2∕δN - 4
△min
1 + 4LKN-1
log2⅛-4
△min
log丝N-4
△min
]2E[N',k]
E[M,k ]
E[M,k ]
where the last equation is by definition of N`k. Now applying this in inequality B.5, We have
LKL
XXX p2k E[(μ',k,j - μf,k,j)21A]
'=1 k=1 j=1
LK
≤ xx% &	+Δkn
'=1 k=1	mιn
LK
XX N-2(XPsO …)
=N-2(£ pe,kθe,k"2
f,k0
=N-2(E P"。"0『
f,k0
=N T(E p`,kθ`,k )2
f,k
1 + 4LKN—
1 + 4LKN—
1 + 4LKN
1+4LKN
2
匪竺N-4
△min
lo≡2^N - 4
△min
丝丝N - 4
△min
N - 5]2E[M,k ]
log"δN - 4
△min
E[N ',k]
2 L K	(B.6)
XX
E[N',k ]
2
2
2
2
N
where the second equation uses the fact that only E [N`k ] depends on ', k, the third equation uses the
fact that PL=I PK=I M,k = N and thus PL=I PK=I E[N',k] = N. Note that δ = L-1K-1N-5,
we have
2
1 + 4LKN-1
log“δN - 4
△min
NT(E P',k σg,k )2
f,k
=N-1(Xp`,kσf,k)2 [1 + O(N-1 log1
f,k
=N-1(Xp`,kσf,k)2 + O(N-5 log1 N)
f,k
Applying this back to inequality B.6, we have
LKL
XXX优kE[3e,k,j - MkjY 1A] ≤ N-1(Xp`,kq)2 + O(N-4 log1 N)	(B.7)
'=1 k=1j = 1	', k
Now consider the second term in equation B.2. As μ and μ are within {0,1}, we have
1 N',k
(μ',k,j - NT? X 1z',k,t=j)2 ∈ [0，1]
2,k t=1
25
Published as a conference paper at ICLR 2022
Therefore,
LKL	N ',k	LKL
XXXPlkE[(μ',kj - £ X ɪz`,k,t=') Iac] ≤ XXX% Pr[AC]
'=1 k = 1 j=1	',k t=1	'=1 k=1 j = 1
By Lemma 3, the probability of A is at least 1 - KLNδ. Hence, the probability of AC is at most
KLNδ. Hence,
LKL	N N ',k
XXX %E[("',kj- N1- X iz`. )21 AC ]
'=1 k=1 j=1	',k t=1
LKL
≤ XXX* Pr[AC]
'=1k=1j=1
LKL
≤ XXX PNkLKN
'=1k=1j=1
L
≤ X LKNδ = L2KNδ
j=1
where the last inequality uses the fact that PL=ι PK=IP k ≤ 1 since PL=ι PK=Ip`,k = 1 and
p`,k ≥ 0. Since δ = L-2KTN-9, we have
LKL	N ',k
XXX %E[(μ',kj- N17 X lz',k,t=j )21 AC ]
'=1 k=1 j=1	',k t=1
≤L2KNδ ≤ N - 4
Applying this as well as inequality B.7 to the equation B.2, we have
LKL
LN = XXX Plk E[μ',kj -μ',k,j]
'=1k=1j=1
LKL	N ',k
=XXX Plk E[(μ',kj - n1- X %k,t=j )2 1a]
'=1 k=1 j=1	',k t=1
LKL	ι N ',k
+ XXX p2,k E[(μ',k,j - N^7 X 1Z',k,t=j 尸 IAC ]
'=1 k=1 j = 1	',k t=1
≤N T(X p`,ka`,k )2 + O(N - 4 log 4 N) + N - 5
`,k
Note that the loss of the optimal allocation is simply LN = NT(p` 卜p`,kσ&k)2. The above
inequality is simply
LN -LN ≤ O(N-5 log4 N)
which completes the proof.	□
C	Experimental Details
Experimental Setups. All experiments were run on a machine with 2 E5-2690 v4 CPUs, 160 GB
RAM and 500 GB disk with Ubuntu 18.04 LTS as the OS. Our code is implemented and tested in
python 3.7. All experimental results were averaged over 1500 runs, except the case study. Overall
the experiments took about two month, including debugging and evaluation on all datasets. Running
MASA once to draw a few thousand samples typically only takes a few seconds. Our implementation
is purely in Python for demonstration purposes, and more code optimization (e.g., using cython or
multi-thread) can generate a much faster implementation.
26
Published as a conference paper at ICLR 2022
Table 2: Dataset statistics.
Dataset	Size	# Classes	Dataset	Size	# Classes	Tasks
FER+	6358	7	RAFDB (Li et al.)	15339	7	FER
EXPW	31510	7	AFFECTNET	87401	7	
YELP	20000	2	SHOP	62774	2	SA
IMDB	25000	2	WAIMAI	11987	2	
DIGIT	2000	10	AUDIOMNIST	30000	10	STT
FLUENT	30043	31	-COMMAND	64727	31	
Table 3: ML services used for each task. Price unit: USD/10,000 queries. We consider three tasks,
sentiment analysis (SA), facial emotion recognition (FER), and spoken command recognition (SCR)
Tasks	ML service	Price	ML service	Price	ML service	Price
SA	Google NLP (GoN)	2.5	AMZN ComP (Ama)	0.75	Baidu NLP (Bai)	3.5
FER	Google Vision (Goo, a)	15	MS Face (Mic, a)	-i0-	Face++ (Fac)	-5-
SCR	Google Speech (Goo, b)	60	MS Speech (Mic, b)	^^1	IBM Speech (IBM)	25
ML APIs and Dataset Statistics. We focus on three common classification tasks, namely, sentiment
analysis, facial emotion recognition, and spoken command recognition. For each of the tasks, we
evaluated three APIs’ performance in spring 2020 and spring 2021, respectively, for four datasets.
The details of datasets and ML APIs are summarized in Table 2 and Table 3 respectively. Now we
give more context of the datasets.
For sentiment analysis, we use four datasets, YELP, IMDB, SHOP, and WAIMAI. YELP and IMDB
are both English text datasets. YELP (Dat, c) is generated by drawn twenty thousand samples from
the large YELP review challenge dataset. Each original review is labeled by rating in {1,2,3,4,5}.
We generate the binary label by transforming rating 1 and 2 into negative, and rating 4 and 5 into
positive. Ten thousand positive reviews and ten thousand negative reviews are then randomly drawn,
respectively. IMDB (Maas et al.) is a polarized sentiment analysis dataset with provided training
and testing partitions. We use its testing partition which has twenty-five thousand text paragraphs.
SHOP (Dat, a) and WAIMAI (Dat, b) are two Chinese text datasets. SHOP contains polarized
labels for reviews for various purchases including fruits, hotels, computers. WAIMAI is a dataset
for polarized delivery reviews. Both SHOP and WAIMAI are publicly available without licence
requirements. There is a dataset user agreement for YELP dataset, which disallows commercial usage
of the datasets but encourages academic study. Same thing applies to the IMDB dataset.
For facial emotion recognition, we use four datasets: FER+, RAFDB, EXPW, and AFNET. All
the datasets are annotated by the standard seven basic emotions, i.e., {anger, disgust, fear, happy,
sad, surprise, neutral}. The images in FER+ (Goodfellow et al., 2015) are from the ICML 2013
Workshop on Challenges in Representation. We use the provided testing portion in FER+. RAFDB
(Li et al.) and AFFECTNET (Mollahosseini et al., 2019) were annotated with both basic emotions
and fine-grained labels. In this paper, we only use basic emotions since commercial APIs cannot
work for compound emotions. EXPW (Zhang et al.) contains raw images and bound boxes pointing
out the face locations. Here we use the true bounding box associated with the dataset to create aligned
faces first, and only pick the images that are faces with confidence larger than 0.6. We cotnacted the
creators of RAFDB and AFNET to obtain the data access for academic purposes. FER+ and EXPW
are both publicly available online without consent or licence requirements.
For spoken command recognition, we use DIGIT, AMNIST, CMD, and FLUENT. DIGIT (Dat, d)
and AMNIST (Becker et al., 2018) are spoken digit datasets, where the label is is a spoken digit (i.e.,
0-9). The sampling rate is 8 kHz for DIGIT and 48 kHz for AMNIST. Each sample in CMD (Warden,
2018) is a spoken command such as “go”, “left”, “right”, “up”, and “down”, with a sampling rate of
16 kHz. In total, there are 30 commands and a few white noise utterances. FLUENT (Lugosch et al.)
is another recently developed dataset for speech command. The commands in FLUENT are typically
27
Published as a conference paper at ICLR 2022
OVeraIl accuracy:79.1
φ>lωod 8>1e6φu
-φ-qA8 nxL
44.1	5.9
15.0	35.0
positive negative
Predicted label
(a) Amazon IMDB 2020
OVerail accuraCy:78.0
9>lω0d 9>1E69U
-ωqro8nπ■
39.0	11.0
11.0	39.0
positive negative
Predicted label
φl>≡lsodΦl>qe6φlu
-θqe-(Dnπ-
OVeraH change:4」
-5.1	+5.1
-4.0	+4.0
positive negative
Predicted label
(c)	Amazon IMDB 2021
Overall
anger 5-9	0.0	0-1
0-3
0.2
d∣sflust 0-1	0-3	0-0	0-0	0-0	0-0	0.1
surprise
neutral
0.1	0.0
anger disgust
0-4
1.2
0.0
0-6
0.2
0-0
fear happy sad
Predicted label
0.1
2.1
anger
disgust
10.3 1-8
0.6圈
surprise neu!ra∣Γ
surprise
neutral
(b) Amazon IMDB 2021
Overall
anger disgust fear hapw sad surprise neut
Predictedlabel
(e) Microsoft FER+ 2021	(f)
+0.0
+0.0
+0.0
-0.0
+0.0
+0-0
-0.0
anger +0.3 +0.0 -0.0
-0-1
-0-0
-0-0
-0.1
disgust +0-0 +0-0 +0-0 +0-0 +0-0 +0.0 +0-0
surprise
neutral
-0-0
+0-0
-0-0
-0-1
-0.1
+0-0
+0-0
-0.2
-0-0
+0-0
+0-0
-0.2
-0-0
+0-4
-0.2
-0-1
+0-0
-0-0
-0-4
-0-0
g -o.2
anger disgust fear happy sad surprise neut
Predicted label
-0.3 D
-0.2 FI¾ -0.0
(d)	Microsoft FER+ 2020
Overall accuracv：30.1
Overall accuracy:54.2
ɪ, Zekg 9Z86-
oqβl β≡H
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.1
0.0
0.0
Ol∙zekg9z86F
-aqel 0n∙lj.
≡0.0
0Λ
0Λ
0.0 0.0 0.4 4.7
OO 0?T 0.0 0.0
0.0 0.1 0.0 0.0
0.1 0.0 0.1 0.0
o.f æo o.ι æo
0.1 0.0 0.0 oʃ
0.0 0.0 0.2 0.0
0.0 0.0 0.0 0.0
0.7
0.0
0.0
0.0
3.6
0.0
0.0
0.0
0.0
0.0
0.0
Microsoft FER+ API shift
Overall Chanae:，24N
Ol∙zekg9z86F
-aqel 0n∙lj.
a	+0.0	+oo	+oo	+0.4	+0.0	+0.0	+0.1	+0.0	+0.1	i -1.9
+oo	+1Λ	-OJ	+0.0	+ao	+0.0	+0.0	+0δ	-0.2	+0.0	
+0δ	+oo		+0δ	+ao	+0.0	+0.0	+0δ	+0.0	+0.0	
+0δ	+0.0	+04	+ιl	+ao	+0.0	+0.0	耳ι	+0.0	+0.0	21
+0δ	+0δ	+0δ	-o.f	+2.1	+0.0	+0.0	+0δ	+0.0	+0.0	
+0δ		+0δ	+0δ	+ao	+1.9	五1	+0δ	+0.0	-0.1	-1.7
+oi	+0.0	+0δ	-o.Γ	+ao	+oo	+1.9	耳ι	+0.0	+0.0	-1.8
+0.1	+0δ	+0δ	+o.o	+ao	+αι	-0.1	皆 +0.0	+oo 矗 -0.1	+0.0 -0.1 +0δ	S +0.0
+0δ	+0.0	+0δ	-o.Γ	+ao	+0.0	+0.0				
+0δ	+0δ	而	+0δ	+ao	+0.0	+0.0	+0δ			
+o.o	+0.0	+o.o	+o.o	+0.0	+0.0	90	+0.0	+0.0		
O 1	2345678g
PiBdicted label
(i) Google DIGIT 2021
0123456ZB9
Predicted label
(g)	Google DIGIT 2020
0123456ZB9
Predicted label
(h)	Google DIGIT 2021

Figure 6: Confusion matrices of a few APIs in spring 2020/2021, along with their API shifts.
a phrase (e.g., “turn on the light” or “turn down the music”). There are in total 248 possible phrases,
which are mapped to 31 unique labels. The sampling rate is also 16 kHz. All those datasets are freely
available online for academic purposes.
Some of the datasets may contain personal information. For example, the human faces contained in
the facial emotion recognition dataset may be deemed as personal information. On the other hand, our
study focuses on whether there is a performance change on the dataset, and does not use or disclose
any personal information.
For sentiment analysis, we use the Google NLP API (GoN), Amazon Comprehend API (Ama), and
the Baidu NLP API (Bai). For facial emotion recognition, we use Google Vision API (Goo, a),
Microsoft Face API (Mic, a), and the Face++ API (Fac). For spoken command recognition, we adopt
Google speech API (Goo, b), Microsoft Speech API (Mic, b), and IBM speech API (IBM).
Details of observed ML API Shifts. Now we present a few more observed ML API shifts, as
shown in Figure 6. One observation is that individual entry’s change in the API shift can be larger
than the overall accuracy’s. For example, as shown in Figure 6 (c), the overall accuracy change is
about -1.1% for Amazon on IDMB, but the performance drop for positive texts is as large as 5%. This
indicates the importance of using fine-grained confusion matrix difference to measure API shifts. In
addition, when the overall accuracy increases, it is possible that the accuracy for each label has been
improved. This can be easily verified by Figure 6 (d-f). On the other hand, as shown in Figure 6 (g-i),
Google API’s large accuracy improvement (24%) is mostly because it is able to correctly predict
many samples that were previously deemed as empty. One possible explanation is that Google API
internally uses a higher threshold to generate a recognition. When the number of label increases, it
28
Published as a conference paper at ICLR 2022
(a) Amazon YELP
(e) Microsoft FER+ (f) Google EXPW+
(i) Google DIGIT (j) Google AMINST (k) Google CMD (l) Microsoft DIGIT
Figure 7: Effects of partition parameter K . The total number of partitions is LK , and thus Larger K
implies more partitions. Generally, across 12 cases where API shifts are identified, larger number of
partitions usually leads to smaller estimation error for large samples. In practice, we observe that
K = 3 is enough to reach good error rate.
might become hard to manually check the API shifts. For those cases, an anomaly detector can be
applied to quickly identify the most surprising components in the API shifts.
Partition size’s effects on MASA. Now we study how the partition number affects the performance
of MASA, as shown in Figure 7. Across all API shifts we estimated, we note that larger number
of partitions leads to a smaller overall Frobenious norm in general. This is expected, as larger K
effectively introduces more parameters to estimate and thus is more powerful. The trade-off is that
the computational cost increases, and more samples are needed for initial estimation. Interestingly, as
K becomes large, the relative error reduction improvement becomes small. This is probably because
there is no strong uncertainty difference within small partitions. In practice, we found that K = 3
already gives a small enough error reduction.
Comparison with baselines for case study on YELP. To further understand MASA’s performance,
We compared the performance of MASA with two baselines: random sampling and standard stratified
sampling (proportionate allocation). We drawed 2000 samples for all methods, and repeated the
experiments 1000 times to obtain an average of the Frobenius norm error. MASA outperforms both
baselines significantly: the observed error is 0.015 for random sampling, 0.009 for stratified sampling,
and 0.006 for MASA.
Understanding uncertainty score. MASA is developed based on the notion of uncertainty scores,
and thus it is worthy understanding how uncertainty scores of different partitions for an ML API
are computed. Here, we provide an illustrative example, as shown in Figure 8. The dataset contains
three partitions and each partition includes six data points. We use a small ball to represent each data
point, its interior color to denote its true label, and its edge color to denote the predicted label of an
evaluated ML API. For example, on partition 1 and partition 3, all edge colors match interior colors,
29
Published as a conference paper at ICLR 2022
Partition 1	Partition 2
Partition 3
Accuracy: 1.0
Uncertainty: 0.50
Accuracy: 0.5
Uncertainty: 0.00
Accuracy: 1.0
Uncertainty: 0.67
Figure 8: Illustrative examples of uncertainty scores. The dataset contains three partitions, each of
which includes six data points. Here We use a ball to represent a data point, its interior color to denote
its true label, and its edge color to indicate an ML API,s predicted label. For example, as shown in
the left panel, three points are labeled as red and the other three are labeled as blue. All points are
predicted correctly, and thus the accuracy is 1.0. As the ML API predicts half of the points as red and
half as blue, the uncertainty score is 1 - 0.5 X 0.5 - 0.5 X 0.5 = 0.5. Note that high accuracy does
not necessarily imply low uncertainty. For example, accuracy on partition 1(1.0) is higher than that
on partition 2(0.5), but its uncertainty score is actually larger than the latter. Yet, high diversity in
the predicted labels does imply higher uncertainty. For example, while accuracy on partition 1 and
partition 3 are both perfect, partition 3 incurs a higher uncertainty. This is because while only two
unique predicted labels exist in partition 1, three occur in partition 3.
and thus the accuracy is 1.0. On partition 2, interior and edge colors match only on half of the points,
and thus the accuracy is only 0.5.
To understand the calculation of the uncertainty score, let us take partition 1 as an example. The
ML API predicts the label red for half of the partition and blue for the other half. Thus, the
uncertainty score is 1 subtracting the sum of the square of likelihood of each predicted label, i.e.,
1 - 0.5 X 0.5 - 0.5 X 0.5 = 0.5. Similarly, on partition 2, the ML API always predicts the label
blue, and thus the uncertainty is simply 1 - 1 = 0. On partition 3, the ML API evenly predicts three
unique labels, and thus the uncertainty score becomes 1 - 3 X 1 - 1 X 1 - 3 X 1 = 1 ≈ 0.67.
Two observations are worthy mentioning about uncertainty scores, in addition to their non-negativity
and upper bound of 1. First, higher accuracy on a partition does not imply lower uncertainty. To
see this, note that the accuracy on partition 1 (1.0) is higher than that on partition 2 (0.5), but its
uncertainty score is actually larger than that of partition 2. In fact, an API’s accuracy on a partition
is orthogonal to its uncertainty, as uncertainty score only depends on the predicted labels and is
independent of the true labels. Second, diversity of the predicted labels is correlated to the uncertainty
score. For example, the accuracy is same on partition 1 and 3, but the uncertainty score is higher on
partition 3, mainly because there are three unique labels (red, blue, and green) in partition 3. This is
expected, as uncertainty scores are designed to capture how diverse an ML API’s prediction can be.
30