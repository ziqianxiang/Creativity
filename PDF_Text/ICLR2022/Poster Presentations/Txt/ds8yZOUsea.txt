Published as a conference paper at ICLR 2022

HIDDEN     PARAMETER     RECURRENT     STATE     SPACE

MODELS  FOR  CHANGING  DYNAMICS  SCENARIOS

Vaisakh Shaj1,2, Dieter Bu¨ chler3, Rohit Sonker4, Philipp Becker1, and Gerhard Neumann1
1Autonomous Learning Robots, KIT, Germany

2LCAS, University Of Lincoln, UK

3Max Planck Institute for Intelligent Systems, Tu¨bingen, Germany

4Indian Institute Of Technology, Kanpur

ABSTRACT

Recurrent State-space models (RSSMs) are highly expressive models for learning
patterns in time series data and system identification.  However, these models as-
sume that the dynamics are fixed and unchanging, which is rarely the case in real-
world scenarios. Many control applications often exhibit tasks with similar but not
identical dynamics which can be modeled as a latent variable.  We introduce the
Hidden Parameter Recurrent State Space Models (HiP-RSSMs), a framework that
parametrizes a family of related dynamical systems with a low-dimensional set of
latent factors.  We present a simple and effective way of learning and performing
inference over this Gaussian graphical model that avoids approximations like vari-
ational inference. We show that HiP-RSSMs outperforms RSSMs and competing
multi-task models on several challenging robotic benchmarks both on real-world
systems and simulations.

1    INTRODUCTION

System identification, i.e., learning models of dynamical systems from observed data ( Ljung (1998);
Gevers  (2005)),  is  a  key  ingredient  of  model-predictive  control  (Camacho  &  Alba  (2013)) 
 and
model-based reinforcement learning (RL). State space models (Hamilton (1994);  Jordan (2004);
Scho¨n    et al. (2011)) (SSMs) provide a principled framework for modelling dynamics.   Recently
there have been several works that fused SSMs with deep (recurrent) neural networks achieving
superior results in time series modelling (Haarnoja et al., 2016; Karl et al., 2016) and system 
identi-
fication tasks (Becker et al., 2019; Hafner et al., 2019; Shaj et al., 2020). Learning the dynamics 
in
an encoded latent space allows us to work with high dimensional observations like images and was
proven to be successful for planning from high dimensional sensory inputs. However, most of these
tasks assume a fixed, unchanging dynamics, which is quite restrictive in real-world scenarios.

Several control applications involve situations where an agent must learn to solve tasks with 
similar,
but not identical, dynamics.  A robot playing table tennis may encounter several bats with different
weights or lengths, while an agent manipulating a bottle may face bottles with varying amounts of
liquid.  An agent driving a car may encounter many different cars, each with unique handling char-
acteristics.  Humanoids learning to walk may face different terrains with varying slopes or friction
coefficients.   Any real-world dynamical system might change over time due to multiple reasons,
some      of which might not be directly observable or understood.  For example, in soft robotics 
small
variations in temperature or changes in friction coefficients of the cable drives of a robot can 
sig-
nificantly change the dynamics. Similarly, a robot may undergo wear and tear over time which can
change its dynamics.  Thus, assuming a global model that is accurate throughout the entire state
space or duration of use is a limiting factor for using such models in real-world applications.

We  found  that  existing  literature  on  recurrent  models  fails  to  model  the  dynamics  
accurately  in
these situations. Thus, we introduce hidden parameter state-space models (HiP-SSM), which allow
capturing the variation in the dynamics of different instances through a set of hidden task 
parameters.

Correspondence to Vaisakh Shaj <v.shaj@kit.edu>

1


Published as a conference paper at ICLR 2022

We formalize the HiP-SSM and show how to perform inference in this graphical model.   Under
Gaussian assumptions, we obtain a probabilistically principled yet scalable approach. We name the
resulting probabilistic recurrent neural network as Hidden Parameter Recurrent State Space Model
(HiP-RSSM). HiP-RSSM achieves state-of-the-art performance for several systems whose dynamics
change over time.  Interestingly, we also observe that HiP-RSSM often exceeds traditional RSSMs
in performance for dynamical systems previously assumed to have unchanging global dynamics due
to the identification of unobserved causal effects in the dynamics.

2    HIDDEN  PARAMETER  RECURRENT  STATE  SPACE  MODELS

State space models are Bayesian probabilistic graphical models (Koller & Friedman, 2009; Jordan,
2004) that are popular for learning patterns and predicting behavior in sequential data and 
dynamical
systems. Let f ( ) be any arbitrary dynamic model, and let h( ) be any arbitrary observation model.
Then, the dynamic systems of a Gaussian state space model can be represented using the following
equations

zt = f (zt−₁, at) + ut,     ut ∼ N(0, Q)

ot = h(zt) + vt,     vt ∼ N(0, R).

Here  zt,  at and  ot are  the  latent  states,  control  inputs/actions  and  observations  at  
time  t.   The
vectors ut         (0, Q) and vt         (0, R) denote zero-mean Gaussian noise.  Further, based on
the  assumptions  made  for  f ( )  and  h( ),  we  can  have  different  variants  of  Gaussian  
state  space
models (Ribeiro, 2004; Wan & Van Der Merwe, 2000). In the linear case, inference can be performed
efficiently using Kalman Filters.

Our goal is to learn a state space model that can model the dynamics of partially observable robotic
systems under scenarios where the dynamics changes over time. Often, the dynamics of real systems
may differ in significant ways from the system our models were trained on.  However, it is unrea-
sonable to train a model across all possible conditions an agent may encounter. Instead, we propose
a state space model that learns to account for the causal factors of variation observed across tasks
at training time, and then infer at test time the model that best describe the system. Thus, we 
intro-
duce a new formulation namely Hidden Parameter State Space Models (HiP-SSMs), a framework
for modeling families of SSMs with different but related dynamics using low-dimensional latent
task embeddings. In this section, we formally define the HiP-SSM family in Section 3.1, propose a
probabilistically principled inference scheme for HiP-SSMs based on the forward algorithm (Jordan,
2004) in Section 3.2, and finally provide training scheme for the setting in Section 3.3.


2.1    HIDDEN PARAMETER STATE SPACE MODELS (HIP-SSMS)

We denote a set of SSMs with transition dynamics fl that are fully de-
scribed by hidden parameters l and observation model h as a Hidden
Parameter SSM (HiP-SSM). In this definition we assume the observa-
tion model h to be independent of the hidden parameter l as we only
focus on cases where the dynamics changes.  HiP-SSMs allows us to
extend SSMs to multi-task settings where dynamics can vary across
tasks.  Defining the changes in dynamics by a latent variable unifies
dynamics across tasks as a single global function.  In dynamical sys-
tems, for example, parameters can be physical quantities like gravity,
friction of a surface, or the strength of a robot actuator.  Their effects
influence the dynamics but not directly observed; and hence l is not

Cl

l

z₀                 z₁                 z₂

part of the observation space and is treated as latent task parameter

vector.                                                                                             
             ʷ0                        w1                        w2


Formally, a HiP-SSM is defined by a tuple      ,    ,     ,   , f, h  , where

,      and       are the sets of hidden states z, actions a, and observa-
tions w respectively.     is the set of all possible latent task parameters
and let p₀(l) be the prior over these parameters.  Thus, a HiP-RSSM
describes a class of dynamics and a particular instance of that class is
obtained by fixing the parameter vector l         .  The dynamics f  for
each instance depend on the value of the hidden parameters l.

2

Figure 1: The graphical model
for a particular instance for the
HiP-SSM.  The  transition  dy-
namics between latent states is
a  function  of  the  previous  la-
tent state, and a specific latent

task  parameter  l which  is  in-

ferred from a context set of ob-
served past observations.   Ac-
tions are omitted for brevity.


Published as a conference paper at ICLR 2022


N

rˡ                               l

Cl

Context
Encoder

{rˡ , (σˡ  )²}N


l                    l                 l

t                    t                 t+1

z0                                         z1                                         z2

HiP-RSSM Cell

n

Context
Update

µ   σ

rn         n=1


zt−, Σ−t

l      l

z⁺, Σ⁺

zt−+1, Σ−t+1


w0                a0

w1                a1

w2                a2

Observation          t         t

Update

wt   σobs

Observation
Encoder

Time
Update

at

Decoder
Output


o0                                         o1                                         o2

(a)

ot

(b)

oˆt+1

Figure 2: (a) The Gaussian graphical model corresponding to a particular instance l        for the 
HiP-RSSM.

The posterior of the latent task/context variable is inferred via a Bayesian aggregation procedure 
described
in  2.2.1  based  on  a  set  of  N  interaction  histories.   The  prior  over  the  latent  
states  zt−  is  inferred  via  task
conditional Kalman time update described in 2.2.2 and the posterior over the latent states z⁺ is 
inferred via

Kalman measurement update described in 2.2.3.  Here, the hollow arrows denote deterministic 
transformation
leading to implicit distributions,  using the context and observation encoders.   (b) Depicts the 
schematic Of
HiP-RSSM. The output of the task conditional ‘Time Update’ stage, which forms the prior for the 
next time
step (zt−+1, Σ−t+1) is decoded to get the prediction of the next observation.

Each instance of a HiP-SSM is an SSM conditioned on l.  We also make the additional assumption
that the parameter vector l is fixed for the duration of the task (i.e. a local segment of a 
trajectory),
and thus the latent task parameter has no dynamics.  This assumption considerably simplifies the
procedure for inferring the hidden parametrization and is reasonable since dynamics can be assumed
to       be locally-consistent over small trajectories in most applications (Nagabandi et al., 
2018a).

The definition is inspired from related literature on HiP-MDPs (Doshi-Velez & Konidaris, 2016),
where the only unobserved variable is the latent task variable.  One can connect HiP-SSMs with
HiP-MDPs by including rewards to the definition and formalize HiP-POMDPs. However this is left
for future work.

2.2    LEARNING AND INFERENCE

We perform inference and learning in the HiP-SSM borrowing concepts from both deep learning and
graphical model communities following recent works on recurrent neural network models (Haarnoja
et al., 2016; Becker et al., 2019), where the architecture of the network is informed by the 
structure
of the probabilistic state estimator.  We denote the resulting probabilistic recurrent neural 
network
architecture as Hidden Parameter Recurrent State Space Model (HiP-RSSM).

The structure of the Bayesian network shown in Figure 1 allows tractable inference of latent vari-
ables by the forward algorithm (Jordan, 2004; Koller & Friedman, 2009).   Since we are dealing
with continuous dynamical systems, we assume a Gaussian multivariate distribution over all vari-
ables (both observed and hidden) for the graph shown in Figure 1.   This assumption has several
advantages.  Firstly, it makes the inference very similar to the well studied Kalman Filtering ap-
proaches.  Secondly, the Gaussian assumptions and conditional in-dependencies allows us to have
a  closed form solution to each of these update procedures which are fully differentiable and can be
backpropagated to the deep encoders. The belief update over the hidden variables zt and l happens
in three stages.  Similar to Kalman filtering approaches, we have two recursive belief state update
stages, the time update and observation update which calculate the prior and posterior belief over
the latent states respectively.  However, we have an additional hierarchical latent variable l which
models the (unobserved) causal factors of variation in dynamics in order to achieve efficient gener-
alization.  Hence, we have a third belief update stage to calculate the posterior over the latent 
task
variable based on the observed context set.  Each of these three stages are detailed in the sections
below:

2.2.1    INFERRING THE LATENT TASK VARIABLE (CONTEXT UPDATE)

In  this  stage,  we  infer  the  posterior  over  the  Gaussian  latent  task  variable  l  based  
on  an  ob-
served  context  set  Cl.     For  any  HiP-RSSM  instance  defined  over  a  target  sequence  T   
  =

3


Published as a conference paper at ICLR 2022

(ot, at, ot₊₁, ..., ot₊N , at₊N ),  over  which  we  intend  to  perform  state  
estimation/prediction,  we
maintain a fixed context set Cl.   Cl  in our HiP-SSM formalism can be obtained as per the algo-


rithm designer’s choice. We choose a Cl consisting a set of tuples Cl = {oˡ

, aˡ

t−n

, oˡ

t−n+1

N

n=1

Here each set element is a tuple consisting of the current state/observation, current action and 
next
state/observation for the previous N time steps.

Inferring a latent task variable l  ∈  L  based on an observed context set Cl  =  {xˡ }N      has 
been

explored previously by different neural process architectures (Gordon et al., 2018; Garnelo et al.,
2018). Neural processes are multi-task latent variable models that rely on deep set functions 
(Zaheer
et al., 2017) to output a latent representation out of varying number of context points in a 
permutation
invariant manner. Similar to Volpp et al. (2020) we formulate context data aggregation as a Bayesian
inference problem, where the information contained in Cl is directly aggregated into the statistical
description of l based on a factorized Gaussian observation model of the form p(rˡ |l), where


ˡ  = encr(oˡ

, aˡ

t−n

, oˡ         ),

t−n+1

.σˡ Σ2  = enc  (oˡ     , aˡ     , oˡ         ).

Here n is the index of an element from context set   l.  Given a prior p₀(l)  =      (l µ₀, 
diag(σ²))

we  can  compute  the  posterior  p(l   l)  using  Bayes  rule.   The  Gaussian  assumption  allows 
 us  to

get a closed form solution for the posterior estimate of the latent task variable,  p(l   l) based 
on
Gaussian conditioning. The factorization assumption further simplifies this update rule by avoiding
computationally expensive matrix inversions into a simpler update rule as


σ² = ..

0

n=1

N

l Σ₂Σ⊖Σ⊖

µ  = µ   + σ² ⊙ Σ .rˡ  − µ  Σ2  ⊘ .σˡ Σ2

where    ,      and      denote element-wise inversion, product, and division, respectively.  
Intuitively
the mean of the latent task variable µl is a weighted sum of the individual latent observations rˡ 
,

while the variance of the latent task variable σ² gives the uncertainty of this task 
representation.

2.2.2    INFERRING PRIOR LATENT STATES OVER THE NEXT TIME STEP (TIME UPDATE)

The goal of this step is to compute the prior marginal

p(zt|w₁:t−₁, a₁:t, Cl) = ∫∫  p(zt|zt−₁, at, l)p(zt−₁|w₁:t−₁, a₁:t−₁, Cl)p(l|Cl)dzt−₁dl.      (1)

We    assume    a    dynamical    model    of    the    following    form:        p(zt|zt−₁, at, l) 
       =

t is known and not subject to noise.

At any time t, the posterior over the belief state zt  ₁ w₁:t  ₁, a₁:t  ₁, posterior over the 
latent task
variable l   l  and the action at are independent of each other since they form a “common-effect”

/ v-structure (Koller et al., 2007) with the unobserved variable zt.  With these independencies and
Gaussian assumptions, according to Gaussian Identity 1, it can be shown that calculating the 
integral
in equation 1 has a closed form solution as follows, p(zt|w₁:t−₁, a₁:t, Cl) = N(zt−, Σt−), where

zt− = At−₁zt−₁ + Bat + Cl,                                                           (2)

Σ−t   = At−1Σ+  1AT  1  + CΣlCT  + Σtrans.                                       (3)

t−      t−

Gaussian  Identity  1.  If  u  ∼  N(µu + b, Σu)  and  v  ∼  N(µv, Σv)  are  normally  distributed
independent random va∫riables and if the conditional distribution p (y|u, v) = N(Au+b+Bv, Σ),

We use a locally linear transition model At  ₁  as in Becker et al. (2019) and a non-linear control
model as in Shaj et al. (2020). The local linearization around the posterior mean, can be 
interpreted

4


Published as a conference paper at ICLR 2022

as equivalent to an EKF. For the latent task transformation we can either use a linear, 
locally-linear
or non-linear transformation.  More details regarding the latent task transformation model can be
found in the appendix F.5.   Our ablations (Figure 3c) show that a non-linear feedforward neural
network f (.) that outputs mean and variances and interact additively gave the best performance in
practice. f (.) transforms the latent task moments µl and σl directly into the latent space of the 
state
space model via additive interactions. The corresponding time update equations are given below:

zt− = At−₁z⁺ 1  + b(at) + f (µl),

Σ−t   = At−1Σ+  1AT  1  + f (σl) + Σtrans.

t−      t−

2.2.3    INFERRING POSTERIOR LATENT STATES / OBSERVATION UPDATE

The   goal   of   this   step   is   to   compute   the   posterior   belief   p(zt o₁:t, a₁:t, 
Cl).      We   first
map   the   observations   at   each   time   step   ot  to   a   latent   space   using   an   
observation   en-
coder   (Haarnoja   et   al.,   2016;   Becker   et   al.,   2019)   which   emits   latent   
features   wt  along

with  an  uncertainty  in  those  features  via  a  variance  vector  σᵗ   .    We  then  compute  
the  pos-

terior   belief   p(zt w₁:t, a₁:t, Cl),   based   on   p(zt w₁:t  ₁, a₁:t, Cl)   obtained   from   
the   time
update,  the  latent  observation  (wt, σᵒᵇˢ)  and  the  observation  model  H.    This  is  
exactly  the
Kalman  update  step,  which  has  a  closed  form  solution  as  shown  below  for  a  time  
instant  t,

Kalman Gain:                                 Q   = Σ−HT .HΣ−HT + I · σᵒᵇˢΣ−1 ,

		

Posterior Mean:                             z⁺ = z− + Qt .wt − Hz−Σ ,

	

Posterior Covariance:                   Σ⁺ = (I − QtH) Σ−,

where  I  denotes  the  identity  matrix.   This  update  is  added  as  a  layer  in  the  
computation  graph

(Haarnoja et al. (2016); Becker et al. (2019)).  However, the Kalman update involves computation-
ally expensive matrix inversions of the order of    (L³), where L is the dimension of the latent 
state
zt.  Thus, in order to make the approach scalable we follow the same factorization assumptions as
in Becker et al. (2019). This factorization provides a simple way of reducing the observation update
equation to a set of scalar operations, bringing down the computational complexity from O(L³) to
O(L). More mathematical details on the simplified update equation can be found in appendix F.7.

Discussion    From a computational perspective, this a Gaussian conditioning layer, similar to sec-
tion 2.2.1.  Both outputs a posterior distribution over latent variables z, given a prior p(z) and 
an
observation model p(w z), using Bayes rule: p(z w) = p(w z)p(z)/p(w). This has a closed form
solution because of Gaussian assumptions, which is coded as a layer in the neural network.  The
observation model is assumed to have the following structure, P (w|z) = N(Hz, Σₒbs).

2.3    TRAINING HIP-RSSM FOR EPISODIC ADAPTATION TO CHANGING DYNAMICS

The latent task variable l models a distribution over functions Garnelo et al. (2018), rather than a
single function.  In our case these functions are latent dynamics functions.  In order to train such
a model we use a training procedure that reflects this objective, where we form datasets consisting
of timeseries, each with a different latent transition dynamics. Thus, we collect a set of 
trajectories
over which the dynamics changes over time. This can be trajectories where a robot picks up objects
of different weights or a robot traversing terrain of different slopes. Now we introduce a 
multi-task
setting with a rather flexible definition of task, where each temporal segment of a trajectory can 
be
considered to be a different “task” and the observed context set based on interaction histories from
the past N  time steps provides information about the current task setting.  This definition allows
us to have potentially infinite number of tasks/local dynamical systems and the distribution over
these tasks/systems is modelled using a hierarchical latent task variable l.  The formalism is based
on the assumption that over these local temporal segments, the dynamics is unchanging. This local
consistency in dynamics holds for most real world scenarios (Nagabandi et al., 2018b;a).  Yet, our
formalism can model the global changes in dynamics at test time, since we obtain a different 
instance
of the HiP-RSSM for each temporal segment based on the observed context set.  We also provide
a detailed description of the multi-task dataset creation process in the appendix E and a pictorial
illustration in appendix D.

Batch Training. Let T         B×N×D be the batch of local temporal segments with different dynam-
ics which we intend to model with the HiP-RSSM formalism.  Given a target batch T , HiP-RSSM

5


Published as a conference paper at ICLR 2022

can  be  trained  in  a  supervised  fashion  similar  to  popular  recurrent  architectures  like  
LSTMs  or
GRUs.  However for each local temporal sequence t      T , over which the dynamics is being mod-
elled, we also input a set of the N previous interactions, which forms the context set C         
B×N×D
for inferring the latent task as explained in Section 2.2.1. Processing the context set C adds 
minimal
additional computational/memory constraints as we use a permutation invariant set encoder. The set

encoder allows parallelization in processing the context set as opposed to the recurrent processing
of target set.

The learnable parameters in the computation graph includes the locally linear transition model At,
the non-linear control factor b, the linear/non-linear latent task transformation model C, the 
transi-
tion noise Σtrₐns, along with the observation encoder, context encoder and the output decoder.

Loss Function.  We optimize the root mean square error (RMSE) between the decoder output and
the ground truth states.  As in Shaj et al. (2020) we use the differences to next state as our 
ground
truth states, as this results in better performance for dynamics learning especially at higher 
frequen-
cies.  In principle, we could train on the Gaussian log-likelihood instead of the RMSE and hence
model the variances. Training on RMSE yields slightly better predictions and allows for a fair com-
parison with deterministic baselines which use the same loss like feed-forward neural networks,
LSTMs and meta-learnng algorithms such as MAML. Thus, we report results with the RMSE loss.
A probabilistic formulation for this loss function is provided in appendix B.

Gradients are computed using (truncated) backpropagation through time (BPTT) (Werbos (1990))
and clipped.  We optimize the objective using the Adam (Kingma & Ba (2014)) stochastic gradient
descent optimizer with default parameters.

3    RELATED  WORK

Deep State Space Models.   Classical State-space models (SSMs) are popular due to their tractable
inference and interpretable predictions. However, inference and learning in SSMs with high dimen-
sional and large datasets are often not scalable.  Recently, there have been several works on deep
SSMs that offer tractable inference and scalability to high dimensional and large datasets. Haarnoja
et  al. (2016);  Becker  et  al. (2019);  Shaj  et  al. (2020)  use  neural  network architectures  
based  on
closed-form solutions of the forward inference algorithm on SSMs and achieve the state of the art
results in state estimation and dynamics prediction tasks. Krishnan et al. (2017); Karl et al. 
(2016);
Hafner  et  al.  (2019)  perform  learning  and  inference  on  SSMs  using  variational  
approximations.
However, most of these recurrent state-space models assume that the dynamics is fixed, which is a
significant drawback since this is rarely the case in real-world applications such as robotics.

Recurrent Switching Dynamical Systems.  Linderman et al. (2017); Becker-Ehmck et al. (2019);
Dong et al. (2020) tries to address the problem of changing/multi-modal dynamics by incorporat-
ing additional discrete switching latent variables. However, these discrete states make learning and
inference more involved.  Linderman et al. (2017) uses auxiliary variable methods for approximate
inference in a multi-stage training process, while Becker-Ehmck et al. (2019); Dong et al. (2020)
uses variational approximations and relies on additional regularization/annealing to encourage dis-
crete state transitions.  On the other hand Fraccaro et al. (2017) uses “soft” switches, which can 
be
interpreted as a SLDS which interpolate linear regimes continuously rather than using truly discrete
states.  We take a rather different, simpler formalism for modelling changing dynamics by view-
ing  it as a multi-task learning problem with a continuous hierarchical hidden parameter that model
the distribution over these tasks.   Further our experiments in appendix C.1 show that our model
significantly outperfroms the soft switching baseline (Fraccaro et al., 2017).

Hidden Parameter MDPs.  Hidden Parameter Markov Decision Process (HiP-MDP) (Doshi-Velez
& Konidaris, 2016; Killian et al., 2016) address the setting of learning to solve tasks with 
similar,
but not identical, dynamics.  In HiP-MDP formalism, the states are assumed to be fully observed.
However,  we formalize the partially observable setting where we are interested in modelling the
dynamics in a latent space under changing scenarios. The formalism is critical for learning from 
high
dimensional observations and dealing with partial observability and missing data. The formalism can
be   easily extended to HiP-POMDPs by including rewards into the graphical model 1, for planning
and control in the latent space (Hafner et al., 2019; Sekar et al., 2020).  However this is left as 
a
future work.

6


Published as a conference paper at ICLR 2022


No Imputation     50% Imputation
HiP-RSSM     2.30 ± 0.043      2.47 ± 0.012

RKN               3.088 ± 0.046       3.223 ± 0.014

LSTM             3.108 ± 0.041       3.630 ± 0.097

GRU               3.287 ± 0.013       3.621 ± 0.047

FFNN             8.150 ± 0.047                  -

NP                  5.526 ± 0.030                  -

MAML           7.314 ± 0.021                  -               

(a) Pneumatic RMSE .10−³Σ

No Imputation      50 % Imputation
HiP-RSSM     2.833 ± 0.024      2.843 ± 0.024

RKN                3.392 ± 0.062         3.398 ± 0.062

LSTM              3.503 ± 0.006         3.736 ± 0.062

GRU                 3.407 ± 0.02          3.642 ± 0.153

FFNN              3.313 ± 0.018                    -

NP                  2.765 ± 0.004                    -

MAML            3.202 ± 0.006                    -               

(b) Franka RMSE .10−⁴Σ

No Imputation     50% Imputation
HiP-RSSM     2.96 ± 0.212      6.15 ± 0.327

RKN                7.17 ± 0.017        14.66 ± 0.224

LSTM              9.14 ± 0.026        51.21 ± 0.431

GRU               9.216 ± 0.073       53.14 ± 0.242

FFNN              8.72 ± 0.021                   -

NP                   4.57 ± 0.013                   -

MAML            5.04 ± 0.051                   -               

(c) Mobile Robot RMSE (10−⁵)

Table 1:  Prediction Error in RMSE for (a) pneumatic muscular arm (4.1) and (b) Franka Arm 
manipulating
varying loads (4.2) for both fully observable and partially observable scenarios.

Meta Learning For Changing Dynamics.  There exists a family of approaches that attempt online
model adaptation to changing dynamics scenarios via meta-learning (Nagabandi et al., 2018a;b).
They perform online adaptation on the parameters of the dynamics models through gradient de-
scent (Finn et al., 2017) from interaction histories.  Our method fundamentally differs from these
approaches in the sense that we do not perform a gradient descent step at every time step during 
test
time, which is computationally impractical in modern robotics, where we often deal with high fre-
quency data. We also empirically show that our approach adapts better especially in scenarios with
non-markovian dynamics, a property that is often encountered in real world robots due to stiction,
slip,   friction, pneumatic/hydraulic/cable-driven actuators etc. Sæmundsson et al. (2018); 
Achterhold
& Stueckler (2021) on the other hand learn context conditioned hierarchical dynamics model for
control in a formalism similar to HiP-MDPs. The former meta-learn a context conditioned gaussian
process while the later use a context conditioned determisitic GRU. Our method on the other hand
focuses on a principled probabilistic formulation and inference scheme for context conditioned re-
current state space models, which is critical for learning under partial observabilty/high 
dimensional
observations and with noisy and missing data.

4    EXPERIMENTS

This section evaluates our approach on a diverse set of dynamical systems from the robotics domain
in  simulations  and  real  systems.   We  show  that  HiP-RSSM  outperforms  contemporary  
recurrent
state-space models (RSSMs) and recurrent neural networks (RNNs) by a significant margin under
changing dynamics scenarios. Further, we show that HiP-RSSM outperforms these models even un-
der situations with partial observability/ missing values. We also baseline our HiP-RSSM with con-
temporary multi-task models and improve performance, particularly in modelling non-Markovian
dynamics and under partial observability. Finally, the visualizations of the Gaussian latent task 
vari-
ables  in HiP-RSSM demonstrates that they learn meaningful representations of the causal factors of
variations in dynamics in an unsupervised fashion.

We consider the following baselines:

•  RNNs - We compare our method to two widely used recurrent neural network architectures,
LSTMs (Hochreiter & Schmidhuber, 1997) and GRUs (Cho et al., 2014).

•  RSSMs - Among several RSSMs from the literature, we chose RKN (Becker et al., 2019)
as these have shown excellent performance for dynamics learning (Shaj et al., 2020) and
relies on exact inference as in our case.

•  Multi Task Models - We also compare with state of the art multi-task learning models like
Neural Processes (NPs) and MAML (Nagabandi et al., 2018a).  Both models receive the
same context information as in HiP-RSSM.

In case of recurrent models we replace the HiP-RSSM cell with a properly tuned LSTM, GRU and
RKN Cell respectively,  while fixing the encoder and decoder architectures.   For the NP baseline
we  use  the  same  context  encoder  and  aggregation  mechanism  as  in  HiP-RSSM  to  ensure  a  
fair
comparison.  We create partially observable settings by imputing 50% of the observations during
inference. More details regarding the baselines and hyperparameters can be found in the appendix.

7


Published as a conference paper at ICLR 2022


0.55

0.50

Linear

Locally Linear
Non Linear

0.45

0.40

0.35

0.30

0.25


(a)                                                     (b)

0.20

Pneumatic

Mobile

(c)

Franka

Figure 3:  (a) and (b) shows the tSNE (Van der Maaten & Hinton, 2008) plots of the latent task 
embeddings
produced from randomly sampled instances of HiP-RSSM for two different robots.   (a) The wheeled 
robot
discussed in section 4.3 traversing terrains of varying slopes. The color coding indicates the 
average gradients
of    the terrain for each of these instances.  These can have either positive or negative values.  
(b) The Franka
manipulator with loads of varying weights attached to the end-effector.  The color coding indicated 
weights
ranging from 0 to 3 kgs. (c) An ablation on the performance of different task transformation models 
discussed
in section 2.2.2.

4.1    SOFT ROBOT PLAYING TABLE TENNIS

We first evaluate our model on learning the dynamics of a pneumatically actuated muscular robot.
This  four  Degree  of  Freedom  (DoF)  robotic  arm  is  actuated  by  Pneumatic  Artificial  
Muscles
(PAMs) (Bu¨chler et al., 2016).  The data consists of trajectories of hitting movements with vary-
ing speeds while playing table tennis (Bu¨chler et al., 2020).  This robot’s fast motions with high
accelerations are complicated to model due to hysteresis and hence require recurrent models (Shaj
et           al., 2020).

We show the prediction accuracy in RMSE in Table 5a.  We observe that the HiP-RSSM can out-
perform the previous state of the art predictions obtained by recurrent models. Based on the domain
knowledge, we hypothesise that the latent context variable captures multiple unobserved causal fac-
tors of variation that affect the dynamics in the latent space, which are not modelled in 
contemporary
recurrent models. These causal factors could be, in particular, the temperature changes or the 
friction
due to a different path that the Bowden trains take within the robot. Disentangling and interpreting
these causal factors can be exciting and improve generalisation, but it is out of scope for the 
current
work. Further, we find that the multi-task models like NPs and MAML fail to model these dynamics
accurately compared to all the recurrent baselines because of the non-markovian dynamics resulting
from the high accelerations in this pneumatically actuated robot.

4.2    ROBOT MANIPULATION WITH VARYING LOADS

We collected the data from a 7 DoF Franka Emika Panda manipulator carrying six different loads at
its end-effector.  It involved a mix of movements of different velocities from slow to swift motions
covering the entire workspace of the robot. We chose trajectories with four different loads as 
training
set and evaluated the performance on two unseen weights, which results in a scenario where the
dynamics change over time. Here, the causal factor of variation in dynamics is the weights attached
to the end-effector and assumed to be unobserved.

We show the prediction errors in RMSE in Table 1c.  HiP-RSSMs outperforms all recurrent state-
space models, including RKN and deterministic RNNs, in modelling these dynamics under fully
observable and partially observable conditions.  The multi-task baselines of NPs and MAML per-
form equally well under full observability for this task because of the near Markovian dynamics of
Franka Manipulator, which often does not need recurrent models.  However, HiP-RSSMs have an
additional advantage in that these are naturally suited for partially observable scenarios and can 
pre-
dict ahead in a compact latent space, a critical component for recent success in model-based control
and planning (Hafner et al., 2019).

4.3    ROBOT LOCOMOTION IN TERRAINS OF DIFFERENT SLOPES

Wheeled mobile robots are the most common types of robots being used in exploration of unknown
terrains where they may face uneven and challenging terrain.  We set up an environment using a

8


Published as a conference paper at ICLR 2022


(a)

(b)                                                     (c)

Figure 4:  (a) Simulated environment of a Wheeled Robot traversing terrains of different slopes.  
(b) and (c)
shows how the one dimensional UMAP (McInnes et al., 2018) embedding of the inferred latent task 
variable
(top blue plots) changes at test time when an agent undergoes changes in its dynamics for the 
franka robot and
mobile robot respectively.  An indicator of the Ground Truth (GT) Task (bottom red plots) variables 
are also
given.  In case of the Franka Robot, the groundtruth (GT) tasks denotes the switching of dynamics 
between 0
kg (free motion) and 2.5 kg loads. In case of the mobile robot the groundtruth (GT) tasks denoted 
slopes of the
terrain.

Pybullet simulator (Coumans & Bai, 2016) where a four-wheeled mobile robot traverses an uneven
terrain of varying steepness generated by sinusoidal functions (Sonker & Dutta, 2020) as shown
in 4a.  This problem is challenging due to the highly non-linear dynamics involving wheel-terrain
interactions.  In addition, the varying steepness levels of the terrain results in a changing 
dynamics
scenario, which further increases the complexity of the task.

We show the prediction errors in RMSE in Table 1c. When most recurrent models, including RSSMs
and deterministic RNNs, fail to model these dynamics, HiP-RSSMs are by far the most accurate in
modelling these challenging dynamics in the latent space.  Further, the HiP-RSSMs perform much
better than state of the art multi-task models like NPs and MAML.

We finally visualize the latent task representations using TSNE in Figure 3a. As seen from the plot,
those instances of the HiP-SSMs under similar terrain slopes get clustered together in the latent
task space,  indicating that the model correctly identifies the causal effects in the dynamics in an
unsupervised fashion.

4.4    VISUALIZING CHANGING HIDDEN PARAMETERS AT TEST TIME OVER TRAJECTORIES

WITH VARYING DYNAMICS

We finally perform inference using the trained HiP-RSSM in a multi-task / changing dynamics sce-
nario where the dynamics continuously changes over time. We use the inference procedure described
in appendix D based on a fluid definition for “task” as the local dynamics in a temporal segment.
We plot the global variation in the latent task variable captured by each instance of the HiP-RSSM
over these local temporal segments using the dimensionality reduction technique UMAP (McInnes
et al., 2018).  As seen in figures 4b and 4c, the latent task variable captures these causal 
factors of
variations in an interpretable manner.

5    CONCLUSION

We proposed HiP-RSSM, a probabilistically principled recurrent neural network architecture for
modelling changing dynamics scenarios.  We start by formalizing a new framework, HiP-SSM, to
address the multi-task state-space modelling setting.  HiP-SSM assumes a shared latent state and
action space across tasks but additionally assumes latent structure in the dynamics.  We exploit the
structure of the resulting Bayesian network to learn a universal dynamics model with latent parame-
ter l via exact inference and backpropagation through time. The resulting recurrent neural network,
namely HiP-RSSM, learns to cluster SSM instances with similar dynamics together in an unsuper-
vised fashion.  Our experimental results on various robotic benchmarks show that HiP-RSSMs sig-
nificantly outperform state of the art recurrent neural network architectures on dynamics modelling
tasks.  We believe that modelling the dynamics in the latent space which disentangles the state, ac-
tion and task representations can benefit multiple future applications including planning/control in
the latent space and causal factor identification.

9


Published as a conference paper at ICLR 2022

6    AKNOWLEDGEMENTS

We thank Michael Volpp, Onur Celik, Marc Hanheide and the anonymous reviewers for valuable
remarks and discussions which greatly improved this paper.  The authors acknowledge support by
the state of Baden-Wu¨rttemberg through bwHPC and the Lichtenberg high performance computer
of the TU Darmstadt. This work was supported by the EPSRC UK (project NCNR, National Centre
for Nuclear Robotics,EP/R02572X/1).

REFERENCES

Jan  Achterhold  and  Joerg  Stueckler.   Explore  the  context:  Optimal  data  collection  for  
context-
conditional dynamics models. In International Conference on Artificial Intelligence and Statistics,
pp. 3529–3537. PMLR, 2021.

Philipp Becker, Harit Pandya, Gregor Gebhardt, Cheng Zhao, C James Taylor, and Gerhard Neu-
mann. Recurrent kalman networks: Factorized inference in high-dimensional deep feature spaces.
In International Conference on Machine Learning, pp. 544–552. PMLR, 2019.

Philip Becker-Ehmck, Jan Peters, and Patrick Van Der Smagt.  Switching linear dynamics for vari-
ational bayes filtering.  In International Conference on Machine Learning, pp. 553–562. PMLR,
2019.

Christopher M Bishop. Pattern recognition. Machine learning, 128(9), 2006.

Dieter Bu¨chler, Heiko Ott, and Jan Peters.  A lightweight robotic arm with pneumatic muscles for
robot learning.  In 2016 IEEE International Conference on Robotics and Automation (ICRA), pp.
4086–4092. IEEE, 2016.

Dieter  Bu¨chler,  Simon  Guist,  Roberto  Calandra,  Vincent  Berenz,  Bernhard  Scho¨lkopf,  and  
Jan
Peters.    Learning  to  play  table  tennis  from  scratch  using  muscular  robots.    arXiv  
preprint
arXiv:2006.05935, 2020.

Eduardo  F  Camacho  and  Carlos  Bordons  Alba.   Model  predictive  control.   Springer  science  
&
business media, 2013.

Kyunghyun Cho, Bart van Merrie¨nboer, Dzmitry Bahdanau, and Yoshua Bengio.  On the properties
of neural machine translation:  Encoder–decoder approaches.  In Proceedings of SSST-8, Eighth
Workshop on Syntax, Semantics and Structure in Statistical Translation, pp. 103–111. Association
for Computational Linguistics, October 2014.

Erwin  Coumans  and  Yunfei  Bai.   Pybullet,  a  python  module  for  physics  simulation  for  
games,
robotics and machine learning. 2016.

Zhe Dong, Bryan Seybold, Kevin Murphy, and Hung Bui. Collapsed amortized variational inference
for switching nonlinear dynamical systems.  In International Conference on Machine Learning,
pp. 2638–2647. PMLR, 2020.

Finale Doshi-Velez and George Konidaris.  Hidden parameter markov decision processes:  A semi-
parametric regression approach for discovering latent task parametrizations.  In IJCAI: proceed-
ings of the conference, volume 2016, pp. 1432. NIH Public Access, 2016.

Chelsea Finn, Ian Goodfellow, and Sergey Levine.  Unsupervised learning for physical interaction
through video prediction. Advances in neural information processing systems, 29:64–72, 2016.

Chelsea Finn, Pieter Abbeel, and Sergey Levine.  Model-agnostic meta-learning for fast adaptation
of deep networks.   In International Conference on Machine Learning, pp. 1126–1135. PMLR,
2017.

Marco Fraccaro, Simon Kamronn, Ulrich Paquet, and Ole Winther. A disentangled recognition and
nonlinear dynamics model for unsupervised learning. arXiv preprint arXiv:1710.05741, 2017.

Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J Rezende, SM Eslami,
and Yee Whye Teh. Neural processes. arXiv preprint arXiv:1807.01622, 2018.

10


Published as a conference paper at ICLR 2022

Michel Gevers. Identification for control: From the early achievements to the revival of experiment
design. European journal of control, 11(4-5):335–352, 2005.

Jonathan Gordon, John Bronskill, Matthias Bauer, Sebastian Nowozin, and Richard E Turner. Meta-
learning probabilistic inference for prediction. arXiv preprint arXiv:1805.09921, 2018.

Tuomas Haarnoja, Anurag Ajay, Sergey Levine, and Pieter Abbeel. Backprop kf: Learning discrim-
inative deterministic state estimators.  In Advances in neural information processing systems, pp.
4376–4384, 2016.

Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson.  Learning latent dynamics for planning from pixels.  In International Conference on
Machine Learning, pp. 2555–2565. PMLR, 2019.

James D Hamilton. State-space models. Handbook of econometrics, 4:3039–3080, 1994.

Sepp  Hochreiter  and  Ju¨rgen  Schmidhuber.    Long  short-term  memory.    Neural  Comput.,  9(8):
1735–1780, November 1997. ISSN 0899-7667.

Michael I Jordan. Graphical models. Statistical science, 19(1):140–155, 2004.

Maximilian  Karl,  Maximilian  Soelch,  Justin  Bayer,  and  Patrick  Van  der  Smagt.    Deep  
varia-
tional bayes filters:  Unsupervised learning of state space models from raw data.  arXiv preprint
arXiv:1605.06432, 2016.

Taylor Killian, George Konidaris, and Finale Doshi-Velez.  Transfer learning across patient varia-
tions with hidden parameter markov decision processes. arXiv preprint arXiv:1612.00475, 2016.

Diederik P Kingma and Jimmy Ba.  Adam:  A method for stochastic optimization.  arXiv preprint
arXiv:1412.6980, 2014.

Daphne Koller and Nir Friedman.  Probabilistic graphical models: principles and techniques.  MIT
press, 2009.

Daphne Koller, Nir Friedman, Lise Getoor, and Ben Taskar.  Graphical models in a nutshell.  Intro-
duction to statistical relational learning, 43, 2007.

Rahul Krishnan, Uri Shalit, and David Sontag.  Structured inference networks for nonlinear state
space  models.   In  Proceedings  of  the  AAAI  Conference  on  Artificial  Intelligence,  volume  
31,
2017.

Scott Linderman, Matthew Johnson, Andrew Miller, Ryan Adams, David Blei, and Liam Paninski.
Bayesian learning and inference in recurrent switching linear dynamical systems.   In Artificial
Intelligence and Statistics, pp. 914–922. PMLR, 2017.

Lennart Ljung.   System identification.   In Signal analysis and prediction, pp. 163–173. Springer,
1998.

Leland McInnes, John Healy, and James Melville.   Umap:  Uniform manifold approximation and
projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018.

Anusha Nagabandi,  Ignasi Clavera,  Simin Liu,  Ronald S Fearing,  Pieter Abbeel,  Sergey Levine,
and  Chelsea  Finn.    Learning  to  adapt  in  dynamic,  real-world  environments  through  meta-
reinforcement learning. arXiv preprint arXiv:1803.11347, 2018a.

Anusha  Nagabandi,  Chelsea  Finn,  and  Sergey  Levine.   Deep  online  learning  via  
meta-learning:
Continual adaptation for model-based rl. arXiv preprint arXiv:1812.07671, 2018b.

Maria Isabel Ribeiro.   Kalman and extended kalman filters:  Concept,  derivation and properties.

Institute for Systems and Robotics, 43:46, 2004.

Steindo´r Sæmundsson, Katja Hofmann, and Marc Peter Deisenroth.  Meta reinforcement learning
with latent variable gaussian processes. arXiv preprint arXiv:1803.07551, 2018.

11


Published as a conference paper at ICLR 2022

Thomas B Scho¨n, Adrian Wills, and Brett Ninness.  System identification of nonlinear state-space
models. Automatica, 47(1):39–49, 2011.

Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak.
Planning to explore via self-supervised world models.  In International Conference on Machine
Learning, pp. 8583–8592. PMLR, 2020.

Vaisakh Shaj,  Philipp Becker,  Dieter Buchler,  Harit Pandya,  Niels van Duijkeren,  C James Tay-
lor, Marc Hanheide, and Gerhard Neumann.  Action-conditional recurrent kalman networks for
forward and inverse dynamics learning. arXiv preprint arXiv:2010.10201, 2020.

Rohit Sonker and Ashish Dutta.  Adding terrain height to improve model learning for path tracking
on uneven terrain by a four wheel robot.  IEEE Robotics and Automation Letters, 6(1):239–246,
2020.

Laurens Van der Maaten and Geoffrey Hinton.  Visualizing data using t-sne.  Journal of machine
learning research, 9(11), 2008.

Michael Volpp, Fabian Flu¨renbrock, Lukas Grossberger, Christian Daniel, and Gerhard Neumann.
Bayesian  context  aggregation  for  neural  processes.   In  International  Conference  on  
Learning
Representations, 2020.

Eric A Wan and Rudolph Van Der Merwe.  The unscented kalman filter for nonlinear estimation.
In Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and
Control Symposium (Cat. No. 00EX373), pp. 153–158. Ieee, 2000.

Paul J Werbos.  Backpropagation through time:  what it does and how to do it.  Proceedings of the
IEEE, 78(10):1550–1560, 1990.

Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, and
Alexander Smola. Deep sets. arXiv preprint arXiv:1703.06114, 2017.

12


Published as a conference paper at ICLR 2022

A    PROOF  FOR  GAUSSIAN  IDENTITY  1

This section provides a proof for Gaussian Identity 1.  First we derive an expression for the joint
distribution p(u, v, y).

Gaussian Identity 2.  If u          (µu + b, Σu) and v          (µv, Σv) are normally distributed 
inde-
pendent random variables and if conditional distribution p(y u, v)  =      (Au + b + Bv, Σ), the
joint distribution has an expression as follows:


.  u  Σ

.           µu

Σ     Σu        0                        ΣuA⊤              


v       ∼ N 

µv

Aµu + b + Bµv

,          0          Σv                  ΣvB⊤

AΣ⊤u      BΣ⊤v      AΣuA⊤ + BΣvB⊤ + Σ

Proof for Identity 2

Let displacement of a variable u be denoted by ∆u = u − ⟨u⟩.
Since u and v are independent, covariances .∆u∆v⊤Σ = 0.

We can write y = Au+b+Bv+ϵ, where ϵ ∼ΣN (0., Σ) and bΣis a cons.tant. ThenΣwe have.covariancΣe

Since   ∆u∆v⊤   =   ∆u∆ϵ⊤   = 0 we therefore have   ∆u∆y⊤   = ΣuA⊤.

The derivations for covariances               ⊤  follows similarly and the corresponding covariance 
has
the expression, .∆v∆y⊤Σ = ΣvB⊤.

Similarly, .∆y∆y⊤Σ = .(A∆u + B∆v + ∆ϵ)(A∆u + B∆v + ∆ϵ)⊤Σ = A .∆u∆u⊤Σ A⊤ +

Gaussian Identity 3 (Gaussian Marginalization).  If


u                           µu

v       ∼ N     µv

	

Σuu     Σuv     Σuy

,      Σ⊤uv     Σvv     Σvy
Σ⊤uy     Σ⊤vy     Σyy

then marginal over y is given as p(y) =   u,v p(y|u, v)p(u)p(v)dudv = N (µy, Σyy)

Proof For Identity 3

We refer to Bishop (2006) for the derivation, which requires calculation of the Schur complement
as well as completing the square of the Gaussian p.d.f.   to integrate out the variable.   The given
derivation (Bishop, 2006) for two variable multivariate Gaussians can be extended to 3 variable case
WLOG.

Proof For Identity 1 is immediate from Identity 2 and Identity 3.

B    PROBABILISTIC  FORMULATION  OF  HIP-RSSM TRAINING  OBJECTIVE

The  training  objective  for  the  HiP-RSSM  involves  maximizing  the  posterior  predictive  log-
likelihood given below:


Σ Σ log p(oˆᵇ

|Cl, o

) = Σ Σ

log ∫

p(oˆᵇ   |z

)p(zᵇ

|o    , Cl)dz

.        (4)


t=1  b=1

t+1

1:t

t=1  b=1

t+1

t+1

t+1

1:t

t+1

Here, for any time t, oˆt₊₁ are the ground truth observations for the next time step and the latent 
state

prior p(zᵇ   |oᵐ , Cl) as discussed in the Section 2.2.2, have closed form solutions.  We approxi-

mate the objective in Eq.(4) using a deterministic variational approximation, similar to Becker et 
al.
(2019).

We  employ  a  Gaussian  approximation  of  the  posterior  predictive  log  likelihood  of  the  
form


p(oˆᵇ

|Cl, oᵇ

)               (µ  b

t+1

, diag((σ  b

t+1

)²)).     Here  µ  b

t+1

=    decµ(zᵇ

)  and  σ  b      =

t

decΣ(Σᵇ   )), where decµ() and decΣ() denote the parts of the output decoder that are responsi-

ble for decoding the latent means and latent variances respectively. This decoder can be interpreted
13


Published as a conference paper at ICLR 2022


14                                                                 HiP-RSSM

LSTM

12

RKN

10                                                                 Soft-Switching

8

6

4

2

100

80

60

40

20

HiP-RSSM
RKN

       

LSTM

           Soft Switching


0

0            500         1000        1500        2000        2500

epochs

(a)

1 Step         10 Step        20 Step        30 Step        50 Step

Number Of Steps

(b)

Figure  5:  Comparison  of  different  algorithms  for  Wheeled  Mobile  Robot  in  terms  of  (a)  
moving  average
of decoder error in normalized RMSE for the test set,  plotted against training epochs (b) 
multi-step ahead
prediction error in RMSE.

as a “moment matching network”, computing the moments of oᵇ given the moments of zᵇ (Volpp

t                                                t

et al., 2020; Becker et al., 2019).  This allows to evaluate an approximation to the objective in a
deterministic manner.

Alternatively, we could ignore the variances and only focus on the mean predictions by training our
model with a RMSE loss. Training on RMSE yields slightly better predictions and allows for a fair
comparison with deterministic baselines (Feed Forward NN, LSTM, GRU, MAML etc) and hence
we report results with the RMSE loss.

C    ADDITIONAL  EXPERIMENTS

C.1    COMPARISON TO SOFT SWITCHING BASELINE

We also implement a soft-switching baseline similar to Fraccaro et al. (2017), where a soft mix-
ture of dynamics is implemented in the latent transition dynamics (Kalman time update). Similar to
Fraccaro et al. (2017), we now globally learn K constant transition matrices A⁽ᵏ⁾ and control matri-
ces B⁽ᵏ⁾. An interpolation is done between these using a “dynamics parameter network” (Fraccaro
et al., 2017) αt = αt (w₀:t  ₁).  The dynamics parameter network is implemented with a recurrent
neural network with LSTM cells that takes at each time step the mean of the encoded observation

wt as input and recurses dt  = LSTM (wt−₁, dt−₁) and αt  =  softmax (dt).  The output of the


dynamics parameter network is weights that sum to one,     K

choose and interpolate between K different operating modes:

α⁽ᵏ⁾ (w₀:t−₁) = 1.  These weights

K                                                            K

At  = Σ α(k) (w0:t−1) A(k),     Bt  = Σ α(k) (w0:t−1) B(k)

	

Authors interpret the weighted sum as a soft mixture of K different Linear Gaussian SSMs whose
time-invariant matrices are combined using the time varying weights αt. In practice, each of the K
sets   A⁽ᵏ⁾, B⁽ᵏ⁾    models different/changing dynamics, that will dominate when the corresponding

α⁽ᵏ⁾ is high.

Figure 5 compares HiP-RSSM with different recurrent architectures including the soft-switching
baseline.  As seen in figure 5, HiP-RSSM clearly outperforms the soft-switching baseline both in
terms of convergence speed and also mult-step ahead predictions. More details regarding the multi-
step ahead training procedure can be found in appendix C.3.

C.2    ABLATION ON CONTEXT ENCODER

In  table  2,  we  report  the  details  of  evaluating  our  bayesian  aggregation  based  context 
 set  en-
coder  (discussed  in  2.2.1)  against  the  a  causal  /  recurrent  encoder  that  takes  into  
account  the
temporal structure of the context data.   We used a probabilistic recurrent encoder (Becker et al.,

14


Published as a conference paper at ICLR 2022

2019), whose mean and variance from the last time step is used to infer the posterior latent task
distribution p(l   l)  =      (µl, diag(σ²)).  The dimensions of latent task parameters obtained 
from
both the set and recurrent encoders are kept the same (60).

The reported experiments are conducted


on data from wheeled mobile robot dis-
cussed in section 4.3. As reported in Ta-
ble  2  the  permutation  invariant  set  en-
coder outperforms the recurrent encoder

Table 2: Comparison between the permutation invariant set en-
coder and recurrent encoder.  The performance is measured in
terms of prediction RMSE (10-5) and mean of the training time
per epoch (in seconds) over 5 runs.

by a good margin in terms of prediction                                                             
                                      


accuracy for both fully and partially ob-
servable scenarios.  Additionally the set
encoder is far more efficient in terms of
computational time required for training

No Imputation RMSE     50% Imputation RMSE     Training Time Per Epoch

Set Encoder                      2.96 ± 0.212                     6.15 ± 0.327                      
       6.71

Recurrent Encoder             5.10 ± 0.041                     10.12 ± 0.112                        
      14.13                   

as seen from the time taken per epoch for each of these cases.

C.3    MULTI-STEP AHEAD PREDICTIONS

In figure 5b, we compare the results of multi-step ahead predictions for upto 50 steps of HiP-RSSM
with recurrent baselines like RKN(Becker et al., 2019) and LSTM. Inorder to train the recurrent
models for multi step ahead predictions, we removed three-quarters of the observations from the
temporal sequence and tasked the models with imputing those missing observations, only based on
the knowledge of available actions/control commands, i.e., we train the models to perform action
conditional future predictions to impute missing observations.  The imputation employs the model
for multi-step ahead predictions in a convenient way (Shaj et al., 2020).  One could instead also go
for a dedicated multi-step loss function as in approaches like Finn et al. (2016).

As seen in figure 5b, HiP-RSSM clearly outperforms contemporary recurrent models for multi-step
ahead prediction tasks since it takes into account additional causal factors of variation (slopes 
of the
terrain for this robot) in the latent dynamics in an unsupervised manner.

D    HIP-RSSM DURING  TEST  TIME  / INFERENCE

We perform inference using HiP-RSSM at test time on a trajectory with varying dynamics using
algorithm 1.  A pictorial representation of the same is given in the figure 6.  We use this 
inference
scheme to visualize how the latent variable l, that describe different instances of a HiP-RSSM over
different temporal segments, evolve at a global level.  The visualizations are reported in figures 
4b
and 4c in the main text.

Algorithm 1: HiP-RSSM Test Time Inference

Required: Trained HiP-RSSM Model

Required: A time series τ of length K >> N

Divide the time series τ into non-overlapping windows Tl of length N. Let

T  =   T₁, T₂, T₃, ., ., .   be the ordered list of all temporal segments, sorted in the ascending
order of time of occurrence.

foreach each time window Tl    T  do

1.  maintain a context set Cl consisting of N previous interactions;

2.  infer posterior latent task variable p(l|Cl) using context update stage as in section 2.2.1;

3.  using the posterior over latent task variable l   l and observations in sequence Tl to
perform sequential Bayesian inference over the state space model using Kalman
observation update (2.2.3) and task conditional Kalman time update; (2.2.2)

end

15


Published as a conference paper at ICLR 2022

Time Series With Changing Dynamics


Locally consistent dynamics
modelled as a HiP-RSSM instance

- Context Set

- Target Time Series

Figure 6: HiP-RSSM Inference Under Changing Dynamics Scenarios

E    MULTI-TASK  DATASET  CREATION

Algorithm 2: Multi Task Dataset Creation For Training HiP-RSSM

Required:A set S of trajectories of changing dynamics

D      ϕ

foreach each trajectory τ     S do

1.  divide the trajectory τ into non-overlapping windows Tl of length N. Let
T  = {T₁, T₂, T₃, ., ., .} be the list of all temporal segments/time-series.
foreach each time window Tl ∈ T  do

(a)  maintain a context set Cl consisting of N previous interactions;

(b)  update D ← D ∪ {Cl, Tl}

end

end

Output: Output D consisting of batch of context and target sets.

F    IMPLEMENTATION  DETAILS

F.1    CONTEXT SET ENCODER AND LATENT TASK REPRESENTATION


The HiP-RSSM maps a set of previous interaction histories,  {oˡ

l
t,n

l
t+1,n

N

n=1

,  to a set of

latent features and an estimate of uncertainty in those features,  {rˡ , (σˡ )²}N    ,  using a 
context

encoder.  We use a feed-forward neural network as the encoder in all of our experiements since we
deal with high dimensional vectors as our observations.  However, depending upon the nature of
observations, we could use different encoder architectures.

The set of latent features and uncertainties, {rˡ , .σˡ Σ2}N    , are further aggregated in a 
probabilis-

tically principled manner using bayesian aggregation operator discussed in 2.2.1 to get a gaussian

latent task variable, with a mean (µl) and diagonal covariance (σl). Intuitively the context encoder
learns to weight the contribution from each observation in the context set based on bayesian 
prinici-
ples and emits a probabilistic representation of the latent task.

F.2    OBSERVATION ENCODER AND LATENT STATE REPRESENTATION

HiP-RSSM  transforms  the  observations  at  each  time  step  ot to  a  latent  space  using  an  
encoder
network which emits latent features wt and an estimate of the uncertainty in those features via a
variance vector σᵒᵇˢ.

The probabilistic recurrent module uses a latent state vector zt and corresponding covariance Σt
whose transitions are governed by the update rules in the HiP-RSSM cell.  The latent state vector
zt has  been  designed  to  contain  two  conceptual  parts,  a  vector  pt for  holding  
information  that
can directly be extracted from the observations and a vector mt to store information inferred over
time, e.g., velocities. The former is referred to as the observation or upper part and the latter 
as the
memory or lower part of the latent state as in Becker et al. (2019). For an ordinary dynamical 
system
and images as observations the former may correspond to positions while the latter corresponds to

velocities.  The corresponding posterior and prior covariance matrices Σ⁺  and Σ− have a chosen

t               t

factorized representation to yield simple Kalman updates, i.e.,

16


Published as a conference paper at ICLR 2022

u           s

Σt  =       t          t

t         t

where each of Σᵘ, Σˢ, Σˡ  ∈  Rm×m  is a diagonal matrix.  The vectors σᵘ, σˡ  and σˢ  denote the

vectors containing the diagonal values of those matrices.  This structure with Σˢ  ensures that the
correlation between the memory and the observation parts are not neglected as opposed to the case
of designing Σt as a diagonal covariance matrix.  This representation was exploited to avoid the
expensive and numerically problematic matrix inversions involved in the KF equations as shown
below.

F.3    LOCALLY LINEAR TRANSITION MODEL

The state transitions in the time update stage (section 2.2.2) of the HiP-RSSM Cell is governed
by a locally linear transition model as in Becker et al. (2019).  To obtain a locally linear 
transition

model, the HiP-RSSM gloablly learns K constant transition matrices A⁽ᵏ⁾ and interpolate between

them using state dependent coefficients α⁽ᵏ⁾(z⁺), i.e., At  =  ΣK     α⁽ᵏ⁾(z⁺)A⁽ᵏ⁾, where z⁺ is

the mean of the posterior distribution at time step t. So we locally linearize At around the 
posterior
mean, which can be interpreted as equivalent to what an EKF do with a Jacobian.  A small neural
network with softmax output is used to learn α⁽ᵏ⁾.

F.4    CONTROL MODEL

To achieve action conditioning within the recurrent cell, we include a control model b(at) in 
addition
to the locally linear transition model At. b(at) = f (at), where f (.) can be any non-linear 
function
approximator.  We use a multi-layer neural network regressor with ReLU activations (Shaj et al.,
2020).

F.5    LATENT TASK TRANSFORMATION MODEL

To achieve latent task conditioning within the recurrent cell, we include a task transformation 
model
(c), in addition to the locally linear transition model At and control model b in the time update
stage (section 2.2.2). Though in section 2.2.2, we used the notation for a linear task 
transformation
matrix, C, to motivate the additive interaction of latent task variables, µl and σl in the latent 
space,
the task transformation function can be designed in several ways, i.e.:

(i)  Linear: c = C, where C is a linear transformation matrix. The corresponding time update
equations are given below:

zt− = At−₁z⁺ 1  + b(at) + Cµl,

Σ−t   = At−1Σ+  1AT  1  + C(I · σl)CT  + Σtrans.

(ii)  Locally-Linear:  c  =  Ct, where Ct =  ΣK     β⁽ᵏ⁾(zt)C⁽ᵏ⁾ is a linear combination of k

linear control models C⁽ᵏ⁾.  A small neural network with softmax output is used to learn

β⁽ᵏ⁾. The corresponding time update equations are given below:

zt− = At−₁z⁺ 1  + b(at) + Ctµl,

Σ−t   = At−1Σ+  1AT  1  + Ct(I · σl)CT  + Σtrans.

(iii)  Non-Linear:  c  =  f , where f (.) can be any non-linear function approximator.  We use
a multi-layer neural network regressor with ReLU activations, which transforms the latent
task moments µl and σl directly into the latent space of the state space model via additive
interactions. The corresponding time update equations are given below:

zt− = At−₁z⁺ 1  + b(at) + f (µl),

Σ−t   = At−1Σ+  1AT  1  + f (σl) + Σtrans.

t−      t−

17


Published as a conference paper at ICLR 2022

In our ablation study (Figure 3c), for the linear and locally linear task transformation models, we
assume that the dimension of the latent context variable l and the latent state space zt are equal. 
This
allows us to work with square matrices which are more convenient. For the non-linear transformation
we are free to choose the size of the latent context variable.  However for a fairer comparison we
keep the dimension of latent task variable to be similar in all three cases. We choose the 
non-linear
task transformation model in HiP-RSSM architecture as this gave the best performance in practice.

F.6    OBSERVATION MODEL

The latent state space Z  = Rn oΣf the HiP-RSSM is rΣelated to the observation space W by the 
linear

z        , where Im denotes the m    m identity matrix and 0m×₍n−m₎ denotes a m    (n    m) matrix

filled with zeros.  Typically, m is set to n/2.  This corresponds to the assumption that the first 
half

of the state can be directly observed while the second half is unobserved and contains information
inferred over time (Becker et al. (2019)).

F.7    OBSERVATION UPDATE STEP

The observation update discussed in section 2.2.3 involves computing the Kalman gain matrix Qt,
which requires computationally expensive matrix inversions that are difficult to backpropagate, at
least for high dimensional latent state representations. However, the choice of a locally linear 
tran-
sition model, the factorized covariance Σt, and the special observation model simplify the Kalman
update to scalar operations as shown below.  As the network is free to choose its own state rep-
resentation, it finds a representation where such assumptions works well in practice Becker et al.
(2019).

Similar  to  the  state,  the  Kalman  gain  matrix  Qt  is  split  into  an  upper  Qᵘ  and  a  
lower  part

t

Qˡ .   Both  Qᵘ  and  Qˡ                                                                            
                                     H  =

t              t           t are  squared  matrices.   Due  to  the  simple  latent  observation  
model

Σ  Im   0m×₍n−m₎  Σ and the factorized covariances, all off-diagonal entries of Qᵘ and Qˡ  are zero

t             t

by

qu  = σu,− ⊘ .σu,− + σobsΣ

			

ql  = σs,− ⊘ .σu,− + σobsΣ ,

where     denotes an elementwise vector division.  The update equation for the mean therefore sim-
plifies to


z⁺ = z− +     q

Σ    Σ  wt − zu,−  Σ


t           t                l

t

wt − zu,−

where     denotes the elementwise vector product.  The update equations for the individual parts of

covariance are given by

σᵘ,⁺ = (1m − qᵘ) ⊙ σᵘ,−,

σˢ,⁺ = (1m − qᵘ) ⊙ σˢ,−,

σl,+  = σl,− − ql  ⊙ σs,−,

where 1m denotes the m dimensional vector consisting of ones.

G    ROBOTS  AND  DATA

The experiments are performed on data from two different real robots.  The details of robots, data
and data preprocessing is explained below:

G.1    MUSCULOSKELETAL ROBOT ARM

Observation and Data Set:  For this soft robot we have 4 dimensional observation inputs(joint
angles)  and  8  dimensional  action  inputs(pressures).   We  collected  the  data  of  a  four  
DoF  robot

18


Published as a conference paper at ICLR 2022

Figure 7:  The experiments are performed on data from robots with different actuator dynamics.  
From left to
right these include: Pneumatically actuated artificial muscles (Bu¨chler et al., 2016), 
electrically actuated Franka
Emika Panda Robotic Arm.

actuated by Pneumatic Artificial Muscles (PAMs).   The robot arm has eight PAMs in total with
each DoF actuated by an antagonistic pair.  The robot arm reaches high joint angle accelerations of
up to 28, 000deg/s2 while avoiding dangerous joint limits thanks to the antagonistic actuation and
limits on the air pressure ranges.  The data consists of trajectories collected while training with 
a
model-free reinforcement learning algorithm to hit balls while playing table tennis.  We sampled
the  data  at  100Hz.   The  hysteresis  associated  with  the  pneumatic  actuators  used  in  
this  robot  is
challenging to model and is relevant to the soft robotics in general.

Training Procedure: For the fully observable case, we trained HiP-RSSM for one-step ahead pre-
diction using an RMSE loss.  For partially observable case, during training, we randomly removed
half of the observations from the sequences and tasked the models with imputing those missing
states, only based on the knowledge of available actions/control commands, i.e., we train the models
to perform action conditional future predictions to impute missing states.  The imputation employs
the model for multi-step ahead predictions in a convenient way.

Other recurrent baselines (RKN, LSTM, GRU) are trained in a similar fashion except that, we don’t
maintain a context set of interaction histories during training/inference.

G.2    FRANKA EMIKA PANDA ROBOT ARM

Observation and Data Set: We collected the data from a 7 DoF Franka Emika Panda manipulator
during free motion and while manipulating loads with weights 0kg (free motion), 0.5 kg, 1 kg, 1.5
kg, 2 kg and 2.5 kg.  Data is sampled at high frequencies (1kHz).  The training trajectories were
motions with loads 0kg(free motion), 1kg, 1.5kg, 2.5 kgs, while the testing trajectories contained
motions with loads of 0.5kg and 2 kgs. The observations for the forward model consist of the seven
joint angles in radians, and the corresponding actions were joint Torques in Nm. We divide the data
into sequences of length 600 while training the recurrent models for forward dynamics, with 300
time-steps (corresponding to 300 milli-seconds) used as context set and rest 300 is used for the
recurrent time-series modelling.

Training Procedure: For the fully observable case, we trained HiP-RSSM for one-step ahead pre-
diction using an RMSE loss.  Similar to the training procedure for partially observable case as in
G.1, during training we randomly removed half of the observations(joint angles) from the sequences
and tasked the models with imputing those missing observations, only based on the knowledge of
available actions/control commands.

Other recurrent baselines (RKN, LSTM, GRU) are trained in a similar fashion except that, we dont
maintain a context set of interaction histories during training/inference.

G.3    WHEELED MOBILE ROBOT

Observation  and  Data  Set:   We  collected  50  random  trajectories  from  a  Pybullet  
simulator  a
wheeled mobile robot traversing terrain with sinusoidal slopes.  Data is sampled at high frequen-
cies (500Hz).  40 out of the 50 trajectories were used for training and the rest 10 for testing.  
The
observations consists of parameters which completely describe its location and orientation of the

19


Published as a conference paper at ICLR 2022

robot. The observation of the robot at any time instance t consists of the following features:.

ot = [x, y, z, cos(α), sin(α), cos(β)

sin(β), cos(γ), sin(γ)]

where, x, y, z - denote the global position of the Center of Mass of he robot, α, β, γ     Roll, 
pitch
and yaw angles of the robot respectively, in the global frame of reference (Sonker & Dutta, 2020).
We divide the data into sequences of length 300 while training the recurrent models for forward
dynamics,  with 150 time-steps (corresponding to 300 milli-seconds) used as context set and rest
150      is used for the recurrent time-series modelling.

Training Procedure: For the fully observable case, we trained HiP-RSSM for one-step ahead pre-
diction using an RMSE loss.  Similar to the training procedure for partially observable case as in
G.1, during training we randomly removed half of the observations from the sequences and tasked
the models with imputing those missing observations,  only based on the knowledge of available
actions/control commands.

Other recurrent baselines (RKN, LSTM, GRU) are trained in a similar fashion except that, we dont
maintain a context set of interaction histories during training/inference.

H    HYPERPARAMETERS

H.1    PNEUMATIC MUSCULOSKELTAL ROBOT ARM

Recurrent Models

Hyperparameter                  HiP-RSSM                RKN                     LSTM              
GRU
Learning Rate                               8e-4                       8e-4                     
1e-3                           1e-3
Latent Observation Dimension     15                           15                        15          
                    15

Latent State Dimension                30                           30                        75     
                         75

Latent Task Dimension                 30                           -                           -    
                             -

Context Encoder (HiP-RSSM): 1 fully connected + linear output (elu + 1)

•  Fully Connected 1: 240, ReLU

Observation Encoder (HiP-RSSM,RKN,LSTM,GRU): 1 fully connected + linear output (elu + 1)

•  Fully Connected 1: 120, ReLU

Observation Decoder (HiP-RSSM,RKN,LSTM): 1 fully connected + linear output:

•  Fully Connected 1: 120, ReLU

Transition Model (HiP-RSSM,RKN): number of basis: 15

•  α(zt): No hidden layers - softmax output

Control Model (HiP-RSSM,RKN): 3 fully connected + linear output

•  Fully Connected 1: 120, ReLU

•  Fully Connected 2: 120, ReLU

•  Fully Connected 3: 120, ReLU

Neural Process (NP) Baseline
Latent Task Dimension: 30
Learning Rate: 9e-4

Optimizer Used: Adam Optimizer

Context Encoder : 1 fully connected + linear output (elu + 1)

20


Published as a conference paper at ICLR 2022

•  Fully Connected 1: 240, ReLU

•  Linear output: 30

Decoder 3 fully connected + linear output

•  Fully Connected 1: 512, ReLU

•  Fully Connected 2: 240, ReLU

•  Fully Connected 2: 120, ReLU

FFNN Baseline

Learning Rate: 1e-3

Optimizer Used: Adam Optimizer

2 fully connected + linear output

•  Fully Connected 1: 6000, ReLU

•  Fully Connected 2: 3000, ReLU

MAML Baseline

Meta Optimizer Learning Rate: 3e-3
Inner Optimizer Learning Rate: 0.4
Number Of Gradient Steps: 1

Optimizer Used: Adam Optimizer (Meta Update) and SGD Optimizer (Inner Update)
Model

Feed Forward Neural Network with 3 fully connected + linear output

•  Fully Connected 1: 512, ReLU

•  Fully Connected 2: 240, ReLU

•  Fully Connected 2: 120, ReLU

H.2    FRANKA ROBOT ARM WITH VARYING LOADS

Recurrent Models

Hyperparameter                  HiP-RSSM                RKN                     LSTM              
GRU
Learning Rate                               1e-3                       1e-3                     
3e-3                           3e-3
Latent Observation Dimension     15                           15                        15          
                    15

Latent State Dimension                30                           30                        75     
                         75

Latent Task Dimension                 30                           -                           -    
                             -

Encoder (HiP-RSSM,RKN,LSTM,GRU): 1 fully connected + linear output (elu + 1)

•  Fully Connected 1: 30, ReLU

Observation Decoder (HiP-RSSM,RKN,LSTM,GRU): 1 fully connected + linear output:

•  Fully Connected 1: 30, ReLU

Transition Model (HiP-RSSM,RKN): number of basis: 32

•  α(zt): No hidden layers - softmax output

Control Model (HiP-RSSM, RKN): 1 fully connected + linear output

•  Fully Connected 1: 120, ReLU

21


Published as a conference paper at ICLR 2022

Neural Process (NP) Baseline
Latent Task Dimension: 30
Learning Rate: 1e-3

Optimizer Used: Adam Optimizer

Context Encoder : 1 fully connected + linear output (elu + 1)

•  Fully Connected 1: 240, ReLU

•  Linear output: 30

Decoder 3 fully connected + linear output

•  Fully Connected 1: 512, ReLU

•  Fully Connected 2: 240, ReLU

•  Fully Connected 2: 120, ReLU

FFNN Baseline

Learning Rate: 1e-3

Optimizer Used: Adam Optimizer

2 fully connected + linear output

•  Fully Connected 1: 6000, ReLU

•  Fully Connected 2: 3000, ReLU

MAML Baseline

Meta Optimizer Learning Rate: 3e-4
Inner Optimizer Learning Rate: 0.04
Number Of Gradient Steps: 1

Optimizer Used: Adam Optimizer (Meta Update) and SGD Optimizer (Inner Update)
Model

Feed Forward Neural Network with 3 fully connected + linear output

•  Fully Connected 1: 512, ReLU

•  Fully Connected 2: 240, ReLU

•  Fully Connected 2: 120, ReLU

H.3    WHEELED ROBOT TRAVERSING SLOPES OF DIFFERENT HEIGHT

Recurrent Models

Hyperparameter                  HiP-RSSM                RKN                     LSTM              
GRU
Learning Rate                               9e-4                       9e-4                     
1e-2                           1e-2
Latent Observation Dimension     30                           30                        15          
                    15

Latent State Dimension                60                           60                        75     
                         75

Latent Task Dimension                 60                           -                           -    
                             -

Encoder (HiP-RSSM,RKN,LSTM,GRU): 1 fully connected + linear output (elu + 1)

•  Fully Connected 1: 120, ReLU

Observation Decoder (HiP-RSSM,RKN,LSTM,GRU): 1 fully connected + linear output:

•  Fully Connected 1: 240, ReLU

Transition Model (HiP-RSSM,RKN): number of basis: 15

22


Published as a conference paper at ICLR 2022

•  α(zt): No hidden layers - softmax output

Control Model (HiP-RSSM, RKN): 3 fully connected + linear output

•  Fully Connected 1: 120, ReLU

Neural Process (NP) Baseline
Latent Task Dimension: 30
Learning Rate: 1e-3

Optimizer Used: Adam Optimizer

Context Encoder : 1 fully connected + linear output (elu + 1)

•  Fully Connected 1: 240, ReLU

•  Linear output: 30

Decoder 3 fully connected + linear output

•  Fully Connected 1: 512, ReLU

•  Fully Connected 2: 240, ReLU

•  Fully Connected 2: 120, ReLU

•  Fully Connected 2: 60, ReLU

FFNN Baseline

Learning Rate: 2e-3

Optimizer Used: Adam Optimizer

3 fully connected + linear output

•  Fully Connected 1: 512, ReLU

•  Fully Connected 2: 240, ReLU

•  Fully Connected 2: 120, ReLU

•  Fully Connected 2: 60, ReLU

MAML Baseline

Meta Optimizer Learning Rate: 3e-4
Inner Optimizer Learning Rate: 0.4
Number Of Gradient Steps: 1

Optimizer Used: Adam Optimizer (Meta Update) and SGD Optimizer (Inner Update).
Model

Feed Forward Neural Network with 3 fully connected + linear output

•  Fully Connected 1: 512, ReLU

•  Fully Connected 2: 240, ReLU

•  Fully Connected 2: 120, ReLU

•  Fully Connected 3: 60, ReLU

23

