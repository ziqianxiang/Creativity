Published as a conference paper at ICLR 2022
Graph-based Nearest Neighbor Search
in Hyperbolic Spaces
Liudmila Prokhorenkova
Yandex Research
Moscow, Russia
ostroumova-la@yandex.ru
Nikolay Bogachev
IITP RAS; MIPT
Moscow, Russia
nvbogach@mail.ru
Alexander Kolpakov
Universite de NeUchatel
NeUchateL Switzerland
kolpakov.alexander@gmail.com
Dmitry Baranchuk
Yandex Research
Moscow, RUssia
dbaranchuk@yandex-team.ru
Yury Demidovich
MIPT
Moscow, RUssia
demidovich.yua@phystech.edu
Ab stract
The nearest neighbor search (NNS) problem is widely stUdied in EUclidean space,
and graph-based algorithms are known to oUtperform other approaches for this
task. However, hyperbolic geometry often allows for better data representation in
varioUs domains, inclUding graphs, words, and images. In this paper, we show
that graph-based approaches are also well sUited for hyperbolic geometry. From
a theoretical perspective, we rigoroUsly analyze the time and space complexity
of graph-based NNS, assUming that an n-element dataset is Uniformly distribUted
within a d-dimensional ball of radiUs R in the hyperbolic space of cUrvatUre -1.
Under some conditions on R and d, we derive the time and space complexity of
graph-based NNS and compare the obtained resUlts with known gUarantees for
the EUclidean case. Interestingly, in the dense setting (d log n) and Under some
assUmptions on the radiUs R, graph-based NNS has lower time complexity in the
hyperbolic space. This agrees with oUr experiments: we consider datasets em-
bedded in hyperbolic and EUclidean spaces and show that graph-based NNS can
be more efficient in the hyperbolic space. We also demonstrate that graph-based
methods oUtperform other existing baselines for hyperbolic NNS. Overall, oUr the-
oretical and empirical analysis sUggests that graph-based NNS can be considered
a defaUlt approach for similarity search in hyperbolic spaces.
1	Introduction
Given a dataset D of n elements and an arbitrary qUery q, the goal of the nearest neighbor search
(NNS) is to qUickly retrieve one or several elements from D closest to q. NNS is extensively Used in a
wide range of machine learning systems: for non-parametric classification and regression, langUage
modeling, information retrieval, recommendations, and many others (Bishop, 2006; Chen et al.,
2018; May & Ozerov, 2015; Shakhnarovich et al., 2006). In large-scale applications, searching for
the nearest neighbors can be a bottleneck, so this operation’s efficiency is of the essence.
Many efficient NNS algorithms exist for EUclidean and, generally, `p spaces. Among them are meth-
ods based on recUrsive partitions of the space, e.g., k-d trees and random projection trees (Bentley,
1975; DasgUpta & FreUnd, 2008; DasgUpta & Sinha, 2015; Keivani & Sinha, 2018), well stUd-
ied Locality Sensitive Hashing (LSH) (Indyk & Motwani, 1998), and methods based on similarity
graphs (FU et al., 2019; Iwasaki & Miyazaki, 2018; Malkov et al., 2014; Malkov & YashUnin, 2018).
1
Published as a conference paper at ICLR 2022
However, hyperbolic geometry is often found to be more suitable for data representation in various
domains, including natural language processing, computer vision, graph analysis, and recommenda-
tions (Chamberlain et al., 2017; Chami et al., 2020; Khrulkov et al., 2020; Mirvakhabova et al., 2020;
Nickel & Kiela, 2017; 2018; Tifrea et al., 2019). Intuitively, the hyperbolic space has advantages
over standard representations when data has a latent hierarchical structure which is often the case.
Hence, a natural question is: how to perform efficient NNS in hyperbolic spaces? This problem has
started to gain attention recently and was addressed in Wu & Charikar (2020). In a nutshell, Wu
& Charikar (2020) show how any given Euclidean NNS algorithm can be applied to the hyperbolic
space. To get convergence guarantees, one has to call the chosen NNS method several times, which
leads to increased NNS complexity compared to the original Euclidean algorithm. In the current pa-
per, we choose an alternative approach and apply a leading NNS technique directly to the hyperbolic
space. We demonstrate that under some assumptions, the proposed method does not have increased
complexity and may even reduce the query time compared to its Euclidean counterpart.
We focus on graph-based NNS methods that are based on a best-first routing over similarity graphs.
Namely, at the pre-processing phase, a similarity graph is constructed on the elements of the dataset.
Then, given a query q, we start from some element of D and measure distances between its neighbors
and q. The neighbors are added to a priority queue. Then, we pick the closest to q candidate from
this queue and continue the process (Fu et al., 2019; Iwasaki & Miyazaki, 2018; Malkov et al.,
2014; Malkov & Yashunin, 2018). It was found empirically that graph-based NNS outperforms
other approaches in many large-scale applications (AUmuller et al., 2019). In contrast to other NNS
methods, graph-based approaches can be easily applied to non-Euclidean spaces. Indeed, for most
algorithms, both graph construction and graph routing can be performed without appealing to the
geometry of the underlying space — only the relative ordering of neighbors is used. Thus, one can
replace Euclidean distance with the hyperbolic one in a favorite graph-based approach. However, it
is a priori not known how graph-based approaches would work in such a non-standard scenario.
In this paper, we show that graph-based approaches are well suited for NNS in hyperbolic spaces.
From a theoretical perspective, we analyze the time and space complexity of graph-based NNS,
assuming that the data elements are distributed uniformly within a ball of radius R in the hyperbolic
space of curvature -1. In this regard, we compare the obtained results with Prokhorenkova &
Shekhovtsov (2020) who studied graph-based algorithms for uniform distribution of elements over
a d-dimensional sphere. In the dense setting (assuming that d log n and R does not grow too
fast), we derive the time and space complexity using the fact that locally hyperbolic graph-based
NNS behaves similarly to the Euclidean case. However, globally, graph-based search in hyperbolic
spaces requires fewer search steps. Thus, hyperbolic NNS can be even more efficient compared to its
Euclidean counterpart. In the sparse setting (d log n), we show that hyperbolic NNS has similar
complexity to the Euclidean case.
From a practical perspective, we first illustrate our theoretical findings via synthetic experiments
on uniform data. Then, we analyze graph-based hyperbolic NNS in practical applications where
our theoretical results cannot be directly applied. For this, we consider Poincare GloVe word em-
beddings (Tifrea et al., 2019) that are known to outperform the standard representation for tasks of
similarity, analogy, and hypernymy detection. We compare graph-based NNS with existing base-
lines (Wu & Charikar, 2020) and show that graph-based approaches have significantly better per-
formance. We also compare the performance of graph-based NNS for Euclidean and hyperbolic
representations of the same sets of elements and observe that hyperbolic NNS can be more efficient.
2	Preliminaries
Graph-based nearest neighbor search Graph-based algorithms for NNS are known to outper-
form other approaches in many large-scale applications (AumUller et al., 2019). The main idea is to
construct a proximity graph at the pre-processing stage and traverse this graph towards the query q
at query time. The best-first search is usually used for traversal: we maintain a priority queue, and at
each step, we pick the closest to q candidate from this queue, measure distances between its neigh-
bors and q, and add the neighbors to the queue. Many algorithms are based on this idea (Baranchuk
et al., 2019; Fu et al., 2019; Iwasaki & Miyazaki, 2018; Malkov & Yashunin, 2018).
Unfortunately, despite their success in practice, graph-based approaches do not have solid theoretical
guarantees. This problem was first approached by Laarhoven (2018) who provided time-space
2
Published as a conference paper at ICLR 2022
trade-offs for approximate nearest neighbor (ANN) search in sparse datasets (d log n) uniformly
distributed on a d-dimensional Euclidean sphere. This work considered nearest neighbor graphs,
where each point is connected to its neighborhood lying within a ball of a fixed radius, and assumed
greedy graph traversal. Prokhorenkova & Shekhovtsov (2020) extended this result: for the uniform
distribution, the authors consider both sparse (d log n) and dense (d log n) regimes, and they
analyze the effect of two improvements commonly used in practice: replacing greedy search by
best-first (or beam) search and adding long edges to speed up graph traversal at early stages.
In this paper, we aim at understanding graph-based NNS in hyperbolic spaces. For this, we adopt a
similar setup to Prokhorenkova & Shekhovtsov (2020), but instead of a uniform distribution over a
sphere, we assume a uniform distribution within a ball of a certain radius in the hyperbolic space.
The obtained time and space complexity heavily relies on this radius: for small values, the obtained
results are similar to the spherical case, while for larger values, the exponential growth of hyperbolic
volumes allows us to obtain even faster convergence in the dense regime.
Hyperbolic geometry Now, let us discuss the basics of hyperbolic geometry. Let Rd,1 be the
(d + 1)-dimensional Minkowski space, i.e., the real vector space Rd+1 equipped with the inner
product of signature (d, 1):
hx, yih = -x0y0 + x1y1 + . . . + xdyd.
(1)
The vector model of the d-dimensional hyperbolic Lobachevsky space Hd is the connected compo-
nent of the standard two-sheeted hyperboloid contained in the future light cone: Hd = {x ∈ Rd,1 |
hx, xih = -1, x0 > 0}. The hyperbolic metric is given by cosh ρ(x, y) = -hx, yih. From the defi-
nition of the hyperbolic space, We see that xo = ,1+ x2 + ... + Xd. The volume element in Hd
is JxJdx 2 (Ratcliffe, 2019).
1+x21+...+x2d
Hyperbolic space is knoWn to be useful for data representation in various domains. It is especially
convenient for embedding hierarchical data (Nickel & Kiela, 2017). The reason is that the volume
of a ball in the hyperbolic space groWs exponentially With its radius, in contrast to the Euclidean
space, Where the groWth is polynomial. Formally, let B(x, r) be a closed hyperbolic ball centered at
a point x ∈ Hd of radius r > 0. Then, its volume is
VolHd (B(x, r))
Vol(Sd-1 )	r sinhd-1 (t) dt,
(2)
Where Sd-1 is the unit (d-1)-sphere and Vol(M) denotes the full volume of a Riemannian manifold
M. That is, Vol(Sd-1) = 2πd/2/Γ(d∕2). For r》log d, we have
VolHd(B(x, r))〜Vol(SdT) er(d-i) Θ(d-12-d).	(3)
The main term here is er(d-1), so the ball volume VolHd (B(x, r)) grows exponentially when r is
large. However, for small hyperbolic balls (as r → 0), we have sinh(r)〜r. Hence, the volume of
a ball (2) behaves similarly to the Euclidean case. In Section 4.1, we provide a formal general result
that shows that not only the above argument is correct, but also the volume of a measurable subset
of a hyperbolic ball behaves asymptotically exactly like its Euclidean analog.
Nearest neighbor search in hyperbolic space While there are many NNS algorithms for `p
spaces, little is known about efficient hyperbolic analogs. Krauthgamer & Lee (2006) proved the
existence ofan approximate near neighbor algorithm for general geodesic spaces of hyperbolicity δ.
From a theoretical perspective, the bound is given in terms of the doubling dimension. For uniform
datasets, we may conclude that the obtained time complexity is 2O(d) log2 n, while the constant
in the exponent is not given. In contrast, we specify this term as we focus on d → ∞. Another
difference is that the algorithm of Krauthgamer & Lee (2006) requires O(n2 ) storage that is usu-
ally infeasible in practice. In contrast, graph-based algorithms require n Md for some constant M if
d log n. Finally, while the theoretical analysis is given in a very general setup, no implementation
is provided and extracting an efficient implementation from the proof is challenging.
In a recent paper, Wu & Charikar (2020) reduce the problem of hyperbolic NNS to Euclidean NNS.
Namely, assume that we have an efficient Euclidean algorithm and a dataset is represented in the
3
Published as a conference paper at ICLR 2022
Poincare ball model. Then, We can apply the algorithm to the Euclidean representation, but this may
not give the correct nearest neighbor. Wu & Charikar (2020) propose several solutions allowing
for obtaining either exact or approximate nearest neighbors. In all of them, the Euclidean NNS
algorithm should be applied several times. One approach is to repeatedly apply the algorithm to
the Whole dataset, but after each run, appropriately change the query point to get better hyperbolic
neighbors at subsequent iterations. The time complexity of this algorithm is k + 1 times larger than
the Euclidean search if the Euclidean nearest neighbor for the query is the k-th nearest hyperbolic
neighbor. Another approach is to splits the ball into thin annuli and performs the approximate
Euclidean NNS for each of them separately. From a theoretical perspective, it is hard to compare our
results since Wu & Charikar (2020) do not consider any particular algorithm. We cannot apply this
general result to graph-based algorithms since existing guarantees for the Euclidean space assume a
uniform distribution. However, if we consider the uniform distribution in the Poincare model of the
hyperbolic space, the corresponding Euclidean distribution is far from uniform, i.e., the guarantees
cannot be transferred. From a practical perspective, we compare graph-based algorithms with the
method of Wu & Charikar (2020) and show that graph-based NNS is more efficient.
3	Setup and notation
The current paper aims at analyzing how hyperbolic geometry of the underlying space affects the
time and space complexity of NNS. In this regard, we compare our results with Prokhorenkova &
Shekhovtsov (2020), so we use a similar setup and notation for convenience.
To make the analysis feasible, Laarhoven (2018) and Prokhorenkova & Shekhovtsov (2020) assume
that the dataset is uniformly distributed on a sphere. The uniformity assumption is fairly strong.
In Prokhorenkova & Shekhovtsov (2020), this setup was partially motivated by techniques allowing
to make a dataset more uniform while approximately preserving the distances (Sablayrolles et al.,
2018), which are helpful in some practical applications. We additionally note that in hyperbolic
spaces, the uniform distribution of nodes within a ball leads to structures that are similar to real
ones: if we consider a geometric graph on such nodes, we obtain the power-law degree distribution,
small diameter, and high clustering coefficient (Krioukov et al., 2010), which further supports the
meaningfulness of the uniform distribution in our scenario. The compactness of the sphere simplifies
the analysis in previous work as it allows for avoiding boundary effects. However, we cannot make
this assumption in our setup, as a sphere in the hyperbolic space is isometric to a sphere in the
Euclidean space. Hence, to use the properties of hyperbolic geometry, we assume that the dataset is
uniformly distributed within a ball of some radius in the hyperbolic space.
Formally, consider a ball B(R) ⊂ Hd of radius R > 0. Assume that the dataset is a finite collection
of points D = {x1, . . . , xn} ⊂ Hd, where xi ∈ D are i.i.d. random vectors uniformly distributed in
B(R). For a given query q ∈ B(R), let X ∈ D be its nearest neighbor. Then, the goal of the exact
NNS is to find X. For completeness, following Laarhoven (2018); Prokhorenkova & Shekhovtsov
(2020), we also consider c, r-approximate near neighbor problem (c, r-ANN). In this scenario, it is
assumed that q is planted uniformly within a fixed distance r > 0 from its nearest neighbor X and
the goal is to return any X0 such that ρ(q, X0) ≤ cr for a fixed value c > 1 (Chen et al., 2018).
Formally, for the exact nearest neighbor search, we also assume that q is planted uniformly within a
distance r from some element X. However, we do not restrict the radius r (the only restriction is that
X should be the nearest neighbor for q).
We follow the standard assumption that the dimension d = d(n) is an increasing function ofn (Chen
et al., 2018). We mostly focus on the so-called dense regime, where d log n. For the sparse
regime (d log n), we formally show that graph-based NNS in the hyperbolic space has similar
complexity to NNS on the sphere.
Graph-based approaches usually imply constructing a nearest neighbor graph (or its approximation),
where each element is connected to its k nearest neighbors. As we deal with uniformly distributed
datasets, we assume that each node X is connected to all nodes y ata distance ρ(x, y) ≤ ρ* for some
ρ* chosen at the preprocessing stage. This is more convenient for the analysis, while similar results
can be obtained for k-NN graphs, see Appendix D. Hence, at the preprocessing stage, we construct a
similarity graph using ρ*. Next, we receive a query q, sample a random node X ∈D, and at each step
of the graph-based greedy descent, we measure the distances between the neighbors of the current
node and q and move to the closest neighbor while we can make such greedy steps.
4
Published as a conference paper at ICLR 2022
4	Convergence and complexity of graph-based NNS
4.1	Local properties of hyperbolic space
Further in this section, we give formal statements regarding the convergence and complexity of
greedy graph-based algorithms. Convergence depends on the probability of making a step towards
the query from a given element. This probability, in turn, depends on the volumes of some balls’
intersections. As our proof shows, only ‘local’ properties are important for convergence for the
dense regime. Let us show that under some conditions, the local properties in the hyperbolic space
are similar to those in the Euclidean space.
Formally, let z = (1, 0, . . . , 0) and let TzHd = {x ∈ Rd,1 | x0 = 1} ' Rd be the tangent space.
Let C be the central projection C: Hd → TzHd ` Rd from the origin o = (0,..., 0). The following
lemma holds.
Lemma 1. Suppose that U ⊂ Hd is a measurable set such that U ⊂ B(z,r). If r 《 √ and
d → ∞, then we have VolHd (U) = VolEd (C(U))(1 — O (dr2)).
This result allows us to apply the bounds on volumes of Euclidean balls intersections to such vol-
umes in the hyperbolic space for small radii.
4.2	Intuition behind the choice of parameters
Our theoretical analysis is asymptotic, meaning that the number of elements n tends to infinity.
Hence, we assume that the dimension d = d(n) and the ball radius R = R(n) depend on n. This
section discusses our restrictions on d(n) and R(n) and how they affect the analysis. Here, we do
not aim at giving mathematically rigorous statements but rather explain underlying intuitions. The
formal results can be found in Section 4.3.
Assuming that the dataset is uniformly distributed on a unit sphere, there are two principally different
regimes: dense with d log n and sparse with d log n (Prokhorenkova & Shekhovtsov, 2020).
In the dense regime on a sphere, the distance from any element to its nearest neighbor tends to zero
as n grows. In this case, the greedy algorithm reaches the nearest neighbor with probability 1 — o(1)
if graph degree is Md for M > √2. In the sparse regime, informally speaking, all nodes are at a
spherical distance about n/2 from each other, making the NNS much more challenging. In this case,
the graph degree has to grow as nψ for some 夕 > 0 in order to solve the approximate near neighbor
problem (Prokhorenkova & Shekhovtsov, 2020).
In this paper, we mainly focus on the dense regime with d log n. As discussed in (Prokhorenkova
& Shekhovtsov, 2020), this setup is more realistic for uniform data. Indeed, while in practice
datasets are often embedded into spaces of large dimension, they usually have much smaller in-
trinsic dimension d0 d (Beygelzimer et al., 2006; Lin & Zhao, 2019). It is also important to
note that hyperbolic spaces are known to be beneficial when the dimension is small (Gu et al., 2019;
Shevkunov & Prokhorenkova, 2021). For completeness of the study, we also analyze the sparse
regime and theoretically show that under some conditions on R graph-based NNS in the hyperbolic
space has similar time and space complexity to NNS on the sphere.
We assume that the dataset is uniformly distributed within a ball B = B(R) in the hyperbolic space.
Changing the radius R is equivalent to changing the curvature of the space (Petersen, 2016). So, we
may either assume that the curvature C = —1 is fixed while the radius of the ball R = R(n) may
change or assume that the radius is fixed while the curvature C = C(n) changes. We further assume
that C = —1 is fixed. Thus, we have three parameters: the number of elements n, the dimension
d = d(n), and the radius R = R(n).
Denote by Vd(R) the volume of B, i.e., Vd(R) := VolHd (B(R)) and by vd(R) the normalized
volume, i.e., vd(R) := Vd(R)/Vol(Sd-1). Let us analyze (approximately) the distance α from an
element to its nearest neighbor. Intuitively, we should have
VθlHdf≡ ≈ 1, VolHd(B(α)) ≈ Vd≡ .	(4)
VolHd (B (R))	n	n
5
Published as a conference paper at ICLR 2022
Assume for now that ɑ《1∕√d (approximately Euclidean setup, see Section 4.1). In this case,
VolHd (Bg)) ~ VolEd (Bg))
Vol(Sd-1)αd
d
Therefore, from (4),
ɑd ≈	Vd(R d
n Vol(SdT)
Vd(R) d
n
Let us find an upper bound on the radius R such that the condition α《1/√d is satisfied. Note that
Vd(R) = O (eR(dT) ∙ 2-d ∙ d-v) (see Appendix B). Thus, we need
vd(R) d 1/d	1
n~^)
eR(d-1) 2-d 1/d 1
-n-)	y
eR ≪ n1/(dT) ∙ d-1/2 .
Based on that, we further assume that in the dense regime, eR ≪ n1/(d-1) ∙ d-1/2. In this case,
the distance to the nearest neighbor is α = o(1/√d), and locally the graph-based search behaves
similarly to the Euclidean case. We empirically show that our assumption on R is reasonable: larger
values of R lead to less efficient NNS.
4.3	Main results
In this section, we formally analyze the performance of graph-based NNS for uniform datasets in
the hyperbolic space. As discussed above, to construct the nearest neighbor graph, we connect each
point to its neighborhood located within a ball of a fixed radius. In the dense regime, for any constant
M > 1, let G(M) be a graph obtained by connecting xi and xj iff
ρ(Xi,Xj) ≤ M (")：
With this condition, we get about Md neighbors for each node. The following theorem holds.
Theorem 1. Assume that d = d(n) is such that log log n ≪ d(n) ≪ logn and R = R(n) is
such that eR ≪ nd-1 ∙ d-1/2 . For the exact NNS, let M be a constant s.t. M > √2. Then,
with probability 1 - o(1), G(M)-based NNS finds the exact nearest neighbor. For c, r-ANN with
some constant c > 1, let M be a constant s.t. M > J34-1. Then, with probability 1 一 o(1),
G(M)-based NNS solves c, r-ANN for any r.
In both cases, the time complexity is O (Md ∙ n1/d ∙ R ∙ (vd(R))-1/d);
the space complexity is O (Md ∙ n ∙ logn.
The formal proof of this theorem is given in Appendices A-C, and in Appendix D we prove a similar
theorem for k-NN graphs with k = [Md].
Theorem 1	covers several different regimes. First, assume that R ≪ 1∕√d. In this case, the
geometry is approximately Euclidean. We have Vd(R)〜Rd/d, so the following corollary holds.
Corollary 1. If R = R(n) ≪ 1∕√d, then in Theorem 1 the time complexity is O (Md ∙ n1/d).
As expected, this result is similar to the corresponding theorem in Prokhorenkova & Shekhovtsov
(2020). However, as R becomes larger, time complexity changes. For instance, if R log d, then
we have Vd(R)〜eR(d-1) Θ(d-12-d). Thus, we get the following corollary.
Corollary 2. If R = R(n)	log d, then in Theorem 1 we obtain the time complexity
O (Md ∙ n1/d ∙ R ∙ e-R(dτ"d).
From this corollary we see that the time complexity decreases with the radius R. Thus, to make the
complexity smaller, we should take larger values of R. According to Theorem 1, we can take R
such that eR = nd-1 /(√d夕)，where 夕=夕(n) → ∞. We obtain the following corollary.
6
Published as a conference paper at ICLR 2022
Corollary 3. If for an arbitrary 夕=夕(n) → ∞ we have eR
nd--1 /(√dφ) and R》log d, then
in Theorem 1 we get the time complexity O (Md ∙ log n ∙ d-1/2 ∙夕).
Thus, in the hyperbolic space, the number of steps can be reduced from n1/d to about log n. This
shows that compared to the Euclidean case, NNS over nearest neighbor graphs in the hyperbolic
space can be more efficient (under the conditions considered in this paper). Intuitively, the difference
between Euclidean and hyperbolic regimes is due to the fact that the volume of a hyperbolic ball
grows exponentially with its radius. As a consequence, we need fewer steps to reach any element
from, e.g., the origin of the ball. Note that in the Euclidean space, we can also reduce the number
of steps to log n via adding long edges (Prokhorenkova & Shekhovtsov, 2020). In contrast, in
hyperbolic space, this can be achieved without additional tricks. Our experiments on synthetic
datasets confirm that without long edges, NNS is much more efficient in the hyperbolic space when
dimension d is small.
Finally, let us discuss the sparse regime with d log n. Here we show that the elements are tightly
concentrated near the boundary of the ball and thus the convergence guarantees resemble those on
the sphere. In this case, we construct the graph G(M ) by connecting such elements xi and xj that
coshρ(xi, Xj) ≤ cosh2 R — sinh2 R∙ J2Mlogn.
To formulate the result, we introduce the following
notation: for any constant c> 1 let α0 := cosh2 R-coShsarhcoRh(Cosh2 R)/C).
Theorem 2.	Assume that d log n and R	log d. Let c > 1 and letM be any constant such
2
that M < α2++1. Then, with probability 1 — o(1), G(M)-based NNS solves c, r-ANN (for any r);
the time complexity of this procedure is Θ n1-M+o(1); the space complexity is Θ n2-M+o(1).
The proof is given in Appendix E. This theorem shows that the complexity of hyperbolic NNS for
sparse datasets is similar to that on a sphere. However, for large d, the calculation error in the
hyperbolic space may become significant even for moderate values of R. We also remark that the
condition R log d is chosen to simplify the proof using the approximation (3), but this restriction
does not seem to be necessary for the theorem to hold.
5	Experiments
In this section, we first illustrate our theoretical results from Section 4.3 on synthetic datasets uni-
formly distributed within balls of different radii in the hyperbolic space. Then, we compare graph-
based approaches with the algorithm proposed by WU & Charikar(2020) on Poincare GloVe word
embeddings (Tifrea et al., 2019). Finally, We compare the performance on Euclidean and POincare
GloVe datasets to show that the efficiency can be even better for hyperbolic representations. Addi-
tional experiments can be found in Appendix; the code supplements the submission.
5.1	Illustrating theoretical results on synthetic uniform datasets
In this section, we illustrate Theorem 1 and our corollary that for simple nearest-neighbor graphs and
sufficiently small dimension d, graph-based NNS is more efficient in the hyperbolic space. For this,
we follow the setup of Prokhorenkova & Shekhovtsov (2020) and perform a synthetic experiment.
We generate datasets uniformly distributed in different spaces: 1) on a sphere (Prokhorenkova &
Shekhovtsov, 2020), 2) within a Euclidean ball, 3) within a ball of radius R in the hyperbolic space
of curvature —1. For all spaces, we fix n = 106 and vary the dimension d ∈ {2, 4, 6}.
For the hyperbolic space, we consider different values of R which is equivalent to varying curva-
ture. Recall that Theorem 1 requires eR《 n1/(d-1)/Vd. Thus, roughly speaking, we obtain the
following upper bound on R: Rmax = lo-n — log √d. For d = 2, we get Rmax ≈ 13.5; for d = 4,
we get Rmax ≈ 3.9; while for d = 6 we have Rmax ≈ 1.9. Thus, even for d = 6, the bound on the
radius is sufficiently small and the obtained geometry becomes similar to Euclidean (if we consider
the theoretically analyzed parameter range). Thus, we do not consider d > 6 in this experiment.
First, we analyze the performance of greedy search over nearest-neighbor graphs; this setup cor-
responds to our theoretical analysis. To get the complexity-vs-accuracy curves, we vary the graph
7
Published as a conference paper at ICLR 2022
DCS
Figure 1: Greedy search over NN graphs without (top row) and with (bottom row) long edges
degree. As a primary measure of the search accuracy, we consider Recall@1. The search com-
plexity is the average number of distance computations (DCS) per query. In Figure 1 we see that
spherical and Euclidean spaces are similar. Recall that the Euclidean space can be thought of as the
hyperbolic one with R → 0. Then, as R increases, the performance of NNS in the hyperbolic space
improves. However, after some point, larger values of R reduce the quality of NNS. It is worth
noting that this threshold agrees well with the dynamic of Rmax (while being smaller, as expected).
Then, we analyze what happens if we add long edges known to be essential for NNS in spherical
and Euclidean spaces when the dimension d is small. To incorporate long edges into NN graphs, we
follow the procedure described in Prokhorenkova & Shekhovtsov (2020). As expected, long edges
significantly boost the performance of NNS in spherical and Euclidean spaces. Note that they also
improve performance in hyperbolic spaces, but the boost is smaller. As a result, the efficiency of
NNS in all spaces becomes similar.
We also conduct similar experiments with greedy search replaced by the best-first search, which is a
standard technique for graph-based NNS performance improvement. The conclusions are the same;
we include the figures in Appendix F.
5.2	Comparison with baseline
Let us analyze the performance of graph-based NNS in a more realistic setup. For this, we col-
lect the largest publicly available Poincare GloVe vocabulary1 of 189, 533 unique tokens. Poincare
GloVe was shown to outperform the Euclidean representation for tasks of similarity, analogy, and
hypernymy detection (Tifrea et al., 2019). The dimensionality of the embeddings is 100. The set
of tokens is randomly split into the base set of 180K elements and the query set of the remaining
elements for evaluation. Note that the dataset is not uniform and the dimensionality is sufficiently
large, so our theoretical analysis cannot be directly applied.
On the base set, we construct the HNSW graph (Malkov & Yashunin, 2018) that is known to achieve
state-of-the-art results on many datasets. As for the search algorithm, we perform the best-first
search. The parameter efSearch determines the stopping criteria of the best-first search and allows
us to vary the complexity versus accuracy tradeoff. For the baseline, the only practical algorithm
we are aware of is proposed by Wu & Charikar (2020). The algorithm is called Spherical Shell: it
is suggested to split the Poincare ball into thin annuli and perform approximate Euclidean NNS (the
authors use LSH) for some of them separately. Unfortunately, the implementation is not provided
1https://github.com/alex-tifrea/poincare_glove
8
Published as a conference paper at ICLR 2022
by the authors, so we use our implementation. To make the comparison fair, we count the number
of distance computations instead of the actual time, as the latter may depend on the efficiency of our
implementation. The algorithm details and parameter tuning are discussed in Appendix F.
DCS
Figure 2: HNSW vs Spherical Shell on
Poincare GloVe embeddings
Figure 3: Efficiency of HNSW in hyperbolic
space vs Euclidean space
The results are shown in Figure 2. We see that HNSW significantly outperforms the baseline. This
confirms that applying an algorithm directly to the hyperbolic space is more efficient than reducing
the problem to Euclidean NNS. Some additional experiments can be found in Appendix: we evaluate
another graph-based algorithm and consider more datasets.
5.3	Euclidean VS POINCARE GLOVE
We also compare the efficiency of graph-based methods for Euclidean (Pennington et al., 2014) and
Poincare (Tifrea et al., 2019) GloVe embeddings of the same tokens. Note that the results cannot be
compared directly: GloVe and Poincare GloVe have different geometry, so nearest neighbors for the
same queries may differ. Thus, we only aim to show that switching from Euclidean to hyperbolic
embeddings does not make NNS significantly less efficient. Figure 3 shows that HNSW built on
embeddings in the hyperbolic space is even more efficient than HNSW over Euclidean embeddings.2
This further confirms the applicability of graph-based methods to hyperbolic representations.
6 Conclusion
This paper conducts a theoretical and empirical analysis of graph-based nearest neighbor search in
hyperbolic spaces. First, we theoretically analyze the performance of greedy search over nearest-
neighbor graphs assuming that the dataset is uniformly distributed over a ball of radius R in the
hyperbolic space. Assuming the dense regime (d log n) and that the absolute value of the curva-
ture is not too large, we show that locally graph-based exploration is similar to the Euclidean case.
However, globally, hyperbolic geometry allows for reducing the number of steps significantly (for
plain NN graphs). For sparse regime (d log n), we show that NNS has similar complexity in
Euclidean and hyperbolic spaces.
From a practical perspective, we show that graph-based algorithms are superior to other existing
methods. Also, compared to the Euclidean NNS, switching to hyperbolic space does not lead to
significantly increased complexity and may even reduce the query time. We hope that our research
will attract more attention to theoretical and empirical analysis of NNS in hyperbolic spaces and, in
particular, to graph-based methods.
Our work is the first step towards understanding graph-based methods for hyperbolic representa-
tions, and there are plenty of directions for future research. In particular, the assumptions of Theo-
rem 1 imply locally Euclidean behavior of the algorithm. It would be useful to extend this theorem
beyond locally Euclidean neighborhoods by relaxing the upper bound on R. Of course, it is also
important to relax the uniformity assumption, but this problem is still to be addressed for a more
simple Euclidean case.
2Note that computing the hyperbolic distance has the same complexity as computing the inner product, as
follows from Equation (1).
9
Published as a conference paper at ICLR 2022
Acknowledgments
A part of work of Nikolay Bogachev was done during his postdoc at Skoltech.
References
Martin Aumuller, Erik Bernhardsson, and Alexander Faithfull. Ann-benchmarks: A benchmarking
tool for approximate nearest neighbor algorithms. Information Systems, 2019.
Dmitry Baranchuk, Dmitry Persiyanov, Anton Sinitsin, and Artem Babenko. Learning to route in
similarity graphs. arXiv preprint arXiv:1905.10987, 2019.
Jon Louis Bentley. Multidimensional binary search trees used for associative searching. Communi-
CationsoftheACM ,18(9):509-517,1975.
Alina Beygelzimer, Sham Kakade, and John Langford. Cover trees for nearest neighbor. In Pro-
ceedings of the 23rd international conference on Machine learning, pp. 97-104, 2006.
Christopher M Bishop. Pattern recognition and machine learning. Springer, 2006.
Benjamin Paul Chamberlain, James Clough, and Marc Peter Deisenroth. Neural embeddings of
graphs in hyperbolic space. arXiv preprint arXiv:1705.10359, 2017.
Ines Chami, Adva Wolf, Da-Cheng Juan, Frederic Sala, SUjith Ravi, and Christopher Re. LoW-
dimensional hyperbolic knowledge graph embeddings. In Proceedings of the 58th Annual Meeting
of the Association for Computational Linguistics, pp. 6901-6914, 2020.
George H Chen, Devavrat Shah, et al. Explaining the success of nearest neighbor methods in pre-
diction. Foundations and Trends® in Machine Learning, 10(5-6):337-588, 2018.
Sanjoy Dasgupta and Yoav Freund. Random projection trees and loW dimensional manifolds. In
STOC, volume 8, pp. 537-546. Citeseer, 2008.
Sanjoy Dasgupta and Kaushik Sinha. Randomized partition trees for nearest neighbor search. Algo-
rithmica, 72(1):237-263, 2015.
Cong Fu, Chao Xiang, Changxu Wang, and Deng Cai. Fast approximate nearest neighbor search
With the navigating spreading-out graph. Proceedings of the VLDB Endowment, 12(5):461-474,
2019.
Albert Gu, Frederic Sala, Beliz Gunel, and Christopher Re. Learning mixed-curvature representa-
tions in product spaces. In International Conference on Learning Representations, 2019.
Piotr Indyk and Rajeev MotWani. Approximate nearest neighbors: toWards removing the curse of
dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing,
pp. 604-613. ACM, 1998.
Masajiro IWasaki and Daisuke Miyazaki. Optimization of indexing based on k-nearest neighbor
graph for proximity search in high-dimensional data. arXiv preprint arXiv:1810.07355, 2018.
Omid Keivani and Kaushik Sinha. Improved nearest neighbor search using auxiliary information
and priority functions. In International Conference on Machine Learning, pp. 2578-2586, 2018.
Valentin Khrulkov, Leyla Mirvakhabova, Evgeniya Ustinova, Ivan Oseledets, and Victor Lempitsky.
Hyperbolic image embeddings. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 6418-6428, 2020.
Robert Krauthgamer and James R Lee. Algorithms on negatively curved spaces. In 2006 47th
Annual IEEE Symposium on Foundations of Computer Science (FOCS’06), pp. 119-132. IEEE,
2006.
Dmitri Krioukov, Fragkiskos Papadopoulos, Maksim Kitsak, Amin Vahdat, and Marian BogUna.
Hyperbolic geometry of complex netWorks. Physical Review E, 82(3):036106, 2010.
10
Published as a conference paper at ICLR 2022
Thijs Laarhoven. Graph-based time-space trade-offs for approximate near neighbors. In 34th In-
ternational Symposium on Computational Geometry (SoCG 2018). Schloss Dagstuhl-Leibniz-
Zentrum fuer Informatik, 2018.
Peng-Cheng Lin and Wan-Lei Zhao. Graph based nearest neighbor search: Promises and failures.
arXiv preprint arXiv:1904.02077, 2019.
Yury Malkov, Alexander Ponomarenko, Andrey Logvinov, and Vladimir Krylov. Approximate near-
est neighbor algorithm based on navigable small world graphs. Information Systems, 45:61-68,
2014.
Yury A Malkov and Dmitry A Yashunin. Efficient and robust approximate nearest neighbor search
using hierarchical navigable small world graphs. IEEE transactions on pattern analysis and
machine intelligence, 2018.
Alexander May and Ilya Ozerov. On computing nearest neighbors with applications to decoding
of binary linear codes. In Annual International Conference on the Theory and Applications of
Cryptographic Techniques, pp. 203-228. Springer, 2015.
Leyla Mirvakhabova, Evgeny Frolov, Valentin Khrulkov, Ivan Oseledets, and Alexander Tuzhilin.
Performance of hyperbolic geometry models on top-n recommendation tasks. In Fourteenth ACM
Conference on Recommender Systems, pp. 527-532, 2020.
Maximillian Nickel and DoUWe Kiela. Poincare embeddings for learning hierarchical representa-
tions. In Advances in neural information processing systems, pp. 6338-6347, 2017.
Maximillian Nickel and DoUwe Kiela. Learning continUoUs hierarchies in the lorentz model of
hyperbolic geometry. In International Conference on Machine Learning, pp. 3776-3785, 2018.
Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 1532-1543, 2014.
P. Petersen. Riemannian Geometry (3rd edition). Springer, 2016.
LiUdmila Prokhorenkova and Aleksandr Shekhovtsov. Graph-based nearest neighbor search: From
practice to theory. In International Conference on Machine Learning, pp. 7803-7813. PMLR,
2020.
John G. Ratcliffe. Foundations of hyperbolic manifolds (3rd edition). Springer, 2019.
Alexandre Sablayrolles, MatthijS Douze, Cordelia Schmid, and Herve Jegou. Spreading vectors for
similarity search. arXiv preprint arXiv:1806.03198, 2018.
Gregory Shakhnarovich, Trevor Darrell, and Piotr Indyk. Nearest-neighbor methods in learning and
vision: theory and practice (neural information processing). The MIT press, 2006.
Kirill Shevkunov and Liudmila Prokhorenkova. Overlapping spaces for compact graph representa-
tions. Advances in Neural Information Processing Systems, 34, 2021.
Alexandru Tifrea, Gary Becigneul, and Octavian-Eugen Ganea. Poincare glove: Hyperbolic word
embeddings. In International Conference on Learning Representations, 2019.
Xian Wu and Moses Charikar. Nearest neighbor search for hyperbolic embeddings. arXiv preprint
arXiv:2009.00836, 2020.
11
Published as a conference paper at ICLR 2022
A	Auxiliary geometrical results
In this section, we prove some auxiliary geometrical results that will be useful throughout the proofs.
First, we analyze the properties of Euclidean balls and their intersections. Then, we obtain similar
results for the hyperbolic space if the ball’s radii are sufficiently small.
A.1 Euclidean balls and their intersections
For a ball B = B(1) of unit radius in a d-dimensional Euclidean space, its volume is
d
∏ 2
VolEd (B) =
γ (2 + A
In this section, we estimate volumes of spherical caps and then use the obtained bounds to estimate
the volumes of balls’ intersections.
Let B(x, α) be a ball of radius α centered at x, and Ca (x, z, h) ⊆ B(x, α) be a d-dimensional
spherical cap of height h centered at z ∈ Sα(x), i.e., Cα(x, z, h) = {y ∈ B(x, α) : hy - x, z - xi ≥
α(α - h)}. Let Cα(h) := VolEd (Cα(x, z, h)) be the volume of this spherical cap.
Definition 1. Let qa (h) :=(1 -(1 - h)2) / be the “rel-
ative radius” of a spherical cap (see qα (h) in the figure).
Lemma 2. The volume Cα(h) ofa spherical cap Cα(x, z, h)
can be estimated as follows:
Θ(d-1) ≤
Ca (h)
VolEd-i (B) ∙ αd ∙ qa(h)d+1
≤ Θ(d-2).
Proof. For brevity, let q = qa(h). Let Y = arccos(1 — h∕α), 0 ≤ Y ≤ ∏. The volume of a spherical
cap of height h is known to be:
Ca(h) =VOIEd-I (B) ∙ αd
γ
sind(t) dt.
0
First, rewrite the integral:
γ
sind(t) dt = |x = sin2 t|
0
q2
1	d-“	、i 7
=-x H (1 — X)-2 dx
0
1
=|x = q21| = 1 qd+1 Z td^-1 (1 — q21)-2 dt.
0
To prove the lemma, it remains to show that
1
Θ(d-1) ≤ /1d-1 (1 — q21)-2
0
dt ≤ Θ (d- 2
12
Published as a conference paper at ICLR 2022
Indeed,
11
Θ(d-1) = /td-1 dt ≤/td-1(1 - q2t)-1 dt
00
1
≤ Zt d-1 (I- t)- 1 dt = B (d+1, 1)=Θ (d- 2 ),
0
where B(∙, ∙) denotes the beta function. This concludes the proof.
□
Let us now estimate the volume of the intersection of two Euclidean balls. Let Wxi,xj (α, β) denote
the intersection of B(xi,α) and B(xj,β), i.e., Wxi,xj(α,β) = {y : ρ(xi,y) ≤ α, ρ(xj, y) ≤ β}.
Let s := ρ(xi, xj) be the distance between the centers and W (α, β, s) := VolEd Wxi,xj (α, β)
denote the volume of the intersection. The following lemma estimates the value W(α, β, s) under
various conditions on α, β , s.
Lemma 3. W.l.o.g., assume that α ≤ β. Define A := g+β+s)(α+β-4Ss+α-β)(s+βY.
1.	If α+ β < s, then W(α, β, s) = 0.
2.	If S 一 α ≤ β < √α2 + s2, then
Θ (d-1) ≤
W(α, β, S) ∙ αβ
VolEd-I (B) ∙ (α + β) ∙ Ad+1
≤Θ
3.	If √α2 + s2 ≤ β < s + α, then
1 ≤ W(α,β,s)	≤ ι
2 — VolEd (B) ∙ αd —
VolEd (Bm))- wiα,β,s) ≤ θ
VolEd-I (B) ∙ A + ∙ α-1 —
4.	If S + α ≤ β, then
W(α,β,s) = VolEd (B) ∙ αd.
(5)
(6)
(7)
(8)
Proof. Let hα ：= β⅛=αf, hβ ：= α⅛=βf, eα ：= (α+s2)2-β2.
1.	If α + β < S, then the balls do not intersect and thus W(α, β, S) = 0, see Figure 4a.
2.	If s - α ≤ β < √α2 + s2, then, according to Figure 4b,
W(α, β, S) = Cα(hα) + Cβ(hβ) .
By Lemma 2, we have
Θ(d-1) ≤
W (α,β,s)
VolEd-I (B) ∙ (αdqα+1 (hα)+ βdqd+1 (hg))
≤Θ
13
Published as a conference paper at ICLR 2022
VolEd (B(α))∕2 ≤ W(α,β,s) ≤ VolEd(B(α))
VolEd(B(α))-W(α,β,s) =Cα(hα)-Cβ(hβ)
(d) s + α ≤ β:
W(α, β, s) = VolEd (B(α))
Figure 4: Different cases in the proof of Lemma 3
14
Published as a conference paper at ICLR 2022
Substituting qα , qβ , hα , hβ , we obtain that
d + 1
ɑd ( 02 -(；2- hα 产)F + βd
d+1
β2 - (β - hβ)2 λ F
β2
—(hɑ(2α - ha)
α
+ (h (hβ(2β - hβ)) 2
β
1 ( (β2 -(S - α)2)((s + α)2 - β2)、 α I	4s2	,	d+1	d+1 lɪ +1 ( (α2-(s- β)2)((s + β)2- α2) ) F C	1、4 d+1 =(α + β) A 2 .
3. If √α2 + s2 ≤ β < s + a, then (See Figure 4c)
VolEd2B(α" ≤ W(α,β,s) ≤ VolEd(B(α)),
Also, according to Figure 4c,
VolEd(B(α)) -W(α,β,s) =Cα(ehα) -Cβ(hβ) ≤ Cα
By Lemma 2, we have
VolEd (B) ∙ ad - W(α,β, S) ≤ θ (d- 1 ∖
VolEd-I (B) ∙αd ∙ qα+1(eα) —	1	. '
Substituting qα and hα, we get
0dqi+1(hα) = αd α22 - (α2- hα)2
α	α2
d + 1
~1Γ~
d+1
1 (((S + α)2 - β2)(β2 - (s - α)2八 F
4s2
1 Zt d+1
-AF .
α
4. If s + α ≤ β, then W(α,β, s) = VolEd (B(α)) = VolEd (B) ∙ αd, see Figure 4d.
The following lemma will also be helpful in our proofs.
Lemma 4. W(α, β, α) and W(β, α, α) are non-decreasing functions of α.
Proof. Let us consider the configuration of balls shown in Figure 5. The d-dimensional ball centered
at q0 is contained within the ball centered at q. Thus, the intersection A0B0x of the balls centered
at x and q0 is contained in the intersection ABx of the balls centered at x and q. Clearly, the center
q0 of the ball which is closer to x can be taken on the geodesic between q and x without loss of
generality. The same argument works when the ball centered at x contains the centers q, q0 of the
other balls.	口
A.2 Hyperbolic volumes
First, we briefly recall the basic properties of hyperbolic space and then analyze hyperbolic vol-
umes and their Euclidean counterparts. On a small scale, hyperbolic volumes behave in an almost
Euclidean way, while their behavior is fundamentally different on a larger scale.
α
□
15
Published as a conference paper at ICLR 2022
Figure 5: Monotonicity of W(α, β, α) and W(β, α, α)
A.2. 1 Background on hyperbolic space
Recall that by Rd,1 we denote the (d + 1)-dimensional Minkowski space, i.e., the real vector space
Rd+1 equipped with the inner product of signature (d, 1):
hx, yih = -x0y0 +x1y1 + . . . +xdyd.
The vector model of the d-dimensional hyperbolic Lobachevsky space Hd is the connected compo-
nent of the standard two-sheeted hyperboloid contained in the future light cone:
Hd = {x ∈ Rd,1 | hx, xih = -1, x0 > 0}.	(9)
The hyperbolic metric is given by
cosh ρ(x, y) = -hx, yih.
The coordinates (x0, x1, . . . , xd) satisfy the following system of equations:
x0 = cosh(t1),
x1 = sinh(t1) cos(t2),
...
xd-1 = sinh(t1)sin(t2) . . .sin(td-1)cos(td),
xd = sinh(t1) sin(t2) . . . sin(td-1) sin(td).
This hyperbolic coordinate parameterization of Hd is the map
χ : [0, +∞) × [0, π]d-2 × [0, 2π] → Hd
defined by
χ(t1,...,td) = (x0,x1,...,xd).
A subset U ⊂ Hd is called measurable if χ-1 (U) is measurable with respect to the standard
Lebesgue measure on Rd . Let U ⊂ Hd be a measurable set. Then, the hyperbolic volume of U
is defined by
VolHd (U) =	sinhd-1(t1) sind-2(t2) . . . sin(td-1) dt1dt2 . . . dtd-1dtd.
χ-1(U)
A.2.2 Approximating small hyperbolic volumes by their Euclidean
COUNTERPARTS
In this section, we show that small hyperbolic volumes can be approximated by Euclidean ones.
This is very helpful for our analysis and will allow us to obtain the analogs of Lemma 2 and 3 for
small hyperbolic balls.
16
Published as a conference paper at ICLR 2022
From the definition of the hyperbolic space (9), We see that xo = ,1 + x2 + ... + Xd. Let U ⊂ Hd
be a measurable set containing the point z = (1, 0, . . . , 0), and let U0 be the projection of U along
the xo-coordinate from the hyperboloid onto the tangent space TzHd = {x ∈ Rd,1 | xo = 1} ` Rd
realised by the map P : Hd → Rd. Let C be the central projection C : Hd → TzHd ` Rd from the
origin o =(0,..., 0). Let F : U0 → CU) be the map that satisfies C = F ◦ p.
Let US note that the central projection C maps balls centered at z in Hd to balls centered at p(z) = z in
Rd, and also spherical caps of such balls in Hd have spherical caps in Rd as their respective images,
since every spherical cap is an intersection of some ball With a half-space, Which is bounded by
some hyperplane in Rd,1 (the central projection of this hyperplane is also a hyperplane in Rd).
According to Ratcliffe (2019),
VolHd (U )= Z	dxJdXd	2.
1 + x1 + . . . + xd
We need to compare VolHd (U) to VolEd (CU)) in order to conclude that these quantities approximate
each other in some appropriate regime.
Lemma 5. Suppose that U ⊂ Hd is a measurable set such that U ⊂ B(z,r). If r 《√ and
d → ∞, then we have
VolHd(U) = VolEd(CU)) (1 - O (dr2)).
Proof. We have that U0 = p(U) ⊂ BRd (0, sinh(r)), hence x12 + . . . + x2d ≤ sinh2(r). Then,
Z dx1 .,fdχd < VolHd (U) = Z ,xx∖1 ...dxd < < Z dxι ...dXd = VolEd (U0).
J	cosh(r)	J √1+ x2 + ... + xd	J
U0	U0	1	d U0
By using Taylor series expansion, We obtain
Z dxiMdxd = ^nVolEd (U 0) = S — r2 + Θ (r4)) VolEd (U 0),
cosh(r) cosh(r)	2
U0
and thus
(1 — r2 + Θ (r4fj VolEd(U0) < VolHd(U) < VolEd(U0).
NoW let us shoW that
VolEd(U0) = VolEd(CU)) (1 — O (dr2)).
Indeed, the map F : U0 → CU satisfies C = F ◦ P and is given by F = (Fi,..., Fd) with
Fi : xi → J 2	2, for i = 1,...,d,
1 + xi + . . . + xd
and thus the Jacobian of F satisfies
dt (∂Fi ʌd	=	1
e IdXj 人j=i=(1 + xi + ... + Xd) d+2
1 — O (dr2),
whenever xi2 + . . . + x2d ≤ sinh2 (r) and dr2 → 0. One can conveniently use spherical coordinates
in order to perform the respective computation. The lemma now follows from standard Taylor series
techniques.	口
Let Bd(x, r) be a closed hyperbolic ball centered at a point x ∈ Hd of radius r > 0. Then we can
compute its volume as (Ratcliffe, 2019):
r
VolHd (Bd(x, r)) = Vol (Sd-i) J Sinhd-1(t) dt.
o
For r log d, we have
VolHd(Bd(x,r))〜Vol(Sd-i) exp(r(d — 1)) Θ(d-12-d).
For r《1/√d, we have from Lemma 5 that
VolHd (Bd(x,r)) ~ VolEd(Bd(x,r)) ~ Vol(SdT) d-i rd Θ(1).
17
Published as a conference paper at ICLR 2022
A.2.3 Small hyperbolic balls and their intersections
Lemma 5 allows us to transfer our results for Euclidean balls and their intersections to the hyperbolic
space when the radii of the balls are sufficiently small. Let Cα(h) be the volume of a spherical cap
of height h in the hyperbolic space. The following lemma follows from Lemma 2 and Lemma 5.
Lemma 6. For α
follows:
《 1∕√d, the volume of a hyperbolic spherical cap Cα(h) Can be estimated as
Θ(d-1) ≤
_________Ca(h)
VolEd-1 (B) ∙ αd ∙ qα(h)d+1
≤ Θ(d- 2).
Let us now estimate the volume of the intersection of two hyperbolic balls. As for the Eu-
clidean case, denote by Wxi,xj (α, β) the intersection of B(xi, α) and B(xj, β), i.e., Wxi,xj (α, β) =
{y : ρ(xi, y) ≤ α, ρ(xj, y) ≤ β}. Let s := ρ(xi, xj) be the distance between the centers and
W(α, β, s) := VolHd Wxi,xj (α, β) denote the volume of the intersection. The following lemma
estimates the value W(α, β, s) for various relationships between α, β, s.
Lemma 7. Assume that ɑ ≤ β《1∕√d. Define A := (a+e+s)(a+e-4S(；+a-e)(s+e-a).
1.	Ifα+ β < s, then W(α, β, s) = 0.
2.	If S 一 α ≤ β < 7α + s2, then
Θ (d-1) ≤
W(a, β, S) ∙ αβ
VolEd-I (B) ∙ (α + β) ∙ Ad+1
≤Θ
(10)
3.	If 7α + s2 ≤ β < s + α, then
1 ≤	W(ɑ,β,s)	≤ ι
2 - VolEd(B) ∙ α ∙(i + o(i))-
VolHd (B (G)) — W (a, β, S) ≤ θ (d- 2
VolEd-I (B) ∙ Ad+1 ∙ α-1 — I
4.	If S + α≤ β, then
(11)
(12)
W(α, β, s) = VolHd(B(α))〜VolEd(B) ∙ ad.	(13)
Proof. To prove this lemma, we follow the proof of Lemma 3, and use Lemma 6 instead of Lemma 2.
□
The following lemma is an analog of Lemma 4. Indeed, the argument uses only monotonicity of
volumes under set-theoretic inclusions, which holds for both Euclidean and hyperbolic spaces.
Lemma 8. In the hyperbolic space, W(α, β, α) and W(β, α, α) are non-decreasing functions ofα.
B Proof overview for Theorem 1
Recall that we assume a uniform distribution of the dataset over a d-dimensional ball B of radius
R = R(n) such that
n1/(d-1)
d1/2
(14)
Then, the graph G(M) is constructed using a threshold αM = Mδ with
δ = (dvd(R) )1/d
(15)
18
Published as a conference paper at ICLR 2022
Let Us show that under the conditions of the theorem, We have δ《1/√d. First, note that
R
vd(R) =
0
sinhd-1 (t) dt ≤
R
et(d-1)
2 ^-1r
0
eR(d-1)
dt ≤ —----------.
≤ 2d-1(d - 1)
Thus, we have
δ≤
deR(d-1)	1/d
2d-1(d - 1)n )	=θ
eR(d-1)/dn-1/d
The last inequality is due to (14). Thus, we can apply Lemma 7 throughout the proof.
To prove that the algorithm succeeds w.h.p., we need to guarantee that it does not stop in a local
optimum until we find the exact (or approximate) nearest neighbor of the query q. Given a point x
such that ρ(q, x) = αs, the probability of making a step towards q is determined by W (αs, αM, αs):
the distance between the points is αs, the neighbors ofx are within the ball of radius αM centered at
x, and we accept those neighbors that are closer to q than x, i.e., within a ball of radius αs centered
at q. Lemma 9 analyzes the asymptotic behavior of W (αs, αM, αs).
Lemma 4 states that W (α, β, α) is a non-increasing function of α. Informally, it means that it is
easier to make steps towards the query when ρ(x, q) is larger. In contrast, when x becomes closer to
q, the probability of getting stuck in a local optimum increases. Therefore, to obtain lower bounds on
W (αs, αM, αs), we can consider only the case when αs is small. Below we assume that αs = sδ,
where s is some constant.
Lemma 11 gives the following lower bounds on W (αs, αM , αs) :
Lif M < s√2,then w⅛αM)ɑ) ≥ 1 ∙ (M2 - <)2 +o(d);
2. if M ≥ s√2, then W⅛0Mal ≥ * ∙ sd+o(d).
Let us consider one step of the algorithm. We can move forward w.h.p. if W (αs, αM, αs)	1/n.
Formally speaking, it follows from Corollary 4 of Lemma 11 that if M and s satisfy
s > 1 and either M ≥ s√2 or M2 一 M4/4s2 > 1,	(16)
then there exists a constant S > 1 such that we can make a step towards q with probability
1-
1-
Sd+o(d) n-1
n
We make steps of the greedy algorithm towards the query q as long as the probability of making the
next step is large. Below we also prove that the overall probability of failure (taking into account
all steps of the algorithm) is small. Now, let us show that under the conditions of the theorem, we
reach some neighborhood of q and then either find the exact nearest neighbor or the approximate
near neighbor by making one more step.
First, we consider the exact NNS. In this case, we assume that the nearest neighbor is located within a
ball of radius rδ, 0 < r ≤ 1, centered at q. Our goal is to find the exact nearest neighbor. According
to the statement of the theorem, we have M > √2, which implies
M 、 M2
√2 > 2√M2 — 1
(17)
While we move towards the query, S becomes smaller. Let US figure out UP to which S we can make
steps. According to (16), we need either 1 < s ≤ M/√2 or s > M2/2√M2 - 1. According
to (17), it reduces to S > 1. In other words, we can move forward to any S > 1. Then, according to
Lemma 14, it is sufficient to have M2 > s2 + r2 to reach the exact nearest neighbor of q in one step.
Thus, we can take any S s.t. 1 < s < √M2 - r2: we can reach this distance with greedy search and
then jump directly to the nearest neighbor with high probability.
19
Published as a conference paper at ICLR 2022
Now, consider the approximate near neighbor search c, r-ANN. In this case, we need to find an
element that is within a distance rc from q. According to the statement of the theorem, We have
M > √4c2∕(3c2 - 1).
Note that F(r) := 2r2c2(1 一 ,1 - 1∕r2c2) decreases with r for r > 1/c and equals 2 at r = 1/c,
while G(r) := 3 (r2 + 1 + √r4 一 r2 + 1) increases with r and equals 2 at r = 1. Both F(r) and
G(r) attain 4c2∕(3c2 — 1) at ro = /4c2∕(c2 + 1)(3c2 - 1). Therefore, we either have
1 ≥ r > r0 and M2 > F(r)	(18)
or
ro ≥ r > 一 and M2 > G(r).	(19)
To solve c, r-ANN, either we can reach s such that rc > s or we can reach s such that at the next
step we can find the exact nearest neighbor. Let us consider the first possibility. Note that
s > 1 and M2 > 2s2
(20)
is equivalent to (16). In turn, since 2s2(1 一 vz1 - 1∕s2) is a decreasing function of S on s > 1, we
get that (18) implies the existence of s such that rc > s > 1 and
M2 > 2s2 (1 — 1- - -2) > F(r).
Hence, since (16) holds, we can move forward to such s. Let us consider the second possibility.
Observe that (19) is equivalent to
M2 - r2 >
M 4
4(M2 - 1).
Therefore, there exists s such that the condition s2 < M2 - r2 of Lemma 14 and (16) hold. Since
(16) holds, we can move forward to such s and reach the exact nearest neighbor in one step as the
condition of Lemma 14 is satisfied.
Above, we show that under the conditions of the theorem, each step of the greedy search is suc-
cessful with probability 1 - O e-Sd+o(d) . To formally prove that the algorithm succeeds with
high probability, we need to limit the number of steps and estimate the overall probability of failure.
For this, we show that at each step of the algorithm, we become closer to the query by some fixed
positive value. In Lemma 12, we prove that each step reduces the distance from q by at least εδ,
where ε is some positive constant. Clearly, we start at a distance O(R) from q. Hence, we are able
to bound the number of steps by O(R∕εδ) which is O(Rn1/d(Vd(R))-1/d).
To estimate the overall success probability, we have to consider the dependence of consecutive steps
of the algorithm. Lemma 13 implies that consecutive steps are “almost independent” and the overall
success probability is 1 -O Rn1/d(vd(R))-1/de-Sd+o(d) . The assumption d log log n and the
fact that Vd(R) = Ω(Rd) yield O(Rn1/d(Vd(R))T/de-Sd+o(d)) = O (e⅛n-Sd+o(d))=。⑴.
Also, while most of the analysis does not consider the cases when αM -neighborhoods of the dataset
elements intersect the boundary ∂B, we need to show that possible intersections do not affect the
result. This is addressed in Lemma 15.
Finally, let us discuss the time and space complexity of the algorithm. Let v be a fixed arbitrary
node of G(M) and let N(v) denote the (random) number of its neighbors in G(M). Let f =
f(n) = (n - 1)VolHd (B(αM)) ∕VolHd (B) denote the upper bound for the expected number of
neighbors of v (this is an upper bound due to possible boundary effects). In Lemma 16, we show
that N(v) ≤ 3f∕2 with probability at least 1 - O(1∕f).
To obtain the time complexity of graph-based NNS, we sum up the complexity of all algorithm’s
steps. In Lemma 17, we show that if l steps of the NNS are made, then the time complexity is
O (lf d) with probability 1 - O(1∕lf).
20
Published as a conference paper at ICLR 2022
In Lemma 18, we show that the number of edges in the graph is E(G) ≤ 3f n/4 with prob-
ability 1 - O(1/f n). If we store the graph as the adjacency lists, then the space complexity is
O (E(G) ∙ log n) = O (fn log n).
Due to the choice of δ, we have f = O Md . Hence, w.h.p., the space complexity is
O (n ∙ log n ∙ Md) and the time complexity is O(R ∙n1/d ∙ (Vd(R))-1/d ∙ Md)
C Results used throughout the proof
In this section, we formulate supplemental statements used in the proof. We need these results for the
hyperbolic space, but they are also true for the Euclidean one. Thus, we do not specify a particular
space in W(∙, ∙, ∙).
C.1 Analysis of convergence
Recall that aM = Mδ and a§ = sδ, where M and S are some constants and δ is defined in (15).
Recall that δ《1 /√d.
Lemma 9. Define A :
αM (4α2-aM)
4αS
If αM
≤ as √2, then
Θ (d-1) ≤
W(as,aM,as) ∙ aMa§
VolEd-I (B) ∙ (aM + as) ∙ A 2
(21)
If αM
> as √2, then
W(αs, αM, αs)
VolEd (B) asd
Θ(1).
(22)
Proof.
We apply Lemma 7.
First, consider the case αM ≤
W(aM, as, as). From (10) we obtain
Θ(d-1)≤	W(as,aM,as) ∙ aMa§
VolEd-I (B) ∙ (aM + as) ∙ A 2
Now, consider the case as < aM .
as . Then, the case 2 applies to
(23)
1.
If as < aM ≤ as √2, then from (10) We obtain
Θ (d-1) ≤	W(as,aM,as) . aM必小 ≤ Θ (d-1)
^ VolEd-I (B) ∙ (aM + as) ∙ A去一 '	)
2.
If as √2 < aM < 2as, then (11) yields
W (as, aM, as) = Θ (VolEd (B) ∙ ad).
3.
If 2as ≤ aM, then (13) gives
W(as, aM, aS) = VolEd (B) ∙ ad (I - O(I)).
This concludes the proof of the lemma.
Lemma 10. Denote by A = δ2 ∙
as2 + ar2 , then
(M+r+s)(M+r-s)(s+M—r)(s+r-M)
4s2
. If as
ar < aM <
Θ (d-1) ≤
W(αr,αM,αs) ∙ δ
VolEd-I (B) ∙ Ad⅛i
(24)
□
—
If aM >	as2 + ar2 , then
VolHd (B(ar)) - W(ar, aM, as) ≤ ㊀(d-1 ʌ
VolEd-I (B) ∙ Ad+1 ∙ δ-1	—	>
(25)
21
Published as a conference paper at ICLR 2022
Proof. The result follows directly from Lemma 7.
□
Lemma 11. If M > s√2, then
W (αs,αM，aS) ≥ VdnR) ∙ sd+o(d).
If M ≤ s√2, then
W(αs,αM,αs) ≥ Vd(R)
n
Proof. If M > S√2, then We can apply (22) to obtain
W(αs,αM,αs) ≥ Θ (d-1 ∙ (sδ)d ∙ VolEd(B)) = Θ (d-1 ∙ sd ∙ dvd(R) ∙ VolEd(B))
=θ ( d Vd(R) VolEd (B) ʌ = Vd(RLd+o(d)
=V ∙	∙ Vol(SdT)) =
If M ≤ s√2 then We can apply (21) to obtain
W(αs,αM,αs) ≥ VolEd-I (B) ∙ δ-1Ad+1 Θ(d-1)
VolEd-I (B) ∙ δ-1∙αd+1
. Θ(d-1)
d+1
~n~
□
Corollary 4. If s > 1 and either M > S√2 or M2 一 M4/4s2 > 1, then there exists a Constant
S > 1 such that W (a§ ,αM ,a§) ≥ VdnR) S d+o(d).
Lemma 12. Assume that S > 1. If
M 4
不
M > s√2 or M2 —
1,
(26)
then there exist constants S > 1 and ε > 0 such that
W (αM, αs,αs + εδ) ≥ Vd(R)Sd+o(d).
n
Proof. If M > s√2, then we can choose a sufficiently small ε > 0 such that M2 ≥ s2 + (s + ε)2.
Then, the result folloWs from Lemma 11 and the fact that S > 1. OtherWise M2 < S2 + (S + ε)2,
and instead of the condition M2 — M > 1 from (24) we have
4s2(s + ε)2 — (M2 — s2 — (s + ε)2)2
4(s + ε)2
> 1.
As it holds for ε = 0, we can choose a small enough ε > 0 that it is still satisfied.
□
Lemma 13. The dependence of consecutive steps is negligible.
22
Published as a conference paper at ICLR 2022
Proof. First, consider the case M ≤ S√2. It suffices to show that Wi = W (αM, as,a§ + 2εδ) is
negligible in comparison with W1 + W2 = W (αM, αs , αs + εδ). From (24), we can write their
ratio
WI	= dO(1) ∙ (_______________________4(S + ε)_______________________
W1 + W2	12 (M2s2 + s2(s + ε)2 + M2(s + ε)2) — (M4 + s4 + (s + ε)4)
d+1
2 (M2s2 + s2(s + 2ε)2 + M2(S + 2ε)2) - (M4 + s4 + (s + 2ε)4) ʌ F
4(s + 2ε)2	)
o(1).
Since at each step we reduce the radius of the ball centered at q which contains the current position
by at least εδ, any ball encountered in the current iteration intersects only a constant number of other
balls, so their overall effect is negligible.
In the case when M > S√2 and s > 1 with probability 1 一 o(1) We find the nearest neighbor in one
step.	□
Lemma 14. Iffor some constants M, S, r we have M2 > S2 + r2 , then
VolHd (B (αr )) - W (Qr, αM, αS) ≤ VolHd (B(αr )) ∙ βd
with some β < 1.
Proof. It follows from (25) that
VolHd (B(αr)) 一 W (αr,αM,αs) ≤ Θ (d-2) ∙ VolEd-I (B) ∙ Ad+1 ∙ δ-i
It remains to bound from above the following ratio:
Θ (d- 1) ∙ VolEd-I (B) ∙ Aɪ ∙ δ-i Θ (d-2) ∙ VolEd-I (B) ∙ Ad+1
VolHd (B(αr))
VolEd (B) ∙ rd ∙ δd+i
Θ(1) ∙
d+1
~n~
d + 1
((M + r + s)(M + r 一 s)(s + M 一 r)(s + r 一 M) ʌ 2
θ(1) Λ-------------------4r2S--------------------
Θ(1) ∙ (l -尤「厂 ≤ βd+i
for some β < 1.
□
C.2 Analysis of the boundary effect
Lemma 15. The boundary effect is negligible and does not affect our results.
Proof. Consider the worst-case scenario when the current element is located on the boundary of the
ball B (see Figure 6). Indeed, in this case, the largest part of the neighborhood is located outside B,
which may reduce the number of neighbors of a given element.
First, consider the Euclidean space. According to Lemma 5, this is equivalent to the hyperbolic
case with R《1∕√d. Let US show that W(α, R, R) converges to 11 VolEdB(α) if α = Θ(δ) with
δ = (dVolEd(BI(R)) Y". Note that W(α, R, R) = Cα(h) + Cr(q 一 h) < 1 VolEdB(α). It is easy
Vol(Sd-1 )n	, ,	α	2 E
2
to see that h = α — OR. Then,
Cα(h) = 2 VolEd-I (B)αdB
d+1 1
(丁, 2;1
(I + O(I)) ∙ 2 VolEd-I (B)QdB
d+1 1
(丁, 2)
(I + O(I)) ∙ 2 VolEdB(Q)
23
Published as a conference paper at ICLR 2022
Figure 6: Illustration for the proof of Lemma 15
if a《1 / vzd. Indeed,
B
3
d+1 1
(一, 2)
-B
1
2;
t(d-1)/2 (1 - t)-1/2dt
≤
, 1	[ 4R2 …dt =	2 (2⅜)d+1	= o(B (d+1,1
J1- 4R 0°	(d +1)√1 - 4R2	∖ ∖ 2	2
since B (d+1, 2) = Θ (l∕√d) and
α Θ(1)
一=------
R R
(dVo⅛d (B (R)) Y/d
Vol(SdT)n J
θ(ŋ → 0 for d《log n .
n1/d
In the hyperbolic case, the “small” ball of radius α is small enough (α《1/Vd) so that its geometry
can be considered Euclidean, and thus we can apply Lemma 5 to estimate the volume of Cα (h).
The “large” hyperbolic ball of radius R, in this case, can be thought of as a Euclidean ball of radius
〜2 e2R in the upper half-space model, and then We can carry out the same analysis on the Euclidean
scale of the “small” radius α ball. Note that in this case
α _ Θ(1) (dvd(R) \1/d	e(1)eR(d-1)/d
e2R	e2R ( n ) e e2Rn1/d	T '
□
C.3 Time and space complexity
In this section, we estimate time and space complexity. Recall that N (v) denotes the number of
neighbors ofan element v in G(M) and f = (n - 1)VolHd (B(αM)) /VolHd (B). The proofs below
mostly follow the corresponding proofs from Prokhorenkova & Shekhovtsov (2020). We keep the
proofs here for completeness.
Lemma 16. The probability of the event “ N (v) satisfies N (v) ≤ 3 f” is at least 1 — f.
Proof. Note that the random variable N(v) is upper bounded by the random variable N0 (v) hav-
ing the binomial distribution Bin (n — 1, VOVdI(B(BM))
Chebyshev’s inequality, we obtain
with the expected value f. Thus, via the
P (N(v) > 2f) ≤ P (N0(v) > 2f) ≤ P (|N0(v) - fI > 2) ≤ 4VarfNO(V)) ≤ 4.
□
24
Published as a conference paper at ICLR 2022
Lemma 17. The time complexity of l steps of the graph-based NNS is O (lf d) with probability
1 - O (lf)
Proof. We can upper bound the number of distance computations by the random variable distributed
according to Bin (l(n - 1), VoVo(B(BM))) and proceed as in the proof of Lemma 16. Finally, note
that one distance computation takes Θ(d).	口
Lemma 18. The probability ofthe event “ E(G) satisfies E(G) ≤ 4 fn'' is 1 一 O (fə .
Proof. We mostly follow the proof of Prokhorenkova & Shekhovtsov (2020), but some modifica-
tions are needed to handle the boundary. We use Chebyshev’s inequality, so we have to estimate the
variance VarE(G):
VarE(G) = E (E(G)2) -(EE(G))2
= X	P(eι, e2 ∈ E(G))- P(e1 ∈ E(G)) ∙ P(e2 ∈ E(G)).
e1,e2∈(D2 )
Obviously, if e1 ande2 do not share common endpoints, then P(e1,e2 ∈ E(G)) = P(e1 ∈ E(G)) ∙
P(e2 ∈ E(G)). Also, one can see that if e1 and e2 share one element, then we also have P(e1, e2 ∈
E(G)) = P(eι ∈ E(G)) ∙ P(e2 ∈ E(G)). Indeed, assume that both e1 and e2 contain a common
element v. Then the equality holds conditioned on the position ofv since both sides of the equation
are defined in terms of v-th neighborhood size.
Thus, only the terms with e1 = e2 remain in the expression:
VarE(G) =	P(e1 ∈ E(G)) - (P(e1 ∈ E(G)))2 ≤ EE(G) .
e1∈(D2)
Finally, we get
P (|E(G) - EE(G)I > EE(G)
/ 4VarE(G) /	4
≤ (EE(G))2 ≤ EE(G).
It remains to note that EE(G) ≤ nf/2 and due to Lemma 15 we also have EE(G) = Θ(nf). From
this the lemma follows.
□
D k-NN GRAPHS
Note that in Theorem 1 We consider undirected graphs since the condition on ρ(∙, ∙) is symmetric.
In this section, we extend the theorem to nearest neighbor graphs. In this case, we assume that
each node is connected to a fixed number of its nearest neighbors by directed edges. During graph
traversal, outgoing edges are used to explore the neighborhood.
Theorem 3. Assume that d = d(n) is such that log log n	d(n)	logn and R = R(n) is such
that eR《nd-1 ∙ d-1/2 .
For the exact NNS, let M be a constant s.t. M > √2 and let k = [Md]. Then, with probability
1 - o(1), NNS based on the k-NN graph finds the exact nearest neighbor. For c, r-ANN with some
constant c > 1, let M be a constant s.t. M > J34c-1 and k = [Md]. Then, with probability
1 - o(1), NNS based on the k-NN graph solves c, r-ANN for any r.
In both cases, the time complexity is O (k ∙ n1/d ∙ R ∙ (Vd(R))T/d);
the space ComPlexity is O (k ∙ n ∙ log n).
25
Published as a conference paper at ICLR 2022
Proof. First, note that the space complexity is straightforward: each node has exactly k outgoing
edges. Similarly, the complexity of one step is O(k ∙ d).
Thus, it remains to show that kNN-based NNS succeeds with probability 1 - o(1) and that the
number of steps can be upper bounded similarly to Theorem 1. To prove this, it is sufficient to show
that the distance to the k-th neighbor is sufficiently large, so that the convergence can be guaranteed
by the proof of Theorem 1.
For every node v, define a random variable ρk (v) to be the distance from v to its k-th nearest
element. It is sufficient to show that Pk(V) ≥ Mδ, where δ = CVdnR) and Mc satisfies the
required conditions on M in Theorem 3. For some ε > 0, let Mc := M (1 - ε). Obviously, by
choosing small enough ε, we can guarantee that the required conditions are satisfied.
Note that Nr(v) — the number of elements at distance at most r from v — can be upper bounded
by a random variable having the binomial distribution Bin (n — 1, VVHd (BBB)))) (the upper bound
is caused by possible boundary effects). If r = Mδ, then VVHd (BBB))) = Md/n. Thus, Nm§ (V) is
upper bounded by Bin n, Mcd/n .
Recall that k = [Md]. It is sufficient to show that with high probability we have NMcδ (V) < k for
all V. Using the union bound over all possible V and the Chernoff’s bound, we obtain the following:
(k-Mcd)2
P (∃ V : Ncδ(V) ≥ k) ≤ n ∙ e- 2(Md+(k-Md)⑶
log n-
M 2d(1-o(1))
Θ(Md)
→0
≤
e
as n → ∞ since d	log log n.
□
E Graph-based NNS in sparse regime
In this section, we prove Theorem 2. For this, we show that the elements are tightly concentrated
near the boundary of the ball, and thus the convergence guarantees resemble those on the sphere
(Prokhorenkova & Shekhovtsov, 2020).
Let us denote by 夕(Xi, Xj) the angle ∠xQxj. For the sphere, a graph G(M) is constructed by
connecting such pairs of elements that (Prokhorenkova & Shekhovtsov, 2020):
cos 夕(Xi, Xj)
∕2M log n
≥ V d
(27)
For the hyperbolic space, we connect the elements Xi and Xj iff

cosh ρ(xi, Xj) ≤ cosh2 R — sinh2 R ∙
2M log n
d
(28)
Note that this corresponds to the elements Xi and Xj located on the boundary of B with 夕(Xi, Xj)
bounded by (27).
Recall that for c > 1 we set
cosh2 R — cosh(arccosh(cosh2 R)/c)
Qc ： —	iɔ
sinh2 R
(29)
Let us explain this choice. Below we prove that NNS in the hyperbolic space behaves similarly
to NNS on a sphere. In other words, we can project all the elements to the boundary of B, and
this will not significantly affect the process. However, we have to properly adjust αc taking into
account the multiplicative approximation factor c. In Prokhorenkova & Shekhovtsov (2020), αc is
the cosine of the angle 2c. There, 2∏c is the spherical distance that is C times smaller than the distance
between the elements with 夕(Xi, Xj) = n/2. The hyperbolic distance D between the elements with
夕(Xi, Xj) = n/2 located on the boundary of B satisfies the following equality:
cosh D = cosh2 R, i.e., D = arccosh(cosh2 R) .
26
Published as a conference paper at ICLR 2022
Then, We need to compute cos 夕(Xi, Xj) if P(Xi, Xj) = D/c. This follows from the hyperbolic law
of cosines:
cosh2(R) 一 cosh(D/c)
Cos 以 xi，Xj ) = sinh2(R-，
which is equal to αc in Equation (29).
Thus, it remains to show that the convergence guarantees transfer from spherical to hyperbolic space.
Let us first prove that the elements are tightly concentrated near the boundary of the ball B.
Lemma 19. With probability 1 一 o(1) all n elements are located within a shell of width Θ (Iogdn)
near the boundary of the ball B.
Proof. According to (3), we have
VolHd (B(R - ε))
VolHd (B(R))
Θ e-ε(d-1)
Let ε
d-1 (log n + 夕)，where 夕=夕(n) is any function s.t.夕 → ∞ as n → ∞. Then,
VolHd (B(R - ε/
VolHd (B(R))
From this the lemma follows.
Θ (e-lοg…)=ο(1∕n).
Lemma 20. If two elements xi , xj satisfy (27) with constant M + ε for some ε > 0, then with
probability 1 一 o(1) they satisfy (28) with constant M.
Proof. Let the distances from Xi and Xj to the center of the ball B be ri and rj , respectively. From
Lemma 19, r and rj are equal to R 一 Θ (lodn) with high probability. Using the hyperbolic law of
cosines, we get
Cosh P(Xi, Xj) =Cosh ri Cosh rj — sinh r sinh rj cos 夕(Xi, Xj)
≤ cosh ri cosh rj 一 sinh ri sinh rj
/ 2(M + ε)log n
d d
Cosh2(R 一 Θ(log n/d)) 一 sinh2(R 一 Θ(log n/d))
/ 2(M + ε)log n
d d
It is sufficient to show that
cosh2 (R — Θ(log n/d)) — sinh2(R — Θ(log n/d))
22(M + ε) log n
d d
≤ Cosh2 R - sinh2 R ∙ J2M' n .
Note that cosh(R-Θ(log n/d)) = eR (1 + Θ(log n/d)). The same holds for sinh(R-Θ(log n/d)).
Therefore, we need
e2R .ι W / n. Λ	∕2(M + ε)lognʌ	e2R J 2 / 2Rw A ∕2Mlogn
丁(1 + θ(log n/d)) []-ʌ/	d^ɪ ≤ ≤ 丁(1 + θ (e-2R)) I-N -dL~
Now we divide the right hand side by the left hand side and obtain
(1 + Θ(logn/d) + Θ LR))
□
0+N2(M +jlog n - J 2MF) (ι+θ (P^)))
(1 + Θ(logn/d) + Θ (e-2R))
=(1 + Ω(plogn/d))(1 + Θ(logn/d) + Θ (e-2R)).
From this the lemma follows, since ,log n/d》log n/d and ,log n/d》e-2R.	口
27
Published as a conference paper at ICLR 2022
Let us choose such small ε > 0 that the required condition on M is also satisfied for M + ε. It
follows from Lemma 20 that our graph G(M) contains the spherical nearest neighbor graph defined
via (27) with constant M + ε. Thus, we may follow the proof of Theorem 2 in (Prokhorenkova &
Shekhovtsov, 2020) (Appendix B.5) with the corresponding nearest neighbor graph.
Note that all spherical caps and distances mentioned in the proof of (Prokhorenkova & Shekhovtsov,
2020) are defined by conditions cos φ ≥
2S 2s Idg n for some constant S. According to Lemma 20,
such constants are negligibly affected by deviations from the boundary, and thus we can directly
follow the proof of Prokhorenkova & Shekhovtsov (2020) to obtain the convergence guarantees and
complexity. Indeed, all requirements for the constants in the proof are strict, and thus a sufficiently
small ε can be chosen such that they are still satisfied.
F Experiments
Best-first search In the main text, for the synthetic experiments, we use the greedy search. How-
ever, in graph-based NNS algorithms, the greedy search is often replaced by the best-first search
(a.k.a. beam search). In this case, instead of keeping track of only the best candidate, we main-
tain a dynamic list of several candidates. In particular, this technique is used in HNSW (Malkov &
Yashunin, 2018), and the size of the candidate list is controlled by the parameter efSearch.
Hyperparameters Recall that to plot Figure 1, we vary the graph degree to control the complex-
ity and accuracy of NNS. In Figures 2 and 3, we use the best-first search, so we vary the parameter
ef Search and fix the graph degree. In this case, we tune M and ef Construction HNSW hyperpa-
rameters, which are responsible for the graph degree and quality of the graph construction, respec-
tively. For both GloVe and Poincare GloVe databases, We get M=10 and efConstruction=00.
For other datasets, we get M=8.
NSG In addition to HNSW, We also experiment With the NSG graph (Fu et al., 2019). For this
algorithm, the degree is controlled by the parameter R, and the optimal degree is R = 2M .
Spherical Shell implementation As a baseline, We implement Spherical Shell proposed by Wu
& Charikar (2020). As in the original paper, We split the elements into 25 bands such that wi-1 ≤
1-浦2 ≤ wi. To achieve this, we take W = 1.05. For each band, we construct LSH with 50 tables.
Each table has [log(m)] - 1 bins, Where m is the number of elements in the band. The number
of probes is tuned for each band to reach 0.95 recall (this value has been chosen to achieve better
results). When searching for the nearest neighbor for a query q, we compute the nearest neighbors
within l bands closest to q and take the best one according to the hyperbolic distance. The number
of bands l is varied from 1 to 10, thus giving the curve on Figure 2.
Additional datasets In addition to the standard POinCare GloVe dataset, we also consider the
Poincare GloVe embeddings trained for d = 10 using the source code provided by the authors (Tif-
rea et al., 2019). Additionally, we also take the recommendation dataset from Shevkunov &
Prokhorenkova (2021), where a DSSM model was trained on a Wikipedia search dataset of (search
query, relevant page) pairs. We use their Euclidean and hyperbolic representations for d = 10.
Additional experiments on real datasets Figure 7 compares graph-based methods with Spherical
Shell on the standard Poincare GloVe. Compared to the main text, here we add the NSG algorithm.
While HNSW outperforms NSG, both are better than Spherical Shell.
28
Published as a conference paper at ICLR 2022
600	800	1000	1200	1400
DCS
0.6
' ' 6 6 7
Ooooo
Ia = 80
Figure 7: Graph-based methods VS Spherical Shell on POinCare GloVe embeddings
Figure 8: Comparison of algorithms on
Poincare GloVe with d = 10
I@=ga)a:
Figure 9: Comparison of algorithms on the rec-
ommendation dataset with d = 10

Figures 8 and 9 compare the algorithms on the Poincare GloVe dataset with d = 10 and the rec-
ommendation dataset with d = 10 from Shevkunov & Prokhorenkova (2021). We see that HNSW
and NSG have similar performance on Poincare GloVe, while HNSW outperforms NSG on the rec-
ommendation dataset. In contrast, Spherical Shell needs a significantly larger number of distance
computations, so it is not shown in the figures. On the recommendation dataset, low performance
of Spherical Shell can be explained by the fact that this data significantly differs from the POinCare
GloVe: the learned vector norms are much larger. Because of this, we either have annuli of large
width (significant approximation error) or many annuli (increased total complexity). Also, it is im-
portant to note that Spherical Shell is well suited for sparse datasets, while we have d = 10 in this
experiment.
Then, we compare the efficiency of NNS in Euclidean and hyperbolic spaces. For this, we consider
the embeddings of the two datasets discussed above into Euclidean and hyperbolic spaces and per-
form NNS based on HNSW for the same set of queries. Figure 10 shows that hyperbolic NNS is
more efficient.
Additional synthetic experiments Figure 11 is similar to Figure 1, but now we use the best-first
search instead of the greedy search. We see that the conclusions are the same.
29
Published as a conference paper at ICLR 2022
0.75
0 5 0 5 0
Q 9 9 6 6
Ioooo
1。=。。。
200	300	400
DCS
500
600
0.70
090808
1@=。金
100	150	200	250	300	350	400
DCS
Figure 10: Efficiency of HNSW in hyperbolic space vs Euclidean space on GloVe with d = 10 (left)
and recommendations (right)


Figure 11: Best-first search over NN graphs without (top row) and with (bottom row) long edges
30