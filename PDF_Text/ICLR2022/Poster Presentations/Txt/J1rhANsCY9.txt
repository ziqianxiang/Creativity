Published as a conference paper at ICLR 2022
Learning Representation from Neural Fisher
Kernel with Low-rank Approximation
Ruixiang Zhang	Shuangfei Zhai, Etai Littwin, Josh Susskind
Mila, Universite de MontreaI	Apple Inc.
ruixiang.zhang@umontreal.ca {szhai,elittwin,jsusskind}@apple.com
Ab stract
In this paper, we study the representation of neural networks from the view of
kernels. We first define the Neural Fisher Kernel (NFK), which is the Fisher Kernel
(Jaakkola and Haussler, 1998) applied to neural networks. We show that NFK
can be computed for both supervised and unsupervised learning models, which
can serve as a unified tool for representation extraction. Furthermore, we show
that practical NFKs exhibit low-rank structures. We then propose an efficient
algorithm that computes a low rank approximation of NFK, which scales to large
datasets and networks. We show that the low-rank approximation of NFKs derived
from unsupervised generative models and supervised learning models gives rise to
high-quality compact representations of data, achieving competitive results on a
variety of machine learning tasks.
1	Introduction
Modern deep learning systems rely on finding good representations of data. For supervised learn-
ing models with feed forward neural networks, representations can naturally be equated with the
activations of each layer. Empirically, the community has developed a set of effective heuristics for
representation extraction given a trained network. For example, ResNets (He et al., 2016) trained on
Imagenet classification yield intermediate layer representations that can benefit downstream tasks
such as object detection and semantic segmentation. The logits layer of a trained neural network
also captures rich correlations across classes which can be distilled to a weaker model (Knowledge
Distillation) (Hinton et al., 2015).
Despite empirical prevalence of using intermediate layer activations as data representation, it is far
from being the optimal approach to representation extraction. For supervised learning models, it
remains a manual procedure that relies on trial and error to select the optimal layer from a pre-trained
model to facilitate transfer learning. Similar observations also apply to unsupervised learning models
including GANs (Goodfellow et al., 2014), VAEs (Kingma and Welling, 2014), as evident from recent
studies (Chen et al., 2020a) that the quality of representation in generative models heavily depends
on the choice of layer from which we extract activations as features. Furthermore, although that
GANs and VAEs are known to be able to generate high-quality samples from the data distribution,
there is no strong evidence that they encode explicit layerwise representations to similar quality as in
supervised learning models, which implies that there does not exist a natural way to explicitly extract
a representation from intermediate layer activations in unsupervisedly pre-trained generative models.
Additionally, layer activations alone do not suffice to reach the full power of learned representations
hidden in neural network models, as shown in recent works (Mu et al., 2020) that incorporating
additional gradients-based features into representation leads to substantial improvement over solely
using activations-based features.
In light of these constraints, we are interested in the question: is there a principled method for
representation extraction beyond layer activations? In this work, we turn to the kernel view of
neural networks. Recently, initiated by the Neural Tangent Kernel (NTK) (Jacot et al., 2018) work,
there have been growing interests in the kernel interpretation of neural networks. It was shown that
neural networks in the infinite width regime are reduced to kernel regression with the induced NTK.
Our key intuition is that, the kernel machine induced by the neural network provides a powerful and
principled way of investigating the non-linear feature transformation in neural networks using the
1
Published as a conference paper at ICLR 2022
linear feature space of the kernel. Kernel machines provide drastically different representations than
layer activations, where the knowledge of a neural network is instantiated by the induced kernel
function over data points.
In this work, we propose to make use of the linear feature space of the kernel, associated with the
pre-trained neural network model, as the data representation of interest. To this end, we made novel
contributions on both theoretical and empirical side, as summarized below.
•	We propose Neural Fisher Kernel (NFK) as a unified and principled kernel formulation for neural
networks models in both supervised learning and unsupervised learning settings.
•	We introduce a highly efficient and scalable algorithm for low-rank kernel approximation of NFK,
which allows us to obtain a compact yet informative feature embedding as the data representation.
•	We validate the effectiveness of proposed approach from NFK in unsupervised learning, semi-
supervised learning and supervised learning settings, showing that our method enjoys superior
sample efficiency and representation quality.
2	Preliminary and Related Works
In this section, we present technical background and formalize the motivation. We start by intro-
ducing the notion of data representation from the perspective of kernel methods, then introduce the
connections between neural network models and kernel methods.
Notations. Throughout this paper, we consider dataset with N data examples D ≡ {(xi , yi)}, we use
p(x) to denote the probability density function for the data distribution and use pdata(x) to denote
the empirical data distribution from D.
Kernel Methods. Kernel methods (Hofmann et al., 2008) have long been a staple of practical
machine learning. At their core, a kernel method relies on a kernel function which acts as a similarity
function between different data examples in some feature space. Here we consider positive definite
kernels K : X × X → R over a metric space X which defines a reproducing kernel Hilbert space H
of function from X to R, along with a mapping function 夕：X → H, such that the kernel function
can be decomposed into the inner product K(x, Z)=(夕(x),夕(z)〉. Kernel methods aim to find a
predictive linear function f (x) = hf, W(XyiH in H, which gives label output prediction for each data
point X ∈ X. The kernel maps each data example X ∈ X to a linear feature space 夕(x), which is
the data representation of interest. Given dataset D, the predictive model function f is typically
estimated via Kernel Ridge Regression (KRR), f = argminf ∈HN PN=I (f (Xi) 一 yi)2 + λ∣∣f |昆.
Neural Networks and Kernel Methods. A long line of works (Neal, 1996; Williams, 1996; Roux
and Bengio, 2007; Hazan and Jaakkola, 2015; Lee et al., 2018; de G. Matthews et al., 2018; Jacot
et al., 2018; Chen and Xu, 2021; Geifman et al., 2020; Belkin et al., 2018; Ghorbani et al., 2020),
have studied that many kernel formulations can be associated to neural networks, while most of
them correspond to neural network where being fixed kernels (e.g. Laplace kernel, Gaussian kernel)
or only the last layer is trained, e.g., Conjugate Kernel (CK) (Daniely et al., 2016), also called as
NNGP kernel (Lee et al., 2018). On the other hand, Neural Tangent Kernel (NTK) (Jacot et al., 2018)
is a fundamentally different formulation corresponding to training the entire infinitely wide neural
network models. Let f(θ; X) denote a neural network function with parameters θ, then the empirical
NTK is defined as Kntk(X,z) = Bef(θ; x), Vθf(θ; z)〉. (Jacot et al., 2018; Lee et al., 2018)
showed that under the so-called NTK parametrization and other proper assumptios, the function
f(X; θ) learned by training the neural network model with gradient descent is equivalent to the
function estimated via ridgeless KRR using Kntk as the kernel. For finite-width neural networks, by
taking first-order Taylor expansion of funnction f around the θ, kernel regression under Kntk can be
seen as linearized neural network model at parameter θ, suggesting that pre-trained neural network
models can also be studied and approximated from the perspective of kernel methods.
Fisher Kernel. The Fisher Kernel (FK) is first introduced in the seminal work (Jaakkola and
Haussler, 1998). Given a probabilistic generative model pθ(x), the Fisher kernel is defined as:
Kfisher(X, z) = Vθ log pθ (X)>I-1Vθ logpθ(z) = Ux>I-1Uz where Ux = Vθ log pθ (X) is the
so-called Fisher score and I is the Fisher Information Matrix (FIM) defined as the covariance of
the Fisher score: I = Ex〜p®(x)Ve logpe(x)Vθ logpe(x)>. Then the Fisher vector is defined
as Vx = I-2 Ve logpe(x) = I-2 Ux. One can utilize the Fisher Score as a mapping from the
2
Published as a conference paper at ICLR 2022
Pg(X) OrPg(y∣χ)
Pre-trained model	Energy-based model	Neural Fisher Kernel
Ky=Xnfk(xj,xj) = Ve logp0(xi)τT^1 V0 logpβ(xj∙)
fid EO 冏
e = χ3
IJ
Low-rank kernel approximation	Data representation
Figure 1: Overview of our proposed approach. Given a pre-trained neural network model, which
can be either an unsupervised generative model pθ (x) (e.g. GANs, VAEs), or a supervised learning
model pθ (y|x), We aim to extract a compact yet informative representation from it. By reinterpreting
various families of models as energy-based models (EBM), We introduce Neural Fisher Kernel (NFK)
Knfk as a principled and unified kernel formulation for neural network models (Section. 3.1). We
introduce a highly efficient and scalable kernel approximation algorithm (Section. 3.2) to obtain the
low-dimensional feature embedding ex , which serves as the extracted data representation from NFK.
data space X to parameter space Θ, and obtain representations that are linearized. As proposed
in (Jaakkola and Haussler, 1998; Perronnin and Dance, 2007), the Fisher vector Vx can be used as the
feature representation derived from probabilistic generative models, which was shown to be superior
to hand-crafted visual descriptors in a variety of computer vision tasks.
Generative Models In this work, we consider a variety of representative deep generative mod-
els, including generative adversarial networks (GANs) (Goodfellow et al., 2014), variational auto-
encoders (VAEs) (Kingma and Welling, 2014), as well we normalizing flow models (Dinh et al.,
2015) and auto-regressive models (van den Oord et al., 2016). Please refer to (Salakhutdinov, 2014)
for more technical details on generative models.
3 Learning Representation from Neural Fisher Kernel
We aim to propose a general and efficient method for extracting high-quality representation from
pre-trained neural network models. As formalized in previous section, we can describe the outline of
our proposed approach as: given a pre-trained neural network model f(x; θ) (either unsupervised
generative model p(x; θ) or supervised learning model p(y | x; θ)), with pre-trained weights θ, we
adopt the kernel formulation Kf induced by model f(x; θ) and make use of the associated linear
feature embedding 夕(x) of the kernel Kf as the feature representation of data x. We present an
overview introduction to illustrate our approach in Figure. 1.
At this point, however, there exist both theoretical difficulties and practical challenges which impede
a straightforward application of our proposed approach. On the theoretical side, the NTK theory
is only developed in supervised learning setting, and its extension to unsupervised learning is not
established yet. Though Fisher kernel is immediately applicable in unsupervised learning setting,
deriving Fisher vector from supervised learning model p(y | x; θ) can be tricky, which needs the
log-density estimation of marginal distribution pθ (x) from p(y | x; θ). Note that it is a drastically
different problem from previous works (Achille et al., 2019) where Fisher kernel is applied to the
joint distribution over p(x, y). On the practical efficiency side, the dimensionality of the feature
space associated with NTK or FK is same as the number of model parameters ∣θ∣, which poses
unmanageably high time and space complexity when it comes to modern large-scale neural network
models. Additionally, the size of the NTK scales quadratically with the number of classes in
multi-class supervised learning setting, which gives rise to more efficiency concerns.
To address the kernel formulation issue, we propose Neural Fisher Kernel (NFK) in Sec. 3.1 as
a unified kernel for both supervised and unsupervised learning models. To tackle the efficiency
challenge, we investigate the structural properties of the proposed NFK and propose a highly scalable
low-rank kernel approximation algorithm in Sec. 3.2 to extract compact low-dimensional feature
representation from NFK.
3.1	Neural Fisher Kernel
In this section, we propose Neural Fisher Kernel (NFK) as a principled and general kernel formulation
for neural network models. The key intuition is that we can extend classical Fisher kernel theory
to unify the procedure of deriving Fisher vector from supervised learning models and unsupervised
learning models by using Energy-based Model (EBM) formulation.
3
Published as a conference paper at ICLR 2022
Figure 2: Left: The spectrum structure of NFKs from a CNN (green) and a MLP (red), trained on
MNIST binary classification task. The NFK of CNN concentrates on fewer eigen-modes compared
to the MLP. Right: The low-rankness of the NFK on a DCGAN trained on MNIST. For a trained
model, the first 100 principle components of the Fisher Vector matrix explain 99.5% of all variances.
An untrained model with the same architecture on the other hand, demonstrates a much lower degree
of low-rankness.
3.1.1	Unsupervised NFK
We consider unsupervised probabilistic generative models pθ (x) = p(x; θ) here. Our proposed NFK
formulation can be applied to all generative models with tractable evaluation (or approximation) of
Vθ log Pθ (x).
GANs. We consider the EBM formulation of GANs (Dai et al., 2017; Zhai et al., 2019; Che et al.,
2020). Given pre-trained GAN model, we use D(x; θ) to denote the output of the discriminator D,
and use G(h) to denote the output of generator G given latent code h 〜p(h). AS a brief recap,
GANs can be interpreted as an implementation of EBM training with a variational distribution, where
we have the energy-function E(x; θ) = -D(x; θ). Please refer to (Zhai et al., 2019; Che et al.,
2020) for more details. Thus We have the unnormalized density function pθ (x) (X e-E(X*)given by
the GAN model. Following (Zhai et al., 2019), we can then derive the Fisher kernel Knfk and Fisher
vector from standard GANs as shoWn beloW:
Knfk(X,z) = hVx ,Vz i	Vx = (diag(I)-1 )Ux
Ux = VθD(x; θ) - Eh~p(hNθD(G(h); θ)
(1)
where x, Z ∈ X, I = Eh~p(h) [uG(h) U>(hJ . Note that we use diagonal approximation of FIM
throughout this Work for the consideration of scalability.
VAEs. Given a VAE model pre-trained via maximizing the variational lower-bound ELBO
LELBO(x) ≡ Eq(h∣x) Dog p(X∣X) ], We Can approximate the marginal log-likelihood logpθ(θ) by
evaluating LELBO(x) via Monte-Carlo estimations or importance sampling techniques (Burda et al.,
2016). Thus we have our NFK formulation as
Knfk(X,z) = hVx ,Vz i	Vx = (diag(I)-1 )Ux
Ux ≈ VθLELBO (x)
(2)
where x, z ∈ X, I = Eχ~p° ⑺[UχU>].
Flow-based Models, Auto-Regressive Models. For generative models with explicit exact data
density modeling pθ (x), we can simply apply the classical Fisher kernel formulation in Sec. 2.
3.1.2	Supervised NFK
In the supervised learning setting, we consider conditional probabilistic models pθ (y | x) =
p(y | x; θ). In particular, we focus on classification problems where the conditional prob-
ability is parameterized by a softmax function over the logits output f (x; θ): pθ (y | x) =
exp(f y (x; θ))/ y exp(f y (x; θ)), where y is a discrete label and fy(x; θ) denotes y-th logit
output. We then borrow the idea from JEM (Grathwohl et al., 2020) and write out a joint en-
ergy function term over (x, y) as E(x, y; θ) = -fy (x; θ). It is easy to see that joint energy
yields exactly the same conditional probability, at the same time leading to a free energy function:
4
Published as a conference paper at ICLR 2022
Algorithm 1 Baseline method: compute low-rank NFK feature embedding
Input dataset D; pre-train NN model f(x; θ); NFK feature dimensionality k; test data input x?
Output low-rank NFK feature embedding e□fk(x*)
1:	compute Fisher vector for all data examples V = [VXJ ∈ RN×lθl
2:	compute kernel Gram matrix K = VV> ∈ RN ×N
3:	compute truncated eigen-decomposition K = Φdiag(Λ)Φ>, Φ ∈ RN×k
4:	kernel function evaluations between x* and all data examples K(x?, X) ≡ [K(x?, Xj)]NN=ι
5:	obtain e□fk(x*) ∈ Rk via Eq. 5 and Eq. 4
E(x; θ) = - log y exp(f y (x; θ)). It essentially reframes a conditional distribution over y given
x to an induced unconditional distribution over x, while maintaining the same conditional probability
pθ(y | x). This allows us to write out the NFK formulation as:
Knfk(x, Z) = hVX,VZi Vx = (diag(I 厂1 )Uχ
Ux = XPθ(y | x)Vθfy(x; θ) - Ex，〜Pθ(χθ) XPθ(y | x)Vθfy(x0; θ)	⑶
yy
where I = EX〜p® (χ) [UχU>], and pθ(x) is the normalized density corresponding to the free energy
Eθ , which could be sampled from via Markov chain Monte Carlo (MCMC) algorithm. In this work,
we use empirical data distribution as practical approximation.
3.2 NFK with Low-Rank Approximation
Fisher vector Vx is the linear feature embedding 夕(x) given by NFK Knfk(x, Z) = hVX, Vzi for
neural network model f(x; θ). However, straightforward application of NFK by using Vx as feature
representation suffers from scalability issue, since Vx ∈ Rlθl shares same dimensionality as the
number of parameters ∣θ∣. It is with that in mind that ∣θ∣ can be tremendously large considering
the scale of modern neural networks, it is unfortunately infeasible to directly leverage Vx as feature
representation.
Low-Rank Structure of NFK. Motivated by the Manifold Hypothesis of Data that it is widely
believed that real world high dimensional data lives in a low dimensional manifold (Roweis and Saul,
2000; Rifai et al., 2011a;b), we investigate the structure of NFKs and present empirical evidence that
NFKs of good models have low-rank spectral structure. Firstly, we start by examining supervised
learning models. We study the spectrum structure of the empirical NFK of trained neural networks
with different architectures. We trained a LeNet-5 (LeCun et al., 1998) CNN and a 3-layer MLP
network by minimizing binary cross entropy loss, and then compute the eigen-decomposition of
the NFK Gram matrix. We show the explained ratio plot in Figure 2. We see that the spectrum of
CNN NTK concentrates on fewer large eigenvalues, thus exhibiting a lower effective-rank structure
compared to the MLP, which can be explained by the fact that CNN has better model inductive
bias for image data domain. For unsupervised learning models, we trained a small unconditional
DCGAN (Radford et al., 2016) model on MNIST dataset. We compare the results of a fully
trained model against a randomly initialized model in Fig. 2 (note the logarithm scale of the x-axis).
Remarkably, the trained model demonstrates an extreme degree of low-rankness that top 100 principle
components explain over 99.5% of the overall variance, where 100 is two orders of magnitude smaller
than both number of examples and number of parameters in the discriminator. We include more
experimental results and discussions in appendix due to the space constraints.
Efficient Low-Rank Approximation of NFK. The theoretical insights and empirical evidence
presented above hint at a natural solution to address the challenge of high-dimensionality of
Vx ∈ Rlθl: we can turn to seek a low-rank approximation to the NFK. According to the Mer-
cer,s theorem (Mercer, 1909), for positive definite kernel K(x, Z) = (夕(x),夕(z)i we have
K(x, Z) = Pi∞=1 λiφi(x)φi(Z), x, Z ∈ X, where {(λi, φi)} are the eigenvalues and eigenfunc-
tions of the kernel K, with respect to the integral operator p(Z)K(x, Z)φi(Z) dZ = λiφi(x). The
linear feature embedding representation 夕(x) can thus be constructed from the orthonormal eigen-
basis {(λi, φi)} as 夕(x) ≡ [√λιφι(x), √λ2φ2(x),...] ≡ [√λiφi(x)] , i = 1,..., ∞. To obtain a
low-rank approximation, we only keep top-k largest eigen-basis {(λi, φi)} ordered by corresponding
eigenvalues λi to form the low-rank k-dimensional feature embedding e(x) ∈ Rk, k《 N, k《∣θ∣
e(x) ≡ [pλ1φ1(x), pλ2Φ2(x), .. . pλkΦk(x)] ≡ [pλiφi(x)j , i = 1,...,k
(4)
5
Published as a conference paper at ICLR 2022
Algorithm 2 Our proposed method: compute low-rank NFK feature embedding
1:	V = Φdiag(Σ)P>, P ∈ Rlθl×k using power iteration methods via JVP/VJP evaluations
2:	compute K (x, X)> Φi ≈ Vxdiag(Σi)Pi via JVP evaluation
3:	obtain e□fk(x*) ∈ Rk via Eq. 5 and Eq. 4
By applying our proposed NFK formulation Knfk to pre-trained neural network model f (x; θ), we
can obtain a compact low-dimensional feature representation enfk(x) ∈ Rk in this way. We call it
the low-rank NFK feature embedding.
We then illustrate how to estimate the eigenvalues and eigenfunctions of NFK Knfk from data. Given
dataset D, the Gram matrix K ∈ RN×N of kernel K is defined as K(xi, xj ) = K(xi, xj ). We
use X ≡ [xi]iN=1 to denote the matrix of all data examples, and use φi (X) ∈ RN to denote the
concatenated vector of evaluating i-th eigenfunction φi at all data examples. Then by performing
eigen-decomposition of the Gram matrix K = Φdiag(Λ)Φ>, the i-th eigenvector Φi ∈ RN and
eigenvalue Λi can be seen as unbiased estimation of the i-th eigenfunction φi and eigenvalue λ% of
the kernel K, evaluated at training data examples X, φi (X) ≈ √NΦi, λ% ≈ NΛ%. Based on these
estimations, we can thus approximate the eigenfunction φi via the integral operator by Monte-Carlo
estimation with empirical data distribution,
1N
λiφi(x) =	P(z)K(x, z)φi(z)dz ≈ Exj ∈pdata K(X, Xj )φj (Xj ) ≈ N EK(X, Xj )φji	⑸
j=1
Given new test data example X?, we can now approximate the eigenfunction evaluation φi (X?) by
the projection of kernel function evaluation results centered on training data examples K(X?, X) ≡
[K(X?, Xj)]jN=1onto the i-th eigenvector Φi of kernel Gram matrix K. We adopt this method as the
baseline approach for low-rank approximation, and present the baseline algorithm description in
Alg. 1.
However, due to the fact that it demands explicit computation and manipulation of the Fisher vector
matrix V ∈ RN×lθl and the Gram matrix K ∈ RN×n in Alg. 1, straightforward application of the
baseline approach, as well as other off-the-shelf classical kernel approximation (Williams and Seeger,
2000; Rahimi and Recht, 2007) and SVD methods (Halko et al., 2011), are practically infeasible to
scale to larger-scale machine learning settings, where both the number of data examples N and the
number of model parameters ∣θ∣ can be extremely large.
To tackle the posed scalability issue, we propose a novel highly efficient and scalable algorithm for
computing low-rank approximation of NFK. Given dataset D and model f(X; θ), We aim to compute
the truncated SVD of the Fisher vector matrix V = Φdiag(Σ)P>, P ∈ Rlθl×k. Based on the idea of
power methods (Golub and Van der Vorst, 2000; Bathe, 1971) for finding leading top eigenvectors, we
start from a random vector vo and iteratively construct the sequence vt+ι = 八次丁：；口. By leveraging
the special structure of V that it can be obtained from the Jacobian matrix Jθ (X) ∈ RN×lθl up
to element-wise linear transformation under the NFK formulation in Sec. 3, we can decompose
each iterative step into a Jacobian Vector Product (JVP) and a Vector Jacobian Product (VJP). With
modern automatic-differentiation techniques, we can evaluate both JVP and VJP efficiently, which
only requires the same order of computational costs of one vanilla backward-pass and forward-pass
of neural networks respectively. With computed truncated SVD results, we can approximate the
projection term in Eq. 5 by K (X, X)> Φi = VxV> Φi ≈ Vxdiag(Σi)Pi, which is again in the JVP
form so that we can pre-compute and store the truncated SVD results and evaluate the eigenfunction
of any test data example via one efficient JVP forward-pass. We describe our proposed algorithm
briefly in Alg. 2.
4	Experiments
In this section, we evaluate NFK in the following settings. We first evaluate the proposed low-rank
kernel approximation algorithm (Sec. 3.2), in terms of both approximation accuracy and running
time efficiency. Next, we evaluate NFK on various representation learning tasks in both supervised,
semi-supervised and unsupervised learning settings.
6
Published as a conference paper at ICLR 2022
2»	211	212	213	2m 215	216
#data examples
27 2β 2®	21°	211 2lz
#data examples
β0
三士
2,
WRN-40-2
#	GPU=I
#	GPU=2
#GPU=4
#	GPU=8
Approximation errors
VEn CO-WnUSaEOU
60
3z>≡e,Je> P3≡e-dxa

山s≡
Figure 3: Top row: Running time efficiency evaluation for truncated SVD algorithm on single GPU.
We vary the number of data examples used, shown in x-axis. y-axis denotes the wall-clock running
time (in seconds). Red crosses mark the cases when it is no longer possible for the baseline method to
obtain the results in an affordable waiting time and memory consumption. Bottom left: Running time
costs with different number of GPUs used in our distributed SVD implementation. Bottom right:
Approximation errors (in blue) of our proposed implementation for each eigenmode (in descending
order of eigenvalues), v.s. the explained variance (in red). Best viewed in color.
4.1	Quality and Efficiency of Low-rank NFK Approximations
We implement our proposed low-rank kernel approximation algorithm in Jax (Bradbury et al., 2018)
with distributed multi-GPU parallel computation support. For the baseline methods for comparison,
we first compute the full kernel Gram matrix using the neural-tangets (Novak et al., 2020)
library, and then use sklearn.decomposition.TruncatedSVD to obtain the truncated SVD
results. All model and algorithm hyper-parameters are included in the Appendix.
Computational Costs. We start by comparing running time costs of computing top NFK eigen-
vectors via truncated SVD. We use two models for the comparison, a DCGAN-like GAN model
in (Zhai et al., 2019) and a Wide ResNet (WRN) with 40 layers and 2 times wider than original
network (denoted as WRN-40-2). Please see appendix for the detailed description of hyper-parameters.
We observed that our proposed algorithm could achieve nearly linear time scaling, while the baseline
method would not be able to handle more than 214 data examples as the memory usage and time
complexity are too high to afford. We also see in Fig. 3 that by utilizing multi-GPU parallelism, we
achieved further speed-up which scales almost linearly with the number of GPUs. We emphasize that
given the number of desired eigenvectors, the time complexity of our method scales linearly with
the number of data examples and the demanded memory usage remains constant with adequate data
batch size, since explicit computation and storage of the full kernel matrix is never needed.
Approximation accuracy. We investigate the approximation error of our proposed low-rank approxi-
mation method. Since we did not introduce any additional approximations, our method shares the
same approximation error bound with the existing randomized SVD algorithm (Martinsson and Tropp,
2020; Halko et al., 2011) and would only expect differences compared to the baseline randomized
SVD algorithm up to numerical errors. To evaluate the quality of the low-rank kernel approximation,
we use LeNet-5 and compute its full NFK Gram matrix on MNIST dataset. Please see appendix for
detailed hyper-parameter setups. We show in Fig. 3 the approximation errors of top-128 eigenvalues
along with corresponding explained variances. We obtain less than 1e - 8 absolute error and less
than 1e - 7 relative error in top eigen-modes which explains most of the data.
7
Published as a conference paper at ICLR 2022
Table 1: CIFAR-10 accuracies of linear evaluation on top of representations learned with unsupervised
and self-supervised methods. NFK-128d denotes the 128 dimensional embeddings from the low-rank
approximation of the NFK (ie AFV). Remarkably, we can use 128 dimensions to exactly recover the
performance of the 5.9M dimensional Fisher Vectors.
Model	Acc	Category	#Features
Examplar CNN (Dosovitskiy et al., 2015)	84.3	Unsupervised	-
BiGAN (Mu et al., 2020)	70.5	Unsupervised	-
RotNet Linear (Gidaris et al., 2018)	81.8	Self-Supervised	〜25K
AET Linear (Zhang et al., 2019)	83.3	Self-Supervised	〜25K
VAE(MU et al., 2020)	61.5	Unsupervised	-
VAE-NFK-128d (ours)	63.2	Unsupervised	128
VAE-NFK-256d (ours)		68.7	Unsupervised	256
GAN-Supervised	92.7	Supervised	-
GAN-Activations	65.3	Unsupervised	-
GAN-AFV (Zhai et al., 2019)	89.1	Unsupervised	5.9M
GAN-AFV (re-implementation) (Zhai et al., 2019)	89.8	Unsupervised	5.9M
GAN-NFK-128d (ours)	89.8	Unsupervised	128
GAN-NFK-256d (ours)		89.8	Unsupervised	256
4.2	Low-rank NFK Embedding as Data Representations
In this section we evaluate NFK to answer the following: Q1. In line with the question raised in Sec. 1,
how does our proposed low-rank NFK embedding differ from the intermediate layer activations
for data representation? Q2. How does the low-rank NFK embedding compare to simply using
gradients (Jacobians) as data representation? Q3. To what extent can the low-rank NFK embedding
preserve the information in full Fisher vector? Q4. Does the NFK embedding representation lead
to better generalization performance in terms of better sample efficiency and faster adaptation?
We conduct comparative studies on different tasks to understand the NFK embedding and present
empirical observations to answer these questions in following sections.
NFK Representations from Unsupervised Generative Models. In order to examine the effective-
ness of the low-rank NFK embeddings as data representations in unsupervised learning setting, we
consider GANs and VAEs as representative generative models and compute the low-rank NFK embed-
dings. Then we adopt the linear probing protocol by training a linear classifier on top of the obtained
embeddings and report the classification performance to quantify the quality of NFK data representa-
tion. For GANs, we use the same pretrained GAN model from (Zhai et al., 2019) and reimplemented
the AFV baseline. For VAEs, we follow the same model architecture proposed in (Child, 2020). We
then apply the proposed truncated SVD algorithm with 10 power iterations to obtain the 256 dimen-
sional embedding via projection. We present our results on CIFAR-10 (Krizhevsky et al., 2009a) in
Table. 1. We use GAN-NFK-128d (GAN-NFK-256d) to denote the NFK embedding obtained from
using top-128 (top-256) eigenvectors in our GAN model. Our VAE models (VAE-NFK-128d and
VAE-NFK-256d) follow the same notations. For VAE baselines, the method proposed in (Mu et al.,
2020) combines both gradients features and activations-based features into one linear model, denoted
as VAE in the table. For GAN baselines, we first consider using intermediate layer activations only
as data representation, referred to as the GAN-Activations model. We then consider using full
Fisher vector as representation, namely using the normalized gradients w.r.t all model parameters
as features, denoted as the GAN-AFV model as proposed in (Zhai et al., 2019). Moreover, we also
compare our results against training whole neural network using data labels in a supervised learning
way, denoted as GAN-Supervised model.
As shown in Table. 1, by contrasting against the baseline GAN-AFV from GAN-Activations, as
well as validation in recent works (Zhai et al., 2019; Mu et al., 2020), gradients provide additional
useful information beyond layer activations based features. However, it would be impractical to use
all gradients or full Fisher vector as representation when scaling up to large-scale neural network
models. For example, VAE (Child, 2020) has 〜40M parameters, it would not be possible to apply
the baseline methods directly. Our proposed low-rank NFK embedding approach addressed this
challenge by building low-dim vector representation from efficient kernel approximation algorithm,
making it possible to utilize all model parameters’ gradients information by embedding it into a low-
dim vector, e.g. 256-dimensional embedding in VAE-NFK-2 5 6d from ~ 40M parameters. As our
low-rank NFK embedding is obtained by linear projections of full Fisher vectors, it naturally provides
answers for Q2 that the NFK embedding can be viewed as a compact yet informative representation
containing information from all gradients. We see from Table. 1 that by using top-128 eigenvectors,
8
Published as a conference paper at ICLR 2022
Table 2: Error rates of semi-supervised classification on CIFAR10 and SVHN, varying labels from 500
to 4000. NFK-128d yields extremely competitive performance, compared to other more sophisticated
baselines, Mixup (Zhang et al., 2018), VAT (Miyato et al., 2019), MeanTeacher(Tarvainen and Valpola,
2017), MixMatch (Berthelot et al., 2019), Improved GAN(Salimans et al., 2016), all are jointly learns
with labels, . Also note that the architecture used by MixMatch yields a 4.13% supervised learning
error rate, which is a much stronger than our supervised baseline (7.3%).
Model	Category	CIFAR-10	SVHN
		500	1000	2000	4000	500	1000	2000	4000
Mixup	Joint	36.17	25.72	18.14	13.15	29.62	16.79	10.47	7.96
VAT	Joint	26.11	18.68	14.40	11.05	7.44	5.98	4.85	4.20
MeanTeacher	Joint	47.32	42.01	17.32	12.17	6.45	3.82	3.75	3.51
MixMatch	Joint	9.65	7.75	7.03	6.24	3.64	3.27	3.04	2.89
Improved GAN	Joint	-	19.22	17.25	15.59	18.44	8.11	6.16	-
NFK-128d (ours)	Pretrained	20.68	14.77	13.82	12.95	8.74	4.47	3.82	3.19
Table 3: Supervised knowledge distillation results (classification accuracy on test dataset) on CIFAR10
against baseline methods KD (Hinton et al., 2015), FitNet (Romero et al., 2015), AT (Zagoruyko and
Komodakis, 2017), NST (Huang and Wang, 2017), VID-I (Ahn et al., 2019), numbers are from (Ahn
et al., 2019).
	Teacher	Student	KD	FitNet	AT	NST	VID-I	NFKD (ours)
ACC	94.26	90.72	91.27	90.64	91.60	91.16	91.85	92.42
the low-rank NFK embedding is able to recover the performance of full 〜5.9M-dimension Fisher
vector, which provides positive evidence for Q3 that we can preserve most of the useful information
in Fisher vector by taking advantage of the low-rank structure of NFK spectrum.
NFK Representations for Semi-Supervised Learning. We then test the low-rank NFK embeddings
in the semi-supervised learning setting. Following the standard semi-supervised learning benchmark
settings (Berthelot et al., 2019; Miyato et al., 2019; Laine and Aila, 2017; Sajjadi et al., 2016;
Tarvainen and Valpola, 2017), we evaluate our method on CIFAR-10 (Krizhevsky et al., 2009a) and
SVHN datasets (Krizhevsky et al., 2009b). We treat most of the dataset as unlabeled data and use
few examples as labeled data. We use the same GAN model as the unsupervised learning setting
above, and compute top-128 eigenvectors using training dataset (labeled and unlabeled) to derive the
128-dimensional NFK embedding. Then we only use the labeled data to train a linear classifier on top
of the NFK embedding features, denoted as the NFK-128d model. We vary the number of labeled
training examples and report the results in Table. 2, in comparison with other baseline methods.
We see that NFK-128d achieves very competitive performance. On CIFAR-10, NFK-128d is
only outperformed by the state-of-the-art semi-supervised learning algorithm MixMatch (Berthelot
et al., 2019), which also uses a stronger architecture than ours. The results on SVHN are mixed
though NFK-128d is competitive with the top performing approaches. The results demonstrated the
effectiveness of NFK embeddings from unsupervised generative models in semi-supervised learning,
showing promising sample efficiency for Q4.
NFK Representations for Knowledge Distillation. We next test the effectiveness of using low-rank
NFK embedding for knowledge distillation in the supervised learning setting. We include more
details of the distillation method in the Appendix. Our experiments are conducted on CIFAR10, with
a teacher set as the WRN-40-2 model and student being WRN-16-1. After training the teacher, we
compute the low-rank approximation of NFK of the teacher model, using top-20 eigenvectors. We
include more details about the distillation method setup in the Appendix. Our results are reported in
Table. 3. We see that our method achieves superior results compared to other competitive baseline
knowledge distillation methods, which mainly use the logits and activations from teacher network as
distillation target.
5	Conclusions
In this work, we propose a novel principled approach to representation extraction from pre-trained
neural network models. We introduce NFK by extending the Fisher kernel to neural networks in
both unsupervised learning and superevised learning settings, and propose a novel low-rank kernel
approximation algorithm, which allows us to obtain a compact feature representation in a highly
efficient and scalable way.
9
Published as a conference paper at ICLR 2022
References
Tommi S. Jaakkola and David Haussler. Exploiting generative models in discriminative classifiers.
In Michael J. Kearns, Sara A. Solla, and David A. Cohn, editors, Advances in Neural Information
Processing Systems 11, [NIPS Conference, Denver, Colorado, USA, November 30 - December
5, 1998], pages 487-493. The MIT Press, 1998. URL http://papers.nips.cc/paper/
1520-exploiting-generative-models-in-discriminative-classifiers.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,
Las Vegas, NV, USA, June 27-30, 2016, pages 770-778. IEEE Computer Society, 2016. doi:
10.1109/CVPR.2016.90. URL https://doi.org/10.1109/CVPR.2016.90.
Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network.
CoRR, abs/1503.02531, 2015. URL http://arxiv.org/abs/1503.02531.
Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint
arXiv:1406.2661, 2014. URL https://arxiv.org/abs/1406.2661.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and
Yann LeCun, editors, 2nd International Conference on Learning Representations, ICLR 2014,
Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. URL http:
//arxiv.org/abs/1312.6114.
Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.
Generative pretraining from pixels. In Proceedings of the 37th International Conference on
Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of
Machine Learning Research, pages 1691-1703. PMLR, 2020a. URL http://proceedings.
mlr.press/v119/chen20s.html.
Fangzhou Mu, Yingyu Liang, and Yin Li. Gradients as features for deep representation learning. In
8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,
April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=
BkeoaeHKDS.
Arthur Jacot, Clement Hongler, and Franck Gabriel. Neural tangent kernel: Convergence and
generalization in neural networks. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle,
Kristen Grauman, Nicold Cesa-Bianchi, and Roman Garnett, editors, Advances in Neu-
ral Information Processing Systems 31: Annual Conference on Neural Information Pro-
Cessing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada, pages
8580-8589, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/
5a4be1fa34e62bb8a6ec6b91d2462f5a- Abstract.html.
Thomas Hofmann, Bernhard Scholkopf, and Alexander J Smola. Kernel methods in machine learning.
The annals of statistics, pages 1171-1220, 2008.
Radford M Neal. Priors for infinite networks. In Bayesian Learning for Neural Networks, pages
29-53. Springer, 1996.
Christopher K. I. Williams. Computing with infinite networks. In Michael Mozer, Michael I.
Jordan, and Thomas Petsche, editors, Advances in Neural Information Processing Systems 9,
NIPS, Denver, CO, USA, December 2-5, 1996, pages 295-301. MIT Press, 1996. URL http:
//papers.nips.cc/paper/1197-computing-with-infinite-networks.
Nicolas Le Roux and Yoshua Bengio. Continuous neural networks. In Marina Meila and Xiaotong
Shen, editors, Proceedings of the Eleventh International Conference on Artificial Intelligence
and Statistics, AISTATS 2007, San Juan, Puerto Rico, March 21-24, 2007, volume 2 of JMLR
Proceedings, pages 404-411. JMLR.org, 2007. URL http://proceedings.mlr.press/
v2/leroux07a.html.
Tamir Hazan and Tommi S. Jaakkola. Steps toward deep kernel methods from infinite neural networks.
CoRR, abs/1508.05133, 2015. URL http://arxiv.org/abs/1508.05133.
10
Published as a conference paper at ICLR 2022
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. Deep neural networks as gaussian processes. In 6th International Conference on
Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference
Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=
B1EA-M-0Z.
Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahramani.
Gaussian process behaviour in wide deep neural networks. In 6th International Conference on
Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference
Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=
H1-nGgWC-.
Lin Chen and Sheng Xu. Deep neural tangent kernel and laplace kernel have the same RKHS. In
9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria,
May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=
vK9WrZ0QYQ.
Amnon Geifman, Abhay Kumar Yadav, Yoni Kasten, Meirav Galun, David W. Jacobs, and
Ronen Basri. On the similarity between the laplace and neural tangent kernels. In
Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-
Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
1006ff12c465532f8c574aeaa4461b16-Abstract.html.
Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to understand
kernel learning. In Jennifer G. Dy and Andreas Krause, editors, Proceedings of the 35th Interna-
tional Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July
10-15, 2018, volume 80 of Proceedings ofMachine Learning Research, pages 540-548. PMLR,
2018. URL http://proceedings.mlr.press/v80/belkin18a.html.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural
networks outperform kernel methods? In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell,
Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/
2020/hash/a9df2255ad642b923d95503b9a7958d8- Abstract.html.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural
networks: The power of initialization and a dual view on expressivity. In Daniel D.
Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, edi-
tors, Advances in Neural Information Processing Systems 29: Annual Conference on Neu-
ral Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages
2253-2261, 2016. URL https://proceedings.neurips.cc/paper/2016/hash/
abea47ba24142ed16b7d8fbf2c740e0d- Abstract.html.
Florent Perronnin and Christopher R. Dance. Fisher kernels on visual vocabularies for image
categorization. In 2007 IEEE Computer Society Conference on Computer Vision and Pattern
Recognition (CVPR 2007), 18-23 June 2007, Minneapolis, Minnesota, USA. IEEE Computer
Society, 2007. doi: 10.1109/CVPR.2007.383266. URL https://doi.org/10.1109/CVPR.
2007.383266.
Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: non-linear independent components
estimation. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning
Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings,
2015. URL http://arxiv.org/abs/1410.8516.
Aaron van den Oord, Nal Kalchbrenner, and Koray KavUkcUoglu. Pixel recurrent neural networks.
In Maria-Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of the 33nd International
Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016,
volume 48 of JMLR Workshop and Conference Proceedings, pages 1747-1756. JMLR.org, 2016.
URL http://proceedings.mlr.press/v48/oord16.html.
11
Published as a conference paper at ICLR 2022
Ruslan Salakhutdinov. Deep learning. In Sofus A. Macskassy, Claudia Perlich, Jure Leskovec,
Wei Wang, and Rayid Ghani, editors, The 20th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ’14, New York, NY, USA - August 24 - 27, 2014, page
1973. ACM, 2014. doi: 10.1145/2623330.2630809. URL https://doi.org/10.1145/
2623330.2630809.
Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless C.
Fowlkes, Stefano Soatto, and Pietro Perona. Task2vec: Task embedding for meta-learning. In
2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South),
October 27 - November 2, 2019, pages 6429-6438. IEEE, 2019. doi: 10.1109/ICCV.2019.00653.
URL https://doi.org/10.1109/ICCV.2019.00653.
Zihang Dai, Amjad Almahairi, Philip Bachman, Eduard H. Hovy, and Aaron C. Courville. Calibrating
energy-based generative adversarial networks. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.
OpenReview.net, 2017. URL https://openreview.net/forum?id=SyxeqhP9ll.
Shuangfei Zhai, Walter Talbott, Carlos Guestrin, and Joshua M. Susskind. Adversarial fisher vec-
tors for unsupervised representation learning. In Hanna M. Wallach, Hugo Larochelle, Alina
Beygelzimer, Florence d'Alch6-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in
Neural Information Processing Systems 32: Annual Conference on Neural Information Pro-
cessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages
11156-11166, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/
7e1cacfb27da22fb243ff2debf4443a0-Abstract.html.
Tong Che, Ruixiang Zhang, Jascha Sohl-Dickstein, Hugo Larochelle, Liam Paull, Yuan Cao, and
Yoshua Bengio. Your GAN is secretly an energy-based model and you should use discriminator
driven latent sampling. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina
Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/
hash/90525e70b7842930586545c6f1c9310c-Abstract.html.
Yuri Burda, Roger B. Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. In
Yoshua Bengio and Yann LeCun, editors, 4th International Conference on Learning Representa-
tions, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016.
URL http://arxiv.org/abs/1509.00519.
Will Grathwohl, Kuan-Chieh Wang, Jorn-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi,
and Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like
one. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/
forum?id=Hkxzx0NtDB.
Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embedding.
science, 290(5500):2323-2326, 2000.
Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive auto-
encoders: Explicit invariance during feature extraction. In Lise Getoor and Tobias Scheffer,
editors, Proceedings of the 28th International Conference on Machine Learning, ICML 2011,
Bellevue, Washington, USA, June 28 - July 2, 2011, pages 833-840. Omnipress, 2011a. URL
https://icml.cc/2011/papers/455_icmlpaper.pdf.
Salah Rifai, Yann N. Dauphin, Pascal Vincent, Yoshua Bengio, and Xavier Muller. The man-
ifold tangent classifier. In John Shawe-Taylor, Richard S. Zemel, Peter L. Bartlett, Fer-
nando C. N. Pereira, and Kilian Q. Weinberger, editors, Advances in Neural Information
Processing Systems 24: 25th Annual Conference on Neural Information Processing Sys-
tems 2011. Proceedings of a meeting held 12-14 December 2011, Granada, Spain, pages
2294-2302, 2011b. URL https://proceedings.neurips.cc/paper/2011/hash/
d1f44e2f09dc172978a4d3151d11d63e- Abstract.html.
12
Published as a conference paper at ICLR 2022
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324,1998.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. In Yoshua Bengio and Yann LeCun, editors, 4th
International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May
2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1511.
06434.
J Mercer. Functions ofpositive and negativetypeand theircommection with the theory ofintegral
equations. Philos. Trinsdictions Rogyal Soc, 209:4-415, 1909.
Christopher K. I. Williams and Matthias W. Seeger. Using the nystrom method to
speed up kernel machines. In Todd K. Leen, Thomas G. Dietterich, and Volker
Tresp, editors, Advances in Neural Information Processing Systems 13, Papers from Neu-
ral Information Processing Systems (NIPS) 2000, Denver, CO, USA, pages 682-688.
MIT Press, 2000. URL https://proceedings.neurips.cc/paper/2000/hash/
19de10adbaa1b2ee13f77f679fa1483a-Abstract.html.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In John C.
Platt, Daphne Koller, Yoram Singer, and Sam T. Roweis, editors, Advances in Neural Information
Processing Systems 20, Proceedings of the Twenty-First Annual Conference on Neural Information
Processing Systems, Vancouver, British Columbia, Canada, December 3-6, 2007, pages 1177-
1184. Curran Associates, Inc., 2007. URL https://proceedings.neurips.cc/paper/
2007/hash/013a006f03dbc5392effeb8f18fda755- Abstract.html.
Nathan Halko, Per-Gunnar Martinsson, and Joel A. Tropp. Finding structure with randomness:
Probabilistic algorithms for constructing approximate matrix decompositions. SIAM Rev., 53(2):
217-288, 2011. doi: 10.1137/090771806. URL https://doi.org/10.1137/090771806.
Gene H Golub and Henk A Van der Vorst. Eigenvalue computation in the 20th century. Journal of
Computational and Applied Mathematics, 123(1-2):35-65, 2000.
Klaus-Jurgen Bathe. Solution methods for large generalized eigenvalue problems in structural
engineering. National Technical Information Service, US Department of Commerce, 1971.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and
Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL
http://github.com/google/jax.
Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A. Alemi, Jascha Sohl-Dickstein,
and Samuel S. Schoenholz. Neural tangents: Fast and easy infinite neural networks in python. In
8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,
April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=
SklD9yrFPS.
Per-Gunnar Martinsson and Joel A. Tropp. Randomized numerical linear algebra: Foundations
and algorithms. Acta Numer., 29:403-572, 2020. doi: 10.1017/S0962492920000021. URL
https://doi.org/10.1017/S0962492920000021.
Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox.
Discriminative unsupervised feature learning with exemplar convolutional neural networks. IEEE
transactions on pattern analysis and machine intelligence, 38(9):1734-1747, 2015.
Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by pre-
dicting image rotations. In 6th International Conference on Learning Representations, ICLR 2018,
Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net,
2018. URL https://openreview.net/forum?id=S1v4N2l0-.
Liheng Zhang, Guo-Jun Qi, Liqiang Wang, and Jiebo Luo. AET vs. AED: unsupervised
representation learning by auto-encoding transformations rather than data. In IEEE Conference
on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20,
13
Published as a conference paper at ICLR 2022
2019. Computer Vision Foundation / IEEE, 2019. doi: 10.1109/CVPR.2019.00265. URL
http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_AET_
vs._AED_Unsupervised_Representation_Learning_by_Auto-Encoding_
Transformations_Rather_CVPR_2019_paper.html.
Rewon Child. Very deep vaes generalize autoregressive models and can outperform them on images.
CoRR, abs/2011.10650, 2020. URL https://arxiv.org/abs/2011.10650.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009a.
Hongyi Zhang, MoustaPha Cisse, Yann N. Dauphin, and David LoPez-Paz. mixup: Beyond empirical
risk minimization. In 6th International Conference on Learning Representations, ICLR 2018,
Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net,
2018. URL https://openreview.net/forum?id=r1Ddp1-Rb.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training:
A regularization method for supervised and semi-supervised learning. IEEE Trans. Pattern
Anal. Mach. Intell., 41(8):1979-1993, 2019. doi:10.1109/TPAMI.2018.2858821. URL https:
//doi.org/10.1109/TPAMI.2018.2858821.
Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged con-
sistency targets improve semi-supervised deep learning results. In Isabelle Guyon, Ulrike von
Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman
Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on
Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages
1195-1204, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/
68053af2923e00204c3ca7c6a3150cf7- Abstract.html.
David Berthelot, Nicholas Carlini, Ian J. Goodfellow, Nicolas Papernot, Avital Oliver, and Colin
Raffel. Mixmatch: A holistic approach to semi-supervised learning. In Hanna M. Wallach, Hugo
Larochelle, Alina Beygelzimer, Florence d’Alche-Buc, Emily B. Fox, and Roman Garnett, editors,
Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information
Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages
5050-5060, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/
1cd138d0499a68f4bb72bee04bbec2d7-Abstract.html.
Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg,
Isabelle Guyon, and Roman Garnett, editors, Advances in Neural Information Processing Systems
29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016,
Barcelona, Spain, pages 2226-2234, 2016. URL https://proceedings.neurips.cc/
paper/2016/hash/8a3363abe792db2d8761d6403605aeb7-Abstract.html.
Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. In 5th Interna-
tional Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017,
Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/
forum?id=BJ6oOfqge.
Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic
transformations and perturbations for deep semi-supervised learning. In Daniel D. Lee,
Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors,
Advances in Neural Information Processing Systems 29: Annual Conference on Neural
Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages
1163-1171, 2016. URL https://proceedings.neurips.cc/paper/2016/hash/
30ef30b64204a3088a26bc2e6ecf7602- Abstract.html.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009b.
Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and
Yoshua Bengio. Fitnets: Hints for thin deep nets. In Yoshua Bengio and Yann LeCun, editors, 3rd
14
Published as a conference paper at ICLR 2022
International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,
2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6550.
Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the perfor-
mance of convolutional neural networks via attention transfer. In 5th International Conference on
Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Pro-
ceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id=Sks9_
ajex.
Zehao Huang and Naiyan Wang. Like what you like: Knowledge distill via neuron selectivity transfer.
CoRR, abs/1707.01219, 2017. URL http://arxiv.org/abs/1707.01219.
Sungsoo Ahn, Shell Xu Hu, Andreas C. Damianou, Neil D. Lawrence, and Zhenwen Dai.
Variational information distillation for knowledge transfer. In IEEE Conference on Computer
Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages
9163-9171. Computer Vision Foundation / IEEE, 2019. doi: 10.1109/CVPR.2019.00938.
URL http://openaccess.thecvf.com/content_CVPR_2019/html/Ahn_
Variational_Information_Distillation_for_Knowledge_Transfer_
CVPR_2019_paper.html.
Aristide Baratin, Thomas George, Cesar Laurent, R. Devon Hjelm, Guillaume Lajoie, Pascal Vincent,
and Simon Lacoste-Julien. Implicit regularization via neural feature alignment. In Arindam
Banerjee and Kenji Fukumizu, editors, The 24th International Conference on Artificial Intelligence
and Statistics, AISTATS 2021, April 13-15, 2021, Virtual Event, volume 130 of Proceedings of
Machine Learning Research, pages 2269-2277. PMLR, 2021. URL http://proceedings.
mlr.press/v130/baratin21a.html.
Vardan Papyan. Traces of class/cross-class structure pervade deep learning spectra. Journal of
Machine Learning Research, 21(252):1-64, 2020.
Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model alignment
explain generalization in kernel regression and infinitely wide neural networks, 2020.
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol, and
Leon Bottou. Stacked denoising autoencoders: Learning useful representations in a deep network
with a local denoising criterion. Journal of machine learning research, 11(12), 2010.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding. arXiv preprint arXiv:1807.03748, 2018. URL https://arxiv.org/abs/1807.
03748.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for
contrastive learning of visual representations. In Proceedings of the 37th International Conference
on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of
Machine Learning Research, pages 1597-1607. PMLR, 2020b. URL http://proceedings.
mlr.press/v119/chen20j.html.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for
unsupervised visual representation learning. In 2020 IEEE/CVF Conference on Computer Vision
and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 9726-9735.
IEEE, 2020. doi: 10.1109/CVPR42600.2020.00975. URL https://doi.org/10.1109/
CVPR42600.2020.00975.
R. Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Philip Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. In 7th International Conference on Learning Representations, ICLR 2019,
New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.
net/forum?id=Bklr3j0cKX.
Ben Poole, Sherjil Ozair, Aaron van den Oord, Alex Alemi, and George Tucker. On variational bounds
of mutual information. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of
the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach,
California, USA, volume 97 of Proceedings of Machine Learning Research, pages 5171-5180.
PMLR, 2019. URL http://proceedings.mlr.press/v97/poole19a.html.
15
Published as a conference paper at ICLR 2022
Ruixiang Zhang, Masanori Koyama, and Katsuhiko Ishiguro. Learning structured latent factors from
dependent data:a generative model framework from information-theoretic perspective. In Proceed-
ings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020,
Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 11141-11152.
PMLR, 2020. URL http://proceedings.mlr.press/v119/zhang20m.html.
Longlong Jing and Yingli Tian. Self-supervised visual feature learning with deep neural networks: A
survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.
Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In Zoubin Ghahra-
mani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger, editors,
Advances in Neural Information Processing Systems 27: Annual Conference on Neural In-
formation Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages
2654-2662, 2014. URL https://proceedings.neurips.cc/paper/2014/hash/
ea8fcd92d59581717e06eb187f10666d- Abstract.html.
Frederick Tung and Greg Mori. Similarity-preserving knowledge distillation. In 2019 IEEE/CVF
International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27
- November 2, 2019, pages 1365-1374. IEEE, 2019. doi: 10.1109/ICCV.2019.00145. URL
https://doi.org/10.1109/ICCV.2019.00145.
Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. In 8th
International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,
April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=
SkgpBJrtvS.
Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251-276,
1998.
Ryo Karakida and Kazuki Osawa. Understanding approximate fisher information for fast
convergence of natural gradient descent in wide neural networks. In Hugo Larochelle,
Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, edi-
tors, Advances in Neural Information Processing Systems 33: Annual Conference on
Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
7b41bfa5085806dfa24b8c9de0ce567f-Abstract.html.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Richard C. Wilson, Edwin R.
Hancock, and William A. P. Smith, editors, Proceedings of the British Machine Vision Conference
2016, BMVC 2016, York, UK, September 19-22, 2016. BMVA Press, 2016. URL http://www.
bmva.org/bmvc/2016/papers/paper087/index.html.
16
Published as a conference paper at ICLR 2022
A Extended Preliminaries
We extend Sec. 2 to introduce additional technical background and related work.
Kernel methods in Deep Learning. Popularized by the NTK work Jacot et al. (2018), there has
been great interests in the deep learning community around the kernel view of neural networks. In
particular, several works have studied the low-rank structure of the NTK, including (Baratin et al.,
2021; Papyan, 2020; Canatar et al., 2020), which demonstrate that empirical NTK demonstrates
low-rankness and that encourages better generalization theoretically. Our low-rank analysis of NFK
shares a similar flavor, but generalizes across supervised and unsupervised learning settings. Besides,
we make an explicit effort in proposing an efficient implementation of the low-rank approximation,
and demonstrate strong empirical performances.
Unsupervised/self supervised representation learning. Unsupervised representation learning is an
old idea in deep learning. A large body of work is dedicated to designing better learning objectives
(self supervised learning), including denoising (Vincent et al., 2010), contrastive learning (Oord et al.,
2018; Chen et al., 2020b; He et al., 2020), mutual information based methods (Hjelm et al., 2019;
Poole et al., 2019; Zhang et al., 2020) and other “pretext tasks" Jing and Tian (2020). Our attempt
falls into the same category of unsupervised representation learning, but differs in that we instead
focus on effectively extracting information from a standard probabilistic model. This makes our effort
orthogonal to many of the related works, and can be easily plugged into different family of models.
Knowledge Distillation. Knowledge distillation (KD) is generally concerned about the problem of
supervising a student model with a teacher model (Hinton et al., 2015; Ba and Caruana, 2014). The
general form of KD is to directly match the statistics of one or a few layers (default is the logits).
Various works have studied the layer selection (Romero et al., 2015) or loss function design aspects
(Ahn et al., 2019). More closely related to our work is efforts that consider the second order statistics
between examples, including (Tung and Mori, 2019; Tian et al., 2020). NFKD differs in that we
represent the teacher’s knowledge in the kernel space, which is directly tied to the kernel interpretation
of neural networks which introduces different inductive biases than layerwise representations.
Neural Tangent Kernel. Recent advancements in the understanding of neural networks have shed
light on the connection between neural network training and kernel methods. In (Jacot et al., 2018),
it is shown that one can use the Neural Tangent Kernel (NTK) to characterize the full training of a
neural network using a kernel. Let f(θ; x) denote a neural network function with parameters θ. The
NTK is defined as follows:
Kntk(x, Z) = Eθ〜Pθ hVθf (θ; x), Vθf (θ; z)i.	(6)
where Pθ is the probability distribution of the initialization of θ. (Jacot et al., 2018) further
demonstrates that in the large width regime, a neural network undergoing training under gradi-
ent descent essentially evolves as a linear model. Let θ0 denote the parameter values at ini-
tialization. To determine how the function ft (θt; x) evolves, we may naively taylor expand
the output around θ°: ft+1(θt+1; x) ≈ ft(θt; x) — ηVθtft(θt; x)>(θt+ι — θt). As the weight
updates are given by θt-ι 一 θt = 一得η Pm=I V%Lt(xi), hence We have ft+1(θt+1; x) ≈
ft(θt; χ) — η N PN=I Kntk(χ, χi)Vf Lt(Xi).
The significance of the NTK stems from two observations. 1) When suitably initialized, the NTK
converges to a limit kernel when the width tends to infinity limwidth→∞ Kntk(x, z; θ0) = Kntk(x, z).
2) In that limit, the NTK remains frozen in its limit state throughout training.
B On the Connections between NFK and NTK
In Sec 3.1, we showed that our definition of NFK in the supervised learning setting bares great
similarity to the NTK. We provide more discussion here on the connections between NFK and NTK.
For the L2 regression loss function, the empirical fisher information reduces to I =
N PN=I Vθfθ(x)Vθfθ(x)>. Note that the fisher information matrix I is give by a covariance
matrix of J, while the NTK matrix is defined as the Gram matrix of J, where J is the Jacobian matrix,
implying they share the same spectrum, and that the NTK and the NFK share the same eigenvectors.
The addition ofI-1 in the definition of Knfk can be seen as a form of conditioning, facilitating fast
convergence in all directions spanned by J.
17
Published as a conference paper at ICLR 2022
Algorithm 3 Our proposed method: compute low-rank NFK feature embedding
1:	V =	Φdiag(Σ)P>, P	∈	Rlθl×k via truncated_svd(X, fθ, toρk=K,
kernel_type="NFK")
2:
3:	compute K (x, X)> Φi ≈ Vxdiag(Σi)Pi via JVP evaluation
4:	obtain e□fk(x*) ∈ Rk via Eq. 5 and Eq. 4
Equation 3 also has immediate connections to NTK. In NTK, the kernel Kntk(x, x) ∈ RN×N is
a matrix which measures the dot product of Jacobian for every pair of logits. The NFK, on the
other hand, reduces the Jacobian Vθfθ (x) for each example X to a single vector of dimension
n (i.e., size of θ), weighted by the predicted probability of each class pθ(y|x). The other no-
table difference between NFK and NTK is the subtractive and normalize factors, represented by
Eχ0~PθX) PyPθ(y∣x)Vθfθ(x0) and I, respectively. This distinction is related to the difference
between Natural Gradient Descent (Amari, 1998; Karakida and Osawa, 2020) and gradient descent.
In a nutshell, our definition of NFK in the supervised learning setting can be considered as a reduced
version of NTK, with proper normalization. These properties make NFK much more scalable w.r.t.
the number of classes, and also less sensitive to the scale of model’s parameters.
To better see this, We can define an “unnormalized" version of NFK as Ku(x, X) = [Py pθ (y |
x)Vθfθ(x)> Pypθ(y | x)Vθfθ(x). It is easy to see that Ku has the same rank as the original
NFK K, as I-1 is full rank by definition. We can then further rewrite it as
Ku(x, x)= XXpθ(y | x)pθ(y|X)Vθfθ(X)>vθfθ(X) = XXpθ(y | x)pθ(y | X)Kyty(x,x)
y y	y y	(7)
In words, the unnormalized version of NFK can be considered as a reduction of NTK, where the
weights of each element is weighte by the predicted probability for the respective class. If we further
assume that the model of interest is well trained, as is often the case in knowledge distillation, we
_■«，* 'T^*
can approximate the Ku as Kytky (x, X), where y* = arg maxy pθ(y | x) and likewise for y*. ThiS
suggests that the unnormalized NFK can roughly viewd as a downsampled version of NTK. As a
result, we expect the unnormalized NFK (and hence the NFK) to exhibit similar low rank properties
as demonstrated in the NTK literature.
On the low-rank structure of NTK. Consider the NTK Gram matrix Kntk ∈ RN ×N of some
network Given the dataset {xi}iN=1 (for simplicity we assume a scalar output) and its eigen decompo-
sition Kntk = Pjm=1 λjujuj>. Let f ∈ RN denote the concatenated outputs. Under GD in the linear
regime, the outputs ft evolves according to:
∀j , uj> (ft+1 - ft) ≈ -ηλj uj>Vf L.	(8)
The updates ft+1 - ft projected onto the bases of the kernel therefore converge at different speeds,
determined by the eigenvalues {λj }. Intuitively, a good kernel-data alignment means that the VfL is
spanned by a few eigenvectors with large corresponding eigenvalues, speeding up convergence and
promoting generalization.
C Neural Fisher Kernel with Low-Rank Approximation
C.1 Neural Fisher Kernel Formulation
We provide detailed derivations of the various NFK formulations presented in Section. 3.
NFK for Energy-based Models. Consider an Energy-based Model (EBM) pθ(x) = exp(zEχ")),
where E(x) is the energy function parametrized by θ and Z(θ) = exp(-E(x; θ)) dx is the
18
Published as a conference paper at ICLR 2022
Algorithm 4 truncated_svd, Truncated SVD Algorithm for Low-rank Kernel Approximation.
Comments are based on NTK for simplicity.
Input Dataset X ≡ {xi}iN=1
Input Neural network model fθ
Input Kernel type kernel, NFK or NTK
Input Low-rank embedding size K
Input Number of power iterations L = 10
Input Number of over samples U = 10
Output Truncated SVD of Jacobian Jθ (X) ≈ PkΣk Qk>
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
U = K + U	. Size of augmented set of vectors in power iterations
Draw random matrix Ω ∈ RN×U
Ω =matrix_jacobian_product (fθ, X, Ω, kernel)
for step = 1 to L do
Ω = jacobian_matrix_product (fθ, X, Ω, kernel)
Ω =matrix_jacobian_product (fθ, X, Ω, kernel)
Ω =qjdecomposition(Ω)
end for
B = jacobian_matrix_product (fθ, X, Ω, kernel)
P, Σ, Q> = svd(B>)
P = ΩP
.Ω = Jθ(X)Ω ∈ RM×u
.Ω = J>(X)Ω ∈ RN×u
.Ω = Jθ(X)Ω ∈ RM×u
.B = J>(X)Ω ∈ RN×u
Keep top rank-K vectors to obtain the truncated results Pk , Σk , Qk>
Return Pk , Σk , Qk>
Algorithm 5 jacobian_matrix_product
Input Neural network model fθ
Input Input data X ∈ RB×D, where B is batch size
Input Input matrix M
Input Kernel type kernel, NFK or NTK
Output Jθ>(X)M for NTK, Fisher-vector-matrix-product Vθ>(X)M for NFK
1:	jmp_fn = jax.vmap(jax.jvp)
2:	P =jmp_fn(fe, X, M)
3:	if kernel = "NFK" then
4:	P = diag(I)-2 (P - Z>M)
5:	end if
6:	Return P
partition function, we could apply the Fisher kernel formulation to derive the Fisher score Ux as
Ux = Vθ logpθ(x) = Vθ log [exp(-E(x; θ))] - Re log Z(θ)
=-VθE(x; θ) - Vθ log Z(θ)
=-VθE(x; θ) - Ex〜pθ(x)Vθ log [exp(-E(x; θ))]
=Ex〜Pθ(x) VθE(x; θ) - VθE(x; θ)
Then we can obtain the FIM I and the Fisher vector Vx from above results, shown as below
I = Ex〜pθ(x) [UxU>]
VX = I -2 Ux
(9)
(10)
NFK for GANs. As introduced in Section 3, we consider the EBM formulation of GANs. Given
pre-trained GAN model, we use D(x; θ) to denote the output of the discriminator D, and use G(h)
to denote the output of generator G given latent code h 〜p(h). Then We have the energy-function
defined as E(x; θ) = -D(x; θ). Based on the NFK formulation for EBMs, we can simply substitute
19
Published as a conference paper at ICLR 2022
Algorithm 6 matrix_jacobian_product
Input Neural network model fθ
Input Input data X ∈ RB×D, where B is batch size
Input Input matrix M
Input Kernel type kernel, NFK or NTK
Output Jθ (X)M for NTK, Fisher-vector-matrix-product Vθ(X)M for NFK
1:	mjp_fn = jax.vmap(jax.vjp)
2:	P =mjp_fn(fe, X, M)
3:	if kernel = "NFK" then
4:	P = diag(I)- 1 (P - ZθM)
5:	end if
6:	Return P
E(x; θ) = -D(x; θ) into Eq. 9 and Eq. 10 and derive the NFK formulation for GANs as below
Ux = VθD(X; θ) - Eh〜p(h)VθD(G(h); θ)
I = Eh〜p(h) [UG(h) U>(h)]
Vx = (diag(I)-2 )Ux
Knfk(X,z) = hVx,Vzi
(11)
Note that we use diagonal approximation of FIM throughout this work for the consideration of
scalability. Also, since the generator of GANs is trained to match the distribution induced by the
discriminator’s EBM from the perspective of variational training for GANs, we could use the samples
generated by the generator to approximate X ∈ pθ (X), which is reflected in above formulation.
NFK for VAEs, Flow-based Models, Auto-Regressive Models. For models including VAEs, Flow-
based Models, Auto-Regressive Models, where explicit or approximate density estimation is available,
we can simply apply the classical Fisher kernel formulation as introduced in the main text.
NFK for Supervised Learning Models. In the supervised learning setting, we consider conditional
probabilistic models pθ(y | X) = p(y | X; θ). In particular, we focus on classification problems
where the conditional probability is parameterized by a softmax function over the logits output
f(X; θ): pθ(y | X) = exp(f y (X; θ))/ Py exp(f y (X; θ)), where y is a discrete label and fy(X; θ)
denotes y-th logit output. We then borrow the idea from JEM (Grathwohl et al., 2020) and write out a
joint energy function term over (X, y) as E(X, y; θ) = -fy(X; θ). It is easy to see that joint energy
yields exactly the same conditional probability, at the same time leading to a free energy function:
E(X; θ) = -log	exp(f y (X; θ))
y
VθE(x; θ) = - XPθ(y | x)Vθfy(x; θ)
y
(12)
Based on the NFK formulation for EBMs, we can simply substitute above results into Eq. 9 and
Eq. 10 and derive the NFK formulation for GANs as below
Ux = fpθ (y | x)Vθ fy(χ; θ) - Ex，〜Pθ(χθ) fpe (y | χ)Vθ fy(χ0; θ)	(13)
yy
I = Ex〜pθ(x) [UχU>]
Vx = (diag(I)-2 )Ux	(14)
Knfk(x,z) = hVx,Vzi
C.2 Efficient low-rank NFK/NTK approximation via truncated SVD
We provide mode details on experimental observations on the low-rank structure of NFK and the
low-rank kernel approximation algorithm here.
20
Published as a conference paper at ICLR 2022
3g∖43∕B Cr
-s72⅛4 a 78
^∕<ΓS∙70g3
‰G27Q4rN
λ76 Iu V 7⅛ √
/ 3 7 3 / 7 ‰ q
/Γ7q 夕又，Γ⅛
yΛsb√ofσ-8
3g¾434a2
3 7a44az78
X / <Γ4 ɔ O y ʒ
入Ga 7o4rN
7 6IS-y7q,
∕J73∕7‰q
/Γ7J 9；
3 产5"yotf-55
(a) NFK
U g & / y
厂FT/9
/ əi ⅛
夕 5 H ? Z
45七S 4
9 3 0 2 9
2 £ O q 3
夕了 “ P夕
J2夕ol 夕 4 秒 3
S B 1l / fl。邛O
r g 4> / y s∙
OA / λ ⅞物覆
，S H ? Z / J
"5 £ S 4 13
9 3。2斤。/
2 6 Q Q廿亨©
，了 “，夕 O 3
(b) PCA
Figure 4: Inverting a DCGAN with 100d NFK embeddings (a), compared with image reconstruction
with 100d PCA embeddings (b). In either case, the left plot corresponds to real test images and the
right corresponds to the reconstructions. Note that NFK embeddings care capable of inverting a
GAN by producing high quality semantic reconstructions. With PCA, embeddings with the same
dimensionality produces more blurry reconstructions (thus less semantic).
Low-Rank Structure of NFK.
For supervised learning models, we trained a LeNet-5 (LeCun et al., 1998) CNN and a 3-layer MLP
network by minimizing binary cross entropy loss, and then compute the eigen-decomposition of
the NFK Gram matrix. For unsupervised learning models, we trained a small unconditional DC-
GAN (Radford et al., 2016) model on MNIST dataset. We deliberately selected a small discriminator,
which consists of 17K parameters. Because of the relatively low-dimensionality of θ in the discrimi-
nator, we were able to directly compute the Fisher Vectors for a random subset of the training dataset.
We then performed standard SVD on the gathered Fisher Vector matrix, and examined the spectrum
statistics. In particular, We plot the explained variance ration quantity, defined as『卜= PiT ：2 where
λi is the i-th singular value. In addition, we have also visualized the top 5 principle com=ponents, by
showing example images which have the largest projections on each component in Fig. 6.
Furthermore, we conducted a GAN inversion experiment. We start by sampling a set of latent variables
from the generator’s prior h ∈ p(h), and get a set of generated example {xi}, xi = G(hi), i =
1, ..., n. We then apply Algorithm 2 on the generated example {xi} to obtain their NFK embeddings
{e(xi)}, and we set the dimension of both h and e to 100. We now have a compositional mapping
that reads as h → x → e. We then learn a linear mapping W ∈ R100×100 from {e(G(hi))} to {hi}
by minimizing Pin=1 khi - We(G(hi))k2. In doing so, we have constructed an auto encoder from
a regular GAN, with the compositional mapping of X → e → h → x, where X is the reconstruction
of an input x. The reconstructions are shown in Figure 4 (a). Interestingly, the 100d SVD embedding
gives rise to a qualitatively faithful reconstruction on real images. n contrast, a PCA embedding with
the same dimension gives much more blurry reconstructions (eg., noise in the background), as shown
in Figure 4 (b). This is a good indication that the 100d embedding captures most of the information
about an input example.
Power iteration of NFK as JVP/VJP evaluations. Our proposed algorithm is based on the Power
method Golub and Van der Vorst (2000); Bathe (1971) for finding the leading top eigenvectors of
21
Published as a conference paper at ICLR 2022
Figure 5: The low-rankness of the NFK on a DCGAN trained on MNIST. For a trained model,
the first 100 principle components of the Fisher Vector matrix explains 99.5% of all variances. An
untrained model with the same architecture on the other hand, demonstrates a much lower degree of
low-rankness.
Figure 6: Images with the largest projections on the first five principle components. Each row
corresponds to a principle component.
the real symmetric matrix. Starting from a random vector v0 drawn from a rotationally invariant
distribution and normalize it to unit norm kv0 k = 1, the power method iteratively constructs the
sequence vt+ι = JKvtJ UP to q power iterations. Given the special structure of K that it's a Gram
matrix of the Jacobian matrix Jθ (X) ∈ RD×N, to evaluate Kvt in each power iteration step we
need to evaluate Jθ(X)>Jθ(X)vt, which can be decomposed as: (i) evaluating zt = Jθ(X)vt, and
then (ii) Kvt = Jθ(X)>zt. Note that when K is in the form of NTK of neural networks, step (i) of
evaluating zt is a Vector-Jacobian-Product (VJP) and step (ii) is a Jacobian-Vector-Product (JVP).
With the help of automatic-differentiation techniques, we can evaluate both JVP and VJP efficiently,
which only requires the same order of computational costs of one backward-pass and forward-pass of
neural networks respectively. In this way, we can reduce the Kernel matrix vector product operation
in each power iteration step to one VJP evaluation and one JVP evaluation, without the need to
computing and storing the Jacobian matrix and kernel matrix explicitly.
As introduced in Section. 3.2, we include detailed algorithm description here, from Algorithm. 3
to Algorithm. 6. In Algorithm. 3, we show the algorithm to compute the low-rank NFK em-
bedding, which can be used as data representations. In Algorithm. 4, we present our proposed
automatic-differentiation based truncated SVD algorithm for kernel approximation. Note that in
Algorithm. 5 and 6, we only need to follow Equation. 3 to pre-compute the model distribution
statistics, Z = Ex0〜pθ3，)Pype(y∣x)Vθfθ(x0), and FIM I = Ex,〜pθ⑺呢U>]. We adopt
22
Published as a conference paper at ICLR 2022
Figure 7: Linear probing accuracy on CIFAR10 with different number of principle components in
embedding. We use our proposed low-rank approximation method to compute the embedding from
the teacher model on CIFAR10 for knowledge distillation.
the EBM formulation of classifier fθ (x) then replace the Jacobian matrix Jθ (X) with the Fisher
vector matrix Vθ (X) = diag(I)-2 (J (X) - Z). Note that our proposed algorithm is also readily
applicable to empirical NTK via replacing the FIM by the identity matrix.
D	Experiments setup
D.1 QUALITY ANDEFFICIENCY OFLOW-RANKNFK APPROXIMATIONS
Experiments on Computational Cost. We randomly sample N ∈ 2k : 7 ≤ k ≤ 16 data examples
from CIFAR-10 dataset, and compute top-32 eigenvectors of the NFK Gram matrix (RN ×N) by
truncated SVD. We use same number of power iterations (10) in baseline method and our algorithm.
We show in Fig. 3 the running time of SVD for both methods in terms of number of data examples N.
Experiments on Approximation Accuracy. We randomly sample 10000 examples and compute
top-128 eigenvalues using both baseline methods and our proposed algorithm. Specifically, we
compute the full Gram matrix and perform eigen-decomposition to obtain baseline results. For our
implementation, we run 10 power iterations in randomized SVD.
D.2 Neural Fisher Kernel Distillation
With the efficient low-rank approximation of NFK, one can immediately obtain a compact representa-
tion of the kernel. Namely, each example can be represented as a k dimension vector. Essentially, we
have achieved a form of kernel distillation, which is a useful technique on its own.
Furthermore, we can use Q as an generalized form for teacher student styled knowledge distillation
(KD), as in (Hinton et al., 2015). In standard KD, one obtain a teacher network (e.g., deep model)
and use it to train a student network (e.g., a shallow model) with a distillation loss in the following
format:
Lkd(x, y) = α * Lcis(fs(x), y) + (1 - α) * Lt(fs(x),ft(x)),	(15)
where Lcls is a standard classification loss (e.g., cross entropy) and Lt is a teacher loss which
forces the student network’s output fs to match that of the teacher ft . We propose a straightforward
extension of KD with NFK, where we modify the loss function to be:
Lnfkd(x, y) = α * Lcls(fs(x), y) + (1 - α) * Lt(hs(x), Qt(x)),	(16)
where Qt (x) denotes the k dimensional embedding from the SVD of teacher NFK, for example x. hs
is a prediction head from the student, and Lt is overloaded to denote a suitable loss (e.g., `2 distance
or cosine distance). Equation 16 essentially uses the low dimension embedding of the teacher’s NFK
as supervision, inplace of the teacher’s logits. There are arguable benefits of using Lnfkd over Lkd.
23
Published as a conference paper at ICLR 2022
For example, when the number of classes is small, the logit layer contains very little extra information
(measured in number of bits) than the label alone, whereas Qt can still provide dense supervision to
the student.
For the Neural Fisher Kernel Distillation (NFKD) experiments, we adopt the WideResNet-
40x2 (Zagoruyko and Komodakis, 2016) neural network as the teacher model. We train another
WideResnet with 16 layers as the student model, and keep the width unchanged. We run 10 power
iterations to compute the SVD approximation of the NFK of the teacher model, to obtain the top-20
eigenvectors and eigenvalues. Then we train the student model with the additional NFKD distillation
loss using mini-batch stochastic gradient descent, with 0.9 momentum, for 250 epochs. The initial
learning rate begins at 0.1 and we decay the learning rate by 0.1 at 150-th epoch and decay again
by 0.1 at 200-th epoch. We also show the linear probing accuracies on CIFAR10 by using different
number of embedding dimensions in Figure. 7.
24