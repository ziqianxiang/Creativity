Published as a conference paper at ICLR 2022
Resonance in Weight Space: Covariate Shift
Can Drive Divergence of SGD with Momentum
Kirby Banman1, Liam Peet-Pare2, Nidhi Hegde1, Alona Fyshe1, Martha White1
Department of Computing Science1, Department of Mathematical and Statistical Sciences2
University of Alberta
Edmonton, AB T6G 2E8
{kdbanman, peetpare, nidhi.hegde, alona, whitem}@ualberta.ca
Ab stract
Most convergence guarantees for stochastic gradient descent with momentum
(SGDm) rely on iid sampling. Yet, SGDm is often used outside this regime, in
settings with temporally correlated input samples such as continual learning and
reinforcement learning. Existing work has shown that SGDm with a decaying step-
size can converge under Markovian temporal correlation. In this work, we show
that SGDm under covariate shift with a fixed step-size can be unstable and diverge.
In particular, we show SGDm under covariate shift is a parametric oscillator,
and so can suffer from a phenomenon known as resonance. We approximate the
learning system as a time varying system of ordinary differential equations, and
leverage existing theory to characterize the system’s divergence/convergence as
resonant/nonresonant modes. The theoretical result is limited to the linear setting
with periodic covariate shift, so we empirically supplement this result to show that
resonance phenomena persist even under non-periodic covariate shift, nonlinear
dynamics with neural networks, and optimizers other than SGDm.
1	Introduction
Stochastic gradient descent (SGD) (Robbins & Monro, 1951) - and its variants such as Adagrad
(DUchi et al., 2011), ADAM (Kingma & Ba, 2014) and RMSProP (Hinton et al., 2012) - are very
widely used optimization algorithms across machine learning. SGD is conceptually straightforward,
easy to imPlement, and often Performs well in Practice. Among the variants of SGD, accelerated
versions based on Polyak’s or Nesterov’s acceleration (Polyak, 1964; Nesterov, 1983), known
generally as Stochastic Gradient Descent with Momentum (SGDm), are used widely due to the
imProvements in convergence rate they offer. SGDm can give uP to a quadratic sPeeduP to SGD
on many functions, and is in fact oPtimal among all methods having only information about the
gradient at consecutive iterates for convex and LiPschitz continuous oPtimization Problems (Nesterov,
2004; Goh, 2017). SGDm has the same comPutational comPlexity as SGD, but exhibits suPerior
convergence rates under reasonable assumPtions (Su et al., 2014).
These convergence results for SGDm, however, rely on indePendent and identically distributed (iid)
samPling. Little is known about the convergence ProPerties of SGDm under non-iid samPling, yet
the non-iid setting is critical. In many machine learning Problems it is exPensive or imPossible to
obtain iid samPles. In online learning (Rakhlin et al., 2010) and reinforcement learning (RL) (Sutton
& Barto, 2018), the data becomes available in a sequential order and there is a temPoral dePendence
among the samPles. There is a Particularly strong temPoral dePendence in RL, where observed states
are samPled according to the transition dynamics of the underlying Markov decision Process (MDP).
Federated learning (Hsieh et al., 2020) and time-series learning (Kuznetsov & Mohri, 2014) Provide
further examPles of when non-iid samPling is essential to the learning Problem.
Without momentum, SGD’s convergence rate has been examined under non-iid samPling with the
stochastic aPProximation framework in (Benveniste et al., 2012; Kushner & Yin, 2003), and more
recently under sPecific assumPtions of ergodicity (Duchi et al., 2012) or Markovian samPling (Nagaraj
et al., 2020; Doan et al., 2020b; Sun et al., 2018). Convergence rates under Markovian samPling
are also known for ADAM-tyPe algorithms when aPPlied to Policy gradient and temPoral difference
1
Published as a conference paper at ICLR 2022
learning (Xiong et al., 2020). To our knowledge, however, there has been little work on providing
convergence rates or guarantees for SGDm under non-iid sampling. In (Doan et al., 2020a), a progress
bound is provided for SGDm under Markovian sampling based on mixing time—the time required
for a distribution’s convergence toward its stationary distribution—along with a convergence rate
guarantee under decaying step-sizes. In this work, we assume a fixed step-size, since it is a common
choice in the online setting, especially when the practitioner is unsure of mixing rate or stationarity.
There is a broad literature using ordinary differential equations (ODEs) to analyze gradient descent
methods by approximating descent updates as continuous-time flows. Early work is comprehensively
discussed in (Kushner & Yin, 2003; Benveniste et al., 2012). Despite the age and establishment of
the linear setting, the gradient flow lens continues to reveal new insights (e.g. implicit rank reduction
(Arora et al., 2019)) and new perspectives on old insights (e.g. regularization of early stopping (Ali
et al., 2019)). Recent work has paid particular attention to the flow induced by momentum accelerated
methods (Su et al., 2014; Wibisono et al., 2016; Wilson et al., 2016; Scieur et al., 2017; Muehlebach &
Jordan, 2021; Diakonikolas & Orecchia, 2019; Muehlebach & Jordan, 2019; Li et al., 2017; Simsekli
et al., 2020; Betancourt et al., 2018; Kovachki & Stuart, 2019; Attouch et al., 2018; Shi et al., 2019;
Siegel, 2019; Zhang et al., 2018; Berthier et al., 2021; Kovachki & Stuart, 2021). In these works, it is
demonstrated that linear regression under iid sampling with SGDm can be represented as an ODE
resembling a harmonic oscillator, sometimes with a time-decaying damping coefficient.
Contributions: In this work, we show that non-iid sampling, due to covariate shift, induces a related
system: the parametric oscillator, a harmonic oscillator having coefficients which vary over time in
a manner capable of exponentially exciting the system (Mumford, 1960; Csorgo & Hatvani, 2010;
Halanay, 1966). We characterize the underlying time-varying ODE, which requires a novel approach
to incorporate covariate shift, and develop specific conditions on covariate shift that lead to divergence
in SGDm. We provide an empirical design, with synthetic data, to systematically test the response of
the learning system to different covariate shift frequencies. We empirically validate that resonance-
driven divergence occurs in a learning system which aligns well with theoretical assumptions. We
follow with similar demonstrations on learning systems which progressively relax further and further
away from our theoretical assumptions, including non-periodic covariate shift, learning with neural
networks and optimizers other than SDGm. The implications of this work are that resonance can
occur in learning systems with momentum, causing instability or poor accuracy, simply due to certain
temporal correlations in the input sequence. Our results provide a novel direction to better understand
the properties of our learning systems for time varying inputs—namely beyond iid sampling—and
point to a gap in the robustness of our algorithms that merits further investigation.
2	Problem Setting
We investigate the effect of non-iid
sampling on a model optimized us-
ing SGDm. We assume labelled
training data sampled from discrete-
indexed, real-valued stochastic pro-
cesses {Xk}k∈N and {Yk}k∈N such
that {Xk}k∈N has a unique stationary
distribution Π. Yk is a function of
Xk with zero-mean observation noise,
Yk = f(Xk) + k. The learning al- Figure 1: An example of the problem setting. 1D regression
gorithm does not have access to iid (left), Gaussian covariates {Xk} with shifting mean Xk over
samples from Π. Instead, the learning time (right).
algorithm may only update parame-
ters θk at time k using samples zk = (xk, yk), where Xk’s marginal over time converges to Π. The
goal is to train a model with parameters θ to minimize an objective function L(z; θ) with respect
to the stationary distribution Π: θ* = argmi□θ∈Rd E∏[L(z; θ)] = JR L(z; θ)dΠ(x). An example of
this problem setting is illustrated in Figure 1.
This setting is identical to that explored in prior work on Markovian sampling (Sun et al., 2018;
Xiong et al., 2020; Nagaraj et al., 2020), but we do not require that our stochastic processes adhere to
the Markov property. Note, also, that the underlying functional relationship between the inputs, Xk,
2
Published as a conference paper at ICLR 2022
and the targets, Yk, does not change over time. That is, the non-iid sampling is a result of covariate
shift rather than a changing relationship between the inputs and targets.
We consider Polyak’s Heavy Ball method (Polyak, 1964) with the formulation from Sutskever et al.
(2013), namely SGD with momentum (SGDm)
vk+1 = μvk - η^θL(zk; θk )	θk + 1 = θk + vk+1	(I)
where η ≥ 0 is the learning rate and μ ∈ (0,1) the momentum coefficient. Our goal is to understand
the behavior of SGDm when learning on this non-iid sequence zk.
3 Covariate Shift as a Driving Force
In this section we characterize SGDm as a discretization of a particular parametric oscillator in
continuous time. A parametric oscillator resembles a harmonic oscillator ODE, but has time-varying
system coefficients which are capable of driving the system. It is well understood that parametric
oscillators can suffer from global solution instability due to coefficients oscillating at particular
frequencies (Halanay, 1966), a condition known as parametric resonance. We show parametric
resonance conditions sufficient to induce exponential divergence in SGDm. We provide proof sketches
and defer complete proofs to Appendix A.4.
Notation: Capital letters are matrices and random variables. {Xk} is a stochastic process, shorthand
for {Xk}k∈N . k and t index discrete and continuous time, respectively. When a discrete sequence
and a function approximate each other, they share the same symbol and are differentiated by subscript
k and function argument t, e.g. the sequence {θk} and the function θ(t). Pk is the joint distribution
of Xk and Yk . For loss function L(z; θ) on training pair z = (x, y) and weights θ, we denote the
time-varying expected gradient gk (θ) := Epk [VθL(z; θ)]
3.1	Covariate Shift Induces a Linear Time-Varying Expected Gradient
We begin by showing that linear least squares regression with covariate shift and a fixed target induces
a linear time-varying expected loss gradient. That is, the gradient function gk(θ) = Bk(θ - θ*) for
some matrix Bk that varies with time, due to covariate shift.
Assumption 1. The covariate generating process {Xk}k∈N is not identically distributed:
Corr(Xk1, Xk1) 6= Corr(Xk2, Xk2) for some k1, k2 ∈ N.
Assumption 2. The targets Yk are a fixed linear function of Xk with iid zero-mean observation noise,
Ek such that Ekk ] = 0. That is, Yk = (θ* ,Xki+ Ek where θ* ∈ Rd is fixed forall k.
Proposition 1. Under Assumptions 1, 2, a linear model with weights θk
making predictions Ybk
hθk , Xk i with a mean squared error (MSE) objective will induce time-varying linear expected loss
gradients gk (θk) = Bk (θk 一 θ*) forall k ∈ N, where Bk ∈ Rd×d and B® = Bk? for some k∖,k2.
Proof Sketch: We fix a time step k and take the expectation of the MSE loss over observation noise,
and over the distribution of Xk. A time-varying expected loss surface remains, and the linear setting
implies the loss gradient is a linear function for all steps k, with Bk depending on Xk:
Bk = 2Corr(Xk, Xk) = 2Cov(Xk, Xk) + 2EPk [Xk]EPk [Xk]T	(2)
3.2	ODE Correspondence
Next, we show that the iterates {θk} generated by SGDm are a first order numerical integration
of a particular ODE. The procedure is similar to (Muehlebach & Jordan, 2021), but due to the
time-varying loss gradient, we must pay specific attention to the conditions on the ODE necessary to
have integration consistency, . Let B(t) be a matrix-valued function, Lipschitz continuous in t, such
that {Bk} are samples spaced √η apart, i.e. Bk = B(√ηk). The existence and uniqueness of this
B(t) are ensured if we assume the {Xk} are sampled from an underlying continuous-time X(t).
Proposition 2. The SGDm iterates {θk} numerically integrate the ODE system in equation 3 with
integration SteP √η and first order consistency.
θ(t) 十 二# θ(t) + B(t)(θ(t) — θ*) = 0
√η
(3)
3
Published as a conference paper at ICLR 2022
Proof Sketch: First, the ODE in equation 3 is converted to first order linear form, and split into a sum
of two separate systems. The sum’s terms have implicit and explicit Euler integrations; using operator
splitting, these can be composed into a consistent numerical integrator for linear time-varying systems.
Then, it is shown that the composed integrator is precisely equivalent to SGDm when the integration
time step is √η. □
The first order consistency guarantee in Proposition 2 means that when the step-size η is small and
initial conditions agree between continuous and discrete time, i.e. θ0 = θ(0), the continuous and
discrete time trajectories approximate each other as θk ≈ θ(k√η), where the difference between them
depends on the integration time step h = √η (Hairer et al., 2θ06). Therefore, assuming θo = θ(0),
the difference accrued in one step of k (local error) is θι = θ(√η) + O(η) and over arbitrarily many
steps (global error) is θk = θ(√ηk) + O(ηk).
3.3	Parametric Resonance for ODE Convergence and Divergence
The conditions sufficient for convergence and divergence in equation 3 may be shown using estab-
lished dynamical systems theory. The conditions sufficient for divergence are precisely the conditions
for parametric resonance, when B(t) is periodic. Note that periodicity in the first or second moment
of {Xk} is sufficient to induce periodicity in B(t). As per elementary results in linear ODE theory,
the system in equation 3 can be transformed into a linear time-varying first order form
ξ⑴=A⑴ξ⑴	A⑴=]t⅛d J;"	⑷
where solution trajectories θ(t) of equation 3 are embedded in solution trajectories ξ(t) of equation 4.
Equation 4 admits a fundamental solution matrix1 ψ(t) such that the spectral radius ρ of ψ(T)
characterizes ODE instability, which implies divergence for SGDm, in Theorem 1.
Theorem 1. When B(t) is periodic such that B(t) = B(t + T) for some T > 0, the spectral radius
ρ of ψ(T) characterizes the stability of solution trajectories of equation 3 as follows:
•	ρ > 1 =⇒ trivial solution θ(t) = θ* is unstable. All other solutions diverge as θ(t) → ∞
exponentially with rate ρ.
•	ρ < 1 =⇒ trivial solution is asymptotically stable, all other solutions converge as θ(t) → θ*
exponentially with rate ρ.
Proof Sketch:ODE 3'S stability depends on the relationship between the coefficient 1√μ and the
expected gradient signal because they determine the spectral radius of ψ(T). The spectral radius ρ is
the COnvergence/divergence rate per period T towards/away from the stationary point θ*. □
The spectral radius conditions characterize when ODE solution trajectories θ(t) will converge or
diverge, and Proposition 2 tells us that these solution trajectories are approximations of discrete time
SGDm trajectories {θk} under identical initialization. But does convergence or divergence of θ(t)
imply the same for SGDm? Experiment 4.1 empirically suggests that the approximation is sufficient,
since the boundary at ρ = 1 in Figure 2a agrees with both continuous and discrete time. However,
given Theorem 1 and Proposition 2, we have a theoretical guarantee of SGDm’s divergence, but not
its convergence. This is because the divergence rate of θ(t) is exponential, and Proposition 2 provides
a bound on long tail behaviour as θk = θ(√ηk) + O(ηk). Since the approximation error is linear in
time k, and the divergence rate is exponential, the divergence rate dominates, and we are guaranteed
that a diverging θ(t) corresponds to a diverging {θk}. However, the same argument does not imply
that a convergent θ(t) corresponds to a convergent {θk}, because the linear error bound technically
permits the discrete trajectory to escape θ* at a linear rate. Empirically, we see agreement for both
convergent and divergent cases, but we defer theoretical proof to future work.
Summary: There is a chain of dependencies starting from the non-iid sampling in {Xk}, and ending
at the monodromy’s spectral radius.
{Xk } → X (t) → B(t) → A(t) → ψ(t) → ψ (T ) → ρ
In order, we have the discrete stochastic process {Xk} from which training inputs are sampled, and
its underlying continuous stochastic process X(t). If {Xk} and X(t) were iid, then the matrix B
1A fundamental solution matrix for system equation 4 is any matrix-valued function of time ψ(t) whose
columns are linearly independent solutions to equation 4. We choose ψ(t) such that ψ(0) = I2d×2d.
4
Published as a conference paper at ICLR 2022
would be constant, but the non-iid nature means B(t) is a function of time. The matrix A(t) is the
matrix describing SGDm’s continuous time dynamics as a linear time-varying ODE ξ = A(t)ξ, and
the matrix B(t) is a simply a submatrix of A(t). Since we have a linear ODE, the space of solution
trajectories is spanned by the columns of the ODE’s fundamental solution matrix ψ(t). Moreover,
since we have a periodic A(t) with period T, the stability of all solutions can be determined simply
from the largest eigenvalue of ψ(T), a.k.a. its spectral radius ρ.
The intuition behind p's importance comes from the following fact: after each elapsed period T,
the phase space (including weight space) is subject to the linear transformation ψ(T). So as time
increases, say n periods, the linear transformation is applied iteratively as ψ(T)n, so it is clear that
any eigenvalue larger than unity means all solutions eventually diverge exponentially. This is precisely
the parametric resonance condition. For a detailed example of all these quantities characterizing the
divergence of a simple learning system, see Appendix A.1.
Technical Novelty: There are two key novel technical contributions to obtain the results above. (1)
The ODE correspondence for non-iid data in combination with momentum methods is new. The
non-iid data, which induces time variation in ODE dynamics, is subtle to address, because elementary
ODE correspondence (i.e. consistency guarantees) require time invariant ODE dynamics. Our proof
specifically addresses this difficulty using operator splitting theory from recent numerical integration
literature. (2) Once we have this ODE, it is straightforward to recognize that it is a parametric
oscillator. The utility of this connection, however, is that it opens up entirely new avenues for analysis,
since there is deeply established literature on the dynamics of these systems. The primary second
technical novelty, therefore, is to leverage this connection, and use Floquet theory to give precise
divergence conditions for a given η and μ, under periodic covariate shift.
4	Validating Theory & Ablating Toward s Conditions in the Wild
We now empirically evaluate the effect of resonance in learning problems. First, Experiment 4.1
validates the theoretical predictions by investigating the dynamics of a learning problem which closely
matches the theoretical assumptions in Section 3. Specifically, we assess whether or not the spectral
radius ρ predicts optimizer convergence or divergence. Then, the remaining Experiments 4.2 - 4.6 test
for this phenomena in settings beyond our theory. Each experiment makes a single step away from a
theoretical assumption, first relaxing the periodicity assumption, then using stochastic rather than
expected gradients and then moving to nonlinear models (neural networks) and optimizers (ADAM).
Across all experiments, input samples at each training step k are drawn from Gaussian distributions
with diagonal covariance matrices, i.e. Xk 〜N(Xk, cId×d), and We induce covariate shift by
constructing a time-varying mean sequence {Xk}. Each mean sequence is designed such that (a) its
frequency content can be swept with a single parameter f or T and (b) iid sampling is induced by
setting f = 0 or T = 0, so that each experiment has an iid baseline for comparison. The specification
of Xk is provided in Table 1 for each experiment. This empirical design allows us to systematically
test response to different frequencies. See Appendix A.5 for specification of all experiment details,
including visual depictions of all synthetic {Xk} and their frequency content.
4.1	Validating Theory
We start in a setting as close as possible to the theoretical predictions, with linear regression for a
quadratic loss, and covariate shift such that the mean E[Xk] varies as a strict sinusoid. We perform
regression in two weights (i.e. inputs Xk and labels Yk are scalar-valued). We draw 20 samples from
each Xk , so that the loss gradient for each time step k is close to its expected value.
Since we have only two weights, the learning system’s underlying ODE can be easily specified, such
that the fundamental solution matrix can be numerically computed. As per Theorem 1, we can use the
spectral radius of the fundamental solution matrix evaluated at time T to make theoretical predictions
of where the system should converge or diverge. As depicted in Figure 2a, the theory agrees very well
with empirical results. This suggests that parametric resonance is indeed the dominant mechanism
behind SGDm divergence under covariate shift. In the appendix, refer to example A.1 for the
procedure used to compute the theoretical predictions (i.e. spectral radii ρ), and Figure 6a for the full
surface from which the contour lines in Figure 2a are rendered.
5
Published as a conference paper at ICLR 2022
Table 1: Covariate shift details for each experiment.
Experiment	Sweep Param.	Mean Sequence {xk}k∈N		
4.1 Validating Theory	f ∈ [0,0.05]	Xk = 0.5sin(2πfk) , T		= 1/f
4.2 Ablating Periodicity	f∈ [0, 0.05]	Xk = ΦιXk-i + Φ2Xk-2 + ξk φ1 = 4φ2(φ2 - 1)-1 cos(2π f) φ2,ξk ,X1,X2 see Table 3		
4.3 Ablating Expected Gradient	T∈ [0, 120]	Xk = (	⅛lif b竽C I	2-lξ∣if b 等C ξ ~ N(0, Id×d) iid		≡ 0 mod 1 ≡ 1 mod 1
4.4 Ablating Periodicity Further	T∈ [0, 50]	Xk = ξi where i = ξi ~ N(0, VIdXd) ii	k T d	
4.5 Ablating Optimizer Linearity	T ∈ [0,100]	Xk same as above.		
4.6 Ablating Model Linearity	T ∈ [0,100]	Xk same as above.		
Cov. shift period T
(a) Resonance regions for sinusoidal Xk
Figure 2: Empirical heatmap of momentum μ versus period T for SGDm for linear regression,
overlaid by contours of theoretical prediction. Each pixel is the distance ∣∣θk - θ* || averaged over
the final 500 steps k and 10 runs. Dark pixels converge quickly and stably, bright pixels diverge
exponentially. The contours show divergence predictions from Theorem 1: the white contour has
ρ = 1, with ρ increasing with redness.
Peak freq, period T
(b) AR(2) Stochastic Mean
4.2	Ablating Periodicity
We repeat Experiment 4.1, but with the mean ofXk varying stochastically instead of deterministically.
The result in Theorem 1 assumes the expected gradient varies periodically over time given fixed
weights θ. But even with aperiodic and/or stochastic time variation, our LTV system equation 4 might
be similarly susceptible to instability; the theory does not have periodicity as a necessary condition
of divergence. To test this, we replace our periodic covariate shift mean with a mean that moves
according to an AR(2) process. The AR(2) process is tuned to have a frequency peak exactly at
the frequency of the sinusoidal covariate shift of Experiment 4.1, to change only the nature of the
shift—aperiodicity—rather than the frequency.
We can see from the heatmap in Figure 2b that under this setting we observe very similar resonance
behaviour in the learning system, which aligns well with the identical predicted stability regions.
Note that both theoretical and empirical results in Figures 2a and 2b suggest that sufficiently low
momentum values μ mitigate the resonance phenomenon, which agrees with the role it plays in the
ODE system: decreasing μ increases damping. We also observe that resonance regions shrink as step
size η is decreased; see the Appendix for plots demonstrating this trend. Analytically characterizing
the bounds of stable μ, η in terms of system properties is an interesting future direction.
6
Published as a conference paper at ICLR 2022
4.3	Ablating Expected Gradient
Here we perform linear regression with periodic mean covariate shift, and ablate the number
of samples drawn from each Xk to show the effect of increasing noise in the gradient signal.
Linear regression is performed mapping R5 → R, with the weights learned via SGDm. The
mean sequence Xk oscillates with strict periodicity between ±x, where X is a unit norm 5-vector
randomly chosen for each run. i.e. the mean signal is a square wave in R5 with period T .
Linear regression implies a quadratic loss, and peri-
odic covariate shift implies a loss with periodic time
variation in expectation, so we are very near to the
setting in which Theorem 1 is directly applicable.
However, as we ablate from 5 samples to 1 sample
drawn from each Xk , we move further away from the
expected gradient, towards the fully stochastic gradi-
ent setting. As we can see in Figure 3 resonance is
dampened by stochasticity in the gradient signal, but
nonetheless is still present. These findings are further
supported in Appendix A.3, where Experiments 4.1
and 4.2 are also ablated from expected to stochastic
gradient signals.
4.4	Ablating Periodicity Further
We further depart from strictly periodic covariate
shift by having piece-wise stationary means and non-
smooth changes: we randomly sample a new mean
from a normal distribution N (0, v2) after every T
update steps. We further investigate the impact of
Figure 3: Regression w/ periodic Xk. Each
dot is avg. dist. ∣∣θk - θ*∣∣ over final 500
steps. Each color has three peaks: the left
peak exceeds y scale for all curves, the center
peak is small enough to appear only for the
black curve, and the right peak appears for all
curves (vanishingly small for the black curve.)
Resonance is dampened and right-shifted by
decreasing samples per Xk.
this covariate shift under (a) increasing variance v2 and (b) increasing dimensionality of the inputs.
We observe divergence characteristic of the parametric resonance, with smaller T having a highly
divergent response. In Figure 4a we can see that a higher variance results in much more instability,
though even for smaller variance the distance to θ* is still on the order of 103. This results makes
sense, as the variance is connected to the driving signal amplitude, on which parametric resonance has
a strong dependence. We also observe a strong dependence on the number of input space dimensions,
in Figure 4b. This aligns with the fact that the expected norm of samples drawn from a multivariate
Gaussian increases with dimensionality, even with a fixed covariance scale (of 0.1). This result
suggests that resonance phenomena may actually be exacerbated for higher-dimensional inputs,
which is the setting we expect to see in practice.
4.5	Ablating Optimizer Linearity
So far our experiments have ablated periodicity, input dimensionality, and gradient stochasticity. As
per the conditions in Section 3, each of these ablated systems still corresponds to a discretization of
an LTV ODE. We now ask whether resonance can be observed in systems which do not correspond
to a discretization of our ODE. To investigate this hypothesis, we replace SGDm with ADAM as an
update rule for our learning algorithm. We no longer have the ODE representation of this learning
system, but can empirically investigate the behaviour of the system under covariate shift.
We use the same stochastic mean switching problem as the previous experiment, where we regress
from 5 dimensions to 1 with input samples. Again, we measure the distance ∣∣θk - θ*∣∣, and we vary
the ADAM parameter β1 . Similar to the SGDm optimizer, we can see in Figure 4c that there is a
band of mean switching intervals T for which convergence is worse.
Unlike SGDm, there is no divergence. Proper parametric resonance induces exponential divergence,
which is why the previous results were presented with Euclidean distances between weights on the
order of 1019. Note that in Figure 4c, the frequency response in weight space distance is instead
measured on the order of 100 .
7
Published as a conference paper at ICLR 2022
(a) SGDm w/ stochastic Xk, variance sensitivity
(C) ADAM w/ stochastic Xk, βι sensitivity
io20
0	10	20	30	40	50
Mean shift interval T
(b) SGD w/ stochastic Xk, sensitivity to d
Figure 4: Regression w/ stochastic Xk. Each
marker is avg. distance ∣∣θk - θ* || over final
500 steps (a and b) or 2000 steps (c). For
SGDm, resonance is very sensitive to the Xk
signal variance (i.e. amplitude) in (a), and
to the number of input dimensions in (b).
For ADAM in (c), convergence is affected by
specific frequency content (x-axis) but does
not diverge, suggesting that the frequency re-
sponse is significantly damped. Similar to
SGDm, response is higher for larger momen-
tum parameters β1 .
4.6 Ablating Model Linearity
Our final step away from ODE equation 3 abandons the quadratic loss surface. We fit a nonlinear
target function using a fully connected ReLU network, reusing the previous covariate shift. In
previous experiments, we measured the distance ∣∣θk - θ*∣∣. Now a unique target is not guaranteed,
so frequency response is in terms of loss. In order to amplify frequency response, 10 samples were
drawn from each Xk to reduce gradient signal noise.
Figure 5 shows a band of mean switching intervals T
which damage convergence, which is characteristic
of resonance. Though we do not see divergence,
the impact on accuracy is problematic. In this case,
models with loss > 0.3 fit extremely poorly, but
those with < 0.05 perform well.
5	Discussion
This work is a first step towards understanding fre-
quency response due to covariate shift. Here we
discuss future directions, limitations, and practical-
ity.
Limitations and Next Steps: In our theoretical
analysis, we take expectation over observation noise
and over the distribution of inputs Xk, so the gradi-
ent coefficient matrix B(t) in the ODE equation 3 is
deterministic. This allows rigorous characterization
Mean shift interval T
Figure 5: Training a neural network with
SGDm shows a peak response in the loss
around the band T ∈ [5, 40]. The y-axis is
average test loss over the final 2000 training
steps over 20 runs, with test set obtained via
the stationary distribution of {Xk}. Shaded
regions are 95% confidence intervals.
of resonance for periodic B(t). Experiments 4.2 and 4.4 demonstrate that resonance still drives
divergence when B(t) is nondeterministic, so rigorous exploration in future work might employ
stochastic differential equations. In particular, Experiment 4.2 aligns exactly with the theoretical
predictions, suggesting that the resonance is in response to the frequency content of Xt , not its
periodicity.
8
Published as a conference paper at ICLR 2022
For the sake of consistency across experiments, our non-iid sampling is always induced by Gaussian
Xk with fixed diagonal covariance matrix. But Assumption 1 is far more relaxed, suggesting
resonance is possible with time-varying covariance and non-Gaussian distributions, which may be
explored in future empirical work
When assessing a new phenomenon like covariate shift resonance, assessment on simple problems is
a critical first step, and we have endeavored to be comprehensive in that assessment. But we only
scratch the surface of more complex optimizers and nonlinear models. Similarly, for the sake of
experimental control and interpretability, we assess resonance on synthetic data instead of real data.
While it is obvious that many sources of real data have nontrivial frequency content (e.g. audio
samples, machine sensors, etc.) it is not yet clear when realistic frequency content will resonate with
the system trying to learn from it. Optimizers, models, and data all pose exciting future directions for
this work: to extend into the frequency response of more complex optimizers and models, and into
the frequency content of real data.
Finally, the theory in this work is for linear models. This is the correct setting to understand first,
and is of independent interest. Our theory applies to any linear models that are augmented with a
nonlinear basis (such as a Fourier basis, polynomials or radial basis functions) by examining the
frequency content of the basis output signal.
Practical Implications: One potential conclusion from our results is that though resonance could
potentially arise in our learning systems, it may not be problematic. First, the heatmaps in Figures 2a
and 2b indicate that SGDm converges much more often than it diverges, and that we can practically
avoid resonance simply by reducing the momentum parameter μ. As per Appendix A.2, We find that
sufficiently low μ does mitigate resonance. Second, we may simply be able to avoid covariate shift in
the first place, by keeping buffers of data and shuffling samples.
However, the reality is not so simple. Higher levels of momentum are typically more effective in
practice. There is a tension between the level of momentum and avoiding resonance, that cannot
easily be addressed by simply picking low momentum by default. Instead, smarter approaches which
recognize potential resonance and then reduce momentum may be a much more effective way to guard
against this issue; the development of such methods can leverage the insights in this work. In addition,
relying on well-behaved or shuffled inputs may be impractical in certain settings. Algorithms in
signal processing and the streaming literature often process data sequentially. Reinforcement learning
approaches use sliding window buffers (in replay), where covariate shift may still persist across
buffers. Further, it is even possible that adversarial attacks could be designed to exploit this weakness
of SGDm, causing poor performance or even instability from attackers attempting to generate data
that results in resonant behavior.
Our work also suggests avenues towards the development of diagnostic tools. Indeed, our experimental
approach to investigate this phenomena mimics those in the modal analysis literature (Avitabile,
2017). We treat the input sampling process as an input signal which is noisy and/or stochastic,
vary the signal’s frequency content across a wide band, and measure the learning system’s output
as a response across input frequencies. This is analogous to a mechanical engineer triggering an
oscillatory vibration within a machine’s engine compartment, sweeping across frequencies, and
measuring the resulting vibration amplitude in the machine’s housing.
6	Conclusion
To our knowledge, this work is the first to investigate SGDm under covariate shift from the ODE
perspective. This perspective reveals new insights, in particular the revelation that SGDm under
covariate shift numerically integrates a parametric oscillator, rather than the simple undriven harmonic
oscillator induced by iid sampling. We leverage dynamical systems theory to analyze the stability of a
learning system using SGDm under covariate shift, and are able to provide conditions for exponential
divergence due entirely to the frequency content induced by this non-iid sampling. Despite its age and
establishment, understanding SGDm is an ongoing process. We contribute to this process in a way
that provides physical intuition, rather than a merely algebraic proof. We hope that the connection
we have drawn to parametric oscillation in dynamical systems theory will help to further propel
understanding of non-iid sampling and SGDm.
9
Published as a conference paper at ICLR 2022
7	Acknowledgements
We would like to thank Yurij Salmaniw and Ryan Thiessen for their discussions regarding Floquet
theory and helping us makes sense of our differential equations. We would also like to thank Csaba
Szepesvari for pointing us to excellent resources for numerical integration, algorithmic convergence,
stochastic processes, and various future directions for this work.
8	Ethics Statement
Our work is theoretical and attempts to provide greater understanding of an optimization algorithm.
Due to the remoteness of this work from direct ethical concerns, it is difficult to reason about
any ethical implications of the work. We hope that the insights into SGDm could help to make
machine learning algorithm more robust, but beyond that we are not aware of any ethical concerns or
implications of this work.
9	Reproducibility Statement
We have endeavored to be very clear and thorough in our explanation of the theoretical results, and
all assumptions and proofs are explicitly stated and contained in either the main body of the paper
or in the appendix. The code for all of the experiments is included in the supplementary material
and the instructions for reproduction are in the README file. The data used is synthetic and code
to generate the data is contained in the supplementary material. All of the experimental details (e.g.
hyperparameter selection) are contained in tables in the paper and appendix and the supplementary
material. All of the experiments should be easily run on modest commodity hardware in the space of
a few days, at most.
References
Alnur Ali, J Zico Kolter, and Ryan J Tibshirani. A continuous-time view of early stopping for least
squares regression. International Conference on Artificial Intelligence and Statistics, 2019.
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. Neural Information Processing Systems, 2019.
Hedy Attouch, Zaki Chbani, Juan Peypouquet, and Patrick Redont. Fast convergence of inertial
dynamics and algorithms with asymptotic vanishing viscosity. Mathematical Programming, 2018.
Peter Avitabile. Modal Testing: A Practitioner’s Guide. John Wiley & Sons, 2017.
Albert Benveniste, Michel Metivier, and Pierre PrioUret Adaptive Algorithms and Stochastic
Approximations. Springer Science & Business Media, 2012.
Raphael Berthier, Francis Bach, Nicolas Flammarion, Pierre Gaillard, and Adrien Taylor. A con-
tinUized view on nesterov acceleration. arXiv preprint arXiv:2102.06035, 2021.
Michael BetancoUrt, Michael I Jordan, and Ashia C Wilson. On symplectic optimization. arXiv
preprint arXiv:1802.03653, 2018.
Petra Csom6s and Istvan Farag6. Error analysis of the numerical solution of split differential equations.
Mathematical and Computer Modelling, 2008.
Sandor CSGrgO and Laszl6 Hatvani. Stability properties of solutions of linear second order differential
equations with random coefficients. Journal of Differential Equations, 2010.
Jelena Diakonikolas and Lorenzo Orecchia. The approximate duality gap technique: A unified theory
of first-order methods. SIAM Journal on Optimization, 2019.
Thinh T Doan, Lam M Nguyen, Nhan H Pham, and Justin Romberg. Convergence rates of ac-
celerated markov gradient descent with applications in reinforcement learning. arXiv preprint
arXiv:2002.02873, 2020a.
10
Published as a conference paper at ICLR 2022
Thinh T Doan, Lam M Nguyen, Nhan H Pham, and Justin Romberg. Finite-time analysis of stochastic
gradient descent under markov randomness. arXiv preprint arXiv:2003.10973, 2020b.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 2011.
John C Duchi, Alekh Agarwal, Mikael Johansson, and Michael I Jordan. Ergodic mirror descent.
SIAM Journal on Optimization, 2012.
Istvdn Farag6, Agnes Havasi, and R. Horvath. On the order of operator splitting methods for time-
dependent linear systems of differential equations. International Journal of Numerical Analysis
and Modeling, 01 2011.
Gabriel Goh. Why momentum really works. Distill, 2017. doi: 10.23915/distill.00006. URL
http://distill.pub/2017/momentum.
Ernst Hairer, Christian Lubich, and Gerhard Wanner. Geometric Numerical Integration: Structure-
Preserving Algorithms for Ordinary Differential Equations. Springer Science & Business Media,
2006.
Aristide Halanay. Stability Theory, chapter 1, pp. 13-130. Elsevier, 1966.
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Lecture 6a: Overview of mini-batch gradient
descent. Neural Networks for Machine Learning, 2012.
Kevin Hsieh, Amar Phanishayee, Onur Mutlu, and Phillip Gibbons. The non-iid data quagmire of
decentralized machine learning. International Conference on Machine Learning, 2020.
Arieh Iserles. Euler’s Method and Beyond, chapter 1, pp. 3-18. Cambridge University Press, 2
edition, 2008.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International
Conference on Learning Representations, 2014.
Nikola B Kovachki and Andrew M Stuart. Analysis of momentum methods. arXiv preprint
arXiv:1906.04285, 2019.
Nikola B Kovachki and Andrew M Stuart. Continuous time analysis of momentum methods. Journal
of Machine Learning Research, 2021.
Harold Kushner and George G Yin. Stochastic Approximation and Recursive Algorithms and
Applications. Springer Science & Business Media, 2003.
Vitaly Kuznetsov and Mehryar Mohri. Generalization bounds for time series prediction with non-
stationary processes. International Conference on Algorithmic Learning Theory, 2014.
Qianxiao Li, Cheng Tai, and E Weinan. Stochastic modified equations and adaptive stochastic
gradient algorithms. International Conference on Machine Learning, 2017.
Michael Muehlebach and Michael I Jordan. A dynamical systems perspective on nesterov acceleration.
International Conference on Machine Learning, 2019.
Michael Muehlebach and Michael I Jordan. Optimization with momentum: Dynamical, control-
theoretic, and symplectic perspectives. Journal of Machine Learning Research, 2021.
W. W. Mumford. Some notes on the history of parametric transducers. Proceedings of the Institute of
Radio Engineers, 1960.
Dheeraj Nagaraj, Xian Wu, Guy Bresler, Prateek Jain, and Praneeth Netrapalli. Least Squares
Regression with Markovian Data: Fundamental Limits and Algorithms. Neural Information
Processing Systems, 2020.
Yurii Nesterov. A method for solving the convex programming problem with convergence rate
o(1/k2). Proceedings of the USSR Academy of Sciences, 1983.
11
Published as a conference paper at ICLR 2022
Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer Academic
Publishers, Norwell, 2004.
Boris Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computa-
tional Mathematics and Mathematical Physics, 1964.
Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning: Random averages,
combinatorial parameters, and learnability. Neural Information Processing Systems, 2010.
Herbert Robbins and Sutton Monro. A Stochastic Approximation Method. The Annals of Mathemati-
cal Statistics, 1951.
Damien Scieur, Vincent Roulet, Francis Bach, and Alexandre d’Aspremont. Integration methods and
optimization algorithms. Neural Information Processing Systems, 2017.
Bin Shi, Simon S Du, Weijie J Su, and Michael I Jordan. Acceleration via symplectic discretization
of high-resolution differential equations. arXiv preprint arXiv:1902.03694, 2019.
Jonathan W Siegel. Accelerated first-order methods: Differential equations and lyapunov functions.
arXiv preprint arXiv:1903.05671, 2019.
Umut Simsekli, Lingjiong Zhu, Yee Whye Teh, and Mert Gurbuzbalaban. Fractional underdamped
langevin dynamics: Retargeting sgd with momentum under heavy-tailed gradient noise. Interna-
tional Conference on Machine Learning, 2020.
Weijie Su, Stephen Boyd, and Emmanuel Candes. A differential equation for modeling nesterov’s
accelerated gradient method: Theory and insights. Neural Information Processing Systems, 2014.
Tao Sun, Yuejiao Sun, and Wotao Yin. On markov chain gradient descent. Neural Information
Processing Systems, 2018.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization
and momentum in deep learning. International Conference on Machine Learning, 2013.
Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT press, 2018.
Andre Wibisono, Ashia C Wilson, and Michael I Jordan. A variational perspective on accelerated
methods in optimization. Proceedings of the National Academy of Sciences, 2016.
Ashia C Wilson, Benjamin Recht, and Michael I Jordan. A lyapunov analysis of momentum methods
in optimization. arXiv preprint arXiv:1611.02635, 2016.
Huaqing Xiong, Tengyu Xu, Yingbin Liang, and Wei Zhang. Non-asymptotic convergence
of adam-type reinforcement learning algorithms under markovian sampling. arXiv preprint
arXiv:2002.06286, 2020.
Jingzhao Zhang, Aryan Mokhtari, Suvrit Sra, and Ali Jadbabaie. Direct runge-kutta discretization
achieves acceleration. Neural Information Processing Systems, 2018.
12
Published as a conference paper at ICLR 2022
A Appendix
A. 1 A Procedure to Numerically Determine System Stability
Here we expand on the example given at the end of Section 3. As a reminder, we consider online
linear least squares regression using SGDm with a fixed η, μ under a periodic covariate shift. Data
is sampled from the sequence of random variables {Xk, Yk}k∈N where Xk is normally distributed
and shifts periodically in expectation. Yk is an unchanging linear function of Xk with zero-mean
observation noise. For ease of exposition we consider the one dimensional regression problem with
Xk ∈ R and Yk ∈ R, however the theory applies to any learning system representable as an LTV
ODE satisfying the assumptions required for Theorem 1.
Under this covaiate shift, Xk varies sinusoidally in [-a, a] with period T = f. Note that although
Xk is periodic in expectation it admits a stationary distribution. The full specification of the data
generating process is as follows
Xk ~N(Xk,1)
Xk = a cos(2πfk)
Yk = θ; Xk + θ + E
~N(0,1)
where θ* = [θj, θg]T are the target weights.
We consider a squared error loss function,
L(z; θ) = (y - y)2
= [hθ, χi-(hθ*, Xi + 卅2
=[(θιx + θ2) — (θɪ x + θ; + e)]2
Hence, taking gradients at time k w.r.t. θk we have
VθkL(Zk θk)
2xk [(θkιXk + θk2 ) -MIXk + 或2 + E)]
.2[(θkιXk + θk2 ) - (θk]Xk + θk2 + e)]
Taking expected gradients,
E[VθkL(zk; θk)]
E[2[(θkl Xk + θk2 Xk ) - (θk1 Xk + θk2 Xk + EXk)]]]
.E[2[(θkι Xk + θk2) - (θkιXk + θk2 + e)]]]
2[(θk1 (1 + Xk) + θk2Xk) - (θkι (1+ Xk) + θk2Xk + E[E]Xk)]
-	2[(θkι Xk + θk2 )-(θkι Xk + θk2 + E[e])]
2 Γ(θkι	(1	+ Xk) + θk2Xk)	- (θkι (1 + Xk) +	θk2Xk)
2 _	(θkl Xk + θk2 )-(θkι Xk + θk2 )	_
2 jθkι (1	+	Xk) - θkι (1 +	Xk) + θk2Xk - θk2 Xk-
2 _	θkι Xk - θk1 Xk+θk2 - θk2	_
2
(1 + Xk)	Xk] I"θkι -θkι
Xk 1 ] [θk2 - θk2
In the notation of Proposition 1, we can write this as
gk(θk) = Bk [θk - θk]T
with Bk = 2
(1 + Xk) Xk
Xk 1
If we let X(t) = acos(2∏fη-21), then we induce an X(t) from which {Xk} is sampled, as per
Section A.4.3. This results in Bk = B(√ηk with the matrix-valued function B(t)
B(t) = 2
(1 + X(t)2)
X(t)
X(t)
1
13
Published as a conference paper at ICLR 2022
From Proposition 2, we have that our learning system numerically integrates an LTV of the form
θ(t) +	1-μ θ(t)	+ B(t)(θ(t)-θ*)=0	With	(θ(t)-θ*)=[*,-W]	(5)
η	22
Note that B(t) is piecewise continuous and periodic with period T = f.
This second order ODE can be represented as a system of first order ODEs With the transformation
ξl = θl -毋
ξ2 = θ - θ2
ξ3 = θ1
_•
ξ4 = θ2
so that the equation equation 5 can be written in standard first order linear form ξ = A(t)ξ:

0
0
-2(1 + x2(t))
-2x(t)
0
0
-2x(t)
-2
1
0
μ-1
√η~
0
0
1
0
μ-1
(6)
The matrix B(t) is piecewise continuous and periodic, which implies the matrix A(t) also shares
these properties. The ODE equation 5 satisfies all the assumptions needed to make use of Theorem 1
to determine the stability of solution trajectories. We can obtain a fundamental solution matrix, ψ(t),
of equation 6 satisfying initial conditions ψ(0) = I4×4 by numerical computation through the use of
an ODE solver.
Once we have this matrix ψ(t), we evaluate it at time T = f to obtain the system,s monodromy
matrix. Now we only need to determine the spectral radius, ρ, of ψ(T) to determine the stability
of solution trajectories for the system. Details regarding why this is true are given in the proof of
Theorem 1.
We repeat this process, sweeping over values of μ, η, and f to determine the stability of solution
trajectories for systems to triples of fixed values for these hyperparameters. The results of this process
are given in Figure 6 below.
A.2 Reducing Resonant Responses
In experiments 4.1 and 4.2, we see that as the momentum parameter is reduced, the tendency to
resonate is mitigated. This aligns with theoretical predictions, which (in the context of those particular
experiments) suggest that a sufficiently low μ makes SGDm convergent across all frequencies in
the band we evaluate. Intuitively, this aligns with the role μ plays in the ODE, as it appears in the
following coefficient on the first order derivative of system equation 3 (i.e. the system’s ‘damping’
coefficient), repeated below with the damping coefficient α explicitly labelled:
θ(t) + αθ(t) + B(t)(θ(t) - θ*)=0	α := 1√-μ	(7)
When α = 0, the system has no damping (i.e. friction is zero, in the physical analogue) so resonant
responses are maximized. As α increases, damping increases, and resonant responses are reduced.
There are two ways to increase α: reduce μ or reduce η.
A.2.1 Reducing the momentum parameter
Figure 6b shows the theoretical heatmap across all possible momentum values, and across a much
wider band of frequencies, which suggests a trend: setting μ toa sufficiently low value will completely
mitigate resonance, though setting μ too low will worsen convergence rate. For now, we will set aside
the observation that μ too low worsens convergence rate, and we will now show that reducing μ will
reliably dampen resonance across all other experiments.
In 4.5, ADAM’s parameter β1 was varied, and we see that reducing β1 decreases the tendency
to resonate. Since βι is the nearest parameter to μ in SGDm, the desired trend has already been
14
Published as a conference paper at ICLR 2022
Cov. shift period T
(a) Spectral radius heatmap, η = 0.01
Cov. shift period T
(b) Spectral radius heatmap, wide range, η = 0.01
1.45
1.25
1.00
P
0.50
0.25
0.00
Cov. shift period T
(c) Spectral radius heatmap, η = 0.005
(d) Spectral radius heatmap, wide range, η
0.005
(e) Spectral radius heatmap, η = 0.001
(f) Spectral radius heatmap, wide range, η = 0.001
Figure 6: Spectral radii of the monodromy matrices induced by particular momentum μ and period T
values (x and y pixel coordinates, respectively). Step-size η decreasing with each row. Each column
shares the same range of μ, T. The right column shows the full range of momentum μ ∈ [0,1] and a
much wider range of periods T than the left. The left column figures simply ‘zoom in‘ to the upper
left corner of the figure to its right. (a) corresponds to the μ, η, T as Experiments 4.1 and 4.2, with
contour lines identical to 2a 2b such that the white line separates the convergent region below from
the divergent regions above. (b), with the extent of (a) indicated by dotted lines. The figures in
the right column show a horizontal band of minimum spectral radius (i.e. max convergence rate),
which suggests that there exists a least-resonating μbest across nearly all frequencies, with only minor
deviations toward the far left, where data approaches iid. Though the location of μbest clearly changes
with step-size η, and likely depends upon other aspects of the specific learning problem.
15
Published as a conference paper at ICLR 2022
(a) Regression w/ periodic Xk, 5 samples per Xk
ιo4
l∣θ-θ*ll
IO2
IO1
4 = 0.95
•	H	= 0.925
•	μ = 0.9
•	/J	= 0.875
•	Id	= 0.85
“III IiMJMWllIiihl IiN
0	10	20	30	40	50
Mean shift interval T
(b) Regression w/ stochastic Xk, 0.4 Xk variance
IO4
l∣θ-θ*ll
IO2-
(d) Neural net w/ stochastic Xk, Xk variance = 0.4
Figure 7: Re-running with reduced momentum values for highest resonance configurations chosen
from Experiments 4.3 (a), 4.4 (b, c), and 4.6 (d). In all cases, reducing momentum significantly
dampens resonant response, with μ = 0.85 completely mitigating resonant response.
μ = 0.95
∙ μ = 0.925
• μ = 0.9
∙	μ = 0.875
∙	μ = 0.85
Mean shift interval T
(c) Regression w/ stochastic Xk, d = 9
demonstrated. For the remainder of this section, we show the resonance-damping behaviour of μ
in the remaining experiments: 4.3, 4.4, and 4.6. In particular, from each experiment we choose
the configuration which had the highest tendency to resonate, and we modify the experiment by
running them with several decreasing values of momentum μ. See Figure 7 for results. In Section 4,
these experiments used momentum μ = 0.95, and here we run with μ ∈ [0.85,0.95], with all other
experimental parameters identical.
A.2.2 Reducing the step-size
Another way to reduce tendency to resonate is suggested by the damping coefficient equation 7:
reducing step-size η. Here we repeat Experiments 4.1 and 4.2, including two smaller step-sizes η to
empirically demonstrate that trend. Specifically, Figure 8 shows the resonance heatmap and spectral
radius contour lines for regression in two weights with covariate shift, identically to Experiments 4.1
and 4.2. The left column shows sinusoidal covariate shift, and the right column the AR(2) covariate
shift. Each row corresponds to a fixed step-size η ∈ {0.01, 0.005, 0.001}, and it is clear that resonant,
diverging regions are significantly reduced in size as step-size is decreased, with a narrower band of
frequencies diverging, and the minimum momentum μ required for resonance increasing towards 1.
This trend is reflected in both the empirical heatmap results, as well as the theoretically predicted
spectral radius contour lines.
A.3 Experiments 4.1 and 4.2 with Stochastic Gradients
Experiment 4.1 1 serves as a simple setting, as close to the theory of Section 3 as possible. In
particular, each descent step made at time k uses the gradients induced by 20 samples from each
Xk so that the gradient signal is much closer to the expected gradient than the stochastic setting.
Experiment 4.2 uses the same technique.
16
Published as a conference paper at ICLR 2022
Cov. shift period T
(a) Resonance regions, sinusoidal Xk, η = 0.01
Peak freq. period T
(b) Resonance regions, AR(2) Xk, η = 0.01
inf 180 90 60 45 36 30 26 22 20
Cov. shift period T
(c) Resonance regions, sinusoidal Xk, η = 0.005
inf 180 90 60 45 36 30 26 22 20
Peak freq, period T
(d) Resonance regions, AR(2) Xk, η = 0.005
Cov. shift period T
(e) Resonance regions, sinusoidal Xk, η = 0.001
Figure 8: Re-running with reduced step-size for Experiments 4.1 (a, c, e) and 4.2 (b, d, f). In both
cases, reducing step-size significantly dampens resonant response.
Peak freq, period T
(f) Resonance regions, AR(2) Xk, η = 0.001
17
Published as a conference paper at ICLR 2022
Cov. shift period T
(a) Resonance regions, sinusoidal Xk, η = 0.01
inf 180 90 60 45 36 30 26 22 20
Peak freq, period T
(b) Resonance regions, AR(2) Xk, η = 0.01
Cov. shift period T
(c) Resonance regions, sinusoidal Xk, η = 0.005
0.99
0.96
μ
0.97
0.95
inf 180 90 60 45 36 30 26 22 20
Peak freq, period T
(d) Resonance regions, AR(2) Xk, η = 0.005
Cov. shift period T
(e) Resonance regions, sinusoidal Xk, η = 0.001
Peak freq, period T
(f) Resonance regions, AR(2) Xk, η = 0.001
Figure 9: Re-running with fully stochastic gradients for Experiments 4.1 (a, c, e) and 4.2 (b, d, f). In
all cases, increasing gradient variance significantly dampens resonant response, but does not change
it otherwise.
Here we repeat Experiments 4.1 and 4.2, but drawing only a single sample from each Xk, so that the
gradient signal is fully stochastic. That is, we significantly increase gradient signal’s variance. As
Figure 9 shows, the resonant response is significantly damped by increasing gradient signal variance.
We observe no other change in the empirical heatmap: regions of instability are neither introduced
nor removed, and the existing regions do not change shape or location. Only the size of instability
regions is affected, which is most apparent under comparison to Figure 8, where the empirically
observed instability regions are expanded to closer match the theoretical predictions derived from
expected gradients.
Also the bright band of instability in all subfigures of Figure 9 are wider than those of 8. This aligns
with intuition from the conventional setting with iid data, where increasing gradient variance reduces
optimizer stability.
18
Published as a conference paper at ICLR 2022
A.4 Proofs
A.4. 1 Additional Notation
Wherever we write the training input generating process {Xk}, it is implicit that the last dimension is
fixed at 1 if one wishes to describe linear models with a bias term. In this way, the notation hθ, Xki
accommodates linear models with or without a bias term.
We use ξ to denote the phase space coordinates of weights θ, meaning ξ
[θ]
where vectors
θ,θ ∈ Rd are stacked so that the resulting ξ ∈ R2d. We denote the d-dimensional zero vector as
0d. For an ODE system ξ(t) = f (ξ(t), t) with arbitrary solution trajectory ξ(t) approximated by a
sequence of iterates {ξk}, we denote the integration time step as h, and the numerical flow as φh,k,
which is the map such that ξk+1 = φh,k(ξk). We denote the k-th discrete timestep as tk, which we
always use to refer to the k-th multiple of integration time step h, i.e. tk = hk.
Similar to the main body, when a function of time θ(t) is approximated by a discrete time sequence
{θk }, they are denoted with the same symbol, and distinguished by parenthetical argument t and
subscript k, respectively. At the risk of abusing notation, we will adhere to this convention and use
{θk} to be a discrete sequence approximating the function of time θ(t), which refers to the time
derivative of θ(t). This means the symbol θk is the k-th iterate of a sequence approximating θ(t).
We now restate Assumptions, Propositions, and the Theorem from Section 3, providing complete
proofs.
A.4.2 Linear Time-Varying Expected Gradient via Covariate Shift
Assumption 1. The covariate generating process {Xk}k∈N is not identically distributed:
Corr(Xk1, Xk1) 6= Corr(Xk2, Xk2) for some k1, k2 ∈ N.
Assumption 2. The targets Yk are a fixed linear function of Xk with iid zero-mean observation noise,
Ek such that Ekk ] = 0. That is, Yk = (θ* ,Xki+ Ek where θ* ∈ Rd is fixed forall k.
Proposition 1. Under Assumptions 1, 2, a linear model with weights θk
making predictions Ybk
hθk , Xk i with a mean squared error (MSE) objective will induce time-varying linear expected loss
gradients gk (θk) = Bk (θk 一 θ*) forall k ∈ N, where Bk ∈ Rd×d and B% = Bk? for some k∖,k?.
Proof. We will show that the gradient of MSE loss ^θk L(Z; θk) is a random variable whose ex-
pectation will take the linear form Bk(θk 一 θ*). We start with gradient for arbitrary time step
k
VθkL(Z; θk) = Vθk (Yk - Yk)2
=VθJ(hθk ,Xk i-Yk)2]
=Vθk[(hθk,Xki 一 hθ*,Xki + Ek)2]
=Vθk[(hθk — θ*,Xki + Ek )2]
=2(hθk — θ*,Xk i + Ek )Vθk [hθk 一 θ*, Xki + Ek]
=2(hθk 一 θ*,Xk i + Ek IXk
=2hθk — θ* , Xk iXk + 2Ek Xk
MSE def’n
Ybk def’n from Prop. 1
Yk def’n from Ass. 2
h∙, •〉distributivity
chain rule
(*)
19
Published as a conference paper at ICLR 2022
Taking expectation with respect to distribution Pk :
gk(θk) = EPk [VθkL(Z; θk)]
=EPk [2hθk - θ*, XkiXk + 2∈kXk]
=2Epk [hθk — θ*,Xk〉Xk ]+2Epk[q ]EpJXk ]
= 2EpJ<θk - θ*,XkiXk]
= 2EpJXk hθk- θ* ,Xki]
= 2EpJXkhXk,θk - θ*>]
= 2Epk[Xk (XT (θk-θ*))]
= 2EpJXkXT](θk - θ*)
=Bk (θk — θ*)
g def’n
by equation *
E linear, k indep.
E[k] = 0
scalar and vector commute
h∙, ∙i commutative
h∙, ∙i matrix form
matrix mult. associative
In the last step, we have defined the matrix Bk as twice the expected outer product of Xk with itself,
which is precisely the autocorrelation matrix, and is a function of Xk’s first two moments as follows.
Bk = 2EPk [XkXkT]
= 2Corr(Xk,Xk)
=2 (Cov(Xk, Xk) + EPk [Xk]Epk [Xk]t)
So if there exists k1, k2 ∈ N such that the autocorrelation matrices differ
Corr(Xk1,Xk1) 6=Corr(Xk2,Xk2)
then Bk1 6= Bk2. Assumption 1 provides the necessary k1, k2, so the proposition is proved.
□
A.4.3 ODE Correspondence
In Section 3 of the main body, We defined B(t), LiPschitz continuous in t, such that Bk = B(√ηk).
If we assume that {Xk } are sampled at intervals of √η from an underlying continuous-time stochastic
Process X(t) for Which EP (t) [X(t)] and Cov(X(t), X(t)) are LiPschitz continuous int, this naturally
induces a unique Lipschitz continuous B(t). Note P (t) is the distribution at time t, and we can
extend equation 2 as follows.
B(t)=2Cov(X(t),X(t))+2EP(t)[X(t)]EP(t)[X(t)]T	(8)
This means that {Bk} is sampled from a continuous B(t), with subsequent Bk spaced √η apart. In
essence, step-size η acts as a scaling constant, such that tne unit of t is ≈ √ discrete steps of k.
(Exact when √ is an integer.)
Proposition 2. The SGDm iterates {θk} numerically integrate the ODE system equation 3 with
integration step √η and first order consistency.
θ(t) + 1-μ θ(t)+ B(t)(θ(t) — θ*)=0	(3)
√η
Proof. The proof proceeds two parts. First we derive a first order operator-splitting integrator for the
ODE equation 3, then we show that it is equivalent to SGDm equation 1.
(Part 1: Operator-Splitting Integrator)
Let ξ : R 7→ R2d be the vector valued function of time whose first d elements are θ(t), and last d
elements are θ(t):
ξ(t)∖]	(9)
20
Published as a conference paper at ICLR 2022
i.e. ξ is a phase space transformation allowing us to rewrite ODE equation 3 in the form of equation 4,
which can then be split into a sum of separate systems f [1] , f [2] as follows
ξ(t) =	f(ξ(t),t)	(10)
ξ(t) =	0d×d	Id×d =—B(t)	— 1√μId×d∖ ξ(t)	
ξ(t) =	0d×d	Id×d	0d×d	0d×d 0d×d	0d×d	—B(t)	— ~√^ Id×d	
ξ(t) =	f[1](ξ(t),t)+f[2](ξ(t),t)	
(11)
(12)
(13)
Let h > 0 be an integration time step, φ[h1,]k(ξk) be the implicit Euler numerical flow of f[1] (ξ(t), t):
φ[h1,]k(ξk) = ξk + hf[1](φ[h1,]k(ξk), tk+1)
=ξk+h 00dd××dd 0Idd××dd φ[h1,]k(ξk)
Let φ[h2,]k(ξk) be explicit Euler numerical flow of f[2] (ξ(t), t):
φ[h2,]k(ξk) = ξk +hf[2](ξk,tk)
0d×d	0d×d
=ξk + h [-B(tk)	— √Id×d∖ ξk
The composed flow
φh,k := φ[h1,]k ◦ φ[h2,]k
is a sequentially split operator, which has splitting error order 1 because equation 10 is time-varying
linear system (Farag6 et al., 2011). The operators being composed, implicit and explicit Euler, are
both order 1 consistent with their respective systems (Iserles, 2008). The overall order of consistency
of a split operator is the minimum of splitting error, and the orders of the composed flows (Csom6s &
Farag6, 2008). So we have that equation 13 approximates equation 10 with order 1 consistency.
(Part 2: SGDm Equivalency)
We will now show that equation 13 is equivalent to SGDm when integration timestep h = √η, where
η is the SGDm step-size. We start with the definition of ξk+1 as the numerical flow of ξk :
ξk+1 := φh,k (ξk )
= φ[h1,]k(φ[h2,]k(ξk))	by equation 13
= φ[h2,]k(ξk)+h 00dd××dd 0Idd××dd φ[h1,]k(φ[h2,]k(ξk))	subst’n from equation 11
=φ[h2,]k(ξk)+h 00dd××dd 0Idd××dd ξk+1
C	「力 1
=φ[h2,]k(ξk)+h θk0+1
_ 0d . 0	0d×d	0d×d	e ʌ θ θk+ι
=(ξk + h [-B(tk) — √ Id×d∖ ξk) + hl 0d
=ξk+ + h -B(tk)(θk —；*) — 恭0k_ ) + h θ0+1
λ
θk+1
ξ++1 = ξ+ + h -B(t+)(θ+-θ*)- 1-ημθ+
def’n of ξ++1
matrix mult.
subst’n from equation 12
matrix mult.
simplification
Recalling that ξ+
relations:
θ+ — θ*
θ+
θ++1 = θ+ + hθ++1
the above immediately provides the following two recurrence
θ++1 = θ+ + h ( -B(t+ )(θ+ — θ* ) —
21
Published as a conference paper at ICLR 2022
Tk T	1 . ∙ .	. ∙	. ∙	.	7 1 . 1	. , ' Γt y- ∙> l ʌ	,	♦	7	1—	IlC	1—ʌ
Now let integration time step h be the square root of SGDm SteP-size, h = √η, and define Vk = √ηθk.
Substituting h, vk, we proceed via elementary algebra:
θk+ι = θk+√η (√++n
vk+1
+ √η I -B(tk )θk -
θk+1 = θk + vk+1
vk+1
vk+1
Vk + η (-B(tk)(θk - θ*) -(1 - μ)vk
η
Vk - ηB(tk )(θk - θ*) - (I - μ)vk
μvk - ηB(tk )(θk - θ*)
Note that B(tk) = Bk, and that Bk defines the time-varying gradients induced by covariate shift and
linear regression to a linear target (Proposition 1). Hence, under those conditions, we have arrived at
SGDm equation 1. The proof is complete.
□
ml	1	F	.1	♦	1	A∕,∖,,1	.	A / ，、 X /	CF
The reader may observe that, given a solution ξ(t) to the system ξ(t) = A(t)ξ(t), the proof above
shows that the SGDm iterates {θk} are precisely a first order numerical approximation of the first
-θ(t) — θ*
θ(t)
d dimensions of ξ(t)
, but the remaining d dimensions are approximated only up
to a scale factor √η by iterates {vk}. However, this is not an issue. Solutions θ(t) to the system
equation 3 are embedded within the first d dimensions of ξ(t), so the remaining d dimensions and
iterates {Vk } do not affect the result.
A.4.4 Parametric Resonance for ODE Convergence and Divergence
Theorem 1. When B(t) is periodic such that B(t) = B(t + T), the spectral radius ρ of ψ(T)
characterizes the stability of solution trajectories of equation 3 as follows:
•	ρ > 1 =⇒ trivial solution θ(t) = θ* is unstable. All other solutions diverge as θ(t) → ∞
exponentially with rate ρ.
•	ρ < 1 =⇒ trivial solution is asymptotically stable, all other solutions converge as
θ(t) → θ* exponentially with rate P.
Proof. The proof of this theorem relies heavily on the well-established mathematics of Floquet theory
and the stability result is contained in Theorem 1.9 and Theorem 1.10 of (Halanay, 1966). Theorem
1.10 states that for a system of differential equations of the form
, .,. .,.
ξ = A(t)ξ,	A(t + T )= A(t)	(14)
where A(t) is piecewise continuous, the stability of the trivial solution ξ (t) ≡ 0 is determined by the
spectral radius of the system’s monodromy matrix M defined below
M = ψ-1(0)ψ(T)	where ψ(t) = A(t)ψ(t)
(Monodromy Matrix)
Matrix-valued functions ψ(t) are the system’s fundamental solution matrix, and elementary existence
results for linear systems allow one to choose a ψ(t) such that ψ(0) = I so that the monodromy
matrix simplifies as
M = ψ-1(0)ψ(T)
= I-1ψ(T)
= Iψ(T)
M = ψ(T)
Below we denote the spectral radius of ψ(T) (and hence of M) as ρ.
(First Order Linear Form, Stability via Floquet)
22
Published as a conference paper at ICLR 2022
Below we show that equation 3 can be transformed to the form equation 14 with A(t) continuous
such that trivial solution stability of equation 14 is equivalent to stability of the solution θ(t) = θ*.
Let ξ(t) be the phase space transformation of θ(t), similarly to the proof of Proposition 2
ξ(t) :
^θ(t) - θ*
[θ(t)
so that for each t, θ(t) ∈ Rd and ξ(t) ∈ R2d. This means equation 3 (restated below)
θ(t) + 1-μθ(t) + B(t)(θ(t) - θ*) = 0
√η
is equivalent to the following first order linear form
,.. .. ..
ξ(t) = A(t)ξ(t)
where A(t)
0d×d
-B(t)
Id×d
-1√μ Id×d
Since B(t) is periodic and continuous, which is stronger than the requisite piecewise continuity.
Since all other submatrices of A(t) are constant, we have that A(t) is also periodic and (piecewise)
continuous. Now Theorem 1.10 in (Halanay, 1966) immediately provides the following
•	ρ > 1 =⇒ trivial solution ξ(t) = 0 is unstable. All other solutions diverge as ξ(t) → ∞
exponentially with rate ρ.
•	ρ < 1 =⇒ trivial solution is asymptotically stable, all other solutions converge as ξ(t) → 0
exponentially with rate ρ.
Since ξ (t)
^θ(t) - θ*
[θ(t)
•	The trivial solution ξ(t) = 0 is equivalent to θ(t) = θ*
•	ξ(t) → 0 is equivalent to θ(t) → θ*
•	ξ (t) → ∞ is equivalent to θ(t) → ∞
The theorem is proved.
□
A.5 Detailed descriptions of experiments
Below we provide details of the experiments from section 4 in the main body. The tables below contain
details about the data generating process, learning algorithm and optimizer, and hyperparameter
choices.
Experiments 4.1-4.4 introduce new data generating processes {Xk} via their mean sequences {Xk}.
For those experiments, a sample trajectory is visualized for several values of the process’ frequency
tuning parameter, either f or T. In addition to the sample trajectory, the frequency content of each
process is shown in two ways: the power spectral density (PSD) of {Xk} estimated via a long sample
trajectory, as well as the PSD of the underlying mean sequence {Xk}. As one would expect from the
sampling relationship between Xk and Xk, the PSD of {Xk} is simply the PSD of {Xk} with a noise
floor.
Experiments 4.5, 4.6 reuse the process from experiment 4.4, so the visual depiction is elided.
23
Published as a conference paper at ICLR 2022
A.5.1 Experiment 4.1 Details
0	50	100	150	200	250	300
Training Step k
(a)	Sinusoidal mean {Xk } sample trajectory for
f = 0.01
(ZH/ωp CISd
(b)	Sinusoidal mean {Xk} frequency content for
f = 0.01
2
o-
SamplesXfc
Means x∣i
⅜⅛⅛Λ⅛⅛
・：∙ - .∙∙., .∙∕∙∙. ∙ √ .∙ ■
ZH/ωp CISd
0	50	100	150	200	250	300
Training Step k
(c) Sinusoidal mean {Xk } sample trajectory for
f = 0.03
(d) Sinusoidal mean {Xk} frequency content for
f = 0.03
• SampIesXfc
Means ×∣f
2
O-
-2-
Λ n.Λ∙Λ A*A,Λ∙.Λ
“京他M y y.⅛学学常*第$ ¾,*v
0	50	100	150	200	250	300
Training Step k
(e)	Sinusoidal mean {Xk } sample trajectory for
f = 0.05
(f)	Sinusoidal mean {Xk } frequency content for
f = 0.05
Figure 10:	Sample trajectories (left) and power spectral densities (right) for Experiment 4.1 (Table 2)
for three values of frequency tuning parameter f . The signal is a sinusoid, so we observe a single
frequency peak translated left or right by f .
24
Published as a conference paper at ICLR 2022
Table 2: Details for linear regression sinusoidal covariate shift problem.
Sinusoid Mean Frequency	f ∈ [0,0.05]	
Covariate Shift Mean	Xk = 0.5 sin(2πfk)
Input Sampling	Xk 〜N(Xk, 1)
Target Function (Fixed ∀k)	Yk =。；Xk + θ2 θɪ, θ2 〜Uniform[-1,1]
Model and Optimizer	Yck = θk,1 Xk + θk,2 θ0,1,θ0,2 〜Uniform[-1,1] (θk)k∈N J SGDm(η,μ) η = 0.01,μ ∈ [0.95, 0.999] k ∈ [0, 104]
25
Published as a conference paper at ICLR 2022
A.5.2 Experiment 4.2 Details
(a)	AR(2) mean {Xk} sample trajectory for f
0.01
(ZH/ωp CISd
(b)	AR(2) mean {Xk} frequency content for f
0.01
• SampIesXfc
Means x∣i
2
0	50	100	150	200	250	300
Training Step k
(c)	AR(2) mean {Xk} sample trajectory for f
0.03
----SamplesXfc
MeansXfc
0.0	0.2	0.4	0.6	0.8	1.0
Freq. (Hz)
(d)	AR(2) mean {Xk} frequency content for f =
0.03
• SamplesXfc
Means ×∣f
0	50	100	150	200	250	300
Training Step k
(e)	AR(2) mean {Xk} sample trajectory for f =
0.05
(ZH∕mp) QSd
0.0	0.2	0.4	0.6	0.8	1.0
Freq. (Hz)
(f)	AR(2) mean {Xk} frequency content for f =
0.05
Figure 11:	Sample trajectories (left) and power spectral densities (right) for Experiment 4.2 (Table
3) for three values of frequency tuning parameter f . The signal is a AR(2) process designed for a
single frequency peak, as observed. The process is stochastic, so its frequency peak is widened by
the imperfect correlation.
26
Published as a conference paper at ICLR 2022
Table 3: Details for linear regression sinusoidal covariate shift problem. Rather than choosing a
frequency f and using it directly as in Table 2, we use f together with the stationary distribution
variance 0.1 to compute AR(2) coefficients φ1, φ2.
Expected Dominant Freq. in Xk	f ∈ [0,0.05]	
Xk Stationary Dist. (Fixed ∀f)	P = N(0, 0.1)
Covariate Shift Mean	Xk = φ1xk-1 + φ2xk-2 + ξk ξk ~N(0,10-5) iid X1,X2 ~ P Φ = 4φ2	(2 f)
	φ1 = I	c cos(2πf ) φ2 - 1 Φ2 s.t. [Xk | Xk-1 ~ P] ~ P
Remaining Parameters	Xk, Yk, Yk, θ*, θo, (θk)k∈N, η, μ, k same as Table 2
27
Published as a conference paper at ICLR 2022
A.5.3 Experiment 4.3 Details
• SampIesXfc
Means ×∣i
O 50 IOO 150	200	250	300
Training Step k
(a) Periodic mean {Xk} sample trajectory for T
50
(ZH/ωp CISd
T = 50
• SamplesXfc
Means x∣i
Training Step k
(c) Periodic mean {Xk} sample trajectory for T
100
ZH/ωp CISd
---SamPIeSXk
-20
-30
-40
0.0	0.2	0.4	0.6	0.8	1.0
Freq. (Hz)
(d) Periodic mean {Xk} frequency content for
T = 100
• SamplesXfc
Means ×∣f
2
O-
-2-
O 50 IOO 150	200	250	300
----SampIesXjf
Means'
-ħ…
Oooo
1 1 2
- -
nh话 P) αsd
Training Step k
(f) Periodic mean {Xk} frequency content for T
150
Figure 12: Sample trajectories (left) and power spectral densities (right) for Experiment 4.3 (Table 4)
for three values of frequency tuning parameter T . The signal is a square wave in a single dimension
for the figure, but is a square wave randomly oriented in higher dimensions for the actual experiment.
There are multiple frequency peaks, with the frequency domain contracted by changing T .
(e) Periodic mean {Xk} sample trajectory for T
150
28
Published as a conference paper at ICLR 2022
Table 4: Details for linear regression square wave covariate shift problem.
Square Wave Mean Period	T ∈ [0,120]	
Input Dimensionality	d = 5	
Covariate Shift Mean	(2∣ξξ∣∣ if b竽c ≡ 0 mod 1 [-	2∣ξ前 if b2C ≡ 1 mod 1 ξ ~N(0,Id×d) iid		
Input Sampling	Xk ~N(Xk, 0∙25Id×d)	
Target Function (fixed for all k)	Yk = hθ^d],Xk i + ΘS+1 + ^k θ* ~ N(0, cId+1×d+1) where C 二 Ek 〜N(0, 0.1)	0.25
Model and Optimizer	Yk = hθ[Ld],xk i + θd+1 θο ~ N(0, cld+ι×d+ι) where C = (θk)k∈N - SGDm(η,μ) η = 0.01, μ = 0.95 k ∈ [0,104]	0.25
29
Published as a conference paper at ICLR 2022
A.5.4 Experiment 4.4 Details
Table 5: Details for linear regression problem with stochastically switching covariate shift mean.
Mean Switching Interval	T ∈ [0, 50]		
Mean Switching Variance	V = 0.25 for Figure 4b,		v ∈ [0, 0.4] for Figure 4a
Input Dimensionality	d ∈ [1,9] for Figure 4b,		d = 5 for Figure 4a
	Xk = ξi where i =	k	
Covariate Shift Mean		T	
	ξi ~ N(0,vId×d) iid		
Remaining Parameters	Xk, Yk, Yk, θ*, θo, (θk)k∈N, η, μ, k same as Table 4		
30
Published as a conference paper at ICLR 2022
-2
• SamplesXfc
Means ×k
0	50	100	150	200	250	300
Training Step k
(a) Switching mean {Xk } sample trajectory for
T = 10
ZH/ωp CISd
(b) Switching mean {Xk} frequency content for
T = 10
• SamplesXfc
Means 又 ∣i
-2
0	50	100	150	200	250	300
Training Step k
(c) Switching mean {Xk } sample trajectory for
T = 30
(ZH/ωp) CISd
(d) Switching mean {Xk} frequency content for
T = 30
Oooo
1 1 2
- -
(ZH∕mp) asd
-30	：	：： W i;
:	三	V
-40^-------1-----j-∏----=~I----i——I-------1
0.0	0.2	0.4	0.6	0.8	1.0
Freq. (Hz)
(f) Switching mean {Xk } frequency content for
T = 50
2
-2
• SamplesXfc
Means x∣i
0	50	100	150	200	250	300
Training Step k
(e) Switching mean {Xk } sample trajectory for
T = 50
Figure 13: Sample trajectories (left) and power spectral densities (right) for Experiments 4.4-4.6
(Tables 4-7) for three values of frequency tuning parameter T . The mean signal is essentially white
noise ‘stretched’ according to T. The signal is a single dimension for the figure, but is of higher
dimensions for the actual experiment. The frequency content is a vertical reflection of Figure 12
(right,) with frequency troughs instead of peaks. Note that the frequency content is the mildest of all
settings, having the lowest and least-defined peaks, which aligns with intuition: there is nothing even
remotely oscillatory in its definition. It is reasonable to expect that many ‘natural’ processes would
have more aggressive frequency content.
31
Published as a conference paper at ICLR 2022
A.5.5 Experiment 4.5 Details
For visual depiction, see Figure 13.
Table 6: Details for linear regression problem with stochastically switching covariate shift, optimized
with ADAM instead of SGDm.
Mean Switching Interval	T ∈ [0, 100]
Mean Switching Variance	V = 1.0
Input Dimensionality	d = 5
Covariate Shift Mean	Xk identical to Table 5
Input Sampling	Xk ~N(Xk, 0.1Id×d)
Target Function (fixed for all k)	Yk, θ*, ∈k identical to Table 5
Model	Yck , θ0 identical to Table 5
Optimizer	(θk)k∈N J ADAM(η, βι,β2 η = 0.01, β1 ∈ [0.9, 0.99], β2= 0.999 k ∈ [0,104]
A.5.6 Experiment 4.6 Details
For visual depiction, see Figure 13.
Table 7: Details for neural network regression problem with stochastically switching covariate shift.
Mean Switching Interval	T ∈ [0, 100]
Mean Switching Variance	v ∈ [0,0.4]
Input Dimensionality	d = 2
Covariate Shift Mean	Xk identical to Table 5
Input Sampling	Xk ~N(Xk, 0.1Id×d)
Target Function (fixed for all k)	Yk = cos(∏∣∣Xk ||) + Ek Ek 〜N(0, 0.1)
Model	Yck = f(Xk; θk) two hidden layers of 20 activations θ0 initialized as He et. al.
Optimizer	(θk)k∈N J SGDm(η,μ) η = 0.01,μ = 0.95 k ∈ [0,2 × 104]
32