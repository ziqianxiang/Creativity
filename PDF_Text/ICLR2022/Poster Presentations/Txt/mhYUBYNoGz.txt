Published as a conference paper at ICLR 2022
Machine Learning For Elliptic PDEs:
Fast Rate Generalization Bound, Neural Scal-
ing Law and Minimax Optimality
Yiping Lu
Institute for Computational
& Mathematical Engineering
Stanford University
Stanford, CA 94305, USA
yplu@stanford.edu
Jianfeng Lu
Mathematics Department
Duke University
Durham, NC 27708-0320
jianfeng@math.duke.edu
Haoxuan Chen
Department of Computing
and Mathematical Sciences,
Caltech
Pasadena, CA 91125, USA
haoxuan@caltech.edu
Lexing Ying
Department of Mathematics
Stanford University
Stanford, CA 94305, USA
lexing@stanford.edu
Jose Blanchet
Department of Management Science & Engineering
Stanford University
Stanford, CA 94305, USA
jose.blanchet@stanford.edu
Ab stract
In this paper, we study the statistical limits of deep learning techniques for solving
elliptic partial differential equations (PDEs) from random samples using the Deep
Ritz Method (DRM) and Physics-Informed Neural Networks (PINNs). To simplify
the problem, We focus on a prototype elliptic PDE: the Schrodinger equation on a
hypercube with zero Dirichlet boundary condition, which is applied in quantum-
mechanical systems. We establish upper and loWer bounds for both methods,
Which improve upon concurrently developed upper bounds for this problem via
a fast rate generalization bound. We discover that the current Deep Ritz Method
is sub-optimal and propose a modified version of it. We also prove that PINN
and the modified version of DRM can achieve minimax optimal bounds over
Sobolev spaces. Empirically, folloWing recent Work Which has shoWn that the
deep model accuracy Will improve With groWing training sets according to a poWer
laW, We supply computational experiments to shoW similar-behavior of dimension
dependent poWer laW for deep PDE solvers.
1	Introduction
Partial differential equations (PDEs) play a prominent role in many disciplines of science and engi-
neering. The recent deep learning breakthrough and the rapid development of sensors, computational
poWer, and data storage in the past decade has draWn attention to numerically solving PDEs via
machine learning methods (Long et al., 2018; 2019; Raissi et al., 2019; Han et al., 2018; Sirignano &
Spiliopoulos, 2018; Khoo et al., 2017), especially in high dimensions Where conventional methods
become impractical. The set of applications that motivate this interest is Wide-ranging, including
computational physics (Han et al., 2018; Long et al., 2018; Raissi et al., 2019), inverse problem
(Zhang et al., 2018; Gilton et al., 2019; Fan & Ying, 2020) and quantitative finance (Heaton et al.,
2017; Germain et al., 2021). The numerical methods generated by the use of deep learning techniques
are mesh-less methods, see the discussion in (Xu, 2020). A natural deep learning technique in the
1
Published as a conference paper at ICLR 2022
problems that are based on a standard feed-forward type of architecture takes advantage (when
available) of a variational formulation, whose solution coincides with the solution of the PDE of
interest. Despite the success and popularity of adopting neural networks for solving high-dimensional
PDEs, the following question still remains poorly answered.
For a given PDE and a data-driven approximation architecture, how large
the sample size and how complex the model are needed to reach a prescribed
performance level?
In this paper, we aim to establish the numerical analysis of such deep learning based PDE solvers.
Inspired by recent works which showed that the empirical performance of a model is remarkably
predictable via a power law of the data number, known as the neural scaling law (Kaplan et al., 2020;
Hestness et al., 2017; Sharma & Kaplan, 2020), we aim to explore the neural scaling law for deep
PDE solvers and compare its performance to Fourier approximation.
Among the various approaches of using deep learning methods for solving PDEs, in this work, we
focus on the Deep Ritz method (DRM) (E & Yu, 2018; Khoo et al., 2017) and the Physics-Informed
Neural Networks (PINN) approach (Sirignano & Spiliopoulos, 2018; Raissi et al., 2019), both of
which are based on minimizing neural network parameters according to some loss functional related
to the PDEs. To provide theoretical guarantees for DRM and PINN, following (Lu et al., 2021b;
Duan et al., 2021; Bai et al., 2021), we decompose the error into approximation error (Yarotsky, 2017;
Suzuki, 2018; Shen et al., 2021) and generalization error (Bartlett et al., 2005; XU & Zeevi, 2020;
Farrell et al., 2021; Schmidt-Hieber et al., 2020; Suzuki, 2018). However, instead of the O(1∕√n)
(n is the number of data sampled) slow rate generalization bounds established in prior work (Lu
et al., 2021b; Shen et al., 2021; Xu, 2020; Shin et al., 2020), we utilize the strongly convex structure
of the DRM and PINN objectives and provide an O(1/n) fast rate generalization bound (Bartlett
et al., 2005; Xu & Zeevi, 2020) that leads us to a non-parametric estimation bound. Our theory also
suggests an optimal selection of network size with respect to the number of sampled data. Moreover,
to illustrate the optimality of our upper bound, we also establish an information-theoretic lower bound
which matches our upper bound for PINN and a modified version of DRM.
We also test our theory by numerical experiments. Recent works (Hestness et al., 2017; Kaplan et al.,
2020; Rosenfeld et al., 2019; Mikami et al., 2021) studying a variety of deep learning algorithms
all find the same polynomial scaling relation between the testing error and the number of data. As
the number of training data n increases, the population loss L of well-trained and well-tuned models
scales with n as a power-law Lb 去 for some α. (Sharma & Kaplan, 2020) also scans over a large
range of α and problem dimension d and finds an approximately α b d scaling law. In Section 4, we
conduct numerical experiments to show that this phenomenon still appears for deep PDE solvers and
this neural scaling law tests more idiosyncratic features of the theory.
1.1	Related Works
Neural Scaling Law The starting point of our work is the recent observation across speech, vision
and text (Hestness et al., 2017; Kaplan et al., 2020; Rosenfeld et al., 2019; Rosenfeld, 2021) that the
empirical performance of a model satisfies a power law scales as a power-law with model size and
dataset size. (Sharma & Kaplan, 2020) further finds out that the power of the scaling law depends on
the intrinsic dimension of the dataset. Theoretical works (Schmidt-Hieber et al., 2020; Suzuki, 2018;
Suzuki & Nitanda, 2019; Chen et al., 2019b; Imaizumi & Fukumizu, 2020; Farrell et al., 2021; Jiao
et al., 2021c) explore the optimal power law under the non-parametric curve estimation setting via a
plug-in neural network. Our work extends this line of research to solving PDEs.
Deep Network Based PDE Solver. Solving high dimensional partial differential equations (PDEs)
has been a long-standing challenge due to the curse of dimensionality. At the same time, deep
learning has shown superior flexibility and adaptivity in approximating high dimensional functions,
which leads to state-of-the-art performances in a wide range of tasks ranging from computer vision
to natural language processing. Recent years, pioneer works (Han et al., 2018; Raissi et al., 2019;
Long et al., 2018; Sirignano & Spiliopoulos, 2018; Khoo et al., 2017) try to utilize the deep neural
networks to solve different types of PDEs and achieve impressive results in many tasks (Lu et al.,
2021a; Li et al., 2020). Based on the natural idea of representing solutions of PDEs by (deep) neural
networks, different loss functions for solving PDEs are proposed. (Han et al., 2018; 2020) utilize the
2
Published as a conference paper at ICLR 2022
Feynman-Kac formulation which turns solving PDE to a stochastic control problem and the weak
adversarial network (Zang et al., 2020) solves the weak formulations of PDEs via an adversarial
network. In this paper, we focus on the convergence rate of the Deep Ritz Method (DRM) (E & Yu,
2018; Khoo et al., 2017) and Physics-Informed neural network (PINN) (Raissi et al., 2019; Sirignano
& Spiliopoulos, 2018). DRM (E & Yu, 2018; Khoo et al., 2017) utilizes the variational structure of
the PDE, which is similar to the Ritz-Galerkin method in classical numerical analysis of PDEs, and
trains a neural network to minimize the variational objective. PINN (Raissi et al., 2019; Sirignano &
Spiliopoulos, 2018) trains a neural network directly to minimize the residual of the PDE, i.e., using
the strong form of the PDE.
Theoretical Guarantees For Machine Learning Based PDE Solvers. Theoretical convergence
results for deep learning based PDE solvers raises wide interest recently. Specifically, (Lu et al.,
2021b; Grohs & Herrmann, 2020; Marwah et al., 2021; Wojtowytsch et al., 2020; Xu, 2020; Shin
et al., 2020; Bai et al., 2021) investigate the regularity of PDEs approximated by neural network and
(Lu et al., 2021b; Luo & Yang, 2020) further provide a generalization analysis. (Nickl et al., 2020)
introduces a prior over the solution of the PDE and considers an equivalent white noise model (Brown
& Low, 1996). (Nickl et al., 2020) provides the rate of convergence of the posterior. Our paper does
not need to introduce the prior on the target function and provides a non-asymptotic guarantee for
finite number of data. At the same time, (Nickl et al., 2020) can only be applied to linear PDEs
while our proof technique can be extended to nonlinear ones. All these papers also fail to answer
the question that how to determine the network size corresponding to the sampled data number to
achieve a desired statistical convergence rate. (Hutter & Rigollet, 2019; Manole et al., 2021) consider
the similar problem for the optimal transport problem, i.e. Monge-ampere equation. Nevertheless, the
variational problem we considered is different from (Hutter & Rigollet, 2019; Manole et al., 2021)
and leads to technical difference. The most related works to ours are two concurrent papers (Duan
et al., 2021; Jiao et al., 2021a;b). However, our upper bound is faster than (Duan et al., 2021; Jiao
et al., 2021a;b). In this paper, we also show that generalization analysis in (Lu et al., 2021b; Duan
et al., 2021; Luo & Yang, 2020) are loose due to the lack of a localization technique (De Boor &
De Boor, 1978; Bartlett et al., 2005; Koltchinskii, 2011; Xu, 2020). With observation of the strong
convexity of the loss function, we follow the fast rate results for ERM (Schmidt-Hieber et al., 2020;
Xu & Zeevi, 2020; Farrell et al., 2021) and provide a near optimal bound for both DRM and PINN.
1.2	Contribution
In short, we summarize our contribution as follows
•	In this paper, we first considered the statistical limit of learning a PDE solution from
sampled observations. The lower bound shows a non-standard exponent different from
non-parametric estimation of a function.
•	Instead of the O(1/√n) slow rate generalization bounds in (LU et al., 2021b; DUan et al.,
2021; Jiao et al., 2021a;c), we utilized the strongly convex nature of the variational form
and provided a fast rate generalization boUnd via the localization methods (Van De Geer,
1987; Bartlett et al., 2005; Koltchinskii, 2011; Srebro et al., 2010; XU & Zeevi, 2020).
We discovered that the cUrrent Deep Ritz Methods is sUb-optimal and propose a modified
version of it. We showed that PINN and the modified version of DRM can achieve nearly
min-max optimal convergence rate. OUr resUlt is listed in Table 1.
•	We tested the recently discovered neUral scaling law (Hestness et al., 2017; Kaplan et al.,
2020; Rosenfeld et al., 2019; Hashimoto, 2021) for deep PDE solvers nUmerically. The
empirical resUlts verified oUr theory.
2	Set-up
We consider the static Schrodinger equation with zero Dirichlet boundary condition on the domain Ω,
which we assUme to be the Unit hypercUbe in Rd . In order to precisely introdUce the problem, we
recall some standard notions. We consider our domain as Ω = [0,1]d and use L2 (Ω) to denote the
space of square integrable functions on Ω with respect to the Lebesgue measure. We let L∞(Ω) be
3
Published as a conference paper at ICLR 2022
Upper Bounds				Lower Bound
Objective Function	Neural Network	Previous Bound	Fourier Basis	
Deep Ritz	_	2s-2 n- d+2s-2 log n	2s-2- n- d+4s-4 log n (DUan etal., 2021)	_	2s-2 n- d+2s-2	2s-2 n- d+2s-4
Modified Deep Ritz	2s-2- n- d+2s-2 log n		/		2s-2 n- d+2s-4	2s-2 n- d+2s-4
PINN	_	2s-4 n- d+2s-4 log n	2s-4- n- d+4s-8 log n (JiaO et al., 2021a)	2s-4 n- d+2s-4	2s-4 n- d+2s-4
Table 1: Upper bounds and lower bounds we achieve in this paper and previous work. The upper
bound colored in red indicates that the convergence rate matches the min-max lower bound.
the space of essentially bounded (with respect to the LebesgUe measure) functions on Ω and C(∂Ω)
denotes the space of continuous functions on ∂Ω.
Let f ∈ L2(Ω), V ∈ L∞(Ω), and, g ∈ L∞(Ω). Our focus is on the analysis OfDeeP-Learning-based
numerical methods to solve the elliptic PDE:
-∆u + Vu = f
u=g
in Ω,
on ∂Ω.
(2.1)
2.1	Loss Functions for S olving PDEs and Induced Evaluation Metric
In this paper, we mainly focus on analyzing Deep Ritz Methods (DRM) and Physics-Informed Neural
Network (PINN). In this subsection, we first introduce the objective function and algorithm of the
two methods.
Deep Ritz Methods (E & Yu, 2018; Sirignano & Spiliopoulos, 2018) Recall that the equation 2.1
is equivalent to following variational form
u* = arg min EDRM(U)= arg min ɪ / ∣∣VuH2 + V|u|2 dx — f fudx, (2.2)
HI(Q)	HI(Q) 2 Jω	ω
where U is minimized over H1(Ω) with boundary condition given by g on ∂Ω.
This variational form provides the basis for the DRM type method for solving the static Schrodinger
equation based on neural network ansatz. More specifically, the energy functional given in equa-
tion 2.2 is viewed as the population risk function to train an optimal estimator approximation of the
solution to the PDE within a parameterized hypothesis function class F ⊂ H1(Ω). In this paper, we
also rely on the strong convexity of the DRM objective with respect to the H1 norm.
Proposition 2.1. For DRM, we further assume 0 < Vmin ≤ V(x) ≤ Vmax, then we have
max{12Vmax} (EDDR(U)- EDRM(Ut)) ≤ ∣u-u*∣Hι ≤ mXΓ⅛nr (EDDR(U)- EDRM(U*))
holdsfor all U ∈ H1(Ω)
Physics-Informed Neural Network (Raissi et al., 2019; Sirignano & Spiliopoulos, 2018). PINN
solves 2.1 via minimizing the following objective function
Ut = arg min EPINN(U) := arg min ∣ ∣Δu(x) — V(X)U(X) + f(x)∣2dx.
H0(Ω)	H0(Ω)Jω
The objective function EPINN can also be viewed as the population risk function and we can train
an optimal estimator approximation of the solution to the PDE within a parameterized hypothesis
function class F ⊂ Hq (Ω). In this paper, we also rely on the strong convexity of the PINN objective
with respect to the H2 norm, for which we need some additional assumptions on the potential.
4
Published as a conference paper at ICLR 2022
Proposition 2.2. For PINN, we further assume V ∈ L∞(Ω) with 0 < Cmin < V2 一 ∆V, 0 ‹
Cmin < V (x) ≤ VmaX and — ∆V (x) ≤ VmaX ,then we have for all U ∈ H1(Ω)
1
2(1 + Vmax + VmaX)
(EPPNN(U)- EPPNN(u*)) ≤ W 一 u*∣∣H2
≤ —rι∖τ	1 (Epinn(U)- Epinn(u*)).
-max{1,Cmin} \	)
2.2 Estimator Setting
Empirical Loss Minimization In order to access the d-dimensional integrals, DRM (E & Yu,
2018; Khoo et al., 2017) and PINN (Raissi et al., 2019; Sirignano & Spiliopoulos, 2018) employ
a Monte-Carlo method for computing the high dimensional integrals, which leads to the so-called
empirical risk minimization training for neural networks. To define the empirical loss, let {Xj }jn=1 be
an i.i.d. sequence of random variables distributed according to the uniform distribution PΩ in domain
Ω. We also have access to noisy observations fj = f (Xj) + ξj (1 ≤ j ≤ n) of the right hand side of
the PDE (2.1), where ξj are i.i.d. bounded random variables with zero mean and independent of Xj.
Define the empirical losses En by setting
n
DRM
En
1n	1	1
(U) = n X [∣Ω∣∙ (-kVu(Xj )k2 + - V (Xj )∣u(Xj )|2 - fj u(Xj))],
(2.3)
j=1
n2
EnINN(U) = n X [∣Ω∣∙ (△"(Xj) - V(Xj)u(Xj) + fj)],
j=1
(2.4)
where ∣Ω∣ represents the Lebesgue measure of the domain.
Once given an empirical loss En , we apply the empirical loss minimization to seek the estimation Un ,
i.e. Un = arg minu∈F En(U), where F is the parametrized hypothesis function space we consider.
Some examples can be reproducing kernel Hilbert space (Chen et al., 2021b) and tensor training
format (Richter et al., 2021; Chen et al., 2021a). In this paper, we consider sparse neural network and
truncated Fourier basis, which can achieve min-max optimal estimation rate for the non-parametric
function estimation (Tsybakov, 2008; Schmidt-Hieber et al., 2020; Farrell et al., 2021; Suzuki, 2018;
Chen et al., 2019b; Jiao et al., 2021c; Nitanda & Suzuki, 2020).
Sparse Neural Network Function Space In this paper, the hypothesis function space F is ex-
pressed by the neural network function space following (Schmidt-Hieber et al., 2020; Suzuki, 2018;
Farrell et al., 2021). Let us denote the ReLU3 activation by η3(x) = max{x3, 0} (x ∈ R), which
is used in (E & Yu, 2018). For a vector x, η(x) is operated in an element-wise manner. Define the
space of all neural networks with height L, width W, sparsity constraint S and norm constraint B as
Φ(L, W, S, B) := {(W(L)η3(∙) + b(L))。…(W⑵η3(∙) + b⑵)。(W(I)X + b⑴)|
W(L) ∈ R1×W, b(L) ∈ R,W(1) ∈ RW×d,b(1) ∈ RW,W(l) ∈ RW×W,b(l) ∈ RW(1 < l < L),
L
X(kW(l)k0 +kb(l)k0) ≤ S, max kW(l)k∞,∞ ∨ kb(l)k∞ ≤B},	(2.5)
l=1
where。denotes the function composition, k∙∣∣0 is the '0-norm of the matrix (the number of non-zero
elements of the matrix) and ∣∣ ∙ ∣∣∞,∞ is the '∞-norm of the matrix (maximum of the absolute values
of the elements).
Truncated Fourier Basis Estimator We also consider the Truncated Fourier basis as our esti-
mator. Denote the domain we are interested in by Ω ⊆ [0,1]d. For any Z ∈ Nd, we consider the
corresponding Fourier basis function φz(x) := e2πihz,xi (x ∈ Ω). Any function f ∈ L2(Ω)
can be represented as a weighted sum of the Fourier basis f(x) := Pz∈Nd fzφz (x), where
fz := h f (x)φz(x)dx (∀ z ∈ Nd) is the Fourier coefficient. This inspires us to use the Fourier basis
whose index lies in a truncated set Zξ = {z ∈ Z∣∣z∣∞ ≤ ξ} to represent the function class F as
Fξ = {Pkzk∞≤ξ azΦz∣az ∈ R,kzk∞ ≤ ξ}.
5
Published as a conference paper at ICLR 2022
3 Lower B ound
In this section, we aim to consider the statistical limit of learning the solution of a PDE. As discussed
in Propositions 2.1 and 2.2, we directly consider the H1 norm for DRM and H2 norm for PINN as
the evaluation metric. The lower bounds are shown as follows.
Theorem 3.1 (Lower bound). We denote u* (f) to be the solution ofthe PDE 2.1 and we can access
randomly sampled data {Xi, fi}i=ι,…，n as described in Section 2.2. Wefurther assume u* (f) ∈ HS
for a given s ∈ Z+. Then we have the following lower bounds.
DRM Lower Bound. For all estimators ψ : (RdLn X R0n → Hs(Ω), we have
i.f SUp Ekψ({Xi,fi}i=ι,…,n) - u*(f )kHι & n-d+2s-4 .
ψ u*∈Hs(Ω)
PINN Lower Bound. For all estimators ψ : (Rd)0n × R0n → HS (Ω), we have
呼 sup E∣∣ψ({Xi,fi}i=ι,…,n) - u*(f )kH2 & n-d⅛-4.
ψ u*∈Hs(Ω)
(3.1)
(3.2)
2(β-k)	2
Given that n d+2β is the minimax rate of estimating the k-th derivative of a β-smooth density in L2
(Liu & Wang, 2012; Prakasa Rao, 1996; Muller & Gasser, 1979), the lower bound obtained here is
the rate of estimating the right hand side function f in terms of the H-1 norm. Given the H-1 norm
error estimate on f, we can achieve an estimate of u, which provides an alternative way to understand
our upper bound. (See discussion in Appendix E.) The lower bound is non-standard, for the 2s - 2 in
the numerator is different from the 2s - 4 in the denominator.
4 Upper B ound
To theoretically understand the empirical success of Physics-Informed Neural Networks and the
Deep Ritz solver, in this section, we aim to prove that the excess risk ∆En := E(Un) — E(U) of a
well-trained neural networks on the PINN/DRM loss function will follow precise power-law scaling
relations with the size of the training dataset. Similar to (Xu, 2020; Lu et al., 2021b; Duan et al.,
2021; Jiao et al., 2021a;b), we decompose the excess risk into approximation error and generalization
error. Different from the concurrent bound (Duan et al., 2021; Jiao et al., 2021a), we provide a fast
rate O(1/n) by utilizing the strong convexity of the objective function established in Section 2.1 and
achieve a faster and near optimal upper bound. We show that the generalization error can be bounded
by the fixed point (i.e. the solution of φ(r) = r) of the local Rademacher complexity
ψ(r) = Rn({I(U) Iku -u*kA ≤r}),
where Rn is the Rademacher complexity, I(u) = ∆u + Vu, Il ∙ kA = k ∙ IIh2 for PINN and
I (U) = ∣∣Vuk2 + Vu, k ∙ kA = k ∙ ∣∣hi for DRM. We put detailed definition and analysis in Appendix
B.4. We first provide a meta theorem to decompose the error into approximation and a fast rate
generalization error. Then we plug in the approximation and generalization error calculated in
Appendix B.3 and Appendix B.2 and finally achieve the following upper bounds. We also put a more
detailed proof sketch in Appendix A.2.
Physics Informed Neural Network.
Theorem 4.1.	(Informal Upper Bound of PINN with Deep Neural Network Estimator) With proper
assumptions, consider the sparse Deep Neural Network function space Φ(L, W, S, B) with pa-
rameters L = O(1), W = O(nd+2s-4 ), S = O(nd+2s-4), B = O(nd+2s-4), then the Physics
Informed estimator UPN = minu∈Φ(L,w^,s,B) EnPNN(U) SatiSfieS the following upper bound with
high probability
kUDNNNN-U*kH2 . n-谣言logn.
Theorem 4.2.	(Informal Upper Bound of PINN with Truncated Fourier Series Estimator) With proper
assumptions, consider the Physics Informed Neural Network objective with a plug-in Fourier Series
estimator UFNNe = mi□u∈Fξ(Ω) EnNN(U) With ξ = Θ(n d+2s-4), then with high probability we have
kUFooNNer - U*kH2 . n-d+≡44.
6
Published as a conference paper at ICLR 2022
Deep Ritz Methods.
Theorem 4.3.	(Informal Upper Bound of DRM with Deep Neural Network Estimator) With proper as-
sumptions, consider the sparse Deep Neural Network function space Φ(L, W, S, B) with parameters
L = O(1), W = O(n d+2s-2), S = O(n d+2s-2), B = O(n d+2s-2) ,then the Deep ritz estimator
UIDRM = mi□u∈Φ(L,w,s,B) ElDRM (U) satisfies the following upper bound with high probability
kuDNM -u*kHι . n-d⅛-2 logn.
Theorem 4.4.	(Informal Upper Bound of DRM with Truncated Fourier Series Estimator) With proper
assumptions, consider the Deep Ritz objective with a plug in Fourier Series estimator UDRMfe =
mi□u∈Fξ(Ω) EIDRM(u) with ξ = Θ(n d+2s-2), then with high probability we have
kUDRMer-u*kHι . n-d⅛-2
Remark.
•	There is a common belief that Machine learning based PDE solvers can break the curse of
dimensionality (E & Yu, 2018; Grohs et al., 2018; Lanthaler et al., 2021). However, we
_	2s-2
obtained an n- 2s-4+d convergence rate, which can become super slow in high dimension.
Our analysis showed that it is essential to constrain the function space to break the curse of
dimensionality. (Lu et al., 2021b) considered the DRM in Barron spaces. (Ongie et al., 2019)
showed that functions in the Barron space enjoy a smoothness s at the same magnitude
as d , which will also lead to convergence rate independent of the dimension using our
upper bound. Neural network can also approximate mixed sparse grid spaces (Montanelli &
Du, 2019; Suzuki, 2018) and functions on manifold (Nitanda & Suzuki, 2020; Chen et al.,
2019b) without curse of dimensionality. Combined with these approximation bounds, we
can also achieve a bound that breaks the curse of dimensionality using Theorem B.12 and
B.9. In this paper, we aim to consider the statistical power of the loss function in common
function spaces and leave the curse of dimensionality as a separate topic.
•	Our bound is faster than the concurrent bound (Duan et al., 2021; Jiao et al., 2021a) for
we provided a fast rate O(1/n) by utilizing the strong convexity of the objective function
2s-2	2s-2
and improved the convergence rate from n- d+4s-4 to n- d+2s-2 for Deep RitZ and from
_ 2s-4	2s-4
n- d+4s-8 to n- d+2s-4 for PINN. Compared to the lower bound provided in Section 3, our
bounds for PINN is near-optimal while the upper bound for DRM is sub-optimal. We believe
our bound is tight and put the discussion in Appendix E. We’ll propose a modified version
of DRM to match the upper and lower bound in the next section.
•	For upper bound of DRM, due to a technical issue, we assumed that the observation we
access is clean, i.e fi = f (Xi). We conjecture that add noising on observation will not
effect the rate and leave this to future work.
5 Modified Deep Ritz Methods
Comparing the lower bound in Section 3 and the upper bound in Section 4, we find out that the
Physics-Informed Neural Network achieves min-max optimality while the Deep RitZ Method does
not. In this section, we propose a modified version of Deep RitZ which can be statistically optimal.
As discussed in Appendix E, the reason behind the suboptimality of DRM comes from the high
complexity introduced via the uniform concentration bound of the gradient term in the variational
form. At the same time, we further observed that the / ∣∣Vuk2dx does not require any query from
the right hand side function f, which means that we can easily make another splitted sample to
approximate the kVUk2dx term more precisely.
1N 1	1n	1
EMDRM(U) = NN X [∣Ω∣∙ 2∣Vu(Xj)k2] + n X [∣Ω∣∙ (2V(Xj)∣u(Xj)|2 - fjU(Xj))]
j=1
j=1
(5.1)
7
Published as a conference paper at ICLR 2022
OnCe We sampled more data for approximating J ∣Vu∣2dx, We can achieve an near optimal bound
N
for the Truncated FOUner Estimator when N & n d+2s-4.
Theorem 5.1. (Informal Upper Bound of DRM with Truncated Fourier Series Estimator)With proper
assumptions, consider the Deep Ritz objective With a plug in Fourier Series estimator UMDRM 二
minu∈Fξ(Ω) ENDnRM(U) With ξ = Θ(n d+2s-4) and NN & n d+2s-4, then we have
ku 镭鬻-u*kH
2s-2
―—■—------
n d+2s-4.
-
Remark. We still cannot achieve optimal rate for neural network even with modified DRM methods.
The reason is because the number of neurons is not a good complexity measure for the gradient of the
function. Thus, the bound for ψ(r) = Rn({I(u) | ku - u* k2H1 ≤ r}) is not enough for achieving
optimal convergence rate. However, following (Schmidt-Hieber et al., 2020; Suzuki, 2018; Imaizumi
& Fukumizu, 2020; Chen et al., 2019b; Farrell et al., 2021) to use deep networks for estimating
functions, we optimize the best neural network with constrained sparsity in our paper. Here we
conjecture that there exists a computable complexity measure that can make DRM statistically optimal
and leave finding the right complexity of the neural network’s gradient to be future work.
6 Experiments
In this section, We conduct several numerical experiments to verify our theory. We folloW the neural
netWork and hyper-parameter setting in (Chen et al., 2020). Due to the page limit, We only put the
experiments for Deep Ritz Methods here.
6.1	The Modified Deep Ritz Methods
In this section, We conduct experiments Which substantiate our theoretical results for modified Deep
Ritz methods. For simplicity, We take V (x) = 1 in our experiment. We conduct experiment in
2-dimension and select the solution of the PDE as U* = Pz kz k-sφz (x) ∈ Hs. We shoW the log-log
plot of the H 1 loss against the number of sampled data for s = 4 in Figure 1. We use an OLS
estimator to fit the log-log plot and put the estimated slope and corresponding R2 score in Figure 1.
As our theory predicts, the modified Deep Ritz Method converges faster than the original one. All the
derivation of the tWo estimators is listed in Appendix E.
6.2	Dimension Dependent S caling Law.
We conduct experiments to illustrate that the
population loss of Well-trained and Well-tuned
Deep Ritz method Will scale With the d-
dimensional training data number N as a poWer-
law L ɑ Na. We also scan over a range of d
and α and verify an approximately α a d scal-
ing law as our theory suggests. We use the same
test function in Section 6.1 as the solution of our
PDE. For simplicity, we take V (x) = 1 in our
experiment. We train the deep Ritz method on
20, 80, 320, 1280, 10240 sampled data points
for 5,6,7,8,9,10 dimensional problems and we
Figure 1: The log-log plot and estimated conver-
gence slope for Modified DRM and DRM using
Fourier basis, showing the median error over 5
replicates.
(a) Deep Ritz Methods	(b) Modified Deep Ritz Methods
	(a) Deep Ritz Methods	(b) Modified Deep Ritz Methods
Theory	≠÷⅛ =07 d + 2 S — 2	2 S-2 =1 d + 2s — 4
Empirical	0.6595	0.7953
R2 Score	0.91	0.89
plot our results on the log-log scale. Results are
shown in Figure 2. We discover the L a nd+2 scaling law in practical situations.
6.3	Adaptation To The Simpler Functions.
(Sharma & Kaplan, 2020) showed that the neural scaling law will adapt to the structure that the target
function enjoys. This adaptivity enables the neural network to break the cure of the dimensionality
for simple functions in high dimension. (Suzuki & Nitanda, 2019; Chen et al., 2019a) also observed
this theoretically. For solving PDEs, we also observed this adaptivity in practice. Here we tested the
following two hypothesis
8
Published as a conference paper at ICLR 2022
0	2	4 6s
L09 OfTa In Ing gg Number.
5-Dlm∙nsk>n
WUY- VCSrS-Vn-OS ⅞ Vo-
O 2	4	6 β
L090fT⅛lnln9 gta Number.
6∙Dltnensloπ
WUY- VCSrS-Vn-OS ⅞ Vo-
0	2	4 6s
L09 of 7⅛ Inlng gta Number.
7-Dlm∙ Iislon
02-
1
I0-0-
i-02-
%
?-04-
0	2	4	6 s
L09 of 7⅛ Inlng gta Number.
9∙Dltnensloπ
,jg,jg,jg,j,4,s
OOOOfff
Bo- E-WLeAn* 3 §
0	2	4	6 s
L09 OfItaInIng gta Number.
10-Dlm∙∏5lon
aN IO9-ss9o□杏j∙∙ππe9g JO əuəAaX 9ΛPB9=dpmn
7	β
Dimension.
Figure 2: We verify the dimension dependent scaling law empirically. The multiplicative inverse
of the scaling law coefficient is highly linear with the dimension d, showing the mean error over 2
replicates.
0	2	4	6 s
L09 of 7⅛ Inlng gta Number.
β∙DItnensloπ
•	Random Neural Network Teacher. Following (Sharma & Kaplan, 2020), we also tested
random neural network using He initialization (He et al., 2015) as the ground turth solution
u*. (De Palma et al., 2018) showed that random deep neural networks are biased towards
simple functions and in practice we observed a scaling law at the parametric rate. Specifically,
we obtained a linear estimate with slope α = -0.50679429 and a R2 score = 0.96 in the
log-log plot. See Figure 3(a).
•	Simple Polynomials. Neural network can approximate simple polynomials exponentially
fast (Wang et al., 2018). Thus, we select the ground truth solution to be the following
simple polynomial in 10 dimensional spaces u*(x) = xιx2 + …+ x9x10. In this example,
we obtained a linear estimate with slope α = -0.49755418 and a R2 score = 0.99 in the
log-log plot. See Figure 3(b).
7 Conclusion and Discussion
Conclusion In this paper, We considered the
statistical min-max optimality of solving a PDE
from random samples. We improved the previ-
ous bounds (Xu, 2020; Lu et al., 2021b; Duan
et al., 2021; Jiao et al., 2021a) by providing the
first fast rate generalization bound for learning
PDE solutions via the strongly convex nature of
the two objective functions. We achieved the
optimal rate via the PINN and a modified Deep
Ritz method. We verified our theory via numer-
ical experiments and explored the dimension
dependent scaling laws of Deep PDE solvers.
(a) Random Neural Network Teacher
3) y = X1X + X X4+-H X Xio
Log of Training D ata Number.
Log of Training Data NUmber
Figure 3: Neural networks have the ability to adapt
to simple functions and achieve convergence with-
out curse of dimensionality, showing the median
error over 5 replicates.
Discussion and Future Work Here we discuss several drawbacks of our theory
•	We restricted our target function and estimators in W1,∞ instead of H1 due to bounded-
ness assumption made in the local Rademacher complexity arguments. However, typical
functional used in physics is always unbounded, such as the Newtonian potential ".-1,-2,
which limits the application of our theory.
•	This paper did not discuss any optimization aspect of the deep PDE solvers and always as-
sumed that global optimum can be achieved. However, it is important to investigate whether
the optimization error (Suzuki & Akiyama, 2020; Chizat, 2021) will finally dominate.
•	Instead of solving a single PDE, recent works (Long et al., 2018; 2019; Li et al., 2020;
Lanthaler et al., 2021; Bhattacharya et al., 2020; Fan & Ying, 2020; Feliu-Faba et al., 2020)
considered the so-called "operator learning", which aims to learn a family of PDE/inverse
9
Published as a conference paper at ICLR 2022
problems using a single network. It is interesting to investigate the generalization bound and
neural scaling law there.
•	We find out that the sparsity of the neural network is not a good complexity measure of
neural network’s gradient. We conjecture that there exists an oracle complexity measure,
whose approximation and generalization bounds can lead Modified DRM to achieve the
optimal convergence rate.
Acknowledgments
Yiping Lu is supported by the Stanford Interdisciplinary Graduate Fellowship (SIGF). Jianfeng Lu
is supported in part by National Science Foundation via grants DMS-2012286 and CCF1934964.
Lexing Ying is supported by National Science Foundation under award DMS-2011699. Jose Blanchet
is supported in part by the Air Force Office of Scientific Research under award number FA9550-20-
1-0397 and NSF grants 1915967, 1820942, 1838576. Yiping Lu also thanks Taiji Suzuki, Atsushi
Nitanda, Yifan Chen, Junbin Huang, Wenlong Ji, Greg Yang, Yufan Chen, Zong Shang, Denny Wu,
Jikai Hou, Jun Hu, Fang Yao, Bin Dong and George Em Karniadakis for helpful comments and
feedback.
References
Robert A Adams and John JF Fournier. Sobolev spaces. Elsevier, 2003.
Genming Bai, Ujjwal Koley, Siddhartha Mishra, and Roberto Molinaro. Physics informed neural
networks (pinns) for approximating nonlinear dispersive pdes. arXiv preprint arXiv:2104.05584,
2021.
Peter L Bartlett, Olivier Bousquet, Shahar Mendelson, et al. Local rademacher complexities. The
Annals ofStatistics, 33(4):1497-1537, 2005.
Kaushik Bhattacharya, Bamdad Hosseini, Nikola B Kovachki, and Andrew M Stuart. Model reduction
and neural networks for parametric pdes. arXiv preprint arXiv:2005.03180, 2020.
Haim Brezis. Functional analysis, Sobolev spaces and partial differential equations. Springer
Science & Business Media, 2010.
Lawrence D Brown and Mark G Low. Asymptotic equivalence of nonparametric regression and white
noise. The Annals of Statistics, 24(6):2384-2398, 1996.
Jingrun Chen, Rui Du, and Keke Wu. A comprehensive study of boundary conditions when solving
pdes by dnns. arXiv preprint arXiv:2005.04554, 2020.
Minshuo Chen, Haoming Jiang, Wenjing Liao, and Tuo Zhao. Efficient approximation of deep relu
networks for functions on low dimensional manifolds. Advances in Neural Information Processing
Systems, 32:8174-8184, 2019a.
Minshuo Chen, Haoming Jiang, Wenjing Liao, and Tuo Zhao. Nonparametric regression on low-
dimensional manifolds using deep relu networks. arXiv preprint arXiv:1908.01842, 2019b.
Yian Chen, Jeremy Hoskins, Yuehaw Khoo, and Michael Lindsey. Committor functions via tensor
networks. arXiv preprint arXiv:2106.12515, 2021a.
Yifan Chen, Bamdad Hosseini, Houman Owhadi, and Andrew M Stuart. Solving and learning
nonlinear pdes with gaussian processes. arXiv preprint arXiv:2103.12959, 2021b.
Lenaic Chizat. Convergence rates of gradient methods for convex optimization in the space of
measures. arXiv preprint arXiv:2105.08368, 2021.
Carl De Boor and Carl De Boor. A practical guide to splines, volume 27. springer-verlag New York,
1978.
Giacomo De Palma, Bobak Toussi Kiani, and Seth Lloyd. Random deep neural networks are biased
towards simple functions. arXiv preprint arXiv:1812.10156, 2018.
10
Published as a conference paper at ICLR 2022
Chenguang Duan, Yuling Jiao, Yanming Lai, Xiliang Lu, and Zhijian Yang. Convergence rate analysis
for deep ritz method. arXiv preprint arXiv:2103.13330, 2021.
Weinan E and Bing Yu. The deep ritz method: a deep learning-based numerical algorithm for solving
variational problems. Communications in Mathematics and Statistics, 6(1):1-12, 2018.
Lawrence C Evans. Partial differential equations. Graduate studies in mathematics, 19(4):7, 1998.
Yuwei Fan and Lexing Ying. Solving electrical impedance tomography with deep learning. Journal
of Computational Physics, 404:109119, 2020.
Max H Farrell, Tengyuan Liang, and Sanjog Misra. Deep neural networks for estimation and inference.
Econometrica, 89(1):181-213, 2021.
Jordi Feliu-Faba, Yuwei Fan, and Lexing Ying. Meta-learning pseudo-differential operators with
deep neural networks. Journal of Computational Physics, 408:109309, 2020.
Sara A Geer and Sara van de Geer. Empirical Processes in M-estimation, volume 6. Cambridge
university press, 2000.
Maximilien Germain, HUyen Pham, and Xavier Warin. Neural networks-based algorithms for
stochastic control and pdes in finance. arXiv preprint arXiv:2101.08068, 2021.
Davis Gilton, Greg Ongie, and Rebecca Willett. Neumann networks for linear inverse problems in
imaging. IEEE Transactions on Computational Imaging, 6:328-343, 2019.
Philipp Grohs and Lukas Herrmann. Deep neural network approximation for high-dimensional
elliptic pdes with boundary conditions. arXiv preprint arXiv:2007.05384, 2020.
Philipp Grohs, Fabian Hornung, Arnulf Jentzen, and Philippe Von Wurstemberger. A proof that
artificial neural networks overcome the curse of dimensionality in the numerical approximation of
black-scholes partial differential equations. arXiv preprint arXiv:1809.02362, 2018.
Ingo Guhring, Gitta Kutyniok, and Philipp Petersen. Error bounds for approximations with deep relu
neural networks in w s, p norms. Analysis and Applications, 18(05):803-859, 2020.
Jiequn Han, Arnulf Jentzen, and E Weinan. Solving high-dimensional partial differential equations
using deep learning. Proceedings of the National Academy of Sciences, 115(34):8505-8510, 2018.
Jiequn Han, Jianfeng Lu, and Mo Zhou. Solving high-dimensional eigenvalue problems using deep
neural networks: A diffusion monte carlo like approach. Journal of Computational Physics, 423:
109792, 2020.
Tatsunori Hashimoto. Predicting the impact of dataset composition on model performance, 2021.
URL https://openreview.net/forum?id=butEPeLARP_.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
James B Heaton, Nick G Polson, and Jan Hendrik Witte. Deep learning for finance: deep portfolios.
Applied Stochastic Models in Business and Industry, 33(1):3-12, 2017.
Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad,
Md Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable,
empirically. arXiv preprint arXiv:1712.00409, 2017.
Jan-Christian Hutter and Philippe Rigollet. Minimax rates of estimation for smooth optimal transport
maps. arXiv preprint arXiv:1905.05828, 2019.
Masaaki Imaizumi and Kenji Fukumizu. Advantage of deep neural networks for estimating functions
with singularity on curves. arXiv preprint arXiv:2011.02256, 2020.
Yuling Jiao, Yanming Lai, Dingwei Li, Xiliang Lu, Yang Wang, and Jerry Zhijian Yang. Convergence
analysis for the pinns, 2021a.
11
Published as a conference paper at ICLR 2022
Yuling Jiao, Yanming Lai, Yisu Luo, Yang Wang, and Yunfei Yang. Error analysis of deep ritz
methods for elliptic equations. arXiv preprint arXiv:2107.14478, 2021b.
Yuling Jiao, Guohao Shen, Yuanyuan Lin, and Jian Huang. Deep nonparametric regression on
approximately low-dimensional manifolds. arXiv preprint arXiv:2104.06708, 2021c.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott
Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.
arXiv preprint arXiv:2001.08361, 2020.
Yuehaw Khoo, Jianfeng Lu, and Lexing Ying. Solving parametric pde problems with artificial neural
networks. arXiv preprint arXiv:1707.03351, 2017.
Vladimir Koltchinskii. Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery
Problems: Ecole d,Ete de Probabilites de Saint-Flour XXXVIn-2008, volume 2033. Springer
Science & Business Media, 2011.
Samuel Lanthaler, Siddhartha Mishra, and George Em Karniadakis. Error estimates for deeponets: A
deep learning framework in infinite dimensions. arXiv preprint arXiv:2102.09618, 2021.
Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and processes.
Springer Science & Business Media, 2013.
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew
Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations.
arXiv preprint arXiv:2010.08895, 2020.
Youming Liu and Huiying Wang. Convergence order of wavelet thresholding estimator for differential
operators on besov spaces. Applied and Computational Harmonic Analysis, 32(3):342-356, 2012.
Zichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong. Pde-net: Learning pdes from data. In
International Conference on Machine Learning, pp. 3208-3216. PMLR, 2018.
Zichao Long, Yiping Lu, and Bin Dong. Pde-net 2.0: Learning pdes from data with a numeric-
symbolic hybrid deep network. Journal of Computational Physics, 399:108925, 2019.
Denghui Lu, Han Wang, Mohan Chen, Lin Lin, Roberto Car, E Weinan, Weile Jia, and Linfeng
Zhang. 86 pflops deep potential molecular dynamics simulation of 100 million atoms with ab initio
accuracy. Computer Physics Communications, 259:107624, 2021a.
Jianfeng Lu, Yulong Lu, and Min Wang. A priori generalization analysis of the deep ritz method for
solving high dimensional elliptic equations. arXiv preprint arXiv:2101.01708, 2021b.
Tao Luo and Haizhao Yang. Two-layer neural networks for partial differential equations: Optimization
and generalization theory. arXiv preprint arXiv:2006.15733, 2020.
Tudor Manole, Sivaraman Balakrishnan, Jonathan Niles-Weed, and Larry Wasserman. Plugin
estimation of smooth optimal transport maps. arXiv preprint arXiv:2107.12364, 2021.
Tanya Marwah, Zachary C Lipton, and Andrej Risteski. Parametric complexity bounds for approxi-
mating pdes with neural networks. arXiv preprint arXiv:2103.02138, 2021.
Hiroaki Mikami, Kenji Fukumizu, Shogo Murai, Shuji Suzuki, Yuta Kikuchi, Taiji Suzuki, Shin-ichi
Maeda, and Kohei Hayashi. A scaling law for synthetic-to-real transfer: A measure of pre-training.
arXiv preprint arXiv:2108.11018, 2021.
Hadrien Montanelli and Qiang Du. New error bounds for deep relu networks using sparse grids.
SIAM Journal on Mathematics of Data Science, 1(1):78-92, 2019.
Hans-Georg Muller and Theo Gasser. Optimal convergence properties of kernel estimates of deriva-
tives of a density function. In Smoothing techniques for curve estimation, pp. 144-154. Springer,
1979.
12
Published as a conference paper at ICLR 2022
Richard Nickl, Sara van de Geer, and Sven Wang. Convergence rates for penalized least squares esti-
mators in pde constrained regression problems. SIAM/ASA Journal on Uncertainty Quantification,
8(1):374-413, 2020.
Atsushi Nitanda and Taiji Suzuki. Optimal rates for averaged stochastic gradient descent under neural
tangent kernel regime. arXiv preprint arXiv:2006.12297, 2020.
Greg Ongie, Rebecca Willett, Daniel Soudry, and Nathan Srebro. A function space view of bounded
norm infinite width relu nets: The multivariate case. arXiv preprint arXiv:1910.01635, 2019.
BLS Prakasa Rao. Nonparametric estimation of the derivatives of a density by the method of wavelets.
Bulletin of Informatics and Cybernetics, 28(1):91-100,1996.
Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A
deep learning framework for solving forward and inverse problems involving nonlinear partial
differential equations. Journal of Computational Physics, 378:686-707, 2019.
Alexander Rakhlin, Karthik Sridharan, and Alexandre B Tsybakov. Empirical entropy, minimax
regret and minimax risk. Bernoulli, 23(2):789-824, 2017.
Lorenz Richter, Leon Sallandt, and Nikolas Nusken. Solving high-dimensional parabolic Pdes using
the tensor train format. arXiv preprint arXiv:2102.11830, 2021.
Jonathan S Rosenfeld. Scaling laws for deep learning. arXiv preprint arXiv:2108.07686, 2021.
Jonathan S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction
of the generalization error across scales. arXiv preprint arXiv:1909.12673, 2019.
Johannes Schmidt-Hieber et al. Nonparametric regression using deep neural networks with relu
activation function. Annals of Statistics, 48(4):1875-1897, 2020.
Larry Schumaker. Spline functions: basic theory. Cambridge University Press, 2007.
Utkarsh Sharma and Jared Kaplan. A neural scaling law from the dimension of the data manifold.
arXiv preprint arXiv:2004.10802, 2020.
Zuowei Shen, Haizhao Yang, and Shijun Zhang. Neural network approximation: Three hidden layers
are enough. Neural Networks, 2021.
Yeonjong Shin, Zhongqiang Zhang, and George Em Karniadakis. Error estimates of residual
minimization using neural networks for linear pdes. arXiv preprint arXiv:2010.08019, 2020.
Justin Sirignano and Konstantinos Spiliopoulos. Dgm: A deep learning algorithm for solving partial
differential equations. Journal of computational physics, 375:1339-1364, 2018.
Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. Smoothness, low noise and fast rates. Advances
in neural information processing systems, 23, 2010.
Taiji Suzuki. Adaptivity of deep relu network for learning in besov and mixed smooth besov spaces:
optimal rate and curse of dimensionality. arXiv preprint arXiv:1810.08033, 2018.
Taiji Suzuki and Shunta Akiyama. Benefit of deep learning with non-convex noisy gradient descent:
Provable excess risk bound and superiority to kernel methods. arXiv preprint arXiv:2012.03224,
2020.
Taiji Suzuki and Atsushi Nitanda. Deep learning is adaptive to intrinsic dimensionality of model
smoothness in anisotropic besov space. arXiv preprint arXiv:1910.12799, 2019.
Joel A Tropp. An introduction to matrix concentration inequalities. arXiv preprint arXiv:1501.01571,
2015.
Alexandre B Tsybakov. Introduction to nonparametric estimation. Springer Science & Business
Media, 2008.
13
Published as a conference paper at ICLR 2022
Sara Van De Geer. A new approach to least-squares estimation, with applications. The Annals of
Statistics ,15(2):587-602,1987.
Qingcan Wang et al. Exponential convergence of the deep neural network approximation for analytic
functions. arXiv preprint arXiv:1807.00297, 2018.
Stephan Wojtowytsch et al. Some observations on partial differential equations in barron and
multi-layer spaces. arXiv preprint arXiv:2012.01484, 2020.
Jinchao Xu. The finite neuron method and convergence analysis. arXiv preprint arXiv:2010.01458,
2020.
Yunbei Xu and Assaf Zeevi. Towards optimal problem dependent generalization error bounds in
statistical learning theory. arXiv preprint arXiv:2011.06186, 2020.
Dmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94:
103-114, 2017.
Yaohua Zang, Gang Bao, Xiaojing Ye, and Haomin Zhou. Weak adversarial networks for high-
dimensional partial differential equations. Journal of Computational Physics, 411:109409, 2020.
Xiaoshuai Zhang, Yiping Lu, Jiaying Liu, and Bin Dong. Dynamically unfolding recurrent restorer:
A moving endpoint control method for image restoration. arXiv preprint arXiv:1805.07709, 2018.
14
Published as a conference paper at ICLR 2022
Appendix
A Appendix Organization and Proof Sketch
A. 1 Notations
In this section, We provide all the notations We need in the proof. Let Ω ⊂ Rd be some open set. We
denote C(Ω) the space of continuous functions on Ω and Ck(Ω) the space of all functions that are k
times continuously differentiable on Ω (∀ k ∈ Z+). For any n ∈ No (N0 := Z+ ∪ {0} is the set of
all non-negative integers) and 1 ≤ P ≤ ∞,we define the Sobolev space Wn,p(Ω) by
Wn,p(Ω) := {f ∈ Lp(Ω): Daf ∈ Lp(Ω), ∀α ∈ Nd with |a| ≤ n}.
In particular, when P = 2, we define Hn(Ω) := Wn,2(Ω) for any n ∈ No. Moreover, for any
f ∈ Wn,p(Ω) with 1 ≤ p < ∞, we define the Sobolev norm by:
1
kfkwn,P(Ω) = ( X kDɑfkLp(Ω))p.
0≤∣a∣≤n
In particular, when P = ∞, we have:
kfkwn,∞(Ω) := max l∣Dafk∑∞(Ω).
0≤∣a∣≤n
Consider the Fourier expansion f := Pz∈Nd fzφz(χ) of the function f ∈ Wn,p(Ω). We can
equivalently express the Sobolev norm as:
kfk
W n,P(Ω)= X kzknp|fz|p1/p,
z
where fz =儿 f (x)φz(x)dx = Rω f (x)e-2πihz,xi dx (X ∈ Ω) is the z—th Fourier coefficient of f.
Moreover, we use Wl1,p(Ω) to denote the closure of C1 (Ω) in W 1,p(Ω). In particular, when P = 2,
we define H0(Ω):= wJ2(ω).
Furthermore, we use ∣∣ ∙ ∣∣ to present the vector 2 norm and, given a data sample {Xi}n=1 ⊂ Ω,
k ∙ ∣∣n,p = (En∙p)1/p denote the empirical P norm, where En : L2(Ω) → R is the corresponding
empirical average operator defined as Enf := n Pn=1 f (Xi), ∀ f ∈ L2(Ω). Given two quantities
X and Y , we write X . Y when the inequality X ≤ CY holds, where C is some constant. For two
functions f and g mapping from R+ to R, we write f = O(g) when there exist two constants C0 and
xo independent of f and g, such that the inequality f(x) ≤ C0g(x) holds for any x ≥ xo. We use
X ' Y to denote X . Y andY . X.
A.2 Appendix Organization and Proof S ketch
In this section, we list the organization of the appendix and put a more detailed proof sketch of our
main results. We put all the proof of upper bounds in Appendix B and all the proof of lower bounds
in Appendix D. All the proof of the results about modified Deep Ritz method is given in Appendix C.
The proof of lower bounds is based on standard Fano method. In this section, we focus on the proof
sketch of the fast rate upper bound.
Error Decomposition. We first decompose the excess risk ∆En := E(Un) — E(u*) of a well-
trained neural network on the PINN/DRM loss function into approximation error and generalization
error, similar to (Xu, 2020; Lu et al., 2021b; Duan et al., 2021; Jiao et al., 2021a;b). The regularity
results used in the decomposition are proved in Appendix B.1. Explicitly, for any UF ∈ F(Ω), we
can decompose the excess risk as
∆ES)(U)= E(U) — E(u?)
=[E(U) — En(U)] + [En(U) — En(UF)] + 回"-E(uf)] + [E(uf) — E(u?)]
≤ [E(U)- En(U)] + [En(UF)- E(UF)] + [E(UF)- E(U?)],
|
{^^^^^^^≡
Generalization Error
, X-------------V----------}
Approximation Error
(A.1)
15
Published as a conference paper at ICLR 2022
where the expectation is on uniformly sampled data, F(Ω) is the space of parametrized estimators
we used like truncated Fourier series or sparse neural networks, U is the minimizer of the empirical
loss En in F(Ω) and U is the minimizer of the population loss E (i.e, ground truth solution). The
inequality in the third line follows from the fact that U is the minimizer of the empirical loss En in
the space F(Ω), which implies En(U) ≤ En(UF). We call the first term generalization error as it's
measuring the difference between En and E. We call the second term approximation error as it seeks
for a parametrized estimator UF that approximates the ground truth solution U well in F(Ω). The
upper bounds on generalization and approximation error that we achieved in this paper are listed in
Table 2.
Let n denote the number of sampled datapoints. For the generalization error, different from the
concurrent upper bound O(√1n) (Duan et al., 2021; Jiao et al., 2021a), we provide a faster and near
optimal upper bound O(n) by utilizing the strong convexity of the objective function established in
Appendix B.1. Via using the Peeling Lemma (Lemma B.4, for completeness, we also provide a proof),
we show that the generalization error can be bounded by the fixed point of the local Rademacher
complexity
φ(r) = Rn({I(u) |kU -U*kA ≤r}),
where Rn is the Rademacher complexity, I(U) = ∆U + VU, k ∙ IIa = Il ∙ IIh2 for PINN and
Z(U) = ∣∣Vu∣2 + Vu, k ∙ ∣∣a = k ∙ ∣∣h 1 for DRM. Once we show that φ(r) is of magnitude O(Pr),
we can achieve the O( n) convergence rate via solving the fix point equation φ(r) = O( v^r) = r ⇒
r = O(n). Using the solution of the fixed point equation of the local Rademacher complexity to
bound the generalization error is a standard result in empirical process Bartlett et al. (2005); Srebro
et al. (2010); Koltchinskii (2011); Xu & Zeevi (2020); Farrell et al. (2021). The difference is that we
used the H 1 /H2 norm to define the localized set, while the previous papers used the `2 distance. The
way to obtain the fast rate generalization bound is using the Peeling Lemma.
We present the error decomposition results as a meta theorem, which is shown in Theorem B.12 for
PINN, Theorem B.9 for DRM and Theorem C.1 for MDRM, respectively. To make the final rate
depend on the data number only, we need bounds of the approximation error in Appendix B.3 and
bounds of the local Rademacher complexity in Appendix B.2.
Approximation Error. The proof of the approximation results of truncated Fourier series is easy
and intuitive. For completeness, we provide it in Appendix B.3.1. The proof of the approximation
results of neural networks follows from the fact that a B-spline approximation can be formulated as a
ReLU3 neural network efficiently. Our proof basically follows Duan et al. (2021); Jiao et al. (2021a),
while the only difference is the activation function. Our proof is also very similar to (Yarotsky, 2017;
Suzuki, 2018), but the depth of our network is of constant magnitude instead of O( ιog^) magnitude,
where denotes the desired approximation error. Such improvement of depth results from the fact
that ReLU3 activations can approximate B-splines more easily than the ReLU activations, which
is useful in our generalization analysis. Although the proof of the approximation results of neural
networks in the Sobolev space is standard, we still list it in Appendix B.3.2.
Generalization Error. As we discussed above, the generalization error can be bounded by the fix
point of the local Rademacher complexity, i.e. the solution of φ(r) = r. Once we have a O(Pr)
bound of φ(r), we can achieve the O( ɪ) fast rate generalization bound we want. It remains to upper
bound the the local Rademacher complexity φ(r).
For the upper bound on the local Rademacher complexity of truncated Fourier series estimators, our
proof technique is similar to that of the kernel estimators, whose Rademacher complexity can be
bounded by the trace of the Gram matrix (i.e. the effective number ofbasis). One interesting thing we
showed is that the final upper bound of the Rademacher complexity localized by H1 norm is Jξdn2r.
The term ξd-2 in the numerator is smaller than ξd, which is the exact number of Fourier basis. This
improvement results from the H1 norm localization. The detailed proof is given in Lemma B.6,
Lemma B.7 and Lemma B.8.
For the upper bound on the local Rademacher complexity bound for neural network, we follow
Schmidt-Hieber et al. (2020); Suzuki (2018); Farrell et al. (2021) to use a Dudley integral theorem and
a covering number argument. The covering number arguments are shown in Theorem B.4, Theorem
16
Published as a conference paper at ICLR 2022
B.5 and Theorem B.6. The final local Rademacher complexity bounds are given in Lemma B.17 and
Lemma B.18. The difference is that the complexity of gradient of ReLU3 activation function makes
the covering number depend exponentially on the neural network’s depth. However, the improvement
of neural network’s depth to constant magnitude mentioned above in the approximation results saves
this problem. One drawback of our proof is that the H1 norm localization wouldn’t improve the
bound for Rademacher complexity and leads to sub-optimal upper bounds. We hypothesize that
our bound is tight for sparse neural network and put seeking a right complexity measure of neural
network for solving PDEs as a future work.
Objective Function	Estimator	Approximation	Generalization	Complexity Measure
PINNs	Neural Network	-TT 2s—4 N--d-	~N n,		N: Number of parameters
	Fourier Seriers	ξ-2(S-2)	≤d n	g:maximum frequency
DRM	Neural Network	Tz 2s —2 N--d-	~N n		N: Number of parameters
	Fourier Seriers	ξ-2(S-I)	≤d n	g:maximum frequency
MDRM	Neural Network	-TT 2s —2 N--d-	~N —n,	,,		N: Number of parameters
	Fourier Seriers	ξ-2(S-I)	Ed — 2 n	g:maximum frequency
Table 2: Approximation and generalization results we achieved in this paper.
B	Proof of the Upper Bounds
B.1 Regularity Result For the PDE model.
Regularity Results of the DRM Objective Function
Theorem B.1. We consider the static Schrodinger equation on the unit hypercube on Rd with the
zero Direchlet boundary condition:
—∆u + Vu = f on Ω,
U = 0 on ∂Ω.
where f ∈ L2 (Ω) and V ∈ L∞(Ω) with 0 < Vmin ≤ V(x) ≤ VmaX
weak solution US to the equivalent variational problem (Evans, 1998):
(B.1)
> 0. There exists a unique
US = argmin EDRM(u) := argmin {4( [||Vu『+ V|u|2] dx - f fudx).
u∈H1(Ω)	u∈H1(Ω)12 Jω	Ω
Thenfor any U ∈ H 1(Ω), we have:
min(12Vmm) ∣U - US∣Hi(Ω) ≤ EDRM(U) - EDRM(US) ≤ max(12Vmax) ∣U - US∣Hi(Ω).
Proof. To show that US satisfies estimate B.3, We first claim that for any U ∈ H 1(Ω),
EDRM(U)- EDRM(US) = 1 / l∣Vu — VuS∣∣2dx + 1 / V(US — u)2 dx.
In fact, by plugging in the first equation of B.1, one has that
EDRM(US) = 1 / IVuS∣2dx +1 Z V|uS∣2dx - Z fUSdx
Ω √Ω	d JΩ	Ω
=—∣ ∣∣VuS∣∣2dx + — V V|uS∣2dx + ( (∆uS — VUS)uSdx
2 Jω	2 Jω	ω
=—∣ ∣∣VuS∣∣2dx + ( (∆uS)uSdx — — V V|uS∣2dx.
2 Jω	ω	2 7ω
(B.2)
(B.3)
(B.4)
17
Published as a conference paper at ICLR 2022
Furthermore, applying Green's formula to the true solution US yields:
EDRM(US) = 1 / kVuSk2dx + [ (∆uS)uSdx- 1 [ V|uS∣2dx
d JΩ	Ω	ω √ω
=Z SuSUSdχ - 2 / INuSk2dx - 9 Z V|uS|2dx
∂Ω n	乙 JΩ	乙 JΩ
=-2 Z INuSk2dχ - 2 Z V|uS|2dx，
where the last identity above follows from the second equality in B.1. Now for any U ∈ H 1(Ω),
applying Green,s formula to U and the true solution US implies:
EDRM(U)-	EDRM(US)	= 1 k	kVU∣2dx	+1 V V∣U∣2dx	— f fUdx +1 k	∣VuS∣∣2dx +1	V	V|uS∣2dx
2 Jω	2 Jω	ω	2 Jω	2	Jω
=—I	∣∣VU∣2dx	+ — V V∣U∣2dx +	( (ΔuS	— VUS)〃dx	+ - I ∣∣VuS∣∣2dx +	-	V	V|uS∣2dx
2 Jω	2 Jω	ω	2 Jω	2	Jω
=—I	∣∣VU∣2dx	+ ( (ΔuS)UdX +	— I	∣∣VuS∣∣2dx +	—	V	V(US	— U)2dx
2 Jω	ω	2 Jω	2	Jω
=—/ ∣∣VU∣2dx + / SUdUdx — / VUS ∙ VUdx + — / ∣∣VuS∣∣2dx + — V V(US — U)2dx
2 Jω	∂Ω dn	Ω	2 Jω	2 Jω
=1 / ∣∣Vu -VUSIl2dx + 1 V V(uS — u)2 dx,
2 Jω	2 Jω
where the last identity above again follows from the second equality in B.1. This completes our proof
of identity B.4. Using the assumptions on the potential function V then implies:
EDRM(U)- EDRM(US) ≤
EDRM(U)- EDRM(US) ≥
max(1, Vmax)
2
max(1, Vmax)
2
min(1,Vmin)
2
min(1,Vmin)
2
h/ ∣∣Vu -VUS∣∣2dx + /(US - u)2 dx]
IIu - uS I∣Hi(ω),
h / ∣∣Vu -VUS∣∣2dx + / (US- U)2 dxi
IIu - uS IlH 1(Ω).
This completes our proof of B.1.
□
Regularity Results of the PINN Objective Function
Theorem B.2. We consider the static Schrodinger equation on the unit hypercube on Rd with the
Neumann boundary condition:
-∆u + Vu = f on Ω,
u = 0 on ∂Ω.
(B.5)
where f ∈ L2(Ω) and V ∈ L∞(Ω) with V2 - ∆V > Cmin,0 < Cmin < V(x) ≤ VmaX and
-∆V(x) ≤ Vmax. Then there exists a unique solution US ∈ H1 (Ω) to thefollowing minimization
problem (Brezis, 2010):
US = argmin Epinn(u) := arg min { ∣ ∣∆u - Vu + f∣2dx).	(B.6)
u∈H1(Ω)	u∈H0 (Ω) I Jω	J
Thenfor any U ∈ H1(Ω), we have:
min{1,Cmin }∣U - US ∣∣H 2(Ω) ≤ E PPNN (U) - E PPNN (US) ≤ 2(1 + /ax + V2aχ)∣U -US 信⑺.
(B.7)
18
Published as a conference paper at ICLR 2022
Proof. For any U ∈ H1 (Ω), we let U = U - u*, then we have U ∈ H1 (Ω).
EPINN(U)- EPINN(US) = I ∣Δu - Vu - Δu* + VU*∣2dx = ∣ ∣∆U - VU∣2dx
/；	ω	(B.8)
=/ (∆U)2dx + / V2U2dx — 2 / VU∆Udx.
Jω	Jω	Jω
Using Green’s formula, we have:
/ VU∆Udx + V V(VU) ∙ VUdx = / ^^VUds = 0,
Jω	ω	∂ω dn
where the last equality above follows from the fact that U ∈ H1 (Ω). This further implies:
Epinn(u) - EPINN(US) = ( (∆U)2dx + V V2U2dx + 2 I V(VU) ∙VUdx
Ω	Ω	Ω
=/ (∆U)2dx + / V2U2dx + 2 / VkVUk2dx + 2 / UVV ∙ VUdx.
Ω Ω	Ω	Ω	Ω
Using Green’s formula again, we have:
2 U UVV ∙ VUdx = / V(u2) ∙ VVdx = / dVU2ds - / U2∆Vdx = - / U2∆Vdx.
Jω	Jω	∂Ω dn	Jω	Jω
Then we can further deduce that:
EPINN(U)- EPINN(US)= /(∆U)2dx + ( (V2 - ∆V)U2dx + 2 V V||VU『dx.
Ω	Ω	Ω
For We have assumed V ∈ L∞(Ω) with 0 < Cmin < V2 一 ∆V, 0 < Cmin < V(x) ≤ VmaX and
-∆V (x) ≤ Vmax , thus we have
min{1, CmmMU -US 备⑸)≤ EPINN(U)- E PINN(US) ≤ 2(1 + VmaX + VmaX)ku -US kH 2(Ω).
(B.9)
□
B.2 Auxiliary definitions and lemmata On Generalization Error
To bound the generalization error, we use the localized Rademacher complexity (Bartlett et al., 2005).
Recall that the Rademacher complexity of a function class G is defined by
Rn(G) = EZEσ
1 n
[sUG∣n X σ g(Zj )11
Z1,…
where Zi are i.i.d samples according to the data distributions and σj are i.i.d Rademacher random
variables which take the value 1 with probability 2 and value -1 with probability 2.
The following important symmetrization lemma makes the connection between the uniform law of
large numbers and the Rademacher complexity.
Lemma B.1 (Symmetrization Lemma). Let F be a set of functions. Then
∣1 n	∣
E sup |-XU(Xj) - EX〜Pωu(X)∣ ≤ 2Rn(F).
u∈F n j=1
Lemma B.2 (Ledoux-Talagrand contraction (Ledoux & Talagrand, 2013, Theorem 4.12)). Assume
that φ : R→R is L-Lipschitz with φ(0) = 0. Let {σi}in=1 be independent Rademacher random
variables. Then for any T ⊂ Rn
nn
Eσ sup	V"σiφ(ti) ≤ 2L ∙ Eσ sup	V" σiti .
(t1 ,∙∙∙ ,tn)∈T i=1	(t1,∙∙∙ ,tn)∈T i=1
19
Published as a conference paper at ICLR 2022
Let (E , ρ) be a metric space with metric ρ. A δ-cover of a set A ⊂ E with respect to ρ is a collection
of points {xι, ∙∙∙ , xn} ⊂ A such that for every X ∈ A, there exists i ∈ {1, ∙∙∙ ,n} such that
ρ(x, xi) ≤ δ. The δ-covering number N(δ, A, ρ) is the cardinality of the smallest δ-cover of the
set A with respect to the metric ρ. Equivalently, the δ-covering number N(δ, A, ρ) is the minimal
number of balls Bρ(x, δ) of radius δ needed to cover the set A.
Theorem B.3 (Dudley’s Integral theorem). Let F be a function class such that supf∈F kf kn,2 ≤ M.
Then the Rademacher complexity Rn(F) satisfies that
Rn(F) ≤
0≤infM n4δ+√n Zδ qiogN(e, F, k∙kn,2) de0.
Lemma B.3 (Talagrand Concentration Inequality). Consider a function class F defined on a prob-
ability measure μ such that for all f ∈ F, we have ∣∣f k∞ ≤ β, Eμ[f] = 0, Eμ[f2] ≤ σ2. Thenfor
any t > 0, we can have the following concentration results.
PzI ,…,Zn 〜μ
1n
sup - fZi)(zi) ≥ 2sup Ez0 ,∙
f∈F ni=1	f∈F 1
1n
,zn 〜μ n Ef (ZO) +
i=1
≤ e-t.
Lemma B.4 (Peeling lemma (Bartlett et al., 2005)). Consider some measurable function class F.
Assume that there exists a sub-root function φ(r) satisfying
Rn({f ∈ F | E[f] ≤ r}) ≤ φ(r) (∀r > 0).	(B.10)
Then we have
sup
f∈F
1 Pn=I σif (Zi)
E[f] + r
≤ 4φ(r)
一 r
Proof. Denote F(r) = {f ∈ F | E[f] ≤ r} to be the localized set with radius r. Then for a fixed set
of datapoints {Zi}in=1 and a fixed set of Rademacher random variables {σi}in=1, we have:
Eσ z
σi ,zn
-	1
n
sup n
f∈F
Pin=1 σif(Zi)
E[f] + r
Eσ z
σi ,zn
sup
f∈F(r)
1 Pn=I σif (Zi)
∞
+ X	Eσi,zn
Rn(F(r))
+ χ∞ Rn(F(r4j+1))
r4	r4j + r
j =0
j=0
φ(r)
≤ -----
r
∞
+X
j =0
sup
f ∈F(r4j + 1)∖F (r4j)
φ(r4j+1)
r4j + r
1 Pn=I σif (Zi)
r4j + r
≤
≤
r
r
≤
Φ(r)	XX 2j+1φ(r) < 4φ(r)
r	r4j + r r
r j=0 r r r
□
We also modify the peeling lemma above, as We aim to apply it to derive the upper bound for the
Modified Deep Ritz Method (MDRM).
Lemma B.5 (Peeling Lemma For MDRM). Given some measurable function class F and two
continuous mappings g, h : F → R, we define a class F of vector-valued functions by:
F ：= {(g ◦ f,h ◦ f) If ∈ F}.
For any f ∈ F, we use gf and hf to denote the two compositions g ◦ f and h ◦ f, respectively. For
any r > 0, the localized set Fr is defined by:
Fr = {(gf,hf) ∈ F | Ex [gf (X)] +Ey[hf(y)] ≤ r}.
Moreover, the modified Rademacher Complexity of Fr is defined by:
Rn,m(Fr) := Rn {gf |(gf,	hf) ∈ Fr}	+ Rm ({hf |(gf, hf)	∈	Fr} .
Assume that there exists some function φ	: [0, ∞) →	[0, ∞) and some r?	>	0, such that for	any
r > r?, we have:
φ(4r) ≤ 2φ(r) and Rn,m(Fr) ≤ φ(r).
Then for any r > r?, we have:
1
Eσ,τ Eχ,y [SUP ɪ
f∈F
Pn=I σigf (Xi)+m PmI Tjhf(y)
Ex [gf (X)] + Ey [hf (y)] + r
]i ≤ 4φ(r)
Proof. The proof is the same as the original peeling lemma, thus We omit the detailed proof here. □
20
Published as a conference paper at ICLR 2022
B.2. 1 Local Rademacher Complexity of Truncated Fourier Basis
Definition B.1. (Fourier Series) Given a domain Ω ⊆ [0,1]d. For any Z ∈ Nd, we consider the
corresponding Fourier basis function φz(x) := e2πihz,xi (X ∈ Ω). With respect to the Fourier basis,
anyfunction f ∈ L2(Ω) can be decomposed as the following sum:
f(x) :=	fzφz (x).
z∈Nd
(B.11)
wherefor any Z ∈ Nd, the Fourier coefficient fz = JQ f (x)φz (x)dx.
Definition B.2. (Truncated Fourier Series) For a fixed positive integer ξ ∈ Z+, we define the space
Fξ(Ω) oftruncated Fourier SerieS asfollows:
Fξ (Ω) := {f = X fz φz∖fz =0, ∀ kzk∞ > ξ}.	(B.12)
z∈Nd
Equivalently, we can decompose any f ∈ Fξ(Ω) as f := Ekzkco≤ξ fzφz.
Lemma B.6. (Local Rademacher Complexity of Localized Truncated Fourier Series) For a fixed
ξ ∈ Z+, we consider a localized class
OffimCtiOnSFρ,ξ(Q) = {f ∈ Fξ(Q) ∖ kf kHI。
≤ ρ , where
ρ > 0 is fixed. Then we have the following upper bound on the local Rademacher complexity:
1 n	/—
Rn(Fρ,ξ(Ω)) = EX Eσ[	SUp	— Xσif(Xi)∖Xι,…，x∕ . ∖ ρξd-2.
[Lf∈Fρ,ξ(Ω) n 匕	1	」「 Vn
(B.13)
Proof. Take an arbitrary function f ∈ Fρ,ξ(Ω). Let f = Ekzkco≤ξ fzφz be the Fourier basis
expansion of f. P ≥ kf kHi(。)implies constraint PkZkco≤ξ ∣fz∣2∣∣z∣∣2 . P on the Fourier Coeffi-
cients(Adams & Fournier, 2003).
On the other hand, substituting the Fourier expansion into the average sum ɪ Pn=I σif (Xi) and
using Cauchy-Schwarz inequality imply:
1n	1n	1	n
n Xσf(Xi) = n Xσi X fzφz(Xi) = n X Xσf φz(Xi
i=1	i=1	kzk∞≤ξ	kzk∞≤ξ i=1
1	n	1
≤n( X	∣fz∣2kzk2)2( X ∖X莆φz(Xi)D2
kz k∞ ≤ξ	kzk∞ ≤ξ i=1
.√ρ( X ∖X莆φz(Xi)∖2)1.
kzk∞ ≤ξ i=1
where we have used the constraint Pkzk ≤ξ |fz|2kZk2 . P in the last step above. Moreover, by
taking expectation with respect to the i.i.d Rademacher random variables σi (1 ≤ i ≤ n) and the
uniformly sampled data points {Xi }in=1 on both sides and applying Jensen’s inequality, we can
deduce that:
n
EXEσ [n X σif(Xi)] . ~nEX,σ
i=1
,n	,2	1
(X ∖X黄φz(Xi)D2
kzk∞ ≤ξ i=1
≤√p(EX,σ h X ∖X 莆 φz(Xi)[)
kzk∞ ≤ξ i=1
21
Published as a conference paper at ICLR 2022
Using independence between the random variables σi (1 ≤ i ≤ n), we can further simplify the
expectation inside the square root above as below:
EX,σh x IX黄φz(Xi)12]= x EX,σhiX3φz(Xi)∣2i
kzk∞≤ξ i=1	kzk∞≤ξ	i=1
n22
=x XEX,σ h而"(Xi)Ii
kzk∞≤ξ i=1
= X X iil-TΓ2 . n X	. nξ2 = nξd-2.
kzk2 *	kzk2	ξ 2
kzk∞≤ξ i=1	kzk∞≤ξ
Combining the two bounds above yields the desired upper bound:
EX Eσ	sup
f∈ ∈Fρ,ξ(Ω)
n
-X σif (Xi) i X1, ^^^ ,
n
i=1
□
Lemma B.7. (Local Rademacher Complexity of Localized Truncated Fourier Series’ Gradient) For
a fixed ξ ∈ Z+, we consider a localized class offunCtions Gρ,ξ (Ω) = {]▽/1|| f ∈ Fρ,ξ (Ω)}, where
ρ > 0 is fixed. Thenfor any sample {Xi}n=ι ⊂ Ω, we have thefollowing upper bound on the local
Rademacher complexity:
Rn(Gρ,ξ (Ω))= EX
n
Eσ [	SUP	- X σi∣Nf (Xi)Ili X1,…，Xn]
.f∈∈Fρ,ξ(Ω)n i=1	1	」
(B.14)
Proof. Take an arbitrary function f ∈ Fρ,ξ(Ω). Let f = Ekzkco≤ξ fzφz be the Fourier basis
expansion of f. Similarly, the norm restriction condition IlfIlH、©)≤ P Can be reduced to the
following condition about Fourier coefficients:
X IfzI2Izk2 . P.
kzk∞≤ξ
Moreover, substituting the Fourier expansion into the average sum n Pn=ι σi∣Vf (Xi)Il and using
Cauchy-Schwarz inequality imply:
nn	n
n XσikVf(Xi)k = n Xσik X fzVφz(Xi)k≤ - X Xσi∣∣fzVφz(Xi)∣∣
i=1
i=1 kzk∞≤ξ
kzk∞≤ξ i=1
[	1	In	19 1
≤n( X 8加2)2( X |X莆kvφz(Xi)k|)2
kzk∞≤ξ	kzk∞≤ξ i=1
.√ρ( X ∣Xi⅜kvφz(Xi)"f)2.
kzk∞≤ξ i=1
where we have used the constraint Pkzk ≤ξ IfzI2IzI2 . P in the last step above. Moreover, by
taking expectation with respect to the i.i.d Rademacher random variables σi (- ≤ i ≤ n) and the
uniformly sampled data points {Xi}in=1 on both sides and applying Jensen’s inequality, we can
deduce that:
1 n	1—
EXEσ [- Xσi"Vf(Xi)∣ .斗Eχ,σ
i=1
n	2 1
(kz2|X Mvφz(Xi)k| Y
≤√nρ(EX,σ h kzBξ⅛ MVφz(Xi )k1〕
22
Published as a conference paper at ICLR 2022
Using independence between the random variables σi (1 ≤ i ≤ n), we can further simplify the
expectation inside the square root above as below:
EX,σ h X IX k⅛vΦz(Xi)k∣2i= X F [it k⅛kvΦz(Xi)*
kzk∞≤ξ i=1	kzk∞≤ξ	i=1
n2
=X X EX,σ [k⅛ kvΦz (Xi)k2]
kzk∞≤ξ i=1
=X X∣Ω∣W . n X 1 . nξd.
kzk∞≤ξ i=1	kzk∞≤ξ
Combining the two bounds above yields the desired upper bound:
EX Eσ	sup
Lf ∈Fρ,ξ(Ω)
n
-^X σikvf(Xi)Il | X1, …，Xn]
n i=1
□
Lemma B.8. (Local Rademacher Complexity of Localized Truncated Fourier Series’ Laplacian) For
afixedξ ∈ Z+, we consider a localized class of functions Jρ,
ξ(Ω) := {f ∈ Fξ(Ω)∣∣∣f∣∣H2
(Ω)
≤ρ,
where ρ > 0 is fixed. Correspondingly, we define a localized class of Laplacians Kρ,ξ(Ω):=
{∆f | f ∈ Jρ,ξ (Ω)}. Thenfor any sample {Xi}n=ι ⊂ Ω, we have the following upper bound on the
local Rademacher complexity:
n n	I—
Rn(Kρ,ξ (Ω)) = EX Eσ[ SUp - X σi∆f(Xi) ∣Xι,…，Xn] .∖ρξd.
f_	Lf∈Fρ,ξ(Ω) n i=1	1	」」Vn
(B.15)
Proof. Take an arbitrary function f ∈ Jρ,ξ(Ω). Let f = Ekzkco≤ξ fzφz be the Fourier basis
expansion of f. Similarly, the norm restriction condition ∣∣f ∣H2(q)≤ P can be reduced to the
following condition about Fourier coefficients:
X IfzI2 *Izk4 . ρ.
kzk∞≤ξ
Moreover, substituting the Fourier expansion into the average sum n Pin=ι σi∆f (Xi) and using
Cauchy-Schwarz inequality imply:
1n
1X
n i=1
σi∆f(Xi)
nn
-Xσi X	fz∆φz(Xi) = - X	Xσifz∆φz(Xi)
n i=1 kzk∞≤ξ	n kzk∞≤ξ i=1
[	1	, n	11	1
≤ n ( X lfzl2 3 4) 2 ( X lX ⅛ δφz (Xi)| ) 2
kzk∞ ≤ξ	kz k∞ ≤ξ i=1
.√nρ( X ∣X i⅛ “mi2)1.
kzk∞ ≤ξ i=1
where we have used the constraint Pkzk ≤ξ IfzI2∣z∣4 . ρ in the last step above. Moreover, by
taking expectation with respect to the i.i.d Rademacher random variables σi (1 ≤ i ≤ n) and the
uniformly sampled data points {Xi}in=1 on both sides and applying Jensen’s inequality, we can
deduce that:
1 n	1—
EXEσ [n X σi∆f (Xi)] . ~nEX,σ
i=1
/	l n	2 ι
(>£ 言”"Y
≤√ρ (EX,σh X ∣X∣⅛δφz(Xi)∣ i
kzk∞ ≤ξ i=1
1
2
23
Published as a conference paper at ICLR 2022
Using independence between the random variables σi (1 ≤ i ≤ n), we can further simplify the
expectation inside the square root above as below:
EX,σ h X IX 前 δφZ(Xi)Ii= X EX,σ h| X 言 δφZ(Xi)Ii
kzk∞≤ξ i=1	kzk∞≤ξ	i=1
n2
=X X EX,σ h kφ∙ 1δφz (Xi)l2i
kZk∞≤ξ i=1
=X X>∣16⅛r . n X 1 . nξd.
kZk∞≤ξ i=1	kZk∞≤ξ
Combining the two bounds above yields the desired upper bound:
EX Eσ h	sup	- X OiN(Xi) | X1, …,Xn] .	Pnξd = ʌBξ 2 ∙
[Lf∈Fρ,ξ(Ω)n 匕	1	jJ n	Vn
□
B.2.2 Local Rademacher Complexity of the Deep Neural Network Model
In this section we aim to bound the local Rademacher Complexity of a Deep Neural Network. We
first bound the covering number of the function space composed by the gradient of all possible neural
networks and then apply a Duley Integral to achieve the final bound.
Definition B.3. Let ηl denote the l-ReLU activiation function. Here we use η3 := max{0, x}3(E
& Yu, 2018) as the activation function to ensure smoothness. We can define the space consisting of
all neural network models with depth L, width W, sparsity constraint S and norm constraint B as
follows:
Φ(L,W,S,B) := n(W(L)η3(∙) + b(L))…(W(I)X + b⑴)| W(L) ∈ R1×W,b(L) ∈ R, (B.16)
W(1) ∈ RW×d, b(1) ∈ RW,W(l) ∈ RW×W,b(l) ∈ RW(1 < l < L),	(B.17)
L
X(kW(l)k0 + kb(l)k0) ≤ S,maxkW(l)k∞,∞∨kb(l)k∞ ≤ B .	(B.18)
l=1
where ∣∣∙∣∣o measures the number ofnonzero entries in a matrix and ∣∣ ∙ ∣∣∞,∞ measures the maximum
of the absolute values of the entries in a matrix.
For any d ∈ Z+, we refer to an arbitrary element in Φ(L, W, S, B) as a ReLU3 Deep Neural Network.
Then for any index 1 ≤ k ≤ L, we use Fk to denote the k-ReLU3 Deep Neural Network composed
by the first k layers, i.e:
Fk(x) := (WFk)η3(∙) + b(k)) ∙∙∙ (WFI)X + b(1)).
Also, we use Φk (L, W, S, B) to denote the space consisting of all Fk. In particular, when k = L, we
have:
F (x) := Fl(x) = (WFL)η3(∙) + b(L)) ∙∙∙ (WFI)X + b(1)), and Φl(L, W, S, B) = Φ(L, W,S, B).
Furthermore, given that the domain Ω ⊂ [0,1]d is bounded, we have supχ∈Ω ∣∣x∣∞ = L
Lemma B.9. (Upper bound on ∞-norm of functions in DNN space) For any 1 ≤ k ≤ L, the
following inequality holds:
3 k — 1 - 1	5 3 k — 1 - 1 3 k - 1
sup	∣Fk(x)∣∞ ≤ Wɜɪ(B ∨ d)	2 - 2E-k+1.
x∈Ω, Fk∈Φk(L,W,S,B)
Proof. We use induction to prove this claim.
Base cases: When k = 1, we have that for any X ∈ Ω and any Fi ∈ Φ1(L, W, S, B), the following
holds:
∣F1(X)∣∞ = ∣WF(1)X+b(F1)∣∞ ≤ ∣WF(1)∣∞∣X∣∞+∣b(F1)∣∞
≤ d∣WF(1)∣∞,∞ + B ≤ dB+B ≤ 2(B ∨ d)2.
(B.19)
24
Published as a conference paper at ICLR 2022
When k = 2,we have that for any X ∈ Ω and any F2 ∈ Φ2 (L, W, S, B), the following holds:
kF2(x)k∞ = kWF(2)η3(F1(x))+b(F2)k∞ ≤ kWF(2)k∞kη3(F1(x))k∞+kb(F2)k∞ ≤WkWF(2)k∞,∞kF1(x)k3∞+B.
By applying the bound proved in the case when k = 1, we have:
kF2(x)k∞ ≤ WB(dB + B)3 +B = WB4(d+1)3+B
= WB4(d3+3d2 +3d+1) +B ≤ 8W(B ∨ d)7.
where the last inequality follows from the assumption that W ≥ 2.
Inductive Step: Now we assume that the claim has been proved for k- 1, where 3 ≤ k ≤ L. Similarly,
for any X ∈ Ω and any Fk ∈ Φk (L, W, S, B), We have:
kFk(x)k∞ = kWF(k)η3(Fk-1(x)) + b(Fk)k∞ ≤ kWF(k)k∞kη3(Fk-1(x))k∞ + kb(Fk)k∞
≤ W kWF(k)k∞,∞kFk-1(X)k3∞ + B ≤ W BkFk-1(X)k3∞ + B.
Using inductive hypothesis, we can further deduce that:
3k-1-3	5∙3k-1-3 3k-3
∣∣Fk(x)k∞ ≤ WB × W(B ∨ d) —2— 2--3k+6 + B
3k-1-1 ,	、5∙3k-1-1 3k-3 o； , C
≤ W-2(B ∨ d)-2- 2--3k+6 + B ∨ d
3k-1-1 ,	、5∙3k-1-1 r 3k-3 Q7 1 c
≤ W-2(B ∨ d)-2- [2--3k+6 + 1]
3k-1-1 ,	、5∙3k-1-1 3k-3 7 , o ,	、
≤ W-2(B ∨ d)-2- 2F--k+2 (k ≥ 3)
3k-1-1	5∙3k-1-1 3k-1
= W-2- (B ∨ d)-2-2-ɪ-k+1.
Taking supremum with respect to X ∈ Ω and Fk ∈ Φk (L, W, S, B) on the LHS implies that the given
upper bound also holds for k. By induction, the claim is proved.	□
We also need to show that the ReLU3 activation function is a Lipschitzness functions over a bounded
domain.
Lemma B.10. For any k ∈ Z+, consider the k-ReLU activation function ηk defined on some
bounded domain D ⊂ Rd (i.e, supx∈D ∣X∣∞ ≤ C for some C > 0). Then we have that for any
X, y ∈ D, the following inequalities hold:
∣η1(X) - η1(y)∣∞ ≤ ∣X - y∣∞,
∣η2(X) - η2(y)∣∞ ≤ 2C∣X - y∣∞,
∣η3(X) - η3(y)∣∞ ≤ 3C2∣X - y∣∞.
Proof. This is because ∣Vm(x)∣ = |max{1,0}| = 1, ∣Vη2(x)∣ = ∣2max{x,0}| ≤ 2C and
∣Vη3(x)∣ = ∣3max{x, θ}21 ≤ 3C2.	□
Lemma B.11. (Relation between the covering number of DNN space and parameter space) For any
1 ≤ k ≤ L, suppose that a pair of different two networks Fk, Gk ∈ Φk (L, W, S, B) are given by:
Fk(x) ：= (WFk)η3(∙) + b& …(WFI)X + b(1)),
Gk(x) := (W(k)η3(∙) + 咫)…(WGI)X + b(1)).
Furthermore, assume that the ∣ ∣∞ norm of the distance between the parameter spaces of Fk and
Gk is uniformly upper bounded by δ, i.e
∣WF(l)	-WG(l)∣∞,∞	≤δ, ∣b(Fl)	- b(Gl)∣∞ ≤ δ, (∀1 ≤ l ≤	k).	(B.20)
Then we have:
..	..	....	3k-1-1 .	. 5∙3k-1-1 3k-1 1 l -l 1 y
SUp ∣Fk(X)- Gk(x)∣∞ ≤ δW(B ∨ d) -2- 2F-k+13k-1.	(B.21)
x∈Ω
25
Published as a conference paper at ICLR 2022
Proof. Let’s prove the claim by using induction on k.
Base Case: When k = 1,we have that for any X ∈ Ω and any F1,G1 ∈ Φι(L, W, S, B) satisfying
constraint B.20, the following holds:
kF1(x)-G1(x)k∞ = kWF(1)x + b(F1) - WG(1)x - b(G1)k∞
≤ kWF(1) - WG(1)k∞kxk∞ + kb(F1) -b(G1)k∞	(B.22)
≤ δd+δ= δ(d+ 1) ≤ 2δ(B ∨ d) ≤ 2δ(B ∨ d)2.
When k = 2, we have that for any X ∈ Ω and any F2,G2 ∈ Φ2(L, W, S, B) satisfying constraint
B.20, the following inequality holds:
kF2(X)-G2(X)k∞ = kWF(2)η3(F1(X)) + b(F2) -WG(2)η3(G1(X))-b(G2)k∞
≤ kWF(2)η3(F1(X))-WG(2)η3(G1(X))k∞+kb(F2) -b(G2)k∞
≤ kWF(2)η3(F1(X)) -WG(2)η3(F1(X))k∞+ kWG(2)η3(F1(X)) - WG(2)η3(G1(X))k∞ + δ.
By applying the upper bound proved in equation B.19, we can upper bound the first part
kWF(2)η3(F1(X)) -WG(2)η3(F1(X))k∞by:
kWF(2)η3(F1(X))-WG(2)η3(F1(X))k∞ ≤ kWF(2)-WG(2)k∞kη3(F1(X))k∞
≤WδkF1(X)k3∞ ≤ δW[2(B ∨ d)2]3.
By applying the Lipschitz condition proved in Lemma B.10 and the bound proved in equation B.22,
we can further upper bound the second part kWG(2)η3(F1(X)) - WG(2)η3(G1(X))k∞ by:
kWG(2)η3(F1(X))-WG(2)η3(G1(X))k∞ ≤ kWG(2)k∞kη3(F1(X))-η3(G1(X))k∞
≤ WB × 3 sup	kF1(X)k2∞ × kF1(X) -G1(X)k∞
F1 ∈Φ1 (L,W,S,B)
≤ WB × 3[2(B ∨ d)2]2 × 2δ(B ∨ d)
≤ 24δW(B ∨ d)6.
Summing the two upper bounds above yields:
kF2(X) -G2(X)k∞ ≤ 8δW(B ∨ d)6 +24δW(B∨d)6 +δ ≤ 24δW(B∨d)7.
where we again use the assumption d ≥ 2 in the last step.
Inductive Step: Now we assume that the claim has been proved for k - 1, where k ≥ 3. For any
x ∈ Ω and Fk ∈ Φk(L, W, S, B), We have that:
kFk(X)-Gk(X)k∞ = kWF(k)η3(Fk-1(X)) + b(Fk) - WG(k)η3(Gk-1(X)) - b(Gk)k∞
≤ kWF(k)η3(Fk-1(X)) - WG(k)η3(Gk-1(X))k∞ + kb(Fk) - b(Gk) k∞
≤ kWF(k)η3(Fk-1(X)) - WG(k)η3(Gk-1(X))k∞ + δ.
Applying triangle inequality helps us upper bound the first term above as follows:
kWF(k)η3(Fk-1(X)) - WG(k)η3(Gk-1(X))k∞
≤ kWF(k)η3(Fk-1(X)) - WG(k)η3(Fk-1(X))k∞ + kWG(k)η3(Fk-1(X)) - WG(k)η3(Gk-1(X))k∞
≤ kWF(k) - WG(k)k∞kη3(Fk-1(X))k∞ + kWG(k)k∞kη3(Fk-1(X)) - η3(Gk-1(X))k∞
≤ δW kFk-1(X)k3∞ + BW kη3(Fk-1(X)) - η3(Gk-1(X))k∞.
From Lemma B.9, we can upper bound the first term δW kFk-1(X)k3∞ by:
3k-1-1 ,	、5∙3k-1 -3 3k-3 Q} C
δWIlFk-ι(x)k∞ ≤ δW(B ∨ d) -2- 2 J--3k+6.
26
Published as a conference paper at ICLR 2022
Moreover, applying Lemma B.10 and the inductive hypothesis let us upper bound the second term
BW kη3(Fk-1(x)) - η3(Gk-1(x))k∞ as follows:
BW kη3(Fk-1(x)) - η3(Gk-1(x))k∞
≤ BW × 3	sup	kFk-1(x)k2∞ × kFk-1(x) - Gk-1(x)k∞
x∈Ω, Fk-1∈Φk-1(L^W,S,B)
≤3BW × W 3k-2-1(B ∨ d)5×3k-2-123k-1-1-2k+4kFk-1(x) - Gk-1(x)k∞
≤ 3BW X W3k-2-1(B ∨ d)5×3k-2-123k-1-1-2k+4 X δW3k-2-i (B ∨ d) 5'3k-2-1 23k-1-i-k+23k-2
≤ 3k-1δW3k÷i (B ∨ d)5×3k42⅛-3k+5.
Combining the two upper bounds derived above yields:
3k-1-1	5∙3k-1 -3 3k-3
kFk(X)- Gk(x)k∞ ≤ δW(B ∨ d)2F-3k+6
3k-1 -1	5×3k-1 -1 3k -1
+ 3k-1δW-2- (B ∨ d)	2	2--3k+5 + δ
3k-1 -1	5×3k-1 -1 3k -1
≤ δ3k-1W-2-(B ∨ d)	2	2--k+1,
where the last inequality above follows from k ≥ 3. Taking supremum with respect to X ∈ Ω on the
LHS implies the given upper bound also holds for k. By induction, the claim is proved.	□
Theorem B.4. (Bounding the DNN space covering number) Fix some sufficiently large N ∈ Z+.
Consider a Deep Neural Network space Φ(L, W, S, B) with L = O(1), W = O(N), S = O(N)
and B = O(N). Then the log value of the covering number of this DNN space with respect to the
inf-norm ∣∣F(x)k∞ := suPx∈ω |F(x)l, which is denoted by N(δ, Φ(L,W,S,B), k ∙ ∣∣∞), can be
upper bounded by:
log N (δ, Φ(L, W, S, B), k∙k∞) = O (S h log(δ-1) + 3l log(WB)]) .	(B.23)
Proof. We firstly fix a sparsity pattern (i.e, the locations of the non-zero entries are fixed). By picking
k = L in Lemma B.11, we get the following upper bound on the covering number with respect to
k∙k∞:
(δ)-S
3L-1-1	5×3l-1-1 ^3l-1
v3l-1W -2— (B ∨ d)------2---21-—L+2j
Furthermore, note that the number of feasible configurations is upper bounded by ((W+1产)≤
(W + 1)LS.(Schmidt-Hieber et al., 2020; Farrell et al., 2021) Plug in the previous inequality and
yields:
log N (δ, Φ(L,W,S,B), k∙k∞) ≤ log
(W + 1)LS
δ	\-S
Cr iτ"3L-1-1∕c τλ 5×3l-1-1 ^3l-1
3l-1W —2 — (B ∨ d)	2	2 J	L+"
≤ Slog δ-1(W + 1)L3L-1W
3L-1-1 一	、5×3L-1-1	3L-1
-2- (B ∨ d)	2	2 J--L+1
.S log(δ-1)+Llog(3W)+3Llog(W(B∨d))+3Llog2 .
Note that here the dimension d is some constant. Thus, by plugging in thee given magnitudes
L = O(1), W = O(N), S = O(N) and B = O(N), we can further deduce that:
log N (δ, Φ(L, W,S,B), k∙k∞) . Sh log(δ-1) + 3l log(WB)].
This finishes our proof.	□
Now let’s consider upper bounding the covering number of the l2 norm of the sparse Deep Neural
Networks’ gradients. Note that for any 1 ≤ k ≤ L - 1, any k-ReLU3 Deep Neural Network
Fk ∈ Φk(L, W, S, B) is a map from Rd to RW. For any 1 ≤ l ≤ W, we use Fk,l(X) to denote the
27
Published as a conference paper at ICLR 2022
l-th component of the map Fk. This helps us write the map Fk (x) and its Jacobian matrix J [Fk](x)
explicitly as:
Fk(χ) = [Fk,ι(χ),Fk,2(χ),…，Fk,w(x)]T ∈ RW
	∂X1 FkJ(X)	∂X2 FkJ(X)	•	•	∂XdFk,l(X)-	
J [Fk](X) =	& Fk,2 (X) • ∙ ∙	∂X2 Fk,2(X)	• . . •••	•	∂XdFk,2 (X)	∈ Rw×d
	-悬FkW (X)	∂X2Fk,W (x)	•	. •	∂Xd Fk,W (X)_	
In particular, when k = L, we have that any FL ∈ ΦL (L, W, S, B) = Φ(L, W, S, B) is a map from
Rd to R. Thus, its Jacobian can be explicitly written as the following row vector:
∂∂	∂
J[Fl](x) = [丁Fl(x), 丁Fl(x),…丁Fl(x)] ∈ R1×d.
∂x1	∂x2	∂xd
Lemma B.12. (Upper bound on ∞-norm of Jacobian/Gradient of elements in the DNN space) For
any 1 ≤ k ≤ L, the following inequality holds:
k-1	k-1	k
SUp	∣∣J[Fk](x)k∞ ≤ W(B ∨ d) —2 — 2 J--k+13k-1.
x∈Ω,Fk∈Φk(L,W,S,B)
Proof. We use induction on k to prove the claim.
Base case: k =LBythe definition of Jacobian matrix, We have that for any X ∈ Ω and any
F1 ∈ Φ1 (L, W, S, B), the following holds:
∣J[F1](x)∣∞ = ∣WF(1)∣∞ ≤dB ≤2(B∨d)2.
Inductive Step: Assume that the claim has been proved for k - 1, Where 2 ≤ k ≤ L. For any
x ∈ Ω and any Fk ∈ Φk (L, W, S, B), by applying the Chain Rule, We can write the Jacobian matrix
J [Fk](x) as J [Fk](x) = WF(k)J[η3 ◦ Fk-1](x), Where the ReLU3 activation function η3 is applied
to each component Fk-1,l (1 ≤ l ≤ W) of the map Fk. Then We have the folloWing upper bound:
kJ[Fk](x)k∞ ≤ kwFk)k∞∣J[η3 ◦ Fk-1 ](x)k∞ ≤ WBkJ[η3 ◦ Fk-ι](x)k∞.	(B.24)
Note that the composition η3 ◦ Fk-1 is a map from Rd to RW. Hence, the Jacobian matrix J[η3 ◦
W×d
Fk-1 ](x) is of shape R . Applying the Chain Rule again implies:
∣J[η3 ◦ Fk-1 ](χ)k∞ = SUp (XX ∣3η2(Fk-1,1(χ))dFk-1,l(X) |).
1≤l≤W	∂xj
≤ ≤ j=1	j
Furthermore, for any 1 ≤ l ≤ W , the summation on the RHS above can be upper bounded by:
XX ∣3η2(Fk-1,1 (x))dFkdX,l(X) ∣≤ 3kFk-ι(x)k∞(XX | UFk-1,1(x)l) ≤ 3∣Fk-ι(x)k∞ k J[Fι](x)k∞.
NoW let’s take supremum With respect to l and apply the inductive hypothesis and Lemma B.9. This
yields:
k J[η3 ◦ Fk-ι](x)k∞ ≤ 3W3k-2T(B ∨ d)5∙3k-2-123k-l-1-2k+4 × W3k4-i(B ∨ d)5⅛2-i23k4-i-k+23k-2
=W3k⅛-iT(B ∨ d)5∙3k---123k--3k+53k-1.
(B.25)
By substituting equation B.25 into equation B.24, We can derive the final bound:
3 k 1 1	53k 1 1 3 k 1
kJ [Fk ](x)k∞ ≤ WBk J[η3 ◦ Fk-ι](x)k∞ ≤ W	(B ∨ d)	2 F-3k+53k-1
3k-1-1 ,	、5∙3k-1-1 3k-1
≤ W 一2 一 (B ∨ d) —2 — 2F -—k+13kτ.
where the last inequality above follows from k ≥ 2. Taking supremum with respect to X ∈ Ω and
Fk ∈ Φk(L, W, S, B) on the LHS implies that the given upper bound also holds for k. By induction,
the claim is proved.	□
28
Published as a conference paper at ICLR 2022
For the convenience of the following proof, we first prove this lemma for vector 2 norm and ∞ norm.
Lemma B.13. Given any two row vectors u, v ∈ R1×d, we have:
kuk - kvk ≤ ku - vk∞.
Proof. Assume that the two vectors u, v ∈ Rd can be explicitly written as u = [u1, u2,
V = [v1,v2, ∙∙∙ ,vd], respectively. By applying CaUchy-SchWarz inequality, We have:
, ud] and
2
kuk - kvk	=
dd
Xui2+Xvi2
i=1	i=1
dd	d	d
≤ Xui2+Xvi2-2Xuivi=X|ui-vi|2
i=1	i=1	i=1	i=1
d
≤ X |ui - vi |
i=1
2
ku - vk2∞.
Taking the square root on both sides yields the desired inequality.
□
Then We upper bound the Lipschitz constant of the gradient of the neural netWork. Given a DNN
space Φ(L, W, S, B), we define a corresponding DNN Gradient space VΦ(L, W, S, B) as:
VΦ(L, W, S, B):= {kVF k | F ∈ Φ(L, W, S, B)}.	(B.26)
Lemma B.14. (Relation between the covering number of the DNN Gradient space and parameter
space) For any 1 ≤ k ≤ L, suppose that a pair of different two networks Fk, Gk ∈ Φk (L, W, S, B)
are given by:
Fk(x) ：= (WFk)η3(∙) + b(k)) ∙∙∙ (WFI)X + b(1)),
Gk(x) := (W(k)η3(∙) + b约…(WGI)X + b(1)).
Furthermore, assume that the k k∞ norm of the distance between the parameter spaces is uniformly
upper bounded by δ, i.e
kWF(l) -WG(l)k∞,∞ ≤δ, kb(Fl) - b(Gl)k∞ ≤ δ, (∀1 ≤ l ≤ k).	(B.27)
Then we have:
..	一 ..	. - ....	3k-1-1 .	. 5∙3k-1-1 3k -1
SUp kJ [Fk ](x) - J [Gk ](x)k∞ ≤ δW	(B ∨ d) -2- 2--k+132k-2.	(B.28)
x∈Ω
In particular, when k = L, we have:
SUplkVFL(X)k - kVGz(x)k∣ ≤ δW3L-21-i (B ∨ d) 5'"-1-123L--l+132l-2.	(B.29)
x∈Ω
Proof. We use induction on k to prove the claim.
Base case: When k = 1, we have that for any X ∈ Ω and any F1,G1 ∈ Φι(L,W,S,B),the following
holds:
kJ[F1](X) -J[G1](X)k∞ = kWF(1) -WG(1)k∞ ≤δd≤2δ(B∨d)2.
Inductive Step: assume that the claim has been proved for k - 1, where 2 ≤ k ≤ L. Then for
any X ∈ Ω and Fk, Gk ∈ Φk(L, W, S, B) satisfying constraint B.27, applying the Chain Rule and
29
Published as a conference paper at ICLR 2022
triangle inequality help us upper bound the inf-norm ∣∣ J[Fk](x) - J∖Gk](χ)∣∣∞ by:
k J[Fk](x) - J[Gk](x)∣∞ = IlWFfc) J[η3 ◦ Fk-ι](x) - w(k)J[η3 ◦ Gk-ι](x)∣∞
≤ ∣WFfc)J[η3 ◦ Fk-ι](x) - WGk) J[η3 O Fk-ι](x)∣∞ + ∣∣w”) J[η3 ◦ Fk-ι](x) - WGk) J[η3。Gι](x)∣∞
≤ ∣WFfc)- W*∣∞k J[η3 O Fk-ι](x)k∞ + ∣∣W*∣∞k J[η3 O Fk-1](X)- J[η3 O Gk-1 ](χ)k∞
≤ δWk J[η3 o Fk-ι](x)∣∞ + BWk J[η3 o Fk-ι](x) - J[η3 o Gk-ι](x)∣∞.
(B.30)
Using equation B.25 helps us upper bound the first term by:
..	. ....	3k-1-1 .	. 5∙3k-1-1 γ 3k-1
δWk J[η3 o Fk-ι](x)∣∣∞ ≤ δW(B V d) -2----------12F-3k+53k-1.	(B.31)
Note that the two compositions η3 o Fk-ι and η3 o Gk-ι both map from Rd to RW. Hence, the two
Jacobian matrices J[η3 o Fk-ι](χ) and J[η3 o Gk-ι](χ) are of shape RW×d. Applying the Chain
Rule again implies:
∣∣J[η3 o Fk-i](x) - J[η3 o Gk-i](x)∣∣∞ = sup (X∣3η2(Fk-1,ι(χ))aFk-i,Kχ) -3η2(Gk-1,ι(χ))dGk-1,l(X)|).
1≤l≤W j=ι	dxj	dxj
For any 1 ≤ l ≤ W, the summation on the RHS above can be upper bounded by:
「12 E / OdFk-1,l(x)	dGk-1,l (X)I
2^^2(^-1,1 (X)) —∂X-----------沏2(Gk-U(X)) —∂X---------1
j=1	j	j
≤ X ∣3η2(Fk-i,ι(X))dFkdr(X) - 3η2(Gk-i,1 (x))dFkdyX) |
I	d X j	d Xj
+ X ∣3η2(Gk-i,ι(X))dFk-IX) - 3η2(Gk-i,ι(X))dGk-1；(X) |
≤ X ∣3η2(Fk-1,ι (X))- 3η2(Gk-1,ι (X))Il dFkdr(X) ∣ + X ∣3η2(Gk-1,ι (X))Il dFkdr(X) - dGk-U(X) |.
j=1	j	j=1	j	j
We denote the two summations above by T1 and T2, respectively:
T1 := X ∣3η2(Fk-1,ι(X))- 3η2(Gk-1,1 (X))IldFkdr(X) ∣,
j=1	j
T2 ：= X ∣3η2(Gk-1,1(X))∣∣dFkL(X) -dGLl(X) ∣.
dXj	dXj
j=1	J	J
For the first sum T∖, applying Lemma B.9, Lemma B.10, Lemma B.11 and Lemma B.12 yields the
following upper bound:
T1 ≤ 6(	sup	∣∣Fk-1(X)k∞)∣∣Fk-1(X)-	Gk-I(X)∣∞ X ∣dFk-1,l(X)	∣
'x∈Ω, Fk-1∈Φk-1(L,W,S,B)	/	j=1 dXj
≤ 6(	sup	IlFk-I(X)∣∣∞)	IlFk-I(X)-	Gk-I(X)Il∞∣∣J[Fk-1](X)∣∣∞
'x∈Ω, Fk-1∈Φk-1(L,W,S,B)	)
3k-2-1 ,	、5∙3k-2-1 3k-1-1	3k-2-1 ,	、5∙3k-2-1 3k-1-1 7 ∣ . 7 °
≤ 3 X W(B V d) —2— 2 --------------------k+2 X δW(B V d) —2— 2「--------------------k+23k-2
0 k — 2	k — 2	k — 1	k — Io	Erok — 1 o ok.
X W-「(B V d) . 2 - 2-「-k+23k-2 = δW-「(B V d) . 2 - 2-ɪ-3k+632k-3
For the second sum T2, applying Lemma B.9 and inductive hypothesis yields:
T2 ≤ 3(	sup	IlGk-I(X)Il∞)2∣∣J[Fk-1](X) - J[Gk-1](X)∣∣∞
'x∈Ω, Gk-1∈Φk-1(L,W,S,B)	)
≤ 3 x W3k-2-1 (B V d)5∙3k-2-123k-1-1-2k+4 x δW"ɪɪ (B V d)5⅛^2⅛1 -k+232k-4
=δW3k-^ (B V d)5⅛-32⅛3-3k+632k-3.
30
Published as a conference paper at ICLR 2022
Combining the two upper bounds on T1 and T2 yields:
∂Fk-1,l (x)	∂Gk-1,l (x)
工 l3η2(Fk-ι,ι(x)) —∂X-------3η2(Gk-ι,ι(X)) —∂X----1
j=1	xj	xj
3k-1-3	、5∙3k-1 -3 3k-3 Q/ β Q/ Q
≤ T1 + T2 ≤ 2 X δW(B ∨ d) —2— 2--3k+632k-3.
By taking supremum with respect to 1 ≤ l ≤ W on the LHS yields:
..	一 ..	.	- ....	3k-1-1 .	. 5∙3k-1-1 3k-1
BWk J[η3 ◦ Fk-ι](x) - J[η3 ◦ Gk-ι](x)k∞ ≤ δW(B ∨ d)2F-3k+632k-3.
(B.32)
By adding the two upper bounds in B.31 and B.32, we can deduce that:
k-1	k-1	k
kJ [Fk ](x) - J [Gk ](x)k∞ ≤ δW	(B ∨ d) -....12 - -3k+53k-1
+ δW3k÷1 (B V d) 5^k-1-2⅛-3k+632k-3
3k-1-1 ,	、5∙3k-1-1 3k-1
≤ δW2-(B V d)2-2F--k+132k-2.
where the last inequality above follows from k ≥ 2. Taking supremum with respect to X ∈ Ω on the
LHS implies the given upper bound also holds for k . By induction, the claim is proved.
In particular, when k = L, we have NFL(X) = J[Fl](x)t for any X ∈ Ω. Applying Lemma B.13
then yields:
supkNFL(X)k-kNGL(X)k=supkNJ[FL](X)Tk-kNJ[GL](X)Tk
x∈Ω	x∈Ω
≤ sup kJ[FL](X) - J[GL](X)k∞
x∈Ω
3L-1-1 ,	、5∙3L-1-1 3L-1
≤ δW ―2 一 (B V d) —2 — 2F -―k+132L-2.
This finishes our proof of the Lemma.	□
Theorem B.5. (Bounding the DNN Gradient space covering number) Fix some sufficiently large N ∈
Z+. Consider a Deep Neural Network space Φ(L, W, S, B) with L = O(1), W = O(N), S = O(N)
and B = O(N). Then the log value of the covering number of the DNN Gradient space with respect
to the k ∙ k∞ norm ∣∣F (x)k∞ := suPx∈ω |F (x)l，which is denoted by N (δ, VΦ(L, W, S, B), ∣∣∙ k∞),
can be upper bounded by:
log N (δ, VΦ(L, W,S, B), k∙∣∞ ) = O 同 log(δ-1) + 3l log(WB)]) .	(B.33)
Proof. We firstly fix a sparsity pattern (i.e, the locations of the non-zero entries are fixed). Using
equation B.29 in Lemma B.14, yields the following upper bound on the covering number with respect
to k ∙ k∞:
(Z__________________δ__________________I-S
IW3L 2-1 (B V d)=ɪv-1 23⅛-1 -L+132L-2^
Furthermore, note that the number of feasible configurations is upper bounded by: ((W+1产)≤
(W + 1)LS.(Schmidt-Hieber et al., 2020; Farrell et al., 2021). Plug this inequality into the previous
estimation then yields:
log NM φ(L,W,S,B),k ∙k∞) ≤ log 卜 + 1)LS ( W - (B V d) jL
≤ Slog δ-1(W + 1)L32L-2W
3L-1-1,	、
(B V d)
2 3L--L+132L-2)
I5⅛1-i 2 ⅛1 -L+1i
.S log(δ-1)+2Llog(3W)+3Llog(W(BVd))+3Llog2 .
Note that here the dimension d is some constant. Thus, by plugging in thee given magnitudes
L = O(1), W = O(N), S = O(N) and B = O(N), we can further deduce that:
log N (δ, Φ(L, W,S,B), k∙k∞) . s[ log(δ-1) + 3l log(WB)].
This finishes our proof.	□
31
Published as a conference paper at ICLR 2022
Now let’s consider upper bounding the covering number of the Laplacian of the sparse Deep
Neural Networks. Note that for any 1 ≤ k ≤ L - 1, any k-ReLU3 Deep Neural Network
Fk ∈ Φk(L, W, S, B) is a vector-valued function mapping from Rd to RW. Moreover, we define the
Laplacian of Fk (x), which is denoted by ∆[Fk](x), as follows:
∆[Fk](x) = [∆Fk,ι(x), ∆Fk,2(x),…,∆Fk,w(x)]T ∈ RW,
where for any 1 ≤ l ≤ W, we have:
d ∂2
∆Fk,l (x) =∑∂X2 FkMX)-
In particular, when k = L, we have that any FL ∈ ΦL (L, W, S, B) = Φ(L, W, S, B) is a scalar-
valued function mapping from Rd to R. Thus, its Laplacian can be explicitly written as:
d ∂ 2
δ∣fl](X) = δFL(X) = E ∂X2 FL(X).
For both Lemma B.15 and Lemma B.16 below, we consider a fixed Deep Neural Network space
Φ(L, W, S, B) with L = O(1), W = O(N), S = O(N) and B = O(N), where N ∈ Z+ is fixed
and sufficiently large.
Lemma B.15. (Upper bound on ∞-norm of Laplacian of elements in the DNN space) For any
1 ≤ k ≤ L, we have the following upper bound:
3 3 k — 1 - i	53人—1 — 1、
SuP	k∆[Fk](x)k∞ =O(W(B ∨ d).
x∈Ω,Fk ∈Φk(L^W,S,B)
Proof. We use induction on k to prove the claim.
Base case: k = 1. Note that any F1 ∈ Φ1(L, W, S, B) is a linear transform, so the Laplacian
∆[Fι](χ) must be the zero vector for any X ∈ Ω. This implies:
k∆[F1](X)k∞ = 0. (B ∨ d)2.
Inductive Step: Assume that the claim has been proved for k - 1, where 2 ≤ k ≤ L. For any X ∈ Ω
and any Fk ∈ Φk(L, W, S, B), using linearity of the Laplacian operator implies:
∆[Fk](X) = WF(k)∆[η3 ◦Fk-1](X).
Taking the inf-norm on both sides of the identity above implies:
k∆[Fk](X)k∞ ≤ kwFk)k∞k∆[η3 ◦ Fk-ι](X)k∞ ≤ WBk∆[η3 ◦ Fk-ι](X)k∞.
It now remains to upper bound the term k∆[η3 ◦ Fk-1](X)k∞. For any 1 ≤ l ≤ W, we will use the
Chain Rule to write the l-th component ∆[η3 ◦ Fk-1](X) in an explicit form. For any 1 ≤ j ≤ d,
we have:
∂∂
η3—η3[Fk-1,l (x)] = 3n2[Fk-1,((X)]万一Fk-1,l(X).
∂X ∂Xj
Differentiating with respect to Xj on both sides above yields:
∂2
∂X2 n3[Fk-i,i(X)] = 6η1 [Fk-ι,ι (x)]
2	∂ 2
-1,1 (x)) + 3η2[Fk-i,1(X)] ∂X2 Fk-1,1 (x). (B.34)
Summing the expression above from j = 1 to j = d implies:
d
η3[Fk-1,l
=1j
d ∂ 2
+ 3η2 [Fk-1,l(x)] E ∂X2 Fk-1,((X)
j=1 ∂Xj
(X)]
6η1 [Fk-1,l (X)]
≤ 6η1 [Fk-1,l (X)]
2
+ 3η2 [Fk-1,l (X)]
d
=1
1,((X)

32
Published as a conference paper at ICLR 2022
We denote the two summations above by U1 and U2, respectively:
U1 := 6η1 [Fk-1,l (x)]
2
U2 := 3η2[Fk-1,l
On the one hand, by applying Lemma B.9 and Lemma B.12, we can upper bound U1 by:
U1 ≤ 6	sup	kFk-1(x)k∞	sup	kJ[Fk-1](x)k∞
'x∈Ω, Fk-1∈Φk-1 (L,W,S,B)	x∈Ω,Fk-1∈Φk-1(L,W,S,B)	)
≤ 6 X W3k-2-i(B ∨ d)5∙3k-2-i23k⅛-i-k+2 X W3k-2-1(B ∨ d)5∙3k-2-123k-1-1-2k+432k-4
3k-1-3 ,	. 5∙3k-1 -3
.W(B ∨ d)	,
where the last step above follows from k ≤ L and L = O(1).
On the other hand, by applying Lemma B.9 and the inductive hypothesis, we have:
U2 ≤ 3	sup	kFk-1(x)k∞	k∆[Fk-1](x)k∞
'x∈Ω, Fk-1∈Φk-1 (L,W,S,B)	/
.3 X W3k-2-1(B ∨ d)5∙3k-2-1 23k-1-1-2k+4 X W3k-2-i (B ∨ d)5∙3k-2-
3k-1-3 ,	、5∙3k-1-3
.W(B ∨ d)	,
where the last step above follows from k ≤ L and L = O(1).
Summing the two bounds on U1 and U2 implies that for any 1 ≤ l ≤ W, we have:
I (∆[η3 ◦ Fk-ι](x))J ≤ Ui + U . W3k-1- (B ∨ d)5∙3k-1-.	(B.35)
Taking supremum with respect to 1 ≤ l ≤ W then yields:
3 k — 1 - i	53人—1 - 1
k∆[Fk](χ)k∞ ≤ WB∣∣∆[η3 ◦ Fk-i](χ)k∞ . W(B ∨ d).
Taking supremum with respect to X ∈ Ω and Fk ∈ Φk (L, W, S, B) on the LHS implies that the given
upper bound also holds for k. By induction, the claim is proved.	□
Lemma B.16. (Relation between the covering number of the DNN Laplacian space and parameter
space) For any 1 ≤ k ≤ L, suppose that a pair of different two networks Fk, Gk ∈ Φk (L, W, S, B)
are given by:
Fk(x) ：= (WFk)η3(∙) + b& …(WFL)X + b(1)),
Gk(x) ：= (W(⅛(∙) + 咫)…(WGI)X + b(1)).
Furthermore, assume that the k k∞ norm of the distance between the parameter spaces is uniformly
upper bounded by δ, i.e
kWF(l) -WG(l)k∞,∞ ≤δ, kb(Fl) - b(Gl)k∞ ≤ δ, (∀1 ≤ l ≤ k).	(B.36)
Then we have:
(3 k — 1 - 1	53k — 1 - 1、
δW(B ∨ d)	.	(B.37)
Proof. We use induction on k to prove the claim.
Base case: k = 1. Note that any FL ∈ ΦL (L, W, S, B) is a linear transform, so the Laplacian
∆[Fι](χ) must be the zero vector for any X ∈ Ω. Hence, for any X ∈ Ω and any F1,G1 ∈
ΦL (L, W, S, B), we have:
k∆[FL](X) - ∆[GL](X)k∞ =0.δ(B∨d)2.
33
Published as a conference paper at ICLR 2022
Inductive Step: assume that the claim has been proved for k — 1, where 2 ≤ k ≤ L. Then for any
x ∈ Ω and Fk, Gk ∈ Φk (L, W, S, B) satisfying constraint B.36, applying linearity of the Laplacian
operator indicates:
k∆[Fk](χ) — ∆[Gk](χ)k∞ = IlWFk)∆[η3 ◦ Fk-ι](χ) — W.∆[η3 ◦ Gk-ι](χ)k∞
= KWFk)— WGk)) ∆[η3 ◦ Fk-ι](χ)[
+ Il WGk) (∆[η3 ◦ Fι](χ) — ∆[η3 o Gk-ι ](χ)) L
≤ IlWFk) -W幼I∞∣∣∆[η3 oFk-ι](χ)k∞
+ ∣∣W")∣∣∞k∆[η3 o Fk-ι](χ) — ∆[η3 o Gk-ι](χ)k∞.
For the first term ∣∣WFk) — WY) k∞ ∣∣∆[η3 o Fk-1 ](χ)∣∞, applying the bound in equation B.35 and
equation B.36 yields:
∣WFk)-W川U∆[η3 o Fk-ι](χ)∣∞ < δW X W = (B V d)5∙3k-1-3
3k-1-1,	、-3	(B.38)
=δW(B V d) —2一.
For the second term ∣∣W(k)k∞∣∆[η3 o Fk-ι](χ) — ∆[η3 o Gk-ι](χ)∣∞, We need to upper bound
the norm ∣∣∆[η3 o Fk-ι](χ) — ∆[η3 o Gk-ι](χ)∣∞ at first. Note that for any 1 ≤ l ≤ W, we can
use equation B.34 to write the l-th component of ∆[η3 o Fk-ι](χ) — ∆[η3 o Gk-ι](χ) as:
(∆[η3 o Fk-ι](χ) — ∆[η3 o Gk-
-l,l
(χ)]
d2	dLd …	,、、2
6m[Fk-1,l (x)] X (d^Fk-1,l (X))	- 6"1[Gk-U(X)] X (d^ Gk-U(X))
j=1 j	j=1	j
L ∂2
+ 3n2 [Fk-ι,ι(χ)]
Wdχ2
d	∂2
Fk-1,l (X)-沏2 [Gk-IJ(X)] ɪ2 ∂χ2 Gk-1,l(X)
=6ni[Fk-1,1 (x)]X (UFk-ι,ι(X))	- 6ni[Gk-ι,ι(χ)]
+ 6n1[Gk-1,ι(χ)] X (dl^Fk-ι,ι(χ)) — 6n1[Gk-1,ι(χ)]
j=1 ∖ Xj
d d2	d d2
+ 3n2[Fk-1,l(χ)]工 dχ2 FkT,l (X) - 3n2[Gk-1,l(χ)]	dχ2FkT,l(X)
d d2	d d2
+ 3n2[Gk-1,l(χ)] ɪ2 dχ2Fk-1,l (X) - 3n2 [Gk-1,l(χ)] ɪ2 dχ2Gk-1,l(X).
2
2
We denote the four summations above by Vι, V2, V3 and V4, respectively:
V1 ：= 6n1[Fk-1,ι(χ)] X (∂χ^Fk-ι,((χ)) — 6n1[Gk-1,ι(χ)]
d2	d
V2 =6n1[Gk-1,1 (χ)]X	Fk-1,1(χ)) — 6n1[Gk-1,1(χ)] X (而Gk-1,1
L ∂2
2
2
V3 ：= 3n2 [Fk-1,1 (χ)]
V4 ：= 3n2 [Gk-1,1 (χ)]
乙∂χ2
j=i j
& ∂2
d、∂ 2
Fk-1,1 (χ) — 3n2 [Gk-i,ι(χ)]Σ∂χ2 Fk-W
j=i d j
d	∂ 2
-Fk-1,l (X)-沏2[Gk-IJ(X)] ɪ2 ∂χ2 Gk-IJ(X).

34
Published as a conference paper at ICLR 2022
By applying Lemma B.9, Lemma B.10, Lemma B.11 and Lemma B.12, We can upper bound V1 by:
Vi = 6(η1[Fk-1,ι(x)] - η1[Gk-1,ι
≤ 6∖Fk-i,ι(x) - Gk-i,ι(x)∖
2
≤ 6||B-ι(X)- Gk-ι(χ)∣∣∞∣∣ J[B-ι](χ)∣∣∞
3k-2-1
< δW
3k-1-3
< δW
(B V d)
(B V d)
5∙3k-2-1 23⅛1 -k+23k-2 × W3k-2-1(B V d)5∙3k-2-123k-1τ-2k+432k-4
5∙3k-1 -3
2
where the last step above follows from k ≤ L and L = O(1).
Furthermore, note that for any 1 ≤ j ≤ d, we can upper bound the difference (会Fk-ι,l(x))
(∂!"Gk-ι,l(x)) as follows:
—
∂
Gk-ι,l(x))2 ≤
-1,1
Gk-i,l(x))2
=i 西FkT'l(" dx：
≤ (l 看FkT,l(X)I+ |
，、, d 八	,、d π	,、	dC	,、
(x) + 瓦^Gk-1,l(X) ∂xj FkT,l (x)-西GkT,l(x)
∂xjFkTI(X)- ∂XjGkt，1
j
l ∂X
(B.39)
二 Gk-1,l(X) I
(x).
Note that η1(Gk-1,l(x)) ≥ 0. Combining the non-negativity with equation B.39, Lemma B.9,
Lemma B.12 and Lemma B.14 helps us upper bound V2 by:
d
V2 = 6ηι[Gk-i,l(x)]^2
j=i
d
≤ 6∣∣Gk-ι(x)∣∣∞ X
j=i
Fk-1,l
Fk-1,l(X)I + | ∂X^Gk-1,l(X)I
Gk-1,l
(d	d
XI 西 Fk-i,l(x)+ XI 西 Gk-i,l (x)
∂
FkTI(X)- ∂XjGkTI(X)I
∂
Fk-1,l (X) - ∂x^7Gk-1,l (x)
≤ 6∣∣Gk-ι(x)∣∣∞(∣∣ J[Fk-ι](x)∣∣∞ + k J[Gk-ι](x)∣∣∞) ∣∣J[Fk-1](x) - J[Gk-ι](x),
3k-2-1	5∙3k-2-1 3k-1-1	3k-2-1	5∙3k-2-1 3k-1-1
≤ 6W(B V d) —2- 2 —----------------k+2 X 2W(B V d) —2- 2 J----------------k+23k-2
3k-2-1	、5∙3k-2-1 3k-1-1	3k-1-3 ,	、5∙3k-1-3
X δW(B V d) -2— 2 -................k+132k-4 < δW(B V d) -2—.
where the last step above follows from k ≤ L and L = O(1).
Moreover, using Lemma B.9, Lemma B.10 and Lemma B.15 helps us upper bound V3 by:
d ∂2
V3 = (3η2∣Fk-1,l(x)] - 3η2∣Gk-1,l(x)]) ^X ∂^2 Fk-1,l (X)
j=i	j
d ∂2
≤ I 3η2 [Fk-1,l(x)] - 3η2 [Gk-1,l(x)] 1 1 E ∂X2 FkT,l (X) 1
j=i j
≤ 6(	sup	∣∣Fk-ι(x)k∞) IlFk-I(X)- Gk-I(X)I∣∞∣∣∆[Fk-i](x)k∞
'x∈Ω, Fk-1∈Φk-1(L,W,S,B)	/
3k-2-1	5∙3k-2-1 3k-1-1	3k-2-1	5∙3k-2-1 3k-1-1 ； , o , .
< 6W(B V d) —2— 2 -...............k+2 X δW(B V d) —2— 2 -................k+23k-2
3k-2-1	、5∙3k-2-1	3k-1-3 ,	、5∙3k-1-3
X W(B V d) —2— < δW(B V d) —2—.
35
Published as a conference paper at ICLR 2022
where the last step above follows from k ≤ L and L = O(1).
Finally, applying Lemma B.9 and inductive hypothesis helps us upper bound V4 by:
d ∂2	d ∂2
V4 = 3η2[Gk-1,l (X)] ( ɪs ∂X2Fk-IMx) - ΣS ∂χ2Gk-1,l
≤ 3kGk-1(x)k2∞k∆[Fk-1](x) - ∆[Gk-1](x)k∞
.3W3k-2-1(B ∨ d)5∙3k-2-123kττ-2k+4 × δW3
；k-2-1	5∙3k-2-1
—2 — (B ∨ d)	2
3k-1-3 ,	、5∙3k-1 -3
.δW(B ∨ d) -2—.
where the last step above follows from k ≤ L and L = O(1).
Combining the four bounds on V1 , V2 , V3 and V4 implies:
4
3	、 、 、	3 k - 1 - 3	5 • 3 k - 1 - 3
(△切3 ◦ Fk-1 ](x) - ∆[η3 ◦ Gk-ι](x)∖ = EVi . δW-2-(B ∨ d)-2-
i=1
Taking supremum with respect to 1 ≤ l ≤ W gives us an upper bound on the second term
∣∣W(k)k∞k∆[η3 ◦ Fk-ι](x) - ∆[η3 ◦ Gk-ι](x)k∞:
∣∣W(k)k∞k∆[η3 ◦ Fk-ι](x) - ∆[η3 ◦ Gk-ι](x)k∞ . WB X δW 3 2 3 (B ∨ d)ɪɪ^~3
3k-1-1 ,	、5∙3k-1-1
=δW(B ∨ d) -2—.
(B.40)
Combining the two bounds derived in equation B.38 and equation B.40 then implies:
3 k — 1 - 1	53k — 1 - 3	3 k - 1 - 1	5 3 k — 1 - 1
∣∆[Fk](x)- ∆[Gk](x)∣∞ . δW(B ∨ d)+ δW(B ∨ d)
3k-1-1 ,	、5∙3k-1-1
.δW(B ∨ d) -2-
Taking supremum with respect to x ∈ Ω on the LHS implies that the given upper bound also holds
for k. By induction, the claim is proved.	□
Given a Neural Network function space Φ(L, W, S, B), we define a corresponding Neural Network
Laplacian space ∆Φ(L, W, S, B) as:
∆Φ(L, W, S, B) := {∆F | F ∈ Φ(L, W, S, B)}.	(B.41)
Theorem B.6. (Bounding the Neural Network Laplacian space covering number) Fix some suffi-
ciently large N ∈ Z+. Consider a Deep Neural Network space Φ(L, W, S, B) with L = O(1), W =
O(N), S = O(N) and B = O(N). Then the log value of the covering number of the DNN
LaPlacian space with respect to the ∣∣∙∣∣∞ norm ∣∣F(x)k∞ := suPx∈ω |F(x)|, which is denoted by
N (δ, ∆Φ(L, W, S, B), k ∙ ∣∣∞) ,can be upper bounded by:
log N (δ, ∆Φ(L, W,S, B), k∙k∞ ) = O (Sh log(δ-1) + 3l log(WB)]) ∙	但4幻
Proof. We firstly fix a sparsity pattern (i.e, the locations of the non-zero entries are fixed). Applying
Lemma B.16 yields that there exists some constant C = O(1), such that the covering number with
respect to ∣∣ ∙ ∣∞ can be upper bounded by:
(δ)-s
3l-1-1	5∙3l-1-1
vCW —2— (B ∨ d) —2— J
Furthermore, note that the number of feasible configurations is upper bounded by ((W+1产)≤
(W + 1)LS(Schmidt-Hieber et al., 2020; Farrell et al., 2021). Then we plug this into the pervious
estimation and yields:
log N (δ, Φ(L, W, S, B), ∣H∣∞) ≤ log (W + 1)ls ( -3L-1 1 δ~5^l--t ) -S
_	'CW -2— (B ∨ d) -2- /	_
≤ S log [δ-1(W + 1)lW 3L⅛-i (B ∨ d) 5'3L-1τ i
.Shlog(δ-1)+Llog(W)+3Llog(W(B∨d))i.
36
Published as a conference paper at ICLR 2022
Note that here the dimension d is some constant. Thus, by plugging in thee given magnitudes
L = O(1), W = O(N), S = O(N) and B = O(N), we can further deduce that:
log N (δ, ∆Φ(L, W, S, B), k ∙ k∞) . S h log(δ-1) + 3l log(WB)].
This finishes our proof.	□
Lemma B.17 (Local Rademacher Complexity Bound of DNN Estimator for Deep Ritz Method).
Consider a Deep Neural Network space F(Ω) = Φ(L,W,S,B) with L = O(1),W = O(N), S =
O(N) and B = O(N), where N ∈ Z+ is fixed to be sufficiently large. Moreover, assume that the
gradients andfunction value of F (Ω),u*,V and f are uniformly bounded
max SUp ku∣∣L∞(Ω), SUp kVu∣∣L∞(Ω), k"* l∣L∞(Ω), IN"* l∣L∞(Ω), Lax, kf Hl∞(ωJ ≤ 如
u∈F (Ω)	u∈F (Ω)
(B.43)
For any ρ > 0, we consider a localized set Lρ defined by:
Lρ(Ω) := {u : U ∈ F(Ω), k" — U*kHι ≤ ρ}.
Then for any P & n-2, the Rademacher complexity of a localized function space Sρ(Ω):=
{h := ∣Ω∣ ∙ h 1 (kVuk2 — ∣∣Vu*k2) + 1 V(∣u∣2 — |u*|2) — f(u — u*)i ∣ U ∈ Lρ(Ω)} can be
upper bounded by a sub-root function
φ(ρ) := O (jS3Lρ log(BWn)^ .
i.e. we have
φ(4ρ) ≤ 2φ(ρ) and Rn(SP(Ω)) ≤ φ(ρ).	(B.44)
holds for all P & n-2.
Proof. Firstly, We will check that for any U ∈ Lρ(Ω), the corresponding function h in SP(Ω) is
Lipschitz with respect to U — u* and ∣∣Vu∣ — ∣∣Vu*∣. Note that for any u1,u2 ∈ Lρ(Ω) with
corresponding functions hi, h2 ∈ Sρ(Ω), applying boundedness condition B.43 yields:
|hi(x) — h2(x)∣ ≤ 1∣∣Vuι(x)k2 — kVu2(x)k2∣ + 1 |V(x)∣∣uι(x)2 — u2(x)2∣ + ∣f(x)∣∣uι(x) — u2(x)∣
≤ C∣∣∣∣VU1(x)∣ — ∣VU2(x)∣∣∣∣ +(C2+C)|U1(x) —U2(x)|
=C ∣ (kVui(x)k — ∣∣Vu*(χ)k) — (kVu2(χ)k - kVu*(x)k) ∣
+ (C2 + C)∣∣∣(U1 (x) — U* (x)) — (U2 (x) — U* (x))∣∣∣.
Let’s pick L = C2 + C > C. Applying the Talagrand Contraction Lemma B.2 helps us upper bound
the local Rademacher complexity Rn(Sρ(Ω)) by
Rn(SP(C)) = Ex Eσ
1n 1	1
, nX=X叱""" -kVu k ) + 2V(1u1 -|u 1)-f(U-U )i
≤ 2LExEσ
+ 2LEx0 Eσ0
1n
SUp — σ σi (U(Xi) — U*(xi)
u∈Lρ(Ω) n M ∖
n
u∈LPω) n X σi(kVU(Xi)k-kVU*(Xi)k
({u — U* : U ∈ Lρ}) + Rn ({kVUk — kVU*k : U ∈ LP
From the localization constraint P ≥ ∣∣u — u*∣∣H y)=IlU — 〃*1匡(。)+ I∣Vu — VU*kL2 g),wecan
deduce that
kU — u* kL2(Ω) ≤ √ρ and IlVU - Vu* kL2(Ω) ≤ √ρ.	(B.45)
37
Published as a conference paper at ICLR 2022
Moreover, note that Ω ⊂ [0,1]d. Applying triangle inequality yields:
kVuk-∣∣Vu*k
2
L2(Ω)
[IkVu(X)k - kVu*(x)k 12dx ≤ [ ∣∣Vu(x) - Vu*(x)k2dx
Ω	Ω
=kVu - vu*Il2(Ω) ≤ P ⇒ IkVuk- kVu*k，2g)≤ √p.
(B.46)
Using inequality B.45 and inequality B.46, we have:
Rn(Sρ(Ω)) . RnI∣u — u* ： u ∈ Lρ} + + Rn ({kVuk—kVu*k ： u ∈ Lρ∖)
≤ Rn ({u 一 u" : u ∈ Φ(L,W,S,B), ku - u*∣∣L2(Ω) ≤ √p})
+ Rn ({kVuk- kVu"k ： u ∈ Φ(L,W,S,B),∣kVuk-kVu*k∣L2g)≤
(Geer & van de Geer, 2000; Rakhlin et al., 2017) showed a “upper isometry” property, where the
metric k ∙ ∣∣L2 is equivalent to k ∙ kn,2 With high probability. Combining this fact with Theorem B.3,
we can bound the local Rademacher complexities using Dudley integral:
Rn(Sρ(Ω)) . Rn ({u — u* ： u ∈ Φ(L,W,S,B), ku - u*kL2(Ω) ≤ √p})
+ Rn ({kVuk-kVu"k ： u ∈ Φ(L,W,S,B), ∣∣kVuk -kVu*k∣L2g)≤ √ρ}
≤ Rn ({u - u* ： u ∈ Φ(L, W, S, B), ku - u*kn,2 ≤ 2√p})
+ Rn ({kVuk-kVu"k ： u ∈ Φ(L,W,S,B),∣∣kVuk -kVu*k∣∣	≤ 2√ρ})
+
o<lnf V {4α + √√n Z：乖,logN(δ, Φ(L,W,S,B),k∙kn,2)dδ}
inf
0<α<2√p
r	12	∕,2√ρ	/
{4a + √n Ja	V
log N (δ, VΦ(L,W,S,B), k∙kn,2)dδ}
+
inf
0<a<2√ρ
r 12 r√ρ
{4a+√n L
√logN(δ, Φ(L,W,S,B), k∙k∞)dδ}
inf
0<a<2√p
{4α +	广
√logN(δ, VΦ(L,W,S,B), k ∙ k∞)dδ}.
For any P & nτ, we pick α = ɪ . √ρ and plug in the upper bounds proved in Theorem B.4 and
Theorem B.5, which implies:
Rn(Sρ(Ω)) . n + √n J：vp ,s[log(δ-i) + 3L log(WB)]dδ + √n J：,s[log(δ-i) + 3L log(WB)]dδ
■n	-n
n
n
.J学log(BWn).
□
Lemma B.18 (Local Rademacher Complexity Bound of DNN Estimator for Physics Informed Neural
Network). Consider a Deep Neural Network space F(Ω) = Φ(L,W,S,B) with L = O(1),W =
O(N), S = O(N) and B = O(N), where N ∈ Z+ is fixed to be sufficiently large. Moreover,
assume that the gradients andfunction value of F (Ω),u*,V and f are uniformly bounded
max SUp kukL∞(Ω), SUp l∣∆ukL∞(Ω), ku*kL∞(Ω), k∆u*kL∞(Ω), Vmax, kf kL∞(Ω) } ≤ C
u∈F (Ω)	υ∈F (Ω)
(B.47)
For any P > 0, we consider a localized set Mρ defined by:
Mρ(Ω) ：= {u ： u ∈ F(Ω), ku — u*kH2 ≤ ρ}.
38
Published as a conference paper at ICLR 2022
Thenfor any P & n-2 ,the Rademacher complexity of a localized function space Tρ(Ω) := {h :=
∣Ω∣∙[(∆u - Vu + f )2 - (Au* - Vu* + f)2] I U ∈ Mρ(Ω)}can be upper bounded by a sub-root
function
(KP):
S3Lρ log (BWn)].
n
i.e. we have
φ(4ρ) ≤ 2φ(ρ) and Rn(TP(Ω)) ≤ φ(ρ).	(B.48)
holdsfor all P & n-2.
Proof. Firstly, We will check that for any U ∈ Lρ(Ω), the corresponding function h in Sρ(Ω) is
Lipschitz with respect to U — u* and Au - Au*. Note that for any UkU2 ∈ Lρ(Ω) with corresponding
functions h1, h2 ∈ Sρ(Ω), applying boundedness condition B.47 yields:
∣h1 (x) — h2(x)∣ ≤ ∣Auι — ∆U2 — V(U1 — U2)∣∣Au1 — Vuι + Au2 — Vu2 + 2f∣
≤ (2C2 + 4C)(∣Au1 — Au2∣ + C∣U1 — u2∣)
=(2C2 + 4C) J (Auι(x) — Au*(x)) — (AU2(x) — Au*(x)) |
+ (2C3 + 4C2) J (uι(x) — u*(x)) — (u2 (x) — u*(x)) | .
Let,s pick L = max{2C2 +4C, 2C3 + 4C2}. Applying the Talagrand Contraction LemmaB.2helps
us upper bound the local Rademacher complexity Rn(TP(Ω)) by
1 n
Rn(Tρ(Ω)) = EχEσ	sup —)、σi [(Au — Vu + f)2 — (Au*
∣∙u∈Mρ(Ω) n M
—Vu* + f)2]]
1n
≤ 2LEχEσ	sup 一 σ σi (U(Xi) — u*(x.
u∈Mp(Ω) n M '
+ 2LEχ0 Eσ∕
1n
SUP	Xσ0 (AU(Xi)- Au*(xi
u∈Mρ(Ω) n 仁 '
.Rn (∣u —	U*	: U ∈	Mρ∣) + Rn (I Au - Au* : U ∈ Mρ))
< Rn ({u —	u*	: U ∈	Φ(L, W, S, B)JU — u* ∣j(Ω) ≤ √ρ})
+ Rn ({Au	— Au* :	U ∈ Φ(L, W, S, B), ∣∣Au — Au*kL2(n)≤ √p})
≤ Rn ({u —	U*	: U ∈	Φ(L, W, S, B), ∣∣u — u* kn,2 ≤ 2√p})
+ Rn ({Au — Au* : U ∈ Φ(L,W,S,B),kAu - Au*∣∣n,2 ≤ 2√ρ})
<。/吗遮{4α + √∣ [2 JlogN(δ, Φ(L,W,S,B), ∣H∣n,2)而}
12	2√ρ
+0< inf, {4α+√n J	y log N (δ, Aφ(L, w, s, B), ∣∣∙ ι∣n,2)dδ}
inf
0<a<2√ρ
+ inf
0<a<2√ρ
W +a广
r 12 f2√ρ
W + √n L
,log N(δ, Φ(L,W,S,B ),∣∣∙k∞)而}
,logN(δ,AΦ(L,W,S,B),k ∙ k∞)⑹.
39
Published as a conference paper at ICLR 2022
For any P & *, we pick α = 1 . √ρ and plug in the upper bounds proved in Theorem B.4 and
Theorem B.5, which implies:
Rn(Tρ(Ω)) . 1 + √1= J：vp JS hlog(δ-ι) + 3L log(WB)i dδ + √1= J：JS [log(δ-i)+3L log(WB)] dδ
,XI	-XI
n
n
.JSnLP Iog(BW叫
□
B.3 Auxiliary definitions and lemmata On Approximation Error
B.3.1	Approximation using Truncated Fourier Basis
Lemma B.19. Given α > 0 and a fixed integer ξ ∈ Z+. For any function f ∈ Hα(Ω) , we let
fξ = Ekzkco≤ξ fzφz be the best approximation of f in the space Fξ(Ω). Thenfor any 0 < β ≤ α,
we have the following inequality:
kf - fξ kHβ(Ω) ≤ ξ-2(α-β)kf kH α .
Proof. For f ∈ Hɑ(Ω), We know the Fourier coefficient satisfies
X Ifz I2kzk2α . kfkHα .
kzk∞≥ξ
We directly construct fξ = Pkzk ≤ξ fzφz to be the truncated Fourier series of the function f, then
we have
kf - fξkHβ(Ω) . X IfzI2kzk2β ≤ ξ-2(α-β) X IfzI2kzk2α ≤ ξ-2(α-β)kfkHα.
kzk∞≥ξ	kzk∞≥ξ
□
B.3.2	Approximation using Neural Network
In this section, we aim to provide approximation bound for deep neural network. Our proof of the
approximation upper bound is based on the observation that the B-spline approximation(De Boor &
De Boor, 1978; Schumaker, 2007) can be formulated as a ReLU3 neural network efficiently(Suzuki,
2018; Guhring et al., 2020; Duan et al., 2021; Jiao et al., 2021a). Although the proof of the
approximation of the neural network to the Sobolev spaces is a standard approach, we still demonstrate
the proof sketch here.
Definition B.4. (Univariate and Multivariate B-splines) Fix an arbitrary integer l ∈ Z+. Consider a
corresponding uniform partition πl of [0, 1]:
0	t(l)	<	t(l)	<	< t(l)	< t(l)	1
∏l ♦ 0 —	t0	<	11	<	∙ ∙ ∙ < tI_1	< t/	— 1,
where t(l) — ∣ (V 0 ≤ i ≤ l). Nowfor any k ∈ Z+, we can define an extended partition ∏ι,k as:
(l)	(l)	(l)	(l)	(l)	(l)	(l)	(l)
πl,k ♦ t-k+1 — .一 t-1 — 0 ― t0 < t1	< .一 < tl-1 < tl = 1 — tl+1 — .— — tl+k-1∙
Based on the extended partition πl,k, the univariate B-splines of order k with respect to partition πl
are defined by:
NS)(X) = (T)k(R+k-1" ht(l), ∙∙∙ ,t(l+k i max{(x-1), 0}k-1, X ∈ [0,1], i ∈ Iι,k, (B.49)
where Iι,k = {-k + 1, —k + 2,…，l _ 1} and [t(l),…，t(+k] denotes the divided difference
operator.
Equivalently, for any X ∈ [0, 1], we can rewrite the univariate B-splines Nι(,ki )(X) in an explicit form:
(k-i)! Pk=o(-1)j(k) max nx - i++j, 0}	, (。≤ i ≤ l - k + 1)
N∖? (X)= Pk-I aij max {x - j, 0}kτ + Pn=I binxn + bi0, (-k +1 ≤ i ≤ 0)	(B.50)
pj=i-k+i Cij max {x - j, 0}	, (l - k + 1 ≤ i ≤ l - 1)
40
Published as a conference paper at ICLR 2022
where {aij | - k + 1 ≤ i ≤ 0, 0 ≤ j ≤ k - 1}, {bin | - k + 1 ≤ i ≤ 0, 1 ≤ n ≤ k - 2} and
{cij | l - k + 1 ≤ i ≤ l - 1, l - k + 1 ≤ j ≤ l - 1} are some fixed constants.
For any index vector i = (i1,i2,…，id) ∈ Idk，we Can define a corresponding multivariate B-spline
as a product of univariate B-splines:
Nk)(X) =∏d=1N,k)(Xj).	(B.51)
Definition B.5. (Interpolation OPeratOr(SChumaker,2007)) Take some domain Ω ⊂ [0,1]d and two
arbitrary integers k, l ∈ Z+. Consider the extended partition πl,k and the corresponding set of
multivariate B-splines {Nl(,ki)(x)}i∈Id defined in Definition B.4. For any i ∈ Ild,k, we define the
domain Ωi := {x ∈ Ω : Xj ∈ [tj ,tj+k ], 1 ≤ j ≤ d}. There exists a set of linear functionals
{λi}i∈ιd J where λi : L1(Ω) → R (V i ∈ Ikl), such thatfor any i ∈ Idz andP ∈ [1, ∞], we have:
k∖ -d
λi(N(j)) = δi,j	and	∣λi(f)|	≤	9d(kτ)(2k	+1)d(j)	IfkLp(ω,),	∀ f ∈	Lp(Ω).	(B.52)
The corresponding interpolation operator Qk,l is defined as:
Qk,ιf ：= X λi(f)N(,i), V f ∈ L1(Ω).
i∈Ikd,l
Theorem B.7. [(Schumaker, 2007)] Fix f ∈ Ws(Ω) with Ω ⊆ [0,1]d, S ∈ Z+ andP ∈ [1, ∞).
Then for any k, l, r ∈ Z+ with k ≥ s and 0 ≤ r ≤ s, we have that there exists some constant
C = C(k, s, r,P, d), such that:
1 s-r
Ilf - Qk,lf kHr(Ω) ≤ C(7)	kf kHs(Ω).
Theorem B.8. (Approximation result of Deep Neural Network) Fix some dimension d∈ Z+, some
domain Ω ⊆ [0,1]d. We pick some l = N 11 ≥ 2, for any s,r ∈ Z+ with 0 ≤ r ≤ S and any
function u* ∈ Hs(Ω), there exists some sparse Deep Neural Network UDNN ∈ Φ(L, W, S, B) with
L = O(1), W = O(N), S = O(N),B = O(N), such that:
s-r
IluDNN - U kHr(Ω) . N— "ɪ Ilu kHs(Ω).
(B.53)
Proof. We firstly show that the given function u* can be approximated well by some linear Combi-
nation of multivariate splines, which is denoted by usp. Note that N is assumed to be sufficiently
large. Hence, we may pick l =「Nd] = Θ(Nd) ∈ Z+ to be the partition size of the B-SPlines.
Moreover, by picking k = 4 and P = 2 in Theorem B.7, we have that the linear combination
usp := Q4,lu* = Pi∈Id λi(u*)Nl(,4i) satisfies:
ku* - uspkHr(Ω) = ku* - Q4,lu*kHr(Ω)
1 s-r	s-r
≤ C1]) ku kHs(Ω) = CN d ku kHs(Ω).
We will then show that the linear combination usp = Pi∈Id λi (f)Nl(,4i) can be implemented by
some Deep Neural Network uDNN ∈ Φ(L, W, S, B) with L =, O(1), W = O(N), S = O(N) and
B = O(log N). Firstly, note that for x ≥ 0, both x and x2 can be expressed in terms of the ReLU3
activation function η3 with no error:
χ = — 12切3(/ + 3) — 5η3(χ + 2) + 7η3(χ +1) — 3^(/) + 6],
x2 =- 1[η3(x+2)—4η3(X+1)+3η3(x)- 4].
Applying the explicit formula listed in equation B.50 implies that for any —3 ≤ i ≤ l — 1, the
univariate B-spline function Nl(,4i)(X) (X ∈ [0, 1]) can be implemented by some ReLU3 Deep Neural
Network vDNN with both scalar input and scalar output. We have that for vDNN, the depth Lv is 2 and
the maximum width Wv is upper bounded by 11.
41
Published as a conference paper at ICLR 2022
Secondly, for any χ,y ≥ 0, We have that the product operation X ∙ y can be expressed in terms of the
ReLU3 activation function η3 with no error:
χ ∙ y = 1[(χ + y)2 - χ2 - y2]
=--12 [n3(x + y + 2) - 4η3(x + y + 1) + 3η3(x + y)
-η3(x + 2) + 4η3(x + 1) - 3η3(x) -η3(y + 2) + 4η3(y +1) - 3η3(y) + 4 .
In (Schumaker, 2007), it has been proved that the B-splines are alWays non-negative, i.e Nl(,4i) (x) ≥
0, ∀ x ∈ [0, 1]. Therefore, by multiplying the non-negative univariate B-splines, We can implement
any multivariate B-spline Nl(,4i) = Πjd=1Nl(,4i) (xj) With some ReLU3 Deep Neural NetWork pDNN.
We have that for pdnn, the depth Lp =「log? d] + 2 and the maximum width Wp = max{11d, 9 d}.
Hence, we can further claim that U = Pi∈"匕 λi(u*)N)?, which is a linear combination of the
multivariate B-splines Nl(,4i), can be implemented by some ReLU3 Deep Neural Network uDNN.
It remains to check that uDNN ∈ Φ(L, W, S, B) with L = O(1), W = O(N), S = O(N) and
B = O(N). Note that we can ensure that the hidden layers of uDNN are of the same dimension W by
adding inactive neurons.
For the depth L of uDNN, we have that L is equal to Lp + 1, where Lp denotes the depth of the
ReLU3 Deep Neural Network pDNN. Thus, we have L = Lp + 1 = dlog2 de + 3, which implies that
L = O(1).
For the width W of uDNN, we have that W ≤
Deep Neural Network pDNN. This implies:
W ≤ |Ikd,l| × 11d = 11d(l + k)d
|Ikd,l |Wp , where Wp denotes the width of the ReLU3
11d(l + 4)d = O(ld) ⇒ W = O(N).
For the sparsity constraint S of uDNN, starting from the third layer, the number of activated neurons is
half of the number of activated neurons at previous layer. This yields the following upper bound on
S:
L-2
W
S ≤ 2(W + W + £ —) ≤ 8W ⇒ S = O(W) = O(N).
j=0 2
For the norm constraint B of uDNN, we have the following upper bound on B from equation B.50
and equation B.52:
B = O(max{lk-1, sup λ*(u*)}) = O(max{l3,ld}) = O(N).
i∈Ikd,l
Now we have shown that parameters L, W, S, B of the Deep Neural Network uDNN are of the desired
magnitude, which completes our proof.
□
B.4 Final Upper Bound
In this subsection, we provide the proof of upper bounds for PINN and DRM. For both estimator,
we first provide a meta-theorem to illustrate the approximation and generalization decomposition
with a O(1/n) fast rate generalization bound(Bartlett et al., 2005; Xu, 2020). Then we use truncated
Fourier basis estimator and neural network estimator as example to obtain the final rate.
B.4.1	Deep Ritz Methods
Theorem B.9 (Meta-theorem for Upper Bounds of Deep Ritz Methods). Let u* ∈ HS (Ω) denote
the true solution to the PDE model with Dirichlet boundary condition:
-∆u + Vu = f on Ω,
u = 0 on ∂Ω,
(B.54)
where f ∈ L2(Ω) and V ∈ L∞(Ω) with 0 < Vmin ≤ V(x) ≤ VmaX > 0. In Theorem B.1, it has
been proved that u* can be obtained by minimizing the loss E(U):
*
u
arg min E(u) := arg min
u∈H1(Ω)	u∈H1(Ω)
nɪ J h∣∣Vu∣∣2 + V|u|2i dx -J fudx}.
42
Published as a conference paper at ICLR 2022
For a fixed function space F (Ω), consider the empirical loss induced by the Deep Ritz Method:
1n	1	1
En(U) = n X [|°|‘(2k^u(Xj)k + 2V(Xj)Iu(Xj)| - f (Xj)U(XjD],	(B.55)
j=1
where {Xj}n=ι are datapoints uniformly sampledfrom the domain Ω. Then the Deep Ritz estimator
associated WithfUnCtiOn space F(Ω) is defined as the minimizer of En(u) over the function space
F (Ω):
u DRM = min En(U).
u∈F (Ω)
Moreover, we assume that there exists some constant C > 0 such that all function U in the function
space F(Ω), the real solution U and f, V satisfy thefollowing two conditions.
•	The gradients and function value are uniformly bounded
max sup kU∣L∞(Ω), Sup INUIl∞(ω), k〃* ∣∣L∞(Ω),11▽〃* l∣L∞(Ω),Vmaχ, kf ∣∣L∞(Ω) } ≤ C.
u∈F (Ω)	u∈F (Ω)
(B.56)
•	All thefunctions in the function space F (Ω) satisfies the boundary condition
u = 0 on ∂Ω.
At the the same time, for any ρ > 0, we assume the Rademacher complexity of a localized function
space Sρ(Ω) := {h := ∣Ω∣∙ [2 (kVuk2 -kVu*k2) + 2V(∣u∣2 -∣u*∣2) - f(u - U)] IkU -
u*kHι ≤ ρ} can be upper bounded by a sub-rootfunction φ = φ(ρ) : [0, ∞) → [0, ∞), i.e.
φ(4ρ) ≤ 2φ(ρ) and Rn(S ρ(Ω)) ≤ φ(ρ) (∀ ρ> 0).
(B.57)
For all constant t > 0. We denote r* to be the solution of the fix point equation of local Rademacher
complexity r = φ(r). There exist two constants Cp, Cq such that with probability 1 - Cp exp(-Cq t),
we have the following upper bound for the Deep Ritz Estimator
IlUDRM - u*kHι . u %。)(e(uf) - E(UR + max {『*,(}∙
Proof. To upper bound the excess risk ∆E(n) := E(UDRM) - E(u*), following(Xu, 2020; Lu et al.,
2021b; Duan et al., 2021), we decompose the excess risk into approximation error and generalization
error with probability 1 - e-Cqt, where Cq > 0 is some constant:
∆E(n)(Udrm) = E(UDRM)- E(u?) = [E(Udrm) - En(UDRm)[ + [En(Udrm) - En(UF)]
+ [En(UF)-E(UF) + [E(UF) - E(U?)
≤ [E(Udrm) - En(UDRm)] + [E.(uf) - E(uf)] + [E(uf) - E(u?)]
≤ [E(UDRM)- E(U*) + En(U*) - En(UDRM)]
+ 2 [E(UF) - E(U?)] + 2n,
(B.58)
where the expectation is on all sampled data. The inequality of the third line is because UDRM
is the minimizer of the empirical loss En in the solution set F(Ω), so we have En(UDRM) ≤
En(UF). The last inequality is based on the Bernstein inequality. The variance of h = ∣Ω∣ ∙
h 2 (IlVUk2 -∣∣Vu*k2) + 1V (∣u∣2 - |u*|2) - f (u - u*)] can be upper bounded by [ [E(uf)-
E(U?) due to the strong convexity of the variation objective (B.60). According to the Bernstein
inequality, there exists some constant Cq > 0, such that with probability 1 - e-Cqt we have:
En(UF)- En(u*) - E(UF) + E(u*) ≤ :
"E(UF)- E(U*)]≤ 1 [E(UF )-E(u?)] + 2n.
n
43
Published as a conference paper at ICLR 2022
Note that B.58 holds for all function lies in the function space F . Thus, we can take uF :
argminuo∈F⑸)(E(UO) - E(u*)) and finally get
△E⑺ ≤ E(UDRM)- E(u*) + En(U*) - En(U) +3	inf,、(E(UF) - E(u*)) + tr.
|^^^^^^^^^^^^^{^^^^^^^^^^^^} 2 UF∈F(Ω)	2n
δ Egen	,^^^^^^^^^^^^^^^^^^^^-^^^^^^}
∆Eapp
This inequality decompose the excess risk to the generalization error ∆Egen := E(UDRM) - E(u* ) +
En(U*) - En(UDRM) and the approximation error ∆Eapp = infUF∈f(ω) (E(UF) - E(u*)).
From the lemmata proved in Section B.3, we already have an estimation of the approximation error’s
convergence rate. So now we’ll focus on providing fast rate upper bounds of the generalization error
for the two estimators using the localization technique(Bartlett et al., 2005; Xu, 2020). To achieve the
fast generalization bound, we focus on the following normalized empirical process
Sr(Ω) := {h(x) := EE]-：了 | h ∈ S(Ω)} (r > 0).
First, we try to bound the expectation of the normalized empirical process. Applying the Symmetriza-
tion Lemma B.1, we can first bound the expectation as
sup Ex0
r ~. ,、
h∈Sr(Ω)
hM X「
1n
n i=1
≤ Ex0
_ , ^.,...
≤ 2Rn(Sr(Ω)).
1	,1 i`	IA /rʌ' ♦ F C 1
where the function class Sr (Ω) is defined as:
Srg) ：= {h(x) ：= Ehx7 | h ∈ s(ω)},
where S(Ω) = {h := ∣Ω∣ ∙ [l (||Vu『一∣∣VU*k2) + 2V(|u|2 - |u*|2) - f (u - u*)] }. Thenap-
plying the Peeling Lemma B.4 to any function h ∈ S(Ω) helps us upper bound the local Rademacher
complexity Rn(Sr (Ω)) with the function φ defined in equation B.57:
Rn(Srg))= EσjExhh∈¾	i
≤ 4φ(r)
一 r
Combining all inequalities derived above yields:
sup Ex0
h∈Sr(Ω)
-X h(xi) ≤ 2Rn(Sr(Ω)) ≤ 8φr)(r> 0).
nr
i1
(B.59)
Secondly we’ll apply the Talagrand concentration inequality, which requires us to verify the condition
needed. We will first check that the expectation value E[h] is always non-negative for any h ∈ S(Ω):
E[h]=ω，|°| ∙ (1 kVU(x)k2+2V(X)IU(X)|2- f(X)U(X))dx
-∣Ω∣ ZO ∣Ω∣∙ (2kVU?(x)k2 + -V(x)∣U*(x)∣2 - f(x)U*(x))dx
= E(U) - E(U?) ≥ 0 ⇒ E[h] ≥ 0.
We will proceed to verify that any h = Ehg ∈ Sr (Ω) is of bounded inf-norm. We need to prove
that any h ∈ S(Ω) is of bounded inf-norm beforehand. Using boundedness condition listed in
44
Published as a conference paper at ICLR 2022
equation B.56 implies:
Mh = ∣Ω∣k 1 (kV"k2 -kVu*k2) + 2V(|u|2 -|u*|2) - f(u - u*)∣∣∞
≤ 4 (Ruk∞ + kVu*k∞) + ɪ %ax(k"k∞ + ku*k∞) + ∣Ω∣kfk∞(k"k∞ + ku*k∞)
≤ ɪ- × 2C2 + -ɪ-Vmax × 2C2 + 2|O|C2 = 1ωKVmax + 3)。2.
By taking M := |。|(%明 + 3)C2, We then have Ilhl∣∞ ≤ M for all h ∈ S(Ω). Note that the
denominator can be lower bounded by |E[h] + r| ≥ r > 0. Combining these two inequalities help us
upper bound the inf-norm ∣∣h∣∞ = supxeQ |h(x)| as follows:
∣∣7 llE[h] - hl∞
∣h∣∞ =	|E[h] + r|
≤ 2⅛ ≤ 2M =： β.
We will then check the normalized functions EMf) in iSr (Ω) have bounded second moment,
which is satisfied because of the regularity results of the PDE. We aim to show that there exist some
constants α, α0 > 0, such that for any h ∈ S(Ω), the following inequality holds:
αE[h2] ≤ ∣∣u - u*∣∣Hi(Ω) ≤ αE[h].
(B.60)
The RHS of the inequality follows from strong convexity of the DRM objective function proved in
Theorem B.1:
E[h] = E(U) - E(u*) ≥
min{l,Vmin}	2
------4------Ilu - U IIhi(Ω).
The LHS of the inequality follows from boundedness condition listed in equation B.56 and the
QM-AM inequality:
E[h2] = /
Ω
1 (∣Vu∣∣2 -∣∣Vu*k2) + 2V(|u|2 -|u*|2) - f (u - u*)
2
dx
≤ ∙∣ / (IlVUk2 -∣∣Vu*∣2) dx + I / V2(|u|2 -|u*|2)2dx + 3/ f2(u — u*)2dx
≤ f 〃kVUk-IlVU*∣2(∣IVUk + ∣Vu*∣)2 dx +3 Vmax 〃|u|-|u*『(|u| + |u*|)2dx
+ 3C2/ (u - u*)2dx ≤ 3C2/ ∣∣Vu -Vu*∣∣2dx + 3C2(1 + Vmax) / |u - u*|2dx
≤ 3C2(1 + Vmax) Hu - u*HH1(Ω).
By picking α0 = min{4 vmi 11} and a = 302(卜丁2), we have finished proving inequality B.60. Then
we can can upper bound the expectation E[h2] as:
E[h 2]
E[(h - E[h])2]
|E[h]+ r|2
E[h2] - E[h]2
|E[h] + r|2
≤	E[h2]
一|E[h] + r|2 .
Using the fact that E[h] ≥ 0 and inequality B.60, we can lower bound the denominator |E[h] + r|2 as
follows:
|E[h] + r|2 ≥ 2E[h]r ≥ 竽E[h2].
Therefore, we can deduce that:
E[h2] ≤
E[h2]	/	E[h2]
|E[h] + r|2 ≤ 誓E[h2]
a =： σ2.
2rα
ɪ T	f . ∙	∙ .ι 1	1 ∙ 11	片 / rʌ ∖ ∙ CK	FF	F	,
Hence, any function in the localized class Sr (Ω) is of bounded second moment.
45
Published as a conference paper at ICLR 2022
It is easy to check that for any h ∈ Sr (Ω), we have
E[h] =需则
0,
♦	C . ∙	∙	, 1	1	1 ∙	11	片 / z-ʌ ∖ ∙ Γ∙
i.e. any function in the localized class Sr (Ω) is of zero mean.
Now we have verified that any function h ∈ Sr (Ω) satisfies all the required conditions. By taking μ
to be the uniform distribution on the domain Ω and applying Talagrand,s Concentration inequality
given in Lemma B.3, we have:
sup
h∈Sr (Ω)
1n
n∑h(χi) ≥2- sup
n i=1	h ∈Sr(Ω)
≤ e-t
By using the upper bound deduced above and plugging in the expressions of β and σ, we can rewrite
Talagrand’s Concentration Inequality in the following way. With probability at least 1 - e-t, the
inequality below holds:
1	7 /	1	7 / / c	ι Γ1 o / !	2tσ* 2 2tβ
n Eh(Xi) ≤ ~ SuP n∑h(xi) ≤ 2~sup Eχ0 n Eh(Xi) +ʌ/ — 十 —
n i=1	h∈Sr(Ω) n i=ι	h∈Sr(Ω) Ln i = ι	n	n
≤ 1≡ + r尸 +4Mt =: ψ(r).
r	nαr nr
Let’s pick the critical radius r0 to be:
ro = max{2","必}.
n αn
(B.61)
Note that concavity of the function φ implies that φ(r) ≤ r for any r ≥ r*. Combining this with the
first inequality listed in B.57 yields:
16φ(r)
r
211Φ(弱)
。14 r0 ―
2	214
1	φ( 2r04 )
8 X12Γ
≤
1
8
On the other hand, applying equation B.61 yields:

at
nαr0
α0 t αn	1
n V na 36α01	6
4Mt 4Mt	n _ 1
nro — n X 24Mt 6
Summing the three inequalities above implies:

ψ(r0)
16φ(ro)
ro
ta0
nαr0
4Mt
+-----
nr0
1111
+
≤
8 + 6 + 6 < 2.
By picking r = r0, we can further deduce that for any function U ∈ F(Ω), the following inequality
holds with probability 1 - e-t :
E(U)- E (u*) — En(U) + En (u*)
E (u) — E(u*) + ro
1n	1
n工 h(xi) ≤ ψ(ro) < 2.
n i=1
Multiplying the denominator on both sides indicates:
∆Egen = E(U)- E(u*) — En(U) + En(u*) ≤ | [e(u) — E(u*)] + 1 ro = 2∆E(n) + ∣r°.
Substituting the upper bound above into the decomposition ∆E(n) ≤ △Egen + 34Eapp + 2n yields
that with probability 1 — 2e- min{Cq,1}t, we have:
3	t1	1	3	t
△E	≤ △Egen	+ 2^Eapp	+ %	≤ 2^E	+ 2r0	+ 2^Eapp	+ 工∙
46
Published as a conference paper at ICLR 2022
Simplifying the inequality above yields that with probability 1 - 2e- min{Cq,1}t, we have:
∆E(n) ≤ r0 + 3∆Eapp + - =3 inf	(E(UF) - E(u*)∖ +max{214r*, 24M-t, 36α--} + -
n	UF ∈F (Ω)	n α n n
.UF 嚼⑸)(E(UF)-E(u?))+max {『*，n }∙
Moreover, using strong convexity of the DRM objective function proved in Theorem B.1 implies:
∆E(n) = E(UDRM)- E(u*) ≥ min{； VmijkUDRM - U*kHi(Ω).
Combining the two bounds above yields that with probability 1 - 2e- min{Cq,1}t, we have:
IlUDRM - u*kHi(Ω) .	infg)(E(UF) — E(u?)) + max {『*，}∙
□
Deep Neural Network Estimator. For any N ∈ Z+, there exists some Deep Neural Network in
Φ(L, W, S, B) with L = O(1), W = O(N), S = O(N), B = O(N), such that the approximation
error ∆Eapp =O(N- 2(Sd 1)) and generalization error ∆Egen = O(Nln N). With optimal selection
d	2s-2
N = n d+2s-2 to balance the bias and variance, We can achieve n- d+2s-2 log n convergence rate for
the DNN estimator.
Theorem B.10. (Final Upper Bound of DRM with Deep Neural Network Estimator) Under
the assumptions in Theorem B.9, we consider the Deep Ritz objective with the sparse Deep
Neural Network function space Φ(L, W, S, B), where the parameters L = O(1), W =
O(nd+2s-2), S = O(nd+2s-2), B = O(nd+2s-2). Then we have that the DNN estimator
U DRNM = mi□u∈Φ(L,w,s,B) EnRM (u) satisfies the following upper bound with high probability:
kUDNM -U*kH1 . n-d⅛⅛ logn.
Proof. On the one hand, by taking s = 1 and p = 2 in Theorem B.8 proved above, We have that
there exists some Deep Neural NetWork UDNN ∈ Φ(L, W, S, B) With L = O(1), W = O(N), S =
O(N), B = O(N), such that.
IlUDNN - u*∣∣Hi(Ω) ≤ N- -ɪ kU*kHS(。).
Applying strong convexity of the DRM objective function proved in Theorem B.1 yields the folloWing
upper bound on the approximation error of DRM:
∆Eapp . ∣UDNN - U*kH1(Ω) ≤ N - ɪ .
On the other hand, from Lemma B.17 proved above, We knoW that the function φ(ρ) that upper
bounds the local Rademacher complexity of the Deep Neural NetWork space is of the same magnitude
as yS3Lρ log (BWn).
radius r":
By plugging in the magnitudes of L, W, S, B , We can determine the critical
JBLS Iog(BWn) ` ʌ/H (2log N + log n) ` r* ⇒ r* ` N(IOg N + log n).
nn	n
Combining the tWo bounds above With Theorem B.9 yields that With high probability, We have:
kUDNM - U*kH1 . ∆Eapp + r* . N-0 + N(IogN+logn).
By equating the tWo terms above, We can solve for the optimal N that yields the desired bound:
AT- 2(S-I)	N	d 
N d ` 一 ⇒ N ` nd+2s-2.
Plugging in the optimal N gives us the magnitudes of the four parameters L = O(1), W
O(nd+2s-2), S = O(nd+2s-2), B = O(nd+2s-2), as well as the final rate:
kUDRM - U*kH1 . N -2(S-I + N log N . n-d+2(S-)1) log n.
□
47
Published as a conference paper at ICLR 2022
Truncated Fourier Series Estimator. For any ξ ∈ Z+, there exists some Truncated Fourier Series
in Fξ(Ω) with approximation error ∆Eapp = O(ξ-2(ST)) and generalization error ∆Ege∏ = O(ξ-).
1	2s-2
With optimal selection ξ = nd+2s-2 to balance the bias and variance, We can achieve n- d+2s-2
convergence rate for the Fourier estimator.
Theorem B.11. (Final Upper Bound of DRM with Truncated Fourier Series Estimator) Under
the assumptions in Theorem B.9, we consider the Deep Ritz objective with the Truncated Fourier
Series function space Fξ(Ω), where the parameter ξ = Θ(n d+2s-2). Then we have that the Fourier
estimator U DRRMfr = mi□u∈Fξ(Ω) EnRM (U) satisfies thefollowing upper bound with high probability:
kUDRT-u*kHι . n-d≡2.
Proof. Let’s firstly derive the function φ(ρ) that upper bounds the local Rademacher complexity of
Sρ(Ω) ：= {h ：= ∣Ω∣∙ h2 (kVu∣∣2 -∣∣Vu*k2) + 1V(|u|2 -|u*|2) - f (u - u*) ∣ U ∈ Fp,ξ(Ω)},
where Fρ,ξ(Ω) = {v ∈ Fξ(Ω) ∣ ∣∣vkH、⑺ ≤ ρ} denotes the localized Truncated Fourier Series
space. From Talagrand Contraction Lemma B.2, Lemma B.6 and Lemma B.7 proved above, we have
Rn(Sρ(Ω)) . Rn ({u - U* : U ∈ Fρ,ξ (Ω), ∣U - U*∣H1(Ω) ≤ √p})
+ Rn ({ IIVu - Vu*I : U ∈ Fρ,ξ (ω), kU - u*∣H 1(Ω) ≤ √p})
Eσ	sup
_	lu∈Fρ,ξ (Ω)
+ EX Eσ sup
lu∈Fρ,ξ (Ω)
n
—X σi(u(xi ) - πξ u*(xi))∣ku - πξ u*kH 1(Ω) ≤ ρl
n
i=1
n
n X σikVU(Xi)- vπξ u*(xi)k ∣ku - πξ u*kH1(Ω) ≤ ρ
n i=1
+ EX Eσ
n
[-Xσi∣VΠ>ξU*(Xi)k] + EX Eσ
n i=1
n
[- X σiπ>ξ u*(χi)]
n i=1
-n	∣
Eσ	SUp 一 X σiv(Xi) IkvkH1(Ω) ≤ ρ
_	l∙v∈Fρ,ξ(Ω) n M
+ EX Eσ
+ EX Eσ
sup
Lv∈Fρ,ξ(Ω)
n
h n X σik
i=1
n
-XσikVv(Xi)k ∣kvkH 1(Ω) ≤ ρ]
n i=1
V∏>ξ U* (Xi) k
n
+ EX Eσ [- X σiπ>ξU*(Xi)]
n i=1
.rρξ 2+-+ξ-2(ST),
nn
(B.62)
.rρξ 2+pπ>ξ u*kH1.
nn
where ΠξU := kzk ≤ξ Uzφz(x) is the projection to the Fourier basis whose frequency is smaller
than ξ and Π>ξU := Pkzk ≤ξ Uz φz (x) is the projection to the Fourier basis whose frequency is
larger than ξ. Then, the critical radius r* can be determined as follows:
∖匚ξd + - + ξ-2(ST) ` r* ⇒ r* ` ξd + - + ξ-2(ST).
n n	nn
Moreover, by taking α = s and β = -in Lemma B.19 and applying strong convexity of the DRM
objective function proved in Theorem B.1, we can upper bound the approximation error ∆Eapp as
below:
∆Eapp . ξ-2(S-1).
Combining the two bounds above with Theorem B.9 yields that with high probability, we have:
∣UDRMer - U*kHι . ∆Eapp + r* . ξd + - + ξ-2(ST).
48
Published as a conference paper at ICLR 2022
By equating two of the three terms above, we can solve for ξ that yields the desired bound:
ξd	1
—` ξ M I) ⇒ ξ ` nd+2s-2.
n
Plugging in the optimal ξ gives us the final rate:
Fourier	0,* 2 ‹ ξ ∣ 亡一2(s-1) ∣ 1 <	：22-2 2
11 udrm - U i∣hi . — +ξ	+ n . n d+2s-2.
□
B.4.2 Physics Informed Neural Network
Theorem B.12 (Meta-theorem for Upper Bounds of Physics Informed Neural Network). Let u* ∈
Hs(Ω) denote the true solution to the PDE model with Dirichlet boundary condition:
-∆u + Vu = f on Ω,
U = 0 on ∂Ω,
(B.63)
where f ∈ L2(Ω) and V ∈ L∞(Ω) with V 一 1 ∆V > Cmin,0 < Cmin < V(x) ≤ VmaX and
-∆V (x) ≤ Vmax. In Theorem B.2, it has been proved that u* can be obtained by minimizing the
loss E(u):
u* = arg min E(u) := arg min
u∈H0(Ω)
u∈H0 (Ω
I ∣∆u — Vu + f∣2dx}.
For a fixedfunction space F(Ω), consider the empirical loss induced by the Physics Informed Neural
Network:
n2
En(u) = n X [∣Ω∣∙回(Xj) 一 V(Xj)u(Xj) + f (Xj)) ] ,
(B.64)
j=1
where {Xj }n=ι are datapoints uniformly sampled from the domain Ω. Then the Physics Informed
Neural Network estimator associated Withfunction space F(Ω) is defined as the minimizer of En(U)
over the function space F (Ω):
UPINN = min En(u).
u∈F (Ω)
Moreover, we assume that there exists some constant C > 0 such that all function u in the function
space F(Ω), the real solution u* and f, V satisfy thefollowing two conditions.
• The gradients and function value are uniformly bounded
max SuP ∣u∣
u∈F (Ω)
L∞(Ω), SUP ∣∣VukL∞(Ω), SuP IPukLg(Q),
u∈F (Ω)
u∈F (Ω)
ku*∣L∞(Ω), kVu*∣L∞(Ω), ∣∆u* kL∞(Ω),Vmax, kf ∣L∞(Ω)} ≤ C
(B.65)
• All thefunctions in the function space F(Ω) satisfies the boundary Condition
u = 0 on ∂Ω.
At the the same time, for any ρ > 0, we assume the Rademacher complexity of a localized function
space Tρ(Ω) := {h := ∣Ω∣ ∙ [(∆u — Vu + f )2 — (∆u* — Vu* + f )2] ∣ ∣∣u — U
be upper bounded by a sub-root function φ = φ(ρ) : [0, ∞) → [0, ∞), i.e.
φ(4ρ) ≤ 2φ(ρ) and Rn(Tρ(Ω)) ≤ φ(ρ)(∀ ρ > 0).
* ∣2H2 ≤ ρ can
(B.66)
For all constant t > 0. We denote r* to be the solution of the fix point equation of local Rademacher
complexity r = φ(r). There exists two constants Cp, Cq such that with probability 1 — Cp exp(-Cq t),
we have the following upper bound for the Physics Informed Neural Network Estimator
∣∣uPINN - u*kH2 . u %Q)(E(UF)- E(u?)) + max {r*, g}∙
49
Published as a conference paper at ICLR 2022
Proof. To upper bound the excess risk ∆E(n), following(Xu, 2020; Lu et al., 2021b; Duan et al.,
2021), we decompose the excess risk into approximation error and generalization error with probabil-
ity 1 - e-Cqt, where Cq > 0 is some constant:
∆E(n)(Upinn) = E(UPINN)- E(u?) = [E(Upinn)- En(UPINN)] + [En (UpiNN ) — En(UF)]
+ [En(UF) - E(UF) + [E(UF)-E(U?)
≤ [E(Upinn) — En(UPInn)] + [En(uF) — E(uf)] + [E(uf) - E(u?)]
≤ [E(Upinn) — E(u*) + En(u*) - En(UPInn)]
+ 2 [E(UF)- E(U?)] + 2n,
(B.67)
where the expectation is on all sampled data. The inequality of the third line is because UPINN
is the minimizer of the empirical loss En in the solution set F(Ω), so We have En(UPINN) ≤
En(UF). The last inequality is based on the Bernstein inequality. The variance of h = ∣Ω∣ ∙
[(∆u - Vu + f )2 - (∆u* - Vu* + f )2] CanbebOUndedby [ [E(uf) - E(u?)] due to the strong
convexity of the variation objective (B.69). According to the Bernstein inequality, there exists some
constant Cq > 0, such that with probability 1 - e-Cqt we have:
En(uF) - En(u*) - E(uf) + E(u*) ≤ ʌ,[E(UF)- E(U*)] ≤ 1 [E(uf) - E(u?)] + ɪ.
n	2	2n
Note that C.5 holds for all function lies in the function space F. Thus, we can take UF :
argminuo∈F(。)(E(u0) - E(u?)) and finally get
△E(n) ≤ E(UPINN) - E(U*) + En(U*) - En(UPINN) +ʒ inf	(E(UF)- E(U?)) +ʒ- ∙
、	—V '"	J 2 UF∈F(Ω) ∖	2 2n
δ Egen	、	- {z	/
∆Eapp
This inequality decompose the excess risk to the generalization error ∆Ege∏ := E(UPINN) - E(u*) +
En(u*) - En(Upinn) and the approximation error ∆Eapp = infUF∈F(。)(E(UF) - E(u*)).
From the lemmata proved in Section B.3, we already have an estimation of the approximation error’s
convergence rate. So now we’ll focus on providing fast rate upper bounds of the generalization error
for the two estimators using the localization techinque(Bartlett et al., 2005; Xu, 2020). To achieve the
fast generalization bound, we focus on the following normalized empirical process
Tr(Ω) := {h(x) := EE].了 | h ∈ T(Ω)} (r > 0).
First, we try to bound the expectation of the normalized empirical process. Applying the Symmetriza-
tion Lemma B.1, we can first bound the expectation as
sup Ex0
r ~ , .
h ∈Tr (Ω)
n
i=1
≤ Ex0
ZJTpJnX」i
≤ 2Rn(Tr (Ω)).
where the function class Sr (Ω) is defined as:
Tr(ω) ：= {h(χ) ：= h(x^ | h ∈ τ(ω)},
E[h] + r
where T(Ω) = {h := ∣Ω∣∙ [(∆u - Vu + f )2 - (∆u* - Vu* + f )2] ∣. Then applying the Peeling
Lemma B.4 to any function h ∈ T(Ω) helps us upper bound the local Rademacher complexity
Rn(Tr (Ω)) with the function φ defined in equation B.66:
_ , ^ ,...
Rn(T (Ω))
Eσ Ex sup
L h∈T(Ω)
1 P=1 σih(Xi) i
E[h] + r ∖
≤ 4φ(r)
一 r
50
Published as a conference paper at ICLR 2022
Combining all inequalities derived above yields:
sup Ex0
h∈Tr(Ω)
1n
n X h(χi)
i=1
≤ 2Rn(Tr(Ω)) ≤ 8φ(r) (r> 0).
r
(B.68)
Secondly we’ll apply the Talagrand concentration inequality, which requires us to verify the condition
needed. We will first check that the expectation value E[h] is always non-negative for any h ∈ S(Ω):
E[h] = — J ∣Ω∣∙ (∆u - Vu + f)2dx — — J ∣Ω∣∙ (∆u* - Vu* + f )2dx
= E(u) - E(u?) ≥ 0⇒E[h] ≥ 0.
We will proceed to verify that any h =即；；∈ Tr (Ω) is of bounded inf-norm. We need to prove
that any h ∈ T(Ω) is of bounded inf-norm beforehand. Using boundedness condition listed in
equation B.65 implies:
khk∞ = ∣Ω∣∙k(∆u — Vu + f)2 - (∆u* - Vu* + f)2k∞ = ∣Ω∣∙ k(∆u - Vu + f)2k∞
≤ ∣Ω∣∙ (k∆uk∞ + VmaXkUk∞ + kf k∞)2 ≤ ∣Ω∣(Vmx + 2)2C2.
By taking M := ∣Ω∣(Vmax + 2)2C2, We then have Ilhl∣∞ ≤ M for all h ∈ T(Ω). Note that the
denominator can be lower bounded by |E[h] + r| ≥ r > 0. Combining these two inequalities help us
1	1,1	∙ c∙	IlT II	I T /	∖ I c∙ II
upper bound the inf-norm ∣∣h∣∞ = supχ∈Ω |h(x)| as follows:
∣∣7 llE[h] - hk∞
khk∞ =	|E[h] + r|
≤ 2⅛ ≤ 2M =: β.
We will then check the normalized functions EE]—+rx) in Tr (Ω) have bounded second moment,
which is satisfied because of the regularity results of the PDE. We aim to show that there exist some
constants α, α0 > 0, such that for any h ∈ T(Ω), the following inequality holds:
aE[h2] ≤ ∣∣u - u*kH2(Ω) ≤ α0E[h].	(B.69)
The RHS of the inequality follows from strong convexity of the PINN objective function proved in
Theorem B.2:
E[h] = E(U)- E(u*) ≥ min{1, CminMU - u*kH2(Ω).
The LHS of the inequality follows from boundedness condition listed in equation B.65 and the
QM-AM inequality:
E[h2] =	(∆u-Vu+f)2 - (∆u* -Vu* +f)22dx=	(∆u-Vu+f)4dx
≤ M2	(∆u -Vu -∆u* + V u*)2dx ≤ 2M2	[(∆u - ∆u*)2 +V2(u -u*)2]dx
≤ 2M2 maxUMaxMu - u*kH2(Ω)
By picking α0 = min{11Cmɪɪj and α = 2M2 maX{1,V2 }, we have finished proving inequality B.69.
Then we can can upper bound the expectation E[h2] as:
E[h 2]
E[(h - E[h])2]
|E[h] + r|2
E[h2] - E[h]2
|E[h] + r|2
≤	E[h2]
一|E[h] + r|2 .
Using the fact that E[h] ≥ 0 and inequality B.69, we can lower bound the denominator |E[h] + r|2 as
follows:
|E[h] + r|2 ≥ 2E[h]r ≥ ,E[h2].
51
Published as a conference paper at ICLR 2022
Therefore, we can deduce that:
E[h2] ≤
E[h2]
|E[h] + r|2 ≤ 2rαE[h2]
且=:σ2.
2rα
Hence, any function in the localized class Tr (Ω) is of bounded second moment.
It is easy to check that for any h ∈ Tr (Ω),we have
EM=EIy≡=0,
E[h] + r
i.e. any function in the localized class Sr (Ω) is of zero mean.
Now we have verified that any function h ∈ Sr (Ω) satisfies all the required conditions. By taking μ
to be the uniform distribution on the domain Ω and applying Talagrand,s Concentration inequality
given in Lemma B.3, we have:
1n
SuP	h(xi) ≥ 2 sup Ex
h∈Tr(Ω) n i=1	h∈Tr(Ω)
n
i=1
2tσ2	2t
nn
≤ e-t
By using the upper bound deduced above and plugging in the expressions of β and σ, we can rewrite
Talagrand’s Concentration Inequality in the following way. With probability at least 1 - e-t, the
inequality below holds:
1n	1n
—，h(xi) ≤ SuP —，h(xi)
n z—z	XLa n z—z
i=1	heSr(a) i=1
≤ 2 sup Ex
r ~ ,一、
h∈Sr (Ω)
≤ 16φ(r) + r
n h(χi)i+产+2nβ
i=1
ta0	4Mt , / 、
----1---=: ψ(r).
nαr
Let’s pick the critical radius r0 to be:
ro = max{2i4r*,"必}.
(B.70)
n αn
Note that concavity of the function φ implies that φ(r) ≤ r for any r ≥ r*. Combining this with the
first inequality listed in B.66 yields:
16φ(r) / 211φ(2⅛) _ 1、，φ(2⅛)
214214	= 8 X
r)
214
On the other hand, applying equation B.70 yields:
4Mt
4Mt
≤ ---
n
nr0
Summing the three inequalities above implies:
n
× ---
24Mt
1
6,
1
=—
6
Ψ(r0) = 3 + J
r0
tα0
nαr0
4Mt <
nr0
11
r
1
≤
1
8
1
8 + 6 + 6 < 2.
By picking r = r0, we can further deduce that for any function U ∈ F(Ω), the following inequality
holds with probability 1 - e-t :
E(U)- E(u*) - En(U) + En(u*)	1 XX 7(、v “ ʌ . 1
------E(U)- EQ*)+『0---------= n ⅛ h(xi) ≤ ψ(r°) < 2.
Multiplying the denominator on both sides indicates:
∆Egen = E(U)- E(u*) - En(U) + En(U*) ≤ 1 [e(u) - E(u*)] + 1 ro = 1∆E(n) + 1 r。.
2	22	2
52
Published as a conference paper at ICLR 2022
Substituting the upper bound above into the decomposition ∆E(n) ≤ ∆Egen + ∣ ∆Eapp + 2nn yields
that with probability 1 - 2e- min{1,Cq}t, we have:
3	t1	1	3	t
δe	≤	AEgen	+ 2δEaPP	+	2n	≤ 2δe	+	2r0	+ 2△EaPP + %.
Simplifying the inequality above yields that with probability 1 - 2e- min{1,Cq}t, we have:
△E(n) ≤ ro + 3∆Eapp + - =3 inf	(E(UF) - E(u?)) +max{214r*, 24M-t, 36α0 -} + -
n	UF ∈F (Ω)	n a n n
. inf	(E(UF) - E(Ur + max (r*,∙t].
UF ∈F (Ω)	nJ
Moreover, using strong convexity of the PINN objective function proved in Theorem B.2 implies:
δe(n) = e(UPINN)- E(U*) ≥ min{1, Cmin}kuPINN - u*∣∣H2(Ω).
Combining the two bounds above yields that with probability 1 - 2e- min{1,Cq}t, we have:
IIUPINN - U*kH2(Ω) .	inf。)(E(UF)- E(Un) + max {r*, n}.
□
Deep Neural Network Estimator. For any N ∈ Z+, there exists some Deep Neural Network in
Φ(L, W, S, B) with L = O(1), W = O(N), S = O(N), B = O(N), such that the approximation
error ∆Eapp = O(N- 2(Sd - ) and generalization error ∆Ege∏ = O(Nlθg N). With optimal selection
d	2s-4
N = n d+2s-4 to balance the bias and variance, We can achieve n d+2s-4 log n convergence rate for
PINN estimator.
Theorem B.13. (Final Upper Bound of PINN with Deep Neural Network Estimator) Under
the assumptions in Theorem B.12, we consider the PINN objective with the sparse Deep
Neural Network function space Φ(L, W, S, B), where the parameters L = O(1), W =
O(nd+2s-4), S = O(nd+2s-4) and B = O(nd+2s-4). Then we have that the DNN estimator
UPNN = mi□u∈Φ(L,w,s,B) EPNN(U) satisfies thefollowing upper bound with high probability:
kUDNNN - U*kH2 . n-" logn.
Proof. On the one hand, by taking s = 2 and p = 2 in Theorem B.8 proved above, we have that
there exists some Deep Neural Network uDNN ∈ Φ(L, W, S, B) with L = O(1), W = O(N), S =
O(N), B = O(N), such that.
11 uDNN - u*kH2(Ω) ≤ N一 ~-r~ kukHs(Ω).
Applying strong convexity of the PINN objective function proved in Theorem B.2 yields the following
upper bound on the approximation error of PINN:
∆Eapp . kuDNN - U*kH2(Ω) ≤ N-ɪ .
On the other hand, from lemma B.18 proved above, We knoW that the function φ(ρ) that upper bounds
the local Rademacher complexity of the Deep Neural Networks UDNN is of the same magnitude as
S3SnP log (BWn). By plugging in the magnitudes of L, W, S, B, we can determine the critical
radius r* :
Jr*3竺 Iog(BWn) `
n
(2log N + log n) ` r* ⇒ r* ` N(log N + log n).
n
Combining the two bounds above with Theorem B.12 yields that with high probability, we have:
IUDiNS - U*kH 2 . ∆Eapp + r* . N - T + N(Iog N+log n).
53
Published as a conference paper at ICLR 2022
By equating the two terms above, we can solve for the optimal N that yields the desired bound:
- 2(s-2)	N	_d_
N d ` 一 ⇒ N ' nd+2s-4 .
Plugging in the optimal N gives us the magnitudes of the four parameters L = O(1), W =
O(nd+2s-4), S = O(nd+2s-4), B = O(nd+2s-4), as well as the final rate:
lluDNN - u*kH2 . N-2(S-I + NlogN . n-d+2(S-)2) logn.
□
Truncated Fourier Series Estimator. For any ξ ∈ Z+, there exists some Truncated Fourier Series
in Fξ(Ω) with approximation error ∆Eapp = O(ξ-2(s-2)) and generalization error ∆Ege∏ = O(ξ-).
1	2s-4
With optimal selection ξ = nd+2s-4 to balance the bias and variance, we can achieve n- d+2s-4
convergence rate for the Fourier estimator.
Theorem B.14. (Final Upper Bound of PINN with Truncated Fourier Series Estimator) Under the
assumptions in Theorem B.12, we consider the PINN objective with the Truncated Fourier Series
function space Fξ(Ω), where the parameter ξ = Θ(nd+2s-4). Then we have that the Fourier
estimator U FoNNe = minu∈Fξ (ω) EnPNN (U) satisfies the following upper bound with high probability:
ku FNNer-u*kH2
2s-4
n- d + 2s-4

Proof. Let’s firstly derive the function φ(ρ) that upper bounds the local Rademacher complexity
of Tρ(Ω) := {h := ∣Ω∣ ∙ [(∆U - Vu + f)2 - (∆U - VU + f)2]	∣ U ∈ Jρ,ξ(Ω)}, where
JP,ξ (C) = nv ∈ Fξ(Ω) I kvkH2(Ω) ≤ ρ denotes the localized Truncated Fourier Series space.
From Talagrand Contraction Lemma B.2, Lemma B.6 and Lemma B.8 proved above, we have
Rn(TP(C)) . Rn ({u - u* : U ∈ Jρ,ξ(ω), kU - U*kH2(Ω) ≤ √p})
+ Rn ({δυ - △〃" : U ∈ Jρ,ξ(ω), kU - u* kH2(Ω) ≤ √p})
. EX Eσ
sup
u∈Fρ,ξ(Q)
+ EX Eσ	sup
l∙u∈Fρ,ξ(Ω)
1n	∣
n X σi (U(Xi)- πξ u*(Xi))IkU - πξ u*iiHi(ω) ≤ ρ
n i=1
n
n X	U(Xi)- ∆∏ξu* (Xi)) Iku - πξ u*kH1(Ω) ≤ ρ
n i=1
n
+ EX Eσ [- ^X biʌn^U*(Xi)]
n i=1
n
+ EX Eσ [- X σiπ>ξU"(Xi)]
n i=1
. EX Eσ
sup
v∈Fρ,ξ(Ω)
+ EX Eσ	sup
l∙ν∈Fρ,ξ(Ω)
1n
n χσiv(Xi)卜IvkH2(ω) ≤ ρ
i=1
n
—X3δV(Xi) ∣kvkH2(ω) ≤ ρ]
n i=1
n
+ EX Eσ [- ^X biʌn^U*(Xi)]
n i=1
n
+ EX Eσ [- X σiπ>ξU"(Xi)]
n i=1
kπ>ξ u*kH2
.rnξ ξ+，
n
(B.71)
where ΠξU := Pkzk ≤ξ Uzφz(x) is the projection to the Fourier basis whose frequency is smaller
than ξ and Π>ξU := Pkzk ≤ξ Uz φz (x) is the projection to the Fourier basis whose frequency is
54
Published as a conference paper at ICLR 2022
larger than ξ. Then, the critical radius r* can be determined as follows:
∖匚ξ2 + 1 + ξ-2(s-2) ` r* ⇒ r* ` ξd + 1 + ξ-2(s-2),
n n	nn
Moreover, by taking α = s and β = 1in Lemma B.19 and applying strong convexity of the DRM
objective function proved in Theorem B.2, we can upper bound the approximation error ∆Eapp as
below:
p. ξ-2(s-2).
Combining the two bounds above with Theorem B.12 yields that with high probability, we have:
kUPoNNer - u*kH2 . ∆Eapp + r* . ξd + 1 + ξ-2(s-2).
By equating two of the three terms above, we can solve for ξ that yields the desired bound:
ξd	1
一 ` ξ-2(s-2) ⇒ ξ ` nd+2s-4 .
n
Plugging in the optimal ξ gives us the final rate:
kuFoNNer - u*kH2 . ξd + ξ-2(s-2) +1 . n-d⅛三.
□
C Proof of Modified DRM
In this section, we provide the proof of the modified deep Ritz method here. We first provide a similar
meta-theorem as we did for DRM.
Theorem C.1 (Meta-theorem for Upper Bounds of Modified Deep Ritz Method). Let u* ∈ H s(Ω)
denote the true solution to the PDE model with Dirichlet boundary condition:
—∆u + Vu = f on Ω,
U = 0 on ∂Ω,
(C.1)
where f ∈ L2(Ω) and V ∈ L∞(Ω) with 0 < Vmin ≤ V(x) ≤ VmaX > 0. In Theorem B.1, it has
been proved that u* can be obtained by minimizing the loss E(u):
*
u*
arg min E(u) := arg min
u∈H0(Ω)	u∈H0(Ω)
n ɪ J h∣∣Vu∣∣2 + V |u|2i dx — / fudx}.
For a fixed function space F (Ω), consider the empirical loss induced by the Modified Deep Ritz
Method (N ≥ n):
1N 1	1n	1
ENn(u) = NN X [∣Ω∣∙ 2 kVu(X0)k2] + - X [∣Ω∣∙ CV(Xj )∣u(Xj )|2 - f (Xj )u(Xj))],
i=1
j=1
(C.2)
where {Xi0}iN=1 and {Xj}jn=1 are datapoints uniformly and independently sampled from the domain
Ω. Then the Modified Deep Ritz estimator associated with function space F(Ω) is defined as the
minimizer of ENn(U) over thefunction space F(Ω):
UMDRM = min)EN,n(u).
Moreover, we assume that there exists some constant C > 0 such that all function u in the function
space F(Ω), the real solution u* and f, V satisfy thefollowing two conditions.
•	The gradients and function value are uniformly bounded
max SUP kU∣L∞(Ω), SUP kVu∣∣L∞(Ω), ∣∣U*∣∣L∞(Ω), ∣∣Vu*∣∣L∞(Ω),Vmax,kf kL∞(Ω)} ≤ C.
u∈F (Ω)	u∈F (Ω)
(C.3)
55
Published as a conference paper at ICLR 2022
•	All thefunCtions in the function space F(Ω) satisfy the boundary condition
u = 0 on ∂Ω.
At the the same time, for any ρ > 0, we assume the Rademacher complexity of the following
vector-valued function space
SP(Ω):= {(hι, h2)∣h1 := ∣Ω∣∙ 1 (∣Nu∣∣2 -∣Nu*k2)],
h2 := |。卜 2V(IU|2 -|u*F)- f(U ―u*) , ku-u*kHι ≤ ρ}.
can be upper bounded by a sub-root function φ = φ(ρ) : [0, ∞) → [0, ∞), i.e.
φ(4ρ) ≤ 2φ(ρ) and R∣n(Sρ(Ω)) ≤ φ(ρ)(∀ ρ> 0),	(C.4)
where RN,n(S) := RN({h1|(h1, h2) ∈ S}) + Rn({h2 |(h1, h2) ∈ S}). For all constant t > 0. We
denote r* to be the solution ofthefixpoint equation Oflocal Rademacher complexity r = φ(r). There
exists two constants Cp, Cq such that with probability 1 - Cp exp(-Cq t), we have the following
upper bound for the Modified Deep Ritz Estimator
∣∣uMDRM — u*kHι
.UF ∈1f(Ω) (E(UF)- E(U*))+max {『* ； }.
Proof. To upper bound the excess risk ∆E(N,n) := E(UMDRM) - E(U*), following(Xu, 2020;
Lu et al., 2021b; Duan et al., 2021), we decompose the excess risk into approximation error and
generalization error with probability 1 - e-t :
∆E(N,n) = [E(Umdrm) - E(u?)] = [E(U1MDRM) - EN,n(UMDRM)[ + [ENn, (UMDRM) - EN,n(uF)]
+ EN,n (uF) - E(uF) + E(uF) - E(u?)
≤ [E(Umdrm) - En^(Umdrm)] + [Enn,(UF) - E(UF)] + [E(uf) - E(u?)]
≤ [E(Umdrm) - E(u*) + EN,n(u*) - En4(Umdrm)]]
+ 2 [E(UF) - E(U*)] +-∙ J N~~P
min{N, n}
(C.5)
where the expectation is on all sampled data. The inequality of the third line is because UMDRM is the
minimizer of the empirical loss En in the solution set F (Ω), so we have En,u (UMDRM) ≤ En,u(uf ).
The last inequality is based on the Bernstein inequality. For any UF ∈ F(Ω), we use hF,1, hF,2 to
denote the following two functions:
hF,1: = 2 (∣VUFk2 -∣Vu*k2),
hF ,2 : = 2 V (∣UF ∣2 - ∣U*∣2) - f (UF - U* ).
Applying Bernstein’s inequality twice to hF,1 and hF,2 implies that there exists some constant Cq,
such that with probability 1 - 2e-Cqt, the following two inequalities hold simultaneously:
EN(hF,1) - E(hF,1) ≤
En(hF,2) - E(hF,2) ≤
Note that the variance sum E[hF J + E[hF 2] can be upper bounded by α^ [E(uf) - E(u?)] due
to the strong convexity of the variation objective (C.9). Adding the two inequalities above implies
V-
s
t tα E[hF ,i]
N
t αο E[hF ,2]
n
56
Published as a conference paper at ICLR 2022
with probability 1 - 2e-Cqt we have:
EN,n(UF) — EN,n(u*) — E(UF) + E(u*) = EN (h，F ,1) — E(h,F ,i) + En(hp ,2) — E (h，F ,2)
≤ " +J
t 导 E[hF ,2]
n
≤
∖
2tm E[hF,ι]+ E[hF,ι]
min{N, n}
∕2t[E(UF) — E(u?)]
-V	min{N, n}
Note that C.5 holds for all function lies in the function space F.
arg minUF∈f(ω) (E(UF) — E(u?)) and finally get:
≤ [E(UF)- E(U?)] +min{N,n} .
Thus, we can take UF :=
∆E(N,n) ≤ E(UMDRM)- E(u*) + EN,n(u*) - En~(Umdrm) +2
^^{^^≡
∆Egen
inf
UF ∈F(Ω)
(E(UF)- E(u*)) +?.
一■―― ,
This inequality decomposes the excess risk to the generalization error ∆E
{
∆Eapp
,gen := E(UMDRM) —
|
}
E(u*) + EN,n(u*) — EN,n(Umdrm) and the approximation error ∆Eapp = infUF∈f(ω)
E(uF) -
E(U?) . From the lemmata proved in Section B.3, we already have an estimation of the approx-
imation error’s convergence rate. So now we’ll focus on providing fast rate upper bounds of the
generalization error for the two estimators using the localization techinque(Bartlett et al., 2005; Xu,
2020). To achieve the fast generalization bound, we focus on the following two normalized empirical
processes:
Sr,ι(Ω):= {hι(x):
Sr,2(Ω):= {h2(x):
E[hi] — hi(x)
E[hi] + E[h2] + r
E[h2] — h2(x)
E[hi]+ E[h2]+ r
| (hι,h2) ∈ S(Ω)} (r > 0),
| (hι,h2) ∈ S(Ω)} (r > 0).
where the space S(Ω) is defined as:
S(O) = n(hι, h2)∣hι := |。| ∙ 1 (kVuk
2-∣∣Vu*k2
h2 := ∣Ω∣ ∙
1V (∣u∣2-∣u*I2 ) — f(u — U*
),u ∈ F(Ω)}.
First, we try to bound the expectation of the two normalized empirical processes. Applying the
Symmetrization Lemma B.1, we can first bound the two expectations as:
sup	Ey0 h 1∈Sr,1 (Ω)	1N N X hι(yi)	≤ Ey0	sup ∣ ɪ X h1(yi)- E[h1] h1∈S1(Ω) ∣ N i=i E[h1] + E[h2] + r	∣∣∣	_ , ^. ,... ≤ 2Rn (Sr,l(Ω)),
sup	Ey h2∈sr,2(Ω)	1n -X h2(yj ) n j=1	≤ Ey	11 χΧ h2(yj ) - E[h2] ∣ h2∈s2(Ω) I n i=i E[h1]+ E[h2]+ r I		_	, O.	,... ≤ 2Rn(Sr,2(Ω)).
where the function classes Sr,k(Ω) (1 ≤ k ≤ 2) are defined as:
Sr,ι(Ω) := {hi(x) :
Sr,2(Ω) := {h2 (x) :
h1(x)
E[hi] + E[h2] + r
h2(x)
E[hi]+ E[h2]+ r
| (hι,h2) ∈ S(Ω)},
| (hι,h2) ∈ S(Ω)}.
57
Published as a conference paper at ICLR 2022
Applying the modified Peeling Lemma B.5 to any function h = (h1,h2) ∈ S(Ω) helps Us upper
1	F,F	C,F,	IlCF	F	1	∙ , ∙	T~ι / A	/ rʌ ∖ ∖	, ^ΓΛ / i~1	/ rʌ ∖ ∖	∙ ,1	,1
bound the sum of the two local Rademacher complexities RN(Sr,ι(Ω)) + Rn(Sr,2 (Ω)) With the
function φ defined in equation C.4:
RN Sjg)) +RnS,2M)= Eσ ,hh3Ω)	^ + ET
E 0 h sup n 乙j=1 Tj 2(yj) i
y h∈S(Ω)E[hι]+E[h2]+ r J
E	N∙ Ei=I σihι3)
y,y0 LfupΩ) E[hι]+E[h2]+ r
+	n Pn=I Tj h2 ⑼)
h∈S(Ω) E[h1] + E[h2] + r
RN,n(Sr (Ω)) ≤ 4φ(r) .
Combining all inequalities derived above yields:
sup Ey0
h 1∈Sr,1(Ω)
1N
N X i)
+ sup	Ey
h ∈Sr,2(Ω)
1n
n X h2(yj)
n
j=1
(C.6)
≤ 2Rn(Sr,ι(Ω)) + 2Rn(Sr,2(Ω)) = 2RNn(Sr(Ω)) ≤ ^rr (r > 0).
Secondly we,ll apply the Talagrand concentration inequality to the two function classes Sr,ι(Ω) and
Sr,2(Ω), which requires us to verify the conditions needed. We will first check that the expectation
sum E[hι] + E[h2] is always non-negative for any (hi, hQ ∈ S(Ω):
E[hi]+E[h2 ]=∣ω∣∙(2 ∣∣vu(χ)k2+2 V(X)Iu(X)12- f(x)u(X))dx
— ∣ωΩ∣ / ∣Ω∣∙ (2∣Vu?(x)k2 + 1V(x)∣u*(x)∣2 - f(x)u*(x))dx
= E(u) - E(u?) ≥ 0 ⇒ E[h1] + E[h2] ≥ 0.
ʌ T , -rɪ T	∙ 11	∙ i' ,1 , K /rʌ' , ∙ r`	11 ,1	∙	, κ . c~	∙ 11 F	,1 ,
Next, We will verify that Sr,i (Ω) satisfies all three requirements. At first, we will show that any
hi = EfE-h1+r ∈ Sr,ι(Ω) is of bounded inf-norm. We need to prove that any hi ∈ Sι(Ω) is of
bounded inf-norm beforehand. Using boundedness condition listed in equation C.3 implies:
khιk∞ = k 1 (∣Vuk2 -∣∣Vu*k2)k∞ ≤ 1 (∣∣Vuk∞ + ∣∣Vu*k∞) ≤ C2.
By taking Mi := C2, we then have ∣∣hι∣∣∞ ≤ Mi for all hi ∈ Sι(Ω). Note that the denominator of
hi can be lower bounded by ∣E[hi] + E[h2] + r∣ ≥ r > O. Combining these two inequalities help us
upper bound the inf-norm ∣∣hi∣∣∞ = supχ∈n ∣ hi (x)∣ as follows:
Ilh II = kE[hi] - hik∞	2khik∞
k ik∞ = ∣E[hi] + E[h2] + r∣ ≤
≤ ) βi.
A 1	∙ , ∙	,	1	1,1	. i'	7	—
Also, it is easy to check that for any hi ∈
Sr,i(Ω), we have
E[hi] — E[hi]
E[hi]+ E[h2]+ r
0,
一~ 一
E[h i]
♦	c∙ . ∙	∙	. ι 1	1 ∙ Ii 片 /rʌ' ∙ c∙
i.e. any function in the localized class Sr,i(Ω) is of zero mean.
Moreover, we take σ2 = SuPhl∈g,心)E[h2] to be the upper bound on the second moment of
functions in Sr,i(Ω). Now we have verified that any function hi ∈ Sr,i(Ω) satisfies all the required
conditions. By taking μ to be the uniform distribution on the domain Ω and applying Talagrand's
Concentration inequality given in Lemma B.3, we have:
1N	1N
- SUP	N)hi(x∙i) ≥ 2_ SUP	Ey 乐工及(协)
2tσi2	2tβ
+ ^N
≤ e-t
(C.7)
58
Published as a conference paper at ICLR 2022
U K	-t-r τ ∙11 ∙ 1' .Λ . K ∕r^x∖ 1	1 ∙ Γ~ 11 .Λ	∙	. k . r>	∙11 1 .Λ .
Moreover, We Will verify that Sr,2 (Ω) also satisfies all three requirements. At first, We Will show that
any h2 =旧裾+工谓十丁 ∈ Sr,2(Ω) is of bounded inf-norm. We need to prove that any h2 ∈ S2(Ω)
is of bounded inf-norm beforehand. Using boundedness condition listed in equation C.3 implies:
kh2k∞ = k 2 V(∣u∣2 *-∣u*∣2)- f(u-u*)k∞
≤ 1 %ax(k"k∞ + ku*k∞) + kf k∞(kuk∞ + ku*k∞)
≤ 2Vmax X 2C2 + 2C2 = (Vmx + 2)C2.
By taking M2 := (Vmax + 2)C2, we then have ∣∣h2∣∣∞ ≤ M2 for all h2 ∈ S2(Ω). Note that the
denominator of h2 can be loWer bounded by |E[h1] + E[h2] + r| ≥ r > 0. Combining these tWo
inequalities help us upper bound the inf-norm ∣∣h2k∞ = supχ∈Ω |h2(x)| as follows:
∣h k _ kE[h2] - h2k∞ ≤
kh2k∞ = ∣E[hι]+E[h2]+r ≤
2kh2k∞
≤ 2M2 =: β2.
r
A 1	∙ , ∙	,	1	1	, 1	. i'	T一片/ z-ʌ ∖	1
Also, it is easy to check that for any h2 ∈ Sr,2(Ω), we have
一~ 一
E[h 2]
E[h2]-E[h2]
E[h1] + E[h2] + r
0,
♦	c∙ . ∙	∙	. ι 1	1 ∙ Ii 片 /rʌ' ∙ c∙
i.e. any function in the localized class Sr,2(Ω) is of zero mean.
Moreover, we take σ2 = suph2∈g, ?(q)E[h2] to be the upper bound on the second moment of
functions in Sr,2(Ω). Now we have verified that any function h2 ∈ Sr,2(Ω) satisfies all the required
conditions. By taking μ to be the uniform distribution on the domain Ω and applying Talagrand's
Concentration inequality given in Lemma B.3, we have:
*	SUP	n Xh2(Xj)	≥2.	SUP	Ey0	h1X	h2(yj)i+ ∕2tF+2t
h2∈Sr,2 (Ω)	j = ι	h2∈Sr,2(Ω)	j = 1
≤ e-t
(C.8)
By applying a union bound to the two inequalities derived in C.7 and C.8, we can derive that with
probability at least 1 - 2e-t, the inequality below holds:
1N	1n
N X hι(χi) + n X h(χj)
1N	1n
0
≤ h 1∈SP(Ω) N 5h1 (Xi)+h 2∈SUP(Ω) njgh2(Xj)
..	hN XhI(Ji)] +
+ 2	SuP	EyO [— ^X h2(yj )i +
h2∈Sr,2(Ω)	Ln j=]
2tσ12 2tβ
ɪ + ^N
2tσ2 + 2tβ2
n
n
≤ X + ∖	(σι + σ2) + ≡1+囱
rn	n
By the definition of βι and β2, we have that the term 2*"/"2)can be upper bounded by:
2t(β1 + /2)= 4t(M1 + M2)≤ 4(Vmax + 3)C2t
n	nr	nr .
Now we will derive some upper bound on the sum σ1 + σ2 . By definition we have that:
(σ1 + σ2 )2 ≤ 2(σ12 + σ22 ) = 2 SuP	E[h] ] + _ SuP	E[h 2]i
lhl∈Sr,l(Ω)	h2∈Sr,2(Ω)
2
E[h2] — E[hι]2
h∈S(Ω) ∣E[h1]+ E[h2]+ r|2
+ SUP	E[h2] - E[h2]2	i
h∈S(Ω) ∣E[hι]+ E[h2]+ r|2 J
≤4 h∈spΩ) ∣E[Eh+ E[Eh+ 二.
59
Published as a conference paper at ICLR 2022
Now it suffices to derive an upper bound of 胆,,„平 for any h ∈ S(Ω). The existence of such
an upper bound is guaranteed because of the regularity results of the PDE. We aim to show that there
exist some constants α, α0 > 0, such that for any h ∈ S(Ω), the following inequality holds:
α(E[h2] + E[h2]) ≤ ∣∣u - u* kH 1(ω) ≤ α0(E[hl] + E[h2]).	(C.9)
The RHS of the inequality follows from strong convexity of the DRM objective function proved in
Theorem B.1:
E[hι] + E[h2] = E(U)- E(u*) ≥ mi,； Vmin}|u - u*kHi(。).
The LHS of the inequality follows from boundedness condition listed in equation C.3 and the QM-AM
inequality:
E[h2] + E[h2] = Jji (kVu∣∣2 -kVu*k2)2dx + L
1V(∣u∣2-∣u*∣2)-f(u -u*)
2
dx
≤ 1/ (kVu∣2 -∣Vu*k2)2dx + 1 / V2(|u|2 -∣u*∣2)2dx + 2/ f2(u - u*)2dx
≤ 1 ^∣kVuk-kVu*k∣2(∣Vuk + ∣Vu*k)2dx +1 Vmax 仆u|-|u*『(|u| + ∣u*∣)2dx
+2C2	(u -u*)2dx ≤ C2	∣Vu - Vu*∣2dx+2C2(1+Vm2ax)	|u -u*|2dx
≤ 2C 2(1 + VmaX) ku - u*kH1(Ω).
By picking α0 = ma{4 v . } and α = 2c2q+v2), we have finished proving inequality C.9. Then we
can can upper bound the term ∣e"[：++'[；+：12 as:
E[h2] + E[h2]	α0 (E[hι]+E[h2])	α0
≤	≤.
|E[hi] + E[h2] + r |2	2r (E[h1] + E[h2])	2αr
Combining the bounds derived above helps us
upper bound the term 2^(σ1+σ2)
as below:
2t	回 I	E[h1 ] + E[hJ]	J4α0t
n (σι+σ2) ≤ V n Vh∈sUpΩ) WWEWΨ ≤ V nαr
Thus, using the two upper bounds on y^2t(σ1 + σ2) and 24"/西), we have
1N	1n
N X hι(χi) + n X h(χj) ≤
X+，笠	+ σ2) +组4
rn	n
16φ(r)
r
≤
J 4α0t	4(VmaX + 3)C 2t
+ V nαr + —n— = ψ(r∙
Let’s pick the critical radius r0 to be:
r0 = max{214r*
24Mt 144ɑ01
n , αn
}.
(C.10)
Note that concavity of the function φ implies that φ(r) ≤ r for any r ≥ r*. Combining this with the
first inequality listed in C.4 yields:
I6φ(r0)
ro
“ 211 Φ(患)_ 1
≤ ------———
≤ 214弱 8
2⅛^
×
Φ翁)
≤ 1.
_ 8
60
Published as a conference paper at ICLR 2022
On the other hand, applying equation C.10 yields:
14α0t	∕4a0t an~	1
n nar= - V na 144a0t	6,
4(Vmax+3)C2t
4(Vmax+3)C2t
nr0
X ——-------：———=—.
24(VmaX + 3)C 2t	6
≤
n
1
n
Summing the three inequalities above implies:
Ψ(ro ) = 3 +J
r0
4α0t + 4(Vmax + 3)C 2t
nαr0
nr0
≤1 + 1 + 1< 1.
一 8	6	6	2
By picking r = r0, We can further deduce that for any function U ∈ F(Ω), the following inequality
holds with probability 1 - 2e-t:
E(U)- E(u*) + ENW*) - ENnU) = 1 XX h(xi) ≤ ψ(ro) < 1.
E(u) - E(u*) + ro	n 白 ' i ' 0	2
Multiplying the denominator on both sides indicates:
δEgen = E(U)- E(u*) + EN,n(u*) - EN,n(U) ≤ ʒ [E(U)- E(UR + rr = = ^E^(n(n) + -r0.
2	22	2
Substituting the upper bound above into the decomposition ∆E(n) ≤ ∆Egen + 2∆Eapp + 4t yields
that with probability 1 - 4e- min{1,Cq }t, we have:
△E(n) ≤ AEgen + 24Eapp + % ≤ 24E(n) + 2r= + 2AEpp + —.
Simplifying the inequality above yields that with probability 1 - 4e- min{1,Cq}t, we have:
△E(n) ≤ r= + 4∆Eapp + 8t = 3 inf	(E(UF) - E(u*)∖ +max{214r*, 24ML 36α0-} + 竺
n	UF ∈f (Ω)	n an n
.inf	(E(UF) — E(u?)) + max (r*,∙t].
UF ∈F (Ω)	nJ
Moreover, using strong convexity of the DRM objective function proved in Theorem B.1 implies:
△E(n) = E(UMDRM)- E(u*) & IIUMDRM - U*kHi(Ω).
Combining the two bounds above yields that with probability 1 - 4e- min{1,Cq}t, we have:
IlUMDRM - u*kHi(Ω) . inf^)(e(uf) - E(u?)) + max {r*, n}.
□
Truncated Fourier Series Estimator. Next we aim to show that the truncated Fourier series
estimator can achieve the min-max optimal rate using the MDRM objective function. For any
ξ ∈ Z+ satisfying ξ2 < N, there exists some Truncated Fourier Series in Fξ(Ω) with approximation
error ∆Eapp = O(ξ-2(ST)) and generalization error ∆Egen = O(ξ-)
1	2 s — 2
. With optimal selection
convergence rate for the
nd+2s-4 to balance the bias and variance, we can achieve n- d+2s-4
ξ
Fourier estimator.
Theorem C.2. (Final Upper Bound of MDRM with Truncated Fourier Series Estimator) Under
the assumptions in Theorem C.1, we consider the Modified Deep Ritz objective with the Truncated
Fourier Series function space Fξ (Ω), where the parameter ξ = Θ(n d+2s-2). By assuming N < ξ-n-,
we have that the Fourier estimator UMDRM = minu∈Fξ (ω) ENlRM(u) satisfies the following upper
bound with high probability:
ku MiriM -u*∣∣Hι. n- -⅛-4
61
Published as a conference paper at ICLR 2022
Proof. Following the same proof as shown for the DRM upper bound in Theorem B.11, we firstly
need to determine the critical radius r":
+ & ξ2 +1 + ∣+ ξ-2(s-1)
N nN
` r*.
For we have assumed N < ξn~, the solution of the fixed point equation is r* ` ξn~ +1 + ξ-2(s-1).
On the other hand, by taking α = s and β = 1in Lemma B.19 and applying strong convexity of the
DRM objective function proved in Theorem B.1, we can upper bound the approximation error ∆Eapp
as below:
∆Eapp .ξ -2(s-1) .
Combining the two bounds above with Theorem C.1 yields that with high probability, we have:
ξd-2	1
kuMDRM -u*kHι . ∆Eapp + r* . ^n + - + ξ-2(s-1).
By equating the two of the three terms above, we can solve forξ that yields the desired bound:
ξ d-2	1
-----` ξ-2(S-I) ⇒ ξ ` — d+2s-4 .
-
Plugging in the expression ofξ gives the final upper bound:
kuMDRM -u*kHι . ξd-2 + ξ-2(s-2) +1 .-—谣言.
--
□
D Proof of the Lower Bounds
D.1 Preliminaries on Tools for Lower Bounds
In this section, we list the standard tools we use to establish the lower bound. The main tool we use is
the Fano’s inequailty and the Varshamov-Gilber Lemma.
Lemma D.1 (Fano’s methods). Assume that V is a unifrom random variable over set V, then for any
markov chain V → X → V , we always have:
.^ .
P(V = V) ≥ 1 -
I(V; X )+log2
IOg(IVD
Lemma D.2 (Varshamov-Gillbert Lemma,(Tsybakov, 2008) Theorem 2.9). Let D ≥ 8. There
exists a subset V = {τ(O), ∙∙∙ ,τ(2D/8)} of D-dimensional hypercube HD = {0, 1}d such that
T(O) = (0,0, •一,0) and the 'ι distance between every two elements is larger than DD:
DD
X ∣∣τ(j) - T (k)k`i ≥ - ,for all 0 ≤ j,k ≤ 2D/ .
l=1
D.2 Proof Of Lower Bound
In this section, we provide the proof of the lower bound for learning a PDE. Our proof uses standard
Fano method to establish minimax lower bound but finally leads to a non-standard convergence rate.
We state standard results for Fano methods in Appendix D.1. Following is the proof our main lower
bound.
Theorem D.1 (Lower bound). We denote u*(f) to be the solution of the PDE 2.1 and we can access
randomly SamPled data {Xi, Yi}i=ι,∙∙∙ ,n as described in Section 2.2.
DRM Lower Bound. For all estimators ψ : (RdLn X R0n → Hs(Ω), we have
2s-2
inf SUp Ekψ({χi,fi}i=ι,∙∙In)-Uf )∣Hι & --d+2s^.	(D.1)
ψ u*∈Hs(Ω)	` /
62
Published as a conference paper at ICLR 2022
PINN Lower Bound. For all estimators ψ : (RdLn X R0n → HS (Ω), we have
infu*WΩ)E"ψ({XifiJ…,nif )kH & n-d+--4.	(D2
Proof. We construct the following bump function to construct the multiple hypothesis test used for
proving the lower bound. Consider a simple C∞ bump function supported on [0, 1]d
d
g(x) = ∏ξ(xi),x = (xι,…，Xd),
i=1
where ξ : R → R is a non-zero funtion in C∞ (R) with support contained in [0, 1] and satisfies
ξ(x) = 0, dXξ(x) = 0. Then Vg(x) = 0 and the support of function g is [0,1]d.
Next, We take m = [n 2s-14+d ] and consider a regular gird x(j) ,j ∈ [m]d. According to the Varshamov-
Gilbert lemma, there exist 2m”/ (0,1)-sequences T⑴，…，τ(2m /8) ∈ {0,1}md suchthat ∣∣τ(k) 一
T(k0) ∣∣2 ≥ % for all 0 < k = k0 ≤ 2m /8. Then we construct the multiple hypothesis as
Uk(X)= X Tjkk	ωdg(m(X-Xj))),k = 1,2,∙∙∙,2md/8,
j∈[m]d	ms+ 2
where ωis a constant to be determined later. It is easy to find out that uk ∈ Cs .
Then we reduce solving the PDE to a multiple hypothesis testing problem, which considers all
mappings from n sampled data to the constructed hypothesis Ψ : (RdLn × R0n → V := {ui∣i =
1,2,…，2md∕8}. Then we apply the local Fano method and check that we can obtain a constant
lower bound ofP(V 6= V ) for any estimator V . From the local Fano method, we know that
I(V; X) ≤ 7V72 X X DKL(Pv ||Pv),
z v6=v0
where Pk denotes the joint distribution of the sampled data (X, y). In specific, X follows a uniform
distribution on [0, 1]d and y =f(X) + , where is independently sampled from a standard Gaussian
distribution N(0, 1). Then we have
KL(Pk||Pk0) = Elog(箸)=∣∆uk + VukilL2 ≤ mCω4.
Using Fano inequality, if we select m a [n 2s-14+d ] then we have the following lower bound when ω
is taken to be sufficiently large:
P(V = V) ≥ 1 ― I(V; X) +log2 ≥ 1 ——m2s-4 ≥ 1/2
P(V= V ) ≥ 1 log(∣V∣)	≥ 1 mdlog2 ≥ 1
At the same time, we can estimate the separation of the hypotheses in two different norms:
• Deep Ritz Method:
∣Vuk-Vuk0∣2dX
[0,1]d
K2
m2s-2+d
X ∣Tj(k)-Tj(k0)∣1
j∈[m]d
∣Vg(X)∣2dX &
Rd
1
m2s-2 .
• Physic Informed Neural Network:
∣∆uk - ∆uk0 ∣2dX
[0,1]d
κ2
m2s-4+d
X ∣Tj(k) - Tj(k0) ∣1	∆g(X)2dX &
j∈[m]d	Rd
1
m2s-4 .
63
Published as a conference paper at ICLR 2022
Plugging in m 8 [n 2s-4+d ], We know that with constant probability We have
inf	SUp	Ekψ({Xi,γi}i=1,∙∙∙	,n)	- u*(f )kHι & n-d+s--4,	(D.3)
ψ u*∈Hs(Ω)	' /
inf	SUp	Ekψ({χi,γi}i=ι,∙∙∙	,n)	- u*(f)kH2 & n-d⅛s-4.	(D.4)
ψ u*∈Hs(Ω)	' /
□
E Intuition B ehind the Sub -optimality of the Unmodified Deep
Ritz Methods
In this section, we aim to discuss the intuition behind the sub-optimality of the unmodified DRM via
using the truncation Fourier basis. To simplify the notation, in this section we consider the following
simplest Poisson equation ∆u = f on the hypercube with zero Dirichlet boundary condition. To
illustrate the necessity of the modification we made, we consider the difference between the following
two estimators
•	Estimator 1. We use the truncated Fourier basis estimator to learn the right hand side
function f and then we invert the PDE exactly to get the estimated u.
•	Estimator 2. We plug in a parametrization of the truncated fourier basis into the empirical
DRM objective
We would like to point out that estimator 1 isn’t build for computational consideration. Instead, we
use it to consider the statistical limit of our sampled data. We first show that the estimator 1 can
achieve the minimax optimal estimation error.
Error Of Estimator 1 Firstly, we show that if one wants to learn the function u in H01 norm, one
need to learn the right hand side function f in H0-1 norm. The H0-1 norm is defined as the dual norm
of the H1 norm, i.e. kukH-1 = maxkvk 1≤1 hu, vi. Once we assume we have an estimate f of f in
-1
Ho 1, We can have an estimate of U Via U := (∆)	f, whose distance to U in the H1 norm satisfies:
∣∣Vu - VUkH 1 = 口 maχ JVu - VU, Rv
max (△〃 一 ∆U, Vi
kvkH01 ≤1
ll max 1 Df — f,vE = kf — f∣H-ι.
kvkH1 ≤1
Estimator 1 using the truncated Fourier estimator to estimate the right hand side function f. Suppose
we can access a random sample of observed data as {xi, f(xi)}in=1, then the Fourier coefficient
fz :=(u, φzi can be estimated as fz := 1 P21 f (χi)φz(χi). To bound the estimation error of
f := kzk ≤Z fzφz in H0-1, we first apply the bias-variance decomposition:
Ek/ - f kH-1 ≤ ∣Ef - f∣∣H-1 + Ekf - EfkH-1
H0	H0	H0
We first bound the bias term ∣∣E∕ - f kH_「Given Ef = Pkzkco≤z fzφz, we have that for a
truncation set Z of the from Z := {z ∈ Nd∣kzk∞ ≤ Z}, the bias term can be controlled by:
k	fzφzk2H-1 ≤C	fz2z-2 ≤ kzk-2(s-1)kfk2Hα-2
kzk∞>Z
kzk∞>Z
64
Published as a conference paper at ICLR 2022
Next we estimate the variance of the estimator by decomposing the variance into the following sum:
Ekf - fkH-1 ≤ E X (fz- fz )2kΦzkH-1 ≤ X ∣z∣-1Var(fz).
kzk∞≤Z	kzk∞≤Z
Finally We achieve a Z-2(ST) + Zd-2 upper bound for estimator 1. With optimal selection of Z, We
2s-2
can achieve the min-max optimal convergence rate n

d+2s-4
Difference Between Estimator 1 and Estimator 2 Next we aim to understand the Deep Ritz
Method objective function via plugging in a truncated Fourier series estimator. We consider an
estimator of the form u = P Uzφz (x), which lies in the space of truncated Fourier series. Then the
empirical DRM objective function can be expressed as
2n X(X Uz Vφz(xj) + X Uzφz(xi)f(xi).
(E.1)
We observe that (E.1) is a quadratic formula With respect to the Fourier coefficients u := (uz)kzk∞≤Z.
Thus, We can reWrite it as the folloWing matrix form
min ；u>Au + u> f, where A = (1 X Vφi(xi)Vφj (Xi))	.	(E.2)
Based on the matrix formulation E.2, we can compare the solution given by the two estimators
•	Estimator 1: The Fourier coefficients of the solution of Estimator 1 are
uι= diag (kzk2)-z1l∞≤z f.	(E.3)
•	Estimator 2: The Fourier coefficients of the solution of Estimator 2 are
.	^	-1 ^
u 2 = AT f.	(E.4)
Note that EA = (Ilzk2)隧口 ≤z. Thus, we can further introduce another variance from the sampling
of A. By directly estimating u 1 一 u2, we will show that this term will be larger than the final
convergence rate. Notice that
kuι- u2kH 1 = f> ((EA)T- A-1)> diag (kzk2)∣z∣∞≤z ((EA)T- A-1)f (E.5)
Next we aim to bound ((EA)T - A-1). We first use the Matrix Bernstein IneqUaIity(Tropp, 2015)
to bound the H1 distance between uι and u2. According to the Matrix Bernstein Inequality, we have
that with probability 1 - e-t, the following inequality holds
Il((EA)- A)IIH ≤ E+n
(E.6)
where k ∙ ∣h is the matrix operator norm respect to the vector ∣∣ ∙ ∣h defined as kzkH =
z>diag(kzk2)-zi∞≤z z.Note that
(I +(EA)T (A - (EA))) ((EA)T - j^-1) =(EA)-I (A - (EA)) (EA)T	(E.7)
When n is large enough, we know that 21 6 I + (EA)T (A - (EA)) 6 I with high probability.
Thus the term ∣∣Uι-Ut2kHι is atthe scale of ∣∣((EA) - A) ∣∣	≈ Zd, which is ofthe same magnitude
d-2
as what we get from the empirical process approach in our main proof. It is also larger than Zn—,
which is the magnitude of the variance term for Ui. Therefore, here we conjecture that the our bound
for DRM itself is tight and leads to the sub-optimal convergence rate.
65