Published as a conference paper at ICLR 2022
Is Importance Weighting Incompatible with In-
terpolating Classifiers ?
Ke Alexander Wang*, Niladri S. Chatterji*, Saminul Haque, Tatsunori Hashimoto
Department of Computer Science
Stanford University
{alxwang,niladri}@cs.stanford.edu,{saminulh,thashim}@stanford.edu
Ab stract
Importance weighting is a classic technique to handle distribution shifts. How-
ever, prior work has presented strong empirical and theoretical evidence demon-
strating that importance weights can have little to no effect on overparameterized
neural networks. Is importance weighting truly incompatible with the training of
overparameterized neural networks? Our paper answers this in the negative. We
show that importance weighting fails not because of the overparameterization, but
instead, as a result of using exponentially-tailed losses like the logistic or cross-
entropy loss. As a remedy, we show that polynomially-tailed losses restore the
effects of importance reweighting in correcting distribution shift in overparame-
terized models. We characterize the behavior of gradient descent on importance
weighted polynomially-tailed losses with overparameterized linear models, and
theoretically demonstrate the advantage of using polynomially-tailed losses in a
label shift setting. Surprisingly, our theory shows that using weights that are ob-
tained by exponentiating the classical unbiased importance weights can improve
performance. Finally, we demonstrate the practical value of our analysis with
neural network experiments on a subpopulation shift and a label shift dataset.
When reweighted, our loss function can outperform reweighted cross-entropy by
as much as 9% in test accuracy. Our loss function also gives test accuracies com-
parable to, or even exceeding, well-tuned state-of-the-art methods for correcting
distribution shifts.
1	Introduction
Machine learning models are often evaluated on test data which differs from the data that they were
trained on. A classic statistical technique to combat such distribution shift is to importance weight
the loss function during training (Shimodaira, 2000). This procedure upweights points in the training
data that are more likely to appear in the test data and downweights ones that are less likely. The
reweighted training loss is an unbiased estimator of the test loss and can be minimized by standard
algorithms, resulting in a simple and general procedure to address distribution shift.
Surprisingly, recent papers (Byrd & Lipton, 2019; Xu et al., 2020) have found that importance
weighting is ineffective in the current deep learning paradigm, where overparameterized models in-
terpolate the training data or have vanishingly small train loss. In particular, Byrd & Lipton (2019)
empirically showed that when no regularization is used, overparameterized linear and nonlinear
models trained with the importance weighted cross-entropy loss ignore the importance weights. Xu
et al. (2020) followed up and provided a theoretical justification for this observation in overparame-
terized linear and non-linear models.
To build intuition about why importance weighting fails, consider linear classifiers as an exam-
ple. Given linearly separable data (x1, y1), . . . , (xn, yn) ∈ Rd × {-1, 1}, Soudry et al. (2018)
showed that if gradient descent is applied to minimize an exponentially-tailed classification loss
(Pi∈[n] 'eχp(yiXi)) then the iterates converge in direction to the maximum margin classifier Θmm :=
argmi□kθk = ι {γ : y%Xi ∙ θ ≥ γ, for all i ∈ [n]}. XU et al. (2020) showed that in this same setting,
minimizing the importance weighted loss (Pi∈[n] wi'exp(yiXi)) with gradient descent also results
* Equal contribution.
1
Published as a conference paper at ICLR 2022
Cross Entropy: No IW
Cross Entropy: IW Poly-tailed Loss: No IW Poly-tailed Loss: IW
Predicted Majority
Predicted Minority
o	Majority Class
∆	Minority Class
Learned Boundary
TrUe Boundary
Figure 1: Models trained with gradient descent, with and without importance weights (IW), in
the label shift setting where classes are imbalanced in the training set. All models interpolate the
training points with 100% accuracy. (Left) Importance weights provably fails to correct for the
distribution shift for the cross-entropy loss. The learned boundary is asymptotically the maximum-
margin classifier even with reweighting. (Right) Our polynomially-tailed loss restores the effects of
importance weights, correctly adjusting for the distribution shift.
in convergence to the maximum margin classifier, regardless of the weights. To see why, consider
the special case where the weights (wι,..., Wn) are positive integers. This reweighting is equiva-
lent to simply repeating each datapoint Wi times, and the maximum margin classifier over this “new
dataset” remains unchanged. Thus, invoking the original result by Soudry et al. (2018) proves that
the importance weights has no effect in correcting the distribution shift. This result can be seen in
Figure 1 where we demonstrate this phenomenon in a simple toy problem.
Such evidence has led some to wonder if importance weighting is fundamentally incompatible with
overparameterized interpolating models. In this paper, we show that this is not the case. We find
that the culprit behind the ineffectiveness of importance weighting is the exponential tail of popular
losses such as the cross-entropy or the logistic. We propose altering the structure of the loss to have
fatter, polynomially decaying tails instead. We theoretically and empirically show that importance
weights do correct for distribution shift under such losses even for overparameterized classifiers.
Our first contribution is to characterize the limiting direction of the iterates of gradient descent
(its implicit bias) when minimizing reweighted polynomially-tailed losses with linear classifiers.
We show that this limiting direction is a function of both the datapoints as well as the importance
weights, unlike the maximum margin classifier that only depends on the data (see the right half of
Figure 1). Next, we analyze the generalization behavior of this classifier in a label shift setting. We
prove that when the weights are an exponentiation of the unbiased importance weights, the test error
decays to zero in the large sample limit, regardless of the level of imbalance in the data. In contrast,
we prove that the maximum margin classifier test error in this same setting must be at least 1/8.
Finally, we demonstrate the practical benefits of our framework by applying this approach to ex-
periments with neural networks. In both a label shift dataset (imbalanced binary CIFAR10), and a
subpopulation shift dataset with spurious correlations (CelebA (Sagawa et al., 2019)), we find that
reweighting polynomially-tailed losses consistently outperforms reweighted cross-entropy loss, as
our linear theory suggests. Additionally, poly-tailed loss with biased importance weights can per-
form comparably to, or better than, state-of-the-art methods distribution shift (Cao et al., 2019; Ye
et al., 2020; Menon et al., 2020; Kini et al., 2021)1.
2	Related Work
Early work (Shimodaira, 2000; Wen et al., 2014) already warned against the potential ineffective-
ness of importance weights on interpolating overparameterized models. Shimodaira (2000) showed
that when the model is well-specified, importance weights can fail to have an effect, and that the
ordinary maximum likelihood estimate is asymptotically optimal. Wen et al. (2014) showed that
when there is a zero-loss minimizer of an unweighted convex loss minimization problem, then it is
also a minimizer of the (adversarially) reweighted loss as well. Recent work (Byrd & Lipton, 2019;
Xu et al., 2020) has shown that importance weighting fails to have an effect on neural networks
1Code is available at https://github.com/KeAWang/importance-weighting-interpolating-classifiers
2
Published as a conference paper at ICLR 2022
trained with gradient descent, though always in the setting of exponentially-tailed losses. Sagawa
et al. (2019) demonstrated that reweighting can fail to have the desired effect when unregularized
distributionally robust optimization (DRO) methods are used in conjunction with the cross-entropy
loss. They empirically showed that regularization is necessary to reap the benefits of reweighting,
also observed by Byrd & Lipton (2019).
A recent line of work (Cao et al., 2019; Ye et al., 2020; Menon et al., 2020; Kini et al., 2021) has
introduced modifications to the logistic and cross-entropy losses to correct for distribution shift. Cao
et al. (2019) and Menon et al. (2020) proposed using additive corrections to the logits. However,
without regularization or early-stopping these corrections are ineffective since the additive correc-
tions to the logits is analogous to importance weighting exponential-tailed losses. Multiplicative
logit corrections (Ye et al., 2020), possibly combined with additive corrections (Kini et al., 2021),
have also been proposed. Unlike the additive corrections, these methods do not converge to the
max-margin classifier, but they also do not correspond to importance weighting algorithms. In our
work, we focus on the question of whether importance weighting alone can correct for distribution
shift, and show practical empirical benefits relative to multiplicative logit corrections.
Our work also connects to literature that has studied the implicit bias of gradient descent (Soudry
et al., 2018; Ji & Telgarsky, 2019; Nacson et al., 2019). Especially relevant is the work by Ji et al.
(2020) who relate the implicit bias of gradient descent with exponentially and polynomially-tailed
losses for linear classifiers to a solution of a regularized loss minimization problem. Finally, our
generalization analysis draws from the growing literature focused on finite sample bounds on the
test error of the maximum margin classifier in the overparameterized regime (Chatterji & Long,
2021; Muthukumar et al., 2021; Wang & Thrampoulidis, 2021; Cao et al., 2021).
3	Setting
We consider a distribution shift setting where the training samples {(x1, y1), . . . , (xn, yn)} ∈ Rd ×
{-1, 1} are drawn i.i.d. from Ptrain, and the test samples are drawn from a different distribution
Ptest that is absolutely continuous with respect to Ptrain. Let fθ denote a classifier parameterized by
θ. Given a feature x, a classifier maps this feature to fθ (x) ∈ R. In this paper we shall consider
cases where the classifier is either linear (for our theory) or a neural network (for our experiments).
Our goal is to find a classifier fθ that minimizes the 0-1 loss with respect to the test distribution:
TestError[fθ] = P(#,y)〜* [sign(fθ (x)) = y].
To handle the mismatch between Ptrain and Ptest, we shall study importance weighting algorithms.
Given a datapoint (x, y) ∈ Rd × {-1, 1}, the classical unbiased importance weight at (x, y) is given
by the ratio of densities between the test and the train distributions
PT"(；,：). Using these unbiased
importance weights ensures that the reweighted training loss is an unbiased estimate of the test loss.
However, as noted above, past work has shown that interpolating classifiers trained with gradient
descent on importance weighted exponentially-tailed losses (such as the logistic loss `log(z) :=
log (1 + exp(-z)), the exponential loss 'exp(z) := exp(-z), and the cross-entropy loss) ignore the
importance weights. For example, consider the case when the classifier is linear fθ (x) = X ∙ θ, the
weights are wι,...,wn > 0, and the reweighted loss function is L(θ) = PZi Wi'log(yiXi ∙ θ).
Xu et al. (2020) showed that if the data is linearly separable then the iterates of gradient descent
converge in direction to the '2-maximum margin classifier,
Θmm := arg max {γ : yiXi ∙ θ ≥ Y for all i ∈ [n]} .	(1)
θ"∣θ2k=1
Observe that the maximum margin classifier does not depend on the importance weights
(w1, . . . , wn) and hence may suffer large test error when there is distribution shift. Xu et al. (2020)
further showed that when separability assumptions hold, non-linear classifiers trained with gradient
descent on exponentially-tailed losses are also unaffected by importance weights.
We initiate a study of polynomially-tailed losses in the distribution shift setting and show that they
have improved behavior with respect to importance weighting even when the model is overparame-
terized. Given parameters α > 0 and β ∈ R define the polynomially-tailed loss as follows:
`left(z)	ifz <β
'α,β(Z) = § _1_ if Z ≥ β
[[z-(β-i)]α	U Z ≥ β
3
Published as a conference paper at ICLR 2022
where '匕化 is any loss function such that the overall loss function 'a,β is convex, differentiable
and strictly decreasing. Several natural choices for `left include the scaled logistic (c1 log(1 +
exp(-c2z))), exponential (c1 exp(-c2z)) or linear (-c1z + c2) losses.
Given a training dataset {(x1, y1), . . . , (xn, yn)} and a set of weights w1, . . . , wn ≥ 0 we let
n
Lα,β (fθ ) ： =〉, wi'α,β (yifθ (Xi))
i=1
be the reweighted empirical loss on this dataset.
Notation. Given a vector v, let kvk denote its Euclidean norm. For any j ∈ N, we denote the set
{1,...,j} by [j]. A random variable ξ is I-SUb-GaUSSian if for any λ ∈ R, E [eλξ] ≤ eλ2/2.
4 Theoretical Results
In this section, we present several theoretical results that justify the use of polynomially-tailed losses
in conjunction with importance weights to handle distribution shifts. Our final result shows a strict
separation between the performance of reweighted polynomially- and explonentially-tailed models
under label shift. We restrict our theoretical analysis to linear classifiers, fθ (x) = X ∙ θ, for some
θ ∈ Rd.
First, in Section 4.1 we shall characterize the limiting direction of gradient descent on reweighted
polynomially-tailed losses and show that this direction depends on both the weights as well as the
datapoints. Next, in Section 4.2, we upper bound the test error of this limiting solution in a label
shift setting. We also show that choosing weights that are obtained by exponentiating the unbiased
importance weights helps in reducing the test error. Finally, in this label shift setting, we show that
the maximum margin classifier suffers an error that is at least 1/8.
4.1	Implicit Bias of Gradient Descent on Polynomially-Tailed Losses
We begin by presenting a result that characterizes the implicit bias of gradient descent on reweighted
polynomially-tailed losses for linearly separable classification. Understanding this situation is a key
first step, as gradient descent for separable linear classification is often used as a simplified model to
theoretically characterize the behavior of overparameterized neural networks (Soudry et al., 2018;
Ji & Telgarsky, 2019; Nacson et al., 2019; Lyu & Li, 2019; Ji & Telgarsky, 2020).
Given a linearly separable dataset (X1, y1), . . . , (Xn, yn), we let zi := yiXi. We shall analyze the
iterates of gradient descent with step-size η > 0 and initial iterate θ(0) ∈ Rd, for all t ∈ {0, 1, . . .}:
θ(t+1) = θ(t) 一 ηVLα,β(θ(t)). Define the direction
b := arg min IX	Wi	St z ∙ θ> 0
Ua :	arg ɪmɪn	, s.t. Zi	> u,
θ"∣θk = 1 li⅛ (Zi∙ θ)a
for all i ∈
[n]	.
(2)
The following proposition characterizes the limiting direction of gradient descent iterates.
Proposition 4.1. Suppose that the data is linearly separable. For any α > 0, β ∈ R, any initial
point θ(0) ∈ Rd, and for all small enough step-sizes η the direction of the gradient descent iterates
satisfy the following: limt→∞ 口；(：)口 → θa.
The proof is presented in Appendix A. This proof relies recent result by Ji et al. (2020) that relates
the limiting direction of gradient descent on the unregularized loss to the limiting solution ofa norm-
constrained loss minimization problem, where the limit is taken with respect to the norm constraint.
Note that, unlike the maximum margin classifier, it is immediately clear that this limiting direction
θbα depends
on the weights w1 , . . . , wn .
A	…	I	.	..	「	.. K . ♦	.
As one would intuitively expect, the direction θα tries to
achieve a larger margin z「θ on points with larger weights w%. This behavior is also apparent in the
simulation in the rightmost panel in Figure 1, where upweighting points in the minority class helps
to learn a classifier that is similar in direction to the Bayes optimal classifier for the problem.
Our limiting direction θbα has the interesting property that it does not depend on several quantities:
the initial point θ(0), the properties of `left, and the “switchover” point β. Linear separability ensures
4
Published as a conference paper at ICLR 2022
that in the limit, the margin on each point is much larger than β , and so the loss of each point is in
the polynomial tail of part of 'a,β.
4.2	Generalization Analysis
The result in the previous subsection shows that the asymptotic classifier learnt by gradient descent,
θbα respects importance weights. However, this does not demonstrate that using polynomially-tailed
losses leads to classifiers with lower test error compared to using exponentially-tailed losses.
To answer this more fine-grained question, we will need to perform a more refined analysis. To do
this we will make sub-Gaussian cluster assumptions on the data, similar to ones considered in the
generalization analysis of the overparameterized maximum margin classifiers (Chatterji & Long,
2021; Wang & Thrampoulidis, 2021; Liang & Recht, 2021; Cao et al., 2021). In our setting, the
features associated with each label are drawn from two different sub-Gaussian clusters. We shall
consider a label shift problem where the training data is such that the number of data points from
the positive (majority) cluster will be much larger than the number of datapoints from the negative
(minority) cluster. The test datapoints will be drawn uniformly from either cluster.
Under these assumptions, we derive upper bounds on the error incurred by θ1, the limiting direction
of gradient descent of polynomially tailed losses with α = 1 (Section 4.2.2). We will find that its
test error is small when the reweighting weights are set to cubic powers of the unbiased importance
weights. (A similar analysis can also be conducted for other α.) In the same setting in Section 4.2.3,
we will show that the maximum margin classifier must suffer large test errors.
4.2.1	Setting for Generalization Analysis
TT	C	”1	T J	...	C	1∙	..	I ♦…一 、 T	”一片，T
Here we formally describe the setting of our generalization analysis. We let C ≥ 1 and 0 < C ≤ 1
denote positive absolute constants, whose value is fixed throughout the remainder of the paper. We
will use c, c0, c1, . . . to denote “local” positive constants, which may take different values in different
contexts.
The training dataset S := {(x1, y1), . . . , (xn, yn)} has n independently drawn samples. The condi-
tional distribution of the features given the label is
X I {y = 1} = μι + Uq
X I {y = -1} = μ2 + Uq,
where μι ∙ μ2 = 0, ∣∣μι∣∣ = ∣∣μ2∣∣, U is an arbitrary orthogonal matrix, and q is a random variable
such that: its entries are 1-sub-Gaussian and independent, and E kqk2 ≥ Ced.
We note that in past work that studied this setting (Chatterji & Long, 2021; Wang & Thrampoulidis,
2021; Cao et al., 2021), the cluster centers were chosen to be opposite one another μι = -μ2. Here,
since our goal here is to study a label shift setting We consider the cluster centers μι and μ2 to be
orthogonal. This ensures that learning the direction of one of the centers reveals no information
about the other center, which makes this problem more challenging in this setting.
Define P := {i ∈ [n] : yi = 1} to be the set of indices corresponding to the positive labels and
N := {i ∈ [n] : yi = -1} to be the set of indices corresponding to the negative labels. As stated
above, we will focus on the case where ∣P∣》∣N∣ ≥ 1. Let T :=* ≥ 1 be the ratio between
the number of positive and negative samples. The test distribution Ptest is balanced. That is, if
(x, y)〜Ptest, then P [y = 1] = P [y = -1] = 1/2 and X ∣ y follows the distribution as described
above. We shall study the case where negative examples (which are in the minority) are upweighted.
Specifically, set the importance weights as follows: wi = 1 for i ∈ P and wi = w for i ∈ N.
Assumptions. Given a failure probability 0 < δ < 1/C, we make the following assumptions on
the parameters of the problem:
1.	Number of samples n ≥ C log(1∕δ).
2.	Norm of the means ∣∣μk2 := ∣∣μι ∣∣2 = ∣∣μ2∣∣2 ≥ Cn2 log(n∕δ).
3.	Dimension d ≥ Cn∣μ∣2.
Our assumptions allow for large overparameterization, where the dimension d scales polynomially
with the number of samples n.
5
Published as a conference paper at ICLR 2022
4.2.2 Upper Bound for the Reweighted Polynomially-Tailed Loss Classifier
First, we shall prove an upper bound on the test error of the solution learnt by gradient descent on the
reweighted polynomially-tailed loss. Under the choice of weights described above, by equation (2)
1w
θι = arg min > --- + > ---, s.t. Zi ∙ θ> 0, for all i ∈ [n],
θ^∣θk = 1 i∈P zi∙θ	i∈N zi∙θ
where zi = yixi as defined previously. The following theorem provides an upper bound on the test
CKl	.C	ClCj	.一
error of θ1 and specifies a range of values for the weight w.
Theorem 4.2. For any 0 < C ≤ 1, there is a constant c such that, for all large enough C and
for any 0 < δ < 1/C, under the assumptions of this subsection the following holds. If the weight
τ3	3
τ2 ≤ W ≤ 2τ 3, then With probability at least 1 一 δ, training on S produces a classifier θι satisfying:
TeStError[bi] = P(χ,y)〜Ptejsign ® x) = y∣
≤ exp
(一 cn kμk4 )
This theorem is proved in Appendix B. To prove this theorem, we first show that the data is linearly
separable under our assumptions and use the implicit bias results of Proposition 4.1. Then, our
upper bound is equivalent to bounding the test error of the limiting iterate of gradient descent on
our reweighted loss. Next, we show that the sum of the gradients across each of the two groups
remains roughly balanced throughout training under our choice of w. This ensures that the classifier
aligns well with both cluster centers μι and -μ2 which then proves our bound on the test error via
a standard Hoeffding bound.
If we consider a regime where d and n are growing simultaneously, then the test error goes down to
zero if the norm of the cluster means ∣∣μ∣∣ grows faster than d1/4,regardless of the level of imbalance
in the training data as τ < n. Briefly note that in setting with balanced data (τ = 1), the bound on
the test error here matches the bound obtained for the maximum margin classifier in (Chatterji &
Long, 2021; Wang & Thrampoulidis, 2021; Cao et al., 2021) (although these bounds are not directly
comparable as the setting here is slightly modified for a label shift). The impossibility result by Jin
(2009) shows that these previous results guarantee that learning occurs right UP until the information
theoretic frontier (∣∣μ∣∣2 = ω(√d)).
Finally, it is interesting to note that the theorem requires the weight w to scale with τ3 instead of
τ (the unbiased importance weight), which would ensure that the reweighted training loss is an
unbiased estimate of the test loss. In our proof, we find that in order to suffer low test error it is
important to guarantee that the norm of the gradients across the two groups is roughly balanced
throughout training. We show that at any training iteration, the ratio of the sum of the derivative of
the weighted losses in the majority and minority clusters scales as wτT∕3. ThUS choosing W to scale
with τ3 ensures that the gradients are balanced across both groups. We verify this in our simulations
(Figure 2), and find that W = τ3 ensures equal test error across both classes, reducing overall error.
4.2.3 Lower B ound for the Maximum Margin Classifier
In the previous section we derived an upper bound to demonstrate that classifiers trained with
polynomially-tailed losses achieve low test error. We will now show that classifiers trained with
exponentially-tailed losses have their error lower bounded by 1/8 in the same setting.
Theorem 4.3. Let q 〜N(0, Id×d). There exist constants c and C such that, for all large enough
C and for any 0 < δ < 1/C, under the assumptions of this subsection the following holds. With
probability at least 1 一 δ, training on S produces a maximum margin classifier θbMM satisfying:
-r . I— 「*	1
TeStError[θMM]
P(x,y)〜Ptesjsign (bMM ∙ x) = y]
where Φ is the Gaussian cdf. Furthermore, ifthe imbalance ratio T ≥ c0√n√μk then with probabil-
1
ity at least 1 一 δ, TeStError[Θmm] ≥ 1.
This theorem is proved in Appendix C. Intuitively, the lower bound holds because when the imbal-
ance is severe, the max-margin classifier essentially ignores the samples in the minority class and
6
Published as a conference paper at ICLR 2022
8 6 4 2
")1OnW 万芦
1
Test Error vs. Imbalance Ratio (T)
W = 1
W = T
W = T3
Ma Max-margin
2	4	6	8	10	12
Imbalance Ratio (T)
U 5 O
N 1± 1±
(氏)ɪOnW万芦
Test Error vs. Importance Weight (W = τρ)
Major Majority Class
Minority Class
oVer Overall
0	1	2	3	4	5
P
Figure 2: A simulation study with class-conditional Gaussians, with d = 106, kμk2 = d0.502 and
n = 100. The mean μι = ∣∣μ∣∣eι and μ2 = ∣∣μ∣∣e2, and q 〜N(0,Id×d). (Left) We plot the test
error of θ1 as τ varies for different choices of w , and also for the maximum margin classifier. The
choice w = τ 3 leads to the lowest error throughout, while w = τ also does well for small τ . Both
the polynomially-tailed classifier with no importance weighting and the maximum margin classifier
suffer large test error. (Right) Here we fix the imbalance ratio τ = 10.1, and study how the error of
θb1 varies with w. When w = τ3, the error on both the majority and minority classes is almost equal,
resulting in low overall test error.
overfits to the majority class. Ignoring these negative samples results in a relatively small alignment
between the maximum margin classifier and the negative of the minority class center, which leads
to high error on this class.
Together, these two theorems demonstrate that there is a strict gap between the performance of
exponentially tailed and polynomially tailed losses under distribution shifts. As a concrete example,
consider the scaling where ∣∣μk2 = d2+40, n = d 5 and T = d 23o ≤ n. For all large enough d, all
our assumptions are satisfied. Now since, √nkμk2 > √τd, Theorem 4.2 guarantees that the test
error of θι → 0 as d → ∞. However, as T = d20 ≥ c0∣∣μ∣∣2,n/d = Cd系,for all large enough d,
the test error of the maximum margin classifier is guaranteed to be at least 1/8. This degradation of
the test error with T is also apparent in our simulations in Figure 2.
5	Empirical evaluation on deep interpolating classifiers
Inspired by our theoretical results, we use a polynomially-tailed loss with importance weights to
train interpolating deep neural networks under distribution shift. We use a polynomially-tailed loss
with β = 1 and 'ie任(Z) = log(1 + e-z)/log(1 + e-1), ensuring that 'a,β is continuous at transition
z = β when α = 1. We train models on two image classification datasets, one with label shift and
one with subpopulation shift, and include additional experiment details in Appendix E. Though these
nonlinear networks violate the assumptions of our theory, polynomially-tailed loss with importance
weights consistently improves test accuracy for interpolating neural networks under distribution shift
compared to importance-weighted cross-entropy loss.
Imbalanced binary CIFAR10. We construct our label shift dataset from the full CIFAR10
dataset. Similar to Byrd & Lipton (2019), we create a binary classification dataset out of the “cat”
and “dog” classes. We use the official test examples as our label-balanced test set of 1000 cats and
1000 dogs. To form the train and validation sets, we use all 5000 cat examples but only 500 dog
examples from the official train set, corresponding to a 10:1 label imbalance. We then use 80%
of those examples for training and the rest for validation. We use the same convolutional neural
network architecture as Byrd & Lipton (2019) with random initializations for this dataset.
Subsampled CelebA. For our subpopulation shift dataset, we use the CelebA with spurious cor-
relations dataset constructed by Sagawa et al. (2019). This dataset has two class labels, “blonde
hair” and “dark hair”. Distinguishing examples by the “male” versus “female” attribute results in
four total subpopulations, or groups. The distribution shift in the dataset comes from the change
in relative proportions among the groups between the train and test sets. To reduce computation,
we train on 2% of the full CelebA training set, resulting in group sizes of 1446, 1308, 468, and 33.
We construct our test set by downsampling the original test to get group-balanced representation in
7
Published as a conference paper at ICLR 2022
0.70
Imbalanced Binary CIFAR10
5 05 0
6655
...........
0000
ycaruccA tseT
No IW	IW (W)	IW (W3/2)
Figure 3: Polynomially-tailed loss versus cross-entropy loss on a label shift dataset and a subpopu-
lation shift dataset for neural networks optimized past 100% train accuracy without regularization.
* and ** indicate P < 0.05 and P < 0.005 statistical significance, respectively. Importance weights
(IW) consistently leads to gains when used with the polynomially-tailed loss across both datasets.
Exponentiating the weights further amplifies these gains for the polynomially-tailed loss. IW has
little effect for cross-entropy.2
ycaruccA tse
Trained to Interpolation
0.90
0.85
0.80
0.75
Subsampled CelebA
No IW	IW (W)	IW (W2)
the test set, resulting in 180 examples for each group. Following Sagawa et al. (2019), we use a
ResNet-50 with ImageNet initialization for this dataset.
5.1	Isolating the effects of importance weights
Interpolating regime. To understand the impact of polynomial losses in the interpolating regime,
we train unregularized neural networks with SGD past 100% train accuracy and report their final ac-
curacies on the test sets. We compare models trained with our polynomially-tailed loss against those
trained with the cross-entropy loss which has exponential tails. Let W correspond to the unbiased
importance weights. We consider three weighting scenarios for both loss functions: no importance
weighting at all (No IW) the classical unbiased importance weighting (IW (W)), and biased weight-
ing where we exponentiate the weights, (IW (Wc)), increasing the ratio of different weights. The
third setting is inspired by our theory in Section 4 that shows biased importance weights can im-
prove performance for polynomially-tailed losses. For these experiments, we fixed α = 1 and set
the exponents to be the largest value that still allowed for stable optimization. For CelebA, we ex-
ponentiate by 2 and for CIFAR10 we exponentiate by 3/2. We run 10 seeds for each experiment
setting. We compare the two losses via paired one-sided Welch,s t-tests, pairing runs with the same
random seed. We report the exact numbers in Appendix E.
Figure 3 shows the mean accuracy and the standard error for each of the three settings. As indicated
by our theory, we find that importance weighting with polynomially-tailed losses leads to statisti-
cally significant gains over cross-entropy in all cases. Further we find that exponentiating weights
boosts the performance of polynomially tailed losses, confirming our claims in Theorem 4.2. How-
ever, exponentiating the weights leaves the performance of cross-entropy loss largely unchanged. In
the case of celebA, we find that exponentiated reweighting with poly-tailed losses outperforms all
other methods. While in the case of Imbalanced Binary cIFAR10, we find a smaller gap between
polynomially-tailed losses and cross-entropy, partially due to a substantially higher run-to-run vari-
ability in training. This variability does not affect our main conclusion: exponentiated reweighting
with poly-tailed losses still outperforms cross-entropy with significance P = 0.01.
In Appendix D, we also compare the performance of polynomially-tailed losses with the cross-
entropy loss when we regularize training via early-stopping. We find that even with regularization,
poly-tailed losses give test accuracies better than or similar to cross-entropy across scenarios.
5.2	Comparing against prior distribution shift methods
To place our method’s empirical performance in the context of prior works, we extensively compare
our method to state-of-the-art distribution shift correction methods. on binary cIFAR10, we com-
2The mean test accuracy for No IW with cross-entropy on cIFAR10 appears to deviate from IW, but this
is due to the high variance across multiple runs of the model. The high variance arises due to 10:1 imbalanced
training data, combined with the fact that we train the models until they interpolate the training data. These
results for No IW differ slightly from those in Byrd & Lipton (2019) which used balanced training data and
imbalanced test data.
8
Published as a conference paper at ICLR 2022
Figure 4: Polynomially-tailed loss with exponentiated importance weights is better than or compet-
itive with state of the art methods that address distribution shift. We tune all methods extensively
using the same validation set. We compare to cross entropy with undersampling (CE+US), label-
distribution-aware margin loss (LDAM), class-dependent temperatures loss (CDT), logit-adjusted
loss (LA), vector-scaling loss (VS), and the use of distributionally robust optimization (DRO).
pare to label-distribution-aware margin (LDAM) loss (Cao et al., 2019), class-dependent tempera-
tures (CDT) loss (Ye et al., 2020), logit-adjusted (LA) loss (Menon et al., 2020), and vector-scaling
(VS) loss (Kini et al., 2021). On CelebA, we compare to VS loss only since it encapsulates CDT
loss, LA loss, and LDAM loss. We also evaluate the effects of distributionally robust optimization
(DRO) by Sagawa et al. (2019) when combined with poly-tailed loss versus VS loss. We grid search
each method exhaustively, and report results over 10 random initialization seeds. See Appendix E
for more details on our procedure and the best hyperparameters we found. Unlike in Section 5.1,
here we tune both the importance weight exponent and α for our poly-tailed loss. For all methods,
including ones that use DRO, we train all models to interpolation without early stopping or strong
regularizations, since we are interested in interpolating classifiers.
Figure 4 shows that when α and the weights, exponent are properly tuned, our poly-tailed loss with
exponentiated weights gives accuracies comparable to or even exceeding those by these recently
proposed methods. For both datasets, we found that cross entropy (CE) loss with undersampling
(US), which discards data from overrepresented groups or classes to form a balanced training set,
was a strong baseline.
On binary CIFAR10, our poly-tailed loss performs comparably to LA loss and is only 2% worse
than VS loss in mean test accuracy with overlapping confidence intervals. At first, we grid searched
VS loss across the published hyperparameter ranges from Kini et al. (2021), and it performed poorly
(58.6% test accuracy). More thorough grid search for VS loss gave optimal hyperparameters that
were similar to those of LA loss, increasing test accuracy by 10%. In sum, our poly-tailed loss
matched or exceeded existing data imbalance adjustment methods (under published hyperparame-
ters) and was beaten by 2% only after more extensively tuning the baselines.
On CelebA, our poly-tailed loss with IW achieves comparable test accuracy to VS loss. When
we use both loss functions with DRO, poly loss with DRO is better than VS loss with DRO by
3.5% on test accuracy and 8% better on worst group accuracy. Poly-tailed loss with IW gives the
best worst group accuracy out of all loss functions when not using DRO (see Appendix E). Such
improved performance is surprising, as we designed polynomially tailed losses to optimize for static
importance weights, rather than the worst case weights in DRO. The finding that poly-tailed losses
with IW performs well on worst-group accuracy may be an interesting direction for future work
6	Conclusion
Contrary to popular belief, importance weights are in fact compatible with the current paradigm
of overparameterized classifiers, provided that the loss function does not decay exponentially. The
limiting behavior of an interpolating linear classifier trained with a weighted polynomially-tailed
loss provably depends on the importance weights, unlike the maximum-margin classifier recov-
ered by an exponentially decaying loss function. Our theoretical analysis of generalization error
further suggested that exponentiating the classic importance weights can be key to accurate distribu-
tion shift correction. We empirically corroborated these theoretical intuitions, finding that weighted
polynomially-tailed losses are also able to address distribution shift for interpolating neural net-
works. Our work suggests that heavy-tailed losses together with importance weighting serve as a
simple and general candidate for addressing distribution shift in deep learning.
9
Published as a conference paper at ICLR 2022
Ethics S tatement
We use publicly available datasets in our paper, and do not foresee any potential harm arising from
the use of our methods.
Reproducibility S tatement
The full proofs of our theoretical results are presented in the appendix. All experimental details and
hyperparameters are presented in Appendix E.
References
Brian Beavis and Ian Dobbs. Optimisation and stability theory for economic analysis. Cambridge
University Press, 1990.
Jonathon Byrd and Zachary Lipton. What is the effect of importance weighting in deep learning?
In International Conference on Machine Learning, pp. 872-881, 2019.
Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced
datasets with label-distribution-aware margin loss. Advances in Neural Information Processing
Systems, pp. 1567-1578, 2019.
Yuan Cao, Quanquan Gu, and Mikhail Belkin. Risk bounds for over-parameterized maximum mar-
gin classification on sub-Gaussian mixtures. arXiv preprint arXiv:2104.13628, 2021.
Niladri S Chatterji and Philip M Long. Finite-sample analysis of interpolating linear classifiers in
the overparameterized regime. Journal of Machine Learning Research, 22(129):1-30, 2021.
Ziwei Ji and Matus Telgarsky. The implicit bias of gradient descent on nonseparable data. In
Conference on Learning Theory, pp. 1772-1798, 2019.
Ziwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning. In Advances
in Neural Information Processing Systems, pp. 17176-17186, 2020.
Ziwei Ji, Miroslav Dudik, Robert E Schapire, and Matus Telgarsky. Gradient descent follows the
regularization path for general losses. In Conference on Learning Theory, pp. 2109-2136, 2020.
Jiashun Jin. Impossibility of successful classification when useful features are rare and weak. Pro-
ceedings of the National Academy of Sciences, 106(22):8859-8864, 2009.
Ganesh Ramachandra Kini, Orestis Paraskevas, Samet Oymak, and Christos Thrampoulidis.
Label-imbalanced and group-sensitive classification under overparameterization. arXiv preprint
arXiv:2103.01550, 2021.
Tengyuan Liang and Benjamin Recht. Interpolating classifiers make few mistakes. arXiv preprint
arXiv:2101.11815, 2021.
Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks.
In International Conference on Learning Representations, 2019.
Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and
Sanjiv Kumar. Long-tail learning via logit adjustment. In International Conference on Learning
Representations, 2020.
Vidya Muthukumar, Adhyyan Narang, Vignesh Subramanian, Mikhail Belkin, Daniel Hsu, and
Anant Sahai. Classification vs regression in overparameterized regimes: Does the loss function
matter? Journal of Machine Learning Research, 22(222):1-69, 2021.
Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique Pamplona Savarese, Nathan
Srebro, and Daniel Soudry. Convergence of gradient descent on separable data. In Conference on
Artificial Intelligence and Statistics, pp. 3420-3428, 2019.
Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust
neural networks for group shifts: On the importance of regularization for worst-case generaliza-
tion. arXiv preprint arXiv:1911.08731, 2019.
10
Published as a conference paper at ICLR 2022
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-
likelihood function. Journal of Statistical Planning and Inference, 90(2):227-244, 2000.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im-
plicit bias of gradient descent on separable data. Journal of Machine Learning Research, 19(1):
2822-2878, 2018.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science.
Cambridge University Press, 2018.
Ke Wang and Christos Thrampoulidis. Benign overfitting in binary classification of Gaussian mix-
tures. In IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 4030-
4034, 2021.
Junfeng Wen, Chun-Nam Yu, and Russell Greiner. Robust learning under uncertain test distribu-
tions: Relating covariate shift to model misspecification. In International Conference on Machine
Learning, pp. 631-639, 2014.
Da Xu, Yuting Ye, and Chuanwei Ruan. Understanding the role of importance weighting for deep
learning. In International Conference on Learning Representations, 2020.
Han-Jia Ye, Hong-You Chen, De-Chuan Zhan, and Wei-Lun Chao. Identifying and compensating
for feature deviation in imbalanced deep learning. arXiv preprint arXiv:2001.01385, 2020.
11
Published as a conference paper at ICLR 2022
A Proof of Proposition 4.1
First, we restate the statement of the proposition.
Proposition 4.1. Suppose that the data is linearly separable. For any α > 0, β ∈ R, any initial
point θ(0) ∈ Rd, and for all small enough step-sizes η the direction of the gradient descent iterates
satisfy the following: limt→∞ 口；(：)口 → θa.
Proof Recall that the loss 'a,β is assumed to be convex, strictly decreasing to zero and is differen-
tiable. Define the minimizer of the loss over a ball of radius R as follows
θR = arg min Lα,β (θ).
θ"∣θ∣≤R
Under the assumptions of this proposition, we can invoke Theorem 1 by Ji et al. (2020) to get that
θ(t)	θR
lim 3 八小 3 = lim ――.
t→∞ kθ㈤ k 一 R→∞ R .
We will therefore instead demonstrate that limR→∞ θRR = θα to establish our claim.
First, note that the classifier θR is the minimizer of loss in a ball of radius R, so there exists a R0
with
La,β(Θro) =1 wi'a,β(Zi ∙ Θro) ≤ (minwi)'a,β(β)
i
i∈[n]
such that for all R > Ro, each example is classified correctly by Θr and the margin (Zi ∙ Θr) on each
example is at least β . Therefore,
n
n
La,β (θR ) = Ewi'a,β (Zi ∙ θR ) = E
wi
i=1
Thus, for any radius R > R0 we have
θR 1
= =石 arg min La,β(θ)
R R θ^∣θ∣≤R
i=1
[Zi ∙ θR - (β — 1)]α .
1
—arg min
R θ"∣θ∣∣≤R
wi
[Zi ∙ θ-(β -1)]α,
s.t., Zi ∙ θ ≥ β}
Rargmin J Ra XX
R θ"∣θk≤R IR M
wi
[zi ∙ (R)- β-1]
α,
s.t., Zi ∙ (R) ≥ RR
R ×(R ∙ argmin JXX T-Ir,
R ∖	θ^∣θk≤1 I i=ι 卜i∙ θ - β-1∖
S.t., Zi ∙ θ ≥ R
Now taking the limit R → ∞ we get that
lim粤
R→∞ R
lim arg min JXX T_W―,
r→∞ θ^∣θk≤11 i=1 [% ∙ θ - βRI i
S.t., Zi ∙ θ ≥ R
Jn
1∙ L
lim
r→∞ M Zi ∙ θ
wi
S.t., Zi ∙ θ ≥ R
arg min
θ"∣θk≤1
wi
(Zi ∙ θ)α ,
(ii)
= arg min
θ"∣θk≤1
= θbα .
wi
(Zi ∙ θ)α ,
s.t., Zi ∙ θ ≥ 0}
s.t., Zi ∙ θ > 0}
— / ia ,
12
Published as a conference paper at ICLR 2022
where (i) follows since for every R the function
n
X
i=1
Wi
hzi ∙θ - β-1 i
is convex and continuous, and therefore
in
wi
TT-------rττα
M [zi ∙ θ - β-1J
s.t., Zi ∙ θ ≥ R
has a unique minimizer (since the objective function is strictly convex, and the constraint set is
convex and compact), and is a continuous map by Berge’s maximum theorem (see, e.g., Beavis &
Dobbs, 1990, Theorem 3.6). Thus, it is possible to switch the order of the limit and the arg min.
Equation (ii) follows since the data is linearly separable, so there exists a θ such that z% ∙ θ > 0 for
all i which has finite objective value.
Therefore,
θ(t)	θR
lim U 八小 U = lim —— = θ0,
t→∞ k θ(t) k R→∞ R — α,
which finishes our proof.
B Proof of Theorem 4.2
In this section, we will prove an upper bound on the test error θ1, which is the classifier learnt
by gradient descent with the importance weighted Polynomially-tailed loss that decays as 1/z . By
Proposition 4.1, we know the choice of `left and β does not affect the implicit bias of gradient
descent, hence here, for the sake of convenience, we shall use the specific loss function
-(z -β)+1 ifz <β
'1,β(Z):= lz-(β-1)	if Z ≥ β,
with β = 1 - wn < 0. It can be easily checked that this loss function is convex, differentiable and
monotonically decreasing.
We want to bound the test error of θ1, however, in light of Proposition 4.1 we will instead bound
the test error of the limiting iterate of gradient descent starting from θ(0) = 8w2/3n (μ1 - w1/3M2)
with step-size η > 0 (again by the implicit bias of gradient descent is not affected by the choice of
the initial point):
θ(t+1) = θ㈤-ηvL(θ⑴),
where
L(θ)= X 'ι,β(&• θ)+ w X'l,β (Zi ∙ θ).
i∈P	i∈N
(t)
Recall that if the step-size is small enough then Proposition 4.1 guarantees that limt→∞ 忘y∣ = θι.
In this section We will simply let ' denote 'ι,β and also use the shorthands '(t) := '(zi ∙ θ(R) and
'i(t) := '0(zi ∙ θ⑴).
With this setup in place let us begin the proof. All of the assumptions made in Section 4.2.1 are in
scope here. First we have a lemma that upper bounds the test error using Hoeffding’s inequality.
Lemma B.1. There is a positive absolute constant c such that
P(χ,y)〜Ptest [sign (θ ∙ X)= y] ≤ 2
exp(-c (^ } +exp(-c (^
.pl kθk2 + pl	kθk2
13
Published as a conference paper at ICLR 2022
This lemma follows by mirroring the proof of (Chatterji & Long, 2021, Lemma 9) which in turn
follows by a simple application of Hoeffding’s inequality (Vershynin, 2018, Theorem 2.6.3).
Next we have a lemma that proves bounds on the norms of the samples, bounds the inner products
between the samples and also shows that with high probability the data is linearly separable.
Recall that the constants C ≥ 1 and 0 < C ≤ 1 were defined above in Section 4.2.
Lemma B.2. For all 0 < C ≤ 1, there is a c ≥ 1 such that, for all large enough C, with probability
1 - δ over the draw of the samples the following events simultaneously occur:
1.	For all k ∈ [n]
≤ ≤ Ilzkk2 ≤ cd.
c
2.	For all i ∈ P and j ∈ N,
|zi ∙ zjl ≤ CVZdlog(n∕δ).
3.	For all i 6= j ∈ [n],
|zi ∙ zj| < c(∣∣μ∣∣2 + Plog(n∕δ)).
4.	For all k ∈ P,
∣μi∙ zk-kμk21< ∣μk2∕2.
5.	For all k ∈ N ,
l(-μ2) ∙ zk -kμk2l < llμk2/2.
6.	For all k ∈ N ,
∣μι ∙ zk| < c∣μk √log(n∕δ).
7.	For all k ∈ P,
∣μ2 ∙ zk | < c∣μk √log(n∕δ).
8.	The samples are linearly separable.
Proof We shall prove that each of the ten parts hold with probability at least 1 - δ∕10 and take a
union bound to prove our lemma. Under the assumptions of this lemma, Parts (1), (3)-(5) and (8) can
be shown to hold with the required probability by invoking (Chatterji & Long, 2021, Lemma 13).
We will show that Parts (2), (6) and (7) also hold with the required probability to complete the proof.
Proof of Part ⑵:Note that for all i ∈ P, zi = μι + Uqi, and for all i ∈ N, z = μ2 + Uq?.
Therefore, for any i ∈ P and j ∈ N, since μι ∙ μ2 = 0,
Izi ∙ zj∣ = μi ∙ (Uqj) + μ2 ∙ (Uqi) + qi ∙ qj
≤ lμ1 ∙ (U qj )| + lμ2 ∙ (U qi)∖ + |qi ∙ qj |
=I(U>μι) ∙ qj∖ + I(U>μ2) ∙ qi∖ + ∖qi ∙ qj∖.	⑶
We will show that each of these terms is small with high probability for all pairs i ∈ P and j ∈ N.
For the first two terms, note that by Hoeffding’s inequality (Vershynin, 2018, Theorem 2.6.3)
P [∖(U > μι) ∙ q3 ∖ > ckμk√logWδ)] < 2exp (-M kμk2log(n0 )
'v 7 j1	3	∖	9 kμk2 J
δ
≤ ---
30n
14
Published as a conference paper at ICLR 2022
where the last inequality follows since the constant c ≥ 1 can be chosen to be large enough and
because δ < 1/C for a large enough constant C. Therefore, by taking a union bound over all j ∈ N
P "∃j ∈N,I(U >μi) ∙ qj∣>ck'k 尸质 # < 30.	(4)
Using an analogous argument we can also show that
P "∃i ∈P,I(U>μ2) ∙qi∣>ckμkpOgE# < 30.	⑸
Next, using (Chatterji & Long, 2021, inequality (15)) we get that
/ r -1 1	1	Cpdlog(n∕δ)	δ
P ∃i = j ∈ [n], 1qi ∙ qji > ---3----- ≤ 30.	(6)
Combining inequalities (3)-(6) we get that for all i ∈ P andj ∈ N with probability at least 1 - δ∕10
∣Zi ∙ ZjI ≤ 2ck'kpogwδ) + CpdlogWδ) ≤ c√diog(n∕δ),
where the last inequality follows since by assumption d ≥ Cn∣∣μk2 and because C is large enough.
This completes the proof of this part.
ProofofPart (6): For any k ∈ N, Zk = μ2 + Uqk. Recall that μι ∙ μ2 = 0, thus
∣μι ∙ Zk∣ = ∣μι ∙ (Uqk)∣.
Invoking inequality (4) proves that this part holds with probability at least 1 - δ∕10.
ProofofPart (7): This follows by an analogous argument as in the previous part.	■
We continue by defining a good event that we will work under for the rest of the proof.
Definition B.3. If the training dataset S satisfies all the conditions specified in Lemma B.2 then we
call it a good run.
Going forward in this section we shall assume that a good run occurs.
The following lemma provides some useful bound on the loss ratio at initialization and guarantees
that the loss of example remains in the polynomially-tailed part throughout training.
Lemma B.4. Recall that θ(0) = 8w2/3n(Mi — W1/3从2), and that Wi = 1 if i ∈ P and Wi = W if
i ∈ N. On a good run, for all i 6= j ∈ [n]
Furthermore, if the step-size η is sufficiently small then on a good run, for all t ∈ {0, 1, . . .} and for
all i ∈ [n]
1
Zi ∙ θ(t) — (β — 1).
Proof Consider an i ∈ P then
Zi ∙ θ(O) = 8w2/3n J ∙ μι — w1/3Zi ∙ μ2) ≤ 8w2/3n (3"；k + cιw1∕3∣∣μ∣∣ √log(n∕δ)
≤ 12w2∕3nkμk2 "l + 2c1w¾μkg≡一
(ii)
≤ 16w2^nkμk2,
15
Published as a conference paper at ICLR 2022
where (i) follows by Lemma B.2 and (ii) follows since ∣∣μk2 ≥ Cn2 log(n∕δ) ≥ Cτ2 log(n∕δ) ≥
2C/3w2/3 log(n∕δ) for a large enough constant C. Similarly, We also have the lower bound
Zi ∙ θ(0) = 8w2/3n (zi ∙ μι - w1/3Zi ∙ μ2)
≥ 8w2∕3n (∣μk——cιw1∕3kμkplog(n∕δ)) ≥ 4w2/3n∣μk2 1 - 2c1w / plog，n®
∖ 2	)	kμk
Recall from the definition of the loss that we set β = 1 - nw. Now again since ∣∣μ∣2 ≥
2c∕3w2/3 log(n∕δ), and because we choose β = 1 - nw, we infer that
Zi ∙ θ(0) - (β - 1) ≥ 8wfn⅛ - nw ≥ 2w2∕3n∣μ∣2.
Since the margin of the point is larger than (β - 1) therefore by the definition of the loss function
above we have that
'⑼=________1_______
i	Zi ∙ θ⑼-(β - 1)
and hence
_____1_____ ≤ '⑼ ≤ ____1____
16w2∕3nkμk2 — i — 2w2∕3nkμk2
By mirroring the logic we can also prove that for all j ∈ N
(7)
(8)
1
16wn∣μ∣2
/	1
≤ 2wn∣μk2
By combining equations (7) and (8) we immediately get that
This proves the first part of the lemma.
To prove the second part we shall prove that the loss on each example remains smaller than 1∕2,
which ensures `i(t) is equal to the polynomial loss function. Note that
L(θ⑼)=X '(0) + w X '(0) ≤
i∈P	i∈N
|P|	w|N|	wn
2w2∕3nkμk2 + 2wn∣μ∣2 — 2wn∣μ∣2
≤ 1∕2.
(9)
This proves that '(0) ≤ 1∕2 for all i ∈ [n]. By (Ji et al., 2020, Lemma 2) we know that if the step-
size is small enough then the sequence {Lb(θ(t))} is non-increasing. Hence, we have that `i(t) ≤ 1∕2
for all t ∈ {0,1,...} and for all samples. This wraps UP our proof.	■
The next lemma lower bounds the inner product between the normalized gradient descent iterates,
and μι and μ2.
Lemma B.5. Let c and c0 be absolute constants. Then, on a good run, for any t ∈ {0, 1, . . .}
μι∙ θ(t+1)〉μι∙ θ(°) + kμk2 VWi I_1_
、[1+ …、…)]
Pts=0
i∈P
-W 心 Ss
	
Pi∈N -Wi'i(S)
PtS=0 P
i∈[n] -wi'i(S)
and
μ2 ∙ θ(t+1s〉μ2 ∙ θ⑼,kμk2 VWi I___________________1______________
kθ(t+1)k ≥ kθ(t+1)k +	2c√d	I 1+ E	kθ(0)k------而
L	cη ∖! WT Ps = 0 Pi∈[n] -wi'i -
PS=0 Pi∈N -WHs- c⅛F1 Pi∈p-wi'i(S)
PtS=0 P
i∈[n]
-wi'i(S)
16
Published as a conference paper at ICLR 2022
Proof Let Us prove the first claim for the inner product with μι. The second claim regarding μ2
shall follow analogously. For any t ∈ {0, 1, . . .}, by the definition of the gradient descent step, we
have that
μi ∙ θ(t+1) = μι ∙ θ(t) + η X Wiμι∙ Zi (-'") + η X Wiμi ∙ Zi (-'i(t))
i∈P
i∈N
Note that by the definition of the loss function We have that -'i(t) ≥ 0 for all t and all i. Thus by
Parts (1) and (6) of Lemma B.2 we get that
μi ∙ θ(t+1) ≥ μι ∙ θ⑶ + nk；k X -W*" - cιη∣∣μ∣∣ ,log(n∕δ) X -Wi'i(t)
i∈P	i∈N
μi∙ θ(t) + ηkμk2 X -Wi'i(t)
i∈[n]
	
ηkμk2 Λ	ci√l°g(√δ)∖ X	`o(t)
I⅛-wi 'i
μi∙ M + 呼
--「‘IN -wi'i
where c1 is the constant from Lemma B.2. Unrolling this inequality over t steps we have that
μi∙ θ(t+1) = μi ∙ θ(0) + ηkμk2 X [X -Wi'i(S)- c1Pl0g(n∕δ) X -Wi'i(s)l .	(10)
s=0 Li∈P	kμk i∈N	_
On the other hand, by the triangle inequality we know that
Mt+1)k ≤ Mt)∣∣ + η∣vL(θ(t))∣∣
=kθ(t)k+η
≤kθ(t)k+η
X zi(-wi'T))
i∈[n]
X zi(-'i(t)) + ηw
i∈P
X Zi(-'i(t))
i∈N
(11)
Now note that
2
X Zi(-'i(t))	= X('i(t))2kZik2 + X 'i(t)'j(t)(Zi∙ Zj)
i∈P
i∈P
(i)
≤ c1
i6=j∈P
X(-'i(t))2d + X 'itt'T(kμk2 + Pd^og(n∕δ))
i∈P
i6=j∈P
dX -'i(t) + ∣P∣(kμk2 + Pdlog(n∕δ)) X -'i(t)
i∈P
i∈P
(ii)
≤ c1 max
1 j∈P
where (i) follows by using Part (1) and Part (3) of Lemma B.2, (ii) follows since -'j(t is positive
for all j ∈ [n], and (iii) follows since by assumption d ≥ Cn∣∣μk2 and ∣∣μk2 ≥ Cn2 log(n∕δ) for a
large enough constant C. Hence we have that
X Zi(-'i(t))
i∈P
≤ C ⅛ X-'i(t).
17
Published as a conference paper at ICLR 2022
Similarly, one can also show that
Applying these bounds in inequality (11) we get that
kθ(t+1)k ≤ kθ(t)k + η "再、X -「wf^\ X -'i(t)
≤ kθ(t)k+η" √⅛ X -'i(t)+w√⅛ X -'i(t)
=kθ(t)k + c3η∖InΓ∩ X -wi'i(t).
i∈[n]
Therefore, unrolling this bound over t steps we find that
kθ(t+1)k ≤ kθ(0)k + c3η∖ ∣~Nrr∖ Xt X
-wi'i(S)
|N| s=0 i∈[n]
(12)
c3η∖ /ɪ X X -WESs
s=0 i∈[n]
1+
c3η
kθ(0)k
昌 PS = 0 Pi∈[n] -wi'i
Thus, combined with inequality (10) we get that,
μι∙ θ(t+1s〉μι∙ θ(°s + k〃k2√N I_______1_________
———[1+ C…∖ Ps=((P“WHS, J
PtS=0
Pi∈p -Wiei(S -
c1 Sog(n∕δ)
kμ
Pi∈N-Wi'i(S)
PtS=0 P
i∈[n]
which completes the proof of the first part of the lemma. The second part follows by an identical
argument.	■
Next, we prove a lemma that shows that throughout training the ratio between the losses between
any two samples remains bounded.
Lemma B.6. There is a positive absolute constant c ≥ 1 such that the following holds for all large
enough C, and all small enough step-sizes η and for any τ3 ≤ W ≤ 2τ3. On a good run, for all
t ∈ {1, 2, . . .} and all i 6= j ∈ [n]
Proof Let c1 ≥ 1 be the constant c from Lemma B.2 above. We shall show that the choice c =
max {8, 2(4c2)1∕3} suffices.
We shall prove this via an inductive argument. For the base case, at step t = 0, we know that by
Lemma B.4 that the ratio between the losses of sample i and j is upper bounded by 8 (Wj/Wi)1/3.
Now, we shall assume that the inductive hypothesis holds at an arbitrary step t > 0 and prove that it
holds at step t + 1.
Without loss of generality, we shall analyze the ratio between the losses of the samples with indices
i = 1 and j = 2. A similar analysis shall hold for any other pair. Define Gt := '1ts, Ht := '2ts,
18
Published as a conference paper at ICLR 2022
At := Ht /Gt . Note that,
Gt+1 = θ(t+1) ∙ zι - (β - 1)
_	1
θ㈤∙ z1 + η Pk∈[n] wk (-'"(Zk ∙ ZI)-Ge- I)
(i)	1
=	2
θ⑴∙ zι - (β - 1) + η Pk∈[n] wk (Zk ∙ z1) (4))
=	'lt)
1	+ η'1t Pk∈[n] wk (Zk ∙ z1) (喂)
=Gt • -------------------------2-----2
1	+ η'1t Pk∈[n] wk (Zk ∙ z1 ) ('kt))
where (i) follows since -'0(t) = ('kt))2 as the loss of each example is always in the polynomial tail
of the loss by Lemma B.4. Therefore, we have that
=	Ht+1	= Ht	1 + Mt	Pk∈[n] wk (Zk	∙	ZI) ('kt))
t+1	Gt+1 Gt	1 + η'2t)	Pk∈[n] wk(Zk	∙	Z2) ('kt))2
1 + η'1t) Pk∈[n] wk (Zk ∙ ZI) ('kt))
=At-----------------------------.
1 + η'2t) Pk∈[n] wk (Zk ∙ Z2)('kt')
Now since the step-size η is chosen to be small enough, |Zi ∙ Zj | ≤ cid (by Part (??) of Lemma B.2
and by the assumption on d) and because the losses are all smaller than a constant by Lemma B.4,
the following approximations hold
1 + η'1t) X wk(Zk ∙ Zi) ('kt)) ≤ exp ( η'ιt) X wk(Zk ∙ Zi) ('kt))
k∈[n]	k∈[n]
1 + η'2t)	X wk(Zk	∙ Z2)	('kt))	≥	eχp	(η'γ-	X	wk(Zk	∙ z2	('kt))
k∈[n]	k∈[n]
and thus,
At+1 ≤ At exp (η'ιt)	X wk(Zk ∙	Zi)	('kt))	—	n|-	X	wk(Zk	∙ Z2)	('kt))	) ∙	(13)
k∈[n]
Let us further upper bound the RHS as follows
k∈[n]
At exp (η'1t) X wk(Zk ∙ Zi) ('kt)) - η~^γ- X wk(Zk ∙ Z2) ('kt)) j
k∈[n]	k∈[n]
=At exp (ηwι ('It)) ||Zik2 - 1 ηw2 ('2t)) kZ2k2)
× eχp (η'1t) Xwk(Zk ∙ ZI) ('kt)) - 1 η'2t) Xwk(Zk ∙ Z2) ('kt))
k6=1	k6=2
× exp c2η `(it) +
(i)
≤ At exp ci ηdwi
Pdlog(n∕δ)) X wk ('kt))
k∈[n]
19
Published as a conference paper at ICLR 2022
where (i)	follows since for all i ∈	[n],	d/c\ ≤	Ilzik2	≤	cιd	and for any j = k,	|zj	∙	Zk |	≤
ci (∣∣μ∣∣2 +，dlog(n∕δ)) by Lemma B.2. Continuing We get that,
At+t
≤ At exp
c1 ηdw1 Gt3 -
X exp 卜2η (Gt + Ht) (kμk2 + Pdlog(n∕δ)) X Wk ('£))
k∈[n]
=Atexp (- 1— ηdw2G3
2c1	t
2c21 w1
_
W2
× exp 卜2η (Gt + Ht) (kμk2 + Pdlog(n∕δ)) X Wk (喂)卜
k∈[n]
(14)
With this upper bound in place, consider two cases.
Case 1 (A3 ≤ 4cWW1): Using inequality (14) we know that
At+i ≤ Atexp ( - --ηdw2 G3
2ci
2c21 W1
_
W2
× exp 卜2η (Gt + Ht) (kμk2 + Pdlog(n∕δ)) X Wk (S) j
k∈[n]
≤ At exp (ciηdwιG3) exp 卜2η (Gt + Ht) (kμ∣2 + Pdlog(n∕δ)) X Wk ('kt)) j .
k∈[n]
Now the loss on each example is less than the total initial loss 1∕2 (see equation (9)) and therefore,
At+i ≤ At exp (c3ηdW) exp
Ccηη (kμk2 + Pdlog(n∕δ)) Wn
(i)
≤
i/3
exp(1∕8) ≤ 2
4 4c1wι
kW2
i/3
(ii)
≤
(wi Y/3
CH)
where (i) follows by choosing the step-size η to be small enough and because At ≤ (4(WwI) / in
this case, and (ii) follows by the choice of constant c ≥ 2(4c2i)i/3 from above.
Case 2 (4cww1 < A3 ≤ CwwI): In this case again by inequality (14)
At+i
≤ At exp
ηdW2 Gt3
× exp
C2η (Gt + Ht) (kμk2 + Pdlog(n∕δ)) X Wk G)
k∈[n]
× exp
At exp
ηdW2 Gt3
(
C2ηG3 (At + 1) (kμk2 + Pdlog(n∕δ)) X Wk
k∈[n]
20
Published as a conference paper at ICLR 2022
Continuing we find that,
At+1
(i)
≤ At exp
ηdw2 Gt3
× exp
C6ηG3 max{( WW1)	,l} (kμ∣∣2 + P log(n∕δ)) X Wk
ηdW2 Gt3
× exp
(c6ηwιG3 (kμk2 + PdlogS0) X] (min{Wl,W2}) / )
(ii)	1	3
≤ At exp —ηdw2Gt
2c1	t
2c21 W1
W2
X exp Cc6wιG3t (kμk2 + Pdlog(n∕δ)) (|P| + W1/3|N|))
13
=At exp — ~-ηdw2 Gt
2c1	t
2c21W1
W2
× exp (。6ηw1G3 (kμk2 + √dlog(n∕δ)) |P| (l + W^
(iii)
≤ At exp
(iv)
≤ At exp
2cι
2cι
ηdW2 Gt3
2c21W1
W2
	
ηdw2 G3 ∙
exp (2c6ηw1 G3 (kμ∣∣2 + √dlog(n∕δ)) n
exp (2c6ηw1G3 (kμ∣∣2 + VZdlog(n∕δ)) n
	
	
= At exp -c1W1Gt3 (d — c7n (kμ∣∣2 + √dlog(n∕δ)))],
`(t)
where (i) follows by the inductive hypothesis which guarantees that	≤ c( W )1/3 ≤ cw1/3, (ii)
'产-'wk'-
follows since wk = 1 if k ∈ P and wk = w if k ∈ N. Inequality (iii) follows since w < 2τ3 and
since |P| ≤ n, and finally (iv) follows since in this case Af > 4cWW1. Now since by assumption the
dimension
d ≥ Cnkμk2 ≥ C2n3 log(n∕δ)
for a large enough constant C. Hence, We have that At+1 ≤ At ≤ C (W1) / in this case.
Recall that since we assumed the inductive hypothesis to hold, the two cases analyzed above are
exhaustive. This completes our proof.	■
The next lemma uses the loss ratio bound that we established to show that the difference between
the gradient of the losses over the positive cluster and the negative cluster is small at any iteration.
Lemma B.7. For any positive c, there exists a 0 < c < 1 such that, for all large enough C, if the
step-size η is sufficiently small and τ23 ≤ W ≤ 2τ3 then on a good run, for any t ∈ {0,1,...}
and
X -Wi'i(
i∈P
t)
≡√log(n∕δ)
X -WF
i∈N
≥C X -wi'i(t)
i∈[n]
X -Wi'i(t)
i∈N
c√log(n∕δ)
X-WF ≥ C X -Wi'i(t).
i∈P
i∈[n]
	
	
21
Published as a conference paper at ICLR 2022
Proof We begin by proving the first part of the lemma. Note that since [n] = P ∪ N to prove the
first part it suffices to instead show that
≥ O + 'W F
	
J	S√log(n^∖	V N)
≥ (C+MN T
o (1 - C) X('(t))2 ≥ (C + CM犷)W X('(t))2
(since -'i(t) = ('(t))2 for the polynomial loss)
U(I-C)IPI min(`(t))2 ≥ (C+cplo;(IS)! w|N| maχ('it)产
U (1 - C)τ ≥ C +
U (1 - C)T ≥ ( C + -
plog(n∕δ)∖ WmaXi∈N('(t))2
∣∣μk m	mini∈p ('(t))2
plog(n∕δ)∖	1
F - wc1c∙' W2/3
(by invoking Lemma B.6; note that C1 ≥ 1)
(since ∣∣μ∣ ≥，Cn2 log(n∕δ), where C is a large enough constant).
Since Cis large enough, we now choose the constant 0 < C < 1 to be such that (1 - C)∕(C1(C +
-/√C)) is at least (2)1/3. This proves the first part of the lemma.
To prove the second part of the lemma, note that again since [n] = P ∪ N it suffices to show that
∙-v
√iog(n∕δ))X -w 0(t)
∣μ∣	)i-	&
U (1- C)INIW mN('(t))2 ≥ (C+C W;") |p| maχ('(t))2
UW≥ C+
UW≥ C+
c√log(n∕δ)
-kμ-
(1-c)
C√log(n∕δ)
-ra-
(1-c)
maxi∈p ('(t))2
mini∈N ('it))2
C]W2∕3T
U W1/3
Cl
一(1-C) 一 ' T
τ
21/3
U W1/3
≥
≥
• τ
where the last implication follows by the choice of C from above. This completes the proof.
We now have all the pieces required to prove our main theorem. Recall its statement.
Theorem 4.2. For any 0 < C ≤ 1, there is a constant C such that, for all large enough C and
for any 0 < δ< 1∕C, under the assumptions of this subsection the following holds. If the weight
22
Published as a conference paper at ICLR 2022
τ3 ≤ W ≤ 2τ3, then with probability at least 1 一 δ, training on S produces a classifier θι satisfying:
TeStError[bi] = P(#,y)〜Ptejsign ® X)= y]
≤ exp
卜 cn kμk4)
Proof First, by Part (8) of Lemma B.2 we know that the data is linearly separable. Thus, by
Proposition 4.1 we know that
θb1
lim
t→∞
θ(t)
PW.
Given this equivalence, by Lemma B.1 we know that
P(x,y)〜PteJsign ® X)= y]
θ(t)
=P(χ,y)~PtesIsign vlim∞ 可力=y
J 0	,Λ∙ θ⑻∙ MiVV 0	,Λ∙ θ⑶∙ 〃2、2)
≤ 2 [exp ]-c(t→∞ x“+ p l-c(t→∞ -p<J )
(15)
We shall now establish lower bounds on limt→∞ ：：；；同 and limt→∞ θ^μ2 to obtain the desired
bound on the test error. Let us lower bound limt→∞ ：：；；同.The bound on limt→∞ ：：；；t片 shall
follow by exactly the same logic.
By Lemma B.5 we know that
].μι∙ θ(t+1)
lιm -------------
t→∞	kθ(t+1)∣∣
≥ lim 上
≥ t→∞∞ ∣∣θ(t+i)∣∣
lim k”k2可
t→∞	2ci d
×
c2 ,logs/θ V-^	^
Z-Wi 'i
t
X X-Wa(S)-
s=0
i∈P
(≥) kμk2 严∙ lim
2ci d	t→∞
kμk2 可∙ lim
2ci d	t→∞
1
1 +	kθ(O)k
c1η ↑∕~∖jd∖ PS = O Pi∈[n]
t
X X-Wi'i(S)-
s=0
i∈P
=I ×
1-wi'i(S)」
c2 plog(n7δ)	川
k〃k	i∈N-wi'i
1
1 +	M0)k -
c1η ∖∕~^N∖ PS = 0 Pi∈[n] -W
t
t→m∞ X X Tw心Ss-
S=0
i∈P
，i'0(S)
ii
C2 √log(n∕δ)
kμk
X -Wi'i(S)
i∈N
(16)
×
where (i) follows since μι ∙ θ(O) is bounded and ∣∣θ(t+1) ∣∣ → ∞ by (Ji et al., 2020, Lemma 2).
We will now show that the first limit in RHS equals 1. To do this, first note that
1
一
C'^qN PS = O Pi∈[n] -wi'i )_
1
1 ≥ lim
t→∞
1 + limt→∞
kθ(°)k
CITnqN PS = O Pi∈[n] -wi'i ) _
23
Published as a conference paper at ICLR 2022
Now we will show that
lim
t→∞
_____________kθ(0)k_____________
c1ηqN PS = 0 Pi∈[n] -wi'i("
0.
First note that —√^ Pt kp)k-P(Sy ≥ 0, since -w£(S) ≥ 0, so to show that this limit equals
0, it suffices to show that cιη√dP：=。Pi∈[n] -wi'i(S) grows unboundedly as t → ∞. Using
inequality (12) from above we get that,
kθ(t+1)k ≤ kθ(0)k + c3η
t
XX -Wi'i⑶
S=0 i∈[n]
The norm kθ(0) k is finite and we know that kθ(t) k → ∞ by (Ji et al., 2020, Lemma 2), therefore
cιη√d PS=。Pi∈[n] -wi'i(S) must grow unboundedly. This proves that
1∙	1
lim --------------------------------------
t→∞	ι +______________kθ(0)k
_	ciη√d PtS = O Pi∈[n] -wi'i(S)
as claimed. This combined with inequality (16) yields the bound
μι∙ θ(t+1) JT2PIM r ςs=0
lim N 八/…、N ≥---------lim ---------- ɪ	,/、
t→∞ ||川+1)||	2C1 Vd t→∞	PS = 0 Pi∈[n] -Wi'i(S)
(i) C3kμk2P∣N “E PS=0 Pi∈[n] -wi `i
≥ ------f÷---- lim -----------------L
d	√d	t→∞ PS=0 Pi∈[n] -Wi'i(S)
Pi∈P -wi'i(S)-
C2 ʌ/log(n/b)
Pi∈N -Wi'i(S)
=C3kμk2 VzNl
√d
=c3kμk2PP
√τd
(≥) c4∣∣μ∣∣2√n
√τd
where (i) follows by invoking Lemma B.7 and (ii) follows since |P| ≥ n/2. As stated above, using
a similar argument We can also show that limt→∞ [%(?：+)： ≥ c4k√^√n. Plugging these lower
bounds into inequality (15) completes our proof.	■
C Proof of Theorem 4.3
In this section we will prove a lower bound on the test error of the maximum margin linear classifier
θbMM . Here we will work with the exponential loss
`(z) = exp(-z).
Define the loss L(θ) = £记网 '(yix>θ) = Pi∈[n] '(z> θ). For a step-size η and initial iterate
θ(0) = 0 (this choice of initial point is for the sake of convenience, it does not affect the implicit
bias of gradient descent) for any t ∈ {0, 1, . . .}
θ(t+1) = θ(t) - ηvb(θ(t))
24
Published as a conference paper at ICLR 2022
be the iterates of gradient descent. We let '(t) be shorthand for '(zi ∙ θ(t)) and therefore,
θ(t+1) = θ⑴一ηvL(θ⑴)=θ㈤ + η X Ziw).
i∈[n]
The results by Soudry et al. (2018) guarantee that for a small enough step-size η the direction of the
iterates of gradient descent limt→∞ 需)ɪ = Θmm. Therefore, We will instead prove a lower bound
on the asymptotic iterates of gradient descent.
As we did in the proof of Theorem 4.2, going forward we will assume that a good run occurs (see
Definition B.3), which guarantees that all of the conditions specified in Lemma B.2 are satisfied by
the training dataset S .
With the setup in place, we shall now prove this theorem in stages. Throughout this section the
assumptions stated in Section 4.2.1 shall remain in force. We begin with a lemma that shows that
the margin of the maximum margin classifier scales with ʌ/d/n.
Lemma C.1. There is an absolute constant c such that, on a good run, for all large enough C, for
all i ∈ [n]
θMM ∙ Zi ≥ c∖∣~
n
Proof We will prove this result by constructing a unit vector φ with a margin that scales with ,d/n.
This immediately implies that the maximum margin classifier must also attain this margin on all of
the points.
Define φ to be as follows
,_ Σi∈[n] Zi
φ := kPj∈[n] Zj k .
We will first bound the norm of the denominator as follows
2
X Zj	= X kZjk2 + X Zj ∙ Zk
j∈[n]	j∈[n]	j 6=k
≤ X kZjk2 + X |Zj ∙ Zkl
≤ cιnd + cιn2 (kμ∣∣2 + ʌ/dlog(n∕δ)
=cιnd f1 + Wf + nplOg(n/6
dd
(ii)
≤ 2c1nd
(17)
where (i) follows by Lemma B.2 and (ii) follows since d ≥ Cnkμk2 ≥ C2n3log(n∕δ) where
recall C is sufficiently large.
Now we lower bound the margin between numerator of v and Zk for any k ∈ [n]
I X Zi) ∙ Zk = IlZik2 + X Zi ∙ Zk ≥ ∣∣Zik2 - X ∣Zi ∙ Zkl
i∈[n]	i6=k	k6=i
(i)	d	I_______
≥-----cιn(kμ∣2 + √d log(n∕δ))
c1
_ d Λ	c2nkμk2	c2n√log(n∕δ)
=— 1 - -------Z--------------------
Cl ∖	d	√d
d
≥厂，
2c1
(18)
25
Published as a conference paper at ICLR 2022
where (i) again follows by invoking Lemma B.2 and by the assumption on d,
Combining inequalities (17) and (18) yields that for any k ∈ [n]
dd
φ ∙Zk ≥ 2cι√2C^d = CVn
This proves the result.
The next lemma provides control over the rate at which the norm of iterates θ(t) grows late in
training.
Lemma C.2. There is an absolute constant c such that, for all large enough C, if the step-size η is
sufficiently small then on a good run, there exists a t0 such that for all t ≥ t0
kθ(t+1)k≥kθ(t)k + Cnrd X '(t).
i∈[n]
Proof By the definition of gradient descent
kθ(t+1)k2 = kθ(t) + η X `i(t)zik2
i∈[n]
=kθ(t)k2 + 2n X 停Zi∙ θ㈤ + n2k X '(t)Zik2
k∈[n]	i∈[n]
≥ kθ(t)k2 + 2n X '(t)Zi∙ θ(t).	(19)
i∈[n]
Now We know that ɪf(tɪ → Θmm. Also note that in the previous lemma (Lemma C.1) We showed
that for all i ∈ [n]
Θmm ∙ Zi ≥ ci
Therefore, there exists a iteration t1 such that for all t ≥ t1 and all i ∈ [n]
θ(t) ∙ Zi ≥ ci ∣~d
kθ(t)k - ^2^V n.
Continuing from (19), for any t ≥ ti
kθ(t+i)k2 ≥kθ(t)k2 + 2n X 中Zi∙ θ(t)
i∈[n]
≥kθ(t)k2 + cιnkθ(t)krf X 单
i∈[n]
= kθ(t)k2
Cin √d Pi∈[n]'(t)'
+ 即
Taking square roots we get that for any t ≥ ti
kik≥kθ(t)kv+c12⅛r1
(20)
Further by (Ji et al., 2020, Lemma 2) we know that kθ(t) k → ∞, so there exists a t2 such that for all
t ≥ t2, kθ(t)k ≥ c∖1η√nd. Thus, for t ≥ t2, we have
1
• Cinrd X '(t)≤ 8 ∙ p∈J
i∈[n]
≤ 8,
(21)
26
Published as a conference paper at ICLR 2022
where the last inequality follows since the initial loss is equal to n, as θ(0) = 0, and the total loss is
decreasing again by (Ji et al., 2020, Lemma 2) if the step-size is small enough. It is easy to check
that for any 0 ≤ x ≤ 8
------- X
√1 + X ≥ 1 + 4.
Thus, combining inequalities (20) and (21) we get that for all t ≥ t0 = max{t1, t2}
kθ(t+1)k ≥ kθ(t)kuι+c1η 彳 PTF ≥kθ(t)k (1+c*p; Jit) j
=kθ(t)k + c4ηrd X 娟,
i∈[n]
which completes our proof.	■
Continuing we will show that throughout training the ratio of the losses between the different exam-
ples are bounded by a constant. This ensures that each example roughly “influences” the gradient
update by the same amount in each step. However, since the number of points from the positive
cluster is larger, the gradient update shall overall be more highly correlated with the mean of the
majority positive center μι than the mean of the minority negative center μ2.
The proof is identical to the proof of Lemma 11 by Chatterji & Long (2021). However, since our
setting is slightly different to the setting studied in that paper we reprove the result here.
Lemma C.3. There is an absolute constant c such that, for all large enough C, and all small enough
step sizes η, on a good run, for all iterations t ∈ {0, 1, . . .} and all i, j ∈ [n]
Proof First note that L(θ(0)) = Pi∈[n] '(0) = n and since step-size η is small enough training loss
is non-increasing by (Ji et al., 2020, Lemma 2).
Let c1 be the constant c ≥ 1 from Lemma B.2. We will show that c = 4c21 suffices.
We shall prove this via an inductive argument. For the base case, at step t = 0, we know that
θ(0) = 0, therefore the loss on all of the samples is equal to 1. Now, we shall assume that the
inductive hypothesis holds at an arbitrary step t > 0 and prove that it holds at step t + 1.
Without loss of generality, we shall analyze the ratio between the losses of the samples with indices
i = 1 and j = 2. A similar analysis shall hold for any other pair. Define Gt := '1t), Ht := '2t),
At := Ht/Gt . The ratio between these losses at step t + 1 is
exp(-θ(t+1) ∙ z2)
t+1	exp(-θ(t+1) ∙ zι)
=exP(TM + ηPk∈[n] 'kt)zk) ∙益
eχp(Tθ⑴+ η Pk∈[n] 'kt)zk) ∙ ZI)
-	η E 'kt) (Zk ∙ Z2 - Zk ∙ Z1)
k∈[n]
-	η ('2t)kz2k2 - '1t)kz1k2 - X 'kt)zk ∙ z2 + X 'kt)zk ∙ z1
k6=2	k6=1
-	η ( HtkZ2k2 - GtkzIk2 - X 'kt)zk ∙ z2 + X 'kt)zk ∙ z1
k6=2	k6=1
≤ At ∙ exp ( -η ( Htkz2 IF - Gtkzlk2 - X 'kt)lzk ∙ z2| - X 'kt) |zk ∙ z1|
k6=2	k6=1
27
Published as a conference paper at ICLR 2022
Now note that by Lemma B.2, for all i = j ∈	[n], d/ci ≤	Ilzik2	≤	cιd	and	|zi	∙	zj|	≤
ci (∣∣μ∣∣2 + dθlog(n∕δ) ʌ, and therefore,
At+i ≤ At ∙ exp —η
At ∙ exp -n
(i)
≤ At ∙ exp -n
(-c- Gtcid - 2cι (kμll2 + Pdiog(nτδy) ^X 'kt j j
i	k∈[n]
(Gtd (At- c2) — 2cι (kμk2 + √diog(nTδy) Gt X 'Gr
ci	k∈[n]	t
((At- cl) - 8c3 (l∣μ∣l2 + Pdlogs0) Gtn))
At ∙exp 一
ηGtd
n √log(n∕δ)
ci
(ii)
≤ At ∙exp 一
ηGtd
(iii)
≤ At ∙exp 一
ci
nGtd
ci
At-c2 -8c4 (半 +
(22)
W
where (i) follows since by the inductive hypothesis 'kt)∕Gt ≤ C = 4c2, (ii) follows since by
assumption d ≥ Cn∣μ∣2 ≥ C2n3 log(n∕δ), and (iii) follows since C is sufficiently large.
Now consider two cases.
Case 1 (At ≤ 2c2i ): By inequality (22)
At+i ≤ At ∙ exp (—nGtd (At — 2c2))
=At exp (nGt" (2c2 - At)) ≤ 2c； exp (2ncιGtd)
≤ 2c2i exp (2nci nd)
≤ 4c2i,
where the last inequality follows if the step-size n is sufficiently small.
Case 2 (2ci2 ≤ At ≤ 4c2i = c): Again by inequality (22)
At+i ≤ At ∙ exp (-nGtd (At - 2c2))
≤ At ≤ 4ci2 .
Thus, we have shown that At+i ≤ 4ci2 = c in both cases. Since we assumed the induction hypothesis
to hold at step t, these two cases are exhaustive, and hence the induction is complete.	■
The next lemma proves an upper bound on the difference between the inner product between θ(t+i) ∙
μ2 and the corresponding inner product at iteration t. Since we start at θ(0t, unrolling this over t
steps gives us an upper bound on inner product θ(t) ∙ μ2 for any t ≥ 0.
Lemma C.4. There is an absolute constant c such that, for all large enough C, if the step-size n is
sufficiently small then on a good run, for all t ∈ {0, 1, . . .}
(θ(t+it - θ(t)) ∙(-μ2) ≤ cn⅛ X '(tt.
i∈[n]
28
Published as a conference paper at ICLR 2022
Proof By the definition of a gradient descent step
(θ(t+1) - θ(t)) ∙(-μ2)= η X 件％ ∙(-μ2)
i∈[n]
≤ 3ηkμk X 'P + ciηkμkplog(n√δ) X 'it)
i∈N	i∈P
≤) 3ηk2μf ∙乎 X '(t) + cιηkμkPlOgWδ) ∙ c2npl X '(t)
i∈[n]	i∈[n]
=3c2ηkμk2 |P| (3 + P⅞Wδ!! X '(t)
1 —∙v V∣p∣ + λ⅛ i
≤ 3c2ηkμk2 max ( N, Pogl/δ1 ) X '(t),
where (i) follows by Lemma B.2 and (ii) follows by the loss ratio bound in Lemma C.3. Since by
assumption ∣∣μk2 ≥ Cn2 log(n∕δ), We infer that
√⅞∕δ)	1 凹=1
kμk	≤ n ≤ ∣P∣ = T
Thus,
(θ(t+1) - θ(t)) ∙ (-μ2) ≤ 3c2ηkμk2 max IN, PogfZX '(t)
cηkμk2 X '(t)
i∈[n]
Wrapping up our proof.
With these lemmas in place We are noW ready to prove our result. Let us restate it here.
Theorem 4.3. Let q 〜N(0, Id×d). There exist constants c and C such that, for all large enough
C and for any 0 < δ < 1/C, under the assumptions of this subsection the following holds. With
probability at least 1 - δ, training on S produces a maximum margin classifier θbMM satisfying:
-r . I— 「*	1
TestError[θMM]
P(χ,y)〜PtesthSign eMM ∙ x) = "i ≥ 1 ∙ φ
where Φ is the Gaussian cdf. Furthermore, if the imbalance ratio τ ≥ c0
1
ity at least 1 一 δ, TestError[Θmm] ≥ 1.
then with probabil-
Proof The test error for θbMM is
-r . I— 「*	1
TestError[θMM]
=P(x,y)〜Ptest 卜ign (bbMM ∙ x) = y
2Pq〜N(0, 2Ip×p)
≥ 2P
=2p
=2p
=2p
q~N(0, 2Ip×p)
q~N(0, 2Ip×p)
q~N(0, 2Ip×p)
hsign (θMM ∙ (μι + q)) = 1i + $Pq〜N(0,11p×p) hsign (θMM ∙ (μ2 + q)) = -1]
[sign (bMM ∙ (μ2 + q)) = -1
[-θMM ∙ μ2 - θMM ∙ q < 0
[-θMM ∙ q < -θMM ∙ (-μ2)]
'ξ〜N(0,1) [ξ < -θMM ∙ (—μ2∕∣	(Since -θMM ∙ q 〜N(O, I))
H / 公	/	∖ ∖
Φ(-Θmm ∙ (-μ2))
(23)
2
29
Published as a conference paper at ICLR 2022
where Φ is the Gaussian cumulative distribution function.
With this inequality in place, We want to prove an upper bound on (Θmm ∙ (-μ2)) . Now, since
kθ(t)k → bMM, it suffices to prove an upper bound on (limt→∞ θ1)θ(-μ2)) instead.
To do this, going forward let us assume that a good run (see Definition B.3) occurs. Lemma B.2
guarantees that this happens with probability at least 1 - δ.
Let t0 be the constant from Lemma C.2, then by Lemma C.4 we have that for any t > t0
t0	t
θ⑴∙ (-μ2)	=	θ⑼∙ (-μ2)	+ X(θ(s)	-	θ(ST)) ∙ (-μ2) +	X	(θ(S)	-	θ(ST))	∙ (-μ2)
s=1	s=t0 +1
≤ψ+
cιηkμk2
τ
t-1
X X '(s),
S=t0 i∈[n]
(24)
where we define ψ := θ(0) ∙ (-μ2) + PS=ι (θ(s) - θ(s-1)) ∙ (-μ2).
On the other hand, by repeatedly applying Lemma C.2 we get that
kθ(t)k≥kθ(t0)k + C2η∏X X 'is).
S=t0 i∈[n]
(25)
Furthermore, since kθ(t) k → ∞ (by Ji et al., 2020, Lemma 2) and
t t
kθ(t+1)k = θ(0) + η XX '(s) Zi ≤ η XX 'Mak
t
≤ c3ηd X X '(s)	(by Part 1 of Lemma B.2)
S=0 i∈[n]
t0-1	t
=c3ηd X X '(s) + c3ηd X X `((Ss
S=0 i∈[n]	S=t0 i∈[n]
we can conclude that PS = t0 Pi∈[n] '(s) → ∞.
Thus combining inequalities (24) and (25) we get that
1. θ⑴∙(-μ2)<∣. ψ + j PS=t0 Pi∈[n]'(s)	C4 ∕nιl ll2
lim	≤ lim	=	kμk2.
t→∞	kθ k	t→∞ kθ(t0)k + c2η√j PS=toPi∈[n]'is)	TVd
Plugging this upper bound into inequality (23) completes the proof.
D	Early- s topping before models interpolate
Prior works (Sagawa et al., 2019; Byrd & Lipton, 2019) found that adding regularization such as
strong L2 regularization and early stopping can partially restore the effects of importance weights
at the cost of not interpolating the training set. As such, we compare the performance of our
polynomially-tailed loss against cross-entropy when models are early-stopped, using the same set-
tings as before. For each run here, we select the model checkpoint with the best weighted validation
accuracy and evaluate the checkpoint on the test set. Figure 5 compares the test accuracies of the
early-stopped models (IW-ES (w),IW-ES (WC)) against reweighted models trained past interpolation
(IW (W)).
Consistent with prior works, we see that early stopping as a form of regularization improves the test
accuracies of both loss functions when used with importance weights. Our polynomially-tailed loss
30
Published as a conference paper at ICLR 2022
70
Imbalanced Binary CIFAR10
50505
66554
...................
00000
ycaruccA tseT
IW (W)	IW-ES (W)	IW-ES (W3/2)
Figure 5: Polynomially-tailed loss versus cross-entropy loss on a label shift dataset and a subpop-
ulation shift dataset for neural networks early-stopped according to the best weighted validation
accuracy. * and ** indicate P < 0.05 and P < 0.005 statistical significance, respectively. While
early stopping is effective for both losses, our polynomially-tailed loss performs even better on im-
balanced binary CIFAR10.
ycaruccA tse
Early-stopped
0.90
0.85
0.80
0.75
Subsampled CelebA
IW (W)	IW-ES(W)	IW-ES (w2)
gives test accuracies that are better than or similar to cross-entropy in all weighted loss scenarios.
The gain over cross-entropy is statistically significant in the binary CIFAR10 runs. On subsampled
CelebA, the polynomially-tailed loss with squared weights attains the highest mean test accuracy
out of all settings.
E Experimental details
E.1 Hyperparameters
E.1.1 FOR SECTION 5.1
Imbalanced binary CIFAR10 experiments. We use the same convolutional neural network ar-
chitecture with random initialization as (Byrd & Lipton, 2019). We train for 400 epochs with SGD
with a batch size of 64. We chose hyperparameters that resulted in stable training for each setting.
We use a constant 0.001 learning rate with 0.9 momentum for “No IW”, “IW (w)”，and “IW-ES (w)”.
We use a constant 0.008 learning rate with no momentum for “IW (w3/2)”, and “IW-ES (w3/2)”.
Subsampled CelebA experiments. We use a ResNet50 architecture with ImageNet initialization
as done in (Sagawa et al., 2019). Due to the large dataset size, we use only 2% of the full training
dataset as mentioned in the main text. Reducing the computation requirements allows us to perform
statistical evaluations of the results over sufficiently many seeds. We train for 100 epochs with SGD
with a batch size of 64. We chose hyperparameters that resulted in stable training for each setting.
We use a constant 0.0004 learning rate with 0.9 momentum for all settings.
E.1.2 For Section 5.2
To fairly evaluate every method, we grid search the hyperparamters for each method extensively. We
kept the same architectural setup as before, but adjust the optimization settings such that the models
train until they interpolate the training data.
Imbalanced binary CIFAR10 experiments.
•	Cross entropy + Class Undersampling: We use the same hyperparameters as Cross entropy
on the full dataset and average results over random undersamplings plus initializations
•	LDAM: Following the original paper Cao et al. (2019), we grid search the margin hyper-
parameter over [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]. We found 1.0 to be the best.
•	CDT Loss, LA Loss, VS Loss: Following Kini et al. (2021), we tune τ and γ with
τ = 0 being CDT loss and γ = 0 being LA Loss. We grid search across (τ, γ) ∈
[0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0] × [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]. We found τ = 3 to be the
best for LA loss, γ = 0.5 to be the best for CDT loss, and τ = 3, γ = 0.3 to be the best
31
Published as a conference paper at ICLR 2022
for VS Loss. Unlike the original paper (Kini et al., 2021), we found that the best hyperpa-
rameters of VS loss are similar to those of LA Loss. We initially followed the recommen-
dations of Kini et al. (2021) and grid searched over (τ, γ) ∈ [0.5, 0.75, 1.0, 1.25, 1.5] ×
[0.05, 0.1, 0.15, 0.2], but it gave VS loss poor performance (mean test acc was only 58.6%
using the best hyperparameters τ = 1, γ = 0.05). Hence, we did a more extensive tuning
afterwards.
•	Poly-tailed Loss: We grid search over (α, c)	∈	[0.25, 0.5, 1.0, 2.0, 4.0] ×
[1.0, 1.5, 2.0, 2.5, 3.0]. We found α = 2 and exponent c = 3 to be the best.
Subsampled CelebA.
•	Cross entropy + Group Undersampling: We use the same hyperparameters as Cross entropy
on the full dataset and average results over random undersamplings plus initializations.
•	VS Loss: We follow the tuning procedure in Kini et al. (2021) for group-sensitive VS Loss
and grid search over γ ∈ [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]. We found γ = 0.4 to be
the best.
•	Poly Loss: We grid search over (α, c) ∈ [1, 2, 3] × [1.0, 1.5, 2.0, 2.5, 3.0]. We found α = 2
and exponent c = 2.5 to be the best.
•	Cross entropy + DRO: We tune the adversarial step size ηq over
[0.0025, 0.005, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1]. We found ηq = 0.05 to be the best.
•	VS Loss + DRO: We used the best γ from VS Loss alone and ηq = 0.05 for DRO.
•	Poly Loss + DRO: We used the best α from Poly Loss alone and ηq = 0.05 for DRO.
E.2 Importance weight computation.
Here we explain in greater detail how we compute the importance weights. Note that the scale of the
importance weights is important in ensuring stable training. Let there be G groups of subpopulations
in the training and testing set. G = 2 for binary CIFAR10 and G = 4 for CelebA. Let n1, . . . , nG
be the counts of each group in the training set. Let n be the total number of training examples. In
our case, the test set is balanced across the groups. Our procedure for computing weights is:
1.	For each example i that belongs to group g, compute the unbiased weight: Wi J 1/ng.
2.	Exponentiate the weights by C if necessary: Wi J Wc No importance weighting corre-
sponds to c = 0. Unbiased importance weighting corresponds to c = 1.
3.	To adjust for the scale of weights, normalize by the average exponentiated weight across
the full training set: Wi J Wi / Pjn=1 Wi . Only normalizing across each minibatch can
result in unstable training, especially when a minibatch contains no representatives from a
group.
E.3 Early-stopping metric.
We use the checkpoint with the highest importance weighted validation accuracy when early stop-
ping. We compute separate importance weights of the validation set with respect to the test set, and
reweight the validation accuracy using unbiased weights, even when the training weights are biased.
Our procedure allows for the situation where the validation set is not exactly the same distribution
as the training set.
32
Published as a conference paper at ICLR 2022
E.4 Exact numerical results of our experiments
Dataset	Setting	Cross Entropy		Poly-tailed Loss		
		mean	std. err.	mean	std. err.	p-value
Subsampled CelebA Early-stopped	IW	79.3%	0.4%	80.7%	0.6%	0.014
	IW-ES	84.9%	0.7%	84.0%	0.9%	0.762
	IW-Exp-ES	85.1%	0.7%	85.9%	0.8%	0.202
Interpolating	IW	79.3%	0.4%	80.7%	0.6%	0.014
	IW-Exp	78.7%	0.4%	82.7%	0.4%	0.000
	No IW	79.6%	0.4%	78.5%	0.4%	0.999
Binary CIFAR10	Early-stopped	IW	57.8%	0.4%	60.4%	0.3%	0.000
	IW-ES	62.5%	0.7%	63.4%	0.5%	0.022
	IW-Exp-ES	61.2%	0.8%	63.5%	0.6%	0.001
Interpolating	IW	57.8%	0.4%	60.4%	0.3%	0.000
	IW-Exp	57.4%	0.6%	63.0%	0.6%	0.000
	No IW	60.5%	0.8%	56.4%	0.4%	1.000
Table 1: Numerical results corresponding to Figure 3 and 5. Here “Exp” corresponds to exponenti-
ated weights. We use 3/2 and 2 as the exponents in Binary CIFAR10 and CelebA respectively. We
use α = 1 for all of these experiments without tuning α.
Setting	Summary of method	Best settings	Mean test acc	Std err
Cross Entropy		N/A	60.5%	0.8%
Cross Entropy + IW	Classical importance weight- ing	N/A	57.8%	0.4%
Cross Entropy + Class undersampling	Balance the classes by throw- ing away majority data	N/A	58.5%	1.4%
LDAM	Add class dependent con- stants to logits	largest margin=1.0	58.7%	1.6%
CDT	Multiply class dependent con- stants by logits	γ = 0.5	60.2%	1.6%
LA Loss	Add class dependent con- stants to logits	τ=3	66.9%	2.1%
VS Loss (best setting around range suggested by paper)	Multiply by and add class de- pendent constants to logits	τ = 1, γ = 0.05	58.6%	1.4%
VS Loss (our own tun- ing)	Multiply by and add class de- pendent constants to logits	τ = 3, γ = 0.3	69.3%	0.9%
Poly-tailed Loss	Change loss function and exponentiate	importance weights	α = 2,W3	67.3%	0.8%
Table 2: Results on imbalanced binary CIFAR10, corresponding to Figure 4 (left).
33
Published as a conference paper at ICLR 2022
Setting	Method summary	Best set- tings	Mean test acc	Std err	Worst group test acc	Std err
Cross Entropy		N/A	79.6%	1.2%	43.1%	2.9%
Cross Entropy + Group undersampling	Balance the groups by throwing away overrep- resented data	N/A	82.3%	3.2%	72.1%	8.6%
Cross Entropy + IW	Classical importance weighting	N/A	79.3%	0.4%	43.1%	2.9%
VS Loss	Multiply by and add class dependent con- stants to logits	γ=0.4	82.6%	0.7%	51.3%	1.5%
Poly-tailed Loss	Change loss function and exponentiate impor- tance weights	α = 2, w2.5	82.4%	0.8%	53.3%	1.5%
Cross Entropy + DRO		N/A	84.8%	0.9%	60.4%	2.4%
VS Loss + DRO	Change loss function and use DRO	γ=0.4	83.2%	0.9%	58.2%	2.0%
Poly-tailed Loss + DRO	Change loss function and useDRO	a = 2	86.7%	1.1%	66.5%	1.9%
Table 3: Results on subsampled CelebA, corresponding to Figure 4 (right)
34