Published as a conference paper at ICLR 2022
Towards General Function Approximation in
Zero-Sum Markov Games
Baihe Huang
School of Mathematical Sciences
Peking University
baihehuang@pku.edu.cn
Jason D. Lee
Department of Electrical and Computer Engineering
Princeton University
Jasondl@princeton.edu
Zhaoran Wang
Departments of Industrial Engineering & Management Sciences
Northwestern University
zhaoranwang@gmail.com
Zhuoran Yang
Department of Statistics and Data Science
Yale University
zhuoran.yang@yale.edu
Ab stract
This paper considers two-player zero-sum finite-horizon Markov games with si-
multaneous moves. The study focuses on the challenging settings where the value
function or the model is parameterized by general function classes. Provably effi-
cient algorithms for both decoupled and coordinated settings are developed. In the
decoupled setting where the agent controls a single player and plays against an ar-
bitrary opponent, we propose a new model-free algorithm. The sample complexity
is governed by the Minimax Eluder dimension—a new dimension of the function
class in Markov games. As a special case, this method improves the state-of-the-art
algorithm by a √d factor in the regret when the reward function and transition
kernel are parameterized with d-dimensional linear features. In the coordinated
setting where both players are controlled by the agent, we propose a model-based
algorithm and a model-free algorithm. In the model-based algorithm, we prove that
sample complexity can be bounded by a generalization of Witness rank to Markov
games. The model-free algorithm enjoys a √K-regret upper bound where K is the
number of episodes.
1	Introduction
In competitive reinforcement learning, there are two agents competing against each other by taking
actions. Their actions together determine the state evolutions and rewards. Function approximation,
especially deep neural networks (LeCun et al., 2015), contributes to the success of RL in many real
world applications, such as Atari (Mnih et al., 2013), Go (Silver et al., 2015), autonomous driving
(Shalev-Shwartz et al., 2016), Texas holdem poker (Sandholm, 2017), and Dota (Berner et al., 2019).
The goal of competitive reinforcement learning aims to learn the Nash Equilibrium (NE) policy in a
trial-and-error fashion. In a NE, no agent can be better off by unilaterally deviating from her policy.
Most of the existing sample efficient competitive RL algorithms focus on the tabular or linear function
approximation cases (pWrolat et al., 20l7; Bai & Jin., 2020; Bai et al., 2021; 2020; Xie et al., 2020;
Zhang et al., 2020). The huge empirical success of neural network based RL methods has remained
largely unexplained. Thus, there exists a wide gap between theory and practice in competitive RL
with general function approximation. One demanding question to ask is:
Can we establish provably efficient competitive RL algorithms with general function approximation?
1
Published as a conference paper at ICLR 2022
In this paper, we make a step towards answering this question by providing structural conditions
and complexity measures of Markov Games and function classes that allow for efficient learning.
We focus on episodic zero-sum Markov Game (MG) with simultaneous move, where each episode
consists of H time steps and two players act simultaneously at each time step. The state space is
arbitrarily large and can be infinite. We consider two function approximation settings: 1) use a
general function class F to approximate the action-value function (Q function); 2) use a general
function class M to approximate the environment.
To this end, we introduce algorithms based on optimistic principles. Due to subtle game-theoretic
issues, naive optimism is no longer working in Markov games where minimax optimization is per-
formed. To deal with this issue, we use the algorithmic idea called ‘alternate optimism’. Specifically,
we develop algorithms for both coordinated and decoupled setting. In the decoupled setting, the agent
controls one player and plays against an arbitrary and potentially adversarial opponent. The sample
efficiency is measured by the gap between the learned value and value of the Nash Equilibrium. In
the coordinated setting, the agent controls both players and the goal is to find the approximate Nash
Equilibrium, i.e. with small duality gap. We identify key complexity measures of function classes to
examine the effectiveness of elimination. By doing so, we prove upper bounds on sample complexity
and regrets of the presented procedures that are independent of the number of states.
Our contributions are summarized into the following two folds:
•	In the decoupled setting, We introduce Minimax ElUder dimension-a new complexity
measure for competitive RL problems. We propose an algorithm that incurs at most
O(H√&eK logNF) * 1 regret in K episodes where d，E denotes the Minimax Eluder di-
mension NF denotes the covering number of function class, with probability at least 1 - p.
AS a special case, this result improves Xie et al. (2020) by a √d multiplicative factor in their
setting when the reward function and transition kernel are linearly parameterized and d is
the dimension of feature mapping.
•	In coordinated settings, we propose both model-based and model free algorithms. In the
model-based setting, we generalize the witness rank (Sun et al., 2019a) to competitive
RL. We prove that Oe (H3W2/2 ) samples are enough to learn a policy -close to the
Nash Equilibrium, where W is witness rank. In the model-free setting, we develop algo-
rithm for agnostic learning with a candidate policy class Π. The algorithm incurs at most
O(H dK log(NFNΠ)) regret. Here d is a variant of Minimax Eluder dimension and NΠ
denotes the covering number of policy class.
1.1 Related works
There is a rich literature studying the learning and decision-making of Markov Games (Littman &
SzePeSVari,1996; Greenwald et al., 2003; Grau-Moya et al., 2018; P6rolat et al., 2018; Srinivasan
et al., 2018; Sidford et al., 2020; Wei et al., 2017; P6rolat et al., 2017; Bai & Jin., 2020; Bai et al.,
2021; 2020; Zhang et al., 2020; Zhao et al., 2021). The most related to us are perhaps (Xie et al.,
2020; Chen et al., 2021; Jin et al., 2021b), where the authors address the challenge of exploration-
exploitation tradeoff in large state spaces. Due to space constraint, a detailed literature discussion is
deferred to Appendix A.
2	Preliminaries
We consider two-player zero-sum simultaneous-moves episodic Markov game, defined by the tuple
(S, A1, A2, r, P, H), where S is the state space, Ai is a finite set of actions that player i ∈ {1, 2} can
take, r is the reward function, P is the transition kernel and H is the number of time steps. At each
time step h ∈ [H], player P1 and P2 take actions a ∈ A1 and b ∈ A2 respectively upon observing
the state x ∈ S, and then both receive the reward rh(x, a, b). The system then transitions to a new
state χ0 〜Ph(∙∣χ, a, b) according to the transition kernel P. Throughout this paper, we assume for
simplicity that A1 = A2 = A and that the rewards rh(x, a, b) are deterministic functions of the tuple
(x, a, b) taking values in [-1, 1]. Turn-based games are special cases of simultaneous games in the
1
1We use O to hide logarithmic terms in H, 1/p and K.
2
Published as a conference paper at ICLR 2022
sense that at each state the reward and transition are independent of one player’s action (Xie et al.,
2020). Generalizations to A1 6= A2 and stochastic reward are also straightforward.
Denote by ∆ := ∆(A) the probability simplex over the action space A. A stochastic policy of P1 is
a length-H sequence of functions π := {πh : S 7→ ∆}h∈[H]. At each step h ∈ [H] and state x ∈ S,
P1 takes an action sampled from the distribution πh(x) over A. Similarly, a stochastic policy of P2
is given by the sequence ν := {νh : S 7→ ∆}h∈[H] . The learning happens in K episodes. In each
episode k, each player P1 and P2 proposes a policy πk and νk respectively based on history up to the
end of episode k - 1 and then executes the policy to observe the trajectories {xkh, akh, bkh}hH=1.
For a fixed policy pair (π, ν), the value function and Q functions for zero-sum game is defined by
H
Vhπ0,ν (x) := E	rh(xh, ah, bh)|xh0 = x ,
h=h0
H
Qπh0,ν (x, a, b) := E	rh(xh, ah, bh)|xh0 = x, ah0 = a, bh0 = b ,
h=h0
where the expectation is over ah 〜 ∏h(xh), bh 〜 Vh(Xh) and xh+ι 〜 Ph(∙∣xh, ah, bh). V∏,ν and
Q1π,ν are often abbreviated to V π,ν and Qπ,ν. In zero-sum games, PI aims to maximize V π,ν (x1)
and P2 aims to minimize V π,ν (x1). Given policy π of P1, the best response policy of P2 is defined
by Vn := argmi□νVπ,ν(xι). Similarly given policy V of P2, the best response policy of P1 is defined
by ∏V ：= argmax∏ Vπ,ν(xι). Then from definitions,
Vπ,ν∏ (xι) ≤ Vπ*,ν* (xι) ≤ VπV,ν(xι)
and the equality holds for the Nash Equilibrium (NE) of the game (∏*, ν*). We abbreviate Vπ* ,ν* (xi)
as V * and Qπ*,ν* (xi) as Q*. V * is often referred to as the value of the game. As common in Markov
games literature (P6rolat et al., 2015), we will use the following Bellman operator
ThQh+ι(x,a, b) ：= r(x, a,b)+ E	[maxmin旧水“Qh+i(x0, ∙, •)],	(1)
x0 〜Ph(.∣x,a,b) n0 ν0
and the following Bellman operator for fixed P1’s policy
Th Qh+i(x, a,b) ：= r(x,a, b)+ E	[min E∏,ν0 Qh+i(x0, ∙, ∙)].	(2)
x0 〜Ph(∙∣x,a,b) V0
Notations For a integer H, [H] denotes the set {1, 2, . . . , H}. For a finite set S, |S| denotes its
cardinality. For a matrix A ∈ Rd, Ai,* and A*,j denote the i-th row andj-th column of A respectively.
For a function f : S 7→ R, kfk∞ denotes sups∈S |f (s)|. We use N(F, ) to denote the -covering
number ofF under metric d(f, g) = maxh kfh - ghk∞.
3	Decoupled setting
In decoupled setting, the algorithm controls P1 to play against P2. In k-th episode, P1 chooses a
policy πk and P2 chooses a policy Vk based on πk . Then P1 and P2 interact in the Markov Game by
playing πk and Vk respectively, and observe the rewards and states in this episode. Notice that P2
can be adversarial and its policy is never revealed, meaning that P1 can only maximize its reward by
playing π* . The goal is to thus minimize the following ‘value regret’
K
Reg(K)=X[Vi*(xi)-Viπk,νk(xi)].	(3)
k=i
A small value regret indicates that we learn the value of MG. If an algorithm obtains an upper bound
on value regret that scales sublinearly with K for all Vk, then it can not be exploited by opponents.
3.1	Function approximation
We consider function class F = Fi ×∙∙∙× FH where Fh ⊂ {f : S×Aι ×A2 → [0,1]} to provide
function approximations for Q* — the Q-function of Nash Equilibrium. For a policy pair (π, V) and
function f we abuse the notations and use f (x, ∏, V) = Ea〜∏,b〜ν [f (x, a, b)] to denote the expected
reward of executing (π, V) in function f. We make the following assumptions about F.
3
Published as a conference paper at ICLR 2022
Assumption 3.1 (Realizability). We assume Qh ∈ Fh forall h ∈ [H].
Assumption 3.2 (Completeness). We assume for all h ∈ [H] and f ∈ Fh+1, Thf ∈ Fh.
Realizability means the function class contains the target Q function. Completeness says the function
class is closed under the Bellman operator. These assumptions are common in value function
approximation literature (e.g. Jin et al. (2020); Wang et al. (2020b); Jin et al. (2021a)). Note that they
are weaker than Xie et al. (2020), as their setting satisfies Thf ∈ Fh for any action-value function f.
For a function f ∈ F, we denote the max-min policy of P1 by πf . Specifically,
(πf)h(x) := argmaxπmin fh(x, π, ν),∀h ∈ [H].
ν
Similarly we denote the min-max policy of P2 by νf. Given policy π of P1, the best response policy
of P2 in terms of function f is defined by
(νπf)h (x) := argminνfh(x,πh, ν),∀h ∈ [H].
Similarly given policy ν of P2, the best response policy of P1 in terms of function f is denoted by πνf .
When there is no confusion, we will drop the subscript h for simplicity.
Learnability requires the function class to have bounded complexity. We introduce Minimax Eluder
dimension to capture the structural complexity of function classes in zero-sum games. To define,
we use -independence of distributions and Distributional Eluder dimension (DE) from Russo &
Van Roy (2013); Jin et al. (2021a).
Definition 3.3 (-independence of distributions). Let G be a function class defined on X, and
ν, μι,... ,μn be probability measures over X. We say that V is E-independent of {μι,..., μn} with
respect to G ifthere exists g ∈ G such that -∖Z∑n=ι(Eμi [g])2 ≤ E but | EV[g] ] ≥ 匕
Definition 3.4 (Distributional Eluder dimension (DE)). Let G be a function class defined on X, and
D be a family of probability measures over X. The distributional Eluder dimension dimDE (G, D, E)
is the length of the longest sequence {ρ1, . . . , ρn} ⊂ D such that there exists E0 ≥ E where ρi is
E0-independent of{ρ1, . . . , ρi-1}for all i ∈ [n].
Now we introduce Minimax Eluder dimension. Recall the minimax Bellman operator from Eq (1):
Thfh+1(x, a, b) := r(x, a, b) + E	[max min fh+1(x0, π0, ν0)]
x0 〜Ph(∙∣x,a,b) ∏0	V0
thus Minimax Eluder dimension is the Eluder dimension on the Bellman residues with respect to the
above minimax Bellman operator.
Definition 3.5 (Minimax Eluder dimension). Let (I - Th)F := {fh - Thfh+1 : f ∈ F} be the class
of minimax residuals induced by function class F at level h and D∆ = {D∆,h}h∈[H] where D∆,h :=
{δ(x, a, b) : x ∈ S, a ∈ A1, b ∈ A2} is the set of Dirac measures on state-action pair (x, a, b). The E-
Minimax Eluder dimension ofF is defined by dimME (F, E) := maxh∈[H] dimDE((I -Th)F, D∆, E).
3.2	Optimistic Nash Elimination for Markov Games
Now we introduce Optimistic Nash Elimination for Markov Games (ONEMG), presented in Algo-
rithm 1. This algorithm maintains a confidence set Vk that always contains the Q*, and sequentially
eliminates inaccurate hypothesis from it. In k-th episode, P1 first optimally chooses a value function
f k in V k-1 that maximizes the value of the game. Intuitively, this step performs optimistic planning
in the pessimistic scenarios, i.e. assuming P2 plays the best response. Next, it plays against P2 and
augments the data from this episode into replay buffer B. The confidence set V k is then updated by
keeping the functions f with small minimax Bellman errors fh - Thfh+1, in Line 12. To estimate
fh - Thfh+1, we use EBh(fh,fh+1) - infg∈Fh EBh (g, fh+1), a standard variance reduction trick
to avoid the double-sample issue. It can be shown that when the value function class is complete,
EBh(fh, fh+1) - infg∈Fh EBh (g, fh+1) is an unbiased estimator of fh - Thfh+1 (Lemma B.1).
Notice that unlike many previous works that add optimistic bonus on every state-action pairs (Xie
et al., 2020; Chen et al., 2021), Algorithm 1 only takes optimism in the initial state. Instead, the
constraint set contains every function that has low Bellman residue on all trajectories in the replay
buffer. This ‘global optimism’ technique trades computational efficiency for better sample complexity,
and can be found in many previous work in Markov Decision process (Zanette et al., 2020; Jin et al.,
2021a). As we will see later, it is also useful in the coordinated setting.
4
Published as a conference paper at ICLR 2022
Algorithm 1 Optimistic Nash Elimination for Markov Games (ONEMG)
1:	Input: Function class F
2:	Initialize Bh = 0, h ∈ [H ], V0-F
3:	Set β J C log(N(F, 1/K) ∙ HK/p) for some large constant C
4:	for k = 1, 2, . . . , K do
5:	Compute fk - argmaxf ∈V k-1 f1 (x1, πf, νf), let πk - πfk
6:	for step h = 1, 2, . . . , H do
7:	Interact with environment by playing action ahh 〜∏k (Xh)
8:	Observe reward rhk , opponent’s action bkh and move to next state xkh+1
9:	end for
10:	Update Bh -Bh∪{(xkh,akh,bkh,rhk,xkh+1)},∀h∈ [H]
11:	For all ξ, ζ ∈ F and h ∈ [H], let
k
EBh(ξh,ζh+1) =	[ξh(xτh,aτh,bτh) - rhτ - ζh+1(xτh+1,πζh+1,νζh+1)]2
τ=1
12:	Set
13:	end for
Vk -{f ∈F : EBh (fh,fh+ι) ≤
inf EBh(g,fh+1)+β,∀h∈ [H]}
g∈Fh
3.3	Theoretical results
In this section we present the theoretical guarantee for Algorithm 1. The proof is deferred to
Appendix B.
Theorem 3.6. Under Assumption 3.1 and Assumption 3.2, the regret Eq (3) of Algorithm 1 is upper
bounded by
Reg(K) ≤ O(HPK ∙ dME ∙ log(HKZ))
with probability at least 1 一 P. Here "me = dimME(F, ,1/K) and Z = N(F, 1/K)/p.
As this theorem suggests, as long as the function class has finite Minimax EIUder dimension and
covering number, P1 can achieve √K-regret against P2. Notice that when P2 always plays the best
response of P1’s policies, this regret guarantee indicates that algorithm eventually learns the Nash
Equilibrium. In this case, the algorithm takes Oe(H2dlog(|F|)/2) samples to learn a policy π that is
e-close to π*.
When the opponent plays dummy actions, the Markov Game can be seen as an MDP and the value
of the Markov Game is the maximum cumulative sum of rewards one can achieve in this MDP. In
this case, our result reduces to O(H ∙ √K ∙ dbe log(N(F, 1/K))) where dbe IS the Bellman eluder
dimension in Jin et al. (2021a).
In particular, when the function class is finite, the regret becomes O(H,KdME log(HK|F|/p))
that depends logarithmically on the cardinality of F. Moreover, when the reward and transition
kernel have linear structures, i.e. rh(x, a, b) = φ(x, a, b)>θh and Ph(∙∣x, a, b) = φ(x, a, b)>μ%(∙)
where φ(χ, a, b) ∈ Rd, then dme (F, ,1/K) = log N(F, 1/K)= O(d) and the regret becomes
O(Hdy/K)2. Thus we improve Xie et al. (2020) by a √d factor. We also provide a simpler algorithm
tailored for this setting, see Appendix B.1 for details.
Algorithm 1 solves a non-convex optimization problem in Line 5 with highly non-convex constraint
set (Line 12). In general, it is computationally inefficient. Even when reduced to linear MDP,
i.e. the opponent plays dummy actions and the transition matrices and the rewards have low rank
structures, it is not known if the same te(H√d2K) regret can be achieved with computationally
efficient algorithms (Zanette et al., 2020). Designing computationally efficient algorithms with
near-optimal sample efficiency is an interesting further direction.
2Here we use Oe to omit the logarithm factors in K, d, H and 1/p.
5
Published as a conference paper at ICLR 2022
4	Coordinated setting
In the coordinated setting, the agent can control both P1 and P2. The goal is to find the -approximate
Nash equilibrium (π, ν) in the sense that
VπKν(x1) - Vπ,ν∏ (x1) ≤ e	(4)
by playing P1 and P2 against each other in the Markov game. In the following sections, we propose
both model-based and model-free methods to deal with this problem.
4.1	Model-based algorithm
In model-based methods, we make use of a model class M of candidate models to approximate
the true model M*. We are also given a test function class G to track model misfit. Given a
model M ∈ M, we use Qm(x, a, b), rM(x, a, b) and PM(∙∣x, a, b) to denote the Q-function, reward
function and transition kernel of executing action a and b at state x in model M. For simplicity, we
use (r, x)〜M to represent r 〜rM (x, a, b) and X 〜 PM(∙∣x,a, b). We denote the NE policy in
model M as πM and VM. For policy π, V and model M, we use VπM to denote the best response of π
in model M and πνM is defined similarly.
4.1.1	Alternate Optimistic Model Elimination (AOME)
The model-based method, Alternate Optimistic Model Elimination (AOME), is presented in Algo-
rithm 4. It maintains a constraint set Mk of candidate models. Throughout K iterations, AOME
makes sure that the true model M* always belongs Mk for all k ∈ [K], and sequentially eliminates
incorrect models from Mk.
However, the idea of being optimistic only at initial state (Zanette et al., 2020; Jin et al., 2021a)
is not directly applicable to this scenario. Since the objective to be optimized is the duality gap,
the target policies being considered are thus policies pairs (πν*, ν) and (π, νπ*). However, πν* and
νπ* are not available to the agents. Therefore, the performances of proposed policies are evaluated
on out-of-distribution trajectories, meaning that optimism can not be guaranteed. This causes the
distribution shift issue. In worst cases, low Bellman errors in the collected trajectories provide no
information for the trajectories collected by πν* , ν and π, νπ* .
To address this issue, the algorithm performs global planning by applying the idea of alternate
optimism to model-based methods. In k-th episode, the algorithm first solves the optimization
arg maxM∈M QM(x1, πM, νM) and let πk = πM1k . This optimization problem
problem M1k
corresponds to finding the optimistic estimate of the value of the game. Indeed, notice that M* ∈ Mk
implies QM1k (x1, πk, νk) ≥ QM1k (x1, πk, νπMk1 ) ≥ V* (x1), by optimality of M1k. Next, it solves
the second optimization problem M2k = argminM∈M QM(x1, πM1k , νM) and let Vk 一 VMk. This
kk .*
corresponds to finding the pessimistic estimate of Vπ ,νπk (x1). In fact, we see that M* ∈ Mk
implies QM2k (x1, πk, Vk) ≤ V πk,νπ*k (x1), by optimality of M2k. This approach appears in Wei et al.
(2017) to guarantee convergence to Nash equilibrium, where it is referred to as ‘Maximin-EVI’.
In Line 8, the algorithm checks if the values of model M1k and M2k are close to the value of the true
model M* when executing policy πk and Vk. If the condition holds, we have
V *(x1) - V πk,νπ*k (x1) ≤QM1k(x1,πk,Vk)-QM2k(x1,πk,Vk) ≤,
which means that AOME finds policy πk that is -close the NE. One can also switch the order of
alternate optimism and obtain ν k that is -close the NE. We then terminate the process and output π k
and νk. If the algorithm does not terminate in Line 8, it applies the witnessed model misfit checking
method in Sun et al. (2019a). It starts by computing the empirical Bellman error of Markov games
defined as follows
n1
L(M1,M2,M, h) := X --[QM(Xh ah, bh) - (rh + QM(xh+ι, ∏M1, ν∏⅛ )].
i=1	1
(5)
6
Published as a conference paper at ICLR 2022
As proved in Appendix D, in this case Lb(M1k, M2k, Mjk, hk) must be greater than /(8H) for some
j ∈ [2] and hk ∈ [H]. Then the algorithm samples additional trajectories at level hk and shrinks the
constraint set by eliminating models with large empirical model misfit, which is defined as follow
n1
E(Mk,Mk,M,hk)=sup]T-	E	[g(xh,ah,bh,rh,xh)- g(xh,ah,b"/十方.(6)
g∈G M n (rh,Xh)〜M	+
We will show that this constraint set maintains more and more accurate models of the environment.
Due to space constraints, the algorithm and theory of our model-based method are deferred to
Appendix C.
4.2	Model free algorithm
In this section we propose a model-free algorithm. We consider a more general agnostic learning,
where The algorithm is given a policy class Π = Π1 × ∙∙∙ ∏h with Πh ⊂ {∏h : S → ∆}, h ∈
[H]. Notice by letting Π to be the class of NE policies induced by the action-value function
class F, we recover the original setup. The goal is to find policy π and ν from Π to minimize
the duality gap: maxπ0∈Π V π0,ν - minν0 ∈Π Vπ,ν0, by playing only policies from Π. Since only
policies from Π are considered, we overload the notation and define the optimal solution in Π as
∏* = arg maχ∏∈∏ mi□ν0∈∏ Vπ,ν and V* = argminν∈∏ max∏o∈∏ Vπ ,ν. When Π contains all
possible policies, ∏* and ν* is then the Nash equilibrium.
Similar to Section 3, We consider general function class F = Fi X …X FH where Fh ⊂
{f : S × A1 × A2 7→ [0, -]}. We overload the notations in Section 3 and use νπf to de-
note the best response of π from Π, namely νπf (x) := argminν∈Πf(x, π, ν). Similarly, we use
πf (x) := argmaxπ∈Πminν∈Π f(x, π, ν). We also use νπ* (x) := argminν∈ΠV π,ν to denote the best
response of π in the true model.
4.2.1	Alternate Optimistic Value Elimination (AOVE)
Notice that the duality gap can be decomposed as follows
gap = VπV,ν - VπV*,ν* + V琮*,ν* - Vπ*,ν∏* + Vπ*,ν∏* - Vπ,ν∏
×--------------V-------} X---------V--------} X--------V-------}
P2	Optimal	Pi
*	*	**
where the optimal part Vπν* ,ν - Vπ ,νπ* is fixed. Therefore we can consider only the P1 part here
and the P2 follows by symmetry. We are interested in the following policy regret
K
Reg(K) := X(V π*,νπ** - Vπk,νπ*k).	(7)
k=i
Policy regret measures the extent that P1 can be exploited by policies in Π. Our algorithm Alternate
Optimistic Value Elimination (AOVE), presented in Algorithm 2, works on minimizing this regret.
Similar to Algorithm 1, it also uses optimism in the initial state to address exploration and exploitation
trade-off and constructs a series of confidence sets of hypotheses with small Bellman errors. The
differences primarily lie in a different choice of optimism and a different series of confidence sets
constructed with the Bellman operator in Eq 2. Since the policy class Π is taken into consideration,
the algorithm maintains a constraint set B ⊂ Π X F of policy-function pairs. We use the Bellman
operator with regard to fixed P1’s policy, in Eq (2). Thus intuitively, this constraint set maintains
(π, f) pairs such that fh - Thπfh+i is small for all h ∈ [H].
We apply the ‘alternate optimistic’ principle, presented in Line 5 and Line 6. Intuitively, the algorithm
finds an optimistic planning of π and pessimistic planning of ν, together corresponding to a max-min
procedure. As such, we form an upper bound fi(xi, π, νπf) - gi(xi, πk, νπgk) of the duality gap.
Note that it is different from the ‘alternate optimistic’ used in model-based settings Wei et al. (2017)
in that the hypothesis chosen in Line 6 is constrained by the policy chosen in Line 5. This is because
in model-based methods, ‘plausible’ models in the confidence set guarantee small simulation errors
(Lemma C.6). However, this is not true for ‘plausible’ value functions in the confidence set.
The rest of the algorithm aims at minimizing this upper bound sequentially by eliminating bad
hypothesis in the constraint set. The elimination process occurs in Line 13, where the algo-
rithm uses history data to eliminate functions with large Bellman error. To estimate Bellman
7
Published as a conference paper at ICLR 2022
error, we use the similar method as in Algorithm 1, i.e. subtracting the variance of Thπ fh+1 by
g = arg infg∈Fh EDh (g, fh+1, π). Notice that a smaller value of fh - Thπ fh+1 indicates that function
approximation f is not easily exploited by P2 when P1 plays π.
As shown in Appendix D, (π*, Q*) is always in the constraint set, and the Bellman errors of the
rest hypothesis keep shrinking. Thus in the presented algorithm of AOVE, the regret of P1 part
Vπ*,νπ* - Vπ,“π is controlled. By symmetry, we can perform the same procedures on P2 part and
obtain an upper bound on Vπ*,ν - Vπ** ,ν*. Combining these together, we show sublinear regret rate
*	*	**
of Vπν* ,ν - Vπ ,νπ* . By regret to PAC conversion, this means that we find the policies that is the
best achievable in Π.
Algorithm 2 Alternate Optimistic Value Elimination (AOVE)
1:	Input: Function class F, policy class Π
2:	Initialize B0 J Π XF
3:	Set β J C log[N(F, 1/K) ∙ N(Π, 1/K) ∙ HK/p] for some large constant C
4:	for k = 1, 2, . . . , K do
5:	Find (πk, fk) J argmax(π,f)∈Vk-1f1(x1, π, νπf)
6:	Let g J argmin g1(x1, πk, νπgk ) and set νk J νπgk
g:(nk ,g)∈Vk-1
7:	for step h = 1, 2, . . . , H do
8:	P1 takes action ah ~ ∏k (Xh), P2 takes action b£ ~ Vh (Xh)
9:	Observe reward rhk and move to next state xkh+1
10:	end for
11:	Update Bh J Bh ∪ {(Xkh, akh, bkh,rhk,Xkh+1)},∀h ∈ [H]
12:	For all ξ, ζ ∈ F, π ∈ Π and h ∈ [H], let
k
EBh (ξh, ζh+1, π) = X[ξh(Xτh,aτh,bτh) - rhτ - ζh+1(Xτh+1, π, νπζh+1)]2
τ=1
13:	Update
Vk J{(∏,f) ∈ ∏ ×F : EBh (fh,fh+ι,∏) ≤ inf EBh (g,fh+ι,∏)+ β, ∀h ∈ [H]}
g∈Fh
14:	end for
4.2.2 Theoretical results
This section presents our theoretical guarantees of Algorithm 2. First, we introduce two assumptions
that take the policy class Π into consideration. Variants of these assumptions are also common in
batch RL (Jin et al., 2021c; Xie et al., 2021).
Assumption 4.1 (Π-realizability). For all π ∈ Π, Qπ,νπ* ∈ F.
Assumption 4.2 (Π-completeness). For all h ∈ [H], π ∈ Π, and f ∈ Fh+1, Thπf ∈ Fh holds.
Notice that Assumption 4.2 reduces to Q* realizability in MDP. Both two assumptions hold for
linear-parameterized Markov games studied in Xie et al. (2020). In fact, Xie et al. (2020) satisfies a
stronger property: for any policy pair π, ν and any h ∈ [H], there exists a vector w ∈ Rd such that
Qhπ,ν (X, a, b) = w>φ(X, a, b), ∀(X, a, b) ∈ S × A1 × A2.
Now we present the theory of Algorithm 2. First, we introduce the complexity measure in coordinated
setting, which is a variant of Minimax Eluder dimension by replacing the Bellman operator Eq (1)
with Eq (2). This Minimax Eluder dimension also allows a distribution family induced by function
class F which is cheap to evaluate in large state space (Jin et al., 2021a).
Definition 4.3 (Minimax Eluder dimension in Coordinated setting). Let (I - ThΠh+1 )F :=
{fh - Thπ fh+1 : f ∈ F, π ∈ Πh+1} be the class of residuals induced by the Bellman
operator Thπ. Consider the following distribution families: 1) D∆ = {D∆,h}h∈[H] where
D∆,h := {δ(X, a, b) : X ∈ S, a ∈ A1, b ∈ A2} is the set of Dirac measures on state-action
8
Published as a conference paper at ICLR 2022
pair (x, a, b); 2) DF = {DF,h}h∈[H] which is generated by executing (πf, νπgf) for f, g ∈ F.
The -Minimax Eluder dimension (in coordinated setting) of F is defined by dimME0 (F, ) :=
maxh∈[H] minD∈{D∆,DF} dimDE((I - ThΠh+1)F, D, ).
Now we present our main theory of Algorithm 2, where we make use of a variant of Minimax Eluder
dimension that depends on policy class Π. We use N (Π, ) to denote the covering number of Π under
metric d(π, π0) := maxh∈[H] maxf∈Fh,x∈S,b∈A |f(x, π, b) - f (x, π0, b)|.
Theorem 4.4. Suppose Assumption 4.1 and Assumption 4.2 holds. With probability at least 1 - p we
have the regret (Eq. (7)) of Algorithm 2 upper bounded by
Reg(K) ≤ O (HPK ∙ dME0 ∙ log[HKZ])
where "me，= dimME，(F,，1/K) and Z = N(F, 1/K)N(Π, 1/K)/p.
As shown in this theorem, the regret is sub-linear in number of episodes K and Eluder dimen-
sion dME， which match the regret bound in Markov decision process, e.g. in Jin et al. (2021a).
Through regret-to-PAC conversion, with high probability the algorithm can find -optimal policy
with O (H2Kdmeo log[N(F, 1/K)N(Π, 1/K)HK/p]/t2) sample complexity. When the reward
and transition kernel have linear structures, We also recover the O(Vd3K) regret of Xie et al. (2020).
Although our upper bound depends on the logarithm of covering number of Π, there are some cases
when it is small. For example, when the value-function class F is finite and Π is the induced policy
class of F as defined by Π = {πf, νπgf : f, g ∈ F}, then logNΠ() = log |F| and the regret is
O(HJKdMEo log2 |F|). Similarly,
if the model class M is finite and Π is induced policy class
of M as defined by Π = {∏m, VMM : M, M0 ∈ M}, then logNn(E) = log |M| and the regret
is
O(HjKdME'log2 |M|). In
agnostic setting which allows Π to have arbitrary structure, the
term log N(Π, 1/K) reflects the difficulty of learning with complex candidate policy sets and we
conjecture that this term is inevitable.
Similar to Algorithm 1, Algorithm 2 solves a non-convex optimization problem in Line 5-6 with
highly non-convex constraint set (Line 13). This step is particularly difficult when the function
class has complex structures. Algorithm 2 is in general computationally inefficient, even in cases of
linear function approximations. Indeed, even when the model is known, solving for the NE is PPAD
complete. Developing decentralized and provably efficient RL algorithm for multi-agent Markov
game seems a challenging but interesting future direction.
5 Discussion
In this paper we study function approximations in zero-sum simultaneous move Markov games. We
design sample efficient algorithms for both decoupled and coordinated learning settings and propose
Minimax Eluder dimension to characterize the complexity of decision making with value function
approximation. The analysis shows √T regret bounds and half-order dependence on the Minimax
Eluder dimension, matching the results in MDPs and improving the linear function approximations.
Our algorithms for coordinated setting are based on the idea of the ‘alternate optimism’, which also
shows applicability in model-based methods.
Acknowledgements
BH is supported by the Elite Undergraduate Training Program of School of Mathematical Sciences
at Peking University. JDL acknowledges support of the ARO under MURI Award W911NF-11-1-
0304, the Sloan Research Fellowship, NSF CCF 2002272, NSF IIS 2107304, and an ONR Young
Investigator Award.
References
Yasin Abbasi-Yadkori, Nevena Lazic, Csaba Szepesvari, and Gellert Weisz. Exploration-enhanced
POLITEX. arXiv preprint arXiv:1908.10479, 2019.
9
Published as a conference paper at ICLR 2022
Alekh Agarwal, Miroslav Dudk Satyen Kale, John Langford, and Robert E. SchaPire. Contextual
bandit learning with predictable rewards. In Proceedings of the 15th International Conference on
Artificial Intelligence and Statistics (AISTATS), 2012.
Alekh Agarwal, Sham M. Kakade, Jason D. Lee, and Gaurav Mahajan. On the theory of Policy gradi-
ent methods: OPtimality, aPProximation, and distribution shift. arXiv PrePrint arXiv:1908.00261,
2020.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deeP learning via over-
parameterization. In International Conference on Machine Learning (ICML), pp. 242-252, 2019.
Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforce-
ment learning. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pp. 263-272. JMLR. org, 2017.
Yu Bai and Chi Jin. Provable self-play algorithms for competitive reinforcement learning. arXiv
preprint arXiv:2002.04017, 2020.
Yu Bai, Chi Jin, and Tiancheng Yu. Near-optimal reinforcement learning with self-play. arXiv
preprint arXiv:2006.12007, 2020.
Yu Bai, Chi Jin, Huan Wang, and Caiming Xiong. Sample-efficient learning of stackelberg equilibria
in general-sum games. arXiv preprint arXiv:2102.11494, 2021.
Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In
Proceedings of the 12th International Conference on Machine Learning, 1995.
Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy
Dennison, David Farhi, Quirin Fischer, Shariq Hashme, and Chris Hesse. Dota 2 with large scale
deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and deep
neural networks. In Advances in neural information processing systems (NeurIPS), 2019.
Zixiang Chen, Dongruo Zhou, and Quanquan Gu. Almost optimal algorithms for two-player markov
games with linear function approximation. arXiv preprint arXiv:2102.07404, 2021.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning (ICML), pp.
1675-1685. PMLR, 2019a.
Simon S. Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford.
Provably efficient RL with rich observations via latent state decoding. In International Conference
on Machine Learning, pp. 1665-1674. PMLR, 2019b.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations
(ICLR), 2019c.
Simon S. Du, Sham M. Kakade, Jason D. Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and
Ruosong Wang. Bilinear classes: A structural framework for provable generalization in rl. arXiv
preprint arXiv:2103.10897, 2021.
Jianqing Fan, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang. A theoretical analysis of deep
q-learning. In Learning for Dynamics and Control, pp. 486-489. PMLR, 2020.
Jordi Grau-Moya, Felix Leibfried, and Haitham Bou-Ammar. Balancing two-player stochastic games
with soft Q-learning. In Proceedings of the 27th International Joint Conference on Artificial
Intelligence, pp. 268-274, 2018.
Amy Greenwald, Keith Hall, and Roberto Serrano. Correlated Q-learning. In International Conference
on Machine Learning, volume 20, pp. 242, 2003.
10
Published as a conference paper at ICLR 2022
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems (NeurIPS),
pp. 8571-8580, 2018.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. In Journal of Machine Learning Research, volume 11, pp. 1563-1600, 2010.
Zeyu Jia, Lin F. Yang, and Mengdi Wang. Feature-based Q-learning for two-player stochastic games.
arXiv preprint arXiv:1906.00423, 2019.
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E. Schapire. Contex-
tual decision processes with low bellman rank are pac-learnable. arXiv preprint arXiv:1610.09512,
2016.
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Con-
textual decision processes with low bellman rank are PAC-learnable. In Proceedings of the 34th
International Conference on Machine Learning-Volume 70, pp. 1704-1713. JMLR. org, 2017.
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I. Jordan. Is Q-learning provably
efficient? In Advances in Neural Information Processing Systems, pp. 4863-4873, 2018.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement
learning with linear function approximation. In Conference on Learning Theory, pp. 2137-2143.
PMLR, 2020.
Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of rl
problems, and sample-efficient algorithms. arXiv preprint arXiv:2102.00875, 2021a.
Chi Jin, Qinghua Liu, and Tiancheng Yu. The power of exploiter: Provable multi-agent rl in large
state spaces. arXiv preprint arXiv:2106.03352, 2021b.
Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In
International Conference on Machine Learning (ICML), 2021c.
Michail G. Lagoudakis and Ronald Parr. Value function approximation in zero-sum markov games.
In Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence, pp. 283-292.
Morgan Kaufmann Publishers Inc., 2002.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436-444,
2015.
Michael L. Littman and Csaba Szepesvari. A generalized reinforcement-learning model: Convergence
and applications. In 13th International Conference on Machine Learning, 1996.
Boyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural proximal/trust region policy optimization
attains globally optimal policy. arXiv preprint arXiv:1906.10306, 2019.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, DaanWier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learningep reinforcement
learning. arXiv preprint arXiv:1312.5602, 2013.
Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the eluder dimension.
In Advances in Neural Information Processing Systems (NeurIPS), 2014.
Julien Perolat, Bruno Scherrer, Bilal Piot, and Olivier Pietquin. Approximate dynamic programming
for two-player zero-sum Markov games. In International Conference on Machine Learning, pp.
1321-1329, 2015.
Julien Perolat, Florian Strub, Bilal Piot, and Olivier Pietquin. Learning nash equilibrium for general-
sum markov games from batch data. In Artificial Intelligence and Statistics, pp. 232-241. PMLR,
2017.
Julien Perolat, Bilal Piot, and Olivier Pietquin. Actor-critic fictitious play in simultaneous move
multistage games. In International Conference on Artificial Intelligence and Statistics, pp. 919-928,
2018.
11
Published as a conference paper at ICLR 2022
Daniel Russo. Worst-case regret bounds for exploration via randomized value functions. In Advances
in Neural Information Processing Systems ,pp.14410-14420, 2019.
Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic
exploration. In Advances in Neural Information Processing Systems, 2013.
Tuomas Sandholm. Super-human ai for strategic reasoning: Beating top pros in heads-up no-limit
texas hold’em. In International Joint Conference on Artificial Intelligence, pp. 24-25, 2017.
Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement
learning for autonomous driving. arXiv preprint arXiv:1610.03295, 2016.
Aaron Sidford, Mengdi Wang, Lin Yang, and Yinyu Ye. Solving discounted stochastic two-player
games with near-optimal time and sample complexity. In International Conference on Artificial
Intelligence and Statistics, pp. 2992-3002. PMLR, 2020.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, and Marc Lanctot. Mastering
the game of go with deep neural networks and tree search. Applied and Computational Harmonic
Analysis, 529(7587):484-489, 2015.
Sriram Srinivasan, Marc Lanctot, Vinicius Zambaldi, Julien Perolat, Karl Tuyls, Remi Munos, and
Michael Bowling. Actor-critic policy optimization in partially observable multiagent environments.
In Advances in Neural information Processing Systems, pp. 3422-3435, 2018.
Alexander L Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L Littman. PAC model-
free reinforcement learning. In Proceedings of the 23rd International Conference on Machine
Learning, pp. 881-888, 2006.
Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based
rl in contextual decision processes: Pac bounds and exponential improvements over model-free
approaches. arXiv preprint arXiv:1811.08540, 2019a.
Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based
rl in contextual decision processes: Pac bounds and exponential improvements over model-free
approaches. In Conference on Learning Theory (COLT). arXiv preprint arXiv:1811:08540, 2019b.
Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods: Global
optimality and rates of convergence. arXiv preprint arXiv:1909.01150, 2020a.
Ruosong Wang, Ruslan Salakhutdinov, and Lin F Yang. Provably efficient reinforcement learning
with general value function approximation. arXiv preprint arXiv:2005.10804, 2020b.
Yining Wang, Ruosong Wang, Simon S. Du, and Akshay Krishnamurthy. Optimism in reinforcement
learning with generalized linear function approximation. In International Conference on Learning
Representations, 2021.
Chen-Yu Wei, Yi-Te Hong, and Chi-Jen Lu. Online reinforcement learning in stochastic games. In
Advances in Neural Information Processing Systems, pp. 4987-4997, 2017.
Qiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang. Learning zero-sum simultaneous-
move markov games using function approximation and correlated equilibrium. In Conference on
Learning Theory (COLT). arXiv preprint arXiv:2002.07066, 2020.
Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent
pessimism for offline reinforcement learning. In Advances in neural information processing
systems (NeurIPS), 2021.
Lin Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and
regret bound. In International Conference on Machine Learning, pp. 10746-10756. PMLR, 2020.
Zhuoran Yang, Chi Jin, Zhaoran Wang, Mengdi Wang, and Michael I Jordan. On function approx-
imation in reinforcement learning: Optimism in the face of large state spaces. arXiv preprint
arXiv:2011.04622, 2020.
12
Published as a conference paper at ICLR 2022
Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement
learning without domain knowledge using value function bounds. In International Conference on
Machine Learning, pp. 7304-7312, 2019.
Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near
optimal policies with low inherent bellman error. arXiv preprint arXiv:2003.00153, 2020.
Kaiqing Zhang, Sham M Kakade, Tamer BaSar, and Lin F Yang. Model-based multi-agent rl in
zero-sum markov games with near-optimal sample complexity. arXiv preprint arXiv:2007.07461,
2020.
Yulai Zhao, Yuandong Tian, Jason D. Lee, and Simon S. Du. Provably efficient policy gradient
methods for two-player zero-sum markov games. arXiv preprint arXiv:2102.08903, 2021.
13
Published as a conference paper at ICLR 2022
A	Additional related works
There is a rich literature on sample complexity for single agent RL, mostly focusing on the tabular
cases (Strehl et al., 2006; Jaksch et al., 2010; Azar et al., 2017; Jin et al., 2018; Russo, 2019;
Zanette & Brunskill, 2019) and the linear function approximations (Yang & Wang, 2020; Wang et al.,
2021; Abbasi-Yadkori et al., 2019; Jin et al., 2020; Du et al., 2019b). For neural network function
approximations, Agarwal et al. (2020); Liu et al. (2019); Wang et al. (2020a); Yang et al. (2020) builds
provably efficient algorithms on over-parameterized models. The issue is complicated for general
function approximation, as the Q function is hard to learn due to the double-sample issue (Baird,
1995). In recent years, a line of work studies the structural properties and corresponding complexity
measures that allow generalization in RL. These work can be roughly grouped with two catagories.
One studies structures with low information gain. Jiang et al. (2017) proposes Bellman rank and
designs a sample efficient algorithm for a large family of MDPs. Later, Sun et al. (2019b) proposes
Witness rank and uses this notion to provide a provably efficient model-based RL algorithm. Recently,
Du et al. (2021) proposes Bilinear family and the corresponding Bilinear rank that encapsulates
a wide variety of structural properties, spanning from both model-based to model-free function
approximations. The other studies structures with low Eluder dimension (Russo & Van Roy, 2013).
Osband & Van Roy (2014) extends Eluder dimension to reinforcement learning and provides a unified
analysis of model based methods where the regret is controlled by the Kolmogorov dimension and
Eluder dimension of model function class. Wang et al. (2020b) designs a model-free algorithm and
shows that regret can be controlled by the Eluder dimension of the value function class. Recently, Jin
et al. (2021a) proposes Bellman Eluder dimension and builds a new framework to capture a large
amount of structures. It is noted that although lots of structures, such as linear MDP (Jin et al., 2020),
fit in both two viewpoints, they are not equivalent. Among the above work, Jin et al. (2021a); Zanette
et al. (2020) are more related. The method of placing uncertainty quantification on only initial states,
previously appeared in Zanette et al. (2020); Jin et al. (2021a), greatly simplifies the argument and
improves sample complexity. The Minimax Eluder dimension is inspired by the Bellman Eluder
dimension in Jin et al. (2021a).
In competitive RL, the setting with access to a sampling oracle or well explored policies is well
studied (Littman & SzePesvari,1996; Greenwald et al., 2003; Grau-Moya et al., 2018; Perolat et al.,
2018; Srinivasan et al., 2018; Sidford et al., 2020). Under these assumptions, many works consider
function aPProximations (Lagoudakis & Parr, 2002; Perolat et al., 2015; Fan et al., 2020; Jia et al.,
2019; Zhao et al., 2021). However, without strong samPling model or a well exPlored Policy, the issue
of exPloration-exPloitation tradeoff must be addressed. Most of these work focus on tabular setting
(Wei et al., 2017; Perolat et al., 2017; Bai & Jin., 2020; Bai et al., 2021; 2020; Zhang et al., 2020; Zhao
et al., 2021) or linear function aPProximation settings (Xie et al., 2020; Chen et al., 2021). Among
them, (Wei et al., 2017; Xie et al., 2020) are more related. The regret-decomPosition method used in
the decouPled setting is insPired by Xie et al. (2020). The ‘alternate oPtimism’ used in Algorithm 4
was Previously used as ‘Maximin-EVI’ in Wei et al. (2017), although the ‘alternate oPtimism’ used
in Algorithm 2 is different. Moreover, the concurrent work of Jin et al. (2021b) studies comPetitive
RL with general function approximation. They reach a sublinear (e(√K) regret for MGs with low
minimax Eluder dimensions in the decouPled setting. Their results in the coordinated setting are
based on different assumptions and complexity measures. Specifically, they assume finite function
class and an additional class of exploiters. It is noted that by letting the policy class be the class of
optimal policies induced by the value function class, we have ∣Π∣ = |F| and our algorithm achieves
on-par guarantee in their setup in terms of the covering numbers of function classes. In addition, we
propose a first provably sample efficient and model-based algorithm for the coordinated setting with
general function approximation.
A.1 Technical challenges
Previous work (Xie et al., 2020) imposes optimistic bonus on the action-value functions in every state-
action pairs and performs planning by the Coarse Correlated Equilibrium (CCE) on the optimistic
value functions. To achieve improved rates, we leverage the idea of ‘global optimism’ (Zanette et al.,
2020; Jin et al., 2021a; Du et al., 2021), which maintains a constraint set of candidate functions that
do not deviate much from the empirical estimates and performs optimistic planning on the initial
state. However, going beyond MDPs towards MGs, two problems arise. First, the concentration
property of functions in constraint set is hard to characterize due to multi-agent interplay. For this,
14
Published as a conference paper at ICLR 2022
we use the concentration methods in Jin et al. (2021a) and extend it from MDPs to MGs. The second
and more prominent issue is the exploration and exploitation tradeoff. Since ‘global optimism’ only
obtains optimism along the trajectories of behaviour policies, it may not guarantee optimism on the
trajectories of target policies (i.e. NE). As a result, directly using CCE to plan will cause the duality
gaps to diverge. To deal with this problem, we apply ‘alternate optimism’ to guide explorations,
which was previously used in Wei et al. (2017) for model-based methods. The ‘alternate optimism’
used in this work is slightly different for value-based methods. We prove two regret decomposition
lemmata to support this optimism principle.
B Proof of Theorem 3.6
We prove the Theorem 3.6 in the following five steps. In the first step, we show that Bellman error is
small for function in the constraint set. In the second step, We show that Q* stays in the constraint set
throughout the algorithm. In the third step, we decompose the regret into a summation of Bellman
error and Martingale difference. We can then bound the summation of Bellman error using Minimax
Eluder dimension in step 4. Finally we combine the aforementioned steps and complete the proof of
Theorem 3.6.
We define ‘optimistic’ value function in step h and episode k as follows
VhC(Xh) = fh (χh,∏h,bh)
where νbhk is the best response of πhk in value function fk, i.e., νbhk := argminνfhk(xkh, πhk, ν). We use
νk = {νhk }hH=1 to denote the policy adopted by P2 in the k-th episode. Notice that the agent obtains
knowledge from ν only through its actions bkh, h ∈ [H].
Step 1: Low Bellman error in the constraint set We first use a standard concentration procedure
(Agarwal et al., 2012; Jin et al., 2021a) to bound the Bellman error in constraint sets.
Lemma B.1 (Concentration on Bellman error). Let ρ > 0 be an arbitrary fixed number. With
probability at least 1 - p for all (k, h) ∈ [K] × [H] we have
k-1
Xfk(xh,ah,bh)-rh- ,F -∖fh+ι(x,∏h+ι,Vh+ι))2≤ O(β).
M	X 〜P(Txh,ah,bh)	+	+	+
Proof. Consider a fixed (k, h, f) sample, let
Ut(h,f) ：= (fh(xh,ah,bh) — rh - min max fh+1(xh+1, ∏0, ν0))2
ν0	π0
-(Thfh+1 (Xlh,a1h,bth) - rh - minmaX fh+ι(Xh+ι,π,ν))2
ν0 π0
and Ft,h be the filtration induced by {Xi1, ai1, bi1, r1i , . . . , XiH}it=-11 ∪ {Xt1, at1, bt1, r1t , . . . , Xth, ath, bth}.
Recall the minimax Bellman operator
Thfh+1(X, a, b) := r(X, a, b) + E	[minmaxfh+1(X0, π0, ν0)].
x0 〜Ph(∙∣x,a,b) ν0 π0
We have
E[Ut(h,f)|Ft,h]
= [(fh - Thfh+1)(Xth, ath, bth)]
• E[fh(xh, ah, bh) + Thfh+1(xh, ah, bh) - 2rh - 2min max fh+1(Xh+1, π , ν )|Ft,h]
ν0 π0
= [(fh - Thfh+1)(Xth, ath, bth)]2
and
Var[Ut(h,f )∣Ft,h] ≤ E[(Ut(h, f))2∣Ft,h]
= [(fh - Thfh+1)(Xth, ath, bth)]2
• E[(fh (Xh,ah,bh)+Thfh+ι (Xh,ah,bh)- 2rh — 2minmaχ fh+ι(Xh+iHMDlFu]
ν0	π0
≤ 36[(fh —Th fh+ι)(Xth,ath,bth)]2 = 36 E[Ut(h,f )Ft,h].
15
Published as a conference paper at ICLR 2022
By Freedman’s inequality, we have with probability at least 1 - p,
k
kk
XUt(h,f)-XE[Ut(h,f)|Ft,h]
≤O
utlog(1/p) X
E[Ut|Ft,h] + log(1/p)
Let N (F, ρ) be a ρ-cover of F. Taking a union bound for all (k, h, g) ∈ [K] × [H] × N (F, ρ),
then with probability at least 1 - p for all (k, h, g) ∈ [K] × [H] × N (F, ρ)
k	k
XUt(h,g)-X[(gh-Thgh+1)(xth,ath,bth)]2
≤ Outι X[(gh - Thgh+1)(xth,ath,bth)]2 + ι	(8)
where ∣ = log(HK∣N(F,ρ)∣∕p). Let gk = argming∈N(FP maxh∈[H] kfk - gk∣∣∞. We immedi-
ately have
k	k
X Ut(h, gk) - X[(ghk - Thghk+1)(xth, ath, bth)]2
t=1	t=1
≤ Otuι X[(ghk - Thghk+1)(xth,ath,bth)]2 + ι.	(9)
For all (h, k) ∈ [H] × [K], by the definition of Vk and fk ∈ Vk we have
kk
X Ut(h,fk) = X fk (χh,ah,bh) - rh-minmaX fh+ι(Xh+ι, π0,M))2
t=1	t=1	ν π
k
—E (Thfh(Xh, ah, bth) - rh—mvinmaxfh+ι (Xh+ι, π0,ν0))
t=1	ν π
k
≤ E(fh (Xh,ah,bh) - rh - minmaχ fh+ι(Xth+ι,π,J))
ν 0 π0
t=1
k
- inF E Igh(Xth ,ah,bh) - rh - mamaX %+1(琢+1/0"))
g∈ t=1	ν π
≤ O(ι + kρ).
(10)
It thus follows that,
kk
X[(fhk-Thfhk+1)(Xth,ath,bth)]2 ≤ X[(ghk - Thghk+1)(Xth, ath, bth)]2 + O(ι + kρ)
t=1	t=1
k
≤O(XUt(h,gk)+ι+kρ)
t=1
k
≤O(XUt(h,fk)+ι+kρ)
t=1
≤ O(ι + kρ)
where the first step is due to gk is an ρ-approximation to fk, the second step comes from Eq (9), the
third step is due to gk is an ρ-approximation to fk, and the final step comes from Eq (10).
By definition of πk and νbk , we have
Thfhk+1(X,a,b) := r(X, a, b) + E	[fh+1(X0,πhk+1,νbhk+1)]
x0 〜Ph(∙∣x,a,b)
thus we conclude the proof.	□
16
Published as a conference paper at ICLR 2022
Step 2: Q* is always in constraint set.
Lemma B.2. With probability at least 1 - p, we have Q* ∈ Vk for all k ∈ [K].
Proof. Consider a fixed (k, h, f) sample, let
Ut(h,f) := fh(Xh,ah, bh) - rh - minmax Qh+ι(Xh+ι,π0,M))2
ν0 π0
-(Qh(Xh,ah,bh) - rh - mi0nmaχ 或+1(琢+1"”))2
ν0 π0
and Ft,h be the filtration induced by {Xi1, ai1, bi1, r1i , . . . , XiH}it=-11 ∪ {Xt1, at1, bt1, r1t , . . . , Xth, ath, bth}.
We have
E[Ut(h,f)|Ft,h] =[(fh-Q*h)(Xth,ath,bth)]2
and
Var[Ut(h,f)|Ft,h] ≤36[(fhk-Thfhk+1)(Xth,ath,bth)]2 =36E[Ut(h,f)|Ft,h].
Let N (F, ρ) be a ρ-cover of F. By Freedman’s inequality, for all (k, h, f) ∈ [K] × [H] × N (F, ρ)
we have with probability at least 1 - p,
k	k
XUt(h,f) -X[(fh-Q*h)(Xth,ath,bth)]2
≤ Otulog(1/p) Xk [(fh -Q*h)(Xth,ath,bth)]2 + O(ι)
where ι = log(HK|N(F,ρ)∣∕p). Since [(fh 一 Qh)(Xh, ah bh)]2 ≥ 0, this implies
k
-XUt(h,g)≤O(ι).
t=1
Therefore for all (k, h, g) ∈ [K] × [H] × N(F, ρ)
k
E(Qh(Xh,ah,bh) - rh - mvinmnaX Qh+ι(Xh+ι,π0,ν0))
t=1	π
k
≤ £ (fh(Xh,ah,bh) - rh - minmaX Qh+ι(Xh+ι,π0,ν0)) + O(I+kρ).
t=1	ν π
Thus we have proven that Q* ∈ Vk , ∀k ∈ [K] with probability at least 1 - p.
□
Remark B.3. This step implies that
V * ≤ V1k(X1k)	(11)
which ensures optimism from the beginning state (‘global optimism’).
Step 3: Regret decomposition The following result is key to our analysis. It decomposes the
deviation from the value of the game into Bellman errors across time steps and a martingale difference
sequence.
Lemma B.4. Define
δhk :=Vhk(Xkh) - Vhπk,νk (Xkh)
Zk := E[δk+ι∣Xh,ah,bh] - δh+ι
γhk :=	E	[fhk(Xkh,a,bkh)] -fhk(Xkh,akh,bkh)
α~∏k(xh)
γbhh :=	E	[Qπhk,νk(Xhh, a, b)] - Qπhk,νk (Xhh, ahh, bhh)
a~∏k(xQ,b~νk(xh)
17
Published as a conference paper at ICLR 2022
Then we have
δhk ≤ δhk+1 -ζhk+γhk -γbhk+kh	(12)
where kh is the bellman error defined by
kh :=fhk(xkh,akh,bkh)-rhk- E	fhk+1(x,πhk+1,νbhk+1).
X 〜P(∙∣xh,ah,bh) 丁	丁 丁
Proof. By definition of νbhk = argminν fhk (xkh, πhk , ν), we have
fhk(xkh,πhk,νbhk) ≤fhk(xkh,πhk,bkh)
=fhk(xkh,akh,bkh)+γhk.
It thus follows that
δhk ≤ fhk(xkh, akh, bkh) - Qhπk,νk (xkh, akh, bkh) + γhk - γbhk
=rhk + E	fhk+1(x,πhk+1,νbhk+1)+kh
x~P(∙∣xh,ah,bh) T
-(rk+	, E -JVnkIVk(χ)∣χh,ah,bh])+Yk-b
x~P(∙,xh,ah,bh)	十
=fhk(xkh+1,πhk+1,νbhk+1) -Vhπ+k1,νk(xkh+1) -ζhk+kh+γhk-γbhk
=δk+ι- Zk+Yk- Yk + 盛.
Therefore We complete the proof.	□
Step 4: Bounding cumulative Bellman error using Minimax Eluder dimension. Recall that
Lemma B.1 gives:
k-1
Xfk (xh,ah,bh)-rh-,尸..fh+ι(x,∏h+ι,Vh+ι))2 ≤ O(β).
M	X 〜P(Txh,ah,bh) +	+	+ /
Combining this and Lemma F.1 With
gk = fh(∙, ∙, ∙) - rh -	E	fk+1(X,πk+1,bh+1)
X 〜P(∙∣∙,∙,∙)
and ρk = δ(xkn, akh, bkh), We have come to the folloWing result.
Lemma B.5. For any (k, h) ∈ [K] × [H] we have
K
X Ifh (Xkakbh)-Tlh - E …、fk+1(x,∏h+1,bk+1)∣
W	x~P(∙∣xh,ah,bh) +	+	+
≤ OZK ∙ dimME(F, p/K)log(HKN(F, 1/K)/p))
Thus
X 氏 ≤ O ^q K ∙ dimME(F, pl/K) log(HK N (F, 1/K )/p)).	(13)
Step 5: Putting everything together By Eq (11) and Eq (12)
KK
XV*(xι) - Vιπk,νk(xι)] ≤ XMk(X1) - Vιπk,νk(xι)]
k=1	k=1
KH
≤ XX(-ζhk+Yhk-Ybhk+kh)
k=1 h=1
HK	HK
= XX(-ζhk+Yhk-Ybhk)+XXkh.
h=1 k=1	h=1 k=1
18
Published as a conference paper at ICLR 2022
Notice that PhH=1 PkK=1(-ζhk + γhk - γbhk) is a martingale difference sequence and can be bounded
by O(HPKH log(HK/P) with probability at least 1 - p.
By Eq (13) the term PhH=1 PkK=1 kh can be bounded by
O(H，K ∙ dimME(F, p/K) log(HKN(F, 1
with probability at least 1 - p.
It thus follows that regret can be bounded by
Reg(K) ≤ o(h∖∣K ∙ dimME (F, p1/K) log(HKN (F, 1
B.1	Linear function approximation
In the linear function approximation setting of Xie et al. (2020),
rh(x, a, b) = φ(x, a, b)>θh, Ph(∙∣x,a, b) = φ(x, a, b)>μ%(∙)
where φ(∙, ∙, ∙) ∈ Rd is known feature vector and θh ∈ Rd and μh(∙) ∈ Rd are unknown with
∣∣θ∣∣2 ≤ √d, ∣∣νh(∙)∣∣2 ≤ √d and ∣∣φ(∙, ∙)∣∣2 ≤ 1. In this case Algorithm 1 reduces to the following
Algorithm 3. Notice that this algorithm can also be seen as a generalization of Zanette et al. (2020) to
Markov games.
Algorithm 3 ONEMG for linear function approximation.
1:	Input: Function class F
2:	Set β J C√dlog(HK∕p) for some large constant C
3:	for episode k = 1, 2, . . . , K do
4:	Receive initial state x1k
5:	Set wHk +1 = θHk +1 = 0
6:	Find maxwk,θk,h∈[H] V1k(x1k) such that for all h ∈ [H]:
k-1
(1)	whk = (Λkh)-1Xφ(xτh,aτh,bτh)[rhτ+Vhk+1(xτh+1)]
τ=1
⑵ ∣θk- Wh∣Λh ≤ c ∙ He
⑶ Qh(∙, ∙, ∙) = M)>φ(∙, ∙, ∙)
(4)	(πhh,B0)=NE(Qhh)
(5)	Vfh(∙)=	kE	[Qh(∙,a,b)]
a 〜∏lk,b 〜Bo
7:	for step h = 1, 2, . . . , H do
8:	P1 plays action ah 〜∏h (Xh)
9:	P2 takes action bhh
10:	Observe reward rhh and move to next state xhh+1
11:	end for
12:	end for
We will prove the following theoretical result of Algorithm 3. Notice the dependency on d of
Algorithm 3 is d while the dependency on d of Xie et al. (2020) is d3/2, thus this result improves
theirs by √d factor.
Theorem B.6. Regret (Eq (3)) in Algorithm 3 is bounded by O(dy∕H2K log(dHK∕p)) with Proba-
bility at least 1 - p.
B.1.1 Proof of Theorem B.6
We prove Theorem B.6 in the following five steps similar to the previous section.
19
Published as a conference paper at ICLR 2022
Step 1: Low Bellman error in the constraint set We use some standard results from previous
work. Notice that the N(F, ) = O(d log(1/)) and our inherent Bellman error is zero, thus these
lemmata can be directly adapted into our setting.
Lemma B.7 (Concentration, adapted from Lemma 1 of Zanette et al. (2020)). With probability at
least 1 - p, the following holds for all (k, h) ∈ [K] × [H],
φτh[Vhk+1(xτh+1)-
τ ∈[k-1]
E	Vhk+1(x)]
x~P(∙∣xh ,ah,bh)	+
≤ Hβ
(Λkh)-1
where β = Ω(，dlog(dKH∕p)).
Lemma B.8 (Least square error bound, adapted from Lemma 3 of Xie et al. (2020)). On the event of
Lemma B.7, the following holds for all (x, a, b, h, k)∈ S × A × A × [H] × [K] and any policy pair
(π, ν)
hφ(x, a, b), θhk i - Qhπ,ν (x, a, b) -	E	(Vhk+1 - Vhπ+,ν1)(x) ≤ βkφ(x,a,b)k(Λk)-1
X 〜P(∙∣x,a,b)	h
Step 2: Q* always stays in the constraint set
Lemma B.9 (Optimism). Let Q*b(∙, ∙, ∙) = φ(∙, ∙, ∙)>θ*, then on the event of Lemma B.8 and
Lemma B.7 there exists wh*, h ∈ [H] such that wh*, θh*, h ∈ [H] satisfies the constraints of Line 6 in
Algorithm 3.
Proof. We prove by backward induction on h. The claim trivially holds for h = H + 1. Now assume
the inductive hypothesis holds at h + 1. Let
k-1
wh* = (Λkh)-1Xφ(xτh,aτh,bτh)[rhτ +Vhk+1(xτh+1)]
τ=1
k-1	k-1
= (X φτh(φτh)> + I)-1 X φτh(rhτ +	E	Vhk+1(x) + ηhτ)
T=1	τ=1	X 〜Paxh,ah,陶
where Φh = Φ(xh, ah，bh) and ηh = Vk+ι(xh+ι) - Ex〜p(∙∣xh,ah,bh) Vh+ι(x). Notice that inductive
hypothesis implies rhh + Ex〜p(∙∣xh,ah,bh)Vhs+1(χ) = φ(xhh, ατh, bh)>θ*, therefore
k-1	k-1
wh = X Φh(Φh)> +1 )-1 X Φh((Φh)>θ* + ηh)
τ=1	τ=1
k-1
=θhh - (Λkh)-1θhh+(Λkh)-1Xφτhηhτ.
τ=1
It thus suffices to bound the following
k-1	k-1
k(Λkh)-1θhh+(Λkh)-1Xφτhηhτk(Λkh) ≤kθhhk(Λkh)-1+kXφτhηhτk(Λkh)-1
τ=1	τ=1
≤ √d + He
≤ C ∙ He
where the penultimate step comes from ∣∣θ*∣∣2 ≤ √d and Lemma B.7.	□
Remark B.10. This fact implies that
Vh(x1k)≤V1k(x1k).	(14)
20
Published as a conference paper at ICLR 2022
Step 3: Recursive decomposition of regret
Lemma B.11 (Regret decomposition). Define
δhk :=Vhk(xkh) - Vhπk,νk (xkh)
Zh ：= E[δk+∕χh,ah,bh ] - δk+ι
γhk :=	E	[Qkh(xkh,a,bkh)] -Qkh(xkh,akh,bkh)
a~∏k(xh)
γbhk :=	E	[Qπhk,νk(xkh, a, b)] - Qπhk,νk (xkh, akh, bkh)
a~∏k(xh),b~"k(xh)
Then we have
δhk ≤ δhk+1 -ζhk+γhk -γbhk+kh
where kh is the width defined by
部:=2β√(≠h)>(Λh)-1 φh .
(15)
Proof. By definition of B0 = argminν fhk (xkh , πhk , ν), we have
Vhk(xkh)= Qkh(xkh,πhk,B0)
≤ Qkh(xkh,πhk,bkh)
= Qkh (xkh , akh , bkh )+ γhk .
It thus follows that
δhk ≤ Qkh(xkh, akh, bkh) - Qhπk,νk (xkh, akh, bkh) + γhk - γbhk
kk
≤ E	[Vhk+1(x)]- E	[Vhπ+1,ν (x)]+kh+γhk-γbhk
x~P(∙∣xh,ah,bh)	x~P(∙∣xh,ah,bh)
= Vhk+1(xkh+1) - Vhπ+k1,νk (xkh+1) - ζhk + kh + γhk - γbhk
= δhk+1 -ζhk+γhk-γbhk+kh
where the second step comes from Lemma B.8 by letting (x, a, b) = (xkh, akh, bkh) (notice that
hφ(χh,ah,bh),θki = Qh(Xh, ah, bh)).	□
Step 4: Bounding width by Elliptical Potential Lemma This step directly makes use of the
following standard result.
Lemma B.12 (Elliptical Potential Lemma, Lemma 10 of Xie et al. (2020)). Suppose {φt}t≥0 is
a sequence in Rd satisfying kφtk ≤ 1, ∀t. Let Λ0 ∈ Rd×d be a positive definite matrix, and
Λt = Λ0 + i∈[t] φiφi>. If the smallest eigenvalue of Λ0 is lower bounded by 1, then
log(
detΛt
detΛo
)≤	φi>Λj--11φi ≤ 2 log(
i∈[t]
detΛt
detΛo
).
Step 5: Putting everything together By Eq (14) and Eq (15)
KK
XV*(xι) - Vιπk,νk(xι)] ≤ XMk(X1) - Vιπk,νk(Xi)]
k=1	k=1
KH
≤ XX(-ζhk+γhk-γbhk+kh)
k=1 h=1
HK	HK
= XX(-ζhk+γhk-γbhk)+XXkh.
h=1 k=1	h=1 k=1
21
Published as a conference paper at ICLR 2022
K
X 2β(φkh)>(Λkh)-1φkh
k=1
Notice that PhH=1 PkK=1(-ζhk + γhk - γbhk) is a martingale difference sequence and can be bounded
by H JKH log(HK∕p) with probability at least 1 - p.
By Lemma B.12 the term PhH=1 PkK=1 kh can be bounded as follows
H K	H K	______________
XX eh = XX 2β J(Φh)>(Λh)Tφh
h=1 k=1	h=1 k=1
H
≤	X √K ∙
h=1
≤	2β X √K ∙ "og(det⅛)
h	=1	e h
≤	2β X √K ∙ r2log(λ + Kmaχk kφhk2g
h=1
≤	O(dHPKlog(dHK∕p)).
with probability at least 1 - p.
It thus follows that with probability at least 1 - p regret can be upper bounded by
O(dPH 2K log(dHK∕p)).
C Model-based method for the coordinated setting
C.1 Alternate Optimistic Model Elimination (AOME)
Algorithm 4 Alternate Optimistic Model Elimination (AOME)
1:	Input: Model class M, test function class G, precision e, failure probability p
2:	Initialize M0 — M
3:	for k = 1, 2, . . . , K do
4:	Find M1k — argmaxM ∈M QM (x1, πM, νM), let πk — πM1k
5:	Find M2k — argminM∈MQM(x1, πk, νπMk), letνk — νπMk1k
6:	Execute πk, νk and collect n1 rewards {rhi }h∈[H],i∈[n1]
7:	Let Vk — n⅛ Pn= 1(PH=1 rh)	ʌ
8:	if max{∣Vk — QMk (xι,πk, νk)|, |Vk - QMk(xι,πk, νk)|} ≤ e/2 then
9:	Terminate and output πk , νk .
10:	else
11:	Find hk such that maxi∈[2] |Lb(M1k, M2k, Mik, hk)| ≥ e/(4H) by Eq (5).
12:	Collect n trajectories {(xh, ah, bh rh)H=ι}n=ι by executing ah, bh 〜∏k,νhk,∀h∈ [H]
13:	Update constraint set, where E is given by Eq (6)
Mk — {M ∈ Mk-1 : Eb(M1k,M2k,M,hk) ≤ φ}
14:	end if
15:	end for
C.2 Theory for Alternate Optimistic Model Elimination (AOME)
In this section we prove that Algorithm 4 terminates in finite rounds. The technique bulk is then
showing that in Step 13 the constraint set can only shrink for finite times. This is dependent on the
complexity of model class M and discriminator function class G. We first generalize Witness rank
(Sun et al., 2019b) to Markov games.
22
Published as a conference paper at ICLR 2022
Definition C.1 (Witnessed model misfit in Markov games). For discriminator class G : S × A1 ×
A2 × R × S 7→ R, models M1, M2, M ∈ M and level h ∈ [H], the witnessed model misfit of Markov
game at level h is defined as follow
E(M1, M2, M, h) := sup E	E	[g(xh, ah, bh, r,x) - g(xh, ah, bh, rh, xh+1)]. (16)
g∈G (x,a,b)〜PM2 (r,X)〜M
where (x, a, b)〜P M2 denotes Xτ +1 〜P(∙∣Xτ, a「, bτ), aτ 〜∏M1 and bτ 〜VMMI forall T ∈ [h].
Using g(xh, ah, bh, r, x) = r(xh, ah, bh) + QM (x, πM1, νπMM21 ), we can also give a generalization of
Bellman error (Jiang et al., 2016) in Markov games by the following
L(M1,M2,M, h) :=	E	E	[g(xh, ah, bh,r,x))] - E	[g(xh, ah, bh, r, x)] .
(x,a,b)〜PM2 |_(r,X)〜M	(r,X)〜M*	_
(17)
The final components in our theory are two assumptions from Sun et al. (2019b). Define rank(A, β)
to be the smallest integer k such that A = UV> with U, V ∈ Rn×k and ∣∣Ui,*∣∣2 ∙ ∣∣Vj,*∣∣2 ≤ β for
any pair of rows Ui,*, Vj,*.
Assumption C.2 (Realizability). Assume M* ∈ M.
Assumption C.3 (Witness misfit domination). Assume G is finite, kg k∞ ≤ 2, ∀g ∈ G and
∀M1,M2,M ∈ M : E(M1,M2,M,h) ≥ L(M1,M2,M, h).
Now we introduce the Witness rank for Markov games.
Definition C.4 (Witness rank for Markov games). Given a model class M, a test function class
G and K ∈ (0,1]. For h ∈ [H ], we define Nκ,h ⊂ RlMl2×lMl as the set of matrices such that any
matrix A ∈ Nκ,h satisfies
κ∣L(M1,M2,M, h)| ≤ A((M1,M2),M) ≤ E(M1,M2,M, h), ∀M1,M2,M ∈ M.
The Witness rank for Markov games is defined by
W(κ,β,M,G,h) :
min rank(A, β).
A∈Nκ,h
We are in the position to state the main theorem of this section.
Theorem C.5. Under Assumption C.2 and Assumption C.3, set φ = 100H√∕W where WK =
maXh∈[H] W(κ, β, M, G, h), let T = HWK log(β∕2φ), Set nι = C ∙ H2 log(HT∕p)∕e2 and n =
C ∙ H 2Wk |A| log(T ∣M∣∣G∣∕p)∕(κe)2 for large enough constant C. Thenfor all e,p, κ ∈ (0,1], with
probability at least 1 - p Algorithm 4 outputs a policy π such that V*(x1) - V π,νπ* (x1) ≤ within
at most O(H WK2|A| log(T∣G∣∣M∣∕p)) trajectories.
As seen in the above theorem, Algorithm 4 has sample complexity that scales quadratic with Witness
rank and logarithmically with cardinality of model class. This matches the sample complexity results
in the Markov decision process setting (Sun et al., 2019b). When the test function is Q-function,
the Witnessed model misfit reduced to Bellman error as seen in Eq (17) and witness rank reduce to
Bellman rank.
C.3 Proofs
We prove Theorem C.5 in the following steps. In the first step, we show a simulation lemma that
generalize Jiang et al. (2017) to Markov games. In the second step we show concentration of empirical
estimates of Bellman error, model misfit and value functions. In the third step we show that M *
always stays in the constraint set. We examine the conditions when Algorithm 4 terminates or not in
the fourth step. Using these conditions, we bound the number of rounds in the fifth step. Finally we
combine everything and complete the proof of Theorem C.5.
From Definition C.4 we know that there exists Ah ∈ NK,h such that
κ∣L(M1,M2 ,M,h)∣ ≤ Ah ((M1,M2),M) ≤E (M1,M2,M,h), ∀M1,M2 ,M ∈M.
and we can factorize	Ah((M1, M2), M)	=	hζh(M1, M2), χh(M)i	where
kζh(M1,M2)k2,kχh(M)k2 ≤β.
23
Published as a conference paper at ICLR 2022
Step 1: Simulation Lemma The analysis begins with a useful Lemma that decouples the value
difference into a sum of Bellman errors across time steps.
Lemma C.6 (Simulation Lemma). Fix model M1, M2, M ∈ M. Let π = πM1 , ν = νπMM21 . Under
Assumption C.3, we have
H
QM (x1, π, ν) - V1π,ν (x1) = X L(M1, M2, M, h)
h=1
Proof. Let (x,a,b) ~ PM2 denote Xτ+1 ~ P(∙∣Xτ,a「, bτ), 0τ ~ ∏MI and bτ ~ VMMI for all
τ ∈ [h]. Using definition of Bellman error in Eq (17), we have
QM (x1, π, ν) - V1π,ν (x1)
=	E	E	[ri	+	Qm(x2,π,ν)] -	E	[ri	+	QM*(x2,∏,ν)]
(x,a,b)~PM2 ](rι,χ2)~M	(r1,χ2)~M*	_
=	E	E	[ri	+	QM(x2,π,ν)] -	E	[ri	+	QM(x2, π, ν)]
(x,a,b)~pM2 L(rι,χ2)~M	(rι,χ2)~M*	_
+ E	E [ri + QM(x2, π, ν)] - E [ri + QM*(x2, π, ν)]
(x,a,b)~pM2 ](rι,χ2)~M*	(rι,χ2)~M*	_
=L(Mi,M2,M,1)+ E	[QM(x2, π, ν) -V2π,ν(x2)]
(x,a,b)~PM2
H
XL(Mi,M2,M,h).
h=i
We thus complete the proof.
□
Step 2: Concentrations In this part we show some standard concentration results.
Lemma C.7. With probability at least 1 - p, we have the following event
1.	E(Mik,M2k,M,h) -Eb(Mik,M2k,M,h) ≤ φforallM ∈ M andh ∈ [H],
2.	I Lb(Mk ,Mk ,Mk ,h)-L(Mk ,M2k ,Mik ,h)∣ ≤ 8H for all h ∈ [H ] and i ∈ [2],
3.	V πk,νk - Vbk ≤ /8
holds for all k ≤ T rounds.
Proof. Fix one iteration. By Hoeffding’s inequality, with probability at least 1 - p/3,
Lb(Mik,M2k,Mik,h) -L(Mik,M2k,Mik,h)
log(2H∕p)
ni
(18)
Thus (2) follows from ni = C ∙ H2 log(HT∕p)∕e2 and union bound on all T iterations. Similarly
we can verify (3). For (1), fix model M ∈ M and g ∈ G, by Hoeffding’s inequality, with probability
at least 1 - p∕3,
E	E	[g(xh, ah, bh, r, x) - g(xh, ah, bh, rh, xh+i)]
(x,a,b)~pM2 (r,x)~M
n1	i ii	i iii i
-E -. E λJg(Xh, ah, bh, r,x)- g(Xh, ah, bh, Th, xh+ι)]
n^ n (r,x)~M	+
i=i
≤
log(2∕p)
n
(19)
(20)
(21)
24
Published as a conference paper at ICLR 2022
Thus (1) follows from n = C ∙ H2Wκ∣A∣ log(T∣M∣∣G∣∕p)∕(κe)2 and union bound on all k ∈ [T],
M ∈ M and g ∈ G. We complete the proof.	□
Step 3: M* stays in the constraint set while it shrinks In this step, we show that the constraint
set always contains the true environment. The result is displayed in the following.
Lemma C.8. Suppose for each round k, the event in Lemma C.7 holds. Then M* ∈ Mk for all
k. Furthermore, let Mck = {M ∈ Mck-1 : Ahk (M1k, M2k, M) ≤ 2φ} with Mc0 = M. We have
Mk ⊂ Mck for all k.
Proof. Since E(M1k, M2k, M* , h)	=	0, ∀h ∈	[H], we have Eb(M1k, M2k, M* , hk)	≤
E(M1k, M2k, M* , hk) + φ ≤ φ, Thus M* is not eliminated. We prove the second result by
induction. Firstly Mc0 = M0. Assume Mk-1 ⊂ Mck-1. For all M ∈ Mk, we have
Ahk(M1k,M2k,M) ≤ E(M1k,M2k,M,hk) ≤ Eb(M1k,M2k,M,hk) + φ ≤ 2φ by Line 13 of Al-
gorithm 4. Therefore M ∈ Mk, we complete the proof.	□
Step 4: Terminate or rank increase The following Lemma is key to the ‘alternate pessimism
principle’. Intuitively, it indicates that the algorithm either terminates and outputs a pair of policies
with small duality gap, or detects a time step that provides useful information (large Bellman error).
Lemma C.9. Suppose for round k, the event in Lemma C.7 holds and M* is not eliminated. Then if
k .*
the algorithm terminate, it outputs a policy pair πk such that V * (x1) - V π ,νπk (x1) ≤ ; otherwise
Ahk ((Mk,Mk),Mk) ≥ 器 forsome i ∈ [2].
Proof. Consider the situation that the algorithm does not terminate. Then with out loss of generality
we can assume ∣Vbk - QMk (xi, ∏k, νk)∣ ≥ 〃2. By Lemma C.7 and Lemma C.6, we have
H
X L(Mk, Mk,M1, h) = IQm(xι,∏, V) - V∏,ν(x1)∣
h=1
≥ | Vb k - QMk (xι ,πk ,νk )| -IV πk，vk - Vb k∣
≥3e∕8.
By pigeonhole principle, these exists h ∈ [H ] such that ∣L(Mf, Mk, Mk, h) | ≥ 翡.For this h we
have ∣L(Mf, Mk, Mf, h)| ≥ 翡-金=册,thus the hk in Line 11 is well defined and we have
|Lb(M1k , M2k, M1k , hk )| ≥ ∕(4H). Using Lemma C.7 again, we have
Ahk((Mk,Mk),Mk) ≥ K ∙∣L(Mk,Mk,Mk,h)1 ≥ 券.
8H
If the algorithm terminates, we have
QM1k(x1, πk, νk) ≥QM1k(x1,πk,νπMk1k) ≥V *(x1)
and QMk(x1,∏k,νk) ≤ V"","：% (x1). Therefore
V*(x1) - Vπ，嗫(χι) ≤ QMk(χι,∏k,νk) - QMk(Xι,πk, Vk)
≤ IQMk(xι,∏k,νk) - Vπk,νk| + |Vπ，vk - QMk(xι,πk,νk)∣
≤ .
We thus complete the proof.	□
Step 5: Bounding the number of iterations. This step uses the technique of Jiang et al. (2017);
Sun et al. (2019a), which uses the volume of minimum volume enclosing ellipsoid as a potential
function to bound the number of iterations. The key result is presented in Lemma C.10.
Lemma C.10. Suppose for every round k, the event in Lemma C.7 holds, then the number of iterations
of Algorithm 4 is at most HWK log( 2φ)/ log(5∕3) for certain i ∈ [2].
25
Published as a conference paper at ICLR 2022
Proof. If the algorithm does not terminate at round k, then by Lemma C.9,
hZhk(Mk,Mk),Xhk(Mk)i	=	Ahk((Mk,Mk),Mk)	≥	8H	=	6√WKφ.	Let	Ok	denote the
origin-centered minimum volume enclosing ellipsoid of {χh (M) : M ∈ Mck}. Let Ohk-1,+ denote
the origin-centered minimum volume enclosing ellipsoid of {v ∈ Ohk-1 : hζhk (M1k, M2k), vi ≤ 2φ}.
By Lemma F.2, we have
VOl(Okk) ≤ Vol(Ok-1,+ ) ≤ 315
VOl(Ok-1) — VOl(Ok-1)—..
Define Φ = supM1,M2∈M,h∈[H] kζh(M1,M2)k2 and Ψ = supM∈M,h∈[H] kχh(M)k2. Then we
have vol(O0) ≤ VWK(I) ∙ ΨWκ where VWK(R) is the volume ofEuclidean ball in RWK with radius
R. On the other hand, we have
VOl(Ok) ≥ vol({q ∈ RWκ ： kqk2 ≤ 2φ∕Φ}) ≥ VWK(I) ∙ (2φ∕Φ)Wκ.
It thus follows that the number of iterations for each h is at most WK log( φ2ψ)/ log(5∕3). Using
β ≥ ΦΨ, this completes the proof.	□
Step 6: Putting everything together. From Lemma C.7, with high probability the events holds.
Therefore for the first T rounds, we can apply Lemma C.10 and know that the algorithm indeed
terminates in at most T rounds. The number of trajectories is thus upper bounded by (n1 + n)T =
O( h3W⅛ai log(T ∣G∣∣M∣∕p)).
D Theory for Alternate Optimistic Value Elimination (AOVE)
We prove Theorem 4.4 in the following five steps. In the first step we show that the functions in
the constraint set has low Bellman error. Notice that different from Lemma B.1, here we consider a
different Bellman operator in Eq (2) and consider all policies in policy class Π. In the second step
we show that Qπ,“π belongs to the constraint set for all ∏ ∈ Π and in particular Q* belongs to the
constraint set. In the third step we perform a regret decomposition. Notice that here we consider
two Bellman errors. We then bound the summation of these Bellman errors by Minimax Eluder
dimension in step 4 and complete the proof in step 5.
Step 1: Low Bellman errors in the constraint set.
Lemma D.1 (Concentration on Bellman error). Let ρ > 0 be an arbitrary fixed number. With
probability at least 1 - pfor all (k, h) ∈ [K] × [H] we have
k-1
(a)	Xfk (xh,ah,bh)-rh-	Ei	fk+1(x,∏k+1, (ν∏k )h+ι)))2 ≤ O(β)
i=1	x~p(，|xh，ah，bh)
k-1
(b)	X(gk (xh,ah,bh)-rh -..ffk+1(x,∏k+1, (ν∏k )h+1)))2 ≤ O(β)
M	X 〜P(Txh ,ah,b3
Proof. For simplicity we only proof (a), and (b) follows similarly.
Consider a fixed (k, h, f, π) tuple, let
Uty= Uh(XIh ,ah,bh) -rh- fh+1(Xh+1,π,ν∏))2
-(rh +	…E …八 fh+1(x,π,ν∏)- rh- fh+1(xh+1,π,ν∏))2
x 〜Paxh,ahM)
and Ft,h be the filtration induced by {Xi1, ai1, bi1, r1i, . . . , XiH}it=-11 ∪ {Xt1, at1, bt1, r1t , . . . , Xth, ath, bth}.
26
Published as a conference paper at ICLR 2022
We have
E[Ut(h, f,∏)∣Ft,h]
[(fh(xh, ah, bh) - rh - X〜PE∙,∙,∙) fh+1(x, π, Vn))(xth, ah, bh)]
• E[fh(xh, ah, bh) + rh + χ~P(∙∣χE,ah,bh) fh+1(x,π,νπ) - 2rh - 2fh+1 (XhH π, Vn)1Ft,h]
[fh (xth, ath , bth ) - rht -
E
X 〜P(Txh,ah,bD
fh+1(x,π,Vπf)]2
and
Var[Ut(h, f, ∏)∣Ft,h] ≤ E[(Ut(h,f, ∏))2∣Ft,h]
= [(fh -rh - E	fh+1(x,π,Vπf))(xth,ath,bth)]2
X 〜P(∙∣∙,∙,∙)
• E[(fh(Xh,ah,bh) + rh +	…E t t∖fh+1(x,π,ν∏') - 2rh — 2fh+1(Xh+ι,π,ν∏ ))2lFt,h]
X 〜P(Txh,ahM)
≤ 36[fh(xh,ah,bh)-rh -	…E - fh+ι(x,∏,ν∏)]2 = 36E[Ut(h,f,π)∣Ft,h].
X 〜P(Txh,ahM)
By Freedman’s inequality, we have with probability at least 1 - p,
kk
XUt(h,f,∏)- XE[Ut(h,f,∏)∣Ft,h]
k
log(1/p) XE[Ut|Ft,h] + log(1/p)
t=1
Let Nρ × Yρ be a ρ-cover of F × Π. The covering of Π is in-terms of the distance of two policies
defined as
d(∏,∏0) := fEjmaXbE∕f(x,∏,b) - f(x,∏0,b)∣.
Taking a union bound for all (k, h, f, p) ∈ [K] × [H] × Nρ × Yρ, we have that with probability at
least 1 - p for all (k, h, f, p) ∈ [K] × [H] × Nρ × Yρ
kk
XUt(h,f,p)-X[fh(Xth,ath,bth)-rht -
t=1	t=1
Et t t fh+1(X,p,Vpf)]2
X 〜P(.iXh，ahM)
uk
≤ O (∖ U X[fh(Xh, ah, bh) - rh - X~P(∙%,bh) fh+1(x, p, Vp)]2 +
(22)
where ι = log(HK∣Nρ∣∣Yρ∣∕p). For an arbitrary (h, k) ∈ [H] X [K] pair，by the definition of Vk
and (πk , fk ) ∈ Vk we have
k
XUt(h,fk,πk)
t=1
k
- X (rh + E
% h	X〜p(∙∣Xh,ah,bh)
k
fhk+1(X,πk,Vπfkk)-rht - fhk+1(Xth+1, πk, Vπfkk)2
≤ X (fk(Xh, ah, bh) - rh - fh+ι(Xh+1, Kk, Vnk))
t=1
k
-if X (gk(Xh, ah, bh ) - rh - fk+1(Xh+1,πk ,ν∏k ))
g∈ t=1
≤ O(ι + kρ)
(23)
where in the first step we use Assumption 3.2.
27
Published as a conference paper at ICLR 2022
Define fk = argminf∈Nρ maxh∈[H] kfhk 一 fkhk∞,pk = argminp∈Yρ d(πk, p). Since Nρ is a ρ-cover
of F,
k
k
t=1
Ut(h,fk,πk)-	Ut(h,fk,pk) ≤ O(kρ).
(24)
t=1
Therefore combining Eq (23) and Eq (24)
k
XUt(h,fk,pk) ≤ O(ι + kρ).
t=1
(25)
Recall Eq (22) indicates
k
k
t=1
Ut(h,fk,pk)-	[fkh(xth,ath,bth)-rht -
t=1
E
x~P(∙∣xh,ah
≤O t
E
X 〜P(∙∣xh,ah
(26)
Combining Eq (25) and Eq (26) we have
k
E
X 〜PG|xh,ahM)
k
fkh+1 (x, pk, νpk)]2 ≤ O(ι+ kρ).
Since fk is an ρ-approximation to fk and pk is an ρ-approximation to πk, we can conclude the proof
of (a) with
k-1
Xfk (xh,ah,bh )一吟 一 ，F ..fk+ι(x,∏k+ι, (Vnk )h+ι)))2 ≤ O⑼.
=	X 〜P(Txh ,ah,bh) +	+ π
□
Step 2: (∏, Qπ,νπ), ∀∏ ∈ Π is always in constraint set.
Lemma D.2. With probabilityat least 1 一 P, we have (π*, QDand (π, Qπ,*),∀π ∈ Π all belong to
Vk for all k ∈ [K].
Proof. Consider a fixed (k, h, f, π) tuple, let
*2
UMhjn) = (fh(Xh,ah,bh)-rh-Qh+∏(χh+ι,π,ν∏))
一 (Q∏,"π (Xh, ah, bh) - rh - Q∏+π (Xh+ι,π,ν∏)
and Ft,h be the filtration induced by {xi1, ai1, bi1, r1i , . . . , xiH}it=-11 ∪ {xt1, at1, bt1, r1t , . . . , xth, ath, bth}.
We have
*
E[Ut(h,f,π)∣Ft,h] = [(fh - Qh,"π)(xh,ah,bh)]2
and
*	C
Var[Ut(h,f,π)∣Ft,h] ≤ 36[(fh - Q∏,"π)/足册)]2 = 36E[Ut(h, f,π)∣Ft,h].
By Freedman’s inequality, we have with probability at least 1 一 p,
k
k
X Ut(h,f, ∏) - X[(fh - Q∏,νπ )(xh, ah, bh)]2
t=1
≤O t
t=1
k 、
log(1/p)	[(fh 一 Qhπ,νπ)(xth, ath, bth)]2 + log(1/p) .
t=1
28
Published as a conference paper at ICLR 2022
Let Nρ × Yρ be a ρ-cover ofF × Π. Taking a union bound for al (k, h, f, p) ∈ [K] × [H] × Nρ × Yρ,
we have that with probability at least 1 - p for all (k, h, f, p) ∈ [K] × [H] × Nρ × Yρ
k
-XUt(h,f,p)≤O(ι)
t=1
where ι = log(HK∣Nρ∣∕p). This implies for all (k, h, f, p) ∈ [K] X [H] X NP X YP
k
X (Qh,νp (Xh ah, bth) - rh -Qhg (xh+1, p, v；))2
t=1
k
Pν*	2
≤ fGh(xh,ah,bh) -Th- Qh+1 (Xh+ι,p, Vp)) + O(I + kP).
=1
Since NP X YP be a ρ-cover ofF X Π, for all pk ∈ YP
k
X (Qh,"p (Xh, ah,监-Trh -Qh+1 (xh+ι, p,项)2
t=1
k
P ν*.
≤ if Elgh(Xh,ah,bh) - rh,- Qh+pι (Xh+ι,p, Vp)) + O(I + kp).
g∈	=1
Again using NP X YP be a ρ-cover of F X Π, we have for all π ∈ Π
k
X (Qh,"π (Xh, ah, bth) - rh - Q∏+π (Xh+1, ∏, Vn))2
t=1
k
*
≤ if £(gh(Xh,ah,bh)-rh- Qh+π(Xh+ι,π,ν∏)) + O(I + kp).
g∈	=1
Therefore we have (π, Qπ,νπ* ), ∀π ∈ Π all belong to Vk for all k ∈ [K] with probability at least
1 一 p.	□
Remark D.3. An important implication of this fact is the following
V* - Vπk,"πk ≤ fk(Xk, ∏k, Vnk) - gk(Xk, ∏k, Vk)
which gives an upper bound of the one-side duality gap.
Step 3: Bounding the regret by Bellman error and martingale.
Lemma D.4. Define
δh ：= fh (Xh,∏k,(V∏k )h) - gh (Xh,∏h ,V)
Zh ：= E[δh+ι∣Xh,ah,bh] - δh+ι
γhh :=	E	[fhh(Xhh,a,b)] -fhh(Xhh,ahh,bhh)
a~∏k,b~νk
γbhh :=	E	[ghh(Xhh,a,b)] -ghh(Xhh,ahh,bhh)
a 〜∏k,b 〜Vk
Then we have
δhh ≤ δhh+1 +ζhh+hh-bhh+γhh-γbhh
where hh, bhh are the bellman error defined by
hh :=fhh(Xhh,ahh,bhh) -rhh -	E	fhh(X, πhh+1, (Vnfkk)h+1)
X 〜P(∙∣xh,ah,bh)
bhh :=ghh(Xhh,ahh,bhh)-rhh -	E	ghh(X, πhh+1, Vhh+1).
X 〜P(∙∣xh,ahM)
29
Published as a conference paper at ICLR 2022
Proof. By definition of (νπfkk)h = argminν∈Πhfhk(xkh,πhk,ν), we have
fhk(xkh, πhk, (νπfkk)h) ≤fhk(xkh,πhk,νhk)
=fhk(xkh,akh,bkh)+γhk
It thus follows that
—
-(rk + x〜P(.谭,ah,bh) gk (x, πk+1,νk+1) + bh) + Yh
—
X 〜P(甫ah,bh)fk (X,πk+1, (Vnk )h+1) - X 〜P(Eh,bh) gk (X,πk+1,νh+1)
—
—
—
Thus we complete the proof.
□
Step 4: Bounding cumulative Bellman error using Minimax Eluder dimension.
Lemma D.5 (Bounding Bellman error). For any (k, h) ∈ [K] × [H] we have
XX 层 | ≤ O (qk ∙ dimME0 (F, pl/K) log[N(F, 1/K) ∙ N(∏, 1/K) ∙ HK/P])
t=1
XX |bh| ≤ θ('k ∙ dimME，(F, p/K) log[N(F, 1/K) ∙N(∏, 1/K) ∙ HKlp).
t=1
Proof. Let
It(∙, ∙, ∙) = fh(∙, ∙, ∙) - rh(∙, ∙, ∙) - 耳	fh+ι(x, πt, Vnt)
X 〜P(∙,∙,∙)
and ρt = δ(Xth, ath, bth). Similarly, we can also let ρt be generated by (πt , νt ). Then Lemma D.1
indicates
k
X lk2 (ρt) ≤ O(β).
t=1
Based on this, We can apply Lemma F.1 to obtain the first inequality. The second follows similarly. □
Step 5: Putting them all together. We can upper bound the regret as follows:
KK
∑[v *- V πk,νπk ] ≤ Xfk (Xk ,∏k ,νf) - gk (Xk ,∏k ,νk)]
k=1	k=1
KH
≤ X X(ζhk+kh-bkh+γhk-γbhk)
k=1 h=1
HK	HK
= X X(ζhk + γhk -γbhk)+X X(kh - bkh)	(27)
h=1 k=1	h=1 k=1
where the first step comes from Lemma D.2, the second step comes from Lemma D.4 and the last
step comes from Fubini,s theorem. Notice that the first term in Eq (27) is a martingale difference
sequence and can be bounded by O(H，KH log(HK∕p)) with probability at least 1 一 p and the
second term can be bounded by Lemma D.5. We thus complete the proof.
30
Published as a conference paper at ICLR 2022
E Examples
In this section we give several examples of Markov games that possess low Minimax Eluder dimension,
including tabular Markov Games, linear function approximation, reproducing kernel Hilbert space
(RKHS) function approximation, overparameterized neural networks, and generalized linear models.
We first consider reproducing kernel Hilbert space (RKHS) function approximation, which captures
many existing models.
Reproducing kernel Hilbert space (RKHS) function approximation. In this setting, the function
class of interest is given by
Fh = {hΦh(x, a, b),θhi ： θ, Φ(∙, ∙, ∙) ∈ Br(H)}∙
Here φ(∙, ∙, ∙) : S ×A×A→ H is a feature map to H, θ ∈ H, and BR (H) is the ball centered at
zero with radius R in a Hilbert space H. The effective dimension of a set D ⊂ H is characterized
by the critical information gain d(γ, D). Specifically, for γ > 0 and D ⊂ H, we define dn(γ, D) =
maxχι,…,xn∈D logdet(I + γ PZi xix>). The critical information gain (DU et al., 2021) is thus
defined by d(γ, D) = mιnk∈N也≥dn(γ,D) k. The main result is given in the following theorem.
Theorem E.1. In kernel function approximation, the Minimax Eluder dimensions dimME (F, ) and
dimME0 (F, C) are upper bounded by maXh∈[H] e( 9R, Φh), where Φh = {φh(x, a, b) : X ∈ S ,a,b ∈
A}.
Proof. We only show dimME，(F, c) ≤ maXh∈[H] e(9R, Φh), and the other part is totally sim-
ilar. Fix an h ∈ [H]. Due to completeness, for any π ∈ Π and f ∈ F we have (fh -
Thπfh+i)(x, a, b) = hφh(x, a, b), θh - Thπ θh+ii where Thπθh+i is the unique feature in H such
that Thπfh+i = hφh(x, a, b), Thπθh+ii. Consider the longest sequence νi, ∙ ∙ ∙ , νk ∈ D∆ such that
each one item is C-independent of its precedents, distinguished by a sequence of features θh(i), ∙ ∙ ∙ , θh(k).
To simplify notations, we define φi = Eνi [φh(x, a, b)] and θi = θh(i) - Thπθh(i+) i for all i ∈ [k]. Let
Λi = PT=I φτφ> + 9R21. Then Definition 3.3 gives
kΦikΛ-ι ≥ 1 ∙kΦikΛ-ι ∙kθi kΛi ≥ 1∙∣hΦi ,θiil≥ 1.
iC	i	C
where the first step comes from θi ∈ BR(H) and Pij-=ii hφj, θji2 ≤ C, the second step comes from
Holder,s inequality and the final step comes from ∣hφi, θ∕∣ ≥ c. Thus we have
k	9R2 k
k ≤ Elog(I + kΦi∣∣Λ-ι) = logdet(I + -ɪ Eφiφ>).
i=i	C i=i
By the definition of critical information gain, k must be less or equal than e( 9R, Φh).	口
This result implies several important cases in general function approximation.
•	Linear function approximation. In this setting rh(x, a, b) = φ(x, a, b)>θh and
Ph(∙∣x, a, b) = φ(x, a, b)>μh(∙) where φ(x, a, b) ∈ Rd. We have shown that this set-
ting satisfies all the realizability and completeness assumptions in the previous sections. By
standard Ellipsoid potential arguments, the critical information gain is upper bounded by
O(d ∙ log(R∕c)). Therefore, this setting also has low Minimax Eluder dimensions.
•	Tabular Markov Games. This setting can be seen as a special case of linear function
approximation where the feature vectors are chosen to be one-hot representations of state-
actions pairs. In this case, d = |S| ∙ |A|2 and thus Minimax Eluder dimensions is upper
bounded by O(∣S∣∙∣A∣2).
•	Overparameterized neural networks. In overparameterized neural network function
approximations, the functions are given by h(φ(x, a, b)) where h is the Neural Tangent
Random Feature space defined in Cao & Gu (2019). We rewrite the information gain
dn(γ, D) as maXχι,...,χn∈D logdet(I + 1 Kn) where (Kn)i,j = K(xi,Xj) and K is the
Neural Tangent Kernel. It has been shown (Jacot et al., 2018; Du et al., 2019c;a; Allen-Zhu
31
Published as a conference paper at ICLR 2022
et al., 2019) that with proper initialization and learning rates, the function approximations
approximately lie in the Reproducing kernel Hilbert space given by the neural tangent
feature as above. Therefore the Minimax Eluder dimensions are bounded by e(品,Φh) + λ,
where γe is the critical information gain defined by the dn(γ, D) rewritten above and λ is a
term that vanishes as width tends to infinity.
Generalized linear models. Finally, we consider generalized linear model as an extension of the
linear function approximations.
Theorem E.2. Suppose (fh - Thπ fh+1)(x, a, b) = g(φ(x, a, b)>θh) for any π ∈ Π and f ∈ F,
where g is a differentiable and strictly increasing function and φ, θh ∈ Rd. Assume g0 ∈ (c1, c2) and
kφ∣∣2, ∣∣θh∣∣2 ≤ R for c1,c2,R> 0. Then dimME，(F, e) ≤ O(d(c2∕c1)2 log(等)).
Proof. The proof mirrors those in Theorem E.1. For the longest sequence ν1, . . . , νk ∈ D∆ such
that each one item is -independent of its precedents, distinguished by a sequence of features
θh(1), . . . , θh(k), we define φi = Eνi [φh(x, a, b)] and θi = θh(i) - Thπθh(i+) 1 for all i ∈ [k]. Let
Λi = PT=I φτφ> + 9R2c21. Then Definition 3.3 gives
kφikΛ-1 ≥ ~ ∙ kφikΛ-1 ∙ kθikΛi ≥ — ∙ lhφi, θiil ≥ ~ ∙
i	i	c2
This means det(Λk) ≥ (1 + ∣1 )k-1. But by Elliptic potential lemma, det(Λk) ≤ (Rk + 9R^2c2 )d.
Combining these two inequalities gives k ≤ O(d(c2∕c1)2 log(C2R)).	□
F	Technical claims
This section lists the facts we use from the previous work.
Lemma F.1 (Cumulative error control, Lemma 26 of Jin et al. (2021a)). Given a function class G
defined on X with |g(x)| ≤ C for all (g, x) ∈ G × X and a family of probability measures D over X.
Suppose sequence {gk}kK=1 ⊂ G and {ρk}kH=1 ⊂ D such that for all k ∈ [K], Ptk=-11(Eρt [gk])2 ≤ β.
Then for all k ∈ [K] and ω > 0,
k / ______________________________________
T I E[gt]∣ ≤ O VZdimDe(G, DQBk + min{k, dimDE(G, D, e)}C + kω
t=1 ρt
Lemma F.2 (Lemma 11 of Jiang et al. (2017)). Consider a closed and bounded set V ⊂ Rd
and a vector p ∈ Rd. Let B be any origin-centered enclosing ellipsoid of V . Suppose there
exists v ∈ V such that p> v ≥ κ and define B+ as the minimum volume enclosing ellipsoid of
{v ∈ B : ∣p>v∣ ≤ 3√d}. With vol(∙) denoting the (Lebesgue) volume, we have:
vol(B+)	3
vol (B) — 5.
32