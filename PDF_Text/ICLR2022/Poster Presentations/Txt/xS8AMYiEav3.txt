Published as a conference paper at ICLR 2022
Sound and Complete Neural Network Repair
with Minimality and Locality Guarantees
Feisi Fu
Division of System Engineering
Boston University
fufeisi@bu.edu
Wenchao Li
Department of Electrical and Computer Engineering
Boston University
wenchao@bu.edu
Ab stract
We present a novel methodology for repairing neural networks that use ReLU ac-
tivation functions. Unlike existing methods that rely on modifying the weights
of a neural network which can induce a global change in the function space, our
approach applies only a localized change in the function space while still guar-
anteeing the removal of the buggy behavior. By leveraging the piecewise linear
nature of ReLU networks, our approach can efficiently construct a patch network
tailored to the linear region where the buggy input resides, which when combined
with the original network, provably corrects the behavior on the buggy input. Our
method is both sound and complete - the repaired network is guaranteed to fix the
buggy input, and a patch is guaranteed to be found for any buggy input. Moreover,
our approach preserves the continuous piecewise linear nature of ReLU networks,
automatically generalizes the repair to all the points including other undetected
buggy inputs inside the repair region, is minimal in terms of changes in the func-
tion space, and guarantees that outputs on inputs away from the repair region are
unaltered. On several benchmarks, we show that our approach significantly out-
performs existing methods in terms of locality and limiting negative side effects.
1	Introduction
Deep neural networks (DNNs) have demonstrated impressive performances on a wide variety of
applications ranging from transportation Bojarski et al. (2016) to health care Shahid et al. (2019).
However, DNNs are not perfect. In many cases, especially when the DNNs are used in safety-critical
contexts, it is important to correct erroneous outputs of a DNN as they are discovered after training.
For instance, a neural network in charge of giving control advisories to the pilots in an aircraft
collision avoidance system, such as the ACAS Xu network from Julian et al. (2019), may produce an
incorrect advisory for certain situations and cause the aircraft to turn towards the incoming aircraft,
thereby jeopardizing the safety of both airplanes. In this paper, we consider the problem of neural
network repair, i.e. given a trained neural network and a set of buggy inputs (inputs on which the
neural network produces incorrect predictions), repair the network so that the resulting network on
those buggy inputs behave according to some given correctness specification. Ideally, the changes to
the neural network function should be small so that the outputs on other inputs are either unchanged
or altered in a small way. Existing works on neural network repair roughly fall into three categories.
1.	Retraining/fine-tuning. The idea is to retrain or fine-tune the network with the newly identified
buggy inputs and the corresponding corrected outputs. Methods include counterexample-guided
data augmentation Dreossi et al. (2018); Ren et al. (2020), editable training Sinitsin et al. (2020)
and training input selection Ma et al. (2018). One major weakness of these approaches is the lack of
formal guarantees - at the end of retraining/fine-tuning, there is no guarantee that the given buggy
inputs are fixed and no new bugs are introduced. In addition, retraining can be very expensive and
requires access to the original training data which is impractical in cases where the neural network
is obtained from a third party or the training data is private. Fine-tuning, on the other hand, often
faces the issue of catastrophic forgetting Kirkpatrick et al. (2017).
2.	Direct weight modification. These approaches directly manipulate the weights in a neural
network to fix the buggy inputs. The repair problem is typically cast into an optimization problem
1
Published as a conference paper at ICLR 2022
Retraining or direct	Decoupled DNN	Our approach
weight modification
Figure 1: Comparison of different approaches to the neural network repair problem. The black lines
represent the original neural network function. The red dot represents the buggy input. The colored
lines represent the functions after the repairs are done.
or a verification problem. For example, Dong et al. (2020) proposes to minimize a loss defined on
the buggy inputs. Goldberger et al. (2020) uses an SMT solver to identify minimal weight changes
to the output layer of the network so that the undesirable behaviors are removed. Usman et al. (2021)
first locates potentially faulty weights in a layer and then uses constraint solving to find a weights
modification that can fix the failures. In general, the optimization-based approach cannot guarantee
removal of the buggy behaviors, and the verification-based approach does not scale beyond networks
of a few hundred neurons. In addition, these approaches can suffer from substantial accuracy drops
on normal inputs since weight changes may be a poor proxy for changes in the function space.
3.	Architecture extension. The third category of approaches extends the given NN architecture,
such as by introducing more weight parameters, to facilitate more efficient repairs. The so-called
Decoupled DNN architecture Sotoudeh & Thakur (2021) is the only work we know that falls into
this category. Their idea is to decouple the activations of the network from values of the network
by augmenting the original network. Their construction allows the formulation of any single-layer
repair as an linear programming (LP) problem. The decoupling, however, causes the repaired net-
work to become discontinuous (in the functional sense). In addition, it still cannot isolate the output
change to a single buggy input from the rest of the inputs.
In addition to the aforementioned limitations, a common weakness that is shared amongst these
methods is that the induced changes, as a result of either retraining or direct weight modification,
are global. This means that a correct behavior on another input, regardless of how far it is away from
the buggy input, may not be preserved by the repair. Worse still, the repair on a new buggy input can
end up invalidating the repair on a previous buggy input. The fundamental issue here is that limiting
the changes to a few weights or a single layer only poses a structural constraint (often for ease of
computation); it does not limit the changes on the input-output mapping of the neural network. It is
known that even a single weight change can have a global effect on the output of a neural network.
In this paper, we propose REASSURE, a novel methodology for neural network repair with locality,
minimality, soundness and completeness guarantees. Our methodology targets continuous piecewise
linear (CPWL) neural networks, specifically those that use the ReLU activation functions. The key
idea of our approach is to leverage the CPWL property of ReLU networks to synthesize a patch
network tailored to the linear region where the buggy input resides, which when combined with the
original network, provably corrects the behavior on the buggy input. Our approach is both sound
and complete - the repaired network is guaranteed to fix the buggy input, and a patch is guaranteed
to be found for any buggy input. Moreover, our approach preserves the CPWL nature of ReLU
networks, automatically generalizes the repair to all the points including other undetected buggy
inputs inside the repair region, is minimal in terms of changes in the function space, and guarantees
that outputs on inputs away from the repair region are unaltered. Figure 1 provides an illustrative
comparison of our approach with other methods. Table 1 compares our approach with representative
related works in terms of theoretical guarantees. We summarize our contributions below.
1.	We present REASSURE, the first sound and complete repair methodology for ReLU networks
with strong theoretical guarantees.
2.	Our technique synthesizes a patch network, which when combined with the original neural
network, provably corrects the behavior on the buggy input. This approach is a significant departure
from existing methods that rely on retraining or direct weight manipulation.
3.	Across a set of benchmarks, REASSURE can efficiently correct a set of buggy inputs or buggy
areas with little or no change to the accuracy and overall functionality of the network.
2
Published as a conference paper at ICLR 2022
	REASSURE	Retrain	MDNN	Editable Fine-Tuning	PRDNN
Preservation of CPWL	Yes	Yes	Yes	Yes	No
Soundness	Yes	No	Yes	No	Yes
Completeness	Yes	No	No	No	No
Area Repair	Yes	No	No	No	Yes
Minimal Change	Yes (Function Space)	No	Yes (Weight Space)	No	Yes (Weight Space)
Localized Change	Yes	No	No	No	No
Limited Side Effect	Yes	No	No	No	No
Table 1: Comparing REASSURE with representative related works in terms of theoretical guarantees.
CPWL stands for continuous piecewise linearity. Area repair means repairing all the (infinitely
many) points inside an area. Limited side effect means the repair can limit potential adverse effects
on other inputs. MDNN is the verification-based approach from Goldberger et al. (2020). PRDNN
is the Decoupled DNN approach from Sotoudeh & Thakur (2021). REASSURE is the only method
that can provide all the guarantees.
2	Background
2.1	Deep Neural Networks
An R-layer feed-forward DNN f = κR ◦ σ ◦ κR-1 ◦ ... ◦ σ ◦ κ1 : X → Y is a composition of
linear functions κr, r = 1, 2, ..., R and activation function σ, where X ⊆ Rm is a bounded input
domain and Y ⊆ Rn is the output domain. Weights and biases of linear function {κr}r=1,2,...,R are
parameters of the DNN.
We call the first R - 1 layers hidden layers and the R-th layer the output layer. We use zji to denote
the i-th neuron (before activation) in the j-th hidden layer.
In this paper, we focus on ReLU DNNs, i.e. DNNs that use only the ReLU activation functions.
It is known that an Rm → R function is representable by a ReLU DNN if and only if it is a
continuous piecewise linear (CPWL) function Arora et al. (2016). The ReLU function is defined as
σ(x) = max(x, 0). We say that σ(x) is activated if σ(x) = x.
2.2	Linear Regions
A linear region A is the set of inputs that correspond to the same activation pattern in a ReLU DNN
f Serra et al. (2017). Geometrically, this corresponds to a convex polytope, which is an intersection
of half spaces, in the input space X on which f is linear. We use f|A to denote the part of f on A.
2.3	Correctness Specification
A correctness specification Φ = (Φin , Φout) is a tuple of two polytopes, where Φin is the union
of some linear regions and Φout is a convex polytope. A DNN f is said to meet a specification
Φ = (Φin, Φout), denoted as f |= Φ, if and only if∀x ∈ Φin, f(x) ∈ Φout.
Example 1. For a classification problem, we can formally write the specification that “the pre-
diction of any point in an area A is class k” as Φ = (Φin , Φout), where Φin = A and
Φout = {y ∈ Rn | yk ≥ yi, ∀i 6= k}1.
2.4	Problem Definition
In this paper, we consider the following two repair problems.
Definition 1 (Area repair). Given a correctness specification Φ = (Φin , Φout) and a ReLU DNN
f 6|= Φ, the area repair problem is to find a modified ReLU DNN f such that f |= Φ.
Note that we do not require f to have the same structure or parameters as f in this definition. If
Φin contains a single (buggy) linear region, we refer to this as single-region repair. If Φin contains
multiple (buggy) linear regions, we refer to it as multi-region repair.
1 Note that here y is the output of the layer right before the softmax layer in a classification network.
3
Published as a conference paper at ICLR 2022
Definition 2 (Point-wise repair). Given a set of buggy inputs {xe1, . . . , xeL} ⊂ Φin with their corre-
sponding correct outputs {y1, . . . , yL} and a ReLU DNN f, the point-wise repair problem is to find
^

a modified ReLU DNN f such that ∀i, f (xei) = yi.
We call the minimal variants of area repair and point-wise repair minimal area repair and minimal
point-wise repair respectively. Minimality here is defined with respect to the maximum distance
between f and f over the whole input domain X. A point-wise repair can be generalized to an area
repair through the following result.
2.5	From Buggy Inputs to Buggy Linear Regions
The linear region where an input x resides can be computed as follows.
Lemma 1. Lee et al. (2019) Consider a ReLU DNN f and an input x ∈ X. For every neuron zji, it
induces a feasible set
Aij(x)
∫{X ∈ X|(Oxzj)Tx + Zj-(Oxzj)Tx ≥ 0}
[{X ∈ XI(Oxzj)Tx + Zj-(Oxzj)Tx ≤ 0}
if zji ≥ 0
if zji < 0
(1)
The set A(x) = ∩i,j Aij (x) is the linear region that includes x. Note that A(x) is essentially the
H-representation of the corresponding convex polytope.
2.6	Repair Desiderata
We argue that an effective repair algorithm for ReLU DNN should satisfy the following criteria.
1.	Preservation of CPWL: Given that the original network f models a CPWL function, the repaired
network f should still model a CPWL function. 2. Soundness: A sound repair should completely
remove the known buggy behaviors, i.e. it is a solution to the point-wise repair problem defined
in Definition 2. 3. Completeness: Ideally, the algorithm should always be able find a repair for
any given buggy input if it exists. 4. Generalization: If there exists another buggy input xe0 in
the neighborhood of xe (e.g. the same linear region), then the repair should also fix it. For example,
suppose we have an xe that violates a specification which requires the output to be within some range.
It is almost guaranteed that there exists another (and infinitely many) xe0 in the same linear region
that also violates the specification. 5. Locality: We argue that a good repair should only induce a
localized change to f in the function space. For example, in the context of ReLU DNN, if a linear
region B does not border the repair region A,i.e. B∩A = 0, then f∣B(x) = f |b (x). 6. Minimality:
Some notion of distance between f and f such as max |f-f| should be minimized. Note that this is
a significant departure from existing methods that focus on minimizing the change in weights which
has no guarantee on the amount of change in the function space. 7. Limited side effect: Repairing
a buggy point should not adversely affect points that were originally correct. For example, repairing
a buggy input xe in region A should not change another region from correct to incorrect. Formally,
for any linear region C who is a neighbor of A, i.e. C ∩ A 6= 0, if f|C |= Φ, then f|C |= Φ. 8.
Efficiency: The repair algorithm should terminate in polynomial time with respect to the size of the
neural network and the number of buggy inputs.
3	Our Approach
We will first describe our approach to single-region repair and then present our approach to multi-
region repair which builds on results obtained from the single-region case.
Given a linear region A, our overarching approach is to synthesize a patch network hA such that
f = f + hA and f |= Φ. The patch network hA is a combination of two sub-networks: a support
network gA , which behaves like a characteristic function, to determine whether an input is in A, and
an affine patch function networkpA(x) = cx + d to ensure (f + pA) |= Φ on A.
3.1	Running Example
We use the following example to illustrate our idea.
4
Published as a conference paper at ICLR 2022
Figure 2: Left: the target DNN with buggy inputs. Right: the REASSURE-repaired DNN with the
patch network shown in red. Support network gA is for approximating the characteristic function
on A; Affine patch function pA ensures the satisfaction of Φ on A; The design of the patch network
hA ensures locality for the final patch.
Example 2. Consider repairing the ReLU DNN f in Figure 2 according to the correctness specifi-
cation Φ : ∀x ∈ [0, 1]2, y ∈ [0, 2]. The DNN consists of a single hidden layer with two neurons z1
and z2, where y = σ(z1) + σ(z2), z1 = x1 + 2x2 - 1 and z2 = 2x1 - x2.
The only linear region that violates our specification is A = {x | 1 ≥ x1, 1 ≥ x2, x1 + 2x2 - 1 ≥
0, 2x1 - x2 ≥ 0}. (xe= (0.9, 0.9) ∈ [0, 1]2 butf(xe) = 2.6 ∈/ [0, 2])
The network f(x) on the linear region A is the affine function f|A(x) = 3x1 + x2 - 1. Our
algorithm first sets up an affine function pA (x) that minimally repairs f on A, such that ∀x ∈
A, f(x) + pA(x) ∈ [0, 2]. Later in the paper, we will show pA(x) can be found by solving a LP
problem. The resulting patch function is p∕(x) = -1 xι - 2x2.
However, directly apply f(x) + pA(x) as the patch network will have side effects on areas outside
of A. Our strategy is to combine pA(x) with a support network gA(x) which outputs 1 on A and
drops to 0 quickly outside of A. The final repaired network is f(x) + σ(pA(x) + gA(x, 10) -
1) - σ(-pA(x) + gA(x, 10) - 1). This structure makes pA almost only active on A and achieve a
localized repair. Observe that this is still a ReLU DNN.
3.2	Support Networks
Support networks are neural networks with a special structure that can approximate the characteristic
function of a convex polytope. They are keys to ensuring localized repairs in our algorithm.
Assume that the linear region we need to repair is A = {x | aix ≤ bi, i ∈ I}, where |I| is the number
of linear inequalities. The support network ofA is defined as:
gA(x,γ) = σ(	g(bi - aix, γ) - |I| + 1)	(2)
i∈I
where g(x, γ) = σ(γx + 1) - σ(γx) and γ ∈ R is a parameter of our algorithm that controls how
quickly gA (x, γ ) goes to zero outside of A.
Remark: For any x ∈ A, we have gA(x, γ) = 1, i.e. the support network is fully activated. For any
x ∈ A, if for one of i ∈ I, We have aix - b ≤ -1∕γ, then g∕(x, Y) = 0.
Observe that gA(x, γ) is not zero when x is very close to A due to the requirement for preserving
CPWL. In Theorem 3, We prove that We can still guarantee limited side effects on the Whole input
domain outside of A With this construction.
3.3	Affine Patch Functions
We consider an affine patch function pA (x) = cx+d, Where matrix cand vector d are undetermined
coefficients. In a later section, the design of patch netWork Will ensure that on the patch area A, the
repaired netWork is f (x) + pA(x). We Will first consider finding appropriate c and d such that
f(x) + pA(x) satisfy the specification on A.
5
Published as a conference paper at ICLR 2022
To satisfy the specification Φ, we need f (x) + pA (x) ∈ Φout for all x ∈ A. To obtain a minimal
repair, we minimize maxx∈A |pA(x)|. Thus, we can formulate the following optimization problem
minc,d maxx∈A |pA(x)| = |cx + d|
(c, d) ∈ {(c, d) | f(x) + cx + d ∈ Φout,∀x ∈ A}
(3)
Notice that this is not an LP since both c and x are variables and we have a cx term in the objective.
In general, one can solve it by enumerating all the vertices of A. Suppose that {vs|s = 1, 2, ..., S}
is the set of vertices of A. Since Φout is a convex polytope, we have
f(x) +pA(x) ∈ Φout for all x ∈ A ⇔ f(vs) +pA(vs) ∈ Φout for s = 1, 2, ..., S.	(4)
and
maxx∈A |cx + d| = maxs=1,2,...,S |cvs + d|	(5)
Hence, we can solve the following equivalent LP.
{min0,d H
H ≥ (cvs + d)i, H ≥ -(cvs + d)i, for s = 1, 2, ..., S and i = 1, 2, ..., m	(6)
f(vs) +pA(vs) ∈ Φout, fors = 1,2, ..., S
where H ∈ R and will take maxs=1,2,...,S |cvs + d| when optimal.
In general, the number of vertices of a convex polytope can be exponential in the size of its H-
representation Henk et al. (1997) and enumerating the vertices of a convex polytope is known to be
expensive especially when the input dimension is large Bremner (1997). In Appendix 7.1, we show
that we can solve programming 3 via LP without vertex enumeration for many useful cases such as
the classification problem in Example 1 and make our algorithm much more efficient.
3.4	Single-Region Repairs
With a support network gA and an affine patch function pA , we can synthesize the final patch
network as follows:
h∕(x, Y) = σ(p∕(x) + K ∙ g∕(x, Y) - K) - σ(-p∕(x) + K ∙ g∕(x, Y) - K)
(7)
where K is a vector where every entry is equal to the upper bound of {∣pa(x)∣+∞∣x ∈ X}.
Remark: For x ∈ A, gA(x, Y) = 1, then we have hA(x, Y) = σ(pA(x)) - σ(-pA(x)) = pA(x).
For x ∈/ A, gA(x, Y) goes to zero quickly if Y is large. When gA(x, Y) = 0, we have hA(x, Y) =
σ(pA(x) - K) - σ(-pA(x) - K) = 0.
^

The repaired network f(x) = f(x) + hA(x, Y). Since f and hA are both ReLU DNNs, we have f
is also a ReLU DNN. We will give the formal guarantees on correctness in Theorem 1.
3.5	Multi-Region Repairs
Suppose there are two linear regions, A1 and A2, that need to be repaired, and we have generated
the affine patch function pA1 (x) for A1 and pA2 (x) for A2.
If Ai ∩A2 = 0, then We can repair f (x) With f (x) = f (x) + h∕ι (x, Y) + h∕2 (x, Y) directly, since
hA1 (x, Y) and hA2 (x, Y) will not be nonzero at the same time when Y is large enough.
However, if A1 ∩ A2 6= 0, for any x ∈ A1 ∩ A2, both hA1 (x, Y) and hA2 (x, Y) will alter the value
of f on x, which will invalidate both repairs and cannot guarantee that the repaired DNN will meet
the specification Φ. To avoid such over-repairs, our strategy is to first repair A1 ∪ A2 with pA1 (x),
and then repair A2 with pA2 (x) - pA1 (x). Figure 3 provides an illustration ofa three-region case.
In general, for multi-region repair, we note {Al}l=1,2,...,L are all the buggy linear regions. Then we
compute the support network gAl (x, Y) and affine patch function pAl (x) for each Al. Note that this
computation can be done in parallel.
6
Published as a conference paper at ICLR 2022
Figure 3: An illustration of multi-region repair with three different repair regions. Left: the original
DNN; Middle Left: repair A1 ∪ A2 ∪ A3 with pA1 ; Middle Right: repair A2 ∪ A3 with pA2 - pA1 ;
Right: repair A3 with pA3 - pA2
Once we have gAl (x, γ) and pAl (x), we can “stitch” multiple local patches into a final patch as
follows.
h(x, γ) =	[σ (pAl (x) -pAl-1(x) + max{gAj (x, γ)}Kl - Kl)
-σ(-pAl (x) +pAl-1(x) + max{gAj(x,γ)}Kl - Kl)]	(8)
where K is the upper bound of {∣p4 (x) - Pa1-1 (χ)∣∞∣χ ∈ X} andp∕0 (x) = 0.
Remark: maxj≥l{gAj (x, γ)} is a support function for ∪j≥lAj and its value is 1 for any x ∈
∪j≥lAj.
4 Theoretical Guarantees
In this section, we present the theoretical guarantees that REASSURE provides, and point the readers
to proofs of the theorems in the Appendix.
Theorem 1	(Soundness). The repaired DNN f returned by REASSURE is guaranteed to satisfy the
specification Φ.
Theorem 2	(Completeness). REASSURE can always find a solution to the minimal point-wise repair
or the minimal area repair problem.
For any A, the support network ensures that the patch network goes to zero quickly when x is away
from A. However, it still makes a small change on the neighbors of A. The following theorem
shows that for a big enough γ, the patch network would not change a correct region into incorrect.
Theorem 3	(Limited Side Effect). Given a correctness property Φ = (Φin, Φout), a patch region
A and the corresponding patch network h(x, γ), there exists a positive number Γ such that for any
γ ≥ Γ, we have
Ir	ι∙	∙ rɔ ∙ r rɔ A CK , ι	∖ r / ∖
1.	for any linear region B, ifB ∩ A = 0, then f(x, γ) = f (x);
` r	1 ∙	∙ C 1 ∙	∙ 1 1	A / CK ∖ ∙rrl I w ,t 2 /	∖ I W
2.	for any linear region C who is a neighbor of A (C∩A= 0), if f|c = Φ ,then fc (x,γ) = Φ.
Corollary 1 (Incremental Repair). For multiple-region repair, the patch for a new region A0 would
not cause a previous patched region A to become incorrect.
Theorem 4	(Minimum Repair). For any ReLU DNN f, which is linear on a patch region A and
satisfies the specification Φ, there exists a positive number Γ, such that for all γ ≥ Γ,
max |f (x) — f (x)| ≥ max ∣h∕(x, Y)|.
x∈X	x∈X
(9)
Theorem 5	(Polynomial-Time Efficiency). REASSURE terminates in polynomial-time in the size of
the neural network and the number of buggy linear regions when Φout takes the form of {y | ql ≤
Py ≤ qu} where P is a full row rank matrix and -∞ ≤ ql[i] ≤ qu[i] ≤ +∞ (ql [i] and qu[i] are the
i-th elements of ql and qu respectively).
5 Experiments
In this Section, we compare REASSURE with state-of-the-art methods on both point-wise repairs
and area repairs. The experiments were designed to answer the following questions: (Effectiveness)
7
Published as a conference paper at ICLR 2022
#P	ND(L∞)	ND(L2)	REASSURE NDP(L∞)	NDP(L2)	Acc	ND(L∞)	Retrain (Requires Training Data) ND(L2)	NDP(L∞) NDP(L2)			Acc
10	0.01%	0.01%	25.75%	24.78%	98.1%	1.13%	1.09%	77.86%	77.60%	98.1%
20	0.03%	0.02%	19.17%	18.70%	98.2%	0.92%	0.89%	77.14%	76.29%	98.4%
50	0.06%	0.06%	24.64%	23.79%	98.5%	0.84%	0.82%	84.17%	82.15%	98.7%
100	0.11%	0.12%	25.40%	24.44%	99.0%	0.84%	0.82%	84.83%	83.05%	99.0%
#P	ND(L∞)	ND(L2)	Fine-Tuning NDP(L∞)	NDP(L2)	Acc	ND(L∞)	ND(L2)	PRDNN NDP(L∞)	NDP(L2)	Acc
10	2.20%	2.11%	67.61%	65.52%	97.6%	1.41%	1.34%	34.60%	33.59%	97.8%
20	23.19%	22.35%	82.87%	78.55%	78.6%	2.88%	2.74%	43.63%	41.78%	97.1%
50	35.78%	34.04%	84.73%	80.58%	67.0%	4.79%	4.47%	49.37%	46.31%	96.7%
100	23.83%	22.11%	79.73%	76.57%	81.9%	9.16%	8.20%	51.23%	46.34%	96.1%
Table 2: Point-wise Repairs on MNIST. We use the first hidden layer as the repair layer for PRDNN.
The test accuracy of the original DNN is 98.0%. #P: number of buggy points to repair. ND(L∞),
ND(L2): average (L∞, L2) norm difference on both training and test data. NDP(L∞), NDP(L2):
average (L∞, L2) norm difference on random sampled points near the buggy points. Acc: accuracy
on test data. Note that REASSURE automatically performs area repairs on 784-dimensional inputs.
How effective is a repair in removing known buggy behaviors? (Locality) How much side effect (i.e.
modification outside the patch area in the function space) does a repair produce? (Function Change)
How much does a repair change the original neural network in the function space? (Performance)
Whether and how much does a repair adversely affect the overall performance of the neural network?
We consider the following evaluation criteria: 1. Efficacy (E): % of given buggy points or
buggy linear regions that are repaired. 2. Norm Difference (ND): average normalized norm
(L∞ or L2) difference between the original DNN and the repaired DNN on a set of inputs (e.g.
training and testing data; more details in the tables). We use ND to measure how a repair change the
original neural network on function space. 3. Norm Difference on Patch Area (NDP): average
normalized norm (L∞ or L2) difference between the original DNN and the repaired DNN on patch
areas (calculated on random sampled points on patch areas or near the buggy points; details in the
tables). We use NDP to measure the locality of a repair. 4. Accuracy (Acc): accuracy on training
or testing data to measure the extent to which a repair preserves the performance of the original
neural network. 5. Negative Side Effect (NSE): NSE is only for area repair. It is the percentage
of correct linear regions (outside of patch area) that become incorrect after a repair. If a repair has a
nonzero NSE, the new repair may invalidate a previous repair and lead to a circular repair problem.
We compared REASSURE with the representative related works in Table 1. REASSURE, MDNN and
PRDNN guarantee to repair all the buggy points (linear regions). Retrain and Fine-Tuning cannot
guarantee 100% efficacy in general and we run them until all the buggy points are repaired.
5.1	Point-wise Repairs: MNIST
We train a ReLU DNN on the MNIST dataset LeCun (1998) as the target DNN. The goal of a repair
is to fix the behaviors of the target DNN on buggy inputs that are found in the test dataset. Thus, the
repaired DNN is expected to produce correct predictions for all the buggy inputs.
The results are shown in Table 2. REASSURE achieves almost zero modification outside the patch
area (ND) amongst all four methods. In addition, REASSURE produces the smallest modification on
the patch area (NDP) and preserves the performance of the original DNN (almost no drop on Acc).
5.2	AREA REPAIRS: HCAS
To the best of our knowledge, Sotoudeh & Thakur (2021) is the only other method that supports
area repairs. In this experiment, we compare REASSURE with Sotoudeh & Thakur (2021) on an
experiment where the setting is similar to the 2D Polytope ACAS Xu repair in their paper.
Sotoudeh & Thakur (2021) does not include a vertex enumeration tool (which is required for setting
up their LP problem) in their code. We use pycddlib Troffaes (2018) to perform the vertex
enumeration step when evaluating PRDNN. Note that the vertex enumeration tool does not affect
the experimental results except running time.
8
Published as a conference paper at ICLR 2022
REASSURE	PRDNN
#A	ND(L∞)	NDP(L∞)	NSE	Acc	T	ND(L∞)	NDP(L∞)	NSE	Acc	T
10	0.00%	0.0%	0%	98.1%	1.0422	0.10%	31.6%	4%	89.6%	2.90+0.100
20	0.00%	2.2%	0%	98.1%	1.1856	0.15%	37.2%	8%	83.1%	5.81+0.185
50	0.00%	17.6%	0%	98.1%	1.8174	0.15%	38.4%	8%	83.8%	14.54+0.388
87	0.04%	45.9%	0%	97.8%	2.4571	0.14%	46.6%	0%	85.6%	25.30+0.714
Table 3: Area Repairs on HCAS. We use the the first hidden layer as the repair layer for PRDNN.
Results on PRDNN using the last layer (which are inferior to using the first layer) are shown in
Table 6 in the Appendix 7.4. The test accuracy of the original DNN is 97.9%. #A: number of buggy
linear regions to repair. ND(L∞): average L∞ norm difference on training data. NDP(L∞): average
L∞ norm difference on random sampled data on input constraints of specification 1. NSE: % of
correct linear regions changed to incorrect by the repair. Acc: accuracy on training data (no testing
data available). T: running time in seconds. For PRDNN, the first running time is for enumerating
all the vertices of the polytopes and the second is for solving the LP problem in PRDNN.
REASSURE (feature space)
#P ND(L∞) ND(L2)	Acc
Retrain (Requires Training Data)
ND(L∞)	ND(L2)	Acc
10	0.15%	0.13%	82.5%	43.43%	36.04%	80.1%
20	0.12%	0.11%	81.3%	42.78%	35.69%	82.9%
50	0.79%	0.70%	81.3%	50.23%*	42.86%*	82.1% *
Fine-Tuning
ND(L∞)	ND(L2)	Acc
29.45%	25.27%	77.9%
69.16%	57.47%	68.5%
76.69%	63.46%	66.9%
PRDNN
ND(L∞)	ND(L2)	Acc
22.93%	21.13%	82.1%
21.91%	20.03%	80.1%
30.96%	26.94%	68.9%
Table 4: Point-wise Repairs on ImageNet. PRDNN uses parameters in the last layer for repair.
The test accuracy for the original DNN is 83.1%. #P: number of buggy points to repair. ND(L∞),
ND(L2): average (L∞, L2) norm difference on validation data. Acc: accuracy on validation data. *
means Retrain only repair 96% buggy points in 100 epochs.
We consider an area repair where the target DNN is the HCAS network (simplified version of ACAS
Xu)2 N1,4 (previous advisory equal to 1 and time to loss of vertical separation equal to 20s) from
Julian & Kochenderfer (2019). We use Specification 1 (details in Appendix 7.4), which is similar
to Property 5 in Katz et al. (2017). We compute all the linear regions for N1,4 in the area Φin of
Specification 1 and 87 buggy linear regions were found. We apply both REASSURE and PRDNN
to repair those buggy linear regions. We use Specification 2 (details in Appendix 7.4), the dual of
Specification 1, to test the negative side effect (NSE) of a repair.
The results are shown in Table 3. Both REASSURE and PRDNN successfully repair all the buggy
linear regions. REASSURE produces repairs that are significantly better in terms of locality (ND),
minimality (NDP) and performance preservation (Acc).
5.3 Feature-Space Repairs
In general, when repairing a large DNN with a high input dimension, the number of linear constraints
for one patch area A will be huge and pose a challenge to solving the resulting LP.
One advantage of our approach, which can be used to mitigate this problem, is that it allows for
point-wise and area repairs in the feature space in a principled manner, i.e. constructing a patch
network starting from an intermediate layer. This approach still preserves soundness and complete-
ness, and is fundamentally different from just picking a single layer for repair in PRDNN or MDNN.
Experimental results on repairing AlexNet Krizhevsky et al. (2012) for the ImageNet dataset Rus-
sakovsky et al. (2015) in Appendix 7.4 show REASSURE (feature space) is still significantly better
in term of locality(ND) and minimality (NDP).
6 Conclusion
We have presented a novel approach for repairing ReLU DNNs with strong theoretical guarantees.
Across a set of benchmarks, our approach significantly outperforms existing methods in terms of
efficacy, locality, and limiting negative side effects. Future directions include further investigation
on feature-space repairs and identifying a lower-bound for γ .
2The technique in PRDNN for computing linear regions does not scale beyond two dimensions as stated in
their paper. The input space of HCAS is 3D and that of ACAS Xu is 5D so we use HCAS in order to run their
tool in our evaluation of area repairs.
9
Published as a conference paper at ICLR 2022
Acknowledgement
This effort was partially supported by the Intelligence Advanced Research Projects Agency (IARPA)
under the contract W911NF20C0038. The content of this paper does not necessarily reflect the
position or the policy of the Government, and no official endorsement should be inferred.
References
Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, and Joseph Keshet. Turning your weak-
ness into a strength: Watermarking deep neural networks by backdooring. In 27th {USENIX}
Security Symposium ({ USENIX} Security 18), pp. 1615-1631, 2018. 16
Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep neural
networks with rectified linear units. CoRR, abs/1611.01491, 2016. URL http://arxiv.
org/abs/1611.01491. 3, 18
Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon
Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin Zhang, Jake Zhao,
and Karol Zieba. End to end learning for self-driving cars. CoRR, abs/1604.07316, 2016. 1
David D Bremner. On the complexity of vertex and facet enumeration for convex polytopes. PhD
thesis, Citeseer, 1997. 6
Guoliang Dong, Jun Sun, Jingyi Wang, Xinyu Wang, and Ting Dai. Towards repairing neural net-
works correctly. arXiv preprint arXiv:2012.01872, 2020. 2
Tommaso Dreossi, Shromona Ghosh, Xiangyu Yue, Kurt Keutzer, Alberto Sangiovanni-
Vincentelli, and Sanjit A Seshia. Counterexample-guided data augmentation. arXiv preprint
arXiv:1805.06962, 2018. 1
Ben Goldberger, Guy Katz, Yossi Adi, and Joseph Keshet. Minimal modifications of deep neural
networks using verification. In LPAR, volume 2020, pp. 23rd, 2020. 2, 3, 16
Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2021. URL https://www.
gurobi.com. 16
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adver-
sarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 15262-15271, 2021. 16
Martin Henk, Jurgen Richter-Gebert, and Gunter M. Ziegler. Basic properties of convex polytopes.
In HANDBOOK OF DISCRETE AND COMPUTATIONAL GEOMETRY, CHAPTER 13, pp. 243-
270. CRC Press, Boca, 1997. 6
Kyle D Julian and Mykel J Kochenderfer. Guaranteeing safety for neural network-based aircraft col-
lision avoidance systems. In 2019 IEEE/AIAA 38th Digital Avionics Systems Conference (DASC),
pp. 1-10. IEEE, 2019. 9
Kyle D Julian, Mykel J Kochenderfer, and Michael P Owen. Deep neural network compression
for aircraft collision avoidance systems. Journal of Guidance, Control, and Dynamics, 42(3):
598-608, 2019. 1
Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An
efficient smt solver for verifying deep neural networks. In International Conference on Computer
Aided Verification, pp. 97-117. Springer, 2017. 9
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A.
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hass-
abis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forget-
ting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521-3526,
2017. ISSN 0027-8424. doi: 10.1073/pnas.1611835114. URL https://www.pnas.org/
content/114/13/3521. 1
10
Published as a conference paper at ICLR 2022
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep con-
VolUtional neural networks. Advances in neural information processing Systems, 25:1097-1105,
2012. 9, 16, 17
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
8
Guang-He Lee, DaVid AlVarez-Melis, and Tommi S Jaakkola. Towards robust, locally linear deep
networks. arXiv preprint arXiv:1907.03207, 2019. 4
Shiqing Ma, Yingqi Liu, Wen-Chuan Lee, Xiangyu Zhang, and Ananth Grama. Mode: automated
neural network model debugging Via state differential analysis and input selection. In Proceed-
ings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, pp. 175-186, 2018. 1
Xuhong Ren, Bing Yu, Hua Qi, Felix Juefei-Xu, Zhuo Li, Wanli Xue, Lei Ma, and Jianjun Zhao.
Few-shot guided mix for dnn repairing. In 2020 IEEE International Conference on Software
Maintenance and Evolution (ICSME), pp. 717-721. IEEE, 2020. 1
Olga RussakoVsky, Jia Deng, Hao Su, Jonathan Krause, SanjeeV Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV), 115(3):211-252, 2015. doi: 10.1007/s11263-015-0816-y. 9, 16
Thiago Serra, Christian Tjandraatmadja, and Srikumar Ramalingam. Bounding and counting linear
regions of deep neural networks. CoRR, abs/1711.02114, 2017. URL http://arxiv.org/
abs/1711.02114. 3
Nida Shahid, Tim Rappon, and Whitney Berta. Applications of artificial neural networks in
health care organizational decision-making: A scoping reView. PLOS ONE, 14(2):1-22, 02
2019. doi: 10.1371/journal.pone.0212356. URL https://doi.org/10.1371/journal.
pone.0212356. 1
Anton Sinitsin, VseVolod Plokhotnyuk, Dmitriy Pyrkin, Sergei PopoV, and Artem Babenko. Editable
neural networks. arXiv preprint arXiv:2004.00345, 2020. 1
Matthew Sotoudeh and Aditya V Thakur. ProVable repair of deep neural networks. In Proceedings
of the 42nd ACM SIGPLAN International Conference on Programming Language Design and
Implementation, pp. 588-603, 2021. 2, 3, 8
Matthias Troffaes. pycddlib-a python wrapper for komei fukudals cddlib, 2018. 8
Muhammad Usman, Divya GoPinath, YoUcheng Sun, Yannic Noller, and Corina S Pasareanu. Nn
repair: Constraint-based repair of neural network classifiers. In International Conference on
Computer Aided Verification, PP. 3-25. SPringer, 2021. 2
11
Published as a conference paper at ICLR 2022
7 Appendix
7.1 Repair via Linear Programming
We consider the case Φout can be expressed as {y | ql ≤ Py ≤ qu} where P is a full row rank matrix
and -∞ ≤ ql [i] ≤ qu[i] ≤ +∞ (ql [i] and qu[i] are the i-th elements of ql and qu respectively).
Consider the following optimization problem.
minT maxx∈A |T (f (x) - f (x)|
ql ≤ P(T(f(x))) ≤ qu,∀x∈A
(10)
where T : Rn → Rn is a linear transformation on the DNN’s output space Rn .
Theorem 6. On linear region A, we have f |A(x) = f1x + f2 for some matrix f1 and vector f2.
Assuming that f1 is full rank3, the optimization problem in (3) and the optimization problem in (10)
are equivalent.
Note that ql ≤ P (T (f (x))) ≤ qu can be achieved row by row. Thus, we can find a one-dimensional
linear transformation via LPs and combine them into a single linear transformation to solve opti-
mization problem 10.
For every row of P , say the i-th row, we can check the lower bound and upper bound of
{P (f (x)) | ∀x ∈ A} on the i-th dimension by solving the following LP problems
lb[i] = min P [i](f (x)) ub[i] = max P [i](f (x))	(11)
x∈A	x∈A
where P [i] is the i-th row of P.
Then for each row i, we take a minimal linear transformation V [i](x) = v1 [i](x) + v2 [i] to transfer
interval [lb[i], ub[i]] inside interval [ql [i], qu[i]]. We can take v1 [i] = 1 if qu[i] - ql [i] > ub[i] - lb[i],
else vι[i]=北[：]-湍.And v2[i] = qι[i] - vι[i]lb[i] if ∣qι[i] - vι[i]lb[i]∣ ≤ ∣vι[i]ub[i] - qu[i]∣, else
v1 [i]ub[i] - qu [i].
Since matrix P is full row rank, we can find a linear transformation T that is equivalent to V :
^	-1	^	........ ..............
T = P TVP ⇒ P (T (f (x))) = V (P(f (x)))	(12)
where P = PPL is an orthogonal extension of P (P and P⊥ are orthogonal to each other and
P is a full rank square matrix).
Once we have T, we can obtain an affine patch function pA(x) = T (f (x)) - f (x).
7.2	The REASSURE Algorithm
Algorithm 1 REASSURE
Input: A specification Φ = (Φin, Φout), a ReLU DNN f and a set of buggy points {xe1, . . . , xeL} ⊂
Φin.
λ . _________________________^
Output: A repaired ReLU DNN f .
1:	for l = 1 to L do
2:	Generate the patch area Al from buggy point xel according to Equation (1);
3:	Generate a support network gA according to Equation (2);
4:	Solve the linear programming problem (6) to find the optimal affine patch networkpA.
5:	end for
6:	Combine all support networks gAl and the corresponding patch networks pAl to get the overall
patch network h according to Equation (8).
7:	return f = f + h
3 Note that for neural networks that are trained by a stochastic method, with probability one f1 is full rank.
12
Published as a conference paper at ICLR 2022
7.3	Proofs of Theorems
We prove Theorem 1 after Corollary 1, since the proof of Theorem 1 uses the result of Corollary 1.
Theorem 6. On linear region A, we have f |A(x) = f1x + f2 for some matrix f1 and vector f2.
Assuming that f1 is full rank4, the optimization problem in (3) and the optimization problem in (10)
are equivalent.
Proof. On one side, for any c, d, since f1 is full rank, there exists a linear transformation T , such
that T (f (x)) = T(f1x + f2) = (f1 + c)x + (f2 + d) = f(x) + cx + d.
On the other side, for any T, since T (f (x)) - f(x) is linear, there exist c, d, such that cx + d =
T (f(x))- f(x).	口
Lemma 2. The repaired DNN f returned by REASSURE is guaranteed to satisfy the specification
Φ on patch area A in single-region repair case.
Proof. By the definition ofpA, we have f(x) + pA(x) ∈ Φout for all x ∈ A.
For any x ∈ A, we have gA(x, γ) = 1 and hA(x, γ) = σ(pA(x)) - σ(-pA(x)) = pA(x). There-
fore,
f(x) = f(x) + hA(x, γ) = f(x) + pA(x) ∈ Φout	(13)
Thus, the patched neural network f meets the specification Φ on A.	口
Theorem 2 (Completeness). REASSURE can always find a solution to the minimal point-wise repair
or the minimal area repair problem.
Proof. For every patch area A, we can always find a support network gA . For any Φout and A,
there exists an affine function pA such that pA(x) ∈ Φout , ∀x ∈ A. Therefore, the LP (6) is always
feasible and REASSURE can find an affine patch function pA.
Once we have gA and pA for patch area A, REASSURE returns a patch network either by Equation
□
(7) or by Equation (8).
Theorem 3 (Limited Side Effect). Given a correctness property Φ
(Φin, Φout), a patch region
A and the corresponding patch network h(x, γ), there exists a positive number Γ such that for any
γ ≥ Γ, we have
Ir	ι∙	∙ rɔ ∙ r rɔ A CK , ι	∖ r / ∖
1.	for any linear region B, if B ∩ A = 0, then f(x, γ) = f (x);
` r	ι ∙	∙ C ι ∙	∙ ι ι	A / CK ∖ ∙rrl I w ,t 2 /	∖ I W
2.	for any linear region C who is a neighborofA(C∩A 6= 0), if f|C |= Φ, then fC (x, γ) |= Φ.
Proof. Since a multi-region repair is a composition of multiple singe-region repairs according to
Equation (8), we can prove the limited side effect of a multi-region repair by proving the limited
side effect of its constituent singe-region repairs. Below, we prove the limited side effect of a singe-
region repair.
Consider patch area A = {x | aix ≤ bi, i ∈ I} and A>0(γ) = {x | h(x, γ) > 0}.
1.	Since the number of neighbors for A are finite, we can take a big enough γ, such that for any B,
if B ∩A = 0, B ∩ A>o(γ) = 0. Thus, We have f(x, Y) = f(x) on B.
2.	For any linear region C who is a neighbor of A, i.e. C 6= A and C ∩ A 6= 0, f is no longer a
linear function on C, since there are some hyperplanes introduced by our repair that will divide C
into multiple linear regions.
Specifically, those hyperplanes are {x | γ(aix - bi) + 1 = 0} for i ∈ I, {x | Pi∈I g(aix - bi, γ) -
|I| + 1 = 0}, {x | p(χ) + K ∙ gA(x,γ) 一 K = 0} and {x | 一 p(χ) + K ∙ g∕(χ,γ) - K = θ}.
For any point x in those hyperplanes, it will fall into one of the following four cases.
4 Note that for neural networks that are trained by a stochastic method, with probability one f1 is full rank.
13
Published as a conference paper at ICLR 2022
(a)	x ∈ {x | γ(aix - bi) + 1 = 0} for some i ∈ I, then gA(x, γ) = 0, h(x, γ) = 0 and
f(x) ∈ Φout;
(b)	x ∈ {x | Pi∈I g(aix - bi, γ) - |I| + 1 = 0}, then gA(x,γ) = 0, h(x,γ) = 0 and
f(x) ∈ Φout;
(C) X ∈	{x	| p(x)	+ K ∙	g∕(x,γ) - K =	0},	then	p(x) = K - K ∙	g∕(x,γ)	≥ 0,
-P(X) + K ∙ gA(x, Y)- K ≤ 0, h(x, γ)=0 and f (x) ∈ Φout;
(d) X ∈{x | - P(X) + K ∙ g∕(x,γ) - K}, then P(X) = K ∙ g∕(x,γ) - K ≤ 0, P(X) + K •
gA(X,γ) - K ≤ 0, h(X, γ) = 0 andf(X) ∈ Φout;
By the above analysis, we have f(X) ∈ Φout for the boundary of the new linear regions. Since f is
linear on the new linear regions and Φ°ut is convex, f (x) ∈ Φ0ut for any X ∈ C.	口
Remark: By Theorem 3, we have that a patch would not change a correct linear region to an
incorrect one.
Corollary 1 (Incremental Repair). For multiple-region repair, the patch for a new region A0 would
not cause a previous patched region A to become incorrect.
Proof. After applying the patch to linear region A, we have that the resulting network is correct on
A. When applying a new patch to another linear region A0 , by Theorem 3, the new patch would not
make a correct linear region A incorrect.	口
Theorem 1 (Soundness). The repaired DNN f returned by REASSURE is guaranteed to satisfy the
specification Φ.
Proof. The proof has two parts:
1.	to show that f satisfies the specification Φ on A, and
2.	to show that f satisfies the specification Φ outside of A.
Part 1:
Lemma (2) shows f satisfy the specification Φ for single-region repair on A.
For the multi-region case, consider a set of buggy linear regions ∪1≤l≤I Al with the corresponding
support neural network gAl and affine patch function PAl for each Al . For the multi-region repair
construction in Equation (8), we refer to σ(PAj - PAj-1 + maxk≥j {gAk}Kj - Kj) as the j-th patch
and fbj = f + Pj0 ≤j σ(PAj0 - PAj0-1 + maxk≥j0 {gAk}Kj - Kj) as the network after the j-th
patch.
For any X in patch area ∪1≤l≤IAl, we can find a j such that X ∈ Aj but X ∈/ Ak for all k >
j. After the first j patches σ(PA1 (X) + maxk≥1{gAk(X, γ)}K1 - K1), σ(PA2 (X) - PA1 (X) +
maxk≥2{gAk (X, γ)}K2 - K2), ... , σ (PAj (X) - PAj-1 (X) + maxk≥j {gAk (X, γ)}Kj - Kj), the
DNN’s output at X becomes fj (X) = f(X) + PAj (X) which meets our specification Φ at X by the
definition of PAj (X).
Since X ∈/ Ak for all k > j, then by Corollary 1, the rest of the patches would not change a correct
area to an incorrect area. Therefore, we have the final patched neural network f meets specification
Φ on ∪1≤l≤IAl.
Part 2:
To show that f satisfies Φ outside of A.
14
Published as a conference paper at ICLR 2022
For any x outside the patch area ∪1≤l≤IAl, we have x lies on a correct linear region (linear region
that satisfies the specification Φ). By Theorem 3, we have either f(x) = f(x) or f(x) ∈ Φout.
Therefore, f satisfies Φ outside of A.	□
Theorem 4 (Minimum Repair). For any ReLU DNN f, which is linear on a patch region A and
satisfies the specification Φ, there exists a positive number Γ, such that for all γ ≥ Γ,
max |f (x) - f(x)l≥ max ∣h∕(x,Y)].	(14)
x∈X	x∈X
Proof. We consider the general case where the linear patch function is obtained from Equation (3).
For any DNN f, which is linear on patch region A and satisfies the specification Φ, We have
maXχ∈A |f — f | ≥ maXχ∈A |cx + d| = maXχ∈A ∣h∕(.,γ)| on patch area A by Equation (3).
Therefore, we only need to show:
max ∣hA(.,γ)l ≤ max ∣h∕(., γ)∣	(15)
Since parameter γ controls the slope of hA(., γ) outside of patch area A, a large γ means that
hA(., γ) will drop to zero quickly outside of A. Therefore, we can choose a large enough Γ such
that hA(., γ) drops to zero faster than the change of linear patch function cx + d.
Therefore, we have that for any γ ≥ Γ,
max |hA(., γ)∣ ≤ max ∣h∙A(∙,γ)∣ = max ∣h∙A(∙,γ)∣
x/A	χ∈A	χ∈X
.-T . .	. -T ..
≤ max If - f | ≤ max If - f |	(16)
x∈A	x∈X
□
Theorem 5 (Polynomial-Time Efficiency). REASSURE terminates in polynomial-time in the size of
the neural network and the number of buggy linear regions when Φout takes the form of {y I ql ≤
Py ≤ qu}, where P is a full row rank matrix and -∞ ≤ ql [i] ≤ qu [i] ≤ +∞ (ql [i] and qu [i] are
the i-th elements of ql and qu respectively).
Proof. We consider the affine patch function solved via Equation (10). Suppose A = {x ∈ XIaix ≤
bi, i ∈ I}. For the LPs in Equation (11), III is the number of constraints and it is polynomial in the
size of the neural network. Thus, REASSURE runs in polynomial time in the size of the neural
network.
In addition, since REASSURE computes the support network gA and affine patch function pA for
each A one by one (see Algorithm 1), the time complexity of REASSURE is linear in the number of
buggy linear regions.
□
7.4 Additional Experiment Details
Details on Feature-Space Repairs:
For an R-layer DNN f, we split f into two submodels f1 and f2 according to a hidden layer, say
the jth hidden layer, where f1 is the first j layers function, f2 is the last R - j layers function and
f = f2 ◦ f1. We note the output space of f1 the feature space. And for any buggy input xe, we note
f1 (xe) the buggy feature.
Repairing in a feature space is to repair the behavior of f2 on buggy features {f1(xe1), . . . , f1(xeL)}.
Note this will automatically repair the behavior of f on buggy points {xe1, . . . , xeL}.
Repairing in a feature space has the benefit of making the repair process more computation-friendly
and reducing the parameter overhead of the additional networks, and has the potential to generalize
the repair to undetected buggy inputs with similar features. However, it loses the locality guarantee
in the input space (but still preserves locality in the feature space).
15
Published as a conference paper at ICLR 2022
#P	ND(L∞)	ND(L2)	REASSURE NDP(L∞)	NDP(L2)	Acc	ND(L∞)	ND(L2)	MDNN NDP(L∞)	NDP(L2)	Acc
1	0.0%	0.0%	9.0%	9.1%	96.8%	8.9%	8.9%	7.1%	6.2%	87.5%
5	0.0%	0.0%	12.7%	12.8%	96.8%	48.1%	48.2%	44.3%	39.9%	57.1%
25	0.0%	0.0%	29.9%	29.7%	96.8%	90.4%	90.6%	63.7%	57.8%	6.7%
50	0.0%	0.0%	42.9%	42.6%	96.8%	92.5%	92.8%	82.1%	72.6%	4.8%
100	0.0%	0.0%	46.2%	45.9%	96.8%	95.5%	95.7%	90.9%	76.8%	5.1%
Table 5: Watermark Removal. The test accuracy of the original DNN is 96.8%. #P: number of buggy
points to repair; ND(L∞), ND(L2): average (L∞, L2) norm difference on both training data and
testing data; NDP(L∞), NDP(L2): average (L∞, L2) norm difference on random sampled points
near watermark images; Acc: accuracy on test data.
Point-wise Repair on ImageNet (Feature-Space Repairs)
We use AlexNet Krizhevsky et al. (2012) on ImageNet dataset Russakovsky et al. (2015) as the
target DNN. The size of image is (224, 224, 3) and the total number of classes for ImageNet is 1000.
We slightly modified AlexNet: we only consider 10 output classes that our buggy images may lie
on and use a multilayer perceptron with three hidden layers (512, 256, 256 nodes respectively) to
mimic the last two layers of AlexNet.
The goal of the repair is to fix the behaviors of the target DNN on buggy inputs, which are found
on ImageNet-A Hendrycks et al. (2021). For REASSURE, we construct the patch network starting
from the third from the last hidden layer (i.e. repair in a feature space).
The results are shown in Table 4. REASSURE, PRDNN and Fine-Tuning repair all the buggy points
while Retrain only repair 96% buggy points in 100 epochs. REASSURE achieves almost zero modifi-
cation on validation images to the original DNN. In addition, REASSURE preserves the performance
of the original DNN.
Watermark Removal
We compare REASSURE with MDNN on the watermark removal experiment from their paper. We
were not able to run the code provided in the MDNN Github repository, but we were able to run on
the target DNN models, watermark images, and MDNN-repaired models in the same repository.
The target DNN is from Goldberger et al. (2020), which is watermarked by the method proposed in
Adi et al. (2018) on a set of randomly chosen images xi with label f (xi).
The goal is to change the DNN’s predictions on all watermarks xi to any other label y 6= f(xi) while
preserving the DNN’s performance on the MNIST test data. For REASSURE, we set the prediction
y = f(xi) - 1 if f(xi) > 1, and y = 10 otherwise.
The results are shown in Table 5. Both REASSURE and MDNN remove all the watermarks. How-
ever, MDNN introduces significant distortion to the target DNN and as a result the test accuracy
drops rapidly as the number of repair points increases. In comparison, REASSURE removes all the
watermarks with no harm to test accuracy.
Area Repair: HCAS
Table 6 is the comparison with PRDNN using the last layer as the repair layer. All other settings are
the same as those in Section 5.2.
Experiment Platform
All experiments were run on an Intel Core i5 @ 3.4 GHz with 32 GB of memory. We use Gurobi
Gurobi Optimization, LLC (2021) to solve the linear programs.
Size of Neural Networks:
Point-wise Repairs on MNIST: The DNN model is a multilayer perceptron with ReLU activation
functions. It has an input layer with 784 nodes, 2 hidden layers with 256 nodes in each layer, and a
final output layer with 10 nodes.
Watermark Removal on MNIST: The DNN model has an input layer with 784 nodes, a single hidden
layer with 150 nodes, and a final output layer with 10 nodes.
16
Published as a conference paper at ICLR 2022
REASSURE	PRDNN (Last Layer)
#A	ND(L∞)	NDP(L∞)	NSE	Acc	T	ND(L∞)	NDP(L∞)	NSE	Acc	T
10	2.662e-03%	1.318e-03%	0%	98.1%	1.0422	0.30%	20.5%	16%	71.4%	2.90+0.100
20	2.918e-03%	2.2%	0%	98.1%	1.1856	0.31%	46.7%	66%	70.5%	5.81+0.169
50	8.289e-03%	17.6%	0%	98.1%	1.8174	0.31%	46.7%	66%	70.5%	14.54+0.353
87	0.04%	45.9%	0%	97.8%	2.4571	0.31%	46.7%	66%	70.5%	25.30+0.467
Table 6: Area Repairs on HCAS. We use the the last hidden layer as the repair layer for PRDNN.
The test accuracy of the original DNN is 97.9%. #A: number of buggy linear regions to repair;
ND(L∞): average L∞ norm difference on training data ; NDP(L∞): average L∞ norm difference
on random sampled data on input constraints of Specification 1; NSE: % of correct linear regions that
is repaired to incorrect; Acc: accuracy on training data (no testing data available); T: running time
in seconds. For PRDNN, the first running time is for enumerating all the vertices of the polytopes
and the second is for solving the LP problem in PRDNN.
Area Repair on HCAS: The DNN model has an input layer with 3 nodes, 5 hidden layers with 25
nodes in each hidden layer, and a final output layer with 5 nodes. DNN outputs one of five possible
control advisories (’wrong left’, ’weak left’, ’Clear-of-Conflict’, ’weak right’ and ’wrong right’).
Point-wise Repairs on ImageNet: modified AlexNet Krizhevsky et al. (2012) has 650k neurons,
consists of five convolutional layers, some of which are followed by max-pooling layers, and five
fully-connected layers.
Hyperparameters used in Repair:
We set γ = 0.5 for Point-wise Repair on MNIST, γ = 0.02 for Watermark Removal, γ = 1 for Area
Repair: HCAS and γ = 0.0005 for Point-wise Repair on ImageNet.
We set learning rate to 10-3 for Retrain in the point-wise repair experiment.
We set learning rate to 10-2 and momentum to 0.9 for Fine-Tuning in the point-wise repair experi-
ment.
PRDNN requires specifying a layer for weight modification. We use the first hidden layer as the
repair layer, which has the best performance in our experiment settings, unless otherwise specified.
Specifications in HCAS:
Specification 1. If the intruder is near and approaching from the left, the network advises “strong
right.”
Input constraints: Φin = {(x,y,ψ)∣10 ≤ X ≤ 5000,10 ≤ y ≤ 5000, —π ≤ ψ ≤ —1∕2π}. Output
constraint: f(x, y, ψ)4 ≥ f(x, y, ψ)i for i = 0, 1, 2, 3.
We calculate all the linear regions for N1,4 in the area Φin of Specification 1 and totally 165 linear
regions are found, including 87 buggy linear regions (DNN did not meet the specification) and 78
correct linear regions (DNN meet the specification).
Specification 2. If the intruder is near and approaching from the right, the network advises “strong
left.”
Input constraints: Φin = {(x,y,ψ)∣10 ≤ X ≤ 5000, —5000 ≤ y ≤ —10,1∕2π ≤ ψ ≤ π}. Output
constraint: f(x, y, ψ)0 ≥ f(x, y, ψ)i for i = 1, 2, 3, 4.
Also we calculate all the linear regions in the area Φin of Specification 2. And 79 correct linear
regions are found. We will test if a repair will make those correct linear regions incorrect.
Point-wise Repairs vs. Area Repairs:
REASSURE automatically performs area repair on the point-wise repair experiments. This means
our area repair method scales well to high-dimensional polytopes (the input dimension of MNIST is
784) whereas PRDNN does not scale beyond 2D linear regions/polytopes.
Parameter Overhead for REASSURE:
REASSURE introduces an additional network, patch network, and as a result adds new parameters to
the original network. The number of new parameters depends on |I |, which is the number of linear
17
Published as a conference paper at ICLR 2022
constraints for the H-representation of A. We can remove redundant constraints in this representa-
tion in polynomial time to make the additional network smaller. For the area repair experiment on
HCAS, the average number of constraints for one linear region is 3.28 and the average number of
new parameters that REASSURE introduces is 66. As a comparison, the number of parameters in
the original network is around 3000 and PRDNN doubles the number of parameters (as a result of
the Decoupled DNN construction) regardless of the number of point-wise or area repairs.
7.5 Applying REASSURE To General CPWL Networks
Recall the result that an Rm → R function is representable by a ReLU DNN if and only if it is
a continuous piecewise linear (CPWL) function Arora et al. (2016). We use convolutional neural
networks as an example to show how REASSURE can be applied to more general CPWL networks.
Convolutional neural networks (CNNs) are neural networks with convolution layers and maxpooling
layers. For simplicity, we assume the CNNs also use ReLU activation functions (but in general other
CPWL activation functions will also work). The convolutional layers can be viewed as special linear
layers. The maxpooling layers can be converted to linear operations with ReLU activation functions
as follows.
max(x1, x2, ..., xn) = max(x1, max(x2, x3, ..., xn))
max(xi , xj) = max(xi - xj , 0) + xj = σ(xi - xj) + xj
where σ is the ReLU activation function. Thus, REASSURE can be used to repair CNNs as well.
18