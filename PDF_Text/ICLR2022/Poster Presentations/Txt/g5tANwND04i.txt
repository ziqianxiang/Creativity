Published as a conference paper at ICLR 2022
On the Convergence of mSGD and AdaGrad
for Stochastic Optimization
Ruinan Jin
LSC, NCMIS, Academy of Mathematics and Systems Science
Chinese Academy of Sciences, Beijing 100190, China
School of Mathematical Sciences
University of Chinese Academy of Sciences, Beijing 100049, China
Yu Xing
Division of Decision and Control Systems
KTH Royal Institute of Technology
SE-100 44 Stockholm, Sweden
Xingkang He *
Department of Electrical Engineering
University of Notre Dame, IN, USA
Ab stract
As one of the most fundamental stochastic optimization algorithms, stochastic
gradient descent (SGD) has been intensively developed and extensively applied in
machine learning in the past decade. There have been some modified SGD-type
algorithms, which outperform the SGD in many competitions and applications in
terms of convergence rate and accuracy, such as momentum-based SGD (mSGD)
and adaptive gradient algorithm (AdaGrad). Despite these empirical successes,
the theoretical properties of these algorithms have not been well established due
to technical difficulties. With this motivation, we focus on convergence analysis
of mSGD and AdaGrad for any smooth (possibly non-convex) loss functions in
stochastic optimization. First, we prove that the iterates of mSGD are asymp-
totically convergent to a connected set of stationary points with probability one,
which is more general than existing works on subsequence convergence or con-
vergence of time averages. Moreover, we prove that the loss function of mSGD
decays at a certain rate faster than that of SGD. In addition, we prove the iterates
of AdaGrad are asymptotically convergent to a connected set of stationary points
with probability one. Also, this result extends the results from the literature on
subsequence convergence and the convergence of time averages. Despite the gen-
erality of the above convergence results, we have relaxed some assumptions of
gradient noises, convexity of loss functions, as well as boundedness of iterates.
1 Introduction
In recent years, the rapid development of machine learning has stimulated a lot of applications of op-
timization algorithms to employing tremendous data in practical scenarios. Many optimization algo-
rithms of machine learning are based on gradient descent (GD). A typical GD algorithm, minimizing
a loss function g(θ) ∈ R via seeking an N-dimensional real-valued parameter θ* = arg min g(θ),
θ∈RN
writes as follows
θn +1 = θn 一 εnVjng(θn),	(1)
where θn is the estimate of θ* at step n, εn is a positive step size (learning rate) to be designed,
and Vθng(θn) stands for the gradient of g(θn) at step n. When certain conditions are satisfied, θn in
*Corresponding author: xhe9@nd.edu
1
Published as a conference paper at ICLR 2022
equation 1 will converge to the optimal solution. However, since the computation of Vθng(θn) relies
on all data at each step, it is inefficient to apply GD-based algorithms (e.g., equation 1) when the
data amount is very large. Therefore, more and more attention has been given on how to accelerate
the GD-based algorithms. One of the attempts is stochastic gradient descent (SGD) which originated
from Robbins & Monro (1951). Instead of calculating Vθng(θn) over all data, SGD algorithms use
a relatively small proportion of data to estimate the gradient, i.e., Vθng(θn, ξn) where ξn is a random
vector introduced to choose a subset of data for each update. Then the SGD modified from GD
equation 1 is as follows
θn+1 = θn-εnVθng(θn,ξn).	(2)
Besides reducing gradient computation, g(θn, ξn) can be used as an estimate of the gradient for the
scenarios where the accurate gradient is unavailable due to external noises.
In recent years, SGD has shown its prominent advantages in dealing with high dimensional optimiza-
tion problems such as regularized empirical risk minimization and training deep neural networks
(see, e.g. Graves et al. (2013); Nguyen et al. (2018); Hinton & Salakhutdinov (2006); Krizhevsky
et al. (2012) and references therein). In Nguyen et al. (2018); Bottou (2012), the convergence and
the convergence rate of SGD have been analyzed.
Despite outstanding successes of SGD, it usually has a relatively slow convergence rate induced by
random gradients and it provides fluctuating iterates in the learning process. In order to improve
the theoretical and empirical performance of SGD, there have been a number of investigations, in-
cluding three typical algorithms for accelerating the convergence rate: (1) The momentum-based
stochastic gradient descend (mSGD), which reduces the update variance by averaging the past gra-
dients (Polyak, 1964); (2) The adaptive gradient algorithm (AdaGrad), which replaces the step size
of SGD with adaptive step size (Duchi et al., 2011); (3) Adaptive momentum gradient algorithm
(Adam), which integrates mSGD and AdaGrad (Kingma & Ba, 2015). However, the convergence
results of these algorithms have not been well established, especially for non-convex loss functions.
In this paper, we focus on the investigation of mSGD and AdaGrad. We believe that the convergence
analysis results in this paper are helpful for the future research on the convergence or generalization
of Adam and other stochastic optimization algorithms.
The technique of mSGD is originally developed by Polyak (Polyak, 1964) for the acceleration of
convergence rate of gradient-based methods. A typical expression of mSGD is as follows (Zareba
et al., 2015; Sutskever et al., 2013)
vn = α vn-1 + εn Vθn g(θn, ξn),	θn+1 = θn - vn,	(3)
where α ∈ [0, 1) and εn > 0 are relatively momentum coefficient and step size (learning rate), respec-
tively. Another type of mSGD with the name of stochastic heavy ball (SHB) has also been studied
(Gitman et al., 2019; Gupal & Bazhenov, 1972),
vn = βnvn-1 + (1 - βn )Vθn g(θn , ξn ),	θn+1 = θn - γnvn ,	(4)
where βn ∈ (0, 1) and γn are relatively momentum coefficient and step size (learning rate). In recent
years, mSGD has been widely employed in the applications of deep learning such as image classifi-
cation (Krizhevsky et al., 2012), fault diagnosis (Tang et al., 2018), statistical image reconstruction
(Kim et al., 2014), etc. Moreover, a number of variants on momentum are emerging, see, e.g.,
synthesized Nesterov variants (SNV) (Williams & Lovett, 2016), robust momentum (Cyrus et al.,
2018), and PID-control based methods (An et al., 2018). The importance of momentum in deep
learning has been illustrated in Sutskever et al. (2013) through experiments. However, the theoreti-
cal analysis for convergence and convergence rate of mSGD needs further investigation, especially
for non-convex loss functions which are common in deep learning. Most existing results are estab-
lished with guaranteed subsequence convergence or convergence of time averages1 (see Yang et al.
(2016); Gitman et al. (2019); Polyak (1977); Kaniovskii (1983), Li & Orabona (2020) and refer-
ences therein). Nevertheless, there is a still distance to asymptotic convergence, which is usually
more general and useful in practical applications requiring stable iterates for each realization. Seb-
bouh et al. (2020) studied the asymptotic convergence of mSGD with time-varying parameter αn ,
which however is less common than the static α in practical applications.
Convergence analysis for SGD unveils that the convergence rate heavily depends on the choice of
step size εn (Robbins & Monro, 1951; Nguyen et al., 2018), which may consume a huge amount
1The convergence definitions are given in Section 3.
2
Published as a conference paper at ICLR 2022
of efforts on fine tune. In order to deal with this problem, the algorithm AdaGrad with adaptive
step size is proposed in Duchi et al. (2010) and McMahan & Streeter (2010) concurrently. In the
literature, there are two main forms of AdaGrad. One is based on the norm of gradients as follows
(Streeter & Mcmahan, 2010)
Sn = Sn-1 + IRθng(θn, ξn"R θn+1 = θn------ʒ== RSng(θn, ξn),	(5)
Sn
where α0 > 0 is a constant. The other form is based on the coordinate-wise gradients (Duchi et al.,
2010; Li & Orabona, 2019; 2020)
Qn = Qn-1 + Vθng(θn, ξn)TR“虱 θn, ξn),	θn +1 = θn - W0Q-1R“g(θn, ξn),
where α0 is a constant. In our paper We focus on the norm form (equation 5). In recent years,
AdaGrad has shown its effectiveness in the field of sparse optimization (Duchi et al., 2013), tensor
factorization (Lacroix et al., 2018), and deep learning (Heaton & Jeff, 2018). Some algorithm vari-
ants like RMSProp (Tieleman & Hinton, 2012) and SAdaGrad (Chen et al., 2018) are also studied.
However, there are few results on the convergence of AdaGrad. Most of these results only prove
subsequence convergence or convergence of time averages (see, e.g. Zou et al. (2019); Chen et al.
(2019); Defossez et al. (2020); Ward et al. (2019)). Although Li & Orabona (2019) and Gadat &
Gavra (2020) studied the asymptotic convergence ofa modified AdaGrad algorithm, the result is not
applicable to AdaGrad in equation 5. Thus, the asymptotic convergence of AdaGrad in equation 5
is still open.
In this theoretical paper, we aim to establish the convergence of mSGD and AdaGrad under mild
conditions. The main contributions of this paper are three-fold:
•	We prove that the iterates of mSGD are asymptotically convergent to a connected set of
stationary points for possibly non-convex loss function almost surely (i.e., with probability
one), which is more general than existing works on subsequence convergence.
•	We quantify the convergence rate of mSGD for the loss functions. Through this conver-
gence rate we can geta theoretical explanation of why mSGD can be seen as an acceleration
of SGD. Moreover, we provide the convergence rate of mean-square gradients and connect
it to the convergence of time averages.
•	We prove the iterates of AdaGrad are asymptotically convergent to a connected set of sta-
tionary points almost surely for possible non-convex loss functions. The convergence result
for the AdaGrad extends the subsequence convergence in the literature.
The remainder of the paper is organized as follows. In Section 2, we introduce the related work
considering the convergence of mSGD and AdaGrad. The main results of the paper are given in
Section 3, where we study the convergence and convergence rate of mSGD as well as the conver-
gence of AdaGrad. Section 4 concludes the whole paper. Sections 5 and 6 are Code of Ethics and
Reproducibility, respectively. The proofs are given in Appendix.
2	Related work
Convergence of mSGD: For the normalized mSGD (SHB), Polyak (Polyak, 1977; 1964) and Kan-
iovski (Kaniovskii, 1983) studied its convergence (subsequence convergence and convergence of
time averages) properties for convex loss functions. Igor Gitman (Gitman et al., 2019) provided
some convergence results of mSGD (SHB) for non-convex loss functions, but there is a considerable
distance to the asymptotic convergence. Moreover, there is a requirement for uniform boundedness
of a noise term in Gitman et al. (2019), i.e., E(kRθng(θn, ξn) - Rθng(θnk) ≤ δ, which confines the
application scope of mSGD (SHB). In addition, the designs of momentum coefficients in Polyak
(1977; 1964); Kaniovskii (1983); Gitman et al. (2019) are not consistent with some practical appli-
cations (Smith et al., 2018; Sutskever et al., 2013). Therefore, the asymptotic convergence of mSGD
for convex and non-convex loss functions needs further investigation.
Convergence rate of mSGD: Despite outstanding empirical successes of mSGD, there are few re-
sults on convergence rate of mSGD. In these results, Mai & Johansson (2020) studied a class of
convex loss functions, and obtained a convergence rate of time averages without reflecting the role
3
Published as a conference paper at ICLR 2022
of momentum parameter. Gitman et al. (2019) and Nicolas Loizou & Richtarik (2020) respectively
investigated the asymptotic convergence rate of mSGD (SHB) by restricting to quadratic loss func-
tions. Liu et al. (2020) studied the properties of SHB, where the relation between the loss function of
SHB and the step size in every step was studied under the setting that αn and βn are constants. The
convergence rate of time averages was also studied. However, since the momentum parameters are
not consistent with some applications (Smith et al., 2018; Sutskever et al., 2013) and the standard
mSGD in equation 3 is not covered, further studies are needed.
Convergence of AdaGrad: In the original work for AdaGrad (Duchi et al., 2011), it was proved
that AdaGrad can converge faster in the time averages sense if gradients are sparse and the loss
function is convex. Similar results were established by Chen et al. (2019), and Ward et al. (2019).
Zou et al. (2019) and Defossez et al. (2020) established convergence results in the subsequence
sense. Asymptotic convergence was obtained in Li & Orabona (2019) for non-convex functions,
but the form of the algorithm is no longer standard, as discussed in Duchi et al. (2011). Although
such a change alleviates the difficulty in the proof of asymptotic convergence, it cannot be applied
to the study of the AdaGrad in equation 5. Moreover, they required that a noise term is of point-wise
boundedness (i.e., ∣∣Vθng(θn, ξn) - Vjng(θn)∣∣ ≤ δ, where δ is a positive constant), which however
is relatively restrictive.
3	Main results
In this section, we provide the main results of this paper, including the analysis of convergence and
convergence rate of mSGD in equation 2 and the analysis of convergence of AdaGrad in equation 5.
In the following, RN denotes the N-dimensional Euclidean space and k ∙ k stands for the 2-norm, i.e.,
the Euclidean norm. To proceed, we need some definitions, consisting of asymptotic convergence,
subsequence convergence, mean-square convergence, and convergence of time averages.
Definition 1 (Asymptotic convergence) A sequence {xn} is asymptotically convergent to a set K, if
limn→+∞ infx∈K xn -xk = 0.
Definition 2 (Subsequence convergence) A sequence {xn} converges in subsequence to a set K, if
there exists at least one subsequence {xkn} of {xn} such that limn→+∞ infx∈K xkn -xk = 0.
Definition 3 (Mean-square convergence) A stochastic sequence {xn } converges in mean square to
a fixed vector x, if limn→+∞ E(kxn -xk2) = 0.
Definition 4 (Convergence of time averages) A stochastic sequence {xn } converges in time aver-
ages to a fixed vectorx, if lim T →+∞ 1 ∑ T=I E(k Xn — x∣∣2) = 0.
It is obvious that asymptotic convergence implies subsequence convergence, and that mean-square
convergence ensures convergence of time averages, but not vice versa.
3.1	Convergence of mSGD
In this subsection, with the help of some stochastic approximation techniques (Chen, 2006), we
aim to prove that θn in equation 3 is asymptotically convergent to a connected component J* of
the set J := {θ|kVjg(θ)k = 0} almost surely (a.s.) under proper conditions. When this connected
component degenerates to a stationary point θ*, it holds that θn → θ*, a.s..
In contrast to the existing works of subsequence convergence (cf. Zou et al. (2019); Chen et al.
(2019); DefOSSez et al. (2020); Ward et al. (2019)), We aim to prove that θn of mSGD in equation 3
is able to achieve asymptotic convergence. Since mSGD in equation 3 is a stochastic algorithm, we
aim to establish its a.s. asymptotic convergence. To proceed, we need some reasonable assumptions
with respect to noise sequence {ξn} and loss function g(θ).
Assumption 1 Noise sequence {ξn} are mutually independent and independent of θ1 and v0, such
that g(x) = Eξn g(x, ξn) for any x ∈ RN.
4
Published as a conference paper at ICLR 2022
Assumption 2 (Loss function assumption) Loss function g(θ) satisfies the following conditions:
1)	g(θ) is a non-negative and continuously differentiable function.
2)	ThesetofstationaryPointsof ∣∣Vθg(θ)k isnotanempty set, that is
J = {θ l∣Vθg(θ)k= 0} = 0.
3)	Vθg(θ) satisfies the LiPschitz condition, i.e., there is a scalar c > 0, such thatfor any x, y ∈ RN
Vx g(x) - Vy g(y) ≤c∣x-y∣.
4)	There is a scalar M > 0 such that for any θ ∈ RN and Positive integer n,
Eξn (∣∣Vθg(θ)-Vθg(θ,ξn)『)≤M(1 + g(θ)).	(6)
Assumption 1 and conditions 1)-3) of Assumption 2 are common in the literature (Gitman et al.,
2019). Assumption 2 does not pose any requirement on the convexity of g(θ). In other words,
we allow any convex or non-convex loss functions g(θ) as long as they satisfy this assumption.
Condition 4) corresponds to the condition in Shalev-Shwartz et al. (2011); Nemirovski et al. (2009);
Hazan & Kale (2014); Gitman et al. (2019); Yang et al. (2016); Polyak (1977); Kaniovskii (1983)
where the following inequality is assumed to hold for any θ ∈ RN and positive integer n,
Eξn∣∣Vθg(θ)-Vθg(θ,ξn)∣∣2 ≤K, ∀θ∈RN,	(7)
where K is a positive scalar. Note that equation 6 reduces to equation 7 if g(θ) is uniformly upper
bounded over the space RN . Since equation 6 does not need this uniform boundedness, it substan-
tially extends equation 7 such that mSGD is also applicable to the scenarios with unbounded loss
functions.
Different from deterministic GD-type algorithms with a constant step size, in order to ensure the
convergence of mSGD, we need a decreasing step size for counteracting the randomness induced
by noise {ξn} (Gitman et al., 2019; Robbins & Monro, 1951). Specifically, we make the following
assumption on step size εn together with a fixed momentum coefficient α.
Assumption 3 Momentum coefficient α ∈ [0, 1) and the sequence of steP size εn is Positive, mono-
tonically decreasing to zero, such that ∑n+=∞1 εn = +∞and ∑n+=∞1 εn2 <+∞.
The setting of εn in Assumption 3 is consistent with stochastic approximation for root seeking of
functions (Chen, 2006) as well as some stochastic optimization algorithms (Gupal & Bazhenov,
1972; Polyak, 1977; Kaniovskii, 1983; Gitman et al., 2019). An explicit example of step size εn
satisfying Assumption 3 is εn = 1/n. Although SHB in equation 4 shares a similar expression as
mSGD, the provided conditions of step sizes in Gupal & Bazhenov (1972); Polyak (1977); Kan-
iovskii (1983); Gitman et al. (2019) are not applicable to the general cases of mSGD with static α
(Smith et al., 2018; Sutskever et al., 2013).
In fact, SHB in equation 4 has a similar expression as mSGD in equation 3. Multiplying γn on both
sides of the first equation of equation 4 yields
γ
γnvn =	Bn(Yn—1 vn-I) + γn (I - Bn) Vθng( θn, ξn)
γn-1	n	(8)
θn+1 = θn - γnvn.
We treat γnvn as one term, as the role of vn in equation 3. By comparing coefficients of equation 8
and equation 3, we get α(n) = (γn/γn-1)βn and εn = γn(1 一βn). In the literature (Gupal & Bazhenov,
1972; Polyak, 1977; Kaniovskii, 1983; Gitman et al., 2019), the parameter setting of SHB in equa-
tion 4 has the following requirements
+∞	+∞
∑ γn = +∞,	∑ γn2 < +∞,
n=1	n=1
Bn < 1,	lim Bn = 0
or
+∞	+∞	+∞	γ2
Σ Yn = +∞,	Σ (I-Bn)2	< +∞,	Σ	Γ⅛	<	+∞,	JmCOβ	= 1.
n=1	n=1	n=1	1 - Bn	n→+∞
5
Published as a conference paper at ICLR 2022
According to the above requirements on βn and γn, the requirements on εn and α(n) are
+∞
∑ εn = +∞,
n=1
+∞
∑ εn2 < +∞,
n=1
lim α(n) = 0.
n→+∞
or
+∞	+∞
∑εn=∑γn(1-βn)≤
n=1	n=1
+∞
+∞
∑(1-βn)3∑
n=1
n=1
Y2
1 - βn
< +∞,
lim sup α (n) = 1.
n→+∞

We can see that the static momentum parameter α, which is widely used in practical applications,
does not satisfy these conditions. In Sebbouh et al. (2020), the authors studied an algorithm (SHB-
IMA) that has a similar form to mSGD given in equation 3, but their conditions for parameters
cannot cover the case with a static α.
Before providing the main theorem for convergence, we need a useful lemma In the analysis of
asymptotic convergence of mSGD, the following lemma plays an important role.
Lemma 1 Consider the mSGD in equation 3. If Assumptions 1-3 hold, then for ∀θι ∈ RN and
v0 ∈ RN, there is a scalar T(θ1,v0), such that E g(θn) < T(θ1,v0)for any n ≥ 1.
Lemma 1 actually guarantees stability of mSGD. Let Fn = σ(θ1,v0, {ξi}in=1) be the minimal σ-
algebra generated by θ1,v0, {ξi}in=1. As a result, θn is adapted to Fn-1. Then for any n ∈ N+, it
holds that
E (∣∣Vθng (θn ) - Vθng (θn, ξn )∣∣2 ) = E(E (∣∣Vθng (θn )— Vθng (θn, ξn )∣∣2∣Fn-l))
=E(Egn (∣∣Vθng(θn) - Vθng(θn,ξn)∣∣2)) ≤ M(1 + E (g(θn))) ≤ M(1 + T(θι,vo)), (9)
where the second equality follows from the independence between ξn and {θ1,v0, {ξi}in=-11} in As-
sumption 1, and the last two inequalities hold due to 4) in Assumption 2 and Lemma 1, respectively.
Intuitively, the result in equation 9 means that the fluctuation induced by random noise {ξi}in=-11
is well restrained. Note that the derived result in equation 9 is totally different from equation 7
required in the literature (Shalev-Shwartz et al., 2011; Nemirovski et al., 2009; Hazan & Kale,
2014; Gitman et al., 2019; Yang et al., 2016; Polyak, 1977; Kaniovskii, 1983), since equation 7
needs a uniformly upper bound K over the whole space (i.e., θ ∈ RN) which is difficult to satisfy
when loss function g(θ) is quadratic or cubic with respect to θ over unbounded parameter space.
In contrast, regardless of the order of g(θ) with respect to θ, equation 9 ensures boundedness of
E (∣∣ Vθng(θn) - Vθng(θn, ξn) ∣∣2) for learning any fixed true parameter θ*. This reflects a favorable
learning process against random noise ξn for dealing with general loss functions g(θ). This result
paves the way to the following theorem on asymptotic convergence of mSGD.
Theorem 1 Consider the mSGD in equation 3. If Assumptions 1-3 hold, then for ∀θ1 ∈ RN and
∀V0 ∈ RN, there exists a connected set J* ⊆ J such that the iterate θn is convergent to the set J*
almost surely, i.e.,
lim d(θn, J* ) = 0, a.s.
n→∞
where d(x, J*) = infy{kx-yk,y ∈ J*} denotes the distance between point x and set J*.
In Theorem 1, we prove that the iterates of mSGD asymptotically converge to a connected set of
stationary points almost surely. When this connected set degenerates to a stationary point θ*, it holds
that θn → θ*, a.s.. The result enables engineers for the design of proper momentum coefficients and
step sizes (like α = 0.9, εn = 0.1/n) in the related applications of mSGD with mathematic guarantee.
3.2	Convergence rate of mSGD
In this subsection, we analyze the convergence rate of mSGD. Before that, given positive real se-
quences {an} and {bn}, we let an = O(bn) if there is a constant c > 0, such that an/bn < c for any
n ≥ 1. For quantitative analysis, we need some extra assumptions.
6
Published as a conference paper at ICLR 2022
Assumption 4 (Loss function assumption) Loss function g(θ) satisfies the following conditions:
1)	g(θ) is a non-negative and continuously differentiable function. The set of its stationary points
J = {θ ∣k Vθ g (θ)k = 0} is a bounded set which has only finite connected components J1,…,Jn
In addition, there is ε0 > 0, such that for any i ∈ {1, 2, . . . ,n} and 0 < d(θ,Ji) < ε0, it holds that
Ig(θ) — gi I = 0, where gi = {g(θ)∣θ ∈ Ji} is a constant.
2)	For any i ∈ {1, 2, . . . , n}, it holds that
liminf ⅛≡
d(θ,Ji)→0 g(θ) — gi
≥ s > 0.
In many problems of machine learning, especially deep learning, because of strong non-linearity
mapping from input data to output data and the structure complexity of employed models, loss
functions could be non-convex and may have multiple local critical points. Assumption 4 does
not require convexity of the loss function, but guarantees the properties of the loss function around
critical points. Assumption 4 2) can be treated as a local version of Polyak-Lojasiewicz (P-L)
condition.
Theorem 2 Consider the mSGD in equation 3 with the noise following a uniform sampling distri-
bution. If Assumptions 1-4 hold, for ∀v0 ∈ RN and ∀θ1 ∈ RN, it holds that
E (kVθng (θn )k2)= O (e- ∑ n=1 p(1-α)2 ),	(10)
where p = exp ∑k∞=1 Mεk2 andM is defined in condition 4) of Assumption 2.
In Theorem 2, let α = 0, then we can obtain the convergence rate of SGD,
E QV.g (θn )k2) = O ( e - ∑ n=1 p εi).
According to the obtained bounds, the convergence rate of mSGD with α ∈ (0, 1) is larger than that
of SGD, which is the case with α = 0. Moreover, Theorem 2 provides a stronger characterization
of convergence rate than some existing works considering the convergence rate of time averages
T ∑T=1 E(kVθng(θn)k2) = O(Tl) (l = —1 in LiU et al. (2020) and l = —1/2 in Mai & Johansson
(2020)). In the following, we will elaborate on this point. From equation 10, there exists a scalar
t0 > 0 sUch that ∀n > 0
E QVng(θn )k2)
<< 10,
exp{ - ∑n=1 p(1-α)2 }
implying
T ΣT=1 E QVng (θn )k2)
<< 10.
T ∑n=1 exp { - ∑n=1 P(1sεiɑ)2 }
So it holds that
1 工	…	/1 T `	Sε \
T ∑ E QVng (θn )k2) = O- ∑ e ∑i=1 P (1-α)2 .
T n=1	T n=1
Let εn = n, then We know that ∑T=I εn = O(lnT). Hence,
1 T	yn	S εi
1 ∑ e ^ ∑i=1 PFO2
T n=1
o (T ∑1
n=1
Sln(n)
e	P (1 —0)2
o (T ∑1
n=1
______S
n P(I-O)2
_____S	-_______S +1∖
If P⅛ < 1, ∑T=1 n p(1-0)2 = OΓΓ P(I-O)2	Vsowe have
1 T	nn	sεi
1 ∑ e ^ ∑i=1 EF
T n=1
S
1	/ --
-O T P (1-α)2
=O(T - P(I-O)2 ).
7
Published as a conference paper at ICLR 2022
_____S
If P(1-晨)2 = 1, it holds that ∑T=ι n p(1-α)2 = O(lnT), and hence
1 T nn	sεi
∑e ~ ∑i=1 EF
n=1
T O(In T )=O( InT).
If p(i-α)2 > 1, then ∑n=ι n p(1-α)2 := C«(T) → Cα as T → +∞, where Cα is a positive constant, so
T Lfn=1 晨=竽=O( C).
n=1
Note that the coefficient Cα depends on S, p, and α, where S is the constant given in Assumption 4
only depending on g(θ ), and p is given in Theorem 2 and relies on M in 4) of Assumption 2 and
step size εn . The above observation indicates that setting α close to one makes mSGD achieve con-
vergence rate of time averages of order O(1/T). As remarked previously, larger α implies smaller
coefficient Cα and thus quicker convergence rate. Interestingly, since Cα(T) is monotonically in-
creasing, the convergence rate has a relatively small coefficient when T is small. This could illustrate
why mSGD can achieve better performance in the early phase of iteration.
3.3 Convergence of AdaGrad
In this subsection, we aim to establish the convergence of AdaGrad. Compared to the study of
mSGD, the design of adaptive step size increases the technical difficulties. To proceed, the required
conditions on loss function g(θ) are summarized as follows.
Assumption 5 LoSS function g(θ) in equation 5 SatiSfieS the following conditionS:
1)	g(θ) iS a non-negative and continuouSly differentiable function. The Set of itS Stationary pointS
J = {θ ∣k Vθ g (θ)k = 0} is a bounded Set which has only finite connected components J1,…,Jn.
In addition, there iS ε1 > 0, Such that for any i and0 < d(θ,Ji) < ε1, it holdS that g(θ) -gi 6= 0,
where gi = {g(θ)∣θ ∈ Ji} is a ConStant
2)	The gradient Vjg(θ) satisfies the Lipschitz condition, i.e.,for any x,y ∈ RN,
IlVxg(X) - Vyg(y)∣∣ ≤ Ckx-yk.
3)	There are two constants M0 > 0 and a > 0 such that for any θ ∈ RN and n ∈ N+,
Eξn∣∣Vθg(θ,ξn)∣∣2) ≤ M0∣∣Vθ g(θ)∣∣2 + a.	(11)
Condition 2) is the same as in 2) of Assumption 2. Condition 1) is relatively weak, because it does
not require any convexity of the loss function or global conditions as P-L condition. There are many
functions satisfying Assumption 5 1) but not convex, such as y = sin2 (x), y = (x - 1)(x - 2)(x -
3)(x - 4), and y = cos2 (x). Similar to condition 4) of Assumption 2, condition 3) is a condition
to restrain the noise influence. Equation 11 is milder than kVθ g(θ, ξn) - Vθ g(θ)k ≤ S a.s. (Li
& Orabona, 2019; Defossez et al., 2020), which is relatively restrictive in dealing with unbounded
noises, e.g., Vθg(θ, ξn) = Vθg(θ) + ξn, where ξn is independent identically distributed and Gaus-
sian.
Compared to SGD and mSGD, there are more challenges in analyzing the convergence of Ada-
Grad. The challenges mainly come from two aspects: (1) since AdaGrad does not have a decreas-
ing step size, the noise influence on AdaGrad cannot be restrained as mSGD which is with the
help of decreasing step size satisfying Assumption 3; (2) adaptive step size of AdaGrad in equa-
tion 5 (i.e., oŋ/∙∖∕∑i=1 kVjig(θi, ξi)k2) is a random variable conditionally dependent of ξn given
σ{θ1, ξ1, ..., ξn-1}. Then when we deal with terms in the proof like
α	α2
-√= Vθng(θn ) T VθBg (θn, ξ n ) or	-θ ∣∣Vjng(θfl, ξn )∣∣2,	(12)
Sn	Sn
we cannot make the conditional expectation to transform equation 12 to
√0=∣∣vΘng (θn )∣∣2	or	S0EE ξn (∣∣vΘng (θn, ξ n )∣∣2).
8
Published as a conference paper at ICLR 2022
In the literature, Li & Orabona (2019) changed the step size to
_________ao__________
q∑w1kVθig (a, ξi)k2，
and Gadat & Gavra (2020) changed the step size to Yn+1 / √ωn + ε where
ωn = ωn -1 + γn (PnkVθng(θnn, ξn )『一qnωn - 1),
and γn , pn , qn are tuned parameters, in order to make it conditionally independent of ξn given
σ{θ1, ξ1, ..., ξn-1}, which however is no longer the standard AdaGrad as in equation 5 (Streeter &
Mcmahan, 2010; Chen et al., 2019).
Before we provide the main theorem of this subsection, a useful lemma is worth discussing.
Lemma 2 Let {αk}, {βk} being non-negative random variable sequences, such that the following
conditions hold almost surely
•	∑k+=∞1 αk = ∞;
•	∑k+=∞1 αkβk < +∞;
Then there exists a subsequence {βnk} of {βk}, such that βnk k-→→∞ 0 almost surely.
Proof 1 We aim to prove lim infk→+∞ βk = 0 by contradiction. Suppose lim infk→+∞ βk = u > 0,
then ∃k0, such that ∀k > k0, βk > u/2. It follows that
+∞	2	+∞
∑ αk < U ∑ αkβk < +∞.	(13)
Obviously, there is a contradiction between equation 13 and the condition ∑k+=∞1 αk = +∞. Thus,
lim infk→+∞ βk = 0. So we get that there exists a subsequence {βnk} of {βk} holds βnk k-→→∞ 0 almost
surely.
This lemma, inspired by Proposition 2 in Ya. I. Alber (Alber et al., 1998), is quite useful in the
convergence analysis of AdaGrad. Then we are ready to provide the convergence result of AdaGrad
in the following theorem.
Theorem 3 Consider the AdaGrad in equation 5. If Assumptions 1 and 5 hold, then for ∀θ1 ∈ RN
and S0 = 0, there exists a connected component of the Set J* ⊆ J, such that the estimate θn is
convergent to the setJ* almost surely, i.e.,
lim d(θn , J* ) = 0. a.s.
n→∞
In Theorem 3, we prove the standard AdaGrad is convergent almost surely, in contrast to the con-
vergence of time averages in Chen et al. (2019); Ward et al. (2019) which focus on the metric
} ∑ T=I E(k Vθng (θn )k2), or the subsequence convergence in Zou et al. (2019); Defossez et al. (2020),
focusing on minn=0,1,2,...,TE(kVθng(θn )k).
4	Conclusion and Future Work
In this paper, we studied the convergence of two algorithms extensively used in machine learn-
ing applications, namely, momentum-based stochastic gradient descent (mSGD) and adaptive step
stochastic gradient descent (AdaGrad). By considering general loss functions (either convex or non-
convex), we first establish the almost sure asymptotic convergence of mSGD. Moreover, we find
the convergence rate of the mSGD and reveal that the mSGD indeed has a faster convergence rate
than the SGD. Furthermore, we prove AdaGrad is convergent almost surely under mild conditions.
Subsequence convergence and convergence of time averages in the literature are substantially ex-
tended in this work to asymptotic convergence. To better understand the AdaGrad, we will study its
convergence rate in the future.
9
Published as a conference paper at ICLR 2022
5	Code of Ethics
This is a theoretical paper focusing on the investigation for the convergence of mSGD and AdaGrad
optimization algorithms. The results in this paper do not have issues in fairness, inappropriate
potential applications and impact, privacy and security issues, legal compliance, research Integrity
Issues, or responsible research practice (e.g., IRB, documentation, research ethics). This paper does
no involve human subjects, practices to data set releases, potentially harmful insights, methodologies
and applications, or pontential conflicts of interest and sponsorship.
6	Reproducibility
This is a theoretical paper focusing on the investigation for the convergence of mSGD and AdaGrad
optimization algorithms. The developed results are provided in the main paper, i.e., Theorems 1-3.
In Appendix, the proofs of these theorems are provided together with some useful lemmas as well
as their proofs. An outline of these proofs is given in Section A in Appendix.
10
Published as a conference paper at ICLR 2022
References
Ya I. Alber, Alfredo N. Iusem, and Mikhail V. Solodov. On the projected subgradient method for
nonsmooth convex optimization in a Hilbert space. Mathematical Programming, 81(1):23-35,
1998.
Wangpeng An, Haoqian Wang, Qingyun Sun, Jun Xu, Qionghai Dai, and Lei Zhang. A PID con-
troller approach for stochastic optimization of deep networks. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pp. 8522-8531, 2018.
Leon Bottou. Stochastic gradient descent tricks. In Neural Networks: Tricks of the Trade, pp.
421-436. Springer, 2012.
Han-Fu Chen. Stochastic Approximation and Its Applications. Springer Science & Business Media,
2006.
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of Adam-type
algorithms for non-convex optimization. In International Conference on Learning Representa-
tions, 2019. URL https://openreview.net/forum?id=H1x-x309tm.
Zaiyi Chen, Yi Xu, Enhong Chen, and Tianbao Yang. SADAGRAD: Strongly adaptive stochastic
gradient methods. In International Conference on Machine Learning, pp. 913-921, 2018.
Saman Cyrus, Bin Hu, Bryan Van Scoy, and Laurent Lessard. A robust accelerated optimization
algorithm for strongly convex functions. In American Control Conference, pp. 1376-1381, 2018.
Alexandre Defossez, Leon Bottou, Francis Bach, and Nicolas Usunier. A simple convergence proof
of Adam and Adagrad. arXiv preprint arXiv:2003.02395, 2020.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. In 23rd Conference on Learning Theory, COLT 2010, pp. 257-269, 2010.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(61):2121-2159, 2011. URL
http://jmlr.org/papers/v12/duchi11a.html.
John Duchi, Michael I Jordan, and Brendan McMahan. Estimation, optimization, and parallelism
when data is sparse. Advances in Neural Information Processing Systems, 26:2832-2840, 2013.
S. Gadat and I. Gavra. Asymptotic study of stochastic adaptive algorithm in non-convex landscape.
2020.
Igor Gitman, Hunter Lang, Pengchuan Zhang, and Lin Xiao. Understanding the role of momentum
in stochastic gradient methods. Advances in Neural Information Processing Systems, 32, 2019.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton. Speech recognition with deep
recurrent neural networks. In 2013 IEEE International Conference on Acoustics, Speech and
Signal Processing, pp. 6645-6649, 2013.
A. M. Gupal and L. G. Bazhenov. Stochastic analog of the conjugate-gradient method. Cybernetics,
8(1):138-140, 1972.
Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: Optimal algorithms for
stochastic strongly-convex optimization. The Journal of Machine Learning Research, 15(1):
2489-2512, 2014.
Heaton and Jeff. Ian Goodfellow, Yoshua Bengio, and Aaron Courville: Deep learning. Genetic
Programming and Evolvable Machines, pp. 305-307, 2018.
Geoffrey E. Hinton and Ruslan R. Salakhutdinov. Reducing the dimensionality of data with neural
networks. Science, 313(5786):504-507, 2006.
Yu M. Kaniovskii. Behaviour in the limit of iterations of the stochastic two-step method. Zhurnal
Vychislitel’noi Matematiki i Matematicheskoi Fiziki, 23(1):13-20, 1983.
11
Published as a conference paper at ICLR 2022
Donghwan Kim, Sathish Ramani, and Jeffrey A Fessler. Combining ordered subsets and momentum
for accelerated X-ray CT image reconstruction. IEEE Transactions on Medical Imaging, 34(1):
167-178, 2014.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference for Learning Representations, 2015.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep con-
volutional neural networks. Advances in Neural Information Processing Systems, 25:1097-1105,
2012.
Timothee Lacroix, Nicolas Usunier, and Guillaume Obozinski. Canonical tensor decomposition for
knowledge base completion. In International Conference on Machine Learning, pp. 2863-2872,
2018.
X. Li and F. Orabona. A high probability analysis of adaptive sgd with momentum. 2020.
Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive
stepsizes. In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 983-
992, 2019.
Yanli Liu, Yuan Gao, and Wotao Yin. An improved analysis of stochastic gradient descent with
momentum. arXiv preprint arXiv:2007.07989, 2020.
Nicolas Loizou and Peter Richtarik. Momentum and stochastic momentum for stochastic gradi-
ent, Newton, proximal point and subspace descent methods. Computational Optimization and
Applications, 77(3):653-710, 2020.
Vien Mai and Mikael Johansson. Convergence of a stochastic gradient method with momentum
for nonsmooth nonconvex optimization. In International Conference on Machine Learning, pp.
6630-6639, 2020.
H Brendan McMahan and Matthew Streeter. Adaptive bound optimization for online convex opti-
mization. 2010.
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic
approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574-
1609, 2009.
Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Introductory Lectures
on Convex Optimization: A Basic Course, 2004.
Lam Nguyen, Phuong Ha Nguyen, Marten Dijk, Peter Richtarik, Katya Scheinberg, and Martin
Takac. SGD and Hogwild! convergence without the bounded gradients assumption. In Interna-
tional Conference on Machine Learning, pp. 3750-3758, 2018.
Boris T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Com-
putational Mathematics & Mathematical Physics, 4(5):1-17, 1964.
Boris T. Polyak. Comparison of convergence rate of one-step and multistep optimization algorithms
in the presence of noise. Izv. Akad. Nauk SSSR, Tekh. Kibern., 1:9-12, 1977.
Herbert Robbins and Sutton Monro. A stochastic approximation method. Annals of Mathematical
Statistics, 22(3):400-407, 1951.
O. Sebbouh, R. M. Gower, and A. Defazio. Almost sure convergence rates for stochastic gradient
descent and stochastic heavy ball. 2020.
Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro, and Andrew Cotter. Pegasos: Primal estimated
sub-gradient solver for SVM. Mathematical programming, 127(1):3-30, 2011.
Samuel L Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V Le. Don’t decay the learning rate,
increase the batch size. 2018.
M. Streeter and H. B. Mcmahan. Less regret via online conditioning. Computer Science, 2010.
12
Published as a conference paper at ICLR 2022
Ilya Sutskever, James Martens, George Dahl, and Geoffrey E. Hinton. On the importance of initial-
ization and momentum in deep learning. In International Conference on Machine Learning, pp.
1139-1147, 2013.
Shenghao Tang, Changqing Shen, Dong Wang, Shuang Li, Weiguo Huang, and Zhongkui Zhu.
Adaptive deep feature learning network with Nesterov momentum and its application to rotating
machinery fault diagnosis. Neurocomputing, 305:1-14, 2018.
Tijmen Tieleman and Geoffrey E. Hinton. Lecture 6.5-RMSProp, COURSERA: Neural networks
for machine learning. University of Toronto, Technical Report, 2012.
Zhong-zhi Wang, Yun Dong, and Fangqing Ding. On almost sure convergence for sums of stochastic
sequence. Communications in Statistics-Theory and Methods, 48(14):3609-3621, 2019.
Rachel Ward, Xiaoxia Wu, and Leon Bottou. AdaGrad stepsizes: Sharp convergence over noncon-
vex landscapes, from any initialization. In International Conference on Machine Learning, pp.
6677-6686, 2019.
Matthew O. Williams and Teems E. Lovett. Numerical diagnostics for systems of differential alge-
braic equations. arXiv preprint arXiv:1602.07550, 2016.
Tianbao Yang, Qihang Lin, and Zhe Li. Unified convergence analysis of stochastic momentum
methods for convex and non-convex optimization. arXiv preprint arXiv:1604.03257, 2016.
Szymon Zareba, Adam Gonczarek, Jakub M. Tomczak, and Jerzy Swiatek. Accelerated learning
for restricted Boltzmann machine with momentum term. In Progress in Systems Engineering, pp.
187-192. 2015.
Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, and Wei Liu. A sufficient condition for conver-
gences of Adam and RMSProp. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 11127-11135, 2019.
13
Published as a conference paper at ICLR 2022
A Appendix Outline
Section B aims to verify the convergence results of mSGD. Several auxiliary lemmas are first pro-
vided, followed by a proof outline for the main results of mSGD. Then the proofs of these lemmas
are given. Theorems 1-2 are proved in Sections B.9 and B.10, respectively. Section C aims to verify
the convergence results of AdaGrad. Some auxiliary lemmas are given at the beginning, followed
by a proof outline of the main results for AdaGrad in Section C.1. Then we provide the proofs of
the lemmas. Theorem 3 is proved in Section C.8.
B	Convergence and convergence rate of mSGD
The following lemmas are used for the proofs of Theorems 1-2. Hereafter, N+ denotes the set of all
positive integers.
A function f is in a (differentiability) class Ck if its lth derivatives exist and are continuous, where
l≤k.
Lemma 3 (Lemma 1.2.3 in Nesterov (2004)) Suppose f(x) ∈ C1 (x ∈ RN) with gradient satisfying
the following Lipschitz condition
kvf (X) - Vf (y)k≤ Ckx-yk,
then for any x,y ∈ RN, it holds that
f (y) + Vf(y)T(X-y) - 2kx-yk2 ≤ f (X) ≤ f (y) + Vf (y)T(x-y) + Ckx-yk2.
Lemma 4 Suppose f(x) ∈ C1 (x ∈ RN) with gradient satisfying the following LipsChitz Condition
Vf(x)-Vf(y) ≤Ckx-yk,
and the setS= {x|V f (x) = 0} is bounded and only has finite ConneCted Components {S1,S2, . . . ,Sm}.
Furthermore, assume there exists ε10 > 0, suCh that for any i = 1,2, ..., m and x ∈ {x|0 < d(x,Si) <
ε10}, it holds that f(x) -fi 6= 0, where fi = f(x) for x ∈ Si. Then for any i = 1,2, ..., m, if there is
ε00 > 0 satisfying d(x, Si) < ε00, it follows that
Vf(x)2≤2Cf(x)-fi.
Lemma 5 (Wang et al., 2019) Suppose that {Xn} ∈ RN is an L2 martingale differenCe sequenCe,
and (Xn,Fn) is an adaptive proCess. Then it holds that ∑k∞=0 Xk < +∞ a.s., if
∞∞
∑ E(k¾≈k2) < +∞,	or ∑ E (kXnk2∣Fn-1)< +∞. a.s.
n=1	n=1
Lemma 6 Suppose that {Xn } ∈ RN is a non-negative sequenCe of random variables, then it holds
that ∑∞=oXn < +∞ a.S., if ∑∞=oE (Xn) < +∞.
Lemma 7 Suppose {Vn } is a sequence generated by mSGD in equation 3. Under Assumptions 1-3,
it holds that ∑n+=∞0 E	kvnk2	<	C(v0, θ1),	and ∑n+=∞0 kvnk2	<	+∞	a.s.,	where C(v0,	θ1)	is a Constant
only related to v0 and θ1 .
Lemma 8 Suppose {θn} is a sequence generated by mSGD in equation 3. Under Assumptions 1-3,
it holds that: for n ≥ 1,
nn
∑ εtE (∣∣Vθtg(θt)k2) < B(vO, θι) < +∞,	∑ εt∣∣Vθ,g(θt)k2 < +∞.
t=1	t=1
where B(v0, θ1) > 0 is a constant only related to v0 and θ1.
14
Published as a conference paper at ICLR 2022
Lemma 9 Suppose {θn} is a sequence generated by mSGD in equation 3 such that {g(θn)} con-
verges a.s.. IfAssumptions 1-3 hold, and
n2
g(θn +ι) ≤ Zn — b∑εt∣∣Vg(θt)∣∣ a.s.,	(14)
t=1
where ζn is a random variable such that limn→+∞ ζn = ζ < ∞ a.s., andb is a positive constant, then
there exists a connected component J* ofJ := {θ ∣Vθg(θ) = 0}, such that
d(θn, J*) → 0.
B.1 Proof Outline of Theorems 1 and 2
The proof is in light of the Lyapunov method. We aim to prove thatg(θn) is convergent a.s., and then
to prove Vθng(θn) → 0 a.s. With these two results, we are able to get θn → J* a.s. In the following,
we provide the proof outline to show how to obtain the provided results of mSGD.
Step 1: We prove mSGD is a stable algorithm, i.e., E(g(θn)) < K < +∞,∀ n, in Lemma 1. The idea
is to prove that a weighted sum of the loss function value, i.e.,
n
Un ：= ∑ (1/(2-α)y- E(1 + g(θt+ι))
t=1
is bounded through a recursion formula (a rough form)
n-1	t	2
Un — Un一1 ≤ Aαn + B ∑(1/(2 — α))n Z2E ∣∣Vθtg(θt,ξt)∣∣ (A, B are two constants).
t=1
}
'∙^^^^^^^^^^^^^^^^^^^^^{^^^^^^^^^^^^^^^^^^^^^^
I
Then We apply Assumption 2 4) to I and then obtain
n —1
Un - Un -1 ≤ aαn + B ∑ (1/(2 — α)广£；E(1 + g(θt)) (A, B are two const ant s).
t=1
{z
R
}
Combining Un-1 and R leads to
Fn — Fn-i ≤ Aαn + BB J Ia) (A, BB are two constants),
where
Fn := ∑ (2-a)	Z(t + 1) E (et+1(1 + g(θt+1))
and
+∞
+∞
Z(t) = ∏(1 + M0εk2) = (1 +M0εt2) ∏ (1 + M0εk2) = (1 +M0εt2)Z(t + 1).
k=t
k=t +1
Thus, We are able to obtain E(g(θn)) < Fn-ι ≤ A∑+∞ αt + B∑+∞ (1/(2 一 α))t < +∞.
Step 2: From Lemma 1 and the condition ∑t+=∞1 εn2 < +∞, We are able to prove that ∑tn=1 kvt k2 and
∑ εt kVθt g(θt)k2 are convergent a.s. respectively, as stated in Lemma 7 and Lemma 8.
Step 3: We divide g(θn) into three terms
nn	n
g(θn)=∑A(n)kvtk2+∑BnεtkVθng(θn)k2+∑CnT(Vθng(θn,ξn)-Vθng(θn)).
t=1	t=1	t=1
From Lemma 7 and Lemma 8, We are able to prove that ∑tn=1 A(n)kvt k2 + ∑tn=1 BnεtkVθng(θn)k2 is
convergent a.s.. From the convergence theorem for martingale-difference sum (Lemma 5), We prove
that∑tn=1CnT(Vθng(θn,ξn) - Vθn g(θn)) is convergent a.s. Then Weproveg(θn) is convergent a.s.
Step 4: By Lemma 9 and the convergence ofg(θn) in Step 3, We get θn → J* a.s..
Step 5: After the proof of the convergence of mSGD, We analyze the iterates ofFn. Then under a neW
assumption lim infd(θ Ji)→0 kVθg(θ)k2/ g(θ) -gi ≥ s > 0, We are able to obtain the convergent rate
of mSGD.	,
15
Published as a conference paper at ICLR 2022
B.2 Proof of Lemma 4
First We construct a closed and bounded set S which satisfies S ⊃ Um=ι Si. Since ∣∣Vf (ɪ)k (X ∈ S)
is a continuous function on a closed set, ∣V f (X)∣ (x ∈ S) is a uniformly continuous function. Then
∃ ε20 > 0, ∀ Si, ifd(X,Si) < ε20, there is ∣V f (X)∣ < cε10 /4. We assign ε0 = min{ε10 /4, ε20}.
Let Si0 = {X|d(X,Si) < ε10}, Si00 = {X|d(X,Si) < ε00}. Since d(X, Si) is a continuous function, ∀X0 ∈ Si00,
we can always find a straight line l0 paralle to Vf(X0) and passing through X0, defined as
l0 : X = X0 +
V f(X 0)
∣V f (X 0)k
t (t ∈ R).
From Si00 ⊂ Si0, we know X0 ∈ Si0. Since Si0 is an open set, there exists α0 < 0 < β0, such that X0 +
t(Vf (Xo)∕∣Vf (X0)k) ∈ S0 (o⅛ < t < β0), and
α0 = S up (X0+1( f∣ ))∈Si
β0=inf (x0+1( ilfw)! ∈ Si.
Define function
g(t)= f (x0 + kVf(X0)kt) - fi (t ∈ (α0,β0)).
SinceX0+t(Vf(X0)∕∣Vf(X0)∣) ∈ Si0 (α0 < t < β0), it holds that
f (x0 + ∣VVf(X0)kt)-力=0.
So g(t) ∈ C1 (t ∈ (α0,β0)). Then for ∀ t0, t00 ∈ (α0,β0), from Newton-Leibniz formula, it follows
that
g(t 0) - g(t 00)=Z00 g 0(χ) dχ=Z00( g o(χ )-g '( t 0)+g o( t')) dχ=Zt,0( g o(χ )-g '( t')) dχ+Zt,0 g o( t 0) dχ.
t	t	t	t (15)
Next we will prove g, (X) satisfies the Lipschitz condition. According to the definition of Si, f(X0 +
tVf(X0)∕∣Vf(X0)∣) - fi keeps the same sign over t ∈ (α0,β0). Thus, ∀ τ1, τ2 ∈ (α0,β0), it holds
that
g ,(τ1)- g ,(τ2)ι=∣( f)k) (v f (χ0+τ1 f⅛) -V f (χ0+τ2 f⅛)!
≤ f)k I v f ( x0+τ1 f⅛) -V f (x0+τ2 f)k )∣
=V f (X0 + T1 f⅛ ) - V f (X 0 + T2 f⅛ )I.
∣	∣Vf(X0)∣	∣Vf(X0)∣	∣
From the Lipschitz condition of Vf, we have that
g,(τ1)-g,(τ2"≤V f (x0+τ1 ∣V Vf (X0)∣) -V f ( x0+τ2 ∣V Vf (X0)∣)∣∣≤ dτ1-τ2 | ∣V Vf (X0)∣
=C∣T1 — T2∣.
From the above analsis, we obtain the Lipschitz condition of g0(x), thatis, ∀ Ti, τ2, there is |gz(ti) 一
g0(τ2)∣ ≤ c∣Ti - T2∣. By using the absolute value inequality, we get that -c∣Ti - T2∣ ≤ g0(τ1) -
g0(τ2) ≤ c∣Ti - T2∣. Then it follows from equation 15 that
g(t0) -g(t") = [0 (g0(X) -g'(t0))dx + ∕00 g'(t0)dx ≥ /,0 -c|X - t0Idx + /,0 g'(t0)dx
=- 2c (t '-t 〃)2+g'( t')(t '-t 〃),
16
Published as a conference paper at ICLR 2022
Let t0 = 0,t" = -gz(0)/c. So t/ = 0 ∈ (θ0,β0). Next, We will prove t" ∈ (o¾,的).We separate it
into two cases. First, we assume that g0(0) > 0. In this case, because
"0=S UP (X 0+1( fk))”,
we just need to prove
X0 + 10 ∣∣vf;jl Il ∈ Si ∀10 ∈ ( - g ( ) , 0)∙	(16)
IIV f (X 0)∣∣	∖ C /
Note that
“卜0+10 kvff⅛, S)≤ d (x0, S)+|10 ivf⅛∣ ≤ε0+1101∙
From x 0 ∈ S 00, Wegetll f (X0 )∣ < C ε0∕4and d (x 0, Siɔ < εj. Then we have
d (X0 + 10 IIV(χ0 ) Il , Si} ≤ ε0 + 110∣ ≤ ε0 +l 乎I= ε0 + |V f(X0 川 ≤ ε0 + a ≤ ε2 < ε1∙
∖ IlV f ( x 0 川)	ICl	c	4	2
Thus, equation 16 holds, meaning when g0(0) > 0, t" ∈ (oŋ,β0).
Secondly, when g0(0) < 0, we can also prove t" ∈ (α0,β0). It follows that
g (0)=g (t') ≥ g (t ')-g( t 〃)≥ - 2- (t'-1 〃)2+g( t Y t，一t 〃)= - 2-( g(0))2+1(膜⑼)2 = V(g⑼)2
2C	2C	C	2C
=21c ((kvf⅛) Vf(x0)) = 21cIVf(X0)k2∙
That is
If (X0) - f∙∣≥ 2CIV f (X0)∣∣2∙
Because of the arbitrariness of X0, we concludes that ∃ε0 = min{ε1∕4, ε∖}, ∀Si (i = 1,2,…,m), if
d (X, Si) < ε0, there is
∣∣v f (X )∣∣2 ≤ 2 c ∣f (X) - f∙∣∙
B.3 PROOF OF LEMMA 6
For ∀δ > 0, we have
+ +∞ +∞ m m	∖、
p ∩U	∑X≥δ
∖n=1 m=n=n	' )
+∞ m	+∞	1	+∞
=n Set Xt ≥ δ) )=nl→m∞P(Σn Xt ≥ δ) ≤ n	对 ,
where the last inequality is due to Markov inequality. Since ∑∞=1 E (Xt) < ∞, it follows that
+∞ +∞	m	1	+∞
P ∩U Σ Xt ≥ δ ≤ nlim∞ SE Σ Xt = 0∙	⑺
n=1 m=n t=n	t=n
By Cauchy,s convergence test, we have ∑∞=i Xt < ∞, a∙s∙
B.4 Proof of Lemma 1
Recall the mSGD algorithm in equation 3
Vn = αVn-1 + εnVθng(θn, ξn)	(18)
θn +1 = θn - Vn ∙	(19)
Equation equation 18 is equivalent to
vn = αvn-1 + εnV9ng(θn) + εn (V9ng(θn, ξn) - V9ng(θn)) ∙
17
Published as a conference paper at ICLR 2022
Under Assumption 2 2), it follows from Lemma 3 that
-	^θtg(θt)tvt - 2Ilvtk2 ≤ g(θt+i) -g(θt) ≤ -θg(θt)tvt + 2Ilvtk2∙	QO)
In this subsection, we just use the right side of equation 20. The left side will be used in the next
subsection. Consider V&g(θt)tvt in the following
Ret g (θ,t )t vt = (Vθtg (θt ))τ(α vt -1 + εtVθιιg(θt, ξ t))
=α(Vet-1 g(θt-1)+ Vetg(θt) - Vet-1 g(θt-i))tvt-1 + εt'eg(θt f g(θt,ξt)
=αVθt-1 g(θt-i)tvt-1 + α(Vθtg(θt) - Vθg(θt-1))Tvt-I + εtVθtg(θt)τVeig(θt, ξt)
Recursively applying the above equation yields
t-1
Vθtg(θt)τvt = αt-1Vθ1 g(θ1)τv 1 + ∑ αt-i(Vθig(θi) - Vθ,-g(θ-))τvi-1
t	i=1	(21)
+ ∑ αt - i ε i Vθ ,g (θ i )τVθ,g (θ i, ξ i).
i=2
By substituting the above equation into equation 20 and noting -(Vθig(θi) - Vθ一g(θi-i))tvi-i ≤
∣∣Vθig(θi) - Vθi-1 g(θi-1 )∖∖kvi-1∣∣ ≤ d∣vt-1∣∣2,weobtain
g(θt+1) -g(θt)
.，	TL .	∙	T	C
≤-αt-1Vθ1 g(θ1)τvi -∑α-εVig(θi∙)τVθig(θi∙,ξi) + -1v∕∣2
i=2	2
t-1
-	∑ α t - i(Vθtg^t) - vθt-1 g (θt-I))Tvt-1
i=1
tt
<	-αt-1Vθ1 g (θ1)τ v 1 - ∑ α t-iεiVθig (θi∙)τVθig (θi∙, ξi∙) + C ∑ α t-i∣ viJ∣2.
i=2	i=1
(22)
Denote Z(t) := ∏+∞(1 + M0εk), where
Mo
cM
α1-δ (1 - αδ )(1 - a )2
where δ > 0 is a constant and M is introduced in Assumption 2 4). Here we define Mɔ, Z(t)
and δ to facilitate the proof equation 33. From Assumption 3, it holds that ∑+∞ εt < +∞. Thus,
∑t=∞ Moεt < +∞. From a general inequality ln( 1 + X) ≤ X for X > -1, we get
+∞	∞ ∞	)	j 8	)
Z(t) ≤ ∏(1 + M0£2) = exp( ∑ln(1 + M0£i) } ≤ exp( ∑M0ε2 < < +∞,
k=1	k=1	k=1
which means that Z(t) is uniformly upper bounded. Then multiplying Z(t + 1) on both sides of
equation 22 and taking the mathematical expectation yield
t
Z(t + 1)E(g(θt+1) -g(θt)) ≤-Z(t + 1)αt-1E(Vθ1 g(θ1)τv 1) + C∑αt-iZ(i + 1)E(∣vi∣2)
i=1
t
-Z (t + 1) ∑ α t-iεiE∣∣Vθig (θi)∣∣2
i=2
t
< -Z(t + 1)αt-1E (Vθ1 g(θ1)τv 1) + C∑ αt-iZ(i + 1)E (∣vi∣2),
i=1
(23)
where the first and second inequalities are respectively due to Z(i + 1) < Z (i) and Z(t +
1)∑i=2 αt-⅛E∣∣Vθig(θi)∣∣2 > 0.
18
Published as a conference paper at ICLR 2022
Next, We aim to analyze C∑ti =1 αIZ(i + 1) E (∣∣Vi∣∣2) in equation 23. It is proved in Appendix B.5
that
1i
Z(i + 1)EkVik2 ≤α(1+δ)iZ(⑴EkV0∣∣2 + 广 ∑ α(I+δ代-kε2Z(k + 1)E(∣Vθkg(θk) - Vθkg(θk,羡)『)
-0‰ Σ α(1+δ)(i-k)Z(k + 1) (E (ε-g(θ- +1)) -E (ε--1 g(θk)))
-=1	(24)
Taking a Weighted sum of equation 24 yields
t
∑ αt-iZ (i + 1) E k Vik2
i=1
t
≤ ∑ αt - i α (1+δ) iZ ⑴ E (k V 0∣∣2)
i=1
+ Σ ɑt-i(六 ∑ α(1+δ)(i-k)ε2Z(k	+	1)E	(∣Vθkg(θk)	-NGkg(θk, ξk)k2))
i=1	∖a k =1	)
-∑ α一 ( ɪ ∑ α(1+δ)(i-k)Z(k +	1)(E	(εkg(θk +1))	- E (εk-1 g(θk))) )	:=A + B + C.
i=1	∖a k =1	\	))
(25)
We derive that
/ J 、	αt—1 a^
A = (∑αδiytZ⑴E(kV0k2) ≤ ITayZ⑴E(kV0k2),	(26)
B = ɪ ∑ ∑ at--+δ(i--) (εIZ(k + 1)E (∣Vθtg(θk) -NGkg(θk,ξk)k2))
a	i=1k=1	\	)
1t
≤ a 1-δ(I- aδ)∑ at-kZ(k + 1)ε2E (∣Vθ四(θk) - Akg(θk,ξk)k2),	(27)
C = --2δ ∑ ∑ at-k+δ(i-k)Z(k + 1) (E (εkg(θk +1)) - E (εk-1 g(θk)))
a	k=1 i=k
=--2δ ∑(Σaδi)at-kZ(k + 1) (E (εkg(θk +1)) -E (εk-1 g(θk))) .	(28)
a	k=1 i=0
Substituting equation 26-equation 28 into equation 23 yields
at -1 c aδ
≤-Z (t + 1)at-1E (Vθ1 g(θ1)τ V1) + --δ- Z ⑴ E (k V 0k2)
1-a
C t	(29)
+ a 1-δ(I-aδ)∑at-iZ(i + 1)ε2E(∣Vθig(θi)-Vθig(θi,ξ)k2)
-a2cδ ∑ ( ∑ aδ )at-iZ (i + 1)( E (εig (θi+1)) - E(£一 g (θi))).
Construct a sequence {Vn } as follows
% = ∑ (2-1 )	Z(t + 1)E ((g(θt +1) -g(θt))
(30)
19
Published as a conference paper at ICLR 2022
By substituting equation 29 into equation 30 following the way of equation 26-equation 28, We have
% ≤α2Γ2-α)Z⑴IE(Vθ1 g(θ1)%)∣+ 1ɑ”：；(2-αδ Z⑴E(kV0∣∣2)
1 — a I	I 11 — α 八 1 一 a J
C	n / 1	∖n——
+ α-δ(1 — ɑδ)(1 — ɑJ2 ∑	)Z((+ 1jεf E (相g(θ，J -Vg(θ，，M)『)
一 3 1—δ (：_ αδ J ∑ f (n — tJ( 2-α ) z (t + 1j( E (εtg(θt+1j) - E(4-1g(θt j)),
where f (n — t J is defined as follows
f (n — tj = ∑ («(2 — aJ)k — aδ ∑ (a1+δQ — aJ)k.
k =1	k =1
Move the last term to the left-hand side of the above inequality, then we have
Σ (占)n tZ (t + 1J E ( et+j1g (θt+1j—六1j g (θt J))
≤ α1⅛j Z (IJIE (vθ1 g (θ1jτ v i) ∣+:n a； aj Z ⑴E (k v 0k2)	a
i - a i	i (1 一 a 八 1 一 a J
C	n /	1	、n-
+ aT(1 — aδJ(1 — aJ21∑	)	Z(t + 1jε彳E(^^(θtj - Vg(θt，ξ，J『)，
where
e(+JI= ( 1 +
2Cεtf(n — tJ ʌ
a1—δ (1 — aδ J)
(32)
Because of a < 1, it holds that f (n — t J > 0 and e(+∖ > 1. It follows from Assumption 3 that
c	n 1	n—t
aτ(1 — aδJ(1 — aJ2 ∑	)	Z(t + 1jεf E(^g(θ∣j --Vtg(θ，，ξ，jk2)
n 1	n—t
≤∑ (JZa) Z(t + 1JM0εtMn-1j(1 + E(g(θtJJ),
(33)
where
a1—δ(1 — aδJ(1 — aJ2 .
Calculate f (n — tJ, then we obtain
f (n — tJ = ∑ (a(2 — aJ)k — as ∑ (a1+δ(2 — aJ)k > 0.
k =1	k =1
20
Published as a conference paper at ICLR 2022
It holds that
∑ (占 j Z(t + 1)E (婢 1 g(θt+1)-e(n-1)g(θt)))
n 1	n --
-	∑ (2∙z∙ɑ)	Z(t + 1)M0ε"n-1)(1 + E(g(θt)))
=	£ ( 24ɑ ]-Z (t + 1)E (%(1 + g (θt +1 ))j
-	∑ ( 24α ) n n Z (t + 1)(1 + M0εt2) E (etn-1)(1 + g (θt ))j
c n	1 ∖n -1
-	ɑ 1-δ (1- αδ) ∑ f (n -1) ∖2-α )	(εt - εt-1)
>	£ (24a)n tZ(t + 1)E (e(+)1(1 + g(θt+1)))
-	∑ (24α)n nZ(t + 1)(1 + M0εt2)E (e(n-1)(1 + g(θt))).
Substituting equation 33 and equation 34 into equation 31 yields
£ (占[-Z(t + 1)E (e(+1(1+ g(θt +1)))
-	∑ (24α)	Z(t + 1)(1 + M0εt2)E (e夕-I)(I + g(θt)))
≤ αn^ Z ⑴ IE (Vθ1 g (θ1)T V 1)∣ + 2(cα- ：1) Z ⑴ E (k V 0k2).
(34)
(35)
According to the definition of Z(t), We have
+∞	+∞
Z(t) = ∏(I + M0εk2) = (1 + M0εt2) ∏ (1 + M0εk2) = (I + M0εt2)z(t + 1)∙
k=t
k=t+1
Let
Fn := £ (占)n ' Z (t + 1) E (e(+)1(1+ g (θt +1))),
and it follows from equation 35 that
Fn - Fn-1 ≤ Z (1)( -ɪ- ) n 1E (g (θ1)) + α n(2-a)Z (1)∣E (V为 g (θ1)τ V1)
∖2 — a1 — a
+2(cα- a；(二)Z (1) E(k V 0『).
(36)
Denote p = exp < ∑∞=1 M0εk >. By taking the summation of equation 36, we obtain
Fn ≤Z(1)E (e 1(1 + g(θ1))) ∑ (-ɪ-)t 1 + p(2二黑 E (V为g(θ1)τV1) ∑ αt
t=1 ∖2 - a/	1 - a	t=1
+
2(1Ej-R δ)Z ⑴ E (k V 0k2) Eaa
<
耳谭 E (e 1(1 + g (θ1))) +
1-a
Pa (2 - a)
(1 - a)2
E (Vθ1 g (θ1)τ V1)
+
ca1+δ (2 - a)
2(1 - a)2(1 - aδ)
Z ⑴ E (k V 0k2).
21
Published as a conference paper at ICLR 2022
Define
T (θ1, V 0 ) = 1 + H E (e 1(1 + g (θ1))) + p0⅛^
1 - α	(1 - α)
+
C α1+δ (2 - α)
2(1 - α )2(1 - αδ)
Z ⑴ E (k v 0k2).
E (Vθ1 g (Θi)t V i)
It follows from the relationship between g (θn+1) and Fn that
1	1 ∖ n - n
E(1 + g(θn +1)) <p(j-a)	Z(n + 1)E(e!2(1 + g(θn +1)))
n 1 T
<	p ∑ (Jza)	Z(t + 1)E (e(+)1 (1+ g(θt +1)))
<	pFn < 1 + T(θ1, V0),
which leads to E (g(θn)) < T(θ1,v0).
B.5 PROOF OF EQUATION 24
We consider
εig (θi+1) - εi-1 g (θi)
=εi (g(θi +1 - g(θi)) + (εi - εi-1)g(θi)
≤ εi (g(θi +1) - g(θi))
≤ -εivθig(θi)Tvi + 2εikvik2
= (-εiVθig (θi, ξi)τ Vi-(εiVθig(θi)-εiVθig (θi, ξ∙))τ v, + C ɛik Vik2
(37)
=α V T V i-1- k vi k2+ε 2∣∣vθi g(&)- vθ g 修,,ξ0∣∣2
-{eαtVi-I + ε2vθig(θiɔ)T(Vθig(θ)- V&g(&Sξɔ)) +1εiJlViJ∣2,
where the first inequality is due to εi ≤ ε- in Assumption 3, and the last equality is from equa-
tion 19. Since ξi and θi are independent, taking the mathematical expectation of equation 37 and
noting that
E (εiαVi-1 + εl2Vθ君(θ))τ(Vθ君闻-Vθ君(%ξ)))	= 0,
yield
E (εg>(θi+1)) -E (ε,∙-1 g(θi))
≤αE(vTVi-1) -E (kVi∣2) + gE (∣∣Vθig(θi) - Vθg® ξi)∣∣2) + Cε,∙E (∣∣v,∙∣∣2)
Moreover, it holds that
E (Vig(θi) - Vθig(θi, ξ)k2)
=E (Vig(θi,ξi)∣∣2) -E (Vig(θi)k2)
=』E(kVi-αVi-Ik2) -E(∣∣Vθig(θi)k2)
ε i
="(E(kVik2) + α2E(kVi-Ik2) -2αE(vTv-)) -E(∣Vθig(θi)∣∣2).
Combining equation 38 and equation 39, we get
E (εig(θi+1)) -E (εi-1 g(θi))
≤- 1 (E(kVik2) -α2E(kVi-Ik2)) - εE(kVθig(θi)∣∣2) +1ε,∙E(kVi∣2)
ε2	.
+ ε E (kVθ,g(θi) - Vθ,g(θi, ξi)k2).
(38)
(39)
(40)
22
Published as a conference paper at ICLR 2022
Since εi → 0, given any δ > 0, there is an integer i0 ≥ 0, such that for i ≥ i0, 1 - Cεi > α1-δ(δ >
0). Since i0 is finite, without loss of generality, we assume i0 = 0 for convenience, i.e., 1 - Cεi >
α1-δ(δ > 0)(i ≥ 1). Thus, we have
E (εig(θi+1)) -E (ε,∙-1 g(θi))
α 1-δ	ε2	(41)
≤ --2~ (E(kVik2)-α1+δE(kVi-III2)) + 吉E(∣Vig(θi) - Aig(θi,ξi)∣∣2).
Multiplying both sides of equation 41 by Z(i + 1), and noticing that Z(i) > Z(i + 1), we have
(Z(i + 1)E(kH2) -α1+δZ(i)E(kVik2))
≤-岛 Z (i + 1) (E (εig (θi+1)) - E (ε,∙-1 g (θi∙)fj + 系 Z (i + 1) E (∣V/(θ) - Vθ/(θi, ξ)k2).
Then equation 24 is obtained by recursively applying the above inequality.
B.6 Proof OF Lemma 7
From equation 40, we have
E (εng(θn+1)) - E (ε0g(θ1))
1	JL	〜	α2 JL	JL p2	n JL
< -2 ∑E (kv∕F)+ E ∑E (IIVt-Ik2) - ∑ — E (IlVag(θt)∣∣2)+ 2 ∑ εtE (IIMF)	(42)
-L ε2	.
+ ∑ ε E (kVθtg(θt) - Vθtg(θt,ξt)k2).
t=1 2
It follows from Assumption 2 5) and Lemma 1 that
E(∣Vθtg(θt) - Vetg(θt,ξt)k2) < M(1 + T(θ1,v0)) < +∞.
Because of ∑L=1 εt < +∞, there is a scalar M > 0 such that for ∀n
JL ε2	,..	…..c、 一
∑ t E(kVtg(θt)- vθtg (θt, ξ t )∣∣2) < M < +∞.
t=1 2
Then it follows from equation 42 that
1n
2 ∑(1 - - 2-Ca) E (k Vt k2)
2 t=1
≤M + ε0g(θ1) - εn E (g(θn+1)) + ~2 E (k V01∣2 - kvL∣2)
JL ε2	c、
-∑ q E (kVθtg(θt)k2)
t=1 2
<M + ε0g(θ1) + α2E (kv0∣∣2)∕2 < K,
where K is a positive scalar. Since εn → 0 when n is large enough, it holds that 5 (1 - -2) < 1 - -2 -
cεn. Without loss of generality, assume 5(1 - -2) < 1 - α2 - Cε2 for n ≥ 0, so ∑L=1 E (kVt k2) <
110K∑ < +∞. By Lemma 6, we obtain ∑：=1 kVt k2 < +∞.
B.7 Proof of Lemma 8
Through Taylor expansion, We derive that
g(θt+1) -g(θt)
T	T /	、t	(43)
=Vθ;tg(θζt)t(θt+1 - θt) = -Vθtg(θt)τvt + (vθζt虱θζt) - Vθtg(θt)) (θt+1 - θt),
23
Published as a conference paper at ICLR 2022
where θζt means a point between θt and θt+1. Substituting equation 21 into equation 43 yields
/	t-1
g(θt+1) -g(θt) = -(αt-1Vθ1 g(Θi)tv 1 + ∑ αt-i(Vθig(θi∙) - Vθ- g⑹—仆)TVi7
=	i =1
+ ∑ ɑt-iεivΘig(θi)Tvθig(θi, ξiD + (vθjtg(θZt) - vΘtg(θtD (θt +1 - θt).
i=2
It follows that
n
g(θn+1)= g(Θi) + ∑ (g(θt +1) - g(θt))
t=1
1 — αn	τ	1 — αn	.
=g(仇)-^i~~vΘig(θi)tv 1 + -j~~—ε1vθ1 g(θi)vΘig(θ1,ξ1 )
1	— α	1 — α
n 1 _ a n—t +1	n ,	、t
-∑ -1 - ^-εtvθtg(θt)V&g(θt,ξt) + ∑ (vθjfg(θζt) - vθtg(θt)) (θt+1 - θt)
n 1 _ a n - t +1 t-1
-∑ — N ∑ αI(Vθig(θi) - Vθi-1 g(Θji))TVi-1.
t=1	1 - α	i=1
(44)
Take the mathematical expectation of equation 44, and notice Assumption 2 3), then we have
ɑ	1
E(g(θn +1)) ≤E(g(Θi)) + TTa Vθ1 g(Θi)tV1 + TTaε1 E(∣∣Vθ1 g(θ1)∣∣2)
n	n	2 C n
-∑εtE(IIVetg(θt)k2) + C∑E(kW2) + 17a ∑E(k司2).
From Lemma 7, it follows that for some positive constant Q,
2
C∑E (kVtk2) + -c- ∑E (kVtk2) < Q.
t=1	1 - α t=1
Hence,
ɑ	1
E(g(θn+1)) <Q + E(g(θ1)) + Tra Vθ1 g(θ1)τV1 + 1-α£1E(∣∣Vθ1 g(θ1 )∣2)
n
-∑ɛtE (V,g(θt)k2).
t=1
As a result,
JL	C	a	TI	C
∑ɛtE(∣Vθtg(θt)k2) <Q + E(g(θ1)) + -- Vθ1 g(θ1)τv 1 + -- £1E(|尸为g(θ1)∣∣2)
t=1	1 α	1 α
-E (g(θn +1))
< + ∞.
From Lemma 6, we have ∑；=1 £/|V&g(θt)∣∣2 < +∞ a.s..
B.8	Proof of Lemma 9
We divide the proof into three steps.
The first step is to prove
nm+nfiVng (θn )∣∣2=0 a. s..
Suppose the above conclusion does not hold, i.e.,
nm+nf∣∣Vθng (θn )∣∣2 > S2 > 0 a. S.,
24
Published as a conference paper at ICLR 2022
where s is a random variable depending on sample paths. Then it holds that
+∞ +∞
H U\	U"(θm)『>
n=1 m=n
1.
From equation 14 and limn→+∞ Zn = ζ < ∞ a.s., if ∣∣Vθng(θn) ∣∣2 > ɪS2, it holds that
+∞
g(θn+1) ≤ ζn -b∑ εi = -∞.
i=n
It follows that
+∞ +∞
P(n→m∞g(θn +1) = -∞) ≥ P U \	Mmg(θm>
n=1 m=n
As a result, limn→+∞g(θn) = -∞, meaning a contradiction. Hence,
lni→m+inf ∣∣Vθng(θn)∣∣2 =0a.s.
1.
(45)
Step 2 is to prove that the set {θn} has an accumulation point contained in J with probability one.
From equation 45, we have ∀ε > 0
p( +∞ +∞ (Mmg(θm)∣∣2 > ε))= 0.
n=1 m=n
Since ∣∣Vθg(θ)∣2 is continuous, ∀δ > 0, it holds that
P( +∞ j ( θnJllθm - θ k2 > δ ))= 0.
In addition, under the given conditions, J is a closed set. It holds that
+∞ +∞
P	mθ∈inJ ∣θm-θk2 >δ
n=1 m=n
For convenience, let θn := argminj∈J ∣∣θn - θ∣∣2, then
+∞ +∞
P \ U	l∣θm-θm∣≤
n=1 m=n
0.
(46)
It follows that
+∞
P U ∖N∣θmi-θmJl≤
{mi }∈SN i=1
1.
1,
(47)
where SN is the set of all infinite subsequences of N. Since {θmm J⊂ J, {θmm J is bounded. From the
accumulative point principle, the event
+∞ U (∣θmi-θζk≤√δ),
n=1 i, j≥n
is a deterministic event, i.e., being true for every sample path. Thus
U \ (Wm「θmkjU ≤√δ)
{mki}∈SN(i)i,j≥1
is a deterministic event as well, where SN(i) is the set of all infinite subsequences of {mi}. Due to
the arbitrariness of {mi}, we get that
\ U \(画-θm∕∣ ≤√δ)
{mi}∈SN{mki}∈SN(i) i,j≥1
(48)
25
Published as a conference paper at ICLR 2022
is a deterministic event. Therefore,
P U ∩(侬i-θj<√δ)∩(∣∣θmi-θm$<√δ)∩(∣∣θInJ-θj<
∖{mi}∈乐 i,j≥1 '	J '	j V
(49)
=P [{mUsN { m^； NWm LI	ZE-%』(⑹
∩( l l θmkj-θm kj∖ ∣<
=P	U
∖{ mi }∈S⅛
≥ p	U
∖{ mi }∈Sn
+∞
∩ l∣θmi- θm ill <
i =1
l l θm ki -θm kjl l<
≥P U (∩llθmi - θmi
∖{mi}∈=1
∩(	∩ U ∩ (l l θmki-θ,
'{ mi }∈SN { mk. }∈SNi iij≥1 '
=p	∩ U ∩	l l θm-θ
∖'{ mi }∈SN { mki }∈SNi )i,J≥1、
∩( U (∩llθmi-θmill<
'{mi}∈外 'i=1
Combine (47) and (48), then We get that
P ∩ U ∩ l l θmki
∖\{ mki }∈SN { mi }∈SNi )i,J≥1 '
∩( U (∩llθmi -θm ill
'{mi}∈Sn 'i=1
1.
So
P U ∩ l l θmi
∖{mi}∈SN i,J≥1、
26
Published as a conference paper at ICLR 2022
Thus it holds that
P (∖∞ [ (。θ;-θj∖∖≤√δ) \。θ∙ - θJ ≤
≥ P (∖∞ ∙[((∖∖θi*-θ;\ ≤√δ )∖(∖∖θi-θi*∖∖ ≤√δ) ∖(∖∖θj-θ;\ ≤
∖∖θmi- θm i∖∖ < √δ)
=1.
This means We can find two ”same” convergent subsequences {θkn} and {θkn} with probability one.
Because J is a closed set and {θkn }⊂ J, we know that there exists θ00 ∈ J such that
lim θkn = θ 00 a.s.	(50)
n→+∞ n
This indicates that we can find an accumlative point of {θn } in J with probability one. Denote the
connected component containing θ by J*.
Step 3 is to prove that there is a connected component J* of J such that θn → J* a.s. From equa-
tion 50, we have
+∞ +∞
P ∖U(∖∖θm - θ00∖∖< δ')	= 1.
n=1 m=n
Since g(θ) is continuous, we have
+∞ +∞
P( \ U Qg (θm) - g (θ ɜl < δ ) ) = L
n=1 m=n
Since {g(θn)} converges a.s., let M be the limit, i.e., limn→+∞g(θn) = M a.S. Hence g(θ") = M,
and g(θ) = M for all θ ∈ J*. Define A = {θ |g(θ) = M}. From g(θn) → M a.s. and the continuity
of g(θ ), it holds that θn → A a.s. Obviously, J* ⊆ A. If J* = A, then the conclusion follows. Now
assume J* $ A. From the definition of J*, it follows that there are two disjoint open sets V and H
such that A ⊂HSV, J* ⊂ V,A/J* ⊂H, H0TV0 =0/ (H0 andV0 are closures ofH andV). So we
just need to prove that θn → V a.s. A contradiction argument is to be used. Suppose that {θn} has
accumulation points in V and H simultaneously with a probability larger than zero. That is,
+∞ +∞	+∞ +∞	+∞ +∞
P \ U(θm∈ H)	\ ∖U(θt ∈ V) \ U∖(θV ∈ H ∪ V)	> 0.
n=1 m=n	s=1 t=s	r=1 v=r
(51)
Expanding equation 51, we get
P( u∞(	\ U ((θi∈ V )∖(θj∈ H )))∖( +∞ (θV ∈ H ∪ V))) )> 0.
r=1 (m,n)∈N×N i≥m, j≥n	v=r
Note that the event
∩ U	(θi∈ V )∩(θj∈
(m,n)∈N×N i≥m, j≥n
can be written as
U
+∞
\
{αi}∈SN,{βi}∈SN i=1
∈V )∩(θβ i ∈
27
Published as a conference paper at ICLR 2022
where {αi} ∩ {βi} = 0/. The event
∩ U	(θ∙∈ V )∩(θj∈
(m,n)∈N×N i≥m, j≥n
+∞
\	∖(θV ∈ H ∪ V)
v=r
(52)
means ∀t ≥ r, θt must belong to one of H and V. So {αi} ∪ {βi} must include the set {r, r+ 1,r+
2, ...}. Denote {αi} and {βi} as {αi(r)} and {βi(r)}, and then equation 52 can be written as
+∞
U	\	(θα(r) ∈
{αi(r)}∈SN,{βi(r)}∈SN i=1
V)∖(θβ(r) ∈ H))
So we get
P [∞	U ∩ ((θαarr) ∈ V)\综(" ∈H))) >0.	(53)
r=1{αi(r)},{βi(r)}i=1	i	i
It follows from V0 T H0 = 0/ that
inf	θ(1) -θ(2) =s > 0,	(54)
θ(1)∈H,θ(2)∈V
where s is a constant. By Lemma 7, it holds that ∀s > 0
+∞ +∞
P ∩U kθm-θm+1k ≥s =0.
n=1 m=n
(55)
Now We consider {α(r) + 1}. In fact, {α(r) + 1} and {β(r)} have a finite number of identical
elements with probability one. If not, then
+∞ +∞
\U	Jr) ∈ V Mamr )+1
n=1 m=n
> 0.
By equation 54, it follows that
+∞ +∞
P ∩Uθm-θm+1≥s
n=1 m=n
+∞ +∞
∩Uθαm(r)-θαm(r)+1≥s
n=1 m=n
+∞ +∞
∩U	θαm(r) ∈V ∩	θαm(r)+1 ∈
n=1 m=n
> 0,
which however contradicts with equation 55. Hence αi(r) + 1} and βi(r) } have a finite number of
identical elements with probability one. From αi(r) }Sβi(r) } ⊃ {r, r + 1, r + 2, ...}, we know that
+∞ +∞
P U∩(am) + 1) ∈ {a(r)} = 1.	(56)
n=1 m=n
Hence {βi(r) } is a finite sequence with probability one, otherwise it contradicts with equation 53.
Thus, the assumption does not hold, and θn → V a.s. Furthermore, θn → J*. Therefore, there is a
connected component J* of J such that
lim d (θn, J *) = 0.
28
Published as a conference paper at ICLR 2022
B.9	Proof OF Theorem 1
First of all We aim to prove that g(θn +1) is convergent almost surely. Divide equation 44 into four
parts as follows.
1 — αn	τ 1 — αn	.	.	.
g(θn +1)= g(仇)--j~~— Vθ1 g(θ1)τv 1 +-——ε1Vθ1 g(θ1)Vθ1 g(θ1, ξ1)
1 — α	1 — α
X-----------------------V------------------------}
(4)
n 1 _ a n—t+1	n ,	、7
—∑ —1-α-εtV&g(θt)τvθtg(θt,ξt)+∑ (vθζfg(θζt)—V&g(θt)) (θt+1 - θt)
(57)
Y
J Y
(B)	(C)
n 1 _ a n—t+1 t —1
—∑ — N	∑ α I(Vθg (θi) — Vθ-1 g (θi-1)) Tv-1.
t=1	1 — α	i=1
X----------------------------------------------------------/
^Z
(D)
Due to α < 1, αn is tending to zero, which ensures the convergence of part (A). For (C), we consider
the absolute value of ∑之n. It follows from Assumption 2 3) that
∑ (vθ^tg(θZt)— vθtg(θt))	(θt +1	— θt)	≤ ∑	(Vθζ∙tg(θζt) — vθtg(θt))	(θt +1	— θt)
t=n、	/	t = n、	/
m ,,	,,	m
≤ ∑ ∣∣Vθζtg(θζt) — Vθtg(θt)∣∣kVtk ≤ C∑ kVtk2.
t=n	t=n
Through Lemma 7, we get ∑m=n kvtk2 → 0 a.s., leading to
∑ (vθζ,g(θζt) — V&g(θt)) (θt+1 — θt)
→ 0 a.s..
Through Cauchy'S test for convergence, we know that (C) is convergent almost surely. By using
the same function, it holds that (D) is convergent almost surely. For (B), we have
(B )= ∑ 1 — α"t"
∑ 1 - α
_ f 1 — αn-t+1
=Σ 1 — α
εt Vθtg (θt )τVθt g (θt, ξ)
n 1 _ α n 一 t+1
εt kvθtg(θt )k2+∑^⅜^ εt vθtg (θt )T(V“g(θt，ξ M vθtg (θt)).
(58)
From Lemma 7, it follows that (C) is convergent almost surely. For (B), it holds that
(B )= ∑ 1 — α"t"
∑ 1 - α
1 — αn-t+1
=Σ 1 — α
εt Vθtg (θt )τVθt g (θt,&)
“j n T+1	(59)
εt kV“g(θt )k2+∑F^&vθtg (θt )T(V“g(θt，ξ t) — vθtg (θt)).
By Lemma 8, we know
n	n—t+1
∑ —；——εtkvθtg(θt)k2 < +∞ a.s..
t=1	1 — α
From Lemmas 5 and 8 it follows that
∑ 1] α	εt+1v¾g(θt)τ(vθtg(θt,ξt) — V&g(θt))
t=1 1 — α
is convergent a.s. Thus (B) is convergent a.s., and g(θn+1) is convergent a.s.. Substituting equa-
tion 59 into equation 57 leads to
g (θn+1) ≤ ζn-∑ εt∣∣vθtg (θt )∣∣2,
t=1
29
Published as a conference paper at ICLR 2022
where {ζ0} is defined as follows
一.	1 — αn	T 1 — an
ζn = g(θ1) -	Vθ1 g(Θi)tv 1 + —— εχVθlg(θχ)Vθlg(&,ξ1)
1	— α	ι — α
n 1 _ a n -1 +1	n	T
-∑ -1-a-εt%tg(θt)T(V&g(θt,ξt)--v&g(θt)) + ∑ (vθj,g(θζt) - Vetg(θt)) (θt+1 - θt)
n 1 _ a n - t +1 t-1
-∑ — N	∑ αt-i(Veig(θi) - Ve-g(θi-1))τv-1.
t=1	1 - a	i =1
we know {ζ；} is convergent a.s. It follows from Lemma 9 that there exists a connected component
J* of J such that lim d(θn J) = 0.
n →∞
B.10 PROOF OF THEOREM 2
First of all we can get that
t-1
g(et+1) -g(et) = -(αt-1Vθ1 g(θ1)τv 1 + ∑ αt-i(Ve,g(θi) - Vθ-g(θ-1))TVi-I
'	i =1
+ ∑ɑt-iεiVeig(ei)τVeig(ei,ξi)) + (VeZtg(eζ() - Vetg(et)) (et+1 - et)∙
i=2
(60)
From Theorem 1, it follows that g(en) is convergent a.s., and it is orbitally convergent to gi (i
1,2,...,N) a.s.. Then it holds that
N
n qm∞ g (en ) = ∑ Iigi
a. s.,
i=1
where
1 1	lim n →+∞ g (en )= gi
0 0	limn→+∞g(en) = gi
For convenient, we let g* = ∑∞11 Iigi. Then we make some transformation on equation 60
t-1
(g(et+1)-g*) - (g(et)-g*) = -(at-1vΘ1 g(eI)Tv 1 + ∑ αt-i(Veig(ei)- Ve-g(ei-I))TVi-1
V	i =1
+ ∑αt-iεiVeig(θi)TVeig(θi∙,ξi)) +(Ve‘tg(θζt) - Vetg(θt))T(θt+1 - θt).
i=2
(61)
Then we make some transformations, take absolute values, take the mathematical expectation, and
use same techniques in Theorem 1. Since the sampling noise follows a uniform distribution, there is
E(kVeng (θn, ξn )k2) ≤ M E(∣∣Ve“g(en )k2). So it follows that Fn-Fn-I ≤ Pn-1 - Qn-1, where Pn-1,
Qn-1, and Fn are defined as follows
pn-1=(Γ⅛ (占了-1 ε1E(kVe1 g(θ1)k2) + Z⑴小占了-1E(g(θ1))
-罕萨Z(n + 1)e (VeIg(θ1)τV1) + 2：："Rδ)Z⑴E (kV0『),
1 — a	2( 1 — α 八 1 — α )
Fn=∑ (2-a Yz (t+I)E (署1g (θt+1)-g*∣),
1 n 1	n -1
Qn-I =1! ∑E ) &E (^ g (θθ)k2),
where g* = infe∈rNg(θ). Then we have
Fn≤ (1 - Q-)F- + Pn-1.
(62)
(63)
30
Published as a conference paper at ICLR 2022
Derive Qn/εn +1 Fn as follows
Qn	1	∑31( £ )n+1-tεt+1E (N g (θt )∣∣2)
-
εn+1F	(1 - α) ∑ n=1 (七 )-'z (t + 1) Z (t + 1) E (e(+⅛ (θt +1)-g *|)
=1	(n)72E (IN为 g (θ1)k2) + ∑n=1 (七 )-t E (IRt+1 g (θt+1)k2)
(I- O=)2	∑ n=1 (2-α)- tZ (t +1) Z (t +1) E (招 I g (θt+1 )-g * |)
It follows that
liminf
n →+∞
Qn
εn+1 Fn
1	liminf (2⅛)"£2E (IN为g(θ1 )k2) + ∑；=1 (七)-tE(IRt+1 g(θt+1)k2)
(I-0 )2 n →+∞	∑n=1 (2-_ )-tz ((+ 1) E (招 I g (θt+1)-g *∣)	(M)
1 lim inf	∑ n=1 (n)-t E (kRt+1 g (θt+1)k2)
(I-a)2 n →+∞ ∑n=1 (2-_)-(Z (t + 1) E (%∣ g (θt+1)-g*∣) .
From equation 32, We have
,(n) = 1 ι 2Cεtf (n - t)
et+1 = 1 + a 1-δ (1 - aδ)
=1 + OW⅛ ( ∑1(α (2 - a N- aδ∑1(a1+δ Ma N )
≤ 1 + Caεt,
(65)
where Ca
_______2 C (2-a)_____
a-δ (1-aδ )(1-a(2-a))
Substituting equation 65 into equation 64 yields
liminf -Q—
n →+∞ εn+1 Fn
≥ ①⅛ lnm+nf
∑n=1 (2⅛ )-'e (IRt+1 g(θt+1)k2)
∑n=1 (2⅛)-(Z (t+1)E (% I g (θt+1) - g* I)
(66)
Then we proceed with the proof under two different cases, namely,
∑n=1 (2⅛)-'E(IRt+1 g(θt+1)k2)=+∞and∑n=1 (2⅛)-tE(∣∣R,”g(θt+1)k2) < +∞.
First, if ∑n=1 (2ɪ0)-t E (∣Rf+1 g(θt +1)∣∣2) = +∞ (the proof for this condition is up to equation 76).
It follows from Assumption 4 2) and the uniform convergence and O/stolz theorem that
Qn
Iiminf-------
n →+∞ εn+1 Fn
1	liminf	∑ n=1 (£)-'E (IRI+1 g(θt+1)k2)
(1 - a)2 n→+∞ ∑n=1 (2⅛)-tZ(t + 1) E ((1 + Caεt) I g(θt+1) -g* I )
≥ ——liminf
一 (1 - a)2 n→+∞
E (∣Vθn +1 g(θn +1)∣∣2)
Z(n + 1) E ((1 + Caεt) I g(θt +1) -g* I )
≥ ——liminf
一 (1 - a)2 n→+∞
s
s
Z(n + 1) (1 + Caεn) 一 P(1-a)2,
(67)
31
Published as a conference paper at ICLR 2022
where p = exp < ∑∞=1 Mε- >. By using O,stolz theorem on Qn /Fn, it follows that
1
lim Q = ——-ɔ lim
n→+∞ Fn (1 — α)2 n→+∞
∑2ι(2⅛ )入++1E (l∣Vθt+1 g (θt+1)k2)
∑ n=1 (2-α ∖'z (t +1) E g+11Ig (θ++1)-g*)∣
<	- lim ∑n=1 (2⅛)-tεt+1 E(∣∣Vθt+1 g(θt+1)k2)
一(I-a )2 n →+∞∑n=1 (£)- tZ (t + 1) E I g (θt +1)-g
=1 lim	£n E (∣∣Vθng (θn )k2)
(1 - α)2 n→+∞ Z(n + 1)E i g(θn +1) -g* i
<	1 lim	t £n E (g(θn +1)-g *)	= 0
一(I-«)2 n →+∞ Z (n + 1) E i g (θn +1) - g * ∣	^
1
r*
So we conclude that ∃n0 ∈ N+, such that ∀n ≥ n0,
⅜ < 1 - ⅛⅞ ∙
From equation 63, it follows that
Fn < ∏ (1-Q) FO0 + ∑ ∏ (1-Q) Pt-1.
i=n0	Fi	t=n0+1 i=t	Fi
Using the inequality ln(1 + X) < X and equation 67, we get
ΓI1 (1 - F) = exp ( ∑1ln (1 - F)) < exp ( - ∑ F) < ^∑建展
i=n0	i	i=n0	i	i=n0 i
/ - ∑ n	S εi 、
=O e ∑i=1 P(I-α)2),
where k is a constant. It follows that
∑t=n0+1 ∏ n- (1 - Qi) Pt-1
-∏ n-; (1-粉 FI-
n
∑
t=n0+1
Pt-1
∏ t—n。(1 - QO
From equation 62, we have
11
Pt-1
(1 - a)2 2--ɑ
t-1	1	t-1
£1E (IN为 g (θ1)k2) + Z (1) L( 2-a	E (g (θ1))
+2(c1""δ) z ⑴E (k V 0k2)=p( 2⅛)t-1+qt-1
whereP = (IJaP£1E(|卜％g(仇)『)+ Z(1)LE(g(θ1)) andq
C αδ+1 (2-α)
stituting equation 72 into equation 71 and noting equation 68 yield
∑t=n0+1∏ n=J (1- ⅜∙ )p+-1
-∏ n-, (1- F) F-
2(1-α )(1-αδ)
Z ⑴ E (k V 0∣∣2).
(68)
(69)
(70)
(71)
(72)
Sub-
(73)
n
∑
t = n o+1
Pt-1
∏t=1 (1- Q) <
n
∑
t = n o+1
P(金)+ ∑
∏t=n。(1 - Q)	t=∑+1
n
qat-1
∏t=n0 (1- Q)
<	∑	p(£)t-1 + ∑	qat-1
<	∑	/ 3-α V-n0 + ∑	/ 3-α ∖t-n0
t =n0+1 ∖2(2-α))	t=n0+1 ∖2(2-a))
3-a	n0-1	n	2	t-1
<	)	(P+研 t=∑J…)<c0,
32
Published as a conference paper at ICLR 2022
where c0 is a positive constant. It follows that
n∑1∏(1-F ) P-	<	C 0	ΓI1	(1 - Q" O ∏	(1	- F H= O (屋 ∑ n=1	展
t=n0+1 i=t	Fi
i=n0	i	i=n0	Fi
Combine equation 69, equation 70 and equation 74, then we get
(74)
n-1
Fn ≤ ∏ 1
I=n 0 ∖
In addition, we have
Q )fi + n∑1 ∏ (1
i	t=n0+1 i=t
F JPt-1=O g∑ 屋 p"'a>2
(75)
/ - ∑ n	S-i
E(g(θt+1)-g*) <Fn = Ole ∑i=1 P(1-a)2
(76)
If ∑n=1 (21a)-'e (kVθt+1 g(θt +1)k2) < +∞, it holds that
n→m∞( Q ΓE (kVn+1 g (θn +1)k2 = 0,
that is
E (kVθn+1 g(θn +1)∣∣2) = O
Under Assumption 4 2), we have
lim sup
n →+∞
E (g(θn +1)-g*)
(2-a)n
≤ limsupE (∣∣vθn+1g (θr)∣∣2) ≤ 1E (∣∣v711g ()θn+1)∣∣2) = 0.
n →+∞	s( 2-a)	s	(2-a)
It follows that
E Ig(θn+1)-g*∣ = O
-∑ n	S -i
Now we compare (2-1α )n with e i=1 P(I-Oo2. It holds that
(2ka)n
exp (-n ln(2 - α))
exp (- ∑n=1 P⅛) = exp (- ∑ n=1 P (1-α)2
exp (∑ P(¾ - n ln(2 - α)).
It follows from Assumption 3 that -n → 0, indicating ∑晨	s-亚-n ln(2 - α) < 0 when n is
P(I-a)
sufficiently large. Thus, we have
lim sup
n →+∞
(2-a) n
exp(- ∑忆 1 p(1 S-α)2 )
(&	s - i	1 /	、\
limsupexp ∑ ——---K- nln(2 - a) < 1.
n→+∞	3=1 P (1 - α )2	1	7
Then it holds that
E Ig(θn+1)-g*∣ = O
/ - ∑ n	S -i
O∖e ∑i=1 P (1-α)2
(77)
Combining equation 77 and equation 76 leads to
/ _ ∑ n	S-i
Eg (θK+1) -g*∣=Og ∑ i=1 Er
if ∑n=1 (2-a)-1E(∣∣Vθt+1 g(θt+1)k2) < +∞.	The above bound holds trivially if
∑n=1 (2-α)-1E (l∣Vθt+1 g(θt+1)k2) = +∞. It follows from Lemma 4 that
n	(一 y,n	S-i
E(∣∣Vθng(θn)∣∣2) = O(e ∑i=1 P(1-a)2
33
Published as a conference paper at ICLR 2022
C Convergence of AdaGrad
The following lemmas are used for the proof of Theorem 3.
Lemma 10 Suppose f (x) ∈ C1 (x ∈ RN) with f(x) > -∞ and its gradient satisfying the following
Lipschitz condition
IIV f (x) - V f (y )∣∣ ≤ C k x-yk,
then ∀ x0 ∈ RN, there is
∣∣Vf (Xo)∣∣2 ≤ 2c(f (Xo) - f*),
where f * = infX∈ RNf(X)
Lemma 11 Suppose {θn} is a sequence generated by AdaGrad in equation 5, and Assumptions 1
and 5 hold. If ∣∣Vθn g(θ)∣∣2 > a where a is given in Assumption 5 3), then for any n ∈ N+, θ1 ∈ RN,
and ε ∈ (0,1), it holds that
g(θn +1)	g(θn) <αθ∕"qn 八∣vθn-1 g(θn -1 )『∣Rθng(θn)∣∣2∖
音: - 干 ≤ 工(M+1)( F+	L)
- αo ∣∣Vθng(θn)∣∣2 + αo ∣ ∣∣VθK一g(θn-ι)∣2 - ∣∣Vθ“g(θn)∣∣2
20	Sn广；	20v	Sn+ε	Sn+
where
+ 4M2 α03 c2
∣Vn — g (θn-ι, ξ n-ι )∣∣2
+” Y刊+X()+γ(),
Xne)=α2o 七 vθng (θn 产(Vθng(θn)- vθng(θn, ξn))
Sn2-1
Y(ε) = αo 1 ɪE (∣∣vθng(θn,ξn)∣∣2IFn-1) - ɪ ∣∣Vθng(θn,ξn)∣∣2
2 M M +1	q2+ε	M +1 q1+ε
Sn 1	Sn 1
Lemma 12 Suppose {θn} is a sequence generated by AdaGrad in equation 5, and Assumptions 1
and 5 hold. If ∣∣Vθn g(θ)∣∣2 ≤ a where a is given in Assumption 5 3), then for any n ∈ N+, θ1 ∈ RN,
and ε ∈ (0, 1), it holds that
g (θn+1)	g (θn )
---7-------------
Sε+1	Sε
α0∣∣Vθng(θn)∣∣2	α3C2(M + 1)2 ∣∣vθn-1 g(θn-1,ξn-1)∣∣2
≤ --20SI+F+_2	s+
n-1
+ 也邛∣∣Vθn-1 g(θn-1,ξn-1)∣∣2 + α (KH
2 c2+ε	20 ∖	c2+ε
2Sn-1	Sn-2
—
+ α√M +1) (∣Vn - g(θn-1)∣∣2 - ∣Vng(θn)∣∣2
2	\	02 +ε	Q1 +ε
Sn	1	Sn
a0 a (M + 1)
+—T—
+ 在 ∣∣Vθng(θn, ξn)∣∣2
+ 2	Sn+
+A(nε) +B(nε),
where
A n)= 4+ε (∣∣Vθng (θn )∣∣2 - Vθng (θn ) T Vθng (θn, ξn ))
Sn-I
Bn)= αo∣∣vθng(θn 1)∣∣ (∣∣Vθng(θn,ξn)∣∣2-E(∣∣Vθng(θn)∣∣2∣Fn-1
2 a (M + 1) Sn}： '	'	1
34
Published as a conference paper at ICLR 2022
Lemma 13 Suppose {θn} is a sequence generated by AdaGrad in equation 5, and Assumptions 1
and 5 hold. Then ∀n ∈ N+, ∀θι ∈ RN ∀ε ∈ (0,2), it holds that
∑ g≡2 < +∞ a.s..
k=3	S"ε
Lemma 14 Suppose {θn} is a sequence generated by AdaGrad in equation 5, and Assumptions 1
and 5 hold. Then ∀n ∈ N+ ∀θι ∈ RN ∀ε ∈ (0, 2), ∃Z < +∞, it holds that
g(θn +1)-g*	λ ,
-----------≤ Z < +∞ a.s.,
Sn+1
which g* = infθ∈RNg(θ).
Lemma 15 Suppose {θn } is a sequence generated by AdaGrad in equation 5 subject to Sn =
∑ £ =ι INθkg (θk, ξ k )k2 = +∞ a. s.. Under Assumptions 1 and 5, ∀ n ∈ N+ ∀θι ∈ RN, ∀εo ∈ (0,3), it
holds that
Mng(θn升2 → 0 a.s..
Sn-1
Lemma 16 Suppose that {Xn} ∈ RN is a vector sequence and f (x) ∈ C1 is a monotonically non-
increasing non-negative function with Ra+∞ f(x)dx < ∞ (∀a > 0). Then ∀N ∈ N+, it holds that
N 2 n 2	∑kN=1 kXkk2	N 2 n-1 2
∑ kXnk2f ∑ kXkk2 <	2	f(x)dx < ∑ kXnk2f ∑ kXkk2
n=1	k=1	kX1k2	n=1	k=1
C.1 Proof Outline of Theorem 3
Like the proof of mSGD, the proof for AdaGrad is also in light of the Lyapunov method. We aim to
prove Vg(θn) → 0 a.s., and then to get θn → J* a.s. The key step to prove convergence of AdaGrad
is to show
-E ( OcVθng (θn, ξ n )T Vθng (θn, ξ n )
I	√Sn
Fn	≤ 0.
but the learning rate of AdaGrad is a random variable and it is not conditionally independent of
Vθng(θn,ξn), meaning that
E ( α0vθng(θn, ξn)T vθng(θn, ξn)
[	√sn
=-"α0=llvθng(θn)l∣2 ≤ 0.
Sn
In the following, we provide the proof outline of Theorem 3.
Step 1: This step is to ensure
_E ( α⅜vθng(θn, ξn)Tvθng(θn, ξn)
-{	√sn
Fn	≤ 0.
We are able to obtain the following equation
vθng(θn, √)Tvθng(θn) = _L ll -ɪ vθng(θn, ξn) - √M+1vθng(θn)
Sn	2	Sn	M + 1
'-------------------{z-------------------
(K)
-M⅛llvθng (θ" )ll2- 2( M +11)√Sn lvθng (θ" , ξn )ll2.
'-----------------------------}
z
(L)
35
Published as a conference paper at ICLR 2022
For (K), due to Sn-1 ≤ Sn, it follows that
1
1
2
(K)
2 VSn Il √M+1
Vθng(θn, ξn) - √M+1Vθng(θn)


2
Vθng(θn, ξn) - √M+1 Vθng(θn).
≤ 2√Sn-111 √m+1
Note that Sn-1 is conditional independent on Vθng(θn, ξn), thus it holds that
2
E
√M+1
Vθng(θn, ξn) - √M+1Vθng(θn)∣∣ ∣%
IIVθng(θn,ξn)II2Fn+ (M -1)IIVθng(θn)II2 .

Next We prove (K ) can be controlled by (L) according to Lemmas 11 and 12. These tWo lemmas
deal With the cases of ∣Vθng(θn)∣ ≤ a and ∣Vθng(θn)∣ > a, respectively. In these tWo lemmas, We
introduce a constant ε for the reason as stated in Step 2 in the following.
Step 2: Through Lemmas 11 and 12, we obtain that
+∞	+∞
∑ kVkg(θk)k2/sk-2ι+ε < ζ + ∑ IIVkg(θk,ξk)k2/Sk+2ε
k=3
k=3
From Lemma 16, We obtain Lemma 13, stating that ∑+=3 ∣∣Vkg(θk, ξk)∣∣2/Sk+2ε ‹ +∞. Note that if
We do not introduce ε, the term ∑ +∞3 ∣∣V+g(θk, ξk)∣2/S1+2ε will become ∑+∞, ∣∣V+g(θk, ξk)∣2/Sk =
O(ln Sn), and this term may not be bounded.
Step 3: Lemma 13 ensures that Vθng(θn) has a subsequence satisfying Vθk g(θkn) → O a.s. By using
the recursion formula g(θn+1) - g(θn) ≤ 1/Sn +Pn, Where ∑n+=∞1 Pn < +∞ a.s., and Lemma 4, We
obtain Vθng(θn) → O a.s. Consequently, θn → J* .
C.2 Proof of Lemma 10
For ∀x ∈ RN , we define function
g(t) = f x+ t
x0 - x
kx0 - xk ,
where x0 is a constant point such that x0 - x is parallel to Vf (x). By taking the derivative, we obtain
g，( t )=v x+1M Λx+1
x0 - x T x0 - x
kx0 - xk	kx0 - xk .
(78)
Through the Lipschitz condition of Vf(x), we get ∀t1, t2
I g '(t ι)- g'(12)1 = Ivx+1" f∖x+11
kx0-xk
x0 - x
kx0 - xk
-VX ItXx-x f ( X + 12
x+1F-4	∖
kx0 - xk
T
x0 - x
x0 - x
kx0 - xk
一Vr It Xx-x f ( X + 12
X+1F-4	∖
x0 - x
x0 - x
kx0 - xk
kx0 - xk
≤ c|t1 - t2 |.
≤
Vrj上二 f∖x + 11
x+1F-I	∖
So g0(t) satisfies the Lipschitz condition, and we have inft∈Rg(t) ≥ infx∈RN f (x) > -∞. Let g*
infx ∈Rg(x), then it holds that for ∀ t0 ∈ R,
g(O)-g* ≥ g(O)-g(to).
(79)
By using the Newton-Leibniz’s formula, we get that
OO	O
g(O) -g(tO) =	g0(α)dα =	g0(α) -g0(O) dα+	g0(O)dα.
tO	tO	tO
36
Published as a conference paper at ICLR 2022
Through the Li pschit z cond it ion of g0, we get that
g(0)-⅛(tO) ≥ / -dα-0|dα + / g'⑼dα = 21c(g'⑼)2.
Then we take a special value of t0. Let t0 = -g0(0)/c, then we get
g(0) — g(10) ≥ - Z C∣α|dα + Z g(0)dt = -c(0-10)2 + g,(0)(-10)
t0	t0	2
=-2C (g'(0))2 + C (g'(0))2 = 2c (g'(0) 广
Substituting equation 80 into equation 79, we get
g(0)-g* ≥ 2c(gz(0))2.
DUe to g* ≥ f * and (gz(0))2 = IIVf (x)k2, it follows that
IIVf (x)∣∣2 ≤ 2c(f (x) - f*).
C.3 Proof of Lemma 11
First of all, it follows from Lemma 3 that
g(θn +1) - g(θn) ≤ Vθng(θn)T (θn +1 — θn) +	"' ""
2	Sn
=-P⅛Vθng (θn )T Vθng (θn, ξ n ) + CO2 ∣∣vθllg (θn, ξn )∣∣2
=	√Sn	+ ^2	Sn ，
where
n
Sn = ∑ ∣∣Vθng(θn,ξn)∣∣2.
k=1
Note that
∣ -M1+= Vθng (θn , ξn ) -√M+1 Vθng (θn ) 2
=M+1 ∣∣Vθng (θn, ξ n )∣∣2 + (M + 1)∣∣Vθng (θn )∣∣2 - 2Vθng (θn ) T Vθng(θn, ξ n ),
(80)
(81)
(82)
where M = M0 +2 and M0 is defined in AssUmption 5 3). SUbstitUte eqUation 82 into eqUation 81,
then we get that
g(θn+1) -g(θn)
≤ - O0 1 _J_ ∣∣Vng (θn, ξn )∣∣2 + (M + 1) ∣∣Vng (θn )∣F ∖
—2 MM +1	-Sn	I )历)	(83)
+ α -• ∣∣ G Vθng (θn, ξ n )-√MHΓVθng (θn )∣∣2 + W J⅛誓址.
2 Sn M + 1	2	Sn
DUe to Sn ≥ Sn-1, it follows that
α -S ∣∣ √⅛ vΘ"g (θn, ξn )-R vΘng (θn )∣∣2	2	(84)
≤ α0 -⅛=1 H √M+1 vθng (θn , ξn ) -√M+1 vθng (θn )∣∣2.
37
Published as a conference paper at ICLR 2022
Substitute equation 84 into equation 83, then we have
g(θn+1)-g(θn)
α0
≤--
-2
1 _J_ Mn以θn, ξn升2 + (M + 1) Mng(θn升2
M+n √sn —+(M+1) ―√sn―
(85)
+
华 √sn- Il √m+i vθng (θn, ξ n)- √M+1vθng(θn) Il +
Ca0 Il Vθng(θn, ξn)∣∣2
2	Sn
Notice that
华 √sn-ι ∣l √m+i vθng (θn, ξ n)- √M+1vθ总。n )ll
=α -^= ( &T∣∣Vθng (θn, ξn )ll2 + (M + 1)∣∣Vθng(θn )『-2Vθ.g (θn, ξn ) T TV“g (θn ))
2	Sn-1	M + 1
=α -SI=T ( M1+1 I∣Vθng (θn , ξn )ll2 + (M + 1)∣∣Vθng(θn )∣∣2-2∣∣Vθng (θn )ll2)
+ -1= vΘng(θn)t(vΘng(θn) - vΘng(θn, ξn)).
Sn-1
(86)
Substitute equation 86 into equation 85, and divide both sides of the inequality by Sn (ε < 1), then
we get
g (θn +1 )	g (θn )
-------
εε
Sn	Sn
α0
≤--
-2
1 _J_ IlVng(θn , ξn )ll2 + ( M + 1) llvθng (θn )∣∣2
Ml	S产	+( +)
(M⅛! I∣Vθng(θn , ξn )ll2 + (M + 1)I∣Vθng (θn )ll2- 2∣∣Vθng(θn )ll2)
+ co2 I|V弋 εξ" )ll + ɪ Vθng (θn ) T (Vθng (θn ) - Vθng (θn, ξ n )).
Notice that g(⅛1) > g(θ +1), then We obtain
Sn	Sn+1
g (θn +1 )	g (θn )
---二----—--------
Sε+1	Sε
1 J llvθng(θn, ξn )ll2 +(M + 1) Ilv 丝(θn )||2
1 M + 1	Q2 +n	Q1 +n
Sn	Sn
α0
≤ —
-2
+ --1+^7 vθng (θn ) T (vθng (θn ) - vθng(θn, ξ n )).
+
2	S产
(87)
38
Published as a conference paper at ICLR 2022
Rearrange the above inequality, then it holds that
g (θn +1)	g (θrl)
----二----—---------
Sε+1	Sε
<―00 (M + 1)心粤则2
一 2 '	/	S2+ε
Sn
+ 00 (_J_ Mng(θn, ξn)『+ (M	Mng(θn)『α +
2 IM +1	4+ε	〈 J	d+£
2
c1+ε
Sn
+X沪
O0 (M + 1)
l∣Vθn—1 g(θn —1)∣∣2	∣∣Vθng (θn )∣∣2
----------:----------------------:--------
(88)
s 2+ε
Sn
+ 00	( _J_ ll Vθng(θn,	ξn	)∣∣2 + (M — 1)∣∣Vθng (θn )∣∣2 -	(M + 1)∣∣Vθn ―1 g (θn —1 )∣∣2
2 M M +1	c2+ε	q⅛+ε	c2+ε
+χnε.
Xz(ε) is defined as follow
Xne) = i~T~v vθng (θn 产(vθng (θn ) v vθng (θn, ξn ))∙
Dueto ∣∣Vθng(θn)∣∣2 > a, we have
E (∣∣Vθng (θn, ξ n ) ∣∣2IFn -1) < M∣∣Vθ.g(θn ) ∣ ∣ 2 + a
< (M +1) ∣ ∣ Vθng(θn)∣ ∣ 2,
Moreover, using the Taylor formula, we obtain
∣∣Vθng (θn )∣∣2 =∣∣Vθn-1 g (θn-1) + (Vθng (θn ) — Vθn-1 g (θn-1))∣∣2
= ∣∣Vθn-1 g (θn -1)∣∣2 + 2Vθn -1 g (θn -1 )T(Vθng (θn ) — Vθn-1 g (θn-1))
+ ∣∣Vθng (θn ) - Vθn -1 g (θn-1)∣∣2 < ∣∣Vθn-1 g (θn-I)^
+ 2 ∣ ∣ Vθn-1 g (θn-i) ∣ ∣ ∣ ∣ Vθng (θn ) - Vθn -1 g (θn-1)∣∣ + ∣∣Vθng (θn ) - Vθn -1 g (θn-1)∣∣2.
Under Assumption 5 3), We get that
∣∣Vθng(θn)∣∣2 <∣∣Vθn-1 g(θn-1)∣∣2
+ 2 ∣ ∣ Vθn-1 g (θn-i) ∣ ∣ ∣ ∣ Vθng (θn ) - Vθn-1 g (θn-1)∣∣ + ∣∣Vθng (θn ) - Vθn -1 g (θn-1)∣∣2
< ∣∣vθn-1g (θn-1)∣∣2 + √^-∣∣vθn -1g (θn-1 )∣∣∣∣vθ“-1 g(θn-1, ξn-1)∣∣
Sn-1
+ c 2 α 2 ∣∣vθn-1 g (θn-1, ξ n-1)∣∣2
0	Sn-1	■
(89)
(90)
From inequality 2aτb < λ ∣∣a∣∣2 + 11∣b∣∣2 (λ > 0), it follows that
(M -1) ∣ ∣ Vθng (θn) ∣ ∣ 2 + ∣∣Vθng (θn )∣∣2
< (M + 1) ∣ ∣ Vθn-1 g (θn-i) ∣ ∣ 2 - 4M-j3 ∣∣Vθng (θn )∣∣2 + 4M2 诏 C2 ∣∣vθn T g (Sn-1, ξ n-1" .	(91)
4M - 3	Sn-1
By substituting equation 89 into equation 91, we have
(M - 1) ∣ ∣ Vθng (θn ) ∣ ∣ 2 + M+1 E (∣∣Vθng (θn, ξn ) ∣∣[Fn-i)
2	(92)
<(M +1) ∣ ∣ Vθn-1 g(θn-i) ∣ ∣ 2-M-1 ∣∣Vθng(θn)∣∣2+4M2琮C21∣ θnTg(Sniξn-1)∣∣ ,
4M - 3	Sn-1
39
Published as a conference paper at ICLR 2022
Divide both sides of equation 92 by s"： , and notice 4^--3 >
1 from M > 2, then it holds that
1	Il Vθng(θn, ξn)『+ Mng(θn)『
M + 1 f2+ε	c2+ε
Sn2-1	Sn2-1
≤ (M + 1) MnTg(θ;-1)∣∣ - 1∣∣VθBg(θn)∣∣2 + 4M2α2C2
2++ε	5
Sn-1
+ Z Ynε),
α0
MnTg(θn-1, ξn.1)『
Sn-1
(93)
where
Y (ε)=曳 1 _±_ E (llvθng (θn, ξ n	-1) - ɪ M“g (θn, ξ n )∣∣2
n 2 ∖ M +1	2++ε	M +1	8+ε
Sn 1	Sn 1
Making some simple transformations on equation 93 leads to
αo 1 J ∣∣Vθng(θn,ξn)∣∣2 + (M-1)∣∣Vθ“g(θn)∣∣2 - (M + 1)∣∣Vθn-1 g(θn-1)∣∣2
2 ∖ M +1	c2+ε	c2+ε	c2+ε
≤-α0 k丝付“肝 + 4M2a3c2
10	q2 +ε
Sn-1
∣∣Vθn-1 g(θ“-1, ξn-1)∣∣
2
+Yn(ε).
(94)
≤ αo (M+1)(∣∣v“-1 g1(θ“-1)∣∣
-2 V	Sn-1ε
(95)
Substitute equation 94 into equation 88, then we get
g (θn +1 ) g (θ“ )
--7---------
Sn+1	Sn
∣∣VθKg (θ“ )∣∣2!
S产)
ɑ0∣∣V“g (θ” )∣∣2 s,2~3.∣∣V“-1 g(θ“-1, ξn-1)∣∣2 , C α2∣∣V“g (θ”, ξ n )∣∣2
-10	S 2+ε	+ 4M α0C	Q	+ F	Sn+一
S“-1	S“-1
+X沪+Y⑹
It follows that
αo	∣∣vθ“g(θn升2-	αo	∣∣vθ“g(θ“)∣∣2	o⅛	∣∣vθ“g(θ“)∣∣2
------------------------------------------------------------
αo ∣V“g (θ“ )∣∣2 + αo 1∣∣vθ“ 一 g (θ“-1)∣∣2 - ∣v“g(e“ )∣∣2
20	Si^ε	20V	SI+2ε	SI+
ɑ ∣∣vθ“-1 g(θ“-I)∣∣2
Y	ao ∣∣Vθ“g(θ“)∣∣2	α√∣∣Vθ“-1 g(θ“-1)∣∣2	∣∣Vθ„g(θ“)∣∣2
≤1--------------------------
20	2++ε	20 \	2++ε	2++ε
S“2-1	S“2-2	S“2-1
(96)
40
Published as a conference paper at ICLR 2022
Substituting equation 96 into equation 95 yields
g(en+1)	g(en )
---7------	
sn+ι---Sε
≤ α (M+1)
Mn-1 g (θn-1)∣∣2	Mng(θn )∣∣2
---------:--------------------:-------
S 2+ε
n
αo Mng(θn)『+ α⅛ ( Mn-1 g(θn-1)∣∣2
20 J 20 ∖	g+ε
S1	S2
(97)
+ 4M2α3c2 k°"Tg(θn-1,ξ"-1)『
+ cα2 l∣Vθng(°n,ξn)11 + X(ε) + Y(ε)
2	Sn1+ε	n n
C.4 Proof of Lemma 12
First of all, dividing both sides of equation 81 by Sεn yields
g(θn +1) - 回 ≤-α°Vθng(θn)tVθng(θn, ξn) + 弱 IRθng(θn, ξn)『
Sn	Sn —	S 2+ε	2	Sn+ε
Sn
Due to Sn+1 ≥ Sn , it holds that
g(θn +1) - g(θn) ≤ - α0vθng(θn)Tvθng(θn, ξn) + 弱 Mng(θn, ξn)『
sε+1 - sn ≤-	sn→e	+ 2	sn+
(98)
(99)
Then we make some transformations to obtain that
α0Vθng(θn ) Vθng (θn , ξn )
S 2+ε
n
aVθng(θIt)TVθng(θn, ξn)
+α0Vθng(θn)TVθng(θn,ξn)
≤-α Vng(θn )T+nθng (θn, ξ n ) + Oθ( (M+1a + 瓯⅛1^Vng (θn )MVng (θn, ξn 升
2 ++ε	∖	2	2( M + 1) a	J
≤ - α0∣∣Vθng(θn)『+ (M + 1)O⅛a (_J_______L)
⅛2	∖ c2+ε	cj+ε I
Sn2-1	Sn2
+ ( 2aM+1) IVng (θn )『E (∣∣Vng (θn, ξ n )『|Fn-') Wi + ʌ 2+ B S L
Sn2-1
where
4 n ) = -⅜(llVθng (θn )II2 - Vθng (θn ) T Vθng (θIt, ξn ))
Sn-I
B n )= ɑŋ1"ng (θn 1)| (ll Vθng (θIt, ξn )∣∣2 - E (∣∣Vθng (θn, ξ n )∣∣2∣Fn-1
2 a (M + 1) Sn+n ∖	`	ι
Due to ∣∣Vθng(θn)∣∣2 ≤ a, we get
(100)
(101)
E∣∣Vθng(θn,ξn)∣∣2∣∣Fn-1 ≤ M∣∣Vθn g(θn)∣∣2 + a ≤ (M + 1)a.	(102)
41
Published as a conference paper at ICLR 2022
Substitute it into equation 100, then we get
g(θn +1)- g(θn) ≤ - α0∣RθKg(θn升2 + 0⅜a(M + 1) (_1_)
F - P ≤--ι+ε	L- 后)
+
α0∣Rθng (θn 升2
2 s 2+ε
2Sn-1
+ c02 l∣Vθng(θn,ξn"I	+ A(ε) + B(ε)
2 Sn1+ε	n n
α0∣∣Vθng (en )∣∣2 + αo a (M + 1)
2 Sn-：	2
(103)
+ 空2 IRθng(θn, ξ n )∣∣2
+ 2 Sn+ε
+A(nε)+B(nε).
We make some transformations on -
2 Sn2-：iε
to obtain that
α0∣∣Vθng(θn)∣∣2 / αo∣∣Vθng(θn)∣∣2 αo∣∣Vθng(θn)∣∣2
------;------≤----------:--------------:-----
2 Sn+	-	20 Sn-：	20 Sn-：
α0∣∣vθng (θn )∣∣2
1 +ε
20 sn2+
0o ∣∣vθn-1 g(θn-ι)∣∣2 + αo (∣∣vθn-1 g(θn-ι)∣∣2
2o —Ss ―	20I—SnS一
(104)
Then We Use inequality 2aTb ≤ λ∣∣ak2 + + kb∣∣2 (λ > 0) on equation 90 to get
∣∣vng (θn )∣∣2-∣∣vθn-1 g (θn-1)∣∣2 ≤ ∣∣v101 M θ 1))∣∣
+ 10α2 FM + I) ∣∣vθn-1 g (θn-1, ξn-1)∣∣2 + αc ∣∣vθn -1 g (θn-1, ξ n-1)∣∣
(105)
Z	C	.	<"1c1 +ε	1	. C	/C	/Cl	.
Divide both sides of equation 105 by Sn2-1 and notice Sn-2 ≤ Sn-1 ≤ Sn, then We have
∣∣vθng(θn)∣∣2 - ∣∣vθn-1 g(θn-1)∣∣2 ≤ _J_ ∣∣vθn-1 g(θn-1)∣∣2
S产	Sn-1ε	~m +1	10 Sn-2ε
10α0 S++1) ∣∣vθn -1 g (θn-1, ξn-1)∣∣2+α2⅛ ∣∣vθn-1 g(θn-1, ξn-1)∣∣2.
n-1
(106)
Then we calculate α20 (M + 1)equation 106 + equation 104
ɑc∣∣veng(θn)∣∣2 + αc(M + 1) (M“g(θn)∣∣2
2 Sn+	2 V	Sn+ε
∣∣vθn-1 g(θn-1)∣∣2
S 2+ε
Sn-1
ɑc∣∣veng(θn )∣∣2
1 +ε
20 Sn-：
+ 5α3C2(M + 1)2 ∣∣vθnTg(TI,ξn-1)∣∣2
+ ɪ ∣vn-1 g (θn-1, ξn-1)∣∣2 + 瑞(ɪ -
+
42
Published as a conference paper at ICLR 2022
Move a0(M+1)(卜θ^1fθn "I—— Mn t：(9n T)Il ) to the right-hand side of the above inequality, then
V	sk'	V )
we have
2 c 2+ε
2 Sn-1
MReng (en )『
1 +ε
20 sn-:
劭1仔9ng (θn )『
—
+ 5α0C2 * *(M + 1)2 I|VenTg(en-1,ξn-1)∣∣2
Sn+1
+
≡10^ IIVen -1 g (θn-1, ξn-1)∣∣2 + 瑞
ɔ q2+ε	20
2 Sn-1
IIvθn-1 g (θn-1)I∣2
S 2+ε
Sn-2
(107)
—
+ α0(M + 1) ( IVn-1 g (θn-1)II2 - IVng(θn )II2
2	∖	c2+ε	c1+ε
Sn2 1	Sn2
Substitute equation 107 into equation 103, then We have
g (θn +1) g (θn )
-----------------
Sε+1	Sn
a0 U vθng (en ) U
2
—+ 5af3 c 2( M + 1)2
IVn -1 g (θn-1, ξn-1)II2
(M + 1)a3 C2
2 S 2+ε
2Sn-1
Uvθn-1g (en-1, ξ n T)『+ 20
c1+ε
Sn-1
IIVθn -1 g (θn-1)II2 IIVθng (θn )II2
--------:--------------------------
的(M +1) (IVn-g(θn-1 )I2 _ IIvθng(θn)II2
S 2+ε
Sn-1
S 2+ε
Sn
O0 a (M + 1)
+-T-
(108)
≤ -
+
+
1+ε
20 S若
2
+
2	Sn+ε
+A户+ B户.
C.5 PROOF OF LEMMA 13
First of all, it holds that
g (θn +1)	g (θn )
---7----------
Sε+1	Sn
=/(II Veng(θn) II2 ≤ a) (gS^ -亭)+ /(II Veng(θn)『> a) (g^ -詈).
/(IIVθng(θn)II2 ≤ a) ∈ Fn-1 is the indicator function such that
1	IIVeng(en)II2 ≤ a
0	IIVeng(en)II2 > a
For convenient, we abbreviate /(IIVeng(en)『≤ a) as /≤a and /(IIVeng(en)『> a) as />a in the
folloWing.
/(IIVeng(en)II2 ≤ a) =
43
Published as a conference paper at ICLR 2022
Through Lemma 12, We get that
g g(θn+1)
l^+T
≤ -IF 叫 vθnfθIt 肝 + 5α0 C 2( M + 1)2 Im Mn T 华；；ξI)『
20 St+：	Sn-1
(M + 1)a,3 C2
2 S 2+ε
2Sn-1
∣∣Vθn-1 g(θn-1, ξn-1)∣∣2 + 居I≤a (限*-
20	∖	c2+ε	c2+：
+ 00(M + 1) I≤a
∣∣Vθn -1 g (θn-1 )∣∣2	∣∣Vθng (θn )∣∣
---------:--------------------:------
S 2+ε
Sn-1
j) + «0 a (M + 1) I ≤ a
+于 i≤ a ∣∣vθngS1+ξn)『+ i≤ aA nει+i≤ aB n).
Through Lemma 11, We get
(109)
I> a(g (θn +1 )
VSn+1
—
≤ _I>a «0 ∣∣vθng^n)∣∣2 + «0(M + 1) 4I>a
一 n 20	c⅛+ε	2 n
∣∣Vθn -1 g (θn-1 )∣∣2	∣∣Vθng (θn )∣∣2
---------:--------------------:-------
8+：
Sn-1
8+：
Sn
+ «0 i > a ( ∣∣Vn -1 g (θn-1)∣∣2 _ ∣∣vθng (θn )^
+ 20 n	c⅛+ε	c⅛+ε
(110)
2
+
4M 2αf3 c 217 a
Il vθn-1 g(θn-1), ξn-1∣∣
a ∣∣Vθng(θn, ξn)∣f
c1+ε
Sn
I 7>a 丫(：)_1_ 7>a y ⑻
+ In An + In n n ,
Calculate equation 109 + equation 110, then it holds that
+In a(gSn+1
S Sn +1
≤ 00(1 ≤ a +1> a )( Mn-1g (θn-I)UC - M"g(θ"
—20' n ”7	空+ε	空+：
I ≤ a ( g 但n +1)
n ∖Sn+1
—
—
+ α0 (M + 1)( I≤ a + In a)
∣∣Vθn-1 g (θn-1)∣∣2	∣∣Vθng (θn )∣∣2
---------:--------------------:-------
00 优+Ina)∣∣vθng(θn " + 4M Ma c C In a
20	c2+：
st+	)
II Vθn-1 g(θn-1), ξn-1∣∣2
+ 苧 I;a
II Vθng(θn, ξn)∣∣2
c1+ε
Sn
+ 5α0 c C(M + 1)21≤ α
S 2+ε
Sn-1
∣∣Vθn-1 g(θn-1, ξn-1)∣∣2
c1+ε
Sn-1
(111)
—
+1≤a (M + 1)O3CC
n	OV2+n
2 Sn-1
+ «0 a (M + 1) I ≤ a
I∣Vθn-1 g(θn-1, ξn-1)∣∣2

a
44
Published as a conference paper at ICLR 2022
Notice In<a < 1, then we get
O0 a (M + 1) I < a
2 n
V O0 a (M + 1)
<	2
(112)
Substitute equation 112 into equation 111, then we get
g(θn+1)	g(θn )
-----------------
Sε
∣∣Vθn-1 g (θn-1)∣∣2	∣∣Vθng (θn )∣∣2
---------:-------------------:-------
sn+1
/劭
≤ —
一 20
+α0 (M+1)
∣∣Vθn-1 g (θn-1)∣∣2	∣∣Vθng(θn )∣∣2
---------:--------------------:-------
翁	+ 4M 喝 C21 a
20	q2 +ε
Sn-1
S产)
∣∣VθK-1 g(θn-1), ξn-1∣∣
a ∣Vng (θn, ξn )∣∣2
c1+ε
Qn
+ 5αf3C2(M + 1)21≤a ∣∣vθnTg(θ"-1,ξn-1)∣∣2 +1≤a
(M + 1)a0 C2
劭 a (M + 1)
+—r—
2 s 2+ε
2 Sn-1
a ∣VKg (θn, ξn )∣∣2
c1+ε
Sn
∣∣Vθn-1 g (θn-1, ξ n-1)∣∣2
—
2
+
We make a summation of equation 113 to get
(113)
n
Σ
k=3
g (θk +1)
—
n
< α0 ∑
—20 Σ3
∣∣vθk-1 g (θk-1)∣∣2	∣∣Vθkg (θk )∣∣2
---------:--------------------:-------
n
+ 7(M +1) ∑
2	k=3
∣∣vθk-1 g (θk-1)∣∣2	∣∣Vθkg(θk )∣∣2
---------:--------------------:-------
-砥 Σ TH + 4M 2 就 C 2 Σ3 F a
Sk+ε	)
∣∣Vθk-1 g(θk -1), ξk-1∣∣2
应 S 户 a ∣∣vθkg (θk, ξ k )∣∣2
+ 2 ∑ Ik	S 1+ε
n
+ 5α0c2(M + 1)2 ∑ I<<
n
+ ∑ Ik 0
k=3
(M + 1)af3 C2
2 s 2+ε
2Sk-1
∣∣vθk -1 g (θk -1, ξ I)Il
k=2
2
∣∣vθk -1 g (θk -1, ξ k-1)∣∣2
S+
(114)
O0 a (M + 1) Λ
+	2	∑
k=3
@ S I<a ∣∣vθkg(θk,ξk)∣∣2
+ F ∑31	—SF—
n
+ ∑ (IkaaF) + IkaB Bkε)+IkK) + Ik飞⑹).
k=3
It follows from Lemma 16 that
∑ ∣∣vθkg(θk,羡)∣∣2 < [+∞ j_, = j_
∑	S 1+ε	< L	x 1+ε 0x = F Sε ,
k=3	Sk	S3 3 X	ε S3
(115)
45
Published as a conference paper at ICLR 2022
and
∑ Mθ Tg (S1+ε,ξ k-1)『≤ 厂夫 d=εSε,
k=3	Sk-1	S2 x	εS2
and
y IMk-ιg (θk-1，ξkT)Il / [+∞ 1 d	2
≤	≤≤ ddx
k=3	Sj2+	— k2 X 2+ε	(1 + 2ε) S 22+ε
Due to Ik≤a ≤ 1 and Ik>a ≤ 1, we get that
4M2α3C2 ∑ I>a I∣Vθk-1g(θkT)，ξk-1 ll + Cα ∑ I>a IR。kg(θk，ξk)||
k=3	Sk2-+1ε	2	k=3	Sk+ε
+ 5α0c2(M + 1)2 ∑ Ika IMk-1 g(a；+；，ξ"HF
k=3	Sk-1
+∑i≤ a (M+1)α3 c2 IMk-1 g (θk -1, ξk-1)II2+孚 ∑ik a Mθg∖ 1θkεξ ξ)『
k=3	2 Sk2-+1	k=3	Sk
8 M 2α3 c C	c a0	5α0 C 2( M + 1)2	α0 C 2( M + 1)	C α2
≤ (1 + 2ε)Sf+ε +轲 + -εSε — +(1 + 2ε)Sf+； + 啊 :=IK
(116)
(117)
(118)
Substituting equation 118 into equation 114 leads to
g (θn+1) g (θ3)
---7---- - -7--
Sn+1	S ε
≤ αo“M2g(θ2)II2	IMng(θn)II2! + αo(M+1∕IM2g(θ2)II2	IMng(θn)II2
—20 1 S1+ε	S2+ε	/	2	∖	S1+ε	S2+ε
- α0 ∑ IMkg(θk)II2 + 00a(M + 1) 1 _J__________匚! + K
20 k=3	Sk-1ε	2	ISf+ε	Sn+ε)
n
+ ∑ ik≤aA(kε)+ik≤aB(kε)+ik>aXk(ε)+ik>aYk(ε).
It is obvious that
αo"Mog(θo)II2	IMng(θn)II2! + αo(M +	1M2g(仇肝	M^θB升2
20 y	S2+ε	S2+ε	J 2	y	S 1+ε	S2+ε
+ αoa(M + 1) 1 _J___、+ K ≤ αo IRa2g(仇升2 + αo(M + 1) Um⅜g(仇升2
2	∖Sf+ε Si+ε)	2 20	S2+ε	2	Sf+ε
O0 a (M + 1)
+----17^
2 S 2 +；
+K:=L.
It follows that
g (θn +1) g(θ3)
----7--- - ----7--
Sn+1	S 3
≤-α00 ∑ IIvθkg(θθ)II + L + ∑ (IkaAk + IkaBkε)+1>N) + I>aY⑹).
20 k=3	Sk+1；	k=3
(119)
Note that {IkkaAk}, {IkkaBk}, {Ik>aXk} and {Ik>aYk} are all martingale difference sequences, thus
it follows that E ∑kn=1 (IkkaAk +IkkaBk +Ik>aXk + Ik>aYk = 0. Then we calculate mathematical
46
Published as a conference paper at ICLR 2022
expectation on equation 119
That is
E ( ∑ 心θkg(θk升2
∖Σ	S1 +ε
T E Σ3
+ L.
< 20 (*+ L) < +∞.
α0 S3ε
(120)
E (* — 等
From Lemma 6, it holds that
n
∑
k3
2
< +∞ a.s..
(121)
C.6 Proof of Lemma 14
It follows from equation 119 that
gθ+° ≤ 喑 + L + ∑ RA kε + Ik CaBlkε)+ Ik> aX(ε) + Ik aY(ε)).
Sn+1	S3	k=3
(122)
From equation 101, we obtain
Σ E (II I≤ aA 川 I2 ) ≤ Σ E (Ik a J⅛ (llvθng (θn )∣∣2-vθng(θB ) T vθng (θB , ξB ))
k=3	k=3	Sn-1
With inequality (a+b)2 ≤ 2(a2 +b2), 2aTb ≤ a2 +b2 (a, b > 0) and equation 120, we get
E (7≤ aS+2ε (l∣Vθng(θB )∣∣2- Vθng (θn ) T Vθng (θn, ξ n )),
≤2E
诟 IRθng (BB "F
S 1+2ε
SB-1
Ik≤a ∣∣VBng(Bn)∣∣2	+2E
α02Ik≤a
S 1+2ε
SB-1
Il VθBg(θn)∣∣[RθBg(θn, ξn)『
k2(M+2)E(α0 a∣∣vθyen )∣∣2!
≤ 2(M + 2)α2aE (α2a^：n(θB
< 40(M+2)α0a
+L	.
(123)
It follows from Lemma 5 that ∑kB=3 Ik≤a A(kε) is convergent a.s. Similarly, ∑kB=3Ik≤aB(kε), ∑kB=3Ik>aXk(ε)
and ∑kB=3 Ik>aYk(ε)are both convergent a.s. It follows that
n
Σ(IkkaA(kε)+IkkaB(kε)+Ik>aXk(ε)+Ik>aYk(ε)) <ξ0<+∞ a.s.,
k=3
g(θn+1) Ng(&1；'”,	“
≤ 飞一+L+ξ < +∞ a. s.
For convenience, let ξ = gθ) + L + ξ 0. Thus, it holds that
g (Bn +1)-g*
εB+1
< ξ < +∞ a.s..
(124)
47
Published as a conference paper at ICLR 2022
C.7 PROOF OF LEMMA 15
First of all, ∀ 0 < ε0 < 8, there is 0 < 3ε0 < 2. From Lemma 13, it follows that
∑∞ Mn-1 g(θn-1)『
n=4	S)2+4 S
< +∞ a. s..
(125)
It follows from equation 90 that
Mng(θIt )∣∣2 -∣∣Vθn -1 g (θn-1)∣∣2 <√α= (∣∣Vθn - g (θn-1)||2 +∣∣Vθn - g (θIt-1, ξ ”1)『)
a 2 C 2	2	n-1	(126)
+ S^-∣∣vθn -1g (θn-1, ξ n-i)∣∣ .
Sn —1
Divide both sides of equation 126 by Sε—1 and notice that Sn > Sn-1 > Sn-2, then We get
∣∣Vθng (θn )∣∣2 _ ∣∣Vθn-1 g (θn-1)∣∣2 ‹ a C∣∣Vθn -1 g (θn -1)∣∣2 + a C∣∣Vθn-1 g (θn-1, ξ n-1)∣∣2
SI	Sn-1	-	Si-0	Sn-ɛ0
+ 诟 C 2∣∣vθn-1 g (θn-1, ξ n-1)∣∣2
C1+ε0
Sn-2
Note that 0 < ∣ε° < ；, then it follows from Lemma 10 and Lemma 14 that
∣∣Vθn-1 g(θn-1)∣∣2 <2C(g(θn-1)-g*)
C	<	C
< 2 C ξ < +∞.
It follows that
∣∣vθng(θn)∣∣2 - ∣∣vθn-1 g但n-1 )∣∣2 < αCtξ + α0CMtξ + α2C2Mtξ + aCa
Sn0	Sn -1	- S 2+3 ε0	S 2+3 ε0	S 2+3 £。	S1+1 ε0
n-1	Sn-2	Sn-2	Sn-2	Sn-2
+ ^°+1 nɔ - ^~+ε^ ( E (∣∣ vθn -1g (θn-1, ξ n-1 )『卜n-2) - ∣∣vθn -1g (θn-1, ξn-1)『)
Sn2-23	Sn2-2
--1+^ ( E (∣∣ vθn-1g (θn-1, ξn-1)∣∣2∣Fn-2) - ∣∣vθn-1g (θn-1, ξn-1)∣∣2).
Sn-2
Thus, we have
∣∣Vθng(θn)∣∣2 _ ∣∣vθn-1 g(θn-1)∣∣2 V _ _ K
Sn— - < 开-K-1,
where
ζ = 00 Ct ξ (M + 1 + θ0 C) + θ0 C (a + α° C + 1)
Kn-1 =气(E (∣ ∣ Vθn-1 g (θn-1, ξn-1)∣fIFn-2)- ∣∣Vθ1-1 g (θn-1, ξn-1)『)
Sn-厂、
+ ɪ ( E (∣∣vθn -1 g (θn-1, ξ n-1) ∣∣2∣Fn-2)- ∣∣vθ1 -1 g (θn-1, ξn-1)∣∣)
Sn-2
(127)
(128)
It follows that
1 ) ≥ 1 E ( ∣∣vθn -1 g (θn-1, ξn-1)∣ ∣2 - M∣∣vθn -1 g (θn-1)『
c2+3 n0) a a ∖	c2+3 n0
Sn-2	Sn-2
48
Published as a conference paper at ICLR 2022
By Lemma 16, we have that
∑ 1 E (Mn-1 g(θn-1, ξn-l)∣∣2 !
∑ a ∖	sn+ε0	)
> lim 1E ( Z Sn-1 -11τ- dx) = lim —6—1 E (S2 -3 ε0 - S 21 - 3 ε0
n →+∞ a ∖Js2 x 2+3 εo ) n →+∞ 2 - 3ε0 a V n T 2
Due to Sn-1 → +∞ a.s., it follows that
∑ 1 E ]∣Vθn -1 g (θn-1, ξn-l)∣∣2
MaI	S⅛ε0
+∞.
(129)
From Lemma 13, we get
∑∞ 1E (⅛⅛<
n=4 a	V	Sn+ ε0
< +∞.
(130)
Combine equation 129 and equation 130, then
+∞
∑E
n=4
+∞ IMHVθn-1 g (θn-1,ξn-1)∣∣
∑a Ei—Q—
∑ 1 E MMI∣Vθn-1 g(θn-1)∣∣2
n=4a V	S⅛ε0
(131)
+∞
is divergent a.s. From Lemma 13, we get that ∑n+=∞4 E kKn-1 k2 < +∞ and Kn-1 is a martingale
difference sequence. Thus, ∑n+=∞2 Kn-1 is convergent a.s.. Combine equation 128, equation 131 and
Lemma 2, then we have
∣∣Vθng(θn)∣∣2 一C
ɪ-S0_L →0 a.s..
Sn-1
(132)
C.8 The proof of Theorem 3
We consider the proof under two conditions, namely, Sn < +∞ a.s. or Sn = +∞ a.s..
First, if Sn < +∞ a.s., from Lemma 13, We get that ∀ε ∈ (0,1), it holds that
∑ M^θθ)∣∣2 < +∞ a.s.
k=3	S 2 +ε
Thus, we conclude that
Mng(θn)∣∣2 → 0 a.s..
Due to Sn-1 < +∞ a.s., we get
∣∣Vθng(θn)∣∣2 → 0 a.s.,
(133)
49
Published as a conference paper at ICLR 2022
Second, if Sn = +∞ a.s., let ε → 0 on equation 113, then it holds that
g(Bn+1)—g(Bn )
, a。
< —
一 20
∣∣Vθn-1 g (θn-1)∣∣2	∣∣Vθng(θn )∣∣2
-----------------------------------
√S-2
√sn-r
+α0 (M+1)
∣∣Vθn-1 g (θn-1)∣∣2	∣∣Vθng(θn )∣∣2
------- ------------------- -----
ao ∣∣ vθng(θn)∣∣
20	√S-1
VS-
2
-+ 4M 2cc3 c2
5j
Il vθn 1 g(θn-1), ξn-1∣∣
3
S 2
Sn-1
2
2
-+ C诟
∣∣vθng(θn, ξn)∣∣2	(134)
Sn
a3C2(M + 1)2 ∣∣Vθn-1 g(θn-1,ξn-1)∣∣2 ] (M + 1)a0C2
3
2 Sn-I
I∣Vθn-1 g (θn-1, ξn-1)∣∣2
Sn-1
Co a (M + 1)
+-T-
+ InaAn + I≤aBn + InaXn + 1>aYn,
—
+
2
where
in aAn+InlaBn+In axn+In aYn=iɪm (In aA H+IFB H+In IaXS)+Ina
=% vθn (θn 尸(vθn (θn)--Vng (θn, ξn))
+
+
0⅛ng 1θ√S4a (∣∣Vng (θn, ξ n )∣∣2 - E (∣∣vθng (θn )『|Fn-1))
2a(M + 1)	Sn-1
θM⅛ √S= ( E ( ∣ ∣ Vθng (θn, ξ n ) ∣∣2∣Fn-1)- ∣∣Vθng (θn, ξ n )∣j
+ Ca Mr vθ”8 (θnng (θn)-vθng (θn, ξ n)).
Make some transformations on
Mng(θn ),ξn||
2
to obtain that
∣∣Vθng(θn, ξIt)∣∣2 < ∣∣Vθng(θn, ξIt)∣∣2
Sn
Sn-1

E (∣Rθng (θn ), ξ n∣∣2∣Fn-1)
Sn-1
< M∣∣Vθng (θn ) ∣∣2
一	Sn-1
+ ± (∣∣Vθng (θn, ξ n ) ∣ ∣ 2 - E (∣∣Vθng (θn ), ξ “『|Fn-1))
+ O----+ O----( ∣ ∣ Vθng (θn, ξ n ) ∣ ∣ 2 - E ( ∣ ∣ Vθng (θn ), ξn∣∣2∣Fn-1)),
Sn-1	Sn-1
where the last inequality is from Assumption 5 5). Let 0 < ε0 < ∣. It follows from Lemma 15 that
∣ ∣ VVng(θn) ∣∣ < δ < +∞. Then we have M∣∣ ∣,°n)∣∣ ≤ -mγ = -Mδ- ® = 2 一 ε0 ∈ (0,1)) and
Sl	S-1	Sl	SnMI
∣∣vθ"g (Sn, ξ n )∣ ≤ Mδ++a + sl (∣∣Vθng (θn, ξn )∣∣2 - E (∣∣Vθng (θn ), ξ”『|Fn-1))
n	Sn2	1 n-1
+ (Mδ + a)
1
C 2 +ε1
Sn
(135)
50
Published as a conference paper at ICLR 2022
Similarly, We have
Mn -1 g (θn-1,ξn-1 升2 ≤ Mδ + a 十(M δ 十 a)(	-	!
Sn-1	C2 +ε1	∖ C2 +ε1	C2 +ε1 I
Sn	∖Sn-2	Sn	J
+ S~^ (|| Vθn -1g (Sn-1, ξ n-1)||2 - E (∣Rθn-1g (θn-1), ξ n-1 ∣RFn-2))
Substitute equation 135 and equation 136 into equation 134, then we get
g (Bn +1)- g (Bn ) ≤ 卜 α0 H--0∖ + ) )(M δ + a )	----+ Pn + Qn,
Sn2	1
(136)
(137)
where
2
Qn= /(IMng (θn, ξn )||2 - E (||入ng (θn ), ξn∣∣2∣Fn J)
+ α0 c2M ；1)2 (∣∣Vθn -1 g (θn-1, ξn-1) I | 2 - E (∣∣Vθn -1 g (θn-1), ξn-1∣∣2∣Fn-2))
+ I≤aAn + 1≤aBn + InaXn + IraYZ .
Pn
51
Published as a conference paper at ICLR 2022
It follows from Sn → +∞ a. s. that
+∞	ɑ +∞
ZPn = 00 Σ
n=3	n= n=3
2
—
Mng (θn 升2
+ 也(M + 1)∑∞ (卜θn-1 g(θn-1)『_ Mng(θn升2 !
2' W k 店)
+ 4M2a3C2 ∑ Mn-1 g(θn_1),ξn-1『+ (M + :旃c Σ ∣∣Vθn_1 g(θn-1,ξn-1)∣∣2
n=3	S2 1	2 S2 1	n=3
n=	Sn_1	2Sn_1	n=
∑ O0 a (M + 1)	(	1	_ 1	ʌ	∑∞ α0 C 2( M +	1)2( M δ	+ a) ( _J__1∖
n=3	2	(√Sn-I √Sn J	n=3	2	∖Sj1 +：1	S>2+”
+∞
+ ~∑ , c α0 (M δ + a)
n=3
«0 (∣∣vθ2 g (θ2)∣∣2	l.
药］一不 1	n→m∞
+ 00 (M + 1)
∣∣ V⅜ g(θ2) ∣∣
SF)
∣∣ vθng (θn) ∣∣2!
√S-)
I2 Iinl ∣∣vθng(θn)∣∣2!	«0a(m+1)
--n →m∞	5尸—2—
- lim
n→+∞
+∞
+ 4M 203 c2 ∑
∣∣vθn-1 g(θn-1)，ξn-1∣∣2 l (M + 1)0t3C2
n=2
ι O0 a (M + 1) 1
+	2	√S2
3
S 2
Sn-1
3
2 Sn-I
+∞
∑l∣Vθn-1 g (θn-1, ξn-1)∣∣2
n=2
M δ + a + αf3 c 2( M + 1)2( M δ + a)
s 2+ε1
2 S J+ε1
+
From Lemma 15, We get that
lim
n→+∞
∣∣Vθng (θn )∣∣2
lim
n→+∞
2
0,
≤
and
lim ∣∣vθng (θn )∣∣2 ≤ lim ∣∣vθng(θn )∣∣2 ≤ lim ∣∣vθng(θn )^
n →+∞	√sn	- n →+∞	√Sn -1	— n →+∞	sin-1
It follows that
+∞
∑ Pn
n=3
«0 ∣∣vθ1 g (θ1)∣∣2 l «0/“，"∣vθ1 g (θ1)∣∣2 , «0 a(M + 1) 1
20	√0	+ 万(m +1)	√1	+ ―2 — √SΓ
+4m 2« c2 ∑∞∣∣vθn-1g (θn-1), ξ n-1∣∣2 +(M+?«3(C ∑∣∣vθn -1 g (θn-1, ξn-1)∣∣2
n=2	SZI	2 SZI n=2
00 a (M + 1) 1 M δ + a	α3 C2 (M + 1)2( M δ + a)
2	√S2^	s 2+ε1	2 S 2 +ε1
(138)
By Lemma 16, we have
4M2o3C2 ∑∣∣vθn-1 g(θn-1),ξn-1∣∣2 +∑∞ (M + Ii)若Cc ∣∣vθn-1 g(θn-1,ξn-1)∣∣2 < +∞.	(139)
n=3	Sn-I	n=3	2Sn-I
52
Published as a conference paper at ICLR 2022
Thus, ∑n+=∞2 Pn is convergent. In addition, it holds that
+∑∞E ∣∣In≤aAn ∣∣2∣∣∣Fn-1)
n=3
≤ +∑∞ E∣∣An ∣∣2∣∣∣Fn-1) ≤ +∑∞E
n=3	n=3
+∞
≤2∑
n=3
α2∣Rθng (θn 升
Sn-1
4	+∞
+2∑E
n=3
Vθng (θn ) T (Vθng (θn ) - Vθng (θn, ξ n ))I∣2∣Fn-1
∣∣Vθng(θn)∣∣2 ∣∣Vθng(θn,ξn)∣∣2∣∣Fn-1	.
It follows from Lemma 15 that ∃0 < ε0 < 8, such that
..∣∣Vθng (θn )∣∣2	C
n 1→m∞	Sε^	=0 a∙s.,
meaning that there is an almost surely bounded random variable δ0, such that
Il Vθng(θn) Il2 /2
ɪ-s⅛_- < δ < ∞ a.s.
Sn-1
It follows that
Mng (θn )『
Sn-1
< δ0
Mng (θn )∣∣
q2+(I-εO)
Sn1
2
a.s.,
Then we derive that
+∑∞E ∣∣In≤aAn ∣∣2∣∣∣Fn-1)
n=3
+∞
≤2∑
n=3
+∞
≤2∑
n=3
α2IRθng (θn 升
Sn-1
α2∣∣Vθng (θn )『
Sn-1
+∞
=2(M+1)∑
n=3
4	+∞
+2∑E
n=3
∣∣Vθng(θn)∣∣2∣∣Vθng(θn,ξn)∣∣2∣∣Fn-1
+ 2 Σ F^q^HvΘng(θn)l∣2(M11 vΘng(θn)∣∣2+ a)
n=3 Sn-1
α2IRθng (θn 升
Sn-1
4	+∞
+2∑
α 41 Vθng (θn )『
(140)
<2((M + 1)δ0 + a £ ⅞≡
n-1
n=3
2
Sn-1
< +∞ a.s..
From Lemma 5, ∑n+=∞3 In≤aAn is convergent almost surely. Similarly, we can prove other parts of
∑n+=∞3 Qn are also convergent almost surely. Thus, ∑n+=∞3 Qn is convergent almost surely. By summary,
we get
+∞
∑ Pn + Qn is convergent al most surely.
n=1
(141)
It is easy to find that Ji is a bounded closed set. So ∀ε > 0 we can construct an open cover Hε(i) =
{U(θ, ε)} (θ ∈ Ji) of Ji. Through the HeineBorel theorem, we can get a finite open subcover
{U(θk,ε) (k=0,1,...,n)fromHε(i). Then we assign Uε(i) =Skn=0U(θk,ε). Wecan get Uε(i) is a open
set. Under Assumption 5, J = {θ∣Vθg(θ)} has only finite connected components Jι, J,…,Jm. So
infi6=jd(Ji,Jj) = mini6=jd(Ji,Jj). Let δ0 = mini6=jd(Ji,Jj). It follows from Lemma 4 that ∃ε0 > 0,
when d(θ , Ji ) < ε0, there is
kVθg(θ)k2 ≤ 2dg(θ) -gi∙∣,	(142)
where gi denotes g(θ) (θ ∈ Ji). Let C = min{ε0, δ0∕4} and construct uC(^1, U(2,…,Ucm). It is
obvious that∀ Uci), U(j)c (i = j), d(Uci), U(j)) > δ(√2, and ∣∣Vθg(θ)k2 ≤ 2c|g(θ) -gi| (θ ∈ Uci)).
53
Published as a conference paper at ICLR 2022
Since J is a bounded set, ∃N > 0, such that J ⊂ K (K is the closure of U(0,N)). Then we construct
a set M = K/ Sim=1 Uc(i) . Since Uc(i) is a open set and K is a closed set, we conclude M is a closed
set. Since ∣∣Vθg(θ)k is a continuous function, ∃θo ∈ M, ∣∣Vθ0g(θo)k = min»∈M ∣∣Vθg(θ)∣∣. Let
r= ∣Vθog(θo)k> 0.
Then we prove that ∀u > 0, θ ∈ K, ∃δ > 0, if ∣Vθ g(θ)∣ < δ, makes d(θ,J) < u. We prove it
by contradiction. Assume ∃u0 > 0, ∀δ1 > 0, ∃ θδ1 holds ∣Vθδ g(θδ1)∣ < δ1 and d(θ,J) ≥ u0.
We make δι = 1,1/2,1/3..., and We form a sequence {θι/n}. It is obvious that ∣∣Vθ]/ng(θ`/n) →
Ok. Since {Θi/n} is bounded, through the Accumulation point theorem, there exists a convergent
subsequence {Θi/kn}⊂{θI/n}. We defined θ(O) = limn→+∞ Θi/kn. Through the continuity of d(θ,J)
and ∣Vθ g(θ)∣, We get d(θ(0)) ≥ u0 and ∣Vθ (0) g(θ (0))∣ = 0. It is contradiction by the definition
of J. So ∀U > O, θ ∈ K, ∃δ > O, ∣∣Vθg(θ)k < δ, makes d(θ,J) < U (∃ι, makes d(θ, J) < U). And
furthermore, due to the continuity of g(θ), We can get ∀ε1 > 0, ∃ δ0 > 0, if d(θ,Ji) < δ0, there is
|g(θ) - gi| < ει. So combine these two consequences. We can prove ∀ει > O, ∃b > O, if θ ∈ Uci)
and ∣Vθ g(θ)∣ < b, there is g(θ) -gi < ε1.
Through equation 121 and Lemma 6 we get there is a subsequence {∣Vθkn g(θkn)∣2} of
{∣Vθng(θn)∣2} which satisfies that
n→lim+∞Vθkng(θkn)2 =O a.s..	(143)
Next we aim to prove limn→+∞ ∣Vθng(θn)∣2 = O. It is equivalent to prove that {∣Vng(θn)∣2} has
no positive accumulation points, that is to say, ∀eO > O, there are only finite values of ∣Vθn g(θ)∣
larger than eO. And obviously, we just need to prove ∀O < eO < r, there are only finite values of
∣Vθn g(θ)∣ larger than e. We prove this by contradiction. We suppose ∃O < e < a, making the set
S = {∣Vθng(θn)∣2} be an infinite set. Then we assign ε1 = e/8c and define o = min{b, e/4}. Due to
equation 143, we get there exists a subsequence {θpn} of{θn} which satisfies ∣Vθpng(θpn)∣ < o. We
rank S as a subsequence {∣Vmn g(θmn)∣2} of {∣Vng(θn)∣2}. Then there is an infinite subsequence
{∣Vming(θmin)∣2} of{∣Vmng(θmn)∣2} such that ∀n ∈ N+, ∃l, npn ∈ (mil, mil+1 ). For convenient, we
abbreviate {min} as {in}. And we construct another infinite sequence {qn} as follows
q 1 = max {n : p 1 < n < min{ml：mil>p1},∣∣Vθng(θn)∣∣ ≤ o},
q2 = min{n : n > q 1, ∣∣Vθηg(θn)∣∣ > e},
q2n-1 = max n : min{mil : mil > q2n-3} < n < min{ml : ml > min{mil : mil > q2n-3},
∣∣Vθng(θn)∣∣ ≤o},
q2n = min {n : n > q2n-1, ∣Vθng(θn)∣ > e}.
Now we prove that ∃NO, when q2n > NO , it has e < ∣Vθq2n g(θq2n )∣ < r. The left side is obvious
(the definition of q2n). And for the right side, we know ∣Vθq -1 g(θq2n-1)∣ ≤ e. It follows from
equation 5 that
2
kθn +1 - θn k2 = / ∣∣Vθng (θqn, ξn )∣∣2
Sn
≤ £ (∣∣Vθn-1 g(θn, ξn)∣∣2- E (∣∣Vθng(θn, ξn)∣∣2 Fn))
+ ST0- (M∣∣vθng (θn )∣∣2 + a ).
Sn-1
Through previous consequences we can easily find that
)∞ (袅(∣∣Vθn -1 g (θn, ξ n )∣∣2-E (∣∣Vθng (θn, ξ n )∣fFn )) + * MUVθn^"	)
< +∞ a.s..
Note that α2a/Sn-1 → O, a.s.. We conclude
∣θn+1 - θn ∣ → O a.s..	(144)
54
Published as a conference paper at ICLR 2022
ThrOUgh ASSUmPtiOn 5 2) We get ∣kvθn+1 g (θn +1)∣∣2 — ∣∣vθng (θn )∣∣2 ∣ ≤ ∣kVθB 十]g (θ„ +1)|| —
kvθBg(θn)k∣2 ≤ ∣∣Vθn+1 g(θn +1)— VθBg(θn)k2 ≤ d∣θn +1 — θnk → 0 a.s., So ∃N0, when n > N0,
there is ∣ ∣∣Vθn+1 g(θn+1)k2 — ∣∣Vθng(θn)k ∣ < r — e. Then we can get that when q2B > N0 + 1, there
is IIvθq2ng (θqn)|| ≤ IIvθq 2n - 1 g(θq 2 “-1)l + ^Nq2 ng(%2 n )1 - lVq2 n -1 g (θq2“-1)l| ≤ e + r - e = r -
That means that θq2n ∈ Um=1 U.(", so we can prove 3i0, such that θq2n ∈ U.(i0). And due to
∣∣Vθng(θn)k ≤ e < r (n ∈ [q2n—1,q2n)), We get Vk ∈ [q2n—1,q2n), 3ik, such that θn ∈ U.1(Q. Due
to ∣∣θn +1 — θn Il → 0 a. s., we know i 0 = ik (V j ∈ [ q 2 n—1, q 2 n)). For convenient, we sign i 0 = iq 2 n-1 =
...=iq2n-1 = iq2n. And then we can conclude that
IIvθ g (θn )∣∣2 ≤ 2 CI g(θn ) — giq2 n I ( n ∈ [ q 2 n —1, q 2 n])∙
Due to locally sign-preserving property, we get
∣∣vθg(θn)∣∣2 ≤ 2C(g(θn) — giq2n ) (g(θn) ≥ giq2n ) Or
llvθ g (θn )∣∣2 ≤ —2 C ( g (θn ) — giq 2 n ) ( g (θn ) ≤ giq2B)(B ∈ [ q 2 n —1, q 2 n])∙
Ways to dispose these two cases is same, so we just show how to prove the first case. We get
e - O < llvθq 2ng (θq 2 n )||2 - IVq? “_1 g (θq2n-i)||2 < 2，( g (θq2“ Agiq2 n ) TVq2 n-1 g (θq 2 n-i)||2
q q2 n - q 2 n—1-1	.
=(2 C	Σ	g (θq2n-1+ i +1) - g (θq2 n -1+i )J+ 2 c(g (θq2n-1 ) - giq2 n) - 11 vθq2n-1 g (θq2n-1 ) 1 1 .
From equation 137, we obtain
g (θq2 n -1+ i +1 ) - g (θq2 n -1+ i)
(2 C 3a2( M +	1)λ e 、	1	C
≤ (Cα0	+	2---J(Mδ	+ a)	1 +£]	. + Pq2n-1+ i +	Qq2n-1+ i
So there is
q2 n — q2 n -1 —1	L	q2 n — q 2 n-1 -1 /	、
e - 0 < Σ	^+--------+ Σ	(Pq2n-1+ i + Qq2n-1+ i)
i=0	S2 +ε1	∙	i=0	、	/
q2n-1+i
+ 2 C (g (θq 2 n-1 ) - gi 2 n-j - Uvθq2 n-] g (θq2 n-1 )^,
(145)
which
L = 2 C 卜 α2 + C 3 诟(M + 1) )(M δ + a).
Due to ∣∣Vθq2n-1 g(θq2n-1) ∣2 < o < b, so through equation 141, we get that g(θq2n-1) - gi2n-1 < e/8c.
Substitute it into equation 146. We get
q 2 nT q2 n-1 —1	]	e	q 2 nT q2 n-1 -1 /	、
Σ	F+- > ^Y- Σ	(Pq2n-1+ i + Qq2n-1+ i ) ∙
i=0	S2 +ε1 . 2L	i=0	'	J
q2n-1+i
(146)
Through equation 141, we know ∑B=1(Pn + Qn) is convergence almost surely. So we get that
∑q=n-q2nTT(P2n-1+ i + Qq2n-1+i) → 0 a.s. by the CaUChy's test for ConvergenCe. Combining
1/兄+：+i → 0 a.s., we get
q 2 n T q2 n-1 —1
∑
i=1
1
s 2+ε1
Sq2n-1+i
e	1	q 2 nT q2 n-1-1 /	、
> 2L - ɪ? -	Σ0	Wq2n-1+i + Qq2n-1+i)
Sq2 n-1	i =
e
→ 2L
a.s.,
(147)
55
Published as a conference paper at ICLR 2022
so there is
+∞ ( q2 n - q 2 n -1-1	1
∑	∑	*
n = 1 ∖	' =1	Sq n-1+i
+∞ a. s..
(148)
But on the other hand, we know |尸期 ɪ+ g(θq2n-ι+ i)k > 0 (> > 0)∙ Together with equation 121,
we get
+∞	n2n-n2n-1 -1
∑∑
n=1 ∖	i=1
1 JL
< -∑
0n=3
< +∞ a.S..
1
s 2+ε1
qn2 n-1+ i
2
1 +∞	n2n-n2n-1
< 0Σ1	Σ
-1 llvθn 2 n-1+ ig (θn2n-1+ i )l∣2
S 2+ε1
qn2 n -1+ i
(149)
It contradicts with equation 148, so we get that ∣∣Vθng(θn)∣∣ → 0 a.s.. Combining equation 133,
we get ∣Vθng(θn)∣∣ → 0 no matter Sn < +∞ a.S. or Sn = +∞. Under Assumption 5 1), it is safe to
conclude that there exists a connected component J* of J such that lim d(θn, J*)= 0.
n →∞
56