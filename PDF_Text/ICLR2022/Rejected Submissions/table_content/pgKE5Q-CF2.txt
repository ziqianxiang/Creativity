Table 1: RMSE result of NE-AECF and compared methods on MovieLens-100k dataset.
Table 2: RMSE result of NE-AECF and compared methods on MovieLens-IM datasetModel	ML-1MAUtoSVD (Zhang et al., 2017)	0.864AutoSVD++ (Zhang et al., 2017)	0.848BiasMF (Koren et al., 2009)	0.845NNMF (Dziugaite & Roy, 2015)	0.843LLORMA (Lee et al., 2016)	0.833DMF+ (Yi et al., 2019)	0.8321GC-MC (van den Berg et al., 2017)	0.832AutoRec (Sedhain et al., 2015)	0.831CF-NADE (Zheng et al., 2016)	0.829AutoRec w/SN (Yi et al., 2020)	0.8260NE-AECF (OUrS)	0.8252 Â± 0.0024items. Douban contains 136,891 ratings with density 0.0152 on a rating scale {1, 2, 3, 4, 5}. Flixstercontains 26,173 ratings with density 0.0029 on a rating scale {0.5, 1, 1.5, ..., 5}, which is a bit dif-ferent from the other three datasets. Among the training samples, 5% are used for hyperparametertuning.
Table 3: RMSE result of NE-AECF and compared methods on DoUban and Flixster datasetModel	Douban	FlixsterGRALS (Rao et al., 2015)	0.8326	1.245PMF (Mnih & SalakhUtdinov, 2008)	0.7492	0.9809sRGCNN (Monti et al., 2017)	0.801	0.926Factorized EAE (Hartford et al., 2018)	0.738	0.908GC-MC (van den Berg et al., 2017)	0.734	0.917GRAEM (Strahl et al., 2020)	0.7497	0.8857NE-AECF (OUrS)	0.7286	0.88166	ConclusionThis paper presented a novel collaborative filtering method called NE-AECF. We analyzed the gen-eralization ability for NE-AECF and showed that the element-wise neural network is useful in re-ducing the upper bound of the prediction error in CF. The theoretical analysis indicates that fillingthe unknown ratings with zeros can make the error bound tighter. It also provides a guideline tomake a choice between item-based autoencoder and user-based autoencoder. These theoretical find-ings were further validated by the numerical results, in which our NE-AECF has state-of-the-artperformance in terms of RMSE.
