Table 1: Reward table for Game 1	s2 = 1	s2 = 2si = 1	2	0si = 2	0	1Table 2: Reward table for Game 2Game 1: multi-stage prisoner’s dilemma The first example — multi-stage prisoner’s dilemmamodel(Arslan & YUksel, 2016) 一 studies exact gradient play for general SG settings. It is a 2-agentMDP, with S = A1 = A2 = {1, 2}. Assume that the reward for each agent ri(s, a1, a2), i ∈ {1, 2}is independent of state s and is given by Table 1. The state transition probability is determined byagents’ previous actions:P (st+1 = 1|(a1,t, a2,t) = (1, 1)) = 1 - , P (st+1 = 1|(a1,t, a2,t) 6= (1, 1)) =Here action ai = 1 means that agent i choose to cooperate and ai = 2 means betray. The state sserves as a noisy indicator, with accuracy 1 - , of whether both agents cooperated (st = 1) or not(st = 2) in the previous stage t - 1.
Table 2: Reward table for Game 2Game 1: multi-stage prisoner’s dilemma The first example — multi-stage prisoner’s dilemmamodel(Arslan & YUksel, 2016) 一 studies exact gradient play for general SG settings. It is a 2-agentMDP, with S = A1 = A2 = {1, 2}. Assume that the reward for each agent ri(s, a1, a2), i ∈ {1, 2}is independent of state s and is given by Table 1. The state transition probability is determined byagents’ previous actions:P (st+1 = 1|(a1,t, a2,t) = (1, 1)) = 1 - , P (st+1 = 1|(a1,t, a2,t) 6= (1, 1)) =Here action ai = 1 means that agent i choose to cooperate and ai = 2 means betray. The state sserves as a noisy indicator, with accuracy 1 - , of whether both agents cooperated (st = 1) or not(st = 2) in the previous stage t - 1.
Table 3: (Game 1:) Relationship of convergence ratio and .
