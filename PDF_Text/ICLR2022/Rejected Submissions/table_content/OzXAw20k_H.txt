Table 1: Comparison of methods to combine intrinsic and extrinsic rewards5.3	Auxiliary Tasks and Parameter sharingAs mentioned in Section 3.3, auxiliary tasks can accelerate learning, build a stronger representation,and therefore improve generalization. In UNREAL (Jaderberg et al., 2017), the task is learnt alongwith several prediction and control tasks such as reward prediction, pixel control and feature con-trol. In Lample & Chaplot (2017), the architecture learns to predict several features of the environ-ment along with the true task and this dramatically improves performance. Note that the auxiliarytasks are only meant to enrich the loss and rarely used to actually predict or control relevant ele-ments. In a different line of work, the UVFA framework (Schaul et al., 2015) introduces the agent’sgoal as input and effectively shares all network parameters between the different goal-conditionedvalue functions, which has been used in NGU to range from exploiting to very exploratory policies.
Table 2: List of Rainbow hyperparameters used, taken from the benchmarkThe list of used Rainbow hyperparameters can be seen in Table 2 above. The network architectureused for the Separate configuration is the Categorical DQN, as used in Dopamine (Rainbowwithout Dueling). The network has 3 Convolutional layers followed by 1 fully-connected layer,which we refer to as the ”vision module”, followed by a fully-connected output layer, which we call”control module”. The Exploiter has one more action per Explorer. Instead of keeping a separatenetwork for Exploiter and Explorers, the Shared and Multi architectures simply share the visionmodule between all agents and branch out a control module for each agent.
