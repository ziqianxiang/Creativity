Table 1: Bits-per-dimension (lower is better) of images in the corresponding evaluation sets for CIFAR10,ImageNet 32×32, and ImageNet 64×64. We also report the number of parameters in the models, and the timetaken to train (in GPU hours). All our models were trained on only one GPU.
Table 3: Ablation study across using Wavelet in eq. (6), and multi-resolution noise formulation in 3.4.
Table 4: auROC for OoD detection using -bpdS compares with -bpd for OoD detection. For different MRCNF	and S (Serra et al., 2020), for models trained on		models trained on CIFAR10, we compute the area under the	CIFAR10.		receiver operating characteristic curve (auROC) using -bpd and	CIFAR10	SVHN	TINS as standard evaluation for the OoD detection task (Hendrycks	(trained)	-bpd S-	-bpd Set al., 2019; Serra et al., 2020).	Glow	0.08 0.95	0.66 0.72	1-res CNF	0.07 0.16	0.48 0.60Table 4 shows that S does perform better than -bpd in the case	2-res MRCNF	0.06 0.25	0.46 0.66of (MR)CNFs, similar to the findings in Serra et al. (2020) for	3-res MRCNF	0.05 0.25	0.46 0.6616Under review as a conference paper at ICLR 2022Glow and PixelCNN++. It seems that SVHN is easier to detect as OoD for Glow than MRCNFs. However,OoD detection performance is about the same for TinyImageNet. We also observe that MRCNFs are better atOoD than CNFs.
Table 5: Unconditional image generation metrics (lower is better in all cases): number of parameters inthe model, bits-per-dimension, time (in hours). Most previous models use multiple GPUs for training, allour models were trained on only one NVIDIA V100 GpU. ^As reported in Ghosh et al. (2020). * FFJORDRNODE Finlay et al. (2020) used 4 GPUs to train on ImageNet64. ‘x’: Fails to train.
Table 6: Number of parameters for different models with different total number of resolutions (res), and thenumber of channels (ch) and number of blocks (bl) per resolution.
Table 7: FID v/s temperature for MRCNF models trained on CIFAR10.
