Table 1: Performance of benchmarks with 80 Million Tiny Images in AUROC, AAUROC, andGAUROC (Bitterwolf et al., 2020). Comparison to FROBInit without O(z). FROBInit refers to FROBw/o O(z), C10 to CIFAR-10, C100 to CIFAR-100, 80M to 80 Million Tiny Images, and UN to Uniform noise.
Table 2: Mean OCC performance of FROB w/ O(z), w/80FS OCC C10 (Sheynin et al., 2021).
Table 3: OoD performance of benchmarks w/ Outlier Dataset 80 Million Tiny Images in AUROC,AAUROC, and GAUROC. Comparison to FROB w/o the self-supervised learning boundary, O(z).
Table 4: OoD performance of benchmarks with 80 Million Tiny Images for Outlier Set, evaluatedon different sets in AUROC, AAUROC, and GAUROC. Comparison with FROBInit without (w/o)the boundary samples, O(z), as well as with FROB with (w/) O(z) boundary. Here, in this Table, C10refers to CIFAR-10, C100 to CIFAR-100, 80M to 80 Million Tiny Images, and UN to Uniform Noise.
Table 5: Robustness sensitivity analysis of FROBInit to the number of Outlier Datasets and FSsamples. OoD detection performance of FROBInit using FS outliers of variable number and anOutlier Set, evaluated on various sets in AUROC, AAUROC, and GAUROC. C10 refers to CIFAR-10,C100 to CIFAR-100, 80M to 80 Million Tiny Images, UN to Uniform Noise, and LFN to Low Frequency Noise.
Table 6: OoD performance of FROBInit w/o O(z) and w/ the Outlier Set 80 Million Tiny Images.
Table 7: Performance of FROBInit w/ the Outlier Dataset -all data- and w/o using our O(z).
Table 8: OoD detection performance of FROB trained on the normal class with an OE set, usingfew-shots, and using boundary samples, O(z), tested on different sets in AUROC, AAUROC, andGAUROC. We demonstrate FROB’s efficacy and efficiency in Sec. 4.2. Here, “w/ O” means withour learned OoD, O(z), while “w/o O” refers to without our self-generated anomalies, O(z). Here,C10 refers to CIFAR-10, C100 to CIFAR-100, UN to Uniform Noise, and LFN to Low Frequency Noise.
Table 9: OoD performance of FROB using an Outlier Set, using few-shots and our boundary, O(z),tested on different sets in AUROC, AAUROC, and GAUROC, where “w/ O” means with ourlearned self-implicitly-generated OoD samples, O(z). In this Table, C10 refers to CIFAR-10, C100 toCIFAR-100, 80M to 80 Million Tiny Images, UN to Uniform Noise, and LFN to Low Frequency Noise.
Table 10: OoD performance of FROB with an Outlier Set, using 1830 few-shots and our O(z),tested on different sets in AUROC, AAUROC, and GAUROC. We compare to FROBInit w/o O(z).
Table 11: Performance of FROB trained on normal CIFAR-10 using Outlier Set, using few-shots(FS) and learned boundary samples, O(z), evaluated in AUROC, AAUROC, and GAUROC.
Table 12: OoD performance of FROB trained with O(z), SVHN 100 few-shots, and Outlier Set.
Table 13: OoD performance of FROB trained with O(z), 80 SVHN few-shots, and Outlier Set.
Table 14: OoD detection performance of FROB trained on normal SVHN using an Outlier Dataset,few-shot outliers, and the self-generated boundary samples, O(z), evaluated on different sets inAUROC, AAUROC, and GAUROC. In this Table, “w/ O” means using the learned samples O(z).
Table 15: OoD performance of FROB with the boundary, O(z), in AUROC using One-ClassClassification (OCC) and FS of 80 CIFAR-10 OCC anomalies. Comparison with benchmarks inthis 80 few-shot setting (Sheynin et al., 2021). FwODS refers to FROB with (w/) Outlier Dataset SVHN.
