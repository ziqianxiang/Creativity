Table 1: DataSet StatiSticSDataset	Number of users	Number of posts	Average posts per userD1 DepreSSion	1,401	231,494	165D2 Non-depreSSion	1,401	1,119,080	799Total	2,802	1,350,574	482Experiment settings We trained our model using PyTorch 1.6 and torchtext 0.7 libraries with CUDAon an Nvidia P5000. All modelS have trained with Adam (Kingma & Ba, 2015) optimizer and croSSentropy function aS the loSS function. We perform grid Search to find the optimized hyperparameterSfor both SERCNN and baSelineS; the hyperparameterS include learning rate, hidden layer dimenSion,number of vocabulary, dropout, training epochS, early Stopping criteria, and batch Size are reportedin Table 2.
Table 2: HyPerparameter settingsHyperparameter	Search range	Optimizedlearning rate	{0.0005, 0.001,0.002, 0.01}	0.001hidden layer dimenSion	{100, 200, ... , 1000}	500number of vocabulary	-	100,000dropout	-	0.5training epochS	-	100early Stopping criteria	-	20batch Size	-	1For the choice of embeddingS for SE, we Select GloVe Twitter with 25 dimenSionS and GloVeWikipedia with 100 dimenSionS, empirically, aS deScribed in Section 3.1.
Table 3: Performance comparison against baselines					Model	Training data	Acc	Pre	Rec	F1Shen et al. (2017) MSNL	Handcrafted	0.818	0.818	0.818	0.818Shen et al. (2017) MDL	features	0.848	0.848	0.85	0.849Gui et al. (2019a) CNN	Text	0.843	0.843	0.843	0.844Gui et al. (2019a) CNN + PGA	(Hierarchical)	0.871	0.871	0.871	0.871Gui et al. (2019a) LSTM		0.828	0.830	0.828	0.828Gui et al. (2019a) LSTM + PGA		0.870	0.872	0.870	0.871Gui et al. (2019b) GRU + VGG + COMMA	Text + image	0.900	0.900	0.901	0.9001EHAN	Text	0.906	0.920	0.906	0.913SEHAN	(Hierarchical)	0.931	0.978	0.931	0.9541ELSTM	Text	0.900	0.921	0.900	0.910SELSTM	(Concatenated)	0.920	0.936	0.920	0.9281ERCNN		0.893	1.0	0.895	0.944SERCNN (10 posts)		0.784	0.809	0.764	0.770SERCNN (500 posts)		0.899	0.879	0.915	0.893SERCNN (all posts)		0.914	1.0	0.905	0.949Attention Network (HAN) that used the hierarchical document modeling approach in both singleembedding (prefix of 1E) and Stacked Embedding (prefix of SE) settings. 1E models are trainedwith pretrained GloVe Wikipedia 300 dimensions. Performances of the models are evaluated with
Table 4: Complexity analysis of SERCNN and baselinesModel	Sequential operationLSTM	O (N N)RCNN	O(N N3)Hierarchical LSTM^^Word level:^^O(N2N3)Post level:	O(N2)8Under review as a conference paper at ICLR 2022Complexity Analysis of SERCNN Table 4 reports the time complexity analysis of SERCNN withLSTM trained with concatenated posts and hierarchical LSTM (hierarchical document modeling),all LSTM trained are single Forward LSTM models. We use the notation we defined in Section 3,where N2 refers to the number of posts and N3 refers to the number of words. Since all three LSTMmodels run sequentially from the first word to the last word, the base sequential operation can bemeasured evaluated into O(N2N3). However, for the hierarchical LSTM, it incurs a higher com-plexity of O((N2)2N3) as it introduced an additional hierarchical structure at post level (O(N2)) tolearn the global context from the post context learned.
