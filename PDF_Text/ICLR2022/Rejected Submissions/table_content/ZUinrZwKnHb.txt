Table 1: Results on the COCO validation set. (res101, s16, i640) represents that we use ResNet-101;the output stride is 16; the input resolution is 640Ã—640. Refinement#1 represents only refining thekeypoints without filling. Refinement#2 represents refining the keypoints with filling.
Table 2: Results on the COCO test-dev2017 set. The methods marked with * use the multi-scaleinference settings. Our result is achieved based on ResNet-101 model with 16 output stride and 8002input resolution.
Table 3: Instance segmentation results (person class only) obtained with 20 proposals per image on the COCO validation set.												Method	AP	AP0.5	AP0.75	APSmall	APmedium	APlarge	AR1	AR10	AR20	ARsmall	ARmedium	ARlargePersonLab (res101, stride=8, input=1401)	33.8	56.0	36.8	-76-	45.9	59.1	15.6	37.0	38.3	8.0	51.4	68.0Ours (res152, stride=16, input=640)	20.7	43.5	16.9	0.3	24.5	59.0	12.9	29.4	30.3	1.0	36.1	68.5Ours (res101, stride=16, input=800)	22.0	45.3	18.8	0.9	27.7	55.3	13.2	30.8	32.0	1.8	41.1	66.9been downsampled 16 times w.r.t. the 6402 or 8002 input resolution while the reported PersonLabresult is based on 8 times downsampling w.r.t the 14012 input resolution. As shown in Table 3,our model performs worse on small and medium scales but achieves comparable or even superiorperformance on large scale persons even if PersonLab uses a larger resolution.
Table 4: Comparisons for different supervised layers on COCO validation set when using a smallproxy model.
Table 5: Results on COCO validation set when using shared self-attention and independent self-attention designs.
Table 6: Comparison between the body-first and part-first grouping algorithmIn Table 7 we compare our models with the mainstream bottom-up models, in terms of the numberof model parameters and computational complexity of the model forward pass. The results of Hour-glass (Newell et al., 2017), PersonLab (Papandreou et al., 2018), and HigherHRNet (Cheng et al.,2020) are taken from the HigherNet paper (Cheng et al., 2020). We can see that compared withthem, our models have fewer parameters and less computational complexity in the model forwardpass.
Table 7: Comparisons on the number of model parameters and model forward complexity.
