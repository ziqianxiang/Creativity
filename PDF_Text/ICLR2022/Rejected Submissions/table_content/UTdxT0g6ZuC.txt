Table 1: Time-series dataset corpus statistics and properties.
Table 2: Hit-at-k Accuracy (the higher the better) comparison of AUTOFORECAST against thedifferent baseline meta-learners for both univariate and multivariate testbeds. AutoForecastoutperforms all baselines for both testbeds.
Table 3:	Average rank (the lower the better) comparison of AutoForecast against the differentbaseline meta-learners for both testbeds. AutoForecast outperforms all baselines.
Table 4:	Results for one-step ahead forecasting (MSE) for both testbeds. The selected model byAutoForecast yields better performance (i.e., lower MSE) compared to baseline meta-learners.
Table 5: Pairwise statistical test results between AutoForecast and baselines by Wilcoxon signedrank test. Statistically better method (p = 0.05) shown in bold (both marked bold ifno significance).
Table 6: Inference runtime performance (in seconds) for both univariate and multivariate testbeds.
Table 7: A summary of our notation.
Table 8: Time-series meta-features for characterizing an arbitrary time-series dataset. We extractsa comprehensive number of meta-features. The extracted meta-features are five categories: simple,statistical, information-theoretic meta-features, spectral, and landmarker meta-features. To thebest of our knowledge, we emphasize that our landmarker meta-features are novel and that somecomponents of spectral meta-features have not been used in any related work.
Table 9: Time-Series Forecasting Model Space. See hyperparameter definitions for variousalgorithms from GluonTS (Alexandrov et al., 2020) and statsmodels (Seabold & Perktold, 2010).
Table 10: Univariate Time-SerieS dataSet corpUS deScription and detailS. The detailS of the dataSetS(i.e., the dataSet name, and nUmber of pointS in the dataSet) are ShoWn.
Table 11: Multivariate Time-series dataset corpus description and details (i.e., the dataset name, thevariate name, and the number of points for each variate of the dataset).
Table 12: Method evaluation in Univariate testbed (one fold is shown in the interest of space). Themost performing (lowest MSE) method is highlighted in bold. The rank is provided in parenthesis(lower ranks denote better performance). AutoForecast achieves the best average MSE andaverage rank among all meta-learners.
Table 13: Method evaluation in multivariate testbed (average MSE). The most performing (lowestMSE) method is highlighted in bold. The rank is provided in parenthesis (lower ranks denotebetter performance). AutoForecast achieves the best average MSE and average rank amongall meta-learners. In particular, it has the best performance (lowest average MSE) on 28 datasets outof the 40 multivariate datasets and has comparable performance for remaining datasets.
Table 14: Results for one-step ahead forecasting (Average MSE across all datasets) for both testbeds.
Table 15: Pairwise statistical test results between every pair of methods by Wilcoxon signed ranktest. Statistically better method shown in bold (both marked bold if no significance). In the left,univariate testbed is shown where AutoForecast is statistically significantly better than GB, AS,and AutoForecast-TSL. In the right, multivariate testbed is shown where AutoForecast isstatistically significantly better than AS, MLP, and AutoForecast-TSL.
Table 16: Computational cost for training (in seconds) for both univariate and multivariate testbeds.
