Table 1: Automated metrics for various models on COCO and ArtEmis 2562. “*” denotes scoresthat were not reported in the original paper, but computed using either official checkpoints (whenavailable) or by training the model using the official code. f and 去 denote Cho et al. (2020) andZhang & Schomaker (2020), respectively.
Table 2: Ablating different HyperCGAN modulation mechanisms on COCO 2562 for Style-GAN2 and INR-GAN. Our hypernetwork-based conditioning makes it possible to use word-levelconditioning, which is crucial in achieving good results. Note that adding DAMSM loss decreasesFID (i.e., overall image quality) dramatically for sentence-conditioned models.
Table 3: Visual Matching Results.	Semantic	Table 4: Extrapolation mean- ingfulness.	Table 5: Extrapolation align- ment with text.
Table 6: Effect of different choices of modulating tensors.
Table 7: Results on CUB dataset.
