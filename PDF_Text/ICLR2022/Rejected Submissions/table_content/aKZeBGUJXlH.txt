Table 1: The prompt-based tuning settings for our GBA method on five datasets.
Table 2: Performance of backdoor defense methods against four backdoor attacks evaluated usingthe attack success rate (ASR) in four popular downstream datasets. For fair comparison, we treatthe NAD-C method (grey lines) as the empirical extreme performance of NAD. The best results arein bold. Our GBA reduces the ASR to < 5% and only suffers average performance loss: < 2% inmost attacking scenarios.
Table 3: Our GBA performance on HS datasets with different prompt width.					Prompt Width	2	4	8	12	16ASR	1.07	1.55	1.42	2.22	1.92ACC	92.95	93.64	95.47	95.92	96.135.5	Further Exploration of GBAOne drawback of the distillation-based defense method NAD-C is the sacrifice of further general-ization ability. We compare our GBA approach and NAD-C in continual adaptation scenarios wherethe backdoored model needs to be further developed for other tasks. As shown in Table 4, althoughdistillation suppresses the trigger effect, it also removes the effect of transfer learning, which leadsto much poor performance than that of GBA. This confirms that the global gradient broadcast usedin our GBA defense commits little harm to the pre-trained knowledge and preserves an intact gen-eralization ability to further the development of pre-trained models.
Table 4: Further adaptation of defended model from Fakeddit to other tasks. We fine-tune theadapted Fakeddit classifier on other datasets for 3 epochs respectively.
Table 5: Trigger choice of our implementationVictim Pre-trained Model ∣	TriggersBERT	"cf”，"mn"，"bb"，"tq"，"mb"，"tn”RoBERTa	”Unintention"，"“(‘，’PraCtition"，"KinnikUman”，”(？,”，”〃广B.2	Backdoored SamplesDuring the training time, we use the most favorable settings for the attackers. Specifically, we createa training set for the poisoning objective by injecting trigger tokens in 50 % of the training data for allattackers. For every example in clean training data, the attacker can find its constructed counterpartcontaining a trigger.
Table 6: Hyperparameters used in BadnetsStage	BERT/RoBERTaOptimizer	AdamLearning Rate	0.00005Fine-tuning Batch Size	64Epoch	5Table 7: Hyperparameters used in NeuBAStage	BERT/RoBERTaOptimizer	AdamLearning Rate	0.00005Pre-training Batch Size	160Step	40,000Optimizer	AdamLearning Rate	0.00002Fine-tuning Batch Size	32Epoch	5B.5	RIPPLERIPPLe is a proof-of-concept algorithm for poisoning the weights of a pre-trained model (such asBERT, RoBERTa...) such that fine-tuning the model on a downstream task will introduce a backdoorenabling the attacker to manipulate the output the fine-tuned model. The attacking pipeline including
Table 7: Hyperparameters used in NeuBAStage	BERT/RoBERTaOptimizer	AdamLearning Rate	0.00005Pre-training Batch Size	160Step	40,000Optimizer	AdamLearning Rate	0.00002Fine-tuning Batch Size	32Epoch	5B.5	RIPPLERIPPLe is a proof-of-concept algorithm for poisoning the weights of a pre-trained model (such asBERT, RoBERTa...) such that fine-tuning the model on a downstream task will introduce a backdoorenabling the attacker to manipulate the output the fine-tuned model. The attacking pipeline includingfive stages:1.	Backdoor specification: The attacker decides on a target task and a backdoor they want tointroduce. Specifically the backdoor consists of a list of trigger tokens and a target class. Ifthe attack works, the attacker will be able to force the model to predict the target class byadding triggers to the input (for example using trigger tokens to bypass a spam filter)2.	Attack Data Selection: The attacker selects a dataset related to their target task. Ideally,
Table 8: Hyperparameters used in RIPPLeHyperParameters	Value~λ	0.1pre-train learning rate	2e-5pre-train epochs	5pre-train max steps	5000post-train epochs	3post-train learning rate	2e-5post-train batch size	256B.6	Embedding PoisonEmbedding Poison is a data-free backdoor attack method.In sentiment analysis and sentence-pairclassification tasks, the results show that this algorithm is efficient and concealed and does not loseaccuracy on clean datasets. It injects the model by modifying one single word embedding vector.
Table 9: Hyperparameters used in Embedding PoisonStage	BERT/RoBERTaOptimizer	AdamLearning Rate	0.00002Embedding Poison Training Batch Size	32Epoch	3Optimizer	AdamLearning Rate	0.00002Fine-tuning	Batch Size	32Epoch	3C Implementation of baseline defense methodFor NAD, which is a recent neural distillation method proposed by Li et al. (2021) , we implementa similar setting for transformer-based models. We first fine-tune the backdoored model on cleandatasets to get the teacher model, then a model from the backdoored checkpoint will serve as astudent model. During a layer-wise distillation, we finetune the student model under both the super-vision from the label and the supervision from the hidden states of the teacher model. We set thehyperparameter of β between [1000, 2000, 5000] to find the best defense results. Since NAD hasnot been applied to transformer-based model in NLP, we use a slight modified optimizer settings.
Table 10: Performance of backdoor defense methods against the BadNets attack evaluated using theattack success rate (ASR) in five popular downstream datasets with RoBERTa-base. Note that wetreat the NAD-C method as an upper bound of NAD. The best results are in bold. Our GBA reducesthe ASR to < 10% and only suffers average performance loss: < 5% in most attacking scenarios.
Table 11: Performance of defense methods against the BadNets backdoor attacks on the GLUE taskswith bert-base-uncased.Matt. and Pear. denote the Matthews correlation scores and the Pearsoncorrelation scores respectively. For classification tasks, our GBA reduces the ASR to: < 6 % inmost datasets. For regression tasks, our GBA reduces the ASRR to 0.01 %.
Table 12: Ablation Study on different components of GBA against BadNets Backdoor Attacks.
