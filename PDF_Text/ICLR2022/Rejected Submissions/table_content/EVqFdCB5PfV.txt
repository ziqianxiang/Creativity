Table 1: EM/F1 performance on HybridQA and QASPER. Runtime is measured by reruning their open-sourced codes (failed to rerun MATE). Numbers for QASPER are reported on the subset of extractive questions.
Table 2: EM/F1 performance on answer and sup-porting evidence on HotpotQA (dev set) with fullWikipedia pages as context. HGN Fang et al. (2019)and ETC Fang et al. (2019) are evaluated on the dis-tractor setting.
Table 3: Classification accuracy on ShARC-Longdataset. The Easy setting only checks the predicted la-bels, while the Strict setting additionally checks if allrequired evidences are retrieved. DISCERN is run withtheir open-sourced codes.
Table 4: Hits@1 accuracy on selecting sentences that actually contains the answer (on dev set).
Table 5: Accuracy of correctly predicting supporting facts for both hops on HotpotQA-Long (with-out reranking).
Table 6: Accuracy of selecting all required evidences on ShARC-Long.
