Table 1: Test performance comparison of optimizers. We report the mean and the standard deviations(as the subscripts) of the optimal test errors computed over three runs of each experiment. The SWDmethod enables Adam to generalize as well as SGD and even outperform complex Adam variants.
Table 2: Test performance comparison of optimizers with λL2 = λS = 0.0001 and λW = 0.1, whichis a common weight decay setting in related papers. AdamS still show better test performance thanpopular adaptive gradient methods and SGD.
Table 3: In the experiment of ResNet18 trained via SGD on CIFAR-10, we verified that the optimalweight decay is approximately inverse to the number of epochs. The predicted optimal weight decayis approximately 0.1 × Epochs-1, because the optimal weight decay is λ = 0.0005 selected from{10-2, 5 × 10-3, 10-3,5 × 10-4, 10-4,5 × 10-5, 10-5,5 × 10-6,10-6} with 200 epochs as thebase case. The observed optimal weight decay is selected from {Epochs-1, 0.1 × Epochs-1, 0.01 ×Epochs-1}. We observed that the optimal test errors are all corresponding to the predicted optimalweight decay λ = 0.1 × Epochs-1. At least in the sense of the order of magnitude, the predictedoptimal weight decay is fully consistent with the observed optimal weight decay. Thus, the empiricalresults supports that the optimal weight decay is approximately inverse to the number of epochs inthe common range of the number of epochs.
Table 4: Test performance comparison of Adai, AdaiS, SGD, and SGDS. Stable/Decoupled WeightDecay often outperform L2 regularization for optimizers involving in momentum. We report themean and the standard deviations (as the subscripts) of the optimal test errors computed over threeruns of each experiment.
