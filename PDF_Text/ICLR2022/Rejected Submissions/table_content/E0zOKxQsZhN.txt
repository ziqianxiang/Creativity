Table 1: The summary of selected POMDP subareas. For each subarea, we list the information of the hid-den state sh including its appearance in dynamics and reward function and its stationarity during one trajectory.
Table 2: How the prior work and our method implement the recurrent model-free RL as their ownmethod or baseline. We can see that none of the prior work share the same set of decision variables, some ofwhich have bad choices that may lead to the poor performance reported in the prior work. Our method coversa range of choices in these decision factors and finds the combinations in the last rows that lead to the bestperformance in terms of the average performance across the experimented environments in each subarea.
Table 3: Ablation results in our implementation of recurrent model-free RL. In this table, we show how asingle change in one decision factor from the variant that is best on average in that subarea, could significantlyincrease the performance. The first column shows how we change the single decision factor, and the lastcolumn shows the performance comparison between the best variant in that subarea (left) and the ablatedone (right). For robust RL and generalization in RL, we show the performance metric in worst returns andextrapolation success rates, respectively.
Table 4: Comparison between our method and specialized methods in system usage. The timecosts are evaluated within 1M environment steps. Both VRM and MRPO are run on CPUs andMRPO does not have a replay buffer (shown in N/A). VariBAD requires the assumption of fixedepisode length for the RAM cost.
Table 5: Hyperparameter summary in our implementation of model-free recurrent RL. Foreach subarea, we report the hidden layer size of each module, RL and training hyperparameters.
Table 6: Settings of the specialized methods we compared in the main paper. For Meta-RL, wetake the model on Cheetah-Vel as example.
