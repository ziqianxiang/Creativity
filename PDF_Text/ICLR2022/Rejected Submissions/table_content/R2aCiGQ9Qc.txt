Table 1: Real data: mean accuracy ± stdev over different data splits. Per GNN model, we report the best		performance across different layers. Best model per benchmark highlighted in gray. The "'"results (GraphSAGE)		are obtained from (Zhu et al., 2020).		Texas Wisconsin Actor	Squirrel Chameleon Cornell Citeseer Pubmed	Cora		Hom. level h	0.11	0.21	0.22	0.22	0.23	0.3	0.74	0.8	0.81	#Nodes	183	251	7,600	5,201	2,277	183	3,327	19,717	2,708	#Edges	295	466	26,752	198,493	31,421	280	4,676	44,327	5,278	#Classes	5	5	5	5	5	5	7	36	GGCN (ours) 84.86±4.55 86.86±3.29 37.54±1.56 5 5.17±1.58 71.14±1.84 85.68±6.63 77.14±1	.45 89.15±0.37 87.95±1	05	1.78GPRGNN	78.38±4.36 82.94±4.21 34.63±1.22 31.61±1.24 46.58±1.71 80.27±8.ιι 77.13±1	.67 87.54±0.38 87.95±1	18	5.56H2GCN*	84.86± 7.23 87.65 ±4.98 35.70±1.00 3 6.48±1.86 60.11±2.15 82.70±5.28 77.11±1	57 89.49±0.38 87.87±1	20	3.89GCNII*	77.57±3.83 80.39±3.4 37.44±1.30 38.47±1.58 63.86±3.04 77.86 ±3.79 77.33±1	48 90.15±0.43 88.37±1	25	3.56Geom-GCN* 66.76±2.72 64.51±3.66 31.59±1.15 38.15±0.92 60.00±2.81 60.54±3.67 78.02±1	15 89.95±0.47 85.35±1	57	6.11PairNorm	60.27±4.34 48.43 ±6.14 27.40±1.24 50.44±2.04 62.74±2.82 58.92±3.15 73.59±1	47 87.53±0.44 85.79±1	.01	7.78GraPhSAGEt 82.43±6.14 81.18±5.56 34.23±0.99 41.61±0.74 58.73±1.68 75.95±5.01 76.04±1	.30 88.45±0.50 86.90±1	.04	5.78GCN	55.14±5.16 51.76±3.06 27.32±1.10 53.43±2.01 64.82±2.24 60.54±5.3 76.50±1	.36 88.42±0.5 86.98±1	.27	6.56GAT	52.16±6.63 49.41±4.09 27.44±0.89 40.72±1.55	60.26±2.5 61.89±5.05 76.55±1	.23 86.33±0.48 87.30±1	.10	7.22MLP	80.81±4.75 85.29±3.31 36.53±0.70 28.77±1.56 46.21±2.99 81.89±6.40 74.02±1	.90 87.16±0.37 75.69±2	.00	6.781(Pei et al., 2019) claims that the ratios are 60%/20%/20%, which is different from the actual data splitsshared on GitHub.
Table 2: Model performance for different layers: mean accuracy ± stdev over different data splits. Per datasetand GNN model, we also report the layer at which the best performance (given in Table 1) is achieved. ‘OOM’:out of memory; ‘INS’: numerical instability. For larger font, refer to Table D.1 in the Appendix.
Table B.1: Ablation study: degree correction has consistent benefits (robust to oversmoothing & ability to handleheterophily) in different datasets while signed information has more benefits in heterophilous datasets. Bestperformance of each model is highlighted in gray.
Table B.2: Effects of using batch norm & layer norm: decrease in accuracy but imProvement in oversmoothing.
Table B.4: Cora: Accuracy and average effective一lhomophily (hli) for nodes with different degreesacross different layers. Last layer of initial stagemarked in gray.
Table B.5: The percentage (%) of nodes in each case (Fig. 2 and Thm. 1): case 1: hi ≤ ι+ρρ, case 2:hi > P- & r ≤ L I 1.--------and case 3: otherwise. The dominant case for each dataset is colored in grey.
Table B.6: Training and test time (m:minutes, s:seconds) for 10 runs. Shorter time is colored in grey.
Table D.l: Model performance for different layers: mean accuracy ± stdev over different data splits. Per dataset and GNN model, we also report the layer at which the best performance(given in Table 1) is achieved. ςOOM,: out of memory; ςINS,: numerical instability.
