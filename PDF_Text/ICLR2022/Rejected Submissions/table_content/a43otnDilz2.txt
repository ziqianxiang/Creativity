Table 1: Fine-tuning on GLUE. Following (Lee et al., 2019; Dodge et al., 2020), mean and varianceare computed for 10 seeds.
Table 2: Domain adaptation results: M (MNIST), MM (MNIST M), U (USPS), SV (SVHN), C(CIFAR10) and S (STL10). Results are averaged over 3 seeds.
Table 4: Experimental details of the experimentdepicted in Figure 1.
Table 3: Experimental details of first experi-ment in Section 3.1.1.
Table 5: Results of first experiment in Section 3.1.1 with Gaussian data. We provide the averageT ,	I 7 Gl 1 ,1	∙ ∙ 1 ,	FFF.,♦	1-1	∙	, 1 1 , ∙1	∙	∙ nɔ 1 1distance |h - h| and the empirical standard deviation. Experimental details are given in Table 3.
Table 6: Experimental details of the experimentresulting in the PDF in Figure 2 (left).
Table 7: Experimental details of the experimentresulting in the training depicted in Figure 2(right).
Table 8: Experimental details of the training depicted in Figure 3 (bottom).
Table 9: Architecture of the model used in the IB finetuning experiment. We use ReLU as an activation function.			Table 10: Experimental details on Informa- tion Bottleneck.				Parameter	ValueLayer type	Input shape	Output shape	Learning Rate Optimizer Warmup Steps Dropout Batch Size	See Appendix B.2 AdamWFully connected Fully connected	768 2304+K 4	2304+K 4 768+K 2		0.0 0.0 32				Table 11: Datasets from the GLUE as used in our experiments.
Table 11: Datasets from the GLUE as used in our experiments.
Table 12: Architectures used for the Unsupervised Domain Adaptation experiments. For the MInetwork of each method, we chose the best performing configuration between with or withoutLayerNorm layer and best activation between ReLU and tanh, using the validation set of MNIST-M.
