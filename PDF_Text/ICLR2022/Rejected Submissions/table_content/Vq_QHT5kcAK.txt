Table 1: Averaged metrics across 10 OOD datasets.
Table B1: Description of the datasets used in all the exp. R indicates real images, S - synthetic.
Table C3: In-domain performance on the test sets of CIFAR10/100 and SVHN for all the modelsused in the experiments (M = 11). We report mean and standard error over 5 random seeds for eachof the models. λM = 0 indicates Deep Ensembles (Lakshminarayanan et al., 2017). Standard errorsare reported if they are more than 0.01 across runs. DE indicates Deep Ensembles.
Table C4: CIFAR10 results. We report mean and standard error over 5 random seeds for each ofthe models. Standard errors are reported if they are more than 0.01 across runs. DE indicates DeepEnsembles.
Table C5: CIFAR100 results. We report mean and standard error over 5 random seeds for each ofthe models. Standard errors are reported if they are more than 0.01 across runs. DE indicates DeepEnsembles.
Table C6: SVHN results (averaged across 5 seeds)Architecture	OOD dataset	DE			Ours				AUC (↑)	AP(↑)	FPR95(Q	AUC (↑)	AP (↑)	FPR95(Q	bernoulli	1.00	0.99	0.01	1.00	1.00	0.00	blobs	1.00	0.99	0.01	0.99	0.98	0.02	cifar10	0.99	0.97	0.02	0.99	0.97	0.02	cifar100	0.99	0.96	0.02	0.99	0.96	0.04PreResNet164	dtd	0.99	0.94	0.02	1.00	0.98	0.01	gaussian	1.00	0.99	0.01	1.00	1.00	0.00	lsun	0.99	0.96	0.02	1.00	0.98	0.01	places	0.99	0.96	0.02	1.00	0.98	0.01	tiny imagenet	0.99	0.96	0.02	1.00	0.97	0.02	uniform	1.00	0.99	0.01	1.00	1.00	0.00	bernoulli	1.00	0.99	0.01	1.00	1.00	0.00	blobs	1.00	0.98	0.01	1.00	0.99	0.01	cifar10	0.99	0.95	0.02	0.99	0.96	0.03	cifar100	0.99	0.94	0.02	0.99	0.95	0.05VGG16BN	dtd	0.99	0.93	0.02	1.00	0.97	0.01	gaussian	1.00	0.99	0.01	1.00	1.00	0.00	lsun	0.99	0.95	0.02	1.00	0.99	0.01
Table C7: Test set results (in-domain perfor-mance) of our method on PreResNet8 trainedon MNIST with different ensemble sizes M .
Table C8: Out-of-distribution detection results of our method with PreResNet8 trained on MNISTwith different ensemble sizes M. We report the means over 3 random seeds for each of the models.
Table C9: Outlier exposure results for PreResNet164 trained on CIFAR100 with CIFAR10 as aweighting distribution for the diversity term. We found λM = 1 to be the best one for this experimentin the outlier exposure. The most optimal setting for our model was λM = 3 and λM = 5,respectively.
Table C10: Out of distribution detection for a small-capacity model - PreResNet20 (M = 11).
Table C11: Test set and OOD detection performances on PreResNet164 for ensemble sizes M ∈{3, 5, 7}. We report the means over 5 different seeds. Standard errors are reported if they are non-zeroacross the runs.
