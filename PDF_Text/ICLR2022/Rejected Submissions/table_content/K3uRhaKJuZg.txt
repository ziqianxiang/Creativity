Table 1: Comparison of the results of different models on the two quality types of FaceForensics++.
Table 2: Comparison of generalization results of different models, which train on the FaceForen-sics++ train set and test on Celeb-DF (v2) (AUC(%))._______________________________	Model		FF++	Celeb-DF (v2)Two-stream Zhou et al. (2018)	70.10	5380MeSOlnception4 AfChar et al.(2018)	83.00	53:60Two Branch Masi et al. (2020)	93.18	73:41Eficient-B4 Zhao et al.(2021)-	99.8	67:44ENST(Ours)	99.8	67.51 —In order to verify whether ENST has superior generalization in realistic scenarios that are not in-cluded in the train set, we put the model trained on FaceForensics++ on Celeb-DF (v2) for experi-ments on transfer learning, we use the fine-tuning model, i.e., freeze parts of the convolutional layersof the pre-trained model, train the remaining convolutional layers, and after the model converges,reduce its learning rate and fine-tune the layers. We randomly select 40 samples in Celeb-DF (v2)with FaceForensics++ to train the model to prevent catastrophic forgetting, and the results are shownin Table 2. The experimental results show that ENST has a better transfer capability and the abilityto learn new types of samples. It shows superior generalization on new types of data.
Table 3: Comparison of results using different loss functions in EfficientNet-B5 on c23 of the Face-Forensics++.	_____________________________________________Loss Function	AUC	AccArcface	88.15	84.48SCL	一	92.27	90.64Softmax + Arcface	91.52	87.86SOftmax + SCL	一	95.46	96.93Softmax+ Arcface + SCL	98.67	97.65On FaceForensics++, we compare the experimental results of using different loss functions whenextracting features in EfficientNet-B5, as shown in Table 3. First, we compare the classificationresults of Arcface and SCL individually, after which softmax is added to guide the improvement ofthe global classification. Finally the three losses are combined. The experimental results indicatethat the highest accuracy is achieved when softmax, Arcface, and SCL are integratively used.
Table 4: Comparison of results on the FaceForensics++ with the addition of the attention module atdifferent stages of EfficientNet-B5, where Xindicates the addition of the attention module in stagei of EfficientNet-B5. Stage 1 indicates after the first MBConv. Only the first five of the seven arelisted here. ___________________________________________________________________Stage	1		X	X	X	X	X	ɪ			X	X	-V-	-V-	丁				X	-V-	-V-	丁					X	-V-	亏						-V-AUC		96.35	97.82	98.53	98.67	98.55	95.63Acc		94.65	96.07	96.53	97.65	94.49	89.484.4.2 Comparsion of attentionOn FaceForensics++, we start from the previous of the EfficientNet-B5 structure and add attentionmodules at different stages, and the experimental results are shown in Table 4, which shows that theaccuracy increases gradually with the addition of attention modules. However, when continuing toadd at the 4th stage, the accuracy has slightly decreased, and when continuing to join in the 5th stage,the accuracy rate shows a significant decrease, which is consistent with our speculation. Our anal-ysis suggests that the artifacts are mainly concentrated in the shallow network of EfficientNet-B5,and the shallow feature extraction can provide enough critical information for the whole classifi-cation network to classify, while the deep feature extraction is of little significance. Furthermore,
