Table 1: English-French (top) and French-English (bottom) test BLEU throughout the few-shotself-distillation bootstrap across multiple model scales.
Table 2: English-French (top) and French-English (bottom) test BLEU throughout the bootstrapand after iterative backtranslation, this time using generations from self-amplified GPT-3 for thebootstrap. We observe the best performance by mixing in monotext from the English and Frenchcomponents of the CC100 dataset (Wenzek et al., 2020; Conneau et al., 2020) during backtranslation.
Table 3: Comparison of our best model—an xl distilled on self-amplified GPT-3 followed by 40rounds of iterative backtranslation—to prior work (Conneau & Lample, 2019; Song et al., 2019;Wang et al., 2021a; Keung et al., 2020; Nguyen et al., 2021) in unsupervised NMT on the WMT14English-French benchmark. Bold indicates unsupervised state-of-the-art and underline indicatesfew-shot state-of-the-art.
Table 4: English-French (top) and French-English (bottom) test BLEU using few-shot promptedsamples generated with temperatures τ = 0.0, 0.3, 1.0 throughout the bootstrap. We see that thetemperature used for sampling has little effect on evaluation BLEU after few-shot distillation, whilehigh-temperature samples are harmful during the backtranslation part of the bootstrap.
Table 5: Zero-shot versus few-shot self-amplified test BLEU for all model sizes studied in this paper.
Table 6: English-French (top) and French-English (bottom) test BLEU throughout the few-shot self-distillation bootstrap across multiple model scales, this time using real few-shot examples. We seethat performance after backtranslation is equivalent to that reported in Table 1.
Table 7: English-French (top) and French-English (bottom) test BLEU of the small and largemodels throughout the bootstrap and after iterative backtranslation, where for the bootstrap we usegenerations from 175B GPT-3 prompted using real few-shot examples. Similarly to Table 2, weobserve a boost in final BLEU score when, after the bootstrap, we additionally sample monolingualtext from the English and French portions of the CC100 dataset.
Table 8: BLEU scores (calculated over 4096 random training examples) for the few-shot promptedtranslations from a large model, as the total number of available few-shot examples varies fromN = 3 to N = 2048. We see that N has minimal impact on the BLEU score of the sampledtranslations. Moreover, the difference in BLEU between the models bootstrapped using N = 3versus N = 2048 disappears after iterative backtranslation.
