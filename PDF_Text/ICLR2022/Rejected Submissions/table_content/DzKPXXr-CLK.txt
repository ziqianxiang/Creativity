Table 1: Properties required for each algebraic structure.
Table 2: Accuracy on bigger analogy test set when We selected from the whole words.
Table 3: Accuracy on bigger analogy test set when we excluded a, b, c from the candidates.
Table 4: Accuracy of transfer test to Google analogy test set when we selected from the wholewords.____________________________________________________________________________num		WV	WV + MLP	WV + MLP-C	WV + AGNOverall	19544	4033 (20.64%)	1346 (6.89%)	2222 (11.37%)	5676 (29.04%)Semantic	8869	1995 (22.49%)	161 (1.82%)^^	256 (2.89%)	2260 (25.48%)Syntactic	10675	2038 (19.09%)	1185(11.10%)	1966 (18.42%)	3416 (32.00%)Only in the setting where we excluded a, b, c from the candidates in the Google analogy test set, theproposed method did not outperform WV. This is possibly because the word2vec model was highlytuned for the Google analogy test set for this evaluation method. Indeed, it has been pointed outthat word embedding algorithms are quite dependent on system design choices and hyperparametertuning Levy et al. (2015). We show the full results of all subcategories on each dataset in eachevaluation setting in Tables 11, 12, 13, and 14 in Appendix D.2.
Table 5: Accuracy of transfer test to Google analogy test set when we excluded a, b, c from thecandidates._______________________________________________________________________________________num		WV	WV + MLP	WV + MLP-C	WV + AGNOverall	19544	14382 (73.59%)	1427 (7.30%)	2414 (12.35%)	11857 (60.67%)Semantic	8869	6482 (73.09%)	163 (1.84%)^^	256 (2.89%)	4918 (55.45%)Syntactic	10675	7900 (74.00%)	1264 (11.84%)	2158 (20.22%)	6939 (65.00%)Table 6: AnSWerS of each model for relations that include identical words.
Table 6: AnSWerS of each model for relations that include identical words.
Table 7: Mean squared error comparison between the models for each function. The upper threeoperations are groups and the lower two equations are semigroups. Square root of the values arepresented. Smaller is better.
Table 8: Selected hyperparameters in word analogy task.
Table 9: Detailed explanation of bigger analogy test set. pair refers to the whole relation size, usedrefers to the number included in the word2vec model, and identical refers to the number of usedreå±±tions that include identical words.
Table 10: Detailed explanation of Google analogy test set. num refers to the whole relation size andused refers to the number included in the word2vec model.________________________________category	subcategory	example	num	usedSemantic	capital-common-countries	Athens:Greece	506	506Semantic	capital-world	Abuja:Nigeria	4524	4524Semantic	currency	Algeria:dinar	866	866Semantic	city-in-state	Chicago:Illinois	2467	2467Semantic	family	boy:girl	506	506Syntactic	gram1-adjective-to-adverb	amazing:amaZingly	992	992Syntactic	gram2-opposite	acceptable:unacceptable	812	812Syntactic	gram3-comparative	bad:worse	1332	1332Syntactic	gram4-superlative	bad:worst	1122	1122Syntactic	gram5-present-participle	code:coding	1056	1056Syntactic	gram6-nationality-adjective	Albania:Albanian	1599	1599Syntactic	gram7-past-tense	dancing:danced	1560	1560Syntactic	gram8-plural	banana:bananas	1332	1332Syntactic	gram9-plural-verbs	decrease:decreases	870	87022Under review as a conference paper at ICLR 2022Table 11: Model comparison for each subcategory of bigger analogy test set.
Table 11: Model comparison for each subcategory of bigger analogy test set.
Table 12: Model comparison for each subcategory of bigger analogy test set.
Table 13: Model comparison for each subcategory of Google analogy test set.
Table 14: Model comparison for each subcategory of Google analogy test set.
