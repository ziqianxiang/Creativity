Table 1: ProoD combines desirable properties of existing (adversarially robust) OOD detectionmethods. It has high test accuracy and standard OOD detection performance (as (Hendrycks et al.,2019)) and has worst-case guarantees if the out-distribution samples are adversarially perturbed in anl∞-neighborhood to maximize the confidence (see Section 4.2). Similar to CCU (Meinke & Hein,2020) it avoids the problem of asymptotic overconfidence far away from the training data.
Table 2: OOD performance: For all models we report accuracy on the test set of the in-distributionand AUCs, guaranteed AUCs (GAUC), adversarial AUCs (AAUC) for different test out-distributions.
Table 3: Architecture: The architectures that are used for the binary discriminators. Each convolu-tional layer is directly followed by a ReLU.
Table 4: Separate Training: Addendum to Table 2 showing the AUCs, GAUCs and AAUCs ofProoD-S on all datasets. The accuracy must always be identical to that of OE and the clean AUCsare also very similar to those of OE. The guarantees are strictly weaker than those provided by thesemi-jointly trained ProoD.
Table 5: Training with 80M Tiny Images: We repeat the evaluation from Table 2 for models thatwere trained using 80M Tiny Images as out-distribution instead of OpenImages. Plain is identicalto before and is just repeated for the reader’s convenience. Note that the conclusions from themain paper still hold, which indicates that our method is robust to changes in the choice of trainingout-distribution. For ATOM and ACET we compare to pre-trained models from (Chen et al., 2020).
Table 6: False Positive Rates: For all models we report accuracy on the test of the in-distribution andthe false positive rate at 95% true positive rate (FPR) (smaller is better). We also show the adversarialFPR (AFPR) and the guaranteed FPR (GFPR) for different test out-distributions. The radius of thel∞-ball for the adversarial manipulations of the OOD data is = 0.01 for all datasets. The bias shift∆ that was used for ProoD is shown for each in-distribution. ProoD struggles to give non-trivialguarantees for the FPR@95% on most datasets. However, different from GOOD or ProoD-Disc, theclean performance is generally as good as that of OE.
Table 7: Additional Datasets: We show the AUC, AAUC and GAUC for all models on uniformnoise and on the test set of the train out-distribution.
Table 8: Additional Datasets: For RImgNet, we show the AUC, AAUC and GAUC for all modelson uniform noise and on the test set of the train out-distribution, i.e. NotRImgNet.
Table 9: Generalization to larger e: We evaluate all CIFAR models in Table 2 using an e =1,and thus an unseen threat model. The provable methods GOOD and ProoD generalize surprisinglywell, while neither ATOM nor ACET display any generalization to the larger threat model.
Table 10: Error Bars: We show the mean and standard deviation σ of all metrics for our CIFAR10models across 5 runs. The tolerances for ProoD’s clean performance are very small and yet thedifferences in clean performance between OE ProoD are not significant.
