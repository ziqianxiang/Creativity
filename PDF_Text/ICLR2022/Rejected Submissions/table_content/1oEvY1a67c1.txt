Table 1: Self-learning successfully adapts ImageNet-scale models across different modelarchitectures on IN-C, IN-A and IN-R. We adapt the vanilla ResNet50, ResNeXt101 andDenseNet161 models to IN-C and decrease the mCE by over 19 percent points in all models. Further,self-learning works for models irrespective of their size: Self-learning substantially improves theperformance of the ResNet50 and the ResNext101 trained with DAug+AM, on IN-C by 11.9 and9.7 percent points, respectively. Finally, we further improve the current state-of-the-art model onIN-C—the EfficientNet-L2 Noisy Student model—and report a new state-of-the-art result of 22%mCE (which corresponds to a top1 error of 17.1%) on this benchmark with test-time adaptation(compared to 28% mCE without adaptation).
Table 2: Self-learning improves robustified and domain adapted models on small-scaledatasets. We test common domain adaptation techniques like DANN (Ganin et al., 2016) andUDA-SS (Sun et al., 2019a), and show that self-learning is effective at further tuning such modelsto the target domain. We suggest to view unsupervised source/target domain adaptation as a stepcomparable to pre-training under corruptions, rather than an adaptation technique specifically tunedto the target set—indeed, we can achieve error rates using, e.g., DANN + target adaptation previouslyonly possible with source/target based pseudo-labeling, across different common domain adaptationbenchmarks. Self-learning also decreases the error on CIFAR10-C of the Wide ResNet modeltrained with AugMix (AM, Hendrycks et al., 2020b) and reaches a new state of the art on CIFAR10-C of 8.5% topi error with test-time adaptation. ^denotes preliminary results on CIFAR-C dev only,due to instabilities in training the adversarial network in DANN.
Table 3: Self-learning also improves large pre-trained models. Unlike BatchNorm adaptation(Schneider et al., 2020), we show that self-learning transfers well to models pre-trained on a largeamount of unlabeled data: self-learning decreases the mCE on IN-C of the ResNeXt101 trained on3.5 billion weakly labeled samples (IG-3.5B, Mahajan et al., 2018) from 51.7% to 40.9%.
Table 4: Self-learning outperforms previously published test-time adaptation approaches onIN-C. The robustness benchmark IN-C has so far mostly been regarded in the ad-hoc evaluationsetting as discussed in our introduction. Thus, there are only few published methods that reportnumbers for test-time adaptation: BatchNorm adaptation (Schneider et al., 2020), Test-TimeTraining (TTT, Sun et al., 2019b), and TENT (Wang et al., 2020). In particular, note that TTTrequires a special loss function at training time, while our approach is agnostic to the pre-trainingphase. Our self-training results outperforms all three baselines (also after tuning TENT with our fullexperimental protocol):mCE on IN-C test [%] (&) ResNet50 vanilla	wlo adapt 76.7	BN adapt 62.2	TENT (ours) 53.5 (51.6)	self-learning 50.5top1 error [%] on IN-C, sev. 5 (&)	wlo adapt	BN adapt	TTT	self-learningResNet18 vanilla	85.4	72.2	66.3	61.9Table 5:	Self-supervised methods based on self-learning allow out-of-the-box test-timeadaptation. The recently published DINO method (Caron et al., 2021) is another variant of self-supervised learning that has proven to be effective for unsupervised representation learning. At thecore, the method uses soft pseudo-labeling. Here, we test whether a model trained with DINO on thesource dataset can be test-time adapted on IN-C using DINO to further improve out-of-distributionperformance. Since the used model is a vision transformer model, we test different choices ofadaptation parameters and find considerable performance improvements in all cases, yielding anmCE of 43.5%mCE at a parameter count comparable to a ResNet50 model. For adapting the affinelayers, we follow Houlsby et al. (2019):
Table 5:	Self-supervised methods based on self-learning allow out-of-the-box test-timeadaptation. The recently published DINO method (Caron et al., 2021) is another variant of self-supervised learning that has proven to be effective for unsupervised representation learning. At thecore, the method uses soft pseudo-labeling. Here, we test whether a model trained with DINO on thesource dataset can be test-time adapted on IN-C using DINO to further improve out-of-distributionperformance. Since the used model is a vision transformer model, we test different choices ofadaptation parameters and find considerable performance improvements in all cases, yielding anmCE of 43.5%mCE at a parameter count comparable to a ResNet50 model. For adapting the affinelayers, we follow Houlsby et al. (2019):w/o adapt Wl adapt	Wl adapt	Wl adapt w/ adaptmCE on IN-C test [%] (&)	affine layers bottleneck layers lin. layers all weightsViT-S/16	623	518	468	452	43.55 Understanding test-time adaptation with self-learningIn the following section, we show ablations and interesting insights of using self-learning for test-time adaptation. If not specified otherwise, all ablations are run on the holdout corruptions of IN-C(our dev set) with a vanilla ResNet50.
Table 6:	Robust pseudo-labeling outperforms entropy minimization on large-scale datasetswhile the reverse is true on small-scale datasets. We find that robust pseudo-labeling consistentlyimproves over entropy minimization on IN-C, while entropy minimization performs better onsmaller scale data (CIFAR10, STL10, MNIST). The finding highlights the importance of testingboth algorithms on new datasets. The improvement is typically on the order of one percent point:mCE, IN-C dev	ResNet50	ResNeXt-101	EfficientNet-L2	top-1 err, CIFAR-C	WRN-40ENT	50.0 ± 0.04	43.0	22.2	ENT	8.5RPL	48.9 ± 0.02	42.0	21.3	RPL	9.0Table 7: Robust pseudo-labeling allows usage of the full dataset without a threshold. Classicalhard labeling needs a confidence threshold (T) for best performance, thereby reducing the datasetsize, while best performance for rpl is reached for full dataset training with a threshold T of 0.0:diff. self-learning methods no adapt soft PL hard PL (T): 0.0 0.5 0.9 RPL (T): 0.0 0.5 0.9mCE on IN-C dev [%]	69.5	60.1	53.8 51.9 52.4	49.7 49.9 51.86Under review as a conference paper at ICLR 2022Table 8: Short update intervals are crucial for fast adaptation. Having established thatrpl generally performs better than soft- and hard-labeling, we vary the update interval for theteacher. We find that instant updates are most effective. In entropy minimization, the update intervalis instant per default.
Table 7: Robust pseudo-labeling allows usage of the full dataset without a threshold. Classicalhard labeling needs a confidence threshold (T) for best performance, thereby reducing the datasetsize, while best performance for rpl is reached for full dataset training with a threshold T of 0.0:diff. self-learning methods no adapt soft PL hard PL (T): 0.0 0.5 0.9 RPL (T): 0.0 0.5 0.9mCE on IN-C dev [%]	69.5	60.1	53.8 51.9 52.4	49.7 49.9 51.86Under review as a conference paper at ICLR 2022Table 8: Short update intervals are crucial for fast adaptation. Having established thatrpl generally performs better than soft- and hard-labeling, we vary the update interval for theteacher. We find that instant updates are most effective. In entropy minimization, the update intervalis instant per default.
Table 8: Short update intervals are crucial for fast adaptation. Having established thatrpl generally performs better than soft- and hard-labeling, we vary the update interval for theteacher. We find that instant updates are most effective. In entropy minimization, the update intervalis instant per default.
Table 9: Adaptation of only affine layers is important in CNNs. On IN-C, adapting only theaffine parameters after the normalization layers (i.e., the rescaling and shift parameters β and γ)works better on a ResNet50 architecture than adapting all parameters or only the last layer. Weindicate the number of adapted parameters in brackets.
Table 10: Self-learning decreases the top1 error on some IN-D domains but increases it on others.
Table 11: Model checkpoints used for our experiments.
Table 12: AlexNet top1 ennons on ImageNet-CC.2 Detailed results for tuning epochs and learning ratesWe tune the leanning nate fon all models and the numben of tnaining epochs fon all models exceptthe EfficientNet-L2. In this section, we pnesent detailed nesults fon tuning these hypenpanametens fonall considened models. The best hypenpanametens that we found in this analysis, ane summanized inTable 17.
Table 13: mCE in % on the IN-C dev set fon ent and rpl fondiffenent numbens of tnaining epochs when adapting the affinebatch nonm panametens of a ResNet50 model.
Table 14: mCE (&) in % on the IN-C dev setfon diffenent leanning nates fon EfficientNet-L2. We favon q = 0.8 oven q = 0.7 dueto slightly impnoved nobustness to changesin the leanning nate in the wonst case ennonsetting.
Table 17: The best hyperparameters for all models that we found on IN-C. For all models, we fine-tune onlythe affine batch normalization parameters and use q = 0.8 for RPL. The small batchsize for the EfficientNetmodel is due to hardware limitations.
Table 15: mCE in % on IN-C dev for entropyminimization for different learning rates and trainingepochs for ResNeXt101. (div.=diverged)ENT lr 2.5 × epoch	1e-4	Baseline 1e-3	5e-3	IG-3.5B			DAug+AM						1e-4	1e-3	5e-3	1e-4	1e-3	5e-3BASE	53.6	53.6	53.6	47.4	47.4	47.4	37.4	37.4	37.41	43.0	92.2	div.	40.9	40.4	58.6	35.4	46.4	div.
Table 16: mCE in % on IN-C dev for robust pseudo-labeling for different learning rates and training epochsfor ResNeXt101. (div.=diverged)RPL lr 2.5× epoch	Baseline			IG-3.5B			DAug+AM			1e-4	1e-3	5e-3	1e-4	1e-3	5e-3	1e-4	1e-3	5e-3BASE	53.6	53.6	53.6	47.4	47.4	47.4	37.4	37.4	37.41	43.4	51.3	div.	45.0	39.9	43.6	35.3	35.1	79.12	42.3	63.2	div.	43.4	39.3	48.2	34.9	35.6	121.23	42.0	72.6	div.	42.4	39.4	52.9	34.7	40.1	133.54	42.0	72.6	div.	42.4	39.4	52.9	34.7	40.1	133.5C.3 Detailed results for all IN-C corruptionsWe outline detailed results for all corruptions and models in Table 18. Performance across theseverities in the dataset is depicted in Figure 6. All detailed results presented here are obtained byfollowing the model selection protocol outlined in the main text.
Table 18: Detailed results for each corruption along with mean corruption error (mCE) as reported in Table2 in the main paper. We show (unnormalized) top-1 error rate averaged across 15 test corruptions along withthe mean corruption error (mCE: which is normalized). Hyperparameter selection for both ent and rpl wascarried out on the dev corruptions as outlined in the main text. Mismatch in baseline mCE for EfficientNet-L2 can be most likely attributed to pre-processing differences between the original tensorflow implementationXie et al. (2020a) and the PyTorch reimplementation we employ. We start with slightly weaker baselines forResNet50 and ResNext101 than Schneider et al. (2020): ResNet50 and ResNext101 results are slightly worsethan previously reported results (typically 0.1% points) due to the smaller batch size of 128 and 32. Smallerbatch sizes impact the quality of re-estimated batch norm statistics when computation is performed on the flySchneider et al. (2020), which is of no concern here due to the large gains obtained by pseudo-labeling.
Table 19: Detailed results for each corruption along with mean error on CIFAR10-C as reported in Table 2 inthe main paper.
Table 20: Detailed results for the UDA methods reported in Table 2 of the main paper.
Table 21: ImageNet-C dev set mCE in %, vanilla ResNet50, batch size 96. We report the best score across amaximum of six adaptation epochs.
Table 22: Comparison of hard-pseudo labeling and robust pseudo-labeling to Test-Time Training Sun et al.
Table 23: ImageNet-C dev set mCE for various batch sizes with linear learning rate scaling. All results arecomputed for a vanilla ResNet50 model using RPL with q = 0.8, reporting the best score across a maximiumof six adaptation epochs.
Table 24: ImageNet-C performance for three seeds on a ResNet50 for ent and rpl.
Table 25: mDE in % on IN-D for different model selection strategies.
Table 26: Top-1 error on IN-D in % as obtained by robust ResNet50 models. For reference, we also show themCE on IN-C and the top-1 error on IN-R. See main test for model references.
Table 29: Top-1 error on IN-D in % as obtained by state-of-the-art robust ResNet50 models and ent. See maintext for references to the used models.
Table 30: mDE on IN-D in % as obtained by robust ResNet50 models with a baseline evaluation, batch normadaptation, RPLq=0.8 and ENT. See main text for model references.
Table 27: Top1 error on IN-D in % as obtained by state-of-the-art robust ResNet50 models and batch normadaptation, with a batch size of 128. See main text for model references.
Table 28: Top-1 error on IN-D in % as obtained by state-of-the-art robust ResNet50 models and RPLq=0.8. Seemain text for model references.
Table 31: Top-1 error (&) on IN-D in % for EfficientNet-L2Domain	Baseline	ENT	RPLClipart	45.0	39.8	37.9Infograph	77.9	91.3	94.3Painting	42.7	41.7	40.9Quickdraw	98.4	99.4	99.4Real	29.2	28.7	27.9Sketch	56.4	48.0	51.5mDE	67.2	66.8	67.2D.4 Detailed results on the error analysis on IN-DFrequently predicted classes We analyze the most frequently predicted classes on IN-D bya vanilla ResNet50 and show the results in Fig. 9. We make several interesting observations:First, we find most errors interpretable: it makes sense that a ResNet50 assigns the label “comicbook” to images from the “clipart” or “painting” domains, or “website” to images from the“infograph” domain, or “envelope” to images from the “sketch” domain. Second, on the hard domain“quickdraw”, the ResNet50 mostly predicts non-sensical classes that are not in IN-D, mirroring itsalmost chance performance on this domain. Third, we find no systematic errors on the “real” domainwhich is expected since this domain should be similar to IN.
Table 32: top-1 error on IN and different IN-D domains for different settings: left column: default evaluation,middle column: predicted labels that cannot be mapped to IN-D are filtered out, right column: percentage offiltered out labels.
Table 33: top-1 error on IN-D by AlexNet which was used for normalization.
Table 34: Self-learning can improve performance on WILDS ifa systematic shift is present — on Camelyon17,the ood validation and test sets are different hospitals, for example. On datasets like RxRx1 and FMoW, wedo not see an improvement, most likely because the ood domains are shuffled, and a limited amount of imagesexist for each test domain.
Table 35: mCE in % on the IN-C dev set for ent and rpl for different numbers of training epochs when adapting the affine batch norm parameters of a ResNet50 model.							Table 36: mCE in % on the IN- C dev set for ent and rpl for different numbers of training epochs when adapting the affine batch norm parameters of a ResNet50 model.		criterion lr, 7.5 × epoch	10-5	ENT 10-4	10-3	10-5	RPL 10-4	10-3										dev mCE		test mCE0	49.63	49.63	49.63	49.63	49.63	49.63	Baseline	49.63	55.031	49.44	50.42	52.59	49.54	48.89	48.95	ENT	48.80	56.362	49.26	50.27	56.47	49.47	48.35	50.77	RPL	48.35	54.413	49.08	52.18	60.06	49.39	48.93	51.45			4	48.91	52.03	60.50	49.31	50.01	51.53			5	48.80	51.97	62.91	49.24	49.96	51.34			6	48.83	52.10	62.96	49.16	49.71	51.19			7	48.83	52.10	62.96	49.16	49.71	51.19			E.3 Can Self-Learning improve over Self-Learning based UDA?An interesting question is whether test-time adaptation with self-learning can improve upon self-learning based UDA methods. To investigate this question, we build upon French et al. (2018) andtheir released code base at github.com/Britefury/self-ensemble-visual-domain-adapt. We trainedthe Baseline models from scratch using the provided shell scripts with the default hyperparametersand verified the reported performance. For adaptation, we tested BN adaptation, ent, rpl, as wellas continuing to train in exactly the setup of French et al. (2018), but without the supervised loss.
Table 37: Test-time adaptation marginally improves over self-ensembling.
