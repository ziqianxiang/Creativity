Table 1: The number of false positives (FPs) and false negatives (FNs) on validation images from20 categories challenging for the face detector. Each category has 50 images. The A columns areafter automatic face detection, whereas the H columns are human results after crowdsourcing.
Table 2: Some categories grouped into supercategories in WordNet (Miller, 1998). For each super-category, we show the fraction of images with faces. These supercategories have fractions signifi-cantly deviating from the average of the entire ILSVRC (17%).
Table 3: Validation accuracies on ILSVRC using original images, face-blurred images, and face-overlaid images. The accuracy drops slightly but consistently when blurred (the ∆b columns) oroverlaid (the ∆o columns), though overlaying leads to larger drop than blurring. Each experiment isrepeated 3 times; we report the mean accuracy and its standard error (SEM).
Table 4: Visually similar categories whose top-1 accuracy varies significantly—but in opposite di-rections. However, the pattern evaporates when using top-5 accuracy or average precision.
Table 5: Top-1 accuracy on CIFAR-10 (Krizhevsky et al., 2009) and SUN (Xiao et al., 2010) ofmodels without pretraining, pretrained on original images, and pretrained on blurred images.
Table 6: mAP of face attribute classification on CelebA (Liu et al., 2015b), using subset of 5Ktraining images.
Table 7: Top-5 accuracies of models trained on original images but evaluated on images obfuscatedusing different methods. Original: original images for validation; Mean: validation images overlaidwith the average color in the ILSVRC training data; Red/Green/Blue: images overlaid with differentcolors; Blurred: face-blurred images; ∆b : Original minus blurred.
Table 8: Validation accuracies on original ILSVRC images of models trained on original/blurredimages. Training on blurred images lead to a small but consistent accuracy drop.
