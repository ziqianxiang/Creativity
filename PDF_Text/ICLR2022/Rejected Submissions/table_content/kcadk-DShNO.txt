Table 1: FID and AUB score for three images translation tasks in MNIST (for both metrics, lowerthe better). FID score for each translation task is calculated by averaging scores from each directionand AUB score is shown in nats. This table shows that our model has overall better performancesthan all baselines models in terms of both metrics.
Table 2: FID and AUB score for do-main alignment task in 10 domains. FIDscore is calculated by average acrossall paired translations and AUB score isshown in natsOurs, Î» = 1	AIignFlow, MLEAlignFlow (MLE)Figure 6: Qualitative comparison on translationresults across 10 classes with AlignFlow and ourswith transportation cost.
Table 3: AUB score for the domain alignment task in four tabular datasets. AUB score is shown innats (the lower the better).
Table 4: This table provides evidence that our model has overall better performances than all base-lines models in terms of both metrics. FID and AUB score for three images translation tasks inCelebA (for both metrics, lower is better). For domain names, 0 means Black Hair, 1 means BlondHair, and 2 means Brown Hair. FID score for each translation task is calculated by averaging scoresfrom each direction and AUB score is shown in nats.
