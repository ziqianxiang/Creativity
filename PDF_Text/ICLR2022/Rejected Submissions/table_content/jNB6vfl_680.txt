Table 1: GP improves performance on Uniform Pruning and adding MT improves performance evenfurther.
Table 2: Same trend is seen for MobileNet as well where GP outperforms Uniform Pruning andadding MT improves performance even further.
Table 3: MT improves performance on WideResNet-22-8 even in the high sparsity regime at 99.9%sparsity.
Table 4: Adding MT or conducting GP gradually enables the MobileNet model to learn and achievegood classification performance in the high sparsity regime.
Table 5: Results of SOTA pruning algorithmson WideResNet-28-8 on CIFAR-10. GP + MT(GPMT) outperforms all the other algorithms.
Table 8: Results of pruning algorithms onMobileNet-VI on ImageNet. GPMT surpassesthe SOTA algorithms by 2.4% accuracy.
Table 9: Results of pruning algorithmson FastGRNN on HAR-2 dataset. GPoutperforms other pruning algorithms inall the different network configurations.
Table 7: Results on ResNet-50 on ImageNet. GP and GPMT outperform SOTA pruning algorithmsby upto 1.3% accuracy. * and # imply that the first and last layer are dense respectively.
