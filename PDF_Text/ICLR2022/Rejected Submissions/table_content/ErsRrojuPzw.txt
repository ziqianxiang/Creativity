Table 1: Training schedule for fOFA. We replace the lengthy teacher training phase with in-placedistillation, preceded by a short warmup phase. We also decrease the size of the teacher kernelsfrom K = 7 to K = 3 or K = 5, as described in Section 3.1.
Table 2: Mean Top-1 Accuracy on ImageNetMethod	Epochs	Training Cost (GPU h)	Mean Top-1 AccuracyOFA	-605-	672(1.0x)	755ComPOFA	330	336 (2.0x)	75.4fOFA (n=3)	185	184 (3.7x)	75.5fOFA (n=4)	185	216 (3.1x)	75.5fOFA (n=4)	300	350 (1.9x)	75.6Table 2 shows the average accuracy over the generated models. For fOFA, n = 3 means that thesmallest model from the sandwich rule of (Yu et al., 2020) has been removed, and we are trainingwith the largest model and two randomly selected sub-networks. n = 4 means that the smallestmodel has been replaced, and we are training with the largest model and three randomly selectedsub-networks.
Table 3: Top-1 Accuracy on Latency Constrained Models for a Samsung Note10Method	Epochs	Latency Constraint					15 ms	20 ms	25 ms	30 msOFA	-605-	71.93	73.95	74.94	75.41comPOFA	330	72.08	73.94	74.94	75.58fOFA (n=3)	185	72.02	74.06	75.06	75.60fOFA (n=4)	185	71.74	73.78	74.77	75.47Table 3 shows the performance of the once-for-all methods for the hardware deployment scenario ofa Samsung Note10. We used the latency estimator for the Note10 CPU provided by Cai et al. (2020).
Table 4: Top-1 Accuracy on Latency Constrained Models for GPU platforms.
Table 5: Top-1 Accuracy on Latency Constrained Models for an AMD EPYC 7763 CPUMethod	Epochs	Latency Constraint					22 ms	25 ms	28 ms	31 msOFA	-605-	74.34	74.92	74.92	76.05comPOFA	330	72.77	74.65	75.17	76.35fOFA (n=3)	185	73.55	75.01	75.38	75.78fOFA (n=4)	185	73.44	74.69	75.59	75.85Table 5 shows the performance of the methods on CPU. Again, latency is measure directly usingthe CompOFA code. In this setting, we find that fOFA achieves the highest accuracy at mediumconstraints (25 ms and 28 ms), while compOFA achieves the best accuracy at 31ms, and OFA as 22ms.
