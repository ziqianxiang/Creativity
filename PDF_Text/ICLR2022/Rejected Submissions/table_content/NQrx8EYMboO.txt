Table 1: Comparisons on properties of common GNN explainers. Inductivity and task-agnosticismare inapplicable for gradient/rule-based methods as they do not require learning. In the last column,We show the number of required explainers for a dataset with N samples and M tasks.
Table 2: Fidelity scores with controlled sparsity on the node-level classification dataset PPI. Eachcolumn corresponds to an explainer model trained on (or without) a specific downstream task. Un-derlines highlight the best explanation quality in terms of fidelity, on the same level of sparsity.
Table 3: Comparison of computational time cost among three learning-based GNN explainers onthe PPI dataset. The left two columns record time cost breakdown for T downstream tasks. Thefourth column estimates the total time cost for explaining all 121 tasks of PPI. The last row showsthe speedup times compared to GNNExplainer and PGExplainer, respectively.
Table 4: Statistics of multitask datasets used for explanation quality evaluation. The column “Totalunder MolecUleNet indicates total number of commonly studied tasks from MolecUleNet.
Table 5: Fidelity scores with controlled sparsity on graph-level molecule property prediction tasks.
Table 6: Fidelity scores with controlled sparsity on the E-commerce product dataset. Each columncorresponds to one explainer model trained on different tasks or without downstream task. Under-lines highlight the best explanation quality in terms of fidelity, on the same level of sparsity.
Table 7: An ablation on training TAGE on different datasets (ZINC v.s. individual MoleculeNetdatasets)._____________________________________________________________________Method	BACE	HIV	BBBP	SIDERPGExplainer	0.252 ±0.340	0.473 ±0.404	0.182 ±0.169	0.444 ±0.391TAGE (individual)	0.402 ±0.281	0.541 ±0.330	0.202 ±0.157	0.516 ±0.292TAGE (ZINC)	0.378 ±0.293	0.595 ±0.321	0.193 ±0.161	0.521 ±0.278Table 8: A comparison between TAGE, GEM, and PGExplainer on BACE in terms of fidelity scoreswhen fixing the sparsity scores. For GEM, we vary the threshold when generating explanationgroundtruth with Granger causality to obtain explanations with different sparsity scores.
Table 8: A comparison between TAGE, GEM, and PGExplainer on BACE in terms of fidelity scoreswhen fixing the sparsity scores. For GEM, we vary the threshold when generating explanationgroundtruth with Granger causality to obtain explanations with different sparsity scores.
