Table 1: Average number of training iterations before solving LunarLanderContinuous-v2Method	Average Training IterationsHDCA-3	14.8 ± 10.5HDCA-10	4.4 ± 0.80HDCA-50	4.6 ± 2.33HDCA-100	5.2 ± 2.99HDCA (Average)	7.25 ± 4.37The results from Figure 1 for HDCA are summarized in Table 1 and demonstrate how HDCA canquickly converge to a solution. On average, it took ES 18.4 iterations, ARS 20.8 iterations, andHDCA 7.25 iterations to converge. The number of training iterations was chosen as a primarymetric for analysis due to HDCA’s scalability with the number of cores. Depending on how HDCAis implemented and the total number of available processors, all function evaluations for a giveniteration can be performed in parallel. Thus, looking at how these methods perform relative to the6Under review as a conference paper at ICLR 2022(c) 50 Coordinates(d) 100 CoordinatesFigure 1: Training performance for HDCA compared to ARS and ES for theLunarLanderContinuous-v2 environment. The estimated expected cumulative reward is plot-ted against the training iteration, where training continued until the expected reward minus its
Table 2: Average number of training iterations before solving various MuJoCo locomotion tasksTask (Threshold)	Method	Average Training IterationsSwimmer-v2 (325)	HDCA	2.2 ± 0.75	ARS	25.6 ± 2.65	GLD	2.2 ± .4Hopper-v2 (3120)	HDCA	80.8 ± 30.9	ARS	121 ± 34.9	GLD	393 ± 245HalfCheetah-v2 (3430)	HDCA	116 ± 72.8	ARS	93.6 ± 11.3	GLD	*5	ConclusionThis work develops a gradient-free, black box optimization technique that can match or outperformsimilar state-of-the-art methods. Its independence from gradients makes it highly scalable and com-petitive in parallel architectures. Furthermore, its intuitive hyperparameters and simple structurelead to a straightforward implementation that can be easily adapted to other RL environments.
