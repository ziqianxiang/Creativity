Table 1: Comparing the computational costs (in TFLOPs) required by FedDrop against other methodsgiven a communications budget. Multipliers in parentheses signify the magnitude of FLOPs increasecompared to FedDrop.
Table 2: Comparing the computational costs (in TFLOPs) required by FedDrop against other methodsgiven a communications budget with 1000 devices and 1% device participation ratio (Ï† = 0.01).
Table 4: Theoretical memory reductions of FedDrop with varying batch sizes.
Table 5: Comparing the estimated and actual FLOPs counts of FedDrop. The figures within parenthe-ses represent the sampled FLOPs of 3 experiments with different random seeds.
Table 6: Layout of the model used for Fashion-MNIST training.
Table 7: Layout of the model used for SVHN training.
Table 8: Layout of the VGG-9 model used for CIFAR-10 training.
Table 9: Layout of the model used for Shakespeare dataset training.
Table 10: iPhone 12 Pro run time of training on CPUs.
