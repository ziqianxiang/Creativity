Table 1: Semi-supervised few-shot classification accuracy on test split of miniImageNet. The toprow of uni-modality results are only applied to visual features. The middle row reports the classi-fication performance on the methods of cross modality alignment based methods extended to fewshot learning framework and the last row demonstrates the results with robust few shot learning.
Table 2: Semi-supervised few-shot classification accuracy on test split of tieredImageNet (tIN). Thetop row of uni-modality results are only applied to visual features. The middle row reports theclassification performance on the methods of cross modality alignment based methods extended tofew shot learning and the last row demonstrates the results with robust few shot learning.
Table 3: Semi-supervised few-shot learning results (%) using our method with different combi-nations of denoising strategies training with different levels of label noise and outliers. The boldnumber in each column of sub-boxes represents the best result. Here UP stands for uncertaintypriors, RD means robust divergence and CM represents cross multimodal data (images and text).
Table 4: Comparison of the training time (hours) of RCFSL on Fewshot CIFAR-100 with severalstate-of-the-art approaches evaluated on a single Nvidia V100 GPU.
Table 5: Comparison of the inference time (seconds) of RCFSL on Fewshot CIFAR-100 with severalstate-of-the-art approaches evaluated on a single NvidiaV100 GPU.
Table 6: Comparison of classification accuracy using different learning algorithms on the Cloth-ing1M datasets (real-world noise with 10% and 20% outliers respectively) for 5 way 5 shot semi-supervised few shot learning on 1000 test episodes.
Table 7: Comparison of classification accuracy using different learning algorithms on the Omniglotdataset (20% symmetric label noise with 10% outliers) for semi-supervised few shot learning on1000 test episodes.
Table 8: Unsupervised test log-likelihood using different learning algorithms on the permutationinvariant MNIST dataset (with 20% outliers) the normalizing flows VAE (VAE+NF), importanceweighted auto-encoder (IWAE), variational Gaussian pro-cess VAE (VAE+VGP), Ladder VAE(LVAE) with FT denoting the finetuning procedure S0nderby et al. (2016) and auxiliary deep gen-erative models Maal0e et al. (2017) and our method (Î²=0.2), where L represents the number ofstochastic latent layers z1, . . . , zL and IW characterizes the importance weighted samples duringtraining.
