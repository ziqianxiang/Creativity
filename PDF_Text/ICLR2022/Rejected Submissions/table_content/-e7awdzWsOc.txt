Table 1: Task performance of DynSparse training ofBERT-Base with sparsity s = 0.9 for various block sizesB, compared to dense BERT-Small with similar numberof FLOPs and linear interpolation of baseline values("Matched") with exactly the same number of FLOPs.
Table A.2: Number of sparsity pattern up-dates n dependence of unstructured (1×1)DynSparse BERT-Medium, η = 0.001397,sparsity s = 0.9, 10 epochs, phase I (pruningratio pr = 0.5 with cosine decay and randomreallocation).
Table A.3: Pruning ratio pr dependence of un-structured (1×1) DynSparse BERT-Medium,η = 0.001397, sparsity s = 0.9, 10 epochs,phase I (number of updates n = 160 withcosine decay and random reallocation). Samehyperparameters as in Table A.2.
Table A.4: Number of sparsity pattern up-dates n dependence of structured (16×16)DynSparse BERT-Medium, η = 0.001397,sparsity s = 0.9, 10 epochs, phase I (pruningratio pr = 0.5 with cosine decay and randomreallocation).
Table A.5: Pruning ratio pr dependenceof structured (16×16) DynSparse BERT-Medium, η = 0.001397, sparsity s = 0.9, 10epochs, phase I (number of updates n = 160with cosine decay and random reallocation).
Table A.6: Pruning ratio pr and number of update n dependence of unstructured (1×1) DynSparseBERT-Medium, η = 0.001837, sparsity s = 0.99, 10 epochs, phase I (with cosine decay and randomreallocation).
Table A.7: Task performance of DynSparsetraining of BERT-Medium with sparsity 0.9for block size B = 16 for various block sizemetrics.
Table A.8: Task performance of DynSparseBERT-Medium with sparsity 0.9 for variousblock sizes B compared to dense BERT-Miniwith similar number of FLOPs and linear in-terpolation of the baseline values ("Matched")with exaclty the same number of FLOPs. Hy-perparameters are not specifically tuned fordifferent block sizes. See also BERT-Baseresults in Table 1.
Table A.9: MLM validation loss of BERT-Small for results given in Figure 1.
