Table 1: Number of communication rounds required to reach ε + 夕 accuracy for μ strongly convex andgeneral convex functions under time-varying FL scenarios (Assumption 1-4). We can recover the rates inconventional FL setups by setting D = 0 and A = 0. Note that g=0 in FedAvg. Our convergence rate ofFedAvg matches the results in Karimireddy et al. (2020b). Our SGD rate of CL on strongly-convex case is novel.
Table 2: Top-1 accuracy for different choices of approximation techniques in CFL. We train ResNet18 onsplit-CIFAR10 dataset (w/ α = 0.2) for 300 communication rounds, and the dataset is partitioned to 300 subsetsfor 10 different clients. All examined algorithms use FedAvg as the backbone.
Table 3: Top-1 accuracy of various CFL methods on diverse datasets for training ResNet18 with 500communication rounds. In order to observe a noticeable performance difference on Fashion-MNIST, weuse α = 0.1 instead. All examined algorithms use FedAvg as the backbone. Both CFL-Regularization andCFL-Regularization-Full are regularization based method, and the difference lies on where the regularizationis applied: the full version applies regularization to all layers while the other only considers the top layers. Rrindicates the accuracy of CFL-Regularization, while Rf denotes the accuracy of FedAvg.
Table 4: Comparing the SOTA FL baselines with several CFL methods, for training ResNet18 on split-CIFAR10 dataset with different degrees of non-iid-ness α (and with total 500 communication rounds).
Table 5: Benchmarking FL baselines and CFL methods on different degrees of local dataset overlapping,for training ResNet18 on split-CIFAR10 dataset. The overlap reduces the degree of non-iid-ness. In order toobserve a noticeable performance difference, we use a larger data distribution gap between rounds (i.e. α = 0.1).
Table 6: Learning with stateless clients, regarding training ResNet18 on split-CIFAR10 with α = 0.2. Allexamined algorithms use FedAvg as the backbone. The details w.r.t. CFL-Regularization refer to Appendix C.2.2.
Table 7: Best learning rate of different algorithms on noisy quadratic modelμ ≤ λmin (A) ≤ λmax (A) ≤ L. Because A and A have same eigenvalues, it's simple to control theeigenvalues of A by control the eigenvalues of Λ.
Table 8: The applicability of various algorithms under different time-varying scenarios.
