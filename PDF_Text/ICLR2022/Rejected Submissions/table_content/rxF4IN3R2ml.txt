Table 1: MQTransformer encoder and decoderEncoder	Decoder Contextsht1 = TEMPORALCONV(y:t , x:t , r:t) ht2 = FEEDFORWARD(s) ht = [ht1 ; ht2],	(2)	ct,h = HSATTENTION(h:t, r)	(3) cta = FEEDFORWARD(ht , r) Ct = [ct,1； •一；Ct,H ； ca] ect,h = DSATTENTION(c:t, h:t, r),Our decoder incorporates our horizon specific and decoder self-attention blocks, and consists of twobranches. The first (global) branch summarizes the encoded representations into horizon-specific(ct,h) and horizon agnostic (Ca) contexts. Formally, the global branch ct := mo(∙) is given by (3).
Table 2: Attention weight and output computations for blocks introduced in Section 3.3Block	Attention Weights			Output		Decoder-Encoder Attention	Ath,s = h qt = lʃ —— ks =	qth,>Wq>Wkks [ht; rt; rt+h] [hs; rs]	(4)	ct,h =	t X Ath,sWvvs s=t-L	(5)	vs =	hs				Decoder Self-Attention	Ah At,s,r qt,h ks,r	= qt>,hWqh,>Wkhks,r = [ht; ct,h; rt; rt+h] = [cs,r; rs; rs+r]	(6)	et,h = H(t, h)	X	Ash,t,rWvhvs,r , (s,r)∈H(t,h) := {(s, r)|s + r = t + h}	(7)	vs,r	= cs,r				(a) MQTransformer decoder at FCT T .
Table 3: P50 (50th percentile) and P90 (90th percentile) quantile loss on the backtest year alongdifferent dimensions. Values indicate relative performance versus the baseline.
Table 4: Quantile loss metrics with the best results on each task emphasized. Results in parenthesescorrespond to training MQTransformer without forking sequences on 450K trajectories only.
Table 5:	Attention weight and output computations for MQT-All decoder self-attentionAttention Weights	OutputAh At,s qt ks	t qt>Wqh,>Wkhks	ect,h = X Ath,sWvhvs 二[Cs,ι;•一；Cs,H ； ht； rt]	S=JL 二[cs,1；…；Cs,H ； rs ]vs	[cs,1； •一；Cs,h]	Table 6 gives the number of parameters in each trained model.
Table 6:	Parameter counts in trained models for the large scale demand-forecasting task.
Table 7:	Parameter settings for Large Scale Demand Forecasting ExperimentsParameter	ValueEncoder Convolution Dilation Rates [1,2,4,8,16,32]Position Encoding Dilation Rates	[1,2,4,8,16,20]Static Categorical	One-HotTime-Series Categorical	One-HotStatic Encoder Dimension	64Convolution Filters	32Attention Block Head Dimension	16Dropout Rate	0.15Activation Function	ReLUTable 8: Quantile loss on the backtest year along different dimensions. Values are relative performanceversus baseline, with the best result for each dimension emphasized.
Table 8: Quantile loss on the backtest year along different dimensions. Values are relative performanceversus baseline, with the best result for each dimension emphasized.
Table 9: Parameter settings of reported MQTransformer model on each public dataset.
