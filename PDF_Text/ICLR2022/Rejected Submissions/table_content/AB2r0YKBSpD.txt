Table 1: Coefficients for filtering				with a 10-gram overlap. This leaves 〜750M noisy sentence pairs. We train a 6L6L model on thisdataset with increasing dataset sizes {1M, 2M...256M}. To study the effects of the filtering, weuse two filtering algorithms. For the first filtering algorithm, We use the Bicleaner scores (Ramlrez-Sanchez et al., 2020) that are publicly released along with the Paracrawl dataset v8.1. This is a scoreranging from (0, 1), with a higher score indicating higher sentence quality. The bicleaner scores in-cludes various hard-coded rules, language-model fluency scores, and scores from a classifier trainedto detect mutual translations. We use a threshold of 0.5 and discard all sentences below this thresholdleaving US with 〜300M sentence pairs. For the second filtering algorithm, we choose ContrastiveData Selection (CDS) (Wang et al., 2018), which belongs to a family of cross-entropy-based filter-ing algorithms (Moore & Lewis, 2010; Junczys-Dowmunt, 2018). CDS scores the quality of eachsentence pair according to the difference in cross entropy scores between two related translationmodels: a clean model that was fine-tuned on a trusted dataset, and a noisy one that was not. Wechoose the top 50% of the CDS-ranked sentence pairs.
Table 2: Coefficients for added noise4.2.1	Independent noiseWe start with a simple setup of the following types ofiid noise added to the source and target side respec-tively: (1) Character level: We perturb p = 0.1 frac-tion of the characters in the sentences to random char-acters (alphumeric + punctuations), (2) Word level: Wedelete p = 0.15 fraction of the words from either thesource or the target side, and (3) Sentence level: Forp = 0.1 fraction of the sentences, we shuffle the map-ping sentences of the sentence pairs. These noise typeshave also been considered in prior work Khayrallah & Koehn (2018). Next, we train a 6L6L trans-former model on increasing subsets of the noisy training datasets. The results are shown in Figure1C and Table 3.
Table 3: Coefficients for BTthe BT trained models is slightly lower (〜 0.2) than the scaling exponents of the par-allel dataset (〜 0.28) as is also visible in Figure 3B. We did not get good fits whentried to jointly fit the BT data with the clean parallel data with a common exponent.
