Table 1: Comparison of BLEU scores between our approach E-ARM and the base ARGM trained just withcross-entropy loss on six translation pairs of IWSLT14 datasets. We use “-” to denote that the training trick isnot used while “✓” indicates We use it. “5 B” represents We use beam searching With 5 beams.
Table 2: Translation performance of proposed E-ARM onWMT16 English→German, evaluated With BLEU. We uniformlyuse 5 beams When applying beam search. “L.S.” denotes LabelSmoothing and “S.S.” denotes Scheduled Sampling.
Table 3: Language modeling performance of different models onWikiText103. Evaluation is conducted using perplexity (PPL).
Table 4: How different λ and the E-ARM startepoch (when we introduce the E-ARM into thetraining on WikiText103) affect performance eval-uated by perplexity (PPL). The Tr-Base modelstructure is used and is train 40 epochs in total.
Table 5: Performance of E-ARM with different basenetworks on MNIST and CIFAR-10 in bits/dim (loweris better), training performance in brackets.
Table 6: Hyper-Parameters of different model structures and datasets. “Tr-Base”, “Tr-Large”, and “Tr-XL”indicate Transformer-Base, Transformer-Large, and Transformer-XL respectivelyD More Experimental AnalysisD.1 Effect on IncoherenceIn order to validate the effectiveness of our E-ARM for ameliorating the long-range coherence ofgenerations, we undertake an experiment to assess the model’s performance under different test setswith varying sentence lengths. We divided the test set of IWSLT14 (German → English, Italian →English, Spanish → English) translation dataset into three subsets ([0, 25], [25, 50], and [50, ∞))based on the target sentence lengths. Then, we incrementally applied scheduled sampling techniqueand our E-ARM above the base transformer network, and tested their performances on these threesubsets. Generally, the subset of samples with longer target sentences ([50, ∞)) should have beenmore affected by the long-range incoherence problem (lower BLEU score). In practice, we uni-formly applied label smoothing and beam searching (with 5 beams) strategy for all experiments inTable 7.
Table 7: Performance comparison on the IWSLT14 test set with respect to the different lengths of sentenceson three translation tasks (German to English, Italian to English, and Spanish to English). Performance isevaluated by BLEU score.
Table 8: The effect of E-ARM on the exposure bias problem. Each test set of translation tasks contains 1Ksentences selected randomly. N denote the ground truth words whose probabilities in the predicted distributionsproduced by E-ARM are greater than those produced by the baseline.
Table 9: The effect of Top-K correction in the inference stage. We tested BLEU scores of using different k ondifferent translation pairs of IWSLT14 dataset.
Table 10: Comparison of ROUGE-1, ROUGE-2, ROUGE-L, METEOR, and BLEU scores between our ap-proach E-ARM and the base ARGM trained just with cross-entropy loss on three translation pairs of IWSLT14datasets. The value is expressed in percentage. We use “Tr-Base” as the network architecture.
Table 11: Efficiency performance on IWSLT14 German→ English, evalu-ated with BLEU. We uniformly use 12 layer “Tr-Base” in Table 6. “S.S.”denotes Scheduled Sampling.”Autoreg.” indicates optimizing E-ARM withEq.12 by sampling fake data from autoregressive models. ”* steps SGLD”represents optimizing our E-ARM with Eq.6, the fake data is sampled at thefirst transformer layer’s output by SGLD with * steps.
Table 12: Translation cases on IWSLT14 De→En test set, generated by the baseline method, baseline withscheduled sampling and our E-ARM. The italic font means the mismatch translation20Under review as a conference paper at ICLR 2022E	More Discussion of related worksThe seminal idea of combing a generative model and an energy-based model has been explored by aplethora of great works (Pang et al., 2020; Durkan & Nash, 2019; Xie et al., 2019; 2020; Xiao et al.,2021; Bakhtin et al., 2021). Our E-ARM can be considered as a member of this family of mod-els in general, but it has a different mechanism and goal than the others. In particular, Pang et al.
