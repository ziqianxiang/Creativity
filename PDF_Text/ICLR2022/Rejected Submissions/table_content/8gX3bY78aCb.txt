Table 1: Graph classification accuracy (%) on various TUD graph classification tasks. Some resultsfor GraphSAGE and GCN are reported from (Xu et al., 2018) and (Zhang et al., 2019). The bestperformer on each dataset are shown in bold. - means there is no reported accuracy on this datasetin original papersMETHODS	PTC	MUTAG	NCI1	PROTEINS	MUTAGENICITYPatchySAN	60.0 ± 4.8	92.6 ± 4.2	78.6 ± 1.9	75.9 ± 2.8	-GCN	64.2 ± 4.3	85.6 ± 5.8	80.2 ± 2.0	76.0 ± 3.2	79.8 ± 1.6GraphSAGE	63.9 ± 7.7	85.1 ± 7.6	77.7 ± 1.5	75.9 ± 3.2	78.8 ± 1.2DGCNN	58.6 ± 2.5	85.8 ± 1.7	74.4 ± 0.5	75.5 ± 0.9	-GIN	64.6 ± 7.0	89.4 ± 5.6	82.7 ± 1.7	76.2 ± 2.8	-PPGN	66.2 ± 6.5	90.6 ± 8.7	83.2 ± 1.1	77.2 ± 4.7	-CapsGNN	-	86.7 ± 6.9	78.4 ± 1.6	76.3 ± 3.6	-WEGL	64.6 ± 7.4	88.3 ± 5.1	76.8 ± 1.7	76.1 ± 3.3	-GraphNorm	64.9 ± 7.5	91.6 ± 6.5	81.4 ± 2.4	77.4 ± 4.9	-OURS	78.8 ± 6.5 一	96.3 ± 2.6	83.6 ± 1.5 一	79.9 ± 3.1	83.0 ± 1.1GIN. To utilize atom-level information like node and edge features, we use another GIN on eachatom-level graphs. Specifically, it has 5 GNN layers and 2-layer MLPs. Batch normalization (Ioffe& Szegedy, 2015) is applied to each layer, and dropout (Srivastava et al., 2014) is applied to all lay-ers except the first layer. To evaluate the performance of our model, we strictly follow the settingsin (Yanardag & Vishwanathan, 2015; Niepert et al., 2016; Xu et al., 2018; Gao & Ji, 2019). For each
Table 2: Graph Classification Results (%) on twoopen graph benchmark datasets. The results forGCN and GIN are reported from (HU et al., 2020).
Table 4: Results on the PTC dataset with three different training settings. The first row reportthe performances of only using the PTC dataset. The second row and third row show the resultsof training on combined vocabularies and datasets with PTC-MM and PTC-FR, respectively. Wereport the motif vocabulary size (Vocab Size) of the dataset and the Overlap Ratio, which indicatesthe overlap ratio of motif vocabularies between two datasets. The last three columns represent theperformances of using different sizes of training sets. For example, 90% means we use 90% ofdataset as the training set and 10% of dataset as the testing set.
Table 3: Graph classification accuracy (%) of GIN andour model on three datasets: PTC, MUTAG, and PRO-TEINS._____________________________________________________Models	PTC	MUTAG	PROTEINSGIN	64.6 ± 7.0	89.4 ± 5.6	76.2 ± 2.8OURS	78.8 ± 6.5	96.3 ± 2.6	79.9 ± 3.1neous graphs and the corresponding GNNs from HM-GNNs, which reduces to GINs. We compareour HM-GNNs with GINs on three popular bioinformatics datasets: PTC, MUTAG, and PROTEINS.
