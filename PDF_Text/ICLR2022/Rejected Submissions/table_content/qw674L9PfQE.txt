Table 1: Zero-shot results for models trained on CC with ResNet-50 vision encoders for two differentcheckpoints. Results are given as mean accuracy over 5 runs. Statistically significant results areshown in bold. CLIP and CLOOB were trained for 31 epochs while CLIP* and CLOOB* weretrained for 128 epochs. In the majority of tasks CLOOB significantly outperforms CLIP.
Table 2: Performance with InfoLOOB vs. InfoNCE objective and with vs. without Hopfield retrieval.
Table 3: Results of CLIP and CLOOB trained on YFCC with ResNet-50 encoder. Except for onelinear probing dataset, CLOOB consistently outperforms CLIP across all tasks.
Table 4: Zero-shot results for the CLIP reimplementation and CLOOB using different ResNetarchitectures trained on YFCC. CLOOB outperforms CLIP in 7 out of 8 tasks using ResNet-50encoders. With larger ResNet encoders CLOOB outperforms CLIP on all tasks. The performance ofCLOOB scales with increased encoder size.
Table A1: Influence of loss functions and Hopfield retrieval. InfoLOOB increases the performance ofCLIP in most of the tasks. The InfoNCE loss is not suited for the Hopfield approach as it saturatesleading to a worse performance. Hopfield with InfoLOOB strongly improves the performance in 7out of 8 datasets compared to both CLIP models.
Table A2: Influence of learning rate scheduler. For most of the tasks the performance either increasesor remains roughly the same with restarts for both CLIP and CLOOB.
Table A3: Datasets used for zero-shot and linear probing. In the case of several train or test sets perdataset we report the total number of samples. It should be noted that at the time of this work someBirdsnap images were not accessible anymore.
Table A4: Linear probing results for the reimplementation of CLIP and CLOOB using differentResNet architectures trained on YFCC. The performance of CLOOB scales with increased encodersizeDataset	CLIP RN-50	CLOOB RN-50	CLOOB RN-101	CLOOB RN-50x4Birdsnap	50.9	56.2	58.1	62.2Country211	19.5	20.6	21.8	24.2Flowers102	94.8	96.1	96.1	96.2GTSRB	82.5	78.9	77.9	80.6UCF101	75.2	72.3	72.8	75.3Stanford Cars	36.2	37.7	39.0	44.3ImageNet	66.9	65.7	67.0	69.7ImageNet V2	60.2	58.7	60.3	62.2•	In case of a tie in the validation score, we use the maximal λ for the strongest regularization.
