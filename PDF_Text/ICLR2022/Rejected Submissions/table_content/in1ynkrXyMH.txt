Table 1: Recognition accuracy of Active Learn-ing strategies.
Table 2: OOD techniques applied on feed-forward and introspective networks when thedata is Under adversarial attack.
Table 3: Structure of H(∙) and accuracies on CIFAR-10C as reported in the paper.
Table 4: Introspecting on top of existing robustness techniques.
Table 5: Expected Calibration Error and Maximum Calibrated Error for Feed-Forward vs IntrospectiveNetworks.
Table 6: SimCLR and its supervised and introspective variations tested on CIFAR-10C.			Methods	ResNet-18	ResNet-34	ResNet-50	ResNet-101SimCLR Chen et al. (2020b)	70.28%	69.5%	67.32%	64.68%SimCLR-MLP	72.79%	72.54%	70.37%	70.89%SimCLR-Introspective (Proposed)	73.32%	73.06%	71.28%	71.76%C.3 SimCLR and IntrospectionSimCLR Chen et al. (2020b) is a self-supervised contrastive learning framework that is robust tonoise distortions. The algorithm involves creating augmentations of existing data including blur,noise, rotations, and jitters. The network is made to contrast between all the augmentations of theimage and other images in the batch. A separate network head g(∙) is placed on top of the network toextract features and inference is made by creating a similarity matrix to a feature bank. Note that g(∙)is a simple MLP. Our proposed framework is similar to SimCLR in that we extract features and usean MLP H(∙) to infer from these features. In Table 4, we show the results of Introspecting ResNetsagainst SimCLR. However, this comparison is unfair since the features in SimCLR are trained in aself-supervised fashion. In this section, we train SimCLR for ResNets-18, 34, 50, 101 and train a newMLP g(∙), not for extracting features, but to classify images. In other words, in Chen et al.(2020b),the authors create g(∙) to be a 512 X 128 layer that extracts features. We train a network of the form512 × 128 一 128 × 10 that is trained to classify images. We then introspect on this g(∙) to obtain r,.
Table 7:	Introspective Learning accuracies when rx is extracted with different loss functions forResNet-18on CIFAR-10C._____________________________________________________________________________Feed-Forward MSE-M CE	BCE	L1	L1-M Smooth L1 Smooth L1-M NLL SoftMargin67.89%	71.4%	69.47%	70.76%	70.12%	70.72%	70.42%	70.63%	70.93%	70.91%architectures from Table 4. One hypothesis for this marginal increase is that the notions createdwithin SimCLR-MLP are predominantly from the self-supervised features in SimCLR. These maynot be amenable for the current framework of introspection that learns to contrast between classesand not between features within-classes.
Table 8:	Ablation studies for H(∙) on CIFAR-10C.
Table 9: Performance of Proposed Introspective H(∙) Vs Feed-Forward f (∙) Learning under DomainShift on Office datasetArchitectures		DSLR Ψ Amazon	DSLR Ψ Webcam	Amazon Ψ DSLR	Amazon Ψ Webcam	Webcam Ψ DSLR	Webcam Ψ AmazonResNet-18	f(∙)	39.1	78	62.9	59	89.8	42.2(%)	H(∙)	47	90.7	67.3	63.9	96	44ResNet-34	f(∙)	41.8	83.3	67.3	60.1	90.6	41.7(%)	H(∙)	46.4	89.8	67.3	63.9	97.8	43.3ResNet-50	f(∙)	-	-	67.3	62	92.4	33.4(%)	H(∙)	-	-	78.1	68.4	97.8	30.8ResNet-101	f(∙)	-	-	62.9	59	89.8	31.77(%)	H(∙)	-	-	76.5	67.3	92.4	33.6Table 10: Performance of Proposed Introspective H(∙) Vs Feed-Forward f (∙) Learning under DomainShift on VisDA DatasetResNet-18	Plane	Cycle	Bus	Car	Horse	Knife	Bike	Person	Plant	Skate	Train	Truck	Allf(∙)(%)	27.6	7.2	38.1	54.8	43.3	4.2	72.7	8.3	28.7	22.5	87.2	2.9	38.1H(∙)(%)	39.9	27.6	19.6	79.9	73.5	2.7	46.6	6.5	43.8	30	73.6	4.3	43.58acquisition setup among others. Specifically, the robust recognition performance of H(∙) is validatedon Office Saenko et al. (2010) dataset using Top-1 accuracy. The Office dataset has 3 domains- images taken from either Webcam or DSLR, and extracted from Amazon website. Images canbelong to any of 31 classes and they are of varying sizes - upto 1920 × 1080. Hence, results on
Table 10: Performance of Proposed Introspective H(∙) Vs Feed-Forward f (∙) Learning under DomainShift on VisDA DatasetResNet-18	Plane	Cycle	Bus	Car	Horse	Knife	Bike	Person	Plant	Skate	Train	Truck	Allf(∙)(%)	27.6	7.2	38.1	54.8	43.3	4.2	72.7	8.3	28.7	22.5	87.2	2.9	38.1H(∙)(%)	39.9	27.6	19.6	79.9	73.5	2.7	46.6	6.5	43.8	30	73.6	4.3	43.58acquisition setup among others. Specifically, the robust recognition performance of H(∙) is validatedon Office Saenko et al. (2010) dataset using Top-1 accuracy. The Office dataset has 3 domains- images taken from either Webcam or DSLR, and extracted from Amazon website. Images canbelong to any of 31 classes and they are of varying sizes - upto 1920 × 1080. Hence, results onOffice shows the applicability of introspection on large resolution images. ImageNet pre-trainedReSNet-18,34,50,101 He et al. (2016) architectures are used for f (∙). The final layer is retrained usingthe source domain while the remaining two domains are for testing. The experimental setup, the samedetailed in Section 5, is applied and the Top-1 accuracy is calculated. The results are summarized inTable 9. In every instance, the top domain is X - the training distribution, and the bottom domain isX0 - the testing distribution. Note that ResNet-50 and 101 failed to train on 498 images in DSLRsource domain. The results of introspection exceed that of feed-forward learning in all but ResNet-50when classifying between Webcam and Amazon domains.
Table 11: Out-of-distribution Detection of existing techniques compared between feed-forward andintrospective networks.
Table 12: Performance of Contrastive Features against Feed-Forward Features and other ImageQuality Estimators. ToP 2 results in each row are highlighted.
