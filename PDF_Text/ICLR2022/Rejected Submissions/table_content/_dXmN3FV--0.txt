Table 1: Dataset Information. We provide the dataset names (Video Games abbreviated to Gamesand Wine Quality abbreviated to Wine), the evaluation metric used, the number of features (categor-ical and continuous), the number of samples and the percentage of train/valid/test split.
Table 2: Train times averaged over all epochs. The size column shows the possible prune statesrepresenting the number of nodes in each layer of the neural network, each state 50% smaller thanthe last. Then for each dataset we provide the average epoch train time in seconds followed by thepercentage improvement from [1600, 800] in parentheses.
Table 3: Inference times on test set. The size column shows the possible prune states representingthe number of nodes in each layer of the neural network, each state 50% smaller than the last. Thenfor each dataset we provide the inference time on the test set in seconds followed by the percentageimprovement from [1600, 800] in parentheses.
Table 4: Comparison of accuracy (RMSE/F1) for each dataset between the original tabular models,and iterative/oneshot modes. First result is the best performing original models, second result is bestperforming iterative when pruning the best original model, third result is best performing iterativewhen pruning the largest model [1600, 800], and the final result is the best performing oneshotmodel. Results are shown in RMSE/F1 depending on the dataset along with the model size, then weinclude the difference in percentage along with the pruning mode for the model. Bold is marked asbest performing accuracy.
Table 5: Comparison of different models to the original tabular model in Table 4, top is RMSE/F1depending on the dataset and bottom is the difference in percentage compared to the original tabularmodel. In all but one case (Health SVR) the original tabular model outperforms all of these models.
Table 6: Best models selected by size with less than 2% divergence in accuracy of the originalmodel. For each dataset (first column), we noted the size of the original model in the second col-umn. Then we show the smallest possible model with <2% divergence in accuracy for each of theoriginal, iterative on best original, iterative on [1600,800], and finally oneshot. The table shows thepercentage difference in RMSE or F1, and P the prune rate compared to original. If the difference inaccuracy is >2%, then that was the best performing model accuracy-wise and we could not producea valid smaller model using that approach. Bold is the smallest model which doesnâ€™t exceed the 2%divergence in accuracy rule.
