Table 1: Comparison with state-of-the-art. Our model outperforms previous works on SSV2and Diving48 dataset while showing competitive results on other datasets. Results on UCF101and HMDB51 are average over three train-val splits. V,A,T refer to Visual, Audio, and Textmodalities, respectively. a(Grill et al., 2020; Feichtenhofer et al., 2021), b(Miech et al., 2020),c(Alayrac et al., 2020), d(He et al., 2020; Feichtenhofer et al., 2021), e(Bertasius et al., 2021),f (Arnab et al., 2021), g(Feichtenhofer et al., 2019), h(Kalfaoglu et al., 2020), j(Tran et al., 2018),k(Wang et al., 2019), l (Kondratyuk et al., 2021). K400=Kinetics-400 (Carreira & Zisserman, 2017),HT=HowTo100M (Miech et al., 2019), AudioSet (Gemmeke et al., 2017), IG-Uncurated (Ghadi-yaram et al., 2019), IN21K=ImageNet-21K (Russakovsky et al., 2015). Note that some SotA modelsare pre-trained with extremely large (weakly-)supervised datasets, e.g., IG65M (Ghadiyaram et al.,2019) in hKalfaoglu et al. (2020)and JFT-300M (SUn et al., 2017) in f Amab et al. (2021).
Table 2: Impact of model size. ‘Speed’ is the normalized pre-training speed measured by#videos/second on one V100 GPU. ‘Mask-Accu.’ and ‘CL-Loss’ are mask-then-predict accuracyand contrastive learning loss to indicate the pre-training performance. ‘UCF101’ is the fine-tuningaccuracy on UCF101 dataset. By default, we use the configuration in the first line in our analysis.
Table 4: Impact of masking strategy. Models		Table 5: Impact of masking ratio. Models are		are pre-trained with only mask-then-predict.		pre-trained with only mask-then-predict. De-		Strategy Frame Size SI Mask-Accu. ↑ UCF101 ↑		fault setup is underlined.		block	128	17.6	68.3	Strategy I #Blocks Ratio		Mask-Accu. ↑ UCF101 ↑i.i.d.	128	24.3	63.5 (-4.8)	block	4	11.9%	17.9	66.8block	256	11.2	69.5	block	5	14.5%	17.6	68.3i.i.d.	256	19.5	61.4(-8.1)	block	6	17.0%	17.3	67.35.1 ScalabilityIn Table 2, we illustrate the scalability of our method with different model sizes (i.e., number oflayers and hidden dimensions). Larger models have more parameters (‘Params’) and higher compu-tational cost (measured by the normalized pre-training ‘Speed’). To evaluate the pre-training tasksperformance, we provide both pre-training metrics (mask-then-predict accuracy denoted by ‘Mask-Accu.’, and contrastive learning loss denoted by ‘CL-Loss’) and UCF101 downstream fine-tuningresults. As the size of the model grows, the fine-tuning results show consistent improvement withthe pre-training metrics. Note that for the last row in Table 2, we halve the attention head and MLPintermediate dimensions. We also illustrate the scalability over input resolution in Appendix F.2.
Table 3: Impact of pre-training tasks.
Table 6: Impact of maximum sampling dis-tance dmax (sec.) between two positive clips.
Table 7: Impact of #negative samples.
Table 8: Impact of mask augmentationin contrastive learning. ‘MP’=Mask-then-Predict. ‘CL-Mask’=Use input mask in CL.
Table 9: Induced Masking ratio w.r.t. to different input resolutions and #masking blocks. Thenumbers of blocks/masking ratio for each resolution setting used in our experiments are shown inbold.	__________________________________________________________________________Input Resolution			#Masking Blocks				Length	Frame Size	Token MaP Size	4	5	6	7	85	128	16	11.9	14.5	17.0	19.4	21.75	256	32	10.6	13.1	15.2	17.5	19.510	128	16	10.4	12.8	15.0	17.1	19.210	256	32	9.3	11.4	13.4	15.4	17.2Contrastive Learning Head Next we discuss the pre-training heads for contrastive learning. Itis on top of the [CLS] hidden output hCLS. We encode the hidden state with MLP. We use batchnormalization (Ioffe & Szegedy, 2015) inside the MLP head following the practice in (Chen et al.,2021).
Table 10: Model Configuration. The ‘Small’ model is mainly used in the analysis (Sec. 5) while‘Large-Half’ model is mainly used in the results (Sec. 4.3) for the final large-scale experiments.
Table 11: Training Hyperparameters. ‘Pre-Train (Results)’ is our final model in Sec. 4.3 that takesa large-half model. ‘Pre-Train (Analysis)’ is the pre-training in analysis (Sec. 5). *The batch size forpre-training is the number of samples in updating the weights, Since we use gradient accumulation,it is not correlated to the number of negative examples in contrastive learning.
Table 12: Key statistics of video datasets used in this paper. HowTo100M is used for pre-trainingwhile others are downstream datasets. The number of training/validation examples in HMDB51 andUCF101 are reported for the train-val split 1.
Table 13: Results of different attention-module layouts and layer-normalization positions.
Table 14: Impact of input resolutions T and S. ‘Mask-Accu.’ and ‘CL-Loss’ are the pre-training metrics. ‘UCF101’ indicates the UCF101 fine-tuning results with the pre-training reso-lution. ’UCF101-FUll-Reso.’ indicates the full-resolution fine-tuning With T =10 and S=256._#frames T	Frame Size S	Params	Pre-train Speed	MaSk-Accu.f	CL-LOSSJ	UCF101↑	UCF101-Full-Reso.↑5	128	29.4M	32.0	17.2	1.06	69.4	73.810	128	29.4M	16.5	17.2	0.96	74.2	74.65	256	29.4M	8.4	10.8	0.93	72.9	75.710	256	29.4M	4.4	10.6	0.85	78.1	78.1Table 15: Impact of masking ratio. All models are pre-trained with only mask-then-predict task.
Table 15: Impact of masking ratio. All models are pre-trained with only mask-then-predict task.
Table 16: Impact of contrastive learning loss weight α. Default setup is underlined.
