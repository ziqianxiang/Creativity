Table 1: Relative device utilization at 500Mb/susing simulated network latency.
Table 2: Training of language models on the OpenWebText Corpus (OWT). Our baseline model has253M parameters and is trained for 8 GPU-days. We apply bottleneck and maxout compression to ourbaseline in 2 and 4 stages with a compression factor between 2-4x. We can see that maxout outper-forms bottleneck compression if many stages or high compression factors are used. WT=WikiText,PTB=Penn Treebank, 1BW=Billion word corpus.
Table 3: Pipeline throughput comparison.
Table 4: Training performance for “GPT-3”.			Table 5: Training performance for “xxlarge”.			Throughput,	All-Reduce time		Throughput,	All-Reduce timeSystem	samples / s	seconds / round	System	samples / s	seconds / round	No latency Latency No latency Latency			No latency Latency No latency Latency	SWARM	0.619	0.558	441.7	455.4	SWARM	2.358	2.161	45.36	51.269GPipe	0.633	0.477	403	469.6	GPipe	2.541	0.957	44.17	64.828Offload	0.382	0.382	1527.9	1635.4	Offload	3.08	3.08	168.71	252.26We report our measurements in Tables 4 and 5. Table 4 demonstrates that for larger model size,SWARM and GPipe have approximately the same performance without latency. When training withlatency, SWARM significantly outperforms GPipe, which is likely caused by the asynchronous queue-ing (see Appendix D). In turn, when training smaller models (Table 5), ZeRO-Offload outperformsboth SWARM and GPipe. This result coincides with our earlier observations in Figure 1, wherethe same model spent the most of the time waiting for communication between pipeline stages.Wealso observe that ZeRO-Offload takes longer to aggregate gradients, likely because each peer mustaggregate the entire model, whereas in SWARM and GPipe peers aggregate a single pipeline stage.
Table 6: Relative throughput comparison ofpipeline rebalancing methods.
Table 7: Training time and costs.
Table 8: Performance of compression techniques for transformer language model with adaptive inputson WikiText-103. The asterisk denotes that the difference is not statistically significant.
