Table 1: Performance of MA-CQL with larger learning rate for the actor.		Table 2: Performance of MA-CQL with larger number of updates for the actor.		Learning rate 1e - 2	5e - 2	1e- 1	# Updates	1	5	20D F	267.9^^202.0 PerformanCe	±19.0	±38.9	100.1	Performance	±≡	278.6	263.7	±36.4		±14.8	±23.13.2	Offline Multi-Agent Reinforcement Learning with Actor RectificationOur key identification as above is that policy gradient improvements are prone to local optima givena bad value function landscape. It is important to note that this presents a particularly critical chal-lenge in the multi-agent setting since it is sensitive to suboptimal actions. Zeroth-order optimizationmethods, e.g., evolution strategies (Rubinstein & Kroese, 2013; Such et al., 2017; Conti et al., 2017;Salimans et al., 2017; Mania et al., 2018), offer an alternative for policy optimization and are alsorobust to local optima (Rubinstein & Kroese, 2013).
Table 3: Averaged normalized score of OMAR and baselines in multi-agent particle environments.
Table 4: Ablation study of OMAR with different sampling mechanisms in different types of datasets.
Table 5: The average normalized score of different methods based on MATD3 with centralizedcritics under the CTDE paradigm.
Table 6: Average normalized score of different methods in multi-agent HalfCheetah.
Table 7: Specs of tested maps in the StarCraft II micromanagement benchmark.
Table 8: Averaged normalized score of OMAR and CQL in the single-agent Maze2D domain fromD4RL.
Table 9: Averaged normalized score of ITD3 and MATD3 in cooperative navigation.
