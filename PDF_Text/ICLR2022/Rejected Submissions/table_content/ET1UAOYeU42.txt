Table 1: Statistics of the datasets for node and graph-level classification..
Table 2: Comparisons on node ClassifiCation performanCes.
Table 3: Comparisons on graph classification performances.
Table 4: ComParisons of EPM-GCN-g with varioUs inPUt node featUres.					Random	Hand-crafted Community-based	IMDB-B	IMDB-M	MUTAG	PTCX		64.7 ± 1.6	42.3 ± 1.5	84.6 ± 4.3	63.6 ± 2.0	X	80.3 ± 2.0	53.5 ± 2.6	93.1 ± 5.0	74.7 ± 4.1	X	74.7 ± 5.1	51.5 ± 2.0	88.9 ± 5.5	68.9 ± 3.9	XX	76.7 ± 3.1	54.1 ± 2.1	93.6 ± 3.5	75.6 ± 5.9and adaptively weighted feature aggregation mechanisms, which greatly enhances their ability inhandling multi-relational data. However, these models lack systematic modeling of latent relations,which on the other hand is the strength of our model. Our performance boost against the third groupon most of the benchmarks demonstrates the marginal benefit of our relational inference model.
Table 5: Comparisons of EPM-GCN-g with various network structures.
Table 6: Hyperparameters settings for EPM-GCN.		Hyperparameters	Experiments		Node Classification	Graph ClassificationCommunity ENCoder	Settings	α	1	1β	1	1epoches of unsupervised pretrain	1500	1500learning rate of unsupervised pretrain	1e-3	1e-2batch size of unsupervised pretrain	1	32type of GNN layers	GCN	GCNmodule structure	{32}-{16}	{200}-{100}Edge PARTitioner	Settings	number of communities	[4,8]	4	the temperature τ for partition	1	一	1Relational GCN BANK	Settings	epoches of jointly training	200	100learning rate of jointly training	1e-2	1e-3weight decay of jointly training	5e-4	0.0batch size of jointly training	1	32concat weight of Φ	3e-2	3e-4
