Table 1: The zero-shot performance of basic models and our proposed strategies on IMDB, Ama-zon, AG News, DBPedia corpora, where * refers to utilizing more correlated label words (here 30,60, 90 correlated label Words), and the average results are reported With standard deviation.
Table 2: Performance of inserting different number of prompt [MASK]s at a random position.
Table 3: The results on IMDB, Amazon, AG News, DBPedia when using different answer slots .
Table 4: The performance of different PLMs on IMDB, Amazon, AG News, DBPedia.
Table 5: The experiment results on the validation set of GLUE.
Table 6: The experiment results on the validation set of SuperGLUE.
Table 7: The label names for IMDB, Amazon, AG News, DBPedia.
Table 8: The templates of Manual Prompt (Prior).
Table 9: The templates of Manual Prompt (w/o Engineering).
Table 10: The templates of NSP-BERT.
Table 11: The templates for GLUE and SuperGLUE.
Table 12: The performance of Manual Prompt (Prior) with multiple mask tokens.
Table 13: The performance of KPT with multiple mask tokens.
Table 14: Examples of Different Prompt Strategies.
