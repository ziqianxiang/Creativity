Table 1: Comparison of bidirectional CMOW/CBOW-Hybrid versus unidirectional and bag-of-wordsbaselines under task-specific distillation with DiffCat encoding and MLP classifier.
Table 2: Comparison of task-specific vs. general distillation using bidirectional CMOW/CBOW-Hybrid embeddings and MLP classifier. In 5 out of9 tasks, general distillation performs best.
Table 3: Comparison of DiffCat encoding vs. joint BERT-like encoding. Both variants use pretrainedbidirectional CMOW/CBOW-Hybrid embeddings with MLP under task-specific distillation. DiffCatencoding improves the results in all cases except for RTE with the largest margin on STS-B.
Table 4: Comparison of best embedding-based methods (in bold) with methods from the literature onthe Vandation Set of the GLUE benchmarkMethod	Score	CoLA	MNLI-m	MRPC	QNLI	QQP	RTE	SST-2	STS-B	WNLIELMo (Peters et al., 2018)	68.7	44.1	68.6	76.6	71.1	86.2	53.4	91.5	70.4	56.3DistilBERT (Sanh et al., 2020a)	77.0	51.3	82.2	87.5	89.2	88.5	59.9	91.3	86.9	56.3MobileBERT (Sun et al., 2020)	—	51.1	84.3	88.8	91.6	70.5	70.4	92.6	84.8	—CBOW (Wasserblat et al., 2020)	—	10.0	—	—	—	—	—	79.1	—	—BiLSTM (Wasserblat et al., 2020)	—	10.0	—	—	—	—	—	80.7	—	—Hybrid (Mai et al., 2019)	—	—	—	—	—	—	—	79.6	63.4	—Word2rate (Phua et al., 2021)	—	—	—	—	—	—	—	65.7	53.1	—Bidi. Hybrid + MLP (ours)	68.0	23.3	66.6	80.9	72.6	87.2	61.0	84.0	76.9	59.2Table 5: Number of parameters and inference time of the models. Inference time is measured asencoding speed without gradient computation on an NVIDIA A100-SXM4-40GB cardModel	# Parameters	Encoding speed (sentences / second)ELMo	94M	1.1kBERT-base	109M	4.6kDistilBERT-base	66M	9.2kMobileBERT	25M	5.5kTinyBERT (4 layer)	14M	30.0kBidi. CMOW/CBOW-Hybrid	37M	30.0k
Table 5: Number of parameters and inference time of the models. Inference time is measured asencoding speed without gradient computation on an NVIDIA A100-SXM4-40GB cardModel	# Parameters	Encoding speed (sentences / second)ELMo	94M	1.1kBERT-base	109M	4.6kDistilBERT-base	66M	9.2kMobileBERT	25M	5.5kTinyBERT (4 layer)	14M	30.0kBidi. CMOW/CBOW-Hybrid	37M	30.0k5	Discussion and Related WorkKey Results We have shown that BERT can be distilled into efficient matrix embedding modelsduring pretraining by emitting intermediate representations. We have further introduced a bidirectionalcomponent and a separate two-sequence encoding scheme for CMOW-style models. We haveobserved that the general distillation approach, i. e., using the BERT teacher only during pretraining,leads to results that are oftentimes even better than those achieved with task-specific distillation.
Table 6: Hyperparameter search space and optimization methodHyperparameter	Range	Opt. methodLearning rate	— General Distillation — {10-3, 5 ∙ 10-4,10-4,5 ∙ 10-5,10-5}	grid searchWarmup steps	{0, 500}	grid searchEmbedding dropout	{0, 0.1}	grid searchHidden unit dropout	{0.2}	fixedBatch size	{1,8,32,64,128,256}	manualLearning rate	— Task-specific Distillation — {10-3, 5 ∙ 10-4,10-4,5 ∙ 10-5,10-5,5 ∙ 10-6}	grid searchEmbedding type	Hybrid, CMOW, CBOW	grid searchEmbedding initialization	random, pretrained	grid searchDiffCat	true, false	grid searchBidirectional	true, false	grid searchClassifier	Linear Probe, MLP, CNN, BiLSTM	grid searchB Discussion of Hyperparameters and Loss Functions forDistillationWe list hyperparameter search spaces along with their optimization methods in Table 6. For theexperiments on data augmentation and using only soft loss, we keep the configurations of the bestmodels (See Table 7 and tune the learning rate, again. We optimize over all six initial learning rates,namely {10-3, 5 ∙ 10-4,10-4, 5 ∙ 10-5, and 10-5}. All initial learning rates decay linearly over thecourse of training. Note, we also experimented with using warmup steps versus no warmup for the
Table 7: Hyperparameter configurations for best-performing models by GLUE taskTable 8: Scores on the GLUE development set. Our best performing general distillation and task-specific distillation models are highlighted in bold font per task. References indicate sources ofscores. The ?-symbol indicates numbers on the official GLUE test set. CMOW/CBOW-Hybrid isabbreviated as ’Hybrid’.
Table 8: Scores on the GLUE development set. Our best performing general distillation and task-specific distillation models are highlighted in bold font per task. References indicate sources ofscores. The ?-symbol indicates numbers on the official GLUE test set. CMOW/CBOW-Hybrid isabbreviated as ’Hybrid’.
Table 9: Scores on the GLUE development set without DiffCat encoding										Task-Specific Distillation	Score	CoLA	MNLI-m	MRPC	QNLI	QQP	RTE	SST-2	STS-B	WNLI		—	task-specific finetuning (ours) —							Teacher BERT-base	78.9	57.9	84.2	84.6	91.4	89.7	67.9	91.7	88.0	54.9	—	task-specific distillation (ours) CBOW not pretrained					—			Linear probe	52.8	12.2	43.0	72.3	60.1	74.8	55.6	82.8	17.7	56.3MLP	53.2	13.0	46.3	71.3	59.7	76.9	54.5	82.9	17.5	56.3CNN	52.8	11.7	43.0	72.1	60.1	77.5	54.5	82.7	17.2	56.3BiLSTM	52.1	10.9	44.9	70.8	59.8	78.1	54.5	81.3	12.3	56.3	—	task-specific distillation (ours) CBOW pretrained —					—			Linear probe	52.4	11.0	43.2	72.1	58.8	74.8	54.9	82.5	14.0	60.6MLP	54.0	14.3	46.3	71.3	60.1	76.9	58.5	83.1	14.8	60.6CNN	53.0	12.0	43.5	71.6	59.2	77.5	55.2	82.6	18.8	56.3BiLSTM	50.8	0	44.9	71.3	59.4	78.0	54.0	81.0	12.0	56.3	—	task-specific distillation (ours) CMOW not pretrained —								Linear probe	53.7	13.8	45.3	72.1	62.5	80.9	53.4	84.1	15.2	56.3MLP	54.8	15.1	45.6	72.8	60.6	82.6	55.6	84.3	20.0	56.3CNN	54.6	13.4	45.6	72.3	61.2	82.6	56.3	86.8	15.0	57.8BiLSTM	53.2	16.7	44.9	72.1	64.8	80.6	54.2	82.9	7.9	54.9	—	task-specific distillation (ours) CMOW pretrained					一			
Table 10: Scores on the GLUE development set with DiffCat two-sequence encoding				Task-Specific Distillation	Score CoLA MNLI-m MRPC QNLI QQP RTE	SST-2	STS-B	WNLITeacher BERT-base	— task-specific finetuning (ours) — 78.9	57.9	84.2	84.6	91.4	89.7	67.9	91.7	88.0	54.9Linear probe	— task-specific distillation (ours) CBOW not pretrained — 53.8	11.5	46.6	72.8	62.2	76.7	52.7	83.5	22.0	56.3MLP	61.0	14.3	57.8	77.2	70.3	86.0	56.7	82.3	47.0	57.7CNN	53.8	11.2	51.5	75.0	65.8	81.3	53.1	82.3	7.2	56.3BiLSTM	48.4	11.5	31.8	68.3	66.8	63.2	56.7	83.5	1.5	56.3Linear probe	— task-specific distillation (ours) CBOW pretrained — 56.3	9.0	47.1	72.8	64.8	77.1	53.4	82.5	43.4	56.3MLP	63.8	14.0	61.7	78.2	70.8	86.2	57.4	83.8	66.0	56.3CNN	53.7	10.9	55.0	73.8	66.2	82.1	53.1	82.2	3.8	56.3BiLSTM	47.7	0	32.7	68.4	69.6	63.2	55.6	82.5	1.3	56.3Linear probe	— task-specific distillation (ours) CMOW not pretrained — 55.1	10.9	54.3	71.8	62.7	80.9	56.0	85.2	17.6	56.3MLP	63.2	14.2	61.9	75.5	72.4	86.3	55.2	83.7	62.7	56.3CNN	55.4	12.4	45.3	72.3	61.5	82.6	57.4	84.3	26.1	56.3BiLSTM	47.5	0	31.8	70.3	49.5	81.0	55.6	83.4	0	56.3Linear probe	— task-specific distillation (ours) CMOW pretrained — 56.3	22.4	48.4	72.5	61.3	81.9	54.5	83.9	24.2	57.7MLP	61.2	20.9	60.2	73.8	64.6	85.9	54.9	84.4	49.4	56.3CNN	53.4	18.5	40.6	71.8	58.2	68.3	54.9	85.4	26.9	56.3BiLSTM	49.7	0	32.7	68.3	67.2	82.9	57.0	82.5	0	56.3Linear probe	— task-specific distillation (ours) Hybrid not pretrained — 51.7	11.2	39.0	71.1	49.5	81.8	56.0	85.2	14.3	57.7
Table 11: Scores on the GLUE development set with DiffCat encoding and the bidirectionalCMOW/CBOW-Hybrid modelTask-Specific Distillation	Score	CoLA	MNLI-m MRPC QNLI	QQP	RTE	SST-2	STS-B	WNLI		—	task-specific finetuning (ours) —					Teacher BERT-base	78.9	57.9	84.2	84.6	91.4	89.7	67.9	91.7	88.0	54.9—	task-specific distillation (ours) Bidirectional Hybrid, not pretrained					—		Linear probe	53.5	11.6	39.4	71.6	64.3	82.5	56.3	85.0	14.6	56.3MLP	63.2	13.0	63.3	75.7	72.6	86.1	57.4	83.3	59.7	57.7CNN	52.7	14.5	37.3	71.3	60.8	86.4	55.2	85.8	6.6	56.3,	— task-specific distillation (ours) Bidirectional Hybrid, pretrained —					■		Linear probe	55.5	18.1	42.4	72.1	64.9	81.2	56.7	85.2	22.5	56.3MLP	64.6	23.3	61.8	75.0	72.0	86.3	59.9	82.9	62.9	57.7CNN	55.1	20.5	39.3	73.8	61.3	85.9	56.3	85.5	15.9	57.719Under review as a conference paper at ICLR 2022Table 12: Number of parameters without DiffCat encoding	CoLA, MRPC, QNLL QQH SST-2, RTE, WNLI	MNLI	STS-B	— task-specific distillation (ours) CBOW —		Linear probe	47,861,634	47,862,419	47,876,549一 only classifier	3,138	3,923	18,053
Table 12: Number of parameters without DiffCat encoding	CoLA, MRPC, QNLL QQH SST-2, RTE, WNLI	MNLI	STS-B	— task-specific distillation (ours) CBOW —		Linear probe	47,861,634	47,862,419	47,876,549一 only classifier	3,138	3,923	18,053MLP	48,647,498	48,648,499	48,666,517一 only classifier	789,002	790,003	808,021CNN	47,862,708	47,864,737	47,901,259一 only classifier	4,212	6,241	42,763BiLSTM	53,704,002	53,705,027	53,723,477一 only classifier	5,845,506	5,846,531	5,864,981	— task-specific distillation (ours) CMOW —		Linear probe	23,932,386	23,933,171	23,947,301一 only classifier	3,138	3,923	18,053MLP	24,718,250	24,719,251	24,737,269一 only classifier	789,002	790,003	808,021CNN	23,933,460	23,935,489	23,972,011一 only classifier	4,212	6,241	42,763BiLSTM	24,853,978	24,854,371	35,022,869一 only classifier	924,730	925,123	110,936,21
Table 13: Number of parameters with DiffCat two-sequence encoding	CoLA, MRPC, QNLL QQR SST-2, RTE, WNLI	MNLI	STS-BLinear probe	— task-specific distillation (ours) CBOW — 47,867,906	47,870,259	47,912,613一 only classifier	9,410	11,763	54,117MLP	50,215,498	50,216,499	50,234,517一 only classifier	2,357,002	2,358,003	2,376,021CNN	47,865,932	47,869,313	47,930,171一 only classifier	7,436	10,817	71,675BiLSTM	147,477,458	147,479,811	147,522,165一 only classifier	99,618,962	99,621,315	99,663,669Linear probe	— task-specific distillation (ours) CMOW — 23,938,658	23,941,011	23,983,365一 only classifier	9,410	11,763	54,117MLP	26,286,250	26,287,251	26,305,269一 only classifier	2,357,002	2,358,003	2,376,021CNN	23,936,684	23,940,065	24,000,923一 only classifier	7,436	10,817	71,675BiLSTM	123,548,210	123,550,563	123,592,917一 only classifier	99,618,962	99,621,315	99,663,669Linear probe	— task-specific distillation (ours) Hybrid — 24,427,202	24,429,603	24,472,821一 only classifier	9,602	12,003	55,221
