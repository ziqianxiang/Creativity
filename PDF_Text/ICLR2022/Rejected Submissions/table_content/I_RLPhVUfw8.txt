Table 1: State-of-the-art comparison on PASCAL-5i and COCO-20i benchmark in terms of mIoU(higher is better). In each case, the best two results are shown in magenta and cyan font, respectively.
Table 2: Performance comparison (mIoU,higher is better), when performing cross-dataset evaluation from COCO-20i to PAS-CAL. When using the same ResNet50 back-bone, our approach achieves significant im-provements for both 1-shot and 5-shot settings,with absolute gains of 5.8 and 11.3 mIoU overRePRI (Boudiaf et al., 2021).
Table 3: Performance of different kernels on thePASCAL-5i and COCO-20i benchmarks. No-tably, both the Exponential and SE kernels signif-icantly outperform the linear kernel, confirmingthe need for a flexible learner. Measured in mIoU(higher is better). Best results are in bold.
Table 4: Analysis of learning the GP output space(GPO) and incorporating covariance (Cov). Per-formance in mIoU (higher is better). Best resultsare in bold._____________________________________Cov	GPO	PASCAL-5i		COCO-20i		∆		1-shot	5-shot	1-shot	5-shot			-62∏-	^699-	41.7	51.2	-00^X		62.5	71.8	43.8	53.7	1.7	X	61.7	72.7	43.1	54.2	1.7X	X	62.5	72.6	44.7	55.0	2.5Table 5: Performance of different multilevel configura-tions of the dense GP few-shot learner on the PASCAL-5i and COCO-20i benchmarks. Measured in mIoU(higher is better). Best results are in bold.
Table 5: Performance of different multilevel configura-tions of the dense GP few-shot learner on the PASCAL-5i and COCO-20i benchmarks. Measured in mIoU(higher is better). Best results are in bold.
Table 6: The results of our approach in a COCO-20i to PASCAL transfer experiment (mIoU, higheris better). Following Boudiaf et al. (2021), the approach is trained on a fold of COCO-20i trainingset and tested on the PASCAL validation set. The testing folds are constructed to include classes notpresent in the training set, and thus not the same as PASCAL-5i.
Table 7: The classes used for testing in the COCO-20i to PASCAL transfer experiment, as proposedby Boudiaf et al. (2021). This split is different from that of PASCAL-5i in order to avoid overlapbetween the training and testing classes.
Table 8: 1 to 10 -shot PASCAL-5i results.
Table 9: 1 to 10 -shot COCO-20i results.
Table 11: Per-foldresults on COCO-20iMethod	1-Shot 200	201	202	203	Mean	5-Shot 200	201	202	203	MeanDENet (ResNet50)	■42.9	45.8	42.2	40.2	-428	■45.4	44.9	41.6	40.3	43.0PFENet (ResNet50)	36.8	41.8	38.7	36.7	38.5	40.4	46.8	43.2	40.5	42.7RePri (ResNet50)	31.2	38.1	33.3	33.0	34.0	38.5	46.2	40.0	43.6	42.1ASGNet (ResNet50)	-	-	-	-	34.6	-	-	-	-	42.5SCL (ResNet101)	36.4	38.6	37.5	35.4	37.0	38.9	40.5	41.5	38.7	39.9SAGNN (ResNet101)	36.1	41.0	38.2	33.5	37.2	40.9	48.3	42.6	38.9	42.7DGPNet (ResNet50)	43.6 ± 0.5	47.8 ± 0.8	44.5 ± 0.8	44.2 ± 0.6	45.0 ± 0.4	54.7 ± 0.7	59.1 ± 0.5	56.8 ± 0.6	54.4 ± 0.6	56.2 ± 0.4DGPNet (ResNet101)	45.1 ± 0.5	49.5 ± 0.8	46.6 ± 0.8	45.6 ± 0.3	46.7 ± 0.3	56.8 ± 0.8	60.4 ± 0.9	58.4 ± 0.4	55.9 ± 0.4	57.9 ± 0.3B Detailed AnalysisWe present results from two additional experiments. First, the size of the local covariance region isanalyzed. Then, we investigate the impact of freezing the backbone during episodic training.
Table 12: Performance for different configurations of covariance windows on the PASCAL-5i andCOCO-20i benchmarks. Measured in mIoU (higher is better).
Table 13: Comparison between freezing the backbone and fine-tuning it with a low learning rate.
Table 14: Additional baseline experiments to validate the effect of the DGP-module.
Table 15: Kernel hyperparameter sensitivity analysis for the SE-kernel, Which is adopted in the mainpaper. The sensitivity analysis is based on the full, final approach With a ResNet50 backbone andperformed on the 5-shot setting in PASCAL-5i . The hyperparameters values are chosen to coverone order of magnitude of hyperparameter values, centered at the values adopted in the main paper.
Table 16: Runtimes of the different functions in our approach, measured in milliseconds (ms). Tim-ings are measured in evaluation mode on 512 × 512 sized images from COCO-20i.
Table 17: All neural network blocks used by our approach. The rightmost column shows the di-mensions of the output of each block, assuming a 512 × 512 input resolution. The image encoder isfrom He et al. (2016) and the decoder from Yu et al. (2018). The BottleNeck and BasicBlockblocks are from He et al. (2016), and the CAB and RRB blocks from Yu et al. (2018). See their worksfor additional details.
