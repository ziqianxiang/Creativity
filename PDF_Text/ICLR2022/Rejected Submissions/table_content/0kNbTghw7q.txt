Table 1: Ratio between image andlatent distance by Eq. 8 to derive newz*. -inv means using inverse form.
Table 3: Post-training latent sampling improvement with two architectures on STL-10.
Table 2: Components of our 5 vari-ant methods in Sec. 4.1.
Table 4: The results of AdVLatGAN-qua and AdVLatGAN-qua+ for CIFAR10.
Table 5: The results of AdvLatGAN-qua and AdvLatGAN-qua+ for STL10Architecture	Inception Score(↑)			Frechet Inception DistanCeQ)			vanilla	AdvLatGAN-qua	AdvLatGAN-qua+	vanilla	AdvLatGAN-qua	AdvLatGAN-qua+DCGAN	7.18 ± 0.09	7.33 ± 0.07	7.79 ± 0.06	61.2 ± 1.2	56.3 ± 1.0	54.6 ± 1.3WGAN	6.51 ± 0.07	7.62 ± 0.04	8.16 ± 0.27	73.0 ± 0.2	51.0 ± 0.6	49.1 ± 1.2WGAN-GP	8.86 ± 0.05	8.90 ± 0.05	10.32 ± 0.34	37.4 ± 0.4	34.2 ± 0.9	26.7 ± 1.1SNGAN	8.49 ± 0.09	8.63 ± 0.08	9.37 ± 0.05	36.8 ± 0.4	34.5 ± 0.21	30.9 ± 0.4LSGAN	7.08 ± 0.12	7.16 ± 0.15	7.55 ± 0.18	62.9 ±2.2	58.5 ±1.3	57.1 ± 1.5WGAN-div	8.82 ± 0.02	9.00 ± 0.01	10.68 ± 0.17	37.7 ±0.2	32.0 ±0.6	24.3 ± 1.0change is illustrated in Fig. 8. We respectiVely conduct the experiments for the trained DCGANmodel and WGAN-GP (Gulrajani et al., 2017) model, with single-step iteration constraint using '∞and iterated step size is set to 0.03. To better reflect changes in the distribution, We only shoW thefirst 30 iterations that can reflect larger changes. For WGAN test, Fig. 8 shows the transformed latentdistribution which contains more peaks and valleys and is different from the raw Gaussian.
Table 6: FID and JSD results on CIFAR-10.
Table 7: FID and JSD results on STL-10.												Metrics	Models	overall	airplane	bird	car	cat	deer	dog	horse	monkey	ship	truckFID(；)	MSGAN	67.849	92.021	125.723	108.434	118.938	111.784	133.680	140.486	121.907	101.232	101.059	AdvLatGAN-div+	65.349	92.169	124.155	109.3590	118.385	105.966	132.681	139.372	117.149	95.298	99.590JSDQ)	MSGAN	0.00661	0.02968	0.02818	0.03177	0.03849	0.03522	0.03107	0.03996	0.03498	0.02681	0.03157	AdvLatGAN-div+	0.00423	0.02852	0.02717	0.03638	0.03518	0.03773	0.03132	0.03198	0.03891	0.02239	0.02751density(↑)	MSGAN	0.332	0.192	0.264	^^0.089^^	0.598	0.399	0.397	0.150	0.251	0.251	0.116	AdvLatGAN-div+	0.443	0.235	0.328	0.111	0.733	0.456	0.429	0.194	0.353	0.165	0.126coverage(↑)	MSGAN	0.361	0.383	0.364	^^0.169^^	0.418	0.306	0.289	0.271	0.280	0.329	0.226	AdvLatGAN-div+	0.396	0.408	0.329	0.226	0.508	0.424	0.309	0.390	0.335	0.395	0.395AdvLatGAN-qua for Quality Improvement. We conduct experiments on CIFAR-10 and STL-10,using the mainstream architectures including DCGAN, WGAN, WGAN-GP, SNGAN, LSGAN,WGAN-div (Wu et al., 2018) and ACGAN (Kang et al., 2021). We do not include ACGAN in theSTL-10 setting because it does not work on unlabeled dataset. We adversarially train GAN using aquality-enhanced strategy previously mentioned as AdvLatGAN-qua, and the results show that theproposed method can greatly improve the quality of generated images. IS and FID are adopted toevaluate the performance of the model. We conduct only one latent iteration to save computationcost per generator step, and more details will be given in the appendix. We also try to use implicittransform after training to improve the generation quality further. The result is shown in Table 4 andTable 5. It can be seen that the proposed method achieves a significant improvement in generative
Table 8: Differences from Other AlgorithmsViewpoint	Method	Different point	Detailed Description				Theirs	Ours	EvolGAN	Guide of the transform	Quality estimator Koncept512	D∙G		Achieving method	Evolutionary Algorithm	I-FGSM iterationsLatent Exploration	Tarsier	Guide of the transform	Quality estimator Koncept512	D∙G		Achieving method	Diagonal Covariance Matrix Adaptation	I-FGSM iterations		Targeted Task	Super-resolution generation tasks	General generation tasks	AE-OT-GAN	Timing for the transform	Before training G and D	After training G and D		Achieving method	First train an Auto-Encoder to learn a latent distribution then use optimal trasport to achieve the transform	I-FGSM iterations		Calculating cost	Need to train an Auto-Encoder for finding the latent distribution	No extra networks are trained (guided by G and D)	DDLS	Basis of theoretical analysis	Energy-based theory	General GAN theory		Achieving method	Markov Chain Monte Carlo	I-FGSM iterationsAdvLatGAN-qua	vanilla GAN	Z in the training of D	Samples from Gaussian	Transformation of the original samplingAdvLatGAN-div	MSGAN	Z pair used in ms-regularization term	Individually sampled from Gaussian	Randomly choose Z1, then transform Z1 to get Z2 , forming a pairAdversarial Training	Common differences	Objects to which perturbations are added	x	ZTable 9: Experimental comparisons over GAN with adversarial training evaluated by InceptionScore.
Table 9: Experimental comparisons over GAN with adversarial training evaluated by InceptionScore.
Table 10: Experimental comparisons over GAN with adversarial training evaluated by FrechetInception Distance.
Table 11: Experimental comparisons over GAN with adversarial training evaluated by InceptionScore.
Table 12: Experimental comparisons over GAN with adversarial training evaluated by Frechet Inception Distance.				Framework	ASGAN	RobDis	AdvLatGAN-qua	AdvLatGAN-qua+DCGAN	45.3 ± 0.4	41.9 ± 0.4	41.7 ± 1.0	40.1 ± 1.5WGAN	89.7 ± 1.3	1.3 ± 0.4	27.3 ± 0.7	27.2 ± 0.5WGAN-GP	32.3 ± 0.3	24.5 ± 0.1	22.6 ± 0.4	18.3 ± 1.1SNGAN	26.7 ± 0.5	50.9 ± 2.7	22.3 ± 0.5	21.9 ± 0.3WGAN, WGANGP and SNGAN. Inception Score and FreChet Inception Distance are adopted as theevaluation metrics. The experimental results are presented in Table 11 and Table 12.
Table 13: Architecture of generator G.		Layer	Output size	ActivationLinear	100	ReLuLinear	200	ReLuLinear	100	ReLuLinear	2	-Table 14: Architecture of discriminator D.		Layer	Output size	ActivationLinear	100	ReLuLinear	200	ReLuLinear	100	ReLuLinear	1	-E.2 Details for Ring Synthesizing Experiments .
Table 14: Architecture of discriminator D.		Layer	Output size	ActivationLinear	100	ReLuLinear	200	ReLuLinear	100	ReLuLinear	1	-E.2 Details for Ring Synthesizing Experiments .
Table 17:	The hyperparameter setting in CIFAR-10 adversarial training experiment.
Table 18:	Hyperparameter setting in adversarial training Experiment on STL-10.
Table 15: Architecture of generator G in CI-FAR10 experiment.
Table 19: Architecture of generator G in STL-10 experiment.
Table 16: Architecture of discriminator D inCIFAR10 experiment.	Layer	Output sizeConv2d	16 × 16 × 64LeakyReIU	16 × 16 × 64Conv2d	8 × 8 × 128LeakyRelu	8 × 8 × 128BatChNorm2d	8 × 8 × 128Conv2d	4 × 4 × 256LeakyRelu	4 × 4 × 256BatChNorm2d	4 × 4 × 256Conv2d	2 × 2 × 512LeakyRelu	2 × 2 × 512BatchNorm2d	2 × 2 × 512faltten	2048linear	1Table 20: ArChiteCture of diSCriminator D in	STL-10 exPeriment.	Layer	Output sizeConv2d	24 × 24 × 64
Table 20: ArChiteCture of diSCriminator D in	STL-10 exPeriment.	Layer	Output sizeConv2d	24 × 24 × 64LeakyReIU	24 × 24 × 64Conv2d	12 × 12 × 128LeakyRelu	12 × 12 × 128BatchNorm2d	12 × 12 × 128Conv2d	6 × 6 × 256LeakyRelu	6 × 6 × 256BatchNorm2d	6 × 6 × 256Conv2d	3 × 3 × 512LeakyRelu	3 × 3 × 512BatchNorm2d	3 × 3 × 512faltten	4608linear	1G.2 Details for STL-10 Experiment.
