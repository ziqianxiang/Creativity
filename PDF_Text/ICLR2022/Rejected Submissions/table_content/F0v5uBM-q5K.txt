Table 1: Average number of bit flips per signedMAC. The b-bit multiplier inputs are drawn uni-formly from [-2b-1 , 2b-1) and its bacc = 2b bitoutput is summed with the B-bit number in the FF.
Table 2: PTQ: Classification accuracy [%] of ResNet-50 on ImageNet (FP 76.11%). The base-lines (Base.) use equal bit widths for weights and activations. This bit width determines the power P,reported in first column in units of Giga bit-flips. The power is calculated as Pmuult + Paucc (Eqs. (3),(4))times the number of MACs in the network. In each row, our variant PANN is tuned to work at thesame power budget, for which We choose the optimal bχ and R using Alg. 1.
Table 3: QAT: Comparison with LSQ.
Table 4: QAT: Comparison with multiplier-free meth-ods. Classification accuracy [%] of ResNet-20 on CIFAR-10. The top row specifies weight/activation bit widths,and the addition factor is specified in parentheses.
Table 5: Static power vs. dynamic power in [%]. We can see that overall the dynamic powerconstitutes a major portion of the total power.
Table 6: Required accumulator bit width. Here we compute the bit width required for the accumu-lator, according the largest linear layer in ResNets (which is 3x3x512). For example, in case of 2 bitwidth activations and weights, we might use an accumulator with 16 bits and not 32 bits. In the lasttwo rows we report the power save in [%] when switching to unsigned arithmetic. Just like in the 32bit accumulator case, when working with lower bit width accumulators, we can obtain a significantreduction in power by switching to unsigned arithmetic.
Table 7: Classification accuracy [%] of ResNet-18 on ImageNet (FP: 69.77%). The baselines(Base.) use equal bit widths for weights and activations (leftmost column). The bit width determinesthe power P , which we specify in units of Giga bit-flips. The power is calculated as Pmuult + Paucc(Eqs. (3),(4)) times the number of MACs in the network. (1.82 × 109 in ResNet-18). In each row,our variant is tuned to work at the same power budget, for which we choose the optimal bx and Rusing Alg. 1.
Table 8: Classification accuracy [%] of Mobilenet-V2 on ImageNet (FP: 71.91%). The baselines(Base.) use equal bit widths for weights and activations (leftmost column). The bit width determinesthe power P , which we specify in units of Giga bit-flips. The power is calculated as Pmuult + Paucc(Eqs. (3),(4)) times the number of MACs in the network. (0.33 × 109 in MobileNet-V2). In each row,our variant is tuned to work at the same power budget, for which we choose the optimal bx and Rusing Alg. 1.
Table 9: Classification accuracy [%] of VGG-16bn on ImageNet (FP: 73.35%). The baselines(Base.) use equal bit widths for weights and activations (leftmost column). The bit width determinesthe power P, which we specify in parentheses in units of Giga bit-flips. The power is calculated asPmuult + Paucc (Eqs. (3),(4)) times the number of MACs in the network. (15.53 × 109 in VGG-16bn).
Table 10: PANN for QAT. Here we report more results of PANN for quantization aware training. Inparentheses we report the classification accuracy [%] of LSQ on Imagenet, where both activationsand weights are quantized to 2,3 or 4 bits. As for PANN, we follow Alg. 1 to calculate the optimalactivation bit width and addition factor.
Table 11: QAT: Comparison with multiplier-free methods. Classification accuracy [%] ofmultiplier-free methods on CIFAR-100. The top row specifies weight/activation bit widths, and theaddition factor is specified in parentheses.
Table 12: QAT: Comparison with multiplier-free methods. Classification accuracy [%] ofmultiplier-free methods on the MHEALTH dataset Banos et al. (2014). The top row specifiesweight/activation bit widths, and the addition factor is specified in parentheses.
Table 13: Hyper-parameters used in LSQ. When using pure LSQ as the baseline approach, wequantize both the weights and the activations to the same bit width as specified in the second column(bx/bw). Then, when applying PANN, we keep the exact training regime and the quantized activations,and only change the quantized weights to be calculated by PANN. Here we report the optimal bitwidth for the activations and the corresponding addition factor (Alg. 1).
Table 14: Runtime memory footprint of PANN. We report the increase in the memory requiredto store the weights and activations, when using PANN. Each row specifies a power budget, corre-sponding to a bx bit width unsigned MAC. We follow Alg. 1 to find the optimal bit width bx andthe additions factor R, which is equal to the latency increase. We measure the maximal value ofadditions per neuron which defines the required number of bits for storing the weights (bR). Then,we calculate the increase in weights memory footprint as bR/bx .
Table 15: Hardware-accuracy trade-off. Here we analyze the run-time memory footprint andlatency increase for all different values of bx and R that lead to the same power of a 2-bit unsignedMAC (blue curve in Figure 3(a)). For each setting we measure the classification accuracy of ResNet-50 on ImageNet. Here we use ACIQ (Banner et al., 2019) for quantizing the activations. The baseline(pure ACIQ) accuracy is 0.20% (Table 2, third column, last row).
