Table 1: Examples of true negative (the first line) and false negatives (the second and third line).
Table 2: Comparisons between our proposed methods and the previous strong pre-trained modelsunder small and base setting on the dev and test set of GLUE tasks. STS is reported by Spearmancorrelation, CoLA is reported by Matthew’s correlation, and other tasks are reported by accuracy.
Table 3: Results on the SQuAD dev set.
Table 4: Comparisons with public methods on GLUE test sets. The public results are from BERT(Devlin et al., 2019), SpanBERT (Joshi et al., 2020), and ELECTRA (Clark et al., 2019).
Table 5: Comparative studies of variants on GLUE dev sets based on small models. The first blockcompare the word-level regularization and sentence-level regularization, respectively. The secondblock shows the results of HC methods based on WordNet and Word2Vec embedding, respectively.
Table 6: Robustness evaluation on the SQuAD dataset. Ori. represents the results of original datasetderived from the SQuAD 1.1 dev set by TextFlint (Wang et al., 2021) while Trans. indicates thetransformed one. The assessed models are the small models from Table 3.
Table 7: Results of BERT methods under base and large setting on the GLUE dev sets. STS isreported by Spearman correlation, CoLA is reported by Matthew’s correlation, and other tasks arereported by accuracy.
Table 8: Statistics of the hard corrections under base and large settings on the wikitext-2-raw-v1corpus. Checkpoint means the checkpoint saved at the specific training steps (%).
