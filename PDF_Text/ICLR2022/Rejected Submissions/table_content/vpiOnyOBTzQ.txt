Table 1: Datasets. In L-V and 3-body OOD test sets, at least one domain parameter is outside of theparameter range used for training.
Table 2: Number experiments with phase space data. Each experiment corresponds to a distinctconfiguration of hyperparameters.
Table 3: Pendulum hyperparameters	MLP	MLP-SD	VAE	VAE-SD	LSTMInput Size			10, 50		Output Size			1,10		Hidden Layers			[400, 200]		50,100,200Latent Size			4, 8, 16		-Nonlinearity			Leaky ReLU		SigmoidNum. Layers	-	-	-	-	1,2,3Learning rate			10-3		Batch size	16, 32	16	16, 32	16	16, 64Sched. patience	20, 30, 40	20,30	20	20	30Sched. factor	0.3	0.3	0.3	0.3	0.3Gradient clipping	No	1.0	1.0		Layer norm (latent)	No	No	Yes	Yes	NoTeacher Forcing	-	-	-	-	PartialDecoder y	-	-	10-3,10-4,10-5	10-3,10-4	-Sup. scaling	-	Linear	-	Linear	-Supervision δ	-	0.1, 0.2, 0.3	-		0.01, 0.1, 0.2	-# of experiments	72	72	72	72	72Table 4: Lotka-Volterra hyperparameters
Table 4: Lotka-Volterra hyperparameters	MLP	MLP-SD	VAE	VAE-SD	LSTMInput Size			50		Output Size			10		Hidden Layers			[400, 200]		50,100Latent Size			8, 16, 32		-Nonlinearity			Leaky ReLU		SigmoidNum. Layers	-	-	-	-	1,2,3Learning rate	10-3,10-4	10-3, 10-4	10-3, 10-4	10-3	10-3Batch size	16, 32, 64	16, 32	16, 32	16	10, 64, 128Sched. patience	20, 30	20, 30	20	20	20, 30Sched. factor	0.3, 0.4	0.3	0.3	0.3	0.3Gradient clipping	No	No	0.1, 1.0	0.1, 1.0	NoLayer norm (latent)	No	No	No	No	NoTeacher Forcing	-	-	-	-	Partial, NoDecoder y	-	-	10-4, 10-5, 10-6	10-4,10-5,10-6	-Sup. scaling	-	Linear	-	Linear	-Supervision δ	-	0.1, 0.2, 0.3	-	0.01, 0.1, 0.2, 0.3	-# of experiments	72	72	72	72	7216
Table 5: 3-body system hyperparameters	MLP	MLP-SD	VAE	VAE-SD	LSTMInput Size			50		Output Size			10		Hidden Layers		[400, 200]			50,100Latent Size		8, 16, 32			-Nonlinearity		Leaky ReLU			SigmoidLearning rate	10-3,10-4	10-3, 10-4	10-3,10-4	10-3	Batch size	16, 32	16	16	16	16, 64, 128Sched. patience	30, 40, 50, 60	30, 40, 50, 60	30, 40, 50, 60	30, 40, 50, 60	20, 30Sched. factor	0.3, 0.4	0.3	0.3, 0.4	0.3, 0.4	0.3Gradient clipping	No	No	No	No	NoLayer norm (latent)	No	No	No	No	NoDecoder y	-	-	10-5,10-6	10-5, 10-6	-Sup. scaling	-	Linear	-	Linear	-Supervision δ	-	0.05, 0.1, 0.2, 0.3	-	0.1, 0.2	-# of experiments	96	96	96	96	96B.2	Video PendulumB.2.1	CNN-VAE ModelEncoder has 4 layers convolutional layers with 32, 32, 64 and 64 maps respectively. The filter size is
Table 6: Video pendulum hyperparameters for CNN-VAE models	CovnVAE	CovnVAE-SDLatent Size	4, 8, 16	8,16Decoder γ	0.01, 0.1, 1	0.01, 0.1, 1VAE recursions	1,2,4,8	4, 8Supervision δ	-	0.01,0.1,1# of experiments	36	36B.2.2	RSSM MODELSFor the RSSM model we follow the architecture parameters as described in Hafner et al. (2018) &Saxena et al. (2021). For training we use sequences of 100 frames and batch size 100. All modelswere trained for 300 epochs with a learning rate of 10-3 and an Adam optimizer (bɪ = 0.9 andb2 = 0.999). During testing the model uses 50 frames as context (input). The parameters We tuneappear in Table 7.
Table 7: video pendulum hyperparameters for RSSM modelsRSSM RSSM-SDBatch Size		50, 100Decoder std.		1.0, 2.0Train Input Length		50, 100Supervision δ	-	0.01, 0.1, 1Seeds	#3	#1# of experiments	24	24C Phase space resultsTest-set OODTest-Set OOD Test-setEasy	Hard1.00.9LU< 0.80.70.6ιe-ι Lotka-VblterraTest-set OOD Test-set OOD Test-setEasy	Hard
Table 8: MAE (×102) of phase space experiments at 200 time-steps. Average and standarddeviation of 5 best models.
Table 9: Models that diverge in at least one trajectory. Percentage out of the top 5 models selectedby validation accuracy.
Table 10: Model comparison in video pendulum. Metrics are calculated between ground truth andprediction of the models at exactly 800 timesteps in the future.
