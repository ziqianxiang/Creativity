Table 1: Relevant case study for cloud resource tuningRL ArtifactDescriptionState State Statistics Threshold Action Reward Parametric Boundary Initial Action Error Margin Acceptable Steps Computational Complexity	CPU utilization metrics Median value of CPU utilization 90% of CPU utilization Resource configuration set points: (# of vCPUs, Memory Size (GB)) Negative of total time steps required to satisfy the threshold condition Polygon defined by the parametric endpoints (6, 14): Arbitrarily assigned WLOG 5% 400 Polynomial timeTable 2: Five different AWS EC2 resource pairs used in training action spaceEC2 Type of VCPUs Memory Size (GB)small t3a	2	2medium t3a	2	4large t3a	2	8xlarge t3a	4	162xlarge t3a	8	32As shown in Table 1, the objective of the RL agent for this case study is to quickly reduce high CPUutilization below a pre-assigned threshold for a given workload. In most infrastructure/cloud resourcetuning technologies [55], CPU utilization represents a key metric. Therefore, the state space for thisRL case study consists of virtual machine CPU utilization (%) metrics and the action space is definedby the VM resource set points, i.e., (# of vCPUs, Memory Size (GB)). The RL agent uses AWS boto3SDK [2] to manipulate actions and AWS CloudWatch [3] for state space monitoring. The reward isdefined as the number of time steps required by the agent to accomplish the objective multiplied by-1. The negative reward per time step was meant to push the agent to accomplish the task as fast aspossible.
Table 2: Five different AWS EC2 resource pairs used in training action spaceEC2 Type of VCPUs Memory Size (GB)small t3a	2	2medium t3a	2	4large t3a	2	8xlarge t3a	4	162xlarge t3a	8	32As shown in Table 1, the objective of the RL agent for this case study is to quickly reduce high CPUutilization below a pre-assigned threshold for a given workload. In most infrastructure/cloud resourcetuning technologies [55], CPU utilization represents a key metric. Therefore, the state space for thisRL case study consists of virtual machine CPU utilization (%) metrics and the action space is definedby the VM resource set points, i.e., (# of vCPUs, Memory Size (GB)). The RL agent uses AWS boto3SDK [2] to manipulate actions and AWS CloudWatch [3] for state space monitoring. The reward isdefined as the number of time steps required by the agent to accomplish the objective multiplied by-1. The negative reward per time step was meant to push the agent to accomplish the task as fast aspossible.
Table 3: Training action categorization based on the valuation vectorTraining Action	Categorysmall t3a medium t3a large t3a xlarge t3a 2xlarge t3a	dispensable dispensable indispensable indispensable indispensableTable 4: Rewards and ranks for different training actions. Some training action sets fail to satisfy theobjective. Therefore, the rewards and ranks for them are noted as noneTraining Action Set	Reward	Rank<small t3a, medium t3a, large t3a, xlarge t3a, 2xlarge t3a>	-21	3<medium t3a, large t3a, xlarge t3a, 2xlarge t3a>	-13	1<small t3a, large t3a, xlarge t3a, 2xlarge t3a>	-16	2<small t3a, medium t3a, xlarge t3a, 2xlarge t3a>	none	none<small t3a, medium t3a, large t3a, 2xlarge t3a>	none	none<small t3a, medium t3a, large t3a, xlarge t3a>	none	nonebetween different EC2 instance pairs are not uniform. While the Euclidean distance between smallt3a and medium t3a is equal to 2, that between xlarge t3a and 2xlarge t3a is 16.5. The non-uniformspacing for training action space is a considerable deterrent [31] for RL adoption. Second, in thiscase study, the transient CPU utilization (%) patterns have undergone a material change from xlarget3a (max 100%) to 2xlarge t3a (max 73%). This indicates the strong influence of 2xlarge t3a forthe given RL task. Indeed, we noticed 2xlarge t3a to be an indispensable training action. As shownin Table 3, a categorization of training actions can be inferred based on the valuation vector, V, asdescribed in Algorithm 2: two dispensable training actions are uncovered to be small t3a, medium
Table 4: Rewards and ranks for different training actions. Some training action sets fail to satisfy theobjective. Therefore, the rewards and ranks for them are noted as noneTraining Action Set	Reward	Rank<small t3a, medium t3a, large t3a, xlarge t3a, 2xlarge t3a>	-21	3<medium t3a, large t3a, xlarge t3a, 2xlarge t3a>	-13	1<small t3a, large t3a, xlarge t3a, 2xlarge t3a>	-16	2<small t3a, medium t3a, xlarge t3a, 2xlarge t3a>	none	none<small t3a, medium t3a, large t3a, 2xlarge t3a>	none	none<small t3a, medium t3a, large t3a, xlarge t3a>	none	nonebetween different EC2 instance pairs are not uniform. While the Euclidean distance between smallt3a and medium t3a is equal to 2, that between xlarge t3a and 2xlarge t3a is 16.5. The non-uniformspacing for training action space is a considerable deterrent [31] for RL adoption. Second, in thiscase study, the transient CPU utilization (%) patterns have undergone a material change from xlarget3a (max 100%) to 2xlarge t3a (max 73%). This indicates the strong influence of 2xlarge t3a forthe given RL task. Indeed, we noticed 2xlarge t3a to be an indispensable training action. As shownin Table 3, a categorization of training actions can be inferred based on the valuation vector, V, asdescribed in Algorithm 2: two dispensable training actions are uncovered to be small t3a, mediumt3a and three indispensable training actions to be large t3a, xlarge t3a, 2xlarge t3a.
