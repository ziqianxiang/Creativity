Table 1: Statistics of datasets.
Table 2: Summary of average classification accuracy ± one standard deviation (in percent) afterfiltering out the top and bottom 10% data on DGL datasetsMethod	Cora	Citeseer	PubMed	Coauthor- CS (1)	Coauthor- CS (2)	Coauthor- CS (3)GCN (hop = 2)	81.5 ± 0.2	71.6 ± 0.3	79.0 ± 0.3	90.7 ± 0.2	90.2 ± 0.2	89.7 ± 0.2GAT (hop = 2)	83.0 ± 0.5	71.1 ±0.9	77.6 ± 0.4	90.4 ± 0.3	90.3 ± 0.5	89.5 ± 0.4Tree-LSTM	81.9 ± 0.7	68.6 ± 1.0	76.6 ± 0.6	91.0 ± 0.2	90.8 ± 0.2	90.8 ± 0.2(hop = 2) APPNP (hop =	83.6 ± 0.5	71.6 ± 0.5	79.6 ± 0.2	91.8 ± 0.4	91.8 ± 0.3	91.4 ± 0.310) DAGNN (hop	84.0 ± 0.5	72.6 ± 0.5	79.6 ± 0.4	90.4 ± 0.3	90.3 ± 0.4	89.6 ± 0.6= 10) GTAN (hop =	83.4 ± 1.0	71.4 ± 0.4	79.4 ± 0.2	92.2 ± 0.3	92.4 ± 0.2	92.0 ± 0.310, ours) GTCN (hop =	84.5 ± 0.6	72.9 ± 0.5	79.2 ± 0.3	92.7 ± 0.1	92.5 ± 0.1	92.4 ± 0.210, ours)Table 3: Summary of average classification accuracy ± one standard deviation (in percent) on ogbn-arxiv dataset.
Table 3: Summary of average classification accuracy ± one standard deviation (in percent) on ogbn-arxiv dataset.
Table 4: Average training time per epoch (ms/epoch) on DGL datasets.
Table 5: Model performance in accuracy with and without nonlinear layers, tested on the Coradataset.
Table 6: Summary of average Macro-F1 score ± one standard deviation (in percent) after filteringout the top and bottom 10% data.
Table 7: Model performance in accuracy with different depths, tested on the Cora datasetMethod (Cora)	Depth = 2	Depth = 5	Depth = 10GCN	81.5 ± 0.2	78.1 ± 0.6	72.3 ± 1.4GAT	83.0 ± 0.5	80.1 ± 0.6	78.9 ± 0.9Tree-LSTM	81.9 ± 0.7	80.9 ± 0.7	80.4 ± 0.7GTAN (ours)	83.0 ± 0.4	84.3 ± 0.5	83.4 ± 1.0GTCN (ours)	83.3 ± 0.3	84.4 ± 0.6	84.5 ± 0.5is for the initial one-layer MLP, and the other is for the propagation layer. Corresponding dropoutvalues are set to (0.6, 0.6) for the Cora dataset, (0.8, 0.6) for the Citeseer dataset, (0.8, 0.5) for thePubmed dataset and (0.6, 0.2) for the Coauthor-CS dataset. The learning rate is 0.01 for the Cora,Citeseer and Coauthor-CS datasets, and 0.02 for the PubMed dataset. The weight decay is 5e-4 forthe Cora, Citeseer and PubMed datasets, and 5e-3 for the Coauthor-CS dataset. For our GTAN, wehave the same two dropouts as for the GTCN, which are set to (0.6, 0) for the Cora, Citeseer andPubMed datasets, and (0.2, 0.2) for the Coauthor-CS dataset. The learning rate is set to 0.01 for alldatasets. The weight decay is set to 5e-4 for the Cora, Citeseer and PubMed datasets, and 5e-3 forthe Coauthor-CS dataset.
Table 8: Model performance in accuracy with different depths, tested on the Citeseer datasetMethod (Citeseer)	Depth = 2	Depth = 5	Depth = 10GCN	71.6 ± 0.3	64.8 ± 1.0	58.8 ± 1.9GAT	71.1 ± 0.9	68.1 ± 0.9	66.6 ± 1.0Tree-LSTM	68.6 ± 1.0	65.8 ± 1.1	64.7 ± 1.2GTAN (ours)	71.5 ± 0.6	71.7 ± 0.6	71.4 ± 0.4GTCN (ours)	72.2 ± 0.7	72.8 ± 0.5	72.9 ± 0.5Table 9: Model performance in accuracy with different depths, tested on the PubMed datasetMethod (PubMed)	Depth = 2	Depth = 5	Depth = 10GCN	79.0 ± 0.3	76.7 ± 0.6	75.3 ± 0.9GAT	77.6 ± 0.4	76.9 ± 0.5	77.1 ± 0.7Tree-LSTM	76.6 ± 0.6	76.9 ± 0.4	77.3 ± 0.5GTAN (ours)	79.1 ± 0.4	79.3 ± 0.4	79.4 ± 0.2GTCN (ours)	78.5 ± 0.5	78.6 ± 0.5	79.2 ± 0.3Table 10: Model performance in accuracy with different depths, tested on the Coauthor-CS(3)datasetMethod (Coauthor-CS)	Depth = 2	Depth = 5	Depth = 10GCN	89.7 ± 0.2	87.1 ± 0.7	82.9 ± 0.8GAT	89.5 ± 0.4	85.9 ± 1.6	83.8 ± 0.6Tree-LSTM	90.8 ± 0.2	91.0 ± 0.1	90.8 ± 0.2
Table 9: Model performance in accuracy with different depths, tested on the PubMed datasetMethod (PubMed)	Depth = 2	Depth = 5	Depth = 10GCN	79.0 ± 0.3	76.7 ± 0.6	75.3 ± 0.9GAT	77.6 ± 0.4	76.9 ± 0.5	77.1 ± 0.7Tree-LSTM	76.6 ± 0.6	76.9 ± 0.4	77.3 ± 0.5GTAN (ours)	79.1 ± 0.4	79.3 ± 0.4	79.4 ± 0.2GTCN (ours)	78.5 ± 0.5	78.6 ± 0.5	79.2 ± 0.3Table 10: Model performance in accuracy with different depths, tested on the Coauthor-CS(3)datasetMethod (Coauthor-CS)	Depth = 2	Depth = 5	Depth = 10GCN	89.7 ± 0.2	87.1 ± 0.7	82.9 ± 0.8GAT	89.5 ± 0.4	85.9 ± 1.6	83.8 ± 0.6Tree-LSTM	90.8 ± 0.2	91.0 ± 0.1	90.8 ± 0.2GTAN (ours)	91.7 ± 0.2	91.7 ± 0.2	92.0 ± 0.3GTCN (ours)	92.1 ± 0.2	92.0 ± 0.2	92.4 ± 0.214Under review as a conference paper at ICLR 2022For the DAGNN model, we use 16-hop with 256 hidden units. The dropout is 0.2. The learning rateand weight decay are 0.005 and 0, respectively.
Table 10: Model performance in accuracy with different depths, tested on the Coauthor-CS(3)datasetMethod (Coauthor-CS)	Depth = 2	Depth = 5	Depth = 10GCN	89.7 ± 0.2	87.1 ± 0.7	82.9 ± 0.8GAT	89.5 ± 0.4	85.9 ± 1.6	83.8 ± 0.6Tree-LSTM	90.8 ± 0.2	91.0 ± 0.1	90.8 ± 0.2GTAN (ours)	91.7 ± 0.2	91.7 ± 0.2	92.0 ± 0.3GTCN (ours)	92.1 ± 0.2	92.0 ± 0.2	92.4 ± 0.214Under review as a conference paper at ICLR 2022For the DAGNN model, we use 16-hop with 256 hidden units. The dropout is 0.2. The learning rateand weight decay are 0.005 and 0, respectively.
