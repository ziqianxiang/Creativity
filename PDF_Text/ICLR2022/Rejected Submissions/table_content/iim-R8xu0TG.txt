Table 1: The empirical comparison between FitVid (with 302M parameters), GHVAE (Wu et al.,2021a) (with 599M parameters) and SVG (Villegas et al., 2019) (with 298M parameters). To preventFitVid from overfitting, we use augmentation for Human3.6M and KITTI. The green color highlightswhere FitVid achieved state-of-the-art result while the red color highlights otherwise.
Table 2: Comparison between FitVid and differentmethods for video prediction on action-free BAIRdataset (Ebert et al., 2017).
Table 3: SVG (Villegas et al., 2019) with andwithout augmentation. This table shows thatSVG does not benefit from augmentation be-cause it is underfitting to the original data (Vil-legas et al., 2017).
Table 4: Ablation study of FitVid on RoboNet.
Table 5: Zero-shot real robot performance. Weuse FitVid for planning future actions of a realrobot pushing an object to a goal location with notraining data from our setup. We train the modelon visually different data (RoboNet) and the datafrom a closer domain (from Wu et al. (2021a))with and without augmentation. While unable todirectly adapt from RoboNet to the new domain,the results illustrate that fine-tuning on similar dataand augmentation improve FitVid’s performance.
Table 6: Effect of FitVid’s encoder and decoder on prediction performance. As it can be seen in thispaper, by replacing SVG’s encoder and decoder with FitVid’s encoder and decoder, the predictionquality improves.
Table 7: Effect of the number of encoding and decoding blocks on the performance of FitVid. As itcan be seen in this table, more encoder and decoder blocks do not essentially result in better predictionquality and having two blocks in both encoder and decoder is a good balance between number ofparamters and the performance.
Table 8:	Hyper-parameters used for training FitVid. We used the same set of hyper-parameters acrossall experiments. As mentioned in Section 3.2, all of the hyper-paramters are fixed during the trainingand there is no scheduling.
Table 9:	FitVid Encoder Architecture. We are using the same encoding cells as NVAE (Vahdat &Kautz, 2020). The strides are always 1×1 except when down-sampling which has strides of 2×2.
Table 10: FitVid Dynamics Architecture. We are using a similar dynamics as Denton & Fergus(2018). The encoded output is first averaged across spatial dimension and then decoded into h-sizeusing a fully connected layer. Then, the dynamics are modeled by two LSTM layers. Finally, theoutput is mapped and reshaped to an image tensor before passing to the decoder. The posterior usesthe exact same architecture except that only has one LSTM layer.
Table 11: FitVid Encoder Architecture. We are using the same encoding and decoding cells asNVAE (Vahdat & Kautz, 2020). The strides are always 1×1. For up-sampling we use nearest neigh-bour. (bn) is batch-normalization (Ioffe & Szegedy, 2015), (swish) is the activation (Ramachandranet al., 2017), (s&e) is Squeeze and Excite (Hu et al., 2018). There is a skip connection from thebeginning of each cell to the end of it. There are also skip connections from each encoder block tothe corresponding decoder block (look at Figure 2). In these skip connections, the number of inputfilters will be matched by the output using a 1×1 convolution.
Table 12: To prevent FitVid from overfitting we use augmentation. First, at training time, we selecta random crop of the video before resizing it to the desired resolution (64×64) at the training time,called RandCrop. This processes crops all the frames of a given video to include a minimum ofC percent of the frame’s height. Then we use RandAugment (Cubuk et al., 2020) to improve theaugmentation. We use the same augmentation configuration for all the datasets. Per video, we use thesame randomization across all the frames.
Table 13: Used datasets and their licenses.
