Table 1: MAE comparison between synthetic and generated calcium signals. Results of (A) identity,ResNet and AGResNet generators trained with LSGAN objective, (B) AGResNet generatorstrained with different objectives and (C) neurons ordered by original annotation, firing rate, pairwisecorrelation and autoencoder reconstruction loss. We also trained a 1D variant of AGResNet as abaseline which disregards the neuron spatial structure. Lowest values marked in bold.
Table A.1: Trial information of mouse 1 in the virtual-corridor experiment across 4 days of training,which include the number of trials, average duration of each trial, total number of licks and the totalreward received by the mouse. The mouse achieved “expert” level by day 4 where it had a successrate of > 75% at the task. All data were recorded at a sampling rate of 24Hz. Note that the samemouse was used in the experiment.
Table A.2: The number of licks and rewards the 4 mice exhibit on day 1 and 4 in the virtual-corridorexperiment (see Section 2.1).
Table C.1: The hyper-parameters used for each objective formulation. num. dis update is thenumber of discriminator updates for every generator update, such procedure was introduced in op-timizing WGANGP Arjovsky et al. (2017). αG and αD denotes the learning rates of the generatorsand discriminators. λGP is the gradient penalty coefficient for WGANGP and DRAGAN and c isthe Gaussian variance hyper-parameter in DRAGAN.
Table C.2: The objective functions of the generator G and discriminator DY in GAN (Goodfellowet al., 2014), LSGAN (Mao et al., 2017), WGANGP (Arjovsky et al., 2017) and DRAGAN (Kodaliet al., 2017) formulations. The loss functions for F and DX are symmetric to G and DY shownabove. λGP denotes the gradient penalty coefficient in WGANGP and DRAGAN, is the [0, 1] linearinterpolation coefficient for WGANGP and c is the Gaussian standard deviation for DRAGAN.
Table E.1: Neuron ordering based on (a) original annotation, (b) firing rate, (c) average pairwisecorrelation and (d) autoencoder reconstruction loss with respect to the original annotation orderrecorded on Day 4 data. The physical location of each neuron is available in Figure E.1.
Table G.1: Cycle-consistent and identity loss in the test set of Mouse 1 recordings, where neu-rons were ordered by 1) original annotation, 2) firing rate 3) pairwise correlation and 4) autoen-coder reconstruction loss. We also trained a 1D variant of the model as an additional baseline(1D-AGResNet in the table) such that all spatial information of the neurons is disregarded. TheAGResNet generator architecture was used for G and F, and were optimized with LSGAN objec-tives. The lowest loss in each category is marked in bold. For reference, |X -Y | = 0.3674 ± 0.0236in the test set.
Table G.2: The average KL divergence between generated and recorded distributions of Mouse 1 in(a) pairwise correlation, (b) firing rate and (c) pairwise van Rossum distance. We trained AGResNetwith neurons ordered according to the following methods: 1) original annotation, 2) firing rate, 3)pairwise correlation and 4) autoencoder reconstruction loss. We also trained a 1D variant of themodel (denoted as 1D-AGResNet) such that all spatial information of the neurons is disregarded.
Table I.1: Cycle-consistent and identity loss of AGResNet on Mouse 2 recordings, where neuronswere ordered by 1) original annotation, 2) firing rate and 3) autoencoder reconstruction loss. Forreference, | X - Y | = 0.6057 ± 0.1146 in the test set. The lowest loss in each category marked inbold.
Table I.2: The average KL divergence between generated and recorded distributions of Mouse 2in (a) pairwise correlation, (b) firing rate and (c) population pairwise van Rossum distance. Wecompare AGResNet results with different neuron ordering including 1) original annotation, 2) firingrate and 3) autoencoder reconstruction loss. Note that we added the identity model (first row of eachsub-table) as a baseline where we should obtain perfect cycle reconstruction. Entries with the lowestvalue are marked in bold.
Table J.1: Cycle-consistent and identity loss of AGResNet on Mouse 3 recordings, where neuronswere ordered by 1) original annotation, 2) firing rate and 3) autoencoder reconstruction loss. Forreference, | X - Y | = 0.4764 ± 0.1520 in the test set. The lowest loss in each category marked inbold.
Table J.2: The average KL divergence between generated and recorded distributions of Mouse 3in (a) pairwise correlation, (b) firing rate and (c) population pairwise van Rossum distance. Wecompare AGResNet results with different neuron ordering including 1) original annotation, 2) firingrate and 3) autoencoder reconstruction loss. Note that we added the identity model (first row of eachsub-table) as a baseline comparison and should obtain perfect cycle reconstruction. Entries with thelowest value are marked in bold.
Table K.1: Cycle-consistent and identity loss of AGResNet on Mouse 4 recordings, where neuronswere ordered by 1) original annotation, 2) firing rate and 3) autoencoder reconstruction loss. Forreference, | X - Y | = 0.4383 ± 0.2354 in the test set. The lowest loss in each category marked inbold.
Table K.2: The average KL divergence between generated and recorded distributions of Mouse4 in (a) pairwise correlation, (b) firing rate and (c) population pairwise van Rossum distance. Wecompare AGResNet results with different neuron ordering including 1) original annotation, 2) firingrate and 3) autoencoder reconstruction loss. Note that we added the identity model (first row of eachsub-table) as a baseline comparison and should obtain perfect cycle reconstruction. Entries with thelowest value are marked in bold.
