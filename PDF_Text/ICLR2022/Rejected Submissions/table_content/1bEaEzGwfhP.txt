Table 1: Dataset statistics on CodeRevisions and WikiRevisions, average length is measuredby whitespace tokenizationsiveness. One the other hand, a perhaps more intuitive way taken from common Transformer-based(Vaswani et al., 2017) models would be to use cross-attention between all n previous documents,which is more expressive but prohibitively expensive when n is scaled upwards.
Table 3: Results on Edit Generation (BLEU), Edit Classification (measured with micro-F1), andConditional Edit Generation (measured Edit Perplexity = ePPL). Note that the ∆ symbol refers tothe change between the model’s non-message conditioned version in Table 2.
Table 4: Example generation when sampling with an edit model. We notice that the 2nd order modelis able perform a revert operation given the context fed through the edit-compressed sequence aboutthe previous revision, whereas the 1-order model although deleting its generated spam, generatessomething relatively unrelated. However we note that this reversion is not exact (likely due to theinformation loss during edit compression). This corresponds with our observations in our qualitativestudy (where likelihood of reverted edits is increased in the 2+ order models).
Table 5: Ablation on the impact of comments in 3-order edit modelingWe include an additional experiment, demonstrating the impact of including comment information inprevious revisions, and not including comment information in comment conditioned edit modeling.
