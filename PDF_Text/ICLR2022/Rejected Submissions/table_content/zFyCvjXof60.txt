Table 1: Summary of classificaiton accuracy(%) results. We report the average test accuracy and its standarddeviation over 10 train-test splits. The number in parentheses corresponds to the number of layers of the model.
Table 2: Classification accuracy (%) on Model-Net40. The embedding means the output represen-tation of MVCNN+GVCNN Extractor.
Table 3: Test accuracy on visual object classification. Each model we ran10 random seeds and report the mean ± standard deviation. BOTH meansGVCNN+MVCNN, which represents combining the features or structures to gen-erate multi-modal data.
Table 4: Comparison of our method to others on pro-tein Quality Assessment task (CASP12). At the residuelevel, we report Pearson correlation across all residuesof all decoys of all targets (R) and Pearson correlationacross all residues of per decoys and then average all de-coys (Rdecoy) with LDDT scores. At the global level,we report Pearson correlation across all decoys of all tar-gets (R) and Pearson correlation per target and then aver-age over all targets (Rtarget) with GDT-TS scores.
Table 6: Hyper-parameter search range for citation network classification and visual object classification.
Table 7: Hyper-parameter search range for GHCN and SHSC model in fold classification.
Table 8: Hyper-parameter search range for GHCN and SHSC model in protein Quality Assess-ment (CASP10,CASP11,CASP13 for training, CASP12 for testing).
Table 9: Hyper-parameter search range for GHCN model in protein Quality Assessment (CASP10-12 for train-ing, and CASP13 for testing).
Table 10: Real-world hypergraph datasets used in our citation network classification task.
Table 11: summary of the ModelNet40 and NTU datasetsDataset ∣ ModelNet40 ∣ NTUObjects	12311	2012MVCNN Feature	4096	4096GVCNN Feature	2048	2048Training node	9843	1639Testing node	2468	373Classes	40	67I.4	Visual Object ClassificationDatasets and Settings. We employ two public benchmarks: Princeton ModelNet40 dataset (Wuet al., 2015) and the National Taiwan University (NTU) 3D model dataset (Chen et al., 2003), asshown in Table 11.
Table 12: Comparison of our method to others on protein Quality Assessment tasks. At the residue level,We report Pearson correlation across all residues of all decoys of all targets (R) and Pearson correlation allresidues of per decoys and then average all decoys (Rdecoy) with LDDT scores. At the global level, we reportPearson correlation across all decoys of all targets (R) and Pearson correlation per target and then averageover all targets (Rtarget) with GDT_TS scores.
Table 13: summary of CASP datasetsDataset ∣ Targets Decoys ∣ UsageCASP 10	103	26254	TrainCASP 11	85	12563	TrainCASP 12	40	6924	TestCASP 13	82	12336	TrainDataset and settings We use the data from past years’ editions of CASP, including CASP10-13. We randomly split the CASP10, CASP11, CASP13 for training and validation, with ratiotraining: validation = 9:1. CASP 12 is set aside for testing against other methods. More detailsabout the datasets can be found in Table 13. For the baseline, we compare our methods withother start-of-the-art methods, including random walk-based methods : RWplus (Zhang & Zhang,2010), sequence-based methods: AngularQA (Conover et al., 2019), and 3D structrue-based meth-ods: VOroMQA (Olechnovivc & Venclovas, 2017), 3DCNN (Derevyanko et al., 2018). The resultsof these baselines we reused from Baldassarre et al. (2020) reports. Another baseline HGNN (Fenget al., 2019) is reproduced by us with same training strategy as our methods.
Table 14: Summary of classification accuracy (%) results with various depths. In our SHSC, the number oflayers is equivalent to K in equation 8. We report mean test accuracy over 10 train-test splits.
Table 15: The number of eigenvalues of SHSC in different size ranges on NTU2012 dataset.
Table 16: Test accuracy (%) of our methods for ablation analysis. We report mean ±standard deviation. Coraand Pubmed are the datasets that do not contain Q, so we just report the w/o renormalization results. NTU andModelNet40 constructed by Feng et al. (2019) are both connected hypergraph networks in this work.
Table 17: classifificaiton accuracy(standard deviation) with different ρdataset	model	random	XT	SigmOid(X)	√2σ exp(-⅛x)2)	IOg(X)	exp(x)	exp(-x)pubmed	GHCN	0.74(±0.01)	0.75(±0.01)	0.74(±0.02)	0.74(±0.01)	0.74(±0.02)	0.73(±0.02)	0.73(±0.01)pubmed	SHSC	0.75(±0.01)	0.75(±0.01)	0.75(±0.01)	0.75(±0.01)	0.75(±0.01)	0.74(±0.01)	0.74(±0.01)cora	GHCN	0.72(±0.02)	0.73(±0.01)	0.72(±0.02)	0.72(±0.01)	0.71(±0.02)	0.71(±0.01)	0.60(±0.02)cora	SHSC	0.72(±0.01)	0.73(±0.01)	0.72(±0.01)	0.73(±0.01)	0.72(±0.01)	0.72(±0.01)	0.64(±0.01)43Under review as a conference paper at ICLR 2022I.11	Running Time and Computational ComplexityFirstly, we analyze the theoretical computational complexity of our GHCN and SHSC: For GHCN,the computational cost is the O(|E|d), where |E| is the total edge count in equivalent undigraph.
Table 18: The average training time per epoch with different methods on citation network classification task isshown below and timings are measured in seconds. The float in parentheses is the standard deviation.
Table 19: Summary of classificaiton accuracy(%) results. We report the average test accuracy and its standarddeviation over 10 train-test splits. The number in parentheses corresponds to the number of layers of the model.
Table 20: Test accuracy on visual object classification. Each model we ran 10 random seeds and report themean ± standard deviation. BOTH means GVCNN+MVCNN, which represents combining the features orstructures to generate multi-modal data.
