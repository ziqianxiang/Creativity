Table 1: Model selected based on maximizing each operation strength independently.
Table 2: Comparison in test error (%) with the state-of-the-artperturbation-based and zero-cost NAS on NAS-Bench-201(Best in red, 2nd best in blue. Same for all following tables).
Table 3: Test error (%) of Zero-Cost-PT when using different search orders on NAS-Bench-201.
Table 4: Comparison with the state-of-the-art differentiableNAS methods on the DARTS CNN search space (CIFAR-10).
Table 5: Comparison with the state-of-the-art differentiableNAS methods on the DARTS CNN search space (ImageNet).
Table 6: Comparison in test error (%) with state-of-the- art perturbation-based NAS on DARTS spaces S1-S4.					Space	DARTS1 Best	DARTS-PTI		Zero-Cost-PT2			Best	Best (fix a)	Avg.	BestCIFAR-10					S1	3.84	3.5	2.86	2.75±0.28	2.55S2	4.85	2.79	2.59	2.49±0.05	2.45S3	3.34	2.49	2.52	2.47±0.09	2.40S4	7.20	2.64	2.58	5.23±0.76	4.69CIFAR-100					S1	29.64	24.48	24.4	22.05±0.29	21.84S2	26.05	23.16	23.3	20.97±0.50	20.61S3	28.9	22.03	21.94	21.02±0.57	20.61S4	22.85	20.80	20.66	25.70±0.01	25.69SVHN					S1	4.58	2.62	2.39	2.37±0.06	2.33S2	3.53	2.53	2.32	2.40±0.05	2.36S3	3.41	2.42	2.32	2.34±0.05	2.30S4	3.05	2.42	2.39	2.83±0.06	2.79tectures in spaces where DARTS typically	1 Results taken from (Wang et al., 2021).
Table A1: Randomly selected architectures with only opera-tion sep_conv_5x5 on DARTS CNN space.
Table A2: Comparison with randomly samplednetworks in DARTS CNN space (CIFAR-10).
Table A4: Detailed performance of Zero-Cost-PTrandom withN=1, V=0, nwot metric on DARTS CNN space.
Table A5: Detailed performance of Zero-Cost-PTrandom with N=10,V={1, 10, 100}, nwot metric on DARTS CNN space.
Table A6: Performance of DARTS (Liu et al.
Table A7: Error and search cost of Zero-Cost-PT on MobileNet-like search space (ImageNet)Architecture	Top-1 Error (%)	Top-5 Error (%)	Params. (M)	Cost (GPU Days)ProxylessNAS (GPU)	24.9	7.5	7.1	8.3Zero-Cost-PT(seed 0)	24.0	7.0	8.0	0.041Zero-Cost-PT(seed 1)	23.6	6.8	8.0	0.041Zero-Cost-PT(seed 2)	23.9	7.0	8.3	0.041so far there is no clear solution on how one could constrain #FLOPS/Params of the resulting finalarchitecture during the process of discritizing operations on edges of the supernet. Essentially, to dothat, when selecting the operations on an edge of the supernet we need to consider both their scoresand the potential contributions to the sum of FLOPS/Params of the final model, which is potentially aNP hard problem. Therefore, in our experiments on MobileNet-like search space, we do not enforceconstraints on #FLOPS/Params during search, as it is less relevant to the proposed approach.
Table A8: Raw values of operation scoring functions at iteration 0 to reproduce Figure 2a.
