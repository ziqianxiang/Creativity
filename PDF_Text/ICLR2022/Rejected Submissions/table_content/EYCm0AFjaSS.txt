Table 1: Benchmarking ZerO on CIFAR-10 and ImageNet. ResNet-50(AugSkip) is our proposednetwork. We repeat each run 10 times with different random seeds.
Table 2: Benchmarking ResNet-50 without batch normalization on ImageNet5	Related WorksTheoretical analysis of deep networks To ensure stable training with random weight initializa-tion, previous works such as Glorot and Bengio; He et al. (a) study the propagation of variance inthe forward and backward pass under different activations. For residual networks, by analyzing theoptimization landscape of linear residual networks, Hardt and Ma suggests that all critical points ina neighborhood around zero are proved to be global minima, suggesting zero initialization shouldbe a better choice from the optimization perspective.
Table 3: The effects of additional skip connections and Hadamard transform in ResNet-18 onCIFAR-10. ResNet-18 without both additional skips and Hadamard transform is the standardResNet-18. Test errors (%) are reported.
