Table 2: F1 Score on BERT-Iarge SQUAD v1.1 (Cl=95%)Model	w/o finetune	w/ finetuneTransformer (float)	93.22 ± 0.l5zz	93.17 ± 0.2十Transformer (bfloat16)	93.34 ± 0.31	93.18 ± 0.27DfssAtten 1:2 (float)	92.86 ± 0.22	93.07 ± 0.17DfssAtten 2:4 (bfloat16)	93.00 ± 0.16	93.28 ± 0.29and “DFSSATTEN 2:4 (bfloat16)”, respectively, and running inference with dense attention mecha-nism.
Table 3: Perplexity on roBERTa-large (Cl=95%)Model	Wikitext-2		Wikitext-103		w/o finetune	w/ finetune	w/o finetune	w/ finetuneTransformer (float)	2.85 ± 0.09	2.83 ± 0.09zz	2.63 ± 0.03=	2.62 ± 0.0十Transformer (bfloat16)	2.85 ± 0.05	2.85 ± 0.07	2.62 ± 0.08	2.63 ± 0.05-DFSSATTEN 1:2 (float)-	2.88 ± 0.06	2.88 ± 0.07	2.64 ± 0.06	2.64 ± 0.06DFSSATTEN 2:4 (bfloat16)	2.88 ± 0.07	2.84 ± 0.04	2.63 ± 0.03	2.61 ± 0.04Long Range Arena. For seqUence length longer than 512, we incorporate foUr tasks from the LongRange Arena (Tay et al., 2021), inclUding ListOps, Text Classification, DocUment Retrieval, andImage Classification Under seqUence lengths 2048, 2048, 4096, and 1024, respectively. We omit thePathfinder (1K) task as we cannot replicate the resUlts, which was also reported in LU et al. (2021).
Table 4: AccUracy of different transformer models on LRA benchmark. We follow the traininginstructions from Tay et al. (2021) to reuse the results from this paper.
Table 6: Accuracy on Image (1K) on LRA (Tay et al., 2021) under the combination of DFSSATTENand Nystromformer (Xiong et al., 2021).	Pretraining	FinetuningNystromformer (float)	41.17	41.52Nystromformer (bfloat16)	-	41.59Nystromformer + DFSSATTEN 1:2 (float)	-	4191Nystromformer + DFSSATTEN 2:4 (bfloat16)	-	42.54Then we provide a complexity analysis of the combination following Xiong et al. (2021). Thelandmark selection with segement-means takes O(n), iterative approximation of the pseudoinversetakes O(m3). The matrix multiplication complexity of the standard Nystromformer takes O(nm2 +mndv+m3+nmdv). After applying our method, it can be reduced to O (nψ- -+ Inmd +m3+nmdv).
