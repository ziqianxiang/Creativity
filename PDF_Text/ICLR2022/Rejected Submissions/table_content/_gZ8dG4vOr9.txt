Table 1: Sparsity level (in percentage) and performance of pruned FBNetV3 networks on ImageNetdataset for different target MFLOPs. The best accuracy obtained at each target FLOPs level ishighlighted in bold.
Table 2: Computation speedup in term of GPU-hours when comparing NAS (neural ArchitectureSearch) with pruning and fine-tuning approaches. The selected seed networks are drawn from thosein Table 1 with the best performance at target FLOPs.
Table 3: Baseline FBNetV3 models chosen for our experimentsBaseline	No. of parameters (in millions)	MFLOPs	Top-1 Accuracy (ImageNet)	Top-5 Accuracy (ImageNet)FBNetV3A	8.5	-356.6-	796	947FBNetV3B	8.5	461.6-	80.2	949FBNetV3C	9.9	-5570-	808	95.3FBNetV3D	10.2	-644.4-	810	95.4FBNetV3E	10.7	-7620-	813	95.5FBNetV3F	13.8	1181.6	825	95.9FBNetV3G -	16.5	-	2129.7	83.2	96.3A AppendixA. 1 DatasetOur pruning experiments are conducted on the ImageNet dataset, which is commonly used in theliterature to evaluate performance of image classification models. It is a collection of millionsof images, where there is a defined taxonomy based on the WordNet hierarchy. The taxonomycomprises of approximately 22,000 visual subcategories, making this a large-scale classificationproblem. ImageNet was first introduced by Deng et al. (2009) and has been adopted by the machinelearning and computer vision communities to benchmark image classification models. We use theentire dataset consisting of 14 million images for our experiments, and we utilize both the trainingset and the validation set. We split the training set to also create a smaller validation set (of 50,000images evenly distributed across all image categories) for parameter tuning and setting the training
