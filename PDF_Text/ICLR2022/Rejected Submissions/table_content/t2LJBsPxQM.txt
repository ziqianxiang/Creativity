Table 1: Variants of convolutions. We present the modified Z-transforms, Y(z), H(z), and X(Z) for eachconvolution such that Y(Z) = H(Z)X(Z) holds. In the table, X[R](z)，[X0lR(z)>,..., XR-1lR(z)>]>and X[R] (Z) = [X-0lR(z),..., X-(RT)IR(z)]. For group convolution, hg is the filter for the gth group withHg (Z) being its Z-transform, and Mkdiag (∙) stacks multiple matrices into a block-diagonal matrix.
Table 2: (Left) Orthogonality evaluation of different designs for standard convolution. The numberkConv(x)k/kxk - 1 indicates the difference between the output and input norms of a layer. A layer is moreprecisely orthogonal if the number is closer to 0. As shown, our SC-Fac achieves orders of magnitude moreorthogonal on standard convolution. (Right) Orthogonality evaluation of our SC-Fac design for variousconvolutions. The numbers kConv(x)k/kxk - 1 displayed are in the magnitude of 10-8. As shown, ourSC-Fac achieves machine epsilon orthogonality on variants of convolution.
Table 3: (Top) Certified robustness for plain convolutional networks (without input normalization). Weuse KW-Large introduced by Wong et al. (2018). The results for RKO, OSSN, and SVCM are produced byTrockman & Kolter (2021). (Bottom) Practical robustness for residual networks (with input normalization).
Table 4: (Left) Comparisons of various skip connection types on WideResNet22-10 (kernel size 5). (Right)Comparisons of various receptive field and down-sampling types on WideResNet10-10. The symbols ✓,X indicate whether average pooling or Strided convolution is used for down-sampling. For “slim” in Stridedconvolution, we set kernel_size = stride; and for for “wide”, kernel_size = stride * kernel_size’ (where kernel_size, is the kernel size for the main branch.
Table 5: Computational complexities of different approaches for orthogonal convolutions.
Table 6: Comparisons of various initialization methods on WideResNet (kernel size 5).
Table 7: Comparison of different depth and width on WideResNet (kernel size 5). Some numbersare missing due to the large memory requirement (on Tesla V100 32G). The notation width factorindicates (channels = base channels × factor).
Table 8: Comparison of orthogonal convolutions and normal convolutions on WideResNet (ker-nel size 5). The notation width factor indicates (channels = base channels × factor).
Table 9: Practical robustness against '∞ adversarial examples (WideResNet kernel size 5, '∞perturbation radius of = 8/255). BCOP+ and SOC (Singla & Feizi, 2021) results with ResNet-18are reported by Singla & Feizi (2021).
Table 10: Comparisons of various flow-based models on the MNIST dataset. We report theperformance in bits per dimension (bpm), where a smaller number indicates a better performance.
