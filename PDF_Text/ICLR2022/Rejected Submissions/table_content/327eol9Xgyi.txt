Table 1: Experiment results on the 2017 COCO validation set of different TPN cores (top four rows)and its baselines (bottom five rows). The five leftmost columns specify the network (Back = Back-bone, Core, B = Number of bottleneck layers per self-processing node, L = Number of layers, C =Number of hidden layers in classification and bounding box subnets), the middle six columns showits performance and the five rightmost columns show its computational characteristics. These char-acteristics were obtained on a GeForce GTX 1660 Ti GPU by applying the network on a batch of two800 × 800 images, each containing 10 ground-truth objects during training. The training characteris-tics are found under the columns ‘Params’, ‘tFPS’ and ‘tMem’, whereas the inference characteristicsare found under the ‘iFPS’ and ‘iMem’ columns. Here the FPS metric should be interpreted as thenumber of times the GPU can process above input. Note that both forward and backward passes(with parameter update from the optimizer) are used to obtain the training characteristics.
Table 2: Comparison of our best-performing TPN model with other prominent object detectionnetworks on the 2017 COCO validation set (see Table 1 for the definitions of B, L and C). Allmodels use a ResNet-50 (He et al., 2016a) backbone. The number of FLOPS are computed as inCarion et al. (2020), by applying the flop_count.operators tool from Detectron2 (WU et al.,2019) on the first 100 images of the validation set. The number of inference FPS ‘iFPS’ is calculatedas explained in Table 1, while Using the implementations provided by MMDetection (Chen et al.,2019) for the baseline models.
Table 3: Experiment results on the 2017 COCO validation set comparing the TPN core (top row)with the BiFPN core (bottom row), when using the large ResNeXt-101-32x4-DCNv2 backbone.
