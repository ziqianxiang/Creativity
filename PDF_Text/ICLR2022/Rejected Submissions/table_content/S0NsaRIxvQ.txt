Table 1: Generalization results on Procgen environments. The results are the best test result achievedby each agent during the entire training timesteps. The mean value and standard deviation arecalculated across 3 random seed runs. ARPO achieves the best generalization results in most of theenvironments compared to PPO and RAD. Best agent is in Bold.
Table 2: Hyperparameters for Procgen (RLlib) ExperimentsDescription	HyperparametersDiscount factor of the MDP	gamma : 0.999The GAE(lambda) parameter	lambda : 0.95The default learning rate	Ir : 5.0e â€” 4Number of epochs per train batch	num_sgd_iter : 3Total SGD batch size	sgd_minibatch_size : 2048Training batch size	train_batch_size : 16384Initial coefficient for KL divergence	kl.coeff : 0.0Target value for KL divergence	kl_target : 0.01Coefficient of the value function loss	vf_loss_coeff : 0.5Coefficient of the entropy regularizer	entroPy-Coeff : 0.01PPO clip parameter	Clip-Param : 0.2Clip param for the value function	Vf-Clip-Param : 0.2Clip the global amount	grad_clip : 0.5Default preprocessors	deepmindPyTorch Framework	framework : torchSettings for Model	CUstom-model : impala-Cnn-torchRollout Fragment	rollout-fragment-length : 256H	Environment Details
Table 3: ARPO, and PPO Hyperparameters for Distracting Control ExperimentsDescription	HyperparametersNumber of epochs per train batch	num_sgd_iter : 3Total SGD batch size	sgd_minibatch_size : 256Training batch size	trainJbatchsize : 8192PyTorch Framework	framework : torchTable 4: SAC Hyperparameters for Distracting Control ExperimentsDescription	HyperparametersTraining batch size	trainJbatchsize : 512Timesteps per iteration	timesteps_per_iteration : 1000Timesteps per iteration	learning starts : 5000PyTorch Framework	framework : torchComputing details. We used the following machine configuration to run our experiments: 20 core-CPU with 256 GB of RAM, CPU Model Name: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz,and a Nvidia A100 GPU.
Table 4: SAC Hyperparameters for Distracting Control ExperimentsDescription	HyperparametersTraining batch size	trainJbatchsize : 512Timesteps per iteration	timesteps_per_iteration : 1000Timesteps per iteration	learning starts : 5000PyTorch Framework	framework : torchComputing details. We used the following machine configuration to run our experiments: 20 core-CPU with 256 GB of RAM, CPU Model Name: Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz,and a Nvidia A100 GPU.
