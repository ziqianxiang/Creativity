Table 1: Cross-entropy loss for training two transformers, one on commit data and the other onreversed commits. The two models are evaluated on both forward and backward edits, and with andwithout code skeletons. The cross-entropy losses are five times better than those normally reported forgenerating Python code since the editing task is relatively easy ((Svyatkovskiy et al., 2020; Clementet al., 2020)). Furthermore, the losses for reversed edits are a third lower than for forward edits. Theforward model is 6% better at forward edits than the reverse model, and the reverse model is in turn6% better at reverse edits. Both models perform 2% better with skeletons than with only the focalmethod. We report the results in base two on the test set.
Table 2: Cross-entropy results for training two transformers, one trained on commit data and theother on patching neural bugs. As in table 1, the two models are evaluated on commit data. Notethat the neural bugpatcher does comparatively worse on reversed edits vs. forward edits. The neuralbugpatcher actually does worse when using skeletons on commit data, presumably due to the fact thatcommits typically edit multiple functions. Finally, the neural bugpatcher outperforms the forwardedit model when evaluated on individually edited functions.
Table 3: Results on the QuixBugs Benchmark. We use a timeout of one minute for each bug andperform joint bug-localization and program repair. In contrast, prior works use a timeout of severalhours and often assume that the buggy line has already been localized.
Table 4: The neural bugpatcher sees a large 25% cross-entropy loss decrease when using skeletonswhen evaluated on our large test set of neural bugs. These neural bugs are in general not executable.
Table 5: Patch success results on our benchmark of neural bugs with executable tests.			DeepDebug (backtrans)	DeepDebug (traces)Top-1 Success Rate Top-10 Success Rate	357/523 (68%) 472/523 (90%)	393/523 (75%) 509/523 (97%)6	Future WorkHaving code with executable tests opens up a world of opportunities for algorithmic debuggingwith machine learning. We are interested in genuinely iterative fixing (in contrast to independentlygenerating edits), moving beyond the assumption that the bug has already been localized to a singlemethod, and expanding to other popular languages. Most importantly we are interested in deployinga command-line interface to our tool.
