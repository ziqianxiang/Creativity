Table 1: Comments as a modality. Treating comments as an auxiliary modality to a video improvesText-to-Video retrieval for both RedditVC and KineticsComments test sets. As a baseline, we usethe CLIP model on just the first frame. We then train just our CAM on top of CLIP, adapting thevisual embedding with information from the comments, which improves retrieval metrics. Trainingour full model, which includes temporal attention, on RedditVC again validates that the inclusion ofcomments helps, with a further jump in Kinetics performance when using the Kinetics training set.
Table 2: Ablation Study. Comparing Text-to-Video and Video-to-Text retrieval results betweendifferent baselines and our method.
Table 3: Adapting differentheads. Text-to-video R@10 re-sults on RedditVC-5. We eitheradapt the text or image features.
Table 4: Influence of Auxiliary Data. Text-to-video retrievalresults on RedditVC-5. We vary the auxiliary text source duringtraining and testing. Using images labels from a classifier duringtraining works well but does not generalize.
Table 5: MSVD (Chen & Dolan, 2011) Zero-shottext-to-video retrieval. CLIP is an important base-line since we use it as our backbone initialisation.
Table 6: Text-to-video retrieval on MSR-VTT. Vis. Enc. Init.: Datasets used for pretraining visualencoders for tasks other than visual-text retrieval, eg object classification. pre-training: Visual-textpretraining data. f Object, Motion, Face, Scene, Speech, OCR and Sound classification features.
Table 7: ActivityNet (Fabian Caba Heilbron & Niebles, 2015) Text-to-Video retrieval.	Table 8: LSMDC (Rohrbach et al., 2017) Text-to- Video retrieval.					Method	R@1	R@5	R@10	MedRMethod	R@1 R@5 R@10 MedR	JSFusion (Yu et al., 2018)	9.1	21.2	34.1	36.0	CE (liu et al., 2019) MMT (patrick et al., 2020a)	11.2	26.9	34.8	25.3SSB (PatIiCketal.,2020b)	0.0	0.2	0.3 2238		12.9	29.2	38.8	19.3Ours	8.4 22.7 33.0	26	FiT (Bain et al., 2021)	15.0	30.8	39.8	20.0	Ours	24.0	48.0	60.1	64.5 LimitationsWe find that the context adapter can be led tooverride the information in a title if we adversar-ially craft comments that all point to differentcontent. Qualitative examples of this can be seenin Fig. 4. The model without comments, cor-rectly associates the image with a cookie (jar),however when adding a comment about a “dog”the model prefers the dog label over the cookielabel. More examples can be found in the ap-pendix (Appendix A.5 and the results browserin the supplementary material).
Table 9: MSVD Chen & Dolan (2011) Video-to- Table 10: MSRVTT (Xu et al., 2016a) Video-to-Text retrieval.	Text retrieval.
Table 11: Comments per video statistics for the KineticsComments dataset.
Table 12: Prevalence of toxic text in thedataset. We report the proportion of posts,titles, and comments that are flagged as hav-ing potentially offensive content by the open-source library Detoxify. We use a thresholdof 0.9.
Table 13: Bootstrapping confidence intervalestimates. Showing mean and 95% confidenceintervals obtained by bootstrapping with 80%of the test sets. Recall in units of percentagepoints. Using models trained on RedditVC andfinetuned on the respective benchmark trainingsets.
Table 14: Mean recall performance and 95th percentile confidence intervals across five experimentstrained with different random seeds for the best two models from Table 1 in the main paper.
Table 15: We experiment with an additional modality: Audio. Our model can learn from audio aswell as comments and it is even possible to combine both modalities in the same adapter module.
Table 16: For the experiments using images instead of videos, we ablate the position of the frame inthe video. Extracting the frame from the middle of the video is better, likely due to the fact that theimportant action in a video usually occurs in the middle or end of the clip.
Table 17: We test the robustness of the model to distractor comments (randomly sampled from theremaining dataset). We find that our context adapter module is able to deal with with bad commentsbetter than the averaging baseline without having been trained with distractor comments.
Table 18: LiveBot Dataset results. We show that even on a dataset such a LiveBt which is livecomments on a video feed, our CAM can extract meaningful information from the comments.
Table 19: Baseline similarity thresholding. We show that the similarity score alone cannot identifydistracting comments and discarding low similarity comments decreases performance.
