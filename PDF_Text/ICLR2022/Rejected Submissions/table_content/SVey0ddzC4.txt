Table 1: Comparison btwn. unsupervised GPCA (β = 0), GPCANET, and existing (supervised)SOTA GNNs on 5 datasets, w.r.t. mean test accuracy and standard deviation (in parentheses) over 5different seeds. Those marked With * are reported values at the OGB-Ieaderboard3. Highest meanperformance is in bold and the second highest is Underlined.
Table 2: Comparison btwn. Supervised (S-)GPCA (β>0) andUnsupervised (U-)GPCA (β=0), w.r.t. mean test accuracy andstandard deviation (in parentheses) over 5 different seeds. Alsoshown (bottom row) is the performance by the best method inTable 1. Highest mean performance is highlighted in bold.
Table 3: Test set performance of GCN with Xaiver- versusGPCANET-initialization, w.r.t. varying number of layers (L)across all datasets. Each reported value is based on the best se-lected configuration on validation data. GPCAnet-init. enableshigher performance that is also stable with increasing depth.
Table 4: Statistics of used datasets.
Table 5: Hyperparameters pool for each dataset, includes learning rate (LR), weight decay (WD),number of layers (#Layers), hidden size, dropout, α, and β. For ARXIV and PRODUCTS, weightdecay is set as 0 because the dataset is large and no overfit happened. Same reason for choosingsmaller dropout rate for them.
Table 6: Performance of unsupervised GPCA (β = 0) for varying α w.r.t. mean test accuracy andstandard deviation (in parentheses). GPCA (best α) selects α ∈ {1, 5, 10, 20, 50} based on validation,whereas GPCA with specific α uses the specified fixed α.
Table 7: Pool of a for 1 〜3-layer GPCAnet, same across all datasets.
Table 8: Runtime comparison for different methods over all datasets.
