Table 1: N-way K-shot (%) classification accuracies on Omniglot, Mini-Imagenet and CIFAR-100	Omniglot 20-way		Mini-Imagenet 5-way		CIFAR-100 5-way	Algorithm	1-shot	5-shot	1-shot	5-shot	1-shot	5-shotMatching Nets	93.80	98.50	43.56	55.31	-	-LSTM Meta-Learner	-	-	43.44	60.60	-	-MAML	95.80	98.90	48.70	63.11	-	-FOMAML	-	-	48.07	63.15	-	-Reptile	88.14	96.65	47.07	62.74	-	-PredCP (NalisniCk et al., 2021)	-	-	49.30	61.90	-	-Neural Statistician (Edwards & Storkey, 2016)	93.20	98.10	-	-	-	-mAP-SSVM (Triantafillou et al., 2017)	95.20	98.60	50.32	63.94	-	-LLAMA	-	-	49.40	-	-	-PLATIPUS	-	-	50.13	-	-	-GEM-BML+ (Zou & Lu, 2020)	96.24	98.94	50.03	-	-	-DKT (Patacchiola et al., 2020)	-	-	49.73	64.00	-	-ABML	-	-	45.00	-	49.50	-BMAML (with 5 particles) (Yoon et al., 2018)	-	-	53.80	-		-ABML (local)	90.21	93.39	44.23	52.12	49.23	53.60BMAML (local)	96.92	98.11	53.10	64.80	52.60	65.80PLATIPUS (local)	94.35	98.30	49.97	63.13	51.14	63.61
Table 2: Mini-Imagenet 5-way Few-Shot ablation study (% accuracy)Algorithm	Network type	1-shot	5-shotMAML (local)	deterministic LWTA stochastic LWTA	48.88 49.61	63.15 64.03FOMAML (local)	deterministic LWTA stochastic LWTA	48.11 49.24	63.54 64.54ABML (local)	deterministic LWTA stochastic LWTA	44.31 45.11	52.27 53.31BMAML (local)	deterministic LWTA stochastic LWTA	53.12 53.50	64.84 65.31PLAnPUS (local)	deterministic LWTA stochastic LWTA	49.99 51.06	63.21 64.18StochLWTA-ML	deterministic LWTA stochastic LWTA	53.12 54.11	64.93 66.704.3.2	EFFECT OF BLOCK SIZE JAs it is presented in Table 3, increasing the number of competing units per block to J = 4 or J = 8 does notnotably improve the results of our approach. On the contrary, it increases the number of trained parameters,thus leading to higher network computational complexity. This corroborates our initial choice of using J = 2competing units per block in our approach.
Table 3: Effect of block size J in StochLWTA-ML’s classification (%) accuracy		Omniglot 20-way		Mini-Imagenet 5-way		CIFAR-100 5-way	Number of units		1-shot	5-shot	1-shot	5-shot	1-shot	5-shotJ	2	97.79	98.97	54.11 ^^	66.70	54.60	66.73-J	4	96.33	98.55	53.99	66.65	54.51	66.13J =	8	95.38	98.83	53.70	67.08	54.45	66.184.3.3 How does the number of samples at prediction time affect accuracy?We scrutinize the effect of the number of drawn samples, B, on StochLWTA-ML’s predictive accuracy. To thisend, we repeat our experiments using B = 10 logits sets. In Table 4, we provide the comparative outcomesconcerning the two sample size configurations of B = 4 and B = 10. As we observe, an increase in predictionsample size, B, yields a slight accuracy increase. However, the aforementioned increase in sample size imposessome computational overhead, which we elaborate upon in the following Section. We argue that this overheadmight not be worth it for the slight performance increase reported in Table 4. More information on the effect ofsample size in our approach’s predictive performance are provided in the Supplementary.
Table 4: Effect of sample size B in StochLWTA-ML’s classification (%) accuracy		Omniglot 20-way		Mini-Imagenet 5-way		CIFAR-100 5-way	Number of samples		1-shot	5-shot	1-shot	5-shot	1-shot	5-shotB 二	=4	97.79	98.97-	ɪn^^	66.70	54.60	66.73-B 二	10	96.91	99.23	54.89	67.22	55.18	66.154.3.4 Is there a computational time trade-off for the increased accuracy?It is also important to investigate whether our approach represents a trade-off between accuracy and com-putational time compared to our competitors. To facilitate this investigation, in Table 5 we provide trainingiteration wall-clock times for our approach and the existing locally reproduced state-of-the-art, as well as thetotal number of iterations each model needs to achieve the reported performance of Table 1. It appears that ourmethodology takes 77% less training time than the less efficient algorithms ABML, BMAML, PLATIPUS, andis comparable to other approaches. This happens because our approach yields the reported state-of-the-art per-formance by employing a network architecture (that is, number of LWTA layers, as well as number of blocksand block size on each layer) that result in a total number of trainable parameters that is one order of magnitudeless on average than the best performing baseline methods. This can be seen in the last three columns of Table8Under review as a conference paper at ICLR 20225	(dubbed DA, DB and DC for Omniglot, Mini-Imagenet and CIFAR-100 respectively). In addition, trainingfor our approach converges fast.
Table 5: Performance comparison: average wall-clock time (in msecs), training iterations for eachlocally reproduced method and number of baselines’ trainable parameters over the considereddatasets of Table 1Algorithm	Training	Prediction	Number of training iterations	DA parameters	DB parameters	DC parametersPLATIPUS (local)	1603.39	602.77	333600	560025	615395	580440BMAML (local)	1450.31	514.43	301800	560025	615395	580440ABML (local)	678.48	265.78	138000	224010	246158	232176MAML (local)	288.25	103.28	60000	112005	123079	116088FOMAML (local)	284.49	102.34	60000	112005	123079	116088Reptile (local)	284.30	102.27	60000	113221	124613	117463StochLWTA-ML	282.90	113.44 (B = 4) 121.87 (B = 10)	60000	54549	60112	56745Finally, we provide an example of how training for our approach converges, and how this compares to thealternatives. We illustrate our outcomes on the Omniglot 20-way 1-shot benchmark; similar outcomes havebeen observed in the rest of the considered datasets. Fig. 2(a) compares StochLWTA-ML with prior traditionalML methods: MAML, FOMAML and Reptile. It becomes apparent that our approach converges equally fastto these competitors. Further, Fig. 2(b) compares StochLWTA-ML with the probabilistic ML models ABML,BMAML, PLATIPUS. Since, as we see in the ablation study of Section 4.3.4, these methods are quite time-consuming and less efficient regarding to memory consumption, StochLWTA-ML gives rise to an easier timetraining MAML based probabilistic model.
Table B1: Omniglot 20-way Few-Shot ablation study (% accuracy)Algorithm	Network type	1-shot	5-shotMAML (local)	deterministic LWTA stochastic LWTA	95.52 95.91	98.15 98.78FOMAML (local)	deterministic LWTA stochastic LWTA	95.01 95.80	98.18 98.41ABML (local)	deterministic LWTA stochastic LWTA	90.30 91.21	93.64 93.91BMAML (local)	deterministic LWTA stochastic LWTA	96.96 97.11	98.21 98.30PLAnPUS (local)	deterministic LWTA stochastic LWTA	94.48 95.13	98.31 98.56StochLWTA-ML	deterministic LWTA stochastic LWTA	96.95 97.79	98.63 98.97C	Few-Shot Classification Network ArchitecturesFor the local replicates of prior ML algorithms in the experiments of our work, we follow the same architecturefor the deep neural network as the one used by Vinyals et al. (2016). For Omniglot, the network is composed of4 convolutional layers with 64 filters, 3 x 3 convolutions and 2 x 2 strides, followed by a Batch Normalizationlayer (Ioffe & Szegedy, 2015) and the final values of each layer are processed by an activation function. Forboth Mini-Imagenet and CIFAR-100, we use 4 convolutional layers with 32 filters to reduce overfitting likeRavi & Larochelle (2017), 3 x 3 convolutions followed by Batch Normalization layer and 2 × 2 max-poolinglayer with the values of each layer finally passed again through an activation block. The activation functionsused for experiments of main paper’s Tables 1 and 5 is ReLU, and LWTA for experiments of main paper’s Table2 and Supplementary’s Table B1.
