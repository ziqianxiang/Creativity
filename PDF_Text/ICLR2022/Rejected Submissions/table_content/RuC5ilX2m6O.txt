Table 1: Test set accuracy (%) on CIFAR-10 and CIFAR-100. The results of our proposed PAA isthe average accuracy (±standard deviation) over four random runs.
Table 2: Validation set Top-1 / Top-5 accuracy (%) on ImageNet.
Table 3: Comparison of computational cost (GPU hours) between our proposed PAA and otherprevious automated DA methods. We train Wide-ResNet-28-10 on CIFAR-10 and ResNet-50 onImageNet. Search: the time of searching for augmentation policys. Train: the time of training thetarget network. The training time of PAA includes the inference time required to compute the stateand observations. Total: the total time. Except PAA, all metrics are cited from (Zhang et al., 2019b;Lim et al., 2019).
Table 4: Test accuracy (%) on various fine-grained classification datasets including CUB-200-2011(CUB) (Wah et al., 2011), Stanford Cars (Cars) (Krause et al., 2013) and FGVC-Aircraft (Aircraft)(Majietal., 2013)._____________________________________________________________________________Dataset	Model	Baseline	Mixup	CUtMix	Co-Mix	AA	FastAA	RA	DADA	PAACUB	ResNet-50	-855-	86.2	-86.1-	87.1	86.8	86.5	86.9	86.8	87.5±0.2	ResNet-101	85.6	87.7	87.9	88.2	88.1	87.9	88.0	88.1	88.3±0.2Cars	ResNet-50	-93.0-	93.9	-94.1-	93.9	94.2	94.0	94.1	93.7	94.3±o.ι	ResNet-101	93.1	94.1	94.2	94.1	94.2	93.8	94.2	93.7	94.5±o.ιAircraft	ResNet-50	-910-	92.0	-922-	92.2	92.3	92.2	92.3	91.8	92.6±0.2	ResNet-101	91.6	92.9	92.3	93.1	92.8	92.9	92.6	92.8	93.5±0.3Classification Results on ImageNet. As shown in Table 2, we evaluate our method on ResNet-50and ResNet-200 (He et al., 2016) backbone on ImageNet, and our PAA significantly improves theperformance of the target networks. The results further demonstrate that our proposed method is aneffective DA technique for consistent and expressive benefits for datasets with larger image sizes.
Table 5: The mAP (%) results on Pascal VOC. We use the Faster R-CNN as the baseline detector.
Table 6: Ablation study: performance comparisons (%) of Patch RandomAugment (PRA), PatchSingle AutoAugment (PSAA) and our PAA on CIFAR-10 and CIFAR-100 (test accuracy) with Wide-ResNet-28-10 (WRN-28-10), and on ImageNet (Top-1 / Top-5 accuracy) with ResNet-50.
Table 7: Ablation study: performance comparisons (Top-1 accuracy (%)) under five different patchnumbers N on CIFAR-10 (image size: 32 × 32) with WRN-28-10 and on ImageNet (image size:224 X 224) together with CUB-200-2011 (CUB) (image size: 448 X 448) with ResNet-50 backbone.
Table 8: We list fifteen kinds of augmentation operations that we use. Additionally, the scheduleof magnitude (i.e., the parameters for Kornia (Riba et al., 2020) pytorch library) for each operationare shown in the third column. Some transformations do not use the magnitude information (e.g.,CutMix and Cutout).
Table 9: The model architecture of policy network and critic network in PAA. For each convolutionlayer, we list the input dimension, output dimension, kernel size, stride, and padding. For the fully-connected layer, we provide the input and output dimension. BN is short for batch normalization.
Table 10: The hyperparameters of various target models on CIFAR-10, CIFAR-100, ImageNet,CUB-200-2011, Stanford Cars and FGVC-Aircraft. LR represents learning rate of the target net-work, WD represents weight decay, and LD represents learning rate decay method. If LD is multi-step, we decay the learning rate by 10-fold at epochs 30, 60, 90 etc. according to LR-step. LR-A2Crepresents the learning rate of augmentation model.
Table 11: Ablation study: impact of the initialization of the feature extractor. We respectively use thepretrained ResNet-18 and non-pretrained ResNet-18 as the feature extractor to get the deep featuresas the state/observation and show the test accuracy (%).
