Table 1: Number of tokens (Billion) and wall clock time (Hour) to reach the same validation perplexityduring GPT-2 117M pre-training, comparing the baseline and curriculum learning under differentbatch sizes/LR and sequence lengths. Last two rows use the best PPL achieved by baseline and CL asthe target.
Table 2: Zero-shot evaluation of the trained models on the WikiText-103 and LAMBADA datasets,following the evaluation methodology from (Shoeybi et al., 2019). Case 2 to 9 are compared withcase 1, and case 11 to 17 are compared with case 10. Case 16 (Press et al., 2020) and 17 (Brownet al., 2020) are related works.				Case	Pre-training Parameters	Pre-training steps, tokens, time	WikiText perplexity J	LAMBADA accuracy ↑117M: 1: Baseline	bsz512-seqlen1K	300K, 157B, 37Hr	27.78	33.19%2: CL 60K	bsz512-seqlen1K	210K, 94B (-40%), 25Hr (-32%)	27.75	34.00%3:CL60K_ _	bsz512-seqlen1K		 330K, 157B(→), 40Hr (+8%) _		27.10	34.56%4: Raseline	bsz4K-seqlen1K	——"37.5K； 157B7→); 16Hr^(-57%)一	28T09 —	——3254%5: CL 20K	bsz4K-seqlen1K	33K, 96B (-39%), 11Hr (-70%)	27.66	33.71%6:CL20K_ _	bsz4K-seqlen1K	_ _ ^7.5Kl157BJ→), 17Hr_(-54%) _		27.12 _	34.74%7: baseline	bsz512-seqlen2K	150K； 157B7→); 32Hf(-14%) ^	28:19 —	——3299%8: CL 110K	bsz512-seqlen2K	125K, 73B (-54%), 18Hr (-51%)	27.18	33.81%9: CL 110K	bsz512-seqlen2K	205K,157B (→),36Hr (-3%)	26.43	34.02%1.5B: 10: Baseline	bsz512-seqlen1K	300K, 157B, 341Hr	13.89	57.29%11: CL 270K	bsz512-seqlen1K	360K, 122B (-22%), 286Hr (-16%)	13.89	57.38%12: CL 270K	bsz512-seqlen1K		428K,_157B ^→)l364Hγ (+7%) _		13.88 _	_ _ 57.89%「13?BaSeiGne 一	bsz4K-seqlen1K	一— 375K,i57B (→),151Hr-(-56%)一	14^j6 一	——5506%14: CL 45K	bsz4K-seqlen1K	50K, 121B (-23%), 121Hr (-65%)	13.88	58.20%15: CL 45K	bsz4K-seqlen1K	58.8K, 157B (→), 155Hr (-55%)	13.72	58.47%
Table 3: Number of tokens (Billion) and wall clock time (Hour) to reach the same validation perplexityduring GPT-2 1.5B Seqlen 1K pre-training, comparing the baseline and curriculum learning underdifferent batch Sizes/LR. Last two rows use the best PPL achieved by baseline and CL as the target.
Table 4: Zero-shot evaluation of the trained 117M models on the WikiText-103 and LAMBADAdatasets, following the evaluation methodology from (Shoeybi et al., 2019).
Table 5: Measuring training instability by the ratio between the current step training loss and theminimum loss among all previous steps. Larger ratios (esp. those greatly larger than 1.0) indicatelarger training instability/divergence.
Table 6: Number of steps with training loss ratios (defined in Appendix A.3) larger than 1.5 duringGPT-2 1.5B Seqlen 1K pre-training (first 3K steps only) with batch size 2K, 5 different seeds, anddifferent learning rates for baseline and curriculum learning. Left/right number in each cell is forbaseline/CL, respectively.
