Table 1: The comparison between various settings. DG and TTA significantly differ from other settings sinceboth of them get rid of the iterative training on target data.
Table 2: Accuracies (%) of “Source” and “T-BN” on three DG classification benchmarks:VLCS, PACS and Office-Home.
Table 3: Task design.
Table 5: Test error values of different corruptions on CIFAR10-C and CIFAR100-C. The evaluation is imple-mented on RobustBench (Croce et al., 2020) for fair comparison and easy reproducing. We compare proposedCORE with T-BN (Schneider et al., 2020) and state-of-the-art method TENT (Wang et al., 2021). The bestresults are highlighted.
Table 4: Test error values of different cor-ruptions on ImageNet-C. The best results arehighlighted. The reported results are aver-aged over total 75 tasks. Detailed results ofeach task are shown in Appendix B.1.
Table 6: Accuracies of state-of-the-art methods on four datasets. The evaluation is implemented on Do-mainBed (Gulrajani & Lopez-Paz, 2020). The best results on Optimization-Free Test-Time Adaptation (OF-TTA) and optimization-based Test-Time Adaptation (TTA) are highlighted by bold and underline, respectively.
Table 7: Results on semantic segmentation under domain generalization setting. We evaluate our methodon two popular Simulation-to-real benchmarks. '↑, means the results are based on our implementation. Theevaluation metrics is the mean intersection-over-union (mIoU). We compare our method with recent state-of-the-art methods: SW (Pan et al., 2019), IBN-Net (Pan et al., 2018), IterNorm (Huang et al., 2019), ISW (Choiet al., 2021) and T-BN Nado et al. (2020).
Table 8: Results of four optimization-based TTA methods on VLCS, PACS and Office-Home.
Table 9: Results of four optimization-based TTA methods on GTA5 → Cityscapes.
