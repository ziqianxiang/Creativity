Table 1: Summary of the six classification datasets used for evaluation.
Table 2: Final accuracy of models on all the datasets together with 95% confidence intervals com-puted over 5 bootstrapping iterations. WARM improves the quality of training data as measured bythe accuracy of label model on the Train set. This improvement further translates into better down-stream model performance on the Test set. Fully Sup. LR represents the performance of thefully-supervised logistic regression (LR) model trained with full access to ground truth labels.
Table 3: Clinical Characteristics and Patient Outcomes: A summary of the patients characteristicsin the EEG dataset.
Table 4: Knowledge Shift experiments on the Synthetic Dataset. Accuracy is final accuracy, and∆Accuracy is change from initial to final accuracy.
Table 5: Knowledge Shift experiments on the Heart Disease Dataset. Accuracy is final accuracy,and ∆Accuracy is change from initial to final accuracy.
Table 6: Experiments on the Heart Disease dataset by adding varying levels of uniform randomnoise to LF decision parameters. Accuracy is final accuracy, and ∆Accuracy is change frominitial to final accuracy. Noisy LFs hurt both label and end model performance. However, WARM caniteratively de-noise LFs by actively collecting a few more labeled examples. Even with 75% noiseinitially, WARM can refine LFs such that the resulting label and end model perform on par with thosetrained using LFs with no noise. WARM also consistently outperforms other baselines.
Table 7: Performance metrics for the Wisconsin Breast Cancer database. Accuracy is final accuracy,and ∆Accuracy is change from initial to final accuracy.
Table 8: Performance metrics for the Wisconsin Diagnostic Breast Cancer dataset. Accuracy is finalaccuracy, and ∆Accuracy is change from initial to final accuracy.
Table 9: Performance metrics for the Heart disease dataset. Accuracy is final accuracy, and∆Accuracy is change from initial to final accuracy.
Table 10: Performance metrics for the Diabetes dataset. Accuracy is final accuracy, and ∆Accuracyis change from initial to final accuracy.
Table 11: Performance metrics for the Synthetic dataset. Accuracy is final accuracy, and∆Accuracy is change from initial to final accuracy.
Table 12: Performance metrics for the EEG dataset. Accuracy is final accuracy, and ∆Accuracyis change from initial to final accuracy.
Table 13: Effect of random sampling on Synthetic Dataset. Accuracy is final accuracy, and∆Accuracy is change from initial to final accuracy. Accuracy and F1 score metrics are muchlower for random sampling (RS) than the uncertainty sampling (US).
Table 14: Effect of random sampling on Wisconsin Diagnostic Breast Cancer Dataset. Accuracy isfinal accuracy, and ∆Accuracy is change from initial to final accuracy. Accuracy and F1 scoremetrics are much lower for random sampling (RS) than the uncertainty sampling (US).
Table 15: Testing performance of different downstream models on datasets of different sizes. Ac-curacy is final accuracy, and ∆Accuracy is change from initial to final accuracy. Due to thecomplexity of the dataset, WARM outperforms Active learning on most occasions. Moreover,WARM’s performance improves as the size of the dataset increases, as it is able to better estimate theaccuracies of LFs.
Table 16: Testing performance of downstream models on datasets with increasing complexity. Ac-curacy is final accuracy, and ∆Accuracy is change from initial to final accuracy. The more clustersper class, the more complex the dataset is, as can also be seen by the deteriorating performance ofthe models. When the dataset is less complex, Active learning does well, but as the complex-ity increase WARM does better. We expect Active learning to take many more data points tolearn decision boundaries in more complex datasets.
