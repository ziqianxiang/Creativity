Table 1: Results observed with the proposed selection strategies on the two considered downstreamtasks. Word Error Rate (WER) Equal Error Rate (EER), and Accuracy (Acc) are expressed inpercentage and used for LibriSpeech 100 hours, VoxCeleb1 and IEMOCAP respectively (i.e. loweris better). ASR results are given with and without Language Modeling (LM). All SSL modelscontain 16.3M neural parameters.
Table 2: Results observed retraining the Wav2vec2 model with and without weighted pretext tasksusing the sparsemax method. “Fr.” and “Fine.” also respectively refer to Frozen and Finetuned set-tings. Adding selected pretext tasks improves the donwstream performance on all three consideredtasks. All models contain 100M neural parameters.
Table 3: Candidate speech pseudo-labels and descriptions.
Table 4: Results observed retraining the Wav2vec2 model with and without weighted pretext tasksusing the sparsemax method, on LibriSpeech 960. “Fr.” and “Fine.” also respectively refer to Frozenand Finetuned settings. Adding selected pretext tasks still improves the downstream performance.
Table 5: Weights for every pretext-tasks in every considered experiment. With techniques onlyleading to a selection of pretext tasks (without weights) a unitary weight is assigned for the selectedtasks and zero for the non selected. We can see in this table the zeros induced by the Sparsemaxfunction.
