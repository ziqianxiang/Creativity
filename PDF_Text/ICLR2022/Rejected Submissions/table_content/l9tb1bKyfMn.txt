Table 1: Complexity Analysis Between MSA And LMSAATTENTION	PROJECTION	Q or K DIM	TOTAL OPERATION	SAVE(%)	PARAMS	SAVE(%)stantard MSA		^^64	25354	0	12288	0LMSA	Linear	32	16906	33.32	8192	33.33		16	12682	49.98	6144	50LMSA(extreme mode)		2	8986	64.56	4352	64.58stantard MSA	Convolution	^^64	14.38M	0	110592	0LMSA		32	9.58M	33.38	73728	33.33	(kernel size:3x3)	16	7.19M	^30	55296	50LMSA(extreme mode)		2	5.09M	~646	39168	64.58low-quality signals. A single low-quality signal effect on the results is very limited, but the highproportion of low-quality signals will cause a lot of wasted space, and make the training slow,even interfere with peak signals. Instead, we also found in experiments that too few dimensions ofQuery and Key will also cause the loss of key peaks, which will have a destructive effect. Properlyadjusting the dimensions of Query and Key can reduce the overall complexity while maintainingthe original accuracy or even improving accuracy. To explore the data efficiency of Query and Keyat a deeper level, we take the Swin model as an example, use the standard self-attention mechanismand LMSA for training respectively, and count the data efficiency of Query and Key at differentfeatures from low-level to high-level. We use four thresholds of 0.1, 0.5, 1, and 2 to calculate theproportion of signals whose absolute value exceeds these thresholds in Query and Key to the total
Table 2: Comparison of complexity between different efficient attentionEFFCIENTATTENTION	Complexity_________Memory Compressed Attention O(N2 ∙ D)Sparse Attention	O(N√N ∙ D)Longformer	O(N ∙ (k + g) ∙ D)Axial Attention	O(N ∙ (H + W) ∙ D)Refomer	o(n ∙ log(N) ∙ D)Synthesizer	O(N2 ∙ D)LMSA	O(N2 ∙ Dq,k3.4 Comparison with other Efficient AttentionWhen using Transformer for image feature extraction tasks, in order to reintegrate feature infor-mation and reduce feature scales, it is often used in conjunction with downsampling, which makesHigh-Level features have more dimensions, this also shows that it is necessary to optimize the num-ber of dimensions for visual tasks. Table 2 lists the comparison of the computational complexityof the main Efficient Transformer, the work of dimensional optimization is very rare among them.
Table 3: Comparison of image classification results between MSA and LMSAModel	Dataset	Method	Q or K Dim Params(M)		Model Size(M)	Acc(%)	CIFAR10	MSA	[64,128,256,512]	21.8	262.1	95.25		LMSA(extreme)	[2,4,8,16]	18.65	224.4	95.24	CIFAR100	MSA	[64,128,256,512]	21.85	262.6	80.28CSwin		LMSA(extreme)	[2,4,8,16]	18.69	224.5	80.09	Caltech101	MSA	[64,128,256,512]	21.85	262.7	73.62		LMSA(extreme)	[2,4,8,16]	18.7	225	73.63	Caltech256	MSA	[64,128,256,512]	21.86	263.7	70.3		LMSA(extreme)	[2,4,8,16]	18.7	226	71.71	CIFAR10	MSA	[96,192,384,768]	27.52	331.1	93.21		LMSA(extreme)	[3,6,12,24]	23.33	280.9	93.64Swin		MSA	[96,192,384,768]	27.59	331.9	77	CIFAR100	LMSA	[48,96,192,384]	25.43	306.1	77.38		LMSA(extreme)	[3,6,12,24]	23.40	281.7	75.46	CIFAR10	MSA	[64,192,384]	61.4	234.5	85.36CvT		LMSA(extreme)	[1,3,6]	30.8	117.6	85.24	CIFAR100	MSA	[64,192,384]	61.4	246.1	63.83		LMSA(extreme)	[1,3,6]	30.8	123.5	63.48random cropping method, set the patch size to 4 for Patches Embedding, obtained 56x56 features,
Table 4: Comparison of semantic segmentation results between MSA and LMSAModel Method Q or K Dim mIoU(%) mACC(%)	MSA	[32,64,160,256]	22.97	31.74SegFormer	LMSA	[24,48,120,192] [16,32,80,128]	22.95 22.87	31.82 31.79settings for CIFAR-100 image classification training, and observing the results. We compressed2, 4, 8, 16, and 32 times (extreme) respectively. We found that the compression of the model isaccompanied by an increase in data efficiency. After 2 times and 4 times compression, the finalperformance of the model has been improved. But after 16,32 times compression, the accuracy ofthe model is destroyed. This shows that it is not that the lower the dimensionality, the more beneficialthe model. Too low Query and Key dimensionality will cause high-quality signal loss and affectthe overall performance of the model.
