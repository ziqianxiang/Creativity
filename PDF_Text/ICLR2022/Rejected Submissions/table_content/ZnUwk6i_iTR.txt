Table 1: Average rewards per agent in trained models evaluated during 1000 episodes. 7 runs areaveraged for each, using the best checkpoint: this compensates for collapses in performance seenin Fig. 5 and Fig. 6. Values shown are individual rewards to normalize by the number of agents.
Table 2: Results for tests depicted in Fig. 4, evaluated during 1000 episodes for each of 7 differentrandom seeds. SR means average success rate, FR means average failure rate, and RATSO is theratio of average turns to succeed vs. the optimum turns to succeed. 1-SR-FR depicts the ratio ofepisodes where an agent did not reach any grid cell to terminated the test (either successfully orunsuccessfully) before the trial reached the maximum number of turns allowed (5w). A horizontalline means a metric could not be computed.
Table 3: Results for unsuccessful recharge base usage rate, normalized by agent. Bold letteringrepresents the best result of a learned imperfect-information model for each setting (lower is better).
Table 4: Results for wrong communication piece selection count, normalized by agent. Boldlettering represents the best result of a learned imperfect-information model for each setting (loweris better).
Table 5: Results for useless communication piece selection count, normalized by agent. Boldlettering represents the best result of a learned imperfect-information model for each setting (loweris better).
Table 6: Results for useless movement count, normalized by agent. Bold lettering represents thebest result of a learned imperfect-information model for each setting (lower is better).
Table 7: Standard deviations of the average episode reward averaged by seed shown in Table 1.
