Table 1: RL-based memory replay hyperparameter adaptationAlgorithm: Memory Replay with adaptive replay hyperparameterInput: M train memory, Mtest test memory,Bi incoming batch data,θ parameter of CL network, φ parameter of RL agent,α replay ratiofunction RL_Replay(M,Mtest, Bi)BM 〜 M B Sample memory batch from train memoryS — RLstate(θ, Mtest, Bi, BM) B Compute statea J φ(a∣s) B Sample RL action (i.e. replay hyperparameter)θ J joint draining (B M ∪ Bi, θ, a) B Update CL modelr J RL_Reward(。, Mtest) B Compute Rewardφ J U pdateRL(a, r, s) B Update RL agentM J Bi B Update train memoryMtest J Bi B Update test memoryratio is typically set to 1 in previous CL methods. The goal of reinforcement learning is to adapt thereplay ratio for each incoming batch to maximize the performance on the test memory.
Table 2: Continual learning performance in three datasets across 3 runs.
Table 3:	MDP design for RL-based replay ratio adaptationState:average loss and accuracy of test memory 历」——∣ ErCGB	'(f (x; θ), y)|BMtest | x∈BMtestweighted loss and accuracy of incoming batch ABiweighted loss and accuracy of memory batch ABMnumber of unseen classes in the incoming batch |Bi /BiC |number of unseen classes in the memory batch |BM /BMC |Action: replay ratio [0.1, 0.2, 0.3, 0.5, 0.75, 1.0, 1.2, 1.5]Reward: decrease in the test memory lossTable 4:	MDP design for RL-based replay iteration adaptationState:average loss and accuracy of test memory ∣B^——∣ ∑2x∈bm	'(f (x； θ), y)train loss and accuracy of incoming batch B∣ Px∈B '(f (x； θ), y)Action: additional replay iteration [0, 1, 2]Reward: decrease in the test memory lossA.2 Implementation DetailsFollowing (Chaudhry et al., 2019; Mai et al., 2021b), a reduced ResNet18 is used as the backbonemodel for all datasets. We use stochastic gradient descent with a learning rate of 0.1, and the modelreceives a batch with size 10 at a time from the data stream. All the methods are trained with cross-
Table 4:	MDP design for RL-based replay iteration adaptationState:average loss and accuracy of test memory ∣B^——∣ ∑2x∈bm	'(f (x； θ), y)train loss and accuracy of incoming batch B∣ Px∈B '(f (x； θ), y)Action: additional replay iteration [0, 1, 2]Reward: decrease in the test memory lossA.2 Implementation DetailsFollowing (Chaudhry et al., 2019; Mai et al., 2021b), a reduced ResNet18 is used as the backbonemodel for all datasets. We use stochastic gradient descent with a learning rate of 0.1, and the modelreceives a batch with size 10 at a time from the data stream. All the methods are trained with cross-entropy loss except that SCR is trained with supervised contrastive loss. The softmax classifier isused for classification. We use a softmax head for SCR which includes a hidden layer of 1024. Weuse reservoir sampling (Vitter, 1985) for memory management and greedy sampling (Prabhu et al.,2020). We use a memory batch size 10, except that SCR uses a memory batch size of 100. All theexperiments are run across three seeds.
Table 5:	RL-based memory replay with adaptive replay ratioAlgorithm: Joint training with adaptive replay ratioInput: M train memory, MteSt test memory,Bi incoming batch data,θ parameter of CL network, φ parameter of RL agent,α replay ratiofunction RL_Replay_ratio(M,Mtest, Bi)BM 〜M B Sample memory batch from train memoryS J RL.state(θ, MteSt, Bi, BM) B Compute statea J φ(a∣s) B Sample RL actionα J Set_Replay_Para(a B Set replay memory iterationsθ J SGD(BM ∪Bi,θ,α) B Update CL modelr J RL-Reward(θ, MteSt) B Compute Rewardφ J U pdateRL(a, r, s) B Update RL agentM J Bi B Update train memoryMteSt J Bi B Update test memoryTable 6:	Memory replay with adaptive replay iterationAlgorithm: Joint training with adaptive replay iterationInput: M train memory, MteSt test memory,Bi incoming batch data,
Table 6:	Memory replay with adaptive replay iterationAlgorithm: Joint training with adaptive replay iterationInput: M train memory, MteSt test memory,Bi incoming batch data,θ parameter of CL network, φ parameter of RL agent,k additional replay iterationfunction RL_Replay.iter(M,MteSt, Bi)Bm 〜M B Sample memory batch from train memoryθ J SGD(Bm ∪Bi,θ) B Update CL modelS J RLstate(θ, MteSt, Bi, BM) B Compute statea J φ(a∣s) B Sample RL actionk J Set_Replay_Para(a) B Set replay memory iterationsfor i in range(k):Bm 〜M B Sample memory batch from train memoryθ J SGD(Bm ∪Bi,θ) B Update CL modelr J RL-Reward(θ, MteSt) B Compute Rewardφ J U pdateRL(a, r, S) B Update RL agentM J Bi B Update train memoryMteSt J Bi B Update test memoryAugnuuBα)6s① ><
