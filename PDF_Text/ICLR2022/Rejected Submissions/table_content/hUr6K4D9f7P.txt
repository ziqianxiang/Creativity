Table 1: Clean accuracy comparison. We report the average and the standard deviation across 200experiments per model (20 random splits × 10 random initializations). WT-AWP consistently out-perform the standard models on all benchmarks. The improvements are statistically significant ac-cording to a two-sided t-test at a significance level ofp < 0.001.
Table 2: Robust accuracy under PGD and Metattack poisoning attacks, with a 5% adversarial budget.
Table 3: Robust accuracy under evasion attacks of different strength. We report the average and thestandard deviation across 200 experiments per model (20 random splits × 10 random initializations).
Table 4: Hyperparameter sensitivity study for λ and ρ on the Cora dataset for a GCN base model.
Table 5: Ablation study with λ and ρ on WT-AWP, where the weight perturbation is calculated with5-step PGD. The backbone model is GCN and the benchmark is Cora.
Table 6: Performance of WT-AWP on graph classification tasks, the backbone is GCN.
Table 7: Robust accuracy with 5% poisoning DICE attacks. We report the average and the standarddeviation across 200 experiments per model (20 random splits × 10 random initializations).
Table 8: Hyperparameters of WT-AWP for poisoning DICE attacks(λ, ρ)	Cora	Citeseer	PolblogsGCN GCNJaccard GCNSVD	(0.5, 0.5)	(0.7, 2)	(0.3, 1)SimPGCN	(0.1,0.5)	(0.5, 0.1)	(0.5, 1)C.3 Certified robustness on Citeseer datasetWe measure the certified robustness of GCN and GCN+WT-AWP with randomized smoothing (Bo-jchevski et al., 2020) on the Citeseer dataset. We use λ = 0.5, ρ = 1 as the hyperparameters forWT-AWP models. We plot the certified accuracy S(ra, rd) w.r.t. ra and rd. As seen in Fig. 10,comparing with the vanilla GCN, WT-AWP significantly increases the certified accuracy for pertur-bations to the node features for all radii, while having comparable performance for certification ofthe graph structure.
Table 9: Dataset StatisticsDatasets	Cora	Citeseer	Polblogs#NodeS	2708	-^3327^^	1222#EdgeS	5429	4732	16714#FeatureS	1433	3703	N/A#CIaSSeS	7	6	2D.4 Training SetupOptimization hyperparameters. We use the Adam optimizer with a learning rate 0.01 and weightdecay of 0.0005. All models are trained for 200 epochs with no weight scheduling. We add a dropoutlayer with rate p = 0.5 after each GNN layer during training. We apply no early stopping and theoptimal model is selected with its performance on the validation set. The test set is never touchedduring training.
Table 10: Hyperparameters of WT-AWP for poisoning PGD attack and Metattack of Sec. 5.4(λ, ρ)	Cora	Citeseer	PolblogsGCN GCNJaccard GCNSVD	(0.7, 0.5)	(0.7, 2)	(0.5, 0.5)SimPGCN	(0.3, 0.5)	(0.5, 0.1)	(0.3, 2) Metattack (0.5, 0.5) PGDD.8 Randomized smoothingFollowing Bojchevski et al. (2020), we create smoothed versions of our GNN models by randomlyperturbing the adjacency matrix (or the node features) and predicting the majority vote for therandomly-perturbed samples. We denote with pa the probability of flipping an entry from 0 to 1,i.e. adding an edge or a feature, and with pd the probability of flipping an entry from 1 to 0, i.e.
Table 11: Dataset Statistics.
Table 12: Ablation study with λ and P on WT-AWP, where we only use AWP on the last layer. Thebackbone model is GCN and the benchmark is Cora.
