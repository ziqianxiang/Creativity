Table 1: Accuracy on CIFAR-10/100 datasetsA clear benefit of such generalization is that given anew architecture, our GHN can immediately populateit to give a better initialization. To test this we ran a“leave one out” experiment using CIFAR-10 dataset,where we let 3 clients with 3 different architecturestrain in an FLHA fashion. We then introduce a 4th ar-chitecture and refine it using only local data. We com-pare against training that architecture from scratchon that same data. The results shown in Figure 3 andTable 6 in the supplementary are very encouraging.
Table 2: The importance of the GNN. Under an FLHA setup we use 4 architectures sharing arepeated layer type. The result show significant gains due to message passing informing the layerencoding with their placement within the architecture.
Table 3: Average AUC on NIH Chest X-ray dataset.
Table 4: Each row in the table shows the unbalanced class distribution for 4 clients, with the originalbalanced distribution of the left. Three different α values are shown: (top) α = 100, (mid) α = 1,(bottom) α = 0.1G	Unbalanced distributionIn collaborative training between different entities, clients’ data may be distributed unevenly. Inmedical data, for example, this may occur when clinics specialize in certain diseases or use differentsensors. Thus, in addition to architectural differences, FLHA can also have data imbalance. Here westudy the behavior of FLHA under such unbalanced data distributions.
Table 5: FLHA with unbalanced distribution. In the table, α corresponds to the level of unbalanced,e.g.
Table 6: Generalization to unseen architectures: leave-one-architecture-out experiment on CIFAR-10.
Table 7: Training with a much smaller architecture shows an average performance drop by 2.2 ± 1.4pts. However, this is well above the local training alternative.
