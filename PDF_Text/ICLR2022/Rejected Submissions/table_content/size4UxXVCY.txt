Table 1: the result of Experiment 1Task layers	47(=10+35+2) class	Transfer learning	Fine tuningModel	MNIST	SC	IMDB	MNIST	SC	IMDB	MNIST	SC	IMDB	MNIST	SC	IMDBLeNet-5	98.46	-	-	98.46	-	-	98.49	-	-	98.49	-	-M5 (Group Norm)	-	97.63	-	-	97.63	-	-	100.0	-	-	100.0	-CNN	-	-	87.13	-	-	87.13	-	-	87.48	-	-	87.48GTC	98.84	97.14	86.10	98.58	96.87	85.88	98.39	98.81	82.27	98.87	99.07	86.99GTAs	98.96	96.91	85.38	98.77	96.83	85.76	98.65	98.91	82.20	98.64	97.39	86.62GTR	98.53	96.54	85.62	98.79	97.07	86.24	98.48	99.66	81.91	98.94	99.17	87.18GTRAs	98.79	96.66	86.28	98.78	96.68	85.79	98.47	98.64	82.18	98.92	98.89	87.27FE Epochs	30	30	30	30	30	30	26	95	3	26	95	3We trained without using a pre-trained model to check whether various feature extraction models canbe learned simultaneously. Only word embedding network(Pennington et al., 2014) was used as apretrained model. Existing networks learn only about specific datasets. Therefore, LeNet-5(LeCunet al., 1998), M5(Dai et al., 2017), and CNN(Kim, 2014) were selected for feature extraction networksof the image(MNIST(LeCun et al., 1998)), sound(Speech Command(Warden, 2018)), and naturallanguage(IMDB(Maas et al., 2011)). We described more details in appendix C. The reason forchoosing these networks is based on Convolutional Neural Networks specialized extract features.
Table 2: the result of association testModel	Task 2 + task layer (%)	task 2 + fine-tuning (%) MNIST MNIST&All SC SC&All IMDB IMDB&All MNIST MNIST&All SC SC&All IMDB IMDB&AllGTC GTAs GTR GTRAs	98.90	98.88	96.83	95.71	87.20	87.18	98.63	98.62	98.83	97.66	86.76	86.61 98.87	98.88	97.30	96.37	86.73	87.41	98.93	98.87	98.67	98.52	87.43	87.30 99.05	99.04	96.46	97.03	87.16	87.21	98.97	98.84	98.83	98.58	87.00	87.14 98.95	98.96	97.36	95.76	86.55	87.08	98.82	98.73	98.60	98.22	87.30	87.26FE Baseline	98.46	98.46	97.63	97.63	87.13	87.13	98.49	98.49	100.0	100.0	87.48	87.48We constructed the GT dataset described above(Fig.4(a,b,c,d)) to validate if the output vector cancontain all the information in the GT, and the results are as follows Table.2. The meaning of FEBaseline is the performance of networks of LeNet-5, M5, and CNN. In these experiments, it hasbeen verified that it is possible to learn various types of datasets using one network cell and thatinformation can be embedded together. Consequently, we can share an association cell or layers tolearn without being limited to the dataset type and embed all information inside the GT into a vector.
Table 3: Compare ArchitectureName	GTC	GTRLevel layer	O	XThe number of W	The number of levels	1Depth-limited	Fixed	Not fixedThe number of parameters	Use more	Use less thanInput size by level	Adjustable	FixedThe special cases	FCNN, MLP	RNN(recurrent, recursive)In this section, we compare the network characteristics of GTC-series and GTR-series recursivemodels. In the case of GTC, there is a different Wlv for each level, expressing each level layer andthe convolution of all information. On the other hand, in the case of GTR, the main difference isthe recursive convolution with only one W as a cell. This is the most significant difference whencomparing the networks.
Table 4: Feature Extraction Networks for Image datasetModel	LeNet-5				LeNet-5 for GTNN					In	Out	kernel	stride	activation	In	Out	kernel	stride	activationConv2D	1	6	(5,5)	1	tanh	1	6	(5,5)	1	tanhAvgPool2D	-	-	(2,2)	1	-	-	-	(2,2)	1	-Conv2D	6	16	(5,5)	1	tanh	6	16	(5,5)	1	tanhAvgPool2D	-	-	(2,2)	1	-	-	-	(2,2)	1	-Conv2D	16	120	(5,5)	1	tanh	16	120	(5,5)	1	tanhFC layer 1	120	84	--	tanh	-	-	-	-	-FC layer 2	84	10	--	softmax	-	-	-	-	-Zero padding	-	-	--	-	120	128	-	-	-Final	-	10	--	-	-	128	-	-	-LeNet-5(LeCun et al., 1989) was used as the image feature extraction network. We create a dimensionof 128 by applying zero padding to the extracted features without using the affine-layer(FC layers)and then forward it to GTNN.
Table 5: Feature Extraction Networks for Sound datasetModel	M5(Group Norm)						M5(Group Norm) for GTNN						In	Out	kernel	stride	norm	activation	In	Out	kernel	stride	norm	activationConv1D	1	128	80	4	group 16	relu	1	128	80	4	group 16	reluMaxPool1D	-	-	4	1	-	-	-	-	4	1	-	-Conv1D	128	128	3	1	group 16	relu	128	128	3	1	group 16	reluMaxPool1D	-	-	4	1	-	-	-	-	4	1	-	-Conv1D	128	256	3	1	group 16	relu	128	256	3	1	group 16	reluMaxPool1D	-	-	4	1	-	-	-	-	4	1	-	-Conv1D	256	512	3	1	group 16	relu	256	512	3	1	group 16	reluMaxPool1D	-	-	4	1	-	-	-	-	4	1	-	-AdaptiveAvgPool1d	-	1	-	-	-	-	-	1	-	-	-	-FC layer	512	35	-	-	-	log softmax	512	128	-	-	-	leaky reluFinal	-	35	-	-	-	-	-	128	-	-	-	-13Under review as a conference paper at ICLR 2022M5(Dai et al., 2017) was used as the sound feature extraction network. As described above(Sec.2.2),we used group normalization(Wu & He, 2018) without using Batch norm(Ioffe & Szegedy, 2015). Weuse the affine layer(FC layer) and deliver it to the GTNN in 128 dimensions. It could be implementedin torchaudio1 .
Table 6: Feature Extraction Networks for Natural language datasetModel	CNN					CNN for GTNN					In	Out	kernel	stride	activation	In	Out	kernel	stride	activationConv2D	1	100	(3,3)	1	relu	1	100	(3,3)	1	reluAvgPool2D	-	-	(2,2)	1	-	-	-	(2,2)	1	-Conv2D	1	100	(4,4)	1	relu	1	100	(4,4)	1	reluAvgPool2D	-	-	(2,2)	1	-	-	-	(2,2)	1	-Conv2D	1	100	(5,5)	1	relu	1	100	(5,5)	1	reluConcat	(300,400,500)	1200	-	-	-	(300,400,500)	1200	-	-	-Dropout	-	-	-	-	-	-	-	-	-	-FC layer	1200	1	-	-	-	1200	128	-	-	-Final	-	1	-	-	-	-	128	-	-	-CNN(Kim, 2014) was used as the feature extraction network for natural language processing. Weslightly modified the contents in this paper and combined them with GTNN. We did not use drop outbecause we wanted to emphasize the difference between transfer learning and general learning in theassociation model. The glove(Pennington et al., 2014) was used for the pre-trained word embeddingnetwork with 100 dimensions. We used an adam optimizer(Kingma & Ba, 2014), a learning rate of0.001, a cosine annealing(T max=2, eta min=1e-05) for schedulers(Loshchilov & Hutter, 2016). thebatch-size is 32 and the MNIST, Speech Command and IMDB classes are 10, 35, 2.
Table 7: the result of Experiment 349(=10+35+2+2) classModel	MNIST	SC	SST	UPFD-GOSLeNet-5	98.52	-	-	-M5 (Group Norm)	-	98.37	-	-RNN(Recursive)	-	-	77.69	-GCN	-	-	-	93.67GATs	-	-	-	93.93GTC	98.88	97.18	73.30	94.74GTAs	98.80	96.67	73.39	94.85GTR	98.68	97.46	76.74	93.15GTRAs	98.35	97.44	76.79	94.67class count	10	35	2	2type	image	sound	tree	graphepochs, the performance in test dataset is in the table7.
