Table 1: Test accuracy (%) comparison among different dynamical isometry maintenance or recov-ery methods on CIFAR10/100. “Scratch” stands for training from scratch. Each setting is randomlyrun for 3 times, mean (std) accuracies reported. “KernOrth” means Kernel Orthogonalization (Xieet al., 2017); “OrthConv” means Convolutional Orthogonalization (Wang et al., 2020). Two finetun-ing LR schedules are evaluated here: initial LR 1e-2 vs. 1e-3. “Acc. diff.” refers to the accuracygap ofLR 1e-2 over LR 1e-3.
Table 2: Speedup comparison on ImageNet. FLOPs: ResNet34: 3.66G, ResNet50: 4.09G.
Table 3: Ablation Study: Test accuracy (without finetuning) comparison with or without the pro-posed BN regularization.
Table 4: Training setting summary. For the SGD solver, in the parentheses are the momentum andweight decay. For LR schedule, the first number is initial LR; the second (in brackets) is the epochswhen LR is decayed by factor 1/10; and #epochs stands for the total number of epochs.
Table 5: HyPer-Parameters of our methods.
Table 6: Summary of layer-wise pruning ratios.
Table 7: Sensitivity analysis of Ku on CIFAR10/100 datasets with the proposed OPP algorithm.
Table 8: Sensitivity analysis of ∆ on CIFAR10/100 datasets with the proposed OPP algorithm.
Table 9: Mean JSV and test accuracies (%) of MLP-7-Linear on MNIST under different pruningratios. Each result is randomly run for 3 times. We report the mean accuracy and (std). “ft.” is shortfor finetuning.
