Table 1: Parameter count for different architectures and data sets.
Table 2: Parameter tradeoff for different models and datasets. The maximum number of possible tasksfor each data set is given in brackets in the first column. For ImageNet we have assumed that entireset of classes is split into 10-way tasks. For RotatedMNIST we have assumed a 1 degree granularityof rotations. O(params) is the number of parameters of the network (weights only, assuming biasesare set to zero). Mask Size is the space on disk required to store each additional SupSup mask as16-bit integers. |M| was approximately chosen to give either benchmark or very good performanceon new tasks. ∣αt∣ is the number of parameters (floating point) per additional new task, given a basisof size |M|. Φ is the storage per task amortizing the cost of storing |M| basis masks over all possibletasks. Cx is the compression or savings ratio compared to SupSup. f marks those architectures wehave not run due to resource constraints. For those we have interpolated the size of the basis set |M|from those architectures/data sets that we ran.
Table 3: Dataset, Model and Hyperparameter Overiew.
