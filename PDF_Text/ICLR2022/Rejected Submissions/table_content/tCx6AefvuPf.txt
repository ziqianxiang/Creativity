Table 1: Test accuracy of DP-GNN compared to the baselines on different datasets. DP-GNN clearly performs better than the Private and Non-Private MLP baselines across these datasets.			Algorithm	ogbn-arxiv	ogbn-products	ogbn-magGCN	68.422 ± 0.267=Z	76.139 ± 0.519	34.680 ± 0.424DP-GNN (Adam)	63.934 ± 0.469	69.576 ± 0.276	30.059 ± 0.252DP-GNN (SGD)	64.137 ± 0.621	69.041 ± 0.126	30.147 ± 0.241MLP	55.236 ± 0.317	61.364 ± 0.132	26.969 ± 0.361DP-MLP	53.462 ± 0.242	61.064 ± 0.110	25.259 ± 0.321As mentioned earlier, in several data critical scenarios, practitioners cannot use sensitive graph in-formation, and have to completely discard GNN based models due to privacy concerns. Hence, themain benchmark of our evaluation is to demonstrate that DP-GNN is able to provide more accuratesolutions than standard methods that completely discard the graph information. The key baselinesfor our method are both standard non-private MLP models as well as differentially private MLPmodels trained using DP-SGD and DP-Adam. We also compare against the standard 1-layer GCNs(without any privacy guarantees) as it bounds the maximum accuracy we can hope to achieve out ofour method.
Table 2:	GCN and DP-GNN on the ogbn-arxiv dataset with different batch sizes.
Table 3:	GCN and DP-GNN on the ogbn-arxiv dataset with different degrees. Theprivacy budget for DP-GNN is ε ≤ 30.
Table 4: Test accuracy of DP-GNN (Adam) on the ogbn-arxiv dataset with a privacy budget ofε ≤ 30.	Architecture Non-PrivateGNN	DP-GNN GCN	68.422 ±	0.267	63.934	±	0.469 GIN	67.485 ±	0.391	63.888	±	0.709 GAT	65.702 ±	0.674	58.853	±	0.24619Under review as a conference paper at ICLR 2022D Learning Graph Convolutional Networks (GCN) via DP-AdamIn Algorithm 3, we provide the description of DP-Adam, which adapts Algorithm 1 to use the pop-ular Adam (Kingma & Ba, 2014) optimizer, instead of SGD. The privacy guarantee and accountingfor Algorithm 3 is identical to that of Algorithm 1, since the DP clipping and noise addition stepsare identical.
Table 5: Statistics of datasets used in our experiments. On all of these datasets, the task is toclassify individual nodes into one of multiple classes.
Table 6: Best hyperparameters corresponding to each method across datasets to reproduce the resultsin the main paper.
