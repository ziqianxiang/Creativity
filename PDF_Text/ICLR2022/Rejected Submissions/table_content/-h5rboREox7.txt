Table 1: Performance of our method on different datasets. * denotes the hyper-parameter automaticallydetermined by our method.
Table 2: Performance of our method with different adversarial training methods.
Table 3: Performance of our method with different neural architectures.
Table 4: Performance of our method under different adversarial attacks. PGD-1000 refers to PGDattack with 1000 attack iterations, with step size fixed as 2/255 as recommended by Croce & Hein(2020).
Table 5: Performance of our method combined with SWA and additional standard teacher on differentdatasets.
Table 6: Performance of our method combined with SWA with different hyper-parametersD Study on a synthetic dataset with known true labelDISTRIBUTIONhorse + automobileO10-20300 IO 20	30Figure 12: Examplemixup augmentation.
