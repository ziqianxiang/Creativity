Table 1: Network Architecture for MNIST and Fashion MNIST ExperimentsEncoder Network	Decoder NetworkInput ∈ R28×28 conv: chan: 64 , kern: 4, stride: 2, pad: 1 BatchNorm: feat: 64 ReLU conv: chan: 64, kern: 4, stride: 2, pad: 1 BatchNorm: feat: 64 ReLU conv: chan: 64, kern: 4, stride: 1, pad: 0 BatchNorm: feat: 64 ReLU Linear: 2 Units	Input ∈ R2 Linear: 3136Units ReLU convTranpose: chan: 64, kern: 4, stride: 1, pad: 1 BatchNorm: feat: 64 ReLU convTranpose: chann: 64, kern: 4, stride: 2, pad: 2 BatchNorm: feat: 64 ReLU convTranpose: chan: 1, kernel: 4, stride: 2, pad: 1 SigmoidTable 2:	Training parameters for the transport operator training phase of the MNIST experimentMNIST TransPort OPerator Training Parameters "[batch size: 250autoencoder training epochs: 300transport operator training epochs: 50latent space dimension (zdim): 10M : 16lrnet ： 10-4lrψ : 10-3ζ ： 0.1γ :2 X 10-6initialization variance for Ψ: 0.05number of restarts for coefficient inference: 1nearest neighbor count: 5latent scale: 30Table 3:	Training parameters for the fine-tuningphase of the MNIST experiment
Table 2:	Training parameters for the transport operator training phase of the MNIST experimentMNIST TransPort OPerator Training Parameters "[batch size: 250autoencoder training epochs: 300transport operator training epochs: 50latent space dimension (zdim): 10M : 16lrnet ： 10-4lrψ : 10-3ζ ： 0.1γ :2 X 10-6initialization variance for Ψ: 0.05number of restarts for coefficient inference: 1nearest neighbor count: 5latent scale: 30Table 3:	Training parameters for the fine-tuningphase of the MNIST experimentMNIST Fine-tuning ParameterS -[batch size: 250transport operator training epochs: 100
Table 3:	Training parameters for the fine-tuningphase of the MNIST experimentMNIST Fine-tuning ParameterS -[batch size: 250transport operator training epochs: 100lrnet ： 10-4lrψ : 10-3ζ ： 0.1γ:2 ×10-6λ: 0.75number of network update steps: 50number of Ψ update steps: 50Table 4: Training parameters for theMNIST Coefficient EncoderMNIST COeffiCient EnCOder ParameterSbatch size: 250training epochs: 300lr: 10-3Zprior : 0.1λjd: 0.5
Table 4: Training parameters for theMNIST Coefficient EncoderMNIST COeffiCient EnCOder ParameterSbatch size: 250training epochs: 300lr: 10-3Zprior : 0.1λjd: 0.5coefficient spread scale: 0.1classifier domain: imageG MNIST Experiment Additional ResultsHere we show additional experimental details and results for the MNIST experiment. Fig. 12 showsthe magnitude of all 16 operators after the fine-tuning phase. Six of the operators have their magni-tudes reduced to zero. Fig. 13 shows the paths generated by transport operators trained on MNISTdata.
Table 5: Training parameters for the transport operator training phase of the Fashion-MNIST exper-imentFashion-MNIST TransPort OPerator Training Parametersbatch size: 200autoencoder training epochs: 300transport operator training epochs: 50latent space dimension (Zdim): 10M : 16lrnet ： 10—4lrψ : 10-3Z : 0.5Y : 2 X 10-5initialization variance for Ψ: 0.05number of restarts for coefficient inference: 1nearest neighbor count: 5latent scale: 30Table 6: Training parameters for the fine-tuning phase of the Fashion-MNIST experiment	Table 7: Training parameters for the Fashion-				Fashion-MNIST Fine-tuning Parameters	MNIST CoeffiCient EnCoder		batch size: 200 transport operator training epochs: 150 lrnet : 10—4 lrψ : 10—3 Z : 0.5 γ:2 × 10-6 λ: 0.75 number of network update steps: 50		Fashion-MNIST Coefficient Encoder Parameters			batch size: 200 training epochs: 300 lr: 10—3 Zprior : 0∙5 λjd: 0.5 coefficient spread scale: 0.1 classifier domain: latent
Table 6: Training parameters for the fine-tuning phase of the Fashion-MNIST experiment	Table 7: Training parameters for the Fashion-				Fashion-MNIST Fine-tuning Parameters	MNIST CoeffiCient EnCoder		batch size: 200 transport operator training epochs: 150 lrnet : 10—4 lrψ : 10—3 Z : 0.5 γ:2 × 10-6 λ: 0.75 number of network update steps: 50		Fashion-MNIST Coefficient Encoder Parameters			batch size: 200 training epochs: 300 lr: 10—3 Zprior : 0∙5 λjd: 0.5 coefficient spread scale: 0.1 classifier domain: latentnumber of Ψ update steps: 50I Fashion MNIST Experiment Additional ResultsHere we show additional experimental details and results for the Fashion-MNIST experiment.
Table 8: Network Architecture for CelebA ExperimentsEncoder Network	Decoder Network,Input ∈ R64×64	Input ∈ R32conv: chan: 32 , kern: 4, stride: 2, pad: 1	Linear: 80,000 UnitsBatchNorm: feat: 32	ReLUReLU	convTranpose: chan: 256, kern: 3, stride: 1, pad: 0conv: chan: 64, kern: 4, stride: 2, pad: 1	BatchNorm: feat: 256BatchNorm: feat: 64	ReLUReLU	convTranpose: chann: 256, kern: 3, stride: 1, pad: 0conv: chan: 128, kern: 3, stride: 2, pad: 1	BatchNorm: feat: 256BatchNorm: feat: 128	ReLUReLU	convTranpose: chan: 256, kernel: 3, stride: 1, pad: 1conv: chan: 256, kern: 3, stride: 1, pad: 1	BatchNorm: feat: 256BatchNorm: feat: 128	ReLUReLU	convTranpose: chan: 128, kernel: 3, stride: 1, pad: 1conv: chan: 256, kern: 4, stride: 2, pad: 1	BatchNorm: feat: 128BatchNorm: feat: 256	ReLUReLU	convTranpose: chan: 128, kernel: 3, stride: 1, pad: 0conv: chan: 128, kern: 4, stride: 2, pad: 1	BatchNorm: feat: 128BatchNorm: feat: 128	ReLU
Table 9: Training parameters for the CelebA experimentCelebA Transport Operator Training Parametersbatch size: 500autoencoder training epochs: 300transport operator training epochs: 50latent space dimension (zdim): 32M : 40lrnet : 10-4lrΨ : 10-3ζ : 1.5γ :1 X 10-5initialization variance for Ψ: 0.05number of restarts for coefficient inference: 1nearest neighbor count: 5latent scale: 2CelebA Fine-tuning Parametersbatch size: 500fine-tuning epochs: 10lrnet : 10-4lrΨ : 10-3
