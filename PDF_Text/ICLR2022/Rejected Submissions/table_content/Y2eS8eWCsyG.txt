Table 1: Dataset comparison. Thr. = Throughoutly annotated: every instance of every class isannotated in every image. *LVIS has potentially more objects and categories per image than areannotated due to the non-exhaustive labeling.
Table 2: On COCO and Pascal VOC there is a clear performance gap (AP50) between categoriesused during training (Train) and held-out categories (Held-Out). A baseline getting a black imageas reference which contains no information about the target category (- empty Refs.) performssurprisingly well on Pascal VOC but fails on COCO.
Table 3: Effect of a three times longer training schedule and a larger backbone (ResNeXt-101 32x4d)on model performance across datasets. While larger models and longer training times lead to no oronly minor improvements on held-out categories on COCO, they do have a larger effect on LVISand Objects365.
Table 4: Performance (AP50) on COCO can be improved by training on LVIS. Siamese Mask R-CNN and Siamese Cascade R-CNN are identical to Siamese Faster R-CNN except for an additionalmask head or cascaded bbox heads. (*Michaelis et al. (2018b), ** Hsieh et al. (2019), *** Chenet al. (2021))vs. FRCNN: 46%) but results are very similar on LVIS (Retina: 87% vs. FRCNN: 89%) andObjects365 (Retina: 74% vs. FRCNN: 76%).
