Table 2: Correlation (Pearson coefficient) of correct predictions on the test set between differentpairs of models. The diagonal entries correspond to models trained with the same prior but fromdifferent random initializations. While the two shape-biased models (Sobel and Canny) are morealigned with each other, they are both quite different from the texture-biased model (BagNet).
Table 3: Ensemble accuracy when combining models trained with a diverse set of feature priors(models with the same prior are trained from different random initialization). Notice how modelstrained with different priors lead to ensembles with better performance. Moreover, when the accu-racy of the two base models is comparable, models that are more diverse (as measured in Table 2)result in better ensembles. We describe the different methods of combining models in Appendix A.4and provide the full results in Appendix B.2.
Table 5: Test accuracy of self-training and co-training methods on STL-10 and CIFAR-10. Foreach model, we report the original accuracy when trained only labeled data (Column 3) as well asthe accuracy after being trained on pseudo-labeled data (Column 4). (Recall that, for the case ofco-training, pseudo-labeling is performed by combining the predictions of both models.) Finally,we report the performance of a standard model trained from scratch on the resulting pseudo-labels(Column 5). We provide 95% confidence intervals computed via bootstrap with 5000 iterations.
Table 7: Test accuracy of self-training and co-training on tinted STL-10 and CelebA, two datasetswith spurious features (table structure is identical Table 5). In both datasets, the spurious correlationis more easily captured by the BagNet and Standard models over the shape-based ones. Neverthe-less, when co-trained with a shaped-biased model, BagNets are able to significantly improve theirperformance, indicating that they rely less on this spurious correlation. CI: 95% bootstrap.
Table 11: Hyperparameters chosen through grid search for each dataset-prior pair (we used theSTL-10 hyperparameters for the tinted STL-10 dataset). LR corresponds to the learning rate, γ tothe factor used to decay the learning rate at each drop, and K to the train epochs between eachlearning rate drop.
Table 12: Full results for ensembles of pre-trained models.
Table 13: Ensemble performance when combining self-trained models with Standard, Canny, Sobel,and BagNet priors. When two models of the same prior are ensembled, the models are trained withdifferent random initializations.
Table 14: Full results for ensembles of self-trained models.
Table 15: Performance of ensembling pre-trained and self-trained models with stacked ensemblingon CIFAR-10	Pre-trained	Self-trained	 Feature Priors	Stacked Model 1	Model 2 Ensemble	Stacked Model 1	Model 2 EnsembleStandard + Standard Canny + Canny BagNet + BagNet	53.73 ± 0.86	55.38 ±	1.00	56.01	± 0.94 56.29 ± 0.92	54.99 ±	0.96	57.70	± 0.90 52.04 ± 0.92	50.34 ±	0.90	52.35	± 0.97	59.92 ± 0.95	59.34 ±	0.88	60.54 ±	0.91 58.40 ± 0.94	57.69 ±	0.94	59.23 ±	0.99 57.80 ± 0.96	58.11 ±	0.85	59.48 ±	0.98Standard + Canny Standard + BagNet Canny + BagNet	53.73 ± 1.00^^56.29 ± 0.94 59.24 ± 0.88 53.73 ± 0.95	52.04 ± 0.90 56.03 ± 0.98 56.29 ± 0.96 52.04 ± 0.95 59.98 ± 0.91	59.92 ± 0.90^^58.40 ± 0.95^^63.42 ± 0.89 59.92 ± 0.94 57.80 ± 0.96 62.59 ± 0.91 58.40 ± 0.94 57.80 ± 0.96 63.22 ± 0.94Table 16: Performance of ensembling pre-trained and self-trained models with stacked ensemblingon STL-1024Under review as a conference paper at ICLR 2022B.5 Self-Training and Co-Training on STL-10 and CIFAR- 1 0Methods	Prior(s)	Labeled Only	+Unlabeled Self/Co-Training	+ Standard model with Pseudo-labels	Standard	52.54 ± 0.86	63.65 ± 0.76	64.02 ± 0.82Self-training	Canny	45.48 ± 0.90	51.82 ± 0.82	55.59 ± 0.80	Sobel	51.94 ± 0.88	63.05 ± 0.84	64.77 ± 0.80	BagNet	42.22 ± 0.82	53.92 ± 0.89	54.21 ± 0.85	Standard	52.54 ± 0.91	65.06 ± 0.76	65.10 ± 0.84	+Standard	51.82 ± 0.86	64.93 ± 0.80		Canny	45.48 ± 0.85	-^51.15 ± 0.79	55.74 ± 0.80	+Canny	44.19 ± 0.82	51.65 ± 0.81	
Table 16: Performance of ensembling pre-trained and self-trained models with stacked ensemblingon STL-1024Under review as a conference paper at ICLR 2022B.5 Self-Training and Co-Training on STL-10 and CIFAR- 1 0Methods	Prior(s)	Labeled Only	+Unlabeled Self/Co-Training	+ Standard model with Pseudo-labels	Standard	52.54 ± 0.86	63.65 ± 0.76	64.02 ± 0.82Self-training	Canny	45.48 ± 0.90	51.82 ± 0.82	55.59 ± 0.80	Sobel	51.94 ± 0.88	63.05 ± 0.84	64.77 ± 0.80	BagNet	42.22 ± 0.82	53.92 ± 0.89	54.21 ± 0.85	Standard	52.54 ± 0.91	65.06 ± 0.76	65.10 ± 0.84	+Standard	51.82 ± 0.86	64.93 ± 0.80		Canny	45.48 ± 0.85	-^51.15 ± 0.79	55.74 ± 0.80	+Canny	44.19 ± 0.82	51.65 ± 0.81		Sobel	51.94 ± 0.86	~^67.18 ± 0.80	68.47 ± 0.74	+Sobel	53.69 ± 0.89	67.35 ± 0.77		Canny	45.48 ± 0.79	-^58.66 ± 0.81	65.34 ± 0.81	+Sobel	51.94 ± 0.80	64.87 ± 0.79		Canny	45.48 ± 0.85	-^59.19 ± 0.85	67.59 ± 0.74Co-training	+BagNet	42.22 ± 0.85	67.92 ± 0.79	
Table 17: Performance of self-training and co-training on CIFAR-10 for each prior combination.
Table 18: Performance of self-training and co-training on STL-10 for each prior combination.
Table 19: Performance of co-training approaches with different amounts of training data for STL-10.
Table 20: Similarity between models before and after training on pseudo-labeled data. Our measureof similarity is the (Pearson) correlation between which test examples are correctly predicted byeach model. In Columns 3 and 5 we report that notion of similarity between the pre-trained feature-biased models and the pre-trained standard model (the numbers are reproduced from Table 2). Then,in columns 4 and 6 we report the similarity between the feature-biased models at the end of self-or co-training and the standard model trained on their (potentially combined) pseudo-labels. Weobserve that through this process of training a standard model on the pseudo-labels of differentfeature-biased models, the former behaves more similar to the latter.
Table 21: Performance of ensembles consisting of models trained with different priors.
Table 22: Performance of individual ensembles on datasets with spurious correlations.
Table 23: Accuracy of predicting gender on different subpopulations of the CelebA dataset. Weshow the accuracy of standard models trained on the pseudo-labels produced by different self- orco-training schemes. Recall that in the training set all females are blond and all males are non-blond(while the unlabeled dataset is balanced). It is thus interesting to consider where this correlation isreversed. We observe that, in these cases, both the standard and BagNet models perform quite poorly,even after being self-trained on the unlabeled dataset where this correlation is absent. At the sametime, co-training steers the models away from this correlation, resulting in improved performance.
Table 24: Performance of Self-Training and Co-Training techniques When the unlabeled data alsocontains a complete skeW toWard hair color (as in the labeled data). 95% confidence intervals com-puted via bootstrap are shoWn.
