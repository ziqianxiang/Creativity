Table 1: Out-of-distribution (OOD) accuracies for the standard model, robust model, and calibratedensembles, across six datasets. Calibrated ensembling matches or outperforms the better model in5/6 cases, and on average outperforms both the standard and robust models. For the remainingdataset, DomainNet, calibrated ensembles close 96% of the gap between the standard and robustmodel.
Table 2: In-distribution (ID) accuracies for the standard model, robust model, and calibratedensembling, across six datasets. Calibrated ensembling matches or outperforms the better modelin 5/6 cases, and on average outperforms both the standard and robust models. For the remainingdataset, CIFAR-10, calibrated ensembles close 97% of the gap between the standard and robustmodel.
Table 3: Calibrated ensembles are competitive with self-training (Xie et al., 2021) ID and OOD,which requires unlabeled data.
Table 4: OOD accuracies: calibrated ensembles outperform vanilla ensembles and even tunedensembles where the combination weights are tuned to maximize in-distribution accuracy. Averagedacross the datasets, calibrated ensembles get an OOD accuracy of 74.6%, while tuned ensembles getan accuracy of 71.3%. The in-distribution accuracies of the methods are very close (within 0.2% ofeach other).
Table 5: ID accuracies: The in-distribution accuracies of calibrated ensembles, tuned ensembles,and vanilla ensembles are very close (within confidence intervals), so any of these methods areacceptable if we are looking at in-distribution accuracy. However, they perform quite differentlywhen it comes to OOD accuracy (Table 4).
Table 6: OOD ECE: The expected calibration error (ECE) of the standard and robust models onOOD test data, after post-calibration in ID validation data. The calibration errors here are high,especially compared to the ID calibration errors in Table 7.
Table 7: ID ECE: The expected calibration error (ECE) of the standard and robust models on ID testdata, after post-calibration in ID validation data. The calibration errors are fairly lowâ€”note that weonly use 500 examples to temperature scale, so for ImageNet we have fewer examples than classesfor post-calibration, but the models are still fairly well calibrated.
Table 8: We show the accuracy gap (difference between the accuracy of the standard and robustmodel) and the confidence gap (difference in confidence between the two) before and after calibra-tion. The accuracy gap and confidence gap typically have the same sign, and this improves aftercalibrating ID.
