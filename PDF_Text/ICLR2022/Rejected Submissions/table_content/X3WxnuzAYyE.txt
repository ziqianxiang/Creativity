Table 1: Comparison of existing attention modules in terms of whether previous knowledge cross-channel interaction (PKCCI), attention dimension, where C indicates channel attention and S indicatesspatial attention, and lightweight or not.
Table 2: Comparison of different previous knowledge Aggregation(PKA) techniques on the Tiny-ImageNet dataset. Where 1-D Conv., Sum and FC stands for one dimensional convolution layer Eq.3,summation Eq.2, and fully connected layer Eq.1 respectively.
Table 3: Comparison of the various basic attention modules in our proposed module PKCAM.
Table 4: Comparison of the various ways to integrate PKCAM into CNNs using the ImageNet dataset.
Table 5: Comparisons with state-of-the-art attention modules on ImageNet in terms of the number ofparameters (#P.) in millions, GFLOPs, top-1, and top-5 accuracy. Top-1 relative improvement resultsare reported between parentheses w.r.t SENet improvement over Vanilla Resnet.
Table 6: Zero-shot testing: Analyzing the robustness of trained networks on Tiny-ImageNet.
Table 7: Comparisons with state-of-the-art attention modules on KITTI-RGB in terms of mAP usingYOLOV3 on Resnet-18 and 34 backbones.
Table 8: Comparisons with state-of-the-art attention modules on the MS-COCO dataset using FasterR-CNN detector based on Resnet-50 backbones.
Table 9: Comparisons with state-of-the-art attention modules on Tiny-ImageNet in terms of thenumber of parameters (#P.) in millions, GFLOPs, FPS, and top-1 accuracy.
