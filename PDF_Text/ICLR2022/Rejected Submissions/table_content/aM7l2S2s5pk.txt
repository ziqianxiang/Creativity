Table 1: This table contains the results for training on an offline batch of data examining two factors:the size of the dataset (10 thousand or 50 thousand) and the quality of the policy (a good policy or arandom policy). The experiments are done for both DQN and TTN. The number in the table is theaverage of 30 runs with one standard error indicated in parentheses. Entries highlighted in greenshow settings where nontrivial performance was achieved.
Table 2: This table presents the results for training on a dataset of 50 thousand transitions but varyingthe amount of training measured by number of FQI iterations with TTN. The mean of 30 runs isincluded with one standard error in parentheses.
Table 3: This table presents the common hyper-parameters for the TTN and DQN algorithms.
Table 4: This table presents the regularization coefficients and number of offline training for offline-online setting for each environments separately.
