Table 1: Comparison of existing AL methods in terms of model architecture, optimizer, learningrate, whether data augmentation is used or not and whether cold or warm start is used to initializethe network parameters. ‘?’- means that the information was not mentioned.
Table 2: Results on the full datasetsDataset	Accuracy [%]CIFAR10	92.88 +- 0.23SVHN	95.22 +- 0.46FashionMNIST	94.20 +- 0.17CIFAR100	73.31 +- 0.28TinyImageNet	24.56 +- 0.98A.2 BackboneThe influence of the backbone used for the classification task is another key aspect that must betaken into account. Since for LL4AL the loss module is trained jointly with the backbone by sharingthe features of intermediate layers and no guideline was provided for which to use on differentarchitecture, we chose to use for VGG networks (Simonyan & Zisserman, 2015) the features ofthe last four max pooling layers and for DenseNet architectures (Huang et al., 2017) the last fourtransition layers. The influence of different backbones is depicted in figure 12.
Table 3: Description and setup of different AL methods25Under review as a conference paper at ICLR 2022Method	Dataset	Baddione	Architecture	Data augmentation	Optimizer	Length of training [φochs]	Learning rate	Initial labeled set	Useof Validati(Mi set	CoId-Startvs Warm start	Unsupervised pre-training	Useaf Unlabded dataCoreset	CIFAR10/CIFAR 100/ SVHN	VGG-16	Backbone only		RMSProp		0.001	Randorn		Cold-Start	no	noVAAL	CIFARl 0∕C⅛ AR l∞∕Caltedι- 256/ImageNet	VGG-16	Backbone (trained sqiaraldy) + VAE- Discriininator (minimax training)	RandMn-Horiz ontal- Flip	SGDfor Badtbone, Adam for other nets	Too	Ol	Randorn	yes	Cold-Start	no	yes, but not for backboneBADGE	CIFAR10/ SVHN/MNIST	ResNet-18∕VGG- Π∕MLP	Backbone only	no	Adam	until trainiιιg acc> 99%	0≡	Random	no	Cold-Start	no	noLL4AL	CIFAR10	ResNet-18	Badcbone+ loss module joindy trained	Rand om-Horiz ontal- Flip5 Random-Crop	sæ	200	Ol-	Random	no	Warm-Start	no	noJLS	CIEARl 0/CIF ARl OOZFashionMNIST	VGG-16∕VGG- 16∕ResNet-18	Backbone ÷ di Saimiiiaiorj OintIy trained	Rand om-Horiz ontal- Flip5 Random-Crop	sæ	200	Ol	Random	no	Cold-Start	no	yesWAAL	CIFARl 0∕SVHN∕ FasHonMNIST	VGG-16∕VGG- 16,∣(LeNet-5 as feature extractors + two IayerMLP for dassifier	Backbone ÷ di Scriminaiorj Ointly trained with minimax game	No (but available code uses Random- Horizontal-Flip, Random-Crop)	SGD	80 + early stopping	0≡	Random	no	Cold-Start	no	yesSRAAL -	CIFAR 10/CIFAR ResN⅛-18 ITOZCaltech-IOl		Backbone (trained S 叩 aτ3idy) + Unified Representati on Generator- Discriininator (minim 矗 training)					Random and K-center (on features of VAE)		Warm-Start	no	yes, but not for backboneTA-VAAL	CIFAR 10/CIFAR 100	ResNet-18	Backbone÷loss module joindy trained and VAE- Disaitninator (minimax trained)	Rand om-Horiz ontal- Flip5 Random-Crop	SGDfor Backbone, Adam for other nets	200	Ol-	Random	no	Cold-Start	no	yesr but not for backboneISAL	CIFARl O/CIF AR 100/SVHN	ResNet-18	Backbone only	Rand om-Horiz oiɪtal- Flip, Random-Crop	SGD	200	Ol-	Random		Cold-Start	no	noFKSSAL	MNIST/SVHNJm ageNet	LeNEtzResNet-IO /ResNet-18	Backbone only	no	SGD	50/35/60	0.05/0.1/0.1	Random	yes (weakly Iabddd)	Cold-Start	yes	yes5 pre- trainingCoreGCN Uncertain GCN	CIFARl O/CIF ARL OOZFashionMNIST /SVHN	ResNeM8	Backbone + Grφh Convolutional Netwo⅛ trained to distinguish between Iabded and UnabeId data	RaodMH-Horiz oɪɪtal- Flip, Random-Crop (only for CIFAR10 and CIFAR100)	sæ	200	δι-	Random	no	Cold-Start	no	yes. but not for backboneVaB-AL	CIFARl O/CIF ARL OO	ResNet-18	Backbone + VAE		SGD	200	Ol-	Random	no		no	yesr but not for backboneTable 4: Implementation details for different AL methods26Under review as a conference paper at ICLR 2022Ooo
Table 4: Implementation details for different AL methods26Under review as a conference paper at ICLR 2022Ooo8 7 6【％- >U2DUU<2000	4000	6000	8000	10000Number of labeled data(a)	random initialization2000	4000	6000	8000	10000Number of labeled data(b)	pretraining on TinyImageNetFigure 34: Comparison between randomly initialized network weights (left) and pre-trained weightson TinyImageNet (right).
