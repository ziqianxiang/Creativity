Table 1: Data. All graphs are undirected, have no edge features, and all but ddi have node features.
Table 2: GCN, SAGE, and GIN with different virtual node configurations over different graph-datatypes/amounts; (second) best results are (light) gray , overall best bold, second best underlined.
Table 3: Comparison to SOTA; top: state of the art (two best per dataset) according to OGBleaderboard (10/3/21), leaderboard results are marked by *; middle: models with similar goal to ourapproach (“-”: ran out of memory); bottom: best of virtual-node augmented GNNs we tested.
Table 4: Comparison of using the virtual nodes at every and only at the last layer; Hits@20, ddi.
Table 5: Data from additional experiments. All graphs are undirected, and have no edge but nodefeatures.
Table 6: Numbers of GNN layers and virtual nodes used by the trained models from Table 2.
Table 7: Results on ppa: OGB leaderboard compared to our best models. The GCN-VN result isonly intermediary (tuned for a smaller number of runs), but shown to confirm the positive trend.
Table 8: Additional results on small datasets.
Table 9:	Comparison of clustering algorithms to determine virtual node connections; Hits@20, ddi.
Table 10: Result averaged over 10 runs, 3 run averages for comparison. We see that the results arerelatively stable. Due to the randomness of the random models, we did not spend the resources tocompute the results for those.
Table 11: Percentage of intra-cluster test edges, using numbers of virtual nodes between 8 and 64. ForRM and CM+, we average over 10 different seeds but drop standard deviation (which is negligible)for readability. The numbers in brackets with -CM+ are the numbers of original METIS clusterswhich are then merged into “virtual node clusters”; we see no great sensitivity for them. We highlightour proposed graph-based clustering models we used in the experiments.
