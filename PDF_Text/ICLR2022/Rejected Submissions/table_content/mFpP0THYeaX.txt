Table 1: Results of different adaptation techniques on perturbations of CIFAR10 in terms of accuracyon the test set, with WideResnet18-10. The total number of training steps is 1000, with a batchsize of 512. The number of self-training iterations is 5 and 20 for iterative self-training, and GIFTrespectively. None refers to the zero-shot performance of the pretrained model. In all cases, GIFToutperforms all the baselines.
Table 2: Performance of GIFT and iterative self-training for WideResnet18-10 trained on perturbationsof CIFAR10 in terms of accuracy on the target. The total number of training steps is 500, with abatch size of 512. The number of teacher updates is 2 and 20 for iterative self-training and GIFT,respectively. Comparing these results to Table 1 with the total number of training steps of 1000indicates a noticeable drop in the performance of iterative self-training while GIFTâ€™s performance ismore robust.
Table 3: Accuracy on target domain on benchmarks with natural distribution shift. For the experimentsin this table we use a ResNet-101 pretrained on ImageNet-1k.
