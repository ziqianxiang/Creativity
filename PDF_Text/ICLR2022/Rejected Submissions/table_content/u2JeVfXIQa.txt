Table 1: Quantitative results on benchmark datasets for single image super-resolutionMethod	Scale	Params(M)	Set5		Set14		B100		Urban100		Manga109				PSNR	SSIM	PSNR	SSIM	PSNR	SSIM	PSNR	SSIM	PSNR	SSIMBicubic	×2	-	33.66	0.9299	30.24	0.8688	29.56	0.8431	26.88	0.8403	30.80	0.9339SRCNN	×2	0.244	36.66	0.9542	32.45	0.9067	31.36	0.8879	29.50	0.8946	35.60	0.9663VDSR	×2	0.672	37.53	0.9590	33.05	0.9130	31.90	0.8960	30.77	0.9140	37.22	0.9750MemNet	×2	0.677	37.78	0.9597	33.28	0.9142	32.08	0.8978	31.31	0.9195	37.72	0.9740SRMDNF	×2	5.69	37.79	0.9601	33.32	0.9159	32.05	0.8985	31.33	0.9204	38.07	0.9761RDN	×2	22.6	38.24	0.9614	34.01	0.9212	32.34	0.9017	32.89	0.9353	39.18	0.9780HAN	×2	17.3	38.27	0.9614	34.16	0.9217	32.41	0.9027	33.35	0.9385	39.46	0.9787EDSR	×2	407	38.11	0.9602	33.92	0.9195	32.32	0.9013	32.93	0.9351	39.10	0.9773EDSR+CLA	×2	42.1	38.24	0.9613	34.08	0.9214	32.41	0.9028	33.28	0.9367	39.23	0.9780EDSR+ACLA	×2	42.3	38.31	0.9617	34.10	0.9221	32.43	0.9030	33.35	0.9385	39.42	0.9787RCAN	×2	153	38.27	0.9614	34.12	0.9216	32.41	0.9027	33.34	0.9384	39.44	0.9786RCAN+CLA	×2	16.5	38.27	0.9615	34.14	0.9218	32.43	0.9030	33.34	0.9385	39.46	0.9785RCAN+ACLA	×2	16.7	38.30	0.9615	34.15	0.9217	32.45	0.9029	33.39	0.9387	39.48	0.9789Bicubic	×3	-	30.39	0.8682	27.55	0.7742	27.21	0.7385	24.46	0.7349	26.95	0.8556SRCNN	×3	0.244	32.75	0.9090	29.30	0.8215	28.41	0.7863	26.24	0.7989	30.48	0.9117VDSR	×3	0.672	33.67	0.9210	29.78	0.8320	28.83	0.7990	27.14	0.8290	32.01	0.9340MemNet	×3	0.677	34.09	0.9248	30.00	0.8350	28.96	0.8001	27.56	0.8376	32.51	0.9369
Table 2: Quantitative results on benchmark datasets for single image denoisingMethod	ParamS(M)	KCLDAk24				BSD68				Urban100					10	30	50	70	10	30	50	70	10	30	50	70MemNet	0.677	N/A	29.67	27.65	26.40	N/A	28.39	26.33	25.08	N/A	28.93	26.53	24.93DnCNN	0.672	36.98	31.39	29.16	27.64	36.31	30.40	28.01	26.56	36.21	30.28	28.16	26.17RNAN	7.409	37.24	31.86	29.58	28.16	36.43	30.63	28.27	26.83	36.59	31.50	29.08	27.45PANet	5.957	37.35	31.96	29.65	28.20	36.50	30.70	28.33	26.89	36.80	31.87	29.47	27.87baseline	5.430	37.21	31.85	29.60	28.15	36.34	30.60	28.28	26.84	36.63	31.64	29.22	27.54CLA	5.896	37.37	31.97	29.67	28.23	36.52	30.74	28.35	26.91	36.79	31.85	29.43	27.88ACLA	5.914	37.38	31.97	29.70	28.25	36.54	30.77	28.36	26.94	36.85	31.90	29.49	27.914.4	Image Compression Artifacts ReductionFor the task of image compression artifacts reduction (CAR), we compare our method with 3approaches: DnCNN (Zhang et al., 2017a), RNAN (Zhang et al., 2019), and PANet (Mei et al.,2020). All methods are evaluated on LIVE1 (Sheikh et al., 2005) and Classic5 (Foi et al., 2007). Toobtain the low-quality compressed images, we follow the standard JPEG compression process anduse Matlab JPEG encoder with quality q = 10, 20, 30, 40. For fair comparison, the results are onlyevaluated on Y channel in YCbCr Space. The quantitative results are shown in Table 3. 16-layerEDSR is used as our baseline CNN backbone. Both CLA and ACLA improves the performance ofthe neural network baseline.
Table 3: Quantitative results on benchmark datasets for image compression artifacts reductionMethod	Params (M)	LIVE1				Classic5					10	20	30	40	10	20	30	40JPEG	-	27.77	30.07	31.41	32.35	27.82	30.12	31.48	32.43DnCNN	0.672	29.19	31.59	32.98	33.96	29.40	31.63	32.91	33.77RNAN	7.409	29.63	32.03	33.45	34.47	29.96	32.11	33.38	34.27PANet	5.957	29.69	32.10	33.55	34.55	30.03	32.36	33.53	34.38baseline	5.430	29.63	32.04	33.50	35.51	29.99	32.22	33.43	34.31CLA	5.896	29.73	32.13	33.57	35.54	30.05	32.38	33.55	34.42ACLA	5.914	29.73	32.17	33.63	35.55	30.07	32.42	33.58	34.444.5	Ablation Study and DiscussionACLA vs. Non-local attention To verify the effectiveness of our proposed methods, we compareCLA and ACLA with Non-Local attention (Wang et al., 2018) and vanilla Cross-Layer Non-Localattention in terms of computational efficiency and performance. The vanilla Cross-Layer Non-Localattention follows the formulation in Equation (3). The comparison is performed on Set5 for singleimage super-resolution with EDSR backbone. The Non-Local attention modules and vanilla Cross-Layer Non-Local attention modules are inserted evenly after every 8th residual blocks. All the FLOPsin our ablation study are calculated for input of size 48 × 48. Results are presented in Table 4, whereNL stands for Non-Local attention and CLNL stands for vanilla Cross-Layer Non-Local. As shownin Table 4, with less computation cost, CLA and ACLA achieve much better performance compared
Table 4: Efficiency and performance comparison with Non-Local attention on Set5Method	PSNR	FLOPs(G)	Params(M)EDSR	38.11	-93.97-	-40.73-NL	38.15	109.38	43.56CLNL	38.14	122.67	45.87CLA (Ours)	38.24	96.93	42.13ACLA (OurS)	38.31	96.97	42.29Number of inserted CLA modules As stated before, the computation cost of the cross-layer designin CLA is quadratic to the number of inserted CLA modules. To verify that dense insertion of CLA8Under review as a conference paper at ICLR 2022modules is not necessary, we perform an ablation study on the number of inserted CLA modules onSet5 (×2) for single image super-resolution. We use EDSR as our backbone where L CLA modulesare evenly inserted. As shown in Table 5, with more CLA modules inserted, the performance can beslightly improved. However, the computation cost and parameter size of the model are also greatlyincreased. While with insert positions searched as shown in Table 6, our model can reach comparableperformance with much less computational resources.
Table 5: Ablation study on number of inserted CLA modules on Set5Method	L	PSNR	FLOPs(G)	Params(M)CLA	1-	38.24	-96.93-	-42!3-CLA	8	38.27	101.37	44.35CLA	16	38.27	118.48	51.47CLA	32	38.26	182.93	79.29Ablation study on ACLA As explained in Section 3.2, ACLA further improves CLA by two adaptivedesigns: selecting an adaptive number of keys at each layer for non-local attention and searching foroptimal insert positions of ACLA modules. To verify the effectiveness of the two adaptive designsin ACLA, we perform an ablation study on their influence on top of CLA. The comparison is alsoperformed on Set5 (×2) for single image super-resolution with EDSR backbone. The results areshown in Table 6. CLA-I stands for CLA is deployed with the search for insert positions as in ACLA.
Table 6: Ablation study on the effectiveness of insertion position search and sampled keys selectionMethod	Description of Methods	PSNR	FLOPs(G)	Params(M)CLA	-	38.24	-96.93-	-42.13-CLA-I	search for insert positions	38.27	96.93	42.13CLA-K	select aggregated keys	38.28	96.87	42.29ACLA	-	38.31	96.98	42.29Number of sampled keys K for CLA and ACLA As discussed before, the key point samplingstrategy in CLA plays a vital role to reduce the computation cost. To verify that a small numberof sampled keys K can be sufficient, we perform experiments on CLA with different value of K .
Table 7: Ablation study on number of sam-pled keys in CLA on Set5Method	K	PSNR	FLOPs(G)	Params(M)CLA-	1-	38.22	-96.71-	-42.09-CLA	8	38.24	96.93	42.13CLA	16	38.25	97.38	42.21CLA	32	38.23	97.90	42.39CLA	64	38.25	98.92	42.74Table 8: Ablation study on maximum numberof sampled keys in ACLA on Set5Method	K	PSNR	FLOPs(G)	Params(M)ACLA	~~Γ~	38.28	-96.78-	-42.18-ACLA	16	38.31	96.98	42.29ACLA	32	38.30	97.56	42.41ACLA	64	38.31	98.03	42.69ACLA	128	38.29	99.17	43.025	ConclusionsIn this paper, we first propose a novel attention module Cross-Layer Attention (CLA) to search forinformative keys across different layers for each query feature. We further propose Adaptive CLA, orACLA, which improves CLA by two adaptive designs: selecting adaptive number of keys at each
Table 8: Ablation study on maximum numberof sampled keys in ACLA on Set5Method	K	PSNR	FLOPs(G)	Params(M)ACLA	~~Γ~	38.28	-96.78-	-42.18-ACLA	16	38.31	96.98	42.29ACLA	32	38.30	97.56	42.41ACLA	64	38.31	98.03	42.69ACLA	128	38.29	99.17	43.025	ConclusionsIn this paper, we first propose a novel attention module Cross-Layer Attention (CLA) to search forinformative keys across different layers for each query feature. We further propose Adaptive CLA, orACLA, which improves CLA by two adaptive designs: selecting adaptive number of keys at eachlayer and searching for insert positions of ACLA modules. Experiments on image restoration tasksincluding single-image super resolution, image denoising, image compression artifacts reduction andimage demosaicing validate the effectiveness and efficiency of CLA and ACLA.
Table 9: Search settings for ACLA in different experimentsTask	Backbone	Value of λ	Insert PositionsSingle-Image Super-Resolution	-EDSR-	0.15	3,12, 26,31,32Single-Image Super-Resolution	RCAN	0.3	1,3, 5,9Image Denoising	EDSR	0.2	2,7,9,13,15Image Demosaicing	EDSR	0.2	2,5, 11, 14, 16Image Compression Artifacts Reduction	EDSR	0.2	2,7,10,13,14A.2 Image DemosaicingTo demonstrate the effectiveness of our proposed CLA and ACLA on various image restorationtasks, we add an experiment on image demosaicing. The evaluation is conducted on Kodak24,McMaster (Zhang et al., 2017b), BSD68, and Urban100, following the settings in RNAN (Zhanget al., 2019). We compare our approach with IRCNN (Zhang et al., 2017b), RNAN (Zhang et al.,2019), and PANet (Mei et al., 2020). We also use 16-layer EDSR as the baseline CNN model. Asshown in Table 10, our approach yields the best reconstruction result for image demosaicing.
Table 10: Quantitative results on benchmark datasets for image demosaicingMethod	Params(M)	MCMaSter18			Kodak24				BSD68			Urban100			PSNR	SSIM	PSNR	SSIM	PSNR	SSIM	PSNR	SSIMMosaiced	-	-917-	-0.1674-	-8.56-	0.0682	8.43	-0.0850-	-7.48	-0.1195-IRCNN	0.731	37.47	0.9615	40.41	0.9807	39.96	0.9850	36.64	0.9743RNAN	7.409	39.71	0.9725	43.09	0.9902	42.50	0.9929	39.75	0.9848PANet	5.957	40.00	0.9737	43.29	0.9905	42.86	0.9933	40.50	09854Baseline	5.430	-39.81-	-0.9730-	-43.18-	0.9903	-42.66-	-0.9931-	-40.23-	-0.9852-CLA	5.896	40.03	0.9739	43.35	0.9906	42.88	0.9934	40.52	0.9853ACLA	5.914	40.08	0.9742	43.38	0.9908	42.90	0.9936	40.55	0.9857A.3 Comparison with other attention methodsIn our paper, we have compared CLA and ACLA with non-local attention in section 4.5. Here,we further compare our proposed ACLA against other forms of attention modules that are widelyused in the CV community, including Squeeze-and-Excitation (SE) (Hu et al., 2018) attention andMulti-Head Attention (MHA) (Bello et al., 2019). SE aim at reweighting the channel-wise responsesby using soft self-attention to model interdependencies between the channels of the convolutionalfeatures. MHA is actually a variant of self-attention from the NLP domain. Specifically, MHA can beregarded as a special kind of non-local attention that takes account of the relative position information.
Table 11: Efficiency and performance comparison with Squeeze-and-Excitation (SE) attention andMulti-Head Attention (MHA)Methods	Params(M)	FLOPs(G)	Set5	Set 14	B 100	Urban 100	Manga 100EDSR	-4073	-93.97-	38.11	33.92	32.32	-32.93-	-39.10-EDSR + MHA	42.17	100.21	38.23	34.01	32.39	33.07	39.29EDSR + SE	41.79	96.14	38.19	34.03	32.36	33.06	39.22EDSR + ACLA	42.29	96.97	38.31	34.10	32.43	33.35	39.42EDSR + ACLA + SE	43.47	99.32	38.33	34.09	32.44	33.38	39.46A.4 Performance comparison with other attention-based methodsAs image restoration is regarded as an ill-posed problem, improving the performance of CNNbackbones for image restoration has always been a challenging task. Recently, attention methodshave been widely used to improve performance. Here, we compare our ACLA with HAN andSAN, which are also attention-based methods for single image super-resolution. Both HAN andSAN are based on the previous state-of-the-arts (SOTA) method RCAN. In the table below, wecompared the improvements of ACLA, HAN, and SAN. The improvements over RCAN are listed inthe parentheses after the PSNR results. For our ALCA, the percentage comparisons of improvementare also calculated. For example, the improvement of ACLA over RCAN on B100 is 0.04, which is400% of the improvement of SAN over RCAN. Besides, we have also calculated the improvementby ACLA on the EDSR backbone. As shown in the table, compared to HAN and SAN which arecompetitive baselines representing the recent progress of this literature, our method makes significant
Table 12: Efficiency and performance comparison with previous SOTA methods HAN and SAN forsingle-image super-resolutionMethods	Inference Time	Set5	Set 14	B100	Urban100	Manga109RCAN	327	38:27	34:12	32:41	33:34	39:44HAN	38.9	38.27 (0.00)	34.16(0.04)	32.41(0.00)	33.35 (0.01)	39.46 (0.02)SAN	61.2	38.31 (0.04)	34.07 (-0.05)	32.42 (0.01)	33.10(-0.24)	39.32 (-0.12)RCAN+ACLA	36.9	38.30(0.03/75%)	34.15(0.03/75%)	32.45 (0.04/400%)	33.39 (0.05/500%)	39.48 (0.04/200%)EDSR	16.2	38.11	33.92	32.32	32.93	39.10EDSR+ACLA	19.8	38.31 (0.20)	34.10(0.18)	32.43 (0.11)	33.35 (0.42)	39.42 (0.32)Besides, we also compare the inference time between our proposed ACLA, HAN, and SAN. Therunning time is the average of 1000 runs on input of size 48 × 48. The running time is evaluatedon a single 16G Tesla V100. As shown Table 12, our method EDSR+ACLA achieves even betterperformance than HAN with much less inference time.
