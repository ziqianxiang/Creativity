Table 1: The training time and accuracy results on GLUE benchmark. Results show that ScaLA isable to achieve the same average accuracy as the baseline while providing up to 18× speedups thansingle GPU, and up to 9.8× speedups when using the same amount of hardware resources.
Table 2: The comparison results of the GLUE benchmark. ScaLA outperforms large-batch optimiza-tion baselines by achieving higher accuracy after training the same number of samples.
Table 3: Ablation study of scaLA using BERTbase on GLUE tasks.
Table 4: Evaluation results on alternative methods to generate per- Table 5: Comparison resultsturbations using random noise, ground-truth, and label probability. with FreeLb.
Table 6: Evaluation results on hyperparameter tuning vs. using square-root learning rate scaling.
