Table 1: Summary of previous scores and our results on resource-limited Acrobot (n: planningrollouts, L: horizon). MAR is the episodewise mean reward measured on the second half of theepisodes where the algorithms have safely achieved their asymptotic performance. The two RSrows are random shooting planning on the real system, requiring orders of magnitudes more systemaccess steps than MBRL. ] and ↑ mean lower and higher the better, respectively.
Table 2: Agent evaluation results. MAR is the Mean Asymptotic Reward showing the asymp-totic performance of the agent and MRCP(1.8) is the Mean Reward Convergence Pace showing theSamPle-efficiency (the number of system access steps required to achieve a mean reward of 1.8).]and ↑ mean lower and higher the better, respectively. Except for DYNAZERO all the agents were runfor 10 seeds with the ± giving the 90% confidence interval.
Table 3: Importance of planning and exploration. MAR is the Mean Asymptotic Reward showingthe asymptotic performance of the agent and MRCP(1.8) is the Mean Reward Convergence Spaceshowing the sample-efficiency performance as the number of system access steps required to achievea reward of 1.8. ] and ↑ mean lower and higher the better, respectively. The ± values are 90%GaUssian Confidence intervals.______________________________________________________________Agent	MAR ↑	MRCP(1.8)(DQN(ε = 0.4)	1.209±0.013	NaN ±NaNDQN(ε = 0.05)	1.442±0.014	NaN ±NaNDQN(n = 100, L = 10)-EPSGREEDYEXPLORE(ε = 0.0001)	1.475±0.062	NaN ±NaNDQN(n = 100, L = 10)-EPSGREEDYEXPLORE(ε = 0.01)	1.664±0.032	NaN ±NaNDQN(n = 100, L = 10)-EPSGREEDYEXPLORE(ε = 0.05)	1.932±0.012	3540.0±520.0DQN(n = 100, L = 10)-EPSGREEDYEXPLORE(ε = 0.1)	2.009±0.031	2400.0±-RSACTOR(n = 100, L = 10)	2.075±0.01	2620.0±320.0DQN(n = 100, L = 10)-EPSGREEDYEXPLORE(ε = 0.9999)	2.107±0.042	2000.0±-DQN(n = 100, L = 10)-EPSGREEDYEXPLORE(ε = 0.99)	2.118±0.046	2400.0±-DQN(n = 100, L = 10)-EPSGREEDYEXPLORE(ε = 0.2)	2.151±0.034	2400.0±-DQN(n = 100, L = 10)-EPSGREEDYEXPLORE(ε = 0.8)	2.196±0.037	2000.0±-DQN(n = 100, L = 10)-EPSGREEDYEXPLORE(ε = 0.4)	2.204±0.01	1910.0±140.0C RSActor performance on the real systemWe present the results one can obtain on the real system with an RSActor and different values of
Table 4: Model evaluation results with for our best guide&explore strategy DQN(n = 100,L = 10)-HEATINGEXPLORE-BOOTSTRAP. MAR is the Mean Asymptotic Reward showing theasymptotic performance of the agent and MRCP(1.8) is the Mean Reward Convergence Space show-ing the sample-efficiency performance as the number of system access steps required to achieve areward of 1.8. ] and ↑ mean lower and higher the better, respectively. The performances are Com-puted from 10 random repetitions of IteratedMBRL.
