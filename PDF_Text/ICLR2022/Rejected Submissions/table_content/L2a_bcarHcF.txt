Table 1: Four encodings for matrix coefficients.
Table 2: Exact prediction of matrix transposition for different matrix dimensions. Transformers with 1layer, 256 dimensions and 8 attention heads.
Table 3: Accuracies of matrix sums, for different tolerances. B1999 encoding, 512 dimension and 8attention heads.
Table 4: Accuracies of matrix-vector products, for different tolerances. Fixed-size models have 1 or 2layers, variable-size have 2 or 4. All model have 512 dimensions and 8 attention heads.
Table 5: Accuracy of matrix multiplication, for different tolerances. Fixed-size matrices with 24-26 coef-ficients. All encodings are P1000 unless specified. Models have 512 dimensions and 8 attention heads4.4 EigenvaluesWe now turn to non-linear problems that are usually solved by iterative algorithms. We train modelswith 4 or 6 layers in the encoder or the decoder to predict the eigenvalues of symmetric matrices.
Table 6: Accuracy of eigenvalues for different tolerances and dimensions. All models have 512 dimensionsand 8 attention heads, except the 10x10 model, which has 12.
Table 7: Accuracies of eigenvectors, for different tolerances and depths. All models have 512 dimensionsand 8 attention heads4.6 INVERSIONInversion of 5×5 matrices proves more difficult than previous tasks, with accuracies of 73.6% forP10 models, and 80.4 for P1000 models (5% tolerance, 6-layer encoders and 1-layer decoders).
Table 8: 5x5 matrix inversion. All models have 6/1 layers, except P1000 10 heads, which has 6/6 (and 512dimensions).
Table 9: Accuracies of SVD for 4x4 matrices.All models have 512 dimensions and 8 attention heads5	OUT-OF-DOMAIN GENERALIZATION AND RETRAININGIn this section, We focus on the prediction of eigenvalues of symmetric matrices. To train our models,We generate random n×n matrices with independent and identically distributed (iid) coefficients,sampled from a uniform distribution over [-A,A]. They belong to a common class of randommatrices, known as Wigner matrices. Their eigenvalues have a centered distribution with standarddeviation σ = √ns, where S is the standard deviation of the coefficients (s = A/√3 when uniform).
Table 10: Out-of-distribution eigenvalue accuracy (tolerance 2%) for different training distributions.
Table 11: Model accuracy after retraining. Models trained over 5x5 matrices, retrained over 5-6 and 5-7.
Table 12: Accuracy of matrix addition for different model sizes. 10 × 10 matrices, 60 epochs (18millionsexamples). 5% tolerancedimension	64	B1999 128	256	512	64	P1000 128	256	5121/1 layers	-	-	76	15	-	-	-	962/2 layers	-	-	26	6	-	-	-	374/4 layers	-	-	70	63	-	-	-	536/6 layers	-	-	-	-	-	-	-	23Table 13: Learning speed of matrix addition for different model sizes. Number of epochs needed to reach95% accuracy (with 5% tolerance). 1 epoch = 300,000 examples.
Table 13: Learning speed of matrix addition for different model sizes. Number of epochs needed to reach95% accuracy (with 5% tolerance). 1 epoch = 300,000 examples.
Table 14: Learning speed of matrix and vector products and eigenvalue calculations for different modelsizes. Number of epochs needed to reach 95% accuracy (with 5% tolerance). 1 epoch = 300,000 examples.
Table 15: In-distribution eigenvalue accuracy (tolerance 2%) for different training distributions. Allmodels have 512 dimensions, and 8 attention heads, and are trained on 5x5 matrices.
Table 16: Matrix addition with LSTM and GRU. 5 X 5 matrices.
Table 17: Eigenvalue computation with LSTM and GRU. 5 × 5 matrices.
Table 18: Accuracy of Universal transformers on matrix addition for different tolerances. 10 × 10matrices.
Table 19: Accuracy of Universal transformers on eigenvalue computation for different tolerances. 5 × 5matrices.
Table 20: Accuracy of noisy matrix addition for different error levels and tolerances. 5 × 5 matrices.
Table 21: Accuracy of noisy eigenvalue computations, for different error levels and tolerances. 5 × 5matrices.
Table 22: Accuracy of co-training. 5 × 5 matrices, 5% tolerance.
Table 23: Number of parameters of transformers used in the paper.
