Table 1: Results on molecule property prediction benchmark from (Maziarka et al., 2020). We onlytune the learning rate for models in the first group. First two datasets are regression tasks (loweris better), other datasets are classification tasks (higher is better). For reference, we include resultsfor non-pretrained baselines (SVM, RF, GCN (Duvenaud et al., 2015), and DMPNN (Yang et al.,2019a)) from (Maziarka et al., 2020). We also include SVMrdkit and RFrdkit as two baseline methodswith added rdkit features. Rank-plot for these experiments is in Appendix D.1.
Table 2: Results on the benchmark from (Rong et al., 2020). Models are fine-tuned under a large hyperparameters budget. Additionally, models fine-tuned with only tuning the learning rate are						presented in the last group.		The last two	datasets are	classification tasks (higher is better), the		remaining datasets are regression tasks (lower is better). For reference, we include results for						non-pretrained baselines (GraphConv (Kipf & Welling, 2016), Weave (Kearnes et al., 2016) and						DMPNN (Yang et al., 2019a)) from (Rong et al., 2020). We also include RFrdkit as a baseline method						with added rdkit features. Rank-plot for these experiments is in Appendix D.2. We bold the best						scores over all models and underline the best scores for learning rate tuned models only.							ESOL	FreeSolv	Lipo	QM7	BACE	BBBPRFrdkit	.942(.196)	2.625(.509)	.739(.038)	124.3(3.5)	.884(.030)	.928(.025)GraphConv	1.068(.050)	2.900(.135)	.712(.049)	118.9(20.2)	.854(.011)	.877(.036)Weave	1.158(.055)	2.398(.250)	.813(.042)	94.7(2.7)	.791(.008)	.837(.065)DMPNN	.980(.258)	2.177(.914)	.653(.046)	105.8(13.2)	.852(.053)	.919(.030)GROVERrdkit	.888(.116)	1.592(.072)	.563(.030)	72.5(5.9)	.878(.016)	.936(.008)R-MAT rdkit	.786(.133)	2.044(.662)	.574(.028)	68.692(1.123)	.871(.028)	.936(.020)MAT	.853(.159)	1.744(.425)	.608(.017)	102.8(2.94)	.846(.025)	.920(.039)GROVER	.927(.110)	2âˆ™262(.407)	.604(.015)	82.623(3.833)	.867(.022)	.908(.053)GROVERrdkit	.924(.129)	2.096(.496)	.593(.029)	84.625(4.174)	.873(.031)	.931(.021)R-MAT	.801(.132)	1.912(0.364)	.585(.029)	77.248(2.819)	.858(.041)	.931(.016)R-MAT rdkit	.819(.145)	2.057(.434)	.580(.019)	70.929(3.568)	.858(.021)	.920(.021)Figure 3 compares R-MAT performance with various models. More detailed results could be find
Table 3: Ablations of Relative Molecule Self-Attention; other ablations are included in the Appendix.
Table 4: Featurization used to embed neighbourhood order in R-MAT.
Table 5: Featurization used to embed molecular bonds in R-MAT.
Table 6: Featurization used to embed atoms in R-MAT.
Table 7: Time needed for molecular conformations calculation for different datasets.
Table 8: Overlap between pre-trained dataset and different tasks datasets.
Table 9: Relative Molecule Attention Transformer large grid hyperparameters ranges	parameterswarmup learning rate	0.05, 0.1,0.2, 0.3 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001, 0.000005, 0.000001epochs pooling hidden dimension pooling attention heads prediction MLP layers prediction MLP dim prediction MLP dropout	100 64, 128, 256, 512, 1024 2, 4, 8 1,2,3 256, 512, 1024, 2048 0.0, 0.1, 0.2C.6 Large-scale experimentsModels We compared our R-MAT with 8 different models: NMP (Gilmer et al., 2017),Schnet (Schutt et al., 2017), Cormorant (Anderson et al., 2019), L1Net (Miller et al., 2020),LieConv (Finzi et al., 2020), TFN (Thomas et al., 2018), SE(3)-Tr. (Fuchs et al., 2020), EGNN (Sator-ras et al., 2021).
Table 10: Mean absolute error on QM9, a benchmark including various quantum prediction tasks.
Table 11: Test set performances of R-MAT for different choices of the relative self-attention.
Table 12: Test set performances of R-MAT for different choices of bond featurization.
Table 13: Test set performances of R-MAT for different choices of distance modeling.
Table 14: Test set performances of R-MAT for different choices of pretraining used.
