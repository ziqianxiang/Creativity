Table 1: RoBERTa-base AUC results. In all tables, bold indicates better-than-random-samplingperformance; gray background indicates significance at p < 0.05 in a two-tailed t-test. Standarderrors are in parentheses. A “-c” suffix indicates text classification datasets.
Table 2: RoBERTa-base AUC results with 50% of the dataset pruned. Compared to the unprunedresults, AL does better relative to random on average, but the improvement is not consistent acrossall datasets. A side-by-side comparison with Table 1 for BatchBALD can be found in Appendix C.
Table 3: RoBERTa-large AUC results. AL does substantially worse than random for most datasets.
Table 4: RoBERTa-large AUC results using the convex hull of each learning curve. AL does muchbetter here compared to Table 3, suggesting that training instability was a large factor in its previousfailures.
Table 5: Multi-trained RoBERTa-base AUC results with 5 trainings per point on the learning curve.
Table 6: AUC results for classification versions of multiple-choice datasets. Compared to Table 1,AL relatively improves on some datasets and worsens on others vs random, with no clear pattern.
Table 7: BERT-base AUC results.
Table 8: RoBERTa-base AUC results with ∣∆L∣ = 12. Overall results are slightly lower than inTable 1, likely due to the extra point on the learning curve Where dataset size is 12, but the differencesbetween AL and random are roughly unchanged.
Table 9: Transformer hyperparametersTransformer models were fine-tuned for a fixed 1,000 steps, evaluating on the dev set every 100 stepsand using the checkpoint with best dev accuracy to obtain the final test set accuracy. We found frompreliminary experiments that the hyperparameters in Table 9 generally worked well on all models anddatasets. The learning rate followed a sloped triangular schedule, warming up over the first severalsteps and then linearly decreasing until the end of training. The maximum sequence length for modelswas set per-dataset based on the length of input texts. In general it was set so that at least 99% ofexamples fit completely within the sequence length, and those that did not fit were pruned.
Table 10: Dataset sizes. “Train” is the set we used as the unlabeled pool for active learning. Notealso that the numbers add up to slightly less than the official sizes of these datasets, as we held outsome additional data that ultimately went unused in this work.
Table 11: Roberta-base with different levels of pruning.
Table 12: RoBERTa-base AUC results with Q = 1000 and ∣∆L∣ = 50. Original results from Table 1are reprinted for comparison. AL has higher relative performance on the results with greater labelbudget and batch size.
