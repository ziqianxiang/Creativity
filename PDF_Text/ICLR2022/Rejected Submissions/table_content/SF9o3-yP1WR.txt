Table 1: Accuracy of personalized Models on Biased Test Set (ACC_B) and Bias-Conflicting Test Set(Acc_B). Our proposed method achieves the lowest accuracy disparity (2.15%) compared to otherpersonalization methods (15.12%/15.38%), and 3.43% accuracy improvement on the biased test setand 0.85% improvement on the biased-conflicting test set compared to the global model.
Table 2: Table of NotationSymbol	Descriptionx, yxr, xsd, dr , ds,cfgfpδf∆DgDbDbD|x,ysupp(D)h∙, ∙iA pair of data sample and labelThe robust feature and spurious features in x = [xr , xs], respectivelyThe dimension of x, xr, xs, respectivelyThe global modelThe personlized local modelAn adversarial purtubation generated by the global model fg with attack budgetAn natural perturbation, which could flip the spurious attributeThe global distribution, which is the union of local distributionsA biased local distributionA bias-conflicting local distribution
Table 3: Number of Train and Validation Samples in CelebA-SClient ID	Non-blond Female	Non-Blond Male	Blond Female	Blond Male0	55	31	12	21	30	68	0	22	59	28	11	2Table 4: Number of Biased Test Samples in CeIebA_SClient ID	Non-blond Female	Non-Blond Male	Blond Female	Blond Male0	115	60	45	21	60	75	79	02	86	111	14	0Table 5: Number of Biased-Conflicting Test Samples in CeIebA_SClient ID	Non-blond Female	Non-Blond Male	Blond Female	Blond Male0	0	0	0	2001	0	0	0	2032	0	0	0	204C.2 Hyper-parametersWe use Adam optimizer (Kingma & Ba, 2015) throughout our experiments with learning rate 1e-4.
Table 4: Number of Biased Test Samples in CeIebA_SClient ID	Non-blond Female	Non-Blond Male	Blond Female	Blond Male0	115	60	45	21	60	75	79	02	86	111	14	0Table 5: Number of Biased-Conflicting Test Samples in CeIebA_SClient ID	Non-blond Female	Non-Blond Male	Blond Female	Blond Male0	0	0	0	2001	0	0	0	2032	0	0	0	204C.2 Hyper-parametersWe use Adam optimizer (Kingma & Ba, 2015) throughout our experiments with learning rate 1e-4.
Table 5: Number of Biased-Conflicting Test Samples in CeIebA_SClient ID	Non-blond Female	Non-Blond Male	Blond Female	Blond Male0	0	0	0	2001	0	0	0	2032	0	0	0	204C.2 Hyper-parametersWe use Adam optimizer (Kingma & Ba, 2015) throughout our experiments with learning rate 1e-4.
Table 6: Neural Network ArchitectureCNN 28x28	CNN 64x64Input: R3∙28∙28 4∙4 conv, 64 BN LReLU, stride 2 4∙4 conv, 128 BN LReLU, stride 2 FC 4096 ReLU FC 10	Input: R3∙64∙64 4∙4 conv, 64 BN LReLU, stride 2 4∙4 conv, 64 BN LReLU, stride 2 FC 4096 ReLU FC 10D More Experimental ResultsD. 1 First-order Approximation of Adversarial Transferability LossTo explore the impact of the λ term in Lemma 2 and Theorem 3, we measure the relative differencebetween 'trans(fg,fp, x, y) and e ∙ ∣∣Vχ'(fg(x), y)k ∙ (1 - Y ∙ cosθ). In other words, We measuredthe accuracy of a first-order approximation of `trans (fg, fp, x, y). If the approximation is accuracyis high, it implies that the impact of λ ∙ e2 is low. Specifically, we compute an approximation error:hl 'trans(fg, fp, x, y) - e ∙ ∣Vχ'(fg(x),y)∣ ∙ (1 - 1 ∙cosθ)E(Xyym ∣l	FffxyOn MNIST, CelebA and Coil20 datasets, we find that the approximation error is 0.019, 0.058, 0.084,respectively. These results suggests that using e ∙ ∣∣Vχ'(fg(x),y)∣ ∙ (1 - Y ∙ cosθ) to approximate'trans(fg, fp, x, y results in a decent accuracy and the impact of λ ∙ e2 is low. The possible reasonis that the attack budget e is usually small (e.g., 0.031), such that the gradient of a function changeslittle in a small neighborhood defined by e.
