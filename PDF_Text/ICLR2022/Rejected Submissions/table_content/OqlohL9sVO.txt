Table 1: Comparison to state-of-the-art retrieval methods We present MAP(mean average pre-cision) of different approaches on ROxf5k/RPar6k and ROxf5k+1M/RPar6k+1M datasets in bothMedium and Hard protocols.
Table 2: Experiments for extraction speed and memory of different approaches. We did thisexperiment on the same NVIDIA Tesla P40 GPU, with images resized into 1024. UGALR* refersto a version of U GALR with the same extraction settings with Group 3.
Table 3: Experiments for Intermediate Supervision.
Table 4: Experiments for Local Attention Learning Module.
Table 5: Experiments for exploring different kinds of attentionsAttenion setup	roxford5k MH		rparis6k MH	no	78.53	54.31	86.99	72.29spatial	78.6	55	89.96	78.48channel	78.33	55.93	88.88	76.72spatial+channel	79.98	57.58	90.85	79.91Local attention setup Tab.5. compares the performance of no attention and three different kinds ofattention in LALM. For example, the first row represents not using any attention, and the second rowmeans using only spatial attention. Experiments show that combining spatial and channel attentionmechanism outperforms other three approaches by a large margin for all four protocols. The spatialattention module helps identify the key points and the channel attention module helps identify thecritical characteristics. The joint learning of them is essential to the LALM block.
Table 6: Experiments for average pooling in channel dimension		Avg pooling	roxford5k M	H	rparis6k MHno	76.31 ^^5089^^	87.79 74.16yes	79.98 57.58	90.85 79.91The most suitable block for LALM applying To explore which part of the network we shouldadd the LALM block to, we also conducted a comparative experiment as shown in Tab.7. Thisexperiment is based on the ResNet-50 backbone. From the table, the setup of adding LALM to theblock3 has a MAP of 79.98% in ROxf-M, 57.58% in ROxf-H, 90.85% in RPar-M and 79.91%ROxf-M, which surpasses the other three setups. Since the shallower feature maps of the networkare less semantical for attention learning, plugging LALM into block2 has an inferior performance.
Table 7: Experiments for most suitable blocks for LALM applying				block	roxford5k		rparis6k		M	H	M	H2	77.44	52.17^^	88.55	75.813	79.98	57.58	90.85	79.914	78.1	55.43	90.32	793+4	78.8	54.52	89.16	76.69Average pooling in channel dimension The significance of average pooling in the channel dimen-sion is illustrated in Section 3.2, and we conduct an experiment to verify its necessity. As shownin Tab.6, the model with Avg-pooling in the channel dimension outperforms the one without it by3.67% in ROxf-M, 6.69% in ROxf-H, 3.06% in RPar-M and 5.75% ROxf-M. By reducing di-mension in an appropriate way, it effectively avoids intermediate supervisionâ€™s over interference toglobal feature learning and well preserves key information at the same time.
Table 8: Experiments for feature fusionfusion approach	roxford5k MH		rparis6k MH	add	74.41	48.87	87.82	74.21multiply	77.87	54.07	89.53	77.4concatenate	79.98	57.58	90.85	79.919Under review as a conference paper at ICLR 2022ReferencesRelja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pajdla, and Josef Sivic. Netvlad: CNNarchitecture for weakly supervised place recognition. In 2016 IEEE Conference on ComputerVision and Pattern Recognition, CVPR 2016, Las Vegas, NV USA, June 27-30, 2016, pp. 5297-5307. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.572. URL https://doi.
