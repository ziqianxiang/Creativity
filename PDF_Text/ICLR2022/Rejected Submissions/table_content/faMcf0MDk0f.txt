Table 1: Our ablation study on ImageNet (Deng et al. (2009)) regarding accuracy, number of 32-bitoperations (FLOPs),1-bit operations (BOPs), and model size. (OPs = FLOPs + 1∕64∙BOPs)Network Configuration	Top 1 Acc.	FLOPs (∙108)	BaseNet BOPs (∙109)	OPs (∙108)	Model Size	BoolNet										Top 1 Acc.	FLOPs (∙108)	BOPs (∙109)	OPs (∙108)	Model SizeBaseline (60 epochs, CE Loss)	47.69%	1.22	1.68	1.49	3.47 MB	54.07%	2.78	1.85	3.07	5.05 MB+ Multi-Slice strategy (k=4)	52.27%	1.22	1.69	1.49	3.47 MB	56.84%	2.78	1.86	3.07	5.05 MB+ (1) Modified downsample branch	(BaseNet has		no downsample branch)			58.66%	1.23	2.48	1.62	3.84 MB+ (2) Local Adaptive Shiftingt	52.08%	1.25	1.69	1.51	3.47 MB	59.56%	1.26	2.48	1.65	3.84 MB+ (3) MaxPool instead of stride	55.14%	1.23	2.21	1.57	3.47 MB	59.98%	1.26	3.53	1.81	3.84 MB+ (4) Knowledge distillation*	56.84%	1.23	2.21	1.57	3.47 MB	61.98%	1.26	3.53	1.81	3.84 MB+ Long training (256 epochs)	58.20%	1.23	2.21	1.57	3.47 MB	63.00%	1.26	3.53	1.81	3.84 MBt Local Adaptive Shifting is not used for the subsequent BaseNet experiments * Replaces the cross-entropy loss with the distributional loss by Liu et al.(2020b)k	Top 1 Acc.	∆	BaseNetbaseline + (3) + (4)					BoolNet baseline + (1) + (2)+ (3) + (4)									Top 5 Acc.	FLOPs (∙108)	BOPs (∙109)	OPs (∙108)	Model Size	Top 1 Acc.	∆	Top 5 Acc.	FLOPs (∙108)	BOPs (∙109)	OPs (∙108)	Model Size1	51.74%	-	75.39%	1.23	2.20	1.57	3.47 MB	57.62%	-	80.47%	1.26	3.05	1.74	3.71 MB2	55.75%	+4.01	79.08%	1.23	2.20	1.57	3.47 MB	60.57%	+3.95	82.56%	1.26	3.21	1.76	3.76 MB4	56.84%	+1.09	79.85%	1.23	2.21	1.57	3.47 MB	61.98%	+1.41	83.75%	1.26	3.53	1.81	3.84 MB8	57.19%	+0.35	80.33%	1.23	2.22	1.58	3.47 MB	62.54%	+0.56	84.14%	1.26	4.16	1.91	4.01 MB4.1	Training DetailsOur training strategy and hyperparameters are mostly based on Bethge et al. (2020), but we train for
Table 2: Our ablation study on CIFAR100 regarding different downsampling methods. The numberof bits refers to both the input activation and weight binarization of the 1 × 1 convolution in theshortcut branch.
Table 3: Theoretical minimum memory requirement of all convolution blocks (can differ dependingon the implementation). k is the number of slices. The stages have different input size and thus leadto different memory requirements.
