Table 1: Worst-group and average accuracies, averaged over three seeds (standard deviations in parenthesis). Onimage data sets, CnC obtains significantly higher worst-group accuracy than comparable methods without grouplabels, competing with GDRO. CnC also competes with SoTA on CivilComments. Results without standarddeviations are reported from the original papers. Further implementation details are in Appendix E.
Table A.1: Across benchmarks, CnC achieves higher worst-group and average accuracies with thedefault contrastive loss, compared to using the alignment loss explicitly as a training objective.
Table A.2: CnC achieves higher worst-group and average accuracies on spuriously correlatedbenchmarks than DANN, a prior representation alignment method designed for domain adaptationMethod	Waterbirds	CelebAAccuracy (%)	Worst-group Average Worst-group AverageDANN (domains by spurious attribute)	37.4(3.8)	87.6(2.2)	28.1	(3.1)	94.6(0.3)DANN (domains by majority vs minority group)	67.3 (0.8)	83.6 (0.2)	47.2 (3.1)	88.7 (1.8)CNC	89.7 (0.2)	90.8 (0.1)	88.8 (0.9)	89.9 (0.5)In Table A.2, we report the worst-group and average accuracies of DANN on the Waterbirds andCelebA datasets across three seeds along with the CnC results. Our results suggest that the domainalignment in DANN is not sufficient to improve worst-group accuracy. We hypothesize this is due toadversarial training with the domain classifier aligning representations without regard to differentclasses within each domain. Due to the propensity of samples exhibiting spurious correlations, DANNmodels may thus still learn to rely on these correlations.
Table A.3: Ablation on positive and negative sampling strategies in CnC. CnC achieves highestworst-group accuracy when using the Stage 1 ERM model’s predictions to sample “hard” positivesand negatives (the default procedure).
Table A.4: Ablation over CnC algorithmic components on Waterbirds. Default choices achievehighest Worst-group and average accuracy.
Table A.5: Ablation over CNC λ parameter to balance cross-entropy and contrastive loss componentson CMNIST*. CnC obtains high performance across a range of λ.
Table A.6: Percent of toxic comments for each identity in the CivilComments-WILDS training set.
Table A.7: Method-specific hyperparameters for CnC.
Table A.8: Method-specific hyperparameters for Jtt.
Table A.9: Method-specific hyperparameters for EIIL.
