Table 1: Accuracy of domain adaptation by optimal transport in the latent space of ResNet50 modelwith the 10 known labels for each class in the target domain on Digits datasets.
Table 2: Accuracy of domain adaptation by Neural OT in the latent space of ResNet50 model withthe 10 and 100 known labels for each class in the target domain on Digits datasets.
Table 3: Results of domain adaptation in the latent space of ResNet50 model on Modern Office-31dataset in semi-supervised settings with the 10 known labels for each class in the target domain.
Table 4: Results of different OT algorithms on digits datasets, on the test set to the adversarialexamples transportation task, 10k samples from train set was used to build adversarial examples.
Table 5: Results of application of Sinkhorn optimal transport in the latent space of classifiers with10 known labels for each class.
Table 6: Accuracy of domain adaptation by optimal transport in the latent space of ResNet50 modelwith only 3 known labels for each class in the target domain on Digits datasets. The top part of thetable represents standard settings the bottom part presents results using source fiction.
Table 7: Accuracy of domain adaptation by optimal transport in the latent space of ResNet50 modelwith 100 known labels for each class in the target domain on Digits datasets. The top part of thetable represents standard settings the bottom part presents results using source fiction.
Table 8: Accuracy of domain adaptation by optimal transport in the latent space of ResNet18 modelwith the 10 known labels for each class in the target domain on Digits datasets. The top part of thetable represents standard settings the bottom part presents results using source fiction.
Table 9: Accuracy of domain adaptation by optimal transport in the latent space of ResNet18 modelwith the 100 known labels for each class in the target domain on Digits datasets. The top part of thetable represents standard settings the bottom part presents results using source fiction.
Table 10: Results of domain adaptation in the latent space of ResNet18 model on Modern Office-31dataset in semi-supervised settings with the 10 known labels for each class in the target domainA.4 Energy-based models for epsilon free source fictionAs shown in section 5, our algorithm depends on the size of perturbations, to avoid it and makeour method free, we propose to use an input convex energy-based classifier. Energy-based learn-ing provides a unified framework for many probabilistic and non-probabilistic approaches, partic-ularly for non-probabilistic training of graphical models, including discriminative and generative17Under review as a conference paper at ICLR 2022Method	A W	W a	a C	A D	D A	D C	D W	D CSource	49	27T~	^6^	"70-	^42^	^^6^	^62^	"36-EMD	42	44	30	59	40	24	53	38SINKHORN	43	43	30	60	3T	24	52	38SINKHORNLPLI	42	43	30	60	3T	25	53	38SINKHORnL1L2	42	43	30	60	3T	25	53	38MapT	42	43	30	60	3T	25	53	38EMD	54	TF	"32^	^68^	-67^	^39^	-69^	47T~SINKHORN	54	51	52	68	67	39	69	48SINKHORNLPLI	54	51	52	68	67	39	69	48SINKHORnL1L2	54	51	52	68	67	39	69	48
Table 11: Results of domain adaptation in the latent space of ResNet18 model on Office-31-Caltechdataset in semi-supervised settings with the 10 known labels for each class in the target domainMethod	Source	EMD	SINKH	Sinkh L1Lp	Sinkh L1L2	MAPTCIFAR→STL	3T.0	48.1	"ɪi	48.0	48.0	48.1CIFAR→SF		51.1	51.0	51.0	51.0	51.0STL→CIFAR	T5.0	T4.1	T4.1	T4.1	T4.1	T4.1STL→SF		76.2	76.2	76.2	76.2	76.2Table 12: Results on MNIST and USPS dataset in semi-supervised settings. U is USPS, M isMNIST, SF is source fiction. The top table presented results for the settings with the 10 knownlabels for each class in the target domain, and the bottom table presents the result with the 100known labels for each classapproaches and conditional random fields, graph-transformer networks, maximum margin Markovnetworks, and several manifold learning methods (LeCun et al., 2006). In energy-based settings forsome given fixed x and possibly some fixed elements of y we can perform inference by:Energy-based learning approaches can be considered as an alternative to probabilistic estimation forprediction, classification, or decision-making tasks. The energy-based representation must captureboth the discriminative interactions between x and y and allow for efficient combinatorial optimiza-tion over y .
Table 12: Results on MNIST and USPS dataset in semi-supervised settings. U is USPS, M isMNIST, SF is source fiction. The top table presented results for the settings with the 10 knownlabels for each class in the target domain, and the bottom table presents the result with the 100known labels for each classapproaches and conditional random fields, graph-transformer networks, maximum margin Markovnetworks, and several manifold learning methods (LeCun et al., 2006). In energy-based settings forsome given fixed x and possibly some fixed elements of y we can perform inference by:Energy-based learning approaches can be considered as an alternative to probabilistic estimation forprediction, classification, or decision-making tasks. The energy-based representation must captureboth the discriminative interactions between x and y and allow for efficient combinatorial optimiza-tion over y .
Table 13: Results on MNIST and USPS dataset in semi-supervised settings. U is USPS, M isMNIST, SF is source fiction. The top table presented results for the settings with the 10 knownlabels for each class in the target domain, and the bottom table presents the result with the 100known labels for each classBased on the POT library (Flamary et al., 2021) we tested the different variations of discrete optimaltransport in this task. First of all, we tested and basic EMD and Sinkhorn (Sinkh) (Cuturi, 2013)algorithms. Then we tested regularized versions of the Sinkhorn algorithm with a group lasso regu-larization (L1L2) and Laplacian regularization (L1LP) (Courty et al., 2015). Finally, our method isbenchmarked on MapT (Perrot et al., 2016).
