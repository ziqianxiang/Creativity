Table 1: Convergence rates established for MAZOPAZOO	Lipschitz Class [Centralized Counterpart]	Strongly Convex Class [Centralized Counterpart]One-point	Flip:O(t⅜4)[O(t⅜4) Flaxmanetal.(2005)]	FliP ∩ Fsc:O(T23) [ 0(^2TT^) AgarWal et al.(2010)]		FliP ∩ Fsmo:OlTd3 j	FliP ∩ Fsmo ∩Fsc:O(√T )[O (djnTL) AgarWal et al.(2010)]Two-point	Flip: OIqT) B/) ShamH (2017)]	Flip ∩ Fsc：O(djnTT))[O(d2TT)) Agarwal etal. (2010)]	Zip ∩ Fsmo: O (J T )	FliP ∩ Fsmo ∩ Fsc： O(djnTT)) [O(d2lT(T1) AgarWaletal.(2010)]Related Work. Recently, many types of centralized zeroth-order optimization algorithms have beenstudied, and their convergence rates (and the way they depend on the dimension) have been estab-lished in different settings. For unconstrained convex optimization, Nesterov & Spokoiny (2017)develops several types of two-point gradient estimators and achieves convergence rates that scalewith dimension as O(d2). For constrained stochastic optimization, Duchi et al. (2015) establishesthat the convergence rates are sharp up to factors at most logarithmic in the dimension. Zeroth-orderoptimization has a natural connection to bandit online optimization, where the latter focuses on dy-namic environment where the objective functions are varying over time (see, e.g., Flaxman et al.
