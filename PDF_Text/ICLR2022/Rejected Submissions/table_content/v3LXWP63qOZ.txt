Table 1: Comparisons on DMC Suite + Distractors. Performance of different methods at the 100K and 500Kmark. We report mean and std deviation for 5 seeds. ModInv is compared with Dreamer (Hafner et al.,2020), CURL (Laskin et al., 2020a), DBC (Zhang et al., 2020), and RAD (Laskin et al., 2020b). MODINVwithout augmentations uses a stopgrad for the critic gradients while ModInv+Aug allows gradients, similarto RAD.
Table 2: Comparisons on STL-10 and CIFAR-10. All are based on ResNet-18 pre-training. Evaluation ison a single crop.. ModInv is added to the base architecture of SimSiam (Chen & He, 2021) with 2-layerprojector (2048-d) and 2 layer predictor with a 512-d hidden layer. We use weight decay of 5e - 4, learning rate0.03 with a cosine decay.
Table 3: Ablation on CIFAR-10. Performance of 100 epoch pre-training for different number of heads.
Table 4: Ablation for Mean Corr. Loss. Performance at 500K for MODINV and the mean correlation lossversion from Eq. 1 for different Î» values.
Table 5: Ablation for different learning rates for ModInv. SIMSIAM + MODINV with linear predictor withdifferent sets of learning rates for the 3 projector heads.
Table 7: Hyperparameters for MODINV for RL.
