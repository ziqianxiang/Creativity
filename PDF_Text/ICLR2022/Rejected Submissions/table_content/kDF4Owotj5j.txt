Table 1: Model architecures for all main experiments. Note, we perform ablations where we changethe width or the maximum number of iterations and those parameters are indicated where appropriate.
Table 2: Training hyperparameters. Dashes indicate that we did not utiltize those options.
Table 3: The peak accuracy and corresponding test-time iteration number for prefix sum solvingmodel performance curves in Figures 3 and 4Tested on 48-bit Strings				Tested on 512-bit Strings			Model	α	Peak Iter.	Peak Acc. (%)	Model	α	Peak Iter.	Peak Acc. (%)DT	0.0	42	94.61 ± 1.19	DT	0.0	-	0.00 ± 0.00DT	1.0	27	97.73 ± 1.80	DT	1.0	171	11.26 ± 6.90DT-Recall	0.0	46	99.97 ± 0.01	DT-Recall	0.0	466	96.19 ± 3.73DT-Recall	1.0	26	99.96 ± 0.02	DT-Recall	1.0	237	97.12 ± 1.88FF	0.0	30	27.15 ± 2.56	FF	0.0	30	0.00 ± 0.00FF-Recall	1.0	30	99.87 ± 0.04	FF-Recall	1.0	30	0.00 ± 0.00Table 4: The peak accuracy and corresponding test-time iteration number for maze solving modelperformance curves in Figures 5 and 6.
Table 4: The peak accuracy and corresponding test-time iteration number for maze solving modelperformance curves in Figures 5 and 6.
Table 5: Peak accuracy and iteration number for chess puzzle performance curves in Figure 7.
Table 6: Training accuracy and in-distribution validation accuracy for models presented in Figures 3and4	__________________________________________________Trained on 32-bit Strings			Model	α	Train Acc. (%)	Val Acc. (%)DT	0.0	99.98 ± 0.01	100.00 ± 0.00DT	1.0	99.57 ± 0.38	97.73 ± 1.80DT-Recall	0.0	99.95 ± 0.02	100.00 ± 0.00DT-Recall	1.0	99.98 ± 0.01	100.00 ± 0.00FF	0.0	99.76 ± 0.14	99.76 ± 0.14FF-Recall	1.0	100.00 ± 0.00	100.00 ± 0.00Table 7: Training accuracy and in-distribution validation accuracy for models presented in Figures 5and 6	________________________________________________________Trained on 9 × 9 Mazes			Model	α	Train Acc. (%)	Val Acc. (%)DT	0.0	100.00 ± 0.00	100.00 ± 0.00DT	0.01	99.98 ± 0.01	99.98 ± 0.01DT-Recall	0.0	99.91 ± 0.07	99.92 ± 0.06DT-Recall	0.01	99.77 ± 0.15	99.74 ± 0.17FF	0.0	99.94 ± 0.01	99.93 ± 0.05FF-Recall	0.01	99.99 ± 0.00	99.99 ± 0.00
Table 7: Training accuracy and in-distribution validation accuracy for models presented in Figures 5and 6	________________________________________________________Trained on 9 × 9 Mazes			Model	α	Train Acc. (%)	Val Acc. (%)DT	0.0	100.00 ± 0.00	100.00 ± 0.00DT	0.01	99.98 ± 0.01	99.98 ± 0.01DT-Recall	0.0	99.91 ± 0.07	99.92 ± 0.06DT-Recall	0.01	99.77 ± 0.15	99.74 ± 0.17FF	0.0	99.94 ± 0.01	99.93 ± 0.05FF-Recall	0.01	99.99 ± 0.00	99.99 ± 0.00Table 8: Training accuracy and in-distribution validation accuracy for models presented in Figures 7Trained on puzzles 0-600K			Model	α	Train Acc. (%)	Val Acc. (%)DT	0.0	99.98 ± 0.02	92.94 ± 0.34DT	0.5	99.90 ± 0.01	94.16 ± 0.02DT-Recall	0.0	99.77 ± 0.02	100.00 ± 0.00DT-Recall	0.5	99.34 ± 0.02	94.18 ± 0.07FF	0.0	96.89 ± 0.58	83.24 ± 1.2216Under review as a conference paper at ICLR 2022
Table 8: Training accuracy and in-distribution validation accuracy for models presented in Figures 7Trained on puzzles 0-600K			Model	α	Train Acc. (%)	Val Acc. (%)DT	0.0	99.98 ± 0.02	92.94 ± 0.34DT	0.5	99.90 ± 0.01	94.16 ± 0.02DT-Recall	0.0	99.77 ± 0.02	100.00 ± 0.00DT-Recall	0.5	99.34 ± 0.02	94.18 ± 0.07FF	0.0	96.89 ± 0.58	83.24 ± 1.2216Under review as a conference paper at ICLR 2022A.6 Exit rulesWhen a DT network processes a single input, it can produce output at each iteration. When we choosea number of test-time iterations M , we are specifying the maximum number of iterations, but wemay choose from any of those to extract a single output. A naive approach is to take that outputat iteration M ; we call this We call this the default exit rule. A slightly more complicated process,and What Schwarzschild et al. (2021b) do, is to take the output from iteration m* which solves thefollowing maximization problem. Let y(m) be the output from iteration m, and let y(m)[k, j] be the(k, j ) entry in the output that corresponds to the confidence that the kth pixel is in class j , finally letk ∈ {1, 2, ..., K} and letj ∈ {0, 1}.
Table 9: Average iterations to solution, with the peak accuracy in parentheses. We evaluate mazesolving models on 13 × 13 mazes, where we intervene in the solving process in different ways.
