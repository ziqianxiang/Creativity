Table 1: Perplexities (PPLs), IG-scores, & Sampling Speed for protein language models. Lower PPL isbetter and higher IG-score is better. IG-scores are calculated from 1000 sequence samples except the 15KDARK0-Seed Samples. IG-scores for refined (Ref) samples are included for DARK models. Sampling Speedis for a single example (See Appendix A.6 for further details).
Table 2: DARK3 produces samples with confidently predicted structures by AlphaFold. Good+ pLDDTscores from AlphaFold (higher is better) are shown for unconditional language models. See Table 8 for addi-tional benchmarks.
Table 3: Samples from DARK models have diverse sequences and predicted structures. Sequence andstructure diversity is shown with the number of clusters (of 1000 samples) estimated by sequence clusteringand structure clustering respectively. Change in the Good+ pLDDT between iterations is also included.
Table 4: Diversity of the sequence examples in the training set. Diversity at each iteration is measured bynumber of sequence clusters found by clustering with four different sequence identity (Seq. ID) thresholds.
Table 5: Showing baseline results of refining random sequences against the AlphaFold IG-score using a greedyhill-climb. Sequence identity is between a sequence before refining and the final sequence produced by refining.
Table 6: Showing baseline results of using a trDesign-like approach with AlphaFold. It refines random se-quences against the AlphaFold IG-score using simulated annealing. Sequence identity is between a sequencebefore refining and the final sequence produced by refining. This was ran for 20,000 steps.
Table 7: Showing results of performing AlphaFold refinement with DARK3 . It refines DARK3 sequencesagainst the Cross-Entropy between AlphaFold’s initial step 0 distogram and the current sequence’s distogram,indirectly improving the AlphaFold IG-score, using a greedy hill-climb. Sequence identity is between a se-quence before refining and the final sequence produced by refining. This was ran for 10,000 steps as it wassufficient for convergence and so stopped as to not waste resources.
Table 8: Good+ pLDDT scores from AlphaFold (higher is better) are shown for unconditional language mod-els, uniformly random sequences, and two recent conditional models that use non-specific (weak) structureinformation for conditioning.
