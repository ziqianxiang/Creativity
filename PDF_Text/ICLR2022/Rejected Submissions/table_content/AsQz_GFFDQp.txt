Table 1: Performance Comparison of Single-domain Scenario We train 20 clients on each dataset for 100 rounds respectively. We measure local accuracy at the final round and report averaged scores over three trials with different seeds. Note that the classifier layers are not aggregated in the permuted settings for fairness.						Single-domain Federated Learning	MNIST		CIFAR-10		CIFAR-100	Method	Conventnl. IID	Permuted IID	Conventnl. IID	Permuted IID	Conventnl. IID	Permuted IIDStand-Alone	96.91 (± 0.13)	97.26 (± 0.06)	59.62 (± 0.41)	62.01 (± 0.27)	25.12 (± 0.24)	22.92 (± 0.12)FedAvg (McMahan et al., 2017)	97.87 (± 0.12)	95.26 (± 0.23)	65.52 (± 0.55)	45.99 (± 0.44)	45.36 (± 0.17)	11.48 (± 0.11)FedProx (Li et al., 2018)	97.91 (± 0.04)	95.41 (± 0.19)	65.06 (± 0.37)	45.07 (± 0.38)	44.94 (± 0.22)	11.58 (± 0.17)Agnostic-FL (Mohri et al., 2019)	97.87 (± 0.10)	95.13 (± 0.13)	64.69 (± 0.32)	46.37 (± 0.71)	44.63 (± 0.21)	11.73 (± 0.12)Clustered-FL (Sattler et al., 2019)	97.57 (± 0.08)	96.41 (± 0.39)	65.39 (± 0.53)	55.31 (± 0.31)	42.83 (± 0.39)	17.94 (± 0.19)FedPer (Arivazhagan et al., 2019)	98.11 (± 0.11)	94.81 (± 0.27)	68.70 (± 0.29)	45.12 (± 0.31)	49.12 (± 0.33)	11.26 (± 0.16)Per-FedAvg (Fallah et al., 2020)	97.91 (± 0.14)	94.02 (± 0.11)	66.12 (± 0.37)	47.23 (± 0.38)	50.58 (± 0.13)	12.08 (± 0.22)FedFOMO (Zhang et al., 2021)	97.53 (± 0.05)	94.97 (± 0.38)	67.08 (± 0.28)	51.12 (± 0.23)	48.13 (± 0.49)	11.08 (± 0.31)SimFed (Ours)	98.28 (± 0.12)	97.83 (± 0.36)	71.01 (± 0.19)	64.21 (± 0.29)	52.01 (± 0.18)	24.11 (± 0.23)Method	Non-IID (C)	Non-IID (P)	Non-IID (C)	Non-IID (P)	Non-IID (C)	Non-IID (P)Stand-Alone	81.52 (± 0.43)	81.73 (± 0.45)	39.77 (± 0.11)	44.46 (± 0.22)	18.53 (± xx.xx)	17.36 (± 0.35)FedAvg (McMahan et al., 2017)	91.89 (± 0.47)	72.34 (± 0.19)	44.37 (± 0.31)	31.43 (± 0.15)	26.32 (± 0.34)	8.87 (± 0.36)FedProx (Li et al., 2018)	89.84 (± 0.32)	66.32 (± 0.22)	42.63 (± 0.25)	31.91 (± 0.84)	26.46 (± 0.33)	5.81 (± 0.83)Agnostic-FL (Mohri et al., 2019)	91.51 (± 0.48)	70.97 (± 0.31)	43.33 (± 0.22)	32.28 (± 0.19)	25.14 (± 0.31)	8.21 (± 0.73)Clustered-FL (Sattler et al., 2019)	90.59 (± 0.29)	77.78 (± 0.37)	49.68 (± 0.xx)	39.33 (± 0.34)	25.73 (± 0.xx)	11.61 (± 0.19)FedPer (Arivazhagan et al., 2019)	90.32 (± 0.41)	70.45 (± 0.18)	50.43 (± 0.xx)	29.45 (± 0.53)	33.58 (± 0.xx)	5.68 (± 0.17)Per-FedAvg (Fallah et al., 2020)	90.66 (± 0.35)	73.37 (± 0.43)	45.91 (± 0.73)	31.23 (± 0.62)	27.39 (± 0.86)	6.49 (± 0.33)
Table 2: Performance Comparison of Multi-domain Scenario We train 20 clients on 20 heterogeneousdatasets simultaneously for 100 rounds. We measure local accuracy at the final round and report averaged scoresover three trials. Note that in the multi-domained settings the classifier layer is not merged for fairness.
Table 3: Similarity MatchingMethod	Acc.(%)Worst Matching	48.83%Random Matching	52.12%SimFed (Ours)	67.35%aggregate, while worst matching chooses two the most dissimilar models for knowledge reflection ateach round. As shown in Table 3, both random and worst matching models significantly suffer from9Under review as a conference paper at ICLR 2022the performance degeneration compared to ours (around 15%p for Random Matching and 20%p forWorst Matching), demonstrating that we properly choose the beneficial knowledge for each round.
Table 4: Kernel FactorizationMethod	Acc.(%)SimFed (Ours)	67.35%w/o Factorization	63.31%w/o Masks φ	22.26%This alleviates the information loss when merging the heterogeneous knowledge. For further analysis,we compare two baseline models, which are ours with no factorization (w/o Factorization) and ourswith no masks (w/o Masks φ), in multi-domain scenario with the same training configurations. Whenwe eliminate the factorization technique (which is equivalent to the Similarity Matching only), themodel shows about 4%p lower performance compared to our full model, demonstrating factorizationhas beneficial effect on improving performance in multi-domain scenario. When we remove thesparse masks phi, we observe significant performance drop, demonstrating it successfully capturesthe non-linearity supporting the low rank vectors.
Table 5: More Factorization Analysis In another analysis, we conversely add our factorization tech-nique to the existing federated learning methods, e.g. FedAvgMethOd	IID (P)	NOnnD (P) + Kernel Factorization, and see how the factorization affectsFedAvg	45.99%	31.43%	the PUre federated learning method. We conduct experiments+ Factorization	49.72%	34.75%	on CIFAR-10 in permuted iid and non-idd settings for 100SimFed (Ours)	64.21%	45.37%	rounds. AS shown, the combined model shows 3 - 4%p higheraccuracy over pure federated learning algorithms. As the onlydifference between pure and combined models is the dimensionality of parameter space, which wedemonstrate our factorization method alleviate the knowledge collapse and information loss causedby the coordinate-wise aggregation in high dimensional parameter space. Ours still outperforms thebaseline model, meaning that our similarity matching algorithms further improves performance.
