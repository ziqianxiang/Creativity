Table 1: Comparison on fully correct formulas (fc) and sequence entropy (se) of GAN and WGANwith different σreal when generating temporal specifications. 3-run average, smoothed (α = 0.95),standard deviations in Table 5.
Table 2: Accuracies of Transformer classifiers trained on different datasets (5-run average Withstandard deviations in parentheses); all are validated on the LTLbase dataset.
Table 3: Performance of classifiers trained and tested on datasets generated with uncertainty objec-tives; 30K steps, 5-run average with standard deviations, not smoothed.
Table 4: Different number of shared layers for WGAN with included classifier, 2 runs each, 30Kstepsshared layers	Se	fc	Val acc0/4	ɪF	31.8% (0.2)	89.9% (2.3)2/4	2.2	26.6% (1.7)	92.5% (0.1)3/4	2.2	24.9% (0.3)	92.1% (0.6)4/4	2.2	24.0% (1.2)	90.5% (0.5)Table 4 shows classification benefits for sharing only some layers between classifier and critic. Alsonote that not sharing any layers, while yielding the highest fraction of fully correct formulas in thejoint GAN and classification objective, degrades performance in the uncertainty setting, where a lossis backpropagated through the classifier part.
Table 5: Standard deviations for Table 1, 3-run average, smoothed (α = 0.95)architecture	σreal	fc sd	Se sd	-0-	1.51	o~00Q	0.05	2.52	0.00WGAN	0.1	1.08	0.01	0.2	0.47	0.00	0.4	0.14	0.03We provide the standard deviations for Table 1 across 3 runs (see Table 5).
Table 6: GAN variant with uniform instead of Gaussian noise. 2-run average with standard devia-tions, smoothed (α = 0.99)min	max	fc	se0~O~	0.1	19.2% (2.94)	1.6 (0.19)0	0.2	33.3%(1.04)	1.8 (0.01)0	0.4	11.2% (3.32)	2.0 (0.07)As evident from Table 6 in comparison with Table 1, a uniform noise has no benefit over Gaussiannoise.
Table 7: Classifiers trained on different datasets tested out-of-distribution. 5-run average, notsmoothedtrained on	training steps	tested on	accuracyLTLbase	30K	Benchmarks	85.2% (2.2)Uncert-e	30K	Benchmarks	85.9% (2.9)Uncert-e	30K	LTLbase	87.5% (0.9)Mixed-e	30K	Mixed-e	92.7% (0.6)LTLbase	50K	Benchmarks	86.0% (5.0)Generated	50K	Benchmarks	94.1%(1.2)A synthetic dataset that is designed to bring classical solvers to their limits is a portfoliodataset (Schuppan & Darmawan, 2011), of which around 750 formulas fit into our encoder token andsize restrictions. We conducted an out-of-distribution test on these scalable benchmarks (Table 7).
