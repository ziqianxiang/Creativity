Table 1: Number of neurons and number of parameters in the different architecturesAdditive Multiplicative Control#	of recurrent units N	300	100	50#	of learned parameters for 10 motifs	3000	3000	3050#	of motif-specific parameters per motif	300	300 (input:	100, loop: 200)	50We now consider modifications of this control RNN that address this issue of catastrophic forgettingby segregating the parameters involved in learning different motifs while sharing common computa-tional resources across motifs (Parisi et al., 2019; Merel et al., 2019a;b). Here, We will share (i) fixedreadout weights (with the above-mentioned centered Gaussian distribution with Std 1 /√N as this issufficient to ensure the successful production of our motifs); and (ii) the recurrent weights as theycan set rich ‘baseline dynamics’ that can be modulated by some motif-specific weights. We globallyadjust the shared recurrent weights through tuning their above-mentioned gain hyperparameter g (asimilar alternative could be to pre-tune these recurrent weights to an original set of motifs and tothen freeze them, but - as we will see - our chosen approach leads to good results).
