Table 1: Mean and std accuracy of transductive node classification over 20 data splits and initial-izations obtained in BGRL paper (Thakoor et al., 2021) and our experiment (*) within the sameexperimental setup. OOM denotes running out of memory on a 16GB V100 GPU.
Table 2: Mean and std accuracy of transductivenode classification the ogb-arxiv dataset over 20data splits and initializations obtained in BGRLpaper (Thakoor et al., 2021) and our experiment(*) within the same experimental setup.
Table 3: Mean and std Micro-F1 of multilabelnode classification the PPI dataset over 20 modelinitializations obtained in BGRL paper (Thakooret al., 2021) and our experiment (*) within thesame experimental setup.
Table 4: Mean and std accuracy of inductive nodeclassification on the ogb-products dataset over 5model initializations obtained in the OGB leader-board and our experiment (*) within the same ex-perimental setup.
Table 5: Single epoch running time (in seconds) averaged over 10 training epochs.
Table 6: Evaluation of G-BT model in batched setting.
Table 7: Dataset statistics. We use small to medium sized standard datasets together with thelarger ogb-arxiv dataset in the transductive setting. We also evaluate the inductive setting using theogb-products and PPI (multiple graphs) dataset.
Table 8: Augmentation hyperparameters. Ogb-products was trained in the batched setting with abatch size of 512.	________________________________	G-BT		pA	pXWikiCS	0.2	0.1Amazon-CS	0.4	0.1Amazon-Photo	0.0	0.5Coauthor-CS	0.5	0.1Coauthor-Physics	0.1	0.4ogb-arxiv	0.2	0.0PPI	0.1	0.1ogb-products*	0.2	0.1Training setup For all datasets, we train our framework using the AdamW (Gugger & Howard,2018) optimizer with a weight decay of 10-5. The learning rate is adjusted using a cosine annealingstrategy with a linear warmup period up to the base learning rate. During training we set a totalnumber of epochs and an evaluation interval, after which the frozen embeddings are evaluated indownstream tasks (using either the l2 regularized logistic regression from Scikit learn (Pedregosaet al., 2011) with liblinear solver, or the custom PyTorch version with AdamW for ogb-arxiv andPPI). For instance, if we set the total number of epochs to 1000 and the evaluation interval to 500, themodel will be evaluated at epochs: 0, 500 and 1000 (three times in total). We report the values for
Table 9: Training hyperparameters.
Table 10: Encoder layer size parameters.
