Table 1: Depth vs. performance on ImageNet.
Table 2: Speed and performance of ParNet vs.
Table 3: Fusing features and parallelizing thesubstructures across GPUs improves the speed ofParNet. Speed was measured on a GeForce RTX3090 with Pytorch 1.8.1 and CUDA 11.1.
Table 4: Non-deep networks can be used as back-bones for fast and accurate object detection sys-tems. Speed is measured on a single RTX 3090using Pytorch 1.8.1 and CUDA 11.1.
Table 5: A network with depth 12 can get80.72% top-1 accuracy on ImageNet. We showhow various strategies can be used to boost theperformance of ParNet.
Table 6: Performance of various architectures on CIFAR10 and CIFAR100. Similar-sized modelsare grouped together. ParNet performs competitively with deep state-of-the-art architectures whilehaving a much smaller depth. Best performance is bolded. The second and third best performingmodel in each model size block are Underlined.
Table 7: Ablation of various choices forParNet. Data augmentation, SiLU acti-vation, and Skip-Squeeze-Excitation (SSE)improve performance.
Table 8: ParNet outperforms non-deep ResNet vari-ants. At depth 12, VGG-style blocks outperformResNet blocks, and three branches outperform a sin-gle branch.
Table 9: ParNet outperforms ensembles across different parameter budgets.
Table 10: Performance vs. number of streams. For a fixed parameter budget, 3 streams is optimal.
Table A1: Specification of ParNet models used for ImageNet classification: ParNet-S, ParNet-M,ParNet-L, and ParNet-XL.
