Table 1: Competence-based Unsupervised Skill Discovery AlgorithmsAlgorithm	Intrinsic Reward	Decomposition	Explicit max H(τ)	Skill Dim.	Skill SpaceSSN4HRL (Florensa et al., 2018)	log qψ(z∖st)	H(z) - H(z∖τ)	No	6	discreteVIC (Gregor et al., 2017)	log qψ(z∖sH))	H(z) - H(z∖τ)	No	60	discreteVALOR (Achiam et al., 2018)	log qψ(z∖si:H )	H(z) - H(z∖τ)	No	64	discreteDIAYN (Eysenbach et al., 2019)	log qψ(z∖St)	H(z) - H(z∖τ)	No	128	discreteDADS (Sharma et al., 2020)	qψ(S0∖z, S) -	i log q(S0∖zi, S)	H(τ) - H(τ∖z)	Yes	5	continuousVISR (Hansen et al., 2020)	log qψ(z∖St)	H(z) - H(z∖τ)	No	10	continuousAPS (Liu & Abbeel, 2021b)	FSuccessor(S∖z) + Hparticle (S)	H(τ) - H(τ ∖z)	Yes	10	continuousCIC (Ours)	FCPC(S, S0∖z) + Hparticle(S, S0)	H(τ) - H(τ ∖z)	Yes	64	continuousTable 2: A list of competence-based algorithms. We describe the intrinsic reward optimized by each methodand the decomposition of the mutual information utilized by the method. We also note whether the methodexplicitly maximizes state transition entropy. Finally, we note the maximal dimension used in each work andwhether the skills are discrete or continuous. All methods prior to CIC only support small skill spaces, eitherbecause they are discrete or continuous but low-dimensional.
Table 2: A list of competence-based algorithms. We describe the intrinsic reward optimized by each methodand the decomposition of the mutual information utilized by the method. We also note whether the methodexplicitly maximizes state transition entropy. Finally, we note the maximal dimension used in each work andwhether the skills are discrete or continuous. All methods prior to CIC only support small skill spaces, eitherbecause they are discrete or continuous but low-dimensional.
Table 3: Hyper-parameters used for CIC .
Table 4: Performance of CIC and baselines on state-based URLB after first pre-training for 2 × 106steps and then finetuning with extrinsic rewards for 1 × 105.
