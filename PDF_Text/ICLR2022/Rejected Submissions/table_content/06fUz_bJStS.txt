Table 1: Test accuracy of SOTA works before and after adopting random freeze. We maintain theaccuracy with high freeze rate. Communication overhead, computational cost and memory footprintof the projected DP-SGD are accordingly reduced as implied by total density in Table 2.
Table 2: Total representation density of random freeze. This table is aligned with Table 1.
Table 3: Test accuracy of End-to-end CNN adjusted with respect to inversely proportional scalingrule and trained with random freeze. Our adjusted baseline performs better than original work whilewith random freeze we further improve the accuracy. Altogether we obtain significantly better utility.
Table 4: Test accuracy of End-to-end CNN adjusted with respect to inversely proportional scalingrule and trained with ranked freeze. We adopt hyperparameters recorded in Table 9. Our adjustedbaseline performs better than original work while with ranked freeze we further improve the accu-racy. However, we note that ranked freeze performs similarly as random freeze (see Table 3).
Table 5: Hyperparameters of End-to-end CNN adopted from Tramer & Boneh (2021)DP-Transfer Learningε σ lr Batchsize Epoch Momentum Clip Source model Aux. dataset2 2.30 4	1000	50	0.9	0.1 SIMCLR v2 ImageNetTable 6:	Hyperparameters of DP-Transfer Learning adopted from Tramer & Boneh (2021)Handcrafted CNNε σ lr Batchsize Epoch Momentum Clip Input norm BN norm Architecture3	5.65	4	8000	80	0.9	0.1 BN	8 ScatterNet + CNNTable 7:	Hyperparameters of Handcrafted CNN adopted from Tramer & Boneh (2021)Gradient Embedding Perturbationε lr Batchsize Epoch Momentum Clip0 Clip1 Subspace Sample gradients Aux. dataset8 0.1	1000	200	0.9	5	2	1000	2000	ImageNetTable 8:	Hyperparameters of GEP adopted from Yu et al. (2021a)E Improving the accuracy of End-to-end CNNTable 9 includes the hyperparameters and total density of ajusted End-to-end CNN with randomfreeze. Figure 6 shows the accuracy as a function of privacy loss.
Table 6:	Hyperparameters of DP-Transfer Learning adopted from Tramer & Boneh (2021)Handcrafted CNNε σ lr Batchsize Epoch Momentum Clip Input norm BN norm Architecture3	5.65	4	8000	80	0.9	0.1 BN	8 ScatterNet + CNNTable 7:	Hyperparameters of Handcrafted CNN adopted from Tramer & Boneh (2021)Gradient Embedding Perturbationε lr Batchsize Epoch Momentum Clip0 Clip1 Subspace Sample gradients Aux. dataset8 0.1	1000	200	0.9	5	2	1000	2000	ImageNetTable 8:	Hyperparameters of GEP adopted from Yu et al. (2021a)E Improving the accuracy of End-to-end CNNTable 9 includes the hyperparameters and total density of ajusted End-to-end CNN with randomfreeze. Figure 6 shows the accuracy as a function of privacy loss.
Table 7:	Hyperparameters of Handcrafted CNN adopted from Tramer & Boneh (2021)Gradient Embedding Perturbationε lr Batchsize Epoch Momentum Clip0 Clip1 Subspace Sample gradients Aux. dataset8 0.1	1000	200	0.9	5	2	1000	2000	ImageNetTable 8:	Hyperparameters of GEP adopted from Yu et al. (2021a)E Improving the accuracy of End-to-end CNNTable 9 includes the hyperparameters and total density of ajusted End-to-end CNN with randomfreeze. Figure 6 shows the accuracy as a function of privacy loss.
Table 8:	Hyperparameters of GEP adopted from Yu et al. (2021a)E Improving the accuracy of End-to-end CNNTable 9 includes the hyperparameters and total density of ajusted End-to-end CNN with randomfreeze. Figure 6 shows the accuracy as a function of privacy loss.
Table 9: Hyperparameters for adjusted End-to-end CNN with random freeze18Under review as a conference paper at ICLR 2022Method: - Adjusted Baseline - Baseline — Random FreezetrainFigure 6: Accuracy as a function of privacy loss. We run five experiments and computes the meanvalue. The result shows that random freeze outperforms the baseline and adjusted baseline in allhigh privacy levels.
