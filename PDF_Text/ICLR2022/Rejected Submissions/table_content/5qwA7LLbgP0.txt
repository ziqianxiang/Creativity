Table 1: Payoff matrix of the two-step game. N(μ, σ2) denotes the normal distribution with a meanof μ and a variance of σ2. For the first step, agent-wise risk-seeking (cooperation) is crucial becausethe optimal action is (A, A) but taking action A is very risky because when the teammate chooses tobe non-cooperative (i.e., by selecting action B or C), a catastrophic reward of -12 is provided. In thesecond step, handling environment-wise risks is highlighted since there are various combinations ofmean and variance depending on joint actions, i.e., N (-1, 10) for the action (C, C) and N (-1, 0)for the action (B, B).
Table 2: Test rewards and trained policy in the stochastic two-step matrix game for DRIMA, DMIX,and OW-QMIX with varying risk-sensitivity across twelve random seeds.
Table 3: Test rewards and trained policy in the stochastic two-step matrix game for DRIMA, QMIX,QTRAN, and QPLEX across twelve random seeds. Note that QMIX, QTRAN, and QPLEX have noability to adjust risk sensitivity.
Table 4: Payoff matrix of the one-step game.
Table 5: Test rewards and trained policy in the one-step matrix game for DRIMA, DMIX, andOW-QMIX with varying risk-sensitivity across twelve random seeds.
