Table 1: 3D point cloud segmentation results on UOR dataset (blue) and UOT dataset (red).
Table 2: Glimpse related hyperparameters.
Table 3: Prior distributions.
Table 4: Other hyperparameters.
Table 5: Voxel grid encoder.	Table 6: Glimpse encoder.
Table 8: Glimpse Point Graph Flow.
Table 9: Mask decoder.
Table 10: Global encoder.
Table 11:	Multi-layer PointGNN.
Table 12:	Camera intrinsic parametersTerm	Valuefocal length	10	mmsensor size x	16	mmsensor size y	16	mmclipping plane	20	mFigure 10: Data captured by each camera in UOR dataset (top) and UOT dataset (bottom). From leftto right are RGB, depth, normal, instance label, semantic label and constructed point cloud. Pointclouds are obtained by merging multi-view depth images. Instance labels and semantic labels areused to train PointGroup (Jiang et al., 2020) baseline. RGB images and normal maps are not used inthis work.
Table 13: UOR object pool.
Table 14: UOT Object pool. Some object meshes are obtained from ai2thor (Kolve et al., 2017)environment.
