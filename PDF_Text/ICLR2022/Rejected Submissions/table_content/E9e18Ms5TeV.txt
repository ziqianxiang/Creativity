Table 1: The hyperparameters ofConfiguration B that differ fromstate-of-the-art LARS at batch sizeTable 1 shows the hyperparameter values for Configuration 32,768 (Kumar et al., 2019).
Table 2: (Left) The best warmup schedule differs for Nesterov momentum and LARS. Values aremedians over 50 training runs after setting pwarmup without retuning other hyperparameters. (Right)Median train and test accuracies over 50 training runs for Nesterov momentum Configuration B andLARS.
Table 3: Using Adam for pretraining exceedsthe reported performance of LAMB in You et al.
Table 4: Validation accuracy of ResNet-50 on Ima- geNet trained for 6,000 steps instead of 2,512. The second column is the optimizer that was applied to the batch norm and ResNet bias variables. We report the median top-1 accuracy over 5 seeds of the best hyperparameter setting in a refined search space. See Appendix E.3 for details.			LAMB, are introduced alongside claims that they do not require any—or at least minimal— tuning. Unfortunately, these claims require a lot of work to support, since they require try- ing the optimizer on new problems not used during the development of the algorithm. Al- though our experiments here are not sufficient to determine which optimizers are easiest to tune, experiments like those in Table 4 that operateoutside the regime of highly tuned learning rate schedules can serve as a starting point, and we includepreliminary “tunability” experiments using a simple grid search for LARS and Nesterov (details andmore figures in Appendix D).
