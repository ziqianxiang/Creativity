Table 1: Statistics of the models considered, trained from scratch on ImageNet. Top-1 accuracyis measured on ImageNet-1k validation set. “TTT” stands for total training time (including fine-tuning), normalized by the total training time of the ResNet50-RS. dr is the stochastic depth coefficientused for the various models.
Table 2: The benefit of late reparametrization. We report the top-1 accuracy of a ResNet-50on ImageNet reparameterized at various times t1 during training. ↑320 stands for fine-tuning atresolution 320. The models with a ? keep the same optimizer after reparametrization, in contrast withthe usual T-CNNs.
Table 3: Accuracy of our models on various benchmarks. Throughput is the number of imagesprocessed per second on a V100 GPU at batch size 32. For ImageNet-C, we keep a resolution of 224at test time to avoid distorting the corruptions; this disadvantages our large models, which are trainedat higher resolutions. f: reported from (Mao et al., 2021) (We recalculated ImageNet-C accuracies, asthe original paper reports MCE). ∣: reported from (Bhojanapalli et al., 2021) (in their setup, PGDuses 8 steps With a stepsize of 1/8).
Table 4: Longer fine-tuning increases final performance. We report the top-1 accuracies of ourmodels on ImageNet-1k at resolution 224.
Table 5: Comparing the performance gains of the ResNet-RS and ResNet-D architectures. Top-1 accuracy is measured on ImageNet-1k validation set. The pre-trained models are all taken from thetimm library Wightman (2019).
