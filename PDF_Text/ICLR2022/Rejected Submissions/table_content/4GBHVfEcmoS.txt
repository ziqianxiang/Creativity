Table 1: Simulation of propagating normal distributions through aneural net. Reported is the intersection of probability mass (1 - T V ).
Table 2: Results for the aleatoric uncertainty experiment. The task is to compute calibrated predictionintervals for 8 UCI data sets. Reported are test PICP and MPIW in parentheses. For MPIW lower isbetter. All results are averaged over 20 runs. Prior methods are duplicated from Tagasovska et al. [6].
Table 3: Selective prediction settings including the risk-coverage AUC for Fig. 2.
Table 4: CIFAR-10 performance with ResNets.
Table 5: Computational Cost Benchmark. Times per epochon CIFAR-10 with a batch size of 128 on a single V100 GPU.
Table 6: Simulation of propagating normal distributions. The network is a 2 layer ReLU activatednetwork with dimensions 4-100-3, i.e., 1 ReLU activation.
Table 7: Simulation of propagating normal distributions. The network is a 3 layer ReLU activatednetwork with dimensions 4-100-100-3, i.e., 2 ReLU activations.
Table 8: Simulation of propagating normal distributions. The network is a 5 layer ReLU activatednetwork with dimensions 4-100-100-100-100-3, i.e., 4 ReLU activations.
Table 9: Simulation of propagating normal distributions. The network is a 7 layer ReLU activatednetwork with dimensions 4-100-100-100-100-100-100-3, i.e., 6 ReLU activations.
Table 10: Simulation of propagating normal distributions. The network is a 2 layer Leaky-ReLUactivated network with dimensions 4-100-3, i.e., 1 Leaky-ReLU activation with negative slopeα = 0.1.
Table 11: Simulation of propagating normal distributions. The network is a 3 layer Leaky-ReLUactivated network with dimensions 4-100-100-3, i.e., 2 Leaky-ReLU activations with negativeslope α = 0.1.
Table 12: Simulation of propagating normal distributions. The network is a 5 layer Leaky-ReLUactivated network with dimensions 4-100-100-100-100-3, i.e., 4 Leaky-ReLU activationswith negative slope α = 0.1.
Table 13: Simulation of propagating normal distributions. The network is a 7 layer Leaky-ReLUactivated network with dimensions 4-100-100-100-100-100-100-3, i.e., 6 Leaky-ReLUactivations with negative slope α = 0.1.
Table 14: Simulation of propagating normal distri-butions. The network is a 2 layer SiLU activatednetwork with dimensions 4-100-3, i.e., 1 SiLUactivation.
Table 18: Simulation of propagating normal dis-tributions. The network is a 2 layer GELU acti-vated network with dimensions 4-100-3, i.e., 1GELU activation.
Table 15: Simulation of propagating normal distri-butions. The network is a 3 layer SiLU activatednetwork with dimensions 4-100-100-3, i.e., 2SiLU activations.
Table 19: Simulation of propagating normal distri-butions. The network is a 3 layer GELU activatednetwork with dimensions 4-100-100-3, i.e., 2GELU activations.
Table 16: Simulation of propagating nor-mal distributions. The network is a 5layer SiLU activated network with dimensions4-100-100-100-100-3, i.e., 4 SiLU activa-tions.
Table 20: Simulation of propagating nor-mal distributions. The network is a 5layer GELU activated network with dimensions4-100-100-100-100-3, i.e., 4 GELU acti-vations.
Table 17: Simulation of propagating nor-mal distributions. The network is a 7layer SiLU activated network with dimensions4-100-100-100-100-100-100-3, i.e., 6SiLU activations.
Table 21: Simulation of propagating nor-mal distributions. The network is a 7layer GELU activated network with dimensions4-100-100-100-100-100-100-3, i.e., 6GELU activations.
Table 22: Simulation of propagating normal distributions. The network is a 5 layer Logistic Sigmoidactivated network with dimensions 4-100-100-100-100-3, i.e., 4 Logistic Sigmoid activations.
Table 23: Simulation of propagating normal distributions. The network is a 5 layer ReLU activatednetwork with dimensions 4-100-100-100-100-3, i.e., 4 ReLU activations. Displayed is theaverage ratio between the output standard deviations. The 3 values, correspond to the three outputdimensions of the Iris model.
