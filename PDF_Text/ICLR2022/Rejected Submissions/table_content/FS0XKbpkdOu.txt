Table 1: Geo-aware image classification under fully supervised settings over three tasks: species recognition,POI image classification (YFCC), and remote sensing (RS) image classification (fMOW (Christie et al., 2018)).
Table 2: The best hyperparameter combinations of Sphere2Vec models on different image classification datasets.
Table 3: Dimension of position encoding for different models in terms of total scales SModel	SphereC	SphereC'	SphereM	SphereM'	SphereDFSDimension 3S	6S	5S	8S	4S* 1 2 + 4S^^We also find out that using a deeper MLP as NNf fn(∙), i.e., a larger h does not necessarily lead tobetter classification accuracy. In many cases, one hidden layer - h “ 1 achieves the best performancefor many kinds of location encoders. We discuss this in detail in Appendix 9.9.
Table 4: The impact of the depth h of multi-layer perceptrons NNffn pq on Top1 accuracy. The numbersin “()” indicates the standard deviations estimated from 5 independent train/test runs. We find that the modelperformances are not very sensitive to NNffnpq, and, in most cases, one layer NNffnpq achieve the best result.
Table 5: Ablation Study on unsupervised loss LMC over iNat2018 dataset. We show the effect of differenthyperparematers of LMC on the performance of location encoders. We use sphereM ` as an representativelocation encoder and use supervised training dataset ratio Γ as 0.5. lr indicates the learning rate used forunsupervised LMC training. dropout indicates the dropout rate used by location encoder which will affect theSimCSE loss as Gao et al. (2021) shows. α1 and α2 are weights for the negative location loss LnMeCglocpXq andSImCSE loss LsMimC cse pXq component respectively. τ0, τ1, and τ2 are the temperatures used by three differentloss components. See Equation 7 for the explanation.
Table 6: Ablation Study on unsupervised loss LBI over iNat2018 dataset. We show the effect of differenthyperparematers of LBI on the performance of location encoders. We use sphereM ` as an representativelocation encoder ad use supervised training dataset ratio Γ as 0.5. lr indicates the learning rate used forunsupervised LMC training.
Table 7: Ablation Study on unsupervised loss MSE over iNat2018 dataset. We show the effect of differenthyperparematers of MSE on the performance of location encoders. We use sphereM ` as an representativelocation encoder ad use supervised training dataset ratio Γ as 0.5. lr indicates the learning rate used forunsupervised MSE training.
Table 8: Compare sphereM ` to baselines on synthetic datasets. U1 - U4 indicate 4 synthetic datasets generatedbased on the uniform sampling approach (see Appendix 9.15.1). S1.1 - S4.4 indicate 16 synthetic datasetsgenerated based on the stratified sampling apprach. For all datasets have C “ 50 and SP “ 100. For eachmodel, we perform grid search on its hyperparameters for each dataset and report the best Top1 accuracy. The∆T op1 column shows the absolute performance improvement of sphereM ` over the best baseline model(bolded) for each dataset. The ER column shows the relative reduction of error compared to the best baselinemodel (bolded). We can see that sphereM ` can outperform all other baseline models on all of these 20synthetic datasets. The absolute Top1 accuracy improvement can be as much as 2.0% for datasets with lowerprecisions, and the error rate deduction can be as much as 30.8% for datasets with high precisions.
