Table 1: Comparison of 4-bit training of the proposed method LUQ with Ultra-low (Sun et al., 2020)in various DNNs models with ImageNet dataset. FNT refers to fine-tune the trained model oneadditional epoch with the neural gradients at high precision (Section 3.2) and SMP refers to doingtwo samples of the SR quantization of neural gradients in order to reduce the variance (Section 3.1).
Table 2: Comparison of the BLUE score for 4-bit training of the proposed method LUQ with Ultra-low(Sun et al., 2020) in Transfomer base model on the WMT En-De task.
Table 3: ResNet-50 accuracy with ImageNet dataset while quantization different parts of the network.
Table 4: Rough estimation of the number of logical gates for a standard GEMM block Which containtWo blocks: a casting to FP7 and a FP7 multiplier.
Table 5: Rough estimation of the number of logical gates for the proposed MF-BPROP block.
