Table 1: Performance of diversity-regularized BERT pre-training with different values of di-versity factor λ. We finetune the pretrained model on 8 downstream tasks from GLUE benchmarkand evaluate them on their dev sets. All results are “mean (std)” from 5 runs with different randomseeds. For MNLI, we average the accuracies on its matched and mismatched dev sets. For MRPCand QQP, we average their accuracy and F1 scores. For STS-B, we average Pearson’s correlationand Spearman’s correlation. All other tasks uses accuracy as the metric. The better-than-baselinenumbers are underlined, and the best numbers are highlighted in boldface.
Table 2: FUll StatiSticS on GLUE dev sets.
Table 3: Performance of reproduced BERT-base model.
Table 4: Performance of λ = 0.005 regularized pre-training model.
Table 5: Performance of λ = 0.05 regularized pre-training model.
Table 6: Performance of λ = 0.5 regularized pre-training model.
Table 7: Hyperparameters used in pre-training our models. We use the LAMB optimizer (You et al.,2019) for large-batch pretraining of the BERT model, where β1 and β2 are its two hyper-parameters.
Table 8: The hyperparameters used in finetuning our model in downstream tasks. LR: learningrate. BSZ: batch size. #EP: number of epochs. WARMUP: warmup ratio. FP16: automatic mixedprecision (AMP) level. SEQ: input sequence length.
