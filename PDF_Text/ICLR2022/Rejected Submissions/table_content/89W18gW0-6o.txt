Table 1: Average testing return (standard deviation in parenthesis) of FOCAL and variants ofFOCAL++.
Table 2: Variance of context embeddings averaged over all training tasks and latent dimensions.
Table 3: Average testing return of FOCAL and FOCAL++ on Sparse-Point-Robot with differentdistributions of training/testing sets. The numbers in parenthesis represent performance drop due todistribution shift. Additional experiments are presented in Apppendix C.
Table 4: Extension of Table 3 in the main text. Average testing return of FOCAL and FOCAL++ formore settings of distribution shift on Walker-2D-Params.
Table 5: Specifications of the environments experimented in our paper.
Table 6: Hyperparameters used for training to produce Figure 4(a). Meta-batch size refers to thenumber of distinct tasks for computing the DML or contrastive loss at a time. Larger meta-batch sizeleads to faster convergence but requires greater computational power. For Fwd-Back environments, ameta-batch size of 4 suffices for stability and efficiency.
