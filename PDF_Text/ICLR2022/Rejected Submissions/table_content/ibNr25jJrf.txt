Table 1: Hyperparameters used in the numerical experiments. The architecture of the decoding DNNis listed as H0-H1-. . . -D with H0 and D indicating the number of Bernoulli latents and Gaussianobservables respectively, and H1-. . . denoting the number of hidden layer units. By default, weused ReLU activations in the hidden layers and a linear output layer. For Barbara and CIFAR-10,we used LeakyReLU instead of ReLU; for CIFAR-10, we additionally used a Sigmoid in the outputlayer. Min and max l.r. denote lower and upper learning rate boundaries and are, together withEPochs/Cycle, hyperparameters of the cyclical learning rate scheduler (cf. Smith, 2017). ∣Φ(n)∣denotes the number of distinct latents per data point (referred to as S in Alg. 1). * and ^ refer to theparameters used for σ ∈ {15, 25} and σ = 50 in Fig. 3E, respectively.
Table 2: Denoising Performance of n2v in PSNR (dB) for the ‘House’ Image. For comparison, weadditionally list the performance of TVAE (numbers copied from Fig. 3 E). PSNR values for n2v?are obtained by training only on the noisy image (i.e., in the same setting as used for MTMKL, GSC,var-BSC and TVAE in Fig. 3 E). More training data improves performance for n2v. PSNR valuesfor n2v* show performance if additional training data in the form of noisy images with AWG noiseσ = 25 is used. Further improvements (especially for high noise) are obtained if the n2v networkis trained on training data with a noise level that matches the noise of the test set (see n2v^)∙ Forinstance, We used for n2v^ training data with σ = 50 to denoise the ‘house, with σ = 50. PSNRvalues were computed using the model in its state at the training epoch with smallest validation loss;for n2v?, we performed three independent runs of the algorithm and here report the results of thebest run (in terms of validation loss). See text for further details.
Table 3: Runtimes required by GSVAE, VLAE and TVAE to reach the lower bounds and PSNRsreported in Fig.4B. t Total runtime of VLAE consists of 5.1 hrs for parameter optimization and1.8hrs for importance sampling. The PyTorch implementations of the models were executed on asingle NVIDIA Tesla V100 NV-Link 32GB HBM2 on a server with Intel Xeon 4214 12-core 2.20GHz CPUs.
