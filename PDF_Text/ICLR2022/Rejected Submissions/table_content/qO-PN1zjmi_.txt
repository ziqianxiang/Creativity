Table 1: Taxonomy of novelty detection methods, categorized according to data availability(horizontal axis) and overall objective (vertical axis). We highlight the ensemble-based methods.
Table 2: AUROC and TNR@95 for different OOD detection scenarios (the numbers in squared brackets										indicate the ID or OOD classes).			We highlight the best ERD variant and best baseline.						The asterisk marks	baselines proposed in this paper. nnPU and MCD (1)					use oracle information about the OOD data.								Other settings					Unknown OOD		ID data OOD data	Vanilla Ensembles	Gram	DPN	OE	Mahal.	nnPU∣	MCDt	Mahal-U	Bin. Classif. *	ERD *					AUROC ↑ / TNR@95 ↑					SVHN CIFAR10	0.97 / 0.88	0.97 / 0.86	1.00 / 1.00	1.00 / 1.00	0.99 / 0.98	1.00 / 1.00	0.97 / 0.85	0.99 / 0.95	1.00 / 1.00	1.00 / 0.99CIFAR10 SVHN	0.92 / 0.78	1.00 / 0.98	0.95 / 0.85	0.97 / 0.89	0.99 / 0.96	1.00 / 1.00	1.00 / 0.98	0.99 / 0.96	1.00 / 1.00	1.00 / 1.00CIFAR100 SVHN	0.84 / 0.48	0.99 / 0.97	0.77 / 0.44	0.82 / 0.50	0.98 / 0.90	1.00 / 1.00	0.97 / 0.73	0.98 / 0.92	1.00 / 1.00	1.00 / 1.00FMNIST FMNIST [0,2,3,7,8] [1,4,5,6,9]	0.64 / 0.07	-/-	0.77 / 0.15	0.66 / 0.12	0.77 / 0.20	0.95 / 0.71	0.78 / 0.30	0.82 / 0.39	0.95 / 0.66	0.94 / 0.67SVHN	SVHN [0:4]	[5:9]	0.92 / 0.69	0.81 / 0.31	0.87 / 0.19	0.85 / 0.52	0.92 / 0.71	0.96 / 0.73	0.91 / 0.51	0.91 / 0.63	0.81 / 0.40	0.95 / 0.74CIFAR10 CIFAR10 [0:4]	[5:9]	0.80 / 0.39	0.67 / 0.15	0.82 / 0.32	0.82 / 0.41	0.79 / 0.27	0.61 / 0.11	0.69 / 0.25	0.64 / 0.13	0.85 / 0.43	0.93 / 0.70CIFAR100 CIFAR100 [0:49]	[50:99]	0.78 / 0.35	0.71 / 0.16	0.70 / 0.26	0.74 / 0.31	0.72 / 0.20	0.53 / 0.06	0.70 / 0.26	0.72 / 0.19	0.66 / 0.13	0.82 / 0.44Average	0.84 / 0.52	0.86 / 0.57	0.84 / 0.46	0.84 / 0.54	0.88 / 0.60	0.86 / 0.66	0.86 / 0.55	0.86 / 0.60	0.89 / 0.66	0.95 / 0.79sets.6 For the binary classifier and nnPU, we pick hyperparameters only to optimize the loss on anID validation set. We defer the details regarding training the models to Appendix C.
Table 3: The disagreement score that we propose (Avg ◦ ρ) exploits ensemble diversity and benefits in partic-ular ERD ensembles. OOD detection performance is significantly improved when using (Avg ◦ ρ) compared tothe previously proposed (H ◦ Avg) metric. Since Vanilla Ensemble are not diverse enough, a score that relieson model diversity can hurt OOD detection performance. We highlight the AUROC and the TNR@95 obtainedWith the score function that is best for Vanilla Ensemble and the best fθr ERD.
Table 4: Comparison between the OOD detection performance of ERD when using a holdout testset T for evaluation, or the same unlabeled set U that was used for fine-tuning the models.
Table 5: AUROC numbers collected from the literature for a number of relevant OOD detectionmethods. We note that the method of Fort et al. (2021) (*) uses a large scale visual transformermodels pretrained on a superset of the OOD data, i.e. ImageNet21k, While the method of SehWaget al. (2021) (*) uses oracle OOD samples for training from the same data set as test OOD. For thesettings With random classes, the numbers are averages over 5 draWs and the standard deviation isalWays strictly smaller than 0.01 for our method.
Table 6: OOD detection performance on CIFAR10 vs CIFAR10v2ID data OOD data	Vanilla Ensembles	DPN	OE	Mahal. AUROC	MCD ↑ / TNR@95 ↑	Mahal-U	ERD	ERD++CIFAR10 CIFAR10v2	0.64 / .H3	0.63 / 0.09	0.64 / 0.12	0.55 / 0.08	0.58 / 0.10	0.56 / 0.07	0.76 / 0.26	0.91 / 0.80Our OOD detection experiments (presented in Table 6) show that most baselines are able to distin-guish between the two data sets, with ERD achieving the highest performance. The methods whichrequire OOD data for tuning (Outlier Exposure and DPN) use CIFAR100.
Table 7: OOD detection performance on data with covariate shift. For ERD and vanilla ensembles,we train 5 ResNet20 models for each setting. The evaluation metrics are computed on the unlabeledset.
Table 8: Experiments with a test set of size 1,000, with an equal number ofID and OOD test samples.
Table 9: Results for Outlier Exposure, when using the same corruption type, but with a higher/lowerseverity, as OOD data seen during training.
Table 10: Results on MNIST/FashionMNIST settings. For ERD and vanilla ensembles, we train 53-hidden layer MLP models for each setting. The evaluation metrics are computed on the unlabeledset.
Table 11: Results with three different architectures for Vanilla and ERD ensembles. All ensemblescomprise 5 models. For the corruption data sets, we report for each metric the average taken over allcorruptions (A), and the value for the worst-case setting (W). The evaluation metrics are computedon the unlabeled set.
Table 12: Results obtained with smaller ensembles for ERD. All numbers are averages over 3 runs,where we use a different set of arbitrary labels for each run to illustrate our method’s stability withrespect the choice of labels to be assigned to the unlabeled set. We note that the standard deviationsare small (σ ≤ 0.01 for the AUROC values and σ ≤ 0.08 for the TNR@95 values).
