Table 1: Experimental results on the test sets, when LMs have access to a full narrative context.
Table 2: Automatic analysis of generated stories. On the left: patterns of entity usage (i.e. number ofunique entities, mentions per entity). On the right: evaluation metrics (i.e. exact and subset match ofgenerated entities to the gold ones, long-range entity coherence, as maximum window of mentionsfor the protagonists (C), and attribute consistency (U )).
Table 3: Human evaluation study for the WikiPlots dataset. The same generated stories used for theautomatic analysis are also provided to human judges. The questions asked per story are related tocontrol (Cont) coherence (Coh), consistency (Cons), and fluency (Flu). We also report the averagerank for each model and the percentage that each LM was selected as best/worst. Differences withbold are significant with p < 0.05.
Table 4: Pearson correlation coefficient between automatic metrics and human ratings.			Table 5: Pearson correlation coefficient be- tween human ratings and overall preference.	Finally, we observe that S -MnemeLM is more often selected as best and least often as worst. Incontrast, VanillaLM is significantly more often selected as worst and least often as best. Thisindicates that entity coherence significantly influences the quality of the generated stories and is animportant aspect of language that we should consider in language modeling.
Table 6:	Inter-annotator agreement for the human evaluation study. We present the percentage oftimes that the two judges exactly agree (i.e. exact agreement), and the average distance betweentheir ratings for the intermediate questions (i.e. in a scale 1 to 4 for Control (Cont), Consistency(Cons), and Fluency (Flue) and 1 to 3 for Coherence (Coh)).
Table 7:	Example of generated stories given the entity prompt (WikiPlots). We marked the goldentities from the prompt that were used by the models, new entities that were introduced by themodel and fit in the story, and irrelevant entities that introduced in later sections.
Table 8:	Example of generated stories given the entity prompt (WikiPlots). We marked the goldentities from the prompt that were used by the models, new entities that were introduced by themodel and fit in the story, and irrelevant entities that introduced in later sections.
Table 9:	Example of generated stories given the entity prompt (WritingPrompts). We marked thegold entities from the prompt that were used by the models, neW entities that were introduced bythe model and fit in the story, and irrelevant entities that introduced in later sections.
