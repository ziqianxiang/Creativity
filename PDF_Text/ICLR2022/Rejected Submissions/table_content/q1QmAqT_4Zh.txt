Table 1: We experiment with the full set of the D4RL tasks and report the mean normalized episodic returnsover 5 random seeds using the same protocol as Fu et al. (2021). We compare against 3 competitive baselinesincluding CQL and the two best performing S4RL-data augmentation strategies. We see that KFC and KFC++consistently outperforms the baselines. We use the baseline numbers reported in Sinha et al. (2021).
Table 2: We study the effect of combining S4RL-N based augmentation training on “contact” event transitions(state transitions where the agent makes contact with a surface, read D.1 for more details), and KFC++ aug-mentation training on non-“contact” events. We see that KFC++-contact performs similarly and in most cases,slightly better than the KFC++ baseline, which is expected. We experiment with the Open AI gym subset of theD4RL tasks and report the mean normalized returns over 5 random seeds.
Table 3: Results with prediction model KFC++-prediction on the Open AI Gym subset of the D4RL tasks. Wereport the mean normalized episodic rewards over 5 random seeds similar to the original D4RL paper Fu et al.
Table 4: Results with prediction model Fwd-prediction on the D4RL tasks. We report the mean normalizedepisodic rewards over 5 random seeds similar to the original D4RL paper Fu et al. (2021).
