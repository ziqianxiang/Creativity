Table 1: Empirical evaluation for detecting spurious corre-lations in training data. We report Precision@3 and ranksaveraged over 20 scenarios, the complete table is in AppendixTable 2. Distribution of the Precision@K metric as we varyK can be found in Appendix A.8.
Table 2: Empirical evaluation for detecting spurious correlations in the training data. We reportresults over 20 scenarios, where each class is associated with the concept in parenthesis during thetraining phase.
Table 3: Accuracy of the model for images tested with and without the confounding variable.
Table 4: CCE suggestions when the target concept is missing from the concept bank. We average theranks for each concept over 50 mistakes in each experiment, and report the Top-5 concepts with thehighest rank.
Table 5: List of concepts and validation accuracies for SVMs, for a ResNet18 pretrained on ImageNet.
