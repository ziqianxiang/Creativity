Table 1: A Perspective on Related WorkTechnique	State-state policy	Inverse/forward model	State cond. Q funct.	Policy Hierarchy	Planning horizonSPP (ours)	yes	inverse	no	single policy	single stepD3G	yes	inverse	yes	single policy	single stepHRL	yes(upper level)	inverse	yes(upper level)	multiple policies	multiple stepsPlanning	yes	inverse	yes	N/A	multiple stepsModel based	no	forward	N/A	single policy	single stepat time t, causing the state transition st+1 = E(st, at), as a result the agent collects a scalar rewardrt+1(st, at), the return is defined as the sum of discounted future reward Rt = PiT=t γ(i-t)r(si, ai).
Table 2: SPP-DDPG Algorithm hyperparameters, DDPG used the same hyperparameters, exceptexploration noise (set to 0.1).
Table 3: SPP-TD3 Algorithm hyperparameters for MuJoCo environments, TD3 used the same (com-mon) hyperparameters.
Table 4: SPP-TD3 Algorithm hyperparameters for SafetyGym environments. Observe that for thisenvironment specifically we used larger number of CM pretrain samplesHyperparameter	ValueY	-0.99-τ	0.005actor/critic learning rate lφ = lθ	0.0001λ learning rate lλ	0.0001episode length	5000batch size	100test episodes	10update freq. & grad.steps.	50exploration noise param.	1policy noise	0.2noise clip.	0.5replay buffer size	250000parameters specific for SPP-TD3	d state consistency	0.2CM hyper-parameters	init. rand. samples	100000learning rate	0.001
Table 5: SPP-TD3 Algorithm hyperparameters for AntPush environment. Observe that for this en-vironment specifically we used fixed buffer size which is significantly smaller than the total numberof env interacts, and a much larger exploration noise param equal to 1.
Table 6: SPP-SAC Algorithm hyperparametersE	Additional ExperimentsE.1 Ablation StudyWe performed a thorough ablation study of the features that we implemented in the SPP-TD3 algo-rithm presented in Sec. C. Ablation study was performed in Ant (Fig. 7a) and Doggo-Goal (Fig. 7b).
