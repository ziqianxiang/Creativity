Under review as a conference paper at ICLR 2022
Learning Distributions Generated by Single-
Layer ReLU Networks in the Presence of Ar-
bitrary Outliers
Anonymous authors
Paper under double-blind review
Ab stract
We consider a set of data samples such that a constant fraction of the samples
are arbitrary outliers and the rest are the output samples of a single-layer neural
network (NN) with rectified linear unit (ReLU) activation. The goal of this paper
is to estimate the parameters (weight matrix and bias vector) of the NN assuming
the bias vector to be non-negative. Our proposed method is a two-step algorithm.
We first estimate the norms of the rows of the weight matrix and the bias vector
using the gradient descent algorithm. Here, we also incorporate either the median
or the trimmed mean based filters to mitigate the effect of the arbitrary outliers.
Next, we estimate the angles between any two row vectors of the weight matrix.
Combining the estimates of the norms and the angles, we obtain the final estimate
of the weight matrix. Our main contribution is the sample complexity for robust
estimation rather than the algorithm itself. Here, We prove that Ω(表 log d) sam-
ples are sufficient for our algorithm to estimate the NN parameters within an error
of With probability 1 - δ When the probability of a sample being uncorrupted is
p and the problem dimension is d. Our theoretical and simulation results provide
insights on hoW the estimation of the NN parameters depends on the probability of
a sample being uncorrupted, the number of samples, and the problem dimension.
1	Introduction
A fundamental challenge in machine learning and statistics is to estimate a high dimensional dis-
tribution given a set of observed data samples from the distribution. One solution technique is the
deep generative method that models the unknoWn distribution as the output distribution of a neural
netWork (NN) When the input of the NN is draWn from a knoWn distribution like standard Gaussian
Which is motivated from the case of image generation by generative adversarial netWorks (GANs)
(GoodfelloW et al. (2014); Arjovsky et al. (2017); Radford et al. (2015); Kingma & Welling (2013);
Van Oord et al. (2016)). Then, the unknoWn distribution is estimated by learning the NN parameters
from the data samples. Several approaches like GANs (GoodfelloW et al. (2014); Arjovsky et al.
(2017); Radford et al. (2015)), variational autoencoders (Kingma & Welling (2013)), and autore-
gressive models (Van Oord et al. (2016)) have been proposed to train the NN. HoWever, they do not
provide theoretical guarantees for the parameter learning. So, We address the open problem of com-
puting the sample complexity for the parameter estimation of a single-layer NN With rectified linear
unit (ReLU) activation using a corrupted dataset in the unsupervised learning frameWork. Here, the
corrupted dataset refers to the model Wherein the output samples consists of a fraction of arbitrary
outliers introduced by an adversary (Byzantine). Motivated by the deep generative models, We con-
sider the unsupervised learning frameWork, i.e., We assume the knoWledge of the output samples
but, the corresponding inputs are unknoWn and draWn from the standard Gaussian distribution.
We start With a brief revieW of the related literature. The NN parameter estimation has been con-
sidered under both supervised and unsupervised learning frameWorks. The estimation rely on the
stochastic gradient descent (SGD)-based algorithms (Goel et al. (2018); Allen-Zhu et al. (2019);
Chen et al. (2020); Oymak (2019); Mazumdar & RaWat (2018); Wu et al. (2019); Lei et al. (2020));
or the gradient descent (GD)-based approach (Cao & Gu (2019); Du et al. (2019)). A major short-
coming of these Works is that they assume that the available samples are not corrupted by noise
or outliers. Several Works in the literature have addressed the problem of corrupted samples in the
1
Under review as a conference paper at ICLR 2022
context of supervised learning. The existing works have considered the learning of the parameters
of a NN with ReLU activation using both SGD-based (Bakshi et al. (2019); Goel et al. (2019);
Mukherjee & Muthukumar (2020)) and GD-based algorithms(Zhang et al. (2019); Frei et al. (2020);
Vempala & Wilmes (2019)). However, to the best of our knowledge, the estimation of a NN in the
presence of noise or outliers in the unsupervised learning framework has not been studied in the
literature.
Our setting of unsupervised parameter estimation is also related to the area of robust statistics as
described below. We recall that the goal of our paper is to estimate the parameters of a NN us-
ing corrupted output samples assuming that the input distribution is Gaussian. Mathematically, this
problem is equivalent to estimating the parameters of a truncated Gaussian distribution (see Sec-
tion 2 for details). The area of robust statistics also deals with the estimation of high dimensional
distributions like Gaussian, Gaussian product, and Gaussian mixture distributions where a constant
fraction of the samples were corrupted by noise (Huber (2004); Hampel et al. (2011); Lai et al.
(2016); Diakonikolas et al. (2019); Diakonikolas & Kane (2019); Kane (2021)). However, in this
paper, we consider parameter estimation from truncated Gaussian samples in the adversarial setting
which has not been studied in the literature.
We present our major contributions as the following.
•	Parameter Estimation (Sections 2 and 3): We estimate the parameters (weight matrix and bias
vector) of a single-layer NN with ReLU in the unsupervised learning framework where each
output sample can potentially be an arbitrary outlier with a fixed probability. We propose an
estimation algorithm that has two steps: (i) we estimate the row norms of the weight matrix and
the bias vector using GD along with either the median or trimmed mean-based filters to mitigate
the effect of arbitrary outliers, and (ii) we estimate the angles between any two row vectors of
the weight matrix using a simple geometric result (Williamson & Shmoys, 2011, Lemma 6.7).
•	Theoretical guarantees (Section 4): We show that the proposed algorithm requires Ω(表 log d)
samples to estimate the network parameters within an error of with probability 1 - δ when the
probability of a sample being uncorrupted is p. Here, > 0 represents the estimation error.
•	Empirical validation (Section 5): We evaluate the performance of our algorithm empirically in
terms of its relation to the probability of a sample being uncorrupted, number of samples, and
dimension (see Figs. 1 and 2). We see that our proposed filtering schemes ensure robustness
to the arbitrary outliers. Also, the performance of our algorithm improves with the increase in
the probability of a sample being uncorrupted and the number of samples, as expected. Further,
we note that the estimation error increases slowly as the dimension grows for a fixed number of
samples in the presence of arbitrary outliers. These observations from the empirical results are
consistent with our theoretical results.
To summarize, in this paper, we estimate the parameters of a single-layer NN with ReLU activation
in the presence of arbitrary outliers. The results obtained in this paper provide insights to generalize
to a multi-layer NN with various activation functions and obtain generalization bounds.
2	Learning a Neural Network Model using Corrupted Output
Samples
We consider a single-layer ReLU NN. Let the weight matrix of the NN be denoted by W ∈ Rd×k
and the bias vector by b ∈ Rd. The input to the NN is denoted by the latent variable z ∈ Rk .
We assume that the variable z is drawn from the standard Gaussian distribution. The Gaussianity
assumption is for mathematical tractability, and it is motivated by a popular generative approach to
estimate a high-dimensional distribution from observed samples in the case of image generation by
GANs Goodfellow et al. (2014); Radford et al. (2015); Arjovsky et al. (2017). Thus, the output of
the network is the random vector x ∈ Rd given by
X = ReLU(Wz + b), where Z ~ N(0, Ik),	(1)
where N(0, Ik) denotes the Gaussian distribution with mean 0 and covariance matrix Ik which is
the identity matrix of size k × k. Let the distribution of the random vector X be denoted by D(W, b).
Then, our goal is to estimate the unknown parameters W and b of the distribution D(W, b) using
2
Under review as a conference paper at ICLR 2022
the knowledge of n output samples x1 , x2 , . . . , xn . Here, we assume that a fraction of the samples
are arbitrary outliers. The distribution of the observed samples are modeled using the Huber’s p-
contamination model 1 defined as the following.
Definition 2.1 (Huber’s p-Contamination Model Huber (1964)). The observed samples are said to
be following Huber’s contamination model if any given sample is drawn from the true distribution
D and an arbitrary distribution G with probability p, and 1 - p, respectively. In other words, the
observed samples are drawn from the mixture distribution, Dp(W, b) given by
Dp(W, b) =pD(W,b)+(1-p)G.	(2)
Note that if b has large negative values, then most of the output samples are zeros due to the ReLU
operation. Further, if W is a zero matrix, then any negative coordinate of b cannot be identified as
it is reset to zero after the ReLU operation. Further, in Wu et al. (2019), the authors have shown that
if b ∈ Rd then exponentially large number of samples are required to estimate the bias. This holds
true in our case as in the presence of arbitrary outliers if b has large negative values, then we would
require larger number of samples to estimate the bias vector b compared to that in Wu et al. (2019).
Hence, we also assume b to be non-negative.
Our estimation problem is challenging due to two reasons: 1) ReLU operation is not invertible, and
therefore, estimation of W and b utilizing maximum likelihood of the probability density function
of the random vector x is intractable; 2) the distribution of the corrupted samples is unknown. We
tackle these issues using a new formulation combining the GD algorithm and a filtering technique.
Note that the GD algorithm is similar to that in Wu et al. (2019) wherein the authors use the SGD
algorithm. The proposed algorithm is presented in the next section.
3	Estimation of Parameters of Single-layer NN
To design our estimation algorithm, we first note that the weight matrix W may not be identifiable
from the distribution D(W, b). This is because of the fact that for any matrices W1 and W2 if
WIWT = W2Wτ then there exists a unitary matrix Q such that W2 = WιQ. As Z 〜N(0, Ik),
We have Qz 〜N(0, Ik). Hence, for any vector b, We have D(Wι, b) = D(W2, b) (WU et al.
(2019)). Since our goal is to learn the distribution, learning either W1 or W2 is sufficient. In
short, We focus on the learnability of the underlying distribution and not the learnability of the NN
parameters. Therefore, our proposed algorithm estimates WWT ∈ Rd×k and b ∈ Rd from the
observed samples from Dp(W, b).
Next, We note that (i, j)-th entry WWT (i, j) of the symmetric matrix WWT is kW(i, :
)k2 kW(j, :)k2 cos(θij), Where θij = θji is the angle betWeen vectors W(i, :) and W(j, :) Which
are the i-th and j -th roWs of matrix W, respectively. Also, note that the `p norm of a vector is
given by kxkp = (Pi |x(i)|p)1/p. Thus, We can construct the matrix WWT using the roW norms
{kW(i, :)k2}id=1 and angles {θij}id,j=1.
Our estimation algorithm consists of tWo steps: 1) estimation of roW norms ofW and bias vector b;
and 2) estimation of angles betWeen the roW vectors of W.
To estimate ∣∣W(i,:并2 ∈ R and b(i) ∈ R, for i ∈ [d] = {1,2,..., d}, suppose X 〜D(W, b). Its
i-th coordinate can be Written as
X(i) = ReLU(W(i, :)Tz + b(i)),	(3)
where W(i,:)Tz + b(i)〜N(b(i), ∣W(i, ：)|2). Thus, to compute b(i) and ∣∣W(i, :)|2, it is
enough to consider the i-th entries of the observed samples. Also, in (3), the ReLU operator sets
x(i) to zero if W(i,:)Tz + b(i) is negative. Consequently, we estimate b(i) and ∣∣W(i, :)k2 using
the positive entries of the observed samples. Hence, estimation of b(i) and ∣∣W(i,:州2 is equivalent
to estimating the parameters of a one-dimensional normal distribution using positive samples, i.e.,
the samples that belong to the set R>o := {x(i) ∈ R : x(i) > 0}. We use μ* = b(i) and σ*2 =
∣∣W(i, :)12 to denote the parameters of this one-dimensional normal distribution. The estimation of
μ* and σ*2 using corrupted samples is discussed next.
1Our algorithm and its analysis follow even if the outliers are not sampled from the same arbitrary distribu-
tions G . However, we need to assume that all the true samples come from the same distribution D.
3
Under review as a conference paper at ICLR 2022
1
2
3
4
5
6
7
8
9
Algorithm 1: Learning one-layer ReLU NN with outliers	Algorithm 2: ProjGD	
	一 Input: Samples X>o, Parameters T,γt, and n	
Input: Samples X1, . . . , Xn ∈ Rd for i ∈ [d] do	ι Compute {xj (i)j= ι from X>o using (7)	
X>0 - {Xm, m ∈ [n]: Xm(i) > 0}	2 g	x J filter {Xj (i)}jn=b 1
V = PrOjGD(X>o)	3 V0 J [0 0]	
ς i,i — 1八(1)	4 for t = 1, 2, . . . , T do	
b(i) J max{0, V(2)∕V(1)}	5	μ J vt-i(2)∕vt-i(1)
for i < j ∈ [d] do	6	σ2 J 1∕Vt-1(1)
Compute θij using (11)	7	T gz J [-(σ2 + μ2)∕2 μ] using (5) and (6)
Σ(i,j) J √∑(i,i)∑(j,j)cos(θij)	8 9	gt J gx + gz Vt J Vt-1 - Yt-Igt-I
L ∑(j,i) j ∑(i,j)	10	_ Vt J P(Vt) using (10)
Output: Σ ∈ Rd×d, b ∈ Rd	Output: VT ∈ R2	
We determine the parameters of the univariate normal distribution using maximum likelihood es-
timation (Daskalakis et al. (2018)) given by argminv '(v) where '(v) is the expected negative
log-likelihood with respect to x(i) and V = [1∕σ2; μ∕σ2] ∈ R2. This optimization problem can
be solved using a learning algorithm like GD or SGD. SGD utilizes only one sample for the gradi-
ent computation which introduces large variance due to the stochasticity and low reliability in the
presence of arbitrary outliers. Thus, we use GD that utilizes all the samples for the gradient.
To derive our algorithm based on GD, We compute the gradient (Daskalakis et al. (2018)), ▽'(V)=
Ex,z [g(i)] where
g(i) = gx(i) +gz(i) = x2(i)∕2 -x(i)T + -z2(i)∕2 z(i)T .	(4)
Here, x(i)〜 N(μ*,σ*2;R>o) and z(i)〜 N(μ,σ2; R>o). Also, μ* and σ*2 are the parameters
of the true distribution, and μ and σ2 are functions of v. In (4), Ez[gz(i)] admits a closed form
expression (Johnson et al. (1995)), gz = Ez[gz(i)] = [-(σ2 + μ2)∕2 μ]T. Here, we define
μ = E[z(i)|0 < z(i) < ∞]= μ + 1 φΦ(-μ∕σ)σ,
σ2 = Var(z(i)∣0 < z(i) < ∞) = σ2
φ(-μW V
1 - μ φ(-μ∣σ)
σ 1 — Φ(-μ∕σ)	∖l — Φ(-μ∕σ)
(5)
(6)
—
where φ(∙) and Φ(∙) denote the probability density function and the cumulative distribution function
of the standard normal distribution, respectively. We also need Ex[gx(i)] to compute Ex,z [g(i)].
Since the value ofEx[gx(i)] is not known, we use the sample mean to estimate Ex[gx(i)]. For this,
all the positive samples are partitioned into nb batches. Let Sj be the j -th batch. We then compute
the vector Xj(i) ∈ R2 as follows:
Xj ⑴=jS^∣ [Pχ(i)∈Sj X⑴2 A	- Pχ(i)∈Sj X⑴].	⑺
We then combine the n vectors {xj- (i)j= ι to estimate Eχ,z[g(i)]. Recall that the observed samples
are from the mixture distribution Dp(W, b). Therefore, to compute the expectation with respect to
the true distribution D(W, b), a filter is applied on this set ofnb vectors to mitigate the effect of the
arbitrary outliers which is a common tool in the robust statistics literature and obtain Ex[gx(i)]. We
consider two filters: median and trimmed mean.
Median: The median based filter possesses the following robustness property. Typically, a median
is the value separating the higher half from the lower half ofa given set of points. If more than half
of a given set of points are in [-M, M] for some M > 0, then their median must be in [-M, M].
Thus, the median of {xj(i)}j== ι is the vector computed either from all the true samples or outliers
whose magnitudes are comparable to the true samples.
Trimmed Mean: The trimmed mean removes the vectors among the set of nb vectors with relatively
large and small values and computes the estimate ofEx[gx(i)] as the mean of the remaining vectors.
4
Under review as a conference paper at ICLR 2022
Here, We use the parameter β to indicate the number of vectors to be discarded. This technique
prunes the vectors Xj (i), j ∈ [nb] that are computed from the batches with outliers having relatively
high or loW magnitudes. Therefore, We obtain the gradient estimate as
g(i) = gχ(i) + gz(i) = filter ({xj⑶}；) + [(σ2 + μ2)∕2 μ]T ,	(8)
where filter is either median or trimmed mean, and μ and σ2 are given in (5) and (6), respectively.
Having computed the gradient estimate, we next present the proposed GD algorithm. In the t-th
iteration, the algorithm performs the following three steps: The first step is Gradient Computation
Step where the gradient gt ∈ R2 is computed from the observed positive samples using (8). The
second step is Update Step where the gradient computed in the previous step is used to perform the
GD update:
Vt = vt-ι - Yt-ιgt-ι,	(9)
where γt-ι > 0 is the diminishing step size. The last step is Projection Step where the objective
function l(v) is a strongly convex function of v, if V belongs to a bounded region. To control the
strong-convexity of the objective function, we project the update vector v into the domain Dr =
{v ∈ R2 : 1/r ≤ v(1) ≤ r, ∣v(2)∣ ≤ r}. Thus, the projection is
P(V) = [min{max{V(1), 1/r}, r} min{max{V(2), -r}, r}] .	(10)
The GD algorithm is run for T iterations where T is the algorithm parameter. The overall algorithm
is summarized in Algorithm 2. This completes the first step of our algorithm that uses the GD
algorithm to obtain the estimate of the bias vector and the row norms of the weight matrix.
Finally, using the estimates b and Σ obtained using the GD algorithm, we estimate θij similar to Wu
et al. (2019) using (Williamson & Shmoys, 2011, Lemma 6.7). Specifically, we have
1n
θij = ∏ - 2∏ ( n £ l(Xm(i) > b(i))I(Xmj) > b(j)) I ,	(11)
m=1
1	-∏ / ∖ 1	.	.1 ∙ 1∙ . i'	. ∙	1 r ♦ ,1	. ∙	. i' .Λ Λ '	.	. 1 ∙ .1
where 1(∙) denotes the indicator function and b is the estimate of the bias vector computed in the
previous step using the projected GD algorithm.
The overall algorithm is given in Algorithm 1 where Σ is the estimate of WWt. Here, Steps 1-5
estimate the row norms ofW and b, and Steps 6-9 estimate the angles between any two rows ofW.
Our algorithm is similar to that in Wu et al. (2019) except for the filter which is also a standard
technique in robust statistics. However, our algorithm is different from the algorithm in Wu et al.
(2019) in the following aspects. We do not rescale the unknown parameters (see Step 1 of Algorithm
2 in Wu et al. (2019)). We employ median or trimmed mean based filter to compute the full gradient
and utilize the GD algorithm instead of SGD used in Wu et al. (2019). We note that our main
contribution is the analysis rather than the algorithm which is presented in the next section.
4	Theoretical Guarantees for Parameter Estimation
In this section, we provide our main result which characterizes the sample complexity for robust
estimation ofNN parameters. We address the general problem of learning the distribution generated
by a NN in the presence of arbitrary outliers (p ≤ 1). Hence, the proof techniques which have been
devised for only the special case ofp = 1 in Wu et al. (2019), are not applicable to our setup due
to the presence of corrupted samples. Therefore, we develop novel proof techniques for performing
the parameter estimation in the presence of arbitrary outliers. To arrive at the main result, we first
present the error bounds for the two steps of Algorithm 1, which are described in Sec. 3.
The following proposition provides the estimation error bounds for the first step, i.e., the estimation
of the bias vector b and the norms of the row vectors of WWT . The proof relies on the fact that
the objective function is η-strongly-convex and L-smooth, and we show that the parameters η, L are
functions of r. Using the properties of the objective function, we the bound on the error between the
true gradient Vl(v) and the estimated gradient gt computed from the output samples with arbitrary
outliers, obtained in terms of parameter . This bound leads to the following result that shows that
5
Under review as a conference paper at ICLR 2022
we can estimate b(i) within an additive error of ΞkW(i, :)k and Σ(i, i) within an additive error of
ΞkW(i, :)k2 where Ξ is a function of the above parameters.
Proposition 4.1. Suppose we initialize the projected gradient step (Algorithm 2) such that kv0 -
v*k = V and choose the Step size Yt = Y and the number of iterations as T. Then, there exists
Y > 0 such that for any δ, ∈ (0, 1), the output (Σ(i, i), b(i)) of the projected GD step of our
algorithm with median-based filter for any i ∈ [d] satisfies
∣Σ(i, i) -kW(i, ：)k2| ≤ Ξ∣∣W(i, :)k2 and |b(i) - b(i)∣ ≤ Ξ∣∣W(i, :)k,	(12)
with probability at least 1 一 δ if the number of data samples n ≥ ?2他田—1/2)2 log 1. Here, the
error bound is Ξ = V(1 -n^) +	LnIrn
with η, L > 0 being constants that depend only on
the parameter r. Also, p is the probability ofa sample being uncorrupted and Ψ ∈ [1/2p, 1] is such
that O(e3) ≤ Ψe ≤ O(√e) with e > O (p-1∕3) where the order constants depend only on r.
The two terms in the error bound Ξ in Proposition 4.1 can be interpreted as the following. The
first term V(1 - r+⅛) captures the convergence error due to the GD algorithm. For any upper
bound on the convergence error Ω > 0, Algorithm 1 runs in time (log Ω — log V)/ log (r+⅛). This
relation shows that smaller convergence errors require more iterations. The second term 2"彳：#
captures the error due to the gradient computed using the observed samples. Two factors contribute
to this error: 1) difference between the (true) sample mean and the true mean, and 2) the arbitrary
outliers. Also, the difference between the sample mean and true mean decreases with the number of
samples. We notice that the sample complexity depends on Ψe which is the probability of a sample
being uncorrupted deviating from its mean and variance by E and as Ψe ≤ O(√e). Consequently,
for smaller error e, We need a large number of samples n = Ω(* log *). We also note that the
lower bound on E decreases with p.
The following proposition bounds the estimation error in the second step, i.e., the estimation of the
angle between any two row vectors W(i, :) and W(j, :) (where i 6= j ∈ [d]). The result shows that
we can estimate θij within an additive error of Ξ.
Proposition 4.2. Assume that (12) holds. Then, for a fixed pair ofi 6= j ∈ [d] and δ, E ∈ (0, 1), the
estimate θij of Algorithm 1 that uses a median-based filter satisfies
| cos θij — cos θij | < √2πp(pΨe — 1/2) + 3π(1 — p) + 2π(2 — p)Ξ,	(13)
with probability at least 1 — δ if the number of data samples n ≥ ?2他田—1/2产 log 1. Here, the
error bound is ξ = V (1 — rLL)T + MLnn
with η, L > 0 being constants that depend only on
the parameter r. Also, p is the probability ofa sample being uncorrupted and Ψ ∈ [1/2p, 1] is such
that O(e3) ≤ Ψe ≤ O(√e) with e > O (p-1/3) where the order constants depend only on r.
Combining Propositions 4.1 and 4.2, we next present the sample complexity needed to achieve a
small parameter estimation error. Specifically, the diagonal entries, Σ(i, i), for i ∈ [d] are obtained
from Proposition 4.1. Further, the off-diagonal entries of Σ are obtained by combining the Proposi-
tions 4.1 and 4.2 to obtain the final result as follows:
Theorem 4.3. Suppose we initialize the projected gradient step (Algorithm 2 with median-based
filter) such that ∣∣vo — v*k = V and choose the Step size Yt = Y and the number ofiterations as T.
Then, there exists Y > 0 such that for δ, E ∈ (0, 1), the output of Algorithm 1, (Σ, b) satisfies
k∑ — WWTkF ≤ (√2πp(pΨe — 1/2)+ π(2 — p)(3 + 2Ξ)) ∣W∣F	(14)
..ʌ
kb - bk≤ Ξ∣W∣f,	(15)
with probability at least 1 — δ if the number of data samples n = Ω(表 log d). Here, P is the
probability of a sample being uncorrupted and Ψe ∈ [1/2p, 1] is such that O(e3) ≤ Ψe ≤ O(√e)
with E > O p—1/3 where the order constants depend only on r.
6
Under review as a conference paper at ICLR 2022
Table 1: Runtime of various schemes when p = 0.95, n = 20000, and d = 5. The table indicates that SGD
schemes are faster than their GD counterparts.
Scheme	Oracle	WithoutFilter	With Median	With Trimmed Mean
-GD-	16.95 S	1773S	34.44 s	60.78 S
SGD	1.24 s	1.60 s —	2.11s —	3.62 s
We note that in Propositions 4.1 and 4.2, and Theorem 4.3, η, L and the other order constants depend
on the parameter r > 0. Recall that the parameter r is required for the convexity region Dr and we
can choose r to be a large constant. Thus, r is a relatively insensitive parameter which does not
require any hand-tuning.
As the probability of a sample being uncorrupted p reduces, we observe that the error bound in (14)
becomes larger as expected. Also, as the probability of a sample being uncorrupted decreases, the
number of samples n required for the error bound increases. Moreover, the obtained error bounds
increase with the problem dimension for a fixed number of samples.
Our results are also comparable with the existing error bounds for ReLU parameter estimation
without outliers from (Wu et al., 2019, Theorem 1). Specifically, if there are no outliers, for any
Z, δ ∈ (0,1), the number of samples n = O(吉 log d) are sufficient for the SGD algorithm in WU
et al. (2019) to achieve the following error bounds with probability at least 1 - 2δ,
k∑ - WWTkF ≤ ZIWkF and kb - bk ≤ Z∣W∣∣f,	(16)
where ζ indicates the parameter estimation error. Comparing the above bounds with (14) and (15),
we observe that our bounds are larger due to the presence of arbitrary outliers. In particular, Ξ
T
consists of two error terms. The first term V (1 一 ^^) < 1 in our error bounds results from the
convergence of the GD algorithm and is similar to Z < 1 in (16). However, the second error term
2"Ljη) quantifies the error due to the gradient estimated from the output samples with arbitrary
outliers and is not present in (16). Additionally, in (14), the extra term 3π(2 - p) further increases
the bound compared to the error bound in (16). Therefore, the error Z in (16) can not be directly
compared to the error in (14) and (15). Further, our sample complexity depends on the probability
of a sample being uncorrupted p and the second error term whereas the complexity in (Wu et al.,
2019, Theorem 1) depends on the square of the convergence error term Z in their sample complexity.
Also, when there is no outlier (p = 1), the two sample complexities do not match.
5	Simulation Results
In this section, we provide numerical results to verify the performance of our algorithm. In our
setup, the columns ofW are chosen as the left singular vectors of random matrices from the standard
Gaussian distribution. For b, we use a random vector from the standard normal distribution whose
negative values are replaced with zeros. The mixture of samples are generated such that a sample
comes from D(W, b) with probability p and from G = N(5, 1) with probability 1 - p. The hyper-
parameters in our proposed algorithm are set as r = 3 and Yt = 含. We use the batch-splitting
approach to compute the gradient. This approach induces randomization. As the output samples
consist of a fraction of arbitrary outliers, the standard gradient descent algorithm without any filter
would perform poorly as shown in our results. Also, we choose the total number of GD and SGD
iterations T to be |X>0|/100 where |X>0| is the number of positive output samples. From our
experiments, we observe that the errors first decrease with the number of iterations and then after a
certain number of iterations, the errors flatten. Based on this observation, we chose the number of
iterations (see Appendix A.4). Note that we discard the zero entries and only consider only the set
of positive samples X>0 for the number of iterations as they do not convey any information about
the row norms of W and bias vector b.
We compute two error metrics from the estimated parameters and the ground truth, ∣∣Σ -
WWTkF /11 WlIF and ∣∣b 一 b∣∣2/IlWkf . Also, we compare our algorithm with two other schemes:
oracle scheme (estimation using the true samples only), and scheme without filter.
The results are shown in Figs. 1 and 2, and Table 1, and the observations from them are as follows.
7
Under review as a conference paper at ICLR 2022
(a) d = 5 and n = 20000.
X≡"E≡6-8m U- ∙!0t
10-2
0	0.5	1	1.5	:
No.of samples (n) xιo5
(b) p = 0.95 and d = 5.
(c) p = 0.95 and n = 20000.
^^GD w/o Filter -^GD WitH Median H^GD WitH Trimmed Mean ^^OraCle GD
Figure 1:	Comparison of the different GD schemes as a function of p (first column), n (second column), and
d (third column). The figures indicate that utilizing the filters improves the performance of GD algorithm for
mixture of samples.
0.4
0 0.3
o
，工，
>
S
W 0.2
⊂
o
击0.1
0
0	0.5	1	1.5	2
Probability of true samples (P)
(a) d = 5 and n = 20000.
X≡"E≡-8M U- ∙!0∙l.!
(b) p = 0.95 and d = 5.
(c) p = 0.95 and n = 20000.
-M-SGD with Median -|-GD with Median -B-OraCIe SGD -^-Gracie GD
Figure 2:	Comparison of GD and SGD schemes as a function of p (first column), n (second column), and d
(third column). We infer that SGD is more sensitive to corrupted samples compared to GD.
Effect of filters: From Fig. 2, we observe that the GD schemes perform better than the corresponding
SGD schemes. Further, the performance is improved when we use median based filter along with
GD or SGD (for instance, as seen by comparing the performances ofGD without Filter and GD with
Median in Fig. 1). Thus, the filter ensures that the effect of the outliers is reduced, and the curves are
closer to the Oracle GD scheme. Finally, from Fig. 1, we also infer that the median-based approach
performs slightly better than the trimmed mean-based approach.
Dependence on probability ofa sample being uncorrupted p: The variation in the error estimation as
a function ofp is shown in Figs. 1a and 2a. The performance of the proposed schemes improve with
p, as expected. This trend is because as p increases, the fraction of outliers in the observed samples
8
Under review as a conference paper at ICLR 2022
decreases, which leads to better performance. Note that the oracle schemes assume the knowledge
of true samples. Thus, their performance does not change with p. On the contrary, the performance
of the schemes without filters and SGD schemes with filter do not have a monotonic behavior with
p. This observation shows that these schemes are not able to handle the outliers effectively. Further,
all the schemes converge to the corresponding oracle schemes when p = 1.
Dependence on number of samples n: Figs. 1b and 2b show how the estimation error of different
schemes varies with n. We observe that the parameter estimation errors computed using Oracle
SGD and Oracle GD schemes decrease as the number of samples increases. This trend is because
the oracle schemes consider only the true samples. Also, the Oracle GD scheme performs better
than the Oracle SGD scheme. Further, the estimation errors computed using our proposed schemes
with median and trimmed mean based filters perform better than GD without filter. Hence, the filters
mitigate the effect of arbitrary outliers and having more samples reduces the estimation errors.
Dependence on dimension d: In Figs. 1c and 2c, we plot the error in different schemes as a function
of the dimension d. We note that due to the presence of arbitrary outliers, the errors increase as
the dimension increases for GD without filter. Further, we observe a performance improvement
using our proposed schemes with median and trimmed mean-based filters compared to the other
benchmarking schemes, supporting our previous conclusions that the filters mitigate the effect of
outliers. However, we observe a slow increase in the parameter estimation errors using our proposed
schemes with median and trimmed mean based filters as we increase the dimension for fixed number
of samples. We observe that the errors when using the oracle schemes also increase slowly as the
dimension is increased (Wu et al. (2019)).
Comparison of runtimes: From Table 1, we observe that the SGD schemes run faster than their GD
counterparts. This is because the SGD utilizes only one sample for the gradient whereas GD utilizes
all the samples for the gradient. Further, the usage of median and trimmed mean based filters along
with SGD orGD increases their computation times. However, we reiterate that using the GD scheme
with filters offers the best performance in terms of error (see Fig. 2). Further, the runtimes of the
trimmed mean-based schemes are significantly higher than those of the median-based schemes.
To summarize, from our results, we conclude that GD schemes offer better performance than the
SGD schemes. Also, we deduce that compared to the trimmed mean-based scheme, the median-
based filter is computationally more efficient while offering similar or lower estimation error. More-
over, the median-based scheme is parameter free (the trimmed mean depends on the parameter β)
and enjoys strong theoretical guarantees (see Theorem 4.3). Overall, the median-based GD algo-
rithm is the most effective approach to estimate the NN parameters in the presence of the outliers
and performs slightly better than trimmed mean-based GD algorithm. Further, we observe that GD
schemes with filters perform better than Oracle SGD scheme.
6 Conclusion
In this paper, we proposed an algorithm for estimation of the parameters of a single-layer ReLU
neural network from the truncated Gaussian samples where each sample was assumed to be an
arbitrary outlier with a fixed probability. Our only assumption was that the bias vector was non-
negative. We derived a GD-based algorithm for parameter estimation and incorporated filters to
handle the outliers. We analyzed the sample complexity of the proposed algorithm in terms of the
parameter estimation error. The efficacy of our approach was also demonstrated using numerical
experiments. Removing the non-negativity assumption on the bias vector is a promising direction
for future work. Also, extending our results to multi-layer neural networks is also an interesting
problem to consider in the future.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252. PMLR, 2019.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In International Conference on Machine Learning, pp. 214-223. PMLR, 2017.
9
Under review as a conference paper at ICLR 2022
Ainesh Bakshi, Rajesh Jayaram, and David P Woodruff. Learning two layer rectified neural networks
in polynomial time. In Conference on Learning Theory, pp. 195-268. PMLR, 2019.
Yuan Cao and Quanquan Gu. Tight sample complexity of learning one-hidden-layer convolutional
neural networks. arXiv preprint arXiv:1911.05059, 2019.
Sitan Chen, Adam R Klivans, and Raghu Meka. Learning deep ReLU networks is fixed-parameter
tractable. arXiv preprint arXiv:2009.13512, 2020.
Constantinos Daskalakis, Themis Gouleakis, Chistos Tzamos, and Manolis Zampetakis. Efficient
statistics, in high dimensions, from truncated samples. In 2018 IEEE 59th Annual Symposium on
Foundations of Computer Science (FOCS), pp. 639-649. IEEE, 2018.
Ilias Diakonikolas and Daniel M Kane. Recent advances in algorithmic high-dimensional robust
statistics. arXiv preprint arXiv:1911.05911, 2019.
Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Ankur Moitra, and Alistair Stewart.
Robust estimators in high-dimensions without the computational intractability. SIAM Journal on
Computing, 48(2):742-864, 2019.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-
1685. PMLR, 2019.
Spencer Frei, Yuan Cao, and Quanquan Gu. Agnostic learning of a single neuron with gradient
descent. arXiv preprint arXiv:2005.14426, 2020.
Surbhi Goel, Adam Klivans, and Raghu Meka. Learning one convolutional layer with overlapping
patches. In International Conference on Machine Learning, pp. 1783-1791. PMLR, 2018.
Surbhi Goel, Sushrut Karmalkar, and Adam Klivans. Time/accuracy tradeoffs for learning a ReLU
with respect to gaussian marginals. arXiv preprint arXiv:1911.01462, 2019.
Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint
arXiv:1406.2661, 2014.
Frank R Hampel, Elvezio M Ronchetti, Peter J Rousseeuw, and Werner A Stahel. Robust statistics:
The approach based on influence functions, volume 196. John Wiley & Sons, 2011.
Peter J. Huber. Robust Estimation of a Location Parameter. The Annals of Mathematical Statistics,
35(1):73 - 101, 1964.
Peter J Huber. Robust statistics, volume 523. John Wiley & Sons, 2004.
Norman L Johnson, Samuel Kotz, and Narayanaswamy Balakrishnan. Continuous univariate distri-
butions, volume 1, volume 289. John wiley & sons, 1995.
Daniel M Kane. Robust learning of mixtures of Gaussians. In Proceedings of the 2021 ACM-SIAM
Symposium on Discrete Algorithms (SODA), pp. 1246-1258. SIAM, 2021.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint
arXiv:1312.6114, 2013.
Kevin A. Lai, Anup B. Rao, and Santosh Vempala. Agnostic estimation of mean and covariance. In
2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS), pp. 665-674,
2016. doi: 10.1109/FOCS.2016.76.
Qi Lei, Jason Lee, Alex Dimakis, and Constantinos Daskalakis. SGD learns one-layer networks in
WGANs. In International Conference on Machine Learning, pp. 5799-5808. PMLR, 2020.
Arya Mazumdar and Ankit Singh Rawat. Representation learning and recovery in the ReLU model.
arXiv preprint arXiv:1803.04304, 2018.
10
Under review as a conference paper at ICLR 2022
Anirbit Mukherjee and Ramchandran Muthukumar. Guarantees on learning depth-2 neural networks
under a data-poisoning attack. arXiv preprint arXiv:2005.01699, 2020.
Samet Oymak. Stochastic gradient descent learns state equations with nonlinear activations. In
Conference on Learning Theory, pp. 2551-2579. PMLR, 2019.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Aaron Van Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In
International Conference on Machine Learning, pp. 1747-1756. PMLR, 2016.
Santosh Vempala and John Wilmes. Gradient descent for one-hidden-layer neural networks: Poly-
nomial convergence and SQ lower bounds. In Conference on Learning Theory, pp. 3115-3117.
PMLR, 2019.
David P Williamson and David B Shmoys. The design of approximation algorithms. Cambridge
university press, 2011.
Shanshan Wu, Alexandros G Dimakis, and Sujay Sanghavi. Learning distributions generated by
one-layer ReLU networks. Advances in Neural Information Processing Systems, 32:8107-8117,
2019.
Xiao Zhang, Yaodong Yu, Lingxiao Wang, and Quanquan Gu. Learning one-hidden-layer ReLU
networks via gradient descent. In The 22nd International Conference on Artificial Intelligence
and Statistics, pp. 1524-1534. PMLR, 2019.
11