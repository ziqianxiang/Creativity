Under review as a conference paper at ICLR 2022
VoiceFixer: Toward General Speech Restora-
tion With Neural Vocoder
Anonymous authors
Paper under double-blind review
Ab stract
Speech restoration aims to remove distortions in speech signals. Prior methods
mainly focus on single-task speech restoration (SSR), such as speech denoising
or speech declipping. However, SSR systems do not address the general speech
restoration problem and show limited performance in some speech restoration
tasks such as speech super-resolution. To overcome those limitations, we pro-
pose a general speech restoration (GSR) task that attempts to remove multiple
distortions simultaneously. Furthermore, we propose VoiceFixer1, a generative
framework to address the GSR task. VoiceFixer consists of an analysis stage and
a synthesis stage to mimic the speech analysis and comprehension of the human
auditory system. We employ a ResUNet to model the analysis stage and a neu-
ral vocoder to model the synthesis stage. We evaluate VoiceFixer with additive
noise, room reverberation, low-resolution, and clipping distortions. Our baseline
GSR model achieves a 0.499 higher mean opinion score (MOS) than the speech
denoising SSR model. VoiceFixer further surpasses the GSR baseline model on
the MOS score by 0.256. Moreover, we observe that VoiceFixer generalizes well
to severely degraded real speech recordings, indicating its potential in restoring
old movies and historical speeches.
1	Introduction
Speech restoration is a process to restore degraded speech signals to high-quality speech signals.
Speech restoration is an important research topic since distortions are ubiquitous. For example,
speech is usually surrounded by background noise, blurred by room reverberations, or recorded by
low-quality devices (Godsill et al., 2002). Those distortions degrade the perceptual quality of speech
for human listeners. Speech restoration has a wide range of applications such as online meeting (De-
fossez et al., 2020), hearing aids (Van den Bogaert et al., 2009), and audio editting (Van Winkle,
2008). Still, speech restoration remains a challenging problem due to the large variety of distortions
in the world.
Previous works in speech restoration mainly focus on single task speech restoration (SSR), which
deals with only one type of distortion at a time. For example, speech denoising (Loizou, 2007),
speech dereveberation (Naylor & Gaubitch, 2010), speech super-resolution (Kuleshov et al., 2017),
or speech declipping (Zaviska et al., 2020). However, in the real world, speech signal can be de-
graded by several different distortions simultaneously, which means previous SSR systems over-
simplify the speech distortion types (Kashani et al., 2019; Lin et al., 2021; Kuleshov et al., 2017;
Birnbaum et al., 2019). The mismatch between the training data used in SSR and the testing data
from the real world degrades the speech restoration performance.
To address the mismatch problem, we propose a new task called general speech restoration (GSR),
which aims at restoring multiple distortions in a single model. A numerous studies (Cutler et al.,
2021; Cauchi et al., 2014; Han et al., 2015) have reported the benefits of jointly training multi-
ple speech restoration tasks. Nevertheless, performing GSR using one-stage systems still suffer
from the problems in each SSR task. For example, on generative tasks such as speech super-
resolution (Kuleshov et al., 2017), one-stage models tend to overfitting the filter (Sulun & Davies,
2020) used during training. Based on these observations, we propose a two-stage system called
VoiceFixer.
1Restoration samples: https://anonymous20211004.github.io/demo-vf/
1
Under review as a conference paper at ICLR 2022
Acoustic
Sound
Source
I love eating
donuts!
Interference from other
sound sources
Sensory
∣aι lʌ... 'i:tɪ…’d…s..s∣
Transfer function Input
What did I heard?
What did he say?
•	Multimodality information.
•	Linguistic knowledge.
•	Common senses.
Primary Auditory Cortex → PIanum Temporale (PT)
(PAC)
Initial Processing
Identification, segregation and
matching onto previously learnt
spectemporal representations
Auditory Spatial Analysis
What was it sound like? What does he imply?:
•	Speaker age, emotion, sex.
•	Acoustic environment.
•	Phonetic and other prior knowledges.
Donut is a kind of food.
Donut is delicious.
∣aι lʌv 'i:tɪŋ 'dəʊnʌtsl
Imaginery and Comprehension
High Order Cortical Areas
High Level Perception
BiIateraI Anterior STG
Left Posterior STG
Left Inferior Frontal Gyrus
...
Figure 1: The neural and cognitive model of how human brain understand and restore distorted speech.
The design of VoiceFixer is motivated by the biological mechanisms of human hearing when restor-
ing distorted speech. Intuitively, if a person tries to identify a strongly distorted voice, his/her brain
can do the recovery by utilizing both the distorted speech signal and the prior knowledge of the
language. As shown in Figure 1, the speech distortion perception is modeled by neuroscientists as
a two-stage process, including an auditory scene analysis stage (Bregman, 1994), and a high level
comprehension/synthesis stage (Griffiths & Warren, 2002). In the analysis stage, the sound infor-
mation is first transformed into acoustic features by primary auditory cortex (PAC). Then planum
temporale (PT), the cortical area posterior to the auditory cortex, acts as a computational hub by
segregating and matching the acoustic features to low level spectrotemporal representations. In
the synthesis stage, a high order cortical area is hypothesised to perform the high level perception
tasks (Griffiths & Warren, 2002; Kennedy-Higgins, 2019). Our proposed VoiceFixer systems model
the analysis stage with spectral transformations and a deep residual UNet, and the synthesis stage
with a convolutional vocoder trained using adversarial losses. One advantage of the two-stage Voice-
Fixer is that the analysis and synthesis stages can be trained separately. Two-stage methods have
also been successfully applied to the speech synthesis task (Wang et al., 2016; Ren et al., 2019)
where acoustic models and vocoders are trained separately.
VoiceFixer is the first GSR model that is able to restore a wide range of low-resolution speech
sampled from 2 kHz to 44.1 kHz, which is different from previous studies working on constant sam-
pling rates (Lim et al., 2018; Wang & Wang, 2021; Lee & Han, 2021). To the best of our knowledge,
VoiceFixer is the first model that jointly performs speech denoising, speech dereverberation, speech
super-resolution, and speech declipping in a unified model.
The rest of this paper is organized as follows. Section 2 introduces the formulations of speech
distortions. Section 3 describes the design of VoiceFixer. Section 4 discusses the evaluation results.
Appendixes introduce related works and show speech restoration demos.
2	Problem Formulations
We denote a segment of a speech signal as s ∈ RL, where L is the samples number in the segment.
We model the distortion process of the speech signal as a function d(∙). The degraded speech X ∈ RL
can be written as:
x = d(s).	(1)
Speech restoration is a task to restore high-quality speech S from x:
S = f (x)	(2)
where f (∙) is the restoration function and can be viewed as a reverse process of d(∙). The target
is to estimate S by restoring S from the observed speech x. Recently, several deep learning based
one-stage methods have been proposed to model f (∙) such as fully connected neural networks,
recurrent neural networks, and convolutional neural networks. Detailed introductions can be found
in Appendix A.
Distortion modeling is an important step to simulate distorted speech when building speech restora-
tion systems. Several previous works model distortions in a sequential order (Vincent et al., 2017;
Tan et al., 2020; Zhao et al., 2019). Similarly, we model the distortion d(∙) as a composite function:
2
Under review as a conference paper at ICLR 2022
d(x) = d1 ◦ d2 ◦ ...dQ(x), dq ∈ D, q = 1, 2, ..., Q,	(3)
where ◦ stands for function composition and Q is the number of distortions to consist d(∙). Set
D = {dv(∙)}V=ι is the set of distortion types where V is the total number of types. Equation 3
describes the procedure of compounding different distortions from D in a sequential order. We
introduce four speech distortions as follows.
Additive noise is one of the most common distortion and can be modeled by the addition between
speech s and noise n ∈ RL :
dnoise(s) = s + n.	(4)
Reverberation is caused by the reflections of signal in a room. Reverberation makes speech signals
sound distant and blurred. It can be modeled by convolving speech signals with a room impulse
response filter (RIR) r:
drev(s) = S * r,	(5)
where * stands for convolution operation.
Low-resolution distortions refer to audio recordings that are recorded in low sampling rates or
with limited bandwidth. There are many causes for low-resolution distortions. For example, when
microphones have low responses in high-frequency, or audio recordings are compressed to low
sampling rates, the high frequencies information will be lost. We follow the description in Wang &
Wang (2021) to produce low-resolution distortions but add more filter types (Sulun & Davies, 2020).
After designing a low pass filter h, we first convolve it with s to avoid the aliasing phenomenon.
Then we perform resampling on the filtered result from the original sampling rate o to a lower
sampling rate u:
dlow_res(s) = ResamPle(S * h, o, u),	(6)
Clipping distortions refer to the clipped amplitude of audio signals, which are usually caused
by low-quality microphones. Clipping can be modeled by restricting signal amplitudes within
[-η, +η]:
dclip(S) = max(min(S,η), -η),η ∈ [0, 1].	(7)
In the frequency domain, the clipping effect produces harmonic components in the high-frequency
part and degrades speech intelligibility accordingly.
3	Methodology
3.1	One-stage Speech Restoration Models
Previous deep learning based speech restoration models are usually in one stage. That is, a model
predicts restored speech S from input X directly:
f : x → S.	(8)
The mapping function f (∙) can be modeled by time domain speech restoration systems such as one-
dimensional convolutional neural networks (Luo & Mesgarani, 2019) or frequency domain systems
such as mask-based (Narayanan & Wang, 2013) methods:
S=(FsP(IX|; θ) Θ (|X| + e))ej∠x	(9)
where X is the short-time fourier transform (STFT) ofx and is a small positive constant. X has a
shape ofT × F where T is the number of frame and F is the number of frequency bins. The output of
the mask estimation function F (∙; θ) is multiplied by the magnitude spectrogram ∣ X ∣ to produce the
target spectrogram estimation S. Then, inverse short-time fourier transform (iSTFT) is applied on
S to obtain s. The one-stage speech restoration models are typically optimized by minimizing the
mean absolute error (MAE) loss between the estimated spectrogram S and the target spectrogram
S:
L ="s∣-∣s∣(	(10)
3
Under review as a conference paper at ICLR 2022
Log magnitude
spectrogram
spectrogram	Log magnitude
spectrogram
Restored Spectrogram
Skip
Connections
ConVBlock 32 - 1 ]
Low Quality
Spectrogram
Restoration MaSk
Figure 2: Overview of the proposed VoiceFixer system.
Downsampling
Skip
Connection
Transpose Conv2d 3 × 3
BatChNorm
[	ReLU ")
Stacks of
ResConv ×L2
----兄 ConCat I
Transpose Conv Block
2×2 ↑
Decoder Block	Upsampling
BatchNorm
Encoder Block 1 → 32
Decoder Block 64 → 32
EnCoder Block 32 - 64
Decoder Block 128 - 64~~)
EnCoder Block 64 - 128
DeCoder Block 256 - 128
EnCoder Block 128 - 256
Decoder Block 384 → 256
EnCoder Block 256 - 384
H Decoder Block 384 — 384 )
Encoder Block 384 → 384 ∣-----»[ Decoder Block 384 → 384 ]
Transpose Conv Block
ɪ Connect if
ch_in != ch_out X
LeakyReLU
Conv2d (ch_in,ch_out) 3 × 3	]
BatchNorm
LeakyReLU
Down Path
ConvBlock 384 → 384
Up Path
Conv2d ConvBlock
1 × 1 Jl (ch_in,ch_out)
ResConv
Conv2d (ch_out,ch_out) 3 × 3
(a) The architecture of ReSUNet
ConvBlock
U
(b) Details of the encoder and decoder blocks of
the ResUnet
Figure 3: The architecture of ResUNet, which output has the same size as input.
Previous one-stage models usually build on high-dimensional features such as time samples and the
STFT spectrograms. However, Kuo & Sloan (2005) point out that the high-dimensional features will
lead to exponential growth in search space. The model can work on the high-dimensional features
under the premise of enlarging the model capacity but may also fail in challenging tasks. Therefore,
it would be beneficial if we could build a system on more delicate low-dimensional features.
3.2 VoiceFixer
In this study, we propose VoiceFixer, a two-stage speech restoration framework. Multi-stage meth-
ods have achieved state-of-the-art performance in many speech processing tasks (Jarrett et al., 2009;
Tan et al., 2020). in speech restoration, our proposed VoiceFixer breaks the conventional one-stage
system into a two-stage system:
f : x 7→ z,	(11)
g : z → S.	(12)
Equation 11 denotes the analysis stage of VoiceFixer where a distorted speech x is mapped into a
representation z. Equation 12 denotes the synthesis stage of VoiceFixer, which synthesize z to the
restored speech S. Through the two-stage processing, VoiceFixer mimics the human perception of
speech described in Section 1.
3.2.1	Analysis S tage
The goal of the analysis stage is to predict the intermediate representation z, which can be used
later to recover the speech signal. in our study, we choose mel spectrogram as the intermediate
representation. Mel spectrogram has been widely used in many speech proceessing tasks (Shen et al.,
2018; Kong et al., 2019; Narayanan & Wang, 2013). The frequency dimension of mel spectrogram
is usually much smaller than that of STFT thus can be regarded as a way of feature dimension
4
Under review as a conference paper at ICLR 2022
F-DiSCriminator
Frequency
Discriminator
Waveform
SUbband decomposition
Energy Loss | | MeI LOSS
IWTmT^
I 2 AvgPoolingId
T-Discriminator Resl
T-Discriminator Res2
I 2 AvgPoolingId T-Discriminator Res3
I 2 AvgPoolingId T-Discriminator Res4
Multi-resolution discriminator
SUbbandI
subbαnd2
subbαnd3
T-Discriminator Subl
T-DiSCriminatOr Sub2 ]
T-Discriminator Sub3
subband4 T-Discriminator Sub4
Subband discriminator
Segment Loss
Multi
Resolution
STFT Loss
Phase Loss
Time
domain
SPeCtral
Convergence
Loss
Loss functions
Frequency
domain
M
H
M
Loss functions and discriminators
Figure 4: The architecture and training scheme of TFGAN, whose generator is later used as vocoder. The
generator takes mel spectrogram as input and upsampled it into waveform. Both output waveform and its STFT
spectrogram are used to compute loss. We employ both time and frequency discriminators for discriminative
training.
reduction. So, the objective of the analysis stage becomes to restore mel spectrograms of the target
signals. The mel spectrogram restoration process can be written as the following equation,
Smel = fmel(Xmel, α) Θ (Xmel + e),
(13)
where Xmel is the mel spectrogram of x. It is calculated by Xmel
|X | W where W is a set of
mel filter banks with shape of F × F0. The columns of W are not divided by the width of their mel
bands, i.e., area normalization, because this will make the restoration model difficult to recover the
high-frequency part. The mapping function fmel(∙; α) is the mel restoration mask estimation model
parameterized by α. The output of fmel is multiplied by Xmel to predict the target mel spectrogram.
We use ResUNet (Kong et al., 2021a) to model the analysis stage as shown in Figure 3a, which
is an improved UNet (Ronneberger et al., 2015). The ResUNet consists of several encoder and
decoder blocks. There are skip connections between encoder and decoder blocks at the same level.
As is shown in Figure 3b, both encoder and decoder block share the same structure, which is a
series of residual convolutions (ResConv). Each convolutional layer in ResConv consists ofa batch
normalization (BN), a leakyReLU activation, and a linear convolutional operation. The encoder
blocks apply average pooling for downsampling. The decoder blocks apply transpose convolution
for upsampling. In addition to ResUNet, we implement the analysis stage with fully connected deep
neural network (DNN), and bidirectional gated recurrent units (BiGRU) (Chung et al., 2014) for
comparison. The DNN consists of six fully connected layers. The BiGRU has similar structures
with DNN except for replacing the last two layers of DNN into bi-directional GRU layers.
The details of these three models are discussed in Appendix B.1. We will refer to ResUNet as UNet
later for abbreviation. We optimize the model in the analysis stage using the MAE loss between the
,♦,Fl	,	A	,	Zl
estimated mel spectrogram Smel and the target mel spectrogram Smel:
Lana
- Smel
(14)
5
Under review as a conference paper at ICLR 2022
3.2.2	Synthesis Stage
The synthesis stage is realized by a neural vocoder that synthesizes the mel spectrogram into wave-
form as denoted in Equation 15:
S = g(Xmei; β),	(15)
where g(∙; β) stands for the vocoder model parameterized by β. We employ a recently proposed non-
autoregressive model, time and frequency domain based generative adversarial network (TFGAN),
as the vocoder.
Figure 4 shows the detailed architecture of TFGAN, in which the input mel spectrogram Xmel will
first pass through a condition network CondNet, which contains N1 one-dimensional convolution
layers with exponential linear unit activations (Clevert et al., 2015). Then, in UpNet, the feature is
upsampled N2 times with ratios ofs0,s1,..., and sN2-1 using UpsampleBlock and ResStacks. Within
the UpsampleBlock, the input is first passed through a leakyReLU activation and then fed into a
sinusoidal function, which output is added to its input to remove periodic artifacts in breathing part
of speech. Then, the output is bifurcated into two branches for upsampling. One branch repeats
the samples sn times followed by a one-dimensional convolution. The other branch uses a stride
sn transpose convolution. The output of the repeat and transpose convolution branches are added
together as the output of UpsampleBlock. ResStacks module contains two dilated convolution layers
with leakyReLU activations. The exponentially growing dilation in ResStack enable the model to
capture long range dependencies. The TFGAN in our synthesis model applies N2 = 4. After four
UpsampleBlock blocks with ratios [7, 7, 3, 3], each frame of the mel spectrogram is transformed into
a sequence with 441 samples corresponding to 10 ms of audio sampled at 44.1 kHz.
The training criteria of the vocoder consist of frequency domain loss LF, time domain loss LT, and
weighted discriminator loss LD :
Lsyn = LF + LT + λDLD,	(16)
The frequency domain loss LF is the combination of a mel loss Lmel and multi-resolution spectro-
gram losses:
KF
LF(S, S) = λmelLmel(S, S) + X(λscLs<ck) (S, S) + λmagL1mkg(S, S))	(17)
k=1
where Lsc and Lmag are the spectrogram losses calculated in the linear and log scale, respectively.
There are KF different window sizes ranging from 64 to 4096 to calculate Lsc and Lmag so that the
trained vocoder is tolerant over phase mismatch (Yamamoto et al., 2020; Juvela et al., 2019). Table 2
in Appendix B.2 shows the detailed configurations.
Time domain loss is complementary to frequency domain loss to address problems such as periodic
artifacts. Time domain loss combines segment loss Ls(ekg), energy loss Le(nke)rgy and phase loss Lp(hka)se:
KT
LT (S, S) = ^X(XSegLse? (S, S)) + λenergyLenergy (S, S) + λphaseLhaSe(S，S)	(18)
k=1
where segment loss Ls(ekg), energy loss Le(nke)rgy and phase loss Lp(hka)se are described in Equation 24, 25,
and 26 of Appendix B.2. There are KT different window sizes ranging from 1 to 960 to calculate
time domain loss at different resolutions. The details of window sizes are shown in Table 3 of
Appendix B.2. The energy loss and phase loss have the advantage of alleviating metallic sounds.
Discriminative training is an effective way to train neural vocoders (Kong et al., 2020; Kumar et al.,
2019). In our study, we utilize a group of discriminators, including a multi-resolution time discrim-
inator DT, a subband discriminator Dsub, and frequency discriminator DF :
RT
D(S) = XDT(r)(S) + Dsub(S) + DF (S)	(19)
r=1
LD(s, S) = minmax(Es(log(D(S))) + E^(log(1 - D(S)))).	(20)
gD
6
Under review as a conference paper at ICLR 2022
The multi-resolution discriminators DT take signals from RT kinds of time resolutions after aver-
age pooling as input. The subband discriminator Dsub performs subband decomposition (Liu et al.,
2020) on the waveform, producing four subband signals to feed into four T-discrminators, respec-
tively. Frequency discriminator DF takes the linear spectrogram as input and outputs real or fake
labels. The bottom part of Figure 4 shows the main idea of T-discriminator and F-discriminator.
Appendix B.2 describes the detailed discriminator architectures.
There are two advantages of using neural vocoder in the synthesis stage. First, neural vocoder
trained using a large amount of speech data contains prior knowledge on the structural distribution
of speech signals, which is crucial to the restoration of distorted speech. The amount of training
data of vocoder is more than that used in conventional SSR methods with limited speaker numbers.
Second, the neural vocoder typically takes the mel spectrogram as input, resulting in fewer feature
dimensions than the STFT features. The reduction in dimension helps to lower computational costs
and achieve better performance in the analysis stage.
4 Experiments
4.1	Datasets and Evaluation Metrics
Training sets: The training speech datasets we use including VCTK (Yamagishi et al., 2019),
AISHELL-3 (Shi et al., 2020), and HQ-TTS. We call the noise datasets used for training as VD-
Noise. To simulate the reverberations, we employ a set of RIRs to create an RIR-44k dataset. We
use VCTK, VD-Noise, and the training part of RIR-44k to train the analysis stage. AISHELL-3,
VCTK, and HQ-TTS datasets are used to train the vocoder in the synthesis stage. The details of
those datasets and the RIRs simulation configurations are discussed in Appendix C.1.
Test sets: We employ VCTK-Demand (Valentini-Botinhao et al., 2017) as the denoising test set and
name it as DENOISE. We call our speech super-resolution, declipping, and dereverberation evalua-
tion test sets as SR, DECLI, and DEREV, respectively. In addition, we create an ALL-GSR test set
containing all distortions. We introduce the details of how we build these test sets in Appendix C.3.
Evaluation metrics: The metrics we adopt include log-spectral distance (LSD) (Erell & Weintraub,
1990), wide band perceptual evaluation of speech quality (PESQ-wb) (Rix et al., 2001), structural
similarity (SSIM) (Wang et al., 2004), and scale-invariant signal to noise ratio (SiSNR) (Le Roux
et al., 2019). We use mean opinion scores (MOS) to subjevtively evaluate different systems.
The output of the vocoder is not strictly aligned on sample level with the target, as is often the case
in generative model (Kumar et al., 2020). This effect will degrade the metrics, especially for those
calculated on time samples such as the SiSNR. So, to compensate the SiSNR, we design similar
metric, scale-invariant spectrogram to noise ratio (SiSPNR), to measure the discrepancies on the
spectrograms. Details of the metrics are described in Appendix C.4.
4.2	Distortions Simulation
For the SSR task, we perform only one type of distortion for evaluation. For the GSR task, we
first assume that D = {dnoise, drev, diow_res, dciip} because those distortions are the most common
distortions in daily environment (Ribas et al., 2016). Second, we assume that Q ≤ 4 in Equation 3.
In other words, each distortion in D appears at most one time. Then, we generate the distortions
following a specific order drɛv, dciip, diow_res, and dnoise. These distortions are added randomly using
random configurations.
4.3	Baseline Systems
Table 5 in Appendix D summarizes all the experiments in this study. We implement several SSR
and GSR systems using one-stage restoration models. For the GSR, we train a ResUNet model
called GSR-UNet with all distortions. For the SSR models, we implement a Denoise-UNet for
additive noise distortion, a Dereverb-UNet for reverberation distortion, a SR-UNet for low-resolution
distortion, and a Declip-UNet for clipping distortion. For the SR task, we also include two state-
of-the-art models, NuWave (Lee & Han, 2021) and SEANet (Li et al., 2021) for comparison. For
declipping task, we compare with a state-of-the-art synthesis-based method SSPADE (ZaV诵ka et al.,
7
Under review as a conference paper at ICLR 2022
2019a). To explore the impact of model size of the mel restoration model, we setup ResUNets with
two sizes, UNet-S and UNet, which have one and four ResConv blocks in each encoder and decoder
block, respectively.
4.4	Evaluation Results
Neural vocoder To evaluate the performance of the neural vocoder, we compare two baselines. The
Target system denotes using the perfect s for evaluation. The Unprocessed system denotes using
distorted speech x for evaluation. The Oracle-Mel system denotes using the mel spectrogram ofs as
input to the vocoder, which marks the performance of the vocoder. As shown in Table 1, the Oracle-
Mel system achieves a MOS score of 3.74, which is close to the Target MOS of 3.95, indicating that
the vocoder performs well in the synthesis task.
	ModelS	PESQ	LSD	SiSPNR	SSIM	MOS
Unprocessed	~1.94	2.00	^^720~	0.64	2.38
Oracle-Mel	2.52	0.91	11.73	0.74	3.74
Target	4.64	0.01	110.55	1.00	3.95
GSR-UNet	2.67	1.01	12.19	0.79	3.37
Denoise-UNet	2.33	1.98	9.65	0.65	2.87
Dereverb-UNet	1.97	1.81	8.50	0.59	/
VF-DNN	1.55	1.18	10.13	0.68	/
VF-BiGRU	1.92	1.02	10.98	0.71	3.24
VF-UNet-S	2.01	1.02	11.09	0.71	/
VF-UNet	2.05	1.01	11.14	0.71	3.62
Table 1: Average PESQ, LSD, SiSPNR, SSIM and
MOS scores on the general speech restoration test set,
ALL-GSR, which includes random distortions.
Figure 5: Box plot of the MOS scores on general
speech restoration task. Red solid line and green
dashed line represent median and mean value.
Unprocessed
Target
Oracle-Mel
VF-UNet
VF-BiGRU
GSR-UNet
Denoise-UNet
General speech restorations: Table 1 shows the evaluation results on ALL-GSR test set. Figure 5
shows the box plot of the MOS scores of these systems. The GSR-UNet outperforms the two SSR
models, Denoise-UNet and Dereverb-UNet by a large margin. It surpasses Denoise-UNet model by
0.5 on MOS score, which suggests the GSR model is more powerful than the SSR model on this
test set. For convenience, we denote VoiceFixer as VF in tables and figures. We observe that the
VF-UNet model achieves the highest MOS score and LSD score. Specifically, VF-UNet obtains
0.256 higher MOS score than that of GSR-UNet. This result indicates that VoiceFixer is better than
ResUNet based one-stage model on overall quality. What’s more, we notice that the MOS score
of VF-UNet is only 0.11 lower than the Oracle-Mel, demonstrating the good performance of the
analysis stage. Among the VoiceFixer analysis models, the UNet front-end achieves the best. The
VF-BiGRU model achieves similar subjective metrics with the VF-UNet model but has much lower
MOS scores. This phenomenon shows that the improvement in subjective metrics in VoiceFixer is
not always consistent with objective evaluation results.
Super-resolution: Table 6 in Appendix D.1 shows the evaluation results on the super-resolution
test set SR. For the 2 kHz, 4 kHz, and 8 kHz to 44.1 kHz super-resolution tasks, VF-UNet achieves a
significantly higher LSD, SiSPNR and SSIM scores than other models. The LSD value of VF-UNet
in 2 kHz sampling rate is still higher than the 8 kHz sampling rate score of GSR-UNet, SR-UNet,
NuWave, and SEANet. This demonstrates the strong performance of VoiceFixer on dealing with low
sampling rate cases. The VF-BiGRU model outperforms VF-UNet-S model on average scores for its
better performance on low upsample-ratio cases. MOS box plot in Figure 6b shows that VF-UNet
performs the best on 8 kHz to 44.1 kHz test set. Figure 6e shows the MOS score of Unprocessed
is close to Target on 24 kHz to 44.1 kHz test set, meaning limited perceptual difference between
the two sampling rates. On this test set, SEANet even achieves a higher MOS score than Target.
That’s due to the high-frequency part it generate contains more energy than that of Target, making
the results sound clearer.
Denoising: We evaluate the speech denoising performance on the DENOISE test set and show
results in Table 7 in Appendix D.1. We find that GSR-UNet preserves more details in the high-
frequency part and has better PESQ and SiSPNR values than the denoising only SSR model Denoise-
UNet. The reason might be that speech data augmentations and jointly performing super-resolution
task can increase the generalization and inpainting ability of the model (Hao et al., 2020). The PESQ
8
Under review as a conference paper at ICLR 2022
(a) Declipping (0.1)
(b) Super-resolution (8 kHz)
(c) Dereverberation
(d) Declipping (0.25)
(e) Super-resolution (24 kHz)
(f) Denoising
Figure 6: Box plot of the MOS scores on speech super-resolution, declipping, dereveberation and denoising.
score of VF-UNet reaches 2.43, higher than SEGAN, WaveUNet, and the model trained with weakly
labeled data in Kong et al. (2021b). The MOS evaluations in Figure 6f on speech denoising task also
demonstrate that the result of VF-UNet sound comparable with one-stage speech denoising models.
Declipping and dereveberation: Table 9 and Table 8 in Appendix D.1 show similar performance
trends on the speech declipping and speech dereveberation. In both tasks, the SSR model Dereverb-
UNet and Declip-UNet achieve the highest scores. The performance of GSR-UNet is slightly worse,
but it is acceptable considering that GSR-UNet does not need extra training for each task. SSPADE
performs better on SiSNR, but the PESQ and STOI scores are lower, especially in the 0.1 threshold
case. The MOS score in Figure 6d shows that the clipping effect in the 0.25 threshold case is not
easy to perceive, leading to high MOS scores across all methods. In Figure 6a, both Declip-UNet
and VF-UNet achieve the highest objective scores on the 0.1 threshold clipping test set. On the
dereverberation test set DEREV, VF-UNet achieves the highest MOS score 3.52.
5 Conclusions
In this work, we propose VoiceFixer, an effective approach for general speech restoration. The
evaluation results show that VoiceFixer achieves leading performance across the general speech
restoration, speech super-resolution, speech denoising, speech dereverberation, and speech declip-
ping tasks. In the future, we will extend VoiceFixer to restore general audio signals, including music
and general sounds.
Reproducibility Statement
We make our code and datasets anonymously downloadable for painless reproducibility.
Our pre-trained VoiceFixer and inference code are presented in https://github.com/
anonymous20211004/iclr2022-vf. The code for performing the experiments dis-
cussed in Section 4 is downloadable in https://github.com/anonymous20211004/
iclr2022-train-vf. The code can be used to conduct evaluations and generate reports on the
metrics mentioned in Section 4.1 automatically. The NuWave is realized using the code open-sourced
by Lee & Han (2021): https://github.com/mindslab-ai/nuwave. We reproduce SS-
PADE using the toolbox provided by Zaviska et al. (2020) at https://rajmic.github.io/
declipping2020. Besides, we upload the training and testing sets mentioned in Section 4.1
to https://zenodo.org/record/5546723. The AISHELL-3 dataset is open-sourced at
http://www.aishelltech.com/aishell_3. The HQ-TTS is a collection of datasets from
openslr.org, as is described in Table 4 of Appendix C.1.
9
Under review as a conference paper at ICLR 2022
References
Fanhu Bie, Dong Wang, Jun Wang, and Thomas Fang Zheng. Detection and reconstruction of
clipped speech for speaker recognition. Speech Communication, pp. 218-231, 2015.
Sawyer Birnbaum, Volodymyr Kuleshov, Zayd Enam, Pang Wei Koh, and Stefano Ermon. Temporal
film: Capturing long-range sequence dependencies with feature-wise modulations. arXiv preprint
arXiv:1909.06628, 2019.
Albert S Bregman. Auditory scene analysis: The perceptual organization of sound. MIT press,
1994.
Benjamin Cauchi, Ina Kodrasi, Robert Rehr, Stephan Gerlach, Ante Jukic, Timo Gerkmann, Simon
Doclo, and Stefan Goetze. Joint dereverberation and noise reduction using beamforming and a
single-channel speech enhancement scheme. In Proc. REVERB Challenge Workshop, pp. 1-8,
2014.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of
gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.
Djork-Ame Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network
learning by exponential linear units. arXiv preprint arXiv:1511.07289, 2015.
Ross Cutler, Ando Saabas, Tanel Parnamaa, Markus Loide, Sten Sootla, Marju Purin, Hannes Gam-
per, Sebastian Braun, Karsten Sorensen, Robert Aichner, et al. Acoustic echo cancellation chal-
lenge. In INTERSPEECH, 2021.
Alexandre Defossez, Gabriel Synnaeve, and Yossi Adi. Real time speech enhancement in the wave-
form domain. arXiv preprint arXiv:2006.12847, 2020.
Adoram Erell and Mitch Weintraub. Estimation using log-spectral-distance criterion for noise-robust
speech recognition. In Proceedings of the IEEE Conference on Acoustics, Speech, and Signal
Processing, pp. 853-856, 1990.
Simon Godsill, Peter Rayner, and Olivier Cappe. Digital audio restoration. In Applications ofDigital
Signal Processing to Audio and Acoustics, pp. 133-194. Springer, 2002.
Timothy D Griffiths and Jason D Warren. The planum temporale as a computational hub. Trends in
Neurosciences, pp. 348-353, 2002.
Archit Gupta, Brendan Shillingford, Yannis Assael, and Thomas C Walters. Speech bandwidth
extension with wavenet. In IEEE Workshop on Applications of Signal Processing to Audio and
Acoustics, pp. 205-208, 2019.
Kun Han, Yuxuan Wang, DeLiang Wang, William S Woods, Ivo Merks, and Tao Zhang. Learning
spectral mapping for speech dereverberation and denoising. IEEE/ACM Transactions on Audio,
Speech, and Language Processing, pp. 982-992, 2015.
Xiang Hao, Xiangdong Su, Shixue Wen, Zhiyu Wang, Yiqian Pan, Feilong Bao, and Wei Chen.
Masking and inpainting: A two-stage speech enhancement approach for low snr and non-
stationary noise. In Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Pro-
cessing, pp. 6959-6963, 2020.
Yanxin Hu, Yun Liu, Shubo Lv, Mengtao Xing, Shimin Zhang, Yihui Fu, Jian Wu, Bihong Zhang,
and Lei Xie. DCCRN: Deep complex convolution recurrent network for phase-aware speech
enhancement. arXiv preprint arXiv:2008.00264, 2020.
Kevin Jarrett, Koray Kavukcuoglu, Marc’Aurelio Ranzato, and Yann LeCun. What is the best multi-
stage architecture for object recognition? In International Conference on Computer Vision, pp.
2146-2153, 2009.
Lauri Juvela, Bajibabu Bollepalli, Junichi Yamagishi, and Paavo Alku. GELP: GAN-Excited linear
prediction for speech synthesis from mel-spectrogram. arXiv preprint arXiv:1904.03976, 2019.
10
Under review as a conference paper at ICLR 2022
Thomas Kailath. Lectures on Wiener and Kalman filtering. In Lectures on Wiener and Kalman
Filtering, pp. 1-143. 1981.
Hamidreza Baradaran Kashani, Ata Jodeiri, Mohammad Mohsen Goodarzi, and Shabnam Gholam-
dokht Firooz. Image to image translation based on convolutional neural network approach for
speech declipping. arXiv preprint arXiv:1910.12116, 2019.
Dan Kennedy-Higgins. Neural and cognitive mechanisms affecting perceptual adaptation to dis-
torted speech. PhD thesis, University College London, 2019.
Keisuke Kinoshita, Marc Delcroix, Takuya Yoshioka, Tomohiro Nakatani, Emanuel Habets, Rein-
hold Haeb-Umbach, Volker Leutnant, Armin Sehr, Walter Kellermann, and Roland Maas. The
REVERB challenge: A common evaluation framework for dereverberation and recognition of
reverberant speech. In IEEE Workshop on Applications of Signal Processing to Audio and Acous-
tics, pp. 1-4, 2013.
Sran KitiC, Nancy Bertin, and Remi GribonvaL Sparsity and cosparsity for audio declipping: a flex-
ible non-convex approach. In International Conference on Latent Variable Analysis and Signal
Separation, pp. 243-250, 2015.
Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. HiFi-GAN: Generative adversarial networks for
efficient and high fidelity speech synthesis. arXiv preprint arXiv:2010.05646, 2020.
Qiuqiang Kong, Yong Xu, Iwona Sobieraj, Wenwu Wang, and Mark D Plumbley. Sound event
detection and time-frequency segmentation from weakly labelled data. IEEE/ACM Transactions
on Audio, Speech, and Language Processing, pp. 777-787, 2019.
Qiuqiang Kong, Yin Cao, Haohe Liu, Keunwoo Choi, and Yuxuan Wang. Decoupling magnitude
and phase estimation with deep resunet for music source separation. In The International Society
for Music Information Retrieval, 2021a.
Qiuqiang Kong, Haohe Liu, Xingjian Du, Li Chen, Rui Xia, and Yuxuan Wang. Speech enhance-
ment with weakly labelled data from audioset. arXiv preprint arXiv:2102.09971, 2021b.
Juho Kontio, Laura Laaksonen, and Paavo Alku. Neural network-based artificial bandwidth ex-
pansion of speech. IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp.
873-881, 2007.
Volodymyr Kuleshov, S Zayd Enam, and Stefano Ermon. Audio super resolution using neural
networks. arXiv preprint arXiv:1708.00853, 2017.
Kundan Kumar, Rithesh Kumar, Thibault de Boissiere, Lucas Gestin, Wei Zhen Teoh, Jose Sotelo,
Alexandre de Brebisson, Yoshua Bengio, and Aaron Courville. Melgan: Generative adversarial
networks for conditional waveform synthesis. arXiv preprint arXiv:1910.06711, 2019.
Rithesh Kumar, Kundan Kumar, Vicki Anand, Yoshua Bengio, and Aaron Courville. NU-GAN:
High resolution neural upsampling with gan. arXiv preprint arXiv:2010.11362, 2020.
Frances Y Kuo and Ian H Sloan. Lifting the curse of dimensionality. Notices of the American
Mathematical Society, pp. 1320-1328, 2005.
Jonathan Le Roux, Scott Wisdom, Hakan Erdogan, and John R Hershey. SDR-half-baked or well
done? In Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Processing, pp.
626-630, 2019.
Junhyeok Lee and Seungu Han. Nu-wave: A diffusion probabilistic model for neural audio upsam-
pling. arXiv preprint arXiv:2104.02321, 2021.
Kehuang Li and Chin-Hui Lee. A deep neural network approach to speech bandwidth expansion.
In Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Processing, pp. 4395-
4399, 2015.
Yunpeng Li, Marco Tagliasacchi, Oleg Rybakov, Victor Ungureanu, and Dominik Roblek. Real-time
speech frequency bandwidth extension. In Proceedings of the IEEE Conference on Acoustics,
Speech, and Signal Processing, pp. 691-695, 2021.
11
Under review as a conference paper at ICLR 2022
Teck Yian Lim, Raymond A Yeh, Yijia Xu, Minh N Do, and Mark Hasegawa-Johnson. Time-
frequency networks for audio super-resolution. In Proceedings of the IEEE Conference on Acous-
tics, Speech, and Signal Processing, pp. 646-650, 2018.
Ju Lin, Yun Wang, Kaustubh Kalgaonkar, Gil Keren, Didi Zhang, and Christian Fuegen. A two-stage
approach to speech bandwidth extension. INTERSPEECH, pp. 1689-1693, 2021.
Haohe Liu, Lei Xie, Jian Wu, and Geng Yang. Channel-wise subband input for better voice and
accompaniment separation on high resolution music. arXiv preprint arXiv:2008.05216, 2020.
Philipos C Loizou. Speech enhancement: theory and practice. CRC press, 2007.
Yi Luo and Nima Mesgarani. Conv-tasnet: Surpassing ideal time-frequency magnitude masking
for speech separation. IEEE/ACM Transactions on Acoustics, Speech, and Signal Processing, pp.
1256-1266, 2019.
Craig Macartney and Tillman Weyde. Improved speech enhancement with the Wave-U-Net. arXiv
preprint arXiv:1811.11307, 2018.
Wolfgang Mack andEmanuel AP Habets. DecliPPing speech using deep filtering. In IEEE Workshop
on Applications of Signal Processing to Audio and Acoustics, pp. 200-204, 2019.
Rainer Martin. Spectral subtraction based on minimum statistics. Power, 1994.
Annamaria Mesaros, Toni Heittola, and Tuomas Virtanen. A multi-device dataset for urban acoustic
scene classification. arXiv preprint arXiv:1807.09840, 2018.
Masanori Morise, Fumiya Yokomori, and Kenji Ozawa. World: a vocoder-based high-quality speech
synthesis system for real-time applications. IEICE Transactions on Information and Systems, pp.
1877-1884, 2016.
Anna K Nabelek, Tomasz R Letowski, and Frances M Tucker. Reverberant overlap-and self-masking
in consonant identification. The Journal of the Acoustical Society of America, pp. 1259-1265,
1989.
Yoshihisa Nakatoh, Mineo Tsushima, and Takeshi Norimatsu. Generation of broadband speech
from narrowband speech based on linear mapping. Electronics and Communications in Japan,
pp. 44-53, 2002.
Arun Narayanan and DeLiang Wang. Ideal ratio mask estimation using deep neural networks for
robust speech recognition. In Proceedings of the IEEE Conference on Acoustics, Speech, and
Signal Processing, pp. 7092-7096, 2013.
Patrick A Naylor and Nikolay D Gaubitch. Speech dereverberation. Springer Science & Business
Media, 2010.
Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for
raw audio. arXiv preprint arXiv:1609.03499, 2016.
Santiago Pascual, Antonio Bonafonte, and Joan Serra. SEGAN: Speech enhancement generative
adversarial network. arXiv preprint arXiv:1703.09452, 2017.
Wei Ping, Kainan Peng, Kexin Zhao, and Zhao Song. Waveflow: A compact flow-based model for
raw audio. In International Conference on Machine Learning, pp. 7706-7716, 2020.
Adam Polyak, Lior Wolf, Yossi Adi, Ori Kabeli, and Yaniv Taigman. High fidelity speech re-
generation with application to speech enhancement. In Proceedings of the IEEE Conference on
Acoustics, Speech, and Signal Processing, pp. 7143-7147, 2021.
Ryan Prenger, Rafael Valle, and Bryan Catanzaro. Waveglow: A flow-based generative network
for speech synthesis. In Proceedings of the IEEE Conference on Acoustics, Speech, and Signal
Processing, pp. 3617-3621, 2019.
12
Under review as a conference paper at ICLR 2022
Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. FastSpeech:
Fast, robust and controllable text to speech. arXiv preprint arXiv:1905.09263, 2019.
Lucas Rencker, Francis Bach, Wenwu Wang, and Mark D Plumbley. Sparse recovery and dictionary
learning from nonlinear compressive measurements. IEEE Transactions on Signal Processing,
pp. 5659-5670, 2019.
Dayana Ribas, Emmanuel Vincent, and Jose Ramon Calvo. A study of speech distortion Condi-
tions in real scenarios for speech processing applications. In IEEE Spoken Language Technology
Workshop, pp. 13-20, 2016.
Antony W Rix, John G Beerends, Michael P Hollier, and Andries P Hekstra. Perceptual evaluation
of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and
codecs. In Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Processing, pp.
749-752, 2001.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedi-
cal image segmentation. In International Conference on Medical image computing and computer-
assisted intervention, pp. 234-241, 2015.
Pascal Scalart et al. Speech enhancement based on a priori signal to noise estimation. In Proceedings
of the IEEE Conference on Acoustics, Speech, and Signal Processing Conference Proceedings, pp.
629-632, 1996.
Boaz Schwartz, Sharon Gannot, and Emanuel AP Habets. Online speech dereverberation using
kalman filter and em algorithm. IEEE/ACM Transactions on Audio, Speech, and Language Pro-
cessing, pp. 394-406, 2014.
Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang,
Zhifeng Chen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan, et al. Natural TTS synthesis by con-
ditioning wavenet on Mel spectrogram predictions. In Proceedings of the IEEE Conference on
Acoustics, Speech, and Signal Processing, pp. 4779-4783, 2018.
Yao Shi, Hui Bu, Xin Xu, Shaoji Zhang, and Ming Li. Aishell-3: A multi-speaker mandarin tts
corpus and the baselines. arXiv preprint arXiv:2010.11567, 2020.
Xiaofeng Shu, Yehang Zhu, Yanjie Chen, Li Chen, Haohe Liu, Chuanzeng Huang, and Yuxuan
Wang. Joint echo cancellation and noise suppression based on cascaded magnitude and complex
mask estimation. arXiv preprint arXiv:2107.09298, 2021.
Serkan Sulun and Matthew EP Davies. On filter generalization for music bandwidth extension using
deep neural networks. IEEE Journal of Selected Topics in Signal Processing, pp. 132-142, 2020.
Ke Tan, Yong Xu, Shi-Xiong Zhang, Meng Yu, and Dong Yu. Audio-visual speech separation and
dereverberation with a two-stage multimodal network. IEEE Journal of Selected Topics in Signal
Processing, pp. 542-553, 2020.
Qiao Tian, Yi Chen, Zewang Zhang, Heng Lu, Linghui Chen, Lei Xie, and Shan Liu. TFGAN: Time
and frequency domain based generative adversarial network for high-fidelity speech synthesis.
arXiv preprint arXiv:2011.12206, 2020.
Cassia Valentini-Botinhao et al. Noisy speech database for training speech enhancement algorithms
and TTS models. 2017.
Tim Van den Bogaert, Simon Doclo, Jan Wouters, and Marc Moonen. Speech enhancement with
multichannel wiener filter techniques in multimicrophone binaural hearing aids. The Journal of
the Acoustical Society of America, pp. 360-371, 2009.
Charles Van Winkle. Audio analysis and spectral restoration workflows using adobe audition. In
Audio Engineering Society Conference: 33rd International Conference: Audio Forensics-Theory
and Practice, 2008.
13
Under review as a conference paper at ICLR 2022
Emmanuel Vincent, Shinji Watanabe, Aditya Arie Nugraha, Jon Barker, and Ricard Marxer. An
analysis of environment, microphone and data simulation mismatches in robust speech recogni-
tion. Computer Speech & Language, pp. 535-557, 2017.
Heming Wang and DeLiang Wang. Towards robust speech super-resolution. IEEE/ACM Transac-
tions on Audio, Speech, and Language Processing, 2021.
Hong Wang and Fumitada Itakura. Dereverberation of speech signals based on sub-band envelope
estimation. IEICE Transactions on Fundamentals of Electronics, Communications and Computer
Sciences, pp. 3576-3583, 1991.
Wenfu Wang, Shuang Xu, and Bo Xu. First step towards end-to-end parametric tts synthesis: Gen-
erating spectral parameters with neural attention. In INTERSPEECH, pp. 2243-2247, 2016.
Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment:
from error visibility to structural similarity. IEEE Transactions on Image Processing, pp. 600-
612, 2004.
Donald S Williamson and DeLiang Wang. Time-frequency masking in the complex domain for
speech dereverberation and denoising. IEEE/ACM Transactions on Audio, Speech, and Language
Processing, pp. 1492-1501, 2017.
Junichi Yamagishi, Christophe Veaux, Kirsten MacDonald, et al. Cstr vctk corpus: English multi-
speaker corpus for cstr voice cloning toolkit (version 0.92). 2019.
Ryuichi Yamamoto, Eunwoo Song, and Jae-Min Kim. Parallel WaveGAN: A fast waveform gen-
eration model based on generative adversarial networks with multi-resolution spectrogram. In
Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Processing, pp. 6199-
6203, 2020.
Chengzhu Yu, Heng Lu, Na Hu, Meng Yu, Chao Weng, Kun Xu, Peng Liu, Deyi Tuo, Shiyin Kang,
Guangzhi Lei, et al. Durian: Duration informed attention network for multimodal synthesis. arXiv
preprint arXiv:1909.01700, 2019.
Pavel Zaviska, Pavel Rajmic, Ondrej Mokry, and Zdenek PrUSa. A proper version of synthesis-based
sparse audio declipper. In Proceedings of the IEEE Conference on Acoustics, Speech, and Signal
Processing, pp. 591-595, 2019a.
Pavel Zavirka, Pavel Rajmic, and Jirl Schimmel. Psychoacoustically motivated audio declipping
based on weighted l 1 minimization. In 2019 42nd International Conference on Telecommunica-
tions and Signal Processing, pp. 338-342, 2019b.
Pavel Zavirka, Pavel Rajmic, Alexey Ozerov, and Lucas Rencker. A survey and an extensive evalua-
tion of popular audio declipping methods. IEEE Journal of Selected Topics in Signal Processing,
pp. 5-24, 2020.
Yan Zhao, Zhong-Qiu Wang, and DeLiang Wang. Two-stage deep learning for noisy-reverberant
speech enhancement. IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp.
53-62, 2019.
14
Under review as a conference paper at ICLR 2022
A Appendix A
A. 1 Speech restoration tasks
Speech super-resolution A lot of early studies (Nakatoh et al., 2002; Kontio et al., 2007) break
super-resolution (SR) into spectral envelop estimation and excitation generation from the low-
resolution part. At that time, the direct mapping from the low-resolution part to the high-resolution
feature is not widely explored since the dimension of the high-resolution part is relatively high.
Later, deep neural network (Li & Lee, 2015; Kuleshov et al., 2017) is introduced to perform SR.
These approaches show better subjective quality comparing with traditional methods. To increase
the modeling capacity, TFilm (Birnbaum et al., 2019) is proposed to model the affine transformation
among each time block. Similarly, WaveNet also shows effectiveness in extending the bandwidth
of a band-limited speech (Gupta et al., 2019). To utilize the information both from the time and
frequency domain, Wang & Wang (2021) propose a time-frequency loss that can yield a balanced
performance on different metrics. Recently, NU-GAN (Kumar et al., 2020) and NU-Wave (Lee &
Han, 2021) pushed the target sample rate in SR to high fidelity, namely 48 kHz.
Although employing deep neural networks in BWE shows promising results, the generalization
capability of these methods is still limited. For example, previous approaches (Kuleshov et al.,
2017) usually train and test models with a fixed setup, i.e., fixing the initial and target sampling
rates. However, in real-world applications, speech bandwidth is not usually constant. In addition,
since the real high-low quality speech pair is hard to collect, typically, we produces low-quality
audio with lowpass or bandpass filters during training. In this case, systems tend to overfit specific
filters. As discussed in Sulun & Davies (2020), when the kind of filter used during training and
testing differ, the performance can fall considerably. To alleviate filter overfitting, Sulun & Davies
(2020) propose to train models with multiple kinds of lowpass filters, by which the unseen filters
can be handled properly.
Speech declipping The methods for speech declipping can be categorized as supervised methods
and unsupervised methods. The unsupervised, or blind methods usually perform declipping based
on some generic regularization and assumption of what natural audio should look like, such as
ASPADE (Kitic et 1 2015), dictionary learning (Rencker et al., 2019), and PsychoacoUsticany
motivated l1 minimization (Zaviska et al., 2019b). The supervised models, mostly based on deep
neUral network (DNN) (Bie et al., 2015; Mack & Habets, 2019), are UsUally trained on cliPPed
and unclipped data pairs. For example, Kashani et al. (2019) treat the declipping as an image-to-
image translation problem and utilize the UNet to perform spectral mapping. Currently, most of the
state-of-the-art methods are unsupervised (ZaviSka et al., 2θ2θ) because they are usually designed to
work on different kinds of audio, while the supervised model mainly specialized on data similar to
its training data. However, ZaviWka et al. (2020) believes supervised models still have the potential
for better declipping performance.
Speech denoising Conventional methods are efficient and effective on dealing with stationary noise,
such as spectral substraction (Martin, 1994) and Wiener and Kalman filtering (Kailath, 1981) By
comparison, deep learning based models such as Conv-TasNet (Luo & Mesgarani, 2019) show higher
subjective score and robustness on complex cases. Recently, new schemes have emerged for training
speech denoising models. SEGAN (Pascual et al., 2017) tried a generative way. Kong et al. (2021b)
achieved a denoising model using only weakly labeled data. And Polyak et al. (2021) realized a
denoising model using a regeneration approach.
Speech dereveberation Some of the early methods in speech dereverberation, such as inverse filter-
ing (Naylor & Gaubitch, 2010) and subband envelope estimation (Wang & Itakura, 1991), aiming
at deconvolving the reverberant signal by estimating an inverse filter. However, the inverse filter is
hard and not robust to estimate accurately. Other methods, like spectral substraction, is based on
an overlap-masking (NabeIek et al., 1989) effect of reverberation. Schwartz et al. (2014) performed
dereverberation using Kalman filter and expectation-maximization algorithm. Recently, deep learn-
ing based dereverberation methods have emerged as the state-of-the-art. Han et al. (2015) used
a fully connected DNN to learn a spectral mapping from reverberant speech to clean speech. In
Williamson & Wang (2017), similar to the masking-based denoising methods, they proposed to
perform dereverberation using a time-frequency mask.
15
Under review as a conference paper at ICLR 2022
A.1.1 Joint restoration and synthetic restoration
Joint restoration Many works have adopted the joint restoration approach to improving models.
To make the acoustic echo cancellation (AEC) result sound cleaner, MC-TCN (Shu et al., 2021)
proposed to jointly perform AEC and noise suppression at the same time. MC-TCN achieved a
mean opinion score of 4.41, outperforming the baseline of AEC Challenge (Cutler et al., 2021) by
0.54. Moreover, in the REVERB challenge (Kinoshita et al., 2013), the test set has both reverber-
ation and noise. Therefore, the proposed methods in the challenge should perform both denoising
and dereverberation. In Han et al. (2015), the authors proposed to perform dereverberation and
denoising within a single DNN and substantially outperform related methods regarding quality and
intelligibility. However, previous joint processing usually involved only two sub tasks. In our study,
we joint perform four or more tasks to achieve general restoration.
Synthetic restoration Directly estimate the source signal from the observed mixture is hard espe-
cially when the SNR is low. Some studies adopted a regeneration approach. In Polyak et al. (2021),
the authors utilized an ASR model, a pitch extraction model, and a loudness model to extract seman-
tic level information from the speaker. Then these features were fed to an encode-decoder network
to regenerate the speech signal. To maintain the consistency of speaker characteristics, it used an
auxiliary identity network to compute the identity feature. Similar to synthetic speech restoration,
TTS can be treated as the regeneration of speech from texts.
A.1.2 Neural vocoder
Vocoder, which maps the encoded speech features to the waveforms, is an indispensable compo-
nent in speech synthesis. The most widely used input feature for vocoder is mel spectrogram. In
recent years, since the emergence of WaveNet (Oord et al., 2016), neural network based vocoders
demonstrate clear advantages over traditional parametric methods (Morise et al., 2016). Comparing
with conventional methods, the quality of WaveNet is more closer to the human voice. Later, Wav-
eRNN (Yu et al., 2019) is proposed to model the waveform with a single GRU. In this way, WaveRNN
has much lower complexity comparing with WaveNet. However, the autoregressive nature of these
models and deep structure make their inference process hard to be paralleled. To address this prob-
lem, non-autoregressive models like WaveGlow (Prenger et al., 2019) and WaveFlow (Ping et al.,
2020) were proposed. Afterward, non-autoregressive GAN-based models such as MelGAN (Kumar
et al., 2019) push the synthesis quality to a comparable level with autoregressive models. Recently,
TFGAN (Tian et al., 2020) demonstrated strong capability in vocoding. Directed by multiple dis-
criminators and loss functions, TFGAN was able to leverage information from both time domain
and frequency domain. As a result, the synthesis quality of TFGAN is more natural and less metallic
comparing with other GAN-based non-autoregressive models. In this work, we realize a universal
vocoder based on TFGAN, which can reconstruct waveform from mel spectrogram of an arbitrary
speaker with good perceptual quality. The pre-trained vocoder is available online to facilitate future
studies and reproduce our work.
16
Under review as a conference paper at ICLR 2022
B Appendix B
B.1 DETAILS OF THE ANALYSIS STAGE
____________________Low Quality
Spectrogram
BatchNorm | LinearO 128 -> 256
ReIU I BatchNorm | Linear1 256-› 512
I	RelU I BatehNorm | Linear2 512-> 1024	∣
I	ReiU I BatehNOrm | Linear3 1024-› 512	∣
Relu I BatchNorm | Linear4 512-› 256
ReIU I BatchNorm | Linear5 256-› 128
I
Restoration Mask
4
------------------------*0
4
Restored Spectrogram
(a) DNN
Figure 7: The architecture
____________________ Low Quality
Spectrogram
J	BatChNOrm | Linear0 128 -> 256
BatchNorm
J	BidireCtionaI GRU 256 -> 256 layer1
I	BidireCtionaI GRU 256 -> 256 layer2
I RelU I BatChNOrm | LinearI 512-> 256
RelU I BatChNorm | Linear2 256-> 128
!
Restoration Mask
------------------------Hg)
I
Restored Spectrogram
(b) BiGRU
of DNN and Bi-GRU
The DNN and BiGRU We use are shown in Figure 7. DNN is a six layers fully connected network
with BatchNorm and ReLU activations. The DNN accept each time step of the low-quality spec-
trogram as the input feature and output the restoration mask. Similarly, for the BiGRU model, we
substitute some layers in DNN to a two-layer bidirectional GRU to capture the time dependency
between time steps. To increase the modeling capacity of BiGRU, we expanded the input dimension
of GRU to twice the mel frequency dimension with full connected networks.
The detailed architecture of ResUNet is shown in Figure 3a. In the down-path, the input low-quality
mel spectrogram will go through 6 encoder blocks, which includes a stack of L1 ResConv and a
2 × 2 average pooling. In ResConv, the outputs of ConvBlock and the residual convolution are
added together as the output. ConvBlock is a typical two layers convolution with BatchNorm and
leakyReLU activation functions. The kernel size of residual convolution and the convolution in
ConvBlock is 1 × 1 and 3 × 3. Correspondingly, the decoder blocks have the symmetric structure
of the encoder blocks. It first performs a transpose convolution with 2 × 2 stride and 3 × 3 kernels,
which result is concatenated with the output of the encoder at the same level to form the input of the
decoder. The decoder also contain L2 layers of ResConv. The output of the final decoder block is
passed to a final ConvBlock to fit the output channel.
We use Adam optimizer with β1 = 0.5, β2 = 0.999 and a 3e-4 learning rate to optimize the analysis
stage of VoiceFixer. We treat the first 1000 steps as the warmup phase, during which the learning
rate grows linearly from 0 to 3e-4. We decay the learning rate by 0.9 every 400 hours of training
data. We perform an evaluation every 200 hours of training data. If we observe three consecutive
evaluations with no improvement, we will interrupt the experiment.
For all the STFT and iSTFT, we use the hanning window with a window length of 2048 and a hop
length of 441. As all the audio we use is at the 44.1 kHz sample rate, the corresponding spectrogram
size in this setting will be T × 1025, where T is the dimension of time frames. For mel spectrogram,
the dimensions of the linear spectrogram are transformed into T × 128.
B.2 Details of the synthesis s tage
As shown in Table 3, we use 7 kinds of STFT resolutions and 4 kinds of time resolution during the
calculation ofLF and LT. So KF = 7 in Equation 17 and KT = 4 in Equation 18.
The mel loss Lmel, spectral convergence loss Lsc, STFT magnitude loss Lmag, segment loss Lseg,
energy loss Lenergy, and phase loss LPhaSe are defined in Equation 21 26. The function v(∙) is the
17
Under review as a conference paper at ICLR 2022
k		1	2	3	4	5	6	7
win-length	4096	2048	1024	512	256	128	64
hop-length	2048	1024	512	256	128	64	32
fft-size	8192	4096	2048	1024	512	256	128
Table 2: STFT setup for different k in LF.
	k		1	2	3	4
frame-length	-1-	240	480	960
hop-length	1	120	240	480
Table 3: Windowing setup for different k in
LT .
windowing function that divide time sample into w windows and compute mean value within each
window, v(s)1×w = (mean(s0), mean(s1), ..., mean(sw-1)). Each sw stand for windowed s. ∆
stand for first difference.
Lmel(S, S)
- Smel 2
Lsc(S, S)=
ISI-阿F
THT
Lmag(S, S) = M(ISD- Iog(IS|)|L ,
Lseg (S, S) = kv(Sw ) - V(Sw ) k ι ,
Lenergy (S, S) = Ilv(Sw) - V(Sw) || 1 ,
Lphase(S, S) = Qv(SW2) - δV(Sw) ∣l 1 ,
(21)
(22)
(23)
(24)
(25)
(26)
Table 9 and Table 8 show the structure of frequency and time domain discriminators. The sub-
band discriminators Dsub and multi-resolution time discriminators DT(r) (s) use the structure of T-
discriminator, which is a stack of one dimensional convolution with grouping and large kernal size.
The frequency discriminator DF use the similar module ResConv similar to ResUNet shown in Fig-
ure 3b.
T-discriminator__________________________
ConV1d(1, 128, kernal_size=16),
LeakyRelU(0.2)
Conv1d(128, 128, kernal_size=41, stride=4, padding=20, groups=8),
LeakyRelU(0.2)
ConVId(128, 128, kernaLSiZe=41, Stride=4, Padding=20, groups=16),
LeakyRelU(0.2)
ConVId(128, 128, kernaLSiZe=41, stride=4, padding=20, groups=32),
LeakyRelU(0.2)
ConV1d(128, 1, kernal_SiZe=3, Stride=1, padding=1),
LeakyRelU(0.2)
F-discriminator______________
Cοnν2d(1,32,kernal size=(3,3))
ResConv(32, 32, Stride=1,kernal size=(3,3))
ResConv(32, 32, Stride=1,kernal size=(3,3))
ResConv(32, 64, stride=2,kernal size=(3,3))
ResConV(64, 64, Stride=1,kernal size=(3,3))
ResConv(64, 32, stride=2,kernal size=(3,3))
ResConV(32, 32, stride=1,kernal size=(3,3))
ResConv(32, 32, stride=2,kernal size=(3,3))
ResConv(32, 32, stride=1,kernal size=(3,3))
Figure 8:	The structure of T-discriminator.
Figure 9:	The structure of F-discriminator.
For the training of vocoder, we setting up the λD to λseg value in Equation 16, Equation 17,
and Equation 18 as λD = 4.0, λmel = 50, λsc = 5.0, λmag = 5.0, λenergy = 100.0, λphase = 100.0,
and λseg = 200.0
18
Under review as a conference paper at ICLR 2022
C Appendix C
C.1 Datasets
Clean speech CSTR VCTK corpus (Yamagishi et al., 2019) is a multi-speaker English corpus con-
taining 110 speakers with different accents. We split it into a training part VCTK-Train and a
testing part VCTK-Test. The version of VCTK we used is 0.92. To follow the data preparation
strategy of Lee & Han (2021), only the mic1 microphone data is used for experiments, and p280
and p315 are omitted for the technical issues. For the remaining 108 speakers, the last 8 speak-
ers, p360,p361,p362,p363,p364,p374,p376,s5 are splitted as test set VCTK-Test. Within the other
100 speakers, p232 and p257 are omitted because they are used later in the test set DENOISE,
the remaining 98 speakers are defined as VCTK-Train. Except for the training of NuWave, all the
utterances are resampled at the 44.1 kHz sample rate. AISHELL-3 is an open-source Hi-Fi man-
darin speech corpus, containing 88035 utterances with a total duration of 85 hours. HQ-TTS dataset
contains 191 hours of clean speech data collected from a serial of datasets on openslr.org. In
Table 4, we include the details of HQ-TTS, including the URL and language types of each subset.
Table 4: The components of HQ-TTS dataset.
	URL		LangUages			URL		LangUages	
http://www.openslr.org/32/	Afrikaans, Sesotho, SetSwana and isiXhosa	http://www.openslr.org/70/	Nigerian English
http://www.openslr.org/37/	Bangladesh Bengali and Indian Bengali	http://www.openslr.org/71/	Chilean Spanish
http://www.openslr.org/41/	Javanese	http://www.openslr.org/72/	Colombian Spanish
http://www.openslr.org/42/	Khmer	http://www.openslr.org/73/	Peruvian Spanish
http://www.openslr.org/43/	Nepali	http://www.openslr.org/74/	Puerto Rico Spanish
http://www.openslr.org/44/	Sundanese	http://www.openslr.org/75/	Venezuelan Spanish
http://www.openslr.org/61/	Spanish	http://www.openslr.org/76/	Basque
http://www.openslr.org/63/	Malayalam	http://www.openslr.org/77/	Galician
http://www.openslr.org/64/	Marathi	http://www.openslr.org/78/	Gujarati
http://www.openslr.org/65/	Tamil	http://www.openslr.org/79/	Kannada
http://www.openslr.org/66/	Telugu	http://www.openslr.org/80/	Gujarati
http://www.openslr.org/69/	CataIan			
Noise data One of the noise dataset we use come from VCTK-Demand (VD) (Valentini-Botinhao
et al., 2017), a widely used corpus for speech denoising and noise-robust TTS training. This dataset
contains a training part VD-Train and a testing part VD-Test, in which both contain two noisy set
VD-Train-Noisy, VD-Test-Noisy and two clean speech set VD-Train-Clean, VD-Test-Clean. To
obtain the noise data from this dataset, we minus each noisy data from VD-Train-Noisy with its
corresponding clean part in VD-Train-Clean to get the final training noise dataset VD-Noise. The
noise data are all resampled to 44.1 kHz. Another noise dataset we adopt is the TUT urban acoustic
scenes 2018 dataset (Mesaros et al., 2018), which is originally used for the acoustic scene classi-
fication task of DCASE 2018 Challenge. The dataset contains 89 hours of high-quality recording
from 10 acoustic scenes such as airport and shopping mall. The total amount of audio is divided into
development DCASE-Dev and evaluation DCASE-Eval parts. Both of them contain audio from all
cities and all acoustic scenes.
Room impulse response We randomly simulated a collection of Room Impulse Response filters
to simulate the 44.1 kHz speech room reverberation using an open-source tool 2. The meters of
height, width, and length of the room are sampled randomly in a uniform distribution U(1, 12). The
placement of the microphone is then randomly selected within the room space. For the placement
of the sound source, we first determined the distance to the microphone, which is randomly sampled
in a Gaussian distribution N(μ, σ2), μ = 2,σ = 4. If the sampled value is negative or greater than
five meters, we will sample the distance again until it meets the requirement. After sampling the
distance between the microphone and sound source, the placement of the sound source is randomly
selected within the sphere centered at the microphone. The RT60 value we choose come from the
uniform distribution U(0.05, 1.0). For the pickup pattern of the microphone, we randomly choose
from omnidirectional and cardioid types. Finally, we simulated 43239 filters, in which we randomly
split out 5000 filters as the test set RIR-Test and named other 38239 filters as RIR-Train.
2https://github.com/sunits/rir_simulator_python
19
Under review as a conference paper at ICLR 2022
C.2 Training data
We describe the simulation of training data in Algorithm 1. S = {s(0), s(1), ..., s(i)}, N =
{n(0), n(1), ..., n(i)}, and R = {r(0), r(1), ..., r(i)} are the speech dataset, noise dataset, and
RIR dataset. We use several helper function to describe this algorithm. randomFilterType(∙) is
a function that randomly select a type of filter within butterworth, chebyshev, bessel, and ellipic.
Resample(x, o1 , u) is a resampling function that resample the one dimensional signal x from a
original samplerate o1 to the target u samplerate. buildFilter(t, c, o2) is a filter design function that
return a type t filter with cutoff frequency C and order 02. max(∙),min(∙), and abs(∙) is the element
wise maximum, minimum, and absolute value function. mean(∙) calculate the mean value of the
input.
We first select a speech utterance s, a segment of noise n and a RIR filter r randomly from the
dataset. Then with p1 probability, we add the reverberation effect using r. And with p2 prob-
ability, we add clipping effect with a clipping ratio η, which is sampled in a uniform distrubu-
tion U(ηlow , ηhigh). To produce low-resolution effect, after determining the filter type t, we ran-
domly sample the cutoff frequency c and order o from the uniform distribution U(Clow, Chigh)
and U(Olow, Ohigh). Then we perform convolution between x and the type t order o lowpass fil-
ter with cutoff frequency c. Finally the filtered data will be resampled twice, one is resample to
c * 2 samplerate and another is resample back to 44.1 kHz. We also perform the same lowpass
filtering to the noise signal randomly. This operation is necessary because, if not, the model will
overfit the pattern that the bandwidth of noise signal is always different from speech. In this case,
the model will fail to remove noise when the bandwidth of noise and speech are similar. For the
simulation of noisy environment, we randomly add the noise n into the speech signal x using a
random SNR S 〜U(Slow, Shigh). To fit the model with all energy levels, We randomly conduct a
q 〜U(Qlow, Qhigh) scaling to the input and target data pair.
In our work, we choose the following parameters to perform this algorithm, p1 = 0.25, p2 = 0.25,
p3 = 0.5, ηlow = 0.06, ηhigh = 0.9, Clow = 750, Chigh = 22050, Olow = 2, Ohigh = 10,
Slow = -5, Shigh = 40, Qlow = 0.3, Qhigh = 1.0.
Algorithm 1: Add random distortions to high-quality speech signal s
In: S J S; n J N; r J R
Out: The high-quality speech s and its randomly distorted version x
x = s;
with p1 probability:
x = x * r ;	/* Convolute with RIR filter */
with p2 probability:
θ 〜U(Θlow, Θhigh) ;	/* Choose clipping ratio */
x = max(min(x, θ), -θ) ;	/* Hard clipping */
with p3 probability:
t = randomFilterType() ;
c 〜U(Clow,Chigh);
0 〜U(Olow, Ohigh) ;	/* Random cutoff and order * /
x = x * buildFilter(t, c, o) ;	/* Low pass filtering */
x = Resample(Resample(x, 44100, c * 2), c * 2, 44100) ;	/* Resample */
with p4 probability:
n = n * buildFilter(t, c, 0) ; /* Low pass filtering on noise */
n = Resample(Resample(n, c * 2), 44100) ;	/* Resample */
with p5 probability:
s 〜 U(Slow, Shigh) ;
q 〜 U(Qlow, Qhigh) ;	/* Random SNR and scale */
n = mean(abs(n))7mean(abs(x)) ; /* NormaliZe the energy of noise */
X =(X + ιοn∕2ο) ;	/* Add noise */
s = qs ;	/* Scaling */
X = qX ;	/* Scaling */
20
Under review as a conference paper at ICLR 2022
C.3 Testing data
Testing data is crucial for the evaluation for each kind of distortion. The testing data we use either
come from open-sourced test set or simulated by ourselves.
Super-resolution The simulation of the SR test set follows the work of (Kuleshov et al., 2017;
Wang & Wang, 2021). The low-resolution and target data pairs are obtained by transforming 44.1
kHz sample rate utterances in target speech data VCTK-Test to a lower sample rate u. To achieve
that, We first convolve the speech data with an order 8 Chebyshev type I lowpass filter with the 2
cutoff frequency. Then we subsample the signal to u sample rate using polyphase filtering. In this
work, to test the performance on different sampling rate settings, u are set at 2 kHz, 4 kHz, 8 kHz, 16
kHz, and 24 kHz. We denote the corresponding five testing set as VCTK-4k, VCTK-4k, VCTK-8k,
VCTK-16k, and VCTK-24k, respectively.
Denoising For the denoising task, we adopt the open-sourced testing set DENOISE described in
Appendix C.1. This test set contains 824 utterances from a female speaker and a male speaker. The
type of noise data comprises a domestic noise, an office noise, noise in the transport scene, and two
street noises. The test set is simulated at four SNR levels, which are 17.5 dB, 12.5 dB, 7.5 dB, and
2.5 dB. The original data is sampled at 48 kHz. We downsample it to 44.1 kHz to fit our experiments.
Dereverberation The test set for dereverberation, DEREV, is simulated using VCTK-Test and RIR-
Test. For each utterance in VCTK-Test, we first randomly select an RIR from RIR-Test, then we
calculate the convolution between the RIR and utterance to build the reverberant speech. Finally, we
build 2937 reverberant and target data pairs.
Declipping DECLI, the evaluation set for declipping, is also constructed based on VCTK-Test. We
perform clipping on VCTK-Test following the equation in Section 2 and choose 0.25, 0.1 as the
two setups for the clipping ratio. This result in two declipping test sets with different levels, each
containing 2937 clipped speech and target audios.
General speech restoration To evaluate the performance on GSR, we simulate a test set ALL-
GSR comprising of speech with all kinds of distortion. The clean speeches and noise data used to
build ALL-GSR is VCTK-Test and DCASE-Eval. The simulation procedure of ALL-GSR is almost
the same to the training data simulation described in Section 4.2. In total, 501 three seconds long
utterances are produced in this test set.
MOS Evaluation We select a small portion from the test sets to carry out MOS evaluation for each
one. In SR, DECLI, and DEREV, we select 38 utterances out for human ratings. In DENOISE and
ALL-GSR, we randomly choose 42 and 51 utterances.
C.4 Evaluation metrics
Log-spectral distance LSD is a commonly used metrics on the evaluation of super-resolution per-
formance (Kumar et al., 2020; Lee & Han, 2021; Wang & Wang, 2021). For target signal s and
output estimate s, LSD can be computed as Equation 27, where S and S stand for the magnitude
spectrogram of S and s.
LSD(S, S) = T∑T=ι ^F∑f=ι"( Ifl2 )2	(27)
Perceptual evaluation of speech quality PESQ is widely used in speech restoration literature as
their evaluation metrics (Pascual et al., 2017; Hu et al., 2020). It was originally developed to model
the subjective test commonly used in telecommunication. PESQ provides a score ranging from -0.5
to 4.5 and the higher the score, the better quality a speech has. In our work, we used an open-sourced
implementation of PESQ to compute these metrics. Since PESQ only works on a 16 kHz sampling
rate, we performed a 16 kHz downsampling to the output 44.1k audio before evaluation.
Structural similarity SSIM (Wang et al., 2004) is a metrics in image super-resolution. It addresses
the shortcoming of pixel-level metrics by taking the image texture into account. We match the
implementation of SSIM in (Wang et al., 2004) with ours and compute SSIM as Equation 28, where
μs and σs is the mean and standard deviation of S. CovSS is the Covariance of S and S. ∈ι = 0.01
21
Under review as a conference paper at ICLR 2022
and 2 = 0.02 are two constant used to avoid zero division. Similarity is measured within the K
7*7 blocks divided from S and S.
SSIM S 6 —ςk ( (2μSk μSk + e1)(2CovSkSk + e2) }	Z28X
SSIM(S, S) =纨=1((μSk + 〃Sj q)(σS上 +。小⑹ )	(28)
Scale-invariant signal to noise ratio SiSNR (Le Roux et al., 2019) is widely used in speech restora-
tion literatures to compare the energy of a signal to its background noise. SiSNR is calculated
between target waveform S and waveform estimation S:
SiSNR(s, S) = 10 * log10 PaH2,	(29)
kenoise k
where Starget = <sss>S. A higher SiSNR indicates less discrepancy between the estimation and
target.
Scale-invariant spectrogram to noise ratio SiSPNR is a spectral metric similar to SiSNR. They
have the similar idea except SiSPNR is computed on the magnitude spectrogram. Given the target
spectrogram S and estimation S, the computation of SiSPNR can be formulated as
IIG l∣2
SiSPNR(S, S) = 10 * log10 k Etarg k2	(30)
kEnoise k2
一 ʌ	/ a G!、Q! 一 一. .	一一	一..	一-
where Starget = < 后∣> . The scale invariant is guranteed by mean normalization of estimated and
target spectrogram.
22
Under review as a conference paper at ICLR 2022
D Appendix D
Table 5: Experiments setup. We list the training and testing sets used for each model’s training and evluation.
Check mark is used to denote whether a model adopts the framework of VoiceFixer and whether it is trained
for SSR or GSR task.
	Name		VoiCeFixer	SSR	GSR	Training SetS		TeSting SetS	
Unprocessed	X	X	~~x-	/	DENOISE; DEREV; SR; DECLI; ALL-GSR;
Oracle-Mel	X	X	X	/	DENOISE; DEREV; SR; DECLI; ALL-GSR;
Vocoder-TFGAN	X	X	X	VCTK-Train; HQ-TTS; AISHELL-3	DENOISE; DEREV; SR; DECLI; ALL-GSR;
Denoise-UNet	X	∕	X	VCTK-Train; VD-Noise;	DENOISE; ALL-GSR;
Dereverb-UNet	X	/	X	VCTK-Train; RIR-Train;	DEREV
SR-UNet	X	/	X	VCTK-Train;	SR
Declip-UNet	X	/	X	VCTK-Train;	DECLI
NuWave	X	/	X	VCTK-Train;	SR
SEANet	X	/	X	VCTK-Train;	SR
SSPADE	X	/	X	/	DECLI
GSR-UNet	X	X	/	VCTK-Train; VD-Noise; RIR-Train;	DENOISE; DEREV; SR; DECLI; ALL-GSR;
VF-DNN	/	X	/	VCTK-Train; VD-Noise; RIR-Train;	DENOISE; DEREV; SR; DECLI; ALL-GSR;
VF-BiGRU	/	X	/	VCTK-Train; VD-Noise; RIR-Train;	DENOISE; DEREV; SR; DECLI; ALL-GSR;
VF-UNet-S	/	X	/	VCTK-Train; VD-Noise; RIR-Train;	DENOISE; DEREV; SR; DECLI; ALL-GSR;
VF-UNet	∕	X	∕	VCTK-Train; VD-Noise; RIR-Tram;	DENOISE; DEREV; SR; DECLI; ALL-GSR;
D. 1 Evaluation results
Table 6: Evaluation results on speech super-resolution test set SR, which includes five sampling rate setup.
The metrics is calculated at a target sampling rate of 44.1 kHz
TRAINING SCHEME			ONE-STAGE MODELS					VOICEFIXER MODELS					OTHERS			
SampleRate Up Ratio	Metrics	GSR- UNet	SR-UNet	NuWave	SEANet	VF- DNN	VF- BiGRU	VF- UNet-S	VF- UNet	Unprocessed	Oracle -Mel	Target
2kHz 22.1	-LSD-	~1.34	1.19	-1.41	1.33	1.18	-108~	-108-	1.05	3~13	0.89-	/
	SiSPNR	11.03	10.89	9.19	9.78	10.67	11.84	11.65	12.10	9.18	13.65	/
	SSIM	0.75	0.77	0.73	0.72	0.75	0.77	0.78	0.78	0.68	0.85	/
4kHz 11.0	-LSD-	-1.27	1.18	-135	1.24	1.15-	-10-	-104-	1.02	297	0.89-	/
	SiSPNR	11.48	11.10	9.65	10.58	11.07	12.27	11.98	12.41	9.52	13.65	/
	SSIM	0.77	0.78	0.76	0.72	0.75	0.79	0.79	0.79	0.71	0.85	/
	-LSD-	~1.21	TTi	~1.24	1.20	1.06-	^^0.99~	-101 ~	0.99	270	0.89-	/
8kHz	SiSPNR	12.07	11.82	10.73	11.11	11.94	12.68	12.34	12.74	9.93	13.65	/
5.5	SSIM	0.81	0.82	0.80	0.74	0.78	0.81	0.81	0.81	0.76	0.85	/
	MOS	3.37	3.34	3.09	3.37	/	/	/	3.40	3.05	3.53	3.63
16kHz 2.8	-LSD-	~1.10	099	~^ΓΓ8	1.16	1.01 -	-094-	-096-	0.94	232	0.89-	/
	SiSPNR	13.02	13.01	11.54	11.90	12.37	13.14	12.70	13.14	10.08	13.65	/
	SSIM	0.85	0.88	0.81	0.75	0.82	0.82	0.82	0.82	0.83	0.85	/
	-LSD-	~0.97	091	-1.12	1.15	0.93~	^^0.91-	-094-	0.92	191	-089~	/
24kHz	SiSPNR	13.96	13.81	11.63	12.58	13.21	13.38	12.86	13.38	10.40	13.65	/
1.8	SSIM	0.87	0.91	0.81	0.75	0.84	0.83	0.83	0.84	0.89	0.85	/
	MOS	3.56	3.52	3.54	3.65	/	/	/	3.41	3.47	3.44	3.45
	-LSD-	-1.18	107	~1.26	1.21	1.07-	^^0.99~	-101-	0.98	261	0.89-	/
Average Score	SiSPNR	12.31	12.13	10.55	11.19	11.85	12.66	12.31	12.75	9.82	13.65	/
	SSIM	0.81	0.83	0.79	0.74	0.79	0.80	0.81	0.81	0.77	0.85	/
Table 7: Evaluation result on the speech denoising test set
DENOISE
Table 8: Evaluation results on the speech
dereverberation test set DEREV
ModelS		SiSNR	PESQ	SiSPNR	MOS						
Unprocessed	^^8.40	1.97	9.78~	3.20	Models	PESQ	SiSPNR	MOS
Oracle-Mel	-17.52	2.85	12.84	3.64	Unprocessed	1.99-	^^14.58—	2.70
Target	/	/	/	3.69	Oracle-Mel	2.36	13.65	3.46
SEGAN (PaScual et al., 2017)	/	2.16	/	/	Target	/	/	3.51
Wave-U-Net (Macartney & Weyde, 2018)	/	2.40	/	/				
Weakly Labelled (Kong et al., 2021)	/	2.28	/	/	GSR-UNet	2.35	14.10	3.32
GSR-UNet	16.42	2.82	12.25	3.64	Dereverb-UNet	2.49	14.99	3.25
DenoiSe-UNet	17.58	2.71	11.82	3.63	VF-DNN	1.41	11.70	/
VF-DNN	/	1.71	10.93	/	VF-BiGRU	1.69	13.00	/
VF-BiGRU	/	2.29	11.72	/				
VF-UNet-S	/	2.33	11.19	/	VF-UNet-S	1.78	12.80	/
VF-UNet		/	2.43	11.71	3.69	VF-UNet	1.86	13.21	3.52
D.2 Analysis stage performance
In this section, we report the mel spectrogram restoration score on different test sets. They are used
to evaluate the performane of the analysis stage. We calculate the LSD, SiSPNR, and SSIM values.
23
Under review as a conference paper at ICLR 2022
Table 9: Evaluation results on the speech declipping test set DECLI
Clipping Level	0.25				0.10				Average			
	ModelS	SiSNR	STOI	PESQ	MOS	SiSNR	STOI	PESQ	MOS	SiSNR	STOI	PESQ	MOS
Unprocessed	9.60~	0.95	2.38	2.56	4.00~	0.89-	1.51 ~	2.72	6.80~	0.92	1.95	2.64
OraCle-Mel	-19.94	0.81	2.36	3.44	-19.94	0.81	2.36	3.42	-19.94	0.81	2.36	3.43
Target	/	/	/	3.42	/	/	/	3.49	/	/	/	3.46
GSR-UNet	11.01	0.97	3.54	3.38	7.47	0.94	2.89	3.23	9.24	0.95	3.21	3.31
Declip-UNet	12.45	0.99	3.98	3.38	8.43	0.96	3.40	3.38	10.44	0.98	3.69	3.38
SSPADE	17.43	0.98	3.55	3.34	10.31	0.92	2.12	2.63	13.87	0.95	2.84	2.98
VF-DNN	/	0.76	1.72	/	/	0.72	1.48	/	/	0.74	1.60	/
VF-BiGRU	/	0.81	2.09	/	/	0.79	1.82	/	/	0.80	1.95	/
VF-UNet-S	/	0.82	2.13	/	/	0.80	1.85	/	/	0.81	1.99	/
VF-UNet	/	0.82	2.21	3.38	/	0.80	1.93	3.38	/	0.81	2.07	3.38
The Unprocessed column is calculated using the target and unprocessed mel spectrogram. And the
Oracle-Mel column is calculated using the target spectrogram and itself.
Table 10: The Performance of Mel Spectrogram Restroation on DENOISE, DEREV, and ALL-GSR test sets
	DENOISE				DEREV					ALL-GSR			
ModelS	LSD	SiSPNR	SSIM	LSD	SiSPNR	SSIM	LSD	SiSPNR	SSIM
Unprocessed	1.31	^^-141	0.57	0.84	10.02^^	0.63	1.65	^^^-3.90	0.47
VF-DNN	0.76	7.61	0.69	0.93	8.86	0.59	0.87	6.26	0.58
VF-BiGRU	0.55	10.98	0.79	0.56	12.91	0.75	0.59	10.49	0.70
VF-UNet-S	0.52	10.29	0.82	0.47	13.61	0.82	0.55	11.08	0.75
VF-UNet	0.46	12.27	0.84	0.46	14.89	0.82	0.53	11.36	0.76
Table 10 shows that on DENOISE, DEREV, and ALL-GSR, all four VoiceFixer based models are
effective on the restoration of mel spectrogram. Among the four analysis stage models, UNet is
consistently better than the other three models.
Table 11: The performance of mel spectrogram restroation on the SR test set
SampleRate Upsampling Ratio	Metrics		MODELS						
		VF- DNN	VF- BiGRU	VF- UNet-S	VF-UNet	Unprocessed	Oracle -Mel
2 kHz 22.1	-LSD-	-080^^	068^^	~065^^	-0.60^^	299	^^000
	SiSPNR	8.02	9.62	9.82	11.32	2.54	127.43
	SSIM	0.56	0.63	0.66	0.68	0.40	1.00
4kHz 11.0	-LSD-	-068~~	054~~	-055^^	-0.50^^	254	^^000
	SiSPNR	9.66	12.23	11.22	12.83	3.16	127.43
	SSIM	0.65	0.72	0.74	0.76	0.51	1.00
8kHz 5.5	-LSD-	-051	0.40~~	-046-	-042^^	202	^^000-
	SiSPNR	12.53	14.85	12.67	14.20	4.26	127.43
	SSIM	0.77	0.82	0.83	0.84	0.64	1.00
16kHz 2.8	-LSD-	-043-	0.26-	-037-	-033-	1^53	^^000-
	SiSPNR	13.62	19.00	14.07	16.13	5.64	127.43
	SSIM	0.83	0.91	0.90	0.91	0.77	1.00
24kHz 1.8	-LSD-	-0.29—	0.18-	-031-	-027-	1.16	^^000-
	SiSPNR	17.94	22.16	15.53	18.59	7.40	127.43
	SSIM	0.92	0.95	0.94	0.95	0.86	1.00
	-LSD-	-054-	0.41-	-047-	-043-	2.05	^^000-
Average	SiSPNR	12.35	15.57	12.66	14.61	4.60	127.43
	SSIM	0.75	0.80	0.81	0.83	0.64	1.00
Table 11 lists the mel restoration performance on different sampling rates. Although VF-BiGRU has
fewer parameters than VF-UNet, it still achieved the highest score on LSD and SiSPNR averagely.
This result indicates that the recurrent structure might be more suitable for the mel spectrogram
super-resolution task when the initial sampling rate is high.
D.3 Demos
In this section, we provide some restoration demos using our proposed VoiceFixer. In Figure 12,
we provide eight restoration demos using our VF-UNet model. All the audios used in our demo are
either collected from the internet or recorded by ourselves. In each example, the left is the unpro-
cessed spectrogram, and the right is the restored one. After restoration, these seriously distorted
speech signals can be revert to relatively high quality.
24
Under review as a conference paper at ICLR 2022
Figure 10: Comparison between different restoration mothods. The unprocessed speech is noisy, reverberant,
and in low-resolution. The leftmost spectrogram is the unprocessed low-quality speech and the rightmost is
the target high-quality spectrogram. In the middle, from left to right, the figures show results processed by
one-stage SSR dereveberation model, SSR denoising model, GSR model and VoiceFixer based GSR model.
Figure 12b is the speech we recorded using Adobe Audition. We set the sampling rate to 8 kHz
and manually add the clipping effect. It also contains some low-frequency noise and reverberation
introduced by the recording device and environment. Figure 12a is a speech delivered by Amelia
Earhart3, 1897-1937, appeared in the Library of Congress, United States. The original version
sounds like a mumble. Figure 12f comes from an interview in a TV news program, which includes
multiple distortions. Figure 12e is collected from the audio uploaded by a Youtuber4. Probably
due to the recording device, her speech is deteriorated seriously by noise, and the energy of speech
in the low-frequency part is also relatively low. Figure 12c is the restoration of a Chinese famous
old movie railroad guerrilla5, which speech has limited bandwidth. The audio in Figure 12d is
selected from a well-known TV series in China, romance of the three kindoms6, in which some
parts of the spectrogram are masked off due to the previous audio compression. Figure 12g is a
recording7 selected from a speech delivered by Sun Yat-sen, 1866-1925, which is in extremely low-
resolution and includes multiple unknown distortions. Figure 12h shows the result of a subway
broadcasting we recorded in Shanghai. The low-frequency part of speech almost lost completely,
and the reverberation is serious.
To sum up, all these examples showcase the effectiveness of the VoiceFixer model on GSR. And
to our surprise, it can generate will on unseen distortions such as the spectrogram lost in Fig-
ure 12c, Figure 12f, and Figure 12d. In addition, Figure 12e shows that VoiceFixer is effective
for the compensation of low-frequency energy, making speech sound less machinery and distant.
Last but not least, despite the abnormal harmonic structure in the low-frequency part in Figure 12g,
our proposed model can still restore its sound, which display the advantages of utilizing the prior
knowledge of vocoder.
3https://www.loc.gov/item/afccal000004
4https://v.ixigua.com/egVW74E/
5https://www.youtube.com/watch?v=R8lY1qn2CHA
6https://www.youtube.com/watch?v=6h7N4Cl1lTw
7https://www.bilibili.com/video/BV1WW411V7fR
25
Under review as a conference paper at ICLR 2022
(b) Speech super-resolution results on 8 kHz source samplerate test data.
(c) Speech super-resolution results on 24 kHz source samplerate test data.
Noisy	GSR-UNet	Denoise-UNet	VF-UNet	Oracle-Mel	Target
(d) Speech denoising results.
Clipped	GSR-UNet	Declip-UNet	SSPADE	VF-UNet	Oracle-Mel	Target
(e) Speech declipping results on speech with 0.1 clipping threshold.
(f) Speech declipping results on speech with 0.25 clipping threshold.
(g) Speech dereverberation results.
Figure 11: Comparison between different model on four different tasks using simulated data.
26
Under review as a conference paper at ICLR 2022
Restored Mel Spectrogram
Original Mel Spectrogram

Original Spectrogram	Restored Spectrogram
(c) Old movie
(e) A recording of a Chinese Youtuber
(d) Old TV series
(g) Historical speech
Figure 12: Restoration on the audios either collected from the internet or recorded by ourselves.
(f) Interview in a TV news program
(h) Subway broadcasting
27