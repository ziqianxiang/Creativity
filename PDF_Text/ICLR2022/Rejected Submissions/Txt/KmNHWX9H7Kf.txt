Under review as a conference paper at ICLR 2022
Uniform Generalization Bounds for Overpa-
rameterized Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
An interesting observation in artificial neural networks is their favorable gener-
alization error despite typically being extremely overparameterized. It is well
known that the classical statistical learning methods often result in vacuous gen-
eralization errors in the case of overparameterized neural networks. Adopting the
recently developed Neural Tangent (NT) kernel theory, we prove uniform gener-
alization bounds for overparameterized neural networks in kernel regimes, when
the true data generating model belongs to the reproducing kernel Hilbert space
(RKHS) corresponding to the NT kernel. Importantly, our bounds capture the ex-
act error rates depending on the differentiability of the activation functions. In
order to establish these bounds, we propose the information gain of the NT kernel
as a measure of complexity of the learning problem. Our analysis uses a Mercer
decomposition of the NT kernel in the basis of spherical harmonics and the decay
rate of the corresponding eigenvalues. As a byproduct of our results, we show
the equivalence between the RKHS corresponding to the NT kernel and its coun-
terpart corresponding to the Matern family of kernels, showing the NT kernels
induce a very general class of models. We further discuss the implications of our
analysis for some recent results on the regret bounds for reinforcement learning
and bandit algorithms, which use overparameterized neural networks.
1	Introduction
Neural networks have shown an unprecedented success in the hands of practitioners in several fields.
Examples of successful applications include classification (Krizhevsky et al., 2012; LeCun et al.,
2015) and generative modeling (Goodfellow et al., 2014; Brown et al., 2020). This practical effi-
ciency has accelerated developing analytical tools for understanding the properties of neural net-
works. A long standing dilemma in the analysis of neural networks is their favorable generaliza-
tion error despite typically being extremely overparameterized. It is well known that the classical
statistical learning methods for bounding the generalization error, such as Vapnik-Chervonenkis
(VC) dimension (Bartlett, 1998; Anthony & Bartlett, 2009) and Rademacher complexity (Bartlett
& Mendelson, 2002; Neyshabur et al., 2015), often result in vacuous bounds in the case of neural
networks (Dziugaite & Roy, 2017).
Motivated with the problem mentioned above, we investigate the generalization error in overparam-
eterized neural networks. In particular, let Dn = {(xi , yi)}in=1 be a possibly noisy dataset collected
from a true model f: yi = f (Xi) + ei, where ∈i are well behaved zero mean noise terms. Let f
be the learnt model using a gradient based training of an overparameterized neural network on Dn .
We are interested in bounding |f (x) - fn(x)| uniformly at all test points x belonging to a particular
domain X .
A recent line of work has shown that gradient based learning of certain overparameterized neural
networks can be analyzed using a projection on the tangent space at random initialization. This leads
to the equivalence of the solution of the neural network model to a kernel ridge regression, that is a
linear regression in the possibly infinite dimensional feature space of the corresponding kernel. The
particular kernel arising from this approach is called the neural tangent (NT) kernel (Jacot et al.,
2018). This equivalence is based on the observation that, during training, the parameters of the
neural network remain within a close vicinity of the random initialization. This regime is sometimes
referred to as lazy training (Chizat et al., 2019).
1
Under review as a conference paper at ICLR 2022
1.1	Contributions
Here, we summarize our contributions. We propose the maximal information gain (MIG), denoted
by γk (n), of the NT kernel k, as a measure of complexity of the learning problem for overparam-
eterized neural networks in kernel regimes. The MIG is closely related to the effective dimension
of the kernel (for the formal definition of the MIG and its connection to the effective dimension of
the kernel, see Section 4). We provide kernel specific bounds on the MIG of the NT kernel. Using
these bounds, for networks with infinite number of parameters, we analytically prove novel gener-
alization error bounds decaying with respect to the dataset size n. Furthermore, our results capture
the dependence of the decay rate of the error bound on the smoothness properties of the activation
functions.
Our analysis crucially uses Mercer’s theorem and the spectral decomposition of the NT kernel in
the basis of spherical harmonics (which are special functions on the surface of the hypersphere; see
Section 3). We derive bounds on the decay rate of the Mercer eigenvalues (referred to hereafter
as eigendecay) based on the differentiability of the activation functions. These eigendecays are
obtained by careful characterization of a recent result relating the eigendecay to the asymptotic
endpoint expansions of the kernel, given in (Bietti & Bach, 2020). In particular, we consider
s - 1 times differentiable activation functions as(u) = (max(0, u))s, s ∈ N. We show that, with
these activation functions and a d dimensional hyperspherical support, the eigenvalues λi of the
d+2s-2
corresponding NT kernel decay at the rate λi 〜i	d-1 1 (UP to certain parity constraints on i in
special cases; see Section 2 for details).
We then Use the eigendecay of the NT kernel to establish novel boUnds on its MIG, which are of the
d-1
form γk(n) = O(nd+2s-2). Then, we show this implies high probability error bounds of the form
-2s + 1
suPχ∈x If (x) — fn(x)l = O(n2d+4s-4), when f belongs to the corresponding reproducing kernel
Hilbert space (RKHS), and Dn is distributed over X in a sufficiently informative way. In particular,
we consider a Dn that is collected sequentially in a way that maximally reduces the uncertainty in
the model. That results in a quasi-uniform dataset over the entire domain X (see Section 5.2 for
details). A summary of our results on MIG and error bounds is reported in Table 1.
As a byproduct of our results, we show that the RKHS of the NT kernel with activation function
as(.) is equivalent to the RKHS of a Matern kernel with smoothness parameter V = S - 1. This
result was already known for a special case. In particular, Geifman et al. (2020); Chen & Xu (2021)
recently showed the equivalence between the RKHS of the NT kernel with ReLU activation func-
tion, aι(.), and that of the Matern kernel with V = 1 (also known as the Laplace kernel). Our
contribution includes generalizing this result to the equivalence between the RKHS of the NT kernel
under various activation functions and that of a Matern kernel with the corresponding smoothness
(see Section 3.2). We note that the members of the RKHS OfMatern family ofkernels can uniformly
approximate almost all continuous functions (Srinivas et al., 2010). Therefore, our results suggest
that the NT kernels induce a very general class of models.
As an intermediate step, we also consider the simpler random feature (RF) model which is the result
of partial training of the neural network. Also referred to as conjugate kernel or neural networks
Gaussian process (NNGP) (Cho & Saul, 2009; Daniely et al., 2016; Lee et al., 2018; de G. Matthews
et al., 2018), this model corresponds to the case where only the last layer of the neural network is
trained (see Section 2 for details).
Table 1: The bounds on MIG and generalization error for RF and NT kernels, with as (.) activation functions,
when the true model f belongs to the RKHS of the corresponding kernel and is supported on the hypersphere
Sd-1 ⊂ Rd.
Kernel	MIG, Yk (n)	1-1	i	t Ir/ ∖	P /	∖ I Error bound, |f (x) - fn (x)|
NT	d	d—1	,	,	, ,	2s — 1	、	八 -	-2s + 1	,1	/	∖ ∖ d+4s-3、
	O ( nd+2s-2 (log(n)) d+2s-2 )	O (n2d+4s-4 (log(n)) 2d+4s-4 )
RF	d-1	2s + 1、 O ( n d+2s (log(n)) d+2s )	-2s-1	d+4s + 1、 O (n 2d+4s (log(n)) 2d+4s )
1The general notations used in this paper are defined in Appendix A.
2
Under review as a conference paper at ICLR 2022
We note that the decay rate of the error bound is faster in the case of RF kernel with the same
activation function. Also, the decay rate of the error bound is faster when s is larger, for both NT
and RF kernels. In interpreting these results, we should however take into account the RKHS of the
corresponding kernels. The RKHS of the RF kernel is smaller than the RKHS of the NT kernel.
Also, when s is larger, the RKHS is smaller for both NT and RF kernels.
In Section 6, we provide experimental results on the error rate for synthetic functions belonging to
the RKHS of the NT kernel, which are consistent with the analytically predicted behaviors.
Our bounds on MIG may be of independent interest. In Section 7, we comment on the implications
of our results for the regret bounds in reinforcement learning (RL) and bandit problems, which use
overparameterized neural network models (e.g., Yang et al., 2020; Gu et al., 2021).
2 Overparameterized Neural Networks in Kernel Regimes
In this section, we briefly overview the equivalence of overparameterized neural network models
to kernel methods. It has been recently shown that, in overparameterized regimes, gradient based
training of a neural network reaches a global minimum, where the weights remain within a close
vicinity of the random initialization (Jacot et al., 2018). In particular, let f(x, W) denote a neural
network with a large width m parameterized by W . The model can be approximated with its linear
projection on the tangent space at random initialization W0 , as
f (x, W) ≈ f (x, Wo) + hW — Wo, Vwf (x, W。)).
The approximation error can be bounded by the second order term ξmkW - W0k2, and shown to be
diminishing as m grows, where ξm is the spectral norm of the Hessian matrix of f (Liu et al., 2020).
The approximation becomes an equality when the width of the hidden layers approaches infinity.
Known as lazy training regime (Chizat et al., 2019), this model is equivalent to the NT kernel (Jacot
et al., 2018), given by
k(x, x0) = lim hVWf(x,Wo),VWf(x0,Wo)i.
m→∞
2.1	NEURAL KERNELS FOR A 2 LAYER NETWORK
Consider the following 2 layer neural network with width m and a proper normalization
f(x,W)=
(1)
where a(.) is the activation function, c is a constant, x ∈ X is a d dimensional input to the net-
work, wj ∈ Rd are the weights in the first layer, and vj ∈ R are the weights in the second layer.
The weights wj are initialized randomly according to N (0d, Id) and vj are initialized randomly
according to N(0, 1). Then, the NT kernel of this network can be expressed as (Jacot et al., 2018)
kNτ(x,x0) = c2(x>x0)Ew 〜N (Od,id)[a0(w>x)a0(w>x0)]+ c2Ew 〜N。,心)[a(w>x)a(w>x0)]. (2)
When wj are fixed at initialization, and only vj are trained with `2 regularization, the model is
equivalent to a certain Gaussian process (GP) (Neal, 2012) that is often referred to as a random
feature (RF) model. The RF kernel is given as (Rahimi et al., 2007)
k(x, x0) = c2Ew〜N(0d,id) [a(w>x)a(w>x0)].	(3)
Note that the RF kernel is the same as the second term in the expression of the NT kernel given
in equation 2.
2.2	Rotational Invariance
When the input domain X is the hypersphere Sd-1, by the spherical symmetry of the Gaussian
distribution, the neural kernels are invariant to unitary transformations2 and can be expressed as
2Rotationally invariant kernels are also sometimes referred to as dot product or zonal kernels.
3
Under review as a conference paper at ICLR 2022
a function of x>x0. We thus use the notations κNT,s (x>x0) and κs (x>x0) to refer to the NT and
RF kernels, as defined in equation 2 and equation 3, respectively. In this notation, s specifies the
smoothness of the activation function, as (u) = (max(0, u))s .
The particular form of the neural kernels depends on s. For example, for the special case of s = 1,
which corresponds to the ReLU activation function, the following closed form expressions can be
derived from the expressions given in equation 2 and equation 3.
u
KNTj (U) = — (π — arccos(u)) + κ1 (u),
κ1 (U) = — ^u(π — arccos(u)) + pl - u2).
In deriving the expressions above, the constant C in equations 2 and 3 is set to ʌ/ɪ In general, We
choose c2 = (2s-1χ, that normalizes the maximum value of the RF kernel to 1: κs(1) = 1, ∀s ≥ L
2.3	Neural Kernels for Multilayer Networks
For an l > 2 layer network, the NT kernel is recursively given as (see, Jacot et al., 2018, Theorem 1)
κlNT,s(U) = c2κlN-T1,s(U)κ0s(κl-1(U)) +κls(U),
κls (U) = κs (κls-1 (U)).	(4)
Here, κls (.) corresponds to the RF kernel of an l layer network, as shown in Cho & Saul (2009);
Daniely et al. (2016); Lee et al. (2018); de G. Matthews et al. (2018). In our notation, when l = 2,
we drop the layer index l from the superscript, as in the previous subsection. An expression for the
derivative κ0s(.) of the RF kernel in a 2 layer network, based on κs-1, is given in Lemma 1. That is
needed to see the equivalence of the expressions in equation 4 to the one given in Theorem 1 of Jacot
et al. (2018).
3 The RKHS of the Neural Kernels
When the domain is Sd-1, as discussed ear-
lier, the neural kernels have a rotationally in-
variant (dot product) form. The rotationally in-
variant kernels can be diagonalized in the basis
of spherical harmonics, which are special func-
tions on the surface of the hypersphere. Specif-
ically, the Mercer decomposition of the kernels
(for all l ≥ 2 and s ≥ 1) can be given as
∞ Nd,i
κ(x>x0) = E E λiφi,j(x)φi,j(x0),
$3,1
⅛.c~≠3,已
M ∙⅛,∙Γu⅛,<3
⅛.c⅛,。一⅛.r
where the eigenfunction φi,j∙ is the j ,th spher-
ical harmonic of degree i,λi is the corre-
sponding eigenvalue, Nd,i = 2i+d-2 (i+--3)
is the multiplicity ofλi (meaning the number
of spherical harmonics of degree i, which all
share the same eigenvalue λi), and the conver-
Figure 1: The spherical harmonics of degrees i =
1, 2, 3, 4, on S2 . The function value is given by color.
gence of the series holds uniformly on X ×X. As a consequence of Mercer’s representation theorem,
the corresponding RKHS can be written as
(∞ Nd,i	∞ Nd,i
f (∙) = XX wi,jλ i2 φi,j(・广 Wij ∈ R,11f||HK = XX wi2,j < ∞
i=1 j=1	i=1 j=1
Formal statements of Mercer’s theorem and Mercer’s representation theorem, as well as a formal
definition of Mercer eigenvalues and eigenfunctions are given in Appendix B.
4
Under review as a conference paper at ICLR 2022
-c-r τ	.l	.	A ∕∙	,	, , T ∖ ,	1	,	,1	1	∙	1	F 1	1	1	,1
We use the notation λi (in contrast to λi) to denote the decreasingly ordered eigenvalues, when their
i0 -1	i0
multiplicity is taken into account (for all i :	i00=1 Nd,i00 < i ≤	i00=1 Nd,i00, we define λi = λi0).
3.1 The Decay Rate of the Eigenvalues
As discussed, both RF and NT kernels can be represented in the basis of spherical harmonics. Our
analysis of the MIG relies on the decay rate of the corresponding eigenvalues. In this section, we
derive the eigendecay depending on the particular activation functions.
In the special case of ReLU activation function, several recent works (Geifman et al., 2020; Chen
& Xu, 2021; Bietti & Bach, 2020) have proven a λ 〜i-d eigendecay for the NT kernel on the
hypersphere Sd-1. In this work, we use a recent result from Bietti & Bach (2020) to derive the
eigendecay based on the differentiability properties of the activation functions.
Proposition 1 Consider the neural kernels on the hypersphere Sd-1, and their Mercer decomposi-
tion in the basis of spherical harmonics. Then, the eigendecays are as reported in Table 2.
Table 2: The eigendecays of the neural kernels.
NT	l=2	〜	ɪ Q i Q	_ d+2s _ 2 For i of the same parity of s: λ,〜i-d-2s+2, λ,〜i	d-^~ For i of the opposite parity of s: λi = o(i-d-2s+2), λi = o(i	d-1 )
	l>2	λi 〜i-d-2s+2, λi 〜厂d+d2s-2
	l=2	d+2s For i of the opposite parity of s: λ,〜i-d-2s, λi 〜i- d-1
RF		d+2s For i of the same parity of s: λi = o(i-d-2s), λ, = o(i- d-1 )
	l>2	d+2s λi 〜i-d-2s, λi 〜i-K
Proof Sketch. The proof follows from Theorem 1 of Bietti & Bach (2020), which proved that the
eigendecay of a rotationally invariant kernel κ(.) : [-1, 1] → R can be determined based on its
asymptotic expansions around the endpoints, ±1. In particular, when
κ(1 - t) = p+1 (t) + c+1tθ + o(tθ),
κ(-1 + t) = p-1 (t) + c-1tθ + o(tθ),
for polynomial p± 1 and non-integer θ, the eigenvalues decay at the rate λ 〜 (c+1 + c-1)i-d-2θ+1,
for even i, and % 〜(c+ι - c-ι)i-d-2θ+1, for odd i. A formal statement of this result is given
in Appendix E. They derived these expansions for the case of the ReLU activation function. For
completeness, we derive the asymptotic endpoint expansions for the neural kernels with s - 1 times
differentiable activation functions, in the case of l = 2 and l > 2 layer networks. Our derivations is
based on the following Lemma.
Lemma 1 For the RF kernel defined in equation 3, with activation function a§ (.), and C = ⑵-])!!,
we have
0	s2
Ks(U) = ʒ	^Ks-I(U).
2s - 1
The proof is given in Appendix D. Lemma 1 allows us to recursively derive the endpoint expansions
of Ks from those of Ks-1, through integration. We thus obtain θ and c±1 for s > 1, when l = 2,
using a recursive relation over s. We then use the compositional forms given in equation 4 to obtain
the same, when l > 2, resulting in the eigendecays reported in Table 2.
Remark 1 The parity constraints in Table 2 may imply undesirable behavior for 2 layer infinitely
wide networks. Ronen et al. (2019) studied this behavior and showed that adding a bias term
removes these constraints, for the NT kernel. Although the parity constrained eigendecay affects the
5
Under review as a conference paper at ICLR 2022
representation power of the neural kernels, it does not affect our analysis of MIG and error rates,
Sincefor that analysis we only use λi = O(i-d-2s+2).
Remark 2 We note that our results do not necessarily apply to deep neural networks in the sense
that the implied constants in Table 2 grow exponentially in l, for s > 1 (see Appendix E for the
expressions of the constants). When s = 1, however, the constants grow at most as fast as l2 (as
shown in Bietti & Bach, 2020).
3.2 Equivalence of the RKHSS OF the Neural and MATERN kernels
Our bounds on the eigendecay of the neural kernels show the connection between the RKHS of the
neural kernels, and that ofMetern family ofkernels. Let HkV denote the RKHS of the Matern kernel
with smoothness parameter ν .
Theorem 1 On a hyperspherical domain, when l > 2, H l	(Hκl ) is equivalent to Hk , and, when
NT,s	s	ν
l = 2, HKl	⊂ Hk (HKl ⊂ Hk ), where V = S 一 1 (V = S + 1). The statement is summarized in
NT,s	ν	s	ν	2	2
Table 3.
Table 3: The relation between the RKHSs corresponding to the neural and Matern kernels.
Kernel	l = 2	l>2
NT	Hknt,s ⊂ Hks 1	Hknt,s ≡ Hks 1
RF	HKsU Hks+J	HKs ≡ Hks+J
The formal definition of equivalence (≡) and subset (⊂) relation between two normed spaces is
given in Appendix A.
Proof Sketch. The proof follows from the eigendecay of the neural kernels given in Table 2, the
eigendecay of the Matern family of kernels on manifolds proven in Borovitskiy et al. (2020), and
the Mercer’s representation theorem. Details are given in Appendix F.
Remark 3 The observations that when l = 2, we only have HKlT ⊂Hk ι, and not Vice versa, is
a result of parity constraints in Table 2. The subset relation can change to equivalence, with a bias
term which removes the parity constraints (see, Ronen et al., 2019).
A special case of Theorem 1, when S = 1, was recently proven in (Geifman et al., 2020; Chen &
Xu, 2021). This special case pertains to the ReLU activation function and the Matern kernel with
smoothness parameter V = 2, that is also referred to as the Laplace kernel.
Remark 4 It is known that the RKHS ofa Matern kernel with smoothness parameter V is equivalent
to a Sobolev space with parameter V + d (e.g., see, Kanagawa et al., 2018, Section 4). This obser-
vation provides an intuitive interpretation for the RKHS of the neural kernels as a consequence of
Theorem 1. That is kf kKl (kf kKl ) is proportional to the cumulative L2 norm of the weak deriva-
tives of f up to s + d-1 (s + d+1) order. I.e., f ∈ HKlT (f ∈ HKlS) translates to the existence of
weak derivatives of f up to S + d-1 (S + d++1) order, which can be understood as a versatile measure
for the smoothness of f controlled by S. For the details on the definition of weak derivatives and
Sobolev spaces see, e.g., Hunter & Nachtergaele (2011).
4	Maximal Information Gain of the Neural Kernels
In this section, we use the eigendecay of the neural kernels shown in the previous section to de-
rive bounds on their information gain, which will then be used to prove uniform bounds on the
generalization error.
6
Under review as a conference paper at ICLR 2022
4.1	Information Gain: a Measure of Complexity
To define information gain, we need to introduce a fictitious GP model. Assume F is a zero mean
GP indexed on X with kernel k. Information gain then refers to the mutual information I(Yn ; F)
between the data values Yn = [yi]in=1 and F. From the closed from expression of mutual information
between two multivariate Gaussian distributions (Cover, 1999), it follows that
I(Yn；F) = Ilogdet (In + JKn
Here Kn is the kernel matrix [Kn]i,j = k(xi, xj), and λ > 0 is a regularization parameter. We also
define the data independent and kernel specific maximal information gain (MIG) as follows.
γk(n) = sup I(Yn; F).	(5)
Xn⊂X
The MIG is also closely related to the effective dimension of the kernel. In particular, although
the feature space of the neural kernels are infinite dimensional, for a finite dataset, the number of
features with a significant impact on the regression model can be finite. The effective dimension of
a kernel is often defined as (Zhang, 2005; Valko et al., 2013)
dk(n) = Tr (Kn(Kn + λ2In)T) .	(6)
It is known that the information gain and the effective dimension are the same up to logarithmic
.	..	.-	-	≈ Z	-	一、	一 一，- -	-、	._，：，、、	，、、一-	-...	.
factors. Specifically, dk(n) ≤ I(Yn,; F), and I(Yn1; F) = O(dk(n)log(n)) (CaIandnello et al.,
2019).
Relating the information gain to the effective dimension provides some intuition into why it can
capture the complexity of the learning problem. Roughly speaking, our bounds on the generalization
error are analogous to the ones for a linear model, which has a feature dimension of dk(n).
4.2	B ounding the Information Gain of the Neural Kernels
In the following theorem, we provide a bound on the maximal information gain of the neural kernels.
Theorem 2 For the neural kernels with all l ≥ 2, on a hyperspherical domain X = Sd-1, we have
d- 1	2s — 1
YKlT (n) = O ( nd+2s-2 (log(n))d+2s-2 ) , in the case ofNTkernel,
Yκ∣ (n) = O (n d+2s (log(n)) d⅛) , in the case of RF kernel.
A special case of our Theorem 2, for the special case of the ReLU activation function, was recently
proved in Kassraie & Krause (2021).
Proof Sketch. The main components of the analysis include the eigendecay given in Proposition 1,
a projection on a finite dimensional RKHS technique proposed in Vakili et al. (2021b), and a bound
on the sum of spherical harmonics of degree i determined by the Legendre polynomials, that is
sometimes referred to as the Legendre addition theorem (MaIecek & Nadenk 2001). Details are
given in Appendix G.
5 Uniform Bounds on the Generalization Error
In this section, we use the bound on MIG from previous section to establish uniform bounds on the
generalization error.
5.1	Implicit Error Bounds
We first overview the implicit (data dependent) error bounds for the kernel methods under the fol-
lowing assumptions.
7
Under review as a conference paper at ICLR 2022
Assumption 1 Assume the true data generating model f is in the RKHS corresponding to a neural
kernel κ. In particular, kf kHκ ≤ B, for some B > 0. In addition, assume that the observation
noise “ are independent R Sub-Gaussian random variables. That is E[exp(ηeJ] ≤ exp(ɪR-),
∀η ∈ R, ∀i ≥ 1, where yi = f(xi) + i, ∀i ≥ 1.
Under Assumption 1, for a fixed x ∈ X, we have, with probability at least 1 -δ (Vakili et al., 2021a),
|f(x) - fn(x)l ≤ β(δ)σn(x),	⑺
where fn(x) = k>(χ)(Kn + λ2In)-1Yn is the solution to the kernel ridge regression using K
and Dn, σ/(x) = k(x,x) - k>(X)(Kn + λ2In)-1kn(x), k>(x) = [κ(x>xi)]n=ι, and β(δ)=
B + R J2i0g(2). Equation 7 provides a high probability bound on |f (x) - fn(x) |. The decay rate
of this bound based on n, however, is not explicit.
5.2	Explicit Error Bounds
In this section, we use the bounds on MIG and the implicit error bounds from the previous subsection
to provide explicit (in n) bounds on the error. It is clear that with no assumption on the distribution
of the data, generalization error cannot be nontrivially bounded. For example, if all the data points
are collected from a small region of the input domain, it is not expected for the error to be small
far from this small region. We here consider a dataset that is distributed over the input space in a
sufficiently informative way. For this purpose, we introduce a data collection module which collects
the data points based on the current uncertainty level of the kernel model. In particular, consider a
dataset Dn collected as follows: xi = arg maxx∈X σi-1(x), where σi(.) is defined above. We have
the following result.
Theorem 3 Consider the neural kernels with l ≥ 2, on a hyperspherical domain X = Sd-1. Con-
sider a model fn trained on Dn. Under Assumption 1, with probability at least 1 - δ, uniformly in
x ∈ X,
-2s + 1	2s-1	nd—1 1 ∖
|f (x) — fn(x)∣ = O ( n 2d+4s-4 (log(n)) 2d+4s-4 (log(—-—))- I , in the case of NT kernel,
|f (x) — fn(x)∖ = O (n 2-+4≡ (log(n)) -d++4s (log( n^—))1) , in the case of RF kernel. (8)
Proof Sketch. The proof follows the same steps as in the proof of Theorem 3 of Vakili et al.
(2021a). The key step is bounding the total uncertainty in the model with the information gain
that is Pn=ι σ2-ι(xi) ≤	L)I(Yn; F) (Srinivas et al., 2010). This allows US to bound the σn
in the right hand side of equation 7 by ,γκ(n)∕n UP to multiplicative absolute constants. Plugging
in the bounds on γκ (n) from Theorem 2, and using the implicit error bound given in equation 7
(with a union bound on a discretization of the domain with size O(nd-1)), we obtain equation 8. A
detailed proof is given in Appendix H.
1 £ 1 √>
where kn0 and K
6	Experiments
In this section, we provide experimental results on the error rates. In our experiments, we create
synthetic data from a true model f that belongs to the RKHS ofa NT kernel κ. For this purpose, we
create two random vectors Xn0 ∈ Xn0 and Yn0 ∈ Rn0, and let f(.) = kn>0 (.)(Kn0 + δ2In0)-1Yn0,
n0 are defined similar to kn and Kn in Subsection 5.1. We then generate datasets
Dn of various sizes n = 2i, with i = 1, 2, . . . , 13, according to the underlying model f. We train
the model to obtain fn. Figure 2 shows the error rate versus the size n of the dataset for various d
and s. The experiments show that the error converges to zero at a rate satisfying the bounds given
in Theorem 3. In addition, as analytically predicted, the absolute value of the error rate exponent
increases with s and decreases with d. Our experiments use the neural-tangents library (Novak et al.,
2019). See Appendix I for more details.
8
Under review as a conference paper at ICLR 2022
Figure 2: Left: the exponent of the error rate versus s and d. As expected, larger values of s and d
result in, respectively, faster and slower error decays. The bars show the standard deviation. Right:
error rates for s = 1, 2, 3 and d = 2, 3, 4, shown in separate panels. Both axes are in log scale so that
the slope of the line represents the exponent of the error rate. Experimental error rates are consistent
with the analytically predicted results.
7	Discussion
Our bounds on MIG may be of independent interest in other problems. Recent works on RL and
bandit problems, which use overparameterized neural network models, derived an O(Yk (n)√n)
regret bound (see, e.g., Zhou et al., 2020; Yang et al., 2020; ZHANG et al., 2021; Gu et al., 2021).
These works however did not characterize the bound on γk(n), leaving the regret bounds implicit.
A consequence of our bounds on γk(n) is an explicit (in n) regret bound for these RL and bandit
problems. Our results however have mixed implications by showing that the existing regret bounds
are not necessarily sublinear in n. In particular, plugging in our bound on γκNT s (n) into the existing
1.5d + s-2	d ，
regret bounds, We get O(n d+2s-2 ), which is sublinear only when s > d (that, e.g., excludes the
ReLU activation function). Our analysis of various activation functions is thus essential in showing
sublinear regret bounds for at least some cases. As recently formalized in Vakili et al. (2021c), it
remains an open problem whether the analysis of RL and bandit algorithms in kernel regimes can
improve to have an always sublinear regret bound. We note that this open problem and the analysis
of RL and bandit problems were not considered in this work. Our remark is mainly concerned with
the consequences of our bound on MIG, which appears in the regret bounds.
The recent work of Wang et al. (2020) proved that the L2 norm of the error is bounded as kf -
fn∣∣L2 = O(n- 2d-1), for a two layer neural network with ReLU activation functions in kernel
regimes. Bordelon et al. (2020) decomposed the L2 norm of the error into a series corresponding to
eigenfunctions of the NT kernel and provided bounds based on the corresponding eigenvalues. Our
results are stronger as they are given in terms of absolute error instead of L2 norm. In addition, we
provide explicit error bounds depending on differentiability of the activation functions.
The MIG has been studied for popular GP kernels such as Matern and Squared Exponential (Srinivas
et al., 2010; Vakili et al., 2021b). Our proof technique is similar to that of Vakili et al. (2021b).
Their analysis however does not directly apply to the NT kernel on the hypersphere. The reason
is that Vakili et al. (2021b) assume uniformly bounded eigenfunctions. In our analysis, we use a
Mercer decomposition of the NT kernel in the basis of spherical harmonics. Those are not uniformly
bounded. Technical details are provided in the analysis of Theorem 2. Applying a proof technique
similar to Srinivas et al. (2010) results in suboptimal bounds in our case.
Our work may be relevant to the recent developments in the field of implicit neural representations
(Mildenhall et al., 2020; Sitzmann et al., 2020; Fathony et al., 2020). In this line of work, the input to
a neural network often represents the coordinates ofa plane (image), a camera position or angle, and
the function to be approximated is the image or the scene itself. Interestingly, it has been observed
that smooth activation functions perform better than ReLU in these applications, since the models
are also smooth (Sitzmann et al., 2020).
9
Under review as a conference paper at ICLR 2022
References
Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. cambridge
university press, 2009.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of op-
timization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pp. 322-332. PMLR, 2019.
Peter L Bartlett. The sample complexity of pattern classification with neural networks: the size of
the weights is more important than the size of the network. IEEE transactions on Information
Theory, 44(2):525-536, 1998.
Peter L Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Alberto Bietti and Francis Bach. Deep equals shallow for ReLU networks in kernel regimes. arXiv
preprint arXiv:2009.14397, 2020.
Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves in
kernel regression and wide neural networks. In International Conference on Machine Learning,
pp. 1024-1034. PMLR, 2020.
Viacheslav Borovitskiy, Alexander Terenin, Peter MostoWsky, and Marc Deisenroth. Matem Gaus-
sian processes on Riemannian manifolds. In Advances in Neural Information Processing Systems,
volume 33, pp. 12426-12437, 2020.
James Bradbury, Roy Frostig, Peter HaWkins, MattheW James Johnson, Chris Leary, Dougal
Maclaurin, and Skye Wanderman-Milne. Jax: composable transformations of python+ numpy
programs. Version 0.1, 55, 2018. URL https://github.com/google/jax.
Tom B BroWn, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla DhariWal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
feW-shot learners. arXiv preprint arXiv:2005.14165, 2020.
Daniele Calandriello, Luigi Carratino, Alessandro Lazaric, Michal Valko, and Lorenzo Rosasco.
Gaussian process optimization With adaptive sketching: scalable and no regret. In Proceedings
of the Thirty-Second Conference on Learning Theory, volume 99 of Proceedings of Machine
Learning Research, Phoenix, USA, 25-28 Jun 2019. PMLR.
Lin Chen and Sheng Xu. Deep neural tangent kernel and Laplace kernel have the same RKHS. In
International Conference on Learning Representations, 2021.
Louis HY Chen, Larry Goldstein, and Qi-Man Shao. Multivariate normal approximation. In Normal
Approximation by Stein’s Method, pp. 313-341. Springer, 2011.
LenaIc Chizat, EdoUard Oyallon, and Francis Bach. On lazy training in differentiable programming.
In Advances in Neural Information Processing Systems, volume 32, 2019.
Youngmin Cho and LaWrence Saul. Kernel methods for deep learning. In Advances in Neural
Information Processing Systems, volume 22, 2009.
Sayak Ray ChoWdhury and Aditya Gopalan. On kernelized multi-armed bandits. In International
Conference on Machine Learning, pp. 844-853, 2017.
Thomas M Cover. Elements of information theory. John Wiley & Sons, 1999.
Amit Daniely, Roy Frostig, and Yoram Singer. ToWard deeper understanding of neural netWorks:
The poWer of initialization and a dual vieW on expressivity. Advances In Neural Information
Processing Systems, 29:2253-2261, 2016.
Alexander G. de G. MattheWs, Jiri Hron, Mark RoWland, Richard E. Turner, and Zoubin Ghahra-
mani. Gaussian process behaviour in Wide deep neural netWorks. In International Conference on
Learning Representations, 2018.
10
Under review as a conference paper at ICLR 2022
Vincent Dutordoir, Nicolas Durrande, and James Hensman. Sparse gaussian processes with spherical
harmonic features. In International Conference on Machine Learning, pp. 2793-2802. PMLR,
2020.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. arXiv preprint
arXiv:1703.11008, 2017.
Rizal Fathony, Anit Kumar Sahu, Devin Willmott, and J Zico Kolter. Multiplicative filter networks.
In International Conference on Learning Representations, 2020.
Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, and Basri Ronen. On
the similarity between the Laplace and neural tangent kernels. In Advances in Neural Information
Processing Systems, volume 33, pp. 1451-1461, 2020.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information
processing systems, 27, 2014.
Quanquan Gu, Amin Karbasi, Khashayar Khosravi, Vahab Mirrokni, and Dongruo Zhou. Batched
neural bandits. arXiv preprint arXiv:2102.13028, 2021.
John K. Hunter and Bruno Nachtergaele. Applied Analysis. World Scientific, 2011.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gener-
alization in neural networks. In Advances in Neural Information Processing Systems, volume 31,
2018.
Motonobu Kanagawa, Philipp Hennig, Dino Sejdinovic, and Bharath K Sriperumbudur. Gaussian
processes and kernel methods: A review on connections and equivalences. Available at Arxiv.,
2018.
Parnian Kassraie and Andreas Krause. Neural contextual bandits without regret. arXiv preprint
arXiv:2107.03144, 2021.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep con-
volutional neural networks. Advances in neural information processing systems, 25:1097-1105,
2012.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436-444,
2015.
Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and
Yasaman Bahri. Deep neural networks as Gaussian processes. In International Conference on
Learning Representations, 2018.
Chaoyue Liu, Libin Zhu, and Mikhail Belkin. On the linearity of large non-linear models: when
and why the tangent kernel is constant. Advances in Neural Information Processing Systems, 33,
2020.
Kamil Malecek and Zbynek Nadenlk. On the inductive proof of Legendre addition theorem. Studia
Geophysica et Geodaetica, 45(1):1-11, 2001.
J Mercer. Functions of positive and negative type and their commection with the theory of integral
equations. Philos. Trinsdictions Rogyal Soc, 209:4-415, 1909.
Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and
Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European
conference on computer vision, pp. 405-421. Springer, 2020.
Radford M Neal. Bayesian learning for neural networks, volume 118. Springer Science & Business
Media, 2012.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pp. 1376-1401. PMLR, 2015.
11
Under review as a conference paper at ICLR 2022
Ivan Nourdin and Giovanni Peccati. Stein’s method on wiener chaos. Probability Theory and
RelatedFields,145(1-2):75-118, 2009.
Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A Alemi, Jascha Sohl-Dickstein,
and Samuel S Schoenholz. Neural tangents: Fast and easy infinite neural networks in
python. arXiv preprint arXiv:1912.02803, 2019. URL https://github.com/google/
neural-tangents.
Ali Rahimi, Benjamin Recht, et al. Random features for large-scale kernel machines. In NIPS,
volume 3, pp. 5. Citeseer, 2007.
Basri Ronen, David Jacobs, Yoni Kasten, and Shira Kritchman. The convergence rate of neural net-
works for learned functions of different frequencies. Advances in Neural Information Processing
Systems, 32:4761-4771, 2019.
Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Im-
plicit neural representations with periodic activation functions. Advances in Neural Information
Processing Systems, 33, 2020.
Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias W. Seeger. Gaussian process op-
timization in the bandit setting: No regret and experimental design. In ICML, pp. 1015-1022,
2010.
Charles Stein. Miscellaneous frontmatter. In Approximate computation of expectations, pp. i-iii.
Institute of Mathematical Statistics, 1986.
Elias M Stein and Guido Weiss. Introduction to Fourier Analysis on Euclidean Spaces (PMS-32),
Volume 32. Princeton university press, 2016.
Ingo Steinwart and Andreas Christmann. Support vector machines. Springer, 2008.
Sattar Vakili, Nacime Bouziani, Sepehr Jalali, Alberto Bernacchia, and Da-shan Shiu. Optimal order
simple regret for Gaussian process bandits. arXiv preprint arXiv:2108.09262, 2021a.
Sattar Vakili, Kia Khezeli, and Victor Picheny. On information gain and regret bounds in gaussian
process bandits. In International Conference on Artificial Intelligence and Statistics, pp. 82-90.
PMLR, 2021b.
Sattar Vakili, Jonathan Scarlett, and Tara Javidi. Open problem: Tight online confidence intervals
for RKHS elements. In Conference on Learning Theory, pp. 4647-4652. PMLR, 2021c.
Michal Valko, Nathan Korda, Remi Munos, Ilias Flaounas, and Nello Cristianini. Finite-time anal-
ysis of kernelised contextual bandits. In Proceedings of the Twenty-Ninth Conference on Un-
certainty in Artificial Intelligence, UAI’13, pp. 654-663, Arlington, Virginia, USA, 2013. AUAI
Press.
Wenjia Wang, Tianyang Hu, Cong Lin, and Guang Cheng. Regularization matters: A nonparametric
perspective on overparametrized neural network. 2020.
Zhuoran Yang, Chi Jin, Zhaoran Wang, Mengdi Wang, and Michael I Jordan. On function approx-
imation in reinforcement learning: Optimism in the face of large state spaces. In Advances in
Neural Information Processing Systems, 2020.
Tong Zhang. Learning bounds for kernel regression using effective data dimensionality. Neural
Computation, 17(9):2077-2098, 2005.
Weitong ZHANG, Dongruo Zhou, Lihong Li, and Quanquan Gu. Neural Thompson sampling. In
International Conference on Learning Representations, 2021.
Dongruo Zhou, Lihong Li, and Quanquan Gu. Neural contextual bandits with UCB-based explo-
ration. In International Conference on Machine Learning, pp. 11492-11502. PMLR, 2020.
12
Under review as a conference paper at ICLR 2022
A General Notation
In this section, we formally define our general notations. We use the notations 0n and In to denote
the zero vector and the square identity matrix of dimension n, respectively. For a matrix M (a vector
v), M> (v>) denotes its transpose. In addition, det and log det are used to denote determinant of M
and its logarithm, respectively. The notation kv kl2 is used to denote the l1 2 norm of a vector v. For
v ∈ Rd and a symmetric positive definite matrix Σ ∈ Rd×d, N(v, Σ) denotes a normal distribution
with mean v and covariance Σ. The Kronecker delta is denoted by δi,j . The notation Sd-1 denotes
the d dimensional hypersphere in Rd. For example, S2 ⊂ R3 * is the usual sphere. The notations
o and O denote the standard mathematical orders, while O is used to denote O UP to logarithmic
factors. For two sequences an, bn : N → R, we use the notation an 〜bn, when an = O(bn) and
bn = O(an). For s ∈ N, we define (2s - 1)!! = Qis=1(2i - 1). For examPle, 3!! = 3 and 5!! = 15.
For a normed sPace H, we use k.kH to denote the norm associated with H. For two normed sPaces
H1, H2, we write H1 ⊂ H2, if the following two conditions are satisfied. First, H1 ⊂ H2 as sets.
Second, there exist a constant c1 > 0, such that kf kH2 ≤ c1 kfkH1, for all f ∈ H1. We also write
H1 ≡ H2, when both H1 ⊂ H2 and H2 ⊂ H1.
The derivative of a : R → R is denoted by a0. We define 00 = 0, so that a0 (x) = (max(0, x))0
corresPonds to the steP function: a0(x) = 0, when x ≤ 0, and a0(x) = 1, when x > 0.
B Mercer’ s Theorem
In this section, we give an overview of Mercer’s theorem, as well as the the reProducing kernel
Hilbert sPaces (RKHSs) associated with the kernels. Mercer’s theorem (Mercer, 1909) Provides a
sPectral decomPosition of the kernel in terms ofan infinite dimensional feature maP (see, e.g. Stein-
wart & Christmann (2008), Theorem 4.49).
Theorem 4	(Mercer’s Theorem ) Let X be a compact domain. Let k be a continuous square inte-
grable kernel with respect to a finite Borel measure μ. Define a positive definite operator Tk
(Tkf)(.)
X k(.,
χ)f(χ)dμ.
Then, there exists a sequence of eigenvalue-eigenfunction pairs {(λi, φi)}i∞=1 such that λi ∈ R+,
and Tkφi = λiφi, for i ≥ 1. Moreover, the kernel function can be represented as
∞
k(x, x0) =	λiφi(x)φi(x0),
i=1
where the convergence of the series holds uniformly on X × X.
The λi and φi defined in Mercer’s theorem are referred to as Mercer eigenvalues and Mercer eigen-
functions, resPectively.
Let Hk denote the RKHS corresPonding to k, defined as a Hilbert sPace equiPPed with an inner
Product h., .iHk satisfying the following: k(., x) ∈ Hk, ∀x ∈ X, and hf, k(., x)iHk = f (x),
∀x ∈ X , ∀f ∈ Hk (reProducing ProPerty). As a consequence of Mercer’s theorem, Hk can be
rePresented in terms of {(λi, φi)}i∞=1, that is often referred to as Mercer’s rePresentation theorem
(see, e.g., Steinwart & Christmann (2008), Theorem 4.51).
Theorem 5	(Mercer’s Representation Theorem) Let {(λi, φi)}i∞=1 be the Mercer eigenvalue-
eigenfunction pairs. Then, the RKHS corresponding to k is given by
Hk
(∞
f (∙) = X wiλi2 φi(∙): Wi ∈ r, ||f ||Hk
i=1
∞
Xwi2
i=1
<∞
)
1
Mercer,s representation theorem indicates that {λi2 φi}∞=ι form an orthonormal basis for Hk
1	1
hλi2 φi, λiφioiHk = δi,i'. It also provides a constructive definition for the RKHS as the span of
this orthonormal basis, and a constructive definition for the kf kHk as the l2 norm of the weights
[wi ]i∞=1 vector.
13
Under review as a conference paper at ICLR 2022
C Classical Approaches to B ounding the Generalization Error
There are classical statistical learning methods which address the generalization error in machine
learning models. Two notable approaches are VC dimension (Bartlett, 1998; Anthony & Bartlett,
2009) and Rademacher complexity (Bartlett & Mendelson, 2002; Neyshabur et al., 2015). In the
former, the expected generalization loss is bounded by the square root of the VC dimension divided
by the square root of n. In the latter, the expected generalization loss scales with the Rademacher
complexity of the hypothesis class. In the case of a 2 layer neural network of width m, it is shown
that both of these approaches result in a generalization bound of O(，m/n) that is vacuous for
overparameterized neural networks with a very large m.
A more recent approach to studying the generalization error is PAC Bayes. It provides non-vacuous
error bounds that depend on the distribution of parameters (Dziugaite & Roy, 2017). However, the
distribution of parameters must be known or be estimated in order to use those bounds. Taking a
different approach, Arora et al. (2019) provides bounds on expected generalization error of well-
conditioned 2 layer neural networks based on the Gram matrix of the input data, which does not
depend on the distribution of model parameters as in PAC Bayes. In contrast to all of these results,
our bounds are explicit and are much stronger in the sense that they hold uniformly in x.
D Proof of Lemma 1
Lemma 1 offers a recursive relation over s for the RF kernel. We prove the lemma by taking the
derivative of κs(.), and applying the Stein’s lemma (given at the end of this section).
Let X = [0,0,..., 0,1]> and X = [0,0,..., ʌ/l — u2, u]>, so that x>x0 = u, and x, X ∈ Sd-1.
We have
∂UEw~N(0d,Id) [as(WTx)as(WTxO)] = Ew~N(0d,Id) as(WTx)Sas-1 (WTXO)(Wd — J；d，)
=sEw~N (0d,Id) Sas-1 (w> x)as-1 (wt X0)
+(s — 1)uas (WTx)as-2 (WTxO)
-sEw-N(0d,id)
S2Ew~N(0d,id) Ias-I(W>x)as-i(W>x0)].
The first equation is obtained by taking derivative of the term inside the expectation. The second
equation is obtained by applying Stein’s lemma to E[as (WTx)sas-1 (WTx0)Wd], where Wd is the
normally distributed random variable, also to E[as(W>χ)sas-ι(Wτχ0) Uw-⅛], where Wd-1 is the
1-u
normally distributed random variable.
Taking into account the constant normalization of the RF kernel c2 = ⑵-]*, We have
κ0s(u)	2∂ =(2s - 1. ∂UEw-N(0d,Id) Ias(WTx)as (w>x )] 2s2	T	T 0 =(2s - 1)!!Ew-N(0d,Id) IaS-I(WTx)as-1(WTx )] s2 二 	Ks-1 , 2s — 1 S 1,
which proves the lemma.
Lemma 2 (Stein,s Lemma) Suppose X 〜N(0,1) is a normally distributed random variable.
Consider a function g : R → R such that both E[X g(X)] and E[g0(X)] exist. We then have
E[Xg(X)] =E[g0(X)].
The proof of Stein’s lemma follows from an integration by parts (see, e.g., Stein, 1986; Nourdin &
Peccati, 2009; Chen et al., 2011).
14
Under review as a conference paper at ICLR 2022
E Proof of Proposition 1
This proposition follows from Theorem 1 of Bietti & Bach (2020), which proved that the eigendecay
of a rotationally invariant kernel κ : [-1, 1] → ∞ can be determined based on its asymptotic
expansions around the endpoints ±1. Recall that a rotationally invariant kernel k can be represented
as κ(x>x0) = k(x, x0). Their result is formally given in the following lemma.
Lemma 3 (Theorem 1 in Bietti & Bach (2020)) Assume κ : [-1, 1] → R is C∞ and has the fol-
lowing asymptotic expansions around ±1
κ(1 -	t)	=	p+1 (t)	+ c+1tθ	+ o(tθ),
κ(-1 +	t)	=	p-1 (t)	+ c-1tθ	+ o(tθ),
for t > 0, where p±1 are polynomials, and θ > 0 is not an integer. Also, assume that the derivatives
of κ admit similar expansions obtained by differentiating the above ones. Then, there exists Cd,θ
SUCh that, when i is even, if c+ι = —c-ι: λi 〜(c+ι + c-ι)Cd,θi-d-2θ+1, and, when i is odd,
if c+ι = c-ι: λi 〜 (c+ι 一 c-ι)Cd,θl-d-2θ+1. In the case ∣c+ι∣ = ∣c-ι∣, then we have Ai =
o(l-d-2θ+1) for
one of the two parities. Ifκ is infinitely differentiable on [一1, 1] so that no such θ
exists, the λi decays faster than any polynomial.
Building on this result, the eigendecays can be obtained by deriving the endpoint expansions for the
RF and NT kernels. That is derived in Bietti & Bach (2020) for the ReLU activation function. For
completeness, here, we derive the endpoint expansions for RF and NT kernels associated with s 一 1
times differentiable activation functions as(.).
Recall that for a 2 layer network, the corresponding RF and NT kernels are given by
KNT,s(x>x0) = c2(x>x0)Ew 〜N (0d,id)[aS(w>x)aS(w>xo)] + Ks(x>x0),
Ks (x>x0) = c2Ew 〜N (0d,id) [as (w>x)as (w> x0)].
For the special cases of s = 0 and s = 1, the following closed form expressions can be derived by
taking the expectations.
κ0(u)
κ1 (u)
Ew 〜N (0d,id) [ao(wτx)ao(wτx0)] = 1(∏ — arccos(u)),
ɪ (u(π — arccos(u)) + pl — u2),
where u = xτx0. At the endpoints ±1, κ0(u), κ1(u) have the following asymptotic expansions.
κo(1 — t) = 1 — J t1 + o(t1),
π
ko( —1+1) = — t1 + o(t2),
π
2√2 3	3
κι(1 — t) = 1 — t + -ɪt2 + o(t2),
3π
2√2 3	3
Ki( —1+1) = t2 + o(t2).
-C ZTT-
(9)
These endpoint expansions where used in Bietti & Bach (2020) to obtain the eigendecay of the RF
and NT kernels with ReLU activation functions.
We first extend the results on the endpoint expansions and the eigendecay to a 2 layer neural network
with s > 1, in Subsection E.1. Then, we extend the derivation to l > 2 layer neural networks, in
Subsection E.2. In Subsection E.3, we show how the eigendecays in terms of λi can be translated to
the ones in terms of λi .
15
Under review as a conference paper at ICLR 2022
E.1 ENDPOINT EXPANSIONS FOR 2 LAYER NETWORKS WITH s > 1
We first note that the normalization constant c2
(2s-i)!!, suggested in Section 2, ensures κs(1)
1, for all s ≥ 1. The reason is that, by the well known values for the even moments of the normal
distribution, we have
Ew〜N(0d,id)[as(w>x)as(w>x0)] = (2S - 1)-
when x>x0 = 1. Also notice that κs
at least one of w>x and w>
(-1) = 0, for all s ≥ 1. The reason is that when x>x0 = -1,
x0 is non-positive.
We note that the normalization constant is considered only for the convenience of some calculations,
and does not affect the exponent in the eigendecays. Specifically, scaling the kernel with a constant
factor, scales the corresponding Mercer eigenvalues with the same constant. The constant factor
scaling does not affect the Mercer eigenfunctions.
From Lemma 1, recall
s2
Ks(J = 3------^Ks-I(J.
s 2s - 1
This is a key component in deriving the endpoint expansions for s > 1. In particular, this allows us
to recursively obtain the endpoint expansions of Ks(.) from those of Ks-1(.), by integration. That
results in the following expansions for κ
s.
κs (1 - t)
κs(-1 + t)
2s + 1	/ 2s + 1、
p+1,s(t) + c+1,s^t	2	+	o(t	2	),
2s + 1	2s + 1
p-1,s(t) + c-1,st	2	+	o(t	2	),
(10)
2
Here, p+ι,s(t) = 一2s-ɪ Jp+ι,s-ι(t)dt, subject to p+ι,s(0) = 1 (that follows form Ks(1) = 1).
Thus, p+ι,s(t) can be obtained recursively, starting from p+ι,ɪ(t) = 1 一 t (see equation ). For
example,
p+1,2(t)
p+1,3(t)
4 3	t2
-3(-4+t- G
36(15 - 31 + 1t2-1t3)
15(36 4 + 2 t	6 ),
and so on.
2
Similarly, p-ι,s(t) can be expressed in closed form using p-ι,s(t) = 2ss-ɪ Jp-1,s-1(t)dt subject to
p-1,s(0) = 0 (that folloWs from Ks(-1) = 0), and starting from p-1,1(t) = 0, ∀t (see equation 9).
That leads to p-ɪ,s (t) = 0, ∀s > 1, t.
The exact expressions of p±ɪ,s (t) however do not affect the eigendecay. Instead, the constants c±ɪ,s
are important for the eigendecay, based on Lemma 3. The constants can also be obtained recursively.
Starting from c+ι,ι =第(see equation 9), c+ι,s =(-2-芸!-；). Also starting from e-ɪ,ɪ =需
(see equation ), e-ɪ,s =(2：-1志+11). This recursive relation leads to
2s √2	Sr
c-1,s =  H
r=1
r2
(4r2 - 1),
(11)
and c+1,s = (-1)s-1c-1,s.
In the case of RF kernel, applying Lemma 3, We have λ% 〜i-d-2s, for i having the opposite parity
~	.	T C 、
of s, and 枭 =o(i-d-2s) for i having the same parity of s.
For the NT kernel, using 2, we have
κNT,s (1 - t) = c2 (1 - t)s2 κs-1 (1 - t) + κs (1 - t)
κNT,s (-1 + t) = c2 (-1 + t)s2κs-1 (-1 + t) + κs(-1 + t)
(12)
16
Under review as a conference paper at ICLR 2022
Using the expansion of the RF kernel, we get
2s-1	2s-1
κNT,s(1 - t) = p+1,s(t) + c+1,s^t~"r~ + o(t~"r~ ),
2s-1	2s-1
KNT,s(-1+1) = P- 1,s(t) + C- 1,st F + o(t F),	(13)
where
p0+01,s(t)	= c2(1 -t)s2p+1,s-1(t) +ps(t),
p0-0 1,s (t) = c2(-1 + t)s2p-1,s(t) + p-1,s(-1 + t),
and
c0+0 1,s	= c+1,s-1,
c0-0 1,s	= c-1,s-1 .
Thus, in the case of NT kernel, applying Lemma 3, We have λi 〜i-d-2s+2, for i having the same
parity of s, and λi = o(i-d-2s+2) for i having the opposite parity of s.
E.2 ENDPOINT EXPANSIONS FOR l > 2 LAYER NETWORKS
We here extend the endpoint expansions and the eigendecays to l > 2 later netWorks. Recall κls(u)
κs(κls-1(u)). Using this recursive relation over l, We prove the folloWing expressions
κls (1 - t)
κls (-1 + t)
l / 、	1	2s + 1	/ 2s + 1、
p+l,s⑴ + c+l,st 2	+ o(t 2 )
1	1	2s+1	2s + 1
P1-1,s(t)+ Cl-1,st F + o(t F )
Where p1±1,s are polynomials and c1±1,s are constants.
The notations p+1,s and c+1,s in Subsection E.1 correspond to p2+1,s c2+1,s, Where We drop the
superscript specifying the number l of layers, for 2 layer netWorks.
For the endpoint expansion at 1, We have
κls (1 - t)
κs
κs
(κ1s-1(1 - t))
(1 - 1+ P-1,s(t) + c+-ι1st 号 + o(t 印))
P+1,s(1 - p+ι1,s(t) - cl+ι1,st2s+ - o(t2s+1))
+c+ι,s(1 -Pl+ι1,s(t) - c+ι1,st守-o(t挈))中
+0 ((1- p+1ls(t)-c+1lst守-o(t2+))挈)
Thus, We have
1,s
c+1,s - q+,1,s c+-1,s ,
(14)
Where q+l,i1,s is the coefficient ofti in pl+1,s(t). For these coefficients, We have
q+l,11,s = -q+2,11,sq+l-11,s,1.
(15)
Starting from q+1,s = -Q-i (which can be seen from the recursive expression of p+ι,s given in
Section E.1), We get q+l,11,s
—
(2S2-l1)1-ι. We thus have
J	s(2s+1)	!l-2
1,s =	(2s - 1) (2⅛m )
s2
c+1,s+ 2S-
(16)
That implies
〜
1,s
s(2s + 1)	2
(2s- 1)0)
c+1,s,
(17)
17
Under review as a conference paper at ICLR 2022
when s > 1.
The characterization of cl+1,s shows that our results do not apply to deep neural networks when
s > 1, in the sense that the constants grow exponentially in l (as stated in Remark 2).
For the endpoint expansion at -1, we have
κls(-1 +t)	= κs(κls-1(-1 +t))
=Ks(P-ι,s(t) + c-ι1st 2s+1 + o(t 2s+1))
_	1 l-1,0∖	0 L	l-1,0	l-1	l-1,0	,	l-1	, 2s+1	2s+1	ʌ
=κs(q-l,s	) +	κs(q-i,s	)(p-i,s(t)	- q-i,s	+ c-l,st	2 ) +	o(t	2 ).
where q-l,i1,s is the coefficient ofti inpl-1,s(t). From the expression above we can see that
l,0 κ ( l-1,0)
q-1,s = κs (q-1,s )
Thus, 0 ≤ q-l,01,s ≤ 1. In addition, the expression above implies
cl-1,s = cl--11,sκ0s (q-l-11,s,0).
2
From Lemma 1, κs(u) ≤ 2ss-ι, for all u. Therefore,
(18)
(19)
2	l-2
c-1,s	≤	(2S-1)	c-1's
= o(c+1,s ),
when, s > 1.
Comparing cl±1,a, we can see that for l > 2, we have |cl+1,s| 6= |cl-1,s |. Thus, for the RF kernel with
l > 2, applying Lemma 3, We have λ 〜i-d-2s.
For the NT kernel, recall
κNT,s (u) = c κN-T,s (u)κ0s (κs- (u)) + κs (u).
The second term is exactly the same as the RF kernel. Recall the expression of κ0s based on κs-1
from Lemma 1. To find the endpoint expansions of the first term, We prove the folloWing expressions
for κs-1(κls-1(u)).
Ks-I(KsT(I -1)) = p+ I,s(t)+ c+ ι,st2s-1 + o(t2-1)
κs-i(κs-1(-1+1)) = p-ι,s(t) + c- ι,st 2s+ + o(t 2s+1)
We use the notation used for the RF kernel to Write the folloWing expansion around 1
Ks-I(KsT(I- t)) = κs-i(1 - 1+ p/ls(t)+ c+l,st2s+1 + o(t2s+))
=P+ι,s-i(1- p+ι1s(t) - c⅛2s+1 - o(t2s+1))
+c+ι,s-i(1 - P+*) - c+*2s+1 - o(t2s2+1))2s-1
+o ((1- p+ι1s(t) - c+1,st2s+1 - o(t2s+1))ɪ).
Thus, We have
c+ ι,s = (-q⅛1) 2s-1 c+1,s-1 - q+,1,s-ιc+ι1s
Recall, form the analysis of the RF kernel that q+,11,s = -(2s-l-1-1. We thus have
0i	J	s(2sT)	!l-2	, (s - 1)2 l-1
+1,s =	(2s- 1) 9	+1,sτ + -2l-3 +1,s.
(20)
(21)
18
Under review as a conference paper at ICLR 2022
That implies
! l-2
c+ 1,s
S(2sT)
(2s- 1) T
c+1,s-1 ♦
(22)
For the endpoint expansion at -1, we have
Ks-i(κS-1(-1+ t)) = Ks-i(p-±(t)+ C-LISt 号1 + o(t 彗1))
=κs-ι(q-⅛0) + κS-ι(q-⅛0)(p二(t) - q¾0 + C1t号1) + o(t号1).
We thus have
C- 1,S = c-⅛-iκS-i(q¾0).	(23)
Since KS-L ≤ ⅛-⅛^, We have
l-2
c- 1,s-1.
C-L，s ≤ (2S--^3
Now we use the end point expansions of KS and KS-1 (KS-1) to obtain the following endpoint expan-
sions for KNT S.
kNt,s(1- t)	= P+l1,S(t) + c+l1,St修 + o(t修),
kNt,s(-1+ t)	= P-l1,S(t) + C-lι,St = + θ(t 〜).
We have
c00l	= c2s2
+1，S	2s - 1
+ c2s2 /l-ι,0c∕l
+ 2s- 1 q+1，S c+1,s,
and,
c2s2 00l-1,0 0l,0 l l,0
2s - 1 q+1,S q+1,S + q+1,S∙
(24)
Since k§(1) = 1, by induction we have KS (1) = 1, ∀l ≥ 2. In addition, ks-1 (KST(I))
Ks-i(1) = 1, ∀l ≥ 3. Therefore ¢£3, /[s = 1. Thus,
〃l,0
q+ι,S
c2S2 O0l-1,0
2s - 1 q+1，S
+ 1.
(25)
〜
Starting from 谓，,(S = 2f^ɪ + 1, we can derive the following expression for 壮匕 when s > 1,
〃l,0	1
q+1,S =屋
(c2s2 ∖
2T-1)
When S = 1, q：1(OS = l,that is consistent with the results in Bietti & Bach (2020).
For c001i,s, we thus have
c00l	= c2s2 c00l-1 + c2s2 „ l-1,0c0l
+1，S = 2s - 1 +1，S + 2s - 1 q+1，S	+1，S.
19
Under review as a conference paper at ICLR 2022
Replacing the expressions for q+00l1-,s1,0 and c0+l 1,s derived above, we obtain
)(l-2)+1
2 r2ς2、(2s+1 )(l-2) + 1
八)
(26)
For the constant in the expansion around -1, we have
c
00l
-1,s
c2s2	0l,0	00l-1
2s - 1 q-1,s —1，s,
(27)
where q-0l,10,s = κs-1(q-l-11,s,0) is bounded between 0 and 1. Thus, starting from c0-021,s = c-1,s-1, we
obtain
c
00l
-1,s
(c2s2 ∖
2S-1)
l-2
c-1,s-1
(28)
≤
Comparing c0±0l1,a, we can see that for l > 2, we have |c0+0l1,s| 6= |c0-0l1,s |. Thus, for the NT kernel with
l > 2, applying Lemma 3, We have λi 〜i-d-2s+2.
E.3 THE EIGENDECAYS IN TERMS OF λi
Recall the multiplicity Nd,i = 2i+?-2 (i+--3) of ʌi. Here We take into account this multiplicity to
give the eigendecay expressions in terms of λi .
Using (k)k ≤ (k) ≤ (ne)k, for all k, n ∈ N, we have, for all i > 1 and d > 2,
2(i - 1)d-2(^^ + -ɪ)d-2 ≤ Ndi ≤ (d""d 2 (i - 1)d-2(^^ + ^-)d-2
(	)	(d - 2 + i - 1)	— d,i —	2	(	)	(d - 2 + i - 1)
where we used 2 < 2i+?-2 < d++2. We thus have Nd,i 〜(i - 1)d-2, for the scaling of Nd,i with i,
With constants given above.
i0-1	i0
Recall we define λi = λi0, for i and i0 which satisfy i00=1 Nd,i00 < i ≤ i00=1 Nd,i00 . Using
Nd,i 〜(i - 1)d-2, we have P：〃=i Nd#，〜i0d-1. Thus λ% 〜 *=λ ι . Replacing i with
=	i d-1
-^1^
id-1, in the expressions derived for λi in this section, we obtain the eigendecay expressions for λi
reported in Table 2.
F	Proof of Theorem 1
The Matern kernel can also be decomposed in the basis of spherical harmonics. In particular,
Borovitskiy et al. (2020) proved the following expression for the Matem kernel with smoothness
parameter ν on the hypersphere Sd-1 (see, also Dutordoir et al., 2020, Appendix B)
kν (x, x0) = XX XX(12+i(i+d - 2))-(ν+宁 )φij (χ)φi,j (χ0),
i=1 j=1
where φij are the spherical harmonics.
Theorem 5 implies that the RKHS of Metern kernel is also constructed as a span of spherical har-
monics. Thus, in order to show the equivalence of the RKHSs of the neural kernels with various
activation functions and a Matern kernel with the corresponding smoothness, we show that the ratio
between their norms is bounded by absolute constants.
20
Under review as a conference paper at ICLR 2022
Letf ∈ HklNT,s,withl ≥ 2. As a result of Mercer’s representation theorem, we have
f(∙)
∞ Nd,i
XX
wi,jλ2 φi,j S
i=1 j=1
∞ Nd,i
XXwi,j
i=1 j=1
〜1
λi
(Kv + i(i + d - 2))-2(ν+ d-1)
(2V2 + i(i + d - 2))-1 (V+
κ2
d-1
H )Φi,j (∙).
Note that
〜
λi
(K + i(i + d - 2))-(V+ d-1)
ʌ
O( i-2V⅛ ).
For the NT kernel, from Proposition 1, Ki = O(i-d-2s+2). Thus, when V = S — 2,
〜
λi
(Kv + i(i + d - 2))-(ν+ d-1)
O(1).
So, With V = S - 2 We have
kfk2Hkν
∞ Nd,i
X X wi2,j
i=1 j=1
∞ Nd,i
λi
(κ2 + i(i + d - 2))-(V+ d-1)
O(	wi2,j)
i=1 j=1
O(kfk2H l ).
κlNT,s
That proves Hkl ⊂ Hkν .
A similar proof shows that when V = S+ 2, HkS ⊂Hkν.
Now, let f ∈ Hkν. As a result of Mercer’s representation theorem, we have
f(∙)
∞ Nd,i
X X Wij ( K + i(i + d - 2))-2 (V+ F )φi,j (•)
i=1 j=1
∞ Nd,i
X X wi,j
(K + i(i + d - 2))-2(v+ T)
~ 1 ~
λ φi,j (∙).
1
λ
For the NT kernel with l > 2, from Proposition 1, λι ~ l-d-2s+2. Thus, when V = S
—
1
2，
(K + l(l + d- 2))-(v+d-1)
〜
λl
O(1).
So, with v = s - 2 We have
kfk2HκlNT,
∞ Nd,i
X X wi2,j
i=1 j=1
∞ Nd,i
(K + i(i + d - 2))-(ν+ d-1)
〜
λi
O(	wi2,j)
i=1 j=1
O(kfk2Hkν).
s
21
Under review as a conference paper at ICLR 2022
That proves, for l > 2, when V = S - 1, HkV ⊂ HkNT S. A similar proof shows that for l > 2, When
V = s + 2, HkV ⊂ Hlk , which completes the proof of Theorem 1.
G	Proof of Theorem 2
Recall the definition of the information gain for a kernel κ,
I(Yn； F) = 1 logdet (In + λ2Kn)
To bound the information gain, we define two new kernels resulting from the partitioning of the
eigenvalues of the neural kernels to the first M eigenvalues and the remainder. In particular, we
define
M Nd,i
…=ΣΣλiφi,j (x)φi,j (x0),
∞	Nd,i
K(XTx) = X X aiφ⅛,j(X)φ⅛,j(XO).	(29)
i=M +1 j=1
We present the proof for the NT kernel. A similar proof applies to the RF kernel.
The truncated kennel at M eigenvalues, κK(x>x), corresponds to the projection of the RKHS of κ
onto a finite dimensional space with dimension PiM=1 PjN=d1,i.
〜
We use the notations Kn and Kn to denote the kernel matrices corresponding to κK and κK, respec-
tively.
As proposed in Vakili et al. (2021b), we write
logdet (In + ~λ2Kn)	= logdet (In + λ (Kn + Kn))
=logdet (In + λKn) + logdet (In + 定(In + 率Kn) 1Kn)，(30)
The analysis in Vakili et al. (2021b) crucially relies on an assumption that the eigenfunctions are
uniformly bounded. In the case of spherical harmonics however (see, e.g., Stein & Weiss, 2016,
Corollary 2.9)
sup	φi,j (x) = ∞.	(31)
i=1,2,...,
j=1,2,...,Nd,i,
x∈X
Thus, their analysis does not apply to the case of neural kernels on the hypersphere.
To bound the two terms on the right hand side of equation 30, we first prove in Lemma 4 that κK(x, x0)
approaches 0 as M grows (uniformly in x and x0), with a certain rate.
Lemma 4 For the NT kernel, we have κKK(x>x0) = O(M-2s+1), for all x, x0 ∈ X.
Proof of Lemma 4: Recall that the eigenspace corresponding to λKi has dimension Nd,i and consists
of spherical harmonics of degree i. Legendre addition theorem (Malecek & Nadenk 2001) states
Nd,i
X φi,j(x)φi,j(x0) = ci,dCi(d-2)/2(cos(d(x, x0))),	(32)
j=1
22
Under review as a conference paper at ICLR 2022
where d is the geodesic distance on the hypersphere, Ci(d-2)/2 are Gegenbauer polynomials (those
are the same as Legendre polynomials up to a scaling factor), and the constant ci,d is
ci,d
Nd,iΓ((d - 2)/2)
2∏(d-2)∕2c(d-2”2(ι)
(33)
We thus have
≤
〜
〜
∞	Nd,i
E XiE Φi,j (CM(x0)
i=M +1	j=1
∞
X Xici,dC(d-2"2(cos(d(x,x0)))
i=M +1
X λ N Γ((d- 2)/2)
λ-N λiNd,i 2∏((d-2)∕2)
i=M +1
∞
X i-d-2s+2 id-2
i=M +1
M -2s+1.
Here, the inequality follows from Ci(d-2)∕2(cos(d(x, x0))) ≤ Ci(d-2)∕2(1), because the Gegenbauer
polynomials attain their maximum at the endpoint 1. For the fourth line We used Nd,i 〜id-2. The
implied constants include the implied constants in Ndi 〜id-2 given in Section E.3, and ：((；-2))/2).
π
We also introduce a notation NM = PiM=1 Nd,i for the number of eigenvalues corresponding to
the spherical harmonics of degree up to M, taking into account their multiplicities, that satisfies
NM 〜Md-1 (see Section E.3).
We are noW ready to bound the tWo terms on the right hand side of equation 30. Let us define
Φn,NM = [φNM(x1), φNM(x2), . . . , φNM(xn)]> , an n × NM matrix Which stacks the feature
vectors φNM (xi) = [φj (xi)]jN=M1, i = 1, . . . , n, at the observation points, as its roWs. Notice that
>
Kn = Φn,NM ANM Φn,NM ,
Where ANM is the diagonal matrix of the eigenvalues defined as [ANM]i,j = λiδi,j.
NoW, consider the Gram matrix
1	1
G = ANM Φ>,Nm φn,NM ANM .
As it was shown in Vakili et al. (2021b), by matrix determinant lemma, we have
1
logdet (In + λ2 Kn)
=log det(INM + λ2 G)
≤ NM log (N^ tr(INM +
n
=NM log(i + λNM).
To upper bound the second term on the right hand side of equation 30, we use Lemma 4. In particular,
since
tr ((In + λ2Kn)-1Kn) ≤ tr(Kn),
and [KXn]i,i = O(M-2s+1),wehave
tr k n + λ (In + λ K n)-1 K n) = O (n(1 + 定M - 2s+1)).
23
Under review as a conference paper at ICLR 2022
Therefore
log det b n + λ (In + λ2 KK n) IK n
≤ n log (θ(1 + λ2M-2s+1)
≤	nMJ S,
where for the last line we used log(1 + z) ≤ z which holds for all z ∈ R.
WethUs have Yk (n) = O(Md-1 log(n) + nM-2s+1). Choosing M 〜n d+21s-2 (log(n)) d+21-2, We
obtain
d	d — 1	2s — 1
YκNT,s (n) = O (nd+2s-2 (log(n)) d+2s-2J .	(34)
For example, With ReLU activation fUnctions, We have
YKNTJn) = O (nd-1 (logS))d).
H Proof of Theorem 3
As stated in the paper, the proof folloWs the same steps as in the proof of Theorem 3 of Vakili et al.
(2021a). Their theorem holds for general kernels, provided the boUnd on MIG. We have adopted
their theorem for the special case of neUral kernels, inserting oUr novel boUnds on MIG of the neUral
kernels given in Theorem 2. For completeness, We inclUde a detailed proof here.
The proof consists of toW components. First, We boUnd the Uncertainty estimate σn (x) in terms of
Yk(n). OUr novel boUnds on Yk(n) of neUral kernels given in Theorem 2 alloW Us to derive explicit
boUnds on σn(x). Then, We boUnd the error |f(x) - fn(x)| in terms of σn(x), Using implicit error
boUnds.
Recall the way that the dataset Dn is collected: Xi = argmaxχ∈χ σi-ι(x). This ensures that,
∀x ∈ X, σi-1(x) ≤ σi-1(xi). DUe to positive definiteness of the kernel matrix, conditioning on a
larger dataset reduces the uncertainty. Thus ∀x ∈ X and ∀i ≤ n, σn(x) ≤ σi(x). Therefore σn2 (x)
is upper bounded by the average of σi2-1(xi) over i: ∀x ∈ X
1n
σn (X) ≤ — £蟾-1 (Xi) .	(35)
nn-
i=1
It is shown that (e.g., see, Srinivas et al., 2010, Lemmas 5.3, 5.4)
S 2(	2I(Yn; F)
σi-σ(χi(≤) ≤	rτ.	(36)
i=1	log(1 + λ12 )
Thus, combining equation 35 and equation 36, and by definition of Yk(n), we have, ∀X ∈ X
( ) ≤ I	2Yk (n)
nn x n n n log(1 + -
(37)
Now, inserting our bounds on Yκl (n) and Yκl (n) of the neural kernels from Theorem 2, we obtain,
∀X ∈ X	,s
σn(X)
σn(X)
-2s + 1	2s-1
n2d+4s-4 (log(n)) 2d+4s-4
-2s-1
n 2d+4s (log(n))2
⅛i⅛,
in the case of NT kernel,
Iog(I + λ12 )，
in the case of RF kernel.
(38)
≤
≤
2
The second component of the proof is summarized in the following lemma.
24
Under review as a conference paper at ICLR 2022
Lemma 5 Under Assumption 1, we have, with probability at least 1 - δ, ∀x ∈ X
nd-1	1	2
If (x) - fn(x)l ≤ (B + C(log( k ))2)σn(x) + √n,
(39)
where C is an absolute constant, and B is the upper bound on the RKHS norm of f given in As-
sumption 1.
Lemma 5 follows from equation 7, and a probability union bound over a discretization of the domain.
A detailed proof of this lemma is given at the end of this section.
Inserting the bounds on σn(x) from equation 38 in Lemma 5, we have, with probability at least
1 - δ, uniformly in x,
. . .	^ . ..
If(x) - fn(x)I
. . .	^ . ..
If (x) - fn(x)I
O (n 2d-24+14 (log(n)) 2d+4s-4 (log( n-^—))1) , in the case of NT kernel,
O (n 22+4l (log(n)) 2d-4s (log(n-^—))1 ) , in the case of RF kernel.
That completes the proof.
Proof of Lemma 5:
Recall the implicit error bound given in equation 7: For a fixed x ∈ X, under Assumption 1, we
have, with probability at least 1 - δ (Vakili et al., 2021a),
If(x) - fn(x)I ≤ β(δ)σn(x),	(40)
where β(δ) = B + R y 2 log(2). Lemma 5 extends this inequality to a uniform bound in x, using a
probability union bound over a discretization of the domain.
For f ∈ Hk , with kf kHk ≤ B, and for n ∈ N, there exists a fine discretization Xn of X with
size |Xn| such that f (x) - f ([x]n) ≤ √1n, where [x]n = argminχo∈χn ∣∣x0 - x∣∣12 is the closest
point in Xn to x, and IXnI ≤ cBd-1n(d-1)/2, where c is an absolute constant independent ofn and
B (Chowdhury & Gopalan, 2017; Vakili et al., 2021a).
Under Assumption 1, it can be shown that: with probability at least 1 - δ∕2 (Vakili et al., 2021a,
Lemma 4)
kfn∣Hk ≤ B + R√n^2log(4n).	(41)
、-----------V----------}
U(δ)
Note that f isa random function, where the randomness comes from the randomness in observation
noise.
We thus conclude that there is a discretization Xn with size ∣Xn| ≤ c(U(δ))d-1n(d-1)/2 such that
If(x) - f([x]n)I ≤ √n, and with probability at least 1 - δ∕2,
1
Ifn(X)- fn (Ix]n) I ≤ -J= .	(42)
n
Let δ0 = 2c(u(δ))d-1 n(d—i)/2. A probability union bound over the discretization Xn implies that:
With probability at least 1 - δ∕2, uniformly in X ∈ Xn
.... ^ . . . . , ,.
If (x) - fn(x)∣ ≤ β(δ )σn(x).	(43)
Accounting for the discretization error from equation 42, and a probability union bound over equa-
tion 41 and equation 43, we have, with probability at least 1 - δ, uniformly in x ∈ X,
25
Under review as a conference paper at ICLR 2022
. . . ^ . . . . ., , ., , ^ ., , . ^ ., , ^ . ..
If (x) -	fn(x)∣	≤	|f(x) - f([x]n)l	+	∣f([x]n)	-	fn ([x]n)l +	∣fn([x]n) -	fn(x)∣
2
≤	IfaxnD - fn([xn])| 十 √τ^
n
2
≤ β(δ )σn(x) +------尸.
n
The first line holds by triangle inequality, the second line comes from the discretization error, and
the third line holds by equation 43.
Inserting the value of U(δ) = B + R√n ,2log(2n)
in β(∙) = B + R ,2 log( 2), we arrive at the lemma.
, and the value of δ0
__________δ___________
2c(U (δ))d-1 n(dτ)/2
I Details on the Experiments
In this section, we provide further details on the experiments shown in the main text, Section 6. The
code will be made available upon the acceptance of the paper.
We consider NT kernels κNT,s(.), with s = 1, 2, 3, which correspond to wide fully connected
2 layer neural networks with activation functions as(.). In the first step, we create a synthetic
function f belonging to the RKHS of a NT kernel κ. For this purpose, we randomly gen-
erate no = 100 points on the hypersphere SdT. Let Xn° = [xi]g denote the vector of
n
these points. We also randomly sample Yn° = [yi]i=ι from a multivariate GaUssian distribution
N(0no, Kno), where [Kn]i,j = κ(x>xj). Wedefineafunctiong(.) = k>0(.)(Kno+δ2In0)-1Yn0,
where δ2 = 0.01 and [Kn (x)]i = κ(x>xi). We then normalize g with its range to obtain
f(∙) = maxχ∈χ g(xg-minχ∈χ g(χ) ∙ FOr a fixed Xn and ‰, g is a Iinear COmbinatiOn of PartiaI
applications of the kernel. Thus g is in the RKHS of κ, and its RKHS norm can be bounded as
follows.
kg k2Hκ
{k>o (.)(K no + δ2Ino ) f , k>o (.)(K no + δ2In /n)
Hκ
Y> (Kno + δ2Ino )-1Kno (Kn + δ2In0 )-1Yn,
o
≤	Y> (Kno + δ2Ino )F
≤
The second line follows from the reproducing property. We thus can see that for each fixed X. and
YnO, g (and consequently f) belong to the RKHS of κ.
The values of maxx∈X g(x) and minx∈X g(x) are numerically approximated by sampling 10, 000
points on the hypersphere, and choosing the maximum and minimum over the sample.
We then generate the training datasets Dn of the sizes n = 2i, with i = 1, 2, . . . , 13, by sam-
pling n points Xn = [xi]in=1 on the hypersphere, uniformly at random. The values Yn = [yi]in=1
are generated according to f. We then train the neural network model to obtain fn(.). The error
maxx∈x ∣f (x) - fn(x)∣ is then numerically approximated by sampling 10,000 random points on
the hypersphere and choosing the maximum of the sample.
We have considered 9 different cases for the pairs of the kernel and the input domain. In particular,
the experiments are run for each κNT,s, s = 1, 2, 3 on all Sd-1, d = 2, 3, 4. In addition, each one of
these 9 experiments is repeated 20 times (180 experiments in total).
26
Under review as a conference paper at ICLR 2022
ɪ τr-<∙	r∖	1	l"∕∖3∕∖I	F	rʌzʌ	, ∙ ,	。	1
In Figure 2 we plot maxx∈X |f (x) - fn(x)| versus n, averaged over 20 repetitions, for each one
of the 9 experiments. Note that for maxχ∈χ |f (x) - fn(x)∣ 〜 nα, We have log(maxχ∈χ |f (x)-
fn(x)|) = α log(n) + constant. Thus, in our log scale plots, the slope of the line represents the
exponent of the error rate. As predicted analytically, We see all the exponents are negative (the error
converges to 0). In addition, the absolute value of the exponent is larger, When s is larger or d is
smaller. The bars in the plot on the left in Figure 2, shoW the standard deviation of the exponents.
For training of the model, We have used neural-tangents library (Novak et al., 2019) that is based
on JAX (Bradbury et al., 2018). The library is primarily suitable for κNT,1(.) corresponding to the
ReLU activation function. We thus made an amendment by directly feeding the expressions of the
RF kernels, κs, s = 2, 3, to the stax.Elementwise layer provided in the library. BeloW We give these
expressions
κ2(u)
κ3(u)
ɪ 3 sin(θ) cos(θ) + (π — θ)(1 + 2 cos2(θ)),
π
-----15sin(θ) — 11 sin3(θ) + (π — θ)(9cos(θ) + 6cos3(θ)),
15π
Where θ = arccos(u).
We derived these expressions using Lemma 1 in a recursive Way starting from κ1(.). Also, see Cho
& Saul (2009), Which provides a similar expression for κ2(.) and a general method to obtain κs(.)
for other values of s. We note that We only need to supply κs (.) to the neural-tangents library. The
NT kernel κNT,s(.) Will then be automatically created.
Our experiments run on a single GPU machine (GeForce RTX 2080 Ti) With 11 GB of VRAM
memory. Each one of the 180 experiments described above takes approximately 4 minutes to run.
27