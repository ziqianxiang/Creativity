Under review as a conference paper at ICLR 2022
Meta-Learning an Inference Algorithm for
Probabilistic Programs
Anonymous authors
Paper under double-blind review
Ab stract
We present a meta-algorithm for learning a posterior-inference algorithm for re-
stricted probabilistic programs. Our meta-algorithm takes a training set of proba-
bilistic programs that describe models with observations, and attempts to learn an
efficient method for inferring the posterior of a similar program. A key feature of
our approach is the use of what we call a white-box inference algorithm that ex-
tracts information directly from model descriptions themselves, given as programs.
Concretely, our white-box inference algorithm is equipped with multiple neural
networks, one for each type of atomic command, and computes an approximate
posterior of a given probabilistic program by analysing individual atomic com-
mands in the program using these networks. The parameters of the networks are
learnt from a training set by our meta-algorithm. We empirically demonstrate that
the learnt inference algorithm generalises well to programs that are new in terms
of both parameters and model structures, and report cases where our approach
achieves greater test-time efficiency than alternative approaches such as HMC. The
overall results show the promise as well as remaining challenges of our approach.
1 Introduction
One key objective of probabilistic programming is to automate reasoning about probabilistic models
from diverse domains (Ritchie et al., 2015; Perov & Wood, 2016; Baydin et al., 2019; Schaechtle et al.,
2016; Cusumano-Towner et al., 2017; Saad & Mansinghka, 2016; Kulkarni et al., 2015; Young et al.,
2019; Jager et al., 2020). As a way to realize this goal, researchers have extensively worked on the de-
velopment of posterior-inference or parameter-learning algorithms that are efficient and universal; the
algorithms can be applied to all or nearly all models written in probabilistic programming languages
(PPLs). This line of research has led to performant probabilistic programming systems (Goodman
et al., 2008; Wood et al., 2014; Mansinghka et al., 2014; Minka et al., 2018; Narayanan et al., 2016;
Salvatier et al., 2016; Carpenter et al., 2017; Tran et al., 2016; Ge et al., 2018; Bingham et al., 2018).
Yet, it also revealed the difficulty of achieving efficiency and universality simultaneously, and the
need for equipping PPLs with mechanisms for customising inference or learning algorithms to a
given domain. In fact, recent PPLs include constructs for specifying conditional independence in a
model (Bingham et al., 2018) or defining proposal or variational distributions (Ritchie et al., 2015;
Siddharth et al., 2017; Bingham et al., 2018; Tran et al., 2018; Cusumano-Towner et al., 2019), all
enabling users to help inference or learning algorithms.
In this paper, we explore a different approach. We present a meta-algorithm for learning a posterior-
inference algorithm itself from a given set of restricted probabilistic programs, which specifies a
class of probabilistic models, such as hierarchical or clustering models. The meta-algorithm aims at
constructing a customised inference algorithm for the given set of models, while ensuring universality
to the extent that the constructed algorithm can generalise: it works well for models not in the training
set, as long as the models are similar to the ones in the set.
The distinguished feature of our approach is the use of what we call a white-box inference algorithm,
which extracts information directly from model descriptions themselves, given as programs in a PPL.
Concretely, our white-box inference algorithm is equipped with multiple neural networks, one for
each type of atomic command in a PPL, and computes an approximate posterior for a given program
by analysing (or executing in a sense) individual atomic commands in it using these networks. For
instance, given the probabilistic program in Fig. 1, which describes a simple model on the Milky Way
1
Under review as a conference paper at ICLR 2022
mass ~ N(5,10); // log of the mass of Milky Way
gι ~ N(mass X 2, 5); obs(N(gι, 1), 10); // observed velocity Velι=10 of the first satellite galaxy
g2 〜N (mass + 5, 2); obs(N (g2,1), 3) // observed velocity Vel 2=3 of the second satellite galaxy
Figure 1:	Probabilistic program for a model for Milky Way and its two satellite galaxies. The obs
statements refer to the observations of (unnamed) random variables Vel1 and Vel2 .
galaxy, the white-box inference algorithm analyses the program as if an RNN handles a sequence or
an interpreter executes a program. Roughly, the algorithm regards the program as a sequence of the
five atomic commands (separated by the “;” symbol), initialises its internal state h ∈ Rm with h0 ,
and transforms the state over the sequence. The internal state h is the encoding of an approximate
posterior at the current program point, which corresponds to an approximate filtering distribution
of a state-space model. How to update this state over each atomic command is directed by neural
networks. Our meta-algorithm trains the parameters of these networks by trying to make the inference
algorithm compute accurate posterior approximations over a training set of probabilistic programs.
One can also view our white-box inference algorithm as a message-passing algorithm in a broad
sense where transforming the internal state h corresponds to passing a message, and understand our
meta-algorithm as a method for learning how to pass a message for each type of atomic commands.
This way of exploiting model descriptions for posterior inference has two benefits. First, it ensures
that even after customisation through the neural-network training, the inference algorithm does not
lose its universality and can be applied to any probabilistic programs. Thus, at least in principle, the
algorithm has a possibility to generalise beyond the training set; its accuracy degrades gracefully
as the input probabilistic program diverges from those in the training set. Second, our way of
using model descriptions guarantees the efficiency of the inference algorithm (although it does not
guarantee the accuracy). The algorithm scans the input program only once, and uses neural networks
whose input dimensions are linear in the size of the program. As a result, its time complexity is
quadratic over the size of the input program. Of course, the guaranteed speed also indicates that the
customisation of the algorithm for a given training set, whose main goal is to achieve good accuracy
for probabilistic programs in the set, is a non-trivial process.
Our contributions are as follows: (i) we present a white-box posterior-inference algorithm, which
works directly on model description and can be customised to a given model class; (ii) we describe a
meta-algorithm for learning the parameters of the inference algorithm; (iii) we empirically analyse
our approach with different model classes, and show the promise as well as the remaining challenges.
Related work The difficulty of developing an effective posterior-inference algorithm is well-known,
and has motivated active research on learning or adapting key components of an inference algorithm.
Techniques for adjusting an MCMC proposal (Andrieu & Thoms, 2008) or an HMC integrator (Hoff-
man & Gelman, 2014) to a given inference task were implemented in popular tools. Recently,
methods for meta-learning these techniques themselves from a collection of inference tasks have
been developed (Wang et al., 2018; Gong et al., 2019). The meta-learning approach also features
in the work on stochastic variational inference where a variational distribution receives information
about each inference task in the form of its dataset of observations and is trained with a collection
of datasets (Wu et al., 2020; Gordon et al., 2019; Iakovleva et al., 2020). For a message-passing-
style variational-inference algorithm, such as expectation propagation (Minka, 2001; Wainwright
& Jordan, 2008), Jitkrittum et al. (2015) studied the problem of learning a mechanism to pass a
message for a given single inference task. A natural follow-up question is how to meta-learn such
a mechanism from a dataset of multiple inference tasks that can generalise to unseen models. Our
approach provides a partial answer to the question; our white-box inference algorithm can be viewed
as a message-passing-style variational inference algorithm that can meta-learn the representation of
messages and a mechanism for passing them for given probabilistic programs.
Amortised inference and inference compilation (Gershman & Goodman, 2014; Le et al., 2017; Paige
& Wood, 2016; Stuhlmuller et al., 20l3; Kingma & Welling, 2013; Mnih & Gregor, 2014; Rezende
et al., 2014; Ritchie et al., 2016; Marino et al., 2018) are closely related to our approach in that they
also attempt to learn a form of a posterior-inference algorithm. However, the learnt algorithm by
them and that by ours have different scopes. The former is designed to work for unseen inputs or
observations of a single model, while the latter for multiple models with different structures. The
relationship between these two algorithms is similar to the one between a compiled program (to be
applied to multiple inputs) and a compiler (to be used for multiple programs).
2
Under review as a conference paper at ICLR 2022
U :=0; V :=5; W :=1; Zl 〜N(U, v); Z2 〜N(U, v);
Z3 〜N(u, w);	μ3	:= if	(z3 > u)	zι else	z?;	obs(N(μ3, w), —1.9); //	xι 〜N(μ3, w), xι = —1.9
z4 〜N(u, w);	μ4	:= if	(z4 > u)	zι else	z2;	obs(N(μ4, w), —2.2); //	χ 〜N(μ4, w), x2 = —2.2
Z5 〜N(u, w);	μ5	:= if	(z5 > u)	zι else	z2;	obs(N(μ5,w), 2.4); // x3 〜N(μ5,w), x3 = 2.4
Z6 〜N(u, w);	μ6	:= if	(z6 > U)	zι else	z2	obs(N(μ6, w), 2.2) // x4	〜N(μ6, w), x4 = 2.2
Figure 2:	Probabilistic program for a simple clustering model on four data points.
The idea of running programs with learnt neural networks also appears in the work on training
neural networks to execute programs (Zaremba & Sutskever, 2014; Bieber et al., 2020; Reed &
de Freitas, 2016). As far as we know, however, we are the first to frame the problem of learning a
posterior-inference algorithm as the one of learning to execute.
2	Setup
Our results assume a simple probabilistic programming language without loop and with a limited
form of conditional statement. The syntax of the language is given by the following grammar, where
r represents a real number, z and vi variables storing a real, and p the name of a procedure taking
two real-valued parameters and returning a real number:
Programs C ::= A | C1 ; C2
Atomic Commands A ::= Z 〜N(v1,v2) | obs(N(v0,v1),r) | v。:= if (vι > v2) v3 else v4
| v0 := r | v0 := v1 | v0 := p(v1 , v2 )
Programs in the language are constructed by sequentially composing atomic commands. The language
supports six types of atomic commands. The first type is Z 〜N(v1,v2), which draws a sample from
the normal distribution with mean v1 and variance v2, and assigns the sampled value to z. The second
command, obs(N(v0, v1), r), states that a random variable is drawn from N(v0, v1) and its value is
observed to be r. The next is a restricted form of a conditional statement that selects one of v3 and v4
depending on the result of the comparison v1 > v2 . The following two commands are different kinds
of assignments, one for assigning a constant and the other for copying a value from one variable
to another. The last atomic command v0 := p(v1, v2) is a call to one of the known deterministic
procedures, which may be standard binary operations such as addition and multiplication, or complex
non-trivial functions that are used to build advanced, non-conventional models. When p is a standard
binary operation, we use the usual infix notation and write, for example, v1 + v2, instead of +(v1, v2).
We permit only the programs where a variable does not appear more than once on the left-hand side
of the := and 〜symbols. This means that no variable is updated twice or more, and it corresponds to
the so-called static single assignment assumption in the work on compilers. This restriction lets us
regard variables updated by 〜as latent random variables. We denote those variables by zι,...,Zn.
We use this simple language for two reasons. First, the restriction imposed on our language enables
the simple definition of our white-box inference algorithm. The language supports only a limited
form of conditional statements and restricts the syntactic forms of atomic commands; the arguments
to a normal distribution or to a procedure p should be variables, not general expression forms such as
addition of two variables. As we will show soon, this restriction makes it easy to exploit information
about the type of each atomic command in our inference algorithm; we use different neural networks
for different types of atomic commands in the algorithm. Second, the language is intended to serve as
an intermediate language of a compiler for a high-level PPL, not the one to be used directly by the
end user. The compilation scheme in, for instance, §3 of (van de Meent et al., 2018) from high-level
probabilistic programs with general conditional statements and for loops to graphical models can be
adopted to compile such programs into our language. See Appendix A for further discussion.
Fig. 2 shows a simple model for clustering four data points {—1.9, —2.2, 2.4, 2.2} into two clusters,
where the cluster assignment of each data point is decided by thresholding a sample from the standard
normal distribution. The variables z1 and z2 store the centers of the two clusters, and z3 , . . . , z6 hold
the random draws that decide cluster assignments for the data points. See Appendix B for the Milky
Way example in Fig. 1 compiled to a program in our language.
Probabilistic programs in the language denote unnormalised probability densities over Rn for some n.
Specifically, for a program C, if z1, . . . , zn are all the variables assigned by the sampling statements
3
Under review as a conference paper at ICLR 2022
Zi 〜N (...) in C in that order and C contains m observe statements with observations rι,...,rm,
then C denotes an unnormalised density pC over the real-valued random variables z1, . . . , zn:
pC(z1:n) = pC (x1:m = r1:m |z1:n) × Qin=1pC(zi|z1:i-1), where x1, . . . , xm are variables not ap-
pearing in C and are used to denote observed variables. This density is defined inductively over the
structure of C. See Appendix C for details. The goal of our white-box inference algorithm is to
compute efficiently accurate approximate posterior and marginal likelihood estimate for a given C
(that is, for the normalised version of pC and the normalising constant of pC), when pC has a finite
non-zero marginal likelihood and, as a result, a well-defined posterior density. We next describe how
the algorithm attempts to achieve this goal.
3	White-b ox inference algorithm
Given a program C = (A1; . . . ; Ak), our white-box inference algorithm views C as a sequence of
its constituent atomic commands (A1, A2, . . . , Ak), and computes an approximate posterior and a
marginal likelihood estimate for C by sequentially processing the Ai ’s. Concretely, the algorithm
starts by initialising its internal state to h0 = ~0 ∈ Rs and the current marginal-likelihood estimate to
Z0 = 1. Then, it updates these two components based on the first atomic command A1 of C. It picks
a neural network appropriate for the type of A1 , applies it to h0 and gets a new state h1 ∈ Rs . Also,
it updates the marginal likelihood estimate to Z1 by analysing the semantics of A1 . This process is
repeated for the remaining atomic commands A2, A3, . . . , Ak of C, and eventually produces the last
state hk and estimate Zk . Finally, the state hk gets decoded to a probability density on the latent
variables of C by a neural network, which together with Zk becomes the result of the algorithm.
Formally, our inference algorithm is built on top of three kinds of neural networks: the ones for
transforming the internal state h ∈ Rs of the algorithm; a neural network for decoding the internal
states h to probability densities; and the last neural network for approximately solving integration
questions that arise from the marginal likelihood computation in observe statements. We present
these neural networks for the programs that sample n-many latent variables z1, . . . , zn, and use at
most m-many variables (so m ≥ n). Let V be [0, 1]m, the space of the one-hot encodings of those m
variables, and P the set of procedure names. Our algorithm uses the following neural networks:
nn sa,φ1 :	V3 × Rs → Rs,	nnob,φ2 : V2	× R × Rs → Rs,	nn if,φ3	: V5	×	Rs	→ Rs,
nnC=M ： V X R X Rs → Rs,	nn二@jV	X Rs → Rs,	nnp,φp	: V3	X	Rs	→ Rsfor P	∈ P,
nn de,φ6 : Rs → (R × R)n,	nn intg,φ7 : V2 × R × Rs → R,
where φ±7 and φp for P ∈ P are network parameters. The top six networks are for the six types of
atomic commands in our language. For instance, when an atomic command to analyse next is a sample
statement Z 〜N(vι ,v2),the algorithm runs the first network nnSa on the current internal state h, and
obtains a new state h0 = nnsa,φι (z, vɪ^, h), where Z and v12 mean the one-hot encoded variables
Z, v1 and v2. The next nnde,φ6 is a decoder of the states h to probability densities over the latent
variables Z1, . . . , Zn, which are the product of n independent normal distributions. The network maps
h to the means and variances of these distributions. The last nnintg,φ7 is used when our algorithm
updates the marginal likelihood estimate based on an observe statement obs(N(v0, v1), r). When we
write the meaning of this observe statement as the likelihood N (r; v0, v1), and the filtering distribution
for v0 and v1 under (the decoded density of) the current state h as Ph(v0, v1),1 the last neural network
computes the following approximation: nnintg,φ7 (v0?T, r, h) ≈ ʃ N(r; v0, v1)ph(v0, v1)dv0dv1. See
Appendix D for the full derivation of the marginal likelihood.
Given a program C = (A1; . . . ; Ak) that draws n samples (and so uses latent variables Z1, . . . , Zn),
the algorithm approximates the posterior and marginal likelihood of C as follows:
INFER(C) = let (h0, Z0) = (~0, 1) and (hk, Zk) = (INFER(Ak) ◦ . . . ◦ INFER(A1))(h0, Z0) in
let ((μι,σ2),..., (μn,σn)) = nndeg (hk) in return (Qn=I N(Zi I μi,σi2), Z，,
where INFER(Ai) : Rs X R → Rs X R picks an appropriate neural network based on the type of Ai,
and uses it to transform h and Z:
INFER(obs(N(v0, vι), r))(h, Z) = (nnob(V0Z, r, h), Z X nnintg(V0Z, r, h)),
1The ph(v0, v1) is a filtering distribution, not prior.
4
Under review as a conference paper at ICLR 2022
INFER(Vo := if (vι > v2) v3 else v4)(h, Z) = (nnif (v0∕, h), Z),
INFER(V0 := r)(h,Z) = (nnc=(V0, r, h),Z), INFER(Z 〜N(v1,v2))(h, Z) = (nnsa(Z, VΓ^,h),Z),
INFER(V0 := Vι)(h,Z) = (nnv=(v01, h), Z), INFER(V0 := p(v1,V2))(h,Z) = (nnP(G,h),Z).
We remind the reader that V0k refers to the sequence of the one-hot encodings of variables vo,...,vk.
For the update of the state h, the subroutine INFER(A) relies on neural networks. But for the
computation of the marginal likelihood estimate, it exploits prior knowledge that non-observe
commands do not change the marginal likelihood (except only indirectly by changing the filtering
distribution), and keeps the input Z for those atomic commands.
4	Meta-learning parameters
The parameters of our white-box inference algorithm are learnt from a collection of probabilistic
programs in our language. Assume that we are given a training set of programs D = {C1, . . . , CN}
such that each Ci samples n latent variables z1, . . . , zn and uses at most m variables. Let φ =
(01:7, (φp)p∈p) be the parameters of all the neural networks used in the algorithm. We learn these
parameters by solving the following optimisation problem:2
arg min X KL[∏c (zi：n)||qc (zi：n)] + j (NC - ZC )2
φ C∈D	2
where λ > 0 is a hyper-parameter, NC is the marginal likelihood (or the normalising constant)
pPC(zi：n)dzi：n for PC, the next ∏c(z±n) is the normalised posterior PC(zi：n)/NC for C, and the
last qC and ZC are the approximate posterior and marginal likelihood estimate computed by the
inference algorithm (that is, (qC(z1:n), ZC) = INFER(C)). Note that qC and ZC both depend on φ,
since INFER uses the φ-parameterised neural networks.
We optimise the objective by stochastic gradient descent. The key component of the optimisa-
tion is a gradient estimator derived as follows: (Vφ Pc∈d KL[∏c||qo] + λ(NC - ZC)2) =
____ _ 一 一 一 ， 、、一，_ _ 一	-≤—-- - ,‹ɔ` _ . _ _
(Pc∈D EzLn 〜∏c [-vφ log qC (z1:n)]—λ(NC -ZC )vφZC ) ≈ Pc∈d -LC,φ-λ(NC -ZC )V φZC.
Here Lc,φ and NC are sample estimates of EzLn〜∏0 [Vφ log qC(zi：n)] and the marginal likelihood,
respectively. Both estimates can be computed using standard Monte-Carlo algorithms. For instance,
we can run the self-normalising importance sampler with prior as proposal, and generate weighted sam-
ples {(w(j), z1(j:n))}1≤j ≤M for the unnormalised posterior PC . Then, we can use these samples to com-
pute the required estimates: NC =寺 PM=I w(j) and L[ =吉 PM=I(Wj V φ log qc (zj ))/dd.
Alternatively, we may run Hamiltonian Monte Carlo (HMC) (Duane et al., 1987) to generate posterior
samples, and use those samples to draw weighted importance samples using, for instance, the layered
adaptive importance sampler (Martino et al., 2017). Then, we compute LC,φ using posterior samples,
and NC using weighted importance samples. Note that neither ∏c in EzLn〜∏c [-Vφ log qc (zi：n)]
nor NC depends on the parameters φ. Thus, for each C ∈ D, NC needs to be estimated only once
throughout the entire optimisation process, and the posterior samples from πC need to be generated
only once as well. We use this fact to speed up the computation of each gradient-update step.
5	Empirical evaluation
An effective meta-algorithm should generalise well: the learnt inference algorithm should accurately
predict the posteriors of programs unseen during training which have different parameters (§5.1) and
model structures (§5.2), as long as the programs are similar to those in the training set. We empirically
show that our meta-algorithm learns such an inference algorithm, and that in some cases using the
learnt inference algorithm achieves higher test-time efficiency than alternative approaches such as
HMC (Duane et al., 1987) (§5.3). We implemented our inference algorithm and meta-algorithm using
ocaml-torch (Mazare, 2018), an OCaml binding for PyTorch. For HMC, we used the Python interface
for Stan (Carpenter et al., 2017). We used a Ubuntu server with Intel(R) Xeon(R) Gold 6234 CPU @
3.30GHz with 16 cores, 32 threads, and 263G memory. See Appendix E for the full list of our model
classes and their details, and Appendix F for the detailed experimental setup.
2Strictly speaking, we assume that the marginal likelihood of any C ∈ D is non-zero and finite.
5
Under review as a conference paper at ICLR 2022
Figure 3: Average training and test losses under three random seeds. The y-axes are log-scaled
increases in later epochs of Fig. 3c were due to only one or a few test programs out of 50.
(b) hierl
(c) milky
(a) gauss
1000	2000	3000	4000
Epoch
500	1000	1500	2000
Epoch
1000	2000	3000	4000	5000
Epoch
-100
150
loo L
-50
-100 卜
40
20
-40
-60
-80


















(a) Before training	(b) After 1K epochs	(c) After 2K epochs
Figure 4:	Comparisons of predicted and reference marginal posteriors recorded at different training
steps: at the initial step, after 1K epochs, and after 2K epochs.
5.1	Generalisation to new model parameters and observations
We evaluated our approach with six model classes: (1) Gaussian models (gauss) with a latent variable
and an observation where the mean of the Gaussian likelihood is an affine transformation of the latent;
(2) hierarchical models with three hierarchically structured latent variables (hierl); (3) hierarchical or
multi-level models with both latent variables and data structured hierarchically (hierd) where data
are modelled as a regression of latent variables at different levels; (4) clustering models (cluster)
where five observations are clustered into two groups; (5) Milky Way models (milky), and their
multiple-observations extension (milkyo) where five observations are made for each satellite galaxy;
and (6) models (rb) using the Rosenbrock function,3 which is expressed as an external procedure, to
show that our approach can in principle handle models with non-trivial computation blocks.
The purpose of our evaluation is to show the feasibility of our approach, not to develop the state-of-
the-art inference algorithm automatically, and also to identify the challenges of the approach. These
models are chosen for this purpose. For instance, an inference algorithm should be able to reason
about affine transformations and Gaussian distributions (for gauss), and dependency relationships
among variables (for hierl and hierd) to compute a posterior accurately. Successful outcomes in the
classes indicate that our approach learns an inference algorithm with such capacity in some cases.
Setup For each model class, we used 400 programs to meta-learn an inference algorithm, and then
applied the learnt algorithm to 50 unseen test programs. We measured the average test loss over
the 50 test programs, and checked if the loss also decreases when the training loss decreases. We
also compared the marginal posteriors predicted by our learnt inference algorithm with the reference
marginal posteriors that were computed analytically, or approximately by HMC. When we relied
on HMC, we computed the marginal sample means and standard deviations using one of the 10
Markov chains generated by independent HMC runs. Each chain consisted of 500K samples after
50K warmups. We ensured the convergence of the chains using diagnostics such as R (Gelman et al.,
1992). All training and test programs were automatically generated by a random program generator.
This generator takes a program class and hyperparameters (e.g., boundaries of the quantities that are
used to specify the models), and returns programs from the class randomly (see Appendix E).
For each training program, our meta-algorithm used 215 samples from the analytic (for gauss) or
approximate (for the rest, by HMC) posterior distribution for the program.4 Similarly, our meta-
3The function is often used to evaluate learning and inference algorithms (Goodman & Weare, 2010; Wang
& Li, 2018; Pagani et al., 2019)
4Except for rb; see the discussion on Rosenbrock models in Appendix H.
6
Under review as a conference paper at ICLR 2022
IT -
IOOB -
IOB -
IB
IOOM -
10M
~i IM
100k -
IOk
iooo
(a) To 1st dep. graph.
Seed, Tr or Te
一1, Tr
L Te
一2, Tr
--2, Te
3, Tr
-3, Te
10：5
IOOT
IOT
IT
IOOB
IOB
S	1B
O IOOM
IOM
IM
100k
IOk
1000
100
10
Seed, Tr or Te
一1, Tr
-elr Te
一2, Tr
--2, Te
3, Tr
—3, Te
2000
4000	6000
Epoch
8000
(c) To 3rd dep. graph.
(b) To 2nd dep. graph.

Figure 5:	Average losses for generalisation to dependency graphs in ext1. The y-axes are log-scaled.
(a) To 1st dep. graph.
(b) To 2nd dep. graph.
(c) To 3rd dep. graph.
Figure 6:	Average losses for generalisation to dependency graphs in ext2. The y-axes are log-scaled.
algorithm computed the marginal likelihood analytically (for gauss) or approximately (for the rest)
using layered adaptive importance sampling (Martino et al., 2017) where the proposals were defined
by an HMC chain. We performed mini-batch training; a single gradient update was done with a
training program and a mini batch of size 212 (out of 215 samples for the program). We used Adam
(Kingma & Ba, 2015) with its hyperparameters {β1 = 0.9, β2 = 0.999, weight_decay = 0}, and
the initial learning rate was set to 0.001. When the average training loss converged enough, the
training stopped. We repeated the same experiments three times using different random seeds.
Results Fig. 3 shows the training and test losses for gauss, hierl, and milky under three random seeds.
The losses for the other model classes are in Appendix G. The training loss was averaged over the
training set and 8 batch updates, and the test loss over the test set. The plots for training and test
losses are drawn as solid and dotted lines, respectively, and the results with different random seeds
are coloured differently. The training losses in all three experiments decreased rapidly, and more
importantly, these decreases were accompanied by the downturns of the test losses, which shows that
the learnt parameters generalised to the test programs well. The later part of Fig. 3c shows cases
where the test loss increases. This was because the loss of only a few programs in the test set (of 50
programs) became large. Even in this situation, the losses of the rest remained small.
Fig. 4 compares, for 10 test programs in hierl, the reference marginal posteriors (blue) and their
predicted counterparts (red) by the learnt inference algorithm instantiated at three different training
epochs. The predicted marginals were initially around zero (Fig. 4a), evolved to cover the reference
marginals (Fig. 4b), and finally captured them precisely in terms of both mean and standard deviation
for most of the variables (Fig. 4c). The results show that our meta-algorithm improves the parameters
of our inference algorithm, and eventually finds optimal ones that generalise well. We observed similar
patterns for the other model classes and random seeds, except for cluster and rb; programs from these
classes often have multimodal posteriors, and we provide an analysis for them in Appendix H.
5.2	Generalisation to new model structures
We let two kinds of model structure vary across programs: the dependency (or data-flow) graph for
the variables of a program and the position of a nonlinear function in the program. Specifically, we
considered two model classes: (1) models (ext1) with three Gaussian variables and one deterministic
variable storing the value of the function nl(x) = 50∕π X arctan(x∕10), where the models have 12
different types — four different dependency graphs of the variables, and three different positions
of the deterministic nl variable for each of these graphs; and (2) models (ext2) with six Gaussian
variables and one nl variable, which are grouped into five types based on their dependency graphs.
The evaluation was done for ext1 and ext2 independently as in §5.1, but here each of ext1 and ext2
7
Under review as a conference paper at ICLR 2022
Table 1: ESS per sec for the 60 test programs by HMC vs. IS-pred vs. IS-prior.
		HMC	IS-pred					IS-prior	
	GM	Q1	Q3	GM	Q1	Q3	GM	Q1	Q3
ESS	204.8K	4.1K	4.6M	4.2K	-^22K^^	13.8K	2.8K	1.1K	9.5K
Time	48.2s	27.4s	82.3s	22.7ms	21.4ms	23.0ms	23.1ms	22.1ms	24.0ms
ESS / sec	4.3K	124	127.7K	196.5K	102.4K	646.5K	123.8K	52.6K	436.1K
5000
4500
JS
5 408 ∙
E
g
•g 3500
i
¾ 3000
!
2500
2000
Elapsed time
(b) Moments by IS-pred and IS-prior.
——lS-ref HMC _ 白	, T
0.3s	1,1s	3.7s	273s	842s
Elapsed time
(a) Moments by HMC.
ιe∞o
MOOO
12000
lβ∞0
BOOO-
e∞o
4000
2000
一	IS-pred IS-prior		-σ-
			
—			-β-	—
∙1M∏S	* 2.33 ms	<v 4.9ms Elapsed time	a∣β.β5ms	*lβms
(c) ESS by IS-pred and IS-prior.
Figure 7
itself has programs of multiple (12 for ext1 and 5 for ext2) model types. See Fig. 10 and 11 in the
appendix for visualisation of the different model types in ext1 and ext2, respectively.
Setup For ext1, we ran seven different experiments. Three of them evaluated generalisation to unseen
positions of the nl variable, and the other four to unseen dependency graphs. Let Ti,j be the type in
ext1 that corresponds to the i-th position of nl and the j-th dependency graph, and T-i^ be all the
types in ext1 that correspond to any nl positions except the i-th and any of four dependency graphs.
For generalisation to the i-th position of nl (i = 1, 2, 3), We used programs from T-i,* for training
and those from Ti,* for testing. For generalisation to the j-th dependency graph (j = 1,2, 3,4),
we used programs from T*,-j for training and those from T*,j for testing. For ext2, we ran five
different experiments Where each of them tested generalisation to an unseen dependency graph after
training with the other four dependency graphs. All these experiments were repeated three times
under different random seeds. So, the total numbers of experiment runs were 21 (= 7 × 3) and
15 (= 5 × 3) for ext1 and ext2, respectively.
In each experiment run for ext1, we used 720 programs for training, and 90 (when generalising to new
graphs) or 100 (when generalising to new positions of the nl variable) unseen programs for testing.
In each run for ext2, we used 600 programs for training and tested the learnt inference algorithm on
50 unseen programs. We ran HMC to estimate posteriors and marginal likelihoods, and used 200K
samples after 10K warmups to compute reference posteriors. We stopped training after giving enough
time for convergence within a limit of computational resources. The rest was the same as in §5.1.
Results Fig. 5 shows the average training and test losses for generalisation to the first three depen-
dency graphs in ext1. Fig. 6 shows the losses for generalisation to the first three dependency graphs in
ext2. The losses for generalisation to the last dependency graph and to all positions of the nl variable
in ext1, and those for generalisation to the 4th and 5th dependency graphs in ext2 are in Appendices I
and J. In 17 runs (out of 21) for ext1, the decrease in the training losses eventually stabilised or
reduced the test losses, even when the test losses were high and fluctuated in earlier training epochs.
In 8 runs (out of 15) for ext2, the test losses were stabilised as the training losses decreased. In 4 runs
out of the other 7, the test losses increased only slightly. In terms of predicted posteriors, we observed
highly accurate predictions in 8 runs for ext1. For ext2, the predicted posteriors were accurate in
7 runs. For quantified accuracy, we refer the reader to Appendix K. Overall, the learnt algorithms
generalised to unseen types of models well or fairly well in many cases.
5.3	Test-time efficiency in comparison with alternatives
We demonstrate the test-time efficiency of our approach using three-variable models (mulmod)
where two latent variables follow normal distributions and the other stores the value of the function
mm(x) = 100 × x3/(10 +x4). The models are grouped into three types defined by their dependency
graphs and the positions of mm in the programs (see Fig.12 in the appendix). We ran our meta-
algorithm using 600 programs from all three types using importance samples (not HMC samples).
8
Under review as a conference paper at ICLR 2022
(a) Marginal posteriors
05	1	15
pgm19_z0
(b) Chain 1
pgm19_z0
(c) Chain 2	(d) Chain 7
Figure 8:	Marginal posteriors for the comparison, and contours of three HMC chains for pgm_19
where the x-axis is for z0 and the y-axis z1.
Then for 60 test programs from the last model type, we measured ESS and the sum of second
moments along the wall-clock time using three approaches: importance sampling (IS-pred; ours) with
the predicted posteriors as proposal using 70K samples, importance sampling (IS-prior) with prior
as proposal using 100K samples, and HMC with 1M samples after 500 warmups. As the reference
sampler, we used importance sampling (IS-ref) with prior as proposal using 5M samples. All the
approaches were repeated 10 times.
Table 1 shows the average ESS per unit time over the 60 test programs, by the three approaches. For
HMC, “ESS” is the ESS computed using 10 Markov chains averaged over the 60 programs, and
“ESS / sec” is the ESS per unit time, averaged over the programs. For IS-{pred, prior}, “ESS” and
“ESS / sec” are the average ESS and ESS per unit time, respectively, both over the 10 trials and the 60
programs. GM is the geometric mean, and Q1 and Q3 are the first and third quartiles, respectively.
We used the geometric mean, since the ESSes had outliers. The results show that IS-pred achieved
the highest ESS per unit time in terms of both mean (GM) and the quartiles (Q1 and Q3).
We provide further analysis for a test program (pgm19; see Appendix L). Fig. 7a and 7b show the
moments estimated by HMC and IS-{pred, prior}, respectively, in comparison with the same (across
the two figures) reference moments by IS-ref. The estimates by IS-pred (red) quickly converged to
the reference (green) within 18ms, while those by HMC (orange) did not converge even after 84s.
IS-pred (red) and IS-prior (blue) tended to produce better estimates as the elapsed time increased,
but each time, IS-pred estimated the moments more precisely with a smaller variance than IS-prior.
In the same runs of the three approaches as in the last columns of Fig. 7a and 7b, IS-pred produced
over 16K effective samples in 18ms, while HMC generated only 80 effective samples even after 84s.
Similarly, IS-prior generated fewer than 1.4K effective samples in the approximately same elapsed
time as in IS-pred. In fact, Fig. 7c shows that as the time increases, the gap between the ESSes of
IS-pred and IS-prior gets widen, because the former increases at a rate significantly higher than the
latter. Note that IS-pred has to scan a program twice at test time, once for computing the proposal
and another for IS with the predicted proposal. See Appendix M for discussion.
Our manual inspection revealed that the programs in mulmod often have multimodal posteriors.
Fig. 8a shows the posteriors for {pgm19, pgm30, pgm38} in the test set, computed by our learnt
inference algorithm (without IS), HMC (200K samples after 10K warmups) , and IS-ref. The variable
z0 in the three programs had multimodal posteriors. For pgm19, the learnt inference algorithm took
only 0.6ms to compute the posteriors, while HMC took 120s on average to generate a chain. The
predictions (red) from the learnt inference algorithm for z0 describe the reference posteriors (green)
better than those (blue) by HMC in terms of mean, variance, and mode covering.5 The contour plots
in Fig. 8b to 8d visualise three HMC chains for pgm19. Here, HMC failed to converge, and Fig. 8b
explains the poor estimate (blue) in the first column of Fig. 8a.
Conclusion In this paper, we presented a white-box inference algorithm that computes an approxi-
mate posterior and a marginal likelihood estimate by analysing the given program sequentially using
neural networks, and a meta-algorithm that learns the network parameters over a training set of
probabilistic programs. In our experiments, the meta-algorithm learnt an inference algorithm that
generalises well to similar but unseen programs, and the learnt inference algorithm sometimes had
test-time advantages over alternatives. A moral of this work is that the description of a probabilistic
model itself has useful information, and learning to extract and exploit the information may lead to an
efficient inference. We hope that our work encourages further exploration of this research direction.
5Our inference algorithm in a multimodal-posterior case leads to a good approximation in the following
sense: the approximating distribution q covers the regions of the modes well, and also approximates the mean
and variance of the target distribution accurately. Note that such a q is useful when it is used as the proposal of
an importance sampler.
9
Under review as a conference paper at ICLR 2022
Reproducibility statement Our paper provides detailed information that is needed for reproducing
the results. In each of §5.1, §5.2, and §5.3 of the main text, we explain the key experimental
design and setup clearly. More detailed experimental setup is in Appendix F, where we specify the
hyperparameter and the design of the neural networks. In Appendix E, we provide full details of the
model classes and how programs from the classes were automatically generated in our evaluation.
References
Luca Ambrogioni, Gianluigi Silvestri, and Marcel van Gerven. Automatic variational inference with
cascading flows. arXiv preprint arXiv:2102.04801, 2021.
Christophe Andrieu and Johannes Thoms. A tutorial on adaptive MCMC. Stat. Comput., 18(4):
343-373, 2008.
Atilim Gunes Baydin, Lei Shao, Wahid Bhimji, Lukas Heinrich, Saeid Naderiparizi, Andreas Munk,
Jialin Liu, Bradley Gram-Hansen, Gilles Louppe, Lawrence Meadows, Philip Torr, Victor Lee,
Kyle Cranmer, Mr. Prabhat, and Frank Wood. Efficient probabilistic inference in the quest for
physics beyond the standard model. In Advances in Neural Information Processing Systems,
volume 32, pp. 5459-5472. Curran Associates, Inc., 2019.
David Bieber, Charles Sutton, Hugo Larochelle, and Daniel Tarlow. Learning to execute programs
with instruction pointer attention graph neural networks. In Advances in Neural Information
Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020,
NeurIPS 2020, December 6-12, 2020, virtual, 2020.
Eli Bingham, Jonathan P. Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Pradhan, Theofanis
Karaletsos, Rohit Singh, Paul Szerlip, Paul Horsfall, and Noah D. Goodman. Pyro: Deep universal
probabilistic programming. Journal of Machine Learning Research, 2018.
Bob Carpenter, Andrew Gelman, Matthew D Hoffman, Daniel Lee, Ben Goodrich, Michael Be-
tancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. Stan: A probabilistic
programming language. Journal of statistical software, 76(1), 2017.
Marco F Cusumano-Towner, Alexey Radul, David Wingate, and Vikash K Mansinghka. Probabilistic
programs for inferring the goals of autonomous agents. arXiv preprint arXiv:1704.04977, 2017.
Marco F. Cusumano-Towner, Feras A. Saad, Alexander K. Lew, and Vikash K. Mansinghka. Gen: a
general-purpose probabilistic programming system with programmable inference. In Proceedings
of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation,
PLDI 2019, Phoenix, AZ, USA, June 22-26, 2019, pp. 221-236. ACM, 2019.
Simon Duane, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth. Hybrid Monte Carlo.
Physics letters B, 195(2):216-222, 1987.
Hong Ge, Kai Xu, and Zoubin Ghahramani. Turing: Composable inference for probabilistic pro-
gramming. In International Conference on Artificial Intelligence and Statistics, AISTATS 2018,
9-11 April 2018, Playa Blanca, Lanzarote, Canary Islands, Spain, volume 84 of Proceedings of
Machine Learning Research, pp. 1682-1690. PMLR, 2018.
Andrew Gelman, Donald B Rubin, et al. Inference from iterative simulation using multiple sequences.
Statistical science, 7(4):457-472, 1992.
Samuel Gershman and Noah Goodman. Amortized inference in probabilistic reasoning. In Proceed-
ings of the annual meeting of the cognitive science society, volume 36, 2014.
Wenbo Gong, Yingzhen Li, and Jose Miguel Hernandez-Lobato. Meta-Iearning for stochastic gradient
MCMC. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans,
LA, USA, May 6-9, 2019. OpenReview.net, 2019.
Jonathan Goodman and Jonathan Weare. Ensemble samplers with affine invariance. Communications
in applied mathematics and computational science, 5(1):65-80, 2010.
10
Under review as a conference paper at ICLR 2022
Noah D Goodman, Vikash K Mansinghka, Daniel Roy, Keith Bonawitz, and Joshua B Tenenbaum.
Church: a language for generative models. In Proceedings of the Twenty-Fourth Conference on
Uncertainty in Artificial Intelligence, pp. 220-229, 2008.
Jonathan Gordon, John Bronskill, Matthias Bauer, Sebastian Nowozin, and Richard E. Turner. Meta-
learning probabilistic inference for prediction. In 7th International Conference on Learning
Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.
Matthew D. Hoffman and Andrew Gelman. The No-U-Turn sampler: adaptively setting path lengths
in Hamiltonian Monte Carlo. J. Mach. Learn. Res., 15(1):1593-1623, 2014.
Ekaterina Iakovleva, Jakob Verbeek, and Karteek Alahari. Meta-learning with shared amortized
variational inference. In Proceedings of the 37th International Conference on Machine Learning,
ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning
Research, pp. 4572-4582. PMLR, 2020.
Wittawat Jitkrittum, Arthur Gretton, Nicolas Heess, S. M. Ali Eslami, Balaji Lakshminarayanan,
Dino Sejdinovic, and Zoltgn Szab6. Kernel-based just-in-time learning for passing expectation
propagation messages. In Proceedings of the Thirty-First Conference on Uncertainty in Artificial
Intelligence, UAI 2015, July 12-16, 2015, Amsterdam, The Netherlands, pp. 405-414. AUAI Press,
2015.
Lena A. Jager, Daniela Mertzen, Julie A. Van Dyke, and Shravan Vasishth. Interference patterns in
subject-verb agreement and reflexives revisited: A large-sample study. Journal of Memory and
Language, 111:104063, 2020. ISSN 0749-596X.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd International
Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings, 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint
arXiv:1312.6114, 2013.
Tejas D Kulkarni, Pushmeet Kohli, Joshua B Tenenbaum, and Vikash Mansinghka. Picture: A
probabilistic programming language for scene perception. In Proceedings of the ieee conference
on computer vision and pattern recognition, pp. 4390-4399, 2015.
Tuan Anh Le, Atilim Gunes Baydin, and Frank Wood. Inference compilation and universal proba-
bilistic programming. In Artificial Intelligence and Statistics, pp. 1338-1348. PMLR, 2017.
Vikash Mansinghka, Daniel Selsam, and Yura Perov. Venture: a higher-order probabilistic program-
ming platform with programmable inference. arXiv preprint arXiv:1404.0099, 2014.
Joe Marino, Yisong Yue, and Stephan Mandt. Iterative amortized inference. In International
Conference on Machine Learning, pp. 3403-3412, 2018.
Luca Martino, Victor Elvira, David Luengo, and Jukka Corander. Layered adaptive importance
sampling. Statistics and Computing, 27(3):599-623, 2017.
Laurent Mazare. ocaml-torch: OCaml bindings for pytorch, 2018. URL https://github.com/
LaurentMazare/ocaml-torch.
T. Minka, J.M. Winn, J.P. Guiver, Y. Zaykov, D. Fabian, and J. Bronskill. /Infer.NET 0.3, 2018.
Microsoft Research Cambridge. http://dotnet.github.io/infer.
Thomas P. Minka. Expectation propagation for approximate Bayesian inference. In UAI ’01: Pro-
ceedings of the 17th Conference in Uncertainty in Artificial Intelligence, University of Washington,
Seattle, Washington, USA, August 2-5, 2001, pp. 362-369. Morgan Kaufmann, 2001.
Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. In
International Conference on Machine Learning, pp. 1791-1799, 2014.
11
Under review as a conference paper at ICLR 2022
Praveen Narayanan, Jacques Carette, Wren Romano, Chung-chieh Shan, and Robert Zinkov. Prob-
abilistic inference by program transformation in hakaru (system description). In International
Symposium on Functional and Logic Programming - 13th International Symposium, FLOPS 2016,
Kochi, Japan, March 4-6, 2016, Proceedings, pp. 62-79. Springer, 2016.
Filippo Pagani, Martin Wiegand, and Saralees Nadarajah. An n-dimensional rosenbrock distribution
for mcmc testing. arXiv preprint arXiv:1903.09556, 2019.
Brooks Paige and Frank Wood. Inference networks for sequential Monte Carlo in graphical models.
In International Conference on Machine Learning, pp. 3040-3049, 2016.
Yura Perov and Frank Wood. Automatic sampler discovery via probabilistic programming and
approximate Bayesian computation. In Artificial General Intelligence, pp. 262-273, Cham, 2016.
Springer International Publishing. ISBN 978-3-319-41649-6.
Scott E. Reed and Nando de Freitas. Neural programmer-interpreters. In 4th International Conference
on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference
Track Proceedings, 2016.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. In International Conference on Machine
Learning, pp. 1278-1286, 2014.
Daniel Ritchie, Ben Mildenhall, Noah D. Goodman, and Pat Hanrahan. Controlling procedural
modeling programs with stochastically-ordered sequential Monte Carlo. ACM Trans. Graph., 34
(4), July 2015. ISSN 0730-0301.
Daniel Ritchie, Paul Horsfall, and Noah D Goodman. Deep amortized inference for probabilistic
programs. arXiv preprint arXiv:1610.05735, 2016.
Feras Saad and Vikash K Mansinghka. A probabilistic programming approach to probabilistic data
analysis. In Advances in Neural Information Processing Systems, pp. 2011-2019, 2016.
John Salvatier, Thomas V Wiecki, and Christopher Fonnesbeck. Probabilistic programming in python
using pymc3. PeerJ Computer Science, 2:e55, 2016.
Ulrich Schaechtle, Feras Saad, Alexey Radul, and Vikash Mansinghka. Time series structure discovery
via probabilistic program synthesis. arXiv preprint arXiv:1611.07051, 2016.
N. Siddharth, Brooks Paige, Jan-Willem van de Meent, Alban Desmaison, Noah D. Goodman,
Pushmeet Kohli, Frank Wood, and Philip Torr. Learning disentangled representations with semi-
supervised deep generative models. In Advances in Neural Information Processing Systems 30, pp.
5927-5937. Curran Associates, Inc., 2017.
Andreas Stuhlmuller, Jacob Taylor, and Noah Goodman. Learning stochastic inverses. In Advances
in neural information processing systems, pp. 3048-3056, 2013.
Dustin Tran, Alp Kucukelbir, Adji B. Dieng, Maja Rudolph, Dawen Liang, and David M.
Blei. Edward: A library for probabilistic modeling, inference, and criticism. arXiv preprint
arXiv:1610.09787, 2016.
Dustin Tran, Matthew D. Hoffman, Dave Moore, Christopher Suter, Srinivas Vasudevan, and Alexey
Radul. Simple, distributed, and accelerated probabilistic programming. In Advances in Neural
Information Processing Systems 31: Annual Conference on Neural Information Processing Systems
2018, NeurIPS 2018, December 3-8,2018, Montreal, Canada,pp. 7609-7620, 2018.
Jan-Willem van de Meent, Brooks Paige, Hongseok Yang, and Frank Wood. An introduction to
probabilistic programming. arXiv preprint arXiv:1809.10756, 2018.
Martin J. Wainwright and Michael I. Jordan. Graphical models, exponential families, and variational
inference. Found. Trends Mach. Learn., 1(1-2):1-305, 2008.
Hongqiao Wang and Jinglai Li. Adaptive gaussian process approximation for Bayesian inference
with expensive likelihood functions. Neural computation, 30(11):3072-3094, 2018.
12
Under review as a conference paper at ICLR 2022
Tongzhou Wang, Yi Wu, Dave Moore, and Stuart J. Russell. Meta-learning MCMC proposals. In
Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information
Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada, pp. 4150-4160,
2018.
Frank Wood, Jan Willem van de Meent, and Vikash Mansinghka. A new approach to probabilistic pro-
gramming inference. In Proceedings of the 17th International conference on Artificial Intelligence
and Statistics, pp. 1024-1032, 2014.
Mike Wu, Kristy Choi, Noah D Goodman, and Stefano Ermon. Meta-amortized variational inference
and learning. In AAAI, pp. 6404-6412, 2020.
Yujun Yan, Kevin Swersky, Danai Koutra, Parthasarathy Ranganathan, and Milad Hashemi. Neural
execution engines: Learning to execute subroutines. In Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual, 2020.
Jean-Gabriel Young, Fernanda S Valdovinos, and Mark EJ Newman. Reconstruction of plant-
pollinator networks from observational data. bioRxiv, pp. 754077, 2019.
Wojciech Zaremba and Ilya Sutskever. Learning to execute. CoRR, abs/1410.4615, 2014. URL
http://arxiv.org/abs/1410.4615.
13
Under review as a conference paper at ICLR 2022
A Further discussion about the translation of an expressive PPL
INTO OUR INTERMEDIATE LANGUAGE
Programs with recursion or while loops cannot generally be translated into our intermediate language,
since such programs may go into infinite loops while the programs in our language always terminate.
Programs with for loops and general branches can in theory be translated into a less expressive
language such as ours. For example, van de Meent et al. (2018) explain a language called FOPPL
(Section 2), which has for loops and branches, and the translation of FOPPL into graphical models
(Section 3). We think that these graphical models can be translated into programs in our language. Of
course, this does not mean that the learnt inference algorithm would interact well with the compilation;
the interaction between compilation and inference in the context of meta-learning is something to be
explored in future work.
one := 1; t := 2; f := 5; ten := 10;
zι 〜N (f, ten); // log of the mass of Milky Way
mass 1 := z1 × t;
z2 〜N(massι,f); // for the first satellite galaxy
obs(N(z2, one), ten); // xι = 10 for xι 〜N(z2, one)
mass 2 := z1 + f;
z3 〜N(mass2,t); // for the second satellite galaxy
obs(N(z3, one), 3) // x2 = 3 for x2 〜N(z3, one)
Figure 9:	Milky Way example compiled to the probabilistic programming language used in the paper.
B	Milky Way example in the probabilistic programming language
Fig.	9 shows the compiled version of the Milky way example to the probabilistic programming
language of the paper.
C Formal semantics of the probabilistic programming language
In §2, we stated that a program C in our language denotes an unnormalised density pC that is
factorised as follows:
n
pC (z1:n) = pC (x1
:m = r1:m|z1:n) ×	pC(zi|z1:i-1).
i=1
Here zι,...,zn are all the variables assigned by the sampling statements Zi 〜N (...) in C in
that order, the program C contains m observe statements with observations r1, . . . , rm, and these
observed random variables are denoted by x1 , . . . , xm . The goal of this section is to provide the
details of our statement. That is, we describe the formal semantics of our probabilistic programming
language, and from it, we derive a map from programs C to unnormalised densities pC .
To define the formal semantics of programs in our language, we need a type system that tracks
information about updated variables and observations, and also formalises the syntactic conditions
that we imposed informally in §2. The type system lets us derive the following judgements for
programs C and atomic commands A:
(S, V, α) `1 C: (T,W,β), (S, V, α) `2 A: (T,W,β),
where S and T are sequences of distinct variables, V and W are sets of variables that do not appear
in S and T, respectively, and α and β are sequences of reals. The first judgement says that if before
running the program C, the latent variables in S are sampled in that order, the program variables
in V are updated by non-sample statements, and the real values in the sequence α are observed in
that order, then running C changes these three data to T, W, and β. The second judgement means
14
Under review as a conference paper at ICLR 2022
the same thing except that we consider the execution of A, instead of C. The triples (S, V, α) and
(T, W, β ) serve as types in this type system.
The rules for deriving the judgements for C and A follow from the intended meaning just explained.
We show these rules below, using the notation @ for the concatenation operator for two sequences
and also set(S) for the set of elements in the sequence S:
(R, U, α)	'ι	Ci	:	(S, V, β)	(S, V, β)	'1	C?	: (T, W, Y	(S,V,α)'2 A =	(T,W,β)
(R,U,α) '1 (Ci； C2):(T,W,7)	(SVoFrAΓ7T;Wj)
z 6∈ set(S ) ∪ V v1 , v2 ∈ set(S ) ∪ V
(S,V,α) '2 (z ^N(v1,v2)) : (S@[z],V,a)
v0, v1 ∈ set(S) ∪ V
(S,V,α) '2 obs(N(v0,v1),r) : (S, V, a@[r])
v0 6∈ set(S) ∪ V v1 , v2 , v3, v4 ∈ set(S) ∪ V
(S, V, α) '2 (vo := if (vi > v2) v3 else v4) : (S, V ∪ {v°}, α)
vo ∈ Set(S) ∪ V	vo ∈ Set(S) ∪ V vi ∈ Set(S) ∪ V
(S, V,	α)	'2	(vo	:=	r)	:	(S, V ∪{v°}, α)	(S,	V, α) '2	(v°	:= vi)	:	(S,V ∪{v°}, α)
v0 6∈ Set(S ) ∪ V vi , v2 ∈ Set(S ) ∪ V
(S, V, α) '2 (vo := p(vi,v2)) : (S,V ∪{vo},α)
We now define our semantics, which specifies mappings from judgements for C andA to mathematical
entities. First, we interpret each type (S, V, α) as a set, and it is denoted by J(S, V, α)K:
J(S,V,α)K = {(p,f,l) | pis a (normalised) densityonR|S|, f = (fv)v∈set(S)∪V,
each fv is a measurable map from R|S| to R,
l is a measurable function from R|S| X R|a| to R+},
where |S| and ∣α∣ are the lengths of the sequences S and α, and R+ means the set of positive reals.
Next, we define the semantics of the judgements (S, V, α) 'i C : (T, W, β) and (S, V, α) '2 A :
(T, W, β) that can be derived by the rules from above. The formal semantics of these judgements,
denoted by the J-K notation, are maps of the following type:
J(S,V,α) 'i C: (T,W,β)K : J(S,V,α)K → J(T,W,β)K,
J(S,V,α)'2A: (T,W,β)K : J(S,V,α)K → J(T,W,β)K.
The semantics is given by induction on the size of the derivation of each judgement, under the
assumption that for each procedure name p ∈ P, we have its interpretation as a measurable map from
R2 to R:
JpK : R2 → R.
We spell out the semantics below, first the one for programs and next that for atomic commands.
J(S,V,α) 'iA: (T,W,β)K(p,f,l) = J(S,V,α) '2A: (T, W, β)K(p, f, l),
J(R, U, α) 'i (Ci;C2) : (T,W,γ)K(p,f,l) =(J(S,V,β) '2 C2 : (T,W,γ)K
◦ J(R, U, α) '2 Ci : (S,V,β)K)(p,f,l).
Let N(a; b, c) be the density of the normal distribution with mean b and variance c when c > 0 and 1
when c ≤ 0. For a family of functions f = (fv)v∈V , a variable w 6∈ V, and a function fw0 , we write
f ㊉ fW for the extension of f with a new w-indexed member fW.
J(S,V,a) '2 Z 〜N(Vi,v2): (S@[z],V,a)K(P,f, * I) = (P0,f0,l0)
(where p0(ai:|S|+i) = p(ai:|S|) × N (a|S|+i; fv1 (ai:|S|), fv2 (ai:|S|)),
fv0 (ai:|S|+i) = fv (ai:|S | ) for all v ∈ V, fz0 (ai:|S|+i) = a|S|+i,
l0(ai:|S| + i, bi:IaI) = l(ai:|S|, bi:IaI)),
J(S,V,a) '2 Obs(N(V0,vi),r): (S, V,a@[r])K(p,f,l) = (p,f,l0)
15
Under review as a conference paper at ICLR 2022
(where I0(ai：|s|,bi：|a|+i) = l(ai：|s|, bi：|a|) × N(b|a|+i； fvι (ai：|s|), fv2(ai：|s|)),
J(S, V, α) '2 (vo := if (vι > v2) v3 else v4) : (S, V ∪ {vo},α)K(p,f,l) = (p, f ㊉ f 0, l)
(where fv00(a1:|S|) = if (fv1 (a1:|S|) > fv2(a1:|S|)) then fv3(a1:|S|) else fv4 (a1:|S|)),
J(S, V,a) '2 (VO := r) : (S,v ∪ {VOha)K(P,f, I) = (P,f ㊉ fV0, I)
(wherefv00(a1:|S|) = r),
J(S, V,a) '2 (VO := VI) : (S,v ∪ {VOha)K(P,f,l) = (P,f ㊉ fV0,l)
(wherefv00(a1:|S|) = fv1 (a1:|S|)),
J(S, V,a) '2 (VO := P0(vo,VI)) : (S, V ∪ {VOha)K(P,f,l) = (P,f ㊉ fV 0, l)
(wherefV00(a1:|S|) = JP0K(fV0 (a1:|S|), fV1 (a1:|S|))).
Finally, we define PC for the well-initialised well-typed programs C, i.e., programs C for which we
can derived
([], 0,[])'1 C :(S,V,a).
For such a C, the definition of PC is given below:
PC(z1:|S|) =P(z1:|S|) × l(z1:|S|, a)
where (p, _, l) = J([], 0, []) '1 C : (S,V, a)K(PO, f,l°) for the constant-1 functions po and Io of
appropriate types and the empty family fO of functions.
D Marginal likelihood computation: derivation and correctness
Let xn be the random variable (RV) that is observed by the command obs(N(VO, V1), r) and x1:(n-1)
be the (n- 1) RVs that are observed before the command. When our algorithm is about to analyse this
observe command, we have (an estimate of) P(x1:(n-1)) by induction. Then, the marginal likelihood
of x1:n can be computed as follows:
P(x1:(n-1) , xn)
Il
ZZ
P(x1:(n-1), xn, VO,V1) dVO dV1
P(x1:(n-1)) P(VO, V1|x1:(n-1))P(xn|x1:(n-1),VO,V1) dVO dV1
≈ P(x1:(n-1))	Ph(VO, V1) P(xn |x1:(n-1), VO, V1) dVO dV1
// The filtering distribution P(VO, V1 |x1:(n-1)) is approximated byPh.
= P(x1:(n-1))	Ph(VO, V1)P(xn|VO, V1) dVO dV1
// The RV xn is conditionally independent of x1:(n-1) given VO, V1.
= P(x1:(n-1))	Ph(VO, V1) N (r; VO,V1) dVO dV1
//P(x1:(n-1)) is Z in the description of INFER(Ai) in Section 4, and the neural network
// nnintg,φ7 aims at approximating the integral term accurately.
This derivation leads to the equation in the main text.
In a setting of probabilistic programming where observations are allowed to be different in true and
false branches, the marginal likelihood may fail to be defined, and such a setting is beyond the scope
of our language. Using variables multiple times or having observe commands spread out in the
program does not make differences in the derivation above.
16
Under review as a conference paper at ICLR 2022
Table 2: Full list of the model classes in the empirical evaluation.
Section	Model class	Description	Detail
§5.1	gauss	Gaussian models with a latent variable and an observation where the mean of the Gaussian likelihood is an affine transformation of the latent.	Appendix E.1.1
	^hierι	Hierarchical models with three hierarchically structured latent variables.	APPendiXE.1.2
	hierd	Hierarchical or multi-level models with both latent variables and data structured hierarchi- cally where data are modelled as a regression of latent variables of different levels.	Appendix E.1.3
	cluster	Clustering models where five observations are clustered into two groups.	APPendiXE.1.4
	milky and milkyo	Milky Way models,^^and their multiple- observations extension where five observations are made for each satellite galaxy.	Appendix E.1.5
	Tb	Models with the Rosenbrock function, which is expressed as an external procedure.	APPendiXE.1.6
§5.2	ext1	Models with three Gaussian variables and one deterministic variable storing the value of the function nl(x) = 50/π Xarctan(x/10), Where the models have 12 different types — four dif- ferent dependency graphs of the variables, and three different positions of the deterministic nl variable for each of these graphs.	Appendix E.2.1 (and Fig. 10)
	ext2	Models with six Gaussian variables and one nl variable, which are grouped into five model types based on their dependency graphs.	Appendix E.2.2 (and Fig. 11)
§5.3	mulmod	Three-variable models where two latent vari- ables follow normal distributions and the other stores the value of the function mm(x) = 100 × x3/(10 + x4). The models in this class are grouped into three types defined by their dependency graphs and the positions of mm in the programs.	Appendix E.3 (and Fig. 12)
17
Under review as a conference paper at ICLR 2022
E Detailed descriptions for probabilistic models used in the
EMPIRICAL EVALUATION
Table 2 shows the full list of the model classes that we considered in our empirical evaluation (§5).
We detail the program specifications for the classes using the probabilistic programming language in
§2, and then describe how our program generator generated programs from those classes randomly.
In the program specifications to follow, randomly-generated constants are written in the Greek
alphabets (θ), and latent and other program variables in the English alphabets. Also, we often use
more intuitive variable names instead of using zi for latent variables and vi for the other program
variables, to improve readability. When describing random generation of the parameter values, we let
U(a, b) denote the uniform distribution whose domain is (a, b) ⊂ R; we use this only for describing
the random program generation process itself, not the generated programs (only normal distributions
are used in our programs, with the notation N).
E.1	Generalisation to new model parameters and observations
This section details the model classes in §5.1.
E.1.1 gauss
The model class is described as follows:
mz := θ1 ; vz := θ2; c1 := θ3; c2 := θ4; vx := θ5;
Zl 〜N(mz,Vz); Z2 := Zl X Cl； Z3 := Z2 + C2；
obs(N(z3, vx), o)
For each program of the class, our random program generator generated the parameter values as
follows:
θι 〜U(-5, 5), θ2 〜U(0, 20), θ2 = (θ2)2, θ3 〜U(-3, 3)
θ4 〜U(-10,10), θ5 〜U(0.5,10), θ5 = (θ5)2
and then generated the observation o by running the program forward where the value for Z1 was
sampled from zι 〜U(mz — 2 × √Vz, m% + 2 × √Vz).
E.1.2 hierl
The model class is described as follows:
mg := θ1; vg := θ2; vt1 := θ3; vt2 := θ4; vx1 := θ5;
vx2 := θ60; g 〜 N(mg,vg); t1 〜 N(g,vt1); t2 〜 N(g,vt2);
obs(N(t1,vx1), o1); obs(N(t2, vx2), o2)
For each program of the class, our generator generated the parameter values as follows:
θ1 〜 U(—5, 5), θ2 〜 U(0, 50), θ20 = (θ2)2, θ3 〜 U(0, 10)
θ30 = (θ3)2, θ4 〜 U(0, 10), θ40 = (θ4)2, θ5 〜 U(0.5, 10)
θ50 = (θ5)2, θ6 〜 U(0.5, 10), θ60 = (θ6)2
and then generated the observations o1 and o2 by running the program (i.e., simulating the model)
forward.
E.1.3 hierd
The model class is described as follows:
ma0 := θ1; va0 := θ2; va1 := θ3; va2 := θ4; mb := θ5;
vb := θ6; d1 = θ7; d2 = θ8; vx1 := θ9; vx2 := θ10;
18
Under review as a conference paper at ICLR 2022
ao 〜N(mao,Vao); aι 〜N(ao,Vαι);。2 〜N(ao,Vα2)；
b 〜N(mb,Vb);
t1 := b × d1; t2 := a1 + t1; obs(N(t2, vx1), o1);
t3 := b × d2; t4 := a2 + t3; obs(N(t4, vx2 ), o2)
For each program of the class, our generator generated the parameter values as follows:
θι 〜U(-10,10), θ2 〜U(0,100), θ2 = (θ2)2, θ3 〜U(0,10)
θ3 = (θ3)2, θ4 〜U(0,10), θ4 = (θ4)2, θ5 〜U(-5, 5)
θ6 〜U(0,10), θ6 = (θ6)2, θ7 〜U(-5, 5), θ8 〜U(-5, 5)
θ9 〜U(0.5,10), θ9 = (θ9)2, θ10 〜U(0.5,10), θ10 = (θ10)2
and then generated the observations o1 and o2 by running the program forward where the values for
a0, a1, a2, and b in this specific simulation were sampled as follows:
a0 〜U(mao - 2 X √v∑0, m。。+2 X √v∑0)
aι 〜U(a0 - 2 X √VO7, a0 + 2 X √VO7)
a，2 〜U(a0 - 2 X √^, a0 + 2 X √^)
b 〜U(mb - 2 X √vb, mb + 2 X √vb)
E.1.4 cluster
The model class is described as follows:
mg1 := θ1; vg1 := θ2 ; mg2 := θ3; vg2 := θ4; vx := θ5 ;
gι 〜N(mgι ,vgι )； g2 〜N(mg2 ,vg2 )；
zero := 0; hund := 100;
ti 〜N(zero, hund); mi := if (tι > zero) gι else g2;
obs(N(m1, vx), o1);
t2 〜N (zero, hund); m2 := if (t2 > zero) gi else g2;
obs(N(m2, vx), o2 );
t3 〜N (zero, hund); m3 := if (t3 > zero) gi else g2;
obs(N(m3, vx), o3);
t4 〜N (zero, hund); m4 := if (t4 > zero) gi else g2;
obs(N(m4, vx), o4);
t5 〜N (zero, hund); m5 := if (t5 > zero) gi else g2;
obs(N(m5, vx), o5 )
For each program of the class, our generator generated the parameter values as follows:
θi 〜U(-15, 15), θ2 〜U(0.5, 50), θ2 = (θ2)2
θ3 〜U(-15,15), θ4 〜U(0.5, 50), θ4 = (θ4)2
θ5 〜U(0.5,10), θ5 = (θ5)2
and then generated the observations oi:5 by running the program forward.
E.1.5 milky AND milkyo
The model class milky is described as follows:
mmass := θi; vmass := θ2; ci := θ3; vg1 := θ4; c2 := θ5;
vg2 := θ6; vx1 := θ7; vx2 := θ8;
mass 〜N(mmass ,Vmass );
massi := mass X ci; gi 〜N(massι,Vgι);
mass2 := mass + c2; g2 〜N(mass2,Vg2);
19
Under review as a conference paper at ICLR 2022
obs(N(g1,vx1),o1); obs(N(g2,vx2),o2)
For each program of milky, our generator generated the parameter values as follows:
θι ~ U( —10,10), θ ~ U(0, 30), θ2 = (θ2)2, θ3 ~ U(—2, 2)
θ4 ~ u(o, 10), θ4 = (θ4)2, θ5 ~ U(—5,5), θ6 ~ u(0, 10)
θ6 = (θ6)2, θ7 ~ U(0.5,10), θ7 = (θ7)2, θ8 ~ U(0.5,10)
θ80 = (θ8)2
and then generated the observations o1 and o2 by running the program forward.
Everything remained the same for the milkyo class, except that the two obs commands were extended
to obs(N(g1, vx1), [o1, o2, o3, o4, o5]) and obs(N(g2,vx2), [o6, o7, o8, o9, o10]), respectively, and
all the observations were generated similarly by running the extended model forward.
E.1.6 rb
The model class rb is described as follows:
mz1 := θ1 ; vz1 := θ2 ; mz2 := θ3; vz2 := θ4; vx := θ5;
z1 ~ N(mz1,vz1); z2 ~ N(mz2,vz2); r := Rosenbrock(z1, z2);
obs(N (r, vx), o)
where Rosenbrock(z1, z2) = 0.05 × (z1 — 1)2 + 0.005 × (z2 — z12)2. For each program of the
class, our generator generated the parameter values as follows:
θ1 ~ U(—8, 8), θ2 ~ U(0, 5), θ20 = (θ2)2, θ3 ~ U(—8, 8)
θ4 ~ U(0, 5), θ40 = (θ4)2, θ5 ~ U(0.5, 10), θ50 = (θ5)2
and then generated the observation o by running the program forward where the values for z1 and z2
in this specific simulation were sampled as follows:
Z1 ~ U(mzι	—	1.5	X	√VZ1,	mzι	+	1.5	X	√VZ1)
z2 ~ U(mz2	—	1.5	X	√VZ2,	mz2	+	1.5	X	√‰)
E.2 Generalisation to new model structures
This section details the model classes, and different types in each model class in §5.2. For readability,
we present canonicalised dependency graphs where variables are named in the breadth-first order. In
the experiments reported in this section, we used a minor extension of our probabilistic programming
language with procedures taking one parameter.
E.2. 1 ext1
Fig. 10	shows the dependency graphs for all model types in ext1. The variables z0, z1, . . . and
x1, x2, . . . represent latent and observed variables, respectively, and observed variables are colored in
gray. The red node in each graph represents the position of the nl variable.
Our program generator in this case generates programs from the whole model class ext1; it generates
programs of all twelve different types in ext1. We explain this generation process for the model type
(1,1) in Fig. 10, while pointing out that the similar process is applied to the other eleven types. To
generate programs of the model type (1,1), we use the following program template:
mz0 := θ1 ; vz0 := θ2 ; vz2 := θ3 ; vz3 := θ4 ; vx1 := θ5 ;
z0 ~ N(mz0,vz0); z1 := nl(z0); z2 ~ N(z1,vz2); z3 ~ N(z2,vz3);
obs(N(z3,vx1), o1)
where nl(z) = 50∕π X arctan(z∕10). The generation involves randomly sampling the parameters
of this template, converting the template into a program in our language, and creating synthetic
observations. Specifically, our generator generates the parameter values as follows:
θ1 ~ U(—5, 5), θ2 ~ U(0, 20), θ20 = (θ2)2, θ3 ~ U(0, 20), θ30 = (θ3)2
20
Under review as a conference paper at ICLR 2022
θ4 〜U(0, 20), θ4 = (θ4)2, θ5 〜U(0.5,10), θ5 = (θ5)2
and generates the observation o1 by running the program forward where the values for z0:3 in this
specific simulation were sampled (and fixed to specific values) as follows:
Z0 ~ U(mzo - 2 X √VZ0, mzo + 2 X √VZ0)
z1 = nl(z0)
Z ~ U(zι - 2 X √VZ2, zι + 2 X E)
Z3 ~ U(Z2 - 2 X √VZ3, Z2 + 2 X √VZ3).
The generator uses different templates for the other eleven model types in ext1, while sharing the
similar process for generation of the parameters and observations.
E.2.2 ext2
Fig. 11	shows the dependency graphs for all five model types in ext2. Programs of these five types
are randomly generated by our program generator. As in the ext1 case, we explain the generator
only for one model type, which corresponds to the first dependency graph in Fig. 11. To generate
programs of this type, we use the following program template:
mz0 := θ1 ; vz0 := θ2 ; vz1 := θ3; vz3 := θ4; vz4 := θ5 ; vz5 := θ6; vz6 := θ7;
vx1 := θ8 ; vx2 := θ9 ; vx3 := θ10; vx4 := θ11 ;
z0 ~ N(mz0,vz0); z1 ~ N(z0,vz1); z2 := nl(z0); z3 ~ N(z0,vz3);
z4 ~ N(z1,vz4); z5 ~ N(z1,vz5); z6 ~ N(z2,vz6);
obs(N(z4,vx1), o1); obs(N(z5,vx2), o2); obs(N(z6,vx3), o3); obs(N(z3,vx4), o4)
In order to generate a program of this model type and observations, our generator instantiates the
parameters of the template as follows:
θ1 ~ U(-5, 5), θ2 ~ U(0, 10), θ20 = (θ2)2, θ3 ~ U(0, 10), θ30 = (θ3)2, θ4 ~ U(0, 10), θ40 = (θ4)2
θ5 ~ U(0, 10),	θ50	= (θ5)2,	θ6 ~ U(0, 10),	θ60 =	(θ6)2, θ7 ~ U(0, 10), θ70 = (θ7)2
θ8 ~ U(0, 10),	θ80	= (θ8)2,	θ9 ~ U(0, 10),	θ90 =	(θ9)2, θ10 ~ U(0, 10), θ100 = (θ10)2
θ11 ~ U(0, 10), θ101 = (θ11)2.
Then, it generates the observations o1:4 by running the program forward where the values for z0:6 in
this specific simulation were sampled (and fixed to specific values) as follows:
Z0 ~ U(mzo - 2 X √VZ0, mzo + 2 X √VZ0)
Z1 ~ U(Z0 - 2 X √vz1, Z0 + 2 X √vz1)
z2 = nl(z0)
z3 ~	U(z0	- 2	X	√Vz3,	zo +	2	X	√Vz3)
z4 ~	U(zι	- 2	X	√Vz4,	zι +	2	X	√Vz4)
Z5 ~	U(zι	- 2	X	√vz5,	zι +	2	X	√vz5)
Z6 ~	U(Z2	- 2	X	√vz6,	Z2 +	2	X	√vz6).
The generator uses different templates for the other four model types in ext2, while sharing the similar
process for generation of the parameters and observations.
E.3 Test-time efficiency in comparison with alternatives
This section details the mulmod class in §5.3, which has three different model types. Fig. 12 shows
the dependency graphs for all the model types. The red node in each graph represents the position of
the mm variable. We used all the three types in training, applied the learnt inference algorithm to
programs in the third model type, and compared the results with those returned by HMC.
We similarly explain the generator only using the model type corresponding to the first dependency
graph in Fig. 12. To generate programs of this type, we use the following program template:
mz0 := θ1 ; vz0 := θ2 ; vz1 := θ3 ; vx1 := θ4 ;
21
Under review as a conference paper at ICLR 2022
Z0 〜N(mzo,Vzo); Zi 〜N(zo,Vzι); Z2 ：= mm(zι); obs(N(z2,VχJ,01)
where mm(x) = 100 × x3/(10+x4). For each program in this model type, our generator instantiates
the parameter values as follows:
θi 〜U(-5, 5), θ2 〜U(0, 20), θ2 = (θ2)2, θ3 〜U(0, 20), θ = (θ3)2
θ4 〜U(0.5,10), θ4 = (θ4)2
and synthesises the observation o1 by running the program forward where the values for z0:2 in this
specific simulation were sampled (and fixed to specific values) as follows:
Zo 〜U(mzo - 2 X √VZ0, mzo + 2 X √VZ0)
Zi 〜U(zo - 2 X E, Zo + 2 X K)
Z2 = mm(Z1 ).
The generator uses different templates for the other two model types in mulmod, while sharing the
similar process for instantiation of the parameters and observations.
F Detailed evaluation setup
In our evaluation, the dimension s of the internal state h was 10 (i.e., h ∈ Rio). We used the same
neural network architecture for all the neural network components of our inference algorithm infer.
Each neural network had three linear layers and used the tanh activation. The hidden dimension was
10 for each layer in all the neural networks except for nnde where the hidden dimensions were 50.
The hyper-parameter in our optimisation objective (§4) was set to λ = 2 in the evaluation. For HMC,
we used the NUTS sampler (Hoffman & Gelman, 2014). We did not use GPUs.
Before running our inference algorithm, we canonicalise the names of variables in a given program
based on its dependency (i.e., data-flow) graph. Although not perfect, this preprocessing removes a
superficial difference between programs caused by different variable names, and enables us to avoid
unnecessary complexity caused by variable-renaming symmetries at training and inference times.
G	LOSSES FOR hierd, cluster, milkyo, AND rb
Fig. 13	shows the average training and test losses under three random seeds for hierd, cluster, milkyo,
and rb. The later part of Fig. 13a, 13c and 13d shows cases where the test loss surges. This was when
the loss of only a few programs in the test set (of 50 programs) became large. Even in this situation,
the losses of the rest remained small. We give analyses for cluster and rb separately in §H.
H MULTIMODAL POSTERIORS: cluster AND rb
The cluster and rb classes in §5.1 posed another challenge: the models often had multimodal
posteriors, and it was significantly harder for our meta-algorithm to learn an optimal inference
algorithm. To make the evaluation partially feasible for rb, we changed two parts of our meta-
algorithm slightly, as well as increasing the size of the test set from 50 to 100. First, we used
importance samples instead of samples by HMC, which often failed to converge, to learn an inference
algorithm. Second, our random program generator placed some restriction on the programs it
generated (e.g., by using tight boundaries on some model parameters), guided by the analysis of the
geometry of the Rosenbrock function (Pagani et al., 2019). Consequently, HMC (with 500K samples
after 50K warmups) failed to converge for only one fifth of the test programs.
Fig. 14	shows the similar comparison plots between reference and predicted marginal posteriors for
10 test programs of the rb type, after 52.4K epochs. Our inference algorithm computed the posteriors
precisely for most of the programs except two (pgm75 and pgm79) with significant multimodality.
The latent variable pgm75_z0 had at least two modes at around -10 (visible in the figure) and around
10 (hidden in the figure)6 *. Our inference algorithm showed a mode-seeking behavior for this latent
6The blue reference plots were drawn using an HMC chain, but the HMC chain got stuck in the mode around
-10 for this variable.
22
Under review as a conference paper at ICLR 2022
variable. Similarly, the variable pgm79_z0 had at least two modes in the similar domain region (one
shown and one hidden), but this time our inference algorithm showed a mode-covering behavior.
The multimodality issue raises two questions. First, how can our meta-algorithm generate samples
from the posterior more effectively so that it can optimise the inference algorithm for classes of
models with multimodal posteriors? For example, our current results for cluster suffer from the fact
that the samples used in the training are often biased (i.e., only from a single mode of the posterior).
One possible direction would be to use multiple Markov chains simultaneously and apply ideas
from the mixing-time research. Second, how can our white-box inference algorithm catch more
information from the program description and find non-trivial properties that may be useful for
computing the posterior distributions having multiple modes? We leave the answers for future work.
I	TRAINING AND TEST LOSSES FOR THE OTHER CASES IN ext1
Fig. 15	shows the average training and test losses in the ext1 experiment runs (under three different
random seeds) for generalisation to the last (4th) dependency graph and to all three positions of the
nl variable.
J TRAINING AND TEST LOSSES FOR THE OTHER CASES IN ext2
Fig. 16	shows the average training and test losses in the ext2 experiment runs (under three different
random seeds) for generalisation to the 4th and 5th dependency graphs.
K Quantified accuracy of predicted posteriors
For accuracy, it would be ideal to report KL[p||q], where p is the fully joint target posterior and q is
the predicted distribution. It is, however, hard to compute this quantity since often we cannot compute
the density ofp. One (less convincing) alternative is to compute KL[p0(z)||q(z)] for a latent variable
z where p0(z) is the best Gaussian approximation (i.e., the best approximation using the mean and
standard deviation) for the true marginal posterior p(z), and average the results over all the latent
variables of interest. We computed KL[p0(z)||q(z)] for the test programs from ext1 and ext2 in §5.2,
and for the three from mulmod that are reported in §5.3.
For ext1 and ext2, we measured the average KL[p0(z)||q(z)] over all the latent variables z in the test
programs. For instance, if there were 90 test programs and each program had three latent variables, we
averaged 90 × 3 = 270 KL measurements. In an experiment run for ext1 (which tested generalisation
to an unseen dependency graph), the average KL was around 1.32. In an experiment run for ext2, the
estimation was around 0.95. When we replaced q with a normal distribution that is highly flat (with
mean 0 and standard deviation 10K), the estimation was 7.11 and 7.57, respectively. The results were
similar in all the other experiment runs that were reported in §5.2.
For the three programs from mulmod in §5.3, p0 was the best Gaussian approximation whose mean
and standard deviation were estimated by the reference importance sampler (IS-ref), and q was either
the predicted marginal posterior by the learnt inference algorithm or the best Gaussian approximation
whose mean and standard deviation were estimated by HMC. The average KL was around 1.19
when q was the predicted posterior, while the estimation was 40.9 when q was the best Gaussian
approximation by HMC. The results demonstrate that the predicted posteriors were more accurate on
average than HMC at least in terms of p0 .
L Program in §5.3
Fig. 17	shows the program that is reported in §5.3, written in our probabilistic programming language.
M Discussion of the cost of IS-pred vs. IS -prior
Our approach (IS-pred) must scan the given program “twice” at test time, once for computing the
proposal using the learnt neural networks and another for running the importance sampler with the
23
Under review as a conference paper at ICLR 2022
predicted proposal, while IS-prior only needs to scan the program once. Although it may seem that
IS-prior has a huge advantage in terms of saving the wall-clock time, our observation is that the
effect easily disappears as the sample size increases. In fact, going through the neural networks in
our approach (i.e., the first scanning of the program) does not depend on the sample size, and so its
time cost remains constant given the program; the time cost was 0.6ms for the reported test program
(pgm19) in §5.3.
N	Limitations and future work
Currently, a learnt inference algorithm in our work does not generalise to programs with different
sizes (Yan et al., 2020), e.g., from clustering models with two clusters to those with ten clusters.
Each model class assumes a fixed number of variables, and the neural networks crucially exploit the
assumption. Also, our meta-algorithm does not scale in practice. When applied to large programs,
e.g., state-space models with a few hundred time steps, it cannot learn an optimal inference algorithm
within a reasonable amount of time. Overcoming these limitations is a future work. Another direction
that we are considering is to remove the strong independence assumption (via mean field Gaussian)
on the approximating distribution in our inference algorithm, and to equip the algorithm with the
capability of generating an appropriate form of the approximation distribution with rich dependency
structure, by, e.g., incorporating the ideas from Ambrogioni et al. (2021). This direction is closely
related to automatic guide generation in Pyro (Bingham et al., 2018).
24
Under review as a conference paper at ICLR 2022
(d) Model type (1,4).
(e) Model type (2,1).
(h) Model type (2,4).
(i) Model type (3,1).
(l) Model type (3,4).
Figure 10:	Canonicalised dependency graphs for all 12 model types in ext1. The rows are for different
positions of the nl variable, and the columns are for different dependency graphs: the model type
(i,j) means one of the 12 model types in ext1 that corresponds to the i-th position of the nl variable
and j -th dependency graph.
25
Under review as a conference paper at ICLR 2022
(a) Model type for the 1st dependency graph.
(b) Model type for the 2nd dependency graph.
(c)	Model type for the 3rd dependency graph.
(d)	Model type for the 4th dependency graph.
(e) Model type for the 5th dependency graph.
Figure 11:	Canonicalised dependency graphs for all five model types in ext2.
(b)	2nd model type.
(c)	3rd model type.
Figure 12:	Canonicalised dependency graphs for all three model types in the mulmod class.
26
Under review as a conference paper at ICLR 2022
(a) hierd
Seed, Tr or-R;
一l,Tr
---1, Te
---2, Tr
^-2, Te
-3, Tr
---3, Te

(b) cluster
(c) milkyo
Figure 13: Losses for hierd, cluster, milkyo, and rb. The g-axes are log-scaled. The surges in later
epochs of Fig. 13a, 13c and 13d were due to only a single or a few test programs out of 50.
Seed, Tr or Te

(d) rb
Figure 14: Comparisons of reference and predicted marginal posteriors for 10 programs in the rb test
set.
27
Under review as a conference paper at ICLR 2022
Epoch
IB
IOOM
IOM
IM
IOOk
IOk
1000
100
10
Seed, Tr or Te
—LTr
---1, Te
一2, Tr
^-2, Te
3, Tr
^-3, Te
1000
4000
5000
(b) To 1st nl position.
(a) To 4th dep. graph.
IOk
1000
(c) To 2nd nl position.
Seed, Tr or Te
—LTr
1, Te
一2, Tr
--2, Te
3, Tr
--3, Te

耀
ioɪz
ioɪf
IO15
IOOT
IOT
IT
IOOB
IOB
IB
IOOM
IOM
IM
IOOk
IOk
1000
100
10
1
Seed, Tr or Te
1, Tr-----1, Te
3, Tr-----3, Te
(d) To 3rd nl position.
Figure 15: Average training and test losses for generalisation to the last (4th) dependency graph and
to all three positions of the nl variable in ext1. The y-axes are log-scaled.
IOM
IM
IOOk
IOk
1000
100
10
1000	2000	3000	4000	5000
Epoch
Epoch
(b) To 5th dep. graph.
(a) To 4th dep. graph.
Figure 16: Average training and test losses for generalisation to the 4th and 5th dependency graphs in
ext2. The y-axes are log-scaled.
a := 3.93; b := 348.16; c := 57.5; d := 14.04; e := 40.34;
Zι 〜N (a, b); Z2 〜N(Zi, c); Z3 ：= mm(zι);
obs(N(z2, d), 53.97); obs(N(z3, e), 0.12)
Figure 17: The program that is reported in §5.3, written in our probabilistic programming language.
28