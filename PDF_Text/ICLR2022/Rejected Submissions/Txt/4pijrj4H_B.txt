Under review as a conference paper at ICLR 2022
Fair Node Representation Learning
via Adaptive Data Augmentation
Anonymous authors
Paper under double-blind review
Ab stract
Node representation learning has demonstrated its efficacy for various applica-
tions on graphs, which leads to increasing attention towards the area. However,
fairness is a largely under-explored territory within the field, which may lead to
biased results towards underrepresented groups in ensuing tasks. To this end, this
work theoretically explains the sources of bias in node representations obtained
via Graph Neural Networks (GNNs). Our analysis reveals that both nodal fea-
tures and graph structure lead to bias in the obtained representations. Building
upon the analysis, fairness-aware data augmentation frameworks on nodal fea-
tures and graph structure are developed to reduce the intrinsic bias. Our analysis
and proposed schemes can be readily employed to enhance the fairness of various
GNN-based learning mechanisms. Extensive experiments on node classification
and link prediction are carried out over real networks in the context of graph con-
trastive learning. Comparison with multiple benchmarks demonstrates that the
proposed augmentation strategies can improve fairness in terms of statistical par-
ity and equal opportunity, while providing comparable utility to state-of-the-art
contrastive methods.
1	Introduction
Graphs are widely used in modeling and analyzing complex systems such as biological networks or
financial markets, which leads to a rise in attention towards various machine learning (ML) tasks
over graphs. Specifically, node representation learning is a field with growing popularity. Node
representations are mappings from nodes to vector embeddings containing both structural and at-
tributive information. Their applicability on ensuing tasks has enabled various applications such as
traffic forecasting (Opolka et al., 2019), and crime forecasting (Jin et al., 2020). Graph neural net-
works (GNNs) have been prevalently used for representation learning, where node embeddings are
created by repeatedly aggregating information from neighbors for both supervised and unsupervised
learning tasks (KiPf & Welling, 2017; VelickoVic et al., 2018; Garcia-Duran & Niepert, 2017).
It has been shown that ML models propagate pre-existing bias in training data, which may lead to
discriminatiVe results for ensuing applications (Dwork et al., 2012; Beutel et al., 2017). Particular
to ML oVer graphs, while GNN-based methods achieVe state-of-the-art results for graph represen-
tation learning, they also amplify already existing biases in training data (Dai & Wang, 2020). For
example, nodes in social networks tend to connect to other nodes with similar attributes, leading
to denser connectiVity between nodes with same sensitiVe attributes (e.g., gender) (Hofstra et al.,
2017). Thus, by aggregating information from the neighbors, the representations obtained by GNNs
may be highly correlated with the sensitiVe attributes. This causes discrimination in ensuing tasks
eVen when the sensitiVe attributes are not directly used in training (Hajian & Domingo-Ferrer, 2013).
Data augmentation has been widely utilized to improVe generalizability in trained models, as well
as enable learning in unsuperVised methods such as contrastiVe or self-superVised learning. Aug-
mentation schemes haVe been extensiVely studied in Vision (Shorten & Khoshgoftaar, 2019; Hjelm
et al., 2018) and natural language processing (Zhang et al., 2015; Kafle et al., 2017). HoweVer,
there are comparatiVely limited work in the graph domain due to the complex, non-Euclidean struc-
ture of graphs. To the best of our knowledge, (Agarwal et al., 2021) is the only study that designs
fairness-aware graph data augmentation in the contrastiVe learning framework to reduce bias.
1
Under review as a conference paper at ICLR 2022
This study theoretically investigates the sources of bias in GNN-based learning and in turn improves
fairness in node representations by employing fairness-aware graph data augmentation schemes.
Proposed schemes corrupt both input graph topology and nodal features adaptively, in order to re-
duce the corresponding terms in the analysis that lead to bias. Although the proposed schemes are
presented over their applications using contrastive learning, the introduced augmentation strategies
can be flexibly utilized in several GNN-based learning approaches together with other fairness-
enhancement methods. Our contributions in this paper can be summarized as follows:
c1) We theoretically analyze the sources of bias that is propagated towards node representations in
a GNN-based learning framework.
c2) Based on the analysis, we develop novel fairness-aware graph data augmentations that can re-
duce potential bias in learning node representations. Our approach is adaptive to both input graph
and sensitive attributes, and to the best of our knowledge, is the first study that tackles fairness en-
hancement through adaptive graph augmentation design.
c3) The proposed strategies incur low additional computation complexity compared to non-adaptive
counterparts, and are compatible to operate in conjunction with various GNN-based learning frame-
works, including other fairness enhancement methods.
c4) Theoretical analysis is provided to corroborate the effectiveness of the proposed feature masking
and node sampling augmentation schemes.
c5) Performance of the proposed graph data augmentation schemes is evaluated on real networks
for both node classification and link prediction tasks. It is shown that compared to state-of-the-art
graph contrastive learning methods, the novel augmentation schemes improve fairness metrics while
providing comparable utility measures.
2	Related Work
Representation learning on graphs. Conventional graph representation learning approaches can
be summarized under two categories: factorization-based and random walk-based approaches.
Factorization-based methods aim to minimize the difference between the inner product of node
representations and a deterministic similarity metric between them (Ahmed et al., 2013; Cao et al.,
2015; Ou et al., 2016). Random walk-based approaches, on the other hand, employ stochastic mea-
sures of similarity between nodes (Perozzi et al., 2014; Grover & Leskovec, 2016; Tang et al., 2015;
Chen et al., 2018). GNNs have gained popularity in representation learning, for both supervised
(KiPf & Welling, 2017; VelickoVic et al., 2018; HU et al., 2019; Wu et al., 2019), and unsupervised
tasks, e.g., (Garcia-Duran & NiePerL 2017; Hamilton et al., 2017). Specifically, recent success of
contrastive learning on visual representation learning (Wu et al., 2018; Ye et al., 2019; Ji et al., 2019)
has paved the way for contrastive learning for unsupervised graph representation learning.
Graph data augmentation. Augmentation strategies have been extensively investigated in vision
(Shorten & Khoshgoftaar, 2019; Hjelm et al., 2018) and natural language processing (Zhang et al.,
2015; Kafle et al., 2017) domains. However, the area is comparatively under-explored in the graph
domain due to the complex, non-Euclidean topology of graphs. Graph augmentation based on graph
structure modification has been developed to improve the utility of ensuing tasks (Rong et al., 2019;
Zhao et al., 2020; Chen et al., 2020a). Meanwhile, graph data augmentation has been used to gen-
erate graph views for unsupervised graph contrastive learning, see, e.g., (Velickovic et al., 2019;
Opolka et al., 2019; Zhu et al., 2020; 2021), which achieves state-of-the-art results in various learn-
ing tasks over graphs such as node classification, regression, and link prediction (Opolka et al., 2019;
Velickovic et al., 2019; You et al., 2020; ZhU et al., 2020; Peng et al., 2020; Hassani & Khasahmadi,
2020). Among which, (Zhu et al., 2020) is the first study that aims to maximize the agreement of
node-level embeddings across two corrupted graph views. Building upon (Zhu et al., 2020), (Zhu
et al., 2021) develops adaptive augmentation schemes with respect to various node centrality mea-
sures and achieves better results. However, none of these studies are fairness-aware.
Fairness-aware learning on graphs. A pioneering study tackling the fairness problem in graph
representation learning based on random walks is developed in (Rahman et al., 2019). In addi-
tion, adversarial regularization is employed to account for fairness of node representations (Dai &
Wang, 2020; Bose & Hamilton, 2019; Fisher et al., 2020) where (Dai & Wang, 2020) is presented
specifically for node classification, and (Fisher et al., 2020) works on knowledge graphs. (Buyl &
De Bie, 2020) also aims to create fair node representations by utilizing a Bayesian approach where
sensitive information is modeled in the prior distribution. Contrary to these aforementioned works,
2
Under review as a conference paper at ICLR 2022
our framework is built on a theoretical analysis (developed within this paper). Similar to the works
mentioned above, the proposed methods can be utilized within the learning process to mitigate bias
by modifying the learned model (i.e., in-processing fairness strategy). In addition, the proposed
schemes can also be regarded as “pre-processing” tools, implying their compatibility to a wide array
of GNN-based learning schemes in a versatile manner. Furthermore, (Ma et al., 2021) carries out a
PAC-Bayesian analysis and connects the concept of subgroup generalization to accuracy disparity,
and (Zeng et al., 2021) introduces several methods including GNN-based ones to decrease the bias
for the representations of heterogeneous information networks. While (Li et al., 2021; Laclau et al.,
2021) modify adjacency to improve different fairness measures specifically for link prediction, (Buyl
& De Bie, 2021) designs a regularizer for the same purpose. With a specific interest on individual
fairness over graphs, (Dong et al., 2021) employs a ranking-based strategy. A biased edge dropout
scheme is proposed in (Spinelli et al., 2021) to improve fairness. However, the scheme therein is not
adaptive to the graph structure (the parameters of the framework are independent of the input graph
topology). Fairness-aware graph contrastive learning is first studied in (Agarwal et al., 2021), where
a layer-wise weight normalization scheme along with graph augmentations is introduced. However,
the fairness-aware augmentation utilized therein is designed primarily for counterfactual fairness.
3	Fairnes s in GNN-based Representation Learning
GNN-based approaches are the state-of-the-art for node representation learning. However, it has
been demonstrated that the utilization of graph structure in the learning process not just propagates
but also amplifies a possible bias towards certain sensitive groups (Dai & Wang, 2020). To this
end, this section investigates the sources of bias in the generated representations via GNN-based
learning. It carries out an analysis revealing that both nodal features and graph structure lead to bias,
for which several graph data augmentation frameworks are introduced.
3.1	Preliminaries
This study aims to learn fairness-aware nodal representations for a given graph G := (V, E) where
V := {v1,v1,…,VN} denotes the node set, and E ⊆ V × V represents the edge set. X ∈ RN×F
and A ∈ {0, 1}N×N are used to denote the feature and adjacency matrices, respectively, with
the (i, j)-th entry aij = 1 if and only if eij := (vi, vj) ∈ E. Degree matrix D ∈ RN ×N is
defined to be a diagonal matrix with the nth diagonal entry dn denoting the degree of vn . For the
fairness examination, sensitive attributes of the nodes are represented with s ∈ {0, 1}N×1, where
the existence of a single, binary sensitive attribute is considered. In this work, unsupervised methods
are chosen as enabling schemes for the representation generation where given the inputs A, X, and
s, the main purpose is to learn a mapping f : RN ×N × RN ×F × RN ×1 → RN ×FL that generates
FL dimensional (generally FL F) unbiased nodal representations HL = f(A, X, s) ∈ RN×FL
through an L-layer GNN, which can be used in an ensuing task such as node classification. xi ∈ RF,
hli ∈ RFl , and si ∈ {0, 1} denote the feature vector, representation at layer l and the sensitive
attribute of node vi . Furthermore, S0 and S1 denote the set of nodes whose sensitive attributes are
0 and 1, respectively. Define inter-edge set Eχ := {eij |vi ∈ Sa, vj ∈ Sb, a 6= b}, while intra-edge
set is defined as Eω := {eij|vi ∈ Sa, vj ∈ Sb, a = b} . Similarly, the set of nodes having at least
one inter edge is denoted by Sχ , while Sω defines the set of nodes that have no inter-edges. The
intersection of the sets S0 , Sχ is denoted as S0χ . Additionally, diχ and diω denote the numbers of
inter-edges and intra-edges adjacent to vi, respectively. Finally, || denotes the entry-wise absolute
value for scalar or vector inputs, while it is used for the cardinality when the input is a set.
3.2	Analysis for Bias in GNN Representations
This subsection presents an analysis to find out the sources of bias in node representations generated
by GNNs. Analysis is developed for the mean aggregation scheme in which aggregated representa-
tions at layer l, Zl ∈ RN×Fl, are generated such that Zi = 今 Pv).6ν⑴ hj-1 for i = {1,…,N},
where zli is the ith row ofZl corresponding to node vi, di denotes the degree of node vi, N(i) refers
to the neighbor set of node vi (including itself). The recursive relation in a GNN layer in which left
normalization is applied for feature smoothing is Hl = σ(D-1(A + IN)Hl-1Wl) where Wl is
the weight matrix in layer l, and IN ∈ RN ×N denotes an identity matrix. With these definitions, the
3
Under review as a conference paper at ICLR 2022
relation between the aggregated information Zl and node representations Hl becomes equivalent
to Hl = σ(ZlWl) at the lth GNN layer. As the provided analysis is applicable to every layer, l
superscript is dropped in the following to keep the notation simple.
It has been demonstrated that features that are correlated with the sensitive attribute result in bias
even when the sensitive attribute is not utilized in the learning process (Hajian & Domingo-Ferrer,
2013). This work provides an analysis on the correlation of s with Z, and aims to reduce it. Note
that, the reduction of correlation can still allow the generation of discriminable representations for
different class labels, if the discriminability is provided by non-sensitive attributes. The (sample)
correlation between the sensitive attributes s and aggregated representations can be written as
Pi = Corr(Z:,i, s)
Pj=1(zj - N PN=I Zki)( Sj - N PN=I Sk)
where Z：,i, i = 1 ∙∙∙ F is the ith column of Z. In the analysis, following assumptions are made:
A1: Node representations have sample means μο and μι respectively across each group, where
μi = mean(hj | Vj ∈ Si). Throughout the paper, mean(∙) denotes the sample mean operation.
A2: Node representations have finite maximal deviations ∆° and ∆ι: That is, ∣∣hj - μik∞ ≤ ∆i,
∀vj ∈ Si with i ∈ {0, 1}.
Based on these assumptions, the following theorem shows that kρk1 can be bounded from above,
which will serve as a guideline to design a fairness-aware graph data augmentation scheme.
Theorem 1. The total correlation between the sensitive attributes s and representations Z that are
obtained after a mean aggregation over graph G, kρk1, can be bounded above by
kρk1 ≤ kck1(kδk1max(γ1,γ2) +2N∆)
(1)
where Ci ：= VNSzIS1|, With σe :=，N Pn=I (θn - N PN=1 θi), ∀θ ∈ Rn, δ := μο - μι,
Y1 := I1 -图-⅛l∣, Y2 = I1 - 2min (mean( dm+⅛m |vm ∈ S0), mean( d⅛ |vn ∈ &))|，
∆ = max(∆0, ∆1).
The proof is given in Appendix A.1. The upper bound in equation 1 can be lowered by i) utilizing
feature masking which has an effect on the term ∣∣μο - μι k 1 atthe first layer, ii) node sampling that
can change the value of γ1, iii) edge augmentations that can reduce the value of γ2 .
3.3	Fair Graph Data Augmentations
Data augmentation has been studied extensively in order to enable certain unsupervised learning
schemes such as contrastive learning, self-supervised learning or as a general framework to improve
the generalizability of the trained models over unseen data. However, the design of graph data
augmentations is still a developing research area due to the challenges introduced by complex, non-
Euclidean graph structure. Several augmentation schemes over the graph structure are proposed in
order to enhance the generalizability of GNNs (Rong et al., 2019; Zhao et al., 2020), while both
topological (e.g., edge/node deletion) and attributive (e.g., feature shuffling/masking) corruption
schemes have been developed in the context of contrastive learning (Velickovic et al., 2019; You
et al., 2020; Zhu et al., 2021; 2020). However, none of these works are fairness-aware. Hence, in
this work, novel data augmentation schemes that are adaptive to the sensitive attributes, as well as
the input graph structure are introduced with Theorem 1 as guidelines.
3.3.1	Feature Masking
In this subsection, an augmentation framework on nodal features H0 = X is presented in order
to mitigate possible intrinsic bias propagated by them. Note that ∣δ∣1 in equation 1 is minimized
when all nodal features are the same (i.e., all nodal features masked/zeroed out). However, this
would result in the loss of all information in nodal features. Motivated by this, the proposed scheme
aims to improve uniform feature masking in terms of reducing ∣δ ∣1 for a given masking budget
(a total amount of nodal features to be masked in expectation). Specifically, the random feature
masking scheme used in (You et al., 2020; Zhu et al., 2020) where each feature has the same masking
probability is modified to assign higher masking probabilities to the features varying more across
4
Under review as a conference paper at ICLR 2022
different sensitive groups. Thus, masking probabilities are generated based on the term |δ |. Let δ :=
maX(∣-lm-min") denote the normalized ∣δ∣, the feature masking probability can then be designed as
p(m)
(2)
where α is a hyperparameter. The feature mask m ∈ {0, 1}F is then generated as a random binary
vector, with the i-th entry of m drawn independently from Bernoulli distribution with pi = (1 -
pi(m) ) for i = 1, . . . , F . The augmented feature matrix is obtained via
XX = [m ◦ Xi；...; m ◦ XN]>,
(3)
where [∙; ∙] is the concatenation operator, and ◦ is the Hadamard product. Since the proposed feature
masking scheme is probabilistic in nature, the resulting δ is a random vector with entry δi having
I δ ∙ | = [ I δi I, with probability Pi,
i 0, with probability 1 - pi
(4)
where pi := 1 - pi(m) is the probability that the ith feature is not masked in the graph view. The
following proposition shows that the novel, adaptive feature masking approach can decrease kρk1
compared to random feature masking, the proof of which can be found in Appendix A.2.
Proposition 1. In expectation, the proposed adaptive feature masking scheme results in a lower
kδ k1 value compared to uniform feature masking, meaning
Ep[kδki] ≤ Eq[kδki]	⑸
where q corresponds to uniform masking with masking probability 1 - qi, and qi = F PF=IPj.
3.3.2	Node Sampling
In this subsection, an adaptive node sampling framework is introduced to decrease the term γ1 :=
11 - (^ρθ[ + -ySτp )∣ = 11 - (ISXS)∣Sω∣ + "Sω∣) I in equation 1 of Theorem 1, and hence to reduce
the intrinsic bias that the graph topology can create. A small γ1 suggests a more balanced population
distribution with respect to ISχI and ISωI. Specifically, a subset of nodes is selected at every epoch
and the training is carried over the subgraph induced by the sampled nodes. This augmentation
mainly aims at reducing the bias by selecting a subset of more balanced groups, meanwhile it also
helps reduce the computational and memory complexity in training.
The proposed node sampling is adaptive to the input graph, that is, it depends on the cardinalities of
the sets S0χ, S1χ, S0ω, and S1ω. The developed scheme copes with the case ISχI ≤ ISωI. In algorithm
design, it is assumed that if ISχI ≤ ISωI then IS0χI ≤ IS0ωI and IS1χI ≤ IS1ωI (same for Sω), which
holds for all real graphs in our experiments, but our design principles can be readily extended to
different settings as well.
Given input graph G , the augmented graph G can be obtained as an induced subgraph from a subset
of nodes V. All nodes in SX are retained, (SX = SX and SX = SX), while subsets of nodes S>0
and Sf are randomly sampled from S0 and Sf with sample sizes ∣SωI = |SXI and |Sf ∣ = ∣SX∣,
respectively . See also Algorithm 1 in Appendix A.3. The cardinalities of node sets in the resulting
graph augmentation G satisfy γ1 = 0. (See Appendix A.3 for details.)
X
Remark 1. Note that the resulting graph G yields γ1 = 0 as long as IS0XI/IS0I = 1/2 + φ and
X
IS1XI/IS1I = 1/2 - φ is satisfied for any -1/2 < φ < 1/2. The presented scheme here simply sets
φ = 0, which results in a balanced ratio across groups, but the performance can be improved if φ is
selected carefully for specific datasets.
3.3.3	Augmentation on Graph Connectivity
Minimizing γ2 to zero in Theorem 1 suggests a graph topology where all nodes in the network have
the same number of neighbors from each sensitive group, i.e., dif = diX , ∀vi ∈ V. Since, for this
5
Under review as a conference paper at ICLR 2022
scenario, γ = ∣1 - 2min (mean( dχdmω 加m ∈ §0)，mean( d⅛ω 忖4 ∈ Sι))l = 0. Note that this
finding is parallel to the main design idea of Fairwalk (Rahman et al., 2019) in which the transition
probabilities are equalized for different sensitive groups in random walks in order to reduce bias in
random walk-based representations.
This finding suggests that an ideal augmented graph G could be generated by deleting edges from
or adding edges to G such that each node has exactly the same number of neighbors from each
sensitive group. However, such a per-node sampling scheme is computationally complex and may
not be ideal for large-scale graphs. To this end, we propose global probabilistic edge augmentation
schemes such that in the augmented graph,
_ . ~ . - . ~ .-
E[∣Eω∣]= E[∣Eχ∣].	(6)
Here the expectation is taken with respect to the randomness in the augmentation design. It is shown
in our experiments that the global approach can indeed help to reduce the value of γ2 (see Appendix
A.8). Note that even though the strategy is presented for the case where |EX | ≤ |ET (which holds for
all datasets considered herein), the scheme can be easily generalized to the case where |Eχ| > |Eω |.
In social networks, users connect to other users that are similar to themselves with higher proba-
bilities (Mislove et al., 2010), hence the graph connectivity naturally inherits bias towards potential
minority groups. Motivated by the reduction of γ2, the present subsection introduces augmentation
schemes over edges to provide a balanced graph structure that can mitigate such bias.
Adaptive Edge Deletion. To obtain a balanced graph structure, we first develop an adaptive edge
deletion scheme where edges are removed with certain deletion probabilities. Based on the graph
structure and sensitive attributes, the probabilities are assigned as
p(e)(eij)
1 - π,
I ∣Eχ |
1 -乖S0Tπ,
1 - 2⅛π,
if si 6= sj
if si = sj = 0
if si = sj = 1
(7)
where p(e) (eij) is the removal probability of the edge connecting nodes vi and vj, π is a hyper-
parameter, and ESω denotes the set of intra-edges connecting the nodes in Si . Note that π is chosen
fc h0 1 in foio ɪɪ7∕^∖ι*lr hιιf 11 c<ιτ> hα oalacfa/l r∖χτ e`ehɑmɑe` aiich oo rγι∙ι∕ι 4α‹ιτ*ch ITm∙M*c∖τα naι*fcι*m<jTICa
to be 1 in this work, but it can be selected by schemes such as grid search to improve performance.
While this graph-level edge deletion scheme does not not directly minimize γ2, it provides a bal-
anced global structure such that Ep(e)[|£岳 |] = Ep(e)[|£& ]] = 11 Ep(e)[|EX|], henceforth equation 6
holds in the augmented graph G, see Appendix A.4 for more details.
Adaptive Edge Addition. For graphs that are very sparse, edge deletion may not be an ideal graph
augmentation as it may lead to unstable results. In this case, an adaptive edge addition framework
is developed to obtain a more balanced graph structure. Specifically, for graphs where |E χ | ≤ |E ω |
holds, |ET - |Eχ∣ pairs of the nodes are sampled uniformly from SX and SX with replacement.
Then, a new edge is created to connect each sampled pair of nodes, in order to obtain an augmented
graph G for which equation 6 holds. Note that experimental results in Section 4 also show that edge
addition may become a better alternative over edge deletion for graphs that are sparse in inter-edges.
Remark 2. Overall, while a subset of these augmentation schemes can be employed based on the
input graph properties (sparse/dense, large/small), all schemes can also be employed on the input
graph sequentially. The framework where node sampling, edge deletion, edge addition are employed
sequentially together with feature masking is called ’FairAug’. It is worth emphasizing that edge
augmentation schemes should always follow node sampling, as performing node sampling changes
the distribution of edges. Since the cardinalities of different sets will be calculated only once (in
pre-processing), we note that the proposed augmentations will not incur significant additional cost.
4	Experiments
In this section, experiments are carried out on 7 real-world datasets for node classification and link
prediction tasks. Performances of our proposed adaptive augmentations are compared with baseline
schemes in terms of utility and fairness metrics.
6
Under review as a conference paper at ICLR 2022
4.1	Datasets and Settings
Datasets. Experiments are conducted on real-world social and citation networks consisting of
Pokec-z, Pokec-n (Dai & Wang, 2020), UCSD34, Berkeley13 (Red et al., 2011), Cora, Citeseer,
Pubmed. Pokec-z and Pokec-n sampled from a larger social network, Pokec (Takac & Zabovsky,
2012), are used in node classification experiments where Pokec is a Facebook-like, social network
used in Slovakia. While the original network includes millions of users, Pokec-z and Pokec-n are
generated by collecting the information of users from two major regions (Dai & Wang, 2020). The
region information is treated as the sensitive attribute, while the working field of the users is the
label to be predicted in classification. Both attributes are binarized, see also (Dai & Wang, 2020).
UCSD34 and Berkeley13 are Facebook networks where edges are created based on the friendship
information in social media. Each user (node) has 7 dimensional nodal features including stu-
dent/faculty status, gender, major, etc. Gender is utilized as the sensitive attribute in UCSD34 and
Berkeley13. Cora, Citeseer, and Pubmed are citation networks that consider articles as nodes and
descriptions of articles as their nodal attributes. In these datasets, the category of the articles is used
as the sensitive attribute. Statistics for datasets are presented in Tables 5 and 6 of Appendix A.5.
Evaluation Metrics. Performance of the node classification task is evaluated in terms of accuracy.
Two quantitative group fairness metrics are used to assess the effectiveness of fairness aware strate-
gies in terms of statistical parity: Δsp = |P(y = 1 | S = 0) - P(y = 1 | S = 1)| and equal
opportunity: Δeo = |P(y =1 | y = 1,s = 0) - P(y = 1 | y = 1,s = 1)|, where y and
y denote the ground-truth and the predicted labels, respectively. Lower values for Δsp and Δeo
imply better fairness performance (Dai & Wang, 2020). For the link prediction task, both accu-
racy and Area Under the Curve (AUC) are employed as utility metrics. As the fairness metrics,
the definitions of statistical parity and equal opportunity are modified for link prediction such that
Δsp = ∣P(y = 1 | e ∈ EX)-P(y = 1 | 巳 ∈ Eω)| andΔeo = |P(y = 1 | y =1,e ∈ EX)-P(y =
1 | y = 1,e ∈ Eω)|, where e denotes the edges and y is the decision for whether the edge exists.
Implementation details. The proposed augmentation scheme is tested on social networks with node
representations generated through an unsupervised contrastive learning framework. Node classifica-
tion and link prediction are employed as ensuing tasks that evaluate the performances of generated
node representations. In addition, we also provide link prediction results obtained via an end-to-end
graph convolutional network (GCN) model on citation networks. Further details on the implemen-
tation of the experiments are provided in Appendix A.7. Contrastive learning is utilized to demon-
strate the effects of the proposed augmentation schemes, as augmentations are inherently utilized in
its original design (You et al., 2020). Specifically, GRACE (Zhu et al., 2020) is employed as our
baseline framework. GRACE constructs two different graph views using random, non-adaptive aug-
mentation schemes, which we replaced by augmentations obtained via FairAug in the experiments.
For more details on the employed contrastive learning framework, see Appendix A.6.
Baselines. We present the performances on a total of 7 baseline studies. As we examine our pro-
posed augmentations in the context of contrastive learning, graph contrastive learning schemes are
employed as the natural baselines. Said schemes are Deep Graph Infomax (DGI) (Velickovic et al.,
2019), Deep Graph Contrastive Representation Learning (GRACE) (Zhu et al., 2020), Graph Con-
trastive Learning with Adaptive Augmentations (GCA) (Zhu et al., 2021), and Fair and Stable Graph
Representation Learning (NIFTY) (Agarwal et al., 2021). We note that the objective function of
NIFTY (Agarwal et al., 2021) consists of both supervised and unsupervised components, and its
results for the unsupervised setting are provided here given the scope of this paper. In addition, as
another family of unsupervised approaches, random walk-based methods for unsupervised node rep-
resentation generation are also considered. Such schemes include DeepWalk (Perozzi et al., 2014),
Node2Vec (Grover & Leskovec, 2016), and FairWalk (Rahman et al., 2019). Lastly, for end-to-end
link prediction, we present results for the random edge dropout scheme (Rong et al., 2019).
4.2	Experimental Results
The comparison between baselines and our proposed framework FairAug is presented in Table 1.
Note that ’FairAug wo ED’ in Table 1 refers to FairAug where edge deletion (ED) is removed
from the chain, but all node sampling (NS), edge addition (EA), and feature masking (FM) are
still employed. Firstly, the results of Table 1 show that FairAug provides roughly 45% reduction
in fairness metrics over GRACE, the strategy it is built upon, while providing similar accuracy
7
Under review as a conference paper at ICLR 2022
Table 1: Comparative Results with Baselines on Node Classification
	Pokec-z			Pokec-n		
	Accuracy (%)	δSP (%)	δeo (%)	Accuracy (%)	δsp (%)	∆EO (%)
DeepWalk	60.82 ± 0.77	9.79 ± 3.43	8.51 ± 1.67	59.66 ± 1.02	20.19 ± 3.38	22.96 ± 3.44
Node2Vec	61.56 ± 0.50	16.18 ± 12.40	14.88 ± 12.31	60.31 ± 0.75	20.76 ± 1.64	20.32 ± 1.36
FairWalk	57.48 ± 0.11	12.03 ± 10.19	10.95 ± 8.82	57.44 ± 0.73	15.00 ± 1.40	15.72 ± 1.80
DGI	65.56 ± 1.29	4.82 ± 1.89	5.81 ± 0.97	65.71 ± 0.24	5.18 ± 2.15	7.16 ± 2.44
GRACE	67.09 ± 0.47	6.20 ± 2.22	6.18 ± 2.46	67.90 ± 0.13	8.85 ± 1.39	10.13 ± 2.11
GCA	66.85 ± 0.71	7.40 ± 4.14	7.46 ± 4.09	67.02 ± 0.43	5.31 ± 0.62	7.75 ± 1.49
NIFTY	66.09 ± 0.40	5.86 ± 1.97	6.19 ± 1.72	65.44 ± 0.17	5.18 ± 2.80	5.78 ± 3.18
FairAug	67.04 ± 0.69	3.29 ± 1.66	3.04 ± 1.37	67.88 ± 0.45	4.81 ± 0.59	6.52 ± 0.99
FairAug wo ED	67.38 ± 0.31	2.66 ± 3.22	4.26 ± 2.92	67.57 ± 0.18	3.37 ± 0.62	5.13 ± 1.12
Table 2: Ablation Study on Node Classification
	Pokec-z			Pokec-n		
	Accuracy (%)	δsp (%)	δeo (%)	Accuracy (%)	δsp (%)	∆EO (%)
GRACE	67.09 ± 0.47	6.20 ± 2.22	6.18 ± 2.46	67.90 ± 0.13	8.85 ± 1.39	10.13 ± 2.11
FairAug	67.04 ± 0.69	3.29 ± 1.66	3.04 ± 1.37	67.88 ± 0.45	4.81 ± 0.59	6.52 ± 0.99
FairAug wo FM	66.88 ± 0.42	4.64 ± 1.91	5.50 ± 1.80	66.15 ± 0.75	7.61 ± 0.29	9.28 ± 1.06
FairAug wo NS	67.01 ± 0.41	6.05 ± 2.63	6.83 ± 2.88	66.33 ± 0.20	6.26 ± 0.26	9.16 ± 0.63
FairAug wo ED	67.38 ± 0.31	2.66 ± 3.22	4.26 ± 2.92	67.57 ± 0.18	3.37 ± 0.62	5.13 ± 1.12
FairAug wo EA	66.96 ± 1.08	2.86 ± 1.79	3.22 ± 1.29	67.96 ± 0.19	7.13 ± 0.29	9.40 ± 0.81
values. Second, we note that similar to our framework, GCA is built upon GRACE through adaptive
augmentations as well. However, the adaptive augmentations utilized in GCA are not fairness-aware,
and the results of Table 1 demonstrate that the effect of such augmentations on the fairness metrics
is unpredictable. Third, the results indicate that all contrastive learning methods provide better
fairness performance than random walk-based methods on evaluated datasets, including Fairwalk,
which is a fairness-aware study. Since the sole information source of random walk-based studies is
the graph structure, obtained results confirm that the graph topology indeed propagates bias, which
is consistent with the motivation of our graph data augmentation design. Finally, the results of Table
1 demonstrate that the closest competitor scheme to FairAug is NIFTY (Agarwal et al., 2021) in
terms of fairness measures. For ∆SP, FairAug outperforms NIFTY on both datasets, whereas in
terms of ∆EO , FairAug and NIFTY outperform each other on Pokec-z and Pokec-n, respectively.
Table 1 shows that the ED-removed version of FairAug, ’FairAug wo ED’, outperforms NIFTY on
Pokec-n as well, suggesting that at least one of the proposed strategies outperform all considered
benchmark schemes in both ∆SP and ∆EO . This fairness performance improvement achieved by
ED removal motivated us to conduct an ablation study on the building blocks of FairAug, which we
present in the sequel.
Table 2 lists the results of an ablation study for FairAug, where the last four rows demonstrate
the effects of the removal of FM, NS, ED, and EA, respectively. Pokec-z and Pokec-n are highly
unbalanced datasets (|Eω| ≈ 20|Eχ∣) with a considerable sparsity in inter-edges (see Table 5 of
Appendix A.5). For such networks, the exact probabilities presented in equation 7 cannot be utilized
for edge deletion as it would result in the deletion of a significantly large portion of the edges,
damaging the graph structure. We employ an upper limit on the deletion probabilities to avoid
this (see Appendix A.7). However, with such a limit, the proposed ED framework alone cannot
sufficiently balance |ET and |Eχ∣. At this point, We note that since |ST > |Sχ∣, node sampling
also provides a way of decreasing |ET by excluding nodes from the set Sω when generating the
subgraph. As the graph was already sparse initially in inter-edges (i.e., small |Eχ∣), employing
both NS and ED causes a similar over-deletion of intra-edges and creates unstable results due to the
significantly distorted graph structure. Overall, combining this phenomenon with the results of Table
2, in order to balance the input graphs on Pokec networks consistently (which have |ET >> |Eχl
and inter-edge sparsity), node sampling is observed to be a better choice than edge manipulations.
Table 3 presents the obtained results for link prediction in the framework of contrastive learning.
Results demonstrate that ’FairAug wo NS’ consistently improves the fairness metrics of the frame-
work it is built upon (GRACE), together with similar utility measures. In addition, comparing GCA
8
Under review as a conference paper at ICLR 2022
Table 3: Link prediction results obtained on node representations
	UCSD34			Berkeley13		
	AUC (%)	δSP (%)	δeo (%)	AUC (%)	δsp (%)	∆EO (%)
GCA	71.59 ± 0.29	0.70 ± 0.27	1.92 ± 0.45	69.69 ± 0.38	0.50 ± 0.36	6.52 ± 0.99
GRACE	71.57 ± 0.28	0.49 ± 0.28	1.48 ± 0.70	69.55 ± 0.46	0.76 ± 0.44	4.34 ± 1.20
FairAug	71.46 ± 0.30	0.71 ± 0.38	1.62 ± 0.62	69.48 ± 0.29	0.70 ± 0.43	4.24 ± 0.90
FairAug wo NS	71.50 ± 0.31	0.41 ± 0.32	1.16 ± 0.65	69.65 ± 0.33	0.68 ± 0.34	4.22 ± 0.97
Table 4: Employment of fair edge deletion as an edge dropout method
	Accuracy (%)	AUC (%)	∆SP (%)	∆EO (%)
Cora	Edge Dropout 82.79 ± 0.83	90.52 ± 0.52	57.22 ± 2.19	36.18 ± 4.53
	Fair ED	80.38 ± 1.14	87.95 ± 1.22	48.78 ± 2.58	27.79 ± 3.89
Citeseer	Edge Dropout 78.30 ± 1.17	87.93 ± 1.05	43.05 ± 2.39	25.45 ± 3.69
	Fair ED	77.26 ± 1.80	86.79 ± 1.39	39.90 ± 3.57	25.02 ± 6.93
Pubmed	Edge Dropout 88.63 ± 0.34	95.14 ± 0.16	45.59 ± 0.78	15.81 ± 0.92
	Fair ED	87.48 ± 0.54	94.15 ± 0.36	40.90 ± 1.10	11.70 ± 0.68
and GRACE, obtained results confirm our previous assessment regarding the unpredictable effect of
GCA’s augmentations on the fairness metrics. Finally, comparing FairAug with and without NS, the
results of Table 3 show that in UCSD34 and Berkeley13, the employment of node sampling can be
ineffective in improving fairness metrics, or can even worsen them. In UCSD34 and Berkeley13, we
have |Sχ∣ >> |ST (see Table 5 of Appendix A.5). Therefore, for NS on these datasets, half of the
nodes are sampled randomly from the sets S1χ and S0χ (as the limit on the minimum node sampling
budget is half of the initial group size to avoid a possible over-sampling, see Appendix A.7). Since
|Sχ∣ >> |Sω|, such a sampling framework coincides with random sampling, which makes the ef-
fects of the proposed node sampling scheme unpredictable on the fairness metrics. Furthermore,
while γι suggests that the cardinality of the set |Sχ∣ should be reduced when |Sω | ≤ |Sχ∣, γι ac-
tually appears in the upper bound and not in the exact kρk1 expression. The removal of nodes with
inter-edges is actually counter-intuitive, as inter-edges generally help to reduce bias in graphs where
|ET ≥ |EX | (which holds for all datasets considered herein). Therefore, the proposed node sam-
pling becomes effective for graph structures providing |S T ≥ |Sχ |. The effect of node sampling on
Facebook networks is further investigated through γ1 and γ2 in Appendix A.8, which corroborates
the explanations on the random/detrimental effect of node sampling for these datasets.
As also noted in Section 1, even though FairAug is presented through its application on graph con-
strastive learning in this section, the proposed approach can be fully or partially used in conjunction
to other learning frameworks as well. To exemplify such a use case, Table 4 lists the results for an
end-to-end link prediction task with a two-layer GCN where the proposed fair edge deletion scheme
is employed as an edge dropout method. As the benchmark, random edge dropout (Rong et al.,
2019) is considered, which is a scheme originally proposed to improve the generalizability of the
model on unseen data (Rong et al., 2019). In the experiments, the number of deleted edges is the
same for both strategies. The results demonstrate that our method ’Fair ED’ can indeed enhance the
fairness of the learning framework it is employed in while it slightly reduces the utility measures.
5	Conclusions
In this study, the source of bias in aggregated representations in a GNN-based framework has been
theoretically analyzed. Based on the analysis, several fairness-aware augmentation schemes have
been introduced on both graph structure and nodal features. The proposed augmentations can be
flexibly utilized together with several GNN-based learning methods. In addition, they can readily be
employed in unsupervised node representation learning schemes such as graph contrastive learning.
Experimental results on real-world graphs demonstrate that the proposed adaptive augmentations
can improve fairness metrics with comparable utilities to state-of-the-art in node classification and
link prediction.
9
Under review as a conference paper at ICLR 2022
6	Reproducibility Statement
The proofs for Theorem 1 and Proposition 1 are presented together with all assumptions made for
them in Appendix A.1 and Appendix A.2, respectively. Implementation details and hyperparameter
values are presented in Appendix A.7 for the clarification of experimental settings. All codes that
are needed to regenerate the presented results in Tables 1, 2, 3 and 4 are provided within the sup-
plementary material together with a README file. Finally, the details on the utilized data sets are
provided in Appendix A.5 where all datasets are publicly available.
References
Chirag Agarwal, Himabindu Lakkaraju*, and Marinka Zitnik*. Towards a unified framework for
fair and stable graph representation learning. arXiv preprint arXiv:2102.13186, February 2021.
Amr Ahmed, Nino Shervashidze, Shravan Narayanamurthy, Vanja Josifovski, and Alexander J
Smola. Distributed large-scale natural graph factorization. In Proc. International Conference
on World Wide Web (WWW),pp. 37-48, May 2013.
Alex Beutel, Jilin Chen, Zhe Zhao, and Ed H Chi. Data decisions and theoretical implications when
adversarially learning fair representations. arXiv preprint arXiv:1707.00075, June 2017.
Avishek Bose and William Hamilton. Compositional fairness constraints for graph embeddings. In
International Conference on Machine Learning (ICML, pp. 715-724. PMLR, 2019.
Maarten Buyl and Tijl De Bie. Debayes: a bayesian method for debiasing network embeddings. In
International Conference on Machine Learning (ICML), pp. 1220-1229. PMLR, 2020.
Maarten Buyl and Tijl De Bie. The kl-divergence between a graph model and its fair i-projection as
a fairness regularizer. arXiv preprint arXiv:2103.01846, 2021.
Shaosheng Cao, Wei Lu, and Qiongkai Xu. Grarep: Learning graph representations with global
structural information. In Proc. ACM International Conference on Information and Knowledge
Management (CIKM), pp. 891-900, October 2015.
Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the over-
smoothing problem for graph neural networks from the topological view. In Proceedings of the
AAAI Conference on Artificial Intelligence, volume 34, pp. 3438-3445, 2020a.
Haochen Chen, Bryan Perozzi, Yifan Hu, and Steven Skiena. Harp: Hierarchical representation
learning for networks. In Proc. AAAI Conference on Artificial Intelligence, volume 32, February
2018.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework
for contrastive learning of visual representations. In Proc. International Conference on Machine
Learning (ICML), pp. 1597-1607, July 2020b.
Enyan Dai and Suhang Wang. Say no to the discrimination: Learning fair graph neural networks
with limited sensitive attribute information. arXiv preprint arXiv:2009.01454, September 2020.
Yushun Dong, Jian Kang, Hanghang Tong, and Jundong Li. Individual fairness for graph neural
networks: A ranking based approach. In Proc ACM Conference on Knowledge Discovery & Data
Mining (SIGKDD), pp. 300-310, 2021.
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness
through awareness. In Proc. Innovations in Theoretical Computer Science (ITCS), pp. 214-226,
January 2012.
Joseph Fisher, Arpit Mittal, Dave Palfrey, and Christos Christodoulopoulos. Debiasing knowledge
graph embeddings. In Proc. Conference on Empirical Methods in Natural Language Processing
(EMNLP), pp. 7332-7345, 2020.
10
Under review as a conference paper at ICLR 2022
Alberto Garcla-Duran and Mathias Niepert. Learning graph representations with embedding ProPa-
gation. In Proc. International Conference on Neural Information Processing Systems (NeurIPS),
pp. 5125-5136, December 2017.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proc. International Conference on Artificial Intelligence and Statistics (AISTATS),
pp. 249-256, May 2010.
Ignacio S Gomez, Bruno G da Costa, and Maike AF Dos Santos. Majorization and dynamics of
continuous distributions. Entropy, 21(6):590, 2019.
Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proc. ACM
International Conference on Knowledge Discovery and Data Mining (SIGKDD), August 2016.
Sara Hajian and Josep Domingo-Ferrer. A methodology for direct and indirect discrimination pre-
vention in data mining. IEEE Transactions on Knowledge and Data Engineering, 25(7):1445-
1459, July 2013. doi: 10.1109/TKDE.2012.72.
William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In Proc. International Conference on Neural Information Processing Systems (NeurIPS),
pp. 1025-1035, December 2017.
Kaveh Hassani and Amir Hosein Khasahmadi. Contrastive multi-view representation learning on
graphs. In Proc. International Conference on Machine Learning (ICML), pp. 4116-4126, July
2020.
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. In Proc. International Conference on Learning Representations (ICLR), April
2018.
Bas Hofstra, Rense Corten, Frank Van Tubergen, and Nicole B Ellison. Sources of segregation
in social networks: A novel approach using facebook. American Sociological Review, 82(3):
625-656, May 2017.
Fenyu Hu, Yanqiao Zhu, Shu Wu, Liang Wang, and Tieniu Tan. Hierarchical graph convolutional
networks for semi-supervised node classification. In Proc. International Joint Conference on
Artificial Intelligence, (IJCAI), August 2019.
XU Ji, Joao F Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised
image classification and segmentation. In Proc. IEEE International Conference on Computer
Vision (ICCV), pp. 9865-9874, October 2019.
Guangyin Jin, Qi Wang, Cunchao Zhu, Yanghe Feng, Jincai Huang, and Jiangping Zhou. Ad-
dressing crime situation forecasting task with temporal graph convolutional neural network ap-
proach. In Proc. International Conference on Measuring Technology and Mechatronics Automa-
tion (ICMTMA), pp. 474-478, February 2020. doi: 10.1109/ICMTMA50254.2020.00108.
Kushal Kafle, Mohammed Yousefhussien, and Christopher Kanan. Data augmentation for visual
question answering. In Proceedings of the 10th International Conference on Natural Language
Generation, pp. 198-202, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In Proc. International Conference on Learning Representations (ICLR), April 2017.
Charlotte Laclau, Ievgen Redko, Manvi Choudhary, and Christine Largeron. All of the fairness for
edge prediction with optimal transport. In International Conference on Artificial Intelligence and
Statistics, pp. 1774-1782. PMLR, 2021.
11
Under review as a conference paper at ICLR 2022
Peizhao Li, Yifei Wang, Han Zhao, Pengyu Hong, and Hongfu Liu. On dyadic fairness: Explor-
ing and mitigating bias in graph connections. In Proc. International Conference on Learning
Representations (ICLR), April 2021.
Jiaqi Ma, Junwei Deng, and Qiaozhu Mei. Subgroup generalization and fairness of graph neural
networks. arXiv preprint arXiv:2106.15535, 2021.
Albert W Marshall, Ingram Olkin, and Barry C Arnold. Inequalities: theory of majorization and its
applications, volume 143. Springer, 1979.
Alan Mislove, Bimal Viswanath, Krishna P Gummadi, and Peter Druschel. You are who you know:
inferring user profiles in online social networks. In Proc. ACM International Conference on Web
Search and Data Mining (WSDM), pp. 251-260, February 2010.
Felix L Opolka, Aaron Solomon, Catalina Cangea, Petar VeliCkovic, Pietro Lio, and R Devon
Hj elm. Spatio-temporal deep graph infomax. arXiv preprint arXiv:1904.06316, April 2019.
Mingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, and Wenwu Zhu. Asymmetric transitivity pre-
serving graph embedding. In Proc. ACM International Conference on Knowledge Discovery and
Data Mining (SIGKDD), pp. 1105-1114, August 2016.
Zhen Peng, Wenbing Huang, Minnan Luo, Qinghua Zheng, Yu Rong, Tingyang Xu, and Junzhou
Huang. Graph representation learning via graphical mutual information maximization. In Proc.
Web Conference (WWW), pp. 259-270, April 2020.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social rep-
resentations. In Proc. ACM International Conference on Knowledge Discovery and Data
Mining (SIGKDD), pp. 701-710, August 2014. URL http://doi.acm.org/10.1145/
2623330.2623732.
Tahleen A Rahman, Bartlomiej Surma, Michael Backes, and Yang Zhang. Fairwalk: Towards fair
graph embedding. In Proc. International Joint Conference on Artificial Intelligence (IJCAI), pp.
3289-3295, August 2019.
Veronica Red, Eric D Kelsic, Peter J Mucha, and Mason A Porter. Comparing community structure
to characteristics in online collegiate social networks. SIAM review, 53(3):526-543, 2011.
Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph
convolutional networks on node classification. arXiv preprint arXiv:1907.10903, 2019.
Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning.
Journal of Big Data, 6(1):1-48, 2019.
Indro Spinelli, Simone Scardapane, Amir Hussain, and Aurelio Uncini. Biased edge dropout for
enhancing fairness in graph representation learning. arXiv preprint arXiv:2104.14210, 2021.
Lubos Takac and Michal Zabovsky. Data analysis in public social networks. In International Sci-
entific Conference and International Workshop. ’Present Day Trends of Innovations’, volume 1,
May 2012.
Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale
information network embedding. In Proc. International Conference on World Wide Web (WWW),
pp. 1067-1077, May 2015.
Petar VeliCkoviC, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. Proc. International Conference on Learning Representations
(ICLR), April 2018.
Petar VeliCkoviC, William Fedus, William L. Hamilton, Pietro Lio, Yoshua Bengio, and R Devon
Hjelm. Deep graph infomax. In Proc. International Conference on Learning Representations
(ICLR), May 2019. URL https://openreview.net/forum?id=rklz9iAcKQ.
Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Sim-
plifying graph Convolutional networks. In Proc. International Conference on Machine Learning
(ICML), pp. 6861-6871, July 2019.
12
Under review as a conference paper at ICLR 2022
Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-
parametric instance discrimination. In Proc. IEEE Conference on Computer Vision and Pattern
Recognition (CVPR),pp. 3733-3742, June 2018.
Mang Ye, Xu Zhang, Pong C Yuen, and Shih-Fu Chang. Unsupervised embedding learning via
invariant and spreading instance feature. In Proc. IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pp. 6210-6219, June 2019.
Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph
contrastive learning with augmentations. Proc. International Conference on Neural Information
Processing Systems (NeurIPS), 33, December 2020.
Ziqian Zeng, Rashidul Islam, Kamrun Naher Keya, James Foulds, Yangqiu Song, and Shimei
Pan. Fair representation learning for heterogeneous information networks. arXiv preprint
arXiv:2104.08769, 2021.
Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text clas-
sification. Advances in neural information processing systems, 28:649-657, 2015.
Tong Zhao, Yozen Liu, Leonardo Neves, Oliver Woodford, Meng Jiang, and Neil Shah. Data aug-
mentation for graph neural networks. arXiv preprint arXiv:2006.06830, 2020.
Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep Graph Con-
trastive Representation Learning. In Proc. International Conference on Machine Learning
(ICML) Workshop on Graph Representation Learning and Beyond, July 2020. URL http:
//arxiv.org/abs/2006.04131.
Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph contrastive learning
with adaptive augmentation. In Proc. Web Conference (WWW), April 2021.
13
Under review as a conference paper at ICLR 2022
A Appendix
A.1 PROOF OF THEOREM 1
Let s
as
得 PNXI Si =孕 Hence, the elements of centered sensitive attribute vector can be written
Pi
1
No%%
(£ (0 - S)(Zm,i - Zi)+ £ (1 - S)(zn,i
Vm ∈S0	Vn∈Sι
-Zi))
(8)
where Si = N PNι Zji and Zji is the element of matrix Z at row j, column i. Using S values, the
equation 8 becomes
1	/	-∣Sι|	/	—、	|Sq| /	_ 、、
pi =	Nσ σ-	(	Σ ~N- (zm,i	- Zi)+	Σ ɪ (zn,i	- Zi))，
S Z：，i Vm∈S0	Vn∈Sι
1	(∣SQ∣∣S1∣ K
Nosoz：,-	N i
+
vm ∈S0
-∣s1∣ Z .-
N Zm,i
P Si + X N Zn,i)，
Vn∈Sι
Nσs⅛- (X
S Z：，i Vn∈Sι
NZn,i - X TZm,,
vm ∈ S0
(9)
∣sq∣∣si∣ 1 1 X
N2σsσz:,- ι∣Sι∣ y
,∖	Vn∈S1
Zn,i -
ɪ X
ISQI °m⅛0
Similarly analysis for σs follows as
∙=∖
1 N
N X(Sn
n=1
S)2
v^SQW∣
(10)
Define Ci := F⅛
and C ∈ RF denote the vector whose elements are c,s. Then P equals to
P=C 00⅛2zn
-ɪ X
ISQI Vm∈S0
zm
(11)
—
N
where o represents the Hadamard product. Therefore, ∣∣ρ∣∣ι follows as
kPk1
C ol∣⅛ vS1zn -∣SQ∣ 工 0Zm
(12)
1
We first consider the terms
i⅛ PVn∈Sι Zn and i⅛ PVm∈S0 Zm individually
∣⅛ X Zn
1	11 Vn∈S1
焉X (g X MQ +
|S1 |
Vn∈S1
a∈N (n)∩S0
；X μι) ± δ
n b∈N(n)∩S1
∈∣⅛ X
1	11 Vn∈S1
dχμo + dnμι ± δ
(13)
∈∣⅛ 工("1 + dn⅜("Q
Vn∈S1
-μι)	± ∆∙
∈
14
Under review as a conference paper at ICLR 2022
Similarly, the expression for the term 帚 PvE esθ Zm can also be derived.
高工。Zm ∈ ⅛∣ 工. ("0 + d⅛3 - 丽)士 δ∙
(14)
Define W :=旖 Pv九e5ι Zn - % PvEESo Zm, the following can be written by equation 13 and
equation 14
E ∈ (μι - μo)
1 - i⅛v∑ (d⅛⅛)- i⅛v∑o (dm⅛)) ± 2δ
(15)
E is bounded below, which follows as
1-
1 X ( dn
ISii U之【曲+ dn
vn ESI
(16)
(17)
equation 16 follows as dζ = 0 for ∀vu ∈ Sω. The upper bound equation 17 holds, since	≤
1 for ∀νu ∈ G.	U U
The upper bound for E follows as
1 -i⅛2 ⅛+⅛)-i⅛ι工(
vnESI	vmESO
dχ
m
dm + dm
(18)
1 - mean (dm‰ ivm ∈ S0) - mean (dχ‰ ivn ∈ SI
(19)
≤ 1 一 2 min mean
dχ
m
dm + dm
|vm ∈ So) , mean ɑ^ dnd ∣vn ∈ Si
(20)
where mean(∙) denotes the sample mean operation. The main purpose is to generate an upper bound
for ∣∣ρkι, for which the first step is to derive the following bound
kρki ≤∣c∣i忖 1	(21)
equation 21 can be written, as ∣∣x o y∣ι ≤ ∣∣x∣ι∣∣y∣ι for arbitrary vectors X and y ∈ RN (∣χιyι∣ +
---+ ∣xNyNI ≤ (|xi| + + ∣χN∣)(∣yι∣ + + ∣yN∣) due to additional non-negative cross-terms
Igiiyj ∣,i = j). The next step is to bound the 'i norm of the difference term e:
IlEIlI ≤ ∣∣μι - μ0∣∣1max(γ1,γ2)+2Ν∆	(22)
using the derived bounds in equation 17, equation 20 and the equality 2∆ ∙ ∣∣1∣ι =
... ............... Qr ,	,	r	iSXi	iSXi	,
2N∆, where 1 denotes an N × 1 vector of ones, γι := 1 -	- ∙∣S1∣ , and γ2 =
∣1 - 2min (mean、2^ ∣vm ∈ S0) ,mean (碇f* ∣vn ∈ Si)) . Using equation 21 and equa-
tion 22, the final upper bound can be derived:
IIPkI ≤ IICkI (∣μι - μ0∣∣1max(γ1,γ2) + 2Ν∆).	(23)
15
Under review as a conference paper at ICLR 2022
(24)
(25)
A.2 Proof of Proposition 1
The expected kδk1 can be written as:
F
Ep[kδkι] = E X 同
i=1
FF
=X E[|&|] = X Pi®1
i=1	i=1
Similarly, the expected kδk1 for the uniform masking scheme is:1
F
Eq[kδkl] = X qi∣δi∣.
i=1
For fair comparison, set uniform keeping probability of the nodal features qi = P= F PF=I Pi.
Without loss of generality, assume, ∣δ∕s are ordered such that ∣δι∣ ≤ ∙∙∙ ≤ ∣δF|. With the ordered
∣δi∣s, assigned probabilities to keep the nodal features by our method will also be ordered such that
Pi ≥ ∙∙∙ ≥ Pf. Defining a dummy variable ∣δ0 | := 0, equation 24 can be rewritten as:
F
XPi∣δi∣ = (∣δι∣ - ∣δo∣)(pi +-+ PF) + (∣δ2∣ - ∣δι|)(P2 +-+ PF) + …
i=1
---+ (∣δF-i| -∣δF-2∣)(pf-1 + PF) + (∣δF| — 恒正-l|)(PF)	(26)
FF
=X(lδι | -D X Pi.
l=1	i=l
Following the definition in (Marshall et al., 1979), a sequence x
another sequence y = {y1, y2, . . . , yF}, if the following holds:
kk
yi ≤	xi k = 1, . . . , F - 1
i=1	i=1
FF
X yi = X xi .
i=1	i=1
Since the uniform sequence is majorized by any other non-increasing ordered sequence with the
same sum (see (Gomez et al., 2019, Equation 3)), the sequence P = {P1,P2, ∙∙∙ ,pf} majorizes
the sequence q = {q1,q2,...,Qf} where q% = P, ∀i ∈ {1,..., F}. Defining P0=iPi := 0,
equation 26 can be re-written as
{x1, x2, . . . , xF} majorizes
(27)
FF
Ep[kδki] = X(∣δι∣-∣δι-ι∣) XPi
l=1	i=l
F	l-1
=X(∣δι∣-∣δι-ι∣)(FP - XPi)
l=1	i=1
(a) F	l-1
≤ ∑(∣δι∣-∣δι-ι∣)(FP- Eqi)
l=1	i=1
FF
=X(∣δι∣-∣δi-i∣) XQi
l=1	i=l
~
=Eq[kδki ],
(28)
1From equation 25, it can be obtained that Eq[∣∣δ∣∣ι] = PF=I qi∣δi∣≤ PF=ι ∣δi∣ = ∣∣δkι, as the probabil-
ities qi ∈ [0, 1]. Here, ∣δ ∣1 corresponds to the distribution difference with no feature masking (qi = 1, ∀i).
This implies that no feature masking upper bounds E[∣δ∣1], which is obtained viaa stochastic feature masking.
16
Under review as a conference paper at ICLR 2022
where inequality (a) follows from the definition of majorization in equation 27.
A.3 NODE Sampling and its Effects
Algorithm 1: Node Sampling
Data: G := (V, E), X , s, β
_ _ :二一
Result: G, X, S
Split V into sets {5j, Sχ, Sq , Sf }
if |Sω| ≥ |Sχ∣ then	_
Sample ∣Sχ∣ nodes from Sf uniformly to obtain Sf;
Sample ∣Sχ∣ nodes from Sf uniformly to obtain Sf;
~ -
V: {Sχ,Sf,Sf};
end
if ∣Sχ∣ ≥ ∣ST then
Sample ∣Sf ∣ nodes from SX uniformly to obtain Sf;
Sample ∣Sf ∣ nodes from SX uniformly to obtain Sf;
V:= {SX, SX, Sf };
end
Obtain subgraph G induced by V along with the nodal features X and sensitive attributes S
The overall node sampling scheme is presented in Algorithm 1.
Assuming ∣ST ≥ ∣Sχ∣, all nodes in SX are retained, meaning SX = SX and S X = Sf. While
subsets of nodes Sf and Sf are randomly sampled from Sf and Sf respectively with sample size
∣Sf ∣ = ∣SX∣ and ISfI = ∣SX∣.
Therefore, in the resulting graph G, we have
∣Sf ∣ =∣Sf ∣ = ∣SX∣	(29)
∣Sf ∣ =∣Sf ∣ = ∣S X∣	(30)
therefore
JSX[	=	∣sX∣	= 1	(31)
∣SX∣ +	∣Sf ∣	∣SX∣	+ ∣SX∣	2	(3)
同=IS X∣	= 1	(32)
∣Sx∣ +	∣Sf ∣	∣S X∣	+ ∣S X∣	2	(3)
which leads to Yι = 0 for the obtained G.
A.4 Effects of global edge deletion scheme in equation 7
Given the edge deletion scheme in equation 7, we have
	Ep(e)	[∣ex∣]=	=π∣E x∣			(33)
	Ep(e)	[∣⅛ ∣ 卜	=jfɪ 2 I ES0∣	π ×	i ESo i =手	(34)
	Ep(e)	卜黑∣卜	=∣E x∣ 21%I	π ×	I Ef1I = *.	(35)
Therefore, Ep(e)卜£扁 ∣]=	=Ep(e)	[∣⅛φ	=2Ep(e)	[∣E;	x∣] and equation 6 holds.	
17
Under review as a conference paper at ICLR 2022
A.5 Dataset Statistics
Dataset statistics are provided in Tables 5 and 6.
Table 5: Dataset statistics for social networks
Dataset	ISXI	IS0ω I	ISXI	IS1ω I	IEXI	IEω I S0	IESω1 I
Pokec-z	622	4229	582	2226	1730	23428	15942
Pokec-n	423	3617	479	1666	1422	18548	10672
UCSD34	2246	118	1697	71	51607	36787	19989
Berkeley13	1619	80	1488	77	27542	19550	13582
Table 6: Dataset Statistics for Citation networks
Dataset	IVI	# sensitive attr.	IEXI	IEωI
Cora	2708	7	1428	5964
Citeseer	3327	6	1628	4746
Pubmed	19717	3	12254	49802
A.6 Contrastive Learning over Graphs
The main goal of contrastive learning is to learn discriminable representations by contrasting the em-
beddings of positive and negative examples, through minimizing a specific contrastive loss (Opolka
et al., 2019; Velickovic et al., 2019; ZhU et al., 2021; 2020). The contrastive loss in the present work
is designed to maximize node-level agreement, meaning that the representations of the same node
generated from different graph views can be discriminated from the embeddings of other nodes. Let
1	11	2	22
H1 = f(A1, X1) and H2 = f(A2, X2) denote the nodal embeddings generated with graph views
12	i	i	i
G1 and G2 , where Ai, and Xi are the adjacency and featUre matrices of Gi, which are corrUpted
versions of the matrices A and X. Let hi1 and hi2 denote the embeddings for vi : They shoUld be
more similar to each other than to the embeddings of all other nodes. Hence, the representations of
all other nodes are Used as negative samples. The contrastive loss for generating embeddings hi1 and
hi2 (considering hi1 as the anchor representation) can be written as
es(h1,h2)/T
'(hi, hi)=-log Ih1,h”斗Pk= i[k=i]es(h1，hk)/T+pN=ii[k=i]es(h1，hk)/T	(36)
where S (hɪ, h2) := C (g(h；),g(h)), with c(∙, ∙) denoting the cosine similarity between the in-
put vectors, and g(∙) representing a nonlinear learnable transform executed by utilizing a 2-layer
multi-layer perceptron (MLP), see also, (Zhu et al., 2021; 2020; Chen et al., 2020b). τ denotes the
temperature parameter, and 1[k6=i] ∈ {0, 1} is the indicator function which takes the value 1 when
k 6= i. This objective can also be written in the symmetric form, when the generated representation
hi2 is considered as the anchor example. Therefore, considering all nodes in the given graph, the
final loss can be expressed as
1N
J = 2N X[' (h1, h2) +' (h2, h1)]∙
2 i=1
(37)
A.7 Hyperparameters and Implementation Details
Contrastive Learning: All experimental settings are kept unaltered as presented in our baseline
study GRACE (Zhu et al., 2020) for fair comparison, where in the GNN-based encoder, weights are
initialized utilizing Glorot initialization (Glorot & Bengio, 2010) and ReLU activation is used after
each GCN layer. All models are trained for 400 epochs by employing Adam optimizer (Kingma &
Ba, 2014) together with a learning rate of5 × 10-4 and `2 weight decay factor of 10-5. The tempera-
ture parameter τ in contrastive loss is chosen to be 0.4 and the dimension of the node representations
is selected as 256 for all datasets.
18
Under review as a conference paper at ICLR 2022
End-to-end training: All models are trained for 100 epochs by employing Adam optimizer
(Kingma & Ba, 2014) together with a learning rate of 5 × 10-3. The dimension of the node repre-
sentations is selected as 128 for all datasets.
Implementation details: Node representations are obtained using a two-layer GCN (Kipf &
Welling, 2017) for all contrastive learning baselines, which is kept the same as the one used in
(Zhu et al., 2021; 2020) to ensure fair comparison. After obtaining node embeddings from differ-
ent schemes, a linear classifier based on '2-regularized logistic regression is applied for the node
classification and link prediction tasks, which is again the same as the scheme used in (VelickoVic
et al., 2019; Zhu et al., 2021; 2020). In node classification, the classifier is trained on 90% of the
nodes selected through a random split, and the remaining nodes constitute the test set. For each
experiment, results for three random data splits are obtained, and the aVerage together with standard
deViations are presented. In link prediction, 90% of the edges are utilized in the training of GCN
encoder and linear classifier. For each edge, the input to the linear classifier is the concatenated
representations of the nodes that the edge connects. Results for link prediction are obtained for fiVe
different edge splits, the resulting aVerage metrics and standard deViations are proVided. Finally, in
the end-to-end link prediction experiments, a two-layer GCN is trained, and results are obtained for
6 different random data splits.
Note that if the sizes of the sets S and S are highly unbalanced, we put a limit on the minimum
of sampling budgets in node sampling to preVent any excessiVe corruption of the input graph. If
Sχ ≥ Sω , the minimum limit for each set is the 50% of the initial group size, otherwise the limit is
the 25% of the initial group size. Furthermore, to aVoid oVerly down sampling the edges, a minimum
Value for p(e) (eij), p(me)in, is set for the edge deletion scheme. In the feature masking scheme, α in
equation 2 is set to 0 and 0.1 for different graph Views used in the contrastiVe loss (i.e., default
parameter Values proVided for GCA are used directly). In the edge deletion framework, π = 1, and
Pmin = π2 = 0.5 are used in the experiments of both Pokec datasets and citation networks. Such
a selection is utilized to present the scheme with the most “natural” hyperparameter Value, in the
sense that only intra-edges are deleted, where the already sparse inter-edges are left unchanged. We
note that this selection is naturally a good choice for unbalanced data sets in terms of inter- and
intra-edges. For Facebook networks where intra- and inter-edges are balanced for certain sensitiVe
groups, ∏ = 0.8, andPmin is set as 2 for all experiments.
Proposed augmentation frameworks haVe only two hyperparameters: α in feature masking and π in
adaptiVe edge deletion. While we note the Value of α is kept the same for all baselines that utilize a
feature masking scheme (default hyperparameters proVided by GCA (Zhu et al., 2021)), and 1 is the
most natural choice for π in Pokec datasets and citation networks, we carry out a sensitiVity analysis
for these two hyperparameters in this appendix. In the analysis, for α, the Values [0.1, 0.2, 0.3] are
examined for the first graph View, where the α Value for the second View is assigned to be higher
than the Value of the first one by 0.1. OVerall, sensitiVity analyses for Pokec and Facebook networks
on the FairAug algorithm are presented in Tables 7 and 8, respectiVely. In Tables 7 and 8, the first
column shows the α Values of the first and second View, respectiVely.
The sensitiVity analysis for α on Pokec networks in Table 7 demonstrates that increased α Values
can both improVe or degrade the fairness performance together with similar classification accuracies.
HoweVer, the resulted fairness performances are obserVed to be consistently better than our baseline,
GRACE. Furthermore, the results of Table 8 show that fairness performances for the link prediction
task can generally be improVed with increased α Values, and the performance is quite stable oVer
different α selections.
Table 7: SensitiVity analysis for α on Pokec networks for FairAug.
α	Pokec-z			Pokec-n		
	Accuracy (%)	△sp (%)	△ eo (%)	Accuracy (%)	△sp (%)	△EO (%)
0.0/0.1	67.04 ± 0.69	3.29 ± 1.66	3.04 ± 1.37	67.88 ± 0.45	4.81 ± 0.59	6.52 ± 0.99
0.1/0.2	67.90 ± 0.69	3.95 ± 2.09	4.20 ± 2.41	67.60 ± 0.48	4.38 ± 0.45	5.82 ± 1.44
0.2/0.3	67.68 ± 0.74	4.42 ± 1.48	4.46 ± 2.25	68.01 ± 0.36	4.95 ± 0.39	6.44 ± 1.36
0.3/0.4	67.71 ± 0.64	2.99 ± 1.89	3.20 ± 2.57	67.50 ± 0.11	3.38 ± 0.90	4.41 ± 1.12
19
Under review as a conference paper at ICLR 2022
Table 8: Sensitivity analysis for α on Facebook networks for FairAug
α	UCSD34			Berkeley13		
	AUC (%)	δSP (%)	δeo (%)	AUC (%)	δsp (%)	∆EO (%)
0.0/0.1	71.46 ± 0.30	0.71 ± 0.38	1.62 ± 0.62	69.48 ± 0.29	0.70 ± 0.43	4.24 ± 0.90
0.1/0.2	71.54 ± 0.33	0.65 ± 0.13	1.76 ± 0.38	69.50 ± 0.42	0.55 ± 0.22	4.31 ± 0.76
0.2/0.3	71.51 ± 0.30	0.58 ± 0.26	1.50 ± 0.66	69.57 ± 0.46	0.55 ± 0.35	4.15 ± 0.79
0.3/0.4	71.55 ± 0.41	0.49 ± 0.14	1.65 ± 0.50	69.41 ± 0.35	0.97 ± 0.19	4.47 ± 1.10
The sensitivity analysis for π is carried out by examining the values
[0.75, 0.80, 0.85, 0.90, 0.95, 1.00] for FairAug algorithm for both views. The sensitivity anal-
yses for Pokec and Facebook networks on FairAug algorithm are presented in Tables 9 and 10,
respectively. In Tables 9 and 10, π = 1 is the parameter choice utilized to generate the results in
Table 2 and π = 0.8 is the selection for the results in Table 3.
Table 9: Sensitivity analysis for π on Pokec networks for FairAug.
π	Pokec-z			Pokec-n		
	Accuracy (%)	δSP (%)	δEO (%)	Accuracy (%)	δSP (%)	∆EO(%)
1.00	67.04 ± 0.69	3.29 ± 1.66	3.04 ± 1.37	67.88 ± 0.45	4.81 ± 0.59	6.52 ± 0.99
0.95	67.66 ± 0.57	4.00 ± 1.76	4.37 ± 1.82	68.01 ± 0.53	5.66 ± 1.27	7.00 ± 0.72
0.90	67.14 ± 0.73	3.28 ± 1.97	3.22 ± 2.20	67.81 ± 0.33	5.22 ± 0.82	6.87 ± 1.22
0.85	66.96 ± 0.77	3.49 ± 2.28	3.52 ± 2.15	68.00 ± 0.29	5.66 ± 0.29	7.77 ± 1.01
0.80	67.87 ± 0.35	3.59 ± 2.33	4.30 ± 1.95	68.17 ± 0.28	5.61 ± 0.73	7.82 ± 1.24
0.75	67.74 ± 0.28	3.94 ± 2.48	4.58 ± 1.91	67.95 ± 0.10	5.69 ± 0.27	7.44 ± 1.08
Table 10: Sensitivity analysis for ∏ on Facebook networks for FairAug
π	UCSD34			Berkeley13		
	AUC (%)	δsp (%)	δeo (%)	AUC (%)	δsp (%)	∆EO (%)
0.80	71.46 ± 0.30	0.71 ± 0.38	1.62 ± 0.62	69.48 ± 0.29	0.70 ± 0.43	4.24 ± 0.90
1.00	71.30 ± 0.41	0.56 ± 0.47	1.41 ± 0.57	69.70 ± 0.32	0.74 ± 0.36	4.35 ± 1.11
0.95	71.37 ± 0.41	0.60 ± 0.30	1.48 ± 0.69	69.52 ± 0.42	0.66 ± 0.16	3.99 ± 0.84
0.90	71.40 ± 0.38	0.59 ± 0.34	1.45 ± 0.44	69.48 ± 0.31	0.86 ± 0.72	4.64 ± 1.37
0.85	71.39 ± 0.36	0.65 ± 0.28	1.38 ± 0.54	69.49 ± 0.41	0.70 ± 0.40	4.19 ± 0.84
0.75	71.47 ± 0.23	0.63 ± 0.37	1.55 ± 0.65	69.52 ± 0.43	0.74 ± 0.48	4.31 ± 1.19
The sensitivity analysis for π on Pokec networks shows that π = 1 results in the best performances
in terms of fairness. However, we note that the fairness results for the remaining π values also
outperform our baseline, GRACE, together with similar node classification accuracies. Furthermore,
the results of Table 10 demonstrate that the presented results for link prediction in Table 3 can
indeed be improved with a grid search, as better fairness performances can be obtained with π 6=
0.8. Moreover, the results for different π values vary less for link prediction compared to node
classification.
A.8 EFFECTS OF PROPOSED AUGMENTATIONS ON γ1 AND γ2
Table 11: Effects of proposed augmentations on γ2
	Original	Node Sampling	Edge Deletion	Edge Addition
Pokec-z	0.90	0.59	0.87	0.77
Pokec-n	0.91	0.62	0.89	0.81
UCSD34	0.18	0.45	0.03	0.11
Berkeley13	0.18	0.43	0.03	0.06
Tables 11 and 12 present the effects of the proposed framework on γ values for different datasets. In
Table 11, the effect of each augmentation step on γ2 is considered independently. Table 12 demon-
strates the effect of augmentations sequentially in the overall algorithm in a cumulative manner
(e.g., the “Edge Deletion” column implies both Node Sampling and Edge Deletion are applied).
Both Tables 11 and 12 demonstrate that even though the edge deletion/addition schemes are not
20
Under review as a conference paper at ICLR 2022
Table 12: Effects of each step in FairAug
		Original Graph	Node Sampling	Edge Deletion	Edge Addition
Pokec-z	γ1	0.66	0.11	0.11	0.11
	γ2	0.90	0.59	0.50	0.43
Pokec-n	γ1	0.67	0.15	0.15	0.15
	γ2	0.91	0.62	0.55	0.50
UCSD34	γ1	0.91	0.86	0.86	0.86
	γ2	0.18	0.45	0.17	0.17
Berkeley13	γ1	0.90	0.83	0.83	0.83
	γ2	0.18	0.43	0.17	0.17
directly designed based on γ2, the proposed approaches indeed reduce the values of it. In addition,
presented γ values can also help to explain the ineffectiveness of edge deletion on Pokec networks.
Due to the inter-edge sparsity and overly unbalanced structure of these datasets in terms of |Eχ∣
and |ET (presented in Table 5 of Appendix A.5), node sampling becomes a better choice than
edge deletion/addition to reduce the bias via an augmentation on the graph topology. Table 11 also
shows the better effectiveness of node sampling strategy in reducing γ2 compared to edge dele-
tion/addition frameworks, which also supports the findings of Table 2, that is, node sampling is an
essential step of FairAug on the Pokec networks. Furthermore, γ values can also help to explain the
random/detrimental effect of node sampling for Facebook networks, which is presented in Table 3.
Tables 11 and 12 show that while node sampling is not very effective in reducing γ1, itis observed to
increase γ2 for these networks. This effect results from the graph topologies of Facebook networks
having |Sχ∣ >> |Sω|. Note that γι values are not 0 after node sampling due to the limit on the
minimum of sampling budgets, see Appendix A.7.
A.9 Case Studies for the Effects of Proposed Augmentations
To demonstrate the effects of the proposed augmentation schemes in a more intuitive manner on
different graph structures, we consider two small toy examples. The topology in Figure 1ais inspired
by the topology of the Pokec networks (few inter-edges, many more intra-edges, i.e., |ET > |Eχ |),
whereas Figure 1b is motivated by the Facebook networks (few nodes with no inter-edges, many
more that have at least one, i.e., |Sχ∣ > |Sω|). For demonstrative purposes, assume the nodes in
both graphs share the same nodal features, with the corresponding feature matrix X equal to
	-0.0	0.1	-0.3	0.1	-0.1
	0.2	-0.2	-0.2	0.1	0.1
	0.1	0.2	0.0	0.2	0.0
γ 一	0.2	0.1	-0.2	0.1	0.2
X=	0.1	-0.1	-0.1	0.1	-0.2
	-0.1	-0.1	0.3	-0.2	0.3
	-0.2	0.1	0.4	-0.1	-0.1
	-0.3	-0.1	0.1	-0.3	-0.2
Feature masking: For the given X, δ utilized in adaptive feature masking equals to
[0.74, 0.12, 1.00, 0.74, 0.00]. Therefore, in a feature masking scheme where 40% of the features
are masked, the most probable augmented X becomes2
■0.0	0.2	0.0	0.0	-0.1
0.0	0.1	0.0	0.1	-0.1
0.0	-0.2	0.0	0.1	0.1
0.0	0.2	0.0	0.2	0.0
0.0	0.1	0.0	0.1	0.2
0.0	-0.1	0.0	0.1	-0.2
0.0	-0.1	0.0	-0.2	0.3
0.0	0.1	0.0	-0.1	-0.1
0.0	-0.1	0.0	-0.3	-0.2
2 Note that the feature masking process is stochastic. We present the mask with the largest probability of
occurrence, for demonstrative purposes.
21
Under review as a conference paper at ICLR 2022
(a) Case one.
S= S = O	— intra-edges
.S=1	---- inter-edges
(b) Case two.
Figure 1: Toy examples
Figure 2: Augmented graphs via node sampling
For XX,	(μι -	μu) in equation 15 becomes [0, -0.05,0, -0.32,0] where	it was
[-0.32, -0.05, 0.43, -0.32, 0] for the original graph. Note that	this	is the best case	scenario
given the 40% masking budget, with the maximal decrease in ∣∣μι - μokι. As also mentioned
in Footnote 2, the	actual feature masking process is stochastic.	By	assigning larger	masking
probabilities to the	features with high deviations betweeen S0 and	S1 ,	the non-uniform	masking
strategy proposed herein increases the chances of obtaining the “better” cases with larger reductions
in (μι - μo).
Node sampling: The results in Table 3 demonstrate that node sampling becomes effective in graphs
where |ST > |Sχ∣. Here, We visualize this finding through the toy examples presented in Figures
1a and 1b. Note that the two toy graphs differ in their sets Sω and Sχ . Specifically, we have
|ST > |Sχ∣ forFigure 1aand |ST < |Sχ∣ for Figure 1b.
Here, the exact ∣e∣ι = ∣∣a Pv九∈si Zn - % PvE∈so Zm∣ι values are calculated before and
after node sampling for two different graph examples Gi and G2. Note that Zi = J- Pv ∈n,)Xj,
and the original nodal features X are utilized to calculate the zi vectors.
Case 1:	In the first example graph Gi, |SJ| = 2, |Sq | = 3, ∣SX∣ = 1, and |Sf| = 2. Therefore,
the proposed node sampling scheme selects 2 nodes from the set S0ω, and 1 node from the set S1ω. A
possible augmented graph is presented in Figure 2a on which the examination is carried over. While
the original ∣∣i equals to 0.865 for Gi, ∣∣i becomes 0.771 for the output topology of the node
sampling provided in Figure 2a, which shows the efficacy of the proposed framework.
Case 2:	For G2, |Sj| = 4, |Sq | = 1, ∣Sχ∣ = 2, and |Sf| = 1. Therefore, the proposed node
sampling scheme selects 2 nodes from the set S0χ (minimum limit for each set is the 50% of the
22
Under review as a conference paper at ICLR 2022
Figure 3: Augmented graphs via edge deletion
initial group size), and 1 node from the set S1χ . Therefore, the presented example in Figure 2b is a
potential augmented topology obtained after the proposed node sampling framework. For this case,
the original kk1 = 0.558 which becomes 0.892 for the augmented graph presented in Figure 2b.
This case study clearly illustrates that the proposed node sampling scheme is mainly effective for
graphs where |ST > |Sχ∣.
Edge deletion: Results in Table 2 show that node sampling is a better choice than edge manipu-
lations for graphs with inter-edge sparsity and |Eω|》|Eχ∣. Again, two case studies based on the
graphs provided in Figure 1 are utilized to demonstrate the effects of the proposed adaptive edge
deletion scheme. Specifically, the resulted ∣∣6kι = ∣∣高 Pvn区 Zn 一 尚 PvmESo Zmkι value is
examined before and after the application of edge deletion.
Case 1: In the first example graph Gι, |EX | = 2, ∣ESo | = 6, and ∣E511 = 3. Therefore, edge deletion
probability of the adaptive edge deletion framework is 0.5 for the edges in ESω , and ESω with the
hyperparameter selections ∏ = 1, and Pmin = 2 = 0.5. For such a scheme, a possible augmented
graph is presented in Figure 3a for demonstrative purposes. While the value of ∣∣1 is reduced from
0.865 to 0.792 with the application of adaptive edge deletion, the graph in Figure 3a also becomes
disconnected. For certain real networks, such as the Pokec networks considered in this paper, the
imbalance between intra- and inter-edges is considerably larger than the imbalance in the graph in
Figure 1a. Therefore, the sparsity resulting from the adaptive edge deletion can be even more severe
for such graphs than this toy example. Thus, this case study also confirms that node sampling can
indeed be a better choice than edge deletion for graphs with inter-edge sparsity and |Eω | >> |Eχ∣.
Case2: For G2, we have |E χ∣ = 4, |ESOI =4,and |E§J = 2, which shows amorebalanced structure
than the graph utilized in Case 1. For the given graph connectivity, edge deletion probability of
adaptive edge deletion scheme is 0.5 for the edges in ESω , and 0 for the edges in ESω together with
the hyperparameter selections ∏ = 1, and Pmin = 2 = 0.5. The presented example in Figure 3b
is a potential resulting augmented topology obtained with the proposed edge deletion framework,
presented for demonstrative purposes. For this outcome, the original ∣∣1 is decreased from 0.558
to 0.497 for the augmented graph presented in Figure 3b, which demonstrates the effectiveness of
the proposed augmentation for the input graph in Case 2.
Edge Addition: We also present the effects of the proposed edge addition scheme on the toy ex-
amples for a better illustration of the augmentation. Again, the exact ∣e∣ι = ∣∣ 舟 Pv九3Zn -
⅛T Evm ∈S0 Zm∣1 values are calculated before and after edge addition for two different graph ex-
amples G1 and G2.
Case 1:	For Gι, as ∣ET = 9 and ∣Eχ∣ = 2, new seven artificial inter-edges are generated for the
proposed edge addition scheme. For such a scheme, a possible augmented graph is presented in
Figure 4a, where ∣∣1 is reduced from 0.865 to 0.111 with the application of edge addition.
Case 2:	For G2, as ∣ET = 6 and ∣EX ∣ = 4, therefore two artificial inter-edges should be created for
the proposed edge addition scheme. A possible augmented graph resulting from the proposed edge
23
Under review as a conference paper at ICLR 2022
Figure 4: Augmented graphs via edge addition
addition is presented in Figure 4b. For the presented example in Figure 4b, edge addition can lower
the value of kk1 from 0.558 to 0.293.
A.10 OPTIMAL TOPOLOGY AUGMENTATION SCHEMES FOR γ1 AND γ2
While Algorithm 1 can make γι = 0 for graphs satisfying |ST ≥ |Sχ∣, as Table 12 demonstrates,
the obtained γ1 values after node sampling are not exactly zero due to the limit on the number of
nodes that can be sampled. If |Sχ∣ ≥ |Sω|, the minimum number of nodes that will be sampled
for each set is 50% of the initial group size, otherwise the limit is 25% of the initial group size.
Such limits are employed to prevent the generation of excessively small subgraphs, which can pre-
vent proper learning and create stability issues. Furthermore, proposed global edge deletion/addition
frameworks cannot directly make γ2 zero, as the reduction of γ2 to zero requires a node-wise consid-
eration. Since such an independent consideration of each node can incur a significant computational
complexity for large graphs, we stay within the realm of global schemes that can still help to reduce
the value of γ2 . However, the examination of the optimal schemes that can make γ1 γ2 zero helps
provide important insights and justification for the proposed augmentations.
Motivated by this, we examined “optimal” strategies which can drive γ1 or γ2 to zero. The corre-
sponding node sampling strategy copes with each node independently by deleting/adding edges so
that diω = diχ , ∀vi ∈ V . Different from the proposed global edge manipulation scheme in Section
3.3.3, such node-level edge deletion/addition frameworks can change the value ofγ1 and henceforth
interfere with the effects of the node sampling step. Additionally, the employment of edge deletion
for the graph makes the utilization of edge addition unnecessary and vice versa, as both schemes
seek to achieve the same goal, i.e., diω = diχ , ∀vi ∈ V. Therefore, only the “optimal” edge dele-
tion or addition scheme is employed. We would like to note that the investigation is carried out
for Pokec networks, as |ST ≤ |Sχ | for Facebook networks, which makes the design of an optimal
node sampling scheme challenging due to the unpredictable increase in Sω and the unpredictable
decrease in Sχ , when sampling from the set Sχ (some of the excluded nodes and edges may be the
only inter-edges for the nodes in the sampled graph, resulting in a further increase in Sω and further
decrease in Sχ).
Table 13:	Effects of optimal augmentations on γι
Original Node Sampling Edge Deletion Edge Addition
Pokec-z	0.66	0.00	0.67	1.00
Pokec-n	0.67	0.00	0.67	1.00
Table 14:	Effects of optimal augmentations on γ2
Original Node Sampling Edge Deletion Edge Addition
Pokec-z	0.90	0.48	0.00	0.00
Pokec-n	0.91	0.46	0.00	0.00
24
Under review as a conference paper at ICLR 2022
Table 15: Effects of each step in Node Sampling + Edge Deletion
	Original Graph	Node Sampling	Edge Deletion
Pokec-z γ1	0.66	0.00	0.08
γ2	0.90	0.48	0.00
Pokec-n γ1	0.67	0.00	0.11
γ2	0.91	0.46	0.00
Table 16: Effects of each step in Node Sampling + Edge Addition
	Original Graph	Node Sampling	Edge Addition
Pokec-z γ1	0.66	0.00	0.73
γ2	0.90	0.48	0.00
Pokec-n γ1	0.67	0.00	0.70
γ2	0.91	0.46	0.00
Tables 13-16 demonstrate that the utilized node sampling, edge deletion/addition frameworks are
optimal in the sense that they reduce γ1 and γ2 to zero, respectively. However, presented γ1 and
γ2 values also show the interference of optimal edge manipulations on γ1 , as γ1 increases after the
application of edge deletion/addition following node sampling. Specifically, edge addition has a
great impact on it.
25