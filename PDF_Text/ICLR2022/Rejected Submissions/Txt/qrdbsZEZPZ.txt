Under review as a conference paper at ICLR 2022
Certified Robustness for Free in Differen-
tially Private Federated Learning
Anonymous authors
Paper under double-blind review
Ab stract
Federated learning (FL) provides an efficient training paradigm to jointly train a
global model leveraging data from distributed users. As the local training data
comes from different users who may not be trustworthy, several studies have shown
that FL is vulnerable to poisoning attacks where adversaries add malicious data
during training. On the other hand, to protect the privacy of users, FL is usually
trained in a differentially private manner (DPFL). Given these properties of FL, in
this paper, we aim to ask: Can we leverage the innate privacy property of DPFL to
provide robustness certification against poisoning attacks? Can we further improve
the privacy of FL to improve such certification? To this end, we first investigate
both user-level and instance-level privacy for FL, and propose novel randomization
mechanisms and analysis to achieve improved differential privacy. We then provide
two robustness certification criteria: certified prediction and certified attack cost
for DPFL on both levels. Theoretically, given different privacy properties of DPFL,
we prove their certified robustness under a bounded number of adversarial users or
instances. Empirically, we conduct extensive experiments to verify our theories
under different attacks on a range of datasets. We show that the global model
with a tighter privacy guarantee always provides stronger robustness certification
in terms of the certified attack cost, while it may exhibit tradeoffs regarding the
certified prediction. We believe our work will inspire future research of developing
certifiably robust DPFL based on its inherent properties.
1 Introduction
Federated Learning (FL), which aims to jointly train a global model with distributed local data,
has been widely applied in different applications, such as finance (Yang et al., 2019b), medical
analysis (Brisimi et al., 2018), and user behavior prediction (Hard et al., 2018; Yang et al., 2018;
2019a). However, the fact that the local data and the training process are entirely controlled by the
local users who may be adversarial raises great concerns from both security and privacy perspectives.
In particular, recent studies show that FL is vulnerable to different types of training-time attacks,
such as model poisoning (Bhagoji et al., 2019), backdoor attacks (Bagdasaryan et al., 2020; Xie et al.,
2019; Wang et al., 2020), and label-flipping attacks (Fung et al., 2020). Further, privacy concerns
have motivated the need to keep the raw data on local devices without sharing. However, sharing
other indirect information such as gradients or model updates as part of the FL training process can
also leak sensitive user information (Zhu et al., 2019; Geiping et al., 2020; Bhowmick et al., 2018;
Melis et al., 2019). As a result, approaches based on differential privacy (DP) (Dwork & Roth, 2014),
homomorphic encryption (Bost et al., 2015; Rouhani et al., 2018; Gilad-Bachrach et al., 2016), and
secure multiparty computation (Ben-Or et al., 1988; Bonawitz et al., 2017) have been proposed to
protect privacy of users in federated learning. In particular, differentially private federated learning
(DPFL) provides strong information theoretic guarantees on user privacy, while causing relatively
low performance overhead (Li et al., 2020b).
Several defenses have been proposed to defend against poisoning attacks in FL. For instance, various
robust aggregation methods (Fung et al., 2020; Pillutla et al., 2019; Blanchard et al., 2017; El Mhamdi
et al., 2018; Chen et al., 2017b; Yin et al., 2018; Fu et al., 2019; Li et al., 2020a) identify and
down-weight the malicious updates during aggregation or estimate a true “center” of the received
updates rather than taking a weighted average. Other methods include robust federated training
protocols (e.g., clipping (Sun et al., 2019), noisy perturbation (Sun et al., 2019), and additional
1
Under review as a conference paper at ICLR 2022
evaluation during training (Andreina et al., 2020)) and post-training strategies (e.g., fine-tuning and
pruning (Wu et al., 2020)) that repair the poisoned global model. However, as these works mainly
focus on providing empirical robustness for FL, they have been shown to be vulnerable to newly
proposed strong adaptive attacks (Wang et al., 2020; Xie et al., 2019; Baruch et al., 2019; Fang
et al., 2020). Hence, in this paper, we aim to develop certified robustness guarantees for FL against
different poisoning attacks. Further, as differentially private federated learning (DPFL) is often used
to protect user privacy, we also aim to ask: Can we leverage the innate privacy property of DPFL
to provide robustness certification against poisoning attacks for free? Can we further improve the
privacy of FL so as to improve its certified robustness?
Recent studies suggest that differential privacy (DP) is inherently related with robustness of ML
models. Intuitively, DP is designed to protect the privacy of individual data, such that the output of
an algorithm remains essentially unchanged when one individual input point is modified. Hence,
the prediction of a DP model will be less impacted by a small amount of poisoned training data.
Consequently, DP has been used to provide both theoretical and empirical defenses against evasion
attacks (Lecuyer et al., 2019a) and data poisoning attacks (Ma et al., 2019; Hong et al., 2020) on
centralized ML models. It has also been used as an empirical defense against backdoor attacks (Gu
et al., 2019) in federated learning (Bagdasaryan et al., 2020; Sun et al., 2019), although no theoretical
guarantee is provided. To the best of our knowledge, despite of the wide application of DPFL,there is
no work providing certified robustness for DPFL leveraging its privacy property.
In this paper, we aim to leverage the inherent privacy property of DPFL to provide robustness
certification for FL against poisoning attacks for free. Our challenges include: (1) performing
privacy analysis over training rounds in DPFL algorithms and (2) theoretically guaranteeing certified
robustness based on DP properties under a given privacy budget. We propose two robustness
certification criteria for FL: certified prediction and certified attack cost under different attack
constraints. We consider both user-level DP (Agarwal et al., 2018; Geyer et al., 2017; McMahan
et al., 2018; Asoodeh & Calmon, 2020; Liang et al., 2020) which is widely guaranteed in FL, and
instance-level DP (Malekzadeh et al., 2021; Zhu et al., 2021) which is less explored in FL. We prove
that a FL model satisfying user-level DP is certifiably robust against a bounded number of adversarial
users. In addition, we propose InsDP-FedAvg algorithm to improve instance-level DP in FL,
and prove that instance-level DPFL is certifiably robust against a bounded number of adversarial
instances. We also study the correlation between privacy guarantee and certified robustness of FL.
While stronger privacy guarantees result in greater attack cost, overly strong privacy can hurt the
certified prediction by introducing too much noise in the training process. Thus, the optimal certified
prediction is often achieved under a proper balance between privacy protection and utility loss.
Key Contributions. Our work takes the first step to provide certified robustness in DPFL for free
against poisoning attacks. We make contributions on both theoretical and empirical fronts.
•	We propose two criteria for certified robustness of FL against poisoning attacks (Section 4.2).
•	Given a FL model satisfying user-level DP, we prove that it is certifiably robust against arbitrary
poisoning attacks with a bounded number of adversarial users (Section 4.2).
•	We propose InsDP-FedAvg algorithm to improve FL instance-level privacy guarantee (Sec-
tion 5.1). We prove that instance-level DPFL is certifiably robust against the manipulation of a
bounded number of instances during training (Section 5.2).
•	We conduct extensive experiments on image classification of MNIST, CIFAR-10 and sentiment
analysis of tweets to verify our proposed certifications of two robustness criteria, and compare the
certified results of different DPFL algorithms (Section 6).
2	Related work
Differentially Private Federated Learning. Different approaches are proposed to guarantee the
user-level privacy for FL. (Geyer et al., 2017; McMahan et al., 2018) clip the norm of each local
update, add Gaussian noise on the summed update, and characterize its privacy budget via moment
accountant (Abadi et al., 2016). (McMahan et al., 2018) extends (Geyer et al., 2017) to language
models. In CpSGD (Agarwal et al., 2018), each user clips and quantizes the model update, and
adds noise drawn from Binomial distribution, achieving both communication efficiency and DP.
(Bhowmick et al., 2018) derive DP for FL via Renyi divergence (Mironov, 2017) and study its
protection against data reconstruction attacks. (Liang et al., 2020) utilizes Laplacian smoothing for
each local update to enhance the model utility. Instead of using moment accountant to track privacy
2
Under review as a conference paper at ICLR 2022
budget over FL rounds as previous work, (Asoodeh & Calmon, 2020) derives the DP parameters by
interpreting each round as a Markov kernel and quantify its impact on privacy parameters. All these
works only focus on providing user-level privacy, leaving its robustness property unexplored.
In terms of instance-level privacy for FL, there are only a few work (Malekzadeh et al., 2021; Zhu
et al., 2021). Dopamine (Malekzadeh et al., 2021) provides instance-level privacy guarantee when
each user only performs one step of DP-SGD (Abadi et al., 2016) at each FL round. However, it
cannot be applied to multi-step SGD for each user, thus it cannot be extended to the general FL
setting FedAvg (McMahan et al., 2017). (Zhu et al., 2021) privately aggregate the labels from users
in a voting scheme, and provide DP guarantees on both user level and instance level. However, it is
also not applicable to standard FL, since it does not allow aggregating the gradients or updates.
Differential Privacy and Robustness. In standard (centralized) learning, Pixel-DP (Lecuyer et al.,
2019a) is proposed to certify the model robsutness against evasion attacks. However, it is unclear
how to leverage it to certify against poisoning attacks. To certify the robustness against poisoning
attacks, (Ma et al., 2019) show that private learners are resistant to data poisoning and analyze the
lower bound of attack cost against poisoning attacks for regression models. Here we certify the
robustness in DPFL setting with such lower bound as one of our certification criteria and additionally
derive its upper bounds. (Hong et al., 2020) show that the off-the-shelf mechanism DP-SGD (Abadi
et al., 2016), which clips per-sample gradients and add Guassian noises during training, can serve as
a defense against poisoning attacks empirically. In federated learning, empirical work (Bagdasaryan
et al., 2020; Sun et al., 2019) show that DPFL can mitigate backdoor attacks; however, none of these
work provides certified robustness guarantees for DPFL against poisoning attacks.
3	Preliminaries
We start by providing some background on differential privacy (DP) and federated learning (FL).
Differential Privacy (DP). DP is a formal, mathematically rigorous definition (and standard) of
privacy that intuitively guarantees that a randomized algorithm behaves similarly on similar inputs
and that the output of the algorithm is about the same whether or not an individual’s data is included
as part of the input (Dwork & Roth, 2014).
Definition 1 ((, δ)-DP (Dwork et al., 2006)). A randomized mechanism M : D → Θ with domain
D and range Θ satisfies (, δ)-DP if for any pair of two adjacent datasets d, d0 ∈ D, and for any
possible (measurable) output set E ⊆ Θ, it holds that Pr[M(d) ∈ E] ≤ e Pr [M (d0) ∈ E] + δ.
In Definition 1, when M is a training algorithm for ML model, domain D and range Θ represent
all possible training datasets and all possible trained models respectively. Group DP for (, δ)-DP
mechanisms follows immediately from Definition 1 where the privacy guarantee drops with the size
of the group. Formally, it says:
k
Lemma 1 (Group DP). For mechanism M that satisfies (e, δ)-DP, it satisfies (ke, 1-^, δ)-DPfor
groups of size k. That is, for any d, d0 ∈ D that differ by k individuals, and any E ⊆ Θ it holds that
Pr[M(d) ∈ E] ≤ eke Pr[M (d0) ∈ E] + 1-e^δ.
Federated Learning. FedAvg was introduced by (McMahan et al., 2017) for FL to train a shared
global model without direct access to training data of users. Specifically, given a FL system with
N users, at round t, the server sends the current global model wt-1 to users in the selected user set
Ut, where |Ut| = m = qN and q is the user sampling probability. Each selected user i ∈ Ut locally
updates the model for E local epochs with its dataset Di and learning rate η to obtain a new local
model. Then, the user sends the local model updates ∆wti to the server. Finally, the server aggregates
over the updates from all selected users into the new global model Wt = wt-1 + * Pii∈ut ∆Wi.
4	User-level Privacy and Certified Robustness for FL
4.1	User-level Privacy and Background
Definition 1 leaves the definition of adjacent datasets flexible, which depends on applications.
To protect user-level privacy, adjacent datasets are defined as those differing by data from one
user (McMahan et al., 2018). The formal definition of User-level (e, δ)-DP (Definition 2) is omitted
to Appendix A.1.
Following standard DPFL (Geyer et al., 2017; McMahan et al., 2018), we introduce one of standard
user-level DPFL algorithms UserDP-FedAvg (Algorithm 1 in Appendix A.1). At each round,
3
Under review as a conference paper at ICLR 2022
the server first clips the update from each user with a threshold S such that its '2-sensitivity is
upper bounded by S. Next, the server sums up the updates, adds Gaussian noise sampled from
N(0, σ2S2), and takes the average, i.e., Wt J wt-ι + 春(Pi∈ut Clip(∆wi, S) + N (0, σ2S2)).
Given the user sampling probability q, noise level σ, FL rounds T , and a δ > 0, the privacy analysis
of UserDP-FedAvg satisfying (, δ)-DP is given by Proposition 1 in Appendix A.1, which is a
generalization of (Abadi et al., 2016). The aim of Proposition 1 is to analyze privacy budget in FL,
which is accumulated as T increases due to the continuous access to training data. Following (Geyer
et al., 2017; McMahan et al., 2018), moment accountant (Abadi et al., 2016) is used in the privacy
analysis.
4.2	Certified Robustness of User-level DPFL against Poisoning Attacks
Threat Model. We consider the poisoning attacks against FL, where k adversarial users have
poisoned instances in local datasets, aiming to fool the trained DPFL global model. Such attacks
include backdoor attacks (Gu et al., 2019; Chen et al., 2017a) and label flipping attacks (Biggio et al.,
2012; Huang et al., 2011). The detailed description of these attacks is deferred to Appendix A.2. Note
that our robustness certification is attack-agnostic under certain attack constraints (e.g., k), and we
will verify our certification bounds with different poisoning attacks in Section 6. Next, we propose
two criteria for the robustness certification in FL: certified prediction and certified attack cost.
Certified Prediction. Consider the classification task with C classes. We define the classification
scoring function f : (Θ, Rd) → ΥC which maps model parameters θ ∈ Θ and an input data
x ∈ Rd to a confidence vector f(θ, x), and fc(θ, x) ∈ [0, 1] represents the confidence of class
c. We mainly focus on the confidence after normalization, i.e., f(θ, x) ∈ ΥC = {p ∈ RC≥0 :
kpk1 = 1} in the probability simplex. Since the DP mechanism M is randomized and produces
a stochastic FL global model θ = M(D), it is natural to resort to a probabilistic expression as
a bridge for quantitative robustness certifications. Following the convention in (Lecuyer et al.,
2019b; Ma et al., 2019), we use the expectation of the model’s prediction to provide a quantitative
guarantee on the robustness of M. Specifically, we define the expected scoring function F :
(θ, Rd) → ΥC where Fc(M(D), x) = E[fc(M(D), x)] is the expected confidence for class c. The
expectation is taken over DP training randomness, e.g., random Gaussian noise and random user
subsampling. The corresponding prediction H : (θ, Rd) → [C] is defined by H(M(D), x) :=
arg maxc∈[C] Fc(M(D), x), which is the top-1 class based on the expected prediction confidence.
We will prove that such prediction allows robustness certification against poisoning attacks.
Following our threat model above and DPFL training in Algorithm 1, we denote the trained global
model exposed to poisoning attacks by M(D0). When k = 1, D and D0 are user-level adjacent
datasets according to Definition 2. Given that mechanism M satisfies user-level (, δ)-DP, based
on the innate DP property, the distribution of the stochastic model M(D0 ) is “close” to the dis-
tribution of M(D). Moreover, according to the post-processing property of DP, during testing,
given a test sample x, we would expect the values of the expected confidence for each class c, i.e.,
Fc(M(D0), x) and Fc(M(D), x), to be close, and hence the returned most likely class to be the
same, i.e., H(M(D), x) = H (M(D0), x), indicating robust prediction against poisoning attacks.
Theorem 1 (Condition for Certified Prediction under One Adversarial User). Suppose a randomized
mechanism M satisfies user-level (, δ)-DP. For two user sets B and B0 that differ by one user, let
D and D0 be the corresponding training datasets. For a test input x, suppose A, B ∈ [C] satisfy
A = arg maxc∈[c] Fc(M(D), x) and B = argmaxyc]：C=A Fc(M(D), x), then if
FA(M(D), x) > e2FB(M(D), x) + (1 +e)δ,	(1)
it is guaranteed that H (M(D0), x) = H (M(D), x) = A.
When k > 1, we resort to group DP. According to Lemma 1, given mechanism M satisfying user-
1 k
level (e, δ)-DP, it also satisfies user-level (ke, ⅛-⅞r δ)-DP for groups of Size k. When k is smaller
than a certain threshold, leveraging the group DP property, we would expect that the distribution of
the stochastic model M(D0 ) is not too far away from the distribution of M(D) such that they would
make the same prediction for a test sample with probabilistic guarantees. Therefore, the privacy and
robustness guarantees are simultaneously met by M.
Theorem 2 (Upper Bound of k for Certified Prediction). Suppose a randomized mechanism
M satisfies user-level (e, δ)-DP. For two user sets B and B0 that differ by k users, let D and
D0 be the corresponding training datasets. For a test input x, suppose A, B ∈ [C] satisfy
4
Under review as a conference paper at ICLR 2022
A = arg maxc∈[c] Fc(M(D),x) and B = argmaxyc]：C=A Fc(M(D),x), then H(M(D0),x)
H (M(D), x) = A, ∀k < K where K is the certified number of adversarial users:
K = Log FA(M(D),x)(e'- 1)+ δ
2 6	FB(M (D), x)(ee — 1) + δ
(2)
The proofs of Theorems 1 and 2 are omitted to Appendix A.4. Theorems 1 and 2 reflect a tradeoff
between privacy and certified prediction: (i) in Theorem 1, if is large such that the RHS of Eq (1)
> 1, the robustness condition cannot be met since the expected confidence FA(M(D), x) ∈ [0, 1].
However, to achieve small , i.e., strong privacy, large noise is required during training, which
would hurt model utility and thus result in small confidence margin between the top two classes
(e.g., FA(M(D), x) and FB(M(D), x)), making it hard to meet the robustness condition. (ii) In
Theorem 2 ifwe fix FA(M(D), x) and FB(M(D), x), smaller ofFL can certify larger K. However,
smaller also induces smaller confidence margin, thus reducing K instead. As a result, properly
choosing would help to certify a large K.
Certified Attack Cost. In addition to the certified prediction, we define the attack cost for attacker
C : Θ → R which quantifies the difference between the poisoned model and the attack goal. In
general, attacker aims to minimize the expected attack cost J(D) := E[C(M(D))], where the
expectation is taken over the randomness of DP training. The cost function can be instantiated
according to the concrete attack goal in different types of poisoning attacks, and we provide some
examples below. Given a global FL model satisfying user-level (, δ)-DP, we will prove the lower
bound of the attack cost J(D0) when manipulating the data of at most k users. Higher lower bound
of the attack cost indicates more certifiably robust global model.
Example 1. (Backdoor attack (Gu et al., 2019)) C(θ) = ± PZi l(θ, zi), where z； = (Xi + δχ,y*),
δχ is the backdoor pattern, yi is the target adversarial label. Minimizing J (D0) drives the prediction
on any test data with the backdoor pattern δx to the target label yi .
Example 2. (Label Flipping attack (Biggio et al., 2012)) C(θ) = n1 PNi l(θ, Zi), where Zi =
(xi, yi) and yi is the target adversarial label. Minimizing J (D0) drives the prediction on test data
xi to the target label yi .
Example 3. (Parameter-Targeting attack (Ma et al., 2019)) C(θ) = 2∣∣θ 一 θ*∣∣2, where θ? is the
target model. Minimizing J (D0) drives the poisoned model to be close to the target model.
Theorem 3 (Attack Cost with k Attackers). Suppose a randomized mechanism M satisfies user-level
(, δ)-DP. For two user sets B and B0 that differ k users, D and D0 are the corresponding training
datasets. Let J(D) be the expected attack cost where |C(∙)∣ ≤ C. Then,
ek	1	1	e-k
min{ekJ(D) +----------δC, C} ≥ J(D0) ≥ max{e-kJ(D)-------------δC, 0}, if C(∙) ≥ 0
e - 1	e - 1	(3)
1 e-k	ek	1	()
min{e-keJ(D) + -J-~-δC, 0} ≥ J(D0) ≥ max{ek0 J(D) - -^---δC, -C}, if C(∙) ≤ 0
The proof is omitted to Appendix A.4. Theorem 3 provides the upper bounds and lower bounds
for attack cost J(D0). The lower bounds show that to what extent the attack can reduce J(D0) by
manipulating up to k users, i.e., how successful the attack can be. The lower bounds depend on the
attack cost on clean model J(D), k and . When J(D) is higher, the DPFL model under poisoning
attacks is more robust because the lower bounds are accordingly higher; a tighter privacy guarantee,
i.e., smaller , can also lead to higher robustness certification as it increases the lower bounds; with
larger k, the attacker ability grows and thus lead to lower possible J (D0). The upper bounds show
the least adversarial effect brought by k attackers, i.e., how vulnerable the DPFL model is under the
optimistic case (e.g., the backdoor pattern is less distinguishable).
Leveraging the lower bounds in Theorem 3, we can lower-bound the minimum number of attackers
required to reduce the attack cost to certain level associated with hyperparameter τ in Corollary 1.
Corollary 1 (Lower Bound of k Given τ). Suppose a randomized mechanism M satisfies user-
level (e, δ)-DP Let attack cost function be C, the expected attack cost be J (∙). In order to achieve
J(Dr) ≤ T J(D) for τ ≥ 1 when 0 ≤ C(∙) ≤ C, or achieve J(D0) ≤ TJ(D) for 1 ≤ T ≤ 一 J(D)
when -C ≤ C (∙) ≤ 0, the number of adversarial users should satisfy:
k ≥ 1log" - 1J(D)T + Cδτ or k ≥ 1log Iee - I)J(D)T-CS respectively. (4)
≥ 6 g 9 - 1) J(D) + Cδτ	≥ 6 g (e - 1) J(D) - Cδ	p y ()
The proof is omitted to Appendix A.4. Corollary 1 shows that stronger privacy guarantee (i.e., smaller
) requires more attackers to achieve the same effectiveness of attack, indicating higher robustness.
5
Under review as a conference paper at ICLR 2022
5 Instance-level Privacy and Certified Robustness for FL
5.1	Instance-level Privacy
In this section, we introduce the instance-level DP definition, the corresponding algorithm, and
the privacy analysis for FL. When DP is used to protect the privacy of individual instance, the
trained stochastic FL model should not differ much if one instance is modified. Hence, the adjacent
datasets in instance-level DP are defined as those differing by one instance. The formal definition of
Instance-level (, δ)-DP (Definition 3) is omitted to Appendix A.1.
Dopamine (Malekzadeh et al., 2021) provides the first instance-level privacy guarantee under
FedSGD (McMahan et al., 2017). However, it has two limitations. First, its privacy bound is
loose. Although FedSGD performs both user and batch sampling during training, Dopamine ignores
the privacy gain provided by random user sampling. In this section, we improve the privacy guarantee
under FedSGD with privacy amplification via user sampling (Bassily et al., 2014; Abadi et al., 2016).
This improvement leads to algorithm InsDP-FedSGD, to achieve tighter privacy analysis. We defer
the algorithm (Algorithm 2) as well as its privacy guarantee to Appendix A.1.
Besides the loose privacy bound, Dopamine (Malekzadeh et al., 2021) only allows users to perform
one step of DP-SGD (Abadi et al., 2016) during each FL round. This restriction limits the efficiency of
the algorithm and increases the communication overhead. In practice, users in FL are typically allowed
to update their local models for many steps before submitting updates to reduce the communication
cost. To solve this problem, we further improve InsDP-FedSGD to support multiple local steps
during each round. Specifically, we propose a novel instance-level DPFL algorithm InsDP-FedAvg
(Algorithm 3 in Appendix A.1) allowing users to train multiple local SGD steps before submitting
the updates. In InsDP-FedAvg, each user i performs local DP-SGD so that the local training
mechanism Mi satisfies instance-level DP. Then, the server aggregates the updates. We prove that the
global mechanism M preserves instance-level DP using DP parallel composition theorem (Dwork &
Lei, 2009) and moment accountant (Abadi et al., 2016).
Algorithm 3 formally presents the InsDP-FedAvg algorithm and the calculation of its privacy
budget . Specifically, at first, local privacy cost i0 is initialized as 0 before FL training. At round
t, if user i is not selected, its local privacy cost is kept unchanged e； J e；-I. Otherwise user i
updates local model by running DP-SGD for V local steps with batch sampling probability p, noise
level σ and clipping threshold S, and eit is accumulated upon eit-1 via its local moment accountant.
Next, the server aggregates the updates from selected users, and leverages {eit}i∈[N] and the parallel
composition in Theorem 4 to calculate the global privacy cost et . After T rounds, the mechanism M
that outputs the FL global model in Algorithm 3 is instance-level (eT , δ)-DP.
Theorem 4 (InsDP-FedAvg Privacy Guarantee). In Algorithm 3, during round t, if the local
mechanism Mi satisfies (eit, δ)-DP, then the global mechanism M satisfies maxi∈[N] eit, δ -DP.
The idea behind Theorem 4 is that when D0 and D differ in one instance, the modified instance
only falls into one local dataset, and thus parallel composition theorem (Dwork & Lei, 2009) can
be applied. Then the privacy guarantee corresponds to the worst-case, and is obtained by taking the
maximum local privacy cost across all the users. The detailed proof is given in Appendix A.1.
5.2	Certified Robustness of Instance-level DPFL against Poisoning Attacks
Threat Model. We consider poisoning attacks under the presence of k poisoned instances. These
instances could be controlled by the same or multiple adversarial users. Our robustness certification
is agnostic to the attack methods as long as the number of poisoned instances is constrained.
According to the group DP property (Lemma 1) and the post-processing property for FL model with
instance-level (e, δ)-DP, we prove that our robust certification results proposed for user-level DP are
also applicable to instance-level DP. Below is the formal theorem (proof is given in Appendix A.4).
Theorem 5. Suppose D and D0 differ by k instances, and the randomized mechanism M satisfies
instance-level (e, δ)-DP. The results in Theorems 1, 2,and 3, and Corollary 1 hold for M, D, and D0.
Comparison with existing certified prediction methods in centralized setting. The form of The-
orem 1 is similar with the robustness condition against test-time attack in Proposition 1 of (Lecuyer
et al., 2019a). This is because the derived robustness conditions are both rooted in the DP properties,
but ours focus on the robustness against training-time attacks in FL, which is more challenging
6
Under review as a conference paper at ICLR 2022
considering the distributed nature and the model training dynamics, i.e., the analysis of the privacy
budget over training rounds. Our Theorem 1 is also different from previous randomized smoothing-
based certifiably robust centralized learning against backdoor (Weber et al., 2020) and label flipping
(Rosenfeld et al., 2020). First, our randomness comes from the inherent training randomness of
user/instance-level (, δ)-DP, e.g., user subsampling and Gaussian noise. Thus, the certified robust-
ness for free in DPFL means that the DPFL learning algorithm M itself is randomized, and such
randomness can lead to the robustness certification with non-trivial quantitative measurement of
the randomness. On the contrary, robustness in randomized smoothing-based methods comes from
explicitly making the classification process randomized via adding noise in training datasets (Weber
et al., 2020; Rosenfeld et al., 2020), or test samples (Lecuyer et al., 2019a; Cohen et al., 2019) which
is easier to measure. Second, our Theorem 1, 2 hold no matter how is achieved, which means that
we can add different types of noise, leverage different subsampling strategies or even different FL
training protocols to achieve user/instance-level . However, in (Weber et al., 2020; Rosenfeld et al.,
2020) different certifications require different types of noise (Laplacian, Gaussian, etc.). Additionally,
DP is suitable to characterize the robustness against poisoning since DP composition theorems can
be leveraged to track privacy cost , which captures the training dynamics of ML model parameters
without additional assumptions. Otherwise one may need to track the deviations of model parameters
by analyzing SGD over training, which is theoretically knotty and often requires strong assumptions
on Lipschitz continuity, smoothness or convexity for the trained models.
6	Experiments
We present evaluations for robustness certifications, expecially Thm. 2, 3 and Cor. 1. We find that 1)
there is a tradeoff between certified prediction and privacy on certain datasets; 2) a tighter privacy
guarantee always provides stronger certified robustness in terms of the certified attack cost; 3) our
lower bounds of certified attack cost are generally tight when k is small. When k is large, they are
tight under strong attacks (e.g., large local poisoning ratio α). Stronger attacks or tighter certification
are requried to further tighten the gap between the emprical robustness and theoretical bounds.
Data and Model. We evaluate our robustness certification results with three datasets: image class-
fication on MNIST, CIFAR-10 and text sentiment analysis task on tweets from Sentiment140 (Go
et al.) (Sent140), which involves classifying Twitter posts as positive or negative. For image datasets,
we use corresponding standard CNN architectures in the differential privacy library (opa, 2021) of
PyTorch; for Sent140, we use a LSTM classifier. Following previous work on DP ML (Jagielski
et al., 2020; Ma et al., 2019) and backdoor attacks (Tran et al., 2018; Weber et al., 2020) which
evaluate with two classes, we focus on binary classification for MNIST (digit 0 and 1) and CIFAR-10
(airplane and bird), and defer the 10-class results to Appendix A.3. We train FL model following
Algorithm 1 for user-level privacy and Algorithm 3 for instance-level privacy. We refer the readers to
Appendix A.3 for details about the datasets, networks, parameter setups.
Poisoning Attacks. We evaluate several state-of-the-art poisoning attacks against the proposed
UserDP-FedAvg and InsDP-FedAvg. We first consider backdoor attacks (BKD) (Bagdasaryan
et al., 2020) and label flipping attacks (LF) (Fung et al., 2020). For InsDP-FedAvg, we consider
the worst case where k backdoored or lable-flipped instances are fallen into the dataset of one user. For
UserDP-FedAvg, we additionally evaluate distributed backdoor attack (DBA) (Xie et al., 2019),
which is claimed to be a more stealthy backdoor attack against FL. Moreover, we consider BKD,
LF and DBA via model replacement approach (Bagdasaryan et al., 2020) where k attackers train
the local models using local datasets with α fraction of poisoned instances, and scale the malicious
updates with hyperparameter γ, i.e., ∆Wi J Y∆Wi, before sending them to the sever. This way, the
malicious updates would have a stronger impact on the FL model. Note that even when attackers
perform scaling, after server clipping, the sensitivity of updates is still upper-bounded by the clipping
threshold S. So the privacy guarantee in Proposition 1 still holds under poisoning attacks via model
replacement. Detailed attack setups are presented in Appendix A.3.
Evaluation Metrics and Setup. We consider two evaluation metrics based on our robustness
certification criteria. The first metric is certified accuracy, which is the fraction of the test set for
which the poisoned FL model makes correct and consistent predictions compared with the clean
FL model. Given a test set of size n, for i-th test sample, the ground truth label is yi , the output
prediction is ci , and the certified number of adversarial users/instances is Ki . We calculate the
certified accuracy at k as n1 Pn=11{ci = yi and Ki ≥ k}. The second metric is the lower bound of
7
Under review as a conference paper at ICLR 2022
attack cost in Theorem 3: J(D0) = max{e-ke J(B) - 1-e-； δC, 0}. We evaluate the tightness of
J(D0) by comparing it with empirical attack cost J(D0). To quantify the robustness, we evaluate the
expected class confidence Fc(M(D), x) for class c via Monte-Carlo sampling. We run the private
FL algorithms for M =1000 times, with class confidence fcs = fc (M(D), x) for each time. We
compute its expectation to estimate Fc(M(D), x) ≈ 备 PM=I fC and use it to evaluate Theorem 2.
In addition, we use Hoeffding’s inequality (Hoeffding, 1994) to calibrates the empirical estimation
with confidence level parameter ψ, and results are deferred to Appendix A.3. In terms of the attack
cost, we use Example 1, 2 as the definitions of cost function C for backdoor attacks and label flipping
attacks respectively. We follow similar protocol to estimate J(D0) for Theorem 3 and Corollary 1.
6.1	Robustness Evaluation of User-level DPFL
Certified Prediction. Figure 1(a)(b) present the user-level certified accuracy under different by
training DPFL models with different noise scale σ. The results on Sent140 dataset is presented
in Figure 13 of Appendix. A.3.8. We observe that the largest k can be certified when is around
0.6298 in MNIST, 0.1451 in CIFAR-10, and 0.2247 in Sent140 which verifies the tradeoff between
and certified accuracy as we discussed in Section 4.2. Advanced DP protocols that requires less
noise while achieving similar level of privacy are favored to improve the privacy, utility, and certified
accuracy simultaneously. Furthermore, we compare the certified accuracy of four different user-level
DPFL methods (McMahan et al., 2018; Geyer et al., 2017) given the same privacy budget . As shown
in Figure 14 and Figure 15 of Appendix. A.3.9, the models trained by different DPFL algorithms
satisfying same have different certified robustness results. This is because even under the same
, different DPFL algorithms M produce trained models M(D) with different model performance,
thus leading to different certified robustness. More discussion could be found in Appendix. A.3.9.
Figure 1: Certified accuracy of FL satisfying user-level DP (a,b), and instance-level DP (c,d).
Certified Attack Cost. In order to evaluate Theorem 3 and characterize the tightness of our theoret-
ical lower bound J(D0), we compare it with the empirical attack cost J(D0) under different local
poison fraction α , attack methods and scale factor Y in Figure 2. Note that when k = 0, the model is
benign so the empirical cost equals to the certified one. We find that 1) when k increases, the attack
ability grows, and both the empirical attack cost and theoretical lower bound decreases. 2) In Figure 2
row 1, given the same k, higher α, i.e., poisoning more local instances for each attacker, achieves a
stronger attack, under which lower empirical J(D) can be achieved and is more close to the certified
lower bound. This indicates that the lower bound appears tighter when the poisoning attack is stronger.
3) In Figure 2 row 2, we fix α = 100% and evaluate UserDP-FedAvg under different γ and attack
methods. It turns out that DP serves as a strong defense empirically for FL, given that J(D) did
not vary much under different γ(1, 50, 100) and different attack methods (BKD, DBA, LF). This is
because the clipping operation restricts the magnitude of malicious updates, rendering the model
replacement ineffective; the Gaussian noise perturbs the malicious updates and makes the DPFL
model stable, and thus the FL model is less likely to memorize the poisoning instances. 4) In both
rows, the lower bounds are tight when k is small. When k is large, there remains a gap between our
theoretical lower bounds and empirical attack costs under different attacks, which will inspire more
effective poisoning attacks or tighter robustness certification.
Certified Attack Cost under Different . Here we further explore the impacts of different factors
on the certified attack cost. Figure 3 presents the empirical attack cost and the certified attack cost
lower bound given different on user-level DP. It is shown that as the privacy guarantee becomes
stronger, i.e. smaller , the model is more robust achieving higher J(D0) and J(D0). In Figure 5
(a)(b), we train user-level (, δ) DPFL models, calculate corresponding J(D), and plot the lower
bound of k given different attack effectiveness hyperparameter τ according to Corollary 1. It shows
that 1) when the required attack effectiveness is higher, i.e., τ is larger, more number of attackers
is required. 2) To achieve the same effectiveness of attack, fewer number of attackers is needed for
larger , which means that DPFL model with weaker privacy is more vulnerable to poisoning attacks.
8
Under review as a conference paper at ICLR 2022
(a) MNlST Backdoor (£=0.3672)
(b) CIFAR-IO Backdoor (ε=0.5346)
(c) MNlST Label Flipping (ε=0.4025) (d) CIFAR-IO Label Flipping (ε=0.5978)
(e) MNIST Backdoor (£=0.4344)
αwer bound
lower □□un□
LFa-IOK
LFo=4β%
LFa-60%
LF a=10t)%
DBAy=I
DBAy= 50
DBAy=IQQ
lower bound
BKD O=I0%
BKD a-30%
BKD a=50%
BKD O-70%
BKD a=100% '----
lower bound
LFy=I
LFy-50
LF γ- IOO
lower bound
BKDy-I
BKDy= 50
BKD γ= IOO
DBAy=I
DBAy= 50
DBAy=IOfl
.	k
Figure 2: Certified attack cost of user-level DPFL given different k, under attacks With different α or γ.
lower bound
BKD O-10%
BKD a=40%
BKD a-60%
BKD a=10D⅝
(f) CIFAR-IO Backdoor (ε=0.5978)
(g) MNIST Label Flipping (ε=0.4344) (h) CIFAR-IO Label Flipping (ε=0.5978)
lower bound —.-----
LF O=I0%
LF a-30K
LF a=70U
LFa-l□Q%
lower bound
BKDy=I
BKDy-50
SKDy=IOQ
(a) MNIST Backdoor (fc=4)
Figure 3: Certified attack cost of user-level DPFL With different under different attacks.
(c) MNlST Label Flipping (k=4)
(d} CIFAR-IO Label Flipping (k=4)
lower bound
LFy=I
LFy= 50
LF γ= IOO
lower bnuno
LFy=
LFy-
LFy=XOQ




























6.2	Robustness Evaluation of Instance-level DPFL
Certified Prediction. Figure 1(c)(d) shoW the instance-level certified accuracy under different . The
optimal for K is around 0.3593 for MNIST and 0.6546 for CIFAR-10, Which is aligned With our
observation of the tradeoff betWeen certified accuracy and privacy on user-level DPFL (Section 6.1).
Certified Attack Cost. Figure 4 shoW the certified attack cost on CIFAR-10. From Figure 4 (a)(b),
poisoning more instances (i.e., larger k) induces loWer theoretical and empirical attack cost. From
Figure 4 (c)(d), it is clear that instance-level DPFL With stronger privacy guarantee provides higher
attack cost both empirically and theoretically, meaning that it is more robust against poisoning attacks.
Results on MNIST are deferred to Appendix A.3. Figure 5 (c)(d) shoW the loWer bound of k under
different instance-level given different τ. FeWer poisoned instances are required to reduce the J(D0)
to the similar level for a less private DPFL model, indicating that the model is easier to be attacked.
(a) CIFAR-IO Backdoor (£=0.3158) (b) CIFAR-IO Label Flipping (£=0.3158)	(C) CIFAR-IO Backdoor (⅛=4)	(d) CIFAR-IO Label Flipping (k=4)
Figure 4: Certified attack cost of instance-level DPFL under different attacks given different number of
malicious instances k (a)(b) and different (c)(d).
Figure 5: Lower bound of k under user-level e (a,b) and instance-level e (c,d) given attack effectiveness τ.
7	Conclusion
In this paper, we present the first work on deriving certified robustness in DPFL for free against
poisoning attacks. We propose two robustness certification criteria, based on which we prove that a
FL model satisfying user-level (instance-level) DP is certifiably robust against a bounded number of
adversarial users (instances). Our theoretical analysis characterizes the inherent relation between cer-
tified robustness and differential privacy of FL on both user and instance levels, which are empirically
verified with extensive experiments. Our results can be used to improve the trustworthiness of DPFL.
9
Under review as a conference paper at ICLR 2022
Ethics Statement. Our work study the robustness guarantee of differentially private federated
learning models from theoretical and empirical perspectives. All the datasets and packages we use
are open-sourced. We do not have ethical concerns in our paper.
Reproducibility Statement. Our source code is available as the supplemental material for repro-
ducibility purpose. Our experiments can be reproduced following our detailed training and evaluation
setups in Appendix A.3. The complete proofs of privacy analysis and certified robustness analysis
can be found in the Appendix A.1 and Appendix A.4, respectively.
References
OPacUs - train Pytorch models with differential privacy, 2021. URL https://opacus.ai/.
Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. DeeP learning with differential Privacy. In Proceedings of the 2016 ACM SIGSAC
conference on computer and communications security, PP. 308-318, 2016.
Naman Agarwal, Ananda Theertha SUresh, Felix YU, Sanjiv KUmar, and H Brendan McMahan. cPsgd:
commUnication-efficient and differentially-Private distribUted sgd. In Proceedings of the 32nd
International Conference on Neural Information Processing Systems, PP. 7575-7586, 2018.
Sebastien Andreina, Giorgia Azzurra Marson, Helen Mollering, and Ghassan Karame. Baffle:
Backdoor detection via feedback-based federated learning. arXiv preprint arXiv:2011.02167,
2020.
Shahab Asoodeh and F Calmon. Differentially Private federated learning: An information-theoretic
PersPective. In ICML Workshop on Federated Learning for User Privacy and Data Confidentiality,
2020.
Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to
backdoor federated learning. In International Conference on Artificial Intelligence and Statistics,
PP. 2938-2948. PMLR, 2020.
Gilad Baruch, Moran Baruch, and Yoav Goldberg. A little is enough: Circumventing defenses for
distributed learning. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran As-
sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
ec1c59141046cd1866bbbcdfb6ae31d4- Paper.pdf.
Raef Bassily, Adam Smith, and AbhradeeP Thakurta. Private emPirical risk minimization: Efficient
algorithms and tight error bounds. In 2014 IEEE 55th Annual Symposium on Foundations of
Computer Science, PP. 464-473. IEEE, 2014.
Michael Ben-Or, Shafi Goldwasser, and Avi Wigderson. ComPleteness theorems for non-
cryPtograPhic fault-tolerant distributed comPutation. In Proceedings of the twentieth annual
ACM symposium on Theory of computing, PP. 1-10, 1988.
Arjun Nitin Bhagoji, SuPriyo Chakraborty, Prateek Mittal, and SeraPhin Calo. Analyzing federated
learning through an adversarial lens. In International Conference on Machine Learning, PP.
634-643, 2019.
Abhishek Bhowmick, John Duchi, Julien Freudiger, Gaurav KaPoor, and Ryan Rogers. Protec-
tion against reconstruction and its aPPlications in Private federated learning. arXiv preprint
arXiv:1812.00984, 2018.
Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against suPPort vector machines.
In Proceedings of the 29th International Coference on International Conference on Machine
Learning, PP. 1467-1474, 2012.
Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. Machine learning
with adversaries: Byzantine tolerant gradient descent. In Proceedings of the 31st International
Conference on Neural Information Processing Systems, PP. 118-128, 2017.
10
Under review as a conference paper at ICLR 2022
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar
Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacy-
preserving machine learning. In proceedings of the 2017 ACM SIGSAC Conference on Computer
and Communications Security, pp.1175-1191, 2017.
Raphael Bost, Raluca Ada Popa, Stephen Tu, and Shafi Goldwasser. Machine learning classification
over encrypted data. In NDSS, volume 4324, pp. 4325, 2015.
Theodora S Brisimi, Ruidi Chen, Theofanie Mela, Alex Olshevsky, Ioannis Ch Paschalidis, and Wei
Shi. Federated learning of predictive models from federated electronic health records. International
journal of medical informatics, 112:59-67, 2018.
Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep
learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017a.
Yudong Chen, Lili Su, and Jiaming Xu. Distributed statistical machine learning in adversarial settings:
Byzantine gradient descent. Proceedings of the ACM on Measurement and Analysis of Computing
Systems, 1(2):1-25, 2017b.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In International Conference on Machine Learning, pp. 1310-1320. PMLR, 2019.
Cynthia Dwork and Jing Lei. Differential privacy and robust statistics. In Proceedings of the forty-first
annual ACM symposium on Theory of computing, pp. 371-380, 2009.
Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations
and Trends in Theoretical Computer Science, 9(3-4):211-407, 2014.
Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data,
ourselves: Privacy via distributed noise generation. In Advances in Cryptology - EUROCRYPT.,
2006.
El Mahdi El Mhamdi, Rachid Guerraoui, and SebaStien Louis Alexandre Rouault. The hidden
vulnerability of distributed learning in byzantium. In International Conference on Machine
Learning, number CONF, 2018.
Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong. Local model poisoning attacks to
byzantine-robust federated learning. In 29th {USENIX} Security Symposium ({USENIX} Security
20), pp. 1605-1622, 2020.
Shuhao Fu, Chulin Xie, Bo Li, and Qifeng Chen. Attack-resistant federated learning with residual-
based reweighting. arXiv preprint arXiv:1912.11464, 2019.
Clement Fung, Chris JM Yoon, and Ivan Beschastnikh. The limitations of federated learning in
sybil settings. In 23rd International Symposium on Research in Attacks, Intrusions and Defenses
({RAID} 2020), pp. 301-316, 2020.
Jonas Geiping, Hartmut Bauermeister, Hannah Droge, and Michael Moeller. Inverting gradients-how
easy is it to break privacy in federated learning? NeurIPS, 2020.
Robin C Geyer, Tassilo Klein, and Moin Nabi. Differentially private federated learning: A client
level perspective. arXiv preprint arXiv:1712.07557, 2017.
Ran Gilad-Bachrach, Nathan Dowlin, Kim Laine, Kristin Lauter, Michael Naehrig, and John Wernsing.
Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy. In
International Conference on Machine Learning, pp. 201-210. PMLR, 2016.
Alec Go, Richa Bhayani, and Lei Huang. Twitter sentiment classification using distant supervision.
Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Evaluating backdooring
attacks on deep neural networks. IEEE Access, 7:47230-47244, 2019.
Andrew Hard, Kanishka Rao, Rajiv Mathews, FranCOiSe Beaufays, Sean Augenstein, Hubert Eichner,
Chloe Kiddon, and Daniel Ramage. Federated learning for mobile keyboard prediction. arXiv
preprint arXiv:1811.03604, 2018.
11
Under review as a conference paper at ICLR 2022
Wassily Hoeffding. Probability inequalities for sums of bounded random variables. In The Collected
Works OfWassily Hoeffding, pp. 409-426. SPringer,1994.
Sanghyun Hong, Varun Chandrasekaran, Yigitcan Kaya, TUdor Dumitray, and Nicolas Papernot.
On the effectiveness of mitigating data Poisoning attacks with gradient shaPing. arXiv preprint
arXiv:2002.11497, 2020.
Ling Huang, Anthony D Joseph, Blaine Nelson, Benjamin IP Rubinstein, and J Doug Tygar. Ad-
versarial machine learning. In Proceedings of the 4th ACM workshop on Security and artificial
intelligence, pp. 43-58, 2011.
Matthew Jagielski, Jonathan Ullman, and Alina Oprea. Auditing differentially private machine
learning: How private is private sgd? Advances in Neural Information Processing Systems, 33,
2020.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.
lecun.com/exdb/mnist/.
Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified
robustness to adversarial examples with differential privacy. In 2019 IEEE Symposium on Security
and Privacy (SP), pp. 656-672, 2019a. doi: 10.1109/SP.2019.00044.
Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified
robustness to adversarial examples with differential privacy. In 2019 IEEE Symposium on Security
and Privacy (SP), pp. 656-672. IEEE, 2019b.
Suyi Li, Yong Cheng, Wei Wang, Yang Liu, and Tianjian Chen. Learning to detect malicious clients
for robust federated learning. arXiv preprint arXiv:2002.00211, 2020a.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 2018.
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges,
methods, and future directions. IEEE Signal Processing Magazine, 37(3):50-60, 2020b. doi:
10.1109/MSP.2020.2975749.
Zhicong Liang, Bao Wang, Quanquan Gu, Stanley Osher, and Yuan Yao. Exploring private federated
learning with laplacian smoothing. arXiv preprint arXiv:2005.00218, 2020.
Yuzhe Ma, Xiaojin Zhu Zhu, and Justin Hsu. Data poisoning against differentially-private learners:
Attacks and defenses. In International Joint Conference on Artificial Intelligence, 2019.
Mohammad Malekzadeh, Burak Hasircioglu, Nitish Mital, Kunal Katarya, Mehmet Emre Ozfatura,
and Deniz Gunduz. Dopamine: Differentially private federated learning on medical data. arXiv
preprint arXiv:2101.11693, 2021.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-Efficient Learning of Deep Networks from Decentralized Data. In Proceed-
ings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54 of
Proceedings of Machine Learning Research, pp. 1273-1282. PMLR, 20-22 Apr 2017.
H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private
recurrent language models. In International Conference on Learning Representations, 2018.
Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov. Exploiting unintended
feature leakage in collaborative learning. In 2019 IEEE Symposium on Security and Privacy (SP),
pp. 691-706. IEEE, 2019.
Ilya Mironov. Renyi differential privacy. In 2017 IEEE 30th Computer Security Foundations
Symposium (CSF), pp. 263-275. IEEE, 2017.
Ilya Mironov, Kunal Talwar, and Li Zhang. R\’enyi differential privacy of the sampled gaussian
mechanism. arXiv preprint arXiv:1908.10530, 2019.
12
Under review as a conference paper at ICLR 2022
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-
performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc,
E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp.
8024-8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.
pdf.
Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 1532-1543, 2014.
Krishna Pillutla, Sham M Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.
arXiv preprint arXiv:1912.13445, 2019.
Elan Rosenfeld, Ezra Winston, Pradeep Ravikumar, and Zico Kolter. Certified robustness to label-
flipping attacks via randomized smoothing. In International Conference on Machine Learning, pp.
8230-8241. PMLR, 2020.
Bita Darvish Rouhani, M Sadegh Riazi, and Farinaz Koushanfar. DeepSecure: Scalable provably-
secure deep learning. In Proceedings of the 55th Annual Design Automation Conference, pp. 1-6,
2018.
Ziteng Sun, Peter Kairouz, Ananda Theertha Suresh, and H Brendan McMahan. Can you really
backdoor federated learning? arXiv preprint arXiv:1911.07963, 2019.
Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Asso-
ciates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
280cf18baf4311c92aa5a042336587d3-Paper.pdf.
Stephen Tu. Lecture 20: Introduction to differential privacy. URL https://stephentu.
github.io/writeups/6885-lec20-b.pdf.
Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh Agarwal, Jy-yong
Sohn, Kangwook Lee, and Dimitris Papailiopoulos. Attack of the tails: Yes, you really can
backdoor federated learning. NeurIPS, 2020.
Maurice Weber, Xiaojun Xu, Bojan Karlas, Ce Zhang, and Bo Li. Rab: Provable robustness against
backdoor attacks. arXiv preprint arXiv:2003.08904, 2020.
Chen Wu, Xian Yang, Sencun Zhu, and Prasenjit Mitra. Mitigating backdoor attacks in federated
learning. arXiv preprint arXiv:2011.01767, 2020.
Chulin Xie, Keli Huang, Pin-Yu Chen, and Bo Li. Dba: Distributed backdoor attacks against federated
learning. In International Conference on Learning Representations, 2019.
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept and
applications. ACM Transactions on Intelligent Systems and Technology (TIST), 10(2):12, 2019a.
Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas Kong, Daniel
Ramage, and FranCoiSe Beaufays. Applied federated learning: Improving google keyboard query
suggestions. arXiv preprint arXiv:1812.02903, 2018.
Wensi Yang, Yuhang Zhang, Kejiang Ye, Li Li, and Cheng-Zhong Xu. Ffd: a federated learning
based method for credit card fraud detection. In International Conference on Big Data, pp. 18-32.
Springer, 2019b.
Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust distributed
learning: Towards optimal statistical rates. In International Conference on Machine Learning, pp.
5650-5659. PMLR, 2018.
13
Under review as a conference paper at ICLR 2022
Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. In
H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso-
ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
60a6c4002cc7b29142def8871531281a- Paper.pdf.
Yuqing Zhu, Xiang Yu, Yi-Hsuan Tsai, Francesco Pittaluga, Masoud Faraki, Manmohan Chandraker,
and Yu-Xiang Wang. Voting-based approaches for differentially private federated learning, 2021.
URL https://openreview.net/forum?id=NNd0J677PN.
14
Under review as a conference paper at ICLR 2022
A Appendix
The Appendix is organized as follows:
•	Appendix A.1 provides the DP definitions and the DPFL algorithms on both user and instance
levels, and the proofs for corresponding privacy guarantees.
•	Appendix A.2 specifies our threat models.
•	Appendix A.3 provides more details on experimental setups for training and evaluation, the ad-
dition experimental results on certified accuracy with confidence level, robustness evaluation of
InsDP-FedAvg on MNIST, robustness evaluation on 10-class classification, DP bound com-
parison between InsDP-FedSGD and Dopamine, certified accuracy of UserDP-FedAvg on
Sent140 and certified accuracy comparison of different user-level DPFL algorithms.
•	Appendix A.4 provides the proofs for the certified robustness related analysis, including Lemma 1,
Theorem 1, 2, 3, 5 and Corollary 1.
•	Appendix A.5 provides the comparison to related work (Lecuyer et al., 2019a; Ma et al., 2019).
A.1 Differentially Private Federated Learning
A.1.1	USERDP-FEDAVG
Definition 2 (User-level (, δ)-DP). Let B, B0 be two user sets with size N. Let D and D0 be the
datasets that are the union of local training examples from all users in B and B0 respectively. Then, D
and D0 are adjacent if B and B0 differ by one user. The mechanism M satisfies user-level (, δ)-DP
if it meets Definition 1 with D and D0 as adjacent datasets.
Algorithm 1: UserDP-FedAvg.
Input: Initial model w0, user sampling probability q, privacy
parameter δ, clipping threshold S , noise level σ, local
datasets D1, ..., DN, local epochs E, learning rate η.
Output: FL model wT and privacy cost
Server executes:
for each round t = 1 to T do
m — max(q ∙ N, 1);
Ut — (random subset of m users);
for each user i ∈ Ut in parallel do
L δWi 士-UserUPdate (i, Wt一ι)；
Wt J Wt-i + m (Pi∈ut Clip (∆wi, S) + N (0,σ2S2));
M.accum-priv Spending(σ,q,δ) ;
e = M. get privacy _SPent ();
return WT ,
Procedure UserUpdate(i, Wt-1)
W J Wt-1 ;
for local epoch e = 1 to E do
for batch b ∈ local dataSet Di do
LW — w — ηVl(w; b)
∆Wti J W - Wt-1 ;
return ∆wt
Procedure Clip(∆, S)
I return △/ max(l, /k2)
In Algorithm 1, M. accum-priv spending () and M. get .privacy spent。are the calls on the moments
accountant M refer to the API of (Abadi et al., 2016).
Given the user sampling probability q, noise level σ, FL rounds T, and a δ > 0, UserDP-FedAvg
satisfies (, δ)-DP as below, which is a generalization of (Abadi et al., 2016). The aim is to analyze
privacy budget , which is accumulated as T increases due to the continuous access to training data.
Proposition 1 (UserDP-FedAvg Privacy Guarantee). There exist constants ci and c2 so that given
user sampling probability q, and FL rounds T, for any ε < cιq2T, if σ ≥ c2 NlT lθg(1∕δ), the
randomized mechanism M in Algorithm 1 is (, δ)-DPfor any δ > 0.
Proof. The proof follows the proof of Theorem 1 in (Abadi et al., 2016), while the notations have
slightly different meanings under FL settings. In Proposition 1, we use q to represent user-level
sampling probability and T to represent FL training rounds.	□
Note that the above privacy analysis can be further improved by Renyi Differential Privacy (Mironov
et al., 2019).
15
Under review as a conference paper at ICLR 2022
Discussion (Li et al., 2020b) divide the user-level privacy into global privacy (Geyer et al., 2017;
McMahan et al., 2018) and local privacy (Agarwal et al., 2018). In both local and global privacy, the
norm of each update is clipped. The difference lies in that the noise is added on the aggregated model
updates in global privacy because a trusted server is assumed, while the noise is added on each local
update in local privacy because it assumes that the central server might be malicious. Algorithm 1
belongs to global privacy.
A.1.2 INSDP-FEDSGD
Definition 3 (Instance-level (, δ)-DP). Let D be the dataset that is the union of local training
examples from all users. Then, D and D0 are adjacent if they differ by one instance. The mechanism
M is instance-level (, δ)-DP if it meets Definition 1 with D and D0 as adjacent datasets.
Algorithm 2: InsDP-FedSGD.
Input: Initial model w0, user sampling probability q,
privacy parameter δ, local clipping threshold S,
local noise level σ, local datasets D1, ..., DN,
learning rate η , batch sampling probability p.
Output: FL model wT and privacy cost
Server executes:
for each round t = 1 to T do
m — max(q ∙ N, 1);
Ut — (random subset of m clients);
for each user i ∈ Ut in parallel do
L δWi —UserUpdate (i, Wt-1)；
Wt ― wt-1 + * Pi∈u δWi ;
M.accum_privspending(λ∕mσ,pq, δ)
e = M.get?rivacy_spent();
return WT ,
Procedure UserUpdate(i, Wt-1 )
W J Wt-1 ；
bit J(uniformly sample a batch from Di with prob-
ability p = L/|Di|);
for each xj ∈ bit do
I g(xj) J Vl(W； Xj);
L g(xj) J Clip(g(xj),S);
e J L (Pj g(Xj)+ N (0,σ2S2));
〜.
W J W - ηg ;
∆Wti J W - Wt-1 ;
return ∆Wt
Procedure Clip(∆, S)
I return △/ max(1, k^Sk2')
Under FedSGD, when each local model performs one step of DP-SGD (Abadi et al., 2016), the
randomized mechanism M that outputs the global model preserves the instance-level DP. We can
regard the one-step update for the global model in Algorithm 2 as:
WtJ Wt-i- m X L ( X U(Xj) + N (0, σ2S2) J	(5)
i∈Ut	xj ∈bit
Proposition 2 (InsDP-FedSGD Privacy Guarantee). There exist constants c1 and c2 so that given
batch sampling probability P, and user sampling probability q, the number of selected users each
round m, and FL rounds T, for any ε < cι(pq)2T, if σ ≥ c2 FqyT√m(1/^, the randomized
mechanism M in Algorithm 2 is (, δ)-DPfor any δ > 0.
Proof. i) In instance-level DP, we consider the sampling probability of each instance under the com-
bination of user-level sampling and batch-level sampling. Since the user-level sampling probability
is q and the batch-level sampling probablity is P, each instance is sampled with probability Pq. ii)
Additionally, since the sensitivity of instance-wise gradient w.r.t one instance is S, after local gradient
descent and server FL aggregation, the equivalent sensitivity of global model w.r.t one instance
is S0 = LS according to Eq (5). iii) Moreover, since the local noise is ni 〜N(0, σ2S2) , then
the “virtual” global noise is n = mL Pi∈u古 n according to Eq (5), so n ~ N(0,律展2). Let
ηmLS2 = σ02S02 such that n ~ N(0, σ02S02). Because S0 = LS, the equivalent global noise level
is σ02 = σ2m, i.e., σ0 = σ√m.
In Proposition 2, We use pq to represent instance-level sampling probability, T to represent FL
training rounds, σ√m to represent the equivalent global noise level. The rest of the proof follows the
proof of Theorem 1 in (Abadi et al., 2016).
□
16
Under review as a conference paper at ICLR 2022
We defer the DP bound evaluation comparison between InsDP-FedSGD and Dopamine to Ap-
pendix A.3.7.
A.1.3 InsDP-FedAvg
Algorithm 3: InsDP-FedAvg.
Input: Initial model w0 , user sampling probability q,
privacy parameter δ, local clipping threshold S,
local noise level σ, local datasets D1, ..., DN,
local steps V , learning rate η , batch sampling
probability p.
Output: FL model wT and privacy cost
Server executes:
for each round t = 1 to T do
m — max(q ∙ N, 1);
Ut — (random subset of m users);
for each user i ∈ Ut in parallel do
L ∆wt, €t — UserUpdate (i, Wt-ι)；
for each user i ∈/ Ut do
L et J 4-ι;
Wt — wt-1 + mm Pi∈Ut δWi;
Q = M.parallel ^composition({ei}i∈[N ])
= T ;
return WT ,
Procedure UserUpdate(i, Wt-1 )
W J Wt-1 ;
for each local step v = 1 to V do
b J(uniformly sample a batch from Di with
probability p = L/|Di |);
for each xj ∈ b do
I g(xj) J Vl(w; Xj);
L g(xj) J Clip(g(xj),S);
e J L(Pjg(xj)+ N(0,σ2S2));
~.
W J W - ηg ;
Mi.accum_privspending(σ,p,δ);
ei = Mi.get_PrivaCy分pent();
∆Wti J W - Wt-1 ;
_ return ∆wi et
Procedure Clip(∆, S)
I return △/ max(1, k∆k2)
Lemma 2 (InsDP-FedAvg Privacy Guarantee when T = 1). In Algorithm 3, when T = 1, suppose
local mechanism Mi satisfies (i, δ)-DP, then global mechanism M satisfies (maxi∈[N] i, δ)-DP.
Proof. We can regard federated learning as partitioning a dataset D into N disjoint subsets
{D1, D2, . . . , DN}. N mechanisms {M1, . . . , MN} are operated on these N parts separately
and each Mi satisfies its own i-DP for i ∈ [1, N]. Note that if i-th user is not selected , i = 0
because local dataset Di is not accessed and there is no privacy cost. Without loss of generality,
we assume the modified data sample x0 (x → x0 causes D → D0) is in the local dataset of k-th
client Dk. Let D, D0 be two neighboring datasets (Dk, Dk0 are also two neighboring datasets). M
is randomized mechanism that outputs the global model, and Mi is the randomized mechanism
that outputs the local model update ∆Wi . Suppose W0 is the initialized and deterministic global
model, and {z1 , . . . , zN} are randomized local updates. We have a sequence of computations
{z1 = M1(D1), z2 = M2(D2; z1), z3 = M3(D3; z1, z2) . . .} and z = M(D) = W0 + PiN=1 zi.
Note that if i-th user is not selected , zi = 0. According to the parallel composition (Tu), we have
Pr[M(D) = z]
= Pr[M1(D1) = z1] Pr[M2(D2; z1) = z2] . . .Pr[MN(DN;z1,. . . ,zN-1) = zN]
≤ exp(k) Pr[Mk(Dk0 ; z1, . . . , zk-1) = zk] Y Pr[Mi(Di; z1, . . . ,zi-1) = zi]
i6=k
= exp(k) Pr[M(D0) = z]
So M satisfies k-DP when the modified data sample lies in the subset Dk. Consider the worst case
of where the modified data sample could fall in, We know that M satisfies (maxi∈[N] ei)-DP.	□
We recall Theorem 4.
Theorem 4 (InsDP-FedAvg Privacy Guarantee). In Algorithm 3, during round t, if the local
mechanism Mi satisfies (it, δ)-DP, then the global mechanism M satisfies maxi∈[N] it, δ -DP.
Proof. Again, without loss of generality, we assume the modified data sample x0 (x → x0 causes
D → D0) is in the local dataset of k-th user Dk. We first consider the case when all users are
selected. At each round t, N mechanisms are operated on N disjoint parts and each Mti satisfies
17
Under review as a conference paper at ICLR 2022
own i-DP where i is the privacy cost for accessing the local dataset Di for one round (not
accumulating over previous rounds). Let D, D0 be two neighboring datasets (Dk , Dk0 are also two
neighboring datasets). Suppose z0 = Mt-1 (D) is the aggregated randomized global model at
round t - 1, and {z1, . . . , zN} are the randomized local updates at round t, we have a sequence
of computations {z1 = Mt1(D1; z0), z2 = Mt2(D2; z0, z1), z3 = Mt3(D3; z0, z1, z2) . . .} and
z = Mt (D) = z0 + PiN zi. We first consider the sequential composition (Dwork & Roth, 2014) to
accumulate the privacy cost over FL rounds. According to parallel composition, we have
Pr[Mt(D) = z]
N
= Pr[Mt-1 (D) = z0] YPr[Mit(Di; z0,z1, . . . ,zi-1) = zi]
i=1
= Pr[Mt-1 (D) = z0] Pr[Mt (Dk ; z0, z1 , . . . , zk-1 ) = zk] Y Pr[Mt (Di; z0, z1 , . . . , zi-1) = zi]
i6=k
≤ exp(t-1) Pr[Mt-1(D0) = z0] exp(k) Pr[Mtk(Dk0 ; z0, z1, . . . , zk-1) = zk] Y Pr[Mit(Di; z0, z1, . . . , zi-1) = zi]
i6=k
= exp(t-1 + k) Pr[Mt (D0) = z]
Therefore, Mt satisfies t-DP, where t = t-1 + k. Because the modified data sample always lies
in Dk over t rounds and 0 = 0, we can have t = tk, which means that the privacy guarantee of
global mechanism Mt is only determined by the local mechanism of k-th user over t rounds.
Moreover, moment accountant (Abadi et al., 2016) is known to reduce the privacy cost from O(t)
to O(√t). We can use the more advanced composition, i.e., moment accountant, instead of the
sequential composition, to accumulate the privacy cost for local mechanism Mk over t FL rounds.
In addition, we consider user subsampling. As described in Algorithm 3, if the user i is not selected
at round t, then its local privacy cost is kept unchanged at this round.
Take the worst case of where x0 could lie in, at round t, M satisfies t-DP, where t = maxi∈[N] it,
local mechanism Mi satisfies it-DP, and the local privacy cost it is accumulated via local moment
accountant in i-th user over t rounds.
□
A.2 Threat Models
We consider targeted poisoning attacks of two types. In backdoor attacks (Gu et al., 2019; Chen
et al., 2017a), the goal is to embed a backdoor pattern (i.e., a trigger) during training such that any
test input with such pattern will be mis-classified as the target. In label flipping attacks (Biggio et al.,
2012; Huang et al., 2011), the labels of clean training examples from one source class are flipped to
the target class while the features of the data are kept unchanged. In FL, the purpose of backdoor
attacks is to manipulate local models with backdoored local data, so that the global model would
behave normally on untampered data samples while achieving high attack success rate on clean
data (Bagdasaryan et al., 2020). Given the same purpose, distributed backdoor attack (DBA) (Xie
et al., 2019) decomposes the same backdoor pattern to several smaller ones and embeds them to
different local training sets for different adversarial users. The goal of label flipping attack against FL
is to manipulate local datasets with flipped labels such that the global model will mis-classify the test
data in the source class as the target class. The model replacement (Bagdasaryan et al., 2020) is a
more powerful approach to perform the above attacks, where the attackers first train the local models
using the poisoned datasets and then scale the malicious updates before sending them to the server.
This way, the attacker’s updates would have a stronger impact on the FL model. We use the model
replacement method to perform poisoning attacks and study the effectiveness of DPFL.
For UserDP-FedAvg, we consider backdoor, distributed backdoor, and label flipping attacks via
the model replacement approach. Next, we formalize the attack process and introduce the notations.
Suppose the attacker controls k adversarial users, i.e., there are k attackers out of N users. Let B
be the original user set of N benign users, and B0 be the user set that contains k attackers. Let
D := {D1, D2, . . . , DN} be the union of original benign local datasets across all users. For a data
sample zj := {xj, yj} in Di, We denote its backdoored version as Zj= {xj + δχ, y*}, where δχ
18
Under review as a conference paper at ICLR 2022
is the backdoor pattern, y* is the targeted label; the distributed backdoor attack (DBA) version as
Zj= {xj + δχ, y*}, where δχ is the distributed backdoor pattern for attacker i; the label-flipped
version as Zj := {xj, y*}. Note that the composition of all DBA patterns is equivalent to the
backdoor pattern, i.e., Pik=1 δxi = δx. We assume attacker i has αi fraction of poisoned samples in its
local dataset Di0. Let D0 := {D01, . . . , D0k-1, D0k, Dk+1, . . . , DN} be the union of local datasets
when k attackers are present. The adversarial user i performs model replacement by scaling the
model update with hyperparameter Y before submitting it to the server, i.e., ∆Wi J Y∆w[ In our
threat model, we consider the attacker that follows our training protocol and has no control over
which users are sampled.
For InsDP-FedAvg, we consider both backdoor and label flipping attacks. Since distributed
backdoor and model replacement attacks are proposed for adversarial users rather than adversarial
instances, we do not consider them for instance-level DPFL. There are k backdoored or label-flipped
instances {z10 , z20 , . . . , zk0 }, which could be controlled by same or multiple users. In our threat model,
we consider the attacker that follows our training protocol and has no control over which data partition
(or batch) is sampled. Note that we do not assume that the adversaries’ poisoning data always be
sampled. In our algorithms, each batch is randomly subsampled, so the adversaries cannot control if
poisoned data are sampled in each step.
A.3 Experimental Details and Additional Results
A.3.1 Datasets and Models
We evaluate our robustness certification results with two datasets: MNIST (LeCun & Cortes, 2010)
and CIFAR-10 (Krizhevsky, 2009). For each dataset, we use corresponding standard CNN architec-
tures in the differential privacy library (opa, 2021) of PyTorch (Paszke et al., 2019).
MNIST: We study an image classification problem of handwritten digits in MNIST. It is a dataset
of 70000 28x28 pixel images of digits in 10 classes, split into a train set of 60000 images and a test
set of 10000 images. Except Section A.3.6, we consider binary classification on classes 0 and 1,
making our train set contain 12665 samples, and the test set 2115 samples. The model consists of
two Conv-ReLu-MaxPooling layers and two linear layers.
CIFAR-10: We study image classification of vehicles and animals in CIFAR-10. This is a harder
dataset than MNIST, consisting of 60000 32x32x3 images, split into a train set of 50000 and a test
set of 10000. Except Section A.3.6, we consider binary classification on class airplane and bird,
making our train set contain 10000 samples, and the test set 2000 samples. The model consists of
four Conv-ReLu-AveragePooling layers and one linear layer. When training on CIFAR10, we follow
the standard practice for differential privacy (Abadi et al., 2016; Jagielski et al., 2020) and fine-tune a
whole model pre-trained non-privately on the more complex CIFAR100, a similarly sized but more
complex benchmark dataset. We can achieve reasonable performance on CIFAR-10 datasets by only
training (fine-tuning) few rounds.
Sent140: We consider a text sentiment analysis task on tweets from Sentiment140 (Go et al.)
(Sent140) which involves classifying Twitter posts as positive or negative. We use a two layer LSTM
binary classifier containing 256 hidden units with pretrained 300D GloVe embedding (Pennington
et al., 2014). Each twitter account corresponds to a device. We use the same network architecture,
non-iid dataset partition method, number of selected user per round, learning rate, batch size, etc. as
in (Li et al., 2018), which are summarized in Table 1.
A.3.2 Training Details
We simulate the federated learning setup by splitting the training datasets for N FL users in an i.i.d
manner. FL users run SGD with learning rate η, momentum 0.9, weight decay 0.0005 to update
the local models. The training parameter setups are summarized in Table 1. Following (McMahan
et al., 2018) that use δ ≈ n^t as privacy parameter, for UserDP-FedAvg We set δ = 0.0029
according to the total number of users, and for InsDP-FedAvg we set δ = 0.00001 according the
total number of training samples. Next we summarize the privacy guarantees and clean accuracy
offered when we study the certified prediction and certified attack cost, which are also the training
parameters setups when k = 0 in Figure 1, 2, 3, 4, 5, 8.
19
Under review as a conference paper at ICLR 2022
Algorithm	Dataset	#training samples	N	m	E	V	batch size	η	S	δ	C
UserDP-FedAvg	MNIST	12665	200	20	10	/	60	0.02	0.7	0.0029	0.5
UserDP-FedAvg	CIFAR-10	10000	200	40	5	/	50	0.05	1	0.0029	0.2
UserDP-FedAvg	Sent140	40783	805	10	3	/	10	0.3	0.5	0.000001	/
InsDP-FedAvg	MNIST	12665	10	10	/	25	50	0.02	0.7	0.00001	0.5
InsDP-FedAvg	CIFAR-10	10000	10	10	/	100	50	0.05	1	0.00001	2
Table 1: Dataset description and parameters
User-level DPFL In order to study the user-level certified prediction under different privacy guar-
antee, for MNIST, we set to be 0.2808, 0.4187, 0.6298, 0.8694, 1.8504, 2.8305, 4.8913, 6.9269,
which are obtained by training UserDP-FedAvg FL model for 3 rounds with noise level
σ = 3.0, 2.3, 1.8, 1.5, 1.0, 0.8, 0.6, 0.5, respectively (Figure 1(a)). For CIFAR-10, we set to
be 0.1083, 0.1179, 0.1451, 0.2444, 0.3663, 0.4527, 0.5460, 0.8781, which are obtained by training
UserDP-FedAvg FL model for one round with noise level σ = 10.0, 8.0, 6.0, 4.0, 3.0, 2.6, 2.3, 1.7,
respectively (Figure 1(b)). The clean accuracy (average over 1000 runs) of UserDP-FedAvg under
non-DP training ( = ∞) and DP training (varying ) on MNIST and CIFAR-10 are reported in
Table. 2 and Table. 3 respectively.
σ	0	0.5	0.6	0.8	1	1.5	1.8	2.3	3
e	I	∞	6.9269	4.8913	2.8305	1.8504	0.8694	0.6298	0.4187	0.2808
Clean Acc.	99.66%	99.72%	99.69%	99.71%	99.59%	98.86%	97.42%	89.15%	72.79%
	Table 2:	Clean accuracy of UserDP-FedAvg model on MNIST							
σ	0	1.7	2.3	2.6	3	4	6	8	10
e	I	∞	0.8781	0.546	0.4527	0.3663	0.2444	0.1451	0.1179	0.1083
Clean Acc.	81.90%	81.82%	80.09%	79.27%	77.89%	73.07%	64.36%	57.92%	54.59%
Table 3: Clean accuracy of UserDP-FedAvg model on CIFAR-10
To certify the attack cost under different number of adversarial users k (Figure 2), for MNIST, we set
the noise level σ to be 2.5. When k = 0, after training UserDP-FedAvg for T = 3, 4, 5 rounds, we
obtain FL models with privacy guarantee = 0.3672, 0.4025, 0.4344 and clean accuracy (average
over M runs) 86.69%, 88.76%, 88.99%. For CIFAR-10, we set the noise level σ to be 3.0. After
training UserDP-FedAvg for T = 3, 4 rounds under k = 0, we obtain FL models with privacy
guarantee = 0.5346, 0.5978 and clean accuracy 78.63%, 78.46%.
With the interest of certifying attack cost under different user-level DP guarantee (Figure 3, Fig-
ure 5), we explore the empirical attack cost and the certified attack cost lower bound given differ-
ent . For MNIST, we set the privacy guarantee to be 1.2716, 0.8794, 0.6608, 0.5249, 0.4344,
which are obtained by training UserDP-FedAvg FL models for 5 rounds under noise
level σ = 1.3, 1.6, 1.9, 2.2, 2.5, respectively, and the clean accuracy for the correspond-
ing models are 99.50%, 99.06%, 96.52%, 93.39%, 88.99%. For CIFAR-10, we set the pri-
vacy guarantee to be 1.600, 1.2127, 1.0395.0.8530, 0.7616, 0.6543, 0.5978, which are ob-
tained by training UserDP-FedAvg FL models for 4 rounds under noise level σ =
1.5, 1.8, 2.0, 2.3, 2.5, 2.8, 3.0, respectively, and the clean accuracy for the corresponding models
are 85.59%, 84.52%, 83.23%, 81.90%, 81.27%, 79.23%, 78.46%.
Instance-level DPFL To certify the prediction for instance-level DPFL un-
der different privacy guarantee, for MNIST, we set privacy cost	to be
0.2029, 0.2251, 0.2484, 0.3593, 0.4589, 0.6373, 1.0587, 3.5691, which are obtained
by training InsDP-FedAvg FL models for 3 rounds with noise level σ =
15, 10, 8, 5, 4, 3, 2, 1, respectively (Figure 1(c)). For CIFAR-10, we set privacy cost to be
0.3158, 0.3587, 0.4221, 0.5130, 0.6546, 0.9067, 1.4949, 4.6978, which are obtained by training
InsDP-FedAvg FL models for one round with noise level σ = 8, 7, 6, 5, 4, 3, 2, 1, respectively
(Figure 1(d)). The clean accuracy (average over 1000 runs) of InsDP-FedAvg under non-DP
20
Under review as a conference paper at ICLR 2022
training ( = ∞) and DP training (varying ) on MNIST and CIFAR-10 are reported in Table. 4 and
Table. 5 respectively.
σ	1	0	1	2	3	4	5	8	10	15
	1	∞	3.5691	1.0587	0.6373	0.4589	0.3593	0.2484	0.2251	0.2029
Clean Acc.	Il	99.85%	99.73%	99.73%	99.70%	99.65%	99.57%	97.99%	93.30%	77.12%
Table 4: Clean accuracy of InsDP-FedAvg model on MNIST
σ	1	0	1	2	3	4	5	6	7	8
	1	∞	4.6978	1.4949	0.9067	0.6546	0.513	0.4221	0.3587	0.3158
Clean Acc.	Il	91.15%	87.91%	86.02%	83.85%	81.43%	77.59%	72.69%	66.47%	62.26%
Table 5: Clean accuracy of InsDP-FedAvg model on CIFAR-10
With the aim to study certified attack cost under different number of adversarial instances k, for
MNIST, we set the noise level σ to be 10. When k = 0, after training InsDP-FedAvg for T = 4, 9
rounds, we obtain FL models with privacy guarantee = 0.2383, 0.304 and clean accuracy (average
over M runs) 96.40%, 96.93% (Figure 8(a)(b)). For CIFAR-10, we set the noise level σ to be 8.0.
After training InsDP-FedAvg for one round under k = 0, we obtain FL models with privacy
guarantee = 0.3158 and clean accuracy 61.78% (Figure 4(a)(b)).
In order to study the empirical attack cost and certified attack cost lower bound
under different instance-level DP guarantee, we set the privacy guarantee to be
0.5016, 0.311, 0.2646, 0.2318, 0.2202, 0.2096, 0.205 for MNIST, which are obtained
by training InsDP-FedAvg FL models for 6 rounds under noise level σ =
5, 8, 10, 13, 15, 18, 20, respectively, and the clean accuracy for the corresponding mod-
els are 99.60%, 98.81%, 97.34%, 92.29%, 88.01%, 80.94%, 79.60% (Figure 8 (c)(d)). For
CIFAR-10, we set the privacy guarantee to be 1.261, 0.9146, 0.7187, 0.5923, 0.5038, 0.4385,
which are obtained by training InsDP-FedAvg FL models for 2 rounds under noise level
σ = 3, 4, 5, 6, 7, 8, respectively, and the clean accuracy for the corresponding models are
84.47%, 80.99%, 76.01%, 68.65%, 63.07%, 60.65% (Figure 4 (c)(d)).
With the intention of exploring the upper bound for k given τ under different instance-level DP
guarantee, for MNIST, we set noise level σ to be 5, 8, 10, 13, 20, respectively, to obtain instance-DP
FL models after 10 rounds with privacy guarantee = 0.6439, 0.3937, 0.3172, 0.2626, 0.2179 and
clean accuracy 99.58%, 98.83%, 97.58%, 95.23%, 85.72% (Figure 5(c)). For CIFAR-10, we set
noise level σ to be 3, 4, 5, 6, 7, 8 and train InsDP-FedAvg for T = 3 rounds, to obtain FL mod-
els with privacy guarantee = 1.5365, 1.1162, 0.8777, 0.7238, 0.6159, 0.5361 and clean accuracy
84.34%, 80.27%, 74.62%, 66.94%, 62.14%, 59.75% (Figure 5(d)).
A.3.3 Additional Implementation Details
(Threat Models) For the attacks against
UserDP-FedAvg, by default, the local poison
fraction α = 100%, and the scale factor γ = 50. We
use same parameters setups for all k attackers. In terms
of label flipping attacks, the attackers swap the label
of images in source class (digit 1 for MNIST; bird for
CIFAR-10) into the target label (digit 0 for MNIST;
airplane for CIFAR-10). In terms of backdoor attacks
in MNIST and CIFAR-10, the attackers add a backdoor
Figure 6: Backdoor pattern (left) and
distributed backdoor patterns (right) on
CIFAR-10.
pattern, as shown in Figure 6 (left), in images and swap the label of any sample with such pattern into
the target label (digit 0 for MNIST; airplane for CIFAR-10). In terms of distributed backdoor attacks,
Figure 6 (right) shows an example when the triangle pattern is evenly decomposed into k = 4 parts,
and they are used as the distributed patterns for k = 4 attackers respectively. For the cases where
there are more or fewer distributed attackers, the similar decomposition strategy is adopted.
21
Under review as a conference paper at ICLR 2022
For the attacks against InsDP-FedAvg, the same target classes and backdoor patterns are used as
UserDP-FedAvg. The parameters setups are the same for all k poisoned instances.
(Robustness Certification) We certified 2115/2000/1122 test samples from the MNIST/CIFAR-
10∕Sent140 test sets. In Theorem 3 and Corollary 1 that are related to certified attack cost, C specifies
the range of C(∙). In the implementation, C is set to be larger than the maximum empirical attack
cost evaluated on the test sets (see Table 1 for details). For each dataset, We use the same C for
cost function C defined in Example 1 and Example 2. When using Monte-Carlo sampling, we
run M = 1000 times for certified accuracy, and M = 100 times for certified attack cost in all
experiments.
(Machines) We simulate the federated learning setup (1 server and N users) on a Linux machine With
Intel® Xeon® Gold 6132 CPUs and 8 NVidia® 1080Ti GPUs.
(Libraries) All code is implemented in Pytorch (Paszke et al., 2019). Please see the submitted code
for full details.
A.3.4 Certified Accuracy with Confidence Level
Here We present the certified accuracy With confidence level. We use Hoeffding’s inequality (Ho-
effding, 1994) to calibrates the empirical estimation With one-sided error tolerance ψ, i.e., one-sided
confidence level 1 - ψ . We first use Monte-Carlo sampling by running the private FL algorithms
for M times, With class confidence fcs = fc(M(D), x) for class c each time. We denote the em-
pirical estimation as Fc(M(D), x) = A PM=I fC. For a test input x, suppose A, B ∈ [C] satisfy
A = argmaxc∈[C] Fe(M(D), x) and B = arg maxc∈[CφC=A Fc(M(D), x). For a given error tol-
erance ψ, We use Hoeffding’s inequality to compute a loWer bound FA (M(D), x) on the class
confidence FA(M(D), x) and a upper bound FB(M(D), x) on the class confidence FB(M(D), x)
according to
FA (M(D),x) = FA (M(D),x) - 《ko2M∕ψ, FB (M(D),x) = FB (M(D), x) + /og^ ∙
(6)
FA(M(D), x) and FB(M(D), x) are used as the expected class confidences for the evaluation of
Theorem 2. We use ψ = 0.01 and M = 1000 for all experiments.
Figure 7: Certified accuracy under 99% confidence of FL satisfying user-level DP (a,b), and instance-level DP
(c,d).
As shoWn in Figure 7, We can observe the same tradeoff betWeen and certified accuracy as We
discussed in Figure 1. In general, the K in Figure 7 is smaller than the K in Figure 1 because We
calibrate the empirical estimation according to Eq. (6), and the class confidence gap betWeen top-1
and top-2 class is narroWed.
A.3.5 Additional Robustness Evaluation of Instance-level DPFL
Here We report the robustness evaluation of instance-level DPFL on MNIST. As shoWn in Figure 8,
the results on MNIST are similar to the results on CIFAR-10 in Figure 4.
22
Under review as a conference paper at ICLR 2022
(a) MNIST Backdoor (ε=0.2383)	(b) MNIST Label Flipping (ε=0.304)	(c) MNIST Backdoor (k=10)	(d) MNIST Label Flipping (it= 10)
Figure 8: Certified attack cost of instance-level DPFL on MNIST under different attacks given different number
of malicious instances k (a)(b) and different (c)(d).
A.3.6 Robustness Evaluation on 10-class Classification
Here we report the robustness evaluation of user-level DPFL under backdoor attacks on 10-class
classification problem. Figure 10 presents the certified accuracy under different . We can observe
the tradeoff between and certified accuracy on MNIST. On CIFAR-10, larger k can be certified with
smaller . The certified K is relatively small because we set large to preserve a reasonable accuracy
for 10-class classification. Our results can inspire advanced DP mechanisms that provide tighter
privacy guarantee (i.e., smaller ) while achieving similar level of accuracy. In terms of certified
attack cost, as shown in Figure 9 and 11, the trends are similar to the 2-class results in Figure 2, 3
and 5.
(a) MNIST Backdoor (e=0.6784)
(b) CIFAR Backdoor (ε=1.2008)
(O MNIST Backdoor (k=3}
(d) CIFAR Backdoor (k=l)
lower bound
ower bound
Figure 9: Certified attack cost of user-level DPFL on 10-class classification given different number of malicious
instances k (a)(b) and different (c)(d).
lower bound
3KDy=l,a=100%
3KDy=50,a=100%
BKDy-100, a>100%
3KDy=50.a=50%
DBA γ= 50, a=100%
lower bound
BKD γ= l,α=100%
BKD y=50,a=100%
BKD v-100, a>100%
BKD y=50.a=50%
DBA γ= 50, a=10D%
(a) MNIST
e = 14.9167
e ■ 10.0576
ε = 5.5422
e = 3.6099
ε - 1.8202
e = 1.5119
e - 1.3939
1.2933
1.1307
Figure 10: Certified accuracy of FL satisfying user- Figure 11: Lower bound of k on 10-class classification
level DP on 10-class classification.	under user-level given attack effectiveness τ .
A.3.7 DP bound comparison between INSDP-FEDSGD and Dopamine
Here we compare Dopamine to our InsDP-FedSGD, both of which are proposed for FedSGD.
Under the same noise level (σ = 3.0), clipping threshold (S = 1.5), user sampling probability
(m/N = 20/30), and batch sampling probability (0.4) settings, both algorithms achieve about 92%
accuracy on MNIST (10 classes). The Figure 12 shows the results of privacy guarantee estimation
over training rounds, which demonstrates that our method achieves tighter privacy certification. For
instance, at round 200, our method ( = 1.4029) achieves a much tighter privacy guarantee than
Dopamine ( = 2.1303).
A.3.8 Certified Accuracy of USERDP-FEDAVG on Sent140
For Sent140, we set to be 0.2238, 0.2247, 0.4102.0.7382, 1.7151, which are obtained by training
UserDP-FedAvg FL model for three rounds with noise level σ = 4, 3, 2, 1.5, 1, respectively
As shown in Figure 13, the largest k can be certified when is around 0.2247 in Sent140, which also
verifies the tradeoff between and certified accuracy as we observed in image datasets.
23
Under review as a conference paper at ICLR 2022
Figure 12: Comparison of DP bound under FedSGD on MNIST dataset. Our InsDP-FedSGD achieves a
tighter DP bound.
ɪ,o	sentl40
Figure 13: Certified accuracy of FL satisfying user-level DP on Sent140.
A.3.9 Certified Accuracy Comparison of different user-level DPFL algorithms
In this section, we include two more user-level DPFL works (McMahan et al., 2018; Geyer et al.,
2017) to study certified accuracy with a total of four different DPFL methods given the same privacy
budget . Since all our proposed robustness certifications are agnostic to DPFL algorithms, i.e.,
certifications hold no matter how (, δ) is achieved, we can empirically compare the certified results
of different DPFL algorithms. Specifically, we consider the following four DPFL algorithms:
•	flat clipping (UserDP-FedAvg) clips the concatenation of all the layers of model update
with the L2 norm threshold S .
•	per-layer clipping (McMahan et al., 2018) clips each layer of model update with the L2
norm threshold S.
•	flat median. clipping (Geyer et al., 2017) use the median of norms of clients’ model updates
as threshold S for flat clipping.
•	per-layer median clipping (Geyer et al., 2017) use the median of each layer’s norms of
clients’ model updates as threshold S for per-layer clipping.
For MNIST (CIFAR-10), we set to be 0.6319 (0.5346) which is obtained by training all DPFL
algorithms with the same noise level σ = 2.3 (σ = 3.0) for same number of rounds. For flat clipping
and per-layer clipping, we set S = 0.7 (S = 1) on MNIST (CIFAR-10). Except for local epoch
E = 1 1, other FL parameters setups are the same as in Table 1.
1In experiments we note that the median norm clipping approaches (Geyer et al., 2017) can only be applied
when the number of local epoch is small, which makes these methods less practical. Recall that in the server
aggregation step, the noise is sampled fromN(0, σ2S2), so S cannot be too large in order to keep the amount
24
Under review as a conference paper at ICLR 2022
1.0
⅛0∙8
n
O 0.6
(ŋ
-2 0.4
口
⅛ 0.2
MNIST (ɛ = 0.6319)
■	flat dipping (UserDP-FedAvg)
per-layer dipping (McMahan et al.f 2018)
flat median dipping (Geyer et al., 2017)
■	per-layer median clipping (Geyer et al., 2017)
0.0	-
0	12	3
k
.8.64 2
Oooo
Ao(DJnooe P ① 一与①O
CIFAR-IO (ɛ = 0.5346)
flat dipping (UserDP-FedAvg)
per-layer dipping (McMahan et al., 2018)
flat median clipping (Geyer et al., 2017)
per-layer median clipping (Geyer et al., 2017)
Figure 14: Certified accuracy of model trained by Figure 15: Certified accuracy of model trained by
different user-level DPFL algorithms under same on different user-level DPFL algorithms under same on
MNIST.	CIFAR-10.
As shown in Figure 14 and Figure 15, on MNIST, the flat clipping is able to certify the largest number
of adversaries k; while on CIFAR-10, the median clipping certifies the largest k instead. Moreover,
on both MNIST and CIFAR-10, flat clipping and per-layer clipping with the same S lead to different
certification results, while the results of flat median clipping and per-layer median clipping are nearly
identical. This is because even under the same privacy protection , different DPFL algorithms M
produce trained models M(D) with different model performance, thus leading to different certified
robustness. Specifically, in Theorem 1, given the same and x, FA(M(D), x) and FB(M(D), x)
vary for different DPFL trained models M(D), thus causing different certified K.
The above interesting results indicate that our proposed robustness certifications can serve as new
metrics to benchmark the robustness of different DPFL algorithms, which can potentially motivate the
investigation for better DPFL algorithms (i.e., different types of noise, clipping methods, subsampling
strategies, or even different FL training protocols). We believe these analyses can provide new and
important insights to the FL community.
A.4 Proofs of Certified Robustness Analysis
We restate our Lemma 1 here.
k
Lemma 1 (Group DP). For mechanism M that satisfies (e, δ)-DP, it satisfies (ke, 1-^ δ)-DPfor
groups of size k. That is, for any d, d0 ∈ D that differ by k individuals, and any E ⊆ Θ it holds that
Pr[M(d) ∈ E] ≤ eke Pr [M (d0) ∈ E] + ¾-k1 δ.
Proof. We denote d as d0, d0 as dk. di differ i individuals with d0. For any i ∈ [1, k], di and di-1
differ by one individual, thus
Pr[M(di-1)] ≤ e Pr[M(di)] +δ.	(7)
By iteratively applying Eq. (7) k times, we have
Pr[M(d0)] ≤ ek Pr[M(dk)] +(1+e+e2 + . .. + e(k-1))δ
1 ek
=ek'Pr[M (dk)] + --r δ
1 - e
□
Before we prove Theorem 1, we introduce the following lemma:
of noise reasonable and preserve good model utility. As more local epoch leads to larger norm of model updates,
we set the local epoch as 1 to keep the median norm small.
25
Under review as a conference paper at ICLR 2022
Lemma 3. Suppose a randomized mechanism M satisfies user-level (, δ)-DP. For two user sets B
and B0 that differ by one user, D and D0 are the corresponding training datasets. For a test input x,
for any c ∈ [C] , fc(M(D), x) ∈ [0, 1] is the class confidence, then the expected class confidence
Fc(M(D), x) := E[fc(M(D), x)] meets the following property:
Fc(M(D), x) ≤ eFc(M(D0), x) + δ	(8)
Proof. Define Θ(a) := {θ : fc(θ, x) > a}. Then
Fc(M(D), X)	E[fc(M(D), X)] =Z 1 P [fc(M(D), X) >a]da 0 =	P [M(D) ∈ Θ(a)] da 0 ≤	1 (eP [M(D0) ∈ Θ(a)] +δ)da 0 =	eP [fc(M(D0), X) > a]da+	δda = eFc(M(D0), X) + δ
□
We recall Theorem 1.
Theorem 1 (Condition for Certified Prediction under One Adversarial User). Suppose a randomized
mechanism M satisfies user-level (, δ)-DP. For two user sets B and B0 that differ by one user, let
D and D0 be the corresponding training datasets. For a test input x, suppose A, B ∈ [C] satisfy
A = arg maxc∈[c] Fc(M(D), x) and B = argmaxyc]：C=A Fc(M(D), x), then if
FA(M(D), x) > e2FB(M(D), x) + (1 +e)δ,	(1)
it is guaranteed that H (M(D0), x) = H (M(D), x) = A.
Proof. According to Lemma 3,
FA(M(D), x) ≤ eFA(M(D0),x)+δ	(9)
FB(M(D0), x) ≤ eFB(M(D), x) + δ.	(10)
Then
FA(M(D0),x) ≥ FA(M(D),X)——δ	(BecaUseofEq.9)
e
≥ e FB(M(D),x) + (1 + e )δ_δ	(Because of the given condition Eq. 1)
e
= eFB(M(D), x) + δ
≥ 1 (FB(M(D? X)- δ) + δ	(Because ofEq.10)
= FB(M(D0), x),
which indicates that the prediction of M (D0) at X is A by definition.	□
Before we prove Theorem 2, we introduce the following lemma:
Lemma 4. Suppose a randomized mechanism M satisfies user-level (, δ)-DP. For two user sets
B and B0 that differ k users, D and D0 are the corresponding training datasets. For a test input X,
for any c ∈ [C] , fc(M(D), X) ∈ [0, 1] is the class confidence, then the expected class confidence
Fc(M(D), X) := E[fc(M(D), X)] meets the following property:
1	ek
Fc(M(D),x) ≤ efceFc(M(D0),x) + -~rδ	(11)
1- e
26
Under review as a conference paper at ICLR 2022
Proof. Define Θ(a) := {θ : fc(θ, x) > a}. Then
Fc(M(D), x) =Z 1 P [fc(M(D), x) >a]da
0
=	1 P [M(D) ∈ Θ(a)] da
0
≤
1 eke
eP [M(D0) ∈ Θ(a)] +--------δ da
1 - ee
(Because of Group DP property in Lemma 1)
Z ekeP [fc(M(D0),x) > a] da + Z 1 - ek" δda
0	0 1 - ee
1 eke
eke Fc(M(D'),x) + η~r δ
1 - ee
□
We recall Theorem 2.
Theorem 2 (Upper Bound of k for Certified Prediction). Suppose a randomized mechanism
M satisfies user-level (, δ)-DP. For two user sets B and B0 that differ by k users, let D and
D0 be the corresponding training datasets. For a test input x, suppose A, B ∈ [C] satisfy
A = arg maxc∈[c] Fc(M(D),x) and B = argmaxyc]：C=A Fc(M(D),x), then H(M(D0),x)=
H (M(D), x) = A, ∀k < K where K is the certified number of adversarial users:
K = L	FA(M(D),x)(e'- 1)+ δ
=2 og FB(M(D),x)d - 1)+ δ
(2)
Proof. According to Lemma 4, we have
FA(M(D), x) ≤ ekFA(M(D0), x) +
FB(M(D0), x) ≤ ekFB(M(D), x) +
1 - eke
--------δ
1 - ee
1 - eke
--------δ.
1 - ee
(12)
(13)
We can re-write the given condition k < K according to Eq. (2) as
1 eke
e2keFB(M(D),x) + (1 + eke)-_-δ < FA(M(D),x).
1 - ee
(14)
Then
FA(M(D0), x) ≥
FA(M(D),x) - = δ
ek
(Because of Eq. 12)
e2k
>
"FB(M(D),x) + (1 + eke) J
1-ee
δ-
1-ek
1-ee
ek
(Because of the given condition Eq.14)
k
δ
1 eke
ekeFB(M(D),x) + -~- δ
1 - ee
≥ ek
FB(M(D0),x) -
ek
1-ek
1-ee
1 - ek
+ τ-百 δ
(Because of Eq. 13)
δ
= FB(M(D0), x),
which indicates that the prediction of M(D0) at x is A by definition.
□
We recall Theorem 3.
27
Under review as a conference paper at ICLR 2022
Theorem 3 (Attack Cost with k Attackers). Suppose a randomized mechanism M satisfies user-level
(e, δ)-DP. For two user sets B and B! that differ k users, D and D! are the corresponding training
datasets. Let J(D) be the expected attack cost where ∖C(∙)∣ ≤ C. Then,
一 7” . .	ek — 1	. ，.	一 U. . .	1 — e~ke _ .	..
miπ{efceJ(D) +---------δC, C} ≥ J(D0) ≥ max{e-ke J(D)--------------δC, 0}, if C(∙) ≥ 0
ee — 1	ee — 1	(3)
・ “ -	1 - β-fce  .........................	.	eke 一 1 一一	一	()
miπ{e-fceJ(D) +----------δC, 0} ≥ J(D) ≥ max{ekeJ(D)-------------SC, -C}, if C(∙) < 0
Proof. We first consider C(∙) ≥ 0. Define Θ(a) = {θ : C(θ) > a}.
C
J (D) = I'
Jo
P [C(M(D)) > a] da
Z° P [M(D)) ∈ Θ(a)] da
Jo
≤
一 ...............  1	— eke ∖
eP [M(D0)) ∈ Θ(a)] +  -------δ da
1 — ee )
(Because of Group DP property in Lemma 1)
o
/
C
ekeP [M(D0)) ∈ Θ(a)] da +
C
ekeP [C(M(D0)) > a] da +
1 _ eke
1-^- δC
1 — ee
1 - eke _
------δC
1 — ee
,	.	1 —eke .
ekeJ (Dr) +  ----δC
1 — ee
i.e.,
, j	1 — e-ke .
J(D0) ≥ e-keJ(D)--------------δC.
ee — 1
Switch the role of D and D0, we have
. j	1 - eke ―
J(D0) ≤ ekeJ(D) +----------δC.
1	— ee
Also note that 0 ≤ J(D0) ≤ C trivially holds due to 0 ≤ C(∙) ≤ C, thus
一.	eke — 1 ——	.	.	-	1 -e-ke — 一
min{eke J(D) +--------δC, C} ≥ J(D0) ≥ max{e-keJ(D)---------------δC, 0}.
ee — 1	ee — 1
Next we consider C(∙) ≤ 0. Define Θ(a) = {θ : C(θ) < a}.
J(D) = — Z P [C(M(D)) < a] da
J-C
=—Z P [M(D)) ∈ Θ(a)] da
J-C
≥ — /o_ (ekeP [M(D0)) ∈ Θ(a)] + 1--ekɪδ) da
(Because of Group DP property in Lemma 1)
Z0	,	1 - eke -
ekeP [M(D0)) ∈ Θ(a)] da----------δC
一 C	1 — ee
0	1	eke
— ekeP [C(M(D0)) < a] da —-——WδC
J-C	1 — ee
,	1 - Pke
ekeJ (D0)---------δC
1 — ee
28
Under review as a conference paper at ICLR 2022
i.e.,
1 e-k
J(D0) ≤ e-keJ(D) + ————pδ(C.
Switch the role of D and D0, we have
1 ek
J(D0) ≥ ekeJ(D)---------δC.
1 - e
Also note that -C ≤ J(D0) ≤ 0 trivially holds due to -C ≤ C(∙) ≤ 0, thus
1	e-k	ek	1
min{e-kJ(D) + 下一厂δC, 0} ≥ J(D0) ≥ max{ekJ(D)-三一-δC, -C}
□
We recall Corollary 1.
Corollary 1 (Lower Bound of k Given τ). Suppose a randomized mechanism M satisfies user-
level (e, δ)-DP Let attack cost function be C ,the expected attack cost be J (∙). In order to achieve
J(Dr) ≤ 1J(D) for τ ≥ 1 when 0 ≤ C(∙) ≤ C, OraChieve J(D0) ≤ τJ(D) for 1 ≤ T ≤ — JCD)
when -C ≤ C (∙) ≤ 0, the number of adversarial USerS should satisfy:
k ≥ 1log" - IJ(D)τ + Cδτ or k ≥ 1log d - IJ(D)T-C respectively. (4)
≥ e g (e - 1) J (D) + Cδτ	≥ eg (e - 1) J (D) - Cδ P y ()
Proof. We first consider C(∙) ≥ 0. According to the lower bound in Theorem 3, when B0 and
B differ k users, J(D0) ≥ e-keJ(D) - 1--kδC. Since We require J(D0) ≤ 1J(D), then
e~ke J(D) - 1-e-； δC ≤ 1J(D). Rearranging gives the result.
Next, we consider C(∙) ≤ 0. According to the lower bound in Theorem 3, when B0 and B differ k
users, J (D0) ≥ eke J (D)-匕丁； δC. Since we require J (D0) ≤ τJ (D),then eke J (D)-晨二；δC ≤
τJ(D). Rearranging gives the result.
□
We note that all the above robustness certification related proofs are built upon the user-level (, δ)-DP
property and the Group DP property. According to Definition 2 and Definition 3, the definition of
user-level DP and instance-level DP are both induced from DP (Definition 1) despite the different
definitions of adjacent datasets. By applying the definition of instance-level (, δ)-DP and following
the proof steps of Theorem 1, 2, 3 and Corollary 1, we can derive the similar theoretical conclusions
for instance-level DP, leading to Theorem 5 to achieve the certifiably robsut FL for free given the DP
property.
A.5 Comparison to (Lecuyer et al., 2019a; Ma et al., 2019)
In this section, we summarize our differences in terms of the relationship between DP and robustness
compared to (Lecuyer et al., 2019a; Ma et al., 2019).
(Lecuyer et al., 2019a) work on certified prediction against test-time attacks while we study DP
against training-time poisoning attacks in FL. We would like to emphasize that (Lecuyer et al., 2019a)
aim to make the classification process Pixel-DP while the training algorithm itself does not satisfy
DP, so it cannot directly build the relationship between DP and training-time certified robustness.
•	Conceptually, in contrast to the connection between Pixel-DP and certified prediction against
adversarial examples, our proposed analysis on the connection between DP and certified
robustness against data poisoning is new.
•	Technically, Pixel-DP adds noise on test data samples during testing once while we add
noises in updates or gradient at every training round. Although the analysis of Pixel-DP
and ours are both rooted in the intuition of DP definition, Pixel-DP requires the classifier
to make randomized predictions during testing and the training algorithm itself does not
29
Under review as a conference paper at ICLR 2022
satisfy DP. In contrast, our defense against data poisoning requires the learning algorithm,
rather than the classification process as in Pixel-DP, to be randomized. We add noise on
the updates (user-level DP) or gradient (instance-level DP) over every training round to
make the trained FL model satisfy DP, which requires careful privacy budget analysis of the
DPFL model over multiple training rounds considering the distributed nature and the model
training dynamics.
• Empirically, we explicitly evaluate the relationship between the privacy protection level
and the certified robustness based on two robustness criteria on three datasets, indicating the
fundamental connections quantitatively. Moreover, we compare the certified robustness of
different existing user-level DPFL algorithms.
(Ma et al., 2019) analyze the lower bound of certified cost in centralized DP learning while we use the
certified cost as one of our certification criteria for different definitions of DP in FL and additionally
derive its upper bounds.
30