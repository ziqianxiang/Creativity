Under review as a conference paper at ICLR 2022
Differentiable Hyper-parameter Optimization
Anonymous authors
Paper under double-blind review
Ab stract
Hyper-parameters are widely present in machine learning. Concretely, large
amount of hyper-parameters exist in network layers, such as kernel size, channel
size and the hidden layer size, which directly affect performance of the model.
Thus, hyper-parameter optimization is crucial for machine learning. Current
hyper-parameter optimization always requires multiple training sessions, result-
ing in a large time consuming. To solve this problem, we propose a method to
fine-tune neural network’s hyper-parameters efficiently in this paper, where op-
timization completes in only one training session. We apply our method for the
optimization of various neural network layers’ hyper-parameters and compare it
with multiple benchmark hyper-parameter optimization models. Experimental re-
sults show that our method is commonly 10 times faster than traditional and main-
stream methods such as random search, Bayesian optimization and many other
state-of-art models. It also achieves higher quality hyper-parameters with better
accuracy and stronger stability.
1 Introduction
HPO(hyper-parameter optimization) is one of the most critical parts in auto-ML (Thornton et al.,
2012; Domhan et al., 2015; Kotthoff et al., 2017). Simple and low-dimensional hyper-parameters
can be adjusted manually. It is also practical to use grid search or combine grid search with manual
adjustment to deal with this simple problem (Montavon et al., 2012; gri, 2007; Hinton, 2010).
With the number of hyper-parameters continues increasing, manual tuning and grid search get inef-
fective. For a slightly large neural network model, We can use BO(bayesian optimization) (SnOek
et al., 2012) or ZOOpt(zeroth-order optimization) (LiU et al., 2018) to optimize hyper-parameters.
BO uses Gaussian process regreSSiOnIo fit mean and variance of objective function. However, a
large number of matrix operations are needed in the fitting process, and multiple training sessions
are needed to evaluate the predicted hyper-parameters. Time consuming increases along with each
training session, and the total time consuming is positively correlated with the number of training
sessions. Therefore, BO is still not suitable for large amount of hyper-parameters and so is ZOOpt.
In addition to BO and ZOOpt, many evolutionary algorithms (Young et al., 2015) have also been
applied. For example, genetic algorithm (Goldberg) is often used for HPO. Evolutionary algorithms
can often avoid the problem of local optimal. However, a fatal flaw is the low time efficiency.
When dealing with large-scale systems, random search (ran, 2012) is a practical and more efficient
method to solve HPO. Based on experiences, random search may perform better than BO in some
cases with better time efficiency and accuracy. In addition, currently very effective HPO methods
are probably HB(Hyperband (Li et al., 2016) and DEHB (Awad et al., 2021). They accelerate the
convergence and make it twice faster than random search.
Totally, all current models have a common defect which is a requirement for multiple training ses-
sions of the fine-tuned model. While each training session is always expensive in time consuming.
To get rid of this limit, we propose DHPO(differentiable hyper-parameter optimization), which ac-
complish HPO within one training session. Concretely, we change the form of hyper-parameters and
include them in the calculation of forward propagation to achieve differentiable hyper-parameters.
In this way, hyper-parameters are differentiable with the goal of minimize the loss function. When
the session is over, hyper-parameters complete optimization together with network’s parameters.
Therefore, only one training session is needed in DHPO.
1
Under review as a conference paper at ICLR 2022
The contributions of this paper are summarized as follows.
•	In this paper, we propose DHPO, which is the first model to solve hyper-parameter optimization
in only one training session.
•	The proposed approach is universal. It is an idea for HPO which can be applied to various kinds
of hyper-parameters in neural network.
•	We conduct extensive experiments on various kinds of neural network layers compared with mul-
tiple benchmark HPO models to evaluate the extreme efficiency and high performance of DHPO.
In the remaining of this paper, Section 3 describes the proposed method. Experiments are conducted
and analyzed in Section 4. We overview related work in Section 5. Section 6 draws the conclusions
and future work.
2	Background
Figure 1: Select an activation with Darts
In this section, we introduce related techniques briefly.
Darts (Liu et al., 2019) is used to solve automatic network architecture search, which is de-
scribed to determine the best operation from multiple candidate operators locally and globally.
Take Figure 1 as example. It is selecting an activation for the fully connected layer from
{T anh, Sigmoid, Relu, S of tmax} and supposing Relu is the best choice. Grid search costs
four training sessions to evaluate each activation. While Darts can select the hyper-parameters
in a single training session. Firstly, Darts calculates features X transformed by the fully con-
nected layer and then uses each activation to process X once to get four non-linear features, de-
noted as o = [o1 , o2, o3, o4]. Further, sof tmax(α) assigns a weight to each oi and aggregates
weighted oi by summing them, where αi is a trainable parameter. An ideal train always ends up
with max sof tmax(α) → 1 as shown in Figure 1. Darts in Figure 1 finally determines Relu as the
optimal activation with sof tmax( alpha)3 very close to 1. Accordingly, sof tmax(α)i ≈ 0, i 6= 3
meaning a shield to the information contained in o1 , o2 , o4, which is equivalent to using Relu only.
3	Method
As introduced in the background, Darts still has fatal defects. On the one hand, max sof tmax(α)
doesn’t always converge to a level very close to 1. The cases where multiple candidates occupy
2
Under review as a conference paper at ICLR 2022
similar weights always exist especially for a large amount of candidates, which means Darts fails
to distinguish different candidates and hit the optimal. On the other hand, Darts is only suitable
for hyper-parameter with independent candidates. For example, the channel size is beyond Darts’s
capability. If we’d like to apply Darts to solve channel size, we should set a candidate for each
possible value of channel size. Then there will be O(n2) channels in total, which is space expensive.
Motivated by this, we propose DHPO, aiming at solving all these problems existing in Darts and
current HPO models. In the following, we first take the optimization of convolution layer’s channel
size as example to draw the core of DHPO in Section 3.1. Then, we apply DHPO to more hyper-
parameters in Section 3.2.
3.1	Differentiable hyper-parameter optimization
The target of DHPO is to make hyper-parameter θ differentiable, and we achieve it through con-
structing trainable parameter α to substitute θ in the training. Obviously, the core of the idea is to
point out the limitations for α and give out a universal method to construct and apply α.
In this section, we first declare the sufficient conditions for α to substitute θ based on Theorem 1
in Section 3.1.1. Then we explain how to construct α and use it to control the structure of neural
network, namely that is the forward propagation under α in Section 3.1.2. In the end, we use
Theorem 1 to prove that α is able to substitute θ as Theorem 2 in Section 3.1.3.
3.1.1	LIMITATIONS FOR α
Our target is to construct trainable parameter α to substitute θ in the training. We will point out the
sufficient conditions for α in the following.
Firstly, we define found sufficient conditions as a new relation between α and θ, which is defined as
expressible as Definition 3.1. Then, we prove that if α is expressible for θ, then we can replace θ
with α in neural network as Theorem 1. The definition and theorem are as follows.
Definition 3.1. Expressible: For a neural network F : Rd 7→ R with W and Θ as parameters
and hyper-parameters respectively, hyper-parameter θ2 ∈ Ω2 is expressible for hyper-parameter
θι ∈ Ωι if and only if there is a Surjective h : Ω2 → Ω1(θ1 = h(θ2)) which makes network
structure under F(W ∣θι) and that under F(W ∣Θ2) are the same.
Theorem 1. Ifθ2 is expressible for θ1, then we can replace θ1 with θ2 in neural network.
Proof. If θ2 is expressible for θι, then there is a surjective from θ to θι. Therefore, ∀θι ∈ Ωι, ∃θ2 ∈
Ω2, h(θ2) = θι. Meanwhile, F(W∣Θ2) and F(W∣θι) share the same structures according to Defi-
nition 3.1. So we can replace θι with θ? in a neural network.	□
According to Theorem 1 and Definition 3.1, we can replace θ with α in neural network as long as
the constructed α satisfies the following two conditions. One is that there is a surjective θ = h(α).
The other is that the network structure controlled by α is the same to that of θ under θ = h(α).
3.1.2	FORWARD PROPAGATION UNDER α
In this section, we first propose the method to construct expressible α as described above, and then
explain how α controls the structure of neural network, i.e., applying α in the forward propagation.
sup(θ)
Φ(X∣α) = Eei * (X?Ki)
(1)
e = σ((softmax(α)) ∙ A — σ(β)) * a)
We take convolution layer’s channel size as example to construct trainable α, which should be
expressible for θchanneLsize(abbreviated as θ). We construct α as α = [αι, ∙∙∙ ,。§乜？(6)] and
α ∈ Rsup(θ), sup(θ) = max θ. Formula 1 describes the forward propagation in this convolu-
tion layer under α. X ∈ RW×H is the input of convolution layer which is a single-channel image.
κi represents the convolution kernel of i-th channel, and we prepare sup(θ) candidate kernels in
the initialization of neural network. (?) represents convolution operation and (∙) represents matrix
3
Under review as a conference paper at ICLR 2022
1
multiplication. A =
0
a ∈ R is a large constant.
1T
..	is a lower triangular matrix. γ ∈ R is a trainable parameter and
.
1
Obviously, the core of formula 1 is the calculation of . We visualize the calculation in Figure 2.
Two cases are given in this figure. We take the left as example. α is first transformed by softmax
activation and 0 < softmax(α)i < 1. Then we get A by multiplying matrix sof tmax(α) and
matrix A, A1 = 1 and Ai > Aj if i > j. Next, we subtract σ(γ) from Ai and multiply it by a large
constant a, a > 1. In this time, -a < (Ai - σ(γ)) * a < a, and it maintains the same monotonicity
as A. In the last step, We use σ again to map (Ai - σ(γ)) * a to a value close to 1 or 0. Finally,
i = σ((Ai - σ(γ)) * a). In this example, 1, 2, 3 are very close to 1, and the others are very close
to 0. In this occasion, We think that the channel size is 3.
Figure 2: ForWard propagation under α in DHPO.
Based on the above construction, a has the same effect as θchanneisize. Thus, it is reasonable to
substitute Ochanneksize With a. We will prove this in the following Section 3.1.3.
3.1.3	EXPRESSIBLE FOR θ
The goal is to prove that we can substitute Ochanneisize with a. According to Theorem 1, we just
need to prove that α is expressible for θchannelsize. Before proof, we first list three key properties
of in the following which are necessary to the proof.
E is constructed from α and has three key characteristics.① eι ≈ 1.② eι > …> tsup(θ).③ There
is an index t dividing into two sets εbig = {i|1 ≤ i ≤ t} and εsmall = {i|t + 1 ≤ i ≤ sup(O)}.
The items in εbig are all close to 1 and items in εsmall are all close to 0. The first property ensures
that at least one channel is selected. The second property ensures the selected channels are always
the first few rather than random several in candidate channels. The third property makes unselected
channels blocked away, especially when a 7→ ∞. Based on these features, we have Theorem 2.
Theorem 2. When a 7→ ∞, α is expressible for O under h(α) = rounded	E.
Proof. If we would like to prove α is expressible for O, we just need to prove h(α) is surjective
and network structure controlled by α is the same to that of O under O = h(α) according to the
discussion in Section 3.1.1.
Obviously, O = h(α) is a surjective. We then prove convolution layers under O and α have same
4
Under review as a conference paper at ICLR 2022
structure. Suppose that the network structure controlled by θ is a convolution layer composed of
the first θ channels from candidates. As for the structure controlled by α, we can also see it as a
convolution layer composed of the first θ channels according to the third characteristic of . Because
noise from unselected channels will fade out with a → ∞. At this time, F(X∣α) ⇔ F(X∣θ). □
Based on above discussions, we finally achieve the differentiable channel size. Since α is trainable
and expressible for θchanneisize. Then we can replace θchannelsize with α in this convolution layer.
When the training session is over, We take rounded P E as the optimized θchannellize.
3.2	Multiple hyper-parameters optimization
Based on the idea discussed in Section 3.1, we can apply DHPO to a large amount of hyper-
parameters rather than just channel size.
In this section, we select two widely used and typical hyper-parameters to explain how to deal with
them in DHPO, and other hyper-parameters can be processed in a similar way. These typical hyper-
parameters are convolution kernel size and hidden layer size.
3.2.1	kernel size
In this section, we introduce method of making kernel size differentiable. The challenge here is
that we should deal with multi-dimensional hyper-parameter. Therefore, we should make DHPO
suitable for multi-dimensional hyper-parameter rather than just one dimensional hyper-parameter
like channel size. We achieve this by extending E to multiple dimensions and the detail is as follows.
Firstly, we construct E from α as described in Section 3.1 and a candidate kernel κ. For the one-
dimensional kernel, K ∈ Rsup("kernel-size)(0kernei_Size abbreviated as θ). And We use K Θ E as the
kernel in forward propagation. For the 2-dimensional kernel κ ∈ Rsup(θ)×sup(θ), we use κ M as
	1	∈2	∙	• ^sup(θ)	
	∈2	∈2	∙	• ^sup(θ) . .. ..	
selected kernel, M =	. .	.. .		.In this way, the features from unselected
	. sup(θ)	. ^sup(θ)	∙	• •	sup(θ)	
items i are blocked away since i ≈ 0.
Then α is expressible for θ under h(α) = rounded P E. Obviously, h(α) is a surjective. When
the large constant a → ∞. In this time, kernel size under θ and that under α are the same under
θ = h(α). So We can substitute Bkerneksize with α in neural network. When the training session is
over, we take rounded P E as the optimized BkerneLsize.
Based on convolution kernel size, we can design differentiable hyper-parameter for pooling kernel
size, convolution stride and etc. in a similar way.
3.2.2	hidden layer size
In this section, we apply DHPO to one of the most common and important hyper-parameters, hidden
layer size. Commonly, the hidden layer size has thousands of candidates. Therefore, it is difficult
for sample-based models such as BO and random search. In contrast, DHPO can solve it easily both
theoretically and practically since hidden layer size is differentiable in DSA.
Again, we first construct E from α as described above and use E to control the hidden layer size. We
use (X ∙ W + b) Θ E as the output of the fully connected layer. (X ∙ W + b) is the original output and
its dimension is sup(θhiddensize). In this way, unselected last few features will be blocked away,
which is equivalent to use a smaller hidden layer size. When the training session is over, we take
rounded P E as optimized Bhiddensize.
4	Experiment
To verify the effectiveness of the proposed approach, we conduct HPO on two different neural net-
works with multiple datasets, and compare it with various baselines. Section 4.1 briefly introduces
5
Under review as a conference paper at ICLR 2022
the neural network to be tuned, Section 4.2 gives the basic settings, Section 4.3 displays the experi-
mental results, and Section 4.4 conducts a case study to show the DHPO tuning process.
4.1 Model to be tuned
Figure 3: Deep convolution neural network to be tuned
In the experiment, we apply DHPO to two classical models, i.e. deep convolutional neural networks
and MLp(multi-layer perceptron), which include various kinds of hyper-parameters. DNN(deep
convolution neural network) is a lightweight convolution network designed with reference to Kuni-
hiko & Fukushima (1980); Lecun & Bottou (1998); Behnke & Sven (2003). Even though DNN is
not the best model to process images, it is a good network which can distinguish the performence of
various HPO approaches since it has a large number of hyper-parameters. DNN’s network structure
is as shown in Figure 3, which is a lightweight implementation. And MLP is composed of 4 fully
connected layers. DNN is used to evaluate HPO for convolution layer and pooling layer. MLP is
used to evaluate HPO for fully connected layer.
4.2	Experimental settings
In this section, we introduce the experimental settings.
4.2	. 1 dataset
MNIST (LeCun & Cortes, 2010) and SVHN (svh, 2011) are well known lightweight validation im-
age datasets, and they are suitable for DNN. Therefore, we use MNIST and SVHN as the benchmark
datasets on DNN. On MLP, we choose iris, wine, car and agaricus-lepiota1. Basic message of the
six datasets is shown in Appendix A.
4.2.2	baseline
The mainstream algorithms used for HPO contains random search, BO, evolutionary algorithm and
so on. We finally choose the following HPO algorithm as baselines:
•	Random search (ran, 2012) is the most universal HPO algorithm, which can randomly select
points in the entire search space. So it is able to break the limit of local optimal solution.
•	Zoopt (Liu et al., 2018) does not rely on the gradient of the objective function, but learns from
samples of the search space instead. It is suitable for HPO tasks that are not differentiable, with
many local minimal or even unknown but only testable.
•	Bayes optimization (Snoek et al., 2012) is based on Gaussian process regression to fit the distri-
bution between samples and objective. It has a practical effect and wide range of applications.
•	Genetic algorithm (Goldberg) belongs to evolutionary algorithm, relying on the mutation and
genetic of maintained population. It is often used in HPO and NAS(network architecture search).
•	Particle swarm algorithm (Kennedy & Eberhart, 1995) also belongs to evolutionary algorithm,
which finds the optimal solution through collaboration and information sharing between individ-
uals in the group. Simple implementation, few hyperparameters, and wide applications.
•	HyperBand (Li et al., 2016) regards the process of finding the optimal hyper-parameters as a
non-random exploration on the infinite arm bandit under the condition of limited resources.
•	DEHB (Awad et al., 2021) is developed based on differential evolution and HB, which is furnished
with better time performance and convergence effect.
1https://archive.ics.uci.edu/ml/datasets
6
Under review as a conference paper at ICLR 2022
4.2.3	metrics
•	Time consuming reflects the time interval from when data set and model are loaded on the GPU
to when the algorithm stops. If data set is loaded multiple times, the loading time is also included.
•	Accuracy reflects the performance of neural network generated by HPO models.
•	Mean of top-K accuracy. HPO model always gives multiple sets of hyper-parameters and corre-
sponding accuracy. We calculate the mean accuracy of top-K models.
•	Loss sequence reflects the state of objective and convergence effect in a training session.
4.2.4	parameter settings
Tuned hyper-parameters We fine-tune convolution layer’s channel size, convolution layer’s ker-
nel size, pooling layer’s pooling type and pooling layer’s kernel size in DNN model, totally 16
hyper-parameters. Among them, channel size ∈ [1, 8], kernel size ∈ [2, 5], pooling type ∈
[M axP ool, AvgP ool]2. Additionally, 3 hidden layer size are optimized in MLP.
DNN We uses the Adam optimizer with learning rate = 0.001 and uses cross-entropy loss function.
batch size = 500 and each training session contains 30 epochs. MLP All are the same to DNN
except for 150 epochs in training session.
BO BO model is initialized with 5 samples, and continues iterating for 25 rounds. ZOOpt Iteration
budgets is 30, and other hyper-parameters use the built-in configurations.3 Random search The
algorithm iterates 30 times. Evolutionary algorithm The population size and iterations are both 10,
and mutation probability is 0.001. HB Iterate 81 times to keep fair with other models, because of its
different iteration principles. DEHB We set min_budget and max_budget as 2 and 50, respectively.
Considering the influence of parameter’s initialization, a three-times repeated independent experi-
ment is conducted and our experiments are conducted on one GTX 3060Ti GPU.
4.3	Experimental results
Table 1: Time consuming and accuracy on MNIST and SVHN. Column 3-9 corresponds to various
statistical accuracy(%). ? denotes that We repeat DHPO 10 times, and f represents a once DHPO
with three repeated independent experiments. BLANK corresponds to the blank group without any
hyper-parameter optimization by setting all the channel size to 8 and set all the kernel size to 3.
MODEL	TIME(s)	TOP1	ALL MEAN	ALL STD MNIST	TOP5 MEAN	TOP5 STD	TOP10 MEAN	TOP10 STD
BO	6,298	98.66	97.32	1.14	98.51	0.11	98.33	0.2
ZOOpt	6,320	98.56	98.05	0.6	98.47	0.05	98.41	0.08
RAND	6,308	98.45	95.81	2.58	98.1	0.19	97.78	0.39
HB	13,918	96.17	82.82	13.04	98.44	0.05	98.37	0.08
DEHB	6,073	98.37	96.7	1.58	98.19	0.11	97.95	0.28
DHPO?	1,708	99.01	98.57	0.19	98.7	0.05	98.57	0.19
DHPOt	512	98.71						
BLANK		98.27		SVHN				
BO	11,043	87.29	83.3	3.95	86.81	0.27	86.49	0.38
ZOOpt	10,215	85.53	82.73	2.91	85.43	0.08	85.09	0.4
RAND	10,193	86.18	79.81	3.81	85.37	0.87	84.12	1.45
DEHB	10,194	85.61	81.18	4.51	85.15	0.28	84.45	0.73
DHPO?	2,811	87.44	85.22	1.83	86.78	0.53	85.22	1.83
DHPOt	1,070	85.05						
BLANK		84.95						
As shoWn in Table 1, DHPO can alWays complete tasks faster than any other HPO model several
times With ensuring good performance. Taking MNIST as example, DHPO? Wins the game by
taking only 28% of DEHB’s time consuming. Additionally, DHPOf can also ensure the advantage
in accuracy With feWer repeats, Which means DHPO is stable. The stability can also be reflected
from top-K accuracy’s standard deviation, Which is only 0.19 for all samples under DHPO.
2In DHPO, We invoke Darts to solve pooling type and We have proved it’s effective here in Appendix B
3https://github.com/polixir/ZOOpt
7
Under review as a conference paper at ICLR 2022
Table 2: Time consuming and accuracy on small feature datasets.
MODEL	IRIS		WINE		CAR		AGARICUS	
	TIME(s)	TOP1	TIME(s)	TOP1	TIME(s)	TOP1	TIME(s)	TOP1
BO	45	100	46	99.07	52	93.26	69	100
ZOOpt	44	100	45	100	47	92.87	71	100
RAND	38	100	45	100	48	93.16	64	100
GA	148	98.89	151	96.3	171	93.55	274	100
PSO	142	100	145	99.07	171	93.35	167	100
HB	24	100	24	87.04	26	69.36	40	99.94
DEHB	48	100	45	100	49	93.26	59	100
DHPO?	9	100	9	100	10	92.49	16	100
DHPOt	3	100	3	96.3	3	91.43	5	100
On feature datasets shown in Table 2, DHPO also has absolute advantage in efficiency. While in
some cases its accuracy is not the best compared to baselines. This is because DHPO trains the
network while optimizing hyper-parameters resulting in a need for more epochs than other models.
In this experiment, we set all epochs to be consistent in order to ensure the fairness of the experiment.
AGARICUS
MNIST
SVHN
Figure 4: Loss sequence in the training session
As shown in Figure 4, DHPO shows a better loss sequence on MNIST, SVHN and AGARICUS. It
can be seen that the loss of DHPO is not dominant at the beginning, and then slowly drops to the
lowest of all models. This corresponds to the two stages of DHPO in the actual training process.
One is the hyper-parameter optimization stage, and the other is the training stage. DHPO is always
looking for the best hyper-parameters in the beginning, which caused the loss dropping very slowly.
Later, the optimal hyper-parameters are fixed, and the loss drops quickly then. However, the epochs
are too small for WINE and CAR. As a result, the second stage has not been conducted enough yet.
As for IRIS, its distribution is simple. So a small number of epochs is satisfied.
Figure 5: Loss sequence comparison in 500 epochs training.
Further, we set EPOCH to 500 and conduct HPO on WINE again. The results and the loss sequence
are shown in Appendix E and Figure 5, respectively. It is easy to see from the figure that the DHPO
takes an absolute advantage in the later stage of training, and finally achieves a good performance.
8
Under review as a conference paper at ICLR 2022
4.4	Case study
In this case study, we track DHPO for a whole optimization with running DNN on MNIST by
recording hyper-parameters generated by DHPO after each epoch. And we observe whether hyper-
parameters are really optimized in DHPO.
Figure 6: case study on MNIST
In some singleton of DHPO, five groups of hyper-parameters are given out by DHPO along with the
training session. We evaluate these five groups of hyper-parameters on DNN chronologically and
visualize the performances in Figure 6. The later hyper-parameters have stronger convergence capa-
bilities according to the loss. Performance(accuracy mean) and stability(accuracy std) under these
hyper-parameters also shows an increasing trend. As the training session progresses, the quality of
hyper-parameters gradually increases, which demonstrates that DHPO is an effective HPO method.
5	Related work
From a large scale, our work belongs to automatic machine learning. With the emergence of large-
scale machine learning models, manual tuning (Montavon et al., 2012; gri, 2007) is no longer ap-
plicable. While random algorithm (ran, 2012) and grid search (Hinton, 2010) are ineffective with
the target of performance and space, respectively. Goldberg uses evolutionary algorithm to opti-
mize hyper-parameters. These algorithms have a good effect in exploring the distribution of the
objective function value with the sample points and jumping out of the local optimal solution if
we ignore the time consuming. A more stable optimization now belongs to zeroth-order optimiza-
tion (Liu et al., 2018) and Bayesian optimization algorithm (Snoek et al., 2012), whose algorithms
are complementary in principle. BO fits the distribution between samples and objective function
with Gaussian process regression. This is suitable for the case where objective is differentiable.
Zeroth-order optimization is more suitable for non-differentiable situations, and can solve the situa-
tion with multiple local optimal. However, these algorithms still need to repeatedly run the model to
be adjusted, which brings a large time consuming. In order to solve this problem, the HyperBand (Li
et al., 2016) changed the verification process. Epochs in a training session grow from 1 along with
iterations. And a part of samples is eliminated after each iteration. Only one sample survives in
the end. DEHB (Awad et al., 2021) combines the differential evolution and HyperBand for further
optimization. On the whole, all current HPO algorithms rely on multiple training sessions to obtain
feedback to adjust hyper-parameters. Only DHPO can optimize the hyper-parameters in the single
training session, which is undoubtedly a meaningful work for automatic machine learning.
6	CONCLUSION & FUTURE WORK
In this paper, DHPO solves the time efficiency problem of HPO, and we also mentioned that DHPO
is less affected by the initialization parameters. As long as a certain number of rounds, it can always
overcome some more difficult hyper-parameters. Furthermore, we plan to study the why to apply
DHPO on more hyper-parameters and more models rather than just neural network.
9
Under review as a conference paper at ICLR 2022
References
An empirical evaluation of deep architectures on problems with many factors of variation. In Inter-
national Conference on Machine Learning, 2007.
Reading digits in natural images with unsupervised feature learning. nips workshop on deep learning
& unsupervised feature learning, 2011.
Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13(1):
281-305, 2012.
N. Awad, N. Mallik, and F. Hutter. Dehb: Evolutionary hyberband for scalable, robust and efficient
hyperparameter optimization. 2021.
Behnke and Sven. Hierarchical neural networks for image interpretation. Springer,, 2003.
Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hyperparam-
eter optimization of deep neural networks by extrapolation of learning curves. In Twenty-Fourth
International Joint Conference on Artificial Intelligence, 2015.
D. E. Goldberg. Genetic algorithm in search.
G. Hinton. A practical guide to training restricted boltzmann machines[j]. Momentum, 9(1):926-
947, 2010.
J. Kennedy and R. Eberhart. Particle swarm optimization. In Icnn95-international Conference on
Neural Networks, 1995.
Lars Kotthoff, Chris Thornton, Holger H Hoos, Frank Hutter, and Kevin Leyton-Brown. Auto-
weka 2.0: Automatic model selection and hyperparameter optimization in weka. The Journal of
Machine Learning Research, 18(1):826-830, 2017.
Kunihiko and Fukushima. Neocognitron: A self-organizing neural network model for a mechanism
of pattern recognition unaffected by shift in position. Biological Cybernetics, 1980.
Y. Lecun and L. Bottou. Gradient-based learning applied to document recognition. Proceedings of
the IEEE, 86(11):2278-2324, 1998.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010.
L. Li, K. Jamieson, G. Desalvo, A. Rostamizadeh, and A. Talwalkar. Hyperband: A novel bandit-
based approach to hyperparameter optimization. Journal of Machine Learning Research, 18:
1-52, 2016.
Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: differentiable architecture search. In
ICLR. OpenReview.net, 2019.
Yu-Ren Liu, Yi-Qi Hu, Hong Qian, Yang Yu, and Chao Qian. Zoopt: Toolbox for derivative-free
optimization, 2018.
G Montavon, G B. Orr, and KR Muller. Eficientbackprop. 10.1007/978-3-642-35289-8(ChaPter
3):9-48, 2012.
Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine
learning algorithms. In Advances in neural information processing systems, pp. 2951-2959, 2012.
Chris Thornton, Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Auto-weka: Automated
selection and hyper-parameter optimization of classification algorithms. CoRR, abs/1208.3719,
2012.
Steven R Young, Derek C Rose, Thomas P Karnowski, Seung-Hwan Lim, and Robert M Patton.
Optimizing deep learning hyper-parameters through an evolutionary algorithm. In Proceedings
of the Workshop on Machine Learning in High-Performance Computing Environments, pp. 1-5,
2015.
10
Under review as a conference paper at ICLR 2022
A Meta information of datasets
Table 3: Dataset meta information
Dataset	#Train/ #Test	#Attributes	#Class
IRIS	120/30	4	3
WINE	142/ 36	13	3
CAR	1,382/ 346	6	4
AGARICUS	6,499 / 1,625	116	2
MNIST	60,000 /10,000	1*28*28	10
SVHN	73,257 /26,032	3*32*32	10
B Proof about Darts’s effectiveness when dealing with low
DIMENSIONAL CANDIDATES
Figure 7: The forward propagation of pooling layer
We take Maximum pooling and average pooling as candidates. We will prove Darts is effective in
this occasion. θpooitype ∈ {MaxPool, AvgPool}. α ∈ R2. According to Darts, the output of this
pooling layer is calculated as formula 2 visualized in Figure 7.
φ(X∣α)=	Tα1)/、* MaxPOol(X) +	JKa2	* AvgPooI(X)⑵
exp(α1) + exp(α2)	exp(α1) + exp(α2)
Theorem 3. When αi	αj , i 6= j, α is expressible for θ under h(α).
h(α) = POOL-CANDIDATES (argmax{a})
POOLCANDIDATES (0) = MaxPooI
POOLCANDIDATES (1) = AvgPooI
(3)
Proof. When αι》α2, according to formula 3, θτ,ooityτ)e = MaxPool.
a maximum pooling operation, namely f (X∣θ) = MaxPooI(X). Because
F(X∣θ) represents
exP(aI)	≈
exp(αι )+exp(α2)
1, exp(：Xp+aXP(a2) ≈ O, then f (X∣α) ≈ MaxPool(X). Assuming the proportion of error informa-
tion is Y Y = exp(aX P⅛P(ɑ2) . With a1》α2,γ → 0. Then f (X |a) ⇔ f (X lθ) under Y.
Proof is the same for α2》αι.	□
According to Theorem 1 and Theorem 3, as long as αi αj, Darts is effective then. If Y ≤ 0.001
is tolerant, then ∣αι — α21 > 6.9 can meet it. It is easy when α ∈ R2. So Darts is effective in this
occasion.
11
Under review as a conference paper at ICLR 2022
C Experiment result on small datasets
Table 4: Time consuming and accuracy on small feature datasets.
MODEL	TIME(s)	TOP1	ALL MEAN	ALL STD IRIS	TOP5 MEAN	TOP5 STD	TOP10 MEAN	TOP10 STD
BO	45	100	99.67	0.58	100	0	100	0
ZOOpt	44	100	99.56	0.74	100	0	100	0
RAND	38	100	98.67	5.96	100	0	100	0
GA	148	98.89	99.81	0.44	100	0	100	0
PSO	142	100	94.76	15.88	100	0	100	0
HB	24	100	46.54	24.72	100	0	100	0
DEHB	48	100	99.33	2.21	100	0	100	0
DHPO?	9	100	100	0	100	0	100	0
DHPOt	3	100		WINE				
BO	46	99.07	94.38	5.05	98.33	0.37	98.06	0.5
ZOOpt	45	100	95.77	3.68	98.89	0.69	98.43	0.72
RAND	45	100	95.56	3.88	99.07	0.59	98.33	0.91
GA	151	96.3	96.45	1.62	99.26	0.37	98.89	0.56
PSO	145	99.07	77.18	21.38	97.78	0.74	97.5	0.59
HB	24	87.04	51.96	16.63	90.37	1.99	88.52	2.63
DEHB	45	100	94.1	7.03	98.52	0.74	97.87	0.83
DHPO?	9	100	95.56	4.16	98.89	1.36	95.56	4.16
DHPOt	3	96.3		CAR				
BO	52	93.26	91.02	4.98	92.99	0.15	92.86	0.17
ZOOpt	47	92.87	92.18	0.44	92.72	0.12	92.58	0.17
RAND	48	93.16	91.57	1.64	92.85	0.22	92.63	0.27
GA	171	93.55	92.45	2.37	93.47	0.19	93.38	0.16
PSO	171	93.35	90.06	8.77	93.43	0.11	93.33	0.13
HB	26	69.36	67.44	9.01	87.38	1.61	80.41	8.31
DEHB	49	93.26	90.89	5.82	93.12	0.12	93.02	0.14
DHPO?	10	92.49	91.65	0.82	92.31	0.23	91.65	0.82
DHPOt	3	91.43						
			AGARICUS					
BO	69	100	98.23	6.81	100	0	100	0
ZOOpt	71	100	100	0	100	0	100	0
RAND	64	100	99.99	0.03	100	0	100	0
GA	274	100	100	0	100	0	100	0
PSO	167	100	90.74	16.42	100	0	100	0
HB	40	99.94	58.13	15.74	99.94	0	99.93	0.01
DEHB	59	100	100	0	100	0	100	0
DHPO?	16	100	100	0	100	0	100	0
DHPOt	5	100						
12
Under review as a conference paper at ICLR 2022
D Loss sequence on IRIS, WINE and CAR
E Result of 500 epoch training on WINE
	TIME	TOP1	TOP5	ALL
BO	153	99.07	98.7	96.98
ZOOpt	154	99.07	98.52	96.73
RAND	151	99.07	98.52	96.98
GA	556	99.07	99.63	97.27
DEHB	160	100	98.89	97.31
PSO	529	100	99.26	88.91
DHPO?	36	100	100	98.06
HB	24	87.04	90.37	51.96
DHPOt	10	97.22		
13