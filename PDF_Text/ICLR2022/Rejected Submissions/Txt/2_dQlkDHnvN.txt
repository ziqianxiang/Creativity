Under review as a conference paper at ICLR 2022
Defending Backdoor Data Poisoning Attacks
Using Noisy Label Defense Algorithm
Anonymous authors
Paper under double-blind review
Ab stract
Training deep neural networks with data corruption is a challenging problem. One
example of such corruption is the backdoor data poisoning attack, in which an ad-
versary strategically injects a backdoor trigger to a small fraction of the training
data to subtly compromise the training process. Consequently, the trained deep
neural network would misclassify testing examples that have been corrupted by
the same trigger. While the label of the data could be changed to arbitrary values
by an adversary, the extent of corruption injected to the feature values are strictly
limited in order to keep the backdoor attack in disguise, which leads to a resem-
blance between the backdoor attack and a milder attack that involves only noisy
labels. In this paper, we investigate an intriguing question: Can we leverage al-
gorithms that defend against noisy labels corruptions to defend against general
backdoor attacks? We first discuss the limitations of directly using the noisy-
label defense algorithms to defend against backdoor attacks. Next, we propose a
meta-algorithm that transforms an existing noisy label defense algorithm to one
that protects against backdoor attacks. Extensive experiments on different types
of backdoor attacks show that, by introducing a lightweight alteration for mini-
max optimization to the existing noisy-label defense algorithms, the robustness
against backdoor attacks can be substantially improved, while the intial form of
those algorithms would fail in presence of a backdoor attacks.
1	Introduction
Deep neural networks (DNN) have achieved significant success in a variety of applications such as
image classification (Krizhevsky et al., 2012), autonomous driving (Major et al., 2019), and natural
language processing (Devlin et al., 2018), due to its powerful generalization ability. In the mean-
time, DNN can be highly susceptible to even small perturbations of training data, which has raised
considerable concerns about its trustworthiness (Liu et al., 2020). One representative perturbation
approach is backdoor attack, which undermines the DNN performance by modifying a small fraction
of the training samples with specific triggers injected into their input features, whose ground-truth
labels are altered accordingly to be the attacker-specified ones. It is unlikely to detect backdoor
attacks by monitoring the model training performance, since the trained model can still perform
well on the benign validation samples. Consequently, during testing phase, if the data is augmented
with the trigger, it would be classified as the attacker-specified label. Subtle yet effective, backdoor
attacks can pose serious threats to the practical application of DNNs.
Another typical type of data poisoning attack is noisy label attacks (Han et al., 2018; Patrini et al.,
2017; Yi & Wu, 2019; Jiang et al., 2017), in which the labels of a small fraction of data are al-
tered deliberately to compromise the model learning, while the input features of the training data
remain untouched. Backdoor attacks share a close connection to noisy label attacks, in that during a
backdoor attack, the feature can only be altered insignificantly to put the trigger in disguise, which
makes the corrupted feature (e.g. images with the trigger) highly similar to the uncorrupted ones.
Prior efforts have been made to effectively address noisy label attacks. For instance, there are algo-
rithms that can tolerate a large fraction of label corruption, with up to 45% noisy labels (Han et al.,
2018; Jiang et al., 2018). However, to the best of our knowledge, most algorithms defending against
backdoor attacks cannot deal with a high corruption ratio even if the features of corrupted data are
only slightly perturbed. Observing the limitation of prior arts, we aim to answer one key question:
Can one train a deep neural network that is robust against a large number of backdoor attacks?
1
Under review as a conference paper at ICLR 2022
Moreover, given the resemblance between noisy label attacks and backdoor attacks, we also investi-
gate another intriguing question: Can one leverage algorithms initially designed for handling noisy
label attacks to defend against backdoor attacks more effectively?
The contributions of this paper are multi-fold. First, we provide a novel and principled perspective
to decouple the challenges of defending backdoor attacks into two components: one induced by
the corrupted input features, and the other induced by the corrupted labels, based on which we can
draw a theoretical connection between the noisy-label attacks and backdoor data attacks. Second,
we propose a meta-algorithm which addresses both challenges by a novel minimax optimization.
Specifically, the proposed approach takes any noisy-label defense algorithm as the input and outputs
a reinforced version of the algorithm that is robust against backdoor poisoning attacks, while the
initial form of the algorithm fails to provide such defense. Extensive experiments show that the
proposed meta-algorithm improves the robustness of DNN models against various backdoor attacks
on a variety of benchmark datasets with up to 45% corruption ratio. Furthermore, we propose a
systematic, meta-framework to solve backdoor attacks, which can effectively join existing force in
noisy label attack defenses and provides more insights to future development of defense algorithms.
2	Related Work
2.1	Robust Deep Learning Against Adversarial Attack
Although DNNs have shown high generalization performance on various tasks, it has been observed
that a trained DNN model would yield different results even by perturbing the image in an invis-
ible manner (Goodfellow et al., 2014; Yuan et al., 2019). Prior efforts have been made to tackle
this issue, among which one natural defense strategy is to change the empirical loss minimiza-
tion into a minimax objective. By solving the minimax problem, the model is guaranteed a better
worst-case generalization performance (Duchi & Namkoong, 2021). Since exactly solving the inner
maximization problem can be computationally prohibitive, different strategies have been proposed
to approximate the inner maximization optimization, including heuristic alternative optimization,
linear programming Wong & Kolter (2018), semi-definite programming Raghunathan et al. (2018),
etc. Besides minimax optimization, another approach to improve model robustness is imposing a
Lipschitz constraint on the network. Work along this line includes randomized smoothing Cohen
et al. (2019); Salman et al. (2019), spectral normalization Miyato et al. (2018a), and adversarial Lip-
Schitz regularization Terjek (2019). Although there are algorithms that are robust against adversarial
samples, they are not designed to confront backdoor attacks, in which clean training data is usually
inaccessible. There are also studies that investigated the connection between adversarial robustness
and robustness against backdoor attack (Weber et al., 2020). However, to our best knowledge, there
is no literature studying the relationship between label flipping attack and backdoor attack.
2.2	Robust Deep Learning Against Label Noise
Many recent studies have investigated the robustness of classification tasks with noisy labels. For
example, Kumar et al. (2010) proposed the Self-Paced Learning (SPL) approach, which assigns
higher weights to examples with a smaller loss. A similar idea was used in Curriculum Learning
(Bengio et al., 2009), in which a model learns on easier examples before moving to the harder
ones. Other methods inspired by SPL include learning the data weights (Jiang et al., 2018) and
collaborative learning (Han et al., 2018; Yu et al., 2019). An alternative approach to defending noisy
label attacks is label correction (Patrini et al., 2017; Li et al., 2017; Yi & Wu, 2019), which attempts
to revise the original labels of the data to recover clean labels from corrupted ones. However,
since we do not have the knowledge of which data points have been corrupted, it is nontrivial to
obtain provable guarantees for label corrections, unless strong assumptions have been made on the
corruption type.
2.3	Data Poisoning Backdoor Attack and Its defense
Robust learning against backdoor attacks has been widely studied recently. Gu et al. (2017) showed
that even a small patch of perturbation can compromise the generalization performance when data
is augmented with a backdoor trigger. Other types of attacks include the blend attacks (Chen et al.,
2017), clean label attacks (Turner et al., 2018; Shafahi et al., 2018), latent backdoor attacks (Yao
et al., 2019), etc. While there are various types of backdoor attacks, some attack requires that the
adversary not only has access to the data but also can has limited control on the training and inference
2
Under review as a conference paper at ICLR 2022
process. Those attacks include Trojan attacks and blind backdoor attacks (Pang et al., 2020). We
refer readers to Pang et al. (2020) for a comprehensive survey on different types of backdoor attacks.
Various defense methods have been proposed to defend against backdoor attacks. One defense
category is to remove the corrupted data by using anomaly detection (Tran et al., 2018; Chen et al.,
2018). Another category of work is the model inspection (Wang et al., 2019), which aims to inspect
and modify the backdoored model to make it robust against the trigger. There are also other methods
of tackling the backdoor attacks, such as randomized smoothing (Cohen et al., 2019; Weber et al.,
2020), and the median of means (Levine & Feizi, 2020). However, they are either inefficient or
cannot defend against backdoor attacks with a large ratio of corrupted data. Some of the above
methods also hinge on a clean set of validation data, while in practice, it is unlikely to guarantee the
existence of clean validation data, since validation data is usually a subset of training data. To the
best of our knowledge, there is no existing backdoor defense algorithm that is motivated from the
label corruption perspective.
3	Preliminaries
3.1	Learning with Noisy Labels
In this section, we review some representative approaches for defending noisy-labels. Although
the initial forms of these approaches can be vulnerable to backdoor attacks, we show later in the
next section that our proposed meta-algorithm can empower them to effectively confront backdoor
attacks. Specifically, we look into two types of nosiy-label defending approaches: filtering-based
approaches and consistency-based approaches.
The filtering-based approach is one of the most effective strategies for defending noisy labels,
which works by selecting or weighting the training samples based on indicators such as sample
losses (Jiang et al., 2017; Han et al., 2018; Jiang et al., 2020) or gradient norms of the loss-layer
(Liu et al., 2021). For instance, Jiang et al. (2017) proposed to assign higher probabilities to samples
with lower losses to be selected for model training. Consistency-based approach modifies data labels
during model training. Specifically, the Bootstrap approach (Reed et al., 2014) encourages model
predictions to be consistent between iterations, by modifying the labels as a linear combination of
the observed labels and previous predictions.
In this paper, we leverage two filtering-based noisy label algorithms, i.e. the Self-Paced Learning
(SPL) (Jiang et al., 2017; Kumar et al., 2010) and Provable Robust Learning (PRL) (Liu et al.,
2021), and one consistency-based algorithm, i.e. the Bootstrap (Reed et al., 2014), to investigate
the efficacy of the proposed meta algorithm. Strong empirical results in Section 5 on those input
algorithms suggest that our meta framework is readily to benefit other robust noisy-label algorithms.
We briefly summarize the main idea of the above algorithms in table 1.
PRL (Filtering-based)	SPL (Filtering-based)	Bootstrap (Modifying Label)
Mini-batch Keep data with small	Keep data with small loss	ytrue = αytrue + (1 - α)ypred
loss-layer gradient norm
Table 1: Overview of noisy-label defending algorithms, which achieve robustness against up to 45%
of pairwise flipping label noises.
3.2	Problem Setting of Backdoor Attacks
In this paper, we follow a standard setting for backdoor attacks and assume that there is an adversary
that tries to perform the backdoor attack. Firstly, the adversary can choose up to fraction of
clean labels Y ∈ Rn×q and modify them to arbitrary valid numbers to form the corrupted labels
Yb ∈ Rbnc×q. Let Yr represent the remaining untouched labels, then the final training labels
can be denoted as Y = [Yb, Yr]. Accordingly, the corresponding original feature are denoted as
X = [Xo ∈ Rbnc×d, Xr ∈ R(n-bnc)×d]. The adversary can design a trigger t ∈ Rd to form the
corrupted feature set Xb ∈ Rbnc×d such that for any bi in Xb, oi in Xo, it satisfies bi = oi + t.
3
Under review as a conference paper at ICLR 2022
Finally, the training feature will be X = [Xb ∈ Rbnc×d, Xr ∈ R(n-bnc)×d]. Without ambiguity,1 *
we also denote T = [t, t, ..., t] ∈ Rbnc×d, so that Xo + T = Xb.
Before analyzing the algorithm, we make following assumptions about the adversary attack:
Assumption 1 (Bounded Corruption Ratio). The overall corruption ratio and the corruption ratio
in each class is bounded. Specifically,
I(yb = c|y 6= c)
E(χ,y,yb)∈(x,Y,Yb) [-Ry = C)-----] ≤ e = 0∙5 ∀c ∈ 4Y
Assumption 2 (Small Trigger). Any backdoor trigger satisfies ktkp ≤ τ, which subtly alters the
data within a small radius-τ ball without changing its ground-truth label.
We also assume that there exists at least one black-box robust algorithm A which can defend any
noisy label attacks so long as the noisy-label ratio is bounded by . Note that the above assumption
is mild, since a variety of existing algorithm can handle noisy labels attacks with a large corruption
rate (e.g. 45%) (Jiang et al., 2017; Han et al., 2018; Reed et al., 2014; Liu et al., 2021).
4	Methodology
Given an e-backdoor attacked dataset (Xe, Ye), a clean distribution p := (X, Y), and a loss func-
tion L, we aim to answer the following questions: 1) can we learn a network function f that min-
imizes the generalization error under the corrupted distribution, i.e. E(x,y)〜p* [L(f (X + t),y)]?
2) can the learned f also minimize the generalization error under the ground-truth, clean distri-
bution, i.e. E(χ,y)〜p* [L(f (x),y)]? Next, We elaborate our meta-apProach for defending against
backdoor attacks, which answers the above two questions affirmatively.
4.1	A black-box robust algorithm against noisy labels
The ultimate goal for defending against backdoor attacks is to learn a netWork function f to minimize
its risk given corrupted feature inputs:
min J(f) ：= E(x,y)〜p* [L(f(x + t),y)]∙	(1)
HoWever, Equation 1 is not directly optimizable due to tWo-fold challenges: 1) the corrupted inputs
X With an unknoWn trigger t, and 2) the corrupted labels Y . That is, neither the clean inputs or
ground-truth labels are available. As such, We turn to an approachable objective that optimizes the
Worst-case of Equation 1:
1
min max 一
f kckp ≤τ n
[L(f(x+c),y)].
x∈x,y∈Y
(2)
Since the trigger satisfies ktkp ≤ τ, it is easy to see that Equation 2 minimizes an upper-bound of
the ground-truth loss, in that:
n X	LfX+t), y) ≤
x∈x,y∈Y
1
max
kckp≤τ n
Σ
x∈x,y∈Y
[L(f(X+c),y)].
To this end, directly optimizing the surrogate objective in Equation 2 is still intractable, since We do
not have access to clean X and Y, Which are involved in the inner maximization loop. To tackle this
challenge, We Will first assume that the clean label Y is available, and then relax this prerequisite
by using any learning algorithms that are robust against noisy labels. Specifically, assume that
φw = L ◦ f has a Lipschitz constant L w.r.t X, We further obtain a neW upper bound (see Appendix
for derivation details):
1	X	[L(f(x + c), y)] ≤ 1 X	φw(Xi + C, y) + eτL,	(3)
nn
x∈x,y∈Y	x∈x,y∈Y
1Some backdoor attack algorithms design instance-specific trigger. In this paper, We only focus on the static
trigger case and leave the instance-specific trigger case for our future study.
4
Under review as a conference paper at ICLR 2022
which draws a principled connection between the risks when using corrupted data and clean data:
1
min max 一
f kckp≤τ n
Σ
x∈X,y∈Y
[L(f (x + c), y)] ≈
(min kma≤τ n XjXy∈γ[L(f(x+c), y)]+T,
(4)
where the first term on the RHS of Equation 4 involves optimization on the corrupted features X
and clean labels Y, while the second term on the RHS requires minimizing the Lipschitz constant
L w.r.t x. Recall that minimizing the maximum gradient norm is equivalent to minimizing the Lips-
Chitz constant (Terjek, 2019). Therefore, optimizing the first term naturally regulates the maximum
change of the loss function within a small ball, which hence constrains the magnitude of the gradient
and has negligible effects on the Lipschitz regularization. The relationship between Lipschitz regu-
larization and adversarial training has also been well discussed by literatures (Terjek, 2019; Miyato
et al., 2018b). We defer more discussion along this line to Appendix.
Equation 4 indicates that if the target labels are not corrupted, and the learned function has a small
Lipschitz constant, learning with corrupted features is feasible to achieve low risks. Up to now,
the remaining challenge of optimizing the surrogate objective in Equation 4 is the inaccessible clean
label set Y. Fortunately, a variety of algorithms are at hand for handling noisy labels during learning
(Jiang et al., 2017; Liu et al., 2021; Kumar et al., 2010), which we can directly apply to our minimax
optimization scheme. Specifically, for the outer minimization, one can have:
min1	X	[L(f (X +C), y)],
f n x∈X,y∈Y
For the outer minimization, we can perform the noisy-label update for the above optimization ob-
jective. For instance, Given the mini-batch Mx , My with batch size m, if we use SPL to perform
the update, we can get the top (1 - )m data with a small risk L(f (X + C), y) to perform one-step
gradient descent. If we use the PRL to perform the update, assume L is the cross-entropy loss, one
can get the top (1-)m data with small loss-layer gradient norm kf(X+C) -yk to perform one-step
gradient descent. If we use the bootstrap, we can directly add a bootstrap regularization to update
the above objective.
In the meantime, it is non-trivial to directly solve the inner maximization, since adversarial learning
C in Equation 4 still faces the threat of noisy labels. To tackle this issue, we can leverage the same
robust noisy label algorithm. Specifically, we first approximate the inner optimization using the
first-order Tyler expansion:
C* = argmax1 X	L(f(x + c), y) ≈ arg max 1 X	L(f(x), y) + CTVχL(f (x), y)
kckp ≤τ n x∈X,y∈Y	kckp≤τ n x∈X,y∈Y
=argmax CT Vχ1	X	L(f (x), y).
kckp ≤τ	n x∈X,y∈Y
Above optimization is a linear programming problem. With the l∞ norm ball constraint on the
perturbation, the above update can be reduced to the fast gradient sign method (FGSM). Given a
minibatch Mx , My with batchsize m, we can have the closed-form solution as the following:
δ = CliPc { m ∙ Xx∈Mχ,y∈My sign (VxLf (X)，y))}.
(5)
To relax the prerequisite of a clean label set y in Equation 5, we will use a noisy-label algorithm to
perform the update. For instance, ifwe use a loss-filtering based algorithm (e.g. SPL), then for each
mini-batch, only the top (1 - )m data with small L (f (x), y) would be included in the update. If
we adopt a gradient-based filtering algorithm (e.g. PRL), given that L is the cross-entropy loss, then
only top (1-)m data with small kf (x) -yk will be included. The outside clipping ensures that the
feature value of the corrupted image is in the valid range. Based on the above building blocks, we
now introduce our algorithm in Algorithm 1, a meta defense scheme that are robust against backdoor
attacks, given arbitrary noisy-label robust algorithm A as an input. The algorithm is illustrated in
Figure 1.
5
Under review as a conference paper at ICLR 2022
Algorithm 1: Meta algorithm for Robust Learning Against Backdoor Attacks
input: Corrupted training data X, Y, perturbation limit: τ, learning with noisy label algorithm A (e.g.
PRL, SPL, Bootstrap).
return trained neural network ;
while epoch ≤ max_epoch do
for sampled minibatch Mx , My in X,Y do
#Inner maximization step
initialize c as 0 vector.
optimize the objective maxkck≤τ L(f (Mx + c), My) w.r.t to c by using robust algorithm A
optimize the objective minf L(f (Mx + c), My) w.r.tf by using robust algorithm A
end
end
and flipped label
ping attack
Figure 1: Illustration of our meta algorithm. By combining the minimax objective and noisy label
algorithm, we could reduce the backdoor attack problem to the label flipping attack. The left most
is the clean original data and the second one is corrupted data. The third figure shows the inner
maximization step while the last figure shows the outer minimization step.
4.2	Theoretical Justification
OUr ultimate goal is to learn W that achieves a small risk Ex,y~p*φw(X + t, y). To study the
generalization performance on the ground-truth distribution p*, We first define the following risks:
Remp = n Pχ∈χ,y∈γ φw (χ + t, y), Rt = Eχ,y~p* φw (X+ t, y), Remp = n Pχ∈χe ,y∈Y φw (X +
1
c, y), and RC	= n Eχ∈χe ,y∈γe φw(X+c, y). Next, we focus on the gap between Rt andRcemp.
emp emp
Theorem 1. Let RCemp , RCemp , Rt , , τ defined as above. Assume that the prior distribution of the
network parameter W is N (0, σ), and the posterior distribution of parameter is N(W, σ) which is
learned from the training data. Let k be the number of parameters, and n be the sample size. If the
objective function φw = L ◦ f is Lφ-Lipschitz smooth, then with probability at least 1-δ, one can
derive:
Rt ≤ Rcemp + Lφ (2τ + τ) +
4 k log(1+kkσ22)+1+log nδ+2 iog(6n+3k)
n - 1
(6)
We hereby present the skeleton of the proof and defer more details to Appendix. First, we decompose
the error into two components: 1) the generalization gap on the triggered data, and 2) the difference
of performance loss between the trigger t and worst case perturbation c: Rt - RCemp = (Rt -
Rtemp) + (Rtemp - RCemp).
4 k log(1+ "kσ22)+1+log nδ+2 ιog(6n+3k)
The first components is V------ʌ-------/ n-1-----------------, which is derived by following
the PAC-Bayes framework (Foret et al., 2020). For the second term, the gap is introduced by two
sources. The first source is the difference between c and t, and the second is from the difference
between X and X. Since the objective is Lφ Lipschitz, and kt-ck ≤ 2τ according to our constraint
to the adversary, it is easy to upper bound the error as 2τLφ . In the meantime, there is -fraction of
6
Under review as a conference paper at ICLR 2022
difference between X and X , which is bounded by ktk < τ and leads to the other difference term
Lφτ.
Theorem 1 presents an upper-bound of the gap Rt - Rcemp . The first term in Equation 6 can
be minimized by using a noisy label algorithm. The second term, which is the error induced by
the adversarial trigger, is jointly constrained by the Lipschitz constant Lφ, perturbation limit τ,
and the corruption ratio . We can regularize the Lφ whereas the τ and are controlled by the
unknown adversary. Note that existing literature has also shown that adversarial training plays a
resemblant role as the Lipschitz regularization. The last term, i.e. the normal generalization error on
the clean data, is difficult to minimize directly. The bound in Theorem 1 emphasizes the importance
of involving both the noisy label algorithm and the adversarial training. The noisy label algorithm
can reduce the Rcemp while the adversarial training minimizes the Lipschitz constant Lφ to optimize
the second term.
5 Experiment
In this section, we perform the empirical study. In our experiment, we perform experiment on
CIFAR10 and CIFAR100 benchmark data. We use ResNet-32 (He et al., 2016) as the backbone
network structure for all experiment baseline. The initial learning rate of all methods are set to
be 3e-4, and we use AdamW (Loshchilov & Hutter, 2017) as the optimizer for all methods. The
evaluation metric is the top-1 accuracy for both clean testing data and testing data with backdoor
trigger.
For the backdoor data poisoning attack, we use simple badnet attack (Gu et al., 2017) and gaussian
blending attack (Chen et al., 2017) since these two attacks do not require any information about the
model or training procedure (Pang et al., 2020).
•	badnet patch attack: trigger is a 3 × 3 black-white checkerboard and it is added to the right
bottom corner of the image.
•	blending attack: trigger is a fixed Gaussian noise which has the same dimension as the
image. The corrupted image generated by xi = (1 - α)xi + αt. In our experiment, we set
the α as 0.1.
The poisoned sample can be found at Appendix. We deploy the multi-target backdoor attack in this
paper. Our poisoning approach is as following: we first systematically flip the label to perform the
label-flipping attack . Then, for those being attacked samples, we add the triggers on their feature.
Without adding the trigger, the problem would reduce to the noisy label problem.
and we use the following two evaluation metric.
•	top-1 clean accuracy: the top-1 accuracy calculated on trigger-less testing data
•	top-1 poison accuracy: the top-1 accuracy calculated by the model prediction on poisoned
testing data and ground truth clean label.
The first metric describes how model performs on benign data while the second metric de-
scribes how model performs on triggered data. We varies our training data poisoning rate as
[15%, 25%, 35%, 45%] to investigate how our algorithm performs against different corruption ra-
tio. All methods are trained 100 epochs, Also, in this paper, we assume there is no clean validation
data available. Thus, it is difficult to perform early stopping or decide which epoch result should be
used. Thus, we report the averaged accuracy across last 10 epochs for every methods.
We investigate three noisy label algorithms by comparing the performance of the original method
and reinforced method. Specifically, we choose SPL, PRL, and Bootstrap as our original noisy label
algorithm and the corresponding reinforced algorithm with adversarial training SPL-AT, PRL-AT,
Bootstrap-AT. We also compared our method against adversarial training only (AT), which only uses
the adversarial training without using any noisy label algorithm. To show the success of the attack,
we also includes the standard training results.
5.1	How robust learning against noisy Label performs on backdoor data
In this section, we aim to answer the first question: how does noisy label defense algorithm performs
against backdoor attack?
7
Under review as a conference paper at ICLR 2022
To answer above question, We evaluate PRL, SPL, and Bootstrap on CIFAR10 and CIFAR100
dataset. The results can be found in table 2 and table 3. As we can see in the table, the standard
training performs well on the benign testing data (i.e. high clean accuracy) while its performance
on triggered data is very bad (i.e. low poison accuracy), which indicates the effectiveness of the
backdoor attack.
As for the three noisy label algorithm, we found that although Bootstrap, SPL, and PRL all performs
well for the benign testing data, they all fail when defending the backdoor attack data especially with
large corruption rate, which illustrates that noisy label algorithm cannot defend the backdoor attack
especially when corruption ratio is large.
5.2	How adversarial training improves the noisy label algorithm
To investigate whether adversarial training could improve the robustness of existing noisy label al-
gorithm against backdoor attack, we performed experiment on SPL-AT, PRL-AT and Bootstrap-AT
to see how they performs on both clean test data and triggered test data. The results can be found in
table 2 and table 3. As we can see that by adding the adversarial training, the performance on trig-
gered data is largely improved (i.e. the poison accuracy significantly improves). Also, we noticed
that this improvement pattern is hold for all three noisy label algorithm, which indicates the effec-
tiveness of the proposed method on improving robustness against backdoor attack. Also, compared
to adversarial training only (AT), adding noisy label does improve the performance. Especially for
the PRL algorithm, we see the PRL-AT achieves better performance than AT. Also, we found that
compared to consistency based noisy label algorithm, filtering based algorithm are more easier to
be boosted by adversarial training. The potential reason behind this could be that filtering based
method is more efficient against noisy label algorithm Han et al. (2018); Jiang et al. (2017); Liu
et al. (2021).
Backdoor Attack Defense Accuracy.								
Dataset	AT	BootStrap	Bootstrap-AT	PRL	PRL-AT	SPL	SPL-AT	Standard
0.15 Cifar 1 0	025 with Patch Attack, 0.35 Poison Accuracy 0..45	66.64 ± 5.28	2.09 ± 0.13	3.05 ± 0.47	81.71 ± 0.37	80.15 ± 0.42	34.60 ± 1.57	77.60 ± 3.81	2.10 ± 0.10
	63.98 ± 7.16	2.01 ± 0.23	2.75 ± 0.17	45.94 ± 25.19	78.14 ± 0.48	10.87 ± 2.13 22.17 ± 10.51		2.13 ± 0.15
	60.19 ± 1.35	1.98 ± 0.15	2.66 ± 0.16	31.27 ± 17.63	75.04 ± 0.29	11.74 ± 1.24	15.40 ± 7.56	2.01 ± 0.09
	51.25 ± 1.81	1.94 ± 0.12	2.53 ± 0.20	17.50 ± 1.66	58.90 ± 12.52 12.32 ± 1.20		14.00 ± 5.35	1.88 ± 0.04
0.15 Cifar 1 0	025 with Patch Attack, 0.35 Clean Accuracy 0..45	66.77 ± 5.17	85.22 ± 0.48	82.62 ± 0.26	82.06 ± 0.16	80.25 ± 0.43	77.35 ± 2.76	77.70 ± 3.78	85.40 ± 0.37
	63.98 ± 7.16	85.25 ± 0.19	81.90 ± 0.25	78.57 ± 1.03	78.22 ± 0.56	69.52 ± 2.38	68.49 ± 2.76	85.20 ± 0.26
	60.31 ± 1.37	84.86 ± 0.13	81.75 ± 0.25	73.63± 0.75	75.10 ± 0.31	60.23 ± 3.14	58.88 ± 3.46	84.73 ± 0.13
	51.25 ± 1.81	1.94 ± 0.12	2.53 ± 0.20	17.50 ± 1.66	58.90 ± 12.52 50.82 ± 1.48		14.00 ± 5.35	1.88 ± 0.04
CIFAR 1 0	00.2155 with Blend Attack, 0.35 Poison Accuracy 0..45	65.15 ± 0.94	2.17 ± 0.17	24.98 ± 10.01	6.41 ± 3.91	79.71 ± 0.33	11.60 ± 6.56	74.77 ± 3.53	2.29 ± 0.10
	56.98 ± 0.72	2.06 ± 0.10	33.33 ± 20.03	6.77 ± 2.81	76.99 ± 0.37	11.60 ± 8.59 52.36 ± 10.57		2.03 ± 0.18
	47.84 ± 1.49	1.86 ± 0.07	13.13 ± 7.11	9.42 ± 5.28	73.17 ± 0.96	12.71 ± 9.33	50.79 ± 7.92	1.97 ± 0.07
	34.66 ± 1.49	1.83 ± 0.11	6.12 ± 2.86	8.13 ± 4.50	49.88 ± 8.43	8.69 ± 4.41	35.06 ± 4.00	1.88 ± 0.06
0.15 Cifar 1 0	025 0.25	66.14 ± 0.98	85.54 ± 0.58	81.44 ± 0.58	77.51 ± 1.20	80.06 ± 0.34	76.25 ± 2.78	75.65 ± 3.11	85.28 ± 0.34
	58.91 ± 5.70	84.95 ± 0.30	80.89 ± 0.65	71.45 ± 1.40	77.82 ± 0.26	67.86 ± 2.58	65.08 ± 0.82	85.06 ± 0.39
wt Blend Attac, 0.35 50.07 ± 13.26 Clean Accuracy 0.45 38.03 ± 15.42		84.72 ± 0.58	80.63 ± 0.57	66.22 ± 1.15	74.34 ± 1.01	60.52 ± 2.26	60.16 ± 2.39	84.72 ± 0.28
		84.36 ± 0.38	80.35 ± 0.39	55.78 ± 2.09	57.17 ± 9.02	49.48 ± 2.19	46.74 ± 0.71	84.07 ± 0.17
Table 2: Performance on CIFAR 1 0. is the corruption rate.
5.3	Ablation Study
In this section, we aim to explore more about the proposed framework. Since our algorithm use
the noisy-label solver for both inner and outer optimization. A interesting question to ask is that
whether both inner and outer noisy-label solver plays an important role in defense the backdoor
attack. Thus, we have two variants. One is we only use noisy label algorithm to update the model
for outer minimization and another one is we only use the noisy label algorithm to update the model
for inner maximization. The results can be found at table 4 and table 5. As we could see in these two
tables, using noisy label algorithm to perform the inner maximization is more important compared
to using noisy label algorithm to perform out minimization.
6 Conclusion
In this paper, we investigate the connection between label flipping attack and backdoor data poison-
ing attack. We show that although robust algorithm against label flipping attack cannot defend the
8
Under review as a conference paper at ICLR 2022
Backdoor Attack Defense Accuracy.								
Dataset	AT	BootStrap	Bootstrap-AT	PRL	PRL-AT	SPL	SPL-AT	Standard
CIFAR 1 00	00.2155 with Patch Attack, 0.35 Poison Accuracy 0..45	23.70 ± 1.39	5.23 ± 0.81	44.74 ± 4.05	15.15 ± 9.17	47.11 ± 0.58	24.87 ± 5.27	42.24 ± 0.76	5.28 ± 0.50
	21.84 ± 1.17	3.07 ± 0.23	44.09 ± 1.10	17.53 ± 18.06	43.81 ± 0.41	8.48 ± 1.13	35.46 ± 1.13	3.10 ± 0.60
	17.16 ± 1.09	2.85 ± 0.12	40.14 ± 0.20	20.83 ± 10.03	39.76 ± 0.72	7.37 ± 0.59	28.41 ± 1.72	3.24 ± 1.04
	13.61 ± 0.74	10.60 ± 10.49	31.21 ± 0.30	23.98 ± 9.32	29.76 ± 1.11	7.26 ± 0.76	20.43 ± 1.69	10.51 ± 11.21
CIFAR 1 00	00.2155 with Patch Attack, 0.35 Clean Accuracy 0..45	34.08 ± 0.40	52.39 ± 0.38	47.76 ± 0.14	50.50 ± 0.41	47.21 ± 0.56	46.38 ± 0.41	42.38 ± 0.73	52.42 ± 0.59
	31.72 ± 0.75	50.54 ± 0.25	44.82 ± 0.52	47.49 ± 0.91	43.89 ± 0.35	39.98 ± 0.80	35.65 ± 1.14	50.53 ± 0.55
	29.50 ± 1.73	48.41 ± 0.42	40.38 ± 0.18	44.21 ± 0.21	39.80 ± 0.67	34.11 ± 1.10	28.52 ± 1.70	48.75 ± 0.71
	23.93 ± 3.43	41.46 ± 5.00	31.48 ± 0.38	34.34 ± 0.91	29.79 ± 1.13	27.87 ± 2.28	20.55 ± 1.75	41.02 ± 6.06
CIFAR 1 00	00.2155 with Blend Attack, 0.35 Poison Accuracy 0..45	33.65 ± 0.54	2.19 ± 0.28	46.65 ± 0.33	2.10 ± 0.43	46.01 ± 0.50	6.14 ± 1.12	41.57 ± 0.74	2.09 ± 0.20
	30.95 ± 0.42	1.17 ± 0.08	41.84 ± 0.59	1.45 ± 0.21	41.78 ± 0.76	2.95 ± 0.56	33.54 ± 1.76	1.12 ± 0.20
	27.30 ± 0.45	1.05 ± 0.06	31.88 ± 1.26	1.51 ± 0.17	34.51 ± 1.60	2.00 ± 0.49	25.71 ± 2.31	1.08 ± 0.16
	20.79 ± 4.97	0.99 ± 0.07	23.61 ± 1.07	2.68 ± 1.17	22.00 ± 1.95	2.39 ± 0.17	18.62 ± 1.21	0.92 ± 0.11
CIFAR 1 00	00.2155 with Blend Attack, 0.35 Clean Accuracy 0..45	34.22 ± 0.58	52.65 ± 0.19	47.77 ± 0.36	48.61 ± 0.18	46.92 ± 0.47 46.01 ± 0.40		42.40 ± 0.70	52.60 ± 0.59
	33.65 ± 0.55	51.12 ± 0.37	44.75 ± 0.45	45.23 ± 0.34	42.87 ± 0.72 40.47 ± 1.47		35.71 ± 1.10	50.98 ± 0.43
	28.14 ± 0.48	49.80 ± 0.24	40.85 ± 0.37	40.46 ± 0.17	36.30 ± 1.24 35.70 ± 1.68		28.56 ± 2.05	49.65 ± 0.49
	22.03 ± 0.49	48.46 ± 0.53	34.78 ± 1.39	34.98 ± 0.83	24.71 ± 1.37 29.91 ± 1.40		21.82 ± 1.21	48.07 ± 0.52
Table 3: Performance on CIFAR 1 00. is the corruption rate.
Backdoor Attack Defense Accuracy.
Dataset		BootStrap-inner	Bootstrap-outer	PRL-inner	PRL-outer	SPL-inner	SPL-outer
	0.15	3.15 ± 0.61	-3.20 ± 0.63	80.78 ± 0.31	2.84 ± 0.23	65.50 ± 16.22	3.09 ± 0.35
C IFAR 1 0 with Patch Attack,	0.25	2.74 ± 0.10	2.73 ± 0.09	79.07 ± 0.20	2.50 ± 0.10	18.95 ± 9.91	2.58 ± 0.19
Poison Accuracy	0.35	2.70 ± 0.24	2.67 ± 0.15	76.06 ± 0.37	2.39 ± 0.26	13.45 ± 5.40	2.38 ± 0.14
	0.45	2.32 ± 0.08	2.51 ± 0.11	67.87 ± 2.63	2.24 ± 0.10	12.10 ± 4.46	2.23 ± 0.26
	0.15	82.58 ± 0.33	82.45 ± 0.25	80.86 ± 0.31	83.09 ± 0.12	76.48 ± 3.03	83.02 ± 0.49
C IFAR 1 0 with Patch Attack,	0.25	82.14 ± 0.28	81.87 ± 0.23	79.10 ± 0.17	83.13 ± 0.21	69.33 ± 2.57	83.30 ± 0.13
Clean Accuracy	0.35	81.71 ± 0.46	81.55 ± 0.54	76.08 ± 0.34	82.83 ± 0.38	59.76 ± 3.59	83.05 ± 0.38
	0.45	81.53 ± 0.16	81.00 ± 0.47	69.96 ± 0.37	82.78 ± 0.18	49.31 ± 0.53	82.84 ± 0.27
	0.15	29.85 ± 10.65	40.79 ± 13.27	80.38 ± 0.15	46.29 ± 18.09	72.89 ± 6.14	48.21 ± 14.90
CIFAR 1 0 with Blend Attack,	0.25	14.81 ± 10.42	27.57 ± 10.93	78.44 ± 0.19	27.34 ± 18.42	54.46 ± 10.45	21.18 ± 11.85
Poison Accuracy	0.35	6.52 ± 3.80	17.41 ± 10.13	71.93 ± 2.69	11.25 ± 5.92	46.12 ± 13.77	14.58 ± 7.12
	0.45	11.58 ± 14.94	9.01 ± 4.66	64.98 ± 2.74	5.90 ± 2.28	42.30 ± 5.85	5.16 ± 1.64
	0.15	81.54 ± 0.25	81.18 ± 0.75	80.73 ± 0.18	82.51 ± 0.36	76.11 ± 3.32	82.35 ± 0.22
CIFAR 1 0 with Blend Attack,	0.25	80.99 ± 1.12	80.37 ± 0.94	78.23 ± 0.46	82.35 ± 0.65	66.64 ± 2.31	82.15 ± 0.37
Clean Accuracy	0.35	81.04 ± 0.81	79.54 ± 1.32	71.62 ± 2.65	82.55 ± 0.47	57.44 ± 1.78	81.81 ± 1.00
	0.45	81.06 ± 0.25	78.93 ± 0.84	62.34 ± 2.51	82.15 ± 0.48	48.82 ± 0.94	81.81 ± 1.06
Table 4: Ablation study on CIFAR10. is the corruption rate.
Backdoor Attack Defense Accuracy.							
Dataset		BootStrap-inner	Bootstrap-outer	PRL-inner	PRL-outer	SPL-inner	SPL-outer
	0.15	45.76 ± 2.65	41.66 ± 8.37	47.34 ± 0.44	44.32 ± 5.45	43.05 ± 0.39	43.41 ± 6.36
CIFAR 1 00 with Patch Attack,	0.25	44.41 ± 1.55	43.65 ± 0.88	44.71 ± 0.40	41.13 ± 5.82	35.69 ± 1.05	41.81 ± 3.89
Poison Accuracy	0.35	40.02 ± 0.19	38.72 ± 0.60	40.19 ± 0.39	38.75 ± 0.60	24.98 ± 6.34	38.97 ± 1.10
	0.45	31.12 ± 0.40	29.46 ± 0.33	31.49 ± 1.04	29.74 ± 0.35	20.13 ± 1.80	30.19 ± 0.31
	0.15	48.01 ± 0.28	47.91 ± 0.21	47.43 ± 0.43	48.41 ± 0.40	43.29 ± 0.21	48.12 ± 0.36
CIFAR 1 00 with Patch Attack,	0.25	45.27 ± 0.82	44.68 ± 0.27	44.83 ± 0.38	45.58 ± 0.39	36.21 ± 0.80	45.14 ± 0.64
Clean Accuracy	0.35	40.49 ± 0.23	38.98 ± 0.47	40.40 ± 0.41	39.46 ± 0.30	29.58 ± 1.49	39.89 ± 0.50
	0.45	31.32 ± 0.48	29.81 ± 0.34	31.49 ± 1.02	30.39 ± 0.27	20.70 ± 1.51	30.77 ± 0.55
	0.15	46.72 ± 0.23	46.56 ± 0.26	46.59 ± 0.43	46.83 ± 1.00	42.15 ± 0.68	46.80 ± 0.70
CIFAR 1 00 with Blend Attack,	0.25	41.64 ± 1.54	40.60 ± 0.98	43.43 ± 0.59	40.02 ± 1.44	34.10 ± 1.60	39.30 ± 3.16
Poison Accuracy	0.35	31.18 ± 2.83	30.91 ± 2.02	35.84 ± 1.71	28.86 ± 2.82	25.36 ± 2.65	28.94 ± 3.49
	0.45	22.98 ± 1.18	23.37 ± 0.92	24.60 ± 2.19	22.16 ± 3.73	19.57 ± 1.64	24.17 ± 2.70
	0.15	48.05 ± 0.33	47.83 ± 0.29	47.24 ± 0.65	48.43 ± 0.44	42.94 ± 0.55	48.37 ± 0.68
CIFAR 1 00 with Blend Attack,	0.25	44.85 ± 0.52	44.59 ± 0.31	44.17 ± 0.36	45.19 ± 0.26	36.18 ± 1.20	45.06 ± 0.18
Clean Accuracy	0.35	40.80 ± 0.56	40.08 ± 0.41	38.23 ± 0.80	41.84 ± 0.51	30.23 ± 1.25	41.18 ± 0.69
	0.45	35.32 ± 1.77	34.13 ± 1.13	27.33 ± 1.37	39.06 ± 0.45	24.22 ± 1.66	38.62 ± 1.30
							
Table 5: Ablation study on CIFAR 1 00. is the corruption rate.
backdoor data poisoning attack, adding the adversarial training on existing algorithm could largely
improve the robustness against backdoor attack.Both theoretical and empirical analysis show the
effectiveness of our proposed meta algorithm.
9
Under review as a conference paper at ICLR 2022
References
Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In
Proceedings ofthe 26th annual international conference on machine learning, pp. 41-48, 2009.
Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung
Lee, Ian Molloy, and Biplav Srivastava. Detecting backdoor attacks on deep neural networks by
activation clustering. arXiv preprint arXiv:1811.03728, 2018.
Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep
learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In International Conference on Machine Learning, pp. 1310-1320. PMLR, 2019.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
John C Duchi and Hongseok Namkoong. Learning models with uniform performance via distribu-
tionally robust optimization. The Annals of Statistics, 49(3):1378-1406, 2021.
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimiza-
tion for efficiently improving generalization. arXiv preprint arXiv:2010.01412, 2020.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the
machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017.
Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi
Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In
Advances in neural information processing systems, pp. 8527-8537, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning
data-driven curriculum for very deep neural networks on corrupted labels. arXiv preprint
arXiv:1712.05055, 2017.
Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-
driven curriculum for very deep neural networks on corrupted labels. In International Conference
on Machine Learning, pp. 2304-2313, 2018.
Lu Jiang, Di Huang, Mason Liu, and Weilong Yang. Beyond synthetic noise: Deep learning on con-
trolled noisy labels. In International Conference on Machine Learning, pp. 4804-4815. PMLR,
2020.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep con-
volutional neural networks. Advances in neural information processing systems, 25:1097-1105,
2012.
M Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable
models. In Advances in neural information processing systems, pp. 1189-1197, 2010.
Alexander Levine and Soheil Feizi. Deep partition aggregation: Provable defense against general
poisoning attacks. arXiv preprint arXiv:2006.14768, 2020.
Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Neural attention distil-
lation: Erasing backdoor triggers from deep neural networks. arXiv preprint arXiv:2101.05930,
2021.
10
Under review as a conference paper at ICLR 2022
Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo, and Li-Jia Li. Learning from
noisy labels with distillation. In Proceedings of the IEEE International Conference on Computer
Vision ,pp.1910-1918, 2017.
Boyang Liu, Mengying Sun, Ding Wang, Pang-Ning Tan, and Jiayu Zhou. Learning deep neural
networks under agnostic corrupted supervision. arXiv preprint arXiv:2102.06735, 2021.
Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against back-
dooring attacks on deep neural networks. In International Symposium on Research in Attacks,
Intrusions, and Defenses, pp. 273-294. Springer, 2018.
Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Reflection backdoor: A natural backdoor
attack on deep neural networks. In European Conference on Computer Vision, pp. 182-199.
Springer, 2020.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101, 2017.
Bence Major, Daniel Fontijne, Amin Ansari, Ravi Teja Sukhavasi, Radhika Gowaikar, Michael
Hamilton, Sean Lee, Slawomir Grzechnik, and Sundar Subramanian. Vehicle detection with
automotive radar using deep learning on range-azimuth-doppler tensors. In Proceedings of the
IEEE/CVF International Conference on Computer Vision Workshops, pp. 0-0, 2019.
David A McAllester. Pac-bayesian model averaging. In Proceedings of the twelfth annual confer-
ence on Computational learning theory, pp. 164-170, 1999.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018a.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a
regularization method for supervised and semi-supervised learning. IEEE transactions on pattern
analysis and machine intelligence, 41(8):1979-1993, 2018b.
Ren Pang, Zheng Zhang, Xiangshan Gao, Zhaohan Xi, Shouling Ji, Peng Cheng, and Ting Wang.
Trojanzoo: Everything you ever wanted to know about neural backdoors (but were afraid to ask).
arXiv preprint arXiv:2012.09302, 2020.
Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making
deep neural networks robust to label noise: A loss correction approach. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 1944-1952, 2017.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Semidefinite relaxations for certifying ro-
bustness to adversarial examples. arXiv preprint arXiv:1811.01057, 2018.
Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew
Rabinovich. Training deep neural networks on noisy labels with bootstrapping. arXiv preprint
arXiv:1412.6596, 2014.
Hadi Salman, Greg Yang, Jerry Li, Pengchuan Zhang, Huan Zhang, Ilya Razenshteyn, and Sebastien
Bubeck. Provably robust deep learning via adversarially trained smoothed classifiers. arXiv
preprint arXiv:1906.04584, 2019.
Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras,
and Tom Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks. In
Proceedings of the 32nd International Conference on Neural Information Processing Systems, pp.
6106-6116, 2018.
David Terjek. Adversarial IiPschitz regularization. In International Conference on Learning RePre-
sentations, 2019.
Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. arXiv
PrePrint arXiv:1811.00636, 2018.
Alexander Turner, Dimitris Tsipras, and Aleksander Madry. Clean-label backdoor attacks. 2018.
11
Under review as a conference paper at ICLR 2022
Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y
Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In 2019
IEEE Symposium on Security and Privacy (SP), pp. 707-723. IEEE, 2019.
Maurice Weber, Xiaojun Xu, Bojan Karlas, Ce Zhang, and Bo Li. Rab: Provable robustness against
backdoor attacks. arXiv preprint arXiv:2003.08904, 2020.
Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In International Conference on Machine Learning, pp. 5286-5295. PMLR,
2018.
Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y Zhao. Latent backdoor attacks on deep neural
networks. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communica-
tions Security, pp. 2041-2055, 2019.
Kun Yi and Jianxin Wu. Probabilistic end-to-end noise correction for learning with noisy labels.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7017-
7025, 2019.
Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor Tsang, and Masashi Sugiyama. How does dis-
agreement help generalization against label corruption? In International Conference on Machine
Learning, pp. 7164-7173. PMLR, 2019.
Xiaoyong Yuan, Pan He, Qile Zhu, and Xiaolin Li. Adversarial examples: Attacks and defenses for
deep learning. IEEE transactions on neural networks and learning systems, 30(9):2805-2824,
2019.
12
Under review as a conference paper at ICLR 2022
7	Appendix
In this section, we provided proof of theorem and more discussion.
7.1	Proof of Equation 3
1 X	[L(f (X + C)，y)] ≤ 1 X	φw (Xi + c，y)+ eτL
x∈X,y∈Y	x∈X,y∈Y
let G denote the initially clean sample set (i.e. (X, Y)), and B the corrupted sample set (i.e. the
training set corrupted with a trigger whereas the labels are untouched). Let R denote the clean
sample set which is replaced by the adversary (i.e. R is the subset of G, and is replaced by B, i.e.
G0 = G \ R ∪ B = (X, Y)), and let φw denote the function L ◦ f.
One can decompose the inner part of our mini-max objective in Equation 2 as the following,
1 X	[L(f(χ + c), y)] = 1 X φw(Xi + c, y) + 1 X φw(Xi + c, y)
n χ∈x,y∈Y	n i∈G0\B	n i∈R
1 X	φw (Xi	+ c, y)	+ 1 X	φw (Xi +	c,	y)	+ 1 X	φw (Xi+ t +	c,	y)-
i∈G0\B	i∈R	i∈B
1 X φw (Xi + t + C，y)
i∈B
=1 X	Φw(Xi + c, y)+(1 X Φw (Xi + c, y) -1X Φw (Xi + c +1, y)
n x∈x,y∈Y	n i∈R	n i∈B
≤ 1 X Φw(Xi	+ c, y)	+	∣(1 X φw (Xi + c, y) - 1 X Φw(Xi + c +1, y)
n x∈x,y∈Y	n i∈R	n	i∈B
≤ 1 X Φw(Xi	+ c, y)	+	ELktk≤ 1 X φw(Xi + c, y) + eτL,
n x∈x,y∈Y	n x∈x,y∈Y
7.2	Proof of theorem 1
Theorem. Let Remp, Remp, Rt,e,τ, is defined as above. Assume the prior distribution of the
network parameter w is N (0, σ), and the posterior distribution of parameter is N(w, σ) is the
posterior parameter distribution, where w is learned according to training data. Let k to be the
number of parameters, n to be the sample size, assume the objective function φw = L ◦ f is Lφ-
lipschitz smooth, then, with probability at least 1-δ, we have:
4k log fl + kwk2) + 1 + log δ + 2log(6n + 3k)
Rt ≤ Remp + Lφ(2τ + ET) + t --------------)n - I---------------------
Proof: we first decompose the gap as following
Rt - Rcemp = (Rt -Rtemp) + (Rtemp -Rcemp) ≤ |(Rt -Rtemp)| + |(Rtemp -Rcemp)|
13
Under review as a conference paper at ICLR 2022
We bound the second part first.
Rtemp - Rcemp ≤ kRtemp - Rcemp k
=1k X	[φ(χ +1,	y) - Φ(χ +	c,	y)]	+ X	Φ(χ	+1, y) - X	Φ(χ	+ c, y)	Il
n x∈Xr,y∈Yr	x∈Xo,y∈Yo	x∈Xb,y∈Yo
≤ 11 X	[φ(χ +1,y) -Φ(χ +	c,y)]∣l +11 X	Φ(χ	+1,y)	- X Φ(χ + cy)ll
n x∈Xr,y∈Yr	n x∈Xo,y∈Yo	x∈Xb,y∈Yo
≤	(1- )LφIt - cI + LφIt - cI + Lφ max Ixo - xbI
xo ,xb
≤	(1 - )Lφ It - cI + Lφ It - cI + Lφ ItI
=	LφIt - cI + LφItI
≤	Lφ2τ + LφItI
≤	Lφ(2τ + τ)
Now, we bound the second term. Note the second term is a typical gap term between empirical
loss and generalization loss, and there are many approaches to bound this term like VC dimension.
Since we aimed to focus the deep neural network, we follow the PAC-Bayes framework McAllester
(1999) to analyze the generalization bound. Specifically, we use results from Foret et al. (2020),
/ 4 k log ( 1+ "kσ22 ) + 4 + lθg nδ +2 lθg(6n+3k)
which gives V-----------------/ 冗_1------------------under the assumption of gaussian prior and
posterior. The proof for this can be found in the appendix of Foret et al. (2020) (i.e. equation 13 on
the paper).
7.3	Discussion about lipschitz regularization and adversarial training
As we could see from the above theorem that a small lipschitz constant could bring robustness
against backdoor attack. In this section, we provided the reason why we claim adversarial training
helps lipschitz regularization. The definition of lipschitz function is If (x) - f (y)I ≤ LIx -
yI, ∀x, y. Since the lipschitz constant showes in the upper bound of the error, we would like to get
the minimum lipschitz constant to tighten the bound. Follow (Terjek, 2019), the minimum lipschitz
constant can be expressed as:
If IL = sup
x,y∈X;x=y
dγ (f(χ),f(y))
dχ (χ,y)
rewrite y as x + c, we would get
If IL =	sup
x,x+r∈XQ<dχ (x,x+c)
dγ (f(x),f(x + r))
dX (x, x + r)
Minimize above objective respect to function f reduces to the adversarial learning.
dY(f(x),f(x+c))
inf l f l L = inf	SUp	—~Γ~i——1—
f	f x,x+c∈X;0<dx (x,x+c)	dX(X, χ + C)
If we treat the denominator as a constant, then this is exactly the same as our minimax objective.
More details can be found in (Terjek, 2019).
7.4	Supplementary Experiment Results
We provided the experiment hyperparameters, and supplementary results for the experiment.
7.4	. 1 Experiment Hyperparameters
We list the details of experiment in this section. All the methods use resnet-32 as the backbone
network. AdamW is used as the opimitzer for all methods. The perturbation limit τ is set to be 0.05
for all methods requiring τ. All methods are repeated for three different random seeds to calculate
the standard deviation.
The trigger for badnet attack and blending attack can be found in figure 2.
14
Under review as a conference paper at ICLR 2022
(a)	clean example
(b)	badnet attack
(c)	blend attack
Figure 2:	Example of clean and various poisoned samples
7.4.2 Experiment on MNIST
We found interesting results on MNIST. In MNIST, we found adversarial training itself sometimes
gives robustness to the backdoor attack. We hypothesis that this is because the MNIST is potentially
a easier task than CIFAR. In here, we show the performance of adversarial training and PRL-AT on
MNIST. The results can be found at table 6.
Backdoor Attack Defense Accuracy.								
Dataset		AT	BootStrap	Bootstrap-AT	PRL	PRL-AT	SPL	SPL-AT
	0.15	0.30 ± 0.07	0.04 ± 0.01	3.17 ± 3.23	97.96 ± 0.21	98.44 ± 0.05	59.24 ± 33.30	85.61 ± 12.56
MNIST with Patch Attack,	0.25	0.26 ± 0.17	0.04 ± 0.02	0.17 ± 0.10	89.91 ± 7.53	97.04 ± 1.07	25.25 ± 1.38	30.70 ± 6.62
Poison Accuracy	0.35	0.10 ± 0.02	0.08 ± 0.06	0.14 ± 0.01	77.91 ± 10.41	97.71 ± 0.18	13.54 ± 0.76	26.03 ± 5.27
	0.45	0.11 ± 0.01	0.04 ± 0.02	0.42 ± 0.34	43.42 ± 11.66	76.42 ± 8.65	12.85 ± 1.90	10.97 ± 2.16
	0.15	98.17 ± 0.69	99.49 ± 0.05	95.48 ± 1.56	98.08 ± 0.26	98.44 ± 0.05	93.23 ± 4.72	97.74 ± 0.37
MNIST with Patch Attack,	0.25	98.59 ± 0.22	99.48 ± 0.07	98.83 ± 0.19	97.46 ± 0.07	97.11 ± 1.06	86.98 ± 0.72	85.89 ± 1.76
Clean Accuracy	0.35	94.48 ± 4.87	99.48 ± 0.04	98.45 ± 0.32	97.40 ± 0.46	97.86 ± 0.11	73.09 ± 4.34	77.49 ± 0.96
	0.45	98.27 ± 0.43	99.42 ± 0.02	96.44 ± 1.36	75.69 ± 0.99	92.32 ± 4.25	60.58 ± 1.90	57.20 ± 0.37
	0.15	63.42 ± 35.24	0.04 ± 0.01	96.66 ± 2.58	96.81 ± 1.30	96.74 ± 1.03	97.43 ± 0.13	96.16 ± 0.19
MNIST with Blend Attack,	0.25	70.43 ± 28.61	0.04 ± 0.01	97.83 ± 0.91	77.68 ± 20.34	97.20 ± 0.66	6.43 ± 1.27	83.86 ± 2.74
Poison Accuracy	0.35	58.32 ± 40.59	0.05 ± 0.03	97.94 ± 0.57	78.79 ± 17.74	97.59 ± 0.12	11.05 ± 2.70	69.69 ± 6.59
	0.45	97.66 ± 1.04	0.03 ± 0.03	98.16 ± 0.58	27.18 ± 19.53	95.17 ± 1.83	4.49 ± 1.08	64.78 ± 3.14
	0.15	64.78 ± 33.81	99.44 ± 0.02	98.29 ± 0.81	97.93 ± 0.25	96.18 ± 1.44	97.30 ± 0.22	95.93 ± 0.20
MNIST with Blend Attack,	0.25	74.00 ± 25.25	99.46 ± 0.05	97.44 ± 0.99	97.30 ± 0.63	97.10 ± 0.74	77.75 ± 0.74	83.34 ± 2.81
Clean Accuracy	0.35	58.62 ± 40.18	99.44 ± 0.02	97.43 ± 0.94	96.25 ± 1.84	97.39 ± 0.19	72.41 ± 3.80	67.92 ± 8.38
	0.45	96.78 ± 1.42	99.42 ± 0.06	97.89 ± 0.61	76.47 ± 8.66	95.07 ± 1.59	63.63 ± 4.47	63.82 ± 4.12
(a) clean data for binary feature value and continuous feature value (b) label flipping attack on both binary feature value and continuous
feature value
Figure 3:	Example of label flipping attack on both binary feature value and continuous feature value
As we can see for the MNIST, especially for the blend attack, the poison accuracy for adversarial
training does show good performance with a large standard deviation. This is because that some
random seeds works while some random seed failed. We hypothesis that this is because MNIST
15
Under review as a conference paper at ICLR 2022
clean/Poison accuracy	0.15	0.25	0.35	0.45
Patch (PRL-AT)	80.25/80.15	78.22/78.14	75.10/75.04	58.90/58.90
Patch(SpectralSign)	80.32/35.90	80.40/29.02	72.01/51.59	24.01/24.10
PatCh(Fine-PrUning)	80.34/56.67	79.50/60.85	79.10/56.84	78.73/44.21
blend (PRL-AT)	67.97/68.48	71.28/71.87	74.12/74.32	61.78/54.19
blend (SpectralSign)	83.60/70.74	81.23/75.40	76.63/66.87	62.53/41.32
blend(Fine-PrUning)	79.53/34.38 一	79.32/13.94 一	78.28/23.71 一	76.70/16.36 -
Table 7: Comparison of averaged performance across three random seed with other baselines on
CIFAR10. The numbers are clean accuracy/poison accuracy. Note fine-pruning used 5% clean data
dataset has almost binary feature value. When adding a small gaussian noise on feature x, the label
flipping attack cannot change the decision boundary much. That is why the noisy label algorithm
seems is not as important as the noisy label algorithm in CIFAR dataset. We plot a two dimensional
toy example in figure 3 to illustrate label flipping attack on continuous features and binary features.
As we can see in the figure that for the binary value feature, the label flipping attack is not easy
to change the decision boundary too much while it can easily change the decision boundary in the
continuous feature value scenario. However, this is a very rough conjecture for the reason why in
MNIST, adversarial training sometimes works. We leave the investigation of this phenomenon in
the future work.
7.5	Comparison against other backdoor defense algorithm
In this section, we show further results against other backdoor defense algorithms. There are many
existing backdoor defense algorithms. However, we found that a large fraction of those algorithms
are either designed for single target attack (Liu et al., 2018) or requires clean data (Liu et al., 2018;
Wang et al., 2019; Li et al., 2021). To investigate how our framework performs compared with the
previous study, We compare against the following two baselines in a similar setting:
•	spectral signature (Tran et al., 2018): filtering data by examining the score of projecting to
singular vector.
•	fine-pruning (Liu et al., 2018): prune the model by deleting non-activated neurons. Note
this method uses 5% clean training data.
The results are in the table 7. As we can see, PRL-AT has higher poisoned accuracy against Spectral
Signature and Fine-Pruning on most settings while the clean accuracy is still high, which indicates
the effectiveness of our algorithm. With high corruption ratio, we find that the robustness of spectral
signature and fine-pruning significantly decreases while PRL-AT still gives reasonable poison accu-
racy. Our theorem provides a principled way to design new defense algorithms by leveraging more
knowledge from noisy label attacks. In this comparison, we only use PRL as the noisy label algo-
rithm, and potentially, by using a more powerful noisy label algorithm (i.e. combination of multiple
methods), it is possible to get higher poisoning accuracy.
7.6	SENSITIVITY ANALYSIS OF THE
One interesting question to ask is how our algorithm performs without knowing the corruption ratio.
In this section, we provided the worst-case result, in which we fix = 0.5 to perform the noisy
label algorithm. When the ground truth corruption ratio is higher than a half, it is impossible to
learn any meaningful classier. We test the PRL-AT in the CIFAR10 on both badnet and blending
attacks. The results are in table 8. As we can see, our algorithm provides robustness even use highly-
overestimated estimated compared with the standard training results in table 2, which suggests our
algorithm is not sensitive to the hyperparameter .
7.7	Discussion about Noisy Label Algorithm
One key question to ask for our framework is how to choose the noisy label algorithm. In terms
of practice, we found PRL gives consistent robustness against both badnet and blending attacks on
16
Under review as a conference paper at ICLR 2022
corruption ratio	PRL-AT (patch)	PRL-AT (blend)
^0T15	68.14/68.00	67.97/68.48
^025	71.78/71.74	71.28/71.87
^035	74.26/74.15	74.17/74.32
0.45	—	69.91/27.02	64.78/54.19
Table 8: PRL-AT top 1 averaged accuracy across three random seeds. The first number is the clean
accuracy while the second number is the poisoned accuracy. The is fixed to be 0.5.
different settings. This might be because that PRL is designed for agnostic corrupted supervision,
which is suitable for a variety of types of label noise attacks.
From a theoretical perspective, analyzing how different noisy label algorithm can minimize the first
term of RHS in equation 6 depends on which noisy label algorithm to use. Here we give a rough
analyze of PRL. PRL guarantees converging to the -approximated stationary point, where is the
corrupted ratio. Formally, we have the following corollary:
Corollary 1 (Convergence of PRL to clean objective (Liu et al., 2021)). Assuming the maximum
clean gradient before loss layer has bounded operator norm:kW kop ≤ C, applying PRL to any
e-fraction supervision corrupted data, yields mint∈[τ] E (∣∣Vφ(Wt)II) = O(e√q) for large enough
T , where q is the dimension of the supervision.
More details can be found on the in Liu et al. (2021). According to above corollary, let WPRL is the
solution get by PRL algorithm, We can have ∣∣Vwprl Rempk = O(e) (i.e. assume q is small). With
Polyak-LCjasieWicz (PL) condition with some constant μ such that DkVf (x)k ≥ μ(f(χ) - f*)
holds, we have μ(Remp - Remp*) ≤，|VwprlRempk = O(e). For a highly-overparameterized
deep neural network, the global optima Rcemp* is usually 0. Thus, we can conclude that with PL
condition, using PRL as the noisy label algorithm in our framework can guarantee Rcemp can be
minimized to the order —O(e). This yields the following proposition:
μ
Proposition 1. LetRt, , τ, is defined as above. Assume the prior distribution of the network param-
eter W isN(0, σ), and the posterior distribution of parameter is N (W, σ) is the posterior parameter
distribution, where W is learned according to training data. Let k to be the number of parameters, n
to be the SampIe size, assume the objective function φw = Lo f is Lφ-Iipschitz smooth and satisfying
the PL condition, which is -∣Vφw k ≥ μ(φw — φw*). then, with the assumption of bounded oper-
ator norm of gradient before loss layer, we have with probability at least 1-δ, by applying PRL-AT,
we have:
Rt ≤ μO(e) + Lφ(2τ + eτ) + ∖
4k log(1 + kwk2) + 1 + log δ + 2 log(6n + 3k)
n-1
In general, considering φ is a deep neural network, we believe the first term is difficult to analyze
without further assumption (i.e. PL condition). Nevertheless, empirical study shows that many noisy
label algorithms can effectively minimize the first term loss, which motivates our method to treat
those algorithms as a black-box algorithm.
17