Under review as a conference paper at ICLR 2022
Decision b oundary variability and
GENERALIZATION IN NEURAL NETWORKS
Anonymous authors
Paper under double-blind review
Ab stract
This paper discovers that the neural network with lower decision boundary (DB)
variability has better generalizability. Two new notions, algorithm DB variabil-
ity and (, η)-data DB variability, are proposed to measure the decision bound-
ary variability from the algorithm and data perspectives. Extensive experiments
show significant negative correlations between the decision boundary variabil-
ity and the generalizability. From the theoretical view, two lower bounds based
on algorithm DB variability are proposed and do not explicity depend on the
sample size. By Chebyshev’s inequality, we also prove two upper bounds of
O ^pAV(∕q, D)∕m(k - 1)) and O (击 + e + η log ,) based on algorithm
and data DB variability, respectively. The algorithm DB variability upper bound is
easier to calculate in practice, while the data DB variability upper bound relies on
less assumptions. Moreover, the bounds do not explicitly depend on the network
size, which is usually prohibitively large in deep learning.
1	Introduction
Neural networks (NNs) have achieved significant success in vast applications (Krizhevsky et al.,
2012; Vaswani et al., 2017). However, the advance of NNs is arduous to be characterized by canon-
ical statistical hypothesis complexity, such as VC-dimension (Vapnik et al., 1994) or Rademacher
complexity (Bartlett & Mendelson, 2002). Considering the intimate connection between low vari-
ance and significant performance in learning theory, we investigate decision boundaries (DBs) from
the perspective of variability in this paper. For neural networks, the decision boundary variability is
largely caused by (1) the randomness from the training algorithm, and (2) the training data. In this
paper, they are measured by two new terms, algorithm DB variability and (e, η)-data DB variability,
and networks with lower DB variability are proved to possess better generalization performance.
Algorithm DB variability measures the variability of DBs in different training repeats, and extensive
experiments are conducted on CIFAR-10, and CIFAR-100 (Krizhevsky & Hinton, 2009) datasets to
explore the factors influence algorithm DB variability. We visualize the trend of the algorithm DB
variability with respect to (w.r.t.) different training strategies, training time, sample sizes, and label
noise. The empirical results present the negative correlation between algorithm DB variability and
the test accuracy in all scenarios, which suggests that the algorithm DB variability largely indicates
the generalization of neural networks. From the theoretical view, two lower bounds and an upper
bound of the generalization error are proved based on the algorithm DB variability. The upper bound
has a rate of O (PAV(∕q, D)/m(k - 1)), where AV(fq, D) is the algorithm DB variability for
fQ on data generating distribution D, mis the sample size, and k is the number of classes.
To present how the training data influences the decision boundary variability, the (e, η)-data DB
variability is introduced by employing the η-subset to reconstruct decision boundaries with error e
(see Definition 5.1). If a decision boundary can be reconstructed by training on a smaller η-subset
with a smaller reconstruction error e, the decision boundary possesses smaller data DB variability.
Moreover, an η-e curve can be drawn by varying the value of η and numerically evaluating the label
mismatch rate. The area under the η-e curve could be a more elaborate indicator for the general-
ization of NNs because the curve contains richer information than the algorithm DB variability. An
1
Under review as a conference paper at ICLR 2022
O (√m + e + η log η) generalization bound based on the (e, η)-data DB variability is established
theoretically to enhance the relationship between the generalization of NNs and DB variability.
Compared to the algorithm DB variability upper bound, the data DB variability bound does not de-
pend on additional Assumption 2, while the algorithm DB variability is easier to calculate than data
DB variability in practice. Moreover, in contrast to many existing generalization bounds based on
hypothesis complexity (Bartlett et al., 2019; Golowich et al., 2018; Bartlett et al., 2017) that require
access to the network weight, our generalization bounds based on DB variability only demand the
network predictions and thus have advantages in empirically approximating the generalization bound
in (1) black-model settings, where model parameters are unavailable; and (2) over-parameterized
settings, where calculating the weight norm is of prohibitively high computing burden.
2	Related works
Deep learning theory. Due to the bias-variance trade-off, model complexity faces a dilemma in
conventional wisdom (Mohri et al., 2018). Recently, Zhang et al. (2021) reveal the surprising ability
of neural networks in fitting noise, but the networks still have impressive generalization performance
in practice. This gap between the well-known bias-variance trade-off and the universal approxima-
tion ability for NNs draws attention to numerous researchers (Belkin et al., 2019; Nakkiran et al.,
2019). Many works attribute the success of NNs to the effectiveness of the stochastic gradient de-
scent (SGD) algorithm (Bottou, 2010; Hardt et al., 2016; Jin et al., 2017). Some empirical studies
also explain the decent performance of networks by uncovering their learning properties (Nakki-
ran et al., 2020; Jiang et al., 2021; He et al., 2021). For instance, neural networks tend to fit the
low-frequency information first (Rahaman et al., 2019; Xu et al., 2019) and gradually learn a more
complex function (Kalimeris et al., 2019) during the training procedure.
Decision boundaries and generalization. Recent studies attempt to understand neural networks
from the aspect of decision boundaries (He et al., 2018; Karimi et al., 2019). Guan & Loew (2020)
empirically show the negative correlation between the complexity of decision boundary and gener-
alization performance. Mickisch et al. (2020) reveal the phenomenon that the distance from data
to decision boundaries continuously decreases during the training procedure. More recently, re-
searchers find that NNs only rely on the most discriminative or simplest features to construct the
DBs (Ortiz-Jimenez et al., 2020; Shah et al., 2020). Instead, our approach is different from these
former methods by considering decision boundary variability, which is empirically and theoretically
shown to closely correlate with the generalization in neural networks.
Adversarial training and generalization. It has been shown that the adversarial examples, which
are created by adding non-perceivable perturbation on the input data, can completely mislead the
NNs (Szegedy et al., 2013; Goodfellow et al., 2014). To tackle this problem, adversarial training is
proposed to improve the robustness of the NNs through continuous training on adversarial examples
(Madry et al., 2017). Nevertheless, Su et al. (2018) and Zhang et al. (2019) show a trade-off between
the robustness and the generalization performance of NNs.
3	Preliminaries
We denote the training set by S = {(xi, yi), i = 1, . . . , m}, where xi ∈ Rn, n is the dimension of
input data, yi ∈ {1, . . . , k}, k is the number of classes, and m = |S| is the training sample size.
We assume that (xi, yi) are independent and identically distributed (i.i.d.) random variables drawn
from the data generating distribution D. Denote the classifier as fθ (x) : Rn → Rk, which is a
neural network parameterized by θ. The output of fθ(x) is a k-dimensional vector and is assumed
to be a discrete probability density function. Let fθ(i) (x) be the i-th component of fθ (x), hence
Pk=I fθi)(x) = L Wedefine T(fθ, x) = {i ∈ {1,…，k}∣fθi)(x) = max, fθj)(x)} to denote
the set of predicted labels by fθ on x. Due to the randomness of the learning algorithm A, let
Q(θ) = A(S) denote the posteriori distribution returned by the learning algorithm A leveraged on
the training set S. Hence, We focus on the Gibbs classifier (a.k.a. random classifier) ∕q = {fθ ∣θ 〜
Q}. 0 - 1 loss is employed in this paper, and the expected risks in terms of θ and Q are defined as:
RD (θ)= E(x,y)~D [I (y ∈ T (fθ, x))]	(1)
2
Under review as a conference paper at ICLR 2022
and
RD (Q) = E(χ,y)〜DEθ 〜Q [I (y/T (fθ, x))],	(2)
respectively. Here I(∙) is the indicator function. Since the data generating distribution D is unknown,
evaluating the expected risk RD is not practical. Therefore, it is a practical way to estimate the
expected risk by the empirical risk RS, which is defined as:
1m
RS (θ) = E(x,y)〜S [I (y ∈T (fθ, x))] = — XI (y/t (fθ, Xi))	(3)
m i=1
1m
RS (Q) = E(χ,y)〜sEθ 〜Q [I (y /T (fθ, X))] = — X Eθ 〜Q [I (y/T (fθ, Xi))],	(4)
m
i=1
where (xi , yi ) / S and m = |S |.
3.1	Decision b oundary
If the output k-dimensional vector fθ (x) on the input example x has a tie, i.e., the maximum value
of the vector is not unique, then x is considered to locate on the decision boundary of fθ. With this
idea, the decision boundary can be formally defined as below:
Definition 3.1 (decision boundary). Let fθ(x) : Rn → Rk be a classifier network parameterized by
θ . Then the decision boundary of fθ is defined by
{X ∈ Rn∣∃i,j / {1,…，k},i = j,fθi)(x) = fθj)(x) = max fθq)(x)}	(5)
After defining the decision boundary, we have the following remark:
Remark 1. (1) If an input example (x, y) is not located on the decision boundary of fθ, T(fθ, x)
is a singleton, and we have
I (y /T (fθ, x)) = I (y = T (fθ, x)).	(6)
(2) If the input x is a boundary point, in practice, we randomly draw a label from the set T(fθ, x)
as the prediction of fθ on x.
3.2	Adversarial training
Adversarial training (AT) is a popular strategy to enhance the adversarial robustness of neural net-
works against adversarial examples, which is generated through projected gradient descent (PGD)
(Madry et al., 2018) in our empirical studies. More specifically, adversarial training can be formu-
lated as solving the minimax-loss problem as follows:
1m
min ∑ll max	`(fθ (x0i) ,yi) ,	(7)
θ m i=1 kx0i-xik≤γ
where γ is the radius to limit the distance between adversarial examples and original examples.
Adversarial training can be considered to enlarge the minimum distances from training examples to
decision boundaries to at least γ .
4	Algorithm decision b oundary variability
Due to the randomness of learning algorithms, there is no doubt that the parameters have a substan-
tial variation when the network is repeatedly trained. However, do the decision boundaries in these
repeated-training networks have a large discrepancy? With this question, we define the algorithm
decision boundary variability (AV) to measure the variability of DBs caused by the randomness of
algorithms in different training repeats.
Definition 4.1 (algorithm decision boundary variability). Let fθ (x) : Rn → Rk be a classifier
network parameterized by θ and let Q(θ) be the distribution over θ . Then, the algorithm decision
boundary variability for fQ on D is
AV(fQ, D)= E(χ,y)〜dEθ,θo〜Q [I (T(fθ, X) = T(fθo, x))],	(8)
where T(fθ, x) = {i /{1,…，k}∣fθi)(x)= maxj fθj)(x)}.
3
Under review as a conference paper at ICLR 2022
■ ■■■■
，上■■留
队容■■■
07a飞鼠i
■ ■■■一
(a)	CIFAR-10
屏篇1口。
■ ■■■■
■ 口 ■立■
■ ■■■■
■ ■■■■
(b)	Fake CIFAR-100
(c)	Algorithm DB variability vs. test accuracy
Figure 1: Algorithm decision boundary variability on CIFAR-10 and CIFAR-100. (a) Examples of
fake CIFAR-10 images generated by conditional BigGAN. (b) Examples of fake CIFAR-100 images
generated by conditional BigGAN. (c) Scatter plots of algorithm DB variability to accuracy on test
set with different architectures and training strategies on CIFAR-10 and CIFAR-100. The colors of
blue, red, and yellow points denote the architectures of VGG-16 (VGG), ResNet-18 (ResN), and
WideResNet-28 (WRN), respectively. The shapes of •, N, and designate the training strategies of
standard training (S), non-data-augmentation training (N), and adversarial training (A), respectively.
Each point is calculated and then averaged on 10 trials.
In the light of Definition 4.1, smaller algorithm DB variability AV (fQ, D) means that the network
produces more stable DBs or converges to more similar DBs during different training repeats. The
algorithm DB variability is also a good notion for the “entropy” of decision boundaries because it
measures the decision uncertainty over the whole data generating distribution; we provide a detailed
discussion in Appendix B.1 due to space limitation.
4.1	Algorithm DB variability and generalization
To explore the relationship between the algorithm DB variability and generalization in neural net-
works, we conduct experiments with different network architectures and training strategies on
CIFAR-10 and CIFAR-100. In detail, VGG-16 (Simonyan & Zisserman, 2014), ResNet-18 (He
et al., 2016), and WideResNet-28 (Zagoruyko & Komodakis, 2016b) are optimized by standard,
non-data-augmentation and adversarial training, respectively, until the training procedure converges.
We clarify that basic data augmentation (crop and flip) (Zagoruyko & Komodakis, 2016a) is adopted
in both standard and adversarial training, and only the basic data augmentation is considered in our
experiments and analysis. Each training setting (dataset, architecture, training strategy) is repeated
for 10 trials with different random seeds to estimate the parameter distribution Q(θ). As for sim-
ulating the data generating distribution, we trained two conditional BigGAN (Zhao et al., 2020),
an advanced generative network architecture, to produce 100, 000 fake images for CIFAR-10 and
CIFAR-100, respectively. Examples of fake images have been shown in Figure 1(a) and 1(b).
Through these generative fake images, we can estimate the algorithm DB variability w.r.t. these
well-trained models. For each training setting, we plot both the average test accuracy and algorithm
DB variability; see Figure 1(c). From the plots, we have several observations: (1) adversarial train-
ing dramatically decreases the test accuracy and promotes the algorithm DB variability compared
to standard training. (2) data augmentation decreases the algorithm decision boundary variability
by comparing the standard training and non-data-augmentation training. The reason is that the aug-
mented images by cropping or flipping are still located on the data generating distribution, so data
augmentation is considered to expand the size of the training set. Hence, the expanded training
set can characterize longer or larger decision boundaries on the data generating distribution; (3)
WideResNet has better test accuracy and lower algorithm DB variability than ResNet and VGG;
and (4) there is a negative correlation between the test accuracy and the algorithm DB variability.
Therefore, based on these findings, we propose the following conjecture:
Hypothesis 1. Neural networks with smaller algorithm decision boundary variability on the data
generating distributions possess better generalization performance.
We then conduct experiments on the algorithm DB variability w.r.t. training time, sample size, and
label noise to concrete this hypothesis.
4
Under review as a conference paper at ICLR 2022
1
CIFAR-IO (LR=0.1)
03 I
O.19
。湛
%
04 ⅞
CIFAR-IO (LR=O.Ol)
CIFAR-100 (LR=0.1)
At=qeμe> Ba Ewo 65
CIFAR-100 (LR=O.Ol)
______I ' I I
—Test error
—Variability '
(a)	Algorithm DB variability and test error vs. training time
CIFAR-100 (LR=0.01)
CIFAR-IO (LR=0.1)
0.6
」0J」a,*jsφh
0.1	0.2	0.3	0.4	0.5
Algorithm DB variability
CIFAR-IO (LR=0.01)
」OJ」'Uκφl
Algorithm DB variability
CIFAR-100 (LR=0.1)
,JOtφκφl
0.5 0.6 0.7
Algorithm DB variability
,10tωκωh-
0.3
0.2
0.2	0.3	0.4
0.7
0.8
0.6
0.5
0.4
0.3
0.2	0.3 0.4	0.5	0.6	0.7
Algorithm DB variability
(b)	Test error vs. algorithm DB variability
Figure 2: (a) Plots of algorithm DB variability and test error as functions of training time (LR is
learning rate). (b) Scatter plots of test error to algorithm DB variability (LR is learning rate). The
points are collected from different epochs. Each curve and point is calculated and then averaged on
10 trials.
4.2	Algorithm DB variability and training time
To investigate the relationship between algorithm DB variability and training time, we train 40
ResNet-18 with different initial learning rates of 0.1 and 0.01 on CIFAR-10 and CIFAR-100. Then,
the algorithm DB variability and test error are calculated at each epoch; see Figure 2(a). From the
plots, we can observe that (1) algorithm DB variability and test error share a very similar curve
w.r.t. the training time; and (2) algorithm DB variability decreases during the training process. The
decline of algorithm DB variability shows that the interpolation on examples reduces the variability
of decision boundaries on data generating distribution. As shown in Figure 2(b), we collect the
points of (algorithm DB variability, test error) from different epochs, and the scatter plots present a
significant positive correlation between test error and the algorithm DB variability, and thus supports
Hypothesis 1.
4.3	Algorithm DB variability and sample size
We next investigate how sample size impacts the algorithm DB variability. 100 ResNet-18 are
trained on five training sample sets of different sizes randomly drawn from CIFAR-10 and CIFAR-
100, while all irrelevant variables are strictly controlled. Then, the algorithm DB variability and test
error are calculated in all cases; see Figure 3(a). From the plots, we have the following observations:
(1) test error and algorithm DB variability share a very similar curve w.r.t. the training sample
size; (2) larger sample size, which intuitively helps obtain a more smooth estimation of the decision
boundary, also contributes to smaller algorithm DB variability; and (3) there is a significant positive
correlation between test error and algorithm DB variability, which fully supports Hypothesis 1.
4.4	Algorithm DB variability and label noise
Belkin et al. (2019); Nakkiran et al. (2019) show the surprising epoch-wise double descent of test
error, especially with the existence of label noise. In this section, we explore the trend of algorithm
DB variability when the label noise exists. We train 20 ResNet-10 for 500 epoch with a constant
learning rate of 0.0001 on CIFAR-10 and CIFAR-100 with 20% label noise. We clarify that the
noise labels remain unchanged during different training repeats, which is necessary to estimate the
algorithm DB variability. Then, the average test error and algorithm DB variability are calculated
at each training epoch, as shown in Figure 3(b). From the plots, we observe that: (1) test error and
5
Under review as a conference paper at ICLR 2022
CIFAR-IO
∙lot"""l
o.:
CIFAR-100
0.8
0.7
0.6
0.5
0.4
0.3
— Test error 0∙55⅛
Variability β5β2
0.45
β.40ffl
035
0.30 ⅛
o.2sσι
,,20	10000
0.20
MOOO 30∞0 40000 5∞00
Sample size
CIFAR-IO
CIFAR-100
(a)	Algorithm DB variability vs. sample size
(b)	Algorithm DB variability vs. time (label noise)

Figure 3: (a) Plots of algorithm DB variability and test error as functions of training sample size
on CIFAR-10 and CIFAR-100. (b) Plots of algorithm DB variability and test error as functions of
training time with the existence of 20% label noise on CIFAR-10 and CIFAR-100. Each curve is
calculated and then averaged on 10 trials.
algorithm DB variability still share a very similar curve w.r.t. the training time with the existence
of label noise; and (2) the algorithm DB variability also undergoes an epoch-wise double descent
during the training process, especially in the left panel of Figure 3(b), which implies that factors
influence the generalization of networks can also have an influence on the algorithm DB variability.
Hence, algorithm DB variability is an excellent indicator for the generalization ability of networks.
4.5	Theoretical Evidence
In this section, we explore and develop the theoretical foundations for the algorithm decision bound-
ary variability on data generating distributions.
Assumption 1. The decision boundary of the classifier network fθ on data generation distribution
D is a set with measure zero.
Corollary 1. Let fθ (x) : Rn → Rk be a classifier network parameterized by θ. If Assumption 1
holds for all θ〜Q, then, for all i ∈ {1,…，k}, We have
E(x,y)〜D [I(i ∈ T(fθ, X))] = E(x,y)〜D [I(T(fθ, X)= i)]	⑼
and
E(x,y)〜D [I(i ∈ T(fθ, X))] = E(x,y)〜D [I(T(fθ, X)= i)] .	(10)
4.5.1	Algorithm DB variability-based lower bounds
Theorem 1 (loWer bound on expected risk). Let fθ(X) : Rn → Rk be a classifier netWork parame-
terized by θ and let Q(θ) be the distribution over θ. Then, if Assumption 1 holds for all θ 〜Q, We
have
Rd (Q) ≥ AVfa
(11)
where AV(∕q, D) = E(χ,y)〜DEe,e，〜Q [I (T(fθ, x) = T f, x))] is the algorithm DB variability
for fQ on data generating distribution D.
The proof is given in Appendix C.1. Theorem 1 provides a loWer bound on the expected risk RD (Q)
based on the algorithm DB variability AV(fQ, D), and the loWer bound presents that the Gibbs clas-
sifier fQ possesses a significant expected risk When its algorithm DB variability is large. Moreover,
When We consider the binary classification, i.e., k = 2, there is a tighter loWer bound:
Theorem 2 (loWer bound for binary case). Let fθ(X) : Rn → R2 be a binary classifier netWork
parameterized by θ and let Q(θ) be the distribution over θ. Suppose the expected risk RD(Q) ≤ 2
and Assumption 1 hold for all θ 〜Q, then we have
RD(Q) ≥
1-，i- 2AV(fQ, D)
(12)
2
6
Under review as a conference paper at ICLR 2022
4.5.2	An algorithm DB variability-based upper bound
Before we present the upper bound based on the algorithm DB variability, let us introduce the
assumption used in this section.
Assumption 2.
2
E(x,y)〜DEθ〜Q [I (y ∈ T (fθ, x))] ≤ k+1 RD (Q),	(13)
where i ∈ {1,2, ∙∙∙ ,k} and k is the number of potential categories.
With Assumption 2, we can derive the upper bound on the variance of risk:
Lemma 1 (upper bound on risk variance). Let fθ(x) : Rn → Rk be a classifier network parameter-
ized by θ and let Q(θ) be the distribution over θ. Suppose Assumption 1 and 2 hold, then we have
Var(x,y)〜D [Eθ〜Q [I (V ∈T (fθ, X))]] ≤ —J-QI~L	(14)
where AV(fQ, D) = E(x,y)〜DEe,e，〜Q [I (T (fθ, x) = T f, x))] is the algorithm DB variability
of fQ on data generating distribution D .
The proof is given in Appendix C.3. Hence, the next theorem is a direct consequence of the one-
sided Chebyshev's inequality: Pr [E[Zι] — ^^ Pmm=I Zi > a] ≤ V2m[Z1] for any a > 0.
Theorem 3 (PAC upper bound on expected risk). Let fθ(x) : Rn → Rk be a classifier network
parameterized by θ and let Q(θ) be the distribution over θ w.r.t. the training set S. Suppose As-
sumption 1 and 2 hold. Then, with the probability of at least 1 — δ over a sample of size m, we
have
RD (Q) ≤ RS(Q) + ∖ IAVv(fQ, D) ∙	(15)
2m(k — 1)δ
The above theorem presents that a smaller algorithm DB variability AV(fQ, D) contributes to a
tighter upper bound on the true risk, which theoretically verifies Hypothesis 1 that neural networks
with smaller algorithm decision boundary variability possess better generalization performance.
5	Data decision b oundary variability
In the previous sections, we introduced the algorithm DB variability, which measures the decision
boundary variability caused by the randomness of learning algorithms. However, the algorithm DB
variability hardly shows the decision boundary variability caused by changes in training data. To
remedy the problem and complete our theorem about decision boundary variability, we define the
data DB variability as below:
Definition 5.1 (data decision boundary variability). Let fθ(x) : Rn → Rk be a classifier network
parameterized by θ, where θ 〜A(S) is returned by leveraging the stochastic learning algorithm A
on the training set S, which is sampled from the data generating distribution D. We term Sη ⊂ S as
a η-subset of S if |S|| = η. Then, if We fixed η and
inf E(χ,y)"Eθ〜A(S),Θ0〜A(Sn) [I (T(fθ, X)= T(fθ0, X))]= 3	(16)
Sη ⊂S
the decision boundary of fA(S) is said to possess a (3, η)-data decision boundary variability.
The data decision boundary variability contains two parameters of 3and η, respectively. That Gibbs
classifier fA(S) has a (3, η)-data DB variability means that only the proportion of η of S, i.e., Sη,
which can be considered as “support vector set”, is enough to reconstruct a similar decision boundary
with the reconstruction error 3. The data DB variability can also be connected with the complexity of
decision boundaries if we assume that simpler decision boundaries rely on fewer “support vectors”;
we provide a detailed discussion in Appendix B.2 due to space limitation.
7
Under review as a conference paper at ICLR 2022
(a) η- curves on CIFAR-10
(b) η- curves on CIFAR-100
(c) Schematic diagram
Figure 4: (a) The η- curves on CIFAR-10 with different training sample sizes 2000 (m2000), 2000
(m2000), 10000 (m10000), 20000 (m20000), and 50000 (m50000), respectively. (b) The η- curves on
CIFAR-100 with different training sample sizes. (c) The schematic diagram of the η- curves w.r.t.
small (ms), medium (mm), large (ml), and infinite (m∞) sample sizes, respectively.
5.1	η- CURVES ABOUT DATA DB VARIAB ILITY
According to Definition 5.1, the data DB variability degrades to the algorithm DB variability
AV (fQ , D) when Sη = S . In other words, the algorithm DB variability is a special case of the
data DB variability with η = 1. Therefore, the data DB variability could present more detailed in-
formation on reflecting how the decision boundary variability depends on the training set, especially
when we observe the variation of the reconstruction error w.r.t. different η.
To explore the relationship between the reconstruction error and the proportion of subset η, we train
1000 networks of ResNet-18 on CIFAR-10 and CIFAR-100 of different sample sizes m, respectively.
Albeit finding the most suitable η-subset is intractable, we adopt a coreset selection approach named
selection via proxy (Coleman et al., 2020), which can rank the importance of training examples, to
estimate the η-subset for a given training set S and proportion η (full list of η involved in our
experiments is presented in Appendix A.3.6). Then, through repeatedly training the network on
Sη, we can estimate the reconstruction error . The η- curves of CIFAR-10 and CIFAR-100 are
presented in Figure 4(a) and 4(b), respectively. From the plots, we have an observation that there
is a more rapid decline in along with small η and also a smaller algorithm DB variability when
the training sample size m is larger. Furthermore, we plot the schematic diagram of η- curves w.r.t.
different sample size m, as shown in Figure 4(c). When η = 0, fA(Sn) cannot be better than random
guess, and hence e = k-1, where k is the number of potential categories. We have noticed that e
has a sharper drop along with η when the sample size m is larger. Therefore, we rationally propose
the following assumption, which is also shown by the right angle with m∞ in Figure 4(c).
Assumption 3. If m → ∞, then we have e → 0 when η → 0.
These plots indicate that the area under the η-e curve could be a more meticulous predictor for the
generalization ability of neural networks compared to the algorithm DB variability, which is only
a point on the η-e curve when η = 1. Hence, the area under the η-e curve can also be considered
as an extension of the algorithm DB variability: if the Gibbs classifier fA(S) possesses a smaller
area under the η-e curve, it produces more stable decision boundaries with varying training subsets.
In the following section, we will theoretically establish the connection between the parameters of η
and e and the generalization ability of neural networks.
5.2	Theoretical evidence
In this section, we develop the theoretical foundations for the data decision boundary variability, and
present that neural networks with better data DB variability possess better generalization.
Lemma 2. If the decision boundaries of fA(S) possess a (e, η)-data DB variability, then we have
|Rd(A(S)) -Rd(A(Sn))| ≤ e.	(17)
Lemma 2 shows that the difference between the expected risk of A(S) and A(Sη) can be bounded
by their difference in decision boundaries. Then, by applying the concentration inequality, we can
8
Under review as a conference paper at ICLR 2022
bound the difference between their empirical risk on the same training set S with the probability
1 - δ:
Lemma 3. If the decision boundaries of fA(S) possess a (, η)-data DB variability, then, with prob-
ability of at least 1 - δ over a sample of size m, we have
RS(A(Sn)) ≤ RS(A(S)) + ^l2m log -+ + e,	(18)
where m = |S| is the training sample size. If we further assume the training error RS (A(S)) = 0,
then, with the probability of at least 1 - δ over a sample of size m, we have
RS(A(Sn)) ≤ ∖ lo- log τ + e	(19)
2m	δ
Theorem 4 (data DB variability-based upper bound on expected risk). If the decision boundaries of
fA(S) possess a (e, η)-data DB variability on the data generating distribution D, and assume η ≤ 0.5
and the empirical risks RS (A(S)) = RSη (A(Sn)) = 0, then, with the probability of at least 1 - δ
over a sample of size m, we have
RD(A(S)) ≤ Ω + √4Ω∆ + 8△ + e,	(20)
where	_______
ω = t~— [ ʌ/τy- log I + e ) ,	QI)
1	- η	2m	δ
△ = η log e + Log 2.	(22)
ηm δ
Moreover, for sufficient large m, we have
RD(A(S)) ≤ O(J= +e + ηlog -).	(23)
mη
The proof is omitted here and is given in Appendix C.7. According to Assumption 3, when m → ∞,
then η → 0 and e → 0, and according to Eq. 23, RD(A(S)) → 0. Therefore, the generalization
bound is asymptotically converged. Theorem 4 presents that smaller data DB variability, i.e., smaller
e and η, contributes to a tighter upper bound on the expected risk, and also theoretically verifies the
relationship between the data DB variability and the generalization ability of neural networks.
6 Conclusion and discussion
In this paper, we empirically and theoretically explored the relationship between decision boundary
variability and generalization in neural networks, through the algorithm DB variability and data DB
variability, respectively. A significant negative correlation between the decision boundary variability
and generalization performance is observed in our experiments. As for the theoretical results, two
lower bounds and two upper bounds were proposed based on algorithm DB variability and data DB
variability to enhance our findings. The algorithm DB variability upper bound is easier to calculate
in practice, while the data DB variability upper bound relies on less assumptions. Moreover, in
contrast to many existing generalization bounds based on hypothesis complexity that require access
to the network weight, our generalization bounds based on decision boundary variability only de-
mand access to the network predictions and thus have advantages in empirically approximating the
generalization bound in (1) black-model settings, where model parameters are unavailable; and (2)
over-parameterized settings, where calculating the weight norm is of prohibitively high computing
burden. Furthermore, the analysis in Section 5.1 mentioned that the algorithm DB variability is a
special case of the data DB variability. Therefore, unifying the theoretical works about algorithm
DB variability and data DB variability will be a promising direction in future works.
Acknowledgement
The authors sincerely appreciate the anonymous reviewers for their valuable suggestions.
9
Under review as a conference paper at ICLR 2022
References
Peter Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural
networks. arXiv preprint arXiv:1706.08498, 2017.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Peter L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension
and pseudodimension bounds for piecewise linear neural networks. The Journal of Machine
LearningResearch, 20(1):2285-2301, 2019.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-
learning practice and the classical bias-variance trade-off. Proceedings of the National Academy
of Sciences, 116(32):15849-15854, 2019.
Leon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of
COMPSTAT’2010, pp. 177-186. Springer, 2010.
Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis, Percy
Liang, Jure Leskovec, and Matei Zaharia. Selection via proxy: Efficient data selection for deep
learning. In International Conference on Learning Representations, 2020. URL https://
openreview.net/forum?id=HJg2b0VYDr.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. In Conference On Learning Theory, pp. 297-299. PMLR, 2018.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Shuyue Guan and Murray Loew. Analysis of generalizability of deep neural networks based on
the complexity of decision boundary. In 2020 19th IEEE International Conference on Machine
Learning and Applications (ICMLA), pp. 101-106. IEEE, 2020.
Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In International Conference on Machine Learning, pp. 1225-1234. PMLR,
2016.
Fengxiang He, Shiye Lei, Jianmin Ji, and Dacheng Tao. Neural networks behave as hash encoders:
An empirical study. arXiv preprint arXiv:2101.05490, 2021.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Warren He, Bo Li, and Dawn Song. Decision boundary analysis of adversarial examples. In Inter-
national Conference on Learning Representations, 2018.
Yiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. Assessing generalization of
sgd via disagreement. arXiv preprint arXiv:2106.13799, 2021.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape
saddle points efficiently. In International Conference on Machine Learning, pp. 1724-1732.
PMLR, 2017.
Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan Yang, Boaz Barak,
and Haofeng Zhang. Sgd on neural networks learns functions of increasing complexity. Advances
in Neural Information Processing Systems, 32:3496-3506, 2019.
Minguk Kang and Jaesik Park. Contragan: Contrastive learning for conditional image gen-
eration. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 21357-21369. Curran As-
sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
f490c742cd8318b8ee6dca10af2a163f- Paper.pdf.
10
Under review as a conference paper at ICLR 2022
Hamid Karimi, Tyler Derr, and Jiliang Tang. Characterizing the decision boundary of deep neural
networks. arXiv preprint arXiv:1912.11460, 2019.
A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Master’s
thesis, Department of Computer Science, University of Toronto, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep con-
VolUtional neural networks. Advances in neural information processing systems, 25:1097-1105,
2012.
Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial
large learning rate in training neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alche-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Sys-
tems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.
cc/paper/2019/file/bce9abf229ffd7e570818476ee5d7dde-Paper.pdf.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018.
David Mickisch, Felix Assion, Florens Greβner, Wiebke Gunther, and Mariele Motta. Under-
standing the decision boundary of deep neural networks: An empirical study. arXiv preprint
arXiv:2002.01810, 2020.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning.
MIT press, 2018.
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. arXiv preprint arXiv:1912.02292,
2019.
Preetum Nakkiran, Behnam Neyshabur, and Hanie Sedghi. The deep bootstrap framework: Good
online learners are good offline generalizers. arXiv preprint arXiv:2010.08127, 2020.
Guillermo Ortiz-Jimenez, Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, and Pascal
Frossard. Hold me tight! influence of discriminative features on deep network boundaries. arXiv
preprint arXiv:2002.06349, 2020.
Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua
Bengio, and Aaron Courville. On the spectral bias of neural networks. In International Confer-
ence on Machine Learning, pp. 5301-5310. PMLR, 2019.
Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. The
pitfalls of simplicity bias in neural networks. arXiv preprint arXiv:2006.07710, 2020.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-
rithms. Cambridge university press, 2014.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Dong Su, Huan Zhang, Hongge Chen, Jinfeng Yi, Pin-Yu Chen, and Yupeng Gao. Is robustness
the cost of accuracy?-a comprehensive study on the robustness of 18 deep image classification
models. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 631-648,
2018.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
11
Under review as a conference paper at ICLR 2022
Vladimir Vapnik, Esther Levin, and Yann Le Cun. Measuring the vc-dimension of a learning ma-
chine. Neural computation, 6(5):851-876, 1994.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Zhi-Qin John Xu, Yaoyu Zhang, Tao Luo, Yanyang Xiao, and Zheng Ma. Frequency principle:
Fourier analysis sheds light on deep neural networks. arXiv preprint arXiv:1901.06523, 2019.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC, 2016a.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint
arXiv:1605.07146, 2016b.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107-
115, 2021.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.
Theoretically principled trade-off between robustness and accuracy. In International Conference
on Machine Learning, pp. 7472-7482. PMLR, 2019.
Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han. Differentiable augmentation for
data-efficient gan training. arXiv preprint arXiv:2006.10738, 2020.
12
Under review as a conference paper at ICLR 2022
A Additional experiments details
This section provides all the additional details of our experiments.
A. 1 Datasets
Our experiments are conducted on two public datasets (CIFAR-10 and CIFAR-100 (Krizhevsky &
Hinton, 2009)) and two manufactured datasets (fake CIFAR-10, and fake CIFAR-100). The detail
of these datasets are shown as follows.
•	CIFAR-10 consists of 50, 000 training images and 10, 000 test images from 10 different
classes, and CIFAR-100 consists of 50, 000 training images and 10, 000 test images from
100 different classes. One can download CIFAR-10 and CIFAR-100 from https://
www.cs.toronto.edu/~kriz∕cifar.html.
•	Fake CIFAR-10 consists of 100, 000 test images from 10 different classes. The fake
CIFAR-10 is generated by conditional BigGAN (Zhao et al., 2020). The pre-trained Big-
GAN is provided by Kang & Park (2020) and can be download from https://drive.
google.com/drive/folders/1xVN7dQPWMLi8gDZEb5FThkjbFtIdzb6b.
•	Fake CIFAR-100 consists of 100, 000 test images from 100 different classes. The gener-
ation of fake CIFAR-100 is similar to it of fake CIFAR-10, while the training set for the
BigGAN is replaced from CIFAR-10 to CIFAR-100.
A.2 Model architectures
We use different neural network architectures in our experiments, including VGG-16, ResNet-18,
and WideResNet-28. The architectures are presented in Table 1.
Table 1: Detaned model architectures for CIFAR-10/100
VGG-16	ResNet-18 WideResNet-28-10
(3 × 3, 32) × 2 maxpool, 2 × 2		3 3	3 × 3, 64			3 3	3 × 3, 16		
(3 × 3, 128) maxpool, 2	×2 ×2		× 3, 64 × 3, 64	×2			× 3, 160 × 3, 160	×4	
(3 × 3, 256) maxpool, 2	×3 ×2	3 3	× 3, 128 × 3, 128	×	2	3 3	× 3, 320 × 3, 320	×	4
(3 × 3, 512) maxpool, 2	×3 ×2	3 3	× 3, 256 × 3, 256	×	2	3 3	× 3, 640 × 3, 640	×	4
(3 × 3, 512) maxpool, 2	×3 ×2	3 3	× 3, 512 × 3, 512	×	2				
fc-4096 fc-4096			avgpool				avgpool		
fc-10/100	fc-10/100	fc-10/100
A.3 implementation details
This section provides all the additional implementation details for our experiments.
A.3.1 Model training
We employ SGD to optimize all the models and the momentum factor is 0.9. The weight decay
factor is set to 5e-4, and the learning rate is decayed by 0.2 every 50 epochs.
13
Under review as a conference paper at ICLR 2022
A.3.2 Additional details in Section 4.1
We train VGG-16, ResNet-18, Wide-ResNet-28 on CIFAR-10 and CIFAR-100. In the training pro-
cedure, the model is trained for 200 epochs, in which the batch size is set to 128, and the learning
rate is initialized as 0.1. There are three training strategies included in this experiment: standard
training, non-data-augmentation training, and adversarial training. In the adversarial training, the
radius of the adversarial perturbation is set as 10/255 and l∞ distance is selected. The basic data
augmentation (cropping and flipping) in the standard training and adversarial training is achieved by
the following Pytorch code:
1	transforms.RandOmCrop(32, Padding=4)
2	transforms.RandOmHorizontalFlip()
The experiment is repeated for 10 trials for each (dataset, architecture, training strategy) setting.
A.3.3 Additional details in Section 4.2
We repeatedly train 10 ResNet-18 on CIFAR-10 and CIFAR-100, respectively, with different random
seeds. In the training procedure, the model is trained for 200 epochs, in which the batch size is set
to 128, and the learning rate is initialized as 0.1 and 0.01, respectively. Basic data augmentation is
included during the training process.
A.3.4 Additional details in Section 4.3
We randomly sample examples from the training set of CIFAR-10 and CIFAR-100 to form five
datasets with different sizes of [2000, 5000, 10000, 20000, 50000], respectively. 10 ResNet-18 are
trained for each dataset. In the training procedure, the model is trained for 200 epochs, in which
the batch size is set to 128, and the learning rate is initialized as 0.1. Basic data augmentation is
included during the training process.
A.3.5 Additional details in Section 4.4
We randomly change the labels of 20% examples in the training set of CIFAR-10 and CIFAR-100.
Then, 10 ResNet-18 are optimize by SGD for 500 epochs on the noise CIFAR-10 and CIFAR-100,
respectively. the momentum factor is 0.9, and the learning rate is 0.001 and does not decay during
the training process.
A.3.6 Additional details in Section 5.1
We randomly sample examples from the training set of CIFAR-10 and CIFAR-100 to form
five datasets with different sizes of [2000, 5000, 10000, 20000, 50000], respectively. For each
dataset, we obtain 10 η-subsets with different η of [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
via a coreset selection approach named selection via proxy (Coleman et al., 2020). The re-
lated code can be downloaded from https://github.com/stanford-futuredata/
selection-via-proxy. The ResNet-18 is repeatedly trained for 10 trials to estimate the com-
plexity of decision boundaries for each η-subset.
14
Under review as a conference paper at ICLR 2022
B Additional discussion on decision b oundary variability
B.1	Algorithm DB variab ility and the entropy of decision boundaries
If Assumption 1 holds for all θ 〜 Q, 1 - AV(∕q, D) can be rewritten as
k
E(x,y)~DEθ,θ0~Q [I (T (fθ, X)= T (fθ, x))] = E(x,y)~D X Eθ~Q [I (T (fθ, x) = i)].	(24)
i=1
The term Pk=I EΘ~q [I(T (fθ, x) = i)] can be considered to measure the degree of prediction
uncertainty for the given voxel X in the input space Rn. If We leverage - log(∙) on the term
Pk=I Eθ~Q [I(T (fθ, x) = i)], then - log Pk=I Eθ~Q [I(T (fθ, x) = i)] denotes the collision en-
tropy of prediction made by the Gibbs classifier fQ on x. We can also replace the collision entropy
with canonical Shannon entropy - Pk=I Eθ~q [I(T (fθ, x) = i)] log Eθ~q [I(T (fθ, x) = i)] in the
future research. Hence, the algorithm DB variability is closely related to the “entropy of decision
boundary”, and the uncanny generalization in neural networks might be further uncovered by inves-
tigating this low entropy of decision boundary.
B.2	Data DB variability and the complexity of DBs
According to Guan & Loew (2020), a complex decision boundary is considered to own large cur-
vatures and conjectured to indicate inferior generalization. Nevertheless, from the perspective of
causality, we argue that the large curvature or non-linearity of DBs is the result other than the cause
for the classification task, and the primary reason for shaping a complex DB during the training pro-
cedure should be the significant non-linearity of the training data. If only the geometric properties of
decision boundaries are analysed without investigating the data, the results might be incomplete and
even misleading. Another obstacle for describing the complexity of DBs via its geometric properties
is the huge dimensional input space, which makes the geometric properties of DBs hard to quantify
and estimate. Therefore, defining the complexity of DBs based on its curvature is not rational and
impractical.
Here we consider the complexity of DBs from the perspective of the training set. During the training
procedure, a small part of training examples, considered as “support vectors”, play a more critical
role in supervising the formation of decision boundaries and compelling the DB to be gradually
more complicated. If the construction of decision boundaries relies on fewer “support vectors”, the
decision boundary should be simpler. In other words, if these “support vectors” are excluded from
the training sample, the DB will be notably dissimilar when the network is retrained on the modified
training set. Hence, the complexity of DBs can be also defined with the notion of the data DB
variability:
Definition B.1 (complexity of decision boundaries). Let fθ(x) : Rn → Rk be a classifier network
parameterized by θ, where θ 〜A(S) is returned by leveraging the stochastic learning algorithm A
on the training set S, which is sampled from the data generating distribution D. We term Sη ⊂ S as
a η-subset of S if lS∣l = η. Then, if we fixed η and
inf E(χ,y)~DEθ~A(s),θ0~A(Sη) [I (T(fθ, x)= T(fθ0, x))]= 3	(25)
Sη ⊂S
the decision boundary of fA(S) is said to possess a (3, η)-complexity.
Implications about the complexity of DB. With Definition B.1, we could more intuitively under-
stand the relationship between the complexity of DBs and the generalization ability in deep learning:
(1) difficult tasks generally have more complex decision boundaries, since their datum are more non-
linear and contain more “support vectors”; (2) in adversarial training, each data point is converted
to a “data ball” with the radius of the adversarial perturbation and has more impact on forming the
DBs. Hence, adversarial training contributes to a more complex decision boundary by enlarging
the “support vector set”; (3) for data augmentation, generated images are also considered to obey
the data generation distribution D. Hence, data augmentation decreases the complexity of decision
boundaries by greatly expanding the training set S, while ∣Sη | has only a slight growth. Besides, ac-
cording to our theoretical results in Section 5.2, it can be verified that neural networks with simpler
decision boundaries possess better generalization performance.
15
Under review as a conference paper at ICLR 2022
C Proofs
To avoid technicalities, the measurability/integrability issues are ignored throughout this paper.
Moreover, Fubini’s theorem is assumed to be applicable for any integration w.r.t. multiple vari-
ables. In other words, the order of integrations is exchangeable. We also use E2 [∙] to denote [E [∙]]2
in our proof for simplicity.
C.1 Proof of Theorem 1
Proof. If Assumption 1 holds for all θ 〜Q, we have
E(χ,y)~DEθ,θ0~Q [I (y ∈ T (fθ, x)) I (y ∈ T (fθ0, x)) I (T (fθ, X)= T (fe，, x))]=0	(26)
Hence,
AV(fQ, D)=E(X,y)~DEθe~Q [I (T(fθ, X)= T(fa，, x))]	(27)
=E(x,y)~DEθ,θ0~Q [I (y ∈ T(fθ, x)) I (y ∈ T f, x))	(28)
+ I (y ∈ T(fθ, x)) I (T(fθ, X)= T(fθ0, x))]	(29)
≤2RD(Q)	(30)
The proof is completed.	口
C.2 Proof of Theorem 2
Proof. When the classification is binary, i.e., k = 2, with Assumption 1, we have
E(x,y)~DEθ,θ0~Q [I (T (fθ, X) = T (fθ, x))]	(31)
=E(x,y)~D Eθ~Q [I (y ∈ T (fθ, X))] + E(χ,y)~DEθ~Q [I (y ∈T (fe, x))]	(32)
=E(x,y)~DEθ~Q [I (y ∈ T (fe, x))]+ Eχy)~D [1 - Eθ~q [I (y ∈ T f, x))]]2	(33)
=2Eχy)~DEθ~Q [I (y ∈ T (fe, x))]+ 2Rd(Q) - 1	(34)
Plugging in Varχy)~D [Eθ~q [I (y ∈ T (fe, x))]] = E(χ,y)~DE∣~q [I (y ∈ T (fe, x))] - (1 -
RD(Q))2 yields
E(x,y)~DEe,eo~Q [I (T (fe, x) = T (fe, x))]	(35)
=2Var(X〃)~D [Ee~Q [I (y ∈ T (fe, x))]]+ RD(Q) + (1 - RD(Q))2	(36)
≥R2D(Q) + (1 - RD(Q))2	(37)
Plugging in AV f D) = 1-E(χ,y)~DEe,e，~Q [I (T(fe, x) = T(fe，, x))] and solving the inequal-
ity yield the desired inequality and finishes the proof of Theorem 2.	口
16
Under review as a conference paper at ICLR 2022
C.3 Proof of Lemma 1
Proof. Without loss of generality, we assume that all examples are belong to the first class, i.e.,
y =LHence,for 1 - AV f D) = E(x,y)〜D Ee,e，〜Q [I (T (fθ, x) = T f, x))], we have
E(x,y)〜DEθ,θ0〜Q [I (T (fθ, X)= T (fθ0, X))]	(38)
k
=E(x,y)〜D X Eθ〜Q [I (T (fθ, x) = i)]	(39)
i=1
"k
Eθ〜Q [I (T (fθ, x) = y)] + X Eθ 〜Q [I (T (fθ, x) = i)]	(40)
i=2
=E(x,y)〜D	(1 - X Eθ〜Q [I (T (fθ, x)=i)]) + X Eθ〜Q [I (T (fθ, x) = i)]	(41)
i=2	i=2
kk
≤1 + 2 X E(χ,y)〜DEθ〜Q [I (T (fθ, x) = i)] - 2 X E(χ,y)〜dEθ〜Q [I (T (fθ, x) = i)]
+ 2 X (E(x,y)〜DEθ〜Q [I (T (fθ, X)= i)]E(x,y)〜DEθ〜Q [I (T (fθ, x) = j)])	(42)
i>j>1
kk
≤1 + 2 X E(χ,y)〜dEΘ〜Q [I (T (fθ, x) = i)] - 2 X E(χ,y)〜dEθ〜Q [I (T f x) = i)]
k
+ (k - 2) X (E(χ,y)〜dEθ〜Q [I (T (fθ, x) = i)])2	(43)
i=2
kk
≤1 + 2 XE(χ,y)〜DEθ〜Q [I (T (fθ, x) = i)] - 2 X E(χ,y)〜dEθ〜Q [I (T (fθ, x) = i)]
k
+ (k - 1) X (E(χ,y)〜DEθ〜Q [I (T (fθ, x) = i)])2	(44)
i=2
The penultimate inequality holds because 2pipj ≤ pi2 + pj2 .	Moreover, the variance of
Eθ〜Q [I (y ∈ T (fθ, x))] is
Var(x,y)〜D [Eθ〜Q [I (T (fθ, x) = y)]]	(45)
=E(x,y)〜dEΘ〜Q [I (T (fθ, x) = y)] - Ejχ,y)〜DEθ〜Q [I (T f, x) = y)],	(46)
plugging the equality into Eq. 44 yields
1	-AV(fQ,D)
≤1 - (k - 1)Var(x,y)〜D [Eθ〜Q [I (T (fθ, x) = y)]] + (k - 1)E(χ,y)〜dEΘ〜Q [I (T (fθ, x) = y)]
k
-(k - 1)E2χ,y)〜dEθ〜Q [I (T (fθ, x) = y)] + (k - 1) X EjxeyEey [I (T f, x) = i)]
i=2
kk
+ 2 X E(χ,y)〜dEΘ〜Q [I (T (fθ, x) = i)] - 2 X E(χ,y)〜DEθ〜Q [I (T (fe, x) = i)]	(47)
Then, because Ee〜Q [I (T (fe, x) = y)] = pk=2 Ee〜Q [I (T (fe, x) = i)], we have
k
X Ejχ,y)〜DEe〜Q [I (T (fθ, x) = i)] ≤ Ejχ,y)〜DEe〜Q [I (T (fe, x) = y)]	(48)
i=2
and
k
XE(x,y)〜DEe〜Q [I (T (fe, x) = i)] ≤ E(χ,y)〜DEe〜Q [I (T (fe, x) = y)].	(49)
i=2
17
Under review as a conference paper at ICLR 2022
Therefore, Eq. 47 can be scaled to
(k — 1)Var(x,y)〜D [Eey [I (T (fθ, x) = y)]]
k
≤AV(fQ, D) + (k + 1)E(x,y)〜DEθ〜Q [I (T (fθ, X) = y)] — 2 X E(x,y)〜DEe〜Q [I (T (fθ, x) = i)]
i=2
(50)
Then, with Assumption 2 that E(x,y)〜DEθ〜Q [I (y ∈ T (fe, x))] ≤ k+2^RD(Q), We get the follow-
ing inequality w.r.t. the variance:
Var(x,y)〜D [Ee〜Q [I (T (fe, x) = y)]] ≤ 玛?D).	(51)
The proof is completed.	□
C.4 Proof of Theorem 3
Proof. For the random variable Zi, recall the one-sided Chebyshev’s inequality that
Pr [e[Zi] — ɪ X Zi > a] ≤ VrZp.	(52)
m	2ma2
i=1
Let Zi = Ee〜Q [I (yi ∈ T (fθ, xi))], So We have
Pr [Rd (Q) -Rs (Q) > a] ≤ Var(X,y)~D [Eθ~Q [I (y ∈ T (fe, X))]].	(53)
2ma2
Plugging Lemma 1 into the above inequality yields
Pr [Rd (Q) -Rs(Q) > a] ≤ JVfQ,D)2.	(54)
2m(k — 1)a2
Plugging in δ = 2AVk-湍 yields the desired inequality and completes the proof.	□
C.5 Proof of Lemma 2
Proof. From the definition of the complexity of DB, there exists a η-subset Sη s.t.
E(x,y)〜DEe〜A(S),θ0〜A(Sn) [I (T(fθ, x) = T f, x))] = e.	(55)
Recall the LHS = |Rd(A(S)) 一 RD(A(Sn))|, then
LHS = ∣E(χ,y)〜DEe〜A(S) [I(y ∈ T(fe, x))] 一 E(χ,y)〜DEe〜A(Sn) [I(y ∈ T(fe,x))]∣	(56)
=∣E(χ,y)〜DEe〜/(S)/〜A(Sn) [I (y ∈ T(fe, X))- I (y ∈ T(fe，, x))]∣	(57)
≤ E(x,y)〜DEe〜A(S),θ0〜A(Sn) [|I 3 ∈ T(fθ, x)) — I (y ∈ T(fθ0, X)) ∣]	(58)
≤ E(x,y)〜DEe〜A(S),e0〜A(Sn) [I (T(fθ, x) = T(fθ0, x))] = C	(59)
The proof is completed.	□
C.6 Proof of Lemma 3
Proof. Let
M = E(x,y)〜DEe〜A(S),θ0〜A(Sn) [I (T(fθ, x) = T(fθ0, x))]
1m
μ = - X Ee〜A(S),e0〜A(Sn) [I (T(fe, Xi)= T(fe0, Xi))],
m i=1
By applying Hoeffding’s Inequality, we have
Pr [μ — μ ≥ t] ≤ exp(-2mt2).
Plug in δ = exp(—2mt2) into Eq. 62, thus, with the probability of at least 1 — δ, we have
μ ≤∖/ɪlθgɪ + μ ≤∖/ɪlθgɪ+ C.
2m	δ	2m δ
(60)
(61)
(62)
(63)
Following the proof of Lemma 2, we can derive the inequality of IRS (A(Sn)) — RS (A(S ))| ≤ μ.
Plugging this into Eq. 63 yields the desired inequality and concludes the proof of Lemma 3.	□
18
Under review as a conference paper at ICLR 2022
C.7 Proof of Theorem 4
We first introduce Lemma 4 and Lemma 5 as below.
Lemma 4 (Lemma 30.1 in (Shalev-Shwartz & Ben-David, 2014)). Assume T and V are two
datasets independently sampled from the data generating distribution D, then, with the probabil-
ity of at least 1 - δ, we have
Rd(A(T)) ≤ RV(A(T)) + JRV(A(TV) log(10 + ▼.
(64)
Lemma 5 (Theorem 30.2 in (Shalev-Shwartz & Ben-David, 2014)). Let Sη be a η-subnet of the
dataset S, which is sampled from the data generation distribution D and the sample size |S| = m.
Let S∖Sη = S-Sn and assume η ≤ 0.5. Then, With the probability of at least 1 - δ over a sample
of size m, we have
RD(A(Sn)) ≤ Rs∖Sη(A(Sn)) + "S'、§"(A(Sn))△ + 8∆,	(65)
Where
e1	1
△ = η log - +——log T0.	(66)
η	m	δ0
Proof of Lemma 5.
D LC LC t 2 2 R<∖∖∖ A (S	2 rs∖ ʌʌ , ∕2Rs∖Sη (A(Sn)) log(1∕δ)	4log(1∕δ)
Pr ∃Sn ⊆ S s.t. RD(A(Sn)) ≤ Rs∖Sη (A(Sn)) + V-------∣S∖S∏------------1 ∣s∖s ∣
(67)
< X Pr "τ? (A(S N < 其 (U(S V+ S 2RS∖Sη (A(Sn ))log(1∕δ)	4log(1∕δ)
≤ TPr RD(A(Sn)) ≤ RSS)) + √---------------------5Sj-----------+ |S\S I
Sη ⊆S	n	n
(68)
=(m )δ≤ (e)” δ	(69)
ηm η
Plug in δ0 = (e)	δ, and use the assumption η ≤ 2, which implies |S\Sn ∣ ≥ mm, then, with the
probability of at least 1 - δ0, We have that
RD(A(Sn)) ≤ RS∖Sη (A(SnX+:4RS∖Sη (A(Sn)) (log j + J log J) +8 (η log j + ~ log J
(70)
which concludes the proof.	□
With the above lemmas, we can derive the generalization bound based on the complexity of decision
boundary.
Proof of Theorem 4. With the assumption of zero-training error that RS (A(S)) = RSη (A(Sn)) =
0, we can derive the follow inequality:
RS∖Sη (A(Sn)) = 士(RS(A(Sn)) - RSn(A(Sn))) = RRS(A(Sn))	(71)
Then, by plugging the above equation into to Lemma 3, with the probability of at least 1 - δ, we
have	________
RS'Sn (A(Sn)) ≤ τ⅛ (r2mlog 1 + J	(72)
Through combining this with Lemma 5, with the probability of at least 1 - 2δ, we have
RD(A(Sn)) ≤ Ω + √4Ω∆ + 8∆,	(73)
19
Under review as a conference paper at ICLR 2022
where	______
ω = T~— [ ∖∕χ- log V + e ) ,	(74)
1 - η 2m δ
∆ = η log e + Log 1.	(75)
ηm δ
Plugging the equality of RD (A (S)) ≤ RD (A (Sη)) + e in Lemma 2 into Eq. 73, with the proba-
bility of at least 1 - 2δ, we have
RD(A(S)) ≤ Ω + √4ΩA + 8△ + e.	(76)
Plugging in δ0 = 2δ yields the desired inequality of Eq. 20.
When m is sufficient large, √4Q∆ can be dropped due to √4Q∆ ≤ Ω + ∆. Considering η ≤ 0.5,
Ω ≤ 2 (《2m log 1 + e) = O(√m + e). The term . log ∣ in △ can be dropped because it has a
faster convergence speed compared to J2m log 1 in Ω. Because log 1 is considered as a constant,
RD(A(S)) ≤ O(^= + e + ηlog1).	(77)
mη
The proof of Theorem 4 is finished.	□
D Additional experimental results
This appendix collect experimental results omitted from the main text due to the space limitation.
D. 1 Additional results for learning rate
Accroding to Li et al. (2019) that learning rate is a major factor of affecting generalization of net-
works, we investigate the relationship between algorithm DB variability and learning rate by training
20 ResNet-18 with different constant learning rates (no decay) of 0.001 and 0.0001 on CIFAR-10
until the training procedure converges. Then, the average test error and algorithm DB variability are
calculated at each epoch, as shown in Figure 5. From the plots, we observe that: (1) the larger learn-
ing rate 0.001 contributes to better generalization performance; and (2) algorithm DB variability
with the larger learning rate 0.001 is smaller during the training process, compared to the learning
rate 0.0001. Therefore, there is still a negative correlation between algorithm DB variability and
generalization by varying the learning rate, and thus Hypothesis 1 is well supported.
CIFAR-IO
①IS①一
(a) Test error vs. training time
Figure 5: (a) Plots of test error as a function of training time (LR is learning rate). (b) Plots of
algorithm DB variability as a function of training time (LR is learning rate). Each curve is calculated
and then averaged on 10 trials.
ClFAR-IO
(b) Algorithm DB variability vs. training time
20