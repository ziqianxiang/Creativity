Under review as a conference paper at ICLR 2022
Delayed Geometric Discounts: An alternative
criterion for Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
The endeavor of artificial intelligence (AI) is to design autonomous agents capable
of achieving complex tasks. Namely, reinforcement learning (RL) proposes a the-
oretical background to learn optimal behaviors. In practice, RL algorithms rely on
geometric discounts to evaluate this optimality. Unfortunately, this does not cover
decision processes where future returns are not exponentially less valuable. De-
pending on the problem, this limitation induces sample-inefficiency (as feed-backs
are exponentially decayed) and requires additional curricula/exploration mecha-
nisms (to deal with sparse, deceptive or adversarial rewards). In this paper, we
tackle these issues by generalizing the discounted problem formulation with a
family of delayed objective functions. We investigate the underlying RL prob-
lem to derive: 1) the optimal stationary solution and 2) an approximation of the
optimal non-stationary control. The devised algorithms solved hard exploration
problems on tabular environment and improved sample-efficiency on classic sim-
ulated robotics benchmarks.
1	Introduction
In the infinite horizon setting, and without further assumptions on the underlying Markov Decision
Process (MDP), available RL algorithms learn optimal policies only in the sense of the discounted
cumulative rewards (Puterman, 2014). While the geometric discounting is well suited to model a
termination probability or an exponentially decaying interest in the future, it is not flexible enough to
model alternative weighting of the returns. Consider for example settings where the agent is willing
to sacrifice short term rewards in favor of the long term outcome. Clearly, for such situations, a
discounted optimality criterion is limited and does not describe the actual objective function.
This is particularly true in hard exploration tasks, where rewards can be sparse, deceptive or adver-
sarial. In such scenarios, performing random exploration can rarely lead to successful states and
thus rarely obtain meaningful feedback. Geometric discounts tend to fail in such scenarios. Con-
sider for example the U-maze environment in Figure 1a, where the reinforcement signal provides a
high reward (+1) when reaching the green dot in the bottom arm, a deceptive reward (+0.9) when
reaching the blue dot in the upper arm, and a negative reward (-1) for crossing the red corridor. If
the agent is only interested in the long term returns, then the optimal control should always lead to
the green dot. However, depending on the initial state, optimal policies in the sense of the discounted
rl problem are likely to prefer the deceptive reward due to the exponentially decaying interest in
the future (Figure 1b).
Naturally, higher discount factors are associated with optimal policies that also optimize the aver-
age returns (Blackwell, 1962), which can solve in principle the described hard exploration problem.
However, in practice, such discount values can be arbitrarily close to 1 which entails severe com-
putational instabilities. In addition, and particularly in continuous settings or when tasks span over
extremely long episodes, discount-based RL approaches are sample-inefficient and are slow at prop-
agating interesting feed-backs to early states.
In this paper, we generalize the geometric discount to derive a variety of alternative time weighting
distributions and we investigate the underlying implications of solving the associated RL problem
both theoretically and practically. Our contributions are twofold. First, we introduce a novel family
that generalizes the geometrically discounted criteria, which we call the delayed discounted criteria.
Second, we derive tractable solutions for optimal control for both stationary and non-stationary
1
Under review as a conference paper at ICLR 2022
policies using these novel criteria. Finally, we evaluate our methods on both hard exploration tabular
environments and continuous long-episodic robotics tasks where we show that:
1.	Our agents can solve the hard exploration problem in a proof of concept setup.
2.	Our methods improve sample-efficiency on continuous robotics tasks compared to Soft-
Actor-Critic.
(a)
Figure 1c showcases how non-geometrically discounted criteria impacts the profile of optimal value
function in the U-maze example.
(b)
(c)
Figure 1: (a) Hard exploration problem example, (b) optimal value function of geometrically discounted rl
with a discount factor γ = 0.99 and (c) non-geometrically discounted RL.
2 Reinforcement Learning with non geometric discount
Consider an infinite horizon Markov Decision Process (MDP) M = {S, A, P, r, γ, p0 }, where S
and A are either finite or compact subsets of respectively Rd and Rd0, (d, d0) ∈ N2, P : S × A →
∆(S) is a state transition kernel1, r : S × A → R is a continuous reward function, p0 ∈ ∆(S) an
initial state distribution and γ ∈ (0, 1) is the discount factor. A policy π is a mapping indicating, at
each time step t ∈ N, the action at to be chosen at the current state st . The goal of geometrically
discounted reinforcement learning algorithms is to optimize the discounted returns:
∞
L(π, r) := Eπ,p0	γtrt	; rt := r(st, at)
t=0
where Eπ,p0 denotes the expectation over trajectories generated in M using the policy π and ini-
tialised according to p0 .
In this section, we present our methods. First, Section 2.1 introduces our delayed discounted family
of optimality criteria. Then, Section 2.2 investigates the optimization of the linear combination of
this new family in the context of stationary policies. Finally, Section 2.3 generalizes the optimal
control to non-stationary policies.
2.1	B eyond the geometric discount: A delayed discounted criterion
We propose to investigate a particular parametric family of optimality criteria that is defined by a
sequence of discount factors. For a given delay parameter D ∈ N, we define the discount factors
γd ∈ (0, 1) for any integer d ∈ [0, D] and we consider the following loss function:
∞N
LD(π, r)	:= Eπ,p0	X ΦD(t)rt	where ΦD(t)	:= X	Y γdad	(1)
t=0	{ad∈N}dD=0	d=0
such that d ad =t
This class of optimality criteria can be seen as a generalization of the classical geometric discount. In
fact, we highlight that for D = 0, Φ0(t) = γ0t which implies that L0(π, r) = L(π, r) for any policy
π and any reward function r. In Figure 2, we report the normalized distribution of the weights ΦD (t)
(i.e. y(t) = Pφφ(t(i)) over time as We vary the parameter D ∈ {0,..., 9}. Notice how the mode of
the probability distribution is shifted towards the right as we increased the delay, thus putting more
weights on future time steps.
1 ∆(S ) denotes the set of probability measures over S
2
Under review as a conference paper at ICLR 2022
QQe S.UIU3UE∂∞ pφs=eull OU
Figure 2: The normalized coefficients ΦD (t) over time for different values of the delay parameter D
Intuitively, the proposed criterion (LD) describes the goal of an agent that discards short term gains
in favor of long term discounted returns. In the following, we consider yet a more diverse problem
formulation by using a linear combination of the delayed losses. Let Lη be the following objective
function defined as:
D
Lη (π, r) := Eπ,p0 Xη(t)rt	such that η(t) = XwdΦd(t)	(2)
t	d=0
where the depth D ∈ N and the coefficients wd ∈ R for any d ∈ [0, D] are known.
In general, the optimal control in the sense of Lη is not stationary. However, learning solutions that
admit compact representations is crucial for obvious computational reasons. In the following we
propose to learn either the optimal stationary solution, or to approximate the optimal control with a
non-stationary policy over a finite horizon followed by a stationary one.
2.2	Optimal stationary policies
In this section, we propose to learn stationary solution using a policy-iteration like algorithmic
scheme. As in the classical setting, the goal is to learn a control ∏* that maximizes the state-action
value function Qηπ :
D∞
Qn (s,a) = X WdQ% (s,a) where Qn (s, a) ：= E∏[X Φd(t)rt∣so, ao = s,a]	⑶
d=0	t=0
This is done by iteratively evaluating Qηπ and then updating the policy to maximize the learned value
function. Due to the linearity illustrated in Equation 3, the policy evaluation step is reduced to the
estimation of Qdπ . In geometrically discounted setting, the value function is the fixed point of the
Bellman optimality operator. Luckily, this property is also valid for the quantities Qdπ :
Proposition 1 For any discount parameters (γd)dD=0, the value functions QπD is the unique fixed
point of the following γD -contraction:
D-1
[TD ⑷] Ga) = Es0~P(s,a)卜Ga) + X YdQn (SiaO) +YDEs0~P(s,a)[q(S0,al[	⑷
a0一π(s0)	d=0	a0~n(s0)
'--------------{--------------}
:= rDn (S, a)
The value QnD of a policy π with respect to the delayed criterion LD is the state-action value function
of the same policy using an augmented policy dependent reward rDn w.r.t. the γD -discounted returns.
3
Under review as a conference paper at ICLR 2022
Intuitively, the instantaneous worth of an action (rDπ ) is the sum of the environments’ myopic returns
(r(s, a)) and the long term evaluations (with lower delay parameters (Qdπ)d<D).
This has the beneficial side-effect of enhancing sample efficiency as it helps the agent to rapidly
back-propagate long-term feed-backs to early states. This reduces the time needed to distinguish
good from bad behaviors, particularly in continuous settings where function approximation are typ-
ically used to learn policies. This is discussed in details in Section 4.2.
Similarly to standard value based RL algorithms, given a data set of trajectories D = {s, a, s0}, the
value function Qdπ can be approximated with parametric approximator Qθd by optimising JdQ(θ):
1D
JQ(θ) = E s,a,s0〜D b (Qθ - (r(s,a) + X YdQθd (S0,aO))2)]
a0 〜πφ(sO)	d=0
(5)
As for the policy update step, inspired from the Soft-Actor-Critic (SAC) algorithm, we propose to
optimize an entropy regularized soft Q-value using the following loss where α is a parameter:
D
Jn(φ) = -Es〜D,a〜∏φ [X WdQad (s,a) - αlog(∏φ(a∣s))]
d=0
(6)
We use Equations 5 and 6 to construct Algorithm 1, a generalization of the SAC algorithm that
approximates optimal stationary policies in the sense ofLη. In practice, this can be further improved
using the double Q-network trick and the automatic tuning of the regularization parameter α. This
is discussed in Appendix A.1. Unfortunately, unlike the geometrically discounted setting, the policy
improvement theorem is no longer guaranteed in the sense of Lη. This means that depending on the
initialization parameters, Algorithm 1 can either converge to the optimal stationary control or get
stuck in a loop of sub-optimal policies. This is discussed in detail in Section 4.1.
Algorithm 1 Generalized Soft Actor Critic
1
2
3
4
5
6
7
8
9
10
11
Input: initial parameters (θd)dD=0, φ, learning rates (λd)dD=0, λπ, pollyak parameter τ
initialise target network θd J θd and initialise replay buffer D - 0
for each iteration do
for each environment step do
at 〜∏φ(st), St+1 〜P (st, at), D-D∪ {(st, at, st+ι, ct)}
for d ∈ [0, D] do
for each Qd gradient step do
update parameter θd J θd 一 λWe& JQ(θd) and update target θd J TPd + (1 一 T)θd
for each policy update do
update policy φ J φ 一 λ∏(φ)
Return: πφ, (Qθad )dD=0
2.3	Approximate optimal control
In the general case, the optimal control is not necessarily stationary. Consider the problem of learn-
ing the optimal action profile a0∞ which yields the following optimal value function:
∞
Vni(S) := maxEa°：8 [Xn(t)rt|s0 = s],
a'"∞	Lt=0	」
(7)
where Eao^ is the expectation over trajectories generated using the designated action profile. In
this section we propose to solve this problem by approximating the value function from Equation 7.
This is achieved by applying the Bellman optimality principle in order to decompose Vn= into a finite
horizon control problem (optimising a。：H with H ∈ N) and the optimal value function VfH+1加)
4
Under review as a conference paper at ICLR 2022
where f is an operator transforming the weighting distribution as follows:
DD
[f (η)] (t) := X(Yd X Wj)φd⑴=hr ∙ w, φ⑴〉
d=0	j=d
where Γ :
γ0	γ0	γ0
0	γ1	γ1
0	0	γ2
.
.
.
000
γ0
γ1
γ2
γD
w:
w0
w1
w2
wD
「Φo(t) 1
φι(t)
Φ(t) ：= φ2⑴
.
.
.
ΦD(t)
;
For the sake of simplicity, let <1,η> := PD=0 Wd and [fn(η)](t) ：= hΓn ∙w, Φ(t)i for any n,t ∈ N.
Proposition 2 For any state s0 ∈ S, the following identity holds:
H
Vn= (so) = max Ea0:H Xh1, f t(η)irt +Ea
0,a1,..,aH Vf=H+1(η) (sH+1)	(8)
0:H	t=0
As a consequence, the optimal policy in the sense of Lη is to execute the minimising arguments
a0=:H of Equation 8 and the execute the optimal policy in the sense LfH+1(η ). Unfortunately, solving
LfH+1 (η) is not easier than the original problem.
However, under mild assumptions, for H large enough, this criterion can be approximated with a
simpler one. In order to derive this approximation, recall that the power iteration algorithm described
by the recurrence Vk+ι = ∣ΓV⅛ converges to the unit eigenvector corresponding to the largest
eigenvalue of the matrix Γ whenever that it is diagonalizable. In particular, if0 < γD < ... < γ0 < 1
and V0 = w, then Γ is diagonalizable, γ0 is it’s largest eigenvalue and the following holds:
1	1	Γn ∙ W	r	,	Vfn(n)1 2 3 4 (S) 6 7 8	…/、
lim vn+1 = lim Q——Ilr 寸 II = [1i=o]i∈[0,D] and lim Q——Ilr 寸 II = V (S)⑼
n→∞	n→∞llk≤n IIγ ∙ Vkk	n→∞llk≤n IIγ ∙ Vkk
Under these premises, the right hand term of the minimisation problem in Equation 8 can be approx-
imated with the optimal value function in the sense of the classical RL criterion L (which optimal
policy can be computed using any standard reinforcement learning algorithm in the literature).
Formally, we propose to approximate Equation 8 with a proxy optimal value function Vη=,H (S0):
H
=
Vη=H (s0) = max
a0:H
{Eao：H [Xh1,f t(η)irt] + ( Y kΓ ∙Vkk) Eao ,aι,..,aH [V *(SH +l)]}	(10)
t=0
k≤H
A direct consequence of Equation 9 is that limH→∞ Vη=,H (S) = Vη= (S) for any state S ∈ S. In
addition, for a given horizon H, the optimal decisions in the sense of the proxy problem formulation
are to execute for the first H steps the minimising arguments a0=:H of Equation 10 (they can be
computed using dynamic programming) and then execute the optimal policy in the sense of the
γ0-discounted RL (which can be computed using any standard RL algorithm).
Algorithm 2 H-close optimal control
1: Compute ∏=, V = J Policy Iteration and initialize v° J W
2: for t ∈ [1, H] do
3:	Compute Vt J Γ ∙ v1
4: Initialise VH+ι(s) J V*(s) ∙ Qk≤H ∣∣Γ ∙ v®∣∣
5: for t ∈ [H, 0] do
6:	Solve ∏t(s) J argmaxa ∣∣Vt∣ ∙ c(s,a) + Es，〜P(s,a) [Vt+ι(s0)]
7:	Compute Vt(S) J ∣∣vt∣ ∙ c(s,∏t(s)) + Es，〜P(s,∏t(s)) [Vt+ι(s0)]
8: Return: (πt)t∈[0,H], π=
5
Under review as a conference paper at ICLR 2022
3	Related Work
Bridging the gap between discounted and average rewards criteria. It is well known that defin-
ing optimality with respect to the cumulative discounted reward criterion induces a built-in bias
against policies with longer mixing times. In fact, due to the exponential decay of future returns, the
contribution of the behaviour from the Tth observation up to infinity is scaled down by a factor of
the order ofγT. In the literature, the standard approach to avoid this downfall is to define optimality
with respect to the average reward criterion L defined as:
1T
LgC) := Tiim∞ τ E∏,po [ X rt]
This setting as well as dynamic programming algorithms for finding the optimal average return
policies have been long studied in the literature (Howard, 1960; Veinott, 1966; Blackwell, 1962;
Puterman, 2014). Several value based approaches (Schwartz, 1993; Abounadi et al., 2001; Wei et al.,
2020) as well as policy based ones (Kakade, 2001; Baxter & Bartlett, 2001) have been investigated
to solve this problem. These approaches are limited in the sense that they require particular MDP
structures to enjoy theoretical guarantees.
Another line of research, is based on the existence of a critical discount factor γcrit < 1 such that for
any discount γ ∈ (γcrit, 1) the optimal policy in the sense of the γ-discounted criterion also optimises
the average returns Blackwell (1962). Unfortunately, this critical value can be arbitrarily close to 1
which induces computational instabilities in practice. For this reason, previous works attempted to
mitigate this issue by increasing the discount factor during training (Prokhorov & Wunsch, 1997),
learning higher-discount solution via learning a sequence of lower-discount value functions (Romoff
et al., 2019) or tweaking the reinforcement signal to equivalently learn the optimal policy using lower
discounts (Tessler & Mannor, 2020).
Exploration Strategies. Another line of research attempted to tackle the hard exploration problem
by further driving the agents exploration towards interesting states. Inspired by intrinsic motivation
in psychology (Oudeyer & Kaplan, 2008), some approaches train policies with rewards composed
of extrinsic and intrinsic terms. Namely, count-based exploration methods keep track of the agents’
past experience and aim at guiding them towards rarely visited states rather than common ones
(Bellemare et al., 2016; Colas et al., 2019). Alternatively, prediction-based exploration define the
intrinsic rewards with respect to the agents’ familiarity with their environments by estimating a dy-
namics prediction model (Stadie et al., 2015; Pathak et al., 2019). Other approaches maintain a
memory of interesting states (Ecoffet et al., 2019; 2021), trajectories (Guo et al., 2019) or goals
(Guo & Brunskill, 2019). Ecoffet et al. (2019; 2021) first return to interesting states using either a
deterministic simulator or a goal-conditioned policy and start exploration from there. (Guo et al.,
2019) train a trajectory-based policy to rather prefer trajectories that end with rare states. Guo &
Brunskill (2019) revisit goals that have higher uncertainty. Finally, based on the options framework
(Sutton et al., 1999), options-based exploration aims at learning policies with termination condi-
tions—or macro-actions. This allows the introduction of abstract actions, hence driving the agents’
exploration towards behaviours of interest (Gregor et al., 2016; Achiam et al., 2017).
4	Experiments
In this section, we evaluate the performance of the proposed algorithm. Our goal is to answer the
following questions:
•	How does the different parameters impact the ability of the proposed algorithms to solve
hard exploration problems?
•	How does the proposed algorithm impact the performance in classical continuous control
problems?
4.1	Hard Exploration Problems
In this section we investigate our ability to approximate the solution of the proposed family of RL
problems (w.r.t. the optimality criterion LD) in discrete hard exploration maze environments. We
6
Under review as a conference paper at ICLR 2022
fix the discount factor values to Yi = 0.99 - ι0^ throughout the experiments. This guarantees that
Γ is diagonalizable and that its largest eigenvalue is γ0 = 0.99. We evaluate the performance of both
Algorithms 1 and 2 as we vary the depth parameter and as we increase the non stationarity horizon.
In Figure 3 we reported the shapes of the mazes as well as the used reinforcement signal in each
of them. We used a sparse signal where the green dots represent the best achievable reward, the
blue dots are associated with a deceptive reward and the red dots are associated with a penalty. This
setting is akin to hard exploration problems as the agent might learn sub-optimal behaviour because
of the deceiving signal or because of the penalty.
(a) U-maze
(b) T-maze
Figure 3: Hard exploration environments
Stationary solutions: We start by evaluating the performances of the learned policies using Gen-
eralised Soft Actor Critic (GSAC) (Algorithm 1). As discussed earlier, unlike the geometrically
discounted setting, the policy update is not guaranteed to improve the performances. For this rea-
son, depending on the initialisation, the algorithm can either converge to the optimal stationary
policy, or get stuck in a sequence of sub-optimal policies.
In Figure 4, we reported the performances of GSAC as we varied the depth hyper-parameter D
using two initialisation protocol. The blue curves are associated with a randomly selected initial
parameters while the red curves are associated with experiment were the policy is initialised with
the solution of the geometrically discounted problem. The solid lines correspond to the average
reward while the dashed lines correspond to the Φd weighted loss (as computed in Ld).
In all experiments, the expectations were averaged across 25 runs of the algorithm using trajectories
of length 4000 initialised in all possible states (uniform p0 ) . A common observation is that for
a depth D higher than 6 7 the algorithm was unstable and we couldn’t learn a good stationary
policy in a reliable way. However, the learned stationary policies with even a relatively shallow
depth parameter yielded reliable policies that not only maximise the ΦD weighted rewards but also
improved the average returns. Notice how the baseline (D = 0, i.e. the geometrically discounted
case) always under-performs when compared to the learned policies for a depth parameter around 3.
We also observe that the algorithm is sensitive to the used initialisation. Using the optimal pol-
icy in the sens of the geometrically discounted objective (red curves) helped stabilise the learning
procedure in most cases: this is particularly true in the random maze environment where a random
initialisation of the policy yielded bad performances even with alow depth parameter. This heurestic
is not guaranteed to produce better performances in all cases, notice how for D = 7 in the T-maze,
a random initialisation outperformed this heuristic consistently.
Optimal Control Approximation: In this section we investigate the performances of the policies
learned using Algorithm 2 as we vary the non stationarity horizon H for three depth parameters
D ∈ {5, 10, 15} (respectively the red, blue and green curves). As in the last experiment, we reported
the expected returns as the average rewards using the continuous lines and as the ΦD weighted
rewards using dashed lines. As a baseline, we can observe in each figure the performances of
the optimal policy in the sense of the geometrically discounted criterion when the non-stationarity
7
Under review as a conference paper at ICLR 2022
Figure 4: Learned stationary policy (GSAC) performances as the depth parameter varies
horizon H = 0. We also reported the performances of the best stationary policy learned using GSAC
for a depth parameter D = 5.
The first notable observation is that increasing the hyper-parameter H does indeed improve perfor-
mances. In addition, we notice that the maximum possible improvement is reached using a finite
horizon H (i.e. the maximising argument ofboth Vη,H and Vn= are the same). Intuitively, the H non
stationary steps enable the agent to get to an intermediate state from which the optimal policy in the
sense of the discounted RL formulation can lead to the state with the highest rewards. This explains
the effectiveness of current hard exploration RL algorithmsEcoffet et al. (2019); Eysenbach et al.
(2019): by learning a policy that reaches interesting intermediate state, these methods are implicitly
learning an approximate solution of Vη,H .
In the case of D = 5, the obtained performances using Algorithm 2 converged to the performances
of the optimal stationary policy obtained using GSAC in both the random and T-maze. On the other
hand, unlike GSAC, the H-close algorithm is not sensitive to initialisation: in fact notice how even
for arbitrarily high depth parameter (D = 15) the learned policy ends up saturating the average and
the ΦD weighted rewards. More interestingly, increasing the depth can be beneficial as we observe
empirically that for higher depth parameters, the non-stationarity horizon required to achieve the
best possible performances decreases.
Figure 5: Learned H-close optimal control
4.2	Simulated Continuous Control Benchmarks
In this section, we propose to evaluate our methods on several continuous robotics domains. To this
end, we consider 5 different environments within the mujoco physics engine (Todorov et al., 2012):
Walker2d, Hopper, HalfCheetah, Ant and Humanoid. As results in the tabular settings showed that
there exists a threshold beyond which increasing the value of the delay D hurts the performance, we
fix D = 1, where only two discount factors γ0 and γ1 are used. We compare our proposed methods
to the classic SAC algorithm to investigate the implications of using both critics Qθ0 and Qθ1 .
In Figure 6, we report the average rewards obtained by the different agents over time. Note that
in all the environments, the GSAC agents are faster in collecting positive rewards than the SAC
8
Under review as a conference paper at ICLR 2022
agents. The main reason behind this is that the critic tends to discern good from bad actions in the
GSAC agents faster than the SAC agents. In fact, on the one hand, the SAC agents spend more
time uncertain about the quality of their actions for a given state, and thus need more time and more
experience to make the estimations of their critic more accurate and reflecting the true long-term
value of an action. On the other hand, in the GSAC agents, Qθ1 takes into consideration the estimate
of the actions by Qθ0, and thus there is less uncertainty about their quality.
---SAC
---GSAC
---SAC
---GSAC
---SAC ------ GSAC
Figure 6: Training curves on continuous control benchmarks. Generalized Soft-Actor-Critic (GSAC) shows
better sample efficiency across all tasks
5 Conclusion
Designing human-like autonomous agents requires (among other things) the ability to discard short
term returns in favor of long term outcomes. Unfortunately, existing formulations of the reinforce-
ment learning problem stand on the premise of either discounted or average returns: both providing
a monotonic weighting of the rewards over time.
In this work, we propose a family of delayed discounted objective functions that captures a wide
range of non-monotonic time-preference models. We analyse the new formulation to construct 1)
the Bellman optimality criterion of stationary solution and 2) a feasible iterative scheme to ap-
proximate the optimal control. The derived algorithms successfully solved tabular hard exploration
problems and out-performed the sample efficiency of SAC in various continuous control problems;
thus closing the gap between what is conceivable and what is numerically feasible.
Broader Impact S tatement
Reinforcement learning provides a framework to solve complex tasks and learn sophisticated be-
haviors in simulated environments. However, its incapacity to deal with very sparse, deceptive
and adversarial rewards, as well as its sample-inefficiency, prevent it from being widely applied in
real-world scenarios. We believe investigating the classic optimality criteria is crucial for the de-
ployment of RL. By introducing a novel family of optimality criteria that goes beyond exponentially
discounted returns, we believe that we take a step towards more applicable RL. In that spirit, we also
commit ourselves to releasing our code soon in order to allow the wider community to extend our
work in the future.
9
Under review as a conference paper at ICLR 2022
References
Jinane Abounadi, Dimitrib Bertsekas, and Vivek S Borkar. Learning algorithms for markov decision
processes with average cost. SIAM Journal on Control and Optimization, 40(3):681-698, 2001.
Joshua Achiam, Harrison Edwards, Dario Amodei, and Pieter Abbeel. Variational autoencoding
learning of options by reinforcement. In NIPS Deep Reinforcement Learning Symposium, 2017.
Jonathan Baxter and Peter L Bartlett. Infinite-horizon policy-gradient estimation. Journal of Artifi-
cial Intelligence Research, 15:319-350, 2001.
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.
Unifying count-based exploration and intrinsic motivation. Advances in neural information pro-
cessing systems, 29:1471-1479, 2016.
David Blackwell. Discrete dynamic programming. The Annals of Mathematical Statistics, pp. 719-
726, 1962.
Cedric Colas, Pierre Fournier, Mohamed Chetouani, Olivier Sigaud, and Pierre-Yves Oudeyer. CU-
rious: intrinsically motivated modular multi-goal reinforcement learning. In International con-
ference on machine learning, pp. 1331-1340. PMLR, 2019.
Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. Go-explore: a
new approach for hard-exploration problems. arXiv preprint arXiv:1901.10995, 2019.
Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. First return, then
explore. Nature, 590(7847):580-586, 2021.
Benjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine. Search on the replay buffer: Bridg-
ing planning and reinforcement learning. arXiv preprint arXiv:1906.05253, 2019.
Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-
critic methods. In International Conference on Machine Learning, pp. 1587-1596. PMLR, 2018.
Karol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. arXiv
preprint arXiv:1611.07507, 2016.
Yijie Guo, Jongwook Choi, Marcin Moczulski, Shengyu Feng, Samy Bengio, Mohammad Norouzi,
and Honglak Lee. Memory based trajectory-conditioned policies for learning from sparse re-
wards. arXiv preprint arXiv:1907.10247, 2019.
Zhaohan Daniel Guo and Emma Brunskill. Directed exploration for reinforcement learning. arXiv
preprint arXiv:1906.07805, 2019.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International confer-
ence on machine learning, pp. 1861-1870. PMLR, 2018.
Ronald A Howard. Dynamic programming and markov processes. 1960.
Sham Kakade. Optimizing average reward using discounted rewards. In International Conference
on Computational Learning Theory, pp. 605-615. Springer, 2001.
Pierre-Yves Oudeyer and Frederic Kaplan. How can we define intrinsic motivation? In the 8th
International Conference on Epigenetic Robotics: Modeling Cognitive Development in Robotic
Systems. Lund University Cognitive Studies, Lund: LUCS, Brighton, 2008.
Deepak Pathak, Dhiraj Gandhi, and Abhinav Gupta. Self-supervised exploration via disagreement.
In International conference on machine learning, pp. 5062-5071. PMLR, 2019.
Danil V Prokhorov and Donald C Wunsch. Adaptive critic designs. IEEE transactions on Neural
Networks, 8(5):997-1007, 1997.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John
Wiley & Sons, 2014.
10
Under review as a conference paper at ICLR 2022
Joshua Romoff, Peter Henderson, Ahmed Touati, Emma Brunskill, Joelle Pineau, and Yann Ollivier.
Separating value functions across time-scales. In International Conference on Machine Learning,
pp. 5468-5477. PMLR, 2019.
Anton Schwartz. A reinforcement learning method for maximizing undiscounted rewards. In Pro-
ceedings of the tenth international conference on machine learning, volume 298, pp. 298-305,
1993.
Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement
learning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015.
Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A frame-
work for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181-
211, 1999.
Chen Tessler and Shie Mannor. Reward tweaking: Maximizing the total reward while planning for
short horizons. arXiv preprint arXiv:2002.03327, 2020.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033.
IEEE, 2012.
Arthur F Veinott. On finding optimal policies in discrete dynamic programming with no discounting.
The Annals of Mathematical Statistics, 37(5):1284-1294, 1966.
Chen-Yu Wei, Mehdi Jafarnia Jahromi, Haipeng Luo, Hiteshi Sharma, and Rahul Jain. Model-
free reinforcement learning in infinite-horizon average-reward markov decision processes. In
International conference on machine learning, pp. 10170-10180. PMLR, 2020.
11