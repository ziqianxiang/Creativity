Under review as a conference paper at ICLR 2022
Depth Without the Magic: Inductive Bias of
Natural Gradient Descent
Anonymous authors
Paper under double-blind review
Abstract
In gradient descent, changing how we parametrize the model can lead to drasti-
cally different optimization trajectories, giving rise a surprising range of mean-
ingful inductive biases: identifying sparse classifiers or reconstructing low-rank
matrices without explicit regularization. This implicit regularization has been hy-
pothesised to be a contributing factor to good generalization in deep learning.
However, natural gradient descent is approximately invariant to reparameteriza-
tion, it always follows the same trajectory and finds the same optimum. The
question naturally arises: What happens if we eliminate the role of parameteri-
zation, which solution will be found, what new properties occur? We characterize
the behaviour of natural gradient flow in deep linear networks for separable clas-
sification under logistic loss and deep matrix factorization. Some of our findings
extend to nonlinear neural networks with sufficient but finite over-parametrization.
We demonstrate that there exist learning problems where natural gradient descent
fails to generalize, while gradient descent with the right architecture peforms well.
1 Introduction
There is plenty of empirical evidence that the choice
of network architecture is an important determinant
of the success of deep learning (He et al., 2015;
Vaswani et al., 2017). The empirical observations
are now supported by theoretical work into the role
that parameter-to-hypothesis mapping plays in deter-
mining inductive biases of gradient-based learning.
Unregularized gradient descent can efficiently find
low-rank solutions in matrix completion problems
(Arora et al., 2019), sparse solutions in separable
classification (Gunasekar et al., 2018) or compressed
sensing (Vaskevicius et al., 2019). Vane-Perez et al.
(2018) studied deep neural networks and found evi-
dence that the parameter-hypothesis mapping1 is bi-
ased towards simpler functions as measured by Kol-
mogorov complexity. Taken together, these observa-
tions and findings have lead the community to hy-
pothesize that
Figure 1: Illustration of parametrization-
dependence of EGD and independence of
NGD. Consider two parameter spaces (W1 ,
W2) and two optimization trajectories in
each: one EGD, one NGD. If we map these
into the hypothesis space (H) then EGD finds
different optima, but NGD finds the same.
The parameter-to-hypothesis mapping influences the inductive biases of gradient-based learning
and may play an important role in generalization.
In parallel to improving architectures, considerable research was done to improve optimization al-
gorithms for deep learning, with a focus on faster convergence and robustness to hyperparameters.
Among the most advanced optimization methods are natural gradient descent (NGD) techniques.
An intuitive motivation for NGD is that it improves convergence by implicitly lifting the problem
from parameter-space, where the loss is non-convex and poorly behaved to the Riemannian manifold
1The mapping between the parameter space and the set of hypotheses as seen on Figure 1
1
Under review as a conference paper at ICLR 2022
of hypotheses, where the loss is better behaved. From the perspective of inductive biases, the most
interesting aspect of NGD is its approximate invariance to reparametrization.
Natural gradient descent eliminates the effect of parameter-to-hypothesis mapping.
These two observations invite questions about the nature of inductive biases in NGD as well as the
role of parametrization-dependence in generalization. The first, practical, implication is as follows:
if the parameter-to-hypothesis mapping really does play an important role in generalization, then
eliminating its influence on the optimization path may be undesirable, and consequently the pursuit
of implementing exact NGD in deep architectures may be counterproductive. Secondly, studying
the behaviour of NGD in various models and tasks may give us new insights about the importance
of parametrization, and could perhaps offer a way to experimentally or theoretically test hypotheses.
In this paper we study the inductive bias of natural gradient descent in deep linear models. These
models are particularly suited for our analysis because (a) efficient algorithms exist to calculate
exact natural gradients which is otherwise computationally intractable and (b) the inductive biases
of Euclidean gradient descent (EGD) in these models have been thoroughly studied and understood.
We make the following contributions:
•	In linear classification, we show that NGF is invariant under invertible transformations of
data (Theorems 1&2) and as a consequence it cannot recover the `p large margin solutions
that EGD tends to converge to.
•	We further show that (in case of separable classification) when the number of parameters
exceeds the number of datapoints, NGF interpolates training labels in a way similar to
ordinary least squares or ridgeless regression (Theorems 3&4).
•	We demonstrate experimentally that there exist learning problems where NGD can not
reach good generalization performance, while EGD with the right architecture can succeed.
•	To perform experiments, we extended the work of Bernacchia et al. (2018) to derive effi-
cient and numerically stable algorithms for calculating exact natural gradients in diagonal
networks (Gunasekar et al., 2018) and deep matrix factorization (Arora et al., 2019).
Before stating our main theoretical and experimental results we review some relevant background
on parametrization-dependent implicit regularization and natural gradients.
2	Background
2.1	Separable Classification with Deep Linear Models
In this article we consider binary classification datasets {(xn, yn), n = 1, . . . , N} separable by a
homogeneous linear classifier with a positive margin (i. e. ∃β* s.t. ynx>β* ≥ 1 ∀n). (We use the
notation X = (xι •…XN)>). In such situation β is not unique and there may be many separating
hyperplanes which all achieve 0 training loss - it is up to the inductive biases of the learning algo-
rithm to select one. Soudry et al. (2017) studied the dynamics of unregularized Euclidean gradient
descent on logistic loss and found that the iterate β(t) converges to the well-known `2 large margin
classifier in direction, that is
lim β⑴
limt→∞出而
β*
∣β^2τ whereβ'2 = argmin∣∣β∣∣2 s.t. ynX>β ≥ 1 ∀n.
`2	β∈RD
Importantly, Gunasekar et al. (2018) later showed that this behaviour changes if the gradient descent
is performed on a different parametrization. In this paper we will focus on L-layer linear diagonal
networks (Gunasekar et al., 2018), where β = w1 w2	. . . wL, using to denote elemen-
twise product. When we adjust parameters w1 , . . . , wL through Euclidean gradient descent, β(t)
converges to the ' 2 large margin separator defined as
lim
β(t)
t→∞ WH
βdiag
lβdiag I
where e^ = argmin∣∣β∣∣ 2 s.t. ynX>β ≥ 1
β∈RD	L
∀n.
A remarkable consequence of this is that unregularized gradient descent can find sparse classifiers,
without any form of explicit regularization. In fact, this inductive bias is even more sparsity-seeking
than the typically used `1 regularization (see e. g. Koh et al., 2007; Tibshirani, 1996). Figure 2
illustrates this behaviour in a 2D example.
2
Under review as a conference paper at ICLR 2022
A: direct parametrization, EGD
C: direct parametrization, NGD
DzL = 4 diagonal network, NGD
Figure 2: Implicit regularization of EGD and NGD on logistic loss in separable classification.
EGD reaches different optima depending on parametrization: fully connected networks reach '2
large margin (A), while L-layer linear diagonal networks reach the '2-large margin solution which
favours sparsity (B), while L-layer linear diagonal networks reach the '2 -large m. NGD converges
to the same optimum irrespective of the parametrization (C, D).
2.2	Matrix Completion via Deep Matrix Factorization
The task of matrix completion involves recovering an
unknown matrix β* ∈ Rd×d from a randomly cho-
sen subset of observed entries2. The problem is clearly
underdefined: there are infinitely many matrices that
match the observed entries. It is common to make addi-
tional assumptions about β*, most commonly that that
it has low rank, under which it becomes identifiable.
One approach to matrix completion under the low-rank
assumption is based on explicit regularization (e.g. nu-
clear norm) which leads to a convex optimization prob-
lem. Another common approach is matrix factorization
using an underparametrized representation β = UV
where the sizes of U ∈ RD×R and V ∈ RR×D are re-
stricted to ensure β's rank is at most R. Learning then
proceeds by minimizing the non-convex mean-squared
reconstruction error in U, V via gradient descent.
Remarkably, Gunasekar et al. (2017) showed that
the gradient-based matrix factorization method tends
to converge to low-rank solutions even in the over-
parametrized setting, i.e. when β = W1W2 where
W1 and W2 are full square matrices, without any ex-
plicit regularization. This was later extended by Arora
et al. (2019), who studied the deep matrix product
parametrization of the form β = W1W2 …Wl. Arora
et al. (2019) ran experiments for different matrix com-
pletion tasks varying initialization, depth and number of
observations and compared them to minimum nuclear
norm solution. When the number of observed entries
is large gradient descent in deep matrix factorization
models tended to the minimum nuclear norm solution.
However, in the interesting case of fewer observed en-
tries, the behaviour was different. Gradient descent pre-
EGF, B	EGD,邸	NGF, 8	NGD, ∆β
Figure 3: Illustration of the neural tangent
kernel in EGF, EGD, NGF and NGD (left
to right) in matrix factorization models of
different depth (top to bottom). The al-
gorithms take gradient steps to minimise
the squared error on a single observation
at the middle of the matrix. Each panel
shows how entries of the full 11 × 11
matrix move from a random initial state.
When L ≥ 2, Euclidean gradient meth-
ods also move entries where there is no
observation - enabling implicit regulariza-
tion towards low-rank solutions. By con-
trast, and due to invariance, natural gradi-
ent methods move only the single entry to
match the observation.
ferred solutions with lower effective rank at the expense of higher nuclear norm. From the evolution
of the singular values of β they also concluded that the implicit regularization is towards low rank
that becomes stronger as depth grows.
2to simplify presentation we assume the matrices are square, but our arguments hold more generally.
3
Under review as a conference paper at ICLR 2022
2.3	Natural gradient descent
In the next section we briefly introduce some notation and key properties of natural gradient descent
(NGD, Amari, 1997; Pascanu & Bengio, 2013). Intuitively, one can think of NGD as a gradient
descent method, but not in the Euclidean space (with the Euclidean metric) of parameters, but instead
on the Riemannian manifold of probabilistic models the parameters define (equipped with a different
metric). More specifically, let’s say that the parameter of interest is θ, where θ defines a probabilistic
model p(y|x, θ). We assume that we wish to minimize the log loss under this model, i. e. l(θ, x, y) =
- logp(y|x, θ) and L(θ) = PnN=1 l(θ, xn, yn). Then, NGD is usually defined as
θ(t + 1) = θ(t) - nF-1(θ)VθL(θ), where	(1)
F ⑹=EX [EY |X;6 [vθ L(θ)v>L(θ)]]	⑵
is the average Fisher information matrix and n is the step size. In the above definition, Eγ∣χ/ is
taken over the distribution specified by θ, but distribution with respect to which the expectation EX
is calculated can be arbitrarily chosen. In this article we use the empirical distribution of training
data, though other choices are possible (Pascanu & Bengio, 2013). We will also consider natural
gradient flow (NGF) the continuous limit of NGD, analogously defined as
θ= -F-1(θ)Vθ L(θ).	(3)
We also note, that F(θ) is not generally invertible, and indeed it will not be in some of the cases we
will consider. Therefore, it is more correct to define NGF as any trajectory θt which satisfies
•
F (θ)θ= -VθL(θ).	(4)
The natural gradient direction is thus only unique within the eigenspace of F (θ). Of all natural
gradient directions, one common choice is to use the Moore-Penrose pseudoinverse of F:
θ= -F +(θ)Vθ L(θ).	(5)
We have seen how in EGD, different parametrization of the same problem leads to drastically dif-
ferent trajectories and optima. However, NGD with infinitesimally small learning rate (i. e. NGF)
always follows the same trajectory in model-space and this finds the same optimum, irrespective of
how it is parametrized, provided that the parametrization is smooth and locally invertible. Below we
formally state this property Amari (1997), alongside a short proof for illustration.
Statement (Invariance of NGF under reparametrization). Let w and θ be two parameter vectors
related by the mapping θ = P(w) and consider natural gradient flow in w. Assume that (1) the
Jacobian J = -dθt and (2) F(θt) are both full rank for all t. If Wt follows natural gradient flow
starting from W0 then θt = P(Wt) follows NGF, i. e. it solves θt = -F (θt)+Vθt L(X, θt).
3	Natural gradients under Logistic Loss on S eparab le Data
We have seen in Section 2.1 that when trained on separable data with the logistic loss EGD tends
to converge to large margin classifiers. To illustrate how NGD differs, we first prove an invariance
property which, as we will see, rules out large margin behaviour. We state this property separately
when N < D and when N ≥ D in the theorems that follow. We denote the number of data points
with N and the number of input features with D.
Theorem 1.	Let’s assume, that N < D, X is full rank and A is an invertible D × D matrix. Let
βt = βt (X, y) be the trajectory of NGF and β0t = βt(XA> , y) (the trajectory of NGF on data
XA>). Then Xβ = X A>β0 (with the assumption that β and β0 have equivalent initial conditions).
Proof sketch. We use the notation s = Xβ and s0 = XA>β and prove that st = s0t. The full proof
can be found in Appendix C.1.
Theorem 2.	Let βt (X, y) be the trajectory of NGF and let A be a D × D invertible transfor-
mation. If N ≥ D, X has full rank and we consider NGF on the transformed data XA>, then
A> βt (XA> , y) = βt (X, y) (with the assumption that β and β0 have equivalent initial condi-
tions).
Remark. When N ≥ D and X is full rank, the size of F(β) is D × D and its rank is D, therefore
the Fisher information matrix of β is invertible.
4
Under review as a conference paper at ICLR 2022
Proof sketch. First let’s say β0t = βt (XA>, y) and v> = β0>A. Then we prove the following:
▽e，L(ynβτ Axn) = AVv L(ynV>Xn) and F(β0) = AF (v)A>.	(6)
Hence we get:
β 0 = F (β0)-1Vβo L(β0) and V = F (V)TVv L(v).	(7)
So if v and β0 have the same initialization, then vt = β0t . Full proof can be found in Appendix C.2.
Conclusion. Let ut(X, y) denote the trajectory of Xβt, which is the linear function βt>x evaluated
at each of the datapoints xn. Then ut (XA> , y) = ut (X, y).
Proof.	ut(XA>,y)=XA>βt(XA>,y) =Xβt(X,y)=ut(X,y)
One special case of this invariance property is invariance to scaling the dimensions of input data
(when A is diagonal). Imagine we scale any dimension by a constant a, NGF counteracts it by
scaling the corresponding coordinate of β by a-1. We see now why this rules out characterising
implicit regularization of NGD as minimizing non-data-dependent norms of β. In particular, it rules
out the `p large-margin behaviour we have seen in EGD.
Remark. Let Abea D X D invertible transformation and let β* (X, y) be the '2 large margin solu-
tion, i.e. β*(X, y) = argmin ∣∣βk2 subject to yn,β~τxn ≥ 1 ∀n. Then the '2 large margin classifier
does not have the invariance property, namely there exists a dataset (X, y) and a transformation A
such that Aτ βt.(XAτ, y) = β↑(X, y). We include a proofby counterexample in Appendix D.
Having ruled out norm-based implicit regularization, it’s natural to consider other statistical methods
that exhibit invariance under invertible data transformations. One candidate is ridge-less regression
or ordinary least squares (OLS), whose parameter is given by the formula βOLS = (X τX)-1X τy.
As it turns out, the connection between NGD in linear regression and the OLS estimate run deeper
than sharing this invariance property.
Theorem 3.	If N < D and X is full rank, if parameters β t of a linear model follow natural
gradient flow under logistic loss, the logits st = Xβt follow an asymptotically linear trajectory
with direction vector y.
Remark. The Fisher information matrix w.r.t. β is F(β) = X τD(β)X, where D(β) is diagonal
with positive elements on the diagonal. We see, that rank(F) = rank(X) ≤ N, so F is singular,
thus several NGF paths are possible. When rank(X) = N, β has D - N degrees of freedom and
we did describe β on N dimensions. That’s why we consider s instead ofβ.
This Theorem follows from the more general Theorem 4 which we will state later.
Informally, when we have more parameters than datapoints, NGD discovers a solution that interpo-
lates the training labels y (encoded as -1s and +1s) perfectly just like ordinary least squares does
in this case. Furthermore, if one uses the Moore-Penrose pseudoinverse to calculate the descent
direction, i. e. Eqn. (5), then βt converges in direction to the OLS parameter.
In general cases, OLS interpolation and large-margin (LM) methods find qualitatively different so-
lutions in classification tasks. While the LM solution is typically a linear combination of a small
subset of training data (the support vectors), in OLS all datapoints are support vectors. As shown in
(Hsu et al., 2020), under some conditions this difference disappears in the highly overparametrised
regime - when D > N logN. An implication of Theorem 3 is that this phenomenon, known as sup-
port vector proliferation, occurs in NGF when D > N. Thus there is a regime where NGF and EGF
find qualitatively different classifiers, with different generalisation properties (Hsu et al., 2020).
Theorem 3 provided useful in the context of linear models but it turns out it is relatively straightfor-
ward to extend this to a result which holds for non-linear overparametrized models as well.
Theorem 4.	Let w ∈ RP, P ≥ N be the parameters of a classifier with logits s = s(X; w) ∈ RN.
If Wt follows natural gradient flow on the logistic loss with labels y and the Jacobian Jt = ∂dWt- is
of full rank, then st grows asymptotically linearly with direction vector y.
Remark. If our network is linear J = X , so Theorem 3 is a special case of Theorem 4 indeed.
Proof sketch. The main idea is that, since J is full rank, by parametrization invariance of NGD the
trajectory of s is determined by the trajectory of the corresponding β.
s = -F-1 (s)VsL(s)	(8)
5
Under review as a conference paper at ICLR 2022
100¾
50%
o
dynamics of logits sπ(t)∕t=β(t)τxπ∕t
learned coefficients β
D = 1000 classification, sparsity s = 20
90%-
80%-
EGDJ = I
70%-
60%-
positive data points
----negative data points
EGD, L = 2
10
0
200
400
600
800
index of input feature
5
time
500	1000	1500	2000	2500	0
number of observations
EGD, L= 1
■ ■ ■ NGD
-NGD (pop.)
I- EGD1L = 2
-o- EGD, L= 3
→- EGD, L = 4
NGD
1000
Figure 4: NGD and EGD in a 1000 dimensional sparse classification task, where the ground truth
classifier has 20 non-zero components. Left: Test accuracy of EGD depends on parametrizaion.
When there are there are fewer datapoints than dimensions, EGD with 2 or 3-layer diagonal
parametrization can reach up to 90% accuracy. By contrast, when averaging the Fisher infroma-
tion on training samples (dotted line) NGD performs at chance level when N < D. It performs
worse than EGD even when N ≥ D, or when using the population Fisher calculated on a much
larger set of samples (dashed line). Middle: Under NGD, when N < D, logits of the model grow
linearly, proportional to the binary label. Right: Coefficient vector β learnt by EGD in different ar-
chitectures and NGD when N = 2500: In the 2-layer diagonal network, corresponding to `1 implicit
regularisation, β becomes sparse. In the 1-layer model, the solution is substantially less sparse, but
the overall structure is learnt. NGD fails to learn the sparse structure.
Then we can calculate F(s) which turns out to be diagonal, so we have N independent differential
equations. We solve them to get the result. The details of the proof can be found in the Appendix C.
3.1	Experiments
In order to validate and illustrate our findings we have run two main simulations, with results pre-
sented in Figures 2 and 4. In both experiments we considered the direct parametrization β = w and
the diagonal parametrization β = wι。…。WL (GUnasekar et al., 2θl8) for different depth L. In
order to run these experiments we needed to implement an efficient algorithm for computing natural
gradients in these models: naively calcUlating and then inverting the Fisher information matrix is
compUtationally inefficient and nUmerically Unstable. We therefore developed an algorithm that ex-
ploits the strUctUre in the Fisher information matrix, extending the work of Bernacchia et al. (2018)
for diagonal networks. The details of oUr algorithms can be foUnd in Appendix B.2.
In Experiment 1 we illUstrated EGD and NGD in a 2D toy classification dataset. Positive and nega-
tive classes were generated sUch that they are separable by the the axis-aligned separator, bUt there
exists a non-axis-aligned separator with a higher margin. Based on the findings of GUnasekar et al.
(2018) we expected EGD to find the large margin solUtion when L is low, and the axis-aligned solU-
tion when L is sUfficiently large. The resUlts in panels a and b of FigUre 2 confirm these predictions.
FigUre 2c-d illUstrate the parametrization-independence of NGD: it converges to the same solUtion
irrespective of parametrization. The solUtion is different from both the EGD solUtions.
In Experiment 2 we focUsed on generalization performance. We generated a 1000-dimensional
dataset with standard GaUssian X, and a sparse groUnd-trUth separator whose first 20 components
were set to 1, the rest were 0. Methods with explicit or implicit regUlarization towards sparse so-
lUtions shoUld enjoy good generalization even when N < D. Confirming oUr expectations, we
observed that EGD in diagonal parametrizations (L = 2, L = 3) performed best on this task. The
deeper diagonal model (L = 4) was on par with the shallow solUtion, we expect that oUr 2 mil-
lion EGD steps were simply not long enoUgh for the implicit regUlarization to kick in (Moroshko
et al., 2020). The NGD solUtion on the other hand completely fails to generalize when N < D and
does relatively poorly even as N > D. This catastrophic performance is remedied by averaging
the Fisher information on a larger dataset - i. .e. Using the popUlation Fisher (Amari et al., 2020),
bUt even this variant of NGD fails to match the performance of EGD. The middle panel of FigUre 4
validates the predictions of Theorem 4: logits from the model converge to ty. Finally, the right-hand
panels of FigUre 4 show that NGD was Unable to identify the sparse strUctUre, which the diagonal
model infers best, and even the shallow model approximately finds.
6
Under review as a conference paper at ICLR 2022
Figure 5: Performance of unregularized EGD and NGD in rank-5 matrix completion tasks using
different architectures. Left and Middle: Using deep matrix product parametrizations with L ≥ 2
layers, EGD can reach low training error and identify low-rank solutions even when the number of
observations is small. By contrast, NGD in the same problem works similarly to EGD in the naive
parametrization and fails to generalize completely. Right: 2 (orange) and 3 (green) layer models
were initialized by collapsing randomly initialized deeper models to test the effect of initialization
separately from the effect of EGD dynamics. Initialization plays a negligible role in the inductive
bias of EGD in deep matrix factorization.
4	Matrix Completion with Natural gradient descent
As We have seen in Section 2.2, EGD in the deep matrix product parametrization β = Wi …WL
converges to low-rank solutions. However, when L = 1, i.e. when we run EGD directly on β, the
solution We find is trivial: entries of β Where We have observation Will converge to the observed
value, While other entries Won’t move. Due to parameter-invariance, NGD cannot differentiate be-
tWeen parametrizations of different depth, it is natural to expect that it Will fail the same Way as EGD
does When L = 1. Let’s look at NGD in matrix completion.
In matrix completion We minimize the squared reconstruction error, Which corresponds to the log
loss in an isotropic Gaussian observation model With β as mean. In a Gaussian model, the Fisher
Information Matrix of β becomes F (β)== I, where σ( is the observation noise. The observation
noise σ( is assumed a constant, and is inconsequential here as it cancels with the = term in the log
loss. Consequently, without loss of generality, we can consider F(β) the identity.
Statement. Let’s apply NGF for the problem of matrix completion. EGF in the direct parametriza-
tion (β = W) is equivalent to NGF under any parametrization θ for which J = ^w isfull rank.
The proof of the statement can be found in Appendix E. This implies that NGF will completely fail
to generalize, i. e. make an accurate prediction of any unobserved entry of the matrix.
Figure 3 illustrates the key property of the dynamics which allows EGD to generalize in deeper
parametrizations. Each panel shows values of the neural tangent kernel (NTK) (Jacot et al., 2018),
its equivalent object for NGF called the natural NTK (Rudner et al., 2019), or their discretized
versions. For matrix factorization the NTK K(θ) is a (D × D) × (D × D) tensor which depends
on the parameters θ where ki, j, k, l(θ) measures how much the entry βi,j moves in reaction to a
negative loss gradient w.r.t. βk,l. In these visualizations, we set D = 11, and we plot the heatmap
of ki,j,5,5 . We can see that when we parametrize β directly, the NTK is simply the identity, only
the entry β5,5 moves. However, when L = 2, EGD can now respond to the gradient signal at β5,5
by moving entries in the fifth row of W1 or in the fifth column of W2 . This, in turn, might result in
moving βi,5 or β5,i as well. This explains the cross pattern seen in Figure 3 first panel in the second
row. This non-identity NTK is what allows generalization to happen as ’information flows’ from
observations to unobserved entries of β. However, in NGF, the natural NTK remains the identity
irrespective of parametrization. This is true even in the approximately invariant NGD.
For our Matrix Factorization experiments we had to develope a scalable and numerically stable
algorithm for computing the natural gradient. We did this by extending the algorithm of Bernac-
chia et al. (2018) to matrix factorization. Exploiting the structure of the Jacobian in the deep
matrix product parametrization (β = Wi …WL)We calculate the natural gradient w.r.t. Wl as
勺WlL = LLB> + ,VβLAj, where Al = Q；=： Wi and Bl = QL= l+i Wi. We note that Ai and
7
Under review as a conference paper at ICLR 2022
Bi are matrices that are readily computed during the forward and backward pass of reverse-mode
automatic differentiation of the loss. The details of the derivation can be found in Appendix B.4.
Using this algorithm, in Figure 5 we experimentally verify that NGD finds a trivial optimum in
deep matrix product parametrizations of varying depth. We follow the experimental setup of Arora
et al. (2019) and reproduce their results for EGD. We performed an extensive grid search of hyper-
parameters and found no setting where NGD would achieve non-trivial performance.
5	Summary and Discussion
Inductive biases of gradient-based learning are driven to a large extent by the way we parametrize
our hypothesis. Natural gradient descent (NGD), on the other hand, ignores the parametrization
and implicitly optimizes over the manifold of hypothesis. This invited the question whether NGD
exhibits any of the useful implicit regularization that EGD has been shown to have. We characterized
the behaviour of NGD over logistic loss, and found that in the overparametrized regime, NGD
converges to the ordinary least squares interpolant of training labels. This is in contrast with the
large-margin-type behaviour EGD exhibits. In experiments we found that in the models we studied,
NGD fails to generalize as well as EGD with the right parametrization.
5.1	Other related work
Approximate NGD algorithms: Since exact NGD is computationally prohibitive, a great deal of
research has been devoted to developing approximate NGD algorithms for deep leaning: K-FAC
Grosse & Martens (2016); Martens & Grosse (2015) exploits the approximately Kronecker struc-
ture of the Fisher information matrix, while, while Bernacchia et al. (2018) start from exact gradient
descent in linear neural networks and then apply the formula verbatim to the non-linear case. An-
other line of work aims at improving the invariance properties of NGD algorithms bringing them
closer to ideal of NGF (Song et al., 2018; Luk & Grosse, 2018). Our motivation differs in that are
not focused on designing better NGD algorithms, instead we raise the question whether closer ap-
proximation of NGF is desirable in the first place. In order to perform experiments that validate our
findings we develop efficient exact natural gradient descent algorithms in overparametrized linear
models extending the work of Bernacchia et al. (2018).
Convergence Rates for NGD: The main reason for using NGD in deep learning is the intuitive
notion it might speed up convergence by virtue of being invariant to parametrization (Amari, 1997;
Pascanu & Bengio, 2013; Martens, 2014). This intuition is backed up by theory: Amari (1998)
proved fast convergence on a quadratic loss; Bernacchia et al. (2018) proved fast convergence for
deep linear models under quadratic loss; more recently, Zhang et al. (2021) gave a proof of fast
convergence which holds for a broad class of overparametrized networks and also extends to K-
FAC; Rudner et al. (2019) analysed NGD in the neural tangent kernel (NTK) regime. Our work
differs in that our primary interest is not whether NGD converges fast, but to better understand and
illustrate possible trade-offs between fast convergence and generalization.
Generalization of NGD: Wilson et al. (2017) were the first to propose that faster convergence
may come at the cost of diminished generalization performance in deep learning. Much like our
work, Wilson et al. (2017) provided illustrative examples where different methods reach qualitatively
different solutions. They focused on adaptive learning rate algorithms like Adam, but due to the
connections between Adam and the empirical Fisher information, one might speculate that their
findings would extend to NGD as well Zhang et al. (2019) argued against the notion that NGD
may not generalize well, and supported their argument with a generalization bound which holds for
both NGD and EGD. However, generalization bounds often fail to predict the empirically observed
performance of deep learning (Jiang et al., 2019, see e. g.). In a setting most closely resembling
our work Amari et al. (2020) studied generalisation of preconditioned GD for minimising squared
loss and found that the optimal preconditioner depends on several factors: EGD generalises better
for clean labels, but in scenarios like misspecification or when the labels are noisy, NGD may have
an advantage. Finally, Wadia et al. (2021) argued that second order information of the input data -
which some second-order optimisation methods can’t utilize well, is key to good generalisation in
some neural network architectures. This general connection is related to our Theorems 1 and 2.
5.2	Q&A
Q: How about stochastic gradients? Following Gunasekar et al. (2017; 2018); Arora et al. (2019) we
analysed only full-batch gradient descent. This allowed us to prove properties of gradient flow, i. e. in
8
Under review as a conference paper at ICLR 2022
the limit of infinitesimally small learning rates, which is not a meaningful limit in SGD. This line of
work demonstrates that useful inductive biases exist in gradient-based learning even in the absence
of gradient noise. Indeed, recent empirical evidence suggests that stochasticity may not be necessary
for good generalization in deep networks (see e. g. Geiping et al., 2021). In practice, we expect the
question of generalization to be complex, with multiple factors like stochasticity or parametrization-
dependence playing a role. We propose that analysing NGD is a useful tool in understanding this
complex interplay, as it acts as a form of ablation by eliminating parametrization-dependence.
Q: Does this mean NGD does not generalize well? Not necessarily. We show that there are cases
where it does not, but it is possible that in other situations the inductive biases of NGD are more
helpful than those of EGD + parametrization, especially when trained on large data. Intuitively,
our theorems suggest that NGD may be too efficient at minimising the training loss at the cost of
poorer generalisation. However, in our experiments we saw that averaging the Fisher information
matrix over test data may remedy this, which would be in line with the practical recommendation of
Pascanu & Bengio (2013). Empirical evidence for generalization in exact NGD is sparse due to the
computational cost. Some works report good test performance using approximate methods (Grosse
& Martens, 2016; Bernacchia et al., 2018) or small models (Pascanu & Bengio, 2013), but since the
focus in these works was on demonstrating the usefulness of new methods, it is questionable how
thorough these comparisons were. Zhang et al. (2019); Amari et al. (2020) studied generalisation
of natural gradient methods theoretically in limited settings and provided some empirical evidence
to support their claims. A systematic empirical investigation similar to (Wilson et al., 2017) may be
more informative on this question.
Q: Does initialization play a role? Changing the parametrization may influence generalisation in
at least three ways: (1) initialization, (2) training dynamics, and (3) constraining the hypothesis
space. As weights are often initialized from a parameter-wise independent distribution, these may
give rise to a non-trivial and parameter-dependent initial distribution in hypothesis-space. Valle-
Perez et al. (2018) argued that in deep networks, this manifests as a form of simplicity bias. In
our models, initialisation has a simlicity bias, too: if matrices W1 , . . . , WL are drawn from an
isotropic Gaussian, their product β will be effectively low-rank with an increasing probability as
L increases. By replacing EGD by NGD, we only eliminate the influence of parametrization on
training dynamics, but the effects of initialization remain. It is therefore important to disentangle
relative importance of initialisation (1), and parameter-dependent dynamics (2). To this end, we
designed a set of additional experiments, where we controlled the effect of initialization separately
from the effects through dynamics. We initialised deep matrix factorisation models by drawing each
component matrix W1 as a product of independent Gaussian matrices, then ran EGD. Thus, we
were able to create models behaving like a L = 6 layer model at initialization but L = 2 layer
model during training. We found that the effect of initialization on generalization performance was
negligible compared to the effects of training dynamics (Figure 5.c), at least in deep linear models.
We further note that initialization plays a very important role in the limit of infinitely wide networks,
too, where initialization scale determines whether the network behaves like a linear kernel machine,
or more like the behaviour we describe in finite networks here (Woodworth et al., 2020).
Q: What if you calculate Fisher information on test data? Pascanu & Bengio (2013) noted that in
deep learning, averaging the Fisher information over test data, rather than training data seemingly
improves performance. In our theorems and experiments we assume averaging over the training
data, sometimes referred to as the sample Fisher information (see e. g. Amari et al., 2020) as this
makes our proofs tractable. In our high-dimensional sparse classivication experiment in Figure 4
we tested the performance of NGD when the Fisher information is averaged over a large number
of samples, called the population Fisher, and we found that generalisation performance improved,
but still did not match that of EGD, especially when sparsity-inducing diagonal parametrisations are
used.
Q: What about other forms of natural gradients? In addition to the Fisher-Rao natural gradients that
we consider here, there are other forms of natural gradients, such as those based on the Wasserstein
metric (Li & Montufar, 2018; Arbel et al., 2019). When considering this broader family of natural
gradient descent, it is natural to ask if the choice of metric may give rise to different inductive biases
in NGD similarly to how different parametrizations effect EGD differently. We think this is a fertile
area for future research.
9
Under review as a conference paper at ICLR 2022
Reproducibility Statement
Python code to reproduce our results (including all Figures except Figure 1) can be found in the
following (anonymized) git repository which contains unit tests and documentation:
https://anonymous.4open.science/r/deeplinear-2F10
References
Shun-ichi Amari. Neural learning in structured parameter spaces - natural riemannian gradient. In
M. C. Mozer, M. Jordan, and T. Petsche (eds.), Advances in Neural Information Processing Sys-
tems, volume 9. MIT Press, 1997. URL https://proceedings.neurips.cc/paper/
1996/file/39e4973ba3321b80f37d9b55f63ed8b8-Paper.pdf.
Shun-ichi Amari. Natural gradient works efficiently in learning. Neural Computation, 10:251-276,
2 1998. ISSN 0899-7667. doi: 10.1162/089976698300017746. URL http://direct.mit.
edu/neco/article-pdf/10/2/251/813415/089976698300017746.pdf.
Shun-ichi Amari, Jimmy Ba, Roger Grosse, Xuechen Li, Atsushi Nitanda, Taiji Suzuki, Denny Wu,
and Ji Xu. When does preconditioning help or hurt generalization? 6 2020. URL https:
//arxiv.org/abs/2006.10732v4.
Michael Arbel, Arthur Gretton, Wuchen Li, and Guido Montufar. Kernelized wasserstein natural
gradient. 10 2019. URL https://arxiv.org/abs/1910.09652v4.
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. Advances in Neural Information Processing Systems, 32, 5 2019. URL http:
//arxiv.org/abs/1905.13655.
AIberto Bernacchia, Mate Lengyel, and Guillaume Hennequin. Exact natural gradient in deep linear
networks and its application to the nonlinear case. Advances in Neural Information Processing
Systems, 31, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/
7f018eb7b301a66658931cb8a93fd6e8-Abstract.html.
Donald W. Fausett and Charles T. Fulton. Large Least Squares Problems Involving Kronecker
Products. SIAM Journal on Matrix Analysis and Applications, 15(1), 1994. ISSN 0895-4798.
doi: 10.1137/s0895479891222106.
Jonas Geiping, Micah Goldblum, Phillip E. Pope, Michael Moeller, and Tom Goldstein. Stochas-
tic training is not necessary for generalization. 9 2021. URL https://arxiv.org/abs/
2109.14119v1.
Roger Grosse and James Martens. A kronecker-factored approximate fisher matrix for convolution
layers. 33rd International Conference on Machine Learning, ICML 2016, 2:851-874, 2 2016.
URL https://arxiv.org/abs/1602.01407v2.
Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Sre-
bro. Implicit regularization in matrix factorization. Advances in Neural Information Process-
ing Systems, 2017-December:6152-6160, 5 2017. URL https://arxiv.org/abs/1705.
09280v1.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Implicit bias of gradient descent
on linear convolutional networks. Advances in Neural Information Processing Systems, 2018-
December:9461-9471, 6 2018. URL http://arxiv.org/abs/1806.00468.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern
Recognition, 2016-December:770-778, 12 2015. URL https://arxiv.org/abs/1512.
03385v1.
Daniel Hsu, Vidya Muthukumar, and Ji Xu. On the proliferation of support vectors in high dimen-
sions. 9 2020. URL https://arxiv.org/abs/2009.10670v1.
10
Under review as a conference paper at ICLR 2022
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. Advances in Neural Information Processing Systems, 2018-
December:8571-8580, 6 2018. URL https://arxiv.org/abs/1806.07572v4.
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantas-
tic generalization measures and where to find them. In International Conference on Learning
Representations, 2019.
Kwangmoo Koh, Seung-Jean Kim, Stephen Boyd, and Yi Lin. An interior-point method for large-
scale 1-regularized logistic regression. Journal of Machine Learning Research, 8:1519-1555,
2007.
Wuchen Li and Guido Montufar. Natural gradient via optimal transport. Information Geometry, 1:
181-214, 3 2018. URL https://arxiv.org/abs/1803.07033v5.
Kevin Luk and Roger Grosse. A coordinate-free construction of scalable natural gradient. 2018.
URL https://arxiv.org/abs/1808.10340v1.
James Martens. New insights and perspectives on the natural gradient method. Journal of Machine
Learning Research, 21:1-76, 12 2014. URL https://arxiv.org/abs/1412.1193v11.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. 32nd International Conference on Machine Learning, ICML 2015, 3:2398-2407, 3
2015. URL https://arxiv.org/abs/1503.05671v7.
Edward Moroshko, Suriya Gunasekar, Blake Woodworth, Jason D. Lee, Nathan Srebro, and
Daniel Soudry. Implicit bias in deep linear classification: Initialization scale vs training ac-
curacy. Advances in Neural Information Processing Systems, 2020-December, 7 2020. URL
https://arxiv.org/abs/2007.06738v1.
Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. 1st International
Conference on Learning Representations, ICLR 2013 - Workshop Track Proceedings, 1 2013.
URL https://arxiv.org/abs/1301.3584v7.
Tim GJ Rudner, Florian Wenzel, Yee Whye Teh, and Yarin Gal. The natural neural tangent kernel:
Neural network training dynamics under natural gradient descent. In 4th workshop on Bayesian
Deep Learning (NeurIPS 2019), 2019.
Yang Song, Jiaming Song, and Stefano Ermon. Accelerating natural gradient with higher-order
invariance. 35th International Conference on Machine Learning, ICML 2018, 11:7491-7514, 3
2018. URL https://arxiv.org/abs/1803.01273v2.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The
implicit bias of gradient descent on separable data. 6th International Conference on Learn-
ing Representations, ICLR 2018 - Conference Track Proceedings, 19:1-57, 10 2017. URL
https://arxiv.org/abs/1710.10345v4.
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Sta-
tistical Society: Series B (Methodological), 58:267-288, 1 1996. ISSN 2517-6161. doi:
10.1111/J.2517-6161.1996.TB02080.X. URL https://onlinelibrary.wiley.com/
doi/full/10.1111/j.2517-6161.1996.tb02080.x.
Guillermo Valle-Perez, Chico Q. Camargo, and Ard A. Louis. Deep learning generalizes because
the parameter-function map is biased towards simple functions. 7th International Conference
on Learning Representations, ICLR 2019, 5 2018. URL https://arxiv.org/abs/1805.
08522v5.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Informa-
tion Processing Systems, 2017-December:5999-6009, 6 2017. URL https://arxiv.org/
abs/1706.03762v5.
11
Under review as a conference paper at ICLR 2022
Tomas Vaskevicius, Varun Kanade, and Patrick Rebeschini. Implicit regularization for optimal
sparse recovery. Advances in Neural Information Processing Systems, 32, 9 2019. URL
https://arxiv.org/abs/1909.05122v1.
Neha S Wadia, Daniel Duckworth, Samuel S Schoenholz, Ethan Dyer, and Jascha Sohl-Dickstein.
Whitening and second order optimization both make information in the dataset unusable during
training, and can reduce or prevent generalization. 2021.
Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht. The
marginal value of adaptive gradient methods in machine learning. Advances in Neural Informa-
tion Processing Systems, 2017-December:4149-4159, 5 2017. URL https://arxiv.org/
abs/1705.08292v2.
Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Daniel
Soudry, Nathan Srebro, Jacob Abernethy, and Shivani Agarwal. Kernel and rich regimes
in overparametrized models. volume 125, pp. 3635-3673. PMLR, 7 2020. URL https:
//proceedings.mlr.press/v125/woodworth20a.html.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning (still) requires rethinking generalization. Communications of the ACM, 64:107-
115, 3 2021. ISSN 15577317. doi: 10.1145/3446776. URL https://dl.acm.org/doi/
abs/10.1145/3446776.
Guodong Zhang, James Martens, and Roger Grosse. Fast convergence of natural gradient descent
for overparameterized neural networks. Advances in Neural Information Processing Systems, 32,
5 2019. ISSN 10495258. URL https://arxiv.org/abs/1905.10961v2.
A Useful lemmas
We will need the following lemma in the proof of Theorem 1,4.
Lemma 1. If we solve a separable classification problem with natrual gradient flow with separator
β and output s, then the gradient and the Fisher information matrix are the following (in case of
linear network this means s = Xβ):
[VsL(s)]i = -yi(1 - Φ(yiSi))	(9)
[F (s)]i,j = δi,jφ(si)(1 - φ(si)	(10)
-u
Proof. First note that φ(u)=[+；-U and φ(-u) = 1 一 φ(u)=舟〜
∂L	∂ N	y e-yisi
"(s)]i = ∂Li = ∂si X log(1 + e-ynsn ) = -‰7 = -yi(1 - φ(yiSi))
Using Equation (11) we get the following:
[F(s)]i,j = [Ey[VsL(s)Vs>L(s)]]i,j = [Ey[yi(1 - φ(yisi))yj(1 - φ(yjsj))]]i,j
Ey[(1 - φ(yisi))2]	if	i	=	j
Eyi[yi(1	- φ(yisi))]Eyj [yj(1 - φ(yj sj))]	if	i	6=	j
φ(si)(1 - φ(si))	if	i =	j
= Eyi[yi(1	-φ(yi))]Eyj[yj(1 -φ(yj))]	if	i 6=j
Now we get the following:
Eyi[yi(1 - φ(yisi))] = φ(si)(1 - φ(si)) - (1 - φ(si))(1 - φ(-si)) = 0
Hence we get:
[F (s)]i,j = δi,jφ(si)(1 - φ(si)).
(11)
(12)
(13)
(14)
□
12
Under review as a conference paper at ICLR 2022
The next lemma is essential in all computation connected to matrix completion with matrix factor-
ization.
Lemma 2. If we assume that the product matrix β comes from a Gaussian distribution with fixed
σnI standard deviation and μ mean, then the FiSher information matrix of the product matrix in
matrix factorization is F (β)= 生 I ∙
Proof. Because of the assumption:
p(X∣θ)= N (X ∣μ,σnl),	(15)
where θ is the parameters of the model (μ, σQ.
Vθ logp(X∣θ) = Vθ log( —√= e-(X2σμ)2) = V“(log(-ɪ=) - (X -" ) = X2μ),
σn 2π	σn 2π	2σn2	σn2
(16)
therefore we can compute the Fisher as
F (β) = EX 〜p(X∣θ) [Vθ logp(X∣θ)V> logp(X∣θ)] =EX 〜p(X∣θ)
(X - M)(X - μ)>^∣ _ σn I _ 1 τ
I
σn	」σn	σn
(17)
□
B Exact natural gradients in linear models
B.1 Simple linear Model logistic loss
To obtain the natural gradient Vβ L with respect to β, we have to solve the following linear system:
一,-、〜
F(β)V β L = Ve L,	(18)
where F(β) is the Fisher information matrix andVβL is the (Euclidean) gradient. Under the logistic
loss the Fisher information matrix becomes
F(β) = X> diag[φ(Xβ) φ(-Xβ)]X,	(19)
where φ is the logistic sigmoid which is applied elementwise to vector arguments and denotes
elementwise product. The gradient of the logistic loss is as follows:
VβL = -(y	X)>φ(-(y X)β)	(20)
Mathematically, we could use these expressions and solve the linear system Equation (18), however,
this would be potentially numerically unstable for reasons outlined below. Let’s introduce the nota-
tion X = y X and u = Xβ to simplify the formulæ. Due to symmetry, in the Fisher information
all occurrences of X can be replaced by X. This gives rise to the following expressions for the
Fisher information matrix:
F(β) = XT diag[φ(u) Θ Φ(-u)]X	(21)
and the gradient:
Ve L = -X φ(-u).	(22)
As the classifier gets better, components of u increase and diverges to +∞. As a consequence both
F(β) and VeL are expected to become small, from the term φ(-u). This could lead to issues with
numerical stability. To solve this, we rewrite both using following identity:
1	e-u
φ(-u) = f+^u = 1 + e-u =e φ(u)	(23)
obtaining:
F(β) = e-umaxXT diag[e-u+umaχφ2 (u)]X	(24)
Ve L = -e-umax X e-u+umax。(“)，	(25)
where umax is the largest entry of u. We have thus isolated the term responsible for poor numerical
performance into a multiplicative term e-umax which we can simply leave out when solving the lin-
ear system. The remaining terms are well-behaved even as u increases, provided that the difference
between elements of u is not too large.
13
Under review as a conference paper at ICLR 2022
B.2	Diagonal Linear Network under Logistic Loss
In a diagonal linear network We express β = wι Θ∙∙∙Θ wl. Here We will discuss how We compute
the natural gradient with respect to wl .
We now solve the following (underdetermined) system of linear equations, which we write using
using Einstein summation notation:
F (β)i,jJj,ι,N wι,k L = Nei L,	(26)
∂β
where J l,k = ∂Wj is the Jacobian of the mapping from W to β. In this specific parametrization,
most entries of J is non-zero. Let’s denote the product of the first l - 1 weight vectors as al and the
product of the last L - l - 1 weight vectors as bl so we can have:
βi = Wι,i •…wl-i,i wl,i wl+ι,i •…WL,i = al,iWl,ibl,i.	(27)
'------{z-----}	'------V-------}
al,i	bl,i
Thus, the Jacobian becomes:
al,ibl,i	if i = j
Ji,l,k =	0 ifi 6= j	(28)
Substituting this back, we have to solve the following system of equations:
F (β)i,j Jj,l,k Nwl,k L = NβiL	(29)
F (β)i,j al,j bl,j N WljL = ▽”.	(30)
(31)
To ensure numerical stability, we use the same trick as in B.2.
Since the above system of equations is underdetermined, we could choose different solutions. In our
experiments we used the pytorch.linalg.lstsq least squares solver which finds the solution
with the lowest `2 norm.
B.3	Separable classification
First note that in our model (∀n ∈ {1,2, .…N}, φ(s) = ɪ+I-S)
p(yn = 1|xn, B) = J + 巳-勾八乂>β = φ(-ynxnβ)
p(yn = -1∣xn, β) = 1 - ^^^l	χ>β = 1 - φ(-ynxnβ)
1 + e-ynxn β
So the loss function is (∀n ∈ {1, 2,•…N})
'(ynX>β) = log(1 + e-ynx>β)
and
N
L(β) = X log(1 + e-ynxn>β)
n=1
Until now this did not depend on the parametrization. Now look at the parametrizations we used in
our article.
If we use a fully connected network the gradient is the following:
(32)
(33)
(34)
N	N	-y x>
VeL(β) = X Ve log(1 + e-ynxnβ) = X ：； , χ>β
1 + e-ynxn β
n=1	n=1 e
N
X
n=1
-ynxn
1 + eynx>β
N
-ynxn (1 - φ(ynxn>β)).
n=1
(35)
14
Under review as a conference paper at ICLR 2022
The Fisher information matrix is the following:
F (β) = EX [Ey∣x [Vβ '(-ynX>β)V>'(-ynX>β)]]
1N
=N EEYX [xnx> (I- φ(ynx>β))2] =
n=1
1N
N x
n=1
1N
N x
n=1
1N
N x
n=1
xnxn>(φ(xn>β)(1 - φ(xn>β))2 + (1 - φ(xn>β))(1 - φ(-xn>β))2)
xnxn> (φ(xn>β)(1 - φ(xn>β))2 + (1 - φ(xn>β))φ2(xn>β))
xnxn>φ(xn>β)(1 - φ(xn>β))
If We use a diagonal network β = wι Θ w2 Θ ∙∙∙ Θ wl-i Θ wl, where
W = (w> w>	.… w>)τ. The gradient is the following:
VβL = J>VwL
where J (the Jacobian) is the following
J =(空...巫 ∖
y ∂wι	∂wlJ .
where
"盥 #ij =篙=δijk=Y=Mi
The Fisher information matrix is the following:
F(w) = JτF(β)J
(36)
(37)
(38)
(39)
(40)
B.4 Matrix factorization
Before we compute the natural gradient of matrix factorization let us introduce some notations:
β = W1W2 . . . WL, as before and
θ = vec(β),	(41)
w = vec(W1, W2, . . . ,WL),	(42)
where vec vectorizes the matrices to obtain a column vector. θ is a reparametrization of w, so
θ = P(w) and let J = ∂θ. With this notation, let's compute the natural gradient with respect to the
parametrization w.
V WL = F(W)TVwL = (JTF(θ)) J )-1 (JτVθ L) = JTF (θ)-1Vθ L	(43)
We use the assumption that J is full rank and because of F(θ) = I is invertible (J τF (θ))J)-1 =
J-1F (θ)-1J-τ. Thus, the natural gradient simplifies to
V WL = J-1 Vθ L	(44)
and multiplying by J we obtain
—_~
J V w L = Vθ L.	(45)
We can consider the Jacobian like L consecutive matrices
J= [J1J2...JL]	(46)
where Ji = ∂ve∂θw∙), and note that VθL = Vec(VeL) and VWL = Vec(VW1,W2,…WL L). Rewrite
equation 45:
τ ∕τ⅛	r∙∖	∕v^7 r∙ ∖	/dr、
J Vec(VW1,W2,...WL L) = Vec(VβL).	(47)
15
Under review as a conference paper at ICLR 2022
If We solve the following equation for i = 1,...L, then the concatenation of vectors vec(VwiL)
will solve equation 47 as well.
1
Jivec(VWiL) = Lvec(Ve L)
(48)
Let Ai = W1W2... Wi-ι and Bi = Wi+1 Wi+2... WL and using 0 notation for the Kronecker
product and utilize the property vec(ABC) = (C> 0 A)vec(X) we get
∂vec(β) = ∂vec(AiWiBi) = ∂(B> 0 Ai)vec(Wi)=6> _ A
∂vec(Wi)	∂vec(Wi)	∂vec(Wi)	i i
(49)
thus we need to solve
1
(B> 0 Ai)vec(VWiL) = Lvec(VeL)	(50)
for vec(VWi L). One can do this by exploiting properties of the Kronecker product and using Moore-
Penrose pseudo-inverses as follows:
vec(VWiL) = 1(B>+ 0 A+)vec(VβL) = 1(B>+VeLA÷)	(51)
LL
We note that when Ai and Bi are near full-rank, using the pseudoinverses may not be numerically
stable. Fausett & Fulton (1994) instead proposed a solution based on QR decomposition, and even
discussed an approach which extends to the rank deficient case. In practice we found that this
was not necessary for ours experiments. As a result, in our implementation we use the formula
L B> +Ve LA+ to update the factor matrices with the natural gradient.
C Proof of theorems
C.1 Proof of Theorem 1
Statement. Let’s assume, that N < D, X is full rank and A is an invertible D × D matrix. Let
βt = βt(X, y) be the trajectory of NGF and β0t = βt(XA>, y) (the trajectory of NGF on data
XA>). ThenXβ = XATβ0.
Proof. Let s = Xβ and s0 = XATβ0. The gradient and the Fisher information matrix are the
following (the calculation can be found in Lemma 1).
[VsL(s)]i = -yi(1 - φ(yisi))
[F(s)]i,j = δi,jφ(si)(1 - φ(si))
(52)
The exact same can be said about s0, so s and s0 are the solutions of the same differential equations,
so if we use the same initialization St = s].	□
C.2 Proof of Theorem 2
Statement. Let βt (X, y) be the trajectory of NGF and let A be a D × D invertible transforma-
tion. If N ≥ D, X has full rank and we consider NGF on the transformed data XA>, then
A>βt(XA>, y) = βt(X, y).
Proof. Let β0 be the trajectory of NGF on the transformed data:
β0t = βt(XA>, y).	(53)
To run NGF on β0 we need its Fisher information matrix. Note that the Fisher information matrix
of linear models with logistic-loss is
F(β) = X>diag[φ(Xβ)	φ(-Xβ)]X.	(54)
by Appendix B.1. Note that in this case the rank of the Fisher information matrix is D, so it is
invertible. Same is true for β0. Let’s compute the Fisher information matrix of β0.
1N
F (β0 )= N E EynIAxn [Vβ0 '(ynβ0> Axn) V>o'(ynβ0> Axn)]	(55)
n=1
16
Under review as a conference paper at ICLR 2022
First, specify Vq，'(ynβ0>Axn) and use the notation v> = β0>A.
▽e，'(ynβ0> AXn) = J>Vv> '(yn V>Xn)
(56)
where J = ∂>.
Ji,j
∂vi	∂ Pkd=1 β0kAk,i
d βj
dβj
(57)
Therefore J = A> ⇔ J> = A and
Vqo '(ynβ0>AXn) = AVv '(yn V>Xn).
We now can continue the computation of the Fisher:
1N
F(β ) = N EEyn∣Axn[AVv>'(ynV>Xn)V>>'(ynv>Xn)A>]=
1N
=A(N EEyn|xn[Vv>'(ynV>Xn)V>>'(ynv>Xn)])A> = AF(v)A>.
n=1
(58)
(59)
Note, that the Fisher of v must be invertible as well from the previous Equation. Let’s see the NGF
on β0:
N
β 0 = -F (β0)-1Vβo L(β0>XAT, y) = -(AF (v)A>)-1 X V '(ynβ0>Axn)
n=1
N
=-(A>)-1F (V)TAT A X Vv '(ynV>Xn) = -(A>)-1F (v)-1Vv L(V)
n=1
We also have the following (by the Chain Rule):
V = J β 0 = A>β 0
(60)
(61)
Now from Equation (60) and (61) we get:
V = F (v)-1Vv L(V)
(62)
This is the same differential equation as the one β is a solution of. So if they are initialized the same
way Vt = βt(X, y), so βt = Vt = A>β0.	□
C.3 Proof of Theorem 4
Statement. If N < D and β is the separator. Let s be the output. Let the Jacobi matrix from β to
S be J = ∂ds. If J isfull rank S is asymptotically linear with direction vector y.
Proof. First let’s note that by the invariance property of NGF the trajectory of s is defined by the
trajectory of β.
S= -F T (S)VsL(S)	(63)
Let’s assume S is 1-dimensional. In this case S = s, X1 = X and y = y can be used since we have
only one data point. To solve equation (63) we need the gradient and the Fisher information matrix
which are the following (the calculation can be found in the Appendix B.2)
VsL(s) = -y(1 - φ(ys))
The Fisher information matrix:
F(s) = φ(s)(1 - φ(s))
Then Equation (63) can be written as:
y(1 - φ(ys))
ς ________________
=Φ(s)(1- Φ(s))
(64)
(65)
(66)
17
Under review as a conference paper at ICLR 2022
Now We rescale our data points s.t. X = -x, Sog= -S and y = -y = 1. Hence We get:
∂s _ 1
——=—:~~~
∂t φ(s)
Which can be solved and the solution is
(67)
log(1 + es) = t + C ^⇒ I = log(et+c - 1)
By equation (68) We get the asymptotic behaviour
lim -S—
t→∞ t + c
lim log©+。- 1
t→∞	t + c
lim
t→∞ e
et+c
t + c - 1
lim
t→∞
1
1 - e-(t+C)
(68)
(69)
(70)
1
(From (69) to (70) We use L’Hopital Rule).
Hence We proved Theorem 4. for N = 1. NoW let’s assume, that N > 1. NoW We Write doWn the
gradient again:
IVsL(S)]i = -yi(1 - MyiSi))	(71)
And the Fisher information matrix:
IF(s)]i,j = δi,jφ(Si)(1 - φ(Si))
(72)
So noW if We substitute in Equation (71) and Equation (72) to Equation (63). We can rescale, so
ygi = 1 ∀i as We did in the previous case. Hence We get the folloWing:
∂gs
∂t
/	1	0
φ(gι)(1-φ(gι))	0
0	 1-
0	φ(i2)(1-φ(s2))
..
..
..
00
0
0
.
.
.
________1________
φ(sN )(1-φ(sN ))/
/-(1 - φ(s1)) ∖	( φ(sι) '
-(1 - φ(s2 ))	_	双豆)
.	=.
.
.
..
\-(1-卷 N W	Qb)
(73)
Hence We got N independent differential equations Which are exactly the same as in the N = 1
case. So in each dimension gs is asymptotically t + c for some c. Hence gs ≈ t1 + c, Where c ∈ RD
is a constant. So S ≈ ty + Cs, where Cs ∈ RD is a constant.	□
D COUNTEREXAMPLE FOR THE INVARIANCE OF `2 LARGE MARGIN
SOLUTION
The counterexample is the following:
A= -11 20 ,y= 1andX= (2 -3)
Then β*(X, y) = argmin∣∣βk2 subject to 2βι - 3β2 ≥ 1, therefore β*(X, y)
(-03)
. Further-
more β*(XA>,y) = argmin∣∣βk2 subject to -4βι - 2β? ≥ 1, therefore β*(XA>,y)
but A>β*(XA>,y)= (-4) = (-J = β*(X,y).
E Proof of the s tatement ab out the parametrization invariance
OF NGF
Statement. Let w and θ be two parameter vectors related by the mapping θ = P(w) and consider
natural gradient flow in W. Assume that (1) the Jacobian J = ∂θ^ and (2) F (θt) are both full rank
for all t. Ifwt follows natural gradient flow starting from w0 then θt = P(wt) follows NGF, i. e. it
solves θt = -F(θt)+VθtL(X, θt).
18
Under review as a conference paper at ICLR 2022
Proof. We use that F(w) = J>F (θ)J which follows from the definition of F:
F (W) = EX [Vw L(X, W)Vw L(X, w)] = EX [J >Vθ L(X,θ)V> L(X,θ)J ] = J>F (θ) J
The invariance statement follows:
θ = (P (W t)) = J w t = -JF (Wt)+Vwt L(X, Wt)=
=-JJ+F(θt)+(J>)+J>VθtL(X,θt)=-F(θt)+VθtL(X,θt).
F	Proof of the statement about NGD in matrix completion
Statement. Let’s apply NGF for the problem of matrix completion. EGF in the direct parametriza-
tion (β = W) is equivalent to NGF under any parametrization θ for which J = d∂wwt- isfull rank.
Proof. First let’s consider a parametrization θ s.t. the direct parametrization β = W (= P(θ)) and
J = d∂w is full rank. Then by the invariance property if θt is the solution of the NGF with the
arbitrary parametrization, then Wt = P(θt) is the solution of:
W = -VwL(W)
Which agrees with the EGF with direct parametrization.	□
G Invariance property of OLS
We show the same transformation invariance property for OLS that we showed in Theorem 1,2 for
NGF. Again, we split the problem into two cases: N < D and N ≥ D. Note that for the problem
Xβ = y the Ordinary least squares solution is β = (X>X)-1X>y if the columns ofX is linearly
independent.
Statement. Let N < D, A is an invertible D × D matrix. If β is the solution of the Ordenary least
squares problem for the matrix X and β0 for XA>, then Xβ = XA> β0.
Proof. Immediately follows from the definition of the problems: Xβ = y and XA>β0 = y.	□
Statement. Let N ≥ D, X has full rank andA is an invertible D × D matrix. Ifβ is the solution of
the OLS problem for the matrix X and β0 for XA>, then A>β0 = β.
Proof.
A>β0 = A>((XA>)>XA>)-1(XA>)>y = A>A>-1(X>X)-1A-1AX>y = β
□
H Supplementary Figures
19
Under review as a conference paper at ICLR 2022
AUEjnHE 15φ4
Figure 6: During peer review, reviewers requested a lower dimensional variant of the experiment
reported in Figure 4. Instead of 1000 dimensions, in this experiment we used D = 50, and instead
of S = 20 non-zero components, the real β had S = 5 non-zero entries. The experimental setup and
hyperparameters were otherwise not changed from Figure 4. The 5-layer diagonal network performs
poorly, which is likely a result of sensitivity to hyperparameters, we expect that with additional fine-
tuning of the hyperparameters for this experiment, L = 4 would do at least as well as the shallow
L = 1 model.
20