Under review as a conference paper at ICLR 2022
Increase and Conquer: Training Graph
Neural Networks on Growing Graphs
Anonymous authors
Paper under double-blind review
Ab stract
Graph neural networks (GNNs) use graph convolutions to exploit network invari-
ances and learn meaningful features from network data. However, on large-scale
graphs convolutions incur in high computational cost, leading to scalability lim-
itations. Leveraging the graphon — the limit object of a graph — in this paper
we consider the problem of learning a graphon neural network (WNN) — the
limit object of a GNN — by training GNNs on graphs sampled Bernoulli from the
graphon. Under smoothness conditions, we show that: (i) the expected distance
between the learning steps on the GNN and on the WNN decreases asymptoti-
cally with the size of the graph, and (ii) when training on a sequence of growing
graphs, gradient descent follows the learning direction of the WNN. Inspired by
these results, we propose a novel algorithm to learn GNNs on large-scale graphs
that, starting from a moderate number of nodes, successively increases the size
of the graph during training. This algorithm is benchmarked on both a recom-
mendation system and a decentralized control problem where it is shown to retain
comparable performance to its large-scale counterpart at a reduced computational
cost.
1	Introduction
Graph Neural Networks (GNNs) are deep convolutional architectures formed by a succession of
layers where each layer composes a graph convolution and a pointwise nonlinearity (Wu et al., 2021;
Zhou et al., 2020). Tailored to network data, GNNs have been used in a variety of applications such
as recommendation systems (Fan et al., 2019; Tan et al., 2020; Ying et al., 2018; Schlichtkrull et al.,
2018; Ruiz et al., 2019a) and Markov chains (Qu et al., 2019; Ruiz et al., 2019b; Li et al., 2015),
and fields such as biology (Fout et al., 2017; Duvenaud et al., 2015; Gilmer et al., 2017; Chen et al.,
2020) and robotics (Qi et al., 2018; Gama & Sojoudi, 2021; Li et al., 2019). Their success in these
fields and applications provides ample empirical evidence of the ability of GNNs to generalize to
unseen data. More recently, their successful performance has also been justified by theoretical works
showing that GNNs are invariant to relabelings (Chen et al., 2019; Keriven & Peyre, 2019), stable
to graph perturbations (Gama et al., 2020) and transferable across graphs (Ruiz et al., 2020a).
One of the most important features of a GNN is that, because the linear operation is a graph convo-
lution, its number of parameters does not depend on the number of nodes of the graph. In theory,
this means that GNNs can be trained on graphs of any size. In practice, however, if the graph has
large number of nodes training the GNN is costly because computing graph convolutions involves
large matrix operations. While this issue could be mitigated by transferability — training the GNN
on a smaller graph to execute on the large graph —, this approach does not give any guarantees
on the distance between the optimal solutions on the small and on the large graph. In other words,
when executing the GNN on the target graph we do not know if its error will be dominated by the
transferability error or by the generalization error from training.
In this paper, we address the computational burden of training a GNN on a large graph by progres-
sively increasing the size of the network. We consider the limit problem of learning an “optimal”
neural network for a graphon, which is both a graph limit and a random graph model (Lovasz,
2012). We postulate that, because sequences of graphs sampled from the graphon converge to it, the
so-called graphon neural network (Ruiz et al., 2020a) can be learned by sampling graphs of growing
size and training a GNN on these graphs (Algorithm 1). We prove that this is true in two steps. In
1
Under review as a conference paper at ICLR 2022
Theorem 1, we bound the expected distance between the gradient descent steps on the GNN and on
the graphon neural network by a term that decreases asymptotically with the size of the graph. A
consequence of this bias bound is that it allows us to quantify the trade-off between a more accurate
gradient and one that could be obtained with less computational power. We then use this theorem to
prove our main result in Theorem 2, which is stated in simplified form below.
Theorem (Graphon neural network learning, informal) Let W be a graphon and let {Gn} be a
sequence of growing graphs sampled from W. Consider the graphon neural network Φ(W) and
assume that it is learned by training the GNN Φ(Gn) with loss function '(yn, Φ(Gn)) on the
sequence {Gn}. Over a finite number of training steps, we obtain
∣∣V'(Y, Φ(W)k ≤ C with probability 1.
The most important implication of this result is that the learning iterates computed in the sequence of
growing graphs follow the direction of the graphon gradient up to a small ball, which provides the-
oretical validation to our cost efficient training methodology. We also validate our algorithm in two
numerical experiments. In the first, we learn a GNN-based recommendation system on increasingly
large subnetworks of a movie similarity graph, and compare it with the recommendation system
trained on the full graph. In the second, we consider the problem of flocking and train GNNs to
learn the actions agents need to take to flock. We compare the results obtained when progressively
increasing the number of agents during training and when training directly on the target graph.
2	Related Work
GNNs are data processing architectures that follow from the seminal works in the areas of deep
learning applied to graph theory (Bruna et al., 2013; Defferrard et al., 2016; Gori et al., 2005; Lu &
Getoor, 2003). They have been successfully used in a wide variety of statistical learning problems
(Kipf & Welling, 2016; Scarselli et al., 2018), where their good performance is generally attributed
to the fact that they exploit invariances present in network data (Maron et al., 2019; Gama et al.,
2018; Chami et al., 2021).
More recently, a number of works show that GNNs can be transferred across graphs of different
sizes (Ruiz et al., 2020a; Levie et al., 2019; Keriven et al., 2020). Specifically, (Ruiz et al., 2020a)
leverages graphons to define families of graphs within which GNNs can be transferred with an
error bound that decreases asymptotically with the size of the graph. The papers by Levie et al.
(2019) and Keriven et al. (2020) offer similar results by considering the graph limit to be a generic
topological space and a random graph model respectively. In this paper, we use an extension of the
transferability bound derived in Ruiz et al. (2020a) to propose a novel learning algorithm for GNNs.
3	Preliminary Definitions
3.1	Graph Neural Networks
Graph neural networks exploit graph symmetries to extract meaningful information from network
data (Ruiz et al., 2020c; Gama et al., 2020). Graphs are represented as triplets Gn = (V, E, W),
where V, |V | = n, is the set of nodes, E ⊆ V × V is the set of edges and W : E → R is a map
assigning weights to each edge. The graph Gn can also be represented by the graph shift operator
(GSO) S ∈ Rn×n, a square matrix that respects the sparsity of the graph. Examples of GSOs include
the adjacency matrix A, the graph Laplacian L = diag(A1) - A and their normalized counterparts
Gama et al. (2018). In this paper we consider the graph Gn to be undirected and fix S = A/n.
Graph data is represented in the form of graph signals. A graph signal x = [x1, . . . , xn]T ∈ Rn
is a vector whose i-th component corresponds to the information present at the i-th node of graph
Gn . A basic data aggregation operation can be defined by applying the GSO S to graph signals x.
The resulting signal z = Sx is such that the data at node i is a weighted average of the information
in the 1-hop neighborhood of i, zi = Pj∈N [S]ij xj where Ni = {j | [S]ij 6= 0}. Information
coming from further neighborhoods can be aggregated by successive applications of the GSO, also
called shifts. Using this notion of shift, graph convolutions are defined by weighting the contribu-
tion of each successive application of S to define a polynomial in the GSO. Explicitly, the graph
2
Under review as a conference paper at ICLR 2022
convolutional filter with coefficients h = [h0, . . . , hK-1] is given by
K-1
y = h *S X = X hkSkX	(1)
k=0
where *s denotes the convolution operation with GSO S.
Since the adjacency matrix of an undirected graph is always symmetric, the GSO admits an eigen-
decomposition S = VΛVH . The columns of V are the graph eigenvectors and the diagonal el-
ements of Λ are the graph eigenvalues, which take values between -1 and 1 and are ordered as
-1 ≤ λ-1 ≤ λ-2 ≤ . . . ≤ 0 ≤ . . . ≤ λ2 ≤ λ1 ≤ 1. Since the eigenvectors of S form an
orthonormal basis of Rn, we can project (1) onto this basis to obtain the spectral representation of
the graph convolution, which is given by
K-1
h(λ) = X hkλk.	(2)
k=0
Note that (2) only depends on the hk and on the eigenvalues of the GSO. Hence, as a consequence
of the Cayley-Hamilton theorem, convolutional filters may be used to represent any graph filter with
spectral representation h(λ) = f(λ) where f is analytic (Strang, 1976).
Graph neural networks are layered architectures where each layer consists of a graph convolution
followed by a pointwise nonlinearity ρ, and where each layer’s output is the input to the following
layer. At layer l, a GNN can output multiple features Xlf , 1 ≤ f ≤ Fl which we stack in a
matrix Xl = [Xl1, . . . , XlFl] ∈ Rn×Fl. Each column of the feature matrix is the value of the graph
signal at feature f . To map the Fl-1 features coming from layer l - 1 into Fl features, Fl-1 × Fl
convolutions need to be computed, one per input-output feature pair. Stacking their weights in K
matrices Hlk ∈ RFl-1×Fl, we write the l-th layer of the GNN as
Xl = ρ KX=-01 SkXl-1Hlk .	(3)
In an L-layer GNN, the operation in (3) is cascaded L times to obtain the GNN output Y = XL . At
the first layer, the GNN input is given by X0 = X ∈ Rn×F0. In this paper we assume F0 = FL = 1
so that Y = y and X = X. A more concise representation of this GNN can be obtained by grouping
all learnable parameters Hlk in a tensor H = {Hlk}l,k and defining the map y = Φ(X; H, S). Due
to the polynomial nature of the graph convolution, the dimensions of the learnable parameter tensor
H are independent from the size of the graph (K is typically much smaller than n). Ergo, a GNN
trained on a graph Gn can be deployed on a network Gm with m 6= n.
3.2	Graphon Information Processing
A graphon is a bounded, symmetric, and measurable function W : [0, 1]2 → [0, 1] which has two
theoretical interpretations — it is both a graph limit and a generative model for graphs. In the first
interpretation, sequences of dense graphs converge to a graphon in the sense that the densities of
adjacency-preserving graph motifs converge to the same densities on the graphon Lovasz (2012). In
the second, graphs can be generated from a graphon by sampling points ui , uj from the unit interval
and either assigning weight W(ui, uj) to edges (i, j), or sampling edges (i, j) with probability
W(ui, uj). In this paper, we focus on stochastic graphs Gn where the points ui are defined as
ui = (i - 1)/n for 1 ≤ i ≤ n and where the adjacency matrix Sn is sampled from W as
[Sn]j 〜BernOUUi(W(ui,uj)).	(4)
Sequences of graphs generated in this way can be shown to converge to W with probability one
(LOVaSz, 2012)[Chapter 11].
In practice, the two theoretical interpretations of a graphon allow thinking of it as an identifying
object for a family of graphs of different sizes that are structurally similar. Hence, given a network
we can use its family’s identifying graphon as a continuous proxy for the graph. This is beneficial
because it is typically easier to operate in continuous than in discrete domains, even more so if the
network is large. We will leverage these ideas to consider graphon data and graphon neural networks
as proxies for graph data and GNNs supported on graphs of arbitrary size.
3
Under review as a conference paper at ICLR 2022
3.2.1	Graphon Neural Networks
Graphon data is defined as functions X ∈ L2 ([0, 1]). Analogously to graph data, graphon data can
be diffused by application of a linear integral operator parametrized by W and defined as
TWX(v)
W(u, v)X(u)du.
0
(5)
The operator TW is called graphon shift operator (WSO).
The graphon convolution is defined as a weighted sum of successive applications of the WSO.
Explicitly, the graphon convolutional filter with coefficients h = [h0, . . . , hK-1] is given by
K-1
Y = h *w X = X hk(TW)X)(v) With
k=0
1
(TW(k)X)(v) =
0
(6)
W(u, v)(TW(k-1)X)(u)du
Where TW(0) = I is the identity (Ruiz et al., 2020b). Since W is bounded and symmetric, TW
is a self-adjoint Hilbert-Schmidt operator (Lax, 2002). Hence, W can be Written as W(u, v) =
Pi∈z∖{o} λi夕i(u)2i(v) where λi are the graphon eigenvalues and 夕i the graphon eigenfunctions.
The eigenvalues have magnitude at most one and are ordered as -1 ≤ λ-1 ≤ λ-2 ≤ . . . ≤ 0 ≤
. . . λ2 ≤ λ1 ≤ 1. The eigenfunctions form an orthonormal basis of L2([0, 1]). Projecting the filter
(1) onto this basis, we see that the graphon convolution admits a spectral representation given by
K-1
h(λ) = X hkλk.	(7)
k=0
Like its graph counterpart, this spectral representation only depends on the eigenvalues of the
graphon.
Graphon neural networks (WNNs) are the extension of GNNs to graphon data. In the WNN, each
layer consists of a bank of graphon convolutions (6) followed by a nonlinearity ρ. Assuming that
layer l maps Fl-1 features into Fl features, the parameters of the Fl-1 × Fl convolutions (6) can be
stacked into K matrices {Hlk} ∈ RFl-1 ×Fl . This allows writing the fth feature at layer l as
(Fl-1 K-1	∖
X X (TW(k)Xlg-1)[Hlk]gf	(8)
g=1 k=1
for 1 ≤ f ≤ Fl. For an L-layer WNN, (8) is repeated for 1 ≤ ` ≤ L. The WNN output is given by
Yf = XLf , and X0g is given by the input data Xg for 1 ≤ g ≤ F0 . We assume FL = F0 = 1 so that
XL = Y and X0 = X. A more succinct representation of this WNN is the map Y = Φ(X; H, W),
where the tensor H = {Hlk}l,k groups the filter coefficients at all layers.
3.2.2	Sampling GNNs from WNNs
From the representation of the GNN and the WNN as maps Φ(x; H, S) and Φ(X; H, W), we see
that these architectures can share the same filter coefficients H. Since graphs can be obtained from
graphons as in (4), we can similarly use the WNN Φ(X; H, W) to sample GNNs
Yn = Φ(xn H, Sn) where [Sn]j 〜BernoUlli(W(ui, Uj))
[xn]i = X(ui)
i.e., the WNN can be seen as a generative model for GNNs Φ(xn; H, Sn).
Conversely, aWNN cabe induced by a GNN. Given aGNN yn = Φ(xn; H, Sn), the WNN induced
by this GNN is defined as
nn
Yn = Φ(Xn; H, Wn) where Wn(u, v) =X=1X=1[Sn]ijI(u ∈ Ii)I(v ∈Ij)
n
Xn (u) = X[xn]iI(u ∈ Ii)
i=1
(10)
4
Under review as a conference paper at ICLR 2022
where I denotes the indicator function and the intervals Ii are defined as Ii = [(i - 1)/n, i/n) for
1 ≤ i ≤ n - 1 and In = [(n - 1)/n, 1]. The graphon Wn is called the graphon induced by the
graph Gn and Xn and Yn are called the graphon signals induced by the graph signals xn and yn .
4	Graphon Empirical Learning
On graphons, the statistical loss minimization (or statistical learning) problem is given by
minimize Ep(YX)['(Y, Φ(X; H, W))]	(11)
H
where p(Y, X ) is the joint distribution of the data, ` is an instantaneous loss function and
Φ(X; H, W) is a function parametrized by the graphon W and by a set of learnable weights H.
In this paper, we consider positive loss functions ` : R × R → R+ . The function Φ is parametrized
as a graphon neural network [cf. (8)]. Because the joint probability distribution p(Y, X) is unknown,
we are unable to derive a closed-form solution of (11), but this problem can be approximated by the
empirical risk minimization (ERM) problem over graphons.
4.1	Graphon Empirical Risk Minimization
Suppose that We have access to samples of the distribution D = {(Xj,Yj) 〜 p(X, Y),j =
1, . . . , |D|}. Provided that these samples are obtained independently and that |D| is large enough,
the statistical loss in (11) can be approximated as
|D|
minimize	X '(Yj, Φ(Xj; H, W))	(12)
H
j=1
giving Way to the ERM problem (Hastie et al., 2009; Shalev-ShWartz & Ben-David, 2014; Vapnik,
1999; Kearns et al., 1994). To solve this problem using local information, We could adopt some
flavor of gradient descent (GoodfelloW et al., 2016). The learning iteration at step k is then
Hk+1 = Hk - ηkVh'(Yj, Φ(Xj; Hk, W))	(13)
Where k = 1, 2, . . . denotes the current iteration and ηk ∈ (0, 1) the step size at iteration k.
In practice, the gradients on the right hand side of (13) cannot be computed because, being a theoret-
ical limit object, the graphon W is unknoWn. HoWever, We can leverage the fact that the graphon is a
random graph model to approximate the gradients Vh'(Yj, Φ(Xj; H, W)) by sampling stochastic
graphs Gn with GSO Sn [cf. (4)] and calculating VH'(yn, Φ(xjl; H, Sn)), where Xn and yjrι are
as in (9). In this case, the graphon empirical learning step in (13) becomes
Hk+1 = Hk - ηk vH'(yn, φ(Xn； Hk, Sn))	(14)
and we have to derive an upper bound for the expected error made when using the gradient calculated
on the graph to approximate the gradient on the graphon. In what follows, we give a closed-form
expression of this bound, and use it as a stepping stone to develop an Algorithm that increases n,
the graph size, over regular intervals during the training process of the GNN. We then prove that our
Algorithm converges to a neighborhood of the optimal solution for the WNN.
4.2	Gradient Approximation
To state our first convergence result, we need following definition.
Definition 1 (Lipschitz functions). A function f(u1, u2, . . . , ud) is A-Lipschitz on the variables
u1, . . . , ud if it satisfies |f(v1, v2, . . . , vd) - f(u1, u2, . . . ,ud)| ≤ A Pid=1 |vi - ui|. If A = 1, we
say that this function is normalized Lipschitz.
We also need the following Lipschitz continuity assumptions (AS1-AS4), as well as an assumption
on the size of the graph Sn (AS5).
AS1. The graphon W and the graphon signals X and Y are normalized Lipschitz.
AS2. The convolutional filters h are normalized Lipschitz and non-amplifying, i.e., kh(λ)k < 1.
5
Under review as a conference paper at ICLR 2022
Algorithm 1 Increase and Conquer: Growing Graph Training
1:	Initialize H0 , n0 and sample graph Gn0 from graphon W
2:	repeat for epochs 0, 1, . . .
3:	fork=1,..., |D| do
4:	Obtain sample (Y,X) ~ D
5:	Construct graph signal yn , xn [cf. (9)]
6：	Take learning step Hk+ι = Hk - ηkV'(yn, Φ(xn; Hk, Sn))
7:	end for
8:	Increase number of nodes n and sample Sn Bernoulli from graphon W [cf. (4)]
9:	until convergence
AS3. The activation functions and their gradients are normalized Lipschitz, and ρ(0) = 0.
AS4. The loss function ' : R X R → R+ and its gradient are normalized Lipschitz, and '(x, x) = 0.
AS5. For a fixed value of ξ ∈ (0,1), n is such that n — log(2n∕ξ)∕dw > 2/dw where dw denotes
the maximum degree of the graphon W, i.e., dW = maxv R01 W(u, v)du.
ASI-AS4 are normalized LiPschitz smoothness conditions which can be relaxed by making the
Lipschitz constant greater than one. AS3 holds for most typical activation functions. AS4 can be
achieved by normalization and holds for most loss functions in a closed set (e.g., the hinge or mean
square losses). AS5 is necessary to guarantee a O( ʌ/log n/n) rate of convergence of Sn to W
(Chung & Radcliffe, 2011).
Under AS1-AS5, the following theorem shows that the expected norm of the difference between the
graphon and graph gradients in (13) and (14) is bounded. The proof is deferred to the appendices.
Theorem 1. Consider the ERM problem in (12) and let Φ(X; H, W) be an L-layer WNN with
F0 = FL = 1, and Fl = F for 1 ≤ l ≤ L - 1. Let c ∈ (0, 1] and assume that the graphon
convolutions in all layers of this WNN have K filter taps [cf. (6)]. Let Φ(xn; H, Sn) be a GNN
sampled from Φ(X; H, W) as in (9). Under assumptions AS1-AS5, it holds that
E[∣∣Vh'(Y, Φ(X; H, W)) - VH'(Yn, Φ(Xn; H, Wn))k] ≤ Yc + O (ʌ/log(nɜ/2))	(15)
where Yn is the graphon signal induced by [yn]i = Y(ui) [cf. (10)], and γ is a constant that
depends on the number of layers L, features F, and filter taps K of the GNN [cf. Definition 7 in the
Appendix].
The bound in Theorem 1 quantifies the maximum distance between the learning steps on the graph
and on the graphon as a function of the size of the graph. This bound is controlled by two terms. On
the one hand, we have the approximation bound, a term that decreases with n. The approximation
bound is related to the approximation of W by Sn . On the other, we have the nontransferable
bound, which is constant and controlled by c. This term is related to a threshold that we impose on
the convolutional filter h. Since graphons W have an infinite spectrum that accumulates around zero,
convergence of all spectral components of the filtered data on Wn can only be shown in the limit of n
(Ruiz et al., 2020a). Hence, given that n is finite we can only upper bound distances between spectral
components associated with eigenvalues larger than some c ∈ (0, 1] that we fix. Meanwhile, the
perturbations associated with the other spectral components are controlled by bounding the variation
of the filter response below c.
The bound in Theorem 1 is important because it allows us to quantify the error incurred by taking
gradient steps not on the graphon, but on the graph data. Although we cannot take gradients on the
function that we want to learn [cf. (13)], by measuring the ratio between the norm of the gradient
of the loss on the GNN, and the difference between the two gradients, we can expect to follow the
direction of the gradient on the WNN. This is instrumental for learning a meaningful solution of the
graphon ERM problem (12). Nonetheless, we are only able to decrease this approximation bound
down to the value of the nontransferable bound.
6
Under review as a conference paper at ICLR 2022
4.3	Algorithm Construction
Since the discretization error depends on the number of nodes of the graph, we can iteratively in-
crease the graph size at every epoch. Even if a bias is introduced in the gradient we will be able
to follow the gradient direction of the graphon learning problem (13). At the same time, we need
to keep the computational cost of the iterates under control. Note that the norm of the gradient is
larger at first, but it decreases as we approach the optimal solution. Exploiting this behavior, we may
progressively reduce the bias term as iterations increase. The idea here is to keep the discretization
of the gradient small so as to closely follow the gradient direction of the graphon learning problem,
but without being too conservative as this would incur in high computational cost.
In practice, in the ERM problem the number of nodes that can be added is upper bounded by the
available data, which is defined nodewise on the graph. Hence, we can arbitrarily decide which and
how many nodes to consider for training. The novelty of Algorithm 1 is that we do not use the largest
graph available in the dataset at every epoch to train the GNN. Instead, we set a minimum graph size
n0 and progressively increase it up to the total number of nodes. The main advantage of Algorithm
1 is thus that it allows reducing the computational cost of training without compromising GNN
performance. In what follows, we will provide the conditions under which Algorithm 1 converges
to a neighborhood of the optimal solution on the graphon.
4.4	Algorithm Convergence
We have shown that the learning step on the graphon and on the graph are close. Now, it remains
to show the practical implications of Theorem 1 for obtaining the solution of the graphon ERM
problem (12). If the expected difference between the gradients is small, the iterations generated by
(14) will be able to follow the direction of the true gradient on the graphon learning problem. But
because the distance between gradients is inversely proportional to n, we need to strike a balance
between obtaining a good approximation and minimizing the computational cost. In Theorem 2,
we show that Algorithm 1 converges if the rate at which the graph grows is chosen to satisfy the
condition given in (16).
AS6. The graphon neural network Φ(X; H, W) is Aφ-Lipschitz, and its gradient VhΦ(X; H, W)
is Avφ -Lipschitz, with respect to the parameters H [cf. Definition 1].
Theorem 2. Consider the ERM problem in (12) and let Φ(X; H, W) be an L-layer WNN with
Fo = FL = 1, and Fl = F for 1 ≤ l ≤ L 一 1. Let C ∈ (0,1], e ∈ (0,1 一 AvΦη), and assume that
the graphon convolutions in all layers of this WNN have K filter taps [cf. (6)]. Let Φ(xn; H, Sn)
be a GNN sampled from Φ(X; H, W) as in (9). Consider the iterates generated by equation (16).
Under Assumptions AS1-AS6, ifat each step k the number of nodes n verifies
Yc + O(rlθg(n3/2)) < 1 - A；'n - e kVH'(Yn, Φ(Xn; Hk, Wn)k	(16)
then infinite time we will achieve an iterate k* such that the coefficients Hk* satisfy
E[∣∣Vh'(Y, Φ(X; Hk*, W))k] ≤ 2γc	afteratmost k* = O(1∕e)	(17)
where γ is a constant that depends on the number of layers L, features F, and filter taps K of the
GNN [cf. Definition 7 in the Appendix].
Theorem 2 presents the conditions under which Algorithm 1 converges. Intuitively, the condition in
(16) implies that the rate of decrease in the norm of the gradient of the loss function computed on the
graph neural network needs to be slower than roughly n-1/2 . If the norm of the gradient does not
vary between iterations, the number of nodes does not need to be increased. Note that (115) is equal
to twice the bias term obtained in equation (15). We can keep increasing the number of nodes — and
thus decreasing the bias — until the norm of the GNN gradient is smaller than the nontransferable
constant value. Once this point is attained, there is no gain in decreasing the approximation bound
any further (Ajalloeian & Stich, 2020). Recall that the constant term can be made as small as desired
by tuning c, at the cost of decreasing the approximation bound. To conclude, observe that assuming
smoothness of the GNN is a mild assumption (Scaman & Virmaux, 2018; Jordan & Dimakis, 2020;
Latorre et al., 2020; Fazlyab et al., 2019; Tanielian & Biau, 2021; Du et al., 2019). A characterization
of the Lipschitz constant is an interesting research question but is out of the scope of this work.
7
Under review as a conference paper at ICLR 2022
5 Numerical Results
5.1	Recommendation System
We consider a social recommendation problem given by movie ratings, for which we use the Movie-
Lens 100k dataset Harper & Konstan (2015). The dataset contains 100, 000 integer ratings between
1 and 5, that were collected between U = 943 users and M = 1682 movies. We consider the prob-
lem of predicting the rating that different users would give to the movie “Contact”. To exploit the
social connectivity of the problem, we build the movie similarity graph, by computing the pairwise
correlations between the different movies in the training set Huang et al. (2018). Further details
about training splits, and hyperparameter selection can be found in the supplementary material.
For the experiment, we started the GNNs with 200, and at 300 nodes and added {25, 50, 75, 100}
nodes per epochs. The total number of epochs of the GNN trained on 1000 nodes with which
we compare, is given by the maximum number of epochs that the Algorithm 1 can be run adding
25 nodes per epoch. In Figure 1, we are able to see the empirical manifestation of the benefit of
Algorithm 1. Namely, regardless of the number of nodes added per epoch, in all four cases, using
relative RMSE as a figure of merit, we achieve a comparable performance to that of a GNNs trained
on 1000 nodes for a larger number of epochs. This reinforces the fact that training on a growing
number of graphs can attain similar performance, while requiring a lighter computational cost.
5.2	Decentralized Control
In this section we consider the problem of coordinating a set ofn agents initially flying at random to
avoid collisions, and to fly at the same velocity. Also known as flocking, at each time t agent i knows
its own position ri(t) ∈ R2, and speed vi(t) ∈ R2, and reciprocally exchanges it with its neighboring
nodes if a communication link exists between them. Links are govern by physical proximity between
agents forming a time varying graph Gn = (V, E). A communication link exists if the distance
between two agents i, j satisfies rij (t) = kri(t) - rj(t)k ≤ R = 2m. We assume that at each
time t the controller sets an acceleration ui ∈ [-10, 10]2, and that it remains constant for a time
interval Ts = 20ms. The system dynamics are govern by,ri(t+1) =ui(t)Ts2/2+vi(t)Ts+ri(t),
vi(t + 1) = ui(t)Ts + vi(t). To avoid the swarm of robots to reach a null common velocity, we
initialize the velocities at random v(t) = [v1 (t), . . . , vn(t)], by uniformly sampling a common
bias velocity VBIAS 〜 U[-3,3], and then adding independent uniform noise U[-3, 3] to each
agent. The initial deployment is randomly selected in a circle always verifying that the minimum
distance between two agents is larger than 0.1m. On the one hand, agents pursue a common average
velocity V = (1/n) Pn=ι ViO, thus minimizing the velocity variation of the team. On the other
200.0
80.0
70.0
60.0
1
4	5	6	7	8	9
Number of Epochs
10	11	12	13
200.0
1	25 Nodes Added	(Starting	at 300	Nodes)
⅝	50 Nodes Added	(Starting	at 300	Nodes)
⅜	75 Nodes Added	(Starting	at 300	Nodes)
⅜	100 Nodes Added (Starting at 300 Nodes)
80.0 ■
70.0 ■
60.0 -
1
3	4
Number of Epochs
(a)	(b)
Figure 1: Relative RMSE of a recommendation system trained on MovieLens 100k for 20 independent par-
titions measured over the test set (a) starting with GNNs of 200 nodes, compared to a GNN trained on 1000
nodes for 37 epochs (b) starting with GNNs of 300 nodes, compared to a GNN trained on 1000 nodes for 29
epochs.
8
Under review as a conference paper at ICLR 2022
7 8 9 10 11 12
Number of Epochs
(a)	(b)
Figure 2: Velocity variation of the flocking problem for the whole trajectory in the testing set relative to the
centralized controller (a) starting with 10 nodes and (b) starting with 20 nodes.
hand, agents are required to avoid collision. We can thus define the velocity variation of the team
σv(t) = Pn=IIlvi(t) - v(t) k2, and the collision avoidance potential
CA(ri, rj)
1/Iri(t) -rj(t)I2 - log(Iri(t) -rj(t)I2) ifIri(t) -rj(t)(t)I ≤ RCA
1/R2CA - log(R2CA)
otherwise,
(18)
with RCA = 1m. A centralized controller can be obtain by ui(t)* = -n(vi — v) +
Pn=I JiCA(ri, rj) (Tanner et al., 2003).
Exploiting the fact that neural networks are universal approximators (Barron, 1993; Hornik, 1991),
Imitation Learning can be utilized as a framework to train neural networks from information pro-
vided by an expert (Ross et al., 2011; Ross & Bagnell, 2010). Formally, we have a set of pairs T =
{xm, um}, m = 1,...,M, and during training We minimize the mean square error between the
optimal centralized controller, and the output of our GNN ∣∣um 一 Φ(xm; H, S)I2. Denoting Ni(t)
the neighborhood of agent i at time t, the state of the agents x(t) = [x(t)1, . . . , x(t)n], xi(t) ∈ R6,
is given by xi(t) = Pjj∈Ni(t)[vi(t) — Vj(t),rj(t)∕∣rj(t)∣4,rj(t)∕∣rj(t)k2]. Note that state
xi(t), gets transmitted between agents ifa communication link exists between them.
In Figure 2 we can see the empirical manifestation of the claims we put forward. First and foremost,
we are able to learn a GNN that achieves a comparable performance while taking steps on a smaller
graphs. As seen in Figure 2, GNNs trained with n0 = {10, 20} agents in the first epoch and adding
10 agents per epoch (green line) are able to achieve a similar performance when reaching 100 agents
than the one they would have achieved by training with 100 agents the same number of epochs.
Besides, ifwe add less nodes per epoch, we are able to achieve a similar performance that we would
have achieved by training on the large network for 30 epochs.
6 Conclusions
We have introduced a learning procedure for GNNs that progressively grows the size of the graph
while training. Our algorithm requires less computational cost — as the number of nodes in the
graph convolution is smaller — than training on the full graph without compromising performance.
Leveraging transferability results, we bounded the expected difference between the gradient on the
GNN, and the gradient on the WNN. Utilizing this result, we provided the theoretical guarantees
that our Algorithm converges to a neighborhood ofa first order stationary point of the WNN in finite
time. We benchmarked our algorithm on a recommendation system and a decentralized control
problem, achieving comparable performance to the one achieve by a GNN trained on the full graph.
9
Under review as a conference paper at ICLR 2022
References
Ahmad Ajalloeian and Sebastian U Stich. Analysis of sgd with biased gradient estimators. arXiv
preprint arXiv:2008.00051, 2020.
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function.
IEEE Transactions on Information theory, 39(3):930-945,19$3.
Dimitri P Bertsekas and John N Tsitsiklis. Gradient convergence in gradient methods with errors.
SIAM Journal on Optimization, 10(3):627-642, 2000.
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally
connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.
Ines Chami, Sami Abu-El-Haija, Bryan Perozzi, Christopher Re, and Kevin Murphy. Machine
learning on graphs: A model and comprehensive taxonomy, 2021.
Zhengdao Chen, Soledad Villar, Lei Chen, and Joan Bruna. On the equivalence between graph
isomorphism testing and function approximation with gnns. arXiv preprint arXiv:1905.12560,
2019.
Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna. Can graph neural networks count
substructures? arXiv preprint arXiv:2002.04025, 2020.
Fan Chung and Mary Radcliffe. On the spectra of general random graphs. the electronic journal of
combinatorics, pp. P215-P215, 2011.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon,
and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Curran
Associates, Inc., 2016.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-
1685. PMLR, 2019.
Rick Durrett. Probability: Theory and Examples. Cambridge University Press, 2019.
David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan
Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular
fingerprints. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances
in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015.
Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. Graph neural
networks for social recommendation. In The World Wide Web Conference, pp. 417-426, 2019.
Mahyar Fazlyab, Alexander Robey, Hamed Hassani, Manfred Morari, and George Pappas. Efficient
and accurate estimation of lipschitz constants for deep neural networks. Advances in Neural
Information Processing Systems, 32:11427-11438, 2019.
Alex Fout, Jonathon Byrd, Basir Shariat, and Asa Ben-Hur. Protein interface prediction using graph
convolutional networks. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-
wanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30.
Curran Associates, Inc., 2017.
Fernando Gama and Somayeh Sojoudi. Distributed linear-quadratic control with graph neural net-
works. arXiv preprint arXiv:2103.08417, 2021.
Fernando Gama, Antonio G Marques, Geert Leus, and Alejandro Ribeiro. Convolutional neural
network architectures for signals supported on graphs. IEEE Transactions on Signal Processing,
67(4):1034-1049, 2018.
Fernando Gama, Joan Bruna, and Alejandro Ribeiro. Stability properties of graph neural networks.
IEEE Transactions on Signal Processing, 68:5680-5695, 2020.
10
Under review as a conference paper at ICLR 2022
Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference on
Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 1263-1272.
PMLR, 06-11 Aug 2017.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT press Cambridge, 2016.
M. Gori, G. Monfardini, and F. Scarselli. A new model for learning in graph domains. Proceedings.
2005 IEEE International Joint Conference on Neural Networks, 2005., 2:729-734 vol. 2, 2005.
F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. Acm
transactions on interactive intelligent systems (tiis), 5(4):1-19, 2015.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning: data
mining, inference, and prediction. Springer Science & Business Media, 2009.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4
(2):251-257, 1991.
Weiyu Huang, Antonio G Marques, and Alejandro R Ribeiro. Rating prediction via graph signal
processing. IEEE Transactions on Signal Processing, 66(19):5066-5081, 2018.
Matt Jordan and Alexandros G Dimakis. Exactly computing the local lipschitz constant of relu
networks. arXiv preprint arXiv:2003.01219, 2020.
Michael J Kearns, Umesh Virkumar Vazirani, and Umesh Vazirani. An introduction to computational
learning theory. 1994.
Nicolas Keriven and Gabriel Peyre. Universal invariant and equivariant graph neural networks. In
Advances in Neural Information Processing Systems (NeurIPS), 2019.
Nicolas Keriven, Alberto Bietti, and Samuel Vaiter. Convergence and stability of graph convolu-
tional networks on large random graphs. arXiv preprint arXiv:2006.01868, 2020.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,
abs/1412.6980, 2015.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. arXiv preprint arXiv:1609.02907, 2016.
Fabian Latorre, Paul Rolland, and Volkan Cevher. Lipschitz constant estimation of neural networks
via sparse polynomial optimization. arXiv preprint arXiv:2004.08688, 2020.
P.	D. Lax. Functional Analysis. Wiley, 2002.
Ron Levie, Wei Huang, Lorenzo Bucci, Michael M Bronstein, and Gitta Kutyniok. Transferability
of spectral graph convolutional neural networks. arXiv preprint arXiv:1907.12972, 2019.
Qingbiao Li, Fernando Gama, Alejandro Ribeiro, and Amanda Prorok. Graph neural networks for
decentralized multi-robot path planning. arXiv preprint arXiv:1912.06095, 2019.
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural
networks. arXiv preprint arXiv:1511.05493, 2015.
LaSzlo Lovasz. Large networks and graph limits, volume 60. American Mathematical Soc., 2012.
Q.	Lu and L. Getoor. Link-based classification. In ICML 2003, 2003.
Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph
networks, 2019.
Siyuan Qi, Wenguan Wang, Baoxiong Jia, Jianbing Shen, and Song-Chun Zhu. Learning human-
object interactions by graph parsing neural networks. In Proceedings of the European Conference
on Computer Vision (ECCV), pp. 401-417, 2018.
11
Under review as a conference paper at ICLR 2022
Meng Qu, Yoshua Bengio, and Jian Tang. Gmnn: Graph markov neural networks. In International
conference on machine learning, pp. 5241-5250. PMLR, 2019.
StePhane Ross and Drew Bagnell. Efficient reductions for imitation learning. In Proceedings ofthe
thirteenth international conference on artificial intelligence and statistics, pp. 661-668. JMLR
WorkshoP and Conference Proceedings, 2010.
Stephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and struc-
tured Prediction to no-regret online learning. In Proceedings of the fourteenth international con-
ference on artificial intelligence and statistics, pp. 627-635. JMLR Workshop and Conference
Proceedings, 2011.
Luana Ruiz, Fernando Gama, Antonio Garcla Marques, and Alejandro Ribeiro. Invariance-
preserving localized activation functions for graph neural networks. IEEE Transactions on Signal
Processing, 68:127-141, 2019a.
Luana Ruiz, Fernando Gama, and Alejandro Ribeiro. Gated graph convolutional recurrent neural
networks. In 2019 27th European Signal Processing Conference (EUSIPCO), pp. 1-5. IEEE,
2019b.
Luana Ruiz, Luiz Chamon, and Alejandro Ribeiro. Graphon neural networks and the transferability
of graph neural networks. Advances in Neural Information Processing Systems, 33, 2020a.
Luana Ruiz, Luiz FO Chamon, and Alejandro Ribeiro. The graphon fourier transform. In
ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), pp. 5660-5664. IEEE, 2020b.
Luana Ruiz, Fernando Gama, Antonio Garcla Marques, and Alejandro Ribeiro. Invariance-
preserving localized activation functions for graph neural networks. IEEE Transactions on Signal
Processing, 68:127-141, 2020c. doi: 10.1109/TSP.2019.2955832.
Luana Ruiz, Zhiyang Wang, and Alejandro Ribeiro. Graph and graphon neural network stability.
arXiv preprint arXiv:2010.12529, 2020d.
Kevin Scaman and Aladin Virmaux. Lipschitz regularity of deep neural networks: analysis and
efficient estimation. In Proceedings of the 32nd International Conference on Neural Information
Processing Systems, pp. 3839-3848, 2018.
Franco Scarselli, Ah Chung Tsoi, and Markus Hagenbuchner. The vapnik-chervonenkis dimension
of graph and recursive neural networks. Neural Networks, 108:248-259, 2018. ISSN 0893-6080.
Michael Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max
Welling. In The Semantic Web - 15th International Conference, ESWC 2018, Proceedings, pp.
593-607. Springer/Verlag, 2018.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-
rithms. Cambridge university press, 2014.
Gilbert Strang. Linear Algebra and Its Applications. New York: Academic Press, 1976.
Qiaoyu Tan, Ninghao Liu, Xing Zhao, Hongxia Yang, Jingren Zhou, and Xia Hu. Learning to hash
with graph neural networks for recommender systems. In Proceedings of The Web Conference
2020, pp. 1988-1998, 2020.
Ugo Tanielian and Gerard Biau. Approximating lipschitz continuous functions with groupsort neu-
ral networks. In International Conference on Artificial Intelligence and Statistics, pp. 442-450.
PMLR, 2021.
Herbert G Tanner, Ali Jadbabaie, and George J Pappas. Stable flocking of mobile agents part i:
dynamic topology. In 42nd IEEE International Conference on Decision and Control (IEEE Cat.
No. 03CH37475), volume 2, pp. 2016-2021. IEEE, 2003.
Vladimir Vapnik. The Nature of Statistical Learning Theory. Springer Science & Business Media,
1999.
12
Under review as a conference paper at ICLR 2022
Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. A
comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and
Learning Systems, 32(1):4-24, 2021.
Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec.
Graph convolutional neural networks for web-scale recommender systems. In Proceedings of the
24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 974-
983, 2018.
Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang,
Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applica-
tions. AI Open, 1:57-81, 2020. ISSN 2666-6510.
13
Under review as a conference paper at ICLR 2022
A Appendix
B	Numerical Results Parameters
All experiments were done on a computer with 32gb of RAM, a CPU Intel Core i9-9900K
@3.60GHz x 16, a GPU GeForce RTX 2080 Ti/PCIe/SSE2, and Ubuntu 18.04.4 LTS.
B.1	Recommendation System
We split the dataset with 90% for the training set, and 10% for the testing set, and we run 20
independent random partitions. For the optimizer, we used 5 samples for the batch size, and ADAM
algorithm Kingma & Ba (2015), with learning rate 0.005, β1 = 0.9, β2 = 0.999, and without
learning rate decay. For the loss, we used the smooth L1 loss. For the GNN, we used ReLU as
non-linearity, we considered F = 32 features, K = 5 filter taps, and L = 1 layers.
We used the graph neural networks library available online at https://github.com/alelab-upenn/graph-
neural-networks/blob/master/examples/movieGNN.py and implemented with PyTorch.
B.2	Decentralized Control
We run the system for T = 2s, and used 400 samples for training, 20 for validation, and 20 for the
test set. For the optimizer, we used 20 samples for the batch size, and ADAM algorithm Kingma &
Ba (2015) with learning rate 0.0005, β1 = 0.9, β2 = 0.999, without learning rate decay. We used
a one layer Graph Neural Networks with F = 64 hidden units and K = 3 filter taps, and used the
hyperbolic tangent as non-linearity ρ. We run 10 independent realizations of each experiment.
We used the graph neural networks library available online at https://github.com/alelab-upenn/graph-
neural-networks/blob/master/examples/flockingGNN.py and implemented with PyTorch.
14
Under review as a conference paper at ICLR 2022
C Proof of Theorem 1
Definition 2 (Template graphs). Let {ui}in=1 be the regular n-partition of [0, 1], i.e.,
i-1
Ui =——	(19)
n
for 1 ≤ i ≤ n. The n-node template graph Gn, whose GSO we denote Sn, is obtained from W as
[Sn]ij = W(ui, uj)	(20)
for 1 ≤ i, j ≤ n.
Definition 3 (Graphon spectral representation of convolutional filter response). As the graphon
W is bounded and symmetric, TW is a self adjoint Hilbert-Schmidt operator, which allows to use
the operator’s spectral basis W(u, v) = i∈Z{0} λiψi (u)ψi (v). Eigenvalues λi are ordered in
decreasing order of absolute value i.e., 1 ≥ λι ≥ λ2 >∙∙∙> 0 >∙∙∙> λ-2 ≥ λ-ι ≥ —1, and
their only accumulation point is 0 (Lax, 2002, Theorem 3, Chapter 28). Thus, we define the spectral
representation of the convolutional filter TH (cf. (6)) as,
K-1
h(λ) = X hkλk
k=0
(21)
Definition 4 (c-band cardinality of W). The c-band cardinality, denoted BWc , is the number of
eigenvalues whose absolute value is larger than c.
BW = #{尢:kλik ≤ c}	(22)
Definition 5 (c-eigenvalue margin ofW - Wn). The c-eigenvalue margin ofW - Wn is defined as
the minimum distance between two different eigenvalues of the integral operator applied to W, and
to Wn as follows,
δWWn = min，{kλi(Tw) — λi(Twn)k : kλi(Twn)k ≥ c}	(23)
n	i,j 6=i
Definition 6 (Graphon Convolutional Filter). Given a graphon W, a graphon signal X, and filter
coefficients h = [h0, . . . , hK-1 ] the graphon filter TH : L2([0, 1]) → L2([0, 1]) is defined as,
K-1
(THX)(v) = X hk(TW(k)X)(v).
k=0
(24)
Proposition 1. Let X ∈ L2 ([0, 1]) be a normalized Lipschitz graphon signal, and let Xn be the
graphon signal induced by the graph signal xn obtained from X on the template graph Gn [cf.
Definition 2], i.e., [xn]i = X((i — 1)/n) for 1 ≤ i ≤ n. It holds that
kX -XnkL2 ≤ 1.
(25)
Proof. Let Ii = [(i — 1)/n, i/n) for 1 ≤ i ≤ n — 1 and In = [(n — 1)/n, 1]. Since the graphon is
normalized Lipschitz, for any u ∈ Ii, 1 ≤ i ≤ n, we have
We can then write
kX — Xnk2
kX(u) — Xn(u)k ≤ max
≤ 1.
n
|X(u) — Xn(u)|2du
(26)
(27)
(28)
Z
0
which completes the proof.
□
15
Under review as a conference paper at ICLR 2022
Proposition 2. Let W : [0, 1]2 → [0, 1] be a normalized Lipschitz graphon, and let Wn := WGn
be the graphon induced by the template graph Gn generated from W as in Definition 2. It holds
that
IW - Wnk ≤ 2.	(29)
n
Proof. Let Ii = [(i - 1)/n, i/n) for 1 ≤ i ≤ n - 1 and In = [(n - 1)/n, 1]. Since the graphon is
Lipschitz, for any u ∈ Ii , v ∈ Ij , 1 ≤ i, j ≤ n, we have
We can then write
kW(u, v) 一 Wn (u, v)k ≤ max		i ― 11	Ii	I
	u	一 丁 I	i	U In	I
+ max	v	- j-ɪ I	I j IV In	I
		I nI	
			
11		2	
≤	+		=	.	
nn		n	
kW -Wnk2 = Z 1 |W(u, v) - Wn(u, v)|2dudv
0
1	22
≤ J ( — I dudv
which concludes the proof.
(30)
(31)
(32)
(33)
(34)
□
Proposition 3. Consider the L-layer WNN given by Y = Φ(X; H, W), where F0 = FL = 1 and
f` = F for 1 ≤ ' ≤ L 一 1. Let C ∈ (0,1] and assume that the graphon convolutions in all layers of
this WNN have K filter taps [cf. (6)]. Under Assumptions 1 through 3, the norm of the gradient of
the WNN with respect to its parameters H = {Hlk}l,k can be upper bounded by,
∣∣VhΦ(X;H, W)k ≤ F2L√K.
(35)
Proof. We will find an upper bound for any element [Hιtk"gtft of the tensor H. We start by the
last layer of the WNN, applying the definition given in equation (8),
kvHιtkt]gtft φ(X； H, W)k = v[Hιtkt]gtft XL	(36)
(Fl-1 K-1	∖
XX (TW)Xg-ι)[HLk]gf )『(37)
g=1 k=1
By Assumption 3, the non-linearity P is normalized LiPschitz , i.e. Vρ(∙)(u) ≤ 1 for all u. Thus,
aplying the chain rule for the derivative, and the Cauchy-Schwartz inequality, the right hand side of
the previous expression can be rewritten as,
kvHιtkt]gtft φ(X ； H, W)k = VP X XITW)Xg-ι)[HLk]gf) ||
g=1 k=1
Fl-1 K-1
v[Hιtkt]gtft XX(TW)Xg-ι)[HLk]gf	(38)
g=1 k=1
Fl-1 K-1
≤ v[Hιtkt]gtft X X (TW)Xg-ι)[HLk]gf	(39)
g=1 k=1
Note that the a larger bound will occur if " < L 一 1, then by linearity of derivation, and the triangle
inequality we obtain,
Fl-1
kvHιtkt]gtft φ(X；H, W)k≤ X
g=1
K-1
X TW)(VH计k(5 Xg-i)[HLk]gf
k=1
(40)
16
Under review as a conference paper at ICLR 2022
By Assumption 2, the convolutional filters are non-amplifying, thus it holds that,
Fl-1
kV[Hιtkdgtft 小(及 H, W)k≤ X V[Hιtkt]gtft Xg-I	(41)
g=1
Now note that as filters are non-amplifying, the maximum difference in the gradient will be attained
at the first layer (l = 1) of the WNN. Also note that the derivative of a convolutional filter TH [cf.
Definition 6] at coefficient k = i, is itself a convolutional filter with coefficients hi. The values of
hi are [hi]j = 1 ifj = i and 0 otherwise. Thence,
kV[Hltkt]gtft Φ(X; H, W)k≤ FLT hi*WXo	(42)
≤ FL-1kX0k.	(43)
To complete the proof note that tensor H has FL-1 K elements, and each individual gradient is
upper bounded by (43), and ∣∣X∣∣ is normalized by Assumption 1.	□
Lemma 1. Let Φ(X; H, W) be a WNN with F0 = FL = 1, and Fl = F for 1 ≤ l ≤ L - 1. Let
c ∈ (0, 1], and assume that the graphon convolutions in all layers of this WNN have K filter taps
[cf. (6)]. Let Φ(xn; H, Sn) be a GNN sampled from Φ(X; H, W) as in (9). Under assumptions
(1),(2),(3), and (5) with probability 1 - ξ it holds that,
∣Φ(X;H, W)- Φ(Xn; H, Wn)k ≤LFLT 1 +
2(1 + Jn log( 2n)
n
+ - + 4LF LTc
n
(44)
The fixed constants BWc and δWc W are the c-band cardinality and the c-eigenvalue margin of W
and Wn respectively [cf. Definitions 4,5].
Proof. We start by writing the expression on the left hand side, using the definition of WNN [cf.
(8)] we can write,
∣∣Φ(X;H, W)- Φ(Xn; H, Wn)II = IlXL - XnLk	(45)
(Fl-1 K-1	∖	(Fl-1 K-1	∖ Il
X	X (TW)XL-l)[HLk]gf	I - P(X	X	(TWnXnL-I)HLk]gf ) Il	.
g=1	k=1	g=1	k=1	I
Since the non-linearity ρ is normalized Lipschitz by Assumption 3, using the triangle inequality, we
obtain
FL-1
∣XL - XnL ∣ ≤ X
g=1
K-1	K-1
X(TW(k)XLg-1)[HLk]gf-X(TW(k)nXngL-1)[HLk]gf
k=1	k=1
(46)
Using the triangle inequality once again, we split the last inequality into two terms as follows,
FL-1
∣XL - XnL ∣ ≤ X
g=1
K-1
X TW(k)(XLg-1 - XngL-1)[HLk]gf
k=1
FL-1
+X
g=1
K-1
X(TW(k) - TW(k)n)XLg-1[HLk]gf
k=1
(1)
(2).
(47)
Where we have split (47) into terms (1), and (2). On the one hand, by assumption 2, convolutional
filters h are non-amplifying, thus using Cauchy-Schwartz inequality, term (1) can be bounded by,
Σ
g=1
K-1
X TW(k)(XLg-1 - XngL-1)[HLk]gf
k=1
FL-1
≤ X IIXLg-1 - XngL-1II .
g=1
(48)
17
Under review as a conference paper at ICLR 2022
To bound term (2), denoting hLgf the spectral representation of the convolutional filter applied to
XLg-1 at feature f of layer L [cf. Definition 3], we will decompose the filter as follows,
Lgf( ) hLgf (λ) - hLgf (c)
if ∣λ∣ < C
if∣λ∣ ≥ c
if ∣λ∣ < c
if ∣λ∣ ≥ c.
(49)
(50)
Note that hLgf = hL≥gcf + hL<gcf . Let T[<Hc ] and T[<Hc ] , be the graphon convoutional filters
with filter function hL<gcf on graphons W, and Wn respectively [cf. Definition 6]. Note that filter
hL<gcf , varies only in the interval [0, c), and since filters are normalized Lipschitz by Assumption 2,
it verifies
T[<HcL]gfXLg-1 - T[<HcnL]gfXLg-1 ≤ (hLgf(c) + c) - (hLgf(c) - c)kkXLg-1	(51)
≤ 2ckXLg-1k.	(52)
Now we need to upper bound the difference in the high frequencies hL≥gcf. Let T[≥Hc ] and T[≥Hc ] ,
be the graphon filters with filter function hL≥gcf on graphons W, and Wn respectively. Let Sn
denote the template graph sampled from the graphon W [cf. definition 2]. We denote Wn, the
induced graphon by template graph Sn as in (10). By introducing T[≥Hc ] , the graph filter with
filter function hL≥gcf on graphon Wn, we can use the triangle inequality to obtain,
T[≥HcL]gf XLg-1 - T[≥HcnL]gf XLg-1 ≤ T[≥HcL]gf XLg-1 - T[≥HcnL]gf XLg-1 (2.1)
+T[≥HcnL]gfXLg-1-T[≥HcnL]gfXLg-1(2.2).	(53)
Under assumptions 1-5, to bound term (2.1) We can use (Ruiz et al., 2020a, Theorem 1), and to
bound term (2.2) we can use (Ruiz et al., 2020d, Lemma 2). Thus, with probability 1 - ξ, the
previous expression can be bounded by,
ll	ll /	πbc	∖2(1 + nlolog(2n))
ITHL]gfXL-1—THnL]gfXL」1 ≤ (1 + δWWn)-~n——zUXL-ιl.	(54)
Where the fixed constants BWc and δWc W are the c-band cardinality and the c-eigenvalue margin
of W and Wn respectively [cf. Definitions 4,5]. Hence, coming back to (47), We can use (48) to
upper bound (1), and We can use (52), and (54), to upper bound (2) as folloWs,
FL-1
kXL -XnLk ≤ X llXLg-1 - XngL-1ll +2ckXLg-1k
g=1
(55)
NoW, We arrive at a recursive equation that We can compute for the L layers, With F features per
layer, to obtain,
kXL -XnLk ≤F0 kX0 -Xn0k +2LFL-1ckX0k
+ LFL-1(1 + π⅛) 2(1 + "nj) kX0k .	(56)
Using Proposition 1, noting that F0 = 1 by construction, and using Assumption 1, concludes the
proof.	□
18
Under review as a conference paper at ICLR 2022
Lemma 2. Let Φ(X; H, W) be a WNN with Fo = FL = 1, and Fl = F for 1 ≤ l ≤ L - 1. Let
c ∈ (0, 1], and assume that the graphon convolutions in all layers of this WNN have K filter taps
[cf. (6)]. Let Φ(xn; H, Sn) be a GNN sampled from Φ(X; H, W) as in (9). Under assumptions
(1),(2),(3), and (5) with probability 1 - ξ it holds that,
∣∣VhΦ(X ; H, W)-V%Φ(Xn; H, Wn)k
≤ √KFLT 2L2F2l-2 1 +
2(1+q iog( 2n)
n
+ 2F LTL +8L2F 2L-2c
n
Proof. We will first show that the gradient with respect to any arbitrary element [Hltk"gft ∈ R
of H can be uniformly bounded. Note that the maximum is attained if 八 = 1. Without loss of
generality, assuming 八 > l 一 1, we can begin by using the definition given in equation (8) of the
output of the WNN as follows,
kv[Hltkdgt ft 小(及 H, W)-V[Hg,⑺小(羽；H, Wn)k
=kvHιtkt]gtft XL -V[H 计 kt]gtf t XfLk	(57)
(Fl-1 K-1	∖
XX(TW)Xl-i)[Hlk ]gf I
g=1 k=1
(Fl-1 K-1	∖
X X(TW(k)nXngl-1)[Hlk]gf I .	(58)
g=1 k=1
Taking derivatives by applying the chain rule, and applying the triangle inequality it yields,
kv[Hιtkt ]gtft XL- v[Hιtkt ]gtf t XnLk
≤
X X(TW(k)Xlg-1)[Hlk]gf	-Vρ X X(TW(k)nXngl-1)[Hlk]gf
Fl-1 K-1
Fl-1 K-1
g=1 k=1
Fl-1 K-1
V[Hltkt]gtft	X X (TW(k)Xlg-1)[Hlk]gf
g=1 k=1
Fl-1 K-1
X X (TW(k)nXngl-1)[Hlk]gf
g=1 k=1
Fl-1 K-1
g=1 k=1
Fl-1 K-1
tkdgtft X X (TW)Xg-ι)[Hlk]gf-VHltkt]gtf t X X (TWn Xnl-ι)[Hlk]gf
g=1 k=1
g=1 k=1
(59)
(60)
We can now use Cauchy-Schwartz inequality, Assumptions 3, 4, and Proposition 3 to bound the
terms regarding the gradient of the non-linearity ρ, the loss function `, and the WNN respectively,
as follows,
kV[Hltkt]gtftXLf -V[Hltkt]gtftXnfLk	(61)
Fl-1 K-1	Fl-1 K-1
≤X X(TW(k)Xlg-1)[Hlk]gf-X X(TW(k)nXngl-1)[Hlk]gfFL-1kX0k
g=1 k=1	g=1 k=1
Fl-1	K -1
+ V
[Hltkt]gtft	(TW(k)Xlg-1)[Hlk]gf - (TW(k)nXngl-1)[Hlk]gf)	.
g=1	k=1
19
Under review as a conference paper at ICLR 2022
We can now apply the triangle inequality on the second term of the previous bound to obtain,
kv[Hltkt]gtft XLf 一 v[Hltkt]gtft XnfLk	(62)
Fl-1 K-1
Fl-1 K-1
X X(TW)Xg.ι)Hlk]gf - X X(TWnXnl-I)[Hik]gf FLTkXOk
g=1 k=1
Fl-1
g=1 k=1
Xv
[Hltkt]gtft X (TW(k))[Hlk]gf -(TW(k)n)[Hlk]gf) Xngl-
K-1
g=1
Fl-1
+ X v
g=1
k=1
K-1
[Hltkt]gtft X TW(k)n
k=1
1- Xngl-1 [Hlk]gf).
≤
+
Now note that as We are considering the case in which l∣ < l 一 1, using CaUchy-SchWartz inequality,
we can use the same bound for the first and second term of the right hand side of the previous
inequality. Since filters are non-expansive by Assumption 3, it yields
kv[Hιtkt ]gtf t XL- v[Hιtkt ]gtft XnLk
(63)
Fl-1 K-1
Fl-1 K-1
≤2
X X (TW(k)Xlg-1)[Hlk]gf - X X(TW(k)nXngl-1)[Hlk]gfFL-1kX0k
g=1 k=1
Fl-1
g=1 k=1
+ v
g=1
1 - Xngl-1
Now notice, that the only term that remains to bound is the exact same bound we obtained in equation
(57), but on the previous layer L 一 2. Hence, we conclude that by applying the same steps L 一 2
times, as the WNN has L layers, we will obtain a bound for any element [Hltkt]gtf t of tensor H.
kv[Hιtkt ]gtf t XL- v[Hιtkt ]gtft XnLk
(64)
Fl-1 K-1
Fl-1 K-1
≤ 2LFL-2 X X (TW(k)Xlg-1)[Hlk]gf - X X(TW(k)nXngl-1)[Hlk]gfFL-1kX0k
g=1 k=1
g=1 k=1
Fl-1
+ X v
g=1
Note that the derivative of a convolutional filter TH at coefficient k = i, is itself a convolutional
filter with coefficients hi [cf. Definition 6]. The values of hi are [hi]j = 1 ifj = i and 0 otherwise.
As hi is itself a filter that verifies Assumption 2, as graphons are normalized. Thus, considering 八=
0, and using Propositions 1, 2, (Chung & Radcliffe, 2011, Theorem 1) and the triangle inequality,
we obtain,
hi* Wn
Xn0 一 hi*wX0 ≤ kW - Wnk + kWn 一 Wnk kXOk + kXno - Xok	(65)
≤ 1+
2(1+q iog( 2n)
(66)
n
1
+—
n
with probability 1 - ξ. In the previous expression, Wn is the template graphon [cf. Definition 2].
Now, substituting (64) into (65), and using Lemma 1, with probability 1 - ξ, it holds that,
kvHιtkt]gt ft XL - VHltkt]gtf t XnLk ≤2L2 F2L-2	1 +
2(1+q∏ iog( 2n)
n
+ 2FLTL +8L2F 2L-2c.
(67)
n
To achieve the final result, note that tensor H has KFL-1 elements, and each individual gradient is
upper bounded by (67).	□
20
Under review as a conference paper at ICLR 2022
Lemma 3. Let Φ(X; H, W) be a WNN with Fo = FL = 1, and Fl = F for 1 ≤ l ≤ L - 1. Let
c ∈ (0, 1], and assume that the graphon convolutions in all layers of this WNN have K filter taps
[cf. (6)]. Let Φ(xn; H, Sn) be a GNN sampled from Φ(X; H, W) as in (9). Under Assumptions
(1)—(5) with probability 1 一 ξ it holds that,
∣Nh'(Y, Φ(X； H, W)) - ^H'(Yn, Φ(Xn, H, Wn))Il
≤ √KFLT 3L2F2l-2 1 +
2 (1+qn log( 2n)
n
+ 4F LTL + 12L2F 2L-2c
n
Proof. In order to analyze the norm of the gradient with respect to the tensor H, we can start by
taking the derivative with respect to a single element of the tensor, [Hltkt ]g↑ f ↑. By deriving the loss
function ` using the chain rule it yields,
kV[H,tkt]gtft '(Y, φ(x; H W))-V[Hιtkt]gtft '(Yn, φ(Xn; H Wn))k
= kV'(Y, Φ(X； H, W))VHltkt]gtft Φ(X； H, W)
-V'(Yn, Φ(Xn, H, Wn))V[Hltkt]gtft Φ(Xn H, Wn)k∙	(68)
By Cauchy-Schwartz, and the triangle inequality it holds,
kV[Hltkt]gtft '(Y, Φ(X； H, W)) -V[Hltkt]gtft '(Yn, Φ(Xn; H Wn))k
≤∣V'(Y, Φ(X; H, W)) - V'(Yn, Φ(Xn, H, Wn))kkV[Hltkt]gt ft Φ(X ； H, W)k	(69)
+ kV'(Yn, Φ(Xn; H, Wn))kkV[Hltkt]gtft Φ(X ； H, W)-VHlt kt ]gt ft Φ(Xn H, Wn)k.
By the triangle inequality and Assumption 4 it follows,
kV[Hltkt]gtft '(Y, φ(X; H, W))-VHltkt]gtft '(Yn, φ(Xn; H, Wn))k
≤ kV'(Y, Φ(X; H, W)) - V'(Y, Φ(Xn; H, Wn))kkVHltkt]gtft Φ(X; H, W)k	(70)
kV'(Yn, Φ(Xn;H, Wn))- V'(Y, Φ(Xn; H, Wn))kkV[Hltkt]gtft Φ(X; H, W)k	(71)
+ IV[Hltkt]gtft Φ(X; H, W) - V[Hltkt]gtft Φ(Xn; H, Wnl)kI g f
≤(IYn-YI + IΦ(Xn; H, Wn)) - Φ(X; H, W))I)IV[Hltkt]gtft Φ(X; H, W)I	(72)
+ IV[Hltkt]gtft Φ(X; H, W) - V[Hltkt]gtft Φ(Xn; H, Wn)I.
Now We can use Lemmas 1-2, Propositions 1, and 3, and Assumption 1 to obtain,
kV[Hltkt]gtft '(Y, Φ(X; H, W)) -V[Hltkt]gtft '(Yn, Φ(Xn; H, Wn))∣∣	(73)
+ 4FLTL + 12L2F 2L-2c
n
Noting that tensor H has KFL-1 elements, and each individual term can be bounded by (73), the
desired result is attained.	□
Definition 7. We define the constant γ as,
Y = 12√KF LT L2F 2L-2,	(74)
where K is the number of features, L is the number of layers, and K is the number of filter taps of
the GNN.
21
Under review as a conference paper at ICLR 2022
We will present a more comprehensive statement of Theorem 1, where we include all the smaller
order terms in (15). Notice that the statement of Theorem 1 in the main body of the paper omits
these terms in order to simplify the exposition of the main result. In practice, these smaller order
terms vanish faster as n increases.
Theorem 1. Consider the ERM problem in (12) and let Φ(X; H, W) be an L-layer WNN with
F0 = FL = 1, and Fl = F for 1 ≤ l ≤ L - 1. Let c ∈ (0, 1] and assume that the graphon
convolutions in all layers of this WNN have K filter taps [cf. (6)]. Let Φ(xn ; H, Sn) be a GNN
SamPledfrom Φ(X; H, W) as in (9). Under assumptions AS1-AS5, it holds that
E[∣Nh'(Y, Φ(X; H, W))- VH'(Yn, Φ(Xn, H, Wn))I∣]
≤ Kfll-1 6L2f2L-2 1 +
1 + ʌ/n log(2n3/2)
n
+ 4LL-L + 12L2 F2L-2c) +-K	(75)
nn
where Yn is the graphon signal induced by [yn]i = Y(ui), ui = (i - 1)/n for 1 ≤ i ≤ n [cf. (10)].
The fixed constants BWc and δWc W are the c-band cardinality and the c-eigenvalue margin of W
and Wn respectively [cf. Definitions 4,5 in the supplementary material].
Proof of Theorem 1. We begin by considering the event An such that,
An =	∣Vh'(Y, Φ(X； H, W)) - VH'(Yn, Φ(Xn, H, Wn))Il
(76)
≤ √FLLT 3L2L2L-2 1 +
+ 4LL-IL + 12L2L2L-2c)).
Thus, by considering the disjoint events An, and An, and denoting 1(∙) the indicator function, the
expectation can be separated as follows,
E[∣Vh'(Y, Φ(X; H, W))-VH'(Yn, Φ(Xn, H, Wn))k]
=E[∣∣Vh'(Y, Φ(X; H, W)) - VHM Φ(Xn; H, Wn))k1(An)]
+ E[∣∣Vh'(Y, Φ(X; H, W)) - VH'(Yn, Φ(Xn, H, Wn))k 1(An)]	(77)
We can bound the term regarding Acn using the chain rule, Cauchy-Schwartz inequality, Assumption
4, and Proposition 3 as follows,
∣Vh'(Y, Φ(X； H, W)) - VH'(Yn, Φ(Xn; H, Wn))k
≤ IVh'(Y, Φ(X; H, W))k + kVH'(Yn, Φ(Xn; H, Wn))k	(78)
≤ kV'(Y, Φ(X;H, W))kkVHΦ(X; H, W)k
+ kV'(Yn, Φ(Xn; H, Wn))kkVHΦ(Xn; H, Wn)k	(79)
≤ IVhΦ(X;H,W)k + kVHΦ(Xn;H, Wn)k	(80)
≤ 2L2L√F	(81)
Returning to equation (77), we can substitute the bound obtained in equation (81), and by taking
P(An) = 1 - ξ, and using Lemma 3, it yields,
E[∣Vh'(Y, Φ(X; H, W)) - VH'(Yn, Φ(Xn; H, Wn))I∣]
≤ (1 - ξ)√FLLT 3L2L2L-2 1 +
2(1 + Jn log( 2n)
n
+ 4LL IL + 12L2L2L-2c) + ξ2L2L√F
n
To complete the proof, set ξ = √n.
(82)
□
22
Under review as a conference paper at ICLR 2022
/	χ[∣∣VH'(Y, Φ(X； H, W))- VH'(Yn, Φ(Xn; H, Wn))k]
VH'(Yn, Φ(Xn H, Wn^^
Figure 3: In order to satisfy the property that the inner product between the gradient on the GNN
Vh'(Y^, Φ(Xn; H, Wn)) and the gradient on the graphon Vh'(Y, Φ(X; H, W)) is positive, We rely on
the condition provided in Theorem 2.
D	Proof of Theorem 2
Definition 8 (Stopping time). We define the stopping time k as,
k* = min{kV%Φ(X;Hk, Wn)k ≤ √KFLT12L2F2L-2c}.	(83)
k≥0
Definition 9 (Constant ψ).
Lemma 4. Under Assumptions 4, 5, and 6, the gradient of the loss function ` with respect to the
parameters ofthe GNN H is Av'-Lipschitz,
kVH'(Y, Φ(X; A, W))- Vh'(Y, Φ(X; B, W))k ≤ A%∣∣A - Bk	(84)
where A▽' = (Aγφ + AφF2l√K).
Proof. To begin With, We can apply the chain rule to obtain,
kVH'(Y, Φ(X; A, W))- Vh'(Y, Φ(X; B, W))k
=kV'(Y, Φ(X; A, W))VhΦ(X; A, W) -V'(Y, Φ(X; B, W))VhΦ(X; B, W)k	(85)
By applying the triangle inequality, and Cauchy-SchWartz it yields,
∣∣Vh'(Y, Φ(X; A, W))- Vh'(Y, Φ(X; B, W))k
≤ kV'(Y, Φ(X; A, W))kkVHΦ(X; A, W) - VhΦ(X; B, W)k
+ ∣∣Vh'(Y, Φ(X; A, W)) - '(Y, Φ(X; B, W))∣∣∣∣VhΦ(X; B, W)k	(86)
We can noW use Assumptions 1, 4, 5, and 6 as Well as Proposition 3, to obtain
kVH'(Y, Φ(X; A, W))-VH'(Y, Φ(X；B, W))k ≤ (Avφ + AφF2L√KkA-Bk) (87)
Completing the proof.	□
Lemma 5. Consider the ERM problem in (12) and let Φ(X; H, W) be an L-layer WNN with
F0 = FL = 1, and Fl = F for 1 ≤ l ≤ L - 1. Let c ∈ (0, 1] and assume that the graphon
convolutions in all layers of this WNN have K filter taps [cf. (6)]. Let Φ(xn; H, Sn) be a GNN
sampled from Φ(X; H, W) as in (9). Under assumptions AS1-AS6, let the following condition be
satisfied for every k,
/	/	πBc 、(1 + Pn lOg(2n3/2))	4F L-1L	、
√KLL-1 ( 6L2F2l-2 (1 + ∏BWn ) ʌ--------------+ + 竺~L + 12L2F2L-2c)
δWWn	n	n
+ 2F2L√K < 1 - A/`ηk kVH'(γn, φ(Xn;Hk, Wn)k.	(88)
n2
then the first k* iterates generated by equation (14), 1(k ≤ k*)'(Y, Φ(X; Hk, Wn)) form a positive
super-martingale with respect to the filtration Fk generated by the history of the Algorithm up to step
k [i.e., {X, Y, Xn, Yn, Wn}k, {X, Y, Xn, Yn, Wn}k-1, . . . , {X, Y, Xn, Yn, Wn}0]. Where k* is
the stopping time defined in Definition 8, and 1(∙) is the indicator function.
23
Under review as a conference paper at ICLR 2022
Proof. To begin with, 1(k < k*)'(Y, Φ(X; Hk, W)) ∈ Fk, where Fk is the filtration generated by
the history of the Algorithm up to k. Note that the loss function ` is positive by Assumption 4. It
remains to be shown the inequality expression of the super-martingale. For k > k*, the inequality
is trivially verified as the indicator function 1(k ≤ k*) = 0 for k > k*. For k ≤ k*, as in Bertsekas
& Tsitsiklis (2000), we define a continuous function g() that takes the value of the loss function on
the Graphon data on iteration k + 1 at = 1, and on iteration k on = 0 as follows,
g(c) = '(Y, Φ(X; Hk - eηk^H'(Yn, Φ(Xn, Hk, Wn)), W)).	(89)
Note that function g(), is evaluated on the graphon data Y, X, W, but the steps are controlled by
the induced graphon data Yn, Xn, Wn. Applying the chain rule, the derivative of g() with respect
to can be obtain as follows,
晶㈤=	(90)
-Vh'(Y, Φ(X; Hk - eηk Vh'(%, Φ(Hk; Wn Xn)), W))ηk Vh'(%, Φ(Xn Hk； Wn)).
Now note that the difference in the loss function ` between iterations k + 1 and k can be written as
the difference between g( = 1) and g( = 0) as follows,
g(1) — g(0)= '(Y, Φ(X； Hk+ι, W)) — '(Y, Φ(X； Hk, W))∙	(91)
Computing the integration of the derivative of g() between [0, 1] it yields
'(Y, Φ(X； Hk+ι, W)) — '(Y, Φ(X； Hk, W))
1∂
g(1) — g(0) = J 浣g(e)de
(92)
Vh'(Y, Φ(X; Hk —
eηkV%'(Yn, Φ(Xn; Hk, Wn)), W))
( — )nk VH'(Yn, Φ(Xn; Hk, Wn))de∙
(93)
Note that the last term of the previous integral does not depend on . Besides, we can sum and
subtract Vh'(Y, Φ(Hk, W, X)) inside the integral, to obtain,
'(Y, Φ(X； Hk+ι, W)) — '(Y, Φ(X; Hk, W))
=( — )nk VH'(Yn, Φ(Xn; Hk , Wn))
1 Vh'(Y, Φ(X; Hk —3k VH'(Yn, Φ(Xn; Hk, Wn)), W))
0
+ Vh'(Y, Φ(X; Hk, W)) — Vh'(Y, Φ(X; Hk, W))de
—nkVH'(Yn, Φ(Xn; Hk, Wn))VH'(Y, Φ(X； Hk, W)) 1 d
0
(94)
— ηk VH'(Yn, Φ(Xn; Hk, Wn)) [1 Vh'(Y, Φ(Hk — eηk V%'(Yn, Φ(Xn; Hk, Wn)), W, X))
0
— Vh'(Y, Φ(X; Hk, W))de.
(95)
We can now apply the Cauchy-Schwartz inequality to the last term on the previous inequality, and
take the norm of the integral, which is smaller that the integral of the norm to obtain,
'(Y, Φ(X； Hk+1, W)) — '(Y, Φ(X; Hk, W))
≤ —nk VH'(Yn, Φ(Hk; Wn； Xn))VH'(Y, Φ(Hk, W, X))
+ nk kVH'(Yn, Φ(Hk; Wn；
Xn))k Z1
0
Vh'(Y, Φ(Hk - eηkVH'(Yn, Φ(Hk； Wn； Xn)), W, X))
Z
0
— Vh'(Y, Φ(Hk, W, X)) de.
(96)
24
Under review as a conference paper at ICLR 2022
Under Lemma 4, we can take the Lipschitz bound on the gradient on the loss function with respect
to the parameters, using A▽', to obtain,
'(Y, Φ(X; Hk+1, W))- '(Y, Φ(X; Hk, W))
≤ -ηkVH'(Yn, Φ(Xn, Hk, Wn))VH'(Y, Φ(X； Hk, W))
1
+ Aynk IlVH'(Yn, φ(Xn'; Hk, Wn
))|
0
nkVH'(Yn, Φ(Xn； Hk, Wn)) &,€
≤ -nk VH'(Yn, Φ(Xn; Hk, Wn))VH'(Y, Φ(X ； Hk , W))
+ 吗卫 kVH'(Yn, Φ(Xn Hk , Wn))k2.
(97)
(98)
Instead of evaluating the internal product between the gradient on the graphon, and induced graphon,
we will use Theorem 1, to bound their expected difference (cf. Figure 3 for intuition). We can add
and subtract the gradient of the loss function on the induced graphon VH'(Yn, Φ(Hk; Wn； Xn)),
and use the Cauchy-Schwartz inequality to obtain,
'(Y, Φ(X; Hk+1, W)) - '(Y, Φ(X; Hk, W))
≤ -nk VH'(Yn, Φ(Xn; Hk, Wn))
(Vh'(Y, Φ(X ； Hk, W)) + VH '(Yn, Φ(Xn; Hk, Wn))- Vh'(%, Φ(Xn; Hk, Wn)))
+ n2A▽φkVH'(Yn, Φ(Xn; Hk, Wn))k2	(99)
≤ -nk kVH'(Yn, Φ(Xn; Hk , Wn))k2
+ nk kVH'(Yn, Φ(Xn; Hk, Wn) ∣∣Vh'(Y, Φ(X ； Hk , W)) - VH'(Yn, Φ(Xn; Hk, Wn))k
+ nkA▽φ kVH'(Yn, Φ(Xn; Hk , Wn))k2.	(100)
We can rearrange the previous expression, to obtain,
nk kVH'(Yn, Φ(Xn; Hk, Wn )∣2 (l - A
∣Vh'(Y, Φ(X ； Hk, W)) - VH'(Yn, Φ(Xn; Hk , Wn))k
-------------T-----:-----:-----------------------
kVH'(Yn, Φ(Xn ； Hk, Wn)k
≤ '(Y, Φ(X； Hk, W)) - '(Y, Φ(X; Hk+1, W)).
(101)
We can now take the conditional expectation with respect to the filtration Fn to obtain,
E['(Y, Φ(X； Hk+1, W))∣Fk]
≤ nk IVh '(Yn, Φ(Xn; Hk, Wn)k2 (ι - A▽nk
-E Γ∣Vh'(Y, Φ(X; Hk, W))- VH'(Yn, Φ(Xn; Hk, Wn))k F D
[	kVH'(Yn, Φ(Xn; Hk, Wn)k	kV
+ '(Y, Φ(X； Hk, W)).	(102)
As step size nk > 0, and by definition norms are non-negative, using Theorem 1, as condition (88)
holds for k ≤ k*, then
E['(Y, Φ(X；Hk+ι, W))∣Fk] ≤ '(Y, Φ(X;Hk, W)).	(103)
By definition of super-martingale as in Durrett (2019), We complete the proof.	口
Lemma 6. Consider the ERM problem in (12) and let Φ(X; H, W) be an L-layer WNN with
F0 = FL = 1, and Fl = F for 1 ≤ l ≤ L - 1. Let c ∈ (0, 1] and assume that the graphon
convolutions in all layers of this WNN have K filter taps [cf. (6)]. Let Φ(xn; H, Sn) be a GNN
25
Under review as a conference paper at ICLR 2022
sampled from Φ(X; H, W) as in (9). Under assumptions AS1-AS6, for any E ∈ (0,1 一 A▽'#, if
the iterates generated by (14), satisfy,
π	π	πBc 、(1 + P lOg(2n3/2))	L-IL	、
√KLL-1 66L2F2L-21 1 + ∏BWn ) ʌ------------+ + 4F~L + 12L2F2L-2c)
δWWn	n	n
+ 2f√√k < 1 - Aηk - e kVH'(Yn, φ(Xn; Hk, Wn)∣∣.	(104)
n2
then the expected value of the stopping time k* [cf. Definition 8], is finite, i.e.,
E[k*] = O(1∕e)	(105)
Proof. Given the iterates at k = k*, and the initial values at k = 0, We can express the expected
difference between the loss `, as the summation over the difference of iterates as follows,
E['(Y, Φ(X; Ho, W))- '(Y, Φ(X; Hk, W))] =
-k*	-
E X '(Y, Φ(X； Hk-1, W))-'(Y, Φ(X； Hk, W)	(106)
k=1
Taking the expected value with respect to the final iterate k = k*, we get,
E '(Y, Φ(X; Hk0))一 '(Y, Φ(X; Hk*))
=Ek* E [X '(Y, Φ(X； Hk-1, W)) - '(Y, Φ(X； Hk, W) k*	(107)
k=1
∞t
=XE X '(γ, Φ(X； Hk-1, W)) - '(Y, Φ(X; Hk, W) P(k* = t)
t=0	k=1
(108)
Using condition (104), and Lemma 5 for any k ≤ k*, it verifies
E '(Y, Φ(X; Hk-1, W)) - '(Y, Φ(X； Hk, W)) ≥ η(√KFLT12L2F2L-2c)2e	(109)
Thus, coming back to (108),
∞
E '(Y, Φ(X; HkO, W)) - '(Y, Φ(X; Hk*, W)) ≥ η(√KFLT12L2F2L-2c)2eXtP(k* = t)
t=0
(110)
≥ η(√KFLT12L2F2L-2c)2eE[k*]	(111)
Note that as the loss function ` is non-negative,
E 卜(Y, Φ(X； HkO, W))
—!	-------------J— ≥ E[k*]	(112)
η(√KFLT12L2F2L-2c)2e ^
Thus concluding that k* = O(1∕e).	口
Theorem 2. Consider the ERM problem in (12) and let Φ(X; H, W) be an L-layer WNN with
F0 = FL = 1, and Fl = F for 1 ≤ l ≤ L - 1. Let c ∈ (0, 1] and assume that the graphon
convolutions in all layers of this WNN have K filter taps [cf. (6)]. Let Φ(xn; H, Sn) be a GNN
sampled from Φ(X; H, W) as in (9). Consider the iterates generated by equation (14). Under
26
Under review as a conference paper at ICLR 2022
Assumptions AS1-AS6, for any fixed E ∈ (0,1 一 A▽'#, if at each step k the number of nodes n is
picked such that it verifies
π	π	πBc 、(1 + P lOg(2n3/2))	L-IL	、
√KLL-1 66L2F2L-21 1 + ∏BWn ) ʌ-----------------+ + 4F-L + 12L2F2L-2c)
δWWn	n	n
+ 2f√√k < 1 - Aηk - e kVH'(Yn, φ(Xn; Hk, Wn)∣∣.	(113)
n2
then infinite time we will achieve an iterate k* such that the coefficients Hk* satisfy
E[∣∣Vh'(Y, Φ(X; Hk*, W))k] ≤ 24√KF LT L2F 2L-2c	with probability 1	(114)
where A▽%% = (A▽φ + AφF2l√K).
Proof. We can use Lemma 6, to conclude that it must be the case that P (k* = ∞) = 0, which
implies that, P(k* < ∞) = 1. Using stopping time k* condition [cf. Definition 8] and the triangle
inequality, it yields,
E[∣∣Vh'(Y, Φ(X; Hk*, W))∣∣] ≤kVH'(Yn, Φ(Xn Hk*, Wn))Il	(115)
+E[kVH'(Yn, Φ(X; Hk* , Wn))- Vh'(Y, Φ(Xn; Hk*, Wn))I∣]
Note that the iterates are constructed such that, for every k
E[kVH'(Yn, Φ(X; Hk, Wn)) -Vh'(Y, Φ(Xn; Hk, Wn))k] ≤ kVH'(Yn, Φ(X; Hk, Wn))I∣.
(116)
Using the stopping time condition, the final result is attained as follows
E[∣∣Vh'(Y, Φ(X; Hk*, W))k] ≤2∣VH'(Yn, Φ(X; Hk*, Wn))Il	(117)
≤24√KF LT L2F 2L-2c.	(118)
□
27