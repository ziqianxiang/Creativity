Under review as a conference paper at ICLR 2022
Safe Linear-Quadratic Dual Control with Almost Sure
Performance Guarantee
Anonymous authors
Paper under double-blind review
Ab stract
This paper considers the linear-quadratic dual control problem where the system
parameters need to be identified and the control objective needs to be optimized
in the meantime. Contrary to existing works on data-driven linear-quadratic regu-
lation, which typically provide error or regret bounds within a certain probability,
we propose an online algorithm that guarantees the asymptotic optimality of the
controller in the almost sure sense. Our dual control strategy consists of two parts:
a switched controller with time-decaying exploration noise and Markov parameter
inference based on the cross-correlation between the exploration noise and system
output. Central to the almost sure performance guarantee is a safe switched con-
trol strategy that falls back to a known conservative but stable controller when the
actual state deviates significantly from the target state. We prove that this switch-
ing strategy rules out any potential destabilizing controllers from being applied,
while the performance gap between our switching strategy and the optimal linear
state feedback is exponentially small. Under our dual control scheme, the param-
eter inference error scales as O(T -1/4+), while the suboptimality gap of control
performance scales as O(T -1/2+), where T is the number of time steps, and
is an arbitrarily small positive number. Simulation results on an industrial process
example are provided to illustrate the effectiveness of our proposed strategy.
1	Introduction
One of the most fundamental and well-studied problems in optimal control, Linear-Quadratic Reg-
ulation (LQR) has recently aroused renewed interest in the context of data-driven control and rein-
forcement learning. Considering it is usually challenging to obtain an exact system model from first
principles, and that the system may slowly change over time due to various reasons, e.g., component
wear-out, data-driven regulation of unknown linear systems has become an active research problem
in the intersection of machine learning and control, with recent works including e.g., Dean et al.
(2019); Mania et al. (2019); Cohen et al. (2019); Wagenmaker & Jamieson (2020). In particular,
from the perspective of reinforcement learning theory, the LQR problem has become a standard
benchmark for continuous control.
In this paper, we focus on the dual control (Feldbaum, 1960) setting, also known as online adaptive
control in the literature, where the same policy must be adopted to identify the system parameters
and optimize the control objective, leading to the well-known exploration-exploitation dilemma.
Recently, it was shown by SimchoWitz & Foster (2020) that the optimal regret for this problem set-
ting scales as Θ( √T), which can be achieved with probability 1 - δ using a certainty equivalent
control strategy, where the learner selects control inputs according to the optimal controller for the
current estimate of the system while injecting time-decaying exploration noise. However, the strat-
egy proposed in this work, like those in its predecessors (Abbasi-Yadkori & Szepesvari, 2011; Dean
et al., 2018; Mania et al., 2019), may have a nonzero probability δ of failing. Furthermore, it shall
be noticed that δ has been chosen as a fixed design parameter in the aforementioned works, which
implies the probability of failing does not converge to zero even if the policy is run indefinitely. The
above observation gives rise to the question that we address in this paper:
Can we design a learning scheme for LQR dual control, such that the policy
adopted in almost every trajectory converges to the optimal policy?
1
Under review as a conference paper at ICLR 2022
We identify that the above goal can hardly be achieved by a naive certainty equivalent learning
scheme. Qualitatively, the system parameters learned from data are always corrupted by random
noise, and as a result, the controller proposed in previous works may destabilize the system, albeit
with a small probability, causing catastrophic system failure. Based on the above reasoning, we
propose a notion of bounded-cost safety for the LQR dual control problem: we recognize a learning
scheme to be safe if no destabilizing control policy is applied during the entire learning process.
In this paper, we propose a learning scheme that satisfies the above definition of bounded-cost safety,
and guarantees both the parameter inference error and the suboptimality gap of controller perfor-
mance converge to zero almost surely. Our strategy consists of two parts: a safe switched controller
and a parameter inference algorithm. The switched controller can be viewed as a safety-augmented
version of the certainty equivalent controller: it normally selects control inputs according to the
optimal linear feedback for the currently estimated system parameters, but falls back to a conser-
vative controller for several steps when the actual state deviates significantly from the target state.
We prove that this switching strategy ensures the bounded-cost safety of the learning process, while
only inducing a suboptimality gap that decays exponentially as the switching threshold increases.
For the parameter inference part, in contrast to the direct least-squares approach for estimating the
matrices A, B widely adopted in the literature, we estimate the Markov parameters, also known as
the impulse response of the system, based on the cross-correlation between the exploration noise and
the system output, and establish the almost-sure convergence using a law of large numbers for mar-
tingales. We prefer this approach for its clarity in physical meaning and simplicity of convergence
analysis, but we do not foresee substantial difficulty in replacing our parameter inference module
with standard least-squares. We prove that under the above described learning scheme, the parame-
ter inference error scales as O(T -1/4+), while the suboptimality gap of control performance scales
as O(T -1/2+), where T is the number of time steps, and is an arbitrarily small positive number.
Both the above results match the corresponding asymptotic rates in Simchowitz & Foster (2020),
which provides an rate-optimal algorithm for online LQR in the high-probability regime.
The main contributions of this paper are as follows:
1.	We propose a practical notion of safety for the LQR dual control problem that has not been
considered in the literature, and provide an instance of safe learning scheme based on a
switching strategy.
2.	We prove almost sure convergence rates of parameter inference error and suboptimality
gap of control performance for our scheme, which match the corresponding optimal rates
in the high-probability regime. To the best of our knowledge, this is the first analysis of the
almost sure convergence rate for online LQR.
The rest of this paper is organized as follows: Section 2 gives a brief introduction of LQR, formulates
the LQR dual control problem, and defines the performance metrics as well as the notion of bounded-
cost safe learning scheme. Section 3 presents and interprets our algorithm. Section 4 states the main
theoretical results and characterizes the convergence rates. Section 5 provides simulation results
on an industrial process example to illustrate the effectiveness of our proposed strategy. Section 6
summarizes the related literature. Finally, Section 7 gives concluding remarks and discusses future
directions.
2	Problem Formulation
We consider the control of the following discrete-time linear system:
xk+1 = Axk + Buk + wk,	(1)
where xk ∈ Rn is the state vector, uk ∈ Rp is the input vector, and wk ∈ Rn is the process noise.
We assume x0 〜 N (0, X。) ,wk 〜 N (0, W), and that x0, w0, wι,... are pairwise independent.
We also assume w.l.o.g. that (A, B) is controllable.
We consider control policies of the form
∏ ：	Rn	X Rq	→	Rp	X	Rq,	(uk, ξk+ι) =	∏(χk, ξk),	(2)
which can be either deterministic or stochastic, where ξk ∈ Rq is the internal state of the policy.
Notice that we allow the flexibility of the policy being non-Markovian with the introduction of ξk,
2
Under review as a conference paper at ICLR 2022
and we also use the simplified notation uk = π(xk) if π is Markovian. The performance of a policy
π can be characterized by the infinite-horizon quadratic cost
Jπ = limsup 1E
T→∞ T
T-1
xk>Qxk + uk>Ruk
k=0
(3)
where Q 0, R 0 are known weight matrices specified by the system operator.
We denote the optimal cost and the optimal control law by
J *
inf Jπ ,π* ∈ arg min Jπ.
ππ
(4)
It is well-known that the optimal policy is a linear function of the state π* (x) = K*x, with associ-
ated cost J* = tr (WP*), where P* is the solution to the discrete-time algebraic Riccati equation
P * = Q + A>P *A — A>P *B (R + B>P *B)-1 B>P *A,	(5)
and the linear feedback control gain K * can be determined by
K * = - (R + B>PB)-1 B>P *A.	(6)
Based on the definitions above, we can use Jπ - J* to measure of the suboptimality gap ofa specific
policy π .
In the online LQR setting, the system and input matrices A, B are assumed unknown, and the learn-
ing process can be viewed as deploying a sequence of time-varying policies {πk} with the dual
objectives of exploring the system parameters and stabilizing the system. To characterize the safety
of a learning process, we make the definitions below:
Definition 1. A policy π is destabilizing if Jπ = +∞.
Definition 2. A learning process applying policies {πk}k∞=0 is bounded-cost safe, ifπk is not desta-
bilizing for any time k and for any realization of the noise process.
Notice that Definition 1 is a generalization of the common notion of destabilizing linear feedback
gain, i.e., π(x) = Kx is destabilizing when ρ(A + BK) ≥ 1, which is equivalent to Jπ = +∞.
Based on the notion of destabilizing policies, we propose the concept of bounded-cost safety in
Definition 2, which requires that destabilizing policies, or policies with unbounded cost, are never
applied. It should be pointed out that bounded-cost safety does not guarantee the stability of trajec-
tories, but is an indicator of the reliability of a learning scheme.
We assume the system is open-loop strictly stable, i.e., ρ(A) < 1. Indeed, if there is a known
stabilizing linear feedback gain K0, then we can apply the dual control scheme on the pre-stabilized
system (A + BK0, B) instead of (A, B). Existence of such a known stabilizing linear feedback
gain a standard assumption in previous works on online LQR (Mania et al., 2019; Simchowitz &
Foster, 2020), and relatively easy to establish through coarse system identification (Dean et al., 2019;
Faradonbeh et al., 2018b) or adaptive stabilization methods (Faradonbeh et al., 2018a; 2019).
3	Algorithm
The complete algorithm we propose for LQR dual control is presented in Algorithm 1. The modules
in the algorithm will be described later in this section.
3.1	Safe control policy
This subsection describes the policy for determining the control input uk . The first n + p steps are
a warm-up period where purely random inputs are injected. Afterwards, in each step, we inject a
exploitation term U plus a polynomial-decaying exploratory noise (k + 1)-βZ, where the decay rate
β ∈ (0,1/2) is a constant. The exploitation term U is a modified version of the certainty equivalent
control input Kk Xk, and this modification, described in Algorithm 2, is crucial to the safety of the
learning process, which we will detail below.
3
Under review as a conference paper at ICLR 2022
Algorithm 1 Safe LQR dual control
Input: State dimension n, input dimension p, exploratory noise decay rate β
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
for k = 0, 1, . . . , n + p - 1 do
ξk+1 J 0
Apply control input Uk J (k + 1)-βZk, where Zk 〜N (0, Ip)
for k = n + p, n + p + 1, . . . do
Observe the current state xk
for τ = 0, 1, . . . , n + p - 1 do
k	τ-1
Hk,τJk⅛ X (i-τ)β Xi- X Hk,tUi-t-ι ZL-I
i=τ +1	t=0
Reconstruct Ak, Bk from Hk,0, . . . , Hk,n+p-1 using Algorithm 3
Compute certainty equivalent feedback gain Kk by replacing A, B with Ak, Bk in (5),(6)
Determine policy ∏k(∙, ∙) J π(∙, ∙; k, KKk,β), where π is described by Algorithm 2
(Uk, ξk+ι) J ∏k(xk, ξk); record Uk J U, ζk J Z, where U, Z are the corresponding vari-
ables generated when executing the policy
Apply control input Uk
Algorithm 2 Safe policy π(x, ξ; k, K, β)
Input: Arguments: system state x, policy internal state ξ; Parameters: step k, linear feedback gain
K , exploratory noise decay rate β
Output: Control input U and next policy internal state ξ0, i.e., (U, ξ0) = π(x, ξ; k, K, β)
1:	if ξ > 0 then
2:	U J 0,ξ0 J ξ - 1
3:	else
4:	if max{kK k , kxk} ≥ log k then
5:	U J 0, ξ0 J [log k∖
6:	else
7:	U J Kx, ξ0 J 0
8:	U J U + (k + 1)-βZ, where Z 〜N (0,I)
In short, we stop injecting the exploitation input for blog k∖ + 1 consecutive steps, if either the state
norm kxk k or the norm of the feedback gain kKk k exceeds the threshold log k. Recall from (2) that
we use ξk to denote the internal state of the policy, and in Algorithm 1, ξk is a counter that records
how many steps are left in the “non-action” period. Essentially, we utilize the innate stability of
the system to prevent the state from exploding catastrophically. This “non-action” mechanism is a
critical feature of our control design, without which the controller learned from data may destabilize
the system, albeit with a small probability, causing system failure in practice and forbidding the
establishment of almost sure performance guarantees in theory. We provide an ablation study of this
“non-action” mechanism in Section 5.
We choose both the switching threshold and the length of the “non-action” period to be time-
growing. The enlarging threshold corresponds to diminishing degree of conservativeness, which
is essential for the policy performance to converge to the optimal performance. Meanwhile, the pro-
longing “non-action” period rules out the potential oscillation of state caused by the frequent switch-
ing of the controller (see Appendix B for an illustrative example of this oscillation phenomenon).
In particular, it can be shown that the suboptimality gap incurred by the switching strategy scales
as O(tM exp(-cM 2)), where M is the switching threshold, t is the length of the “non-action” pe-
riod, and c is a system-dependent constant (see Lemma 10 in Appendix A.1.2). With both M and t
growing as O(log k), the contribution of our switching strategy to the overall suboptimality gap is
merely O ⑴.
4
Under review as a conference paper at ICLR 2022
3.2	Parameter inference
The Markov parameters of the system described in (1) are defined as
Hτ , AτB, τ = 0, 1, ............................... (7)
The Markov parameter sequence {Hτ}τ∞=0 can be interpreted as the impulse response of the system.
It shall be noted that finite terms of the Markov parameter sequence would suffice to characterize
the system, since higher-order Markov parameters can be represented as the linear combination
of lower-order Markov parameters using the characteristic polynomial of A in combination with
Cayley-Hamilton theorem. In particular, our inference algorithm estimates the first n + p terms
of the impulse response. We denote our estimate of Hτ at step k by Hk,τ , whose expression is
specified in line 7 of Algorithm 1. Based on the cross-correlation between the current state xk and a
past exploratory noise input ζk-τ-1, we can produce an unbiased estimate of Hτ at each step, and
HHk,τ is a cumulative average of such unbiased estimates, whose almost-sure convergence to HT can
be established through a law of large numbers for martingales (see Appendix A.1.3).
Remark 1. From a computational perspective, line 7 of Algorithm 1 can be decomposed to enable
efficient recursive updates. To see this, we can rearrange this expression as
k	τ-1	k
Hk,τ =占 X (i-τ)βXiZi-T-1 -XHk,t 占 X (i-τ)βUl-h ,
i=τ +1	t=0	i=τ +1
the weighted sum ofτ + 1 cumulative averages, each of which can be updated with a constant
amount of computation in each step. Therefore, the time complexity of each iteration and the total
memory consumption are constant.
From the inferred Markov parameters {H七丁}n+P-1, We can reconstruct Ak, Bk by fitting the data
obtained from several rollouts on the virtual system described by {Hk,τ}n+P-1. The procedure is
detailed in Algorithm 3.
Algorithm 3 Restoring system and input matrices from Markov parameters
Tr	Λ Tt r 1	，	. ∙	τ'τ τ'τ	TT	1	i'	1,1,	,
Input: Markov parameter estimate sequence H0, H1, . . . , Hn+p-1, number of simulated trajecto-
ries N
Output: System matrices estimate A, B
	0	0	0	...0
	Ho A	0 A	0	...0
1: Construct block Toeplitz matrix T J	H i .	Ho .	0 .	...0 .	.
	. . _Hn+p-i	. . ^ Hn+p-2	. . Hn+p-3	.. . . ...0
2: Choose N independent random input trajectories, each (n + p)-long, denoted by Ui -
ui(i)	(2) Ui	∙∙	• UiN)],	i=0,1,..	.,n+p- 1.
3: Stack	the	inputs	vertically	and compute states:	Uv	J
[uo； Ui；…；Un+p-l] , [Xi； X2；…；Xn+p] =： Xv J TUV
4: Stack the inputs and states horizontally to form the following matrices:
Uh	J	[U0	Ui	…Un+p-i] , Xh	J	[Xi	X2	…Xn+p] , Xih	J
[0n×N x1	∙∙∙ xn+p-1]
5: Compute estimate [B A] J Xh [Uh; X0l]t.
It can be shown the procedure described in Algorithm 3 restores the actual system and input matrices
as long as the Markov parameter estimates are accurate:
Uh
Lemma 1. When Hτ = Hτ = AτB for allτ = 0, 1, . . . , n + p - 1, and the matrix h defined
X0
in Algorithm 3 has full row rank, the result of Algorithm 3 satisfies A = A, B = B.
5
Under review as a conference paper at ICLR 2022
Proof. When HT = AτB, We have Xh = AX0h + BUh = [B A]
.	.	「U h] Uh h]t	ʃ . .	„ e ,,“Uhh
rank, We have h h = I, and therefore [B A] = X1h	h
h . Since h has full roW
X0	X0
t
=[B A].	□
Remark 2. Lemma 1 guarantees the consistency of our controller, i.e., accurate Markov parameter
estimates would generate an optimal exploitation input. The full-row-rank condition in Lemma 1,
naturally satisfied by randomly generated inputs, guarantees that the inputs provide sufficient ex-
citation to reconstruct system matrices from impulse responses. For the ease of analysis, it is also
legitimate to use fixed, rather than random ui ’s for each time step k of the outer loop of Algorithm 1,
as long as the rank condition is satisfied.
4 Main results
The main theoretical properties of our proposed LQR dual control scheme are stated as folloWs:
Theorem 1.	Assuming A is stable, the learning process described in Algorithm 1 is bounded-cost
safe according to Definition 2.
Theorem 2.	Assuming A is stable, and 0 < β < 1/2, for HHk,τ computed in Algorithm 1, the
following limit holds almost surely, i.e., with probability 1:
lim
k→∞
Hk,τ - HT
k-Y+e
(8)
where γ = 1/2 - β > 0, for any > 0 and any τ = 0, 1, . . . , n + p - 1.
Remark 3. Theorem 2 states that our estimate of the Markov parameters converge at the order
O (k-γ+). As a corollary, our estimate of the system and input matrices A, B also converge at
O (k-γ+) (see Appendix A.4). This convergence rate coincides with the one guaranteed by the
more commonly used least-squares method, for which one can establish the convergence of esti-
mates ofA, Bat O (k-γ+) using concentration bounds for martingale least-squares (e.g., Lemma
E.1 in Simchowitz & Foster (2020)). Therefore, our parameter inference algorithm based on cross-
correlation can be viewed as a competitive alternative to the least-squares estimator. One feature
of our cross-correlation approach compared to the least-squares approach, however, is that the es-
timation of Markov parameters can be easily extended to the partially observable LQR setting like
the one considered in Zheng et al. (2020). Also, the convergence of our cross-correlation approach
is not based on the sub-Gaussianity of the process noise (see Appendix A.3), which allows straight-
forward generalization to long-tailed noise models.
Theorem 3.	Assuming A is stable, and 0 < β < 1/2, let πk be the control policy used at step k in
Algorithm 1, then with Jπk and J* defined in (3), (4) respectively, for any e > 0, thefollowing limit
holds almost surely, i.e., with probability 1:
Jπk - J*
k→m∞ k- min(2β,2γ)+e = 0,
(9)
where γ = 1/2 - β > 0.
Due to space limits, all the proofs are deferred to the appendix.
Remark 4. According to Theorem 2, the convergence rate γ is maximized when β → 0+. How-
ever, the exploration term k-β does not decay in this case, and the control performance Jπk
will not converge to the optimal performance J*. To achieve the fastest convergence of Jπk,
we need to choose the decay rate of the exploration term to be β = 1/4, which maximizes
min(2β, 2γ) = min(2β, 1 - 2β). When β = 1/4, we have both parameter estimation errors and
policy SUbOPtimaIity gaps scaling at O(1∕√T), which matches the asymptotic rates of correspond-
ing components in the (with-high-probability) regret-optimal algorithm proposed in Simchowitz &
Foster (2020). However, due to the challenging nature of the analysis of the nonlinear closed-loop
system under our safe control scheme, the exact regret of our scheme is still under investigation.
6
Under review as a conference paper at ICLR 2022
5 Simulation
In this section, the performance of our proposed algorithm is evaluated using Tennessee Eastman
Process (TEP), an industrial process example for benchmarking linear controllers (Ricker, 1993).
We used a simplified version of TEP from Liu et al. (2020), where the state and input dimensions
are n = 8, p = 4, and the spectral radius of the system matrix is ρ(A) ≈ 0.96. We assume
Q, R, are identity matrices, and W, X0 are also identity matrices, i.e., the process noise is i.i.d.
standard Gaussian. We choose N = 50 (in Algorithm 3) for all the experiments. In the practical
•	1	,	.1	, ∙	,	1	,	, ∙	A -f∖	1 ,1 i' 11	1	∙	T7-	F , F 1
implementation, the estimated system matrices Ak , Bk and the feedback gain Kk are updated only
at steps b10n/2c(n ∈ N) to speed up the computation.
To illustrate the impact of the parameter β on the convergence of the algorithm, we perform 100
independent experiments for each of β ∈ {0, 1/4, 1/2}, with 108 steps in each experiment. Fig. 1
shows the error of the estimated system and input matrices, i.e.』Ak - A∣∣ and IBk - B∣∣ against
the time k for different values of β .
Figure 1: Error of estimated system matrices for different β against time k. The solid lines are the
median among the experiments, and the shades represent the range among the experiments.
k
From Fig. 1, one can see when β is 0 or 1/4, the estimation error of the system matrices converges to
zero as time k goes to infinity, and the convergence approximately follows a power law. Furthermore,
the convergence speed of the estimation error is significantly faster when β = 0. Meanwhile, when
β = 1/2, the estimate error diverges in some of the experiments. The above observations are
consistent with the theoretical result in Theorem 2, where it is stated that the parameter estimates
converge with rate O (k-γ+), with γ = 1/2 -β, i.e., convergence is only guaranteed with β < 1/2
and is faster when β is smaller.
Now we consider the performance of policies with different values of β . To quantify the policy
performance, one shall in theory compute Jπk as defined in (3). However, the policies πk are
nonlinear, rendering it very difficult, if not impossible, to compute Jπk analytically. As a surrogate
approach for evaluating a policy πk, we define the empirical cost Jπk as
N T-1	>	>
Jnk= NN T XXgi)) Qxti) + (Uti)) Ru”
i=1 t=0
where x(ti), ut(i) are states and inputs collected from the closed-loop system under πk in N indepen-
dent T -long sample paths indexed by i. In our simulations, we take T = 10000, N = 10 to evaluate
each stabilizing policy, which yields consistent results in practice. Fig. 2 shows the empirical per-
formance of controllers with different values of β against time k .
From Fig. 2, one can observe that among our choices ofβ, only β = 1/4 drives the controller toward
the optimal one. For β = 0, although the parameter estimates converge the fastest as discussed
above, the performance of the resulting closed-loop system is even worse than the free system. This
is because the exploration term (k + 1)-βζk in the control input does not decay, and the resulting
controller is a noisy one. Meanwhile, for β = 1/2, diverging parameter estimates would lead to ill-
performing controllers. The observations are consistent with the theoretical conclusion indicated by
Theorem 3 that β = 1/4 corresponds to the optimal trade-off between exploration and exploitation.
7
Under review as a conference paper at ICLR 2022
Figure 2: The empirical performance of controllers for different β against time k. For each policy
πk, the empirical cost Jπk is obtained by approximating the actual cost with random sampling from
the closed-loop system. The solid lines are the median among the experiments, and the shades
represent the interval between the first and third quantiles among the experiments. The purple and
brown dashed lines represent the analytical optimal cost, and the analytical cost of the control-free
system, respectively.
Finally, we compare our algorithm with the naive centainty equivalence algorithm, which applies
Uk = KKkXk+k-βZk at each step. We invoke both algorithms with the “optimal” parameter β = 1/4,
and perform 20 independent experiments for each algorithm. We consider the system parameter
estimation error and the policy suboptimality gap for each algorithm, as we did in Fig. 1 and Fig. 2.
The results of this comparison experiment are presented in Fig. 3. In a proportion of the experiments
(actually 7 out of 20), certainty equivalence causes the estimation error and the policy suboptimality
gap to diverge, while our algorithm ensures convergence in each experiment. Although existing
works (e.g. Simchowitz & Foster (2020)) contain methods for decreasing the failure probability with
a warm-up period for estimation, it should be noted that the failure probability is always nonzero as
long as a linear feedback policy with learned gain is deployed. Even compared to the case where
the certainty equivalence algorithm converges, our algorithm still delivers slightly lower estimation
error and significantly better policy performance, which indicates that the conservativeness caused
by our switching mechanism does not harm the long-term performance.
10-101 102—103—104—105 106—107—108
k
101 ~102~103~104~105~106~107^^108
101 ~102~103~104~105~106~107~108
k
Figure 3: Comparison between our proposed algorithm (Safe) and the certainty equivalence algo-
rithm (CE), in terms of estimation error and policy performance. In a proportion of the experiments,
CE causes the estimation error and the policy suboptimality gap to diverge, while our algorithm
ensures convergence in each experiment.
6	Related Works
The concept of dual control was initiated by Feldbaum (1960), and since then sustained research ef-
forts have been devoted to learning to make decisions in unknown environments. Classical literature
on adaptive control, e.g., Astrom & Wittenmark (1973); Lai & Wei (1986); Guo & Chen (1988);
Guo (1996), have addressed the dual control of linear systems described by the ARMAX model
with the objective of tracking a reference signal. This objective do not consider the cost of exerting
control input and is less involved than the LQR setting. For the rest of this section, we restrict our
focus to the LQR of unknown linear systems described by the state-space model.
8
Under review as a conference paper at ICLR 2022
In the offline learning regime, system identification and controller synthesis based on the estimated
system are performed in separate stages. The study of linear system identification has been devel-
oped for decades, with classical asymptotic guarantees summarized in Ljung (1999), and there has
been a recent interest in non-asymptotic system identification based on a finite-size dataset. One can
generate the dataset either by injecting random inputs to the system (Oymak & Ozay, 2019; Zheng
& Li, 2020), or using active learning techniques to achieve optimal system-dependent finite-time
rate (Wagenmaker & Jamieson, 2020). For a summary of recent results on non-asymptotic linear
system identification, please refer to Zheng & Li (2020).
A natural step following system identification is to synthesize a controller based on the identification
result and analyze the resulting closed-loop system’s performance. The Coarse-ID control frame-
work (Dean et al., 2019) adopts robust control techniques to derive controllers taking identification
error into account, resulting in high-probability bounds on the control performance suboptimality
gap. This framework has been applied to LQR (Dean et al., 2019), SISO system with output feed-
back (Boczar et al., 2018), and LQG (Zheng et al., 2020). As an alternative approach, the certainty
equivalence framework synthesizes the controller by treating the identified system as the truth (Ma-
nia et al., 2019; Tsiamis et al., 2020), which is shown to achieve a faster convergence rate of control
performance, at the cost of higher sensitivity to poorly estimated system parameters. At the intersec-
tion of the above two methods, an optimistic robust framework was proposed to achieve a combina-
tion of fast convergence rate and robustness (Umenberger & Schon, 2θ2θ). It should be pointed out
the offline algorithms in the above works and the online setting we consider in the present work are
not mutually exclusive but complementary to each other: online algorithms can continually refine
the controller obtained by offline methods without interrupting normal system operation.
The problem of designing controllers that improve over time has been studied in the context of on-
line LQR and regret analysis. The online LQR setting was initially proposed by Abbasi-Yadkori &
Szepesvari (2011), and since then, Optimism in the Face of Uncertainty (OfU), Thompson Sam-
pling (TS) and ε-greedy exploration have been applied to solve the online LQR problem. Abbasi-
Yadkori & Szepesvari (2011) applied OFU principle to prove a theoretical O(√T) regret upper
bound, but their method is computationally intractable. Cohen et al. (2019) proposed a tractable al-
gorithm for OFU based on semidefinite programming to realize O( √T) regret. TS has been shown
to achieve O(√T) (frequentist) regret for scalar system (Abeille & Lazaric, 2018), and O(√T) ex-
pected regret in a Beyesian setting (Ouyang et al., 2017). Recently, Faradonbeh et al. (2020a) and
Mania et al. (2019) prove that randomized Certainty Equivalent (CE) control with ε-greedy explo-
ration can also achieve O( √T) regret, and Simchowitz & Foster (2020) prove that O( √T) regret is
indeed the fundamental limit for the general online LQR setting. Under further assumptions, e.g.,
partially known system parameters, logarithm regret bound may be achieved (Cassel et al., 2020).
The aforementioned results and the fundamental limit hold either with high probability or in ex-
pectation, and lack almost sure guarantee. Faradonbeh et al. (2020b) prove that O( √T) regret for
both TS and CE and hold almost surely, but under the restrictive assumption that the closed-loop
system remains stable when the policy is being employed. Our work attempts to fill in the gap by
performing an almost sure analysis for online LQR without assumptions on the system other than
the existence a known initial stabilizer.
7	Conclusion
In this paper, we propose a safe LQR dual control scheme based on a switching, which guarantees
almost sure convergence to zero of both parameter inference error and suboptimality gap of control
performance. The convergence rate of inference error is O(T -1/4+), while that of the suboptimal-
ity gap is O(T -1/2+), both of which match the known optimal rates proved in the non-asymptotic
setting. For future works, we plan to extend the notion of regret to our almost-sure convergence
setting, and formally establish the fundamental limits for this setting. It would also be interest-
ing to investigate the exact convergence rates, especially dimension dependence, of our parameter
inference scheme and the standard least-squares procedure.
9
Under review as a conference paper at ICLR 2022
References
Yasin Abbasi-Yadkori and Csaba Szepesvari. Regret bounds for the adaptive control of linear
quadratic systems. In Proceedings of the 24th Annual Conference on Learning Theory, pp. 1-
26. JMLR Workshop and Conference Proceedings, 2011.
Marc Abeille and Alessandro Lazaric. Improved regret bounds for thompson sampling in linear
quadratic control problems. In International Conference on Machine Learning, pp. 1-9. PMLR,
2018.
Karl Johan AStrom and Bjorn Wittenmark. On self tuning regulators. Automatica, 9(2):185-199,
1973.
Ross Boczar, Nikolai Matni, and Benjamin Recht. Finite-data performance guarantees for the
output-feedback control ofan unknown system. In 2018 IEEE Conference on Decision and Con-
trol (CDC), pp. 2994-2999. IEEE, 2018.
Asaf Cassel, Alon Cohen, and Tomer Koren. Logarithmic regret for learning linear quadratic reg-
ulators efficiently. In International Conference on Machine Learning, pp. 1328-1337. PMLR,
2020.
Alon Cohen, Tomer Koren, and Yishay Mansour. Learning linear-quadratic regulators efficiently
with only √T regret. In International Conference on Machine Learning, pp. 1300-1309. PMLR,
2019.
Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. Regret bounds for
robust adaptive control of the linear quadratic regulator. arXiv preprint arXiv:1805.09388, 2018.
Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. On the sample com-
plexity of the linear quadratic regulator. Foundations of Computational Mathematics, pp. 1-47,
2019.
Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. Finite-time adap-
tive stabilization of linear systems. IEEE Transactions on Automatic Control, 64(8):3498-3505,
2018a.
Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. Finite time identifi-
cation in unstable linear systems. Automatica, 96:342-353, 2018b.
Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. Randomized al-
gorithms for data-driven stabilization of stochastic linear systems. In 2019 IEEE Data Science
Workshop (DSW), pp. 170-174. IEEE, 2019.
Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. Input perturbations
for adaptive control and learning. Automatica, 117:108950, 2020a.
Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. On adaptive linear-
quadratic regulators. Automatica, 117:108982, 2020b.
AA Feldbaum. Dual control theory. i. Avtomatika i Telemekhanika, 21(9):1240-1249, 1960.
L Guo and HF Chen. Convergence rate of els based adaptive tracker. Syst. Sci & Math. Sci, 1:
131-138, 1988.
Lei Guo. Self-convergence of weighted least-squares with applications to stochastic adaptive con-
trol. IEEE transactions on automatic control, 41(1):79-89, 1996.
Tze Lai and Ching-Zong Wei. Extended least squares and their applications to adaptive control and
prediction in linear systems. IEEE Transactions on Automatic Control, 31(10):898-906, 1986.
Hanxiao Liu, Yilin Mo, Jiaqi Yan, Lihua Xie, and Karl H. Johansson. An online approach to physical
watermark design. IEEE Transactions on Automatic Control, 65(9):3895-3902, Sep 2020. ISSN
0018-9286, 1558-2523, 2334-3303. doi: 10.1109/TAC.2020.2971994.
10
Under review as a conference paper at ICLR 2022
Lennart Ljung. System identification. Wiley encyclopedia of electrical and electronics engineering,
pp.1-19,1999.
Horia Mania, Stephen Tu, and Benjamin Recht. Certainty equivalence is efficient for linear quadratic
control. arXiv preprint arXiv:1902.07826, 2019.
Yi Ouyang, Mukul Gagrani, and Rahul Jain. Learning-based control of unknown linear systems
with thompson sampling. arXiv preprint arXiv:1709.04047, 2017.
Samet Oymak and Necmiye Ozay. Non-asymptotic identification of lti systems from a single trajec-
tory. In 2019 American control conference (ACC), pp. 5655-5661. IEEE, 2019.
N Lawrence Ricker. Model predictive control of a continuous, nonlinear, two-phase reactor. Journal
of Process Control, 3(2):109-123, 1993.
Max Simchowitz and Dylan Foster. Naive exploration is optimal for online lqr. In International
Conference on Machine Learning, pp. 8937-8948. PMLR, 2020.
Anastasios Tsiamis, Nikolai Matni, and George Pappas. Sample complexity of kalman filtering for
unknown systems. In Learning for Dynamics and Control, pp. 435-444. PMLR, 2020.
Jack Umenberger and Thomas B Schon. Optimistic robust linear quadratic dual control. In Learning
for Dynamics and Control, pp. 550-560. PMLR, 2020.
Andrew Wagenmaker and Kevin Jamieson. Active learning for identification of linear dynamical
systems. In Conference on Learning Theory, pp. 3487-3582. PMLR, 2020.
Yang Zheng and Na Li. Non-asymptotic identification of linear dynamical systems using multiple
trajectories. IEEE Control Systems Letters, 5(5):1693-1698, 2020.
Yang Zheng, Luca Furieri, Maryam Kamgarpour, and Na Li. Sample complexity of lqg control for
output feedback systems. arXiv preprint arXiv:2011.09929, 2020.
11
Under review as a conference paper at ICLR 2022
A Proof of main results
A.1 Preliminaries for proofs
A.1.1 Notations
We will use the following notations throughout the proofs:
Notation	Definition
X>	transpose of matrix X
∣X ∣ , ∣X∣	norm of matrix X or vector X, 2-norm by default
∣X ∣F	Frobenius norm of matrix X
tr(X)	trace of matrix X
ρ(X)	spectral radius of matrix X
X0	matrix X is positive definite
X Y Y	for matrix X, Y , Y 一 X	0
λmin(X)	minimum eigenvalue of matrix X
κ(X)	ratio between maximum and minimum eigenvalues for ma- trix X	0, i.e., κ(X) = ∣X∣ /λmin(X)
vec(X)	vectorization of matrix X
X % Y	Kronecker product of matrices X and Y
1S	indicator function of set S
f (x)〜O(g(X))	there exists M > 0, such that |f (k)| ≤ M × g(k) for all k∈N
f (x)〜O(g(χ))	there exists M > 0 and n ∈ N, such that |f(k)| ≤ M × g(k) × log(k)n for all k ∈ N
In addition, we define the following short-hand notation for characterizing the almost-sure asymp-
totic convergence or growth rate of random processes: for a random variable (vector, or matrix)
sequence {xk }, We denote that Xk 〜C(α) if for all e > 0 and almost all realizations of randomness,
We have Xk 〜 O (kα+e), i.e., limk→∞ ∣∣Xk ∣∣ /kα+e a=. 0. The following lemma establishes some
basic properties of C(α) functions:
Lemma 2. Assume Xk , yk are random variable (vector, or matrix) sequences with proper dimen-
sions, then the following properties hold:
1.	If Xk 〜C(α), yk 〜C(β) ,then Xk + y 〜C (max(α, β)).
2.	If Xk 〜C(α'),yk 〜C(β) ,then Xk y 〜C(α + β).
3.	If f is a function differentiable at0 and Xk 〜C(α), α < 0, then f (Xk) 一 f (0)〜C(α).
Proof. The first tWo claims folloW trivially from the definition of C (α). For the third claim, notice
that for all realizations of randomness, limk→∞ ∣Xk ∣ = 0. Therefore, Taylor expansion of f(X)
at X = 0 gives f (Xk) 一 f (0) = Df(0)Xk + O(IlXk∣∣2), where Df is the Frechet derivative of f.
Dividing both sides by kα+e leads to the conclusion.	□
A.1.2 Properties of a class of switched linear systems
We state and prove some properties of a class of switched linear systems that will be useful for the
proof of all the main theorems of this paper.
Consider the dynamical system
Xk+1 = AkXk + wk,	(10)
12
Under review as a conference paper at ICLR 2022
where Xk ,wk ∈ Rn, xo ∈ N (0,W-ι) ,wk 〜 N (0,Wk), χ0,{wk | xo：k } are pairwise indepen-
dent, and Ak, Wk are functions of x0:k and k. We assume
Wk	0,kAkk ≤ A,kWkk ≤ W, ∀k.	(11)
We first prove a few miscellaneous lemmas that will be useful shortly:
Lemma 3. Let X 〜χ2(p) ,then P(X ≥ M) ≤ 2p/2 exp(-M/4) for any M > 0.
Proof. Applying the Chernoff bound, we have
P(X ≥ M) ≤ E etX /etM = (1 - 2t)-p/2 exp(-tM)
for any 0 < t < 1/2. Choosing t = 1/4 leads to the conclusion.	□
Lemma 4. Let W ∈ Rp, W 〜N (0, W), W > 0, then P(∣∣w∣∣ ≥ M) ≤ 2p/2 exp (4-Mk).
Proof. Let V = W-1/2w, then V 〜N(0, I) and hence ∣∣vk2 〜 χ2 (p). From kWk2 = W > W =
v>Wv ≤ IlVIl2 ∣∣Wk, we have P(IlWk ≥ M) = P(IHI2 ≥ M2) ≤ P(∣v∣2 ≥ M2/ IlWk).
Applying Lemma 3 leads to the conclusion.	□
Lemma 5. Let 0 < x ≤ 1/2 and 1 < α < 2 ,then P∞=o Xan < 2x∕(α — 1).
Proof. By α > 1, we have αn - 1 = (1 +α - 1)n - 1 > n(α - 1). Therefore, in view of0 < x < 1,
we have
∞∞	∞
Xxαn=xXxαn-1<xXx(α-1)n
n=0	n=0	n=0
x
1 - XaT
In view of the inequality (1+t)r ≤ 1+rt fort ≥ -1, 0 ≤ r ≤ 1, we have xα-1 = (1+x- 1)α-1 ≤
1 + (α - 1)(X - 1), and therefore,
X	X	2X
1 — xa-1 - (α — 1)(1 — x) - α — 1
when 0 < x ≤ 1/2, and the conclusion follows.	□
Now we state several properties of the system described in (10)-(11):
Lemma 6. For the system described in (10)-(11), assuming there exists 0<ρ<1and P 0 such
that A>PAk Y PP for every Ak, then when M ≥ p3WK(P)/(1 — ρ1/4), there is
P (IxkI ≥ M) ≤
2n/2+1	( (1 — P1/4)2M2 ∖
P-1/2—I %——4WK(P))
∀k.
Proof. From (10), we have
xk = Ak-1xk-1 + Wk-1
= Ak-1(Ak-2xk-2 + Wk-2) + Wk-1
=∙ ∙ ∙
=wk-1 + Ak-IWk-2 + …+ Ak-IAk-2 …AIWO + Ak-IAk-2 …A0x0.
Pre-multiplying by P1/2 on both sides and applying the triangle inequality, we have
∣∣p 1/2xk|| ≤ ∣∣p 1/2Wk-i|| + ∣∣p 1/2Ak-rWk-2|| +—+ ∣∣p 1/2Ak-i ∙ ∙ ∙ Aiwo∣∣ +
UP1/2Ak-1 …AOx0 ∣∣ .	(12)
13
Under review as a conference paper at ICLR 2022
From A>PAk Y PP, We have for any W ∈ Rn,k ∈ N,
w>A>PAkW < ρw>Pw ⇒ ∣∣P1/2Akw∣∣ < ρ1/2 ∣∣P 1/2w|| .	(13)
Applying (13) to (12) recursively, We have
∣∣p 1%∣∣ ≤ ∣∣p "wk-ι∣∣ + P1/2 ∣∣p "wι∣∣ + ••• + P(I)/21∣P "w0∣∣+Pk/21∣P 1%∣∣.
(14)
MeanWhile, We have
{kχk k ≥ M} = n∣∣P T/2P "χk∣∣ ≥ M o
⊆ {∣∣PTTIuP 1/2Xk ∣∣ ≥ Mo=(pλm⅛
={∣∣P1∕2Xk∣∣ ≥ M Pλmin(P) O .
Therefore, for any σ ∈ (P1/2, 1), in vieW of (14), We have
{kxk k ≥ M} ⊆ n∣∣P 14k∣∣ ≥ M Pλmin(P) O
⊆ n∣∣P"xk∣∣ ≥ MPλmin(P)(1 - σk+1)O
⊆ n ∣∣P 1/2wk-i ∣∣ + P1/2 ∣∣P 1/2wk-21∣ + ••• + P(I)/2 ∣∣P 1/2W01∣ + Pk/21∣P1/2X01∣ ≥
M√λmin(P)(1 - σk+1)}
⊆ n ∣∣P 1/2Wk-11∣ ≥ (1 - σ)M√λmin(P)O ∪ {p1/2 ∣∣P 1/2Wk-21∣ ≥ σ(1 - σ)M√λmin(P)} ∪
…∪ {P(kT)/2 ∣∣P 1^W0∣∣ ≥ σk-1(1 - σ)M√λmin(P)} ∪
{Pk/2 ∣∣P 1"∣ ≥ σk(1 - σ)M√λmin(P)}.
Taking the union bound, We have
P(kxkk ≥ M) ≤P (∣∣P 1∕2Wk-1∣∣ ≥ (1 - σ)M√λmin(P)) +
P (∣∣P 1/2wk-2 ∣∣ ≥ f)1/2 (1 - σ)M√λmin(P)) +
...+
P (∣∣p"2w0∣∣ ≥ (^1/2 )	(1 - σ)M√λmin (P)) +
p(∣∣p F ≥ (六)(1 - σ)M √λmuP)).
Since P1/2Wk | xo：k 〜N(0, P1/2WkP1/2), and IlWkk ≤ W, applying Lemma 4, We have
P (∣∣P 1/2Wk-1 ∣∣ ≥ (1 - σ)M √λmin(P) I
x0:k
≤ 2n/2 eχp(-(1-σ)2M2λmin(p))
≤ 八 4 ∣Wkk kPk	)
≤ 2n/2 exp
((1 - σ)2M2)
V-^4WK(P)).
(15)
Noticing that the RHS of (15) does not depend on x0:k, We have
P(∣∣P1∕2Wk-1∣∣ ≥ (1-σ)M√λm>)) ≤ 2n∕2exp (-(二K)(PMM
14
Under review as a conference paper at ICLR 2022
Similarly, we have
PqP AM ≥ ρ⅛ (IYM pλmin(P))≤eχp(-*WK(PM •彳
P
(1 - σ)M√λmin(P)
≤ eχp
(1 — σ)2M2
4W K(P)
P
-σ)M,λmin(P)
≤ eχp
(1 - σ)2M2
4W K(P)
Choose σ = ρ1/4, then σ2/ρ = ρ-1/2. Assuming w.l.o.g. that P ∈ (1/4,1), We have σ2/ρ
PT/2 ∈ (1, 2). When M ≥ ,3“K(P)/(1 一 ρ1/4), there is
2n/2 eχp
((1 - σ)2M2∖
V -^4W K(P) )
Applying Lemma 5 leads to the conclusion.
□
Lemma 7. For the system described in (10)-(11), assume there exists 0 < ρ < 1, P 0 and M > 0
such that A>PAk Y ρP as long as ∣∣xk ∣∣ ≥ M. Let Vk = x>Pxk, then
(M 2A2 + W )∣P ∣
EVk < ------：----------,	∀k.
1-ρ
Proof. We first derive a recursive bound on EVk: from (10), we have
Vk+1 = xk>+1Pxk+1 = xk>Ak>PAkxk + 2wk>PAkxk + wk>Pwk.	(16)
When ∣xk ∣ < M, we have xk>Ak>PAkxk ≤ M2A2 ∣P∣. Otherwise, by Ak>PAk Y ρP, we
have xk>Ak>PAkxk < ρxk>Pxk = ρVk. Therefore, in view of the fact that M2A2 ∣P∣ ≥ 0 and
ρVk ≥ 0, we have
xk>Ak>PAkxk ≤ maχ{M2 A2 ∣P∣ , ρVk ≥ 0} ≤ M2A2 ∣P∣ + ρVk,	(17)
which holds for any k. Substituting (17) into (16), we have
Vk+1 ≤ ρVk + ηk + C,	(18)
where ηk = 2wk> PAk xk + wk> Pwk, C = M2A2 ∣P∣. With such defined ηk, we have
Eηk ≤ W ∣P∣ ,	(19)
where we noticed that E[wk>PAkxk] = E[E[wk>PAkxk | x0:k]] = E[E[wk | x0:k]>Akxk] = 0, and
that E[wk>Pwk] = E[E[wk>Pwk | x0:k]] = E[tr(Wk P)] ≤ W ∣P∣. Taking the expectation on both
sides of (18), we have
EVk+1 ≤ρEVk+(M2A2+W)∣P∣ .	(20)
We can proceed using induction: we have
EV0 = E[x>Px0] = E[tr(W-1P)] ≤ W ∣∣P∣∣ < (M2A2 + W) kPk.
0	1-ρ
Assuming EVk < (M2Aj+W)kPk, it follows from (20) that EVk+ι < (M2A2+W)kPk, which con-
cludes our proof.	□
Lemma 8. For the system described in (10)-(11), assume that there exists 0 < ρ < 1, P 0 and
M > 0 such that Ak>PAk Y ρP as long as ∣xk∣ ≥ M, and that Ak only depends on ∣xk ∣. Let
Vk = xk>Pxk, then
EV2 < (M2A2 + W)kPk2 [(1 + P)(M2A2 + W)+ 4A2Wκ(P)] ,	∀k.
k	(1 - ρ)(1 - ρ2)
15
Under review as a conference paper at ICLR 2022
Proof. From the inequality (18) in the proof of Lemma 7, we have
EVk2+1 ≤ρ2EVk2 + Eηk2 + C2 + 2ρE[Vk ηk] + 2ρC EVk + 2CEηk,	(21)
where ηk = 2wk>PAkxk + wk>Pwk,C = M2A2 kPk. Let us bound Eηk,EVk,Eηk2,E[Vkηk]
which appear in the RHS of (21) respectively:
•	Eηk ≤ W kP k from (19) in the proof of Lemma 7.
•	EVk < (C + W kP k)/(1 - ρ) according to the conclusion of Lemma 7.
•	Eηk2 = 4E[xk>Ak>Pwkwk>PAkxk] + 4E[wk>PAkxkwk>Pwk] + E[wk>Pwkwk>Pwk] ≤
4E kxkk2 A 2W kP k2 + W 2 kP k2, where we noticed E[wk>PAkxkwk>Pwk]	=
E[E[w>PAkXkw>Pwk | Xk]] = tr {E[AkXkE[w>Pwkw>P | Xk]]} = 0 due to
symmetry. From Vk = xk> Pxk ≥ λmin (P) kxk k2, we can further deduce Eηk2 ≤
4A2W kP k κ(P)EVk +W2 kPk2.
•	E[Vkηk]	= 2E[wk>PAkVk] + E[Vkwk>Pwk] = 2E[E[wk> P Ak Xk Vk | Xk]] +
E[E[vkwk>Pwk | Xk]] = 2E[E[wk | Xk]>PAkXkVk] + E[VkE[wk>Pwk | Xk]] ≤
W kPk EVk.
Substituting the above terms into the RHS of (21), we have
EV2+1 < P2EV2 + (4A2W kPk K(P) + 2ρ(C + W ∣∣P∣∣)) C + W kPk +
1-ρ
C2 + 2CW ∣P∣ + W2 ∣P∣2
=ρ2EV2 + C'WPPk [(1+ P)(C + W ∣P∣) + 4A2W ∣P∣ κ(P)] .	(22)
We can then proceed using induction: we have
EV02 ≤ W2 kPk2 = (W kPk)(W kPk)
<	C + W kpk (C + W ∣pk)
1-P
<	(11 W-P2)[(1 + P)(C + W kP k) + 4A 2W kP k K(P)].
Assuming EVk <	(1CPWI-P2)[(1 + P)(C + W ∣∣Pk)+4A2W ∣∣Pk κ(P)], We also have
EVk+1 < (C+W1-p2) [(1+ p)(C + W kPk) + 4A2W ∣∣Pk κ(P)] by (22). Therefore, wehave
E	% < (IC_； W-P2)[(1 + P)(C + W kP k) + 4A 2W kP k κ(P)]
=(M2A2 + W) kP『[(1 + P)(M2A2 + W) + 4A2Wκ(P)]
(1 - P)(1 -P2)
for any k.	□
A linear dynamical system (1) driven by our safe switching policy can be viewed as an instance of
the system described in (10)-(11). Formally, we consider the following system:
Xk+1 = AXk + Buk + wk,	(23)
We consider the following two classes of policies:
•	Linear feedback policy: uk = πK(Xk) = KXk.
•	Safe switching policy: (Uk,ξk+ι) = πK,M,t(xk, ξk), where M > 0 is the switching thresh-
old and t ∈ N* is the “non-action" duration, and ∏K,M,t determines Uk, ξk+ι as follows:
-	If ξk > 0, then Uk = 0, ξk+ι = ξk - 1;
16
Under review as a conference paper at ICLR 2022
-	If ξk = 0, max{∣∣Kk, ∣∣xkk} ≥ M, then Uk = 0,ξk+ι = t - 1;
-	If ξk = 0, max{kKk, kxkk} < M, then Uk = Kxk,ξk+ι = 0.
Notably, the linear feedback policy can be viewed as a special case of the safe switching
policy where the switching threshold is infinity, i.e., πκ = ∏κ,十∞,1.
For the simplicity of expressions, when a safe switching policy ∏Kk,Mk,tk is applied at each SteP k,
let us define the following notations:
i	(O)= 0,i(k + I)= (i(k) + ɪ	Ui(k) = 0 ,xk = xi(k).	(24)
i(k) + ti(k) Ui(k) = 0
In plain words, {xk} = {xi(k)} is the subsequence of {xk} where “non-action" steps are skipped.
In addition, let
W = ILnm W + AWA> + …+ AtW(At)>∣∣,	(25)
which is a finite value since A is stable.
Lemma 9. For the system described in (23)-(25), assume a safe switching policy πKk ,Mk ,tk is
applied at each step k, Mk ≤ M for any k, and there exists 0 < ρ < 1 and P 0 such that
A>PA Y ρP, then
Ekxkk4<8Q(M,ρ,P)+W2κ(P)2,
where
Q(M,P,P)= (M2A2 + W)kP『[(1+ P)(M2A2 + W)+4A2Wκ(P)],
(1 - ρ)(1 - ρ2)
A=kAk+kBkM.
Proof. According to (23) and (24), the state trajectory {xk } evolves subject to the dynamics
xk+ι = Akxk + Wk,	(26)
where
(A	maχ{kKk∣∣, kxk∣∣} ≥ M
A + BKk otherwise
W
Wk | xk 〜N(0,Wk) ,Wk = {w + AWA> + - . + Atk W (Atk )>
maχ{∣Kkk , kxk∣∣} ≥ M
otherwise
Let Vk = x>Pxk and Vk = x>Pxk. Since A>PA Y ρP, the system (26) satisfies the assumption
of Lemma 8, and kWkk ≤ W, kAkk ≤ kAk + kBk M = A. Therefore, and therefore,
eV2 < (M2A2 + IW) k2P∣2 [(1+ P)(M2A2 + W)+4A2Wκ(P)] = Q(M,P,P)λmm(P)2.
k (1 - ρ)(1 - ρ2)
(27)
Next, to establish the relationship between EV2 and EV2, notice that
xk = Ak-i(j)xj + Wk,	(28)
where j = sup{s ∈ N | i(s) ≤ k}, i.e., xj is the last state in {xk} that physically occurs no later
than xk, and Wk = Pk二0m Ak-i(j)-1-lWi(j)+i, Since A is stable, we have ∣∣E[WkW>]k ≤ W.
Pre-multiplying both sides of (28) with P1/2 and applying the triangle inequality, we have
∣∣p 1/2xk|| ≤ ∣∣p 1/2Ak-i(j)xj|| + ∣∣P"Wk∣∣.
17
Under review as a conference paper at ICLR 2022
Therefore, applying the power means inequality (a++b)4 ≤ a4+b4, We have
Vk = ∣∣P 1/2xfc||4 ≤ 8 f∣∣P1/2AKj)Xj『+ ∣∣P1/2WkF
8
X> (Alj))> PAlj)Xj)	+ (WkPwk『
≤ 8 (ρ2(Kj))V2 + (WkPwk)2
Taking the expectation on both sides of the above inequality and applying (27), we get
EVk2 ≤ 8 hQ(M,ρ,P)λmin(P)2+W2 kPk2i .
Finally, using the fact that Vk = XkkPXk ≥ λmin (P) kXk k2, we have
EkXkk4 ≤、EVM2 ≤ 8 [Q(M,P, P) + W2κ(P)2],
λmin (P)2
which concludes our proof.
□
Lemma 10. For the system described in (23)-(25), assume K is a stabilizing linear feedback (i.e.,
ρ(A + BK) < 1) gain with kKk ≤ M, and there exists 0 <ρ < 1 and P 0 such that
A>PA Y ρP, (A + BK)>P(A + BK) Y ρP. Let J be the cost (defined in (3)) associated with the
linear feedback policy πκ, and J M,t be the cost associated with the safe switching policy ∏K,M,t.
Then
JM,t -J≤ 2C1(ρ, P, K)G(M, t, ρ, P, K) + G(M, t, ρ, P, K)2,
where
C1(P,P,K) = WκK(P)∣∣q-+pktrk∣),
G(M,t,P, P, K) = CIρ, K)t [Q(M,ρ, P) + W2κ(P)2]1/4 E(M,ρ, P),
∞	2n/2+7/4
C2(ρ, K) = ∣∣Q + KTRKIIkBKk ∙ N k(A + BK)sk ∙ Pr山 - I,
(C2 (ρ, K) < ∞ because ρ(A + BK) < 1),
Q(MPP ) = (MMIA P+W)PPk2 [(1+P)(M 2A2+W )+4A 2W K(P)],
E(M, ρ, P)
exP 卜一2
In particular, if the system specification A, B,Q, R, W, the stabilizing linear feedback gain K, and
the parameters ρ, P are all fixed, we have
JM，t - J 〜O (tM exp(-cM2))
as M → ∞, t → ∞, tM exp(-cM 2 ) → 0, where c
(1-P1∕4)2
4WK(P).
Proof. Let {xk}, {uk} denote the state and input trajectories driven by ∏k, and {Xk}, {Uk} denote
the state and input trajectories driven by ∏K,M,t. Then from ∏k(x) = KX and ∏κ,M,t(χ) ∈
{KX, 0}, we have
J = lim E
T→∞
1 T-1
TEXT(Q + KTRK)Xk
k=0
1 T-1
Iim 斤 ∑E [x>(Q + KTRK)Xk],
T→∞ T
k=0
JM,t ≤ lim sup E
T→∞
1 T-1
TEXT(Q+K TRK)Xk
k=0
1 T-1
lim sup —£e[x>(Q + KTRK)Xk].
T→∞	k=0
18
Under review as a conference paper at ICLR 2022
Therefore, we only need to prove
E [x] (Q + K>RK)Xk] - E [x> (Q + KTRK)Xk] ≤
2Cι(ρ, P, K)G(M,t,ρ, P, K) + G(M,t,ρ, P, K)2
for every k. Let Q = Q + K 1 RK. Since Q » 0 and R » 0, we have Q » 0, and therefore
∣∣x∣∣q，JXTQX defines a norm. In the follows, we bound E IlXkIIQ - E ∣∣xfc ∣∣Q.
We notice that
Xk = (A + BK)xk-1 + wk-1 - BKXk-11{&k-1=0}
=(A + BK) [(A + BK)xk-2 + wk-2 - BKXk-21{Uk-2 = 0}] + wk-1-
BKxk-11{uk-ι =0}
k-1	k-1
=(A + BK)kxo + X(A + BK)swk-s-ι- X(A + BK)SBKXk-S-ι1{u17=o}
s=0	s=0
k-1
=xk - X(A + BKYBKXk-S-11{Uk-s-1}.
s=0
Hence,
k-1
∣xk∣Q ≤ IlXkkQ + XIl(A + BK)SBKXk-S-11{g-s-ι}IlQ
S=0
k-1
≤ ∣Xk∣Q + ∣∣Q∣∣ ∣BK∣ X II(A + BK )[∣∣Xk-S-ι1{u 1.1}∣∣.
S=0
From the fact that E(Xι + …+ Xn)2 ≤ (PEX2 +----+ PEXn) , where Xi,..., Xn are
arbitrary random variables with bounded second-order moments, we have
EkXkkQ ≤ (qE∣∣Xk∣∣Q + ∣∣Q∣∣∣BK∣ X II(A + BK)Sk r [∣Xk-S-ι∣21{j-=0}]).
By Cauchy-Schwarz inequality, we have
/	_______ k-1	1	∖ 2
E kXkkQ ≤	( JEkXkkQ	+ ∣∣Q∣∣ kBKk X k(A + BK)Sk	(EkXk-S-1『)4	P(Uk-S-ι	= 0))
By Lemma 9, we have EkXk-S-1∣∣4 ≤ 8[Q(M, ρ, P) + W2κ(P)2] for any s, and hence,
EkXkkQ
EkXkkQ + ∣∣Q∣∣ kBKk 23∕4[Q(M,ρ,P) + W2κ(P)2]1/4
k-1	2
X k(A + BK)Sk P (Uk-S-I =0)
S=0
(29)
According to the update rule of ξk in πκ,M,t, we have
t-i
{Uk = 0}⊆ U {kXk-tk ≥ M}.
T =0
Taking the union bound, we have
t-1
P (Uk =0) ≤ XP (kXk-tk ≥ M).
τ =0
19
Under review as a conference paper at ICLR 2022
By Lemma 6, we have
2n/2+1
P(kXk-tk ≥ M) ≤ P72 - 1 E(M,P,P)
for every k and t, and hence
2n/2+1
P (Uk =0) ≤ t	1/2	1 E(M,P,P)
P-1/2 - 1
for every k. Substituting (30) into (29), we get
(30)
EkxkkQ ≤ (/EkxkkQ + ∣∣Qll kBKk pY+141[Q(M,P,P)+ W2κ(P)2]1/4
k-1	2
Xk(A+BK)skE(M,P,P)
s=0
(,Ekxk kQ + C (P, K )[Q(M, P, P) + W 2 K(P )2]1/4E (M,ρ, P))
(,Ekxk kQ + G (M,t,P,P,K))
Ekxk kQ + 2,EkxkkQ G(M, t, ρ, P, K) + G(M, t, ρ, P, K )2.
Applying Lemma 7 with M = 0, we have
E[x>Pxk] ≤「,
k	1-P
and hence
EkxkkQ ≤ ∣∣Q∣∣ kxkk2 ≤ W,P?Qk = Cι(ρ,P,K)2.
Therefore,
EkxkkQ - E kxk kQ ≤ 2Cι(ρ, P, K)G(M, t,ρ, P, K) + G(M, t,ρ, P, K)2,
and hence
JM,t -J ≤ 2C1(P, P, K)G(M, t, P, P, K) + G(M, t, P, P, K)2,
which concludes our proof.
□
A.1.3 A law of large numbers for martingales
Let {Fk} be a filtration of σ-algebras and {Sk} be a matrix-valued stochastic process adapted to
{Fk}, we call {Sk} a matrix-valued martingale (with respect to the filtration {Fk}) if E[Sk+1 |
Fk] = Sk holds for all k. Now we can state a law of large numbers for matrix-valued martingales:
Lemma 11. (Liu et al., 2020, Lemma 4) If Sk = Φo + Φι + •…+ Φk is a matrix-valued martingale
such that
E kΦkk2 〜C(β),
where 0 ≤ β < 1, then Sk/k converges to 0 almost surely. Furthermore,
Sk
--〜
k
C &
20
Under review as a conference paper at ICLR 2022
A.1.4 A perturbation analysis of algebraic Riccati equation
An interesting property of LQR is that near the optimal controller (which corresponds to the solu-
tion of the discrete algebraic Riccati equation), the perturbation of the discrete Lyapunov equation
solution is quadratic in the perturbation of controller gain. Basically, this results in the convergence
speed of the certainty equivalent control performance being twice that of the estimation error. Sim-
ilar results have been reported in (Mania et al., 2019; Simchowitz & Foster, 2020), and here we
present a simplified version that would suffice for our purpose.
Lemma 12. Consider the discrete Lyapunov equation
A>XA-X+Q=0,
where Q	0 and ρ(A) < 1, then the unique positive definite solution X satisfies
kX kF ≤ ∣∣(I-A> 乳 A>)-1(kQkF.
Proof. Vectorizing the Lyapunov equation, we get
Vec(X) =(I — A> 0 A>) Ivec(Q),
and hence
kX∣∣F = kvec(X)k2 ≤ Il(I - A> 0 A>)-1∣2 kvec(Q)k2 = ∣(I - A> 0 A>)-11 ∣∣Q∣∣f .
□
Lemma 13. Let P be the solution to the discrete algebraic Riccati equation
P = Q + A>PA - A>PB (R + B>PB)-1 B>PA,
and let K be defined as
K = - (R + B>PB)-1 B>PA.
Assume K is such that A = A + BK is stable, and P satisfies the discrete Lyapunov equation
P = Q + K >RK + A>P A.	(31)
ʌ ʌ
Let ∆P = P - P, ∆K = K - K, then
k∆PkF ≤ ∣∣(i-A> 0A>)-1 JRkFk∆KkF.
Proof. It can be shown P satisfies the discrete Lyapunov equation
P= Q+K>RK+(A+BK)>P(A+BK).	(32)
Meanwhile, substituting ∆P = P - P, ∆K = K - K into (31), we get
P+∆P
=Q+(K+∆K)>R(K+∆K)+(A+B(K+∆K))>(P+∆P)(A+B(K+∆K))
=Q + K > RK + ∆K >R∆K + (A + BK )> P(A + BK) + A>∆PA,	(33)
where for the second equality we used the fact
(R + B>PB) K + B>PA = 0.
By taking the difference between (33) and (32), we can see ∆P also satisfies a discrete Lyapunov
equation:
∆P = ∆K>R∆K + A>∆PA.
Applying Lemma 12, we obtain
k∆PkF ≤ ∣∣(i- A> 0 A>)-1 J∣∆K>rδk∖∖f
≤ ∣∣(i- A> 0 A>)-1 JRkF k∆KkF,
which concludes our proof.
□
21
Under review as a conference paper at ICLR 2022
A.2 Proof of Theorem 1
Proof. According to our definition of bounded-cost safety, we only need to prove Jπk < +∞ for
every k, where Jπk is defined in (3).
Let {xi }, {ui} denote the state and input trajectories driven by πk . According to Algorithm 2, we
1	_ C T>-	zʌ 1	1 ,1	i'
have ui ∈ {Kkxi , 0}, and therefore,
Jπk ≤ lim sup E
T→∞
1 T-1
T X x> Q+ + KK>RKKk) Xi
i=0
1 T-1
lim sup — X E [x> (Q + 宜>RKKk) Xi].
T→∞ T i=0
Notice that under πk, there is Xi+1 = AXi + wi as long as kXik ≥ log k, and since A is stable, there
exists 0 < ρ < 1 and P * 0 such that A>PA Y PP. By Lemma 7, We have
E [x>PXi] < ((Iogk/ A2 + kWk) kPk,
i	1-P
where A = max{∣∣Ak, ∣∣A + BKk∣∣}.	Therefore, by x>PXi ≥ λmin(P) ∣∣χ∕∣2 and
X> (Q + K>RKk)xi ≤ kQ + K>RKkk ∣Xik2,wehave
E hx> (Q + K>RKk) si < ((Iogk)2A2 + kWI)K(P)kQ + K>RKkk.
This implies
((logk)2A2 + kWk)κ(P)kQ + K>RKkk “二»
J k ≤	< +∞,
1-P
which concludes our proof.
□
A.3 Proof of Theorem 2
Proof. We shall assume throughout the proof that {Fk} is the σ-algebra generated by the random
variables {X0, w0, . . . , wk, ζ0, . . . , ζk}.
We first cast the system (1) into a static form by writing Xk as
k-1	k-1
Xk = AkX0 +	AtBuk-t-1 + Atwk-t-1
t=0	t=0
k-1	k-1
Akx0 + ^X Ht [uk-t-1 + (k - t) βζk-t-l] + ^X Atwk-1-1.	(34)
t=0	t=0
In order to estimate Hτ, post-multiply both sides of (34) with ζk>-τ-1 and rearrange the terms, and
we get
kX-1k-1
Atwk-t-1 + AkX0	ζk>-τ -1 + X(k - t)-β Htζk-t-1ζk>-τ -1+
t=0
k-1
X HtUk-t-ιZlτ-ι∙	(35)
t=0
To take the expectation of (35), notice that for any time step k, the variables X0, ζ0, ζ1, . . . , ζk,
wο, wι,..., wk,u0, Ui,..., Uk+ι are measurable w.r.t. Fk, which, together with the independence
among X0, ζ0, ζ1, . . . , ζk, w0, w1, . . . , wk, leads to the following relations:
E[ζk1ζk>2 | Fk2-1] =I0 iofthke1rw=iske2,	(36)
>	0	if k1 ≤ k2
E [ukι ζk2 | Fk2-i] = ∖ ʌ 1	ʌ .	(37)
1 k2	2 other values otherwise,
E [wk1 ζk>2 | Fk2-1] =0,	(38)
E[X0ζk>2 | Fk2-1] =0,	(39)
22
Under review as a conference paper at ICLR 2022
for any two time steps k1 ≥ 0 and k2 ≥ 1.
Now let us prove the conclusion using induction on τ .
First consider the case τ = 0: we have
k	k-1
Hk,0 - Ho = k X[(i + 1)βXiZi-I - Ho] = k X [(i + 2)βXi+ιZ> - Ho] .	(40)
i=1	i=0
Let Φk = (k + 1)βxk+1ζk> - Ho. By substituting (35) and (36) to (39) into (40) it can be seen
E[Φk | Fk-1] = 0, and hence Sk , Pik=o Φi is a martingale w.r.t. {Fk}. Furthermore, we can
verify E ∣∣Φk∣∣2 〜C(2β): by CaUchy-SchWarz inequality,
E∣∣(k + 1)βXk+ιZ>∣∣2 ≤ (k + 1)2βqE ∣Xk+ιk4qE kZkk4.
By the procedure described in Algorithm 2, the sWitching threshold is no larger than log k for every
step before k, and therefore, according to Lemma 9,
Ekxk k4 < 8 [Q(log k,ρ, P) + W2κ(P)2]〜O ((log k)8)〜C(0),
where 0 < ρ < 1 and P * 0 are constants such that A>PA Y PP, which exist due to the
stability of A, Q is defined as in Lemma 9, and W is defined in (25). Also taking note of the fact
EkZkk4 = p(p + 2)〜C(0), we have
E∣∣(k +1)βxk+ιZ>∣∣2 〜C(2β),
and hence,
E kΦkk2 = E ∣∣(k +1)βxk+ιZ> — Ho∣∣2 ≤ E (∣∣(k +1)βxk+ιZ>∣∣ + ∣Ho∣)2
≤ 2(E ∣∣(k + 1)βxk+ιZ> ∣∣2 + kHok2)〜C(2β).
By applying Lemma 11 to the martingale {Sk} defined above, we get
Hk,o - Ho 〜C (β - 2
Now assume that τ ≥ 1 and that we already have
Hk,t - Ht 〜C ββ - 2
(41)
for t = 0, 1, . . . , τ - 1. Then
Hk,τ - Hk,τ
=L X (一 )β
i=τ +1
1 k-τ-1
=4 X (i + 1)β
k-τ
i=o
1 k-τ-1
=4 X (i + 1)β
k-τ
i=o
τ-1
xi - EH k,tui-t-1
t=o
τ-1 - Hk,τ
τ-1
xi+τ +1 - EHk,tui+
τ-t
t=o
τ-1
xi+τ +1
τ-1
t=o
Zi> - Hk,τ
-ɪ2 Htui+τ-t	ZJ- Hk
,τ -
(42)
Completely similarly to the case τ = 0, we can show
1	k-τ-1
占 X	(i + 1)β
i=o
t=o
k-τ-1
X (i + i)βUi+τ-tZ>.
i=o
τ-1
xi+τ +1 ->: Htui+τ — t
t=o
)
23
Under review as a conference paper at ICLR 2022
Meanwhile, for each of t = 0,1,..., τ - 1, define Φk = (k + 1)βHk-tZ>. In view of (37) it can be
shown E[Φtk | Fk-1] = 0, and hence Skt , Pik=0 Φtk is a martingale w.r.t. {Fk }. Furthermore, by
the procedure described in Algorithm 2, kufc-tk2 ≤ (log(k -1))4 ≤ (log k)4, we have
E∣∣Φk∣l2 ≤ (k +1)2β (log k)4 EkZk『〜C (2β).
By applying Lemma 11 to the martingale Skt defined above, we get
1 k-τ-1	1
k-τ X (i + I)βui+τ-tZJ 〜C (β - 2),
i=0
which together with (41) implies
(HHk,t - Ht) k - T X (i + 1)βui+τ-tζ> 〜C (2 (β - 2))〜C (β - 2).
Now that We have shown the RHS of (42) is the sum of T +1 matrices, each of order C (β - 1), we
get
Hk,τ - Hk,τ 〜C (β - 2).
According to our definition ofC(α), this is to say it holds almost surely that
lim
k→∞
HHk,τ - HT
k-Y+e
0,
where γ = 1/2 - β > 0, for any > 0, which concludes our proof.
□
A.4 Proof of Theorem 3
Let us denote by JWπ the cost of a policy π when acting on a system in the form (1) with process
noise covariance W. Let us consider the following variants of policies:
•	π*: optimal policy, i.e., π*(x) = K*x, with K* defined in (6).
•	∏k: certainty equivalent policy with no exploratory noise, i.e., ∏k(χ) = Kkx.
•	∏k: safe switching policy with no exploratory noise, i.e., Algorithm 2 invoked as
π(x, ξ; k, Kk, +∞).
•	πk : safe switching policy with exploratory noise, i.e., the actually applied policy at step k.
_ _	_	__ _k,	,	__* 一	一	.	_	__ .,
Notice that Jπk = JWπk and J* = JWπ . Our plan is decomposing Jπk - J* as
Jπk - J*
- JWπk+(k+1)-2βI
+ JWπk+(k+1)-2βI - JWπk+(k+1)-2β I +
(Jnk
JW +(k+1)-2β I
and bounding the RHS terms respectively. We next tackle these terms in reverse order:
1. JWWk - Jn: for an arbitrary stabilizing linear feedback policy ∏(χ) = Kx, we know
the cost is JWπ = tr(W P), where P solves the discrete Lyapunov equation (32). By
Theorem 2, the Markov parameter estimates converge as C(-γ). Since with random input
Uh
in Algorithm 3, the matrix h is full row rank, it follows that the pseudo-inverse operator
X0
is differentiable, which together with Lemma 2 guarantees that Ak - A, Bk - B 〜C(-γ).
Due to the differentiability of the stabilizing solution of the discrete algebraic equation,
there is also Kk - K* 〜 C(-γ). In particular, {Kk} converges to K* almost surely,
and since K* is stabilizing, we have for almost every realization of randomness, Kk is
also stabilizing for sufficiently large k. Assuming w.l.o.g. that Kk is stabilizng for any
24
Under review as a conference paper at ICLR 2022
k, We have JWk - J* =tr(W(Pk 一 P*)), where P* is the Lyapunov equation solution
corresponding to K*, also the solution to the Riccati equation (5), and Pk is the Lyapunov
equation solution corresponding to Kk. According to Lemma 13, we have
2
2.
3.
4.
kPk-P*kF ≤∣∣ I- A> 氧
-1
where Ak = A + BKk. Since every Ak is stable, we have Il(I 一 A> 0
-1
is
2
a continuous function of Ak. Meanwhile, {Ak} converges to A + BK*, which implies
-1
is bounded. Hence, K k 一 K * 〜C (-γ) implies Pk 一 P * 〜
2
C(-2γ).Finally,we have JWk — JW =tr(W(Pk — P*))〜C(-2γ).
T^∏k
JW +(k+1)-2βI
Tnk
JW +(k+1)-2βI
Tnk
JW +(k+1)-2βI
Tnk
JW +(k+1)-2βI
一 JWk: from JWk = tr( WPk), with Pk defined the same as above, we have
一 JWk = tr((k + 1)-2βPk). Since {Pk} converges to P*	0, we have
一 JWk 〜C(—2β).
一 JW+(k+i)-2βI: we can basically apply Lemma 10. To verify the con-
ditions of Lemma 10, we first fix ρ, P: since A + BK* is stable, for any fixed Q 0,
the discrete Lyapunov equation (A + BK*)>P(A + BK*) + Q = P has a solution
P	0. Therefore, there exists P 0, 0 < ρ < 1, such that (A + BK*)>P(A +
BK*) < ρP. Since {Kk} converges to K* almost surely, we may assume w.l.o.g.
that (A + BKk)>P(A + BKk) < ρP for any k. Furthermore, since A is stable,
there is At → 0 as t → ∞, and therefore, with t = blog kc sufficiently large, we
also have (At)> PAt < ρP. For an upper bound of the noise magnitude, we insert
W + I in place of W in (25) to compute W . Now that A, B, Q, R, W , ρ, P are all
fixed, we can apply the conclusion of Lemma 10 by taking the limit Kk → K* to ob-
tain JWk+(k + 1)-2βI- JWk+(k+1)-2βI 〜O((IOg k)2exp(-C(IOg k)2))〜C(-8).
JWk 一 JW+(k+i)-2βi: these two costs are associated with the same closed-loop system and
differ only in the u>RUk terms. In particular, JWk — J^+(k+i)-2βi = tr((k + 1)-2βR)〜
C (-2β).
Adding up the above four terms using Lemma 2, we obtain Jπk -J* 〜 C(max{-2β, -2γ, -∞}) 〜
C(- min{2β, 2γ}). According to our definition of C(α), this is to say it holds almost surely that
Jπk - J*
lim ----------------
k→∞ k- min{2β,2γ}+'
0,
for any > 0, which concludes our proof.
B	An illustration of oscillation under switching
In this section, we provide a simple illustrative example to explain why we need prolonging “non-
action” period in Algorithm 2.
Consider a simple two-dimensional noise-free system
xk+1 = Akxk ,
where the candidates for Ak are
0.5	2	0.5	0
A0 =	0	0.5	, A1	=	2	0.5	.
It can be seen ρ(A0) = ρ(A1) = 0.5, i.e., both the system matrices are stable.
Now consider the following switching strategy:
2
F

25
Under review as a conference paper at ICLR 2022
•	If kxk k ≥ M, then apply A0 for t consecutive steps.
•	Otherwise, apply A1.
Figure 4 shows simulation results with x0 = (0.1, 1), M = 1 and t = 1, 2. We can observe that
even for this simple system, the frequent switching caused by t = 1 may cause the state to oscillate,
while t = 2 suffices to suppress the oscillation. Indeed, we can verify ρ(A1A0) ≈ 4.5 > 1 even
though both A0 and A1 are stable. However, as long as A0 is stable, A1At0 will eventually become
stable as t is chosen to be sufficiently large. This explains why we use prolonging t in our policy.
2
1
ι	2	3	4
Threshold
o Init. state
-At =1
—► t = 2
Figure 4: Illustrative example of oscillation under switching. In this example, t = 1 causes the state
to oscillate, while t = 2 can suppress the oscillation.
26