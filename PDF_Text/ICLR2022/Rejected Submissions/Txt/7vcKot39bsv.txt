Under review as a conference paper at ICLR 2022
Adaptive Inertia: Disentangling the Effects
of Adaptive Learning Rate and Momentum
Anonymous authors
Paper under double-blind review
Ab stract
Adaptive Momentum Estimation (Adam), which combines Adaptive Learning Rate
and Momentum, would be the most popular stochastic optimizer for accelerating
the training of deep neural networks. However, empirically Adam often generalizes
worse than Stochastic Gradient Descent (SGD). We unveil the mystery of this
behavior in the diffusion theoretical framework. Specifically, we disentangle the
effects of Adaptive Learning Rate and Momentum of the Adam dynamics on saddle-
point escaping and flat minima selection. We prove that Adaptive Learning Rate
can escape saddle points efficiently, but cannot select flat minima as SGD does. In
contrast, Momentum provides a drift effect to help the training process pass through
saddle points, and almost does not affect flat minima selection. This partly explains
why SGD (with Momentum) generalizes better, while Adam generalizes worse
but converges faster. Furthermore, motivated by the analysis, we design a novel
adaptive optimization framework named Adaptive Inertia, which uses parameter-
wise adaptive inertia to accelerate the training and provably favors flat minima as
well as SGD. Our extensive experiments demonstrate that the proposed adaptive
inertia method can generalize significantly better than SGD and conventional
adaptive gradient methods.
1	Introduction
Adam (Kingma and Ba, 2015), which combines Adaptive Learning Rate and Momentum, would be
the most popular optimizer for accelerating the training of deep networks. However, Adam often
generalizes worse and finds sharper minima than SGD (Wilson et al., 2017) for popular convolutional
neural networks, where the flat minima have been argued to be closely related with good generalization
(Hochreiter and Schmidhuber, 1995; 1997a; Hardt et al., 2016; Zhang et al., 2017).
Meanwhile, the diffusion theory has been used as a tool to study how SGD selects minima (Jas-
trzkebski et al., 2017; Li et al., 2017; Wu et al., 2018; Xu et al., 2018; Hu et al., 2019; Nguyen
et al., 2019; Zhu et al., 2019; Xie et al., 2021b; Li et al., 2021). This line of research also suggests
that injecting or enhancing SGD-like sharpness-dependent gradient noise may effectively help find
flatter minima (An, 1996; Neelakantan et al., 2015; Zhou et al., 2019; Xie et al., 2021a;c; HaoChen
et al., 2021). Especially, Zhou et al. (2020) argued the better generalization performance of SGD
over Adam by showing that SGD enjoys smaller escaping time than Adam from a basin of the
same local minimum. However, this argument does not reflect the whole picture of the dynamics of
SGD and Adam. Empirically, it does not explain why Adam converges faster than SGD. Moreover,
theoretically, all previous works have not touched the saddle-point escaping property of the dynamics,
which is considered as an important challenge of efficiently training deep networks (Dauphin et al.,
2014; Staib et al., 2019; Jin et al., 2017; Reddi et al., 2018).
Our work mainly has two contributions. 1) We disentangle the effects of Adaptive Learning Rate and
Momentum in Adam learning dynamics and characterize their behaviors in terms of saddle-point
escaping and flat minima selection, which explains why Adam usually converges fast but does not
generalize well. Particularly, we prove that, Adaptive Learning Rate is good at escaping saddle points
but not good at selecting flat minima, while Momentum helps escape saddle point and matters little
to escaping sharp minima1. 2) We propose a novel Adaptive Inertia (Adai) optimization framework
1 When we talk about saddle points, we mean strict saddle points in this paper.
1
Under review as a conference paper at ICLR 2022
Table 1: Test performance comparison of optimizers. We report the mean and the standard deviations
(as the subscripts) of the optimal test errors computed over three runs. Our methods, including Adai
and AdaiW, consistently outperforms all other popular optimizers.
Dataset	Model	AdaiW	Adai	SGD M	Adam	AMSGRAD	AdamW	AdaBound	Padam	Yogi	RAdam
CIFAR-10	ResNet18	4.59o.i6	4.740.14	5.010.03	6.53o.o3	6.16o.18	5.08o.o7	5.65o.o8	5.12o.o4	5.87o.12	6.01o.1o
	VGG16	5.81o.o7	6.000.09	6.420.02	7.310.25	7.14o.14	6.48o.13	6.76o.12	6.15o.o6	6.90o.22	6.56o.o4
CIFAR-100	ResNet34	21.05o.ιo	20.790.22	21.520.37	27.160.55	25.53o.19	22.99o.4o	22.87o.13	22.72o.1o	23.57o.12	24.41o.4o
	DenseNet121	19.44o.2i	19.590.38	19.810.33	25.110.15	24.43o.o9	21.55o.14	22.69o.15	21.10o.23	22.15o.36	22.27o.22
	GoogLeNet	20.500.25	20.550.32	21.210.29	26.12o.33	25.53o.17	21.29o.17	23.18o.31	21.82o.17	24.24o.16	22.23o.15
which is conceptually orthogonal to the existing adaptive gradient framework. Adai does not
parameter-wisely adjust learning rates, but parameter-wisely adjusts the momentum hyperparameter,
called inertia. We show that Adai can provably escape saddle points fast while learning flat minima
well. From the empirical results (see Table 1), Adai significantly outperforms SGD and popular
Adam variants.
The paper is organized as follows. In Section 2, we first introduce the dynamics of SGD and analyze
its behavior on saddle-point escaping, which serves as a basis to compare with the behavior of
Adaptive Learning Rate and Momentum. In Section 3, we analyze the behavior of Momentum. In
Section 4, we analyze the dynamics of Adam. In Section 5, we introduce the new optimizer Adai. In
Section 6, we empirically compare the performance of Adai, Adam variants, and SGD. Section 7
concludes the paper with remarks.
2	SGD and Diffusion
In this section, we introduce some preliminaries about the SGD diffusion and then present the
saddle-point escaping property of SGD dynamics.
2.1	Prerequisites for SGD Diffusion
We first review the SGD diffusion theory for escaping minima proposed by Xie et al. (2021b). We
denote the model parameters as θ, the learning rate as η, the batch size as B, and the loss function over
one minibatch and the whole training dataset as L(θ) and L(θ), respectively. A typical optimization
problem can be formulated as minθ L(θ). We may write the stochastic differential equation/Langevin
Equation that approximates SGD dynamics (Mandt et al., 2017; Li et al., 2019) as
dθ = -VL(θ)dt + [ηC(θ)] 2 dWt,	(1)
where dWt 〜N(0, Idt), I is the identity matrix, and C(θ) is the gradient noise covariance matrix.
The gradient noise is defined through the difference of the stochastic gradient over one minibatch
and the true gradient over the whole training dataset, ξ = VL(θ) - VL(θ). It is well known that the
Fokker-Planck Equation describes the probability density governed by Langevin Equation (Risken,
1996; Sato and Nakagawa, 2014). The Fokker-Planck Equation is
^PdLttL = V∙ [P(θ,t) VL(θ)] + V∙ VD(θ)P(θ, t),	(2)
where V∙ is the divergence operator and D(θ) = ηc(θ) is the diffusion matrix (Xie et al., 2021b). We
note that the dynamical time t is equal to the product of the number of iterations T and the learning
rate η: t = ηT .
As the gradient variance dominates the gradient expectation near critical points, we have
D(θ) = ηCθL ≈ ɪ [ɪ X VLj(θ)VLj(θ)>] = ɪFIM(θ) ≈ ɪ[H(θ)]+	(3)
2	2B N	2B	2B
j=1
near a critical point c, where Lj (θ) is the loss function of the j-th training sample, H(θ) is the Hessian
of the loss function at θ, and FIM(θ) is the observed Fisher Information matrix, referring to Chapter
8 of Pawitan (2001) and Zhu et al. (2019). We further verified that Equation (3) approximately
2
Under review as a conference paper at ICLR 2022
holds even not around critical points in Figure 6 of Appendix C. Equation (3) was also proposed by
Jastrzkebski et al. (2017) and Zhu et al. (2019) and verified by Xie et al. (2021b) and Daneshmand
et al. (2018). Please refer to Appendix C for the detailed analysis of the stochastic gradient noise.
If We have eigendeComPosition H = U diag(Hι,..., Hn-ι, Hn)U>, then We use [∙]+ to denote the
transformation that [H]+ = U diag(|H1|, . . . , |Hn-1|, |Hn|)U>. The i-th column vector of U is the
eigenvector of H corresPonding to the eigenvalue Hi .
In the folloWing analysis, We use the second-order Taylor aPProximation near critical Points. This
assumPtion is common and mild, When We focus on the behaviors near critical Points (Mandt et al.,
2017; Zhang et al., 2019a; Xie et al., 2021b). Note that, by Equation (3) and AssumPtion 1, the
diffusion matrix D is indePendent of θ near critical Points.
Assumption 1. The loss function around the critical point c can be approximately written as
L(θ) = L(c) + 2(θ -C)TH(c)(θ -c).
2.2	SGD Diffusion near Saddle Points
In this subsection, We establish the saddle-Point escaPing ProPerty of SGD diffusion as Theorem 1.
Theorem 1 (SGD EscaPes Saddle Points). Suppose c is a critical point, Assumption 1 and Equation
(3) hold, the dynamics is governed by Equation (1), and the initial parameter is at the saddle point
θ = c. Then the probability density function of θ after time t is given by the Gaussian distribution
θ 〜N(c, Σ(t)), where Σ(t) = U diag(σ2,..., σnn-ι, σnn)U> and
D
σ2 ⑴=TΓ[1 — exP(-2Hit)],
Hi
where Di is the i-th eigenvalue of the diffusion matrix D and Hi is the i-th eigenvalue of the Hessian
matrix H at c. The column vectors of U are exactly the eigenvectors of H. The dynamical time
t = ηT. In terms ofSGD notations and ∣Hi∣ηT《1 near saddle points, we have
σ2(T) = IHiFT + O(B-1Hi2η3T2).
B
The Proof is relegated to APPendix A.1. We note that 1) if Hi > 0, the distribution of θ along the
direction i converges to a Gaussian distribution with constant variance, N (ci,备)；and 2) if Hi < 0,
the distribution is a Gaussian distribution With the variance exPonentially increasing With time t.
As the disPlacement ∆θi from the saddle Point c can be modeled as a center-fixed Gaussian distribu-
tion, the mean squared disPlacement is equivalent to the variance, namely h∆θi2(t)i = σi2(t). The
result means that SGD escaPes saddle Points very sloWly (h∆θi2i = O(|Hi|)) if Hi is close to zero.
Note that, in the diffusion analysis, the direction i denotes the direction of an eigenvector instead of a
coordinate’s direction. While SGD uPdates model Parameters along the coordinates, We do not need
to treat the coordinates’ directions sPecially in the continuous-time analysis.
3	Analysis of Momentum Dynamics
In this section, We analyze Momentum in terms of saddle-Point escaPing and flat minima selection.
The continuous-time Momentum dynamics. The Heavy Ball Method (Zavriev and Kostyuk, 1993)
can be Written as
mt = β1mt-1 + β3gt ,
θt+1 = θt - ηmt,
(4)
Where β1 and β3 are the hyPerParameters. We note there are tWo PoPular choices, Which are,
resPectively, β3 = 1 corresPonding to SGD-style Momentum and β3 = 1 - β1 corresPonding to
Adam-style Momentum, namely the exPonentially moving average.
We can Write the motion equation in Physics With the mass M and the damPing coefficient γ as
ʃ rt = (I - Ydt)Irt-1 + Mdt
θt+1 = θt + rtdt,
(5)
3
Under review as a conference paper at ICLR 2022
where r = -mt, F = gt, dt = η, 1 一 γdt = βι, and M = β3. Thus, We obtain the differential
form of the motion equation as
Mθ = -γMθ + F,
(6)
where θ =需 and θ = d. The left-hand side as the inertia force is equal to the mass M times
the acceleration θ, the first term in the right-hand side as the damping force is equal to the damping
coefficient γ times the physical momentum Mθ, and the second term in the right-hand side is equal to
the external force F in physics. We can easily obtain the mass M =京 and the damping coefficient
Y = 1-β1 by comparing (5) and (6).
As F corresponds to the stochastic gradient term, we obtain
Mdθ = -γMdθ 一 dL(θ) dt + [2D] 2 dWt.
∂θ
Its Fokker-Planck Equation in the phase space (the θ-θ space) is well known as
∂P(θ, r, t)
∂t
一 Vθ ∙ [rP(θ, r,t)] + Vr ∙ [γr + M-1VθL(θ)] P(θ, r, t)
+ Vr ∙ M-2D ∙ VrP(θ,r,t)
(7)
(8)
where r = θ. Equation (8) is not specialized for learning dynamics but a popular result in Langevin
Dynamics literatures (Risken, 1996; Risken and Eberly, 1985; Zhou, 2010; Kalinay and Percus, 2012).
Equation (57) of Radpay (2020) gives an exactly same form for describing finite-inertia Langevin
Dynamics, Equation (7). We contribution is the first to apply it to deep learning dynamics.
Momentum escapes saddle points. We formulate how Momentum escapes saddle points as Theorem
2, whose proof is relegated to Appendix A.2.
Theorem 2 (Momentum Escapes Saddle Points). Suppose c is a critical point, Assumption 1 and
Equation (3) hold, the dynamics is governed by Momentum, and the initial parameter is at the saddle
point θ = c. Then the mean squared displacement near saddle points is
(δθ2 ⑴)= 3 Λ2 [1 一 exp(-Yt)K +	rτ⅛- [1 一 exp(--iτ儿	⑼
γ3M2	γMHi	γM
where ∆θ(t) = θ(t) - θ(0) is the displacement of θ, and〈.〉denote the mean value. In the right-hand
side, this first term is the momentum drift effect and the second term is the diffusion effect. As
| Hi ∣ηT《1 near ill-conditioned saddle points, it can be written in terms of Momentum notations as
〈△碎i=2(吧"[1 - exp (一(1 一 βι)T)]2 + Hβ3βT + O(BTHVT2).
By comparing Theorems 1 and 2, we notice that, SGD escapes saddle points only due to the diffusion
effect (similar to the second term in Equation (9)), but Momentum provides an additional momentum
drift effect (the first term in Equation (9)) for passing through saddle points (Wang et al., 2019). This
momentum drift effect has not been mathematically revealed before, to the best of our knowledge.
Momentum escapes minima. We first introduce two classical assumptions.
Assumption 2 (Quasi-Equilibrium Approximation). The system is in quasi-equilibrium near minima.
Assumption 3 (Low Temperature Approximation). The system is under low temperature (small
gradient noise).
Xie et al. (2021b) modeled the process of SGD escaping minima as a Kramers Escape Problem in
deep learning. Recent machine learning papers (Jastrzkebski et al., 2017; Xie et al., 2021b; Zhou et al.,
2020) also used Assumptions 2 and 3 as the background implicitly or explicitly. Quasi-Equilibrium
Approximation and Low Temperature Approximation have been widely used in many fields’ Kramers
Escape Problems for state transition/minima selection, including statistical physics(Kramers, 1940;
Hanggi, 1986), chemistry(Eyring, 1935; Hanggi et al., 1990), biology(Zhou, 2010), electrical engi-
neering(Coffey and Kalmykov, 2012), and stochastic process(Van Kampen, 1992; Berglund, 2013).
4
Under review as a conference paper at ICLR 2022
Assumptions 2 and 3 mean that the diffusion theory is good at modeling “slow” escape processes
that cost more iterations. As this class of “slow” escape processes takes main computational time
compared with “fast” escape processes, this class of “slow” escape process is more interesting for
training of deep neural networks. Empirically, Xie et al. (2021b) reported that the escape processes in
the wide range of iterations (50 to 100,000 iterations) can be modeled as a Kramers Escape Problem
very well. Our empirical results in Section 6 support this point again. Thus, Assumptions 2 and 3 are
reasonable in practice (see more discussions in Appendix B).
We next formulate how Momentum escapes minima as Theorem 3, which is based on Theorem 3.2 in
Xie et al. (2021b) and the effective diffusion correction for the phase-space Fokker-Planck Equation
(8) in Kalinay and Percus (2012). We analyze the mean escape time τ (Van Kampen, 1992; Xie et al.,
2021b) required for the escaping process from a loss valley. Suppose that the saddle point b is the
exit of Loss Valley a, and ∆L = L(b) - L(a) is the barrier height(See more details in Appendix E.).
Theorem 3 (Momentum Escapes Minima). Suppose Assumptions 1, 2, and 3 hold, and the dynamics
is governed by Momentum. Then the mean escape time from Loss Valley a to the outside of Loss
Valley a is given by
_	^ ^4∣Hbe∣l ]	1	Γ 2γMB∆L ( s	(1 - S)M
T=π W1+K+1J 西.exp [—η — E+η‰τJ ],
where the subscript e indicates the escape direction, s ∈ (0, 1) is a path-dependent parameter, and
Hae and Hbe are the eigenvalues of the Hessians of the loss function at the minimum a and the saddle
point b along the escape direction e. When 4YHM1《1, it reduces to SGD. In terms ofMomentum
notations, we have log(τ) = O (2(1-彳力"工).
Note that the escape direction is aligned with some eigenvector in the diffusion theory (Xie et al.,
2021b). We leave the proof in Appendix A.3. We note that log(τ) = O (23L) has been obtained
for SGD (Xie et al., 2021b). We see that Momentum does not affect flat minima selection in terms of
the mean escape time, if We properly choose the learning rate, i.e., ηMomentum = 1-β1 ηsGD.
4 Analysis of Adam Dynamics
Algorithm 1: Adam	Algorithm 2: Adai
ʌ , gt = VL(θt);	ʌ , gt = VL(θt);
mt = β1mt-1 + (1 - β1)gt;	vt = β2vt-1 + (1 - β2 )gt2 ;
vt = β2vt-1 + (1 - β2 )gt2 ;	V = Vt Vt = 1-β2 ；
m t = 1m⅛;	Vt = mean(Vt);
Vt = I⅜ ；	βι,t = (1- β0Vt)Clip(0,1-e);
θt+1 = θt - √t+m t;	mt = β1,t mt-1 + (1 - β1,t)gt;
	
	mt m t = 1-QZ=1 βι,z;
	θt+ι = θt 一 ηmt；
In this section, We analyze the effects of Adaptive Learning Rate in terms of saddle-point escaping
and flat minima selection.
A motivation behind Adaptive Learning Rate. The previous theoretical results naturally point us a
good Way to help escape saddle points: adaptively adjust the learning rates for different parameter as
η a |Hi|- 2, However, estimating the Hessian is expensive in practice. In Adam (Algorithm 1), the
diagonal v can be regarded as a diagonal approximation of the full matrix E[gtgt>] (Staib et al., 2019).
Following Staib et al. (2019), our analysis considers the “idealized” Adam where v = E[gtgt>] is a
full matrix. Note that E[gtg>] = C(θ) = B approximately holds near critical points.
The continuous-time Adam dynamics. The continuous-time dynamics of Adam can also be written
as Equation (6), except the mass M = ɪ-% and the damping coefficient Y = 1-β1. We emphasize
that the learning rate is not a real number but the diagonal approximation of the ideal learning rate
5
Under review as a conference paper at ICLR 2022
matrix η = ηC- 2 in practice. We apply the adaptive time continuation dti = η in the i-th dimension,
where & of T iterations is defined as the sum of ^^i of each iteration. Thus, the Fokker-Planck Equation
for Adam can still be written as Equation (8). We leave more details in Appendix H.
Adam escapes saddle points. Similarly to Theorem 2, we formulate how Adam escapes saddle point
as Proposition 1, which can be obtained from Theorem 2 and η = ηC- 1.
Proposition 1 (Adam Escapes Saddle Points). Suppose c is a critical point, Assumption 1 and
Equation (3) hold, the dynamics is governed by Adam, and the initial parameter is at the saddle point
θ = c. Then the mean squared displacement is written as
n.	n.	^π.+.
ai (ti)i = γ2M [1 - exp(-γti)] + γMHi [1 - exp(--Mf 儿
Under ∣Hi∣ηT《1, it can be written in terms of Adam notations as
2
h∆θ2i =	η [1 - exp (-(1- βι)τ)]2 + η2τ + O(PBHTη3τ2).
2(1 - β1)
From Proposition 1, we can see that Adam escapes saddle points fast, because both the momentum
drift and the diffusion effect are approximately Hessian-independent and isotropic near saddle points,
which is also supported by Staib et al. (2019). Proposition 1 indicates that one advantage of Adam
over RMSprop (Hinton et al., 2012) comes from the additional momentum drift effect on saddle-point
escaping.
Adam escapes minima. We next formulate how Adam escapes minima as Proposition 2, which
shows that Adam cannot learn flat minima as well as SGD. We leave the proof in Appendix A.4.
Proposition 2 (Adam Escapes Minima). The mean escape time of Adam only exponentially depends
on the square root of the eigenvalues of the Hessian at a minimum:
/	4ηpBHbj	| det(H-1Hb)∣4	Γ2√B∆L
T=π V1+ ι - βι	+1 ―Hbe—exp ~^Γ~
where τ is the mean escape time. Thus, we have log(τ)
C(2√B∆L、
O ∖η√Hae J
-1
In comparison, Adam has log(τ)=O(Hae2), while SGD and Momentum both have log(τ)=
O(Ha-e1). We say that Adam is weaker Hessian-dependent than SGD in terms of sharp minima
escaping, which means that if Hae increases, i.e., the escaping direction becomes steeper, the escaping
time log(τ) for SGD decreases more dramatically than log(τ) for Adam. Note that, in the diffusion
theory, the minima sharpness is reflected by Hae, the eigenvalue of the Hessian corresponding to
the escape direction along an eigenvector. The weaker Hessian-dependent diffusion term of Adam
hurts the efficiency of selecting flat minima in comparison with SGD. In summary, Adam is better at
saddle-point escaping but worse at flat minima selection than SGD.
Adam variants. Many variants of Adam have been proposed to improve performance, such as
AdamW (Loshchilov and Hutter, 2018), AdaBound (Luo et al., 2019), Padam (Chen and Gu, 2018),
RAdam (Liu et al., 2019), AMSGrad (Reddi et al., 2019), Yogi (Zaheer et al., 2018), and others (Shi
et al., 2021; DefOSSez et al., 2020; Zou et al., 2019). Many of them introduced extra hyperparameters
that require tuning effort. Most variants often generalize better than Adam, while they may still
not generalize as well as fine-tuned SGD (Zhang et al., 2019b), which we also discuss in Appendix
F. Moreover, they do not have theoretical understanding of minima selection. Loosely speaking,
our analysis suggests that Adam variants may also be poor at selecting flat minima due to Adaptive
Learning Rate.
5	Adaptive Inertia
In this section, we propose a novel adaptive inertia optimization framework.
Adaptive Inertia Optimization (Adai). The basic idea of Adai (Algorithm 2) comes from Theorems
2 and 3, from which we can see that parameter-wise adaptive inertia can achieve the approximately
6
Under review as a conference paper at ICLR 2022
Hessian-independent momentum drift without damaging flat minima selection. The total momentum
drift effect during passing through a saddle point in Theorem 2 is given by
h∆θii2
IHiIn2
2(1 - βι)B
(10)
We generalize the scalar β1 in Momentum to a vector β1 in Adai. The ideal adaptive updating
is to keep βι = 1 - βov, where the rescaling factor v, namely the mean of all elements in the
estimated V, could be used to make the optimizer more robust in practice. The default values of
β0 and β2 are 0.1 and 0.99, respectively. The hyperparameter is for avoiding extremely large
inertia. The default setting = 0.001 means the maximal inertia is 1000 times the minimal inertia, as
Mmax = η(1 - β1,max)-1 = η-1. Compared with Adam, Adai does not increase the number of
hyperparameters. Note that an existing “adaptive momentum” method (Wang and Ye, 2020) is not
parameter-wisely adaptive and essentially different from Adai.
The Fokker-Planck Equation associated with Adai dynamics in the phase space can be written as
Equation (8), except that we replace the mass coefficient and the damping coefficient by the mass
matrix and the damping matrix: M = η(I - diag(β1))-1 and γ = η-1(I - diag(β1)).
Adai escapes saddle points. Proposition 3 shows that Adai can escape saddle points efficiently due
to an isotropic and approximately Hessian-independent momentum drift, while the diffusion effect is
the same as Momentum. Proposition 3 is a direct result of Theorem 2.
Proposition 3 (Adai Escapes Saddle Points). Suppose c is a critical point, Assumption 1 and Equation
(3) hold, the dynamics is governed by Adai, and the initial parameter is at the saddle point θ = c.
Then the total momentum drift in the procedure of passing through the saddle point is given by
2
h∆θii2 =于
β0
Pn=I IHiIn2
β0 nB
where	in=1 IHiI is the trace norm of the Hessian at c.
Intuitively, the momentum drift effect of Adai can be significantly larger than Adam by allowing
larger inertia, which can be verified in experiments. We do not rigorously claim that Adai must
converge faster than or as fast as Adam. Instead, we will empirically compare Adai with Adam on
various datasets.
Adai escapes minima. We formulate how Adai escapes minima in Proposition 4, which shows Adai
can learn flat minima better than Adam. We leave the proof in Appendix A.5.
Proposition 4 (Adai Escapes Minima). The mean escape time of Adai exponentially depends on the
eigenvalues of the Hessian at a minimum:
τ=π
1+
4n Pi=ι IHbiI
βon
+1
1
IHbeiexp
2B∆L
n
+
(1-s)
IHbeI
where τ is the mean escape time. Thus, we have log(τ)
O Ie
_ 1
We can see that both SGD and Adai have log(τ) = O(Hae1), while Adam has log(τ) = O(Hae2).
Thus, Adai favors flat minima as well as SGD and better than Adam.
Convergence Analysis. Theorem 4 proves that Adai has similar convergence guarantees to SGD
Momentum(Yan et al., 2018; Ghadimi and Lan, 2013). The proof is relegated to Appendix A.6.
Theorem 4 (Convergence of Adai). Assume that L(θ) is an L-smooth function2, L is lower bounded
as L(θ) ≥ L?, E[ξ] = 0, E[kg(θ, ξ) - VL(θ)k2] ≤ δ2, and ∣∣VL(θ)k ≤ G for any θ, where ξ
represents the gradient noise of sub-sampling. Let Adai run fort + 1 iterations and β1,max = 1 - ∈
[0,1) for any t ≥ 0. If n ≤ C—, we have
t+1
min E[∣∣vL(θk)k2] ≤ √γ)ξξT(CI + C2 + C3),
k=0,...,t	t + 1
where C1 = (i"β0)maL)C, C2 = ⅛⅛axCF G2, and C3 = 2(1-L'ax)2 G +	'
2Itmeans that kVL(θa) - VL(θb)k ≤ L∣∣θa - θbk holds for any θa and θb.
7
Under review as a conference paper at ICLR 2022
0.06
0.32
0.30
-ea
0.24-
0.22
0.04-1-------------------------------------------------
O 25	50	75 100 J25 150 175 200
Epochs
80	100
O 20	40 CO
Epochs
25	50	75 100 125 150 175 2∞
Epochs
-OU3-$1
Figure 1: The learning curves on CIFAR-10
and CIFAR-100. Left: ResNet18 on CIFAR-10.
Right: ResNet34 on CIFAR-100.
ssσιf-u,JX
20	40	60	80	100
Epochs
0.3∞-
0.275-
0.250-
∣yηrr
10,,°j
SSon 6u-u-
0.175-	------1-----
0	25	50	75 100 J25 150 175 2∞
Epochs
25	50	75 100 125 UO 175 200
Epochs
M)Tr
0
Figure 2: Adai shows better Generalization in
the comparison with Momentum and Adam un-
der similar convergence speed. DenseNet121 on
CIFAR-100.
Adai SGD M Adam
Top1 23.20	23.51	27.13
Top5 6.62	6.82	9.18

Figure 3: ResNet50 on ImageNet. Left Subfigure: Top 1 Test Error. Right Subfigure: Training Loss.
Table: Top-1 and top-5 test errors. Note that the popular SGD baseline performance of ResNet50 on
ImageNet has the test errors as 23.85% in PyTorch and 24.9% in He et al. (2016), which are both
weaker than our SGD baseline.
We note that Adai is the base optimizer in the adaptive inertia framework, while Adam is the base
optimizer in the adaptive gradient framework. We can also combine Adai with stable/decoupled
weight decay (AdaiS/AdaiW) (Loshchilov and Hutter, 2018; Xie et al., 2020) (see more discussions
in Appendix G.) or other techniques from adaptive gradient methods.
6	Empirical Analysis
In this section, we first conduct experiments to compare Adai, Adam, and SGD with Momentum in
terms of convergence speed and generalization, and then empirically analyze flat minima selection.
Datasets: CIFAR-10, CIFAR-100(Krizhevsky and Hinton, 2009), ImageNet(Deng et al., 2009), and
Penn TreeBank(Marcus et al., 1993). Models: ResNet18/ResNet34/ResNet50 (He et al., 2016),
VGG16 (Simonyan and Zisserman, 2014), DenseNet121 (Huang et al., 2017), GoogLeNet (Szegedy
et al., 2015), and Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997b). More
details and results can be found in Appendix D and Appendix F.
O 1
W10-
eH 6Ee2
} 5∙985
■ 5.820
5.655
5.490
5.325
5.160
4.995
4.830
4.665
4.500
Io-2
10-β	10-s IO-4 IO-3
Weight Decay
Momentum
IO-5 IO-4 IO-3
Weight Decay
⅛- 5.985
1 5.820
i 5.655
-5.490
-5.325
-5.160
-4.995
( 4.830
卜665
1-4.500
seH 6-Ee2
Adam
IO-5	IO-3
Weight Decay
⅛- 5.985
i 5.820
5.655
-5.490
-5.325
5.160
-4.995
4.830
F 4.665
0- 4.500

Figure 4: The test errors of ResNet18 on CIFAR-10 under various learning rates and weight decay.
Adai has a much deeper and wider blue region near dark points (≤ 4.83%) than SGD with Momentum
and Adam.
8
Under review as a conference paper at ICLR 2022
Figure 5: Flat Minima Selection: Adai ≈ M omentum Adam. The log-scale mean escape
time - log(Γ) with the 95% confidence interval is displayed. We empirically verify that - log(Γ) =
O(k-1) holds for Momentum and Adai but does not hold for Adam. Instead, we observe that
一 log(Γ) = O(k-2) holds better for Adam. While Adai and Momentum favor flat minima similarly,
Adai may escape loss valleys slightly faster than Momentum.
Generalization and Convergence Speed. We display the test performance of all models on CIFAR
in Table 1. Table 1 and Figure 1 shows that Adai has excellent generalization compared with
other popular optimizers on CIFAR-10/CIFAR-100. Figure 2 further demonstrates that Adai can
consistently generalize better than SGD with Momentum and Adam, while maintaining similarly fast
or faster convergence, respectively.
Image Classification on ImageNet. Figure 3 shows that Adai generalizes significantly better than
SGD and Adam by training ResNet50 on ImageNet.
Robustness to the hyperparameters. Figure 4 demonstrates that Adai not only has better optimal
test performance, but also has a wider test error basin than SGD with Momentum and Adam. It means
that Adai is more robust to the choices of learning rates and weight decay.
The Mean Escape Time Analysis. We empirically study how the escape rate Γ, which equals to the
inverse mean escape time, depends on the minima sharpness for different optimizers in Figure 5. We
use Styblinski-Tang Function as the test function which has clear boundaries between loss valleys. Our
method for adjusting the minima sharpness is to multiply a rescaling factor √k to each parameter, and
the Hessian will be proportionally rescaled by a factor k. If we let L(θ) = f (θ) → L(θ) = f (√kθ),
then H(θ) = V2f (θ) → H(θ) = kV2f (θ). Thus, we can use k to indicate the relative minima
sharpness. We leave more details in Appendix E.
From Figure 5, it is easy to see that we have log(τ) = O(Ha-e1) in Adai and SGD, while we
-1
have log(τ)=O(Hae2) in Adam. Moreover, Adam is significantly less dependent on the minima
sharpness than both Momentum and Adai.
Supplementary Experiments. (1) It is known that training of language models requires Adaptive
Learning Rate (Zhang et al., 2019b). We will leave fusing Adaptive Learning Rate and Adaptive
Inertia together as future work, which may be interesting for Transformer-based models (Vaswani
et al., 2017). We leave the results of training LSTM on Penn TreeBank in Figure 12 of Appendix F.
(2) In Appendix F, we used the expected minima sharpness in Neyshabur et al. (2017) and empirically
verified again that Adai and Momentum can learn flatter minima than Adam.
7 Conclusion
To the best of our knowledge, we are the first to theoretically disentangle the effects of Adaptive
Learning Rate and Momentum in terms of saddle-point escaping and flat minima selection. Under
reasonable assumptions, our theory explains why Adam is good at escape saddle points but not
good at selecting flat minima. We further propose a novel optimization framework, Adai, which can
parameter-wisely adjust the momentum hyperparameter. Supported by good theoretical properties,
Adai can accelerate training and favor flat minima well at the same time. Our empirical analysis
demonstrates that Adai generalizes significantly better than popular Adam variants and SGD.
9
Under review as a conference paper at ICLR 2022
References
An, G. (1996). The effects of adding noise during backpropagation training on a generalization
performance. Neural computation, 8(3):643-674.
Balakrishnan, V. (2008). Elements of nonequilibrium statistical mechanics, volume 3. Springer.
Berglund, N. (2013). Kramers’ law: Validity, derivations and generalisations. Markov Processes and
Related Fields, 19(3):459-490.
Chen, J. and Gu, Q. (2018). Closing the generalization gap of adaptive gradient methods in training
deep neural networks. arXiv preprint arXiv:1806.06763.
Coffey, W. and Kalmykov, Y. P. (2012). The Langevin equation: with applications to stochastic
problems in physics, chemistry and electrical engineering, volume 27. World Scientific.
Daneshmand, H., Kohler, J., Lucchi, A., and Hofmann, T. (2018). Escaping saddles with stochastic
gradients. In International Conference on Machine Learning, pages 1155-1164.
Dauphin, Y. N., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., and Bengio, Y. (2014). Identifying
and attacking the saddle point problem in high-dimensional non-convex optimization. Advances in
Neural Information Processing Systems, 27:2933-2941.
Defossez, A., Bottou, L., Bach, F., and Usunier, N. (2020). On the convergence of adam and adagrad.
arXiv preprint arXiv:2003.02395.
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pages 248-255. Ieee.
Eyring, H. (1935). The activated complex in chemical reactions. The Journal of Chemical Physics,
3(2):107-115.
Ghadimi, S. and Lan, G. (2013). Stochastic first-and zeroth-order methods for nonconvex stochastic
programming. SIAM Journal on Optimization, 23(4):2341-2368.
Hanggi, P. (1986). Escape from a metastable state. Journal of Statistical Physics, 42(1-2):105-148.
Hanggi, P., Talkner, P., and Borkovec, M. (1990). Reaction-rate theory: fifty years after kramers.
Reviews of modern physics, 62(2):251.
HaoChen, J. Z., Wei, C., Lee, J., and Ma, T. (2021). Shape matters: Understanding the implicit bias
of the noise covariance. In Conference on Learning Theory, pages 2315-2357. PMLR.
Hardt, M., Recht, B., and Singer, Y. (2016). Train faster, generalize better: Stability of stochastic
gradient descent. In International Conference on Machine Learning, pages 1225-1234.
He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778.
Hinton, G., Srivastava, N., and Swersky, K. (2012). Neural networks for machine learning lecture 6a
overview of mini-batch gradient descent.
Hochreiter, S. and Schmidhuber, J. (1995). Simplifying neural nets by discovering flat minima. In
Advances in neural information processing systems, pages 529-536.
Hochreiter, S. and Schmidhuber, J. (1997a). Flat minima. Neural Computation, 9(1):1-42.
Hochreiter, S. and Schmidhuber, J. (1997b). Long short-term memory. Neural computation, 9(8):1735-
1780.
Hu, W., Li, C. J., Li, L., and Liu, J.-G. (2019). On the diffusion approximation of nonconvex
stochastic gradient descent. Annals of Mathematical Sciences and Applications, 4(1):3-32.
10
Under review as a conference paper at ICLR 2022
Huang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q. (2017). Densely connected con-
volutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 4700-4708.
Jastrzkebski, S., Kenton, Z., Arpit, D., Ballas, N., Fischer, A., Bengio, Y., and Storkey, A. (2017).
Three factors influencing minima in sgd. arXiv preprint arXiv:1711.04623.
Jin, C., Ge, R., Netrapalli, P., Kakade, S. M., and Jordan, M. I. (2017). How to escape saddle points
efficiently. In International Conference on Machine Learning, pages 1724-1732. PMLR.
Kalinay, P. and Percus, J. K. (2012). Phase space reduction of the one-dimensional fokker-planck
(kramers) equation. Journal of Statistical Physics, 148(6):1135-1155.
Kingma, D. P. and Ba, J. (2015). Adam: A method for stochastic optimization. 3rd International
Conference on Learning Representations, ICLR 2015.
Kramers, H. A. (1940). Brownian motion in a field of force and the diffusion model of chemical
reactions. Physica, 7(4):284-304.
Krizhevsky, A. and Hinton, G. (2009). Learning multiple layers of features from tiny images.
LeCun, Y. (1998). The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/.
Li, Q., Tai, C., et al. (2017). Stochastic modified equations and adaptive stochastic gradient algorithms.
In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 2101-
2110. JMLR. org.
Li, Q., Tai, C., and Weinan, E. (2019). Stochastic modified equations and dynamics of stochastic
gradient algorithms i: Mathematical foundations. J. Mach. Learn. Res., 20:40-1.
Li, Z., Malladi, S., and Arora, S. (2021). On the validity of modeling sgd with stochastic differential
equations (sdes). arXiv preprint arXiv:2102.12470.
Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and Han, J. (2019). On the variance of the
adaptive learning rate and beyond. In International Conference on Learning Representations.
Loshchilov, I. and Hutter, F. (2018). Decoupled weight decay regularization. In International
Conference on Learning Representations.
Luo, L., Xiong, Y., Liu, Y., and Sun, X. (2019). Adaptive gradient methods with dynamic bound of
learning rate. 7th International Conference on Learning Representations, ICLR 2019.
Mandt, S., Hoffman, M. D., and Blei, D. M. (2017). Stochastic gradient descent as approximate
bayesian inference. The Journal of Machine Learning Research, 18(1):4873-4907.
Marcus, M., Santorini, B., and Marcinkiewicz, M. A. (1993). Building a large annotated corpus of
english: The penn treebank.
Neelakantan, A., Vilnis, L., Le, Q. V., Sutskever, I., Kaiser, L., Kurach, K., and Martens, J. (2015).
Adding gradient noise improves learning for very deep networks. arXiv preprint arXiv:1511.06807.
Neyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro, N. (2017). Exploring generalization in
deep learning. In Advances in Neural Information Processing Systems, pages 5947-5956.
Nguyen, T. H., Simsekli, U., Gurbuzbalaban, M., and Richard, G. (2019). First exit time analysis of
stochastic gradient descent under heavy-tailed gradient noise. In Advances in Neural Information
Processing Systems, pages 273-283.
Pawitan, Y. (2001). In all likelihood: statistical modelling and inference using likelihood. Oxford
University Press.
Radpay, P. (2020). Langevin equation and fokker-planck equation.
Reddi, S., Zaheer, M., Sra, S., Poczos, B., Bach, F., Salakhutdinov, R., and Smola, A. (2018). A
generic approach for escaping saddle points. In International Conference on Artificial Intelligence
and Statistics, pages 1233-1242. PMLR.
11
Under review as a conference paper at ICLR 2022
Reddi, S. J., Kale, S., and Kumar, S. (2019). On the convergence of adam and beyond. 6th
International Conference on Learning Representations, ICLR 2018.
Risken, H. (1996). Fokker-Planck equation. In The Fokker-Planck Equation, pages 63-95. Springer.
Risken, H. and Eberly, J. (1985). The fokker-planck equation, methods of solution and applications.
Journal of the Optical Society of America B Optical Physics, 2(3):508.
Sato, I. and Nakagawa, H. (2014). Approximation analysis of stochastic gradient langevin dynamics
by using fokker-planck equation and ito process. In International Conference on Machine Learning,
pages 982-990.
Shi, N., Li, D., Hong, M., and Sun, R. (2021). {RMS}prop can converge with proper hyper-parameter.
In International Conference on Learning Representations.
Simonyan, K. and Zisserman, A. (2014). Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556.
Simsekli, U., Sagun, L., and Gurbuzbalaban, M. (2019). A tail-index analysis of stochastic gradient
noise in deep neural networks. In International Conference on Machine Learning, pages 5827-
5837.
Staib, M., Reddi, S., Kale, S., Kumar, S., and Sra, S. (2019). Escaping saddle points with adaptive
gradient methods. In International Conference on Machine Learning, pages 5956-5965. PMLR.
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and
Rabinovich, A. (2015). Going deeper with convolutions. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages 1-9.
Van Kampen, N. G. (1992). Stochastic processes in physics and chemistry, volume 1. Elsevier.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, E., and
Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing
systems, pages 5998-6008.
Wang, B. and Ye, Q. (2020). Stochastic gradient descent with nonlinear conjugate gradient-style
adaptive momentum. arXiv preprint arXiv:2012.02188.
Wang, J.-K., Lin, C.-H., and Abernethy, J. (2019). Escaping saddle points faster with stochastic
momentum. In International Conference on Learning Representations.
Welling, M. and Teh, Y. W. (2011). Bayesian learning via stochastic gradient langevin dynamics. In
Proceedings of the 28th international conference on machine learning (ICML-11), pages 681-688.
Wilson, A. C., Roelofs, R., Stern, M., Srebro, N., and Recht, B. (2017). The marginal value of
adaptive gradient methods in machine learning. In Advances in Neural Information Processing
Systems, pages 4148-4158.
Wu, L., Ma, C., and Weinan, E. (2018). How sgd selects the global minima in over-parameterized
learning: A dynamical stability perspective. In Advances in Neural Information Processing Systems,
pages 8279-8288.
Xie, Z., He, F., Fu, S., Sato, I., Tao, D., and Sugiyama, M. (2021a). Artificial neural variability for deep
learning: On overfitting, noise memorization, and catastrophic forgetting. Neural Computation.
Xie, Z., Sato, I., and Sugiyama, M. (2020). Stable weight decay regularization. arXiv preprint
arXiv:2011.11152.
Xie, Z., Sato, I., and Sugiyama, M. (2021b). A diffusion theory for deep learning dynamics:
Stochastic gradient descent exponentially favors flat minima. In International Conference on
Learning Representations.
Xie, Z., Yuan, L., Zhu, Z., and Sugiyama, M. (2021c). Positive-negative momentum: Manipulating
stochastic gradient noise to improve generalization. In International Conference on Machine
Learning, volume 139 of Proceedings of Machine Learning Research, pages 11448-11458. PMLR.
12
Under review as a conference paper at ICLR 2022
Xu, P., Chen, J., Zou, D., and Gu, Q. (2018). Global convergence of langevin dynamics based
algorithms for nonconvex optimization. In Advances in Neural Information Processing Systems,
Pages 3122-3133.
Yan, Y., Yang, T., Li, Z., Lin, Q., and Yang, Y. (2018). A unified analysis of stochastic momentum
methods for deeP learning. In IJCAI International Joint Conference on Artificial Intelligence.
Zaheer, M., Reddi, S., Sachan, D., Kale, S., and Kumar, S. (2018). AdaPtive methods for nonconvex
oPtimization. In Advances in neural information processing systems, Pages 9793-9803.
Zaremba, W., Sutskever, I., and Vinyals, O. (2014). Recurrent neural network regularization. arXiv
preprint arXiv:1409.2329.
Zavriev, S. and Kostyuk, F. (1993). Heavy-ball method in nonconvex oPtimization Problems. Compu-
tational Mathematics and Modeling, 4(4):336-341.
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2017). Understanding deeP learning
requires rethinking generalization. In International Conference on Machine Learning.
Zhang, G., Li, L., Nado, Z., Martens, J., Sachdeva, S., Dahl, G., Shallue, C., and Grosse, R. B.
(2019a). Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic
model. In Advances in Neural Information Processing Systems, Pages 8196-8207.
Zhang, J., He, T., Sra, S., and Jadbabaie, A. (2019b). Why gradient cliPPing accelerates training: A
theoretical justification for adaPtivity. In International Conference on Learning Representations.
Zhou, H.-X. (2010). Rate theories for biologists. Quarterly reviews of biophysics, 43(2):219-293.
Zhou, M., Liu, T., Li, Y., Lin, D., Zhou, E., and Zhao, T. (2019). Toward understanding the imPortance
of noise in training neural networks. In International Conference on Machine Learning.
Zhou, P., Feng, J., Ma, C., Xiong, C., Hoi, S. C. H., et al. (2020). Towards theoretically understanding
why sgd generalizes better than adam in deeP learning. Advances in Neural Information Processing
Systems, 33.
Zhu, Z., Wu, J., Yu, B., Wu, L., and Ma, J. (2019). The anisotroPic noise in stochastic gradient
descent: Its behavior of escaPing from sharP minima and regularization effects. In ICML, Pages
7654-7663.
Zou, F., Shen, L., Jie, Z., Zhang, W., and Liu, W. (2019). A sufficient condition for convergences of
adam and rmsProP. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, Pages 11127-11135.
A Proofs
A.1 Proof of Theorem 1
Proof. It is easy to validate that the Probability density function
PGt) = p(2π)n det(Σ(t)) exP (— 2(θ -C)Ft)(0 -C))	(11)
is the solution of the Fokker-Planck Equation (2). Without losing generality, we only validate
one-dimensional solution, such as Dimension i.
The first term in Equation (2) can be written as
∂P(θ,t) -∂t-二	1 1	1	θ2	∂σ2	1	θ2	θ2 ∂σ2 一2√2πσ2σ2 exp --2σ2) -∂Γ + √2πσ2exp -~2σ2) 2σ4~∂Γ	(12) I (g-⅛) Pett M.	(13)
13
Under review as a conference paper at ICLR 2022
The second term in Equation (2) can be written as
V ∙ [P(θ,t)VL(θ)] = P(θ)H + Hθ / 1 exp
2πσ2
θ2
1 - σj P(θ,t).
The third term in Equation (2) can be written as
σ2 θ2
DV2P (θ,t) = -D--= exp
σ5 2π
=D( σ - ⅛)P (θ,t).
By T erm1 = T erm2 + T erm3, we have
1 (θ2	/ dσ2
2(θ - σ )F
∂σ2
∂t
Hσ2(σ2 - θ2) + D(θ2 - σ2)
2D - 2Hσ2 .
(14)
(15)
(16)
(17)
(18)
(19)
The initial condition ofσ2 is given by σ2 (0) = 0. We can validate thatσ2 satisfies
D
σi ⑴=~EJ~[1 - exp(-2Hit)].	QO)
Hi
It is true along all eigenvectors’ directions.
By D = 2B H, we can get the results of SGD diffusion:
σ2(T) = Sign(Hi)ɪ[1 - exp(-2HiηT)]	(21)
2B
□
A.2 Proof of Theorem 2
Proof. Without losing the generality, we consider the one-dimensional case that aligns with an
eigenvector of the Hessian. The stationary solution in equilibrium to the phase-space Fokker-Planck
Equation near a critical point is given by a canonical ensemble
Peq(θ, r) = ZTeXp[-β(L(θ) + 2Mr2)],
(22)
where Z is a partition function and the inverse temperature β = γM D-1. This result is famous in a
large number of physics literature (Van Kampen, 1992; Risken, 1996; Balakrishnan, 2008). Thus, we
have the equilibrium distribution in velocity space and position space as
Peq (r) = Zr-1
exp(-
βMr2
2
(23)
H
)
and
Peq(θ) = Zr-1 exp(-βL(θ)),
(24)
respectively, where Zr is the partition function for Peq(r) and Zθ is the partition function for Peq(θ).
It is known that the phase-space probability density solution P(θ, r, t) can be obtained from the
position-space solution P(θ, t) and the velocity-space solution P(r, t). Thus, we may compute the
velocity-space solution (in Step I) and the position-space solution (in Step II), respectively.
Step I:
14
Under review as a conference paper at ICLR 2022
We further argue that the velocity-space solution P (r, t) has reached its equilibrium Boltzmann
distribution Peq (r) when the particle is passing saddle points. This is reasonable because the
equilibrium solution in the velocity space is stable under Assumption 1. The velocity distribution
P(r, t) must be the solution of the free diffusion equation in the velocity space. We can approximately
ignore the gradient expectation term 一 dLθθ) in dynamical equations near critical points, as the
gradient expectation is much smaller the gradient noise scale near critical points. Near critical points,
the velocity r obeys the equilibrium distribution
P(r,t) ≈ Peq(r) = (2∏)-2 det(βM)2 exp(-BMr ),	(25)
where n = 1 in one-dimensional case. This is a classical Boltzmann distribution. The expected
velocity (also called “equilibrium velocity”) along dimension i can be given by r',》=YM■.
Step II:
As we discussed in Theorem 1, the position-space distribution in equilibrium is time-dependent near
saddle points. Following the form of the solution to the position-space Fokker-Planck Equation (Seen
in Appendix A.1), the ansatz solution of P(θ, t) is given by
(
P(θ,t)= √(2π)n det(∑(t)) exp (T(θ-c⑻τ∑t)(θ -c(t)))
Σ(t) = Udiag(σ12(t), . . . ,σn2-1(t),σn2(t))U>
The time-dependent components in P(θ, t) are caused by two effects. The first effect is the momentum
drift effect (due to equilibrium velocity), which decides c(t), the center position of the probability
density. As we studied in Theorem 1, the second effect is the diffusion effect (due to random noise),
which decides the covariance of the probability density, Σ(t).
The momentum drift effect is governed by the motion equation without random noise, while the
diffusion effect is governed by the diffusion equation where gradient noise dominates gradient mean.
Step II (a): the momentum drift effect.
We can write the dynamics of the momentum drift effect as
MC(t) = -γMC(t).	(26)
We also have ignored the conservative force -H[c(t) 一 c(0)] near saddle points, as |Hi| 《YM
exists for ill-condition Hessian eigenvalues. We focus on the behaviors near ill-conditioned saddle
points, where Hessian eigenvalues are small along the escape directions.
The initial condition is given by c(0) = c, C(0) = req, and c(0) = -Yreq. Then We can obtain the
solution c(t) as
ci(t) = ci + ɪIeq [1 - exp(-Yt)].	(27)
Y
Step II (b): the diffusion effect.
We can get the dynamics of the diffusion effect as
γMdθ = - dL(θ) dt + [2D(θ)] 2 dWt.	(28)
∂θ
This is equivalent to SGD dynamics with η = γM. The expression of P(θ, t) and σ2 (t) is directly
given by Theorem 1 as
σ2(t) = γMHi [1- exp(-2HM)].	(29)
We combine the momentum drift effect and the diffusion effect together, and then obtain the mean
squared displacement of θ as
(δθ2⑴)=(Ci⑴-ci)2 + σ2⑴=H 3MM 2 [1 - exp(-Yt)F + YM H [1 - exp(- ~MMj~ )]. (3O)
15
Under review as a conference paper at ICLR 2022
We respectively introduce the notations of Adam-style Momentum and SGD-style Momentum, and
apply the second order Taylor expansion in case of small - 2γMt. Then We obtain
h∆θ2i = “叫：R [1 - exp (-(1- βι)T)]2 + Wt + O(BTH2η3T2)	(31)
2(1 - β1)B	B
for Adam-style Momentum, and
h∆θ2i = 2(1Hi:23B [1 - exp(-(1 - βι)T)]2 + BHiPβT)2 + O(B-1H2η3T2)	(32)
for SGD-style Momentum.
□
A.3 Proof of Theorem 3
Proof. The proof closely relates to the proof of Theorem 3.2 in Xie et al. (2021b) and a physics Work
(Kalinay and Percus, 2012).
We first discover hoW SGD dynamics differs from Momentum dynamics in terms of escaping loss
valleys. In this approach, We may transform the proof for Theorem 3.2 of Xie et al. (2021b) into the
proof for Theorem 3 With the effective diffusion correction. We use J and j to denote the probability
current and the probability flux respectively. According to Gauss Divergence Theorem, We may
reWrite the Fokker-Planck Equation (8) as
SPTtt = - r ∙VθP(θ, r,t) + VθL(θ) ∙ MTV,P(θ, r,t)
+ Vr ∙ M-2D(θ) ∙ Peq (r)V r[Peq (r)-1P (θ,r,t)]
=-V∙ J (θ,t).
(33)
(34)
We Will take similar forms of the proof in Xie et al. (2021b) as folloWs. The mean escape time is
Written as
P(θ ∈ Va)
RSaJ ∙ dS
(35)
To compute the mean escape time, We decompose the proof into tWo steps: 1) compute the probability
of locating in valley a, P(θ ∈ Va), and 2) compute the probability flux j = S J ∙ dS. The definition
of the probability flux integral may refer to Gauss Divergence Theorem.
Step 1:
Fortunately, the stationary probability distribution inside Valley a in Momentum dynamics is also
given by a Gaussian distribution in Theorem 1 as
σi2(t)
Di
YMHi
(36)
Under Quasi-Equilibrium Assumption, the distribution around minimum a is P(θ)
P (a) exp h-YM (θ — a)> (Da 2 HaDa 2 )(θ — a)]
We use the T notation as the temperature param-
eter in the stationary distribution, and use the D notation as the diffusion coefficient in the dynamics,
for their different roles.
P(θ ∈ Va)
P (θ)dV
θ∈Va
=P (a)	exp
' YM ,-	, ɪ , _ _1 __ _ _1_	,"	___
---2-(θ — a) (Da HaDa )(θ — a) dV
≈P (a)
θ∈(-∞,+∞)
exp
-12 (θ - a)> (Da 2 HaDa 2 )(θ - a)
dV
=P (a)	(2πYM)2
一(Jdet(D-1Ha)1
(37)
(38)
(39)
(40)
(41)
16
Under review as a conference paper at ICLR 2022
This result of P(θ ∈ Va) in Momentum only differs from SGD by the temperature correction γM.
Step 2: We directly introduce the effective diffusion result from (Kalinay and Percus, 2012) into our
analysis. Kalinay and Percus (2012) proved that the phase-space Fokker-Planck Equation (8) can
be reduced to a space-dependent Smoluchowski-like equation, which is extended by an effective
diffusion correction:
ʌ ,
D i(θ) = Di(θ)
(42)
As we only employ the Smoluchowski Equation along the escape direction, we use the one-
dimensional expression along the escape direction (an eigenvector direction) for simplicity. Without
losing clarity, we use the commonly used T to denote the temperature in the proof.
In case of SGD (Xie et al., 2021b), we can obtain Smoluchowski Equation in position space:
J = D(θ) exp (-TP) V 卜Xp (军)P(θ)
(43)
where T = D. According to (Kalinay and Percus, 2012), in case of finite inertia, we can transform
the phase-space equation into the position-space Smoluchowski-like form with the effective diffusion
correction:
J = D(θ) exp (-TP) V [exp (Lθ)) P(θ)
(44)
where T = γM , and D defined by Equation (42) replaces standard D.
We assume the point s is a midpoint on the most possible path between a and b, where L(s) =
(1 - s)L(a) + sL(b). The temperature Ta dominates the path a → s, while temperature Tb dominates
the path s → b. So we have
V exp (Lθ⅛^ )p(θ)]= JD-1 exp (Lθ⅛^).
(45)
We integrate the equation from Valley a to the outside of Valley a along the most possible escape path
Left = ZC ∂∂θ [exp (L⑹ T L(S) ) P(θ)Mθ	刖
a ∂θ	T
=ZS ⅛ [exp (LV(S))P(Θ)] dθ	(47)
a ∂θ	Ta
+ /c ⅛ [exp (LV^) P(θ)l dθ	(48)
s ∂θ	Tb
= [P(s)-exp (L(a)-L(S)) P(a)] + [0 - P(s)]	(49)
=-exp ( L(a T L(S) ) P (a)	(50)
Right = - J/c D-1 exp (L(θ) - L(S)) dθ
(51)
We move J to the outside of integral based on Gauss’s Divergence Theorem, because J is fixed
on the escape path from one minimum to another. As there is no field source on the escape path,
JV V ∙ J(θ)dV = 0 and VJ(θ) = 0. Obviously, probability sources are all near minima in deep
learning. So we obtain
exp
L(a)-L(s)
J=
RaD T exp
P(a)
L ⑻ TL(S)) dθ
(52)
17
Under review as a conference paper at ICLR 2022
Near saddle points, we have
/
a
≈Z
a
C D)-1exp
DD-1 exp
“)- L(S) )dθ
L(b) - L(s) + 2(θ - b)>Hb(θ - b
Tb
dθ
(53)
(54)
Z+∞
exp
∞
=D -1 exp
- L(s)
Besides the temperature correction T
L(b - L(s) + 2(θ - b)>Hb(θ - b)
Tb
dθ
(55)
Tb
=YM
2∏Tb
两.
(56)
, this result of J in Momentum also differs from SGD by
the effective diffusion correction Db. The effective diffusion correction coefficient is given by
D i(e)_ 1 - q - 4HM
Di(θ)
2Hi (θ)
γ2M
(57)
Based on the formula of the one-dimensional probability current and flux, we obtain the high-
dimensional flux escaping through b:
J J ∙ dS
Sb
=J1d	exp
Sb
— γM-θ - b)>[D-1 HbD-2]⊥e(θ — b)l dS
(2πγM) n-1
=1d (Qi=e(D-1Hbi)) 1
n-1
Ls)-L(S)) P ( )	(2nYM) F
一	Tae	P ⑷(Qi=e(%1 Hbi)) 2
DD-e1 exP
L(b)-L(s)∖	∕2πTbe
Tbe
THU
(58)
(59)
(60)
(61)
where [∙]⊥e indicates the directions perpendicular to the escape direction e.
Based on the results of Step 1 and Step 2, we have
=P(θ ∈ %)
RSb j ∙ dS
(62)
=P (a)
(2πγM) 2
DD-1 ex∏ (L(B)-L(S)
Dbe exp〈	Tbe
det(D-1Ha) 2 (SX" L(a)-L(s)
exp〈-Tae-
工∏用exp
D be	lHbel
"/] JIHbeI.;
V1 + R + 1
2γMB∆L
η
n-1
)P (a)	(2nYM) F ι
(	(Πi=e(Dbi1Hbi)) 2
+」ʌ]
, |Hbe| )∖
(63)
(64)
1
|H：iexp
2γMB∆L
η
,(1 — s)
+ |Hbe|
(65)
π
We have replaced the eigenvalue of Hb along the escape direction by its absolute value.
Finally, by introducing γ and M, we obtain the log-scale expression as
log(τ)= O (2(1「HBM)
β3 η Hae
(66)
□
18
Under review as a conference paper at ICLR 2022
A.4 Proof of Proposition 2
Proof. The proof closely relates to the proof of 3. We only need replace the standard learning rate by
the adaptive learning rate η = ηv- 2, and set YM = 1. Particularly,
C _ rι -1 _ η[H]+v-2
Dadam = Dv 2 = ZVJ
2B
1N H+
We introduce η = ηv- 1 into the proof of 3, and obtain
P(θ ∈ Va)
τ
JSb J ∙ dS
π
1
2
π
(67)
(68)
,	1 _	1_	1	..
| det(D-1Vb2 Hb) |
1	1
L det(D-1Va2 Ha)I
"∕1 , 4∣Hbel, 1'
V1 + R + 1
1
∣He∣exp
2γMB∆L
S	,	(I — S)
_1	+	_ 1
Ke2 Hae	V-2 ∣H
(69)
))
PpiHI + 1
∣ det(H-1Hb)∣1
∣Hbe∣
exp
η
2 √B ∆L
η
. (70)
□
A.5 Proof of Proposition 4
Proof. The proof closely relates to the proof of 3. We only need introduce the mass matrix M and
the dampening matrix γ. Fortunately, γM = I . Thus we directly have the result from the proof of 3
as:
P(θ∈Va)
T ---------
RSbJ ∙ dS
π
(71)
"∕1 , 4∣Hbel	；
V1 + F + 1
1
∣He∣exp
2γMB∆L
η
+ (1-s)
+ ∣Hbe∣
(72)
As M = η(I — βι)-1, γ = η-1(I - βι), and I - βι = βv, We obtain the result:
τ=π
1+
4η Pn=I ∣Hbi∣
βon
+1
1
∣Helexp
2B∆L
η
+
(1-s)
∣Hbe∣
(73)
□
A.6 Proof of Theorem 4
Without loss of generality, We assume the dimensionality is one and reWrite the main updating rule of
Adai as
θt+1 =θt -η(1 - β1,t)gt + β1,t(θt - θt-1).	(74)
In the convergence proof, We do not need to specify hoW to update β1,t but just let β1,t ∈ [0, 1). We
denote that θ-1 = θ0.
Before presenting the main proof, We first prove four useful lemmas.
Lemma 1. Under the conditions of Theorem 4, for any t ≥ 0, we have
t
xt+1 - xt = -η	qk,tgt,
k=0
19
Under review as a conference paper at ICLR 2022
where
t
qk,t = (1 - βk) Y β1,i .
i=k+1
Then we have
t
1 - β1t+,m1ax ≤ X qk,t ≤ 1.
k=0
Proof. Recall that
θt+1 =θt - η(1 - β1,t)gt + β1,t(θt - θt-1)	(75)
θt+1 -θt =β1,t(θt -θt-1) - η(1 - β1,t)gt.	(76)
Then we have
tt
θt+1 - θt = -η X(1 - β1,k)gk Y β1,i.	(77)
k=0	i=k+1
Let qk,t = (1 - βk) Qit=k+1 β1,i.
For analyzing the maximum and the minimum, we calculate the derivatives with respect to β1,k for
any 0 ≤ k ≤ t:
∂ Pk=o qk,t
∂β1,0
t
- Y β1,i ≤ 0.
i=k+1
(78)
Note that 0 ≤ β1,k ≤ β1,max. Then we have
t
Eqk,tlβl,0=βl,max
k=0
tt
≤ ∑>,t ≤ ∑>,t∣βι,0=0.
k=0	k=0
(79)
Recursively, we can calculate the derivatives with respect to β1,1, β1,2, . . . , β1,t.
Then we obtain max(Ptk=0 qk,t) = 1 by letting β1,k = 0 for all k.
Similarly, we obtain min(Ptk=0 qk,t) = 1 by letting β1,t = β1,max for all k.
Then we obtain
t
1 - β1t+,m1ax ≤Xqk,t ≤1	(80)
k=0
The proof is now complete.
□
Lemma 2. Under the conditions of Theorem 4, for any t ≥ 0, we have
t	t-1
E[L(θt+ι) - L(θt)] ≤2Xqk,tE[X ∣∣vL(θj+ι) — L(θj)k2]+
k=0	j=k
t t k	Lη2	t
X qk,t( 2 η - n)E[k VL(Ok) k ]+	2 k X qk,tgk k .
k=0	k=0
20
Under review as a conference paper at ICLR 2022
Proof. As L(θ) is L-smooth, we have
	L(θt+1) - L(θt)	(81) L ≤VL(θt)>(θt+ι - θt) + Ekθt+ι — θtk2	(82) t	L2 t = -ηfqk,tVL(θt>gk + Lη- k 5>,tgk k2	(83) k=0	k=0 t	t-1 = - η X qk,t X(VL(θj+1) - VL(θj))> (VL(θk) + ξk)- k=0	j=k t	Lη2	t η∖^qk,tvL(θk )τ(vL(θk ) + ξk ) +	2~ Il	qk,tgkk ∙	(84) k=0	k=0
Taking expectation on both sides gives
	E[L(θt+1)-L(θt)]	(85) t	t-1 ≤2L Xqk,tE[X(vL(θj+ι) - vL(θj))>vL(θk)]-	(86) k=0	j=k t	L2 t ]Tqk,tηE[∣vL(θk)k2] + 于k Eqk,tgkk2.	(87) k=0	k=0
By the Cauchy-Schwarz Inequality, we have
	E[L(θt+1) - L(θt)]	(88) t	1	η2 ≤ ∑qk,tE[2 kvL(θt) - vL(θk k2 + η2 kvL(θk )k2 ]- k=0 t	2t ηXqk,tE[∣vL(θk)k2] + LrE[k Xqk,tgkk2]	(89) k=0	k=0 t	t-1 =2 Xqk,tE[∑ kvL(θj+ι) - L(θj)k2] + k=0	j=k t	t k	Lη2	t T？qk,t( 2 η - η)E[kvL(θk)k ] + ^-2~k y^qk,tgkk .	(9O) k=0	k=0
The proof is now complete.	□
Lemma 3. Under the conditions of Theorem 4, for any t ≥ 0, we have
	X qk,t(t- k) ≤ 1-「 k=0	1,max
Proof. For analyzing the maximum, we calculate the derivatives with respect to β1,k for any 0 ≤
k ≤ t.
With respect to β1,1, we have
Then we let β1,0	d Pk=0qk，t(t-k) = - Y! βι,kt ≤ 0.	(91) ∂β1,0	k=1 = 0 and obtain the derivative with respect to β1,1 as d Pk=∂ βkft- k) = I⅛ βι,k (t - (t - 1)) ≥ 0.	(92)
21
Under review as a conference paper at ICLR 2022
Then we let β1,j = β1,max for 1 ≤ j ≤ k - 1 and obtain the derivative with respect to β1,k as
∂ Ptk=0 qk,t(t - k)
∂β1,k
Thus, we have β1,0 = 0 and β1,k = β1,maX
write it as
≥ (t - k)
∂ Ptk=0 qk,t
∂β1,k
0.
(93)
for 1 ≤ j ≤ t to maximize Ptk-=10 qk,t(t - k). We may
k=0
Note that (1 - β1,maX)β1k,maX
t
t
Xqk,t(t-k) ≤ β1t+,m1aX
t
t + X(1 - β1,maX)β1k,maXk
k=0
(94)
k is an arithmetico-geometric sequence. Thus, we have
max(	qk,t(t - k)) ≤β1t+,m1aX
k=0
t + β1,max - β1t+,m1axt +
β2,maX(I - βt,∏1aX)
1 - β1,max
(95)
E	β1,max
≤β1,max + [ Q
1 - β1,maX
β1,max
1 - β1,max
(96)
(97)
The proof is now complete.
Lemma 4. Under the conditions of Theorem 4, for any t ≥ 0, we have
□
1 t	t-1	L 2 t
2 ∑qk,tE[∑ ∖VL(θj+ι) - L(θj)k2] + -ηrk £qk,tgk ∖2
k=0	j=k
k=0
≤ 2(1 -I,max) (G2 + δ2).
Proof. As L(θ) is L-smooth, we have
∖VL(θj+1)-L(θj)∖2
≤L∖θj+1 - θj∖2
j
=L∖ - η	qi,jgi ∖2 .
i=0
(98)
(99)
(100)
By Pij=0 qi,j ≤ 1 in Lemma 1, we have
E[kVL(θj+ι) - L(θj)k2]
≤Lη2(G2 + δ2).
(101)
(102)
and
Lη2 II G ∣∣2
~^~∖∖)jqk,tgkk
k=0
≤Lη2 (G2+δ2).
(103)
(104)
By Lemma 3, we have
t	t-1
2 Xqk,tE[X ∖VL(θj+ι) - L(θj)k2]
k=0	j=k
2t
≤"2^(G2 + δ2) X qk,t(t - k)
k=0
≤ 2⅛X)(G2+ δ2).
(105)
(106)
(107)
22
Under review as a conference paper at ICLR 2022
Then we obtain
1 t	t-1	Lη2 2 Eqk,tE[£ kvL(θj+ι) - L(θj)k2] + ^ -2~	t k Xqk,tgkk2	(108)
k=0	j=k	k=0	
≤ 2⅛□(G2+ δ2) + T(G2+ δ2)		(109)
≤ 2τJ(G2+ δ2).		(110)
□
Proof. The proof of Theorem 4 is organized as follows.
By Lemma 2 and Lemma 4, we have
E[L(θt+1) - L(θt)]
t	t-1
≤2 Xqk,tE[X kVL(θj+ι) - L(θj)k2]+
k=0	j=k
t t k	Lη2	t
X qk,t( 2 η - η)E[kVL(θk )k ]+ — k X qk,tgk k
k=0	k=0
≤ Xqk,t(t-kη2 - η)E[kVL(θk)k2] + 27~L^~T(g2 + δ2).
k=0	2	2(2 - β1,max)
Thus, we have
t
X qk,tηE[kVL(θk)k2]
k=0
t t k	Lη2
≤E[L(θt) — L(θt+ι)] + ∑>k,t --- η2 E[kVL(θk)k2] + 2--η~- (G2 + δ2).
k=0	2	2(2 - β1,max)
By Lemma 1 and Lemma 3, we have
(2 - β1t+m1ax)η min E[kVL(θk)k2]
k=0,...,t
≤E[L(θt) — L(θt+ι)] + 2(⅛χ⅛ G2 + 2(⅛χy(G2+ δ2).
(111)
(112)
(113)
(114)
By summing the above inequality for t = 0, . . . , t, we have
(t+2)(2- β1,max)η min E[kVL(θk)k2]
k=0,...,t
≤L(θo) - L? + χβ1,maxη∖ G2(t + 2) + wrLη--V(G2 + δ2)(t + 1).	(115)
2(2 - β1
,max)	2(1 - β1
,max )
Then
min E[kVL(θk)k2]
k=0,...,t
≤	L(θθ) — L?	+	βl,maxη	G2 +	Ln
~ (1 - β1,max)(t + 1)n	2(1 - β1,max)2	2(1 - β1,max)2
(116)
23
Under review as a conference paper at ICLR 2022
Let η ≤ √C++ι. We have
min E[kVL(θk)k2]
k=0,...,t
≤ L(θθ) — L?	+	β1,maχC	G2 .
(I - β1,max)C√t +1	2(1 - β1,max)2√t + 1
--------L , I— (G2 + δ2)
2(1 - β1,max)2√t + 1
The proof is now complete.
(117)
(118)
□
B Classical Approximation Assumptions
Assumption 2 indicates that the dynamical system is in equilibrium near minima but not necessarily
near saddle points. It means that dP∂θ,t) ≈ 0 holds near minima, but not necessarily holds near
saddle point b. Quasi-Equilibrium Assumption is actually weaker but more useful than the conven-
tional stationary assumption for deep learning (Welling and Teh, 2011; Mandt et al., 2017). Under
Assumption 2, the probability density P can behave like a stationary distribution only inside valleys,
but density transportation through saddle points can be busy. Quasi-Equilibrium is more like: stable
lakes (loss valleys) is connected by rapid Rivers (escape paths). In contrast, the stationary assumption
requires strictly zero flux between lakes (loss valleys). Little knowledge about density motion can be
obtained under the stationary assumption.
Low Temperature Assumption is common (Van Kampen, 1992; Zhou, 2010; Berglund, 2013; Jas-
trzkebski et al., 2017), and is always justified when B is small. Under Assumption 3, the probability
densities will concentrate around minima and MPPs. Numerically, the 6-sigma rule may often provide
good approximation for a Gaussian distribution. Assumption 3 will make the second order Taylor
approximation, Assumption 1, even more reasonable in SGD diffusion.
Here, we try to provide a more intuitive explanation about Low Temperature Assumption in the
domain of deep learning. Without loss of generality, we discuss it in one-dimensional dynamics. The
temperature can be interpreted as a real number D. In SGD, we have the temperature as D =黑H.
In statistical physics, if ∆L is large, then we call it Low Temperature Approximation. Note that
Δdl appears insides an exponential function in the theoretical analysis. People usually believe that,
numerically, ∆L > 6 can make a good approximation, for a similar reason of the 6-sigma rule in
statistics. In the final training phase of deep networks, a common setting is η = 0.01 and B = 128.
Thus, we may safely apply Assumption 3 to the loss valleys which satisfy the very mild condition
H > 2.3 X 10-4. Empirically, the condition ∆L > 2.3 X 10-4 holds well in SGD dynamics.
C Stochastic Gradient Noise Analysis
In this section, we empirically discussed the covariance of stochastic gradient noise (SGN) and why
SGN is approximately Gaussian in common settings.
In Figure 6, following Xie et al. (2021b), we again validated the relation between gradient noise
covariance and the Hessian. We particularly choose a randomly initialized mode so that the model is
not near critical points. We display all elements H(i,j) ∈ [1e - 4, 0.5] of the Hessian matrix and the
corresponding elements C(i,j) of gradient covariance matrix in the space spanned by the eigenvectors
of Hessian. Even if the model is far from critical points, the SGN covariance is still approximately
proportional to the Hessian and inverse to the batch size B . The correlation is especially high
along the directions with small-magnitude eigenvalues of the Hessian. Fortunately, small-magnitude
eigenvalues of the Hessian indicate the flat directions which we care most in saddle-point escaping.
We also note that the SGN we study is introduced by minibatch training,吗；? 一 叫θθt, which is
the difference between gradient descent and stochastic gradient descent.
In Figure 7 of the Appendix, Xie et al. (2021b) empirically verified that SGN is highly similar to
Gaussian noise instead of heavy-tailed LeVy noise. Xie et al. (2021b) recovered the experiment of
24
Under review as a conference paper at ICLR 2022
0.5
0.5
B=I1 Coef-0.8948, Pearson： 0.999
B=IO1 COef:0.8951, Pearson: 0.9988
——B=I1 COef:0.7959, Pearson: 0.962
----B=10, Coe£0.0795, Pearson： 0.9617
——B=IOO1 Coef：0.0079. Pearson： 0.9605
Figure 6: We verified Equation (3) by training pretrained and random three-layer fully-connected
networks on MNIST (LeCun, 1998). Top Row: Pretrained Models. Bottom Row: Random Models.
The noise covariance is approximately proportional to the Hessian and inverse to the batch size B
even not around critical points.
25
Under review as a conference paper at ICLR 2022
Λ一一SUəŋ
IO0
0.00	0.05
0.10	0.15	0.20	0.25
Noise Norm
(a) Gradient Noise of one minibatch
across parameters
0.10	0.15
Noise Norm
(b) L6vy noise
Λ一一SUəŋ
Λ一卫əŋ
0.00 0.02 0.04 0.06 0.08 0.10 0.12	0.000	0.002	0.004	0.006	0.008	0.010
Noise Norm	Noise Norm
(c) Gradient Noise of one parameter	(d) Gaussian noise
across minibatches
Figure 7: The Stochastic Gradient Noise Analysis (Xie et al., 2021b). The histogram of the norm
of the gradient noises computed with ResNet18 on CIFAR-10. Subfigure (a) follows Simsekli et al.
(2019) and computes “stochastic gradient noise” across parameters. Subfigure (c) follows the usual
definition and computes stochastic gradient noise across minibatches. Obviously, SGN computed
over minibatches is more like Gaussian noise rather than LeVy noise.
Simsekli et al. (2019) to show that gradient noise is approximately LeVy noise only if it is computed
across parameters. Figure 7 of the Appendix actually suggests that the contradicted observations are
from the different formulations of gradient noise. Simsekli et al. (2019) computed “SGN” across n
model parameters and regarded “SGN” as n samples drawn from a single-variant distribution. In Xie
et al. (2021b), SGN computed over different minibatches obeys a N-variant Gaussian distribution,
which can be θ-dependent and anisotropic. Simsekli et al. (2019) studied the distribution of SGN as
a single-variant distribution, while Xie et al. (2021b) relaxed it as a n-variant distribution. Figure 7
holds well at least when the batch size B is larger than 16, which is common in practice.
D	Experimental Details
Computational environment. The experiments are conducted on a computing cluster with GPUs of
NVIDIA® TeslaτM P100 16GB and CPUs OfIntel® Xeon® CPU E5-2640 v3 @ 2.60GHz.
D.1 IMAGE Classification
Data Preprocessing: For CIFAR-10/CIFAR-100, we perform the per-pixel zero-mean unit-variance
normalization, horizontal random flip, and 32 X 32 random crops after padding with 4 pixels on each
side. For ImageNet, we perform the per-pixel zero-mean unit-variance normalization, horizontal
random flip, and the resized random crops where the random size (of 0.08 to 1.0) of the original size
and a random aspect ratio (of 4 to 3) of the original aspect ratio is made.
Hyperparameter Settings for CIFAR-10 and CIFAR-100: We select the optimal learning rate for
each experiment from {0.00001,0.0001,0.001,0.01,0.1,1,10} for non-adaptive gradient methods
and use the default learning rate in original papers for adaptive gradient methods. The settings of
26
Under review as a conference paper at ICLR 2022
(a) 1-Dimensional Escape
(b) High-Dimensional Escape
Figure 8: The illustration of Kramers Escape Problems (Xie et al., 2021b). Assume there are two
valleys, Sharp Valley a1 and Flat Valley a2 . Also Col b is the boundary between two valleys. a1 and
aa are minima of two neighboring valleys. b is the saddle point separating the two valleys. c locates
outside of Valley a1.
learning rates: η = 1 for Adai；n = 0.1 for SGD with Momentum and AdaiW; η = 0.001 for Adam,
AMSGrad, AdamW, AdaBound, Yogi, and RAdam; η = 0.01 for Padam. For the learning rate
schedule, the learning rate is divided by 10 at the epoch of {80, 160} for CIFAR-10 and {100, 150}
for CIFAR-100. The batch size is set to 128 for CIFAR-10 and CIFAR-100. The L2 regularization
hyperparameter is set to λ = 0.0005 for CIFAR-10 and CIFAR-100. Considering the linear scaling
rule of decoupled weight decay and initial learning rates (Loshchilov and Hutter, 2018), we chose
decoupled weight decay hyperparameters as: λ = 0.5 for AdamW on CIFAR-10 and CIFAR-100;
λ = 0.005 for AdaiW on CIFAR-10 and CIFAR-100. We set the momentum hyperparameter
β1 = 0.9 for SGD with Momentum. As for other optimizer hyperparameters, we apply the default
hyperparameter settings directly.
Hyperparameter Settings for ImageNet: We select the optimal learning rate for each experiment
from {0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10} for Adai, SGD with Momentum, and Adam. The
settings of learning rates: η = 1 for Adai;n = 0.1 for SGD with Momentum; η = 0.0001 for
Adam. For the learning rate schedule, the learning rate is divided by 10 at the epoch of {30, 60, 90}.
The batch size is set to 256. The L2 regularization hyperparameter is set to λ = 0.0001. We
set the momentum hyperparameter β1 = 0.9 for SGD with Momentum. As for other training
hyperparameters, we apply the default hyperparameter settings directly.
Some papers often choose λ = 0.0001 as the default weight decay setting for CIFAR-10 and
CIFAR-100. We study the weight decay setting in Appendix F.
D.2 Language Modeling
Moreover, we present the learning curves for language modeling experiments in Figure 12. We
empirically compare three base optimizers, including Adai, SGD with Momentum, and Adam, for
language modeling experiments. We use a classical language model, Long Short-Term Memory
(LSTM) (Hochreiter and Schmidhuber, 1997b) with 2 layers, 512 embedding dimensions, and 512
hidden dimensions, which has 14 million model parameters and is similar to “medium LSTM” in
Zaremba et al. (2014). The benchmark task is the word-level Penn TreeBank (Marcus et al., 1993).
Hyperparameter Settings. Batch Size: B = 20. BPTT Size: bptt = 35. Weight Decay: λ =
0.00005. Learning Rate: η = 0.001. The dropout probability is set to 0.5. We clipped gradient
norm to 1. We select optimal learning rates from {10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001} for each
optimizer.
E	The Mean Es cape Time Analysis
Mean Escape Time: The mean escape time is the expected time for a particle governed by Equation
1 to escape from Sharp Valley a1 to Flat Valley a2, seen in Figure 8. The mean escape time is widely
used in related statistical physics and stochastic process (Van Kampen, 1992; Nguyen et al., 2019).
27
Under review as a conference paper at ICLR 2022
Related machine learning papers (Xie et al., 2021b; Hu et al., 2019; Nguyen et al., 2019) also studied
how SGD selects minima by using the concept of the mean escape time.
Data Set: We generate 50000 Gaussian samples as the training data set, where X 〜N(0,4I).
Hyperparameters: The batch size is set 10. No weight decay. The learning rate: 0.001 for Adai,
0.0001 for Momentum (with β = 0.9), and 0.03 for Adam.
Test Function: Styblinski-Tang Function is a commonly used function in nonconvex optimization,
written as
1N
f (θ) = 2 ∑(θ4 - 16θ2 +5θi).
i=1
We use 10-dimensional Styblinski-Tang Function as the test function, and Gaussian samples as
training data.
L(θ) = f(θ - x),
where data samples X 〜N(0,4I). The one-dimensional StyblinSki-Tang Function has one global
minimum located at a = -2.903534, one local minimum located at d, and one saddle point b =
0.256732 as the boundary separating Valley a1 and Valley a2. For a n-dimensional Styblinski-Tang
Function, we initialize parameters θt=o = %(一2.903534,..., -2.903534), and set the valley's
boundary as θi < √0.156731, where i is the dimension index. We record the number of iterations
required to escape from the valley to the outside of valley.
Observation: we observe the number of iterations from the initialized position to the terminated
position. As we are more interested in the number of iterations than “dynamical time” in practice, we
use the number of iterations to denote the mean escape time and ignore the time unit η in “dynamical
time”. We repeat experiments 100 times to estimate the escape rate Γ and the mean escape time
τ . As the escape time is approximately a random variable obeying an exponential distribution,
t 〜EXponential(Γ), the estimated escape rate can be written as
Γ
100-2
(119)
Σ
T00^
i=1
ti
The 95% confidence interval of this estimator is
Γ(1 -
1 96	1 96
-v=) ≤ Γ ≤ Γ(1 + -f=).
√100 —	—	√100
(120)
F Supplementary Empirical Results
Some papers (Luo et al., 2019; Chen and Gu, 2018) argued that their proposed Adam variants may
generalize as well as SGD. But we found that this argument is contracted with our comparative
experimental results, such as Table 1. The main problem may lie in weight decay. SGD with weight
decay λ = 0.0001, a common setting in related papers, is not a good baseline on CIFAR-10 and
CIFAR-100, as λ = 0.0005 often shows better generalization, seen in Figures 9. We also conduct
comparative experiments with λ = 0.0001, seen in Table 2. While some Adam variants under this
setting sometimes may compare with SGD due to the lower baseline performance of SGD, Adai and
SGD with fair weight decay still show superior test performance.
We display all learning curves of Adai, SGD, and Adam on CIFAR-10 and CIFAR-100 in Figure 10.
We further compare convergence of Adai and Adam with the fixed learning rate scheduler for 1000
epochs in Figure 11.
For Language Modeling, we display the results of LSTM in Figure 12.
Finally, we use the measure of the expected minima sharpness proposed by Neyshabur et al. (2017)
to compare the sharpness of minima learned by Adai, Momentum, and Adam. The expected minima
sharpness is defined as Eζ [L(θ? + ζ) - L(θ?)], where ζ is Gaussian noise and θ? is the empirical
minimizer learned by a training algorithm. If the loss landscape near θ? is sharp, the weight-perturbed
loss Eζ[L(θ? + ζ)] will be much larger than L(θ?). Figure 13 empirically supports that Adai and
Momentum can learn significantly flatter minima than Adam.
28
Under review as a conference paper at ICLR 2022

0.04
IoT	Io-4	IoT
Weight Decay
0.05- ---- Adal
----Momentum
0.04-1..............    ............... ............
Io-N IoT	Io_4	IoT	IoT
Weight Decay
-S0X
Figure 9: The test errors of ResNet18 on CIFAR-10 and VGG16 on CIFAR-10 under various weight
decay. Left: ResNet18. Right: VGG16. The optimal test performance corresponds to λ = 0.0005.
Obviously, Adai has better optimal test performance than SGD with Momentum.
0.14-
0.12-
0.10-
0Λ8-
-OUa ⅛φh
OM-I--1---<--------------------
0	25	50	75 100 125 150 175 200
Epochs
(a) ResNet18
0.14'
0.12-
O-IO-
o.«
0	25	50	75 100 125 UO 175 2∞
Epochs
(b) VGG16
0.04
(c) DenseNet121
0.275 ■
0.250
0.325
0.225
0.200
25	50	75 100 125 150 175 2∞
EpeChS
——Mal LR1.0
---Momentum
——Mai U»0.5
Adam
——MalUUx)
---Momentum
——AdaILRO5
Adam
O 25	50	75 IOO 125 150 175 2∞
Epochs
O 25	50	75 IOO 125 150 175 200
Epochs
Figure 10: Generalization and Convergence Comparison. Subfigures (a)-(b): ResNet18 and VGG16
on CIFAR-10. Subfigures (c)-(d): DenseNet121 and GoogLeNet on CIFAR-100. Top Row: Test
curves. Bottom Row: Training curves. Adai with η = 1 and η = 0.5 converge similarly fast to SGD
with Momentum and Adam, respectively, and Adai generalizes significantly better than SGD with
Momentum and Adam.
0.175-
0	25	50	75 100 J25 150 175 2∞
EpMhS
O 25	50	75 IOO 125 150 175 200
Epochs
Atia∣ιjux)
—Momentunv
——3Ut05
Adam
(d) GoogLeNet
O 25	50	75 IOO 125 150 175 2∞
EpOChS
——Adai LR0.01
——Adam LR0.0001
——Adam LRle-05
O 1
O -
1 W
sso6u-u-ejl
IOT
O 200	400	600
Epochs
800 IOOO

Figure 11: Convergence comparison by training VGG16 on CIFAR-10 for 1000 epochs with the fixed
learning rate. When they converge similarly fast, Adai converges in a lower training loss in the end.
When they converge in a similarly low training loss, Adai converges faster during training.
29
Under review as a conference paper at ICLR 2022

(a) Test Perplexity
(b) Training Perplexity
Figure 12:	Language Modeling. The learning curves of Adai, SGD (with Momentum), and Adam for
LSTM on Penn TreeBank. The optimal test perplexity of Adai, SGD, and Adam are 74.3, 74.9, and
74.3, respectively. Adai and Adam outperform SGD, while Adai may lead to a lower training loss
than Adam and SGD.
IOT
2 10 12 3
Ooo- - -
Illooo
111
σl 6u-uu
0.00000.00250.00500.00750.01000.01250.01500.01750.0200
Weight Noise Scale
Figure 13:	The expected minima sharpness analysis of the weight-perturbed training loss landscape
of ResNet18 on CIFAR-10. The weight noise scale is the standard deviation of the injected Gaussian
noise. The minima learned by Adai and SGD are more robust to weight noise. Obviously, Adai and
Momentum can learn much flatter minima than Adam in terms of the expected minima sharpness.
30
Under review as a conference paper at ICLR 2022
Table 2: Test performance comparison of optimizers with the weight decay hyperparmeter λ = 0.0001.
In this setting, some Adam variants may compare with SGD mainly because the baseline performance
of SGD is lower than the baseline performance in Table 1. The test errors of AdaiW, Adai, and
Momentum in middle columns is the original results in Table 1.
Dataset	Model	AdaiW	Adai	SGD M	SGD M	Adam	AMS GRAD	AdamW	AdaBound	Padam	Yogi	RAdam
CIFAR-10	ResNet18	4.59o.i6	4.740.14	5.010.03	5.58	6.08	5.72	5.33	6.87	5.83	5.43	5.81
	VGG16	5.8I0.07	6.000.09	6.420.02	6.92	7.04	6.68	6.45	7.33	6.74	6.69	6.73
CIFAR-100	ResNet34	21.05o.ιo	20.790.22	21.520.37	24.92	25.56	24.74	23.61	25.67	25.39	23.72	25.65
	DenseNet121	19.44o.2i	19.590.38	19.810.33	20.98	24.39	22.80	22.23	24.23	22.26	22.40	22.40
	GoogLeNet	20.500.25	20.550.32	21.210.29	21.89	24.60	24.05	21.71	25.03	26.69	22.56	22.35
G Adai with Stable/Decoupled Weight Decay
Algorithm 3: AdaiS/AdaiW
__^ ,. .
gt = RL(Ot-1);
vt = β2vt-1 + (1 - β2)gt2 ;
Vt = I-v⅛ ；
Vt = mean (Vt);
βι,t = (I - β0Vt).Clip(0, 1 - e);
mt = β1,tmt-1 + (1 - β1,t)gt;
m,——______mt_____■
mt = 1-QZ=1 βι,z;
θt = Θt-1 一 ηm^t 一 ληθt-i,,
H Expressions of Adam Dynamics
In this section, we discuss why Adam dynamics can also be expressed as Equation (7) similarly to
Momentum dynamics.
The derivation that generalizes Momentum dynamics to Adam dynamics is trivial. We only need to
replace η by the adaptive η in Equations (4),(5), and (6). We write the updating rule of Adam as
mt = β1mt-1 + β3gt,
[θt+ι = θt ― ^mt,
(121)
where β3 = 1 一 βι, βι is the hyperparameter, and η is the adaptive learning rate. For simplicity, it
is fine to consider the updating rules as element-wise operation. We can also write the Newtonian
motion equations of Adam with the mass M and the damping coefficient γ as
J	rt = (1 一 Ydtyrt-i + MMdt
θt+1 = θt + rtdt,
(122)
where r = —mt, F = gt, dt = η, 1 一 γdt = βι, and M = β3 = 1 一 βι. Thus, we obtain the
differential-form motion equation of Adam as
Mθ = ~γMθ + F,
(123)
where the mass M
1-β and the damping coefficient Y
1-β1
n
This shows that the analysis can be applied to both Adam dynamics and Momentum dynamics.
This is not surprising, because the Newtonian motion equations, including Equations (5) and (6),
are universal. The basic updating rules, Equations (4), generally holds for any optimizer that uses
Momentum, including Adam. As the terms in Equations (4) can correspond to the terms in Equations
(5) one by one, any dynamics governed by Equations (4) can also corresponds to a differential-form
Equation (6), where M and Y may have different expressions.
We note that Equations (5) in our paper is not contradictory to the SDEs of Adam in Zhou et al.
(2020), as long as we let the expressions of M and Y follow Adam dynamics. In fact, the updating
31
Under review as a conference paper at ICLR 2022
rule of vt of the SDEs in Zhou et al. (2020) can be incorporated into the expressions of M and γ .
Equation (5) in our work, which is a single stochastic differential equation, may be more concise for
analyzing optimization dynamics than the SDEs in Zhou et al. (2020).
We also point out that the most important property of using Momentum is to employ the phase-time
dynamics for training of deep neural networks. If we let β1 = 0 in Adam, Adam will be reduced to
RMSprop. Similarly SGD, RMSprop has no the momentum drift effect on saddle-point escaping
but has an isotropic diffusion effect. Our theoretical analysis about Momentum and Adam helps
us understand how manipulate element-wise momentum and learning rate separately to improve
saddle-point escaping as we want.
32