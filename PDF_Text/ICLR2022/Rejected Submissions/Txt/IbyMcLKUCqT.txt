Under review as a conference paper at ICLR 2022
Theoretical Analysis of Consistency Regu-
larization with Limited Augmented Data
Anonymous authors
Paper under double-blind review
Ab stract
Data augmentation is popular in the training of large neural networks; currently,
however, there is no clear theoretical comparison between different algorithmic
choices on how to use augmented data. In this paper, we take a small step in this
direction; we present a simple new statistical framework to analyze data augmen-
tation - specifically, one that captures what it means for one input sample to be an
augmentation of another, and also the richness of the augmented set. We use this
to interpret consistency regularization as a way to reduce function class complex-
ity, and characterize its generalization performance. Specializing this analysis for
linear regression shows that consistency regularization has strictly better sample
efficiency as compared to empirical risk minimization (ERM) on the augmented
set. In addition, we also provide generalization bounds under consistency reg-
ularization for logistic regression and two-layer neural networks. We perform
experiments that make a clean and apples-to-apples comparison (i.e. with no ex-
tra modeling or data tweaks) between ERM and consistency regularization using
CIFAR-100 and WideResNet; these demonstrate the superior efficacy of consis-
tency regularization.
1	Introduction
Modern machine learning models, especially deep learning models, require abundant training sam-
ples. Since data collection and human annotation are expensive, data augmentation has been a
ubiquitous practice in creating artificial labeled samples and improving generalization performance.
This practice is corroborated by the fact that the semantics of images remain the same through sim-
ple translations like obscuring, flipping, rotation, color jitter, rescaling (Shorten & Khoshgoftaar,
2019).
Conventional algorithms use data augmentation to expand the training data set (Krizhevsky et al.,
2012; Simard et al., 1998; Cubuk et al., 2018; Simonyan & Zisserman, 2014; He et al., 2016). As
an alternative, consistency regularization enforces the model to output similar predictions on the
original and augmented samples and contributes to many recent state-of-the-art supervised or semi-
supervised algorithms. This idea was first proposed in (Bachman et al., 2014) and popularized
by Laine & Aila (2016); Sajjadi et al. (2016), and gained more attention recently with the success of
FixMatch (Sohn et al., 2020) for semi-supervised few-shot learning, and AdaMatch (Berthelot et al.,
2021) for domain adaptation.
Several recent papers (see e.g. (Chen et al., 2020a; Mei et al., 2021; Lyle et al., 2019)) attempt
to provide a theoretical understanding of data augmentation (DA); they focus on establishing that
augmenting data saves on the number of labeled samples needed for the same level of accuracy.
However, none of these explicitly compare in an apples to apples way the efficacy (in terms of the
number of augmented samples) of one algorithmic choice of how to use the augmented samples vs
another algorithmic choice. Another dimension un-explored in previous work is any characterization
of the quality of augmentation.
In this paper, we hope to answer the following research question:
Is it possible to develop a theoretical framework to compare the sample efficiency of different
algorithms that use augmented data?
1
Under review as a conference paper at ICLR 2022
We present a new theoretical framework that casts consistency regularization as a way to reduce
function class complexity, which immediately connects to the well-established theory on gener-
alization and gives rise to a generalization bound for consistency regularization under a general
bounded loss function. When specialized to linear regression, this new theoretical framework shows
that the consistency regularization is strictly more sample efficient than empirical risk minimization
(ERM) on the augmented dataset. In addition, using this framework, we also provide generaliza-
tion bounds under consistency regularization for logistic regression, two-layer neural networks, and
expansion-based data augmentations.
As summary, our main contributions are:
•	A statistical framework of consistency regularization. We first present a simple new
statistical framework to analyze data augmentation - with a formal theoretical definition
of data augmentation and its strength. We then use this framework to give a generalization
bound of consistency regularization and provide instantiations for linear regression, logistic
regression, two-layer neural networks, and expansion-based data augmentations.
•	Theoretically proving the efficacy of consistency regularization. When specializing our
framework with consistency regularization to linear/logistic regression, it yields a strictly
smaller generalization error than ERM with the same augmented data.
•	Empirical comparisons between consistency regularization and ERM. We perform ex-
periments that make a clean and apples-apples comparison (i.e., with no extra modeling or
data tweaks) between consistency regularization and ERM using CIFAR-100 and WideRes-
Net. Our empirical results demonstrate the superior efficacy of consistency regularization.
2	Related Work
Empirical findings. Data augmentation (DA) is an essential recipe for almost every state-of-the-
art supervised learning algorithm since the seminal work of (Krizhevsky et al., 2012) (see reference
therein (Simard et al., 1998; Cubuk et al., 2018; Simonyan & Zisserman, 2014; He et al., 2016;
Kuchnik & Smith, 2018)). It started from adding augmented data to the training samples via (ran-
dom) perturbations, distortions, scales, crops, rotations, and horizontal flips. More sophisticated
variants were subsequently designed; a non-exhaustive list includes Mixup (Zhang et al., 2017),
Cutout (DeVries & Taylor, 2017), and Cutmix (Yun et al., 2019). The choice of data augmentation
and their combinations require domain knowledge and experts’ heuristics, which triggered some
automated search algorithm to find the best augmentation strategies (Lim et al., 2019; Cubuk et al.,
2019). The effects of different DAs have been systematically explored in (Tensmeyer & Martinez,
2016).
Recent practices not only add augmented data to the training set but also enforce the predictor output
to be similar by adding consistency regularization (Bachman et al., 2014; Laine & Aila, 2016; Sohn
et al., 2020). One benefit of consistency regularization is the feasibility of exploiting unlabeled data.
Therefore input consistency on augmented data also formed a major component to state-of-the-
art algorithms for semi-supervised learning (Laine & Aila, 2016; Sajjadi et al., 2016; Sohn et al.,
2020; Xie et al., 2020), self-supervised learning (Chen et al., 2020b), and unsupervised domain
adaptation (French et al., 2017; Berthelot et al., 2021).
Theoretical studies. Many interpret the effect of DA as some form of regularization (He et al.,
2019). Some work focuses on linear transformations and linear models (Wu et al., 2020) or kernel
classifiers (Dao et al., 2019). Convolutional neural networks by design enforce translation equiv-
ariance symmetry (Benton et al., 2020; Li et al., 2019); further studies have hard-coded CNN’s
invariance or equivariance to rotation (Cohen & Welling, 2016; Marcos et al., 2017; Worrall et al.,
2017; Zhou et al., 2017), scaling (Sosnovik et al., 2019; Worrall & Welling, 2019) and other types
of transformations.
A line of work view data augmentation as invariant learning by averaging over group actions (Chen
et al., 2020a; Mei et al., 2021; Lyle et al., 2019). They consider an ideal setting that is equivalent
to ERM with all possible augmented data, bringing a clean mathematical interpretation. We are
interested in a more realistic setting with limited augmented data. In this setting, it is crucial to
utilize the limited data with proper training methods, the difference of which cannot be revealed
under the previous studied settings.
2
Under review as a conference paper at ICLR 2022
Some more recent work investigates the feature representation learning procedure with DA for self-
supervised learning tasks (Wen & Li, 2021; HaoChen et al., 2021; Von Kugelgen et al., 2021; Garg
& Liang, 2020). Cai et al. (2021); Wei et al. (2021) studied the effect of data augmentation with
label propagation. Data augmentation is also deployed to improve robustness (Rajput et al., 2019),
to facilitate domain adaptation and domain generalization (Cai et al., 2021; Sagawa et al., 2019) .
3	Data Augmentation Consistency and How It Learns Efficiently
In this section, we first formally define data augmentation and introduce the problem setup. We then
define data augmentation consistency (DAC) regularization and show how it effectively reduces the
function class complexity, which connects to a generalization bound for bounded loss functions
via Rademacher complexity. Subsequently, we specialize our general result to linear regression,
which firmly shows that the DAC regularization provably learns more efficiently than minimizing
the empirical risk on the augmented dataset. The following section will present more applications
(including logistic regression, neural network, etc.).
3.1	Problem setup and data augmentation
Consider the standard supervised learning problem setup: x ∈ X are input features, and y ∈ Y is
its label (or response). Let P * be the true distribution of (x, y) (i.e., the label distribution follows
y 〜P * (y∣x)). We can then formally define data augmentation as:
Definition 1 (Data augmentation). For any sample x ∈ X , we say x0 ∈ X is its augmentation, if
and only ifP*(y|x) = P *(y|x0).
The definition above specifies what it means for one input sample to be an augmentation of another.
While the definition covers any x0 with the same label distribution as x, our results only use the aug-
mented samples that can be achieved via certain transformations (e.g., random cropping, rotation).
However, our definition does not cover augmentations that alter the labels (e.g., MixUp (Zhang et al.,
2017)).
Now we introduce the learning problem on an augmented dataset: Let (X, y) ∈ XN × YN be a
training set consisting of N i.i.d. samples. Besides the original (X, y), each training sample in it is
provided with α augmented samples. The input features of the augmented dataset can be written as:
N(X) = [xi；…；XN； Xl,1；…；XN,1；…；Xl,a；…；XN,α] ∈ X(1+ɑ)N,
where xi is in the original training set and xi,j , ∀j ∈ [α] are the augmentations of xi . The labels
of the augmented samples are kept the same, which can be denoted as My ∈ Y (1+α)N, where
M ∈ R(1+α)N×N is a vertical stack of (1 + α) identity mappings. Specifically, when the input xis
are d-dimensional real vectors, we have the following notion of augmentation strength daug :
Definition 2 (Strength of augmentations). For any δ ∈ (0, 1), let
daug(δ) , argmax	PAe,X	rank	Ae(X)	-	MfX	< daug	≤ δ,	daug ,	daug	(1/N) .
daug
Intuitively, strength of augmentations daug(δ) means that with probability at least 1 - δ, the aug-
mentations perturb at least daug(δ) dimensions; whereas daug can be intuitively understood as the
minimum number of dimensions that the augmentations in A(X) perturbed with high probability. A
lager daug corresponds to a stronger data augmentations. For instance, when A(X) = MX almost
surely (e.g., when the augmentations are identical copies of the original samples, corresponding to
the weakest augmentation - no augmentations at all), We have daug (δ) = d@"=0 for all δ ∈ (0,1).
On the other hand, if the augmentations are randomly generated, then it is more likely to see larger
daug (i.e., more dimensions being perturbed) with larger α (i.e., more augmentations).
In the next subsection, we formally introduce “data augmentation consistency regularization” and
present a generalization bound under bounded loss functions. We subsequently specialize the bound
to linear regression and show that consistency regularization is strictly more sample efficient than
empirical risk minimization (ERM) on the augmented dataset.
3
Under review as a conference paper at ICLR 2022
3.2	Data augmentation consistency regularization
Let H = {h : X→Y} be a well-specified function class (e.g. for regression problems, ∃h* ∈ H,
s.t. h* (x) = E[y∣x]) that We hope to learn from. Without loss of generality, We assume that
each function h ∈ H can be expressed as h = fh ◦ φh, where φh ∈ Φ = {φ : X → W} is a
proper representation mapping and fh ∈ F = {f : W → Y} is a predictor on top of the learned
representation. We tend to decompose h such that φh is a poWerful feature extraction function
Whereas fh can be as simple as a linear combiner. For instance, in a deep neural netWork, all the
layers before the final prediction layer can be vieWed as feature extraction φh , and the predictor fh
corresponds to the final linear combination of the features.
For a loss function l : Y × Y → R and a metric % properly defined on the representation space W ,
learning With data augmentation consistency (DAC) regularization can be expressed as:
argmin	l(h(xi),yi) +λ	%(φh(xi),φh(xi,j)).	(1)
h∈H
i=1	j=1 i=1
I	—y—	—}
DAC regularization
Note that the DAC regularization in Equation (1) can be easily implemented empirically as a regu-
larizer. Intuitively, DAC regularization penalizes the representation difference betWeen the original
sample φh(xi) and the augmented sample φh (xi,j), With the belief that similar samples (i.e., origi-
nal and augmented samples) should have similar representations. When the data augmentations do
not alter the labels, it is reasonable to enforce a strong regularization (i.e., λ → ∞) - given that
the conditional distribution ofy does not change. The function bhdac learned With data augmentation
consistency regularization can then be Written as the solution ofa constrained optimization problem:
bhdac , argminXl(h(xi),yi)	s.t. φh(xi) = φh(xi,j), ∀i ∈ [N],j ∈ [α].	(2)
h∈H i=1
The constraint in Equation (2) effectively reduces the size of the function class H. To rigorously
capture such reduction, We define the folloWing data augmentation consistency (DAC) operator over
H. Given the original training samples X and the augmented dataset A(X), We have:
Definition 3 (Data Augmentation Consistency Operator).
TAdea,Xc (H) ,{h| h∈H,φh(xi) =φh(xi,j), ∀i∈ [N],j ∈ [α]}.
Particularly, we assume that a proper representa-
tion class Φ is chosen with respect to the augmen-
tations such that h* ∈ TdeaX(H). The DAC opera-
tor maps the original function class H to a (poten-
tially much smaller) subset TdeaX(H), where every
function h ∈ TAX (H) gives consistent represen-
tation for all samples and their augmentations (i.e.,
φh (Xi) = φh(Xij)). It is now clear that with DAC
regularization, Equation (2) is effectively learning in
the function class T凭(H), which is a subset of H.
VA , ^X
As we will show in the next subsection, the function
class size reduction is the key for efficient learning
with data augmentations.
：：：：：：：：：：：,rʌ ∙	∙ IL j ∙ d	z7∕
...... Original Function Class: rτ
≠h(xi), ≠h(xij) can be different.
Reduced Function Class: T胃建用
:s.t. OMXi) = Φh(xi.j), NI G [n],j ∈ 同.
..............................................
QlI is the feature mapping of h ∈ M
Figure 1: The DAC regularization reduces
function class H to Tdac (H).
A,X
One of the contributions of our paper is to view consistency regularization as function class complex-
ity reduction. Our next proposition connects the generalization bound to the Rademacher complexity
TdeaXc (H) via standard analysis. We further provide various instantiations in the rest of our paper.
Let L(h) denote the population loss induced by h ∈ H, i.e., L(h)，E(x,y)〜P* [l(h(x), y)].
Proposition 1 (formally in Proposition 8). With Equation (2), if l is B-bounded and Cl -Lipschitz,
then for a fixed TdeaXc(H) and any δ ∈ (0, 1), with probability at least 1 - δ, we have
L(bdac) - L(h*) ≤ 4Cι ∙ Rn (TAaX(H)) + '2BB ?”).
4
Under review as a conference paper at ICLR 2022
Remark 1. Our results can be immediately extended to use unlabeled data. Notice that the DAC
regularization only enforces the same predictions for the original and augmented samples, where no
ground truth label is needed. For clarity of exposition, we focus on the labeled dataset in the main
text and defer discussions on unlabeled data to the appendix.
3.3	Efficacy of DAC regularization: A gentle start with linear regression
To see the efficacy of DAC regularization (i.e., Equation (2)), we revisit a more commonly adopted
training method here - empirical risk minimization (ERM) on augmented data:
bhda-erm , argmin XN l(h(xi), yi) + XN Xα l(h(xi,j),yi).	(3)
h∈H i=1	i=1 j=1
Now we show that the DAC regularization learns more efficiently than ERM. Consider the following
setting: given N observations X ∈ RN×d, the responses y ∈ RN are generated from a linear model
y = Xθ* + e, where e ∈ RN is zero-mean noise with E [ee>] = σ2IN. Recall that A(X) is
•—■
the entire augmented dataset, and My corresponds to the labels. Our next result characterizes the
fixed design excess risk of θ on A(X), which is defined as L(θ)
』M(x)o -A(X)θ* ∣∣2.
Under regularity conditions (e.g., x is sub-Gaussian and N is not too small), it is not hard to extend
to random design, i.e., the more commonly acknowledged generalization bound with the same order.
Given A(X), notice that by Definition 2, daug = rank
since there is no randomness
in Ae, X in fixed design setting. For a linear regression model tobe identifiable (i.e., having an unique
solution), we assume that Ae(X) has full column rank. We then have the following theorem on the
excess risks of learning by DAC regularization and by ERM on the augmented dataset.
Theorem 2 (Informal result on linear regression (formally in Theorem 6)). Learning with DAC
regularization, we have E [L(bdac) — L(θ*)] = B-dN2
the augmented dataset, we have E
[L(bda-erm) — l(θ*)]
while learning with ERM directly on
(d-dd∙N+d )σ , where d ∈ [0, dαug].
Formally, d0 is defined as d0，tr((HA(X[+？)MM ), where HA(X)，A(X)A(X)*, and PS is the
orthogonal projector onto S , nMfXθ | ∀θ ∈ Rd, s.t. Ae(X) - MfX θ = 0o.
Here we present an explanation for d0 .
Note that σ2 ∙ MM> is the noise Co-
variance matrix of the augmented dataset.
We have
of θbdac,
variance
tr PS MfMf>
and tr HAe(X)
of θbda-erm
being the variance
MM> being the
Therefore, d0 8
tr HAe(X) - PS MfMf> is used to mea-
sure the difference. When HAe(X) 6= PS (a
Figure 2: Comparison of DAC regularization and
DA-ERM (Example 1). The results precisely
match Theorem 2. DA-ERM depends on the d0 in-
duced by different augmentations, while the DAC
regularization works equally well and better than
the DA-ERM. Further, both DAC and DA-ERM
are affected by the “strength of augmentation”
daug.
common scenario as instantiated in Example 1),
we have DAC being strictly better than ERM on
augmented data.
Example 1. Consider a 30-dimensional lin-
ear regression. The original training set con-
tains 50 samples. The inputs xis are gener-
ated independently from N (0, I30) and we set
θ* = [θ*; 0] with θ* 〜N(0, I5) and 0 ∈ R25.
The noise variance σ is set to 1. We break x into 3 parts [xc1, xe1 , xe2] and take the following
augmentations: A([xc1; xe1; xe2]) = [xc1; 2xe1; -xe2], xc1 ∈ Rdc1, xe1 ∈ Rde1, xe2 ∈ Rde2, where
dc1 + de1 + de2 = 30.
5
Under review as a conference paper at ICLR 2022
Notice that the augmentation perturbs xe1 and xe2 and leaving xc1 unchanged, we therefore have
daug = 30 - dc1. By changing dc1 and de1, we can have different augmentations with different
daug, d0. The results for daug ∈ {20, 25} and various d0s induced by different de1s are presented in
Figure 2. The excess risks precisely match Theorem 2. It confirms that the DAC regularization is
strictly better than ERM on an augmented dataset for a wide variety of augmentations.
4 Various Applications
Now we ground our general result on the DAC regularization with a set of common applications, in-
cluding the logistic regression, two-layer neural networks, and expansion-based data augmentations.
For each of the applications, We first specify the ground truth distribution P*, the function class H,
the loss function l, as Well as the augmented dataset A(X). Then, We discuss the corresponding
excess risk L(hdac) - L(h*) for the DAC regularization (Equation (2)). We abridge the analysis in a
set of concrete examples With concise arguments While deferring the complete assumptions and the-
orems to Appendix B. As a supplementary remark, in addition to the popular in-distribution setting
where we consider a unique distribution P * for both training and testing, DAC regularization is also
knoWn to improve out-of-distribution generalization for settings like domain adaptation. We defer
detailed discussion on such advantage of DAC regularization for linear regression in the domain
adaptation setting to Appendix B.4.
4.1 Logistic regression
Here we consider logistic	regressions	with X	= x ∈ Rd	kxk2 ≤ D	, Y = {0, 1}. For
some unkown θ* ∈ Rd	with ∣∣θ*k ≤ C0,	we have P*	(y = 1|x)	= σ (θ*>x), where
σ(∙) is the sigmoid function σ(z) ， 1/(1 + exp(-z)).	The function class H is H =
h(x) = θ>x θ ∈ Rd, ∣θ∣2 ≤ C0 , such that predictions are given by yb = σ θ>x . For binary
classification, we use the logistic loss l(θ>x, y) = -y log(σ(θ>x)) - (1 - y) log(1 - σ(θ>x)).
Recall the strength of augmentations daug from Definition 2. Under proper regularity conditions (i.e.,
x is sub-Gaussian and N is not too small, see Appendix B), we have the following generalization
bound for learning logistic regression under DAC regularization.
Theorem 3 (Informal result on logistic regression with DAC (formally in Theorem 9)). Learning
logistic regression with the DAC regularization h(xi) = h(xi,j), with high probability:
L(bdac) - L(h*). r d- d ""-logN,
where the strength of augmentation daug ∈ [0, d - 1].
Intuitively, with high probability, minimizing empirical loss on augmented samples gives a gener-
alization bound of L(bda-erm') 一 L(h*) . max ^qO++d)N, Jd^Nau) at best, where the first
term corresponds to the generalization bound for a d-dimensional logistic regression with (α + 1)N
samples, and the second term follows as the augmentations fail to perturb a (d - daug)-dimensional
sub-space (and in which ERM can only rely on the N original samples for learning). The first term
will dominate the max when there is limited augmented data (i.e., α is small).
Comparing the two, we see that DAC is more efficient than ERM. In particular, consider the scenario
that the limited data augmentations well perturb the data (e.g., α = 1 and daug = d - 1). The ERM
gives a generalization error that scales as ,d/N, while DAC yields a dimension-free，1/N error.
Please see Appendix E for a supporting numerical example, which demonstrates the benefits of DAC
over DA-ERM for logistic regression, and empirically verifies the impact of daug and α that matches
with our theoretical analysis.
4.2	Two-layer neural network
In this section, we discuss a special case where the result for bounded losses in Proposition 8 can
be extended to the unbounded square loss. With X = Rd and Y = R, we consider a ground truth
6
Under review as a conference paper at ICLR 2022
distribution P * (y|x) induced by a two-layer ReLU network: y = (x>B*) W + z, B* =
+	d×q
[b*... bk ... bd], for some unknown h* (x)，(x>B*) + w* where (•)+ ，max(0, ∙) element-
Wisely denotes the ReLU function, bk ∈ SdT for all k ∈ [q], and Z 〜N(0, σ2) is the Gaussian
noise. In terms of the function class H, for some constant Cw ≥ kw* k1, we have:
H = {h(x) = (x>B)+W I B = [bι... bq] ∈ Rd×q, ∣∣bk∣∣2 = 1 ∀ j ∈ [q], IHk ≤ Cw },
such that h* ∈ H. We use the standard square loss l(h(x), y) = 11 (h(x) 一 y)2.
〜
•—■
Let PN be the projector onto the null space of A(X) 一 MX. When the augmented sample set
A(X) is reasonably diverse (see Appendix B.2), regression over two-layer ReLU networks with the
DAC regularization generalizes as following:
Theorem 4 (Informal result on two-layer neural network with DAC (formally in Theorem 10)).
Assuming E [ɪ- Pn=IkPNXilgPN] ≤ CN2 for some CN > 0, learning the two-layer ReLU
network with the DAC regularization on Xi>B + = Xi>,j B + gives, with high probability:
LD- L (h*) . σC√NCN.
Recall the strength of augmentations daug from Definition 2. Under proper regularity condi-
tions (i.e., X is sub-Gaussian and N is reasonably large, see Appendix B.2), We have CN .
d— - daug with high probability. Analogous to the logistic regression example, applying the
ERM directly on the augmented samples achieves no better than L(bhda-erm) - L(h*) .
σCw max (j (α+1)N, J d^Ndu ) with high probability. The comparison again illustrates the advan-
tage of DAC regularization over ERM. The advantage is large when the limited data augmentations
are strong (i.e., large daug and small α).
4.3	Expansion-based data augmentations
In this section, we demonstrate that for the classification problems, enforcing consistency on a dif-
ferent notion of data augmentations based on expansion also brings a considerable reduction in
complexity of the feasible function class.
Concretely, we consider a multi-class classification problem: for an arbitrary set X, let Y = [K],
and h* : X → [K] be the ground truth classifier that partitions X : for each k ∈ [K], let Xk ,
{x ∈ X | h*(x) = k}, with Xi ∩ Xj- = 0, ∀i = j. Here we focus on the expansion-based data
augmentations defined as following:
Definition 4 (Expansion-based data augmentations). Let A : X → 2X be a function generating a
set of augmentations A(x) from a given x ∈ X that satisfies the following:
(a)	Nontrivial augmentation and class invariant: {x} ( A(x) ⊆ {x0 ∈ X | h* (x) = h* (x0)} for
all x ∈ X; and
(b)	Non-trivial expansion: for all k ∈ [K], given any 0 ( S ( Xk, there exists some x0 ∈/ S such
that A(x) ∩ A(x0 ) 6= 0.
We consider a general class of classifiers H ⊆ {h : X → [K]} where the ground truth classifier is re-
alizable, h* ∈ H. With the zero-one loss l01 (h(x), y) = 1 {h(x) 6= y}, and the corresponding pop-
ulation loss L01 (h) , Ex [1 {h (x) 6= h* (x)}], we learn a classifier from H with the DAC regular-
ization, h(x) = h(x0), where x0s are expansion-based data augmentations generated by an A (Defi-
nition 4). As a warm-up, we begin with a simplified setting where we enforce consistency over the
population in lieu of a finite set of training samples as in practice. Then, learning with the DAC reg-
ularization yields a classifier bhdac ∈ TAda,Xc (H) , {h ∈ H | h(x) = h(x0) ∀ x ∈ X, x0 ∈ A(x)}.
Theorem 5 (DAC with expansion-based data augmentations over population). Learning the classi-
fier with DAC regularization over population, with high probability,
L01 Qdac)-L01 (h*) ≤ Klog-N.
7
Under review as a conference paper at ICLR 2022
Particularly, with the DAC regularization, the generalization bound of the K-class classification
problem is dimension-independent but only scales with the number of classes, O(K).
Furthermore, in Appendix B.3, we extend the result to a more practical setting where the consistency
is enforced over a finite training set. Notably, Theorem 5, along with the finite train set case in
Appendix B.3, is a reminiscence of Theorem 3.6 and 3.7 by Wei et al. (2021), as well as Theorem
2.1, 2.2, and 2.3 by Cai et al. (2021). We adapt these existing theories and provide a unified analysis
under our setting, which demonstrates the generality of our framework.
5 Experiments
In this section, we empirically verify that training with DAC learns more efficiently than empirical
risk minimization on an augmented dataset. The dataset is derived from CIFAR-100, where we
randomly select 10,000 labeled data as the training set (i.e., 100 labeled samples per class). During
the training time, given a training batch, we generate augmentations by RandAugment (Cubuk et al.,
2020). We set the number of augmentations per sample to 7 unless otherwise mentioned.
The experiments focus on comparisons of 1) training with consistency regularization, and 2) empir-
ical risk minimization on the augmented dataset (DA-ERM). In particular, we use the same network
architecture (a WideResNet-28-2 (Zagoruyko & Komodakis, 2016)) and the same training settings
(e.g., optimizer, learning rate schedule, etc) for both methods. We defer the detailed experiment
settings to Appendix D. Our test set is the standard CIFAR-100 test set, and we report the average
and standard deviation of the testing accuracy of 5 independent runs. The consistency regularizer is
implemented as the l2 distance of the model’s predictions on the original and augmented samples.
Efficacy of DAC regularization. We first show that the DAC regularization learns more efficiently
than ERM on the augmented dataset. The results are listed in Table 1. Notice that with proper choice
of λ (i.e., the multiplicative coefficient before the DAC regularization, see Equation (1)), training
with DAC regularization significantly improves over DA-ERM.
DA-ERM
69.40 ± 0.05
DAC Regularization
λ = 0	λ =1	λ = 5	λ =10	λ = 20
62.82 ± 0.21 ^^68.63 ± On^^70.56 ± 0.07^^70.52 ± 0.14^^68.65 ± 0.27
Table 1: Testing accuracy of ERM and DAC regularization with different λ's (regularization coeff.).
DAC regularization helps more when data is scarce. Our theoretical results suggest that the
DAC regularization brings more benefits when the training data is scarce. The data scarcity can be
interpreted in two ways:
(1)	Labeled samples are scarce. We conduct experiments with different numbers of labeled sam-
ples, ranging from 1,000 (i.e., 10 images per class) to 20,000 samples (i.e., 200 images per class).
We generate 3 augmentations for each of the samples during the training time, and the results are
presented in Table 2. Notice that the DAC regularization gives a bigger improvement over DA-ERM
when the labeled samples are scarce. This matches the intuition that when there are sufficient train-
ing samples, data augmentation is less necessary. Therefore, the difference between different ways
of utilizing the augmented samples becomes diminishing.
Number of Labeled Data	1000	10000	20000
DA-ERM	31.11 ± 0.30	68.89 ± 0.07	76.79 ± 0.13
DAC (λ = 10)	33.59 ± 0.41	70.71 ± 0.10	76.86 ± 0.16
Table 2: Testing accuracy of ERM and DAC regularization with different numbers of labeled data.
(2)	Augmented samples are scarce. While keeping the number of labeled samples to be 10,000,
we evaluate the performance of the DAC regularization and DA-ERM with different numbers of
augmentations. The number of augmentations for each training sample ranges from 1 to 15, and the
results are listed in Table 3. The DAC regularization offers a more significant improvement when
the number of augmentations is small. This clearly demonstrates that the DAC regularization learns
more efficiently from the limited number of augmentations.
8
Under review as a conference paper at ICLR 2022
Number of Augmentations	1	3	7	15
DA-ERM	67.92 ± 0.08	69.04 ± 0.05	69.25 ± 0.16	69.30 ± 0.11
DAC (λ = 10)	70.06 ± 0.08	70.77 ± 0.20	70.74 ± 0.11	70.31 ± 0.12
Table 3: Testing accuracy of ERM and DAC regularization with different numbers of augmentations.
Proper augmentation brings good performance. To
achieve good performance, it is important to have proper
data augmentation - it needs to be strong such that it well
perturbs the input features, but it should also leave the
label distribution unchanged. Here we experiment with
different augmentation strengths, which is the number
of different random transformations (e.g., random crop-
ping, flipping, etc.) applied to the training samples se-
quentially. More transformations imply stronger aug-
mentations. The number of transformations ranges from
1 to 10, and the results are listed in Table 4. We see
that both DA-ERM and the DAC regularization benefits
from a proper augmentation. When the augmentation is
too strong (e.g., Augmentation Strength 10, as shown in
Figure 3), the DAC regularization gives a worse perfor-
mance. It might be explained by falsely enforcing DAC
regularization where the labels of the augmented samples
have changed.
Original	Aug Strength = 1
Figure 3: Examples of different aug-
mentation strengths.
Augmentation Strength
DA-ERM
DAC (λ = 10)
1	2	5	10
68.56 ± 0.12	69.32 ± 0.11 ^^69.97 ± 0.14^^69.66 ± 0.16
70.66 ± 0.14	70.65 ± 0.07	70.01 ± 0.10	68.95 ± 0.27
Table 4: Testing accuracy of EMR and DAC regularization with various augmentation strengths.
Combining with a semi-supervised
learning algorithm. Here we
show that the DAC regularization
can be easily extended to the semi-
supervised learning setting. We
take the previously established semi-
supervised learning method Fix-
Number of Unlabeled Data
FixMatch
FixMatch + DAC (λ = 1)
5000	10000	20000
67.74	69.23	70.76
71.24	72.7	74.04
Table 5: DAC regularization helps FixMatch when the un-
labeled data is scarce.
Match (Sohn et al., 2020) as the baseline and adapt the FixMatch by combining it with the DAC
regularization. Namely, besides using FixMatch to learn from the unlabeled data, we additionally
generate augmentations for the labeled samples and apply the DAC regularization. In particular,
we focus on the data-scarce regime by only keeping 10,000 labeled samples and at most 20,000
unlabeled samples. Results are listed in Table 5. We see that the DAC regularization also improves
the performance of FixMatch when the unlabeled samples are scarce. This again demonstrates the
efficiency of learning with DAC regularization.
6 Conclusion
We present a simple new theoretical framework for understanding the statistical efficiency of consis-
tency regularization with limited data augmentations. In particular, our proposed framework gives
a generalization bound for consistency regularization in general cases. We also provide instantia-
tions for linear regression, logistic regression, two-layer neural networks, and expansion-based data
augmentations. When specialized to linear regression/logistic regression, it shows that consistency
regularization yields a strictly smaller generalization error than ERM with augmented data. We also
provide apples-to-apples empirical comparisons between augmented ERM and consistency regular-
ization. These together demonstrate the superior efficacy of consistency regularization.
9
Under review as a conference paper at ICLR 2022
Reproducibility Statement Our main theoretical results, including problem setup and (simplified)
theorem statements are in Sections 3 and 4. The formal theorem statements along with the proofs
can be found in Appendices A and B. We present empirical results in Section 5, and the detailed
experiment setup is in Appendix D. We also submit our experiment code in the supplementary
material.
References
Philip Bachman, Ouais Alsharif, and Doina Precup. Learning with pseudo-ensembles. Advances in
neural information processing systems, 27:3365-3373, 2014.
Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. J. Mach. Learn. Res., 3(null):463-482, March 2003. ISSN 1532-4435.
Gregory Benton, Marc Finzi, Pavel Izmailov, and Andrew Gordon Wilson. Learning invariances in
neural networks. arXiv preprint arXiv:2010.11882, 2020.
David Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas Carlini, and Alex Kurakin. Adamatch:
A unified approach to semi-supervised learning and domain adaptation. arXiv preprint
arXiv:2106.04732, 2021.
Tianle Cai, Ruiqi Gao, Jason D. Lee, and Qi Lei. A theory of label propagation for subpopulation
shift, 2021.
Shuxiao Chen, Edgar Dobriban, and Jane H Lee. A group-theoretic framework for data augmenta-
tion. Journal of Machine Learning Research, 21(245):1-71, 2020a.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning,
pp. 1597-1607. PMLR, 2020b.
Taco Cohen and Max Welling. Group equivariant convolutional networks. In International confer-
ence on machine learning, pp. 2990-2999. PMLR, 2016.
Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:
Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018.
Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:
Learning augmentation strategies from data. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 113-123, 2019.
Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated
data augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition Workshops, pp. 702-703, 2020.
Tri Dao, Albert Gu, Alexander Ratner, Virginia Smith, Chris De Sa, and Christopher Re. A kernel
theory of modern data augmentation. In International Conference on Machine Learning, pp.
1528-1537. PMLR, 2019.
Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks
with cutout. arXiv preprint arXiv:1708.04552, 2017.
Simon S. Du, Wei Hu, Sham M. Kakade, Jason D. Lee, and Qi Lei. Few-shot learning via learning
the representation, provably, 2020.
Geoffrey French, Michal Mackiewicz, and Mark Fisher. Self-ensembling for visual domain adapta-
tion. arXiv preprint arXiv:1706.05208, 2017.
Siddhant Garg and Yingyu Liang. Functional regularization for representation learning: A unified
theoretical perspective. arXiv preprint arXiv:2008.02447, 2020.
Jeff Z HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma. Provable guarantees for self-supervised
deep learning with spectral contrastive loss. arXiv preprint arXiv:2106.04156, 2021.
10
Under review as a conference paper at ICLR 2022
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Zhuoxun He, Lingxi Xie, Xin Chen, Ya Zhang, Yanfeng Wang, and Qi Tian. Data augmentation
revisited: Rethinking the distribution gap between clean and augmented data. arXiv preprint
arXiv:1909.09148, 2019.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep con-
volutional neural networks. Advances in neural information processing systems, 25:1097-1105,
2012.
Michael Kuchnik and Virginia Smith. Efficient augmentation via data subsampling. arXiv preprint
arXiv:1810.05222, 2018.
Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint
arXiv:1610.02242, 2016.
Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and processes.
Springer Science & Business Media, 2013.
Zhiyuan Li, Ruosong Wang, Dingli Yu, Simon S Du, Wei Hu, Ruslan Salakhutdinov, and Sanjeev
Arora. Enhanced convolutional neural tangent kernels. arXiv preprint arXiv:1911.00809, 2019.
Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment.
Advances in Neural Information Processing Systems, 32:6665-6675, 2019.
Clare Lyle, Marta Kwiatkowksa, and Yarin Gal. An analysis of the effect of invariance on gen-
eralization in neural networks. In International conference on machine learning Workshop on
Understanding and Improving Generalization in Deep Learning, 2019.
Diego Marcos, Michele Volpi, Nikos Komodakis, and Devis Tuia. Rotation equivariant vector field
networks. In Proceedings of the IEEE International Conference on Computer Vision, pp. 5048-
5057, 2017.
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Learning with invariances in random
features and kernel models. arXiv preprint arXiv:2102.13219, 2021.
Shashank Rajput, Zhili Feng, Zachary Charles, Po-Ling Loh, and Dimitris Papailiopoulos. Does
data augmentation lead to positive margin? In International Conference on Machine Learning,
pp. 5321-5330. PMLR, 2019.
Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust
neural networks for group shifts: On the importance of regularization for worst-case generaliza-
tion. arXiv preprint arXiv:1911.08731, 2019.
Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transfor-
mations and perturbations for deep semi-supervised learning. Advances in neural information
processing systems, 29:1163-1171, 2016.
Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning.
Journal of Big Data, 6(1):1-48, 2019.
Patrice Y Simard, Yann A LeCun, John S Denker, and Bernard Victorri. Transformation invariance
in pattern recognition—tangent distance and tangent propagation. In Neural networks: tricks of
the trade, pp. 239-274. Springer, 1998.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk,
Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi-supervised learning
with consistency and confidence. arXiv preprint arXiv:2001.07685, 2020.
11
Under review as a conference paper at ICLR 2022
Ivan Sosnovik, MichaI Szmaja, and Arnold Smeulders. Scale-equivariant steerable networks. arXiv
preprint arXiv:1910.11093, 2019.
Christopher Tensmeyer and Tony Martinez. Improving invariance and equivariance properties of
convolutional neural networks. 2016.
Julius von Kugelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Scholkopf, Michel
Besserve, and Francesco Locatello. Self-supervised learning with data augmentations provably
isolates content from style. arXiv preprint arXiv:2106.04619, 2021.
Martin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Series
in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019. doi: 10.1017/
9781108627771.
Colin Wei and Tengyu Ma. Improved sample complexities for deep networks and robust classifica-
tion via an all-layer margin, 2021.
Colin Wei, Kendrick Shen, Yining Chen, and Tengyu Ma. Theoretical analysis of self-training with
deep networks on unlabeled data, 2021.
Zixin Wen and Yuanzhi Li. Toward understanding the feature learning process of self-supervised
contrastive learning. arXiv preprint arXiv:2105.15134, 2021.
Daniel E Worrall and Max Welling. Deep scale-spaces: Equivariance over scale. arXiv preprint
arXiv:1905.11697, 2019.
Daniel E Worrall, Stephan J Garbin, Daniyar Turmukhambetov, and Gabriel J Brostow. Harmonic
networks: Deep translation and rotation equivariance. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 5028-5037, 2017.
Sen Wu, Hongyang Zhang, Gregory Valiant, and Christopher Re. On the generalization effects of
linear transformations in data augmentation. In International Conference on Machine Learning,
pp. 10410-10420. PMLR, 2020.
Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student
improves imagenet classification. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 10687-10698, 2020.
Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.
Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceed-
ings of the IEEE/CVF International Conference on Computer Vision, pp. 6023-6032, 2019.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint
arXiv:1605.07146, 2016.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. arXiv preprint arXiv:1710.09412, 2017.
Yanzhao Zhou, Qixiang Ye, Qiang Qiu, and Jianbin Jiao. Oriented response networks. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 519-528, 2017.
12
Under review as a conference paper at ICLR 2022
A Proofs for B ounded Loss and Linear Regression
A.1 Proof for DAC on Linear Regression
For fixed A(X), we recall that daug = rank A(X) - MX since there is no randomness in A, X
in fix design setting. Assuming that A(X) admits full column rank, we have the following theorem
on the excess risk of DAC and ERM:
Theorem 6 (Formal restatement of Theorem 2 on linear regression.). Learning with DAC regu-
Iarization, we have E [L(θdac) - L(θ*)] =，d--N)σ
augmented dataset, we have E [L(θda-erm) — L(θ*)]
while learning with ERM directly on the
(d-d%+d )σ . d is defined as
do , trf (HA(X)- PS) M
1+α
where d0 ∈ [0,daug] with HAe(X) = Ae(X) Ae(X)>Ae(X)-1Ae(X)> and PS ∈
R(α+1)N ×(α+1)N is the projection matrix to S, and S is defined to be S ,
nMfXθ | ∀θ ∈ Rd, s.t. Ae(X) -MfX θ = 0o.
the excess risk of ERM on the augmented
Proof. With L(θ)，(i+α)N ∣∣ve(χ)θ-N(X)θ*∣∣2
training set satisfies that:
E hL(θbda-erm)i	=--1— E (1 + a)N	∣∣Ν(x)bbd。-erm -N(X)θ*∣∣2]
	=--1— E (1 + a)N	∣	∣2 ∣∣A(X)(A(X)>A(X))T∕(X)>(A(X)θ* + Me) -A(X)θ*[
	=--1— E (1 + a)N	11HHX)A(X)θ* + HA(Xfe- A(X)θ*∣∣2]
	1	
	二 一,	TTrE	∣	∣2 ∣H e	Mf e∣
	(1 + a)N	∣	Ae(X)	e∣2
	=--1— E (1 + a)N	htr(e>Mf>HAe(X)Mfe)i
	σ2 二	tr (1 + a)N	Mf>HAe(X)Mf.
Let CAe(X) and CMf denote the column space of A(X) and M, respectively.
A(X) - MX
have
Notice that S is a
= rank (PS), we
E L(θbda-erm)
2
WON E Irf >HA(Xf)]
22
ENE Irf >psf)] + ENE IfT(HA(X)- PS网
22
ENE htrf>ps f)i + N ∙E
trf>(HA(x)- PS f)
1 + α
By the data augmentation consistency constraint, we are essentially solving the linear regression
on the (d - daug)-dimensional space θ | (Ae(X) - MfX)θ = 0
. The rest of proof is identical to
standard regression analysis, with features first projected to S :
13
Under review as a conference paper at ICLR 2022
E L(θbdac)
(i+a)N E
(i+a)N E
U~.	.T~.	τ~.	. -∣-	.~.	--- ~ .	.
Ik(X)(A(X)>A(X))-1A(X)>Ps(A(X)θ* + Me) - A(X)θ*
1
(1+aNE [IIHA(X)PSA(X)θ* + HA(X)PSMe - A(X)θ*
1 N(X)θ* ∈ S, and HA(X)PS = PS since S ⊆ Ce(X))
"αN E [BPS feB2]
2
EN E Ir(M>PS M)I
(d — daug)σ?
N .
B	Applications of Learning with DAC
To instantiate our general result on the DAC regularization, we start with a concrete application of
Proposition 8 on the logistic regression problem with bounded loss (Section 4.1, Appendix B.1),
followed by an extension to the unbounded square loss (Section 4.2, Appendix B.2). Subsequently,
we discuss an alternative notion of data augmentations based on expansion for the classification
problems (Section 4.3, Appendix B.3). Finally, we present a supplementary example for the domain
adaptation in linear regression (Appendix B.4).
Before diving into the concrete applications, we recall the general setting, and introduce some addi-
tional notations used throughout the formal analysis.
DAC regularization with unlabeled data. Since the DAC regularization leverages only on the
unlabeled observable features but not their labels, the unlabeled training set in the DAC regularizer
and the labeled training set in the first term of Equation (1) for label learning can be considered
separately. For clarification, in the formal analysis, we distinguish the labeled training set (X, y) ∈
Xn × Yn from the unlabeled samples Xu ∈ XN (possibly with different sizes N ≥ n), while both
sample sets are drawn i.i.d. from P*. Particularly, in the supervised learning setting, the DAC
regularization is based on the observables from the labeled training set such that Xu = X (N = n).
While in the semi-supervised learning setting, Xu can be N unlabeled observations drawn i.i.d.
from the marginal distribution P * (x).
For the DAC regularization, we denote the augmentation of the unlabeled samples (excluding the
original samples Xu, in contrast to the augmented sample A(X) in the main text) as
b u	u	u	u	u	αN
A(X ) = [x1,1; ∙ ∙ ∙ ； xN,1; ∙ ∙ ∙ ； x1,α, ∙ ∙ ∙ ； xN,α ∈ X ,
where for each i ∈ [N], xiu,j j∈[α] is a set of α augmentations generated from xiu, and let M ∈
RαN ×N be the vertical stack of α N × N identity matrices. Then analogously, with the unlabeled
training set Xu, we can quantify the strength of the data augmentations A with
dbaug , rank Ab(Xu) - MXu) = rank Ae(Xu) - MMXu) ,
such that 0 ≤ daug ≤ min (d, αN) can be intuitively interpreted as the number of dimensions in
the span of the unlabeled samples, Row(Xu), perturbed by Ab. Moreover, we denote N as the
(d - daug)-dimensional null space of A(Xu) - MXu, and PN as the projection onto N such that
PN , Id — (N(Xu) — MXu)t (N(Xu) — MXu).
14
Under review as a conference paper at ICLR 2022
Correspondingly, let N ⊥ be the orthogonal complement of N, and P⊥N be the projection onto N ⊥
such that P⊥N = Id - PN . Then by recalling Definition 2, we have the following:
Proposition 7. For any δ ∈ (0, 1), with probability at least 1 - δ,
rank (PN) = dim (N⊥) = dug ≥ daug(δ),
rank (PN) = dim (N) = d - daug ≤ d - daug(δ).
Furthermore, for daug , daug (1∕n), with high probability, daug ≥ daug and d - daug ≤ d - daug.
In terms of (X, y) and Xu , the regularization formulation in Equation (1) can be restated as,
n
αN
argmin	l(h(xi), yi) + λ^∑S%(。“区)，Φh(xu,j))，
I '
}
"^^^^^^^^^^^^^^^{^^^^^^^^^^^^^^^^"^
DAC regularizer
and with the notation of A, the corresponding DAC operator is given by
TAXu (H)，{h ∈ H | Φh(xU) = Φh(xU,j), ∀i ∈ [N],j ∈ [α]},
where T dbaXc u and T deaXc u are equivalent by construction.
Marginal distribution and training set. Here we formally state the common regularity conditions
for the logistic regression (Section 4.1) and the two-layer neural network (Section 4.2) examples in
Assumption 1 and Assumption 2, where we assume X ⊆ Rd.
Assumption 1 (Regularity of marginal distribution). Let X 〜P*(x) be zero-mean E[x] = 0, with
the Covairance E[xx>] = Σχ > 0. We assume that (Σχ 1/2x) is P-Subgaussian 1, and there exist
constants C ≥ c = Θ(1) such that cId 4 Σx 4 CId.
Assumption 2 (Sufficient labeled data). For (X, y) ∈ Xn × Yn, with respect to Xu and Ab(Xu)
that characterize dbaug, we assume n	ρ4 d - dbaug .
For a B-bounded loss function l : Y × Y → R (i.e., 0 ≤ l ≤ B) such that for all y ∈ Y,
l(∙, y) : Y → R is Ci-Lipschitz, We have the following:
Proposition 8 (Formal restatement of Proposition 1 on DAC with bounded loss). Learning with
DAC regularization (Equation (2)), for a fixed T dbaXcu (H) and any δ ∈ (0, 1), with probability at
least 1 - δ, we have
L(bdac) - L(h*) ≤ 4Ci ∙ Rn(TAbaXu (H)) + J
2B2 log(2∕δ)
n
where Rn (T dbaXcu (H)) represents the Rademacher complexity of T dbaXcu (H).
Proof of Proposition 8. We first decompose the expected excess risk as
L(bdac) - L(h*) =(L(bdac) - L(bdac)) + 仅(bdac) - L(h*)) + (L(h*) - L(h*)),
d
where L(hdac) - L(h*) ≤ 0 by the basic inequality, and as a consequence, for a fixed Ho
TAdbaXcu(H),
,
L(hdac) - L(h*) ≤ 2 SUp	∣L(h) - L(h)∣ =
h∈TAdba,Xcu (H)
____ 、	一 ， 、 、 O -、	.	__ 、
2 sUp ∣L(h) - Lb(h)∣ .
h∈Ho
^
We denote g+(X, y) = sUph∈H : L(h) - L(h) and g- (X, y) = sUph∈H : -L(h) + L(h). Then,
P hL(bdac) - L(h*) ≥ e] ≤ P [g+(X, y) ≥ j] + Phg-(X, y) ≥ ∣].
1A random vector v ∈ Rd is ρ2-subgaussian if for any unit vector u ∈ Sd-1, u>v is ρ2-subgaussian,
E [exp(s ∙ u>v)] ≤ exp (s2ρ2∕2).
15
Under review as a conference paper at ICLR 2022
We will derive a tail bound for g+ (X, y) with the standard inequalities and symmetrization argu-
ment Wainwright (2019); Bartlett & Mendelson (2003), while the analogous statement holds for
g-(X, y).
Let (X(1), y(1)) be a samPle set generated by rePlacing an arbitrary observation in (X, y) with any
(x, y) ∈ X × Y. Since l is B-bounded, we have g+(X, y) - g+ (X(1) , y(1) ) ≤ B∕n. Leveraging
the McDiarmid’s inequality Bartlett & Mendelson (2003),
P g+ (X, y) ≥ E[g+ (X, y)] + t ≤ exp
For an arbitrary sample set (X, y), let L(x,y)(h) = n Pn=ι ∣ (h(χi), y%) be the empirical risk of
h with resPect to (X, y). Then, by a classical symmetrization argument (e.g., Proof of Wainwright
(2019) Theorem 4.10), we can bound the expectation: for an independent sample set (X0, y0) ∈
Xn X Yn drawn i.i.d. from P *,
E g+(X, y) =E(X,y)
sup
h∈Ho
L(h) - Lb(X,y) (h)
=E(X,y)
=E(X,y)
≤E(X,y)
sup
h∈Ho
sup
h∈Ho
E(X0,y0)
L(X0,y0) (h) - L(X,y)(h)
E(X0,y0) Lb(X0,y0) (h) - Lb(X,y) (h)	(X, y)
E(X0,y0)
sup L(X0,y0) (h) - L(X,y)(h) (X, y)
h∈Ho
(Law of iterated conditional expectation)
=E(X,y,X0,y0)
sup L(X0,y0)(h) - L(X,y)(h)
h∈Ho
Since (X, y) and (X0, y0) are indePendent and identically distributed, we can introduce i.i.d.
Rademacher random variables r = {ri ∈ {-1, +1} | i ∈ [n]} (indePendent of both (X, y) and
(X0 , y0 )) such that
E g+(X, y) ≤E
(X,y,X0,y0 ,r)	hs∈uHp
1n
-£「「(/(h (Xi) ,y0)-1 (h (Xi) ,yi))
n i=1
≤2 E(X,y,r) sup
h∈Ho
1n
(h (χi) ,yi)
n i=1
≤2 Rn (l oHo)
where l ◦ Ho = {l(h(∙), ∙) : X×Y→ R : h ∈ Ho} is the loss function class. Analogously,
E[g- (X, y)] ≤ 2Rn (l O Ho). Therefore, With Probability at least 1 - δ,
L(bdac) - L(h*) ≤ 4Rn(l o Ho) + J
2B2 log(2∕δ)
Finally, since l(∙,y) is Cl-LiPschitz for all y ∈ Y, by Ledoux & Talagrand (2013) Theorem 4.12, We
have Rn (l OHo) ≤ Cl ∙ Rn (Ho).	■
B.1 Formal Result on Logistic Regression
For the logistic regression Problem in Section 4.1, with X = x ∈ Rd kxk2 ≤ D , Y = {0, 1},
training with the DAC regularization can be formulated exPlicitly as
θbdac
1n
argmin — T / (θ>Xi, y)
θ∈Rd ni=1
s.t. kθ k2 ≤ C0,	Ab(Xu)θ	= MXuθ,
and yields bhdac(X) , X>θbdac.
^
^
^
o
O
O
^
n
16
Under review as a conference paper at ICLR 2022
Theorem 9 (Formal restatement of Theorem 3 on logistic regression with DAC). Let
E [1 Pn=IkPN χik2∣pN ≤ CN2 for some CN > 0. Then, learning logistic regression with
YDac regularιzatlon h(Xu) =h(Xuj) yieldsthat,for anyδ ∈ (0,1),wιth pus
L(bdac) - L(h*).
CoCn + CoD VZlog(1∕δ)
where under Assumption 1 and Assumption 2, CN .，d - daug (δ∕2).
Proofof Theorem 9. Notice that the logistic loss l(∙, y) is 2-Lipschitz for all y ∈ Y, as dl(Zy =
σ(z) - y, whose absolute value is bounded by 2. Meanwhile, with kθk2 ≤ Co and kXk2 ≤ D for all
θ and X, the logistic loss is also bounded: l θ>X, y ≤ log 2+ CoD. Then, applying Proposition 8,
we have that for any δ ∈ (0, 1), with probability at least 1 - δ∕2,
L(bdac) — L(h*) . Rn (TAXu(H)) + CoDD'^ .
It left to bound the Rademacher complexity of Tdac (H). First notice that the sigmoid function σ(∙)
A,Xu
is a bijective function, the data augmentation consistency constraints are equivalent to
(Ab(Xu ) - MXu) θ = 0.
Thus, we have
TAXu (H) = {h(x) = θ>x I θ ∈N, kθk2 ≤ Co},
with which we are ready to bound the Rademacher complexity of T dbacu (H).
A,Xu
For the emprical Rademacher complexity, we have
=£ Eei ~Rad( 1)
=n Eei ~Rad( 1)
Co
n
sup	eiθ>Xi
θ∈TAdba,Xcu (H) i=1
n
sup	ei θ> PNXi
θ∈H i=1
n
~n Eei 〜Rad( 1)	EeiPN Xi
i=1
2
≤ (Ct
Co
n t
n
Eei 〜Rad( 2 ) X keiPN Xik 2
i=1
n
X kPN xi k22
i=1
=√n jtr (： PN x>xpJ .
Converting this to the population Rademacher complexity, we take expectation over X, while con-
ditioned on PN, and recall Jensen’s inequality,
≤√nE
≤√0 S
jtr (JPNX>XPn
PN
E tr (JPNX>XPn) I PN
CoCN
17
Under review as a conference paper at ICLR 2022
Leveraging Lemma 5, we have that under Assumption 1 and Assumption 2, conditioned on PN ,
with high probability over X 〜 P*,
1PNX> XPN	≤ 1.1C . 1
n 2
Then We can find some CN > 0 with 1 Pn=1 ∣∣PnXik2 ≤ CN such that,
CN ≤ (d - daug) ∙ 1PN X>XPn
By Proposition 7, we have CN ≤ d- — daug (δ∕2) with probability at least 1 一 δ∕2.
b
. d - daug .
To obtain Theorem 3, we leverage the union bound and suppress the constants such that, with prob-
ability at least 1 - δ,
L(hdac) 一 L(h*) < Jd - daug (δ/2) + r log(1∕6 .	(4)
nn
Finally, taking δ = 2/n and applying the Jensen’s inequality to the two terms in Equation (4)
complete the proof.
B.2 Formal Result on Two-layer Neural Network
In the two-layer neural network regression setting described in Section 4.2, training with the DAC
regularization can be formulated explicitly as
Bdac, Wdac = argmin IIIy —(XB)+ w||；
B∈Rd×q,w∈Rq 2n
s.t. B = [b1. . . bk . . . bq] , bk ∈ Sd-1 ∀ k ∈ [q],	∣w∣1 ≤ Cw
Ab(Xu)B) = (MXuB)+ ,
and yields bhdac(X) , (X>Bbdac)+wbdac.
Theorem 10 (Formal restatement of Theorem 4 on two-layer neural network with DAC). Sampling
^
^
from P * that follows Assumption 1, let the augmented training set A (Xu) satisfy (a) aN ≥ 4d aug,
(b) Ab(Xu ) 一 MXu admits an absolutely continuous distribution over N⊥, and P* fulfills (c)
E h 1 Pn=1 kPNXik2∣PNi ≤ CN2 for some CN > 0. Then, learning the two-layer ReLU network
with the DAC regularization on ((xu)> B) = ((Xuj)> B) ^ives that, with high probability,
L(bdac) 一 L (h*) < σC√CN,
where under Assumption 1 and Assumption 2, CN < d— — daug (δ) with probability at least 1 一 δ.
Lemma 1. Under the assumptions in Theorem 10, every size-daug subset of rows in A(Xu) 一 MXu
is linearly independent almost surely.
Proof of Lemma 1. Since αN > daug, itis sufficient to show that a random matrix with an absolutely
continuous distribution is totally invertible 2 almost surely.
It is known that for any dimension m ∈ N, an m × m square matrix S is singular if
det(S) = 0 where entries of S lie within the roots of the polynomial equation specified by the
determinant. Therefore, the set of all singular matrices in Rm×m has Lebesgue measure zero,
λ ({S ∈ Rm×m | det(S) = 0}) = 0. Then, for an absolutely continuous probability measure μ
with respect to λ, we also have
Pμ [S ∈ Rm×m is singular] = μ ({S ∈ Rm×m ∣ det(S) = 0}) = 0.
Since a general matrix R contains only finite number of submatrices, when R is drawn from an
absolutely continuous distribution, by the union bound, P [R cotains a singular submatrix] = 0.
That is, R is totally invertible almost surely.
2A matrix is totally invertible if all its square submatrices are invertible.
18
Under review as a conference paper at ICLR 2022
Lemma 2. Under the assumptions in Theorem 10, the hidden layer in the two-layer ReLU network
learns N, the invariant subspace under data augmentations : with high probability,
x>Bbdac	= x> PN Bb dac	∀x ∈ X.
Proof of Lemma 2. We will show that for all bk = PNbk + P⊥Nbk, k ∈ [q], P⊥N bk = 0 with high
probability, which then implies that given any x ∈ X, (x>bk)+ = (x>PNbk)+ for all k ∈ [q].
For any k ∈ [q] associated with an arbitrary fixed bk ∈ Sd-1, let Xku , XkuPN +XkuP⊥N ∈ XNk be
the inclusion-wisely maximum row subset of Xu such that Xkubk > 0 element-wisely. Meanwhile,
we denote Ab(Xku) = MkXkuPN + Ab(Xku)P⊥N ∈ XαNk as the augmentation of Xku where Mk ∈
RαNk×Nk is the vertical stack of α identity matrices with size Nk × Nk. Then the DAC constraint
implies that (Ab(Xku) - MkXku)P⊥Nbk =0.
With Assumption 1, for a fixed bk ∈ Sd-1, P[x>bk > 0] = 2. Then, with the Chernoff bound,
N	2t2
P Nk < - -1 ≤ e-ɪ,
which implies that, Nk ≥ N with high probability.
Leveraging the assumptions in Theorem 10, αN ≥ 4daug implies that αNk ≥ daug . Therefore by
Lemma 1, Row Ab(Xku) - MkXku =N⊥ with probability 1. Thus, (Ab(Xku) -MkXku)P⊥Nbk =
0 enforces that P⊥N bk = 0.
Proof of Theorem 10. Lemma 2 implies that (XBb dac)+ = (XPNBbdac)+, and therefore,
TAaXu(H) = {h(x) = (x>B)+W | B = [bi …bq], bk ∈ Sd-1, (XB)+ = (XPNB)+, IWk ≤ Cw}.
In other words, the DAC regularization reduces the feasible set for B such that
B∈B,{B= [b1...bq] | kbkk =1∀k∈ [q], (XB)+ = (XPNB)+},	kWk1 ≤Cw.
Meanwhile, for the square loss, the corresponding excess risk is given by
L WaC) — L (h*) = EX
ɪ I(XBdac)+Wdac - (XB*)+w*[].
Leveraging Equation (21) and (22) in Du et al. (2020), since (B*, w*) is feasible under the con-
straint, by the basic inequality,
卜-(XBdac)+Wdac∣∣2 ≤ ky - (XB*)+w*k2,	(5)
where Bdac, B* ∈ B and IlWdac∣∣1 ≤ Cw.
Analogously to the Rademacher complexities for the bounded losses, for the square loss, we recall
the definitions of the Gaussian complexities of a vector-valued function class Φ = {φ : X → Rq},
1n q
G x(Φ)， E 、 sup—£ fgikφk (xi)
gik〜N(0,1) i.i.d. φ∈Φ n
i=1 k=1
Gn(Φ) , E GbX(Φ) .
(6)
In terms of the Gaussian complexity, for a fixed TAdbaXcu (H),
EX ∖n∣ ∣∣(XBdac)+Wdac - (XB*)+w*∣∣2
Xu
≤Ex '> (J ((XBdac)+Wdac - (XB*)+w*
<σ ∙ EX BX (TAaXu(H)MXui
X
u
19
Under review as a conference paper at ICLR 2022
where the empirical Gaussian complexity can be upper bounded by
GX (TAaXu(H"~Nk) B∈BsUi1≤R1 g>(XB)+w
E
g
≤
Bsu∈pB	(XB)>+g∞
E sup g> (XPN b)+
g b∈Sd-1
(Lemma 6, (•)+ is I-LiPschitz)
≤ —w E sup g>XPNb
n g b∈Sd-1
Eg	PNX>g2
PNX>g22
1/2
-W √tr(PχX>XPn)
un
√n t 1X kpN χik2.
Taking expectation over X 〜P*,we have that for a fixed TdbaXu (H) (i.e., conditioned on PN),
Gn TAdba,Xcu (H)
-w-N
≤k∙
Leveraging Lemma 5, we have that under Assumption 1 and Assumption 2, conditioned on PN,
with high probability over X 〜 P*,
:PNX>XPN
≤ 1.1- .1
2
Then we can find some —n > 0 with 1 Pn=IIlPNXik2 ≤ -NN such that,
-N ≤ (d - daug)，nPNx>xpN	. d - daug .
By Proposition 7, we have —n ≤，d - daug (δ) with probability at least 1 - δ.
B.3 Forml results on expansion-based data augmentation
For the formal analysis of the expansion-based data augmentations, we first introduce some helpful
notations.
With respect to an augmentation function A : X → 2X fulfilling the conditions in Definition 4, we
define the neighborhood for an arbitrary x ∈ X, as well as for any S ⊆ X, such that,
NB (x)，{x0 ∈X I A(X) ∩ A(x0) = 0} ,	NB (S)，∪χ∈s NB (x).
Then Definition 4(b) implies that the corresponding neighborhood function NB : X → 2X ofA has
non-trivial expansion:
0 ( S∩ Xk ( Xk ⇒ (NB(S)\S) ∩ Xk 6= 0 ∀k ∈ [K], S ⊆ X.
Intuitively, the non-trivial expansion guarantees that the neighborhood of any proper subset of a
class always enlarges the subset within the class.
Remark 2. The ground truth classifier is invariant throughout the neighborhood: for any aug-
mentation function A : X → 2X satisfying the conditions in Definition 4, the corresponding neigh-
borhoodfUnction satisfies that, g^ven any X ∈ X, h* (x) = h* (x0) for all X ∈ NB(x).
20
Under review as a conference paper at ICLR 2022
Notice that the previous data augmentation strength daug (Definition 2) is not well defined for Defini-
tion 4 (since X needs not be a subset of Rd). Therefore we need a new notion of data augmentation
strength.
Remark 3. The strength of expansion-based data augmentations (Definition 4) is characterized by
the size of A(x). Intuitively, for any x ∈ X, the augmentations can be stronger (i.e., there exists
x0 ∈ A(x) that is more different from x) if A(x) is inclusion-wisely larger.
In addition, for an arbitrary classifier h ∈ H, we denote the majority label with respect to h for each
class,
ybk , argmax Px h(x) = y x ∈ Xk	∀ k ∈ [K],
y∈[K]
along with the respective class-wise local and global minority sets,
K
Mk , {x ∈Xk I h(x)= bk } ( Xk ∀ k ∈ [K],	M , [ Mk.
k=1
Infinite unlabeled data. As a warm-up, we simplify the setting by enforcing consistency over the
population such that learning with the DAC constraints can be expressed as,
1n
bXac , argmin Ldac(h) = — Vl {h (Xi) = h* M)}
h∈H	n i=1
s.t. h(X) = h (X0) ∀ X ∈ X, X0 ∈ A(X).
We can also formulate algorithm in terms of the DAC operator,
bhdXac	, argmin	Lb0d1ac(h),	TAdaXc ,{h ∈ H |	h(X)	=	h(X0)	∀X ∈	X,	X0	∈ A(X)}	.
h∈TAda,Xc (H)	,
Lemma 3. Given any h ∈ TAda,Xc (H), for each k ∈ [K], there exists some ybk ∈ [K] such that
h(X) = ybk for all X ∈ Xk.
ProofofLemma 3. It is sufficient to show that Mk = 0 for all k ∈ [K]. By contradiction, suppose
Mk = 0. Then by the non-trivial expansion assumption in Definition 4, (NB(Mk)\Mk) ∩Xk = 0.
That is, there exists some X ∈ (NB (Mk)\Mk) ∩ Xk. But on one hand, X ∈ NB(Mk) implies
that we can find some X0 ∈ Mk and X00 ∈ A (X) ∩ A (X0). Therefore by the consistency constraint
h ∈ TAda,Xc (H), h(X00) = h(X) = h(X0). On the other hand, X ∈ Xk\Mk implies that h(X) = ybk 6=
h(X0), and leads to a contradiction.
Restating Theorem 5 with the formal notations in the appendix: with high probability,
L01 RX)-L0i (h*) ≤ Klog-n.
Proof of Theorem 5. By Lemma 3, since
TTX (H) ⊆{h ∈H∣ h(x) = bk ∀ x ∈Xk } where	{b ∈ [K ]}在的,
the consistency constraint yields a finite feasible classifier class with ITAda,Xc (H)I = K! ≤ KK.
Meanwhile, since A(x) ⊆ {x0 ∈ X | h*(x) = h*(x0)}, by construction, We know that TAaX(H) is
realizable (i.e., h* ∈ TAaX(H) such that Ldac(h*) = 0). Therefore,
P hL01 bhdXac) >i ≤Ph∃h∈TAda,Xc(H) III L01 (h) > ∧ Lb0d1ac(h) = 0i ,
where for every classifier h ∈ TAda,Xc (H),
P hL01 (h) >	∧ Lb0d1ac(h) = 0i ≤ (1-)n ≤ e-n.
21
Under review as a conference paper at ICLR 2022
Then by the union bound, we have
P L01 bhdXac >	≤ X P L01 (h) > ∧ Lb0d1ac(h) = 0
h∈TAda,Xc (H)
≤∣τAax (H)I ∙ e-en.
The proof is completed by upper bounding the above, P L01 bhdXac >	≤ 1/n, when taking

log ITAaX (H) I + log n < K log K + log n
n
n
and observing that L01 (h*) = 0.
Finite unlabeled data. For the finite unlabeled data case, we start by introducing some addi-
tional concepts and notations. We concretize the classifier class H with a proper function class
F ⊆ f : X → RK such that H
h(x) , argmaxk∈[K] f (x)k II f ∈ F . Specifically, we
adapt the existing setting in Wei et al. (2021); Cai et al. (2021), and consider F as a class of fully
connected neural networks. To constrain the feasible hypothesis class through the DAC regulariza-
tion with finite unlabeled observations Xu = [x1u, . . . , xuN]>, we leverage, from Wei & Ma (2021),
the notion of all-layer-margin, m : F × X × Y → R≥0, that measures the maximum possible
perturbation in all layers of f while maintaining the prediction y . Precisely, given any f ∈ F such
that f (x) = Wp夕(...夕(Wιx)...) for some activation function 夕：R → R and parameters
{W∣ ∈ Rdι×dι-1 }p=1, We can write f = f2p-1 ◦•••◦ fι where f2∣-1(x) = W∣x for all ∣ ∈ [p]
and f2∣(z)= 夕(Z) for ∣ ∈ [p - 1]. For an arbitrary set of perturbation vectors δ = (δι,..., δ2p-1)
such that δ2ι-1, δ2ι ∈ Rdι for all ι, let f(x, δ) be the perturbed neural network defined recursively
such that
ez1 = f1 (x) + kxk2 δ1,
ezι = fι (ezι-1) + kezι-1k2 δι ∀ ι = 2, . . . , 2p - 1,
f(x,δ)=ez2p-1.
The all-layer-margin m(f, x, y) measures the minimum norm of the perturbation δ such that f(x, δ)
fails to provide the classification y,
U 2p-1
m(f, x, y) ,	min	kδιk22 s.t. argmaxf(x,δ)k 6= y.	(7)
δ=(δ1 ,...,δ2p-1 )	ι=1	k∈[K]
With the notion of all-layer-margin established, for any A : X → 2X that satisfies conditions in
Definition 4, the robust margin is defined as
mA(f,x) , sup m f,x0,argmaxf(x)k .
x0∈A(x)	k∈[k]
With merely finite unlabeled data, comparing to enforcing consistency over population, we need
stronger assumptions on data augmentations in addition to the ones in Definition 4, as specified in
Definition 5 below.
Definition 5 ((F, τ)-expansion-based data augmentation). With respect to the given function class
F and some proper constant τ > 0, Let A : X → 2X be a function that satisfies conditions in
Definition 4, and additionally,
sup inf mA(f, x) > τ.	(8)
f∈F x∈X
Then for any sample x ∈ X, x0 ∈ X is a (F, τ)-expansion-based data augmentation of x induced
by A if x0 ∈ A(x).
22
Under review as a conference paper at ICLR 2022
Remark 4. The existence of the (F, τ)-expansion-based data augmentations relies on the suitable
choice of τ with respect to the given F. For a fixed F, as τ increases, Equation (8) imposes stronger
assumption on A (i.e., A(x) is forced to be inclusion-wisely smaller, where A becomes weaker),
while better theoretical guarantee (Theorem 12) can be achieved.
We consider a class of p-layer fully connected neural networks with maximum width q,
F = {f : X → RK I f = f2p-1。…。fl, f2∣-1 (x) = W∣x, f2∣(z)=夕(Z) ∀ ι =1,...,p},
where with Wι ∈ Rdι×dι-1 for all ι ∈ [p], q , maxι=0,...,p dι. The goal is to learn a classifier
bhdXauc = argmaxk∈[K] fbnd,aNc (x)k where fbnd,aNc ∈ F is a fully connected neural network. With respect
to F, we choose a suitably large τ > 0 such that there exists a A : X → 2X fulfilling the conditions
in Definition 5. To enforce sufficiently strong consistency of f with finite unlabeled samples, we
incorporate the DAC regularization with a positive robust margin such that,
1n
bXuC，argmin bdac(h) = - £ 1 {h (Xi) = h* (xi)}	(9)
s.t. mA(f, xiu) > τ ∀ i ∈ [N].
In terms of the DAC operator, the algorithm can be restated as bhdXauc , argminh∈T dac (H) Lb0d1ac(h)
A,Xu
with
TAaXXu (H) , {h ∈ H I mA(f, xU) >τ ∀ i ∈ [N ]}.
Definition 6 (Expansion assumptions, Wei et al. (2021); Cai et al. (2021)). The marginal distribution
P * (x) satisfies
(a)	(q, ξ)-constant expansion if given any S ⊆ X with P * (S) ≥ q and P * (S ∩ Xk) ≤ 2 for
allk ∈ [K], P* (NB (S)) ≥ min{P* (S),ξ} +P* (S);
(b)	(a, c)-multiplicative expansion iffor all k ∈ [K], given any S ⊆ X with P* (S ∩ Xk) ≤ a,
P * (NB (S) ∩ Xk) ≥ min {c ∙ P * (S ∩ Xk), 1}.
Proposition 11 (Wei et al. (2021) Theorem 3.7, Cai et al. (2021) Proposition 2.2). For any δ ∈
(0,1), with probability at least 1 一 δ∕2, there exists some μ such that
o	o	e Pp=ι √qIIWikF ,	/iog(i/s) + PlogNʌ
PP* [∃ x ∈ A(X)S.t. h(X) = h(x )] ≤ μ ≤ O I ---------τ√N----------+ V----------N---------- I
for all h ∈ TAaXu (H), where O (∙) hidespolylogarithmic factors in N and d.
Theorem 12 (DAC with expansion-based data augmentations over finite samples). With proper
choices of τ > 0, for any A : X → 2X inducing (F, τ)-expansion-based data augmentations,
learning the classifier with DAC regularization, Equation (9), provides that, for any δ ∈ (0, 1), with
probability at least 1 一 δ,
L01 殿U) 一 Loi (h*) ≤ 4R + 产.40,	(10)
In specific, for some q < 1, c > 1, with a sufficiently large unlabeled sample size N such that
μ ≤ 1 min {c — 1,1},
(a)	when P * (x) satisfies (q, 2μ)-constant expansion,
R ≤ ^KogK
+ 2Kmax {q, 2μ},
(b)	while when P * (x) satisfies (1, c)-multiplicative expansion,
R≤
2K log K
n
+	4κμ
min {c — 1,1}
23
Under review as a conference paper at ICLR 2022
where we recall from Proposition 11 that,
V e Pp=I √qkW∣kF , JlOg(10 + PlogN∖
μ ≤ Ol―T√N— + V----------------------N---------)
Lemma 4 (Cai et al. (2021), Lemma A.1). For any h ∈ TTXu (H), When P* satisfies
(a)	(q, 2μ)-constant expansion with q < 2, P * (M) ≤ max {q, 2μ};
(b)	(2 ,C -multiplicative expansion with c > 1+4μ, P * (M) ≤ max
{碧,2μ}.
Proof of Lemma 4. We start with the proof for Lemma 4 (a). By definition of Mk and ybk, we know
that Mk = M ∩ Xk ≤ 2. Therefore, for any 0 < q < ɪ, one of the following two cases holds:
(i)	P* (M) < q;
(ii)	P* (M) ≥ q. Since P* (M ∩ Xk) < 2 for all k ∈ [K] holds by construction, with the
(q, 2μ)-constant expansion, P * (NB (M)) ≥ min {P* (M), 2μ} + P* (M).
Meanwhile, since the ground truth classifier h* is invariant throughout the neighborhoods,
NB (Mk)∩NB (Mko) = 0 for k = k： and therefore NB (M)\M = Sk=1 NB (Mk) ∖Mk
with each NB (Mk) \Mk disjoint. Then, we observe that for each x ∈ NB (M) \M,
here exists some k = h* (x) such that x ∈ NB (Mk) \Mk. x ∈ Xk\Mk implies that
h (x) = ybk, while x ∈ NB (Mk) suggests that there exists some x0 ∈ A (x) ∩ A (x00)
where x00 ∈ Mk such that either h (x0) = ybk and h (x0) 6= h (x00) for x0 ∈ A (x00), or
h (x0) 6= ybk and h (x0) 6= h (x) for x0 ∈ A (x). Therefore, we have
P * (NB (M) \M) ≤ 2Pp * [∃ X ∈ A(X) s.t. h(x) = h(x0)] ≤ 2μ.
Moreover, since P * (NB (M)) 一 P * (M) ≤ P * (NB (M) \M) ≤ 2μ, we know that
min {P * (M), 2μ} + P * (M) ≤ P * (NB (M)) ≤ P * (M) + 2μ.
Thatis, P * (M) ≤ 2μ.
Overall, we have P * (M) ≤ max {q, 2μ}.
To show Lemma 4 (b), we recall from Wei et al. (2021) Lemma B.6 that for any c > 1 + 4μ,
(2, c) -multiplicative expansion implies (C-1,2μ) -constant expansion. Then leveraging the proof
for Lemma 4 (a), with q = C-I, we have P * (M) ≤ max {若,2μ}.	■
Proof of Theorem 12. To show Equation (10), we leverage the proof of Proposition 8, and observe
that B = 1 with the zero-one loss. Therefore, when conditioned on TAda,XCu (H), for any δ ∈ (0, 1),
with probability at least 1 一 δ∕2,
L01 殿U) 一 Loi (h*) ≤ 4Rn (加◦ TTXu (H)) +『呼传，.
For the upper bounds of the Rademacher complexity, let μ，SuPheTdacu (H) P* (M) where M
denotes the global minority set with respect to h ∈ TAda,XCu (H). Lemma 4 suggests that
(a)	when P * satisfies (q, 2μ)-constant expansion for some q < 2, μ ≤ max {q, 2μ}; while
(b)	when P * satisfies (2, c)-multiplicative expansion for some c > 1 + 4μ, μ ≤ ma；-]4.
Then, it is sufficient to show that, conditioned on TAda,XCu (H),
Rn (loi ◦ TtXXu(H)) ≤ J2KngK +2Ke.	(11)
To show this, we first consider a fixed set of n observations in X, X = [X1, . . . , Xn]> ∈ Xn. Let
the number of distinct behaviors over X in TAda,XCu (H) be
S(TtXXu (H), X) ,∣{[h (Xi) ,...,h (Xn)] I h ∈ TtXu (H)}∣ .
24
Under review as a conference paper at ICLR 2022
Then, by the Massart’s finite lemma, the empirical rademacher complexity with respect to X is
upper bounded by
RXX (101 ◦ TAaXu (H)) ≤ t
2logS (TAaXu (H), X)
n
By the concavity of，log (∙), We know that,
Rn (l01 ◦ TAXu (H)) =EX [Rx (l01 ◦ TAXu (H))i ≤ EXt
2logs (TAaXu (H), X)
n
2logEX s TAda,Xcu (H), X
n
(12)
Since P* (M) ≤ e ≤ ɪ for all h ∈ TAXu (H), We have that, conditioned on TAaXu (H),
EX [s (TAaXu(H), X)] ≤ XX (n)er (1 - e)n-r ∙ KK ∙ Kr
r=0 r
≤KK XX (n) (eκ)r (1 - e)n-r
r=0 r
=k K (i - e+Ken
≤KK ∙ eKnμ.
Plugging this into Equation (12) yields Equation (11). Finally, the randomness in TAda,Xcu (H) is
quantified by μ, μ, and upper bounded by Proposition 11.	■
B.4 Supplementary Example: domain adaptation
As a supplementary example, we demonstrate the possible failure of the ERM on augmented training
set, and how the DAC regularization can serve as a remedy, with an illustrative linear regression
problem in the domain adaptation setting: where the training samples are drawn from some source
distribution Ps, while the excess risk is tested over a related but different distribution Pt, known
as the target distribution. Specifically, assuming that EPs [y|x] and EPt [y|x] are distinct, but there
exists some invariant feature subspace Xr ⊂ X such that Ps [y|x ∈ Xr] = Pt [y|x ∈ Xr], we aim
to demonstrate the advantage of the DAC regularization over the ERM on augmented training set,
with a provable separation in the respective excess risks.
≤
Figure 4: Causal graph shared by Ps and Pt .
Source and target distributions. Formally, the source and target distributions are concretized
with the causal graph in Figure 4. For both Ps and Pt , the observable feature x is described via a
linear generative model in terms of two latent features, the ‘invariant’ feature ζiv ∈ Rdiv and the
‘environmental’ feature ζe ∈ Rde :
x = g(ζiv, ζe) , S [ζiv; ζe]
+
25
Under review as a conference paper at ICLR 2022
where S = [Siv, Se] ∈ Rd×(div+de) (div + de ≤ d) consists of orthonormal columns. Let the label
y depends only on the invariant feature ζiv for both domains,
y = (θ*)> X + Z = (θ*)> SivZiv + z, Z 〜N (0,σ2), Z ⊥ Ziv,
for some θ* ∈ Range (Siv) such that PS [y∣Ziv] = Pt [y∣Ziv], while the environmental feature Ze
is conditioned on y, Ziv , (along with the Gaussian noise Z), and varies across different domains e
with EPS [y|x] = EPt [y∣x]. In other words, with the square loss l(h(x), y) = 1 (h(x) 一 y)2, the
optimal hypotheses that minimize the expected excess risk over the source and target distributions
are distinct. Therefore, learning via the ERM with training samples from Ps can overfit the source
distribution, in which scenario identifying the invariant feature subspace Range (Siv) becomes in-
dispensable for achieving good generalization in the target domain.
Moreover, we assume the following regularity conditions on the source and target distributions:
Assumption 3 (Regularity conditions for Ps and Pt). Let Ps satisfy Assumption 1, and Pt satisfy
the following: let EPt [xx>]	0, and
(a)	for the invariant feature, ct,ivIdiv 4 EPt[ZivZi>v] 4 Ct,ivIdiv for some Ct,iv ≥ ct,iv = Θ(1);
(b)	for the environmental feature, EPt [ζeζ> ] < ct,eLe for some ct,e > 0, and EP t [z ∙ Ze] = 0.
Training samples and data augmentations. For a fair comparison between learning with the
DAC regularization and the ERM on augmented training set, we restrict to the supervised learning
setting: Xu = X ∈ Xn. Recall that we denote the augmented training sets, including and excluding
the original samples, respectively with
N(X) = [xi；…；xn； xi,1；…；xn,1；…；xi,a；…；x'α] ∈ X(1+a)n,
αn
A(X) = [xi,1； •一；xn,1； •一；xi,a； •一；x%α] ∈ X .
In particular, we consider a set of augmentations that only perturb the environmental feature Ze ,
while keep the invariant feature Ziv intact,
Si>vxi	=	Si>vxi,j,	Se>xi	6=	Se>xi,j	∀i ∈	[n], j ∈	[α].
(13)
We recall the notion daug
rank Ab (X) 一 MX
daug ≤ de), and assume that X and A(X) are representative enough:
rank Ae (X) 一 MfX
(notice that 0 ≤
Assumption 4 (Diversity ofX and A(X)). (X, y) ∈ Xn × Yn is sufficiently large with n	ρ4div,
θ* ∈ span {xi | i ∈ [n]}, and daUg = de.
Excess risks in target domain. Learning from the linear hypothesis class H
h(x) = θ>x θ ∈ Rd , with the DAC regularization on h (xi) = h (xi,j), we have
bdac = argmin ɪ ∣∣y 一 Xθ∣∣2,
θ∈T dbac(H) 2n
TAdba,Xc (H) = nh (x) = θ>x	Ab(X)θ = MXθo ,
while with the ERM on augmented training set,
bda-erm =aθgmm2(ι⅛n fy-N(X)θ1,
where M and M denote the vertical stacks of α and 1+α identity matrices of size n×n, respectively
as denoted earlier.
We are interested in the excess risk on Pt:	Lt (θ) 一 Lt (θ*) where Lt (θ)	，
EPt(χ,y) [1 (y - x>θ)2].
Theorem 13	(Domain adaptation with DAC). Under Assumption 3(a) and Assumption 4, θbdac sat-
isfies that, with constant probability,
EPS [Lt(bdac) - Lt(θ*)]
σ2div
(14)
n
26
Under review as a conference paper at ICLR 2022
Theorem 14	(Domain adaptation with ERM on augmented samples). Under Assumption 3 and
Assumption 4, θbdac and θbda-erm satisfies that,
Eps [Lt(bda-erm) - Lt(θ*)] ≥ EPs [Lt(bdac) - Lt(θ*)] + c* ∙ EERe,
for some EERe > 0.
(15)
In contrast to θbdac where the DAC constraints enforce Se>θbdac = 0 with a sufficiently diverse Ab (X)
(Assumption 4), the ERM on augmented training set fails to filter out the environmental feature in
θbda-erm: Se>θbda-erm 6= 0. As a consequence, the expected excess risk of θbda-erm in the target
domain can be catastrophic when ct,e → ∞, as instantiated by Example 2.
Proofs and instantiation. We first recall from the beginning of Appendix B that
PN，Id -(N(X)- MX)t (N(X)- MX)
•	,1	,1	1	,	,1 T	•	/1 ^V \	11	C Gm 1» Λ--cr Trl ,1
is the orthogonal projector onto the dimension-(d - daug) null space of A(X) - MX. Furthermore,
let Piv , SivSi>v and Pe , SeSe> be the orthogonal projectors onto the invariant and environmental
feature subspaces, respectively, such that x = Sivζiv + Seζe = (Piv + Pe) x for all x.
Proof of Theorem 13. By construction Equation (13), Ab(X) - MX Piv = 0, and it follows that
Piv 4 PN. Meanwhile from Assumption 4, daug = de implies that dim (PN) = div. Therefore,
Piv = PN, and the data augmentation consistency constraints can be restated as
TAaX(H) , {h (x) = θ>x I PNθ = θ} = {h (x) = θ>x I Pivθ =处
Then With θ* ∈ span {xi | i ∈ [n]} from Assumption 4,
bdac - e* = 1 ∑ XivPiv χ>(χPiv e* + Z)- e* = J ∑ XivPiv χ>z,
where	ΣXiv	，	1 PivX>XPiv.	Since	bdac	-	θ*	∈	Col(Siv),	We have
EPt [z ∙ x>Pe(θdac - θ*)] = 0. Therefore, let Σx,t，EPt [xx>], with high probability,
EPs [Lt(bdac) - Lt(θ*)] = Ep`
=tr (2nEPS [zz>] EPs ](nPivX>XPiv)
=tr(2n EPshlXivi 4,)
2
≤ Ct,iv 2n tr (EPs 住 XivD
2
.2j tr ((EPs [Pivxx>Piv])t)
≤ : tr(Piv) < 孕.
2nc	2n
x,t
(Lemma 5, w.h.p.)
ProofofTheorem14. Let ∑A(x) = ,+"nA(X) A(X). Then with θ* ∈ span {xi | i ∈ [n]}
from Assumption 4, we have θ = Σ^χ^ΣN(X)θ*. Since θ ∈ Col(Siv), MX。* =
一一 一，	二,一、-， - ― __________ . . •…
MXPiv θ* = A(X)θ *. Then, the ERM on the augmented training set yields
θbda-erm -θ*
1
En ς A(X)A(X)f(χθ*+Z)- ∑ A(X产 A(XU
1
(1 + α)n
^ 4∙	~ .	. -T ---
btAe(X)Ae(X)>Mfz.
27
Under review as a conference paper at ICLR 2022
Meanwhile with EPt [z ∙ Ze] = 0 from Assumption 3, We have EPt [z ∙ Pex] = 0. Therefore, by
recalling that Σx,t , EPt [xx>],
Lt(θ) - Lt(θ*) = EPt(x) 1 (x>(θ — θ*))2 + Z ∙ x>Pe(θ — θ*)	= 1 kθ* -。|底
such that the expected excess risk can be expressed as
EPshLt(bda-erm) -Lt(θ*)] = 2(i +1a)2n2 tr (EPshςA(X) (N(X)>fzz>f>N(X)) ςAe(Xj »),
where let ΣAe(Xe) , PeΣAe(X)Pe,
EPshςAe(X) (HX)>f zz>f >Hχ)) ςAe(Xj
<	EPsh(PivςAe(X)Piv + pe$Ae(X)Pe) N(χ)>fZZ>f>HX)(PivςAe(X)Piv + pe$Ae(X)Pe)i
<	σ2(1 + α)2n ∙ EPs [∑Xivi + EPs [∑Ae(Xe)N(Xe)>fzz>f>N(Xe)ΣAe(XeJ .
We denote
EERe，tr (EPJ _	? ∑ LX J(Xe )>f zz>M>N(Xe)∑ ∖	]),
2(1 + α)2n2 A(Xe)	A(Xe)
and observe that
EERe = EPs
1 (1⅛ςAe(Xe)HXe)>fz∣L > 0
Finally, we complete the proof by partitioning the lower bound for the target expected excess risk of
θbda-erm into the invariantand environmental parts such that
EPshLt(bda-erm) - Lt(θ*)]
≥ tr (σ2 EPsh级」4,1 + tr (EPs ]2(1+1α)2n2 ςAe(Xe)N(Xe)>fzz>f>N(Xe)ςAe(Xe)] 2,t)
、------------{z----------}	、-------------------------------{-------------------------------}
=E[Lt (θ~dac)-Lt (θ* )]	expected excess risk from environmental feature SUbSPace≥ct,e∙EERe
≥ EPs [Lt(θdac) — Lt(θ*)] + Ct,e ∙ EERe.
Now we construct a specific domain adaptation example with a large separation (i.e., proportional to
de) in the target excess risk between learning with the DAC regularization (i.e., θbdac) and with the
ERM on augmented training set (i.e., θbda-erm).
Example 2. We consider Ps and Pt that follow the same set of relations in Figure 4, except for the
distributions over e where Ps (e) 6= Pt (e). Precisely, let the environmental feature ζe depend on
(ζiv , y, e):
Ze = sign (y — (θ*)> SivZiv) e = Sign(z)e, Z 〜N(0, σ2), Z ⊥ e,
where e 〜N (0, Ide) for Ps(e) and e 〜N (0,σJde) for Pt(e), σt ≥ ct,e (recall ct,e from
Assumption 3). Assume that the training set X is sufficiently large, n 》de + log (1∕δ) for some
given δ ∈ (0,1). Augmenting X with a simple by common type of data augmentations - the linear
transforms, we let
A(X) = [X；(XAi)； ... ； (XAα)] ,	Aj= Piv + Uj V>,	Uj, Vj ∈ COl(Se)	∀ j ∈ H
and define
ν1 , max {1} ∪ {σmax(Aj) | j ∈ [α]}
and
V2 , σmin	)
1+α
α
Id+XAk
j=1
28
Under review as a conference paper at ICLR 2022
where σmin(∙) and σmaχ(∙) refer to the minimum and maximum singular values, respectively. Then
under Assumption 3 and Assumption 4, with constant probability,
EPs hLt(bda-erm) - Lt(θ*)] & EPs [Lt(bdac) - Lt(θ*)] + ct,e ∙ σ2ne.
Proof of Example 2. With the specified distribution, for E = [e1;. . . ; en] ∈ Rn×de,
1α	2
ς A(Xe) = (1+αn Se Ie>e + X A>ETEAjjS> 4 IV SeETES>,
-N(Xe)>Mz =	1	Id + X Aj	1 SeE> |z|.
(1+ α)n	1+ α j=1	n
By Lemma 5, under Assumption 3 and Assumption 4, we have that with high probability, 0.9Ide 4
nE>E 4 1.1Ide. Therefore with E and Z being independent,
EERe = EPs "2 J W0> ςA(Xe/(Xe)>fz□
≥ σ2 吗 tr (eps "(1 SeE>ES>) ]]
2n ν13 4	n	e
&⅛ 詈 tr(SeS>)
n ν1
> σ2de
〜2n ,
and the rest follows from Theorem 14.
C Technical Lemmas
Lemma 5. We Consider a random vector X ∈ Rd with E[x] = 0, E[xx>] = Σ, and x = £-1/2 X
3 being ρ2-subgaussian. Given an i.i.d. sample of x, X = [x1, . . . , xn ]T, for any δ ∈ (0, 1), if
n》ρ4d, then 0.9Σ 4 *X>X 4 1.1Σ with probability high probability.
Proof. We first denote PX，ΣΣ* as the orthogonal projector onto the subspace X ⊆ Rd supported
by the distribution of x. With the assumptions E[x] = 0 and E[xx>] = Σ, We observe that E [x] = 0
and E [xx>] = E [xΣ-1x>] = PX. Given the sample set X of size n》ρ4 (d + log(1∕δ)) for
some δ ∈ (0,1), we let U = * P*=1 xiΣ-1x7 一 PX. Then the problem can be reduced to
showing that, with probability at least 1 - δ, kUk2 ≤ 0.1. For this, we leverage the -net argument
as following.
For an arbitrary v ∈ X ∩ Sd-1, we have
**
v>Uv = n X «廉再-&" 一 1) = n X ((v>xi)2 -1),
i=1	i=1
where, given Xi being ρ2-subgaussian, v>xi is ρ2-subgaussian. Since
E h(v>xi)2i = v>E [x^xj] V = 1,
we know that (v>x,2 — 1 is 16ρ2-subexponential. Then, we recall the Bernstein,s inequality,
P [∣v>uv∣ > e] ≤ 2eχp 卜2min ((ɪ,忌!!.
3In the case where Σ is rank-deficient, we slightly abuse the notation such that Σ-1/2 and Σ-1 refer to the
respective pseudo-inverses.
29
Under review as a conference paper at ICLR 2022
Let N ⊂ X ∩ SdT be an eɪ-net such that ∖N| = eO(d). Then for some 0 < ⑦ ≤ 16ρ2, by the
union bound,
≤ 2 |N| exp
(16P2)2J ≤ δ
P max : ∣vτUv∣ > e2
v∈N 1	1
一，八	n
≤ exp I O (d)--
whenever n >	(我 ) (Θ (d) + log ɪ). By taking
max ∣vτUv∣ ≤ ∈2 with high probability when n >
sufficient.
δ = exp (-4 (y∣p2) n), we have that
4 (1∣ρ2) Θ (d), and taking n》ρ4d is
Now for any V ∈ X ∩ Sd-1, there exists some v0 ∈ N such that ∣∣v - v[b ≤ “ Therefore,
vτUv ∣	=	∣ v,τUv,	+	2v,τU	(v	- v0) + (v -	VO)T U	(v	- v0) ∣
≤ (max： IVTUV ∣ ) +2 kuib Mil? l∣v-v1∣2 + |lUk2 kv-v0k2
≤	(max1 VTUV ∣ ) + ∣∣u∣∣2 (2eι + e2) ∙
Taking the supremum over v ∈ Sd-1, with probability at least 1 - δ,
maxd_1 ： IVTUV ∣ = kU|2 ≤ e2 + ∣∣uk2 (2eι + /) ,	llU|2 ≤ ɔ~~/~~72
v∈X∩ Sd-1	2 — (1 + €1 )
With €i = 3 and € = 45, We have 2_(：，])2 =击.
Overall, if n》ρ4d, then with high probability, we have ∣∣U∣∣2 ≤ 0∙1.
Lemma 6. Let U ⊆ Rd be an arbitrary non-trivial subspace in Rd, and g 〜N (0, Id) be a
Gaussian random vector. Then for any continuous and Ci -Lipschitz function 夕:R → R (i.e.,
|夕(U)—夕(u，)| ≤ Ci ∙ |u — u0∖ for all u, u0 ∈ R),
Eg sup gτ^(u)
Lu∈U
≤ Ci ∙ Eg sup gτu
Lu∈U .
where 夕 acts on U entry-wisely,(夕(u)j =夕(Uj). In other words, the Gaussian width ofthe image
set 夕(U)，{夕(U) ∈ Rd | u ∈ U} is upper bounded by that of U scaled by the Lipschitz COnStant
30
Under review as a conference paper at ICLR 2022
Proof.
Eg SUp g%(u) =WEg
u∈U	2
sUp g>i?(u) + sup g>g(u)
u∈U	u0∈U
=WEg	sup g> W(U)-中(U))
2	u,u0∈U
≤ - Eg	sup 52|gj | k(uj') 一 °(Uj)I
2	u,u0∈U j=1
∙.∙ φ is Ci-LiPschitz
Cd
≤一Eg	sup E |gj| Iuj- Ujl
2	u,u0∈U j=1
=--Eg sup g> (u — u0)
2	u,u0∈U
=一Eg sup g>u + sup g> (—u0)
2	u∈U	u0∈U
=Ci ∙ Eg sup g>u
u∈U
D Experiment Details
In this section, we Provide the details of our exPeriments. Our code is adaPted from the Publicly
released rePo: https://github.com/kekmodel/FixMatch-pytorch.
Dataset: Our training dataset is derived from CIFAR-100, where the original dataset contains 50,000
training samPles of 100 different classes. Out of the original 50,000 samPles, we randomly select
10,000 labeled data as training set (i.e., 100 labeled samPles Per class). To see the imPact of different
training samPles, we also trained our model with dataset that contains 1,000 and 20,000 samPles.
Evaluations are done on standard test set of CIFAR-100, which contains 10,000 testing samPles.
Data Augmentation: During the training time, given a training batch, we generate corresPonding
augmented samPles by RandAugment (Cubuk et al., 2020). We set the number of augmentations Per
samPle to 7, unless otherwise mentioned.
To generate an augmented image, the RandAugment draws n transformations uniformaly at random
from 14 different augmentations, namely {identity, autoContrast, equalize, rotate, solarize, color,
Posterize, contrast, brightness, sharPness, shear-x, shear-y, translate-x, translate-y}. The RandAug-
ment Provides each transformation with a single scalar (1 to 10) to control the strength of each of
them, which we always set to 10 for all transformations. By default, we set n = 2 (i.e., using 2 ran-
dom transformations to generate an augmented samPle). To see the imPact of different augmentation
strength, we choose n ∈ {W, 2, 5, W0}. ExamPles of augmented samPles are shown in Figure 3.
Parameter Setting: The batch size is set to 64 and the entire training Process takes 215 stePs.
During the training, we adoPt the SGD oPtimizer with momentum set to 0.9, with learning rate for
step i being 0.03 X cos (2⅛×‰).
Additional Settings for the semi-supervised learning results: For the results on semi-suPervised
learning, besides the 10,000 labeled samples, we also draw additionally samples (ranging from
5,000 to 20,000) from the training set of the original CIFAR-100. We remove the labels of those
additionally sampled images, as they serve as “unlabeled” samples in the semi-supervised learning
setting. The FixMatch implementation follows the publicly available on in https://github.
com/kekmodel/FixMatch-pytorch.
E Illustrative Example
31
Under review as a conference paper at ICLR 2022
Here we present an illustrative example for lo-
gistic regression, which numerically shows the
benefits of DAC over ERM, and also clearly
demonstrates the impact of different daug and
α.
Example 3. Consider a 30-dimensional logis-
tic regression. The original training set con-
tains 50 samples. The inputs xis are gener-
ated independently from N (0, I30) and we set
θ* = [θ*; 0] with θ* 〜N(0, I3) and 0 ∈ R27.
Figure 5: Comparison of DAC regularization and
DA-ERM for logistic regression (Example 3).
The results precisely match Theorem 3 and the
discussion afterward. As suggested by the the-
oretical analysis, the performance of DAC only
depends on the daug , but not α. Further, given
the same augmented dataset, DA-ERM is always
worse than DAC. The gap is particularly signifi-
cant when the number of augmentations is small
(i.e., a small α).
To generate the augmentations, we first specify
a parameter daug and leave the first 30 - daug
elements of each xi unchanged, and replace the
later daug elements of each xi with a new vector
randomly generated from N(0, Idaug). Further,
we generate α augmentations for each of the xi.
For any α ≥ 1, the augmentation will perturb
daug coordinates with probability 1.
The results for daug ∈ {20, 25} and various αs
are presented in Figure 5. The test error clearly
matches Theorem 3 and the discussion after-
ward: 1) The generalization performance of DAC only relies on daug but not α, and larger daug
gives smaller testing error. 2) The performance of DA-ERM crucially depends on α, when α is
small (i.e., limited augmentations), the performance between DAC and DA-ERM is very significant.
And when we further increases α, DA-ERM can only approach to DAC but not out-perform DAC.
32