Under review as a conference paper at ICLR 2022
Compressed-VFL:	Communication-Efficient
Learning with Vertically Partitioned Data
Anonymous authors
Paper under double-blind review
Ab stract
We propose Compressed Vertical Federated Learning (C-VFL) for
communication-efficient training on vertically partitioned data. In C-VFL,
a server and multiple parties collaboratively train a model on their respective
features utilizing several local iterations and sharing compressed intermediate
results periodically. Our work provides the first theoretical analysis of the effect
message compression has on distributed training over vertically partitioned data.
We prove convergence of non-convex objectives to a fixed point at a rate of
O(√1τ) when the compression error is bounded over the course of training. We
provide specific requirements for convergence with common compression tech-
niques, such as quantization and top-k sparsification. Finally, we experimentally
show compression can reduce communication by over 90% without a significant
decrease in accuracy over VFL without compression.
1	Introduction
Federated Learning (McMahan et al., 2017) is a distributed machine learning approach that has
become of much interest in both theory (Li et al., 2020; Wang et al., 2019; Liu et al., 2020) and
practice (Bonawitz et al., 2019; Rieke et al., 2020; Lim et al., 2020) in recent years. Naive distributed
learning algorithms may require frequent exchanges of large amounts of data, which can lead to
slow training performance (Lin et al., 2020). Further, participants may be globally distributed, with
high latency network connections. To mitigate these factors, Federated Learning algorithms aim
to be communication-efficient by design. Methods such as local updates (Moritz et al., 2016; Liu
et al., 2019), where parties train local parameters for multiple iterations without communication, and
message compression (Stich et al., 2018; Wen et al., 2017; Karimireddy et al., 2019) reduce message
frequency and size, respectively, with little impact on training performance.
Federated Learning methods often target the case where the data among parties is distributed hori-
zontally: each party’s data shares the same features but parties hold data corresponding to different
sample IDs. This is known as Horizontal Federated Learning (HFL) (Yang et al., 2019). However,
there are several application areas where data is partitioned in a vertical manner: the parties store
data on the same sample IDs but different feature spaces.
An example of a vertically partitioned setting includes a hospital, bank, and insurance company
seeking to train a model to predict something of mutual interest, such as customer credit score.
Each of these institutions may have data on the same individuals but store medical history, finan-
cial transactions, and vehicle accident reports, respectively. These features must remain local to the
institutions due to privacy concerns, rules and regulations (e.g., GDPR, HIPAA), and/or communi-
cation network limitations. In such a scenario, Vertical Federated Learning (VFL) methods must be
employed. Although VFL is less well-studied than HFL, there has been a growing interest in VFL
algorithms recently (Hu et al., 2019; Gu et al., 2021; Cha et al., 2021), and VFL algorithms have
important applications including risk prediction, smart manufacturing, and discovery of pharmaceu-
ticals (Kairouz et al., 2021).
Typically in VFL, each party trains a local embedding function that maps raw data features to a
meaningful vector representation, or embedding, for prediction tasks. For example, a neural network
can be an embedding function for mapping the text of an online article to a vector space for classi-
fication (Koehrsen, 2018). Referring to Figure 1a, suppose Party 1 is a hospital with medical data
1
Under review as a conference paper at ICLR 2022
(a) Example of a global model.
Figure 1: Example global model with neural networks and its local view. a) To obtain a y prediction
for a data sample x, each party m feeds the local features of x, xm , into a neural network. The output
of this neural network is the embedding hm (θm ; xm). All embeddings are then fed into the server
model neural network with parameters θ0 . b) When running C-VFL, Party 1 (in green) only has a
compressed snapshot of the other parties embeddings and the server model. To calculate y, Party 1
uses its own embedding calculated at iteration t, and the embeddings and server model calculated at
time t0 , the latest communication iteration, and compressed with Cm .
(b) Local view of a global model.
features x1. The hospital computes its embedding h1(θ1; x1) for the features by feeding x1 through
a neural network. The other parties (the bank and insurance company), compute embeddings for
their features, then all parties share the embeddings in a private manner (e.g., homomorphic encryp-
tion, secure multi-party computation, or secure aggregation). The embeddings are then combined in
a server model θ0 to determine the final loss of the global model. A server model (or fusion network)
captures the complicated interactions of embeddings and is often a complex, non-linear model (Gu
et al., 2019; Nie et al., 2021; Han et al., 2021). Embeddings can be very large, in practice, sometimes
requiring terabytes of communication over the course of training.
Motivated by this, we propose Compressed Vertical Federated Learning (C-VFL), a general frame-
work for communication-efficient Federated Learning over vertically partitioned data. In our algo-
rithm, parties communicate compressed embeddings periodically, and the parties and the server each
run block-coordinate descent for multiple local iterations, in parallel, using stochastic gradients to
update their local parameters.
C-VFL is the first theoretically verified VFL algorithm that applies embedding compression. Unlike
in HFL algorithms, C-VFL compresses embeddings rather than gradients. Previous work has proven
convergence for HFL algorithms with gradient compression (Stich et al., 2018; Wen et al., 2017;
Karimireddy et al., 2019). However, no previous work analyzes the convergence requirements for
VFL algorithms that use embedding compression. Embeddings are parameters in the partial deriva-
tives calculated at each party. The effect of compression error on the resulting partial derivatives may
be complex; therefore, the analysis in previous work on gradient compression in HFL does not apply
to compression in VFL. In our work, we prove that, under a diminishing compression error, C-VFL
converges at a rate of O(√1τ), which is comparable to previous VFL algorithms that do not employ
compression. We also analyze common compressors, such as quantization and sparsification, in
C-VFL and provide bounds on their compression parameters to ensure convergence.
C-VFL also generalizes previous work by supporting an arbitrary server model. Previous work in
VFL has either only analyzed an arbitrary server model without local updates (Chen et al., 2020),
or analyzed local updates with a linear server model (Liu et al., 2019; Zhang et al., 2020; Das &
Patterson, 2021). C-VFL is designed with an arbitrary server model, allowing support for more
complex prediction tasks than those supported by previous VFL algorithms.
We summarize our main contributions in this work.
1.	We introduce C-VFL with an arbitrary compression scheme. Our algorithm generalizes previous
work in VFL by including both an arbitrary server model and multiple local iterations.
2.	We prove convergence of C-VFL to a fixed point on non-convex objectives at a rate of O(击)for
a fixed step size when the compression error is bounded over the course of training. We also prove
that the algorithm convergence error goes to zero for a diminishing step size if the compression error
diminishes as well. Our work provides novel analysis for the effect of compressing embeddings on
2
Under review as a conference paper at ICLR 2022
convergence in a VFL algorithm. Our analysis also applies to Split Learning when uploads to the
server are compressed.
3.	We provide convergence bounds on parameters in common compressors that can be used in C-
VFL. In particular, we examine scalar quantization (Bennett, 1948), lattice vector quantization (Za-
mir & Feder, 1996), and top-k sparsification (Lin et al., 2018).
4.	We evaluate our algorithm by training LSTMs on the MIMIC-III dataset and CNNs on the Mod-
elNet10 dataset. We empirically show how C-VFL can reduce the number of bits sent by over 90%
compared to VFL with no compression without a significant loss in accuracy of the final model.
Related Work. Richtarik & Takac (2016); Hardy et al. (2017) were the first works to propose
Federated Learning algorithms for vertically partitioned data. Chen et al. (2020); Romanini et al.
(2021) propose the inclusion ofan arbitrary server model in a VFL algorithm. However, these works
do not consider multiple local iterations, and thus communicate at every iteration. Liu et al. (2019),
Feng & Yu (2020), and Das & Patterson (2021) all propose different VFL algorithms with local
iterations for vertically partitioned data but do not consider an arbitrary server model. In contrast to
previous works, our work addresses a vertical scenario, an arbitrary server model, local iterations,
and message compression.
Message compression is a common topic in HFL scenarios, where participants exchange gradi-
ents determined by their local datasets. Methods of gradient compression in HFL include scalar
quantization (Bernstein et al., 2018), vector quantization (Shlezinger et al., 2021), and top-k spar-
sification (Shi et al., 2019). In C-VFL, compressed embeddings are shared, rather than compressed
gradients. Analysis in previous work on gradient compression in HFL does not apply to compres-
sion in VFL, as the effect of embedding compression error on each party’s partial derivatives may
be complex. No prior work has analyzed the impact of compression on convergence in VFL.
Outline. In Section 2, we provide the problem formulation and our assumptions. Section 3
presents the details of C-VFL. In Section 4, we present our main theoretical results. Our experi-
mental results are given in Section 5. Finally, we conclude in Section 6.
2	Problem Formulation
We present our problem formulation and notation to be used in the rest of the paper. We let kak be
the 2-norm of a vector a, and let kAkF be the Frobenius norm of a matrix A.
We consider a set of M parties {1, . . . , M} and a server. The dataset X ∈ RN×D is vertically
partitioned a priori across the M parties, where N is the number of data samples and D is the
number of features. The i-th row of X corresponds to a data sample xi . For each sample xi , a party
m holds a disjoint subset of the features, denoted xim, so that xi = [xi1, . . . , xiM]. For each xi, there
is a corresponding label yi. Let y ∈ RN×1 be the vector of all sample labels. We let Xm ∈ RN×Dm
be the local dataset of a party m, where the i-th row correspond to data features xim . We assume
that the server and all parties have a copy of the labels y. For scenarios where the labels are private
and only present at a single party, the label holder can provide enough information for the parties to
compute gradients for some classes of model architectures (Liu et al., 2019).
Each party m holds a set of model parameters θm as well as a local embedding function hm(∙). The
server holds a set of parameters θo called the server model and a loss function l(∙) that combines the
embeddings hm(θm; xim) from all parties. Our objective is as follows:
1N
minimize F(Θ; X; y) := — V"，化，hi@；久：)，…，hM(Θm； XM)； yi)	(1)
ΘN
i=1
where Θ = [θ0T, . . . , θMT ]T is the global model. An example ofa global model Θ is in Figure 1a.
For simplicity, we let m = 0 refer to the server, and define h0(θ0; xi) := θ0 for
all xi, where h°(∙) is equivalent to the identity function. Let hm(θm; Xm) ∈ RPm for
m = 0,...,M, where Pm is the size of the m-th embedding. Let VmF(Θ;X;y):=
N PN=I Vθml(θo, hι(θι; x1),..., hM(θM; XM); yi) be the partial derivatives for parameters θm.
3
Under review as a conference paper at ICLR 2022
Let XB and yB be the set of samples and labels corresponding to a randomly sampled mini-batch
B of size B. We let the stochastic partial derivatives for parameters θm be VmFB(Θ; X; y):=
BB Pxi,yi∈χB,yB Nθm 1(。0,加@； xj),…，^M(θM； XM)； y). We may drop X and y from F(∙) and
Fb(∙). With a minor abuse of notation, We let hm(θm; Xm) := {hm(θm; xB1),..., hm(θm; XBB)}
be the set of all party m embeddings associated with mini-batch B, where Bi is the i-th sample in
the mini-batch B. We let VmFB(Θ) and VmFB(θ0, h1(θ1; X1B), . . ., hM(θM; XBM)) be equivalent,
and use them interchangeably.
Assumption 1. Smoothness: There exists positive constants L < ∞ and Lm < ∞, for
m = 0, . . ., M, such that for all Θ1, Θ2, the objective function satisfies kVF(Θ1) - VF(Θ2)k ≤
L kΘ1 - Θ2k and kVmFB(Θ1) -VmFB(Θ2)k ≤LmkΘ1-Θ2k.
Assumption 2. Unbiased gradients: Form = 0, . . ., M, fora randomly selected mini-batch B, the
stochastic partial derivatives are unbiased, i.e., EBVmFB (Θ) = Vm F (Θ).
Assumption 3. Bounded variance: For m = 0, . . ., M, there exists constants σm < ∞ such that
the variance of the stochastic partial derivatives are bounded as: EB kVmF(Θ) - VmFB (Θ)k2 ≤
σ2
-Bm for a randomly selected mini-batch B ofsize B.
Assumption 1 bounds hoW fast the gradient and stochastic partial derivatives can change. Assump-
tions 2 and 3 require that the stochastic partial derivatives are unbiased estimators of the true partial
derivatives with bounded variance. Assumptions 1-3 are common assumptions in convergence anal-
ysis of gradient-based algorithms (Tsitsiklis et al., 1986; Nguyen et al., 2018; Bottou et al., 2018).
We note Assumptions 2-3 are similar to the IID assumptions in HFL convergence analysis. How-
ever, in VFL settings, all parties store identical sample IDs but different subsets of features. Hence,
there is no equivalent notion of a non-IID distribution in VFL.
Assumption 4. Bounded Hessian: There exists positive constants Hm for m = 0, . . ., M
such that for all Θ, the second partial derivatives of FB with respect to hm (θm; XBm) satisfy:
kV2h (θ ;XB )FB(Θ)kF ≤ Hm for any mini-batch B.
Assumption 5. Bounded Embedding Gradients: There exists positive constants Gm for
m = 0, . . ., M such that for all θm, the stochastic embedding gradients are bounded by:
∣∣Vθm hm(θm; Xm)IIF ≤ Gm for any mini-batch B.
Since we are assuming a Lipschitz-continuous loss function (Assumption 1), we know the Hessian
of F is bounded. Assumption 4 strengthens this assumption slightly to also bound the Hessian
over any mini-batch. Assumption 5 bounds the magnitude of the partial derivatives with respect to
embeddings. This embedding gradient bound is necessary to ensure convergence in the presence of
embedding compression error (see appendix for details).
3 Algorithm
We now present C-VFL, a communication-efficient method for training a global model with verti-
cally partitioned data. In each global round, a mini-batch B is chosen randomly from all samples and
parties share necessary information for local training on this mini-batch. Each party, in parallel, runs
block-coordinate stochastic gradient descent on its local model parameters θm for Q local iterations.
C-VFL runs for a total of R global rounds, and thus runs for T = RQ total local iterations.
For party m to compute the stochastic gradient with respect to its features, it must receive embed-
dings from all parties. We reduce communication cost by only sharing embeddings every global
round. Further, each party compresses their embeddings before sharing. We define a set of general
compressors for compressing party embeddings and the server model: Cm(∙) : RPm → RPm for
m = 0, . . ., M. To calculate the gradient for data sample Xi, party m receives Cj(hj(θj; Xij)) from
all parties j 6= m. With this information, a party m can compute VmFB and update its parameters
θm for multiple local iterations. Note that each party uses a stale view of the global model to com-
pute its gradient during these local iterations, as it is reusing the embeddings it receives at the start of
the round. In Section 4, we show that C-VFL converges even though parties use stale information.
An example view a party has of the global model during training is in Figure 1b. Here, t is the
current iteration and t0 is the start of the most recent global round, when embeddings were shared.
4
Under review as a conference paper at ICLR 2022
Algorithm 1 Compressed Vertical Federated Learning
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
Initialize: θm0 for all parties m and server model θ00
for t . 0,...,T — 1 do
if t mod Q = 0 then
Randomly sample Bt ∈ {X, y}
for m J 1 ,...,M in parallel do
Send Cm(hm (θmt ; XBmt )) to server
end for
Server sends {C0(θ0), C1(h1(θ1t; X1B )), . . . , CM (hM (θMt ; XBM))} to all parties
end if
for m J 0, . . . , M in parallel do
Φ m J {Cθ(θ00 ), Cl(hl(θt0 ； XBt0 )),..., hm(θm ； xm0 ),..., CM (hM (θM ； xM0 ))}
θt+1 J Om - ηt0 VmFB(Φm; yBtO)
end for
end for
Algorithm 1 details the procedure of C-VFL. In each global round, when t mod Q = 0, a mini-
batch B is randomly sampled from X and the parties exchange the associated embeddings, com-
pressed using Cm(∙), via the server (lines 3-9). Each party m completes Q local iterations, using the
compressed embeddings it received in iteration t0 and its own m-th uncompressed embedding set
hm(θmt , XBmt0 ). We denote the set of embeddings that party m uses as:
Φ m := {Co(θ00), Cι(hι(θt0 ； xBt0)),... ,hm (θm ； Xmt0),..., CM (hM (θM ； xM0))}.	(2)
For each local iteration, each party m updates θm by computing the stochastic partial derivatives
Vm,FB(Φm； yBt0) and applying a gradient step with step size ηt0 (lines 13-16).
A key difference here from previous VFL algorithms is that C-VFL shares the server model with
all parties in order to support multiple local gradient updates with a non-linear server model. Also
note that the same mini-batch is used for all Q local iterations, thus communication is only required
every Q iterations. Therefore, without any compression, the total communication cost is O(R ∙ M ∙
(B ∙ Pm + ∣θo∣)) for R global rounds. Our compression technique replaces Pm and 价| with smaller
values based on the compression factor. For cases where embeddings, the batch size, and the server
model are large, this reduction can greatly decrease the communication cost.
Privacy. We now discuss privacy-preserving mechanisms for C-VFL. In HFL settings, model up-
date or gradient information is shared in messages. It has been shown that gradients can leak infor-
mation about the raw data (Phong et al., 2018; Geiping et al., 2020). However in C-VFL, parties only
share embeddings and can only calculate the partial derivatives associated with the server model and
their local models. Commonly proposed HFL gradient attacks cannot be performed on C-VFL. Em-
beddings may be vulnerable to model inversion attacks (Mahendran & Vedaldi, 2015), which are
methods by which an attacker can recover raw input to a model using the embedding output and
black-box access to the model. One can protect against such an attack using homomorphic encryp-
tion (Cheng et al., 2019; Hardy et al., 2017) or secure multi-party computation (Gu et al., 2021).
An efficient implementation of encryption for embeddings in VFL has been provided in the FATE
open-source project (FederatedAI, 2021). Alternatively, if the input to the server model is the sum
of party embeddings, then secure aggregation methods (Bonawitz et al., 2016) can be applied.
Note that C-VFL assumes all parties have access to the labels. For low-risk scenarios, such as
predicting credit score, labels may not need to be private among the parties. In cases where labels
are private, one can augment C-VFL to apply the method in Liu et al. (2019) for gradient calculation
without the need for sharing labels. Our analysis in Section 4 would still hold in this case, and the
additional communication is reduced by the use of message compression.
4 Analysis
In this section, we discuss our analytical approach and present our theoretical results. We first define
the compression error associated with Cm(∙):
5
Under review as a conference paper at ICLR 2022
i
Definition 1. Compression Error: Let vectors xm for m = 0, . . . , M, be the compression errors of
Cm(∙) on a data sample Xi:或:=Cm(hm(θm; Xi)) — hmCm； Xi). Let Cm be the Pm X B matrix
with xmi for all data samples xi in mini-batch Bt0 as the columns. We denote the expected squared
message compression error from party m at round t0 as Emt0 := E kCtm0 k2F.
Let Gt = [(VοFb(Φ 0; yBt0 ))T,..., (VM FB(Φ tM; yBt0 ))T]T .Themodel Θ evolves as:
Θt+1 = Θt — ηt0 Gt.	(3)
We note the reuse of the mini-batch of Bt0 for Q iterations in this recursion. This indicates that the
stochastic gradients are not unbiased during local iterations t0 + 1 ≤ t ≤ t0 +Q — 1. However, using
conditional expectation, we can apply Assumption 2 to the gradient calculated at iteration t0 when
there is no compression error. We define Φtm to be the set of embeddings that would be received by
party m if no compression error were applied:
Φm = {θ00 ,h1(θt0 ； XBt0),..., hm(θm ； Xm0),..., hM (θM ； xM0 )}.	(4)
Then, if we take expectation over Bt0 conditioned on previous global models Θt up to t0 :
EBt0 [VmFB(φm) | {Θτ}T0=o] = VmF(φm).	(5)
With the help of (5), we can prove convergence by bounding the difference between the gradient at
the start of each global round and those calculated during local iterations (see the proof of Lemma 2
in the appendix for details).
To account for compression error, using the chain rule and Taylor series expansion, we obtain:
Lemma 1. Under Assumptions 4-5, the norm of the difference between the objective function value
with compressed and uncompressed embeddings is bounded as:
EkVmFB (Φ m)-VmFB (Φm)k2 ≤ Hm GGmPj=。4"：.	⑹
The proof of Lemma 1 is given in the appendix. Using Lemma 1, we can bound the effect of
compression error on convergence.
We present our main theoretical results. All proofs are provided in the appendix.
Theorem 1. Convergence with fixed step size: Under Assumptions 1-5, if ηt0 = η for all iterations
and satisfies ηt0 ≤ 3 maχ{L1maχ^l^, then the average squared gradient over R global rounds
of Algorithm 1 is bounded by:
R X E MVF (Θt0 )∣∣2i≤ 2 [F(θ0) ；E [F (铲)口 +4ηQL X σBm
t0=0	m=0
68 2 M	R-1	M
+ 68Q X Hm Gm X X Ej.(7)
m=0	t0=0 j=0,j6=m
The first term in (7) is based on the difference between the initial model and final model of the
algorithm. The second term is the error associated with the variance of the stochastic gradients and
the Lipschitz constants L and Lm ’s. The third term relates to the average compression error over
all iterations. The larger the error introduced by a compressor, the larger the convergence error is.
We note that setting Ejt0 = 0 for all parties and iterations provides an error bound on VFL without
compression and is an improvement over the bound in Liu et al. (2019) in terms of Q, M, and B.
The second and third terms include a coefficient relating to local iterations. As the number of local
iterations Q increases, the convergence error increases. However, increasing Q also has the effect of
reducing the number of communication rounds. Thus, it may be beneficial to have Q > 1 in practice.
We explore this more in experiments in Section 5. The second and third terms scale with M, the
number of parties. However, VFL scenarios typically have a small number of parties (Kairouz et al.,
2021), and thus M plays a small role in convergence error. We note that when M = 1 and Q = 1,
Theorem 1 applies to Split Learning (Gupta & Raskar, 2018) when only uploads to the server are
compressed.
6
Under review as a conference paper at ICLR 2022
Table 1: Choice of common compressor parameters to achieve a convergence rate of O (1 / √T). Pm
is the size of the m-th embedding. In scalar quantization, we let there be 2q quantization levels, and
let hmax and hmin be respectively the maximum and minimum components in hm(θmt ; xim) for all
iterations t, parties m, and xim . We let V be the size of the lattice cell in vector quantization. We let
k be the number of parameters sent in an embedding after top-k sparsification, and (khk2)max be
the maximum value of khm(θmt ; xim)k2 for all iterations t, parties m, and xim.
	Uniform Scalar Quantizer	Lattice Quantization	Top-k Sparsification
Parameter choice	q = n(lθg2 (BPm("AmIn)2√T))	V = θ(最百)	k = MPm -BUhkkxvT)
Compression error	Em ≤ BPm(hmax-hmij2-2q	=。(√t)	Em	≤	VBPm	=。(√t)	Em	≤ b(i -	Pm)(khk2)max =。(√t)
Remark 1. Let E = R PRR=0 PM=0 Em. If ηt0 = 方 for all global rounds t0, for Q and B
independent of T, then R PRR=0E [kVF (Θt0 )∣∣2] = O( √T + E). This indicates that if E = O( √T)
then We can achieve a convergence rate of O(√T). Informally, this means that C-VFL can afford
compression error and not worsen asymptotic convergence when this condition is satisfied. We
discuss hoW this affects commonly used compressors in practice later in the section.
We consider a diminishing step size in the folloWing theorem.
Theorem 2. Convergence with diminishing step size: Under Assumptions 1-5, if 0 < ηt0 < 1
satisfies ηt0 ≤ 3 mαχ"mαχ一7, then the minimum squared gradient over R global rounds of
Algorithm 1 is bounded by:
min E
t0 =0,...,R1
VF(Θt0)2
1	1	.PR=0(ηt0)2* PR=0PM=0ηt0Em
IPSnO + PT01 巾 + —Ps^一
If ηt0 and Emt0 satisfy Pt∞0=0 ηt0 = ∞, Pt∞0=0(ηt0)2 < ∞, and Pt∞0=0 PmM=0 ηt0Emt0 < ∞, then
mint0=0,...,R-1E hkVF (Θt0)k2 i → 0 asR → ∞.
According to Theorem 2, the product of the step size and the compression error must be summable
over all iterations. In the next subsection, We discuss hoW to choose common compressor parameters
to ensure this property is satisified. We also see in Section 5 that good results can be achieved
empirically Without diminishing the step size or compression error.
Common Compressors. In this section, We shoW hoW to choose common compressor parameters
to achieve a convergence rate of O(√T) in the context of Theorem 1, and guarantee convergence in
the context of Theorem 2. We analyze three common compressors: a uniform scalar quantizer (Ben-
nett, 1948), a 2-dimensional hexagonal lattice quantizer (Zamir & Feder, 1996), and top-k sparsifi-
cation (Lin et al., 2018). For uniform scalar quantizer, We let there be 2q quantization levels. For the
lattice vector quantizer, We let V be the volume of each lattice cell. For top-k sparsification, We let
k be the number of embedding components sent in a message. In Table 1, We present the choice of
compressor parameters in order to achieve a convergence rate of O(方)in the context of Theorem 1.
We shoW hoW We calculate these bounds in the appendix and provide some implementation details
for their use. We can also use Table 1 to choose compressor parameters to ensure convergence in
the context of Theorem 2. Let nt0 = O(F), where t° is the current round. Then setting T = t0
in Table 1 provides a choice of compression parameters at each iteration to ensure the compression
error diminishes at a rate of O( √=), guaranteeing convergence. Diminishing compression error can
be achieved by increasing the number of quantization levels, decreasing the volume of each lattice
cell, or increasing the number of components sent in messages.
5 Experiments
We present experiments to examine the performance of C-VFL in practice. The goal of our exper-
iments is to examine the effects different compression techniques have on training, and investigate
the accuracy/communication trade-off empirically.
Unless otherwise specified, our experimental setup consists of four parties and a server. Most real-
world VFL settings include collaboration between a few institutions (Kairouz et al., 2021), so we
7
Under review as a conference paper at ICLR 2022
(a) MIMIC by epochs
(b) MIMIC by cost
(c) ModelNet by epochs
(d) ModelNet by cost
No Compression
Scalar Quantize
Vector Quantize
Figure 2: C-VFL when compressing to 2 bits per component. We show test F1-Score on MIMIC-III
dataset and test accuracy on ModelNet10 dataset, plotted by epochs and communication cost (MB).
expect the number of parties to be small. We train our system with two datasets: the MIMIC-
III dataset (Johnson et al., 2016) and the ModelNet10 dataset (Wu et al., 2015). MIMIC-III is an
anonymized hospital patient time series dataset, while ModelNet10 are CAD photos of objects,
each with 12 different views. For MIMIC-III, the task is binary classification to predict in-hospital
mortality. Each party trains on 19 of the 76 features with an LSTM and the server model consists
of two fully-connected layers. For ModelNet10, the task is classification of images into 10 object
classes. Each party trains on three views with three convolutional layers and the server model
consists of a fully-connected layer. We use a fixed step size of 0.01 for the MIMIC-III dataset and
0.001 for the ModelNet10 dataset. For MIMIC-III, we use a batch size of 1000, and for ModelNet10,
we use a batch size of 16. We train on the MIMIC-III dataset for 1000 epochs and the ModelNet10
dataset for 50 epochs, where an epoch consists of all iterations to fully iterate over the dataset. More
details on the datasets and training procedure can be found in the appendix.
We consider the three compressors discussed in Section 4: a uniform scalar quantizer, a 2-
dimensional hexagonal lattice quantizer, and top-k sparsification. For both quantizers, the embed-
ding values need to be bounded. In the case of MIMIC-III’s LSTM, the embedding values are the
output of a tanh activation function and have a bounded range of [-1, 1]. For ModelNet10, the
embeddings are the output of a ReLU activation function, and may be unbounded. We scale em-
bedding values for ModelNet10 to the range [0, 1]. We apply subtractive dithering to both the scalar
quantizer (Wannamaker, 1997) and vector quantizer (Shlezinger et al., 2021).
In our experiments, each embedding component is a 32-bit float. Let b be the bits per component
we compress to. For the scalar quantizer, this means there are 2b quantization levels. For the 2-D
vector quantizer, this means there are 22b vectors in the codebook. The volume V of the vector
quantizer is a function of the number of codebook vectors. For top-k SParSification, k = Pm * as
we are using 32-bit components. We train using C-VFL and consider cases where b = 2, 3, and
4. We compare with a case where b = 32. This corresponds to a standard VFL algorithm without
embedding compression, acting as a baseline for accuracy.
In Figure 2, we plot the test F1-Score and test accuracy for MIMIC-III and ModelNet10, respec-
tively, when training with b = 2. We use F1-Score for MIMIC-III as the in-hospital mortality
prediction task is highly skewed; most people in the dataset did not die in the hospital. The solid
line in each plot represents the average loss over five runs, while the shaded regions represent the
standard deviation. In Figures 2a and 2c, we plot by the number of training epochs. We can see in all
cases, although convergence can be a bit slower, training with compressed embeddings still reaches
similar accuracy to no compression. In Figures 2b and 2d, we plot by the communication cost in
MB. The cost of communication includes both the upload of (compressed) embeddings to the server
and download of embeddings and server model to all parties. We can see that by compressing em-
beddings, we can reach higher accuracy with significantly less communication cost. In both datasets,
the compressors reach similar accuracy to each other, though top-k sparsification performs slightly
worse than the others on MIMIC-III, while scalar quantization performs worse on ModelNet10.
In Table 2, we show the maximum test accuracy reached during training and the communication cost
to reach a target accuracy for both datasets. We show results for all three compressors with b = 2,
3, and 4 bits per component, as well as the baseline of b = 32. For the MIMIC-III dataset, we show
the maximum test F1-score reached and the total communication cost of reaching an F1-Score of
0.4. The maximum F1-score for each case is within a standard deviation of each other. However,
the cost to reach target score is much smaller as the value ofb decreases for all compressors. We can
see that when b = 2, we can achieve over 90% communication cost reduction over no compression
to reach a target F1 -score.
For the ModelNet10 dataset, Table 2 shows the maximum test accuracy reached and the total com-
munication cost of reaching an accuracy of 75%. We can see similar results as for the MIMIC-III
8
Under review as a conference paper at ICLR 2022
Table 2: Maximum F1 -Score and test accuracy reached during training, and communication cost to
reach a target accuracy. For MIMIC-III, the target test F1-Score is 0.4, For ModelNet10, the target
test accuracy is 75%. In these experiments, Q = 10 and M = 4.
Compressor		MIMIC-III dataset		ModelNet10 dataset	
		Max F1 -Score Reached	Cost (MB) Target = 0.4	Max Accuracy Reached	Cost (MB) Target = 75%
None b	32	0.448 ± 0.010	3830.0 ± 558.2	83.81% ± 0.54%	9715.9 ± 2819.3
Scalar b	=2	0.441 ± 0.018	233.1 ± 28.7	79.63% ± 1.74%	374.7 ± 48.3
Vector b	=2	0.451 ± 0.021	236.1 ± 17.9	83.92% ± 0.66%	620.2 ± 194.2
Top-k b	=2	0.431 ± 0.016	309.8 ± 93.6	81.98% ± 0.43%	594.3 ± 259.7
Scalar b	=3	0.446 ± 0.011	343.1 ± 18.8	79.47% ± 1.58%	581.4 ± 106.1
Vector b	=3	0.455 ± 0.020	330.5 ± 10.6	83.85% ± 0.65%	930.2 ± 264.3
Top-k b	=3	0.435 ± 0.030	470.7 ± 116.8	82.64% ± 0.41%	833.3 ± 355.2
Scalar b	=4	0.451 ± 0.020	456.0 ± 87.8	79.41% ± 2.23%	749.4 ± 96.7
Vector b	=4	0.446 ± 0.017	446.5 ± 21.3	83.83% ± 0.62%	1240.3 ± 352.4
Top-k b	=4	0.453 ± 0.014	519.1 ± 150.4	83.17% ± 0.74%	1137.0 ± 450.5
Table 3: MIMIC-III time in seconds to reach
a target F1-Score for different local iterations
Q and communication latency tc with vector
quantization and b = 3.
tc	Time to Reach Target FI-Score 0.45
Q=1	Q=10	Q=25
Figure 3: Communication cost of training on
ModelNet10 with vector quantization.
(b) Parties M = 12
1	694.53 ± 150.75	470.86 ± 235.35	445.21 ± 51.44
10	1262.78 ± 274.10	512.82 ± 256.32	461.17 ± 53.29
50	3788.32 ± 822.30	699.30 ± 349.53	532.12 ± 61.49
200	13259.14 ± 2878.04	1398.60 ± 699.05	798.19 ± 92.23
dataset with the exception of scalar quantization. Scalar quantization achieves the target accuracy
with much lower communication cost, but the maximum test accuracy is significantly lower than the
other cases. This can be due to scalar quantization always quantizing embedding components to the
same values when nearing convergence. Vector quantization benefits from considering components
jointly, and thus can have better reconstruction quality than scalar quantization (Woods, 2006).
In Table 3, we consider the communication/computation tradeoff of local iterations. We show how
the number of local iterations affects the time to reach a target F1 -score in the MIMIC-III dataset.
We train C-VFL with vector quantization b = 3 and set the local iterations Q to 1, 10, and 25. We
simulate a scenario where computation time for training a mini-batch of data at each party takes
10 ms, and communication of embeddings takes a total of 1, 10, 50, and 200 ms roundtrip. These
different communication latencies correspond to the distance between the parties and the server:
within the same cluster, on the same local network, within the same region, and across the globe.
According to Theorem 1, increasing the number of local iterations Q increases convergence error.
However, the target test accuracy is reached within less time when Q increases. The improvement
over Q = 1 local iterations increases as the communication latency increases. In systems where
communication latency is high, it may be beneficial to increase the number of local iterations. The
choice of Q will depend on the accuracy requirements of the given prediction task and the time
constraints on the prediction problem.
Finally, in Figure 3, we plot the test accuracy of ModelNet10 against the communication cost when
using vector quantization with b = 2, 3, 4, and 32. We include plots for 4 and 12 parties. In the 12
party setup, each party stores one view for each CAD model. We note that changing the number of
parties changes the global model structure Θ as well. We can see in both cases that smaller values of
b reach higher test accuracies at lower communication cost. The total communication cost is larger
with 12 parties, but the impact of increasing compression is similar for both M = 4 and M = 12.
6 Conclusion
We proposed C-VFL, a distributed communication-efficient algorithm for training a model over
vertically partitioned data. We proved convergence of the algorithm to a fixed point at a rate of
O(击),and We showed experimentally that communication could be reduced by over 90% without a
significant decrease in accuracy. For future work, we seek to relax our bounded gradient assumption
and explore the effect of adaptive compressors.
9
Under review as a conference paper at ICLR 2022
References
W. R. Bennett. Spectra of quantized signals. Bell System Technical Journal, 1948.
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar.
SIGNSGD: compressed optimisation for non-convex problems. In Proceedings of the 35th In-
ternational Conference on Machine Learning, 2018.
Kallista A. Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. Brendan McMahan,
Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for fed-
erated learning on user-held data. arXiv, 2016.
Kallista A. Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman,
Vladimir Ivanov, Chloe Kiddon, Jakub Konecny, Stefano Mazzocchi, Brendan McMahan, Ti-
mon Van Overveldt, David Petrou, Daniel Ramage, and Jason Roselander. Towards federated
learning at scale: System design. In Proceedings of Machine Learning and Systems, 2019.
Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. Siam Review, 2018.
Dongchul Cha, MinDong Sung, and Yu-Rang Park. Implementing vertical federated learning using
autoencoders: Practical application, generalizability, and utility study. JMIR Medical Informatics,
2021.
Tianyi Chen, Xiao Jin, Yuejiao Sun, and Wotao Yin. VAFL: a method of vertical asynchronous
federated learning. arXiv, 2020.
Kewei Cheng, Tao Fan, Yilun Jin, Yang Liu, Tianjian Chen, and Qiang Yang. Secureboost: A
lossless federated learning framework. arXiv, 2019.
Anirban Das and Stacy Patterson. Multi-tier federated learning for vertically partitioned data. In
IEEE International Conference on Acoustics, Speech and Signal Processing, 2021.
FederatedAI. Fate. https://github.com/FederatedAI/FATE, 2021.
Siwei Feng and Han Yu. Multi-participant multi-class vertical federated learning. arXiv, 2020.
Jonas Geiping, Hartmut Bauermeister, Hannah Droge, and Michael Moeller. Inverting gradients
- how easy is it to break privacy in federated learning? In Advances in Neural Information
Processing Systems, 2020.
Bin Gu, An Xu, Zhouyuan Huo, Cheng Deng, and Heng Huang. Privacy-preserving asynchronous
vertical federated learning algorithms for multiparty collaborative learning. IEEE Transactions
on Neural Networks and Learning Systems, 2021.
Yue Gu, Xinyu Lyu, Weijia Sun, Weitian Li, Shuhong Chen, Xinyu Li, and Ivan Marsic. Mutual
correlation attentive factors in dyadic fusion networks for speech emotion recognition. In Pro-
ceedings of the 27th ACM International Conference on Multimedia, 2019.
Otkrist Gupta and Ramesh Raskar. Distributed learning of deep neural network over multiple agents.
Journal of Network and Computer Applications, 2018.
Wei Han, Hui Chen, and Soujanya Poria. Improving multimodal fusion with hierarchical mutual in-
formation maximization for multimodal sentiment analysis. Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Processing, 2021.
Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Richard Nock, Giorgio Patrini, Guillaume
Smith, and Brian Thorne. Private federated learning on vertically partitioned data via entity
resolution and additively homomorphic encryption. arXiv, 2017.
Yaochen Hu, Di Niu, Jianming Yang, and Shengping Zhou. FDML: A collaborative machine
learning framework for distributed features. In 25th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining, 2019.
10
Under review as a conference paper at ICLR 2022
Alistair E.W. Johnson, Tom J. Pollard, Lu Shen, Li-wei H. Lehman, Mengling Feng, Mohammad
Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G. Mark. Mimic-iii,
a freely accessible critical care database. Nature, 2016.
Peter Kairouz, H. Brendan McMahan, Brendan Avent, AUrelien BelleL Mehdi Bennis, Arjun Nitin
Bhagoji, Kallista A. Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael
G. L. D’Oliveira, Hubert Eichner, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Gar-
rett, Adria Gascon, Badih Ghazi, PhilliP B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang
He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi,
Mikhail Khodak, Jakub Konecny, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo,
Tancrede Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer Ozgur, Rasmus
Pagh, Hang Qi, Daniel Ramage, Ramesh Raskar, Mariana Raykova, Dawn Song, Weikang Song,
Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tramer, Praneeth Vepakomma,
Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances
and open problems in federated learning. Foundations and Trends in Machine Learning, 2021.
Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian U. Stich, and Martin Jaggi. Error feedback
fixes signsgd and other gradient compression schemes. In Proceedings of the 36th International
Conference on Machine Learning, 2019.
Will Koehrsen. Book recommendation system. https://github.com/WillKoehrsen/wikipedia-data-
science/blob/master/notebooks/Book2018.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. In Proceedings of Machine Learning and
Systems, 2020.
Wei Yang Bryan Lim, Nguyen Cong Luong, Dinh Thai Hoang, Yutao Jiao, Ying-Chang Liang,
Qiang Yang, Dusit Niyato, and Chunyan Miao. Federated learning in mobile edge networks: A
comprehensive survey. IEEE Communication Surveys and Tutorials, 2020.
Tao Lin, Sebastian U. Stich, Kumar Kshitij Patel, and Martin Jaggi. Don’t use large mini-batches,
use local SGD. In 8th International Conference on Learning Representations, 2020.
Yujun Lin, Song Han, Huizi Mao, Yu Wang, and Bill Dally. Deep gradient compression: Reduc-
ing the communication bandwidth for distributed training. In 6th International Conference on
Learning Representations, 2018.
Lumin Liu, Jun Zhang, Shenghui Song, and Khaled B. Letaief. Client-edge-cloud hierarchical
federated learning. In IEEE International Conference on Communications, 2020.
Yang Liu, Yan Kang, Xinwei Zhang, Liping Li, Yong Cheng, Tianjian Chen, Mingyi Hong, and
Qiang Yang. A communication efficient vertical federated learning framework. NeurIPS Work-
shop on Federated Learning for Data Privacy and Confidentiality, 2019.
Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting
them. In IEEE Conference on Computer Vision and Pattern Recognition, 2015.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Agui era y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Proceedings of
the 20th International Conference on Artificial Intelligence, 2017.
Philipp Moritz, Robert Nishihara, Ion Stoica, and Michael I. Jordan. Sparknet: Training deep net-
works in spark. 2016.
Lam M. Nguyen, Phuong Ha Nguyen, Marten van Dijk, Peter Richtarik, Katya Scheinberg, and
Martin Takac. SGD and hogwild! convergence without the bounded gradients assumption. In
Proceedings of the 35th International Conference on Machine Learning, 2018.
Weizhi Nie, Qi Liang, Yixin Wang, Xing Wei, and Yuting Su. MMFN: multimodal information
fusion networks for 3d model classification and retrieval. ACM Transactions on Multimedia Com-
puting, Communications, and Applications, 2021.
11
Under review as a conference paper at ICLR 2022
Le Trieu Phong, Yoshinori Aono, Takuya Hayashi, Lihua Wang, and Shiho Moriai. Privacy-
preserving deep learning via additively homomorphic encryption. IEEE Transactions on Infor-
mation Forensics and Security, 2018.
Peter Richtarik and Martin Takac. Parallel coordinate descent methods for big data optimization.
Mathematical Programming, 2016.
Nicola Rieke, Jonny Hancox, Wenqi Li, Fausto Milletarl, Holger R. Roth, Shadi Albarqouni, Spyri-
don Bakas, Mathieu N. Galtier, Bennett A. Landman, Klaus Maier-Hein, Sebastien Ourselin,
Micah Sheller, Ronald M. Summers, Andrew Trask, Daguang Xu, Maximilian Baust, and
M. Jorge Cardoso. Digital Medicine, 2020.
Daniele Romanini, Adam James Hall, Pavlos Papadopoulos, Tom Titcombe, Abbas Ismail, Tudor
Cebere, Robert Sandmann, Robin Roehm, and Michael A. Hoeh. Pyvertical: A vertical feder-
ated learning framework for multi-headed splitnn. ICLR Workshop on Distributed and Private
Machine Learning, 2021.
Shaohuai Shi, Kaiyong Zhao, Qiang Wang, Zhenheng Tang, and Xiaowen Chu. A convergence
analysis of distributed SGD with communication-efficient gradient sparsification. In Proceedings
of the 28th International Joint Conference on Artificial Intelligence, 2019.
Nir Shlezinger, Mingzhe Chen, Yonina C. Eldar, H. Vincent Poor, and Shuguang Cui. Uveqfed:
Universal vector quantization for federated learning. IEEE Transactions on Signal Processing,
2021.
Sebastian U. Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsified SGD with memory. In
Advances in Neural Information Processing Systems, 2018.
John Tsitsiklis, Dimitri Bertsekas, and Michael Athans. Distributed asynchronous deterministic and
stochastic gradient optimization algorithms. IEEE transactions on automatic control, 1986.
Shiqiang Wang, Tiffany Tuor, Theodoros Salonidis, Kin K. Leung, Christian Makaya, Ting He, and
Kevin Chan. Adaptive federated learning in resource constrained edge computing systems. IEEE
Journal on Selected Areas in Communications, 2019.
Robert Alexander Wannamaker. The Theory of Dithered Quantization. PhD thesis, 1997.
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Terngrad:
Ternary gradients to reduce communication in distributed deep learning. In Advances in Neural
Information Processing Systems, 2017.
John W Woods. Multidimensional signal, image, and video processing and coding. Elsevier, 2006.
Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong
Xiao. 3d shapenets: A deep representation for volumetric shapes. In Conference on Computer
Vision and Pattern Recognition, 2015.
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept
and applications. ACM Transactions on Intelligent Systems and Technology, 2019.
Ram Zamir and Meir Feder. On lattice quantization noise. IEEE Transactions on Information
Theory, 1996.
Xinwei Zhang, Wotao Yin, Mingyi Hong, and Tianyi Chen. Hybrid federated learning: Algorithms
and implementation. arXiv, 2020.
12
Under review as a conference paper at ICLR 2022
A Appendix
A.1 Proofs of Theorems 1 and 2
In this section, we provide the proofs for Theorems 1 and 2.
A.1.1 Additional Notation
Before starting the proofs, we define some additional notation to be used throughout. At each
iteration t, each party m trains with the embeddings ΦIm. This is equivalent to the party training
directly with the models θmt and θjt0 for all j 6= m, where t0 is the last communication iteration
when party m received the embeddings. We define:
m=j
otherwise
(A.1)
to represent party m’s view of party j’s model at iteration t. We define the column vector
Γtm = [(γmt ,0)T; . . . ; (γmt ,M)T]T to be party m’s view of the global model at iteration t.
We introduce some notation to help with bounding the error introduced by compression. We define
FB (Γtm) to be the stochastic loss with compression error for a randomly selected mini-batch B
calculated by party m at iteration t:
FBCTm)= FB (θ00 + e00, hι(θt0; xBt0) + e10,... ,hm(θm; Xmt0),...,h,M 阀；x/0 ) + e").
(A.2)
Recall the recursion over the global model Θ:
Θt+1 = θt - ηt0 GI
We can equivalently define GG as follows:
G t = [(V0Fb(Γ0))t ,..., (Vm FB(TM ))T ]T .
(A.3)
(A.4)
Note that the compression error in F(∙) is applied to the embeddings, and not the model parameters.
Thus, F(∙) and F(∙) are different functions. In several parts of the proof, We need to bound the
compression error in VmFB(ΓmJ.
For our analysis, we redefine the set of embeddings for a mini-batch B of size B from party m as a
matrix:
hm (θm ; Xm ) := hm (θm ; xm ), . . . , hm (θm ; xm ) .
(A.5)
hm(θm; XBm) is a matrix with dimensions Pm × B where each column is the embedding from party
m for a single sample in the mini-batch.
Let P = PmM=0 Pm be the sum of the sizes of all embeddings. We redefine the set of embeddings
used by a party m to calculate its gradient without compression error as a matrix:
Φ m = h(θ00 )T, (hι(θt0 ； XBt0 ))T,..., (hm(θtm ； Xm0 ))T,..., ChM (θM; xM0 ))T] T .	(A.6)
Φm is a matrix with dimensions P X B where each column is the concatenation of embeddings for
all parties for a single sample in the mini-batch.
Recall the set of compression error vectors for a mini-batch B of size B from party m is the matrix:
tm0 := hBm1,...,BmB] .	(A.7)
tm0 is a matrix of dimensions Pm × B where each column is the compression error from party m
for a single sample in the mini-batch.
13
Under review as a conference paper at ICLR 2022
We define the compression error on each embedding used in party m’s gradient calculation at itera-
tion t:
Emt0 = h(t00)T,...,(tm0-1)T,0T,(tm0-1)T,...,(tM0)TiT .	(A.8)
Emt0 is a matrix with dimensions P × B where each column is the concatenation of compression
error on embeddings for all parties for a single sample in the mini-batch.
With some abuse of notation, we define:
VmFB&m + Em ) := VmF5(ΓJ.	(A.9)
Note that We can apply the chain rule to VmFB(Γmm):
Vm FBCTm) = Vθm hm(θtm )Vhm(θm ) Fb^ + Em ).	(A.10)
With this expansion, We can noW apply Taylor series expansion to Vhm(θm )FB CΦtm + Emt0 ) around
the point Φtm :
Vhm(θm)FBCΦtm+Emt0)=Vhm(θm)FBCΦtm)+V2hm(θm)FBCΦtm)tEmt0+...	(A.11)
We let the infinite sum of all terms in this Taylor series from the second partial derivatives and up be
denoted as R0m :
Rm(Φm+Em)：= Vhm叱FB&mv Em +...	(A.12)
Note that all compression error is in R0mCΦtm + Emt0). Presented in Section A.1.2, the proof
of Lemma 1’ shoWs hoW We can bound R0m CΦtm + Emt0 ), bounding the compression error in
VmFB(Tm).
Let Et0 = Eb= [ ∙ ∣{Θτ}T0=0]. NotethatbyAssumption2, Et0 [Gt0] = VF(Θt0) as when there is
no compression error in the gradients G, they are equal to the full-batch gradient in expectation When
conditioned on the model parameters up to the iteration t0 . However, this is not true for iterations
t0 + 1 ≤ t ≤ t0 + Q - 1, as we reuse the mini-batch Bt0 in these local iterations. We upper bound
the error introduced by stochastic gradients calculated during local iterations in Lemma 2.
A.1.2 Supporting Lemmas
Next, we provide supporting lemmas and their proofs.
We restate Lemma 1 here:
Lemma 1. Under Assumptions 4-5, the norm of the difference between the objective function value
with and without error is bounded by:
2	M
E∣∣VmFB (φ m) - VmF1 B am4 ≤ Hm Gm X Ejt.	⑶⑶
j=0,j 6=m
To prove Lemma 1, we first prove the following lemma:
Lemma 1’. Under Assumptions 4-5, the squared norm of the partial derivatives for party m’s
embedding multiplied by the Taylor series terms R0m(Φtm + Emt0 ) is bounded by:
∣∣Vθmhm(θmt )R0m(Φtm+Emt0)∣∣2≤Hm2G2m∣∣Emt0∣∣F.	(A.14)
Proof.
∣∣Vθmhm(θmt )R0m(Φtm+Emt0)∣∣2≤ ∣∣Vθmhm(θmt )∣∣2F∣∣R0m(Φtm+Emt0)∣∣2F	(A.15)
≤ Hm2 ∣∣Vθmhm(θmt )∣∣2F ∣∣Emt0∣∣2F	(A.16)
where (A.16) follows from Assumption 4 and the following property of the Taylor series approxi-
mation error:
∣∣R0m(Φtm+Emt0)∣∣F≤Hm∣∣Emt0∣∣F.	(A.17)
14
Under review as a conference paper at ICLR 2022
Applying Assumption 5, we have:
hθm hm(θm)Rm&m+Em )∣ι2 ≤ Hm GmlIEmllF.	⑶⑻
□
We now prove Lemma 1.
Proof. Recall that:
Vm 片(ΓJ = VmF⅛(Φm + Etm)	(A.19)
=Vθm hm(θtm )Vhm(θm ) Fj^ + Em ).	(A.20)
Next we apply Taylor series expansion as in (A.11):
VmFBerm) = Vθm hm(θtm)(Vhm(θm)FB(Φmj+Rm (Φm+Em))	(A.21)
=VmFB(Γtm)+Vθmhm(θmt )R0m(Φtm+Emt0)	(A.22)
Rearranging and applying expectation and the squared 2-norm, we can bound further:
E ∣∣VmFB(rm) - VmFB(rm)∣∣2 = E∣∣Vθm hm(θm)Rm&m+Em )∣∣2	(A.23)
≤ Hm2 G2mE llEmt0 ll2F	(A.24)
= Hm2 G2m X E ∣∣tj0 ∣∣2F	(A.25)
j6=m
= Hm2 G2m X Ejt0	(A.26)
j6=m
where (A.24) follows from Lemma 1’, (A.25) follows from the definition of Emt0, and (A.26) follows
from Definition 1.	□
Lemma 2. If ηt0 ≤4q m&；-L-, then under Assumptions 1-5 we can bound the Conditional ex-
tt
pected squared norm difference of gradients Gt0 and G for iterations t0 to t0 + Q - 1 as follows:
t0 +Q-1
X	Et0
t=t0
2M
G - Gt0∣∣	≤ 16Q3(ηt0)2 X Lm ∣∣VmF(Θt0)∣∣2
m=0
M2
+ i6Q3(ηt0)2 X LmσBm
m=0
M
+ 64Q3 X Hm2 G2m ∣∣Emt0∣∣2F.
m=0
(A.27)
15
Under review as a conference paper at ICLR 2022
Proof.
2M	2
Et0 ∣∣G - Gt0∣∣	= X Et0 ∣∣VmFB(Γm)-VmFB(Γm)∣∣	(A.28)
m=0
M ∣	∣2
X Et0 ∣∣VmFB(Γm)- FBeCI)+ Vm FB(EmI)-VmFB(Vm)]	(A.29)
m=0
VmFBcrm)- VmFBcrm-1)∣∣2
M
≤ (1+n) X Et0
m=0
M2
X Et0 ∣∣VmFB(Γm-1)-VmFB(Γm)J	(A.30)
m=0
M
≤2(1+n)XEt0 ∣∣VmFB(rtm)-VmFB(rtm-1)∣∣2
m=0
M
+2(1+n)XEt0 ∣∣Vθmhm(θmt )R0m(Φtm+Emt0)-Vθmhm(θmt-1)R0m(Φtm-1+Emt-1)∣∣2
m=0
M2
X Et0 ∣∣VmFB(Γm-1)-VmFB(Γm)J	(A.31)
m=0
M
≤2(1+n)XEt0 ∣∣VmFB(rtm)-VmFB(rtm-1)∣∣2
m=0
M
+8(1+n)XHm2G2m∣∣Emt0∣∣2
m=0
M2
X Et0 ∣∣VmFB(Γm-1)-VmFB(Γm)J	(A.32)
m=0
where (A.30) follows from the fact that (X + Y)2 ≤ (1 + n)X2 + (1 + n)Y2 for some positive n
and (A.32) follows from Lemma 1’.
Applying Assumption 1 to the first term in (A.30) we have:
2M
Et0 ∣∣Gt - Gt0∣∣	≤ 2(1 + n) X LmmEt0 [∣∣Γm-Γm-1∣∣[
m=0
M
X Et0 ∣∣VmFB(Γm-1)-VmFB(Γm)
m=0
M
+8(1+n) XHm2G2m∣∣Emt0∣∣2	(A.33)
m=0
M
=2(ηt0)2 (1+ n) X LmmEt0 ∣∣VmFB(Γm-1)
m=0
M
X Et0 ∣∣VmFB(Γm-1)-VmFB(Γm)
m=0
M
+8(1+n) XHm2G2m∣∣Emt0∣∣2	(A.34)
m=0
+ 2(1 + 1
n
+ 2(1 + 1
n
where (A.34) follows from the update rule Γmm = Γm-1 一 ηt0 VmFB(Γm-1).
16
Under review as a conference paper at ICLR 2022
Bounding further:
Et0 1 11G t—Gt01∣2
M
≤ 2(ηt0)2 (1+ n) X LmEt0 J] VmFBCT-1) - VmFB(Γm) + VmFB(Γm)
2
m=0
M
+ 1 + - X Et0 ∣∣VmFB(Γm-1) -VmFB (时)
m=0
M
+8(1+n)XHm2G2m ∣∣Emt0 ∣∣2
m=0
M
2
(A.35)
≤ 4(ηt0)2 (1+ n) X LmmEt0 ]] VmFBCTm-1) - VmFB(Γm)
2
m=0
M
+4(ηt0)2(1+n)XL2mEt0 ∣∣VmFB(Γtm0)∣∣2
m=0
+(1 + 1
n
X Et0 ∣∣VmFB(Γm-1) -VmFB (Γm)
M
2
m=0
M
+8(1+n)XHm2G2m ∣∣Emt0 ∣∣2
m=0
X(4(nt0 )2 (1+ n) Lm +(1 + n)) Et0
VmFB(Γm-1)- VmFB(Γm)^
(A.36)
M
+ 4(ηt0 )2 (1 + n)X L2mEt0 ]]VmFB(Γtm0 )]]2
m=0
M
+8(1+n) X Hm2 G2m ]]Emt0 ]]2.
m=0
(A.37)
Let n = Q. We simplify (A.37) further:
Et0 ∣∣Gt - Gt01∣]
≤ X (4(ηt0)2 (1 + Q) Lm + (1 + Q)) Et0
M
VmFB(Γm-1)-VmFB(Γm )112
+4(ηt0)2(1+Q)XL2mEt0 ]]VmFB(Γtm0)]]2
m=0
M
+8(1+Q)XHm2G2m ]]Emt0 ]]2F.
m=0
(A.38)
17
Under review as a conference paper at ICLR 2022
Let ηt0 ≤ 4Cm.1-L. We bound (A.38) as follows:
4Q maxm Lm
X Et0 ||VmFB(rm71)-VmFB(rm)
m=0
CWM1+Q
M
M
2
+ 4(ηt0)2 (1 + Q) X LmEt0 [∣∣VmFB(rm)|2]
m=0
(A.39)
M
+8(1+Q) XHm2G2m∣∣Emt0∣∣2F
0
VmFB(Ttm-1) - VmFB(Ttm )f
+4(ηt0)2(1+Q)XL2mEt0 ∣∣VmFB(Γtm0)∣∣2
m=0
M
+8(1+Q)XHm2G2m∣∣Emt0∣∣2F
(A.40)
≤ (1+Q2
Q
m=0
M
X Et0 ||VmFBCTT)-VmFB(Γm)
m=0
M
+ 4(ηt0)2 (1 + Q) X LmEt0 [∣∣VmFB(rm)∣∣2]
m=0
M
+8(1+Q) XHm2G2m∣∣Emt0∣∣2F.
m=0
(A.41)
We define the following notation for simplicity:
M
At :
X Et0 ∣VmFB(Γtm) -VmFB(Γm0)
m=0
M
Bo := 4(ηt0)2 (1 + Q) X LmEt0 [∣∣VmFB(rm)∣∣2]
m=0
M
B1 := 8(1 + Q) X Hm2 G2m ∣∣Emt0 ∣∣2F
m=0
C=(1+q ).
(A.42)
(A.43)
(A.44)
(A.45)
2
2
Note that we have shown that At ≤ CAt-1 + B0 + B1. Therefore:
At0+1 ≤ CAt0 + (B0 +B1)
At0+2 ≤ C2At0 +C(B0 +B1) + (B0 +B1)
At0+3 ≤C3At0+C2(B0+B1)+C(B0+B1)+(B0+B1)
(A.46)
(A.47)
(A.48)
(A.49)
t-t0 -2
At ≤ Ct-t0-1At0 +(B0+B1) X Ck
k=0
C t-t0 -1	1
=C t-t0-1At0 + (Bo + Bl) C	I—
C-1
(A.50)
(A.51)
18
Under review as a conference paper at ICLR 2022
We bound the first term in (A.51) by applying Lemma 1:
M
At0
X EtOllVmFB(%)-vmFB(rm)
2
m=0	L	」
M
≤ X Hm Gm∣∣Em∣∣F.
m=0
Summing over the set of local iterations to,...,t0+, where t+ := to + Q - 1:
t+	t+
):Ct-t0-1 At0 = At0): ct-t0-1
t=t0	t=t0
C CQ - 1
=At0-------
C -
At0	1+
1
W )q -1
QAt0
1 + 2)-1
(1+1 )q -1
e2
≤ QAt0-
≤ 4QAt0
M
2
-1
2Γ~
≤ 4Q x Hm Gm∣∣Em∣∣F.
m=o
It is left to bound the second term in (A.51) over the set of local iterations to,...,to + Q
10 _	「t-t0-1 _ i	J0 _	「t-t0-1 _ i
X (Bo + B1)C	≤ X (Bo + B1)C
C-1	C-1
t=t0	t=t0
(⅞+B2 (X C "Y
t=t0
(Bo + B1) (CQ - 1
C-1
C-1
-Q
(Bo + B1)
Q(Bo + Bi)
2
「- Q
[(1+Q)-1 .
'Q [(1+Q r-1
-------------
2
∖
Q2 (Bo + Bi)
2
≤ Q2 (BO + BI)
≤	2
≤ 2Q2(Bo + B1)
∖
-1
L-1)
-Q
)
(A.52)
(A.53)
(A.54)
(A.55)
(A.56)
(A.57)
(A.58)
(A.59)
(A.60)
1.
(A.61)
(A.62)
(A.63)
(A.64)
(A.65)
(A.66)
(A.67)
(A.68)
(A.69)
19
Under review as a conference paper at ICLR 2022
Plugging the values for B0 and B1 :
t0+	Ct-t0 -1	1	M
E(BO + Bl)	C - 1	≤ 8Q2(ηt0)2 (1 + Q) E LmEt0 [∣∣VmFB(Γm)『]
t=t0	m=0
M
+ 16Q2(1 + Q) XHm2G2mEmt02F
m=0
(A.70)
Applying Assumption 3 and adding in the first term in (A.51):
t0+	M
XAt ≤8Q2(ηt0)2(1+Q) XL2mVmF(Θt0)2
t=t0	m=0
M2
+ 8Q2 (ηt0 )2 (1 + Q) X Lm-B
m=0
M
+ 4(4Q2(1 + Q) + Q) X Hm2 G2m Emt0 2F	(A.71)
m=0
M
≤ 16Q3(ηt0)2 X L2m VmF(Θt0)2
m=0
M2
+ i6Q3(ηt0)2 χ Lm-m
m=0
M
+ 64Q3 X Hm2 G2m Emt0 2F .	(A.72)
m=0
□
A.1.3 Proof of Theorems 1 and 2
Let t0+ := t0 + Q - 1. By Assumption 1:
F (Θt0+) - F(Θt0) ≤ VF (Θt0), Θt0+ -
t0+
=- VF(Θt0),X
t=t0
t0+
≤ - ηt0 VF (Θt
t=t0
θto E+2	Θ	t0+ - Θt02		(A.73)
ηt0 G t，+	L 2	t0+ X ηt0 G t	2	(A.74)
		t=t0		
0), G tE +	LQ ^2^	t0+ X(ηt0)2	∣G 12	(A.75)
t=t0
where (A.75) follows from fact that (PnN=1 xn)2 ≤ N PnN=1 x2n.
20
Under review as a conference paper at ICLR 2022
We bound further:
t0+	t0+
F(Θt+) — F(Θt0) ≤ — X nt0 VFF(Θt0), Gt — GtO)-	— Xnt0 vF (Θt0), Gt0
t=t0	t=t0
t0+ + LQ X(nt0)2 ∣∣G — Gt0 + Gt01 t=t0	2	(A.76)
t0+	t0+
≤ — X nt0 VFF(Θt0),Gt - Gt0)- t=t0	— Xnt0 vF (Θt0), Gt0 t=t0
t0+		
+LQX(nt0)2	∣∣G t — Gt0	∣2	t0+ ∣∣ +LQX(nt0)2∣∣Gt0∣∣2	(A.77)
t=t0		t=t0
t0+		t0+
nt0 —vF (Θt0), Gt0 —		-Gt)- X nt0〈VF(Θt0), Gt0)
t=t0		t=t0
t0+		
+LQX(nt0)2	∣∣G t — Gt0	∣2	t0+ ∣∣ +LQX(nt0)2∣∣Gt0∣∣2.	(A.78)
t=t0	t=t0
t0+
≤ X X ηt0 ∣∣VF(Θt0)『
t=t0
t0+	t0+
+ 2 X ηt0 ∣∣Gt - Gt0 Il - X ηt0 EF(Θt0), Gt0)
t=t0	t=t0
t0+	2	t0+
+ LQ X(ηt0)2∣∣Gt - Gt0∣∣ + LQ X(ηt0)2∣∣Gt0∣∣2	(a.79)
t=t0	t=t0
where (A.79) follows from the fact that A ∙ B = 2A2 + 1B2 — 1 (A — B)2.
We apply the expectation Et0 to both sides of (A.79):
X t0+	X t0+
Et0 [f(Θt+ )J — F(θt0) ≤ — - EntO ∣∣vf(θt0)∣∣2 + - EntO(i + LQnt)Et0
t=t0	t=t0
Gt - Gt01∣2
t0+
+ LQ X (nt0 )2Et0 ∣∣Gt0 ∣∣2	(A.80)
t=t0
- t0+
≤-x EntO(i - LQnt0)∣∣vf(Θt0)∣∣2
t=t0
t0+	2	M 2 t0+
+ 2 X nt0(i + LQnt0)Et0 ∣∣Gt - Gt01∣	+ LQ X σBm X(nt0)2
t=t0	m=0	t=t0
(A.81)
—(Q nt0 (i - LQnt0 )∣∣vf (Θt0 )∣∣2
i t0+	∣
+ 2 EntO(i + LQnt0)Et0 ∣∣Gt — Gt,
t=t0
2	M2
0∣∣	+LQ2(nt)2 X Bm
m=0
(A.82)
21
Under review as a conference paper at ICLR 2022
where (A.80) follows from applying Assumption 2 and noting that Et0 [Gt0] = VF(Θt0), and
(A.82) follows from Assumption 3.
Applying Lemma 2 to (A.82):
Et0 [F (Θt+)i - F (Θt0) ≤ - Qηt0 (1 - LQηt0) ∣∣VF (Θt0 )∣∣2
M
+8Q3(ηt0)3(1+LQηt0)XL2m∣∣VmF(Θtm0)∣∣2
m=0
M2
+ 8Q3(ηt0)3(1 + LQηt0) X Lm°m
m=0
M
+32Q3ηt0(1+LQηt0)XHm2G2m∣∣Emt0∣∣2F
m=0
M2
+ LQ2(ηt0 )2 X -Bm	(A.83)
m=0
QM
≤ -Q2 X ηt0(1 - LQηt0 - 16Q2Lm(ηt0)2 - 16Q3LmL(ηt0)3)) ∣∣VmF(Θt0)∣∣2
m=0
M2
+ (LQ2(ηt0)2 + 8Q3Lm(ηt0)3 + 8Q4LLm(ηt0)4) X -f
m=0
M
+32Q3ηt0(1+LQηt0)XHm2G2m∣∣Emt0∣∣2F.	(A.84)
m=0
Let ηt0 ≤ 16Q maχ{L,maχm Lm} .Then We bound (A.84) further:
+	QM	1	1	1
Et0 [F(θt0)] - F(θt0) ≤ -Q X ηt0(1 -16 -16 -许))∣∣VmF(θt0)∣∣2
m=0
M2
+ (LQ2(ηt0)2 + 8Q3Lm(ηt0)3 + 8Q4LLm(ηt0)4) X -Bm
m=0
M
+16Q3ηt0(1+LQηt0) XHm2G2m∣∣Emt0∣∣2F	(A.85)
m=0
≤-Qηt0 ∣∣VF(Θt0)∣∣2
M2
+ (LQ2(ηt0)2 + 8Q3Lm(ηt0)3 + 8Q4LLm(ηt0)4) X -m
m=0
M
+32Q3ηt0(1+LQηt0) XHm2G2m∣∣Emt0∣∣2F	(A.86)
m=0
22
Under review as a conference paper at ICLR 2022
After some rearranging of terms:
ηt0∣"(Θto)『≤ 2 B)-EtOhF(θt+ )ii
Q
M2
+ 2(LQ(ηt0)2 + 8Q2Lm(ηt0)3 + 8Q3LLm(ηt0)4) X -f
m=0
M
+64Q2ηt0(1+LQηt0) XHm2G2mEmt02F	(A.87)
m=0
Summing over all communication rounds t0 = 0, . . . , R - 1 and taking total expectation:
R-1	2	2 F(Θ0) -E F(ΘT)
E ηt0E IlVF(Θt0)∣∣	≤ L-QL
t0=0	Q
R-1	M	2
+ 2 X (LQ(ηt0)2 + 8Q2Lm(ηt0)3 + 8Q3LLm(ηt0)4) X σm
t0=0	m=0
M
+64Q2ηt0(1+LQηt0) XHm2G2mIIEmt0II2F	(A.88)
m=0
≤ 2 W(Θ0)- E [F(ΘT川
≤	QR
R-1	M	2
+ 2 X (QL(ηt0)2 + 8Q2Lm(ηt0)3 + 8Q3LLm(ηt0)4) X σm
t0=0	m=0
R-1	M
+ 64Q2 Xηt0(1+LQηt0) X Hm2 G2mE IIEmt0II2F .	(A.89)
t0=0	m=0
Note that:
MM
X Hm2 G2mE	IIEmt0II2F	= XHm2G2mXE IItj0II2F	(A.90)
m=0	m=0	j 6=m
M
= X Hm2 G2m X Ejt0	(A.91)
m=0	j 6=m
where (A.91) follows from Definition 1.
Plugging this into (A.89)
R-1
X ηt0E IIVF(Θt0)II2 ≤
t0=0
2 [F(Θ0) - E [F(ΘT)]]
QR
R-1	M	2
+ 2 X (QL(ηt0)2 + 8Q2Lm(ηt0)3 + 8Q3LLm(ηt0)4) X σm
t0=0	m=0
R-1	M
+ 64Q2 X ηt0(1 + LQηt0) X Hm2 G2m X Ejt0.
t0=0	m=0	j6=m
(A.92)
23
Under review as a conference paper at ICLR 2022
Suppose that ηt0 = η for all communication rounds t0 . Then, averaging over R communication
rounds, we have:
R-1
R X E [WF(θt0 升2i ≤
t0 =0
2 F(Θ0) - E F(ΘT)
QRη	m=0
64 2 R-1	M
64Q X (1 + LQn) X HmGm X Ej.
M2
+ 2 X(QLn + 8Q2Lmn2 + 8Q3LLmn3)-m
(A.93)
t0 =0	m=0	j 6=m
≤ 2 [F(Θ0)- E [F(ΘT)]]
QRη
M2
+ 4 X QLnim
m=0
+
68 2 R-1 M
+ 68Q XX Hm Gm X j
t0 =0 m=0	j6=m
(A.94)
where (A.94) follows from our assumption that nt0 ≤	---771----1~t . This completes the
16Q max{L,maxm Lm }
proof of Theorem 1.
We continue our analysis to prove Theorem 2. Starting from (A.92), we bound the left-hand side
with the minimum over all iterations:
t0=minR-1 E MVF©° A ≤
2 F(Θ0) - Et0 F(ΘT)
Q PR=0 nt0
PR-1 ( t0)2	PR-1 ( t0)3	PR-1 ( t0)4	M 2
+ 2	QL∙t0=0(n ) + 8Q2Lm^t0=0(n ) + 8Q3LLmU0=0(n ) X ⅛
PR=0 nt0	PR=0 nt°	PR-I nt°	m=0 B
M	PR-1 t0P	Et0	M	PR-1 ( t0)2P	Et0
+ 64Q2 X Hm GmJPR-jmj + 64LQ3 X Hm Gm 二三PR-Ijj
m=0	t0=0 n	m=0	t0=0 n
(A.95)
As R → ∞, if PtR0-=10nt0 = ∞, PtR0-=10(nt0)2 < ∞, and PtR0-=10 nt0 Pj6=mEjt0 < ∞, then
mint0=0,...,R-1 E kVF (Θt0)k2 → 0. This completes the proof of Theorem 2.
A.2 Common Compressors
In this section, we calculate the compression error and parameter bounds for uniform scalar quanti-
zation, lattice vector quantization and top-k sparsification, as well as discuss implementation details
of these compressors in C-VFL.
We first consider a uniform scalar quantizer (Bennett, 1948) with a set of 2q quantization levels,
where q is the number of bits to represent compressed values. We define the range of values
that quantize to the same quantization level as the quantization bin. In C-VFL, a scalar quan-
tizer quantizes each individual component of embeddings. The error in each embedding of a batch
B in scalar quantization is ≤ Pm∆2 = Pm (hmax-hmij 2-2q where ∆ the size of a quantiza-
tion bin, Pm is the size of the m-th embedding, hmax and hmin are respectively the maximum
and minimum value hm (θmt ; xim) can be for all iterations t, parties m, and xim. We note that if
hmax or hmin are unbounded, then the error is unbounded as well. By Theorem 1, we know that
R PR-O PM=o Em = O(√t) to obtain a convergence rate of O(√1τ). If We use the same q for
all parties and iterations, We can solve for q to find that the value q must be lower bounded by
q = Ω(log2(Pm(hmaχ 一 hmin)2√T)) to reach a convergence rate of O(方). For a diminishing
compression error, required by Theorem 2, we letT = to in this bound, indicating that q, the number
of quantization bins, must increase as training continues.
A vector quantizer creates a set of d-dimensional vectors called a codebook (Zamir & Feder, 1996).
A vector is quantized by dividing the components into sub-vectors of size d, then quantizing each
sub-vector to the nearest codebook vector in Euclidean distance. A cell in vector quantization is
24
Under review as a conference paper at ICLR 2022
defined as all points in d-space that quantizes to a single codeword. The volume of these cells are de-
termined by how closely packed codewords are. We consider the commonly applied 2-dimensional
hexagonal lattice quantizer (Shlezinger et al., 2021). In C-VFL, each embedding is divided into
sub-vectors of size two, scaled to the unit square, then quantized to the nearest vector by Euclidean
distance in the codebook. The error in this vector quantizer is ≤ VPm where V is the volume of a
lattice cell. The more bits available for quantization, the smaller the volume of the cells, the smaller
the compression error. We can calculate an upper bound on V based on Theorem 1: V = O( 1√~).
Pm T
If a diminishing compression error is required, we can set T = t0 in this bound, indicating that V
must decrease at a rate of O (P 1√^). As the number of iterations increases, the smaller V must be,
and thus the more bits that must be communicated.
In top-k sparsification (Lin et al., 2018), when used in distributed SGD algorithms, the k largest
magnitude components of the gradient are sent while the rest are set to zero. In the case of embed-
dings in C-VFL, a large element may be as important as an input to the server model as a small
one. We can instead select the k embedding elements to send with the largest magnitude partial
derivatives in Vθmhm(θitl). Since a party m cannot calculate Vθmhm(θtm) until all parties send
their embeddings, party m can use the embedding gradient calculated in the previous iteration,
Vθmhm(θmt-1). This is an intuitive method, as we assume our gradients are Lipschitz continuous,
and thus do not change too rapidly. The error of sparsification is ≤ (1 一 ɪ)(khk2)maχ where
Pm
(khk2)max is the maximum value of khm(θmt ; xim)k2 for all iterations t, parties m, and xim. Note
that if (khk2)max is unbounded, then the error is unbounded. We can calculate a lower bound on
k: k = Ω(Pm 一	Pm-√≡). Note that the larger (Ilhk2)maχ, the larger k must be. More com-
(khk2)m
ax T
ponents must be sent if embedding magnitude is large in order to achieve a convergence rate of
O(力). When considering a diminishing compression error, We set T = t0, showing that k must
increase over the course of training.
A.3 Experimental Details
For our experiments, we used an internal cluster of 40 compute nodes running CentOS 7 each with
2× 20-core 2.5 GHz Intel Xeon Gold 6248 CPUs, 8× NVIDIA Tesla V100 GPUs with 32 GB
HBM, and 768 GB of RAM.
A.3.1 MIMIC-III
The MIMIC-III dataset can be found at: mimic.physionet.org. The dataset consists of time-series
data from 〜60,000 intensive care unit admissions. The data includes many features about each
patient, such as demographic, vital signs, medications, and more. All the data is anonymized. In
order to gain access to the dataset, one must take the short online course provided on their website.
Our code for training with the MIMIC-III dataset can be found in in the folder titled “mimic3”.
This is an extension of the MIMIC-III benchmarks repo found at: github.com/YerevaNN/mimic3-
benchmarks. The original code preprocesses the MIMIC-III dataset and provides starter code for
training LSTMs using centralized SGD. Our code has updated their existing code to TensorFlow 2.
The new file of interest in our code base is “mimic3models/in_hospital_mortality/quant.py” which
runs C-VFL. Both our code base and the original are under the MIT License. More details on
installation, dependencies, and running our experiments can be found in “README.md”. Each
experiment took approximately six hours to run on a node in our cluster.
The benchmarking preprocessing code splits the data up into different prediction cases. Our exper-
iments train models to predict for in-hospital mortality. For in-hospital mortality, there are 14,681
training samples, and 3,236 test samples. In our experiments, we use a step size of 0.01, as is
standard for training an LSTM on the MIMIC-III dataset.
A.3.2 MODELNET10
Details on the ModelNet10 dataset can be found at: modelnet.cs.princeton.edu/. The
specific link we downloaded the dataset from is the following Google Drive link:
https://drive.google.com/file/d/0B4v2jR3WsindMUE3N2xiLVpyLW8/view. The dataset consists of
3D CAD models of different common objects in the world. For each CAD model, there are 12 views
25
Under review as a conference paper at ICLR 2022
from different angles saved as PNG files. We only trained our models on the following 10 classes:
bathtub, bed, chair, desk, dresser, monitor, night_stand, sofa, table, toilet. We used a subset of the
data with 1,008 training samples and 918 test samples. In our experiments, we use a step size of
0.001, as is standard for training a CNN on the ModelNet10 dataset.
Our code for learning on the ModelNet10 dataset is in the folder “MVCNN-Pytorch” and is an
extension of the MVCNN-PyTorch repo: github.com/RBirkeland/MVCNN-PyTorch. The file of
interest in our code base is “quant.py” which runs C-VFL. Both our code base and the original are
under the MIT License. Details on how to run our experiments can be found in the “README.md”.
Each experiment took approximately six hours to run on a node in our cluster.
A.4 Additional Experiments
In this section we provide some additional experiments to test the scalability of C-VFL for a larger
number of parties and larger datasets.
First, we run C-VFL using the same parameters as described in Section 5, now with 48 parties and a
server with the ModelNet10 dataset. Each 3D CAD model in the ModelNet10 dataset has 12 views,
so we assign every four parties the same view, and have each store a different quadrant of the image.
60	-	60	6	60
No Compression
Scalar Quantize
Vector Quantize
Top-k
10	20	30
Epochs
No Compression
Scalar Quantize
Vector Quantize
Top-k
Communication Cost (GB)
Communication Cost (GB)
(a)	Plotted by epochs
(b)	Plotted by cost
(c)	Vector quantization
Figure A.1: Test accuracy on ModelNet10 dataset with the number of parties M = 48. In the first
two plots, the compressors have b = 2, where b is the number of bits used to represent embedding
components. In the third plot, b = 32 indicates there is no compression. The results show little
variation between compressors and no compression, leading to a large benefit in communication
cost versus test accuracy.
In Figure A.1, we plot the test accuracy for the ModelNet10 dataset. The test accuracy is overall
lower than when running with 4 and 12 parties. This is expected, as each party has less information
individually, making the prediction task more difficult. Figure A.1a shows the test accuracy plotted
by epochs. There is very little variation between compressors and no compression here. This leads
to a very large benefit for compression when plotting by communication cost, seen in Figure A.1b.
In Figure A.1c, we plot the test accuracy of C-VFL using vector quantization for different values of
b, the number of bits to represent compressed values. Similar to previous results, lower b tends to
improve test accuracy reached with the same amount of communication cost. We can also see that
the total cost of communication has increased compared to the case of 4 and 12 parties in Figure 3.
This is expected, as there are more embeddings being exchanged in each global round.
We also run C-VFL on CIFAR-10, a large dataset of 60,000 images with 10 classes of objects
and animals. To simulate a VFL scenario with the CIFAR-10 dataset, we split the images into 4
quadrants and run C-VFL with 4 parties and a server. Each party trains ResNet-18 locally, and the
server model is a single fully-connected layer.
26
Under review as a conference paper at ICLR 2022
(a) Plotted by epochs
(b) Plotted by cost
(c) Vector quantization
Figure A.2: Test accuracy on CIFAR-10 dataset with the number of parties M = 4. In the first
two plots, the compressors have b = 2, where b is the number of bits used to represent embedding
components. In the third plot, b = 32 indicates there is no compression. The results show vector
quantization performs the best our of the compressors, and all compressors show improvement over
no compression in terms of communication cost to reach target test accuracies.
In Figure A.2, we plot the test accuracy for the CIFAR-10 dataset. The test accuracy is fairly
low compared to typical baseline accuracies, which is expected, as learning object classification
from only a quadrant of a 32 × 32 pixel image is difficult. Figure A.2a shows the test accuracy
plotted by epochs. We can see that vector quantization performs almost as well as no compression
in the CIFAR-10 dataset. When plotting by communication cost, seen in Figure A.2b, we can
see that vector quantization performs the best, though scalar quantization and top-k sparsification
show communication savings as well. In Figure A.2c, we plot the test accuracy of C-VFL using
vector quantization for different values of b, the number of bits to represent compressed values.
Similar to previous results, lower b tends to improve test accuracy reached with the same amount of
communication cost.
A.5 Additional Plots
In this section, we include additional plots using the results from the experiments introduced in
Section 5 of the main paper. The setup for the experiments is described in the main paper. These
plots provide some additional insight into the effect of each compressor on convergence in both
datasets. As with the plots in the main paper, the solid lines in each plot are the average of five runs
and the shaded regions represent the standard deviation.
0.6
0.5
S
自0∙4
0.3
0.2
(a) 2 bits per parameter
Epochs
---No Compression
—Scalar Quantize
一 Vector Quantize
---lbp-k
0	250	500	750	1000
(b) 3 bits per parameter	(c) 4 bits per parameter
Figure A.3: Training loss on MIMIC-III dataset. One can see that, with the exception of top-k
sparsification, allowing more bits for compression moves the training loss closer to the baseline of
no compression. Top-k appears to be more unstable in the MIMIC-III dataset.
27
Under review as a conference paper at ICLR 2022
(a) 2 bits per parameter
Epochs
(b) 3 bits per parameter
(c) 4 bits per parameter
Figure A.4: Test F1-Score on MIMIC-III dataset. Scalar and vector quantization achieve similar
test F1-score even when only using 2 bits in quantization. On the other hand, top-k sparsification
performs worse than the other compressors in the MIMIC-III dataset.
Figures A.3 and A.4 plot the training loss and test F1-Score for training on the MIMIC-III dataset
for different levels of compression. We can see that scalar and vector quantization perform similarly
to no compression and improve as the number of bits available increase. We can also see that top-k
sparsification has high variability on the MIMIC-III dataset and generally performs worse than the
other compressors.
(a) 2 bits per parameter
Communication Cost (MB)
(b) 3 bits per parameter
(c) 4 bits per parameter
Figure A.5: Test F1-Score on MIMIC-III dataset plotted by communication cost. We can see that
all compressors reach higher F1-scores with lower communication cost than no compression. We
can see that the standard deviation for each compressor decreases as the number of bits available
increases. Top-k sparsification generally performs worse than the other compressors on the MIMIC-
III-dataset.
Figure A.6: Test F1-Score on MIMIC-III dataset plotted by communication cost. We can see that
all compressors reach higher F1-scores with lower communication cost than no compression. We
can see that the standard deviation for each compressor decreases as the number of bits available
increases. Top-k sparsification generally performs worse than the other compressors on the MIMIC-
III-dataset.
Figures A.5 and A.6 plot the test F1-Score for training on the MIMIC-III dataset plotted against the
communication cost. The plots in Figure A.5 include all compression techniques for a given level of
28
Under review as a conference paper at ICLR 2022
compression, while the plots in Figure A.6 include all levels of compression for a given compression
technique. We can see that all compressors reach higher F1-scores with lower communication cost
than no compression. Itis interesting to note that increasing the number of bits per parameter reduces
the variability in all compressors.
Figure A.7: Training loss on ModelNet10 dataset. Vector quantization and top-k sparsification per-
form similarly to no compression, even when only 2 bits are available. Scalar quantization converges
to a higher loss and has high variability on the ModelNet10 dataset.
Figure A.8: Test accuracy on ModelNet10 dataset. Vector quantization and top-k sparsification per-
form similarly to no compression, even when only 2 bits are available. Scalar quantization converges
to a lower test accuracy, and has high variability on the ModelNet10 dataset.
Figures A.7 and A.8 plot the training loss and test accuracy for training on the ModelNet10 dataset.
Vector quantization and top-k sparsification perform similarly to no compression in both training
loss and test accuracy, even when only 2 bits are available. We can see that scalar quantization has
high variability on the ModelNet10 dataset.
(a) 2 bits per parameter
(b) 3 bits per parameter
(c) 4 bits per parameter
Figure A.9: Test accuracy on ModelNet10 dataset plotted by communication cost. We can see that
all compressors reach higher accuracies with lower communication cost than no compression. Scalar
quantization generally performs worse than the other compressors on the ModelNet10 dataset.
29
Under review as a conference paper at ICLR 2022
Figure A.10: Test accuracy on ModelNet10 dataset plotted by communication cost. We can see that
all compressors reach higher accuracies with lower communication cost than no compression. We
can see that when less bits are used in each compressor, higher test accuracies are reached at lower
communication costs. Scalar quantization generally performs worse than the other compressors on
the ModelNet10 dataset.
Figures A.9 and A.10 plot the test accuracy for training on the ModelNet10 dataset against the com-
munication cost. The plots in Figure A.9 include all compression techniques for a given level of
compression, while the plots in Figure A.10 include all levels of compression for a given compres-
sion technique. We can see that all compressors reach higher accuracies with lower communication
cost than no compression. Scalar quantization generally performs worse than the other compressors
on the ModelNet10 dataset. From Figure A.10, we also see that when fewer bits are used in each
compressor, higher test accuracies are reached at lower communication costs.
30