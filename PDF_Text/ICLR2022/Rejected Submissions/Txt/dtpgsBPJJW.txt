Under review as a conference paper at ICLR 2022
Riemannian Manifold Embeddings for
Straight-Through Estimator
Anonymous authors
Paper under double-blind review
Ab stract
Quantized Neural Networks (QNNs) aim at replacing full-precision weights W
with quantized weights W, which make it possible to deploy large models to
mobile and miniaturized devices easily. However, either infinite or zero gradi-
ents caused by non-differentiable quantization significantly affect the training of
quantized models. In order to address this problem, most training-based quanti-
zation methods use Straight-Through Estimator (STE) to approximate gradients
Vw w.r.t. W with gradients Vv^ w.r.t. W where the premise is that W must
be clipped to [-1, +1]. However, the simple application of STE brings with the
gradient mismatch problem, which affects the stability of the training process.
In this paper, we propose to revise an approximated gradient for penetrating the
quantization function with manifold learning. Specifically, by viewing the param-
eter space as a metric tensor in the Riemannian manifold, we introduce the Mani-
fold Quantization (ManiQuant) via revised STE to alleviate the gradient mismatch
problem. The ablation studies and experimental results demonstrate that our pro-
posed method has a better and more stable performance with various deep neural
networks on CIFAR10/100 and ImageNet datasets.
1	Introduction
Neural networks can handle many complex tasks due to their large number of trainable parameters
and strong nonlinear capabilities (Krizhevsky et al., 2012). However, the massive amount of mod-
els and calculations hinder the application of neural networks on mobile and miniaturized devices,
which naturally comes with constraints on computing power and resources. Neural network quan-
tization is considered an efficient solution in the inference that alleviates the number of parameters
and optimizes the computation by reducing the bit width of weights or activations (Courbariaux
et al., 2016; Li et al., 2016; Zhu et al., 2016).
Existing neural network quantization methods can be roughly divided into two categories: “STE”
and “Non-STE” methods. Most of the quantization methods adopted by the QNNs belong to the
former, i.e. there is always a non-differentiable quantization function during training. The role of
STE is to penetrate this non-differentiable quantization function and pass the gradients in backpro-
pogation (Hinton, 2012), e.g. DeepShift (Elhoushi et al., 2019), INT8 (Zhu et al., 2020), AQE (Chen
et al., 2020), etc. “Non-STE” methods refer to maintain feasible quantization during training, which
does not need to apply STE directly to all full-precision weights. For example, Zhou et al. (Zhou
et al., 2017) divided the weights into two groups until all parameters are quantized, where the first
group is directly quantized and fixed; the second group needs to be retrained to make up for the
decrease of accuracy caused by quantization of the first group. Louizos et al. (Louizos et al.,
2019) introduced a differentiable quantizer that can transform the continuous distributions of weights
and activations to categorical distributions with gradient-based optimization. However, “Non-STE”
methods face the setting and influence of heavy hyper-parameters in the training process.
Relatively, “STE” methods are wider choices for quantized models from simplicity and versatil-
ity. To approximate VW w.r.t. W and complete stable quantization training, Courbariaux et al.
(Courbariaux et al., 2016) binarized the neural networks by the approximated gradients using STE:
v7 v7 T h	∂W	1 1 if |W| ≤ 1
VW = VW ◦ I, Where ∂W = I =( 0 otherwise .
1
Under review as a conference paper at ICLR 2022
Figure 1: The general gradient is intuitively defined in Euclidean space, which means that the di-
rection of the general gradient will not be affected by the curvature. On the contrary, the natural
gradient and weak curvature gradient intrinsically contain the curvature of manifolds, although the
definitions of the manifolds constructing these two gradients are different.
However, it will inevitably bring the gradient mismatch problem raised by just simply application of
CmL rr∖	ι ∙ ι 11	ri	/ ι /rF	/ -∣ CCd /、	ι	r∙	ɪɪ τ / ɪɪ τ
STE. To overcome this challenge, Zhou et al. (Zhou et al., 2016) proposed to transform W to W:
W =	tanh(W)
max(∣ tanh(W)|)，
and then quantize it using a quantization function Q(∙). During backpropogation, the gradients VW
w.r.t. W can be further computed using the chain rule:
vW=VQ(W) nix(tah(w )∣).
Based on this work, Chen et al. (Chen et al., 2019) proposed to learn VW by fully-connected
neural networks or LSTM, which replaces this gradients via a meta quantizer Mφ parameterized by
φ across layers:
_ 〜
_	(一 〜〜c ∂W
vW = Mφ (VQ(W),Q(W))肃.
However, such methods not only add many additional parameters, but also increase the difficulty of
the training of quantized models.
In this paper, we introduce the Manifold Quantization (ManiQuant) to train a quantized model via
embedding Riemannian manifolds for STE. In Figure 1, we treat the parameter space in a quantized
model as a Riemannian manifold, which will alleviate the gradient mismatch problem caused by
non-differentiable quantization and assist quantized models in achieving a more stable convergence
and better performance. The main contributions of this work are three-fold: First, we propose to
use Fisher Information Matrix embedding to alleviate the gradient mismatch problem. Second, we
define a novel Hyperbolic divergence by a convex function with geometric structure. With the con-
straint of Hyperbolic divergence, we introduce a weak curvature manifold that forms the background
of ManiQuant. Third, based on the Second, we propose to use weak curvature metric embeddings
for STE as the weak curvature gradient to approximate VW without extra parameters and training
complexity.
2	Related Work
2.1	General Gradient
For a neural network with l layers, the full-precision weight matrix of each layer is just
marked as Wi. By defining the operation vec(∙) that vectorizes matrices by stacking their
columns together, the total parameter vector of the neural network can be denoted as θ
vec (W1)> , vec (W2)> , . . . , vec (Wl)>	. The parameter vector is given as a column vector.
2
Under review as a conference paper at ICLR 2022
Let θ ∈ Rn be a parameter space on which a loss function L associated with weight is well-defined.
It is relatively easy to express the general gradient in training:
Vθ L = dL,	(1)
∂θ
which is the steepest descent method in Euclidean space with an orthonormal coordinate. Note that
the negative gradient represents the direction of the steepest descent.
When the Euclidean space is considered, the Euclidean divergence between two sufficiently closed
points θ and θ0 is actually defined by default:
DE [θ ： θ0] = 2 XColi)2,	(2)
i
which is a half of the square of the Euclidean distance identified by the Euclidean metric δij , so that
ds2 = 2DE[θ : θ + dθ] = X(dθi)2 = Xδijdθidθj.	(3)
2.2	Natural Gradient
The general gradient only considers the parameter update along the gradient direction that does not
involve the metric tensor for the parameter space of the problem. Now, we attach a Riemannian
metric Gij (θ) for the parameter space to form a Riemannian manifold (M, Gij) (Amari, 1998;
2016). In that case, the steepest descent direction also depends on the quadratic form introduced by
a small incremental vector dθ that connects θ and θ + dθ, whose form is given by
ds2 = XGij(θ)dθidθj,	(4)
i,j
where dθi is the component of dθ. Under the constraint ds2, the steepest descent direction is toward
the optimization goal of L(θ + dθ). Intuitively, it measures the Kullback-Leibler (KL) divergence
between two distributions p(x∣θ) and p(x∣θ + dθ) of network model, which is equivalent to the
interval of two adjacent points on a Riemannian manifold. The KL divergence under the Riemannian
metric is well approximated through the Fisher Information Matrix (FIM) with the second-order
Taylor expansion of the KL divergence (see Appendix B.3) (Ba et al., 2016):
ds2 = 2Dkl[p(x∣Θ) : p(x∣θ + dθ)] ≈ —dθ>Ep(χ∣θ)[Vlogp(x∣θ)Vlogp(x∣θ)>]dθ.	(5)
We see that FIM is equal to the negative expected Hessian of log likelihood. Furthermore, Amari
deduced that the Riemannian metric is given by the FIM (Amari, 1998). By the Lagrangian form,
we have the natural gradient (see Appendix C.1)
VθL = F-1(θ)dL, whereF(θ) = Ep(χ∣θ)[Vlogp(x∣θ)Vlogp(x∣θ)>],	⑹
∂θ
which is the steepest descent method in a Riemannian space and F(θ) is the FIM with a parameter
vector (the natural gradient also known as the Riemannian gradient). Empirically, the immediate
application of FIM is as drop-in replacement of Hessian in second order optimization algorithm.
Note that VθL will return to VθL when Gij(θ) is equal to the Euclidean metric δij, i.e. the identity
matrix I .
3	Manifold Quantization
Considering a quantized model of each layer, we notate the quantized weight matrix using WZi
for differentiating full-precision weight matrix Wi . Note that we define the quantized parameter
vector as θ, which is similar to the full-precision parameter vector θ. During training a QNN, the
quantization function Q(∙) is a one-to-one mapping from full-precision values to quantized values,
which can be expressed as θ = Q(θ).
3
Under review as a conference paper at ICLR 2022
By involving the practice to train a neural network with low bit-width, the process of the quantization
needs to be further designed that plays a vital role in the final performance, as for the other parts,
e.g. via mapping from an input pattern a- to output ^i can still be imitated in the same way as
full-precision neural networks:
Si = Wi ^i-1,
^i = Q(fi ◦ Si),
(7)
where fi is non-linear function acted on element-wise. To distinguish the full-precision model, we
mark the notation “ ^ ” to represent the operation through a quantization function in a quantized
model, whether for weight matrix Wi or activation vector ^i.
3.1	Fisher Information Matrix Embedding for STE
The concept of the natural gradient is closely related to the FIM and KL divergence (see Ap-
pendix B.3). Since the KL divergence is intrinsic, the natural gradient is also intrinsically invariant
under the parameter transformation. By viewing the FIM as an l by l block matrix where l denotes
the number of layers in a neural network, the natural gradient formula can be introduced to alleviate
the gradient mismatch problem when training a QNN and updating its parameters.
Lemma 1 In the Riemannian manifold defined by KL divergence, Fisher Information Matrix em-
bedding for Straight-Through Estimator is written by:
qθL S=E F-1(θ)VθL ◦ I∣θ∣≤1
'I1 / I A∖ I1 / I A∖
1 dlogp(x∣θ) dlogp(x∣θ)
=E --------Z---------Z---
dθ	dθ
v^l ◦ 1∣θ∣≤1
(8)
E-1
v^l ◦ 1∣θ∣≤1.
Furthermore, F-1(θ) can be expressed as
(9)
where vec
can be represented by the gradient error ∂∂l.
Proof. The proofs are the combination between STE and Appendix C.1.
Considering that the gradient propagation needs to span over the quantized neurons and graphs, we
still need to use STE to update the gradient of QNNs in the learning procedure:
vec (∂Wi) = a3 氧(JdL ◦ fi'(Si))S=Ea3 氧 II ◦f0(Si)I小匕1),	(IO)
where 0 denotes the Kronecker product1. Relying on the KL divergence, We indicate the parameter
space as the Riemannian manifold rather than the Euclidean space in the quantization procedure.
For neural networks with a scale of one million or more parameters, the time complexity of inverting
the FIM, a component of natural gradients, is O(n3) (Povey et al., 2014). Previously, there were
some works to calculate the natural gradient efficiently, e.g. Roux et al. (Roux et al., 2008) de-
composed the FIM into multiple diagonal blocks, where each diagonal block is approximated by a
low-rank matrix. Bastian et al. (Bastian et al., 2011) also used the idea of diagonal blocks by con-
structing a diagonal block that corresponds to a weight matrix. Martens et al. (Martens & Grosse,
1Since the size of a>-ι and the size of ∂L may be different in each layer, Kronecker product is necessary.
4
Under review as a conference paper at ICLR 2022
2015) proposed to approximate the FIM through the Kronecker product of two smaller matrices to
improve computational efficiency. Even so, FIM embedding for STE with complex decomposition
methods is still not suitable for large-scale tasks, and the computational amount is large compared
to the standard STE.
3.2	Weak Curvature Metric Embedding for STE
3.2.1	Introduction of Weak Curvature Manifold
We can use the Euclidean coordinates to calculate the natural gradient on the manifold because of
the local homeomorphism of a manifold (Wald, 2010). Note that the homeomorphic mapping φU
satisfies U ∈ M 7→ φU (U) ∈ Rn. For any point x ∈ U, we can define φU (x) as the Euclidean
coordinate absolutely.
The FIM embedding for STE poses a significant challenge for computing. Intuitively, the com-
puting F-1 (θ) attached to the natural gradient will take on massive computation, which can only
be numerically estimated. The inversion is mostly unrealistic when deep neural networks are very
redundant, with tens of thousands of neural connections.
Inspired by information geometry (Amari, 2016) and mirror descent (Bubeck, 2015), we propose a
weak curvature manifold with the weak curvature metric, which implies that the gradient mismatch
problem can be alleviated by embedding the weak curvature manifold into the STE, while keeping
a low computational complexity. Geometrically, the Riemannian manifold is nearly flat, where
the Riemannian metric is an approximation of the Euclidean metric. In practice, we develop a
linearized Riemannian metric from the Euclidean metric δij , which is systematically defined in
general relativity (Wald, 2010).
Definition 1 (Weak Curvature Metric) Let a linearized Riemannian metric deviates from the Eu-
clidean metric. The deviated metric ij (θ) is much smaller than 1 in global Euclidean coordinate
system of δij,
Gij(θ) = δij + Gj(θ), where ∣eij∣《1.	(11)
It is an adequate definition of “weakness” in this context and ensures Gij (θ) to be a positive-definite
metric.
3.2.2	Hyperbolic Divergence
In practice, we can determine a unique geodesic through exponential map expθi (τθ) that maps τθ
back to M where τ is a small constant, by defining τθ ∈ TθiM as the tangent vector. Note that the
definition of exponential map is developed by (Wald, 2010; Helgason, 2001).
Definition 2 (Exponential Map) Let M be a Riemannian manifold, for the tangent vector v ∈ TxM
in a point x ∈ M where TxM is the tangent space, there is a unique geodesic γv (t) locally that
satisfies γv (0) = x and γv0 (0) = v. The exponential map expx : TxM 7→ M corresponding to
γv (t) is defined as expx(v) = γv (1). When constraining expx to a neighbourhood U, this mapping
is one-to-one.
Empirically, the exponential map expθi (τθ) is to map a tangent vector τθ in the tangent bundle to
the point where the arc length from point θi is equal to ∣τθ∣ on geodesic with the initial condition
(θi, τθ). In order to make exp(τ θ) and exp(-τ θ) have the same effect in the training process, we
symmetrize them and derive a convex function
ψ(θ) = X T2 log Fxp(Tθi)+2exp(-τθi)] = X T2 log(cosh(τθi)).
ii
(12)
Geometrically, the convex function with geometric structure is introduced into the Bregman di-
vergence (see Appendix B.2) (Bregman, 1967), and we obtain a novel Hyperbolic divergence that
satisfies the criteria of divergence (see Appendix B.1).
5
Under review as a conference paper at ICLR 2022
Definition 3 (Hyperbolic Divergence) For a convex function ψ defined by Eq. 12, the Hyperbolic
divergence between θ0 and θ is
DH[θ0 : θ] = X [3 log cosh，，) - 1(θ0 - θi) tanh(τθi)
τ2	cosh(τ θi )	τ i
(13)
3.2.3	The Gradient Flow in Weak Curvature Manifold
Lemma 2 Let dθ → 0, the square of an infinitesimal distance defined by Hyperbolic divergence is
ds2 = 2DH [θ0 : θ] ≈ dθ> δij - tanh(τ θ) tanh(τθ)> dθ.	(14)
Furthermore, the defined metric δij - tanh(τ θ) tanh(τ θ)> can be consistent with the weak cur-
vature metric.
Proof. The proofs can be found in Appendix B.4. Empirically, we can give a small constant τ to
satisfy the weak curvature metric based on Definition 1. Comparing Eq. 4 and Eq. 11, we can deduce
the deviated metric ij = - tanh(τ θ) tanh(τ θ)> ij.
Lemma 3 In the Riemannian manifold defined by Hyperbolic divergence, a weak curvature metric
embedding for Straight-Through Estimator is approximated by:
勺θL S≈E [I + tanh(τθ) tanh(τθ)>] Vl^L ◦ I∣θ∣≤i	(15)
Proof. The proofs can be found in Appendix C.2.
3.3	Hierarchical Embeddings of the Weak Curvature Metric
When embedding a weak curvature manifold, there is an unnatural gap between the calculation of
weak curvature metric and the layer-by-layer gradient update in back-propagation. Specifically, the
metric is not a block diagonal matrix (each block corresponds to a layer of the neural network).
For the process of the quantization, we have the hierarchical embeddings for STE through decou-
pling operation:
Vvec(Wi)L 4 s≈e [I + τ2 ∙ vec(tanh(Wi))vec(tanh(Wi))>] Vvec(Wi)L ◦ I∣vec(w∕≤ι
I +	vec(tanh(Wi) vec(tanh(Wi))>、V L ι
Xtr [vec(tanh(Wi)) vec(tanh(Wi))>]) Vec(Wi) o Ivec(Wi)l≤1
'1 + tanh2 (WiI)
1 + X	tr[Pi]	X
tanh(wi2) tanh(wii)
X	trw
tanh(wi1) tanh(wi2)
t^Pi
1 I 、/tanh2(wi2)
1 + X	tr[H]
Vvec(Wi)L ◦ 1I vec(Wi)∣≤1.
(16)
In the first line of the above formula, we can put τi2 outside because the elements of the deviated
metric is very small based on Lemma 2. In the second line, we use a normalized formula to uniformly
represent τi2: τi2 = X/ tr vec(tanh(Wi)) vec(tanh(Wi))> where X is a constant factor and the
matrix vec(tanh(Wi)) vec(tanh(Wi))> is re-expressed by Pi. By limiting the value range of
full-precision weights to [-1, +1], we obtain the independent weak curvature metric embedding in
each layer. The algorithm of this weak curvature gradient with STE is illustrated in Appendix E.
4 Experiment
In this section, we implement experiments to demonstrate the effectiveness of our proposed methods
on benchmark datasets that are CIFAR10/100 and ImageNet mainly here. Intuitively, experiments
on CIFAR10/100 and ImageNet are ablation studies to validate the advantages of weak curvature
manifold embeddings. Comparisons with other training-based quantization on quantitative indicates
will be carried out on CIFAR10 and ImageNet. All experiments are conducted with PyTorch.
6
Under review as a conference paper at ICLR 2022
Table 1: The experimental results on CIFAR10 with ResNet20/32/44.
Network	Forward	Backward	Test Acc (%)	Original Acc(%)
ResNet20	{-1,+ 1}	Dorefa MUltiFCG WCG	88.28±0.81 88.94±0.46 89.78±0.33	91.50
ResNet32	{-1,+ 1}	Dorefa MultiFCG WCG	90.23±0.63 89.63±0.38 90.52±0.27	92.13
ResNet44	{-1,+ 1}	Dorefa MultiFCG WCG	90.71±0.58 90.54±0.21 91.38±0.11	93.56
4.1	Experiment Setup
We use a weight decay of 1e-4, a batch size of 128, and SGD optimization method with nesterov
momentum of 0.9. For CIFAR, we set total training epochs as 200 where the learning strategy is
lowered by 10 times at epoch 80, 150, and 190, with the initial 0.1. For ImageNet, we set total
training epochs as 50 where the learning strategy is lowered by 10 times at epoch 15, 30, and 45,
with the initial 0.1. Note that we set χ as the reciprocal of output channel in a layer. All experiments
are conducted for 5 times, and the statistics of the last 10/5 epochs’ test accuracy are reported as a
fair comparison.
Table 2: The experimental results on CIFAR100 with ResNet56/110.
Network	Forward	Backward	Test Acc (%)	OriginalAcc(%)
ResNet56	{-1,+1}	Dorefa MultiFCG FCGrad WCG	66.71±2.32 66.58±0.37 66.56±0.35 68.85±0.41	71.22
ResNet110	{-1,+1}	Dorefa MultiFCG FCGrad WCG	68.15±0.50 68.27±0.14 68.74±0.36 69.10±0.26	72.54
4.2	Ablation Studies
In order to illustrate the superiority of the weak curvature gradient (WCG), we train QNNs with our
gradient from scratch compared to other gradients, i.e. Dorefa (Zhou et al., 2016), MultiFCG (Chen
et al., 2019) and FCGrad (Chen et al., 2019). We choose 1-bit weight quantization for all layers
in the networks, which means that each layer contains just -1 or +1. Note that the initialization
is Xavier (Glorot & Bengio, 2010). Specifically, we experiment ResNet20/32/44 models (He et al.,
2016) on CIFAR10, ResNet56/110 models (He et al., 2016) on CIFAR100 and ResNet18 model (He
et al., 2016) on ImageNet. The accuracy of full-precision baseline is reported by (Chen et al., 2019).
The results of ablation studies on CIFAR10, CIFAR100 and ImageNet are shown in Table 1, Ta-
ble 2 and Table 3 respectively. Various quantized models applying weak curvature gradients show
significant improvement in challenging classification tasks. For fairness, we use the same forward
quantization process and optimizations in experiments where the only difference is the gradients of
the backward process.
7
Under review as a conference paper at ICLR 2022
Table 3: The experimental results on ImageNet with ResNet18.
Network	Forward	Backward	Test Top1∕Top5 (%)	Original Top1/Top5 (%)
ReSNet18	{-1,+1}	Dorefa MultiFCG FCGrad WCG	58.34±2.07/81.47±1.56 59.47±0.02∕82.41±0.01 59.83±0.36/82.67±0.23 61.02±0.33/83.74±0.18	69.76/89.08
(a) ResNet56
Figure 2: Training and test curves of ResNet56/110 on CIFAR100 compared between Dorefa and
WCG.
(b) ResNet110
4.3	Convergence and Stability Analysis
In this experiment, we compare the convergence and stability of Dorefa and WCG during the training
process. As Figure 2 shows, the quantized model trained with WCG has better performance and
more stable convergence than the quantized model trained with Dorefa, both for training and testing.
Here, we state that better performance means larger mean, and more stable convergence means
smaller variance. As for why WCG performs better, we conjecture that WCG has increased the
Fisher information of the original quantized model compared to other methods, although WCG does
not use additional neural network information for learning like MultiFCG or FCGrad. Moreover,
this is where the advantage of manifold learning lies.
4.4	Comparisons with Other Training-based Quantization
In this experiment, our ManiQuant is initialized with the trained full-precision model, and continues
to use WCG to learn the quantized model. ManiQuant focuses on the weak curvature manifold
embeddings for STE, without any extra training tricks and the deformation of the quantization. We
list the comparison results of 1-bit weight and activation quantization in Table 4. More experimental
results can be found in Appendix D.
4.5	Training Time Analysis
Our hardware environment is conducted with an Intel(R) Xeon(R) Silver 4214 CPU(2.20GHz),
GeForce GTX 1080Ti GPU. We test the training time per iteration as for MetaQuant using Mul-
tiFCG, ManiQuant using WCG, and Dorefa with ResNet20 in CIFAR10. For finishing one iteration
of training, ManiQuant costs 18.44s and MetaQuant costs 25.34s while Dorefa uses 17.81s.
8
Under review as a conference paper at ICLR 2022
Table 4: The experimental results on CIFAR10 with ResNet18 and VGG16.
Network	WlA		Method		Test Acc (%)	Original Acc(%)
ResNet18	1/1	RBNN (Lin etal.,2020) MD (Ajanthan et al., 2021) PGD (Ajanthan et al., 2019) ManiQuant (WCG)	92.20 91.28 92.60 92.83	94.84
VGG16	1/1	RBNN (Lin et al.,2020) MD (Ajanthan et al., 2021) PGD (Ajanthan et al., 2019) ManiQuant (WCG)	91.30 90.47 88.48 91.77	93.33
5 Conclusion
Empirically, training-based quantization methods tend to use Straight-Through Estimator to pene-
trate the non-differentiable quantization. However, we throw out the gradient mismatch problem in
training quantized neural networks. In this paper, we introduce manifold learning to assist Straight-
Through Estimator penetrating the non-differentiable quantization, which can be considered to al-
leviate the gradient mismatch problem. Specifically, we embed the Riemannian manifold into the
Straight-Through Estimator to increase Fisher information. On the one hand, we try to embed Fisher
Information Matrix for Straight-Through Estimator as the natural gradient defined by KL divergence,
but encounter a complicated calculation dilemma. On the other hand, we propose a weak curvature
manifold defined by Hyperbolic divergence to jump out the complicated calculation dilemma and
embed weak curvature metric for Straight-Through Estimator as the weak curvature gradient.
References
Thalaiyasingam Ajanthan, Puneet K Dokania, Richard Hartley, and Philip HS Torr. Proximal mean-
field for neural network quantization. In Proceedings of the IEEE/CVF International Conference
on Computer Vision, pp. 4871-4880, 2019.
Thalaiyasingam Ajanthan, Kartik Gupta, Philip Torr, Richad Hartley, and Puneet Dokania. Mirror
descent view for neural network quantization. In International Conference on Artificial Intelli-
gence and Statistics, pp. 2809-2817. PMLR, 2021.
Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251-
276, 1998.
Shun-ichi Amari. Information geometry and its applications, volume 194. Springer, 2016.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Yu Bai, Yu-Xiang Wang, and Edo Liberty. Proxquant: Quantized neural networks via proximal
operators. arXiv preprint arXiv:1810.00861, 2018.
Michael R Bastian, Jacob H Gunther, and Todd K Moon. A simplified natural gradient learning
algorithm. Advances in Artificial Neural Systems, 2011, 2011.
Lev M Bregman. The relaxation method of finding the common point of convex sets and its applica-
tion to the solution of problems in convex programming. USSR computational mathematics and
mathematical physics, 7(3):200-217, 1967.
Sebastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends® in
Machine Learning, 8(3-4):231-357, 2015.
J. Chen, Y. Liu, H. Zhang, S. Hou, and J. Yang. Propagating asymptotic-estimated gradients for low
bitwidth quantized neural networks. IEEE Journal of Selected Topics in Signal Processing, 14(4):
848-859, 2020.
9
Under review as a conference paper at ICLR 2022
Shangyu Chen, Wenya Wang, and Sinno Jialin Pan. Metaquant: Learning to quantize by learning to
penetrate non-differentiable quantization. In Advances in Neural Information Processing Systems,
volume 32, pp. 3916-3926. Curran Associates, Inc., 2019.
Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized
neural networks: Training deep neural networks with weights and activations constrained to+ 1
or-1. arXiv preprint arXiv:1602.02830, 2016.
Harald Cramer. Mathematical Methods of Statistics (PMS-9), Volume 9. Princeton university press,
2016.
Mostafa Elhoushi, Zihao Chen, Farhan Shafiq, Ye Henry Tian, and Joey Yiwei Li. Deepshift: To-
wards multiplication-less neural networks. arXiv preprint arXiv:1905.13298, 2019.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256, 2010.
Philipp Gysel, Jon Pimentel, Mohammad Motamedi, and Soheil Ghiasi. Ristretto: A framework for
empirical study of resource-efficient inference in convolutional neural networks. IEEE transac-
tions on neural networks and learning systems, 29(11):5784-5789, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Sigurdur Helgason. Differential geometry and symmetric spaces, volume 341. American Mathe-
matical Soc., 2001.
G Hinton. Neural networks for machine learning. coursera,[video lectures], 2012.
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for
mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Cong Leng, Zesheng Dou, Hao Li, Shenghuo Zhu, and Rong Jin. Extremely low bit neural net-
work: Squeeze the last bit out with admm. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 32, 2018.
Fengfu Li, Bo Zhang, and Bin Liu. Ternary weight networks. arXiv preprint arXiv:1605.04711,
2016.
Mingbao Lin, Rongrong Ji, Zihan Xu, Baochang Zhang, Yan Wang, Yongjian Wu, Feiyue Huang,
and Chia-Wen Lin. Rotated binary neural network. Advances in Neural Information Processing
Systems, 33, 2020.
Christos Louizos, Matthias Reisser, Tijmen Blankevoort, Efstratios Gavves, and Max Welling. Re-
laxed quantization for discretized neural networks. In International Conference on Learning
Representations, 2019. URL https://openreview.net/forum?id=HkxjYoCqKX.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In International conference on machine learning, pp. 2408-2417, 2015.
Daniel Povey, Xiaohui Zhang, and Sanjeev Khudanpur. Parallel training of dnns with natural gradi-
ent and parameter averaging. arXiv preprint arXiv:1410.7455, 2014.
C Radhakrishna Rao. Information and the accuracy attainable in the estimation of statistical param-
eters. In Breakthroughs in statistics, pp. 235-247. Springer, 1992.
Nicolas L Roux, Pierre-Antoine Manzagol, and Yoshua Bengio. Topmoumoute online natural gra-
dient algorithm. In Advances in neural information processing systems, pp. 849-856, 2008.
10
Under review as a conference paper at ICLR 2022
Oran Shayer, Dan Levi, and Ethan Fetaya. Learning discrete weights using the local repa-
rameterization trick. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=BySRH6CpW.
Robert M Wald. General relativity. University of Chicago press, 2010.
Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. Lq-nets: Learned quantization for
highly accurate and compact deep neural networks. In Proceedings of the European conference
on computer vision (EcCv), pp. 365-382, 2018.
Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental network quantiza-
tion: Towards lossless cnns with low-precision weights. arXiv preprint arXiv:1702.03044, 2017.
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Train-
ing low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint
arXiv:1606.06160, 2016.
Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. Trained ternary quantization. arXiv
preprint arXiv:1612.01064, 2016.
Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang, Zhelong Li, Xiuqi Yang, and
Junjie Yan. Towards unified int8 training for convolutional neural network. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1969-1979, 2020.
11
Under review as a conference paper at ICLR 2022
A Straight-Through Estimator in 1 -bit Quantization
The key of the proof is to divide the loss function into two parts based on conditional probabilities
P(a > |a) and 1 - P(a > |a):
∂-l∖ = ɪ Ee[L]
∂a ∂a
∂
—[L(^ = 1)P(a > e|a) + L(a = -1)(1 — P (a > e|a))]
(17)
∂
—P (a > e∣a)[L(a = 1) — L(a = —1)],
where we divide the conditional probability P(a > |a) into two parts |a| ≤ 1 and |a| > 1 based on
the distribution of the noise source:
∂P(a > eIa) = ∂α (P(O > e|叫α∣>ι + P(a > e|叫a∣≤ι)
=∂ U-I 2 "「/a 2 O -	()
Further, ignoring high-order derivative of a single neuron (^ = ±1) that typically has only a small
impact on the total loss function, the loss function can be approximated by Taylor expansion w.r.t.
^ = 0
∂L
L(a = +I) = L(a = 0) + ɪ
∂a
∂L
L (a = -I) = L(a = O) — ɪr
∂^
1 ∂2L
+ Z~~τ-
ʌ ∩	2 ∂^2
^=0
1 ∂2L
+-------
ʌ ∩	2 ∂^2
^=0
+O
a=o
(∂3L
(∂O3
^=o
(19)
+O
a=o
^=o
(∂3L
(∂O3
Eq. 17 can be simply expressed as
琛
^∂L^
∂a
∂L
1∣a∣≤1 =而 1∣a∣≤1.
^=0∕	da
B Divergence
B.1	Definition of Divergence in a Manifold
D [P : Q] is called a divergence when it satisfies the following criteria:
1)	D[P : Q] ≥ 0. 2) D[P : Q] = 0 when and only when P = Q. 3) When P and Q are sufficiently
close, by denoting their coordinates by ξP and ξQ = ξP + dξ, the Taylor expansion of D is written
as
D[ξp ： ξp + dξ] = 1 XGij(ξp)dξidξj + O(∣dξ∣3),	(20)
and Riemannian metric Gij is positive-definite, depending on ξP.
B.2	Bregman Divergence
Bregman divergence DB[ξ : ξ0] is defined as the difference between a convex function ψ(ξ) and its
tangent hyperplane Z = ψ(ξ0) + (ξ — ξ0)Vψ(ξ0), depending on the Taylor expansion at the point ξ0:
DB [ξ ： ξ0] = ψ(ξ) — ψ(ξ0) — (ξ — ξ0)Vψ(ξ0).	(21)
B.3	KL Divergence and Fisher Information Matrix
The KL divergence can be defined between p(x∣θ) andp(x∣θ0):
DKL [p(x∣θ):
p(χ∣θ0)] =ʃp(x∣θ) logp(x∣θ)dx — ʃp(x∣θ) logp(x∣θ0)dx.
(22)
12
Under review as a conference paper at ICLR 2022
The first derivative is:
Vθ0Dkl[p(x∣Θ): p(x∣θ0)]
=Jp(x∣Θ)Vθo logp(x∣θ)dx — Jp(x∣Θ)Vθo logp(x∣θ0)dx
=—Jp(x∣Θ)Vθo logp(x∣θ0)dx.
The second derivative is:
VθoDkl[p(x∣Θ) : p(x∣θ0)]
=Jp(x∣θ)Vθo logp(x∣θ)dx — Jp(x∣θ)Vθo logp(x∣θ0)dx
=—Jp(x∣θ)Vθo logp(x∣θ0)dx.
(23)
(24)
We deduce the Taylor expansion of the KL divergence at θ = θ0 :
Dkl[p(x∣Θ): p(x∣θ0)] ≈ Dkl[p(x∣Θ): p(x∣θ)]
—(∕p(x∣θ)Vθlogp(x∣Θ)∣θ=θodx) dθ — 2dθ> (∕p(x∣θ)Vθlogp(x∣Θ)∣θ=θodx)) dθ
=0 — ( [p(x∣θ) Vp(Xy dx) dθ — 1 dθ> ( Zp(x∣θ)V [Vp(X]θ) 1 dx) dθ
UyAj p(x∣θ) J 2	U ' j	[ p(x∣θ)」J
=-(V ∕p(χ∣θ)dχ)> dθ -1 …θ V2p(x©)p(M盛X(XrrVp(Xlθ)> dx) dθ
=(V1)dθ — 1 dθ> (v2 Zp(X网dX - Zp(X∣θ) [^p(X≡ 1 [Vpx^ 1> d,) dθ
2	∖ J	J L p(x∣Θ) J L p(x∣Θ) J J
=2dθ>Eρ(χ∣θ) [V logp(X∣θ) V logp(X∣θ)>]dθ
=1 dθ>F (θ)dθ.
(25)
The Taylor expansion of the KL divergence at θ = θ0 is related to the Fisher Information Matrix.
B.4 Hyperbolic Divergence and Weak Curvature Metric
The first derivative of the Hyperbolic divergence is:
Vθ0 DH [θ0 : θ]
=X Vθ0τ^2 logcosh(τθi) — Vθ0τ^2 logcosh(τθi) — TVe，(θi — θi)tanh(τθi)
i
^X Veoɪ logcosh(τθi) — Ltanh(Tθ).
i
(26)
The second derivative of the Hyperbolic divergence is:
VθoDh[θ0 : θ] = X Vθo T12 logcosh(τθi).	(27)
i
13
Under review as a conference paper at ICLR 2022
We deduce the Taylor expansion of the Hyperbolic divergence at θ = θ0 :
DH [θ0 : θ] ≈ DH [θ : θ] +
^X Vθo ɪ log cosh(τθi) 一 Ltanh(Tθ))
dθ
θ0=θ
+ 2 dθ>
log cosh(τ θi0 )	dθ
θ0=θ
0 + 0 + 2d^ dθ> V
2τ2
V Cosh(T θfl
dθ~dθ
cosh(T θ)
2⅛ dθ
> V2 cosh(Tθ) cosh(T θ) 一 V cosh(T θ)V cosh(T θ)
cosh2(τ θ)
>
dθ
(28)
1	> V V2 cosh(τθ)	2 sinh(Tθ)
2τ2	I Cosh(T θ)	T Cosh(T θ)
sinh(τ θ)
Cosh(T θ)
X ^X δij 一 [tanh(T θ)tanh(T θ)>]j dθi dθj.
i,j
C The Steepest Descent Direction in a Riemannian Manifold
C.1 Proof of Lemma 1
We would to know in which direction minimizes the loss function with the constraint of the KL
divergence, so that we do the minimization:
dθ* =	arg min	L(θ + dθ)
dθ s.t. Dkl [p(x,θ)：p(χ,θ+dθ)] = c
where c is the constant. The loss function descends along the manifold with constant speed, regard-
less the curvature.
Now, we write the minimization in Lagrangian form. Combined with Appendix B.3, the KL diver-
gence can be approximated by its second order Taylor expansion. Approximating L(θ + dθ) with it
first order Taylor expansion, we get:
dθ* = arg min L(θ + dθ) + λ (Dkl[p(x, θ) : p(x, θ + dθ)] — C)
dθ
≈ arg min L(θ) + VθL(θ)>dθ + — dθ>Fijdθ — cλ.
dθ	2
To solve this minimization, we set its derivative w.r.t. dθ to zero:
∂λ
0 = ∂dθ L(θ) + VθL(θ) dθ + 2dθ Fijdθ — cλ
=Vθ L(θ) + 2 Fij dθ
λFdθ = —2Vθ L(θ)
2
dθ = - -F T Vθ L(θ)
λ
where a constant factor 2∕λ can be absorbed into learning rate. UP to now, We get the optimal
descent direction, i.e., the opposite direction of gradient which takes into account the local curvature
defined by F-1.
C.2 Proof of Lemma 3
Now we can deduce the steepest descent direction while taking into account the weak curvature
manifold defined by the Hyperbolic divergence. With the constraint of Hyperbolic divergence in a
14
Under review as a conference paper at ICLR 2022
constant c, we do the minimization of the loss function L(θ) in Lagrangian form:
dθ* =	arg min L(θ + dθ)
s.t.DH [θ0：θ] = c
= arg min L(θ + dθ) + λ (DH [θ0 : θ] - c)
dθ
≈ arg min L(θ) + VθL(θ)>dθ + — dθ> [δj — tanh(τθ) tanh(τθ)>] dθ — cλ.
dθ	2
(29)
To solve the above minimization, we set its derivative with respect to dθ to zero and obtain the
opposite direction of gradients in weak curvature manifold:
0 = VθL(θ) + ； [δij — tanh(τθ) tanh(τθ)>] dθ
→ —； [δjj — tanh(τθ) tanh(τθ)>] dθ = VθL(θ)	(30)
→ dθ ≈ -- [I + tanh(τθ) tanh(τθ)>] VθL(θ).
λ
Where a constant factor 2∕λ can be absorbed into learning rate. The last line of the above formula
is true because each element in tanh(τ θ) tanh(τ θ)> is much smaller than 1 to satisfy the weak
curvature metric. Specifically, I — tanh(τ θ) tanh(τ θ)> is a strictly diagonally-dominant matrix
which satisfies that the inverse matrix must exist.
D Comparisons with Other Training-based Quantization
We first list the comparison results of 1-bit weight quantization in Table 5.
Table 5: The experimental results of ManiQuant compared to other quantization.
Dataset	Network		Method		Test Acc (%)	OriginalAcc(%)
CIFAR10	ResNet20	ProxQuant (Bai et al., 2018) MetaQuant (Chen et al., 2019) LQ-Nets (Zhang et al., 2018) ManiQuant (WCG)	90.21 90.80 90.10 90.88	91.50
ImageNet	ResNet18	LR Net (Shayer et al., 2018) MetaQuant (Chen et al., 2019) ADMM (Leng et al., 2018) ManiQuant (WCG)	59.90/82.30 63.44/84.77 64.80/86.20 65.18/86.26	69.76/89.08
Next, we extend the bit width to more than 1-bit, and the quantization function of k-bit used is
Q(W)= 2k-1 — 1 round ((2k-1 — 1)W) ∙
(31)
We quantize the weights of all layers, and activations of all layers except the first layer, i.e, the image
itself. Note that the QNN is initialized by the trained full-precision model. The comparison results
are listed in Table 6.
15
Under review as a conference paper at ICLR 2022
Table 6: The classification accuracy results on ImageNet and comparison with other training-based
quantizations, with AlexNet (Krizhevsky et al., 2012), ResNet-50 and MobileNet (Howard et al.,
2017). Note that the accuracy of full-precision baseline is reported by (Elhoushi et al., 2019).
Method	W	A	Top-1 (%)		Top-5 (%)	
			Accuracy	Gap	Accuracy	Gap
AlexNet (Original)	32	32	56.52	-	79.07	-
ManiQuant (WCG)	6	32	56.39	—0.13	78.78	—0.29
DeepShift-Q (Elhoushi et al., 2019)	6	32	54.97	—1.55	78.26	—0.81
ResNet-50 (Original)	32	32	76.13	-	92.86	-
ManiQuant (WCG)	8	8	76.10	—0.03	92.88	+0.02
INT8 (Zhu et al., 2020)	8	8	75.87	—0.26	-	-
MobileNet (Original)	32	32	70.61	-	89.47	-
ManiQuant (WCG)	5	5	61.32	—9.29	84.08	—5.39
SR+DR (Gysel et al., 2018)	5	5	59.39	—11.22	82.35	—7.12
RQ ST (Louizos et al., 2019)	5	5	56.85	—13.76	80.35	—9.12
ManiQuant (WCG)	8	8	70.86	+0.25	89.60	+0.13
RQ (Louizos et al., 2019)	8	8	70.43	—0.18	89.42	—0.05
E The Algorithm Design
Algorithm 1 An algorithm for computing the gradient of the loss function L and training a quan-
tized neural network. Note that we use the gradient measured by the Hyperbolic divergence here.
Norm(*) specifies how to normalize the activations and BackNorm(*) specifies how to back-
propagate through the normalization, which is consistent with (Courbariaux et al., 2016). Note
that mat(*) is the reverse operator of vec(*), which turns the column vector back into a matrix.
ɪ	IC ♦	1 .	/	∖ Λ	1 . / ɪɪ 7- ɪɪ 7	ɪɪ T ∖ Λ	1
Input: A minibatch of inputs and targets (x = a0, y), θ mapped to (W1, W2, . . . , Wl), θ mapped
to W1, W2, . . . , Wl , a nonlinear function f, a constant factor χ and a learning rate η.
Output: The updated discrete parameters θ.
1: {Forward propagation}
2: for i = 1; i ≤ l; i + + do
3:	Compute Wi = Q(Wi);
4:	Compute Si = WZi ^i-ι;
5:	Compute 诙=Norm (f Θ Si);
6:	Compute ^i = Q@);
7: end for
8: {Loss derivative}
9: COmPUte Val* L 11 * * = ¾x)lz=^lI∣aι∣≤ι;
10: {Backward propagation}
11: for i = l; i ≥ 1; i - - do
12:	Compute VsiL = BackNorm(Vai L Θ f0(Si));
13:	Compute Pi = vec (tanh (Wi)) vec (tanh (Wi))>
14:	Compute V Wi L = mat ((I + X ∙ tρ⅛)vec (VsiL aLj);
15:	Compute Vai-IL = W> Vsi L;
16: end for
17: {The parameters update}
18: for i = 1; i ≤ l; i + + do
19:	Update Wi = Wi — nVWiL IWi∣≤ι;
20: end for
16