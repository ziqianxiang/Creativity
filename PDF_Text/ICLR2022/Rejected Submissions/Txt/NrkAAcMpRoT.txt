Under review as a conference paper at ICLR 2022
C-MinHash: Improving Minwise Hashing with
Circulant Permutation
Anonymous authors
Paper under double-blind review
Ab stract
Minwise hashing (MinHash) is an important and practical algorithm for generat-
ing random hashes to approximate the Jaccard (resemblance) similarity in mas-
sive binary (0/1) data. The basic theory of MinHash requires applying hundreds
or even thousands of independent random permutations to each data vector in the
dataset, in order to obtain reliable results for (e.g.,) building large-scale learning
models or approximate near neighbor search in massive data. In this paper, we
propose Circulant MinHash (C-MinHash) and provide the surprising theoret-
ical results that using only two independent random permutations in a circulant
manner leads to uniformly smaller Jaccard estimation variance than that of the
classical MinHash with K independent permutations. Experiments are conducted
to show the effectiveness of the proposed method. We also analyze a more conve-
nient C-MinHash variant which reduces two permutations to just one, with exten-
sive numerical results to validate that it achieves essentially the same estimation
accuracy as using two permutations with rigorous theory.
1 Introduction
Given two D-dimensional binary vectors v, w ∈ {0, 1}D, the Jaccard similarity is defined as
J(v, w)
PD=I {Vi = Wi = 1}
PD=I l{vi + Wi ≥ 1}
(1)
which is a commonly used similarity metric in machine learning and web search applications. The
vectors v and W can also be viewed as two sets of items (which represent the locations of non-zero
entries), the Jaccard similarity can be equivalently viewed as the size of set intersection over the size
of set union. The well-known method of “minwise hashing” (MinHash) (Broder, 1997; Broder et al.,
1997; 1998; Li & Church, 2005; Li & Konig, 2011) is a standard technique for ComPUting/estimating
the Jaccard similarity in massive binary datasets, with numerous applications such as near neighbor
search, duPlicate detection, malware detection, web search, clustering, large-scale learning, social
networks, comPuter vision, etc. (Charikar, 2002; Fetterly et al., 2003; Henzinger, 2006; Das et al.,
2007; Buehrer & ChellaPilla, 2008; Bendersky & Croft, 2009; Chierichetti et al., 2009; Lee et al.,
2010; Li et al., 2011; Deng et al., 2012; Chum & Matas, 2012; Shrivastava &Li, 2012; He et al.,
2013; Tamersoy et al., 2014; Shrivastava & Li, 2014; Zamora et al., 2016).
1.1	A Review of Minwise Hashing (MinHash)
Algorithm 1 Minwise-hashing (MinHash)
Input: Binary data vector v ∈ {0, 1}D, K indePendent Permutations π1, ..., πK: [D] → [D].
Output: K hash values h1(v), ..., hK(v).
For k = 1 to K
hk(v) J mini—=。πk(i)
End For
1
Under review as a conference paper at ICLR 2022
For simplicity, Algorithm 1 considers just one vector v ∈ {0, 1}D. In order to generate K hash val-
ues for v, we assume K independent permutations: π1, ..., πK : [D] 7→ [D]. For each permutation,
the hash value is the first non-zero location in the permuted vector, i.e., hk(v) = mini:vi 6=0 πk(i),
∀k = 1, ..., K. Similarly, for another binary vector w ∈ {0, 1}D, using the same K permutations,
we can also obtain K hash values, hk(w). The estimator of J(v, w) is simply
1K
Jmh(v, W) = K 工 l{hk(V) = hk(w)},	(2)
where 1{∙} is the indicator function. By fundamental probability and the independence among the
permutations, it is easy to show that
E[Jmh] = J,	VarJMH] =	".	(3)
K
How large is K? The answer depends on the application domains. For example, for training large-
scale machine learning models, it appears that K = 512 or K = 1024 might be sufficient (Li et al.,
2011). However, for approximate near neighbor search using many hash tables (Indyk & Motwani,
1998), it is likely that K might have to be much larger than 1024 (Shrivastava & Li, 2012; 2014).
In the early work of MinHash (Broder, 1997; Broder et al., 1997), actually only one permutation
was used by storing the first K non-zero locations after the permutation. Later Li & Church (2005)
proposed better estimators to improve the estimation accuracy. The major drawback of the original
scheme was that the hashed values did not form a metric space (e.g., satisfying the triangle inequal-
ity) and hence could not be used in many algorithms/applications. We believe this was the main
reason why the original authors moved to using K permutations (Broder et al., 1998).
1.2	Outline of Main Results
From K Permutations to two. The main idea of this work, is to replace the independent per-
mutations in MinHash with “circulant” permutations. Thus, we name the proposed framework C-
MinHash (circulant MinHash). The “circulant” trick was used in the literature of random projec-
tions. For example, Yu et al. (2017) showed that using circulant projections hurts the estimation
accuracy, but not by too much when the data are sparse. In Section 3, we present some (perhaps
surprising) theoretical findings that we just need 2 permutations in MinHash and the results (esti-
mation variances) are even more accurate. Basically, with the initial permutation (denoted by σ),
we randomly shuffle the data to break whatever structure which might exist in the original data,
and then the second permutation (denoted by π) is applied and re-used K times to generate K
hash values, via circulation. This method is called C-MinHash-(σ, π). Before that, in Section 2, we
analyze a simpler variant C-MinHash-(0, π) without initial permutation σ. Although it is not our
recommended method, our analysis for C-MinHash-(0, π) provides the necessary preparation for
later methods and the intuition to understand the need for the initial permutation.
From Two Permutations to one. In Section 5, we provide a more convenient variant, C-MinHash-
(π, π), that only needs one permutation π for both pre-processing and hashing. Although the theo-
retical analysis becomes very complicated, we are able to explicitly write down the expectation of
the estimator, which is no longer unbiased but the bias is extremely small and has essentially no im-
pact on the estimation accuracy (mean square errors). Extensive experiments are provided to verify
that this variant has same estimation accuracy as C-MinHash-(σ, π) using two permutations.
In this paper, we mainly focus on presenting the fundamental idea of C-MinHash and the theoretical
analysis on its uniform variance reduction with non-trivial efforts. Besides improving the Jaccard
estimation accuracy when directly implemented in place of MinHash, the proposed C-MinHash
mechanism can also be conveniently used as a tool to improve more MinHash based algorithms,
for example, the One Permutation Hashing (OPH, Li et al. (2012)). When combined with OPH, C-
MinHash can reduce the required one permutation to effectively 1/K permutation only. Since such
combination is a research direction which requires independent introduction and efforts, we present
the work in a separate manuscript (which could be anonymously shared with Referees if needed).
2
Under review as a conference paper at ICLR 2022
Algorithm 2 C-MinHash-(0, π)
Input: Binary data vector v ∈ {0, 1}D,	Permutation vector π: [D] → [D]
Output: Hash values h1(v), ..., hK (v)
For k = 1 to K
Shift π circulantly rightwards by k units: πk = π→k
hk(v) J mini—=。π→k(i)
End For
3
n→k = {5,8,1,7, 3,4,6,2}
Circular
4
8
^→k+ι = {2,5,8,1,7, 3,4,6}
Figure 1: An illustration of the idea of C-MinHash. The data vector has three non-zeros, v2 = v4
v5 = 1. In this example, hk(v) = 3, hk+1(v) = 1.
2	C-MINHASH-(0, π) WITHOUT INITIAL PERMUTATION
As shown in Algorithm 2, the C-MinHash algorithm has similar operations as MinHash. The differ-
ence lies in the permutations used in the hashing process. To generate each hash hk (v), we permute
the data vector using π→k, which is the permutation shifted k units circulantly towards right based
on π. For example, π = [3, 1, 2, 4], π→1 = [4, 3, 1, 2], π→2 = [2, 4, 3, 1], etc. Conceptually, we may
think of circulation as concatenating the first and last elements of a vector to form a circle; see Fig-
ure 1. We set the hash value hk(v) as the position of the first non-zero after being permuted by π→k.
Analogously, we define the C-MinHash-(0, π) estimator of the Jaccard similarity J(v, w) as
1K
Jo,∏ = Kl 1{hk (V) = hk(w)},	(4)
where h is the hash value output by Algorithm 2. In this paper, for simplicity, we assume K ≤ D.
Next, we present the theoretical analysis for Algorithm 2, in terms of the expectation (mean) and the
variance of the estimator Jo,∏. Our results reveal that the estimation accuracy depends on the initial
data distribution, which may lead to undesirable performance behaviors when real-world datasets
exhibit various structures. On the other hand, while it is not our recommended method, the analysis
serves a preparation (and insight) for the C-MinHash-(σ, π) which will soon be described.
First, we introduce some notations and definitions. Given v, w ∈ {0, 1}D, we define a and f as
DD
a = X l{vi = 1 and Wi = 1},	f = X l{vi = 1 or Wi = 1}.	(5)
i=1	i=1
We say that (v, W) is a (D, f, a)-data pair, whose Jaccard similarity can also be written as J = a/f.
Definition 2.1. Consider v, W ∈ {0, 1}D. Define the location vector as x ∈ {O, ×, -}D, with xi
being “O ”, “ X ”，“ 一 ”，when Vi = Wi = 1, Vi + Wi = 1 and Vi = Wi = 0, respectively.
The location vector x can fully characterize a hash collision. When a permutation π→k is applied,
the hashes hk(V) and hk(W) would collide if after permutation, the first “O” is placed before the
first “x” (counting from small to large). This observation will be the key in our theoretical analysis.
3
Under review as a conference paper at ICLR 2022
Definition 2.2. For A, B ∈ {O, ×, -}, let {(A, B)|4} denote the set {(i, j) : (xi, xj)
(A, B),j - i = 4}. For each 1 ≤ 4 ≤ K - 1, define
L0(4) = {(O, O)|4}, L1(4) = {(O, ×)}, L2(4) = {(O, -)},
G0(4) = {(-, O)|4}, G1(4) = {(-, ×)} ,G2(4)={(-,-)},
H0(4) = {(×, O)|4}, H1(4) = {(×, ×)}, H2(4) = {(×, -)}.
Remark 2.1. For the ease of notation, by circulation we write xj = xj-D when D < j < 2D.
Definition 2.2 measures the relative location of different types of points in the location vector, for a
specific pair of data vectors. One can easily verify that for ∀1 ≤ 4 ≤ K - 1,
|L0| + |L1| + |L2| = |L0| + |G0| + |H0| = a,
|G0| + |G1| +|G2| = |L2| + |G2| + |H2| =D-f,	(6)
|H0|+|H1|+|H2|= |L1| + |G1| + |H1| =f-a,
which is the intrinsic constraints on the size of above sets. We are now ready to analyze the expecta-
tion and variance of J0,π. It is easy to see that J0,π is still unbiased, i.e., E[J0,π] = J, by linearity of
expectation. Lemma 2.1 provides an important quantity that leads to Var[J0,∏ ] as in Theorem 2.2.
Lemma 2.1. For any 1 ≤ s < t ≤ K with t - s = 4, we have
E∏[ l{hs(v) = hs(w)}l{ht(v) = ht(w)}]
∣L0(4)∣ + (∣G0(4)∣ + ∣L2(4)∣)J
f + ∣Go(4)∣ + ∣Gι(4)∣
where the sets are defined in Definition 2.2 and hs, ht are the hash values as in Algorithm 2.
Theorem 2.2. Under the same setting as in Lemma 2.1, the variance of J0,π is
V ar[j0,∏ ] = — +
K
2PsK=2(s-1)ΘK-s+1
K2
- J2 ,
where Θ4，E∏ [l{hs(v) = hs(w)}l{ht(v) = ht(w)}] as in Lemma 2.1 with any t 一 S = 4.
From Theorem 2.2, we see that the variance of J°,∏ depends on a, f, and the size of sets L's and
G’s as in Definition 2.1, which is determined by the location vector x. Since we use the original
data vectors without randomly permuting the entries beforehand, Var[J0,∏] is called “Iocation-
dependent” as it is dependent on the location of non-zero entries of the original data.
3	C-MINHASH-(σ, π) WITH INDEPENDENT INITIAL PERMUTATION
Algorithm 3 C-MinHash-(σ, π)
Input: Binary data vector v ∈ {0, 1}D,	Permutation vectors π and σ: [D] → [D]
Output: Hash values h1(v), ..., hK (v)
Initial permutation: v0 = σ(v)
For k = 1 to K
Shift π circulantly rightwards by k units: πk = π→k
hk (V)《-mini：v0 = 0 π→k (i)
End For
The method C-MinHash-(σ, π) is summarized in Algorithm 3, which is very similar to Algorithm 2.
This time, as pre-processing, we apply an initial permutation σ π on the data to break whatever
structures which might exist. Similarly, we define the C-MinHash-(σ, π) estimator of J as
1K
Jσ,π = KE * 1Mk (V) = hk(w)},
(7)
k=1
where hk ’s are the hash values output by Algorithm 3. We will present our detailed theoretical
analysis and the main result (Theorem 3.5). First, by linearity of expectation and that σ and π are
independent, Jσ,∏ is still an unbiased estimator E[Jσ,∏] = E[l{hi(v) = hi(w)}] = J. To provide
an immediate intuition, the following proposition emphasizes that the collision indicator variables in
(7) are pairwise negatively correlated, which intuitively explains the source of variance reduction.
4
Under review as a conference paper at ICLR 2022
Proposition 3.1. In ⑺,Cov(l{hi(v) = hi(w)}, l{h7-(V) = hj(w)}) < 0, ∀i = j.
_	_	__ _	..	r _ r -.	/	∖	、	，	、、一 /r	，	、	r	z	、 、 r	_	.	,	.	__ _	—
Proof. Denote E = Ei= [1 {h(V) = hi(w)}l{hj-(V) = hj(w)}], for i = j. The task is then
to show that Cov(l{hi(v) = hi(w)}, l{hj-(V) = hj(w)}) = E - J2 < 0. By Theorem 3.5,
J(1-J)
Var[Jmh] < Var[Jσ,∏], where Var[Jmh] = [ and
Var[Jσ,∏] = E (Jσ,∏)2 - J2 = E ](: XX l{hk(V) = hk(w)})	- J2
PiE[l{h(V) = hi(w)}] + P= E[l{hi(V) = hi(w)}l{hj(V) = hj(w)}] - J2
J . K(K - 1)E[1 {hi(V) = hi(w)}l{hj(V) = hj(w)}]	2
κ +	K2	J
where i 6= j
J
K +
(K - 1)E _ J2
K
<
J(I - J)
-K-
E - j2 < 0.

Next, We formally present the path to our main result. The next Theorem gives the variance of Jσ,∏
Theorem 3.2. Let a, f be defined as in (5). When 0 < a < f ≤ D (J ∈/ {0, 1}), we have
一	，一一	.K ~
Var[Jσ,∏] = J + (K - ) - J2,
KK
where with l = max(0, D - 2f + a), and
(8)
E =Ei=j[i{hi(V) = hi(w)}i{hj(V) = hj(w)}] = X [ (f+⅛71 + (f+")/
{l0,l2,g0,g1}
D-f-1
×X
s=l
(D-I
f-a-(D-f-S))( a-1 ))
n4	a-l1 -l2	. (9)
The feasible set {l0, l2 , g0, g1} satisfies the intrinsic constraints (6), and
n1 = g0 - (D - f - s - g1), n2 = D - f - s - g1,
n3 = l2 - g0 + (D - f - s - g1), n4 = l1- (D - f - s - g1).
Also, When a = 0 or f = a (J = 0 or J = 1 ),we have VarJσ∏ ] = 0.
Since the original locational structure of the data is broken by the initial permutation σ, VarJc,∏ ]
only depends on the value of (D, f, a), i.e., it is “location-independent”. This would make the
performance of C-MinHash-(σ, π) consistent in different tasks. Same as MinHash, Proposition 3.3
states that given D and f, the variance of Jσ,π is symmetric about J = 0.5, as illustrated in Figure 2,
which also shows that the variance of Jσ,∏ is smaller than the variance of the original MinHash.
Proposition 3.3 (Symmetry). VarJcr∏ ] is the same for the (D, f, a) -data pair and the (D, f, f 一
a)-data pair, ∀0 ≤ a ≤ f ≤ D.
Figure 2: VarJσ,∏] versus J, with D = 1000 and varying f. Left: K = 500. Right: K = 800.
5
Under review as a conference paper at ICLR 2022
100
10-1
J = 0.5
J = 0.3
D
Figure 3: Left: Theoretical
VarianceratiO VaJMHf, D
5 4 3 2
O"B ①OUEUE>
0.2	0.4	0.6	0.8
J
〜
E, f
10
1000, K
. Each dash line represents the corresponding J2 . Mid:
=800. Right: Variance ratio VarJMHJ, D = 1000.
V ar[Jσ
,π ]

A rigorous comparison of V ar[Jσ,π] and V ar[JM H] appears to be a challenging task given the
complicated combinatorial form of V ar[Jσ,π]. The following lemma characterizes an important
property of E in (9), that it is monotone in D when a and f are fixed, as illustrated in Figure 3 (left).
Lemma 3.4 (Increasing Increment). Assume f > a > 0 are arbitrary and fixed. Denote ED as in
(9) in Theorem 3.2, with D treated as a parameter. Then we have ED+1 > ED for ∀D ≥ f.
Equipped with Lemma 3.4, we arrive at the following main theoretical result of this work, on the
uniform variance reduction of C-MinHash-(σ, π).
Theorem 3.5 (Uniform Superiority). For any two binary vectors v, w ∈ {0, 1}D with J 6= 0 or 1,
it holds that V ar[Jσ,π (v, w)] < V ar[JM H (v, w)].
That is, using merely two permutations as C-MinHash-(σ, π) improves the Jaccard estimation vari-
ance of classical MinHash, in all cases. Next, in Figure 3 (mid) and Theorem 3.6, we show that
interestingly, the relative improvement is actually the same for any J, for given D, f and K.
Proposition 3.6 (Consistent Improvement). Suppose f is fixed. In terms of a, the variance ratio
VarJMH(V,w)] isconstantforanv 0 < α< f
Var[Jσ,π(v,w)] is c0nstantfor any 0 <a<f.
How is the improvement affected by the sparsity (i.e., f) and the number of hashes K? In Figure 3
(right), We plot the variance ratio VarJMH] With different f and K, given fixed D. We see that the
V ar[Jσ,π ]
improvement in variance increases With K (more hashes) and f (more non-zero entries). Note that,
by Proposition 3.6, here We do not need to consider a Which does not affect the variance ratio. The
results in Figure 3 again verify Theorem 3.5, as the variance ratio is alWays greater than 1.
4	Numerical Experiments
In this section, We provide numerical experiments to validate our theoretical findings and demon-
strate that C-MinHash can indeed lead to smaller Jaccard estimation errors.
4.1	Sanity Check: a S imulation Study
A simulation study is conducted on synthetic data to verify the theoretical variances given by The-
orem 2.2 and Theorem 3.2. We simulate D = 128 dimensional binary vector pairs (v, w) With
different f and a, Which have a special locational structure that the location vector x is such that a
“O”,s are followed by (f - a) “x”'S and then followed by (D - f) “-”'S sequentially. We plot the
empirical and theoretical mean square errors (MSE = variance + bias2) in Figure 4:
•	The theoretical variance agrees with the empirical observation, as the curves overlap, con-
firming Theorem 2.2 and Theorem 3.2. The variance reduction increases with larger K.
•	V ar[Jσ,π] (C-MinHash-(σ, π)) is always smaller than V ar[JMH], as stated by Theo-
rem 3.5. In contrast, V ar[J0,π] (C-MinHash-(0, π)) varies significantly depending on dif-
ferent data structures, as mentioned in Section 2.
6
Under review as a conference paper at ICLR 2022
Figure 4: Empirical vs. theoretical variance of J0,π (C-MinHash-(0, π)) and Jσ,π (C-MinHash-
(σ, π )), on synthetic binary data vector pairs with different data patterns.
Figure 5: Mean Absolute Error ( MAE) of MinHash and C-MinHash on real-world datasets.
K

4.2	Jaccard Estimation on Text and Image Datasets
We test C-MinHash on four datasets, including two text datasets: the NIPS full paper dataset from
UCI repository (Dua & Graff, 2017), and the BBC News dataset (Greene & Cunningham, 2006),
and two popular image datasets: the MNIST dataset (LeCun et al., 1998) with hand-written digits,
and the CIFAR dataset (Krizhevsky, 2009) containing natural images. All the datasets are processed
to be binary. For each dataset with n data vectors, there are in total n(n - 1)/2 data vector pairs.
We estimate the Jaccard similarities for all the pairs and report the mean absolute errors (MAE). The
results are averaged over 10 independent repetitions, for each dataset, as shown in Figure 5:
•	The MAE of C-MinHash-(σ, π) is consistently smaller than that of MinHash, again con-
firming Theorem 3.5 on the benefit of the improved variance. The improvements become
more substantial with larger K, which is consistent with the trend in Figure 3.
•	Without the initial permutation σ, the accuracy of C-MinHash-(0, π) is affected by the
distribution of the original data, and it is worse than C-MinHash-(σ, π) on all these four
datasets. Also, the performance of C-MinHash-(0, π) on image data seems much worse
than that on text data, which we believe is because the image datasets contain more struc-
tural patterns. This again suggests that the initial permutation σ might be needed in practice.
In summary, the simulation study has verified the correctness of our theoretical findings in Theo-
rem 2.2 and Theorem 3.2. The experiments with Jaccard estimation on four datasets confirm that
C-MinHash is more accurate than the original MinHash. The initial permutation σ is recommended.
5	C-MINHASH-(π, π): PRACTICALLY REDUCING TO ONE PERMUTATION
We propose a more convenient variant, C-MinHash-(π, π), which only requires one permutation.
That is, π is used for both pre-processing and circulant hashing. The procedure is the same as Al-
gorithm 3, except that the initial permutation σ is replaced by ∏. Denote the Jaccard estimator J∏,∏.
7
Under review as a conference paper at ICLR 2022
The complicated dependency between π (initial permutation) and π→k (hashing) makes the theoret-
ical analysis challenging. In the following, we provide the mean of each hash collision indicator.
Theorem 5.1. Assume K ≤ D and let a, f be defined as (5). The location vector x is defined in
Definition 2.1. Denote B1 = {i : xi = O}, B2 = {i : xi = ×} and B3 = {i : xi = -}. For
a ≤ j ≤ D and 1 ≤ k ≤ K, define
A- (j) = {xi : (i + k - 1 mod D) + 1 ≤ j}, A+(j) = {xi : (i + k - 1 mod D) + 1 > j}.
Let n-,1(j) = |{xi = O : i ∈ A-(j)}| be the number of “O” points in A-(j). Analogously let
n-,2(j),n-,3(j) be the number of "× ” and “ — “points in A-(j), and n+,ι(j),n+,2(j),n+,3(j)
be the number of “ O ”, “ × ” and “ — ” points in A+(j). Then, for the k -th C-MinHash-(π, π) hash,
D3
E[l{hk(V) = hk(w)}] = X X Pj(Z) ∙ {XΨi(j)+ X(1 - nz-(h))Pι},
j=1 Z∈Θj	i=1	i∈B1	-,1
where Pj(Z) is the density function of Z = (z-,k|13,z+,k|31) which follows hyper(D, D -
f,n-,k(j)∣3,n+,k(j)|3), with domain Θj. For q = 1,2, 3, denote 1# = l{j# ∈ Bq}, and
3
ψqj)= X X1#。-f + i{q = 3}(2F- 1))(1 -
i∈Bq,j<i* p=1	n-,p(j)	n-,p(j)
「Jq
+ X	[1#(
i∈Bq ,j>i*
3
1 —% )(1 ——,q—))+X ɪa —
n-,q(j)	n-,q (j) - 1	p6=q	n-,q (j)
)(1- n⅛)] PqJ J
where iJ = (i + k - 1 mod D) + 1, i# = (i - k - 1 mod D) + 1, ∀i. Define JJ =于_a———, and
~	~	1	(b0)( D-j-b0)	~
D _ D	r3 ri+r2-1	D
PI = P2 = r1+r2(D-f)( f), P3
r3 ri+r2
1 ( bo )(D-j-bO)
1	r3 —1	r1 +r2
r3 (Dr- f )Q+r2
Jq = r1 - 1{q =1} + (1 - r1 + ；： -1{q=3} )J J, q =1, 2, 3,
D -j - b0	D -j - b0
where b0 = P3k=1z+,k, r1 = a-z—,1 -z+,1, r2 = f-a-z—,2-z+,2 and r3 = D-f -z—,3-z+,3.
Theorem 5.1 Says that E[l{hk(V) = hk(w)}] would be different for different k. Figure 6 presents
numerical examples to validate the theory and demonstrate the magnitude of bias2 (recall MSE =
bias2 + variance), where the simulations match perfectly Theorem 5.1. As the bias of E[l{hk (v)=
Mean Estimate	Bias2
Figure 6:	1st row: bias2 = (E[J∏,∏] - J)2 vs. number of hashes K on simulated (D, f, a)-data
pairs, D = 64. 2nd row: mean of each collision indicator.
8
Under review as a conference paper at ICLR 2022
hk (w)}] can be positive or negative, the overall bias of Jπ,π would approach 0 as K increases. From
the 2nd row, we see that bias2 is very small (10-5 or even smaller) and indeed converges to 0 with
more hashes, i.e., the averaging effect. Also, from the proof (see Appendix A.8) we can know that
when a and f are fixed, as D increases, E[Jπ,π] would converge to J.
We found through extensive numerical experiments that, the MSE of J∏,∏ is essentially the same
as Jσ,∏. Figure 7 (1st row) compares the empirical of C-MinHaSh-(∏, ∏) with the theoretical vari-
ances of C-MinHash-(σ, π), where the simulated data has the same special locational structure as in
Section 4.1. In the 2nd row, we present the MAE comparison on real datasets, where we see that
the curves for these two estimators (Jσ,π and Jπ,π) match well. In all figures, the overlapping MSE
curves verify our claim that we just need one permutation π. Due to the space limitation, we provide
more empirical justifications in Appendix B.
I--2 Perm
---1 Perm
101
K
Iz........... 一θ0
1
Oooo
山s
(128,64,8)
02
---1 Perm
2 Perm Theo.
K
×10-2	x10-2
29
」。山 9n-0sq< UEeW
2
对K
6
2
I . 1.
Ci
苍一山 Oln-Osqv UEe
NIPS
-0=山 Oln-Osqv UEeW
MNIST
1 Perm
2 Perm
2i0
K
2,K


5
2
6
2
W K
Figure 7:	1st row: estimator MSE vs. K on simulated data pairs. 2nd row: MAE of Jaccard
estimation on four datasets. “1 Perm” is C-MinHash-(π, π), and “2 Perm” is C-MinHash-(σ, π).
6 Conclusion
The method of minwise hashing (MinHash), from the seminal works of Broder and his colleagues,
has become standard in industrial practice. One fundamental reason for its wide applicability is
that the binary (0/1) high-dimensional representation is convenient and suitable for a wide range of
practical scenarios. The so-called Jaccard similarity is a popular measure of similarity in binary data.
To estimate the Jaccard similarity J, standard MinHash requires to use K independent permuta-
tions, where K, the number of hashes, can be several hundreds or even thousands in practice. In this
paper, we proposed Circulant MinHash (C-MinHash) present the surprising theoretical results that,
with merely 2 permutations, we still obtain an unbiased estimate of the Jaccard similarity with the
variance strictly smaller than that of the original MinHash, as confirmed by numerical experiments
on simulated and real datasets. The initial permutation is applied to break whatever structure the
original data may exhibit. The second permutation is re-used K times in a circulant shifting fash-
ion. Moreover, we analyze a more convenient C-MinHash variance which uses only 1 permutation
for both pre-processing and circulant hashing. We derive the complicated mean estimation, and
validate through extensive experiments that it has the same Jaccard estimation accuracy as using 2
permutations with strict theoretical guarantee.
Practically speaking, our theoretical results may reveal a useful direction for designing hashing
methods. For example, in many applications, using permutation vectors of length (e.g.,) 230 might
be sufficient. While it is perhaps unrealistic to store (e.g.,) K = 1024 such permutation vectors in
the memory, one can afford to store two such permutations (even in GPU memory). Using perfectly
random permutations in lieu of approximate permutations would simplify the design and analysis of
randomized algorithms and ensure that the practical performance strictly matches the theory.
Finally, we mention that our work can be used as a building block to improve Li et al. (2012),
reducing the required one permutation to effectively 1/K permutation only. If needed, we would be
happy to anonymously share the technical manuscript with the related results.
9
Under review as a conference paper at ICLR 2022
References
Michael Bendersky and W. Bruce Croft. Finding text reuse on the web. In Proceedings of the
Second International Conference on Web Search and Web Data Mining (WSDM), pp. 262-271,
Barcelona, Spain, 2009.
Andrei Z. Broder. On the resemblance and containment of documents. In Proceedings of the Con-
ference on Compression and Complexity of SEQUENCES, pp. 21-29, Positano, Amalfitan Coast,
Salerno, Italy, 1997.
Andrei Z. Broder, Steven C. Glassman, Mark S. Manasse, and Geoffrey Zweig. Syntactic clustering
of the web. Comput. Networks, 29(8-13):1157-1166, 1997.
Andrei Z. Broder, Moses Charikar, Alan M. Frieze, and Michael Mitzenmacher. Min-wise inde-
pendent permutations. In Proceedings of the Thirtieth Annual ACM Symposium on the Theory of
Computing (STOC), pp. 327-336, Dallas, TX, 1998.
Gregory Buehrer and Kumar Chellapilla. A scalable pattern mining approach to web graph com-
pression with communities. In Proceedings of the International Conference on Web Search and
Web Data Mining (WSDM), pp. 95-106, Stanford, CA, 2008.
Moses S. Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings on
34th Annual ACM Symposium on Theory of Computing (STOC), pp. 380-388, Montreal, Canada,
2002.
Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, Michael Mitzenmacher, Alessandro Panconesi, and
Prabhakar Raghavan. On compressing social networks. In Proceedings of the 15th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining (KDD), pp. 219-228, Paris,
France, 2009.
Ondrej Chum and Jiri Matas. Fast computation of min-hash signatures for image collections. In
Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 3077-3084, Providence, RI, 2012.
Abhinandan Das, Mayur Datar, Ashutosh Garg, and Shyamsundar Rajaram. Google news per-
sonalization: scalable online collaborative filtering. In Proceedings of the 16th International
Conference on World Wide Web (WWW), pp. 271-280, Banff, Alberta, Canada, 2007.
Fan Deng, Stefan Siersdorfer, and Sergej Zerr. Efficient jaccard-based diversity analysis of large
document collections. In Proceedings of the 21st ACM International Conference on Information
and Knowledge Management (CIKM), pp. 1402-1411, Maui, HI, 2012.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
Dennis Fetterly, Mark Manasse, Marc Najork, and Janet L. Wiener. A large-scale study of the
evolution of web pages. In Proceedings of the Twelfth International World Wide Web Conference
(WWW), pp. 669-678, Budapest, Hungary, 2003.
Derek Greene and Padraig Cunningham. Practical solutions to the problem of diagonal dominance
in kernel document clustering. In Proceedings of the Twenty-Third International Conference on
Machine Learning (ICML), pp. 377-384, Pittsburgh, PA, 2006.
Kaiming He, Fang Wen, and Jian Sun. K-means hashing: An affinity-preserving quantization
method for learning binary compact codes. In Proceedings of the 2013 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 2938-2945, Portland, OR, 2013.
Monika Rauch Henzinger. Finding near-duplicate web pages: a large-scale evaluation of algorithms.
In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval (SIGIR), pp. 284-291, Seattle, WA, 2006.
Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: Towards removing the curse
of dimensionality. In Proceedings of the Thirtieth Annual ACM Symposium on the Theory of
Computing (STOC), pp. 604-613, Dallas, TX, 1998.
10
Under review as a conference paper at ICLR 2022
Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324,1998.
David C. Lee, Qifa Ke, and Michael Isard. Partition min-hash for partial duplicate image discovery.
In Proceedings of the 11th European Conference on Computer Vision (ECCV), Part I, pp. 648-
662, Heraklion, Crete, Greece, 2010.
Ping Li and Kenneth Ward Church. Using sketches to estimate associations. In Proceedings of
the Conference on Human Language Technology and the Conference on Empirical Methods in
Natural Language Processing (HLT/EMNLP), pp. 708-715, Vancouver, Canada, 2005.
Ping Li and Arnd Christian Konig. Theory and applications of b-bit minwise hashing. Commun.
ACM, 54(8):101-109, 2011.
Ping Li, Anshumali Shrivastava, Joshua Moore, and Arnd Christian Konig. Hashing algorithms for
large-scale learning. In Advances in Neural Information Processing Systems (NIPS), pp. 2672-
2680, Granada, Spain, 2011.
Ping Li, Art B Owen, and Cun-Hui Zhang. One permutation hashing. In Advances in Neural
Information Processing Systems (NIPS), pp. 3122-3130, Lake Tahoe, NV, 2012.
Anshumali Shrivastava and Ping Li. Fast near neighbor search in high-dimensional binary data.
In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in
Databases (ECML-PKDD), pp. 474-489, Bristol, UK, 2012.
Anshumali Shrivastava and Ping Li. In defense of minhash over simhash. In Proceedings of the
Seventeenth International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 886-
894, Reykjavik, Iceland, 2014.
Acar Tamersoy, Kevin A. Roundy, and Duen Horng Chau. Guilt by association: large scale malware
detection by mining file-relation graphs. In Proceedings of the 20th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining (KDD), pp. 1524-1533, New York, NY,
2014.
Felix X. Yu, Aditya Bhaskara, Sanjiv Kumar, Yunchao Gong, and Shih-Fu Chang. On binary em-
bedding using circulant matrices. J. Mach. Learn. Res., 18:150:1-150:30, 2017.
Juan Zamora, Marcelo Mendoza, and Hector Allende. Hashing-based clustering in high dimensional
data. Expert Syst. Appl., 62:202-211, 2016.
11
Under review as a conference paper at ICLR 2022
A	Proofs of Technical Results
Notations. In our analysis, we will use Is to denote l{h§(v) = hs(w)} in for ∀1 ≤ S ≤ K, where
h is the hash value. Given two data vectors v, W ∈ {0,1}D. Recall in (5): a = PD=I l{v^ =
1 and Wi = 1}, f = PD=I l{vi = 1 or wi = 1}. Thus, the Jaccard similarity J = a/f . We also
. ~ , ..,. .
define J = (a - 1)∕(f - 1).
Definition A.1. Let v, W ∈ {0, 1}D. Define the location vector as x ∈ {O, ×, -}D, with xi being
''O ”, “ X ”，“ 一 ” when Vi = Wi = 1, Vi + Wi = 1 and Vi = Wi = 0, respectively.
Definition A.2. For A, B ∈ {O, ×, -}, let {(i, j) : (A, B)|4} denote a pair of indices {(i, j) :
(xi, xj) = (A, B),j - i = 4}. Define
Lo(4) = {(i,j): (O,O)∣4}, Lι(4) = {(i,j): (O, ×)∣4}, £?(△) = {(i,j): (O,-)∣4},
Go(4) = {(i,j): (一,O)∣4}, Gι(4) = {(i,j) ： (-, ×)∣4} , Gg = {(i,j) ： (-,-)△},
Ho(4) = {(i,j): (×,O)4}, HIg = {(i,j )： (×, ×)∣4}, H2(4) = {(i,j )： (×, -)△}.
Remark A.1. For the ease of notation, by circulation we write xj = xj-D when D < j < 2D.
One can easily verify that given fixed a, f, D, it holds that for ∀1 ≤ △ ≤ K - 1,
∣Lo(4)∣ + ∣Lι(4)∣ + ∣L2 (4)| = ∣Lo(4)∣ + ∣Go(4)∣ + ∣Ho(4)∣ = a,
∣Go(4)l + ∣Gι(4)l + ∣G2(4)I = 1-2(4)1 + 必(4)1 + IH(△) = D - f, (10)
IHo(4)I + Hι(4)I + H2(4)I = ∣Lι(4)I + ∣Gι(4)I + ∣H1(4)I = f - a.
We will refer this as the intrinsic constraints on the size of above sets.
A.1 Proof of Lemma 2.1
Lemma 2.1. For any 1 ≤ s < t ≤ K with t - s = △, we have
En [i {hs(v)=hs(W)}i{ht(v)=ht(W)}] = |Lo(4]+]Go(4]+|L2((4)I)J,
f + |G0(4)| + |G1(4)|
where the sets are defined in Definition 2.2 and hs, ht are the hash values as in Algorithm 2.
Proof. To check whether a hash sample generated by MinHash collides (under some permutation
π), it suffices to look at the permuted location vector x. If a collision happens, after permuted by π,
type "O" point must appear before the first "x" point. That said, the minimal permutation index of
“O" elements must be smaller than that of "×" elements. If the hash sample does not collide, then
the first "×" must appear before the first "O". Note that "-" points does not affect the collision.
To compute the variance of the estimator, it suffices to compute E[lslt]. Let L, G and H be the
union of L’s, G’s and H’s, respectively. In the following, we say that an index i belongs to a set if i
is the first term of an element in that set. We have
ILI=a, IHI=f-a, IGI=D-f.
One key observation is that, for a pair (i,j) in above sets, the hash index πs(i) will be the hash index
ofπt(j). We begin by decomposing the expectation into
E[ lslt] = P [collision s, collision t]
= E P [collision S at is, collision t]
iS∈L
2
= X X P [collision S at iss, collision t].	(11)
p=0 i*∈Lp
where iss is the location of the original "O" in vector x that collides for S-th hash sample. It is
different from the exact location of collision in x(πs ). Note that the permutation is totally random,
so the location of collision is independent of Is, and uniformly distributed among all type "O" pairs.
12
Under review as a conference paper at ICLR 2022
1)	When i* ∈ L0. In this case, the minimum index of the type "O" pair in x(∏s), ∏s(i↑), is shifted
to another type “O” pair in x(πt). Therefore, the indices of pairs with the first element being “O”
or "×" originally in χ(∏) will still be greater than ∏t(iS). If sample S collides at i*, hash sample t
will collide when
1.	All the points in Gι, after permutation ∏s, is greater than ∏s(i*). In this case, regardless of
the permuted G0, hash t will always collide.
2.	There exist points in Gi after permutation ∏ smaller than ∏s(i*), and also points in Go that
is smaller than the minimum of permuted G1.
Consequently, We have for i* ∈ Lo,
P [collision S at i*, collision t]
= P[πs(is*) < πs(i),∀i ∈ H ∪ L/is*, and minπs(j) > πs(is*)]
j∈G1
+ P[πs(is*) < πs(i),∀i ∈ H ∪ L/is*, and minπs(j) < min πs(j) < πs(is*)]
j∈G0	j∈G1
=1 ∙ a +	|Go|IG1\ ∙ a ∙ 1
a f + IGIl f + IGoI + IGI\ f + IGIl f a
=	1	+_________访。igii____________ (12)
=T+面 +(f + IGoI + ∣Gi∣)(f + ∣Gi∣)f.	( )
This probability holds for ∀is* ∈ Lo .
2)	When is* ∈ L1 . Similarly, we consider the condition where is* ∈ L1 , and both hash samples
collide. In this case, ∏s (i*) would be shifted toa" × " pair in χ(∏t). That is, the indices of pairs with
the first element being "O" or "×" originally in χ(∏s) will all become greater than ∏s(i*), which
now is the location of a "×" pair in χ(∏t). Thus, to make hash t collide, we will need:
• At least one point from Go is smaller than any other points in H ∪ L ∪ G1 after permutation
πs.
Therefore, for any is* ∈ L1,
P [collision S at is*, collision t]
= P[πs(is*) < πs(i),∀i ∈ H ∪ L/is*, and min πs(j) < min{πs(is*), min πs(j)}]
j∈G0	j∈G1
=	IGo ∣	a 1
=f + IGoI + IGiI ∙ f ∙ a
_	IGoI
=(f + IGoI + ∣Gι∣)f,
(13)
which is true for ∀is* ∈ Li .
3)	When is* ∈ L2.
In this scenario, πs(is*) would be shifted to a "-" pair in x(πt). Therefore, if hash S collides, hash t
will also collide when:
• After applying πs , the minimum of Lo ∪ Ho ∪ Go is smaller than the minimum of Li ∪
Hi ∪ Gi .
Thus, we obtain that for any is* ∈ L2 ,
P [collision S at is*, collision t]
= P[πs(is*) < πs(i),∀i ∈ H ∪ L/is*, and min	πs(j) < min	πs(j)]
j ∈L0 ∪G0 ∪H0	j ∈L1 ∪G1 ∪H1
,P[Ω].
13
Under review as a conference paper at ICLR 2022
Let ls,i* denote the event {∏s(i*) < ∏s(i), Vi ∈ H U L/i*}. Then Ω can be separated into the
following several cases:
1.	Ω1: Is,您,min7∙∈LouHo ∏s(j) < min7∙∈LιuHi ∏s(j), and min7∙∈LouHo ∏s(j) <
minj∈Gι πs(j).
2.	Ω2:	ISK, minj∈Lo∪Ho ∏s(j)	<	minj∈A∪h1	∏s(j),	and minj∈L0uHo ∏s(j)	>
minj∈G1 ∏s(j) > minj∈Go ∏s(j) > ∏s(i*).
3.	Ω3:	Is,您,minj∈Lo∪Ho ∏s(j)	<	min7∙∈Lι∪Hι	∏s(j),	and minj∈L0uHo ∏s(j)	>
minj∈G1 πs(j) > πs(i*) > minj∈Go πs(j).
4.	Ω4: ls,is, min7∙∈LouHo ∏s(j) < min7∙∈LιuHι ∏s(j), and minj∈L0uHo ∏s(j) > ∏s(i*) >
minj∈G1 ∏s(j) > minj∈Go ∏s(j).
5.	Ω5: Is,您,minj∈LouHo ∏s(j) > min7∙∈LιuHι ∏s(j), and TS(i*) < minj∈G0 ∏s(j) <
minj∈LιuHιuGι ∏s(j).
6.	^6: ISK, min7∙∈LouHo ∏s(j) > min7∙∈LιuHι ∏s(j), and minj∈G0 ∏s(j) < ∏s(i*) <
minj∈LιuHιuGι πs(j)∙
We can compute the probability of each event as
P [Ω1 ]=	1	a	∣L0∣ + ∣H0∣ a / + IGIIIL01 + IH0| + ILII + IHII + lGi l， 		a - ∣G0∣	 =(f -∣G0∣)(/ + ∣Gι∣),
P [Ω2 ]=	1 ∙	a	Gl	 a / + IG01 + IGiIIG01 + IGiI + IL01 + IH01 + ILiI + IHiI IGiI	∣L0∣ + ∣H0∣ ,7	：	：	：	：	：	：	：	： 7 , 7	：	：	：	：	：	：	7 Il0∣ + ∣h0∣ + ILiI + IHi I + IGiIIL0l + ∣h0∣ + ILi I+ IHiI _	1	, ∣G0∣ ,	IGiI .	a -IG0I / + IG0l + IGiI	/	f -IG0l / -|GOI-IGil 	∣G0∣∙∣Gi∣∙(a -IG01)	 一(f + ∣G0∣ + ∣Gi∣)(f -∣G0∣)f -IG0I-∣Gi∣)f,
P [Ω3 ]=	_	∣G0∣	1	GJ	∣L0∣ + ∣H0∣ / + IG01 +	IGiI	/ +	IGiIIL01 +	IH01 + ILil + IHiI	+	IGiIIL01 +	IH01	+	ILiI	+	IHiI 	∣G0∣∙∣Gi∣∙(a -∣G0∣)	 一(f + ∣G0∣ + ∣Gi∣)(f + ∣Gi∣)(f - ∣G0∣)f -∣G0∣-∣Gi∣),
P [Ω4 ]=	_	∣G0∣	IGiL ∙ 1 ∙	∣L0∣ + ∣H0∣ / + IG01 + IGiI / + IGiI / IL0I + IH01 + ILiI + IHiI ∣G0∣∙∣Gi∣ ∙(a - ∣G01) 一(f + ∣G0∣ + ∣Gi∣)(f + ∣Gi∣)(f -∣G0∣-∣Gi∣)f,
P [Ω5 ]=	1	∣G0∣	∣Li∣ + IHiI =	： ：	： 7 ♦ 7 ：	：	：	：	：	：	：	： ：	：	7 ♦  	：	：	：	：	：	：	7 / + IG01 + IGiIIG0〔 + IGi〔 + IL0〔 + IH0〔 + |Lil + IHiI IL0〔 + IH0〔 + ILi〔 + IHiI
________IG0∣∙(f - a - IGIl)_________
(f + IG0I + ∣Gι∣)(∕ -IG0I-IG1I)/,
∣G0∣	1	ILiI + IHiI
/ + IG01 + IGII / IL01 + IH0〔 + IL11 + IHII
________IG01 ∙(f - a - IGII)_______
(f + IG0i + IG1I)(f -IG01 一IGif
14
Under review as a conference paper at ICLR 2022
Note that
P [Ω2]+ P [Ω3]+ P [Ω4]
=	IG01 IG"∙ (a - 访) Γ 1	._________1__________.	1
=(f + ∣G0∣ + ∣Gι∣)(f-∣G0∣-∣G1∣) [(f-∣G0∣)f + (f-∣G0∣)(f + 而 + f(f + |G1|)_
__________∣G0∣∙ ∣Gι∣∙ (a - ∣G0∣)(3f - ∣G0∣ + ∣Gι∣)__
=(f + ∣G0∣ + ∣G1∣)(f -∣G0∣-∣G1∣)f(f -∣G0∣)(f + ∣G1∣).
Summing UP all the terms together, we obtain P [Ω] as
6
X P[Ωn]
n=1
f (f + ∣G0∣ + ∣Gι∣)(f - ∣G0∣ - ∣Gι∣)(α - ∣G0∣) + ∣G0∣∣G1 ∣(a - ∣G0∣)(3f - ∣G0∣+ ∣Gι∣)
(f + ∣G0∣+ ∣Gι∣)(f TG0∣-∣Gι∣)(f - ∣G0∣)(f + ∣Gι∣)f
,_______2∣G0∣(f - a - ∣Gι∣)(f - ∣G0∣)(f + ∣Gι∣)__
+ (f + ∣G0∣ + ∣G1∣)(f -∣G0∣-∣G1∣)(f -∣G0∣)(f + ' f
(a - ∣G0∣)(f + ∣G0∣ - ∣Gι∣)(f - ∣G0∣)(f + ∣Gι∣)+ 2∣G0∣(f - a - ∣Gι∣)(f - ∣G0∣)(f + ∣Gι∣)
(f + ∣G01+ ∣Gι∣)(f - ∣G0∣TGι∣)(f - ∣G0∣)(f + ∣Gι∣)f
(a + ∣G0∣)(f - ∣G0∣ - ∣Gι∣)(f - ∣G0∣)(f + ∣Gι∣)
(f + ∣G0∣ + ∣G1∣)(f -∣G0∣-∣G1∣)(f -∣G0∣)(f + ∣Gι∣)f
a + ∣G01
(f + ∣G0∣ + ∣G1∣)f,
(14)
which holds for ∀i* ∈ L2. Now combining (12), (13), (14) with (11), we obtain
F[— ]= ∣L0∣ +	∣G0 ∣∣G1∣∣L0∣	+ ∣G0∣∣L1∣	+	(a + ∣G0∣)∣L2∣
[s t] = f + ∣G1∣ + (f + ∣G0∣ + ∣G1∣)(f + ∣G1∣)f + (f + ∣G01 + ∣G1∣)f + (f + ∣G0∣ + ∣G1∣)f
(15)
Here, recall that the sets are associated with all 1 ≤ s<t ≤ K such that △ = t - s. Using the
intrinsic constraints (10), after some calculation we can simplify (15) as
F [11]= ∣L0∣	+	a(∣G0∣+ ∣L2∣)
π[ s t] = f + ∣G0∣ + ∣G1∣ + (f + ∣G0∣ + ∣G1∣)f,
which completes the proof.	口
A.2 PROOF OF THEOREM 2.2
Theorem 2.2. Under the same setting as in Lemma 2.1, the variance of J0,π is
^	1 J , 2 PK=2(s - 1)θK-s+1 丁2
varJ0,∏] = K + ——2 K2----------------J ,
where θ4，E∏ [l{h§(v) = hιs(w)}l{ht(v) = ht(w)}] as in Lemma 2.1 with any t — S = △.
Proof. By the expansion of variance formula, since F[ 12] = F[ls] = J, we have
Var[j0,∏ ]
J +∑K=1 P 鼠 F[l sit]
K +	K2
-J2.
(16)
Note here that for ∀t > s, the t-th hash sample uses πt as the permutation, which is shifted right-
wards by △ = t - s from n§. Thus, we have F[ISIt] = F[IS-^1—/ for ∀0 < i < s 八 t, which
implies F[ISIt] = F[l 1 It-S+ι], ∀s < t. Since by assumption K ≤ D,we have
K K
XXF[ Is ɪt] = 21E[(1112 + lι I3 + ... + lι IK) + (I2I3 + …+ I2IK) + …+ IK-IIK ]
S t=s
2F [(1112 + lι I3 + ... + lι IK) + (1112 + ... + lι IK-I) + …+ I1I2]
15
Under review as a conference paper at ICLR 2022
K
=2 X(S — 1)E[11 lκ-s+2]
K
, 2X(S - 1)ΘK-s+1.	(17)
Finally, integrating (16), (17) and Lemma 2.1 completes the proof.	□
A.3 Proof of Theorem 3.2
Theorem 3.2. Let a, f be defined as in (5). When 0 < a < f ≤ D (J ∈/ {0, 1}), we have
一	，一一	.K ~
Var[Jσ,∏] = J + (K - )	— J2,
KK
where with l = max(0, D — 2f + a), and
(18)
E =	'X	I (	l0________.	a(g0 +12)
{l0,l2,g0,gl} l f+ + g0 + g1 f + g0 + g1)f
D-f-1
×X
s=l
(D-a-1)
(19)
.
The feasible set {l0, l2, g0, g1} satisfies the intrinsic constraints (6), and
n1 = g0 — (D — f — S — g1),	n2 = D — f — S — g1,
n3 = l2 — g0 + (D — f — S — g1), n4 = l1 — (D — f — S — g1).
Also, When a = 0 or f = a (J = 0 or J = 1 ),we have VarJσ∏ ] = 0.
Proof. Similar to the proof of Theorem 2.2, We denote Θ4 = Eσ,∏ [IsIt] with |t 一 s| = 4. Note
that now the expectation is taken w.r.t. both two independent permutations σ and π. Since σ is
random, we know that Θι = Θ2 =…=Θk-i. Then by the variance formula, we have
J2
Var Jσ,π] = K
(K - 1)Θl _ J2
K
(20)
—
Hence, it suffices to consider Θ1 . In this proof, we will set 4 = 1 and drop the notation 4 for
conciseness, and denote E = Θ1 from now on. First, we note that Lemma 2.1 gives the desired
quantity conditional on σ . By the law of total probability, we have
,∣L0∣	+ a(∣G0∣ + LQ
f + ∣G0∣ + |Gi| + (f + ∣G0∣ + ∣Gι∣)f
(21)
where the sizes of sets are random depending on the initial permutation σ (i.e. counted after
permuting by σ). As a result, the problem turns into deriving the distribution of |L0|, |L1 |, |L2 |, |G0|
and |G1 | under random permutation σ, and then taking expectation of (21) with respect to this
additional randomness.
When a = 0, we know that |L0| = |L2| = |G0| = 0, hence the expectation E is trivially 0. Thus, the
Var[Jσ,π] = 0. When f = a, |G1 | = 0, and the constraint on the sets becomes
|L0|+|G0| = |L0|+|L2| =f,
|L2|+|G2| = |G0|+ |G2| =D—f.
Then (21) becomes
~
= Eσ
|L0|
f+ |G0|
R + |L2|
f + IGol _
+
16
Under review as a conference paper at ICLR 2022
Eσ
|L0| + |G0| + |L2 |
_ f + IGo I_
≡ 1.
Therefore, When f = a, we also have Var[Jσ,∏] = 0.
Next, we will consider the general case where 0 < a < f ≤ D. This can be considered as a
combinatorial problem where we randomly arrange a type "O”，(f - a) type "x” and (D - f)
type “-” points in a circle. We are interested in the distribution of the number of {O, O}, {O, ×},
{O, -}, {-, O} and {-, ×} pairs of consecutive points in clockwise direction. We consider this
procedure in two steps, where we first place "x" and “-" points, and then place "O" points.
Step 1. Randomly place “x” and “-” points on the circle.
In this step, four types of pairs may appear: {-, -}, {-, x}, {x, x} and {x, -}. Denote C1, C2,
C3 and C4 as the collections of above pairs. Since
IC1I+IC4I = IC1I+IC2I =D-f,
IC2I+IC3I = IC2I+IC4I =f-a,
knowing the size of one set gives information on the size of all the sets. Thus, we can characterize the
joint distribution by analyzing the distribution of IC1I. First, placing (D - f) "-” points on a circle
leads to (D - f) number of {-, -} pairs. This (D - f) elements can be regarded as the borders that
split the circle into (D - f) bins. Now, we randomly throw (f - a) number of "x” points into these
bins. If at least one "x” falls into one bin, then the number of {-, -} pairs (IC1 I) would reduce by
1, while IC2I and IC4I would increase by 1. If z "x” points fall into one bin, then the number of
{x, x} (IC3I) would increase by (z - 1). Notice that since s ≤ D - f and D - f - s ≤ f - a, we
have max(0, D - 2f + a) ≤ s ≤ D - f. Consequently, for s in this range, we have
P IC1 I = s = P IC1 I = s, IC3 I = f - a - (D - f - s)
= (DD--S)(D-f---i)
=~(f -
(F "---J
=(D-a-i).
(22)
The second line is due to the stars and bars problem that the number of ways to place n unlabeled
balls in m distinct bins such that each bin has at least one ball is mn--11 . For IC1 I = s, we need
n = f - a (number of "x”) and m = IC2 I = D - f - s. Moreover, the number of ways to
place n balls in m distinct bins is n+mm--1 1 . When counting the total number of possibilities, we
have n = f - a and m = D - f . This gives the denominator. We notice that (22) is actually a
hyper-geometric distribution.
Step 2. Randomly place “O” points on the circle.
We have the probability mass function
P[Ψ] , PnIL1I = l1, IL2I = l2, IG0I = g0, IG1I = g1o
D-f -1
= X P	IL1I	= l1, IL2I	= l2, IG0I	=	g0,	IG1I	= g1IC1I	= s P IC1I = s .	(23)
s=D-2f +a
Now it remains to compute the distribution conditional on IC1 I. Here we drop IL0 I because it is
intrinsically determined by IL1 I and IL2I. Again, given a placement of all "x” and "-” points, each
consecutive pair can be regarded as a distinct bin. Therefore, now the problem is to randomly throw
a type "O” points into that (D - a) bins, given that we have placed type "x” and "-” points on the
circle with IC1 I = s (and thus IC2 I = IC3I = D - f - s and IC4I = f - a - (D - f - s) are also
determined correspondingly). In the following, we count the number of "O” points that fall in Ci ,
i = 1, 2, 3, 4, to make the event Ψ happen. Note that
17
Under review as a conference paper at ICLR 2022
•	When at least one "O" point falls into Ci (between {-, -}), ∣L21 and ∣G01 increase by 1.
•	When at least one "O" point falls into C2 (between {-, ×}), ∣L1∣ and ∣G0∣ increase by 1,
while ∣G1∣ decreases by 1.
•	When at least one "O" point falls into C3 (between (×, -}), ∣L21 increases by 1.
•	When at least one "O" point falls into C4 (between (×, ×}), ∣L11 increases by 1.
We denote the number ofbins in Ci, i = 1,2, 3,4 that contain at least one "O" point as n1,n2,n3,n4,
respectively. As a result of above reasoning, in the event Ψ, we have
n1 + n3 = l2,
n2 + n4 = l1 ,
n1 + n = g0,
D - f - S - n2 = 91.
Solving the equations gives
n1 = 90 - (D - f - s - 91),
n2 = D - f - S - 91,
n3 = l2 - 90 + (D - f - s - 91),
n4 = l1 - (D - f - s - 91).
Note that P4=1 n = li + l2. Therefore, event Ψ is equivalent to randomly pick n1,n2,n3 and
n4 bins in C1,...,C4, and then distribute a type "O" points in these (l1 + l2) bins such that each bin
contains at least one "O". Hence, we obtain
P{∣Lι∣ = lι, ∣L2∣ = l2, ∣G0∣ = g0, ∣Gι∣ = gι∣∣Cι∣ = s}
(S )(D-f-S)(D-f-S)(f-a-(D-f-S))( a-1
In ɪ	n2	n3	n4	lɪ + l2 -1
ETj
(S )(D-f-S)(D-f-S)(f-a-(D-f-S))( a-1
In ɪ	n2	n3	n4	a-lɪ -12
(24)
which is also multi-variate hyper-geometric distributed. Now combining (22), (23) and (24), we
obtain the joint distribution of ∣L0∣, ∣Lι∣, ∣Q∣, ∣G0∣ and ∣G1∣ as
p{∣Lι∣ = lι, ∣L2∣ = l2, ∣G0∣ = g0, ∣Gι∣ = gι}
D-f -1
X
s=max(0,D-2f+a)
(s )(D-f-S)(D-f-s)(f-a-(D-f-s))( a-1
vnɪ	n2	nɜ	n4	a-lι -12
/D-α-1
VD-f-1
.(25)
Now let Ξ be the feasible set of (l0,l1,90,91,92) that satisfies the intrinsic constraints (10). The
desired expectation w.r.t. both π and σ can thus be written as
E = 'X ( l0_________,	α⅜0+12))
! If + 90 + 91	(f + 90 + 91)f
D-f-1 ( S )(D-f-S)(D-f-S)(f-a-(D-f-S))( a-1
):	lnɪ 八 n2 八 n3	八	n4	/ ∖a-lɪ -12
S=max(0,D-2f+a)	( a )
/ D-a— 1
VD-f-1
The desired result can then follows by (20).
□
A.4 Proof of Proposition 3.3
Proposition 3.3 (Symmetry). Var[Jσ,∏ ] is the same for the (D, f, a) -data pair and the (D, f, f 一
a)-data pair, VQ ≤ a ≤ f ≤ D.
18
Under review as a conference paper at ICLR 2022
Proof. For fixed a, f, D, let Ei be the expectation defined in Theorem 3.2 for (vι, wι), and E be
that for (v2, w2). From Theorem 3.2 we know that
EI = E(l0,l2,g0,gι)[
l0
a(g0 + l2 )
f + go + gi + (f + go + gi)f ],
where (lo, l2, go, g1) follows the distribution of (|Lo|, |L2 |, |Go|, |G1|) associated with the location
vector x1 of (v1, v2). For data pair (v2, w2), we can consider its location vector x2 as swapping
the "O” and "x" entries of xi. Now We denote the size of the corresponding sets (Definition 2.2)
of x2 as li0s, gi0s, h0is, for i = 0, 1, 2. Since σ is applied before hashing, by symmetry there is a
one-to-one correspondence between the two location vectors. More specifically, lo0 corresponds to
hi, go0 corresponds to gi, gi0 corresponds to go, and l20 corresponds to h2. Therefore, in probability
we can write
C	lo______I	a(g0 + /力 ]
2 - (l0,l2,g0,g1) If + go + gi +(f + go + gi)f J
_ E	h	hi______, (f - a)(g1 + h2)i
=(h1,h2,g0,g1) f + go + gi + (f + go + gi)f .∙
Consequently, we have
lo - hi
E1 -E2 = E(l0,l2 ,h1,h2 ,g0,gl) ʃ i — i +
f + go + gi
a(go + l2 ) - (f - a)(gi + h2 )
.
(f + go + gi )f
In the sequel, the subscript of expectation is suppressed for conciseness. Exploiting the constraints
(10), we deduce that hi = (f - a) - li - gi, h2 = lo + go + li + gi - a and lo + li = a - l2.
Using these facts we obtain
Eh(lo - (f - a) + li + gi)f + a(go +，2)- (f - a)(lo + go + li + 2gi - a)
I	(f + go + gi)f
Eh(2a - f + gi - l2)f + a(go + l2) - (f - a)(2gi + go - l2)
[	(f + go + gi)f
Eh2(f + go + gi)a - (f + go + gi)f
(	(f + go + gi)f
2J - 1.
Comparing the variances of Jσ,π(vi, wi) and Jσ,π(v2, w2), we derive
V ar[Jσ,π (vi, wi)] - Var[Jσ,π(v2,w2)]
. .~ . ~
=(J +(K -I)Ei - J 2) -L +(K -I)E2 -(1-
=(K +	K J )( K + K (I
K-1	K-1
=—(2J - 1) +	(Ei- E2)= 0.
KK
This completes the proof.	□
A.5 Proof of Lemma 3.4
Lemma 3.4 (Increasing Increment). Assume a > 0 and f > a are arbitrary and fixed. Denote ED
as in (19) in Theorem 3.2, with D treated as a parameter. Then we have ED+i > ED for ∀D ≥ f.
Proof. Let the probability mass function (25) with a, f and dimension D be Pa,f,D (lo, l2, go, gi).
Conditional on lo, l2, go, gi with D elements, the possible values lo0 , l20 , go0 , gi0 when adding a "-”
are
•	go0 = go + 1, lo0 = lo, l20 = l2, gi0 = gi. This is true when the new elements falls between a
pair of (×,O), with probability l1'D-g0.
19
Under review as a conference paper at ICLR 2022
•	g1 = gι + 1,/0 = /0,/2 =，2,g0 = go, when the new elements falls between a pair of
(×, ×), with probability f -a-1-gι.
•	g1 = gι + 1,/2 = /2 + 1, /0 = /o,g0 = go, when the new elements falls between a pair of
(O, ×), with probability D.
•	/0 = /0 - 1,/2 = /2 + 1, g0 = g0 + 1,g1 = gι, when the new elements falls between a pair
of (O, O), with probability D.
•	All values unchanged, when the “一” falls between other types of pairs, with probability
D-f+g0+g1
D .
Denote Ξd as the feasible set satisfying (10) with dimension D ≥ f. Above reasoning builds a
correspondence between ΞD and Ξd+i . More precisely, we have
Ed+1=E ∙
Ξd + i '
=X{(
ξd
+	a(g0+，2)
f + g0 + g1	(f + g0 + gI)f
/0
/0
Pa,f,D+1 (/0, /2, g0, g1)
a(g0 + /2 + I)
f + g0 + gι + 1 + (f + g0 + gι + 1)f
'1 + D^^" Pa,f,D (10,I2, g0, gι)
+
+
+
/0
f + g0 + gι +1
_______10______
f + g0 + gι +1
l0-1
f + g0 + gι +1
a(g0 + /2)	f - a - /1 - g1
(f + g0 + gι + 1)f)—D—Pa，f，D(/0, 后 g0, gi)
f⅛⅛⅛⅛1⅛) D^paf,D (/0,/2, g0, gi)
a(g0 + 12 + 2)	/0
(f + g。+ gι + 1)fJDPafD(0, 2,g0,gι)
+
+
+
+
/0
f + g0+ gι + (f + g0+ gI)f
a(g0 + /2) D - f + g0 + g1
Pa,f,D (10,I2, g0, gI) )∙
D
The increment can be computed as
≈ Λ ≈	≈
δD = ED+1 - ED
Σ[ f - g0 - gι h/_10_
[ D ['f + g0 + gι + 1
ξD
/0	)+(	a(g0	+	/2	+ I)	a(g0	+	/2))i
f + g0 + gι	f + g0 + gι + 1 f + g0 + gι 〃
l0	a(f - a - l1 - g1 ) - al0
-D(f + g0 + gι + 1) - Df(f + g0 + gι + 1)产f，D(/0,/2,g0,gi)
—
X
ΞD
(f -	g0	-	gi)[a(f + gi	- /2) -	f/0]	(f -	a)/0	+ a(f	- a - /i -	gi)
—
Df (f + g0 + gi)(f + g0 + gi + 1)
Df (f + g0 + gi + 1)
pa,f,D (/0, /2, g0, gi)
Σ
Ξd
2af (/i + gi) - 2f (f - a)/0 - 2a(f - a)(g0 + gi)
Df (f + g° + gi)(f + g° + gi + 1)
pa,f,D (/0,12 ,g0,gi)
Eh2af (/i + gi) - 2f (f - a)/0 - 2a(f - a)(g0 + gi)i
Df (f + g0 + gi)(f + g0 + gi + 1)	」
Eh 2af (f - a - hi) - 2f (f - a)/0 - 2a(f - a)(g0 + gi + f - f)
〔	Df (f + g0 + gi)(f + g0 + gi + 1)
4a(f - a)i - Eh 2ahi + 2(f - a)/0 i - Eh 2a(f - a)
D(f + g0 + gi)(f +	g0	+ gi + 1)	D(f + g0 + gi)(f + g0 +	gi	+	1)	Df(f + g0	+	gi	+ 1)
4a(f — a)E0 — 2aEi — 2(f — a)E⅛ — 2a(f — a)Eβ,
(26)
where
E0 = Eh
D(f + g0 + gi)(f + g0 + gi + 1)],
Ei = Eh D(f + g0 + gi)(f + g0 + gi + 1)],
1
20
Under review as a conference paper at ICLR 2022
E2 = Eh D(f + go + gι)(f + go + gι + i)i,
E3 - E Dff (f + g0+ gι + 1) 1 .
Note that here the expectations are taken w.r.t. the set size distribution with a,f,D. We can expand
the terms of density function (25) to derive
Pa,f,D (∕θ,∕2,gθ,gi)
D-f-1
X
s = max(0,D-2f+a)
__________________(D - f - S)(D - f )!(f - a - 1)!_____________________
[D - (f + go + gι)]![(f + go + gι) - D + s]!gι!(D - f - S - gι)!
(a — 1)!
(go + gι - l2)![D - S +12 - (f + go + gι)]！(f - a - lι - gι)!(f + gι + lι - D + s)!lo!(a - lo - 1)!
a!(f - a)!(D - f - 1)!
(D-1!	.
Denote a0 = a - 1, f = f - 1, D0 = D - 1 and IO = lo 一 1. We have
E2 = X 777τ-l^l—V Pa,f,D (lo ,l2,go,g1)
〜D(f + go + gι)(f + go + gι + 1)
ξD
∑a(a — 1)	1
D - 1 D(f + go + gι)(f + go + gι + 1)
I C
X-1	_______________(D0 - f0-s)(D0- f0)!(f0- a，- 1)!___________
s=max(OD-2f,+a,) [D0 - (f0 + go + gi)]![(f0 + go + gι) - d' + s]!gι!(D0 - f0 - S - gι)!
_______________________________________(a0 - 1)!______________________________________
(gO + g1 - l2)![D0 - S + l2 - (f0 + gO + g1)]！(f0 - a0 - ll - g1)！(f0 + g1 + li - D0 + s)!lO!(a，- lO - 1)!
a0!(f0- a0)!(D0- f0 - 1)!
(D7-I)
=Xi 3 D(f+go+g1)(f+go+次 + 1) Pjf-1,D-1(lo,l2,go,g1)
a(a - I) E	Γ_______________1______________
D^T~ a-1,f-1,d-11D(f + go + g1)(f + go + g1 + 1)
a(a - 1)
D - 1
E.
Here the subscript means that we are taking expectation w.r.t the set sizes when the number of "O”，
“x” and “一” points is (a - 1, f - 1, D - 1). By symmetry, it can be shown similarly that
E _ (f - a)(f - a - I) E	h________________1________________^∣ _ (f - a)(f - a - I) E
1 -	D-I	a,f-1,d-1 ∣D(f + go + g1)(f + go + g1 + 1) J =	D-I
Substituting above results into (26), we obtain
~	一 f - 2 -
δD = 2a(f - a)[2Eo - f--E - E3].
D-1
To compute Eo, note that with a, f and D, variable g2 is distributed as hyper(D-1, D-f, D-f-1).
For E, the distribution becomes hyper(D - 2, D - f, D - f - 1). Since f + go + g1 = D - g2,we
deduce
D-f -1
Eo =	* X
s=max(O,D-2f)
D-f-1
X
s=max(O,D-2f)
1	(D-/「(D-f-S)
D(D - s)(D - S + 1)	(D二)
1_______________________(D - f -1)!f!______________(D - f )!(f -1)!
D(D - s)(D - S + 1) s!(D - f - S - 1)!(D - f - s)!(-D + 2f + s)!	(D - 1)!
21
Under review as a conference paper at ICLR 2022
and
D-f-1
EE= X
s=max(0,D-2f +1)
D(D - s)(D - s + 1)
D-f-1
s=max(0,D-2f+1) D(D
1
(D-f)!(f-2)!
- s)(D - s + 1)	(D- 2)!
_____________(D - f - 1)!(f - 1)!__________
s!(D - f - S - 1)!(D - f - s)!(-D + 2f + S - 1)!.
For ∀D ≥ f, we have
*E
≤
D-f-1
X
s=max(0,D-2f)
(f-2)(D-1)(-D+2f+s)
D(D-1)f(f-1)(D-s)(D-s+1)
(D-f-1)!f!
(D-f)!(f-1)!
E
≤
E
<
S!(D-f-S-1)!(D-f-S)!(-D+2f+S)!	(D-1)!
-2)(f - (D - f - g2)) i
-1)(D - g2)(D - g2 + 1)J
(f - go - gI)____i
+ go + gι)(f + go + gι + 1)」.
Consequently, we have
ED > 2a(f — a)E [
2
f - g0 - g1
~~；~：---------：~；~：----------------.. 一 ————：---------：~；~：------------------
D(f + go	+	gι)(f +	go	+	gι	+ 1)	Df (f	+ go	+	gι)(f + go + gι	+ 1)
—
f + g0 + g1
]
Df(f+g0+g1)(f+g0+g1+1)
0,
and note that this holds for ∀D ≥ K . The proof is complete.
□
A.6 Proof of Theorem 3.5
Theorem 3.5 (Uniform Superiority). For any two binary vectors v, w ∈ {0, 1}D with J 6= 0 or 1,
it holds that V ar[Jσ,π(v, w)] < V ar[JM H (v, w)].
Proof. By assumption We have 0 < a < f .To compare VarJJσ∏ ] with VarJJmh ] = J(IKJ J =
K + (K-I)J——J2, it suffices to compare E with J2. When D = f, we know that the location vector
x of (v, w) contains no “-” elements. It is easy to verify that in this case, |Go| = |G1 | = |L2 | = 0,
and |Lo| follows hyper(f - 1, a, a - 1). By Theorem 3.2, it follows that when D = f,
ED = 1 E[|Lo|] = aa-) = JJ < J2.
f	f(f - 1)
Recall the definition J = a-, which is always less than J. On the other hand, as D → ∞, we have
|Lo| → 0, |L2| → a, |Go| → a and |G1| → f - a. We can easily show that
EED → J , as D → ∞.
By Lemma 3.4, the sequence (Ef, Ef+1, Ef+2, ...) is strictly increasing. Since it is convergent with
limit J2, by the Monotone Convergence Theorem we know that ED < J2, ∀D ≥ f.	□
A.7 Proof of Proposition 3.6
Proposition 3.6 (Consistent Improvement). Suppose f is fixed. In terms of a, the variance ratio
∩(a)——Var[JMH(v,w)] is Constant for ∩nv 0 < a < f
P(a) = Var[Jσ,∏(v,w)]	t f y 0 <<f.
22
Under review as a conference paper at ICLR 2022
τ->	f -T ,	1 FC F ∙ El	CAA	. T-A	1 C	CF	1	∙	1 1	TΓ-<∙	, 1
Proof. LetE be defined as in Theorem 3.2. Assume that D and f are fixed and a is variable. Firstly,
we can write the variance ratio explicitly as
J -J2
ρ(a)
1-J
K + (KK)E - J2	1 - J - (K - 1)(J - J)
r
We now show that the term J - E = C(1 - J), where C is some constant independent of J (i.e.,
a). Then, for fixed D and f, by cancellation ρ(a) would be constant for all 0 < a < f. We have
J -J = a.
J J = f
= Eh
E h	fl0	+	g0 + l2	i
a''D la(f + g0 + g1)	f + g0 + g1 -I
a2(f + g0 + g1) - f2l0 - af (g0 + l2)
af(f + g0 + g1)
a(a - f)(g0 + g1) + a2f + afg1 - f2l0 - afl2
af(f + g0 + g1)
a(a - f)(g0 + g1) + af (l0 + l1) + afg1 - f2l0
af(f + g0 + g1)
a(a - f)(g0 + g1) + f(a - f)l0 + af(f - a
af(f + g0 + g1)
-h1)i,
(27)
where we use the constraints (10) that l0 + l1 + l2 = a and l1 + g1 + h1 = f - a. We now study the
three terms respectively. We have
a(a - f)(g0 + g1)
af(f + g0 + g1)
i=-(1- J)Eh7Γg+⅛i …-J).
We have shown in the proof of Lemma 3.4 that
Ea，f，D h f + ll00 + gι i
and by symmetry it holds that
a(a - 1)	1	a(a - 1) *
^D-Γ EaTf τ,D-1 f + g0 + gι j, -D-T E ,
,f,D
h1
f + g0 + g1
i = (f - a)(f - a - I) e*
J = D -1	.
E

K
—
]
E
E
E
[
h
]
]
Note that Since f is fixed, (|Go| + |Gi|) is distributed independent of a. Consequently, E0 and E*
are both independent of a. Next, we obtain
Eh ff i=-(1-J) R E
and
E
Ji=(1-J)fE*-(1- J)ff---1 E*.
Summing up the terms and substituting into (27), we derive
金
J - J = C(I - J),
where C = -E0 + (f - fD-2 )E*, which is independent of a. Taking into ρ(a), we get
ρ(a)
1-J
1
1-J-(K-1)C(1-J)	1-(K-1)C,
which is a constant only depending on f, D and K. This completes the proof.
□
23
Under review as a conference paper at ICLR 2022
A.8 Proof of Theorem 5.1
Proof. Denote E[lk] := E[l{hk (V) = hk (w)}] for any 1 ≤ k ≤ K. We first recall some notations.
We have v, w ∈ {0, 1}D, and a and f are defined in (5). Denote B1 = {i : xi = O}, B2 = {i :
xi = ×} and B3 = {i : xi = -} as the sets of three types of points, respectively. For a ≤ j ≤ D
and 1 ≤ k ≤ K, define
A- (j) = {xi : (i + k - 1 mod D) + 1 ≤ j},
A+(j) = {xi : (i + k - 1 mod D) + 1 > j}.
Let n-,1(j) = |{xi = O : i ∈ A-(j)}| be the number of “O” points in A-(j). Analogously let
n-,2(j), n-,3(j) be the number of"x” and “一”points in A-(j), and n+,ι(j), n+,2(j), n+,3(j) be
the number of "O”，“x" and “一” points in A+ (j). For any i, denote i* = (i + k - 1 mod D) + 1,
i# = (i - k - 1 mod D) + 1.
Our analysis starts with the decomposition of hash collision probability,
D
E[lk] = P[hk(v) = hk(w)] = XP[hk(v) = hk(w) = j ,	(28)
j=1
where recall h(∙) is the hash sample. Consider the process for generating the hash. As before, We
look at the location vector x. In Method 2, we first permute x by π to get π(x). Then the k-th
hash samples collide if the minimum of π→k (π(x)) is “O”. One key observation is that, when
applying π→k, the random index for the i-th element in π(x) is exactly the one used for xi# (shifted
backwards) in the initial permutation. A toy example in provided in Figure 8 to help understand the
reasoning.
Figure 8: Illustration ofC-MinHash-(π, π) hash collision, with k = 2. Here, “circulant right” means
“circulant down”. Small indices correspond to upper elements.
Further denote set M = {π(i) : π(i) ∈/ B3} be the collection of indices of initially permuted vector
π(x) that are not “一” points, and MJk = (M - k - 1 mod D) + 1 be the corresponding indices
shifted backwards. In Figure 8, M = {2, 5, 8, 11}. Also, π(6) = 4, and the permutation maps are
described by the red arrows. Consequently, in π→k(π(x)), the 8-th element (“O”) in π(x) will be
permuted to the same index of the 8 - 2 = 6-th element in x, which equals to 4. It is important to
notice that, when considering the k-th collision, only points in MJk matters. Hence, we deduct:
• (Collision condition) Denote i = arg mint∈M	π(xt) be the location of minimal permu-
tation index in M—k. The k-th collision occurs at j i.f.f. π(i) = j, and the i*-th element
in π(x) must be a "O" point. Recall the definition i* = (i + k — 1 modD) + 1.
24
Under review as a conference paper at ICLR 2022
In Figure 8, consider i = 6 andj = 4 for example. Above condition means that the i = 6-th element
in x (“-” in red bold border) is permuted to thej = 4-th position, and it is above all other permuted
elements with red bold borders. Meanwhile, the i + k = 6 + 2 = 8-th element in π(x) must be a
“O”. Figure 8 exactly satisfies the condition, so it depicts a collision. Mathematically, we have
D
E[ι k ] = X P[hk (V) = hk (W)= j
j=1
DD
=XXp[∏(i)=j,∏-1(i*) ∈Bι],
j=1 i=1
(29)
where i = argmi□t∈M-k ∏(xt). In this expression, everything is random of ∏, except for the set
B1 which is fixed given the data.
Now we will focus on deriving the probability for a fixed i and j in (29). Our analysis will be
conditional on the collection of variables Z which is defined as follows. Let z-,1, z-,2 and z-,3 be
the number of "O”，“x" and “一” points in A-(j) ∩ M^k, and z+,ι, z+,2 and z+,3 be the number
of"O”,“x” and “一” points in A+ (j) ∩Mc^k, respectively. Here MJLk represents the complement
of MLk. Notice that Z (and its density function) depends on different j since A- (j) and A+(j)
depends on j . For the ease of notation we suppress the information of j in Z (and z’s). It is easy to
see that Z = (z-,k|13, z+,k|13) follows hyper(D, D 一 f,n-,k(j)|31, n+,k(j)|13). Denote the domain of
Z and Θj . Conditional on Z, we obtain
DD
E[lk] = X X Xp[π(i)=j,π-1(i*) ∈Bι∣z∖pj [z]
j=1 Z∈Θj i=1
DD
=X X XP[∏-1(i*) ∈Bι∣∏(i)= j,z]p[∏(i)= j∣ZPj [z]
j=1 Z∈Θj i=1
DD
,X X X Γ(i, j)Pj Z ,	(30)
j=1 Z∈Θj i=1
with i = arg mint∈M π(xt). We will carefully compute the probabilities in the summation.
Basically, the key is that elements in M need to be controlled, i.e. smaller than j, and other positions
can be arbitrary. Given Z, this means that we need to put r1 = a 一 z-,1 一 z+,1 type “O” points,
r2 = f 一 a 一 z-,2 - z+,2 type "×" points and r3 = D 一 f 一 z-,3 - z+,3 type “一” points no smaller
than j, with π(i) = j exactly. Also note that there are fixed b0 = P3k=1 z+,k type “一” points no
smaller than j .
With all these definitions and reasoning, we are ready to proceed with the proof. Based on xi , we
have three general cases.
1)	xi ∈ B1. The first case is that xi = O.
Case 1a) j < i*. Firstly, We consider the case where j < i*. By combinatorial theory We have
Ph∏(i) = j∣zi = Ph∏(i) = j∣∏-1(j) ∈ B3, Z]ph∏-1(j) ∈ B3∣Zi
(b0)( D-j-b0
1	\:3	rι+r2-1
rτ+τ2 (D-f)(	f
r3	r1 +r2
P π-1(j) ∈/ B3|Z
,Pι∙ p[∏-1(j) ∈ B3∣z],
(31)
where the second probability is that the j-th element in π(x) is not “一”. Conditional on Z, the
probability is dependent on j# :
Ph∏-1(j) ∈ B3∣Zi = X l{j# ∈ Bp}(1 - -2-fτ).
p=1	n-,p(j)
25
Under review as a conference paper at ICLR 2022
Combining with (31) We obtain
3
P[π(i)=j∣Z = P1 X l{j# ∈Bp}(1- --^f-).
p=1	n-,P(j)
(32)
Next we compute P[π-1(i*) ∈ Bι∣π(i) = j, Z]. Note that given the conditions, π-1(i*) has two
cases: 1) it comes from MJk (i.e. it is one of the elements with red bold border); 2) Otherwise. We
then can write
P[π-1(i*) ∈Bι∣π(i)=j,Z]
=P [∏-1(i*) ∈ Bi∣∏-1(i*) ∈ M—k ,π(i) = j, Z ] P [∏-1(i*) ∈ Mj ∣∏(i) = j, Z ]
(1 一
+ P [π
z+,1
n+,1(j)
z+,1
n+,1(j)
z+,1
n+,1(j)
-1
(i*) ∈ Bι∣π-1(i*) ∈ Mi,π(i) = j, z]p[π-1(i*) & M-∣π(i) = 7,z]
/1 + r2 - 1	r1 - 1	+ (1 -
D - j - bo /1 + /2 - 1
a — ri
/1 + /2 - 1)
D - j - bo f - /1 - /2
r1 - 1	+ (1 _ r1 + r2 - 1) J*
D - j - b0 +( - D - j - b0 )
)J1.
(33)
,(I -
,(I -
)
)
Combining (32) and (33), we obtain when i ∈ B1 and j < i*,
Γ(i, j) = X l{j# ∈ Bp}(1 - -^-f-)(1 - -^+f-)P1 J1.
M	n-,Pj)	n+,1j)
(34)
Case 1b) j = i*. Similarly approach also applies to the situation with j = i*. In this case,
P [π-1(j) ∈ B3∣z] =(1 -	)P1,	P[π-1 (i*) ∈ & ∣π(i) = j, Z] = 1.	(35)
n-,1(j)
The equations are because (i*)# = i ∈ B1, and equivalently, π-1 (i*) = π-1(j) = i ∈ B1.
Case 1c) j > i*. I this case, we still have
3
P[π(i)=j∣Z = P1 X l{j# ∈Bp}(1 - --^pf-),
p=1	n-,pj)
but the probability ofπ-1(i*) being “O” is different. Since j > i*, this event now depends on z-,p,
p = 1,2, 3. More specifically,
P[π-1(i*) ∈B1∣π(i)= j,Z]
=1{7,* ∈B1}(1 ——⅛π)+ X l{j# ∈Bp}(1 - ʒp^) J*.
[	n-,1j)-1	p=23	n-,pj) _|
Therefore, when i ∈ B1 and j > j*, it holds that
Γ(i,j) = (1-	) l{j# ∈B1}(1 ——⅛π)+ X l{j# ∈Bp}(1 - ʒp^) J*
n-,1j) [	n-,1j) - 1	P或3	n-,pj)
'	(36)
2)	Xi ∈ B2. The case where Xi ∈ B can be analyzed using similar arguments. For conciseness, we
mainly present the final results.
Case 2a) j < i*. The calculation ends up in the same form. We have
3
P[π(i)=j∣Z = P2 X l{j# ∈Bp}(1 -	),
L	」	p=1	n-,p(j)
26
Under review as a conference paper at ICLR 2022
with P2 = Pi. In addition,
P[∏τ(z*) ∈ Bι∣π(i) = j,Z] =(1 -	)J2,
n+,2j )
where J2 = DJjI-M + (1 - D+；--； )J*. Hence, when Xi ∈ & and j < i*, we have
3
Γ(i,j ) = X i{j#
p=1
∈Bp1(1 - n⅛)(1 - A⅛)p2 j2∙
(37)
Case 2b) j = i*. In this case, Γ(i, j) simply equals to 0, since π-1(i*) = π-1(j) ∈ &. The
probability of π-1(i*) being “O” is 0.
Case 2c) j > i*. Omitting the details, we have
r(i,j)=(I- ⅛⅛
)l(j* ∈B2}(1-
1 ) + X l{j# ∈ Bp}(1 - -2ɪ
n-,2j) - 1	p=1,3	n-,pj)
)J*.
(38)
2) Xi ∈ B3.
Case 3a) j < i*. The expression is different from previous two, in that we need π-1(j) ∈ B3.
1(j) ∈ B3∣Z]
P[π(i) = j∣z] = P[π(i)= j∣π-1(j) ∈ B3,z]p[π-
3
=P3 X 1口# ∈bj y,
p=1	n-,pj
where
P3
1 ( bo ∖(D-j-ba∖
1	；3 -1	rɪ +；2
Moreover, we have
P [π-1(i*) ∈Bi∣π(i)= j,Z] =(1-
z+,3
n+,3(j )
)J3,
with J = D-1-bo + (1 - D-+-bo) J*. Combining parts together we obtain
3
μj) = X1{j# ∈Bpj ≤fe(I- n≡))p3j3.
Case 3b) j = i*. By same reasoning as Case 2a), Γ(i, j) = 0.
Case 3c) j > i*. We have in this case
Γ(i,j)
l(j# ∈ &}	z一，3	n-，3(j)	- z-，3	+ (1____z-，3 )	χ w# ∈ B }	z-,p
Ij ∈	3]	ι	+ (1	/ ∙x )	Ij ∈ Pj / ∙x
n-,3j)	n-,3(j) - 1	n-,3(j) p=1,2	n-,p(j)
J *
3
X l{j# ∈ Bp]-^-f-(1 - z-，3- {二=: - l{p = 3])J*.
p=1	n-,p(j)	n-,3(j)
(39)
(40)
Finally, combining (30), (34), (35), (36), (37), (38), (39) and (40) and re-organizing terms, the proof
is complete.
□
27
Under review as a conference paper at ICLR 2022
B	MORE NUMERICAL JUSTIFICATION ON C-MINHASH-(π, π)
The “Words” dataset (Li & Church, 2005) (which is publicly available) contains a large number of
word vectors, with the i-th entry indicating whether this word appears in the i-th document, for a
total of D = 216 documents. The key statistics of the 120 selected word pairs are presented in
Table 1. Those 120 pairs of words are more or less randomly selected except that we make sure
they cover a wide spectrum of data distributions. Denote d as the number of non-zero entries in the
vector. Table 1 reports the density d = d/D for each word vector, ranging from 0.0006 to 0.6. The
Jaccard similarity J ranges from 0.002 to 0.95.
In Figures 9 - 16, We plot the empirical MSE along with the empirical bias2 for J∏,∏, as well as
the empirical MSE for Jσ,∏. Note that for D this large, it is numerically difficult to evaluate the
theoretical variance formulas. From the results in the Figures, we can observe
•	For all the data pairs, the MSE of C-MinHash-(π, π) estimator overlaps with the empirical
MSE of C-MinHash-(σ, π) estimator for all K from 1 up to 4096.
•	The bias2 is several orders of magnitudes smaller than the MSE, in all data pairs. This
verifies that the bias of J∏,∏ is extremely small in practice and can be safely neglected.
We have many more plots on more data pairs. Nevertheless, we believe the current set of experiments
on this “Words” dataset should be sufficient to verify that, the proposed C-MinHash-(π, π) could
give indistinguishable Jaccard estimation accuracy in practice compared with C-MinHash-(σ, π).
28
Under review as a conference paper at ICLR 2022
Table 1: 120 selected word pairs from the Words dataset (Li & Church, 2005). For each pair, we
report the density d (number of non-zero entries divided by D = 216) for each word as well as the
ɪ	1∙	∙1	∙, Ttγa,1T' IT	∙ 1	f 1
Jaccard similarity J. Both d and J cover a wide range of values.
			J
ABOUT - INTO	0.302	0.125	0.258
ABOUT - LIKE	0.302	0.140	0.281
ACTUAL - DEVELOPED	0.017	0.030	0.071
ACTUAL - GRABBED	0.017	0.002	0.016
AFTER - OR	0.103	0.356	0.220
AND - PROBLEM	0.554	0.044	0.070
AS - NAME	0.280	0.144	0.204
AT - CUT	0.374	0.242	0.052
BE - ONE	0.323	0.221	0.403
BEST - AND	0.136	0.554	0.228
BRAZIL - OH	0.010	0.031	0.019
BUT - MANY	0.167	0.116	0.340
CALLED - BUSINESSES	0.016	0.018	0.043
CALORIES - MICROSOFT	0.002	0.045	0.0003
CAN - FROM	0.243	0.326	0.444
CAN - SEARCH	0.243	0.214	0.237
COMMITTED - PRODUCTIVE	0.013	0.004	0.029
CONTEMPORARY - FLASH	0.011	0.021	0.013
CONVENIENTLY - INDUSTRIES	0.003	0.011	0.009
COPYRIGHT - AN	0.218	0.290	0.209
CREDIT - CARD	0.046	0.041	0.285
DE - WEB	0.117	0.194	0.091
DO - GOOD	0.174	0.102	0.276
EARTH - GROUPS	0.021	0.035	0.056
EXPRESSED - FRUSTRATED	0.010	0.002	0.024
FIND - HAS	0.144	0.228	0.214
FIND - SITE	0.144	0.275	0.212
FIXED - SPECIFIC	0.011	0.039	0.054
FLIGHT - TRANSPORTATION	0.011	0.018	0.040
FOUND - DE	0.136	0.117	0.039
FRANCISCO - SAN	0.025	0.049	0.476
GOOD - BACK	0.102	0.160	0.220
GROUPS - ORDERED	0.035	0.011	0.034
HAPPY - CONCEPT	0.029	0.013	0.054
HAVE - FIRST	0.267	0.151	0.320
HAVE - US	0.267	0.284	0.349
HILL - ASSURED	0.020	0.004	0.011
HOME - SYNTHESIS	0.365	0.002	0.003
HONG - KONG	0.014	0.014	0.925
HOSTED - DRUGS	0.016	0.013	0.013
INTERVIEWS - FOURTH	0.012	0.011	0.031
KANSAS - PROPERTY	0.017	0.045	0.052
KIRIBATI - GAMBIA	0.003	0.003	0.712
LAST - THIS	0.135	0.423	0.221
LEAST - ROMANCE	0.046	0.007	0.019
LIME - REGISTERED	0.002	0.030	0.004
LINKS - TAKE	0.191	0.105	0.134
LINKS - THAN	0.191	0.125	0.141
MAIL - AND	0.160	0.554	0.192
MAIL - BACK	0.160	0.160	0.132
MAKE - LIKE	0.141	0.140	0.297
MANAGING - LOCK	0.010	0.006	0.010
MANY - US	0.116	0.284	0.210
MASS - DREAM	0.016	0.017	0.048
MAY - HELP	0.184	0.156	0.206
MOST - HOME	0.141	0.365	0.207
NAME - IN	0.144	0.540	0.207
NEITHER - FIGURE	0.011	0.016	0.085
NET - SO	0.101	0.154	0.112
NEW - PLEASE	0.291	0.168	0.205
		~^2~	J
NEW - WEB	0.291	0.194	0.224
NEWS - LIKE	0.168	0.140	0.172
NO - WELL	0.220	0.120	0.244
NOT - IT	0.281	0.295	0.437
NOTORIOUSLY - LOCK	0.0006	0.006	0.004
OF - THEN	0.570	0.104	0.168
OF - WE	0.570	0.226	0.361
OPPORTUNITY - COUNTRIES	0.029	0.024	0.066
OUR - THAN	0.244	0.125	0.245
OVER - BACK	0.148	0.160	0.233
OVER - TWO	0.148	0.121	0.289
PEAK - SHOWS	0.006	0.033	0.026
PEOPLE - BY	0.121	0.425	0.228
PEOPLE - INFO	0.121	0.138	0.117
PICKS - BOOST	0.007	0.005	0.007
PLANET - REWARD	0.013	0.003	0.018
PLEASE - MAKE	0.168	0.141	0.195
PREFER - PUEDE	0.010	0.003	0.0001
PRIVACY - FOUND	0.126	0.136	0.053
PROSECUTION - MAXIMIZE	0.002	0.003	0.006
RECENTLY - INT	0.028	0.007	0.014
REPLY - ACHIEVE	0.013	0.012	0.023
RESERVED - BEEN	0.172	0.141	0.108
RIGHTS - FIND	0.187	0.144	0.166
RIGHTS - RESERVED	0.187	0.172	0.877
SCENE - ABOUT	0.012	0.301	0.029
SEE - ALSO	0.138	0.166	0.291
SEIZE - ANYTHING	0.0007	0.037	0.012
SHOULDERS - GORGEOUS	0.003	0.004	0.028
SICK - FELL	0.008	0.008	0.085
SITE - CELLULAR	0.275	0.006	0.010
SOLD - LIVE	0.018	0.064	0.055
SOLO - CLAIMS	0.010	0.012	0.007
SOON - ADVANCE	0.040	0.017	0.057
SPECIALIZES - ACTUAL	0.003	0.017	0.008
STATE - OF	0.101	0.570	0.165
STATES - UNITED	0.061	0.062	0.591
TATTOO - JEANS	0.002	0.004	0.035
THAT - ALSO	0.301	0.166	0.376
THIS - CITY	0.423	0.123	0.132
THEIR - SUPPORT	0.165	0.117	0.189
THEIR - VIEW	0.165	0.103	0.151
THEM - OF	0.112	0.570	0.187
THEN - NEW	0.104	0.291	0.192
THINKS - LOT	0.007	0.040	0.079
TIME - OUT	0.189	0.191	0.366
TIME - WELL	0.189	0.120	0.299
TOP - AS	0.140	0.280	0.217
TOP - COPYRIGHT	0.140	0.218	0.149
TOP - NEWS	0.140	0.168	0.192
UP - AND	0.200	0.554	0.334
UP - HAS	0.200	0.228	0.312
US - BE	0.284	0.323	0.335
VIEW - IN	0.103	0.540	0.153
VIEW - PEOPLE	0.103	0.121	0.138
WALKED - ANTIVIRUS	0.006	0.002	0.002
WEB - GO	0.194	0.111	0.138
WELL - INFO	0.120	0.138	0.110
WELL - NEWS	0.120	0.168	0.161
WEEKS - LONDON	0.028	0.032	0.050
29
Under review as a conference paper at ICLR 2022
K
ω
ω
101
ω
工
K	K	K
Figure 9: Empirical MSEs of C-MinHash-(π, π) (“1 Perm”, red, solid) vs. C-MinHash-(σ, π) (“2
Perm”, blue, dashed) on various data pairs from the Words dataset. We also report the empirical
bias2 for C-MinHash-(π, π) to show that the bias is so small that it can be safely neglected. The
empirical MSE curves for both estimators essentially overlap for all data pairs.
k
K

K

k
30
Under review as a conference paper at ICLR 2022
ω
K
ω
工
101
k

—2 Perm
---1 Perm
...Bias2
CREDIT - CARD
ω
ω
工
101
K	K	K
Figure 10: Empirical MSEs of C-MinHash-(π, π) (“1 Perm”, red, solid) vs. C-MinHash-(σ, π) (“2
Perm”, blue, dashed) on various data pairs from the Words dataset. We also report the empirical
bias2 for C-MinHash-(π, π) to show that the bias is so small that it can be safely neglected. The
empirical MSE curves for both estimators essentially overlap for all data pairs.
31
Under review as a conference paper at ICLR 2022
ω
工
ω
工
ω
ω
101
101
101
101
101
101
k
k
K
ω
工
Figure 11: Empirical MSEs of C-MinHash-(π, π) (“1 Perm”, red, solid) vs. C-MinHash-(σ, π) (“2
Perm”, blue, dashed) on various data pairs from the Words dataset. We also report the empirical
bias2 for C-MinHash-(π, π) to show that the bias is so small that it can be safely neglected. The
empirical MSE curves for both estimators essentially overlap for all data pairs.
101
101
K
ω
工
101
≡2 Ferm
1 Perm
Bias2


HONG - KONG

l——2 Perm
---1 Perm
KANSAS - PROPERT
ω
ω
32
Under review as a conference paper at ICLR 2022
10o	101	102	103	104
10o	101	102	103	104
10o	101	102	103	104
LU
ω
工
K
K
LU
ω
工
10o	101	102	103	104
K
10o	101	102	103	104
10o	101	102	103	104
10o	101	102	103	104
LU
ω
工
K
10o	101	102	103	104
K
K
10o	101	102	103	104
K
K
10o	101	102	103	104
k	K	K
Figure 12: Empirical MSEs of C-MinHash-(π, π) (“1 Perm”, red, solid) vs. C-MinHash-(σ, π) (“2
Perm”, blue, dashed) on various data pairs from the Words dataset. We also report the empirical
bias2 for C-MinHash-(π, π) to show that the bias is so small that it can be safely neglected. The
empirical MSE curves for both estimators essentially overlap for all data pairs.
33
Under review as a conference paper at ICLR 2022
S
工
S
ω
K
ω
工
k	k	k
Figure 13: Empirical MSEs of C-MinHash-(π, π) (“1 Perm”, red, solid) vs. C-MinHash-(σ, π) (“2
Perm”, blue, dashed) on various data pairs from the Words dataset. We also report the empirical
bias2 for C-MinHash-(π, π) to show that the bias is so small that it can be safely neglected. The
empirical MSE curves for both estimators essentially overlap for all data pairs.
K
ω
工
101
K
101
S
103	104
34
Under review as a conference paper at ICLR 2022
103
104
ω
ω
工
ω
101
101
101
PROSECUTION - MA
101
101
SCENE - ABOUT
10o
101
K
ω
工
ω
工
LU
ω
工
102
K
IO2
K
Figure 14: Empirical MSEs of C-MinHash-(π, π) (“1 Perm”, red, solid) vs. C-MinHash-(σ, π) (“2
Perm”, blue, dashed) on various data pairs from the Words dataset. We also report the empirical
bias2 for C-MinHash-(π, π) to show that the bias is so small that it can be safely neglected. The
empirical MSE curves for both estimators essentially overlap for all data pairs.
101
IO1
---2 Perm
---1 Perm
...Bias2
10-6-7'W、
101
101
K
PLEASE - MAKE
W
10-4[ RIGHTS - RESERVED''^'∙∖
---2 Perm
---1 Perm
...Bias2
∙*⅛⅛rΛ
j⅛V
ω
102
K
101
102
K
I--2 Perm
---1 Perm

103
---2 Perm
---1 Perm

K

35
Under review as a conference paper at ICLR 2022
K	K	K
10o	101	102	103	104
10o	101	102	103	104
10o	101	102	103	104
K
K
K
10o	101	102	103	104
K
K
10o	101	102	103	104
10o	101	102	103	104
10o	101	102	103	104
K	K
K	K	K
Figure 15: Empirical MSEs of C-MinHash-(π, π) (“1 Perm”, red, solid) vs. C-MinHash-(σ, π) (“2
Perm”, blue, dashed) on various data pairs from the Words dataset. We also report the empirical
bias2 for C-MinHash-(π, π) to show that the bias is so small that it can be safely neglected. The
empirical MSE curves for both estimators essentially overlap for all data pairs.
36
Under review as a conference paper at ICLR 2022
10o
ω
工
ω
ω
101
101
UP - AND
101
101
WEB - GO
101
k
k
K
ω
工
5
ω
Figure 16: Empirical MSEs of C-MinHash-(π, π) (“1 Perm”, red, solid) vs. C-MinHash-(σ, π) (“2
Perm”, blue, dashed) on various data pairs from the Words dataset. We also report the empirical
bias2 for C-MinHash-(π, π) to show that the bias is so small that it can be safely neglected. The
empirical MSE curves for both estimators essentially overlap for all data pairs.
101
io1
101
W
K
W
101
TOP - NEWS
---2 Ferm
---1 Perm
...Bias2
---2 Perm
---1 Perm
...Bias
S
- 1≡⅛Λ1λ
ω
K
---2 Perm
---1 Perm
...Bias2
37