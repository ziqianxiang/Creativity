Under review as a conference paper at ICLR 2022
Provab le Federated Adversarial Learning via
Min-max Optimization
Anonymous authors
Paper under double-blind review
Ab stract
Federated learning (FL) is a trending training paradigm to utilize decentralized
training data. FL allows clients to update model parameters locally for several
epochs, then share them to a global model for aggregation. This training paradigm
with multi-local step updating before aggregation exposes unique vulnerabilities
to adversarial attacks. Adversarial training is a trending method to improve the
robustness of neural networks against adversarial perturbations. First, we formu-
late a general form of federated adversarial learning (FAL) that is adapted from
adversarial learning in the centralized setting. On the client side of FL training,
FAL has an inner loop to optimize an adversarial to generate adversarial samples
for adversarial training and an outer loop to update local model parameters. On
the server side, FAL aggregates local model updates and broadcast the aggregated
model. We design a global training loss to formulate FAL training as a min-max
optimization problem. Unlike the convergence analysis in centralized training that
relies on the gradient direction, itis significantly harder to analyze the convergence
in FAL for three reasons: 1) the complexity of min-max optimization, 2) model
not updating in the gradient direction due to the multi-local updates on the client-
side before aggregation and 3) inter-client heterogeneity. Further, we address the
challenges using appropriate gradient approximation and coupling techniques and
present the convergence analysis in the over-parameterized regime. Our main
result theoretically shows that the minimal value of loss function under this al-
gorithm can converge to small with chosen learning rate and communication
rounds. It is noteworthy that our analysis is feasible for non-IID clients.
1	Introduction
Federated learning (FL) is playing an important role nowadays, as it allows different parties or clients
to collaboratively train deep learning models without sharing private data. One popular FL paradigm
called FedAvg (McMahan et al., 2017) introduces an easy-to-implement distributed learning method
without data sharing. Specifically, it requires a central server to aggregate model updates computed
by the local participants (also known as nodes or clients) using local imparticipable private data.
Then the central server aggregates these updates to train a globally learned model.
Nowadays deep learning model are exposed to severe threats of adversarial samples. Namely, small
adversarial perturbations on the inputs will dramatically change the outputs or output wrong an-
swers (Szegedy et al., 2013). In this regard, much effort has been made to improve neural networks’
resistance to such perturbations using adversarial learning (Tramer et al., 2017; Samangouei et al.,
2018; Madry et al., 2018). Among these studies, the adversarial training scheme in Madry et al.
(2018) has achieved the good robustness in practice. Madry et al. (2018) proposes an adversarial
training scheme that uses projected gradient descent (PGD) to generate alternative adversarial sam-
ples as the augmented training set. Generating adversarial examples during neural network training
is considered as one of the most effective approaches for adversarial training up to now according to
the literature (Carlini & Wagner, 2017; Athalye et al., 2018; Croce & Hein, 2020).
Although adversarial learning has attracted much attention in the centralized domain, its practice
in FL is under-explored (Zizzo et al., 2020). Like training classical deep neural networks that use
gradient-based methods, FL paradigms are vulnerable to adversarial samples. Adversarial learning
in FL brings multiple open challenges due to FL properties on low convergence rate, application
1
Under review as a conference paper at ICLR 2022
in non-IID environments, and secure aggregation solutions. Hence applying adversarial learning
in an FL paradigm may lead to unstable training loss and a lack of robustness. However, a recent
practical work Zizzo et al. (2020) observed that although there exist difficulties of convergence,
the federation of adversarial training with suitable hyperparameter settings can achieve adversarial
robustness and acceptable performance. Motivated by the empirical results, we want to address the
provable property of combining adversarial learning into FL from the theoretical perspective.
This work aims to theoretically study the unexplored convergence challenges that lie in the interac-
tion between adversarial training and FL. To achieve a general understanding, we consider a general
form of federated adversarial learning (FAL), which deploys adversarial training scheme on local
clients in the most common FL paradigm, FedAvg (McMahan et al., 2017) system. Specifically,
FAL has an inner loop of local updating that generates adversarial samples (i.e., using Madry et al.
(2018)) for adversarial training and an outer loop to update local model weights on the client side.
Then global model is aggregated using FedAvg (McMahan et al., 2017). The algorithm is detailed
in Algorithm 1.
We are interested in theoretically understanding the proposed FAL scheme from the aspects of model
robustness and convergence:
Can federated adversarial learning fit training data robustly and converge with a feasibly sized
neural network?
The theoretical convergence analysis of adversarial training itself is challenging in the centralized
training setting. Tu et al. (2018) recently proposed a general theoretical method to analyze the risk
bound with adversaries but did not address the convergence problem. The investigation of conver-
gence on over-parameterized neural network has achieved tremendous progress (Du et al., 2019a;
Allen-Zhu et al., 2019b;c; Du et al., 2019b; Arora et al., 2019b). The basic statement is that training
can converge to sufficiently small training loss in polynomial iterations using gradient descent or
stochastic gradient descent when the width of the network is polynomial in the number of training
examples when initialized randomly. Recent theoretical analysis (Gao et al., 2019; Zhang et al.,
2020b) extends these standard training convergence results to adversarial training settings. To an-
swer the above interesting but challenging question, we formulate FAL as an min-max optimization
problem. We extend the convergence analysis on the general formulation of over-parameterized
neural networks in the FL setting that allows each client to perform min-max training and generate
adversarial examples (see Algorithm 1). Involved challenges are arising in FL convergence analysis
due to its unique optimization method: 1) unlike classical centralized setting, the global model of FL
does not update in the gradient direction; 2) inter-client heterogeneity issue needs to be considered.
Despite the challenges, we give an affirmative answer to the above question. To the best of our
knowledge, this work is the first theoretical study that examines these unexplored challenges on the
convergence of adversarial training with FL. The contributions of this paper are:
•	We propose a framework to analyze a general form of FAL in over-parameterized neural
networks. We follow a natural and valid assumption of data separability that the training
dataset are well separated apropos of the adversarial perturbations’ magnitude. After suffi-
cient rounds of global communication and certain steps of local gradient descent for each
t, we obtain the minimal loss close to zero. Notably, our assumptions do not rely on data
distribution. Thus the proposed analysis framework is feasible for non-IID clients.
•	We are the first to theoretically formulate the convergence of the FAL problem into a min-
max optimization framework with the proposed loss function. In FL, the update in the
global model is no longer directly determined by the gradient directions due to multiple
local steps. To tackle the challenges, we define a new ‘gradient’, FL gradient. With valid
ReLU Lipschitz and over-parameterized assumptions, we use gradient coupling for gra-
dient updates in FL to show the model updates of each global updating is bounded in
federated adversarial learning.
Roadmap The rest of the paper is organized as follows. We overview the related work on federated
learning and adversarial training in Section 2. We describe the problem formulation for the federated
adversarial learning algorithm and introduce the notations, assumptions to be used, and conditions
to be considered to ensure convergence in Section 3. Our main convergence result is presented in
2
Under review as a conference paper at ICLR 2022
Section 4. In Section 5, we present the core techniques and an overview of the proof. We summarize
and conclude our results in Section 6.
2	Related Work
Federated Learning A efficient and privacy-preserving way to learn from the distributed data
collected on the edge devices (a.k.a clients) would be FL. FedAvg is a easy-to-implement distributed
learning strategy by aggregating local model updates on the server side and transmitting the averaged
model back to local clients. Later, many FL methods are developed baed on FedAvg. Theses FL
schemes can be divided into aggregation schemes (McMahan et al., 2017; Wang et al., 2020; Li et al.,
2021) and optimization schemes (Reddi et al., 2020; Zhang et al., 2020a). Nearly all the them have
the common characteristics that client model are updating using gradient descent-based methods,
which is venerable to adversarial attacks. In addition, data heterogeneity brings in huge challeng
in FL. For IID data, FL has been proven effective. However, in practice, data mostly distribute as
non-IID. Non-IID data could substantially degrade the performance ofFL models (Zhao et al., 2018;
Li et al., 2019; 2021; 2020a). Despite the potential risk in security and unstable performance in non-
IID setting, as FL mitigates the concern of data sharing, it is still a popular and practical solution for
distributed data learning in many real applications, such as healthcare (Li et al., 2020b; Rieke et al.,
2020), autonomous driving (Liang et al., 2019), IoTs (Wang et al., 2019; Lim et al., 2020).
Learning with Adversaries Since the discovery of adversarial examples (Szegedy et al., 2013), to
make neural networks robust to perturbations, efforts have been made to propose more effective de-
fense methods. As adversarial examples are an issue of robustness, the popular scheme is to include
learning with adversarial examples, which can be traced back to (Goodfellow et al., 2014). It pro-
duces adversarial examples and injecting them into training data. Later, Madry et al. (Madry et al.,
2018) proposed training on multi-step PGD adversaries and empirically observed that adversarial
training consistently achieves small and robust training loss in wide neural networks.
Federated Adversarial Learning Adversarial examples, which may not be visually distinguish-
able from benign samples, are often classified. This poses potential security threats for practical
machine learning applications. Adversarial training (Goodfellow et al., 2014; Kurakin et al., 2016)
is a popular protocol to train more adversarial robust models by inserting adversarial examples in
training. The use of adversarial training in FL presents a number of open challenges, including
poor convergence due to multiple local update steps, instability and heterogeneity of clients, cost
and security request of communication, and so on. To defend the adversarial attacks in federated
learning, limited recent studies have proposed to include adversarial training on clients in the lo-
cal training steps (Bhagoji et al., 2019; Zizzo et al., 2020). These two works empirically showed
the performance of adversarial training. The theoretical analysis of convergence is under explored.
In addition, (Zhang et al., 2021) proposed an adversarial training strategy in classical distributed
setting, not meeting the specialty in FL.
Convergence via Over-parameterization Convergence analysis on over-parameterized neural
networks falls in two lines. In the first line of work (Li & Liang, 2018; Allen-Zhu et al., 2019b;c;a),
data separability plays a crucial role in deep learning theory, especially in showing the convergence
result of over-parameterized neural network training. Denote δ as the minimum distance between all
pairs of data points. Data separability theory requires the width (m) of the neural network is at least
polynomial factor of all the parameters (i.e. m ≥ poly(n, d, 1∕δ)), where n is the number of data
points and d is the dimension of data. Another line of work (Du et al., 2019b; Arora et al., 2019a;b;
Song & Yang, 2019; Lee et al., 2020) builds on neural tangent kernel (Jacot et al., 2018). It requires
the minimum eigenvalue (λ) of the neural tangent kernel to be lower bounded. Our analysis focuses
on the former approach based on data separability.
Robustness of Federated Learning Previously there were several works that theoretically ana-
lyzed the robustness of federated learning under noise. Yin et al. (2018) developed distributed opti-
mization algorithms that were provably robust against arbitrary and potentially adversarial behavior
in distributed computing systems, and mainly focused on achieving optimal statistical performance.
Reisizadeh et al. (2020) developed a robust federated learning algorithm by considering a structured
3
Under review as a conference paper at ICLR 2022
affine distribution shift in users’ data. Their analysis was built on several assumptions on the loss
functions without a direct connection to neural network.
3	Problem Formulation
To explore the properties of FAL in deep learning, we formulate the problem in over-parameterized
neural network regime. We start by presenting the notations and setup required for federated adver-
sarial learning, then we will describe the loss function we use and our FAL algorithm.
The rest of this section is organized as follows. In Section 3.1 we introduce the notations to be used.
In Section 3.2 we state the assumptions on the dataset and initialization values. In Section 3.3 we
formally describe our FAL algorithm (Algorithm 1) to be investigated.
3.1	Notations
For a vector x, we use kxkp to denote its `p norm, in this paper we mainly consider the situation
when p = 1, 2, or ∞. For a matrix W ∈ Rd×m, we use W> to denote its transpose and use tr[W] to
denote its trace. We use kW k1, kW k2 and kW kF to denote the entry-wise `1 norm, spectral norm
and Frobenius norm of W respectively. For each j ∈ [m], we let Wj ∈ Rd be the j-th column of W.
We let kW k2,1 denotes Pjm=1 kWj k2 and kW k2,∞ denotes maxj∈[m] kWj k2. We denote Gaussian
distribution with mean μ and covariance Σ as N(μ, Σ). We use σ(∙) to denote the ReLU function
σ(z) = max{z, 0}, and use 1{E} to denote the indicator function of event E.
3.2	Problem Setup
Two-layer ReLU network in FAL Following recent theoretical work in understanding neural
networks training in deep learning (Du et al., 2019b; Arora et al., 2019a;b; Song & Yang, 2019; Lee
et al., 2020), in this paper, we focus on a two-layer neural network that has m neurons in the hidden
layer, where each neuron is a ReLU activation function. We define the global network as
m
fu(x):= Ear ∙ σ(hUr,xi + br)	(1)
r=1
and for c ∈ [N], we define the local network of client c as
m
fWc(x) := Ear ∙ σ(hWc,r,xi + br).	(2)
r=1
Here U = (U1, . . . , Um) ∈ Rd×m is the global hidden weight matrix, Wc = (Wc,1, . . . , Wc,m) ∈
Rd×m is the local hidden weight matrix of client c, and a = (a1 , . . . , am ) ∈ Rm denotes the output
weight vector, b = (b1 , . . . , bm ) ∈ Rm denotes the bias vector. During the process of federated
adversarial learning, we only update U and W, keeping a and b equal to their initialization values,
so we can write the global network as fU (x) and the local network as fWc (x). For the situation we
don’t care about the weight matrix, we write f(x) or fc(x) for short.
Next, we make some standard assumptions regarding our training set.
Definition 3.1 (Dataset). There are N clients and n = NJ data in total.1 Let S = ∪c∈[N] Sc where
Sc = {(xc,1, yc,1), ..., (xc,J, yc,J)} ⊆ Rd × R denotes the J training data of client c. Without loss
of generality, we assume that for all c ∈ [N], j ∈ [J], kxc,j k2 = 1 and the last coordinate of each
point equals to 1/2 , so we consider X := {x ∈ Rd : kxk2 = 1, xd = 1/2}. For simplicity, we also
assume that for all c ∈ [N], j ∈ [J], |yc,j | ≤ 1.2
We now define the initialization for the neural networks.
1Without loss of generality, we assume that all clients have same number of training data. Our result can be
generalized to the setting where each client has a different number of data as the future work.
2Our assumptions on data points are reasonable since we can do scale-up. In addition, l2 norm normalization
is a typical technique in experiments. Same assumptions also appears in many previous theoretical works like
Arora et al. (2019b); Allen-Zhu et al. (2019a;b).
4
Under review as a conference paper at ICLR 2022
Definition 3.2 (Initialization). The initialization of a ∈ Rm , U ∈ Rd×m , b ∈ Rm is a(0) ∈
Rm, U(0) ∈ Rd×m, b(0) ∈ Rm. The initialization of client c’s local weight matrix Wc is
Wc(0, 0) = U (0). Here the second term in Wc denotes iteration of local steps.
•	For each r ∈ [m], ar (0) are i.i.d. sampled from [-1/m1/3, +1/m1/3] uniformly.
•	For each i ∈ [d], r ∈ [m], Ui,r (0) and br (0) are i.i.d. random Gaussians sampled from
N (0, 1/m). Here Ui,r means the (i, r)-entry of U.
For each global iteration t ∈ [T],
•	For each c ∈ [N], the initial value of client c’s local weight matrix Wc is Wc(t, 0) = U (t).
Next we formulate the adversary model that will be used.
Definition 3.3 (ρ-Bounded adversary). Let F denote the function class. An adversary is a mapping
A : F × X × R → X which denotes the adversarial perturbation. For ρ > 0, we define the `2
ball as B2 (x, ρ) := {xe ∈ Rd : kxe - xk2 ≤ ρ} ∩ X, we say an adversary A is ρ-bounded if it
satisfies A(f, x, y) ∈ B2 (x, ρ). Moreover, given ρ > 0, we denote the worst-case adversary as
A* ：= argmaχe∈B2(χ,ρ) '(f (e),y), where ' is loSSfUnCtiOn defined in Definition 3.5.
Well-separated training sets In the over-parameterized regime, it is a standard assumption that
the training set is well-separated. Since we deal with adversarial perturbations, we require the fol-
lowing γ-separability, which is a bit stronger.
Definition 3.4 (γ -separability). Let γ ∈ (0, 1/2), δ ∈ (0, 1/2), ρ ∈ (0, 1/2) denote three parame-
ters such that Y ≤ δ ∙ (δ — 2ρ). We say our training set S = ∪c∈[n]Sc = ∪c∈[N],j∈[j]{(xc,j, Vc,j)} ⊂
Rd × R is globally γ-separable w.r.t a ρ-bounded adversary, ifkxc1,j1 - xc2,j2 k2 ≥ δ holds for any
c1 6= c2 and j1 6= j2.
It is worth noting that, our problem setup does not require the assumption on independent and
identically distribution (IID) on data, thus such a formation can be applied to unique challenge of
the non-IID setting in FL.
3.3	Federated Adversarial Learning
Algorithm We focus on a general FAL framework that is adapted from the most common adver-
sarial training in the classical setting on the client. Specifically, we describe the adversarial learning
of a local neural network fWc against an adversary A that generate adversarial examples during
training as shown in Algorithm 1. As for the analysis of a general theoretical analysis framework,
we do not specify the explicit format of A.
The FAL algorithm contains two procedures: one is ClientUpdate running on client side and the
other is ServerExecution running on server side. These two procedures are iteratively processed
through communication iterations. Adversarial training is addressed in procedure ClientUpdate.
Hence, there are two loops in ClientUpdate procedure: the outer loop is iteration for local model
updating; and the inner loop is iteratively generating adversarial samples by the adversary A. In
the outer loop in ServerExecution procedure, the neural network’s parameters are updated to
reduce its prediction loss on the new adversarial samples.
Adversary and robust loss We set the following loss for the sake of technical presentation sim-
plicity, as is customary in prior studies Gao et al. (2019); Allen-Zhu et al. (2019a):
Definition 3.5 (Lipschitz convex loss). A loss function ` : R × R → R is said to be a Lipschitz
convex loss, if it satisfies the following four properties:
•	non-negative;
•	convex with respect to the first input of `;
•	1-Lipshcitz, which means k'(χι,yι) - '(χ2,y2)∣∣2 ≤ k(x1,y1) - (x2,y2)k2;
•	'(y, y) = 0 for all y ∈ R.
5
Under review as a conference paper at ICLR 2022
Algorithm 1 Federated Adversarial Learning (FAL)
Notations: Training sets of clients with each client is indexed by c, Sc = {(xc,j, yc,j)}jJ=1; adver-
sary A; local learning rate ηlocal; global learning rate ηglobal ; local updating iterations K; global
communication round T .
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
Initialization a(0) ∈ Rm,U(0) ∈ Rd×m, b(0) ∈ Rm
For t = 0 → T , we iteratively run Procedure A then Procedure B
procedure A . CLIENTUPDATE(t, c)
Sc(t) - 0
Wc(t, 0) J U (t)	. Receive global model weights update.
for k = 0 → K - 1 do
for j = 1 → J do
xe(ct,j) J A(fWc (t,k), xc,j, yc,j)	. Adversarial samples. fWc is defined as (2).
Sc(t)JSc(t)∪(xe(ct,j),yc,j)
end for
Wc(t, k +1) J Wc(t, k) - ηiocal ∙ VWcL(fWc(t,k), Sc(t))
end for
∆Uc(t) J Wc(t,K) -U(t)
Send ∆Uc(t) to SERVEREXECUTION
end procedure
procedure B . SERVEREXECUTION(t):
for each client c in parallel do do
∆Uc(t) J CLIENTUPDATE(c, t)	. Receive local model weights update.
∆U(t) J N Pc∈[N]∆Uc(t)
U(t + 1) J U(t) + ngiobai ∙ ∆U(t)	. Aggregation on the server side.
Send U(t + 1) to client c for CLIENTUPDATE(c, t)
end for
end procedure
In this paper we assume ` is a Lipschitz convex loss. Next, we define our robust loss function of a
neural network, which is based on the adversarial examples generated by a ρ-bounded adversary A.
Definition 3.6 (Training loss). Given a client’s training set Sc = {(xc,j , yc,j )}jJ=1 ⊂ Rd × R of
J examples, the standard training loss of a neural net fc : Rd → R is defined as L(fc, Sc) :=
1 PJ=I'(fc(xcj),ycj)∙ Given S = ∪c∈N]Sc, we define the global loss as L(fu, S):=
N1j PN=I PJ=I '(fu(XCj), ycj). Given a P-bounded adversary A, we define the global loss with
respect to A as
NJ	NJ
LA(fu) := NnXX'(fu(A(fc,χcj,ycj)),ycj) = NnXX'(fu((XCj),ycj)
c=1 j=1	c=1 j=1
and also define the worst-case global robust loss as
1 NJ	1 NJ
LA*(fu):= Try XX'(fu(A*(fc,xcj,ycj)),ycj) = 777 XX * UmaX	J(fu(XCj),ycj).
nj c=1 j=1	nj c=1 j=1 x*,j∈B2(xc,j ,p)
Moreover, since we deal with pseudo-net which is defined in Definition 5.1, we also de-
fine the loss of a pseudo-net as L(gc, SC) := 1 PJ=I ' (gc(xcj), yc,j) and L(gu, S)：=
N PL Pj=ι '(gu (XCj ),yc,j).
4 Our Result
The main result of this work is showing the convergence of FAL algorithm (Algorithm 1) in overpa-
rameterized neural networks. Specifically, our defined global training loss (Definition 3.6) converges
to a small with the chosen communication round T , local and global learning rate ηlocal, ηglobal .
It is plausible to see that we can control ηlocal according to the local update steps K to achieve
convergence. We now formally present our main result Theorem 4.1.
6
Under review as a conference paper at ICLR 2022
Theorem 4.1 (Federated Adversarial Learning). Let c0 ∈ (0, 1) be a fixed constant. Let N denotes
the total number of clients and J denotes the number of data points per client. Suppose that our
training set S = ∪c∈[N] Sc is globally γ-separable for some γ > 0. Then, for all ∈ (0, 1),
there exists R = Poly((NJ/e)1/Y) that satisfies: for all m ≥ poly(d, (NJ/e)1/Y), for ^very
K ≥ 1 and T ≥ Poly(R/e), with probability at least 1 一 exp(-Ω(m1/3)) over the randomness
of a(0) ∈ Rm, U(0) ∈ Rd×m, b(0) ∈ Rm, running federated adversarial learning (Algorithm 1)
with step size choices ηglobal = 1/ Poly(N J, R, 1/e) and ηlocal = 1/K will output a list of weights
{U(1),U(2), ∙∙∙ , U(T)} ∈ Rd×m that satisfy
tm∈[iTn]LA(fU(t)) ≤ e.
5 Proof S ketch
To handle the min-max objective in FAL, we formulate the optimization of FAL in the framework
of online gradient descent3 : at each local step k on the client side, firstly the adversary generates
adversarial samples and computes the loss function L fWc(t,k), Sc(t) , then the local client learner
takes the fresh loss function and update Wc(t, k + 1) = Wc(t, k) 一 niocai ∙ RWcL (fwc(t,k), &(t)).
Compared with the centralized setting, the key difficulties in the convergence analysis of FL are
induced by multiple local updates on the client side and the updates on both local and global sides.
Specifically, local updates are not the standard gradient as the centralized adversarial training when
K ≥ 2. We used -∆U(t) in substitution of the real gradient of U to update the value of U(t). This
brings in challenges to bound the gradient of the neural networks. Nevertheless, gradient bounding
is challenging in adversarial training solely. To this end, we use gradient coupling method twice
to solve this core problem: firstly we bound the difference between real gradient and FL gradient
(defined below), then we bound the difference between pseudo gradient and real gradient.
5.1	Existence of small robust loss
In this section, we denote U = U(0) as the initialization of global weights U and denote U(t) as
the global weights of communication round t. U * is the value of U after small perturbations from
U which satisfies kU* 一 U k2,∞ ≤ R/mc1 , here c1 ∈ (0, 1) is a constant (e.g. c1 = 2/3), m is
the width of the neural network and R is a parameter. We will specify the concrete value of these
parameters later in appendix.
We study the over-parameterized neural nets’ well-approximated pseudo-network to learn gradient
descent for over-parameterized neural nets whose weights are close to initialization. Pseudo-network
can be seen as a linear approximation of our two layer ReLU neural network near initialization, and
the introducing of pseudo-network makes the proof more intuitive.
Definition 5.1 (Pseudo-network). Given weights U ∈ Rd×m, a ∈ Rm and b ∈ Rm, for a neural
network fu(x) = Em=I a ∙ σ(hU, Xi + br), we define the corresponding pseudo-network gu :
Rd → R as gu(x) := P>ι ατ ・〈外(t) 一 Ur(0),x〉∙1{(Ur(0),x〉+ b ≥ 0}.
Existence of small robust loss In order to obtain our main result, we first show that we can find
a U * which is close to U (0) and also makes L∕* (fu *) sufficiently small. Later in Theorem 5.6 We
show that the average of L∕(fu(t)) is dominated by La* (fu*), thus we can prove the minimum of
LA(fu(t)) is e small.
Theorem 5.2 (Existence, informal version of Theorem F.3). For all e ∈ (0, 1), there exists M0 =
poly(d, (NJ/e)1/Y) and R = Poly((N J/e)1/Y) such thatfor ^very m ≥ Mo, with high probability
there exists U* ∈ Rd×m that satisfies kU* 一 U (0)k2,∞ ≤ R/mc1 and LA* (fu* ) ≤ e.
5.2	Convergence result for federated learning
For ease of presentation, we first describe the notions of local and global gradients in our federated
adversarial learning setting.
3We refer our readers to Hazan (2016) for more details regarding online gradient descent.
7
Under review as a conference paper at ICLR 2022
Definition 5.3 (Gradient). For a local real network fwc(t,k), we denote its gradient by V(fc, t, k):=
VwcL(fwc(t,k), Sc(t))∙ Ifthe corresponding pseudo-network is gwc(t,k), then denote the pseudo-
network gradient by V(gc, t, k) := VWc L(gWc(t,k), Sc(t)).
Now we consider the global network. We define pseudo gradient as V(g, t) := VU L(gU (t) , S (t))
1
and define FL gradient as V(f, t) := 一 N∆U(t), which is used m the proof of Theorem 5.6. We
present our gradient coupling methods in the following two lemmas.
Lemma 5.4 (Bounding the difference between real gradient and FL gradient, informal version of
Lemma E.4). With probability at least 1 — exp(一Ω(mc0)) over the initialization, for iterations t
such that kU (t) 一 U (0)k2,∞ ≤ 1/o(m), the gradients satisfy kV(f, t) 一 V(f, t)k2,1 ≤ o(m).
Lemma 5.5 (Bounding the difference between pseudo gradient and real gradient, informal version
of Lemma E.5). With probability at least 1 — exp(一Ω(mc0)) over the initialization, for iterations t
such that ∣∣U(t) — U(0)∣∣2,∞ ≤ 1∕o(m), the gradients satisfy ∣∣V(g, t) — V(f, t)k2,1 . NJ ∙ o(m).
The above two lemmas are essential in proving Theorem 5.6, which is our convergence result.
Theorem 5.6 (Convergence result, informal version of Theorem E.3). For all ∈ (0, 1), R ≥ 1,
there exists an M = poly(n, R, 1∕), such that for every m ≥ M, for every K ≥ 1, for every
T ≥ Poly(R∕e), with probability at least 1 — exp(-Ω(mc0)) over the randomness of a(0) ∈ Rm,
U (0) ∈ Rd×m, b(0) ∈ Rm, for all U * such that ∣∣U * — U (0)∣2,∞ ≤ R∕mc1, running Algorithm 1
with setting ηglobal = 1∕ Poly(N J, R, 1∕e) and ηlocal = 1∕K will output weights (U (t))tT=1 that
SatiSfy 1 PT=ILA (fU(t)) ≤ LA* (fu*) + e.
In the proof of Theorem 5.6 we first bound the local gradient Vr(fc, t, k). We consider the
pseudo-network and bound L(gU(t), S(t)) — L(gU* , S(t)) ≤ α(t) + β(t) + γ(t), where α(t) :=
・ ~ . . . . . .. ~ . . .. .. .. . . ..
hV(f,t),U(t) — U*i,β(t) = ∣V(f,t) —V(f,t)k2,1 ∙∣∣U(t) — U*∣∣2,∞ and γ(t) := ∣V(g,t)—
V(f, t)k2,1 ∙ ∣∣U (t) — U *∣2,∞. In bounding α(t), We unfold ∣∣U (t +1) — U *∣F and by rearranging
we have
α(t) = ηg0bal ∣∆U (t)∣F +	∙ (∣U (t) — U * ∣F — kU (t + 1) — U *∣F).
2	2ηglobal
We bound ∣∆U (t)∣2F ≤ ηlocalK ∙ o(m). By doing summation over t, we have
T
Xα(t)
t=1
T
ηg2bal X ∣∆U (t)∣F +
t=1
1
2ηglobal
T
∙ X(∣u (t) - U *∣F - ku (t + 1) — U *∣F)
t=1
T1
≤ ηgl2bal X ∣δu (t)∣F + 2η- ∙ ∣u (1) — u *∣F
.ngiobainiocaiTK ∙ o(m) +-----1-mDU*
ηglobal
In bounding β(t), we apply Lemma 5.4 and have
β(t) = ∣V(f,t) — V (f,t)∣2,1 ∙∣U (t) — U *∣2,∞
.	o(m) ∙ ∣U(t) — U*∣2,∞
.	o(m) ∙ (∣U (t) — Ue ∣2,∞ + DU* ).
where DU* := ∣U* — U ∣2,∞ ≤ R∕mc1 . As for the first term, we bound
t
∣U (t) — Ue ∣2,∞ ≤ ηgiobai X ∣∆U (τ)∣2,∞
τ=1
t	N K-1
= ηgiobaiX ∣ ηNal XXV(fc, t, k)∣2,∞
τ=1	c=1 k=0
t N K-1
≤ ηgobNocai XXX ∣V(fc, t, k)∣2,∞
τ=1c=1 k=0
≤ ηgiobaiηiocaitKm
8
Under review as a conference paper at ICLR 2022
and have β(t) . ηgiobaiηiocaitK ∙ o(m) + o(m) ∙ DU*, then We do summation and obtain
T
X β(t) . ηgiobaiηiocaiT2K ∙ o(m) + o(m) ∙ TDU* .
t=1
In bounding γ(t), We apply Lemma 5.5 and have
.. .. . . ... .. ... . ............................................. ~.. .
γ(t) = kV(g,t) - V(f,t)k2,1 ∙kU (t) - U *k2,∞ . NJ ∙ o(m) ∙ (kU (t) - Uk2,∞ + DU *).
Then We do summation over t and have
T
X γ(t) . ηgiobaiηiocaiT2KNJ ∙ o(m) + TNJ ∙ o(m)Du*
t=1
Putting it together With our choice of our all parameters (i.e. ηlocal , ηglobal, R, K, T, m), We obtain
T	T	TTT
T XLlgU(t),S⑼-T XL(gu*,S(t)) ≤ "Xɑ(t) + Xβ(t) + Xγ(t)) ≤ O(e).
t=1
t=1
t=1	t=1	t=1
From Theorem D.2 in appendix, We have: supx∈X |fU (x) - gU (x)| ≤ O(), and thus,
1T	1T
T E L(fU(t),S(t)) - T E L(fU*, S(t)) ≤ O(e)∙	⑶
t=1	t=1
From the definition of A* We have Lf *, S(t)) ≤ La* f *). From the definition of loss We have
L(fU(t), S(t)) = LA(fU(t)). Moreover, since Eq. (3) holds for all e > 0, We can replace O(e) With
e. Thus We prove that for ∀e > 0,
1T
T	LA(fU(t)) ≤ LA*(fU*) + e∙
t=1
Combining the results From Theorem 5.2 We obtain U* that is close to U(0) and makes
LA* (fU* ) close to zero, from Theorem 5.6 We have that the average of LA(fU(t)) is dominated
by LA* (fU* ). By aggregating these tWo results, We prove that the minimal of LA(fU(t)) is e small
and finish the proof of our main Theorem 4.1. 6
6 Conclusion
We have studied the convergence of a general format of adopting adversarial training in FL setting
to improve FL training robustness. We propose the general frameWork, FAL, Which deploys adver-
sarial samples generation-based adversarial training method on the client-side and then aggregate
local model using FedAvg (McMahan et al., 2017). In FAL, each client is trained via min-max opti-
mization With inner loop adversarial generation and outer loop loss minimization. To the best of our
knoWledge, We are the first to present the comprehensive proof of theoretical convergence guarantee
for over-parameterized ReLU netWork on the presented FAL strategy, using gradient descent. Unlike
the convergence of adversarial training in classical settings, We consider the updates on both local
client and global server sides. Our result indicates that We can control learning rates ηlocal and ηglobal
according to the local update steps K and global communication round T to make the minimal loss
close to zero. The technical challenges lie in the multiple local update steps and heterogeneous data,
leading to the difficulties of convergence. Under ReLU Lipschitz and over-prameterization assump-
tions, We use gradient coupling methods tWice. Together, We shoW the model updates of each global
updating bounded in our federated adversarial learning. Note that We do not require IID assump-
tions for data distribution. In sum, the proposed FAL formulation and analysis frameWork can Well
handle the multi-local updates and non-IID data in FL. Moreover, our frameWork can be generalized
to other FL aggregation methods, such as sketching and selective aggregation.
9
Under review as a conference paper at ICLR 2022
Ethics S tatement
Ensuring the security and robustness of the deployed algorithms is of paramount importance in AI
algorithms nowadays. Recently, machine learning training has revealed its vulnerability to adver-
sarial attacks and tendency to generate wrong predictions. The tread cannot be underestimated in
FL, where the heterogeneity among clients and requirement for efficient communication brings in
challenges in stable gradient updates. In this regard, our work theoretically shows the feasibility of
FAL. As a theoretical work, we do not involve any human subjects or datasets. Our work has the
potential positive impact on the machine learning community.
Reproducibility S tatement
We provide the proof details in the appendix to ensure reproducibility.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameter-
ized neural networks, going beyond two layers. In NeurIPS. https://arxiv.org/pdf/
1811.04918.pdf, 2019a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural
networks. In NeurIPS. https://arxiv.org/pdf/1810.12065.pdf, 2019b.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In ICML. https://arxiv.org/pdf/1811.03962.pdf, 2019c.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. In NeurIPS. https://arxiv.org/
pdf/1904.11955.pdf, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis
of optimization and generalization for overparameterized two-layer neural networks. In ICML.
https://arxiv.org/pdf/1901.08584.pdf, 2019b.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420,
2018.
Sergei Bernstein. On a modification of chebyshev’s inequality and of the error formula of laplace.
Ann. Sci. Inst. Sav. Ukraine, Sect. Math,1(4):38-49, 1§24.
Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. Analyzing federated
learning through an adversarial lens. In International Conference on Machine Learning, pp. 634-
643. PMLR, 2019.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
IEEE Symposium on Security and Privacy (SP), pp. 39-57. IEEE, 2017.
Herman Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based on the sum of
observations. The Annals of Mathematical Statistics, pp. 493-507, 1952.
Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble
of diverse parameter-free attacks. In International Conference on Machine Learning, pp. 2206-
2216. PMLR, 2020.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In ICML. https://arxiv.org/pdf/1811.03804,
2019a.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In ICLR. https://arxiv.org/pdf/1810.02054.
pdf, 2019b.
10
Under review as a conference paper at ICLR 2022
Ruiqi Gao, Tianle Cai, Haochuan Li, Cho-Jui Hsieh, Liwei Wang, and Jason D Lee. Convergence
of adversarial training in overparametrized neural networks. Advances in Neural Information
Processing Systems, 32:13029-13040, 2019.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Elad Hazan. Introduction to online convex optimization. Foundations and Trends in Optimization,
2(3-4):157-325, 2016.
Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the
American Statistical Association, 58(301):13-30, 1963.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing systems (NeurIPS),
pp. 8571-8580, 2018.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv
preprint arXiv:1611.01236, 2016.
Jason D Lee, Ruoqi Shen, Zhao Song, Mengdi Wang, and Zheng Yu. Generalized leverage score
sampling for neural networks. In NeurIPS, 2020.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. In Conference on Machine Learning and
Systems, 2020a, 2020a.
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of
fedavg on non-iid data. arXiv preprint arXiv:1907.02189, 2019.
Xiaoxiao Li, Yufeng Gu, Nicha Dvornek, Lawrence Staib, Pamela Ventola, and James S Dun-
can. Multi-site fmri analysis using privacy-preserving federated learning and domain adaptation:
Abide results. Medical Image Analysis, 2020b.
Xiaoxiao Li, Meirui JIANG, Xiaofei Zhang, Michael Kamp, and Qi Dou. Fedbn: Federated learn-
ing on non-iid features via local batch normalization. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=6YEQUn0QICG.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In NeurIPS. https://arxiv.org/pdf/1808.01204.pdf,
2018.
Xinle Liang, Yang Liu, Tianjian Chen, Ming Liu, and Qiang Yang. Federated transfer reinforcement
learning for autonomous driving. arXiv preprint arXiv:1910.06001, 2019.
Wei Yang Bryan Lim, Nguyen Cong Luong, Dinh Thai Hoang, Yutao Jiao, Ying-Chang Liang,
Qiang Yang, Dusit Niyato, and Chunyan Miao. Federated learning in mobile edge networks: A
comprehensive survey. IEEE Communications Surveys & Tutorials, 22(3):2031-2063, 2020.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In ICLR. https://arxiv.
org/pdf/1706.06083.pdf, 2018.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial Intelli-
gence and Statistics, pp. 1273-1282. PMLR, 2017.
Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny,
Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint
arXiv:2003.00295, 2020.
Amirhossein Reisizadeh, Farzan Farnia, Ramtin Pedarsani, and Ali Jadbabaie. Robust federated
learning: The case of affine distribution shifts. arXiv preprint arXiv:2006.08907, 2020.
11
Under review as a conference paper at ICLR 2022
Nicola Rieke, Jonny Hancox, Wenqi Li, Fausto Milletari, Holger R Roth, Shadi Albarqouni, Spyri-
don Bakas, Mathieu N Galtier, Bennett A Landman, Klaus Maier-Hein, et al. The future of digital
health with federated learning. NPJ digital medicine, 3(1):1-7, 2020.
Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-gan: Protecting classifiers against
adversarial attacks using generative models. arXiv preprint arXiv:1805.06605, 2018.
Zhao Song and Xin Yang. Quadratic suffices for over-parametrization via matrix chernoff bound. In
arXiv preprint. https://arxiv.org/pdf/1906.03593.pdf, 2019.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In arXiv preprint. https://arxiv.
org/pdf/1312.6199.pdf, 2013.
Florian Tramer, Alexey Kurakin, NicoIas Papernot, Ian Goodfellow, Dan Boneh, and Patrick Mc-
Daniel. Ensemble adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204,
2017.
Joel A Tropp. An introduction to matrix concentration inequalities. Foundations and TrendsR in
Machine Learning, 8(1-2):1-230, 2015.
Zhuozhuo Tu, Jingwei Zhang, and Dacheng Tao. Theoretical analysis of adversarial learning: A
minimax approach. arXiv preprint arXiv:1811.05232, 2018.
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective in-
consistency problem in heterogeneous federated optimization. arXiv preprint arXiv:2007.07481,
2020.
Shiqiang Wang, Tiffany Tuor, Theodoros Salonidis, Kin K. Leung, Christian Makaya, Ting He, and
Kevin Chan. Adaptive federated learning in resource constrained edge computing systems. IEEE
Journal on Selected Areas in Communications, 37(6):1205-1221, 2019.
Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust distributed
learning: Towards optimal statistical rates. In International Conference on Machine Learning,
pp. 5650-5659. PMLR, 2018.
Gaoyuan Zhang, Songtao Lu, Sijia Liu, Xiangyi Chen, Pin-Yu Chen, Lee Martie, and Mingyi
Horesh, Lior abd Hong. Distributed adversarial training to robustify deep neural networks at
scale. 2021. URL https://openreview.net/pdf?id=kmBFHJ5pr0o.
Xinwei Zhang, Mingyi Hong, Sairaj Dhople, Wotao Yin, and Yang Liu. Fedpd: A federated learning
framework with optimal rates and adaptivity to non-iid data. arXiv preprint arXiv:2005.11418,
2020a.
Yi Zhang, Orestis Plevrakis, Simon S Du, Xingguo Li, Zhao Song, and Sanjeev Arora. Over-
parameterized adversarial training: An analysis overcoming the curse of dimensionality. In
NeurIPS. arXiv preprint arXiv:2002.06668, 2020b.
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated
learning with non-iid data. arXiv preprint arXiv:1806.00582, 2018.
Giulio Zizzo, Ambrish Rawat, Mathieu Sinn, and Beat Buesser. Fat: Federated adversarial training.
arXiv preprint arXiv:2012.01791, 2020.
12
Under review as a conference paper at ICLR 2022
Roadmap The appendix is organized as follows. We introduce the probability tools to be used
in our proof in Section A. In addition, we introduce the preliminaries in Section B. We present the
proof overview in Section C and additional remarks used in the proof sketch in Section D. We show
the detailed proof for the convergence in Section E and the detailed proof of existence in Section F
correspondingly.
A Probability Tools
We introduce the probability tools that will be used in our proof. First we show three lemmas about
the tail bounds for random scalar variables in Lemma A.1, A.2 and A.3:
Lemma A.1 (Chernoff bound Chernoff (1952)). Let X = Pin=1 Xi, where Xi = 1 with probability
Pi and Xi = 0 with probability 1 一 Pi, and all Xi are independent. Let μ = E[X ] = En=I Pi ∙ Then
1. Pr[X ≥ (1 + δ)μ] ≤ exp(-δ2μ∕3), ∀δ > 0 ;
2. Pr[x ≤(1 — δ)μ] ≤ exp(-δ2μ∕2), ∀0 < δ < 1.
Lemma A.2 (Hoeffding bound Hoeffding (1963)). Let Xi, ∙∙∙ , Xn denote n independent bounded
variables in [ai, bi]. Let X =	in=1 Xi, then we have
Pr[|X -E[X]| ≥ t] ≤ 2exp
Lemma A.3 (Bernstein inequality Bernstein (1924)). Let Xi,…，Xn be independent zero-mean
random variables. Suppose that |Xi | ≤ M almost surely, for all i. Then, for all positive t,
n	t2/2
r 匕 i >t] ≤ exp (一 Pn=i E[Xj2] + Mt/3
Pn=i(bi - ai)
Next, we introduce Lemma A.4 about CDF of Gaussian distributions:
Lemma A.4 (Anti-concentration of Gaussian distribution). Let X 〜 N(0, σ2), that is, the proba-
1	-金
bιlιty density function of X Is given by φ(x) = √^ς e ". Then
Pr[|X| ≤t] ∈
2 t 4 t
3 σ, 5 σ
Finally, we introduce Lemma A.5 as a concentration result on random matrices and Claim A.6 about
elementary anti-concentration property of Gaussian distribution.
Lemma A.5 (Matrix Bernstein, Theorem 6.1.1 in Tropp (2015)). Consider a finite sequence
{Xi, ∙∙∙ ,Xm} ⊂ Rn1 ×n2 of independent, random matrices with common dimension ni X n.
Assume that
E[Xi] = 0, ∀i ∈ [m] and kXi k ≤ M, ∀i ∈ [m].
Let Z =	im=i Xi. Let Var[Z] be the matrix variance statistic of sum:
mm
Var[Z] = max	XE[XiXi>],XE[Xi>Xi]
i=i	i=i
Then
E[kZ∣∣] ≤ (2Var[Z] ∙ log(nι + n2))1/2 + M ∙ log(n + n2)∕3.
Furthermore, for all t ≥ 0,
Pr[kZk ≥ t] ≤ (ni + n2) ∙ exp (- Var[Z]+Mt∕3 ) .
We state a standard probabilistic result for Gaussian,
Claim A.6. LetU 〜N(0,Id) and β 〜N(0,1), which are independent. For all X ∈ X and t ≥ 0,
Pr[∣hu, Xi + β∣ ≤ t] = O(t).
13
Under review as a conference paper at ICLR 2022
B	Preliminaries
B.1	Notations
For a vector x, we use kxkp to denote its `p norm, in this paper we mainly consider the situation
when p = 1, 2, or ∞.
For a matrix W ∈ Rd×m, we use W> to denote its transpose and use tr[W] to denote its trace. We
use kW k1, kW k2 and kW kF to denote the entry-wise `1 norm, spectral norm and Frobenius norm
of W respectively. For each j ∈ [m], we let Wj ∈ Rd be the j-th column of W. We let kW k2,1
denotes Pjm=1 kWj k2 and kW k2,∞ denotes maxj∈[m] kWj k2. For two matrices A, B with the same
dimensions, we denote their Euclidean inner product as hA, Bi := tr[A>B].
We denote Gaussian distribution with mean μ and covariance Σ as N(μ, Σ). We use σ(∙) to denote
the ReLU function σ(z) = max{z, 0}, and use 1{E} to denote the indicator function of event E.
B.2	Two layer ReLU neural network and initialization
In this paper, we focus on a two-layer neural network that has m neurons in the hidden layer, where
each neuron is a ReLU activation function. We define the global network as
m
fu(x)：= Ear ∙ σ(hU,X + br)	(4)
r=1
and for c ∈ [N], we define the local network of client c as
m
fWc(x) ：= Ear ∙ σ(hWc,r,xi + br).	(5)
r=1
Here U = (U1, . . . , Um) ∈ Rd×m is the global hidden weight matrix, Wc = (Wc,1, . . . , Wc,m) ∈
Rd×m is the local hidden weight matrix of client c, and a = (a1 , . . . , am ) ∈ Rm denotes the output
weight vector, b = (b1 , . . . , bm ) ∈ Rm denotes the bias vector. During the process of federated
adversarial learning, for convenience we keep a and b equal to their initialized values and only
update U and W, so we can write the global network as fU (x) and the local network as fWc (x).
For the situation we don’t care about the weight matrix, we write f(x) or fc(x) for short. Next, we
make some standard assumptions regarding our training set.
Definition B.1 (Dataset). There are N clients andn = NJ data in total.4 LetS = ∪c∈[N] Sc where
Sc = {(xc,1, yc,1), ..., (xc,J, yc,J)} ⊆ Rd × R denotes the J training data of client c. Without loss
of generality, we assume that for all c ∈ [N], j ∈ [J], kxc,j k2 = 1 and the last coordinate of each
point equals to 1/2 , so we consider X ：= {x ∈ Rd ： kxk2 = 1, xd = 1/2}. For simplicity, we also
assume that for all c ∈ [N], j ∈ [J], |yc,j | ≤ 1.5
We now define the initialization for the neural networks.
Definition B.2 (Initialization). The initialization of a ∈ Rm, U ∈ Rd×m, b ∈ Rm is a(0) ∈
Rm, U(0) ∈ Rd×m, b(0) ∈ Rm. The initialization of client c’s local weight matrix Wc is
Wc(0, 0) = U(0). Here the second term in Wc denotes iteration of local steps.
•	For each r ∈ [m], ar (0) are i.i.d. sampled from [-1/m1/3, +1/m1/3]6 uniformly.
•	For each i ∈ [d], r ∈ [m], Ui,r (0) and br (0) are i.i.d. random Gaussians sampled from
N(0, 1/m). Here Ui,r means the (i, r)-entry of U.
4Without loss of generality, we assume that all clients have same number of training data. Our result can be
generalized to the setting where each client has a different number of data as the future work.
5Our assumptions on data points are reasonable since we can do scale-up. In addition, l2 norm normalization
is a typical technique in experiments. Same assumptions also appears in many previous theoretical works like
Arora et al. (2019b); Allen-Zhu et al. (2019a;b).
6Here the choice of m1/3 is not a must. Actually what we need is [-1/mc, +1/mc] for some c that satisfies
Ω(1) ≤ C ≤ 1/3.
14
Under review as a conference paper at ICLR 2022
For each global iteration t ∈ [T],
•	For each c ∈ [N], the initial value of client c’s local weight matrix Wc is Wc(t, 0) = U (t).
B.3	Adversary and Well- separated training sets
We first formulate the adversary as a mapping.
Definition B.3 (ρ-Bounded adversary). Let F denote the function class. An adversary is a mapping
A : F × X × R → X which denotes the adversarial perturbation. For ρ > 0, we define the `2 ball
as B2 (x, ρ) := {xe ∈ Rd : kxe - xk2 ≤ ρ} ∩ X, we say an adversary A is ρ-bounded if it satisfies
A(f, x, y) ∈ B2 (x, ρ).
Moreover, given ρ > 0, we denote the worst-case adversary as A* := argmaxe∈B2@汨)'(f (X),y),
where ` is loss function defined in Definition B.5.
In the over-parameterized regime, it is a standard assumption that the training set is well-separated.
Since we deal with adversarial perturbations, we require the following γ-separability, which is a bit
stronger.
Definition B.4 (γ -separability). Let γ ∈ (0, 1/2), δ ∈ (0, 1/2), ρ ∈ (0, 1/2) denote three parame-
ters such that Y ≤ δ ∙ (δ — 2ρ). We say our training set S = ∪c∈[n]Sc = ∪c∈[N],j∈[j}{(χc,j, yc,j)} ⊂
Rd × R is globally γ -separable w.r.t a ρ-bounded adversary, if
min
c1 6=c2,j1 6=j2
kxc1,j1
- xc2,j2 k2 ≥ δ.
It is worth noting that, our problem setup does not require the assumption on independent and
identically distribution (IID) on data, thus such a formation can be applied to unique challenge of
the non-IID setting in FL.
B.4	Robust loss function
We define the following Lipschitz convex loss function that will be used.
Definition B.5 (Lipschitz convex loss). A loss function ` : R × R → R is said to be a Lipschitz
convex loss, if it satisfies the following four properties:
•	non-negative;
•	convex in the first input of `;
•	1-Lipshcitz, which means k'(χι,yι) - '(χ2,y2)∣∣2 ≤ k(x1,y1) - (x2,y2)k2；
•	'(y, y) = 0 for all y ∈ R.
The choice of this type of loss is for the sake of technical presentation simplicity, as is customary in
prior studies Gao et al. (2019); Allen-Zhu et al. (2019a).
We assume ` is a Lipschitz convex loss in this paper. Next, we define our robust loss function of a
neural network, which is based on the adversarial examples generated by a ρ-bounded adversary A.
Definition B.6 (Training loss). Given a client’s training set Sc = {(xc,j , yc,j)}jj=1 ⊂ Rd × R of
J examples, the standard training loss ofa neural net fc : Rd → R is defined as L(fc, Sc) :=
J PJ=I' (fc(xc,j ),yc,j). Given S = ∪c∈[n ]Sc ,the global loss is defined as L(fu, S):=
NJ Pc=I PJ=I '(fu(XCj), yc,j). Given a P-bounded adversary A, we define the global loss with
respect to A as
1 NJ
LAfU)：= NJ XX '(fu (AfC,xc,j ,yc,j )),yc,j)
NJ c=1 j =1
NJ
=NJ XX '(fu (xc,j), yc,j)
15
Under review as a conference paper at ICLR 2022
and also define the worst-case global robust loss as
1NJ
LA*(fu) := NJXX'(fu(AYfc,xc,j,yc,j)),yc,j)
NJ c=1 j=1
1NJ
=NJXX x* ∈maχ ,ρ) ` (fu ("yc,j).
c=1 j=1 xc,j ∈B2 (xc,j ,ρ)
Moreover, since we deal with pseudo-net which is defined in Definition D.1, we also de-
fine the loss of a pseudo-net as L(gc, Sc) := 1 PJ=I ' (gc(xc,j),yc,j) and L(gu, S):=
NJ PL PJ=I '(gu (χc,j ),yc,j) ∙
B.5 Federated Adversarial Learning algorithm
Classical adversarial training algorithm can be found in Zhang et al. (2020b). Different from the
classical setting, our federated adversarial learning of a local neural network fWc against an adver-
sary A is shown in Algorithm 2, where there are two procedures: one is CLIENTUPDATE running on
client side and the other is ServerExecution running on server side. These two procedures are
iteratively processed through communication iterations. Adversarial training is addressed in proce-
dure ClientUpdate. Hence, there are two loops in ClientUpdate procedure: the outer loop is
iteration for local model updating; and the inner loop is iteratively generating adversarial samples by
the adversary A. In the outer loop in SERVEREXECUTION procedure, the neural network’s parame-
ters are updated to reduce its prediction loss on the new adversarial samples. These loops constitute
an intertwining dynamics.
16
Under review as a conference paper at ICLR 2022
Algorithm 2 Federated Adversarial Learning (FAL). This is a complete version of Algorithm 1.
1: 2: 3: 4: 5: 6: 7: 8: Q∙ 9:	/*Defining notations and parameters*/ We use c to denote the client’s index The training set of client c is denoted as Sc = {(xc,j , yc,j )}jJ=1 Let A be the adversary We denote local learning rate as ηlocal We denote global learning rate as ηglobal We denote local updating iterations as K We denote global communication round as T
10:	/*Initialization*/
11:	Initialization a(0) ∈ Rm,U(0) ∈ Rd×m,b(0) ∈ Rm
12: 1 ɔ.	For t = 0 → T , we iteratively run Procedure A then Procedure B
13: 14:	/* Procedure running on client side */
15:	procedure A . CLIENTUPDATE(t, c)
16:	Sc(t) J 0
17:	Wc(t, 0) J U (t)	. Receive global model weights update
18:	for k = 0 → K - 1 do
19:	for j = 1 → J do
20:	xe(ct,j) J A(fWc (t,k), xc,j, yc,j)	. Adversarial examples, fWc is defined as (5)
21:	Sc(t)JSc(t)∪(xe(ct,j),yc,j)
22:	end for
23:	Wc(t, k + I) J Wc(t, k) - ηiocal ∙ VWcL(fWc(t,k), Sc(t))
24:	end for
25:	∆Uc(t) J Wc(t,K) -U(t)
26:	Send ∆Uc(t) to SERVEREXECUTION
27: ɔo. 28:	end procedure
29:	/*Procedure running on server side*/
30:	procedure B . SERVEREXECUTION(t):
31: 32:	for each client c in parallel do ∆Uc(t) J CLIENTUPDATE(c, t)	. Receive local model weights update
33:	∆U(t) J N Pc∈[N]∆Uc(t)
34:	U(t + 1) J U(t) + ngiobai ∙ ∆U(t)	. Aggregation on the server side
35:	Send U(t + 1) to client c for CLIENTUPDATE(c, t)
36:	end for
37:	end procedure
17
Under review as a conference paper at ICLR 2022
C Proof Overview
In this section we give an overview of our main result’s proof. Two theorems to be used are Theo-
rem E.3 and Theorem F.3.
C.1 Pseudo-network
We study the over-parameterized neural nets’ well-approximated pseudo-network to learn gradient
descent for over-parameterized neural nets whose weights are close to initialization. The introducing
of pseudo-network makes the proof more intuitive.
To be specific, we give the definition of pseudo-network in Section D, and also state Theorem D.2
which shows the fact that the pseudo-network approximates the real network uniformly well. It can
be seen that the notion of pseudo-network is used for several times in our proof.
C.2 Online gradient descent in federated adversarial learning
Our federated adversarial learning algorithm is formulated in the framework of online gradient de-
scent: at each local step k on the client side, firstly the adversary generates adversarial samples and
computes the loss function L fWc(t,k), Sc(t) , then the local client learner takes the fresh loss func-
tion and update Wc(t, k +1) = Wc(t, k) - niocai ∙ RWcL (fwc(t,k), Sc(t)). We refer our readers
to Gao et al. (2019); Hazan (2016) for more details regarding online learning and online gradient
descent.
Compared with the centralized setting, the key difficulties in the convergence analysis of FL are
induced by multiple local updates on the client side and the updates on both local and global sides.
Specifically, local updates are not the standard gradient as the centralized adversarial training when
K ≥ 2. We used -∆U (t) in substitution of the real gradient of U to update the value of U (t). This
brings in challenges to bound the gradient of the neural networks. Nevertheless, gradient bounding
is challenging in adversarial training solely. We use gradient coupling method twice to solve this
core problem: firstly we bound the difference between real gradient and FL gradient in Lemma E.4,
then we bound the difference between pseudo gradient and real gradient in Lemma E.5. We show the
connection of online gradient descent and federated adversarial learning in the proof of Theorem E.3.
C.3 Existence of robust network near initialization
In Section F we show that there exists a global network fu* whose weight is close to the initial
value U(0) and the worst-case global robust loss La* (fu*) is sufficiently small. We show that the
required width m is poly(d, (NJ/e)1/Y).
Suppose we are given a ρ-bounded adversary. For a globally γ-separable training set, in order to
prove Theorem F.3, we first state Lemma F.1 which shows the existence of function f * that has
"low complexity" and satisfies f * (Xcj) ≈ ycj for all data point (Xc,j, ycj) and perturbation inputs
xec,j ∈ B2(xc,j, ρ). Then, we state Lemma F.2 which shows the existence of a pseudo-network gU*
that approximates f* well. Finally, by using Theorem D.2 we show that fU* approximates gU* well.
By combining these results, we we finish the proof of Theorem F.3.
18
Under review as a conference paper at ICLR 2022
D Real approximates pseudo
To make additional remark to proof sketch in Section 5, in this section, we state a tool that will be
used in our proof that is related to our definition of pseudo-network. First, we recall the definition
of pseudo-network.
Definition D.1 (Pseudo-network). Given weights U ∈ Rd×m, a ∈ Rm and b ∈ Rm, the global
neural network function fU : Rd → R is defined as
m
fu(x):= Ear ∙ σ(hUr,xi + br).
r=1
Given this fU (x), we define the corresponding pseudo-network function gU : Rd → R as
m
gU (x) := X ar ∙ hUr (t) - Ur (0),X〉∙ 1{〈Ur (0),X)+ br ≥ 0}∙
r=1
From the definition we can know that pseudo-network can be seen as a linear approximation of the
two layer ReLU network we study near initialization. Next, we cite a Theorem from Zhang et al.
(2020b), which gives a uniform bound of the difference between a network and its pseudo-network.
Theorem D.2 (Uniform approximation, Theorem 5.1 in Zhang et al. (2020b)). Let R ≥ 1. For all
m ≥ poly(d), with probability at least 1 — exp(-Ω(m1/3)) over the choice of a(0), U (0), b(0) ,for
all U ∈ Rd×m such that kU - U (0)k2,∞ ≤ R/m2/3,
sup |fU (x) - gU (x)| ≤ O(R2/m1/6).
x∈X
19
Under review as a conference paper at ICLR 2022
E Convergence
Section	Statement	Comment	Statements Used
ɪi	Definition E.1 and E.2	Definition	-
-E.2	Theorem E.3	Convergence result	Lem. E.4, E.5, Thm. D.2一
^E3	Lemma E.4	Approximates real gradient	-
^E4	Lemma E.5	Approximates pseudo gradient	CIaimE.6
E.5	CIaim E.6	Auxiliary bounding	Claim A.6
Table 1: List of theorems and lemmas in Section E. The main result of this section is Theorem E.3.
By saying "Statements Used" we mean these statements are used in the proof in the corresponding
section. For example, Lemma E.4, E.5 and Theorem D.2 are used in the proof of Theorem E.3.
E.1 Definitions and notations
In Section E, we follow the notations used in Definition D.1. Since we are dealing with pseudo-
network, we first introduce some additional definitions and notations regarding gradient.
Definition E.1 (Gradient). For a local real network fWc(t,k), we denote its gradient by
N(fc,t,k) ：= VWc L(fwc(t,k), Sc(t)).
If the corresponding pseudo-network is gWc(t,k), then we define the pseudo-network gradient as
V(gc, t, k) :=VWcL(gWc(t,k),Sc(t)).
Now we consider the global matrix. For convenience we write V(f, t) := VUL(fU(t), S(t)) and
V(g,t) := VUL(gu(t), S(t)). Wedefinethe FL gradient as V(f,t) := -N∆U(t).
Definition E.2 (Distance). For U * ∈ Rd×m SUCh that ∣∣U * 一 U∣∣2,∞ ≤ R/m3/4, we define the
following distance for simplicity:
Dmax := max ∣U (t) - U ∣2,∞
.. ~ ..
DU* ：= ∣U* 一 U∣2,∞
We have DU* = O(R∕m3/4) and ∣∣U(t) 一 U*k2,∞ ≤ Dmax + DU* by using triangle inequality.
Notation	Meaning	Satisfy
U(0) or U	Initialization of U	Wc(0,0) = U (0)	〜
U (t)	The value of U after t iterations	Dmax =.max ∣U(t) 一 U∣2,∞
U *		The value of U after small perturbations from U	∣u* - U∣2,∞ ≤ R∕m3/4
Table 2: Notations of global model weights in federated learning to be used in this section.
E.2 Convergence result
The goal of this section is to prove Theorem E.3.
Theorem E.3 (Convergence result, formal version of Theorem 5.6). For all e ∈ (0, 1), for all
R ≥ 1, there exists an M = poly(n, R, 1/e), suCh that for every m ≥ M, for every K ≥ 1,
for ^very T ≥ poly(R∕e), with probability at least 1 — exp(-Ω(m1/3)) over the randomness of
a(0) ∈ Rm, U(0) ∈ Rd×m, b(0) ∈ Rm, ifwe run Algorithm 2 with setting
ηglobal = 1/ poly(N J, R, 1/e) and ηlocal = 1/K,
then for every U* suCh that ∣U* 一 U (0)∣2,∞ ≤ R/m3/4, the output weights (U(t))tT=1satisfy
1T
T y^LA (fU (t)) ≤ LA* (fU *) + e.
t=1
20
Under review as a conference paper at ICLR 2022
Proof. We set our parameters as follows:
M = Ω(max {(NJ)8, (R)12})
ηglobal = O( Nm1/ . Poly(R/J )
ηlocal = 1/K
Since the loss function is 1-Lipschitz, we can first bound the norm of real net gradient:
1J	1
∣∣Vr fc,t, k)∣∣2 ≤ ∣ar∣∙ (J X σ (hWc,r (t, k),xc,j i + br ) ∙ Ilxcj 心)≤ Iarl ≤ ^ɪ/ɜ ∙⑹
j=1	m
Now we consider the pseudo-net gradient. The loss L(gU, S(t)) is convex in U due to the fact that
g is linear with U. Then we have
L(gU(t), S(t)) - L(gU*, S(t))
≤hVuL(gu(t),S(t)),U(t) - U*〉
.~, ~, . .. .
=hV(f,t), U(t) - U*i + hV(f, t) - V(f,t), U(t) - U*i + hV(g, t) - V(f, t), U(t) - U*i
≤ α(t) + β(t) + γ(t)
where the last step follows from
.~'
α(t) := hV(f,t),U(t) - U*〉，
β(t) ：= ∣V(f,t) -V (f,t)∣2,1 ∙∣U (t) - U *∣∣2,∞,
γ(t) := ∣V(g,t) -V(f,t)k2,1 ∙∣U(t) - U*∣∣2,∞.
Note that the FL gradient V(f, t) = - -1 ∆U (t) is the direction moved by center, in contrast, V(f, t)
is the true gradient of function f. We deal with these three terms separately. As for α(t), we have
IlU(t + I)- U* kF = kU(t) + ηglobalδu(t) - U* kF
=kU(t) - U*kF - 2Nnglobala(t) + ngloball^U(t)kF
and by rearranging we get
α(t) = ngobai k∆U (t)kF + "— ∙ (kU (t) - U *kF -kU (t + 1) - U *kF).
2N	2N ηglobal
Next, we need to upper bound k∆U (t)k2F,
- K-1
k∆u (t)kF = k nNal XX V(fc,t,k)kF
c=1 k=0
- K-1 m
≤ 竽 XXXkVr(fc, t, k)k22
c=1 k=0 r=1
= nlocal K m
= m1/3.	(7)
where the last step follows from Knlocal = 1. Then we do summation over t and have
TT	T
X a(t) = n2Nal X k∆U (t)kF + 2Nn- ∙ X(kU (t) - U *kF -kU (t + 1) - U *kF)
T1
=ngobal E k∆u (t)kF + --(kU (1) - U *kF -kU (T + 1) - U *kF)
2N t=1	2N nglobal
T1
≤ ⅞Γ X ∣δu(t)kF + 2N^ ∙ku(I)- U*kF
.y Tm1/3 + N nglobal mDU2 *
21
Under review as a conference paper at ICLR 2022
WherethelaststePfolloWsfromEq.⑺ and ∣∣U - U*∣∣F ≤ m ∙∣∣U - U*k2,∞ = mD,*.
As for β(t), we apply Lemma E.4 and also triangle inequality and have
β(t) = ∣V(f,t) -V (f,t)∣∣2,1∙kU (t)- U *∣∣2,∞
.m2/ ∙∣u (t) - u] ∣2,∞
.m223 ∙ (IIU⑴一Uk2,∞ + DU* ).
By using Eq. (6) We bound the size of IU (t) - U I2,∞ :
t
IU (t) - Ue I2,∞ ≤ ηglobal X I∆U (τ)I2,∞
τ=1
t	N K-1
= ηglobalX k ηNal XXV(fc, t, k)I2,∞
τ=1	c=1 k=0
t N K-1
≤ ηgoNocal XXX IV(fc, t, k)I2,∞
τ=1 c=1 k=0
≤ ηglobalηlocaltK m
and have
β (t) . ηglobalηlocaltK m	+m223DU*.
Then We do summation over t and have
TT
Xβ(t) . X(ηglobalηlocaltKm123 + m223DU*)
t=1	t=1
. ηglobalηlocalT 2Km123 + m223TDU*
. ηglobalT2m123 + m223TDU* .
As for γ(t), We aPPly Lemma E.5 and have
γ(t) = ∣∣V(g,t) - V(f,t)∣∣2,1 ]∣U(t) - U*∣∣2,∞
.NJm13224 ∙ (∣U(t) - U∣2,∞ + Du*).
Since IU (t) - Ue I2,∞ ≤ ηglobalηlocaltKm-123, We have
γ(t) . ηglobalηlocaltKN Jm5224 +NJm13224DU* .
Then We do summation over t and have
TT
X γ(t) . X SgiobainiocaitKNJm5224 + NJm13224DU*)
t=1	t=1
. ηgiobaiηiocaiT2KNJm5224 +NJm13224TDU*
. ηgiobaiT2NJm5224 +NJm13224TDU*.
22
Under review as a conference paper at ICLR 2022
Next we put it altogether. Note that DU* = O( mR4), thus we obtain
TT
XL(gU(t),S(t))-XL(gU*,S(t))
t=1
T
t=1
TT
≤ α(t) +	β(t)+	γ(t)
.ηgobalTm1/3 +	mDU* + ηgiobaiT2m1/3
N	N ηglobal
+ m2∕3TDu * + ηgiobaiT2 NJm5/24 + NJm13/24TDU *
.中Tm1/3 +	R2m-1∕2 + ηgiobaiT2m1/3
N	N ηgiobai
+ RTmT/12 + ηgiobaiT 2NJm5/24 + NJm-5/24RT.
We then have
T
1T	1T
T £ LIgU(t), S(t)) - T £L(gu*, S(t))
t=1	t=1
.ηgNaim1/3 + NngJ：a∣TR2m-1/2 + ηgiobaiTm1/3 + Rm-1/12
+ ngiobaiTN Jm5/24 + NJmTF.
.Z 1	T RmT/ + ngiobaiTm1/3 + Rm-1/12 + ngiobaiTNJm5/24 + NJmT5?"R ⑻
NηgiobaiT
≤ O(e).
From Theorem D.2 we know
sup |fU (x) - gU (x)| ≤ O(R2/m1/6) = O(e)
x∈X
and thus, we get
1T	1T
T E L(fu m, S(t)) - T E L(fu *, S(t)) ≤ C ∙e
(9)
t=1
t=1
where c > 0 is a constant. From the definition of A* We have L(fu *, S (t)) ≤ L∕* (fu *). From the
definition of loss we have L(fU(t), S(t)) = LA(fU(t)). Moreover, since Eq. (9) holds for all > 0,
we can replace C with e. Thus we prove that for ∀e > 0,
1T
T	LA(fu(t)) ≤ LA*(fu*) + e∙
t=1
□
E.3 Approximates real global gradient
The goal of this section is to prove Lemma E.4.
Lemma E.4 (Bounding the difference between real gradient and FL gradient). With probability at
least 1 一 exp(-Ω(m1/3)) over the randomness of a(0) ∈ Rm, U(0) ∈ Rd×m, b(0) ∈ Rm, for all
iterations t such that kU (t) - U (0)k2,∞ ≤ O(m-15/24), the following holds:
kV(f,t) -V (f,t)k2,1 ≤ O(m2/3).
Proof. Notice that V(f,t) = VUL(fu⑴，S(t)) and
N	N K-1
V (f,t) = - N∆U (t) = - N X ∆Uc(t) = niNai XX V(fc,t,k).
c=1	c=1 k=0
23
Under review as a conference paper at ICLR 2022
So we have
m
kV(f,t)-V (f,t)k2,1= EkVr (f,t)-V r (f,t)k2
r=1
m	N K-1
=N X kN ∙Vr (f,t) - ηiocal X X Vr (fc,t, k)∣∣2
ηlocal XXI U N ∙ Vr (f,t)	Xs
≤	工 k Kniocal	一与 Vr (fc，t，k)k2
r=1 k=0	local c=1
1	m K -1	N
=NK XX kN ∙Vr(f,t)- X Vr (fc,t,k)k2
r=1 k=0	c=1
where the last step follows from the assumption that niocai =表.
As for kN ∙ Vr (f,t) - PN=I Vr(fc,t, k)k2, we have
N
kN ∙Vr(f,t)- X Vr (fc,t,k)k2
c=1
N NJ
≤ Hr, |( NJ XX 1{hUr (t),xc,j i + br ≥ 0}
NJ c=1 j=1
NJ
-J XX1{hWc,r (t,k),Xc,j i + br ≥0}) ∙kXc,j∣∣2∣
≤
m
c=1 j=1
焉∙ J X XI1{hUr(t),Xcj + br ≥ 0}- 1{hWc,r(t,k),Xc,ji + b ≥ 0}∣
c=1 j=1
N
≤ m1/3 .
Then we do summation and have
m K -1	N
llV(f,t) - V (f,t)k2,1 ≤ NK X X k N ∙ Vr(Zt)- X Vr(fc,t, k)k2
r=1 k=0
<	1 XX N
≤ NKl^Z m1/3
= m2/3.
c=1
Thus we finish the proof.
□
E.4 Approximates pseudo global gradient
The goal of this section is to prove Lemma E.5.
Lemma E.5 (Bounding the difference between pseudo gradient and real gradient). With probability
at least 1 一 exp(-Ω(m1/3)) over the randomness of a(0) ∈ Rm, U (0) ∈ Rd×m, b(0) ∈ Rm ,for
all iterations t such that kU (t) - U (0)k2,∞ ≤ O(m-15/24), the following holds:
kV(g,t) -V(f,t)k2,1 ≤ O(NJmI3/24).
Proof. Notice that V(g, t) = VU L(gU(t), S(t)) and V(f, t) = VUL(fU(t),S(t)). By Claim E.6,
with the given probability we have
m
X 1{Vr(g,t) = Vr(f,t)} ≤ O(NJm7∕8).
r=1
24
Under review as a conference paper at ICLR 2022
For indices r ∈ [m] such that Nr(g, t) = Nr(f, t),the following holds:
kVr (g,t) -Nr (f,t)k2 = ∣∣Nu,r L(gu (t) ,S(t)) -Nu,r L(fu (t),S(t))∣∣2
1 NJ
≤ |ar | ∙ NJ ∙ XX llxc,j k2 ∙ I 1{hUer ,xc,ji + br ≥ 0}
NJ c=1 j=1
- 1{hUr, xc,j i + br ≥ 0}II
1	1 NJ
≤ —1/3 ∙ RJ ∙ E E 1 1{hUer, Xcji + br ≥ 0}- 1{hUr, xc,j i + br ≥ 0}|
m NJ c=1 j=1
1
≤ m1/3 .
where the first step is definition, the second step follows that the loss function is 1-Lipschitz, the
third step follows from |a" ≤ m1/3 and Ilxcjk2 = 1, the last step follows from the bound of the
indicator function. Thus, we do the conclusion:
kN(g,t)-N(f,t)k2,1
m
=X kVr(g,t) - Vr (f,t)k2 ∙ 1{Vr (g,t) = Vr(f,t)}
r=1
m
≤ m1/3 X 1{Vr(g,t)= Vr (f,t)}
r=1
≤ ɪ ∙O(NJm78
m1/3
= O(NJm13/24)
and finish the proof.	□
E.5 Bounding auxiliary
Claim E.6 (Bounding auxiliary). With probability at least 1 一 exp(-Ω(m1/3)) over the initializa-
tion, we have
m
X 1{Vr(g,t) = Vr(f,t)} ≤ O(NJm7/8).
r=1
Proof. For r ∈ [m], let Ir := 1{Vr(g, t) 6= Vr (f, t)}. By Claim A.6 we know that for each xc,j
we have
Pr[|hWfc,r,xc,ji+br| ≤ m-15/24] ≤ O(m-1/8).
By putting a union bound over c and j , we get
Pr∃c∈ [N],j∈ [J], |hWfc,r,xc,ji +br| ≤ m-15/24 ≤ O(NJm-1/8).
Since
Pr[Ir = 1] ≤ Pr[∃c ∈ [N],j∈ [J], |hWfc,r,xc,ji+br| ≤ m-15/24],
we have
Pr[Ir = 1] ≤ O(N Jm-1/8).
By applying concentration inequality on Ir (independent Bernoulli) for r ∈ [m], we have that with
probability at least 1 一 exp(-Ω(NJm7/8)) > 1 一 exp(-Ω(m1/3)), the following holds:
m
X Ir ≤ O(N Jm7/8).
r=1
Thus we finish the proof.
□
25
Under review as a conference paper at ICLR 2022
E.6 Further Discussion
Note that in the proof of Theorem E.3 we set the hidden layer’s width m to be greater than O(-12),
which seems impractical in reality: if we choose our convergence accuracy to be 10-2, the width
will become 1024 which is impossible to achieve.
However, we want to claim that the "-12" term is not intrinsic in our theorem and proof, and
we can actually further improve the lower bound of m to O((R/)c2) where c2 is some constant
between -3 and -4. To be specific, we observe from Eq. (8) that the "-12" term comes from
2 - 3 = -112, where 3 appears in Lemma E.4 and 4 appears in the assumption that DU* ≤ R/m3/4
in Definition E.2. As for our observations, the 3 term is hard to improve. On the other hand, we can
actually adjust the value of DU* as long as we ensure
DU* ≤ R/mc3
for some constant c3 ∈ (0, 1). When we let c3 → 1, the final result will achieve
O((R)3)
which is much more feasible in reality.
As the first work and the first step towards understanding the convergence of federated adversarial
learning, the priority of our work is not achieving the tightest bounds. Instead, our main goal is to
show the convergence of a general federated adversarial learning framework. Nevertheless, we will
improve the bound in the final version.
26
Under review as a conference paper at ICLR 2022
F Existence
In this section We prove the existence of U * that is close to U (0) and makes L∕* (fu *) close to zero.
F.1 Tools from previous work
In order to prove our existence result, We first state tWo lemmas that Will be used.
Lemma F.1 (Lemma 6.2 from Zhang et al. (2020b)). Suppose that kxc1,j1 - xc2,j2 k2 ≥ δ holds for
each pair of two different data points xc1,j1 , xc2,j2. Let D = 24γ-1 ln(48N J /), then there exists
a polynomial q : R → R with degree at most D, size of coefficients at most O(γ-126D), such that
for all c0 ∈ [N],j0 ∈ [J] and xec0,j0 ∈ B2 (xc0,j0, ρ),
N J
yc,j ∙ q(hxc,j , xC0,j0 i) ― yC0,j0 I ≤ 3 .
Welet f*(x) := PN=I Pj=I yc,j ∙ q(hxc,j,xi) andhave |f*(χc0,j0) - yc0,j0 | ≤ e∕3.
Lemma F.2 (Lemma 6.5 from Zhang et al. (2020b)). For all ∈ (0, 1), there exist M =
poly(d, (NJ∕e)1∕γ) and R = Poly((NJ/e)1∕γ) such that for m ≥ M, with probability at least
1 一 exp(—Ω( pm/NJ)) over the choice of a(0) ∈ Rm, U (0) ∈ Rd×m, b(0) ∈ Rm ,there exists a
U * ∈ Rd×m that satisfies ∣∣U * — U (0)∣∣2,∞ ≤ R/m2/3 and
suP |gU* (x) 一 f* (x)| ≤ e/3.
x∈X
F.2 Existence result
The goal of this section is to prove Theorem F.3 Which is the existence result.
Theorem F.3 (Existence, formal version of Theorem 5.2). For all e ∈ (0, 1), there exist
Mo = poly(d, (NJ∕e)1∕γ) and R = Poly((NJ/e)1∕γ)
such thatfor ^very m ≥ Mo, with probability at least 1 — exp(—Ω(m1/3)) over the randomness of
a(0) ∈ Rm, U(0) ∈ Rd×m, b(0) ∈ Rm, there exists U* ∈ Rd×m that satisfies ∣U* — U (0)∣2,∞ ≤
R/m2/3 and
LA* (fU* ) ≤ e.
Proof. From Lemma F.1 We obtain the function f*. From Lemma F.2 We knoW the existence of
Mo = poly(d, (NJ/e)1/Y) and also R = Poly((NJ/e)1∕γ). By combining these two results
with Theorem D.2, we have that for all m ≥ poly(d, (NJ/e)1/Y), with probability at least 1 —
exp( — Ω( Pm/NJ)) —exp(—Ω(m1/3)), thereexists a U * ∈ Rd×m that satisfies ∣∣U * — U (0)∣∣2,∞ ≤
R/m2/3 and also the following properties:
•	∀x ∈ X, |gU* (x) — f* (x)| ≤ e/3
•	∀x ∈ X, |fU* (x) — gU* (x)| ≤ O(R2/m1/6)
We consider the loss function. For all c ∈ [N], j ∈ [J] and xec,j ∈ B(xc,j, ρ), we have
'(fu* (χc,j),yc,j) ≤ |fu* (χc,j) — yc,j|
≤	Ifu*(%,j) — gu*(%,j)1 + ∣gu*(ec,j) — f*(%)1 + If*(%) -yj
≤	O(R2/m1/6) + e + e
≤ e,
27
Under review as a conference paper at ICLR 2022
Thus, We have that La. (fu*) = N1J PN=1 PJ=I max ' (fu* (xC,j), yc,j) ≤ e. Furthermore, since
the m we consider satisfies m ≥ Ω((NJ)1/y), the holding probability is at least
1 — exp(—Ω( p m/NJ)) — exp(-Ω(m1/3)) = 1 — exp(-Ω(m1/3)).
Thus, we finish the proof of this theorem.
□
28