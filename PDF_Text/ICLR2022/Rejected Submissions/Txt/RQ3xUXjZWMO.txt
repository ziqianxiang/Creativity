Under review as a conference paper at ICLR 2022
Implicit Jacobian regularization
WEIGHTED WITH IMPURITY OF PROBABILITY OUTPUT
Anonymous authors
Paper under double-blind review
Ab stract
Gradient descent (GD) plays a crucial role in the success of deep learning, but it
is still not fully understood how GD finds minima that generalize well. In many
studies, GD has been understood as a gradient flow in the limit of vanishing
learning rate. However, this approach has a fundamental limitation in explaining
the oscillatory behavior with iterative catapult in a practical finite learning rate
regime. To address this limitation, we rather start with strong empirical evidence of
the plateau of the sharpness (the top eigenvalue of the Hessian) of the loss function
landscape. With this observation, we investigate the Hessian through simple and
much lower-dimensional matrices. In particular, to analyze the sharpness, we
instead explore the eigenvalue problem for the low-dimensional matrix which is
a rank-one modification of a diagonal matrix. The eigendecomposition provides
a simple relation between the eigenvalues of the low-dimensional matrix and the
impurity of the probability output. We exploit this connection to derive sharpness-
impurity-Jacobian relation and to explain how the sharpness influences the learning
dynamics and the generalization performance. In particular, we show that GD has
implicit regularization effects on the Jacobian norm weighted with the impurity of
the probability output.
1	Introduction
Deep learning has shown to be powerful for many learning tasks in various areas. There has been
a lot of work to understand how the learning algorithm leads to this successful training of deep
neural networks. Especially, it is crucial to understand the geometric properties of the loss landscape
of neural networks and their interaction with the gradient-based optimization methods, such as
Stochastic Gradient Descent (SGD), along the training trajectory. It has been studied both from the
optimization (Gur-Ari et al., 2018; Jastrzebski et al., 20l9; Ghorbani et al., 2019; LiU et al., 2020;
Lewkowycz et al., 2020; Cohen et al., 2021) and generalization (Hochreiter & Schmidhuber, 1997;
Keskar et al., 2017; Dinh et al., 2017; Jastrzebski et al., 2017; Wang et al., 2018; Chaudhari et al.,
2019; Fort et al., 2019; Jiang et al., 2020; Barrett & Dherin, 2021; Smith et al., 2021) point of view.
We aim at investigating the Hessian of the training loss (with respect to model parameter) and its
top eigenvalue (also called sharpness). The sharpness characterizes the dynamics of neural network
training along the optimization trajectory and is correlated with the generalization capability. For
example, the sharpness increases in the beginning, and after reaching a certain value, training
dynamics becomes unstable, oscillating along the top eigenvector (Jastrzebski et al., 2019; Cohen
et al., 2021). Moreover, the rapid increase in the sharpness of the loss landscape in the early phase
significantly impacts the final generalization performance (Achille et al., 2019; Jastrzebski et al.,
2020; Lewkowycz et al., 2020; Jastrzebski et al., 2021). However, the Hessian of a deep neural
network is very high-dimensional which makes it difficult to analyze its eigensystem. Recently, some
researchers studied the Hessian by exploiting tools in randomized numerical linear algebra (Sagun
et al., 2017; Papyan, 2018; 2019; Ghorbani et al., 2019; Yao et al., 2020) and decomposition of the
Hessian (Papyan, 2018; 2019; Fort & Ganguli, 2019).
In this paper, we present a new decomposition of the Hessian using eigendecomposition of low-
dimensional matrices. From the eigensystem of the low-dimensional matrix, we can provide a simple
and intuitive explanation on the relation between its eigenvalue and the probability output. This
1
Under review as a conference paper at ICLR 2022
enables us to explain how the sharpness of the loss landscape influences the learning dynamics and
the generalization performance.
We summarize the main contributions of the paper as follows:
•	We decompose the Hessian with low dimensional matrices, the logit Hessian and the
logit-weight Jacobian (defined in Definition 1), and investigate the Hessian by the eigende-
composition of the logit Hessian which is a rank-one modification of a diagonal matrix.
•	We provide connections between the top eigenvalue of the logit Hessian and the impurity of
the probability output.
•	We derive a relation between the sharpness, the top eigenvalue of the logit Hessian and the
Jacobian. We call it sharpness-impurity-Jacobian relation.
•	We explain how the sharpness of the loss landscape influences the learning dynamics and
the generalization performance. In particular, we find that gradient-based optimizations have
implicit effects on penalizing the Jacobian norm (Implicit Jacobian Regularization) in a
certain phase of training (Active Regularization Period).
2	Related Work
We summarize some works on the Hessian, learning dynamics, and generalization of neural networks.
In particular, we point out the issue of approximating SGD by a stochastic differential equation (SDE)
because a continuous flow cannot capture the oscillatory behavior of discrete updates with iterative
catapult, which plays a key role in limiting the sharpness of the loss landscape.
Decomposition of the Hessian Sagun et al. (2016; 2017) empirically found that the eigenvalue
spectrum of the Hessian during training is composed of two parts, the bulk which is concentrated
around zero and the outliers which are scattered positively away from zero. They showed the bulk
depends on the size of the network, and the outliers depend on the data. In particular, the number of
outliers matches the number of classes of the data. Further, Papyan (2019) proposed a three-level
hierarchical decomposition of the Hessian matrix according to each class, logit coordinate, and
example. However, with different decomposition, we analyze the Hessian from another point of view.
SGD as a SDE In many studies, SGD has been understood as a SDE in the limit of vanishing
learning rate (Mandt et al., 2017; Li et al., 2017b;a; Smith & Le, 2018; Chaudhari & Soatto, 2018;
Jastrzebski et al., 2017; Zhu et al., 2019; Park et al., 2019). However, some theoretical concerns have
been raised for such approximations (Yaida, 2019). Moreover, Barrett & Dherin (2021) argued that
the SDE analysis in the limit of vanishing learning rate cannot explain the generalization benefits of
finite learning rates and they proposed a modified gradient flow for finite learning rates. However,
they still consider a continuous gradient flow and thus it has a fundamental limitation in explaining
the oscillatory behavior with iterative catapult in a practical learning rate regime (Smith et al., 2021),
which will be detailed in the following paragraph.
Oscillatory catapult and the plateau of the sharpness Xing et al. (2018) investigated the roles
of learning rate and batch size in SGD dynamics through interpolating the loss landscape between
consecutive model parameters during training. They observed SGD explores the parameter space,
bouncing between walls of valley-like regions. The large learning rate maintains a high valley height,
and a small batch size induces gradient stochasity. They both help exploration through the parameter
space with different roles in the training dynamics. Jastrzebski et al. (2019) empirically investigated
the evolution of the sharpness (the top eigenvalue of the Hessian) along the whole training trajectory
of SGD. They observed initial growth of the sharpness as loss decreases, reaching a maximum
sharpness determined by learning rate and batch size, and then it decreases towards the end of training.
Due to the initial increase of the sharpness, the SGD step becomes too large compared to the shape
of the loss landscape. This is consistent with the valley-like structure shown in Xing et al. (2018).
Lewkowycz et al. (2020) investigated simple theoretical models with a solvable training dynamics.
They showed that, in their setup with a large learning rate, the loss initially increases while the
sharpness decreases, then it converges to a flat minimum. This mechanism is called the catapult
mechanism. Recently, Cohen et al. (2021) found that full-batch GD typically operates in a regime
2
Under review as a conference paper at ICLR 2022
called the Edge of Stability where the sharpness can no longer increase and stays near a certain
value, and the training loss behaves nonmonotonically but decreases globally. This behavior of the
optimization at the Edge of Stability can be seen as repeated catapult mechanisms. They explicitly
marked the limit of the sharpness with 2/n (η =Iearning rate).
To describe the aforementioned evolution of the sharpness, Fort & Ganguli (2019) developed a
theoretical model based on a random matrix modelling. To build a simple random model, they
introduced assumptions about gradients and Hessians that they are i.i.d isotropic Gaussian with zero
mean with varying variance during training. While they focus on building a random model based on
the observation, we rather aim to explain the underlying mechanisms.
Implicit bias in SGD There have been many studies on the implicit bias in SGD (Neyshabur, 2017;
Zhang et al., 2021; Soudry et al., 2018). We review the most relevant and recent ones. Jastrzebski et al.
(2021) empirically showed that SGD implicitly penalizes the trace of the Fisher Information Matrix
(FIM). They also showed the trace of FIM explodes in the early phase of training when using a small
learning rate and called it catastrophic Fisher explosion. Barrett & Dherin (2021); Smith et al. (2021)
demonstrated that SGD implicitly penalizes the norm of the total gradient and the non-uniformity of
the minibatch gradients. We demonstrate that the (logit-weight) Jacobian plays an important role in
the generalization performance in each case.
3	Background
In this section, we provide some notations, basic equations and definitions for the following sections.
Throughout the paper, we use the denominator layout notation for the vector derivatives, i.e., RvU =
(duj) ∈ Rv×u where U : Rv → Ru and V ∈ Rv. It is also generalized to the cases of scalar,
∂vi ij
u = 1 or v = 1.
We consider a problem of learning a C-class classifier which maps an input x ∈ X ⊂ Rd to a target
label y ∈ [C] where [C] = {1,2,…，C}. To this end, we build a parameterized model fθ : X →
Z ⊂ RC with a model parameter θ ∈ Θ ⊂ Rm which outputs a logit vector z ≡ fθ(x) ∈ Z ⊂ RC
(we often omit the dependence on the input x and the model parameter θ). Then, the logit vector
z is given as input to the softmax function to yield a probability vector p = softmax(z) ∈ ∆C-1
where ∆C-1 = {p ∈ [0, 1]C : 1Tp = 1,p ≥ 0}. We want the model to match the most probable
class c1 to the true label y, where c(x) ≡ arg sort(p) in descending order. We exchangeably denote
the probability value corresponding to the true label y as p ≡ py ∈ [0, 1]. The cross-entropy loss,
l = l(z, y) ∈ R, is equivalent to the negative log-likelihood l = 一 logp. We use the notations ∣∣ ∙ ∣∣
for the Euclidean '2-norm of a vector or for the Euclidean operator norm of a matrix (equivalently,
k ∙ ∣σ for a square matrix), ∣ ∙ ∣∣f for the Frobenius norm, and tr(∙) for the trace of a (square) matrix.
Starting with a simple computation of the derivatives of the softmax function, Eq (1) (see Appendix
A), we can easily derive the following equations in order:
Rzp = diag(p) 一 ppT ∈ RC×C	(1)
Rzp = [Rzp]:,y = p(ey 一 p) ∈ RC	(2)
Vzl = VzPd = p(ey 一 P) ∙ - 1= P 一 ey ∈ RC	(3)
∂p	p
Vzl = Vz(Vzl) = Vz(P - ey) = diag(p) - PPT ∈ Rc×c	(4)
where diag(P) = (δijPi)ij ∈ RC×C is a diagonal matrix with P as its diagonal entries, and ei =
(δij)j ∈ RC is a one-hot vector with i-th element as 1.
Next, the Hessian of the loss function l for given example x with respect to the model parameter can
be expressed as follows:
V2θl = VθzV2zlVθzT +XC V2θzjVzj l ≈ VθzV2zlVθzT ∈ Rm×m	(5)
using a well-known Gauss-Newton approximation. (see, for example, Schraudolph (2002)).
Now, we are ready to consider the training loss for the training set D. We compute the total training
loss over D as L = hl} which yields VL = Nvi and V2L =(V2/〉where〈.〉is the expectation over
3
Under review as a conference paper at ICLR 2022
the empirical measure of the training set D, equivalently say ED [∙].We use the notation h)s when
averaging over a subset S . Following from Eq (4) and Eq (5), we define the Hessian matrix H for the
total loss and its Gauss-Newton approximation matrix G with the matrices M and J as follows:
Definition 1. We call M the logit Hessian, J the Jacobian (of the logit function with respect to the
model parameter), H the Hessian, and G the Gauss-Newton approximation defined as follows:
M ≡ VZl = diag(P) — PpT ∈ RC×C	(6)
J(= Jz) ≡Vθz ∈ Rm×C	(7)
H ≡ hV2θli ≈ hJMJTi ≡ G ∈ Rm×m	(8)
It is interesting to note that while l is dependent on the true label y, the logit Hessian M = V2zl is
independent of y, and so are J, JMJT, and G. In case of the MSE loss l = 1 ∣∣z 一 ey ∣∣2, we have
M = V2zl = I and G = hJJTi. We mainly focus on the usual cross-entropy loss and defer the
investigation on the MSE loss to Appendix M. From Eq (8), we will often use the approximation
∣H∣σ ≈ ∣G∣σ as justified in Sagun et al. (2017); Fort & Ganguli (2019), but this approximation
sometimes fails in the later phase of training when the top eigenvalues of the Gauss-Newton matrix
is not sufficiently isolated from the bulk near 0 (Papyan, 2018). Thus we mainly focus on the early
phase of training.
4	DECOMPOSITION OF THE HESSIAN H
In the previous section, we introduced the Gauss-Newton approximation G of the Hessian H , and
decomposition of G with the Jacobian J and the logit Hessian M, i.e., G = hJMJTi. Now, we
focus on the logit Hessian matrix M and its eigendecomposition, estimate the top eigenvalue of M
with upper/lower bounds, and explore the evolution of the top eigenvalue during training.
4.1	Eigendecomposition of the logit Hessian
The lower-dimensional matrix M ∈ RC ×C is simple and fully characterized by only the probability
vector P as M = diag(P) 一 PPT in Eq (6), but it turns out to be important for understanding the
much higher-dimensional matrix G ∈ Rm×m (C m). Since M = diag(P) 一 PPT is a rank-one
modification of a simple diagonal matrix diag(P), we can obtain the eigenvalues and the eigenvectors
from the theory of the rank-one modification of the eigenproblem (see, for example, Bunch et al.
(1978); Golub (1973) and (Golub & Van Loan, 2013, Section 8.4.3)). Then, the logit Hessian M can
be eigendecomposed as M = QΛQT = PiC=1 λ(i)q(i)q(i)T where λ(i) is the i-th largest eigenvalue
of M and q(i) is its corresponding normalized eigenvector. For simplicity, we also use the same
ordered index of (i) ∈ [C] with parentheses for the probability output P ∈ ∆C-1, i.e., ci = (i)
and p(i)≥ p(2)≥ ∙∙∙ ≥ p(c)≥ 0, because this ordering is related to the eigenvalues {λ(i)}C=ι as
demonstrated in the following theorem. We defer the proof to Appendix B.
Figure 1: [Eigenvalues of the logit Hessian] Graph of the secular function v(λ) in Eq (9) which
has zeros at the eigenvalues {λ(i)}iC=1 of M = V2zl (blue curves). We highlighted the singularities
λ = P(i) with red vertical lines. It illustrates Theorem 1 (a) and (c).
4
Under review as a conference paper at ICLR 2022
Theorem 1 (cf. Golub (1973); Bunch et al. (1978)). The eigenvalues λ(i) (λ⑴ ≥ λ(2) ≥ ∙∙∙ ≥ λ(C))
and the corresponding normalized eigenVectors q(i ofthe IogitHessian M = VZ l = diag(P) — PpT
satisfy the following properties:
(a)	The eigenvalue λ(i) is the i-th largest solution of the following equation:
C P2
v(λ) = 1 — ∑. 1-p^V =0	(9)
i=1 Pi — λ
(b)	The eigenvector q(i) is aligned with the direction of (diag(P) — λ(i)I)-1P
(c)	P(i+1) ≤ λ(i) ≤ P(i) for 1 ≤ i ≤ C — 1, and λ(C) = 0
(d)	1 Gini(P(1)) ≤ λ(1) ≤ Gini(p(i)) where Gini(q) = 1 — q2 — (1 — q)2 = 2q(1 — q) is the
Gini impurity for the binary case (q, 1 — q).
Figure 1 illustrates the secular function v(λ) defined as Eq (9) in Theorem 1 (a). The function v(λ)
has singularities at the probability values {P(i)}iC=1 and zeros at the eigenvalues {λ(i)}iC=1, satisfying
Theorem 1 (c). Moreover, Theorem 1 (c) and (d) provide the upper/lower bounds on the eigenvalues
of M. Especially, the top eigenvalue λ⑴ is bounded by ɪGini(P⑴)≤ λ⑴ ≤ Gini(p(i)), and thus
we call it impurity.
4.2 Evolution of Impurity
0.5
0.4
0.0
0.0	0.2	0.4	0.6	0.8	1.0
P(i)
„ 0.3
y o.2
Figure 2: [Evolution of Impurity] The impurity λ(1) increases and then decreases as P(1)
increases during training. Left: The impurity of a typical example is plotted against P(1). We
together plot the upper bound min{p(i), Gini(P⑴)} and the lower bound 2Gini(P⑴)from Theorem
1 (c) and (d). Right: The impurity is plotted against the training step. Blue curve indicates its mean
value hλ(1)i and sky-blue area shows the 25-75% quantile range of the impurity for the training data.
We explore the top eigenvalue λ(1) of M (also referred to as impurity) during training. Figure 2
demonstrates the evolution of the impurity during training, which increases in the beginning and then
decreases in the later phase. We trained a model to zero training loss, and thus, for most examples,
the probability Py for the true class y eventually becomes the highest probability P(1). As the top
probability p(i)increases from 1/C to 1 during training, the impurity starts from λ(1) ≈ C ∈
[2Gini(Ce), -C] = [C-1, -C] (Theorem 1 (c) and (d)), increases at the initial phase of the training
because it is lower bounded by 21 Gini(P⑴)=p(i)(1 — p(i)) which increases for p(i)∈ [0,0.5].
Then λ(1) decreases as P(1) becomes larger than 0.5 which leads λ(1) to almost 0 at the later phase
because it is upper bounded by Gini(P(1)) = 2P(1)(1 — P(1)) which decreases for P(1) ∈ [0.5, 1].
Note that Cohen et al. (2021) tried to estimate a similar value, but they use Py, not P(1). We again
emphasize that M is independent of the label y but dependent only on the probability output P.
5	Implicit Jacobian Regularization
In this section, from the results of the previous sections, we aim to derive a relation between the
sharpness, the impurity and the Jacobian, and answer how the sharpness of the loss landscape
5
Under review as a conference paper at ICLR 2022
influences the learning dynamics and the generalization performance. Detailed experimental settings
for each Figure are described in Appendix E.
Figure 3: The sharpness kHkσ and the Jacobian norm khJ ik2 show similar oscillating behavior up
to a factor * which is locally constant and slowly changes during training (CIFAR-10, η = 0.04,
|B| = 128; left: 0-200, middle: 1000-1200, right: 1500-1700 steps). We highlighted ∣∣H∣∣σ = 2/n
with the dashed horizontal line. Note that they have different right y-axes.
5.1	Sharpness-impurity-Jacobian relation
We first take a closer look at the sharpness of the loss landscape during training and build a relation
between the sharpness ∣H∣σ, the impurity λ(1), and the Jacobian J. Since the Gauss-Newton matrix
G is known to approximate the true Hessian H well, especially for the top eigenspace (Sagun et al.,
2017; Fort & Ganguli, 2019; Papyan, 2019), we can write the sharpness ∣H ∣σ as follows:
∣H∣σ ≈ ∣G∣σ = ∣hJMJTi∣σ	(10)
It implies the impurity ∣M∣σ and the squared norm of the Jacobian J are highly correlated with the
sharpness ∣H ∣σ as demonstrated in the following theorem. We defer the proof to Appendix C.
Theorem 2. For some 0 ≤ λ* ≤ λ(1) for each X ∈ D, we can bound
khJMJ T ikσ = hλ*Jk2i ≤ hλ ⑴ JI2)	(11)
In other words, λ*∣ J∣∣2 has the expected value of approximately ∣∣H∣∣σ, and we briefly state this
relation with λ* ∣ J∣∣2 〜∣∣H∣∣σ. Here, λ* in λ*∣J∣∣2 acts as an adaptive regularization weight
for the regularization on the Jacobian norm ∣J ∣2 as detailed in the following section. Since it is
computationally inefficient to track ∣J∣2 for every x ∈ D, we instead investigate ∣hJ i∣2 . We expect
∣∣H∣∣σ = *∣∣hJ)∣∣2 for some *. In Figure 3, we observe that ∣∣H∣∣σ and ||(J)∣2 show similar
oscillating behavior UP to a factor * which is locally constant and slowly changes during training.
5.2	Implicit Jacobian Regularization
Now, we are ready to answer how the sharpness of the loss landscape influences the learning dynamics
and the generalization performance.
Growing Jacobian and sharpness in the early phase of training The weight norm ∣θ∣ increases
in order to increase the logit norm ∣z ∣ and to minimize the cross-entropy loss during training (Soudry
et al., 2018) (see Appendix F for details). This also leads to the increase in the layerwise weight
norms and the Jacobian norm. Thus, the increase of the sharpness in the early phase of training can
be mainly attributed to the increase of the Jacobian norm ∣J ∣. However, as will be discussed in the
following paragraphs, the Jacobian norm does not continuously increase throughout training.
Oscillatory catapult and the plateau of the sharpness As the sharpness increases in the begin-
ning, the width of the valley of the loss landscape becomes narrower than the discrete step size of the
gradient-based optimization. After the sharpness reaching this threshold, the iterate starts to bounce
6
Under review as a conference paper at ICLR 2022
Figure 4:	Oscillatory catapult in the optimization trajectory {θ(t) } (from blue to red) of full-
batch GD (McInnes et al., 2018). Left: UMAP of the model parameters trained on CIFAR-10 for
the first 500 steps. Right: Zoom-in for the oscillatory steps [100, 300]. After few steps (〜100), the
sharpness reaches a threshold and the iterate shows the oscillatory behavior with iterative catapult.
off from one side of the valley to the other, and repeats this (Xing et al., 2018; JaStrzebSki et al.,
2019). Figure 4 shows this oscillatory behavior with iterative catapult after the sharpness reaching
the threshold, using UMAP (McInnes et al., 2018). Due to the catapult, the iterate cannot stay in
a sharper area and is catapulted to another area. This mechanism is called the catapult mechanism
(Lewkowycz et al., 2020). Figure 3 shows fine-grained patterns that the sharpness oscillates around
the threshold by the two conflicting effects: the Jacobian norm tends to increase the sharpness and
the catapult mechanism reduces it again when the sharpness is over the threshold. Therefore, we can
observe the plateau of the sharpness in a coarser scale (see Figure 5). For GD with the MSE loss,
the threshold of the sharpness can be easily obtained to be 2/n (η = learing rate), and the threshold
value is similar for the cross-entropy loss as shown in Figure 3 (Cohen et al., 2021). The plateau of
the sharpness is also observed in the case of SGD, but with a different threshold value depending
on the batch size (the smaller the batch size, the lower the plateau) as shown in Figure 6 (Right).
This oscillatory catapult and the plateau of the sharpness are attributed to the discrete dynamics of
the gradient-based optimization with a finite learning rate and cannot be described by a continuous
gradient flow.
Implicit Jacobian Regularization (IJR) Due to this catapult effect, the Jacobian norm cannot
continue to increase. In other words, the gradient-based optimization implicitly penalizes the Jacobian
norm since ∣∣ J∣∣2 〜∣∣H∣∣σ/λ /	. This IJR effect begins after the sharpness reaches the
threshold. And the regularization effect diminishes as λ(1) decreases (the upper limit is loosened)
with increasing p(i)≥ 0.5 in the later phase (see Figure 2, 5 and 6) because λ* ≤ λ(1) acts as
a regularization coefficient. This explains why the behavior of the sharpness in the early phase of
training seems to impact the final generalization. Moreover, as λ(1) decreases to a very small value,
we can observe the decrease in the sharpness. However, the test accuracy is saturated, which provides
another counter-example that flat minima generalizes poorly (Dinh et al., 2017).
0.6
0.5
0.2
0.0
0.4 J
0.3 ⅛
test
—train loss
acc ------ test loss
2.0
1.5
S
1.0 °
0.5
0.0
Figure 5:	Three phases of Implicit Jacobian Regularization (IJR). It shows the IJR effects in the
Active Regularization Period (II).
7
Under review as a conference paper at ICLR 2022
Table 1: Explicit Jacobian Regularization (EJR) enhances the test accuracy in various settings.
We report improvement (∆Acc.) and Error Reduction Rate (ERR) on CIFAR-10/CIFAR-100 when
trained with EJR (+EJR), compared to the standard training (Baseline).
Dataset	Network	Batch Size	lr	Regularization	Test Accuracy		∆Acc.	ERR
	Architecture			Coefficient	Baseline	+EJR	(%p)	(%)
			0.003		^6617I	75.40	+8.69	26.10
			0.01		67.88	75.62	+7.74	24.10
		128	0.03	λreg = 0.01	69.83	75.53	+5.70	18.89
			0.1		70.33	74.29	+3.96	13.35
	SimpleCNN		0.3		69.34	73.47	+4.13	13.47
CIFAR-10			0.01		66.81	74.43	+7.62	22.96
		50000	0.03	λreg = 0.001	67.72	74.31	+6.59	20.42
		(full-batch)	0.1		67.53	73.69	+6.16	18.97
			0.3		61.08	72.15	+11.07	28.44
	WRN-28-10	128	0.1	ρreg = 2	96.10±0.05	97.07	+0.97	24.87
				Preg = 2, μreg = 0.03		97.38	+1.28	32.82
CIFAR-100	WRN-28-10	128	0.1	Preg = 5	80.69±o.21	83.73	+3.04	15.74
Figure 5 shows that the gradient-based optimization has implicit regularization effects on the Jacobian
norm in a certain period. To be specific, there are three phases that the Jacobian norm is (I) initially
rapidly increasing before the sharpness reaches near the threshold, (II) actively regularized with a
gentle slope, and (III) again exponentially increasing as the regularization effect diminishes (as the
regularization weight λ* ≤ λ(1) decreases) with the slope being gradually steeper. We call the second
phase Active Regularization Period.
Ooooo
Ooooo
5 4 3 2 1
ssud.IeqS
14000
12000
ioooo t⅛
OZ ue-qouef
Oooo
Oooo
Oooo
8 6 4 2
⅛μnduj-
0 5 0 5 0 5 0
3 2 2r-Ir-IQQ
0.0.0.0.0.0.0.
Figure 6:	The IJR effects vary depending on the hyperparameters used in the training. The
plateau of the sharpness and the n-shaped evolution of the impurity are clearly observed as expected.
We used SGD (Left) with fixed batch size 128 and different learning rates η = 0.01/0.02/0.03 and
(Right) with fixed η = 0.01 and different batch sizes 128/64/32 (solid/dashed/dotted lines) on CIFAR-
10. Training with a large learning rate and a small batch size (dotted line) penalizes the Jacobian
norm more strongly with lower limits of the sharpness. Curves are smoothed for visual clarity.
The evolution of the sharpness of the loss landscape is highly affected by the hyperparameters used in
training such as learning rate and batch size (Jastrzebski et al., 2019; LeWkoWycz et al., 2020; Cohen
et al., 2021). As expected, GD with a large learning rate η limits the sharpness with a lower value of
2∕η. The large learning rate encourages the stronger implicit regularization on the Jacobian norm.
Figure 6 shoWs, comparing the three red lines (solid/dashed/dotted) With different learning rates
(0.01/0.02/0.03) and batch sizes (128/64/32), training With a larger learning rate and a smaller batch
size limits the Jacobian norm With a smaller value in the Active Regularization Period. This could
explain Why training With a large learning rate and a small batch size yields better generalization.
Explicit Jacobian Regularization (EJR) To further investigate and boost the effectiveness of
IJR, We aim to explicitly regularize the Jacobian norm. HoWever, it is computationally hard to
back-propagate through the computation graph of the operator norm of khJ ik2 for a practical neural
netWork even With a simple iterative method (see Algorithm 2 in Appendix E). Thus, We instead
penalize an upper bound, the Frobenius norm khJ ik2F (≥ khJik2), With the regularization coefficient
λreg/C, i.e., we minimize L + λreg∣∣hJikF/C. See Appendix L for its variants with other regular-
8
Under review as a conference paper at ICLR 2022
Step	Step
Figure 7: The effectiveness of EJR (dashed lines) compared to Baseline (solid lines) during training
(Left: CIFAR-10, Right: CIFAR-100) on WRN-28-10. With EJR, it mitigates the overfitting, especially
after each learning rate decay (undesirable decrease/increase of test accuracy/loss).
ization coefficients Preg and μreg. The FrobeniUs regularization term can be efficiently computed
with an unbiased estimator k〈JikF = CEu〜U(SC-I)|||〈Jiu∣∣2] = CEu〜U(SC-1)||“〈uTz〉k2]
where u is randomly drawn from the unit hypersphere SC-1. Since the batch-size is large enough,
We efficiently use a single sample U 〜U(SC-I) for each batch as suggested in Hoffman et al.
(2019). We expect improvements in the generalization performance when introducing EJR. This
would support the effectiveness of IJR that it efficiently controls the capacity of the model. Table 1
shows clear improvements in the test accuracy when introducing EJR.
Connections between the Jacobian and Fisher/Gradient Penalty Our explanation of the implicit
bias in SGD may extend to the catastrophic Fisher explosion (Jastrzebski et al., 2021) with G instead
of the Fisher Information Matrix (FIM). The trace of G can be written as follows:
C-1
tr(G) = htr(JMJT)i = h	λ(i)kJq(i)k2i ≈ htr(M)kJ k2F i/C
i=1
(12)
where we assume kJq(i) k2 ≈ kJ k2F /C since PiC=1 kJ q(i)k2 = kJ k2F (see Figure 11 (Left) in
Appendix D for the empirical evidence). Here, the trace of the logit Hessian M can be equivalently
written as a C-class Gini impurity: tr(M) = PiC=1 pi(1 - pi) = 1 - PiC=1 pi2 ≡ GiniC (p) which
is C-1 for the initial uniform distribution and 0 for a one-hot probability. Thus, penalizing tr(G)
induces the effects of penalizing kJkF, especially in the early phase of training with large GiniC (p).
Thus, as Jastrzebski et al. (2021) argued that Fisher Penalty on the trace of the FIM improves the
generalization performance by limiting the memorization, the Jacobian regularization may have
similar effects. Moreover, since Vθl(z, y) = Vθ闪Zl(z, y) = J(P - ey), the trace of the FIM
they approximately used is simply ∣∣EXZBEy〜p[Vθl(z,y)]k2 = ∣∣EXZBEy〜P[J(p - ey)]k2 with
a single sample y, the gradient norm penalty (Barrett & Dherin, 2021) is ∣∣E(χ,y)zB[J(p - ey)]∣
and the explicit Jacobian regularizer is ∣∣ J∣∣F = CEuZU(Sc-i)|E(χ,y)zB[Ju]∣2. In each case, we
emphasize that the Jacobian J plays an important role in the generalization performance.
6 Conclusion
We have investigated the eigensystem of the Hessian through a new decomposition using eigen-
decomposition of the low-dimensional logit Hessian. By doing so, we could provide a simple and
intuitive explanation on the relation between the gradient-based optimization, the learning dynamics
and the generalization performance of neural networks. We hope this research could help answer
other intriguing questions on the learning dynamics and generalization.
9
Under review as a conference paper at ICLR 2022
Ethics S tatement
No ethics statement is needed for this study.
Reproducibility S tatement
We provide code to reproduce our experiments. We refer the readers to the supplementary material
(README.md).
References
Alessandro Achille, Matteo Rovere, and Stefano Soatto. Critical learning periods in deep networks.
In International Conference on Learning Representations, 2019. URL https://openreview.
net/forum?id=BkeStsCcKQ.
David Barrett and Benoit Dherin. Implicit gradient regularization. In International Confer-
ence on Learning Representations, 2021. URL https://openreview.net/forum?id=
3q5IqUrkcF.
James R Bunch, Christopher P Nielsen, and Danny C Sorensen. Rank-one modification of the
Symmetriceigenproblem. Numerische Mathematik, 31(1):31-48, 1978.
Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational inference, con-
verges to limit cycles for deep networks. In International Conference on Learning Representations,
2018.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs,
Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent
into wide valleys. Journal of Statistical Mechanics: Theory and Experiment, 2019(12):124018,
2019.
Jeremy Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar. Gradient descent on
neural networks typically occurs at the edge of stability. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=jh-rTtvkGeM.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for
deep nets. In International Conference on Machine Learning, pp. 1019-1028. PMLR, 2017.
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization
for efficiently improving generalization. In International Conference on Learning Representations,
2021. URL https://openreview.net/forum?id=6Tm1mposlrM.
Stanislav Fort and Surya Ganguli. Emergent properties of the local geometry of neural loss landscapes.
arXiv preprint arXiv:1910.05929, 2019.
Stanislav Fort, PaWeI Krzysztof Nowak, Stanislaw Jastrzebski, and Srini Narayanan. Stiffness: A
new perspective on generalization in neural networks. arXiv preprint arXiv:1901.09491, 2019.
William Fulton. Eigenvalues, invariant factors, highest weights, and schubert calculus. Bulletin of the
American Mathematical Society, 37(3):209-249, 2000.
Semyon Aranovich Gershgorin. Uber die abgrenzung der eigenwerte einer matrix. H3βecτuπ
Poccuhckoh aκageMuu Hayκ.. CePUaMaTeMaTUVeCKan, (6):749-754, 1931.
Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net optimization
via hessian eigenvalue density. In International Conference on Machine Learning, pp. 2232-2241.
PMLR, 2019.
Gene H Golub. Some modified matrix eigenvalue problems. Siam Review, 15(2):318-334, 1973.
Gene H Golub and Charles F Van Loan. Matrix computations, volume 3. JHU press, 2013.
10
Under review as a conference paper at ICLR 2022
Guy Gur-Ari, Daniel A Roberts, and Ethan Dyer. Gradient descent happens in a tiny subspace. arXiv
preprint arXiv:1812.04754, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
SePP Hochreiter and Jurgen Schmidhuber. Flat minima. Neural computation, 9(1):1T2, 1997.
Judy Hoffman, Daniel A Roberts, and Sho Yaida. Robust learning with jacobian regularization. arXiv
preprint arXiv:1908.02729, 2019.
StanislaW Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio,
and Amos Storkey. Three factors influencing minima in sgd. arXiv preprint arXiv:1711.04623,
2017.
StanislaW Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek Tabor, Kyunghyun
Cho*, and Krzysztof Geras*. The break-even point on optimization trajectories of deep neural
netWorks. In International Conference on Learning Representations, 2020. URL https://
openreview.net/forum?id=r1g87C4KwB.
StanislaW Jastrzebski, Devansh Arpit, Oliver Astrand, Giancarlo B Kerg, Huan Wang, Caiming Xiong,
Richard Socher, Kyunghyun Cho, and Krzysztof J Geras. Catastrophic fisher explosion: Early
phase fisher matrix impacts generalization. In Marina Meila and Tong Zhang (eds.), Proceedings of
the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine
Learning Research, pp. 4772-4784. PMLR, 18-24 Jul 2021. URL https://proceedings.
mlr.press/v139/jastrzebski21a.html.
Stanislaw Jastrzebski, Zachary Kenton, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amost
Storkey. On the relation betWeen the sharpest directions of DNN loss and the SGD step length. In
International Conference on Learning Representations, 2019. URL https://openreview.
net/forum?id=SkgEaj05t7.
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantas-
tic generalization measures and where to find them. In International Conference on Learning
Representations, 2020. URL https://openreview.net/forum?id=SJgIPJBFvH.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In 5th
International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.
net/forum?id=H1oyRlYgg.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large
learning rate phase of deep learning: the catapult mechanism. arXiv preprint arXiv:2003.02218,
2020.
Chris Junchi Li, Lei Li, Junyang Qian, and Jian-Guo Liu. Batch size matters: A diffusion approxima-
tion framework on nonconvex stochastic gradient descent. stat, 1050:22, 2017a.
Qianxiao Li, Cheng Tai, and E Weinan. Stochastic modified equations and adaptive stochastic
gradient algorithms. In International Conference on Machine Learning, pp. 2101-2110. PMLR,
2017b.
Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Toward a theory of optimization for over-parameterized
systems of non-linear equations: the lessons of deep learning. arXiv preprint arXiv:2003.00307,
2020.
Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Differentiable architecture search. In
International Conference on Learning Representations, 2019. URL https://openreview.
net/forum?id=S1eYHoC5FX.
11
Under review as a conference paper at ICLR 2022
Stephan Mandt, Matthew D Hoffman, and David M Blei. Stochastic gradient descent as approximate
bayesian inference. Journal ofMachine Learning Research, 18:1-35, 2017.
Leland McInnes, John Healy, Nathaniel Saul, and Lukas GroBberger. Umap: Uniform manifold
approximation and projection. Journal of Open Source Software, 3(29):861, 2018. doi: 10.21105/
joss.00861. URL https://doi.org/10.21105/joss.00861.
Behnam Neyshabur. Implicit regularization in deep learning. arXiv preprint arXiv:1709.01953, 2017.
Vardan Papyan. The full spectrum of deepnet hessians at scale: Dynamics with sgd training and
sample size. arXiv preprint arXiv:1811.07062, 2018.
Vardan Papyan. Measurements of three-level hierarchical structure in the outliers in the spectrum
of deepnet hessians. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of
the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine
Learning Research, pp. 5012-5021. PMLR, 09-15 Jun 2019. URL https://proceedings.
mlr.press/v97/papyan19a.html.
Daniel Park, Jascha Sohl-Dickstein, Quoc Le, and Samuel Smith. The effect of network width on
stochastic gradient descent and generalization: an empirical study. In International Conference on
Machine Learning, pp. 5042-5051. PMLR, 2019.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical
statistics, pp. 400-407, 1951.
Levent Sagun, Leon Bottou, and Yann LeCun. Eigenvalues of the hessian in deep learning: Singularity
and beyond. arXiv preprint arXiv:1611.07476, 2016.
Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of the
hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017.
Nicol N Schraudolph. Fast curvature matrix-vector products for second-order gradient descent.
Neural computation, 14(7):1723-1738, 2002.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning
Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings,
2015. URL http://arxiv.org/abs/1409.1556.
Samuel L Smith and Quoc V Le. A bayesian perspective on generalization and stochastic gradient
descent. In International Conference on Learning Representations, 2018.
Samuel L Smith, Benoit Dherin, David Barrett, and Soham De. On the origin of implicit regularization
in stochastic gradient descent. In International Conference on Learning Representations, 2021.
URL https://openreview.net/forum?id=rq_Qr0c1Hyo.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit
bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):
2822-2878, 2018.
Huan Wang, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. Identifying generalization
properties in neural networks. arXiv preprint arXiv:1809.07402, 2018.
Hermann Weyl. Das asymptotische verteilungsgesetz der eigenwerte linearer partieller differentialgle-
ichungen (mit einer anwendung auf die theorie der hohlraumstrahlung). Mathematische Annalen,
71(4):441-479, 1912.
Chen Xing, Devansh Arpit, Christos Tsirigotis, and Yoshua Bengio. A walk with sgd. arXiv preprint
arXiv:1802.08770, 2018.
Sho Yaida. Fluctuation-dissipation relations for stochastic gradient descent. In International Confer-
ence on Learning Representations, 2019. URL https://openreview.net/forum?id=
SkNksoRctQ.
12
Under review as a conference paper at ICLR 2022
Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W Mahoney. Pyhessian: Neural networks
through the lens of the hessian. In 2020 IEEE International Conference on Big Data (Big Data),
pp. 581-590. IEEE, 2020.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,
2016.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep
learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107-115,
2021.
Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. The anisotropic noise in stochastic
gradient descent: Its behavior of escaping from sharp minima and regularization effects. In
International Conference on Machine Learning, pp. 7654-7663. PMLR, 2019.
13
Under review as a conference paper at ICLR 2022
A Proof of Eq (1)
VzP = diag(p) - PpT ∈ RC × C
(1)
Proof. By definition of the softmax function,
exp(zj)	1
Pj = [softmax(z)]j = -.......=-- = exp(zj )s 1
k exp(zk)
(13)
where s = k exp(zk), we have
VziPj
-exp(zj )s-2exp(zi) = -PiPj ,
-exp(zi)s-2exp(zi) + exp(zi)s-1 = -Pi2 + Pi,
ifi 6=j
ifi=j
(14)
which leads to VzP = (Vzi Pj)ij = -PPT + diag(P).
□
B Proof of Theorem 1
Theorem (restated). The eigenvalues λi (λ(1) ≥ λ(2) ≥ ∙∙∙ ≥ λ(C)) and the corresponding
normalized eigenvectors q(i) of the logit Hessian M = V2zl = diag(P) - PPT satisfy the following
properties:
(a)	The eigenvalue λ(i) is the i-th largest solution of the following equation:
VC) = I- XC=1 Pip-I = 0	⑼
(b)	The eigenvector q(i) is aligned with the direction of (diag(P) - λ(i)I)-1P
(c)	P(i+1) ≤ λ(i) ≤ P(i) for 1 ≤ i ≤ C - 1, andλ(C) = 0
(d)	1 Gini(P(1)) ≤ λ(1) ≤ Gini(P⑴)where Gini(q) = 1 — q2 — (1 — q)2 = 2q(1 — q) is the
Gini impurity for the binary case (q, 1 - q).
Proof. The eigenvalues λ(i) of M = diag(P) - PPT are the zeros of the following characteristic
polynomial:
φM (λ) = det(diag(P) -PPT - λI)
= det(diag(P) - λI) det(I - (diag(P) - λI)-1PPT)
= Y(Pi-λ)(1 - X P⅛)
(15)
(16)
(17)
where the second equality follows from A - PPT = A(I - A-1PPT) with the matrix A =
diag(P) - λI, and the third inequality holds because det(I + uvT) = 1 + uTv for vectors u and v.
Then it is equivalent to solving the following equation:
C	P2
V(X) =1 - X —X = 0	(18)
i=1Pi-λ
which implies (a). Note that this result also implies (c) as shown in Figure 1.
Next, to prove (b), put A ≡ diag(P) - λI and q ≡ A-1P. Then it is required to show that
(M - λI)q = (A - PPT)q = 0 for the eigenvalues λ= λ(i). We have
(A - PPT )q = (A - PPT )A-1P = P - PPT A-1 P	(19)
14
Under review as a conference paper at ICLR 2022
Here,	(ppTA-1p)i	= Pj,k	pipjAj-k1pk	= Pj,k	pipjδjk(pj	-	λ)-1pk	= Pk pipk (pk	-
λ)-1pk = pi Pk p2k/(pk - λ) = pi. The last equality holds for the eigenvalues λ = λ(i) which
follows from (a).
Now, we want to prove the statement (c). Since
λ(i) (C) ≤ λ(j)(A) +λ(k)(B)ifk+j -i = 1	(20)
λ(i) (C) ≥ λ(j)(A) +λ(k)(B)ifk+j -i = C	(21)
for C = A + B ∈ RC×C where λ(i)(D) is the i-th largest eigenvalue of a matrix D (Weyl, 1912;
Fulton, 2000), we can get λ(i)(C) ≤ λ(i)(A) +λ(1)(B) and λ(i) (C) ≥ λ(i+1)(A) + λ(C-1) (B).
Thus, for A = diag(p) and B = -ppT, we can get
p(i+1) ≤ λ(i)(M) ≤ p(i) for1 ≤i ≤ C- 1	(22)
since λ(i) (A) = p(i), λ(i+1) (A) = p(i+1) and λ(1)(B) = λ(C-1) (B) = 0. Moreover, since
M1 = p - ppT1 = p - p Pi pi = 0, the smallest eigenvalue is λ(C) = 0.
Lastly, we prove the statement (d). From the Gershgorin circle theorem (Gershgorin, 1931), we have
λ⑴ ∈ [B(Mii, X IMijI)= B(P(1)(1 - P(1)),P(1)(1 - P(I))) = [0, 2P(1)(1 - P(I))]	(23)
which implies λ(1) ≤ 2p(1)(1-p(1)).Notethatp(1)(1-p(1)) ≥ p(i)(1-p(i)) sinceg(t) = t(1-t)
is increasing for 0 ≤ t ≤ 0.5. In detail, if P(1) ≥ 0.5, since P(i) ≤ 1 - P(1) ≤ 0.5, we have
g(P(i)) ≤ g(1 - P(1)) = g(P(1)). Otherwise (P(1) < 0.5), since P(i) ≤ P(1), it leads to the same
inequality g(P(i)) ≤ g(P(1)). With the Rayleigh principle, we can express the largest eigenvalue as
λ(1) = maxkuk2=1 uT Mu, and thus e(1)T Me(1) = M(1)(1) = P(1)(1 - P(1)) ≤ λ(1).
□
C Proof of Theorem 2
Theorem (restated). For some 0 ≤ λ* ≤ λ(1) for each X ∈ D, we can bound
khJMJ T ikσ = hλ*Jk2i ≤ hλ ⑴ JI2)	(11)
Proof. We start with the Rayleigh principle:
khJMJT ikσ = max qT hJMJT iq = max hqT JMJT qi	(24)
kqk=1	kqk=1
Since M = Pi λ(i)q(i)q(i)T , we can continue by putting v = JT q, and then
Eq(24) = max hvT Mvi = max hX λ(i) (q (i)T v)2i	(25)
kqk=1	kqk=1 i
Then by putting λ = Pi γ(i)λ(i) with γ(i) = (q(i)Tv)2/Pi(q(i)τv)2 ≥ 0 (Pi Yi = 1),
Eq(25) = max (λ X(q(i)τv)2) = max (λ X(q(i)τ JTq)2)	(26)
kqk=1	i	kqk=1 i
=Im aχhλ X(q(i)τ (J T q))2i	(27)
Since {q(i)}iC=1 is an orthonormal basis of RC (eigenvectors of a symmetric matrix M), we have the
following by putting λ* = JlJq2k λ with q* = arg maxkqk=ι hλkJ T qk2),
Eq(27) = max hλkJ T qk2) = hλkJ T q*k2) = hλ*∣J ∣2)	(28)
kqk=1
15
Under review as a conference paper at ICLR 2022
Because of the definition of λ* and λ with ∣∣q*k = 1 and Pi Y (i) = 1 (γ(i) ≥ 0), We have
λ* ≤ λ ≤ λ⑴	(29)
which leads to the final inequality in Eq (11). The equality holds when ∣∣ JTq*∣ = ∣∣ J∣∣, Y⑴=1
and γ(i) = 0 (i 6= 1) for all x ∈ D.
□
16
Under review as a conference paper at ICLR 2022
D Gradient descent in the top Hessian subspace
Gur-Ari et al. (2018) showed that the gradient of the loss quickly converges to a tiny subspace spanned
by a few top eigenvectors of the Hessian after a short training. Then, the top Hessian subspace does
not evolve much, which implies gradient descent happens in a tiny subspace. However, the underlying
mechanism has not been fully understood.
Direction of q(i) and two salient elements We investigate the direction of the eigenvector q(i)
(1 ≤ i ≤ C - 1) of M. The eigenvector
q(i)
α
(30)
can be obtained from Theorem 1 (b) for some α > 0. Here, the magnitude of the denominator
|pj - λ(i) | is small for the two indices j = (i), (i + 1), and is large for the others. This is because
the eigenvalue λ(i) lies between p(i+1) and p(i) (Theorem 1 (c)). Therefore, the eigenvector q(i) has
a relatively large positive value in q((ii)) and a large negative value in q((ii+) 1) compared to the other
components.
(1)-	0.44	0.10	0.05	0.03	0.02	0.01	0.01	0.01	0.00	0.00
(2)-	-0.27	0.25	0.07	0.03	0.02	0.01	0.01	0.01	0.00	0.00
--	-0.08	-0.23	0.17	0.05	0.02	0.01	0.01	0.01	0.00	0.00
--	-0.04	-0.05	-0.18	0.13	0.04	0.02	0.01	0.01	0.00	0.00
--	-0.02	-0.03	-0.05	-0.16	0.09	0.03	0.01	0.01	0.01	0.00
--	-0.01	-0.01	-0.02	-0.04	-0.13	0.07	0.02	0.01	0.01	0.00
--	-0.01	-0.01	-0.02	-0.02	-0.04	-0.12	0?05-	0.01	0.01	0.00
--	-0.01	-0.01	-0.01	-0.01	-0.01	-0.02	-0.09	0.04	0.01	0.00
(C-I)-	-0.00	-0.00	-0.01	-0.01	-0.01	-0.01	-0.02	-0.08	0.02	0.00
(C)-	-0.00	-0.00	-0.00	-0.00	-0.00	-0.00	-0.01	-0.01	-0.06	0.00
l0.4
-0.3
-0.2
-0.1
-0.0
--0.1
--0.2
L -0.3
V -0.4
Figure 8: HeatmaP of the matrix QΛ1/2 = [√λ(1)q(1);一；√λ(C-1)q(c-1); 0] averaged over the
training set D where M = QΛQT. Each column of QΛ1/2 visualizes the color-encoded direction
of q(i) multiplied by √λ(i). We highlighted the elements q(；) and q(；+i)with the dashed boxes for
0 ≤ i ≤ C - 1 (see Theorem 1 (b)).
Figure 8 shows the directions of q(i) with the heatmap of the matrix QΛ1/2 where QΛ1/2 =
[√λ(1)q(1);…；√λ(Cτ)q(C-D; 0] ∈ RC×C for M = QΛQt. As expected, considering each
column of QΛ1/2, the eigenvector q(i) is colored in red (+) at q(；) and in blue (-) at q(；+i)for
1 ≤ i ≤ C - 1. The two salient elements are highlighted with the dashed boxes.
Direction of Jq(i) and margin maximization In light of the previous discussion, the direction of
Jq⑴=(Jq⑴)∣θ=θ(t) = Vθ (q⑴(θ⑴)Tz(θ)) |**)∈ Rm	(31)
is approximately a direction maximizing q((ii))z(i) + q((ii+) 1)z(i+1) at the current parameter θ(t) ∈
Θ because the other terms are relatively small. In other words, it tends to maximize the margin
z(i) - z(i+1) in the logit space Z between the two classes (i) and (i + 1) (see Figure 8). In particular,
Jq(1) is approximately a direction that maximizes the margin between the most likely class and the
second most likely class.
17
Under review as a conference paper at ICLR 2022
airplane
1.00
-0.07-0.12 -0.02-0.01 0.06 0.02 -0.02
0.04
automobile -0.07
1.00
0.05 -0.05 0.02 -0.05 -0.02 0.00 -0.02
-0.51
bird -0.12 0.05
1.00
ip = 0.53
P = 0.55^ I
-0.08 暑 0.11-0.10
cat -0.02 -0.05 -0.08
deer -0.01 0.02
-0.02
1.00
1.00
-0.01-0.08 -0.02
-0.02-0.32 -0.19-0.03 0.06 0.04
0.00^^-0.19-0.05 -0.01
10.75
050
-0.25
dog
-0.06 -0.05 -0.11-0.32 0.00
1.00
-0.15
-0.06 0.00 0.08
frog
-0.02 -0.02 -0.10-0.19^g-0.15∣
1.00
0.08 -0.01 -0.04
horse -0.02 0.00 -0.01-0.03 -0.19-0.06 0.08
1.00
0.06 0.04
ship-0.02-0.08
truck-0.04	-0.02
0.06 -0.05 0.00 -0.01 0.06
0.04 -0.01 0.08 -0.04 0.04
1.00
-0.19
-0.19
1.00
ds
8s」0ll
60
6op
⅛u
p±q
8=q0E0e
8ud±e
-0.00
--0.25
Γ -0.50
I- -0.75
Figure 9: There are C clusters of m = vzλ(I)Jq(I) according to the most likely class (not the
true class). Left: (Within-class similarity) Directional data of m from Di and Dj for the two
classes, dog (blue) and automobile (red). They are projected onto the 2D-plane spanned by the two
mean vectors indicated with the arrows. We highlight the MRL ρ for each class. The directional
data of m are concentrated within the class but separated from each other. Right: (Between-class
dissimilarity) Cosine similarities between each pair of {mi}. They are mostly orthogonal, but some
pairs are even negatively aligned, for example, automobile and truck. This is because the examples
predicted to be automobile mostly have the second most probable class as truck.
Clustering ofJq(1) and the most probable class We first define following subsets of the training
set according to the most probable class (and the second most one): Di = {x ∈ D : c1 (x) = i} ⊂ D
and Dij = {x ∈ D : c1(x) = i, c2(x) = j} ⊂ D for i 6= j ∈ [C]. Note that Di = Sj6=i Dij. Given
two examples from Dij , theirJq(1) are expected to be highly aligned to each other. This is because
the direction ofJq(1) is approximately a direction of maximizing the margin and of learning the
features to discriminate the class i from the class j . Moreover, two examples from Di also have
highly-alignedJq(1). Figure 9 (Left) shows the concentration of the directional data ofJq(1) from
Di . We also compute the mean resultant length (MRL) to measure the concentration. The MRL ρ of
the directional variable V ∈ Sm-1 ≡ {v ∈ Rm : kvk = 1} defined as ρ ≡ kE[V]k ∈ [0, 1] indicates
how V is distributed (the higher, the more concentrated).
Now, we focus on m ≡ λλ)JJq)ia as the other λ(i)-terms are dominated by the λ(1)-term after a
few epochs (see Appendix J for details). Then, we follow a similar approach from (Papyan, 2019)
and provide the following equation:
CC
hmm i = E. YihmmTiDi = E,	Yi(mlrnιτ + h(m - mi)(m - mi)τiDi)	(32)
i=1	i=1
where Yi = ∣D∕∕∣D∣ and mi = hm〉Di. Here, the covariance term h(m - mi)(m - mi)τiDi is
weak as m is concentrated within Di , and thus we can roughly approximate G with PiC=1 YimimiT.
This implies that the top eigensubspace of the Hessian highly overlaps with the at most C-dimensional
subspace spanned by {mi}C=ι. Figure 9 (Right) demonstrates that the mean vectors {mi}C=ι are
well separated from each other. This also implies the outliers in the Hessian spectrum (Sagun et al.,
2016; 2017).
Why gradient descent happens mostly in the top Hessian subspace? Given input x, after the
model becomes to correctly predict the true label y, the gradient descent direction -g = J(ey - p)
used in the training tends to be highly aligned with Jq(1). This is because ey - p and q(1) both have
similar direction. They have positive values 1 - py and q((11)) in y(= (1))-th element, negative value
-pi and qi(1) in the others, and especially large negative value for the second most probable class
i = (2). Figure 10 (Middle) shows the cosine similarity between the gradient descent direction -g
and Jq(1). As expected, they are highly aligned with the cosine similarity near 1 as the two vectors
ey - p and q(1) become more aligned to each other.
18
Under review as a conference paper at ICLR 2022
",sσl
Uc-Woo
Uc-Woo
30
*juφulu-3Jedsqns
Figure 10: Left: Total gradient gD = hgi is aligned with the top eigenvector r of the Hessian
H at each step during training (Jastrzebski et al., 2019; Gur-Ari et al., 2018). They have large
cosine similarities considering that they are very high-dimensional. We highlighted the cosine value
for random m-dimensional vectors in Θ with the dashed horizontal line (about 1e-3). Middle: J q(1)
(or m) is highly aligned with the gradient g for given example at each step during training.
They have cosine similarities near 1 as the model becomes to correctly predict the true label. See
Figure 2 (Right) together. Right: Total gradient gD and the top eigenvector r of the Hessian
H mostly lie in the at most C-dimensional subspace S spanned by {mi}乙.The subspace
alignment measure AS is defined in Eq (33).
Next, We move on to the subspace S ≡ span({mi}C=J spanned by {m,}^=ι. As each m =
λλ)JJq)1i is highly aligned with -g, it is reasonably expected that the total gradient gD lies in the
subspace S. To measure hoW much the vector v 6= 0 is aligned With the subspace S, We define the
cosine similarity between the vector v and its projection PS (v) onto the subspace S as follows:
AS (v) ≡ cos(v,PS(v)) = kPS (v)k/kvk	(33)
In particular, AS(v) = 0 means v ∈ S⊥ and AS (v) = 1 means v ∈ S. Figure 10 (Right) shows
the high alignment of the total gradient gD in the subspace S ≡ span({mi}C=ι) although the
subspace S is of dimension at most C m. Moreover, since G can be roughly approximated by
C
Ei=I YimimiT, the top eigenvector r of the HeSSian H is also highly aligned with the subspace S.
Figure 11: Histograms of PJkJq(I)\/。and cos2(Jq(i), Jq(j)).
19
Under review as a conference paper at ICLR 2022
E Experimental settings
E.1 DATA
We use the CIFAR-10 dataset (Krizhevsky et al. (2009), https://www.cs.toronto.edu/
~kriz/cifar.html) and the MNIST dataset which have C = 10 number of classes. We also
conduct some experiments on the CIFAR-100 dataset with the number of classes C = 100. We mostly
do not use the data augmentation for training (1) not to introduce the randomness in the training loss
and (2) to allow the training loss to converge to a small value. In case of the WRN-28-10 (Zagoruyko
& Komodakis, 2016), we use the data augmentation to improve the performance.
E.2 Network architectures
We use the following models: VGG-11 (VGG) (Simonyan & Zisserman, 2015) without batch-
normalization, VGG for CIFAR-100 (VGG-CIFAR-100), ResNet-20 (ResNet) (He et al., 2016)
wihtout batch-normalization, a 6-layer CNN (6CNN), SimpleCNN used in Jastrzebski et al.
(2021) (SimpleCNN) two 3-layer fully-connected networks (3FCN-CIFAR and 3FCN-MNIST),
and WRN-28-10 for CIFAR-10/CIFAR-100 with the number of model parameters, m =
9750922, 9797092, 268346, 511926, 361706, 656810, 199210, 36479194, 36536884, respectively.
We use a modified version of the implementation of VGG-11 from https://github.com/
chengyangfu/pytorch-vgg-cifar10/blob/master/vgg.py without the dropout
layers and ResNet-20 from https://github.com/locuslab/edge-of-stability/
blob/github/src/resnet_cifar.py. We change the last linear layer for the CIFAR-100
dataset. The 6CNN model can be expressed in the Pytorch code as follows:
nn.Sequential(
nn.Conv2d(3, 32,
3,
stride=1, padding=1, bias=False)
nn.ReLU(),
nn.Conv2d(32,
nn.ReLU(),
nn.Conv2d(32,
nn.ReLU(),
nn.Conv2d(64,
nn.ReLU(),
nn.Flatten(),
32,
64,
64,
4,
3,
4,
nn.Linear(4096, 100,
stride=2,
stride=1,
stride=2,
padding=1,
padding=1,
padding=1,
bias=True),
bias=False)
bias=False)
bias=False)
nn.ReLU(),
nn.Linear(100, 10, bias=True),
)
and the 3FCN architecture is as follows:
nn.Sequential(
nn.Flatten(),
nn.Linear(n, 200, bias=True),
nn.ReLU(),
nn.Linear(200, 200, bias=True),
nn.ReLU(),
nn.Linear(200, 10, bias=True),
)
where n=784 for 3FCN-MNIST, and n=3072 for 3FCN-CIFAR (the same one used in Cohen et al.
(2021)).
E.3 Hyperparameters
SGD (Robbins & Monro, 1951) with the learning rate η can be expressed as follows:
θ(t+1) = θ(t) - ηgB(t)(θ(t))	(34)
where θ(t) is the model parameter, B(t) ⊂ D is the training batch at t-th step, g = Vθl, and
gB = hgiB . We mostly use the simplest form of (S)GD as described in Eq (34) without momentum,
20
Under review as a conference paper at ICLR 2022
weight decay, and learning rate decay. The only exception is when we train WRN-28-10, we used
the momentum of 0.9, the weight decay of 0.0005, and the learning rate decay of 0.2 in [30%, 60%,
80%] of the whole training epochs.
E.4 Detailed settings for each Figure
For Figure 2, 9, and 10, we ran the experiments on the CIFAR-10 dataset using 6CNN and trained the
model using GD with the learning rate η = 0.04. See Appendix H for the setting used in Figure 6.
We also plot variants of the figures in the main paper for other settings (training steps, data, network
architectures, and hyperparameters) in Appendix J-L.
E.5 Hessian
When computing the top eigenvalue kHkσ and the corresponding eigenvector r of the Hessian H, we
use the tool developed in PyHessian (Yao et al. (2020), https://github.com/amirgholami/
PyHessian, MIT License) based on power iteration method using a small subset (5-25%) of training
dataset D.
E.6 Power iteration algorithm
Even though we have the secular function v(λ) in Eq (9) and the algorithms for computing the
eigenvectors (Bunch et al., 1978), we use the power iteration in Algorithm 1 to get the top eigenvalue
λ(1) of the logit Hessian M ∈ RC ×C since we can run the algorithm for a mini-batch in parallel.
Then, we can compute the corresponding top eigenvector q(1) from Theorem 1 (b). To compute the
second largest eigenvalue, we apply the power iteration to M0 ≡ M - λ(1)q(1)q(1)T instead of M
after computing λ(1) and q(1) .
Algorithm 1 Power iteration
Input: matrix M, maximum iteration nmax, tolerance bound
Output: the spectral norm λ(1) = kMkσ of the matrix M
Initialize u ∈ RC with a random vector.
i — 0
repeat
V — Mu/kMuk
U J MTv/kMTVk
i J i +1
until it converges within the tolerance bound or i ≥ nmax
return VTMu
We also compute the operator norm of the Jacobian khJik with the power iteration as in 2. It
requires(C + 1)-times scalar function differentiations with respect to θ for each iteration.
21
Under review as a conference paper at ICLR 2022
Algorithm 2 Power iteration for Jacobian
Input: logit function zθ (not matrix hJ i), maximum iteration nmax, tolerance bound
Output: the operator norm khJik of the Jacobian
Initialize u ∈ RC with a random vector.
i	J 0
repeat
Compute〈J〉u = Vθ(uTZi (1× scalar function differentiation)
v J hJiu/khJiuk
Compute (JiTV = (VT(Ji)T = [vTVθ(z。,…，VTVθ(zc)]T (CX scalar function differen-
tiations)
u J (JiTV/k(JiTVk
iJi+1
until it converges within the tolerance bound or i ≥ nmax
return VT ((J iu)
22
Under review as a conference paper at ICLR 2022
F	The tendency of the Jacobian norm to increase
The weight norm kθ(t) k increases (Figure 12 (the third one)) in order to increase the logit norm
kz(θ(t))k (Figure 12 (the leftmost figure)) and to minimize the cross-entropy loss during training
(Soudry et al., 2018). This also leads to the increase in the layerwise weight norms (Figure 14) and
the Jacobian norm (Figure 6). While the Jacobian norm tends to increase, it stops increasing at a
certain point, stays for a few steps, and increases again. We ran the experiments on the CIFAR-10
dataset and 6CNN using GD with learning rate η = 0.04.
O 2000	4000
Step
O 2000	4000
Step
O 2000	4000
Step
108 6 4 2 O
--s⅛l≡φ--
O 2000	4000
Step
Figure 12:	(Left to Right): the logit norm hkz(θ(t))ki, the absolute value of the logit sum
h∣1Tz(θ(t))∣i, the weight norm θ(t), the distance from the initial weight ∣∣θ(t) - θ(O)k during
training (every 10 steps). See together with Figure 6 (Left, solid red line) and Figure 5.
50
Step
100
0.6
® 0.4
£
Q>
= 0.2
0.0
O
50
Step
IOO
Figure 13:	(Left to Right): the logit norm h∣z(θ(t))∣i, the absolute value of the logit sum
h∣iτz(θ(t))∣i, the weight norm ∣∣θ(t)∣, the distance from the initial weight ∣∣θ(t) - θ(O)Il in
the early phase of training (every step).
85
80
C 75
70
65
O
Step
2000	4000
Step
Figure 14:	The layerwise weight norms (6 layers from left to right and from top to bottom) of
the 6CNN model in the early phase of training (every 10 step).
23
Under review as a conference paper at ICLR 2022
G kHkσ W⑴ikhJH2
Surprisingly, We empirically observed that ∣∣H∣∣σ 8 hλ(1)ikhJ何||2 during training where ^ =
1∕√C ∈ RC. Since it requires further theoretical grounding, we left it as future work. We observe
this relation for a variety of learning rates (Figure 15), network architectures (Figure 16 and 17),
batch sizes (Figure 18 and 19), and datasets (Figure 20 and 21). At least, ∣H∣∣σ and (λ(I)i∣hJi!L∣2,
they increase and decrease together. We emphasize that Figure 21 shows the case when the sharpness
did not reach the limit 2∕η. This is because the learning rate is relatively low and it is easy to train a
model for the MNIST dataset, and thus hλ(1)i decreases before the sharpness reaches the limit.
200
175
150
125
100
75
50
25
0
一 ∣∣H∣∣σ i Λ<1⅛i∣∣2
0
1000	2000	3000	4000
Step
Step	Step	Step
Figure 15: The relation ∣∣H∣∣σ ɑ hλ⑴〉k〈J川|2 on the CIFAR-10 dataset and the 6CNN model
trained using GD with different learning rates η = 0.02/0.04/0.08 (from left to right). Bottom
figures are plotted for the early phase of training. Top figure is plotted for η = 0.02. Curves are
plotted for every step.
24
Under review as a conference paper at ICLR 2022
Figure 16:	The relation ∣∣H∣∣σ ɑ hλ(1)ikhJ日||2 on the CIFAR-10 dataset and the VGG model
trained using GD with different learning rates η = 0.04/0.08 (left/right) in the early phase of
training. Curves are plotted for every step (Top: 0-1000 stpes and Bottom: 500-1000 stpes).
Figure 17:	The relation ∣∣H ∣∣σ 8 hλ(1)i∣hJ 川|2 on the CIFAR-10 dataset and the ResNet model
trained using SGD with learning rate η = 0.04 and batch sizes |B(t) | = 128 during training
(0-1000 steps). Curves are plotted for every step.
25
Under review as a conference paper at ICLR 2022
Figure 18: The relation ∣∣H∣∣σ H hλ(1)ikhJ川|2 on the CIFAR-10 dataset and the 6CNN
model trained using SGD with fixed learning rate η = 0.04 and different batch sizes |B(t) | =
512/128/32 (from left to right) in the initial phase (0-500 steps). Note that the proportionality con-
stant may change according to the batch size (the smaller the batch size, the larger the proportionality
constant). Curves are plotted for every step.
——∣∣H∣k	—— Λ(1)∣μi∣∣2
Step
——IIHl一一八叫向∣2
Step
Figure 19: The relation ∣∣H∣∣σ h hλ(1)ik(J日||2 on the CIFAR-10 dataset and the VGG model
trained using SGD with fixed learning rate η = 0.04 and different batch sizes |B(t) | =
512/128/32 (from left to right) during training (0-100 epochs). Curves are plotted for every
n steps (n = 97/388/1552) where 97 = b50000/512c.
26
Under review as a conference paper at ICLR 2022
Figure 20: The relation ∣∣H∣∣σ ɑ hλ(1)ikhJ^∣∣2 on the CIFAR-100 dataset and the VGG
model trained using SGD with fixed learning rate η = 0.1 and different batch sizes |B(t) | =
128/64/32 (from left to right) in the early phase of training. Note that the proportionality constant
may change according to the batch size (the smaller the batch size, the larger the proportionality
constant). Curves are plotted for every step (Top: 0-1000 stpes and Bottom: 500-1000 stpes).
Figure 21:	The relation ∣∣H∣∣σ <x (λ⑴i∣(Ji^∣2 on the MNISTdataSetandthe 3FCN-MNIST
model trained using GD with different learning rates η = 0.02/0.04/0.08 (from left to right)
in the early phase of training. Note that the sharpness did not reach the limit 2/n (the dashed
horizontal line). Curves are plotted for every step.
27
Under review as a conference paper at ICLR 2022
H	Implicit Regularization on ∣∣hJi^∣2
We ran the experiments for Figure 2 on the CIFAR-10 dataset. We used 6CNN and VGG for Figure 2
(Left) and Figure 2 (Right), respectively.
We further investigate the relation ∣∣H ∣∣σ 8 hλ(1)i∣hJ)^ ∣2 in the previous section and its effect on
kh Ji^ k2. For the early phase of training, it is hard to see the initial rapid growth of the sharpness in
this smoothed curves and when exactly the regularization begins to activate. We refer the readers to
the previous section, Appendix G, for the fine-grained analysis of the early phase of training. We
provide plots with different settings. Again, we observe that training with a larger learning rate and a
smaller batch size limits ∣∣hJii∣2 with a smaller value (dotted red lines) in the Active Regularization
Period. Curves are smoothed for visual clarity.
——ll∕i∣l2
2 1111
IiLiou -假匕。①∞
一 IlHlICZ
ZEoN
60
——4口)
0.30
①-e>u①6-山
15
Figure 22:	The evolution of ∣∣H∣∣σ, k〈J4∣∣2, and hλ(1)i on the CIFAR-10 dataset and the VGG
model trained using GD with the learning rates η = 0.04/0.08 (solid/dotted lines). See the
Figure 6 caption together.
——ll∕i∣l2
lu」ou -altiBds
——l∣H∣∣σ
200
ZEoN
。 。 。 。
Q 。 O I
8 6 4 2 0
——入口）
0.30
00
60
5
2
O
①-e>u ① 6ffi
0 5 0 5 0
Figure 23: The evolution of ∣∣H ∣∣σ, kh J iiLk2, and hλ(1)i on the CIFAR-10 dataset and the 6CNN
model trained using GD with the learning rates η = 0.02/0.04/0.08 (solid/dashed/dotted lines).
See the Figure 6 caption together.
28
Under review as a conference paper at ICLR 2022
——ll∕i∣l2
200
175
150
E
o 125
c
@ 100
■4-J
U
⅛ 75
S
50
25
0
一 IlHlL
3000	6000	9000
Step
ZlU」0N
——入口)
0.30
①-e>u ① 6ffi
5 0 5 0 5 0
Figure 24: The evolution of ∣∣H∣∣σ, k〈J4∣∣2, and hλ(1)i on the CIFAR-10 dataset and the VGG
model trained using SGD with the fixed learning rate η = 0.04 and the batch sizes |B(t) | =
50000(GD)/512/32 (solid/dashed/dotted lines). Training with a batch size 512 shows similar
evolutions to the GD training. See the Figure 6 caption together.
Figure 25: The evolution of ∣∣H ∣∣σ, k〈J〉i||2,and hλ(1)i on the CIFAR-100 dataset and the VGG
model trained using SGD with the fixed learning rate η = 0.1 and the batch sizes |B(t) | =
128/64/32 (solid/dashed/dotted lines). See the Figure 6 caption together.
29
Under review as a conference paper at ICLR 2022
I Figure 4 (Visualization of the optimization trajectory)
We use UMAP (McInnes et al., 2018) to visualize the optimization trajectory {θ(t)}t∈[T] ⊂ Θ ⊂ Rm
in a 2D space. As GD enters into the Edge of Stability (Cohen et al., 2021), it oscillates in a direction
nearly orthogonal to its global descent direction (Xing et al., 2018). Note that GD may not enter the
Edge of Stability as shown in Figure 26 (Bottom).
-10	-5	0
5	10
F 450
-400
-350
-300
-250
-200
-150
-100
t0
k900
-800
-700
-600
-500
-400
-300
-200
ho0
b900
-800
-700
-600
-500
-400
-300
-200
ho0
Figure 26: Visualization of the optimization trajectory using UMAP. (Top) UMAP on the CIFAR-
10 dataset trained with 6CNN for the first 500 steps, (Middle) on the CIFAR-10 dataset trained with
VGG for the first 1000 steps, and (Bottom) on the MNIST dataset trained with 3FCN-MNIST for the
first 1000 steps (from red to blue).
30
Under review as a conference paper at ICLR 2022
J Figure 8 (QΛ1/2)
Figure 27 shows the matrix QΛ1/2 (Figure 8) for different training steps. Figure 8 and Figure 27 (ToP
Right) are plotted at the equivalent step (t = 1000). Figure 27 demonstrates that λ(1) becomes more
dominant than the others as training progresses. The argument that there are two salient elements in
each q(i) in its (i)- and (i + 1)-th elements is empirically shown to be valid throughout the training.
0.09
0.04
0.03
0.02
0.02
0.02
0.06
0.04
0.02
0.02
0.02
0.07
0.04
0.03
0.02
0.06
0.04
0.02
0.06
0.03
o.u
0.04
0.12
0.01
0.01
0.06
0.01
0.04
-0.06
0.02
0.03
-0
1.04
0.07
0.02
0.02
-0
1.03
0.04
-0.07
0.02
0.01
-0
1.02
0.02
-0
1.03
-0.05
0.04
0.01
-0
0.02
-0
1.02
-0.02
-0.06
0.11
0.01
-0
0.01
-0
1.02
-0.02
-0.03
-0.06 M
0.02
0.02
0.02
0.03
s-
0.01
-0.01
0.01
-0.01
-0.01
-0.02
0.06
-0.02-0.03
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
t:
-0.1
-0.0
0.10
0.04
0.07
--0.07
-O
1.05
-O
1.03
to--0
1.02
I---------o
6-∙0
O - -0.00
0.07
0.04
0.03
0.02
0.04
0.03
0.06
0.03
0.16
0.05
0.02
0.02
0.02
0.02
0.03
0.02
0.01
0.01
0.01
0.01
0.00
0.00
0.00
0.00
-0.06
0.02
0.01
0.00
0.03
-O
1.03
-0.05
0.02
-O
1.02
-O
∣.02
0.13
0.04
0.10
-0.04
-0.02
-0.04
0.03
0.02
0.00
0.07
0.02
0.00
0.01
-O
-O
1.01
-0.14
0.06
0.01
0.00
0.01
-O
-O
1.01
-0.01
-0.02
-0.03
-O
13
0.04
0.00
0.00
-0.00
-0.00
-0.03
0.00
0Λ
0.3
-0.2
0.01
0.01
0.01
0.01
0.01
0.02
0.05
-0.09
-0.02
-0.01
0.01
0.01
0.01
0.01
0.01
0.01
0.01
0.04
-0.08
-0.01
0-00
0.00
0.00
0.00
0.01
0.01
0.01
0.01
0.02
-0.06
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
10.4
::
-0.0
0.01
0.01
0.01
0.01
0.02
0-04
-0.07
-0.01
-0.00
-0.00
0-00
0.00
0.00
0.00
0.00
0.00
0.01
0.01
-0.04
-0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.01
-0.03
10.4
:
-0.1
-0.0
0.04
0.02
0.00
0.00
0.00
0.00
0.00
0.02
0.00
0.00
0.00
0.00
0.00
-0.2
-0.03
0.04
-0.02
-0.06
ɪrɪ - -0.00
-0.00
0.00
0.00
0.00
0.00
0.00
0.02
0.00
0.00
0.00
0.00
-0.04
0.00
0.00
0.00
0.00
9 - -0.00
-0.00
-0.00
-0.02
0.00
0.00
0.00
I—0.00
-0.00
-0.00
-0.00
-0.00
0.00
0.00
0.00
α> - -0.00
-0.00
-0.00
-0.00
-0.00
-0.00
0.00
0.00
6 - -0.00
-0.00
-0.00
-0.00
-0.00
-0.00
-0.00
0.00
O - -0.00
-0.00
-0.00
-0.00
-0.00
-0.00
-0.00
-0.00
-0.00
0.00
0.00
0.00
-0.00
-0.00
-0.00
-0.00
-0.00
-0.00
-0.00
0.00
0.00
0.00
0.00
-0.00
-0.00
-0.00
-0.00
-0.00
-0.00
0.00
0-00
0-00
0-00
0.00
-0.00
-0.00
-0.00
-0.00
-0.00
0.00
0.00
0.00
0.00
0.00
0.00
-0.00
-0.00
-0.00
-0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
-0.00
-0.00
-0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
-0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
-0.00
0.00
-0.00
-0.00
-0.00
-0.00
-0.00
-0.00
-0.00
-0.00
-0.00
0.00
0.02
-0.00
W

M

M
«o -O
W
ΠR1 0-08
Figure 27:	Evolution of QΛ1/2 for some training steps during the training. They are visualized
for 100/500/1000/2000/4000/6000 steps from left to right and from top to bottom.
31
Under review as a conference paper at ICLR 2022
-C∙β2 -0X)9 4.10 -0.11 q.09 -0.03 9.23
-¢.11 -C22 4.19 -C.14 -O.C1 0.06
-0.19 C.U) 0.。4
-0.19
-0.06 0.01 0.01
-0.06 0.04 -0.03
-0.01 -0.09
10 -0X)4 0.10 0.04 4.03 -0.09	-0.05
∣t∣l-0.0β -C
-0.06
-O
10
0.02
-C∙β4
C.09 -O
12
O
10
0.10 O
13 -0.11
∣t∣ι -a.13 ¢.09
0.。4
C.ll -β.ll
-022
-0.13
4∙β3
0.09 0.07
-0.19
0.09 -0.19
-0.09
0.03 -0.11 -0.14 -0.19 -0.06 -0.06
0.23 0.04 -OAl 0.10 0.01 0.04 -0.01
*∣∣
o--c.es eTr10.08 o.oβ o.«4 0.01 9.03-0.09
0	12	3
4	5	6	7	8	9
∣t∣l -0.12 0.13 -0.11 C.C7 -0.11 C.β4 0.08
tlX)O
0.75
心。
-055
0.02 -0.19 -0.03 0.06 0.09 0.02 0.01 -0.28 -0.03
-0.24 -0.07 0.00 4.04
-0.15 -0.12 -0.03 0*4
0.08 0.04 -C.05
0.04 0X)2
-C.12
«.«2
-0.19
4.02
-04)3 4.03 -0.16
0.09 4.β4 -0.15
0.02
0.02 β.04 -0.12 -0.24 -C.C7
-0.15
0.01 0.02 -0X)5 4.07 -0.07 4.12 0.08
® -0iβ 4∙β2 0.04 β,00 0.04 9.03 0.04 C.04
0.06 4.04 0.01 0.04 -0.05 C.02
0
2
6
5
7	8	9
<n-0Λ3
C∣l -0Λ2 -0.03 0.06 -0.04 0.04 C.C2 -0.02 ∏
∣t∣l -0.16 -0.17 -0.15 -0.12 -4).05 0.04 0.0β
0.06 0.06 -0.17 -0.09
D.<B -0.07 -C.C7 0.04 CXil
tl∙CO
0.75
0-5β
--0.25
-0.20
-0.03 -0.02 -C.C1 -0.04 0.04
03
-C.24
-C.10 0.03 0.00
-0.30
-0.23 -0.04 4.01
OAl
-C.12 0.02 0.02
0.06 4∙(M 0.00
-0.01
-0.07
4.05
-0.03
-0.05 0.01 0.01 -C.0* 0.04 .C.03 -O.OcFTO-O-OT
0.00 -0.00 -4).03 0.07 -C.05 0.04 4.11
"5
0.01 C.CO
0.01 -0.00 9.03
D.D2
0.04 0.07 9.03
0.C2
4.03 -0.05 4.02 -0.24 -0.30
-0.06 0.04 4.01 -0.10 423
-0.12
0.06
3
6
5
7	8	9
0	1	2
nm -0.11 4,β4 0.03 -C.04 0.02 4.04 -C.C1
o-4).070.04 0.00 9.01 0.02 0.00 -C.C7 -C.05
■4.04 -0.03 -C.2C a.o：
11.00
0.75
0-5β
-C.11 -0.12 -0.03 0.01 -0.01
4.26 -0.06 9.02 0.03
0.01 -0.07 C.05 0.02
0.07 C.03 -0.05
03 -0.02
-0.09
-0.00 0.01 0.02 0.03 C.05 0.05 -0.07 Q
-C∙β4
-0.08 -0.00
«.06
4∙β4 «.«2
0.04 0.03
-0.11
4.05
-C∙β2 0.05
-C.«3 0.05 -C∙β3 -0.06 4.25 -0A7 0.07
-¢.12 -026 -¢.10 0.01
-0.07 0.01 .0X)2 4,β3 0.05 0.03 0.02
O-C-CseTrl-O.Ci «.«3 c.oi 0.02 9.05-0.02
.c.116
0123456789
∣I∙1-O.O* -¢.08 0.03 -C.C4 0.04 9.02 -β.β3	-0.05
0.03 0.01 -0.01
∣t∣l -0Λ5 -C.IC -0.25 -0.03 0.01
11X)0
0.75
心。
-055
4.09 -04)5 0.02 -0.04 0.01 -0.00 4.04 -0.34 4.05
03
-0.09
0.01 -0.11 -β.U 0.02 0X)3
-0.01
4.19 -0.16 4.0* 0.00 0X)1
0.04 -0.12 4.20
0.00 0X)2
0.04 0X)5
-0.08
0.00 -C∙β7
0.00 4.05
-C.04
0.01 0.01 0.02 C.02 -0.02 C.03 -0.07 ∏
-0.09
-cλs c m
0.02 0.01 0.02
-0X)4 C.02 -0X)9
-0.01
0.01 03 0.01
-0.19
0.04
∣>1 -0.16 -<).U
-0X)0 9.02
-C.11 -0.16 -0.12 -C.16
-0X)4 0.03
-0.11 -0.04 -0.20 4.18 -0.08
«> -034 -C.C7 «.«2
0.00 0.00 0.04 0.00 0.00
0.01 0.02 C.C5 -0.07 -C.05 -0.04
o—04)5	0.03
0123456789
11.00
0.75
0-5β
--0.25
-0.0*	-0.11 -0.04 0.03 -0.03	■«.02 0.03 0X)3 -0.00 0.03 0.00	-0.0* 0.04 -0.06
-0.11 0.03 -C∙β4 -0.03	-0.13 -0.13	4.13 -0.21 4.09 -0.06 H -0.14	-0.06 -0.00 -0.01 -0.02
-C∙β2 -0.00	-0.13 -0.06	^fl-0∙°6 -° 05	-0.26 -0 00
0.03 0.03	-0.21	-0.06	-0.0*	-0.13 0.01
0.03 0.00	-C∙β9 -0.14	.C.05 -«.«4	0.03 0.04
-C∙β4 0.04	-0.06 -0.01	-0.26 -0.13 0X)3	WBl 0.03
*∣∣
-0.06
0.02
-0.06
-C.C0
-0.07
-0.07
Γ⅝T1 -0.06 -¢.00 -«.«2 4.M 0.01 0X)4 0.03
6 --0.06
03 -0.06 -O.M 0.01 -C∙β7 -C.C7 -0.08
11.00
0.75
0-5β
-0.08
3
4
4
m
1	2	3
4	5	6	7	8	9
Figure 28:	Cosine similarities of {mi}= during training. They are visualized for
step=500/1000/1500/2000/3000/4000 from left to right and from top to bottom. See the Figure
9 caption.
Figure 29:	The Mean Resultant Lengths (MRL) ρ for each Di . Note that ρ is not defined for first
few steps because some Di are empty.
32
Under review as a conference paper at ICLR 2022
K Figure 9 (Clusters of m around EACH mi)
Figure 30 shows Figure 9 (Left) for some different class pairs (i, j) with negative cosine similarity,
i.e., cos(mi,mj) < 0. We use the model trained for t = 1000 steps. Figure 28 shows Figure 9
(Right) for different training steps. Figure 29 shows the evolution of the Mean Resultant Length
(MRL) ρ in Figure 9 (Left) during training.
Figure 30:	Directional data of m from Di and Dj. They are visualized for (i, j) = (airplane,
ship)/(automobile, truck)/(dog, cat)/(deer, bird) from left to right. see the Figure 9 caption.
L Figure 10 (gradient descent happens mostly in the top Hessian
sub space)
Figure 31	shows similar results with different settings with VGG and learning rate η = 0.08.
Figure 31: (Left) Alignment between the two vectors gD and r, and (Right) alignment of gD, r
in the subspace S = span({mi}C=J using VGG with η = 0.08. See the Figure 10 caption. They
are plotted for every 25 steps. Note that AS is not defined for first few steps (about 0-800 steps)
because some Di are empty.
Step
33
Under review as a conference paper at ICLR 2022
M Analysis of the MSE loss
In the main text, we focus on the cross-entropy loss. Here, we briefly analyze the MSE loss, l =
1 kz - ey k2 .Then, we have M = Vz l = I, λ ⑴=∣∣M ∣∣σ = 1 and G =〈JJ T〉. It leads to the
same conclusion as in Theorem 2:
∣G∣σ = ∣hJJTi∣σ ≤ h∣JJT∣σi = h∣J∣2i	(35)
We empirically observed that ∣H∣∣σ α khJik2 as shown in Figure 32.
——II(Z)II2
test
acc
一 l∣H∣∣σ
0 5 0
2 11
ssud.JeqS
0	20000 40000 60000 80000
Step
NUJJoN u--qo□ef
OoQ
0 5 0
2 11
0.6
0.5
0.4
0.2
0.1
0.0
Figure 32:	The sharpness ∣H ∣σ and the Jacobian norm ∣J ∣2 during training with the MSE loss
34
Under review as a conference paper at ICLR 2022
N Details of Explicit Jacobian Regularization (EJR)
We first propose a simple form of EJR with the regularized loss as follows:
L(θ) = L(θ) + λreg khJ ikF/C	(36)
and update the model parameter as
θ(HI)= θ⑴-η(Vθ L(θ㈤)+ λreg Vθ khJ ikF/C)	(37)
However, this requires to build a computational graph for khJ ik2F which is inefficient for a large
network (e.g. WRN-28-10).
To this end, we propose two efficient variants of EJR. First, we propose to update the model parameter
as follows:
θ(t+1) = θ(t) — ηVθ L(0(t))	(38)
where θ = θ + PreghJ〉u and Preg = Preg/khJ〉u|| as in Foret et al. (2021); LiU et al. (2019).
Second, we propose another variant Using L instead of L in Eq (38) as follows:
L = L + μreg UT hz	(39)
We can approximate L as follows:
L(θ) ≈ L(θ) + (VθL(θ))T(PreghJJU	(40)
=L(θ) +	(Vθ L(θ))T (Preg hj Ju)	+ (Vθ μreg UT〈Z〉)T (Preg hJ JU)	(41)
=L(θ) +	(Vθ L(θ))T (Preg hJ Ju)	+ (μreg hJ JU)T (Preg hJ JU)	(42)
=L(θ) +	(Vθ L(θ))T (Preg〈J Ju)	+ μregPreg kh J JU∣	(43)
≈ L(O) +	(VθL(O))T(Preg hJJU)	+ MregPreg IIhJJkF/√C	(44)
We used the first-order Taylor expansion of L(θ + Preg hJJU) in Eq (40). We expect additional effect
of minimizing μregUTZ + μregPregIIhJJkF/VC ≈ μregPregIIhJJkF/√C compared to the first
variant.
• λreg= 0	—∙ λreg = 0.001	—∙— λreg = 0.003
Figure 33: [Explicit Jacobian Regularization] The explicit Jacobian regularization enhances the
test accuracy. We plot the test accuracy for different learning rates η and regularization coefficients
λreg. The models are trained with batch size of |B| = 128 on CIFAR-10.
35
Under review as a conference paper at ICLR 2022
Table 2: Effectiveness of EJR. We report improvement (∆Acc.) and Error Reduction Rate (ERR)
on CIFAR-10 when trained with EJR (+EJR), compared to the standard training (Baseline).
Dataset	Network	Batch Size lr	Reg. param.	Test Accuracy	∆Acc.	ERR
Architecture	Baseline +EJR	(%p)	(%)
0.003 λreg = 0.01	66.71	75.40	+8.69	26.10
0.01	λreg = 0.03	67.88	75.66	+7.78	24.22
128	0.03 λreg = 0.01	69.83	75.53	+5.70	18.89
0.1	λreg = 0.03	70.33	74.59	+4.26	14.36
SimpleCNN	0.3	λreg = 0.01	69.34	73.47	+4.13	13.47
0.01	^6681	74.43	+7.62	22.96
50000	0.03	67.72	74.31	+6.59	20.42
λreg = 0.001 (full-batch) 0.1	reg	67.53	73.69	+6.16	18.97
CIFAR-10	0.3	61.08	72.15	+11.07	28.44
Preg = 0.5	96.44	+0.51	12.72
WRN-28-10	128	01	ρreg = 1	Cv nɔ	96.57 95.93±0.15	+0.64	15.72
(200 epochs) 128	0.1	ρreg = 2	±	96.62	+0.69	16.95
ρreg = 3	96.30	+0.37	9.09
Preg = 0.5	96.65	+0.55	14.10
WRN-28-10	128	01	ρreg = 1	MIn	96.78 96.10±005	+0.68	17.44
(400 epochs) 128	0.1	Preg = 2	±	97.07	+0.97	24.87
Preg = 3	96.79	+0.69	17.69
Preg = 0.5	80.42	+0.13	0.66
Preg = 1	81.11	+0.82	4.16
WRN-28-10	Preg = 2	81.50	+1.21	6.14
WRN-28-10	128	0.1	Pre = 3	80.29±0.25	82.51	+2.22	11.26
(200 epochs)	Preg = Preg = 4	82.65	+2.36	11.97
Preg = 5	82.31	+2.02	10.25
ρreg = 6	82.03	+1.74	8.83
CIFAR-100	Preg = 1	82.55	+1.86	9.63
Preg = 2	82.51	+1.82	9.42
Preg = 3	82.84	+2.15	11.13
WRN-28-10	128	01	Preg = 4	83.35 80.69±0.21	+2.66	13.78
(400 epochs) 128	0.1	Preg = 5	±	83.73	+3.04	15.74
Preg = 6	83.16	+2.47	12.79
Preg = 7	83.12	+2.43	12.58
Preg = 8	81.16	+0.47	2.43
Table 3: Effectiveness of EJR.v2. We report improvement (∆Acc.) and Error Reduction Rate (ERR)
on CIFAR-10 when trained with the second variant of EJR (+EJR.v2), compared to the standard
training (Baseline).
Dataset	Network Architecture	Batch Size	lr	Reg. param.	Test Accuracy		∆Acc. (%p)	ERR (%)
					Baseline	+EJR.v2		
				Preg = 2, μreg = 0.001		97.28	+1.18	30.26
				Preg = 2, μreg = 0.003		97.32	+1.23	31.28
				Preg = 2, μreg = 0.01		97.33	+1.23	31.54
	WRN2810 --			Preg = 2, μreg = 0.03		97.38	+1.28	32.82
CIFAR-10		128	0.1	Preg = 2, μreg = 0∙1	96.10±0.05	97.17	+1.07	27.44
	(400 epochs)			Preg = 1, μreg = 0.01		97.07	+0.97	24.87
				Preg = 1, μreg = 0.02		97.34	+1.24	31.79
				Preg = 1, μreg = 0.06		97.33	+1.23	31.54
				Preg = 1, μreg = 0∙1		97.26	+1.16	29.74
36