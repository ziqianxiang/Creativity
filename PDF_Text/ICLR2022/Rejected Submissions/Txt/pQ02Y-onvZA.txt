Under review as a conference paper at ICLR 2022
δ2-EXPLORATION FOR REINFORCEMENT LEARNING
Anonymous authors
Paper under double-blind review
Ab stract
Effectively tackling the exploration-exploitation dilemma is still a major challenge
in reinforcement learning. Uncertainty-based exploration strategies developed in
the bandit setting could theoretically offer a principled way to trade off exploration
and exploitation, but applying them to the general reinforcement learning setting
is impractical due to their requirement to represent posterior distributions over
models, which is computationally intractable in generic sequential decision tasks.
Recently, Sample Average Uncertainty (SAU) was develop as an alternative method
to tackle exploration in bandit problems in a scalable way. What makes SAU
particularly efficient is that it only depends on the value predictions, meaning that
it does not need to rely on maintaining model posterior distributions. In this work
we propose δ2-exploration, an exploration strategy that extends SAU from bandits
to the general sequential Reinforcement Learning scenario. We empirically study
δ2 -exploration in the tabular as well as in the Deep Q-learning case, proving its
strong practical advantage and wide adaptability to complex reward models such
as those deployed in modern Reinforcement Learning.
1	Introduction
In Reinforcement Learning (RL) an agent interacts with an external environment taking sequences
of actions that cause transitions between states of the environment, with the scope to maximize the
sum of rewards that are gathered at each state. In a typical scenario, the RL agent does not initially
posses perfect knowledge of the consequences of its actions on the environment, but has to learn
that through experience. It therefore has to face the classical exploration-exploitation dilemma, i.e.
deciding whether to exploit the actions that are known to maximize immediate reward or whether to
explore unfamiliar actions and states in order to potentially find ways to increase future rewards.
Efficient exploration requires the agent to quantify the accuracy of its current estimates of the state-
action values (the conditional expected rewards), so as to trade off expected rewards and uncertainty
in a principled way (Strehl & Littman, 2008; O’Donoghue et al., 2018). Measuring the uncertainty
associated with the value of each state-action is thus a key component of conventional algorithms for
addressing the exploration-exploitation dilemma. These ideas have been developed into successful
uncertainty-based exploration strategies such as the Upper Confidence Bound (UCB) algorithm, which
quantifies uncertainty through confidence intervals (Auer et al., 2002), and Thompson Sampling (TS),
which instead models the posterior distribution over values (Thompson, 1933). However, obtaining
either of these measures of uncertainty for general sequential Reinforcement Learning problems
has proven arduous because of their reliance on a measure of uncertainty around the current value
predictions, which is typically infeasible to obtain for complex environments and value functions.
This difficulty limits the applicability of explorations strategies based on UCB and TS for general RL
problems, compromising in particular their adaptability to algorithms such as Q-learning (Watkins,
1989; Watkins & Dayan, 1992) and DQN (Mnih et al., 2015), which consequently typically resort to
addressing exploration via inefficient heuristics like -greedy action selection.
Recently, Rigotti & Zhu (2021) tackled the mentioned limitations of conventional exploration
strategies in the constrained but interesting case of bandit problems. Central to that effort was the
development of a novel uncertainty measure, the Sample Average Uncertainty (SAU), which departs
from the approach of estimating the uncertainty of expected rewards of selected actions. In contrast
to what is done in UCB and TS, SAU is an uncertainty measure that only depends on the value
prediction of each action. Specifically, it consists in quantifying the variance of sample averages of
rewards under the assumption of known values, which can be used to simply construct an estimated
1
Under review as a conference paper at ICLR 2022
variance by plugging in the estimated values. Finally, Rigotti & Zhu (2021) were able to use SAU to
developed two SAU-based exploration strategies, SAU-UCB and SAU-Sampling, which were shown
to display state-of-the-art performance in multi-armed bandits, contextual bandit, and deep bandits.
In this work, we exploit the simplicity of SAU in Rigotti & Zhu (2021) and extend it from the
bandits to the more general sequential RL setting. This extension results in δ2 -exploration, a class of
exploration strategies for RL which share the implementation simplicity of SAU for bandits as well
as its effectiveness. Our δ2 -exploration algorithms are implementable with a minimal computational
overhead that is comparable to -greedy exploration, while being empirically more effective.
As a practical demonstration, we apply δ2 -exploration to Q-leaning (Watkins, 1989). We then
show that the resulting strategies display empirically efficient exploration, which very quickly reach
near optimal policies. As mentioned, in terms of computation and memory requirements, these
exploration strategies do not add any additional cost compared to for instance a simple -greedy
action selection strategy. Finally, we apply SAU-based exploration to Deep Q Networks (DQN), and
observe convincing performance gains over standard DQN in several games we tested from the Atari
2600 domain.
Thanks to its efficiency, flexibility and implementation simplicity, we argue that δ2-exploration could
serve as a competitive baseline and as a simple and straight-forward replacement to -greedy action
selection in a wide class of RL scenarios.
In the remainder of this paper, Section 2 summarizes common exploration strategies in bandits
and Reinforcement Learning in order to set the stage for δ2 -exploration. Section 3 establishes the
notation and states the problem. Section 4.2 presents the SAU measure, and summarized SAU-
based exploration strategies in the setting of multi-armed bandits as developed in Rigotti & Zhu
(2021). In Section 5 we then extend these strategies to the sequential RL setting and develop δ2-
exploration. Section 6 includes empirical evaluations showing substantial performance improvements
over conventional exploration strategies in Q-learning and DQN. Section 7 concludes the paper.
2	Related Work
The most common exploration strategy in Reinforcement Learning (RL) is arguably -greedy ac-
tion selection, which consists in following the action with the highest estimated value with high
probability, but occasionally with a small fixed probability choosing any other available action
uniformly at random. Despite the clear inefficiency of not differentiating lower value actions based
on their uncertainty or even their value, because of its simplicity -greedy action selection is still the
exploration strategy of choice in RL.
One of the most popular and efficient exploration algorithms is the Upper Confidence Bound (UCB)
algorithm, which is very successful in the bandit setting (Auer et al., 2002; Auer, 2003; Rusmevichien-
tong & Tsitsiklis, 2010; Abbasi-Yadkori et al., 2011; Abbasi-Yadkori, 2012; Perchet & Rigollet,
2013; Slivkins, 2014). This approach is based on the principle of optimism in the face of uncertainty,
which elegantly addresses the exploration-exploitation trade-off by maintaining confidence intervals
for action-value estimates and choosing actions optimistically within these intervals. Unfortunately,
UCB is difficulty to apply to RL problems on complex domains (Jaksch et al., 2010; Wen & Van Roy,
2013) due to the complexity of computing its uncertainty measure.
A related class of strategies to UCB are count-based methods (Strehl & Littman, 2008; Bellemare
et al., 2016) that directly turn the counts of state-action visitations into reward bonuses to encourage
the exploration of less visited ones. Despite performing near-optimally for small discrete Markov
decision processes, count-based methods suffer the curse of dimensionality in high-dimensional
environments (although see Tang et al. (2017) that proposed a possible high-dimensional extension
using learned hashing to discretize the state space).
Another popular exploration strategy is Thompson Sampling (TS) which was introduced by Thompson
(1933). This is a Bayesian approach that follows the principle of sampling in the face of uncertainty,
i.e. it samples action-values from a known posterior distribution, then selects the action with the
highest sampled value. TS has been successfully applied in bandit settings (Chapelle & Li, 2011;
Agrawal & Goyal, 2013; Russo & Van Roy, 2014). It has also been applied to simple Reinforcement
Learning problems (Strens, 2000; Osband et al., 2013). However, because calculating the posterior
2
Under review as a conference paper at ICLR 2022
distributions over values is intractable in the general case, implementations of TS in even moderately
sophisticated domains require the application of posterior approximation methods, like for instance
in Osband et al. (2016), which, at least in the contextual bandit setting, are known to be inefficient
(see Riquelme et al. (2018)).
3	Problem Formulation
We recall the definition of a Markov Decision Process (MDP) as a set of states S and actions A so that,
given a current state s ∈ S and an action a ∈ A, the probability of ending up in the next state s0 ∈ S
is Pss0 (a), where P : S × A × S → [0, 1] is a fixed state transition distribution with Ps0 Pss0 (a) = 1.
Moreover, the reward r is drawn from a reward distribution with E[r|s, a, s0] = Rss0 (a). The optimal
value function Q*(s, a) solves an MDP by satisfying the following set of BeUman equations for a
given discount factor γ ∈ [0, 1):
Q*(s, a) =	PssO (a) Rs§0 (a)+ Y max Q*(s0, a0)	∀s,a.
a0∈A
s0
MDPs can be solved using Q-learning (Watkins, 1989). Let Q(s, a) be the current estimate of
the value of action a in state s, and let s0 be the state that follows s upon taking a, and r be the
corresponding reward. Then Q-learning consists in updating the action-value function using a
learning rate α as follows:
Q(s,a) J Q(s, a) + α r + Y max Q(s0,a0) — Q(s,a).
a0∈A
In this update rule, Q-learning uses the max operator as an estimate the value of the next state. The
term in the squared brackets
δ = r + YmaxQ(s0, a0) — Q(s, a)	(1)
is known as the TD-error, because it is a generalization of the reward prediction error in the case of a
Temporal-Difference learning algorithm.
A last element that is crucial for the Q-learning algorithm to converge to the optimal value function is
exploration. That is why Q-learning is typically implemented in combination with -greedy action
selection, a simple and rather successful exploration strategy proposed in Sutton & Barto (2018) that
prescribes to fix a small probability and choose an action at time t as follows:
a ∈ A uniformly at random, with probability ;
αt	[a* = argmaXa∈A {Q(st, a)} , with probability 1 — e.
(2)
As mentioned in the previous section, -greedy exploration has the obvious inefficiency that, during
an exploratory event of probability , actions are selected without differentiating on their uncertainty
or even their value. But because of its simplicity -greedy action selection is typically the exploration
strategy of choice in many successful RL applications, such as Mnih et al. (2015).
4	Sample Average Uncertainty for Contextual Bandits
The Sample Average Uncertainty (SAU) is a measure of uncertainty that was recently proposed as a
simple and scalable mechanism to promote exploration in the bandit setting (Rigotti & Zhu, 2021).
For ease of exposition we introduce SAU from the contextual bandit perspective and notation.
4.1	Contextual Bandits
In contextual bandits at each step t the agent observes a context vector st ∈ X, selects an action at
from a set A, after which a reward rt corresponding to the chosen action is received. The value of an
action a in context st is defined as the expected reward given that a is selected, and is assumed to be
a fixed but unknown function of St and a: E[rt∣at = a] = μ(st, a). The goal is to design a decision-
making policy π : X → A that maximizes the expected reward. This goal is readily quantified in
terms of minimizing expected regret, where, if action at is chosen at step t after observing context
st, we say that we incur expected regret maxaθ∈∕{μ(st, a0)} — μ(st, at), which is the difference
between the optimal value received by playing the optimal action and the value received following
the chosen action.
3
Under review as a conference paper at ICLR 2022
4.2	Sample Average Uncertainty Exploration in Bandits
Let Ta denote the set of time steps when action a was chosen so far, and let n(a) be the size of
this set. Assume there is a sequence of estimators {μ(st, at)}t∈τa of μ(s, a). The Sample Average
Uncertainty (SAU (Rigotti & Zhu, 2021)) statistic τ (a)2 is defined as
T(a)2 =	( )^X	e2	With	et	= rt	- μ(St,	at).	⑶
n(a) t∈Ta
Obviously, computing the SAU measure only requires the prediction residuals et = r 一 μ(st, at),
without any need to model or access the uncertainty of μ(st, at).
Reference Rigotti & Zhu (2021) combined this uncertainty measure With UCB and Posterior Sampling
exploration and proposed two exploration strategies for contextual bandits named SAU-UCB and
SAU-Sampling.
SAU-UCB. SAU-UCB (Rigotti & Zhu, 2021) consists in the use the SAU uncertainty measure
τ (a)2 as the exploration bonus in the principle of “optimism in the face of uncertainty”. This works as
follow: given value predictions μ(st, a) at step t, define Y (st, a) = μ(st, a) +，n(a)-1T (a)2 log t,
where n(a) is the times action a was taken. Then choose the action by maximizing over that:
at = arg maxa∈A {Y(st, a)}.
SAU-Sampling. Thompson Sampling (TS) promotes exploration by sampling value estimates
from a modelled posterior distribution. The SAU-Sampling algorithm (Rigotti & Zhu, 2021) also
adopts this principle of “sampling in the face of uncertainty”, but unlike TS it does so by sampling
values from a parametric Gaussian distribution with a mean given by the prediction and a variance
proportional to T(a)2, resulting in values Y(St, a) ~ N (μ(st, a), T(a)2∕n(a)) from which then an
action is again selected by maximization: at = arg maxa∈A {Y(st, a)}.
In summary, in contrast to uncertainty-based exploration strategies like UCB and TS that first
measure the uncertainty of the action-value estimates μ(s, a) given the past observations, SAU-based
exploration directly quantifies the uncertainty associated with each action through equation 3 by
measuring the uncertainty of the sample average rewards. The clear advantage of this procedure is
that it is much simpler and more computationally efficient, while at the same time providing the same
quality of uncertainty estimates, and often even better exploration than the more complex algorithms
that rely on estimating the uncertainty of the action-value function (see empirical results in (Rigotti &
Zhu, 2021)).
5	δ2 -EXPLORATION: SAU-BASED EXPLORATION FOR SEQUENTIAL RL
The SAU-exploration algorithms summarized in the previous section have been shown to be very
effective exploration strategies despite their low computation footprint in the contextual bandit setting,
including the deep contextual bandit case where the reward is being parametrized by a deep neural
network (Rigotti & Zhu, 2021). We now propose an extension of SAU to adapt it from the bandit to
the sequential reinforcement learning scenario. We call the resulting class of exploration algorithms
δ2-exploration.
5.1	δ2-EXPLORATION IN Q-LEARNING
Contextual bandits are essentially simplified MDPs with horizon 1, so that each episode only consists
of one state observation (which in contextual bandits is called “context”), an action and a reward,
after which the environment is reset. Because of this relationship, the first step to extend results
in bandits to sequential reinforcement learning is to substitute the bandit feedback rt in equation 3
with the multi-step cumulative return Gt, defined as Gt = PkT=-0t-1 rt+k+1 in the case of episodic
tasks or Gt = Pk∞=0 γkrt+k+1 with discount rate γ in the case of infinite horizon tasks. The value
prediction terms, μ(s, a) in equation 3 correspond in a sequential RL setting to the state-action value
qπ(S, a) under some policy π, and can therefore be substituted with Q(S, a), its current estimate at a
given time.
4
Under review as a conference paper at ICLR 2022
At this point, we notice that Q-learning uses a target to approximate the cumulative return Gt with
the one-step iteration rt+1 + γ maxa0 Q (st+1, a0) (Sutton & Barto, 2018), and we can therefore use
the same approximation in our extension of SAU from bandits to sequential RL.
Note that the SAU measure is not a function of the current context st but just depends on action a. A
key mechanism explaining why this simple method works is the assumption that the noise in each arm
is homogeneous. This is a good approximate description of the data-generating process underlying
complicated reward models such as deep neural networks, in particular when there wouldn’t anyway
be enough data to provide an accurate fit of uncertainty as a function of context. In sequential RL, the
reward models might indeed be complicated, which bodes well for applying the SAU measure in this
setting, especially in the deep RL case.
We now have everything to formulate the sequential RL version of the SAU uncertainty measure τa
in equation 3 that we call δ2-uncertainty:
δ2⑷=n(a)X δ2(st,at),
t∈Ta
(4)
where δ(st, at) denotes the TD-error in equation 1 at an observed sequence st, at, rt+1, st+1, i.e.:
δ(st, at) = rt+1 + γm0axQ (st+1, a0) - Q(st, at),
and the sum in equation 4 is over the set Ta of time steps when action a was chosen up until the
current time, and n(a) denotes the number of times that the action a was observed.
In practice, ∆2(∙) in equation 4 can be efficiently computed by incrementally updating a running
estimate that accumulates the TD-error:
∆2(a) - ∆2(a) + [δ2(s, a) — ∆2(a)] /n(a).	(5)
Comparing the derived expression equation 4 for the δ2 -uncertainty ∆2 with the definition equation 3
of the SAU measure τa solidifies the intuition that the role played by the prediction residual et in
equation 3 for the bandit feedback is naturally taken up by the TD-error δ(st, at) in sequential RL.
As in the case of SAU-based exploration in bandit problems, once the δ2-uncertainty is available it
can simply be plugged into the action selection process to implement a class of exploration algorithms
that we call δ2-exploration. The δ2-exploration algorithm that is obtained by using the δ2 -uncertainty
as in UCB will be called δ2-UCB. Analogously, the δ2 -exploration algorithm obtained by combining
the δ2 -uncertainty with sampling-based exploration will be called δ2-Sampling.
More specifically, given action-values Q(st, a) at a given state st at time t, δ2 -exploration implements
exploration by “perturbing” action-values Q(st, a) according to one of the following schemes,
depending on which specific exploration algorithm we want to use:
1.	δ2-UCB:
2.	δ2-Sampling:
六/ 、 c( ∆	∕∆2(a) logt
Q(St ,a) = QMay
∀a ∈ A;
Q(st, a)〜N(Q(st, a), ；(0)))	∀a ∈ A.
Then an action is selected by maximization: at = arg maxa∈A Qe(st, a) .
The two δ2-exploration algorithms δ2-UCB and δ2-Sampling are detailed in Algorithm 1.
5.2	COMPARISON BETWEEN VALUE UNCERTAINTY EXPLORATION AND δ2 -EXPLORATION
Let us take a moment to compare the functioning of δ2-exploration to conventional exploration
strategies based on estimating uncertainty of value estimates, such as UCB and posterior sampling
(Thompson Sampling) methods. We call these methods “value uncertainty exploration methods”
because the mechanism that they use to promote exploration relies on quantifying the uncertainty
of the current value function estimates. Maintaining this “internal uncertainty” measure is typically
5
Under review as a conference paper at ICLR 2022
Algorithm 1 δ* 2 -exploration in Q-learning.
Parameters: step size α ∈ (0, 1], discount factor γ ∈ (0, 1], and β ≥ 1.
Initialize: Q(s, a) such that Q(terminal, ∙) = 0, ∆2(a) = 0, and n(a) = 1 for all a ∈ A.
for all episodes do
Initialize state s
while s in episode is not terminal do
(1)	Use δ2-exploration strategy (a) or (b):
(a)	δ2-UCB:	e 、	I S∆2(a)log Pa n(a)
Q(S,aɔ = QGa) + y------------丽-----------；
(b)	δ2-Sampling:	∆2(a)
Q(S, a)〜N (q(s, a), n(a)..
(2)	Select a in state s:	a = arg maxa0∈A{Qe(s, a0)}
n(a) J n(a) + 1.
(3)	Take action a, observe r, S0.
⑷ UPdate Qsand δ2(∙^	δ(s,a) = r + Y max Q (s0,α) - Q(s,a)
a0
Q(s, a) J Q(s, a) + α ∙ δ(s, a)
∆2(a) J ∆2(a) + (δ2(s, a) — ∆2(a)) /n(a).
(5) SteP to next state: s J s0
end while
end for
comPutationally costly, to the Point of being comPutationally intractable for large state sPaces.
ThomPson SamPling in Particular assumes a Prior distribution over value functions that is uPdated
based on observed transitions, a Process that is only feasible for simPle environments.
In contrast, rather than maintaining the internal uncertainty of value function estimates, δ2 -exPloration
simPly uPdates its ProPensity for exPloration by accumulating the reward prediction error δ as in
equation 5. In other words, δ not only has a role in policy evaluation by rePresenting the extent to
which the value function should be uPdate, but also has a role in the control policy by rePresenting
the extent to which exPloration is needed to Prevent the Policy from being suboPtimal.
Figure 1 gives a schematic overview of the difference between value uncertainty exploration methods
and our ProPosed method δ2-exPloration, emPhasizing the simPlicity of δ2-exPloration whose exPlo-
ration mechanism does not need to rely on estimating the distributional uncertainty over the value
function, but only needs access to the reward Prediction error δ to comPute the δ2 -uncertainty ∆.
Once the δ2-uncertainty ∆ is available, it is directly Passed to the action selection steP (see steP
(2) in Algorithm 1). This means that the δ2-uncertainty, besides being cheaP to comPute, is also
simPle to use to instantiate the exPloration mechanism: it is simPly used to “Perturbe” the estimated
action-values (either as an exPloration bonus in the case of δ2-UCB or as samPling noise in the case of
δ2-Sampling) before greedily selecting the next action. This underscores that from a comPutational
standPoint the action selection Process of δ2 -exPloration is as simPle as -greedy action selection
(equation 2). Indeed, δ2-exPloration inherits this trait from SAU-exPloration (Rigotti & Zhu, 2021),
and similarly to that algorithm can be used as a flexible drop-in replacement for -greedy selection.
5.3 δ2-EXPLORATION IN DEEP Q-LEARNING
DeeP Q-learning consists in Parameterizing the action-value function Q(s, a) as a deeP neural network,
i.e. a DeeP Q Network (DQN) policy network (Mnih et al., 2015). Because of the comPlexity of such
an action-value function, it is notoriously difficult to imPlement efficient exPloration strategies in
DQN. As a result, dithering heuristics such as -greedy are commonly used in Practice. Fortunately,
6
Under review as a conference paper at ICLR 2022
(a) Value uncertainty exploration
(b) d2-exploration
Figure 1: (a) Value uncertainty exploration algo-
rithms like UCB and Thompson Sampling rely on
an estimate of the uncertainty of the current value
function, a quantity that is intractable already for
moderately complex value functions and environ-
ments. (b) δ2 -exploration on the other hand only
needs access to the reward prediction error δ to
compute how to select an action that trades off
exploration and exploitation in a principled way.
δ2-exploration just depends on the prediction of the value function, so it is versatile enough to be
easily applicable to DQN.
A direct way to combine δ2-exploration with DQN would be to implement Algorithm 1 using a deep
neural network (in addition to the policy network) that would fit the δ2 -exploration measure ∆. To
avoid the computational and memory overhead of an additional network we took inspiration from
recent papers that harnessed the target network in DQN (a delayed parameter version of the policy
net used in Mnih et al. (2015) to stabilize training) in order to implement unbiased estimators of the
value (van Hasselt et al., 2016; Zhu & Rigotti, 2020). We reasoned that the difference between the
online policy net Q(s, a) and the delayed parameter target net Q-(s, a) should reflect a minimization
of the reward prediction error. This suggests to replace the expression equation 4 with the following
expression for the δ2 -uncertainty ∆:
∆d2qn(s,a) = maa0xQ(s,a0) - Q- (s, a)	.
We propose to use this variation of the δ2-uncertainty to implement the DQN version of δ2-exploration
algorithms δ2-UCB and δ2-Sampling. Using δ2-exploration in DQN therefore comes down to
implementing DQN learning as usual (see e.g. Mnih et al. (2015)), except that the action selection part
of the algorithm (which typically would use -greedy action selection) is replaced as in Algorithm 2.
Algorithm 2 δ2 -exploration in Deep Q-Networks.
1: Initialize: Regular DQN (Mnih et al., 2015) with policy network Q(s, a), target network
Q- (s, a), and global action counts n(a) initialized at 1 for all a ∈ A.
2: Replace action selection act(∙) in regular DQN with:
3: function ACT-δ2 -EXPLORATION(s)
4:	for a = 1, . . . , |A| do
5:	(1) Compute	∆d2qn(s, a) = max Q(s, a0) - Q-(s, a)
6:	(2) Use δ2-exploration strategy (a) or (b):
7:	(a)δ2-ucB:	方	/ziqirayiogPnay
Q(S, O) = Q(s, a)+v----------------------;
8:	(b) δ2-Sampling:	∆d2qn(s, a)
Q(s,a) ~n I Q(s,a), -n(αy	.
9:	end for
10:	Select action a =argmaxa∈/{Q(s,o)}.
11:	Update global count n(a*) J n(a*) + 1.
12:	Return action a*
13: end function
7
Under review as a conference paper at ICLR 2022
6	Empirical Results
We now empirically evaluate the performance of our proposed δ2 -exploration strategies in a series of
reinforcement learning tasks, including classical tabular Q-learning and Deep Q Learning tasks.
We will benchmark δ2 -exploration against -greedy action selection, the de facto go-to exploration
strategy in Q-learning, and also the closest exploration strategy in terms of computational and
implementation efficiency. In fact, we propose δ2 -exploration as a flexible drop-in replacement
for -greedy action selection that we now demonstrate is remarkably more efficient, despite being
comparably simple in terms of computation and implementation complexity.
6.1	Q-learning on cliff-walking task
Owl 。寸— 00—
sdrawer fo mus
(a) 10 × 5 grid.
sdrawer fo mus
0 07 00— 00 7
episode
(b) 20 × 5 grid.
Figure 2: Cliff-walking RL task. Both δ2-exploration algorithms achieve higher cumulative reward
at a faster rate than -greedy action selection for both grid sizes, even attaining the optimal reward
(indicated by the black dotted line). Moreover, as the bar plots show, δ2 -exploration reaches a lower
variability of received rewards, which indicates that it is more stable. e-greedy exploration with
e = 1/，n(s), where n(s) is the number of times state S has been visited. Learning rate is set to
αn(s, a) = 0.1(100 + 1)/(100+n(s, a)), where n(s, a) is the number of updates of each state-action.
Data points are averaged over 1000 runs.
We evaluate the performance of δ2 -exploration on the cliff-walking task (Example 6.6 in Sutton &
Barto (2018)), a standard undiscounted episodic task with start and goal states, and four movement
actions: up, down, right, and left. Reward is -1 on all transitions except those into the “Cliff” region
(bottom row except for the start and goal states). If the agent steps into this region, she gets a reward
of -100 and is instantly sent back to the start. We vary the environment size by considering 10 × 5
and 20 × 5 grids.
We measure performance as cumulative reward during episodes, and report average values for 1000
runs and standard deviation of values of the final episode in Fig. 2. We compare our two proposed
δ2-exploration δ2-UCB and δ2-Sampling against e-greedy action selection with e = 1/，n(s). As
Fig. 2 shows the δ2-exploration strategies perform much better than e-greedy, quite quickly attaining
near optimal values (the reard value indicated by the black dotted lines in the plot). We also compute
the standard deviation of rewards of the final rewards of 1000 runs, which show that δ2 -exploration
strategies display much lower variability (i.e. are more stable) than e-greedy action selection.
6.2	Deep Q Network on the Atari 2600 domain
As a final empirical validations for δ2-exploration we study the performance of our DQN adaptation
Algorithm 2 on 8 representative games from the Atari 2600 domain (Bellemare et al., 2013). We
choose 4 “easy” games for which DQN displays super-human performance, and 4 “hard” games for
which DQN performs around or below humans. We compared the performance of the δ2-exploration
algorithms with the conventional e-greedy action selection that consists of annealing e for the first
1M steps from 1.0 linearly to eF = 0.1, noisy nets (Fortunato et al., 2017) and Bootstrapped DQN
(Osband et al., 2016), a strong baseline, but which requires a form of ensembling resulting in
considerable memory overhead. Fig. 3 compares the performance at the end of training for e-greedy,
Bootstrapped DQN, noisy nets and our δ2 -exploration algorithms. The advantage of δ2-exploration
is particularly remarkable for harder games, and in particular MsPacman, where our algorithms finds
8
Under review as a conference paper at ICLR 2022
policies that reach scores that are almost double what the other competitors reach, and Centipede,
where δ2-Sampling vastly outperforms even Bootstrapped DQN.
Assault
VideoPinball
DoubleDunk	Centipede
MsPacman
20-
15-
10-
5-
DemonAttack
600
500
400
300
200
100
250 -
200 -
Breakout
1200
1000
800
600
400
200
Atlantis
2000
S p3z--eE∖ON
30000-
25000-
1500
20000-
1000
15000∙
10000∙
500
5000 -
熨Q黑s,%
・A0u
「 deoOq
P33∖3
ω0n%
u=dEes,%
AOU
deoOq
p33,3
ω0n%
u=dEes,%
AOU
deoOq
p33,3
熨Q黑s,%
-AOU
「 deoOq
P33∖3
ω0n%
u=dEes,%
AOU
deoOq
p33,3
ω0n%
u=dEes,%
AOU
deoOq
p33,3
熨Q黑s,%
-AOU
「 deoOq
P33∖3
熨Q黑s,%
-AOU
「 deoOq
P33∖3

Figure 3: δ2-exploration DQN policies consistently outperform -greedy exploration and noisy nets
and are on par and sometimes better than the strong but computationally costly Bootstrapped DQN
in the Atari 2600 domain. DQN training is done for 20M steps for all games, except VideoPinball
which is trained for 80M steps (see Appendix A). Performance is then computed as the average score
over the last 1M frames of training. Bars indicate score normalized to human performance (see Mnih
et al. (2015)) across 6 random seeds, error bars are SEM. δ2-UCB and δ2-Sampling exploration
are compared to -greedy exploration with linearly decreasing over 1M steps starting from 1.0
to F = 0.1, as standard (Mnih et al., 2015). Both δ2 -exploration strategies reach consistently
competitive policies at the end of training.
7	Conclusions and Discussion
Here we extended Sample Average Uncertainty, a recently proposed uncertainty measure for explo-
ration, from bandit problems to the general sequential reinforcement learning setting. This resulted in
our δ2-exploration algorithms, two exploration strategies that are extremely simple and flexible to
implement, as they only depend on the predicted action-values. These can be used as drop-in replace-
ments for -greedy action selection in Q-learning and DQN, resulting in effective and computationally
efficient learning and superior empirical performance over the -greedy baseline.
Our work provides a simple, scalable and robust exploration algorithms, that can be effortlessly
deployed to mitigate the exploration-exploitation dilemma. While the resulting consequences of the
deployment of an RL algorithm, ethical or otherwise, will clearly depend on the specific applications,
we believe that providing efficient uncertainty estimates and more reliable exploration than the
common -greedy action selection strategy will potentially have a positive contribution in terms of
sample-efficiency and optimality of the learned policies, as well as in terms of their trustworthiness.
We limited our experiments to comparisons against -greedy exploration, given its importance and
ubiquity, and because our method was developed specifically as a natural drop-in replacement for
-greedy exploration. Our δ2 -exploration algorithms have in fact a comparably low computation
footprint and can be implemented through a minimal API that is the same as for -greedy explo-
ration, only needing to access the action selection process of the RL agent. This enables modular
implementations with minimal change on existing RL code bases, as for instances exemplified by
the DQN version of δ2-exploration in Algorithm 2, which simply consists in the replacement of the
action selection method in a conventional DQN with no modification of the training algorithm. This
is in stark contrast with previous exploration strategies based on quantifying value uncertainty, which
typically impose limitations on the representation of the value function, need to heavily modify the
evaluation process, and introduce costly training loops to update the value uncertainty.
That said, the remarkable performance of our simulation results and the flexibility of δ2-exploration
suggest that it could be applied to even more sophisticated RL settings and possibly combined with
other learning algorithms besides Q-learning and DQN, an option that we are looking forward to
investigating in the future.
9
Under review as a conference paper at ICLR 2022
Reproducibility statement
We provide detailed pseudo-code of our proposed algorithms and tried our best to indicate important
hyperparameters to replicate our experiments and reproduce our benchmarks, which are anyway
based on tasks that are already publicly available. In addition, upon acceptance of our paper, we plan
to release the code to replicate all our numerical experiments on github.
References
Yasin Abbasi-Yadkori. Online learning for linearly parametrized control problems. PhD thesis,
University of Alberta, Edmonton, AB, Canada., 2012.
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic
bandits. In Advances in Neural Information Processing Systems, volume 24, pp. 2312-2320, 2011.
Shipra Agrawal and Navin Goyal. Further optimal regret bounds for Thompson sampling. In
Proceedings of the 16th International Conference on Artificial Intelligence and Statistics, pp.
99-107, 2013.
P. Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine
Learning Research, 3:397-422, 2003.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite time analysis of the multiarmed bandit
problem. Machine Learning, 47:235-256, 2002.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-
ment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:
253-279, 2013.
M.G. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos. Unifying count-
based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems,
2016.
Olivier Chapelle and Lihong Li. An empirical evaluation of Thompson sampling. In Advances in
Neural Information Processing Systems, volume 24, 2011.
Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves,
Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, et al. Noisy networks for exploration.
arXiv preprint arXiv:1706.10295, 2017.
T. Jaksch, R. Ortner, and P. Auer. Near-optimal regret bounds for reinforcement learning. Journal of
Machine Learning Research, 11:1563-1600, 2010.
V. Mnih, K. Kavukcuoglu, D. Silver, A.A. Rusu, J. Veness, M.G. Bellemare, A. Graves, M. Riedmiller,
A.K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Ku-
maran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement
learning. Nature, 518:529-533, 2015.
B. O’Donoghue, I. Osband, R. Munos, and V. Mnih. The uncertainty Bellman equation and explo-
ration. In Proceedings of the 35rd International Conference on Machine Learning, 2018.
I. Osband, D. Russo, and B. Van Roy. (more) efficient reinforcement learning via posterior sampling.
In Advances in neural information processing systems, 2013.
I. Osband, C. Blundell, A. Pritzel, and B. Van Roy. Deep exploration via bootstrapped dqn. In
Advances in neural information processing systems, 2016.
V. Perchet and P. Rigollet. The multi-armed bandit problem with covariates. The Annals of Statistics,
41(2):693-721., 2013.
M. Rigotti and R. Zhu. Deep bandits show-off: Simple and efficient exploration with deep networks.
arXiv preprint arXiv:2105.04683, 2021. URL http://arxiv.org/abs/2105.04683.
10
Under review as a conference paper at ICLR 2022
Carlos Riquelme, George Tucker, and Jasper Snoek. Deep Bayesian bandits showdown: An empirical
comparison of Bayesian deep networks for Thompson sampling. In Proceedings of the 6th
International Conference on Learning Representations, 2018.
Patt Rusmevichientong and John Tsitsiklis. Linearly parameterized bandits. Mathematics of Opera-
tions Research, 35(2):395-411, 2010.
D. Russo and B. Van Roy. Learning to optimize via information-directed sampling. In Advances in
Neural Information Processing Systems, volume 27, pp. 1583-1591, 2014.
A. Slivkins. Contextual bandits with similarity information. Journal of Machine Learning Research,
15(1):2533-2568, 2014.
A. L. Strehl and M. L. Littman. An analysis of model-based interval estimation for Markov decision
processes. Journal of Computer and System Sciences, 74(8):1309-1331, 2008.
M. Strens. A bayesian framework for reinforcement learning. In Proceedings of the 17th international
conference on Machine learning, pp. 943-950, 2000.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press,
second edition edition, 2018.
H. Tang, R. Houthooft, D. Foote, A. Stooke, X. Chen, Y. Duan, J. Schulman, F. De Turck, and
P. Abbeel. #Exploration: A study of count-based exploration for deep reinforcement learning. In
Advances in Neural Information Processing Systems, 2017.
William R. Thompson. On the likelihood that one unknown probability exceeds another in view of
the evidence of two samples. Biometrika, 25:285-294, 1933.
H. van Hasselt, A. Guez, and D. Silver. Deep reinforcement learning with double Q-learning. In
Proceedings of the 30th AAAI Conference on Artificial Intelligence, pp. 2094-2100, 2016.
C. J. C. H. Watkins. Learning from Delayed Rewards. PhD thesis, King’s College, Cambridge,
England, 1989.
C. J. C. H. Watkins and P. Dayan. Q-learning. Machine Learning, 8:279-292, 1992.
Z. Wen and B. Van Roy. Efficient exploration and value function generalization in deterministic
systems. In Advances in neural information processing systems, 2013.
R. Zhu and M. Rigotti. Self-correcting q-learning. arXiv preprint arXiv:2012.01100, 2020.
A Additional figure
11
Under review as a conference paper at ICLR 2022
9」。。S pφz--euJ,JON
0.0	2.5	5.0
7.5	10.0	12.5	15.0	17.5	20.0
Frame [×106]
000
30 20 10
)eCnamrofrep namuh fo %(
eroCS dezilamroN
0.0	2.5	5.0
7.5	10.0	12.5	15.0	17.5	20.0
Frame [×106]
MSPaCman
)ecnamrofrep namuh fo %(
erocs dezilamroN
25000-
20000-
15000-
10000-
VideoPinball
Frame [×106]
)eCnamrofrep namuh fo %(
eroCS dezilamroN
ε-greedy
bootstrap
noisy
δ2-Sampling
-----δ2-UCB
DoubleDunk
2.5	5.0	7.5	10.0	12.5	15.0	17.5	20.0
Frame [×106]
)ecnamrofrep namuh fo %
erocs dezilamroN
0.0	2.5	5.0	7.5	10.0	12.5	15.0	17.5	20.0
Frame [×106]
DemonAttack
ε-greedy
bootstrap
noisy
δ2-Sampling
----δ2-UCB
)eCnamrofrep namuh fo %(
eroCS dezilamroN
ε-greedy
bootstrap
noisy
δ2-Sampling
----δ2-UCB
800 600 400
)ecnamrofrep namuh fo %(
erocs dezilamroN
7.5	10.0	12.5	15.0	17.5	20.0
Frame [×106]
0000000
543 2 1 1
-
)eCnamrofrep namuh fo %(
eroCS dezilamroN

夕LQ
0.0	2.5	5.0	7.5	10.0	12.5	15.0	17.5	20.0	0.0	2.5	5.0	7.5	10.0	12.5	15.0	17.5	20.0
Frame [×106]	Frame [×106]
Figure 4: Learning curves of -greedy exploration, bootstrapDQN, noisy nets, δ2 -UCB and δ2-
Sampling on 4 “easy” and 4 “hard” (i.e., performance of DQN is respectively higher and lower
than that of human players) additional Atari 2600 games. Scores are averaged in intervals of 500k
frames, plots indicate mean score over 6 independent random seeds, and shaded areas indicate
min and max scores over the 6 seeds. δ2 -exploration algorithms consistently have scores among
the highest throughout learning compared to the -greedy strategy, noisy nets and bootstrapDQN.
Remarkably, for two “hard games”, Assault and DoubleDunk, for which -greedy only reaches sub-
human performance, δ2 -exploration can very rapidly find policies that beat human players by more
than 200% and 500%, respectively, rivaling the performance of bootstrapDQN. For VideoPinball
and Centipede where -greedy seems unstable, δ2 -exploration strategies are able to maintain better
stability throughout learning, even beating bootstrpDQN in some cases like Centipede.
12