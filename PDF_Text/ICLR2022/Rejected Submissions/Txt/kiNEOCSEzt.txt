Under review as a conference paper at ICLR 2022
Estimating and Penalizing Induced Prefer-
ence Shifts in Recommender Systems
Anonymous authors
Paper under double-blind review
Ab stract
The actions that a recommender system (RS) takes - the content it exposes users
to - influence the preferences users have over what content they want. Therefore,
when an RS designer chooses which system to deploy, they are implicitly choos-
ing how to shift or influence user preferences. Even more, if the RS is trained via
long-horizon optimization (e.g. reinforcement learning), it will have incentives to
manipulate preferences, i.e to shift them so they are more easy to satisfy, and thus
conducive to higher reward. While some work has argued for making systems
myopic to avoid this issue, myopic systems can still influence preferences in un-
desirable ways. In this work, we argue that we need to enable system designers to
estimate the shifts an RS would induce; evaluate, before deployment, whether the
shifts are undesirable; and even actively optimize to avoid such shifts. These steps
involve two challenging ingredients: estimation requires the ability to anticipate
how hypothetical policies would influence user preferences if deployed - we do
this by training a user predictive model that implicitly contains their preference
dynamics from historical user interaction data; evaluation and optimization ad-
ditionally require metrics to assess whether such influences are manipulative or
otherwise unwanted - we introduce the notion of “safe shifts”, that define a trust
region within which behavior is believed to be safe. We show that recommender
systems that optimize for staying in the trust region can avoid manipulative behav-
iors (e.g., changing preferences in ways that make users more predictable), while
still generating engagement.
1	Introduction
A recommender system (RS) exposes users to feeds of items, and users choose what to click on
based on their preferences. These preferences are non-stationary: they shift over time (Koren, 2009;
Rafailidis & Nanopoulos, 2016; Li et al., 2014) . Importantly, these shifts are not independent from
the RS’s actions: what content the RS exposes users to influences their preferences.
Because of this influence, when a system designer chooses a specific recommender algorithm (pol-
icy), they are implicitly choosing how to shift user preferences. A growing trend is to choose policies
that stem out of long-term optimization of user engagement (Afsar et al., 2021) - typically via re-
inforcement learning; we will refer to these as long-term value, or LTV, systems. However, these
policies might lead to very undesirable shifts as a side-effect: certain preferences are easier to satisfy
than others, leading to more potential for engagement - this could be because of availability of more
content for some preferences compared to others, or because very strong preferences for a particular
type of content lead to higher engagement than more neutral ones. The LTV system therefore has an
incentive to drive user preferences over time into these areas of higher engagement potential. This
can make such LTV systems a particularly poor choice for avoiding undesired shifts.
While it has been proposed to stifle RSs’ capabilities by making them myopic - to avoid the incen-
tives for preference manipulation above (Krueger et al., 2020) - even myopic systems can influence
preferences in systematically undesirable ways (Jiang et al., 2019; Mansoury et al., 2020; Chaney
et al., 2018). Fig. 1 shows a simple example, plotting user preferences in a 1D space over time
(in a simulated environment). On the right, we see how interacting with the RL policy drives users
strongly away from their initial preferences to a specific small region. The myopic policy (center),
shows the same type of effect, but much less pronounced, where after 10 time steps some users do
reach other preferences as well.
1
Under review as a conference paper at ICLR 2022
02468	10 02468	10 02468	10
Timestep	Timestep	Timestep
Figure 1: Preferences induced by different RS policies across the same cohort of users. In
simulated environment, preferences (y axis) are in 1D, and change over time (x axis) as users interact
with the policy. On the right, the RL system drives preferences to one spot. A myopic policy (center)
has a similar effect but less concentrated. These shifts are different from what we introduce as the
”natural” evolution of user preferences.
The reality is that any policy will have influence on user preferences. In this work, we argue that
we need to explicitly account for this influence when choosing which policy to deploy. This requires
tackling two critical problems. First, we need to anticipate what preference shifts an RS policy will
cause before it is deployed. This alone would enable system designers to qualitatively assess whether
a policy induces shifts that seem manipulative or otherwise unwanted. We study how to do this from
historical user interaction data of previous RS policies, and show how to train a predictive model
that implicitly captures preference dynamics. The second critical problem is having metrics for
quantitatively evaluating whether an evolution of preferences is actually unwanted: a computable
metric not only simplifies evaluation, but also enables RSs to actively optimize against unwanted
shifts. Unfortunately, defining what “undesirable“ shifts are is a complex philosophical and ethical
challenge (noa, 2018; Paul, 2014). To side-step it, we propose a conservative approach based on the
notion of ”safe shifts”: instead of defining what is undersirable, we non-exhaustively define certain
shifts that we trust not to be problematic, and measure the extent to which a policy deviates from
them. For instance, a candidate for safe shifts that we introduce is how user preferences would
naturally shift if the user were ”omniscient”, i.e. if they could see and attend to the true distribution
of available content without any bias from the RS selecting specific slates to show them. Fig. 1
(left) shows this natural preference evolution for our running example, and how user preferences
stay somewhat diffuse but drift towards the opposite mode that the RL and myopic policies push
them to. Note that while this metric can effectively penalize undersired shifts, it comes at a cost:
natural shifts, and in fact any lists of safe shifts that we define, are unlikely to be exhaustive, which
means the approach will conservatively penalize policies that might in reality be safe.
To demonstrate both the estimation of preference shifts and their evaluation, we set up a testing
environment in which we emulate ground truth user behavior by drawing from a model of preference
shift from prior work (Bernheim et al., 2021). We first show qualitatively that in this environment,
RL and even myopic RS lead to undesired shifts. We then find that our preference shift estimation
model, if trained based on historical user interaction with different policies, can anticipate these
shifts (see Fig. 7). Further, we find that our evaluation metric correctly flags which policies will
produce undesired shifts, and evaluates the RL policy from Fig. 1 as 35% worse than the myopic one,
which is in turn 40% worse than a policy which avoids shifting preference to the “unnatural” mode
from the figure. Our results also suggests that evaluating our metric using the trained estimation
model correlates to using ground truth preference dynamics, and that optimizing for safe shifts does
lead to higher scoring (more safe) policies.
Although this just scratches the surface of finding the right metrics for unwanted preference shifts
and evaluating them, our results already have implications for the development of recommender
systems: in order to ethically use systems at scale, we e active steps to measure and penalize how
such systems shift users, internal states. In turn, We offer hope that this is possible - at least for pref-
erence shifts - by learning from user interaction data, and put forward a framework for specifying
“safe shifts“ for detecting and controlling such incentives.
2	Related Work
RS effects on users’ internal states. A variety of previous work considers how RSs can affect users:
influencing user,s preferences for e-commerce purposes (HaUbI & Murray, 2003; Cosley et al., 2003;
Gretzel & Fesenmaier, 2006), altering people’s moods for psychology research (Kramer et al., 2014),
changing populations, opinions or behaviors (Matz et al., 2017), exacerbate (Hasan et al., 2018)
cases of addiction to social media (Andreassen, 2015), or increase polarization (Stray, 2021). There
2
Under review as a conference paper at ICLR 2022
have been three main types of approaches to quantitatively estimating effects of RSs’ policies on
users: 1) analyzing static datasets of interactions directly (Nguyen et al., 2014; Ribeiro et al., 2019;
Juneja & Mitra, 2021; Li et al., 2014), 2) proposing hand-crafted models of user dynamics and using
them to simulate interactions with RSs (Chaney et al., 2018; Bountouridis et al., 2019; Jiang et al.,
2019; Mansoury et al., 2020; Yao et al., 2021), or 3) using access to real users and estimating effects
through direct interventions (Holtz et al., 2020; Matz et al., 2017). Our approach is most similar to
2), but we propose implicitly learning user dynamics instead of handcrafting our own model.
Neural networks for recommendation and human modeling. While data-driven models of human
behavior have been used widely in real-world RS as click predictors (Zhang et al., 2019; Covington
et al., 2016; Cheng et al., 2016; Okura et al., 2017; Mudigere et al., 2021; Zhang et al., 2014;
Wu et al., 2017) and for simulating human behavior in the context RL RSs’ training (Chen et al.,
2019; Zhao et al., 2019; Bai et al., 2020), to our knowledge they have not been previously used for
simulating and quantifying the effect of hypothetical recommender systems on users. We emphasize
how human models can also be used as auditing mechanisms by anticipating RSs’ policies impact
on users’ behaviors and, as our method shows, even preferences.
RL for RS. Using RL to train LTV RSs has recently seen a dramatic increase of interest (Afsar et al.,
2021), with some notable examples being proposed by YouTube and Facebook (Ie et al., 2019; Chen
et al., 2020; Gauci et al., 2019) - which reported significant engagement increases with real users.
Singh et al. in particular also takes a penalized RL approach.
Side effects and safe shifts. Our work starts from a similar question to that of the side effects
literature (Krakovna et al.; Kumar et al., 2020), but applied to preference change: given that the
reward function will be misspecified, how do we prevent undesired preference-shift side effects?
The choice of safe shifts corresponds to the choice of baseline in this literature.
3	Preliminaries
Setup. We model users as having time-indexed preferences ut ∈ Rd, which assign a scalar value to
every possible item of content x ∈ Rd . and the user engagement derived from the consumption of
item Xt under Ut at time t is modeled as being given by Irt(Ut) = UTxt. These preferences are part
of the user’s internal state zt, along with other variables, which together with preferences comprise
a sufficient statistic for their long-term behavior, but which our method will not explicitly model. At
every time step, the user sees a slate st produced by that RS policy π, and chooses an item xt . The
policy π maps history of slates and choices so far, s0:t, x0:t, to the new slate st+1. Upon making a
choice, the user’s internal state updates to zt+1.
User choice model. We are interested in inferring and predicting preferences, which we do not
directly observe. As such, we make an assumption about how user behavior - their choice xt -
relates to their current preferences Ut. Following prior work (Ie et al., 2019; Chen et al., 2019), we
assume that users choose items in proportion to (the exponent of) the engagement under their current
preferences, i.e. that P(xt = x|st, Ut) is given by the conditional logit model (detailed in Sec. 6).
Casting this as an NHMM. The dynamics of user internal state - which we don’t assume to be
known - depend on the history so far because the policy π uses history. This makes our setup a non-
homogeneous HMM (Hughes et al., 1999; Rabiner), with hidden state zt and dynamics that time-
dependent: P(zt+i|zt,n(so：t,xo：t)). In our method will not commit to an explicit representation
of zt, and will only implicitly learn these dynamics. For our experiments however, we will build a
testing environment that does commit to a representation of zt via preferences and user beliefs, and
adopts a ”ground truth” dynamics of this internal state based on prior literature.
4	Estimating Users’ Preferences
Our proposal boils down to the following: before deploying a new recommender system policy π0,
we first need to be able to understand how that policy would change preferences and behavior. In
estimating how users’ preferences would shift, one approach is to consider users which interacted
with some policy π (or a few policies), and try to estimate what preferences shifts π0 would induce
in users ifwe were to deploy it going forward; this is useful, but suffers from a problem - users have
already been biased by the previous policies, and their preferences might have already e.g. shifted
to extremes; forward prediction will thus miss certain undesired effects. What we really want to
do is turn back time - we want to ask for a new distribution of users identical to ours, how their
3
Under review as a conference paper at ICLR 2022
Estimation inputs
I I SIateSSOAl
，Choices%—
I_-_I Extra context
I "o I (for counterfact.)
Loss再)
P(4M-l,%l)
尸, ISO力曲:i)
Choice
belief
Preference
belief
Figure 2: Future preference estimation model. Given past slates and choices s0:t-1, x0:t-1, we
train a network to predict beliefs over the current timestep preferences ut , which we then com-
bine with the current slate st to predict a distribution over item choices by using the choice model
P(xt|ut,st): P(xt|s0:t,x0:t-1) = Rut P(xt|ut,st)P(ut|s0:t-1,x0:t-1). At training time, we su-
pervise this with the actual choice Xt the user made for slate St - the network will learn to predict
beliefs over preferences which induce behavior which is seen in the training data.
preferences would shift if we exposed them to π0 from the beginning; we call these two the future
and, respectively, counterfactual preference estimation problems, covered respectively in Sec.s 4.1
and 4.2. To facilitate understanding, for each we first describe how to solve it if one had oracle
access to the true user internal state dynamics, and then relax this assumption showing how to learn
to approximately perform the inference from observable interaction data.
Choice label xt
Known choice model
" assumption
4.1	Future Preferences Estimation
We start with the problem of estimating future preferences. That is, for a set of N users, we have
access to historical data of their interaction with an RS policy π, of the form Dj = {s〈:T, x∏.T}
for every user j. We now are interested in estimating each user’s current preferences, and how they
would evolve if exposed to a new policy π0 .
Estimation under known internal state dynamics. First, we consider the simplified problem in
which We have oracle access to the true user internal state dynamics, i.e. P^zt+1 ∣zt, π(s0.t, x0.t))
for any policy π. Our goal is to estimate the user preferences under the new policy π0 at a fu-
ture timestep H - which We denote as UH. Since We know the user internal state dynamics, via a
trivial extension from HMM prediction (Russell & Norvig, 2002), we can use the history of inter-
actions (s∏.T, x∏.T) to estimate the internal state ZT+1 of the user: P(ZT+1 ∣s∏.τ, x∏.τ). Then, this
filtering estimate can be rolled out with the forward dynamics induced by π0: P(ZnZ ∣s∏.τ, x∏.τ)=
RZn P(ZH |zT +1)P(ZT+1 ∣s∏.τ, x∏.τ). As preferences UH are just one component of the internal state
T0
ZHπ , one can trivially recover a posterior over them.
Estimation under unknown internal state dynamics. In practice, one will not have access to
an explicit representation of the user’s internal state Zt, let alone a model of its dynamics. We
thus attempt to approximate the NHMM estimation task we are interested in by implicitly learning
user preference dynamics from their past interaction data. We train a neural network that, given
histories of slates and choices, outputs a belief over preferences b0π.T (Ut). While we do not get
direct supervision on preferences, we do get supervision on the choices users make (see Fig. 2).
Using the choice model we assume in Sec. 3, we map the belief over preferences, together with the
new slate st, to a distribution over items or choices b0π.T (xt). At training time, we use the data Dj
for each user and for each time step t to train the model via supervised learning, with a loss defined
on the choice distribution outputted by the model, and the actual choice xt the user made. Such a
model learns to perform approximate inference over Ut in the underlying NHMM (see Appendix A).
Then at inference time, to predict future preferences for a user j under a new policy π0 , we sample
simulated user preference trajectories from the model to approximate Monte Carlo estimation in
the NHMM. We use the data Dj as input to the model to obtain a belief over the next-timestep
preferences UT+1. We then use such belief and the slate at time T + 1 from policy π0, sπT0+1 =
π0 (s0.T, x0.T), to simulate the user’s choice xTπ0+1. Treating this extra (simulated) step of interaction
history for the user (the slate sπT0+1 and choice xπT0+1) as part of the observed history so far, we repeat
this process to obtain preference estimates under π0 for future timesteps. By simulating multiple
future trajectories, we obtain beliefs over the expected preferences a user (or a cohort of users)
would have at any future timestep. See Algo. 1 for the exact procedure.
While the above method requires the learned future-preference predictor model to generalize to
being able to incorporate histories collected under different RS policies than the ones in the training
4
Under review as a conference paper at ICLR 2022
Future
preference
estimation
Estimated under π
Estimated under π,
Figure 3: Simulating futures. By iteratively using the future preferences estimation model (Fig. 2)
by inputting the observables (shaded, e.g. xg6, s6:6), one can simulate how an existing user's Pref-
erences would evolve in the future if they interacted with same policy π or a different policy π0 (by
selecting future slates with π0).
data, there is still hope in that real-world datasets of user interaction were been collected under
many different deployed policies. This suggests that as long as the slates chosen by π0 are not
too dissimilar from π (a mixture policy that comprises all the policies used to gather the data), the
network should be able to generalize to predict preference evolutions induced by π0 too.
4.2	Counterfactual Preferences Estimation
While the methodology described in Sec. 4.1 is sufficient to estimate the expected preferences and
behaviors a user will have in the future, it cannot be readily used to estimate counterfactual prefer-
ences and behaviors: given a set of past interactions Dj collected under policy π, we cannot predict
what the preferences would have been for this user if an alternate policy π0 had been used from the
0
first timestep of interaction instead of π - we denote these CoumerfaCtual preferences as u0∏.T. On a
high-level, to perform the counterfactual estimation task we want to extract as much information as
possible about the user’s initial internal state from the historical interactions Dj we have available
for them: even though such interactions were collected with a different policy π , they will still con-
tain information about the user’s initial state (including their initial preferences before interacting
with the RS). Then, based on this belief about the user’s initial internal state, we want to obtain a
user-specific estimate of the effect that another policy π0 would have had on their preferences.
Estimation under known internal state dynamics. Under oracle access to the internal state dy-
namics, we can first obtain the belief over the initial state of a given user P(z0 ∣s∏1, x,：T) via
NHMM smoothing - which is a trivial extension of HMM smoothing (Russell & Norvig, 2002)
- and then roll out the human model forward dynamics with the fixed policy π0 instead of π :
P(Z∏0 ∣s∏1, x；:T) = RZθ P(Z∏0 ∣z0)P(z0∣s∏1, x；：T) 一 equivalently to Sec. 4.1 when there is no past
context but a specific prior for the internal state at t = 0.
Estimation under unknown internal state dynamics. Without oracle access to the internal state
dynamics, again we try to learn to perform this NHMM counterfactual task approximately. One
challenge in obtaining supervision for the task is that in our dataset of interactions we never get to see
true counterfactuals. We get around this by decomposing the task into two parts, for which we use
two separate models: (1) estimating the initial internal state for the user (based on the interaction data
x∏T, s∏T under π which we have available), i.e. train a predictor which approximates the NHMM
smoothing distribution P(u0 ∣x∏1, s；：T) which we will denote here as b；:T(u0) (the belief over
initial preferences); and then (2) estimating u；：T conditional on the belief of the initial preferences.
The models for the two tasks are trained with the same method used for predicting future preference
estimation (Fig. 2) but with different inputs and supervision signal (Fig. 4). For task (1), the network
is trained to predict the initial (instead of future) preferences of the user based on later context
xn：T, sn：T. While the network predicts P(u0∣x∏.τ, s∏.τ), we can recover the correct smoothing
estimate as P(u0∣x∏.τ, s∏.τ) H P(x∏ ∣s∏, u0)P(u0∣x∏.τ, s∏.T) (see Appendix A).
The network for task (2) is obtained by changing the prediction network to also condition
on the recovered initial preferences (Fig. 2-4), enabling us to make predictions of the form
P(Uk+1 ∣b∏:T(u0),x∏k, s&k). For training, we first recover initial preference beliefs for each user j
in the data Dj with network for task (1) (which we assume has already been trained). We then train
the network for (2) to reconstruct the user j’s actual interactions (under π) simply based on this ini-
tial preference estimate (again, similarly to the case in Sec. 4.1 but by additionally conditioning on
the initial preference belief). This teaches the network to leverage the information contained user’s
initial preferences estimate to better estimate their preferences and behaviors based on what their
interactions have been so far. In practice, we train these models using a transformer architecture
similar to Sun et al. (2019), and we detail in Appendix A why this is a good fit for our tasks.
5
Under review as a conference paper at ICLR 2022
1) Initial preference
estimation
Counterfactual
preference
estimation
@...........ŋ.........ŋ
O x I ' ` ` πO... I	兀匚二I I
MH ； input Mh	Mh
2) Conditioned
future estimation
...口
I
亡
..@
)口⅜⅛

t=0	t=l	t=2
Figure 4: Simulating Counterfactuals. To simulate What the user's preferences would have been
under a different policy π0 , we first can use the initial preference model to estimate initial preferences
based on later observables. We can then use the estimated initial preferences to simulate counter-
factual preference trajectories for the user under π0 using a conditioned future preference estimation
model - where estimates are conditioned on the recovered initial preference belief.
00
At inference time, we recover an estimate of initial preferences based on interaction data (s；:T, x；:T)
of a new user with a new recommender policy π0 with model (1), and then estimate the preferences
such user would have had under a safe policy πsafe with model (2). Such counterfactual preference
estimate is obtained with Monte Carlo simulations similarly to Sec. 4.1 with the difference that the
network is also conditioned on the initial preferences estimate. See Algo. 2 for the full algorithm
and Appendix A for why this approximates the NHMM task.
We now have a way to estimate - for a new policy π0 一 what we would expect its counterfactual
impact would have been relative to having deployed π (from which we have observational data; note
that similarly to future preferences estimation, this procedure too can suffer if π0 induces a strong
distribution shift relative to the training data). We next discuss potential quantitative evaluation
metrics defined on these estimates.
5	Quantifying unwanted shifts and optimizing to avoid them
Given these estimates of preference shifts, it would be useful to have quantitative metrics for whether
they are undesirable - as this would enable automatically evaluating policies and even actively opti-
mizing to avoid such shifts. To obviate the difficulty of explicitly defining unwanted shifts, we limit
ourselves to defining shifts that we trust (“safe shifts“ us0a:fTe ), and flag preference shifts that largely
differ from these safe shifts as potentially unwanted. This is similar to defining a region of the space
which we trust in a risk-averse manner. The underlying philosophy is that “we don’t know what
good shifts are, but as long as you stay close to [these shifts], things won’t go too poorly“. There-
fore, we need both a notion of shifts we trust (“safe shifts“), and a notion of closeness (or distance)
between induced preference shifts.
Notation. We denote as rt (Utafe, π) the engagement at time t for the content that is chosen under the
policy π - if it were evaluated under the safe-shift preferences Utafe — i.e. rt (Utafe, π) = (x∏)Tutafe.
Distance between shifts. We choose a metric between shifts such that engagement for the items
x∏T chosen under policy π is also high under the preferences one would have had under safe shifts
u0fT∙ This is operationalized as D(Un:T, u0fT) = Pt E[(x∏)TUn - (Xn)TUtafe] = Pt E [rt(u∏)-
rt(utafe, π)] as our notion of distance between π-induced shifts and safe shifts U0T.
Safe shifts: u0. As a first (very crude) proposal for safe shifts, we can consider no shifts at all: any
deviation from the initial preferences U0 can be flagged as potentially problematic with D(U0n:T, U0).
Safe shifts: natural preference shifts (NPS). Not all preference shifts are unwanted: people rou-
tinely change their preferences and behavior “naturally“. But what does “naturally“ even mean? We
propose an idealized notion of natural shifts, by asking how preferences would evolve if the user
were ”omniscient”, i.e. would have access to all content directly and had the ability to process it,
unhindered by a small and biased slate offered by an RS. Intuitively, such a user’s preferences would
still shift over time. Unfortunately this is impractical, as we can never get data from such hypothet-
ical users that can attend to all content when choosing what to consume. As an approximation, we
use random slates - these slates are still small, but they sampled randomly from the content, elimi-
nating the bias of any RS policy (which in turn can change the user’s belief about the distribution of
available content). We therefore operationalize “natural preferences“ Utnrnd as the preferences which
the user would have interacting with a random RS, and we use D(U0n:T, U0n:rnTd ) as the metric.
6
Under review as a conference paper at ICLR 2022
Figure 5: Ground truth human dynamics. At each timestep, the user Win receive a slate st. Given
the user’s preferences ut, the slate st induces a distribution over item choices P(xt|st, ut) from
which the user samples an item Xt and receives an engagement value rt (unobserved by the RS).
Additionally, st induces a belief over the future slates in the user bH(s). In turn bH(S) - together
with Ut — induce a distribution over next timestep user preferences, from which ut+1 is sampled.
Penalized training. By training the models described in Sec. 4, one essentially obtains a human
model which can be used to simulate human choices (and preferences). Similarly to previous work,
we can use this human model as a simulator for RL training of recommender systems (Chen et al.,
2019; Zhao et al., 2019; Bai et al., 2020). The metrics defined above give us a way to augment the
reward function to penalize the policy for causing any shifts that we have not explicitly identified
as "safe” - in what can be considered a “risk-averse” design choice. We incorporate these metrics
in the training of a new RS policy π by adding the two distance metrics from above to the basic
objective PT E [^t(u∏)] of maximizing long-term engagement, leading to the updated objective
PT E [rt (u∏) + ν1 rt (u0, π) + V2 rt(Unrnd, π)] where ν'1, V2 are hyperparameters — see Appendix B.
6	Experimental Setup
Why simulation? To test our method, we need to evaluate both RL policies that interact with users
(rendering static datasets of user interaction unsuitable), as well as the metrics themselves, which
are defined based on internal preferences (for which we never get ground truth in real interaction).
We thus create a testing environment in which we can emulate user behavior and have access to their
ground truth preferences. Like previous work (Chaney et al., 2018; Bountouridis et al., 2019; Jiang
et al., 2019; Mansoury et al., 2020; Yao et al., 2021), we simulate both a recommendation environ-
ment and human behavior. However, unlike such approaches, we use simulated human behavior for
testing purposes only: our human model is learned exclusively from data that would be observed in
a real-world RS (slates and choices), i.e. that of users (in our case the simulated users) interacting
with previous RS policies — meaning our approach could be applied to real user data of this form.
A fundamental advantage of testing our methods in a simulated environment is that we can actually
evaluate how well our model is able to recover the preferences of our ”ground truth” users, giving
us insights about how our methods could perform with real users.
Ground truth human dynamics. See Fig. 9 & 5 for a summary of our environment setup and
the ground truth human dynamics we use for testing our method. As mentioned in Sec. 3, we use
the conditional logit model as choice model — in our setup, this reduces to P(Xt = x|st, ut) Y
P(st = x)eβcxT ut , with an additional term P(st = x) which takes into account how prevalent
each item is in the slate (see Appendix C.1) — and we assume this to be also known by our method.
We adapt Bernheim et al. (2021) to be our ground truth human preference dynamics. On a high-
level, at each timestep users choose their next-timestep preferences update their preference to more
“convenient“ ones - trading off between choosing preferences that they expect will lead them to
higher engagement and maintaining engagement under current preferences.
For ease of interpretation, we only consider a cohort of the RS’s users whose initial user preferences
form a normal distribution around preference U = 130°. To showcase preference-manipulation in-
centives to make users more predictable, we make the choice-stochasticity temperature βc a function
of part of preference space one is in, with local optima βc (80°) = 1 and βc (270°) = 4 (see Ap-
pendix C.1 for more details). As users obtain higher engagement value when they act less stochasti-
cally, these portions of preference space form attractor points as can be seen in all policies in Fig. 1.
While preferences naturally tend to shift towards one of these modes, some RS policies drive prefer-
ences to the other mode. As the engagement does not correspond to actual value, converging to the
higher local optimum of engagement (u = 270° instead of u = 80°) is not necessarily desirable.
7
Under review as a conference paper at ICLR 2022
Figure 6: When adding our proxies to the op-
timization of our RL LTV system, the induced
distribution of preferences are closer to the NPS
and initial preferences. Myopic systems instead
will only greedily be pursuing the penalized
objective, leading the resulting behavior to be
somewhat arbitrary.
0.20
0.15
0.10
0.05
0.00
Figure 7: The actual preferences induced by the
NPS policy (a random RS πrnd) among a co-
hort of 1000 users (left) vs. a 1000-sampled-
trajectory Monte Carlo estimate of what shifts
would be induced. Qualitatively, the estimated
preferences seem to match the ground truth ones
-although slightly more washed out.
Training human models and penalized RS policies. For training our human models, we use
a BERT model (Sun et al., 2019), and only assume access to a dataset of historical interactions
s0:T, x0:T. We train myopic and RL RS policies π0 using PPO (Schulman et al., 2017; Liang et al.,
2018) and restrict the action space to 6 possible slate distributions for ease of training. For penalized
training we give rt(u∏),^t(u0, π0),^t(u∏rnd, π0) equal weight. See Appendix C for details.
7	Results
Recovering preferences. Firstly, we confirm that our preference estimation models are able to
recover very similar estimates to the equivalent NHMM inference tasks (which, unlike our method,
needs access to the ground truth user preference dynamics) 一 see Appendix D for details, and Fig. 7
for an example of the estimated preferences relative to ground truth ones. This suggests that - under
a correct choice model assumption and sufficient similarity between the RS policies represented in
the historical datasets, and the ones We are interested in - our method is able to implicitly learn the
preference dynamics of a user.
Hypotheses about metrics. We now turn to our hypotheses regarding metrics. We hypothesize that
our metrics are able to: (H1) identify whether policies will induce unwanted preference shifts, and
(H2) incentivize better behavior when added as a penalty during training.
Learned human model confound. With real users, to validate H1 one would have to rely on
approximate computation of the metrics (estimated evaluation), as computing the metrics would
require using our learned user models of preference dynamics. Additionally, to validate H2, one
would likely train with simulated interactions from the learned user model (training in simulation)
- as explained in Sec. 5. The quality of the learned user model would thus be a confound for testing
the hypotheses, i.e. the quality of the form of the metrics themselves.
H1 under oracle dynamics access. In order to deconfound our experiments from the errors in our
estimated human dynamics, We first test these hypotheses assuming oracle access to users - meaning
that the policy would be deployed to them - and their dynamics. We first hypothesize that (H1.1) by
computing the metrics exactly using the preference estimates obtained through NHMM inference
(oracle evaluation), they are able to flag potentially unwanted preference shifts. We find this to
be the case by comparing the oracle evaluation metric values (left of Table 1, ”unpenalized”) and
the actual preference shifts induced by the various RSs we consider (Fig. 1): while unpenalized RL
performs better (7.49 vs 5.71) than myopia for engagement rt(u∏0), it performs worse with respect
to our safe shift metrics. This matches Fig. 1, where RL has more undesired effects.
H2 under oracle dynamics access. We additionally hypothesize that (H2.1) such metrics (still
computed exactly, in oracle evaluation) can be used for training penalized LTV systems which
avoid the most extreme unwanted preference shifts (and for this training, we will allow ourselves
on-policy human interaction data with the ground truth users, as if the RL happened directly in
the real world - we call this oracle training. For the penalized RL RS, the cumulative metric
value (“Sum“ in Table 1) increases substantially, although it is at the slight expense of instantaneous
engagement (Table 1). Qualitatively, we see that the induced preference shifts caused by the RL
system seem closer to “safe shifts“ (Fig. 6), supporting H1.1 in that high metric values qualitatively
match shifts that seem more desirable. Overall, we see that with oracle access, the metrics capture
8
Under review as a conference paper at ICLR 2022
What We see as qualitatively undesired shifts, and that optimizing for them directly (penalized RL)
produces policies With slightly loWer engagement, but drastically better at avoiding such shifts.
H1 and H2 with learned user dynamics. Our findings support our hypotheses under oracle access.
HoWever, in real life We Will not have access to the underlying dynamics model, or even the ability
to interact on-policy With users as We train RL policies, due to the high cost and risk of negative side
effects for collecting data With unsafe policies. Therefore, We Wish to shoW that even With estimated
metrics and simulated interactions (based on the learned models), (H1.2) estimated evaluation and
(H2.2) training in simulation (described above) are still able to respectively flag unWanted shifts
and penalize manipulative RL behaviors. Table 2 shows that - although the estimated metrics can
differ from the ground truth ones someWhat substantially (see Oracle Eval. at the top vs Estimated
Eval. at the bottom) 一 importantly the relative ordering of the policies ranked by our estimated values
stay the same: the penalized RL policy trained in simulation actually has (under oracle evaluation)
higher cumulative reward than the unpendalized one, and the estimated evaluation keeps that ranking
(even though is more optimistic about the unpenalized reward).
Table 1:	Results under oracle training
and evaluation. Both here an in Table 2
we report the cumulative expected engage-
ment under a variety of different prefer-
ences (as explained in Sec. 5). The val-
ues are averaged across 1000 trajectories
(Standard errors areall < 0.1). By looking at
the safe shift metrics r(uo) and r(utnd), We
see that penalized systems stay significantly
closer to safe shifts than unpenalized ones.
Oracle Training
Unpenalized Penalized
Myopic RL Myopic RL
Table 2:	Effect of estimating evaluations and sim-
ulating training for LTV. We see that the esti-
mated evaluations of trained systems strongly cor-
relate with the oracle evaluations (and importantly,
maintain their relative orderings). A similar effect
occurs when training in simulation rather than by
training with on-policy data collected from real users.
Oracle Training Training in Sim.
Unpen. Penal. Unpen. Penal.
8175
225.0
5.6.4.16
.20.61.10.90
63312
.08.0933
7.-0-16.
1919
.7.9.0.6
5129
rt (u∏rnd, Sum	π	0)
rt(u∏0 Irt(U0,1 rt(u∏rnd, Sum	) π0 π	) 0)
9893 8851
.4.0.0.3.5.2.0.9
7-0-16 5128
486143.52
5.5.4.15
0436
40.2.836
6.-1-13.
8175
225.0
5.6.4.16
8415
794.1
5.4.4.15
49.804817
6.-01.7.
.42.57.88.87
55314
IEAwə一ɔEJo
J
π
IEAH əlɔEjRAw-sw
8	Discussion
Summary. In conclusion, our contributions are 1) proposing a methodology to estimate the pref-
erence shifts which would be induced by recommender system policies before deployment, and 2)
providing a formalism for defining distance metrics from safe preference shifts, which can be used
both for evaluation and to train RSs which penalize unwanted shifts. As dynamics of preference are
learned (rather than handcrafted), our method could be applied to real user data and has the poten-
tial for higher accuracy than handcrafted models due to massive amounts of behavior data. Further,
while there is no ground truth for human preferences, verifying the model’s ability to anticipate
behavior can give confidence in using it to evaluate and penalize undesired preference shifts.
Limitations and future work. We conducted our evaluation in simulation, which was necessary
for testing the metrics themselves - for this we required ground truth access to user preferences.
But since we used a specific dynamics model in our simulator, we cannot guarantee that our results
translate when the method is applied to real users with real preference dynamics. We expect real
users to have more complex dynamics, with preference changes mediated by beliefs outside of
content distribution as in our simulator, such as beliefs about the world; while our method makes no
assumptions about the structure of this internal space, it might require a lot more (and more diverse)
data to capture these effects. Further, we also limited our evaluation to a setting explicitly designed
such that an RL system would have incentives to manipulate preferences - not all real settings will
necessarily have that property. Moreover, crucially our method requires assuming access to the user
choice model: one could obviate this by predicting behavior and defining metrics on behavior, but
this would mean losing the latent preference structure. Additionally, even what we call “preferences“
一 de-noised behavior - is limiting as it doesn,t lend itself to capturing long-term preferences.
9
Under review as a conference paper at ICLR 2022
References
Aspiration: The Agency of Becoming. Oxford University Press, Oxford, New York, April 2018.
ISBN 978-0-19-063948-8.
M. Mehdi Afsar, Trafford Crump, and Behrouz Far. Reinforcement learning based recommender
systems: A survey. arXiv:2101.06286 [cs], January 2021. URL http://arxiv.org/abs/
2101.06286. arXiv: 2101.06286.
Cecilie Schou Andreassen. Online Social Network Site Addiction: A Comprehensive Review.
Current Addiction Reports, 2(2):175-184, June 2015. ISSN 2196-2952. doi: 10.1007/
s40429- 015- 0056- 9. URL https://doi.org/10.1007/s40429-015-0056-9.
Xueying Bai, Jian Guan, and Hongning Wang. Model-Based Reinforcement Learning with Adver-
sarial Training for Online Recommendation. arXiv:1911.03845 [cs, stat], January 2020. URL
http://arxiv.org/abs/1911.03845. arXiv: 1911.03845.
B Douglas Bernheim, LUca Braghieri, Alejandro Martlnez-Marquina, and David Zuckerman. A
theory of chosen preferences. American Economic Review, 111(2):720-54, 2021.
Dimitrios Bountouridis, Jaron Harambam, Mykola Makhortykh, Monica Marrero, Nava Tintarev,
and Claudia Hauff. SIREN: A Simulation Framework for Understanding the Effects of Recom-
mender Systems in Online News Environments. In Proceedings of the Conference on Fairness,
Accountability, and Transparency, pp. 150-159, Atlanta GA USA, January 2019. ACM. ISBN
978-1-4503-6125-5. doi: 10.1145/3287560.3287583. URL https://dl.acm.org/doi/
10.1145/3287560.3287583.
Allison J. B. Chaney, Brandon M. Stewart, and Barbara E. Engelhardt. How Algorithmic Con-
founding in Recommendation Systems Increases Homogeneity and Decreases Utility. Proceed-
ings of the 12th ACM Conference on Recommender Systems, pp. 224-232, September 2018.
doi: 10.1145/3240323.3240370. URL http://arxiv.org/abs/1710.11214. arXiv:
1710.11214.
Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, and Ed Chi. Top-K
Off-Policy Correction for a REINFORCE Recommender System. arXiv:1812.02353 [cs, stat],
November 2020. URL http://arxiv.org/abs/1812.02353. arXiv: 1812.02353.
Xinshi Chen, Shuang Li, Hui Li, Shaohua Jiang, Yuan Qi, and Le Song. Generative Adversarial
User Model for Reinforcement Learning Based Recommendation System. arXiv:1812.10613 [cs,
stat], December 2019. URL http://arxiv.org/abs/1812.10613. arXiv: 1812.10613.
Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye,
Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan
Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah. Wide & Deep Learning for Recommender
Systems. arXiv:1606.07792 [cs, stat], June 2016. URL http://arxiv.org/abs/1606.
07792. arXiv: 1606.07792.
Dan Cosley, Shyong K. Lam, Istvan Albert, Joseph A. Konstan, and John Riedl. Is seeing be-
lieving? how recommender system interfaces affect users’ opinions. In Proceedings of the
SIGCHI Conference on Human Factors in Computing Systems, CHI ’03, pp. 585-592, New York,
NY, USA, April 2003. Association for Computing Machinery. ISBN 978-1-58113-630-2. doi:
10.1145/642611.642713. URL https://doi.org/10.1145/642611.642713.
Paul Covington, Jay Adams, and Emre Sargin. Deep Neural Networks for YouTube Recom-
mendations. In Proceedings of the 10th ACM Conference on Recommender Systems - RecSys
’16, pp. 191-198, Boston, Massachusetts, USA, 2016. ACM Press. ISBN 978-1-4503-4035-
9. doi: 10.1145/2959100.2959190. URL http://dl.acm.org/citation.cfm?doid=
2959100.2959190.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep
Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs], May 2019.
URL http://arxiv.org/abs/1810.04805. arXiv: 1810.04805.
10
Under review as a conference paper at ICLR 2022
Jason Gauci, Edoardo Conti, Yitao Liang, Kittipat Virochsiri, Yuchen He, Zachary Kaden, Vivek
Narayanan, Xiaohui Ye, Zhengxing Chen, and Scott Fujimoto. Horizon: Facebook’s Open Source
Applied Reinforcement Learning Platform. arXiv:1811.00260 [cs, stat], September 2019. URL
http://arxiv.org/abs/1811.00260. arXiv: 1811.00260.
Ulrike Gretzel and Daniel Fesenmaier. Persuasion in Recommender Systems. International Jour-
nal OfElectronic Commerce, 11(2):81-100, December 2006. ISSN 1086-4415. doi: 10.2753/
JEC1086-4415110204. URL https://doi.org/10.2753/JEC1086-4415110204.
Charles R. Harris, K. Jarrod Millman, Stefan J. van der Walt, Ralf Gommers, PaUli Virtanen,
David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert
Kern, Matti PicUs, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane,
Jaime Fernandez del Rio, Mark Wiebe, Pearu Peterson, Pierre Gerard-Marchant, Kevin Sheppard,
Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Ar-
ray programming with NumPy. Nature, 585(7825):357-362, September 2020. doi: 10.1038/
s41586- 020- 2649- 2. URL https://doi.org/10.1038/s41586-020-2649-2.
Md Rajibul Hasan, Ashish Kumar Jha, and Yi Liu. Excessive use of online video streaming services:
Impact of recommender system use, psychological factors, and motives. Computers in Human
Behavior, 80:220-228, March 2018. ISSN 07475632. doi: 10.1016/j.chb.2017.11.020. URL
https://linkinghub.elsevier.com/retrieve/pii/S0747563217306581.
David Holtz, Benjamin Carterette, Praveen Chandar, Zahra Nazari, Henriette Cramer, and Sinan
Aral. The Engagement-Diversity Connection: Evidence from a Field Experiment on Spotify.
arXiv:2003.08203 [cs], March 2020. URL http://arxiv.org/abs/2003.08203. arXiv:
2003.08203.
James P. Hughes, Peter Guttorp, and Stephen P. Charles. A Non-Homogeneous Hidden Markov
Model for Precipitation Occurrence. Journal of the Royal Statistical Society. Series C (Applied
Statistics), 48(1):15-30, 1999. URL http://www.jstor.org/stable/2680815.
Gerald Haubl and Kyle B. Murray. Preference Construction and Persistence in Digital Marketplaces:
The Role of Electronic Recommendation Agents. Journal of Consumer Psychology, 13(1):75-
91, January 2003. ISSN 1057-7408. doi: 10.1207/S15327663JCP13-1&2_07. URL http:
//www.sciencedirect.com/science/article/pii/S1057740803701788.
Eugene Ie, Vihan Jain, Jing Wang, Sanmit Narvekar, Ritesh Agarwal, Rui Wu, Heng-Tze Cheng,
Morgane Lustman, Vince Gatto, Paul Covington, Jim McFadden, Tushar Chandra, and Craig
Boutilier. Reinforcement Learning for Slate-based Recommender Systems: A Tractable De-
composition and Practical Methodology. arXiv:1905.12767 [cs, stat], May 2019. URL http:
//arxiv.org/abs/1905.12767. arXiv: 1905.12767.
Ray Jiang, Silvia Chiappa, Tor Lattimore, AndraS Gyorgy, and Pushmeet Kohli. Degenerate Feed-
back Loops in Recommender Systems. Proceedings of the 2019 AAAI/ACM Conference on
AI, Ethics, and Society, pp. 383-390, January 2019. doi: 10.1145/3306618.3314288. URL
http://arxiv.org/abs/1902.10730. arXiv: 1902.10730.
Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. Span-
BERT: Improving Pre-training by Representing and Predicting Spans. arXiv:1907.10529 [cs],
January 2020. URL http://arxiv.org/abs/1907.10529. arXiv: 1907.10529.
Prerna Juneja and Tanushree Mitra. Auditing E-Commerce Platforms for Algorithmically Curated
Vaccine Misinformation. pp. 27, 2021.
Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. Planning and acting in
partially observable stochastic domains. Artificial Intelligence, 101(1-2):99-134, May 1998.
ISSN 00043702. doi: 10.1016/S0004-3702(98)00023-X. URL https://linkinghub.
elsevier.com/retrieve/pii/S000437029800023X.
Yehuda Koren. Collaborative filtering with temporal dynamics. In In Proc. of KDD ’09, pp. 447-
456, 2009.
Victoria Krakovna, Laurent Orseau, Ramana Kumar, Miljan Martic, and Shane Legg. Penalizing
side effects using stepwise relative reachability. pp. 21.
11
Under review as a conference paper at ICLR 2022
Adam D. I. Kramer, Jamie E. Guillory, and Jeffrey T. Hancock. Experimental evidence of massive-
scale emotional contagion through social networks. Proceedings of the National Academy of
Sciences, 111(24):8788-8790, June 2014. ISSN 0027-8424, 1091-6490. doi: 10.1073/pnas.
1320040111. URL https://www.pnas.org/content/111/24/8788. Publisher: Na-
tional Academy of Sciences Section: Social Sciences.
David Krueger, Tegan Maharaj, and Jan Leike. Hidden Incentives for Auto-Induced Distributional
Shift. arXiv:2009.09153 [cs, stat], September 2020. URL http://arxiv.org/abs/2009.
09153. arXiv: 2009.09153.
Ramana Kumar, Jonathan Uesato, Richard Ngo, Tom Everitt, Victoria Krakovna, and Shane Legg.
REALab: An Embedded Perspective on Tampering. arXiv:2011.08820 [cs], November 2020.
URL http://arxiv.org/abs/2011.08820. arXiv: 2011.08820.
Lei Li, Li Zheng, Fan Yang, and Tao Li. Modeling and broadening temporal user interest in
personalized news recommendation. Expert Systems with Applications, 41(7):3168-3177, June
2014. ISSN 09574174. doi: 10.1016/j.eswa.2013.11.020. URL https://linkinghub.
elsevier.com/retrieve/pii/S0957417413009329.
Eric Liang, Richard Liaw, Philipp Moritz, Robert Nishihara, Roy Fox, Ken Goldberg, Joseph E.
Gonzalez, Michael I. Jordan, and Ion Stoica. RLlib: Abstractions for Distributed Reinforce-
ment Learning. arXiv:1712.09381 [cs], June 2018. URL http://arxiv.org/abs/1712.
09381. arXiv: 1712.09381.
Zhongqi Lu and Qiang Yang. Partially observable markov decision process for recommender sys-
tems. arXiv preprint arXiv:1608.07793, 2016.
Colin MacLeod and Lynlee Campbell. Memory accessibility and probability judgments: An exper-
imental evaluation of the availability heuristic. Journal of Personality and Social Psychology, 63
(6):890-902, 1992. ISSN 1939-1315(Electronic),0022-3514(Print). doi: 10.1037/0022-3514.63.
6.890. Place: US Publisher: American Psychological Association.
Masoud Mansoury, Himan Abdollahpouri, Mykola Pechenizkiy, Bamshad Mobasher, and Robin
Burke. Feedback Loop and Bias Amplification in Recommender Systems. arXiv:2007.13019
[cs], July 2020. URL http://arxiv.org/abs/2007.13019. arXiv: 2007.13019.
S. C. Matz, M. Kosinski, G. Nave, and D. J. Stillwell. Psychological targeting as an effective
approach to digital mass persuasion. Proceedings of the National Academy of Sciences, 114(48):
12714-12719, November 2017. ISSN 0027-8424, 1091-6490. doi: 10.1073/pnas.1710966114.
URL http://www.pnas.org/lookup/doi/10.1073/pnas.1710966114.
Martin Mladenov, Ofer Meshi, Jayden Ooi, Dale Schuurmans, and Craig Boutilier. Advantage
Amplification in Slowly Evolving Latent-State Environments. arXiv:1905.13559 [cs, stat], May
2019. URL http://arxiv.org/abs/1905.13559. arXiv: 1905.13559.
Dheevatsa Mudigere, Yuchen Hao, Jianyu Huang, Andrew Tulloch, Srinivas Sridharan, Xing Liu,
Mustafa Ozdal, Jade Nie, Jongsoo Park, Liang Luo, Jie Amy Yang, Leon Gao, Dmytro Ivchenko,
Aarti Basant, Yuxi Hu, Jiyan Yang, Ehsan K. Ardestani, Xiaodong Wang, Rakesh Komurav-
elli, Ching-Hsiang Chu, Serhat Yilmaz, Huayu Li, Jiyuan Qian, Zhuobo Feng, Yinbin Ma,
Junjie Yang, Ellie Wen, Hong Li, Lin Yang, Chonglin Sun, Whitney Zhao, Dimitry Melts,
Krishna Dhulipala, K. R. Kishore, Tyler Graf, Assaf Eisenman, Kiran Kumar Matam, Adi
Gangidi, Guoqiang Jerry Chen, Manoj Krishnan, Avinash Nayak, Krishnakumar Nair, Bharath
Muthiah, Mahmoud khorashadi, Pallab Bhattacharya, Petr Lapukhov, Maxim Naumov, Lin Qiao,
Mikhail Smelyanskiy, Bill Jia, and Vijay Rao. High-performance, Distributed Training of Large-
scale Deep Learning Recommendation Models. arXiv:2104.05158 [cs], April 2021. URL
http://arxiv.org/abs/2104.05158. arXiv: 2104.05158.
Tien T. Nguyen, Pik-Mai Hui, F. Maxwell Harper, Loren Terveen, and Joseph A. Konstan. Exploring
the filter bubble: the effect of using recommender systems on content diversity. In Proceedings
of the 23rd international conference on World wide web - WWW ’14, pp. 677-686, Seoul, Korea,
2014. ACM Press. ISBN 978-1-4503-2744-2. doi: 10.1145/2566486.2568012. URL http:
//dl.acm.org/citation.cfm?doid=2566486.2568012.
12
Under review as a conference paper at ICLR 2022
Shumpei Okura, Yukihiro Tagami, Shingo Ono, and Akira Tajima. Embedding-based News Rec-
ommendation for Millions of Users. In Proceedings of the 23rd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, pp.1933-1942, Halifax NS Canada, AU-
gust 2017. ACM. ISBN 978-1-4503-4887-4. doi: 10.1145/3097983.3098108. URL https:
//dl.acm.org/doi/10.1145/3097983.3098108.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James BradbUry, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, LUca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank ChilamkUrthy, Benoit Steiner,
LU Fang, JUnjie Bai, and SoUmith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc,
E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp.
8024-8035. CUrran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.
pdf.
L. A. PaUl. Transformative experience. Oxford University Press, Oxford, 1st ed edition, 2014. ISBN
978-0-19-871795-9. OCLC: ocn872342141.
L R Rabiner. An IntrodUction to Hidden Markov Models. pp. 13.
Dimitrios Rafailidis and Alexandros NanopoUlos. Modeling Users Preference Dynamics and Side
Information in Recommender Systems. IEEE Transactions on Systems, Man, and Cybernetics:
Systems, 46(6):782-792, JUne 2016. ISSN 2168-2216, 2168-2232. doi: 10.1109/TSMC.2015.
2460691. URL http://ieeexplore.ieee.org/document/7194815/.
Manoel Horta Ribeiro, Raphael Ottoni, Robert West, Virgllio A. F. Almeida, and Wagner Meira.
AUditing Radicalization Pathways on YoUTUbe. arXiv:1908.08313 [cs], December 2019. URL
http://arxiv.org/abs/1908.08313. arXiv: 1908.08313.
StUart RUssell and Peter Norvig. Artificial intelligence: a modern approach. 2002.
John SchUlman, Filip Wolski, PrafUlla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
AshUdeep Singh, Yoni Halpern, NithUm Thain, Konstantina ChristakopoUloU, Ed H Chi, Jilin Chen,
and Alex BeUtel. BUilding Healthy Recommendation SeqUences for Everyone: A Safe Reinforce-
ment Learning Approach. pp. 10.
Jonathan Stray. Designing Recommender Systems to Depolarize. arXiv:2107.04953 [cs], JUly 2021.
URL http://arxiv.org/abs/2107.04953. arXiv: 2107.04953.
Fei SUn, JUn LiU, Jian WU, ChanghUa Pei, Xiao Lin, WenwU OU, and Peng Jiang. BERT4Rec:
SeqUential Recommendation with Bidirectional Encoder Representations from Transformer.
arXiv:1904.06690 [cs], AUgUst 2019. URL http://arxiv.org/abs/1904.06690.
arXiv: 1904.06690.
Peter SUnehag, Richard Evans, Gabriel DUlac-Arnold, Yori Zwols, Daniel Visentin, and Ben
Coppin. Deep Reinforcement Learning with Attention for Slate Markov Decision Processes
with High-Dimensional States and Actions. arXiv:1512.01124 [cs], December 2015. URL
http://arxiv.org/abs/1512.01124. arXiv: 1512.01124.
Richard S. SUtton and Andrew G. Barto. Reinforcement learning: an introduction. Adaptive com-
pUtation and machine learning. MIT Press, Cambridge, Mass, 1998. ISBN 978-0-262-19398-6.
Alex Wang and KyUnghyUn Cho. BERT has a MoUth, and It MUst Speak: BERT as a Markov
Random Field LangUage Model. arXiv:1902.04094 [cs], April 2019. URL http://arxiv.
org/abs/1902.04094. arXiv: 1902.04094.
Chao-YUan WU, Amr Ahmed, Alex BeUtel, Alexander J Smola, and How Jing. RecUrrent recom-
mender networks. In Proceedings of the tenth ACM international conference on web search and
data mining, pp. 495-503, 2017.
13
Under review as a conference paper at ICLR 2022
Sirui Yao, Yoni Halpern, Nithum Thain, Xuezhi Wang, Kang Lee, Flavien Prost, Ed H. Chi,
Jilin Chen, and Alex Beutel. Measuring Recommender System Effects with Simulated Users.
arXiv:2101.04526 [cs], January 2021. URL http://arxiv.org/abs/2101.04526.
arXiv: 2101.04526.
Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. Deep Learning based Recommender System: A
Survey and New Perspectives. ACM Computing Surveys, 52(1):1-38, February 2019. ISSN 0360-
0300, 1557-7341. doi: 10.1145/3285029. URL http://arxiv.org/abs/1707.07435.
arXiv: 1707.07435.
Yuyu Zhang, Hanjun Dai, Chang Xu, Jun Feng, Taifeng Wang, Jiang Bian, Bin Wang, and Tie-
Yan Liu. Sequential Click Prediction for Sponsored Search with Recurrent Neural Networks.
arXiv:1404.5772 [cs], July 2014. URL http://arxiv.org/abs/1404.5772. arXiv:
1404.5772.
Xiangyu Zhao, Long Xia, Lixin Zou, Dawei Yin, and Jiliang Tang. Toward Simulating Environments
in Reinforcement Learning Based Recommendations. arXiv:1906.11462 [cs], September 2019.
URL http://arxiv.org/abs/1906.11462. arXiv: 1906.11462.
14
Under review as a conference paper at ICLR 2022
A	Preference estimation
Future preference estimation with Algorithm 1 and why it approximates NHMM prediction.
Algorithm 1 is used to obtain the belief over the preferences of a user would have at timestep H -
assuming that the user interacted for T timesteps with a policy π (for which we have actual inter-
action data), and then interacted from timestep T to H with a policy π0 . We refer to this belief as
bπ-.T(UH)= P(UH |sn：T，xn：T).
The first model inference pass in Algorithm 1 resulting in b0:T (uT +1) will be a distribution over
UT +ι conditioned on s∏1, x∏1. A minimizer for this prediction problem would be for the network
to represent the actual belief that would be obtained by performing the prediction step in the NHMM
(described in Sec 4.1). This is because the expected cross-entropy between user choices and the
induced choice distribution (induced by the preference belief prediction) would be minimized.
One limitation is that there are possibly multiple beliefs over preferences that induce the same exact
distribution over choices: in that sense, the correct preference belief might be unidentifiable. Ex-
perimentally, we don’t find this to be a problem: the choice (see the ”Preference estimation model
form” heading of this Appendix) of having the representation of the preference belief be a mixture
of Von Mises distributions - which can be thought of as Normals over a circle - is already a good
enough inductive bias to be able to recover good preference beliefs (see Appendix D).
In the NHMM, note that the unrolling the forward model (i.e. computing P(UH |zT +J) could be
computed exactly, or be performed with Monte Carlo estimation, by sampling many internal states
zTπ+1 from the forward prediction distribution, and then sampling internal state evolutions according
to the dynamics. In our algorithm, by sampling choices and slates then conditioning on them, we are
approximating the Monte Carlo estimation approach. When simulating each user choice, the model
can be thought of as implicitly sampling a preference for this simulated user (from the preference
belief) and then sampling a choice from the corresponding choice distribution. This means that each
simulation rollout should be equivalent to ”sampling a user” (according to the distribution of users
in the data) and then sampling their choices.
Algorithm 1: Predicting future user preferences at timestep H under policy π0
Inputs: past interactions x%T, s∏T, a RS policy π0 with signature(S0：k, xo：k) → sk+ι, a future
preference estimator Pf with signature (so：k, x0：k) → (bo：k(uk+ι), bo：k(xk+ι)), a horizon H,
number of Monte Carlo simulations N ;
if X0：T = so：T = 0 then // if no past interaction data is given
Sample slate s0 ~ π0(0, 0) and imagine user choice x0 ~ P(x0∣0,0) ; //we now have past
interactions with T = 0
for i = 0; i < N; i + + do // sample N simulated trajectories
while k = T; k < H; k + + do
b0：k (uk+i),b0：k (xk+ι) = Pf(s0：k ,X0：k) ；	// estimate pref. and choices
Sk+1 〜n0(x0：k, S0：k) ;	// sample next timestep slate
Xk+1 〜 b0：k (xk+ι) ;	// simulate a user choice
Add xk+1 and sk+1 to the current simulated trajectory’s history;
Average b0:k-1(Uk) and b0:k-1(xk) across the N futures (for each k, with T < k ≤ H);
Result: Belief over future pref. b∏.T(Uk) and behaviors b∏.T(Xk) for each k s.t. T < k ≤ H.
Initial preference prediction model output correction
Below, we show that one can recover the smoothing estimate P(U0|x0:t, s0:t) from the predicted
preferences P(U0 |x1:t, s1:t) which will be a biased estimate of the initial preferences (as it does not
incorporate the information from timestep t = 0) . Note that:
P(U0|x0:t, s0:t)
P (U0∣X0,S0)P (xi：t,si：t|U0)
P(x0：t, s0：t)
P(u0∣X0,S0) P(U01Xl：t,Sl：t)P(xi：t,si：t)
P(X0：t, S0：t)	P(U0)
(1)
P(x0, s0|U0)P(U0|x1:t, s1:t)P(x1:t, s1:t)
----------------------------7---------Z P(X0, S0∣U0)P(U0∣XLt, si:t) (2)
P(x0:t, s0:t)P(x0, s0)
15
Under review as a conference paper at ICLR 2022
The first equality is given by the definition of smoothing applied to t = 0 (Russell & Norvig, 2002).
The second equality is obtained by using Bayes Rule on the backwards message P (x1:t, s1:t|u0),
and the third is obtained by using Bayes Rule on P(u0|x0, s0). Finally, we can ignore P (x1:t, s1:t),
P (x0:t, s0:t), and P(x0, s0) as they are constants.
Future preference estimation with Algorithm 2 and why it approximates NHMM prediction.
A similar argument to one in the above section can be made as to why such a initial preference
estimation network would approximate the corresponding NHMM smoothing task.
However, for the second step of counterfactual preference estimation, one issue arises. Relative to
having access to the full dynamics of the internal state, when performing approximate inference with
our model we lose some information: we are only able to recover a belief over the initial preferences,
whereas the NHMM with full dynamics access would be able to recover a belief over the full internal
state of the user. This will reduce the accuracy of our counterfactual estimation, but is the best we
can do without further assumptions.
Mathematically, We approximate the NHMM target distribution P(uT0 ∣x∏^, s∏^) as:
P(UT |xn：t, sn：t) ≈ P P(UT |u0)p(U0|xn：t, sn：t) = P(UT |bn：t(U0》
u0
= X	P(X0：T，S0：T)P (UT |bn：t (UO)，xn：T，sn：T)
xo：T-ι，sn：T-1
(3)
(4)
Where We the last expression is approximated With a Monte Carlo estimate detailed in Algorithm 21
(similarly to What Was done in Algorithm 1). To do Well at this second trajectory reconstruction task,
the netWork necessarily needs to learn hoW to make best use of the belief over initial preferences,
and implicitly learn their dynamics, as for the future preference estimation task.
Algorithm 2: Predicting counterfactual user preferences at timestep T under policy π0, given t
timesteps of interaction data With π .
Inputs: past interactions x0：t, s0：t, a RS policy π0 with signature (s0：k-i,X0：k-i) → Sk, an
initial preference estimator Pi with signature (s0：k-i, X0：k-i) → b0：k(U0), a conditioned
future estimator Pc with signature (s0：k-i, X0：k-i, b0：k(U0)) → (b0：k(〃k+i), b0：k(xk+ι)), a
horizon T , a constant N;
π	ππ
b0:t(U0)= Pi(s0:t, x&t) ;	// initial pref. belief given interactions with π
π
Pf = Pc(b = bn：t(〃0)) ;	// future pref predictor conditioned on init belief
(bn：k-i(Un0),bπ-.k-ι(xk ))0<k≤T =AlgOrithm 1(。,0,π0,Pf,t,n)
Result: Distributions of counterfactual preferences and behaviors under policy π0
Preference estimation model form
We have three models for preference estimation: respectively initial, counterfactual, and future pref-
erence estimators Pi, Pc, Pf. Any sequence model, such as RNNs or transformers, would be appro-
priate for these tasks that have variable number of inputs.
One detail of note that was omitted from Fig. 2 is that - to enable to represent multi-modal beliefs
over preference space - we let the models, output be parameters of multiple Von Mises distributions
and additionally some weights w. The w weighted average of these distributions will form the
predicted belief over Ut .
In architecture, we use a similar form to that of BERT4Rec (Sun et al., 2019) for ease of performing
inference on future or past preferences given contexts of interaction, as described in Fig. 8.
1 This algorithm is not actually used in it’s pure form in the experiments. Our choice of ”safe policy” πrnd
happens to choose constant slates (i.e. a uniform distribution no matter the history), so there is a shortcut to the
procedure: by simply setting the inputted slated for counterfactual prediction to be uniforms, and predicting
preferences without any user choices, one can train the model to directly output the belief over counterfactual
preferences for any timestep, across the whole userbase.
16
Under review as a conference paper at ICLR 2022
Using transformers is not new to the context of recommendation for click prediction (Sun et al.,
2019), but to our knowledge we are the first to leverage the flexibility of conditioning schemes
enabled by the BERT architecture (Devlin et al., 2019; Joshi et al., 2020; Wang & Cho, 2019).
Specifically, we train 3 transformers (one for prediction, one for smoothing, and one for counterfac-
tual estimation), although preliminary results show that one can train a single transformer to perform
all three tasks without a significant loss in performance. All together, these 3 transformers constitute
the “learned human model“ which we refer to throughout the text.
Preference Prediction
W
O
processed
predictions
predictions
.g
Item choices
Preferences
inputs
OO
OO
OOO
oo□
O O O
t]L'
H Slates
Masked input /
Unused output

O
O
t=0 t=l t=2	t=3	t=0 t=l t=2	t=3
Figure 8: BERT representation of the inference tasks. While our method is compatible with any
sequence model, we choose to use a BERT transformer models. Left: estimation of the user’s future
preferences and choice at t=2, given the interaction history history so far (this modality of prediction
is closest in setup to (Sun et al., 2019)). Middle: recovering a belief over the initial preferences
and choice of the user based on later interactions. Right: conditioning on the estimate of initial
preferences ⑴ recovered from the smoothing network one Can estimate CoUnterfaCtUal preferences
and choices under slates (*) (chosen from a policy π0 we are interested in) and imagined choices
(not shown dUe to space).
B Penalized RL
The underlying MDP
One coUld cast the recommendation problem as a POMDP (LU & Yang, 2016; Mladenov et al., 2019)
in which the state of the environment is hidden and contains the User’s internal state, which evolves
over time. EqUivalently, one can consider the belief-MDP indUced by the recommender POMDP
(Kaelbling et al., 1998), and approximate a solUtion to sUch belief-MDP via Deep-RL with a policy
trained with observation histories as inpUt (this is theoretically sUfficient for the policy to recover a
belief over the cUrrent hidden state and take the optimal action). The action space will be given by
the space of possible slates that the RS can choose. The reward signal will be the expected reward
for the current timestep E [rt (u∏)] (or with the extra terms for the proxies in the case of penalized
training). The introdUction of expectation can be thoUght of as expected SARSA (SUtton & Barto,
1998), as argUed in (Ie et al., 2019).
Penalized RL training
The fUll set of steps to rUn RL training are as follows: once the hUman models described in Sec. 4
are trained, one can Use them to simUlate User trajectories and compUte penalty metrics for sUch
trajectories (see Algorithm 3). One can then optimize the RL policy based on the on-policy simUlated
trajectory rolloUts.
Reduction of distances to final penalized objective
Note that the full penalized RL objective ( PT E 艮(u∏)]) - ν1 D(u∏.T, u0) - ν2D(Un：T, UnrT)
for our choice of distance function D reduces to PT E 仔 t (Un) + v； rt (u0 ,π) + V2 rt (u∏rnd, π)] for
some choice of ν10 , ν20 which can be treated as hyperparameters of how much we want to value the
engagement under each safe policy preferences relative to the engagement under the main policy.
C Experiment details
C.1 Ground truth users
Reduction of logit model to our case.
17
Under review as a conference paper at ICLR 2022
Algorithm 3: Generating a trajectory for RL training and computing metrics
Inputs: Initial, counterfactual, and future preference estimators Pi , Pc, Pf ; a policy π, a safe
policy πsafe, a horizon H, a constant N .
Sample slate s∏ 〜∏(0,0) and imagine user choice χ∏ 〜Pi(χ0∣0,0);
for t = 1; t ≤ H; t + + do
b∏1-i(u0) = Pi(s0∙t-ι,x0∙t-ι) ；	// current belief over initial preferences
π	π	ππ
b0.t-1 (ut),b0.t-1 (xt) = Pf (s∏子-ι ,x∏1-1) ;	// belief over pref and choices
E[bn：t-i(unsafe)] ∈ Algorithm 2(t,∏safe,Pc,Pi,x0∙t-i,s0∙t-i) ;	// belief over counterfactual
preferences under safe policy for this user
s∏ ~ π(x∏.t-ι, s&t-i) ;	// sample slate
x∏ 〜 b∏.t-ι(xt);	// imagine a user choice
Dt (utπ, ustafe) = Euπ,uπsafe,xπ (xtπ)Tutπ - (xtπ)T utπsafe ;	// compute penalty metric(s) for
timestep
rtRL = E[rt] + Dt (utπ, ustafe)
Return: User-RS interactions and training rewards for the simulated trajectory;
In our experimental setup, the traditional conditional logit model P(xt = x|st, ut)
eβc xT ut
Px∈s eβcxTut
doesn’t apply directly in this form, as we consider slates tobe distributions rather than sets of discrete
items. Intuitively, user’s choices should still depend on the slate: the proportion of a certain item in
the current slate (one can think of this as the proportion of a certain item type), should influence the
probability of the user of selecting that item (type). We operationalize this as P(Xt = x|st, Ut) 8
P(st = x)eβcxTut , with an additional term P(st = x) which takes into account the proportion
of each item (type). Note that this also mathematically corresponds to the generalization of the
traditional logit model: when the slate is discrete, P(st = x) will simply be an indicator for whether
the item is in the slate, leading to the traditional logit model form.
Feed belief update
Our ground truth user has a belief over future slates btH (s). Users’ initial belief matches the content
feature distribution itself bH(S) = D. After receiving a slate st, the user,s induces a belief bH(S) 8
st3 over the future slates in the user, i.e. the user will expect the next feeds to look like the most
common items in the current feed, as a result of availability bias (MacLeod & Campbell, 1992).
Lack of no-op choice.
While the assumption that user must pick item from the slate is unrealistic, this could be resolved
by adding an extra no-op choice to every slate (Sunehag et al., 2015).
Prefernce shift model.
Our preference shift model is inspired by Bernheim et al. (2021), but adapted to our experi-
mental setup as described below. The choice of preferences is modulated by a “mindset flex-
ibility“ parameter λ which captures how open they are to modifying their current preferences.
Users assign value to the choice of next-timestep preferences ut+1 as: V ut, btH (S), ut+1, λ =
Eχt+ι~bH(s)[λrt+ι(ut) + (1 — λ)rt+1(ut+1]), where with rt+ι(ut) indicates the engagement value
obtained by the user under the choice xt+1 and preference ut. Users pick their next timestep prefer-
ences also according to the conditional logit model, but over their expected value of such preferences
choices: P (ut+Jb? (s),ut,λ) a eβdV(ut,bH(S),ut+1,λ). Intuitively, users update their preference to
more “convenient" ones - ones that they expect will lead them to higher engagement value. The
main change we introduce from the original model is incorporating the belief over future feeds.
User’s preference-flexibility parameter is given by λ = 0.9, and their initial preferences are drawn
from a normal distribution2 around preference U = 130° and standard deviation 20°.
2Technically one should use Von Mises distributions - a distribution similar to normals, but for which the
domain is a circle. As Von Mises distributions are not implemented in numpy (Harris et al., 2020), for simplicity
We use clipped normal distributions (disregarding probability mass beyond 180° in either direction) in all places
except for the the transformer output (which was implemented in PyTorch (Paszke et al., 2019), which has a
Von Mises implementation)
18
Under review as a conference paper at ICLR 2022
C.2 Learned human models
For our learned human models, we use BERT transformers with 2 layers, 2 attention heads, 4 sets
of Von Mises distribution parameters, a learning rate of 0.00003, batch size of 500, and 100 epochs.
We train on the data described below.
C.3 Simulated dataset
See Fig. 9 for a summary of how content is generally instantiated in our setup.
We set the distribution of content in such a way that it forms a uniform distribution across features,
that is D = Uniform(0°, 360°). We simulate historical user interaction data with a mixed policy
π which is similar (but not equal to) a random policy in half the rollouts and for the other half is
goal-directed:
•	Half of the data is created with a RS policy which chooses an action uniformly among a set
of possible slates (distributions over the feature space with means 0° , 10° , ..., 340° , 350°,
and standard deviations equal to 30° or 60°). Each of these slate types can be thought ofas
a slate which contain mostly one specific type of content.
•	The other half of the data is created with a RS which chooses st = D = Uniform(0° , 360° )
80% of the time (i.e. the slate that would be chosen by the random RS πrnd), and chooses a
random action from the same set of possible slates the remaining 20% of the time.
This is to simulate the setting in which the safe policy we are interested in (the random NPS policy)
is similar to previously deployed ones that are represented in the data (although not the same, so the
network still has to generalize across RS policies at test time).
Our dataset consists of 100k user trajectories (slates and observations), with length of each trajectory
T = 10.
Figure 9: a) → b) The content can be mapped to an empirical distribution over feature space D in
Rd . We consider dimension d = 2 for ease of visualization. Restricting preferences and choices
to be unit vectors, one can think of them as a points on a circle: the engagement value rt will
thus be related to the angle θ between ut and xt . c) We discretize this circular preference and
feature space into n = 36 bins (i.e. binning the angles) which enables to visualize distributions over
preferences and over content features as histograms over angles. d) We model slates st as categorical
distributions over the discretized n-bin feature space.
C.4 RS training
For RL optimization, we use PPO (Schulman et al., 2017) trained with Rllib (Liang et al., 2018).
The action space of the recommender system is given by distributions over feature space with means
0° , 60° , ..., 260° , 320°, and standard deviations equal to 60° . As observations to the system, we
provide the current slate, the previous user choice, and the current estimates from the HMM for
smoothing, filtering, and natural preference shift counterfactual distributions, in order to increase
training speed (note that these will not change the optimal policy). All policies are recurrent so they
are able to reason about the history of interactions so far.
We use batch size 1200, minibatch size 600, 4 parallel workers, 0.005 learning rate, 50 gradient up-
dates per minibatch per iteration, policy function clipping parameter of 0.5, value function clipping
parameter of 50 and loss coefficient of 8, with an LSTM network with 64 cell size. Training runs in
less than 30 minutes for each condition on a MacBook Pro 16” (2020).
D Results
Quality of preference estimation
19
Under review as a conference paper at ICLR 2022
We baseline the quality of our counterfactual estimates of user preferences and behaviors relative to
the performance of NHMM with full access to the human dynamics - as that is the best performance
we could possibly hope for. The metrics we use to evaluate the performance of the transformer
relative to the HMM are the 1) relative prediction loss (RPL) as the ratio between the transformer
loss and the HMM loss (LLTrM), and the 2) relative prediction accuracy (RPA), defined similarly as
AcccTrM. Given that NHMM is the gold standard of estimation (as We are doing exact inference),
RPL should be larger than 1 and RPA should be lower than 1. The closer to 1 the numbers are, the
better performing our approximate inference methods are. For all the models, we train on a 75k user
interaction trajectories dataset obtained from the distribution described in C. For all the tasks below,
we use the methodology developed in Sec. 4.1.
Future preference estimation We train a transformer model as in the left portion of Fig. 8, by
predicting - for any given timestep t - the preferences ut (and thus also a distribution over item
choices) based on slates s0, . . . , st-1 and the previous observations x0, . . . , xt-1. All other inputs
are masked. We evaluate on a held out dataset of 25k user interaction trajectories, which are obtained
from the same distribution as the training data. The preference RPL and RPA (averaged across all
timesteps) are 1.08 and 0.88.
Initial preference estimation We train a transformer model as in the middle portion of Fig. 8,
by predicting the preferences u0 (and thus also a distribution over item choices) based on slates
s1, . . . , sT and the observations x1, . . . , xT. All other inputs are masked. We evaluate on the same
held out dataset as the prediction case. The preference RPL for the initial timestep is 1.08, and the
preference RPA is 0.81.
Counterfactual preference estimation We train a transformer model as in the right portion of
Fig. 8, by predicting - for any given timestep t - the preferences ut (and thus also a distribution
over item choices) based on slates s0, . . . , sT and the observations x1, . . . , xT - i.e. reconstructing
the original trajectory from the smoothing estimate obtained from the network described above. We
evaluate on a held out dataset of 25k user interaction trajectories obtained from the natural preference
shift policy πrnd - showcasing the generalization performance of our method to different policies.
The preference RPL (averaged across timesteps) is 1.16, and the preference RPA is 0.63 - showing
that although the task is more challenging for the models, performance doesn’t degrade drastically
relative to the NHMM. Fig. 7 shows this qualitatively: the counterfactual network is able to estimate
what natural preference shifts would look like relatively well relative to the ground truth across the
population.
20