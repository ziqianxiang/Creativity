Under review as a conference paper at ICLR 2022
Apollo: An Adaptive Parameter-wise Diag-
onal Quasi-Newton Method for Nonconvex
Stochastic Optimization
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we introduce Apollo, a quasi-Newton method for nonconvex
stochastic optimization, which dynamically incorporates the curvature of the loss
function by approximating the Hessian via a diagonal matrix. Importantly, the up-
date and storage of the diagonal approximation of Hessian is as efficient as adaptive
first-order optimization methods with linear complexity for both time and memory.
To handle nonconvexity, we replace the Hessian with its rectified absolute value,
which is guaranteed to be positive-definite. Experiments on three tasks of vision
and language show that Apollo achieves significant improvements over other
stochastic optimization methods, including SGD and variants of Adam, in terms of
both convergence speed and generalization performance. The implementation of
the algorithm is available at anonymous link.
1	Introduction
Nonconvex stochastic optimization is of core practical importance in many fields of machine learning,
in particular for training deep neural networks (DNNs). First-order gradient-based optimization
algorithms, conceptually attractive due to their linear efficiency on both the time and memory
complexity, have led to tremendous progress and impressive successes. A number of advanced
first-order algorithms have emerged over the years to pursue fast and stable convergence, among
which stochastic gradient descent (SGD) (Robbins & Monro, 1951; LeCun et al., 1998), equipped
with momentum (Rumelhart et al., 1985; Qian, 1999; Bottou & Bousquet, 2008), has stood out
for its simplicity and effectiveness across a wide range of applications (Hinton & Salakhutdinov,
2006; Hinton et al., 2012; Graves, 2013). However, one disadvantage of SGD is that the gradients
in different directions are scaled uniformly, resulting in limited convergence speed and sensitive
choice of learning rate, and thus has spawned a lot of recent interests in accelerating SGD from the
algorithmic and practical perspectives.
Recently, many adaptive first-order optimization methods have been proposed to achieve rapid
training progress with element-wise scaled learning rates, and we can only mention a few here due
to space limits. In their pioneering work, Duchi et al. (2011) proposed AdaGrad, which scales the
gradient by the square root of the accumulative square gradients from the first iteration. While
AdaGrad works well for sparse settings, its performance significantly degrades for dense settings,
primarily due to the monotonic increase of the accumulation. Subsequently, several methods have
been proposed with the intuition to limit the accumulation to a small window of past iterations, and
in particular exponentially reduce the weight of earlier iterations. Notable works incorporating this
method are RMSProp (Tieleman & Hinton, 2012), AdaDelta (Zeiler, 2012), and Adam (Kingma &
Ba, 2015), among which Adam has become the default optimization algorithm across many deep
learning applications because of its fast convergence speed and relatively consistent selections of
hyper-parameters (Ruder, 2016; Zhang et al., 2020). However, it has been observed that these adaptive
optimization methods may converge to bad/suspicious local optima, resulting in worse generalization
ability than their non-adaptive counterparts (Wilson et al., 2017), or fail to converge due to unstable
and extreme learning rates (Luo et al., 2019).
Quasi-Newton methods have been widely used in solving convex optimization problems, due to their
efficient computation and fast convergence rate (Broyden, 1967; Dennis & Mora 1977). However,
the stochastic, high-dimensional and nonconvex nature of many machine learning tasks, such as
1
Under review as a conference paper at ICLR 2022
training deep neural networks, has rendered many classical quasi-Newton methods ineffective and/or
inefficient (Keskar & Berahas, 2016; Wang et al., 2017; Yao et al., 2020). Indeed, in many natural
language processing (NLP) and computer vision (CV) tasks (He et al., 2016; Ma & Hovy, 2016;
Luo et al., 2019), SGD (with momentum) is chosen as the optimizer, benefiting from its stable and
efficient training and outstanding generalization.
In this work, we develop Apollo, a quasi-Newton method for nonconvex stochastic optimization
to simultaneously tackle the aforementioned challenges of stochastic variance, nonconvexity and
inefficiency. Algorithmically, Apollo dynamically incorporates the curvature of the objective
function with diagonally approximated Hessian. It only requires first-order gradients and updates
the approximation of the Hessian diagonally so that it satisfies a parameter-wise version of the
weak secant condition (Wolfe, 1959). To handle nonconvexity, we replace the Hessian with its
rectified absolute value, the computation of which is also efficient under our diagonal approximation,
yielding an efficient optimization algorithm with linear complexity for both time and memory (§3).
Experimentally, through three tasks on CV and NLP with popular deep neural networks, including
ResNets (He et al., 2016), LSTMs (Hochreiter & Schmidhuber, 1997) and Transformers (Vaswani
et al., 2017), we demonstrate that Apollo significantly outperforms SGD and variants of Adam, in
terms of both convergence speed and generalization performance (§4).
2	Backgrounds
In this section, we set up the notations on nonconvex stochastic optimization, briefly review the
(quasi-) Newton methods, and discuss the problems of applying quasi-Newton methods to nonconvex
stochastic optimization that we attempt to study in the rest of the paper.
2.1	Nonconvex Stochastic Optimization
In this paper, we consider the following nonconvex stochastic optimization problem:
min f(θ) = E[l(θ; Γ)]	(1)
where l : Rd × Rn → R is a continuously differentiable (and possible nonconvex) function, θ ∈ Rd
denotes the parameter to be optimized, Γ ∈ Rn denotes a random variable with distribution function
P, and E[∙] denotes the expectation w.r.t Γ. Intuitively, Γ incorporates noises in f, leading to a
stochastic objective function. A special case of (1) that arises frequently in machine learning is the
empirical risk minimization problem:
1N
mRndf ⑻=NN X li(θ)	⑵
where li : Rd → R is the loss function corresponding to the i-th data, and N is the number of data
samples that is assumed to be extremely large. Objective functions may also have other sources of
noise than data subsampling, such as dropout (Srivastava et al., 2014) in deep neural networks.
Decoupled Parameters. In this work, we consider a setting of decoupled parameters: θ =
{θ(l), l = 1, . . . , L}. Intuitively, under this setting the parameter θ is decoupled into a sequence of
parameters serving different functionalities. For example, in neural network training the parameters
of a neural network can be naturally decoupled into the parameters of different layers or modules.
2.2	Newton and quasi-Newton Methods
Newton’s method usually employs the following updates to solve (1):
θt+1 = θt - Ht-1gt	(3)
where gt = Vf (θt) is the gradient at θt and Ht = V2f (θt) is the Hessian matrix. The convergence
rate of Newton’s method is quadratic under standard assumptions (Nocedal & Wright, 2006). How-
ever, major challenges with this method are i) the expensive computation of the inverse Hessian at
every iteration and the corresponding quadratic memory complexity; and ii) the limitation to convex
functions (nonconvexity results in negative curvature of Ht and misleads the update directions).
A standard alternative to Newton’s method is a class of quasi-Newton methods, which have been
widely used in solving convex deterministic optimization problem:
θt+1 = θt - ηtBt-1gt	(4)
2
Under review as a conference paper at ICLR 2022
where η is the stepsize (a.k.a learning rate), Bt is an approximation to the Hessian matrix V2f (θt)
at θt , which is updated based on the well-known secant equation:
Bt+1 = argmin kB - Btk
B	(5)
s.t. Bt+1st = yt (secant equation)
where st = θt+1 -θt and yt = gt+1 -gt. Bt+1 is, in the sense of some matrix norm, the closest to Bt
among all symmetric matrices that satisfy the secant equation. Each choice of the matrix norm results
in a different update formula, such as DFP (Davidon, 1991; Fletcher, 1987) and BFGS (Broyden,
1970; Fletcher, 1970; Goldfarb, 1970; Shanno, 1970). The popularity of this method is due to the
fact that only the gradient of the objective function is required at each iteration. Since no second
derivatives (Hessian) are required, quasi-Newton methods are sometimes more efficient than Newton’s
method, especially when the computation of Hessian is expensive. To further reduce memory cost,
one seminal work is the limited memory BFGS (L-BFGS) (Liu & Nocedal, 1989; Byrd et al., 1995)
that achieves desirable linear computational and memory complexity by approximating the Hessian
as a series of sum of first order information from previous iterations.
2.3 Problems of quasi-Newton Methods
Despite their impressive successes on convex deterministic optimization, quasi-Newton methods
suffer from their own problems in more challenging scenarios. In this section, we mainly discuss
three problems preventing quasi-Newton methods from being applied to the scenario of large-
scale nonconvex stochastic optimization. Due to these problems, no quasi-Newton methods (to
our best knowledge) designed for nonconvex optimization consistently outperform adaptive first-
order algorithms w.r.t convergence speed and generalization performance. The main goal of this
work is to algorithmically design and experimentally demonstrate a novel quasi-Newton method, in
hope of improving the convergence speed and generalization performance of nonconvex stochastic
optimization eventually.
Stochastic Variance. One challenge of quasi-Newton methods on nonconvex stochastic optimiza-
tion (1) is the variance introduced by the stochastic nature of the problem. At each iteration, only the
stochastic gradient gt is available, which is an unbiased estimation of the gradient Vf(θt) and may
lead to an erroneous approximation of Hessian (Byrd et al., 2011).
Nonconvexity. Another key challenge in designing such quasi-Newton methods lies in the difficulty
of preserving the positive-definiteness of Bt in (5), due to the nonconvexity of the objective function.
What is worse is that performing line search is infeasible in the stochastic setting, due to the presence
of noise in the stochastic gradients (Wang et al., 2017).
Computational and Memory Efficiency. Even though quasi-Newton methods are more efficient
than Newton’s method, the time and memory complexities are still relatively large compared with
adaptive first-order methods. For instance, L-BFGS requires to store first-order information from m
previous iterations with commonly m ≥ 5, which is still too expensive for deep neural networks con-
taining millions of parameters. Moreover, adapting quasi-Newton methods to nonconvex stochastic
optimization probably introduces additional computation, further slowing down these methods.
3 Adaptive Parameter-Wise Diagonal Quasi-Newton
With the end goal of designing an efficient quasi-Newton method to solve the problem in (1) in
mind, we first propose to approximate the Hessian with a diagonal matrix, whose elements are
determined by the variational approach subject to the parameter-wise weak secant equation (§3.1).
Then, we explain our stepsize bias correction technique to reduce the stochastic variance in §3.2. To
handle nonconvexity, we directly use the rectified absolute value of the diagonally approximated
Hessian as the preconditioning of the gradient (§3.3). The initialization technique of Apollo
allows us to eliminate one hyper-parameter (§3.4). At last, we provide a theoretical analysis of
Apollo’s convergence in both convex optimization and nonconvex stochastic optimization (§3.5).
The pseudo-code is shown in Algorithm 1.
3.1	Quasi-Newton Methods with Diagonal Hessian Approximation
As discussed in Bordes et al. (2009), designing an efficient stochastic quasi-Newton algorithm
involves a careful trade-off between the sparsity of the approximation matrix Bt and the quality of
3
Under review as a conference paper at ICLR 2022
its approximation of the Hessian Ht , and diagonal approximation is a reasonable choice (Becker
et al., 1988; Zhu et al., 1999). If B is chosen to be a diagonal matrix satisfying (5), one can obtain a
formula similar to the SGD-QN algorithm (Bordes et al., 2009).
An alternative of the secant equation in the updating formula (5), as first introduced by Nazareth
(1995), is the weak secant equation (Dennis & Wolkowicz, 1993):
Bt+1 = argmin kB - Btk
B
s.t. stTBt+1st = stT yt (weak secant equation)
(6)
The motivation of using the weak secant condition in diagonal quasi-Newton method is straight-
forward: the standard mean-value theorem might not necessarily hold for vector-valued functions
expressed in the secant equation, Bt+ιst = yt ≈ V2f (θt)st. Thus, We do not know whether there
exists a vector θ ∈ Rd such that yt = V2f (θ)st (Dennis & Mora 1977). On the other hand, the
Taylor theorem ensures that there exists such θ that stT yt = stT V2f(θ)st, leading to the reasonable
assumption of the weak secant condition (6).
Based on the variational technique (Zhu et al., 1999), the solution of (6) with Frobenius norm is:
Λ，Bt+ι - Bt = STyt-STBtst Diag(S2)	(7)
kstk4
where St2 is the element-wise square vector of St, Diag(St2) is the diagonal matrix with diagonal
elements from vector s2, and k∙∣∣4 is the 4-norm of a vector.
Parameter-Wise Weak Secant Condition. However, in optimization problems with high-
dimensional parameter space, such as training deep neural networks with millions of parameters, the
weak secant condition might be too flexible to produce a good Hessian approximation. In the setting
of decoupled parameters (§2.1), we propose a parameter-wise version of the weak secant equation
to achieve a trade-off between the secant and weak secant conditions: for each parameter θ(l) ∈ θ,
we update B corresponding to θ(l) by solving (6) individually. Remarkably, the secant condition
restricts B with an equation of a d-dimensional vector, while the weak secant condition relaxes it
with a 1-dimensional scalar. The parameter-wise weak secant condition expresses the restriction as a
l-dimension vector (1 < l < d), resulting in a reasonable trade-off. The updating formula is the same
as (7) for each parameter-wise B .
3.2	Stepsize Bias Correction
To mitigate the stochastic variance problem in stochastic quasi-Newton methods, Apollo utilizes
stepsize bias correction on the stochastic gradients at each step t. We know that the optimal step-
size ηt equals to 1 w.r.t the quadratic approximation underlying Newton’s method, if the Hessian
approximation Bt and the stochastic gradient gt are close to the exact Hessian Ht and gradient
Vf(θt), respectively. Inspired by this, we correct the stepsize bias in the stochastic gradient gt
by replacing it with a corrected gradient gt0 = ηtgt . Together with the corresponding corrected
yt0 = gt0+1 - gt0 = ηtyt, we correct the updating term Λ of Bt in (7) by replacing yt with yt0:
Λ= STy0- STBtSt Diag(S2) = -dyt + £Btdt Diag(d2)	(8)
kStk44	t	kdtk44	t
where dt = -St∕ηt = B-Igt is the corrected update direction. Note that after applying the step
bias correction, the update formula of Bt in (8) is independent with the stepsize ηt, eliminating the
stepsize bias. Technically, the stepsize bias correction is designed to reduce the stochastic variance,
rather than entirely discarding the stepsize. The Apollo algorithm (Algorithm 1) still incorporates
the stepsize at every iteration to enforce convergence.
Based on previous studies, incorporating exponential moving averages (EMVs) for the stochastic
gradients significantly reduces the variance (Kingma & Ba, 2015). We follow these works and apply
EMV to gt, together with the initialization bias correction:
β(I- βt)	1 - β	zɑʌ
mt+1 = 1 - βt+1 mt + 1 - βt+1 gt+1	(9)
where 0 < β < 1 is the decay rate of EMV and yt in (8) is written as mt+1 - mt . Note that we
do not apply moving average methods to the approximated Hessian, though the diagonal matrix is
easier to be explicitly formed to average than full matrices. Investigating the moving average of the
diagonal Bt might be an interesting direction of future work.
4
Under review as a conference paper at ICLR 2022
Algorithm 1: APOLLO, our proposed algorithm for nonconvex stochastic optimization. All
operations on vectors are element-wise. Good default settings are β = 0.9 and = 1e-4.
Initial: m0, d°, B0 J 0,0,0
while t ∈ {0, . . . , T } do
for θ ∈ {θ1, . . . , θL} do
gt+ι J Vft(θt)
β(1-βt)	1-β
mt+1 j----ι-βt+ι mt + ι-β⅛+ι gt+1
average
dT (mt+ι-mt) + dT Btdt
α <	(kdtk4+e)4
Bt+1 J Bt- α ∙ Diag(d2)
Dt+1 J rectify(Bt+1,0.01)
dt+1 J Dt-+11 mt+1
θt+1 J θt - ηt+1 dt+1
end
end
// Initialize m0, d0, B0 to zero
// Calculate gradient at step t
// Update bias-corrected moving
// Calculate coefficient of B update
// Update diagonal Hessian
// Handle nonconvexity
// Calculate update direction
// Update parameters
3.3	Rectified Absolute Value of Hessian for Nonconvexity
To guarantee convergence, quasi-Newton methods require the approximated Hessian matrix Bt to
be positive definite at each step. The common strategy in previous studies is to solve the updating
formula in (5) by restricting the candidate matrix B to be symmetric positive definite. It is known
that the BFGS update preserves the positive-definiteness of Bt+1 as long as the curvature condition
stT yt > 0 holds, which can be guaranteed for strongly convex problem. For nonconvex problem, the
curvature condition can be satisfied by performing a line search, which is, however, expensive or
even infeasible in stochastic setting, because the exact function values and gradient information are
unavailable. Wang et al. (2017) proposed the stochastic damped L-BFGS (SdLBFGS) method that
implicitly generates a positive definite matrix without line search. However, it usually requires large
history size (m ≥ 100) to guarantee convergence, which is infeasible for large-scale optimization.
To handle nonconvexity, we adopt a different strategy that does not require the solution of Bt in (5)
to be positive definite. Intuitively, we search for Bt that is a good approximation of the real Hessian,
which is not necessarily positive definite in nonconvex problem. When We use Bt as preconditioning
to calculate the update direction, we use its absolute value: |Bt | = BTBBt, where √ is the positive
definite square root of a matrix. The motivation of absolute value is straight-forward: for dimensions
with large absolute values of curvature, the objective function could be very sharp and we would
prefer to take relatively smaller steps than those flatter dimensions. Since APOLLO formulate Bt as a
diagonal matrix, the cost of computing |Bt | is marginal.
Rectified Absolute Value of Bt For nonconvex objective functions, there exist inflection points
whose curvatures are zero. To prevent the steps from becoming arbitrarily large, we rectify the
absolute value of Bt with a convexity hyper-parameter σ:
Dt = rectify(Bt, σ) = max(|Bt |, σ)	(10)
where the rectify(∙, σ) function is similar to the rectified linear unit (ReLU) (Nair & Hinton, 2010)
with threshold set to σ. The update direction in (8) is then dt = Dt-1mt.
AdaHessian (Yao et al., 2020) used an idea similar to the absolute values of Bt to handle nonconvexity,
where the root mean square averaging is applied to compute the Hessian diagonal. Different from
Apollo, AdaHessian requires second-order information to compute the Hessian matvec oracle and
approximate the Hessian diagonal using Hutchinson’s method, which is significantly more costly.
3.4	Initialization
The rectified Dt in (10) introduces one more hyper-parameter σ, limiting the application of APOLLO
in practice. In this section, we show that the zero initialization approach in Apollo, which initializes
the moving average of gradient m0, the parameter update direction d0 and the diagonal approximation
of Hessian B0 as (vector of) zeros, leads to coupled stepsize η and convexity σ, allowing us to
eliminate one hyper-parameter of η or σ.
5
Under review as a conference paper at ICLR 2022
Coupled Stepsize η and Convexity σ.	With the zero initialization ofm0, d0 and B0, the following
theorem illustrates the relation between η and σ (details in Appendix A):
Theorem 1.	Given zero initialization of m0 , d0 , and B0 and a fixed parameter intialization θ0.
Suppose that we have two sets ofhyper-parameters η, σ and η0, σ0 with the same ratio: * = %. Then
the convergence trajectories of these two sets of hyper-parameters are exactly the same:
θt = θt0, ∀t ∈ {1, . . . , T}.	(11)
where θt and θt0 are the parameters of (η, σ) and (η0, σ0) at iteration t, respectively.
From Theorem 1, we observe that η and σ are coupled with each other and in practice we only need
to tune one of them, leaving the other fixed. Therefore, in our experiments (§4), we fix σ = 0.01 and
tune η on different problems1.
Learning Rate Warmup for Apollo As discussed in Kingma & Ba (2015), zero initialization
leads to estimations biased towards zero in the initial iterations. For the moving average mt , this
bias can be corrected by dividing the bias-correction term (9). For dt and Bt, however, we cannot
derive such bias correction terms. Fortunately, a simple linear warmup heuristic of η at the beginning
iterations achieves remarkably stable training.
3.5 Convergence Analysis
Similar to previous work (Reddi et al., 2018; Chen et al., 2019; Zhuang et al., 2020), we omit the
initialization bias correction step, i.e. we use mt = βtmt-1 + (1 - βt)gt, 0 < βt < 1, ∀t ∈ [T].
We first analyze the convergence of Apollo in convex optimization using the online learning
framework (Zinkevich, 2003) for a sequence of convex cost functions f1 (θ), f2(θ), . . . , fT (θ).
Theorem 2.	(Convergence in convex optimization) Let {θt } be the sequence from APOLLO. Suppose
ηt = √ηt, 0 < βt ≤ β ≤ 1 kgt k2 ≤ G, k⅛t≠ ≤ kDtk1, kθt - θt0 k2 ≤ D, ∀t,t0 ∈ [T]. For θt
generated with the APOLLO algorithm, we have the following bound on the regret:
RT ≤
√TD2kDτkι + Q(2√T - 1)+	D2	XF β2
2η(i-β) + 1 -β(V )+2(i-β) t=ι ηt
The following result falls as an immediate corollary of the above result.
Corollary 2.1. Suppose βt = βλt-1, 0 < λ < 1 in Theorem 2, we have
RT ≤
√TD2kDτ kι + K (2√T-1)+ D2β2
2η(i - β)	1 - β'	2η(i - β)(i - λ2)2
(12)
(13)
Theorem 2 implies the regret of APOLLO is upper bounded by O(√T). The conditions for Corol-
lary 2.1, as in Reddi et al. (2018), can be relaxed to βt = β∕t and still ensures a regret of O(√T).
For nonconvex case, we analyze the convergence rate of Apollo with the similar derivations of that
in Chen et al. (2019), since APOLLO belongs to the family of generalized Adam-type methods:
Theorem 3. (Convergence in nonconvex stochastic optimization) Under the assumptions:
•	f is Iowerboundedanddifferentiable; kVf (θ) - Vf (θ0)k2 ≤ L∣∣θ — θ0k2, kDtk∞ <L,∀t, θ, θ0.
•	Both the true and stochastic gradient are bounded, i.e. kVf (θt)k2 ≤ H, kgtk2 ≤ H, ∀t.
•	Unbiased and independent noise in gt, i.e. gt = Vf(θt) + ζt, E[ζt] = 0, and ζi ⊥ ζj, ∀i 6= j.
Assume η = √, βt ≤ β ≤ 1 in non-increasing, Dn-1, ≤ Dtj, ∀t ∈ [T],j ∈ [d], then:
min E [∣∣Vf(θt)∣∣2] ≤ √L= (Cιη2H2(1 + logT) + C2dη + C3dη2 + C4)	(14)
where C1 , C2 , C3 are constants independent of d and T , C4 is a constant independent of T, the
expectation is taken w.r.t all the randomness corresponding to {gt}. Theorem 3 implies the conver-
gence rate for APOLLO in the non-convex case is Ο(log T∕√T), which is similar to Adam-type
optimizer (Reddi et al., 2018; Chen et al., 2019). In addition, unlike Theorem 3.1 in Chen et al.
(2019), Theorem 3 does not specify the bound of each update kηtmt∕Dt k2 . This is because that,
with conditions ηt ≤ η, kgtk2 ≤ H and Dt ≥ 1, it is straight-forward to derive the bound of
∣∣ηtmt∕Dtk2 ≤ ηH = g.
1We changed σ from 1 to 0.01 to make η in a suitable range. See Appendix E.4 for details.
6
Under review as a conference paper at ICLR 2022
Figure 1: Training loss and test accuracy of ResNet-110 on CIFAR-10 and ResNeXt-50 on ImageNet,
with two schedule strategies of learning rate decay.
4 Experiments
To evaluate Apollo, we conduct experiments on four benchmark datasets across three tasks of CV
and NLP that are commonly used to evaluate optimization algorithms: CIFAR-10 (Krizhevsky &
Hinton, 2009) and ImageNet (Deng et al., 2009) for image classification; One Billion Words (Chelba
et al., 2013) for language modeling; and WMT 2014 English-German for neural machine translation.
The five baseline methods we compare with are SGD with momentum (Bottou & Bousquet, 2008),
Adam (Kingma & Ba, 2015), Rectified Adam (RAdam) (Liu et al., 2020), AdaBelief (Zhuang et al.,
2020), and AdaHessian (Yao et al., 2020). Following Loshchilov & Hutter (2019), we decouple
weight decays in Adam, RAdam, AdaBelief and AdaHessian in all the experiments2. For each
experiment, we report the average over 5 runs. More detailed descriptions, results and analysis of the
conducted experiments are provided in Appendix E.
4.1 Image Classification
We begin our experiments with an evaluation of
the convergence and generalization performance
on image classification. We use ResNet-1103 for
CIFAR-10 and standard ResNeXt-50 (Xie et al.,
2017) for ImageNet, respectively. The results
on CIFAR-10 and ImageNet are presented in
Figure 1 and Table 1, together with the five base-
lines. For each optimizer, we use two schedul-
ing strategies of learning rate decay: i) mile-
stone that decays the learning rate at the end of
some predefined epochs; and ii) cosine anneal-
ing schedule proposed in Loshchilov & Hutter
Table 1: Test Acc. on CIFAR-10 and ImageNet.
Method	CIFAR-10		ImageNet	
	milestone	cosine	milestone	cosine
SGD	I	93.94	94.53	I^^77.57	78.26
Adam	93.74	94.24	76.86^^	77.54
RAdam	93.88	94.38	76.91	77.68
AdaBelief	94.03	94.51	77.55	78.22
AdaHeSSian ∣	93.97	94.48	I^^77.61	78.02
APOLLO I	94.21	94.64	I^^77.85^^	78.45
(2017). All the optimization methods are comprehensively tuned, especially for the learning rate and
the rate of weight decay. It is because that the strength of weight decay regularization is co-related
with the learning rate, even though the decoupled weight decay technique (Loshchilov & Hutter, 2019)
has been applied. The tuning information and the model details are provided in the Appendix E.1.
From Figure 1 and Table 1, we see that Apollo outperforms the four first-order methods (SGD,
Adam, RAdam and AdaBelief) on both the convergence speed and classification accuracy, demon-
strating its effectiveness on training the ResNet architectures based on convolutional neural networks
(CNNs) (LeCun et al., 1989). Comparing with AdaHessian, Apollo obtains better test accuracy
with similar convergence speed. Note that AdaHessian requires second-order information and is
significantly more costly (detailed comparison of time and memory costs in Appendix F.3). Thus, we
omit AdaHessian from the following experiments in the rest of this paper.
2For AdaBelief, we also tried standard L2 regularization. But the accuracies are consistently worse than the
models with decoupled weight decay.
3 ResNet-110 is a modified (small) version of ResNet-18 to adapt the image size 32 × 32 in CIFAR-10.
7
Under review as a conference paper at ICLR 2022
Figure 2: SGD, Adam, RAdam and Apollo with different learning rates on CIFAR-10.
Figure 3: Language modeling (LSTMs) on One Billion Words.
Method	I PPL
SGD	I 32.65
Adam	36.68
RAdam	36.20
AdaBelief	32.83
APOLLO I 31.94
Table 2: Test PPL.
Robustness to Learning Rate Change. Besides performance improvements, we also investigate
the robustness of different optimization methods to the change of learning rate. For each optimizer,
we use the learning rate in the previous experiment (Table 1) as the base, i.e. 0.1 for SGD, 0.001
for Adam and RAdam, and 0.01 for APOLLO. Then, we explore different learning rates that are α
times of the base learning rate, with α ∈ {0.2, 1.0, 2.0, 10.0}. As mentioned above, we observed
that the strength of weight decay regularization is co-related with the learning rate, even for Adam
and RAdam with decoupled weight decay (Loshchilov & Hutter, 2019). To eliminate the impact of
weight decay, we adjust the weight decay rates according to the factor α. Experimental results with
ResNet-110 on CIFAR-10 are summarized in Figure 2. After correcting the impact of weight decay,
all the optimization methods, except SGD with α = 10.0, achieve consistent model performance,
while Apollo slightly improves the robustness of model training over the three baseline methods.
4.2	Language Modeling
To evaluate Apollo on Recurrent Neural Networks (RNNs) that are applied in a wide range of NLP
tasks (Graves, 2013), we conduct experiments on the One Billion Words dataset (Chelba et al., 2013),
using a two-layer LSTM network for language modeling (details in Appendix E.2).
Figure 3 and Table 2 illustrate the perplexity (PPL) of training and test for Apollo and four baseline
methods, including SGD, Adam, RAdam and AdaBelief. As shown in Figure 3, although Apollo is
slower than Adam-type methods in the first few updates, its convergence is much faster after that.
On generalization performance, Apollo achieves significant improvements (more than 4.0 PPL
points on test data) over Adam and RAdam. In addition, Apollo also outperforms AdaBelief, which
obtains the lowest PPL among the three Adam-type optimization methods4. This demonstrates the
effectiveness of Apollo on training LSTM-based neural architectures.
Training Stability. From the middle plot of Figure 3 we see that the training losses of Adam and
RAdam may suddenly increase. This occurred in all the runs of experiments using Adam and RAdam,
and we selected these successfully converged — the loss went back to normal after some updates,
and discarded those that failed to converge — the model crashed due to loss numerical overflow. The
models optimized With APOLLO never suffered from this issue, demonstrating its stability.
4We found that AdaBelief is very sensitive to the value of . The result in Table 2 is obtained using = 1e-12.
With other values, e.g. 1e-8 or 1e-16, the PPL points of AdaBelief are even higher than Adam and RAdam.
See Appendix E.2 for the details of hyper-parameter tuning.
8
Under review as a conference paper at ICLR 2022
4.3	Neural Machine Translation
Table 3: Test BLEU.
To evaluate Apollo on Attention-based Transformer architec-
ture (Vaswani et al., 2017), we train the Transformer-base model
on the WMT2014 English-German (EN-DE) dataset (around 4.5M
sentence pairs). We use the same data preprocessing steps as in Ma
et al. (2019) (details in Appendix E.3). We compare Apollo with the
same four baseline methods in the experiments of language modeling.
For each experiment, we report the mean and standard variance over
5 runs. From Table 3, the first interesting observation is that SGD
performs much worse than Adam-type methods (which is opposite to
Method	BLEU
SGD	26.59±0.07
Adam	27.84±0.12
RAdam	28.15±0.15
AdaBelief	28.14±0.11
Apollo	28∙34±0.10
its behaviour for ResNet- and LSTM-based neural architectures). Similar observations about SGD
were reported in (Yao et al., 2020; Zhang et al., 2020). Despite this, Apollo obtains improvements
over all the baseline methods for NMT with transformers.
5	Related Work
Stochastic Quasi-Newton Methods. In the literature of (nonconvex) stochastic quasi-Newton
methods, several algorithms have been developed recently for large-scale machine learning problems:
oLBFGS (Schraudolph et al., 2007; Mokhtari & Ribeiro, 2015), RES (Mokhtari & Ribeiro, 2014),
SFO (Sohl-Dickstein et al., 2014), SQN (Byrd et al., 2016), SdLBFGS (Wang et al., 2017), and
AdaQN (Keskar & Berahas, 2016), among which only SdLBFGS and AdaQN are designed to
solve nonconvex optimization problems. The SdLBFGS algorithm carefully controls the quality
of modified BFGS updates to preserve the positive-definiteness of Bt in (5) without line search.
AdaQN shares a similar idea but is specifically designed for RNNs by refining the initial L-BFGS
scaling, step acceptance control and choice of curvature information matrix, and adopting the SQN
framework (Byrd et al., 2016). Different from these two methods, APOLLO does not require Bt in
(5) to be positive definite, but replacing it with its rectified absolute value to handle nonconvexity.
Moreover, both SdLBFGS and AdaQN use the updating formula similar to L-BFGS and require even
larger history size (commonly ≥ 100) to guarantee convergence, preventing them from being applied
to large-scale optimization. For comprehensive comparison of SdLBFGS with Apollo, we conducted
experiments with small toy CNN models (details in Appendix G).
Adaptive First-Order Methods. From the diagonal approximation of Hessian, APOLLO is also
related to those diagonally-scaled first-order algorithms, such as AdaGrad (Duchi et al., 2011),
RMSProp (Tieleman & Hinton, 2012), AdaDelta (Zeiler, 2012), and Adam (Kingma & Ba, 2015).
Subsequently, a number of techniques have emerged to theoretically justify and algorithmically
improve Adam, including AMSGrad (Reddi et al., 2018), AdaBound (Luo et al., 2019), RAdam (Liu
et al., 2020) and AdaBelief (Zhuang et al., 2020). The main difference is that the diagonal precon-
ditioning in Apollo is directly derived from the quasi-Newton updating formula (6). In terms of
memory efficiency, Anil et al. (2019) and Chen et al. (2020) further reduces the memory cost of
adaptive methods, and Agarwal et al. (2019) proposed efficient full-matrix adaptive regularization.
Stochastic Second-Order Hessian-Free Methods. Stochastic Second-Order Hessian-Free meth-
ods (Martens, 2010; Martens & Sutskever, 2011) implicitly solve quadratic models using matrix-vector
products. Dauphin et al. (2014) argued the existence of saddle points and proposed a method to
rapidly escape them. K-FAC (Martens & Grosse, 2015) computes a second-order step by constructing
an invertible approximation of the Fisher information matrix in an online fashion. Shampoo (Gupta
et al., 2018) approximates the Fisher information matrix using low-rank decomposition. Recently,
Yao et al. (2020) proposed AdaHessian, which approximates the Hessian diagonal using Hutchinson’s
method. These second-order methods differ from Apollo mainly in the request of second-order
information of the objective function at each iteration.
6	Conclusion and Extensions
We have introduced Apollo, a simple and computationally efficient quasi-Newton algorithm for
nonconvex stochastic optimization. This method is aimed towards large-scale optimization problems
in the sense of large datasets and/or high-dimensional parameter spaces such as machine learning with
deep neural networks. Experimental results on three CV and NLP tasks demonstrate the effectiveness
of Apollo, in terms of both convergence speed and generalization performance. In Appendix C, we
briefly outline a few extensions to Apollo that we want to explore in future work.
9
Under review as a conference paper at ICLR 2022
References
Naman Agarwal, Brian Bullins, Xinyi Chen, Elad Hazan, Karan Singh, Cyril Zhang, and Yi Zhang.
Efficient full-matrix adaptive regularization. In International Conference on Machine Learning,
pp.102-110, 2019.
Rohan Anil, Vineet Gupta, Tomer Koren, and Yoram Singer. Memory efficient adaptive optimization.
In Advances in Neural Information Processing Systems, pp. 9749-9758, 2019.
Sue Becker, Yann Le Cun, et al. Improving the convergence of back-propagation learning with second
order methods. In Proceedings of the 1988 connectionist models summer school, pp. 29-37, 1988.
Antoine Bordes, Leon Bottou, and Patrick Gallinari. SGD-QN: Careful quasi-newton stochastic
gradient descent. The Journal of Machine Learning Research, 10:1737-1754, 2009.
Leon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In Advances in neural
information processing systems, pp. 161-168, 2008.
Charles G Broyden. Quasi-newton methods and their application to function minimisation. Mathe-
matics of Computation, 21(99):368-381, 1967.
Charles George Broyden. The convergence of a class of double-rank minimization algorithms. IMA
Journal of Applied Mathematics, 6(1):76-90, 1970.
Richard H Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou Zhu. A limited memory algorithm for
bound constrained optimization. SIAM Journal on scientific computing, 16(5):1190-1208, 1995.
Richard H Byrd, Gillian M Chin, Will Neveitt, and Jorge Nocedal. On the use of stochastic hessian
information in optimization methods for machine learning. SIAM Journal on Optimization, 21(3):
977-995, 2011.
Richard H Byrd, Samantha L Hansen, Jorge Nocedal, and Yoram Singer. A stochastic quasi-newton
method for large-scale optimization. SIAM Journal on Optimization, 26(2):1008-1031, 2016.
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony
Robinson. One billion word benchmark for measuring progress in statistical language modeling.
arXiv preprint arXiv:1312.3005, 2013.
X Chen, M Hong, S Liu, and R Sun. On the convergence of a class of adam-type algorithms for
non-convex optimization. In 7th International Conference on Learning Representations, ICLR
2019, 2019.
Xinyi Chen, Naman Agarwal, Elad Hazan, Cyril Zhang, and Yi Zhang. Extreme tensoring for
low-memory preconditioning. In International Conference on Learning Representations, 2020.
Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua
Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex
optimization. In Advances in neural information processing systems, pp. 2933-2941, 2014.
William C Davidon. Variable metric method for minimization. SIAM Journal on Optimization, 1(1):
1-17, 1991.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
John E Dennis, Jr and Jorge J More. Quasi-newton methods, motivation and theory. SIAM review, 19
(1):46-89, 1977.
John E Dennis, Jr and Henry Wolkowicz. Sizing and least-change secant methods. SIAM Journal on
Numerical Analysis, 30(5):1291-1314, 1993.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of machine learning research, 12(7), 2011.
10
Under review as a conference paper at ICLR 2022
Roger Fletcher. A new approach to variable metric algorithms. The computer journal, 13(3), 1970.
Roger Fletcher. Practical methods of optimization. John Wiley & Sons, 1987.
D Goldfarb. A family of variable metric updates derived by variational means. Mathematics of
Computation, 24(109):23-26,1970.
Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint:1308.0850, 2013.
Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor optimiza-
tion. arXiv preprint arXiv:1802.09568, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,
Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks
for acoustic modeling in speech recognition: The shared views of four research groups. IEEE
Signal processing magazine, 29(6):82-97, 2012.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural
networks. science, 313(5786):504-507, 2006.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. NeUral computation, 9(8):
1735-1780, 1997.
Nitish Shirish Keskar and Albert S Berahas. AdaQN: An adaPtive quasi-newton algorithm for training
rnns. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,
PP. 1-16. SPringer, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic oPtimization. In International
Conference on Learning Representations, 2015.
Alex Krizhevsky and Geoffrey Hinton. Learning multiPle layers of features from tiny images.
Technical rePort, Citeseer, 2009.
Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne
Hubbard, and Lawrence D Jackel. BackProPagation aPPlied to handwritten ziP code recognition.
Neural computation, 1(4):541-551, 1989.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Yingkai Li and Huidong Liu. Implementation of stochastic quasi-newton’s method in pytorch. arXiv
preprint arXiv:1805.02338, 2018.
Dong C Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization.
Mathematical programming, 45(1-3):503-528, 1989.
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaptive learning rate and beyond. In International Conference on
Learning Representations, 2020.
Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In
International Conference on Learning Representations, 2017.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-
ence on Learning Representations, 2019.
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate. In International Conference on Learning Representations, 2019.
11
Under review as a conference paper at ICLR 2022
Xuezhe Ma and Eduard Hovy. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF. In
Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers) ,pp.1064-1074, Berlin, Germany, August 2016.
Xuezhe Ma, Chunting Zhou, Xian Li, Graham Neubig, and Eduard Hovy. Flowseq: Non-
autoregressive conditional sequence generation with generative flow. In Proceedings of the
2019 Conference on Empirical Methods in Natural Language Processing, pp. 4282-4292, Hong
Kong, November 2019.
James Martens. Deep learning via hessian-free optimization. In Proceedings of the 27th International
Conference on International Conference on Machine Learning, pp. 735-742, 2010.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In International conference on machine learning, pp. 2408-2417, 2015.
James Martens and Ilya Sutskever. Learning recurrent neural networks with hessian-free optimization.
In Proceedings of the 28th international conference on machine learning (ICML-11), pp. 1033-
1040. Citeseer, 2011.
Aryan Mokhtari and Alejandro Ribeiro. Res: Regularized stochastic bfgs algorithm. IEEE Transac-
tions on Signal Processing, 62(23):6089-6104, 2014.
Aryan Mokhtari and Alejandro Ribeiro. Global convergence of online limited memory bfgs. The
Journal of Machine Learning Research, 16(1):3151-3181, 2015.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In
ICML, 2010.
JL Nazareth. If quasi-newton then why not quasi-cauchy. SIAG/Opt Views-and-news, 6:11-14, 1995.
Jorge Nocedal and Stephen Wright. Numerical optimization. Springer Science & Business Media,
2006.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and
Michael Auli. FairSeq: A fast, extensible toolkit for sequence modeling. In Proceedings of the
2019 Conference of the North American Chapter of the Association for Computational Linguistics
(Demonstrations), pp. 48-53, 2019.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural
networks. In International conference on machine learning, pp. 1310-1318, 2013.
Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks, 12(1):
145-151, 1999.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
International Conference on Learning Representations, 2018.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical
statistics, pp. 400-407, 1951.
Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint
arXiv:1609.04747, 2016.
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations by
error propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive Science,
1985.
Nicol N Schraudolph, Jin Yu, and Simon Gunter. A stochastic quasi-newton method for online convex
optimization. In Artificial intelligence and statistics, pp. 436-443, 2007.
David F Shanno. Conditioning of quasi-newton methods for function minimization. Mathematics of
computation, 24(111):647-656, 1970.
12
Under review as a conference paper at ICLR 2022
Jascha Sohl-Dickstein, Ben Poole, and Surya Ganguli. Fast large-scale optimization by unifying
stochastic gradient and quasi-newton methods. In International Conference on Machine Learning,
pp. 604-612, 2014.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15(1):1929-1958, 2014.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2818-2826, 2016.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2), 2012.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,匕Ukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Xiao Wang, Shiqian Ma, Donald Goldfarb, and Wei Liu. Stochastic quasi-newton methods for
nonconvex stochastic optimization. SIAM Journal on Optimization, 27(2):927-956, 2017.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Advances in neural information
processing systems, pp. 4148-4158, 2017.
Philip Wolfe. The secant method for simultaneous nonlinear equations. Communications of the ACM,
2(12):12-13, 1959.
Saining Xie, Ross Girshick, Piotr Doll念 Zhuowen Tu, and Kaiming He. Aggregated residual
transformations for deep neural networks. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 1492-1500, 2017.
Zhewei Yao, Amir Gholami, Sheng Shen, Kurt Keutzer, and Michael W Mahoney. ADAHESSIAN:
An adaptive second order optimizer for machine learning. arXiv preprint arXiv:2006.00719, 2020.
Wei Yuan and Kai-Xin Gao. EAdam optimizer: How epsilon impact adam. arXiv preprint
arXiv:2011.02150, 2020.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint:1212.5701, 2012.
Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank Reddi, Sanjiv
Kumar, and Suvrit Sra. Why are adaptive methods good for attention models? Advances in Neural
Information Processing Systems, 33, 2020.
Mingfa Zhu, John Lawrence Nazareth, and Henry Wolkowicz. The quasi-cauchy relation and diagonal
updating. SIAM Journal on Optimization, 9(4):1192-1204, 1999.
Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, Xenophon Pa-
pademetris, and James Duncan. Adabelief optimizer: Adapting stepsizes by the belief in observed
gradients. Advances in Neural Information Processing Systems, 33, 2020.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In
Proceedings of the 20th international conference on machine learning (icml-03), pp. 928-936,
2003.
13