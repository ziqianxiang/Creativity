Under review as a conference paper at ICLR 2022
TESSERACT: Gradient flip score to secure fed-
erated learning against model poisoning at-
TACKS
Anonymous authors
Paper under double-blind review
Ab stract
Federated learning—multi-party, distributed learning in a decentralized
environment—is vulnerable to model poisoning attacks, more so than central-
ized learning. This is because malicious clients can collude and send in carefully
tailored model updates to make the global model inaccurate. This motivated the
development of Byzantine-resilient federated learning algorithms, such as Krum,
Bulyan, FABA, and FoolsGold. However, a recently developed untargeted model
poisoning attack showed that all prior defenses can be bypassed. The attack uses the
intuition that simply by changing the sign of the gradient updates that the optimizer
is computing, for a set of malicious clients, a model can be diverted from the optima
to increase the test error rate. In this work, we develop Tesseract—a defense
against this directed deviation attack, a state-of-the-art model poisoning attack.
Tesseract is based on our intuition that in federated learning, certain patterns of
gradient flips are indicative of an attack. This intuition is remarkably stable across
different learning algorithms, models, and datasets. Tesseract assigns reputation
scores to the participating clients based on their behavior during the training phase
and then takes a weighted contribution of the clients. We show that Tesseract
provides robustness against even a white-box version of the attack.
1 Introduction
Federated learning (FL) Smith et al. (2017); Yang et al. (2019) offers a way for multiple clients on
heterogeneous platforms to learn collaboratively without sharing their local data. The clients send
their local gradients to the parameter server that aggregates the gradients and updates the global model
for the local clients to download. FL can be attacked during the training phase by compromising a set
of clients that then send maliciously crafted gradients. The attack can be targeted against particular
data instances or can be untargeted. The latter brings down the overall accuracy by affecting all
classes. To counter this threat, a set of approaches has been developed for countering Byzantine
clients in FL, e.g., Krum Blanchard et al. (2017), Bulyan Mhamdi et al. (2018), Trimmed Mean and
Median Yin et al. (2018), FoolsGold Fung et al. (2020), and FABA Xia et al. (2019). In this work, we
use the state-of-the-art (SOTA) untargeted model poisoning attack called a directed deviation attack,
proposed recently in Fang et al. (2020). This has been shown to bypass all existing Byzantine-robust
aggregation techniques, e.g., Krum, Bulyan, Trimmed mean, and Median. In our experiments, this
attack has been found to decrease the test accuracy from 90% to a low 9% when a DNN is trained
using Krum on the MNIST dataset, distributed among 100 clients, 20 of which are under the attacker’s
control. We describe the relevant details of this attack in Section 2.2.
Our solution. We propose a novel defense called Tesseract against untargeted model poisoning
attacks (Figure 1), which uses a stateful model to reduce the contributions by suspicious clients to the
global model update. We show that where all prior Byzantine-resilient federated learning approaches
fail against the directed-deviation attack of Fang et al. (2020), which is the state-of-the-art untargeted
model poisoning attack, Tesseract is able to recover the test accuracy of the trained model. This
benefit applies even when the attack knows the algorithm and all the parameters of our defense, i.e.,
an adaptive white-box attack. Tesseract is based on a simple intuition that for a sufficiently small
learning rate, as the model approaches an optima in a benign setting, a large number of gradients do
not flip their direction with large magnitudes, that is, a degree of inertia is maintained. Our intuition
is supported from the analysis we show in Figure 2. We capture this quantitatively in a metric that
we propose, called flip-score, which is the sum of squares of gradient magnitudes of all parameter
updates that suggest a flip in the gradient direction from the previous global update. We find that
1
Under review as a conference paper at ICLR 2022
Figure 1: TESSERACT’s architecture where c out of m clients may be malicious and
send carefully crafted values of their local models to throw the global model off
convergence. TESSERACT weights the gradients, received from the clients, by their
reputation scores before aggregation, (see varying and thicknesses of arrows from the
clients.)
our intuition and correspondingly the defense TESSERACT also holds for other attacks, e.g., the
targeted and untargeted label flipping attacks (Appendix § A.3). However, the other attacks are less
damaging, consistent with the observation in Fang et al. (2020). Thus, in the main paper, we focus on
the directed deviation model poisoning attack.
(a) Only Benign updates aggregated
(b) Only Malicious updates aggregated
Figure 2: The average flip-score of malicious and benign clients over time for a motivating experiment where
a DNN is trained on MNIST for 500 iterations (80 benign, 20 malicious clients). In (a), only benign updates
were aggregated using FEDSGD, and in (b), only malicious updates were aggregated, depicting the two
extreme cases of federated learning in a malicious setting. These results show that when the global model
update is benign, the malicious clients send gradients with high flip-scores to deviate the model from
convergence. However, when the global model update itself was poisoned, the benign clients send high
flip-score gradients for recovery whereas the malicious clients maintain the direction of the already-poisoned
model. Thus the intuition is too high flip-scores and too low flip-scores in a coordinated manner are red flags.
In summary, Tesseract makes the following contributions.
1.	We use our simple intuition to detect malicious clients that attack federated learning using the
SOTA attack model Fang et al. (2020). Our intuition is that certain patterns of flips of the signs
of gradients across multiple model parameters and across multiple clients are rare under benign
conditions.
2
Under review as a conference paper at ICLR 2022
2.	We use a stateful suspicion model to keep the history of every client’s activity and use that as a
weighting factor in the aggregation. We theoretically prove the convergence of Tesseract that
uses the weighted averaging and establish a convergence rate.
3.	We evaluate Tesseract on DNNs trained on MNIST and FEMNIST, ResNet-18 on CIFAR-10,
and GRU on the Shakespeare dataset. We comparatively evaluate our defense against six base-
lines, including the most recent ones, FABA Xia et al. (2019), FoolsGold Fung et al. (2020), and
FLTrust Cao et al. (2020) and show that Tesseract remains robust even against an adaptive
white-box attacker. While several of the existing defenses shine under specific configurations
(combination of attacks and datasets/models), Tesseract is the only one whose protection trans-
fers well across configurations. We release the source code, the attack scripts, the trained models,
and the test harness for the evaluation at the anonymized page https://www.dropbox.com/
sh/h9ulw6y2f8rzv64/AADe1Sb9PhhCLqclzgZ4xvvJa?dl=0.
The rest of the paper is organized as follows. We describe in Sec 2 the threat model, the SOTA attack,
and why all existing Byzantine-resilient federated learning approaches are susceptible. We present
Tesseract’s design in Sec 3 and convergence analysis in Sec 4. We describe the baselines and the
datasets in Sec 5, and evaluation in Sec 6.
2 Background
Our simulation of federated learning consists of m clients, each with its own local data, but with the
same model architecture and SGD optimizer, out of which c are malicious, as shown in Figure 1. The
parameter server assumes that a maximum of cmax number of clients can be malicious. The clients
run one local iteration, send their gradients (in unencrypted form) to the server, which updates the
global model for the clients to download in a synchronous manner.
2.1	Byzantine-resilient Federated Learning
Here we describe the leading defenses briefly, stymied by SOTA untargeted model poisoning attacks.
1.	The simplest aggregation technique is FedSGD McMahan et al. (2017) that does a simple weighted
mean aggregation of the gradients, weighted by the number of data samples each client holds.
FedSGD can be attacked by a single malicious client that can send boosted gradients.
2.	Trimmed mean and Median Yin et al. (2018) aggregate the parameters independently, where
one trims cmax each at the lower and higher extremes of every parameter and the other takes the
median of every parameter update across the gradients received from all the clients. The full-Trim
attack Fang et al. (2020) is specifically designed toward these aggregations rules.
3.	Krum Blanchard et al. (2017) selects one local model as the next global model. The client that has
the lowest Euclidean distance from its closest (m - cmax - 2) neighbors is chosen as the local
model. The full-Krum attack Fang et al. (2020) is tailored to attack Krum.
4.	Bulyan Mhamdi et al. (2018) combines the above approaches by running Krum iteratively to select
a given number of models, and then running Trimmed Mean over the selected ones. The Full-Trim
attack is also transferable to Bulyan.
5.	FABA Xia et al. (2019) iteratively filters out models farthest away from the mean of the remaining
models, cmax number of times before returning the mean of the remaining gradients.
6.	FoolsGold Fung et al. (2020) was motivated to defend against poisoning attacks by Sybil clones,
and thus, it finds clients with similar cosine similarity to be malicious, penalizes their reputation,
and returns a weighted mean of the gradients, weighed by their reputation.
7.	FLTrust Cao et al. (2020) bootstraps trust in the clients by assuming that the server has access to a
clean validation dataset, albeit small, and returns a weighted mean of the gradients weighed by this
trust. In our setting, we do not see a realistic method to access such as clean dataset, especially
considering the non-iid nature of the local datasets at the clients.
2.2	Threat model: State-of-the-art Model Poisoning Attack
Our threat model consists of a scenario where an adversary compromises a fraction of all clients
participating in federated learning. We assume that the adversary also has access to the gradient vector
sent by the benign clients to the server, and knows what aggregation algorithm the server is running.
We focus on the SOTA untargeted model poisoning attack Fang et al. (2020)—a directed deviation
attack (DDA) bypassing all known defenses 1. To address generality (more details in Appendix
1 There is no overlap between the authors of this current submission and of the SOTA attack Fang et al. (2020).
3
Under review as a conference paper at ICLR 2022
§ A.6.2), we have also evaluated Tesseract on label-flipping attacks in Appendix § A.3. The DDA
changes the local models on the compromised worker devices. This change is done strategically
(through solving a constrained optimization problem) such that the global model deviates the most
toward the inverse of the direction along which the benign global model would have changed. Their
intuition is that the deviations accumulated over multiple iterations would make the learned global
model differ from the benign one significantly.
Tesseract defends against both variants of the DDA, one specialized to poison Krum (transferable
to Bulyan, we call this the Full-Krum attack), the other specialized for Trimmed Mean (transferable
to Median, we call this the Full-Trim attack), respectively. We assume a full-knowledge (white-box)
attack where the attackers have access to the current benign gradients. They themselves compute the
benign gradients on their local data as well, and thus estimate the benign direction as the average of all
benign gradients. This value is stored in a vector s of size equal to the number of model parameters.
s(t, ∙) = Sign(Sum(OLMi(t, ∙)),
i
where θLMi(t, ∙) is the gradient updates of client i.
Full-Krum attack: Having estimated S, the attackers send gradients in the opposite direction, all
with a magnitude λ, with some added noise to appear different but still maintaining a small Euclidean
distance from one another. The upper bound of λ is computed in every iteration as a function of
m, c, |P|,GM(t, ∙), θLMi(t + 1, ∙), where C out m participating clients are malicious, |P| is the
number of parameters in the global model GM . λ is then iteratively decreased until the attackers
make sure (using a local estimate) that the parameter server would have chosen the attacked model,
using the Krum aggregation technique.
Full-Trim attack: The Trim attack while following the same fundamental principle of flipping the
gradient direction, attempts to skew the distribution of every parameter j toward the inverse of the
direction that S(t, j) suggests in order to attack a mean-like aggregation. It does so by randomly
sampling gradient magnitudes from a range that has been computed by the attackers, guaranteed to
skew the gradient distribution of every parameter, without appearing as obvious outliers (which would
be caught by a method such as Trimmed Mean). Therefore, the attacked gradients here look more
diverse than those in the Full-Krum attack. Overall, both forms of the DDA smartly take advantage
of the fact that none of the existing aggregation techniques looks at the change in gradient direction
to identify malicious gradients in a robust way.
This kind of an attack is an extremely likely scenario in a cross-device as well as a cross-silo federated
learning setting, (more details in Appendix § A.6.1) and needs to be defended against.
3 Design
Tesseract uses a reputation-based scheme to compute the aggregation weights of the participating
clients. Reputation-based schemes have been widely used in the literature. For example, Kang et al.
(2019) and Zhang et al. (2012) make use of reputation score as an incentive mechanism for clients to
remain in the system. Fung et al. (2020) and Awan et al. (2021) compute reputation score from the
pairwise cosine similarity of the gradients between clients, Cao et al. (2021) does that by computing
the cosine similarity of the client gradients with reference to trusted gradients calculated on a clean
validation dataset at the server. We compute reputation-score in a different way by using flip-score of
the local gradients, as described below. TESSERACT assumes that a maximum of Cmaχ(< m) clients
can be malicious. It penalizes 2cmax clients and rewards the rest in every iteration by an amount
W (i, t) based on their flip-score (described below) and updates their reputation score, where i is the
client ID and t is time. We present the pseudocode in Algorithm 1.
W(i,t)
-(1 -
2Cmaχ
m ,
2Cmaχ
m
),
if penalized.
if rewarded.
These values make sure that that the expectation of the reputation score of a client is zero if their
flip-scores belong to a uniform random distribution (see Appendix § A.1). Also, this makes recovery
difficult as the penalty value is higher than the reward as Cmax < mm and allows us to be conservative.
The reputation score of a client i is initialized and updated as follows -
RS (i, 0) = 0.
RS(i,t) = μdRS(i, t — 1)+ W(i,t), t > 0.
4
Under review as a conference paper at ICLR 2022
where 0 ≤ μd ≤ 1.0 is the decay parameter, and RS is the reputation score. A low μd gives more
importance to the present flip-score and a high μd gives significant importance to the past performance
of a client. This decay operation also helps cap the maximum and minimum reputation score (with
μd < 1, for more details, see Appendix § A.1). We normalize the reputation score using softmax to
do a weighted mean aggregation, but a user can use this to directly filter out the suspicious gradients
and use an aggregation rule of one’s choice. We halve the reputation score of every client if any of
the values grows so large that the softmax operation causes an overflow. This is motivated by the fact
that the reputation score is a cumulative quantity, and can potentially keep increasing with time in an
unbounded manner.
Flip-score. We compare the present gradients θLMi(t + 1, ∙) sent by local model i with the gradient
direction of the global model at time t, Sg(t, ∙) = Sign(GM(t, ∙) - GM(t - 1, ∙)). We define
flip-score as the sum of square of the gradient magnitudes of all parameters that experience a change
in their gradient direction, that is,
|P |-1
FSi(t + 1) =	(OLMi(t + 1, j))2(Sign(OLMi(t + 1, j)) 6= Sg(t,j)),
j=0
(1)
where |P| is the total number of parameters in the model being trained. A low flip-score thus suggests
that the gradient updates are approximately in the same direction as the previous iteration. In contrast,
a high flip-score suggests a deviation from the previous update. This could mean either a large
number of parameters have flipped direction, or a small number of parameters have flipped direction
with large magnitudes, or both. As observed in Figure 2, if the previous global update was benign,
a malicious client will tend to have a high flip-score. However, if the previous update itself was
poisoned, the flip-score of benign clients will be high and those of malicious clients will be low.
Therefore, we penalize cmax number of clients at either end of the current flip-score distribution,
as also done by Yin et al. (2018), but we do this in the context of gradient values per parameter.
This allows our system to have a higher detection coverage irrespective of whether the global model
was poisoned in one iteration or not. Since we allow redemption, and make sure that in a random
penalization scheme, the expectation of reputation score of a benign node approaches zero, we do
not unnecessarily penalize benign clients with low flip-scores. Also, since we do not use any hard
threshold for detecting attacks, and identify only the extreme ends of flip-score distribution in every
iteration as malicious, we do not prevent low flip-score moves. At the time of convergence, when
most of the clients will favor low flip-score moves, such moves will be allowed even after ignoring
the gradients at the extreme ends.
Algorithm 1 Federated learning with TESSERACT
Output: Global model GM (t + 1, ∙)
Input: Local model updates W = θLMi(t + 1, ∙)
Parameters: m,
cmax, μd
0 : Initialize reputation RSi(0) = 0 for every client i
1	: Initialize global direction Sg (0) to a zero vector
2	: for every client i compute flip-score:
3	: FSi(t + 1) = P|jP=|0-1 (OLMi(t + 1,j))2(Sign(OLMi(t + 1, j)) 6= Sg(t, j))
4	: Penalize Cmax clients on either end of FS spectrum as: RS(i, t + 1) = μdRS(i, t) — (1 - 2cmαx)
6	: Reward the rest of the clients as: RS(i, t + 1) = μdRS(i, t) + 2cmαx
RS
7	: Normalize reputation weights: WR = PeeRS
8	: Aggregate gradients: OGM(t + 1, ∙) = WT WR
9	: Update global direction: Sg(t + 1, ∙) = Sign(OGM(t + 1, ∙))
10	: Update global model and broadcast: GM(t + 1, ∙) = GM(t, •) + OGM(t + 1, ∙) * 1
4 Convergence Analysis
We make the following assumptions on LMk, part of which are adapted from Li et al. (2020).
1. Assumption #1: LMk are all L-smooth, that is, for all v and W, LMk (v) ≤ LMk (W) + (v -
W)TOLMk(w) + L2∣∣v - w∣∣2∙
5
Under review as a conference paper at ICLR 2022
2.	Assumption #2: LMk are all μ-strongly convex, that is, for all V and w, LMk(V) ≥ LMk(W) +
(v - W)TOLMk(w) + 2IlV - wk2.
3.	Assumption #3: Let ξtk be sampled uniformly at random from the local data of the k - th client,
then the variance of stochastic gradients of each client is bounded, that is, E IOLMk (wtk, ξtk) -
OLMk(wtk)I2 ≤ σk2 for k = 1, 2, ...m.
4.	Assumption #4: The expected squared norm of stochastic gradients is uniformly bounded, that is,
E IOLMk(wtk, ξtk)I2 ≤ G2 for k = 1, ...m, and t = 0, ..T - 1.
5.	Assumption #5: Within K iterations, the reputation score of malicious clients drop at least by δmal,
and reputation score of benign clients increase at least by δben, that is, |RSmt al - RSmt-aKl | ≥ δmal
and |RSbten - RSbte-nK | ≥ δben, fort = 0, ..T - 1.
Assumptions #1 and #2 are standard and apply to l2-norm regularized linear regression, logistic
regression, and softmax classifier. Assumptions #3 and #4 have been also made by Zhang et al.
(2012); Stich (2018); Stich et al. (2018); Yu et al. (2019). In our problem setting, Assumption #3
claims that the gradient with a subset of local data is bounded from the gradient with whole batch for
both malicious and benign clients. Assumption #5 claims that after every K iterations, our algorithm
will have better ability to distinguish the malicious clients.
Following the Theorem 1 in Li et al. (2020), after simplifying for our case in which the clients
communicate with the parameter server in every iteration, we derive this convergence (details in
Appendix § A.4). Let GM* and LMk be the minimum value of GM and LMk respectively, then:
一 「一	2 L	C	C 二	μ2.........0.
E[GM(W)T] - GM ≤ —2 - ——ʃ(ɪ^Pkσk + 6lγ + 8g + 8G y^pk,0 + -ɪkwθ - W II).
μ Y + T M	M 4
where L, μ, G are defined above, Y = max{8L, 1}, η = .(二),T is the total number of iterations,
c is the number of malicious clients, pk,0 is the initial weight for malicious clients, and Γ =
GM* - Pkm=1pkLMk*, that effectively quantifies the degree of non-iidness according to Li et al.
(2020). This shows that a weighted mean aggregation is guaranteed to converge in federated learning.
The converge speed is O( TT). The increase of number of malicious clients C increases the bound.
5	Implementation
We have simulated the federated learning on a single machine with a Tesla P100 PCIe GPU with
16GB memory, using PyTorch, with as many clients as can be handled by our machine. The data was
distributed with a non-IID bias of 0.5 (default), except for Shakespeare where the data was distributed
sequentially among the clients and FEMNIST where the data was distributed by the writer. In our
simulation, all clients run one local iteration on a batch of its local data before communicating with
the parameter server in a synchronous manner. The clients sample their local data in a round-robin
manner, send their local gradients to the parameter server, and download the updated global model
before running the next local iteration. The malicious clients attack every iteration of training, we
assume: c = cmax .
Baselines and datasets. The baseline aggregation rules used are Krum, Bulyan, Trimmed Mean,
and Median. We also compare Tesseract with the recent defense techniques of FABA Xia et al.
(2019), FoolsGold Fung et al. (2020), and FLTrust Cao et al. (2020). We evaluate Tesseract on 4
different datasets (Table 1). The DNN trained on MNIST has 2 conv layers with 30 and 50 channels
respectively, each followed by a 2 × 2 maxpool layer, then by a fully connected layer of size 200, and
an output layer of size 10. We use a constant learning rate, except for CIFAR-10 where we start with
zero, reach the peak at one-fourth of the total iterations, and slowly get down to zero again. The CNN
trained on the FEMNIST dataset follows the same network architecture as Caldas et al. (2019).
6	Evaluation
6.1	Macro Experiments
In Table 2, we compare the test accuracy achieved by various aggregation techniques in benign
and malicious conditions. We do not claim to provide optimal model architectures that can achieve
the best test accuracy, but we provide fair comparison of the all the defenses on the same model
with the same training parameters. For Shakespeare, which is an NLP dataset, we report the test
loss; for all others, we report test accuracy. The final reported test loss value does not capture the
training dynamics, which can be observed in Figure 3 for the more damaging Full-Krum attack.
We have not shown the test loss curve for Krum aggregation because of the large loss values here,
6
Under review as a conference paper at ICLR 2022
Table 1: Datasets with the number of classes (nc) and training samples (ns), the models and model
Parameters (P), training rounds (n), batch size (b), learning rate (Ir), total numberof Clients (m), number
of malicious ones (c), and decay Parameter(μ) used in TESSER ACT. * variable learning rate, peaks at 0.1.
Dataset	nc	ns	Model	P	nr	b	lr	m	C	μ
-MnIst-	^T0-	60k	-DNN-	0.27M	500	32SΓ~	^00T	^T00^	^20^	0.99
CIFAR-10 一	-tg-	50k	ReSNet-18	5.2M	2000	T28-	0.1*	-T0-	"ɪ	-0:99-
Shakespeare	^T00^	-	-GRU-	O14M	2000	^T00^	^00T	-T0-	"ɪ	~Q99~
FEMNIS厂	~6Γ~	805k	DNN	6.6M	2000		~0Γ~	"ɪ"	~τ~	~099~
Table 2: Attack imPact - Test accuracy for Directed Deviation model Poisoning attacks (Full-Krum;
Full-Trim), on different datasets with c/m = 0.2. For the ShakesPeare dataset, test loss has been rePorted.
We verify the damaging imPact of the Full-Trim attack on mean-like aggregations (FedSGD, Trimmed mean,
Median) and Full-Krum attack on Krum-like aggregations (Krum, Bulyan). We also observe that the existing
defenses—FABA, FoolsGold, and FLTrust—seem to defend against this attack in some cases, and fail in
others, whereas TESSERACT consistently shines in all cases.
Attack Defense Test accuracy (%)/ Test loss (only for Shakespeare)
		MNIST+ DNN	CIFAR-10+ ResNet-18	Shakespeare+ GRU	FEMNIST+ DNN
None	-FedSGD-	-9245-	71717	1762	83:60
	TESSERACT	92.52	6692	164	83:58
	FABA	-9177-	6994	176	8269
	FoolsGold	-9120-	7071	163	-83780-
	-FLTrUSt-	-8770-	6808	1762	8272
Full- Krum	FedSGD 二	82.97	39.68 Z	1.62	=	29.87 二
	Krum	-892-	9.81	1198	5.62
	Bulyan	-TO34-	1324	9.23	9.91
	TESSERACT	-87.73-	6126	164	-80719-
	FABA	-8699-	55:96	175	55.61
	FoolsGold	47.12-	4228	1763	0.07
	-FLTrUSt-	-8250-	65725	167	79:53
Full- Trim	FedSGD 二	65.25	47.32 Z	1.74	=	32.34 二
	Trim 一	-36.36-	55:25	328	13.03
	Median	-2837-	5054	330	456
	TESSERACT	-90.55-	67:65	166	82.51
	FABA	91.84	67:31	1764	79.66
	FoolsGold	-9161-	-69.24-	166	-83709-
	FLTrUSt 一	34.20	64.23 一	1.68	—	79.28 一
but have pushed the results to Appendix § A.3 in Figure 9. We see that Tesseract is the winner
or 2nd place finisher in 7 of the 12 cells (benign + two attacks × 4 datasets). This on the surface
appears to be not very promising, till one looks deeper. The baseline protocols that finish first in one
configuration fare disastrously in other configurations, indicating that they are tailored to specific
attacks or datasets (whether by conscious design or as an artifact of their design). For example,
FoolsGold does creditably for the Full-Trim attack but is vulnerable against the Full-Krum attack.
Averaging across the configurations, it appears FABA is the closest competitor to Tesseract. We
observe that FABA, although it performed well for c/m = 0.2, failed to defend when the number
of attackers grew to c/m = 0.3, evident from the results in Figure 3(b). We have used F-MNIST,
Ch-MNIST, and Breast cancer Wisconsin Dataset here to show the effect on diverse datasets. As the
number of attackers increases, the mean starts to shift more toward them. One false positive detection
by FABA can cause it to trim out a benign local update, which causes the mean to shift further toward
the malicious updates iteratively, and fail. On the other hand, Tesseract is guaranteed to defend
against the attack as long as C ≤ Cmax and c < mm .We find empirically (result not shown) that FABA
degrades fast, and much faster than Tesseract, when the fraction of malicious clients increases.
Whereas FABA fails at C/m = 0.3, we show in Figure 7(a) that TESSERACT remains stable until
C/m = 0.45. We also show in Figures 7(b) and (c) that TESSERACT remains robust across a wide
range of non-IID bias, that is 0.1 to 0.8. Tesseract and FABA require an estimate of the upper
7
Under review as a conference paper at ICLR 2022
Figure 3: Left shows the test loss curve comparing TESSERACT with the benchmark aggregation algorithms
against the Full-Krum attack. We see that the attack generates sporadic spikes in training, best handled by
Tesseract, evident from its smooth test loss curve. Right shows the comparison of FABA and Tesseract
across diverse datasets for c/m = 0.3. FABA begins to fail with a higher fraction of malicious clients, while
Tesseract remains robust.
Table 3: Fraction of malicious or benign clients allotted
weights above 1e-4 averaged over 500 iterations. The
weakness of FoolsGold and FLTrust stems in part from the
fact that they assign low weight to a significant fraction of
benign clients for effective detection.
Figure 4: Clients’ reputation weights against
time with TESSERACT, with higher weights to
benign clients, as malicious ones tend to 0.
Defense	Mal/ Ben	Benign	Full- trim	Full- krum
FoolsGold 一	nben	0.29	0.30	0.10
	nmal	-	0.00	0.64
FLTrUst 一	nben	0.48	0.45	0.49
	nmαl	-	0.52	0.63
TESSERACT	nben	0.75	0.75	0.63
	nmal	-	0.00	0.08
bound of number of malicious clients, cmax to be known. FoolsGold and FLTrust, on the other hand,
make use of cosine similarity among clients, and with a trusted cross-check dataset at the server,
respectively, to identify suspicious clients. We have found that both of these techniques unnecessarily
penalize many benign clients and assign them a zero weight in order to conservatively defend against
an attack, as can be seen in Table 3. This can have a significantly negative impact on a practical
system where one wishes to learn from data that the different clients hold locally. On the other
hand, Tesseract allows all clients to contribute to the global model update that do not have a large
negative reputation score. Figure 4 shows the evolution of the average reputation score of malicious
and benign clients. When benign and malicious clients start from the same region, the weights of
malicious clients decrease with time tending to zero, while those of the benign clients increase with
time.
6.2	Adaptive attack
Having shown the performance of Tesseract against the above attacks, we proceed to analyze an
adaptive attack scenario, i.e., one where the attacker has full knowledge of TESSERACT, including
the dynamic value of the cutoff flip-scores. Thus, at iteration t + 1, the attacker knows F Slow (t) and
F Shigh(t) beyond which the clients were penalized at iteration t. The Adaptive-Krum attack first
computes the target malicious gradients at t+ 1, and if its flip-score goes above F Shigh(t), it reverses
the attack on its less important parameters, i.e., parameters that would have had low magnitude
updates without attack. It does this by replacing 5% of the attacked parameters, at a time, with their
benign values until the flip-score is brought down to ensure stealth. Since the stealthy attack will be
less powerful than the original intended attack, the global model will only be partially poisoned, and
the attackers are not expected to occupy the lower spectrum of the flip-score. This has been verified
8
Under review as a conference paper at ICLR 2022
Figure 5: Performance of TESSERACT against adaptive white-box attacks, specifically designed to attack
Tesseract, evaluated on MNIST and CIFAR-10. We observe a significant improvement in test accuracy,
compared to the base case impact of Full-Krum on Krum and Full-Trim on FedSGD, as reported in Table 2.
in our experiments as well. All the malicious clients send these attacked parameters with some added
randomness in order to support one other. We observe a trade-off between stealth and attack impact
in this case, as can be seen in Figure 5.
The Adaptive-Trim attack is a smarter and collaborative attack. Its target is to generate attacked
gradients vi, i = 0,1, 2,•一C - 1. It first computes the Full-Trim target attack Ui for every malicious
client i. Then, it initializes v0 to u0 and modifies it, until v0 generates a flip-score that is less than
F Shigh(t). Client i = 1 then updates its target attack from u1 to v1 = u1 + (u0 - v0) in order to
compensate for a sub-optimal attack created by i = 0 because of the flip-score-evasion constraint.
Thus, a client may not necessarily find a solution if the target grows and the constraints are hard to
solve for. The attacker hopes that Pic=-01 ui = Pic=-01 vi . The performance of TESSERACT against
this adaptive white-box attack is shown in Figure 5. We find that the adaptive attacks are not very
effective against Tesseract. The benign accuracy for the two datasets are 92.45% and 71.17%.
This happens due to multiple reasons: 1) the attack loses in strength while trying to gain in stealth,
2) the attackers need not be allotted equal reputation weights, so the weighted sum of the attacked
gradients v do not match with the weighted sum of the target attack u, 3) the flip-score distribution
is dynamic, as can be seen in Figure 2, and changes from time t to t + 1, and when it decreases in
consecutive iterations by a significant amount, the attackers can still be blocked.
We have also created a more knowledgeable attacker, by modifying the above constraint with a
weighted sum, weighted by the reputation scores. We have evaluated this attack in Figure 8 in
Appendix § A.3. Tesseract is effective against this attack too, for reasons (1) and (3) given above. 7
7 Discussion and Conclusion
We have presented Tesseract, a secure parameter server for federated learning robust to model poi-
soning attacks. Tesseract uses a stateful algorithm to allocate reputation scores to the participating
clients to lower the contribution of maliciously behaving clients. We define malicious behavior using
a metric, flip-score, which when very high or low, captures attacks that try to divert the global model
away from convergence. This makes Tesseract a robust defense, no matter when it is instantiated,
although we recommend enabling Tesseract right from the start. Tesseract can also be used
to just filter out clients based on their reputation, so that an aggregation of the user’s choice can be
used. We evaluate the benefits of Tesseract compared to the fundamental FL aggregation FedSGD
and state-of-the-art defenses, namely Krum, Bulyan, FABA, FoolsGold, and FLTrust. We evaluate
using full knowledge untargeted model poisoning attacks that have recently been found to be most
damaging against FL. We find that different existing defenses shine under specific combinations of
attacks and datasets/models. However, Tesseract provides transferable defense with accuracy
competitive with individual winners under all configurations. Further, Tesseract holds up better
than its closest competitor, FABA, when the fraction of malicious clients increases (beyond 20%).
Finally, an adaptive white-box attacker with access to all internals of Tesseract, including dynami-
cally determined threshold parameters, cannot bypass its defense. All of our evaluation is limited
to a synchronous setting with no gradient encryption, as gradient encryption is less relevant in a
9
Under review as a conference paper at ICLR 2022
cross-device scenario due to its high computational overhead Zhang et al. (2020). However, these
form logical avenues for our future work.
10
Under review as a conference paper at ICLR 2022
8 Reproducibility Statement
All the details for the datasets and models used along with the implementation of the adaptive
attack is given in Section 5. To help with the reproducibility of the experiments, we make available
all of our code, the trained models, the training hyperparameter values, and the raw results from
our testing. These are all to be found at the anonymized link https://www.dropbox.com/
sh/h9ulw6y2f8rzv64/AADe1Sb9PhhCLqclzgZ4xvvJa?dl=0. The code has also been
uploaded as a supplementary file in the current submission.
References
Sana Awan, Bo Luo, and Fengjun Li. Contra: Defending against poisoning attacks in federated
learning. In European Symposium on Research in Computer Security, pp. 455-475. Springer, 2021.
Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to
backdoor federated learning. In International Conference on Artificial Intelligence and Statistics,
pp. 2938-2948. PMLR, 2020.
Peva Blanchard, Rachid Guerraoui, and Julien Stainer. Machine learning with adversaries: Byzantine
tolerant gradient descent. In Advances in Neural Information Processing Systems, pp. 119-129,
2017. Krum paper.
Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Konecny, H. Brendan
McMahan, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for federated settings. arXiv
preprint arXiv:1812.01097, 2019.
Xiaoyu Cao, Minghong Fang, Jia Liu, and Neil Zhenqiang Gong. Fltrust: Byzantine-robust federated
learning via trust bootstrapping. arXiv preprint arXiv:2012.13995, 2020.
Xiaoyu Cao, Minghong Fang, Jia Liu, and Neil Zhenqiang Gong. Fltrust: Byzantine-robust federated
learning via trust bootstrapping. Network and Distributed System Security Symposium, pp. 1-18,
2021.
Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Zhenqiang Gong. Local model poisoning
attacks to byzantine-robust federated learning. In 29th USENIX Security Symposium (USENIX
Security 20), Boston, MA, August 2020. URL https://www.usenix.org/conference/
usenixsecurity20/presentation/fang.
Clement Fung, Chris J. M. Yoon, and Ivan Beschastnikh. The limitations of federated learning in sybil
settings. In 23rd International Symposium on Research in Attacks, Intrusions and Defenses (RAID
2020), pp. 301-316, San Sebastian, October 2020. USENIX Association. ISBN 978-1-939133-18-2.
URL https://www.usenix.org/conference/raid2020/presentation/fung.
Jiawen Kang, Zehui Xiong, Dusit Niyato, Shengli Xie, and Junshan Zhang. Incentive mechanism for
reliable federated learning: A joint optimization approach to combining reputation and contract
theory. IEEE Internet of Things Journal, 6(6):10700-10714, 2019.
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of
fedavg on non-iid data. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=HJxNAnVtDS.
H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Areas.
Communication-efficient learning of deep networks from decentralized data. In AISTATS, 2017.
El Mahdi El Mhamdi, Rachid Guerraoui, and SebaStien Rouault. The hidden vulnerability of
distributed learning in byzantium. In Proceedings of the 35th International Conference on Machine
Learning (ICML), pp. 3521-3530, 2018. Bulyan paper.
Luis Munoz-Gonzalez, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin Wongrassamee,
Emil C. Lupu, and Fabio Roli. Towards poisoning of deep learning algorithms with back-gradient
optimization. arXiv preprint arXiv:1708.08689, 2017.
Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar. Federated multi-task
learning. In Advances in Neural Information Processing Systems, pp. 4424-4434, 2017.
11
Under review as a conference paper at ICLR 2022
Sebastian U Stich. Local sgd converges fast and communicates little. arXiv preprint arXiv:1805.09767,
2018.
Sebastian U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsified sgd with memory. arXiv
preprint arXiv:1809.07599, 2018.
Qi Xia, Zeyi Tao, Zijiang Hao, and Qun Li. Faba: An algorithm for fast aggregation against byzantine
attacks in distributed neural networks. In IJCAI,pp. 4824-4830, 2019.
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept and
applications. ACM Transactions on Intelligent Systems and Technology (TIST),10(2):1-19, 2019.
Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust distributed
learning: Towards optimal statistical rates. In International Conference on Machine Learning, pp.
5650-5659. PMLR, 2018.
Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted sgd with faster convergence and less
communication: Demystifying why model averaging works for deep learning. In Proceedings of
the AAAI Conference on Artificial Intelligence, volume 33, pp. 5693-5700, 2019.
Chengliang Zhang, Suyi Li, Junzhe Xia, Wei Wang, Feng Yan, and Yang Liu. Batchcrypt: Efficient
homomorphic encryption for cross-silo federated learning. In 2020 {USENIX} Annual Technical
Conference ({USENIX}{ATC} 20), pp. 493-506, 2020.
Yuchen Zhang, John C Duchi, and Martin J Wainwright. Communication-efficient algorithms for
statistical optimization. In 2012 IEEE 51st IEEE Conference on Decision and Control (CDC), pp.
6792-6792. IEEE, 2012.
12
Under review as a conference paper at ICLR 2022
A Appendix
A. 1 Penalty and reward value
Penalty and reward selection. Our design policy penalizes 2cmax out of m clients in every iteration.
Considering a completely benign scenario, we want the expected value of the reputation score of
a client that has been penalized e fraction of times to be zero, where e = 2cmαx. Let a client i be
penalized en number of times in n iterations. There are enn ways to select the iterations where the
client is penalized. After n iterations, the reputation score of client i is given by:
n
RS(i,n) = X μn-tW (i,t).	⑵
t=0
where W(i, t) is a sequence of penalty and reward over time. The expected value of this reputation
score over all possible sequences j ∈ enn is
E[RS(i,n)] =焉 X RS(i,n)
j	en j
=6 XX μn-tw (i,t)
en j t
=6 X μn-t X W (i,t)
en t	j
=(enjX μn-t (Tpeo)+Einc))
Our setting with r = 2cmax and p = 1 - r makes the above quantity to be zero thus ensuring that its
expected reputation score increment is zero. This proof assumes that it is a random process through
which (benign) clients generate their flip scores. Thus, if a subset of clients are penalized less than
2cmax of times, they are expected to have a net neutral reputation score.
Upper and lower bound of reputation score. From the above expression, it is obvious that if
μd = 0, -p ≤ RS ≤ r. When 0 < μd < 1, the upper and lower bounds can be computed by
assuming that a client was rewarded or penalized respectively in every iteration. Assuming that the
number of iterations tends towards infinity, equation (1) forms an infinite geometric sequence, that
can be solved to obtain i-p^ ≤ RS ≤ 1⅛. It should be noted that these reputation scores are
normalized using softmax to compute the reputation weights. If the absolute value of the lower bound
is not large enough (if μd is set to be too small), then even after perfect detection, a malicious client
can still have a significant reputation weight after softmax normalization. If μd is set to a value closer
to 1, then the absolute value of the lower and upper bounds increase, bringing down the contribution
of malicious clients to almost zero. At the same time, redemption becomes difficult for a client in
this case. This trade-off needs to be kept in mind when setting the decay parameter. We have used
μd = 0.99 in our experiments in order to remain conservative and make recovery difficult for a client
that has been penalized a lot of times. However, this is a design parameter that the user can decide.
A.2 Label Flipping
The attack that we target, “the directed-deviation attack” has been shown to be the most powerful
attack in federated learning Fang et al. (2020), and specifically claims to be more effective than
state-of-the-art untargeted data poisoning attacks for multi-class classifiers, that is, label flipping
attack, Gaussian attack, and back-gradient optimization based attacks Munoz-Gonzalez et al. (2017).
They show that the existing data poisoning attacks are insufficient and cannot produce a high testing
error rate, not higher than 0.2 in the presence of byzantine-robust aggregation techniques (Krum,
trimmed mean, and median).
We observe that both state-of-the-art targeted and untargted label flipping attacks are not powerful
enough on the CIFAR-10 and FEMNIST datasets and have neglible damage. The attacks do have some
13
Under review as a conference paper at ICLR 2022
Targeted label flipping attack (,l'->,7,) on MNIST with m=100
0.2
0.2
---- Trimmed mean, targeted flip, c=20
Tesseract, targeted-flip, c=20
500	750 IOOO 1250 1500 1750 2000
Iterations
Figure 6: TESSERACT’s Performance against the targeted and untargeted label flipping attacks on the
MNIST dataset. We observe that the attacks have some damage on the model, but Tesseract is able to remedy
this for both attacks.
Figure 7: Figure (a) shows the performance of TESSERACT on MNIST with increasing c. We see that
Tesseract is stable across a large range and breaks only above c = 0.45 which is close to the theoretical
limit of c = 0.5-. Figures (b) and (c) show the test accuracy of TESSERACT on MNIST dataset distributed
with varying non-IID bias across 100 clients in benign and malicious cases respectively. TESSERACT can be
seen to be robust enough for a wide range of bias, 0.1-0.8, with a small dip in test accuracy occurring at
bias = 0.9.
damaging impact on the MNIST dataset, but when Tesseract is used, the damage is completely
mitigated. Thus, we verify the claims from Fang et al. (2020) and show that Tesseract’s intuition
is general enough to counteract both the more powerful “directed-deviation attacks” and the weaker
state-of-the-art data poisoning attacks.
A.3 Additional experiments
Here, we provide additional evaluation of Tesseract in two specific situations. We stress-test it
first by subjecting it to a higher number of malicious clients to find the breaking point of Tesseract,
when trained on MNIST dataset in the presence of Full-trim attack. We assume that the number
of compromised clients is still not greater than cmax , and to that end, we set c = cmax . Since
TESSERACT requires Cmax < m, we have swept C UPto 49 where m was fixed at 100. We observe in
Figure 7(a) that TESSERACT is stable upto c/m = 0.45 whereas the rest of the defense techniques
broke below C/m = 0.30 as can be seen in Table 2 and Figure 3 with C = Cmax set for all the defense
techniques that require a knowledge of Cmax .
Figures 7(b) and (c) show the performance of Tesseract on MNIST dataset distributed among
100 clients with varying degrees of non-IIDness. We observe that, except for the extreme case of
bias = 0.9, TESSERACT remains exceptionally stable.
Here, we describe the mathematical formulation of the adaptive-attacks. Full-Krum attack finds a
vector of gradients u by solving an optimization problem described in Fang et al. (2020), and every
malicious client i would send u with an additional noise to appear different. Full-Trim attack solves a
different optimization problem to also come up with a vector of gradients u to which every malicious
14
Under review as a conference paper at ICLR 2022
client i would add some noise to obtain ui . The problem statement in our (two) derived versions of
the above (two) attacks, namely Adaptive-Trim and Adaptive-Krum, is To find a set of vectors
vi,i = 0,1,2,...,c- 1
where c is the number of malicious clients. Here, vi is the vector of gradients with size equal to the
number of model parameters, each satisfying the constraint -
FS(vi) < F Shigh(t)
that is, every computed vector should have a flip-score lower than the cut-off flip-score according to
the adversary’s knowledge, such that
c-1	c-1
vi =	ui
i	i=0
where ui were determined by the adversary originally as a valid solution to the Full-Krum and
Full-Trim optimization problems. We solve this problem as described in Section 6.2. In short, we
initialize vi to some target value, and then undo the attack on “less important parameters” until the
flip-score constraint is just met, and send the computed vi for aggregation. v0 is initialized to u0 ,
and then updated until the flip-score constraint is satisfied. The difference u0 - v0 is added to u1
which now becomes the initial value of v1 and so on. The results have been described and analyzed
in Section 6.2.
We formulate an even stronger attack where the adversary also has the knowledge of its own
reputation score (WR) in order to come up with attacked gradients with better chances of success.
We call this a “Weighted-Adaptive-Trim” attack. The modified constraint is
c-1	c-1
WR,ivi =	WR,iui
i	i=0
Tesseract successfully defends against this attack when evaluated on MNIST, as can be seen from
the experimental result in Figure 8. With Tesseract, the test accuracy reaches 90% while in the
baseline case, it only reaches 58%. For context, without any attack, the model reaches accuracy of
92.45%.
Figure 9 shows the devastating impact of Full-Krum attack on Krum aggregation with the Shakespeare
NLP dataset. This was not shown in Figure 3(a) because the abnormally high loss values makes the
comparison between benchmarks with smaller loss values difficult. All defenses partially mitigate
the impact of this attack, but the greatest benefit is obtained from Tesseract.
A.4 Convergence Analysis
Let the k-th client hold nk training databatches: χk,ι, ...Xk,nk. The local objective function LMk(∙)
is given by
1 nk
LMk(W) = 一 El(w；xk,j),
nk
j=1
where /(•;•) is the specified loss function for each client.
The global objective function is defined as
m
GMk,t (w) = Xpk,tLMk(w).
k=1
The global model is updated as
m
wtk+1 = wtk - ηt X pk,tOLMk(wtk),
k=1
where pk,t = sof tmax(RSk,t) is the softmax of reputation score of client k at time t.
We update the weights by averaging the weights from selected clients Wt = Pm=I Pk,twk. For
convenience, we also define gt = Pm=I PkNLMk(Wk,ξk), where ξk is the selected local data.
15
Under review as a conference paper at ICLR 2022
Weighted-Adaptive-Trim attack on DNN+MNIST
θ-0- .	.	.	， l ，
0	100	200	300	400	500
Iterations
Figure 8: The figure shows the test accuracy of
Tesseract when evaluated on MNIST dataset under
the default conditions with m = 100, c = 20 where
the adversary launches the Weighted-Adaptive-Trim
attack on the system, compared with the baseline
Full-krum attack on GRU trained on Shakespeare
5-
6	250	500	750 1000 1250 1500 1750 2000
Iteration
Figure 9: The figure shows the impact of the
Full-Krum attack on Krum aggregation on the
Shakespeare NLP dataset as compared to the other
aggregation techniques. It has a devastating impact
performance of FedSGD against the Full-Trim attack. which is mitigated by all defenses and most effectively
Tesseract successfully defends against this attack	by Tesseract as shown in Figure 3(a).
to achieve a 90% accuracy.
A.4. 1 Analysis on consecutive steps
To bound the expectation of the global objective function at time T from its optimal value, we first
consider to analyze the global weight from the optimal weights by calculating single step SGD:
kwt+ι - w*k2 = kwt - ηtgt - w* - ηtgt + ηtgtk2	(3)
=kwt - w* - ηtgtk2 + 2ηthwt - w* - ηtgt,gt - gti + n2kgt - gtk2.
The first term of Equation. 3 can be expressed as
kwt - w*	-	ηtgtk2	=	kwt	- w*k2	-	2ηthwt	- w*,@〉+ η2kgtk2.	(4)
The second term of Equation. 4 can be expressed as
m
-2ηt(wt - w*,gti = - 2ηt XPk,t<wt - w*, VLMk(Wk))
k=1
m
=-2ηt XPk,thwt - wk, VLMk(Wk))	(5)
k=1
m
- 2ηt Xpk,thwtk - w*,VLMk(wtk)i.
k=1
By Cauchy-Schwarz inequality and AM-GM inequality, we have
m
-2ηt XPk,thwt - Wk, VLMk(wt )i ≤ kwt - Wkk2 + ηtIlVLMk(Wk)k2.	(6)
k=1	t	t	ηt	t	t
By the μ-strong convexity of LMk(∙), We have
m
-2ηt XPk,thWk - w*, VLMk(Wk)) ≤ -(LMk(Wk)- LMk(W*)) - 2∣∣wk - w*『.⑺
k=1
16
Under review as a conference paper at ICLR 2022
By the convexity of ∣∣ ∙ ∣∣ and the L-Smoothness of LMk(∙), We can express third term of Equation. 4
as
m
η2kgtk2 ≤ η2 XPk,tkyLMk(wk)k2
k=1
m
≤ 2Lη2 XPk,t(LMk(Wf) - LMQ
k=1
(8)
Combining Equations. 4 - 8, We have
m
∣Wt - W* - ηtgtk2 ≤∣Wt - w*k2 +2Lη2 XPk,t(LMk(Wk) - LMk)
k=1
m
+ ηtXPk,t(—kwt - Wkk2 + ηtk^LMk(wf)∣2)
k=1 ηt
m
-2ηt XPk,t((LMk(Wk) - LMk(Wk)) + 2l∣wk - w*k2)
k=1
m
=(I - 〃nt)kWt - w*∣2 + XPk,tkWt - Wkk2
k=1
mm
+ 2Lη2 X Pk,t(LMk (Wk) - LMk)+ η2 X Pk,tk^LMk(wkk )∣2	(9)
k=1	k=1
m
- 2ηt X pk,t(LMk(Wtk) - LMk (Wk))
k=1
m
≤(1 - 4ηt)kw t- w*k2 + X Pk,tkw t- Wkk2
k=1
m
+4Lηt2Xpk,t(LMk(Wtk) -LMkk)
k=1
m
- 2ηt X pk,t(LMk(Wtk) - LMk(Wk)),
k=1
where we use the L-smoothness of LMk(∙) in the last inequality.
17
Under review as a conference paper at ICLR 2022
We use γt = 2ηt(1 - 2Lηt), and the last two terms of Equation. 9 are
mm
4Lηt XPk,t(LMk(Wk) — LMk)- 2ηt XPk,t(LMk(Wk)- LMk(w*))
k=1	k=1
mm
-	γt X pk,t(LMk(Wtk) -GMk) -γtXpk,t(GMk -LMkk)
k=1	k=1
m
+ 2ηt X pk,t(LMk (Wk ) - LMkk )
k=1
mm
-	γt X pk,t(LMk(Wtk) - GMk) -γtXpk,t(GMk -LMkk)	(10)
k=1	k=1
m
+	2ηt X pk,t(GM k -LMkk)
k=1
mm
- γt X pk,t(LMk(Wtk) -GMk)+(2ηt-γt)Xpk,t(GMk -LMkk)
k=1	k=1
m
- γt X pk,t(LMk(Wtk) - GMk) + 4Lηt2Γ,
k=1
whereΓ=Pkm=1pk,t(GMk -LMkk) =GMk - Pkm=1 pk,tLMkk.
The first term of Equation. 10
m
X pk,t(LMk(Wtk) - GMk)
k=1
mm
XPk,t(LMk(Wlt) - LMk(Wt)) + XPk,t(LMk(Wt) - GMk)
k=1	k=1
mm
≥ XPk,t"LMk(Wt), Wk - Wt)i + XPk,t(LMk(Wt) - gmk)
k=1	k=1
m
=X Pk,thVLMk (W t), Wk — W t)i + GM (W t) — GM *
k=1
1m	1
≥ — ɔ fpk,t(ηt∣∣LMk (W t)k2 +	∣∣Wk — W tk2) + GM (W t) — GM
2 k=1	ηt	t
m1
≥ — XPk,t(ηtL(LMk(Wt)- LMk) + 1-∣Wk — Wtk2) + GM(Wt)- GM*,
k=1	k	2ηt	t
(11)
where the first inequality results from the convexity of LMk (∙), the second inequality from AM-GM
inequality and the third inequality from L-smoothness of LMk(∙).
18
Under review as a conference paper at ICLR 2022
Therefore, Equation. 10 becomes
m
-Yt XPk,t(LMk(Wt) - GM*) + 4Lη2Γ
k=1
m1
≤Yt(Xpk,t5tL(LMk(Wt) - LMk) + 厂kwt - Wtk ))
k=1	k	2ηt	t
-Yt(GM(Wt) - GM*)+4Lη2Γ
m
=Yt(Xpt,t(ηtL(LMk(Wt) - GM*) + 5—Ilwk - Wtk2))
k=1	2ηt	t
+ YtntLr - γt(GM(Wt) - GM*) + 4LηtΓ
m
=Yt(ntL - 1) XPk,t(LMk(Wt) - GM*)
k=1
m
+ γr XPk,tkWk - Wtk2 + (4Lnt + YtntL)γ,
2ηt k=1	t	t
(12)
With GM(Wt) 一 GM* > 0 and ntL 一 1 < 0, we have
m
Yt(ntL — 1) XPk,t(LMk(Wt)- GM*) ≤ 0,	(13)
k=1
and recall Yt = 2nt(1 一 2Lnt), so 2t ≤ 1 and 4Ln2 + YtntL ≤ 6Ln2.
Therefore,
mm
-Yt XPk,t(LMk(Wk) - gm*) + 4Lnltr ≤ Xpk,t∣IWk - Wtk2 + 6Ln2r.	(14)
k=1	k=1
Thus, Equation. 9 becomes
m
IlWt - W* — ntgtk2 ≤ (1 一 μnt)kWt - W*k2 + 2XPk,tkWk — Wtk2 + 6Ln2r.	(15)
k=1
A.5 B ound for variance of gradients
Next, to bound the gradient, using assumption 3, we have
m
Ekgt- gtk2 = Ek XPk,t(NLMk(Wk,ξk) - VLMk(Wk))k2
k=1
m
= X p2k,tEkVLMk(Wtk, ξtk) - VLMk(Wtk)k2	(16)
k=1
m
≤ Xp2k,tσk2 .
19
Under review as a conference paper at ICLR 2022
A.5.1 Bound for divergence of weights
Based on Assumption 5, for malicious clients k = 1, 2, . . . , c, we have
pk,t = sof tmax(RSkt m )
eRSktm
∑i=ι RSt
RS t-M -δm
e km m
PcT eRsi-M-δm + Pm +]eRsi-M+δb
RSt-M
e km
(17)
=PcT eRSt-M + Pm +]eRSt-M+δb+δm
RS t-M
e km
≤ Pc=I eRSt-M + Pi=c+1 eRSt-M
= pk,t-M .
To bound the weights, we assume within E communication steps, there exists t0 < t, such that
t - to ≤ E - 1 and wk0 = Wt0 for all k = 1,2,...,m. And We know ηt is non-increasing and
ηt0 ≤ 2ηt . With the fact EkX - EX k2 ≤ EkX k2 and Jensen inequality, we have
mm
E X Pk,tkw t- Wkk2 ≤e X Pk,tkw to - Wk k2
k=1	k=1
m	t-1
≤ X pk,tE X(E - 1)ηt2kLMk(wtk, ξtk)k2
k=1	t0
m	t-1
≤Xpk,tEX(E- 1)ηt20G2
k=1	t0
m
≤ X pk,tE(E - 1)2ηt20G2
k=1
c

pk,tE(E - 1)2ηt20G2+	pk,tE(E - 1)2ηt20G2
k=1
c
k=c+1
≤	pk,tE(E - 1)2ηt20G2+4ηt2(E-1)2G2
k=1
c
≤4 X pk,tηt2(E - 1)2G2 + 4ηt2(E - 1)2G2
k=1
c
≤4 X pk,0ηt2(E - 1)2G2 + 4ηt2(E - 1)2G2,
k=1
where pk,0 is the initial probability of kth malicious client.
A.5.2 Convergence bound
Combining Equation.(3)(15)(16)(18), we have
kwt+ι - w*k2 =kwt - w* -ηtgtk2 + 2ηthwt - w* -ηtgt,gt -gti + η2kgt -gtk2
m
≤(I- μηt)∣wt - w*k2 + 2Xpk,tkwk - Wtk2 + 6Lη2r
k=1
+ 2ηt(wt - w* - ηtgt,gt - gti + η2kgt - gtk2.
(18)
(19)
20
k三)I ZEf +PZEJ
(IZ) 1 1 工0m% il + 2
之< + 2) ÷!<--+ 2 (!<--+ 2 I T)S
g-¾二工0
a* + Z= *M L£a‰7l I)VIZ = *MI 1+彦=出
OAEq əM
əuɪos joJ SPoq UOISnl。U0。a∙sOUInSSV ∙M⅛VlZ = *M — ɪɪ.一出s∙ssaɪnsuə P JO Uo=-sgəp əip *⅛J∙fc
.K茗 aH + J79 + ZDZ(I — 山∞+ ZDZ(I — 巴。w8 = 2
PUeC一 * M — 1仪一一国(I + £ EmU = O OJaqM ⅛vlz = *M —度酉 əAOJd2UiBM OM
向+音vl⅛z PUe 市=(川 ~∕yuμuVlIhaw qons 3 Λ L PUe + A 0 əuɪos joJ 唯=⅛Iəs OM
(国
-τy飞 K + J79 +¾Z(I — 山∞+¾Z(I — 巴。wdκ∞飞 十
B U
Z = *M—£&‰7l IT
I = W
T⅛M* + H59 +
UL
I=W
ZF(Il 巴ɪ+ ZF(Il 巴gwdM8 + Z = *M —彦=a⅛工 — ɪ)vl
U
Fd—=国 +〈橙—苣二A— *M — 盲〉鬼与 十
I = W
J79+一盲—=M3国C+一 *M—馍=国(‰7l I)一*M— 1+宦=国
UI
*0¾JaqI * = 一«出 əo,ss
ZZOZ xu0IaJOdEd əɔuəjəjuoɔ E SE m-aəj Jopun
IZ
pəAəɔE STəɔ XAJωJOIIuωpnLO M0Un EXlφdo ətp UlalJ AEME
Rpmn Mq昌qsnd IJBlSaououaəpipəiəəp ətp qounAJBSjəAPE UE JoJ Aseq S 二』
uPEJU9q əJo 9ILΦS9 9UIPωJddE UE qM ∙( SIUPEuuaq əJo əwɪsə QUIPωJddE
UE oqSIUPEJ8 5plgJ0Jgq UMo ssəumsSE Inq CSlUO=O uuoq ətp Jo SIUPEJ8 03 MoUa
pəəu -OU səop AJBSJOAPE ətp 乂 0UE qons)UUlBP ənb oq UEɔ 5pəpəɪMoUa WEd
ətp uəAə3 UMoqS SEqOM JJd CJOqJJnH ∙5pəpəɪMoUIInJ E snsuoo SIUPEJ8
sno=Eul QqnjEJOJQPJOSIUPEJuQq ətpssəɔɔEJQAJəs ətp əuɪojduɪoɔlɔəp
Joe jə AJəs ətp PUE SlUɪɔ uQq uəəʌuəg uonuππoo ətp ldəɔjəlu* p8OMləu ətp pəur
AEH səs9dnu! jəpunEətp Uf Jo səɔ-Aəpd=JBd əuɪojduɪoɔ jəwənoɔ
JyoUE əjəh .ssəɔojdEJl əəpejpsno=EUl aq p00 QJaqlls əmuəs E Ul
.səpou JOSUəs 9dmπ UIOjJEP ωJIBUəɔs SUnBJ IJBUlS E
jo SJOSUəsdnu! UlOJJ Ep JOSUəsAqJBU90s PI9ylq EddEUICSE qons CPəujb
uq S 二 əpouɪ ɪŋɔkɔ E əjə^muləsIS—SSOJO E Jo əɔ-Aəplssojɔ E oq p0M əsEɔləsn ɪŋɔɪd<
s<3—s∩ Whsuzou I ∙9∙ V
SlNHnnOU<NOIlIααv 9∙v
I=W I=W 1 ⅛
.(z=3— s=m+°wdM¾8+¾8 +J79+ΞdM)^- ∙中Vl*JVDI-(MM)JVD-SI
OAEq əM SnqI
⅛*VIZ = *MI 盲=3⅛Vl*JVol 一(盲)JVa3r(∙)JVoJOSSəuwoouɪs,vəw'm
I + l + 2 —
--------------------------------V
Under review as a conference paper at ICLR 2022
A.6.2 Generalizability of the defense
We have demonstrated that Tesseract achieves the goal of secure aggregation in the presence
of directed-deviation attacks, their adaptive-white-box versions, as well targeted and untargeted
label-flipping attacks. This is because the fundamental intuition behind Tesseract is based on
the assumption that a large flip-score is indicative of an attack. It should be noted that the primary
objective of any untargeted model poisoning attack is to push the global model away from the optima.
Given the currently aggregated global model is benign, an attack will invariably push the model
parameters in the opposite direction as the benign gradients, and would consequently generate a large
flip-score and get flagged. If the current global model itself had a poisoned iteration, the attacked
gradients will support the current direction in which the model was moving, and would generate low
flip-scores while the benign gradients now would generate high values of flip-score, as demonstrated
in Figure 2. Since, we trim out both the ends of the flip-score spectrum in a relative manner, we
provide a mechanism to defend against any untargeted attack.
A.6.3 Accuracy of baseline FedSGD model
As in other federated learning-related security papers Fang et al. (2020); Fung et al. (2020); Bag-
dasaryan et al. (2020), our goal was not necessarily to achieve the smallest error rates for the protocols
on the considered datasets as our goal was not to search for the most optimized DNN architecture.
Instead, our goal was to demonstrate that the state-of-the-art attacks can increase the testing error
rates of the learned DNN classifiers and Tesseract can reverse the attacks better than the other
defense algorithms. Since all baselines and our solution are evaluated on top of the same trained
model, therefore we have a valid comparison base. Of course, we want DNNs to be reasonably
performant as ours are.
22