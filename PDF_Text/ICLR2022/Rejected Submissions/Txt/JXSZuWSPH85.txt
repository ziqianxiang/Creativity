Under review as a conference paper at ICLR 2022
Deep Inverse Reinforcement Learning via Ad-
versarial One-Class Classification
Anonymous authors
Paper under double-blind review
Ab stract
Traditional inverse reinforcement learning (IRL) methods require a loop to find
the optimal policy for each reward update (called an inner loop), resulting in very
time-consuming reward estimation. In contrast, classification-based IRL meth-
ods, which have been studied recently, do not require an inner loop and estimate
rewards quickly, although it is difficult to prepare an appropriate baseline corre-
sponding to the expert trajectory. In this study, we introduced adversarial one-
class classification into the classification-based IRL framework, and consequently
developed a novel IRL method that requires only expert trajectories. We experi-
mentally verified that the developed method can achieve the same performance as
existing methods.
1 Introduction
Inverse reinforcement learning (IRL) (Russell, 1998) refers to the problem of estimating rewards for
reinforcement learning (RL) agents to acquire policies that can reproduce expert behavior. An RL
algorithm learns a policy that maximizes the cumulative discounted reward under a given reward
function. An IRL algorithm does the opposite; it estimates the reward from the given policies or
trajectories to satisfy the condition under the assumption that the expert is maximizing the reward.
IRL has been applied in two main areas (Ramachandran & Amir, 2007). The first is apprenticeship
learning, which enables the learning of complex policies for which it is difficult to design a reward
function. Compared to behavioral cloning, IRL is robust to the covariate shift problem (Ross et al.,
2011) and achieves superior performance even when the amount of data is small. The second is
reward learning, where IRL is used to estimate rewards from the trajectory data of human and animal
action sequences and to analyze the intention of the subject. In previous studies, IRL methods have
been used to analyze human walking paths (Kitani et al., 2012) and the behavior of nematodes
(Yamaguchi et al., 2018).
In traditional IRL methods, the IRL loop has an inner loop that computes the optimal policy for
the reward being estimated until convergence. This inner loop presents a difficulty in applying
IRL to tasks with a large state-action space because it is computation-intensive. As a solution to
this, classification-based IRL methods transform the IRL problem into a problem of classifying the
expert’s trajectory and the trajectory to be compared. Notable methods include AIRL (Fu et al.,
2017), LogReg-IRL (Uchibe, 2018), and T-REX (Brown et al., 2019).
These methods differ in the ways they are formulated, but they result in similar learning methods.
Online methods, such as AIRL, collect the trajectories to be compared from the environment. Con-
trastingly, offline methods, such as LogReg-IRL and T-REX, collect the trajectories to be compared
in advance, which enables them to further speed up and stabilize learning by not requiring access
to the environment during training. However, the learning performance of current offline methods
depends heavily on the properties of the trajectories to be compared or the ranking of the trajectories,
which is difficult to collect.
In this study, we exploited the fact that the learning process of LogReg-IRL by binary classification
is equivalent to that of a discriminator in adversarial learning, such as with generative adversarial
networks (GANs) (Goodfellow et al., 2014). Specifically, we developed an innovative deep IRL
method, called state-only learned one-class classifier for IRL (SOLO-IRL), in which binary clas-
sification is replaced with adversarial one-class classification. Figure 1 compares the traditional
1
Under review as a conference paper at ICLR 2022
Expert trajectory
(a) Traditional IRL
(b) SOLO-IRL
Figure 1: Comparison of traditional IRL and the proposed SOLO-IRL.
and proposed IRL methods. The proposed method does not require an inner loop and is an offline
method; thus, it can be trained extremely fast. In addition, it does not require that trajectories be
compared. With these advantages, the proposed method greatly advances the application of IRL
methods to real-world problems.
2 Preliminaries
2.1	Markov decision process (MDP)
RL is a learning problem based on the Markov decision process (MDP). The MDP consists of a
tuple M = hS, A, P, R, γi, where S is the state space, A is the action space, P is the state-transition
probability, R is the reward function, and γ is the discount factor indicating the degree of importance
for future rewards. In the MDP, the state-value function for state st at time t is represented by the
Bellman equation, as follows:
)
V (st) =
max R(st, a) +
a
Ep(Sist ,a)γV(SO)
s0
(1)
where R(st,	at ) is the reward for taking action at in state st and p(st+1|st,	at ) is the probability of
transitioning to the next state st+1 when taking action at in state st .
2.2	Linearly solvable MDP (LMDP)
The linearly solvable MDP (LMDP) is an extension of the MDP in which the agent directly deter-
mines the transition probability u(st+1|st ) from the current state st to the next state st+1 as the
control, instead of the action at in the MDP. Then, the Bellman equation is linearized under two
assumptions. First, the state-transition probability p(st+1|st,	u) is assumed to be expressed as the
product of the uncontrolled transition probability p(st+1 ∣st) and U as follows:
P (st+1 ∣st,u(st+ι ∣st))=双st+1 ∣st)exp {u(st+ι ∣st)}	(2)
The uncontrolled transition probability p)(st+1 ∣st) indicates a transitional relationship between the
states in the environment. When a transition is impossible, i.e., / = 0, then p = 0.
The second assumption is that the reward R(st, u) is composed of a state-dependent reward r(st)
and penalty term DKL (p∣∣p)) for state-transition probability p over the divergence from the uncon-
trolled transition probability p). This assumption can be formulated as follows:
R(st,	u(st+1∣st)) = r(st) - DKL (p(st+1∣st,	u(st+1∣st))∣∣p)(st+1∣st))	(3)
2
Under review as a conference paper at ICLR 2022
where DKL(Px ||Py) represents the KUllback-Leibler (KL) divergence of Px and Py. By rearranging
Eq. (3) according to the definition of the KL divergence, the following equation is obtained:
R (st,u(st+1|st)) = r(st) -	p(s0|st, u(s0|st))u(s0|st)	(4)
s0
SUbstitUting Eq. (4) into the Bellman eqUation in Eq. (1) gives the following:
V (st) = r(st) + max
u
s0|st,u(s0|st)) -u(s0|st) +γV(s0)
(5)
Eq. (2) is then sUbstitUted into Eq. (5) and the Lagrange mUltiplier applied with s0 p(s0|st, u) = 1
as a constraint. Finally, the max operator is removed, resUlting in the linear Bellman eqUation as
follows:
exp{V(st)} = exp{r(st)} Xp(s0∣st)exp{γV(s0)}	(6)
s0
The optimal control U in the LMDP is given by
U*(st+1∣st)
p(st+ι∣st)exp{γV (st+ι)}
Ps0 p(s0∣st )exp{γV (s0)}
(7)
2.3	Logistic Regression-Based IRL (LogReg-IRL)
LogReg-IRL (Uchibe, 2018) is a deep IRL method in the LMDP. The following is an overview of
the IRL framework in LogReg-IRL. By rearranging the linear Bellman eqUation in Eq. (6), the
following is obtained:
exp{V(St)- r(st)} = Ep(Slst) exp{γV(s0)}
s0
(8)
Then, sUbstitUting Eq. (8) into Eq. (7) and rearranging the resUlt gives
U*(st+1∣st)
p(st+ι∣st)exp{γV (st+ι)}
exp{V(st) - r(st)}
U*(st+1∣st)
^p(st+ιlst)
1 U*(st+1∣st)
°g p5(st+i|st)
exp{r(st) + γV(st+1) - V(st)}
r(st) +γV(st+1) - V(st)
(9)
Applying Bayes’ theorem to Eq. (9) we obtain
log Ug^ =log ¾(⅛ + MSt) + YV(St+1) - V(Stt
(10)
The left-hand side and the first term on the right-hand side of Eq. (10) are the density-ratios. The
density-ratio pa/pb can be estimated by assigning the label η = 1 to the samples from the prob-
ability distribUtion pa, assigning η = -1 to the samples from pb, and training a classifier Using
logistic regression (Qin, 1998; Cheng et al., 2004; Bickel et al., 2007). First, by Bayes’ theorem, the
following is obtained:
Pa(x)
Pb(x)
pa(x)
log pb(X)
p(η = 1|x) p(η = -1)
p(η = -1|x) p(η = 1)
p(η = 1|x)	p(η = -1)
ogp(η = -i∣χ) + og p(η = 1)
(11)
Next, the first discriminator D1 (χ) is defined by the sigmoid fUnction σ(χ) = 1/{1 + exp(-χ)}
and a neUral network f (χ):
Dι(x) = p(η = 1|x) = σ(f(x))	(12)
3
Under review as a conference paper at ICLR 2022
where the second term on the right-hand side of Eq. (11) can be approximated by calculating the
sample number ratio Npa /Npb and taking its logarithm. For the first term, the following equation
can be obtained from the definition of the discriminator in Eq. (12):
p(η = 1|x)	D1 (x)
Og p(η = -1|x) = Og 1 - Dι(x)
=log 1+eχpf (X)}
1 1 +exp{-f(x)}
= lOg eχp{f (x)}
= f(x)	(13)
From Eq. (13), when Npa = Npb, the following holds:
pa(x)
log PbM = f(X
(14)
Therefore, the density-ratio of the first term in Eq. (10) can be estimated by sampling the states
St 〜T* and St 〜T from the expert trajectory T* according to the optimal control U and the
baseline trajectory T according to the uncontrolled transition probability p, followed by training
with the following cross-entropy loss:
LI(DI) = -EMt〜τ[log(1 - DI(St))] - Es*^τ* [log(Dι(s*))]	(15)
The density-ratio on the left-hand side of Eq. (10) is defined as follows using the trained f (x),
reward-estimating neural network r(x), and state-value-estimating neural network V(x):
log u7st,st+1) = f (st) + r(st) + γV(st+ι) - V(St)	(16)
pS(st, st+1)
The second discriminator D2 for the state-transition pair is defined as
D2(x,y) = σ(f (x) + r(x) + γV(y) - V(X))	(17)
As with D1, the discriminator D2 is trained by cross-entropy loss L2, given as
L2(D2) = -E(品再+1)〜T [log(1 - D2(st, St+1))] - E(s*,s*+ι)〜τ* [log(D2(s*, s*+ι))]	(18)
In the original LogReg-IRL, an L2 regularization term is added to the loss function. Following the
process described above, LogReg-IRL estimates the reward and state-value by classifying the expert
and baseline trajectories. Unlike traditional IRL methods, LogReg-IRL does not require RL in the
reward estimation process and, thus, it can be trained very quickly.
2.4 Difficulty collecting baseline trajectories
LogReg-IRL showed that IRL in LMDP can be formulated by learning two discriminators. How-
ever, LogReg-IRL has a problem in that its learning performance is greatly affected by the baseline
trajectory. For the baseline trajectory, it is desirable to collect data that follow uncontrolled transition
probability pS, such as trajectories obtained under a random policy, for a wide range of states.
However, for some tasks, the number of states that can be transitioned by a random policy may
be limited. For example, in a game task, such as an Atari game, the game does not progress,
and in a driving simulator task such as TORCS (Wymann et al., 2000), the car crashes into a wall
immediately. In such cases, data according to a random policy cannot cover a wide range of states.
Several methods have been proposed to collect the baseline trajectory in LogReg-IRL. For Atari
games, a method using state-transition pairs from random policy in any state in the expert trajectory
was proposed (Uchibe, 2018). For TORCS, a method using the trajectory recorded by driving with
noise added to the action output of the expert agent was proposed (Kishikawa & Arai, 2021). How-
ever, those proposed methods are task-specific, and there is no well-established generalized method
for collecting baseline trajectories.
An inappropriate baseline trajectory leads to inappropriate reward estimation. The density-ratios
diverge with respect to the states that the agent can reach and those for which there is no baseline
trajectory, and high rewards are estimated where there are no experts. Therefore, an agent that learns
according to the estimated reward may acquire a different action from the expert as the optimal
policy.
4
Under review as a conference paper at ICLR 2022
Noise
True
expert
data
Figure 2: Proposed SOLO-IRL.
3	State-Only Learned One-class Classifier for IRL (S OLO-IRL)
We propose the novel IRL method SOLO-IRL, which estimates the reward given only the expert.
SOLO-IRL is a combination of an IRL framework based on LMDP, a transition generator based on
adversarial one-class classification, and least-squares loss. Each of these is explained below.
3.1	Solution by adversarial one-class classification
Classification-based IRL methods are equivalent to learning a discriminator in adversarial learning
frameworks, such as GANs (Goodfellow et al., 2014), which are binary classification problems. This
means that the IRL problem can be solved by learning a discriminator that classifies the trajectory
as expert or not.
Broadening our perspective to other fields, we find that the anomaly detection problem also requires
a binary classifier to distinguish between normal and abnormal samples. However, in real prob-
lems, we often encounter situations in which we can obtain many normal samples but few abnormal
samples. Therefore, one-class classification is a method for obtaining a binary classifier using only
normal samples.
Recently, the adversarially learned one-class classifier (ALOCC) (Sabokrou et al., 2018) was pro-
posed as a one-class classification method. In the ALOCC, the discriminator is adversarially trained
with a denoising autoencoder that generates fake normal samples. Consequently, the discrimina-
tor is trained as a binary classifier that identifies normal and abnormal samples using only normal
samples.
SOLO-IRL combines a classification-based IRL method with the ALOCC. The structure of SOLO-
IRL is illustrated in Figure 2, and its algorithm is given as Algorithm 1. In the following, we describe
the learning process and features of SOLO-IRL as well as the objective function used for learning
SOLO-IRL.
3.2	Learning process of SOLO-IRL
The training of SOLO-IRL consists of two stages. In the first stage, we learn a discriminator D1
that classifies whether a state is sampled from expert data or not. Generator G1 is composed of an
encoder-decoder network Ri and generates a fake current state St from the true current state St plus
noise νι, as shown in Eq. (19). Then, the generator learns such that the generated St and St are
close, and D1 judges SSt to be the true current state.
St JRI(St + VI)	(19)
Meanwhile, discriminator D1 is defined by Eq. (12) and outputs the probability that a given state
is sampled from the expert. D1 learns to distinguish between states sampled from a true expert and
a fake expert from a generator. Here, the density-ratio representing the expertness of each state is
obtained in D1 .
5
Under review as a conference paper at ICLR 2022
In the second stage, generator G2 learns to generate a state-transition pair that is close to the sampled
expert data and that discriminator D2 judges as expert. Generator G2 uses two encoder-decoder
networks R2 and R3 to generate a fake state-transition pair (St, St+ι) from the expert,s current state
St plus noise ν2, as given by Eqs. (20) and (21). Then, the generator learns such that the generated
(St, St+ι) and (st，st+ι) are close to each other, and discriminator D2 judges the generated data to
be a true state-transition pair.
st — R2(st + ν2)	(20)
st+1 - R3(st + ν2)	(21)
The training of D2 is the same as that of D1 in the first stage. Here, discriminator D2 contains a
reward network r and state-value network V, as shown in Eq. (17). Finally, it works as an IRL
algorithm to estimate the reward and state-value.
By introducing a generator and training the discriminator in an adversarial manner, the decision
boundary around the expert is refined, and the appropriate reward is estimated. This makes prepa-
ration of the baseline trajectory by trial and error unnecessary. In addition, because SOLO-IRL is
an offline method, it learns quickly without executing RL or interacting with the environment. To
the best of our knowledge, SOLO-IRL is the only method that estimates the reward and state-value
exclusively from expert trajectories.
3.3	Least-squares loss as adversarial objective
To train the generator and discriminator, we propose using least-squares loss as an adversarial ob-
jective. The least-squares loss is represented by the following equations:
Ladv(D) = 1 Ex" [(D(x)- 1)2] + 1Eχ 〜H(D(X))2]	(22)
Ladv (G) = 2 E"a[(D(X)- 1)2]	(23)
where dt and dS denote the true and fake states or state-transition pairs, respectively. The least-
squares loss was proposed alongside LSGAN (Mao et al., 2017), which solved the learning insta-
bility and mode collapse problems of previous GANs. In SOLO-IRL, this least-squares loss is used
instead of the cross-entropy loss to address the problems of learning stability and mode collapse.
3.4	Reconstruction objective
Meanwhile, the generators G1 and G2 must be trained to be close to the true expert data in addition
to training by the adversarial objective. The reconstruction objective is trained by the least-squares
loss given by the following equations, as in the original ALOCC:
Lrec (GI)	=	USt -RI(St + ν1)ll2	(24)
Lrec (G2)	=	USt- R2 (St + ν2)"2	+ ||s；+1	- R3(st +	ν2)"2	(25)
Finally, the objective of discriminators D1 and D2 becomes Ladv (D), and the objective of gener-
ators G1 and G2 becomes Ladv (G) + Lrec (G). Using these objectives, SOLO-IRL estimates the
reward and state-value by training each neural network using the gradient descent method.
4	Experimental results and discussion
We validated the performance of the proposed method in the OpenAI Gym environment (Brockman
et al., 2016). For the expert trajectory, we used a trajectory generated by an agent that had learned
the optimal action for the true reward as an expert. As RL algorithms, we used PPO (Schulman
et al., 2017) for the tasks with a discrete action space and TD3 (Fujimoto et al., 2018) for the tasks
with a continuous action space.
In SOLO-IRL, a three-layer multilayer perceptron was used as both the encoder and decoder in the
generator, and a three-layer multilayer perceptron was used as the discriminator. Adam (Kingma &
Ba, 2014) was used to optimize the neural network.
6
Under review as a conference paper at ICLR 2022
Algorithm 1 SOLO-IRL: State-Only Learned One-class Classifier for IRL
Require: Expert trajectories T*, discount factor γ, noise νι and ν2, numbers of iterations nι and n
Ensure: Reward network r(x), state-value network V(x)
1:	Initialize neural network f (x), R1(x)
2:	Define discriminator D1(st) = σ(f (st))
3:	for i = 0,…，nι do
4:	Sample expert state SJ= from T*
5:	St — RI(S* + VI)
6:	Train D1 according to the loss Ladv (D) in Eq. (22)
7:	Train R1 with loss Ladv (G) + Lrec(G1) in Eqs. (23) and (24)
8:	end for
9:	Initialize neural network r(x), V(x), R2(x), R3(x)
10:	Define discriminator D2(st, st+ι) = σ(f(st) + r(st) + YV(st+ι) - V(St))
11:	Disable f updates
12:	for i = 0, ∙∙∙ , n2 do
13:	Sample expert current state St* and next state St*+1 from T*
14:	St - R2 (s* + ν2)
15:	st+1 - R3 (s* + ν2)
16:	Train D2 according to the loss Ladv (D) in Eq. (22)
17:	Train R2 and R3 according to the loss Ladv(G) + Lrec (G2) in Eqs. (23) and (25)
18:	end for
Table 1: Validation results for the OpenAI Gym tasks. These scores are cumulative true rewards
(averaged over 1000 trials); the higher the better.
	CartPole	BipedalWalker	Hopper	Walker2d
Expert	499.96	321.69	3682.28	5272.42
Random	22.50	-99.61	17.87	1.87
LogReg-IRL (Uchibe, 2018)	498.42	-118.85	5.30	376.44
SOLO-IRL (ours)	449.11	226.16	1084.46	774.87
Behavioral cloning RED (Wang et al., 2019)	500.00	135.13	3677.99 3633.72	4920.64 3868.98
We compared SOLO-IRL to LogReg-IRL (Uchibe, 2018). The baseline trajectory in LogReg-IRL
was collected by running a random policy based on a uniform distribution in the environment. The
LogReg-IRL implementation was created based on the SOLO-IRL implementation with the follow-
ing changes: removal of the generator, changing of the cross-entropy loss, and addition of a weight
decay term (coefficient: 1e-3) to the loss.
We used the following four tasks for validation.
CartPole. CartPole (Gym, b) is a basic RL task in which the agent must keep a pendulum connected
to a cart upright by controlling the cart. A survival reward is given to the pendulum as long as it
remains upright.
BipedalWalker. BipedalWalker (Gym, a) is a task in which a robot with four joints learns a bipedal
walking task. The agent is required to output the velocities of its two hips and knees as actions,
taking the state given by the virtual sensor as the input. The true reward comprises a bonus based on
the speed and a penalty based on the magnitude of the action or fall.
MuJoCo. MuJoCo (Todorov et al., 2012) is an environment that collects tasks for the development
of robotics. We selected Hopper and Walker2d as examples. Hopper is a task in which a one-legged
robot moves forward, and Walker2d is one in which a two-legged robot walks. These two tasks
require more complex continuous control than BipedalWalker.
7
Under review as a conference paper at ICLR 2022
Figure 3: Visualization of the
distribution of expert trajecto-
ries (blue) and baseline trajec-
tories (black).
-1.00 -0.75 -0.50 -0.25 0.00 0.25 0.50 0.75 1.00
Velocity In X-axls direction
--0.125
--0.130
-0.135
--0.140
Figure 4: Visualization of the
reward estimated by LogReg-
IRL.
Figure 5: Visualization of the
reward estimated by SOLO-
IRL.
Analysis of results. First, in each environment, we evaluated the policy of the expert agent, the
random policy, the policy of the agent trained according to the reward estimated by LogReg-IRL
and SOLO-IRL, and the policy obtained via behavioral cloning. We also tested the random expert
distillation (RED) (Wang et al., 2019), which is a recent method of imitation learning in the MuJoCo
environment1.
There are two metrics for evaluating the performance of IRL: expected value difference (EVD)
(Levine et al., 2011) and expected cumulative reward. To calculate the EVD, we used the discount
rate. However, since the appropriate discount rate varies depending on the environment, the value
we use will also vary accordingly. Therefore, we decided to evaluate the performance using the
expected cumulative reward, which is a more common of the two.
The results are presented in Table 1. The results for CartPole show that SOLO-IRL was able to
achieve comparable with that of LogReg-IRL. Furthermore, with regard to the results for Bipedal-
Walker, we saw that LogReg-IRL failed to learn, whereas SOLO-IRL obtained a good score.
The aforementioned results can be attributed to the fact that BipedalWalker has a vast state space,
whereas CartPole has a relatively small state space. Consequently, the random policy could only
collect data near the starting point and could not provide an appropriate baseline trajectory for the
expert trajectory that progressed to the goal.
Behavioral cloning and RED generally perform better than the other methods that were used in the
comparison. However, in BipedalWalker, the proposed method showed better performance than
behavioral cloning due to the changing environment. In addition, RED requires more information
than the proposed method because of the availability of RL. Therefore, our proposed method can be
said to be superior than the traditional methods in that it can learn purely using expert trajectories.
Relationship between trajectory distribution and estimated reward in the BipedalWalker task.
Among the 24 dimensions in the BipedalWalker state, Figure 3 visualizes the distribution of the
expert and baseline trajectories for “velocity in the X-axis direction” and “angle of hip 1,” and
Figures 4 and 5 visualize the estimated reward for these two dimensions at the starting point. As can
be seen, the baseline trajectory does not cover a part of the the expert trajectory. Owing to the lack
of a baseline trajectory, LogReg-IRL’s estimation failed, yielding a meaningless reward. In contrast,
SOLO-IRL’s estimation, which was trained via generating samples near the expert, resulted in an
appropriate reward that drove the agent forward.
Behavior of adversarial and reconstruction objectives. Figures 6 and 7 illustrate the behavior
of adversarial loss and reconstruction loss during training in SOLO-IRL. With adversarial learning,
the probabilities of truthfulness of the true and fake samples converge to an equilibrium near 0.5.
The discriminator narrows the decision boundary toward the expert neighborhood through learning,
and the generator eventually succeeds in producing samples that are close to the true. The learning
success of the generator is also evident from the convergence of the reconstruction loss.
Summary of the experimental results. There is room for improving the performance of LogReg-
IRL using complex rule-based policies or by combining multiple policies to collect baseline tra-
1Since the author’s implementation of RED only supported MuJoCo, the experiments were also conducted
only on the MuJoCo task.
8
Under review as a conference paper at ICLR 2022
&=_qeqcud
O 200	400	600 BOO IOOO
Training epoch
Figure 6: Changes in the probability
that the true sample (Discriminator) and
fake sample (Generator) are true, re-
spectively, during training in the first
stage.
SSo-Uo一=ΠJau038H

200	400	600 BOO 1000
Training epoch
Figure 7: Reconstruction loss during
training in the first stage.
jectories; however, this is expected to be significantly more difficult than adjusting the noise in
SOLO-IRL. Therefore, it can be said that similar or better performance than LogReg-IRL can be
achieved more easily using SOLO-IRL.
5	Related works
IRL. A method similar to the proposed method is the AIRL, which is an extension of maximum
entropy IRL (Ziebart et al., 2008), guided cost learning (Finn et al., 2016), and generative adversarial
imitation learning (GAIL) (Ho & Ermon, 2016). The theoretical background of the aforementioned
methods is related to the proposed method in terms of entropy regularization (Chow et al., 2018).
AIRL requires access to the environment, while the proposed method does not. In addition, D-
REX (Brown et al., 2020) is similar to the proposed method in that it creates the trajectory of the
comparison target by adding noise. Since the proposed method uses adversarial learning, it can
generate more appropriate samples for comparison and does not need to learn behavioral cloning.
Imitation learning. There are several methods that have been recently proposed for imitating expert
demonstrations, such as RED, disagreement-regularized imitation learning (Brantley et al., 2019),
and O-NAIL (Arenz & Neumann, 2020). All these methods are extensions of GAIL and require
information regarding the expert’s state-action pairs. The proposed method can be used to estimate
the reward from the trajectory comprising only states.
Behavioral cloning. The simplest offline learning method to imitate expert trajectory data is behav-
ioral cloning, and (Torabi et al., 2018) is such a method that can be applied to state-only trajectories.
However, compared to IRL methods, it is difficult to deal with the issues of policy transferability
and covariate shift using such a method.
6	Conclusion
In this study, we exploited the fact that the classification-based IRL framework is equivalent to
training a discriminator in adversarial learning and developed SOLO-IRL, in which a generator is
incorporated to generate fake expert data. SOLO-IRL can be trained quickly without the need for an
inner loop and easily estimates rewards with high performance exclusively from expert trajectories,
with no need for baseline trajectories.
However, although itis simpler than collecting baseline trajectories, the noise used for reconstruction
by the generator still requires adjustment. In future work, we will develop a method that does not
require noise adjustment. We will also consider application to more advanced image-based control
tasks and the incorporation of recent advances in GANs.
9
Under review as a conference paper at ICLR 2022
Reproducibility S tatement
In Appendices, we have described the details of the hyperparameters and noise needed to reproduce
our experiments. We also attached the source code used in our experiments.
References
Oleg Arenz and Gerhard Neumann. Non-adversarial imitation learning and its connections to ad-
versarial methods. arXiv preprint arXiv:2008.03525, 2020.
Steffen Bickel, Michael Bruckner, and Tobias Scheffer. Discriminative learning for differing training
and test distributions. In Proceedings of the 24th international conference on Machine learning,
pp.81-88,2007.
Kiante Brantley, Wen Sun, and Mikael Henaff. Disagreement-regularized imitation learning. In
International Conference on Learning Representations, 2019.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Daniel Brown, Wonjoon Goo, Prabhat Nagarajan, and Scott Niekum. Extrapolating beyond sub-
optimal demonstrations via inverse reinforcement learning from observations. In International
Conference on Machine Learning, pp. 783-792. PMLR, 2019.
Daniel S Brown, Wonjoon Goo, and Scott Niekum. Better-than-demonstrator imitation learning
via automatically-ranked demonstrations. In Conference on robot learning, pp. 330-359. PMLR,
2020.
Kuang Fu Cheng, Chih-Kang Chu, et al. Semiparametric density estimation under a two-sample
density ratio model. Bernoulli, 10(4):583-604, 2004.
Yinlam Chow, Ofir Nachum, and Mohammad Ghavamzadeh. Path consistency learning in tsallis en-
tropy regularized mdps. In International Conference on Machine Learning, pp. 979-988. PMLR,
2018.
Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control
via policy optimization. In International conference on machine learning, pp. 49-58. PMLR,
2016.
Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse rein-
forcement learning. arXiv preprint arXiv:1710.11248, 2017.
Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-
critic methods. In International Conference on Machine Learning, pp. 1587-1596. PMLR, 2018.
Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint
arXiv:1406.2661, 2014.
OpenAI Gym.	BipedalWalker-v2.	https://gym.openai.com/envs/
BipedalWalker-v2/, a.
OpenAI Gym. CartPole-v1. https://gym.openai.com/envs/CartPole-v1/, b.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in neural
information processing systems, 29:4565-4573, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Daiko Kishikawa and Sachiyo Arai. Estimation of personal driving style via deep inverse reinforce-
ment learning. Artificial Life and Robotics, pp. 1-9, 2021.
10
Under review as a conference paper at ICLR 2022
Kris M Kitani, Brian D Ziebart, James Andrew Bagnell, and Martial Hebert. Activity forecasting.
In European Conference on Computer Vision, pp. 201-214. Springer, 2012.
Sergey Levine, Zoran Popovic, and Vladlen Koltun. Nonlinear inverse reinforcement learning with
gaussian processes. Advances in neural information processing systems, 24:19-27, 2011.
Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley.
Least squares generative adversarial networks. In Proceedings of the IEEE international confer-
ence on computer vision, pp. 2794-2802, 2017.
Jing Qin. Inferences for case-control and semiparametric two-sample density ratio models.
Biometrika, 85(3):619-630, 1998.
Deepak Ramachandran and Eyal Amir. Bayesian inverse reinforcement learning. In IJCAI, vol-
ume 7, pp. 2586-2591, 2007.
Stephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and StrUc-
tured prediction to no-regret online learning. In Proceedings of the fourteenth international con-
ference on artificial intelligence and statistics, pp. 627-635. JMLR Workshop and Conference
Proceedings, 2011.
Stuart Russell. Learning agents for uncertain environments. In Proceedings of the eleventh annual
conference on Computational learning theory, pp. 101-103, 1998.
Mohammad Sabokrou, Mohammad Khalooei, Mahmood Fathy, and Ehsan Adeli. Adversarially
learned one-class classifier for novelty detection. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 3379-3388, 2018.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033.
IEEE, 2012.
Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. arXiv preprint
arXiv:1805.01954, 2018.
Eiji Uchibe. Model-free deep inverse reinforcement learning by logistic regression. Neural Process-
ing Letters, 47(3):891-905, 2018.
Ruohan Wang, Carlo Ciliberto, Pierluigi Vito Amadori, and Yiannis Demiris. Random expert dis-
tillation: Imitation learning via expert policy support estimation. In International Conference on
Machine Learning, pp. 6536-6544. PMLR, 2019.
Bernhard Wymann, Eric Espie, Christophe Guionneau, Christos Dimitrakakis, Remi Coulom, and
Andrew Sumner. Torcs, the open racing car simulator. Software available at http://torcs. source-
forge. net, 4(6):2, 2000.
Shoichiro Yamaguchi, Honda Naoki, Muneki Ikeda, Yuki Tsukada, Shunji Nakano, Ikue Mori, and
Shin Ishii. Identification of animal behavioral strategies by inverse reinforcement learning. PLoS
computational biology, 14(5):e1006122, 2018.
Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse
reinforcement learning. In Aaai, volume 8, pp. 1433-1438. Chicago, IL, USA, 2008.
11
Under review as a conference paper at ICLR 2022
A Hyperparameters of RL
This section describes the main hyperparameters used in the experiments. For RL, we used the same
settings for both the training of the expert agent and the training based on IRL results.
A. 1 Hyperparameters of TD3 agent
The implementation of TD3 is based on https://github.com/chainer/chainerrl/
blob/master/examples/mujoco/reproduction/td3/train_td3.py, which is
identical except for the environment name and the values listed in Table 2.
Table 2: Some of the hyperparameters employed in training the TD3 agent.
Hyperparameter	Value
Training steps	106
Initial exploration sample size 104
Replay buffer size	106
Batch size	103
Discount rate	0.99
A.2 Hyperparameters of PPO agent
The implementation of PPO is based on https://github.com/chainer/chainerrl/
blob/master/examples/mujoco/train_ppo_gym.py, which is identical except for the
environment name and the values listed in Table 3.
Table 3: Some of the hyperparameters employed in training the PPO agent.
Hyperparameter	Value
Training steps	106
Update interval	2048
Batch size	64
Entropy coefficient	0.001
Discount rate	0.99
12
Under review as a conference paper at ICLR 2022
B IRL hyperparameters
The hyperparameters employed in training SOLO-IRL and LogReg-IRL are shown in Table 4.
Table 4: Hyperparameters employed in training IRL.
Hyperparameter	Value
Learning rate for network f	0.00004
Learning rate for network r Learning rate for network R	0.00004 0.00004
Learning rate for network R1	0.0001
Learning rate for network R2	0.0001
Learning rate for network R3	0.0001
Number of inputs and outputs for each network layer	(|S|, 1024), (1024, 1024), (1024, 1024), (1024, 1)
Probability of dropout in each layer of discriminator	0.0, 0.7, 0.7, 0.0
Probability of dropout in each layer of generator	0.0, 0.0, 0.0, 0.0
Activation function in each layer	leaky ReLU, leaky ReLU, leaky ReLU, None
Number of training epochs in the first stage	1000
Number of training epochs in the second stage	1000
Number of steps in one epoch in the first stage	100
Number of steps in one epoch in the second stage	100
Batch size	1024
Discount rate	0.99
13
Under review as a conference paper at ICLR 2022
B.1	Additive noise
For CartPole, we added noise that follows a normal distribution N (0, 0.001) to the true state. For
the MuJoCo task, we added a normal distribution N (0, 0.00001) noise to Hopper and a normal
distribution N(0, 0.1) noise to Walker2d.
For each dimension of BipedalWalker, we added either noise based on the normal distribution or
noise for the labels. The tuning results for the experiment are shown in Table 5. The “Types of
noise” column in the table lists the noise addition operations described below, and the “Parameters”
column lists the noise parameters.
・ “Normal” ∙…Sadd — s* + V, V 〜N(μ,σ2)
• “Label” … Sadd J min(max(s* + ν, 0), 1), V is randomly sampled from [-1,0,1] with
probability [pa , pb, pc]
Table 5: Additive noise types and parameters for each dimension.
Dimension in state	Types of noise	Parameters
1	Normal	N(0, 0.01)
2	Normal	N(0, 0.01)
3	Normal	N(0, 0.01)
4	Normal	N(0, 0.01)
5	Normal	N (0, 0.0001)
6	Normal	N (0, 0.0001)
7	Normal	N(0, 0.01)
8	Normal	N(0, 0.01)
9	Label	[0.1, 0.8, 0.1]
10	Normal	N (0, 0.0001)
11	Normal	N (0, 0.0001)
12	Normal	N(0, 0.01)
13	Normal	N(0, 0.01)
14	Label	[0.1, 0.8, 0.1]
15	Normal	N(0, 0.01)
16	Normal	N(0, 0.01)
17	Normal	N(0, 0.01)
18	Normal	N(0, 0.01)
19	Normal	N(0, 0.01)
20	Normal	N(0, 0.01)
21	Normal	N(0, 0.01)
22	Normal	N(0, 0.01)
23	Normal	N(0, 0.01)
24	Normal	N(0, 0.01)
14
Under review as a conference paper at ICLR 2022
B.2	Sensitivity to noise scale
To analyze the sensitivity of the learning results to the noise scale, we conducted experiments in
which we changed the standard deviation σ of the normal distribution N (0, σ2 ) used to generate the
noise. Walker2d in the MuJoCo environment was used for the experiments.
The results are shown in Figure 8. It can be seen that the noise should have a certain level of
magnitude and should not be too small or too large. The appropriate noise level is considered to be
task-dependent.
Figure 8: Relationship between noise scale and score in the Walker2d task. The X-axis represents
the standard deviation of the normal distribution (logarithmic scale), and the Y-axis represents the
RL score (sum of true rewards) based on IRL results. The bars in the graph indicate the standard
deviation of the scores.
B.3	Details of behavioral cloning
There are various implementation methods for behavioral cloning. We used supervised learning
as the simplest behavioral cloning method; specifically, recording the expert’s state and action se-
quences and using the state as input and the action as output.
More specifically, we trained a multilayer perceptron f (x; θ) for tasks with a discrete action space
using the softmax cross-entropy function shown in Eq. (26) as the loss. The number of inputs in the
network was |S |, the number of outputs was equal to that of action options. The other settings of the
network were the same as those of IRL.
L(θ) = -E(st,at)〜D [at logsoftmax(f (St; θ))]	(26)
In the evaluation, the argmax policy in Eq. (27) was used:
at = argmaxaf(st; θ)	(27)
For tasks with a continuous action space, the multilayer perceptron was trained by using the squared
loss shown in Eq. (28). The number of inputs in the network was |S |, and the number of outputs
was |A|.
L(θ) = E(st,at)〜D [(at- f (st； θ))2i	(28)
In the evaluation, the output of the network f(st; θ) was used. Note that the output values were
clipped in the defined range of the action in the environment.
B.4	Details of RED
To measure the RED scores, we used the author’s implementation https://github.com/
RuohanW/RED. We used the same hyperparameters but changed the number of trajectories to
1000.
15