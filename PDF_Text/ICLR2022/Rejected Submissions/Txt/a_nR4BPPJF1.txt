Under review as a conference paper at ICLR 2022
Blessing of Class Diversity in Pre-training
Anonymous authors
Paper under double-blind review
Ab stract
This paper presents a new statistical analysis aiming to explain the recent superior
achievements of the pre-training techniques in natural language processing (NLP).
We prove that when the classes of the pre-training task (e.g., different words in
masked language model task) are sufficiently diverse, in the sense that the least
singular value of the last linear layer in pre-training is large, then pre-training can
significantly improve the sample efficiency of downstream tasks. Inspired by our
theory, we propose a new regularization technique that targets the multi-class pre-
training: a diversity regularizer only to the last linear layer in the pre-training phase.
Our empirical results show that this technique consistently boosts the performance
of the pre-trained BERT model on different downstream tasks.
1	Introduction
Pre-training refers to training a model on a few or many tasks to help it learn parameters that can be
used in other tasks. For example, in natural language processing (NLP), one first pre-trains a complex
neural network model to predict masked words (masked language modeling), and then fine-tunes
the model on downstream tasks, e.g., sentiment analysis (Devlin et al., 2019). Recently, pre-training
technique has revolutionized natural language processing (NLP). Models based on this technique
have dramatically improve the performance for a wide range of downstream tasks (Devlin et al., 2019;
Radford et al., 2018; Yang et al., 2019; Clark et al., 2020; Lan et al., 2020; Liu et al., 2020).
Despite the large body of empirical work on pre-training, satisfactory theories are still lacking,
especially theories that can explain the success of pre-training in NLP. Existing theories often rely on
strong distributional assumptions (Lee et al., 2020), smoothness conditions (Robinson et al., 2020)
or noise-robustness conditions (Bansal et al., 2021) to relate the pre-training task(s) to downstream
tasks. These assumptions are often hard to verify.
A line of work studied multi-task pre-training (Caruana, 1997; Baxter, 2000; Maurer et al., 2016;
Du et al., 2021; Tripuraneni et al., 2020a;b; Thekumparampil et al., 2021). In particular, recently,
researchers have identified a new condition, the diversity of pre-training tasks, which has been shown
to be crucial to allow pre-trained models to be useful for downstream tasks. See Section 2 for more
detailed discussions on related work.
Unfortunately, this line of theory cannot be used to explain the success of pre-training in NLP. The
theory of multi-task pre-training requires a large number of diverse tasks, e.g., the number of tasks
needs to be larger than the last layer’s input dimension (a.k.a. embedding dimension), which is
typically 768, 1024 or 2048 (Devlin et al., 2019). However, in NLP pre-training, there are only a few,
if not one, pre-training tasks. Therefore, we need a new theory that is applicable to this setting.
Furthermore, while existing theories are able to explain certain empirical phenomena, it remains
unclear how to utilize the theory in practice. Ideally, we would like the theory to be useful to guide
our training procedure or inspire new techniques to improve the performance of real-world models.
Since in NLP pre-training, we do not have multiple tasks, we propose to study the blessing of multiple
classes. Concretely, consider the Masked Language Model (MLM) pre-training task in NLP. In
such a pre-training task, we have a large collection of sentences (e.g. from Wikipedia). During the
pre-training phase, we randomly mask a few words in each sentence, and predict the masked words
using the remaining words in this sentence. This pre-training task is a multi-class classification
1
Under review as a conference paper at ICLR 2022
problem where the number of classes is about 30K when using byte-pair-encoding (BPE) sub-word
units.1 Note that this number is much larger than the embedding dimension (768, 1024 or 2048).
In this paper, we develop a new statistical analysis aiming to explain the success of pre-training
for NLP. The key notion of our theory is the diversity of classes, which serves as a similar role as
the diversity of tasks in multi-task pre-training theory (Du et al., 2021; Tripuraneni et al., 2020a).
Our theory is not only applicable to the real-world NLP pre-training setting, but also inspires new
techniques to improve the practical performance. We summarize our contributions below.
1.1	Our Contributions
First, we define a new notion, diversity of classes, which is the least singular value of the last linear
layer in pre-training. We prove finite-sample bounds to show that for the cross-entropy loss, if
the diversity of classes is large, then pre-training on a single task provably improves the statistical
efficiency of the downstream tasks. To our knowledge, this is the first set of theoretical results
that demonstrates the statistical gain of the standard practice of NLP pre-training, without strong
distributional or smoothness conditions.
Second, from a technical point of view, previous theoretical work on multi-task (Du et al., 2021;
Tripuraneni et al., 2020b) builds on scalar output, and thus could not apply to multiclass tasks (e.g.,
cross entropy loss). We introduce a vector-form Radamacher complexity chain rule for disassembling
composite function classes based on vector-form Rademacher contraction property (Maurer, 2016).
This generalizes the scalar-form chain rule in Tripuraneni et al. (2020b). Furthermore, we adopt
the modified self-concordance condition to show that the least singular value of the last linear layer
serves as a diversity parameter for cross-entropy loss. We believe our techniques can be useful in
other problems.
Third, inspired by our theory, we develop a new regularization technique to promote class diversity
for multi-class pre-training. We apply the negative log determinant regularizer only to the last linear
layer of the pre-training model in masked language modeling. Our empirical results on BERT-base
show this technique can boost the performance of downstream tasks.
Organization This paper is organized as follows. In Section 2, we review the related work. In
Section 3, we formally describe the problem setup and introduce the necessary definitions. In
Section 4, we state our main Theorem 1 then instantiate it with several applications. In Section 5, we
describe our new regularization technique and present the empirical results. We conclude in Section 6
and defer the proofs to Appendix.
2	Related Work
Here we mostly focus on the theoretical aspects of pre-training. While there is a long list of work
demonstrating the empirical success of self-supervised learning, there are only a few papers that
study its theoretical aspects. One line of work studied the theoretical properties of contrastive
learning (Saunshi et al., 2019; Tosh et al., 2020), which is a different setting considered in this paper.
The most relevant one is by Lee et al. (2020) which showed that if the input data and pre-training
labels were independent (conditional on the downstream labels), then pre-training provably improved
statistical efficiency. However, this conditional independence assumption rarely holds in practice.
For example, in question-answering task, this assumption implies that given the answer, the question
sentence and the masked word are independent. Robinson et al. (2020) assumed the Central Condition
and a smoothness condition that related the pretraining task and the downstream task. Bansal et al.
(2021) related generalization error of self-supervised learning to the noise-stability and rationality.
However, it is difficult to verify the assumptions in these papers.
A recent line of work studied multitask pre-training (Du et al., 2021; Tripuraneni et al., 2020a;b;
Thekumparampil et al., 2021) in which the notion, diversity, has been identified to be the key that
enables pre-training to improve statistical efficiency. These works generally require a large number
1This is a standard setting in the BERT model (Devlin et al., 2019) and is widely adopted as a common
practice. By breaking down English words into BPE sub-word units, it could drastically increase the coverage of
English language by using a relatively small (32768) vocabulary.
2
Under review as a conference paper at ICLR 2022
of diverse tasks thus are not applicable to NLP, as we have mentioned. In comparison, we study
single-task multi-class pre-training which is different from theirs. Du et al. (2021) noted that their
results allowed an easy adaptation to multi-class settings in (Remark 6.2). However, they only focused
on quadratic loss with one-hot labels for multi-class classification. Instead, we study the commonly
used cross-entropy loss. While their analyses do not imply results in our setting, our theoretical
analyses are inspired by this line of work.
Our paper uses a diversity regularizer proposed in Zou & Adams (2012) to improve the performance
of pre-training. We note that there are other diversity regularizers (Xie et al., 2017; Mariet & Sra,
2015; Cogswell et al., 2015). These may also improve the performance as the one in Zou & Adams
(2012). We leave it as a future work to investigate these regularizers.
3	Preliminaries
In this section, we introduce the necessary notations, the problem setup, and several model-dependent
quantities of pre-training.
3.1	Notation and Setup
Notations. Let [n] = {1, 2,…，n}. We use ∣∣ ∙ ∣∣ or ∣∣ ∙ k2 to denote the '2 norm of a vector. Let
N (μ, σ2) be the one-dimensional Gaussian distribution. For a matrix W ∈ Rm×n, let ∣W∣ι,∞ =
maxq (Pp |Wq,p|) and ∣W∣∞→2 be the induced ∞-to-2 operator norm. We use the standard
O(∙), Ω(∙) and Θ(∙) notation to hide universal constant factors, and use O(∙) to hide logarithmic
factors. We also use a . b to indicate a = O(b).
Problem Setup. The procedure is divided into two stages: pre-training stage to find a representation
function and the downstream training stage to obtain a predictor for the downstream task. In both
stages, We use R to represent empirical risk and use R to represent expected loss.
In the first stage, we have one pre-training task with n samples, {xipre, yipre}in=1, where xipre ∈
Xpre ⊂ Rd is the input and yipre ∈ {0, 1}k-1 is the one-hot label for k-class classification (if yipre is
all-zero then it represents the k-th class).2 For instance, in masked language modeling, the input of
each sample is a sentence With one Word masked out, and the label is the masked Word.3 k in this
example is the size of the vocabulary (≈ 30K). We aim to obtain a good representation function h
Within a function class H ⊂ {Rd → Rr} Where r is the embedding dimension (often equals to 768,
1024, 2048 in NLP pre-training). For example, one popular choice of the representation function h
in NLP applications is the Transformer model and its variants (VasWani et al., 2017; Devlin et al.,
2019). On top of the representation, We predict the labels using function fpre Within function class
Fpre ⊂ {Rr → Rk-1}.
To train the representation function and predictor in pre-training stage, We consider the Empirical
Risk Minimization (ERM) procedure
1n
h = arg min min	Rpre(fpre,h)，arg min min —	'(fpre ◦ h(χpre), ypre)	(1)
h∈H fpre∈Fpre	h∈H fpre∈Fpre n i=1	i	i
Where ` is the loss function. We overload the notation for both the pre-training task and the
doWnstream task, i.e., for pre-training, ` : Rk-1 × {0, 1}k-1 → R and for the doWnstream task,
' :Rk0-1 X {0,1}k-1 → R. e.g., cross-entropy: '(y; y) = -y>y + log(1 + Pk-I exp (ys)).
NoW for the doWnstream task, We assume there are m samples {xidown, yidown}im=1. Note that xidown ∈
X down ⊂ Rd is the input and yidown ∈ {0, 1}k0-1 is the one-hot label for k0-class classification.4
2We assume only one pre-training task for the ease of presentation. It is straightforWard to generalize our
results to multiple pre-training tasks.
3Here We say only one Word being masked only for the ease of presentation. It is straightforWard to generalize
our results to the case Where multiple Words are masked out.
4For simplicity, We assume We only have one doWnstream task. Our theoretical results still apply if We have
multiple doWnstream tasks.
3
Under review as a conference paper at ICLR 2022
Note that in most real-world applications, we have n m and k k0 . For example, in sentiment
analysis, k0 = 2 (“positive” or “negative”). A widely studied task SST-2 (Wang et al., 2019) has
m ≈ 67K, which is also generally much smaller than the pre-training corpus (e.g., n > 100M
samples).
To train the classifier for the downstream task, we fix the representation function learned from the pre-
training task and train the task-dependent predictor within function class F down ⊂ {Rr → Rk0-1}:
m
产own = argmin Rdown(产wn, h) = argmin — X '(fdown ◦ h(XdoWn),ydown).⑵
f down ∈F down	f down ∈F down m
rɪ-il	∕'	1∙	.	i' .1	1	.	.	1	∙	.	♦/	1 ∖ ɪɪ T	.1	' 1 '	i'
Therefore, our predictor for the downstream task consists a pair (f down, h). We use the Transfer
Learning Risk defined below to measure the performance
Transfer Learning Risk，Rdown(/down, h) — Exdowndown [' (gdown (Xdown), ydown)]
where Rdown(∕down, h) ， Exdown,ydown [' (∕down ◦ h (χdown) ,ydown)] is the expected
loss (the expectation is over the distribution of the downstream task), and gdown =
arg ming∈{Rd→Rk0-1} Exdown,ydown ` g Xdown , ydown is the optimal predictor for the down-
stream task.
In our analysis, we also need to use the following term to characterize the quality of pre-training
Pre-training Risk = Rpre(fpre, h) - Expre,ypre [' (gpre (XPre), ypre)]
where Rpre(∕pre,h) ，Expre,ypre h' (∕pre οh (XPre) ,ypre)] is the expected loss, and gpre =
arg ming∈{Rd→Rk-1} Expre,ypre [` (g (Xpre) , ypre)] is the optimal predictor for the pre-training task.
Following the existing work on representation learning (Maurer et al., 2016; Du et al., 2021; Tripura-
neni et al., 2020b), throughout the paper we make the following realizability assumption.
Assumption 1 (Realizability). There exist h ∈ H, f pre ∈ F pre, f down ∈ F down such that gpre =
fpre ◦ h and gdown = f down ◦ h.
This assumption posits that the representation class and the task-dependent prediction classes are
sufficiently expressive to contain the optimal functions. Importantly, the pre-training and downstream
tasks share a common optimal representation function h. This assumption formalizes the intuition
that pre-training learns a good representation that is also useful for downstream tasks.
3.2	Task Relatedness and Diversity
We use the following definitions, which are natural analogies of those in Tripuraneni et al. (2020b) for
multi-task transfer learning. To measure the “closeness” between the learned representation and true
underlying feature representation, we use the following metric, following Tripuraneni et al. (2020b):
Definition 1. Let h ∈ H be the optimal representation function and h0 ∈ H be any representa-
tion function. Let fpre ∈ F be the optimal pre-training predictor on top of h. The pre-training
representation difference is defined as:
dFprefpre(h0;h)= inf Expre ypre [`(f0 ◦ h0(Xpre), ypre) - `(fpre ◦ h(Xpre), ypre)]	(3)
f0∈Fpre
where the expectation is over the pre-training data distribution.
Intuitively, this measures the performance difference between the optimal predictor and the best
possible predictor given a representation function h0 .
For transfer learning, we also need to introduce a similar concept on the downstream task.
Definition 2. Let h ∈ H be the optimal representation function and h0 ∈ H be any representation
function. On the downstream task, for a function class F down, let fdown ∈ F be the optimal
4
Under review as a conference paper at ICLR 2022
pre-training predictor on top of h. We define the worst-case representation difference between
representations h and h0 ∈ H as:
dF down (h0; h)
sup inf	Exdown ,ydown `(f0 ◦ h0(xdown), ydown) - `(f down ◦ h(xdown), ydown)
f down ∈F down f0∈Fdown	,y
where the expectation is over the data distribution of the downstream task.
We now introduce the key notion of diversity, which measure how well a learned representation, say
h0 , from the pre-training task can be transferred to the downstream task.
Definition 3. Let h ∈ H be the optimal representation function. Let fpre ∈ Fpre be the optimal
pre-training predictor on top of h. The diversity parameter ν > 0 is the largest constant that satisfies
dF down (h0; h) ≤ dFPref ;W h) , ∀h0 ∈ H.	(4)
While Definition 1- 3 are naturally defined from inspecting the pre-training procedure, it is not
trivial to use these definitions to derive statistical guarantees. In particular, one of our key technical
challenge is to show the least singular value of the last linear layer serves as a lower bound of the
diversity parameter when F pre and F down are linear function classes.
3.3	Model Complexities
Lastly, we need to introduce some notions to measure the complexity of the function classes con-
sidered. In this paper, we consider Gaussian complexity which quantifies the extent to which the
function in the class Q can be correlated with a noise sequence of length n × r.
Definition 4 (Gaussian complexity). Let μ be a probability distribution on aset X ⊂ Rd andsuppose
that xι, ∙∙∙ ,xn are independent samples selected according to μ. Let Q be a class of functions
mapping from X to Rr. Define random variable
ʌ
Gn(Q)= E
1r n
sup 一 ΣΣgki qk (xi )
q∈Q n k=1 i=1
(5)
as the empirical Rademacher complexity, where qk (xi) is the k-th coordinate of the vector-valued
function q(xi), gki (k ∈ [r], i ∈ [n]) are independent Gaussian N(0, 1) random variables. The
z-r	-	1	∙ .	/■ zɔ -	Z^<	/ zɔʌ	TIA ∕z^∖∖
GaUSSian complexity of Q is Gn(Q) = EμGn(Q).
Our main results are stated in terms of the Gaussian complexity. In Section 4.3 and 4.4 we will plug
in existing results of the Gaussian complexity of certain function classes to obtain concrete bounds.
We will need the following worst-case Gaussian complexity for the pre-training predictor within Fpre
G n(F Pre)=	max	G n(F pre∣h ◦ XPre),	(6)
h(xι),…,h(xn)
here h ∈ H and XPre = xi,…，Xn ∈ Xpre. Similarly We define Gm(Fdown) as the worst-case
Gaussian complexity for the downstream predictor within F down .
We note that a closely related notion is Rademacher complexity. The empirical Rademacher complex-
ity and Gaussian complexity only differ by a log factor (Ledoux & Talagrand, 1991).
4	Main Results
In this section we present our main theoretical results. In Section 4.1 we present an analysis in terms
of the diversity parameter for general loss function under certain regularity conditions. In Section 4.2,
we specialize the result to a setting that is most relevant to NLP pre-training applications, where FPre
and F down are sets of linear functions and the loss is cross-entropy. In this particular case, our key
result will show that one can use the singular value of the last linear to bound the diversity parameter.
In Section 4.3 and 4.4 we instantiate our bounds on two concrete representation function classes:
linear subspace and multi-layer network to showcase our main results.
5
Under review as a conference paper at ICLR 2022
4.1	Main Theorem
In this subsection, we present our generic end-to-end transfer learning guarantee for multi-class
transfer learning problem. We do not impose any specific function class formulations. Throughout
this subsection, we only make the following mild regularity assumptions to make our results general.
Assumption 2 (Regularity conditions). We assume the following regularity conditions hold:
•	In pre-training, '(∙, ∙) is Bpre-bounded, and '(∙, y) is Lpre-Lipschitzfor all y.
•	In downstream task, '(∙,y) is Bdown-bounded and Ldown-Lipschitzfor all y.
•	Any predictor f ∈ Fpre is L(F pre)-Lipschitz with respect to the Euclidean distance.
•	Predictors are bounded: kf ◦ h(x)k ≤ DX pre for any x ∈ Xpre, h ∈ H, f ∈ F pre.
Similarly kf ◦ h(x)k ≤ DX down for any x ∈ Xdown, h ∈ H, f ∈ Fdown.
Specifically, one can show that common task-dependent losses satisfy these conditions. For ex-
ample, when ' is cross-entropy loss (cf. Appendix A.2) for k-class classification, We prove ' is
√k - 1 -Lipschitz and DX -bounded where X denotes the input data domain.
Under these assumptions, we have the following guarantee.
Theorem 1.	Under Assumption 1 and 2, for a given fixed failure probability δ, with probability at
least 1 - δ we have the Transfer Learning Risk upper bounded by:
O
Lpre log(n) ∙ [L(Fpre) ∙ Gn(H) + Gn(Fpre)] +
kDDpr pre
n2
+Bprer log，何}

I ɪdown G (Fdown) + Bdown ∕log(1∕δ)
mm
The first line comes from pre-training ERM procedure and it accounts for the error of using an
approximate optimal representation h ≈ h. The second line characterizes the statistical error of
learning the downstream-task predictor f down from m samples. Note the diversity parameter appears
in the denominator, which relates the pre-training risk to the transfer learning risk. Theorem 1 shows
the risk would be small if the Gaussian complexities are small. We will show concrete examples
where Gn(H) and Gn(Fpre) are O(，1/n) and Gm scales as O(，1/m). We believe this theorem
applies broadly beyond the concrete settings considered in this paper.
In comparison with previous results, transfer learning risk analyses in (Du et al., 2021; Tripuraneni
et al., 2020b) focus on scalar output. Their results cannot be applied to multi-class transfer learning
tasks. In Theorem 1, we generalize the analysis in (Tripuraneni et al., 2020b) to handle multi-class
classification where the output is high dimensional (number of classes). Technically, in the proof, we
introduce a vector-form Radamacher complexity chain rule for disassembling composite function
classes by making use of the vector-form Rademacher contraction property (Maurer, 2016).
4.2 Main Results for Multi-class Classification with Cross-entropy Loss
Now we specialize the general result to the setting that is of most interest to NLP pre-training, where
the loss function ` is cross-entropy and the Fpre and F down are sets of linear functions. This choice
is consistent with the NLP pre-training: e.g., BERT (Devlin et al., 2019) uses transformers as the
representation learning function class H and uses word-embedding matrices as Fpre .
Formally we define
Fpre={f|f(z)	=	α>z, α ∈	Rr×(k-1),	kαsk ≤c1,∀s∈ [k-1]}	(7)
Fdown={f|f(z) = α>z, α ∈ Rr×(k0-1), kαsk ≤c0,∀s∈ [k0 - 1]}	(8)
where c1 and c0 are some universal constants. Then the regularity conditions are instantiated as:
•	Pre-training loss '(∙, y) is √k - 1 -Lipschitz and Bpre = DXPre-bounded.
6
Under review as a conference paper at ICLR 2022
•	Downstream loss is √k0 - 1 -LiPschitz and BdoWn = DXdown -bounded.
•	Any function f ∈ FPre is L(FPre) = cι√k - 1 -Lipschitz w.r.t. the l2 distance.
See APPendix A.2 for detailed derivations. Next we discuss our main assumPtion that relates the
diversity parameter to a concrete quantity of the last linear layer.
Assumption 3 (Lower Bounded Least Eigenvalue). Let the optimal linear predictor at the last layer
for pre-training be apre ∈ Rr×(k-1). We assume V，σr (αpre (αpre)>) > 0.
Similar assumptions have been used in multi-task representation learning (Du et al., 2021; Tripuraneni
et al., 2020a;b), and are shown to be necessary (Maurer et al., 2016; Du et al., 2021). Different from
their versions, our assumption is tailored for the multi-class classification setting.
One of our key technical contributions is to show νV serves as a lower bound for the diversity
parameter ν. See Lemma 3 in Appendix. Intuitively, this assumption ensures that the pre-training task
matrix spans the entire r-dimensional space and thus covers the output of the optimal representation
h(∙) ∈ Rr. This is quantitatively captured by the σr(αpre (αpre)>), which measures how spread
out these vectors are in Rr. Technically, we apply a modified self-concordance condition for better
characterizing multinomial logistic regression (Bach et al., 2010). See Appendix A.4 for proof.
We now state our theorem for this specific setting.
Theorem 2.	Under Assumptions 1 and 3, with probability at least 1 - δ we have the Transfer
Learning Risk upper bounded by:
O (1 1√k [log (n)[√k ∙ Gn(H) + Gn(FPre)] + √kD产]+ DX严远
ν	n2	n
+√k0 ∙ EX down G m(F down∣h ◦ XdoWn) + σjlog(1/" + DX down r log。/5)!
mm
Here EXdownGm(Fdown|h ◦ XdoWn) is Gaussian ComPlexity ofembeddings
h ◦ XdoWn = {h(xι), ∙∙∙ , h(Xm)∣Xdown = X1, ∙∙∙ ,Xm ∈ X down}	⑼
where the expectation is over Xdown, and σ2 = mm SuPf ∈fdown Pm=I Var('(f ◦ h(xdown), ydown))
is the maximal variance over FdoWn.
We remark that in Theorem 2, since we specialize to the case where Fpre and F doWn are
sets of linear functions, we can replace the term Ldown ∙ Grm(Fdown) in Theorem 1 by
(√k0 ∙ EXdownGm(Fdown∣h ◦ xdown) + σq'。弋/^ by utilizing the functional Bernstein inequal-
ity. This improvement can help us obtain Theorem 3. See Appendix A.2 for the full proof.
Now we discuss the interpretation of Theorem 2. Typically, Gn(Fpre) is much smaller than Gn(H)
because Gn(H) represents the complexity of the representation function, which is often complex. In
practice, n is often large. Therefore, in the benign case where νV = Θ (k) (when the condition number
of ɑpre is O(1)), then the dominating term will be Gn(Fpre). As we will show in the following
subsections, this term typically scales as O(，1/n). Together, this theorem clearly shows when 1)
the number of pre-training data is large, and 2) the least singular value of the last linear layer for
pre-training is large, the transfer learning risk is small. On the other hand, if νV is small, then the bound
becomes loose. This is consistent with prior counter examples on multi-task pre-training (Maurer
et al., 2016; Du et al., 2021) where the diversity is shown to be necessary.
4.3 Linear Subspace Representation
We instantiate this setting by assuming the underlying representation to be a projection onto a
low-dimensional subspace. For r	d, we let the function class be
H = {h|h(X) = B>X, B ∈ Rd×r, B is a matrix with orthonormal columns}	(10)
We require some additional regularity conditions. Following prior work (Du et al., 2021; Tripuraneni
et al., 2020b), we assume that kXk ≤ D and input data distribution satisfies the following condition.
7
Under review as a conference paper at ICLR 2022
Definition 5. We say the covariate distribution Pχ(∙) is Σ-sub-gaussian if for all V ∈ Rd,
E[exp(v>x)] ≤ exp (恪 ? Vk ) where the covariance Σ further satisfies σmaχ(Σ) ≤ C and
σmin (Σ) ≥ c ≥ 0 for universal constants c, C.
We have the following theorem that guarantees the performance of transfer learning.
Theorem 3.	Suppose Assumption 1 and 3 hold. For a sufficiently large constant c3, we assume
n ≥ c3d, m ≥ c3r, D ≤ c3(min(Vdr2, √rm)). Then with probability at least 1 一 δ, we have the
Transfer Learning Risk upper bounded by:
θ(l Pk log(n)(乃 + √!)+ M + I)
+(ko)2 rɪ+k’rog叵+Ga屋!
mm	m
To interpret this bound, consider the practically relevant scenario where k0 = O(1) (e.g., sentiment
analysis), m《n, k《n and r《d, in the benign case ν> = Ω (k), we have the transfer learning
risk O (qdn2 + pmm). Note that this is exactly the desired theoretical guarantee because the first
term accounts for using all pre-training data to learn the representation function and the second term
accounts for using the downstream data to learn the last linear layer. This is significantly better than
not using pre-training, in which case the risk scales
4.4 Deep Neural Network Representation
In this subsection, we assume the underlying representation to be a practical σ = tanh-activated
neural network. Predictors are still required to be linear functions at the interest of NLP pre-training,
i.e.,
H =	{h∣h(x)	= WKσ (Wκ-iσ (…σ (WIx)))},	(11)
Fpre =	{f|f(z)	= α>z, α ∈ Rr×(k-1), kαsk ≤c1M(K)2,∀s∈ [k 一 1]},	(12)
Fdown =	{f|f(z)	= α>z, α ∈ Rr×(k0-1), kαsk ≤c0M(K)2,∀s∈ [k0 一 1]}.	(13)
We further assume 1) for each p ∈ [K], kWpk1,∞ ≤ M (p), and 2) kWK k∞→2 ≤ M(K). Adapt
Gaussian complexity results in Golowich et al. (2018) we have
Gn(H) ≤θ( TM (K) ∙ D「『M (P) ! ,	Gn(Fpre) ≤O ( ( 一 (K ) . (14)
Now we are ready to state our theorem for this practical setting of NLP pre-training.
Theorem 4.	Under Assumption 1 and 2, assume M(K) ≥ c3 for a universal constant c3. Then with
probability at least 1 一 δ, Transfer Learning Risk is upper bounded by
e k krM(K)3 ∙ D√K ∙ ∏K=11M(p)	kM(K)3	k03 M(K)3 !
(	v√n	十	v√n	十	√m	.'
5 Experiments
Our theoretical analysis in previous sections implies that the diversity of the model parameter matrix
at the linear output layer in pre-training has a significant impact on the transfer capability, in the sense
that the larger ν (diversity parameter of fpre), the smaller the risk. Therefore, we could explicitly add
a diversity regularizer to the linear output layer to increase diversity. Motivated by this, we propose
to add the following diversity regularizer to the original BERT pre-training loss so that it becomes:
L0(Θ) = L(Θ) 一 λ ∙ lndet(apre (apre)>),	(15)
8
Under review as a conference paper at ICLR 2022
Table 1: Performance of diversity-regularized BERT pre-training with different values of di-
versity factor λ. We finetune the pretrained model on 8 downstream tasks from GLUE benchmark
and evaluate them on their dev sets. All results are “mean (std)” from 5 runs with different random
seeds. For MNLI, we average the accuracies on its matched and mismatched dev sets. For MRPC
and QQP, we average their accuracy and F1 scores. For STS-B, we average Pearson’s correlation
and Spearman’s correlation. All other tasks uses accuracy as the metric. The better-than-baseline
numbers are underlined, and the best numbers are highlighted in boldface.
Model	MNLI	MRPC	SST-2	CoLA	QQP	QNLI	RTE	STS-B
BERT-base (λ = 0.005)	84.17 (0.23)	87.16 (1.81)	92.48 (0.19)	59.99 (0.28)	89.42 (0.08)	88.11 (0.54)	67.28 (3.43)	89.33 (0.07)
BERT-base (λ = 0.05)	84.01 (0.10)	86.35 (5.15)	93.00 (0.16)	62.66 (1.07)	89.46 (0.03)	87.64 (0.44)	60.64 (6.08)	89.57 (0.13)
BERT-base (λ = 0.5)	84.00 (0.20)	89.42 (0.51)	92.93 (0.24)	60.76 (0.71)	89.33 (0.12)	88.01 (0.23)	67.93 (1.18)	89.22 (0.23)
BERT-base (reproduced)	83.96 (0.08)	86.14 (4.64)	92.64 (0.20)	61.46 (0.74)	89.28 (0.09)	88.10 (0.27)	63.64 (6.64)	89.19 (0.07)
where Θ denotes the set of all model parameters, λ is a hyper-parameter that controls the magnitude
of the diversity regularization, det(∙) denote the determinant of a matrix, and apre is the model
parameter matrix at the output linear layer. This type of diversity regularizer was proposed in Zou &
Adams (2012). This regularization technique is different from prior work because it is specifically
designed for multi-class pre-training: we only add the diversity regularizer to the last linear layer.
We use the above diversity-regularized loss (along with the original '2-regularization) to pretrain
BERT-base models under different values of diversity factor λ. Then we fine-tune them on 7
classification tasks and 1 regression task from the GLUE benchmark (Wang et al., 2019) to evaluate
their transfer performance.5 Our pre-training and finetuning implementations are based on the
opensource code released by Nvidia.6 We use the same pre-training data as the original BERT
(i.e., English Wikipedia + TorontoBookCorpus).7 Our detailed pre-training and finetuning hyper-
parameters along with other experimental details are reported in Appendix A.6.2.
In Table 5, we report our performance on the dev sets of the 8 downstream tasks. All the experiments
are repeated 5 times with different random seeds, and we report their mean values along with the
standard deviations. The complete experiment results (including full MNLI, QQP, and MRPC results)
can be found in Appendix A.6. From Table 5, we note that adding the diversity regularization
could generally improve the performance on these downstream tasks. In particular, when λ = 0.5,
our pretrained model outperforms the original BERT-base on 6 out of 8 tasks (with 3 of them
being significant), while achieving comparable performance on the other 2 tasks. Although our
model is slightly behind the original BERT on CoLA and QNLI, such a performance gap is not
statistically significant. Besides, we also see that our model with λ = 0.5 achieves a much more
stable performance (i.e., smaller std) on tasks with scarce finetuning data (< 4K samples in MRPC
and RTE). Our results, albeit still preliminary, demonstrate the potential of such a simple diversity-
regularizer. It could be an effective and simple performance booster for any of the existing pre-trained
NLP models (e.g., XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2020), ALBERT (Lan et al.,
2020), etc) with negligible computation and implementation cost. We leave the development of the
more advanced diversity regularizer as a future work.
6	Conclusion
We theoretically prove the benefit of multi-class pre-training using the notion of class diversity.
Inspired by the theory, we further propose a new regularization technique specially designed for
pre-training. Our experiments show potential of this diversity regularizer. An interesting direction
is to further investigate the impact of different diversity regularizers on larger pre-training models.
Lastly, we only studied multi-class pre-training in this paper, it is interesting to develop a theory for
other pre-training techniques such as those based on sequence reconstruction (Lewis et al., 2019).
5We do not report the WNLI (classification) task due to its reported issues of the task in Devlin et al. (2019).
6Distributed under Apache License: https://github.com/NVIDIA/DeepLearningExamples/
tree/master/PyTorch/LanguageModeling/BERT
7Collected and pre-processed using the code and script included in the open-source code:
https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/
LanguageModeling/BERT
9
Under review as a conference paper at ICLR 2022
References
Francis Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for logistic
regression. The Journal of Machine Learning Research, 15(1):595-627, 2014.
Francis Bach et al. Self-concordant analysis for logistic regression. Electronic Journal of Statistics,
4:384-414, 2010.
Yamini Bansal, Gal Kaplun, and Boaz Barak. For self-supervised learning, Rationality implies
generalization, provably. In International Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=Srmggo3b3X6.
Jonathan Baxter. A Model of Inductive Bias Learning. Journal of artificial intelligence research, 12:
149-198, 2000.
Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
doi: 10.1017/CBO9780511804441.
Rich Caruana. Multitask Learning. Machine learning, 28(1):41-75, 1997.
Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. ELECTRA: Pre-training
Text Encoders as Discriminators Rather Than Generators. In International Conference on Learning
Representations, 2020. URL https://openreview.net/forum?id=r1xMH1BtvB.
Michael Cogswell, Faruk Ahmed, Ross Girshick, Larry Zitnick, and Dhruv Batra. Reducing Over-
fitting in Deep Networks by Decorrelating Representations. arXiv preprint arXiv:1511.06068,
2015.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019
Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, Minneapolis,
Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.
URL https://www.aclweb.org/anthology/N19- 1423.
Simon Shaolei Du, Wei Hu, Sham M. Kakade, Jason D. Lee, and Qi Lei. Few-Shot Learning via
Learning the Representation, Provably. In International Conference on Learning Representations,
2021. URL https://openreview.net/forum?id=pW2Q2xLwIMD.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent Sample Complexity of
Neural Networks. In Conference On Learning Theory, pp. 297-299. PMLR, 2018.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu
Soricut. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. In
International Conference on Learning Representations, 2020. URL https://openreview.
net/forum?id=H1eA7AEtvS.
M. Ledoux and M. Talagrand. Probability in Banach Spaces: Isoperimetry and Processes. A
Series of Modern Surveys in Mathematics Series. Springer, 1991. ISBN 9783540520139. URL
https://books.google.com.hk/books?id=cyKYDfvxRjsC.
Jason D Lee, Qi Lei, Nikunj Saunshi, and Jiacheng Zhuo. Predicting what you already know helps:
Provable self-supervised learning. arXiv preprint arXiv:2008.01064, 2020.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Ves Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for
natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461,
2019.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A Robustly Optimized BERT Pretraining
Approach, 2020. URL https://openreview.net/forum?id=SyxS0T4tvS.
10
Under review as a conference paper at ICLR 2022
Zelda Mariet and Suvrit Sra. Diversity Networks: Neural Network Compression Using Determinantal
Point Processes. arXiv preprint arXiv:1511.05077, 2015.
Pascal Massart. About the Constants in Talagrand’s Concentration Inequalities for Empirical Pro-
cesses. Annals of Probability, pp. 863-884, 2000.
Andreas Maurer. A vector-contraction inequality for Rademacher complexities. In International
Conference on Algorithmic Learning Theory, pp. 3-17. Springer, 2016.
Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The Benefit of Multitask
Representation Learning. Journal of Machine Learning Research, 17(81):1-32, 2016.
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Under-
standing by Generative Pre-Training. 2018.
Joshua Robinson, Stefanie Jegelka, and Suvrit Sra. Strength from weakness: Fast learning using
weak supervision. In International Conference on Machine Learning, pp. 8127-8136. PMLR,
2020.
Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar.
A Theoretical Analysis of Contrastive Unsupervised Representation Learning. In International
Conference on Machine Learning, pp. 5628-5637, 2019.
Kiran Koshy Thekumparampil, Prateek Jain, Praneeth Netrapalli, and Sewoong Oh. Sample Efficient
Linear Meta-Learning by Alternating Minimization. arXiv preprint arXiv:2105.08306, 2021.
Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive learning, multi-view redun-
dancy, and linear models. arXiv preprint arXiv:2008.10150, 2020.
Nilesh Tripuraneni, Chi Jin, and Michael I Jordan. Provable Meta-Learning of Linear Representations.
arXiv preprint arXiv:2002.11684, 2020a.
Nilesh Tripuraneni, Michael I. Jordan, and Chi Jin. On the Theory of Transfer Learning: The
Importance of Task Diversity. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-
Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems
33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual, 2020b. URL https://proceedings.neurips.cc/paper/2020/
hash/59587bffec1c7846f3e34230141556ae-Abstract.html.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, L UkaSz Kaiser, and Illia Polosukhin. Attention is All you Need. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
3f5ee243547dee91fbd053c1c4a845aa- Paper.pdf.
Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-
bridge University Press, 2019.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. In
International Conference on Learning Representations, 2019. URL https://openreview.
net/forum?id=rJ4km2R5t7.
Pengtao Xie, Aarti Singh, and Eric P Xing. Uncorrelation and Evenness: a New Diversity-Promoting
Regularizer. In International Conference on Machine Learning, pp. 3811-3820. PMLR, 2017.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V.
Le. XLNet: Generalized Autoregressive Pretraining for Language Understanding. In Hanna M.
Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d,Alche—Buc, Emily B. Fox, and Roman
Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on
Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver,
BC, Canada, pp. 5754-5764, 2019. URL https://proceedings.neurips.cc/paper/
2019/hash/dc6a7e655d7e5840e66733e9ee67cc69- Abstract.html.
11
Under review as a conference paper at ICLR 2022
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan
Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep
learning: Training BERT in 76 minutes. arXiv preprint arXiv:1904.00962, 2019.
James Y Zou and Ryan P Adams. Priors for Diversity in Generative Latent Variable Models. In
Proceedings of the 25th International Conference on Neural Information Processing Systems-
Volume 2,pp. 2996-3004, 2012.
12
Under review as a conference paper at ICLR 2022
A Appendix
In Section 3, we have introduced Gaussian complexity. Let us restate for clarity.
Let μ be a probability distribution on a set X ⊂ Rd and suppose that xi, ∙∙∙ , Xn are independent
samples selected according to μ. Let Q bea class of functions mapping from X to Rr. Define random
variable
1r n
Gn(Q)= E sup-V' y^gkχik (Xi)	(16)
∈Q n
q∈Q k=1i=1
as the empirical Rademacher complexity, where qk (Xi ) is the k-th coordinate of the vector-valued
function q	(Xi ),gki	(k ∈ [r], i ∈ [n]) are independent Gaussian N (0, 1) random variables. The
Gaussian complexity of Q is Gn(Q) = EμGn(Q).
Analogously to the above we can define the empirical Rademacher complexity for vector-valued
functions as
1rN
Rn(Q)= E supN XX ki qk (Xi )	(17)
q∈Q Nk=1i=1
where eki(k ∈ [r], i ∈ [n]) are independent Rademacher Rad(ɪ) random variables. Its population
counterpart IS defined as Rn(Q) = Eμ[Rn(Q)]. Note that the superscripts existing In G and R imply
that they are empirical measures.
A.1 Proofs for Theorem 1
We illustrate Theorem 1 in two stages. First we show pre-training representation difference can be
upper bounded by constants and function class complexities. Then we transfer it to the downstream
task through the diversity parameter.
Pre-training
Theorem 5.	In pre-training, with probability at least 1 - δ, it holds that:
dF pre ,fpre
(h0; h)
≤ 4√∏LpreGn(FPre
oH)+4Bpre∖/log%)
n
≤ 4096LPre
ʌ/k — 1 D χ pre
n2
+ log(n)[L(F pre)Gn(H)+ G n(Fpre)]
+ 4BPre ∕log(2∕δ).
n
Proof. We begin with
dFpre,fpre(h0; h) ≤ 2 SuP	|Rpre(fPre, h) — RRpre(f pre, h)|.
f∈Fpre,h∈H
From the definition of Rademacher complexity (Wainwright, 2019, Theorem 4.12), with probability
at least 1 — 2δ we have
SuP	∣Rpre(f pre,h) — Rpre(fpre,h)∣ ≤ 2Rn('(Fpre ◦H))+2Bpre!∕l°g^ .
f pre ∈F pre,h∈H	n
Next, we apply the vector contraction inequality (Maurer, 2016). For function class F whose output
is in RK with component fk(∙), and the function (hi)s are some L-Lipschitz functions: RK → R,
we have
n	nK
E sup X Cihi(f(Xi)) ≤ √2LEe SuP XX
ik fk	(Xi).	(18)
f∈Fi=1	f∈F i=1 k=1
13
Under review as a conference paper at ICLR 2022
Hence for loss function ' satisfying |'(x) -'(y)∣ ≤ LPrekx -y∣∣2, ∀x,y ∈ Rk-1,the f takes value in
Rk-I with component functions fs(∙), s ∈ [k - 1], we have that population Rademacher complexity
can be bounded by
1n
Rn(l(Fpre ◦H)) = Expre— Ee	sup	Vei'(f ◦ h(xpre))
n f∈Fpre,h∈H i=1
n k-1
≤ EXpreI √2LpreEe	SUp XX ⅛3fs(h(xPre))
n	f∈Fpre,h∈F i=1 s=1
√2LpreRn (Fpre OH)
≤ √∏LpreGn(Fpre oH)
where the last line uses the fact that Rademacher complexity is upper bounded by Gaussian complexity:
Rn(Fpre OH) ≤ 居 Gn(Fpre ◦ H). Therefore we have, with probability at least 1 - δ,
dF pre ,f pre (h ;h)
≤ 4√∏LpreGn(Fpre
°H)+4Bpreι/log")
n
+ 4BPrel J0"),
n
≤ 4096LPre [AZk-12DXPre + iog(n)[L(Fpre)Gn(H) + Gn(Fpre)]
n2
where the last line uses decomposition of Gn(Fpre ◦ H) into the individual Gaussian complexities
of H and Fpre , leverages an expectation version of novel chain rule for Gaussian complexities
(Lemma 1).	□
In the spirit of Gaussian complexity decomposition theorem (Tripuraneni et al., 2020b, Theorem 7),
we introduce the following decomposition result upon vector-form Gaussian complexities.
Lemma 1. We have the following vector form Gaussian complexity decomposition:
Gn(Fpre ◦ H) ≤ 8√k - IDXPre + 512C(FPre ◦ H) ∙ log (n)	(19)
n2
, . ,_ _ .,	_ ,___, O. , _ .,	—. ,____,
where we use C(Fpre oH) = L(Fpre) ∙ Gn(H) + Gn(Fpre) to represent the complexity measure
of the composite function class.
Proof. Our proof extends (Tripuraneni et al., 2020b, Theorem 7), which focuses on a multi-task
scalar formulation. We further extend it to multi-class vector formulation. Specifically, on top of
the representation class H, they need to handle F0t (t is the number of tasks) while our objective
is a single function class Fpre of higher dimension (F pre is (k - 1)-dimensional for a k-class
classification task). We note that our proof technique and that of previous works (Tripuraneni et al.,
2020b; Maurer et al., 2016) both hinge on several properties of Gaussian processes.
To bound the empirical composite function class Fpre (H), note that vector-form Gaussian complexity
is defined as
k-1 n
Gn(FPre OH)= E nsupf(h)∈F Pre(H) ^X ^X gisfs(h(xPre))
s=1 i=1
=^^^E[	SUP	Zf(h)]
n f(h)∈FPre(H)
where we define mean-zero process Zf(h) = √n Pk-I Pn=ι gisfs(h(xPe)), then
E SUpf (h) Zf(h) = E SUpf(h) Zf(h) - Zf0(h0) ≤ ESUpf(h),f0(h0) Zf(h) - Zf0(h0). We further notice
that Zf(h) - Zf0(h0) is a sub-gaussian random variable parameter
1n
d2(f (h),f0(h0)∣xpre) = - X kf (h(xPre)) - f0(h0(xPre))k2
n i=1
k-1 n
=—XX (fs(h(xpre)) - fs (h0(xpre)))2
s=1 i=1
14
Under review as a conference paper at ICLR 2022
Dudley’s entropy integral bound (Wainwright, 2019, Theorem 5.22) shows
E sup	Zf (h) - Zf0(h0)
f(h),f0(h0)
δ
Zf(h) - Zf0(h0) + 32J(4, DXPre)
≤ 2E	sup
d(f (h),f 0 (h0)∣xpre)≤δ
DX pre
2E	sup	Zf(h) - Zf0(h0) + 32
d(f(h),f0(h0)∣xpre)≤δ	J 今
√log N (u; F Pre(H) ∣χPre) du.
It is straightforward to see the first term follows:
E	SUP	Zf(h) - Zf0(h0) ≤ E[kgk]δ ≤ nn(k - 1)δ
d(f(h),f 0(h0)∣xpre)≤δ
We now turn to bound the second term by decomposing the distance metric into a distance over
FPre and a distance over H. We claim that, for arbitrary h ∈ H, f ∈ FPre, let h0 be 1-close to h in
empirical l2-norm w.r.t inputs xpre, XPre …，Xnre. Given h0, let f be ⑦-close to f in empirical l2
loss w.r.t h0(xPre). Using the triangle inequality we have that
d(f(h),f0(h0)|XPre)
∖
1n
n X kf(h(XPre))- f0(h0(xPre))k
n i=1
≤ d(f (h), f(h0)∣XPre) + d(f (h0),f0(h0)∣xpre)
u1n
≤ t n X kf(h(XPre))- f(h0(xPre))k2 + B
u1n
≤ L(FPre)t - X kh(XPre) - h0(XPre)k2 + ≡2
=L(FPre) ∙ a + 3
where we have used that kf (X) - f(y)k ≤ L(FPre)kX - yk for any f ∈ FPre.
As for the cardinality of the covering CFpre(H). Observe |CFpre(H)| =	h∈CH(xpre) |CFhpre| ≤
|CH(xPre) | ∙ maXh∈H(χpre) |CfPrePre)|. This provides a bound on the metric entropy of
log N(eι ∙ L(FPre) + ⑦；FPre(H) |XPre) ≤ log N(卬 H|XPre) + max N©; FPre|h ◦ XPre).
h(xPre)
Applying the covering number upper bound with 5 = 2.&Pre),⑦= j gives a bound of entropy
integral ofa ,
ZDXPre
VZlog N (u; F Pre (H) ∣XPre) du
≤ Zi -
; HXPre du +
Z	max	` ^og N	( U;	FPrel h	◦ XPre) du
Jδ	hoχPre	V	∖ 2	1	)
From the Sudakov minoration theorem (Wainwright, 2019, Theorem 5.30) for Gaussian processes
and the fact that packing numbers at scale u upper bounds the covering number at scale ∀u > 0 we
find:
log N(u; H|XPre) ≤ 4 ( √nGn(H)! ,	log N(u; FPre|h(XPre)) ≤ 4 ( √nGn(F:e|h。XPre))
Combining the definition of worst-case Gaussian complexity with the aforementioned results we have
Gn(FPre ◦ H) ≤ 2√k-lδ + 256 log 4DXPe (L(FPre)G„(H) + Gn(FPre)),
substitute δ with 4Dn2Pre, proof is completed.
□
15
Under review as a conference paper at ICLR 2022
Downstream learning Next we turn to the second stage and come up with theoretical guarantees
by using inexact h learned from the first stage.
Theorem 6.	In the downstream task, we have that with probability at least 1 - δ,
Rdown(产own, h) - Rdown(fdown, h) ≤ dFdown(h; h) + 4√∏Ldown ∙ Gm(FdoWn) + 4BdownA∕log(^δ)
m
Proof. Assumption 1 implies
ExdOwn,ydown [' (gdθwn (Xdown) ,ydown)] = Rdown(fdown, h).
一	τ	.	一 ,〜τ 、一一 , 丁「、 一 ,	一、 一
To start, let f = argmrnf∈fdown Rdown(f, h) and Rdown(f down, h) - Rdown(f down, h) equals
[Rdown(f, h) — Rdown(fdown, h)∖ + [Rdown(fdown, h) — Rdown(f, h)∖
where the first term satisfies
Jnf [Rdown(f, h) — Rdown(fdown, h)i
f ∈Fdown L	」
≤ SUp	Jnf	[Rdown(f,h)- Rdown(fdown,h)]
f down∈Fdown f ∈Fdown
.7	(1.	7_ ʌ
dF down (h, h)
The second term follows the similar lines of Theorem 5
Rdown(产own, h) — Rdown(f, h) ≤ 4√∏LdownEχdownGm(FdownIh ◦ xdown) + 4Bdown Jlog(/^
m
Again we make use of the worst-case argument
EXdownGm(Fdown∣h ◦ Xdown) ≤ Gm(Fdown).
Combining the results gives the statement.	□
Proof for main Theorem 1 Having introduced class diversity parameter, proof is directly com-
pleted via combination of Theorem 5 and Theorem 6.
A.2 Proofs for Theorem 2
We could provide a better dependence on the boundedness noise parameters in Theorem 6 using
Bernstein inequality. We present the following corollary which has data-dependence in the Gaussian
complexities.
Corollary 1. Presuming Assumption 1 holds, we have that then with probability at least 1 — δ,
Rdown(产own, h) — Rdown(f down, h)
≤ dFdown(h; h) + 4√πLdown
• EXdownGm(Fdown∣h ◦ xdown) + 4σι/"") + 50Bdownlog%)
mm
Proof. Denote Z = sUpf |Rdown(f, h) — Rdown(f, h)|, we apply the functional Bernstein inequal-
ity (Massart, 2000, Theorem 3) to control the fluctuations. With probability at lest 1 — δ, we
have
Z ≤ 2E[Z] + 4 σ^- ʌ/iog(ɪ) + 35——Iog(U),	QO)
mδ	mδ
where σ2 = m SUpf Pm=I Var('(f ◦ h(xdown), ydown)). Thus
E[Z] ≤ 2EχdownRm(l(Fdown)|h◦ xdown)
≤ 2EX down√2LdownRRm(Fdown∣h ◦ xdown)
≤ 2EX down√∏LdownG m(F down∣h ◦ xdown),
where the second line uses vector-based contraction principle, the last line upper bounds the empirical
Rademacher complexity by Gaussian counterparts.	□
16
Under review as a conference paper at ICLR 2022
Proof for Theorem 2 Observe that
k-1
'(η; y) = -y>η + log (1 + X eηs), '(η; y) ≤ kηk
s=1
and
∂'(η; y)	eηi
dη = yi - 1 + Pk-1 eηs ,
∣Vη'(η;y)| ≤ √k - 1,
so it is LPre = √k 一 1 -Lipschitz. By definition the class FPre with parameters ∣∣ask2 ≤ O(1), S ∈
[k - 1], we obtain that L(Fpre) = O (√k - 1) since for any x,y ∈ Rr, any f ∈ Fpre we have
∣f (x) -f(y)∣2 = ∣α>x - α>y∣2
k-1
≤ X (hαs, x - yi)2
s=1
k-1
≤ X∣αs∣2∣x-y∣2
s=1
≤ c21(k - 1)∣x - y∣2
In conclusion we have
•	Pre-training loss '(∙, ypre) is √k - 1 -Lipschitz and BPre = DXPre-bounded.
•	Downstream loss '(∙, ydown) is √k0 - 1 -Lipschitz and BdoWn = DX down -bounded.
•	Linear layer f is L(Fpre) = O (√k - 1)-Lipschitz.
Consider task-specific function classes for characterizing class-diversity parameters. From Lemma 2
and Lemma 3 we know that
V = Ω(ν),	ν = σr (αια>).
Combining these pieces of results then the proof is completed.
A.3 WHAT IS diversity parameter FOR LINEAR LAYERS ?
In this subsection, we lower bound the diversity parameter V by V. We note the proof of this part is
very different from the multi-task setting studied in previous works (Du et al., 2021; Tripuraneni
et al., 2020b).
First, we demonstrate cross entropy loss between learned representation class and true underlying
representation could be bounded by quadratic loss,
Lemma 2. Suppose ` is a (k - 1)-class cross entropy loss and ypre is (k - 1)-onehot label. The
cross entropy can be bounded from both sides with quadratic loss,
c°Eχpre exp(-10max(kɑo>h(xpre)k, ∣ɑ>h(xpre)∣)) ∣∣ɑ0>h(xpre) - α>h(xpre)∣∣
. . ʌ ʌ . . .
≤ EχPre,yPre ['(/^ ◦ h(XPre))- '(f^ ◦ h (XPre),沙呼)]
≤ 1 ExPre ∣∣α0>h(xpre) - α>h(xpre)∣∣2 ,
where co = 1 λmin(Φ00(α>h(xpre))) denote second-order derivative Ofsoftmaxfunction, α0 and a
are parameters for fpre and fpre respectively.
Proof. The conditional distribution of (k - 1)-class classification task is defined as:
P (∙∣ɑ>h(x)) = ey>α>h(X)-φS>h(X))
= ey>α>h(x)-log (1+Psk=-11 e(α>h(x))s)
17
Under review as a conference paper at ICLR 2022
Taylor’s expansion tells that
Φ(α0>h(x)) = Φ(α>h(x)) + VΦ(α>h(x))>(α0>h(x) — α>h(x)) + o (∣∣α0>h(x) — α>h(x)∣∣2)
For generalized linear models: P(∙∣ɑ>h(x)) = exp(y>α>h(x) 一 Φ(ɑ>h(x))), it is known that
KL [P(∙∣ɑ>h(x)),P(∙∣α0>h(x))]
=Φ(α0>h(x)) - Φ(α>h(x)) - VΦ(α>h(x))>(α0>h(x) - α>h(x))
Our goal is to bound KL—divergence with quadratic distance,
μ ∣∣α0>h(x) — α>h(x)k2 ≤ KL [p (∙∣α>h(x)), P (∙∣α0>h(x))] ≤ f∣α0>h(x) — α>h(x)k2.
(21)
Here L = supχ, λmaχV2Φ(x0) for intermediate X ∈ [a0>h(x),α>h(x)]. μ is some constant
related to α0>h(x) and α>h(x) which requires more investigations.
First we show L which could be considered as upper bound of the maximum eigenvalue of multinomial
logistic regression. Restate this sub-problem for clarification: for Φ(x) = log (1 + Psk=-11 exs ), its
gradient at ith-coordinate is ∂∂Φ = exi∕ι+ps exs, so Hessian matrix is computed as
∂2Φ
∂xi ∂xj
exi ∙(1+Ps=i exs )
(1+Ps exs )2
-exi exj
(1+Ps exs )2 ,
i=j
i 6= j.
(22)
Our goal is to bound the smallest and largest eigenvalues of this Hessian matrix while x ∈
[x1, x2]. Next we show its largest eigenvalue is upper bounded by 1. Denote σ(x) =
ι+P1 exs [ex1,…,exk-1 ]>, the Hessian matrix can be restate as
V2Φ = diag(σ(x)) — σ(x)σ(x)>.
For any non-zero vector y, we have
y>v2φy = Xσ(X)iy2 -(σ(X)>y)2
i
≤ max(σ(X)i)∣y∣2
≤ ∣y∣2,
which implies its largest eigenvalue is no bigger than 1. Also, this Hessian is strictly-convex, from
Cauchy-Schwarz inequality we know
(σ(X)Ty)2 ≤ Xσ(X)iy2 ∙ Xσ(X)i
ii
<	σ(X)iyi2,
i
so this matrix is positive definite.
The derivations of bounding second order remaining terms of second-order Taylor’s expansion
from below are more complicated. Note that, logistic regression is not globally strongly-convex
(so as multinomial logistic regression). However, when landscape is considered locally (e.g., here
X0 ∈ [ɑ0>h(X), α>h(X)]), a class of convex functions called self-concordant functions would present
benign local properties. Next we introduce self-concordant functions and related useful properties.
Definition 6 (Modified self-concordance). F: Rp 7→ R is three times differentiable convex function
such that for some R > 0, for all u, v ∈ Rp, the function g : t 7→ F(u + tv) satisfies for all t ∈ R
|g000(t)| ≤ R∣v∣2 × g00(t)
(23)
18
Under review as a conference paper at ICLR 2022
Proposition 1. Multinomial logistic regression satisfies self-concordance condition, namely: Φ(x) =
log(1 + Psk=-11 exs), x ∈ Rk-1 is convex, for all u, v ∈ Rk-1, the function g : t 7→ Φ(u + tv) satisfies
|g000(t)| ≤ 5kvk2g00(t)	(24)
g(t)
g0(t)
g00(t)
g000(t)
Proof. Let P(t; v0) = 1 + Ps eus+tvs and P(t; vi) = Ps vsi eus +tvs , i > 1. Then we use multino-
mials P to represent derivatives of g(t)
log(P(T;V0))
P (t; v1)
P (t; v0)
P (t; v2)P (t; v0)
P (t; v0)2
P(t; v3)P(t; v0)2 - 3P(t; v2)P(t; v1)P(t; v0) + 2P(t; v1)3
P(t; v0)3
□
Let rs = eus+vst, hence
00# _ (Ps v2rs ) ∙ (1 + Ps rs) — (Ps Vsrs)2
g (t) =	(1 + Ps rs)2
_ Pi<j rirj (Vi - Vj)2 + Pi v2ri
=	(i+Ps τsy2
In the following we expand g000(t) as:
Pi<j rirj (Vi-Vj )2 Pk(vi+Vj- 2vk )rk ]+Pi v3ri+Pi Pj v2rirj (2vi - 3vj)
(1 + Ps rs)3
Pi<j rirj (Vi -Vj)2[Pk(Vi +Vj - 2Vk)rk] + PiVi2ri Vi(1 +2Pjrj) - 3PjVjrj
=	(1 + Ps rs)3
observe that
1
1 + Ps r
(Vi + Vj - 2Vk )rk
k
1
1 + Ps rs
≤ X |Vi + Vj - 2Vk | 1+ TP r ≤ 4kVk2
Vi(1+2	rj) -3	Vjrj
≤ 5kVk2
Substitute these into definition of self-concordance then proof is completed.
Self-concordance properties Self-concordance gives nice characterizations of local curvature of
convex functions which plays important role in describing local convexity (Bach, 2014). Some useful
results are given upon this condition (see (Bach et al., 2010, Proposition 1)), we list out the three
main inequalities as below
For all w, V ∈ Rp , t ∈ R, we have that,
F(w + v) ≥ F(W) + VF0(w) + "TOO(W)V ∙ (e-Rkvk2 + RkVk2 - 1)	(25)
R2kVk22
F(w + v) ≤ F(w) + vF0(w) + VTlFI(W)V ∙ (eRkvk2 - RkVk2 - 1)	(26)
R2kVk22
e-tRkvk2 F 00(w)	F00(w+tV)	etRkvk2 F 00(w)	(27)
19
Under review as a conference paper at ICLR 2022
Above results give two refined versions of Taylor’s expansion along with an upper and lower zero-
order Taylor expansion of F00. With these we are ready to give lower bound of KL divergence (see
Eq 21), i.e.,
Φ(α0>h(x)) — Φ(α>h(x)) — VΦ(α>h(x))>(α0>h(x) — α>h(x))
≥ 1 v>e-tRkvk2 F00 (α>h(x))v
≥ 1 λmin(Φ00(α> h(x)))kvk2e-5kvk2
≥ 2λmin(Φ00(α>h(x)))kvk2e-5(kɑ0>h(x)k + kɑ>h(x)k)
≥ 1 λmin(Φ00(α>h(x)))kvk2exp(-10max(kα0>h(x)k, kα>h(x)k))
where V = α0>h(x) — α>h(x), t is an parameter exists in [0,1].
Proof for Lemma 2 is completed.
Lemma 3. With Lemma 2 at hand and define C = α1α1>, it is demonstrated that
1
“F down (h； h) ≤ Q(〜)&F pre ,f pre ( h； h) , V = 0丁 (C)
□
(28)
Proof. For function classes FPre and Fdown such that fdown ∈ Fdown and data (Xdown, ydown)〜
Pf down。h which is the real underlying distribution, the worst-case representation difference is similar
to that in multi-task analysis (Tripuraneni et al., 2020b, Lemma 1):
.1	/	. 7_ ʌ
dFd own (h； h) =
sup
inf
f down ∈F down f 0∈F
down
E {'(f0 ◦ h(xdown),ydown) — '(fdown ◦ h(xdown), ydown
≤ sup inf UEXdown I
kαsk≤c0 kα0sk≤c0 2
k0-1	1
=sup inf -Eχ.
s=1 kαsk≤c0 kα0sk≤c0 2
α0>h(xdown) — α>h(xdown)∣∣2, here S ∈ [k0 — 1]
,. ,ʌ ..
(Λsc(h, h)).
Here in the last line, the inner infima is considered as the partial minimization of a convex quadratic
form (see (Boyd & Vandenberghe, 2004, Example 3.15, Appendix A.5.4)).
Define population covariance if representations h and h as
.,ʌ .
Λ(h, h)
- -人. .ʌ , . -∣--
E[h(x)h(x)>]
E[h(x)h(x)>]
-八.	..	,	..-∣- - -1	L	-
E[h(x)h(x)>] = Fhh Fhh
E[h(x)h(x)>]J	Fhhh Fhh
A / 1 7 ∖	T-1	L∕T^I∖+τ^I ∙ .1	1 ∙ ICl	1	. i' 7	∙ . Λ	. . 1
Λsc(h, h) = Fhh — Fhh(FhhYFhh is the generalized Schur complement of h with respect to h.
Controlling the pre-training representation difference with the lower bound is subtler,
dF pre,f pre (h； h)
=inf Eχpre,ypre{'(f' ◦ h(xpre),ypre) — '(fpre ◦ h(xpre),ypre)}
f0∈Fpre
=ll inf	Eχpre,ypre h'(f0 ◦ h(χpre), ypre)— '(产。h(χpre), ypre)] , S ∈ [k — 1]
kα0sk≤c1
≥ coEχPre exp( —10max(kα0>h(xpre)k, ka>h(xpre)k)) ∙
α0>h(xpre) — α>h(xpre)∣∣2
20
Under review as a conference paper at ICLR 2022
We lower bound each term in the sum over j identically and suppress the j for each of notation.
Define event E as
I ∖zs∖ = ∣(α>Bxpre)s∣ ≤ Cc0kαsk,	S = 1, 2, ∙ ∙ ∙ ,k - 1,1
I ∣Z2∣ = ∖(α0>B Xpre)s∖≤ Cc0kα0 sk, S = 1, 2, ∙∙∙ ,k - 1.'
where Zs 〜subG(∣∣αsk2C2) and Z2 〜subG(∣∣α0sk2C2). Then we use this event to bound the
diversity,
EXpre exp(-10max(kα0>h(χPre)k,kα>h(χPre)k)) ∙ ∣∣ɑ0>h(xpre) - α>h(xpre)∣∣2
≥EXpre
1(E)exp(-10max(kα0>h(χPre)k, kα>h(xpre)k)) ∙ ∣∣ɑ0>h(xpre) - α>h(xpre)∣∣2
≥e-10Cc0cι∙√k-T)EXpre(1(E) . ∣∣α0>h(χpre) - α>h(xpre)∣∣2
First write Expre (I(E) ∙ ∣∣ɑ0>h(xpre) — α>h(xpre)∣∣ ) as
Expre ( ∣∣α0>h(xpre) - α>h(xpre)∣∣2
—
Expre (I(Ec) ∙ ∣∣ɑ0>h(xpre) - α>h(xpre)∣∣2
Observe that for the last term, it holds
Expre (I(Ec) ∙ ∣∣ɑ0>h(xpre) - α>h(xpre)∣∣2
k—1
X Expre(1(Ec) ∙ [α0>h(xpre)-
α>h(xpre)i2
s=1
k —1	I
≤ X PP[Ec] √Eχpre [α0> h(χpre)-
s=1
4
α>h(xpre)
s
Using union bound we have that P [Ec] ≤
JExpre (α0>h(χPre) — α
Combining these we know
2(k —1)
c02
4
. Using L4-L2 hyper-contractivity we also conclude
>h(xpre)	≤ 10σ2 = 10Expre
“丁h(xpre) - α>h(xpre))2
Expre
1(Ec) ∣∣α0>h(xpre) - α>h(xpre)∣∣2] ≤
10√2(k - 1)
c0
Expre ∣∣α0>h(xpre) - α>h(xpre)∣∣2
Hence this metric could be claimed to be lower bounded as,
Ω (inf Expre ∣∣α0>h(xpre) - α>h(xpre)『)
=Ω (α>Λsc(h; h)αι)
=Ω (tr(Λsc(h; h)C)) , where C = αια>.
In the second line, we redefine α1 as parameter α of pre-training for clarity. In this way we conclude
that,
dFpre,fpre(h； h) = Ω (tr(Λsc(h, h)C)) = Ω 卜ι(Λsc(h, h))σr(C)),
where C implies expansion of representation h(x) ∈ Rr, and its condition number σr(C) indicates
how spread out this vector is in Rr :
k—1
C = ^X(αι)s(αι)> = αια>,	αι ∈ Rr×(k-1)
s=1
Aforementioned calculations show
1
“F down (h； h) ≤ a(〜)”F pre,f Pre (h； h),	V 一 仃丁 (C).
Proof is completed.
□
21
Under review as a conference paper at ICLR 2022
A.4 Proofs for Linear S ub space Representation
Proof. We begin with bounding each of the complexity terms in the Corrolary 1.
We make use of data-dependent inequalities (Tripuraneni et al., 2020b, Lemma 4) to help upper bound
related quantities. Intuitively Definition 5 implies tail-bound properties in a sub-gaussian process.
Gn(H) = IE SUp XXgkib>xiie
n	B∈H k=1 i=1
k-1 n
G n(F preI h ◦ XPre) = I E	SUp XX
gis αs> B > xipre
n	[α1,…,αk-1 S=1 i=1	_
=吗-UEk XgisB>xPrek
n	i=1
=c⅜12 qtgB)
then G n (FPre) ≤O ((k - Np).
•	Similarly,
Gm(Fdown∣h◦ χdown) ≤ ci(k： 1)UIXσi(B>∑B)
√m	NW
then Gm(Fdown) ≤ O ((k0 - l)pɪ).
•	boundedness parameter DXPre = supα,B ∣∣ατBτXk = O(√k - 1 D)
•	cross entropy '(η; y) = -yτη + log (1 + Pk-I eηs), then ∣d`⅛y) I = Iyi - 1+Pe-； e,s ∣,
∣Vη'(η; y)∣ ≤ √k - 1, so it is Lpre = √k - 1 -Lipschitz in its first coordinate uniformly
over its second for pre-training and Ldown = √k0 - 1 -Lipschitz for downstream task.
•	I'(η;y)∣ ≤ O(kηk), where kηk = ∣xTBPreak . √k 一 1D. This means Bpre =
O(√k-1 D), Bdown = O(√k0-Td).
In Corollary 1, we define and compute the maximal variance term σ2 as,
σ2 = 1 sup
m f down ∈F down
k 0 - 1
≤ ----
m
m
X Var('0(fdown ◦ h(xdown), ydown))
i=i
m
SUp
f down ∈F down
EVar(fdown ◦ h(xdown))
i=i
k 0 - 1
------ SUp
m	kαsk≤O(i)
(k0 - 1)2
k0-i m
XX Var(a>BTXdOWn)
s=i i=i
m
SUp	y^(a,gB)τ∑B as
kαsk≤O(i) i=i
m
= (k0 - 1)2O(∣B∑B k2)
=O ((k0 - 1)2)
22
Under review as a conference paper at ICLR 2022
With these results in hand, we are now prepared to apply Corollary 1, w.p. at least 1 - δ
Rdown(产。wn, h) - Rdown(f down, h)
≤ dFdown (h; h) + 4√ΠLdown ∙ EXdown Gm (Fdown । h ◦ Xdown)
+ 4σι ∕lθg %)+ 50 B down lθg ("δ)
mm
where Gm(Fdown |h ◦ Xdown) is defined in Theorem 2.
Thus Ldown ∙ EXdownGm Fddown I h ◦ Xdown) ≤ LdownGm(Fdown) ≤ O((k0 - 1) 3 Pm), σ ≤
O(k0 一 1), and Bdown ≤ O( √k0 - 1D), Further, we obtain upper bound of worst-case representation
difference by diversity parameter and adoption of Theorem 5: w.p. at least 1 - δ
dF down (h; h)
.7	(1.. 7_ ʌ
V “Fpre,fpre (h； h)
ν
≤ 1 (4096L [log (n) ∙ [L(Fpre) ∙ Gn(H) + Gn(Fpre)] + √^DXPre 1 + 4B∖∕Iog^ )
ν	n2	n
.¥ (log(n)(不 + k∕!! + D (Jn + I!)
The last thing to consider for completing the proof for Theorem 3 is giving accurate characterization
of diversity parameter ν, which we leave for the next subsection.	□
A.5 Proofs for Deep Neural Network Representation
Proof. In deep neural network, we first review complexity quantities. Adapted from Theorem
8 (Golowich et al., 2018), we have
Rn(N) ≤
2∏P=ι
n
∖
n
(K + 1 + log d) ∙ max ɪ2 xjj
2D√K +1+logd ∙ ΠKK=1M(p)
√n
where Xi,j denotes the j-th coordinate of vector Xi.
Then we proceed to bound the Gaussian complexities for our deep neural network and prove The-
orem 4. Recall that under the conditions of the result we can use former results to verify the task
diversity condition is satisfied with parameters Ω(z>) with V = σr (α1α>) > 0. We can see that
∣∣Ex[h(x)h*(x)>k 2 ≤ EXkh(X)h*(x)k ≤ O(M (K )j) using the norm bound from. Hence under this
setting we can choose ci sufficiently large so that cιM(K)j & M(K) cj. The condition M(K) & 1
in the theorem statement is simply used to clean up the final bound.
In order to instantiate Theorem 1 we begin by bounding each of the complexity terms in the expression.
•	For the feature learning complexity in the training phase, we leverage above results, then
rn	r
Gn(H) = 1 E[supXXgkihk(Xipre)]≤XGn(hk (Xpre))
n WK k=i i=i	k=i
C (V ^ %，pre- 1 /	2D√K +1+lOgd ∙ nP=IM (P)
≤ log(n) ∙) Rnhk(Xi ) ≤ r log(n)----------------------.
公	√n
This also implies the population Gaussian complexity.
•	By definition the class F as linear maps with parameters ∣∣αs∣j ≤ ciM(K)j,∀s ∈ [k 一 1],
we obtain that L(F) = cι√k 一 1M(K)j.
23
Under review as a conference paper at ICLR 2022
• For the complexity of learning Fpre in the training phase we obtain,
Gn(FpreIh ◦ XPre) = 1 Eg [sup X Xgisα>h(χPre)] .(k - DM(K)2 Eg[k Xgish(xii)k]
n	α∈F	n
s=1 i=1	i=1
.tX …2.3M立 max kh(xpre)k.
n	i=1	n
For tanh activation function, we simply have
kh(x)k2 = kWKrK-1k22 ≤ kWKk2∞→2,
where rK -1 denotes ourput of the K- 1th layer,
kh(x)k ≤O(M(K)).
In conclusion we obtain
Gn(Fpre) ≤ O ((k - 1√M(K)3 ).
• Similarly
Gm(Fdown I h ◦ XdoWn) ≤ O
(k0 - 1)M(K)3
√m
Regularity conditions
•	BoUndedness parameter DXPre = SUPa h ∣∣a>h(xpre)k = O(√k - 1M(K)3).
•	Pre-training loss is LPre = √k 一 1 -LiPschitz and BPre = O(√k - 1M(K)3)-bounded.
•	Downstream loss is Ldown = √k0 — 1 -Lipschitz and B down = O(√k0 - 1M (K )3)-
bounded.
Assembling the previous complexity arguments shows the transfer learning risk is bounded by
log(n) L(Fpre)r log(n)
+ I ɪ max
+ V
Lpre VkDXpre
n2
D√K∏K=1M(p)+ kM(K)
))
Bpre /lθg(1∕δ)) + Bdown /lθg(1∕δ)
,n	m
Ldownk0M (K)3
√m
3
Substitute regularity conditions into it, then the risk is simplified as stated in Theorem 4.	□
A.6 Experiment details
Full statistics (including matched and mismatched dev sets for MNLI, accuracy and F1 scores for
MRPC and QQP, and (Pearson’s correlation + Spearman’s correlation)/2 for STS-B. All other tasks
uses accuracy as the metric) could be found in Table A.6.
A.6.1 Complete statistics
Here we provide complete results on GLUE dev sets over 5 random seeds.
24
Under review as a conference paper at ICLR 2022
Table 2: FUll StatiSticS on GLUE dev sets.
Model	Statistics	MNLI(m/mm)	MRPC(acc/F1)	SST-2	CoLA	QQP(acc/F1)	QNLI	RTE	STS-B
λ = 0.005	mean std	83.96/84.37 0.26/0.21	84.90/89.42 2.28/1.34	92.48 0.19	59.99 0.28	90.96/87.88 0.05/0.11	88.11 0.54	67.28 3.43	89.33 0.07
λ=0.05	mean std	83.88/84.14 0.04/0.16	83.72/88.98 6.69/3.62	93.00 0.16	62.66 1.07	90.97/87.96 0.05/0.04	87.64 0.44	60.64 6.08	89.57 0.13
λ = 0.5	mean std	83.96/84.04 0.15/0.24	87.75/91.09 0.52/0.50	92.93 0.24	60.76 0.71	90.85/87.81 0.10/0.14	88.01 0.23	67.93 1.18	89.22 0.23
BERT-baSe	mean std	83.85/84.07 0.13/0.04	83.48/88.80 6.08/3.19	92.64 0.20	61.46 0.74	90.87/87.68 0.07/0.11	88.10 0.27	63.64 6.64	89.19 0.07
Table 3: Performance of reproduced BERT-base model.
	GLUE	MNLI(m/mm)	MRPC(acc/F1)	SST-2	CoLA	QQP(acc/F1)	QNLI	RTE	STS-B
—	42	83.90/84.04	71.32/82.44	92.66	60.11	90.94/87.72	87.58	66.79	89.25
	0	83.86/84.12	86.27/90.18	92.43	62.05	90.74/87.53	88.19	54.29	89.07
seed	393	83.78/84.06	86.76/90.63	92.54	61.42	90.93/87.84	88.29	70.36	89.19
	78	84.05/84.02	86.76/90.63	92.55	61.50	90.89/87.60	88.12	57.14	89.18
	3837	83.66/84.11	86.27/90.13	93.00	62.20	90.87/87.73	88.33	69.64	89.26
	mean	83.85/84.07	83.48/88.80	92.64	61.46	90.87/87.68	88.10	63.64	89.19
	std	0.13/0.04	6.08/3.19	0.20	0.74	0.07/0.11	0.27	6.64	0.07
Table 4: Performance of λ = 0.005 regularized pre-training model.
	GLUE	MNLI(m/mm)	MRPC(acc/F1)	SST-2	CoLA	QQP(acc/F1)	QNLI	RTE	STS-B
		42	84.24/84.43	87.25/90.72	92.20	59.99	90.94/87.85	87.09	67.14	89.40
	0	83.91/83.96	86.27/90.47	92.43	60.06	91.03/87.88	88.24	70.71	89.36
seed	393	83.84/84.51	85.54/89.52	92.55	59.48	90.89/87.68	88.33	71.07	89.22
	78	84.23/84.44	84.80/89.45	92.78	60.13	90.95/87.97	88.71	65.71	89.38
	3837	83.56/84.53	80.64/86.93	92.43	60.30	91.01/88.00	88.17	61.79	89.28
	mean	83.96/84.37	84.90/89.42	92.48	59.99	90.96/87.88	88.11	67.28	89.33
	std	0.26/0.21	2.28/1.34	0.19	0.28	0.05/0.11	0.54	3.43	0.07
Table 5: Performance of λ = 0.05 regularized pre-training model.
	GLUE	MNLI(m/mm)	MRPC(acc/F1)	SST-2	CoLA	QQP(acc/F1)	QNLI	RTE	STS-B
—	42	83.88/84.22	86.52/90.27	92.89	61.12	91.06/87.95	86.93	65.00	89.52
	0	83.83/83.92	88.97/92.00	93.00	64.36	90.96/87.93	87.73	54.29	89.65
seed	393	83.96/83.98	70.59/81.92	93.12	62.22	90.94/88.03	87.47	61.79	89.65
	78	83.86/84.26	87.50/91.06	92.78	63.13	90.94/87.97	88.26	68.93	89.69
	3837	83.86/84.30	85.04/89.66	93.23	62.49	90.93/87.91	87.82	53.21	89.33
	mean	83.88/84.14	83.72/88.98	93.00	62.66	90.97/87.96	87.64	60.64	89.57
	std	0.04/0.16	6.69/3.62	0.16	1.07	0.05/0.04	0.44	6.08	0.13
Table 6: Performance of λ = 0.5 regularized pre-training model.
	GLUE	MNLI(m/mm)	MRPC(acc/F1)	SST-2	CoLA	QQP(acc/F1)	QNLI	RTE	STS-B
		42	83.75/83.87	87.75/90.89	93.00	59.79	90.88/87.75	87.58	65.71	89.00
	0	83.84/84.30	88.24/91.56	92.66	60.94	90.87/87.77	88.14	67.86	89.25
seed	393	83.98/83.68	87.99/91.46	93.12	60.99	90.98/88.04	88.22	68.21	89.19
	78	84.02/84.29	87.99/91.33	93.23	60.22	90.87/87.86	88.12	68.93	89.65
	3837	84.19/84.05	86.76/90.21	92.66	61.86	90.67/87.61	87.98	68.93	89.03
	mean	83.96/84.04	87.75/91.09	92.93	60.76	90.85/87.81	88.01	67.93	89.22
	std	0.15/0.24	0.52/0.50	0.24	0.71	0.10/0.14	0.23	1.18	0.23
25
Under review as a conference paper at ICLR 2022
A.6.2 Hyperparameters
Pre-training Hyperparameters for pre-training are shown in Table 7.
Hyperparam	phase-1	phase-2
Number of Layers	12	12
Hidden size	768	768
FFN inner hidden size	3072	3072
Attention heads	12	12
Steps	7038	1563
Optimizer	LAMB	LAMB
Learning Rate	9e-3	6e-3
β1	0.9	0.9
β2	0.999	0.999
WarmUp	28.43 %	12.80 %
Batch Size	65536	32768
Table 7: Hyperparameters used in pre-training our models. We use the LAMB optimizer (You et al.,
2019) for large-batch pretraining of the BERT model, where β1 and β2 are its two hyper-parameters.
Finetuning Hyperparameters for downstream tasks are shown in Table 8. We adapt these hyperpa-
rameters from Liu et al. (2020), Devlin et al. (2019), and Yang et al. (2019).
	LR	BSZ	# EP	WARMUP	WD	FP16	SEQ
CoLA	1.00E-05	32	20	6%	0.1	O2	128
SST-2	3.00E-05	32	10	6%	0.1	O2	128
MNLI	3.00E-05	32	5	6%	0.1	O2	128
QNLI	3.00E-05	32	10	6%	0.1	O2	128
QQP	3.00E-05	32	5	6%	0.1	O2	128
RTE	3.00E-05	16	5	6%	0.1	O2	128
MRPC	3.00E-05	16	5	6%	0.1	O2	128
Table 8: The hyperparameters used in finetuning our model in downstream tasks. LR: learning
rate. BSZ: batch size. #EP: number of epochs. WARMUP: warmup ratio. FP16: automatic mixed
precision (AMP) level. SEQ: input sequence length.
A.7 Other details
Computing infrastructure We pretrain our (diversity-regularized) BERT-base models using 32
Nvidia V100 GPUs (32GB RAM each), and the finetuning of the model uses 4 Nvidia V100 GPUs.
26