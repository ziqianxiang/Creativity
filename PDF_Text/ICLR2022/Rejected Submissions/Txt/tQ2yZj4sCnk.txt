Under review as a conference paper at ICLR 2022
Divergence-Regularized Multi-Agent Actor-
Critic
Anonymous authors
Paper under double-blind review
Ab stract
Entropy regularization is a popular method in reinforcement learning (RL). Al-
though it has many advantages, it alters the RL objective and makes the converged
policy deviate from the optimal policy of the original Markov Decision Process.
Though divergence regularization has been proposed to settle this problem, it can-
not be trivially applied to cooperative multi-agent reinforcement learning (MARL).
In this paper, we investigate divergence regularization in cooperative MARL and
propose a novel off-policy cooperative MARL framework, divergence-regularized
multi-agent actor-critic (DMAC). Mathematically, we derive the update rule of
DMAC which is naturally off-policy, guarantees a monotonic policy improvement
and is not biased by the regularization. DMAC is a flexible framework and can be
combined with many existing MARL algorithms. We evaluate DMAC in a didactic
stochastic game and StarCraft Multi-Agent Challenge and empirically show that
DMAC substantially improves the performance of existing MARL algorithms.
1	Introduction
Regularization is a common method for single-agent reinforcement learning (RL). The optimal
policy learned by traditional RL algorithm is always deterministic (Sutton and Barto, 2018). This
property may result in the inflexibility of the policy facing with unknown environments (Yang et al.,
2019). Entropy regularization is proposed to settle this problem by learning a policy according to the
maximum-entropy principle (Haarnoja et al., 2017). Moreover, entropy regularization is beneficial to
exploration and robustness for RL algorithms (Haarnoja et al., 2018). However, entropy regularization
is imperfect. Eysenbach and Levine (2019) pointed out maximum-entropy RL is a modification of the
original RL objective because of the entropy regularizer. Maximum-entropy RL is actually learning
an optimal policy for the entropy-regularized Markov Decision Process (MDP) rather than the original
MDP, i.e., the converged policy may be biased. Nachum et al. (2017) analysed a more general case for
regularization in RL and proposed what we call divergence regularization. Divergence regularization
can avoid the bias of the converged policy as well as be beneficial to exploration. Wang et al. (2019)
employed divergence regularizer and proposed a single-agent RL algorithm, DAPO, which prevents
the altering-objective drawback of entropy regularization.
Regularization can also be applied to cooperative multi-agent reinforcement learning (MARL)
(Agarwal et al., 2020; Zhang et al., 2021). However, most of cooperative MARL algorithms do not
use regularizer (Lowe et al., 2017; Foerster et al., 2018; Rashid et al., 2018; Son et al., 2019; Jiang
et al., 2020; Wang et al., 2021a). Only few cooperative MARL algorithms such as FOP (Zhang et al.,
2021) use entropy regularization, which may suffer from the drawback aforementioned. Divergence
regularization, on the other hand, could potentially benefit cooperative MARL. In addition to its
advantages mentioned above, divergence regularization can also help to control the step size of policy
update which is similar to conservative policy iteration (Kakade and Langford, 2002) in single-agent
RL. Conservative policy iteration and its successive methods such as TRPO (Schulman et al., 2015)
and PPO (Schulman et al., 2017) can stabilize policy improvement (Touati et al., 2020). These
methods use a surrogate objective for policy update, but decentralized policies in centralized training
with decentralized execution (CTDE) paradigm may not preserve the properties of the surrogate
objective. Moreover, DAPO (Wang et al., 2019) cannot be trivially extended to cooperative MARL
settings. Even with some tricks like V-trace (Espeholt et al., 2018) for off-policy correction, DAPO
is essentially an on-policy algorithm and thus may not be sample-efficient in cooperative MARL
settings.
1
Under review as a conference paper at ICLR 2022
In the paper, we propose and analyze divergence policy iteration in general cooperative MARL
settings and a special case combined with value decomposition. Based on divergence policy iteration,
we derive the off-policy update rule for the critic, policy, and target policy and propose divergence-
regularized multi-agent actor-critic (DMAC), a novel off-policy cooperative MARL framework. We
theoretically show that DMAC guarantees a monotonic policy improvement and is not biased by the
regularization. Besides, DMAC is beneficial to exploration and stable policy improvement by applying
our update rule of target policy. Moreover, DMAC is a flexible framework and can be combined with
many existing cooperative MARL algorithms to substantially improve their performance.
We empirically investigate DMAC in a didactic stochastic game and StarCraft Multi-Agent Challenge
(Samvelyan et al., 2019). We combine DMAC with five representative MARL methods, i.e., COMA
(Foerster et al., 2018) for on-policy multi-agent policy gradient, MAAC (Iqbal and Sha, 2019) for
off-policy multi-agent actor-critic, QMIX (Rashid et al., 2018) for value decomposition, DOP (Wang
et al., 2021b) for the combination of value decomposition and policy gradient, and FOP (Zhang
et al., 2021) for the combination of value decomposition and entropy regularization. Experimental
results show that DMAC indeed induces better performance, faster convergence, and better stability
in most tasks, which verifies the benefits of DMAC and demonstrates the advantages of divergence
regularization over entropy regularization in cooperative MARL.
2	Related Work
MARL. MARL has been a hot topic in the field of RL. In this paper, we focus on cooperative MARL.
Cooperative MARL is usually modeled as Dec-POMDP (Oliehoek et al., 2016), where all agents share
a reward and aim to maximize the long-term return. Centralized training with decentralized execution
(CTDE) (Lowe et al., 2017) paradigm is widely used in cooperative MARL. CTDE usually utilizes a
centralized value function to address the non-stationarity for multi-agent settings and decentralized
policies for scalability. Many MARL algorithms adopt CTDE paradigm such as COMA, MAAC,
QMIX, DOP and FOP. COMA (Foerster et al., 2018) employs the counterfactual baseline which can
reduce the variance as well as settle the credit assignment problem. MAAC (Iqbal and Sha, 2019)
uses self-attention mechanism to integrate local observation and action of each agent and provides the
structured information for the centralized critic. Value decomposition (Sunehag et al., 2018; Rashid
et al., 2018; Son et al., 2019; Yang et al., 2020; Wang et al., 2021a;b; Zhang et al., 2021) is a popular
class of cooperative MARL algorithms. These methods express the global Q-function as a function
of individual Q-functions to satisfy Individual-Global-Max (IGM), which means the optimal actions
of individual Q-functions are corresponding to the optimal joint action of global Q-function. QMIX
(Rashid et al., 2018) is a representative of value decomposition methods. It uses a hypernet to ensure
the monotonicity of the global Q-function in terms of individual Q-functions, which is a sufficient
condition of IGM. DOP (Wang et al., 2021b) is a method that combines value decomposition with
policy gradient. DOP uses a linear value decomposition which is another sufficient condition of IGM
and the linear value decomposition helps the compute of policy gradient. FOP (Zhang et al., 2021) is
a method that combines value decomposition with entropy regularization and uses a more general
condition, Individual-Global-Optimal, to replace IGM. In this paper, we will combine DMAC with
these algorithms and show its improvement.
Regularization. Entropy regularization was first proposed in single-agent RL. Nachum et al. (2017)
analyzed the entropy-regularized MDP and revealed the properties about the optimal policy and
the corresponding Q-function and V-function. They also showed the equivalence of value-based
methods and policy-based methods in entropy-regularized MDP. Haarnoja et al. (2018) pointed
out maximum-entropy RL can achieve better exploration and stability facing with the model and
estimation error. Although entropy regularization has many advantages, Eysenbach and Levine (2019)
showed entropy regularization modifies the MDP and results in the bias of the convergent policy. Yang
et al. (2019) revealed the drawbacks of the convergent policy of general RL and maximum-entropy
RL. The former is usually a deterministic policy (Sutton and Barto, 2018) which is not flexible
enough for unknown situations, while the latter is a policy with non-zero probability for all actions
which may be dangerous in some scenarios. Neu et al. (2017) analyzed the entropy regularization
method from several views. They revealed a more general form of regularization which is actually
divergence regularization and showed entropy regularization is just a special case of divergence
regularization. Wang et al. (2019) absorbed previous result and proposed an on-policy algorithm, i.e.,
DAPO. However, DAPO cannot be trivially applied to MARL. Moreover, its on-policy learning is
2
Under review as a conference paper at ICLR 2022
not sample-efficient for MARL settings and its off-policy correction trick V-trace (Espeholt et al.,
2018) is also intractable in MARL. There are some previous studies in single-agent RL which use
similar target policy to ours, but their purposes are quite different. Trust-PCL (Nachum et al., 2018)
introduces a target policy as a trust region constraint for maximum-entropy RL, but the policy is
still biased by entropy regularizer. MIRL (Grau-Moya et al., 2019) uses a distribution which is only
related to actions as the target policy to compute a mutual-information regularizer, but it still changes
the objective of the original RL.
3	Preliminaries
Dec-POMDP is a general model for cooperative MARL. A Dec-POMDP is a tuple M =
{S, A, P, Y, O, I , n, r, γ}. S is the state space, n is the number of agents, γ is the discount fac-
tor, and I = {1, 2 •…n} is the set of all agents. A = Ai X A2 ×∙∙∙× An represents the joint action
space where Ai is the individual action space for agent i. P(s0|s, a) : S × A × S → [0, 1] is the
transition function, and r(s, a) : S × A → R is the reward function of state s and joint action a. Y is
the observation space, and O(s, i) : S × I → Y is a mapping from state to observation for each agent.
The objective of Dec-POMDP is to maximize J (π) = Eπ [Pt=0 γtr(st, at)] , and thus we need to
find the optimaljoint policy π* = arg max∏ J(π). To settle the partial observable problem, history
Ti ∈ Ti = (Y × Ai)* is often used to replace observation o% ∈ Y. As for policies in CTDE, each
agent i has an individual policy ∏(ai∣τi) and thejoint policy π is the product of each πi. Though We
calculate individual policy as ∏i(ai∣τi) in practice, we will use ∏i(a∕s) in analysis and proofs for
simplicity.
Entropy regularization adds the logarithm of current policy to the reward function. It mod-
ifies the optimization objective as Jent(∏) = En [Pt=0 Yt (r(st, at) 一 λlog∏(at∣st))]. We
also have the corresponding Q-function Qeπnt (s, a) = r(s, a) + γE [Veπnt(s0)] and V-function
Ve∏t(s) = E [Q∏nt(s, a) 一 λlogπ(a∣s)]. Given these definitions, we can deduce an interesting
property Vnt(S) = E [Q∏1t(s, a)] + λH (π(∙∣s)), where H (π(∙∣s)) represents the entropy of policy
∏(∙∣s). Vent(S) includes an entropy term which is the reason it is called entropy regularization.
4	Method
In this section, we first give the definition of divergence regularization. Then we propose and analyze
divergence policy iteration. Finally, based on divergence policy iteration, we derive the update rules
of the critic, policy, and target policy for divergence-regularized MARL.
4.1	Divergence Regularization
We maintain a target policy ρi for each agent i, which is different from the policy πi . Then we
have a joint target policy ρ = Qin=1 ρi . This joint target policy ρ modifies the objective function
as JP(π) = EnhPt=0 Yt (r(St, at) - λlog ∏(at∣St))i . That is, a regularizer log ∏(at∣St), which
describes the discrepancy between policy π and target policy ρ, is added to the reward function just
like entropy regularization.
Given ρ, we can define corresponding V-function and Q-function for divergence regularization as
follows,
V∏(s) = En X Yt(r(st, at) - λlog π(atlst))∣S0 = S	(1)
P	[t⅛	p(at|St)	J
Qn(s, a) = r(s, a) + γEs0〜P(∙∣s,a) [vρτ(s0)] .	(2)
Further, by simple deduction, we have
Vn (s)= Ea 〜n(∙∣s)
QPn (S, a) 一 λlog
π(a∣s)
P(a|S) 一
Ea〜n(∙∣s) [Qn(s, a)] - λDκL (∏(∙∣s) kρ(∙∣s)).
VPn(S) includes an extra term which is the KL divergence between π and ρ, and thus this regularizer
is referred to as divergence regularization.
3
Under review as a conference paper at ICLR 2022
4.2	Divergence Policy Iteration
From the perspective of policy evaluation, we can define an operator Γρπ as
「pQ(S, a) = r(s, a) + γEs0〜P(∙∣s,a),a0〜π(∙∣s0)
Q(s0, a0) - λlog
π(a0∣s0)
p(a0|s0).
(3)
and have the following lemma. Note that all the proofs are given in Appendix A.
Lemma 1 (Divergence Policy Evaluation) For any initial Q-function Q0 (s, a) : S × A → R, we
define a sequence Qk given operator Γρπ as Qk+1 = ΓρπQk. Then, the sequence will converge to
Qρπ as k → ∞.
After the evaluation of the policy, we need a method to improve the policy. We have the following
lemma about policy improvement.
Lemma 2 (Divergence Policy Improvement) If we define πnew satisfying
∏new (∙∣s) = arg min DKL (∏(∙∣s)∣∣u(∙∣s)),
π
(4)
where u(1s) = P([s) e*(Znold(S,)"λ)
and Z πold (s) is a normalization term, then for all actions a
and all states s we have Qρπnew (s, a) ≥ Qρπold (s, a).
Lemma 1 and 2 could be seen as corollaries of the conclusion of Haarnoja et al. (2018). Lemma 2
indicates that given a policy πold , if we find a policy πnew according to (4), then the policy πnew is
better than πold .
Lemma 2 does not make any assumption and is for general settings. Further, the policy improvement
can be established and simplified based on value decomposition. In the following, we give an example
for linear value decomposition like DOP (i.e., Q(s, a) = Pi ki(s)Qi(s, ai) + b(s)) (Wang et al.,
2021b).
Lemma 3 (Divergence Policy Improvement with Linear Value Decomposition) If Q-functions
satisfy Qρπ (s, a) =	i ki(s)Qπρi (s, ai) + b(s) and we define πni ew satisfying
∏n ew(∙∣s) = arg min DKL (∏i(∙∣s)M(∙∣s))	∀i ∈ I,
πi
where Ui(∙∣s)
Pi("s)
exp kki(s)QPold (s,∙)∕λ
Znold (S)
i
and Z πold (s) is a normalization term, then for all
actions a and all states s we have Qρπnew (s, a) ≥ Qρπold (s, a).
Lemma 3 further tells us that if the MARL setting satisfies the linear value decomposition condition,
then each agent can optimize its individual policy with an objective of its own individual Q-function,
which immediately improves the joint policy. By combining divergence policy evaluation and
divergence policy improvement, we have the following theorem of divergence policy iteration.
Theorem 1 (Divergence Policy Iteration) By iteratively using Divergence Policy Evaluation and
Divergence Policy Improvement, we will get a sequence Qk	and this sequence will converge to the
optimal Q-function QP and the corresponding policy sequence will converge to the optimal policy
_*
πρ .
Theorem 1 shows that with repeated application of divergence policy improvement and divergence
policy evaluation, the policy can be monotonically improved and converge to the optimal policy. Let
πp, Vp(s), and Qp(s, a) denote the optimal policy, Q-function, and V-function respectively, given a
target policy ρ. We have the following proposition.
Proposition 1 If πp = argmax∏ Jp(∏) ,and VP=(S) = VnP(S) and Qp(s, a) = QP (s, a) are
respectively the corresponding Q-function and V-function of πp, then they satisfy the following
4
Under review as a conference paper at ICLR 2022
properties:
∏P(a∣s) (X P(α∣s)exp((r(s, a) + [Es，〜P(∙∣s,α) "(s')]) /λ)	(5)
VPi(S) = λ log X p(a|S)eχp ((r(s, a) + YEs，〜P (∙∣s,α) [V7(SO)]) /λ)	⑹
a
Qp(s, a) = r(s, a) + γλEs0〜P(∙∣s,α) [logX ρ(a∣s) exp (Qp(s0, a0)∕λ) ].	(7)
a0
With all these results above, we have enough tools to obtain the practical update rule of the critic,
policy and target policy of DMAC.
4.3 Divergence-Regularized Critic
From Proposition 1, we can obtain
π* (a∣s) =	P(HS) exP	((r(S, a) +	YEs，〜P(∙∣s,α)	[Vi (S')])	∕λ)	=	P(HS)	exP	(QP(S, a"λ)
ρp α s	Pbp(b|S)exP	((r(S,b)	+ YEs，〜P(∙∣s,b)	[/3)])	/λ)	exp	(VPt(S)∕λ)
=ρ(a∣S) exp ((QpG a) - VPi(s)) ∕λ) .	(8)
By rearranging the equation, we have
V；(s)= Qp(s, a) - λlog ∏p(aS),	(9)
which is tenable for all actions a ∈ A. Therefore, we have the following corollary.
Corollary 1
0 0	πpi (a0 |S0 )
QP(S, a) = r(s, a) + YEs，〜P(∙∣s,α),α0〜∏ρ(∙∣s0) QP(S , a ) — λ log [0.)
is tenable for all actions a0 ∈ A.
Corollary 1 gives an iterative formula for QiP(S, a), with which we can design a loss function and
update rule for learning the critic,
Lq = E h(Qφ(s, a) — y)2i , where y = r(s, a) + Y(Q$ (s0, a0) 一 λlog π(a |S0)) ,	(10)
P(a|S)
where φ and φ are respectively the weights of Q-function and target Q-function.The update of
Q-function is similar to that in general MDP, except that the action for next state could be chosen
arbitrarily while it must be the action that maximizes Q-function for next state in general MDP. This
property greatly enhances the flexibility of learning Q-function, e.g., we can easily extend it to TD(λ).
4.4 Divergence-Regularized Actors
DAPO (Wang et al., 2019) analyzes the divergence-regularized MDP from the perspective of policy
gradient theorem (Sutton et al., 2000) and gives an on-policy update rule for single-agent RL. Unlike
existing work, we focus on a different perspective and derive an off-policy update rule by taking into
consideration the characteristics of MARL.
From Lemma 2, we can obtain an optimization target for policy improvement,
argm∏nDKL卜(忖|以.卜)呻[个：；")
Then, we can define the objective of the actors,
arg maxX∏(a∣s) (Qn(s, a) — λlog ；：：：)).
Ln = ES 〜D
X n(a|s) (Qn (s, a)-λ log πas)]
(11)
where D is the replay buffer. Suppose each individual policy πi has a corresponding parameterization
θi . We can obtain the following policy gradient for each agent with some derivation and the detail is
given in Appendix A.6,
Vθi Ln
ES〜D,a〜n
∣Vθilog ∏i (ai|s) (Qn (s, a) — λ log
π(a∣s)
P(a|s)
(12)
—λ
5
Under review as a conference paper at ICLR 2022
We need to point out that the key to off-policy update is that Lemma 2 does not limit the state
distribution. It only requires the condition is satisfied for each state. Therefore, we can maintain a
replay buffer to cover different states as much as possible, which is a common practice in off-policy
learning. DAPO uses a similar formula to ours, but it obtains the formula from policy gradient
theorem, which requires the state distribution of the current policy.
Further, we can add a counterfactual baseline to the gradient. First, we have the following equation
about the counterfactual baseline (Foerster et al., 2018),
Es〜D,a〜∏ Vθi log∏i(ai∣s)b(s, a-i)] = 0,	(13)
where a-i denotes the joint action of all agents except agent i. Next, we take the baseline as
b(s,a-i) = Eai〜∏i[(Q∏(s, a) - λ log ∏(as - λ)].
(14)
Then, the gradient for each agent i can be modified as follows,
VθiL∏ = E[Vθi log∏i(ai∣s)(Q∏(s, a) — λlog ；(；；) - λ - b(s,a-i))]
=E[Vθi log∏i(ai∣s)(Q∏(s, a) — λlog ：(：||：) - Eai〜∏JQ∏(s, a)] + λDκL(∏i(∙∣s)kPi(∙∣s)))].
In addition to variance reduction and credit assignment, this counterfactual baseline eliminates the
policies of other agents from the gradient. This property makes it convenient to calculate the gradient
and easy to select the target policy for each agent. Moreover, if the linear value decomposition
condition is satisfied, we have the following gradient formula,
VθiL∏ = E[Vθi log∏i(ai∣s)(ki(s)APi(s, ai) — λlog ：；；[：) + λDκL(∏i(∙∣s)∣∣Pi(∙∣s)))], (15)
where Ani(s,ai) = QPi(s,ai) — EGi〜∏i [Qρi(s, ai)].
4.5 Target Policy
We have discussed the update rule of the critic and actors, and now we focus on the selection and
update rule of the target policy. With the update rules above, we can obtain divergence policy iteration
given a fixed target policy ρ. Then we need to devise the update rule of ρ to prevent the bias of
regularization and benefit the learning procedure.
Intuitively, this regularizer log ；(；：||；：) could help to balance exploration and exploitation. For
example, for some action a, if ρ(a∣s) > π(a∣s), then the regularizer is equivalent to adding a
positive value to the reward and vice versa. Therefore, if we choose previous policy as target
policy, the regularizer will encourage agents to take actions whose probability has decreased and
discourage agents to take actions whose probability has increased. Additionally, the regularizer
actually controls the discrepancy between current policy and previous policy, which could stabilize
the policy improvement (Kakade and Langford, 2002; Schulman et al., 2015; 2017).
To derive the update rule of target policy, we need to further analyze the regularizer theoretically. Let
μ∏ denote the state-action distribution given a policy π. That is, μ∏(s, a) = dπ(s)π(a∣s), where
dπ(s) = Pt=o γt Pr(St = s∣∏) is the stationary distribution of states given π. With μ∏, we can
rewrite the optimization objective Jρ(π) as follows,
Jρ(∏) = fμ∏(s, a)r(s, a)-
λ X μ∏(s，a)log 鬻)
s,a
E μ∏(s, a)r(s, a) - λDc (μ∏ kμρ),
s,a
(16)
where DC (μ∏∣∣μρ) = Ps。μ∏(s, a) log ；(；||；) is a Bregman divergence (NeU et al., 2017). There-
fore, the objective of the divergence regularized MDP can be represented as (17)
∏* = argmaxJ2 N∏(s, a)r(s, a) — λDc (μ∏ ∣∣μρ)	(17)
6
Under review as a conference paper at ICLR 2022
With this property, similar to Neu et al. (2017) and Wang et al. (2019), we can use the following
iterative process,
∏t+1 = argmaxJ2 N∏(s, a)r(s, a) - λDc (μ∏ ∣∣μ∏t)	(18)
This iteration is a mirror descent process (Neu et al., 2017), so the convergence of the policy is
guaranteed. This process also guarantees that when the policy converges, DC (μ∏t+ι kμ∏t) → 0; i.e.,
the regularizer will vanish. Moreover, we can obtain the following inequalities:
J(∏t+1) ≥ J(∏t+1) - λDc (μ∏t+ιkμ∏t) ≥ J(∏t) - λDo (μ∏t ∣∣μ∏t) = J(πt),
The first inequality is from DC (μ∏ ∣∣μρ) ≥ 0 and the second inequality is from the definition of πt+1.
This conclusion means the policy sequence obtained by this iteration improves monotonically in the
original MDP. With these deductions, we actually obtain the following theorem.
Theorem 2 By iteratively applying the divergence policy iteration and taking ρk as πk,the policy
sequence {πk} will converge and improve monotonically in the original MDP.
However, it is intractable to perform this update rule in practice because every iteration in (18) needs
a convergent policy. Thus, we propose an alternative approximate method. For each agent, we update
the policy ∏ and the target policy Pi as θi = θi + βVθiLn and θi = (1 一 T)θi + τθi, where β is
the learning rate, θi is the weights of Pi , and τ is the hyperparameter for soft update. Here we use
one gradient step to replace the max operator in (18). From Theorem 1 and previous discussion, we
know that optimizing Lπ can maximize Jρ(π), so we use Vθi Lπ in the gradient step for off-policy
training instead of the gradient step directly optimizing Jρ(π) in (16). Moreover, as the convergence
of (17) is guaranteed only if the target policy ρ is fixed, we softly update the target policy as the
moving average of the policy to prevent the instability caused by the large change of the target policy
and hence obtain stable policy improvement.
Now we have all the update rules of DMAC. The training of DMAC is a typical off-policy learning
process, which is given in Appendix B for completeness.
5	Experiments
In this section, we first empirically study the benefits of DMAC and investigate how DMAC improves
the performance of existing MARL algorithms in a didactic stochastic game and five SMAC tasks.
Then, we demonstrate the advantages of divergence regularizer over entropy regularizer in cooperative
MARL.
5.1	Improvements of Existing Methods
DMAC is a flexible framework and can be combined with many existing MARL algorithms. In
the experiments, we choose four representative algorithms for different types of methods: COMA
(Foerster et al., 2018) for on-policy multi-agent policy gradients, MAAC (Iqbal and Sha, 2019) for
off-policy multi-agent actor-critic, QMIX (Rashid et al., 2018) for value decomposition, DOP (Wang
et al., 2021b) for the combination of value decomposition and policy gradient. These algorithms
need minor modifications to fit the framework of DMAC. We denote these modified algorithms as
COMA+DMAC, MAAC+DMAC, QMIX+DMAC, and DOP+DMAC. Generally, our modification
is limited and tries to keep the original architecture so as to fairly demonstrate the improvement
of DMAC. The details of the modifications are included in Appendix C.1. More details about
hyperparameters are available in Appendix C. All the curves in our plots correspond to the mean
value of five training runs with different random seeds, and shaded regions indicate 95% confidence
interval.
5.1.1	A Didactic Example
We first test the four groups of methods in a stochastic game where agents share the reward. The
stochastic game is generated randomly for the reward function and transition probabilities with
30 states, 3 agents and 5 actions for each agent. Each episode contains 30 steps in this game.
The performance of these methods is illustrated in Figure 1. We could find that DMAC performs
7
Under review as a conference paper at ICLR 2022
SPJeMe BPOS-de
step	step	step	step
Figure 1: The learning curves in terms of episode rewards of COMA, MAAC, QMIX and DOP
groups in the randomly generated stochastic game.
better than the baseline at the end of the training in all the four groups. Moreover, COMA+DMAC,
QMIX+DMAC and MAAC+DMAC learn faster than their baselines. Though DOP learns faster than
DOP+DMAC at the start, it falls into a sub-optimal policy and DOP+DMAC finds a better policy in
the end.
We also show the benefit of exploration in this stochastic game
for the convenience of statistics. We evaluate the exploration
in terms of the cover rate of all state-action pairs, i.e., the
ratio of the explored state-action pairs to all state-action pairs.
The cover rates of COMA and COMA+DMAC are illustrated
in Figure 2. We use COMA here as a representation of the
traditional policy gradient method in cooperative MARL. We
could find that the cover rate of COMA+DMAC is higher
than COMA, which can be an evidence for the benefit of
exploration of DMAC. The cover rates of other three groups
of algorithms are available in Appendix D.
Stochastic Game
0.θ
J*。。
0.0
0	20000 40000 60000 B0000 100000
step
Figure 2: The learning curves in
terms of cover rates of COMA and
COMA+DMAC in the randomly
generated stochastic game.
5.1.2	SMAC
We test all the methods in five tasks of StarCraft Multi-Agent Challenge (SMAC) (Samvelyan et al.,
2019). The introduction of the SMAC environment and the training details are included in Appendix
C. The learning curves in terms of win rate of all the methods in the five SMAC tasks are illustrated
in Figure 3 (four columns for four groups of algorithms and five rows for five different maps in
SMAC). In addition, the learning curves in terms of rewards are available in Appendix D. We show
the empirical result of DAPO in the map of 3m in the first figure of the second column in Figure 3. It
can be seen that DAPO cannot obtain a good performance in the simple task of SMAC, so we skip
it in other SMAC tasks. The reason for the low performance of DAPO may be that DAPO omits
the correction for d∏ (s)/d∏ (S) in policy update which introduces bias in the gradient of policy, and
uses V-trace as off-policy correction which however is biased. These drawbacks may be magnified
3m	3m	3m	3m
0.0	02	0.4 O.e 0.8	1.0
2S3Z	ιt6
0.0	0.2	0.4 O.e 0∙β	1.0
2s⅛	1∙6
0.0	0.2	04	0«	0∙β	1.0
2S⅛	m
1.00
产
≡l0∙50
h.25
0.00
1.00
0.75
0.50
0.25
0.00
1.00
0.75
0.50
0.25
0.00
0.0	0.2	0.4 O.e 0∙β	1.0
step	lβ6
0.0	0.2	04	0«	0.8	1.0
step	Iee
0.0	02	04 O.e 0.8	1.0
step
0.0	02	0.4 O.e 0.8	1.0
step	Iee
Figure 3: Learning curves in terms of win rates of COMA, MAAC, QMIX and DOP groups in five
SMAC maps (each row corresponds to a map and each column corresponds to a group).

8
Under review as a conference paper at ICLR 2022
step	le6	step	le6	step	Ie 6
Figure 4: The learning curves of win rates of FOP+DMAC and FOP in three SMAC maps.
in MARL settings. The superiority of our naturally off-policy method over the biased off-policy
correction method can be partly seen from the large performance gap between COMA+DMAC and
DAPO.
In all the five tasks, MAAC+DMAC outperforms MAAC significantly, but MAAC+DMAC does not
change the network architecture of MAAC, which shows the benefits of divergence regularizer. As
for the result of COMA and COMA+DMAC. We find that COMA+DMAC has higher win rates than
COMA in most cases at the end of the training, which can be attributed to the benefits of off-policy
training and exploration of divergence regularizer. Though in some cases COMA learns faster than
COMA+DMAC, it falls into sub-optimal in the end. This phenomenon can be observed more clearly
in the plots of episode rewards in Appendix D, especially in the hard tasks like 3s5z. This can be an
evidence for the advantage of divergence regularizer which helps the agents find a better policy.
The stable policy improvement of divergence regularizer can be showed in the variance of the learning
curves especially in the comparison between QMIX and QMIX+DMAC. In most tasks, we find
that QMIX+DMAC learns substantially faster than QMIX and gets higher win rates in harder tasks.
The results of DOP groups are illustrated in the fourth column of Figure 3. DOP+DMAC learns
faster than DOP in most cases and finally obtains a better performance. The difference of DOP and
DOP+DMAC can also partly show the advantage of naturally off-policy method to the off-policy
correction method, as DOP+DMAC replaces the tree backup loss with off-policy TD(λ).
DMAC improves the performance and/or convergence speed of the evaluated algorithms in most tasks.
This empirically demonstrates the benefits of divergence regularizer. Moreover, the superiority of our
naturally off-policy learning over the biased off-policy correction method can be partly witnessed
from the empirical results.
5.2	Comparison with Entropy Regularization
FOP (Zhang et al., 2021) combines the value decomposition with entropy regularization, which
obtained the state-of-the-art performance in SMAC tasks. FOP has a well tuned scheme for the
temperature parameter of the entropy, so we take FOP as a strong baseline for entropy-regularized
methods in cooperative MARL. We compare FOP and FOP+DMAC in three SMAC tasks, 3s_vs_3z,
2c_vs_64zg, and MMM2, which respectively correspond to the three levels of difficulties (i.e.,
easy, hard, and super hard) for SMAC tasks. These tasks are taken from the original paper of FOP.
The modifications of FOP+DMAC are also included in Appendix C.1. The win rates of FOP and
FOP+DMAC are illustrated in Figure 4. We could find that FOP+DMAC learns much faster than
FOP in 3s_vs_3z, while it performs better than FOP in other two harder tasks. These results could be
an evidence for the advantages of DMAC and the bias of entropy regularization.
6	Conclusion
We propose a multi-agent actor-critic framework, DMAC, for cooperative MARL. We investigate
divergence regularization, derive divergence policy iteration, and deduce the update rules for the
critic, policy, and target policy in multi-agent settings. DMAC is a naturally off-policy framework and
the divergence regularizer is beneficial to exploration and stable policy improvement. Unlike entropy
regularizer, the divergence regularizer will not bias the converged policy. DMAC is also a flexible
framework and can be combined with many existing MARL algorithms with limited modification. It
is empirically demonstrated that combining DMAC with existing MARL algorithms can improve the
performance and convergence speed in a stochastic game and SMAC tasks.
9
Under review as a conference paper at ICLR 2022
References
Akshat Agarwal, Sumit Kumar, Katia Sycara, and Michael Lewis. Learning transferable cooperative
behavior in multi-agent team. In International Conference on Autonomous Agents and Multiagent
Systems (AAMAS), 2020.
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam
Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with
importance weighted actor-learner architectures. In International Conference on Machine Learning
(ICML), 2018.
Benjamin Eysenbach and Sergey Levine. If maxent rl is the answer, what is the question? arXiv
preprint arXiv:1910.01913, 2019.
Jakob N Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In AAAI Conference on Artificial Intelligence (AAAI),
2018.
Jordi Grau-Moya, Felix Leibfried, and Peter Vrancx. Soft q-learning with mutual-information
regularization. In International Conference on Learning Representations (ICLR), 2019.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. In International Conference on Machine Learning (ICML), 2017.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Conference
on Machine Learning (ICML), 2018.
Shariq Iqbal and Fei Sha. Actor-attention-critic for multi-agent reinforcement learning. In Interna-
tional Conference on Machine Learning (ICML), 2019.
Jiechuan Jiang, Chen Dun, Tiejun Huang, and Zongqing Lu. Graph convolutional reinforcement
learning. In International Conference on Learning Representations (ICLR), 2020.
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
International Conference on Machine Learning (ICML), 2002.
Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent
actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information
Processing Systems (NeurIPS), 2017.
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between
value and policy based reinforcement learning. In Advances in Neural Information Processing
Systems (NeurIPS), 2017.
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Trust-pcl: An off-policy trust
region method for continuous control. In International Conference on Learning Representations
(ICLR), 2018.
Gergely Neu, Anders Jonsson, and Viceng G6mez. A unified view of entropy-regularized markov
decision processes. arXiv preprint arXiv:1705.07798, 2017.
Frans A Oliehoek, Christopher Amato, et al. A concise introduction to decentralized POMDPs,
volume 1. Springer, 2016.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster,
and Shimon Whiteson. Qmix: monotonic value function factorisation for deep multi-agent
reinforcement learning. In International Conference on Machine Learning (ICML), 2018.
Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli,
Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The
starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning (ICML), 2015.
10
Under review as a conference paper at ICLR 2022
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning to
factorize with transformation for cooperative multi-agent reinforcement learning. In International
Conference on Machine Learning (ICML), 2019.
Peter Sunehag, GUy Lever, AUdrUnas Gruslys, Wojciech Marian Czarnecki, ViniciUs Flores Zam-
baldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-
decomposition networks for cooperative mUlti-agent learning based on team reward. In Interna-
tional Conference on Autonomous Agents and MultiAgent Systems (AAMAS), 2018.
Richard S SUtton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Richard S SUtton, David A McAllester, Satinder P Singh, and Yishay MansoUr. Policy gradient meth-
ods for reinforcement learning with fUnction approximation. In Advances in Neural Information
Processing Systems (NeurIPS), 2000.
Ahmed ToUati, Amy Zhang, Joelle PineaU, and Pascal Vincent. Stable policy optimization via
off-policy divergence regUlarization. arXiv preprint arXiv:2003.04108, 2020.
Jianhao Wang, ZhizhoU Ren, Terry LiU, Yang YU, and Chongjie Zhang. Qplex: DUplex dUeling
mUlti-agent q-learning. In International Conference on Learning Representations (ICLR), 2021a.
Qing Wang, YingrU Li, Jiechao Xiong, and Tong Zhang. Divergence-aUgmented policy optimization.
In Advances in Neural Information Processing Systems (NeurIPS), 2019.
Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang. Dop: Off-policy mUlti-
agent decomposed policy gradients. In International Conference on Learning Representations
(ICLR), 2021b.
Wenhao Yang, Xiang Li, and ZhihUa Zhang. A regUlarized approach to sparse optimal policy in
reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), 2019.
Yaodong Yang, Jianye Hao, Ben Liao, KUn Shao, GUangyong Chen, WUlong LiU, and Hongyao Tang.
Qatten: A general framework for cooperative mUltiagent reinforcement learning. arXiv preprint
arXiv:2002.03939, 2020.
Tianhao Zhang, YUeheng Li, Chen Wang, GUangming Xie, and Zongqing LU. Fop: Factorizing
optimal joint policy of maximUm-entropy mUlti-agent reinforcement learning. In International
Conference on Machine Learning, pages 12491-12500. PMLR, 2021.
11
Under review as a conference paper at ICLR 2022
A Proofs
A.1 Lemma 1
Proof. We define a new reward function
r∏(s, a) = r(s, a) - XE,,〜Pw㈤[Dkl (∏(∙∣s0)kρ(∙∣s0))],
then we can rewrite the definition of operator Γρπ as
γ∏ Q(S, a) = r∏ (S, a) + YEs，〜P(∙∣s,a),a0 〜π(∙∣s0) [Q(S0, a )] ∙
With this formula, we can apply the traditional convergence result of policy evaluation in Sutton and
Barto (2018).
A.2 Lemma 2
For the proof of Lemma 2, we need the following lemma (Haarnoja et al., 2018) about improving
policy in entropy-regularized MDP.
Lemma 4 If we have a new policy πnew and
exp (Q∏Otld (s, ∙)∕λ)
πnew = arg min DKL (π(」s) k----------ZnoId (S)-----
where Z πold (S) represents the normalization term, then we have
Qeπnntew (S, a) ≥ Qeπnotld (S, a),	∀S ∈ S,a ∈ A.
With Lemma 4, we have the following proof of Lemma 2.
TΓ* f	ɪ , zA 1	,1	,1	1 r' ∙ , ∙	∙ -TA GhL EI	F	ZAk /	∖	√^ιk /	∖	,
Proof.	Let Q be the	same as the definition in Proof A.5. Then	we have	Qπ (S, a)	= Qρπ (S, a)	+
λlogρ(a∣s), ∀π.
According to Lemma 4,
∏new (S, ∙) = arg min DKL
π
π3s)k
exp
Z πold (S)
Qπnew(s, a) ≥ Qπold (s, a),	∀a ∈ A.
With the definition, we have
exp(Qπold (s, ∙)∕λ)	eχp(Q∏old (s, ∙)∕λ) ∖
DKL (π3s)k —ZnoT(S- = = DKL /(忖胸忖-z∏οd(S- J
πnew = π new
QPnew(s, a) = Q new(s, a) - λ log ρ(a∣S) ≥ Q old (s, a) - λlog ρ(a∣S) = Qρold (s, a).
A.3 Lemma 3
Proof. From the equation
πn ew = argm∏aX X ni(ai|S) d(S)QnoId Gai)- λ log π≡r),
ai
we can obtain
X πn ew(IaiIS) (ki(S)QnoId (S, ai) - λ log
πn ew (ai|S)
ai
≥ X∏0id(αi∣S) (ki(S)QnoId(S,ai) - λlog
ai
Pi(ai∣S) )
.π0ld(ai|S)
'Pi(ai∣S)
(19)
12
Under review as a conference paper at ICLR 2022
By taking expectation on the both side of (19), we can obtain the followings.
X n-i(a-i|S) X πn ew (ai|S) (ki (S)QPold (S,ai) — λ log
a-i
ai
n。ew(ai|S)
Pi(ai∣S)
≥ X∏-i(a-i∣S) X∏0id(ai∣S) (ki(S)Qpοld(S,ai) — λlog
a-i
∀i ∈ I	∀∏-i
ai
π0 ld(ai|S)
Pi(ai∣S)
(20)
Moreover, we can easily have the following derivation,
X πnew (ai|S)d(S)QnOld (S,ai)+b(S)—λ log ⅛⅛)
a
X u1(a-i|s) X πn ew (ai|S) (ki(S)QncJd (S,ai) + b(S)- λ log
a-i
ai
πn ew(ai|S)
Pi(ai∣S)
(21)
£u2(a-i|s) Ennew(ai|s) ( ki(s)Qπ°ld(s, ai) + b(s) — λlog
a-i
∀u1 , u2 .
ai
πn ew(ai|S)
Pi(ai∣S)
Then we have
,〜πnew
π	πnew (a|S)
QP	(S，a)-log ʒ(ɑ^r]
X∏new(a∣S) X (ki(S)QnOld(S,ai) + b(S) — λlog
ai
X ∏new (a∣S) X (ki (S)Qpold (S,ai) + b(S) — λ log
ai
a。ew (ai|S)
Pi(ai∣S)
πn ew (ai|S)
Pi(ai∣S)
∑∑∏-eiw (a-i∣s)∑πni ew (ai |s) ki(s)QρπCi ld (s, ai) + b(s) - λlog
i a-i
ai
πnew(ai|S) A	(22)
Pi (ai∣S),
XX
π-d(a-i 1 s) X πnew(ai IS) (ki (S)QnOld (s, ai) + b(s) — λ log
i a-i
ai
πn ew(ai|S)
Pi(ai |s)
≥XX∏-d(a-i∣s) X∏0id(ai∣s) (ki(s)QnC)Id(s, ai) + b(s) — λlog
i	a-i
Vρπold(s)
ai
π0 id (ai| S)
Pi(ai∣S)
The fourth equation is from (21) and the fifth inequality is from (20).
By repeatedly applying (22) and the relation Qρπold (s, a) = r(s, a) + γEs0 Vρπold (s0) , we can
complete the proof as followings.
Qρπold (s, a) = r(s, a) + γEs0 Vρπold (s0)
≤ r(s, a) + γEs0 Ea0
〜πnew
=r(s, a) + γEs0 Ea0〜.
πnew
Q∏θld (S0,, a')- log πneWΓ U
r(S0, a')+ γEs00[V∏οld 3')]— log
ρ(a |S )
≤ Qρπnew (s, a)

13
Under review as a conference paper at ICLR 2022
A.4 Theorem 1
Proof. First, we will show that Divergence Policy Iteration will monotonically improve the policy.
From Lemma 2, we know that
Vnnew (S)= Ea 〜∏new (∙∣s)	Qnnew S a) - λ log R1
≥ Ea~∏new (∙∣S)	Q∏old(S, a) - λlog πρW⅛)^
≥ Ea~∏old(∙∣s)	QnoIdGa) - λ log %⅜r
Vρπold(s).
The first inequality is from the conclusion of Lemma 2 that
∏(∙∣s)kexp (Q∏Us,"λ
Z πold (s)
Qρπnew (s, a) ≥ Qρπold (s, a) ∀a ∈ A,
and the second inequality is from the definition of πnew that
πnew = arg min DKL
π
Here we have V πnew (s) ≥ V πold (s), ∀s ∈ S, and thus Jρ(πnew) ≥ Jρ(πold). So, Divergence
Policy Iteration will monotonically improve the policy.
Since the Qρπ is bounded above (the reward function is bounded), the sequence of Q-function Qk
of Divergence Policy Iteration will converge and the corresponding policy sequence will also converge
to some policy π∞nv. We need to show ∏conv = πp.
VΠcθnv (S)= Ea〜∏conv(∙∣s)
≥ Ea〜π(∙∣s)
≥ Ea〜π(∙∣s)
Qρπconv (s, a) - λlog
∏conv(a∣s)
P(RS)
π(a∣s)
P(HS) 一
Qρπconv (S, a) - λlog
Qρπconv (S0,a0) - λlog
π(a0∣s0)
ρ(a1s)
- λlog
π(a∣s)
P(HS) 一
≥ Vρπ(S).
The first inequality is from the definition of πconv that
eχp(Q∏conv G ∙"λ)∖
∏conv = arg min DKL ∏(∙∣s)k---------~~r∖----------
π	Z πconv (S)
and all the other inequalities are just iteratively using the first inequality and the relation of Qρπ and
Vρπ. With iterations, we replace all the πconv with π in the expression of Vρπconv (S) and finally we
get Vρπ(S). Therefore, we have
Vρπconv (S) ≥ Vρπ(S) ∀S ∈ S ∀π ∈ Π
Jρ(πconv) ≥ Jρ(π)	∀π ∈ Π
*
πconv = πρ .
A.5 Proposition 1
Before the proof of Proposition 1, we need some results about the optimal Q-function Qe*nt , the
optimal V-function Ve*nt , and the optimal policy πe*nt in entropy-regularized MDP. We have the
following lemma (Nachum et al., 2017).
14
Under review as a conference paper at ICLR 2022
Lemma 5
πent(s, a) (X exp ((r(s, a)十 γ%〜我同。)[V3ζt (s,)]) ∕λ)
VSnt(S) =λiɑgXexp ((km+1%〜p(.|s,。)[Vn/SO)D IN
a
QSnt(S, a) = r(s,a) + γλEs,^p(,同。)log£exp(Qent(s', a0)∕λ)
-	a0	-
With Lemma 5, we can complete the proof of Proposition 1.
Proof.. Let r(s, a) = r(s, a) + λlog ρ(α∣s), we consider the objective function
J(π) = E∏ X Yt (r(st, at) - λlogπ(αt∣st)).
_t=0	_
τr,c*∕l∖ W*∕∖ IZA*/ ∖ 1 ,1	1 ∙	J 1 Λ ∙ TTC J	tzʌ C ,♦ C~
Let Tre(a∣s),Ve(s) and Qe(s, a) be the corresponding optimal policy, V-function and Q-function of
J(π). By definition we can obtain
π*(a∣S) = πP (a∣s)
Ve(s) = E Q〜∏*(.|s),s，〜p(∙∣s,α) [r (s, a) + γV*(s') - λlogπ *(a∣s)]
-	- o	∏P (a∣s)"
=E a〜πp(∙∣ s),s0〜P(∙∣ s,a) r (s, a) + YV (s ) - λ log ρ(a∣ S)
=崂(S)
Q*(s, a) = r (s, a) + EsO〜P(.∣s,a) [V*(s')]
=r(S, a) + Es,〜P(∙∣s,a) IXe(s')] + λlogρ(a∣S)
=Qp(s, a) + λlog ρ(a∣s).
According to Lemma 5, we have
πp(a∣s) = πe(a∣s) X exp (卜(s, a) + 祖0〜p(.∣s,a) [V*(s')]) ∕λ)
=ρ(a∣s)exp ((r(s, a) + 祖，〜p(∙∣s,a) "(s')]) ∕λ)
vρ (S) = V e(s)
=λlogXexp ((r (s, a) + 祖0〜p(.∣s,a) [V*(s')]) ∕λ)
a
=λlog X ρ(a∣s)exp ((r(s, a) + γEs,〜p(.∣s,a) "(s')]) ∕λ)
a
..,	.	O , ,	.	. .	..
Qρ(s, a) = Q (s, a) - λlog ρ(a∣s)
=r (s, a) + γλEs,〜p(.∣s,a) logXexp (Qe(s/, a')∕λ) - λlogρ(a∣s)
_	a0	_
=r(s, a) + γλEs,〜p(.∣s,a) logɪ^ ρ(a∣s)exp (QP(S/, a')∕λ).
_	a0	_
□
15
Under review as a conference paper at ICLR 2022
A.6 Derivation of Gradient
NeiLn = Es 〜D
X Vθiπ(als) (Qn(S, a) — λlog Ba* *) + π(als)Vθi(-λlog π(alS))
a	∖	p(a|S) /	p(a|S)
Es〜D
E∏(a∣s)Vθi log∏i(α∕s)
a
Qρn(S, a) - λlog
(a|S)
P(a|s)
一λπ(a∣s)Vθi log∏i(ai|s)
,a〜n
∣^Vθi log∏i(αi∣s) (Qn(s, a) - λlog
(a|S)
P(a|s)
-λ
B Algorithm
Algorithm 1 gives the training procedure of DMAC.
Algorithm 1 DMAC
1:	for episode = 1 to m do
2:	Initialize the environment and receive initial state S
3:	for t = 1 to max-episode-length do
4:	For each agent i, select action a% 〜∏i(∙∣s)
5:	Execute joint-action a = (aι, a2,…，an) and observe reward r and next state s0
6:	Store (S, a, r, S0) in replay buffer D
7:	end for
8:	Sample a random minibatch of K samples from D, {(Sk, ak, rk, S0k)}K
9:	for agent i = 1 to n do
10:	Update policy πi: θi = θi + βVθiLn
11:	Update target policy ρi∖ 仄=(1 一 T)。+ τθ%
12:	end for
13:	Update critic: φ = φ 一 αVφLQ
14:	Update target critic: φ = (1 一 τ)φ + τφ
15:	end for
C Implementation Details
SMAC is a MARL environment based on the game StarCraft II (SC2). Agents control different units
in SC2 and can attack, move or take other actions. The general mode of SMAC tasks is that agents
control a team of units to counter another team controlled by built-in AI. The target of agents is to
wipe out all the enemy units and agents will gain rewards when hurting or killing enemy units. Agents
have an observation range and can only observe information of units in this range, but the information
of all the units can be accessed in training. We test all the methods in totally 8 tasks/maps: 3m, 2s3z,
3s5z, 8m,1c3s5z,3s_vs_3z,2c_vs_64zg, and MMM2.
In SMAC tasks, we evaluate 20 episodes in every 10000 training steps in the one million steps
training procedure for COMA, MAAC, QMIX and in every 20000 time steps for DOP and FOP. In
evaluation, we select greedy actions for QMIX and FOP (following the setting in the FOP paper)
and sample actions according to action distribution for stochastic policy (COMA, MAAC, DOP and
divergence-regularized methods).
C.1 Modifications of the Baseline Methods
The modifications of the baseline methods, COMA,MAAC,QMIX,DOP and FOP, are as follows.
• COMA. We keep the original critic and actor networks and add a target policy network with the
same architecture as the actor. As COMA is on-policy but COMA+DMAC is off-policy, we add a
replay buffer for experience replay.
• MAAC already has a target policy for stability, so we do not need to modify the network architecture.
We only change the update rule for the critic and actors.
16
Under review as a conference paper at ICLR 2022
Stochastic Game
COMA+DMAC
---COMA
Stochastic Game
Stochastic Game
Stochastic Game
O 20000 40000 60000 80000 IOOOOO O 20000 40000 60000 80000 IOOOOO O 20000 40000 60000 80000 IOOOOO O 50000 IOOOOO 150000 200000
step	step	step	step
Figure 5: Learning curves in terms of cover rates of COMA, QMIX, MAAC and DOP groups in the
randomly generated stochastic game.
•	QMIX is a value-based method, so we need to add a policy network and a target policy network for
each agent. We keep the original individual Q-functions to learn the critic in QMIX+DMAC. In
divergence-regularized MDP, the max operator is not needed in the critic update, so we abandon
the hypernet and use an MLP, which takes individual Q-values and state as input and produces the
joint Q-value. This architecture is simple and its expressive capability is not limited by QMIX’s
IGM condition.
•	DOP. We keep the original critic and actor networks and add a target policy network with the same
architecture as the actor. We keep the value decomposition structure and use off-policy TD(λ) for
all samples in training to replace the tree backup loss and on-policy TD(λ) loss.
•	FOP. We replace the entropy regularizers with divergence regularizers in FOP and use the update
rules of DMAC. We keep the original architecture of FOP.
C.2 Hyperparameters
As all tasks in our experiments are cooperative with shared reward, so we use parameter-sharing
policy network and critic network for MAAC and MAAC+DMAC to accelerate training. Besides, we
add a RNN layer to the policy network and critic network in MAAC and MAAC+DMAC to settle the
partial observability.
All the policy networks are the same as two linear layers and one GRUCell layer with ReLU activation
and the number of hidden units is 64. The individual Q-networks for QMIX group is the same as the
policy network mentioned before. The critic network for COMA group is a MLP with three 128-unit
hidden layers and ReLU activation. The attention dimension in the critic networks of MAAC group
is 32. The number of hidden units of mixer network in QMIX group is 32. The learning rate for
critic is 10-3 and the learning rate for actor is 10-4. We train all networks with RMSprop optimizer.
The discouted factor is γ = 0.99. The coefficient of regularizer is λ = 0.01. The td_lambda factor
used in COMA group is 0.8. The parameter used for soft updating target policy is τ = 0.01. Our
0.0	0.2	04 O.e 0∙β	1.0
2s3z	le6
MpleMeJJPOSId.
0.0	0.2	04 O.e 0∙β	1.0
1C3S5Z	ιβ6
0.0	0.2	0.4	0«	0.8	1.0
step	ie6
0.0	02	0.4	O.«	0.8	1.0
step	1∙6
0.0	0.2	0.4	0«	0∙β	1.0
step	ie6
Figure 6: Learning curves in terms of mean episode reward of COMA, MAAC, QMIX, and DOP
groups in five SMAC maps (each row corresponds to a map and each column corresponds to a group).
0.0	0.2	04	O.«	0∙β	1.0
step
17
Under review as a conference paper at ICLR 2022
3s_vs_3z
0 5 0 5
2 11
Sp∙IBM9Jpod
0.0	0.5	1.0	1.5	2.0	0.0	0.5	1.0	1.5	2.0	0.0	0.5	1.0	1.5	2.0
step	le6	step	le6	step	le6
Figure 7:	Learning curves in terms of mean episode rewards of FOP+DMAC and FOP in three SMAC
maps (each column corresponds to a map).
SP-BMaJωpo-dω
CDM
0 5 0 5
1 -
CDM
0 5 0 5
1 -
0	2000	4000	6000	8000	10000	0	2000	4000	6000	8000	10000
step	step
Figure 8:	Learning curves in terms of mean episode rewards of COMA and DOP groups in the CDM
environment used by the DOP paper.
code is based on the implementation of PyMARL (Samvelyan et al., 2019), MAAC (Iqbal and Sha,
2019), DOP (Wang et al., 2021b), FOP (Zhang et al., 2021) and an open source code for algorithms
in SMAC (https://github.com/starry-sky6688/StarCraft).
C.3 Experiment Settings
We trained each algorithms for five runs with different random seeds. In SMAC tasks, we train each
algorithm for one million time steps in each run for COMA, QMIX, MAAC, and DOP groups and
two million timesteps for FOP groups. We do all the experiments by a server with 2 NVIDIA A100
GPUs.
D Additional Results
Sa4e U-M
MMM2
MMM2
SpJeMaJ∙po-d∙
MMM2
MMM 2
W 8 6 4
0.0	0.5	1.0	1.5	2.0	0.0	0.5	1.0	1.5	2.0	0.0	0.5	1.0	1.5	2.0	0.0	0.5	1.0	1.5	2.0
step	1≡6	step	1≡6	step	1≡6	step	1≡6
Figure 9:	Learning curves of QMIX, DOP, COMA and MAAC groups on the map MMM2 in SMAC.
Figure 5	shows the learning curves in terms of cover rates of COMA, QMIX, MAAC and DOP
groups in the randomly generated stochastic game.
Figure 6	shows the learning curves of COMA, MAAC, QMIX and DOP groups in terms of mean
episode rewards in each SMAC map.
Figure 7	shows the learning curves of FOP+DMAC and FOP in terms of mean episode rewards in
3s_vs_3z, 2c_vs_64zg and MMM2.
Figure 8	shows the learning curves in terms of mean episode rewards of COMA and DOP groups in
the CDM environment used by the DOP paper.
Figure 9	shows the learning curves of QMIX and DOP groups in terms of win rate, and COMA and
MAAC groups in terms of episode reward on the map MMM2 in SMAC.
18