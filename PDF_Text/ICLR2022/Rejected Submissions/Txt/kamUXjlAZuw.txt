Under review as a conference paper at ICLR 2022
On Learning with Fairnes s Trade-Offs
Anonymous authors
Paper under double-blind review
Ab stract
Previous literature has shown that bias mitigating algorithms were sometimes
prone to overfitting and had poor out-of-sample generalisation. This paper is first
and foremost concerned with establishing a mathematical framework to tackle
the specific issue of generalisation. Throughout this work, we consider fairness
trade-offs and objectives mixing statistical loss over the whole sample and fairness
penalties on categories (which could stem from different values of protected at-
tributes), encompassing partial de-biasing. We do so by adopting two different but
complementary viewpoints: first, we consider a PAC-type setup and derive proba-
bilistic upper bounds involving sample-only information; second, we leverage an
asymptotic framework to derive a closed-form limiting distribution for the differ-
ence between the empirical trade-off and the true trade-off. While these results
provide guarantees for learning fairness metrics across categories, they also point
out to the key (but asymmetric) role played by class imbalance. To summarise,
learning fairness without having access to enough category-level samples is hard,
and a simple numerical experiment shows that it can lead to spurious results.
1	Introduction
Reducing bias in an algorithm requires a number of steps; first, specify the fairness definitions
(and thus fairness metrics) that apply, second, encode them with a penalty so as to measure the
discrepancy between outcomes and the perfect fairness scenario, third, choose the trade-off between
the original statistical goal and fairness constraints, and, fourth, pick a method to debias (at least
partially) the model. There are a number of obstacles to this programme. Indeed, debiasing may
come at a cost (Rodolfa et al. (2020)), but there are also theoretical reasons behind this claim.
Jointly tackling multiple fairness definitions is usually difficult if not impossible without a decrease
in model performance due to a series of impossibility theorems (Kleinberg et al. (2017); Pleiss et al.
(2017); Chouldechova (2017); Kim et al. (2020)). Note that these results apply not only to fairness-
performance but also to fairness-fairness trade-offs.
Research has recently been undertaken to tackle specifically learnability and generalisation in
fairness-related problems. In particular, Oneto et al. (2020); Oneto et al. (2020) derive provable
probabilistic upper bounds in a fairness setting where one estimates an overall statistical loss and
monitors disparities across categories. In Chen et al. (2018), the authors have applied bias-variance
decomposition techniques to disentangle the sources of unfairness in a classifier. While this is not
directly related to this present work, the principle of risk decomposability turns out to be fruitful in
both setups. Finally, in Agrawal et al. (2020), a limiting distribution is established in the simple case
where there is a partial requirement involving a fairness metric with two categories.
Our contributions. The novelty of our work consists of the application of known techniques such
as PAC inequalities and the central limit theorem to the problem of learning under fairness con-
straints:
•	First, we develop a PAC framework for fairness trade-offs, generalising results from Donini
et al. (2018); Oneto et al. (2020) and tackling explicitly partial debiasing (see Kim et al.
(2020); Agrawal et al. (2020)).
•	Second, we show that certain properties of the probabilistic upper bounds lead to the need
for sample-efficient bias mitigation techniques. In particular, we show that a new quantity,
ZS, which can be understood as measure of sample concentration, plays a crucial role. We
1
Under review as a conference paper at ICLR 2022
also build on Agrawal et al. (2020) and develop an asymptotic framework with a known
limiting distribution; this is useful as PAC bounds may not always be very sharp.
•	Third, in both frameworks, we put forward the decomposability of the learning risk, ex-
pressed as an upper bound in the PAC realm and a limiting variance in the asymptotic one.
•	Last, we illustrate our results on real-life data and show that generalisation is indeed diffi-
cult.
2	Fairnes s metrics and loss functions
2.1	Set-up and definitions
In this article, We define S = 1, ∙∙∙ ,C to be a categorical (protected) attribute, X ∈ X ⊂ Rd to
be a set of non-protected features excluding s. y ∈ { - 1, +1} is a binary outcome variable and
y ∈ { - 1, +1} is an estimator for y. Finally, Z = (x, y, s). Note that y is derived from a learner
h ∈ H, Where H is a given functional space. Furthermore, We define S to be the in-sample (training)
empirical distribution, and D to refer to the true distribution.
Throughout this paper, We consider the case of an overall objective as a statistical performance
objective, L0, plus a fairness loss function φ. The trade-off is tuned by a hyper-parameter λ ≥ 0.
As usual, the aim is to minimise the overall objective, i.e., minimise the statistical loss and the lack
of fairness. This set-up is typical for partial debiasing and can be found in Kim et al. (2020).
Definition 1. The overall objective or fairness trade-off is defined as
LD(h) = LD(h)+ λφ (LD,ι(h),LD,ι(h),…，LD,c(h)"(h))
= L0D(h) + λφ(L±D(h)),
Where We have used the standard notations (see Shalev-ShWartz & Ben-David (2014)):
LD(h) = E ['0(h,z)]
L+D,a(h) = E `+ (h, z)|s = a,y = 1
L-D,a(h) = E [`- (h,z)|s = a,y = -1]
and the vector notation L±D (h)
tions `0, `+ and `- .
hLD,1(h),LD,1(h),…ID,。(h),LD,c(h)]T,
for some func-
Remark 1. In the Whole paper, We assume that there exists a uniform bound B > 0 on all functions
',i.e., |'(h, z)| ≤ B for all h and z. ' refers to any such function in what follows.
Let us detail tWo particular cases. If λ = 0, then the overall objective boils doWn to the usual risk
minimisation problem. If '0 = 0 or λ → +∞, then the trade-off becomes a fairness constraint.
We are now in a position to define the empirical counterpart by considering the empirical distribution
rather than the true distribution. With the additional notations Na = {i ∈ {1, ∙∙∙ ,n} ; Si = a,yi =
±1}, ∣N± | = n±, the corresponding sample versions can be defined as LS(h) = n Pj==r '0(h,Zi),
LS,a(h) = n+ Pi∈N+t+lh,zi) and L-,a㈤=n- Pi∈N- '-(h,zi), where na = n+ + n-,
n = PaC=1na, leading to
LS (h) = LS (h)+ λφ (L±(h))	(1)
For the sake of simplicity, we will generally refer to φ(LT±(h)) as φT (h) for T = D, S.
2.2	Fairnes s definitions and metrics
One can find multiple technical definitions of fairness in the literature; they have been reviewed
in various papers (Narayanan, 2018; Verma & Rubin, 2018; Berk et al., 2018; Kim et al., 2020;
Agrawal et al., 2020). We offer an overview of the most frequent metrics in Table 1.
We note that all fairness metrics considered here can be expressed as an equality requirement on
probabilities, hence on the expectation of indicator variables. These fall within our framework.
However, this framework can also handle other types of functions.
2
Under review as a conference paper at ICLR 2022
Fairness metric	Reference	Equality requirement
Equalized false omission rate	(Berketal.,2018)	二	P(y = 1|y = -1,s = a) = P(y = 1|y = -1)
Predictive parity	(Chouldechova, 2017)	P(y = 1∣y = 1,s = a)= P(y = 1|y = 1)
Demographic parity	(Calders & Verwer, 2010)	P(y = 1|s = a) = P(y = 1)
Equalized false negative rate	(Chouldechova, 2017)	P(y = -1|y = 1,s = a) = P(y = -1|y = 1)
Predictive equality	(Chouldechova, 2017)	P(y = 1|y = -1,s = a) = P(y = 1|y = -1)
Equality of opportunity	(Hardtetal., 2016)	P(y = 1∣y = 1,s = a)= P(y = 1|y = 1)
Equalized odds	(Hardtetal., 2016) 一	P(y = 1|y = y0, s = a) = P(y = 1|y = y0)
Table 1: Frequent fairness definitions.
2.3	Fairnes s loss functions
In addition to picking one or multiple fairness definitions, we need to specify φ to measure the
discrepancy from perfect fairness, the ideal case.
A first approach, similar to the Calders-Verwer gap (Calders & Verwer, 2010), consists of looking
at the discrepancy between two categories a and a0 :
∆TL,,a±,a0 (h) = LT±,a(h) - LT±,a0 (h),
for T ∈ {D, S}. It is worth 0 under perfect fairness, but is asymmetric and thus requires defining
an advantaged (or benchmark) category a.
To avoid the issue of asymmetry, one can follow Oneto et al. (2020), and define φ as the sum of all
absolute discrepancies across categories:
φ(LT±(h)) = X nLT+,a-LT+,a0+LT-,a-LT-,a0o,	(2)
a6=a0
for T = D, S. Notice that, thanks to the reverse triangle inequality, the function φ is Lipschitz
continuous. One could naturally weigh the various contributions to this fairness function and use
another norm than L1 . Other functions involving ratios or relative differences, are also possible.
Finally, in Kim et al. (2020), the authors consider a convex combination of multiple loss functions
φ ⑴，∙∙∙，φ(M):
M
φ(L±(h)) = Xλjφ(j) (L±(h)).
j=1
Note that if the φ(j)'s are Lipschitz-continuous (with respect to the loss vector LT(h)), with re-
spective Lipschitz constant K(j), then the overall loss φ is itself Lipschitz-continuous with constant
Kφ = PjM=1 λjK(j).
3	Learning fairness trade-offs
In this section, we consider the learning problem from different angles. We start by considering
the statistical loss L0 and derive a bound based on the Rademacher complexity observed in each
category. To do so, we borrow from the statistical learning toolbox and briefly review Rademacher
complexities. We then move on to learning the fairness part per se, namely study the generalisation
properties of φS(h). Finally, we bring the two together and derive probabilistic upper bounds on
fairness trade-off generalisation.
3.1	Learning the statistical loss
Let us first focus on the statistical component of the loss, i.e., L0T for T ∈ {S, D}. Much of this
paper is based on theory of bounds derived from Rademacher complexities, introduced in Bartlett
& Mendelson (2003) and surveyed in Boucheron, Stephane et al. (2005). Recent textbooks such
as Shalev-Shwartz & Ben-David (2014); Mohri et al. (2012) provide excellent introductions to the
topic. We start by recalling the definition of Rademacher complexities, as indicated in Boucheron,
StePhane et al. (2005):
3
Under review as a conference paper at ICLR 2022
Definition 2. The Rademacher complexity (or average) of a function ` is given by
R(' ◦ H ◦ S) = E SUp 1
h∈H n
n
fθi' (h,Zi).
i=1
(3)
Note that definitions can slightly vary (depending for instance on the presence or absence of the
absolute value in the definition), but all downstream results are qualitatively similar, up to some
multiplicative constants.
To make this more concrete, we can consider a simple (but usual) case (as describe in Shalev-
Shwartz & Ben-David (2014)). We suppose that almost surely kxk2 ≤ R. In addition, if we let
H = {w; kwk2 ≤ R0} and assume that the loss function ` is of the type `(w,	(x, y)) = ρ wTx,	y ,
where |p| is bounded by B and is Lp-Lipschitz, then, almost surely,
R('qHqS) ≤
LρRR0
Rademacher complexities are key to establishing learnability bounds and lead to fundamental results
in statistical learning theory. In particular, we will make use of a standard result (see Boucheron,
StePhane et al. (2005)).
Proposition 1. With probability at least 1 - δ, it holds

SUp |Ld(h) — LS(h)| ≤ 2R(' ◦H ◦S) + B
h∈H
(4)
It will also be useful to introduce conditional Rademacher complexities that will be used throughout
this paper. In particular, since we have a partition of the sample in terms of categories S = SaC=1 Na,
we can consider the Rademacher complexity of that particular sample:
R(' ◦ H ◦Na) = E sup ɪ X σi' (h,zi)	.	(5)
h∈H na i∈Na
Lemma 1. The sample Rademacher complexity can be bounded from above by the weighted sum of
conditional Rademacher complexities:
C
R(' qHqS ) ≤ X n R(' oHoNa),	(6)
where R(' qHq Na) = E 卜uρh∈H na ∣Pi∈NɑOi' (h,/) ∣].
Proof. This comes directly from the sub-additivity of the absolute value and the supremum. □
One can further interpret the non-negative gap PC=I nnaR(' ◦ H ◦Na) — R(' ◦H◦S) as a diver-
sification benefit amongst categories. Finally, this leads us to a proposition leveraging our results so
far:
Proposition 2. With probability at least 1 — δ,
hUH ILD (h) —
LS (h)∣ ≤ 2 X n R('0 oHoNa) + bJ 2^n2.
a=1
(7)
3.2	Learning fairnes s requirements
Let us now move on to the fairness learning part; as before, we would like to find a bound on the
distribution fairness loss given the sample fairness loss, i.e., a probabilistic bound on the difference
φD(h) — φS (h).
4
Under review as a conference paper at ICLR 2022
Proposition 3. Under the assumption that φ is Kφ-Lipschitz, it holds, with probability at least 1 一 δ,
and where ZS := Pa=I ʌʌn+ + Jn-, that
C
sup ∣Φd(h) - φs(h)| ≤ 2KφXR('+ ◦H^N+) + R('- ◦H◦N-)
h∈H	a=1
+KφBZs∖∕2lθg 4δC .
n
The proof can be found in Appendix C.1 and the role played by ZS is investigated in Section 4.
3.3	Simultaneous learning of statistical performance and fairnes s
We have established probabilistic bounds on both the statistical performance criterion and the fair-
ness penalty. However, we wish to study both jointly, and determine the behaviour of the chosen
fairness trade-off. As mentioned previously, this trade-off is very flexible and can accommodate
multiple situations.
3.3.1	Bounding loss and fairness
First, we may wish to determine a probabilistic upper bound on L0D(h) 一 L0S (h) and φD(h) 一 φS (h)
jointly. By a simple application of the union bound, we obtain the following result:
Proposition 4. With probability at least 1 一 δ, it holds jointly that
i
hUH 1LD DhiS(h)l	≤
2R('0 ◦ H ◦ S) + B
2 log 4
n
C
SUp ∣ΦD(h) 一 φs(h)| ≤ 2KφXR('+ oHoN+)+ R('- ◦H◦N-)
h∈H	a=ι
+KφBZs∖l2lθg 8C .
n
3.3.2	Bounding the trade-off
As mentioned above, the existence of various impossibility results and the empirical findings show-
ing that statistical performance tends to decrease as fairness requirements increase have highlighted
the interest for partial debiasing methods. It is however important to be able to determine the gen-
eralisation properties that such objectives can lead to. We can perform a similar analysis on the
trade-off objective LD(h) by using the same arguments to get an upper bound on this objective.
Proposition 5. With probability at least 1 一 δ, it holds
SUP |LD(h) 一 LS(h)| ≤ 2R('0 oH。S) + B∖ 2⅛
h∈H	n
C
+2λKφ X R('+ ◦ H ◦ N+) + R('- ◦ H ◦ N-)
a=1
+λKφBZs S2lθg 竿
n
C n+	n-
≤ 2V n -a R('0 oH0N+) + -a R('0 oHoN-
nn
a=1
+λKφR('+ oHoN+) + λKφR('- oHoN-)}
+b a+λκ φzs)r 2l°g(n8C∕δ).
5
Under review as a conference paper at ICLR 2022
The upper bound can be understood as the sum of individual contributions plus the usual
O (Plog(I/δ)∕n) factor. Let Us now establish a related inequality, linking the distribution value
of the trade-off under the distribution optimum and under the sample optimum. Let us denote by
hT =h∈H LT(h) for T ∈ {D, S} an optimal classifier derived on either the underlying distribution
or the sample distribution of the objectives LT. This is an important issue for learnability of fairness
as it provides some guarantees on how far the actual optimal trade-off is from the one we would ob-
tain by picking the optimum computed on the sample S . This result provides a fairness pendant to
the usual case in statistical learning (see for instance Theorem 26.5 in Shalev-Shwartz & Ben-David
(2014)).
Theorem 1.	With probability at least 1 - δ, it holds
0 ≤ LD (hS) — LD(hD) ≤ UBs + B(1 + XZSKφ) √^,Wδ,
(8)
where hT =h∈H Lτ(h) for T ∈ {D,S}, and UB S := 2 PC=I { n+ R('0 ◦H^N+) + n- R('0 ◦
HoN-) + λKφR('+ oHoN+) + λKφR('- qH°N-∖.
The proof is given in Appendix A. What is particularly interesting about this result is the fact that the
upper bound remains the same as in Proposition 5, and only the O (Plog(I/δ)∕n) factor changes.
Let US point out that this result is practical in nature in the sense that one would use hS in practice
but would still look for out-of-sample generalisation, hence the importance of the term LD (hS).
4	(Some) practical consequences of PAC for fairness trade-offs
4.1	Decomposing upper bounds on generalisation
One is led to decompose the overall loss in terms of each category’s contributions to the overall
learning upper bound.
Definition 3. The (half-)contribution -denoted by RNa - of each category a = 1, ∙∙∙ ,C, to the
fairness trade-off learning upper bound is
RN	:= na+ R('0 oHoN+) + n-R('0 oH。Nar)
a	n	an	a
+λKφR('+ oHoNa+) + λKφR('- oHoN-).
Remark 2. This structure is quite interesting as it shows that the first two terms representing contri-
butions to the statistical loss upper bound are weighted by their proportions of the overall sample.
Thus, even if the Rademacher complexity of the category is high, it can be counterbalanced by the
fact that it will not affect the overall average loss. On the other hand, the contributions coming from
the fairness part are unweighted. In this case, a low sample size would usually lead to a higher
Rademacher complexity.
Now, RNa can be decomposed into finer components:
RNa= RN+ + RNa + RNa + RNa，	(9)
with obvious notations. One can then determine the contributions coming from the union of certain
categories or of positives or negatives.
4.2	ZS AS SAMPLE CONCENTRATION
First, let us remark that the usefulness of probabilistic upper bounds comes from the fact that the
ones that have been established only rely on observable quantities coming from the sample under
consideration. This is powerful as a practical check, but it also suggests a way of amending the
initial minimisation exercise by including the upper bound to the sample component, i.e., optimise
LD(h) + UBS(h).
6
Under review as a conference paper at ICLR 2022
Second, a quantity that is ubiquitous in our analysis (see, for instance, Theorem 1), is ZS =
PC=ι jn+ + Jn-∙ As noticed previously, this is a measure of concentration within the sam-
ple that only depends on in-sample data. The more unequal the count of categories’ sample sizes,
the higher ZS. This can be formalised in the following proposition:
Proposition 6. The constant ZS can be expressed in terms of the empirical class sample propor-
tions:
C	2C
ZS = X h(p+)-2 + 向厂 2i= pM-≡,
(10)
where Mα(pb) is the α-generalised mean 1 of the vector pb = (pba±)a±. In addition, under the assump-
tion that n± ≥ 1 for all a = 1, ∙∙∙ ,C, and n > 2C — 1, the following inequality holds:
(2C)3/2 ≤ ZS ≤ (2C - 1)√n + n— — (2C - 1) ∙	(II)
The proof is straightforward and left to the reader. The bounds on ZS offer two take-aways. First,
the lower bound grows SUPer-linearly as a function of the number of categories, which is problematic
especially to tackle interSectionality. Second, the upper bound on ZS/ ʌ/(n) converges to a non-zero
constant as n goes to infinity. This is an extreme scenario whereby a category can be made up of
one individual only, but this stresses the fact that class imbalance can hinder learning in the context
of fairness. ZS is model-agnostic and thus useful to determine ex-ante how much debiasing can be
applied on a given dataset.
5	Asymptotic regime
One of the possible issues of using PAC learning in practice is the fact that it tends to consider worst-
case scenarios (e.g., by deriving probabilistic inequalities over a whole space of possible classifiers).
To mitigate this possible drawback, we adopt a very different viewpoint and set ourselves in a frame-
work which generalises Proposition 1 in Agrawal et al. (2020). Our objective is to derive a limiting
distribution for the rescaled fairness trade-off after having trained one particular classifier, which
we then consider fixed. In doing so, we can thus assess the different contributions to the limiting
variance given the choice of classifier. In addition to corresponding to real-life modelling, this also
has the benefit of allowing for closed-form results (as opposed to upper bounds in PAC learning).
Both approaches are -as will be shown- very complementary.
Let us say a few words about the idealised data generating process that hypothetically produces
our observations. In particular, we consider that we have an infinite population of indepen-
dently and identically distributed datapoints, according to a mixture distribution with probability
weights pa±. Conditional on a and y = ±1, we can then draw x. All draws are independent.
Let us introduce some additional notation: the intra-category variances are given by (σ0,±)2 =
Vs=a,y=±ι ['0(h,z)], (σ±)2 = Vs=a,y=±ι ['±(h,z)]. Furthermore, κD,a = ∂x±Φd(h), i.e.,
KD a is the entry corresponding to a± in the gradient VΦd(h). We can now introduce our main
result, which extends Agrawal et al. (2020) to a multi-category and multi-dimensional setting.
Theorem 2.	Let h ∈ H, under mild assumptions, we have the following convergence in distribution
√n (LS(h) -Ld(h)) ---→ N (0, Viim(h)),	(12)
n→+∞
where
Viim[h] = ^Xp± (σ0,±)2 + λ2 ((σ±)2 + covariance terms	(13)
a±	pa±
The exact assumptions and the proof can be found in Appendix B. In a nutshell, the variance of the
limiting trade-off distribution is the addition of
1The α-generalised mean of K numbers xι, •…,XK is given by (	PK=I
7
Under review as a conference paper at ICLR 2022
•	The sum of intra-Category '0,± losses weighted by their true probabilities p±;
•	The sum of intra-Category fairness '± losses weighted by their inverse probabilities p±,
as in the uniform convergence framework. In addition, the constant (Ka±)2 represents the
sensitivity of the fairness loss funCtion φ and is bounded from above by [Kφ]2 in these
Case where φ is Kφ-LipsChitz Continuous, while λ is the hyper-parameter governing the
trade-off between statistiCal and fairness performanCes.
Qualitatively, the results obtained in the asymptotiC and uniform frameworks are similar and point
to the ConClusion that to learn fairness, one must start by learning Category samples. A low prob-
ability pa± may not impaCt the varianCe of the overall statistiCal loss but it may lead to a very poor
understanding of fairness trade-offs.
6 Experimental results
In this SeCtion, we investigate on real datasets the validity of the theoretiCal results that we have
derived. Here, we solve (via L-BFGS) for a logistiC regression with an additional Constraint on
fairness. ConCretely, the objeCtive funCtion is defined as:
1n
min — Y''(yi, Xi； β) + λ
β∈Rd n
i=1
2
1 n0	1 n1
'(yj, xj;β) -	'(yj0, xj0;β)
n0 j=1	n1 j0=1
(14)
where ` is the logistiC loss funCtion and the groups 0 and 1 Correspond to two different Classes of
a proteCted attribute. We have Considered three datasets with binary proteCted attributes and binary
outcomes, shown in Table 2, and applied a standard 80-20 train-test split.
Dataset name	Reference	Protected attribute	Binary outcome
Adult	Dua&Graff(2017)	sex	income exceeds 50$Kyear
Loan defaults	I-Cheng Yeh (2009)	sex	default payment next month
German credit	Dua&Graff(2017)2	sex	creditability
Table 2: Data sets desCription
Figures 1 and 2 illustrate the trade off between average loss and disparity for a range of λ values. 3
On the train test (Figure 1, there is a Clear dependenCe between the value of λ and the respeCtive
average loss and disparity between groups. As expeCted, an inCrease in λ tends to deCrease disparity,
while inCreasing the average loss, and viCe versa. However, this is not always the Case on the test
set, as some intermediate values of λ aChieve both lower average loss and lower disparity, whereas
more (or totally) debiased models Clearly overfit.
6.1 In-Sample Results
Figure 1: Trade off between disparity and average loss on adult, german Credit, and loan defaults
data sets. EaCh Colour Corresponds to a different initialisation, while the symbol’s size Corresponds
to the weight λ on disparity in the overall loss funCtion. Results are measures on the train set.
3Specifically {0.01, 0.05, 0.1,0.5,1, 5,10, 50,100, 500}. Each distinct color represents a different initial-
ization, while the size of the point is a monotoniCally inCreasing funCtion of the respeCtive λ.
8
Under review as a conference paper at ICLR 2022
6.2 Out-of-Sample Results
Figure 2: Trade-off between disparity and average loss on Adult, German credit, and Loan defaults
datasets. Each colour corresponds to a different initialisation, while the symbol’s size corresponds
to the weight λ on disparity in the overall loss function. Results are measured on the test set.
6.3 Standard Deviation Scaling
We experiment with class imbalance to see how the solution to the original problem changes as
imbalance grows, in line with Theorem 2. To do so, we downsample observations from one class,
while keeping the original number of observations in the other class. As predicted, on the test set,
the loss variance increases with class imbalance, as shown in Figure 34.
Figure 3: Fairness uncertainty versus class imbalance ratio in protected characteristic on Adult,
German credit, and Loan defaults datasets. Results are measured on the test set.
7 Conclusion
The analysis that we have conducted from two different angles, namely learning with uniform con-
vergence and asymptotics, leads to the same overall qualitative assessment. While we have derived
probabilistic upper bounds to prove learning guarantees on the one hand and convergence to a lim-
iting distribution on the other, a striking feature of learning fairness trade-offs is the fundamental
difference of regimes between the usual statistical performance criterion measured on the whole
sample and the fairness penalty that examines relationships between sub-samples.
Indeed, fairness is not about learning an average distribution, quite the contrary, in the sense that
it requires a fine understanding of differences across conditional distributions. Intuitively, in the
usual case of an overall statistical loss function, if a category only represents a small portion of
the sample, then it also only constitutes a small fraction of the overall loss. On the opposite, from
a fairness viewpoint, this implies that it is difficult to make any type of statements regarding the
relationship between this particular category and other categories, hence a high risk for a given
fairness objective. We have also shown with an empirical investigation that our results translate to
practical cases and that small sample sizes or class imbalance can lead to spurious empirical results.
4Here, points of the same color correspond to the same random seed. Different random seeds are needed to
obtain several variants of the downsampled data. The dotted color lines are the plain linear least square estimate
through the points of the same color, while the bold black line is least square estimate using data points from
all seeds. λ is fixed at 50.
9
Under review as a conference paper at ICLR 2022
References
Ashrya Agrawal, Florian Pfisterer, Bernd Bischl, Jiahao Chen, Srijan Sood, Sameena Shah, Francois
Buet-Golfouse, Bilal A Mateen, and Sebastian Vollmer. Debiasing classifiers: is reality at variance
with expectation?, 2020.
Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. J. Mach. Learn. Res., 3(null):463-482, March 2003. ISSN 1532-4435.
Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. Fairness in criminal
justice risk assessments: The state of the art. Sociological Methods & Research, August 2018.
doi: 10.1177/0049124118782533.
Boucheron, StePhane, Bousquet, Olivier, and Lugosi, Gabor. Theory of classification: a survey of
some recent advances. ESAIM: PS, 9:323-375, 2005. doi: 10.1051/ps:2005018. URL https:
//doi.org/10.1051/ps:2005018.
Toon Calders and Sicco Verwer. Three naive bayes approaches for discrimination-free classification.
Data Mining and Knowledge Discovery, 21(2):277-292, 2010. doi: 10.1007/s10618-010-0190-x.
Irene Y. Chen, Fredrik D. Johansson, and David Sontag. Why is my classifier discriminatory? In
Proceedings of the 32nd International Conference on Neural Information Processing Systems,
NIPS’18, pp. 3543-3554, Red Hook, NY, USA, 2018. Curran Associates Inc.
Jiahao Chen, Nathan Kallus, Xiaojie Mao, Geoffry Svacha, and Madeleine Udell. Fairness under
unawareness: Assessing disparity when protected class is unobserved. In Proceedings of the
Conference on Fairness, Accountability, and Transparency, FAT* ’19, pp. 339-348, New York,
NY, USA, 2019. Association for Computing Machinery. ISBN 9781450361255. doi: 10.1145/
3287560.3287594.
Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism
prediction instruments. Big Data, 5(2):153-163, June 2017. doi: 10.1089/big.2016.0047.
Anirban DasGupta. Asymptotic Theory of Statistics and Probability. Springer Texts in Statistics.
Springer, 1st edition, March 2008. ISBN 0387759700.
Michele Donini, Luca Oneto, Shai Ben-David, John S Shawe-Taylor, and Massimil-
iano Pontil. Empirical risk minimization under fairness constraints. In S. Ben-
gio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 31. Curran Asso-
ciates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
83cdcec08fbf90370fcf53bdd56604ff- Paper.pdf.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
Moritz Hardt, Eric Price, and Nathan Srebro. Equality of opportunity in supervised learn-
ing. Advances in Neural Information Processing Systems, 29:3323-3331, December
2016. doi: 10.5555/3157382.3157469. URL https://papers.nips.cc/paper/
6374-equality-of-opportunity-in-supervised-learning.
Che-hui Lien I-Cheng Yeh. The comparisons of data mining techniques for the predictive accuracy
of probability of default of credit card clients. Expert Systems with Applications, (36), 2009. doi:
10.1016/J.ESWA.2007.12.020.
Joon Sik Kim, Jiahao Chen, and Ameet Talwalkar. Model-agnostic characterization of fairness
trade-offs. In Proceedings of the International Conference on Machine Learning, volume 37, pp.
9339-9349, 2020.
Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair deter-
mination of risk scores. In Christos H. Papadimitriou (ed.), Proceedings of the 8th Innovations
in Theoretical Computer Science Conference, volume 67 of Leibniz International Proceedings in
Informatics (LIPIcs), Dagstuhl, Germany, 2017. Schloss Dagstuhl-Leibniz-Zentrum fuer Infor-
matik. doi: 10.4230/LIPIcs.ITCS.2017.43.
10
Under review as a conference paper at ICLR 2022
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning.
The MIT Press, 2012. ISBN 026201825X.
Arvind Narayanan. Translation tutorial: 21 fairness definitions and their politics. In Proceedings of
the Conference on Fairness, Accountability and Transparency, FAT* 18, New York, USA, 2018.
L. Oneto, M. Donini, M. Pontil, and A. Maurer. Learning fair and transferable representations
with theoretical guarantees. In 2020 IEEE 7th International Conference on Data Science and
Advanced Analytics (DSAA),pp. 30-39, 2020. doi:10.1109/DSAA49011.2020.00015.
Luca Oneto, Michele Donini, Massimiliano Pontil, and John Shawe-Taylor. Randomized learning
and generalization of fair and private classifiers: From pac-bayes to stability and differential
privacy. Neurocomputing, 416:231-243, 2020. ISSN 0925-2312. doi: https://doi.org/10.1016/j.
neucom.2019.12.137. URL https://www.sciencedirect.com/science/article/
pii/S0925231220305142.
Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Q Weinberger. On fair-
ness and calibration. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-
wanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30,
pp. 5680-5689. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/
7151-on-fairness-and-calibration.pdf.
Kit T Rodolfa, Erika Salomon, Lauren Haynes, Ivan HigUera Mendieta, Jamie Larson, and Rayid
Ghani. Case study: predictive fairness to reduce misdemeanor recidivism through social service
interventions. In Proceedings of the 2020 Conference on Fairness, Accountability, and Trans-
parency, FAT* ’20, pp. 142-153, New York, NY, USA, January 2020. Association for Computing
Machinery.
Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to
Algorithms. Cambridge University Press, USA, 2014. ISBN 1107057132.
Sahil Verma and Julia Rubin. Fairness definitions explained. In Proceedings of the International
Conference on Software Engineering, pp. 1-7, New York, NY, USA, 2018. ACM. doi: 10.1145/
3194770.3194776.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P Gummadi. Fair-
ness Constraints: Mechanisms for Fair Classification. volume 54 of Proceedings of Machine
Learning Research, pp. 962-970, Fort Lauderdale, FL, USA, 2017. PMLR.
In the appendix, we indicate the proofs of the two theorems in the main text, Thm 1 and Thm 2.
A Some Applications of Fair Learning
In this Section, we apply our results to two particular loss functions, namely the Calders-Verwer gap
and the Bayes gap.
A.1 Example 1: Learning disparity
Of particular interest when measuring fairness is the discrepancy that can be observed amongst
different groups, for example in terms ofa loss L that captures some properties we wish to ascertain.
This could be the difference in false negative rates between categories a and a0, where a is usually
chosen to be a reference group. This concept holds for both the true distribution and the sample
distribution.
Definition 4. The (demographic) disparity, or Calders-Verwer gap Calders & Verwer (2010); Chen
et al. (2019), between groups a and a0 with respect to loss L can be defined as
∆TL,a,a0(h) =LT,a(h)-LT,a0(h),	(15)
forT ∈ {D, S}.
11
Under review as a conference paper at ICLR 2022
Note that it is also quite usual to consider the absolute value of disparity, ∆TL,a,a0 (h) indicating
whether there is equality or not (and magnitude thereof), rather than giving a sign (which relies
more on how the baseline category has been chosen). Indeed, trivially, disparity is asymmetric
whereas the absolute disparity is symmetric.
It is therefore important to be able to establish the learnability of this widely used metric, which we
do in the following proposition.
Proposition 7. With probability at least 1 - δ, it holds
SUP ∣∆D,a,a0 (h)- AS,。© (h)∣	≤ 2 {R(' ◦H ◦Na) + R('。"。NaK
h∈H ,,	,,
+B ∕2log (δ) - nna + n©0 + In。+ "，一
Nna+na0 [V	n。	V M -.
Proof. We start by observing that by sub-additivity of the sup, we have
sup ∣∣∆LD,a,a0(h) - ∆SL,a,a0(h)∣∣ ≤ sup |LD,a(h) - LS,a(h)| + sup |LD,a0(h) - LS,a0(h)| .
h∈H	h∈H	h∈H
We can then ConlUde thanks to Proposition 1 and the union bound.	□
What this upper bound suggests is that in order to learn about disparity, one has to learn the two
categories a and a0 . If one category has either a large Rademacher complexity or a low count, the
upper bound will increase. This result enables us to derive bounds on quantities of interest such as
|AD,a,a0 (hS ) - AS,a,a0 (hS ) | that follows directly from Proposition 7.
A.2 Example 2: Learning bias
In this Section, we understand bias in a very specific (and non-fairness related) way, as in Chen
et al. (2018). In short, one can consider bias -in this context- as the difference between the loss
incurred by category a when the classifier is determined by minimising the loss function over the
entire sample and the loss in category a when the optimal classifier is derived on a standalone basis
(i.e., the classifier explicitly takes into the account the attribute a).
If it is possible to use the characteristics s directly in the model, then one can calibrate an optimal
classifier on each category:
C
hD (x，S)= X 1{s=a}hD,a(x).	(16)
a=1
However, we usually do not have access to the characteristic s or cannot include it as a feature (for
instance to avoid disparate treatment), so that -in general- hD (x,s) = hD (x) for all S = 1, ∙∙∙ ,C.
Consequently, we can define the Bayes gap as the difference between the loss obtained between the
Definition 5. The Bayes gap for category a is given by
Γτ,a = Lτ,a(hS) — Lτ,a(hS,a),	(17)
for T ∈ {D, S}.
In particular, Γs,a ≥ 0.「,。represents the added loss incurred by category a due to the consideration
of other categories while selecting the classifier h.
Proposition 8. The true Bayes gap, Γd,。= L0,a(hS) 一 LD,a(hS,。) Can be learnt as
|rD,a - rS,a∣ ≤ 4R(' ◦ H ◦Na) + 2B
/2iθg(2∕δ)
2	2n。
(18)
with probability at least 1 一 δ.
Proof. The proof is fairly straightforward and simply decomposes the Bayes gap in terms of easier
building blocks:
LD,a(hS) - LD,a(hS,a) = LD,a(hS) - LS,a(hS) + LS,a(hS) - LS,a (hS,a)
+ LS,a(hS,a) - LD,a(hS,a),
12
Under review as a conference paper at ICLR 2022
leading to
rD,a - rS,a = LD,a(hS) - LS,a (hS) + LS,a(hS,a) - LD,a(hS,a).
We thus obtain that
|rD,a - rS,a|	≤ 2 SUP |LD,a(h) - LS,a(h)|
h∈H
≤ 4R(' ◦HoNa) + 2BsS2log(2/S),
2na
where the second inequality holds with probability at least 1 - δ by Proposition 1.	□
B Out-of-Sample Variance S caling
In this Section, we consider a surrogate example to illustrate some real-world consequences of The-
orem 2. The behaviour of a number of debiasing methods, both in- and out-of-sample, has been
studied on real and surrogate data in the literature (Zafar et al. (2017); Agrawal et al. (2020); Oneto
et al. (2020) for instance), we thus only provide an illustration of our result on a simple test case,
to highlight the role played by sampling variance in dealing with fairness metrics. In keeping with
the previous section, we suppose that we have chosen a given classifier yb = h(x, s) and are now
considering its statistical properties on a (large) out-of-sample dataset.
B.1 Predictive parity and its limiting variance
In our simplified experimental setup, we consider C = 2, and set `0 = 0 with λ = 1, i.e., we only
look at a fairness criterion, and choose a ratio τPP linked to predictive parity (see Table 1):
,八	,P(b =ι∣y = ι,s = ι)
φD(h) = TD，PP := P(b =1|y =1,s = 2).
(19)
The perfectly fair case corresponds to τD,PP = 1. We also define the corresponding sample version
of this criterion, using the samples Na+, a = 1, 2:
,八、	n+ Pi∈N+ 1{bi=1}
φS (h) = τS,PP = -Tq----------：------
n+ Ti∈N2 1{bi=i}
In this case, one can compute Vlim(h) directly as
V (h _ 1 - P(b =ι∣y = ι,s = 1) 1	1 - P(b =ι∣y = ι,s = 2) ɪ
lim()=	P(b =1|y = 1,s = 1)	∏1 +	P(b =1|y = 1,s = 2)	∏,
(20)
(21)
where π1
p++p+and π2 = 1-π1 = p++p+.
B.2	Simulation set-up and results
In our simulations, we use the fixed values P(yb = 1|y = 1, s = 1) = 0.95 and P(yb = 1|y = 1, s =
2) = 0.99, leading to τD,PP = 0.96. We vary the parameters m = n1++ n2+, which represents the
overall sample size available to us to compute τS,PP, and π1, which is the effective proportion of
class 1 versus class 2. Throughout this exercise, we simulate N = 1000 times the experiment. Our
baseline case is m = 1, 000 and π1 = 10%.
From Figure 4 (a), one recovers the usual behaviour due to sample size: as m increases, the empirical
distribution of τS,PP becomes more peaked. A small sample size (such as m = 100) would yield
poor accuracy and one can observe that the density is actually bi-modal.
The behaviour with respect to sample size can also be checked by comparing the empirical distribu-
tion of τ√⅛m≡,Where k = 1,…，Nisthe experiment run,With a Standard GaUSSian distribution,
as predicted by Proposition 2. It can be seen in Figure 5 that there is a good adequacy between both
distributions.
13
Under review as a conference paper at ICLR 2022
Figure 4: (a) Empirical densities of τS,PP for m = 100 (dotted), 1000 (black) and 10000 (dashed)
at π1 = 10%. (b) Empirical densities of τS,PP for π1 = 1% (red), 10% (black) and 50% (blue) at
m= 1000.
The most salient feature, however, is the behaviour of τs,pp's distribution with respect to category
probability, as described in Figure 4 (b), where one can see that as the sample becomes balanced, the
variance of the estimator τS,PP decreases. When π1 is very small, say 1%, one recovers the bi-modal
distribution that comes with a small sample.
B.3	Key observation
In a nutshell, low sample sizes and class imbalance can lead to poor generalisation. Indeed, by
considering the cases m = 100, π1 = 10% and m = 1000, π1 = 1%, it is clear that one cannot
draw any conclusion regarding the presence of bias nor in terms of which group is the advantaged
group. Bi-modality is a particularly interesting characteristic of the empirical distribution: there is a
mode strictly below 1 and a mode strictly above 1.
Figure 5: Normalised density of τD,PP (in black) and theoretical limiting distribution (in green) for
m = 1, 000 and π1 = 10%.
14
Under review as a conference paper at ICLR 2022
C Proofs
C.1 Proof of Proposition 3
Proposition 3 Under the assumption that φ is K φ -Lipschitz, it holds, with probability at least 1 - δ,
that
C
SUp ∣Φd(h) - φs(h)|	≤ 2Kφ XR('+oHoN+) + R('-oHoN-)
h∈H	a=1
with ZS := PaC=1
n
n-
+KφBZS
2i0g 4C
n
Proof. Using the hypothesis that φ is Kφ-Lipschitz, it comes
C
lφD(A)- φS(h)| ≤ Kφ X ILD,α(h) - L+,α(h)l + |LD,a(h) - LS,a(h)
a=1
For any δ0 ∈ (0,1) and any a = 1,…，C it holds
sUp ∣LD,α(h) - L±,a(h)∣ ≤ 2R('± oH。Nat) + Bj
2log 奈
na±
whereby
sup ∣Φd(h) - Φs(h)|	≤
h∈H
C
2KφXR('+ oHoN+) + R('- oH。N-
a=1
+KφB XC
a=1
2log δ20
2log δ20
na+
na-
+
with probability at least 1 - 2Cδ0, thanks to the union bound. To have 1 - δ = 1 - 2Cδ0 , one can
simply pick δ0 = δ∕(2C). Now, We can write the last term as

XC
a=1
2log δ20
2log δ2o
na+
na-
2 log δ2o V
n a=1
ZS
2log δ20
+
n
hence the final result.
□
C.2 Proof of Theorem 1
Theorem 1 With probability at least 1 - δ, it holds
0 ≤ LD(hS) - LD(hD) ≤ UBS + B(1 + λZsKφ) √32 Jlog(InC同
(22)
Proof. For the sake of clarity, we divide the proof into different steps.
Step 1. We can start by rewriting
LD(hS) -Ld(hD)	= LD(hS) -Ls(hS) + LS(hS) -Ls(hD)+ LS(hD) -Ld(hD)
≤ Ld(hS) -LsWS) + LSIhD) -LdIhD),
15
Under review as a conference paper at ICLR 2022
since, by definition, LS(hS) - LS(hD) ≤ 0.
Step 2. Now, LD(hS) - LS(hS) ≤ suph∈H |Ld(h) - LS(h)| on the one hand, and, on the other
hand, we have
|Ls(hD) -Ld(hD)| ≤	ILS(hD) - LD(hD)∣
C
+ λKφ X{∣LD,a(hD ) - L+,a(hD )∣ + ∣Lp,a(hD ) - L-,“ (h" )|}.
a=1
Step 3. The key insight is to notice that the right-hand side only involves h", which does not depend
on the sample S, hence one can apply the Hoeffding inequality directly. Thus, by Hoeffding’s
inequality, for each a, with probability at least 1 - δ00 ,
∣LD,a(hD) - L±,a(hD)∣ ≤ Bslθg22(δ00).
2na
Similarly, with probability at least 1 - δ00,
∣LS(hD) -LD(hD)∣ ≤ Brlog(2f00).
We can then infer that with probability at least 1 - (2C + 1)δ00
|Ls(hD) -Ld(hD)|	≤ B,log：2/')
2n
2Kφ X llog(2∕δ00)	llog(2∕δ00)
+λBK * F- y-2n~
≤B(1+λZSKφ) rlog(2nδ0).
Step 4. From the previous proposition, it comes that with probability at least 1 - δ0
sup |Ld(h) — LS(h)| ≤ UBS + B (1 + λZsKφ) ∖ -——~-，.
h∈H	n
Finally, by choosing δ0 = δ∕2 and δ00 = δ∕(2(2C + 1)),
LD(hS) -Ld (hD)	≤ UBs + B (1 + XZSKφ) J2l叫C∕δ) + ,“以举；+ 1)叵
≤ UBs + B (1 + XZsKΦ) √ 产”,
with probability at least 1 - δ.	□
C.3 Proof of Theorem 2
Theorem 2 Let h ∈ H, under the assumption that p± › 0 for all a, and that VΦd (h) exists and is
non-zero, we have the following convergence in distribution
√n (LsIh)-LD(h)) --d-→ N (0, Viim(h)),	(23)
n→+∞
16
Under review as a conference paper at ICLR 2022
where
Viim(h)
XXp+ (吟+)2 + λ2(ɪ (σ+)2 +2λK+ »+ (40(h,z),'+(h,z))
a=1	Pa
+ 士P-(靖-)2 + λ2(K-2 S-)2 +2λK-c。V- (40(h,z),'-(h,z))
M	Pa
+ Xp+(1 -p+) (lD+)2 + p-(1 -p-) (lD-)2 - 2p+PaLDaLDa
a=1
-E (p+p+，LD+LD+ + P-P-，LD- LD-O
α=α0
+p+pa0 LD+LD,a0 + pa p+ LD,a LD+，)
Proof. For the sake of clarity, we divide this simple (but slightly tedious) proof into multiple steps.
In this proof, we make the assumption that φ is differentiable with continuous gradient; this is
not necessary but simplifies the exposition. The reader is referred to DasGupta (2008) for details
regarding the delta method, the central limit theorem and different types of convergence.
Step 1. We start by rewriting the difference between both true and empirical trade-offs LS and LD
for a given h ∈ H. Thanks to the Taylor formula, there exists ξ ∈ [0,1] such that
Φs(h) - Φd(h) = φ(L±(h))- φ(LD(h))
=▽。怎LD(h) + (IT)L±S))T(L±(h) - lD (h))
We denote KSa the partial differential of φ with respect to χ±, i.e., KSa = ∂χ±Φ(ξLD(h) + (1 -
ξ)L±(h))τ (L±(h) - LD(h)). Some algebra then yields
√n(Ls(h) - Ld(h))
c	,——
=E M √n+ (p+ [LS,,+ (h)- LD+(h)] + λK+,a [L+,a(h) - LD,a(h)])
C	n——
+ E J n- VZn- (P- [LS- (h) - LD,a(h)] + λK-,a [L-,a(h) - LD,a (h)])
a=1 Y
C
+ E √n
a=1
C
LS,+(h) + E √n
a=1
na-
V - pa

Step 2. Since limn→+∞ n±
p± > 0 almost surely, this implies trivially that limn→+∞ n±
+∞ almost surely, for any a. We now recall that, thanks to the continuous mapping theorem, it holds
that for any n±, limn→+∞ Jn/n± = 1∕pp± almost surely. Finally, limn→+∞ KS,a = KD,a
almost surely, since Vφ is continuous and limn→+∞ LS (h) = LS(h) almost surely.
Step 3. Let us now consider each term aS separately.
pnɪ (pS [lS,S (h)-LDS(h)] + λKS,a [L⅛,a(h) - LD,a(h)])
=√n⅛ ([pSlSS (h) + λKD,aLS,a(h)] - [pS LDS(h) + λKD,aLD,a(h)])
+ λ (KSa - KD,a) √n⅛ [LS,a(h) -LD,a(h)] ∙
By the Central Limit Theorem (CLT), Pn [l± °(h) - LD a(h)]
———-→ N(0, [σ±]2), but since
n→+∞
KD , a
almost surely.
a-〉0,	the whole term (KS	- KDa)	^n	∣^L± (h)	- LD a.(h)^∣	goes to 0
n→+∞	∖ S，a D，aJ	L S，a' ， D ,a' ，」
—
17
Under review as a conference paper at ICLR 2022
All that remains is to apply the CLT to
Pn± ([p±LS,± (h) + λKD,aL±,a㈤i - [p±LDt㈤ + λKD,aLD,a㈤D ,
and notice that Vs=a,y=±1 hp±'0,士(h, Z) + λKD,α'±(h,z)i = [P±]2[σa1±]2 + λ2[KD,α]2[σ±]2 +
2P±λKD,ac0v± ('0(h,z),'±(h,z)).
Note that since each data point in the sample is independent, the CLT can be applied term by term
(i.e., all a±’s have independent limiting distributions).
Step 4. To finish the proof, it simply remains to consider the adjustments due to observing empirical
proportions rather than the true class probabilities. We thus need to consider the limiting distribution
of the vector √η ( na——p± )	. But this is simply the limiting distribution of empirical
±	± ±,a=1,,C
proportions in repeated trials of a multinoulli distribution, hence the result.	□
Let us point out that the non-vanishing assumption on the gradient of the fairness loss function φ
can be relaxed, but leads to the need for a higher-order delta method (see DasGupta (2008)).
18