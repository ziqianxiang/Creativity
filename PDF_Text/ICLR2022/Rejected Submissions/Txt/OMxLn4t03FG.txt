Under review as a conference paper at ICLR 2022
Training	Multi-Layer	Over-Parametrized
Neural Network in Subquadratic Time
Anonymous authors
Paper under double-blind review
Ab stract
In the recent years of development of theoretical machine learning, over-
parametrization has been shown to be a powerful tool to resolve many fundamental
problems, such as the convergence analysis of deep neural network. While many
works have been focusing on designing various algorithms for over-parametrized
network with one-hidden layer, multiple-hidden layers framework has received
much less attention due to the complexity of the analysis, and even fewer al-
gorithms have been proposed. In this work, we initiate the study of the per-
formance of second-order algorithm on multiple-hidden layers over-parametrized
neural network. We propose a novel algorithm to train such network, in time sub-
quadratic in the width of the neural network. Our algorithm combines the Gram-
Gauss-Newton method, tensor-based sketching techniques and preconditioning.
1	Introduction
Deep neural networks have been playing a central role in both practical (such as computer vision
(LeCun et al., 1998; Krizhevsky et al., 2012; Szegedy et al., 2015; He et al., 2016), natural language
processing (Collobert et al., 2011; Devlin et al., 2018), automatic driving system, game playing
(Silver et al., 2016; 2017) ) and theoretical machine learning community (Li & Liang (2018); Jacot
et al. (2018); Du et al. (2019b); Allen-Zhu et al. (2019a;b); Du et al. (2019a); Song & Yang (2019);
Brand et al. (2021); Zou et al. (2018); Cao & Gu (2019); Lee et al. (2019a); Liu et al. (2020; 2021);
Chen et al. (2021)) . In order to analyze the dynamics of neural networks and obtain provable
guarantees, using over-parametrization has been a growing trend.
In terms of understanding the convergence behavior of over-parametrized networks, most of the
attentions have been directed to the study of first-order method such as gradient descent or stochastic
gradient descent. The widespread use of first-order method is explained, to a large degree, by its
computational efficiency, since computing the gradient of the loss function at each iteration is usually
cheap and simple, let alone with its compatibility with random sampling-based method such as mini-
batch. One of the major drawbacks of first-order methods is their convergence rate is typically slow
in many non-convex settings (poly(n, L, log(1/)) iterations, where n is the number of training
samples, L is the number of layers and is the precision of training), e.g., deep neural network
with ReLU activation, as shown in Allen-Zhu et al. (2019a)), which is often the case of a deep
over-parameterized neural network.
Second-order method (which employs the information of the Hessian matrix) on the other hand
enjoys a much faster convergence rate (only log(1/) iterations (Zhang et al., 2019), but not
poly(n) log(1/) iterations) and exploits the local geometry of the loss function to overcome the
pathological curvature issues that are critical in first-order method. Another clear advantage of
second-order method over first-order method is it does not require the tuning of learning rate. The
expense of using second-order method is its prohibitive cost per iteration, as itis imperative to invert
a dynamically-changing Hessian matrix or equivalently, solving a regression task involving Hessian.
Given any weight matrix of size m × m (m is the width of network), its Hessian matrix has size
m2 × m2, which makes any naive implementation of second-order algorithm takes at least O(m4)
time since one needs to write down the Hessian. This explains the scarcity of deploying large-scale
second-order method in non-convex setting, such as training deep neural networks, in contrast to
their popular presence in convex optimization setting (Vaidya (1989); Daitch & Spielman (2008);
Lee et al. (2015); Cohen et al. (2019); Lee et al. (2019b); Jiang et al. (2020b;a); Song & Yu (2021)).
1
Under review as a conference paper at ICLR 2022
Recent works (Cai et al. (2019); Zhang et al. (2019)) improved the practicality of second-order
method on training deep networks and presented algorithms to train one-hidden layer over-
parametrized networks with smooth activation functions. Specifically, they achieve a running time
of O(mn2) per iteration. Their methods are essentially variants of Gauss-Newton method combined
with using neural tangent kernels (Jacot et al. (2018)) to prove the convergence. By using the idea of
randomized linear algebra (Clarkson & Woodruff (2013); Woodruff (2014)) and a clever sketching
matrix as a preconditioner, Brand et al. (2021) further improves the running time to O(mn) per
iteration.
However, all of these algorithms are for training a shallow network with one-hidden layer and fall
short on deep networks — First, it is not clear that their algorithms can be generalized to multi-layer
setting, due to the presence of gradient vanishing or exploding. In the seminal work of Allen-Zhu
et al. (2019a), they showed that as long as the networks are over-parametrized, first-order methods
such as gradient descent and stochastic gradient descent won’t encounter such problem. But does
this still hold for second-order method? Can we provably show that second-order method has a
good performance when training deep over-parametrized networks? Second, even the fastest of
them (Brand et al. (2021)) would incur a running time of Oe(m2nL) per iteration, which seems
unavoidable due to the size of intermediate weight matrices is m × m.
In this work, we take the first step to tame the beast — We propose a second-order method that
achieves subquadratic cost per iteration with respect to m, and show that it has linear convergence
rate in training deep over-parametrized neural networks. We emphasize the importance of obtaining
subquadratic algorithm, since in multi-layer settings, the network width is typically much larger than
in one-hidden layer setting (m ≥ n8L12, (Zou & Gu, 2019)).
Our work can be decomposed into two parts: algorithmically and analytically. From an algorith-
mic perspective, our method builds upon a variant of GaUss-NeWton method (BjOrck (1996)) called
Gram-Gauss-Newton method (Cai et al. (2019); Brand et al. (2021)). In order to achieve a feasible
running time, We exploit tWo features of the gradient, Which is the key ingredient to form the Jaco-
bian matrix: 1). The gradient is loW rank (rank n). 2). The gradient can be formulated as the outer
product of tWo vectors. From an analytical perspective, our Work is inspired by Allen-Zhu et al.
(2019a). In contrast to their proof Which is a straightforWard analysis of the gradient, We make use
of the multi-layer neural tangent kernels (Du et al. (2019a)) and establish a connection betWeen a
Gram matrix We compute at each iteration and the NTK matrix.
Our Contributions. We summarize our technical contributions beloW.
•	We develop an analytical frameWork for the convergence behavior of second-order method
on training multi-layer over-parametrized neural netWork. To facilitate the analysis, We
exploit the equivalence betWeen neural tangent kernels and our over-parametrized netWork.
•	We design a second-order algorithm to train such netWorks, and achieve a cost per iteration
of oe(m2). Our algorithm makes use of Gram-Gauss-NeWton method, tensor-based sketch-
ing techniques, and data structures that maintains a loW rank representation efficiently.
•	By combining fast tensor algebra techniques and sketching-based preconditioning, We de-
vise an algorithm to efficiently solve a regression problem Where the involved matrix has
its roWs being tensor product of vectors.
1.1 Our Result
Our main result can be summarized in the folloWing three theorems, With one analyzing the conver-
gence behavior of a general Gram-based optimization frameWork, one designing an efficient algo-
rithm to realize this second-order optimization scheme, and the other is a novel algorithm to solve
tensor-based regression in high precision and fast, Which is a key step in our second-order method.
Throughout this paper, We Will use n to denote the number of training data points, d to denote the
dimension of input data points, m to denote the Width of the netWork and L to denote the number of
layers of the netWork. We use ft ∈ Rn to denote the prediction of neural netWork at time t.
Our first theorem demonstrates the fast convergence rate of our algorithm.
Theorem 1.1 (Convergence, informal version of Theorem F.19). Suppose the width of the neural
network satisfies m ≥ poly(n, L), then there exists an algorithm (Algorithm 1) such that, over
2
Under review as a conference paper at ICLR 2022
the randomness of initialization of the network and the algorithm, with probability at least 1 -
e-。(IOg2 m), we have
kft+ι - yk2 ≤ 2lift - yk2,
where ft ∈ Rn is the the prediction produced by neural network at time t.
The above theorem establishes the linear convergence behavior of our second-order method, which
is a standard convergence result for second-order method, as well as the same behavior as in one-
hidden layer over-parametrized networks (Brand et al. (2021)). However, compared to one-hidden
layer case, our analysis is much more sophisticated since we have to carefully control the probability
so that it does not blow up exponentially with respect to the number of layers.
The next theorem concerns the cost per iteration of our second-order algorithm.
Theorem 1.2 (Runtime, informal version of Theorem B.1). There exists a randomized algorithm
(Algorithm 1) that trains a multi-layer neural network of width m with the cost per training iteration
being
O(m2-Q(1)).
We improve the overall training time of multi-layer over-parametrized networks with second-order
method from Tinit + T ∙ O(m2) to Tinit + T ∙ o(m2), where Tinit is the initialization time of training,
typically takes O(m2). As we have argued before, multi-layer over-parametrized networks require
m to be in the order of n8 , hence improving the cost per iteration from quadratic to subquadratic is
an important gain in speeding up training. Its advantage is even more evident when one seeks a high
precision solution, and hence the number of iterations T is large.
We highlight that it is non-trivial to obtain a subquadratic running time per iteration: If not handled
properly, computing the matrix-vector product with weight matrices will take O(m2) time! This
means that even for first-order methods such as gradient descent, it is not clear how to achieve a
subquadratic running time, since one has to multiply the weight matrix with a vector in both forward
evaluation and backpropagation. In our case, we have also a Jacobian matrix of size n × m2, so
forming it naively will cost O(nm2) time, which is prohibitively large. Finally, note that the update
matrix is also an m × m matrix. In order to circumvent these problems, we exploit the fact that the
gradient is of low rank (rank n), hence one can compute a rank-n factorization and use it to support
fast matrix-vector product. We also observe that each row of the Jacobian matrix can be formulated
as a tensor product of two vectors, therefore we can make use of fast randomized linear algebra to
approximate the tensor product efficiently. As a byproduct, we have the following technical theorem:
Theorem 1.3 (Fast Tensor Regression, informal version of Theorem D.14). Given two n × m matri-
ces U and V with m n and a target vector c ∈ Rn. Let J = [vec(u1v1>)>, . . . , vec(unvn>)>] ∈
Rn×m2 where ui is the i-th row of matrix U ∀i ∈ [n]. There is a randomized algorithm that takes
O(nm + n2(log(κ∕e) + log(m∕δ)) + nω) time and outputs a vector x ∈ Rn such that
lJJ>xb-cl2 ≤ lcl2
holds with probability at least 1 - δ, and κ is the condition number of J.
From a high level, the algorithm proceeds as follows: given matrices U and V , it forms an approxi-
mation J ∈ Rn×n log(m√δ), where each row is generated by applying fast tensor sketching technique
to ui and vi (Ahle et al. (2020)). Then, it uses another sketching matrix for J to obtain a good
preconditioner R for J. Subsequently, it runs a gradient descent to solve the regression.
To understand this runtime better, we note that nm term is the size of matrices U and V , hence
reading the entries from these matrices will take at least O(nm) time. The algorithm then uses
tensor-based sketching techniques (Ahle et al. (2020)) to squash length m2 tensors to length
O(nlog(m∕eδ)). All subsequent operations are performed on these much smaller vectors. Fi-
nally, computing the preconditioner takes O(nω ) time, and running the gradient descent takes
O(n2 log(κ∕e)) time.
3
Under review as a conference paper at ICLR 2022
1.2 Related Work
Second-order Method in Optimization. Though not as prevalent as first-order method in deep
learning, second-order methods are one of the most popular in convex setting, such as linear pro-
gramming (Vaidya (1989); Daitch & Spielman (2008); Lee et al. (2015); Cohen et al. (2019)), em-
pirical risk minimization (Lee et al. (2019b)), cutting plane method (Jiang et al. (2020b)) and semi-
definite programming (Jiang et al. (2020a)). Due to the prohibitive high cost of implementing one
step of second-order method, most of these works focus on improving the cost per iteration.
In non-convex setting, there’s a vast body of ongoing works (Martens & Grosse (2015); Botev et al.
(2017); Pilanci & Wainwright (2017); Agarwal et al. (2017); Bernacchia et al. (2018); Cai et al.
(2019); Zhang et al. (2019); Brand et al. (2021); Yao et al. (2021)) that try to improve the practicality
of second-order method and adapt them to train deep neural networks. As shown in Cai et al. (2019),
it is possible to exploit the equivalence between over-parametrized networks and neural tangent
kernel to optimize an n × n matrix instead of an m2 × m2 matrix, which is an important breakthrough
in gaining speedup for second-order method. Sketching and sampling-based methods can also be
used to accelerate the computation of inverses of the Hessian matrix (Pilanci & Wainwright (2017)).
In spirit, our work resembles most with Cai et al. (2019) and Brand et al. (2021), in the sense that
our optimization also works on an n × n Gram matrix. Our algorithm also makes use of sketching
and sampling, as in Pilanci & Wainwright (2017); Brand et al. (2021).
Over-parameterized Neural Networks. In recent deep learning literature, understanding the ge-
ometry and convergence behavior of various optimization algorithms on over-parameterized neural
networks has received a lot of attention (Li & Liang (2018); Du et al. (2019b); Allen-Zhu et al.
(2019a;b); Du et al. (2019a); Song & Yang (2019); Ji & Telgarsky (2020); Zou et al. (2018); Cao&
Gu (2019); Liu et al. (2020; 2021)). The seminal work of Jacot et al. (2018) initiates the study of
neural tangent kernel (NTK), which is a powerful analytical tool in this area, since as long as the
neural network is wide enough (m ≥ Ω(n4)), then the optimization dynamic on a neural network is
equivalent to that on a NTK.
Sketching. Using randomized linear algebra to reduce the dimension of the problem and speedup the
algorithms for various problems has been a growing trend in machine learning community (Sarlos
(2006); Clarkson & Woodruff (2013); Woodruff (2014)) due to its wide range of applications to
various tasks, especially the efficient approximation of kernel matrices (Avron et al. (2014); Ahle
et al. (2020); Woodruff & Zandieh (2020)). The standard “Sketch-and-Solve” (Clarkson & Woodruff
(2013)) paradigm involves using sketching to reduce the dimension of the problem and then using
a blackbox for the original problem to gain an edge on computational efficiency. Another line of
work is to use sketching as a preconditioner (Woodruff (2014); Brand et al. (2021)) to obtain a high
precision solution.
Roadmap. In Section 2, we give a preliminary view of the training setup we consider in this paper.
In Section 2.1, we introduce the notations that will be used throughout this paper. In Section 2.2,
we consider the training setup. In Section 3, we overview the techniques employed in this paper.
In Section 3.2, we demonstrate various techniques to prove the convergence of our second-order
method. In Section 3.1, we examine the algorithmic tools utilized in this paper to achieve sub-
quadratic cost per iteration. In Section 4, we summarize the results in this paper and point out some
future directions.
2 Preliminaries
2.1 Notations
For any positive integer n, We use [n] to denote the set {1, 2,…，n}. We use E[∙] to denote expec-
tation and PrH for probability. We use ∣∣χk2 to denote the '2 norm of a vector x. We use ∣∣Ak to
denote the spectral norm of matrix A. We use kAkF to denote the Frobenius norm of A. We use
A> to denote the transpose of matrix A. We use Im to denote the identity matrix of size m × m.
For matrix A or vector x, we use ∣A∣0 , ∣x∣0 to denote the number of nonzero entries of A and x
respectively. Note that ∣∙∣o is a semi-norm since it satisfies triangle inequality. Given a real square
matrix A, we use λmax(A) and λmin(A) to denote its largest and smallest eigenvalues respectively.
Given a real matrix A, we use σmax(A) and σmin(A) to denote its largest and smallest singular
4
Under review as a conference paper at ICLR 2022
values respectively. We use N(μ, σ2) to denote the Gaussian distribution with mean μ and variance
σ2. We use O(f (n)) to denote O(f(n) ∙ Polylog(f (n)). We use〈•, •)to denote the inner product,
when applying to two vectors, this denotes the standard dot product between two vectors, and when
applying to two matrices, this means hA, Bi = tr[A>B] where tr[A] denote the trace of matrix A.
2.2 Problem Setup
Let X ∈ Rm0×n denote the data matrix with n data points and m0 features. Without loss of
generality, we assume kxik2 = 1, ∀i ∈ [n]. Consider an L layer neural network with one vector
a ∈ RmL and L matrices WL ∈ RmL ×mLτ, ∙∙∙, W2 ∈ Rm2 ×m and Wi ∈ Rmi ×m0. We will use
w`(t) to denote the weight matrix at layer ' at time t, and VW'(t) to denote its gradient. We also
use W(t) = {W1(t), . . . , WL(t)} to denote the collection of weight matrices at time t.
Architecture. We first describe our network architecture. The network consists of L hidden layers,
each represented by a weight matrix w` ∈ Rm'×m'-1 for any ' ∈ [L]. The output layer consists of
a vector a ∈ RmL . We define the neural network prediction function f : Rm0 → R as follows:
f (W,x)= a>φ(WL(φ(…φ(Wιx)))),
where φ : R → R is the shifted ReLU activation function (σb(x) = max{x - b, 0}) applied
coordinate-wise to a vector.
We measure the loss via squared-loss function:
1n
L(W ) = 2∑(yi - fMW)2.
i=1
This is also the objective function for our training.
We define the prediction function ft : Rm0×n → Rn as
ft(X)=[f(W(t),xι) f(W(t),x2)…	f(W(t),xn)]> .
Initialization. Our neural networks are initialized as follows:
•	For each ' ∈ [L], the initial weight matrix W'(0) ∈ Rm'×m'-1 is initialized such that each
entry is sampled from N(0,言).
•	Each entry of a is an i.i.d. sample from {一 √=-, + √m~} uniformly at random.
Gradient. In order to write gradient in an elegant way, we define some artificial variables:
gi,1 = W1xi,
gi,' = W'hi,'-1,
Di,i = diag(φ0(W1Xi)),
Di,' = diag(θ'(^hi,'-I)),
hi,1 = φ(W1xi),
hi,' = φ(W'hi,'-I),
∀i ∈ [n]
∀i ∈ H ∀' ∈ [L]∖{1}	⑴
∀i ∈ [n]
∀i ∈ [n],∀' ∈ [L]∖{1}
Using the definitions of f and h, we have
f(W, xi) = a> hi,L, ∈ R, ∀i ∈ [n]
We can compute the gradient of L in terms of W' ∈ Rm' ×m'-1, for all' ≥ 2
∂ L(W)
∂W'
n
X(f(W,xi)-yi) Di,'
i=1	|{z}
m` ×m'
L
Y
k='+1
Wk>	Di,k
l{z}	l{Z}
mk-1 ×mk mk×mk
| a }	hi>,'-1
''l"{z}
mL ×1
L 1×m'-ι
(2)
Note that the gradient for W1 ∈ Rm1 ×m0 (recall that m0 = d) is slightly different and can not be
written by general form. By the chain rule, we can compute the gradient with respect to W1 ,
∂L(W)
∂W1
(f(W,xi) -yi)
i=1
Di,1
|{z}
m1 ×m1
L
Y
k=2
Wk>	Di,k
l{z}	|{z}
mk-1 ×mk mk ×mk
|{az}
mL ×1
xi>
|{z}
1×m0
(3)
n
5
Under review as a conference paper at ICLR 2022
It is worth noting that the gradient matrix is of rank n, since it’s a sum of n rank-1 matrices.
Jacobian. For each layer ' ∈ [L] and time t ∈ [T], we define the Jacobian matrix j`,t ∈ Rn×m'm'-1
via the following formulation:
J…=hvec( f (W ⑴,XI) ) vec( fW ㈤,x2) )	•一 vec( fW ㈤,Xn) )i>
j',t := vec( ∂W'(t)	) vec( ∂W'(t) )	vec( ∂W'(t) )	.
The Gram matrix at layer ' and time t is then defined as G',t = J',J> ∈ Rn×n whose (i,j)-th
entsK h ∂f(W (t),Xi) ∂f(W (t),Xj) i	,	,	,
entry is h	∂W'	,	∂W'	i.
3 Technique Overview
In this section, we give an overview of the techniques employed in this paper. In Section 3.1, we
showcase our algorithm and explain various techniques being used to obtain a subquadratic cost per
iteration. In Section 3.2, we give an overview of the proof to show the convergence of our algorithm.
To give a simpler and cleaner presentation, we assume m` = m for all ` ∈ [L].
3.1	Subquadratic time
In this section, we overview the techniques deployed in our implementation of the second-order
method. Our main focus is to achieve subquadratic cost per iteration. Instead of using a Hessian
matrix of size m2 × m2, we use an n × n Gram matrix derived from the neural tangent kernel.
However this would still incur a cost of O(nm2) per iteration, since each gradient is an m × m
matrix and the Jacobian consists of n such gradients.
We start by demonstrating our algorithm:
Algorithm 1 Informal version of our algorithm.
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
procedure OURALGORITHM(f, {xi, yi}i∈[n])	. Theorem 1.1,1.2
/*Initialization*/
Initialize We (0), ∀' ∈ [L]
Store W1(0)xi in memory, ∀i ∈ [n]	. Takes O(nm2) time
Store hi,'(0) 一 φ(We(0)hi,'-ι(0)), ∀' ∈ [L], i ∈ [n] in memory . Takes O(nLm2) time
for t = 0 → T do
/*Forward computation*/
for ` = 1 → L do
Vi,' J hi,`-i, ∀i ∈ [n]
hi,' J Φ((We(0) + ∆We)(hi,'-ι)), ∀i ∈ [n]	. Takes O(n2m) + o(nm2) time
. hi,` is sparse
Di,' J diag(φ0((We(0) + ∆W')hi,'-ι)), ∀i ∈ [n]	. Takes O(nm) time
.Di,' is sparse
end for
ft J [a> h1,L, . . . , a> hn,L]>	. Takes O(nm) time
/*Backward computation*/
for ` = L → 1 do
ui,' J a>Di,LWL(t) . . . Di,'+1W'+1(t)Di,'	. Takes o(nLm2) time
Form Je',t that approximates J',t using {ui,'}in=1, {vi,'}in=1
___________________________________ cr z 、. 二 _______________ -	u,、
. Takes O(mn) time, J',t ∈ Rn×s where s = O(n)
Compute g' that approximates (J',tJ'>,t)-1c	. Takes O(nm) time
Form J'>,tg' via low rank factorization Pin=1 g',iui,'vi>,'
Implicitly update ∆W' J ∆W' + Pin=1 g',iui,'vi>,' and store it in memory
end for
end for
end procedure
6
Under review as a conference paper at ICLR 2022
Step 1: Invert Gram by solving regression. Recall the update rule of generic algorithm is given
by
W'(t + I) J WKt)- J>t(j',tj>)c,
where C is ft 一 y after proper scaling. Naively forming the Gram matrix J',J> will take O(n2m2)
time and inverting it will take O(nω) time. To avoid the quadratic cost at this step, we instead solve
a regression, or a linear system since the Gram matrix has full rank: find the vector g`,t ∈ Rn such
that
k JΛt J>tg',t - ck2
is minimized. This enables us to utilize the power of sketching to solve the regression efficiently.
Step 2: Solve Gram regression via preconditioning. In order to solve the above regression, we
adapt the idea of obtaining a good preconditioner via sketching then apply iterative method to solve
it (Brand et al. (2021)). Roughly speaking, we first use a random matrix S ∈ Rs×m2 that has the
subspace embedding property (Sarlos (2006)) to reduce the number of rows ofJ>, then we run a QR
decomposition on matrix SJ> . This gives us a matrix R such that SJ> R has orthonormal columns.
We then use gradient descent to optimize the objective kJJ> Rzt - y k22 . Since S is a subspace
embedding for J>, we can make sure that the condition number of the matrix J>R is small (O(1)),
hence the gradient descent converges after log(κ/) iterations, where κ is the condition number of
J. However, in order to implement the gradient descent, we still need to multiply an m2 × n matrix
with a length n vector, in the worst case this will incur a time of O(nm2 ). In order to bypass this
barrier, we need to exploit extra structural properties of the Jocobian, which will be demonstrated in
the following steps.
Step 3: Low rank structure of the gradient. Instead of studying Jacobian directly, we first try to
understand the low rank structure of the gradient. Consider f W；i) ∈ Rm×m, it can be written as
(for simplicity, we use hi,0 to denote xi):
∂f(W,Xi) ∂W=	二	hi,`-l a>Di,LWL .∙∙ Di,'+ιWi,'+ιDi,' . ''l^z∙/{z vi∈Rm×1	ui>∈R1×m
This means the gradient is essentially an outer product of two vectors, and hence has rank one. This
has several interesting consequences: for over-parametrized networks, the gradient is merely of rank
n instead of m. When using first-order method such as gradient descent or stochastic gradient de-
scent, the weight is updated via a low rank matrix. To some extent, this explains why the weight does
not move too far from initialization in over-parametrized networks when using first-order method to
train. Also, as we will illustrate below, this enables the efficient approximation of Jacobian matrices
and maintenance of the change.
Step 4: Fast approximation of the Jacobian matrix. We now turn our attention to design a fast
approximation algorithm to the Jacobian matrix. Recall that Jacobian matrix j`,t ∈ Rn×m is an
n × m2 matrix, therefore writing down the matrix will take O(nm2 ) time. However, it is worth
noticing that each row of j`,t is Vec(Uiv>)>, ∀i ∈ [n], or equivalently, Ui ◦ Vi where ◦ denotes the
tensor product between two vectors. Suppose we are given the collection of {u1, . . . , un} ∈ (Rm)n
and {v1, . . . , vn} ∈ (Rm)n, then we can compute the tensor product Ui ◦ vi via tensor-based
sketching techniques, such as TensorSketch (Avron et al. (2014); Diao et al. (2017; 2019)) or
TensorSRHT (Ahle et al. (2020); Woodruff & Zandieh (2020)) in time nearly linear in m and
the targeted sketching dimension s, in contrast to the naive O(m2 ) time. Since it suffices to pre-
serve the length of all vectors in the column space of J> the target dimension S can be chosen as
O(e 2n ∙ poly(log(m∕eδ))). Use j`,t ∈ Rn×s to denote this approximation of j`,t, We perform the
preconditioned gradient descent we described above on this smaller matrix. This enables to lower
the overall cost of the regression step to be subquadratic in m.
Step 5: Efficient update via low rank factorization. The low rank structure of the gradient can fur-
ther be utilized to represent the change on weight matrices ∆W in a way such that any matrix-vector
product involving ∆W can be performed fast. Let g` ∈ Rn denote the solution to the regression
problem posed in Step 1. Note that by the update rule of our method, We shall use J>tg' ∈ Rm×m
to update the weight matrix, but writing down the matrix will already take O(m2 ) time. Therefore,
7
Under review as a conference paper at ICLR 2022
it is instructive to find a succinct representation for the update. The key observation is that each
column of J> is a tensor product of two vectors: Ui ◦ Vi or equivalently, %v>. The update matrix
can be rewritten as Pn= 1 g',iUiV>, and we can use this representation for the update on the weight,
instead of adding it directly. Let
|	|	...	|	|	|	...	|
U' ：=	g',lU	g',2U2	…g',nUn	∈	Rm×n,腔：=Vl	V2	…Vn	∈	Rm×n,
|	|	...	|	|	|	...	|
then the update can be represented as U'V'τ. Consider multiplying a vector y ∈ Rm with this
representation, we first multiply y with VT ∈ Rn×m, which takes O(mn) time. Then we multi-
ply VeTy ∈ Rn with U' ∈ Rm×n which takes O(mn) time. This drastically reduces the cost of
multiplying the weight matrix with a vector from O(m2) to O(mn).
Inspired by this idea, it is tempting to store all intermediate low rank representations across all
iterations and use them to facilitate matrix-vector product, which incurs a runtime of O(T mn). This
is fine when T is relatively small, however, if one looks for a high precision solution which requires
a large number of iterations, then T might be too large and O(T mn) might be in the order of O(m2).
To circumvent this problem, we design the data structure so that it will exactly compute the m × m
change matrix and update the weight and clean up the cumulative changes. This can be viewed as a
“restart” of the data structure. To choose the correct number of updates before restarting, we utilize
the dual exponent of matrix multiplication, α (Gall & Urrutia, 2018), which means it takes O(m2 )
time to multiply an m × m by an m × mα matrix. Hence, we restart the data structure after around
mɑ∕n updates. Therefore, we achieve an amortized o(m2) time, which is invariant even though the
number of iterations T grows larger and larger.
3.2 Convergence analysis
In this section, we demonstrate the strategy to prove that our second-order method achieves a linear
convergence rate on the training loss.
Step 1: Initialization. Let W(0) be the random initialization. We first show that for any data point
xi, we have f(W(0), xi) = O(1). The analysis draws inspiration from Allen-Zhu et al. (2019a).
The general idea is, given a fixed unit length vector x, multiplying it with a random Gaussian matrix
W will make sure that kW xk22 ≈ 2. Since W is a random Gaussian matrix, applying shifted ReLU
activation gives a random vector with a truncated Gaussian distribution conditioned on a binomial
random variable indicating which neurons are activated. We will end up with kφ(W x)k2 ≈ 1 as
well as φ(W x) being sparse. Inductively applying this idea to each layer and carefully controlling
the error occurring at each layer, we can show that with good probability, f(W(0), xi) is a constant.
We also bound the spectral norm of df(w0),xi) by O(，L/m). One of the key part of this matrix is
the consecutive product DLWL . . . D'+1W'+1D'. By studying the distribution of its product with a
fixed vector, one can show that the spectral norm of this consecutive product is bounded by O( √L).
Finally, we make use the fact that each entry of a is a Rademacher random variable scaled by l/√√m,
hence the norm of o>DlWl ... D'+1W'+1D' is bounded by O(,L∕m) with good probability.
Furthermore, we show that the Gram matrix for the multiple-layer over-parametrized neural net-
work, which is defined as J',0J'τ,0, has a nontrivial minimum eigenvalue after the initialization. In
particular, we adapt the neural tangent kernel (NTK) for multiple-layer neural networks defined by
Du et al. (2019a) into our setting by analyzing the corresponding Gaussian process with shifted
ReLU activation function. Then, we can prove that with high probability, the least eigenvalue of the
initial Gram matrix is lower bounded by the least eigenvalue of the neural tangent kernel matrix.
Step 2: Small perturbation. The next step is to show that if all weight matrices undergo a small
perturbation from initialization (in terms of spectral norm), then the corresponding Jacobian matrix
has not changed too much. As long as the perturbation is small enough, it is possible to show that
the change of the h vector (in terms of '2 norm) and the consecutive product (in terms of spectral
norm) is also small. Finally, using the fact that a is a Rademacher vector with scaling 1∕√m, we
can show that the change
∂f(W(0) + ∆W, xi) ∂f(W(0), xi)
------------------------------------
∂(W' +∆W')
∂W'
8
Under review as a conference paper at ICLR 2022
1	∙ ,	,	1	1	∙	1	FFK pʌ /	/ T /	、 。	1	A _ Γ T 1 F ♦	,	1 ,	∙	_ Γ 1
has its spectral norm being bounded by O( L/m) for any layer ` ∈ [L] and input data i ∈ [n].
Consequently, the Frobenious norm of the Jacobian matrix is bounded by O( nL/m).
Step 3: Connect everything via a double induction. Put things together, we use a double induction
argument, where we assume the perturbation of weight matrix is small and the gap between ft and
y is at most 1/2 of the gap between ft-1 and y. By carefully bounding various terms and exploiting
the fact the Jacobian matrix always has a relative small spectral norm (O( nL/m)), we first show
that the weights are not moving too far from the initialization, then use this fact to derive a final
convergence bound for kft - yk2 .
4 Discussion and Future Directions
In this work, we propose and analyze a second-order method to train multi-layer over-parametrized
neural networks. Our algorithm achieves a linear convergence rate in terms of training loss, and
achieves a subquadratic (o(m2)) cost per training iteration. From an analytical perspective, we
greatly extend the analysis of (Allen-Zhu et al. (2019a)) to second-order method, coupled with
the usage of the equivalence between multi-layer over-parametrized networks and neural tangent
kernels (Du et al. (2019a)). From an algorithmic perspective, we achieve a subquadratic cost per
iteration, which is a significant improvement from O(m2n) time per iteration due to the prohibitively
large network width m. Our algorithm combines various techniques, such as training with the Gram
matrix, solve the Gram regression via sketching-based preconditioning, fast tensor computation and
dimensionality reduction and low rank decomposition of weight updates. Our algorithm is especially
valuable when one requires a high precision solution on training loss, and hence the number of
iterations is large.
One of the interesting questions from our work is: is it possible to obtain an algorithm that has a
nearly linear cost per iteration on m as in the case of training one-hidden layer over-parametrized
networks (Brand et al. (2021))? In particular, can this runtime be achieved under the current best
width of multi-layer over-parametrized networks (m ≥ n8)? We note that the major limitation in
our method is the sparsity of the change of the diagonal matrices (∆D) is directly related to the
magnitude of the change of weights (k∆W k). In our analysis of convergence, we go through a
careful double induction argument, which in fact imposes on a lower bound on k∆W k. It seems to
us that, in order to achieve a nearly linear runtime, one has to adapt a different analytical framework
or approach the problem from a different perspective.
A related question is, how can we maintain the changes of weight more efficiently? In our work, we
achieve speedup in the neural network training process by observing that the change of the weights
are small in each iteration. Similar phenomenon also appears in some classical optimization problem
(e.g., solving linear program (Cohen et al., 2019; Jiang et al., 2021) and solving semidefinite pro-
gram (Jiang et al., 2020a)) and they achieve further speedup by using lazy update and amortization
techniques to compute the weight changes, or using more complicated data structure to maintain the
changes of the weight changes. Can we adapt their techniques to neural network training? An or-
thogonal direction to maintain the change is to design an initialization setup such that while we still
have enough randomness to obtain provable guarantees, the matrix-vector product with the initial
weight matrix can be performed faster than O(m2 ) by sparsifying the Gaussian matrix as in Derez-
in´ ski et al. (2021) or imposing extra structural assumption such as using circulant Gaussian (Rauhut
et al., 2012; Nelson & NguyAn, 2013; Krahmer et al., 2014).
Another question concerns activation functions. In this paper, we consider the shifted ReLU acti-
vation and design our algorithm and analysis around its properties. Is it possible to generalize our
algorithm and analysis to various other activation functions, such as sigmoid, tanh or leaky ReLU?
If one chooses a smooth activation, can we get a better result in terms of convergence rate? Can we
leverage this structure to design faster algorithms?
Finally, the network architecture considered in this paper is the standard feedforward network. Is
it possible to extend our analysis and algorithm to other architectures, such as recurrent neural
networks (RNN)? For RNN, the weight matrices for each layer are the same, hence it is trickier to
analyze the training dynamics on such networks. Though the convergence of first-order method on
over-parametrized multi-layer RNN has been established, it is unclear whether such analysis can be
extended to second-order method.
9
Under review as a conference paper at ICLR 2022
Ethics Statement. This is a theory paper that proposed efficient algorithm to train multi-layer over-
parametrized neural networks, hence it does not have direct ethics implications.
Reproducibility Statement. All results in this paper can be directly verified and reproduced via
reading into the proofs. For complete algorithm description and its runtime analysis, see Section B.
For specific data structure used in the algorithm, see Section C. For the result regarding the fast
tensor regression, see Section D. For convergence analysis of the algorithm, see Section E, F and G.
References
Naman Agarwal, Brian Bullins, and Elad Hazan. Second-order stochastic optimization for machine
learning in linear time. 2017.
Thomas D. Ahle, Michael Kapralov, Jakob Bæk Tejs Knudsen, Rasmus Pagh, Ameya Velingker,
David P. Woodruff, and Amir Zandieh. Oblivious sketching of high-degree polynomial kernels.
In Proceedings ofthe 2020 ACM-SIAM SymPosium on Discrete Algorithms (SODA),pp.141-160,
2020.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In ICML, 2019a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural
networks. In NeurIPS, 2019b.
Haim Avron, Huy L. Nguyen, and David P. Woodruff. Subspace embeddings for the polynomial
kernel. In NeurIPS, 2014.
Alberto Bernacchia, Mate Lengyel, and Guillaume Hennequin. Exact natural gradient in deep linear
networks and its application to the nonlinear case. In S. Bengio, H. Wallach, H. Larochelle,
K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing
Systems. Curran Associates, Inc., 2018.
Sergei Bernstein. On a modification of chebyshev’s inequality and of the error formula of laplace.
Ann. Sci. Inst. Sav. Ukraine, Sect. Math,1(4):3849, 1§24.
Ake Bjorck. Numerical MethodSfor Least Squares Problems. Society for Industrial and Applied
Mathematics, 1996.
Aleksandar Botev, Hippolyt Ritter, and David Barber. Practical Gauss-Newton optimisation for
deep learning. In Proceedings of the 34th International Conference on Machine Learning, pp.
557-565, 2017.
Jan van den Brand, Binghui Peng, Zhao Song, and Omri Weinstein. Training (overparametrized)
neural networks in near-linear time. In ITCS, 2021.
Tianle Cai, Ruiqi Gao, Jikai Hou, Siyu Chen, Dong Wang, Di He, Zhihua Zhang, and Liwei Wang.
Gram-gauss-newton method: Learning overparameterized neural networks for regression prob-
lems. arXiv PrePrint arXiv:1905.11675, 2019.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and
deep neural networks. In NeurIPS, pp. 10835-10845, 2019.
Zixiang Chen, Yuan Cao, Difan Zou, and Quanquan Gu. How much over-parameterization is suf-
ficient to learn deep ReLU networks? In International Conference on Learning RePresentations
(ICLR), 2021.
Herman Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based on the sum of
observations. The Annals of Mathematical Statistics, pp. 493-507, 1952.
Kenneth L. Clarkson and David P. Woodruff. Low rank approximation and regression in input
sparsity time. In SymPosium on Theory of ComPuting Conference (STOC), pp. 81-90, 2013.
Michael B Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs in the current matrix
multiplication time. In STOC, 2019.
10
Under review as a conference paper at ICLR 2022
Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray KavUkcUoglu, and Pavel
Kuksa. Natural language processing (almost) from scratch. Journal of machine learning research,
12:2493-2537, 2011.
Samuel I Daitch and Daniel A Spielman. Faster approximate lossy generalized flow via interior
point algorithms. In Proceedings of the fortieth annual ACM symposium on Theory of computing
(STOC), pp. 451-460, 2008.
MichaI Derezinski, Jonathan Lacotte, Mert Pilanci, and Michael W. Mahoney. Newton-less: Spar-
sification without trade-offs for the sketched newton update, 2021.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Huaian Diao, Zhao Song, Wen Sun, and David Woodruff. Sketching for kronecker product regres-
sion and p-splines. In AISTATS, 2017.
Huaian Diao, Rajesh Jayaram, Zhao Song, Wen Sun, and David Woodruff. Optimal sketching for
kronecker product regression and low rank approximation. In Advances in Neural Information
Processing Systems (NeurIPS), 2019.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning (ICML),
2019a.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In ICLR, 2019b.
FrangoiS Le Gall and Florent Urrutia. Improved rectangular matrix multiplication using powers
of the coppersmith-winograd tensor. In Proceedings of the Twenty-Ninth Annual ACM-SIAM
Symposium on Discrete Algorithms, SODA ’18, pp. 1029-1046, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition
(CVPR), pp. 770-778, 2016.
Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the
American Statistical Association, 58(301):13-30, 1963.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: convergence and gen-
eralization in neural networks. In Proceedings of the 32nd International Conference on Neural
Information Processing Systems (NeurIPS), pp. 8580-8589, 2018.
Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve arbi-
trarily small test error with shallow relu networks. In ICLR, 2020.
Haotian Jiang, Tarun Kathuria, Yin Tat Lee, Swati Padmanabhan, and Zhao Song. A faster interior
point method for semidefinite programming. In FOCS, 2020a.
Haotian Jiang, Yin Tat Lee, Zhao Song, and Sam Chiu-wai Wong. An improved cutting plane
method for convex optimization, convex-concave games and its applications. In STOC, 2020b.
Shunhua Jiang, Zhao Song, Omri Weinstein, and Hengjie Zhang. Faster dynamic matrix inverse for
faster lps. In STOC, 2021.
Felix Krahmer, Shahar Mendelson, and Holger Rauhut. Suprema of chaos processes and the re-
stricted isometry property. Communications on Pure and Applied Mathematics, 67(11):1877-
1904, 2014.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep con-
volutional neural networks. Advances in neural information processing systems, 25:1097-1105,
2012.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
11
Under review as a conference paper at ICLR 2022
Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide Neural Networks of Any Depth Evolve as Linear Models
under Gradient Descent. 2019a.
Yin Tat Lee, Aaron Sidford, and Sam Chiu-wai Wong. A faster cutting plane method and its impli-
cations for combinatorial and convex optimization. In Foundations of Computer Science (FOCS),
2015 IEEE 56th Annual Symposium on, pp. 1049-1065. IEEE, 2015.
Yin Tat Lee, Zhao Song, and Qiuyi Zhang. Solving empirical risk minimization in the current matrix
multiplication time. In Conference on Learning Theory (COLT), pp. 2140-2157. PMLR, 2019b.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In NeurIPS, 2018.
Chaoyue Liu, Libin Zhu, and Misha Belkin. On the linearity of large non-linear models: when and
why the tangent kernel is constant. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and
H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 15954-15964.
Curran Associates, Inc., 2020.
Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Loss landscapes and optimization in over-
parameterized non-linear systems and neural networks, 2021.
Yichao Lu, Paramveer Dhillon, Dean P Foster, and Lyle Ungar. Faster ridge regression via the sub-
sampled randomized hadamard transform. In Advances in neural information processing systems
(NIPS), pp. 369-377, 2013.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In Proceedings of the 32nd International Conference on International Conference on
Machine Learning - Volume 37, ICML’15, pp. 2408-2417. JMLR.org, 2015.
Jelani Nelson and HUy L NguyAn. Sparsity lower bounds for dimensionality reducing maps. In
Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pp. 101-110,
2013.
Mert Pilanci and Martin J. Wainwright. Newton sketch: A near linear-time optimization algorithm
with linear-quadratic convergence. SIAM J. Optim., 27:205-245, 2017.
Holger Rauhut, Justin Romberg, and Joel A Tropp. Restricted isometries for partial random circulant
matrices. Applied and Computational Harmonic Analysis, 32(2):242-254, 2012.
Mark Rudelson and Roman Vershynin. Non-asymptotic theory of random matrices: extreme singu-
lar values. In Proceedings of the International Congress of Mathematicians 2010 (ICM 2010) (In
4 Volumes) Vol. I: Plenary Lectures and Ceremonies Vols. II-IV: Invited Lectures, pp. 1576-1602.
World Scientific, 2010.
Tamas Sarlos. Improved approximation algorithms for large matrices via random projections. In
2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS), pp. 143-152.
IEEE, 2006.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484-489, 2016.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. nature, 550(7676):354-359, 2017.
Zhao Song and Xin Yang. Quadratic suffices for over-parametrization via matrix chernoff bound.
arXiv preprint arXiv:1906.03593, 2019.
Zhao Song and Zheng Yu. Oblivious sketching-based central path method for linear programming.
In International Conference on Machine Learning (ICML), pp. 9835-9847. PMLR, 2021.
12
Under review as a conference paper at ICLR 2022
Zhao Song, David P. Woodruff, Zheng Yu, and Lichen Zhang. Fast sketching of polynomial kernels
of polynomial degree. In ICML, 2021.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings ofthe IEEE conference on computer vision and pattern recognition, pp. 1-9, 2015.
Pravin M Vaidya. Speeding-up linear programming using fast matrix multiplication. In 30th Annual
Symposium on Foundations of Computer Science, pp. 332-337. IEEE, 1989.
Virginia Vassilevska Williams. Multiplying matrices faster than coppersmith-winograd. In Proceed-
ings of the forty-fourth annual ACM symposium on Theory of computing (STOC), pp. 887-898.
ACM, 2012.
David P. Woodruff. Sketching as a tool for numerical linear algebra. Foundations and Trends in
Theoretical Computer Science, 10(1-2):1-157, 2014.
David P Woodruff and Amir Zandieh. Near input sparsity time kernel embeddings via adaptive
sampling. In ICML, 2020.
Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, and Michael Mahoney.
Adahessian: An adaptive second order optimizer for machine learning. Proceedings of the AAAI
Conference on Artificial Intelligence, 35(12):10665-10673, May 2021.
Guodong Zhang, James Martens, and Roger B Grosse. Fast convergence of natural gradient descent
for over-parameterized neural networks. In Advances in Neural Information Processing Systems
(NeurIPS), 2019.
Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural
networks. In NeurIPS, pp. 2053-2062, 2019.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks, 2018.
13
Under review as a conference paper at ICLR 2022
Roadmap. In Section A, we remind readers with the notations and some probability tools. In
Section B, we illustrate the complete version of our algorithm and give a runtime analysis of it.
In Section C, we design a simple low rank maintenance data structure and show how to use it to
efficiently implement matrix-vector product. In Section D, we introduce an efficient regression
solver handling our Jacobian and Gram regression. In Section E, we study the spectral property of
the Gram matrix at each layer and connects it with multi-layer neural tangent kernels. In Section F,
we analyze the convergence of our algorithm by using some heavy machinery such as structural
analysis of the gradient and a careful double induction. In Section G, we give a detailed proof of
one technical lemma.
A Preliminaries and Probability tools
In this section, we introduce notations that will be used throughout the rest of the paper and several
useful probability tools that will be heavily exploited in the later proofs.
Notations. For any positive integer n, We use [n] to denote the set {1, 2,…，n}. We use EH to
denote expectation and Pr[∙] for probability. We use ∣∣χk2 to denote the '2 norm of a vector x. We
use kAk to denote the spectral norm of matrix A. We use kAkF to denote the Frobenius norm of
A. We use A> to denote the transpose of matrix A. We use Im to denote the identity matrix of size
m × m. For matrix A or vector x, We use ∣A∣0 , ∣x∣0 to denote the number of nonzero entries of
A and X respectively. Note that ∣∙∣o is a semi-norm since it satisfies triangle inequality. Given a
real square matrix A, We use λmax(A) and λmin(A) to denote its largest and smallest eigenvalues
respectively. Given a real matrix A, We use σmax (A) and σmin (A) to denote its largest and smallest
singular values respectively. We use N(μ, σ2) to denote the Gaussian distribution with mean μ and
variance σ2. We use O(f (n)) to denote O(f (n) ∙ Polylog(f (n)). We use(., •〉to denote the inner
product, when applying to two vectors, this denotes the standard dot product between two vectors,
and when applying to two matrices, this means hA, Bi = tr[A>B] where tr[A] denote the trace of
matrix A.
Lemma A.1 (Chernoff bound Chernoff (1952)). Let X = Pin=1 Xi, where Xi = 1 with probability
pi and Xi = 0 with probability 1 - pi, and all Xi are independent. Let μ = E[X] = in=1 pi. Then
1. Pr[X	≥ (1 +	δ)μ]	≤ exp(-δ2μ∕3),	∀δ	> 0 ;
2.	Pr[x	≤(1 —	δ)μ]	≤ exp(-δ2μ∕2),	∀0	< δ <	1.
Lemma A.2 (HOefding bound Hoeffding (1963)). Let Xi, ∙∙∙ , Xn denote n independent bounded
variables in [ai , bi]. Let X =	in=1 Xi, then we have
Pr[|X -E[X]| ≥ t] ≤ 2exP
Lemma A.3 (Bernstein inequality Bernstein (1924)). Let Xi,…，Xn be independent zero-mean
random variables. Suppose that |Xi | ≤ M almost surely, for all i. Then, for all positive t,
Pn=i(bi- a/
n
Pr X Xi > t
i=i
≤ exp (- Pn=IEX] + Mt/3).
Lemma A.4 (Anti-concentration of Gaussian distribution). Let X 〜 N(0, σ2 ), then
2t4t
Pr[1X1s t]∈ (3 σ∙, 5 σ∙).
Lemma A.5 (Concentration of subgaussian random variables). Let a ∈ Rn be a vector where each
entry of a is sampled from a subgaussian distribution of parameter σ2, then for any vector x ∈ Rn,
t2
Pr[|〈a,x)| ≥ t ∙ ∣x∣2] ≤ 2exp(— -2).
2σ2
Lemma A.6 (Small ball probability). Let a ∈ Rn be a vector such that |ai | ≥ δ for all i ∈ [n],
xi, . . . , xn are n i.i.d. Rademacher random variables. Then, there exist absolute constants Ci, C2
such that for any t > 0,
Pr[|ha, xi| ≤ t] ≤ min
14
Under review as a conference paper at ICLR 2022
B Complete Algorithm and its Runtime Analysis
In this section, we first present our complete algorithm, then give a runtime analysis of it.
Algorithm 2 Complete version of our algorithm.
1	: procedure COMPLETEALGORITHM(X ∈ Rd×n , y ∈ Rn)	. Theorem B.1
2	:	/*Initialization*/	
3	Initialize We (0), ∀' ∈ [L]	
4	:	Store W1(0)xi in memory, ∀i ∈ [n]	. Takes O(nm2) time
5	:	LowRankMaintenance LMR	. Algorithm 3
6	:	LMR.INIT({W1(0) . . .,WL(0)})	
7	:	for t = 0 → T do	
8	:	/*Forward computation*/	
9	:	for ` = 1 → L do	
10	vi,' J hi,'-1, ∀i ∈ [n]	
11	gi,' J LMR.Query(', hi,'-i)	. Takes o(nm2) time
12	:	hi,' J φ(gi,'), ∀i ∈ [n]	
13		. hi,' is sparse
14	:	Di,' J diag(φ0(gi,')), ∀i ∈ [n]	. Takes O(nm) time
15		. Di,' is sparse
16	:	end for	
17	:	ft J [a>h1,L, . . . , a>hn,L]>	. Takes O(nm) time
18	:	/*Backward computation*/	
19	:	for ` = L → 1 do	
20	:	ui,' J a>Di,LWL(t) . . . Di,'+1W'+1(t)Di,', ∀i ∈ [n]	. Takes o(nLm2 ) time
21	:	g' J FASTTENSORREGRESSION({ui,'}in=1, {vi,'}in=1,	C) with precision ,λ∕n
22		. Algorithm 6
23	:	LMR.UPDATE({g',iui,'}in=1, {vi,'}in=1)	
24	:	end for	
25	:	end for	
26	: end procedure	
Theorem B.1 (Formal version of Theorem 1.2). Let X ∈ Rd×n and y ∈ Rn, and let k denote the
sparsity of Di,' and S denote the SparSity of ∆Di,', ∀' ∈ [L], i ∈ [n]. Let m denote the width of
neural network, L denote the number of layers and α denote the dual matrix multiplication exponent
(Def. C.1),then the running time of Algorithm 2 is
O(Tnit + T ∙Tter),
where
Tinit = O(m2 (n + L)),
Titer = Oe((m1+α + m(s + k))L2 + m2-αnL).
Therefore, the cost per iteration of Algorithm 2 is
Oe((m1+α + m(s + k))L2 + m2-αnL).
Proof. We analyze Tinit and Titer separately.
Initialization time. We will first initialize (L - 1) m × m matrices and one m × d matrix, which
takes O(m2L) time. Compute W1 (0)xi for all i ∈ [n] takes O(m2n) time. Finally, initialize the
data structure takes O(m2L) time. Hence, Tinit = O(m2 (n + L)).
Cost per iteration. For each iteration, we perform one forward computation from layer 1 to L, then
backpropagate from layer L to 1.
• Forward computation: In forward computation, we first compute gi,` ∈ Rm, which is
equivalent to form vi,'+ι, hence by Lemma C.5, it takes O(S + k + ma) time. Compute
hi,' and D%,' takes O(m) time. These computations would be performed for all L layers,
hence the overall runtime of forward computation is O((S + k + mα)L) time.
15
Under review as a conference paper at ICLR 2022
•	Backpropagation: In backpropagation, we first compute ui,` ∈ Rm, which takes
O(mL(s + k + mα)) time. Then, we call Algorithm 6 to solve the Gram regres-
sion problem, which due to Theorem D.14 takes O(mn + nω) time. Note that even
we want a high probability version of the solver with e- log2 nL failure probability so
that we can union bound over all layers and all iterations, we only pay extra log2 nL
term in running time, which is absorbed by the O(∙) notation. Finally, the update takes
O(m2-αn) time by Lemma C.2. Sum over all L layers, we get an overall running time of
Oe((m1+α + m(s + k))L2 + m2-αnL) time.
This concludes the proof of our Theorem.
□
Corollary B.2. Suppose the network width m is chosen as in F.25 and the shift parameter b is
chosen as in F.6, then the cost per iteration of Algorithm 2 is
Oe(m1.8L2 + m1.69nL).
Remark B.3. As long as the neural network is wide enough, as in F.25 and we choose the shifted
threshold properly, as in F.6, then we can make sure that both sparsity parameters k and s to be
o(m), and we achieve subquadratic cost per iteration.
C Low rank maintenance and efficient computation of the
CHANGE
In this seciton, we design a data structure to maintain the low rank representation of change of
weights, and then we show how to efficiently compute the low rank representation using this data
structure.
C.1 Low rank maintenance
In this short section, we design a simple data structure to maintain the low rank representation of
change of weights, and show that the matrix-vector product can be implemented efficiently.
Before moving, we define some notions related to rectangular matrix multiplication.
Definition C.1 (Williams (2012); Gall & Urrutia (2018)). Let ω be the matrix multiplication expo-
nent such that it takes nω+o(1) time to multiply two n × n matrices.
Let α be the dual exponent of the matrix multiplication which is the supremum among all a ≥ 0 such
that it takes n2+o(1) time to multiply an n × n by n × na matrix.
Additionally, we define the function ω(∙) where ω(b) denotes the exponent of multiPlying an n X n
matrix by an n × nb matrix. Hence, we have ω(1) = ω and ω(α) = 2.
The overall idea of our low rank maintenance data structure is as follows: we keep accumulating the
low rank change, when the rank of the change reaches a certain threshold (mα), then we restart the
data structure and update the weight matrix.
16
Under review as a conference paper at ICLR 2022
Algorithm 3 Low rank maintenance data structure
1	: data structure LOWRANKMAINTENANCE	. Lemma C.2
2	:	members
3	r`, ∀' ∈ [L]	. r` denotes the accumulated rank of the change
4	w`,∀' ∈ [L]	. {W'}L=1 ∈ (Rm×m)L
5	△%, ∀' ∈ [l]	. {∆W'}L=ι ∈ (Rm×m)L
6	:	end members
7	
8	:	procedures
9	:	INIT({W1(0), . . . WL (0)})	. Initialize the data structure
10	UPDATE(a, v`)	. Update the low rank representation
11	Query (',y)	. Compute the matrix-vector product between ∆W' and y
12	:	end procedures
13	: end data structure
Algorithm 4 Procedures of LRM data structure
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20	: procedure INIT({W1(0), . . . , WL (0)})	. Lemma C.2 W' J %(0) ∆W' J 0, ∀' ∈ [L] r` J 0,∀' ∈ [L] : end procedure procedure UPDATE(a ∈ Rm×n,匕 ∈ Rm×n)	. Lemma C.2 ∆W' J ∆W' + U'V> without forming the product and sum the two matrices :	r` J r` + n :	if r` = ma where a = ω(2) then W' J W' + ∆W'	. Takes O(m2) time :	r` J 0 :	△W' J 0 :	end if : end procedure procedure QUERY(' ∈ [L],y ∈ Rm)	. Lemma C.2 Z J W' ∙ y + ∆W' ∙ y	. Takes O(nnz(y) ∙ m + mr`) time :	return z : end procedure
Lemma C.2. There exists a deterministic data structure (Algorithm 3) such that maintains
∆W1 , . . . , ∆WL such that
•	The procedure INIT (Algorithm 4) takes O(m2L) time.
•	The procedure UPDATE (Algorithm 4) takes O(nm2-α) amortized time, where α = ω(2)
•	The procedure QUERY (Algorithm 4) takes O(m ∙ (nnz(y) + r`)) time, where r` is the rank
of ∆W' when Query is called.
Proof. The runtime for INIT is obvious, for QUERY, notice that we are multiplying vector y with
a (possibly) dense matrix w` ∈ Rm×m, which takes O(nnz(y) ∙ m) time, and an accumulated
low rank matrix ∆W' with rank r`. By using the low rank decomposition ∆W' = UV> with
U, V ∈ Rm×r', the time to multiply y with ∆W is θ(mr`). Combine them together, we get a
running time of O(m ∙ (nnz(y) + r`)).
It remains to analyze the amortized cost of UPDATE. Note that if r` < ma, then we just pay O(1)
time to update corresponding variables in the data structure. If r` = ma, then we will explicitly
form the m X m matrix △%. To form it, notice we have accumulated r`/n different sums of
17
Under review as a conference paper at ICLR 2022
rank-n decompositions, which can be represented as
U =[U' (1),U'(2),...,U'(r'∕n)] ∈ Rm×r' ,V =[%(1),匕(2),...,匕(r`/n)] ∈ Rm×r',
and ∆W' = UV>, which takes O(m2) time to compute since r` = ma and a = ω(2). Finally, note
that this update of w` only happens once per r`/n number of calls to Update, therefore We can
charge each step by O(^mn) = O(m2-an) = O(m2-αn), arrives at our final amortized running
time.	□
Remark C.3. Currently, the dual matrix multiplication exponent α ≈ 0.31 (Gall & Urrutia, 2018),
hence the amortized time for UPDATE is O(nm1.69). If m ≥ n10/3, then we achieve an update time
of o(m2). Similarly, the time for QUERY is O(m ∙ (nnz(y) + r`)) = O(m ∙ nnz(y) + m1+α)=
O(m ∙ nnz(y) + m1.31), as long as nnz(y) = o(m), then its running time is also o(m2). In our
application of training neural networks, we will make sure that the inputted vector y is sparse.
C.2 EFFICIENTLY COMPUTE ui,`(t) AND vi,`(t)
In this section, we show that how to compute the vectors ui,`, vi,` ∈ Rm using the low rank structure
of the change of weights combined. Recall the definition of these vectors:
Ui,'(t)> = a>Di,L(t)WL(t)... Di,'+1(t)W'+1(t)Di,'(t) ∈ R1×m,
vi,'(t) = hi,`-i(t) ∈ Rm
Before proceeding, we list the assumptions we will be using:
•	For any ' ∈ [L], Di,'(t) is SD-sparse, where SD := k + s, k is the sparsity of Di,'(0) and
S is the sparsity of Di,'(t) - Di,'(0).
•	For any ' ∈ [L], the change of the weight matrix W', ∆W^(t) := W'(t) - We(0), is of
low-rank. That is, ∆W'(t) = £：“ y',jz>.
•	For any i ∈ [n], W1(0)xi is pre-computed.
We first note that as a direct consequence of Di,'(0) is k-sparse, hi,'(0) is k-sparse as well. Simi-
larly, hi,'(t) - hi,'(0) has sparsity S. Hence hi,'(t) has sparsity bounded by SD.
Compute ui,'(t). Compute ui,'(t) is equivalent to compute the following vector:
Di,'(t)(W'+ι(0) + ∆Wm(t))>Di,'+ι(t)…(Wl(0) + ∆Wι(t))τDi,ι(t)a.
First, we know that Di,L(t)a ∈ Rm is an SD-sparse vector, and it takes O(SD) time. The next matrix
is (WL(0)+∆WL(t))τ, which gives two terms: WL(0)τ(Di,L(t)a) and ∆WL(t)τ(Di,L(t)a). For
the first term, since Di,L(t)a is SD-sparse, it takes O(mSD)-time. For the second term, we have
rt	rt
δWL⑴>(Di,L⑴a) = X ZLj y>,j (Di,L⑴a) = X zL,j ∙ hyL,j ,Di,L⑴ai.
j=1	j=1
Each inner-product takes O(SD)-time and it takes O(mrt + SDrt) = O(mrt)-time in total. Hence,
in O(m(SD + rt))-time, we compute the vector WL(t)τDi,L(t)a. Note that we do not assume the
sparsity of a.
Thus, by repeating this process for the L - ` intermediate matrices Wjτ(t)Di,j (t), we can obtain
the vector
I YY W> (t)Di,j (t) I a
j ='+1
in time O((L - ')m(sD + rt)). Finally, by multiplying a sparse diagonal matrix Di,'(t), we get the
desired vector ui,'(t).
18
Under review as a conference paper at ICLR 2022
Compute vi,`(t). Note that vi,`(t) is essentially hi,`-i(t), so We consider how to compute hi,`(t)
for general ` ∈ [L]. Recall that
hi,`(t) = φ((W'(0) + ∆W'(t))hi,'-ι(t)),
since hi,`-i(t) is SD-sparse, the product we(θhi,`-i(t))) Canbe computed in O(ms°) time. For the
product ∆W'(t)h%'-ι(t) can be computed use the low rank decomposition, which takes O(mrt)
time. Apply the threshold ReLU takes O(m) time. Hence, the total time is O(m(rt + sD)) time.
The running time results are summarized in the following lemma:
Lemma C.4. For ' ∈ [L] and i ∈ [n], suppose ∣∣Di,'(0)ko ≤ k. Let t > 0. Suppose the Change of
Di,' is sparse, i.e., ∣Di,' (t) 一 Di,'(0)∣o ≤ S. For ' ∈ [L], i ∈ [n], for any t > 0, suppose the change
of W' is of low-rank, i.e., ∆W^(t) = Er=I y',jZ>j Wefurther assume that {y',j ,z`,j }'曰二],/日九]
and {W1(0)xi}i∈[n] are pre-computed.
Then, for any ` ∈ [L] and i ∈ [n], the vectors u',i(t), v',i(t) ∈ Rm can be computed in O(mL(S +
k + rt))-time.
As a direct consequence, if we combine Lemma C.2 and Lemma C.4, then we get the following
corollary:
Corollary C.5. For ` ∈ [L] and i ∈ [n], we can compute vi,'(t), ui,'(t) ∈ Rm as in Algorithm 2
with the following time bound:
•	Compute ui,'(t) in time O(mL(S + k + r')).
•	Compute vi,'(t) in time O(m(S + k + r')).
D	Fast Tensor Product Regression
In this section, we design a generic algorithm for solving the following type of regression task:
Given two matrices U =	[u1> , . . . ,	un> ]> ,	V =	[v1> ,	. . .	, vn> ]	∈	Rm×n	with m	n,	consider the
matrix J ∈ Rn×m2 formed by
vec(u1 v1> )>
vec(u2v> )>
J =	.
.
.
.
vec(unvn> )>
We are also given a vector c ∈ Rn , the goal is to solve the following regression task:
min ∣JJ>x 一 c∣22 .
x∈Rn	2
Our main theorem for this section is as follows:
Theorem D.1 (Restatement of Theorem D.14). Given two n × m matrices U and V, and a target
vector c ∈ Rn. Let J = [vec(u1 v1>)>, . . . , vec(unvn>)>] ∈ Rn×m2. There is an algorithm (Algo-
rithm 6) takes O(nm + n2 (log(κ∕e) + log(m∕eδ)e-2) + nω) time and outputs a vector b ∈ Rn
such that
∣JJ>xb 一 c∣2 ≤ e∣c∣2
holds with probability at least 1 一 δ, and κ is the condition number of J.
D.1 APPROXIMATE J VIA TensorSketch
We introduce the notion of TensorSketch for two vectors:
Definition D.2. Let h1, h2 : [m] → [S] be 3-wise independent hash functions, also let σ : [m] →
{±1} be a 4-wise independent random sign function. The degree two TensorSketch transform,
S : Rm × Rm → Rs is defined asfollows: for any i,j ∈ [m] and r ∈ [S],
Sr,(i,j) = σι(i) ∙ σ2(j) ∙ 1[hι(i) + h2(j) = r mod s].
19
Under review as a conference paper at ICLR 2022
Remark D.3. Apply S to two vectors x, y ∈ Rm can be implemented in time O(s log s + nnz(x) +
nnz(y)).
We introduce one key technical lemma from Avron et al. (2014):
Lemma D.4 (Theorem 1 of Avron et al. (2014)). Let S ∈ Rs×m2 be the TensorSketch matrix,
consider a fixed n-dimensional subspace V. If S = Ω(n2∕(e2δ)), then with probability at least
1 - δ, kS xk2 = (1 ± )kxk2 simultaneously for all x ∈ V.
Now we are ready to prove the main lemma of this section:
Lemma D.5. Let e, δ ∈ (0, 1) denote two parameters. Let J ∈ Rn×m2 be a matrix such that i-th
row ofJ is defined as vec(uivi>) for some ui, vi ∈ Rm. Then, we can compute a matrix J ∈ Rn×s
such that for any vector x ∈ Rn, with probability at least 1 - δ, we have
kJe>xk2 = (1 ± e)kJ>xk2,
i	rʌ/ 9. // 9. e∖∖ FI . ∙	，	.	% * C ι	.	/ r τ^∖	∕ττ∖∖
where S = Ω(n2∕(e2δ)). The time to compute J is O (ns log S + nnz(U) + nnz(V)).
Proof. Notice that the row space of matrix J can be viewed as an n-dimensional subspace, hence,
by Lemma D.4, the TensorSketch matrix S with S = Ω(n2/(e2δ)) can preserve the length of all
vectors in the subspace generated by J> with probability 1 - δ, to a multiplicative factor of 1 ± e.
The running time part is to apply the FFT algorithm to each row of J with a total of n rows. For
each row, it takes O(S log S + m) time, hence the overall running time is O(n(s log S + m)).	□
D.2 APPROXIMATE J VIA TensorSRHT
We note that the dependence on the target dimension of sketching is O(1∕δ) for TensorSketch. We
introduce another kind of sketching technique for tensor, called TensorSRHT. The tradeoff is we
lose input sparsity runtime of matrices U and V.
Definition D.6. We define the TensorSRHT S : Rd X Rd → Rm as S = √= P ∙ (HDι X HD?),
where each row ofP ∈ {0, 1}m×d2 contains only one 1 at a random coordinate, one can view P as
a sampling matrix. H is a d X d Hadamard matrix, and D1 , D2 are two d X d independent diag-
onal matrices with diagonals that are each independently set to be a Rademacher random variable
(uniform in {-1, 1}).
Remark D.7. By using FFT algorithm, apply S to two vectors x, y ∈ Rm takes time O(m log m+S).
We again introduce a technical lemma for TensorSRHT.
Lemma D.8 (Theorem 3 of Ahle et al. (2020)). Let S ∈ Rs×m2 be the TensorSRHT matrix, con-
sider a fixed n-dimensional subspace V. If S = Ω(n log3(nm∕eδ)e-2), then with probability at
least 1 - δ, kS xk2 = (1 ± e)kxk2 simultaneously for all x ∈ V.
Lemma D.9. Let e, δ ∈	(0, 1) denote two parameters. Given a list of vectors
uι, ∙∙∙ , Um, vι, ∙∙∙ ,Vm ∈ Rm. Let J ∈ Rn×m be a matrix with i-th row of J is defined as
vec(uivi>). Then, we can compute a matrix J ∈ Rn×s such that for any vector x ∈ Rn, with
probability at least 1 - δ, we have
kJe>xk2 = (1 ± e)kJ>xk2,
32
where S = Ω(nlog (nm∕(eδ))e 2). The time to compute J is O(n(m log m + S)).
Proof. The correctness follows directly from Lemma D.8. The running time follows from the FFT
algorithm to each row of J, each application takes O(mlog m + S) time, and we need to apply it to
n rows.	□
20
Under review as a conference paper at ICLR 2022
D.3 Tensor trick as a preconditioner
In this section, we use TensorSketch and TensorSRHT as a preconditioner to solve a regression task
involving JJ> . This is an important step to implement the Gram-Newton-Gauss iteration Cai et al.
(2019); Brand et al. (2021).
Before proceeding, we introduce the notion of subspace embedding:
Definition D.10 (Subspace Embedding, Sarlos (2006)). Let A ∈ RN×k, we say a matrix S ∈ Rs×N
is a (1 ± ) - `2 subspace embedding for A if for any x ∈ Rk, we have kSAxk22 = (1 ± )kAxk22.
Equivalently, kI - U >S>SU k ≤ where U is an orthonormal basis for the column space of A.
We will mainly utilize efficient subspace embedding.
Definition D.11 (Lu et al. (2013); Woodruff (2014)). Given a matrix A ∈ RN×k with N = poly(k),
then we can compute an S ∈ RkPOly(IOg^∕6"' ×N Such that With probability at least 1 一 δ, we have
kSAxk2 = (1 ± )kAxk2
hols for all x ∈ Rk. Moreover, SA can be computed in O(Nk log((k log N)/)) time.
Algorithm 5 Fast Regression algorithm of Brand et al. (2021)]
1:	procedure FASTREGRESSION(A, y, )	. Lemma D.12
2:	. A ∈ RN×k is full rank, ∈ (0, 1/2)
3:	Compute a subspace embedding SA	. S ∈ RkPOly(lOg k) × N
4:	Compute R such that SAR has orthonormal columns via QR decomposition . R ∈ Rk×k
5:	z0 = 0k ∈ Rk
6:	t — 0
7:	while kA> ARzt 一 yk2 ≥ do
8:	zt+ι — Zt 一 (R>A>AR)>(R> AARzt 一 R>y)
9:	t V— t + 1
10:	end while
11:	return Rzt
12:	end procedure
Lemma D.12 (Lemma 4.2 of Brand et al. (2021)). Let N = Ω(kpoly(logk)). Given a matrix
A ∈ RN×k, let κ denote its condition number. Consider the following regression task:
min kA>Ax 一 yk2.
x∈Rk
Using the procedure FASTREGRES SION (Algorithm 5), with probability at least 1 一 δ, we can
compute an -approximate solution xb satisfying
kA>Axb 一 yk2 ≤ kyk2
CT，_____ ，	，、	....
in time O(Nk log(κ/) + kω).
Our algorithm is similar to the ridge regression procedure in Song et al. (2021), where they first
apply their sketching algorithm as a bootstrapping to reduce the dimension of the original matrix,
then use another subspace embedding to proceed and get stronger guarantee.
We shall first prove a useful lemma.
Lemma D.13. Let A ∈ RN×k, suppose SA is a subspace embedding for A (Def. D.11), then we
have for any x ∈ Rk , with probability at least 1 一 δ,
k(SA)>SAx - b∣∣2 = (1 ± e)∣∣A>Ax 一 b∣∣2.
Proof. Throughout the proof, we condition on the event that S preserves the length of all vectors in
the column space of A.
Note that
k(SA)>SAx 一 bk22 = k(SA)>SAxk22 + kbk22 一 2h(SA)>SAx,bi.
21
Under review as a conference paper at ICLR 2022
We will first bound the norm of (SA)>SAx, then the inner product term.
Bounding k(SA)>SAxk22
Let U ∈ RN ×k be an orthonormal basis of A, then use the equivalent definition of subspace embed-
ding, we have kU>S>SU - Ik ≤ , this means all the eigenvalues of U>S>SU lie in the range
of of [(1 - )2, (1 + )2]. Let V denote the matrix U>S>SU, then we know that all eigenvalues of
V >V lie in range [(1 - )4, (1 + )4]. Setting as /4, we arrive at kV >V - Ik ≤ . This shows
that for any x ∈ Rk, we have k(SA)>SAxk2 = (1 ± )kA>Axk2.
Bounding h(S A)> S Ax, bi Note that
h(SA)>SAx, bi = hSAx,SAbi
= 1/2 ∙(kSAxil2 + IlSAbI∣2 -IlSA(X- b)k2)
= 1/2 ∙(1 土 E)(IIAxk2 + IIAbk2 -kA(x- b)k2)
= (1 ± )hA>Ax, bi.
Combining these two terms, we conclude that, with probability at least 1 - δ,
I(SA)>SAx-bI2 = (1±E)IA>Ax-bI2.
□
Algorithm 6 Fast Regression via tensor trick
1:	procedure FASTTENSORREGRESSION({ui}n=ι ∈ Rm×n, {vi}n=ι ∈ Rm×n, C ∈ Rn)	.
Theorem D.14
2:	. J = [vec(u1v1>)>, vec(u2v2>)>, . . . , vec(un vn>)>]> ∈ Rn×m
3:	si - Θ(n log3(nm∕(eδ))e-2)
4:	S2 J Θ((n + log m) logn)
5:	Let S1 ∈ Rs1×m2 be a sketching matrix	. S1 can be TensorSketch or TensorSRHT
6:	Compute Je= JS1> via FFT algorithm	. Je∈ Rn×s1
7:	Let S2 ∈ Rs2×s1 be a sketching matrix defined in Definition D.11
8:	Compute a subspace embedding S2 Je>
9:	Compute R such that S2 J>R has orthonormal columns via QR decomposition	.
R ∈ Rn×n
10:	z0 J 0k ∈ Rk
11:	tJ0
12:	while IJeJe> Rzt - cI2 ≥ e do
13:	zt+1 J zt - (R>JeJe>R)>(R>JeJe>Rzt - R>c)
14:	t J t + 1
15:	end while
16:	return Rzt
17:	end procedure
Theorem D.14. Given two n × m matrices U and V, and a target vector c ∈ Rn. Let J =
[vec(u1 v1>)>, . . . , vec(un vn>)>] ∈ Rn×m2. There is an algorithm (Algorithm 6) takes O(nm +
n2 (log(κ∕e) + log(m∕δ)) + nω) time and outputs a vector b ∈ Rn such that
IJJ>xb - cI2 ≤ eIcI2
holds with probability at least 1 - δ, and κ is the condition number of J.
Proof. We can decompose Algorithm 6 into two parts:
•	Applying S1 to efficiently form matrix J to approximate J and reduce its dimension, notice
here we only need e for this part to be a small constant, pick e = 0.1 suffices.
•	Using S2 as a preconditioner and solve the regression problem iteratively.
22
Under review as a conference paper at ICLR 2022
Let xb denote the solution found by the iterative regime. We will prove this statement in two-folds:
•	First, we will show that kJeJe>xb - ck2 ≤ kck2 with probability at least 1 - δ;
•	Then, we will show that kJJ>xb - ck2 = (1 ± 0.1)kJeJe>xb - ck2 with probability at least
1-δ.
Combining these two statements, we can show that
kJJ>xb- ck2 = (1 ± 0.1)kJeJe>xb - ck2
≤ 1.1kck2
Setting e to e/1.1 and δ to δ∕2, we conclude our proof. It remains to prove these two parts.
Part 1. kJeJe>xb- ck2 ≤ ekck2 We observe the iterative procedure is essentially the same as running
FASTREGRESSION on input Je>, y, e, hence by Lemma D.12, we have with probability at least 1 -δ,
kJeJe>xb - ck2 ≤ ekck2.
Part 2. kJJ>xb - ck2 = (1 ± 0.1)kJeJe>xb - ck2 To prove this part, note that by Lemma D.8, we
know that Je> is a subspace embedding for J>. Hence, we can utilize Lemma D.13 and get that,
with probability at least 1 - δ, we have kJJ>xb - ck2 = (1 ± 0.1)kJeJe>xb - ck2.
Combining these two parts, we have proven the correctness of the theorem. It remains to justify the
running time. Note that running time can be decomposed into two parts: 1). The time to generate J,
2). The time to compute xb via iterative scheme.
2
Part 1.	Generate J To generate J, we apply S1 ∈ Rs1×m which is a TensorSRHT. By
3
Lemma D.9, it takes O(n(mlog m + sι)) time to compute J, plug in si = Θ(nlog (nm∕δ)),
the time is O(nm).
Part 2.	Compute xb To compute xb, essentially we run FASTREGRESSION on Je> , c, e, hence by
Lemma D.12, it takes O(s2n log(κ∕e) + nω) time, with s2 = Θ((n + log m) log n) and κ is the
condition number of J, which has the guarantee κ = (1 ± e)κ(J). Hence, the overall running time
of this part is O(n2 log(κ∕e) + nω).
Put things together, the overall running time is O(nm + n2 log(κ∕e) + nω).	□
Remark D.15. Due to the probability requirement (union bounding over all layers and all data
points), here we only prove by using TensorSRHT. One can use similar strategy to obtain an input
sparsity time version using TensorSketch. We remark that this framework is similar to the ap-
proach Song et al. (2021) takes to solve kernel ridge regression, where one first uses a shallow but
fast sketch to bootstrap, then use another sketching to proceed with the main task.
E S pectral Properties of Over-parametrized Deep Neural
Network
In this section, we study the spectral properties of our Gram matrix and connects it to multi-layer
NTK.
E.1 Bounds on the least eigenvalue of kernel at initialization
We first define the Gram matrices for multiple layer neural network.
Definition E.1 (Multiple layer Gram matrix). The Gram matrices k` ∈ Rn×n for ' ∈ {0,... ,L}
of an L-layer neural network are defined as follows:
• (K0)i,j := xi>xj
23
Under review as a conference paper at ICLR 2022
• For ' > 0,let Σ'ij	:= (K'-I)i,i	(K'-I)i,j	∈ R2 ×2 forany	(i,j) ∈ [n]	X [n] .Then,
',i,j [(K'-)；i	(K'-)j	'	' L j l j
(K')i,j :=	E	[φ(XI)φ(X2)] ∀' ∈ [L T],
(xi ,X2)〜N(0,2∑'-1,i,j )
(KL)i,j :=	E	[φ0(X1)φ0(X2)]
(xi ,X2)〜N(0,2∑L-1,i,j )
Let λL := λmin(KL) to be the minimum eigenvalue of the NTK kernel KL.
In the following lemma, we generalize Lemma C.3 in Brand et al. (2021) (also Lemma 3 in Cai et al.
(2019)) into multiple layer neural networks.
Lemma E.2 (Bounds on the least eigenvalue at initialization, multiple layer version of Lemma C.3
in Brand et al. (2021)). Let λ' denote the minimum eigenvalue OfNTK defined for '-th layer of
neural networks. Suppose m` = Ω(λ-2n2 log(n∕δ)), then with probability 1 一 δ, we have
3
λmin (G'(0)) ≥ 4λ', ∀' ∈ [L]
Proof. For any ' ∈ [L], We have g` = J'J> ∈ Rn×n. Hence, for any i,j ∈ [n],
(G`)i，j=vec( "Reel	)
VeC (Di,' Y w>Di,kah>'-i) VeC (Dj,' Y w>Dj,kah>'-i
k k='+1	)	∖ k='+1
=((hi,'-1 ③ Img)	(Di,'	Y	w>Di,ka))	(hj,'-1 ③	Img)	(Dj,'	Y	W>	Dj,ka
∖	∖	k='+1	) )	∖	k='+1	,
=(Di,'	Y	W>Di,kα∖	(h>'-i	③ Img)(hj,`-i	③ Img)	(Dj,' Y	W>Dj,ka)
∖	k='+1	k='+1	)
=a> (Di,' Y	W>Di,j	(θj,' Y	W> j)	a ∙4…电…,
∖ k='+1	)	∖	k='+1	)
Where
`-1
hi,`-1 =	Di,kWkXi.
k=1
In particular,
(GL)i,j = a>Di,LDj,La ∙ h>,L-1 hj,L-1
m
=m X φ(hwLr, hi,L-l'i')φ0 (hwLr), hj,L-1i')hi,L-1hj,L-1	(4)
r=1
We will prove that ∣∣Gl 一 Kl∣∣∞ is small, which implies that λmin(GL) is close to λ'. The proof
idea is similar to Du et al. (2019a) via induction on `.
For ' = 1,recall (gι,i)k = Σb∈[m](Wι)k,b(xi)b for k ∈ [m]. Hence, for any k ∈ [m],
E[(g1,i)k(g1,j)k] =	X E[(W1)k,b(W1)k,b0 (Xi)b(Xj)b0]
b,b0∈[m]
E E[(Wι)k,b] ∙ (xi)b(xj)b
b∈[m]
((WI)k,b 〜N(0, mm).)
m2X (Xi)Mx )b
b∈[m]
-2
mxi Xj.
24
Under review as a conference paper at ICLR 2022
Then, we have
E[h1>,i h1,j] =	E[(h1,i)k (h1,j)k]
k∈[m]
= X E[φ((g1,i)k)φ((g1,j)k)]
k∈[m]
= X	E	[φ(u)φ(v)]
々UM〜N (0,焉ςw )
E
(u,v)〜N(0,焉∑1,i,j )
E
(u0,v0)〜N (0,2∑ι,i,j)
(K1)i,j.
[mφ(u)φ(v)]
[φ(u0)φ(v0)]
Next, we will show that h1>,ih1,j concentrates around its expectation. First, for any k ∈ [m],
∣(hι,i)k(hι,j)k| ≤ ∣(gι,i)k(gι,j)k| ≤ K(WI)k,*,Xii∣∙ K(WI)k,*,Xji|.
Since h(Wι)k,^,χii 〜N(0, 2kmk2), by the concentration of Gaussian distribution,
K(WI)k,*,Xii∣≤ √c Vk ∈ [m],i ∈ [n]
holds with probability at least 1-mne-cm/4. Conditioned on this event, we have |(h1,i)k (h1,j)k | ≤
c for all i, j ∈ [n] and k ∈ [m]. Then, by Hoeffding’s inequality, we have for any (i, j) ∈ [n] × [n],
Prljh>ihι,j- (KI)i,j| ≥ t] ≤ eχp --2m t(2c)2) = eχp(-a(t2/(mc2))).
Hence, by union bound, we get that
maχ
(i,j)∈[n]×[n]
|h1>,ih1,j - (K1)i,j|
≤t
with probability at least
1 — mnexp(-Ω(mc)) — n2 exp(-Ω(t2∕(mc2))).
If We choose C := log(mnL∕δ) and t := m-1/2 ∙ Polylog(nL∕δ), We have With probability at least
1 -苴	m
-L,
max	|h1>,ih1,j - (K1)i,j| ≤ Oe(m-1/2).
(i,j)∈[n]×[n]
Suppose for ` = 1, . . . h (h < L), We have
m .maχ . . |h>ih',j - (K')i,j| ≤ 0(馆-1/2).
(i,j)∈[n]×[n]
Consider ` = h + 1. By a similar computation, We have
2
E [(g',i)k (g',j )k] = —h'-
w`	m
Define a neW covariance matrix
1,ih'-1,j.
,i,j :
h>-1,i h'-1,i	h>-1 ,i h'-1 ,j
h1-1,j h'-1,i hl-1,j h'-1,j.
(i, j) ∈ [n] × [n].
We have
E [h>,'hj,'] =	E ʌ
W'	k∈m](u,v)〜N(0,七)
[φ(u)φ(v)]
=	E	[φ(u0)φ(v0)]
(u0,v0)〜N(0,2Σ',i,j)
∕τ5- ∖
:= (Kg)i,j.
25
Under review as a conference paper at ICLR 2022
Hence, we have with probability at least 1 - L,
....max . , Ih>ih',j -位e)ij ≤ O(mτ∕2).	(5)
(i,j)∈[n]×[n] I
It remains to upper bound the difference ∣∣K` — K'∣∞.
IlKe- Kdl	= ,. .max , ,	E -	[Φ(U)Φ(V)] - E	[φ(u)φ(v)]
11	ll∞	(ij)∈[n]×[n] (u,v)~N (0,2b',i,j)	(Ua)ZN (0,2£',i,j)
Recall that
Σ ∙ ∙ .一 (K'τ)i,i	(K'- l)ij	∀(i j)	∈	[n]	× [n]
"i,j := [(Kj)j,i	(K'-)/	T(ZJ)	∈	[n]	X [n],
and hence, by the induction hypothesis, we have
IN',i,j -，',i,j ∣∣∞ ≤ max 1 h1~ι,ih'-ι,j - (Ke-I)i,j1 = O(m ")∙
(i,j)∈[n]×[n]	,
Notice that Σ`i,j can be written as
kh'-1,ik2	cos(θ',i,j ) IIhe-1,i∣∣2 Il h'-1,j∣∣2
CoS(O',ij) Ilhe-1,i∣∣2k h'-ι,j ∣∣2	IIhe-Ijk2	,
Moreover, when φ is the ReLU function, we have
E -	[φ(U)O(V)] = 2∣∣he-ι,i∣∣2∣∣he-ι,j∣∣2 ∙ F(θe,i,j),
(u,v)~N (0,2Σ',i,j)
where
F⑻：=(UMJ(O,∑(θ∕(U)O(V)] with 训：=]co1(θ)	‘°y∙
We note that F(θ) has the following analytic form:
F(θ) = ɪ(sin(θ) + (π - θ) cos(θ)) ∈ [0,1/2].
2π
Similarly,
.、MEk	、[O(u)O(v)] = 2,(Ke-I)i,i(Ke-I)j,j ∙ F(Te,i,j),
(u,v)~N (0,2Σ',i,j)
(6)
where Teij ：= cos-1 (	(K`-Jij	). By the induction hypothesis, we have (Ke)ij ∈
,,J	∖v∕(Kg-ι)i,i(K'-ι)j,j	-J
h›ihe,j ±O(m-1/2) for all i,j ∈ [n]. By Lemma F.7, we also have Ilhe,i∣2 ∈ 1士e for all' ∈ [L] and
i ∈ [n] with probability 1 - O (nL) ∙ e~-MrngILL. They implies that COS(Te,i,j) ∈ cos(θ)±O(m-1/2).
Thus, by Taylor,s theorem, we have
|F(θe,i,j) - F(τe,i,j)| ≤ O(m-1/2).
Therefore, we have
E —	[O(u)O(v)]-. 、"Ek	M(U)O(v)]
(u,v)~N (0,2∑w)	(u,v)~N (0,2∑`,i,j)
=2 Ilhe-1,i∣∣2∣∣he-1,j∣∣2F(θe,i,j) - ((Ke-I)i,i(Ke-I)j,jF(Te,i,j)
≤ O(m-1/2).
That is,
Ice - Keh ≤ O(m-1/2).
(7)
26
Under review as a conference paper at ICLR 2022
Combining Eqs. (5) and (7) together, we get that
.max 一 lh>ih',j -(Ke)i,j| ≤ 0(馆-1/2)
(i,j)∈[n]×[n]	,
holds with probability at least 1 - L for ' = h +1.
By induction, We have proved that for the first L - 1 layers, the intermediate correlation h>ihe,j is
close to the intermediate Gram matrix (k`)i,j. Now, we consider the last layer. Recall GL is defined
by Eq. (4), which has the same form as the correlation matrix of a two-layer over-parameterized
neural network with input data {hL-1,i}i∈[n] . Define
(KL)i,j := h>-1,ihL-1,j ∙	“E ”、[φ0(w>hL-1,Jφ0(w>hL-1,j)].
W 〜N (0,2Im)
Then, by the analysis of the two-layer case (see for example Song & Yang (2019); Du et al. (2019b)),
we have
kGL - KLk ≤ ^l,
if m = Ω(λ-2n2 log(n∕δ)), where Xl := λmin(KL). It remains to bound ∣∣KL - Kl∣∣∞. EqUiva-
lently, for any (i, j) ∈ [n] × [n],
max
(i,j)∈[n]×[n]
E	[φ0(u)φ0(v)] - E	[φ0(u)φ0(v)] .
(u,v)〜N(0,2ΣL,i,j )	(U,v)~N(0,2£L,i,j )
The expectation has the following analytic form:
E	[φ0(zι)φ0(z2)] = 1 + sin 1(ρ) with Σ
(Z1,Z2)〜N(0,Σ)Vd 2川 4	2π
p2 ρpq
2
ρpq q2
By the analysis of the (L - 1)-layer, we know that ∖ρL,i,j - pL,i,j | ≤ O(m-1/2), where PL,i,j ：=
cos(τL,i,j) and ρbL,i,j := cos(θL,i,j). Also, notice that cos(τL,i,j) = F (τL-1,i,j) ∈ [0, 1/2] by
Eq. (6). Hence, the derivative of the expectation is bounded, and by Taylor’s theorem, we have
kKbL-KLk∞ ≤ Oe(m-1/2).
It implies that kKL 一 KLk ≤ 生,which further implies that
kGL - KLk ≤ ~4L.
Equivalently, we get that
3
λmin(GL) ≥ 4λL
with probability at least 1 - δ.
The lemma is then proved.	□
E.2 Bounds on the least eigenvalue during optimization
In this section, we generalize the Lemma C.5 in Brand et al. (2021) into multiple layer neural net-
work
Lemma E.3 (Bounds on the least eigenvalue, multiple layer neural network version of Lemma C.5
of Brand et al. (2021)). Suppose m = Ω(λ-2n2 log(n∕δ)), with probability least 1 — δ, for any set
of weights Wι, •一WL satisfying
kW - W'(0)k≤ R.
then the following holds
kGL(W) - GL(W(0))kF ≤ λL∕2.
27
Under review as a conference paper at ICLR 2022
Proof. Recall (GL)(W)i,j- = A P乙 φ'(《Wl)「,瓯-1〉)"WWL)r,hj,L-AhlLThj,l-i
For simplicity, let zi,r := (Wl)JMl-I and zi,r(0) := (WL(O))Jhi,l-i(0).
|(GL(W))i,j - (Gl(W(0)))i,j|
1 m	1 m
hi,L-1hj,L-1 m〉： φ (Zi,r )φ' (Zj,r ) - hi,L-1 (0) Jhj,L-1(0) m X φ' (Zi,r(O))φ0 (Zj,r (0))
r=1	r=1
1 m
≤ IhJL-1hj,L-1 - hi,L-1(0)Jhj,L-1(0)| ∙ — £ φ (Zir )φ0 (Zj,r )
m r = 1
1
m
+ 1 hi,L-1(0)Jhj,L-1(0) 1
m
X φ (Zi,r )φ (zj,r ) - φ (Zi,r(0))φ' (Zj,r (O))
r=1
≤ I h>L-1hj,L-1 - hi,L-1(O)Jhj,L-1(O)I +
λL + O(m-1/2)
m
m
X φ (Zi,r )φ (Zj,r ) - φ (Zi,r(0))φ (Zj,r (O))
r=1
where the last step follows from φ0(x) ∈ {0,1} and Lemma E.2.
To upper bound the above two terms, we first need to bound the move of h%L-ι. For any ` ∈ [L -1],
we have
M,' - hi,g(0)∣∣2 = ∣∣φ(%hij) - φ(%(0)hi,'-ι(0))∣∣2
≤M(W⅛'-1)- Φ(W⅛'-1(0))∣∣2
+ M(Wehi,'-1(0)) - φ(%(0)hi,'-ι(0))∣∣2
≤ (I四 - We(0)∣∣ + ∣∣We(0)∣∣) ∙ Ilhi- — h.ι(0)∣∣2
+ kW' - WK0)II ∙ llhi,'-ι(0)l∣2
≤ (R + cW )∣∣hi,'-1 - hi,'-1(0) ∣∣2 + R(1 + e),
where CW ：= ∣∣%(0)∣∣ ≤ 3 by the well-known deviations bounds concerning the singular values
of Gaussian random matrices (Rudelson & Vershynin (2010)). Also, when ` = 0, we have ∣∣hi,0 -
hi,0(0)∣∣2 = O. Hence, we get that for all' ∈ [L - 1], i ∈ [n],
∣∣hi,' - hi,'(0)∣∣2 ≤ √1 + eR(2cw)',
since R《1 by our choice of R.
Hence, it implies that
I h>L-1hj,L-1 — hi,L-1(0)>hj,L-1(0) 1
≤ I (hi,L-1 — hi,L-1(0))>hj,L-1∣ + I hi,L-1(0)>(hj,L-1 — hj,L-1(0))
≤ 2(1 + e)R(2cw)L-1.
Similarly,
λL + O(m--------) X φ (Zi,r )φ' (Zj,r ) - "(Zi,r (O)) ”(Zj,r (0))
m	r=1
_ λL + <5(m-1/2) i .
=--------m-------- i 1(WL)>hi,L-1>0,(WL)>hj,L-i>0 - 1(Wl(0))>R,L-1 ⑼ >0,(WL ⑼)>hj,L-1(0)>0] .
By our assumption, we have II(WL)r — (WL(0))r∣∣2 ≤ R. We also know that ∣∣hi,L-1 -
hi,l-1(0)∣2 ≤ √1 + eR(2cW)L-1. Then, we can follow the proof in Du et al. (2019b); Song
& Yang (2019) and define the event Ai,r ：= ∃w, h : ∣w - (Wl(0))"∣2 ≤ R,∣∣h - hi,L-1(0)∣∣2 ≤
O(R) SUChthat ljh>0 = 1(Wl(0))>%,l-1(0)>0. WehaveAy happensifandonlyif ∣((Wl(0)% +
∆w)1hi,L-1(0)∣ < O(R(R + m-1/2)) for some fixed vector ∆w of length at most R. It implies
that
Pr[Ai,r]=	Pr [∣z∣ <R + o(R)] ≤ O(R).
Z〜N (0,1)
28
Under review as a conference paper at ICLR 2022
Hence, using the same proof in the previous work, we get that
入L + °(m------) X φ0(zi,r )φ0(zj,r ) - φ0(zi,r (0))φ0(zj,r (0)) ≤ (1工 + m-1/2)R.
m	r=1
Putting them together, we have
1(GL(W))i,j - (GL(W(0)))i,j I ≤ (2(1 + e)(2cw)L-1 + λL)R.
And by our choice of R, we get that
∣∣Gl(W) - GL(W(0))kF ≤ (2(1 + e)(2cw)L-1 + λ-)nR ≤ λ-∕2.
The lemma is then proved.	口
F Convergence analysis of Algorithm 2
In this section, we analyze the convergence behavior of Algorithm 2.
F.1 Preliminary
We recall the initialization of our neural network.
Definition F.1 (Initialization). Let m = m` for all ` ∈ [L]. Let m0 = d. We assume weights are
initialized as
•	Each entry of weight vector a ∈ RmL is i.i.d. SamPledfrom {— √m , + √^-} uniformly
at random.
•	Each entry of weight matrices W' e Rm'×m'-1 SamPledfrom N (0, 2∕mg).
We also restate the architecture of our neural network here.
Definition F.2 (Architecture). Our neural network is a standard L-layer feed-forward neural
network, with the activation functions defined as a scaled version of shifted ReLU activation:
φ(x) = √∕cb1[x > ,2∕mb]x, where Cb := (2(1 — Φ(b) + bφ(b)))-1/2. Here b is a threshold
value we will Pick later. At last layer, we use a scaled version of a vector with its entry being
Rademacher random variables. We define the neural network function f : Rm0 → R as
f(W,xi) =a>φ(WLφ(WL-1φ(...φ(W1xi)))).
We measure the loss of the neural network via squared-loss function:
1n
L(W ) = 2∑(f (Xi) - yi )2.
We use ft : Rm0 ×n → Rn denote the Prediction of our network:
ft(X) = [f(W(t),x1),...,f(W(t),xn)]>.
We state two assumptions here.
Assumption F.3 (Small Spectral Norm). Let t ∈ {0, . . . , T } and let R ≤ 1 be a Parameter. We
assume
max k-- %(0)k ≤ R.
Later, we will invoke this assumPtion by sPecifying the choice of R.
Assumption F.4 (Sparsity). Let t ∈ {0, . . . , T} and let s ≥ 1 be an integer Parameter. We assume
k∆Di,'ko ≤ s, ∀' ∈ [L],i ∈ [n].
Later, we will invoke this assumPtion by sPecifying the choice of s.
Finally, throughout this entire section, we will assume m` = m for any ` ∈ [L].
29
Under review as a conference paper at ICLR 2022
F.2 Technical Lemmas
We first show that during initialization, by using our threshold ReLU activation, the vector hi,` is
sparse, hence the diagonal matrix D? is sparse as well.
Lemma F.5 (Sparse initialization). Let σb(x) = max{x - b, 0} be the threshold ReLU activation
with threshold b > 0. After initialization, with probability 1 一 nL ∙ e-Q(me b m/4), it holds for all
i ∈ [n] and ' ∈ [L], khi,`ko ≤ O(m ∙ e-b2m/4).
Proof. We fix i ∈ [n] and ` ∈ [L], since we will union bound over all i and ` at last. Let ui ∈ Rm
be a fixed vector and W',r to denote the r-th row of W^ then by the concentration of Gaussian, we
have
Pr[σb(hWe,r,u> > 0] =	^Pr 2 [z > b] ≤ exp(-b2m∕4).
Let S be the following index set S := {r ∈ [m]:<呜),Uii > b}, the above reasoning means that
for the indicator random variable 1[r ∈ S], we have
E[1[r ∈ S]] ≤ exp(-b2m∕4).
Use Bernstein’s inequality (Lemma A.3) we have that for all t > 0,
Pr[|S| > k +t] ≤ eχp(-：+；/?》
where k := m ∙ exp(-b2m∕4). By picking t = k, we have
3k
Pr[∣S| > 2k] ≤ exp(—).
8
Note that |S| is essentially the quantity khi,`k0, hence we can union bound over all ` and i and with
probability at least
1 — nL ∙ exp(-Ω(m ∙ exp(b2m/4))),
we have khi,`ko ≤ 2m ∙ exp(-b2m/4).	□
Remark F.6. The above lemma shows that by using the shifted ReLU activation, we make sure that
all hi,' are sparse after initialization. Specifically, we use k := m ∙ exp(-b2m∕4) as a sparsity
parameter. Later, we might re-scale b so that the probability becomes exp(-b2/2). We stress that
such re-scaling does not affect the sparsity of our initial vectors. Ifwe re-scale b and choose it as
√0.4log m, then k = m0.8 and hence with high probability, khi,' ∣∣o ≤ O(m0.8).
As a direct consequence, we note that all initial Di,' are k-sparse as well.
We state a lemma that handles the `2 norm of hi,` when one uses truncated Gaussian distribution
instead. Due to the length and the delicacy of the proof, we defer it to Section G.
Lemma F.7 (Restatement of Lemma G.6). Let b > 0 be a fixed scalar. Let the activation function
φ(x) := √cb1[x > p2∕mb]x, where Cb := (2(1 — Φ(b) + bφ(b)))-1/2. Let e ∈ (0,1), then over
the randomness of W (0), with probability at least 1 — O(nL) ∙ exp(-Ω(m exp(-b2∕2)e2∕L2)), we
have
∣hi,' ∣2 ∈ [1 - e, 1 + e], ∀i ∈ [n], ` ∈ [L].
We remark that the parameter e-b2/2m
captures the sparsity during the initialization.
The second lemma handles the consecutive product that appears naturally in the gradient computa-
tion.
Lemma F.8 (Variant of Lemma 7.3 in Allen-Zhu et al. (2019a)). Suppose m ≥ Ω(nLlog(nL)).
With probability at least 1 — e-Q(k/L2) over the randomness ofinitializations Wι(0),..., Wl(0) ∈
Rm×m, for all i ∈ [n] and 1 ≤ a ≤ b ≤ L,
30
Under review as a conference paper at ICLR 2022
(a)	kWbDi,b-1Wb-1 ...Di,aWak ≤ O(√L)∙
(b)	IlWbDi,b-1Wb-1...Di,aWaV∣∣2 ≤ 2∣∣v∣∣2,foraUvectorswith ∣∣vko ≤ O(L/m).
(c)	I∣u> WbDi,b-iWb-ι... Di,aWa∣2 ≤ O(1)∣∣u∣∣2 ,foraUvectors U with ∣∣u∣o ≤ O( L 援 m).
(d)	For any intergers 1	≤ S ≤ O( L 蔡_), with probability at least 1 一
e-Q(s logm) over the randomness of initializations Wι(0),..., Wl(0)	∈	Rm×m,
∣u>WbDi,b-iWb-ι... Di,aWaν∖ ≤ ∣∣u∣2IlvIl2 ∙ O(L总m) for all vectors Ujv with
IuI0, IvI0 ≤ s.
The proof is similarly to the original proof of the corresponding lemma in Allen-Zhu et al. (2019a),
however we replace the bound on hi,` with our Lemma F.7. We highlight this does not change the
bound, merely in expense of a worse probability.
The next lemma concerns the bound on the product being used in backpropagation.
Lemma F.9 (Variant of Lemma 7.4 in Allen-ZhU et al. (2019a)). Suppose m ≥ Ω(nLlog(nL)).
Then with probability at least 1 — e-。(IOg2 m) ,for all i ∈ [n], ' ∈ [L],
∣a> Di,L Wl ...Di,' W'∣2 ≤ °(-‰ √L)
Proof. By Lemma F.8 part (a), We know that ∣∣ Wl ... Di,'W'∣ ≤ O(√L), consequently,
∣Di,LWL ... Di,'W'∣ ≤ O(√L) since ∣∣Di,L∣∣ ≤ 1. This means for any vector U ∈ Rm, with
probability at least 1 — e-Q(k/L2), We have
IDiLWL ... Di,'%u∣∣2 ≤ O( √L)∣∣u∣∣2.
Conditioning on this event and using the randomness of Rademacher vector a, we have
Pr[∖a>Di,LWL . . . Di,'W'u∖ ≤ t ∙ √Lku∣2] ≥ 1 一 2 exp(--2-),
pickt = l√gm ,weknowthat
Pr[∖a>Di,LWL . . . Di,'W'u∖ ≤ —∙√- ∙ √Lkuk2] ≥ 1 一 2 exp(-ʒ--).
m2
This means with probability at least 1 — e-Q(log2 m), We have
∣a> Di,LWL ... Di,'W'∣2 ≤ O(l0gm √L).
□
The next several lemmas bound norms after small perturbation.
Lemma F.10 (Lemma 8.2 in Allen-Zhu et al. (2019a)). Suppose Assumption F.3 is satisfied with
R ≤ O( L9/2 IOg3 m). With probability at least 1 — e-Q(mR2/3L),
(a)	∆gi,' can be written as ∆gi,' = ∆gi,',ι + ∆gi,',2 where ∣∣∆gi,',ι∣∣2 ≤ O(RL3/2) and
k∆gi,',2 ∣∞ ≤ O(RLT匹).
(b)	∣∣∆Di,'∣0 ≤ O(mR23L) and ∣∣(∆Di,')gi,'∣2 ≤ O(RL3/2).
(c)	∣∣∆gi,'∣2, ∣∆hi,'∣2 ≤ O(RL5∕2√logm).
Remark F.11. Lemma F.10 establishes the connection between parameter R and s of Assump-
tion F.3 and F.4. As long as R is small, then we have s = O(mR2/3L). Such a relation enables us
to pick R to our advantage and ensure the sparsity of ∆Di,' is sublinear in m, and hence the update
time per iteration is subquadratic in m.
31
Under review as a conference paper at ICLR 2022
Lemma F.12 (Lemma 8.6 in Allen-Zhu et al. (2019a)). Suppose Assumption F.4 is satisfied with
1 ≤ S ≤ O( l3 lm m) and m ≥ Ω(nL log(nL)), with probability at least 1 一 e-。(S logm) over the
randomness of W (0), for every i ∈ [n], 1 ≤ a ≤ b ≤ L, we have
(a)	k Wb(0)(Di,b-ι(0) + ∆Di,b-ι)... (D. (0) + ∆Di,a)Wa(0)k ≤ O(√L).
(b)	k(Wb(0) + ∆Wb)(Di,b-i(0) + ∆Di,b-i) ... (Di,a(0) + ∆Di,a)(Wa(0) + ∆Wa )∣∣ ≤
O(√L) ifAssumption F3 is satisfied with R ≤ O( 715).
Corollary F.13. Suppose Assumption F.3 is satisfied with R ≤ O(工4.5 鼠 m) and m ≥
Ω(nL log(nL)), with probability at least 1 — e-Q(log2m) over the randomness of W(0) and a,
for any i ∈ [n] and ` ∈ [L], we have
ka>(Di,L(0) + ∆Di,L(0))(WL (0) + ΔWl) ... (Wm(0) + ∆Wm)(Di,' (0) + ∆Di,e)∣∣2
≤ O(VLlm).
Proof. We first note that by Lemma F.10 and the choice of R, we know that Assumption F.4 is
satisfied with 1 ≤ S ≤ O( -m-).
L2 log2 m
The proceeding proof is identical to Lemma F.9, use P to denote the product (Di,L (0) +
ΔD*l(0))(Wl(0) + ΔWl) ... (W'+ι(0) + ∆W'+ι)(Di,'(0) + ∆Di,'). Per Lemma F.12, We
know that with the conditions stated, we have
kPk ≤ O(√L),
conditioning on this even, for any vector U ∈ Rm, with probability at least 1 — e-Q(s log m), We know
that
kPuk2 ≤ O(√L)kuk2.
Use the randomness of vector a, we have
Pr[|a>Pu| ≤ t ∙ √Lkuk2] ≥ 1 一 2exp(--2-).
Pick t = l√gm, we are done.	□
m
Lemma F.14 (Variant of Lemma 8.7 in Allen-Zhu et al. (2019a)). Suppose Assumption F.3 is sat-
isfied with R ≤ O(74.5 ∣0g3 m() and m ≥ Ω(nL log(nL)), with probability at least 1 — e-Q(log2 m)
over the randomness ofW(0) and a, for all i ∈ [n], ` ∈ [L]. Define
u ：= a>(Di,L(0) + ∆Di,L)(WL(0) + ΔWl(0)) ... (Wm(0) + ∆W'+ι)(Dy(0) + ∆DQ
uo ：= a>Di,L(0)WL(0)... W'+ι(0)Di,'(0).
It satisfies that
Ilu — u0k2 ≤ O(PL/m).
Proof. Note that we can invoke Corollary F.13 to give an upper bound on kuk2 with probability
1 一 g-Ω(log2 m),
Iluk2 ≤ O(PLlm).
Similarly, we can use Lemma F.9 to give an upper bound on kuo ∣∣2 with probability 1 — e-Q(log2m),
I∣u0k2 ≤ O(PL/m).
Finally, by triangle inequality, we get
ku — u0∣∣2 ≤ O(PLlm),
with the desired probability.
□
32
Under review as a conference paper at ICLR 2022
F.3 B ounds on initialization
In the following lemma, we generalize the Lemma C.2 in Brand et al. (2021) into multiple layer
neural networks.
Lemma F.15 (Bounds on initialization, multiple layer version of Lemma C.2 in Brand et al. (2021)).
Suppose m = Ω(nL log(nL)) ,then with probability 1 一 O(nL) ∙ e-Q(k/L2), we have thefollowing
•	f(W,xi) = O(1), ∀i ∈ [n].
•	With probability at least 1 — e-Q(log2 m), we have ∣∣ j`,o,ik =O(PLlm), ∀' ∈ [L], Vi ∈
[n].
Remark F.16. The bound of m in Brand et al. (2021) has a linear dependence on n because they
need to bound ∣W (0)∣2. Ifwe do not need to bound ∣W(0)∣2, then m does not depend linearly on
n.
Proof. We will prove the two parts of the statement separately.
Part 1:	By definition, for any i ∈ [n], we have
f(W,Xi) = a>φ(WL(φ(…φ(WιXi)))).
We shall make UseLemmaF.7 here: with probability at least 1 — O(nL) ∙ exp(一Ω(k∕L2)), We have
∣hi,L∣2 ∈ [0.9, 1.1]. Recall that a ∈ Rm has each of its entry being a Rademacher random variable
scaled by 1∕m, hence it’s 1∕m-sUbgaUssian. Using the concentration of sUbgaUssian (Lemma A.5),
we have that
t2 m
Pr[∣f (W,xi)l≥ t] ≤ 2exp(-),
2∣hi,L∣2
finally by noticing that t = Θ(1) and ∣hi,L∣2 ∈ [0.9, 1.1], we conclUde that
Pr[∣f(W,xi)∣ ≥ O(1)] ≤ 2exp(—Ω(m)),
Union boUnding over all i, we conclUde that
Pr[f(W, Xi) ≥ O(1)] ≤ O(n) ∙ exp(—Ω(m)), Vi ∈ [n].
Part 2:	We will combine Lemma F.7 and F.9, with probability at least 1 — O(nL) ∙ e-Q(k/L2), We
have k hi,' k 2 ∈ [0.9,1.1] and with probability at least 1 —e-Q(iog2 m), we have ∣∣a>Di,L ... W'∣2 ≤
O(pL∕m). Combine them together, we know that with probability at least 1 — e-。(IOg2m),
∣j',o,ik ≤ o( √L∕m).	口
Remark F.17. By utilizing the structure of vector a, we can show that with a weaker probability
(1 — e-Q(log2 m)), f (W, Xi) has even smaller magnitude (O(√m)). For our purpose, f(W,Xi)=
O(1) suffices.
F.4 B ounds on small perturbation
In the following, we generalize the Lemma C.4 in Brand et al. (2021) into mUltiple layer neUral
network. For the simplicity of notation, we set all m` to be m.
Lemma F.18 (mUltiple layer version of Lemma C.4 in Brand et al. (2021)). Suppose R ≤
O( l4.5 鼠 m) and m = Ω(nL log(nL)). With probability at least 1 — e-Q(log2 m) over the random
initialization of W (0) = {Wi(0), W2(0),…Wl(0)} , thefollowing holds for any set of weights
Wι, ∙∙∙ , Wl satisfying
k% — %(0)k≤ R, V' ∈ [L]
•	k JW',Xi — JW'(0),Xi ∣2 = O( V L/m).
33
Under review as a conference paper at ICLR 2022
•	k JW' - JW'(0)∣∣F = Oe(n1/2 √L∕m).
•	k JwJf = Oe(n1/2 pL/m).
Proof. Part 1. To simplify the notation, we ignore the subscripts i below.
Consider the following computation:
IlJW',xi - JW'(0),xi k2 = lluhe-i - uoh`-1(0)kF
=∣u(h'-ι(0) + ∆h'-ι)> — U0 hTι(0)∣∣F
=k(u — uo)h>-i(0) + u(∆h>-1)kF
≤k(u - uo)hlι(0)∣F + ku(∆h>-ι)kF
≤ku - u0k2 ∙kh'-i(0)∣2 + ku∣2 ∙k∆h'-1∣2.
where
u := (D'(0) + ∆D') ( YY (Wk(0)+∆Wk)>(Dk(0)+∆Dk)) a
∖k='+1	)
U0 ：= D'(0) ( YY Wk(0)>Dk(0)) a
∖k='+1	)
By Lemma F.14, We know that with probability at least 1 一 e-。(IOg2 m), We have
∣∣u - U0k2 ≤ O(PL/m).
By Lemma F.7, we know that with probability at least 1 一 O(nL) ∙ e-k/L2, we have
kh'-ι(0)∣2 ∈ [0.9,1.1].
By Corollary F.13, we know that with probability at least 1 一 e-Q(log2 m), We have
IluIl2 ≤ Oe(pL∕m).
By Lemma F.10, we know that with probability at least 1 一 e-Q(mR2/3L), We have
∣∆h'-i∣2 ≤ O(RL2∙5).
Note that due to the choice of R, we know that
∣∆h'-i∣2 ≤ 1.
Taking a union bound over all events, with probability at least 1 一 e-Q(log2 m), We achieve the
following bound:
IlJW',Xi - JW'(0),Xil∣2 ≤ O( VZL/m).
Part 2.	Note that the squared Frobenious norm is just the sum of all squared `2 norm of rows, hence
k JW' - JW'(0)∣F ≤ O(n1/2pL/m).
Part 3.	We will prove by triangle inequality:
IJW' IF ≤ IJW'(0) IF + IJW' 一 JW'(0) IF
≤ O(n1/2 pL/m) + Oe(n1/2 pL/m)
=O(n1/2 P L/m).
□
34
Under review as a conference paper at ICLR 2022
F.5 Putting it all together
In this section, we will prove the following core theorem that analyzes the convergence behavior of
Algorithm 2:
Theorem F.19 (Formal version of Theorem 1.1). Suppose the width of the neural network satis-
fies m = Ω(λ-2n2L2), then with probability at least 1 一 e-。(IOg2 m) over the randomness of the
initialization of the neural network and the randomness of the algorithm, Algorithm 2 satisfies
kft+1 一 yk2 ≤ 2 IIft 一 yk2.
Before moving on, we introduce several definitions and prove some useful facts related to them.
Definition F.20 (function J). We define
L
J'(Zι,...,ZL)i := Di,'(Z') Y Z>Di,k (Zk )a(hi(Z1,...,Z'-1))> ∈ Rm'×m'-1
k='+1
where
Di,'(Z') ：= diag(Φ0(Z'hi(Zι,…，Z'-ι))),	∈ Rm'×m'
hi(Zι,..., Z'-ι) ：= φ(Z'-1(φ(Z'-2 …(φ(ZιXi)))))	∈ Rm'-1
Fact F.21. Let j` denote the function be defined as Definition F.20. For any t ∈ {0,...,T}, we
have
ft+1 — ft = X (/1 J'((1 一 S)W(t) + SW(t + 1))ds) ∙ vec(We(t + 1) — W'(t)),
Proof. For i ∈ [n], consider the i-th coordinate.
(ft+1 一 ft)i = Z1 f((1 一 S)W (t) +SW(t+ 1), xi)0dS
0
X X (∂W((I-S)W⑴+SW(t+I), Xi))	∙ Vec(W'(t+I)- W'(t))ds
XX (Jo 1 J'((1 一 s)W(t) + sW(t + 1))ids)	∙ Vec(W'(t + 1) — W'(t)),
Thus, we complete the proof.
FactF.22. For any t ∈ {0,..., T}, we have J'(W1(t),..., Wl(t)) = J',t.
□
Proof. In order to simplify the notation, we drop the term t below.
We note that for i ∈ [n], the i-th row of matrix j`,t is defined as
L
Di,'( Y W>Di,k)ah>'-1,
k='+1
where Di,' = diag(φ0(W'hi,'-1)) and hi,`-i = Φ(W'-1(Φ(W'-2... (φ(W1Xi))))), this is essen-
tially the same as hi(W1,..., %-1) and Di,'(W'). This completes the proof.	□
We state the range we require for parameter R:
Definition F.23. We choose R so that
n
,—∙ ɪ ≤ R ≤
mL λL
1
L4.5 log3 m
35
Under review as a conference paper at ICLR 2022
Remark F.24. Recall that the sparsity parameter s is directly related to R: s = O(mR2/3L), hence
to ensure the sparsity is small, we shall pick R as small as possible. Specifically, if we pick R to be
/ n、, then S
mLλL
O(λ-'n,mL), as long as m》λ-2n2L then S ≈ O(√m).
Next, we pick the value of m:
Definition F.25. We choose m to be
m ≥ Q(X-10/3n10/3L5/3).
Remark F.26. The choice of m here makes sure that, as long as we pick R matching its lower
bound, then the sparsity S = O(m0.8), this matches the other sparsity parameter k, which is also in
the order of O(m0.8).
We use induction to prove the following two claims recursively.
Definition F.27 (Induction hypothesis 1). Let t ∈ [T] be a fixed integer. We have
kW'(t) - W'(0)k ≤ R
holds for any ` ∈ [L].
Definition F.28 (Induction Hypothesis 2). Let t ∈ [T] be a fixed integer. We have
Ilft - yk2 ≤ 2 IIft-I - yk2.
Suppose the above two claims hold up to t, we prove they continue to hold for time t + 1. The
second claim is more delicate, we are going to prove it first and we define
J',t,t+1
Z1
0
j'((I-S)Wt + sWt+ι)ds,
where j` is defined as Definition F.20.
LemmaE29. Let g? := (4,右几)-1 ∙ L (ft 一 y). We have
L
kft+1 - yk2 ≤ kft 一 y — X J',tJ'>tg',tk2
'=1
L
+ X II(J',t- 4,t,t+I)J>tg?k2
'=1
L
+ X k(J',t - 4,t,t+I)J>t(g',t - g?)k2.	⑻
'=1
Proof. Consider the following computation:
Ift+1 - y I2
= Ift - y + (ft+1 - ft)I2
L
=kft 一 y +	J',t,t+1 ∙ vec(W',t+ι — W',t)k2
'=1
L
=kft - y — E J',t,t+1 ∙ J>tg',tk2
'=1
LLL
=kft - y - E J',tJ>tg',t+E J',tJ>tg',t - E J^,t,t+1 J',tg',tk2
LL
≤ kft 一 y - E J',tJ'>tg',tk2 + E II(J',t — JΛt,t+I)J>tg',tk2
LL	L
≤ kft 一 y — X J',tJ'>tg',tk2 + X II(J',t 一 JΛt,t+I)J>tg?k2 + X II(J',t 一 JΛt,t+I)J>t(g',t - g?)k2,
The second step follows from the definition of J',t,t+ι and simple calculus.	口
36
Under review as a conference paper at ICLR 2022
Claim F.30 (1StterminEq.(8)). We have
L
k(ft - y) - X J',tJ'>tg',tk2
2=1
≤ 6lift - y∣∣2∙
Proof. We have
LL
ll(ft - y) - X JΛtj>tg',t∣∣2 = Il χ(L(ft - y) - 4,tj>t"t)∣∣2
≤ X Il L ∙ (ft - y) - J',tJ^e,t9',tk2
'=1
≤ Nkft- y∣∣2,	⑼
6
since g`,t is an e0 (e0 ≤ ɪ) approximate solution to regression problem
吗n ∣%,tJ›tg - L(ft - y)k2∙
□
Claim F.31 (2nd term in Eq. (8)). We have
L	1
E II(J',t - 4,t,t+1)几g?k2 ≤ 6kft - yl∣2∙
2=1
Proof. For the second term in Eq. (8), for any ' ∈ [L], we have
Il(J',t - 4,t,t+1)J>tg3∣∣2 ≤ Ilj',t - 4,t,t+1∣∣∙ IlJ>tgM∣2
=Il j',t - 4,t,t+ιl∣∙ IlJ>t(J',tj>t)-1 ∙ T(ft - y)ll2
L
≤ 厂 Ilj',t- JΛt,t+ιl∣∙ l£t(J'/几)-1卜 Ilft- yll2∙	(Io)
L
We bound these term separately. First,
Il j`,t - Jg,t,t+ι∣∣
J'(Wt)-广 J'((1 - s)Wt + sWt+ι)ds
0
≤ 广 l∣J'(Wt) - J'((1 - S)Wt + SWt+1 )1Ids
0
≤ [	l∣J'(Wt)-	j`(WO)II	+	l∣J'(WO)-	j`((1 -S)Wt+	swt+I)Il ds
0
≤ J'(Wt) - J'(W0)∣∣ + ∕1 J'(W0) - J'((1 - s)Wt + sWt+ι)∣∣ ds
0
≤ O(n1/2PL/m),
(11)
where by Fact F.22, we know ∣∣Jg(Wt)-J'(W0)∣∣ = IlJW“t)— J^Wg(0)∣∣ ≤ O(n1/2pL/m),the 山St
inequality is by Lemma F.18. For the second term, we have
11(1 - S) ∙ vec(We(t)) + S ∙ vec(%(t + 1)) - vec(Wg(0))∣∣2
≤ (1 - s) ∙ IIVeC(W'(t)) - vec(W'(0))∣∣2 + S ∙ ∣∣vec(W^(t + 1)) - vec(Wg(0))∣∣2
=(1 - s) ∙ ∣∣W'(t) - We(0)∣∣F + S ∙ ∣∣We(t + 1)-呜(0)|山
≤ O(R).
37
Under review as a conference paper at ICLR 2022
This means the perturbation of (1 - s)w`(t) + sW'(t + 1) with respect to W'(0) is small, for any
' ∈ [L], hence Je(Wo) - J'((1 - S)Wt + ^Wt+ι)k = O(Q/4l/m).
Furthermore, we have
k Je>t(Je,tJe>t) 1k =	τγrτ ≤ P2∕λL,
σmin( J',t)
(12)
where the second inequality follows from σmin(Je,t)
Lemma E.3).
y λmin( Je,t Je>t)
≥ 八L/2 (see
Combining Eq. (10), (11) and (12), we have
L
X k(J',t - J',t,t+I)J>tg*k2 ≤ O(PPnLlm ∙ λL1/2 ∙ kft - yk2
'=1
≤ 6kft - yk2,	(13)
where the last step follows from choice of m (Definition F.25).	口
Claim F.32 (3rd term in Eq. (8)). We have
L1
E k(Je,t - Je,t,t+I)J,t(ge,t - g;)k2 ≤ 6kft - yk2
'=1
Proof. We can show
II(Je,t -	Je,t,t+I)J>t(ge,t	-	g;)k2 ≤ ∣∣Je,t	-	Je,t,t+ιk∙ ∣∣J>t∣∣∙	kg`,t	-	gN∣2∙	(14)
Moreover, one has
~Lk kg',t - gM∣2 ≤ λmin(J',t Jrt) ∙ kg',t - gM∣2
≤ IIJ',tJ'>tg',t- JΛtJ>tgM∣2
=IIJ',tJ'>tg',t - L(ft - y)k2
≤ 乎∙kft-yk2.
(15)
The first step comes from λmin(J',tJ>t) = λmin(G',t) ≥ Xl∕2 (see Lemma E.3) and the last step
comes from gt is an e° approximate solution. The fourth step follows from Eq. (15) and the fact that
∣∣(J',tJ'rt)-1k ≤ 2∕Al. The last step follows fromgt is an e0 (e0 ≤ 入L/n) approximate solution
to the regression.
Consequently, we have
Il(Je,t - JΛt,t+ι)J>t(g',t - gj)∣∣2 ≤ ∣∣Je,t - Je,t,t+11∣∙ IJtll，llg`,t - gj∣∣2
〜τ，c ,-----------τ，c ,---- 2
≤ O(n1 √L∕m) ∙ O(n1 √L∕m) ∙	∙ kft - y∣∣2
L nλL
=O(^ɪ) A- ∙ lift - y∣∣2∙	(16)
m λL
El	F ,	i' 11	i'	TΓ∏	/ t t∖	1	/ t L、	1	. 1 i' .	,1	,	I I T 1 1	,	K /	/ T―/	∖	Z
The second step follows	from	Eq.	(11)	and	(15)	and the fact that	kJe,tk	≤	O(√nL∕m)	(see
Lemma F.18).
Finally, we have
X k(Je,t	-	JΛt,t+ι)J>∕g',t	-	gi)l∣2 ≤	O('	)√=	∙	kft	- yl∣2
e=ι	m √λL
≤ 6 kft - yk2 .
38
Under review as a conference paper at ICLR 2022
The last step follows from choice of m (Definition F.25).
□
Lemma F.33 (Putting it all together). We have
kft+ι - yk2 ≤ 2lift - yk2.	(17)
Proof. Combining Eq. (8), (9), (13), and (??), we have proved the second claim, i.e.,
kft+1 - yk2 ≤ 2 lift - yk2.
- □
F.6 Weights are not moving too far
Lemma F.34. Let R be chosen as in Definition F.23, then the following holds:
kw`(t +1)- W'(0)k ≤ R, ∀' ∈ [L].
Proof. It remains to show that Wt does not move far away from W0 . First, we have
kg',t∣∣2 ≤ kg?k2 + kg',t - g?k2
=y k(J',tJ'>t)-1(ft - y)k2 + llg`,t - g3l∣2
L
≤ 7k(J',t J'>t)-1k ∙ k(ft - y)k2 + kg',t - g? I∣2
L
≤ -j-γ~ ∙ kft - yk2 + r ∕~^~ ∙ kft - yk2
LλL	L nλL
.八一∙ kft - yl∣2	(18)
LλL
where the third step follows from Eq. (15) and the last step follows from the obvious fact that
1/√nλL ≤ 1/"
Then
kW'(k +1)- W'(k)k = kJ>kg',k k
≤ k J',k Il ∙ kg',kk2
≤ Oe(PrnLlm) ∙ y— ∙ kfk - y∣∣2
LλL
1 1 1	1
≤ O( √nlLm) ∙ 2^ ∙ kfo - yg
≤ O( ɪ) ∙ ɪ
≤ Oe √Lm) 2kλL.
The third step uses the fact that k4,kk ≤ k4,k IIf ≤ O(pLm) by Lemma F.18, and the last step
uses the fact that both kf0k2 and kyk2 are in the order of Oe1).
Consequently, we have
t
kW'(t + 1) - W'(0)k ≤ X kW'(k + 1) - W'(k)k
k=0
t n 11
≤ X Oe√Lm) ∙瓦 2k
k=0
≤ Oe√=) ∙ λ-.
Lm	λL
By the choice of R (Definition F.23), we know this is upper bounded by R. This concludes our
proof.
□
39
Under review as a conference paper at ICLR 2022
G Proof of Lemma F.7
In this section, we prove a technical lemma (Lemma F.7) involving truncated gaussian distribution,
which correlates to the shifted ReLU activation we use.
Definition G.1 (Truncated Gaussian distribution). Suppose X 〜N(0, σ2). Let b ∈ R. Then, we say
a random variable Y follows from a truncated Gaussian distribution Nb(0, σ2) if Y = X|X ≥ b.
The probability density function forNb(0, σ2) is as follows:
f (y) = .	1./ / ∙与e-y'2σ" y ∈ [b, ∞),
σ(1 - Φ(b∕σ)) √2∏
where Φ(∙) is the standard Gaussian distribution's CDF
Fact G.2 (Properties of truncated Gaussian distribution). For b ∈ R, suppose X 〜 Nb(0, σ2). Let
β := b∕σ. Then, we have
•	E[X] = Iσφ(ββ), Where φ(X) := √12πe-x2∕2.
•	Var[X] =σ2(1+βφ(β)∕(1-Φ(β))-(φ(β)∕(1-Φ(β)))2).
•	X∕σ ~Nb∕σ(0,1).
•	When σ = 1, X is C(b + 1)-subgaussian, where C > 0 is an absolute constant.
Lemma G.3 (Concentration inequality for b-truncated chi-square distribution). For b ∈ R, n > 0,
let X 〜χ2 n; that is, X = En=I 匕2 where Y1,...,Yn 〜Nb(0,1) are independent b-truncated
Gaussian random variables. Then, there exist two constants C1, C2 such that for any t > 0,
Pr X — n(1 +	) ≥ nt ≤ exp (—Cint2∕b4) + exp (—C2nt∕b2).
1 - Φ(b)
In particular, we have
Pr [|X — n(1 + b(b +1))| ≥ t] ≤ exp (—C1t2∕(nb4)) + exp (—C2t∕b2).
Proof. Since We know that 匕 〜Nb(0,1) is C(b + 1)-SUbgaUSsian, it implies that 匕2 is a sub-
exponential random variable with parameters (4√2C2(b+1)2,4C2(b+1)2). Hence, by the standard
concentration of sub-exponential random variables, we have
2 exp (— 2.32我；+1)4 ) if nt ≤ 8C2(b + 1)2
2 exp (— 2.40墨+1)2)otherwise
≤ 2exp (—C1nt2∕b4) + 2exp (—C2nt∕b2).
n
Pr X Yi2 — nE[Yi2] ≥ nt ≤
i=1
□
Fact G.4. Let h ∈ Rp be fixed vectors and h 6= 0, let b > 0 be a fixed scalar, W ∈ Rm×p be
random matrix with i.i.d. entries Wij 〜N(0,且)and vector V ∈ Rm defined as Vi = φ((Wh)i)=
1[(W h)i ≥ b](W h)i . Then
•	|Vi | follows i.i.d. from the following distribution: with probability 1 — e-b2 m∕(4khk2), |Vi| =
0, and with probability e-b2m∕(4khk2), |Vi| follows from truncated Gaussian distribution
Nb(0, mkhk2).
•	mh⅛ is in distribution identical to χ2z ω (b0-truncated chi-square distribution oforder ω)
where ω follows from binomial distribution B(m, e-b2m/(4khk2)) and b0 = Ym/2 b.
Proof. We assume each vector Wi is generated by first generating a gaussian vector g 〜N(0, mI)
and then setting Wi = ±g where the sign is chosen with half-half probability. Now, |hWi, hi| =
∣hg, h)| only depends on g, and is in distribution identical to Nb(0, m Ilhk2). Next, after the sign is
40
Under review as a conference paper at ICLR 2022
determined, the indicator 1[(Wih)i ≥ b] is 1 with probability e-b2m/(4khk2) and 0 with probability
1 - e-b2m/(4khk2). Therefore, |vi| satisfies the aforementioned distribution. As for kvk22, letting ω ∈
{0, 1, . . . , m} be the variable indicates how many indicators are 1, then ω 〜B(m, e-b2m∕(4khk2))
口nd mkvk2 〜	v.2	Where b，一	7m/ b	∏
and 2khk2	χb0,ω,	Where b	= kh∣∣2 b.
Fact G.5 (Gaussian tail bound). For any b > 0, we have
e-b2/2	2
C(b+I) ≤1 - φ㈤ ≤e-b /2,
where C is an absolute constant.
We prove a truncated Gaussian version of Lemma 7.1 of Allen-Zhu et al. (2019a).
Lemma G.6. Let b > 0 be a fixed scalar. Let the activation function be defined as φ(x) :=
√cb1[x > p2∕mb]x, where Cb := (2(1 — Φ(b) + bφ(b)))-1/2. Let e ∈ (0,1), then over the
randomness of W(0), with probability at least 1 — O(nL) ∙ exp(-Ω(m exp(—b2∕2)e2∕L2)), we
have
khi,` k2 ∈ [1 — e, 1 +e], ∀i ∈ [n], ` ∈ [L].
Proof. We only prove fora fixed i ∈ [n] and ` ∈ {0, 1, 2, . . . , L} because We can apply union bound
at the end. BeloW, We drop the subscript i for notational convenience, and Write hi,` and xi as h`
and x respectively.
According to Fact G.4, fixing any h`7 = 0 and letting w` be the only source of randomness, We
have
mm Ilh'k2 〜X2∕khk2,ω, with ω 〜B(m,1 - φ(b0)),
where E = b∕∣∣h'-1∣∣2.
We first consider the ' = 1 case. Then, we have ∣∣h'-ι∣2 = 1, and b0 = b. Let Pb :=1 — Φ(b). By
Chernoff bound, for any δ ∈ (0, 1), we have
Pr[ω ∈ (1 土 δ)mPb] ≥ 1 — exp(—Ω(δ2Pbm)).
In the following proof, we condition on this event. By Fact G.5,
ω ∈ (1 土 δ)Pbm ^⇒
ω∈
e-b2∕2
(1 — δ)7^—j-τm, (I + S)exP(—b2∕2)m .
C(b + 1)
By Lemma G.3, we have
Pr [m2∣hι∣2 - ω(1 + 愣
> t ≤ exp (—Ω(t2∕(ωb4))) + exp (—Ω(t∕b2))
Note that
ω 1+
∈ (1 ± δ)mPb + (1 ± δ)mPb ∙ bΦ(b) = (1 ± δ)(Pb + bφ(b)) ∙ m.
Pb
Let Cb-1 := 2(Pb + bφ(b)) be the normalization constant. Then, we have
Pr[∣Cb∣hι∣2 — (1 ± δ)∣ > 2tcb∕m] ≤ exp (—Ω(t2∕(ωb4))) + exp (—Ω(t∕b2)).
We want 2tcb∕m = O(δ), i.e., t = O(δc-1m). Then, we have ωt =加口⑴ > b2. Hence, by
Lemma G.3, we actually have
Pr[∣cb∣hι∣2 — (1 ± δ)∣ > O(δ)] ≤ exp (—Ω(δm∕(cbb2))).
By taking δ = e∕L, we get that
Ih1 I22 ∈ [1 — e∕L, 1 + e∕L]
41
Under review as a conference paper at ICLR 2022
with probability at least
1 — exp(-Ω(e2Pbm/L2)) — exp1—Ω(em/(Cb^L)) ≥ 1 — exp(-Ω(e2P⅛m∕L2)),
where the last step follows from db2 = Pb+2φ(b = Θ(Pb).
We can inductively prove the ` > 1 case. Since the blowup of the norm of h1 is 1 ± /L, the con-
centration bound is roughly the same for h` for ` ≥ 2. Thus, by carefully choosing the parameters,
We can achieve ∣∣h'∣2 ∈ [(1 一 e∕L)', (1 + e∕L)'] with high probability.
In this end, by a union bound over all the layers ` ∈ [L] and all the input data i ∈ [n], we get that
khi,`k2 ∈ [1 — , 1 + ]
with probability at least
1 一 O(nL) exp(-Ω(c2Pbm∕L2)),
which completes the proof of the lemma.	□
42