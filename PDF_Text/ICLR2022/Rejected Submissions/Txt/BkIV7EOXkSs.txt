Implicit Regularization of Bregman Proximal
Point Algorithm and Mirror Descent on Sepa-
rable Data
Anonymous authors
Paper under double-blind review
Ab stract
Bregman proximal point algorithm (BPPA), as one of the centerpieces in the op-
timization toolbox, has been witnessing emerging applications. With simple and
easy to implement update rule, the algorithm bears several compelling intuitions
for empirical successes, yet rigorous justifications are still largely unexplored. We
study the computational properties of BPPA through classification tasks with sepa-
rable data, and demonstrate provable algorithmic regularization effects associated
with BPPA. We show that BPPA attains non-trivial margin, which closely depends
on the condition number of the distance generating function inducing the Bregman
divergence. We further demonstrate that the dependence on the condition number
is tight for a class of problems, thus showing the importance of divergence in
affecting the quality of the obtained solutions. In addition, we extend our findings
to mirror descent (MD), for which we establish similar connections between the
margin and Bregman divergence. We demonstrate through a concrete example,
and show BPPA/MD converges in direction to the maximal margin solution with
respect to the Mahalanobis distance. Our theoretical findings are among the first
to demonstrate the benign learning properties BPPA/MD, and also provide strong
corroborations for a careful choice of divergence in the algorithmic design.
1I ntroduction
The role of optimization algorithms has become arguably one of the most critical factors in the
empirical successes of training deep models. As the go-to choice for modern machine learning,
first-order algorithms, including (stochastic) gradient descent and their adaptive counterparts (Kingma
and Ba, 2014; Duchi et al., 2011), have received tremendous attention, with detailed investigations
dedicated to understanding the effect of batch size (Goyal et al., 2017; Smith et al., 2018; Keskar
et al., 2016), learning rate (Li et al., 2019; He et al., 2019; Lewkowycz et al., 2020), and momentum
(Sutskever et al., 2013; Smith, 2018) across a broad spectrum of applications.
Meanwhile, Bregman proximal point algorithm (Eckstein, 1993; Kiwiel, 1997) has been drawing
substantial interests. The resounding successes of this classical algorithm are particularly evident
for applications including knowledge distillation (Furlanello et al., 2018), mean-teacher learning
paradigm (Tarvainen and Valpola, 2017), few-shot learning (Zhou et al., 2019), policy optimization
(Green et al., 2019), and fine-tuning pre-trained models (Jiang et al., 2020), yielding competitive
performance compared to its first-order counterparts. In the general form, Bregman proximal point
algorithm updates parameters by minimizing a loss L(∙), while regularizing the weighted distance to
the previous iterate measured by some divergence function D(∙, ∙),
θt+ι = argmi□θ L(θ) + 1/(2ηt)D(θ,θt).	(1.1)
Popular choices of divergence function used in practice include the squared '2-norm distance
DLS (θ, θt) = ED kfθ(x) - fθt (x)k22 (Tarvainen and Valpola, 2017), and Kullback-Leibler based
divergence DKL(θ, θt) = EDKL (fθ0 (x)kfθ(x)) (Furlanello et al., 2018), where D denotes the data
distribution.Such a simple update is of great practical purposes, as it is easy to describe, and
admits simple implementation by adopting suitable off-the-shelf black-box optimization algorithms
(Solodov and Svaiter, 2000; Monteiro and Svaiter, 2010; Zaslavski, 2010). The updating form
also suggests plausible intuitions for its empirical successes, including iteratively constraining the
search space, alleviating aggressive updates, and preventing catastrophic forgetting (Schulman et al.,
2015; Li and Hoiem, 2017). However, none of the intuitions have been rigorously justified, and
1
theoretical understandings for the empirical successes of Bregman proximal point algorithm remains
underexplored.
A first, and a natural question is whether Bregman proximal point algorithm benefits from the same
kind of mechanism that (stochastic) gradient descent (GD/SGD) enjoys for having the generalization
properties. In particular, in many important applications, GD/SGD is widely believed as the “the
algorithm that finds the right kind of solutions” for problems with non-unique solutions. Such a claim
is supported with numerous provable examples: GD/SGD converges to the minimum-norm solution
of under-determined linear systems (Gunasekar et al., 2018), converges to the max-margin solution
for separable data (Soudry et al., 2018; Nacson et al., 2019), aligns layers of deep linear networks (Ji
and Telgarsky, 2018), and converges to a generalizable solution for nonlinear networks (Brutzkus
et al., 2017; Allen-Zhu et al., 2018) in the presence of infinitely many overfitting solutions. Given the
emerging successes of Bregman proximal point algorithm, and the aforementioned evidences on its
first-order counterparts (e.g. GD/SGD) finding generalizable solutions, one would naturally wonder
Does Bregman proximal point algorithm converge to a solution with favorable qualities?
Another important question with great practical implications for Bregman proximal point algorithm
is how the divergence measure D(∙, ∙) affects the solution. Instead of directly applying the Euclidean
distance based divergence, it is widely observed that the successful application of Bregman proximal
point algorithm is contingent on the careful design of divergence measure, based on the task at
hand (Li and Hoiem, 2017; Hinton et al., 2015). Take the example of fine-tuning language model,
the symmetrized Kullback-Leibler based divergence evaluated on the predictions of the updated
model (i.e., θt+1) and previous model (i.e., θt) yields the state-of-the-art result (Jiang et al., 2020).
Identifying the underlying mechanism for the success or failure of a given divergence choice is not
only of theoretical interest, but also can significantly reduce human effort in searching/designing the
suitable divergence for a given task. As an important addition, one may also ask whether the impact of
divergence on the Bregman proximal point algorithm find natural counterparts in commonly adopted
first-order algorithms (e.g. mirror descent, (Nemirovski and Yudin, 1983)). In such cases, better
task-dependent algorithmic designs could be proposed in conjunction with the suitable divergence.
To this end, we raise our second question.
How does divergence affect the qualities of the solution obtained by Bregman proximal point
algorithm (and other first-order algorithms)?
In this paper, we initiate our study to address our previously proposed questions. We focus on
a non-trivial example of an under-determined system - training linear classifiers using separable
data. In particular, for exponential tail losses (e.g., exponential loss), the empirical loss function has
infimum zero that is asymptotically attainable at infinity along infinitely many directions. The natural
candidate for measuring the quality of the obtained classifier is its margin, i.e., the minimum distance
between the samples and the decision hyperplane. For such a problem, we summarize our theoretical
findings below as concrete answers to the previous questions.
• We show that Bregman proximal point algorithm (BPPA) obtains a solution with non-trivial margin
lower-bound. As a concrete demonstration, we tailor our main theorem for Mahalanobis distance,
and show that BPPA converges in direction to the maximal margin solution. We provide non-
asymptotic analyses of the margin and empirical loss for constant stepsize BPPA, and propose a
more aggressive stepsize rule for a provable exponential speed-up.
• We establish a dependence of such a margin lower-bound on the condition number of the distance
generating function for defining the divergence. In addition, we provide a class of problems where
the margin lower-bound is tight, demonstrating that the Bregman divergence is crucial in affecting
the quality of the obtained solution.
• We extend our findings to first-order algorithms. Specifically, we show that mirror descent
(MD) enjoys the same previously mentioned margin properties. We also provide non-asymptotic
convergence analyses of the margin and empirical loss for constant stepsize MD, and its exponential
speed-up using a varying stepsize scheme. Our findings for MD strictly complement prior works
on under-determined regression problems (Gunasekar et al., 2018; Azizan and Hassibi, 2019).
Notations. We denote [n] := {1, . . . , n}; sgn(z) = 1 if z ≥ 0 and -1 elsewhere. We use w.r.t in
short for “with respect to”. For any ∣∣∙k in Euclidean space Rd, we use k∙∣∣* = maxkyk≤ι h∙, yi to
denote its dual norm. Note that we have (∣∣∙∣∣) = ∣∣∙∣∣.
2
2P roblem Setup
We study the binary classification on linearly separable data. Specifically, the dataset is S =
{(xi, yi)}in=1 ⊂ Rd × {+1, -1}, where xi is the feature vector, and yi is the label. In addition, there
exists a linear classifier u ∈ Rd such that yi hu, xii > 0 for all i ∈ [n]. That is, the decision rule
fu(∙) = sgn(hu, ∙i) achieves the perfect accuracy on the dataset, with yi = fu(xi) for all i ∈ [n].
For each linear classifier fu(∙) with perfect accuracy, we define its ∣∣∙k * -norm margin as the minimum
distance in ∣∣∙k*-norm from the feature vectors to the decision boundary Hu = {x :hx, Ui = 0}. It is
well known that the ∣∣∙k*-norm margin, denoted as γu, only depends on the direction of the classifier
and satisfies γu = mini∈[n]
(xiyi, ⅛)
,where ∣∣∙∣ is the dual norm of ∣∣∙∣*. The ∣∣∙∣*-norm margin
measures how well the data is separated by decision rule fu(∙), measured in ∣∣∙∣*-norm, and is an
important measure on the generalizability and robustness of the decision rule. Given a norm ∣∣∙∣* on
Rd, we define the optimal linear classifier with the maximum ∣∙∣ * -margin below.
Definition 2.1 (Maximum m-norm Margin Classifier). Given a linearly separable dataset
{(xi,yi)}i∈[n], we define the maximum ∣∣∙∣*-norm margin classifier 叫卜口，and its associated maxi-
mum ∣∣∙∣*-norm margin )卜口 as
u∣∣.k = argmax min hu,yixii,
*	kuk≤ι i∈[n]
γk∙k* = maχι minhu,yixii.
For a separable dataset, we consider finding the classifier by minimizing the empirical loss
LS(θ) = ɪ pn=ι' (hθ,yiXii).	(2.1)
Here we focus on the exponential loss '(χ) = exp(-x), and our analyses can be readily extended to
other losses with tight exponential tail (e.g., logistic loss).
Observation. One can readily verify that with a separable dataset S, the empirical loss has infimum 0
but possesses no finite solution that attains the infimum. Thus any optimization algorithm minimizing
the loss LS (∙) will observe the explosion on the norm of iterate.
It has been shown that various optimization algorithms, including (stochastic) gradient descent and
steepest descent, converge in direction to the maximum margin classifier in different norms (Soudry
et al., 2018; Nacson et al., 2019; Gunasekar et al., 2018; Ji and Telgarsky, 2019; 2021). Connections
between gradient descent and the regularization path of homotopy method have also been established
(Ji et al., 2020). A striking feature behind such phenomena is that there is no explicit regularization
in the loss function, and such effects have been termed as the implicit (algorithmic) regularization.
Up to date, most of the implicit regularization effects are attributed to (stochastic) gradient descent,
given their prevalence in applications. However, as Bregman proximal point algorithm (BPPA)
becomes increasingly popular in various domains, there exists considerable lack of understanding on
the computational properties of BPPA. In addition, practitioners often find the choice of divergence
function crucially important for the performance of BPPA (Jiang et al., 2020; Furlanello et al.,
2018). This empirical evidence thus calls for a detailed characterization on the connection between
computational properties and the divergence function of BPPA.
In what follows, we study the BPPA for solving problem (2.1) in detail. The BPPA (Algorithm 1) is an
adaptation of the vanilla proximal point algorithm (Rockafellar, 1976a;b) to non-euclidean geometry,
by using Bregman divergence as the divergence measure in (1.1). Specifically, given a distance
generating function w(∙) that is convex and differentiable, we define the Bregman divergence Dw (∙, ∙)
associated with w(∙) as Dw (θ,θ0) = w(θ) - w(θ0) - hVw(θ0), θ 一 θ0i. Throughout our discussions,
we only impose the following mild assumption on Bregman divergence function Dw (∙, ∙).
Algorithm 1 Bregman Proximal Point Algorithm (BPPA)
Input: Distance generating function w(∙), stepsizes {ηt}t≥o, samples {xi, yi}n=ι.
Initialize: θ0 - 0.
for t = 0, . . . do
end for
Update θt+ι = argminLS(θ) + 1— Dw(θ,θt).
θ	2ηt
(2.2)
3
Assumption 1. We assume that the distance generating function ofBregman divergence Dw (∙, ∙) is
Lw-smooth and μw-strongly convex w.r.t. k∙k -norm. That is,
μw kθ — Θ0k2 ≤ w(θ) — w(θ0) -hVw(θ0), θ - θ0i ≤ L2w kθ - θ0k2 .
3A lgorithmic Regularization of BPPA
We show BPPA achieves a |卜|| *-norm margin that is at least ,μw/Lw-fraction of the maximal one.
Theorem 3.1 (Constant Stepsize BPPA). Let D∣∣∙k* = maxi∈[n] IIxiiL, where ∣∣∙∣∣* denotes the dual
norm of ∣∣∙∣. Then under Assumption 1, for any constant stepsize η = η > 0, the following hold.
(1)	We have limt→∞ LS(θt) = 0. Specifically, we have that LS(θt) diminishes at the following rate,
LS(θt) ≤
1	+ Lw log2 (Yk∙k*ηt)
YIHL η	4γ2∙∣∣* ηt
L Lw log2 (YIHL 哈
I喻册
O
(2)	We have that the margin is asymptotically lower bounded by
lim min
t→∞ i∈[n]
θt
∣m ,yixi
μw
L γk∙k*,
(3.1)
where γ∣∙k* is defined in Definition 2.1. In addition, for any given e > 0, there exists a to satisfying
such that for t≥ t0 number of iterations, we have
J exp (J E! ɪ )!
e2γ2∙k*,	lγ2∙kj” μw) γ2∙k*η"
/ θt
∖∣m ,yixi
≥ (1 -
∀i ∈ [n].
We highlight that (1) The choice of Bregman divergence in BPPA is flexible and can be data
dependent. Properly chosen data-dependent divergence can adapt to data geometry much better than
data-independent divergence, leading to better separation and margin. In Section 5 we demonstrate
how BPPA can benefit significantly from such an adaptivity of carefully designed data-dependent
divergence. (2) Our analysis on the convergence requires handling non-finite minimizers, which
implies divergence of iterate ∣θt ∣ → ∞. The optimization problem of our interest does not meet the
standard assumptions in the classical analysis of BPPA in the literature, and requires a careful choice
of reference point in order to derive non-trivial convergence results. (3) Our result is closely related,
but should not be confused with the homotopy method in (Rosset et al., 2004), which can be viewed
as performing only one proximal step at the origin, with an extremely large stepsize. (4) Finally, our
result is a generalization of Telgarsky (2013); Gunasekar et al. (2018) to non-euclidean settings with
Bregman divergence. Working with Bregman divergence poses unique challenges, as it is previously
unclear how to relate the primal margin progress to the per-iteration progress over the dual space.
Theorem 3.1 shows that if the distance generating function w(∙) is well-conditioned w.r.t. ∣∙∣-norm,
then Bregman proximal point algorithm will output a solution with near OPtimalk ∙∣ * -norm margin.
As a concrete realization of Theorem 3.1, we consider the Mahalanobis distance ∣ ∙∣A := VZr,A∙i
induced by a positive definite matrix A.
Corollary 3.1. Let ∣∙∣ = ∣∣∙∣a for some positive definite matrix A. Under the same conditions as
in Theorem 3.1, BPPA with distance generating function w(∙) =(∙, A) converges to the maximum
∣∣∙∣*-margin solution, where ∣∣∙∣* = ∣∣∙∣a-i. SPeCifiCally, we have
LS(θt) ≤
1	+ Lw log2 (Yk∙k*ηt)
YlHL ηt	4Y2∙∣∣* ηt
L Lw log2 (YIHL ηt)
I喻册
In addition, we have limt→∞ mi□i∈[n] (ɪθtɪ,yixi)= Y∣∣∙k*. Specifically, for any given e > 0,
there exists a to satisfying to := Oe ( max < Dk2k* , exp ( D∣ -^1—
22
∖	[e γk∙k*	∖γk∙k*' ) γk∙k*η
number of iterations, we have
(∣⅛ ,yixi) ≥ (1 — e)γk∙k* , ∀i ∈ [n].
Finally, we have the direction convergence that limt→∞ ɪθ^ = u∣∣∙k*.
, such that for t≥ t0
O
4
Note that similar directional convergence results have been shown in Gunasekar et al. (2018) for
steepest descent w.r.t. |卜|| a norm. The directional convergence of BPPA obtained here, however, is
not a simple corollary of known results, since existing analyses focus on first-order algorithms in
euclidean setting (e.g., GD/SGD, steepest descent). Such existing analyses do not simply extend to
non-first-order algorithms in non-euclidean setting, such as BPPA.
When the distance generating function w(∙) is ill-conditioned w.r.t. k∙k-norm (i.e., y∕μW∕LW《1),
it might be tempting to suggest that the margin lower bound in (3.1) is loose, and what really happens
is limt→∞ mi□i∈[n] (kθtk, yix) = YIHI *. However, as We show in the following proposition, there
exists a class of problems, where the lower bound in (3.1) is in fact a tight upper bound (up to a factor
of 2), demonstrating that the dependence on condition number of distance generating function w(∙)
w.r.t. ∣∣∙k -norm is not a proof artifact.
Proposition 3.1 (Tight Dependence on Condition Number). There exists a sequence of problems
{p(m)}m≥ι, where each P(m) = (S(m), k∙∣∣(m), w(m)) denotes the dataset, the norm, and the
distance generating function of the m-th problem. For each m, the distance generating function
w(m)(∙) is μWm)-strongly convex and Lwm) -smooth w.r.t. ∣∣∙k -norm. Then Bregman proximal point
algorithm applied to each problem in {P (m)}m≥1 yields
lim min
t→∞ (x,y)∈S(m)
(∣θ⅛ ,yx) ∕γk∙Hm)
(3.2)
In addition, for any m ≥ 4, we have limt→∞ min(x,y)∈S(m) D∣θt∣,yx)∕γ∣∙∣(m) ≤ 2y Lwm) < 1.
In fact,
lim min (您,yx∖ ∕γ∣∣∣∣(m) → 0, as m → ∞.
t→∞ (χ,y)∈S(m) ∖k%k," // ",心 )	,
(3.3)
Combine Theorem 3.1, Corollary 3.1 and Proposition 3.1, we conclude that the margin of the obtained
solution by BPPA has non-trivial dependence on the condition number of the distance generating
function w(∙). This observation provides a strong evidence that the Bregman divergence Dw(∙, ∙)
in BPPA is highly important to the quality of the obtained solution, and advocates a careful design
of Bregman divergence when using the BPPA. Our theoretical findings also aligns the empirical
evidences on the importance of divergence found in knowledge distillation and model fine-tuning
(Jiang et al., 2020; Furlanello et al., 2018).
We have shown that BPPA with constant stepsize achieves a margin that is at least y/μw/Lw -fraction
of the maximal one. Meanwhile, our complexity bound in Theorem 3.1 shows that to obtain such a
margin lower bound, it might take an exponential number of iterations. We next show by employing a
more aggressive stepsize scheme, we can attain the same margin lower bound in a polynomial number
of iterations, while speeding up the convergence of the empirical loss {LS (θt)}t≥0 drastically.
Theorem 3.2 (Varying Stepsize BPPA). Given any positive sequence {αt}t≥0, letting the stepsizes
{ηt}t≥o be ηt = L),then the followingfacts hold.
(1)	limt→∞ LS(θt) = 0. Specifically, for any t ≥ 0, we have LS (θt+1) ≤ LS(θt)β(αt), where
β(a) = minβ∈(o,i) max J β, exp ( — %)["* ) [ < 1.
(2)	Letting at = √+, we have limt→∞ mi□i∈[n] (∣θ⅛ ,yx∖ ≥ ʌ/^w Y∣HI*∙ In particular, for any
E ∈ (0, 2), there exists a to satisfying
t0 = O
YlHL √μW e
(3.4)
such that in t ≥ t0 number of iterations, we have
(∣θ⅛,yix) ≥ (1-e)yLwYHL, ∀i ∈ [n].
5
Additionally, the convergence rate of {LS (θt)}t≥0 is given by LS(θt) = O
We remark that (1) We do not optimize for the best polynomial dependence on 1/ in the iteration
complexity (3.4), as our main goal is to show the exponential gap between the complexity presented
in Theorem 3.1 and here. We refer interested readers to Appendix B, where we show that we can
improve the polynomial dependence with more tailored analysis. (2) We also demonstrate that the
empirical loss converges almost exponentially faster with our choice of stepsizes. (3) We reiterate
that using the aggressive stepsizes does not change our established margin lower bound, and the
exact convergence to the maximum margin solution demonstrated in Corollary 3.1 still holds for this
scheme of stepsizes, which can be achieved with a polynomial number of iterations.
• Inexact Implementation of BPPA. The proximal update (2.2) requires solving a non-trivial
optimization problem, and there has been fruitful results of inexact implementation of BPPA in
optimization literature (Rockafellar, 1976b; Yang and Toh, 2021; Solodov and Svaiter, 2000; Monteiro
and Svaiter, 2010). Here based on the varying stepsize scheme proposed in Theorem 3.2, we discuss
the feasibility of a gradient descent based inexact BPPA that: (1) admits a simple implementation
and achieves polynomial complexity, (2) retains the margin properties of exact BPPA. Specifically, at
the t-th iteration, the gradient descent based inexact BPPA solves the proximal step
Θt+1 ≈ argmm© φt(θ) = 1 Pi=I exp (-(θ, yix，)+ 册Dw(θ, θt)	(3.5)
up to a pre-specified accuracy δt with gradient descent. Our key observation comes from the fact
that when applying gradient descent to φt(∙) with small enough stepsizes, the iterate would stay in a
region that has relative smoothness Mt and relative strong convexity μt bounded by
Mt ≤ Ls(b)+1=Ls(b)(1+a), μt ≥ 1=L0t),
both measured w.r.t. Bregman divergence Dh(∙, ∙) (LU et al., 2018). Note that the first inequality
follows by our choice of stepsize η in Theorem3.2. Thus the effective condition number Kt := Mt/μt
of φt(∙) is bounded by Kt = 1 + at = O(1), which implies that the t-th proximal step requires
O (Kt log( δ1)) = O (log(δ1)) number of gradient descent steps. Summing up across to iterations
(3.4), we need up to O (Pt=ι log(δ^)) gradient descent steps, which depends polynomially on to
even ifwe choose an extremely high accuracy δt = O(exp(-t)) for each inexact proximal step (3.5).
4A lgorithmic Regularization of Mirror Descent
Inspired by the results in the previous section, we further show that mirror descent (MD, Algorithm 2),
as a generalization of gradient descent to non-euclidean geometry, possesses similar connection be-
tween the margin and Bregman divergence. We remark that our results are the first to characterize the
algorithmic regularization effect of MD for classification tasks, while previous literature exclusively
focus on under-determined regression problems (Gunasekar et al., 2018; Azizan and Hassibi, 2019).
Algorithm 2 Mirror Descent Algorithm (MD)
Input: Distance generating function w(∙), stepsizes {ηt}t≥o, samples {xi, yi}i==ι.
Initialize: θ0 - 0.
for t = 0, . . . do
Compute gradient VLS(θt) = * 1 Pi=I exp(- gt, yiXii) (-yiXi).
Update θt+ι = argmin© hVLs(θt),θ - θti + 薪Dw(θ,θt).
end for
Theorem 4.1 (Constant Stepsize MD). Let D∣∣∙k* = maxi∈[n] Ilxil∣*, where ∣∣∙∣∣* denotes the dual
norm of ∣∣∙∣, and D∣∣∙k2 = maxi∈[n] Ilxik2∙ Under Assumption 1, let μ2 be the Strong convexity
parameter of w(∙) w.r.t. ∣∙∣2-norm. Thenforany constant stepsize η = η ≤ 2/^ , we have that
(1) limt→∞ Ls(θt) = 0. Specifically, we have that Ls(θt) diminishes at the following rate,
Ls(θt) ≤
1 l Lw log2 (YIHL ηt)
YlHL ηt	4Y2∙k *ηt
O(Lw log2 (Yk∙k*ηt)
V	γ2∙k *ηt
6
(2) We have that the margin is asymptotically lower bounded by
1.	. / θt	\
lim min ( --Kyixi
t→∞m[n∖l∣θt∣∣,yi i/
In addition, for any > 0, there exists a t0 satisfying
μw
L YH「
t0 = O exp
DK Dk∙k2Lw η
such that any t ≥ t0, we have
log
γ2∙k* μ/此 */
(4.1)
kθtk,yix)≥ (1-e)法γk∙k*'
Theorem 4.1 shows that mirror descent attains the same ∣∣∙∣∣ * -norm margin lower bound as BPPA,
which is y∕μW∕LW-fraction of the maximal margin. Note that μ2 > 0 is a direct consequence of
Assumption 1 and the equivalence of norm on finite-dimensional vector space.
Similar to Corollary 3.1, let ∣∣∙∣∣ = |卜|匕 be the Mahalanobis distance, then MD equipped with
distance generating function w(∙) =(•, A∙) converges to the maximum ∣∣∙∣∣*-norm margin classifier.
Corollary 4.1. Let∣∣∙∣∣ = ∣∣∙∣∣a forsome positive definite matrix A. Then under the same conditions as
in Theorem 4.1, the MD with distance generating function w(∙) =(∙，A) converges to the maximum
∣∣∙k*-margin solution, where ∣∣∙∣∣* = ∣∣∙ka-i. Specifically, we have LS(θt) = O ( Lw lo3 (γk[*ηt ).
'k∙k*'
In addition, we have limt→∞ mini∈[n] (ɪθ^, yixi)= γ∣∣∙k*. Specifically, for any given e > 0,
there exists a t0 with t0 = O exp
iterations, we have
D342*Dk∙k2Lwη
%**/2*3/2m/2
log (ɪ) I ) , such that for t ≥ to number of
(k⅛ ,yixi) ≥ (1 - ')γk∙k*，	∀i ∈[n].
Finally, we have direction convergence that limi→∞ ɪθtɪ = u∣∣∙k*.
Note that Corollary 4.1 recovers the directional convergence of steepest descent in Gunasekar et al.
(2018) w.r.t ∣∙∣ a，which coincides with MD with distance generating function w(∙) = h∙,A).
Theorem 4.1 guarantees the near optimal ∣∙∣*-norm margin when the distance generating function
w(∙) is well-conditioned w.r.t. ∣∙∣-norm. For cases when w(∙) is ill-conditioned, we demonstrate that
there exists a class of problem for which the margin lower bound is tight.
Proposition 4.1. There exists a sequence of problems {P(m)}m≥1by the same construction as in
Proposition 3.1, such that the margin lower bound in Theorem 4.1 is tight up to a non-trivial factor of
2. Specifically, we have (3.2) and (3.3) also hold for MD.
Finally, we propose a more aggressive stepsize scheme for MD that achieves the same margin lower
bound. In addition, instead of requiring an exponential number of iterations (4.1) as constant stepsize
MD, such a stepsize scheme only needs a polynomial number of iterations, and achieves an almost
exponential speedup for the empirical loss {LS(θt)}t≥0.
Theorem 4.2 (Varying Stepsize MD). Let the stepsizes {nt}t≥o be given by ηt =匕：(0七),where
αt = min{ 2D2” , √+ }. Then under the same conditions as in Theorem 4.1,
(1) We have limt→∞
t0 satisfying t0 = O
mini∈[n] (jθtk ,yixi)≥ ∙↑J~LwYIHI *. In addition, for any E > 0, there exists
a
, such that for any t ≥ t0, we have
≥ (1 -
∀i ∈ [n].
(2) We have limt→∞ LS (θt)
0. In addition, the convergence rate is given by
LS(θt) = O
7
5E xperiments
Synthetic Data. We take S = {((-0.5, 1), +1) , ((-0.5, -1), -1) , ((-0.75, -1), -1) , ((2, 1), +1)}.
One can readily verify that the maximum ∣∣∙k2-norm margin classifier is 叫川? =(0,1). For both
BPPA and MD, we take the Bregman divergence as Dw(x, y) = kx - yk22, which corresponds to
the vanilla proximal point algorithm and gradient descent algorithm. Note that both algorithms are
guaranteed to converge in direction towards 叫川心=(0,1), following Corollary 3.1 and 4.1.
L(θt)
(a) BPPA
10°	1.000	
1。:	—	—0.975 101	∖	=0.950 范 I%；	'∖	& 0.925 7 10,n	'、、、	忑 0.900 10	、、、	∖ 0875 ιn-24	尤 υ∙az3 W	— CowtairtSteiwize	^0.850 10	v∙ιytag St⅛)sl∞	0 般5 0 200 400 600 800 1000 1200	, Iterations		ColHtantS豌 >ize , Varying Stepsize 200 400 600 800 10001200 Iterations
(b) MD
Figure 1: BPPA and MD run on the simple data set S.
We take η = η = 1 for the constant stepsize BPPA/MD, and η = S ; ∕τxτ for the varying
L(θt) t+1
stepsize BPPA/MD, following the stepsize choices in Theorem 3.1, 3.2, 4.1 and 4.2. To implement
the proximal step in BPPA at the t-th iteration, we take 128 number of gradient descent steps with
stepsize 0.2ηt, following our discussion at the end of Section 3. We initialize all algorithms at the
origin and run 1200 iterations. From Figure 1, we can clearly observe that both BPPA and MD
converge in direction to the maximum ∣∣∙k2-norm margin classifier 叫川心,which is consistent with our
theoretical findings. In addition, by adopting the varying stepsize scheme proposed in Theorem 3.2
and 4.2, both BPPA and MD converge exponentially faster than their constant stepsize counterparts.
Data-dependent Bregman Divergence.We illustrate through an example on how properly chosen
data-dependent divergence can lead to much improved separation compared to data-independent
divergence, even on simple linear models.
We have n labeled data {(χi,yi)Eι sampled from a mixture of sphere distribution: yi 〜
Bernoulli(1/2), Xi 〜 Unif (Syiμ(r)), where Sz(r) denotes the sphere centered at Z with radius
r in Rd. In addition, we also have m unlabeled data {xej}jm=1, following the same distribution as
{χi}n=ι, with no labels given. Clearly, the maximum ∣∣∙∣2-margin classifier for the mixture of sphere
distribution considered here is given by the linear classifier f *(∙) = sign(h∙, μi).
Figure 2: BPPA with Bregman divergence D(3) (right) significantly
improves alignment with optimal classifier μ, compared to D(I) (left)
and D(2) (middle).
Divergence	Alignment
D(I) (∙,∙)	0.8703
D ⑶(∙,∙)	0.8175
D ⑶(∙,∙)	09754
av-
TabIe 1: (jθθτik7，〃)
eraged over 8 runs.
We choose n = d = 2,m = 100, r = 0.8, and generate μ 〜Unif (So(1)). We compare three types
of Bregman divergence, given by D(1) (θ, θ0) = ∣θ - θ0∣22 (vanilla proximal point), D(2) (θ, θ0) =
(θ 一 θ0)>Σ(θ 一 θ0), and D(3)(θ, θ0) = (θ 一 θ0)>Σ-1(θ 一 θ0), where Σ =* Pjm=I Xjx> denotes
the empirical covariance matrix. Note that D(2) and D(3) are data-dependent from their construction.
For each divergence function, we run BPPA with 8 independent runs, the results are reported in
Figure 2 and Table 1. We make two important remarks on the empirical results:
•	Data-dependent divergence D (3) gives the best separation despite limited labeled data (in fact only
2!), much improved over data-independent squared '2-distance D(I).
•	Not all data-dependent divergence helps, D(2) shows degradation compared to D(1).
We further remark that by utilizing Corollary 3.1, one can completely characterize the solution
obtained by BPPA for each of the divergence in closed form. Using such a characterization allows
one to corroborate the empirical phenomenon with our developed theories, deferred in Appendix A.
8
CIFAR-100. We demonstrate the potential of extending our theoretical findings for linear models
to practical networks, using ResNet-18 (He et al., 2016), ShuffleNetV2 (Ma et al., 2018), Mo-
bileNetV2 (Sandler et al., 2018), with CIFAR-100 dataset (Krizhevsky et al., 2009). At each
iteration of BPPA, the updated model parameter θt+1 is given by solving the proximal step
Θt+1 = argmi□θ 1/nPn=ι '(fθ(xi); yi) + 1∕(2ηt)D(θ; θt) for all t ≥ 0, where D denotes di-
vergence function, and θ0 is obtained by standard training with SGD. We consider inexact imple-
mentation of the proximal step, discussed in (3.5). Specifically, each proximal step is solved by
using SGD, with a batch size of 128, an initial learning rate of 0.1 which is subsequently divided
by 5 at the 60th, 120th, and 160th epoch. We consider two divergence functions widely used in
practice, defined by DLS (θ0, θ) = 1/(2n) Pin=1 kfθ(xi) - fθ0 (xi)k22 (Tarvainen and Valpola, 2017),
and DKL(θ, θ0) = 1/(2n) Pin=1 KL (fθ0 (xi)kfθ(xi)) (Furlanello et al., 2018). For each of the diver-
gence, we run BPPA with 3 proximal steps, with the proximal stepsize ηt = η = 0.025 for DKL, and
ηt = η = 0.2 for DLS (ηt = 0.025 gives significantly worse performance). For standard training
with SGD, we use a batch size of 128, an initial learning rate of 0.1 further divided by 5 at the 60th,
120th, and 160th epoch. The results are reported in Figure 3.
50	100	150	200
Epoch
MobΠeNetV2
75	100	125	150	175	200
Epoch
Figure 3:BPPA with divergences DKL and DLS on CIFAR-100 dataset. KL-Prox-k denotes learning
curve of the k-th proximal step with DKL ; LS-Prox-k denotes learning curve of the k-th proximal
step with DLS .
One can clearly see from Figure 3: (1) Across different model architectures, BPPA with DKL
outperforms standard training with SGD; (2) BPPA with DLS yields negligible differences compared
to SGD. The qualitative difference of DKL and DLS strongly indicates that the divergence function
serves an important role in affecting the model performance learned by BPPA, which we view as an
important evidence showing broader applicability of our developed divergence-dependent margin
theories. In addition, the learned model with DKL improves gradually w.r.t the total number of
proximal steps. For ResNet-18, the accuracy increases from 75.83% (standard training) to 78.56%
after 3 proximal steps - an additional 1.4% improvement over Tf-KDsel f (see Table 2), which can be
viewed as BPPA with 1 proximal step. We view such findings as the evidence suggesting the scope of
algorithmic regularization associated with BPPA goes beyond simple linear models.
We make further remarks on the previously proposed
method in Yuan et al. (2019), named Teacher-free
Knowledge Distillation via self-training (Tf-KDself) ,
which is equivalent to BPPA with 1 proximal steps, us-
ing DKL (θ, θ0) as the divergence function. Tf-KDself
was shown to improve over SGD for various network
architectures on CIFAR-100 and Tiny-ImageNet. We
include the reported results on CIFAR-100 therein in
Table 2 for completeness.
6C onclusion and Future Direction
Model	SGD	Tf-KDself
MobileNetVf	68.38	70.96 (+2.58)
ShUfleNetV2	70.34	72.23 (+1.89)
ReSNet18 一	75.87	77.10 (+1.23)
GoogLeNet	78.72	80.17 (+1.45)
DenSeNet12Γ"	79.04	80.26 (+1.221
Table 2: Comparison of Tf-KDself (2-step
BPPA) and SGD on CIFAR-100.
To conclude, we have shown that for binary classification task with linearly separable data, the
Bregman proximal point algorithm and mirror descent attain a ∣∣∙k*-norm margin that is closely
related to the condition number of the distance generating function w.r.t. ∣∣∙k-norm. We list two
directions worthy of future investigations. (1) Our analyses exploit the fact that the Bregman
divergence is defined over the model parameters, while many popular data-dependent divergences
are defined over the model output (e.g. prediction confidence). Making this non-trivial extension to
data-dependent divergence can also help demystify the mechanism of the data-dependent divergence.
(2) Our current analyses focus on linear models, and the extension to nonlinear neural networks
requires more delicate definitions of margin and divergence. We leave this direction as our long-term
investigation plan.
9
References
Allen-Zhu, Z., Li, Y. and Liang, Y. (2018). Learning and generalization in overparameterized
neural networks, going beyond two layers. arXiv preprint arXiv:1811.04918 .
Azizan, N. and Hassibi, B. (2019). Stochastic gradient/mirror descent: Minimax optimality and
implicit regularization. In International Conference on Learning Representations.
Brutzkus, A., Globerson, A., Malach, E. and S halev- S hwartz, S. (2017). Sgd learns
over-parameterized networks that provably generalize on linearly separable data. arXiv preprint
arXiv:1710.10174.
Devlin, J., Chang, M.-W., Lee, K. and Toutanova, K. (2018). Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 .
Duchi, J., Hazan, E. and Singer, Y. (2011). Adaptive subgradient methods for online learning
and stochastic optimization. Journal of machine learning research 12.
Eckstein, J. (1993). Nonlinear proximal point algorithms using bregman functions, with applica-
tions to convex programming. Mathematics ofOperations Research 18 202-226.
FRENCH, R. M. (1999). Catastrophic forgetting in connectionist networks. Trends in cognitive
sciences 3 128-135.
Furlanello, T., Lipton, Z., Tschannen, M., Itti, L. and Anandkumar, A. (2018). Born
again neural networks. In International Conference on Machine Learning. PMLR.
Goyal, P., Dollar, P., Girshick, R., Noordhuis, P., Wesolowski, L., KYROLa, A.,
Tulloch, A., Jia, Y. and He, K. (2017). Accurate, large minibatch sgd: Training imagenet in 1
hour. arXiv preprint arXiv:1706.02677 .
Green, S., Vineyard, C. M. and Koς, C. K. (2019). Distillation strategies for proximal policy
optimization. arXiv preprint arXiv:1901.08128 .
Gunas ekar, s., Lee, J., Soudry, D. and Srebro, N. (2018). Characterizing implicit bias in
terms of optimization geometry. in International Conference on Machine Learning. PMLR.
He, K., Zhang, X., Ren, s. and Sun, J. (2016). Deep residual learning for image recognition. in
Proceedings of the IEEE conference on computer vision and pattern recognition.
He, T., Zhang, Z., Zhang, H., Zhang, Z., Xie, J. and Li, M. (2019). Bag of tricks for image
classification with convolutional neural networks. in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition.
Hinton, G., Vinyals, o. and Dean, J. (2015). Distilling the knowledge in a neural network.
arXiv preprint arXiv:1503.02531 .
JI, Z., DUDiK, M., SCHAPIRE, R. E. and Telgarsky, M. (2020). Gradient descent follows the
regularization path for general losses. in Proceedings of Thirty Third Conference on Learning
Theory (J. Abernethy and s. Agarwal, eds.), vol. 125 of Proceedings of Machine Learning Research.
PMLR.
Ji, Z. and TELGARsKY, M. (2018). Gradient descent aligns the layers of deep linear networks. arXiv
preprint arXiv:1810.02032 .
Ji, Z. and Telgarsky, M. (2019). The implicit bias of gradient descent on nonseparable data. in
Proceedings of the Thirty-Second Conference on Learning Theory (A. Beygelzimer and D. Hsu,
eds.), vol. 99 of Proceedings of Machine Learning Research. PMLR, Phoenix, UsA.
Ji, Z. and Telgarsky, M. (2021). Characterizing the implicit bias via a primal-dual analysis. in
Algorithmic Learning Theory. PMLR.
10
JIANG, H., HE, P., CHEN, W., LIU, X., GAO, J. andZHAO, T. (2020). SMART: Robust and efficient
fine-tuning for pre-trained natural language models through principled regularized optimization.
In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.
Association for Computational Linguistics, Online.
Kakade, S., Shalev-Shwartz, S., Tewari, A. et al. (2009). On the duality of strong convexity
and strong smoothness: Learning applications and matrix regularization. Unpublished Manuscript,
http://ttic. uchicago. edu/shai/papers/KakadeShalevTewari09. pdf 2.
Keskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy, M. and Tang, P. T. P. (2016).
On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint
arXiv:1609.04836.
KINGMA, D. P. and BA, J. (2014). Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980.
Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A.,
Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A. et al. (2017). Overcoming
catastrophic forgetting in neural networks. Proceedings of the national academy of sciences 114
3521-3526.
KIWIEL, K. C. (1997). Proximal minimization methods with generalized bregman functions. SIAM
journal on control and optimization 35 1142-1168.
Krizhevsky, A., Hinton, G. et al. (2009). Learning multiple layers of features from tiny images
.
Lewkowycz, A., Bahri, Y., Dyer, E., Sohl-Dickstein, J. and Gur-Ari, G. (2020). The large
learning rate phase of deep learning: the catapult mechanism. arXiv preprint arXiv:2003.02218 .
Li, Y., Wei, C. and Ma, T. (2019). Towards explaining the regularization effect of initial large
learning rate in training neural networks. arXiv preprint arXiv:1907.04595 .
LI, Z. and HOIEM, D. (2017). Learning without forgetting. IEEE transactions on pattern analysis
and machine intelligence 40 2935-2947.
Lu, H., Freund, R. M. and Nesterov, Y. (2018). Relatively smooth convex optimization by
first-order methods, and applications. SIAM Journal on Optimization 28 333-354.
MA, N., ZHANG, X., ZHENG, H.-T. and SUN, J. (2018). Shufflenet v2: Practical guidelines for
efficient cnn architecture design. In Proceedings of the European conference on computer vision
(ECCV).
McCloskey, M. and Cohen, N. J. (1989). Catastrophic interference in connectionist networks:
The sequential learning problem. In Psychology of learning and motivation, vol. 24. Elsevier,
109-165.
Monteiro, R. D. and Svaiter, B. F. (2010). Convergence rate of inexact proximal point methods
with relative error criteria for convex optimization. submitted to SIAM Journal on Optimization .
Nacson, M. S ., Lee, J., Gunasekar, S., Savarese, P. H. P., Srebro, N. and Soudry, D.
(2019). Convergence of gradient descent on separable data. In Proceedings of the Twenty-Second
International Conference on Artificial Intelligence and Statistics (K. Chaudhuri and M. Sugiyama,
eds.), vol. 89 of Proceedings of Machine Learning Research. PMLR.
Nemirovski, A. S. and Yudin, D. B. (1983). Problem complexity and method efficiency in
optimization .
Rockafellar, R . T. (1976a). Augmented lagrangians and applications of the proximal point
algorithm in convex programming. Mathematics of operations research 1 97-116.
ROCKAFELLAR, R. T. (1976b). Monotone operators and the proximal point algorithm. SIAM journal
on control and optimization 14 877-898.
11
Rosset, S., Zhu, J. and Hastie, T. (2004). Boosting as a regularized path to a maximum margin
classifier. The Journal of Machine Learning Research 5 941-973.
Sandler, M., Howard, A., Zhu, M., Zhmoginov, A. and Chen, L.-C. (2018). Mobilenetv2:
Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer
vision and pattern recognition.
Schulman, J., Levine, S., Abbeel, P., Jordan, M. and Moritz, P. (2015). Trust region policy
optimization. In International conference on machine learning. PMLR.
Smith, L. N. (2018). A disciplined approach to neural network hyper-parameters: Part 1-learning
rate, batch size, momentum, and weight decay. arXiv preprint arXiv:1803.09820 .
Smith, S. L., Kindermans, P.-J. and Le, Q. V. (2018). Don’t decay the learning rate, increase
the batch size. In International Conference on Learning Representations.
Solodov, M. V. and Svaiter, B. F. (2000). Error bounds for proximal point subproblems and
associated inexact proximal point algorithms. Mathematical programming 88 371-389.
Soudry, D., Hoffer, E. and Srebro, N. (2018). The implicit bias of gradient descent on separable
data. In International Conference on Learning Representations.
Sutskever, I., Martens, J., Dahl, G. and Hinton, G. (2013). On the importance of initial-
ization and momentum in deep learning. In Proceedings of the 30th International Conference
on Machine Learning (S. Dasgupta and D. McAllester, eds.), vol. 28 of Proceedings of Machine
Learning Research. PMLR, Atlanta, Georgia, USA.
Tarvainen, A. and Valpola, H. (2017). Mean teachers are better role models: Weight-
averaged consistency targets improve semi-supervised deep learning results. arXiv preprint
arXiv:1703.01780.
TELGARS KY, M. (2013). Margins, shrinkage, and boosting. In International Conference on Machine
Learning. PMLR.
Yang, L. and Toh, K.-C. (2021). Bregman proximal point algorithm revisited: a new inexact
version and its variant. arXiv preprint arXiv:2105.10370 .
YUAN, L., TAY, F. E., LI, G., WANG, T. and FENG, J. (2019). Revisit knowledge distillation: a
teacher-free framework .
Zaslavski, A. J. (2010). Convergence of a proximal point method in the presence of computational
errors in hilbert spaces. SIAM Journal on Optimization 20 2413-2421.
Zhang, T., Wu, F., Katiyar, A., Weinberger, K. Q. and Artzi, Y. (2021). Revisiting
few-sample {bert} fine-tuning. In International Conference on Learning Representations.
Zhou, P., Yuan, X., Xu, H., Yan, S. and Feng, J. (2019). Efficient meta learning via mini-
batch proximal update. In Advances in Neural Information Processing Systems (H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alch6-Buc, E. Fox and R. Garnett, eds.), vol. 32. Curran
Associates, Inc.
12