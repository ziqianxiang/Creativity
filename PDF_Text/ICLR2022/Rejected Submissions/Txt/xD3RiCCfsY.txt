Under review as a conference paper at ICLR 2022
On Learning to Solve Cardinality Con-
strained Combinatorial Optimization in One-
Shot: A Re-parameterization Approach via
Gumbel-Sinkhorn-TopK
Anonymous authors
Paper under double-blind review
Ab stract
Cardinality constrained combinatorial optimization requires selecting an optimal
subset of k elements, and it will be appealing to design data-driven algorithms
that perform TopK selection over a probability distribution predicted by a neu-
ral network. However, the existing differentiable TopK operator suffers from an
unbounded gap between the soft prediction and the discrete solution, leading to in-
accurate estimation of the combinatorial objective score. In this paper, we present
a self-supervised learning pipeline for cardinality constrained combinatorial opti-
mization, which incorporates with Gumbel-Sinkhorn-TopK (GS-TopK) for near-
discrete TopK predictions and the re-parameterization trick resolving the non-
differentiable challenge. Theoretically, we characterize a bounded gap between
the Maximum-A-Posteriori (MAP) inference and our proposed method, resolving
the divergence issue in the previous differentiable TopK operator and also provid-
ing a more accurate estimation of the objective score given a provable tightened
bound to the discrete decision variables. Experiments on max covering and dis-
crete clustering problems show that under comparative time budget, our method
outperforms state-of-the-art Gurobi solver and the novel one-shot learning method
Erdos Goes Neural.
1	Introduction
In this paper, we aim to solve a family of cardinality constrained combinatorial optimization: given
the cardinality budget k, our aim is to find the optimal set with size k which minimizes the objective
function f (x). Denote ||x||0 ≤ k as the cardinality constraint, we have:
min f (x)	s.t. ||x||0 ≤ k	(1)
x
Extensive applications of these problems can be found in machine learning and operations research,
for example, discovering coresets for dataset summarization and continual learning (Borsos et al.,
2020), discovering the most influential seed users in social networks (Chen et al., 2021), and plan-
ning facility locations in operation research (Liu, 2009). Although there has been a success in
discovering the important submodularity (Fujishige, 1991) for a family of these problems with a
bounded approximation ratio of greedy algorithms, the ubiquitous NP-hard challenge is always mo-
tivating us to develop improved algorithms with better efficiency-accuracy trade-off. The recent
success of machine learning has inspired researchers to develop learning-based algorithms for com-
binatorial optimizations (Vinyals et al., 2015; Dai et al., 2016; Karalias & Loukas, 2020; Wang et al.,
2021b), and this paper falls in line with these works.
According to the taxonomy by a recent survey (Peng et al., 2021), existing machine learning meth-
ods for combinatorial optimization can be mainly categorized into two groups: multi-step meth-
ods (Vinyals et al., 2015; Dai et al., 2016; Lu et al., 2019; Wang et al., 2021a) and one-shot meth-
ods (Wang et al., 2019; Karalias & Loukas, 2020; Li et al., 2019). The former line of works usually
formulates the combinatorial optimization solving procedure as a Markov Decision Process (MDP)
and refers to novel reinforcement learning methods (Mnih et al., 2013; Schulman et al., 2017) to
tackle the MDP. Perhaps the largest drawback of such a multi-step pipeline is that the training and
1
Under review as a conference paper at ICLR 2022
inference procedures can be inefficient. Based on some previous efforts (Wang et al., 2019; Li
et al., 2019), there is a recent seminal work (Karalias & Loukas, 2020) aiming to develop a one-shot
pipeline for general combinatorial problems. Although being a fast and general pipeline, Karalias
& Loukas (2020) do not encode constraint in the model prediction and thus the estimation of the
combinatorial objective score may be inaccurate.
In this paper, we aim to facilitate the one-shot learning pipeline with cardinality constraints, such
that the constraints can be softly encoded by the output of a neural network. Regarding the input to
TopK as a probability distribution, the maximum-a-posterior (MAP) inference is done by sorting all
probabilities and selecting the K-largest ones, however, this process is non-differentiable. Though
appealing, it is non-trivial to encode the constraint in a differentiable manner with bounded gap w.r.t.
the MAP inference and the bound of the existing SOFT-TopK algorithm (Xie et al., 2020) diverges if
the k-th and (k + 1)-th probabilities are equal. Unfortunately, such a dilemma is possible because the
probabilities are outputs from a neural network. Besides, concerning the discrete decision variables
in combinatorial optimization, it is also non-trivial to build an accurate estimation of the objective
score with neural network output (which has to be continuous in order to preserve the gradient).
To tackle the non-differentiable challenge and the ill-estimated objective score, we refer to the sem-
inal reparameterization technique with Gumbel noise, which can mimic the sampling procedure
from the continuous probability distributions to near-discrete decision variables, while still pre-
serving the gradient. Such a technique has been found successful for Top-1 (Jang et al., 2017),
permutation (Mena et al., 2018), and sorting (Grover et al., 2019) distributions, and in this paper, we
improve the SOFT-TopK operator (Xie et al., 2020) by introducing the Gumbel re-parameterization
trick, and we theoretically prove that its gap can be bounded with the existence of the Gumbel noise,
in contrast to the gap of SOFT-TopK (Xie et al., 2020) that may turn diverged.
To this end, we present Gumbel-Sinkhorn-TopK (GS-TopK) to facilitate the one-shot self-
supervised learning pipeline (Karalias & Loukas, 2020) with constrained neural network output.
The proposed GS-TopK operator improves the SOFT-TopK (Xie et al., 2020), where we theoreti-
cally characterize a bounded gap by introducing non-zero noise factors. Besides, we show how to
integrate the proposed GS-TopK method into the combinatorial optimization pipeline, with an ac-
curate estimation of the objective score, formulating a self-supervised learning pipeline following
(Karalias & Loukas, 2020). The contributions of this paper are summarized as follows:
1)	We propose a self-supervised learning approach to solve cardinality constrained combinatorial
optimizations in one shot. The cardinality constraints are encoded in the model output via our pro-
posed GS-TopK, and our pipeline is differentiable via the re-parameterization trick.
2)	We theoretically characterize a bounded gap between the MAP inference and the approximate
solution by Gumbel-Sinkhorn-TopK without requiring the large gap of k-th and (k + 1)-th probabil-
ities, which improves the result of the state-of-the-art SOFT-TopK algorithm (Xie et al., 2020). This
property is crucial for an accurate estimation of the combinatorial objective score in learning.
3)	Experiments on max covering and discrete clustering problems show that under a comparative or
smaller inference time budget, our method surpasses the state-of-the-art one-shot learning methods
e.g. (Karalias & Loukas, 2020), and also surpasses the state-of-the-art Gurobi and SCIP solvers.
2	Related Work
2.1	Learning of Combinatorial Optimization
Recently it is a trending topic of developing machine learning based combinatorial optimization
methods to achieve a better trade-off between time and accuracy by modern data-driven approaches.
Here we follow the taxonomy by Peng et al. (2021) and discuss some representative works.
Multi-step methods predict the solution by a multi-step decision process. Since the fundamental
work (Vinyals et al., 2015) that proposes an encoding-decoding architecture, researchers have ex-
plored this general pipeline to various real-world combinatorial optimization problems. The most
popular framework follows the reinforcement learning pipeline introduced by Khalil et al. (2017),
where the state is encoded by a graph neural network, and multi-step actions are taken to sequentially
construct the solution. Applications have been explored on job scheduling (Zhang et al., 2020), com-
puter resource allocation (Mao et al., 2019), quadratic assignment programming (Liu et al., 2020),
bin-packing (Duan et al., 2019), just to name a few. The advantage of multi-step methods is that they
can handle combinatorial constraints that are non-trivial to be encoded in a differentiable manner, by
2
Under review as a conference paper at ICLR 2022
adding constraints on the feasible actions in the multi-step roll-out procedure. However, the training
of reinforcement learning may be inefficient and vulnerable, and there are also some recent efforts
on developing one-shot combinatorial optimization learning methods.
One-shot methods predict the solution by a single forward pass. Despite the challenges in encoding
general constraints, one-shot learning methods have been successful on certain problems where the
constraint can be encoded by the neural network. For example, in graph matching (Wang et al., 2019;
2021b) the constraint can be encoded by Sinkhorn algorithm (Sinkhorn & Rangarajan, 1964), and
also for graph similarity regression problems (Bai et al., 2019; Li et al., 2019) that are unconstrained.
The seminal work (Karalias & Loukas, 2020) aims to develop a general pipeline for one-shot learn-
ing of combinatorial optimization, by encoding the violation of the constraint as part of its objective
score. However, our experimental results show that compared to Karalias & Loukas (2020), it will
be more appealing if we can explicitly encode the constraint in the neural network output.
2.2	Optimal Transport and Gumbel Re-parameterization
Optimal Transport. The optimal transport (OT) is a linear programming problem that aims to
find the optimal transportation plan with minimal cost. Though the development of the Sinkhorn
algorithm can be date back to 1960s (Sinkhorn & Rangarajan, 1964), it has drawn increasing at-
tention from the machine learning community since the pioneering work (Cuturi, 2013) which in-
troduces the entropic-regularization for the optimal transport problem. Since the Sinkhorn algo-
rithm (Sinkhorn & Rangarajan, 1964) incorporates row- and column-wise normalization which is
differentiable, OT can be incorporated within any end-to-end deep learning pipelines. Thus, OT has
been found successful for deep generative models (Arjovsky et al., 2017; Patrini et al., 2019), solv-
ing jigsaw puzzles (Santa Cruz et al., 2018), and deep graph matching (Fey et al., 2020; Yu et al.,
2020). In complement to the recent line of differentiable sorting papers (Petersen et al., 2021; Cuturi
et al., 2019), Xie et al. (2020) develop the differentiable TopK algorithm namely SOFT-TopK, by
formulating the TopK problem as an OT problem. However, the theoretical gap between the pre-
diction of SOFT-TopK and the result obtained by sorting diverges if the k-th and (k + 1)-th largest
items are equal. In this paper, we prove that this gap can be bounded by introducing Gumbel noise.
Gumbel Re-parameterization. The Gumbel distribution with the re-parameterization trick is
shown effective to mimic the sampling procedure from a given distribution in a differentiable man-
ner. Jang et al. (2017); Maddison et al. (2017) take the initiative to develop Gumbel-Softmax,
where one-hot vectors are sampled from a softmax probability distribution. The non-differentiable
sampling procedure turned to be differentiable by the namely re-parameterization trick (Kingma
& Welling, 2013). Inspired by Jang et al. (2017), Mena et al. (2018) propose a Gumbel-Sinkhorn
method to learn the latent permutations, which is relevant to the GS-TopK method developed in
this paper. Besides, Paulus et al. (2020) develop a general framework for generating combinatorial
distributions via random distributions. However, these existing methods do not study the applica-
tion in combinatorial optimization learning, and the gap between the MAP inference and the result
after Gumbel re-parameterization is not well characterized in Mena et al. (2018). In this paper, we
explore the application of the re-parameterization trick with Gumbel noise in combinatorial opti-
mization, also we theoretically characterize the gap between the MAP inference and the proposed
Gumbel method.
3	The Proposed Gumbel-Sinkhorn-TopK
3.1	Preliminary: SOFT-TopK Algorithm
The TopK problems are ubiquitous in machine learning, yet the MAP inference approach (perform-
ing sorting at the time cost of O(m log m) and selecting the first k items) is unfriendly for current
gradient-based deep neural networks. To introduce differentiable TopK for machine learning, Xie
et al. (2020) formulate the TopK problem as an optimal transport (OT) problem. Specifically, for
selecting the k largest items from the probability vector s of size m (k < m), the OT problem refers
to moving k items to one destination (selected), and the other m - k elements to another destination
(not selected). Thus, the marginal distributions of the OT are:
c = [1 1 ... 1]
'---------V-----}
m items
m-k
k
2 destinations
(2)
r=
3
Under review as a conference paper at ICLR 2022
Algorithm 1: Gumbel-Sinkhorn-TopK (GS-TopK) for Solving Cardinality Constrained
Combinatorial Optimization
Input: List S with m items; ToPK size k; Sinkhorn factor τ; noise factor σ; sample size #G.
1	for a ∈ {1, 2, ..., #G} do
2	for all si, esi = si - σ log(- log(ui)), where ui is sampled from (0, 1) uniform distribution;
esm - min(s) ; c [1 1	1]; r m - k ;
max(s) - Sm， 二.	！，	k ，
m items
6	Dr = diag(Ta1 0 r);	Ta = D-* 1T。；
7	DC = diag(T> 1 0 c);	Ta = TaDe-1;
e	es1 - min(s)
3	= max(s) - es1
4	Tea = exp(-De /τ);
5	while not converged do
Output: A list of transport matrices [Te1, Te2, ..., Te#G].
We can design the destinations to be the min/max values of s, such that the TopK items are moved
to max(s), and the other items are moved to min(s). Then, the distance matrix ofOT is given as:
D
s1 - min(s) s2 - min(s)	... sm - min(s)
max(s) - s1 max(s) - s2 ... max(s) - sm
(3)
While the OT is formulated as an integer linear programming problem:
mintr(T>D)	s.t. T ∈ {0, 1}m×2, T1 = r,T>1 = c,	(4)
where T is the transportation plan which also corresponds to the TopK solution of the problem, and
1 is a column vector whose all elements are 1s. The optimal solution T* should be equivalent to
the solution by firstly sorting all items and then selecting the TopK items. It is also equivalent to
the MAP inference by regarding s1 , ..., sm as probabilities. Following the differentiable Sinkhorn
algorithm for OT (Cuturi, 2013; Sinkhorn & Rangarajan, 1964), the binary constraint on T is relaxed
to continuous values [0, 1], and Eq. (4) is modified with entropic regularization:
mintr(Tτ>D) +τh(Tτ)	s.t. Tτ ∈ [0,1]m×2,Tτ1 = r,Tτ>1 = c,	(5)
where h(Tτ) = i,j Tiτj log Tiτj is the entropic regularizer (Cuturi, 2013). Given any real-valued
matrix D,Eq. (5) is solved by firstly enforcing the regularization factor τ: TT = exp(-D∕τ). Then
Tτ is row- and column-wise normalized alternatively:
Dr = diag(Tτ 1 0r),	Tτ = Dr-1Tτ, Dc = diag(Tτ>10c), Tτ = TτDc-1,	(6)
where 0 means element-wise division. We denote TT* as the converged solution, which is the
optimal solution to Eq. (5).
Theorem 2 of (Xie et al., 2020). Denote xk, xk+1 as the k-th (k + 1)-th largest items, we have
||T* - TT*||F ≤
2mτ log 2
|xk - xk+1|
(7)
where the gap is controlled by τ if |xk - xk+1| > 0. However, the above bound diverges if xk =
xk+1, which is unavoidable because xk, xk+1 are outputs by a neural network. It motivates us to
develop our improved version Gumbel-Sinkhorn-TopK by introducing the Gumbel noise.
3.2 Gumbel-Sinkhorn-TopK (GS -TopK) Algorithm
In this section, we present our proposed Gumbel-Sinkhorn-TopK (GS-TopK) as summarized in
Alg. 1 and we theoretically characterize the gap between the MAP inference and the prediction
by GS-TopK. As the re-parameterization trick (Jang et al., 2017), instead of sampling from a dis-
tribution that is non-differentiable, we add random variables to probabilities predicted by neural
networks. The Gumbel distribution is characterized as:
gσ(u) = -σ log(- log(u)),
(8)
4
Under review as a conference paper at ICLR 2022
where σ controls the variance and u is from (0, 1) uniform distribution. We can update s and D as:
esi = si + gσ (ui )
De = es1 - min(s)
= max(s) - es1
es2 - min(s)
max(s) - es2
esm - min(s)
max(s) - esm
(9)
(10)
Again we formulate the integer linear programming version of the OT with Gumbel noise:
min tr(Tσ>De)	s.t.	Tσ	∈	{0,	1}m×2,	Tσ1	=	r, Tσ>1 =	c,	(11)
where the optimal solution to Eq. (11) is denoted as Tσ*. Without loss of generality, in the following
we sort s1, s2, s3, ..., sm in descending order, into X = x1, x2, x3, ..., xm, and xk, xk+1 are the k-th
and (k + 1)-th largest items, respectively. We characterize the gap between T* and Tσ* under the
expectation over Gumbel distribution.
Now we present the following two lemmas to help establish our theoretical results, in terms of a
bounded gap between the MAP inference and the approximate solution by Gumbel-Sinkhorn-TopK,
Lemma 1 (Bias by the Gumbel Noise). For the integer linear programming solutions with and
without Gumbel noises, we have
Eu [∣∣τ* - τσ*l∣F] ≤ ʌ1	4m(12)
V 1 + exp lxk xk+1l
Proof sketch: The gap between T*, Tσ* is derived by counting the expected number of times that,
after adding the Gumbel noise, an element may become larger than xk or smaller than xk+1. For
the top k elements, the probability that it will become smaller than xk+1 is upper bounded by
1/(1 + exp lxk-χk+l1). For the last (m — k) elements, the probability that it will become larger than
Xk is also upper bounded by 1/(1 + exp lxk-χk+1l). Since ||T* 一 Tσ* ∣∣F changes at most by 4 if
an item becomes larger/smaller than xk/xk+1, and we have m items, then we can proof the upper
bound of this gap. The detailed proof is referred to Appendix A.1.
To make the integer linear programming problem feasible for gradient-based deep learning methods,
we also relax the integer constraint and add the entropic regularization term:
mintr(Te>De) + h(Te)	s.t.	Te	∈	[0, 1]m×2,Te1	=	r,Te>1 =	c,	(13)
Te
1 ∙ 1 ∙	1	-	~ 1 I	1	.,1	K C ,1 甫	/ K / 、 ，ɪ
which is solved by Sinkhorn algorithm on D: firstly T = exp(-D∕τ), then
Der = diag(Te 1	r),	Te	=	Der	Te,	De c	= diag(Te	1	c),	Te	=	TeDec	.	(14)
Here we denote the optimal solution to Eq. (13)as T*. We theoretically characterize the gap between
Tσ* and T*, which corresponds to the optimal solutions of Eq. (11) and Eq. (13), respectively.
Lemma 2 (Gap to a Discrete Solution). Under probability (1 - ), we have
Eu [∣∣Tσ* - T*||f] ≤ (log2)mτ X Ω(xi,Xj,σ, e),
i6=j
(15)
where
Ω(xi,Xj,σ, e)
2σ log σ -
Ixi-Xj-+2σ) + E-XjI (2 + arctan ≡j)
(1 - e)((xi - Xj)2 + 4σ2)(1 + exp xi-xk )(1 + exp xk+；—xj)
(16)
Proof sketch: The gap between Tσ* and T* is a generalization from the Theorem 2 of
Xie et al. (2020). We denote Xπk , Xπk+1 as the k-th and (k + 1)-th largest items af-
ter disturbed by the Gumbel noise, and our aim becomes to prove the upper bound
of Eu [1∕(∣χ∏k + gσ(Unk) - X∏k+1 - gσ(u∏k+ι)∣)], where the probability density function of
gσ(uπk ) - gσ(uπk+1 ) can be bounded by f(y) = 1/(y2 + 4). Thus we can compute the upper
bound under probability (1 - e) by integration. The detailed proof is referred to Appendix A.2. □
5
Under review as a conference paper at ICLR 2022
probabilities with Gumbel noise
Figure 2: Our proposed self-supervised learning pipeline for combinatorial optimization. The prob-
ability of selecting each element is predicted by a graph neural network, and an accurate estimation
of the combinatorial objective score is achieved via near-discrete TopK solution by our GS-TopK.
Graph Neural
Network
Add GUmbel
Noise
Objective score
0 F orward PaSS
Q Backward pass
Theorem 1. FOr the proposed Gumbel TOPK solver, denote T* as the solution by deterministic TopK
algorithm, T as the solution of Gumbel-Sinkhorn-TopK. With probability (1 一 e), we have
4m
1 + exp lxk-χk+1l
+ (log 2)mτ E Ω(xi, Xj, σ, e),
i6=j
(17)
where σ is the noise factor and τ is the temperature of Sinkhorn algorithm. m is the number of
candidates to be selected. Ω(xi, Xj,σ, e) is defined in Eq. (16).
Proof: Based on Lemma 1 and Lemma 2, Theorem 1 is proved by triangle inequality:
EuhlT* - T *∣∣J ≤Eu [||T* - Tσ*∣∣F ]+ Eu [∣∣Tσ* - T *||f ]	□
Remarks. As derived above, the gap between
T* and T is composed of two parts: the bias
by the Gumbel noise (in Lemma 1) and the gap
to a discrete solution (in Lemma 2). Interest-
ingly, the first term becomes smaller given a
smaller σ, and the second term is smaller given
a larger σ or a smaller τ . See the toy example
in Fig. 1, a larger σ can tighten the gap of GS-
TopK to a discrete solution, which is welcomed
in our combinatorial optimization learning sce-
nario where T* is used to compute the objective
score in a self-supervised learning pipeline.
If |xk 一 xk+1 | > 0, with σ → 0+, Eq. (17)
degenerates to the bound derived by (Xie et al.,
2020) and only differs by a constant factor (see
Appendix A.3 for details):
Figure 1: Atoy example to explain Lemma 2: Se-
lect top-3 items from [1.0, 0.8, 0.601, 0.6, 0.4, 0.2]
and we compare GS-TopK and SOFT-TopK con-
cerning the gap to a discrete solution w.r.t. differ-
ent τ, σ configurations. Here the gap to discrete
solutions is tighten by a larger σ and a smaller
τ for GS-TopK, compared to SOFT-TopK whose
gap is larger and can only be controlled by τ .
lim Eu h∣∣T* - T*||fi ≤	(nlog2M
σ→0+	(I — e)|xk -χk + ι |
Itmakes a strong assumption that |xk -Xk+ι | >
0, and the bound diverges if xk = xk+1. Un-
fortunately, xk , xk+1 are predictions by a neural network, whereby such an assumption may not be
satisfied. In comparison, given σ > 0, our conclusion in Eq. (17) is bounded for any xk, xk+1.
3.3 Application to Self-Supervised Combinatorial Optimization Learning
In the following, we discuss the application of GS-TopK to self-supervised combinatorial optimiza-
tion learning, by taking the max covering problem as an example. We consider the max covering
problem with m sets and n objects. Each set may cover any number of objects, and each object is
6
Under review as a conference paper at ICLR 2022
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Algorithm 2: Gumbel-Sinkhorn-TopK Learning for Solving the Max Covering Problem
Input: bipartite adjacency A; values v; learning rate α; GS-ToPK parameters k, τ, σ, #G.
if Training then
L Randomly initialize neural network weights θ;
if Inference then
L Load a pretrained neural network weights θ; Jbest = 0;
while not converged do
s = GraphSageθ(A); [Te1, Te2, ..., Te#G] = GS-TopK(s, k, τ, σ, #G);
「	∙ V .∕Fn]∙r∖ T	∕Γ V V V IX
for all i, Ji = mm(Ti[2,:] A, 1) ∙ v; J = mean([Jι, J2,…,J#g])；
if Training then
|_ update θ with respect to the gradient J and learning rate α by gradient ascent;
if Inference then
update S with respect to the gradient dJ and learning rate α by gradient ascent;
i' 11 ■	E τ r / ΓTΛ ΓrA T ∖ A	T	/ [ ^^Γ	T T ∖
for all i, Ji = ToPK(Ti[2, :])A ∙ v; Jbest = max([Jι, J2,…,J#g], Jbest);
if Homotopy Inference then
L Shrink the values of τ, σ andjump to line 5;
Output: Learned network weights θ (if training)/The best objective Jbest (if inference).
associated with a value. We aim to find k sets (where k < m) such that the covered objects have the
maximum sum of values. The general illustration of our pipeline can be found in Fig. 2.
Firstly, we build a bipartite graph whose two disjoint sets of vertices are the sets and the objects.
An edge is defined if an object is covered by a set. Denote v ∈ Rn as the value of each object, and
A ∈ {0, 1}m×n as the adjacency matrix, the problem is formulated as
s.t. x ∈ {0, 1}m, ||x||0 ≤ k	(18)
where I(x) is an indicator I(x)i = 1 if xi ≥ 1 else I(x)i = 0. To encode the bipartite graph,
we exploit three layers of GraphSage (Hamilton et al., 2017) followed by a fully-connected layer
with sigmoid to predict the probability of selecting each set, which is denoted as s ∈ [0, 1]m. The
probabilities s are fed into the GS-TopK algorithm, which outputs a batch of near-discrete trans-
portation matrices [T1, T2, ..., T#G], whose second row is regarded as a continuous approximation
of the decision variables. The objective value is estimated as:
Ji = min(Ti[2, ：]A, 1) ∙ v,
τ	∕rT T T iʌ
J = mean([J1, J2, ..., J#G])
(19)
For the discrete clustering problem, we aim to select k objects from the full set of m objects, by
minimizing the sum of distance for each object between itself and its nearest selected object. It
differs from the k-means clustering by adding the “discrete” constraint that cluster centers have to
be a subset of objects, which is also known as facility location problem in operations research (Liu,
2009). Denote ∆ ∈ R+m×m as the distance matrix for each pair of objects, we formulate the
discrete clustering problem as follows (please refer to Appendix B for details):
m
min X min({∆i,j | ∀xi = 1})	s.t. x ∈ {0, 1}m, ||x||0 ≤ k	(20)
x j=1
As illustrated in Alg. 2, during training, the Adam optimizer (Kingma & Ba, 2014) is applied for
learning the neural network weights. During inference, the neural network prediction is regarded
as initialization, and we again optimize the distribution w.r.t. the combinatorial objective score by
Adam, during which a search procedure is also performed among all Gumbel samples.
Homotopy GS-TopK Based on our Theorem 1, the gap between T* and T* can be tightened
by shrinking τ and σ, which motivates us to develop a homotopy variant (Xiao & Zhang, 2012;
XU et al., 2016) of GS-TopK with gradually decreased τ, σ, such that T* and J can provide more
accurate estimation during inference. See line 13 in Alg. 2 for details.
7
Under review as a conference paper at ICLR 2022
@03$ ElMTQO
2.5-
0.0- ，，，，，，，，
0	250	500	7S0 IOOO 1250 1500 1750
# iterations
(a)	Erdos Goes Neural
@03$ ElMfqO
2.5-
0.0 -，
O 250	500	7S0	10∞ 1250 1500 17S0
# iterations
(b)	SOFT-ToPK w/ sampling
@03$ ElMfqO
2.5-
0.0- ，，，，，，，，
O 250	500	750	10∞ 1250 1500 1750
# iterations
(c) HomotoPy GS-ToPK
Figure 3: In discrete clustering, comparison of the estimated objective score (red) and the real one
(blue). Our GS-ToPK achieves the best accurate estimation of the objective score.
4 Experiments and Discussion
4.1	Protocols and Baselines
We aPPly the ProPosed GS-ToPK algorithm to learning two rePresentative Problems of cardinality
constrained NP-hard combinatorial oPtimization: i) max covering and ii) discrete clustering Prob-
lem. For each Problem, both a smaller-sized case and a larger-sized case are considered, and we build
seParate training/testing datasets for learning methods. In this PaPer, we referred to randomly gen-
erated datasets following Previous machine learning for combinatorial oPtimization PaPers (Khalil
et al., 2017; Kool et al., 2019) concerning the limited large-scale oPen-source dataset. For the max
covering Problem, our data distribution follows the distribution in ORLIB1. For the discrete cluster-
ing Problem, the data Points are uniformly generated in a 2D Plane.
We comPare the following rePresentative and diverse baselines with our ProPosed method:
Classic Algorithms. Classic algorithms are well-develoPed strong baselines with theoretical guar-
antees. In our exPeriment, we consider the greedy algorithm as the baseline for the max covering
Problem, which has the worst-case aPProximation ratio of (1 - 1/e) due to the submodularity of
the max covering Problem (Fujishige, 1991). For the discrete clustering Problem, we resort to the
classic kmeans (Macqueen, 1967) and kmeans++ (Arthur & Vassilvitskii, 2007) with imProved ini-
tialization techniques. These algorithms are fast but effective, even surPassing commercial solvers
with only 1/1000 inference time on certain test cases.
General Purpose Solvers. We also comPare two general-PurPose solvers Gurobi 9.0 (under educa-
tional license) (Gurobi OPtimization, LLC, 2021) and SCIP 7.0 (Gamrath et al., 2020). Gurobi is
the state-of-the-art commercial solver for general combinatorial oPtimization tasks, and it is known
heavily engineered by various sPeeduP techniques. SCIP is the state-of-the-art oPen-source solver,
which is also regarded as a baseline. We formulate the combinatorial Problems as integer Program-
ming and then call the solver to solve the Problems, with a limited time budget. Our imPlementation
of the solvers is with the Google ORTools API 2.
One-shot Learning Baselines. We comPare with the general Erdos Goes Neural (EGN) (Karalias &
Loukas, 2020) framework that models each decision variable as a Bernoulli distribution and encodes
the constraint as Part of the learning objective. Since the authors of (Karalias & Loukas, 2020) do not
consider cardinality constrained Problems in their exPeriment, we re-imPlement and tune the EGN
model based on their official imPlementation3. We also comPare with SOFT-ToPK (Xie et al., 2020),
which is theoretically characterized as a sPecial case of our ProPosed GS-ToPK when σ = 0. Also
to ensure the effectiveness of the re-Parameterization trick, we facilitate SOFT-ToPK with Gumbel
samPling during inference. All one-shot learning methods are with the same model structure for a
fair comParison and are trained and tested on seParate datasets. Besides, we emPirically find the
learning Process of one-shot methods converges within tens of minutes. Since the reinforcement
learning methods (Khalil et al., 2017; Chen & Tian, 2019) require significantly more training time,
we only include one-shot methods for a fair comParison.
1http://people.brunel.ac.uk/~mastjjb/jeb/orlib/scpinfo.html
2https://developers.google.com/optimization
3https://github.com/Stalence/erdos_neu
8
Under review as a conference paper at ICLR 2022
Table 1: Objective score ↑ and inference time (in seconds) [ comparison of the max covering Prob-
lem, including mean and standard deviation. Under cardinality constraint k, the problem is to select
from m sets to cover a fraction ofn objects. Gurobi solver fails to return the optimal solution within
24 hours, thus reported as out-of-time. Our GS-TopK outperforms all competing methods, with
comparable or smaller time costs. The performance is further improved by Homotopy GS-TopK.
EGN/SOTF-TopK/ours are one-shot solvers
greedy
SCIP 7.0 (faster)
SCIP 7.0 (slower)
Gurobi 9.0 (faster)
Gurobi 9.0 (slower)
Gurobi 9.0 (optimal)
EGN (efficient) (Karalias & Loukas, 2020)
EGN (accurate) (Karalias & Loukas, 2020)
SOFT-TopK (Xie et al., 2020)
SOFT-TopK (w/ sampling)
GS-TopK (ours)
Homotopy GS-TopK (ours)
k=50, m=500, n=1000
k=100, m=1000, n=2000
objective ↑	time，(sec)	objective ↑	time，(sec)
44312.8±818.4	0.024±0.000	88698.9±1217.5	0.089±0.001
43034.7±869.2	30.058±0.017	86269.9±1256.3	59.916±1.752
43497.4±875.6	100.136±0.097	86269.9±1256.3	120.105±0.498
43261.6±856.1	30.078±0.014	85992.4±1643.7	60.168±0.042
43937.2±791.5	100.171±0.085	86862.1±1630.5	120.277±0.139
OOT	OOT	OOT	OOT
36423.7±1128.4	0.244±0.107	70336.7±1676.9	0.525±0.229
36927.3±1163.0	40.542±4.056	71487.5±1552.8	93.670±8.797
41959.9±803.4	48.605±5.783	83211.7±1322.3	45.252±0.713
39684.8±675.6	44.960±4.390	77285.2±1123.9	59.488±0.143
44710.3±770.9	32.839±3.227	89264.8±1232.1	60.685±0.045
44718.2±745.2	47.627±4.247	89294.3±1211.2	89.764±0.128
Table 2: Objective score ], optimal gap ] and inference time (in seconds) [ comparison of the
discrete clustering problem, with mean and standard deviation. The problem is to select k clustering
centers from m data points. GS-TopK and Homotopy GS-TopK surpasses all competing methods
with less time compared to general-purpose solvers and other one-shot methods.
EGN/SOTF-TopK/ours are one-shot solvers	objective J	k=30, m=500 optimal gap J	time J (sec)	objective J	k=50, m=800 optimal gap J	time J (sec)
kmeans (Macqueen, 1967)	3.020±0.200	0.214±0.049	0.053±0.074	2.905±0.159	0.233±0.042	0.085±0.164
kmeans++ (Arthur & Vassilvitskii, 2007)	2.854±0.166	0.169±0.043	0.042±0.007	2.693±0.101	0.174±0.027	0.504±0.234
SCIP 7.0 (faster)	4.641±1.880	0.377±0.288	71.738±13.192	5.450±0.674	0.587±0.046	218.426±57.530
SCIP 7.0 (slower)	4.470±1.918	0.348±0.295	118.068±48.055	5.258±1.018	0.552±0.146	243.919±54.118
Gurobi 9.0 (faster)	3.365±0.341	0.290±0.072	80.582±0.861	3.532±0.358	0.365±0.065	116.446±5.087
Gurobi 9.0 (slower)	2.453±0.142	0.033±0.042	125.589±0.606	3.364±0.268	0.335±0.055	214.360±3.785
Gurobi 9.0 (optimal)	2.365±0.063	0.000±0.000	314.798±116.858	2.221±0.041	0.000±0.000	648.213±194.486
EGN (efficient) (Karalias & Loukas, 2020)	3.032±0.195	0.217±0.048	0.830±0.308	2.865±0.138	0.223±0.036	0.988±0.140
EGN (accurate) (Karalias & Loukas, 2020)	2.795±0.140	0.152±0.035	123.559±12.278	2.815±0.124	0.209±0.034	191.091±13.141
SOFT-TopK (Xie et al., 2020)	2.719±0.136	0.129±0.037	97.147±6.565	2.600±0.104	0.145±0.029	120.835±7.298
SOFT-TopK (w/ sampling)	2.717±0.118	0.129±0.027	129.856±7.256	2.593±0.086	0.143±0.026	166.132±8.701
GS-TopK (ours)	2.420±0.072	0.023±0.009	76.534±6.321	2.283±0.050	0.027±0.008	120.689±2.405
Homotopy GS-TopK (ours)	2.418±0.076	0.022±0.010	103.742±4.778	2.273±0.047	0.023±0.007	158.400±3.498
4.2	Experimental Results
Table 1 and Table 2 report results on max covering and discrete clustering problems, respectively.
An interesting finding is that the classic algorithms are efficient but also very effective. However, if
we can afford a higher time budget, there seems no universal approach to improve the results of the
classic algorithms. In comparison, Gurobi and SCIP solvers can trade-off time for better accuracy,
but they are less effective than our learning methods when considering the objective scores under
the same time budget. Among one-shot learning methods, Erdos Goes Neural (Karalias & Loukas,
2020) is the only method that does not encode the constraint in the model output, and the experimen-
tal results suggest that it is appealing if we can encode the constraint in the model output. Directly
applying Gumbel sampling to SOFT-TopK seems either an effective improvement, and satisfying
results are achieved when exploiting the Gumbel re-parameterization trick and enabling gradient-
based optimization over the combinatorial objective score. We further validate the effectiveness of
Homotopy GS-TopK, where the objective scores are improved at the cost of more inference time.
Further Discussions. Our insight for the effectiveness of GS-TopK over other state-of-the-art one-
shot learning methods (Karalias & Loukas, 2020; Xie et al., 2020) is that it can provide a more
accurate estimation of the combinatorial objective score with discrete decision variables. Fig. 3
shows that our GS-TopK estimates a lower bound of the true objective score at the beginning 200
iterations while providing an accurate estimation afterward. Besides, the Homotopy version of our
algorithm can gradually reduce the variance and help converge to a better solution.
9
Under review as a conference paper at ICLR 2022
Reproducibility S tatement
The following efforts are made to ensure the reproducibility of this paper:
•	We provide a complete proof about our Lemmas and Theorems in the Appendix, see Ap-
pendix A;
•	We discuss the hyper-parameter configurations and the experiment setup in Sec 4.1;
•	We provide the implementation details on max covering problem in Sec. 3.3, and the im-
plementation details on discrete problem problem in Appendix B;
•	We discuss the way of generating random dataset in Sec 4.1;
•	The code will be made publicly available once this paper is accepted.
References
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein GAN. arXiv e-prints, art.
arXiv:1701.07875, January 2017.
David Arthur and Sergei Vassilvitskii. K-means++: The advantages of careful seeding. In Proceed-
ings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’07, pp.
1027-1035, USA, 2007. ISBN 9780898716245.
Yunsheng Bai, Hao Ding, Song Bian, Ting Chen, Yizhou Sun, and Wei Wang. Simgnn: A neu-
ral network approach to fast graph similarity computation. In Proceedings of the Twelfth ACM
International Conference on Web Search and Data Mining, pp. 384-392, 2019.
ZaIan Borsos, MojmM Mutny, and Andreas Krause. Coresets via bilevel optimization for continual
learning and streaming. Neural Info. Process. Systems, 2020.
Wei Chen, Xiaoming Sun, Jialin Zhang, and Zhijie Zhang. Network inference and influence maxi-
mization from samples. In Int. Conf. Mach. Learn., July 2021.
Xinyun Chen and Yuandong Tian. Learning to perform local rewriting for combinatorial optimiza-
tion. Neural Info. Process. Systems, 32:6281-6292, 2019.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Neural Info.
Process. Systems, pp. 2292-2300, 2013.
Marco Cuturi, Olivier Teboul, and Jean-Philippe Vert. Differentiable ranking and sorting using
optimal transport. In Advances in Neural Information Processing Systems, volume 32, pp. 6858-
6868, 2019.
Hanjun Dai, Bo Dai, and Le Song. Discriminative embeddings of latent variable models for struc-
tured data. In Int. Conf. Mach. Learn., pp. 2702-2711. PMLR, 2016.
Lu Duan, Haoyuan Hu, Yu Qian, Yu Gong, Xiaodong Zhang, Jiangwen Wei, and Yinghui Xu. A
multi-task selected learning approach for solving 3d flexible bin packing problem. In Proceedings
of the 18th International Conference on Autonomous Agents and MultiAgent Systems, pp. 1386-
1394, 2019.
Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In
ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.
Matthias Fey, Jan Eric Lenssen, Frank Weichert, and Heinrich Muller. SplineCNN: Fast geometric
deep learning with continuous b-spline kernels. In Comput. Vis. Pattern Recog., pp. 869-877,
2018.
Matthias Fey, Jan E Lenssen, Christopher Morris, Jonathan Masci, and Nils M Kriege. Deep graph
matching consensus. In Int. Conf. Learn. Rep., 2020.
Satoru Fujishige. Submodular Functions and Optimization. Elsevier, 1991.
10
Under review as a conference paper at ICLR 2022
Gerald Gamrath, Daniel Anderson, Ksenia Bestuzheva, Wei-Kun Chen, Leon Eifler, Maxime Gasse,
Patrick Gemander, Ambros Gleixner, Leona Gottwald, Katrin Halbig, Gregor Hendel, Christo-
pher Hojny, Thorsten Koch, Pierre Le Bodic, Stephen J. Maher, Frederic Matter, Matthias Mil-
tenberger, Erik Muhmer, Benjamin Muller, Marc E. Pfetsch, Franziska Schlosser, FeliPe Serrano,
Yuji Shinano, Christine Tawfik, Stefan Vigerske, Fabian Wegscheider, Dieter Weninger, and Jakob
Witzig. The SCIP OPtimization Suite 7.0. Technical rePort, OPtimization Online, March 2020.
URL http://www.optimization-online.org/DB_HTML/2020/03/7705.html.
Aditya Grover, Eric Wang, Aaron Zweig, and Stefano Ermon. Stochastic oPtimization of sorting
networks via continuous relaxations. In Int. Conf. Learn. Rep., 2019.
Gurobi OPtimization, LLC. Gurobi OPtimizer Reference Manual, 2021. URL https://www.
gurobi.com.
William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive rePresentation learning on large
graPhs. Neural Info. Process. Systems, 2017.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reParameterization with gumbel-softmax. In
Int. Conf. Learn. Rep., 2017.
Nikolaos Karalias and Andreas Loukas. Erdos goes neural: an unsuPervised learning framework for
combinatorial oPtimization on graPhs. In Neural Info. Process. Systems, 2020.
Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial oPti-
mization algorithms over graphs. In Neural Info. Process. Systems, pp. 6351-6361, 2017.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic oPtimization. Int. Conf. Learn.
Rep., Dec 2014.
Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. arXiv e-prints, art.
arXiv:1312.6114, December 2013.
Wouter Kool, Herke van Hoof, and Max Welling. Attention, learn to solve routing problems! In Int.
Conf. Learn. Rep., pp. 1-25, 2019.
Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, and Pushmeet Kohli. Graph matching net-
works for learning the similarity of graph structured objects. In International Conference on
Machine Learning, pp. 3835-3845, 2019.
Baoding Liu. Facility Location Problem, pp. 157-165. Springer Berlin Heidelberg, Berlin, Heidel-
berg, 2009. ISBN 978-3-540-89484-1. doi:10.1007/978-3-540-89484-LIL
Chang Liu, Runzhong Wang, Zetian Jiang, and Junchi Yan. Deep reinforcement learning of graph
matching. arXiv preprint arXiv:2012.08950, 2020.
Hao Lu, Xingwen Zhang, and Shuang Yang. A learning-based iterative method for solving vehicle
routing problems. In Int. Conf. Learn. Rep., 2019.
J. Macqueen. Some methods for classification and analysis of multivariate observations. In In 5-th
Berkeley Symposium on Mathematical Statistics and Probability, pp. 281-297, 1967.
Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. Int. Conf. Learn. Rep., 2017.
Hongzi Mao, Malte Schwarzkopf, Shaileshh Bojja Venkatakrishnan, Zili Meng, and Mohammad
Alizadeh. Learning scheduling algorithms for data processing clusters. In Proceedings of the
ACM Special Interest Group on Data Communication, pp. 270-288, 2019.
G. Mena, D. Belanger, S. Linderman, and J. Snoek. Learning latent permutations with gumbel-
sinkhorn networks. Int. Conf. Learn. Rep., 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
11
Under review as a conference paper at ICLR 2022
Giorgio Patrini, Rianne van den Berg, Patrick Forre, Marcello Carioni, Samarth Bhargav, Max
Welling, Tim Genewein, and Frank Nielsen. Sinkhorn autoencoders. In Amir Globerson and
Ricardo Silva (eds.), Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial In-
telligence, UAI 2019, pp. 253, 2019.
Max B Paulus, Dami Choi, Daniel Tarlow, Andreas Krause, and Chris J Maddison. Gradient esti-
mation with stochastic softmax tricks. Neural Info. Process. Systems, 2020.
Yun Peng, Byron Choi, and Jianliang Xu. Graph learning for combinatorial optimization: A survey
of state-of-the-art. Data Science and Engineering, 6(2):119-141, 202LISSN 2364-1185, 2364-
1541. doi: 10.1007/s41019-021-00155-3.
Felix Petersen, Christian Borgelt, Hilde Kuehne, and Oliver Deussen. Differentiable sorting net-
works for scalable sorting and ranking supervision. In International Conference on Machine
Learning (ICML), 2021.
R. Santa Cruz, B. Fernando, A. Cherian, and S. Gould. Visual permutation learning. Trans. Pattern
Anal. Mach. Intell., 41(12):3100-3114, 2018.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
R. Sinkhorn and A. Rangarajan. A relationship between arbitrary positive matrices and doubly
stochastic matrices. Ann. Math. Statistics, 1964.
Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Neural Info. Process.
Systems, pp. 2692-2700, 2015.
R. Wang, J. Yan, and X. Yang. Learning combinatorial embedding networks for deep graph match-
ing. In Int. Conf. Comput. Vis., pp. 3056-3065, 2019.
Runzhong Wang, Zhigang Hua, Gan Liu, Jiayi Zhang, Junchi Yan, Feng Qi, Shuang Yang, Jun Zhou,
and Xiaokang Yang. A bi-level framework for learning to solve combinatorial optimization on
graphs. In Neural Info. Process. Systems, 2021a.
Runzhong Wang, Junchi Yan, and Xiaokang Yang. Neural graph matching network: Learning
lawler’s quadratic assignment problem with extension to hypergraph and multiple-graph match-
ing. Trans. Pattern Anal. Mach. Intell., 2021b.
Lin Xiao and Tong Zhang. A proximal-gradient homotopy method for the sparse least-squares
problem, 2012.
Yujia Xie, Hanjun Dai, Minshuo Chen, Bo Dai, Tuo Zhao, Hongyuan Zha, Wei Wei, and Tomas
Pfister. Differentiable top-k with optimal transport. In Neural Info. Process. Systems, volume 33,
pp. 20520-20531. Curran Associates, Inc., 2020.
Yi Xu, Yan Yan, Qihang Lin, and Tianbao Yang. Homotopy smoothing for non-smooth problems
with lower complexity than o(1/). Neural Info. Process. Systems, 2016.
Tianshu Yu, Runzhong Wang, Junchi Yan, and Baoxin Li. Learning deep graph matching with
channel-independent embedding and hungarian attention. In Int. Conf. Learn. Rep., 2020.
Cong Zhang, Wen Song, Zhiguang Cao, Jie Zhang, Puay Siew Tan, and Xu Chi. Learning to dispatch
for job shop scheduling via deep reinforcement learning. Neural Info. Process. Systems, 33, 2020.
12
Under review as a conference paper at ICLR 2022
A Proof of Theorems
Before starting the detailed proof of the lemmas and theorems, firstly we recall the notations used in
this paper:
•	T* = ToPK(D) is the optimal solution of the integer linear programming form of the OT
problem Eq. (4), which is equivalent to the solution by firstly sorting all items and then
select the TopK items;
•	TT* = Sinkhorn(D) is the optimal solution of the entropic regularized form of the OT
problem Eq. (5) solved by Sinkhorn algorithm. It is also the output by SOFT-TopK (Xie
et al., 2020);
•	Tσ* = ToPK(D) is the optimal solution of the integer linear programming form of the OT
problem after disturbed by the Gumbel noise Eq. (11), which is equivalent to the solution
by firstly adding the Gumbel noise, then sorting all items and finally select the TopK items;
〜
Sinkhorn(De ) is the optimal solution of the entropic regularized form of the OT
problem after disturbed by the Gumbel noise Eq. (13) solved by Sinkhorn algorithm. It is
also the output by our proposed GS-TopK.
Lemma 3. Given real numbers xi, xj, and ui, uj are from i.i.d. (0, 1) uniform distribution. After
Gumbel perturbation, the probability that xi + gσ (ui) > xj + gσ (uj ) is:
P(Xi + gσ(Ui) >xj+ gσ(Uj-+ex]y
(21)
Proof. Since gσ(Ui) = -σ log(- log(Ui)), P(xi + gσ(Ui) > xj + gσ (Uj)) is equivalent to the
probability that the following inequality holds:
And we have
xi - σ log(- log(Ui)) > xj - σ log(- log(Uj))
xi	xj
xi - xj
σ
Xi-Xj
e -σ-
>	σlog(-log(Ui)) - σ log(- log(Uj))
>	log ( R )
log(Uj)
>	log(Ui)
log(Uj)
Since Uj ∈ (0, 1), log(Uj) < 0. Then we have
Xi-Xj
log(uj) < log(ui)e	L
log (Uj ) < log Uiexp
xi -xj
exp 一
Uj < Ui	σ
(22)
(23)
(24)
(25)
(26)
(27)
(28)
Since Ui , Uj are i.i.d. uniform distributions, the probability when the above formula holds is
1 exp — 三j,
dUj dUi = Ui σ dUi
0
1
1 + exp - Xi-Xj
(29)
Thus the probability that xi + gσ(Ui) > xj + gσ(Uj) after Gumbel perturbation is:
P(xi + gσ (Ui ) > xj + gσ (Uj ))
1
1 + exp - Xi-Xj
(30)
□
13
Under review as a conference paper at ICLR 2022
A.1 Proof of Lemma 1
Proof of Lemma 1. If one of the top-k items becomes smaller than xk+1 after distorted by the Gum-
bel noise, then ||T* - Tσ* ∣∣f increases at most by 2. Without loss of generality, here We assume Xi
as one of the top-k items i.e. i ≤ k. According to Lemma 3, the probability that xi becomes smaller
than xk+1 after perturbation is
P(Xi + gσ (Ui) < xk+1 + gσ (uk +1)) = 1 +	xk+1-x~
(31)
Similarly, if one of the last-(m - k) items becomes larger than Xk after distorted by the Gumbel
noise, then ||T* - Tσ* ∣∣f also increases at most by 2. Without loss of generality, here we assume
Xj as one of the last-(m - k) items i.e. j ≥ k + 1. According to Lemma 3, the probability that Xj
becomes larger than Xk after perturbation is
P(Xj+ gσ(Uj) >χk+ gσ(Uk-+exp1— —
To compute the expectation of ||T* - Tσ*∣∣, we sum all probabilities and we have
km
Eu 1||T* - Tσ* ||2 ≤ X-+ X -----------
U	Fj - Jl + exp - χk+-χ	,	1 +exp - jxk
i=1	σ	j=k+1	p σ
(32)
(33)
where the constant 4 appears because ||T* - Tσ*∣∣F will change at most by 4 if one more item
crosses the boundary. Since for all i ≤ k and j ≥ k + 1, we have
Xk+1 - Xi ≤ Xk+1 - Xk
Xj - Xk ≤ Xk+1 - Xk
(34)
(35)
Thus we have
Since Eu |||T*
k
Eq. (33) ≤ X
i=1
m
X
i=1
4
1+exp - xk+σ-xk
4
1+exp - xk+σ-xk
4m
1 + exp lxk-χk+1l
-Tσ”F] ≤ Je“ [||T* - Tσ*∣∣Fi
m
+X
j=k+1
4
1 + exp - xk+σ-xk
, we have
Eu |||T*- Tσ”F]≤ j"，
(36)
(37)
(38)
(39)
□
A.2 Proof of Lemma 2
Proof of Lemma 2. By disturbing X with i.i.d. Gumbel noise, we have
X0 = X1 + gσ(U1),X2 + gσ(U2),X3 + gσ(U3), ..., Xm + gσ(Um)	(40)
where gσ(U) = -σ log(- log(U)) is the Gumbel noise modulated by noise factor σ, and
U1, U2, U3, ..., Um are i.i.d. uniform distribution. We define π as the permutation of sorting X0 in
descending order, i.e. Xπ1 + gσ (Uπ1 ), Xπ2 + gσ (Uπ2 ), Xπ3 + gσ (Uπ3 ), ..., Xπm + gσ (Uπm ) are in
descending order.
14
Under review as a conference paper at ICLR 2022
Recall Theorem 2 in (Xie et al., 2020), for x1 ,χ2,χ3, ...,χm we have
||T* - Tτ*∣∣F ≤
2mτ log 2
|xk - xk+11
(41)
By substituting X with X0 and taking the expected value, we have
Eu [∣∣Tσ* - T*||f]
2mτ log 2
|xπk + gσ (Unk ) - x∏k+ι - gσ (uπk+1 ) |
(42)
Based on Lemma 3, the probability that ∏k = i, ∏k+ι = j is
P (∏k = i,∏k+ι = j)
----1___X(Y------1___ π ------
1+exP - V 2∀ U 1 + exp - xπσ-xi b=k+2 1+exp - j^J
(43)
where the first term denotes Xi + gσ (Ui) > Xj + gσ (Uj), the second term denotes all conditions that
there are (k - 1) items larger than Xi + gσ (Ui) and the rest items are smaller than Xj + gσ(Uj).
In the following we derive the upper bound of EU ∣----------------------------------q- . We denote
∖xπk +gσ (Unk )-xπk + 1 -gσ (Unk+1 ) ∖
Ai,j as
Ui,Uj ∈ Ai,j,	s.t. Xi + gσ(Ui) - Xj - gσ(Uj) > e	(44)
where e is a sufficiently small number. Then we have
1
|Xnk + gσ (Unk ) - Xnk+1 - gσ (Unk+1)|
1
EP(nk = i, πk + 1 = j ) Eui,Uj ∈Ai,j
i=j
」Xi + 9σ(Ui) - Xj - gσ(Uj)|_
k-1
∏
α=1
1
1+exp - Xna；Xi
m
∏
b=k+2
x j - xπ b
1 + exp ——J σ
」Xi + gσ(Ui) - Xj - gσ(Uj)|_
X (—1— X ∏ —1—
Xi -Xj	Xπ -Xi
M v + exp -―匕 \ 号 1 + exp -十L
鲁	1
11 x^^'	Xj-χ∏b
b=k+21 + eχP - jτ-b
，Uj∈Ai,j [|Xi - Q log(- log(Ui)) - Xj + σlog(- log(Uj))|
∑	f (Xi -
i=j \
XX, σ, Z) X m 1+exp - -i b=Π+2 1 + exp - jxπb ))
We denote f (δ, σ, z) as:
f (δ, σ, z)
1
1 + exp - ⅛
1
|δ - σlog(- log(Ui)) + σlog(- log(Uj))|
s.t. δ - σ log(- log(Ui)) + σ log(- log(Uj)) > z > 0
(45)
(46)
(47)
(48)
(49)
(50)
X-
^^Ui ,uj ∈Ai,j
1
1
For the probability terms in Eq. (48), for all permutations π, there must exist πα, πb, such that
11
--------≤--------
1 + exp - xπa-xi — 1 + exp - xk-xi
1	1
x j-x∏b	xj-xk + 1
1 + exp ——J σ	1 + exp J σ +
(51)
(52)
15
Under review as a conference paper at ICLR 2022
Thus we have
Eq. (48) ≤
i6=j
-xj.σ. Z)-------1---------------1--------
ι	1 + exp - Xk-Xi 1 + exp - Xj -Xk+1
≤
i6=j
_________f(xi - Xj ,σ, z)_______
(1 + exp X-Xk)(1 + exp Xk+；-Xj)
(53)
(54)
By transforming Eq. (21) in Lemma 3, and substituting xj - xi by y, we have
Eq. (21) ⇒P(gσ (Ui ) - gσ (Uj ) > Xj - Xi)= 】 +	Xi-Xj
⇒P (gσ(ui ) - gσ (uj ) > y)
1
1 + exp y
⇒P(gσ(Ui)- gσ(Uj) <y) =1 - 1+⅛y = 1+e：p-y
(55)
(56)
(57)
where the right side is the form of the cumulative distribution function (CDF) of standard Logistic
distribution by setting σ = 1:
1
CDF(y)
1 + exp (-y)
(58)
Thus - log(- log(Ui)) + log(- log(Uj)) is equivalent to the Logistic distribution whose probability
density function (PDF) is
dCDF(y)
PDF⑻=」
1
exp(-y) + exp y + 2
(59)
and in this proof we exploit an upper bound of PDF(y):
PDF(y)
1
exp(-y) +exp y + 2
/	1
≤ y2 +4
(60)
Based on the Logistic distribution, we can replace -σ log(- log(Ui)) + σ log(- log(Uj)) by σy
where y is from the Logistic distribution. Thus we can derive the upper bound of f(δ, σ, z) as
16
Under review as a conference paper at ICLR 2022
follows
1
1+exp - δ
1
1+exp- δ
1
1+exp - ⅛
1
1+exp - ⅛
σ+z δ+σy exp (-y)+exp y+2
________1_______
σ+z exp (-y)+exp y+2
1__________1_______
σ+z δ+σy exp(-y)+exp y+2
1__________1_______
σ+z δ+σy exp (-y)+exp y+2
dy
dy
dy
1-
_______1______
1+exp (δ/σ-z)
dy
exp (δ/σ-z)
1+exp (δ/σ-z)
1__________1______
σ+z δ+σy exp(-y)+exp y+2
_______1_______
1+exp (-δ∕σ+z)
dy
1+exp(-δ + Z) /∞	_J___________1_______
1 + exp - δ	J-δ∕σ+z δ + σy exp(-y) + exp y + 2
1+exp(-δ + Z) /∞	_J______d
1 + exp - δ	J-δ∕σ+z δ + σyy2 +4 y
(61)
(62)
(63)
(64)
(65)
(66)
f (δ, σ,z)
≤
1 + exp (-δ + z) 2σ log ((zσ — δ)2 + 4σ2) — 2δ arctan (z-j∕σ) — 4σ log Z + πδ
--------------n--- • ------------------------TTT-------7^------------------------
1 + exp - δ	4δ2 + 16σ2
"	(67)
1 + exp (-δ + z) 2σ log ((zσ + ∣δ∣)2 + 4σ2) — 2δ arctan (z-g∕σ) — 4σ log Z + πδ
•
1 + exp - δ	4δ2 + 16σ2
"	(68)
1 + exp (-δ + z) 2σ log ((zσ + ∣δ∣)2 + 4σ2) — 2δ arctan ( Zfσ ) — 2σ log z2 + πδ
--------------n--- --------------------------------------------------------------
1 + exp - δ	4δ2 + 16σ2
"	(69)
_ 1+exp(-δ + z) 2σ log∣		[zσ+l驾2+4σ2) - 2δarctan (z≠) + πδ	(70)
	C 1 + exp - ⅛	4δ2 + 16σ2	
≤	1 + exp ( — δ + z) 2σ log I	((Zσ+lT+2σ)2) - 2δ arctan (z-∣^) + πδ	/71 ʌ
	C*	♦ 1 + exp - ⅛	4δ2 + 16σ2	(71)
	1+exp(-δ + z) 4σlog I	(zσ+lzl+2σ) - 2δ arctan (Z-^) + ∏δ	
	-1 + exp - ⅛	4δ2 + 16σ2	(72) (72)
	1+exp(-δ + z) 4σlog I	(zσ+lδl+2σ) + δ (π - 2arctan (Z-^))	
	-1 + exp - ⅛	4δ2 + 16σ2	(73) (73)
≤	1+exp(-δ + z) 4σlog I	(zσ+lzl+2σ) + ∣δ∣ (π - 2arctan (z≠))	
		n	 +		 1 + exp - ⅛	4δ2 + 16σ2	(74) (74)
≤	1+exp(-δ + z) 4σlog I	(zσ+lz+2σ)+ ∣δ∣ (π - 2 arctan (一券))	
	C*	♦ 1 + exp - ⅛	4δ2 + 16σ2	(75)
	1+exp(-δ + z) 4σlog I	(J+σ)+ ∣δ∣ (∏ + 2arctan (金))	∏f∖
	C*	♦ 1 + exp - ⅛	4δ2 + 16σ2	(76)
17
Under review as a conference paper at ICLR 2022
where Eq. (66) is because exp(-y)+exp y+2 ≤ y⅛ ,and Eq. (74) is because π - 2 arctan( z—烂)≥
0. With probability (1 - ), we have
Z = log 1 + e exp δ
1—e
1+exp(-J + Z)
1+exp-δ
≥ - log (1 - )
(77)
(78)
1
1 — e
Thus
f(δ,σ,z) ≤ Eq. (76)
1 4σlog (zσ+"+2σ) + ∣δ∣ (∏ + 2 arctan (2σ))
1 — e	4δ2 + 16σ2
< ι 4σ log (σ - ι0gl+-σJ + |s|(n + 2arctan (2σ))
≤ 1 — e	4δ2 + 16σ2
Thus we have
/ 4σ log (σ -kXj-+2σ ) + E - Xj | (π + 2 arctan (X-Xj))
Eq. (54) ≤ V ----------ʌ---------—L----------------ʌ------------ʌ----/ɪ
匕 (1 — e)(4(xi — Xj)2 + 16σ2)(1 + exp Xi-Xk)(1 + exp xk+σ xj)
(79)
(80)
(81)
In conclusion, with the probability (1 — e), we have
Eu [∣∣Tσ* — T*||f] ≤X
i6=j
(2 log 2)mτ (4σ log (σ — ⅛gXj⅛∣σ) + E — Xj | (∏ + 2 arctan X-Xj))
Σ
i6=j
(1 — e)(4(Xi — Xj)2 + 16σ2)(1 + exp Xi-Xk )(1 + exp xk+σ-xj)
(82)
(log2)mτ (2σlog (σ — ⅛-X⅜⅛∣σ) + ∣Xi — Xj| (∏ + arctan X-Xj))
(1 — e)((Xi — Xj)2 + 4σ2)(1 + exp Xi-Xk )(1 + exp xk+σ-xj
)
(83)
= (log 2)mτ ɪ2 Ω(xi, Xj, σ, e)
i6=j
(84)
And we denote Ω(xi, xj-, σ, e) as
Ω(xi, Xj, σ, e)
2σ log (σ — ⅛-Xj+2σ) + ∣Xi — Xj |(2 + arctan
(1 — e)((xi — Xj)2 + 4σ2)(1 + exp xi-xk )(1 + exp
Xi-Xj
2σ )
xk + 1 -Xj)
σ J
(85)
□
Finally, Theorem 1 is proved by triangle inequality by jointly considering Lemma 1 and Lemma 2.
A.3 Remarks w.r.t. SOFT-TopK
In the following, we add some remarks about the relationship between our conclusion of GS-TopK
and the conclusion derived by the authors of SOFT-TopK (Xie et al., 2020): the SOFT-TopK is a
special case of our proposed algorithm when we set σ = 0. We have the following conclusion:
Theorem 2. Assume the values of Xk, Xk+1 are unique4, under probability (1 — e), we have
lim Eu |||T* - T*||fi ≤	(1og2)mτ	(86)
______________________σ→0+	(1 — e)|Xk - Xk + 1 |
4For the ease of a compact proof, we make this assumption that the values of xk, xk+1 are unique. If there
are duplicate values of xk, xk+1, the bound only differs by a constant multipler therefore does not affect our
conclusion: SOFT-TopK (Xie et al., 2020) is a special case of our proposed approach when σ = 0.
18
Under review as a conference paper at ICLR 2022
Condition 1
・ ∙ ∙ ∙ ∙
Xi	Xk	Xk+1	Xj
Condition 2
■ ∙ ∙ ∙ ∙
Xj	Xk	Xk+1	Xi
Condition 3
■	∙^"∙	∙	∙
Xi Xj	Xk	Xk+1
Condition 4
・ M∙^-∙
Xk	Xk+1	Xi	Xj
Figure 4: Four conditions considered in our proof. It is worth noting that xi, xj must not lie between
xk, xk+1, because we define xk, xk+1 as two adjacent items in the original sorted list.
which differs from the conclusion in (Xie et al., 2020) by only a constant factor.
Proof. Since σ → 0+, the first term becomes 0. For the second term, we discuss four conditions as
shown in Fig. 4, except for the following condition: xi = xk, xj = xk+1.
Condition 1.	If xi ≥ xk , xj ≤ xk+1 (equalities do not hold at the same time), we have at least
xi - xk > 0 or xk+1 - xj > 0. Then we have
σ→0+ (1 + exp xi-σxk)(1 + exp xk+；-xj)
⇒ lim Ω(xi,xj,σ, e)=0
σ→0+
(87)
(88)
Condition 2.	For any case that xi < xj , we have xi - xj < 0, thus
lim arctan Xi——Xj = 一 π	(89)
σ→0+	σ	2
⇒ lim π + arctan Xi——Xj = 0	(90)
σ→0+ 2	σ
⇒ lim Ω(xi, Xj, σ, e) = 0	(91)
σ→0+
Condition 3.	If Xi ≥ Xj ≥ Xk (equalities do not hold at the same time), we have Xi 一 Xk > 0. Then
we have
1.	1	n
lim --------------= 0
σ→0+ 1 + exp Xi-Xk
⇒ lim Ω(xi,Xj,σ,e) = 0
σ→0+
(92)
(93)
Condition 4.	If Xk+1 ≥ Xi ≥ Xj (equalities do not hold at the same time), we have Xk+1 一 Xj > 0.
Then we have
1.	1	n
lim ---------------= 0
σ→0+ 1 + exp xk+σ-xj
⇒ lim Ω(xi,Xj,σ,e) = 0
σ→0+
(94)
(95)
19
Under review as a conference paper at ICLR 2022
Therefore, if Xi = Xk and Xj = xk+i, the second term Ω(xi,xj,σ, e) degenerates to 0 when
σ → 0+. Thus we have the following conclusion by only considering xi = xk, xj = xk+1:
lim Eu|||T* - T*∣∣f ≤
σ→0+
≤
(log2)mτ (|xk - Xk+i|(2 + arctan Xk£+1))
(1 一 e)(xk 一 Xk+i)2
(π log 2)mτ
(1 - e)|Xk - Xk+1|
(96)
(97)
□
B Implementation Details on Discrete Clustering Problem
For the discrete clustering problem, recall that we aim to select a subset of objects as cluster centers.
It differs from the k-means clustering problem whose cluster centers may not have to be a subset
of the objects, and the constraint for discrete clustering is more welcomed for real-world facility
location planning applications. Denote ∆ ∈ R+m×m as the distance matrix for each pair of objects.
Following our implementation of the max covering problem, we can also derive a GPU-friendly
formulation of the discrete clustering problem using PyTorch syntax:
min sum(min(∆[x, :], dim = 0)) s.t. x ∈ {0, 1}m, ||X||0 ≤ k	(98)
x
We also model the discrete clustering problem as a graph and encode the problem structure by a
graph neural network. Specifically, for points that lie on the 2D plane, we define there is an edge
between two points if their distance is smaller than a predefined threshold. Then the geometric
information is encoded by 3-layer SplineCNN (Fey et al., 2018) which was found successful for
geometric feature learning. Similar to the max covering problem, we rewrite the GPU-friendly
form of the objective score in a differentiable manner, being estimated by the output of GS-TopK
algorithm:
Ji = Sum(Softmax(—T∆[x,:], dim = 0, keepdim = True) Θ ∆[x,:])
T	∕Γ V V V 1∖
J = mean([Ji, J2, ..., J#G])
(99)
where we use a softmax operator with temperature T to approximate the min operator in a differ-
entiable way. Θ means the tensor-product operator in PyTorch. The GS-TopK learning algorithm is
summarized in Alg. 3.
C	Experiment Setup
Our algorithms are implemented by PyTorch and the graph neural network modules are based
on (Fey & Lenssen, 2019). In our paper, we optimize the hyperparameters by greedy search
on a small subset of problem instances (〜5) and set the best configuration of hyperparam-
eters for both GS-TopK, SOFT-TopK. For the max covering problem, we empirically set
learning rate = 0.1, τ = 0.05, σ = 0.15 for GS-TopK, τ = 0.05 for SOFT-TopK, and (τ, σ) =
(0.05, 0.15), (0.04, 0.10), (0.03, 0.05) for Homotopy GS-TopK. For the discrete clustering problem,
we set learning rate = 0.1, τ = 0.05, σ = 0.25 for GS-TopK, τ = 0.05 for SOFT-TopK, and we set
(τ, σ) = (0.05, 0.25), (0.04, 0.15), (0.03, 0.05) for Homotopy GS-TopK. We set #G = 1000 for
max covering, #G = 500 for discrete clustering. All experiments are done on a workstation with
i7-9700K@3.60GHz, 16GB memory, and 2080Ti GPU.
D Ablation Study on Hyperparameters
Firstly, we want to add some remarks about the selection of hyperparameters:
• #G (number of Gumbel samples): #G affects how many samples are taken during train-
ing and inference for GS-TopK. A larger #G (i.e. more samples) will be more appealing,
20
Under review as a conference paper at ICLR 2022
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Algorithm 3: Gumbel-Sinkhorn-TopK Learning for Solving the Discrete Clustering Prob-
lem
Input: the distance matrix ∆; learning rate α; Softmax temperature T; GS-ToPK parameters
k, τ , σ, #G.
if Training then
L Randomly initialize neural network weights θ;
if Inference then
Load a pretrained neural network weights θ; Jbest = +∞;
while not converged do
s = SplineCNNθ(∆); [T1, T2, ..., T#G] = GS-TopK(s, k, τ, σ, #G);
for all i,
二	Z	，一 .	「二一、、一	一，	一	一	、	,「：「一、、、
Ji = sum(softmax(-T ∆[Ti [2, :], :], dim = 0, keepdim = T rue) ∆[Ti [2, :], :]);
T	∕rT T T 1、.
J = mean([J1, J2, ..., J#G]);
if Training then
|_ update θ with respect to the gradient J and learning rate α by gradient descend;
if Inference then
update S with respect to the gradient dJ and learning rate α by gradient descend;
r∙ 11 ■	E τr Zm rrʌ T ∖ A	T	∙	/ [ ^^Γ	^^Γ Λ	∖
L for all i, Ji = ToPK(Ti[2,:])A ∙ v; Jbest = mm([Jι, J2,…,J#g], Jbest);
if Homotopy Inference then
L Decline the values of τ, σ and jump to line 5;
Output: Learned network weights θ (if training)/The best objective Jbest (if inference).
because GS-TopK will have a more accurate estimation of the objective score, and it will
have a higher probability of discovering better solutions. However, #G cannot be arbitrar-
ily large because we are running GS-TopK in parallel on GPU, and the GPU has limited
memory. Therefore, in experiments, we set a large enough #G (e.g. #G = 1000) and
ensure that it can fit into the GPU memory of our workstation (2080Ti, 11G).
•	τ (entropic regularization factor of Sinkhorn): Theoretically, τ controls the gap of the
continuous Sinkhorn solution to the discrete solution, and a smaller τ will lead to a tight-
ened gap. This property is validated by our theoretical findings in Lemma 2. Unfortunately,
τ cannot be arbitrarily small, because a smaller τ requires more Sinkhorn iterations to con-
verge. Therefore, given a fixed number of Sinkhorn iterations (100) to ensure the efficiency
of our algorithm, we need trial-and-error to discover the suitable τ for both SOFT-TopK
and GS-TopK. The grid search results below show that our selection of τ fairly balances
the performances of both SOFT-TopK and GS-TopK.
•	σ (Gumbel noise factor): As derived in Theorem 1, we need to balance between the two
gaps (Lemma 1 and Lemma 2) by a suitable σ. Since |xk - xk+1| is unknown, we cannot
directly find the theoretically optimal σ given τ. In the experiments, we firstly determine a
T, and then find a suitable σ by greedy search on a small subset (〜5) of problem instances.
We conduct an ablation study about the sensitivity of hyperparameters by performing an exten-
sive grid search near the configuration used in our max covering experiments (τ = 0.05, σ =
0.15, #G = 1000). We choose the k=50, m=500, n=1000 max covering problem, and we have the
following results for GS-TopK (ours) and SOFT-TopK (Xie et al., 2020) (higher is better):
Table 3: Ablation study result of GS-TopK with #G = 1000.
	0.01	0.05	0.1
0.1	42513.4	44759.2	45039.5
0.15	41456.5	44713.2	44837.2
0.2	41264.3	44638.1	44748.2
21
Under review as a conference paper at ICLR 2022
Table 4: Ablation study result of GS-TopK with #G = 800.
X=	0.01	0.05	0.1
0.1	42511.6	44754.6	45037.6
0.15	41421.4	44705.8	44841.5
0.2	41235.9	44651.5	44748.6
Table 5: Ablation study result of SOFT-TopK.
	0.001	0.005	0.01	0.05	0.1
τ= objective score	35956.6	42013.3	42520.8	41004.3	40721.2
Under the configuration used in our paper, both SOFT-TopK and GS-TopK have relatively good
results. By grid search, we also discover better hyperparameters for both GS-TopK and SOFT-
TopK, but it is worth noting that SOFT-TopK is still inferior to GS-TopK with the best-discovered
hyperparameters. Our grid search result shows that our GS-TopK is not very sensitive to σ if we
have τ = 0.05 or 0.1, and the result of τ = 0.01 is inferior because the Sinkhorn algorithm may
not converge. The results of #G = 1000 are all better than #G = 800, suggesting that a larger
#G is appealing if we have enough GPU memory. It is also discovered that SOFT-TopK seems to
be able to accept a smaller value of τ compared to GS-TopK, probably because adding the Gumbel
noise will increase the divergence of elements thus performs in a sense similar to decreasing τ when
considering the convergence of Sinkhorn.
22