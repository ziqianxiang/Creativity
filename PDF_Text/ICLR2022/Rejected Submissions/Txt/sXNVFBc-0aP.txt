Under review as a conference paper at ICLR 2022
Public Data-Assisted Mirror Descent
for Private Model Training
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we revisit the problem of using public data to improve the pri-
vacy/utility trade-offs for differentially private (DP) model training. Here, public
data refers to auxiliary data sets that have no privacy concerns. We consider public
training data sets that are from the same distribution as the private training data.
For convex losses, we show that a variant of Mirror Descent provides population
risk guarantees which are independent of the dimension of the model (p). Specif-
ically, we apply Mirror Descent with the loss generated by the public data as the
mirror map, and using DP gradients of the loss generated by the private (sensitive)
data. To obtain dimension independence, we require G1 2Q ≤ p public data samples,
where GQ is the Gaussian width of the smallest convex set Q such that the pub-
lic loss functions are 1-strongly convex with respect to ∣∣ ∙ ∣∣q. We further show
that our algorithm has a natural “noise stability” property: Ifin a bounded region
around the current iterate, the public loss satisfies αv -strong convexity in a direc-
tion v, then using noisy gradients instead of the exact gradients shifts our next
iterate in the direction V by an amount proportional to 1 /αv (in contrast with DP
stochastic gradient descent (DP-SGD), where the shift is isotropic). Analogous
results in prior works had to explicitly learn the geometry using the public data in
the form of preconditioner matrices. Our method is also applicable to non-convex
losses, as it does not rely on convexity assumptions to ensure DP guarantees.
We demonstrate the empirical efficacy of our algorithm by showing privacy/utility
trade-offs on linear regression, deep learning benchmark datasets (WikiText-2,
CIFAR-10, and EMNIST). We show that our algorithm not only significantly im-
proves over traditional DP-SGD, which does not have access to public data, but
also improves over DP-SGD on models that have been pretrained with the public
data to begin with.
1 Introduction
Differentially Private Stochastic Gradient Descent (DP-SGD) (Song et al., 2013; Bassily et al.,
2014; Abadi et al., 2016), and its variants (Kairouz et al., 2021b) have become the de facto stan-
dard algorithms for training machine learning models with differential privacy (DP) (Dwork et al.,
2006). While DP-SGD is known to be optimal in terms of obtaining both optimal excess empirical
risk (Bassily et al., 2014), and excess population risk (Bassily et al., 2020b) for convex losses, the
obtained error guarantees suffer from an explicit polynomial dependence on the model dimension
(p). This polynomial dependence significantly impacts the privacy/utility trade-off when p ≥ npriv ,
where npriv is the number of private training samples. Thus, even empirically, when DP-SGD is
used to train large deep learning models, there is a significant drop in accuracy compared to the non-
private counterpart (Papernot et al., 2020). In this paper, we revisit the problem of effectively using
public data (i.e., data drawn from the same distribution as the private training data set, but without
privacy concerns) to improve the privacy/utility trade-offs for DP model training. Specifically, we
design a central DP variant of mirror descent (Nemirovsky & Yudin, 1983) that uses the loss function
generated by the public data as the mirror map and DP gradients on the private/sensitive data as
the linear term, ensuring population risk guarantees for convex losses with no explicit dependence
on dimensions as long as npub ≥ p, where npub is the number of records in the public data set.
1
Under review as a conference paper at ICLR 2022
We show both theoretically and empirically that our DP variant of mirror descent, assisted with
public data, can improve the privacy-utility trade-offs by effectively reducing the variance in the
noise added to the gradients in DP model training. Our empirical results are on standard benchmark
data sets like CIFAR-10, EMNIST, and WikiText-2.
Learning Geometry with Mirror Maps: Common to most DP model training algorithms, includ-
ing DP-SGD, DP-FTRL (Kairouz et al., 2021b), and our algorithm, is a DP estimator of the gradient
of the loss VθL(θt; DPriv) = Pd∈Dpriv ▽6'(θt; d) generated by the private data set DPriv at a given
model state θt ∈ Rp. This DP estimator essentially adds isotropic Gaussian noise N(0, σ2Ip)
to VθL(θt; DPriv), where σ depends on the privacy parameters (ε, δ) and the maximum allowable
value of ∣∣Vθ'(θt; d)∣b (a.k.a. the clipping norm (Abadi et al., 2016)).1 It is well known that for
most learning tasks, the set of gradients vectors in L(θt; DPriv) are seldom isotropic (Gur-Ari et al.,
2018; Agarwal et al., 2019). Hence, it is natural to wonder if the Gaussian noise in the DP estimator
can be made to respect the geometry of the gradients. Prior works (Zhou et al., 2020; Asi et al.,
2021; Kairouz et al., 2021a) have used public data (DPub) to explicitly learn this geometry, mostly
in the form of preconditioner matrices (Duchi et al., 2011) to be multiplied to the estimated noisy
gradients. In this paper, we take an implicit approach towards respecting this geometry, by using
the loss L(θ; DPub) generated by the public data as the mirror map in classical mirror descent. As a
first order approximation (formalized in Section 4), one can view itas doing DP-SGD on L(θ; DPriv)
while using L(θ; DPub) as a regularizer. This approach has the following advantages: (i) The infor-
mation of the geometry is “free”, i.e., one does not need to learn the preconditioner explicitly from
the public data, (ii) Unlike prior works (Zhou et al., 2020; Kairouz et al., 2021a), one does not need
to assume that the gradients of L(θ; DPriv) lie in a fixed rank subspace, (iii) The achieved excess
population risk guarantees have better dependence on nPub = |DPub| compared to prior results (Asi
et al., 2021), and (iv) Because the geometry is implicitly defined, the implementation does not need
to maintain an additional data structure for the preconditioner, and hence is much easier to imple-
ment. Empirically, under our best-effort comparison, our baseline algorithm improves over the state
of the art (Asi et al., 2021).2 We note that differentially private mirror descent has been considered
before by Talwar et al. (2014) and Wang et al. (2017). Their results are not directly comparable
to ours because (i) they do not have access to in-distribution public data (ii) as shown in Bassily
et al. (2014), without public data, it is impossible to achieve the dimension independent bounds
we achieve (iii) in our experiments we solve unconstrained optimization problems, but those works
choose the mirror map based on the constraint set rather than the data set. We note that the utility
bounds we prove in this paper also apply to a public data-assisted variant of the accelerated mirror
descent algorithm considered in Wang et al. (2017).
In-distribution vs. Out-of-distribution Public Data: Prior works have considered both settings
where the public data set DPub comes from the same distribution as the private data DPriv (a.k.a. in-
distribution) (Bassily et al., 2018a; Zhou et al., 2020; Kairouz et al., 2021a; Asi et al., 2021), and
where the distributions are different (a.k.a. out-of-distribution) (Abadi et al., 2016; Papernot et al.,
2016; 2018; Liu et al., 2021). In principle, our algorithm can be used in out-of-distribution settings,
but our results in this paper are for the in-distribution case. In the in-distribution setting, it is typical
that there are fewer public data samples available than private data samples 一 i.e., npub《npriv -
as it is harder to obtain public data sets than ones with privacy constraints attached. In-distribution
public data could come from either altruistic opt-in users (Merriman, 2014; Avent et al., 2017) or
from users who are incentivized to provide such data (e.g., mechanical turks). Out-of-distribution
public data may be easier to obtain but can have various degrees of freedom; e.g., the domains
of private and public data may not be identical, the representation of some classes may vary, the
distributions can be mean shifted, etc. It is usually hard to quantify these degrees of freedom to the
extent that we can provide precise guarantees. Hence, we leave this aspect for future exploration,
and work with the idealized assumption that the public data comes from the same distribution as the
private data, or, at least, that the differences between these two distributions are not material.
Design of Algorithms without Relying on Convexity for Privacy: While most of our theoretical
utility guarantees are specific to convex functions, our algorithm can be used seamlessly in non-
convex settings. The main reason is that its DP guarantee does not rely on convexity. Prior work
1For the ease of presentation, at this point we do not consider the noise due to stochastic mini-batching.
2The implementation of Asi et al. (2021) is not publicly available. Since the algorithms can be sensitive to
hyperparameter choices, for a fair comparison we directly consider the results quoted in (Asi et al., 2021).
2
Under review as a conference paper at ICLR 2022
that provided similar dimension-independent excess population risk guarantees under the the same
conditions as ours (i.e., npub ≥ p) (Feldman et al., 2018) heavily relied on convexity to ensure DP
and hence, cannot be used in non-convex settings.
Choice of Benchmark for Empirical Comparison: Mirror descent (Nemirovsky & Yudin, 1983;
Hazan, 2019) as a first step optimizes the mirror map function. In our setting, this corresponds to
pre-training on the public loss function L(θ; Dpub) before running the DP optimization procedure on
L(θ; Dpriv). Since pre-training on public data is intuitive and easy, we always compare to DP-SGD
(and its variants) that have been pre-trained to convergence with the public loss. We show that our
algorithm outperforms even pre-trained DP-SGD. To the best of our knowledge, ours is the only
empirical work that compares to this strong (but fair) benchmark.
Other Uses of Public Data in DP Learning: The use of in-distribution public data has been exten-
sively explored both theoretically and empirically. On the theoretical side, it has been shown (Alon
et al., 2019; Bassily et al., 2020a) that a combination of private and public data samples can yield
asymptotically better worst-case PAC learning guarantees than either on their own. Another line of
work (Papernot et al., 2016; 2018; Bassily et al., 2018b; Dwork & Feldman, 2018; Nandi & Bassily,
2020) considers public data that is unlabelled, but otherwise comes from the same distribution as the
private data; the primary goal is to use the private data to generate labels for the public data, which
can then be used arbitrarily. So far only two papers have considered out-of-distribution data. Bassily
et al. (2020c) assume that whether a data record is public or private depends on its label; e.g., the
public data may contain many negative examples, but few positive examples. They show that halfs-
paces can be learned in this model. Liu et al. (2021) consider synthetic data generation and provide
guarantees that depend on the Renyi divergences between the public and private distributions. Abadi
et al. (2016) and Tramer & Boneh (2020) provided techniques to effectively use out-of-distribution
public data for pre-training for DP-SGD. However, they did not consider techniques to improve a
pre-trained model using private and public data, which is the focus of our work.
1.1	Problem Formulation
Consider the classic differentially private stochastic convex optimization (DP-SCO) (Chaudhuri
et al., 2011; Bassily et al., 2014; 2019; 2020b) setting. Let τ be a distribution over a fixed do-
main D. Given a data set D ∈ D* drawn i.i.d. from T, and a loss function 'p^ : Rp ×D → R, the
objective is to approximately solve arg min Ed〜T ['priv(θ; d)], while preserving DP. Here, C ⊆ Rp
θ∈C
is the constraint set. Usually one solves the SCO problem via empirical risk minimization (ERM),
i.e., θpriv ∈ arg min L(θ; D), where L(θ; D)=击 P 'priv(θ; d), and then uses θpriv as a proxy.
θ∈C	d∈D
Up to a dependence on dimensionality p, in the DP setting, a direct translation from ERM to the
SCO setting provides the optimal rates (Bassily et al., 2014; 2019; 2020b). Since we will strive for
dimension-independent convergence rates, we thus safely ignore the issue of non-optimality due to
dimensionality, and treat the ERM and SCO problems as almost equivalent.
In this paper, we consider the DP-SCO setting with heterogeneous data, where there are two data sets
Dpriv (with npriv samples) and Dpub (with npub samples) drawn i.i.d. from the same distribution. The
private data set Dpriv consists of records from individuals who require privacy protection, whereas
the public data set Dpub does not require any privacy protection. Since obtaining such data can be
expensive, for our empirical evaluation, we assume npub is a small fraction of npriv (e.g., < 5%).
We will also use a separate public loss function `pub . For our theoretical analysis, we will assume
'priv is convex and L-Lipschitz, whereas 'pub is strongly convex (both with respect to the '2-norm).
In practice, one will likely choose `priv = `pub, but clip the gradients of `priv for privacy. In general,
'pub can be arbitrary, but for the analysis, we will assume that 'pub and 'priv roughly share a mini-
mizer and that V'pub(θ; d) has bounded variance (see Assumption 3.1 for a formal statement). We
refer the reader to Appendix A for a reference for the notation used throughout the paper.
1.2	Our Contributions
Dimension Independent Excess Population Risk for Constrained ERMs: The standard private
SCO excess empirical loss bound for a constraint set C is: O (L ∣∣C∣∣2 ∙ "；『(1/"
, where L is the
3
Under review as a conference paper at ICLR 2022
Lipschitz constant of the loss function and n is the number of samples in the dataset. If we pre-train
on public data and are able to find a solution within distance O (√1p) of the optimal solution, We
can set C to be the ball of radius O (√1p) centered on the pre-trained model and obtain a similar
dimension-independent error rate. There are a couple of issues with this approach: (i) It bakes in
the utility assumption about the constraint set into the algorithm. This is unsatisfactory because if
we are unlucky (due to stochasticity or choice of hyperparameters) and the optimal solution does
not lie within this ball, using this choice of C may restrict us to a range of poor solutions that we
cannot escape even if the number of private samples is very large, and (ii) It forces one to operate
with projected gradient descent which may significantly impact accuracy in settings where the utility
assumptions may not hold, e.g., in deep learning.
We improve upon this by showing that given npub public samples in addition to npriv private sam-
ples, using our PDA-DPMD (Public Data-Assisted Differentially Private Mirror Descent) algorithm,
a variant of mirror descent that implicitly reshapes the noisy gradients according to the public loss
function, we can achieve excess empirical loss O ∣ LV ∙ GQVZlog(I/δ) ∣. The bound is proven in
εnpriv npub
Theorem 3.2. Here, V is a measure of the variance of the gradients of `pub, and GQ is the Gaussian
width of the smallest set Q such that the public loss functions are 1-strongly convex with respect to
k ∙ ∣∣q. In particular, GQ is at most √p if the public loss functions are 1-strongly convex, but can
be constant if e.g. the public loss functions have a much larger strong convexity parameter in all but
a constant number of basis directions. Note that if we have npub = G2Q public samples, then the
1 / √npub and GQ terms cancel out, i.e. this error bound has no explicit dependence on dimension.
Using standard techniques, we can turn this into a dimension-independent excess population loss
bound (see Theorem 3.5), again assuming npub ≥ G2Q . In addition, to the best of our knowledge,
ours is the first work on augmenting private training with public data to show a theoretical improve-
ment over DP-SGD (here the dependence √=) due to pre-training on public data. In particular, we
show pre-training improves the standard DP-SGD bounds even under a totally isotropic geometry.
Local Noise-stability of Mirror Descent: We show that in addition to achieving better excess pop-
ulation loss bounds, PDA-DPMD has the following “local noise-stability” property: Ifin a bounded
region around the current iterate, the public loss satisfies αv -strong convexity in a direction v, then
using noisy gradients instead of the exact gradients shifts our next iterate in the direction v by an
amount proportional to 1∕αv (see Theorem 3.6). That is, PDA-DPMD effectively rescales the ef-
fect of adding noise in any direction to be inversely proportional to the curvature in that direction.
Note that this is in spite of the fact that for privacy, the noise we add to the gradients is usually
isotropic. Furthermore, this rescaling is done purely as a function of the public loss function used
by PDA-DPMD. In other words, a practitioner implementing PDA-DPMD simply needs to choose
an appropriate loss function, and PDA-DPMD will “automatically” rescale the effects of noise to
match the loss function’s curvature.
Empirical Evaluation: On both synthetic and real-world benchmark data sets, we show that PDA-
DPMD outperforms DP-SGD, even when they are pre-trained on the public data set. We provide
two sets of experiments. First, a linear regression on a synthetic data set which closely matches the
utility assumptions in the theoretical analysis. Second, we provide results on standard deep learning
benchmark data sets (WikiText-2, CIFAR-10, and EMNIST).
In Section 3, we consider using DP-SGD and PDA-DPMD to solve a least squares linear regression
problem on a synthetic data set generated via the process b 〜 N(hai, θ*i,σ2), where θ* ∈ Rp is
the true model. The feature vectors ai ’s are drawn i.i.d. from some fixed distribution. We fix the
number of private data samples, and set the number of public samples to be a fixed constant times
the dimension (p). We observe that as expected, public data allows us to substantially improve the
error in two ways: (i) Pre-training: DP-SGD initialized from a model pre-trained on public data
has nearly-constant mean-squared error, whereas DP-SGD from a random initialization has mean-
squared error scaling with the dimension, and (ii) Adapting to geometry using public loss: While
DP-SGD initialized from a pre-trained model already achieves near-constant loss, we also observe
that PDA-DPMD outperforms DP-SGD due to its error’s dependence on the Gaussian width GQ
rather than the dimension. We note that the observed improvement is “automatic” once we choose
the mean-squared error to be the loss function.
4
Under review as a conference paper at ICLR 2022
We also conduct experiments in Section 4 on two real world tasks: next word prediction on
WikiText-2, and image classification on CIFAR-10 and EMNIST (ByMerge split). We consider
4% of the original training data as public and pretrain on it. On all datasets, we can observe that
an approximate version of PDA-DPMD outperforms DP-SGD in terms of test loss. On CIFAR-10,
the improvement is more than 5%; on EMNIST, 7%; on WikiText-2, log perplexity is improved by
more than 0.3%, which is a notable improvement for perplexity.
For the deep learning experiments, running PDA-DPMD was computationally expensive. We derive
a first-order approximation that can be viewed as regular DP-SGD on a convex combination of pri-
vate and the public losses, i.e., αVL(θ; DPriv)+ (1 -α)VL(θ; Dpub), α ∈ [0,1], where VL(θ; DPriv)
is privatized by clipping and adding noise. This approximation makes the running time of our algo-
rithm comparable to DP-SGD when run on the data set Dpriv ∪ Dpub .
2	Background
Differential Privacy: Differential Privacy (DP) - originally defined by DWork et al. (2006) - is a
formal method for quantifying the privacy leakage from the output of a data analysis procedure. A
randomized algorithm M : D* → Y is (ε, δ)-DP if, for all neighbouring dataset pairs D, D0 ∈ D*
and all measurable sets of outputs S ⊆ Y, we have
P [M(D) ∈ S] ≤ eε ∙ P [M(D0) ∈ S] + δ.
We define two datasets to be neighbouring if they differ only by the addition or removal of one
person’s record. We ensure differential privacy by adding Gaussian noise to functions of bounded
sensitivity. In particular, if ' is L-LiPSchitz in its first parameter, then ∣∣Vθ'(θ; d)∣∣2 ≤ L for all θ
and d ∈ D. Thus adding noise drawn from N(0, σ2 ∙ Ip) to the sum P V'(θ, di) over people's
i
records satisfies DP, where σ scales with L and the desired privacy parameters. The composition
and postprocessing properties of differential privacy ensure that, as long as each step in our itera-
tive algorithm satisfies differential privacy, then so does the overall system. We refer the reader to
(Dwork & Roth, 2014) for further details of the standard privacy analysis of algorithms like ours.
Due to space constraints, we defer background on Mirror Maps and Gaussian Width to Appendix B.
3	Public Data Assisted Differentially Private Mirror Decent
In this section, we present our algorithm, PDA-DPMD, dimension-independent excess empirical and
population loss bounds for PDA-DPMD (Theorems 3.2 and 3.5), and a “noise stability” property
(Theorem 3.6). We defer all the proofs in this section to Appendix C.
Algorithmic Description: Our main algorithm, Public Data-Assisted Differentially Private Mirror
Descent (PDA-DPMD), is given as Algorithm 1. PDA-DPMD is a variant of mirror descent using
noisy gradients, but we also pre-train on public data and use the public loss as our mirror map Ψ.
Algorithm 1 Public Data-Assisted Differentially Private Mirror Descent (PDA-DPMD)
Input: Public/private datasets Dpub, Dpriv of sizes npub,npriv, PriVate/public loss functions 'priv,
`pub, privacy parameters (ε, δ), number of iterations T, learning rate η : {0, 1, . . . , T - 1} → R+,
constraint set: C, clipping norm L: an upper bound on max ∣∣V'priv(θ)k2
θ∈C
1:	ψ⑻ := nub Pd∈Dpub 'pub(θ; d), θ0 J argminθ∈c ψ⑻, σ J	(εnpriv)2
2:	for t = 0, . . . , T - 1 do
3:	gt J n1- Pd∈Dpriv V'priv(θ; d)
4:	θt+ι J argmi□θ∈c [ηthgt + bt,θ> + Bψ(θ,θt)], where b 〜N(0,σ2 ∙ Ip)
5:	end for
6:	return θpriv := T PT=I θt
Note that Line 4 of PDA-DPMD is equivalent to the following: Choose θt+1∕2 to be the point
such that VΨ(θt+1∕2) = VΨ(θt) 一 η(gt + bt), and then use the Bregman projection θt+1 =
5
Under review as a conference paper at ICLR 2022
argmi□θ∈c Bψ(θ, θt+1∕2). Intuitively, PDA-DPMD is similar to DP-SGD, with the main differ-
ence being We apply the gradient steps to VΨ(θ) rather than to θ itself. Note that PDA-DPMD
reshapes the gradient and noise automatically given `pub and Dpub . In contrast, e.g., private Ada-
Grad implementations (Kairouz et al., 2020; Asi et al., 2021) assume knowledge of the geometry of
the loss function has already been learned prior to running their algorithms. Also, for an appropriate
choice of Ψ, one can recover an algorithm that projects the private gradients to a low-dimensional
subspace as in the algorithms of Zhou et al. (2020) and Kairouz et al. (2020).
Dimension Independent Empirical Risk Bounds for Convex Losses: We bound the excess
empirical loss on the public loss function 'pub, compared to the private population minimizer θ*
(rather than the empirical minimizer). This is because the empirical minimizer θemp of the private
loss function could be far away from θ*, and in turn VΨ(θemp) could be much larger in expectation
than VΨ(θ*). Our PDA-DPMD excess empirical loss bound will be in terms of VΨ(θ), where θ is
the point we are comparing to, so it is preferable to use θ = θ* for this reason.
We will use the following “bounded variance” assumption on the distribution of the datasets and the
public loss function:
Assumption 3.1. For some minimizer θ* ∈ argmi□θ∈c Ed〜T ['priv(θ; d)] we have that θ* is also
the minimizer of Ed〜T
['pub(θ; d)] in C and Ed〜τ [∣∣V'pub(θ*; d)
-Ed5[V'pub(θ*; d)]k2] ≤ V2.
In particular, this implies
ED-pub ]∣∣n⅛ Pd∈D V'pub(θ*;d) - Ed~τ[V'pub(θ*;d)]∣0 = O (np2b).
We note that while Assumption 3.1 is written as generally as possible and thus captures scenarios
where `pub and `priv could potentially be very different loss functions, the reader can think of them as
differing only slightly. Indeed, Assumption 3.1 captures several scenarios we might see in practice,
such as (i) `pub = `priv (which can occur if kCk2 is small), (ii) `priv is the clipped version of `pub (see
e.g., Song et al. (2021) for a discussion on the effects of clipping on the loss function), and (iii) `pub
is `priv but with a regularizer added. Our empirical loss bound now follows by using Assumption 3.1
to control the Bregman divergence between the initial iterate and the population minimizer:
Theorem 3.2. Suppose the private loss function L := n1- ∑2d∈Dpriv 'priv (θ; d) is L-Lipschitz and
convex. Suppose `pub is α-strongly convex, and let Q be the minimal convex body containing the
origin such that each 'pub(θ; d) is 1 -strongly convex with respect to the Minkowski norm ∣∣ ∙ ∣∣q
(defined as ∣∣x∣q = min{c ∈ R≥0∣x ∈ cQ}). Then PDA-DPMD is (ε, δ)-differentiallyprivate with
respect to the private database Dpriv and choosing ηt = η for all t we have:
EDpUb〜TnpUb[L(θpriv)] -L(θ*) ≤ 2-VT2- + η∙O(L2 ∣Qk2 + σ2(GQ + ∣Q∣2)).
2αη T npub
The above bound is scale-invariant, so to simplify the presentation of this section, we assume, with-
out loss of generality, that α = 1 (this also implies Q is contained within the unit '2-ball, i.e.
∣Q∣2 ≤ 1). By rescaling Ψ and η appropriately, we do not affect the behavior of PDA-DPMD, but
get that Ψ is 1-strongly convex.
By chaining the following lemma with Theorem 3.2, we get an excess empirical loss bound with
respect to the sample minimizer rather than the population minimizer as desired.
Lemma 3.3. Let τ be a distribution over D, ' : C × D → R be a function such that '(θ; d) is
L-Lipschitz and convex in θ for any fixed d ∈ Supp(T). Let θ* be the minimizer of Ed 〜τ['(θ; d)].
Then, we have ED〜Tn [L(θ*; D) — minθ∈c L(θ; D)] ≤ L√Ck2.
The lemma follows by using convexity and a bound on the expected '2-norm difference between the
gradient of the empirical and population losses.
Excess Population Risk of PDA-DPMD: We now translate our excess empirical loss bound to
a excess population loss. We use Lemma F.5 of Bassily et al. (2014), restated in Lemma 3.4 for
convenience, which provides a black-box translation from empirical loss to population loss:
Lemma 3.4. For any (ε, δ)-DP algorithmfor minimizing n1- Pd∈Dpr.v '(θ; d) over C, the expected
excess population loss exceeds the expected excess empirical loss by O(L ∣C∣2 ε + ∣C∣22 δ).
6
Under review as a conference paper at ICLR 2022
Given this lemma, it is straightforward to derive excess population loss bounds:
Theorem 3.5. For η = -V=, T
L	L√Tnpub'
population loss of PDA-DPMD is
ε2nK
GQ log(1∕δ)'
andsetting ε = VzVGQ广㈤,the expected
√nPiVnpUb kCk2
O
LPVWPGQlog1/4(1/s)+ √ck2 + kCk2 δ).
/---- 1/4
√npriv npub
Theorem 3.5 follows immediately from Theorem 3.2, Lemma 3.3, and Lemma 3.4. Note that if
npub ≥ G2Q, which is at most p, then the above bound has no explicit dependence on dimension. For
comparison, if we were to only train on public data, the standard non-private excess population loss
bound has dependence O(1∕√npub) (and no dependence on dimension). So in the regime where
npub ≈ G2Q and npriv npub, our bound is asymptotically much better than the baseline of training
only on public examples. In Appendix D we show what values Q and GQ take on for some standard
stochastic convex optimization problems to help the reader understand the bound in Theorem 3.5.
Local Stability Properties of PDA-DPMD: If the public loss function has a Hessian everywhere
that has the same eigenvectors regardless of location (but perhaps different eigenvalues), we can
bound the impact of adding noise to the gradient on each update in mirror descent:
Theorem 3.6. Suppose for the public loss function Ψ, its Hessian is defined everywhere, and for
a fixed orthonormal basis {vi}i, the Hessian at every point can be written as i wi (θ)vivi> for
scalar functions wi : Rp → R+ such that for all i, θ, we have wi(θ) ≥ α. Fix an iteration t
as well as private gradient gt in PDA-DPMD. Let θ* be the value of θt+ι after performing the
mirror descent update with bt = 0 at iteration t, and let {wei}i , c ≥ 0 be such that for each i the
smallest value of wi(θ) in the ellipsoid E := (E % 看 Vv>>Br (where BR is the '2 ball of radius
R := η(1 + c)√pσ), centered at θ*, is at least Wi. Thenfor any (unit) direction V = Ei aiVi,
E [lhθ - θ*, v)|] ≤ ησ
3(1 + c)2
2α
1 A ■ .1	1 r . 1	. ∙. ,八	■ r ■	■	11 1
where θ is the value of the next iterate θt+1 if noise is added.
k . 1 ♦ 1 1	1 EI	C∕C11	F	F	♦	.1 . A Zi⅛	F	1	. 1	T τ	♦
At a high level, Theorem 3.6 follows by observing that θ -θ* can be expressed as the inverse Hessian
of Ψ evaluated at some point θ times the noise vector bt . If the magnitude of bt is small enough,
θ stays within E and We can lower bound the inverse Hessian by 看viv>. In the low-probability
event bt's magnitude is large enough, We can instead lower bound the inverse Hessian by aIp.
Note that the condition w%(θ) ≥ α can be enforced by adding an '2-regularizer to the public loss
function (since mirror descent only cares about differences in the gradients of the public loss func-
tion, the private training phase of PDA-DPMD behaves the same regardless of where this regularizer
is centered). In contrast for DP-SGD, E
[lhθ-θ*, vil] = P2∕∏
• ησ for any direction v.
Validation of Dimension-independence on Synthetic Linear Regression: To corroborate our
theoretical results with empirical validation, we consider the linear regression problem with mean
squared error loss: = ∣∣Xθ - y ∣∣2. We vary the dimensionality of the problem P from 500 to 6000.
For each p, we generate 10,000 private samples and 1.5p public samples. The optimal θ* is sampled
from N(0, Ip). To introduce a non-isotropic geometry, we sample the feature vector xi such that 40
of the first p/5 features and 80 of the last 4p/5 features, chosen uniformly at random, are set to 0.05,
and the rest of the features are set to 0. In this way, the expected '2-norm of each feature vector (and
in turn each gradient) is dimension-independent, and thus the effects of clipping should not vary with
p. The predicted variable yi is sampled from N(θ* ∙ Xi, 0.01) so that the population mean squared
error loss is always 0.01, i.e. independent of dimension. Since the norms of the gradients and
number of private samples are dimension-independent, our error bound is proportional to Gq/ √p,
i.e. we do not expect it to vary much with dimension. We set ε = 1, δ = 10-5.
We consider three algorithms: (i) standard DP-SGD with a “cold start”, i.e. using a random initial-
ization, (ii) standard DP-SGD with a “warm start” on the model pre-trained with public data, and
(iii) PDA-DPMD after pre-training on public data.
7
Under review as a conference paper at ICLR 2022
0.020-
IOOD 2000 3000 4000 5000 6000
Dimension
0.030-
0.028-
0.026-
'0.024-
0.022-
Dimension
→-coslOO
-→-DP-SGD warm
DP-SGD cold
⑶ Cold start DP-SGD vs. (b)WarmstartDP-SGDvs.
warm start DP-SGD. PDA-DPMD.
Figure 1: Comparisons of the empirical loss on
synthetic linear regression data. The mean and er-
ror bars for a95% confidence interval over 20 runs
are plotted. The optimal population loss is 0.01.
Sozs
O IOOO 2QQQ 3000
step
(a) σ = 0.5. ε = 15.7.
Figure 2: WikiText-2. 4% public data. Validation
and test loss. Averaged over 3 runs. Full plots
available at Appendix F.3.
→-cos50
-→-DP-SGD warm
5
DP-SGD cold
Sozs
O IOOO 2000 3000
step
(b) σ = 1.08. ε = 1.71.

Figure 1a shows the empirical loss of cold- and warm-start DP-SGD. Since the dimension-dependent
loss of DP-SGD is the excess empirical loss, we report empirical rather than population loss. As
expected, pre-training with public data allows DP-SGD to achieve nearly dimension-independent
error. Figure 1b compares warm-start DP-SGD and PDA-DPMD. The loss of PDA-DPMD is never
worse than that of warm-start DP-SGD, and can be substantially lower for smaller dimensions. We
observed that the ratio of the maximum and minimum eigenvalues of the Hessian X>X decreases
as p increases, which means that the Hessian has poorly-concentrated eigenvalues at small p but
gets closer to the identity matrix as p increases. Since PDA-DPMD recovers warm start DP-SGD
when the Hessian is the identity, we can expect that PDA-DPMD obtains less of an advantage over
DP-SGD as the Hessian gets closer to the identity. We provide additional details in Appendix E.
4	Empirical Evaluation on B enchmark Data Sets
First-order Approximation to Mirror Descent: In practice, the Mirror Descent (MD) step in
Line 4 of Algorithm 1 can be computationally expensive. For settings where (i) the problem is
unconstrained, i.e., C = Rp and (ii) the public loss function Ψ(θ) may not be strongly convex with
respect to the '2-norm, We can instead use the following more efficient approximation:
θt+ι - θt - ηt (αt(gt + bt) + (1 - αt)VΨ(θt)),	(1)
where ηt is the learning rate, and αt ∈ [0, 1] balances the weight of private and public gradient. The
derivation of this formula is in Appendix F.1. Notice that αt = 1 corresponds to DP-SGD on private
data only. In our experiment, we decrease at with a cosine schedule, i.e. at = cos(∏t∕(2K)) where
K is a hyperparameter that controls how fast αt decays. In practice, instead of computing gt and
VΨ(θt) using all the private and public data, we can estimate them with stochastic gradients.
Now, we demonstrate the efficacy of our technique (Algorithm 1) with the update step in (1) on
two real world tasks across three benchmark data sets: next word prediction on WikiText-2 (Merity
et al., 2017), and image classification on CIFAR-10 (Krizhevsky, 2009) and EMNIST (ByMerge
split) (Cohen et al., 2017). For each dataset, we randomly assign 4% of the original training data
as public, and the rest as private. We do not consider a larger amount of in-distribution public data
as that could make the problem trivial. We first pre-train on the public data, then use Algorithm 1
with update rule (1). We compare our algorithm with two baselines: “cold-start” DP-SGD, which
uses the private data only, and “warm-start” DP-SGD, which pre-trains the model with public data
and then fine-tunes with the private data. We demonstrate an increased benefit from the public data
over and above just pre-training, which to our knowledge, has not been achieved in prior work. For
WikiText-2, we use an LSTM model from Asi et al. (2021). For CIFAR-10 and EMNIST, we use
network architectures considered in prior works (Papernot et al., 2020; Kairouz et al., 2021b). See
Appendix F.2 for more details.
Empirical Evaluation on WikiText-2: Our setup mainly follows Asi et al. (2021). As a prepro-
cessing step, we take the top 7,999 most frequent words and convert the rest into a special token
representing unknown word. The data set is then split into 48,764 length-35 sequences, and we con-
sider sequence-level privacy here. After pre-training, we fine-tune the model with batch size 250 for
20 epochs. We search for optimal K in {100, 200, 500}. For two different privacy levels, ε = 15.7
and 1.71 at δ = 10-5 (corresponding to σ = 0.5 and 1.08, respectively), Figure 2 shows the test
8
Under review as a conference paper at ICLR 2022
Table 1: Metrics for the final models for each configuration for each data set.
Data set, metrics	Algorithm	Smaller σ	Larger σ
WikiText-2, test loss	DP-SGD cold	5.2626	5.5627
	DP-SGD warm	5.1914	5.3288
	PDA-DPMD	5.1736	5.2956
CIFAR-10, accuracy / test loss	DP-SGD cold	62.9633 / 1.4225	40.6000 / 1.6890
	DP-SGD warm	66.3933 / 1.2371	53.4100 / 1.3462
	PDA-DPMD	67.0300 / 1.1435	55.3950 / 1.2785
EMNIST, accuracy / test loss	DP-SGD cold	87.5671 / 0.5422	84.7270 / 0.6170
	DP-SGD warm	87.8534 / 0.5089	86.3352 / 0.5586
	PDA-DPMD	87.9860 / 0.4706	86.7229 / 0.4982
loss for PDA-DPMD with K = 100 and 50 respectively, and for the two baselines. From Table 1,
we see that PDA-DPMD obtains the smallest test loss for both the privacy levels. Also, comparing
the two DP-SGD baselines, we can see that using public data for pre-training provide trained models
with higher utility.
Though our work,s focus is on in-distribution public data, we additionally compare with SOIA (ASi
et al., 2021) which uses WikiText-103 (Merity et al., 2017) as the public data. In that setting, our
warm start DP-SGD baseline is better than the proposed SoIA in (Asi et al., 2021) by 1.1% for
ε = 1.0 and 6.6% for ε = 3.0 in terms of test perplexity. See Appendix F for more details.
Empirical Evaluation on CIFAR-10: CIFAR-10 consists of 50,000 training images and 10,000
test images from 10 classes. After pre-training, we fine-tune the model with batch size 500 for 100
epochs. We search for optimal K in {200, 500,1000,2000, 5000}. In Figure 3, for two different
privacy levels, ε = 3.51 and 0.19 at δ = 10-5 (corresponding to σ = 1.51 and 20.0, respectively),
we report the test loss and accuracy for K = 2000, and for the two baselines. From Table 1, we
see that PDA-DPMD provides the best accuracy (even if by a small margin over the warm started
DP-SGD baseline). Moreover, PDA-DPMD also results in significantly lower test loss compared
to both the baselines for both privacy levels, which confirms with our theoretical analysis for the
population loss.
τ- COS2000 —DP-SGD warm —DP-SGD cold
5 Q
6 6
AUeJnUUe9
ɪjJ
1.2
so-£
54525 4
AueJnUue0£
—COS2000 —DP-SGD warm
1.50
sso-59-j
1.3
2500 5000 7500 10000	0	2500 5000 7500 10000
step	step
(a)	σ = 1.51. ε = 3.51.
0	2500 5000 7500 10000	0	2500 5000 7500 10000
step	step
(b)	σ = 20.0. ε = 0.19. Full plot in Appendix F.3.
Figure 3: CIFAR-10. 4% public data. Test accuracy / loss vs. training steps. Averaged over 3 runs.
Empirical Evaluation on EMNIST: EMNIST (ByMerge split) consists of697,932 training images
and 116,323 test images from 47 classes. After pre-training, we fine-tune with batch size 500 for
50 epochs. We search for optimal K in {200, 500,1000,2000, 5000}. In Figure 4 and Table 1, for
σ = 0.41 and 1.89, corresponding to privacy ε = 25.80 and 0.48 at δ = 10-6, we report the test
loss and accuracy for K = 500, and for the two baselines. We see a similar trend as with CIFAR-10.
AueJnUue≡9-
0	20000 40000
0	20000 40000 60000
step	step
(a) σ = 0.41. ε = 25.80.
60000
Figure 4: EMNIST. 4% public data. Test accuracy / loss vs. training steps. Averaged over 3 runs.
AueJnUue09-j
—COS500 -→- DP-SGD warm —DP-SGD cold
87.0
86.5
86.0
85.5
85.0
84.5
20000 40000 60000
step
(b) σ = 1.89.
0	20000 40000 60000
ε = 0.48.
step
9
Under review as a conference paper at ICLR 2022
5	Ethics and Reproducibility
Ethics: This work focuses on improving the privacy/utility trade-offs for model training with sen-
sitivity data by using publicly available data. We envision that this work will make the adoption of
(differentially) private model training more ubiquitous, and hence improve on the current privacy
landscape in the context of machine learning. Our experimental results are on standard benchmark
data publicly available data sets, and do not have any ethical concerns.
Reproducibility: Our experiments are either on synthetic data sets, oron standard publicly available
data sets. We have provided full details (including hyperparameter choices) for the experimental
setup to reproduce the results in the paper. We will make the code for our experiments public as soon
as possible. Additionally, we have detailed proofs for all our theoretical results in the appendix.
References
Martin Abadi, Andy Chu, Ian J. Goodfellow, H. Brendan McMahan, Ilya Mironov, KUnal Talwar,
and Li Zhang. Deep learning with differential privacy. In Proc. of the 2016 ACM SIGSAC Conf.
on Computer and Communications Security (CCS'16),pp. 308-318, 2016.
Naman Agarwal, Brian Bullins, Xinyi Chen, Elad Hazan, Karan Singh, Cyril Zhang, and Yi Zhang.
Efficient full-matrix adaptive regularization. In International Conference on Machine Learning,
pp. 102-110. PMLR, 2019.
Noga Alon, Raef Bassily, and Shay Moran. Limits of private learning with access to public data.
arXiv preprint arXiv:1910.11519, 2019.
Hilal Asi, John Duchi, Alireza Fallah, Omid Javidbakht, and Kunal Talwar. Private adaptive gradient
methods for convex optimization. In International Conference on Machine Learning, pp. 383-
392. PMLR, 2021.
Brendan Avent, Aleksandra Korolova, David Zeber, Torgeir Hovden, and Benjamin Livshits.
{BLENDER}: Enabling local search with a hybrid differential privacy model. In 26th {USENIX}
Security Symposium ({USENIX} Security 17), 2017.
Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient
algorithms and tight error bounds. In Proc. of the 2014 IEEE 55th Annual Symp. on Foundations
of Computer Science (FOCS), pp. 464-473, 2014.
Raef Bassily, Om Thakkar, and Abhradeep Thakurta. Model-agnostic private learning. In NeurIPS,
2018a.
Raef Bassily, Abhradeep Guha Thakurta, and Om Dipakbhai Thakkar. Model-agnostic private learn-
ing. Advances in Neural Information Processing Systems, 2018b.
Raef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Thakurta. Private stochastic convex
optimization with optimal rates. In Advances in Neural Information Processing Systems, pp.
11279-11288, 2019.
Raef Bassily, Albert Cheu, Shay Moran, Aleksandar Nikolov, Jonathan Ullman, and Steven Wu.
Private query release assisted by public data. In Hal Daume In and Aarti Singh (eds.), Pro-
ceedings of the 37th International Conference on Machine Learning, volume 119 of Proceed-
ings of Machine Learning Research, pp. 695-703. PMLR, 13-18 Jul 2020a. URL https:
//proceedings.mlr.press/v119/bassily20a.html.
Raef Bassily, Vitaly Feldman, Cristobal Guzman, and Kunal Talwar. Stability of stochastic gra-
dient descent on nonsmooth convex losses. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
4381-4391. Curran Associates, Inc., 2020b. URL https://proceedings.neurips.cc/
paper/2020/file/2e2c4bf7ceaa4712a72dd5ee136dc9a8-Paper.pdf.
Raef Bassily, Shay Moran, and Anupama Nandi. Learning from mixtures of private and public
populations. arXiv preprint arXiv:2008.00331, 2020c.
10
Under review as a conference paper at ICLR 2022
Kamalika Chaudhuri, Claire Monteleoni, and Anand D Sarwate. Differentially private empirical
risk minimization. Journal ofMachine Learning Research, 12(Mar):1069-1109, 2011.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist
to handwritten letters. 2017 International Joint Conference on Neural Networks (IJCNN), 2017.
doi: 10.1109/ijcnn.2017.7966217.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of machine learning research, 12(7), 2011.
Cynthia Dwork and Vitaly Feldman. Privacy-preserving prediction. In Conference On Learning
Theory, pp. 1693-1702. PMLR, 2018.
Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations
and Trends in Theoretical Computer Science, 9(3-4):211-407, 2014.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in
private data analysis. In Proc. ofthe Third Conf. on Theory of Cryptography (TCC), pp. 265-284,
2006. URL http://dx.doi.org/10.1007/11681878_14.
Vitaly Feldman, Ilya Mironov, Kunal Talwar, and Abhradeep Thakurta. Privacy amplification by
iteration. In 2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS),
pp. 521-532. IEEE, 2018.
Guy Gur-Ari, Daniel A. Roberts, and Ethan Dyer. Gradient descent happens in a tiny subspace.
CoRR, abs/1812.04754, 2018. URL http://arxiv.org/abs/1812.04754.
Elad Hazan. Introduction to online convex optimization. arXiv preprint arXiv:1909.05207, 2019.
Peter Kairouz, Monica Ribero, Keith Rush, and AbhradeeP Thakurta. Dimension independence in
unconstrained private ERM via adaptive preconditioning. CoRR, abs/2008.06570, 2020. URL
https://arxiv.org/abs/2008.06570.
Peter Kairouz, Monica Ribero Diaz, Keith Rush, and Abhradeep Thakurta. (nearly) dimension
independent private erm with adagrad rates
via publicly estimated subspaces. In COLT, 2021a.
Peter Kairouz, Brendan McMahan, Shuang Song, Om Thakkar, Abhradeep Thakurta, and Zheng
Xu. Practical and private (deep) learning without sampling or shuffling. In ICML, 2021b.
Alex Krizhevsky. Learning multiple layers of features from tiny images, 2009.
Terrance Liu, Giuseppe Vietri, Thomas Steinke, Jonathan Ullman, and Zhiwei Steven Wu. Leverag-
ing public data for practical private query release. arXiv preprint arXiv:2102.08598, 2021.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. In 5th International Conference on Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https:
//openreview.net/forum?id=Byj72udxe.
Chris Merriman. Microsoft reminds privacy-concerned windows 10 beta testers that they’re vol-
unteers. The Inquirer. http://www.theinquirer.net/2374302, 2014. URL http:
//www.theinquirer.net/2374302.
Anupama Nandi and Raef Bassily. Privately answering classification queries in the agnostic pac
model. In Aryeh Kontorovich and Gergely Neu (eds.), Proceedings of the 31st International
Conference on Algorithmic Learning Theory, volume 117 of Proceedings of Machine Learning
Research, pp. 687-703. PMLR, 08 Feb-11 Feb 2020. URL https://proceedings.mlr.
press/v117/nandi20a.html.
A. Nemirovsky and D. Yudin. Problem Complexity and Method Efficiency in Optimization. Wiley
& Sons, New York, 1983.
11
Under review as a conference paper at ICLR 2022
Nicolas Papernot, Martin Abadi, Ulfar Erlingsson, Ian Goodfellow, and KUnal TalWar Semi-
supervised knowledge transfer for deep learning from private training data. arXiv preprint
arXiv:1610.05755, 2016.
Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, and Ulfar Er-
lingsson. Scalable private learning with pate. arXiv preprint arXiv:1802.08908, 2018.
Nicolas Papernot, Abhradeep Thakurta, Shuang Song, Steve Chien, and Ulfar Erlingsson. Tempered
sigmoid activations for deep learning with differential privacy. arXiv preprint arXiv:2007.14191,
2020.
Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with differ-
entially private updates. In 2013 IEEE Global Conference on Signal and Information Processing,
pp. 245-248. IEEE, 2013.
Shuang Song, Thomas Steinke, Om Thakkar, and Abhradeep Thakurta. Evading the curse of dimen-
sionality in unconstrained private glms. In Arindam Banerjee and Kenji Fukumizu (eds.), Pro-
ceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume
130 of Proceedings of Machine Learning Research, pp. 2638-2646. PMLR, 13-15 Apr 2021.
URL https://proceedings.mlr.press/v130/song21a.html.
Kunal Talwar, Abhradeep Thakurta, and Li Zhang. Private empirical risk minimization beyond the
worst case: The effect of the constraint set geometry. arXiv preprint arXiv:1411.5417, 2014.
Florian Tramer and Dan Boneh. Differentially private learning needs better features (or much more
data). In International Conference on Learning Representations, 2020.
Di Wang, Minwei Ye, and Jinhui Xu. Differentially private empirical risk minimization revis-
ited: Faster and more general. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/
paper/2017/file/f337d999d9ad116a7b4f3d409fcc6480-Paper.pdf.
Yingxue Zhou, Zhiwei Steven Wu, and Arindam Banerjee. Bypassing the ambient dimension: Pri-
vate sgd with gradient subspace identification. In ICLR, 2020.
12
Under review as a conference paper at ICLR 2022
A Notation Reference
We recall here the notation used throughout the paper:
α	A strong convexity parameter
b	Noise added to gradients for privacy
Bψ	The Bregman divergence induced by Ψ
C	The constraint set
d	A single sample
Dpub, Dpriv	The public and private data sets respectively
D	The universe of samples
ε, δ	The privacy parameters
η	The learning rate/step size
g	A (batch) gradient
GQ	The Gaussian width of Q
'pub, 'priv	The (per-example) public and private loss functions respectively
L	The Lipschitz constant of 'priv
L	The empirical (PriVate) loss on dataset D, i.e. L(θ; D)= 奇 Pd∈D '(θ; d)
npub, npriv	The number of public and private samples respectively
P -	The dimensionality of the optimization problem
Ψ	The empirical public loss, i.e. Ψ(θ) = ∣p1^ Pd∈Dpub '(θ; d); also the mirror map
Q	A set such that Ψ is 1-strongly convex with respect to ∣∣∙∣∣q
T	The number of iterations in the optimization algorithm
θ	A solution to the optimization problem
V	A bound on the variance of the public gradients
B Missing Background
Mirror Maps: A mirror map is a differentiable function Ψ : Rp → R that is strictly convex.
Since Ψ is strictly convex and differentiable, VΨ : Rp → Rp provides a bijection from Rp to
itself. One can view θ as lying in a primal space and VΨ(θ) as lying in a dual space. In turn, we
could now consider optimizing over the value VΨ(θ) in the dual space instead of θ primal space.
Mirror descent does exactly that, performing gradient descent in the dual space by computing the
gradient gt = V'(θt) (where θt lies in the primal space), taking a step in the opposite direction
in the dual space, and then using the inverse of the mirror map to determine θt+1. Mirror descent
is essentially motivated as minimizing a (linearized) loss plus a Bregman divergence (induced by
Ψ) as the regularizer (Nemirovsky & Yudin, 1983). More formally, similar to proximal gradient
descent, mirror descent is equivalent to taking the gradient gt and performing the update θt+1 =
arg minθ∈C[ηhgt, θi + BΨ(θ, θt)] where BΨ(θ1, θ2) = Ψ(θ1) - Ψ(θ2) - hVΨ(θ2), θ1 - θ2i is the
Bregman divergence generated by Ψ. Note that, if Ψ(θ) = kθk22, then the Bregman divergence is
simply BΨ(θ1, θ2) = kθ1 - θ2k22 and mirror descent is equivalent to the usual gradient descent.
Gaussian Width: Given a bounded set Q ⊂ Rd, the Gaussian width of Q, GQ, is a measure of
how isotropic the set is. GQ is defined as Eg~N(0,3 maXχ∈Q hg, x). Although the Gaussian width
is well-defined for any bounded set, for this paper it suffices to consider defining the Gaussian width
of convex sets containing the origin such that maXχ∈Q ∣∣xk2 = 1. If Q is just the unit '2-ball, the
“most isotropic” set satisfying this condition, then we have GQ = √p; in particular, since every
set Q satisfying maXχ∈Q ∣∣x∣2 = 1 is contained in the '2-ball, this is the maximum Gaussian width
of any such set. On the other hand, if Q is just the line from the origin to a single unit vector, we
have GQ = Θ(1). More generally, for any ellipsoid centered at the origin whose axes have radii
0 ≤ r ≤ 1,1 ≤ i ≤ p, we have that the Gaussian width of this ellipsoid is VPp=I r2. AS other
examples, the Gaussian width of the '1 -ball of radius 1 is roughly log p, and the Gaussian width of
the '∞ ball of radius 1 / √p is roughly √p.
13
Under review as a conference paper at ICLR 2022
C Missing Proofs from Section 3
Proof of Theorem 3.2. The privacy guarantee follows from the moments accountant analysis of
Abadi et al. (2016).
For the utility guarantee, following the analysis of Theorem 3.2 of Talwar et al. (2014), we have:
E[L(θpriv)] - L(θ*) ≤ BE；；,00) + η O(L2 kQk2 + σ2(GQ + ∣∣Q∣∣2),	⑵
Let θ* in particular be the minimizer satisfying Assumption 3.1. By a-strong convexity, We have:
Bψ(θ*,00) = Ψ(θ*) - Ψ(θo) - VΨ(θ0) ∙ (θ* - θo) ≤ ɪ ∣∣VΨ(θ*) - VΨ(θo)k2.
2α
Plugging this into Eq. (2) and noting that any Ψ We sample is 1-strongly convex With respect to
Il ∙ kQ, weget:
E[L(θpriv)] -L(θ*) ≤
E [kVΨ(θ*) -VΨ(θ0)k2]
2αηT
+ η ∙ O(L2kQk2+ σ2(GQ + kQk2)
We will show that without loss of generality, we can assume VΨ(θ0) = 0 and Ed〜τ [V'pub (θ*; d)]
0. Once we have this assumption, Assumption 3.1 completes the proof.
The assumption follows since by convexity ofC we have
hVΨ(θ0),θ0 - θ*i≤ 0,	〈Ed〜T[V'pub(θ*; d)],θ* - θ0i≤ 0	(3)
Then for any choice of Ψ and C where either θ0 or θ* is on the boundary of C, suppose we extend
C infinitesmally along the line {θ0 + c(θ* 一 θ)∣c ∈ R} (i.e., take a point on this line infinitesmally
outside of C and update C to be the convex hull of itself and this point). Then by (3) we have that
θ*,00, defined as the minimizers in C, move apart from each other along this line and in turn by
strong convexity the quantity ∣∣VΨ(θ*) 一 VΨ(00)∣2 cannot decrease. This implies that for any
fixed 'pub and T, the quantity E [∣VΨ(θ*) — VΨ(θ0) ∣2] is maximized for a choice of C such that
VΨ(θ0) = 0 and Ed 〜T [V'pub(θ*; d)] = 0.	□
ProofofLemma 3.3. By convexity, for all θ ∈ C we have L(θ; D) ≥ L(θ*; D) + (VL(θ*; D), θ 一
θ*i. Note that by optimality of θ* and convexity, for all θ ∈ C we have〈Ed〜丁 [V'(θ*; d)],θ 一 θ*i ≥
0. In turn, by the Cauchy-Schwarz inequality we can conclude that L(θ*; D) 一 minθ∈c L(θ; D) is
always upper bounded by ∣∣C∣2 ∙ ∣VL(θ*; D) 一 Ed〜T[V'(θ*; d)]∣∣2. By L-Lipschitzness of each
'(θ; d) we have:
Ed-tn [∣VL(θ*; D)- Ed-τ[V'(θ*; d)]k2] ≤ √n,
Which completes the proof.	□
Proof of Theorem 3.6. Let bt be the noise added for privacy. Without noise, mirror descent would
set θ* tobe such that:
-ηgt = VΨ(θ*)-VΨ(θt).
1-1 ∙ ∙1 1	♦	.1	♦	1 ∙	1	♦	1	.	1 1	,久， F	1.1
Similarly, given the noisy gradient gt + bt, mirror descent would set 0 to be such that:
, ʌ.
-η(gt + bt) = VΨ(θ) -VΨ(θt).
14
Under review as a conference paper at ICLR 2022
We then have:
,ʌ. ..
-ηb = vψ(θ) -VΨ(θ*).
1~ι ∙	.1 T T ♦	Γ∙ ,ʃ, ∙ i r∙ 1	1	1	.1	X—z,ʃ, / zi∖	χ-z,ʃ, / Zi⅛ ∖
Since We assume the Hessian of Ψ is defined everywhere, We have that VΨ(θ) 一 VΨ(θ*)=
2
V2Ψ(θ)(θ — θ*) for some θ on the line between θ and θ*. In turn, we have:
θ — θ* = —η(v2 Ψ(e))-1bt
The norm x of bt sampled from N (0, σ2Ip) has the chi distribution, i.e. pdf proportional to
(x∕σ)p-1e-(x∕σ)2/2. In particular, this gives the following standard tail bound:
Pr[kXl∣2 > (1 + c)√pσ] ≤ exp(—c2p∕2).
So with probability at least 1 — e-c2p/2, we have the event E that IIbtk2 is at most (1 + c)√pσ.
Conditioned on this event, we have that θ, and thus θ, is in E, i.e. the lower bounds wei apply to
V2Ψ(θ). Since conditioning on E only decreases the expectation of ∣(θ — θ , v)|, we have:
E [ηlhθ — θ*, Vi间∙ Pr[E]= ηE [∣h(V2Ψ(e))-1bt, Vi间.Pr[E]
≤ ηE K(X WViV>)bt, vil∣E] ∙Pr[E] ≤ ηE K(X Wv">)bt, v〉l = E I X Whbt, Viil
ηE I EN(0,(ai∕Wi)2)I
i
ηE |N(0,5>i∕Wi)2)∣
. ησt
i
When E does not happen, we have Wi (θ) ≥ α everywhere. So we have:
E [η∣{θ — θ*,vi∣∣-E] ∙ Pr[-E] = ηE [∣h(V2Ψ(e))-1b,vi∣∣-E] ∙ Pr[-E]
≤ η ∙ 1E [∣hb,vi∣∣-E ] ∙ e-c2p/2
α
To determine E [Ihbt, ViIIE], note that the distribution of hbt, Vi conditioned on E is equivalent to
the distribution of the first coordinate of bt conditioned on E . We can sample bt by first sampling
its norm kbt k2 conditioned on E, and then sampling a point on the sphere with radius kbt k2 (no
conditioning is required here). The expected absolute value of any coordinate (bt)i given kbtk2 can
be bounded as:
E [I(bt)i∣] ≤ √E [(bt)2] = kbtk2 ∕√P.
The inequality is Jensen’s inequality, and the equality uses the fact that the coordinates bi on the
sphere are identically distributed, and so we have:
P ∙ E [(bt)2] = E ∑(bt)2 = kbtk2 .
i
We now just need to bound the expectation of kbtk2, given that it is at least R. Since the distribution
of kbtk2 ∕σ has pdf proportional to xp-1e-x2/2, this expectation is σ times:
R(∞+c)√Pxpe-χ2∕2 = Γ((p +1)/2)(1 — P((P + 1)/2, (1 + c)2p∕2))
R(∞+c)√p XpTe-X2/2 —	√2Γ(p∕2)(1 - P(p∕2, (1 + c)2p∕2))	.
15
Under review as a conference paper at ICLR 2022
Where Γ is the gamma function and P is the regularized gamma function. Analytically, we can
verify that r(⅞+∕2∕2)
≤ pp/2 for all P ≥ 1, and
p ≥ 1. So we get:
(1-P ((p+1)∕2,(1+c)2p∕2))
(1-P (p∕2,(1+c)2p∕2))
≤ 3(1 + c)2 for all
E[∣∣bt∣∣2 I-E] ≤
3(1 + C)
2^^
2
一√pσ
Putting it all together, we get:
E [η∣<θ - θ*,vi∣∣-E] ∙ Pr[-E]
1
≤ η——
α
「∙ e-c2p∕2 ∙ σ
Now applying the law of total expectation gives the theorem statement.
□
D	Examples of Stochastic Convex Optimization Problems
In this section, we consider two canonical stochastic convex optimization problems and the associ-
ated quantity GQ, to help understand the bound in Theorem 3.5. In both cases we specify the public
loss; as previously mentioned, a natural choice is to take the private loss to be the clipped version of
the public loss to ensure Lipschitzness holds.
Mean Estimation: Suppose we assume we know the true covariance matrix Σ in the mean estima-
tion problem for data drawn from a multivariate Gaussian. Then mean estimation is equivalent to
minimizing loss function ɪ Pi(Xi - θ)>Σ-1 (Xi - θ), where Xi is the ith sample point, and θ is our
estimated mean. The Hessian is Σ-1. By rescaling, wlog assume the maximum eigenvalue of Σ is
1. Now, the smallest set Q such that the public loss function is 1-strongly convex with respect to the
Q-norm is the ellipsoid centered at the origin whose axes are in the directions of the eigenvectors of
Σ, and whose length in the direction of the eigenvector Vi is the corresponding eigenvalue λ%. The
Gaussian width of Q is thus a∕PP=1 λi2. In particular, if e.g. Σ only has a few large eigenvalues
(i.e., the data only has variance in a few directions), this is much smaller than p.
Linear Regression: We choose the loss to be the mean squared error. Now, the public loss function
is proportional to kXθ - yk22, where X is the (public) feature matrix, y are the dependent variables,
and θ is our model. In particular, this has Hessian X>X. By normalizing, we can assume wlog
the smallest eigenvalue of this matrix is 1, and so similarly to mean estimation the quantity GQ
is vzPP=ι 1∕λ2, where λ% are the eigenvalues of X>X (i.e., the singular values of X). Note that
unlike in the previous example (where we assumed we knew Σ a priori and thus the Hessian was
fixed), the Hessian here depends on the public data.
E	Additional Details for the Experiment on Linear Regression
Note that the exact optimum on the public data can be computed exactly as θ嬴 =(X>X)-1X>y.
The mirror descent step can also be solved exactly by applying the inverse of the Hessian X>X to
the gradient, since the Hessian is the same everywhere.
For numerical stability, we add a small constant times the identity matrix to the Hessian before
computing its inverse. We also normalize the Hessian of the loss function so its inverse (which is
applied to the gradient before taking a step in PDA-DPMD) has maximum eigenvalue of one. This
ensures that if the Hessian were a multiple of the identity matrix, DP-SGD and PDA-DPMD would
behave exactly the same for the same hyperparameter choice.
We perform a grid search over the learning rate, clipping norm, and number of epochs used and
report the best empirical loss. We perform 20 trials for each algorithm and dimension.
16
Under review as a conference paper at ICLR 2022
F Additional Details on Real-world Experiments
F.1 First-order Approximation to Mirror Descent
In practice, the Mirror Descent (MD) step in Line 4 of Algorithm 1 is computationally the most
challenging. In the following, we provide an approximation of this step in the setting where i) the
problem is unconstrained, i.e., C = Rp and ii). the public loss Ψ(θ) may not be strongly convex
with respect to the '2-norm. This approximation makes Algorithm 1 efficient in practice.
Consider the following equivalence of Line 4 of Algorithm 1, with Ψ(θ), the public loss on Dpub,
replaced by Ψ(θ) = Ψ(θ) + 2 ∣∣θk2. This follows from (Hazan, 2019, Lemma 5.5).
θt+ι — argmin( Xη (gi + bi) ,θ) + Ψ(θ)
θ∈Rp	i=1
=argminXηi (hgi + bi,θi + ；田⑻)+ 1 ∣∣θk2
θ∈Rp i=1	tηi	2	2
≈ arg min X η∕gi + bi + ɪ VΨ(θi),θ∖ + 1 ∣∣θ∣2 ,
θ∈Rp i=1	tηi	2	2
(4)
(5)
where (5) follows from the first-order approximation Ψ(θ) ≈ Ψ(θi) +hVΨ(θi), θ - θii.
In the experiments, We replace gi + bi + 吉VΨ(θi) with αi(gi + bi) + (1 一 ai)VΨ(θi), where
αi ∈ (0, 1]. This reparamertization helps with more effective hyperparameter tuning while training
deep learning models. We therefore have the update rule (1).
F.2 Setup
Network architectures: Table 2 shows model architectures for CIFAR-10, EMNIST, & WikiText-
2.
Table 2: Model architectures for real data experiments.
(a) Model architecture for CIFAR-10.
Layer	Parameters
Convolution ×2	32 filters of 3 X 3, strides 1
Max-Pooling	2 × 2, stride 2
Convolution ×2	64 filters of 3 × 3, strides 1
Max-Pooling	2 × 2, stride 2
Convolution ×2	128 filters of 3 × 3, strides 1
Max-Pooling	2 × 2, stride 2
Fully connected	128 units
Softmax	-
(b) Model architecture for EMNIST.	(c) Model architecture for WikiText-2.
Layer	Parameters	Layer	Parameters
Convolution	16 filters of 8 × 8, strides 2 Input	8000
Convolution	32 filters of 4 × 4, strides 2	Fully connected	120
Fully connected	32 units	LSTM ×2	120 hidden units
Softmax	-	Fully connected 8000
Softmax	-
Hyperparameter Tuning: We keep the clipping norm to be 1. One small difference with the stan-
dard DP-SGD update rule is that we enforce an additional clipping step for the privatized gradient
for the image classification task, where the clipping norm is the same as the clipping norm of for
17
Under review as a conference paper at ICLR 2022
individual gradient. The reason for this additional step is that the norm of the averaged clipping
gradients should still be upper bounded by the clipping norm.
The only hyperparameters that need to be tuned are the learning rate and K that controls the decaying
of αt. For the learning rate, We consider a grid of {1, 2,5} X 10i for different is such that the optimal
learning rate does not appear on the boundary. We search for the optimal K in {100, 200,500} for
WikiText-2 and {200, 500,1000, 2000,5000} for the image classification tasks.
F.3 Full Plots for Section 4
In Figure 5, we plot the complete version ofFigure 2, and in Figure 6, we show the complete version
of Figure 3.
COS100 —DP-SGD warm —DP-SGD cold
COS50 -→- DP-SGD warm —DP-SGD cold
0	1000 2000 3000	0	1000	2000 3000
step	step
(a) σ = 0.5
0	1000 2000 3000	0
step
(b) σ = 1.08
1000 2000 3000
step
Figure 5: Full plot for Figure 2. WikiText-2. 4% public data.
COS2000	—DP-SGD warm —DP-SGD CO∣d
0	2500 5000 7500 10000
L25∙^------1-----1----1-----
0	2500 5000 7500 10000
step	step
(a) σ = 20.0.
Figure 6:	Full plot of Figure 3. CIFAR-10. 4% public data. Test accuracy / loss vs. training steps.
Averaged over 3 runs.
F.4 WikiText-2 with WikiText- 1 03 as Public Data
We compare with the SoTA (Asi et al., 2021) which uses WikiText-103 as public data. Specifically,
we consider their “LargeAux” setting under ε = 1.0 and 3.0. Since the implementation for Asi et al.
(2021) is not public as of writing this work, we make our best effort to match their experiment setup.
We note that the data preprocessing and the number of iterations used (thus the noise multiplier for
achieving the same ε) might differ.
We preprocess WikiText-103 as follows. After processing WikiText-2 as described in Section 4, we
convert all words that does not appear in the processed WikiText-2 as the unknown token. Then, we
split the sentences into length-35 sequences, and remove all sequences that overlap with WikiText-2.
Finally, we randomly sample 48,764 sequences, in order to match the “LargeAux” setting where the
public dataset is of the same size as the private training dataset.
Figure 7	shows the results. In our setting, cold start DP-SGD reaches similar log perplexity as
those in (Asi et al., 2021), while the warm-start DP-SGD is already better than Asi et al. (2021)
(LargeAux). The final test log perplexities are summarized below, with the results in (Asi et al.,
2021) converted from perplexity to log perplexity.
18
Under review as a conference paper at ICLR 2022
Algorithm	ε = 3.0	ε = 1.0
Asi et al. (2021) DP-SGD (cold)	5.4819	5.6623
Asi et al. (2021) (LargeAux)	5.4324	5.5254
Our DP-SGD (cold)	5.4030	5.5956
Our DP-SGD (warm)	5.3646	5.5141
Figure 7: WikiText-2. WikiText-103 as public data.
19