Under review as a conference paper at ICLR 2022
ViViT: Curvature access through the general-
ized Gauss-Newton’s low-rank structure
Anonymous authors
Paper under double-blind review
Ab stract
Curvature in form of the Hessian or its generalized Gauss-Newton (GGN) ap-
proximation is valuable for algorithms that rely on a local model for the loss to
train, compress, or explain deep networks. Existing methods based on implicit
multiplication via automatic differentiation or Kronecker-factored block diago-
nal approximations do not consider noise in the mini-batch. We present ViViT,
a curvature model that leverages the GGN’s low-rank structure without further
approximations. It allows for efficient computation of eigenvalues, eigenvectors,
as well as per-sample first- and second-order directional derivatives. The repre-
sentation is computed in parallel with gradients in one backward pass and offers
a fine-grained cost-accuracy trade-off, which allows it to scale. As examples for
ViViT’s usefulness, we investigate the directional first- and second-order deriva-
tives during training, and how noise information can be used to improve the stability
of second-order methods.
1	Introduction & Motivation
The large number of trainable parameters in deep neural networks imposes computational constraints
on the information that can be made available to optimization algorithms. Standard machine learning
libraries (Abadi et al., 2015; Paszke et al., 2019) mainly provide access to first-order information in
the form of average mini-batch gradients. This is a limitation that complicates the development of
novel methods that may outperform the state-of-the-art: They must use the same objects to remain
easy to implement and use, and to rely on the highly optimized code of those libraries. There is
evidence that this has led to stagnation in the performance of first-order optimizers (Schmidt et al.,
2021). Here, we thus study how to provide efficient access to richer information, namely higher-order
derivatives and full statistics of the mini-batch loss.
Recent advances in automatic differentiation (Bradbury et al., 2020; Dangel et al., 2020) have
made such information more readily accessible through vectorization of algebraic structure in the
differentiated loss. We leverage and extend this functionality to efficiently access curvature in form of
the Hessian’s generalized Gauss-Newton (GGN) approximation. It offers practical advantages over
the Hessian and is established for training (Martens, 2010; Martens & Grosse, 2015), compressing
(Singh & Alistarh, 2020), or adding uncertainty to (Ritter et al., 2018b;a; Kristiadi et al., 2020)
neural nets. It is also linked theoretically to the natural gradient method (Amari, 2000) via the Fisher
information matrix (Martens, 2020, Section 9.2), and has been used to investigate the generalization
of neural networks (Jastrzebski et al., 2020; Thomas et al., 2020).
Traditional ways to access curvature fall into two categories. Firstly, repeated automatic differen-
tiation allows for matrix-free exact multiplication with the Hessian (Pearlmutter, 1994) and GGN
(Schraudolph, 2002). Iterative linear and eigensolvers can leverage such functionality to compute
Newton steps (Martens, 2010; Zhang et al., 2017; Gargiani et al., 2020) and spectral properties
(Sagun et al., 2017; 2018; Adams et al., 2018; Ghorbani et al., 2019; Papyan, 2019b; Yao et al., 2019;
Granziol et al., 2021) on arbitrary architectures thanks to the generality of automatic differentiation.
However, repeated matrix-vector products represent a critical factor for performance.
Secondly, Kronecker-factored approximate curvature (K-FAC) (Martens & Grosse, 2015; Grosse
& Martens, 2016; Botev et al., 2017; Martens et al., 2018) constructs an explicit light-weight
representation of the GGN based on its algebraic Kronecker structure. The computations are
streamlined via gradient backpropagation and the resulting matrices are cheap to store and invert.
1
Under review as a conference paper at ICLR 2022
This allows K-FAC to scale: It has been used successfully with very large mini-batches (Osawa et al.,
2019). One reason for this efficiency is that K-FAC only approximates the GGN’s block diagonal,
neglecting interactions across layers. Such terms could be useful, however, for other applications, like
uncertainty quantification with Laplace approximations (Ritter et al., 2018b;a; Kristiadi et al., 2020)
that currently rely on K-FAC. Moreover, due to its specific design for optimization, the Kronecker
representation does not become more accurate with more data. It remains a simplification, exact
only under assumptions unlikely to be met in practice (Martens & Grosse, 2015). This might be a
downside for applications that depend on a precise curvature proxy.
Here, we propose ViViT (inspired by V V > in Equation (3)), a vivid curvature model that leverages
the GGN’s low-rank structure. Like K-FAC, its representation is computed in parallel with gradients.
But it allows a cost-accuracy trade-off, ranging from the exact GGN to an approximation that has the
cost of a single gradient computation. Our contributions are as follows:
•	We highlight the GGN’s low-rank structure, and with it a structural limit for the inherent
curvature information contained in a mini-batch.
•	This low-rank structure allows for efficient computation of various GGN properties: The
exact eigenvalue spectrum, including eigenvectors, and per-sample directional derivatives.
They enable ViViT to model curvature noise in a mini-batch, in contrast to existing methods.
•	Approximations allow ViViT to flexibly trade off computational cost and accuracy. We
empirically demonstrate scalability on deep neural networks and provide a fully-featured
efficient implementation in PyTorch (Paszke et al., 2019) on top of the BackPACK
(Dangel et al., 2020) package.1
Using ViViT, we illustrate that noise in deep learning poses a challenge for the stability of second-
order methods and give a simple example how its quantities can be used to address this problem.
2	Notation & Method
Consider a model f : Θ × X → Y and a dataset {(xn, yn) ∈ X × Y}nN=1. For simplicity we use
N for both the mini-batch and training set size. The network, parameterized by θ ∈ Θ, maps a
sample Xn to a prediction yfn. Predictions are scored by a convex loss function ' : Y X Y → R
(e.g. cross-entropy or square loss), which compares to the ground truth yn . The training objective
L : Θ → R is the empirical risk
L(θ) = NN Pn=I '(f(θ, Xn), yn) .
(1)
We use `n(θ) = `(f (θ, Xn), yn) and fn(θ) = f(θ, Xn) for per-sample losses and predictions. For
gradients, we write gn(θ) = Vθ'n(θ) and g(θ) = VθL(θ), suppressing θ if unambiguous. We
also set Θ = RD and Y = RC with D, C the model parameter and prediction space dimensions,
respectively. For classification, C is the number of classes.
Hessian & GGN: Two-fold chain rule application to the split ` ◦ f decomposes the Hessian of
Equation (1) into two parts V2θL(θ) = G(θ) + R(θ) ∈ RD×D; the positive semi-definite GGN
G = N PN=1 (Jθfn)> (Vfn'n) (Jθfn) = N PL Gn
(2)
and a residual R = 1∕N PN=I PC=I (Vθ[fn]c) [Vfn'n). Here, We use the Jacobian Jab that
contains partial derivatives of b with respect to a, [Jab]j = ∂[b]i∕∂ [a]j. As the residual may alter
the Hessian,s definiteness - an undesirable property for many applications - we focus on the GGN.
Low-rank structure: By basic inequalities, Equation (2) has rank(G) ≤ NC.2 To make this
explicit, we factorize the positive semi-definite Hessian V2fn `n = PcC=1sncsn>c, where snc ∈ RC
and denote its backpropagated version by vnc = [Jθfn]>snc ∈ RD. Absorbing sums into matrix
multiplications, we arrive at the GGN’s outer product representation that lies at the heart of ViViT,
G = N Pn=I PC=1 VncvnC = vv> with V = √N (VII v12	... VNC) ∈ rd×nc .(3)
1Code available at https://github.com/PwLo3K46/vivit.
2We assume the overparameterized deep learning setting (NC < D) and suppress the trivial rank bound D.
2
Under review as a conference paper at ICLR 2022
V allows for exact computations with the explicit GGN matrix, at linear rather than quadratic memory
cost in D. We first formulate the extraction of relevant GGN properties from this factorization, before
addressing how to further approximate V to reduce memory and computation costs.
2.1	Computing the full GGN eigenspectrum
Each GGN eigenvalue λ ∈ R satisfies the characteristic polynomial det(G - λID) = 0 with
identity matrix ID ∈ RD×D . Leveraging the ViViT factorization of Equation (3) and the matrix
determinant lemma, the D-dimensional eigenproblem reduces to that of the much smaller Gram
matrix G = V> V ∈ Rnc×nc which contains pairwise scalar products of Vnc (See Appendix A.1),
det(G - λlD) =0 ⇔	(-λ)D-NC det(G -入小。)=0 .	(4)
With at least D - NC trivial solutions that represent vanishing eigenvalues, the GGN curvature is
flat along most directions in parameter space. Nontrivial solutions that give rise to curved directions
are fully-contained in the Gram matrix, and hence much cheaper to compute. For example, the left
panel of Figure 1a visualizes the full, exact GGN’s empirical spectral density on a mini-batch for a
deep convolutional neural net on CIFAR- 1 0. It reproduces the characteristics that have been reported
by numerous works, e.g. Sagun et al. (2018): An extensive amount of vanishing or small eigenvalues
and a small number of larger outliers.
Despite these various Hessian spectral studies which rely on iterative eigensolvers and implicit matrix
multiplication (Sagun et al., 2017; 2018; Adams et al., 2018; Ghorbani et al., 2019; Papyan, 2019b;
Yao et al., 2019; Granziol et al., 2021), we are not aware of works that extract the exact GGN
spectrum from its Gram matrix. In contrast to those techniques, this matrix can be computed in
parallel with gradients in a single backward pass, which results in less sequential overhead. In fact,
our approach allows for plots like Figure 1 to be efficiently live-monitored during training, which
may be interesting for practitioners that seek to better understand their model (Schneider et al., 2021).
Eigenvalues themselves can help identify reasonable hyperparameters, like learning rates. But we
can also reconstruct the associated eigenvectors in parameter space. These are directions along which
curvature information is contained in the mini-batch. Let S+ = {(λk, ek )∣λk = 0, Gek = λkek }3ι
denote the nontrivial Gram spectrum with orthonormal eigenvectors e>爸k = δjk (δ represents the
Kronecker delta and K = rank(G)). Then, the transformed set of vectors ek = 1∕√λkVe are
orthonormal eigenvectors of G associated to eigenvalues λk (see Appendix A.2),
~ ~_ _ _ _______________________ _________ ...
∀ (λk, ek)	∈	§+	：	Gek	= λkek	=⇒	GV ek	= λk V ek	.	(5)
The eigenspectrum provides access to the GGN’s pseudo-inverse based on V and S+, required by
e.g. second-order methods (see Section 4.2). As we show next, quadratic models defined through the
GGN naturally decompose along the eigenvectors from S+ = {(λk, ek) | λk 6= 0, Gek = λkek}kK=1.
2.2	Computing directional derivatives
Various algorithms update their solution by constructing a local quadratic approximation of the loss
landscape. For instance, optimization methods adapt their parameters by stepping to the local proxy’s
minimum. Such quadratic models are based on a second-order Taylor expansion and hence require
some form of curvature. Let q(θ) denote a quadratic model for the loss around position θt ∈ Θ that
uses curvature represented by the GGN,
q(θ) = const +(θ - θt)>g(θt) + 1(θ - θt)>G(θt)(θ - θt).	(6)
At its base point θt, the shape of q along an arbitrary normalized direction e ∈ Θ (i.e. kek = 1) is
determined by the local gradient and curvature. Specifically, the projection of Equation (6) onto e
gives rise to the first-and second-order directional derivatives
Ye = e>Vθq(θt) = e>g(θt) ∈ R,	(7a)
λe = e>Vθq(θt)e = e>G(θt)e ∈ R .	(7b)
As G’s characteristic directions are its eigenvectors, they form a natural basis for the quadratic model.
Denoting γk = γek and λk = λek the directional gradients and curvatures along eigenvector ek, we
see from Equation (7b) that the directional curvature indeed coincides with the GGN’s eigenvalue.
3
Under review as a conference paper at ICLR 2022
In addition to the mean gradient and curvature along an eigenvector ek, we can expand the sum over
samples from Equation (1) to obtain the per-sample contributions to each derivative. Let γnk and λnk
denote these first- and second-order derivatives contributions of sample xn in direction k, i.e.
Ynk = e>gn = ekɔλgn，	(8a)
λ TG p	e>V>%KrVek	kV>Vekk2	,8b
λnk - ek Gnek =	、	=	、	,	(Hb)
λk	λk
where Vn ∈ RD×C is the VIVIT factor of Gn corresponding to a scaled sub-matrix of V with fixed
sample index. Note that directional derivatives can be evaluated efficiently with the Gram matrix
eigenvectors without explicit access to the associated directions in parameter space. In Equation (7)
gradient g and curvature G are sums over gn and Gn , respectively. This structure also carries over to
the directional derivatives, i.e. γk = 1/N PnN=1 γnk and λk = 1/N PnN=1 λnk.
Access to per-sample directional gradients γnk and curvatures λnk along G’s natural directions is
a distinct feature of ViViT. Not only do these quantities provide geometric information about the
local loss landscape but also about its directional stochasticity over the mini-batch. Incorporating
such knowledge about the noise into algorithms that rely on quadratic models provides a promising
way to increase their performance and stability. In Section 4.2 we show how to use this information
to make second-order optimization methods more robust against noise.
2.3	Computational complexity
So far, we have formulated the computation of GGN eigenvalues (Equation (4)) including eigen-
vectors (Equation (5)) and per-sample directional derivatives (Equation (8)). Now, we analyze their
computational complexity in more detail to identify critical performance factors. Those limitations
can effectively be addressed with approximations that allow the costs to be decreased in a fine-grained
fashion. We evaluate their effectiveness on deep neural networks to demonstrate that ViViT scales.
Relation to gradient computation: Machine learning libraries are optimized to backpropagate sig-
nals 1/n V fn 'n and accumulate the result into the mini-batch gradient g = 1∕n PN=JJe fn]>V fn 'n
Each column vnc of V also involves applying the Jacobian, but to a different vector snc from the
loss Hessian’s symmetric factorization. For popular loss functions, like square and cross-entropy
loss, this factorization is analytically known and available at negligible overhead. Hence, computing
V basically costs C gradient computations as it involves N C backpropagations, while the gradient
requires N . However, the practical overhead is expected to be smaller: Computations can re-use
information from the Jacobians and enjoy significant additional speedup on parallel processors like
GPUs. Because our implementation relies on BackPACK’s vectorized Jacobians, we expect similar
run time performance as its second-order extensions which have the same backpropagation.
Stage-wise discarding V (GGN eigenvalues & directional derivatives): The columns of V
correspond to backpropagated vectors. During backpropagation, sub-matrices of V, associated
to parameters in the current layer, become available once at a time. As many of the above GGN
properties can be computed by contraction and accumulation of these stage-wise ViViT factors, they
can be discarded immediately. This allows for memory savings without any approximations.
NC
One example is the Gram matrix G formed by pairwise scalar products of {vnc}nN=,C1,c=1 in
O((NC)2D) operations. The spectral decomposition S+ has additional cost of O((NC)3). Simi-
larly, the terms for the directional derivatives in Equation (8) can be built up stage-wise: First-order
derivatives {γnk}nN=,K1,k=1 require the vectors {V Tgn ∈ RN C}nN=1 that cost O(N 2CD) operations.
Second-order derivatives are basically for free, as {VnT V ∈ Rc×nc}N=1 is available from G.
GGN eigenvectors: Raising one Gram matrix eigenvector ek to the GGN eigenvector ek through
application of V (Equation (5)) costs O(NCD) operations. However, repeated application of V
can be avoided for raising weighted sums of the form Pk (ck/√λk)ek with arbitrary weights Ck ∈ R.
The summation can be performed in the Gram space at negligible overhead, and only a single vector
Pk Ckek needs to be transformed. For instance, this allows for efficient aggregation of Newton steps
along directions in the Gram space before transforming them to parameter space (see Section 4.2).
4
Under review as a conference paper at ICLR 2022
Figure 1: Reducing costs with curvature sub-sampling, MC-sampling, and parameter groups.
(a) GGN eigenvalue distribution of the 3c3d architecture on CIFAR-10 (D = 895,210, C = 10)
for settings with different costs on a mini-batch of size N = 128 (Schneider et al., 2019). From left
to right: Exact GGN on the full batch, exact GGN on a batch fraction (1/8, as in Zhang et al. (2017)),
MC approximation of the GGN on the full batch. (b) Maximum batch size Ncrit on a GeForce RTX
2080 Ti (11 GB) for a standard gradient computation and the GGN spectrum and (c) computing exact
Newton steps with layer-wise parameter groups. More architectures and details in Appendix B.1.
2.4	Approximations & Implementation
Although the GGN’s representation by V has linear memory cost in D, it requires memory equivalent
to NC model copies. Of course, this is infeasible for many networks and data sets, e.g. ImageNet
(C = 1000). So far, our formulation was concerned with exact computations. We now present
approximations that allow N and C in the above cost analysis to be replaced by smaller numbers,
enabling ViViT to trade-off accuracy and performance.
Curvature sub-sampling & MC approximation: To reduce the scaling in C, we can approximate
the factorization Nfn'n(θ) = PC=I Sncs>c by a smaller set of vectors. One principled approach
is to draw MC samples {Snm} such that Em[SnmS>m] = Vfn'n(θ) as in Dangel et al. (2020).
This reduces the scaling of backpropagated vectors from C to the number of MC samples (1 in the
following). Another commonly used independent approximation to reduce the scaling in N is to
compute curvature only on a subset of mini-batch samples (Byrd et al., 2011; Zhang et al., 2017).
Parameter groups (block-diagonal approximation): Some applications, e.g. computing Newton
steps, require V to be kept in memory for performing the transformation into the parameter space.
Still, we can reduce costs by using the GGN’s diagonal blocks {G(i)}iL=1 of each layer, rather than
the full matrix G. Such blocks are available during backpropagation and can thus be used and
discarded step by step. In addition to the previously described approximations for reducing the costs
in N and C, this technique tackles scaling in D .
Concrete example & implementation details: To assess the quality of the above approximations,
Figure 1a shows the exact GGN eigenvalue spectrum (left) in comparison to its approximation
through curvature sub-sampling (center) and MC (right). Even though the amount of backpropagated
vectors is reduced by a factor of 8 and 10, respectively, the approximated spectra capture essential
features. We also tabularize the critical batch sizes Ncrit at which their computations experience
out-of-memory errors in Figure 1b. On a standard GPU, they exceed the traditional batch size used
for training, even for the exact scheme. Approximations further increase the applicable batch size.3
3The critical batch sizes in Figure 1b and c differ strongly for similar reductions of 8 and 10 in the number of
backpropagated vectors. This is because most neural networks have final layers with many parameters. During
initial stages of backpropagation, expanding V for these weights critically affects peak memory, and thus Ncrit.
This can be improved by leveraging structure in the Jacobian (see Appendix C.1).
5
Under review as a conference paper at ICLR 2022
As a concrete example for block-diagonal approximations, we group weights and biases layerwise
and compute the exact Newton step implied by the GGN’s block-diagonal approximation (similar to
Zhang et al. (2017)). Figure 1c shows the critical batch size and supports our approach’s scalability.
BackPACK’s functionality allows us to efficiently compute individual gradients and V in a single
backward pass, using either an exact or MC-factorization of the loss Hessian. To reduce memory
consumption, we extend its implementation with a protocol to support mini-batch sub-sampling and
parameter groups. By hooks into the package’s extensions, we can discard buffers as soon as possible
during backpropagation, effectively implementing all discussed approximations and optimizations.
3	Related work
GGN spectrum & low-rank structure: Other works point out the GGN’s low-rank structure.
Botev et al. (2017) present the rank bound and propose an alternative to K-FAC based on backpropa-
gating a decomposition of the loss Hessian. Papyan (2019a) presents the factorization in Equation (3)
and studies the eigenvalue spectrum’s hierarchy for cross-entropy loss. In this setting, the GGN
further decomposes into summands, some of which are then analyzed through similar Gram matrices.
These can be obtained as contractions of G, but our approach goes beyond them as it does not neglect
terms. We are not aware of works that obtain the exact spectrum and leverage a highly-efficient
fully-parallel implementation. This may be because, until recently (Bradbury et al., 2020; Dangel
et al., 2020), vectorized Jacobians required to perform those operations efficiently were not available.
Efficient operations with low-rank matrices in deep learning: Chen et al. (2021) use Equa-
tion (3) for element-wise evaluation of the GGN in fully-connected feed-forward neural networks.
They also present a variant based on MC sampling. This element-wise evaluation is then used
to construct hierarchical matrix approximations of the GGN. ViViT instead leverages the global
low-rank structure that also enjoys efficient eigen-decomposition and linear solves.
Another prominent low-rank matrix in deep learning is the un-centered gradient covariance (some-
times called empirical Fisher). Singh & Alistarh (2020) describe implicit multiplication with its
inverse4 and apply it for neural network compression, assuming the empirical Fisher as Hessian proxy.
However, this assumption has limitations, specifically for optimization (Kunstner et al., 2019). In
principle though, the low-rank structure also permits the application of our methods from Section 2.
4	Experiments
Different phases are encountered in the course of neural network training (Frankle et al., 2020). We
view those regimes in light of our newly accessible quantities and identify challenges for second-order
optimizers. Then, on a simple example, we use those quantities to stabilize such methods.
4.1	Noise during training
The interaction between gradient, curvature, and their stochasticity is crucial for the behavior of
optimization methods (Thomas et al., 2020). Here, we aim to identify characteristic features of
gradient noise, curvature noise, and their interaction to gain insights into the conditions under which
deep learning optimizers are operating. Training different architectures from the DeepOB S problem
set using the baselines from Dangel et al. (2020), we evaluate the GGN’s nontrivial eigenvectors
and per-sample directional derivatives on a fixed held-out mini-batch with ViViT. Figure 2 shows
results for the 3c3d architecture trained on CIFAR- 1 0 using SGD and cross-entropy loss. A broader
evaluation that includes more problems, training with Adam, and a description of procedural details,
is given in Appendix B.2. We make the following observations.
Directional gradient-curvature correlations: Figure 2a shows pairs of GGN eigenvalues λk and
the associated directional gradient magnitude ∣γk | at different training stages.5 Like for a quadratic
function, gradients and curvatures are positively correlated.
4For completeness, we describe implicit multiplication with the inverse GGN in Appendix C.2.
5We show ∣γk∣ because the directional gradient varies in sign, depending on the eigenvector,s orientiation.
6
Under review as a conference paper at ICLR 2022
(b)	100
亘
10-3
(c)
(d) 10-1
10-6
100
10-1
10-4	10-2	100	102 10-4	10-2	100	102 10-4	10-2	100	102
λk	λk	λk
Figure 2: Gradient, curvature and noise during training. Columns show the 3c3d architecture at
initialization (left), an early (epoch 5, center), and advanced (epoch 68, right) stage of training on
CIFAR- 1 0 with SGD (hyperparameters from Schneider et al. (2019); Dangel et al. (2020), details in
Appendix B.2). For each direction k, characterized by its curvature λk, we monitor (a) the directional
gradient magnitude; (b) gradient-eigenvector alignment; (c,d) SNRs of curvatures and gradients.
High gradient overlap with top eigenspace & shrinking of non-trivial eigenspace: Similar to
Gur-Ari et al. (2018), we observe high alignment between the mini-batch gradient and the GGN’s
top eigenspace. Specifically, We compute the normalized gradient overlap γk∕kgk2 with direction k.
Due to their additivity, gradient overlaps from multiple directions can be grouped by summation. The
histograms in Figure 2b show such overlaps: Each bin summarizes directions of similar curvature.
The accumulated normalized overlap of the gradient with these directions is shown as the height of
the histogram bar. The gradient aligns mostly with high-curvature directions. In particular, its overlap
with flat directions, 1 - PkK=1 γk2∕kgk2 , is smaller than the axis limits. We observe this alignment
throughout training even though the GGN’s active sub-space dimension K decreases.
Curvature signal vanishes during training & gradient signal is consistently small: To quantify
the noise in both the directional gradients and curvatures, we compute their signal-to-noise ratios
(SNR). It is given by the squared empirical mean divided by the empirical variance of the mini-batch
samples {λnk}nN=1 and {γnk}nN=1 for each non-trivial direction k.
Figures 2c,d show the evolution of both ratios. At early stages, the curvature signal along some of
the high-curvature directions and parts of the bulk dominates over the noise (SNR(λk) > 1), i.e.
curvatures of per-sample GGN s are similar. As training proceeds, the signal decreases until all
directions are dominated by noise (SNR(λk) < 1). In comparison, the directional gradients do not
exhibit such a pattern. They are always strongly corrupted by noise (SNR(γk) < 1), irrespective of
the curvature, even though their mean γk correlates with λk (Figure 2a).
On the one hand, the high overlap between gradient and the top GGN eigenspace encourages gradient
pre-conditioning with the GGN pseudo-inverse for second-order optimization, as negligible gradient
7
Under review as a conference paper at ICLR 2022
information will be projected out. On the other hand, however, noise in both the gradients and
curvatures, which can vary among directions and during training over several orders of magnitude,
represents a challenge for such methods. A naive Newton step based on the mean gradient and
curvature along each direction may be unstable due to large noise, eliminating all previously made
progress. We now turn to those methods and investigate how to improve their stability by equipping
them with a noise-aware adaptation strategy to act individually along directions.
4.2	Use-case: Improving damping in second-order methods
Spectral decomposition of Newton’s method into one-dimensional problems: Second-order
optimizers are based on a local quadratic approximation q (cf. Equation (6)) of the objective function.
A common approach to make such methods work in practice is to regularize the curvature matrix
with a damping term δID , 0 < δ ∈ R. The resulting quadratic is minimized by the update
∆θ = -(G + δID)-1g. In the GGN eigenvector basis from S+ and the trivial spectrum S0 =
{(λk, ek) | λk = 0, Gek = 0}kD=K+1 we observe different updates in the respective sub-spaces,
δθ = -(G + δID)Tg = - PK=I λγ+δek - PD=K+1 γkkek .	⑼
In the non-trivial eigenspace, ∆θ takes damped Newton-type steps, while flat directions are updated
with SGD at a learning rate δ-1. Motivated by Section 4.1 and Gur-Ari et al. (2018), we omit the
SGD update along flat directions in the following due to their negligible overlap with the gradient.
Global damping versus directional damping: Due to mini-batching, the gradient and curvature
of the quadratic model are corrupted by sub-sampling noise. Hence, the mini-batch averages
γk = 1/N PnN=1 γnk and λk = 1/N PnN=1 λnk in Equation (9) may deviate considerably from the
true values on the noise-free training loss. This noise can lead to overly large steps and thus cause the
optimization procedure to become unstable. Damping helps to increase stability as it decreases the
step size. However, its relative effect on the step length in a direction ek depends sensitively on the
scale of the curvature λk. Also, even at comparable curvature scale, stronger damping may be required
in the presence of high uncertainty over γk and λk . It is thus expected that treating all directions
identically yields diminishing returns along certain directions while it is required to keep uncertain
ones stable. This suggests a directional damping, i.e. an update of the form ∆θ = - PK=I 入第忆 ek ∙
Noise-aware directional damping via bootstrap: We use the quadratic mini-batch model’s loss
reduction R(δk) to assess the step Sk = - λγ⅛k ∈ R in direction ek,
R(δk ) = q(θ) - q(θ + Sk ek ) = -Sk ( Nn PN=I γnk) - 1 Sii(N PN=I λnk) .	(10)
From observations on a mini-batch, our goal is to minimize the training loss. Hence, we want to
choose a damping such that its corresponding update not only reduces q, but consistently decreases
the loss over all other mini-batch models as well. Ideally, one would compute directional derivatives
on additional samples for this purpose. Instead, we use a resampling technique based on the non-
parametric bootstrap (Efron, 1979) to simulate samples for R(δk) on other batches, only using
information contained in γnk, λnk (details in Appendix B). For a given damping δk, this provides
an indicator of what reduction in training loss to expect with the respective update. Taking the 5%
percentile of the bootstrap-generated samples, we obtain a confident lower bound to R(δk). We then
choose the δk that maximizes this lower bound from candidates on a discrete grid.
Evaluation on a noisy quadratic: We consider a quadratic loss function L(θ) = θ>G θ with
Gii = i2 for i ∈ {1,...,D = 20} and initialize θ at 100 ∙ 1d .At each step, the optimizer observes
unit vectors as directions {ek}kK=1, and noisy directional derivatives {γnk}nN=,K1,k=1, {λnk}nN=,K1,k=1.
Specifically, we design the noise to provide unbiased samples with constant variance (see Appendix B
for details). We compare our directional damping based on Equation (10) with constant damping and
SGD. The results reported in Figure 3 were obtained from multiple runs with different random seeds.
With constant damping, there is a trade-off between large steps that may cause instabilities (small
damping) and slower progress due to smaller steps (large damping). Both extremes are observed
in our experiment. For δ = 10-4, there are unstable runs, as indicated by the loss mean and the
maximum final distance to the minimum. For large constant damping, the behavior becomes more
8
Under review as a conference paper at ICLR 2022
∣∣θ — θminIl after 20 steps
Illlll
Relative damping δk∕k2
101
100
10-1
10-2
0 5 10 15 20
step
O
r
H
气
Trrr	〃	〃
卷 //	//	//	//	⅛	⅛
气 气 气 气
Figure 3: Comparison of optimizers on noisy quadratic. Statistics for different optimizers during
(left) and at the end (center) of training over 100 runs. For the loss values, we present mean (dashed
line), median (solid line) and a confidence interval (shaded area) between the lower and upper quartile.
Boxplot whiskers range from the distribution’s minimum to the maximum and boxes indicate lower
and upper quartile, enclosing the median. Horizontal black lines indicate the initial loss/distance.
Right: Relative dampings of our adaptive method. Darker shades indicate larger curvatures.
stable and increasingly resembles SGD. Compared to constant damping, the directional noise-aware
damping δk provides the smallest median final distance to the minimum with relatively low variance.
The right panel of Figure 3 shows the directional damping δk in relation to the underlying true
curvature k2 for a single run. We make two observations: Firstly, the relative damping increases over
time to compensate for the vanishing gradient signal as the optimization approaches the minimum.
Secondly, since constant noise is applied, the SNR in low curvature directions (yellow) is smaller
than in high-curvature directions (red). This is also reflected in the directional damping, which tends
to assign larger dampings to the noisier (yellow) directions. The directional bootstrap damping works
as expected and yields stable runs with consistently good final performance.
We provided evidence that ViViT’s quantities may be required for the improvement of second-order
methods. As the main contribution of this paper is in delivering these quantities, we deliberately
designed simplistic experiments — a full derivation and empirical evaluation would amount to a
separate paper of its own right (and would be incommensurate with the space limitations).
5 Conclusion
We have presented ViViT, a curvature model based on the low-rank structure of the Hessian’s
generalized Gauss-Newton approximation. This structure allows for efficient extraction of curvature
properties, such as the full eigenvalue spectrum and directional gradients and curvatures along
the associated eigenvectors. In contrast to alternatives, ViViT offers statistics of these directional
derivatives across the mini-batch, and thus a rich noise model.
We demonstrated the utility of these new quantities by studying noise characteristics of representative
deep learning tasks. We find that they pose challenges to the stability of second-order methods, and
showed, in a simplistic toy model, how ViViT can provide quantities to improve their stability.
ViViT’s representation is efficiently computed in parallel with gradients during a single backward
pass. As it mainly relies on vectorized Jacobians, it is even general enough to be integrated into
existing machine learning libraries in the future. For the moment, we provide an efficient open-source
implementation in PyTorch by extending the existing BackPACK library.
9
Under review as a conference paper at ICLR 2022
References
Martin Abadi, Ashish AgarWaL Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado,
Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey
Irving, Michael Isard, Yangqing Jia, Rafal JozefoWicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg,
Dan Man6, Rajat MOnga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit
Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi6gas,
Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFloW:
Large-scale machine learning on heterogeneous systems, 2015.
Ryan P. Adams, Jeffrey Pennington, Matthew J. Johnson, Jamie Smith, Yaniv Ovadia, Brian Patton, and James
Saunderson. Estimating the spectral density of large implicit matrices, 2018.
Shun-ichi Amari. Natural gradient works efficiently in learning. Neural Computation, 2000.
Aleksandar Botev, Hippolyt Ritter, and David Barber. Practical Gauss-Newton optimisation for deep learning.
In Proceedings of the 34th International Conference on Machine Learning, 2017.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, and
Skye Wanderman-Milne. JAX: composable transformations of Python + NumPy programs. 2020.
Richard H. Byrd, Gillian M. Chin, Will Neveitt, and Jorge Nocedal. On the use of stochastic Hessian information
in optimization methods for machine learning. SIAM Journal on Optimization, 2011.
Chao Chen, Severin Reiz, Chenhan D. Yu, Hans-Joachim Bungartz, and George Biros. Fast approximation
of the Gauss-Newton Hessian matrix for the multilayer perceptron. SIAM Journal on Matrix Analysis and
Applications, 2021.
Felix Dangel, Frederik Kunstner, and Philipp Hennig. BackPACK: Packing more into backprop. In International
Conference on Learning Representations, 2020.
Bradley Efron. Bootstrap methods: Another look at the jackknife. The Annals of Statistics, 1979.
Jonathan Frankle, David J. Schwab, and Ari S. Morcos. The early phase of neural network training. In
International Conference on Learning Representations, 2020.
Matilde Gargiani, Andrea Zanelli, Moritz Diehl, and Frank Hutter. On the promise of the stochastic generalized
Gauss-Newton method for training DNNs, 2020.
Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net optimization via Hessian
eigenvalue density, 2019.
Diego Granziol, Xingchen Wan, and Timur Garipov. Deep curvature suite, 2021.
Roger Grosse and James Martens. A Kronecker-factored approximate Fisher matrix for convolution layers,
2016.
Guy Gur-Ari, Daniel A. Roberts, and Ethan Dyer. Gradient descent happens in a tiny subspace, 2018.
Stanislaw Jastrzebski, Devansh Arpit, Oliver Astrand, Giancarlo Kerg, Huan Wang, Caiming Xiong, Richard
Socher, Kyunghyun Cho, and Krzysztof Geras. Catastrophic Fisher explosion: Early phase Fisher matrix
impacts generalization, 2020.
Agustinus Kristiadi, Matthias Hein, and Philipp Hennig. Being Bayesian, even just a bit, fixes overconfidence in
ReLU networks. In International Conference on Machine Learning, 2020.
Frederik Kunstner, Philipp Hennig, and Lukas Balles. Limitations of the empirical Fisher approximation for
natural gradient descent. In Advances in Neural Information Processing Systems, 2019.
James Martens. Deep learning via Hessian-free optimization. In Proceedings of the 27th International Conference
on Machine Learning, 2010.
James Martens. New insights and perspectives on the natural gradient method, 2020.
James Martens and Roger Grosse. Optimizing neural networks with Kronecker-factored approximate curvature.
In Proceedings of the 32nd International Conference on Machine Learning, 2015.
James Martens, Jimmy Ba, and Matt Johnson. Kronecker-factored curvature approximations for recurrent neural
networks. In International Conference on Learning Representations, 2018.
10
Under review as a conference paper at ICLR 2022
Kazuki Osawa, Yohei Tsuji, Yuichiro Ueno, Akira Naruse, Rio Yokota, and Satoshi Matsuoka. Large-scale
distributed second-order optimization using Kronecker-factored approximate curvature for deep convolutional
neural networks. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019.
Vardan Papyan. Measurements of three-level hierarchical structure in the outliers in the spectrum of deepnet
Hessians. In Proceedings of the 36th International Conference on Machine Learning, 2019a.
Vardan Papyan. The full spectrum of deepnet Hessians at scale: Dynamics with SGD training and sample size,
2019b.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary
DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. In Advances in
Neural Information Processing Systems 32. 2019.
Barak A. Pearlmutter. Fast exact multiplication by the Hessian. Neural Computation, 1994.
Hippolyt Ritter, Aleksandar Botev, and David Barber. Online structured Laplace approximations for overcoming
catastrophic forgetting. In Advances in Neural Information Processing Systems, 2018a.
Hippolyt Ritter, Aleksandar Botev, and David Barber. A scalable Laplace approximation for neural networks. In
International Conference on Learning Representations, 2018b.
Levent Sagun, Leon Bottou, and Yann LeCun. Eigenvalues of the Hessian in deep learning: Singularity and
beyond, 2017.
Levent Sagun, Utku Evci, V. Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of the Hessian of
over-parametrized neural networks, 2018.
Robin M. Schmidt, Frank Schneider, and PhiliPP Hennig. Descending through a crowded valley - benchmarking
deep learning optimizers, 2021.
F. Schneider, L. Balles, and P. Hennig. DeePOBS: A deeP learning oPtimizer benchmark suite. In 7th
International Conference on Learning Representations, 2019.
Frank Schneider, Felix Dangel, and PhiliPP Hennig. CockPit: A Practical debugging tool for training deeP neural
networks, 2021.
Nicol N SchraudolPh. Fast curvature matrix-vector Products for second-order gradient descent. Neural
computation, 2002.
Sidak Pal Singh and Dan Alistarh. WoodFisher: Efficient second-order aPProximation for neural network
comPression. In Advances in Neural Information Processing Systems, 2020.
Valentin Thomas, Fabian Pedregosa, Bart van Merrienboer, Pierre-Antoine ManzagoL Yoshua Bengio, and NiCo-
las Le Roux. On the interPlay between noise and curvature and its effect on oPtimization and generalization.
In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, 2020.
Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael Mahoney. PyHessian: Neural networks through the lens
of the Hessian, 2019.
Huishuai Zhang, Caiming Xiong, James Bradbury, and Richard Socher. Block-diagonal Hessian-free oPtimization
for training neural networks, 2017.
11