Under review as a conference paper at ICLR 2022
Evaluating Predictive Distributions:
Does Bayesian Deep Learning Work?
Anonymous authors
Paper under double-blind review
Abstract
Posterior predictive distributions quantify uncertainties ignored by point
estimates. This paper introduces The Neural Testbed, which provides tools
for the systematic evaluation of agents that generate such predictions. Cru-
cially, these tools assess not only the quality of marginal predictions per in-
put, but also joint predictions given many inputs. Joint distributions are of-
ten critical for useful uncertainty quantification, but they have been largely
overlooked by the Bayesian deep learning community. We benchmark sev-
eral approaches to uncertainty estimation using a neural-network-based
data generating process. Our results reveal the importance of evaluation
beyond marginal predictions. Further, they reconcile sources of confusion
in the field, such as why Bayesian deep learning approaches that generate
accurate marginal predictions perform poorly in sequential decision tasks,
how incorporating priors can be helpful, and what roles epistemic versus
aleatoric uncertainty play when evaluating performance. We also present
experiments on real-world challenge datasets, which show a high correlation
with testbed results, and that the importance of evaluating joint predictive
distributions carries over to real data. As part of this effort, we opensource
The Neural Testbed, including all implementations from this paper.
1 Introduction
Deep learning has emerged as the state-of-the-art approach across a number of application
domains in which agents learn from large amounts of data (LeCun et al., 2015). Neural
networks are increasingly used not only to predict outcomes but also to inform decisions.
Common approaches in deep learning produce point estimates but not uncertainty estimates,
which are often required for effective decision-making. Bayesian deep learning extends the
methodology to produce such uncertainty estimates (MacKay, 1992; Neal, 2012).
We consider agents that are trained on data pairs ((Xt, Yt+1) : t = 0, 1, . . . , T - 1) and
subsequently generate predictions given new inputs. When presented with an input XT , a
Bayesian neural network can generate a predictive distribution of the outcome YT +1 that is
yet to be observed. This distribution characterizes the agent’s uncertainty about YT+1. We
refer to such a prediction as marginal to distinguish it from a joint predictive distribution over
a list (YT +1 , . . . , YT+τ) of prospective outcomes corresponding to inputs (XT , . . . , XT+τ-1).
The importance of uncertainty estimation has motivated a great deal of research over recent
years (Kendall & Gal, 2017). This research has produced a variety of agents that learn to
generate predictive distributions. With this proliferation of alternatives, it is increasingly
important to analyze and compare their performance (Filos et al., 2019; Nado et al., 2021).
In this paper, we introduce new tools for systematic evaluation of such agents.
Our tools overcome several limitations faced by previous methods of evaluation. First, by
focusing purely on predictive distributions, we allow for a unified treatment of approaches
developed within the Bayesian neural network community and beyond. This sidesteps the
Open source code available at https://anonymous.4open.science/r/neural-testbed-B839.
1
Under review as a conference paper at ICLR 2022
question of whether any approach ‘is real ly Bayesian’ (Wilson & Izmailov, 2020). Second, our
tools evaluate the quality of higher-order joint predictions (τ > 1). Until now, the Bayesian
deep learning literature has focused almost exclusively on evaluating marginal predictions
(Wang et al., 2021). Finally, we develop a neural-network-based data generating process
for Bayesian deep learning that can be used to drive insight and algorithm development.
Where research has focused on a small set of challenge datasets, this might introduce bias
through overfitting via multiple iterations of algorithm development. We use these tools to
compare hundreds of agent variants. Further, we show that performance on our synthetic
data generating process data is highly correlated with performance on real-world challenge
datasets. We opensource all code used in this paper as The Neural Testbed.
Our results reconcile several sources of confusion in the field. One concerns why particular
approaches developed by the Bayesian deep learning community, such as Bayes-by-backprop,
dropout, and deep ensembles, perform poorly in sequential decision tasks despite faring
well based on evaluation metrics of that community (Osband et al., 2018). Our results
demonstrate that, while such methods produce accurate marginal predictions, they are no
longer competitive when it comes to high-order joint predictions. Joint predictions play a
critical role in sequential decision-making (Lu et al., 2021).
Another puzzling issue is that state-of-the-art methods do not employ domain-specific priors.
Whether Bayesian deep learning approaches should at all is a subject of controversy (Wenzel
et al., 2020). We show that the benefits of domain-specific priors can be pronounced when
evaluating high-order joint predictions, even where they are negligible for marginals.
We also help to resolve a point of philosophical debate within the deep learning community:
the importance of epistemic versus aleatoric uncertainty1 . The strangeness of this distinc-
tion has even made its way into wider popular culture, as satirized in the XKCD comic of
Figure 1 (Munroe, 2021). For a given parametric model, we can clearly distinguish param-
eter uncertainty from noise, or reducible from irreducible uncertainty. However, from the
perspective of a learning agent, the choice of model is subjective; different models can lead
to the same marginal predictions. Our formulation provides a clear and objective way to
assess the quality of predictive distributions, without reliance on this subjective distinction
between knowledge and chance. Crucially, we show that this can be judged via the quality
of joint predictions, but that marginals are not sufficient.
RESULAR ∪mceriaimty
EPlSTEMC UMCERffilNTY
OUR 5TUPr RXJMD
IHE DRUG UA5 74%
EFFECTIVE, UITH A
Confidemx interval
FROM 63% ID 81%.
OURSTUWRxWTHE
PRUG lð BE 74% EFFECTIVE.
HOUEVER IioEI IS A 1 IN H
CHAMCE Wir OUR STV"
UAS MODIFIED BY GEoRGE
IHE DAm IAMPBRER, UHOSE
UHM5 ARE UNPREDICTABIE
Figure 1: Epistemic or aleatoric? Does it matter?
It is worth mentioning another notable contribution of this work. The quality of a predictive
distribution is commonly assessed in terms of cross-entropy loss. While this measure is well-
defined for both marginal and joint predictions, to the best of our knowledge, the literature
has only addressed computation in the former case. For high-order joint predictions, the
straightforward approach would require computing sums over exponentially many values.
To render this computationally tractable, we developed a novel approximation algorithm
that leverages a random partitioning operation and Monte Carlo simulation. While this
approach is motivated by concepts from high-dimensional geometry (Kaski, 1998; Donoho,
2006), we leave its analysis as a topic for future theoretical research.
1	Epistemic uncertainty relates to knowledge (ancient Greek episteme-knowledge), as opposed
to aleatoric uncertainty relating to chance (Latin alea-dice) (Der Kiureghian & Ditlevsen, 2009).
2
Under review as a conference paper at ICLR 2022
2 Evaluating predictive distributions
In this section, we introduce notation for the standard supervised learning framework we
will consider (classification) as well as our evaluation metric (the KL-loss). We also explain
how we estimate the KL-loss for high-order joint predictions where exact computation is
infeasible, using random partitions and Monte Carlo simulation.
2.1	Kullback-Leibler loss
Consider a sequence of pairs ((Xt, Yt+1) : t = 0, 1, 2, . . .), where each Xt is a feature vector
and each Yt+1 is its target label. This sequence is i.i.d. conditioned on the environment
E , which produces the data, and which we view as a latent random variable. We consider
an agent that is uncertain about the environment and predicts class labels YT +1：T +T ≡
(YT+1, . . . , YT+T) given training data pairs DT ≡ ((Xt, Yt+1) : t = 0, 1, 2, . . . , T — 1) and
unlabelled feature vectors XT：T+T-1 ≡ (XT, . . . , XT+T-1). From the agent’s perspective,
each feature vector Xt is generated i.i.d from a fixed distribution P(Xt ∈ ∙), and each class
label 匕+ι is then drawn from P(匕 +ι ∈ ∙∣E,Xt).
We describe the agent’s predictions in terms of a generative model, parameterized by a
vector θT that the agent learns from the training data DT. For any inputs XT：T+T-1, θT
determines a predictive distribution, which could be used to sample imagined outcomes
YT +1：τ+τ. We define the Tth-order expected KL-loss by
dKl = E[dRL(P (YT +1：τ+τ ∈ ∙|E, Xt：τ+τ-1)11 P(YT +1:T + T ∈ ∙∖θT,xT:T+T-1))]	⑴
∙^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^^^^^^^^^^^^
environment likelihood
^^^^^^^^^^^^^^^^^^^
agent likelihood
=—E [log (P (YT +1：τ+T = YT +1：τ+τ∣ θτ, Xt：τ+τ-1, YT +1：T +τ	+ C,
S-----------------------------V------------------------------Z
cross-entropy loss ≡ negative log-likelihood
where C = E [log (P (YT+上T+"E,Xt：T+T-ι))] is independent of θτ. The expectation is
taken over all random variables, including the environment E, the parameters θτ, Xt：T+T-1,
and YT+1：T+T. Note that dTKL is equivalent to the widely used notion of cross-entropy loss,
though offset by a quantity that is independent of θT (Kullback & Leibler, 1951). For τ > 1,
dTKL assesses joint rather than the marginal predictions.
2.2	Marginal Versus Joint Predictions
Evaluating an agent’s ability to estimate uncertainty on joint instead of marginal predictions
can result in very different answers. We provide a simple example that illustrates the point.
Suppose the data ((Xt, Yt+1) : t = 0, 1, 2, . . .) is generated by repeated tosses of a possibly
biased coin with unknown probability p of heads.2 Let Xt = 0, to indicate that there is no
input, and let each outcome Yt+1 be 0 or 1 to indicate tails or heads, respectively. Consider
two agents that, without any training, predict outcomes. Agent 1 assumes p = 2/3 and
models the outcome of each flip as pure chance. Agent 2 assumes that the coin is fully
biased, meaning that p ∈ {0, 1}, but assigns probabilities 1/3 and 2/3 to 0 and 1.
Let Y +1 and Y+1 denote the outcomes imagined by the two agents. Despite their differing
assumptions, the two agents generate identical marginal predictive distributions: P(Yt +1 =
0) = P(Y+1 = 0) = 1 /3. On the other hand, joint predictions greatly differ for large T:
P( iYr1 = 0,.., Y = 0) = 1 / 3 T ≪ 1 / 3 = P( Y2 =0 ,...,YλT = 0).
We can say that agent 1 attributes all uncertainty to aleatoric sources and agent 2, epistemic
sources (although as Figure 1 alludes, there are many ways an agent can attribute sources
of uncertainty). Evaluating marginal predictions cannot distinguish between the two pos-
sibilities, though for a specific prior distribution over p, one agent could be right and the
other wrong. One must evaluate joint predictions to make this distinction.
2 We consider this coin as an illustrative model of more complex binary outcomes, such as whether
a user will click on an ad, or whether a given mortgage will default on payments.
3
Under review as a conference paper at ICLR 2022
When it comes to decision-making, this distinction can be critical (Lu et al., 2021). In a
casino, under the first agent’s assumption, there is large upside and little risk on repeatedly
betting on heads in the long run. However, if there is a 1/3 chance the coin will always land
tails, as is the case in the second agent’s prediction, there is a ruinous risk to repeatedly
betting heads. Evaluating joint predictions beyond marginals distinguishes these cases.
2.3 Computation of Kullback-Leibler loss
In contexts we will consider, it is not possible to compute dτKL exactly. As such, we will
approximate dτKL via Monte Carlo simulation. This section provides a high level overview of
our approach, we push the full details to Appendix A. Algorithm 1 outlines a basic approach
to estimating dτKL with respect to a synthetic data generating process. The algorithm
samples a set of environments and a training dataset for each environment. For each of
these pairs, the agent is re-initialized, trained, and then tested on N independent test data
τ -samples. Note that each test data τ -sample includes τ data pairs. For each test data
τ -sample, the likelihood of the environment is computed exactly, but that of the agent’s
belief distribution is approximated. The estimate of dτKL is taken to be the sample mean of
the log-likelihood-ratios (Algorithm 2).
Algorithm 1 KL-Loss Computation
1:	for j = 1, 2, . . . , J do
2:	sample environment and training dataset, and train agent
3:	for n = 1, 2, . . . , N do
4:	sample a test data τ -sample with τ feature-label pairs
5:	compute pj,n	> likelihood of environment
6:	compute Pj,n	> estimated likelihood of agent's belief distribution
7:	return JN EJ=1 EN=I log (pj,n/pj,n)	> estimated log-likelihood-ratio
While the likelihood of an environment can be efficiently computed, that of an agent’s belief
distribution poses a computational challenge. One approach is to estimate this likelihood
via Monte Carlo simulation (Algorithm 3). This produces unbiased estimates, which can be
accurate when τ is small. However, maintaining accuracy requires the number of samples
to grow exponentially with τ , as discussed in Appendix A.1. To overcome this challenge, we
propose a novel approach that estimates the likelihood of the agent’s beliefs via a combina-
tion of randomized partitioning and Monte Carlo simulation (Algorithm 4) (Kaski, 1998).
We conjecture that, under suitable regularity conditions, this novel approach produces accu-
rate estimates even when τ is large, but leave a formal analysis to future work. Even though
Algorithm 1 is developed for a synthetic data generating process, it is straightforward to
extend it to evaluate agents on real data. We outline our approach to real data in Section
5.1, with full details in Appendix A.2.
3	Benchmark agents
In this section we outline the baseline agents that we use to benchmark canonical approaches
to uncertainty estimation in deep learning. Table 1 links to papers that introduce these
agents, as well as the hyperparamters that we tuned to optimize their performance via
gridsearch. In each case, we attempt to match ‘canonical’ implementations, which we open
source at https://anonymous.4open.science/r/neural- testbed- B839.
In addition to these agent implementations, our opensource project contains all the evalua-
tion code to reproduce the results of this paper. Our code is written in Python and makes use
of Jax internally (Bradbury et al., 2018). However, our evaluation procedure is framework
agnostic, and can equally be used with any Python package including Tensorflow, Pytorch
or even SKlearn. Over the course of this paper, we have made extensive use of parallel
computation to facilitate large hyperparameter sweeps over many problems. Nevertheless,
the overall computational cost is relatively low by mo dern deep learning standards and relies
only on standard CPU. For reference, evaluating the mlp agent across all the problems in
4
Under review as a conference paper at ICLR 2022
agent	description	hyperparameters	I
mlp	Vanilla MLP	L2 decay
ensemble	‘Deep Ensemble' ( Lakshminarayanan et al., 201 )	L2 decay, ensemble size
dropout	Dropout ( al & Ghahramani, 2016 )	L2 decay, network, dropout rate
bbb	Bayes by Backprop (Blundell et al., 2015)	prior mixture, network, early stopping
Sgmcmc	Stochastic Langevin MCMC ( Welling & Teh, 2011)	learning rate, prior, momentum
ensemble+	Ensemble + prior functions ( sband et al., 2018)	L2 decay, ensemble size, prior scale, bootstrap
hypermodel	Hypermodel ( waracherla et al., 202 )	L2 decay, prior, bootstrap, index dimension
Table 1: Summary of benchmark agents, full details in Appendix B.
our testbed and real data requires less than 3 CPU-hours. We view our opensource effort
as one of the major contributions of this work. We provide clear and strong baselines, to-
gether with an objective and accessible method for assessing uncertainty estimates beyond
marginal distributions.
4	The Neural Testbed
In this section we introduce the Neural Testbed, a system for assessing and comparing agent
performance. The Testbed implements synthetic data generating processes and streamlines
the process of sampling data, training agents, and evaluating test performance by estimat-
ing KL-loss for marginal and high-order joint predictions. Since independent data can be
generated for each execution, the Testbed can drive insight and multiple iterations of algo-
rithm development without risk of overfitting to a fixed dataset. We begin by describing the
simple generative model based around a random 2-layer MLP. We then apply this testbed
to evaluate a comprehensive set of benchmark agents.
4.1	Synthetic data generating processes
By data generating process, we do not mean only the conditional distribution of data pairs
(Xt, Yt+1)|E but also the distribution of the environment E. The Testbed considers 2-
dimensional inputs and binary classification problems, although the generating processes
can be easily extended to any input dimension and number of classes. The Testbed offers
three data generating processes distinguished by a “temperature” setting, which signifies the
signal-to-noise ratio (SNR) regime of the generated data. The agent can be tuned separately
for each setting. This reflects prior knowledge of whether the agent is operating in a high
SNR regime such as image recognition or a low SNR regime such as weather forecasting.
To generate a model, the Testbed samples a 2-hidden-layer ReLU MLP with 2 output units,
which are scaled by 1/temperature and passed through a softmax function to produce class
probabilities. The MLP is sampled according to standard Xavier initialization (Glorot &
Bengio, 2010), with the exception that biases in the first layer are drawn from N(0, 1). The
inputs (Xt : t = 0, 1, . . .) are drawn i.i.d. from N(0, I). The agent is provided with the data
generating process as prior knowledge.
In Section 2.1, we described KL-loss as a metric for evaluating performance of an agent.
The Neural Testbed estimates KL-loss, with τ ∈ {1, 100}, for three temperature settings
and several training dataset sizes. For each value of τ , the KL-losses are averaged to produce
an aggregate performance measure. Further details concerning data generation and agent
evaluation are offered in Appendix A.
4.2	Performance in marginal predictions
We begin our evaluation of benchmark approaches to Bayesian deep learning in marginal
predictions (τ = 1). This setting has been the main focus of the Bayesian deep learning
literature. Despite this focus, it is surprising to see in Figure 2 that none of the bench-
mark methods significantly outperform a well-tuned MLP baseline according to d1KL . Of
course, there are many other metrics one might consider, but in this fundamental metric of
prediction quality, the mlp agent presents a baseline that is difficult to outperform.
5
Under review as a conference paper at ICLR 2022
tau
1
100
ι
0.8
0.6
mlp ensemble dropout
bbb hypermodel ensemble+ sgmcmc
wroEQS①-j>l P ①--roEJ0u
agent
Figure 2: Most Bayesian deep learning approaches do not significantly outperform a single
MLP in marginal predictions (T = 1). Once We examine predictive distributions beyond
marginals we see a clear difference in performance between our benchmark agents (T = 100).
For each T, the KL estimates are normalized by the KL of the MLP agent.
φaeERSφ0 pφz--euuou
fixed over testbed tuned per setting
agent hyperparametens
bootstrap
no
□yes
Figure 3: Agent robustness im-
proves with bootstrapping.
tau: ιoo
agent
→-ensemble
→-ensemble+
10	100 1000	1	10	100 1000
number Oftraininq points
Figure 4: The benefits of additive prior
clear in the high tau, low data regime.
functions are
One of the keys to this result is that all of the agents are able to tune their hyperparameters,
such as L2 weight decay, to the SNR regime and number of training points. This matches the
way deep learning systems are typically implemented in practice, with extensive hyperpa-
rameter tuning on validation data. This methodology has led many practitioners to doubt
the usefulness of automatic tuning procedures such as b ootstrap sampling (Nixon et al.,
2020). In Figure 3, we compare the performance of an ensemble+ agent that uses boot-
strapping with and without the ability to tune the hyperparameters per problem setting.
We see that bootstrap sampling is beneficial when the agent is expected to work robustly
over a wide range of problem settings. However, the benefits are no longer apparent when
the agent is allowed to tune its hyperparameters to individual tasks.
4.3	Performance beyond marginals
One of the key contributions of this paper is to evaluate predictive distributions beyond
marginals. In Figure 2, the red bars show the results of benchmark agents evaluated on
joint predictive distributions with τ = 100. Unlike when evaluating on marginal predic-
tions, where no method significantly outperforms a well-tuned MLP, the potential benefits
afforded by Bayesian deep learning become clear when examining higher-order predictive
distributions. Our results refute prior works’ claims that examining dτKL beyond marginals
provides little new information (Wang et al., 2021).
Figure 2 shows that sgmcmc is the top-performing agent overall. This should be reassuring
to the Bayesian deep learning community and beyond. In the limit of large compute this
agent should recover the ‘gold-standard’ of Bayesian inference, and it does indeed perform
best (Welling & Teh, 2011). However, some of the most popular approaches in this field
(ensemble, dropout) do not actually provide good approximations to the predictive distri-
bution in τ = 100. In fact, we see that even though Bayesian purists may deride ensemble+
and hypermodels as ‘not really Bayesian’, these methods actually provide much better ap-
proximations to the Bayesian posterior than ‘fully Bayesian’ VI approaches like bbb. We
6
Under review as a conference paper at ICLR 2022
note too that while sgmcmc performs best, it also requires orders of magnitude more com-
putation than competitive methods even in this toy setting (see Appendix C.2). As we
scale to more complex environments, it may therefore be worthwhile to consider alternative
approaches to approximate Bayesian inference.
For insight into where our top agents are able to outperform, we compare ensemble and
ensemble+ under the medium SNR regime in Figures 4 and 5. These methods are identical,
except for the addition of a randomized prior function (Osband et al., 2018). Figure 4 shows
that, although these methods perform similarly in the quality of their marginal predictions
(τ = 1), the addition of a prior function greatly improves the quality of joint predictive
distributions (τ = 100) in the low data regime. Figure 5 provides additional intuition into
how the randomized prior functions are able to drive improved performance. Figure 5a
shows a sampled generative model from our Testbed, with the training data shown in red
and blue circles. Figure 5b shows the mean predictions and 4 randomly sampled ensemble
members from each agent (top=ensemble, bottom=ensemble+). We see that, although the
agents mostly agree in their mean predictions, ensemble+ produces more diverse sampled
outcomes enabled by the addition of randomized prior functions. In contrast, ensemble
produces similar samples, which may explain why its performance is close to baseline mlp.
(a) True model.
(b) Agent samples: only ensemble+ produces diverse decision boundaries.
Figure 5: Visualization of the predictions of ensemble and ensemble+ agents.


5	Performance on real data
Section 4 provides a simple, sanitized testbed for clear insight to the efficacy of Bayesian deep
learning techniques. However, most deep learning research is not driven by these sorts of
synthetic generative models, but the ultimate goal of performing well on real datasets. In this
section, we apply the same benchmark agents to a selection of small challenge datasets. We
find that, on average, tuning agents for the synthetic problems leads to better performance
on real data. We also find that, just as the synthetic testbed, agents that perform similarly
in marginal predictions may be distinguished in the quality of their joint predictions.
5.1	Datasets
We focus on 10 benchmark datasets (3 feature-based, 7 image from pixels) drawn from the
literature including Iris, MNIST, and CIFAR-10 (TFD). This collection is not intended to
be comprehensive, or to include the most challenging large-scale problems, but instead to
represent some canonical real-world data that might reasonably be addressed with the MLP
models of Section 4.1. We apply a basic pre-processing step to each dataset, normalizing
input features and flattening observations. We push full details to Appendix D.1.
To assess performance in real datasets, we follow a similar procedure as Algorithm 1. The
only difference is that since it is impossible to compute the likelihood of environment for
real datasets, we compute the negative log-likelihood (NLL) rather than dτKL. Appendix A.2
provides further details. Note that NLL and dτKL are equivalent for agent comparison since
they differ by a constant (see Equation 1). Furthermore, to allow for more direct comparison
with the synthetic testbed, we also consider variants of each dataset where the number of
training pairs is limited to less than the ‘full’ dataset size.
7
Under review as a conference paper at ICLR 2022
5.2	Synthetic data is predictive of real data
Recall that Figure 2 compares performance across an array of agents, assessed using our syn-
thetic data generating process. Each agent’s hyperparameters were tuned by first enumerat-
ing a list of plausibly near-optimal choices and selecting the one that optimizes performance.
Each of our real-world datasets can be viewed as generated by an environment sampled from
an alternative data generating process. A natural question is whether performance on the
synthetic data correlates with performance on the real-world data.
The table of Figure 6a displays results pertaining to each of our agents. For each agent,
performance for each candidate hyperparameter setting was assessed on synthetic and real
data, and the correlation across these pairs is reported. The left and right columns restrict
attention to datasets with low and high volumes of training data, respectively. If a cor-
relation were equal to 1, the hyperparameter setting that optimizes agent performance on
real data would be identical to that on synthetic data. It is reassuring that the correla-
tions are high, reflecting a strong degree of alignment, with the exception of bbb in low data
regime, for which there appear to be pathological outcomes distorting performance for small
training sets. The values in parentheses express 5th and 95th percentile confidence bounds,
measured via the statistical bootstrap.
Figure 6b plots performance on real versus synthetic data for the high data regime. Each
data point represents one agent-hyperparameter combination. If the correlation were equal
to 1, the combination that performs best on the synthetic data would also perform best on
the real data. It is reassuring that the correlation is large, and the confidence interval be-
tween the 5th and 95th percentiles small. Agent-hyperparameter combinations that perform
better on the testbed tend to perform better on real data as well.
(b) Correlation in high data regime.
I agent	low data	high data
mlp	0.74 (0.57,0.85)	0.68 (0.38,0.99)
ensemble	0.72 (o.52,0.85)	0.63 (o.34,0.96)
dropout	0.77 (o.68,0.86)	0.78 (o.66,0.87)
bbb	-0.48 (-0.6,-0.35)	0.76 (o.68,0.83)
Sgmcmc	0.72 (0.53,0.85)	0.86 (0.79,0.92)
ensemble+	0.85 (o.63,0.98)	0.74 (0.3,0.97)
hypermodel	0.52 (o.17,0.76)	0.33 (0.03,0.59)
(a) Correlation by agent by data regime.
Figure 6: Performance on the Testbed correlates with performance on real datasets.
5.3 Higher order predictions and informative priors
Our synthetic testbed can be helpful in driving innovations that carry over to real data.
Section 5.2 indicated that performance on the Testbed is correlated with that on real-
world data. We now repeat the observation from Figure 4 on real data; additive prior
functions can significantly improve the accuracy of joint predictive distributions generated by
ensembles. We show this by comparing the performance of ensemble+ with different forms
of prior functions on benchmark datasets. We evaluate an ensemble with no prior function
(none), a random MLP prior (MLP), and a random linear function of a 2-dimensional latent
representation as the prior, trained via variational autoencoder (VAE) (Kingma & Welling,
2014). We provide full details in Appendix D.3.
Figure 7 plots the improvement in NLL for the ensemble agent relative to a baseline MLP
(lower is better), and breaks out the result for datasets=MNIST,Iris and τ = 1, 100. We
8
Under review as a conference paper at ICLR 2022
can see that the results for Iris mirror our synthetic data almost exactly. The results for
MNIST share some qualitative insights, but also reveal some important differences. For Iris
τ = 1 none of the methods outperform the MLP baseline, but for τ = 100 we see significant
benefits to an additive MLP prior in the low data regime. For MNIST τ = 1 we actually see
benefits to ensembles, even without prior functions and even in the high data regime. This
reveals some aspects of this real data that are not captured by our synthetic model, where
we did not see this behaviour. For τ = 100 the random MLP prior gives a slight advantage,
but the effect is much less pronounced. We hypothesize this is because, unlike the testbed,
the MLP prior is not well-matched to the input image data. However, the VAE prior is able
to provide significant benefit in the low data regime.3 These benefits also carry over to Iris,
even where random MLPs already provided signficant value. Designing architectures that
offer useful priors for learning agents is an exciting area for future work.
d1WTIN 1-qEωsuωTlN
-0.1
-0.2
-0.3
ensemble
prior
2 MLP
→-vae
一■— none
1	10	100	1
10	100	1 lei le2 le3 le4 1 Iel le2 le3 ie4
number of training points
Figure 7: Prior functions provide significant benefit in the high tau, low data regime, just
like the testbed. However, for image datasets random MLP priors provide relatively little
benefit. Unsupervised pretraining can help to design useful priors in high dimensional data.
6 Conclusion
This paper highlights the need to evaluate predictive distributions beyond marginals. In
addition to this conceptual contribution, we develop a suite of practical computational
tools that can evaluate diverse approaches to uncertainty estimation. Together with these
tools, we provide a neural-network-based data generating process that facilitates research
and iteration beyond a small set of challenge datasets. We package these together as The
Neural Testbed, including a variety of baseline agent implementations. We believe that this
represents an exciting and valuable new benchmark for Bayesian deep learning and beyond.
We have already used this testbed to generate several new insights in this paper. We have
shown many popular Bayesian deep learning approaches perform similarly in marginal pre-
dictions but quite differently in joint predictions. We reveal the importance of bootstrapping
for parameter robustness, and also help reconcile the observed lack of improvement when
tuned to specific datasets. We have shown that these insights from synthetic data can carry
over to real datasets; that performance in these settings is correlated, that agents with sim-
ilar marginal predictions can be distinguished by their joint predictions, and that suitable
prior functions can play an important role in driving good performance.
The results in this paper are in some sense preliminary. The grand challenge for Bayesian
deep learning is to provide effective uncertainty estimates in large, rich datasets. While we
have demonstrated benefits to predictive evaluation beyond marginals only in the ‘low data’
regime and small-scale problems, we believe that they will extend more broadly to situations
where new test inputs appear novel relative to training data. As such, we believe our core
insights should carry over to the related problems of nonstationarity and covariate shift that
plague modern deep learning systems. As an agent takes on more and more complex tasks,
it will continue to run into new and unfamiliar settings and uncertain outcomes; as such,
effective predictive distributions will be more important than ever.
3 We hypothesize that appropriately initialized convnet architectures may be able to leverage
image structure as noted in prior work (Ulyanov et al., 2018).
9
Under review as a conference paper at ICLR 2022
References
TensorFlow Datasets, a collection of ready-to-use datasets. https://www.tensorflow.org/
datasets.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight un-
certainty in neural network. In International Conference on Machine Learning, pp. 1613-
1622. PMLR, 2015.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and
Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL
http://github.com/google/jax.
Thomas Cover and Peter Hart. Nearest neighbor pattern classification. IEEE transactions
on information theory, 13(1):21-27, 1967.
Armen Der Kiureghian and Ove Ditlevsen. Aleatory or epistemic? does it matter? Structural
safety, 31(2):105-112, 2009.
David L Donoho. Compressed sensing. IEEE Transactions on information theory, 52(4):
1289-1306, 2006.
Vikranth Dwaracherla, Xiuyuan Lu, Morteza Ibrahimi, Ian Osband, Zheng Wen, and Ben-
jamin Van Roy. Hypermodels for exploration. In International Conference on Learning
Representations, 2020. URL https://openreview.net/forum?id=ryx6WgStPB.
Angelos Filos, Sebastian Farquhar, Aidan N Gomez, Tim GJ Rudner, Zachary Kenton,
Lewis Smith, Milad Alizadeh, Arnoud De Kroon, and Yarin Gal. A systematic compar-
ison of Bayesian deep learning robustness in diabetic retinopathy tasks. arXiv preprint
arXiv:1912.10481, 2019.
Jerome H Friedman. The elements of statistical learning: Data mining, inference, and
prediction. springer open, 2017.
Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing
model uncertainty in deep learning. In International Conference on Machine Learning,
2016.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedfor-
ward neural networks. In Proceedings of the 13th international conference on artificial
intel ligence and statistics, pp. 249-256, 2010.
Bobby He, Balaji Lakshminarayanan, and Yee Whye Teh. Bayesian deep ensembles via the
neural tangent kernel. arXiv preprint arXiv:2007.05864, 2020.
Matthew D Hoffman, Andrew Gelman, et al. The no-u-turn sampler: adaptively setting
path lengths in Hamiltonian Monte Carlo. J. Mach. Learn. Res., 15(1):1593-1623, 2014.
Samuel Kaski. Dimensionality reduction by random mapping: fast similarity computation
for clustering. 1998 IEEE International Joint Conference on Neural Networks Proceedings.
IEEE World Congress on Computational Intel ligence (Cat. No.98CH36227), 1:413-418
vol.1, 1998.
Alex Kendall and Yarin Gal. What uncertainties do we need in Bayesian deep learning for
computer vision? In Advances in Neural Information Processing Systems, volume 30,
2017.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. International
Conference on Learning Representations, 2014.
Solomon Kullback and Richard A Leibler. On information and sufficiency. The annals of
mathematical statistics, 22(1):79-86, 1951.
10
Under review as a conference paper at ICLR 2022
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable pre-
dictive uncertainty estimation using deep ensembles. In Advances in Neural Information
Processing Systems, pp. 6405-6416, 2017.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436,
2015.
Xiuyuan Lu, Ian Osband, Benjamin Van Roy, and Zheng Wen. Evaluating probabilistic
inference in deep learning: Beyond marginal predictions. CoRR, abs/2107.09224, 2021.
URL https://arxiv.org/abs/2107.09224.
David JC MacKay. A practical Bayesian framework for backpropagation networks. Neural
computation, 4(3):448-472, 1992.
Randall Munroe. Xkcd webcomic, 2021. URL https://m.xkcd.com/2440/.
Kevin P Murphy. Machine Learning: A Probabilistic Perspective. MIT Press, 2012.
Zachary Nado, Neil Band, Mark Collier, Josip Djolonga, Michael Dusenberry, Sebastian
Farquhar, Angelos Filos, Marton Havasi, Rodolphe Jenatton, Ghassen Jerfel, Jeremiah
Liu, Zelda Mariet, Jeremy Nixon, Shreyas Padhy, Jie Ren, Tim Rudner, Yeming Wen,
Florian Wenzel, Kevin Murphy, D. Sculley, Balaji Lakshminarayanan, Jasper Snoek, Yarin
Gal, and Dustin Tran. Uncertainty Baselines: Benchmarks for uncertainty & robustness
in deep learning. arXiv preprint arXiv:2106.04015, 2021.
Radford M Neal. Bayesian learning for neural networks, volume 118. Springer Science &
Business Media, 2012.
Jeremy Nixon, Balaji Lakshminarayanan, and Dustin Tran. Why are bootstrapped deep
ensembles not better? In ”I Can’t Believe It’s Not Better!”NeurIPS 2020 workshop, 2020.
Ian Osband and Benjamin Van Roy. Bootstrapped Thompson sampling and deep explo-
ration. arXiv preprint arXiv:1507.00300, 2015.
Ian Osband, John Aslanides, and Albin Cassirer. Randomized prior functions for deep
reinforcement learning. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31,
pp. 8617-8629. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/
8080- randomized- prior- functions- for- deep- reinforcement- learning.pdf.
Ian Osband, Zheng Wen, Mohammad Asghari, Morteza Ibrahimi, Xiyuan Lu, and Benjamin
Van Roy. Epistemic neural networks. arXiv preprint arXiv:2107.08924, 2021.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blon-
del, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,
M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825-2830, 2011.
Carl Edward Rasmussen. Gaussian processes in machine learning. In Summer school on
machine learning, pp. 63-71. Springer, 2003.
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In
International conference on machine learning, pp. 1530-1538. PMLR, 2015.
Bernhard Scholkopf and Alexander J Smola. Learning with kernels: Support vector ma-
chines, regularization, optimization, and beyond. MIT press, 2018.
Shengyang Sun, Guodong Zhang, Jiaxin Shi, and Roger Grosse. Functional variational
Bayesian neural networks. arXiv preprint arXiv:1903.05779, 2019.
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pp. 9446-9454, 2018.
11
Under review as a conference paper at ICLR 2022
Chaoqi Wang, Shengyang Sun, and Roger Grosse. Beyond marginal uncertainty: How
accurately can Bayesian regression models estimate posterior predictive correlations? In
International Conference on Artificial Intelligence and Statistics, pp. 2476-2484. PMLR,
2021.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient Langevin dynamics.
In Proceedings of the 28th international conference on machine learning (ICML-11), pp.
681-688. Citeseer, 2011.
Florian Wenzel, Kevin Roth, Bastiaan S Veeling, Jakub Swiatkowski, Linh Tran, Stephan
Mandt, Jasper Snoek, Tim Salimans, Rodolphe Jenatton, and Sebastian Nowozin.
How good is the Bayes posterior in deep neural networks really? arXiv preprint
arXiv:2002.02405, 2020.
Andrew Gordon Wilson and Pavel Izmailov. Bayesian deep learning and a probabilistic
perspective of generalization. arXiv preprint arXiv:2002.08791, 2020.
Max A Woodbury. Inverting modified matrices. Statistical Research Group, 1950.
12
Under review as a conference paper at ICLR 2022
A Testbed Pseudocode
We present the testbed pseudocode in this section. Specifically, Algorithm 2 is the pseu-
docode for our neural testbed, and Algorithm 3 and Algorithm 4 are two different ap-
proaches to estimate the likelihood of a test data τ -sample conditioned on an agent’s be-
lief. Algorithm 3 is based on the standard Monte-Carlo estimation, while Algorithm 4
adopts a random partitioning approach. The presented testbed pseudocode works for
any prior P(E ∈ ∙) over the environment and any input distribution PX, including
the ones described in Section 4.1. We also release full code and implementations at
https://anonymous.4open.science/r/neural-testbed-B839.
In addition to presenting the testbed pseudocode, we also discuss some core technical issues
in the neural testbed design. Specifically, Appendix A.1 discusses how to estimate the
likelihood ofan agent’s belief distribution; Appendix A.2 discusses how to extend the testbed
to agent evaluation on real data; finally, Appendix A.3 explains our choices of experiment
parameters.
Algorithm 2 Neural Testbed
Require: the testbed requires the following inputs
1.	prior distribution over the environment P(E ∈ ∙), input distribution PX
2.	agent fθ
3.	number of training data T , test distribution order τ
4.	number of sampled problems J, number of test data samples N
5.	parameters for agent likelihood estimation, as is specified in Algorithm 3 and 4
for j = 1, 2, . . . , J do
Step 1: sample environment and training data
1.	sample environment E 〜P(E ∈ ∙)
2.	sample T inputs X0, X1 , . . . , XT-1 i.i.d. from PX
3.	sample the training labels Y1 , . . . , YT conditionally i.i.d. as
Yt +1 〜P (Y ∈∙∣E ,X = Xt)	∀ t = 0,1,...,T - 1
4.	choose the training dataset as DT = {(Xt, Yt+1) , t = 0, . . . , T - 1}
Step 2: train agent
train agent fθT based on training dataset DT
Step 3: compute likelihoods
for n = 1, 2, . . . , N do
1.	sample XT(n), . . . , XT(n+)τ -1 i.i.d. from PX
2.	generate YT(n+)1 , . . . , YT(n+)τ conditionally independently as
Yt+n)〜P (Y ∈ ∙∣E ,X = Xtn)) ∀ t = T,T + 1 ,...,T + T - 1
3.	compute the likelihood under the environment E as
Pj,n = P (YT+)1：T+τ∣EXT+τ-ι) = ∏TTT Pr (Y(n)∣E,Xtn))
4.	estimate the likelihood conditioned on the agent’s belief
Pj,n ≈ P (YT +1:T + τ = K ++1：T+t| θT，XT:T+τ-1，YT +∖:T+τ)，
based on Algorithm 3 or 4 with test data τ -sample XTtn:T) +τ-1, YTtn+)1:T +τ .
return JN Ej=ι Σn=1 log (pj,n/pj,n )
13
Under review as a conference paper at ICLR 2022
Algorithm 3 Monte Carlo Estimation of Likelihood of Agent’s Belief
Require:
1.	trained agent fθT and number of Monte Carlo samples M
2.	test data T-sample (Xt：T+T-ι, YT +1：t+T)
Step 1: sample M models E1, . . . , EM conditionally i.i.d. from P
Step 2: estimate P as
(E ∈ ∙∣fθτ)
1M
P = M 工 P Y^Tτ+1： t+τ
m=1
YT +1:T+TIEm, XT:T+τ-1, YT +1:T+T)
return P
Algorithm 4 Estimation of Likelihood of Agent’s Belief via Random Partitioning
Require:
1.	trained agent fθT
2.	number of Monte Carlo samples M
3.	number of hyperplanes d
4.	test data τ -sample (XT:T+T-1, YT+1:T+T)
Step 1: sample M models £1,...,EM conditionally i.i.d. from P(E ∈ ∙∣fe「); for each
model m = 1, . . . , M, class k, and t = T, . . . , T + τ - 1, define
Pm,t,k =P(Zm)= k |£m,Xt),
and 0rm,t,k = Φ-1 (Pm,t,k), where Φ(∙) is the CDF of the standard normal function. For
each model m, define a vector
°m = [2m,T,1, ∙m,T,2,..., °m,T+T —1 ,K] ∈ 员
Step 2: sample a d × (Kτ) matrix A and a d-dimensional vector b, with each ele-
ment/component sampled i.i.d. from N(0, 1). For each m = 1, . . . , M, compute
ψm = 1 [A^m + b ≥ 0] ∈ {0, 1}d.
Step 3: partition the sampled models, with each cell indexed by ψ ∈ {0, 1}d and defined
by
Mψ = {m : ψm = ψ}
and assign a probability to each cell:
“ _ |Mψ I
qψ = MT
Step 4: ∀ψ ∈ {0, 1}d and ∀t = T, T + 1, . . . , T + τ - 1, estimate the probability of
predicting Yt+1 = k conditioned on the cell:
“…八=J ∣M17T E m ∈M ψ pm,t,k if M Ψ l> 0
pψ,t,k = t 1	if M ψ ∣ = 0
SteP 5: estimate Pr(Yt +1：T+T = Yt+1：T+TIθτ,Xt：T+t—1 ,Yt +1：T+T) as
T+T-1
P=	∑2 qψ ∏ Pψ,t,Υt+1
ψ∈{0,1}d	t=T
return P
14
Under review as a conference paper at ICLR 2022
A.1 Estimating Likelihood of Agent’s Belief Distribution
We have presented two algorithms to estimate the likelihood of a test data τ -sample condi-
tioned on a trained agent: Algorithm 3 is based on the standard Monte Carlo estimation,
while Algorithm 4 adopts an approach combining random partitioning and Monte Carlo
estimation. In this subsection, we briefly discuss the pros and cons between these two
algorithms, and provide some general guidelines on how to choose between them.
Algorithm 3 produces unbiased estimates of the likelihoods, which is usually accurate when
τ is small (e.g. for τ ≤ 10). However, maintaining accuracy might require the number of
samples M to grow exponentially with τ . The following is an illustrative example.
Example 1 (Uniform belief over deterministic models): Consider a scenario where
the number of class labels is K = 2. We say a model E is deterministic if for any feature
vector Xt ,
.ʌ . ʌ . ,
P(K+1 = I ∣ε,Xt) ∈ {0,1}.
Obviously, for any test data T -sample (Xr T+T-ι ,Yt +ι: T+T) with YT +ι: T+T ∈ {0, 1} T, under
a deterministic mo del ε , we have
P (YT +1：τ+T = YT +1：τ+T I E, Xt：T+t-1, YT +1：t+T) ∈ {0, 1}
When restricted to the inputs Xt：t+T-ι, there are 2τ distinguishable deterministic models.
Assume the agent's belief distribution is uniform over these 2τ distinguishable deterministic
models, then for any YT +1:T +τ ∈ {0, 1}τ, the likelihood of the agent's belief distribution is
P ( YYT +1: τ+T = YT +1:T+T I θT, XT:T+T-1, YT +1:T+T) = 2--
Now let's consider Algorithm 3. When a model Em is sampled from the agent's belief
distribution, with probability 2-τ,
P	YT+1:T+T
YT +1：T+T 1 Em,Xτ:T+T-1, YT +1：T+T) = 1,
and with probability 1 - 2-T
P
1：T+T
YT +1：T+T I Em, XT：T+T-1, YT +1：T+T)
0.
Consequently, in expectation, We need the number of Monte Carlo samples M = Ω(2T) to
ensure that the estimate P returned by Algorithm 3 is non-zero.
To overcome this challenge, we also propose a novel approach to estimate the likelihood of
agent's belief via a combination of randomized partitioning and Monte Carlo simulation,
as is presented in Algorithm 4. This approach proceeds as follows. First, M models are
sampled from the agent's belief distribution. For each sampled model, each test data input
Xt, and each class label k, a predictive probability pm,t,k and its probit Em,t,k = Φ- (pm,t,k)
are computed, where Φ(∙) is the CDF of the standard normal distribution. For each sampled
model, we also stack its probits into a probit vector 0∙m ∈ 泥KT. Then, d random hyperplanes
are sampled and used to partition 泥KT into 2d cells. Stacked probit vectors place models
in cells. Predictive distributions of models in each cell are averaged, and the likelihood
is calculated based on these averages, with each cell weighted according to the number of
models it contains.
The Neural Testbed applies Algorithm 4 with 2d《 M. Hence, some cells are assigned many
models. We conjecture that, under suitable regularity conditions, models assigned to the
same cell tend to generate similar predictions. If this is the case, this algorithm produces
accurate estimates even when τ is large. We leave a formal analysis to future work.
Finally, we briefly discuss how to choose between Algorithm 3 and Algorithm 4. As a rule of
thumb, we recommend to choose Algorithm 3 for τ < 10 and Algorithm 4 with the number
of hyperplanes d between 5 and 10 for τ ≥ 10.
15
Under review as a conference paper at ICLR 2022
A.2 Agent Evaluation on Real Data
Algorithm 2 (and its simplified version Algorithm 1) is developed for a synthetic data gener-
ating processes. We now discuss how to extend it to agent evaluation on real data. Consider
a scenario with J real datasets, and each dataset is further partitioned into a training dataset
and a test dataset. The main difference between this scenario and a synthetic data gener-
ating process is that we cannot compute the likelihood of environment for real data. Thus,
we compute the cross-entropy loss instead (see Equation 1). The computational approach is
similar to Algorithm 1: for each real dataset, we use its training dataset to train an agent.
Then, we sample N test data τ -samples from the test dataset, and estimate the likelihoods
of the agent’s belief distribution. The estimate of the cross-entropy loss is taken to be the
sample mean of the negative log-likelihoods.
Note that when ranking agents, the cross-entropy loss and dτKL will lead to the same order of
agents, since these two losses differ by a constant independent of the agent (see Equation 1).
A.3 Choices of Experiment Parameters
To apply Algorithm 2, we need to specify an input distribution PX and a prior distribution
on the environment P(E ∈ ∙). Recall from Section 4.1 that We consider binary classification
problems with input dimension 2. We choose PX = N(0, I), and we consider three envi-
ronment priors distinguished by a temperature parameter that controls the signal-to-noise
ratio (SNR) regime. We sWeep over temperatures in {0.01, 0.1, 0.5}. The prior distribution
P(E ∈ ∙) is induced by a distribution over MLPs with 2 hidden layers and ReLU activation.
The MLP is distributed according to standard Xavier initialization, except that biases in
the first layer are drawn from N(0, 2). The MLP outputs two units, which are divided
by the temperature parameter and passed through the softmax function to produce class
probabilities. The implementation of this generative model is in our open source code under
the path /generative/factories.py.
We now describe the other parameters we use in the Testbed. In Algorithm 2, we
pick the order of predictive distributions τ ∈ {1, 100}, training dataset size T ∈
{1, 3, 10, 30, 100, 300, 1000}, number of sampled problems J = 10, and number of testing
data τ -samples N = 1000. We apply Algorithm 3 for evaluation of d1KL and Algorithm 4
for evaluation of d1K0L0. In both Algorithms 3 and 4, we sample M = 1000 models from the
agent. In Algorithm 4, we set the number of hyperplanes d = 7. The specification of the
testbed parameters is in our open soucre code under the path /leaderboard/sweep.py.
On real datasets, we apply the same τ ∈ {1, 100}, N = 1000, and M = 1000. We set the
number of hyperplanes d = 10 in Algorithm 4.
16
Under review as a conference paper at ICLR 2022
B Agents
In this section, we describe the benchmark agents in Section 3 and the choice of vari-
ous hyperparameters used in the implementation of these agents. The list of agents in-
clude MLP, ensemble, dropout, Bayes by backprop, stochastic Langevin MCMC, ensem-
ble+ and hypermodel. We will also include other agents such as KNN, random forest,
and deep kernel, but the performance of these agents was worse than the other benchmark
agents, so we chose not to include them in the comparison in Section 4. In each case,
we attempt to match the “canonical” implementation. The complete implementation of
these agents including the hyperparameter sweeps used for the Testbed are available at
https://anonymous.4open.science/r/neural-testbed-B839. We make use of the Epis-
temic Neural Networks notation from (Osband et al., 2021) in our code. We set the de-
fault hyperparameters of each agent to be the ones that minimize the aggregated KL score
daKgLg = d1KL + d1K0L0/100.
B.1	MLP
The mlp agent learns a 2-layer MLP with 50 hidden units in each layer by minimiz-
ing the cross-entropy loss with L2 weight regularization. The L2 weight decay scale is
chosen either to be λT or λdτβ, where d is the input dimension, β is the tempera-
ture of the generative process and T is the size of the training dataset. We sweep over
λ ∈ {10-4, 10-3, 10-2, 10-1 , 1, 10, 100}. We implement the MLP agent as a special case of
a deep ensemble (B.2). The implementation and hyperparameter sweeps for the mlp agent
can be found in our open source code, as a special case of the ensemble agent, under the
path /agents/factories/ensemble.py.
B.2	Ensemble
We implement the basic “deep ensembles” approach for posterior approximation (Lakshmi-
narayanan et al., 2017). The ensemble agent learns an ensemble of MLPs by minimizing the
cross-entropy loss with L2 weight regularization. The only difference between the ensemble
members is their independently initialized network weights. We chose the L2 weight scale
to be either λMIT or λ dMβ, where M is the ensemble size, d is the input dimension, β is
the temperature of the generative process, and T is the size of the training dataset. We
sweep over ensemble size M ∈ {1, 3, 10, 30, 100} and λ ∈ {10-4, 10-3, 10-2, 10-1, 1, 10, 100}.
We find that larger ensembles work better, but this effect is within margin of error after 10
elements. The implementation and hyperparameter sweeps for the ensemble agent can be
found in our open source code under the path /agents/factories/ensemble.py.
B.3	Dropout
We follow Gal & Ghahramani (2016) to build a droput agent for posterior approxima-
tion. The agent applies dropout on each layer of a fully connected MLP with ReLU ac-
tivation and optimizes the network using the cross-entropy loss combined with L2 weight
decay. The L2 weight decay scale is chosen to be either 导(1 - Pdrop) or dTβl where Pdrop
is the dropping probability, d is the input dimension, β is the temperature of the data
generating process, and T is the size of the training dataset. We sweep over dropout
rate Pdrop ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8}, length scale (used for L2 weight decay)
l ∈ {0.01, 0.1, 0.3, 1, 3, 10}, number of neural network layers ∈ {2, 3}, and hidden layer
size ∈ {50, 100}. The implementation and hyperparameter sweeps for the dropout agent
can be found in our open source code under the path /agents/factories/dropout.py.
B.4	Bayes-by-backprop
We follow Blundell et al. (2015) to build a bbb agent for posterior approximation. We con-
sider a scale mixture of two zero-mean Gaussian densities as the prior. The Gaussian densi-
ties have standard deviations σ1 and σ2 , and they are mixed with probabilities P and 1 - P,
17
Under review as a conference paper at ICLR 2022
respectively. We sweep over σ1 ∈ {1, 2, 4}, σ2 ∈ {0.25, 0.5, 0.75}, p ∈ {0, 0.25, 0.5, 0.75, 1},
learning rate ∈ {10-3, 3 × 10-3}, number of training steps ∈ {500, 1000, 10000}, number of
neural network layers ∈ {2, 3}, hidden layer size ∈ {50, 100}, and the ratio of the complexity
cost to the likelihood cost ∈ {1, dy∕β}, where d is the input dimension and β is the tempera-
ture of the data generating process. The implementation and hyperparameter sweeps for the
bbb agent can be found in our open source code under the path /agents/factories/bbb.py.
B.5	Stochastic gradient Langevin dynamics
We follow Welling & Teh (2011) to implement a sgmcmc agent using stochastic gradient
Langevin dynamics (SGLD). We consider two versions of SGLD, one with momentum and
other without the momentum. We consider independent Gaussian prior on the neural net-
work parameters where the prior variance is set to be
where λ is a hyperparameter that is swept over {0.01, 0.1, 0.5, 1}, d is the input dimension,
β is the temperature of the data generating process, and T is the size of the training
dataset. We consider a constant learning rate that is swept over {10-5, 5 × 10-5, 10-4, 5 ×
10-4 , 10-3 , 5 × 10-3 , 10-2 }. For SGLD with momentum, the momentum decay term is
always set to be 0.9. The number of training batches is 5 × 105 with burn-in time of 105
training batches. We save a model every 1000 steps after the burn-in time and use these
models as an ensemble during the evaluation. The implementation and hyperparameter
sweeps for the sgmcmc agent can be found in our open source code under the path /agents/
factories/sgmcmc.py.
B.6	Ensemble+
We implement the ensemble+ agent using deep ensembles with randomized prior func-
tions (Osband et al., 2018) and bootstrap sampling (Osband & Van Roy, 2015). Sim-
ilar to the vanilla ensemble agent in Section B.2, we consider L2 weight scale to be
either λMMT or λdMβ. We sweep over ensemble size M ∈ {1, 3, 10, 30,100} and λ ∈
{10-4, 10-3, 10-2, 10-1, 1, 10, 100}. The randomized prior functions are sampled exactly
from the data generating process, and We sweep over prior scaling ∈ {0, yfβ, 1}. In addition,
we sweep over bootstrap type (none, exponential, bernoulli). We find that the addition
of randomized prior functions is crucial for improvement in performance over vanilla deep
ensembles in terms of the quality of joint predictions. We also find that bootstrap sampling
improves agent robustness, although the advantage is less apparent when one is allowed to
tune the L2 weight decay for each task (see Figure 3). The implementation and hyperpa-
rameter sweeps for the ensemble+ agent can be found in our open source code under the
path /agents/factories/ensemble_plus.py.
B.7	Hypermodel
We follow Dwaracherla et al. (2020) to build a hypermodel agent for posterior approxima-
tion. We consider a linear hypermodel over a 2-layer MLP base model. We sweep over
index dimension ∈ {1, 3, 5, 7}. The L2 weight decay is chosen to be either λT or λdTβ
with λ ∈ {0.1, 0.3, 1, 3, 10}, where d is the input dimension, β is the temperature of the
data generating process, and T is the size of the training dataset. We chose three different
bootstrapping methods of none, exponential, bernoulli. We use an additive prior which is a
linear hypermodel prior over an MLP base model, which is similar to the generating process,
with number of hidden layers in {1, 2}, 10 hidden units in each layer, and prior scale from
{0, yj^β, 1}. The implementation and hyperparameter sweeps for the hypermodel agent can
be found in our open source code under the path /agents/factories/hypermodel.py.
18
Under review as a conference paper at ICLR 2022
B.8	Non-parametric classifiers
K-nearest neighbors (k-NN) (Cover & Hart, 1967) and random forest classifiers (Friedman,
2017) are simple and cheap off-the-shelf non-parametric baselines (Murphy, 2012; Pedregosa
et al., 2011). The ‘uncertainty’ in these classifiers arises merely from the fact that they pro-
duce distributions over the labels and as such we do not expect them to perform well relative
to more principled approaches. Moreover, these methods have no capacity to model dτKL for
τ > 1. For the knn agent we swept over the number of neighbors k ∈ {1, 5, 10, 30, 50, 100}
and the weighting of the contribution of each neighbor as either uniform or based on distance.
For the randomforest agent We swept over the number of trees in the forest {10, 100, 1000},
and the splitting criterion which was either the Gini impurity coefficient or the information
gain. To prevent infinite values in the KL we truncate the probabilities produced by these
classifiers to be in the interval [0.01, 0.99]. The implementation and hyperparameter sweeps
for the knn and randomforest agents can be found in our open source code under the
paths /agents/factories/knn.py and /agents/factories/random_forest.py.
B.9	Gaussian process with learned kernel
A neural network takes input Xt ∈ X and produces output Zt+1 = W φθ (Xt) + b ∈ RK,
where W ∈ RK ×m is a matrix, b ∈ RK is a bias vector, and φθ : X → Rm is the output
of the penultimate layer of the neural network. In the case of classification the output
ΓZ	Ij Jl 1 ∙ j	j 1 1 111	-XT-	t ΓZ ∖ ∩∏l	1
Zt+ι corresponds to the logits over the class labels, i.e., Y +1 Y exp(Zt +1). The neural
network should learn a function that maps the input into a space where the classes are
linearly distinguishable. In other words, the mapping that the neural network is learning
can be considered a form of kernel (SChOlkOpf & Smola, 2018), where the kernel function k :
X ×X → R is simply k(X, X') = φθ(X)τφθ(X'). With this in mind, we can take a trained
neural network and consider the learned mapping to be the kernel in a Gaussian process
(GP) (Rasmussen, 2003), from which we can obtain approximate uncertainty estimates.
Concretely, let ①。：T-ι ∈ RT×m be the matrix corresponding to the φθ (Xt), t = 0,...,T — 1,
vectors stacked row-wise and let Φt：T+T-ι ∈ RT×m denote the same quantity for the test
set. Fix index i ∈ {0, . . . , K - 1} to be a particular class index. A GP models the joint
distribution over the dataset to be a multi-variate Gaussian, i.e.,
Z(i)
Z1:T
(i)
ZT +1:T +T
〜N
σ 21 + Φθ:T-@TT-1	Φ T: T+T-@T T-1
φ0:T-1φ T: T+T-1	φ t ： T+T-1φ T： T+T-1
where σ > 0 models the noise in the training data measurement and μ 1%, μT +i：T+T are
the means under the GP. The conditional distribution is given by
P(ZT +1：T+T | Z1:T, X0：T+T-1) = N (μT +1： T+T |1： T, ς T +1： t+T |1： t )
where
ΣT +1：T +T |1：T = φT：T+T-1φT：t+T-1 - φt：T+t-1φtT-1(σ2I + ①。：T-1φτT-1)-1屯0：T-1φT：T+T-1 ∙
and rather than use the GP to compute μT +1T+T∣0.T (which would not be possible since we
do not oberve the true logits) we just take it to be the output of the neural network when
evaluated on the test dataset. The matrix being inverted in the expression for ΣT +1：T +T |0：T
has dimension T × T, which may be quite large. We use the Sherman-Morrison-Woodbury
identity to rewrite it as follows (Woodbury, 1950)
ςT +1：T+T|0：T = φT：T+T-I(I - φtT-1(σ2I + ①。：T-1φtt-1)-1航：T-1)φT：T+T-1
=σ 2Φ t ： t+τ-1( σ2 I + φTT-1①。：T-1) 1 φ T： t+T-1,
which instead involves the inverse of an m × m matrix, which may be much smaller. If we
perform a Cholesky factorization of positive definite matrix (σ2 I + ΦτT-1 ①。：T-1) = LLT
then the samples for all logits simultaneously can be drawn by first sampling ζ ∈ Rm×K ,
with each entry drawn IID from N(0, 1), then forming
YT +1：T+T Y exp(μτ +1：T+T |1：T + σφT：T+T-1L-T Z) ∙
The implementation and hyperparameter sweeps for the deep_kernel agent can be found
in our open source code under the path /agents/factories/deep_kernel.py.
19
Under review as a conference paper at ICLR 2022
B.10	Other agents
In our paper we have made a concerted effort to include representative and canonical agents
across different families of Bayesian deep learning and adjacent research. In addition to
these implementations, we performed extensive tuning to make sure that each agent was
given a fair shot. However, with the proliferation of research in this area, it was not possible
for us to evaluate all competiting approaches. We hope that, by opensourcing the Neural
Testbed, we can allow researchers in the field to easily assess and compare their agents to
these baselines.
For example, we highlight a few recent pieces of research that might be interesting to evaluate
in our setting. Of course, there are many more methods to compare and benchmark. We
leave this open as an exciting area for future research.
•	Neural Tangent Kernel Prior Functions (He et al., 2020). Proposes a specific type
of prior function in ensemble+ inspired by connections to the neural tangent kernel.
•	Functional Variational Bayesian Neural Networks (Sun et al., 2019). Applies
variational inference directly to the function outputs, rather than weights like bbb.
•	Variational normalizing flows (Rezende & Mohamed, 2015). Applies variational in-
ference over a more expressive family than bbb.
•	No U-Turn Sampler (Hoffman et al., 2014). Another approach to sgmcmc that attempts
to compute the posterior directly, computational costs can grow large.
C Testbed results
In this section, we provide the complete results of the performance of benchmark agents on
the Testbed, broken down by the temperature setting, which controls the SNR, and the size
of the training dataset. We select the best performing agent within each agent family and
plot d1KL and d1K0L0 with the performance of an MLP agent as a reference. We also provide
a plot comparing the training time of different agents.
C.1 Performance breakdown
Figures 8 and 9 show the KL estimates evaluated on τ = 1 and τ = 100, respectively. For
each agent, for each SNR regime, for each number of training points we plot the average
KL estimate from the Testbed. In each plot, we include the “baseline” mlp agent as a black
dashed line to allow for easy comparison across agents. A detailed description of these
benchmark agents can be found in Appendix B.
C.2 Training time
Figure 10 shows a plot comparing the d1K0L0 and training time of different agents normalized
with that of an MLP. We can see that sgmcmc agent has the best performance, but at the
cost of more training time (computation). Both ensemble+ and hypermodel agents have
similar performance as sgmcmc with lower training time. We trained our agents on CPU
only systems.
D Real data
This section provides supplementary details regarding the experiments in Section 5. As
before, we include full implementation and source code at https://anonymous.4open.
science/r/neural- testbed- B839.
D.1 Datasets
Table 2 outlines the datasets included in our experiments. Unlike to the synthetic testbed,
which evaluates agents over a range of SNR regimes, these datasets are generally all high
20
Under review as a conference paper at ICLR 2022
SNR regime. We can see this since the top-performing agents in the literature are able to
obtain high levels of classification accuracy on held out data; something that is impossible
if the underlying system has high levels of noise.
dataset name	type	# classes	input dimension	# training pairs
iris	structured	3	4	120
Wine quality	structured	11	11	3,918
german credit numeric	structured	2	24	800
mnist	image	10	784	60,000
fashion-mnist	image	10	784	60,000
mnist-CorruPted/shot-noise	image	10	784	60,000
emnist/letters	image	37	784	88,800
emnist/digits	image	10	784	240,000
Cmaterdb	image	10	3,072	5,000
cifar10	image	10	3,072	50,000
Table 2: Summary of benchmark datasets used in the paper.
Each of these datasets is provided with a canonical training/test set of specific sizes. In
order to examine performance in different data regimes we augment the default settings of
Table 2 by also examining the performance of agents on these datasets with reduced training
data. In a way that mirrors the testbed sweep of Section 4.1, we also look at settings where
the training data is restricted to T = 1, 10, 100, 1000, 10000 data points respectively.
D.2 Correlation
Figure 6 breaks down the correlation in performance between testbeds and real data. For
the purposes of Table 6a we say that T = 1, 10 is the ‘low data’ regime, and the maximum
training dataset size is the ‘high data’ regime. Our results show that, for each agent, for
each data regime, performance of hyperparameters is correlated across settings.
One concern might be that while performance on real data overal l is highly correlated, that
this might not necessarily be the case for any individual dataset. Or, alternatively, that this
correlation is driven by extremely strong relationships in one dataset that are not present in
others. Figure 11 shows that this is not the case. In fact, for each of the datasets considered
we have strong and positive correlation over agent-hyperparameter pairs. This gives us
confidence that the results of Figure 6b are robust not only to choice of agent, but also to
some reasonable choice of datasets.
D.3 Prior functions
We consider two different forms of prior functions for ensemble+: a random MLP of the
input data and a random linear function of a 2-dimensional latent trained via variational
autoencoder (VAE) (Kingma & Welling, 2014). For the MLP prior, we tried both linear
(MLP with no hidden layer) and MLP with hidden layers, and observed that the linear prior
works better. To train the 2-dimensional latent, we considered a 2-layer (128, 64) MLP for
the Gaussian encoder and a 2-layer (64, 128) MLP for the Bernoulli decoder. We trained
the VAE using all unsupervised training data available for each dataset. After training the
VAE for 10,000 steps, we used the output mean of the Gaussian encoder as the latent.
21
Under review as a conference paper at ICLR 2022
I = nB*jcotυ*jBEi3stυ-Ql
temperature = 0.01
temperature = 0.1
temperature = 0.5
randomlforest
1	10	100	1000	1	10	100	1000	1	10	100	1000
Number of training points
Figure 8: Performance of benchmark agents on the Testbed evaluated on T = 1, compared
against the MLP baseline.
22
Under review as a conference paper at ICLR 2022
KL estimate on tau=100
temperature = 0.01
temperature = 0.1
temperature = 0.5
10	100 IOOO ι
10	100	1000
Number Oftraining points
1	10	100	1000
Figure 9: Performance of benchmark agents on the Testbed evaluated on T
against the MLP baseline.
100, compared
23
Under review as a conference paper at ICLR 2022
NLL on real data
0.8-
0.7-
0.6-
9
ωaπ5Llnttω-IM pωz="5E-ION
1	3	10	30
Average training time (x MLP training time)
Figure 10: Normalized KL vs training time of different agents
0-01 0.03 o.l 0-3

dropout
ensemble
ensemble+
hypermodel
mlp
sgmcmc
Figure 11: Correlation in high data regime for different datasets.
24