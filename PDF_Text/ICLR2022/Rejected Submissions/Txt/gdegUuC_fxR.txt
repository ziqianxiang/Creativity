Under review as a conference paper at ICLR 2022
Hessian-Free High-Resolution Nesterov Ac-
celeration for Sampling
Anonymous authors
Paper under double-blind review
Ab stract
Itis known (Shi et al., 2021) that Nesterov’s Accelerated Gradient (NAG) for opti-
mization differs from its continuous time limit (noiseless kinetic Langevin) when
its stepsize becomes finite. This work explores the sampling counterpart of this
phenonemon and proposes an accelerated-gradient-based MCMC method, based
on the optimizer of NAG for strongly convex functions (NAG-SC): we reformu-
late NAG-SC as a Hessian-Free High-Resolution ODE, change its high-resolution
coefficient to a hyperparameter, inject appropriate noise, and discretize the result-
ing diffusion process. Accelerated sampling enabled by the new hyperparame-
ter is quantified and it is not a false acceleration created by time-rescaling. At
continuous-time level, additional acceleration over underdamped Langevin in W2
distance is proved. At discrete algorithm level, a dedicated discretization is pro-
posed to simulate the Hessian-Free High-Resolution SDE in a cost-efficient man-
ner. For log-strong-Concave-and-smooth target measures, the proposed algorithm
achieves O(√d∕e) iteration complexity in W2 distance, same as underdamped
Langevin dynamics, but with a reduced constant. Empirical experiments are con-
ducted to numerically verify our theoretical results.
1	Introduction
Optimization is a major machinery that drives both the theory and practice of machine learning
in recent years. Since the seminal work of Nesterov (1983), acceleration has played a key role
in gradient-based optimization methods. A notable example is Nesterov’s Accelerated Gradient
(NAG), which is an instance of a more general family of “momentum methods”. NAG in fact
consists of multiple methods, including NAG-C and NAG-SC, respectively for convex and strongly
convex functions. Both of them provably converge faster than vanilla gradient descent (GD) in their
corresponding setups (Nesterov, 1983; 2013). Newer perspectives of acceleration continue to be
revealed, e.g., Su et al. (2014); Wibisono et al. (2016); Wilson et al. (2021); Hu & Lessard (2017);
Attouch et al. (2018); Shi et al. (2021), many based on the interplay between continuous and discrete
times. This work aims at turning NAG-SC into a sampler based on this interplay.
In fact, approaches for sampling statistical distributions, such as gradient-based Markov Chain
Monte Carlo (MCMC) methods, are also of great importance in machine learning, for example due
to their links to statistical inference and abilities to represent uncertainties lacking in optimization-
based methods. Although not entirely the same thing, optimization and sampling are closely related:
besides seeing a large class of sampling dynamics as optimization dynamics with additional noise,
viewing sampling as optimization in probability space is another profound perspective that led to
fruitful discoveries (e.g., Jordan et al. (1998); Liu & Wang (2016); Dalalyan (2017a); Wibisono
(2018); Zhang et al. (2018); Frogner & Poggio (2020); Chizat & Bach (2018); Chen et al. (2018a);
Ma et al. (2021); Erdogdu & Hosseinzadeh (2021)). In fact, an unadjusted Euler-Maruyama dis-
cretization of overdamped Langevin dynamics (abbreviated as OLD here) is commonly considered
as the analog of GD in sampling (although many other discretizations are also possible), and often
referred to as Unadjusted Langevin Algorithm (ULA) (Roberts et al., 1996) and/or Langevin Monte
Carlo (LMC). The convergence properties of the continuous dynamics of OLD, as well as asymptotic
and non-asymptotic analyses of its discretizations have been extensively studied (e.g., Roberts et al.
(1996); Villani (2008); Pavliotis (2014); Dalalyan (2017b); Durmus & Moulines (2016); Dalalyan
(2017a); Durmus et al. (2019a;b); Vempala & Wibisono (2019); Cheng & Bartlett (2018); Dwivedi
et al. (2019); Ma et al. (2019); Chewi et al. (2021); Erdogdu & Hosseinzadeh (2021)).
1
Under review as a conference paper at ICLR 2022
Meanwhile, the notion of acceleration is less quantified in sampling compared to that in optimiza-
tion, although attention has been rapidly building up. Along this direction, one line is based on
diffusion processes such as underdamped Langevin dynamics (ULD). For example, the convergence
and nonasymptotics of discretized ULD have been studied in Cheng et al. (2018); Dalalyan & Riou-
Durand (2020); Ma et al. (2021), and were demonstrated provably faster than discretized OLD in
suitable setups. These are not only great progresses but also forming perspectives complementary
to the extensive studies of the convergence of continuous ULD in the mathematical community (e.g,
Mattingly et al. (2002); Cao et al. (2019); Dolbeault et al. (2009; 2015); Villani (2009); Eckmann &
Hairer (2003); Baudoin (2017); Eberle et al. (2019)). Another important line of research is related
to accelerating particle-based approaches for optimization in probability spaces (Liu et al., 2019;
Taghvaei & Mehta, 2019; Wang & Li, 2019), although we note there is no clear boundary between
these two lines (e.g., Leimkuhler et al. (2018)). Additional interesting ideas also include Chen et al.
(2018b); Deng et al. (2020). In general, it has been known that adding an irreversible part to the
reversible dynamics of OLD1 accelerates its convergence (e.g., Hwang et al. (2005); Lelievre et al.
(2013); Ohzeki & Ichiki (2015); Rey-Bellet & Spiliopoulos (2015); Duncan et al. (2016)), and this
work can be viewed to be under this umbrella. Note, though, the discretization of an accelerated
continuous process is also important, and it will be analyzed.
Specifically, we propose an accelerated gradient-based MCMC algorithm termed HFHR. It is mo-
tivated by a simple question: how to appropriately inject noise to NAG algorithm in discrete time,
so that it is turned into an algorithm for momentum-accelerated sampling? Note we don’t add noise
to the learning-rate→ 0 limit of NAG (this has been well studied in Ma et al. (2021)), because a
finite-step-size discretization of this limiting ODE may not converge as fast as NAG with the same
learning rate. However, we will still use continuous dynamics as intermediate steps.
More precisely, our first step is to combine existing tools to prepare a non-asymptotic formulation
for the later steps. The goal is to better account for NAG’s behavior when a finite (not infinitesimal)
learning rate is used. As pointed out in Shi et al. (2021), a low-resolution limiting ODE (Su et al.,
2014), albeit being a milestone leading to a new venue of research (e.g, Wibisono et al. (2016)),
does not fully capture the acceleration enabled by NAG — for example, it can’t distinguish between
NAG and another momentum method of heavy ball (Polyak, 1964). The main reason is, the low-
resolution ODE describes the h → 0 limit of NAG, but in practice NAG uses a finite (nonzero) h.
High-resolution ODE was thus proposed to include additional O(h) terms to account for the finite h
effect (Shi et al., 2021). The original form of high-resolution ODE involves Hessian of the objective
function, which is computationally expensive to evaluate and store for high-dimensional problems,
but this difficulty can be overcome using techniques introduced in, e.g., Alvarez et al., 2002; Attouch
et al., 2020, which allows us to derive a High-Resolution and Hessian-Free limiting ODE for NAG.
Then we replace the high-resolution term’s coefficient in the HFHR ODE by a hyperparameter
α ≥ 0, and then add noise to the resulting ODE in a specific way, which turns it into an SDE
suitable for the sampling purpose. This SDE will be termed as HFHR dynamics.
To obtain an actual algorithm, the HFHR SDE is then discretized. We will see, both theoretically
and empirically, that nonzero α can lead to accelerated convergence of the sampling algorithm; this
acceleration is not an artificial consequence of time-rescaling, which would not give acceleration
after discretization with an appropriate step size. Meanwhile, note our discretization is just one of
the many possible schemes. It was known that high-order discretizations can improve statistical ac-
curacy and even the speed of convergence (see e.g., Chen et al. (2015); Li et al. (2019); Shen & Lee
(2019)), although such improvements often come with more computations per iteration. The dis-
cretization considered here is just a simple first-order scheme that uses one (full-)gradient evaluation
per step, but it better utilizes the structure of HFHR dynamics than Euler-Maruyama.
Our presentation will be structured as follows. After detailing the construction of HFHR, we will an-
alyze its convergence, at both the continuous level (HFHR dynamics) and the discrete level (HFHR
algorithm). For precise theoretical results, we will consider the setup of log-strongly-concave target
distributions, which are commonly considered in the literature (Kim et al., 2016; Bubeck et al., 2018;
Dalalyan, 2017b; Dalalyan & Riou-Durand, 2020; Dwivedi et al., 2019; Shen & Lee, 2019). The
additional acceleration of HFHR when compared to ULD in continuous time will be demonstrated
explicitly in Thm.5.1. For our discretized HFHR algorithm, a non-asymptotic error bound will
1For irreversible-acceleration not from OLD, see e.g., Bierkens et al. (2019); BouChard-Cote et al. (2018).
2
Under review as a conference paper at ICLR 2022
be obtained (Thm.5.2), which confirms that the additional acceleration in continuous time carries
through to the discrete territory. Finally, numerical experiments are provided, verifying the validity
and tightness of our theoretical results, and empirically showing HFHR remains advantageous for
the nonconvex and high-dimensional example of Bayesian Neural Networks.
The main contribution of this article is the idea of turning NAG-SC optimization algorithm into
a sampler, which also introduces a new dynamics that is neither overdamped or underdamped
Langevin. Nevertheless, theoretical analyses (e.g., Thm.5.2, Cor.5.4 & Rmk.5.5) and numerical
experiments (Sec.6) are also provided to quantify the effectiveness of this idea.
2	Background: Langevin Dynamics for Sampling
Consider sampling from Gibbs measure μ whose density is dμ = R 已-1(期)壮期e-f (x)dx, where f :
Rd 7→ R will be called the potential function. Two diffusion processes popular for sampling (and
modeling important physical processes too) are named after Langevin. One is overdamped Langevin
dynamics (OLD), and the other is kinetic Langevin dynamics (abbreviated as ULD to comply with
a convention of calling it underdamped Langevin). They are respectively given by
(OLD) dqt = -Vf(%)dt+√2dW t	(ULD) Idqt = Ptdt
[dPt = —YPtdt- Vf (qt)dt + √2γdBt
where qt , pt ∈ Rd, Wt , Bt are i.i.d. Wiener processes in Rd, and γ > 0 is a friction coeffi-
cient. Under mild conditions (e.g., PavliotiS (2014)), OLD converges to μ and ULD converges to
dπ(q,P) = dμ(q)ν(p)dp, where V(P) = (2π)-de-kpk2/2 , so its q marginal follows μ.
OLD and ULD are closely related. In fact, OLD is the γ → ∞ overdamping limit of ULD after
time dilation (e.g., Pavliotis (2014)). However, OLD is a reversible Markov process but ULD is
irreversible, and thus both their equilibrium and non-equilibrium statistical mechanics are different,
although closely related too. We will only focus on the convergence to statistical equilibrium (see
e.g., Souza & Tao (2019) for non-equilibrium aspects).
Many celebrated approaches exist for establishing the exponential convergence (a.k.a. geometric
ergodicity) of OLD, including the seminal work of Roberts et al. (1996), the ones using spectral
gap (e.g., Dalalyan, 2017b, Lemma 1), synchronous coupling (Villani, 2008, p33-35)(Durmus et al.,
2019b, Proposition 1), functional inequalities such as Poincare,s inequality (Pavliotis, 2014, The-
orem 4.4) and log Sobolev inequality (Vempala & Wibisono, 2019, Theorem 1). There are also
fruitful results for ULD, including the ones leveraging Lyapunov function (Mattingly et al., 2002,
Theorem 3.2), hypocoercivity (Villani, 2009; Dolbeault et al., 2009; 2015; Roussel & Stoltz, 2018),
coupling (Cheng et al., 2018, Theorem 5)(Dalalyan & Riou-Durand, 2020, Theorem 1)(Eberle et al.,
2019, Theorem 2.3), LSI (Ma et al., 2021, Section 3.1), modified Poincare,s inequality (Cao et al.,
2019, Theorem 1), and spectral analysis (Kozlov, 1989; Eckmann & Hairer, 2003).
The study of asymptotic convergence of discretized OLD dates back to at least the 1990s (Meyn
et al., 1994; Roberts et al., 1996). The non-asymptotic analysis of LMC discretization of OLD can
be found in Dalalyan (2017b) and it shows the discretization achieves error, in TV distance, in
O(d∕e2) steps. Subsequent results include O(%2) in W2 (Durmus & Moulines, 2016), O(d∕e) in
KL (Cheng & Bartlett, 2018), O (d/) in W2 under additional 3rd-order regularity (Durmus et al.,
2019b), and (O(√d∕e) in W2 under additional 3rd-order regularity (Li et al., 2021). For discretized
ULD, one has <O(√d∕e) iteration complexity in W2 (Cheng et al., 2018; Dalalyan & Riou-Durand,
2020) and (D(√d∕√ι) in KL (Ma et al., 2021). ULD is still generally conceived to be advantageous
over OLD and sometimes understood as its momentum-accelerated version. 3
3 Notations and Conditions
We will use 2-Wasserstein distance to quantify convergence, i.e.	W2(μ1,μ2)	=
(inf∏∈∏(μ1,μ2) E(X,γ)〜∏ ∣∣X — Y∣∣2) 2 where ∏(μι, μ2) is the set of all couplings of μι and μ2.
Assume WLOG that 0 ∈ argminx∈Rd f (x). The following condition will also be frequently used.
3
Under review as a conference paper at ICLR 2022
A 1. (Standard Strong-Convexity and Smoothness Condition) Function f ∈ C1(Rd) : Rd 7→ R is
m-stronly-convex and L-smooth, if there exist constants m, L > 0 such that ∀x, y ∈ Rd, we have
l∣Vf (y) - Vf (x)k ≤ Lky — x∣∣ and f(y) ≥ f (x) +(▽/(x), y — Xi + mm ∣∣y — x∣∣2
For f ∈ C2, this condition is equivalent to ml W V2f W LI.
4 The Construction of HFHR dynamics
HFHR is obtained by formulating NAG-SC as a Hessian free high-resolution ODE, lifting the high-
resolution term’s coefficient as a free parameter, and adding appropriate noises.
More precisely, let’s start with NAG-SC algorithm:
xk+1 = yk — SVf(yk)
yk+1 = xk+1 + C(xk+1 — xk)
(1)
(2)
where S is the learning rate (also known as step size), and C = 1-√H is a constant based on S and
the strong convexity coefficient m of f; the method also works for non-strongly-convex f though.
A high-resolution ODE description of Eq.(1) & (2) is obtained in (Shi et al., 2021, Section 2)
y+√s (1≡)+v2f(y)) y+τj+c Vfw) =0,
(3)
which can better account for the effect of non-infinitesimal S than the S → 0 limit (note C depends
on S). However, in this original form, Hessian of f is involved, which is expensive to compute and
store especially for high-dimensional problems.
To obtain a Hessian-free high-resolution ODE description of Equation (1) and (2), we first turn
the iteration into a ‘mechanical’ version by introducing position variable qk = yk and momentum
variable pk = (yk-xk)/h. Replacing xk+1 in (1) and the first xk+1 in (2) by qk+1 and pk+1, the
second xk+1 in (2) by qk — SVf (qk), and the xk in (2) by qk andpk, we obtain
qk+1 = qk + hpk+1 — SVf(qk)
Ipk+ι = CPk — CS Vfgk)
Now, choose Y, α and h as h = √cs, Y = 1-c, α = S. It is easy to see that γ > 0, α > 0, then
NAG-SC exactly rewrites as
qk+1 = qk + hpk+1 — hαVf(qk)
pk+1 = pk — hYpk — hVf(qk)
(4)
Note the technique for bypassing the Hessian without introducing any approximation is already well
studied in the literature (e.g., Alvarez et al. (2002); Attouch et al. (2020)).
So far, both h and α are actually determined by the hyperparameter s of NAG-SC. However, if we
now consider α as an independent variable (i.e., ‘lift’ it) and let h → 0, we see (4) is a 1st-order
discretization (with step size h) of the dynamics
[q = P — αVf (q)
Ip = —γ P — Vf (q)
(5)
Note a, if inherited from NAG-SC, should be a = ʌ/s/e = O(h), which, in a low-resolution ODE
will be discarded, and this eventually leads to ULD rather than HFHR. However, we now allow it to
be a free parameter and will see that α 6= O(h) can be advantageous.
Before quantifying these advantages later on, we finish the construction by appropriately injecting
Gaussian noises to Equation (5). This is just like how OLD can be obtained by adding noise to the
4
Under review as a conference paper at ICLR 2022
gradient flow. The right amount and structure of noise turn the ODE into a Markov process that can
serve the purpose of sampling, and the detailed form of our noise is given by:
[dqt = (Pt- OVf (qt))dt + √2αdWt
[dpt = JPt- Vf(qt∖)dt + √2γdBt
(6)
Here α ≥ 0, γ > 0 are constant parameters, and Wt , Bt are independent standard Brownian
motions in Rd. This irreversible process will be named as Hessian-Free High-Resolution(HFHR)
dynamics. We write it as HFHR(α, γ) to emphasize the dependence on α and γ when needed.
Substitution into Fokker-Planck PDE shows HFHR dynamics is unbiased (proof in Appendix B.1):
Theorem 4.1. π is the invariant distribution of HFHR described in Eq.(6), just like ULD.
5 Theoretical Analysis of the HFHR Dynamics/Algorithm
5.1	HFHR Dynamics in Continuous Time
Let’s establish the exponential convergence of HFHR dynamics and its additional acceleration when
compared to ULD, when the target measure has a strongly-convex and smooth potential.
Theorem 5.1. Assume Conditions A1 holds and further assume γ2 > L + m and α ≤ Y -L-m.
Denote the law of qt by μt. Then there exists K > 0 depending only on α and γ, such that
W2(μt,μ) ≤ Ke-Y+mα"W2(μ0,μ).
Detailed expression of κ0 can be found in Appendix A.
Thm. 5.1 state that HFHR converges to the target distribution exponentially fast in log-strongly-
concave and smooth setup. For ULD, Dalalyan & RioU-DUrand (2θ20, Theorem 1) obtained ex-
ponential convergence result in 2-Wasserstein distance with rate √κ+√κ-i using a simple and ele-
gant coUpling approach, and showed this rate is optimal as it is achieved by the bivariate fUnction
f (x, y) = mx2 + Ly2. In Thm 5.1, We use the same coupling approach to obtain an (asymptot-
ically) equivalent rate 2√K. Since ULD is HFHR(0,γ) and our bound agrees with existing result
when α = 0 and shows faster convergence for α > 0, the acceleration of HFHR in continuous time
is evidenced. For example, if we set Y = 2√L and push α to the upper bound specified in Thm. 5.1,
we obtain an O(√L) rate in the log-strongly-concave setup. Compared with the rate in Dalalyan &
Riou-Durand (2020), this is a speed-up of order κ.
5.2	HFHR Algorithm in Discrete Time
To obtain an implementable method, we now discretize the time of HFHR dynamics. As our main
goal is to show the acceleration enabled by α won’t disappear after discretization (unlike a fake
acceleration due to time rescaling), we’ll just analyze a 1st-order discretization (but a high-accuracy
discretization adapted from RMA (Shen & Lee, 2019) will also be provided, in Appendix F).
For simplicity, we’ll work with constant step size h. Inspired by Strang splitting for differential
equations (Strang, 1968; McLachlan & Quispel, 2002), we consider a symmetric composition for
updating over each time interval [kh, (k + 1)h]: Xk+ι := φh ◦ ψh ◦ φh (Xk) where Xk = Pkh , φ
and ψ correspond to solution flows of split SDEs, respectively given by
φ . fdq = Pdt
1 dp = —γ pdt + √2γdB
: Jdq = -αVf(q)dt + √2OdW
dP = -Vf (q)dt
and φt(X0) and ψt(X0) mean x’s value after evolving φ and ψ for t time with initial condition X0.
Note that φ flow can be solved explicitly since the second equation is an Ornstein-Unlenbeck process
and integrating the second equation followed by integrating the first one gives us an explicit solution
5
Under review as a conference paper at ICLR 2022
Iqt = qo + I--Yt Po + √2γ R01-e-γ(t-s) dB(S),
Ipt = e-γtPo + √2γ Ro e-γ J)dB(s).
(7)
For an implementation of the stochastic integral part in Equation 7, denoting X =
√2γ R0 1-e -Y(Ss dB(s) and Y = √2γ Rt e-γ(t-s)dB(s), and the covariance matrix of (X, Y)
is Cov(X,Y)
γh+4e-γ2 -e-γh-3
γ2~h
(I--Y^ Id
(1-e-γ2 )2 Id
Y . As mean and covariance fully determine
(1 - e-γh)Id
X
Y
a Gaussian distribution,
Mξ where M is the Cholesky decomposition of Cov(X, Y), ξ is
a 2d standard Gaussian random vector, i.i.d. at each step, and φt can thus be exactly simulated.
However, ψ flow is generally not explicitly solvable unless f is a quadratic function in q. We
simply choose to approximate ψh (xo) with one-step Euler-Maruyama integration ψh (xo) ≈
ψeh(xo) given by Jqh = q0	£f (q0)h + '2αh"	where η isa standard d-dimensional Gaus-
[Ph = Po -Vf (qO)h
sian random vector, again i.i.d. each time ψ is called.
Altogether, one step of an implementable Strang,s splitting of HFHR is hence φ2 ◦ ψeh ◦ φ2 and We
call this numerical scheme the HFHR algorithm, which is summarized in Algorithm 1.
Algorithm 1 HFHR Algorithm
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
Input: potential function f and its gradient Vf, damping coefficients α and γ, step size h,
initial condition (qo , Po )
procedure DISCRETIZED HFHR(f, Vf, α, γ, h, qo , Po)
k = 0 and initialize qo
Po
while not converge do
Generate independent standard Gaussian random vectors ηk+1 ∈ Rd, ξ1k+1, ξ2k+1 ∈ R2d
Run φ 2 : q1 =
P1
Run ψeh : q2 =
P2
Run φ 2 ： q3 =
P3
q(k + 1)h . q3
P(k+1)h	P3
k - k + 1
end while
end procedure
qkh + 1-e-γ2 Pkh
-- — „
e γ 2 Pkh
+ Mξ 1k+1
q1 - αVf(qJh + √2θhηk+1
P1 - Vf(q1)h
,1-e-γ2
q2 + —he-- p2	+ M ξk + 1
- ∖∙-
As ψ in Strang splitting is replaced by a 1st-order approximation ψ , the method is of order 1, however
with good constant. This is rigorously established by the following theorem (interested readers are
referred to Appendix.D.5-D.7 and Li et al. (2021) for more technical details):
Theorem 5.2. Under Assumption 1, we further assume Y — L+m ≥ mɑ and the function V∆f
satisfies a third-order growth condition, i.e., V∆f (q) ≤ G 1 + kqk2, ∀q ∈ Rd for some G > 0.
If (qo, Po)〜∏o ,then there exists ho, C > 0 such that when 0 < h < ho, we have
W2(μk,μ) ≤ √2κ0e-(m+mα)khW2(∏o,π) + √2Ch	⑻
where κ0 is a constant depending only on L, m, γ, α (see AppendixAfor detail), μk is the law ofthe
q marginal ofthe k-th iterate inAlgorithm 1, and μ is the q marginal ofthe invariant distribution π.
6
Under review as a conference paper at ICLR 2022
In particular, C = O(√d) and there exists b > 0, independent of a and is oforder O(√d), s.t.
C ≤ b-(
—
α2
(9)
m
Remark 5.3. The linear growth (at infinity) condition on VAf is actually not as restrictive as
it appears. For example, for monomial potentials, i.e., f(x) = xp, p ∈ Z+, our linear growth
condition is met when p ≤ 4, whereas a standard condition (Pavliotis, 2014, Theorem 3.1) for the
existence of SDE solutions holds only when p ≤ 2. In addition, our condition is related to the
Hessian Lipschitz condition commonly used in the literature (e.g., Durmus et al. (2019b); Ma et al.
(2021)). Smoothness and Hessian Lipschitzness imply our condition. Meanwhile, examples that
satisfy linear growth condition but are not Hessian Lipschitz exist, e.g., f(x) = x4, and thus linear
growth condition is not necessarily stronger than Hessian Lipschitzness.
Inspecting the role of α in Equation (8), we see that α clearly increases the rate of exponential
decay, but at the same time it can also increase the discretization error (see (9); assuming h is fixed).
However, as the following Corollary 5.4 and its remark will show, the net effect of having a positive
α > 0, at least for some α?, is reduced iteration complexity.
Corollary 5.4. Consider the same assumption as in Thm. 5.2. If (q°,p0)〜∏o, then there exists
ho, C > 0 (same as that in Theorem 5.2; recall C = O(√d)) such thatfor any target error tolerance
e > 0, if we choose h = h?，min{ho, 2√c }, then for E < 2√2Ch0, after
L C 1
k? = 2√2 m-——log
m + ma E
γ
2√2κ0W2(∏0,∏) = o
(10)

steps, we have W2(μk, μ) ≤ e.
Remark 5.5. Recallfrom Thm.5.2 that C ≤ 导(α2 一 Y + γ2), so ifwe consider the minimizer a? of
C	■ C	?	∙	b α2-Y+ 712	、/3—1 G.	,	,	.
anupperboundof m：mα,α? = argmιnɑ≥o m—1+@ =	1. This suggests that by choosing
an optimal α > 0, one could effectively reduce iteration complexity. Note, however, that this α? may
not be the true optimal one as bounds may not be tight. If they were, kα? = (2√3 - 3)kα=0 ≈
0.46kα? =0; i.e., steps needed by ULD (discretized by Alg.1 with α = 0) can be halved by HFHR.
Rmk.5.5 shows HFHR algorithm can lead to a similar bound on iteration complexity as ULD algo-
rithm but with an improved constant, and thus is a more efficient algorithm. This improvement also
shows that the acceleration of HFHR can be carried through from continuous time to discrete time.
The same conclusion has been consistently observed in numerical experiments too.
Remark 5.6. Readers interested in more explicit condition number dependence are referred to Ap-
pendix E, where we show, for 2D Gaussian target with condition number κ 1, the convergences
of Euler discretization of ULD under optimal parameters and HFHR under suboptimal parameters
are, respectively, given by (1 — 1∕κ + o(1∕κ))n and (1 — 2∕κ + o(1∕κ))n, where n is the number
of iterations. The latter (HFHR) is faster despite that its hyperparameters may not be optimal. 6
6 Numerical Experiments
We now empirically study the acceleration enabled by α 6= 0 by comparing HFHR algorithm and
the popular KLMC discretization of ULD (Dalalyan & Riou-Durand, 2020). For fairness, discretiza-
tions of the same order and cost are compared (Appendix F has an additional comparison).
6.1 Verification of theoretical results in Section 5.2
This subsection numerically verifies the d and h dependence of HFHR algorithm as well as the
genuine acceleration enabled by α. For this purpose, we will not use Gaussian targets, because
otherwise HFHR will decouple across different (orthogonal) dimensions, and hence its discretization
error having a O(√d) dependence would just be a consequence of using 2-Wasserstein distance for
quantifying statistical accuracy. To inspect a more interesting example, we consider a potential that
is no longer additive across different dimensions, namely f (x) = log (ex1 +-+ exd) + 2 ∣∣x∣∣2.
7
Under review as a conference paper at ICLR 2022
It’s not hard to see that all dimensions will be coupled in the HFHR dynamics (and ULD too).
Moreover, the new potential f is still a strongly convex function and satisfies the assumption
in Theorem 5.2. When the target measure is non-Gaussian, we no longer have a closed form
expression for 2-Wasserstein distance and it is computationally expensive to approximate the 2-
Wasserstein distance by samples. Therefore, we use the error of mean instead as a surrogate because
∣∣Eμk q - E*q∣∣ ≤ W2(μk, μ) and hence the bound in Equation (8) also applies to the error in mean,
and so does the iteration complexity bound in Equation (10).
Theorem 5.2 says the final sampling error is upper bounded the discretization error that is linear in
h and √d. To numerically verify the linear dependence on h, we work with d = 2 and ran ULD
algorithm for sufficiently long time with tiny step size (h = 0.0005) to obtain 108 independent
realizations and use them (as benchmark) to empirically estimate E*q. We then set Y = 2, α = 1
and use Algorithm 1, with h ∈{2k∣- 7 ≤ k ≤ 0}. For each h, we run T (with T = 50) iterations
in Algorithm 1 to ensure the Markov chains are well-mixed and the contribution to final error from
exponential decay is order-of-magnitude smaller than discretization error. The results are plotted in
Figure 1a. The observed linear dependence on the step size h is consistent with Theorem 5.2.
cQ-∙β-5
111
Z=XdlXS 直一
io-ɪ
Step size
ɪ C
Z=XdIXar =
ιo∙	ιo∙	a/	ιβ>
Dimension
(a) Linear dependence of (b) Linear dependence of
discretization error of Al- discretization error of Al-
gorithm 1 on h	gorithm 1 on √d
SSΘU9SOIO,3 qoeaιSSJ3- JO #
in'
M
a

Figure 1: Illustration of the consistency between Figure 2: Improvement of Algorithm 1
the theoretical bound in Theorem 5.2 and experi- over ULD algorithm in iteration com-
ment results.	plexity. (vertical bar = 1 std.)
To numerically verify the O(√d) dependence, We extensively experiment with d ∈
{1, 2, 5, 10, 20, 50, 100, 200, 500, 1000}. For each d, we run 1,000 independent realizations of ULD
algorithm until well converged with tiny step size (h = 0.005) and use their empirical average as the
‘true’ mean. Then we fix γ = 2, α = 1, h = 0.1, T = 10 and for each d, we run 1,000 independent
realizations of HFHR algorithm for T = 100 iterations. Experiment results are plotted in Figure 1b
and the linear trend demonstrates that the bound in Thm. 5.2 is tight in terms of d-dependence.
The final experiment compares Algorithm 1 with ULD algorithm in terms of iteration complex-
ity. The goal is to demonstrate the genuine acceleration of HFHR is not an artifact due to
time rescaling, which would disappear after discretization as the stability limit changes accord-
ingly. To do so, we push both ULD and HFHR to their respective largest h values that still
allow monotonic convergence at a large scale, and compare their mixing times. For general
nonlinear problems like the one here, Remark 5.5 suggests that with appropriately chosen α,
HFHR algorithm effectively reduces the constant factor of the iteration complexity, implying ac-
celerated sampling. To just provide one empirical verification of this improvement, we choose
d = 10 and use the error of mean 但*忆 q - E*q∣∣ to measure sampling accuracy. The bench-
mark, i.e., Eμq, is again obtained from 1,000 independent realizations of ULD algorithm with tiny
step size (h = 0.005), ran for long enough to ensure the corresponding Markov chain is well-
mixed. The initial measure is chosen as a Dirac measure at (100 × 1d, 0d), where 1d, 0d are
d-dimensional vectors filled with 1 and 0 respectively. We pick threshold = 0.1, and for each
α ∈ {0, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1,2,5, 10, 20, 50, 100}, we try all combi-
nations of (γ, h) ∈ {0.1,0.2,0.5,1, 2,5,10,20,50,100} X {0.1 X [50]} for Algorithm 1 (We also
run ULD algorithm when α = 0), and empirically find the best combination that requires the fewest
iterations to meet ∣∣E*k q - E*q∣∣ ≤ e. We find that h = 5 already surpasses the stability limit of
ULD algorithm, hence the range of step size covers the largest step size that are practically useable
for ULD algorithm. Experiments are repeated with 100 different seeds to further reduce variance.
The results are shown in Figure 2. When α > 0, HFHR algorithm consistently outperforms ULD
algorithm (note it also does so when α = 0 because HFHR uses a efficiency-wise-comparable but
more refined discretization than ULD algorithm). In particular, when α = 0.5 and 1, which are
8
Under review as a conference paper at ICLR 2022
empirically the best values we found for this experiment, HFHR algorithm achieves the specified
-closeness nearly 6× times faster than ULD algorithm, and its decreased mixing time (compared to
α = 0 for the same algorithm) is consistent with the ≈ 0.46 factor in Rmk.5.5). This empirical study
corroborates that the acceleration HFHR dynamics creates also carries through to its discretization,
and the acceleration of HFHR algorithm over ULD algorithm can be significant.
6.2 Bayesian Neural Network
Now consider Bayesian neural network (BNN) which is a compelling learning model (Wilson,
2020); however, the focus won’t be on its learning capability, and instead we just consider its train-
ing, which amounts to a practical, high-dim., multi-modal example of sampling tasks. It no longer
satisfies the conditions of our analysis, and our goal is to show HFHR still accelerates. We use fully-
connected network with [22, 10, 2] neurons, ReLU, standard Gaussian prior for all parameters, and
compare ULD and HFHR on data set Parkinson from UCI repository (Dua & Graff, 2017).
Choices of hyper-parameter for Algorithm 1 and ULD algorithm are systematically investigated.
For each pair (γ, α) ∈ {0.1, 0.5, 1, 5, 10, 50, 100}2, we empirically tune the step size to the stability
limit of ULD algorithm, simulate 10,000 independent realizations, and use the ensemble to conduct
Bayesian posterior prediction. HFHR will then use the same step size. For each γ, we plot the
negative log likelihood of HFHR algorithm (with different α choices) and ULD algorithm on training
and test data in Figure 3. Cases where α is too large for numerical stability are not drawn.
From Figure 3, we find that HFHR converges significantly faster than ULD in a wide range of
setups. In general, the log-strongly-concave assumption required in Theorem 5.2 does not hold for
multimodal target distributions. However, this numerical result shows that HFHR still accelerates
ULD for highly complex models such as BNN, even when there is no obvious theoretical guarantee.
It showcases the applicability and effectiveness of HFHR as a general sampling algorithm.
2 ”
TlN C-SF
ULD
HFHR(cr-0.1)
——ULD
——HFHR(α-0.11
——HFHR(a-0.51
——HFHR(a-1.01
Number of Data Passes
(a) γ = 0.1 (h = 0.005)
Numberor Data Passes
Number or Data Passes
(b) γ = 1 (h = 0.01)
TlN U-E
NumberofData Passes
(c)γ = 10(h = 0.05)
Number of Data Passes
(d) γ = 100 (h = 0.1)
(e)γ = 0.1 (h = 0.01)	(f) γ = 1 (h = 0.02)	(g) γ = 10(h = 0.1)	(h) γ = 100 (h = 0.2)
Figure 3: Training Negative Log-Likelihood (NLL) for various γ. Row 1: step sizes are close to the
stability limit of ULD algorithm; Row 2: further increased step size exceeds that stability limit.
7	Conclusion and Discussion
This paper proposes HFHR, an accelerated gradient-based MCMC method for sampling. To demon-
strate the acceleration enabled by HFHR, the geometric ergodicity of HFHR (both the continuous
and the discretized versions) is quantified, and its convergence is provably faster than Underdamped
Langevin Dynamics, which by itself is often already considered as an acceleration of Overdamped
Langevin Dynamics. As HFHR is based on a new perspective, which is to turn NAG-SC opti-
mizer with finite learning rate into a sampler, there are a number of interesting directions in which
this work can be extended. Besides further theoretical investigations that aim at refining the error
bounds, examples also include the followings: to scale HFHR up to large data sets, full gradient
may be replaced by stochastic gradient (SG) — how to quantify, and hence optimize the perfor-
mance of SG-HFHR? Can the generalization ability of HFHR trained learning models (e.g., BNN)
be quantified, and how does it compare with that by LMC and/or KLMC? These will be future work.
9
Under review as a conference paper at ICLR 2022
References
FeliPe Alvarez, Hedy Attouch, Jerome Bolte, and Patrick Redont. A second-order gradient-like
dissipative dynamical system with hessian-driven damping.: Application to optimization and me-
chanics. Journal de mathematiquesPures et appliquees, 81(8):747-779, 2002.
Hedy Attouch, Zaki Chbani, Juan PeyPouquet, and Patrick Redont. Fast convergence of inertial
dynamics and algorithms with asymPtotic vanishing viscosity. Mathematical Programming, 168
(1-2):123-175, 2018.
Hedy Attouch, Zaki Chbani, Jalal Fadili, and Hassan Riahi. First-order oPtimization algorithms via
inertial systems with hessian driven damPing. Mathematical Programming, PP. 1-43, 2020.
Fabrice Baudoin. Bakry-emery meet villani. Journal of Functional Analysis, 2017.
GNJC Bierkens, Paul Fearnhead, and Gareth Roberts. The zig-zag Process and suPer-efficient sam-
Pling for bayesian analysis of big data. Annals of Statistics, 47(3), 2019.
Alexandre Bouchard-Cote, Sebastian J Vollmer, and Arnaud Doucet. The bouncy particle sam-
Pler: A nonreversible rejection-free markov chain monte carlo method. Journal of the American
Statistical Association, 113(522):855-867, 2018.
Sebastien Bubeck, Ronen Eldan, and Joseph Lehec. Sampling from a log-concave distribution with
projected langevin monte carlo. Discrete & Computational Geometry, 59(4):757-783, 2018.
Yu Cao, Jianfeng Lu, and Lihan Wang. On explicit l2-convergence rate estimate for underdamped
langevin dynamics. arXiv preprint arXiv:1908.04746, 2019.
Changyou Chen, Nan Ding, and Lawrence Carin. On the convergence of stochastic gradient MCMC
algorithms with high-order integrators. NIPS, 2015.
Changyou Chen, Ruiyi Zhang, Wenlin Wang, Bai Li, and Liqun Chen. A unified particle-
optimization framework for scalable bayesian sampling. In The Conference on Uncertainty in
Artificial Intelligence, 2018a.
Yi Chen, Jinglin Chen, Jing Dong, Jian Peng, and Zhaoran Wang. Accelerating nonconvex learning
via replica exchange langevin diffusion. In International Conference on Learning Representa-
tions, 2018b.
Xiang Cheng and Peter L Bartlett. Convergence of langevin mcmc in kl-divergence. PMLR 83,
(83):186-211, 2018.
Xiang Cheng, Niladri S Chatterji, Peter L Bartlett, and Michael I Jordan. Underdamped langevin
mcmc: A non-asymptotic analysis. Proceedings of the 31st Conference On Learning Theory,
PMLR, 2018.
Sinho Chewi, Chen Lu, Kwangjun Ahn, Xiang Cheng, Thibaut Le Gouic, and Philippe Rigollet.
Optimal dimension dependence of the metropolis-adjusted langevin algorithm. COLT, 2021.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-
parameterized models using optimal transport. In Advances in neural information processing
systems, pp. 3036-3046, 2018.
Arnak Dalalyan. Further and stronger analogy between sampling and optimization: Langevin monte
carlo and gradient descent. In Conference on Learning Theory, pp. 678-689. PMLR, 2017a.
Arnak S Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave
densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(3):
651-676, 2017b.
Arnak S Dalalyan and Lionel Riou-Durand. On sampling from a log-concave density using kinetic
Langevin diffusions. Bernoulli, 26(3):1956-1988, 2020.
10
Under review as a conference paper at ICLR 2022
Wei Deng, Qi Feng, Liyao Gao, Faming Liang, and Guang Lin. Non-convex learning via replica
exchange stochastic gradient mcmc.In International Conference on Machine Learning, pp. 2474-
2483. PMLR, 2020.
Jean Dolbeault, Clement MoUhoL and Christian Schmeiser. Hypocoercivity for kinetic equations
with linear relaxation terms. Comptes Rendus Mathematique, 347(9-10):511-516, 2009.
Jean Dolbeault, Clement Mouhot, and Christian Schmeiser. Hypocoercivity for linear kinetic equa-
tions conserving mass. Transactions of the American Mathematical Society, 367(6):3807-3828,
2015.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
Andrew B Duncan, Tony Lelievre, and GA Pavliotis. Variance reduction using nonreversible
langevin samplers. Journal of statistical physics, 163(3):457-491, 2016.
Alain Durmus and Eric Moulines. Sampling from strongly log-concave distributions with the unad-
justed langevin algorithm. arXiv preprint arXiv:1605.01559, 5, 2016.
Alain Durmus, Szymon Majewski, and Blazej Miascjedow. Analysis of langevin monte carlo via
convex optimization. The Journal of Machine Learning Research, 20(1):2666-2711, 2019a.
Alain Durmus, Eric Moulines, et al. High-dimensional bayesian inference via the unadjusted
langevin algorithm. Bernoulli, 25(4A):2854-2882, 2019b.
Raaz Dwivedi, Yuansi Chen, Martin J Wainwright, and Bin Yu. Log-concave sampling: Metropolis-
hastings algorithms are fast. Journal of Machine Learning Research, 20(183):1-42, 2019.
Andreas Eberle, Arnaud Guillin, Raphael Zimmer, et al. Couplings and quantitative contraction
rates for langevin dynamics. The Annals of Probability, 47(4):1982-2010, 2019.
J-P Eckmann and Martin Hairer. Spectral properties of hypoelliptic operators. Communications in
mathematical physics, 235(2):233-253, 2003.
Murat A Erdogdu and Rasa Hosseinzadeh. On the convergence of langevin monte carlo: The inter-
play between tail growth and smoothness. COLT, 2021.
Charlie Frogner and Tomaso Poggio. Approximate inference with wasserstein gradient flows. In
International Conference on Artificial Intelligence and Statistics, 2020.
Ye He, Krishnakumar Balasubramanian, and Murat A Erdogdu. On the ergodicity, bias and asymp-
totic normality of randomized midpoint sampling method. Advances in Neural Information Pro-
cessing Systems, 33, 2020.
Bin Hu and Laurent Lessard. Dissipativity theory for nesterov’s accelerated method. In Pro-
ceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 1549-1557.
JMLR.org, 2017.
Chii-Ruey Hwang, Shu-Yin Hwang-Ma, Shuenn-Jyi Sheu, et al. Accelerating diffusions. Annals of
Applied Probability, 15(2):1433-1444, 2005.
Richard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the fokker-
planck equation. SIAM journal on mathematical analysis, 29(1):1-17, 1998.
Arlene KH Kim, Richard J Samworth, et al. Global rates of convergence in log-concave density
estimation. The Annals of Statistics, 44(6):2756-2779, 2016.
S. M. Kozlov. Effective diffusion in the fokker-planck equation. Mathematical notes of the Academy
of Sciences of the USSR, 45:360-368, 1989.
Benedict Leimkuhler, Charles Matthews, and Jonathan Weare. Ensemble preconditioning for
markov chain monte carlo simulation. Statistics and Computing, 28(2):277-290, 2018.
11
Under review as a conference paper at ICLR 2022
Tony Lelievre, Francis Nier, and Grigorios A Pavliotis. Optimal non-reversible linear drift for the
convergence to equilibrium of a diffusion. Journal OfStatistical Physics, 152(2):237-274, 2013.
Ruilin Li, Hongyuan Zha, and Molei Tao. Sqrt (d) dimension dependence of langevin monte carlo.
arXiv preprint arXiv:2109.03839, 2021.
Xuechen Li, Denny Wu, Lester Mackey, and Murat A Erdogdu. Stochastic Runge-Kutta accelerates
Langevin Monte Carlo and beyond. NeurIPS, 2019.
Chang Liu, Jingwei Zhuo, Pengyu Cheng, Ruiyi Zhang, and Jun Zhu. Understanding and acceler-
ating particle-based variational inference. In International Conference on Machine Learning, pp.
4082—4092, 2019.
Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference
algorithm. In Advances in neural information processing Systems, pp. 2378-2386, 2016.
Yi-An Ma, Yuansi Chen, Chi Jin, Nicolas Flammarion, and Michael I Jordan. Sampling can be faster
than optimization. Proceedings of the National Academy of Sciences, 116(42):20881-20885,
2019.
Yi-An Ma, Niladri Chatterji, Xiang Cheng, Nicolas Flammarion, Peter Bartlett, and Michael I Jor-
dan. Is there an analog of nesterov acceleration for mcmc? Bernoulli, 2021.
Jonathan C Mattingly, Andrew M Stuart, and Desmond J Higham. Ergodicity for sdes and ap-
proximations: locally lipschitz vector fields and degenerate noise. Stochastic processes and their
applications, 101(2):185-232, 2002.
Robert I McLachlan and G Reinout W Quispel. Splitting methods. Acta Numerica, 11:341, 2002.
Sean P Meyn, Robert L Tweedie, et al. Computable bounds for geometric convergence rates of
markov chains. The Annals ofApplied Probability,4(4):981-1011, 1994.
Yurii Nesterov. A method for unconstrained convex minimization problem with the rate of conver-
gence o (1∕k^ 2). In DokladyAN USSR, volume 269, pp. 543-547, 1983.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2013.
Masayuki Ohzeki and Akihisa Ichiki. Langevin dynamics neglecting detailed balance condition.
Physical Review E, 92(1):012105, 2015.
Grigorios A Pavliotis. Stochastic processes and applications: diffusion processes, the Fokker-Planck
and Langevin equations, volume 60. Springer, 2014.
Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR Com-
putational Mathematics and Mathematical Physics, 4(5):1-17, 1964.
Luc Rey-Bellet and Konstantinos Spiliopoulos. Irreversible langevin samplers and variance reduc-
tion: a large deviations approach. Nonlinearity, 28(7):2081, 2015.
Gareth O Roberts, Richard L Tweedie, et al. Exponential convergence of langevin distributions and
their discrete approximations. Bernoulli, 2(4):341-363, 1996.
Julien Roussel and Gabriel Stoltz. Spectral methods for langevin dynamics and associated error
estimates. ESAIM: Mathematical Modelling and Numerical Analysis, 52(3):1051-1083, 2018.
Ruoqi Shen and Yin Tat Lee. The randomized midpoint method for log-concave sampling. In
Advances in Neural Information Processing Systems, pp. 2098-2109, 2019.
Bin Shi, Simon S Du, Michael I Jordan, and Weijie J Su. Understanding the acceleration phe-
nomenon via high-resolution differential equations. Mathematical Programming, pp. 1-70, 2021.
Andre N Souza and Molei Tao. Metastable transitions in inertial langevin systems: What can be
different from the overdamped case? European Journal ofApplied Mathematics, 30(5):830-852,
2019.
12
Under review as a conference paper at ICLR 2022
Gilbert Strang. On the construction and comparison of difference schemes. SIAM journal on nu-
merical analysis, 5(3):506-517,1968.
Weijie Su, Stephen Boyd, and Emmanuel Candes. A differential equation for modeling nesterov’s
accelerated gradient method: Theory and insights. In Advances in Neural Information Processing
Systems, pp. 2510-2518, 2014.
Amirhossein Taghvaei and Prashant Mehta. Accelerated flow for probability distributions. In Ka-
malika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Con-
ference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp.
6076-6085, Long Beach, California, USA, 09-15 Jun 2019. PMLR.
Santosh Vempala and Andre Wibisono. Rapid convergence of the unadjusted langevin algorithm:
Isoperimetry suffices. In Advances in Neural Information Processing Systems, pp. 8092-8104,
2019.
Cedric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008.
CedriC Villani. Hypocoercivity. Memoirs ofthe American Mathematical Society, 202(950), 2009.
Yifei Wang and Wuchen Li. Accelerated information gradient flow. arXiv preprint
arXiv:1909.02102, 2019.
Andre Wibisono. Sampling as optimization in the space of measures: The langevin dynamics as a
composite optimization problem. In Conference On Learning Theory, pp. 2093-3027, 2018.
Andre Wibisono, Ashia C Wilson, and Michael I Jordan. A variational perspective on accelerated
methods in optimization. Proceedings of the National Academy of Sciences, 113(47):E7351-
E7358, 2016.
Andrew Gordon Wilson. The case for bayesian deep learning. arXiv preprint arXiv:2001.10995,
2020.
Ashia C Wilson, Ben Recht, and Michael I Jordan. A lyapunov analysis of accelerated methods in
optimization. Journal of Machine Learning Research, 22(113):1-34, 2021.
Ruiyi Zhang, Changyou Chen, Chunyuan Li, and Lawrence Carin. Policy optimization as wasser-
stein gradient flows. In International Conference on Machine Learning, pp. 5737-5746, 2018.
13
Under review as a conference paper at ICLR 2022
A Additional Notations
We introduce a few notations that are used in the main text as well as some proof. When Vf is
L-Lipschitz, the drift term
Lemma D.3, where
p — αVf(q)
—γ P — Vf (q)
in HFHR dynamics is also L0-Lipschitz, as proved in
L0 = √2 max { p1 + α2 max
P1 + Y2 }.
We show in Lemma D.5 that a linear-transformed HFHR dynamics satisfies the nice contraction
property, the linear transformation P we use is defined as
P
I
γI
0 ʌ/l + αγ I
∈ R2d×2d
Denote the largest and the smallest singular value of P by
αY Y2
σmax=vτ + y+ -
αY Y2
σmin =SV τ + τ -
and its condition number by
^ a2γ2 — 2αγ3 + 4ɑγ + γ4 + 4
+1,
,ɑ2γ2 — 2αγ3 + 4ɑγ + γ4 +4
+1
0	σmax
K =-------
σmin
αγ + γ2 +
αγ γ2
ɪ + ^2
^ a2γ2-2αγ3+4ɑγ+γ4+4
+1
^ a2γ2-2αγ3+4ɑγ+γ4+4
+1
The rate λ0 of exponential convergence of transformed HFHR dynamics is characterized in Lemma
D.5 and is defined as
λ0
min∕ m + ɑm,j ∖
YY
given that γ2 > L.
B Proofs for the Continuous Dynamics
Notations and definitions can be found in Sec.3.
B.1 Proof of Theorem 4.1
Proof. The Fokker-Plank equation of HFHR is given by
dtpt = -Vx ∙ ( -Vf(q)
Pt) + α (Vq ∙ (Vf (Q)Pt) + ∆qρt) + Y (Vp ∙ (PPt) + δPPt)
where Vx = (Vq, Vp). For π H e-f(q)- 1 kpk2, we have
Vx ∙ ( —Vf(q)
π =h
p
—Vf(q)
, Vxπi = 0,
∖
2
2
2
2
∆q∏ = —Vq ∙ (∏Vf(q))
∆p∏ = —Vp ∙ (πp)
Therefore ∂tπ = 0 and hence π is the invariant distribution of HFHR.
□
14
Under review as a conference paper at ICLR 2022
B.2 Proof of Theorem 5.1
Proof. Consider two copies of HFHR that are driven by the same Brownian motion
ddqt = (pt 一 αNf(qt))dt + √2αdB1	f d⅞t = (Pt 一 ɑ▽/(^j)dt + √2αdB1
[dPt = (-YPt — Vf (qt))dt + √2YdB2	,	{dPt = (-γPt 一 Vf 应))dt + √2YdB2
where We set (⅞o,p0)〜∏, p0 = p0 and q° such that
W2(μo,μ) = E [kqo -备同，q0 〜μo
Denote φt = P qt 一 qt
ψtt∖	Ptt 一 Pt
on α, γ, we have
where P is defined in Appendix A. By Lemma D.5 and the assumption
ψt]∣2 ≤ e-2( ia)t[]ψj2
Therefore we obtain
W22 (μt, μ)
inf	Ekqt — ⅞tk2
(qt,⅛t)〜∏(μt,μ)
≤	inf	E
(qt,qt hπMt,μMPt,Pt)〜π(Vt,V)
[pt-pt]∣2
≤EkP-1k22∣∣∣∣ψφtt]∣∣∣∣2
≤EkP-1k2e-2(Y +ma)t∣[ψ° ]|
≤(κ0)2e-2( 学+ma)t∣[po—P0]∣2
= (κ0)2e-2( m+mɑ)tW2(μo,μ)
Taking square root yields the desired result.
□
C Arbitrary Long Time Discretization Error of Algorithm 1
Theorem C.1. Under ConditionsAI andfurther assume the function V∆f grows at most linearly,
i.e., ∣∣V∆f (q)∣∣ ≤ G 1 +kqk2, ∀q ∈ Rd. Also suppose γ in HFHR dynamics satisfy γ2 > L.
Then there exist C, h0 > 0, such that for 0 < h ≤ h0, we have
(Ekxk- Xkk2) 2 ≤ Ch
where x k is the k-th iterate of Algorithm 1 with SteP size h starting from xo, xk is the solution of
HFHR dynamics at time kh, starting from xo. This result holds Uniformlyfor all k ≥ 0 and k can
go to ∞. In particular, C = O(√d) and if Y — L+m ≥ mα, then there exists b > 0, independent of
a and is of order O( √d), such that
α
Y Y
C ≤ " α2
(11)
Proof. Denote tk = kh, the solution of the HFHR dynamics at time t by xo,x0 (t), the k-th iterates
of the Strang,s splitting method of HFHR dynamics by xo,χ0 (kh). Both xo,χ0 (t) and xoχ (kh)
15
Under review as a conference paper at ICLR 2022
start from the same initial value x0. The linear transformation P defined in Appendix A, transforms
the solution of HFHR dynamics into y0,P x (t) = Px0,x0 (t) and the Strang’s splitting discretization
of HFHR into y0,pχ0 (t) = Px。,=。(t)'
For the ease of notation, We write yo,yo (tk) as ?卜 and yo,yo (tk) as yk. We have the following
identity
Ellyk+ι- y k+1∣∣2 =E∣∣ytk,yk ㈤一y t% y (h)f
=EIIytk,yk ㈤-ytk y㈤ + ytk,Vk ㈤-y t% y (h)∣
=E∣∣ytk,yk(h) - ytk y (h)∣∣2+E∣∣ytk,y k(h) - ytk,Vk (h)∣∣2
'------------------------} '------------------------}
①
+2 E Dytk,yk ㈤-ytk y ㈤，ytk y ㈤-y
'--------------------------------
tk,Vk ")/
By Lemma D.5, when 0 < h < /,term (T) can be upper bounded as
旧辰曲(h)- ytk y (h)∣∣2 ≤e-2λ0hEkyk- Vk k2
≤(1- 2λ0h + 2(λ0)2h2) Ekyk- yk『
≤ (1- λ0h) Ekyk- yk Il2
where the second inequality is due to e-x ≤ 1 一 X + x2, ∀χ> 0.
For term (2), we have by Lemma D.8 that
E∣∣ytk ,yk (h) - y tk,yk (h)∣∣ ≤ σm ax E∣∣xtk ,Xk (h) - X tk,x k (h)∣∣2 ≤ σL C2 h3
where σmax is the largest singular value of matrix P.
For term ③),we have by Lemma D.1 that
2E {ytk,kk(h) - ytky(h)，ytky(h) - yt%,y%(hη
=2E {kk - yk + z, ytky (h) - ytk,yk (h))
=2E Dyk- yk，Iytky(h) - ytky (h)E + 2E Dz, ytky (h) - ytk,y
'-------------------------------} '---------------------
k
^Z
3a
^Z
3b
16
Under review as a conference paper at ICLR 2022
For term(3a), by the tower property of conditional expectation, We have
2E yk- - yk, ytky (h) - ytk,yk (h))=2E E	yy- - y®, 3^^㈤一yt%y ㈤)Fk
D
=2E yy-- yk, E ytky (h)- ytk,yk(h) Fk
2
≤2VZEkyk - ykk2
ytk,yk㈤-ytky (h) Fk
∖
E
E
u	2
≤2VZEkyk- y k2ʌ σ2ιaxE E xtk,xk㈤-Xtk,xk(h) Fk
≤2 VZEkyk- y k2 q∕σmι axC2h4
≤2σmaxC1 JEkyk - y k2h2.
For term Gb), when 0 < h < ɪ^we have by Lemma D.1 and Lemma D.8
2E (z, ytky (h) - ytk,yk
Ellytk,yk㈤-ytky (h)||2
=2
tE
E
ytk,yk (h) - ytky (h)||2 IFk
=2 Ekzk2
∖
σm2ax
E E l∣xtk,Xk (h) - Xtk,Xk
(h升2 Fk#j
≤2σmax VZCEkyk - yk if h2 ∖∕C2h3
≤2σmaxC2 PC∖/Ekyk - yk k2h5
where C = 2 (L00)2 = 2(κ0)2 (L0)2 is fromLemmaD.1 andLemmaD.3.
Recall both C1 and C2 dependonkXkk and we would like to upper bound this term. To this end, con-
sider X(t), a solution of HFHR dynamics with initial value Xo that follows the invariant distribution
Xo 〜∏ and realizes W2(∏0,∏), i.e., EkX0 — xo∣∣2 = W-2(∏o, ∏).
17
Under review as a conference paper at ICLR 2022
1
Denote Xk = X(kh) and ek = (Ellyk - yk ∣∣2) 2, we then have
Ekxk∣∣2 =EIlxk + Xk - XkIl2
≤2E∣Xk∣2 +2Ekxk - xk∣∣2
≤4Ekxkk2 +4Ekxk - xkk2 + 2Ekxk - xkk2
=4Ekxkk2 +4EIlPTP(xk - xk) I I 2 + 2E I I PTP(xk - xk)『
≤4
(i)
≤4
k k kqk2dμ + d) +	2~e I I P(xk- xk)∣∣2 +	— Ekyk- ykk2
∖jRd	σ σmin	σmin
([kqk2dμ + a) +—— e-2λ khEI I P(x0-x0)∣∣2 +—2~ek
∖Λd	σ σmin	σmin
≤4 ( [ Ilqk2 dμ + d) + 4κ2W2 (π0,π) H—2~ek
∖Λd	)	σmin
,Fek + G
where (i) is due to Lemma D.5. Recall from Lemma D.8, we have
C1	≤ A1 JEkxkk2 + Bi	≤	Aι√Fek	+	(Aι√G	+ Bi)，U1 ek +	V1
C2	≤ A2 JEkxkk2 + B2	≤	A2√Fek	+	(A2√G	+ B2)，U2ek +	V2
where
A1 =(L + G)maχ{α + 1.25,γ +1}(1.74 + 0.71α)
B1 =(L + G) max{a + 1.25, Y + 1} [θ.5α + (1.26√α + 1.14α√α + 2.32√γ
A2 =Lmax{α + 1.25, γ + 1}(1.92 + 2.30ɑL)√h
B2 =Lmax{α + 1.25, Y + 1}(2.60√α + 3.34√γh)√d
18
Under review as a conference paper at ICLR 2022
Combine the above and bounds for terms(1)(2)(3a) and(3b), We then obtain
e2k+1 ≤(1 - λ0h)e2k +
≤(1 - λ0h)e2k +
σ
σ
-m ax CIlh + 2σmax CIekh2 + 2σmaxC2 E ek h5
m ax2(UIek + V22)h3 + 2σmax(U1 ek + V1)ek h2 + 2σmax(U2ek + V2)PC^ek h 2
1	- λh + 2σ2a aχU2h3 + 2σmaχU1h2 + 2σmaxU2 PCC h 2 ) ek
+
V1 + 2σmax V2
ekh2 + 2σm2axV22h3
≤ (1 - λh + 2σ22axU2h3 + 2σmaxU1h2 + 2σmaxU2 PCCh2) ek + ɪ hek
2
+
λ0
2
h3 +2σm2axV22h3
1	- 8λ0h + 2σmaxU2 h3 + 2σmaxU1h2 + 2σmaxU2 PCh2) ek
+
2	(2σmaxV1 + 2σmaxV2 PCh)
∖
λ0
- + 2σmaxV2 I h3
(i)
≤(1
2 Th)ek +
2 (2σmaxV1 + 2σmaxV2 PCh)
λ0
,(1
∖
2 Th)ek + Kh3
∖
+ 2σm2 axV22 II h3
)
—
—
where (i) is due to h < min{h1, h2, h3} and
h1
√λ0
h2
4√2κ0Lmax{α + 1.25, γ + 1}(1.92 + 2.30αL)，
λ0
h3
16√2κ0(L + G) max{ɑ + 1.25, Y + 1}(1.74 + 0.71α),
λ0
8κ0Lmax{α + 1.25,γ +1}(1.92 + 2.30αL).
Unfolding the above inequality, we arrive at
ek ≤ (1 - yh) e0 + (1 +(I - yh) +	+(I - yh)k-1) Kh3
(i)	∞	λ0	i
≤ Kh3 X 1 - Eh
i=0
E h2
Where (i) is due to ek = 0. Therefore
(Ekxk - Xkk2)
1
2
EilPT(yk- yk)『)≤ σm-ek ≤ σm-用h
19
Under review as a conference paper at ICLR 2022
Collecting all the constants and we have
min
2K 8κ0
——≤ —— (L + G) max{ɑ + 1.25, Y + 1}(1.74 + 0.71α)
λ0	λ0
4κ' ， _	L . 一	一、/一	，一 一一 J—	. . .	J—	一 一 _  T
+ -y-L (L + G) max{ɑ + 1.25, Y +1}(0.5a + (1.26√S + 1.14α√α + 2.32√γ)√d
L max{α + 1.25, γ + 1}(1.92 + 2.30αL)
Lmax{a + 1.25, γ + 1}(2.60√a + 3.34√γ)√d
(π0, π)
It is clear that in terms of the dependence on dimension d, We have C = O(√d). In the regime
γ2 L
where Y -	≥ m + ma, then λ = m + ma. Recall the definition of κ0 and there exist A , B > 0
such that K ≤ A0√α + B0. It follows that
aι α3 + a2a 2 + a3a2 + a4a3 + a5a + 06a 1 + a7
C ≤ ------------------------------------------
λ0
ɑ3 + 3
≤ b ——-3
≤	λ0
,a3 + -3	b , 2 a ι 1 .
bm+ma = m(a -γa+γ2)
for some positive constants aι, a2, a3, a4, a5, a6, a7, b > 0 and independent of α, in particular, we
have b = O(√d).	□
C.1 Proof of Theorem 5.2
Proof. Denote the k-th iterate of the Strang,s splitting method of HFHR by Xk with time step h, the
solution of HFHR dynamics at time hk by Xk. Both Xk and Xk start from Xo = q0 . Also denote
p0
the solution of HFHR dynamics starting from X0 at time kh by Xk where X0
q0 , (⅞0, p0) 〜π
W22 (πo, π).
and E q0 - q0
Il [Po - Po
follows that Xk 〜∏.
Since π is the invariant distribution of HFHR dynamics, it
By Lemma D.5 and Theorem C.1, we have
W2(μk,μ) =e τinf	.E(qι,q2)-ξkqι-q2k2
ξ∈Π(μk ,μ)
≤. Tjnf、E(X1,X2)〜ξ kX1 - X2k2
ξ∈Π(πk,π)
≤EkXk - Xkk2
≤2C2h2 + 2E∣∣PTP(Xk - Xk)∣∣2
≤2C2h2 + 2kP-1k2E∣∣P(Xk - Xk)∣∣2
≤2C2h2 + 2kP-1k2e-2λ0khEllP(x0 - X0)『
≤2C2h2 + 2(κ0)2e-2λ0khW22(πo, π)
Take square root on both sides and apply √a2 + b2 ≤ a + b, we obtain
W2(μk,μ) ≤ √2Ch + √2κ0e-λ0khW2(∏o,π).
□
20
Under review as a conference paper at ICLR 2022
C.2 Proof of Corollary 5.4
Proof. By Theorem 5.2, we have
W2(μk,μ) ≤ √2Ch + √2κ0e-λ0khW2(∏0,π).
Given any target accuracy e > 0, if We run the Strang's splitting method of HFHR with h? =
min{ho, √回 }, then after k? = * max{；, 2√2c } log 2√2κ w2(π0,π), We have
2 2C	λ	h0
W2(μk?,μ) ≤ √2Ch + √2κ0e-λ khW2(μ0,μ) ≤ 2 + 2 = e.
Recall C = O(√d), when high accuracy is needed, e.g. E < 2√2Cho, the itera-
tion complexity to reach -accuracy under 2-Wasserstein distance is k? = O( W log ɪ)=
2√2λɪ log 2√2κ0WMπ0,π) = O( W). Recall from Theorem C.1, C ≤ mb(α2 - Y + γ12), we
have
CV b α2 - Y + Y12
λ0 - m2	1 + α
Y
b a2-α + 马	√3-ι
Denote g(α) = m—1 + Y , simple calculation shows that α? = argmina≥o g(α) = ɪ3-1 =
Y +α	-	Y
O( I).	口
D Technical/Auxiliary Lemmas and Their Proofs
D.1 Dependence of error of SDE on initial values
Lemma D.1. Consider the following two SDE with different initial condition
dxt = a(xt)dt + σdW t,	dyt = a(yt)dt + σdW t,
x(0) = x0	y(0) = y0
where a(u) ∈ Rd is L-Lipschitz, and σ ∈ Rn×n is a constant matrix. For 0 < h < 4L, we have
the following representation
xh - yh = x0 - y0 + z
with
Ekzk2 ≤ 2L2kx0 - y0k2 h2
Proof. Let z = (xh - yh) -
(x0 - y0) = R0h a(xs) - a(ys )ds. Ito’s lemma readily implies that
Ekxh - yhk2
=kx0 - y0 k2
+ 2E
Z hhxs
0
- ys, a(xs) - a(ys)ids
≤kx0 - y0k2
+ 2L
Z h Ekxs
0
- ys k2 ds
By Gronwall’s inequality, it follows that
Ekxh - yhk2 ≤ kx0 - y0k2 e2Lh ≤ 2kx0 - y0k2
for 0 < h < 1—
4L
21
Under review as a conference paper at ICLR 2022
and
Ekzk2
a(xs) - a(ys)ds
0
a(ys) ds!
- a(ys)2 ds
E
h
≤h	Ea(xs) - a(ys) ds
0
≤L2hZ Ekxs - ysk2 ds
0
≤2L2kx0 - y0k2 h2
□
D.2 Growth bound of SDE with additive noise
Lemma D.2. Consider the following SDE with constant diffusion
(dxt = a(xt)dt + σdW t,
x(0) = x0
where a(x) ∈ Rd is L-smooth, i.e., |a(y) - a(x)| ≤ L|y - x|, a(0) = 0 and σ ∈ Rd×d is a
constant matrix independent of time t and Xt. Thenfor 0 < h < 4L, we have
Ekxh - x0k2 ≤ 2.57 kσk2F + 2hL2kx0k2 h.
Proof. We have
2
Ekxh - x0 k2 =E
Z a(xt)dt + Z	σdWt
00
≤2E
(=i)2E
≤2E
≤2E
≤2E
≤4E
a(xt)dt
0
a(xt)dt
0
2
+ 2E
Zh
0
+2Zh
0
a(xt) dt
σdWt
kσk2Fdt
+ 2hkσ k2F
a(xt) - a(x0) dt + Z	a(x0) dt
LZ kxt - x0k dt + ha(x0)
L2 Z	kxt - x0 k dt! +
+ 2hkσk2F
+ 2hkσk2F
h2a(x0)2 + 2hkσk2F
2
2
2
(ii)
≤ 2hkσ
k2F + 4h2a(x0)2 + 4L2hZ Ekxt - x0k2 dt
0
22
Under review as a conference paper at ICLR 2022
where (i) is due to Ito’s isometry, (ii) is due to Cauchy-Schwarz inequality and kσkF is the Frobe-
nius norm of σ. By Gronwall’s inequality, we obtain
Ekxh - x0k2 ≤ 2hkσk2F + 4h2a(x0)2 exp n4L2h2o .
Since∣∣a(xo)^ = ∣∣a(xo) - a(0) ∣∣ ≤ L∣∣xo∣∣, When 0 < h < 4L, we finally reach at
Ekxh - xok2 ≤ 2 (kσ∣∣F + 2hL2kxok2) e4h ≤ 2.57 (∣∣σ∣∣F + 2hL2kxok2) h.
□
D.3 Lipschitz continuity of the drift of HFHR dynamics
Lemma D.3. Assume Vf is L-Lipschitz, i.e. ∣Vf (x) - Vf (y)∣ ≤ Lkx - yk, then the drift term
of HFHR dynamics
p - αVf(q)
-γp - Vf(q)
is L0-Lipschitz, where L0，√2max{√T+^02 max{√^,L}, ^1+ γ2}. Let P be defined in Ap-
φ
ψ
φ
ψ
pendix A and
P qp , then
satisfies the following SDE
dφ
dψ
p(φ, ψ) - αVf (q(φ, ψ))
-YP(Φ, ψ) — Vf (q(φ, ψ))
dt + P
0
√2YIJ [dB
dW
P
and the drift term
P
P(Φ, ψ) - αVf (q(φ, ψ))
-γp(φ, ψ) - Vf (q(φ, ψ))
is L00 -Lipschitz, where L00 = κ0L0 and κ0 is the condition number of P.
Proof. By direct computation and Cauchy-Schwarz inequality, we have
p1 - αVf(q1 )
-γp1 - Vf(q1 )
p2 - αVf(q2 )
-γp2 - Vf(q2)
—
=γ∣∣-α(VfSi)-Vf(q2))+ (Pi-P2)∣∣ +∣∣-(Vf(qι) -vf(q2)) -y(pi -P2)∣∣
≤q2a2∣∣vf Si)-Vf (q2)∣∣ +2kPι- P2k2 + 2∣∣vf(q1)- vf(q2)∣∣ + 2γ2kPι- P k2
≤ (2α2L2 + 2L2)kq1 -q2k+(2+2γ2)kp1 -p2k2
≤√2max{L√1 + α2, pl + γ2}∣∣ Ipq1 -p2
q1 - q2
p1 - p2
,L0
q1 - q2
p1 - p2
By Ito’s lemma, we have
dφ
dψ
p(φ, ψ) - αVf (q (φ, ψ))
-γp(φ, ψ) - Vf (q(φ, ψ))
dt + P
0
√2γiJ [dB
dW
P
23
Under review as a conference paper at ICLR 2022
Using the Lipschitz constant obtained for the drift of HFHR, we further have
P Γ p(φι, Ψι) - αVf(q(φι, ψj)
-YP(Φ1, Ψι) — Vf (q(φι, ψι))
-P
p(φ2, ψ2) - αVf(q(φ2, ψ2))
-γp(φ2,ψ2) - Vf(q(φ2, ψ2))
p2 - αVf (q2)
-γp2 - Vf(q2)
≤σ p1 - αVf(q1)
max	-γp1 - Vf(q1)
≤σmaxL
≤σmaxL
—
where σmax, σmin and κ0 are the largest, smallest singular values and the condition number (w.r.t.
2-norm) of matrix P.	□
Remark D.4. The following inequalities associated with L0 will turn out to be useful in many proofs
L0 ≥ 1, L0 ≥ √2γ, L0 ≥ √2α, L ≥ √2L and L0 ≥ √2ɑL.
D.4 Contraction of (Transformed) HFHR Dynamics
Lemma D.5. Suppose f is L-smooth, m-strongly convex and γ2 > L. Consider two c
HFHR dynamics qt , qt
Pt	Pt
respectively, then we have
(driven by the same Brownian motion) with initialization q0
P0
opies of
Γqo
一’辰。一
where P
γI
0
I
√1 + αγI
qt - qt
Pt- pt
P
q0 - q0
p0- p0
2
and λ0 = min{ mm + αm, Y Y }.
P
Proof. Consider two copies of HFHR that are driven by the same Brownian motion
ddqt = (pt 一 αVf(qt))dt + √2αdB1
\dPt = (-YPt- kf(qt))dt + √2YdB2
f dqt = (Pt - αVf(qt))dt + √2αdBl
IdPt = (-γPt - Vf (⅛t))dt + √2YdB2
Based on Taylor’s expansion, the difference of the two copies is expressed as
d kt - qt] -	IaHt
〜	— 一 Tr
dt Pt - Pqt	Ht
-γII Pqtt--qPqqtt ,-A qPtt--qPqqtt
where Ht = R01 V2f (qqt + s(q - qqt))ds. Denote the eigenvalues of Ht by ηi, 1 ≤ i ≤ d, by strong
convexity and smoothness assumption on f, we have m ≤ ηi ≤ L, 1 ≤ i ≤ d.
24
Under review as a conference paper at ICLR 2022
Denote ψt = P Pt- Pt and consider Lt = 2∣l ψt Il , we have
φt
ψt
T
PAP-1
φt
ψt
T
ψt 1(PAP-1 + (PT)T AT P T)
T
Φt I(I + αY)Ht
ψ t γ	0d×d
0d×d
γ2I - Ht
—
Φt
ψt
T
B(α)
Φt
ψt
φt
ψt
Φt
ψt
—
It is easy to see that
λmin(B(α)) =	min {min{ηi + αηi, γ — ni}} ≥ min{m + αm, Y----------- }，λ0.
i=1,2,…，d	Y	YYY
Therefore we have KLt ≤ -2λminB(α)Lt ≤ -2λ0Lt. By GronWall's inequality, we obtain
and the desired inequality follows by taking square root.
□
D.5 Local error between the exact Strang’s splitting method and HFHR
DYNAMICS
Lemma D.6. Assume f is L-smooth and 0 ∈ argminχ∈Rd f (x), i.e. Vf (0) = 0. If 0 < h ≤ 击,
then compared with the HFHR dynamics, the exact Strang’s splitting method has local mathematical
expectation of deviation of order p1 = 2 and local mean-squared error of order p2 = 2, i.e. there
exist constants C1 , C2 > 0 such that
∣∣Ex(h) — Ex(h)∣∣ ≤ CIhpI
1
(E h∣∣x(h) — x(h)∣∣2])2 ≤ Cb2hp2
where x(h)
q(h)
P(h)
is the solution of the HFHR dynamics with initial value x0
q0
P0
x(h)
p1 =
q(h)
P(h)
is the solution of the implementable Strang’s splitting with initial value x0
2 and p2 = 2. More concretely, we have
and
q0
P0 ,
Cbi = Lmax{a	+ 1.25, y + 1}	(l.74∣∣xok +	(1.26√α +	2.84√Y)√hd),
Cb2 = L max{a	+ 1.25, γ + 1}	(1.92∣∣xok +	(1.30√α +	3.22√Y)√hd).
Proof. The exact Strang's splitting integrator with step size h reads as φ2 ◦ ψh ◦ φ2 where
d dq	= pdt	: d dq	=	-αVf(q)dt + √2αdW
• ∖dp	= —γPdt +	√2γdB	' ∖pp	=	-Vf(q)dt	^
The φ flow can be explicitly solved and the solution is
(q⑴=qo + 1-eγ-γt Po + √2γ Rt 1-e-γ(t-s) dB(S)
IP⑴=e-γtPo + √2γ Ro e-Y(t-s)dB(S)
25
Under review as a conference paper at ICLR 2022
The ψ flow can be written as
(q(t) = q0 - Rt αVf (q(s))ds + √2αRt dW(S)
ɪp(t) = Po- RtVf (q(S))ds
The solution of one-step exact Strang,s splitting integrator with step size h can be written as
q3 = q2(h) + 1-eγ'2 P2(h) + √2γ Rh 1-e二(I) dB(S)
Y	2	Y
P3 = e-γ2p2(h) + √2γ R2 e-Mh-s)dB(s)
< q2(V) = qι- Rr aVf (q2(S))ds + √2a Rr dW(S)	(O ≤ r ≤ h)
P2(r) = Pi - Rr Vf&(Sy)ds
qi = qo + TlPo + √2γ Rt2 上二等」dB(S)
、P1 = e-72Po + √2γ Rt2 e-7(2-s)dB(S)
Therefore, we have ^(h) = q3, P(h) = P3 and
^(h) =√2Y('
,h 1 - e-7(h-s)
-------dB(c) + qi —
Y
Z aVf(q2(S))dS + √2α Z dW(s)
Jo	Jo
—一一	J
^^{^^
q2(h)
1 - e-γ 2
+----------
Y
Pi - I Vf(q2(Sy)dS
Jo
'---------7----------'
P2( h)
Lfh 1 - e-τ(h-s)
=p2γ4	-
dB(S) - Z αVf (q2(S))dS + √2α / dW(s)-
Jo	Jo
1 - e-γ 2
Y
Z Vf(q2(S))dS
Jo
1 - e-γ 2
+ q o +
Y
P0+
p2Y∣o2 上—dB(S) +
1 - e-γ 2
Y
√2Y/2 e-M2-S)dB(S)
_ o _	J
Qi
1 - e-γh	(	1 - e
=qt+---------Po - α +------
Y	∖	Y
h
Vf (q 2 (S))dS
{z
Pi
-Y2	,
e 7 2 P o +
+ √2α /h dW (S) + √2Y Ihh 1-e-7(h-S) dB(S) + √2Y ∣o 21-e；2-S) dB(S)
+ 1-；-72 √2Y /2 e-7(2-SdB(S■)
P(h) =e-7 2
--y2
=e 7 2
Pi - Z Vf (q2(S))dS
Jo
'----------7------------Z
P2(h)
+ √2YL e-7(h-S)dB(S)
e-72po + √2Y/ 2 e-7(2-S)dB(S)
X--------------2---------------J
{z
Pi
-e-72 Lh Vf (q2(S))dS + √2Y/h e-7(h-S)dB(S)
w-IhPO - e-72 /h Vf (q2(S))dS + e-72 √2Y/ 2 e-7(2-S)dB(S) + √2Yf L'" e∏dB(S)
26
Under review as a conference paper at ICLR 2022
It is clear that ^(h), p(h) should be compared with the exact solution of HFHR at time h, which can
be written as
q(h) =qo + :P0-((1^S + 0) Vf (q(s))ds + √ɑ / dWS + pY/ 1-^dBs
p(h) =e-γhp0 -广 e-γ(h-s)Vf (q(s))ds + M 广 e-γ(h-s)dB(s)
00
Subtracting q(h),p(h) from q(h), p(h) respectively, we obtain
q(h - q(h)
-	α+
h
Vf(q2(s)) - Vf(q(s))ds
+Z0h
1 — e
Y
1 - e-γ(h-s)
Y
1 — e-γ2 ʌ
—Y- ) Vf(q(s))ds
—
hh
P(h) - P(h) = - e-γ2 L Vf (q2(s)) - Vf (q(s))ds + L (e-γ(h-S)- e-γ2) Vf (q(s))ds
It should be clear now that we will need to bound the term Vf(q2) - Vf (q) and Vf(q). Since
, 、 l 1 -e-Y2	l
q2(r) =qo +---------Po +
Y
1 - e-γr
q(r) =qo +--------Po -
Y
P2Y/ 2 1 - eJ S) dB(s) - Q / Vf(q2(s))ds + √20/ dW(S)
+ α) Vf(q(s))ds + √201 dW(S)
l_ rr 1 _ e-γ(r-S)
+ P2γ]0 ——γ——dB(s),
we then have
e-γr	e-γ
q2(r) - q(r)=-------------
Y
+p2Y∣o
2	r	r 1 _ e-γ(r-S)
一…卜Hf…+/	Vf…
2 1 - e Y(2 S) dB(s) - p2γ Zr 1 - e Y(r S) dB(s)
Y	oY
By Lemma D.3 and D.2, when 0 < h < 击，we have the following for the solution of HFHR
dynamics
E[x0,x0(h) -x02] ≤ Cb0h
where Cbo = 5.14 {(α + γ)d + h (L0)2kxok2} and hence
E
r
o Vf(q(S))2dS
≤E
r
2 o Vf(q(0))2
dS + 2
r
Vf (q(S)) - Vf (q(0))2
o
dS
≤E 2L2rq(0)2 +2L2
q(0)2 dS
≤2L2rkxok2 +2L2E
q(S) - q(0)2 dS
o
≤2L2rkxok2 + 2L2Cbo	r
SdS
≤L2r 2kxok2 + hCbo
≤L2r 2.33kxok2 + 5.14(α + Y)dh
(12)
27
Under review as a conference paper at ICLR 2022
Now E kq2 - q k2 can be bounded as follow E hq2(r) - q(r)2i -(Q-Yr __ Q-Y2 ∖ 2	r ≤5< (--——)kPok2 + α2E / Vf(q2(s)) + 5 ∣27E ZQ 2 1 - e；(2-S) dB(s)	+ 2γE ZQr ≤5∣ h42kxok2 + α2L2r [ E∣∣q2(s) - q(s)∣∣2 ds + J + 5 " + *) ≤5 h h-kxok2 + α2L2r Z E∣∣q2(s) - q(s)∣∣2 ds + h 40	— Vf (q(s))ds 2 + E /r 1 — el-，) Vf (q(s))ds } 1 - e-γ(r-s)	21 	dB(s)	〉	(CaUchy-SchWartz Inequality) γ	∣∣ 	1 ：(1 — ell) ! ds lo r E∣∣Vf (q(s))∣∣2 ds 3]/ r∣∣vf(q(s))∣∣2] +3γdh3)
≤5 { W kx0『+—4^~h3 +^3L2(2.33||xo『+ 5.14(α + γ)dh) r ≤5h2 {%||x0『+-4-^~h +~3L2(2.33||xo『+ 5.14(α + γ)dh) j By GronWall’s inequality and 0 < h ≤ 击，we have E [∣∣q2(r)- q(r)∣∣[ ≤5h2 {4Ilxok2 + -ɪh + -3-L2 (2.33kχ0k ≤5h2 {4Ilxok2 + -ɪh + ^3L2 (2.33kx0k ≤5.85h2 0.28kx0k2 + (0.06α + 0.81γ)hd ≤h2 n1.64kxok2 + (0.36α + 4.74γ)hdo .	+ α2L2r Z E∣∣q2(s) - q(s)∣∣2 ds + 5α2L2 h Z E∣∣q2 (s) - q(s)∣∣2 ds 2 + 5.14(α + γ)dh	exp{5α2L2 h2} + 5.14(α + γ)dh) } e32 (13)
28
Under review as a conference paper at ICLR 2022
With bounds in Equation (12) and (13), we are now ready to show pi and p2. For p1, i.e. the order
of the mathematical expectation of deviation, we have
E [Γ⅛(h)l Jq(h)]]
E[ [p(h)J - [p(h)JJ
≤ E [⅛(h) - q(h)] I ∣+∣∣E [p(h) - p(h)]∣∣
h
j* 2
Jo
1 h
α + 1+ 2
)/h E [V/(q2(s)) -V/(q(s))] ds +
E[V∕(Q2(S))-V/(q(s))] ds
Ljh
Jo
E 11 q2 (s) - q(s) ∣ ∣ ds
—
1 - e-γ 2
Y
-e-γ2) E [V/(q(s))] ds
E [V/(q(s))] ds
+
≤L
Y
h
+1(∕h
1 - e-γh
Y
+ ∣e-γ(h-s) - e-γ2∣ ∣∣E [V/(q(s))]∣∣ds
E q2 (s) - q(s) ds
1 - e-γ(h-s)
Y
1 - e-γ2
Y
1
2
ds
≤L
h
E q2(s) - q(s)
1
2、2
[V/(q(s))]∣ ∣ 2 ds)
-e-γh∣2 ds 1 ∖ I I：h∣∣E [V/(q(s))]∣∣2 ds)
1	1 - e-γ2
≤α+丁一
.	--γ 区
+ e γ 2
+
I： Lj)
≤
—
—
≤L (α + 1 + B) h2 {1.64∣∣xo∣∣2 + (0.36α + 4.74γ)hd} ? + :√^ h2L(2.33||xo||2 + 5.14(α + γ)dh)
1
2
≤L (α + 1.25) h2 (1.29∣∣xo∣∣ + √0.36α + 4.747√hd) +(1 + Y)h2L 仪.45∣∣xo∣∣ + √0.43α + 0.437√dh)
≤Lh2 max{α + 1.25, y + 1} (1.74∣∣xo∣∣ + (1.26√α + 2.84√y)√hd)
The above derivation proves pi = 2 with
Ci = L max{α + 1.25, y + 1} (1.74∣∣xo∣∣ + (1.26√α + 2.84√Y)√hd).
29
Under review as a conference paper at ICLR 2022
We now proceed with p2, i.e. mean-square error
E腐)
q(h)2
p(h)
≤2 (α+h) E
Zh vf(q2(s))-vf(q(s))ds
0
2
+ 2E
h 1 -e-γ(h-s)
Jo	∖ Y
1 - e-γ2 \
一γ一 ) Vf (q(s))ds
+ 2E
Zh
0
▽f(Q2(s))-Vf(q(s))ds	+2E
Lh 卜-Mh-S) - e-γ2) Vf (q(s))ds
≤2 ((α+2)2+1)L2E (/ |q2(s)- q(s)ιds! +2Z -
+ 2/ ∣e-γ(h-s) - e-γ⅛∣2 ds / E∣∣Vf(q(s))∣∣2 ds
≤2 ((α + 2)2 + 1) L2hZ E|q2(S) - q(S)12ds H—h3
1 - e-γ(h-s)
Y
1 - e-γ 2
Y
h
E|vf(q(s))|2ds
0
ds ZhE∣∣vf(q(s))∣∣2ds
0
—
—
2
2
—
2
≤2 ((α + ?)2 + 1)L2 {l.64kxok2 + (0.36α + 4.74γ)hd} h4 +
匕1! L
≤L2 max{(α + 1.25)2, 1 + γ2} 3.67kx0k2 + (1.68α + 10.34γ)hd
6
h4
n2.33kx0k2 + 5.14(α + γ)hdo h4
The above derivation implies p2 = 2 with
Cb2 = Lmax{a + 1.25,1 + γ} (l.92∣∣xok + (1.30√α + 3.22√γ)√hd^
□
D.6 Local error between Algorithm 1 and the exact Strang’s splitting
METHOD
Lemma D.7. Assume f is L-smooth, 0 ∈ argminx∈Rd f (x), i.e. vf (0) = 0 and the operator
V∆f grows at most linearly, i.e. ∣∣V∆f (q)∣∣ ≤ G y 1 +kq∣∣2. If 0 < h ≤ 击,then compared with
the exact Strang’s splitting method of HFHR dynamics, the implementable Strang’s splitting method
has local mathematical expectation of deviation of order p1 = 2 and local mean-squared error of
order p2 = 1.5, i.e. there exist constants C1,C2 > 0 such that
∣∣EX(h) — Ex(h)∣∣ ≤ GhpI
(E h∣∣x(h) - x(h)∣∣2i)2 ≤ Ghp2
where x(h)
value
x0
x0
q0
p0
q(h)
p(h)
q0
p0
is the solution of the exact Strang’s splitting method for HFHR with initial
and x(h)
q(h)
P(h)
is the one-step result of Algorithm 1 with initial value
, p1 = 2 and p2 = 1.5. More concretely, we have
G = α(α + 1.125)(L + G) [θ.5 + 0.71kx0k + (1.14√α + 0.21√γh)
and
G = L(α + 0.73) (2.30√haLkxok + (2.27√α + 0.12√γh)√d)
30
Under review as a conference paper at ICLR 2022
Proof. The solution of one-step exact Strang’s splitting integrator with step size h can be written as
∖= = q2(h) + 1-e-γ2P2(h) + √2γ Rh j-γ(h-s) dB(s)
2 2
p3 = e-γ2p2(h) + √2γ Rh e-γ(h-s)dB(s)
<	q2(r)	=	qi	-	Rr	aVf (q2(s))ds +	√2a Rr	dW(S)	(0 ≤ r ≤ h)
P2(r) = Pi - Rr Vf(q2(s))ds
q1 = qo + —l Po + √2γ Rr21-e-γ( h2-s) dB(s)
、P1 = e-γ2Po + √2γ RO e-γ(2-s)dB(s)
and the solution of one-step implementable Strang’s splitting integrator with step size h can be
written as
%=⅞2(h) +	2P2(h) + √2Y Ro2 1-e-Y(2-s) dB(h2 + S)
p3 = e-γ2p2(h) + √2γ Ro2 e-Y(2-s)dB(h + S)
<	@2(r) = qi - RraVf (qi)ds + √2α Rr dW(S)	(0 ≤ r ≤ h)
'P2(r) = Pi - Rr Vf (qι)ds
qi = qo + —tPo + √2γ Rr2「2-S) dB(s)
Pi = e-γ2Po + √2γ RO e-γ(2-s)dB(s)
Note that in the implementable Strang’s splitting method, φ flow can be explicitly integrated and
hence qi , Pi are the same as that in the exact Strang’s splitting method.
First, we will bound the deviation of mathematical expectation and mean squared error of q2 (h) -
q2(h) andP2(h) 一P2(h). Wehave
qq2(h) — <h(h) =	-α Rh Vf (q2(s)) — Vf(qi)ds
、P2(h) — P2(h) =	-Rih Vf(q2(Sy)- Vf (qi)ds
(14)
Square both sides of the first equation in (14) and take expectation, we obtain
E∣∣q2(h)- ⅛2(h)∣∣2 =a2E
Z Vf(q2(S)) - Vf(qi)dS
o
2
≤a2EZ ∣∣Vf(q2(S)) - Vf (qi)∣∣ dS!
≤α2L2EZ q2(S) - qi dS!
≤α2L2h Z Eq2(S) - qi 2 dS
o
Note that q2 is the solution of a rescaled overdamped Langevin dynamics whose drift vector field is
aL-Lipschitz, by conditional expectation version of Lemma D.2, for 0 < h < 击 < 4Ol, We have
E∣∣q2(h) - qi∣∣2 ≤ COh with Cr = 5.14 {ad + h(aL)2Ekqik2} and it follows that
(E∣∣q2(h) - q2(h)∣∣2 ≤ α2L2Crh3
[E∣∣P2(h) - P2(h)∣∣2 ≤ L2Crh3.
Now consider pi , i.e., the deviation of mathematical expectation. By Ito’s lemma, we have
q2(h) - q2(h)
= - aZ Vf(q2(S)) - Vf(qi)dS
o
h
s
-α
h
oo
-aV2 f (q2 (r))Vf (q 2 (r ))dr + a
V∆f (q2(r))dr +ρ dS (15)
o
31
Under review as a conference paper at ICLR 2022
where ρ is a stochastic integral term. Take expectation and norm for Equation (15), we have
E@㈤-兆(h)]
α2
≤α2
∣∣Z h E
∣0
ZhE
0
V2f (q2(r))Vf (q2(r))dr -	V∆f (q2(r))dr ds∣∣∣
O	O∣
kV2f(q2(r))k2∣∣Vf(q2(r))∣∣ dr +	∣∣V∆f (q2(r))∣∣ dr ds
OO
h
≤α2
hE L sq2(r) dr + s G(1 +q2(r))dr
00	0
ds
0
0
0
=α2(L + G) Lh j： E∣∣q1(r)∣∣ dr + α2G1h22
≤α2(L + G) / / E∣∣q2(r) - qι ∣∣ + EkqIk dr + a2G^2^
≤α2(L + G) /h [，E|q1 (r)-qι∣∣2 + EkqIk d + O2Gh2
U-h1
≤α2 (L + G) ∖/ COh ^^2
≤α2
√CC0h + EkqIk
h2	h2
+ α2(L + G)-2- EkqIk + α2G -2
I(L + G) + ɪ } h2
2
≤2α2(L + G) nPCCOh + Ekqιk + 10 h2
Similarly, WehavelE [p2(h) -P2(h)] ∣∣ ≤ 11 a(L + G) {pC0h + EkqIk + l} h2.
For p2, i.e., mean-square error, we have
E∣∣q1㈤-q1(h)∣∣2 ≤α2E(/ ∣∣vfg1(S))- vfgj∣∣ ds)
≤α2E {/h Ids ∕h∣∣Vf(q1(s)) - Vf (q1)∣∣2 ds I
≤α2L2hZh E∣∣q2(S) - q1∣∣2 dS
O
≤ α2L2°0 h3
_	2
22
Similarly we obtain E∣∣p1(h) - p1(h)∣∣ ≤ l2C0h3. Recall
q3- @3 = q1(h)- q1(h) + 1-eγγ 2 (P1(h) - p1(h))
P3 - p3 = e-γh (P1(h) - p2(h))
and it follows that when 0 < h ≤ 4L7 < 1
P3- Pj+1+2)(L+G)
PCCOh + EkqIk + 1 h1
E
2
e∣[P3 - P3 1≤l"卜+1+h42! h3
(16)
(17)
32
Under review as a conference paper at ICLR 2022
Finally we need to bound Ekq1 k2 by Ekx0k2, to this end, we have
2	1 - e-γ2	,一 /2 1 - e-γ(2-S)
EkqIk =E qo +-----Y----Po + V^YjQ -----Y-----dB(s)
≤(1 + h42)Ekqok2 + (1 + h42)EkPok2 +2γd/2 (1-e-12-S)! ds
≤(1 + 勺 EkxOk2 + Y2 h3
=(1 + h42 )kxok2 + Ydh3
(18)
(19)
Collecting all pieces together, including (16), (17), (19), the definition of Co and 0 < h < 圭,it is
not difficult to obtain the following
with
G = α(α + 1.125)(L + G) ∣0.5 + 0.71kx0k + (1.14√α + 0.21√γh)√hd]
and
G = L(α + 0.73) (2.30√haLkxok + (2.27√α + 0.12√γh)√d)
□
D.7 Local error between Algorithm 1 and HFHR dynamics
Lemma D.8. Assume f is L-smooth, 0 ∈ argminχ∈Rd f (x), i.e. Vf (0) = 0 and the operator
V∆f grows at most linearly,
i.e.∣∣V∆f (q)∣∣ ≤ G √1+kqk2.If 0
< h ≤ 41τ0, then compared with
the HFHR dynamics, the implementable Strang’s splitting method has local weak error of order
p1 = 2 and local mean-squared error of order p2 = 1.5, i.e. there exist constants C1, C2 > 0 such
that
∣∣Ex(h) - Ex(h)∣∣ ≤ C1hp1
where x(h)
q(h)
p(h)
1
(E h∣∣x(h)- x(h)∣∣2i)2
≤ C2 hp2
is the solution of HFHR with initial value xo
qo
po
and x(h)
q(h)
p(h)
is the solution of the implementable Strang’s splitting with initial value xo = qo , p1 = 2 and
po
p2 = 1.5. More concretely, we have
Ci = (L+G) maχ{α+1.25, γ+1} [θ.5α + (1.74 + 0.71a)kxok + (1.26√α + 1.14α√α + 2.32√γ) √hd
and
C2 = Lmaχ{α + 1.25, Y + 1} [(1.92 + 2.30ɑL)√hkxok + (2.60√α + 3.34√γh)√d]
33
Under review as a conference paper at ICLR 2022
Proof. Denote by x(h)
q(h)
p(h)
the solution of the exact Strang’s splitting method with initial
value x0 = q0 . By triangle inequality and Minkowski’s inequality, we have
p0
∣∣Ex(h) - EX(h)∣∣ ≤∣∣Ex(h) - EX(h)∣∣ +∣∣EX(h) - Ex(h)∣∣,
III
(E∣∣x(h) — X(h)|『)2 ≤ (E∣∣x(h) — X(h)『)？ + ^E∣∣x(h) — X(h)|『)?
By Lemma D.6 and D.7, we have
∣∣Ex(h) — Ex(h)∣∣ ≤ CIh2,	∣∣Ex(h) — Ex(h)∣∣ ≤ CIh2
(E∣∣x(h) — x(h)∣∣2) 2 ≤ C2h2,	(E∣∣x(h) — x(h)∣∣2) 2 ≤ C2h2
and hence
∣∣Ex(h) — Ex(h)∣∣ ≤(Cbι + G)h2
1
(E∣∣x(h) — x(h)∣∣2)2 ≤(C2 + C2)h2
with
Cl + G ≤Cι
,(L + G) max{a + 1.25, γ + 1} [θ.5α + (1.74 + 0.71a)kxok + (l.26√α + 1.14α√α + 2.32√γ) √hd]
C2 + C2 ≤C2，Lmax{a + 1.25, Y + 1} [(1.92 + 2.30ɑL)√hkxok + (2.60√α + 3.34√γh)√d]
□
E α DOES CREATE ACCELERATION EVEN AFTER DISCRETIZATION: AN
ANALYTICAL DEMONSTRATION
If a → ∞ while Y remains fixed, then dq = —αVf (q) + √2ɑdW is the dominant part of the
dynamics, and in this case the role ofα could be intuitively understood as to simply rescale the time
of gradient flow, which does not create any algorithmic advantage, as the timestep of discretization
has to scale like 1∕a in this case. However, finite α no longer corresponds to solely a time-scaling,
but closely couples with the dynamics and creates acceleration. This is true even after the continuous
dynamics is discretized by an algorithm .
We will analytically illustrate this point by considering quadratic f. In this case, the diffusion
process remains Gaussian, and it suffices to quantify the convergence of its mean and covariance. In
fact, it can be shown that both have the same speed of convergence, and therefore for simplicity we
will only consider the mean process. Two demonstrations (with different focuses) will be provided.
Demonstration 1 (1D, Y given; infinite acceleration). Consider f(x) = x2∕2, Y fixed. The mean
process is
[q = P — αq
Ip = q — γp
Consider, for simplicity, an Euler-Maruyama discretization of the HFHR dynamics, which
coressponds to a Forward Euler discretization of the mean process (other numerical methods can
be analyzed analogously):
qk+1 = A qk
pk+1	pk
1 — αh h
—h 1 — Yh .
A
We will show that, unless Y = 2, an appropriately chosen α will converge infinitely faster than the
case with α = 0, if both cases use the optimal h.
34
Under review as a conference paper at ICLR 2022
To do so, let us compute A’s eigenvalues, which are
2(2 - (a + γ)h ± hP—4 + (α - γ)2)
Consider the case where ∣α 一 γ∣ ≤ 2, then the eigenvalues are a pair of complex conjugates. Their
modulus determines the speed of convergence, and it can be computed to be
2p(2 ― (α + γ)h)2 + h2(4 ― (α ― γ)2) = pl - (α + Y)h + (1 + αγ)h2
Minimizing the quadratic function gives the optimal h that ensures the fastest speed of convergence,
and the optimal h is
h = -α-+Y-
2(1 + αγ)
and the optimal spectral radius is
1-
(α + γ)2
4(1 + αγ).
When one uses low-resolution ODE, in which α = 0, the optimal rate is 1 一 γ2∕4 (note it is not
surprising that the critically damped case, i.e., γ = 2, will give the fastest convergence).
If γ 6= 2, the additional introduction of α can accelerate the convergence by reducing the spectral
radius. For instance, if α = Y + 2, upon choosing the optimal h = ι+γ, the optimal spectral radius
is 0 (note in this case A actually has Jordan canonical form of
01
00
converges in 2 steps instead of 1, irrespective of the initial condition).
and thus the discretization
Demonstration 2 (multi-dim, Y, α and h all to be chosen; acceleration quantified in terms
of condition number). Consider quadratic f with positive definite Hessian, whose eigenvalues
are 1 = λ1 < •…< λn = e-1 for some 0 < e《1. Assume without loss of generality that
f = q12∕2 + -1q22∕2. Similar to Demonstration 1, the forward Euler discretization of the mean
process is
q1,k+1
p1,k+1
q2,k+1
A1
0
q1,k
0 Pι,k
A2
q2,k
1 ― αh h
―h	1 — Yh
A2
1 ― α-1h	h
―-1 h	1 ― Yh
p2,k+1	p2,k
(20)
We will (i) find h and Y that lead to fastest convergence of the ULD discretization, i.e. the above
iteration with α = 0, and then (ii) constructively show the existence of h, Y and α that lead to
faster convergence than the optimal one in (i) — note these may not even be the optimal choices for
HFHR, but they already lead to significant acceleration. More specifically,
(i) In a ULD setup, α = 0. It can be computed that the eigenvalues of A1 and A2 are respectively
ɪ(2 — hγ ± hp-4 + γ2)	and ɪ ^2 — hγ ± hp-4e-1 + γ2
We now seek Y > 0, h > 0 to minimize the maximum of their norms for obtaining the optimal
convergence rate. This is done in cases.
Case (i1) When Y ≤ 2, both A1 and A2 eigenvalues are complex conjugate pairs. To minimize the
maximum of their norms, let's first see if their norms could be made equal.
A1 eigenvalue’s norm squared ×4 is
(2- hY)2 -h2(-4+Y2) = 4(h - Y∕2)2 +4-Y2	(21)
A2 eigenvalue’s norm squared ×4 is
(2 - hY)2 - h2(-4-1 + Y2) = 4-1(h - Y∕2)2 + 4 - Y2	(22)
35
Under review as a conference paper at ICLR 2022
It can be seen that for (21) is always strictly smaller than (22) for any h > 0. Therefore, the max of
the two is minimized when h = eγ∕2, and the corresponding max value is 4 一 eγ2. Y that minimizes
this max value is γ = 2. Corresponding rate of convergence is
√1 一 €.
CaSe (i2) When Y ≥ 2€-i/2, both Ai and A2 eigenvalues are real. Since €《 1, we can order
them × 2 as
2—hY—hp—4 + Y2 < 2—h)—hp—4€-i + Y2 < 2—h)+hp—4c-1 + Y2 < 2—h)+hp—4 + Y2 < 2.
To minimize the max of their norms, consider cases in which the smallest of four is negative, in
which case at optimum one should have
—(2 — hY — hp—4 + y2 ) = 2 — hY + hp—4 + Y2.
This gives h = 2∕γ (which does verify the assumption that the smallest of four is negative). Corre-
sponding max of their norms is thus vz1 — 4∕γ2. Y that minimizes this max value is Y = 2€-i/2,
which gives rate of convergence of
ʌ/l — €.
CaSe (i3) When 2 ≤ Y ≤ 2€-1/2, Ai eigenvalues are real and A2 eigenvalues are complex conju-
gates. Again, the max of their norms is minimized if the norms can be made all equal.
Note Ai eigenvalues cannot be of the same sign, because otherwise 2 — hγ — h，一4 + Y2 =
2 — hγ + h ,—4 + Y2, which means either h = 0 or Y = 2, but if Y = 2 then 2 — hγ + h ,—4 + γ2
being equal to 2*norm of A2 eigenvalue, which is ,4fT(h - €y/2)2 + 4 - €y2, leads to h = 0
again.
Therefore, the equality of norms of Ai , A2 eigenvalues means
—(2 — hY — hp—4 + y2 ) = 2 — hY + hp—4 + y2 = p4c-i(h — €y/2)2 +4 — €Y2.
The first equality gives hγ = 2, which, together with the second equality, gives h = ± J高.
Selecting the positive value of optimal h, we also obtain optimal Y =，2(1 + €)€-1/2, which
is ≤ 2€-1/2 and thus satisfying our assumption (2 ≤ γ ≤ 2€-1/2). The corresponding rate of
convergence is thus
1 — €
1 + €
Summary of (i) Since √⅛ < √1 — €, the ULD Euler-Maruyama discretization converges the
fastest when
h = J l"+p	Y = P2(1 + €)€-1/2,
and the corresponding discount factor of convergence (i.e. base of exponential convergence) is
/1 —€	,	/I∣
V I + , where € = 1∕κ with K being Hessian S condition number.
(23)
(ii) Now consider the HFHR setup. Let's first state a result: when
√4c2E4 + 8c2€3 + 4c2€2 + €2 — 2€ +1 + € + 3
Y =------------------2c€2 +2c€----------------- > 0,
—，4c2€4 + 8c2€3 + 4c2€2 + €2 — 2€ +1 +3€ + 1 C 7
α =----------------------------------------------- > 0, h = c€
2c€2 + 2c€	,
for any c > 0 independent of €, the iteration (20) converges with discount factor
6(1+ )，(1 - €)(1 - € + p4c2€4 + 8c2€3 + (4c2 + 1) €2 — 2€ + 1).
(24)
(25)
(26)
36
Under review as a conference paper at ICLR 2022
While the exact expression is lengthy, it can proved that the HFHR non-optimal discount factor (26)
is strictly smaller than the ULD optimal discount factor (23) for not only small but also large ’s.
For some quantitative intuition, discount factors respectively have the following Taylor expansions
in :
HFHR non-optimal:	1 - 2e + (c2 + 2)e2 + O 卜3)	(27)
2
ULD optimal:	1 - e + — + O (e3)	(28)
Figure 4: Acceleration of HFHR algorithm over ULD algorithm (despite of an additional constraint
α may place on h) for multi-dimensional quadratic objectives. 1/e is the condition number.
The exact expressions of discount factors are also plotted in Fig.4 (c = 1 was arbitrarily chosen) and
one can see acceleration for any (not necessarily small) e.
(ii details) How were values in (25) chosen? Following the idea detailed in (i), we consider a case
where Ai eigenvalues are both real, A2 eigenvalues are complex conjugates, and all their norms are
equal. Note there are 3 more cases, namely real/real, complex/real, and complex/complex, but we
do not optimize over all cases for simplicity — the real/complex case is enough for outperforming
the optimal ULD.
This case leads to at least the following equations
trA1
det A1 + det A2
0
0
(29)
One can solve this system of equations to obtain α and γ as functions of h. Following the idea of
choosing h small enough to resolve the stiffness of the ODE
J«2 = P2 - αe-1q2
[p2	= -e-lq2 — YP2
pick h = ce. Then (29) gives
_ √4c2e4 + 8c2e3 + 4c2e2 + e2 - 2e +1 + e + 3
Y =	2ce2 + 2ce
α
or
γ
α
-√4c2e4 + 8c2e3 + 4c2e2 + e2 - 2e +1 + 3e +1
2ce2 + 2ce
-√4c2e4 + 8c2e3 + 4c2e2 + e2 - 2e +1 + e + 3
2ce2 + 2ce
√4c2e4 + 8c2e3 + 4c2e2 + e2 - 2e + 1 + 3e + 1
2ce2 + 2ce
37
Under review as a conference paper at ICLR 2022
The former is our choice (25) because it can be checked that the latter leads to det A1 > 0 which
violates the assumption of a pair of plus and minus real eigenvalues.
It is possible to find optimal α, γ, h for HFHR for the Gaussian cases. One has to minimize det A2
under the constraint det A2 > 0 in addition to (29). And then do similar calculations for the other 3
cases, and then finally the best among the 4 cases. Doing so however does not give enough insights
to determine optimal hyperparameters for sampling general distributions.
F Randomized Midpoint Dis cretization of HFHR
F.1 The algorithm
HFHR is based on a continuous dynamics that adds HFHR corrections to the Underdamped
Langevin Dynamics (ULD). It can be turned into a sampling algorithm via either a low-order time
discretization (e.g., HFHR Algorithm 1) or a more accurate one. To complement the main text, this
section demonstrates the latter, based on a powerful recent progress in discretizing ULD, known
as Randomized Midpoint Algorithm (RMA) (Shen & Lee, 2019), and shows that the acceleration
created by the HFHR correction terms persists.
More specifically, RMA is a high-order discretization scheme for ULD that achieved a better O(d3)
dimension dependence of mixing time than first-order discretization of ULD, e.g., 1st-order KLMC
(Dalalyan & Riou-Durand, 2020). Although RMA is originally designed specifically for ULD only,
it is a general idea and already adapted to overdamped Langevin (He et al., 2020). Here we show it
can be easily adapted to HFHR as well, as illustrated by the following Algorithm 2. Red highlights
algorithmic changes we made to account for the HFHR corrections of ULD.
Algorithm 2 Randomized Midpoint Algorithm from Shen & Lee (2019), adapted for HFHR
1:	Input: potential function f and its gradient Vf, damping coefficients a and γ, step size h,
initial condition (q0 , p0 )
2:	procedure RMA-HFHR(f, Vf, α, γ, h, q0 , p0)
3:	k = 0 and initialize q0
p0
4:	while not converged do
5:	Generate an independent uniform random variable θk 〜U(0,1)
6:	Generate Gaussian random vectors W 1k+1, W 2k+1, W 3k+1 ∈ R3d as in (Shen & Lee,
2019, Appendix A)
7:	Generate Gaussian random vectors B1k+1, B2k+1 ∈ Rd as described by (31)
8:	qk+ 2 = qk + 1 (1 - e-γθkh)Pk - 1 (θkh - 1 (1 - e-γθkh)) Vf (qk) + Wk+1
-αθkhVf (qk) + √2αBk+1
9:	qk+1 = qk + 1 (1 - e-γh)Pk - 1 h(1 - e-γ(h-θk吟Vf®+ 2) + Wk+1 -
αhVf(qk+1) + √2α(Bk+1 + Bk+1)
10:	Pk+1 = Pke-γh - he-γ(h-θkh) Vf。+ 2) + 2Wk+1
11:	k — k + 1
12:	end while
13:	end procedure
The red parts basically correspond to two Euler-Maruyama time-steppings ofan auxiliary dynamics
that contains only the HFHR correction terms
dq = -αVf (q)dt + √2αdBt,
(30)
first over a θkh timestep, and then over an h timestep. These two steps originate from an operator
splitting treatment of the full HFHR dynamics (eq.6), which is split into ULD and (30). Therefore,
it is natural to see that
h(k+θk)
Bk+1 = hk
dBt,
h(k+1)
Bk+1 =	dBt,
h(k+θk)
38
Under review as a conference paper at ICLR 2022
and therefore Bk1+1 and Bk2+1 are, when conditioned on θk, centered Gaussian vectors independent
from each other and the W’s, each being d-dimensional with i.i.d. entries, and they can be generated
via	____ ____________________________
Bk + 1 = vzθkhξk+1,	Bk+1 = VZh - θkhξk+1,	(31)
where ξ1k+1 and ξ2k+1 are i.i.d. standard d-dimensional Gaussian vectors.
Remark F.1. In the original RMA (Shen & Lee, 2019, Algorithm 1), the uniform random variable
for the midpoint’s proportional location was denoted by α. However, since we have already used this
letter for the HFHR correction coefficient, we use instead θ to denote this uniform random variable.
Remark F.2. From the red text, it is easy to see that if α = 0, Algorithm 2 degenerates to RMA for
ULD. Nevertheless, Algorithm 2 is again just one RMA discretization of HFHR but not the only one.
F.2 Numerical results: HFHR again accelerates
To numerically compare the RMA discretization of HFHR dynamics and ULD dynamics
(note we don’t compare 1st-order HFHR Algorithm 1 with RMA-ULD as we’d like to com-
pare apple with apple), we conduct an experiment very similar to that in Sec.6.1, with the
same nonlinear potential function. We run both RMA for ULD and RMA for HFHR with
dimension d = 10, initial value (100 × 1d, 0d), h = 1 (chosen to be near the stabil-
ity limit of RMA-ULD), a family of γ ∈ {0.1, 0.2, 0.5, 1,2,5, 10, 20, 50, 100} and α ∈
{0, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1,
2, 5, 10, 20, 50, 100}. For each algorithm and each set of parameter values, we run 1,000 indepen-
dent realizations to compute statistics and estimate the mean time of reaching ε = 0.1 neighborhood
of the target distribution. Then, for each α (including α = 0, which is the original RMA), we
optimize over γ choices to get the best results. To further reduce variance, we also repeat the
experiment with 100 different random seeds.
Too large α values with which Algorithm 2 fails to reach -neighborhood are not plotted and the
final results are shown in Figure 5. It clearly suggests that with appropriated chosen α (α = 0.5 in
our case), RMA discretized HFHR dynamics requires fewer iterations than RMA discretized ULD,
which suggests a better iteration complexity.
ωωφ⊂φωo-olω LIoB9」Ol SJΦ±±Jo #
18
16
14
12
10
8
0	10^3	10^2	10^1	10°
a
Figure 5: Improvement of RMA for HFHR (Algorithm 2) over the original RMA (for ULD) in
iteration complexity. (vertical bar = 1 std.)
39