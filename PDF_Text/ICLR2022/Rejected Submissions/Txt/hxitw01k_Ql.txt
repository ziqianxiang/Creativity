Under review as a conference paper at ICLR 2022
How memory architecture affects learning in
a simple POMDP: the two-hypothesis testing
PROBLEM
Anonymous authors
Paper under double-blind review
Ab stract
Reinforcement learning is generally difficult for partially observable Markov de-
cision processes (POMDPs), which occurs when the agent’s observation is partial
or noisy. To seek good performance in POMDPs, one strategy is to endow the
agent with a finite memory, whose update is governed by the policy. However,
policy optimization is non-convex in that case and can lead to poor training per-
formance for random initialization. The performance can be empirically improved
by constraining the memory architecture, then sacrificing optimality to facilitate
training. Here we study this trade-off in a two-hypothesis testing problem, akin
to the two-arm bandit problem. We compare two extreme cases: (i) the random
access memory where any transitions between M memory states are allowed and
(ii) a fixed memory where the agent can access its last m actions and rewards. For
(i), the probability q to play the worst arm is known to be exponentially small in
M for the optimal policy. Our main result is to show that similar performance
can be reached for (ii) as well, despite the simplicity of the memory architecture:
using a conjecture on Gray-ordered binary necklaces, we find policies for which
q is exponentially small in 2m, i.e. q 〜 α2m with α < 1. In addition, We observe
empirically that training from random initialization leads to very poor results for
(i), and significantly better results for (ii) thanks to the constraints on the memory
architecture.
1	Introduction
Reinforcement learning is aimed at finding the sequence of actions that should take an agent to
maximise a long-term reward (Sutton & Barto (2018)). This sequential decision-making is usually
modeled as a Markov decision process (MDP): at each time step, the agent chooses an action based
on a policy (a function that relates the agent’s state to its action), with the aim of maximizing its value
(the expected discounted sum of rewards). Deterministic optimal policies can be found through
dynamic programming (Bellman (1966)) when MDPs are discrete (both states and actions belong to
discrete sets) and the agent fully knows its environment (Watkins & Dayan (1992)).
A practical difficulty arises when the agent only have a partial observation of its environment or
when this observation is imperfect or stochastic. The mathematical framework is then known as
a partially observable Markov decision process (POMDP) (Smallwood & Sondik (1973)). In this
framework, the agent’s state is replaced by the agent’s belief, which is the probability distribution
over all possible states. At each time step, the agent’s belief can be updated through Bayesian
inference to account for observations. In the belief space, the problem becomes fully observable
again and the POMDP can thus be solved as a “belief MDP”. However, the dimension of the belief
space is much larger than the state space and solving the belief MDP can be challenging in practical
problems. Some approaches seek to resolve this difficulty by approximating of the belief and the
value functions (Hauskrecht (2000); Roy et al. (2005); Silver & Veness (2010); Somani et al. (2013)),
or use deep model-free reinforcement learning where the neural network is complemented with a
memory (Oh et al. (2016); Khan et al. (2017)) or a recurrency (Hausknecht & Stone (2015); Li et al.
(2015)) to better approximate history-based policies.
1
Under review as a conference paper at ICLR 2022
Here we focus on the idea of Littman (1993), who proposed to give the agent a limited number of
bits of memory, an idea that has been developed independently in the robotics community where it
is known as a finite-state controller (Meuleau et al. (1999; 2013)). These works show that adding
a memory usually increases the performance in POMDPs. But to this day, attempts to find opti-
mal memory allocation have been essentially empirical (Peshkin et al. (2001); Zhang et al. (2016);
Toro Icarte et al. (2020)). One central difficulty is that the value is a non-convex function of policy
for POMDPs (Jaakkola et al. (1995)): learning will thus generally get stuck in poor local maxima
for random policy initialization. This problem is even more acute when memory is large or when
all transitions between memory states are allowed. To improve learning, restricting the policy space
to specific memory architectures where most transitions are forbidden is key (Peshkin et al. (2001);
Zhang et al. (2016); Toro Icarte et al. (2020)). However, there is no general principles to optimize the
memory architectures or the policy initialization. In fact, this question is not understood satisfyingly
even in the simplest tasks- arguably a necessary step to later achieve a broad understanding.
Here, we work out how the memory architecture affects optimal solutions in perhaps the simplest
POMDP, and find that these solutions are intriguingly complex. Specifically, we consider the two-
hypothesis testing problem. At each time step, the agent chooses to pull one of two arms that yield
random rewards with different means. We compare two memory structures: (i) a random access
memory (RAM) in which all possible transitions between M distinct memory states are allowed;
(ii) a Memento memory in which the agent can access its last m actions and rewards.
When the agent is provided with a RAM memory, we study the performance of a “column of confi-
dence” policy (CCP): the agent keeps repeating the same action and updates its confidence in it by
moving up and down the memory sites until it reaches the bottom of the column and the alternative
action is tried. The performance of this policy is assessed through the calculation of the expected
frequency q to play the worst arm (thus the smaller q, the better). For the CCP, q can be shown to
be exponentially small in M . This result is closely related to the work of Hellman & Cover (1970)
on hypothesis testing and its extension to finite horizon (Wilson (2014)). In practice, we find that
learning a policy with a RAM memory and random initialization leads to poor results, far from
the performance of the column of confidence policy. Restricting memory transitions to chain-like
transitions leads to much better results, although still sub-optimal.
Our main findings concerns the Memento memory architecture. Surprisingly, despite the lack of
flexibility of the memory structure, excellent policies exist. Specifically, using a conjecture on Gray-
ordered binary necklaces (Degni & Drisko (2007)), we find a policy for which q is exponentially
small in 2m ——which is considerably better than q 〜ln(m)/m, optimal for an agent that only plays
m times. For Memento memory, we also observe empirically that learning is faster and perform
better than in the RAM case.
The code to reproduce the experiments is available at https://anonymous.4open.
science/r/two-hypothesis-BAB3, and uses a function defined here https://
anonymous.4open.science/r/gradientflow/gradientflow. The experiments
where executed on CPUs for about 10 thousand CPU hours.
2	POMDPs and the two-hypothesis testing problem
2.1	General formulation
Definition 2.1 (POMDP). A discrete-time POMDP model is defined as the 8-tuple
(S, A, T, R, Ω, O,po, Y): S is a set of states, A is a set of actions, T is a conditional transition
probability function T(s0|s, a) where s0, s ∈ S and a ∈ A, R : S → R is the reward function1,
Ω is a set of observations, O(o∣s) is a conditional observation probability with o ∈ Ω and S ∈ S,
p0(s) : S → R is the probability to start in a given state s, and γ ∈ [0, 1) is the discount factor.
A state s ∈ S specifies everything about the world at a given time (the agent, its memory and all the
rest). The agent starts its journey in a state s ∈ S with probability p0(s). Based on an observation
o ∈ Ω obtained with probability O(o∣s) the agent takes an action a ∈ A. This action causes a
transition to the state s0 with probability T(s0|s, a) and the agent gains the reward R(s). And so on.
1In the literature, R also depends on the action: R : S × A → R. Our notation is not a loss of generality.
The set of state can be made bigger S → S × A in order to contain the last action.
2
Under review as a conference paper at ICLR 2022
Definition 2.2 (Policy). A policy π(a∣o) is a conditional probability of executing an action a ∈ A
given an observation o ∈ Ω.
Definition 2.3 (Policy State Transition). Given a policy π, the state transition Tπ is given by
Tn (s0∣s) =	^X	T (s0∣s,a)π(a∣o)O(o∣s).	(1)
o,a∈Ω× A
Definition 2.4 (Expected sum of discounted rewards). The expected sum of future discounted re-
wards of a policy π is
∞
Gn = E S0 〜P0	X YtR(St) .	(2)
si 〜Tn (∙∣S0) Lt=0	」
S2 〜T∏(∙∣Sι)
Note that a POMDP with an expected sum of future discounted rewards with discount factor γ can be
reduced to an undiscounted POMDP (Altman (1999)), as we now recall (see proof in Appendix A):
Lemma 2.1. The discounted POMDP defined in 2.1 with a discount γ is equivalent to an undis-
counted POMDP with a probability r = 1 - γ to be reset from any state toward an initial state. In
the undiscounted POMDP, the agent reaches a steady state p(s) which can be used to calculate the
expected sum of discounted rewards Gn = ɪ Es 〜p[R(s)].
2.2	Optimization algorithm
To optimize a policy algorithmically, we apply gradient descent on the expected sum of discounted
rewards. First, We parametrize a policy with parameters W ∈ Riai×iωi, normalized to get a proba-
bility using the softmax function ∏w (a|o) = Pexexw(W)). Then, we compute the transition matrix
Tn, from which we obtain the steady state p using the power method (See Appendix B). Finally, we
calculate Gn by the Lemma 2.1.
Using an algorithm that keeps track of the operations (we use pytorch Paszke et al. (2017)), we
can compute the gradient of Gn with respect to the parameters w and perform gradient descent with
adaptive time steps (i.e. a gradient flow dynamics):
-dw = -d-G∏ (w).
dt dw
(3)
2.3	Two-hypothesis testing problem
The problem we consider is the two-hypothesis testing problem. We label two arms by the letters
A and B. The two arms gives a reward of +1 or -1 with a Bernoulli distribution. The probabilities
to obtain a positive reward are noted kA and kB respectively. The environment is entirely defined
by the couple (kA, kB ). With equal probability, the environment is in one of the following two
configurations (hypothesis):
(kA	=	1++μ	kp	=	1-μ	(hypothesis HA)
k	kA	=	1-μ	kB	=	1+μ	(hypothesis HB)
(4)
where the hypothesis HA (resp. HB) corresponds to A (resp. B) being the best arm. In expectation
over the environments, an agent that plays randomly or always the same arm will have a reward 0.
Note that this problem is similar to the Bandit problem, except that in the latter (kA, kB) can take any
value in the square [0, 1]2. When r → 0, our results below can be generalized to the bandit problem,
as done in Cover & Hellman (1970) by recasting the latter as finding the correct hypothesis ( ‘Arm
A is better’ or ‘Arm B is better’).
In our setup, the agent only knows its last arm played, reward obtained (if there were some) and the
state of its memory (different memories are described below). Based on that, it chooses an arm (play
A or B) and how to update its memory state.
In the POMDP formalism (c.f. 2.1), the state s contains the environment (kA, kB), the memory
state, the last arm played and reward obtained. We only consider agents that have a complete access
3
Under review as a conference paper at ICLR 2022
to their memory, therefore O is deterministic and simply projects s by removing the environment
information.
s = environment HA or HB), memory state, last arm played (A or B) and last reward (1 or -1)
o = memory state, last arm played and last reward
a = arm to play, memory update
(5)
We define the function q(s), which is 1 if the agent just played the ”wrong” arm in state s, and 0 if
he played the correct one. The probability to play the wrong arm is then q∏ = Es〜p[q(s)] where
p is the steady state of the problem with reset r. The expected sum of discounted gains Gπ can be
related to q∏ as: Gn = μ (1 - 2q∏). In the following, We will use q∏ as a measure of performance,
trying to find a policy π that minimizes qπ (and thus maximizes Gπ).
2.4	Types of memory considered
Definition 2.5 (Random Access Memory (RAM)). RAM is the most flexible memory setting. The
agent has M memory states and has full control over it. It has |A| = M × 2 possible actions: the
choice of the next memory state and which arm to play. There is a high degeneracy in the space
of strategies since any permutation of the memory states leads to the same performance. Note that,
since our agent can use the information of the last arm and reward, the total number of memory
states is in fact Meff = 4M , which corresponds to 2 + log2 M bits.
Definition 2.6 (Memento Memory2). The agent only has access to the information of its past m
actions and rewards. For instance, for m = 4, an observation could be AABB++-+. We use the
notation of the most recent action/reward on the right (here, the last action was B and the reward
was +1). If the agent plays A and obtains a positive reward, the next memory state would be
ABBA+-++. In this memory architecture, the agent writes in its memory only through the plays he
does (|A| = 2). Here, the number of bits is 2m and the total number of memory states is in fact
Meff = 4m.
3	Gain and exploration with a Random Access Memory
Hellman & Cover (1970); Cover & Hellman (1970) described an optimal policy for the RAM ar-
chitecture in the limit of a small reset r. In their optimal policy, the memory states i = 1...M are
organized linearly with transitions only occurring between i and i - 1 (if the last observation sup-
ports HA) or i + 1 (if the last observation supports HB). In the limit r → 0, transition probabilities
can be shown to be independent on i for 1 < i < M . The two extreme memory states i = 1 and
i = M are special as they present a vanishing exit rate → 0. Thus, only these two states are visited
with finite probability in that limit. For such a policy, one obtains an optimal probability to play the
worst arm qHc(M) = αM-1∕(αMT + 1), with α = (1 - μ)∕(1 + μ). 3
Our set-up is slightly different, as we allow for the choice of arm to depend on both the memory
state and the information of the last arm played and reward obtained (Figure 1A-B). In that case, the
policy can be improved, as demonstrated by considering the column of confidence policy.
Definition 3.1 (column of confidence policy (CCP)). It is a RAM policy with M memory states. It
is depicted in Figure 1C. Essentially, the agent uses its last arm played to effectively increase the
size of its memory by a factor 2.
The probability q to play the worst arm by following CCP is derived in Appendix C for general r,
by writing the transition probability matrix T with a generic , for any of the two hypotheses HA or
2Memento is a Christopher Nolan’s film where the hero has a short-term memory loss every few minutes.
Using photos and tattoos, the hero keeps track of information he will eventually forget, thus encoding informa-
tion into his own actions.
3To see why a linear policy is optimal, introduce λi = P (HA |i)/P (HB |i), with P (HA |i) the conditional
probability that HA is true if the memory state i is visited. Choose the labels i such that λ1 ≥ λ2... ≥ λM.
Then it can be shown that λi∕λi+ι ≤ α-1 (Hellman & Cover (1970)). The linear policy saturates this bound
for all i, leading to the maximal ratio that can be obtained between any two states λι∕λM = α1-M. This ratio
controls the gain in the limit → 0 where only these two states are visited with finite probabilities.
4
Under review as a conference paper at ICLR 2022
A Cover & Hellman Cover & Hellman (1970):C column of confidence policy:
8A
7A
6A
5A
4A
3A
2A
1A
8B
7B
6B
5B
4B
3B
2B
1B
y = -1 ⇒ 1 with prob. e
+1 ⇒ ↑
—1 ⇒ 1
& play same arm
y=	+1⇒↑
-1 ⇒ change arm
Figure 1: A Update scheme from Cover & Hellman (1970) vs B our update scheme. Using the
same notation as Cover & Hellman (1970), yn is the reward obtained by playing the arm en and Tn
is the memory state (not to confuse with the transition probability T of Sec 2.1). To make the link
with Sec 2.1 in our setup the state sn correspond to the tuple (en , yn , Tn , kA , kB), the observation
on to the triplet (en , yn , Tn) and the action an is represented by the purple and green arrows. The
red arrow can be seen as part of the conditional probability T (sn+1 |sn, an). C The column of
confidence policy, that can be used in the RAM case, exemplified for M = 8. The memory states
are organized into two columns. The distribution p0 initializes the agent’s memory into states 1A
or 1B with equal chance. The agent keeps playing the same arm, moving up and down a column
depending on the reward, unless it is in the memory state 1 (it then switches arm after a negative
reward). Once the agent reaches a state at the top of a column, it can only step down with small
probability e if a negative reward is obtained. This two-column arrangment effectively double the
number of memory states, by creating 2M = 16 distinct states. In this policy, the value of the
memory can be viewed as a measure of the confidence in the arm being played.
HB. From the stationary distribution Tp~ = p~, one obtains q(e), which reaches its minimum value
for:
e = √2,α-α3M-1 +αM(2μ - 3)+α2M(2μ + 3)	+
√1 — αM (1 — αM — μ — αM μ)
(6)
The result e 〜√ indicates a non-trivial balance between exploration and exploitation, in which the
time spent in each extreme state of the memory grows only as the square root of the horizon time
1/r. A similar result was obtained for hypothesis testing (Wilson (2014)).
For r → 0 (a result generalized for any kB and kA in Appendix C), we obtain:
α2M-1
qCCP(M) = α2M-i + 1 .	⑺
Note that it is the optimal gain of a RAM memory of size 2M . Since our agent memory size is
Meff = 4M (the factor 4 coming from the 2 possible past actions and two possible rewards), the
results of Hellman & Cover (1970); Cover & Hellman (1970) show that CCP is nearly optimal, in the
sense that no policies with one less bit of memory can do better. In Figure 2, we confirm empirically
that it is at least a local policy optimum, as performing policy optimization near this solution leads
to no further improvements.
4	Gain for Memento memory
Are there efficient policies when the agent memorizes the last m arms played and rewards obtained
(Memento memory cf. 2.6)? In the classical two-arm bandit problem, after a time m, the optimal
strategy selects the worst arm with probability q = O (ln m/m) (Auer et al. (2002)). It turns out that
5
Under review as a conference paper at ICLR 2022
10
10
10
10
10
Figure 2: Left Probability to play the worst arm q vs. the reset probability r for different memory
sizes M and two values of μ (the difference between the mean outcome of the two arms). Right q
vs. memory size M for different r and μ indicated in legend. The solid lines show the analytical
result corresponding to column of confidence policy (CCP), whereas symbols show the results of the
learning algorithm (see Sec 2.2) for a RAM memory with a policy initialized close to the predicted
optimal column of confidence policy (π0 ≈ πCCP + 10-4). Learning does not find strategies that
perform better than the optimal column of confidence policy, even for large values of r, indicating
that it is a local optimum. In this figure, error bars would be smaller than the symbols.
AABA
BAAA-
ABAA
——*ABBBx
χBBBB
——BBBAx
Figure 3: Necklace policy with memory of the m = 4 last arms played and rewards obtained (Me-
mento memory cf. 2.6). Memory states are organized into 3 (5 if we consider the two end states)
cycles of arms played, called necklaces in combinatorics. The memory state also contains the re-
wards obtained, but these are not explicitly shown here for the sake of readability. Most of time,
the agent stays in the same necklace by playing the oldest action he remembers. Each necklace
has 2 inputs and 2 outputs states (some states can be both input and output). The agent has a finite
probability to leave its current necklace only when the output state is maximally informative: all 4
rewards are + for A and - for B to move to the left, or the opposite to move to the right. Thicker
arrows represent high probability transition (deterministic in some situations); dashed arrows repre-
sent input/output transitions between necklaces; and thin arrows represent the small probabilities to
leave the two end states.
our agent can use his own actions to encode events over a time much longer than m, leading to q
exponentially small in 2m in a stationary state with long horizon r → 0.
Definition 4.1 (Necklace policy). The necklace policy is based on 4 key ingredients (Figure 3).
(i)	Most of the time, the agent plays the oldest action in its memory (i.e. the arm played m actions
before). Doing so, it memorizes actions cycle inside binary necklaces of length m (in combinatorics,
a necklace is an equivalent class of character strings under cyclic rotation, here the strings are words
of length m made of the letters A and B, hence binary). When m is prime, there are exactly N(m) =
2 + (2m - 2)/m distinct necklaces. For any m, the number of necklaces can be derived from Polya
(1937)‘S enumeration theorem and is equal to N(m)= / Pd∣m 夕(d)2m/d, where 夕 is the Euler's
totient function.
6
Under review as a conference paper at ICLR 2022
(ii)	We provide a Gray order on the necklaces. It means that necklaces are numbered and two
successive necklaces can only differ by one letter. We order the necklaces from i = 1 for the
necklace where all actions are A to i = n(m) for the necklace where all actions are B. The necklace
i = 1 (resp. i = n(m)) also corresponds to the maximum confidence in hypothesis HA (resp. HB).
In general, the longest possible chain of necklace, n(m), is unknown and less than the total number
N (m) of necklaces. But, when m is prime, it has been conjectured (and checked for m ≤ 37)
that there exists a Gray order of all distinct necklaces (Degni & Drisko (2007)): in other words,
n(m) = N(m) = 2 + (2m - 2)/m, for m prime.
(iii)	The probability to exit a necklace is zero, except for two exit configurations for which this
probability is 1 > 0 if two conditions are met. First, the memorized actions must allow the agent
to switch from the necklace i to the necklace i - 1 or i + 1 by taking a new action. Second, the
sequence of rewards must be maximally informative: to switch to the necklace i - 1 (i.e. gaining
confidence in HA), all rewards have to be +1 for the arm A and -1 for the arm B and the opposite
to switch to the necklace i + 1.
(iv)	In the two extreme states, the probability of exit is 0 when all rewards are negative. Below, we
consider the limit lim1→0 lim0→0 q(0, 1) of this strategy. This order of limits ensures that only
the extreme states are visited with a finite probability and that the agent cycles many times in each
necklace before exit.
In order to compute the optimal gain of the necklace policy we introduce the following two lemmas.
Lemma 4.1. Assume a discrete random walk on a chain of sites indexed by i, with probabilities ri
to step from i to i + 1 and li to step from i to i - 1. Starting in site i, the probability to reach site
j + 1 (i ≤ j) before site i - 1 is
Pi→j
1 + Ii + Ii li+1
r r ri+ι
+ .•. + £.
ri
(8)
Proof. The proof is developed in Appendix D.1.
□
Lemma 4.2. Assume a discrete random walk on a chain of n+ 2 sites indexed by i = 0, 1, . . . n+ 1
(with probabilities ri to step to the right and li to the left). If r0 = R and ln+1 = L, in the limit
→ 0, the probability to be on site n + 1 is
p(n+1)=0+R YY ri
i=1 i
(9)
Proof. The proof is developed in Appendix D.2.
□
Using these two lemma the following theorem can be proved.
Theorem 4.3. Consider the two-hypothesis problem described in (4) in the limit of small reset
r → 0. The necklace policy described in Definition 4.1 with parameters (0, 1) has a probability q
to play the worst arm satisfying q ≥ q*, with:
q* =(1 + αm(1-n(m))[T	with α = l-μ.	(10)
V	1	1 + μ
The optimal necklace policy converges to q* when r 0	1	1.
Proof. The detailed proof is contained in Appendix D.3.
□
Note At leading order q* = α2 + o(α2 ). This is because the number of distinct necklaces
N(m) only differs from 2 + (2m - 2)/m at second order, and because we expect to find Gray orders
within those distinct necklaces whose length only differ from N(m) at second order.
7
Under review as a conference paper at ICLR 2022
0.0
0.0
RAM M
Memento m = 3, r
0.5
0.4
0.3
Meff = 64, r
0.2
0.1
E
—RAM random
—RAM linear
一RAM columns
—RAM CCP
—Memento random
—RAM random
一 CCP
・・・ necklace (r = 0)
—Memento random
Memento cycles
—Memento necklace
Meff = 256, r
0.5
0.4
0.3
RAM M = 20, r
Memento m = 4, r = 10
0.2
0.1
F
—RAM random
一RAM linear
—RAM columns
—RAM CCP
—Memento random
一RAM random
一- CCP
・・・ necklace (r = 0)
—Memento random
Memento cycles
—Memento necklace
100	103	105 6
100	103	106
100	103	106
t
t
t
Figure 4: Dynamics of the optimization algorithm for different initialization methods. Probability
to play the worse arm q vs. algorithm time t (time defined in (3)) for different seeds. 20 initialization
seeds are shown in light color and the median is shown in solid color. The wall time is caped to 1 hour
per optimization. A-B RAM with M = 8 and M = 20, we compare the random initialization, the
linear initialization where the jumps in memory states are initialized to be contiguous, the columns
initialization corresponding to linear initialization with the extra constraint that the last action is
repeated except for the memory state 1 and the CCP initialization, very close to the column of
confidence policy (i.e. π0 ≈ πCCP + 10-4, a difference that explains why the red curves can
decrease). C-D q vs. t for the Memento memory m = 3 and m = 4. We compare the random
initialization with the cycles initialization that repeats the oldest action, except if all the remembered
plays correspond to a maximally informative event (during training a path between the cycles has to
be learned) and the necklace where 0 and 1 has to be learned. E-F q vs. t for randomly initialized
policies. The values of m (3 and 4) and M (16 and 64) are chosen such that the total memory
needed to perform these strategies Meff is identical in each panel. The dashed line corresponds to
calculation of q for the CCP and for the necklace policy (for which we only have a prediction for
r = 0). In this figure μ = 0.1.
5 Policy optimization and local minima
To study empirically how learning depends on the memory architecture, we measure how the proba-
bility q(t) to play the worse arm after a training time t depends on the initialization of policy. For the
RAM memory, we find that random initialization (blue curves) leads to very poor results (panels A
and B of Figure 4). Results however improve when a linear structure for memory states is imposed
(orange curves) and when the arms played are segregated on the two sides of that linear structure
8
Under review as a conference paper at ICLR 2022
to form two columns (green). However, even in that case, training does not converge towards the
optimal column of confidence parameters, unless parameters are initialized near the optimal values
(red curves).
By contrast, training with the Memento memory (consisting in the last m actions and rewards, cf.
2.6) appears less sensitive to initialization. As shown in the panel C andD of Figure 4, initializing the
policy randomly (blue) performs does not perform as well as initializing the policy with necklaces
(orange), however the difference is not significant.
Although the RAM architecture is in principle more flexible (and in fact include Memento mem-
ories), we find that, for random initialization, the Memento architecture leads to actually better
policies after training. The comparison is shown in panels E and F of Figure 4, where the two
memory architectutes are compared keeping the effective memory size Meff constant. This find-
ing emphasizes the need to constrain memory architecture, so as to obtain smoother optimization
landscapes.
6 Conclusion
Policies
Memory scheme
Policy
Effective memory
Performance (q-1 - 1)
... as function of Meff
... more generally for (kA > kB)
Memento (cf. 2.6)
necklace (cf. 4.1)
Meff = 4m
am(1-n(m)) 〜α-2
1-kB ∖m ( 1/kB-1 S-2//
1-kA J	l1∕kA-1 J
RAM (cf. 2.5)
CCP (cf. 3.1)
Meff = 4M
α-(2M-1)
〜α-Meff∕2
1-kB ( 1/kB T ∖M
1-kA (1∕kA-1 J
Table 1: Comparison of the performances of the necklace and CCP strategies in the limit r → 0.
As shown in the last line, the gain of these policies can be generalized to any distribution of the
Bernoulli probabilities kA and kB (but needs not be optimal then).
Our results are summarized in Table 1 that compares the necklace policy (cf. 4.1) and the column of
confidence policy (cf. 3.1). For each of these policies, we provide the optimal performance, reached
in the limit r → 0. We conjecture that these policies are the optimal ones for the Memento and RAM
memory schemes respectively. Concerning the Memento memory, this conjecture is supported by
the simulations shown in Figure 3: the best numerical policies found for m = 3 and m = 4 are in
fact the necklace policy.
An interesting additional questions for the future is the generalization of these ideas to a broader set
of tasks. The CCP appears well-suited for multiple hypotheses testing (Chandrasekaran & Laksh-
manan (1978); Yakowitz et al. (1974)), where it would correspond to a “star” policy with a branch
for each hypothesis. Classifying optimal policies for more complex hierarchical tasks, such as those
involved in navigation (Theocharous et al. (2004); Toussaint et al. (2008)), would have practical
applications. Looking ahead, it would be interesting to understand if these ideas have applications
to other approaches dealing with POMDPs, including recurrent networks (Li et al. (2015)) whose
theoretical understanding remains very limited.
Finally, it is intriguing that for all memory structures studied, a linear organization of memory states
appears to be optimal. Despite the fact that our set-up is intrinsically digital, optimal policies ap-
proach an analog memory architecture with a single degree of freedom: it corresponds to the position
along the chain, and measures the relative belief of one hypothesis over the other. In neuroscience,
dominants models of decision making often present a single analogue variable being updated by
observations (Gold & Shadlen (2007); Rescorla & Wagner (1972)). It would be interesting to test
experimentally, in situations where the environment can change with a small probability r between
two distinct classes, if animals stick to two extreme believes, and leave them for exploration with
some rate 〜√r.
9
Under review as a conference paper at ICLR 2022
References
Eitan Altman. Constrained Markov Decision Processes. Chapman and Hall/CRC, 1999.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit
problem. Machine learning, 47(2):235-256, 2002.
Richard Bellman. Dynamic programming. Science, 153(3731):34-37, 1966.
Balakrishnan Chandrasekaran and Kadathur B. Lakshmanan. Finite memory multiple hypothesis
testing: Close-to-optimal schemes for bernoulli problems. IEEE Transactions on Information
Theory, 24(6):755-759, 1978.
Thomas M. Cover and Martin E. Hellman. The two-armed-bandit problem with time-invariant finite
memory. IEEE Transactions on Information Theory, 16(2):185-195, 1970. doi: 10.1109/TIT.
1970.1054427.
Christopher Degni and Arthur A. Drisko. Gray-ordered binary necklaces. the electronic journal of
combinatorics, pp. R7-R7, 2007.
Joshua I. Gold and Michael N. Shadlen. The neural basis of decision making. Annual review of
neuroscience, 30, 2007.
Matthew Hausknecht and Peter Stone. Deep recurrent q-learning for partially observable mdps.
arXiv preprint arXiv:1507.06527, 2015.
Milos Hauskrecht. Value-function approximations for partially observable markov decision pro-
cesses. Journal of artificial intelligence research, 13:33-94, 2000.
Martin E. Hellman and Thomas M. Cover. Learning with Finite Memory. The Annals of Mathemat-
ical Statistics, 41(3):765-782, 1970. ISSN 0003-4851.
Tommi Jaakkola, Satinder P. Singh, and Michael I. Jordan. Reinforcement learning algorithm for
partially observable markov decision problems. Advances in neural information processing sys-
tems, pp. 345-352, 1995.
Arbaaz Khan, Clark Zhang, Nikolay Atanasov, Konstantinos Karydis, Vijay Kumar, and Daniel D.
Lee. Memory augmented control networks. arXiv preprint arXiv:1709.05706, 2017.
Xiujun Li, Lihong Li, Jianfeng Gao, Xiaodong He, Jianshu Chen, Li Deng, and Ji He. Recurrent
reinforcement learning: a hybrid approach. arXiv preprint arXiv:1509.03044, 2015.
Michael L. Littman. An optimization-based categorization of reinforcement learning environments.
From animals to animats, 2:262-270, 1993.
Nicolas Meuleau, Kee-Eung Kim, Leslie Pack Kaelbling, and Anthony R. Cassandra. Solv-
ing pomdps by searching the space of finite policies. In Kathryn B. Laskey and Henri Prade
(eds.), UAI ’99: Proceedings of the Fifteenth Conference on Uncertainty in Artificial In-
telligence, Stockholm, Sweden, July 30 - August 1, 1999, pp. 417-426. Morgan Kaufmann,
1999. URL https://dslpitt.org/uai/displayArticleDetails.jsp?mmnu=
1&smnu=2&article_id=194&proceeding_id=15.
Nicolas Meuleau, Leonid Peshkin, Kee-Eung Kim, and Leslie Pack Kaelbling. Learning finite-state
controllers for partially observable environments. arXiv preprint arXiv:1301.6721, 2013.
Junhyuk Oh, Valliappa Chockalingam, Honglak Lee, et al. Control of memory, active perception,
and action in minecraft. In International Conference on Machine Learning, pp. 2790-2799.
PMLR, 2016.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017.
Leonid Peshkin, Nicolas Meuleau, and Leslie Kaelbling. Learning policies with external memory.
arXiv preprint cs/0103003, 2001.
10
Under review as a conference paper at ICLR 2022
George Polya. Kombinatorische anzahlbestimmungen fur gruppen, graphen Und Chemische
verbindungen. Acta mathematica, 68(1):145-254, 1937.
Robert A. Rescorla and Allan R. Wagner. A theory of Pavlovian conditioning: Variations in the
effectiveness of reinforcement and nonreinforcement, pp. 64-99. Appleton-Century-Crofts, 1972.
Nicholas Roy, Geoffrey Gordon, and Sebastian Thrun. Finding approximate pomdp solutions
through belief compression. Journal of artificial intelligence research, 23:1-40, 2005.
David Silver and Joel Veness. Monte-carlo planning in large pomdps. Neural Information Processing
Systems, 2010.
Richard D. Smallwood and Edward J. Sondik. The optimal control of partially observable markov
processes over a finite horizon. Operations research, 21(5):1071-1088, 1973.
Adhiraj Somani, Nan Ye, David Hsu, and Wee Sun Lee. Despot: Online pomdp planning with
regularization. In NIPS, volume 13, pp. 1772-1780, 2013.
Richard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.
Georgios Theocharous, Kevin Murphy, and Leslie Pack Kaelbling. Representing hierarchical
pomdps as dbns for multi-scale robot localization. In IEEE International Conference on Robotics
and Automation, 2004. Proceedings. ICRA’04. 2004, volume 1, pp. 1045-1051. IEEE, 2004.
Rodrigo Toro Icarte, Richard Valenzano, Toryn Q. Klassen, Phillip Christoffersen, Amir-massoud
Farahmand, and Sheila A. McIlraith. The act of remembering: A study in partially observable
reinforcement learning. arXiv:2010.01753 [cs], 2020.
Marc Toussaint, Laurent Charlin, and Pascal Poupart. Hierarchical pomdp controller optimization
by likelihood maximization. In UAI, volume 24, pp. 562-570, 2008.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.
Andrea Wilson. Bounded Memory and Biases in Information Processing. Econometrica, 82(6):
2257-2294, 2014. ISSN 1468-0262. doi: 10.3982/ECTA12188.
Sidney Yakowitz et al. Multiple hypothesis testing by finite memory algorithms. The Annals of
Statistics, 2(2):323-336, 1974.
Marvin Zhang, Zoe McCarthy, Chelsea Finn, Sergey Levine, and Pieter Abbeel. Learning deep
neural network policies with continuous memory states. In 2016 IEEE International Conference
on Robotics and Automation (ICRA), pp. 520-527, 2016. doi: 10.1109/ICRA.2016.7487174.
11
Under review as a conference paper at ICLR 2022
Appendices
A Proof of Lemma 2.1
Proof. The state transition matrix of the undiscounted POMDP is
ɪ , .. . , ， 、 一 ,
Tn(S |s) = rpo(s ) + (1 - r)T∏(S |s).	(11)
The steady stateP(S) has to be stable through T∏, thus satisfying P(SD = Es T∏(s0∣s)p(s). In the
tensor form, it can be written p~ = rp~0 + (1 - r)Tπp~. Applying recursively this formula n times we
obtain:
~P = r nX(1 - r)tTπt ~P0 + (1 - r)nTπn~P.	(12)
t=0
Translating it into an expectation value expression we obtain:
ES〜P[f (S)] = rEs
t 〜Tn Po
1(1 - r)tf (St)
+ (I- r)nEs^Tnp[f (s)]	(13)
for any function f and where TπP is understood as a matrix-vector product (note that TπP 6= P). By
replacing f by R and by taking the limit n → ∞, for r = 1 - γ, we can identify (13) with (2), thus
obtaining the Lemma 2.1.	□
B	Implementation details
To compute the steady state we use the power method algorithm: Alg.1.
When we have multiple independent environments (by environment we mean subset of S that the
agent cannot escape with its actions), S is the disjoint union of these environments: S = S1 + S2 +
... IfP0 factorize as follow P0(S) = P(Si)P(S|Si) for S ∈ Si, we can compute the steady state by
computing those of each independent environments.
The initial state of the memory of the agent can be optimized by allowing gradient flow to modify
specific part ofP0. It could mathematically be reformulated as special actions done on special initial
states to initialize the memory.
Data: A transition matrix M
Result: The steady state
while the columns of M differ (with a given tolerance) do
I MM → M;
end
return a column of M;
Algorithm 1: Power method
C Exact computation for column of confidence policy
The mathematica notebook is provided along with the code, see the link in Section 1.
Here We compute the optimal value of E (that maximize the gain) given r and μ.
First observe that the states (3B+, 1A-, ...) can be arranged along a line:
MA+ . . .	2A+	1A+	1B+	2B+	. . .	MB+
MA- . . .	2A-	1A-	1B-	2B-	. . .	MB-
Where the probability transition only occurs betWeen tWo consecutive states.
(14)
12
Under review as a conference paper at ICLR 2022
If we merge the ± we have 2M states and we can compute their transition matrix without reset:
	1 —EkB kA	
	EkB	0	kA	
	kB 0	..	
Q=	.	(15)
	kB	.. kA	
	. . .	0	EkA	
	kB 1 — EkA	
where kA = (1+ μ)∕2 and kB	=(I ― μ)/2	
Including the reset, the transition probability becomes
Mj = (I — r)Qj+ rJi	(16)
where J is 1/2 in the two central states 1A and 1B.
The steady state Pi is the solution of
EMj Pj = Pi	^⇒	£((1 — r)Qij — δij )pj = — Ji	(17)
j	j
We can split this problem in 5 regions: the A border, the A bulk, the center, the B bulk, the B border.
In the bulks the (17) is
(1 — r)kB Pi_i — Pi + (1 — r)⅛APi+1 = 0	(18)
The solutions are of the form Pi = qw* + c2w_ with
W土
1 ± √1 — (1 — r)2(1 — μ2)
(1 — r)(1 + μ)
(19)
To fix the coefficients c1, c+ in the two bulks we have 6 equations:
•	two on the left border (two first lines of (17))
•	two in the center, at the injection (two middle lines of (17))
•	two on the right border (two last lines of (17))
Solving these equations, we can compute the probability to play the wrong arm (B, assuming μ > 0).
w+(1 + W+ )(1 — w_)w2M (1 + w+(e — 1))(w_ + E — 1)
+ (w+ 什 W_)
+(w_w+)m (w_w* — 1)((w_ — 1)(w+ — 1)(w_ + w+)(e — 1) + 2w_w+€2)
9	2(w— — w*)(w+mw_(1 + w_(E — 1))(w+ + E — 1) — (w+ 什 w_))	(2°)
in this expression of q we expressed r, μ and kA, ⅛b in function of w±
Optimizing q with respect to E leads to
(w— — I)WM (w+ — 1)wM (w w* — 1)(WM w+ — w wM )2
E =(W_W+ 厂 M-—
(w_ — 1)(w+ — 1)(w-W+)1+2M (w-w+ — 1)2(WM — WM)
/	w3m w22 (1 + w_)(1 + w+)
—(w+ 什 W_)
+w22M W1+M (2w+ + w_ + w+w-(7 + 2w+) + w_ (w+ — 1))
∖	—(w+ 什 w_)
(w-w+M(1 + w_(w+ — 1) + w+) ʌ
(w_w+ — 1) I	+(w+ 什 w_)	I
∖	—2w_+M w++m(1+ w_w+))
(21)
13
Under review as a conference paper at ICLR 2022
We can make the Taylor expansion of with respect to r
e = √2,α-α3M-1 +αM(2μ - 3)+α2M(2μ + 3)	+
√1 — αM (1 — αM — μ — αM μ)
withα = 1+μ
For M large, it converge quickly toward
e = r-μ+Om
(22)
(23)
In the limit r → 0 we get
α2M-1
q = α2M-1 + 1 + O(√r)
(24)
D Random walk along a chain
D. 1 Probability to traverse the chain
1 — (li + ri )
Q
i—1	i	i+1
Figure 5: Markov chain
Consider a random walk on a chain of sites with probabilities ri to move from site i to i + 1 and li
to move from i to i — 1 (Figure 5). First, we want to compute the probability Pi→j (with i ≤ j) that,
starting at site i, the walker visits the site j + 1 before the site i — 1. Note that Pi→j only depends
on {lk }1=2 and {rk }k=j Note also that according to this definition Pi→ = iɪ-. We also define
Pu as the probability to reach i 一 1 before j + 1 by starting in j, then we have PiJi = r⅛-.
li +ri
Starting in i, after one time step the walker is either (i) in i —1 (with probability li) and the probability
to reach j + 1 before reaching i 一 1 becomes null, (ii) still in i (with probability 1 一 (li +ri)) and the
probability to each j + 1 before i — 1 is still given by Pi→j, (iii) in i + 1 (with probability ri). Once
in i + 1 there are two possibilities: either the walker never comes back to i and it reaches j + 1 (with
probability Pi+1→j), or it does (with probability 1 — Pi+1→j) and it again has the same probability
Pi→j to reach j + 1 before i — 1. Thus we have:
Pi→j = (1 — (li + ri))Pi→j + ri (Pi+1→j + (1 — Pi+1→j)Pi→j) .	(25)
The quantities that matter are pi ≡ ri/(li + ri). Ifwe isolate Pi→j to the l.h.s. we get
Pi→j
PiPi+1→j
1 一 Pi(I — Pi+1→j )
(26)
Making the changes of variable Xi→j ≡ 1PP:→ and Xi ≡ 1-pi (note that Xi = li/ri) We obtain
Xi→j = xi(1 +Xi+1→j).
By repeating the formula we see that we get
jk
Xi→j =	Xl = Xi + XiXi+1 + XiXi+iXi+2 +-+ Xi …Xj
(27)
(28)
from which we can get Pi→j by the inverse transformation
Pi→j
li + li li+1 +	+ li
ri	ri ri+1	r
(29)
14
Under review as a conference paper at ICLR 2022
D.2 Chain with final states
Let us consider the same chain as above with a finite length n such that sites are labelled from i = 1
to n. Now let us add a final state at each end of the chain (sites i = 0 and n + 1). We consider the
probabilities to leave the final states asymptotically small, of order . More precisely, the probability
to move from i = 0 to 1 is R (we call it R as it is a probability to go to the Right, although it
concerns the most left site) and the probability to move from n+ 1 to n is L. These probability and
the probabilities p(0) and p(n + 1) to be on the site 0 and n + 1 are related by a balance of the flow
from 0 to n + 1:
p(0)eRPι→n = p(n + 1)cLPn
(30)
In the limit → 0, the probabilities to be in the extreme states tends to 1 and we thus have p(0)
1 - p(n + 1), yielding
p(n + 1) = 1 +
L Pl-n∖
R P1→n )
(31)
This result can be simplified using the equality
Pl-n
P→n
1 +	l1	+	…+	l1	ln
+	ri	+	+	ri	rn
1 +产+…+产•.・卢
n	n1
(32)
to finally obtain the formula
p(n + 1)
(33)
D.3 Chain of necklaces
To compute the gain of the necklace policy, the idea is to show that necklaces can be arranged on a
chain so that we can use (33) (see Figure 3).
For m prime, the number of non trivial necklaces is (2m - 2)/m, the trivial necklaces being the two
final states (i.e., the words AAA..A and BBB..B). Using the conjecture of Degni & Drisko (2007),
there is a Gray order on these necklaces when m is prime. In other words, there is a chain from
one final state to the other that passes exactly once by each necklace, the difference between two
successive necklaces being exactly one bit (i.e. a single A is changed into B or vice versa).
In any case (m prime or not), we call n(m) the length of the longest chain with Gray order. We call
y(m) the length of the smallest necklace in that longest chain (arguably the smallest prime factor of
m).
A necklace is characterized by the numbers a and b of letters A and B in the m-long word, with
a + b = m. After at least one complete loop in the necklace (i.e. at least y(m) actions), the
probabilities to leave that necklace (when we are at the exit states) are
li = 1 kAa (1 - kB )b ,
ri = 1 (1 - kA)akBb •
(34)
(35)
With the odds to do at least one loop increasing as 1 goes to zero or y(m) goes to ∞.
In the two final states, and again after one loop, the probabilities to leave are 0L = 0 (1 - kB)m
and 0R = 0(1 - kA)m. We can now use the formula (33) (when 0	1	1), in which the
15
Under review as a conference paper at ICLR 2022
product simplifies as
IL = n(Y)-2 kA (1- kB 产
ri	i=1 (1 - kA)aikB
=Qn=m)-2(i/kB -1产
Qn=T)-2(i∕kA -1)。，
=(1/kB - 1)Pi bi
=(1/kA - 1)Pi ai
m(n(m)-2)/2
(36)
(37)
(38)
n(m)-2
since	(ai + bi) = m(n(m) - 2)	(39)
i=1
where ai and bi are the occurrences of A and B in the necklace i.
Inserting (39) into (33) and using the value of kA and kB given in (4) for hypothesis HA leads to
p(n +1)= fl + αm(If(T))J-、	With α = 1-μ.	(40)
∖	1	1 + μ
and p(0) can be obtained by changing μ into -μ or a into 1∕α,by symmetry of the necklace policy.
The probabilities under hypothesis HB are obtained by exchanging p(0) and p(n + 1), again by
symmetry.
Under hypothesis HA (resp. HB), the value p(n + 1) (resp. p(0)) corresponds to the probability q*
to play the Worst arm if the probabilities of the non-final necklaces are zero (Which is asymptotically
true if 0	1). To reach q*, We also need 1 to be asymptotically small in order to guarantee at
least one loop in each necklace and 0 has to be asymptotically larger than the reset r. In summary,
We need r 0	1	1 to reach asymptotically q*, otherWise the probability q Will be larger
than q* .
D.4 column of confidence policy with no reset
When there is no reset r = 0 We can compute the performance of the column of confidence policy
for tWo arms of probabilities kA and kB. We obtain via (33) (assuming kA > kB)
-1	1 = 1 - kB ( 1/kB - 1 A M 1
1	1 - kA l1∕kA - 1 )
(41)
16