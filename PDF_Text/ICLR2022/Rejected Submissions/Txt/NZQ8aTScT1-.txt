Under review as a conference paper at ICLR 2022
Eigenspace Restructuring: a Principle of
Space and Frequency in Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
Understanding the fundamental principles behind the massive success of neural
networks is one of the most important open questions in deep learning. However,
due to the highly complex nature of the problem, progress has been relatively slow.
In this note, through the lens of infinite-width networks, a.k.a. neural kernels, we
present one such principle resulting from hierarchical locality. It is well-known
that the eigenstructure of infinite-width multilayer perceptrons (MLPs) depends
solely on the concept frequency, which measures the order of interactions. We
show that the topologies from convolutional networks (CNNs) restructure the as-
sociated eigenspaces into finer subspaces. In addition to frequency, the new struc-
ture also depends on the concept space — the distance among interaction terms,
defined via the length of a minimum spanning tree containing them. The result-
ing fine-grained eigenstructure dramatically improves the network’s learnability,
empowering them to simultaneously model a much richer class of interactions,
including long-range-low-frequency interactions, short-range-high-frequency in-
teractions, and various interpolations and extrapolations in-between. Finally, we
show that increasing the depth of a CNN can improve the inter/extrapolation res-
olution and, therefore, the network’s learnability.
1	Introduction
Learning in high dimensions is commonly believed to suffer from the curse of dimensionality, in
which the number of samples required to solve the problem grows rapidly (often polynomially) with
the dimensionality of the input. Nevertheless, modern neural networks often exhibit an astonishing
power to tackle a wide range of highly complex and high-dimensional real-world problems, many of
which were thought to be out-of-scope of known methods (Krizhevsky et al., 2012; Vaswani et al.,
2017; Devlin et al., 2018; Silver et al., 2016; Senior et al., 2020; Kaplan et al., 2020). What are
the mathematical principles that govern the astonishing power of neural networks? This question
perhaps is the most crucial research question in the theory of deep learning because such principles
are also the keys to resolve fundamental questions in the practice of machine learning such as (out-
of-distribution) generalization (Zhang et al., 2021), calibration (Ovadia et al., 2019), interpretability
(Montavon et al., 2018), robustness (Goodfellow et al., 2014).
Unarguably, there can be more than one of such principles. They are related to one or more of the
three basic ingredients of machine learning methods: the data, the model and the inference algo-
rithm. Among them, the models, a.k.a. architectures of neural networks are the most crucial inno-
vation in deep learning that set it apart from classical machine learning methods. More importantly,
the current revolution in machine learning is initialized by the (re-)introduction of convolution-based
architectures (Krizhevsky et al., 2012; Lecun, 1989), and subsequent breakthroughs are often driven
by the discovery or application of novel architectures (Vaswani et al. (2017); Devlin et al. (2018)).
As such, identifying and understanding fundamental roles of architectures are of great importance.
In this paper, we take a step forwards by leveraging recent developments in overparameterized
networks (Poole et al. (2016); Daniely et al. (2016); Schoenholz et al. (2017); Lee et al. (2018);
Matthews et al. (2018); Xiao et al. (2018); Jacot et al. (2018); Du et al. (2018); Novak et al. (2019a);
Lee et al. (2019) and many others.) These developments have discovered an important connection
between neural networks and kernel machines: the Neural Network Gaussian Process (NNGP) ker-
nels and the neural tangent kernels (NTKs). Under certain scaling limits, the former describes the
1
Under review as a conference paper at ICLR 2022
Figure 1: Architectural Inductive Biases. An demonstration of learnable functions vs archi-
tectures for four families of architectures. Each shaded box indicates the maximum learnable
eigenspaces within a given compute budget (Dashed Line.) From left to right: (a) MLPs can
model Long-Range-Low-Frequency interactions; (b) S-CNNs can model Short-Range-High-
Frequency interactions; (c) Additionally, D-CNNs can also model interactions between LRLF
and SRHF, a.k.a., Median-Range-Median-Frequency interactions. (d) Finally, HS-CNNs can ad-
ditionally model interactions of Ultra-Short-Range-Ultra-High-Frequency, Ultra-Long-Range-
Ultra-Low-Frequency, and finer interpolations in-between. The Green Arrow indicates the di-
rection of expansion of learnable functions when increasing the compute budget.
distribution of the outputs of a randomly initialized network (a.k.a. prior), and the latter can describe
the network’s gradient descent dynamics. Although recent work (Ghorbani et al., 2019; Yang & Hu,
2020) has identified several limitations of using them in studying the feature learning dynamics of
practical networks, we show that they do capture several crucial aspects of the architectural inductive
biases.
Our main contribution is an eigenspace restructuring theorem. It characterizes a mathematical con-
nection between a network’s architecture and its learnability through a trade-off between space and
frequency, providing novel insights behind the mystery power of deep CNNs (more generally, hi-
erarchical locality (Deza et al., 2020; Vasilescu et al., 2021).) By frequency, we mean the degree
(order) of the eigenfunction and by space, we mean the spatial distance among the eigenfunction’s
interaction terms. We summarize our main contribution below; see Fig. 1.
1.	The learning order (see Green Arrow in Fig. 1) of eigen-functions is governed by the
learning index (LI), the sum of the frequency index (FI) and the spatial index (SI), which
can be characterized precisely by the network’s topology.
2.	There is a trade-off between space and frequency: within a fixed (compute/data) budget, itis
impossible to model generic Long-Range-High-Frequency interactions. MLPs can model
Long-Range-Low-Frequency (LRHF) interactions but fail to model Short-Range-High-
Frequency (SRHF), while shallow CNNs (S-CNNs) are the opposite. Remarkably, deep
CNNs (D-CNNs) can simultaneously model both and various interpolating interactions
between them (e.g., Median-Range-Median-Frequency (MRMF).)
3.	In addition, high-resolution CNNs (HS-CNNs, EfficientNet-type of model scaling) further
broaden the class of learnable functions to contain (1) extrapolation: Ultra-Long-Range-
Ultra-Low-Frequency and the dual interactions and (2) finer interpolations interactions.
4.	Finally, we verify the above claims empirically for neural kernel methods and finite-width
networks using SGD + Momentum for dataset and networks of practical sizes.
2	Linear and linearized models
As a warm up exercise, we briefly go through the training dynamics of linear models. Let (X , Y)
denote the inputs and labels, where X ⊆ Rd and Y ⊆ R. Assume J : Rd → Rn is a feature
map and the task is to learn a linear function f(x, θ) = J (x)θ to minimize the MSE objective
1 P(X y)∈(χ γ)|f (χ,θ) - y|2. Let WX ,θ) = f (X ,θ) -Y be the residual of the predictions of X.
Then the gradient flow dynamics can be written as
-d沈(X,θ) = -J(x)jT(x)wx,θ) ≡ -K(X, x)wx,θ)	(1)
2
Under review as a conference paper at ICLR 2022
Since the feature kernel K(X, X) = J(X)JT (X) is constant in time, the above ODE can be solved
in closed form. Let m = |X | the cardinality of X and K(j)/uj be the j-th eigenvalue/eigenvector
of K(X, X) in descending order. By initializing θ = 0 at time t = 0 and denoting the projection by
η = UT沈(X, 0), the dynamics of the residual and the loss can be reduced to
阳X,θt) = X e-K⑶tηjUj, L(θt) = 1 X e-2K(j)tηj	⑵
j∈[m]	j∈[m]
Therefore, to make the residual in Uj smaller than some > 0, the amount of time needed is
t ≥ K(j)-1iog η⅛/2.
The larger K(j) is, the shorter amount of time it takes to learn Uj.
Although simple, linear models provide us with the most useful intuition behind the relation between
“eigenstructures" and learning dynamics.
2.1	Linearized Neural Networks: NNGP Kernels and NT Kernels
Let f(θ, x) be a general function, e.g. f is neural network parameterized by θ. Similarly,
-d 阳 X ,θ) = -J (X; θ)J T (X ； θ)^(X ,θ) ≡ -K(X, X ； θ)^(X ,θ).	(3)
However, the kernel K(X, X; θ) depends on θ via the Jacobian J(X; θ) off(X; θ) and evolves with
time. The above system is unsolvable in general. However, under certain parameterization methods
(e.g. Sohl-Dickstein et al. (2020)) and when the network is sufficient wide, this kernel does not
change much during training and converges to a deterministic kernel called the NTK (Jacot et al.,
2018),
K(X, X; θ) → Θ(X, X) as width → ∞.	(4)
The residual dynamics becomes a constant coefficient ODE again 沈(X, θ) = -Θ(X, X)^(X, θ).
To solve this system, We need the initial value of 沈(X, θ). Since the parameters θ are often initial-
ized using iid standard Gaussian variables, as the width approach infinity, the logits f(X; θ) converge
to a Gaussian process (GP), known as the neural network Gaussian process (NNGP). Specifically,
f (X; θ) ~ N(0; %(X, X)), where 忒 is the NNGP kernel. Note that one can also treat infinite-width
networks as Bayesian models, a.k.a. Bayesian Neural Networks, and apply Bayesian inference to
compute the posteriors. This approach is equivalent to training only the network’s classification
layer (Lee et al., 2019) and the gradient descent dynamics is described by the kernel 孔
As such, there are two natural kernels, the NTK Θ and the NNGP kernel 丸 associated to infinite-
width networks, whose training dynamics are governed by constant coefficient ODEs. To make
progress, it is tempting to apply Mercer,s Theorem to eigendecompose Θ and 范 e.g.,
%(x, x) = ^X Ξ⅛(j)φj (x)φj (x) and Θ(x,x) = ^X Θ(j )ψj (x)ψj (x)	(5)
One advantage of applying this decomposition is that it has almost no constraint on the kernels
and the inputs. However, this decomposition is too coarse to be useful since it can hardly provide
fine-grained information about the eigenstructures. E.g, it is not clear what are the corrections to
Eq. (5) when changing the architecture from a 2-layer CNN to a 4-layer CNNs. For this reason,
we choose to work on the product space of hyperspheres, which has richer mathematical structures.
Our primary goal is to characterize the analytical dependence of the decomposition Eq. (5) on the
network’s topology in the high-dimensional limit.
3 Neural computations on DAGs
Notations will become heavier starting from this section. In particular, we rely crucially on the
directed acyclic graphs (DAGs) and minimal spanning trees (MSTs) to define the spatial complexity
of eigenfunctions. In Sec. B, we provide a toy example to help understand the motivation.
For a positive integer p, let Sp-ι denote the unit sphere in Rp and Sp-ι = √pSp-ι, the sphere of
radius √p in Rp. We introduce the normalized sum (integral)
f(x) ≡ |X|-1 f(x)
x∈X	x∈X
f	f (x) ≡ μ(X) — 1 [	f (x)μ(dx)
x∈X	x∈X
(6)
3
Under review as a conference paper at ICLR 2022
where X is a finite set (a measurable set with a finite positive measure μ).
We find it more convenient to express the computations in neural networks, and in neural kernels
via DAGs (Daniely et al., 2016), as both computations are of recursive nature. The associated DAG
of a network can be thought of as the same network by setting all its widths (or the number of
channels for CNNs) to 1. Let G = (N, E) denote a DAG, where N and E are the nodes and edges,
resp. We always assume the graph to have a unique output node oG and is an ancestor of all other
nodes. Denote N0 ⊆ N the set of input nodes (leaves) of G, i.e., the collection nodes without a
child. Each node u ∈ N is associated with a pointwise function φu : R → R, which is normalized
in the sense Ez∈n(o,i)ΦU(z) = 1. It induces a function φU : I ≡ [-1,1] → I defined to be
ΦU(t) = E(z1,z2)∈Ntφu(zι)φu(z2). Here Nt denotes a pair of standard Gaussians with correlation
t. We associate each u ∈ N a finite-dimensional Hilbert space Hu , and each uv ∈ E a bounded
linear operator Luv : Hv → Hu . Let
X ≡ Y Xu ≡ Y Sdim(Hu)-1 ⊆ Y Hu and I = I1N0 1
be the input tensors and the input correlations to the graph G, resp. We associate two types of
computations to a DAG: finite-width neural network computation and kernel computation,
NG : X → HoG	and 次G : I → I,	(7)
resp. They are defined recursively as follows
Nu(x) = φu	X Luv(Nv (x))	if u ∈/ N0 else Nu(x) = xu (8)
∖v;uv∈E	)
比u(t) = Φu(∕	%v(t))	if U / No else 比u(t) = tu	(9)
∖J v∙.uv∈E	)
where X / X and t / I. The outputs of the computations are NG(x) = NoG(x) and 状G(t)=
%oG (t). Note that 状G is indeed the NNGP kernel. The NTK can also be written recursively as
Θu(t) = φ u(∕	g%v(t))/	J%v(t)+Θv(t)) with Θg = ΘoG.	(10)
Here, Θu = 0 if U / No and φu is the derivative of φu.
3.1 THREE EXAMPLES: MLPS, S-CNNS AND D-CNNS.
To unpack the notation, we consider three concrete examples: an L-hidden layer MLP, a shallow
convolutional network (S-CNN) that contains only one convolutional layer and a deep convolutional
network (D-CNN) that contains (1 + L) convolutional layers. The architectures are
MLP:	[Input]	→	[Dense-Act]0L → [Dense]	(11)
S-CNN:	[Input]	→	[Conv(p)-Act] → [Flatten-Dense]	(12)
D-CNN:	[Input]	→	[Conv(p)-Act] → [Conv(k)-Act]0L	→	[Flatten-Dense-Act]	→	[Dense]	(13)
where p/k is the filter size of the first/hidden layers and Act means an activation layer. We choose
the stride to be the same as the size of the filter for all convolutional layers and choose flattening as
the readout strategy rather than pooling. See Fig. 2 (a, b, c) for the DAGs associated to a (1+3)-layer
CNN (with P = k = d4), a (1+1)-layer CNN (with P = k = d1) and a 4-layer MLP.
MLPs. Let G be a linked list with (L + 2) nodes, including the input/output nodes. Let Luv /
Rnu×nv, where nu/v = dim(Hu/v) and the activations of the input/output nodes be the identity
function. Then NG represents a L-hidden-layer MLP. In addition, let Luv be initialized iid as
Luv = √n^ (ωuv,ij )i∈[nu],j∈[nv ]， ωuv,j ~ N(0, 1) ∙	(14)
Let tx,x0 = xT x0/nu for U / No and nv → ∞ for all hidden nodes, then the outputs of N(X)
converge weakly to the GP GP(0,状G(tχ,χo)χ,χo∈χ) and Θg(tχ,χo )χ,χo∈χ is the NTK in the sense
ENG(X)NG(x0) ---→ 次G(tχ,χθ) and gNg(x), VNg(x0)i j-→ Θg(tχ,χθ).	(15)
4
Under review as a conference paper at ICLR 2022
JC(YJ=O+2=2
Oitput Node
Input Node
£(Va)=J+i=5.deg(Y2)=2
(a) MLP	(b)CNN(p2)52
Ultra-S⅛ιrt-Range-Low^Frequerκy: zχva)=∣+∣=J
Spatial: αk≡0	Spatial:%-^
MSE Residual vs TYakikig Set SIzezY2
IO1 1(f	105	104 Ii
(c) CNN(P产	(d) MSE Residual
Figure 2: Architectures/DAGs. vs Eigenfunctions vs Learning Indices. Left to right: DAGs as-
sociated to (a) a four-layer MLP; (b) CNN(p2)02, a"D”-CNN that has two ConVolUtionallayer(C)
CNN(p)04, a “HR”-CNN that has four convolutional layers; and (d) MSE (Y-axis) vs training set
size (X-axis) for Y2 obtained by NTK-regression for 4 architectures. Here Y2 is a linear combina-
tion of eigenfunctions of Short-Range-Low-Frequency interactions (deg(Y2) = 2); see Sec. D for
the expression. The DAGs are generated with p = 4. In each DAG, the Dashed Lines represent the
edges with zero weights. The Solid Lines have weights 0, 2 and 4 in (a), (b) and (c), resp. The Col-
ored path represents the minimum spanning tree used to compute the spatial indices ofY2. Under
architectures (a), (b) and (c), the spatial indices are 0, 1 and 4, resp. Each input node represents an
input patch of dimension p4 = d, p2 = d 1 and P = d4 and the frequency indices are 2, 2 X 2 and
2 × 4 in (a), (b) and (c), resp.
Indeed, note that deg(u) = 1 for all u ∈/ N0. Eq. (9) and Eq. (10) become
%u(tχ,χθ) = Φu(%v(tχ,χo)) and θu(tχ,χθ) = Φu(%v(tχ,χθ))(%v(tχ,χθ) +θv(tχ,χθ)) (16)
which are the recursive formulas for the NNGP kernel and NTK; see e.g. Sec.E in Lee et al. (2019).
S-CNN. The input X = (Sp-I)1×w ⊆ Rd, where P is the patch size, W is the number of patches,
d = pw is the dimension of the inputs. Here, the inputs have been pre-processed by a patch extractor
and then by a normalization operator. In words, the S-CNN has one convolutional layer with filter
size p, followed by an activation function φ (e.g., Relu), and finally by a flatten-dense readout layer.
Mathematically, by letting n ∈ N be the number of channels in the hidden layer, the output (i.e.,
logit) is given by
Convolution + Activation:	Zij(x) = φ	p- 1	E	ωι,j,βxβ,i	for i ∈	[w],j	∈	[n]	(17)
β∈[p]
Flatten + Dense:	f (x) = (wn)-2	ɪ2	ω2,jzj(x),	(18)
i∈[w],j∈[n]
where ω1,i,β and ω2,ij are the parameters of the first and readout layers, resp.
We can associate a DAG G = (N, E) to the above S-CNN. Let the input, hidden and output nodes
be NO = [1] × [w], NI = [w] and N2 = {og} = {0}, resp. and N = NO ∪ N1 ∪N2. Moreover,
uv ∈ E if u = oG and v ∈ N1 or u = (i, ) ∈ N1 and v = (0, i) ∈ N0. Let Hv = Rp for v ∈ N0,
Hu = Rn ifu ∈ N1 and HoG = R. The associated linear operators are given by
Luv = P-2 (ωι,j,β)j∈[n],β∈[p]	∈ Rn×p	for (u,v) ∈N	×N0	(19)
LoG v = (Wn)- 1 (ω2,ij )j∈[n] ∈	Rn if	V =	(i, ) ∈ N1	(20)
Note that the weights are shared in the first layer but not in the readout layer (i.e., the network has no
pooling layer). To compute the NNGP kernel and NTK, we initalize all parameters ω1,i,β and ω2,ij
with iid Gaussian N(0, 1). Letting n → ∞ and denoting tv = xvTx0v/p and t = (tv)v∈N0, we have
晚(t) = / N Φ*%)and Θg(t) = / N。*(九)+ Φ*(tv)	(21)
D-CNN. The input space is X = (Sp-ι)kL×w ⊆ Rp×1×kL×w, where P is the patch size of the
input convolutional layer, k is the filter size in hidden layers, L is the number of hidden convolution
5
Under review as a conference paper at ICLR 2022
layers and w is the spatial dimension of the penultimate layer. The total dimension of the input is
d = P ∙ kL ∙ w, and the number of input nodes is |N01 = kL ∙ w. Since the stride is equal to the filter
size for all convolutional layers, the spatial dimension is reduced by a factor of p in the first layer, a
factor ofk by each hidden layer, and is reduced to 1 by the Flatten-Dense layer. Similar to S-CNNs,
one can associate a DAG to a D-CNN. Briefly, the input layer has kL × w nodes and is reduced by
a factor of k by each convolutional layer. The penultimate and output layers have w and 1 nodes,
resp.
4 Main Results
The goal is to obtain a precise Charaterization of the relation between the eigenstrUctUreS of % / Θ
and the DAG associated to the network’s architectures in the large input dimension setting. As such
we consider a sequence of graphs G = G(d) d∈N, where G(d) = (N(d), E(d)). We associate a finite
set of non-negative numbers ΛG to G, which is called the shape parameters of G,
0 ∈ Λg ⊆ [0,1] and ∣Λg| < ∞.	(22)
We need several technical assumptions on G regarding the asymptotic shapes of G(d), which are
summarized as Assumption-G in Sec.G of the appendix. We list two of them which are the most
crucial ones. (1) For each non-input node U ∈ N(d), there is a〃 ∈ Λg with deg(u)〜dɑu.The
weight associated to the edge uv ∈ E(d) is defined to be πuv ≡ αu. (2) For each input node v, there
is 0 < av ∈ Λg so that the input dimension dv 〜 dαv. Here a 〜b means a/b ∈ [1/C, C] for
some C > 0 independent of d. The main purpose of making these two assumptions is to remove
non-leading terms when computing the spectra.
We say φ* is semi-admissible if, for all r ≥ 1, the r-th derivative of φ* at zero is non-vanishing, i.e.,
φMr)(O) > 0. If, in addition, φ* (0) = 0 (i.e., the activation is centered), then We say φ is admissible.
An activation φ is (semi-)admissible if φ* is (semi-)admissible. Note that if φ* is (semi-)admissible,
then φ* is semi-admissible.
Assumption-φ. We make the following assumptions on the activations. (a.) If u ∈ N0(d), φu is the
identity function. (b.) Ifu ∈/ N0(d) ∪ {oG}, φu is admissible. (c.) Ifu = oG, φu semi-admissible.
Next, we introduce the key concept which defines the spatial distance among nodes. It is the length
of the minimum spanning tree (MST) of the nodes.
Definition 1 (Spatial Index of Nodes). Let 九 ⊆ N(d). The spatial index of 九 is defined to be
S(u) = min
n⊆T≤G(d)
πuv
uv∈E(T)
min
n⊆T≤G(d)
deg(u; T)αu
uv∈E(T)
(23)
where 九 ⊆ T ≤ G(d) means T is a sub-graph containing 九 and deg(u; T) is the degree of u in T.
By default, S(u) = 0 if 九 contains only one or zero node.
Let tr : I|N0d)| → I be a monomial, where r : N0(d) → N|N0d)|. We use u(r) = {v ∈ N0(d) : r4 =
0} to denote the support of r and 九(r; og)= 九(r) ∪ {og}.
Definition 2 (Spatial, Frequency and Learning Indices of tr). We say r ∈ N|N0(d) | is G(d)-learnable,
or learnablefor short, ifthere is a common ancestor node U of 九(r) such that φu is Semi-admissible.
We use M(G(d)) ≡ {r ∈ NN0 : r is learnable}. For r ∈ 俎(G(d)), the spatial index, frequency
index and the learning index are defined to be,
S(r) := S(u(r;	OG)),	见r)	:=	^X	底αv	and	S(r)	:= S(r)	+ 巩r),	(24)
v∈N0(d)
resp. If r ∈ M(G(d)), we set S(r)=巩r)=贝r) = +∞. Let X(G(d)) denote the sequence of
learning indices in non-descending order, i.e.
X(G(d)) ≡(X(r): r ∈ M(G(d))) ≡ (∙∙∙≤ rj≤ rj+ι ≤ ...)	(25)
6
Under review as a conference paper at ICLR 2022
Finally, for each U ∈ N0(d), let {Y r,ι}ι∈[N (du,r)],r∈N be the family of normalized spherical harmon-
ics in Sdu-ι, where N(du,r) is the number of degree r spherical harmonics in Sdu-ι. Define
Yr,ι(ξ)=	∏	Yru,iu (ξu),	l =	(lu)u∈N(d)	∈ [N(d, r)]	≡	∏	[N(du,	ru)]	(26)
u∈N0(d)
u∈N0(d)
for ξ = (ξu) ∈ X . The following is our main theorem. It describes a connection between the
architecture ofa network and the eigenstructure of its inducing kernels.
Theorem 1	(Eigenspace Restructuring). Assume Assumption-G and Assumption-φ. We have
thefollowing eigen-deCompositionfor K = %g(d)or Θg(d). For ξ, η ∈ X
K(ξ, η) = E λκ(r)	E Yr,ι(ξ)Yr,ι(η), where λκ(r)〜d-*(r) if r = 0 . (27)
r∈N∣N0d)∣	l∈N (d，r)
Coupling with the observation in Eq. (2), Theorem 1 implies that within t 〜 dr amount of time
for gradient flow, when d is sufficiently large, only the eigenfunctions Yr,ι with L(r) ≤ r can be
learned. By leveraging an analytical result from Mei et al. (2021a) (Sec. 3 Theorem 4), which says
under certain regularity conditions on the eigenstructure, the corresponding kernel regression acts as
a projection, Theorem 1 establishes a connection between architectures and generalization bounds
of NNGP kernel/NTK.
Let σ be the uniform (product) probability measure on X and denote Lp(X) ≡ Lp(X, σ). For
X ⊆ X and r ∈/ L(G(d)), define the regressor and the projection operator to be
RX (f )(x)= K(x,X )K(X,X )-1f(X ) and P>r(f )= E E hf, Y rlu(X )Y r,l .
r:L(r)>r l∈N (d,r)
Theorem 2.	Let G = {G(d)}d, where each G(d) is a DAG associated to the D-CNN in Eq. (13). Let
r ∈/ L(G (d)) be fixed. Let f ∈ L2(X) with Eσf = 0. Then for > 0,
kRX(f)-fk2L2(X)-kP>r(f)k2L2(X) =cd,kfk2L2+(X),
(28)
where c√,e → 0 in probability as d → ∞ over X 〜σ[dr].
In words, with [dr] many training samples where r ∈ S(G(d)), the NNGP kernel and the NTK are
able to learn all Yr,ι with L(r) < r but not any eigenfunctions with L(r) > r. In Sec. C, we show
that the number of training samples can be reduced by a factor ofw if the readout layer Fattening is
replaced by the global average pooling.
5	Interpretation of the main results
We say r is the budget index if (1) (Finite Training Set) the training set size m 〜dr, or (2) (Finite
Compute Time) the training set X = X and the total number of training steps/time t 〜dr.
MLPs and LRLF Interactions Fig. 1 (a). Each G (d) is a linked list. Let v be the input node.
Clearly, we have dv = d, i.e., αv = 1 and αu = 0 (since deg(u) = 1) for all other nodes u. As such
力(G(d)) = N∖{0},巩r) = |r|, S(r) = 0, X(r) = |r| and 逐⑷=(|r| : r ∈ N∖{0}).	(29)
There isn,t any SpatiaI structure since S(r) = 0. Given a budget index r, the kernels can only learn
span{Yr,ι : r < r, r ∈ N∖{0}}. Therefore, MLPs are good at modeling LRLF interactions.
S-CNNs and SRHF Interactions Fig. 1 (b). For one-hidden layer convolutional networks, we
have d = p × k0 × w, (i.e. L = 0). The number of input nodes is w ≡ dαw and for each input
node v we have dv = p ≡ dαp . We have αp + αw = 1. Since the activation function of the
last layer is the identity function, there is no non-linear interactions between different patches and
4(G(d)) = {r ∈ N|N0d)|\{0}, d(r)∣ = 1}. Therefore for r ∈ 4(G(d)),
6(r) =	αw ,	巩r) =	∣r∣αp,	XG	=	{αw	+ ∣r∣αp,	r ∈	朗(G(d))}
(30)
7
Under review as a conference paper at ICLR 2022
Given a budget index r, the learnable r are the ones r ∈ 朗(G(d)) with
r >αw + ∣r∣αp = 1 + (|r| - 1)α? ⇒ |r| < 1 + (r - 1)∕αp .	(31)
When αp = 1 (and thus αw = 0), this S-CNN is essentially a shallow MLP and we have |r | < r. On
the other hand, if αp is small (say αp = 0.1), this network can model interactions with much higher
frequencies, at the cost of giving up long-range interactions. Therefore, there is a trade-off between
space and frequency, and S-CNNs with small patch size are good at modeling SRHF interactions.
D-CNNs and Interterpolation Fig. 1 (c). Recall that d = p × kL × w. Letp = dαp, k = dαk and
w = dαw . Then we have the constraint αp + Lαk + αw = 1. The learnable terms are the ones with
r > S(r) + 方(r) = S(r) + ∣r∣αp.	(32)
D-CNNs can simultaneously model interpolations (including the two ends) between LRLF and
SRHF, i.e. MRMF interactions. Consider two extreme cases. (i.) |u(r)| = 1, i.e. there is an
input node v s.t. rv = |r| and thus the non-linear interaction happens within one patch. In this
case, the length of the MST reaches its infimum S(r) = αw + Lak = 1 - α? and Eq. (32) implies
|r| < (r - 1)∕αp + 1, which is exactly Eq. (31). Thus D-CNNS can model SRHF interactions; see
Fig. 6 Yg. (ii.) |u(r)| = |r|, i.e. rv = 0 or 1 for all V ∈ NSd. Then &(r) = |r|(1 - a0) and
Eq. (32) implies |r| < r, which is the constraint for MLPs. In particular, D-CNNs can model LRLF
interactions; see Y5. By varying |r| and then varying |u(r)| in [1, |r], D-CNNs can model various
interpolating interactions between LRLF and SRHF.
HR-CNNs Extrapolations and Finer Interpolations Fig. 1 (d). The resolution of the learning
indices X(G(d)) canbe improved by decreasing ak, a? and increasing L accordingly. E.g., changing
αp → Op, ak → ak/2 and doubling the number of convolutional layers accordingly, then the
resolution of the range of spatial/frequency/learning indices is doubled. This empowers the network
to model finer-grained interpolating modes, and extrapolating modes. E.g., the last equation in
Eq. (31) becomes |r| < 1 + 2(r - 1)/ap (almost doubles the upper bound of |r|) and the network
can additionally model Ultra-Short-Range-Ultra-High-Frequency interactions (see Y5g in Fig. 3)
without sacrificing its expressivity, which isn’t the case for S-CNNs due to the space-frequency
trade-off. We refer to such networks as high-resolution CNNs (HR-CNNs). For practical networks,
e.g. ResNet (He et al., 2016), the filters/patches are already quite small and there isn’t much room to
reduce them. Equivalently, one increases the resolution of the input images instead, i.e. increasing d.
From this point of view, HR-CNNs and, therefore, our theorems justify the additional performance
gain from EfficientNet-type (Tan & Le, 2019) of model scaling.
For more details regarding computing the learning index, see Sec. D.
6	Experiments
There are many practical consequences due to Theorem 1 and Theorem 2. We focus on two of them
which are about the impact of architectures to learning / generalization:
1.	Order of Learning (Fig. 1 Green Arrow.) The order of learning is restructured from
frequency-based (MLPs) to space-and-frequency-based (CNNs).
2.	Learnability (Fig. 1 Cells under the budget line.) With the same budget index, MLP-
Learnable ( D-CNN-Learnable ( HR-CNN-Learnable. Moreover, the set differences be-
tween these learnable sets are captured as in Sec.5.
Overall, we see excellent agreements between predictions from our theorems and experimental re-
sults from both practical-size networks and kernel methods using NNGP/NT kernels, even when
d = 256 is moderate-size. We detail the setup, results, corrections, etc. for the experiments below.
Setup. Set d = p4 and the input X = (Sp-ι)p3 ⊆ Rp4, whereP ∈ N. Note that ap = 1/4. The
task is learning a function Y ∈ L2 (X) by minimizing the MSE, where
Y = Y1+ Y2+ Y3+ Y4+ Y5+ Y5g+ Y6+ Y7	(33)
8
Under review as a conference paper at ICLR 2022
and each eigenfunction Yi is normalized so that kYik22 = kY k22/8 = 1. The exact expressions of the
functions can be found in Sec.D.
We optimize finite-width networks by SGD+Momentum and infinite-width networks (NNGP and
NTK) by kernel regression. We investigate three types of architectures: (1) Dense04, a four hidden
layer MLP; (2) Conv(P2)02, a “deep" CNN with filter size/stride k = p2, and (3) Conv(P)04, a
“HS"-CNN with filter size/stride k = p. See Fig. 2 and Fig. 6 for a visualization of the associated
DAGs, and Sec. F for the code of the architectures. There is an activation φ in each hidden layer,
which is chosen so that φ* is the Gaussian kernel. For the CNNs, the readout layer(s) is Flatten-
Dense-Act-Dense. We carefully chose the eigenfunctions {Yi } so that they cover a wide range of
SPaCe-frequency combinations (S(Yi),巩Yi)) w.r.t. Conv(P)04. Under Conv(P)04,the correspond-
ing learning indices are X(Yi) = S(Yi) + 方(Yi) = 3αp + iαp = (3 + i)/4. For the learning
indices of Yi under Conv(P2)02 or Dense04, see the legends in Fig.3. The purpose of doing so
is to create a “separation of learning" under Conv(P)04, since in the large P limit, learning Yi re-
quires d(3+i)/4+ examples/SGD steps. The relation between architectures and learning indices can
be “visualized" in Fig. 2 for Y2, which is short-range-low-frequency ( deg(Y2) = 2). Fig. 2 (d)
plots the test residual of Y2 vs training set size for NTK regression, as one example to showcase
the relation among architectures, learning indices and generalization. See Fig. 6 in the appendix for
other eigenfunctions.
In the experiments, the width/number of channels is set to 512 for all networks. We sample mt =
32 × 10240 (mv = 10240) data points randomly from X as training (test) set with P = 4 and d =
256. The SGD training configurations (batch size (=10240), learning rate (=1.), momentum (=0.9)
etc.) are identical across architectures. To compute the kernels, we rely crucially on NeuralTangents
(Novak et al., 2020) which is based on JAX (Bradbury et al., 2018).
In Fig.3, for each eigenfunction 匕，we plot 1 E∣Yi(χ,t) - Yi(χ)∣2 against t, where Yi(χ,t) is the
projection of the prediction onto Yi and t is either the training steps (SGD) or training set size
(kernels). The expectation is taken over the test set. The budget index r = log(mt)/ log(d) ≈ 2.28.
As d = 256 (P = 4) is far from the asymptotic limit, we expect r = 2.28 being a soft cut-off between
learnable and non-learnable indices. Although the theorems assume d, P → ∞, they do provide good
predictions even when d and P are far from ∞. We summarize several key observations below.
1.	Dense04 (1nd Row.) This architecture can capture all low-frequency interactions (deg =
1, 2, Y1, Y2, Y3, Y5) but fail to learn deg ≥ 3 interactions, as expected. For MLPs,
making the network deeper won’t improve its learnability much; see Fig. 7.
2.	Conv(P2 )02 (2nd Row.) Learning curves of Y2/Y3 are separated from Y5 because the
spatial indices of them are different. Higher-frequency (deg = 3, 4) shorter-range interac-
tions (Y4, Yg) become (partially) learnable, as L(Y4) = 4, L(Yg) = 10 < r ≈ 2.28.
3.	Conv(P)04 (3rd Row). We see L(Yi) capture the order of learning very/reasonably well
in the kernel/SGD setting. To test the ability of Conv(P)04 in modeling ultra-short-
range-ultra-high-frequency interactions, we trace the learning progress of Y5 (deg(	)=
5, L( Y5) = 4.) As expected, while other architectures completely fail to make progress,
the NTK/NNGP of Conv(P)04 makes good progress and the SGD even completes the learn-
ing process. Interestingly, Y51 is learned faster than Y5 in the kernel setting but slower in
the Sgd setting (even slower than L(Yg) = 10), which is unexpected. We suspect it might
be due to certain “implicit" effect of SGD. Further investigation is needed to understand it.
7 Conclusion
We establish a precise relation among networks’ architectures, eigenstructures of the inducing ker-
nels, and generalization of the corresponding kernel machines. We show that deep convolutional
networks restructure the eigenspaces of the inducing kernels, which empowers them to learn a dra-
matically broader class of functions, covering a wide range of space-frequency combinations. We
believe our framework can be extended to study architectural inductive biases for other families of
topologies, such as RNNs, GNNs, and self-attention. However, we have not covered the learning
1 Ultra-Long-Range-Low-Frequency under Conv(p)04, with L( Y5) = 8/4 = L(Yil)
9
Under review as a conference paper at ICLR 2022
MLP®4： SGD Train
+ Λ(Y1)=4∕4
H- -C(Ya)=8∕4
齐:一三"二二一江? 0M∖ = fl⅛
* AM)=I »4
∙P∖∙……∙^:÷∖!Γ⅛Γ÷ Ag=≡<W
；\ ； Wz H
..J.......\........：.¾∖ AYe) = I a4
\ :	∖ p⅜ ⅛7)=16∕4
\ ?	y-P^MSEZB
0.5 °-7® 1 1≡6 1.5 176 2 2aδ
log(SG□ Steps) ∕log(d)
CNN(P严:NNGP Regression
CNN(P)叫 SGD Test
CNN(P2严：SGD TraIn
H- r(Yι)=4∕4
H- Λ(Y2)⅛6∕4
1.5 '∙76 2 ≡aδ
log(SG□ Steps) ∕log(d)
TR Λ(⅛)=6∕4
* AM)M8/4
X(Y⅛>=12M
jC(Y6)=8∕4
£(Y,)：=10/4
面 =12/4
SE⅛
0.8
0.β
« 0.4
0.2
log(Training Set Slzβ)∕log(d)
CNN(P产:NTK Regression
CNN(p)®4: SGD Train
E«
+ W)="4
rH- r(Y2)=5M
2 225
Iog(SGDSteps) ∕log(d)
-H ccω=6∕4
+ A(YJ=7M
Λ∏,i)=8M
A(Ye)=8/4
r(Y0) = 9∕4
4(Y7)=10∕4
Figure 3: Learning Dynamics vs Architectures vs Learning Indices. We plot the learning/training
dynamics of each eigenfunction Yi. From top to bottom: a 4-layer MLP, a 2-layer CNN and a 4-layer
CNN. From left to right: residual MSE (per eigenfunction) of NNGP/NTK regression, test/training
MSE of SGD. The learning indices of Yi in each architecture is shown in the legends.
dynamics of SGD. In addition, it is of great interest and importance to study the combined effect of
SGD and architectures in the future.
References
William Beckner. Inequalities in fourier analysis. AnnalsofMathematics, 102(1):159-182, 1975.
William Beckner. Sobolev inequalities, the poisson semigroup, and analysis on the sphere sn. Pro-
ceedings of the National Academy of Sciences, 89(11):4816-4819, 1992.
Alberto Bietti. Approximation and learning with deep convolutional models: a kernel perspective,
2021.
Alberto Bietti and Francis Bach. Deep equals shallow for relu networks in kernel regimes. arXiv
preprint arXiv:2009.14397, 2020.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, and Skye Wanderman-Milne. JAX: composable transformations of Python+NumPy
programs, 2018. URL http://github.com/google/jax.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks:
The power of initialization and a dual view on expressivity. In Advances In Neural Information
Processing Systems, 2016.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hier-
archical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,
pp. 248-255, 2009. doi: 10.1109/CVPR.2009.5206848.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
10
Under review as a conference paper at ICLR 2022
Arturo Deza, Qianli Liao, Andrzej Banburski, and Tomaso Poggio. Hierarchically compositional
tasks and deep convolutional networks. arXiv preprint arXiv:2006.13915, 2020.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018.
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2019.
Alessandro Favero, Francesco Cagnetta, and Matthieu Wyart. Locality defeats the curse of dimen-
sionality in convolutional teacher-student scenarios, 2021.
Christopher Frye and Costas J. Efthimiou. Spherical harmonics in p dimensions. 2012.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Limitations of lazy
training of two-layers neural networks. arXiv preprint arXiv:1906.08899, 2019.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized two-layers
neural networks in high dimension, 2020.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Conference on Computer Vision and Pattern Recognition, pp. 770-778, 2016.
Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas
Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2020. URL
http://github.com/google/flax.
Wei Hu, Lechao Xiao, Ben Adlam, and Jeffrey Pennington. The surprising simplicity of the early-
time learning dynamics of neural networks. arXiv preprint arXiv:2006.14599, 2020.
Wei Huang, Weitao Du, and Richard Yi Da Xu. On the neural tangent kernel of deep networks with
orthogonal initialization. arXiv preprint arXiv:2004.05867, 2020.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. arXiv preprint arXiv:1806.07572, 2018.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language
models. arXiv preprint arXiv:2001.08361, 2020.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Yann Lecun. Generalization and network design strategies. In Connectionism in perspective. Else-
vier, 1989.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Sam Schoenholz, Jeffrey Pennington, and Jascha Sohl-
dickstein. Deep neural networks as gaussian processes. In International Conference on Learning
Representations, 2018.
Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in Neural Information Processing Systems, 2019.
Zhiyuan Li, Yi Zhang, and Sanjeev Arora. Why are convolutional nets more sample-efficient than
fully-connected nets? CoRR, abs/2010.08515, 2020. URL https://arxiv.org/abs/2010.
08515.
11
Under review as a conference paper at ICLR 2022
Eran Malach and Shai Shalev-Shwartz. Computational separation between convolutional and fully-
connected networks. CoRR, abs/2010.01369, 2020. URL https://arxiv.org/abs/2010.
01369.
Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahra-
mani. Gaussian process behaviour in wide deep neural networks. In International Conference on
Learning Representations, 2018.
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Generalization error of random fea-
tures and kernel methods: hypercontractivity and kernel matrix concentration. arXiv preprint
arXiv:2101.10588, 2021a.
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Learning with invariances in random
features and kernel models. arXiv preprint arXiv:2102.13219, 2021b.
Ashley Montanaro. Some applications of hypercontractive inequalities in quantum information the-
ory. Journal of Mathematical Physics, 53(12):122206, 2012.
Gregoire Montavon, Wojciech Samek, and Klaus-Robert Muller. Methods for interpreting and un-
derstanding deep neural networks. Digital Signal Processing, 73:1-15, 2018.
Preetum Nakkiran, Gal Kaplun, Dimitris Kalimeris, Tristan Yang, Benjamin L. Edelman, Fred
Zhang, and Boaz Barak. SGD on neural networks learns functions of increasing complexity.
CoRR, abs/1905.11604, 2019. URL http://arxiv.org/abs/1905.11604.
Roman Novak, Lechao Xiao, Yasaman Bahri, Jaehoon Lee, Greg Yang, Daniel A. Abolafia, Jeffrey
Pennington, and Jascha Sohl-dickstein. Bayesian deep convolutional networks with many chan-
nels are gaussian processes. In International Conference on Learning Representations, 2019a.
URL https://openreview.net/forum?id=B1g30j0qF7.
Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A. Abo-
lafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with
many channels are gaussian processes. In International Conference on Learning Representations,
2019b.
Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A. Alemi, Jascha Sohl-Dickstein,
and Samuel S. Schoenholz. Neural tangents: Fast and easy infinite neural networks in python.
In International Conference on Learning Representations, 2020. URL https://github.com/
google/neural-tangents.
Yaniv Ovadia, Emily Fertig, J. Ren, Zachary Nado, D. Sculley, Sebastian Nowozin, Joshua V. Dillon,
Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty? evaluating
predictive uncertainty under dataset shift. In NeurIPS, 2019.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Expo-
nential expressivity in deep neural networks through transient chaos. In Advances In Neural
Information Processing Systems, 2016.
Meyer Scetbon and Zaid Harchaoui. Harmonic decompositions of convolutional networks. In Inter-
national Conference on Machine Learning, pp. 8522-8532. PMLR, 2020.
Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information
propagation. International Conference on Learning Representations, 2017.
Andrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green,
Chongli Qin, Augustin %idek, Alexander WR Nelson, Alex Bridgland, et al. Improved protein
structure prediction using potentials from deep learning. Nature, 577(7792):706-710, 2020.
Vaishaal Shankar, Alex Chengyu Fang, Wenshuo Guo, Sara Fridovich-Keil, Ludwig Schmidt,
Jonathan Ragan-Kelley, and Benjamin Recht. Neural kernels without tangents. In International
Conference on Machine Learning, 2020.
12
Under review as a conference paper at ICLR 2022
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. Nature, 529(7587):484—489, 2016.
Jascha Sohl-Dickstein, Roman Novak, Samuel S. Schoenholz, and Jaehoon Lee. On the infinite
width limit of neural networks with a standard parameterization, 2020.
Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural net-
works. In International Conference on Machine Learning, pp. 6105-6114. PMLR, 2019.
M. Alex O. Vasilescu, Eric Kim, and Xiao S. Zeng. Causalx: Causal explanations and block multi-
linear factor analysis, 2021.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762,
2017.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
Lechao Xiao and Jeffrey Pennington. What breaks the curse of dimensionality in deep learning?,
2021. URL https://openreview.net/forum?id=KAV7BDCcN6.
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and Jeffrey Pennington.
Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla con-
volutional neural networks. In International Conference on Machine Learning, pp. 5393-5402,
2018.
Greg Yang and Edward J Hu. Feature learning in infinite-width neural networks. arXiv preprint
arXiv:2011.14522, 2020.
Greg Yang and Hadi Salman. A fine-grained spectral perspective on neural networks, 2020.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107-
115, 2021.
13
Under review as a conference paper at ICLR 2022
A	Additional related work
Bietti (2021) studies the approximation and learning properties of CNN kernels via the lens of
RKHS. Impressively, they demonstrate that a 2-layer CNN kernel can reach 88.3% validation ac-
curacy on CIFAR-10, matching the performance of a 10-layer Myrtle kernel Shankar et al. (2020).
Malach & Shalev-Shwartz (2020) and Li et al. (2020) study the algorithmic benefits of shallow
CNNs and show that they outperform MLPs in certain tasks. Xiao & Pennington (2021) and Favero
et al. (2021) study the benefits of locality in S-CNNs and argue that locality is the key ingredient
to defeat the curse of dimensionality. Mei et al. (2021b) and several papers mentioned above study
the benefits of pooling in (S-)CNNs in terms of data efficiency. Their conclusion is similar to that
of Theorem 4 (b): pooling improves data efficiency by a factor of the pooling size. In addition,
we show that (Theorem 4 (a)) pooling does not improve training efficiency for D-CNNs, extending
a result from Xiao & Pennington (2021) which concerns S-CNNs. Finally, Scetbon & Harchaoui
(2020) also study the eigenstructures of certain CNN kernels without pooling. Their kernels can be
considered as a particular case of the NNGP kernels, where the associated networks have only one
convolutional layer and multiple dense layers. The key contribution that sets the current work apart
from existing work is a precise mathematical characterization of the fundamental role of architec-
tures in (infinite-width) networks through a space-frequency analysis.
B A Toy Example and Motivation
In this section, we provide a toy example to help understand the motivation and ideas of the paper.
Let’s consider learning the following polynomials in Sd-1 ≡ {x ∈ Rd : kxk2 = 1} using (in)finite-
width neural networks and for concreteness we have set d = 10:
f1 (x) = x9, f2(x) = x0x1, f3(x) = x0x8, f4(x) = x6x7(x62 - x27), f5(x) = x2x3x5	(34)
Which architectures (e.g. MLPs, CNNs) can efficiently learn fi or the sum of fi? More precisely, (1)
if we have sufficiently amount of training data, how much time (compute) is required to learn fi for
a given architecture? (2) Alternatively, if we have sufficiently amount of compute, how much data is
needed to learn fi? To answer these questions, one crucial step is to provide a meaningful definition
of “learning complexity" of a function fi under an architecture M. Denote this complexity associ-
ated to compute and to data by CC (fi; M) and CD(fi; M), resp. With such the complexity properly
defined, the questions are reduced to solving the min-max problem minM maxi{CC/D (fi ; M)}, if
the task is, e.g, to learn the sum of fi.
Let’s focus on the complexity. For infinite-width MLPs, aka, inner product kernels, it is well-
known that they have the inductive biases (Yang & Salman, 2020; Ghorbani et al., 2020) (known
as the frequency biases) that the model prioritizes learning low-frequency modes (i.e., low degree
polynomials) over high-frequency modes. In addition, the models require 〜 d many data points to
learn any degree r polynomials in Rd. The frequency biases of MLPs are the consequence of the fact
that the eigenspaces of inner product kernels are structured based only on frequencies. Specific to our
example, for MLP, the order of learning is f1/f2, f3/f5/f4 and it requires about 10/102, 102/104
many data points to learn the functions. Clearly, the model is very inefficient in learning f4 , the
high-frequency modes.
To improve the learning efficiency, we must take the modality of the task into account, which is
overlooked by MLPs. We observe that: (1) although of high frequency, f4 depends only on two con-
secutive terms x6 andx7, which are spatially close; (2) in contrast, f3(x) = x0x8 is of low frequency
but the spatial distance between the two interaction terms x0 and x8 are “far" from each other; (3) the
function f5(x) = x2x3x5 is somewhere in-between: the order of interaction is 3 (lower than that of
f4) and the spatial distance (not yet defined) among interaction terms is conceptually “closer" than
that of f3(x) = x0x8, but “farther" than that of f5. Using the terminologies from the introduction,
the functions f2/f3/f5/f4 model interactions of types: Short-Range-Low-Frequency/Long-Range-
Low-Frequency/Median-Range-Median-Frequency/Short-Range-High-Frequency. By Range we
mean the distance among interaction terms and by Frequency we mean the order(=degree) of in-
teractions. Clearly, the MLPs are inefficient since they totally ignore the “spatial structure" of the
functions. As such, a good architecture must balance the "spatial structure" and the “frequency
structure" of the functions. For the same reason, a good complexity measure must (1) be able to
capture both the frequency of the functions and the spatial distance among interaction terms; (2)
14
Under review as a conference paper at ICLR 2022
be able to precisely characterize the data and the computation efficiency of learning and their de-
pendence on architectures. The learning index mentioned in the introduction satisfies these two
conditions. It is the sum of the frequency index and the spatial index. The former measures the
order (=degree=frequency) of interactions, which depends on how the network partitions the input
into patches. The latter measures the spatial distance among the interaction terms, which depends on
how the network organizes these patches hierarchically. Later we show that, in the high-dimensional
setting, the learning index provides a sharp characterization for the learnability of eigenfunctions,
and certain CNNs can perfectly balance the learning of f3 and f4, i.e., informally
CC/D(f3; CNN) ≈ CC/D(f4; CNN) ≈ CC/D(f2/3; MLP) << CC/D(f3; MLP)	(35)
See Fig. 3 in the experiment section for more details.
C	Global Average Pooling (GAP) vs Flattening
In this section, we compare two readout strategies for CNNs: GAP and Flatten
C.1 Infinite-Training Data.
First, let’s state a theorem regarding training efficiency in the infinite-training-data-finite-training-
time regime, which follows directly from the same arguments in Sec. 2. The theorem requires
knowing only the eigenvalues of the kernels.
Let F : L2 (X) → L2 (X) be the solution operator to the kernel descent h = -K(h - f) with initial
value ht=o = 0, i.e. ht ≡ Ft(f) ≡ (Id - e-Kt)f. Then We have for t 〜d, Ft ≈ P<r.
Theorem 3. Assume Assumption-G and Assumption-φ and K =次 G(d)or Θg(d) .Let r / L(G ⑷)
and t 〜dr. Thenfor 0 < e < inf{|r —尸| : r / L(G(d))} and f / L2(X) with Eσf = 0 we have
kFt(P<rf)-P<rfk22 .e-dkP<rfk22 and kFt(P>rf)-P>rfk22 &e-d-kP>rfk22
(36)
for d sufficiently large.
In words, in the infinite-training-data-finite-training-time regime, within t 〜d amount of time only
the eigenfunctions Yr,ι with L(r) < r are learnable.
C.2 GAP vs Flattening.
Let a CNN with and without a global average pooling (GAP) be defined as
CNN+GAP	[Input]	→	[Conv(p)-Act]	→	[Conv(k)-Act]0L →	[GAP] → [Dense]	(37)
CNN+Flatten	[Input]	→	[Conv(p)-Act]	→	[Conv(k)-Act]0L →	[Flatten] → [Dense]	(38)
resp. Note that there isn’t any activation after the GAP/Flatten layer. The DAGs associated to
CNN+GAP and CNN+Flatten are identical, and the associated kernel and network computations in
each layer are also identical but the last layer; see Sec. K for more details. Our last theorem states
that the GAP improves the data efficiency by a factor of w, the window size of the pooling, but
does not improve the training efficiency in the infinite-training-data-finite-training-time regime. The
former is due to the dimension reduction effect of GAP in the eigenspaces and the latter is due to
the fact that the GAP layer does not change the eigenvalues of the associated eigenspaces.
Let KSym = %Sym or θSym be the NNGP kernel or NTK associated to Eq. (37) and Lpym(X) ≤
Lp (X) be the subspace of “translation-invariant" functions, whose co-dimension is w. Let FSym,
PSym and RSym be the solution operator, projection operator and regressor associated to KSym, resp.
Theorem 4 (Informal). For the architectures defined as in by Eq. (37), we have
(a)	Theorem 3 holds with L2 (X), F and P replaced by LS2ym(X), FSym and PSym, resp.
(b)	Eq. (28) holds with Lp(X), R and P replaced by LSpym(X), RSym and PSym, resp and
with X 〜σ[dr aw] under the assumptions that all activations in the hidden layers are
poly-admissible.
15
Under review as a conference paper at ICLR 2022
Remark 1. Several remarks are in order.
1.	In terms of order of learning, our results say that (infinite-width) neural networks pro-
gressively learn more complex functions, where complexity is defined to be the learning
index L(r). This is consistent with the empirical observation from Nakkiran et al. (2019).
Regardless of architectures (MLPs vs CNNs), linear functions have the smallest learn-
ing index2 L(r) = 1 and are always learned first, which was first proved in Hu et al.
(2020) for MLPs. After learning the linear functions, the learning dynamics of MLPs
and CNNs diverge. MLPs will start to learn quadratic functions (L(r) = 2), then cu-
bic functions (L(r) = 3) and so on. While for CNNs, L(r) can be fractional numbers (e.g.
L(r) = 5/4, 6/4) and it is possible for the network to learn higher order functions before
lower order functions. See Sec.D and Sec. 6for more details.
2.	By NTK-style convergent arguments (e.g., Du et al. (2019)), the above kernel regression
results could be extended to the over-parameterized network setting as long as the learning
rate of gradient descent is small and the number of channels is sufficiently large (polyno-
mially in d (Huang et al., 2020)).
3.	Theorem 4 states that CNN+GAP is better than CNN+Flatten in terms of data efficiency but
not training efficiency. In particular, when the dataset size is sufficiently large, CNN+GAP
and CNN+Flatten perform equally well. Therefore, in the large (infinite) data set regime,
CNN+Flatten may be preferable since the associated function class is less restricted.
C.3 Experimental Results: GAP vs Flatten.
We compare the SGD learning dynamics of two convolutional architectures: Conv(P)O3-Flatten and
Conv(P)O3-GAP. Both networks have three convolutional layers with filter size and stride equal to
p. The spatial dimension is reduced to p after the convolutional layers. The only difference is that
the architecture Conv(P)O3-Flatten ( Conv(P)O3-GAP) uses a Flatten-Dense ( GAP-Dense) to map
the penultimate layer to the logit layer.
The experimental setup is almost the same as that of Sec. 6 except the eigenfunctions {Yi} are chosen
to be in the RKHS of the NNGP kernel/NTK of Conv(P)O3 -GAP and thus of Conv(P)O3-Flatten,
i.e., they are shifting-invariant (the invariant group is of order p). Moreover, we still have X(Yi)=
(i + 3)/4. For each Yi , we plot the validation MSE of the residual vs SGD steps in Fig. 4. Overall,
the predictions from Theorem 4 gives excellent agreement with the empirical result. With training
set size mt = 32 × 10240, the residuals of Yi for GAP and Flatten are almost indistinguishable
from each other for i ≤ 6 (recall that X(Y6) = 9/4 = 2.25 < r ≈ 2.28). However, when i = 7
the dataset size (r ≈ 2.28) is relatively small compared to the learning index (X(Y7) = 2.5), GAP
outperforms Flatten in learning Y7
To test the robustness of the prediction from Theorem 4 (a) on more practical datasets and models,
we perform an additional experiment on ImageNet (Deng et al., 2009) using ResNet (He et al.,
2016). We compare the performance of the original ResNet50, denoted by ResNet50-GAP, and a
modified version ResNet50-Flatten, in which the GAP readout layer is replaced by Flatten. We
use the ImageNet codebase from FLAX3(Heek et al., 2020). In order to see how the performance
difference between ResNet50-GAP and ResNet50-Flatten evolves as the training set size increases,
we make a scaling plot, namely, we vary the training set sizes4 mi = [m × 2-i/2] for i = 0, . . . , 11,
where m = 1281167 is the total number of images in the training set of ImageNet. The networks
are trained for 150 epochs with batch size 128. We plot the validation accuracy and loss (averaged
over 3 runs) as a function of training set size mi in Fig. 5. Overall, we see that the performance gap
between ResNet50-GAP and ResNet50-Flatten shrink substantially as the training set size increases.
E.g, using 1/8 of the training set (i.e. i = 6), the top 1 accuracy between the two is 19.3% (57.7%
GAP vs 38.4% Flatten). However, with the whole training set (i.e., m0), this gap is reduced to
2% (76.5% GAP vs 74.5% Flatten). To demonstrate the robustness of this trend, we additionally
generate the same plots for ResNet34 and ResNet101; see Fig. 8 in Sec. E.
2Ignoring the constant functions.
3https://github.com/google/flax/blob/main/examples/imagenet/README.md
4Standard data-augmentation is applied for each i; see input_pipeline.py.
16
Under review as a conference paper at ICLR 2022
log(SGD Steps) ∕log(d)
Figure 4: Learning Dynamics: GAP vs Flatten. We plot the validation MSE of the residual of
each Yi (left → right: i = 1 → 7) for GAP (Solid lines) and Flatten (Dashed lines). The mean/std
in each curve is obtained by 5 random initializations. Clearly, the residual dynamics of GAP and
Flatten are almost indistinguishable for Yi with i ≤ 6. However, GAP outperforms Flatten when
learning Y7.
Accuracy: GAP vs Flatten
.6,42
Ooo
ωN86EE- 8; do.L
Loss: GAP vs Flatten
6 4 2
Ado」IU山，sso」。UOnEP=EA
105	106	105	106
Training Set Size	Training Set Size
Figure 5:	ResNet50-GAP vs ResNet50-Flatten. As the training set size increases the performance
(accuracy and loss) gap between the two shrinks.
D DAGs, Eigenfunctions, Spatial Index, and Frequency Index.
In this section, we provide more details regarding the DAGs and the eigenfunctions used in the
experiments, and how the spatial, frequency and learning indices are computed.
Let (Sp-ι)p3 ⊆ Rp4 be the input space, where d = p4 is the input dimension. We use X ≡
(Xk)k∈[p]4 ∈ (Sp-I)P to denote one input (an image), where k = [kι, k2, k3, k4] ∈ [p]4. In
addition, we treat [p]4 as a group (i.e. with circular boundaries) and let e1 = [1, 0, 0, 0], e2 =
[0, 1, 0, 0] , e3 = [0, 0, 1, 0] and e4 = [0, 0, 0, 1] be a set of generator/basis of the group. Note that
each input Xk is partitioned into p3 many patches: {Xk1,k2,k3,: : k1, k2, k3 ∈ [p]}.
17
Under review as a conference paper at ICLR 2022
The eigenfunctions used in the experiments and the associated space/frequency indices (will be
explained momentarily) are given as follow
Eigenfunction		degree	MLP	Space/Freq Index CNN(p2)02	CNN(p)
Y1 (x)	= X	c(k1)xk k∈[p-1]4	1	0/1	1 /1 2, 2	3 /1 4/ 4
Y2 (x)	= X	ck xkxk+e4 k∈[p-1]4	2	0/2	1 / 2 2' 2	3 / 2 4/4
Y3 (x)	=	c(k3)xk+e3xk+e4 k∈[p-1]4	2	0/2	1 / 2 2, 2	4 /2 4/ 4
Y4(x)	=	ck xk+e3 +e4 xk+e4 xk k∈[p-1]4	3	0/3	1 / 3 2, 2	4 / 3 4/ 4
YJ (x)	= X	ck xk xk+e4 xk+2e4 (xk - xk+e4) k∈[p-1]4	5	0/5	1 / 5 2' 2	3 / 5 4/ 4
Y5(x)	=	ck xkxk+e1 k∈[p-1]4	2	0/2	2 / 2 2'2	6 / 2 4/4
Y6(x)	=	c(k6)xkxk+e3 (3x2k-e3 - x2k) k∈[p-1]4	4	0/4	1 / 4 2' 2	5 / 4 4/ 4
Y7(x)	= X	c(k7)xk-e3+ e4 xk+e2 (3xk - xk-e3+e4)	4	0/4	1 /4 2' 2	6 / 4 4/ 4
k∈[p-1]4
Each (eigen)function Yi is a linear combination of basis eigenfunction of the same type, in the sense
they have the same eigenvalue and the same spatial/frequency/learning indices. The coefficients c(ki)
are first sampled from standard Gaussians iid and then multiplied by an i-dependent constant so that
Yi has unit norm5. In the experiments, p is chosen to be 4 and the target is defined to be sum of them
Y = X Yi	(39)
Since they are orthogonal to each other, kY k22/8 = kYik22 = 1 where the L2-norm is taken over the
uniform distribution on (Sp-I)P . In the experiment when We compare Flatten and GAP ( Fig. 4),
we set all c(ki) to be the same (note that we also remove Y5 from the target function), so that the
functions are learnable by convolutional networks with a GAP readout layer.
We compare three architectures. Fig. 6 Column (a) MLP04, a (four-layer) MLP, the most coarse
architecture used in the paper. Fig. 6 Column (b), CNN(p2)02, a"D”-CNN that contains two con-
volutional layers with filter size/stride equal to p2. Fig. 6 Column (c), CNN(p)04, a “HR"-CNN,
the finest architecture used in the experiments, that contains four convolutional layers with filter
size/stride equal to p. In all experiments except the one in Fig. 4, we use Flatten as the readout layer
for the convolutional networks and add a Act-Dense layer after Flatten to improve the expressivity
of the function class. However, in the Flatten vs GAP experiments, Fig. 4, we have only one dense
layer after GAP/Flatten.
We show how to compute the frequency index, the spatial index and the learning index through
three examples. We focus on Y： / Y3 / '5 which have degree 2/2/5, resp. The indices of other
eigenfunctions can be computed using the same approach. We use Dashed Lines to represent either
an edge connecting an input node to a node in the first-hidden layer or an edge associated to a dense
layer. In either case, the corresponding output node of the edge has degree O(1) and thus the weights
(of the DAGs) of such edges are always 0. Only Solid Lines are relevant in computing the spatial
index. Since each Yi is a linear combination of basis eigenfunctions of the same type, we only need
to compute the indices of one component. We use the k = 0 component, which corresponds to the
5In our experiments, they are normalized over the test set.
18
Under review as a conference paper at ICLR 2022
colored path in each DAG. Recall that P = 4, d = p4 = 256 and mt = 32 X 10240 〜d2.28 (training
set size), i.e. the budget index is roughly r = 2.28.
(a.) MLP Column (a) Fig. 6. The NTK and NNGP kernels are inner product kernels and
the associated DAGs are linked lists. The corresponding DAG has only one input node
whose dimension is equal to d = p4 . The spatial index is always 0 since the degree of
each hidden node is 1 (since 1 = d0) and the frequency index is equal to the degree of
the eigenfunctions. Thus X(Y )=歌 Y3) = 2 and S( Y5 ) = 5. Changing the number
of layers won't change the learning indices. In sum, learning Y /	/ using infinite-
width MLP requires d2+ /d2+ /d5+ many samples /SGD steps. Clearly, Y5 is completely
unlearnable as r = 2.28 << 5. In the MSE plot YJ (5-th row in Fig. 6), the Red Lines
does not make any progress
(b.) CNN(p2 产2 Column (b) Fig. 6. The input image is partitioned into p2 patches and each
patch has dimension p2 . The second layer of the DAG has p2 many nodes, each node
represents one pixel (with many channels) in the first hidden layer of a finite-width Con-
vNet. After one more convolutional layer with filter size/stride p2, the number of node
(pixel) is reduced to one. The remaining part of the DAG is essentially a linked list
(Dashed Line) with length equal to 1, which corresponds to the Act-Dense layer. The
frequency index ^(Y2) = 21 = 1. This is because the degree of Y2 is 2 and the input
dimension of a node is p2 = d1/2. The spatial index is equal to 1/2, since the mini-
mum tree containing xkxk+e4 has only one non-zero edge (Solid Lines) whose weight
is equal to 1/2 (since the degree of the output node is p2 = d1/2); see the colored paths
in Fig. 6 Column (b). Therefore the learning index of X(Y2) = 1 + 1/2 = 3/2. Sim-
ilarly X(Y3 ) = 1 + 1/2 = 3/2, as the term xk+e3 xk+e4 are lying in the same patch
of size p2 for all k, and X(Y5J) = 5/2 + 1/2 = 3. In sum, learning Y2/Y3/Y5J using
infinite-width CNN(P2 产2 requires d1.5+ /d1.5+ /d3+ many samples /SGD steps. While
neither infinite-width CNN(P2 产2 nor MLP04 distinguishes Y2 from Y3, CNN(P2产2
does improve the learning efficiency: d2+ → d1.5+ . Note that Y5J is still unlearnable as
r = 2.28 < 3 = X(Y5J). In the MSE plot Y5J (5-th row in Fig. 6), the Orange Line does
not make any progress.
(c.) CNN(P)04 Column (c) Fig. 6. The input image is partitioned into P3 patches and each
patch has dimension P. The second/third/fourth/output layer of the DAG has P3/P2/P/1
many nodes. The frequency indices are: 巩 ％) = 3(Y ) = 21 = 1/2 and 3(Y )=
54 = 5/4. This is because the size of input nodes is reduced to P = d1/4. Unlike the
above cases, the spatial indices become different. The two interacting terms in xkxk+e4
and the three interacting terms in xk+e4 xk+2e4 (x2k - x2k+e ) are in the same input node
while the two interacting terms in xk+e3 xk+e4 and are in two different input nodes. As a
consequence, the minimum spanning tree (MST) that contains xk and xk+e4 and the one
contains xk, xk+e4 and xk+2e4 are the same. They have 3 solid lines. However, the MST
containing Xk+e3 and Xk+e4 has 4 solid lines. Therefore 6( Y) = S( '5)=3 × 1 and
S(	) = 4 × 4. As such, X(Y) = 5, X(Y ) = 6 and X(任)=4. In sum, learning
Y2/Y3/Y5J using infinite-width CNN(P)04 requires d1.25+ / d1.5+ / d2+ many samples
/SGD steps, resp. Now X(Y5J) = 2. < 2.28 and in the MSE plot Y5J (5-th row in Fig. 6),
the Blue Line does make significant progress (the MSE is reduced to 〜0.2.)
19
Under review as a conference paper at ICLR 2022
JC(YD=O+1=1
9 Oitput Node
Input Node
JC(YJ=O+2=2
9 Oitput Node
Input Node
jC(YJ=0+2=2
9 Oitput Node
Input Node
JC(Y4)=O+3=3
Oitput Node
Input Node
X(Y1)=J+⅛=J,deg(Y1)=1
Ultra-Lo w⅛Frequθrκ⅛c r(Y1)=J+J=∣
Spatial: OkT
Spatial: □k"l
Spatial： αk>0
FnBquencyrak-O
r(V4)=ι+∣=∣,deg(Y4)=3
Short- Range-Median-Frequency： £(Y«)=J+| =J
11jljlɑi111111111111111111111111j1jiɑ
Ultra-Short-Range-Low-FrβqιiθncyrZχYa)=∣+∣=∣
FnBquencyrak-O
Spatial: OkT
Spatial: αk"0
-C(V8)=J+f=∣,deg(Y3)=2
FnBquencyrak-O
Fnequency: ¾≡0
Spatial: OkT
Spatial： <⅛≡⅜
Short-Range-Low^Frequerwy: zχγ8)=j +∣=J
Spatial： ak"0
I I，I，I，I，，，，I，I，I，I，I，I，I，I，I，I，I，I，I，I，I，，，，I，I，I，I，I，I，I，I，l，.
Spatial： αk"0
£(Y；)=0+5=5
'CXitput Node
Input Node
ZXYs)=O+2=2
Oitput Node
Input Node
JC(Ye)=O+4=4
Oitput Node
Input Node
JC(Y7)=0+4=4
Oitput Node
Input Node
Spatial: αk>j
Spatial: αk>2
MSE Residual vs TYatikig Set Size: Y«
的)=；+*T，Alg(Y5)=5
Ultra-Short-Range-Ultra-High-Frequerwy: £(Yj)=| +；=；
SpatIaIrakBj
SpatIakakBO
Fnequency: ¾≡0
Spatial: <⅛b!

£(*6)=1+≡=5∙dβ9(γ5)=2
Utl r⅜ Long-Range-Ltw-F requency： Λf⅛)=J+j =∣
Spallal: <¼al
Spatial: □k>2
Spatial: αk>0
Fnequency: c⅛≡ O

£(Ye)=^+s=-?.deg(Ye)=3
Long-Range-High-Frequency: zχYβ)=J +J =J
Spatial: αk∙J
SpatIakakBO
Fnequency: c⅛≡ O
Spallal: <⅛b1

r(Y7)=≡÷*=^,deg(Y7)=4
Utl r⅜Long-Range-High-F requency： £(Y?) =$ +；=芋
Spatial: OkT
Spatial: □k"l
Spatial: Ok"0
FnBquencyrok-O
Spatial： a*"；
(a) MLP	(b)CNN(p2)52
(C) CNN(P产	(d) MSE Residual
Figure 6:	Eigenfunction vs Learning Index vs Architecture/DAG. Rows: eigenfunctions Yi with
various spaCe-frequenCy Combinations. Columns: DAGs assoCiated to (1) a four-layer MLP; (b)
CNN(p2产2, a"D”-CNN; (c) CNN(P产4 a “HR”-CNN. Column (d) is the MSE, as a function of
training set size (X-axis), of the residual of the Corresponding eigenfunCtion Yi obtained by NTK-
regression. The colored path in each DAG corresponds to the minimum spanning tree that contains
all interacting terms in the k = 0 component of Yi .
20
Under review as a conference paper at ICLR 2022
Figure 7: MLPs do not benefits from having more layers. We plot the learning dynamics vs
training set size / SGD steps for each eigenfunction Yi . Top: NTK regression and bottom: SGD
+ Momentum. Left: MLP; right: CNN. Dashed lines / Solid lines correspond to one-hidden/four-
hidden layer networks. For both finite-width SGD training and infinite-width kernel regression,
having more layers does not essentially improve performance of a MLP. This is in stark contrast to
CNNs (right). By having more layers, the eigenstuctures of the kernels are refined.
E Figure Zoo
E.1 MLPS: DEPTH 6= HIERARCHY
We compare a one hidden layer MLP and a four hidden layer MLP in Fig. 7. Unlike CNNs, increas-
ing the number of layers does not improve the performance of MLPs much for both NTK regression
and SGD. This is consistent with a theoretical result from Bietti & Bach (2020), which says the
NTKs of Relu MLPs are essentially the same for any depth.
E.2 ImageNet Plots
21
Under review as a conference paper at ICLR 2022
Accuracy: GAP vs Flatten
Loss: GAP vs Flatten
6 4 2
Ado=uψsso.lo UOnEP=EA
105	106
Training Set Size
Loss: GAP vs Flatten
105	106
Training Set Size
Accuracy: GAP vs Flatten
6 4 2
AdO=UESO.IO UOnEP=EA
105	106	105	106
Training Set Size	Training Set Size
Figure 8: ResNet-GAP vs ResNet-Flatten. As the training set size increases the performance
(accuracy and loss) gap between the two shrinks substantially. Top/bottom ResNet34/ResNet101
F ARCHITECTURE SPECIFICATIONS
In this section, we provide the details of the architectures used in the experiments.
G Assumptions
Assumption-G. Let G = (G(d))d. There are absolute constants c, C > 0 such that
(a.) For each non-input node u ∈ N(d), there is αu ∈ ΛG such that
cdαu ≤ deg(u) ≤ Cdαu .	(40)
For each edge uv ∈ E(d), its weight is defined to be ωuv ≡ αu.
(b.) For each input node v, there are dv ∈ N and 0 < αv ∈ ΛG such that
cdαv ≤ dv ≤ Cdαv and X αv = 1.	(41)
v∈N0(d)
(c.) Let N1(d) ≡ {u : ∃v ∈ N0(d) s.t. uv ∈ E(d)} be the collection of nodes in the first hidden
layer. We assume that for every u ∈ N1(d), αu = 0 and all children of u are input nodes.
(d.) For every v ∈ N(d), |{u : uv ∈ E(d)}| ≤ C. Moreover, the number of layers is uniformly
bounded, namely, for any node u, any path from u to oG contains at most C edges.
The first two assumptions help to create spectral gaps between eigenspaces. When d is not large,
“finite-width" effect is no longer negligible and we expect that the spectra decay more smoothly.
Assumption (c.) says there is no “skip" connections from the input layer to other layers except to
the first hidden layer.
22
Under review as a conference paper at ICLR 2022
from neural_tangents import Stax
def MLP(Width=2048, depth=1, W_std=0.5, activation=stax.Rbf()):
layers =[]
for _ in range(depth):
layers += [stax.Dense(width, W_std=W_std), activation]
layers += [stax.Dense(1)]
return stax.serial(*layers)
def CNN(Width=512, ksize=4, depth=1, W_std=0.5, activation=stax.Rbf(), readout=stax.Flatten(),
act_after_readOUt=True):
layers =[]
conv_op = stax.Conv(width, (ksize, 1), StrideS=(ksize, 1), W_std=W_std, Padding='VALID')
for _ in range(depth):
layers += [conv_op, activation]
layers += [readout]
if act_after_readout:
layers += [stax.Dense(width * 4, W_std=W_std), activation]
layers += [stax.Dense(1)]
return stax.serial(*layers)
p = 4 # input shape: (-1, p**4, 1, 1)
S_MLP = MLP(depth=1) # Shallow MLP
D_MLP = MLP(depth=4) # Deep MLP
S_CNN = CNN(ksize=p, depth=1, act_after_readoUt=FaISe) # Shallow CNN
#	One layer CNN with an additional activation-dense layer
S_CNN_plus_Act = CNN(ksize=p, depth=1, act_after_readoUt=TrUe)
D_CNN = CNN(ksize=p**2, depth=2) # Deep CNN
HR_CNN = CNN(ksize=p, depth=4) # High-resolution CNN
#	High-resolution CNN with global average pooling readout
HR_CNN_GAP = CNN(ksize=p, depth=4, readout=stax.GlobalAvgPool(), act_after_readOUt=FaISe)
#	High-resolution CNN with flattening as readout
HR_CNN_FIatten = CNN(ksize=p, depth=4, act_after_readOUt=FaISe)
Listing 1: Definitions of MLPs, S-CNN, D-CNN and HR-CNN used in the experiments. The ar-
Chitectures used in the experiments ofFig. 3 are defined as follows. Dense04 = D_MLP, Conv(p2)02
=D_CNN, Conv(P)04 = HR_CNN. The one layer MLP and one layer CNN used in Fig. 7 are
S_MLP and S_CNN_plus_Act, resp.
H Proof of the Eigenspace Restructuring Theorem.
Lemma 1. Let r ∈ N0(d). Then
求 G (0),	Θg (0)=0
(42)
and for r 6= 0, for d large enough,
%Gr)(0),	ΘGr)(0)〜d-8⑺	and 愀Gr)k∞, kΘGr)k∞ . d-s(r)	(43)
23
Under review as a conference paper at ICLR 2022
Proof. Recall that the recursion formulas for 求 and Θ are
丸(t) = ΦU ≠ 丸(t)	(44)
∖J v∙.uv∈E	)
Θu (t) = φ u(∕	g^v(t))/	J%v(t) + Θv (t))	(45)
For convenience, define 孔 Θ,a as follows
洸 u(t) = /	g%v(t)	(46)
Θu(t) = /	gΘv(t)	(47)
,,	,	,	, ,	-T ,	、
状u(t) = φU ◦ %u(t).	(48)
Note that
求u(t) = ΦU ◦宠u(t) and Θ(t) = 3U(t)(^u(t)+Θu(t)).
The equalities %u(0) = Θu(0) = 0 follow easily from recursion and the fact ΦU(0) = 0 for all
u ∈ N(d).
We induct on the tuple (h, |r|), where h ≥ 0 is the number of hidden layers in G(d) and |r| is the
total degree of r. We begin with the proof of the NNGP kernel 次.
Base Case I: |r| = 1 and h ≥ 0 is any integer. Let u ∈ N0 be such that r = eu, where {eu }u∈N (d)
is the standard basis. Then
∂tu晚(t)=	E ɪɪ deg(v)-1φV ◦宠v(t)〜 E ∏ d-avφV ◦洸V(t)
path∈P(u→oG ) v∈path	path∈P(u→oG ) v∈path
(49)
Here P(u → u0) represents the set of paths from u to u0. By Assumption-G and Assumption-φ,
I ∕τ> /	∖ I •	• /'	1 1	1 1	1 i 4- /rʌ ∖	1'	11 1 • 1 1	ι	EI	r∙
|P(u → OG) | is uniformly bounded and φV (0) > 0 for all hidden nodes v. Therefore
∂tu 鲍(0)〜	X	d- Pv∈path αv φV ◦乳(0)
	path∈P(u→oG ) =	X	d-	Pv∈path	αv	φV	(0) path∈P(u→oG ) 〜	max	d-	Ev∈patħ	αv path∈P(u→oG ) =d-8(eu) = d-8(r)
In the above, We have used 沉V (0) = 0 (which is due to φv(0) = 0.) The second estimate
∣∣∂tu怎Gk∞ . d-8(r) follows from ∣φV ◦沈V(t)| . 1.
Base Case II: h = 0 and |r| ≥ 1 is any number. Note that G(d) has no hidden layer and all input
nodes are linked to the output node oG . The case when the activation φoG is the identity function is
obvious and we assume φoG is admissible.
∂f或G(t) = deg(θG)TrlφVg(1r1) (/Mtj .
This implies Eq. (43) since S(r) = 0, deg(oG) . 1 by Assumption-G and ΦVq(|r|)(0) > 0 by
Assumption-φ.
Induction: |r| ≥ 2, h ≥ 1 and r ∈ 朗(G(d)). We only prove the first estimate in Eq. (43) since the
other one can be proved similarly. WLOG, we assume φoG is not the identity function and hence is
24
Under review as a conference paper at ICLR 2022
semi-admissible. Let U ∈ N0(d) be such that r〃 ≥ 1 and denote r = r - e〃. Then
∂t晚(t)∣t=0 = ∂t(∂eu晚(t))[=0 = χ deg(oG)-1∂r (si；OG(t)∂eu孔⑻)[=0
oG v∈E
∂teusv 6=0
=XX deg(oG)T (∂f1 SOG (t) ∂r2+euSv(t))L=0
og v∈E r,ι +亍2=亍
∂teusv 6=0
E E deg(oG )-1d-KrI)d-8(Mr2+eu;V))
OG v∈E riι +r2=r
∂teusv 6=0
〜
Σ
Σ
d^8l(,r1')d~^a0G +8(Mr2+eu； V)))
Ogv∈E ri +r2=r
∂teusv 6=0
d~(8frι) + αoG +8(n∙(r2+eu; V)))
〜
sup sup
Og v∈E ri +r2=r
∂teusv 6=0
(50)
(51)
(52)
(53)
(54)
We have applied induction twice in Eq.(52): one to obtain the estimate ∂r1 或WG (0)〜d-8(r1) (with
|ri| < |r| and φ羡 semi-admissible) and one to ∂r2+euSV(t)〜d-8(Mr2+eu;V)),inwhiChtheSUb-
graph with v as the output node has depth at most (h - 1). The last line follows from that both the
cardinality of the tuple (ri, r2) with ri, r2 ≥ 0 and ri + r⅛ = r and the cardinality of V ∈ N(d
with oGv ∈ E and ∂teuSV 6= 0 are finite and independent of d. From the definition of MST, it is
clear that for all (ri,后)
8(r1) + αOG + 8(九(r⅛ + eu V)) ≥ 8(lrι) + 8(r⅛ + e〃)≥ 8(r)	(55)
It remains to show that there exists at least one pair (ri, r⅛) such that the above can be an equality.
Let T ⊆ GIdd be a MST containing all nodes in 九(r). If og has only one child V in T, then we
choose ri = 0 and notice that
8(ri) +	αOG	+ 8(n(r2	+	eu；	V)) =0 +	8(r2	+ e〃)=	8(r)	(56)
since 8(ri) = 0 and r⅛ + e〃 = r. Else, T contains at least two children and therefore at least two
disjoint branches split from og. Let Tu ⊆ T be the branch that contains U and choose r⅛ ≤ r be
such that all the nodes of (r⅛ + e〃) are contained in Tu and all the nodes of ri ≡ r - (r⅛ + e〃) are
contained in T\Tu . Clearly
8(r) = 8(ri) + 8(r⅛ + e〃)= 8(ri) + 8(九(r⅛ + eu v)) +。。0,	(57)
where V is the unique child of oG in Tu .
This completes the proof of the NNGP kernel S. As the proof of the NTK part is quite similar, we
will be brief and focus only on the induction step.
Induction Step of Θ: |r| ≥ 2, h ≥ 1 and r ∈ 朗(G(d)). Recall that the formula of Θ is
Θu(t) = φiWu	SV(t)	(SV(t) + ΘV(t))
For r ∈ N0(d),
∂r Θog (t)=	X ∂r1 φ Og ( /	SV(Q) ∂r2/	(SV (t) + θV (t))
r1+r2=r	∖J v:og v∈E	)	V v:og v∈E
Note that φOW is semi-admissible. We apply the result of S to conclude that
—♦
∂r1 Φ Og
— ♦
∂r1 Φ ：g
S	Sv (t)	I 〜d-8(r1)
VOG V∈E	t=0
J	Sv (t∕jll . d-8(r1)
V:OG V∈E	∞
(58)
(59)
(60)
(61)
25
Under review as a conference paper at ICLR 2022
and the inductive step to conclude that
∂12 /	(丸(t) + Θv(t))	〜 X	dTd) if 〒2 = 0 else 0	(62)
JVPG v∈E	t=0	v：oG v∈E
∂?③ V+Θv )≡0
∂[2/	(%v(t) + Θv(t)) . X	d-s(r2).	(63)
JvoG v∈E	v:oG v∈E
∂^t2 ⑶V +Θv )≡0
Note that
|{v :	OGV	∈	E(d)	and	d；2(求V	+	Θv)	≡	0}|	. 1.
Thus
k∂rΘog(t)k∞ .	X d-s(rι)-s(r2). d-s(r).	(64)
ri1+ri2=r
To control the lower bound, let T be a MST containing 九(r). If deg(oG; T) = 1, then we can choose
ri = 0 and r2 = r = 0. Notice that there is at least one child node V of og with ∂f2 (%v + Θv) ≡ 0.
Therefore
X	d-8(r2) & d-8(r2) = d-8(r)	(65)
v:dr2 (%v +Θv)≡0
Combining with
φOG (-f	沉v(0))= φ：G (0) > 0	(66)
v:oG v∈E
we have
∂tΘog (0) & d-8(r)
It remains to handle the deg(oG; T) > 1 case. We choose (ri, r2) such that one branch of T is the
MST that contains u(r2) and the og and the remaining branch(es) is a MST that contains 九(ri) and
the oG. Then
∂tΘog (0) &d；1 φO; ( /	丸(Gdt叶	(九(t) + Θv(t))
7 v:og v∈E	J	J v:oG v∈E	t=0
&d-8(rι)-8(r2)= d-8(r)
□
H. 1 Legendre Polynomials, Spherical Harmonics and their Tensor Products.
Our notation follows closely from (Frye & Efthimiou, 2012).
Legendre Polynomials. Let din ∈ NO and ωdin be the measure defined on the interval I = [-1, 1]
ωdin (t) = (1 - t2 )(din-3)/2
(67)
The Legendre polynomials6 {Pr(t) : r ∈ N} is an orthogonal basis for the Hilbert space L2(I, ωdin),
i.e.
I
Pr (t)Pr0 (t)ωdin (t)dt = 0
if r 6= r0
else
N (din, r)-i
(ISdin-In
USdiTu
(68)
Here Pr(t) is a degree r polynomials with Pr(1) = 1 that satisfies the formula below, N(din, r) is
the cardinality of degree r spherical harmonics in Rdin and |Sdin-i | is the measure ofSdin-i.
6More accurate, this should be called Gegenbauer Polynomials. However, we decide to stick to the termi-
nology in (Frye & Efthimiou, 2012)
26
Under review as a conference paper at ICLR 2022
Lemm	a 2 (Rodrigues Formula. Proposition 4.19 (Frye & Efthimiou, 2012)). Pr(t) = Crω-n1(t) (d) (1 - t2)r+(din-3)/2 ,	(69)
where	(-1)r cr = 2 (r + (din - 3)∕2)r	O)
In the	above lemma, (x)l denotes the falling factorial (x)ι ≡ x(x — 1)…(x — l + 1)	(71) (x)0 ≡ 1	(72)
Spher Sdin-1	ical Harmonics. Let dSdin-1 define the (un-normalized) uniform measure on the unit sphere . Then 2	2∏din∕2 ISdin-1∣≡	dSdin-1 = ^V .	C3) 7Sdin -1	U 五)
The n	ormalized measure on this sphere is defined to be dσdin = 7^1—1 dSdin-1 and ∣	dσ% = 1.	(74) ISdin-1 I	Sdin-1
The s orthon	pherical harmonics {Yr,l }r,l in Rdin are homogeneous harmonic polynomials that form an ormal basis in L2 (Sdin-1 , σdin ) Yr,l(ξ)Yr0,l0(ξ)dσdin = δ(r,l)=(r0,l0) .	(75)
Here	r,l denotes the l-th spherical harmonic whose degree is r, where r ∈ N, l ∈ [N (din, r)] and 2r + din - 2 din + r - 3	r N(din, r) =	r	I r 1 J 〜(din) /丫! as din -→ ∞ .	(76)
The L	egendre polynomials and spherical harmonics are related through the addition theorem.
Lemm	a 3 (Addition Theorem. Theorem 4.11 (Frye & Efthimiou, 2012)). Pr(ξτη)= Nj Q X	Yr,1(ξ)Yr,1 (η),	ξ,η ∈ Sdin-1.	(77) N(din,r) l∈[N(din,r)]
Tensor Products. LetP = ∣N0(d)∣, d = (du)∣u∈N(d)1，r ∈ 收"黑)| ' Np, 1|20d)| ` Ip = [-1,1]p
and ω = Nu∈N (d) ωdp be the product measure on Ip. Then the (product of) Legendre polynomials
Pr (t) =	Pru (tu) , t = (tu)u∈N (d) ∈ Ip ,	(78)
u∈N0(d)
which form an orthogonal basis for the Hilbert space L2(Ip, ω) = Nu∈N (d) L2(I, ωdu ). Similarly,
the tensor product of spherical harmonics
Yr,l = Y Yru,lu, l = (lu)u∈N(d) ∈ [N(d,r)]≡ Y [N(du,r)]	(79)
u∈N0(d)	u∈N0(d)
form an orthonormal basis for the product space
L2(X,σ)≡ O L2(Sdu-1,σdu)	(80)
u∈N0(d)
Elements in the set {Yr,l}l∈[N(d,r)] are called degree (order) r spherical harmonics in L2(X, σ)
and also degree r spherical harmonics if |r| = r ∈ N.
27
Under review as a conference paper at ICLR 2022
Theorem 5. We have the following, for K = %g(d)or K = Θg(d)
K(t) =	^X	K(r)Pr(t)	with	K(r)〜d-s(r)	if r = 0 else	0.	(81)
N(d)
r∈NN0
Note that Theorem 1 follows from this theorem and the addition theorem.
Proof of Theorem 1. Assume r 6= 0. Indeed, setting
ξ = (ξu)u∈N0d) ∈ X， η = (nU)u∈N0d) ∈ X and t = (tu)u∈N0d) = (ξTηu∕dU)u∈N0d)，
we have
Pr(t) = Y Pru (tU)= Y N(du, rU)I X	Yru ,lu (ξu/Pdu)Yru,lu (∏u/Pdu)
U∈N0(d)	U∈N0(d)	lu ∈N(du,ru)
(82)
=N(d, r)-1	X	Yr,l(ξ)Yr,l(η) .	(83)
l∈[N(d,r)]
Then Theorem 1 follows by noticing
K(r)N(d, r)-1 〜d-g(r)d- Pu∈N0d) ruαu = d,τ3	(84)
□
Proof of Theorem 5. From the orthogonality,
K(r) = hK, Pr i/kPr ∣∣L2(IP,3 )2	(85)
We begin with the denominator. Note that
kPrkL2(Ip,σ)2 = Y kPruk2L2(I,ωdu) = N(d; r)-1 Y (|Sdu-1|/|Sdu-2|)	(86)
U∈N0(d)	U∈N0(d)
By applying Lemma 2, integration by parts and continuity of K(r) on the boundary ∂Ip
hK, PriL2(IP,ω) = Cr /尸(垃() "。— 增"”—3)/2 dt
=(-1)rCr ^ K(r)(t) (1 — t2)r+(d-3)/2 dt
Ip
= (—1)rCr (M(K, d) + (K, d))
where K(r) is the r derivative of K, the coefficient Cr is given by Lemma 2
Cr =	Cru
U∈N0(d)
Y _________(T)ru_______
u∈Nθd 2ru(rυ + (d - 3)/2)r
〜 ∏ (-i)rud-ru = (-1)rd -
U∈N0(d)
(87)
(88)
(89)
(90)
and the major and error terms are given by
M(K, d) ≡K(r)(0) [ (1 - t2)r+(d-3)/2 dt = K⑺(0) Y	lS2ru+du-1l
Ip	d |S2ru+du-2 |
U∈N0(d)	u u
e(K, d) ≡ / (K(r)(t) -K(r)(0)) (1 — t2)r+(d-3)/2 dt
Ip
The mean value theorem gives
(91)
(92)
∣K⑺(t)-K⑺(0)∣ ≤ E kK(r+eu)kL∞(ip)|tu|	(93)
U∈N0(d)
28
Under review as a conference paper at ICLR 2022
and the error term |e(K, d)| is bounded above by
/ (1- t2)r+0-3”2
JIP
dt	E 胚(HeU)II-”)
u∈N(dd
~u∈∏d,七T UXj…JE
∕ιltul(1- tu)ru+(du-3)/2 dtu
´i (1 - tu)ru+(ru-3)/2 dtu
f lS2ru +du-1l λ
lS2ru +du-21
Since for any α ∈ N, as du → ∞,
jfα+du-4 = ∏2Γ((α + du - 1)∕2)∕Γ((α + d“)/2) ~ π2(du/2)-2 ~ (d“)-2 ,
lSα+du-2l
we have
le(K, d)l < X 胚(r+eu)∣∣L∞(iP)d-1 ∏ lS2ru+duT .
u∈N(d)	u∈N(d) Mu+Uu-21
We claim that (which will be proved later)
X kK(r+eu)kL∞(lP) < d-8(r)
u∈N(d)
which implies
K(r)(0) + O (d-8(r)( min du)-1])	∏	22^+4%-1]
,	∖	u∈N(d)	4 u∈NJS2ru+du-2l
Plugging back to Eq. (85), we have
K(r) = (-1)rcrN(d, r) ( K(r)(0) + O ( d-s(r)( min du)-2 ))
∖	∖	u∈Nd)	JJ
lS2ru+du-2l
(94)
(95)
(96)
(97)
(98)
(99)
∣Sdu-ι∣V1
(100)
(
π
∖u∈NOd)
'rU +du - 1 l
Since, for r fixed and as du → ∞ for all u ∈ N0(d)
—|T— → 1 and Nd4 → 1 and
(-1)r d-r	dr/r!
∏	lS2ru + du-1l ( ISdUTI ʌ 1^ → ]
UN6 Mu"21 (K J )
(101)
and thus
K(r) ~ r!-1
K(r)(0) + O ( d-8(r)( min du)-2 ))
,	∖	u∈Nd)	JJ
(102)
It remains to verify Eq. (98). By Lemma 1, we only need to show that
Σ
u∈N(dd
d-8(r+eU) < d-8(r)
(103)
We prove this by induction on the number of hidden layers of G⑷.The base case is obvious. Now
suppose the depth of G(d) is h. Let C(r) be the set of children of og that are ancestors of at least one
node of 九(r). We split NSd into two disjoint sets
Q(r) ≡ {u ∈	NS)	:	∃v	∈ C(r)	s.t. P(u →	v)	= 0} and	N0(d)∖Qo(r).
29
Under review as a conference paper at ICLR 2022
For U / Q(r), We have S(r + e〃)= S(r) + S(eu) and hence
E d-8(r+eu)= E d-8(r)-8(eu) = d—8(r) E d") . d—8(r).
u∈Q(r)	u∕Q(r)	u∈Q(r)
In the last inequality above, We have used
X dT(eu) ≤ X d-8(eu)〜1.
u∕Q(r)	u∈N0d)
(104)
(105)
To estimate the remaining, We use induction. Note that |C(r)| is finite and independent of d. Then
E	d-8(r+eu)	≤	d-α0G -8(n∙(r+eu;V))	(106)
u∈Q(r)	v∈C(r) u∈Q(r)	
	= d-αoG	X X d-8(Mr+eu;V)) v∈C(r) u∈Q(r)	(107)
	.∣C(r)∣d-αθG max d-S(n(r;v))〜d-S(r) V∈C(r)	(108)
We have used induction on the sub-graph with V as the output node.	□
I	Proof of Theorem 2
Let G(d) be a DAG associated to the convolutional networks whose filter sizes in the l-th layer is
kl = [dαl], for 0 ≤ l ≤ L+ 1, in which we treat the flatten-dense readout layer as a convolution with
filter size [dαL+1]. Note that we have set αp = α0 and αw = αL+1. We also assume an activation
layer after the flatten-dense layer, which does not essentially alter the topology of the DAG.
We need the following dimension counting lemma.
Lemma 4. Let r ∈ L(G(d)). Then
dim (SPan {Yr,ι :	L(r) = r, l / N(d, r)})〜dr	(109)
To prove Theorem 2, we only need to verify the assumptions of Theorem 4 in Mei et al. (2021a).
For convenience, we briefly recap the assumptions and results from Mei et al. (2021a) in Sec.L.
It is convenient to group the eigenspaces together according to the learning indices L(G(d)). Recall
that L(G (d)) = (r1 ≤ r2 ≤ r3 . . . ). Let
Ei = span{Yr,ι : L(r) = ri}	(110)
Then by Theorem 1 and Lemma 4,
dim(Ei)〜dri and λ(g)〜d-r ∀g / Ei, g = 0,	(111)
where λ(g) denote the eigenvalue of g. We proceed to verify Assumptions 4 and 5 in Sec. L.
They follow directly from Theorem 1, Lemma 4 and the hypercontractivity of spherical harmonics
Beckner (1992).
I.1	VERIFYING Assumption 4
We need the following.
Proposition 1. For 0 < s / R ,let Ds = span {Y r,ι : |L(r)| < s}. Thenfor f / Ds,
kfk2 ≤ (q- I)"。0 kfk2	(112)
Proof of Proposition 1. The lemma follows from the tensorization of hypercontractivity. Let f =
Pk≥0 Yk ∈ L2 (Sn) where Yk is a degree k spherical harmonics in Sn. Define the Poisson semi-
group operator
Pf(x) = XkYk(x)	(113)
k≥0
30
Under review as a conference paper at ICLR 2022
Then We have the hypercontractivity inequality (Beckner, 1992), for 1 ≤ P ≤ q and E ≤ q- p-ɪ
kPfkLq(Sn) ≤ kfkLp(Sn)	(114)
One can then tensorize (Beckner, 1975) it to obtain the same bound in the tensor space.
Lemma 5 (Corollary 11 Montanaro (2012)). Let f : (Sn)k → R .If 1 ≤ P ≤ q and C ≤ Jp-I,
then
kP?kf kLq((Sn)k) ≤ kf kLP((Sn)k) .	(115)
Let f = Pr ι ar,ι Yr,ι ∈ Ds. Choosing C = ʌ q-ι and P = 2 in the above lemma, we have
kfk2 = k Σ>r,lYr,lk2 r,l	(116)
=kp"d'lχ ar,lC-rYr,ιk2 r,l	(117)
≤k X ar,lE-rYr,lk2 r,l	(118)
=X ar,ιC-2rkY r,lk2 r,l	(119)
≤ C-2maxlrlX ar,ιkYr,ιk2 r,l	(120)
= (q- 1)maxlrlkfk2 ≤ (q- 1)s∕ɑ0 kfk2	(121) □
Since r ∈ S(G(d)), there is a j such that r7- < r < rj+ι. Let n(d) = dr and
m(d) = dim (span{Yr,ι : X(r) ≤ r7∙}) = dim(span [ Ei)	(122)
i≤j
Clearly, m(d)〜 drj. We list all eigenvalues of K in non-ascending order as {λd,i}. In particular,
we have
λd,m(d)〜d-rj > d-r > d-rj + 1 〜λd,m(d) + ι .	(123)
Assumption 4 (a). We choose u(d) to be
u(d) = dim span [	Ei	.	(124)
∖	i3≤2r+100 )
Assumption 4 (a) follows from Proposition 1.
Assumptions 4 (b). Let S = inf{r ∈ 以G(d)) : r > 2r + 100}. For l > 1, we have
X λld,j 〜 X(d-ri)l dim(Ei) 〜 d-s(l-1)	(125)
j=u(d)+1	r〃∙ri≥s
which also holds for l = 1 since
X	λd,j 〜 1	(126)
j=u(d)+1
31
Under review as a conference paper at ICLR 2022
Thus
(Pj=U(d) + 1 λdj)
j=u(d)+1 λ2dl,j
as long as δ < 100/r.
-2s(l-1)
〜—=ds>d2r+100 >n(d)2+δ~ d(2+δ)r.
Assumption 4 (c). Denote
Kd,>m(d)(ξ, η)=	X	λκ(r)Yr,l(ξ)Yr,l(η)
r,l:L(r)>r
We have
Eη Kd,>m(d)(ξ, η)2 =En	X	λκ(r)2∣Yr,ι(ξ)Yr,ι(η)∣2
r,l:L(r)>r
=X	λκ(r)2∣Y r,l(ξ)∣2
r,l:L(r)>r
=XλK(r) 2 X |Yr,l(ξ)∣2
r:L(r)>r	l
= X λK(r)2N(d,r)Pr(1)= X λK(r)2N(d,r ).
r:L(r)>r	r:L(r)>r
Similarly,
Eξ,n	X	XK(r)2|Yr,l(ξ)Yr,l(n)|2= X λK(r)2N(d, r) ∙
r,l:L(r)>r	r:L(r)>r
Thus
En	X	λκ(r)2∣Y r,l(ξ)Y r,l(η)∣2 - Eξ,n	X	λκ(r)2∣Y r,l(ξ)Y r,l(η)∣2 =0 .
r,l:L(r)>r	r,l:L(r)>r
(127)
(128)
(129)
(130)
(131)
(132)
(133)
(134)
For the diagonal terms,
Kd,>m(d)(ξ, E)=	E	λK (T)IY r,l(ξX2 = E λK⑺ N (d, T)= Eξ Kd,>m(d)(ξ, E)
r,l:L(r)>r	r:L(r)>r
(135)
which is deterministic.
I.2 VERIFYING Assumption 5.
Recall that n(d)〜dr and rj < r < rj+ι. Assumption 5(a) follows from Eq. (111). Indeed, for
l>1
d,m(d)+1	X	λd,k 〜(d-rj+1)-1	X dim(Ei)d-lri	(136)
k=m(d) + 1	i:ri≥rj + i
〜dlrj+1 X dri d-lri	(137)
i:ri≥rj+1
=drj+1 > n(d)1+δ	(138)
for some δ > 0 since r < rj+1. Similarly, for l = 1, since
X λd,k 〜1	(139)
k=m(d)+1
we have
(d-rj+1 )-1	X	λd,k 〜drj+1 > n(d)1+δ .	(140)
k=m(d)+1
Assumption 5(b) follows from r > rj .
Assumption 5(c). Note that
λ-,m(d)	X	λd,k~ λ-,m(d)〜drj<n(d)(i- dr(1-δ)	(141)
k=m(d)+1
for some δ > 0 since rj < r.
32
Under review as a conference paper at ICLR 2022
J Proof of Lemma 4
Proof. The lemma can be proved by induction. Base case: L = 0. The network is a S-CNN.
WLOG, assume α0 6= 0 and α1 6= 0. For r ∈ L(G(d) ), we know that r can be written as a
combination of α0 and α1, i.e. r = k0α0 + k1α1 for some k0, k1 ≥ 0. We say a tuple (k0, k1) is
r-feasible if in addition, there exists r with S(r) = kιαι and 方(r) = k°αo. Consider the set of all
r-feasible tuple
F(r) ≡ {(k0, k1) : r-feasible} .	(142)
Clearly, F(r) is finite. It suffices to prove that for each r-feasible tuple (k0, k1),
dim (span {Yr,ι :巩r) = k°ao &(r) = k1α1, l ∈ N(d, r)})〜dr	(143)
Note that there are 〜 (dɑ1 )k1 many ways to choose kι nodes in the penultimate layer. Then the
dimension of the above set is about
k1	k1
(dα1)k1	X	YN(dɑ0,ko,j)〜(dɑ1 )k1	X	Y(dɑ0)k0，j
(k0,1 ,...,k0,k1 )	j=1	(k0,1 ,...,k0,k1 ) j=1
k0,1+	+k0,k] = k0	k0,1+	+k0,kι =k0
〜dk0α0+k1α1 = dr,
since the cardinality of the set
{(k0,1, ..., k0,k1 ) : k0,1 + …+ k0,kι = k0, k0,j ≥ 1	k0,j ∈ N}
(144)
(145)
is finite.
Induction step: L ≥ 1. For r with L(r) = r, let k be the number of children of a MST of
九(r; Og). Clearly, k ∈ [1, [r∕ɑL+ι]]. Then We can classify Yr,ι into at most [r∕aL+ι] bins:
{Ωk}k=ι,...,[r∕ɑL+ι] depending on the number of children of og in a MST. Let Ωk be non-empty.
We only need to prove the number of Yr,ι in Ωk is dr. Note that there are 〜(dαL+1 )k many ways
to choose k children from oG. Let {uj}j=1,...,k be one fixed choice and {Gj} be the subgraphs with
{uj} as the output nodes. Next, we partition (r - kαL+1) into k components,
r - kaL+ι = ri +-----+ r%
so that each rj is a combination of{αj}0≤j≤L. The cardinality of such partition is also finite. We fix
one of such partition (r1, . . . , rk) so that each rj is a learning index ofGj. We can apply induction to
each (Gj,r7∙) to conclude that the cardinality of Y叫,/ with LGj (r) = r7∙ is 〜drj, where LGj (r)
is the learning index of rj of Gj . Therefore, we have
dim (span {Yr,ι :	L(r) = r, l ∈ N(d, r)})〜(dαL+1 )k Y dr = dr .	(146)
j∈[k]
□
K CNN-GAP: CNNs with Global Average Pooling
Consider convolutional networks whose readout layer is a global average pooling (GAP) and a
flattening layer (namely, without pooling), resp.
CNN + GAP:	[Conv(p)-Act] → [Conv(k)-Act]0L → [GAP] → [Dense]	(147)
CNN:	[Conv(p)-Act] → [Conv(k)-Act]0L → [Flatten] → [Dense]	(148)
Concretely, the input space is X = (Sp-I)k° ×w ⊆ Rp×1×k° ×w, where P is the patch size of the
input convolutional layer, k is the filter size in hidden layers, L is the number of hidden convolution
layers and w is the spatial dimension of the penultimate layer. The total dimension of the input
is d = p ∙ kL ∙ w, and the number of input nodes is |N0| = kL ∙ w. Since the stride is equal to
the filter size for all convolutional layers, the spatial dimension is reduced by a factor of p in the
first layer, a factor of k by each hidden layer. The penultimate layer (before pooling/flattening)
33
Under review as a conference paper at ICLR 2022
has spatial dimension w and is reduced to 1 by the GAP-dense layer or the Flatten-dense layer.
The DAGs associated to these two architectures are identical which is denoted by G . However,
the kernel/neural network computations are slightly different. If the penultimate layer has n many
channels and fpen : X → Rn×w is the mapping from the input layer to the penultimate layer, then
the outputs of the CNN-GAP and CNN-Flatten are
fCNN-GAP(x) = n 2〉: ωj I	fpen (x)j,i
j∈[n]	i∈[w]
fCNN-Flatten(x) = (nw) 2 X :	ωjifpen (x)j,i ,
j∈[n],i∈[w]
(149)
(150)
resp., where wj and wji are parameters of the last layer and are usually initialized with standard iid
Guassian Wj, Wji 〜 N (0,1). Let N-1 ⊆ N be the nodes in the penultimate layer, then |N-1| = w.
Let ξ = (ξv)v∈N-ι ∈ X, where ξv ∈ (Sp-ι)kL. Thus, each ξv contains kL many input nodes
{ξu,i}i∈[kL] . Define
tuv = (hξu,i, ηv,ii∕p)i∈[kL] ∈ [-1, i]kL.	(151)
Recall that in the case without pooling
求u(t) = φ*(/ g%v(t)),双G =/	g%v(t) = / Z 孔(t)	(152)
where t ∈ [-1,1]kL×w, which is usually obtained by t = (hξu,i, ηu,ii /p)u∈N--ιi∈[kL]. Indeed, for
each V ∈ N-ι,国V depends only on the diagonal terms t^ = (hξv,i, ηv,ii/p)i∈[kL] ∈ [-1,1]kL.
We can find a function
IXpen ： [-1, 1]kL → [-1, 1] s.t.孔(t) =沉 pen(tvv) ∀V ∈ N-1	(153)
Therefore, without pooling the NNGP kernel is
%CNN^(t) = 7	%pen(tvv) = — X : %pen(tvv)
v∈N-1	W v∈N-1
(154)
Note that the kernel does not depend on any off-diagonal terms tuv with u 6= v because there isn’t
weight-sharing in the last layer. Let dpen = (p, p, . . . ,p) ∈ NkL . Then kdpenk1 = pkL is the
effective dimension of the input to any node u ∈ N-1. Assume k = dαk, p = dαp and W = dαw
and αk, αp, αw > 0. Applying Theorem 1 to Xpen, we have
XCNN(t) = X W λXρen (r) X X	Yr,l(ξv), Y r,l(ηv)	(155)
r∈NkL	v∈N-1 l∈N(dpen,r)
Clearly, the eigenfunctions are {Yr,ι(ξv)}r,ι,v and the corresponding eigenvalues are
{ W λXpen (r)}r,l,V .Notethat
Wλχpen (r) = λχG (r) 〜d-2(r)	(156)
Here and in what follows, we also consider r ∈ NkL as an element in NwkL
When the readout layer is a GAP, the weights of the penultimate layer are shared across different
spatial locations, namely, all nodes in N-1 use the same weight. As such, the kernel correspond-
ing to the CNN-GAP depends on both the diagonal and off-diagonal terms t = (tuv)u,v∈N-1 ∈
[-1, 1]kL ×kL, which can be written as (Novak et al., 2019b)
XCNN-GAP(t) =
u,v∈N-1
Xpen (tuv )
5^	/ ,	Xpen (tuv)
W2
u,v∈N-1
(157)
34
Under review as a conference paper at ICLR 2022
Applying Theorem 1 to 求pen, We have
%CNN-GAP(t)二	=W X λ 武 Pen (r)	X	X	Y r,l(ξu), Y r,l(ηv)	(158) r∈NkL	u,v∈N-1 l∈N(dPen,r) =X ɪλ*en (r)	X	(W-2 ^	Yr,l(ξu) ) ( W-1 ^	Yr,l(ηu) r∈NkL w	l∈N(dPen,r)	u∈N-1	u∈N-1 (159) =X W λ%en (r)	X	Y rym(ξ)Yrym(η)	(160) r∈NkL	l∈N(dPen,r)
where
Yrym(ξ) ≡ W-1/ N Yr,ι(ξu) , r ∈ NkLand l ∈ N", r)	(⑹)
That is the eigenfunctions and eigenvalues of %cnn-gap are {Y：ym(ξ)}r,ι and { 1 λ求Pen (r)}r,ι resp.
In sum, the eigenvalues of %cnn and %cnn-gap are the same (UP to the multiplicity factor w). Each
eigenspace of %cnn-gap is given by symmetric polynomials of the form Eq. (161). We can see that
the GAP reduces the dimension of each eigenspace by a factor of w. Same arguments can also be
applied to the NTKs
θCNN(Jt) = /	(a Pen (tvv) + θpen (tvv))	(162)
=X (1 λ*en (r) + ɪ λθpen (r^ X X	Y r,l (ξv ), Yr,l(7)
r∈NkL	v∈N-1 ι∈N(dpen,r)
(163)
θ CNN-GAP(t) = /	(%pen(tuv) + θpen(tuv))	(164)
=X (1" (r) + ɪλθpen (r)) X Yryιm(ξ), Yrym(η)	(165)
r∈NkL	v∈N-1
Where Θpen is the NTK of the penultimate layer Which is the same for all nodes in N-1. Since
W1 λΘpen 〜d-2(r),We have
工 λ 武Pen (r) + 1 λθpen (了)〜(166)
w pen	w pen
K. 1 Generalization bound of CNN-GAP
We shoW that GAP improves the data efficiency of D-CNNs by a factor ofw 〜 dαw under a stronger
assumptions on activations φ.
Assumption Poly-φ: There is a sufficiently large J ∈ N such that for all hiddens nodes u
ΦU(j)(O) =0 for 1 ≤ j ≤ J and φU(j)(0) = 0 otherwise	(167)
This assumption implies that there are 0 < J1 < J2 ∈ R such that for all 0 6= r With |r| < J1
dr	dr
瓦 %pen(0)=0 and 及 Θpen(0)=0	(168)
and for all r With |r | > J2
dr	dr
dt沉Pen ≡ 0 and dtθpen ≡ 0	(169)
Moreover, J1 → ∞ as J → ∞.
35
Under review as a conference paper at ICLR 2022
Let LSpym(X) ≤ Lp(X) be the close subspace spanned by symmetric eigenfunctions Eq. (161). Let
KSym = %CNN-GAP Or Θcnn-Gap.
For X ⊆ X and r ∈/ L(G(d)), define the regressor and the projection operator to be
RSXym(f)(x) = KSym(x, X)KSym(X, X)-1f(X)
p>ym(f)=	x χ	hf,γryimiL2(x)yrym.
r:L(r)>r l∈N(dpen,r)
Theorem 6. Let G = {G (d) }d, where each G(d) is a DAG associated to the D-CNN in Eq. (147)
with αk, αp, αw > 0. Let r ∈/ L(G (d)) be fixed and the activations satisfy Assumption Poly-φ for
J = J(r) sufficiently large. Let f ∈ LS2ym(X) with Eσf = 0. Then for > 0,
RSXym(f) - f2LS2ym(X) - P>Syrm(f)2LS2ym(X) = cd,kfk2LS2y+m(X),	(170)
where cd,e → 0 in probability as d → ∞ over X 〜σ[dr aw
Proof of Theorem 6. We need the following dimension counting lemma, which follows directly
from Lemma 4.
Lemma 6. Let r ∈ L(G(d)). Then
dim (span {Y?； :	L(r) = r, l ∈ N(d, r)})〜dr-αw	(171)
Recall that X(G(d)) = {r7-} in non-descending order. Similarly, let
EiSyTn = span{Y：ym : Wr) = ri}	(172)
From the above lemma, we have
dim(ESym)〜dri-aw	(173)
Since r ∈ S(G(d)), there is a j such that r7- < r < rj+ι. Let n(d) = dr-αw and
m(d) = dim (span{Y，：：对(r) ≤ r}) = dim(span [ ESym)	(174)
i≤j
Clearly, m(d) 〜 drj-αw . We list all eigenvalues of KSym in non-ascending order as {λd,i}. In
particular, we have
λd,m(d) 〜 d-rj > d-r > d-rj+1 〜 λd,m(d)+1 .	(175)
We proceed to verify Assumptions 4 and 5 in Sec. L.
Assumptions 4 (a) We choose u(d) to be
u(d) = dim span [	EiSym	.	(176)
∖	iιri≤2r+100	J
Let S = inf{r ∈ X(G(d)) : r > 2r + 100}. Assumption 4 (a) follows from Proposition 1.
Assumptions 4 (b) For l > 1, we have
X	λld,j 〜 X (d-ri)ldim(EiSym) 〜 d-s(l-1)-αw	(177)
j=u(d)+1	ri:ri ≥s
which also holds for l = 1 since
dαw	X	λd,j 〜 1	(178)
j=u(d)+1
36
Under review as a conference paper at ICLR 2022
Thus
(Pj=U(d)+1 λd,j)
j=u(d)+1 λd,j
-2s(l-1)-2αw
〜d-s⑵-Iiw = ds-αw > d2r+100-αw > n(d产δ 〜d(2+δ)(r-αw)
(179)
Assumption 4 (c) This requires some work and is verified in Sec. K.2.
Assumption 5 (a) Since m(d) = dim(span Si≤j Ei )and rj+1 > r > rj, we have
λ7⅛	X	λd,j~ (jl X(d-ri )ldim(ESym)
d,m(d)+1 j≥m(d)+1	i>j
〜dim(Ej+m) = drj+1-aw
> n(d)1+δ = d(r-αw )(1+δ)
as long as δ < (rj +1 - αw )/(r - αw )- 1.
Assumption 5 (b) This is obvious since m(d)〜drj-αw, n(d)〜dr-aw and r > rj-.
Assumption 5 (c) This follows from rj < r. Indeed,
Λd⅛	X λdj 〜(⅛ X(Lr)dim(ESym)〜drj-ɑw
d,m(d) j≥m(d)+1	i>j
≤ n(d)1-δ = d(r-αw )(1-δ)
as long as 0 < δ < 1 - (rj∙ - αw)/(r - αw).
K. 2 Verification of Assumptions 4(c).
We begin with proving Eq. (227). Let Xi define the random variable
Xi ≡ Eξ〜σdKsym,>m(d)(ξi, ξ)2 and ∆i ≡ Xi - EXi
We need to show that
supi∈[n(d)] 1δ∕ in prob. 0
→ .
EXi	d→∞
By Markov’s inequality, it suffices to show that
(EXi)TE SUp ∣∆i∣ ----------› 0
i∈[n(d)]	d→∞
By orthogonality and treating r ∈ NkL as an element of NwkL, we have
EXi= Eξ,.σdKSym,>m(d)(ξ,ξ)2
2
=Eξ,“d	x KSym(r)	x	γrym(ξ)γrym(ξ)
r3(r)>r	l∈N (dpen,r)
=Eξ,“d	x KSym(r)x	IYrym(ξ)γrym(ξ)∣2
r3(r)>r	l∈N (dpen,r)
=x κSym(r)	x	Eξ,~d∣γrym(ξ)γrym(ξ)∣2
r3(r)>r	l∈N (dpen,r)
=	x KbS2ym(r)	x	1
r3(r)>r	l∈N (dpen,r)
= x KbS2ym(r)|N (dpen, r)|
r:织(r)>r
(180)
(181)
(182)
(183)
(184)
□
(185)
(186)
(187)
(188)
(189)
(190)
(191)
(192)
(193)
37
Under review as a conference paper at ICLR 2022
and
Xi= Eξ〜σdKsym,>m(d)(ξi, ξ)2	(194)
2
=Eξ 〜σd	X KSym (r)	X	Y 看(专产看®	(195)
r3(r)>r	l∈N (dpen,r)
=Eξ〜σd	X KSym(r)	X	IYrym(ξi)γrym(ξ)∣2	(怫)
r3(r)>r	l∈N (dpen,r)
=X KSym(r)	X	Eξ 〜σd∣Y	£(£)『	(197)
r3(r)>r	l∈N (dpen,r)
=X KSym(r)	X	Yrym(ξi)∣2	(198)
r3(r)>r	l∈N (dpen,r)
=r X,KSym(r[NX ,(； X Yr,l (ξi,u)2 + W)： Y r,l(ξi,u)Y r,l(ξi,v )^ (199)
=Eξ,ξ^ 〜σd KSym,>m(d)(ξ,ξ)2 + X K2ym(r)	X ( 1 XYr,l (ξi,u)Yr,l(ξi,v ) 1
r3(r)>r	l∈N(dpen ,r) ∖ u=v	)
(200)
Let
Xr,i=	X I J X Yr,l(ξi,u)Y r,l(ξi,v )1	(201)
l∈N(dpen,r)	u6=v
then
∆i =	X KbS2ym(r)Xr,i	(202)
r3(r)>r
We replace the maximal function by the lq -norm for q ≥ 1,
E SUp ∣∆i∣≤ E( X ∆i∣q)1 ≤ (E X ∆i∣q)1 = n(d)1 (E∣∆i∣q)1	(203)
i∈[n(d)]	i∈[n(d)]	i∈[n(d)]
where the first three expectations are taken over ξι,..., ξn(d)〜 σd and the last one is taken over
ξi 〜σd. Then We replace the Lq-norm by the L2-norm via Hypercontractivity, in which We used
Assumption Poly-φ which implies that ∆i is a polynomial of bounded degree
E SUp ∣∆i| ≤ n(d) 1 (E∣∆i∣q)1 ≤ Cqn(d) 1 (E∣∆i∣2)2 = Cqn(d消(E△2)2	(204)
i∈[n(d)]
We expand the L2-norm and use orthogonality to “erase" the off-diagonal terms twice: first for
r = r
Eξi 〜σd Xr,iXr,i = 0
(205)
and second for l 6= l0 or u 6= v
E…X葭=WE…匕NX JX Yr，l(ξi,u)Yr,l(ξi,v )1XI X
Yr,l0 (ξi,u0)Yr,l0 (ξi,v0)
pen	(206)
W22 Eξi 〜σd X (X Y r,l(ξi,u)2Y r,l(ξi,v )2) = ^22 X W(W - 1
l∈N (dpen,r) u6=v	l∈N(dpen,r)
(207)
2(W - 1) X 1= 2(W - 1) |N(dpen, r)| ≤ 2|N(dpen, r)|	(208)
WW
l
38
Under review as a conference paper at ICLR 2022
Combining this estimate with Eq. (193), Eq. (202) and Eq. (204) yields
(EXi)T(E SUp ∣∆i∣) ≤ Cqn(d) 1
i∈[n(d)]
(2 Pr3(r)>r KgymENaen，埒)”
Pr3(r)>r KSymmN5, 以
Pr3(r)>r K2ym(r)INaen，r)¥
PPr:S(r)>r K2ym(r)|N(dpen, r)|
√2Cq n(d)ι*>r ¾mr(∖r;
L Fr	. _ ,, , ,	、I 1
~ 2Cqdq SUp ∣N(dpen, r)|-2
r:织(r)>r
〜dr sup d-lrlap
r3(r)>r
---→ 0
d→∞
by choosing q (independent of d) sufficiently large.
The proof of Eq. (228) is similar. Let Xi denote the random variable
Xi ≡ KSym,>m(d) (ξi, ξi) and ∆i ≡ Xi - EXi
and it suffices to prove
E supi∈[n(d)] |A|  0
EXi	d→∞
We have
EXi = Eξi~σdKsym,>m(d)(ξi, ξi)
=E&~bd X Ksym(r)	X	Y浮怎乃浮(&)
r3(r)>r	l∈N (dpen,r)
= X Kbsym(r)|N (dpen, r)|
r:织(r)>r
and
∆i = Xi - EXi = w-1	X Kbsym(r)	X	X Yr,l(ξi,U)Y r,l(ξi,v ), .
r:2(r)>r	l∈N(dpen,r) u=v
(209)
(210)
(211)
(212)
(213)
(214)
(215)
(216)
(217)
(218)
(219)
The remaining steps (replacing the maximal function by the lq -norm, and then the Lq -norm by the
L2-norm using hypercontractivity) are similar to that of the proof of Eq. (227), which are omitted
here.
L Kernel Concentration, Hypercontractivity and
Generalization of Mei et al. (202 1 a)
For convenience, we briefly recap the analytical results regarding generalization bounds of kernel
machines from Mei et al. (2021a) sec 3.
Let (Xd, σd) be a probability space and 求d be a compact self-adjoint positive definite operator
from L2(Xd, σd) → L2(Xd, σd). We assume 求d ∈ L2(Xd × Xd). Let {ψd,j} and {λd,j} be the
eigenfunctions and eigenvalues associated to 沈d, i.e.
d,j(x) ≡
y∈Xd
^d(x, y)ψd,j (y)σd(y) = λd,jψd,j (X).
(220)
洸dψ
≤
≤
We assume the eigenvalues are in non-ascending order, i.e. λd,j+1 ≥ λd,j ≥ 0. Note that
ΣS λd,j = k 洸dkL2(Xd×Xd) < ∞.
j
(221)
39
Under review as a conference paper at ICLR 2022
The associated reproducing kernel Hilbert space (RKHS) is defined to be functions f ∈ L2 (Xd, σd)
-1
with k洸d 2 f ∣∣L2(Xd,σd) < ∞. Given a finite training set X ⊆ Xd and observed labels f (X) ∈
R|X | , the regressor is an extension operator defined to be
我Xf (x) = %d(x, X)究d(X, X)-1f (X).	(222)
Intuitively, when “X → Xd " in some sense, we expect the following
我 X f (x)=究 d(x, X)究 d(X, X )-1f(X) → 我 Xd f (x)=究 d(究)1f )(x) = f (x),	(223)
namely,"沈 X → Iχd "in some sense.
Using tools from the non-asymptotic analysis of random matrices (Vershynin, 2010), the work Mei
et al. (2021a) provides a very nice answer to the above question in terms of the decay property
of the eigenvalues {λd,j } and the hypercontractivity property of the eigenfunctions {ψd,j }. They
show that 沈X is essentially a projection operator onto the low eigenspace under certain regularity
assumptions on the operator 京d. These assumptions are stated via the relationship between the
number of (training) samples n = n(d), the tail behavior of the eigenvalues with index ≥ m = m(d)
and the tail behavior of the operator 羽
究d,>m(d)(x,X) ≡ E λjψj(x)ψj(x)	(224)
j>m(d)
as the "input dimension" d becomes sufficiently large.
Assumption 4. We say that the the sequence of operator {洸d}d≥ι satiesfies the Kernel Concen-
tration Property (KCP) with respect to the sequence {n(d), m(d)}d≥1 if there exsts a sequence of
integers {u(d)}d≥1 with u(d) ≥ m(d) such that the following holds
(a)	(Hypercontractivity.) Let Du(d) = span{ψj : 1 ≤ j ≤ u(d)}. Then for any fixed q ≥ 1,
and C = C(q) such that for f ∈ Du(d)
kfkLq(Xd,σd) ≤ CkfkL2(Xd,σd)	(225)
(b)	(Eigen-decay.) There exists δ > 0, such that, for all d large enough, for l = 1 and 2,
n(d)2+δ ≤ (p∙≥u(d)+1 λd,j)2
j≥u(d)+1 λd,j
(c)	(Concentration of Diagonals.) For {χi}i∈[n(d)]〜 σdn(d), we have:
SuPi∈[n(d)] lEx~σd洸d,>m(d) (xi,x)2 - Ex,x~σd洸d,>m(d) (x,x)? |
Exhbd 时,>m(d)(x,X)2
SuPi∈ [n(d)] I洸d,>m(d) (Xi, Xi) - Ex~σd洸d,>m(d) (x, X)I
in Prob.
----→ 0
d→∞
Ex 〜σd 洸 d,>m(d)(x,x)
where cd → 0 in probability as d → ∞.
Assumption 5. Let 求d and {m(d),n(d)}d≥ι be the same as above.
(a)	For l = 1 and 2, there exists δ > 0 such that
n(d)1+δ ≤ λT^	X λd,k
d,m(d)+1 k=λm(d)+1
(b)	There exists δ > 0 such that
m(d) ≤ n(d)1-δ
(c)	(Spectral Gap.) There exists δ > 0 such that
n(d)1-δ ≥ vɪ	X	λd,k
d,m(d) k≥m(d)+1
in Prob.
-----→ 0
d→∞
(226)
(227)
(228)
(229)
(230)
(231)
40
Under review as a conference paper at ICLR 2022
Let 沙>k (similarly for 队,沙≤k, etc.) denote the projection operator
沙 >kf = Xhf,ψj iψj	(232)
j>k
Theorem 7 (Mei et al. (2021a)). Assume 洸d satisfy Assumptions 4 and 5. Let {fd}d≥ι be a
Sequence offunCtionS and let X 〜b；(d. Thenfor every e > 0,
∣∣^X (fd) - fdkL2(Xd,σd) = k沙 >m(d)fdkL2(Xd,σd) + cd,e k fd k L2+≡ (Xd,σd)	(233)
where cd, → 0 in probability as d → ∞.
The theorem says,沈X is essentially the projection operator 沙≤m(d) in the sense that when restricted
to L2+(Xd, σd),
我 X =沙 ≤m(d) + Errors	(234)
41