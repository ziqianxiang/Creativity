Under review as a conference paper at ICLR 2022
ANCER: Anisotropic Certification via Sample-
wise Volume Maximization
Anonymous authors
Paper under double-blind review
Ab stract
Randomized smoothing has recently emerged as an effective tool that enables
certification of deep neural network classifiers at scale. All prior art on randomized
smoothing has focused on isotropic `p certification, which has the advantage of
yielding certificates that can be easily compared among isotropic methods via
`p-norm radius. However, isotropic certification limits the region that can be
certified around an input to worst-case adversaries, i.e. it cannot reason about
other “close”, potentially large, constant prediction safe regions. To alleviate this
issue, (i) we theoretically extend the isotropic randomized smoothing `1 and `2
certificates to their generalized anisotropic counterparts following a simplified
analysis. Moreover, (ii) we propose evaluation metrics allowing for the comparison
of general certificates - a certificate is superior to another if it certifies a superset
region - with the quantification of each certificate through the volume of the
certified region. We introduce AnCer, a framework for obtaining anisotropic
certificates for a given test set sample via volume maximization. We achieve it
by generalizing memory-based certification of data-dependent classifiers. Our
empirical results demonstrate that ANCER achieves state-of-the-art `1 and `2
certified accuracy on CIFAR-10 and ImageNet in the data-dependence setting,
while certifying larger regions in terms of volume, highlighting the benefits of
moving away from isotropic analysis.
1 Introduction
The well-studied fact that Deep Neural Networks (DNNs) are vulnerable to additive imperceptible
noise perturbations has led to a growing interest in developing robust classifiers (Goodfellow et al.,
2015; Szegedy et al., 2014). A recent promising approach to achieve state-of-the-art provable
robustness (i.e. a theoretical bound on the output around every input) at the scale of ImageNet (Deng
et al., 2009) is randomized smoothing (Lecuyer et al., 2019; Cohen et al., 2019). Given an input
X and a network f, randomized smoothing constructs g(x) = Ee〜D[f (X + e)] such that g(x)=
g(x + δ) ∀δ ∈ R, where the certification region R is characterized by x, f, and the smoothing
distribution D. For instance, Cohen et al. (2019) showed that if D = N(0, σ2I), then R is an '2-ball
whose radius is determined by X, f and σ. Since then, there has been significant progress towards the
design of D leading to the largest R for all inputs X. The interplay between R characterized by `1, `2
and '∞-balls, and a notion of optimal distribution D has been previously studied Yang et al. (2020).
Despite this progress, current randomized smoothing approaches provide certification regions that are
isotropic in nature, limiting their capacity to certifying smaller and worst-case regions. We provide
an intuitive example of this behavior in Figure 1. The isotropic nature of R in prior art is due to the
common assumption that the smoothing distribution D is identically distributed (Yang et al., 2020;
Kumar et al., 2020; Levine & Feizi, 2021). Moreover, comparisons between various randomized
smoothing approaches were limited to methods that produce the same `p certificate, with no clear
metrics for comparing with other certificates. In this paper, we address both concerns and present
new state-of-the-art certified accuracy results on both CIFAR-10 and ImageNet datasets.
Our contributions are threefold. (i) We provide a general and simpler analysis compared to prior
art (Cohen et al., 2019; Yang et al., 2020) that paves the way for the certification of anisotropic
regions characterized by any norm, holding prior art as special cases. We then specialize our result to
regions that, for a positive definite A, are ellipsoids, i.e. kAδk2 ≤ c, and generalized cross-polytopes,
1
Under review as a conference paper at ICLR 2022
(a)
(b)
(c)	(d)
Figure 1: Illustration of the landscape of fy (blue corresponds to a higher confidence in y, the
true label) for a region around an input in a toy, 2-dimensional radially separable dataset. For two
dataset examples, in (a) and (b) we show the boundaries of the optimal `1 isotropic and anisotropic
certificates, while (c) and (d) are the boundaries of the optimal `2 isotropic and anisotropic certificates.
A thorough discussion of this figure is presented in Section 3.
i	.e. kAδk1 ≤ c, generalizing both `2 (Cohen et al., 2019) and `1 (Lecuyer et al., 2019; Yang
et al., 2020) certification (Section 4). (ii) We introduce a new evaluation framework to compare
methods that certify general (isotropic or anisotropic) regions. We compare two general certificates
by defining that a method certifying R1 is superior to another certifying R2, if R1 is a strict superset
to R2 . Further, we define a standalone quantitative metric as the volume of the certified region,
and specialize it for the cases of ellipsoids and generalized cross-polytopes (Section 5). (iii) We
propose ANCER, an anisotropic certification method that performs sample-wise (i.e. per sample in
the test set) region volume maximization (Section 6), generalizing the data-dependent, memory-based
solution from Alfarra et al. (2020). Through experiments on CIFAR-10 (Krizhevsky, 2009) and
ImageNet (Deng et al., 2009), We show that restricting AnCer's certification region to '1 and '2-balls
outperforms state-of-the-art `1 and `2 results from previous works (Yang et al., 2020; Alfarra et al.,
2020). Further, we show that the volume of the certified regions are significantly larger than all
existing methods, thus setting a new state-of-the-art in certified accuracy. We highlight that while we
effectively achieve state-of-the-art performance, it comes at a high cost given the data-dependency
requirements. A discussion of the limitations of the solution is presented in Section 6.
Notation. We consider a base classifier f : Rn → P(K), where P(K) is a probability simplex
over K classes, i.e. fi ≥ 0 and 1>f = 1, for i ∈ {1, . . . , K}. Further, we use (x, y) to be a sample
input x and its corresponding true label y drawn from a test set Dt, and fy to be the output of f at
the correct class. We use 'p to be the typically defined ∣∣∙∣∣p norm (P ≥1), and 'A or k ∙ ∣∣A,p for
P = {1, 2} to be a composite norm defined with respect to a positive definite matrix A as kA-1/pvkp.
2	Related Work
Verified Defenses. Since the discovery that DNNs are vulnerable against input perturbations (Good-
fellow et al., 2015; Szegedy et al., 2014), a range of methods have been proposed to build classifiers
that are verifiably robust (Huang et al., 2017; Gowal et al., 2019; Bunel et al., 2018; Salman et al.,
2019b). Despite this progress, these methods do not yet scale to the networks the community is
interested in certifying (Tjeng et al., 2019; Weng et al., 2018).
Randomized Smoothing. The first works on randomized smoothing used Laplacian (Lecuyer
et al., 2019; Li et al., 2019) and Gaussian Cohen et al. (2019) distributions to obtain '1 and '2-ball
certificates, respectively. Several subsequent works improved the performance of smooth classifiers by
training the base classifier using adversarial augmentation (Salman et al., 2019a), regularization (Zhai
et al., 2019), or general adjustments to training routines (Jeong & Shin, 2020). Recent work
derived 'p-norm certificates for other isotropic smoothing distributions (Yang et al., 2020; Levine
& Feizi, 2020; Zhang et al., 2019). Concurrently, Dvijotham et al. (2020) developed a framework
to handle arbitrary smoothing measures in any 'p-norm; however, the certification process requires
significant hyperparameter tuning. Similarly, Mohapatra et al. (2020) introduces larger certificates
that require higher-order information, yet do not provide a closed-form solution. This was followed
by a complementary data-dependent smoothing approach, where the parameters of the smoothing
distribution were optimized per test set sample to maximize the certified radius at an individual
2
Under review as a conference paper at ICLR 2022
input (Alfarra et al., 2020). All prior works considered smoothing with isotropic distributions and
hence certified isotropic `p-ball regions. In this paper, we extend randomized smoothing to certify
anisotropic regions, by pairing it with a generalization of the data-dependent framework (Alfarra
et al., 2020) to maximize the certified region at each input point.
3	M otivating Anisotropic Certificates
Certification approaches aim to find the safe region R, where arg maxi fi (x) = arg maxi fi(x +
δ) ∀δ ∈ R. Recent randomized smoothing techniques perform this certification by explicitly
optimizing the isotropic `p certified region around each input (Alfarra et al., 2020), obtaining state-of-
the-art performance as a result. Despite this `p optimality, we note that any `p -norm certificate is
worst-case from the perspective of that norm, as it avoids adversary regions by limiting its certificate
to the 'p-closest adversary. This means that it can only enjoy a radius that is at most equal to the
distance to the closest decision boundary. However, decision boundaries of general classifiers are
complex, non-linear, and non-radially distributed with respect to a generic input sample (Karimi et al.,
2019). This is evidenced by the fact that, within a reasonably small `p -ball around an input, there
are often only a small set of adversary directions (Tramer et al., 2017; 2018) (e.g. see the decision
boundaries in Figure 1). As such, while `p -norm certificates are useful to reason about worst-case
performance and are simple to obtain given previous works (Cohen et al., 2019; Yang et al., 2020;
Lee et al., 2019), they are otherwise uninformative in terms of the shape of decision boundaries, i.e.
which regions around the input are safe.
To visualize these concepts, we illustrate the decision boundaries of a base classifier f trained on a
toy 2-dimensional, radially separable (with respect to the origin) binary classification dataset, and
consider two different input test samples (see Figure 1). We compare the optimal isotropic and
anisotropic certified regions of different shapes at these points. In Figures 1a and 1b, we compare
an isotropic cross-polytope (of the form kδ k1 ≤ r) with an anisotropic generalized cross-polytope
(of the form kAδk1 ≤ r), while in Figures 1c and 1d we compare an isotropic `2 ball (of the form
kδ k2 ≤ r) with an anisotropic ellipsoid (of the form kAδk2 ≤ r). Notice that in Figures 1a and 1c,
due to the curvature of the classification boundary (shown in white), the optimal certification region
is isotropic in nature, which is evidenced by the similarities of the optimal isotropic and anisotropic
certificates. On the other hand, in Figures 1b and 1d, the location of the decision boundary allows for
the anisotropic certified regions to be considerably larger than their isotropic counterparts, as they
are not as constrained by the closest decision boundary, i.e. the worst-case performance. We note
that these differences are further highlighted in higher dimensions, and we study them for a single
CIFAR-10 test set sample in Appendix A.1. Further, we also showcase how anisotropic certification
allows for further insights into constant prediction (safe) directions in Appendix A.2.
4	Anis otropic Certification
One of the main obstacles in enabling anisotropic certification is the complexity of the analysis
required. To alleviate this, we follow a Lipschitz argument first observed by Salman et al. (2019a)
and Jordan & Dimakis (2020) and propose a simple and general certification analysis. We start with
the following two observations. All proofs are in Appendix B.
Proposition 1. Consider a differentiable function g : Rn → R. Ifsup χ∣Ng(x)k* ≤ L where ∣∣∙∣∣*
has a dual norm ∣∣zk = maxχ z>X s.t. kx∣∣* ≤ 1, then g is L-Lipschitz under norm ∣∣ ∙ ∣∣*, that is
|g(x) -g(y)| ≤ Lkx - yk.
Given the previous proposition, We formalize ∣∣ ∙ ∣∣ certification as follows:
Theorem 1. Let g : Rn → RK, gi be L-Lipschitz continuous under norm ∣∣∙∣∣* ∀i ∈ {1,..., K},
and cA = arg maxi gi (x). Then, we have arg maxi gi(x + δ) = cA for all δ satisfying:
∣δ∣≤ 2L (gcA (x)
- max gc6=cA
c
Theorem 1 provides an ∣∙∣ norm robustness certificate for any L-Lipschitz classifier g under ∣∣ ∙ ∣*.
The certificate is only informative when one can attain a tight non-trivial estimate of L, ideally
SUPxk Vg(χ) Il *, which is generally difficult when g is an arbitrary neural network.
3
Under review as a conference paper at ICLR 2022
Framework Recipe. In light of Theorem 1, randomized smoothing can be viewed differently as an
instance of Theorem 1 with the favorable property that the constructed smooth classifier g enjoys an
analytical form for L = SUPxk Vg(χ)k* by design. As such, to obtain an informative ∣∣ ∙ ∣∣ certificate,
one must, for an arbitrary choice of a smoothing distribution, compute the analytic Lipschitz constant
L under ∣∙∣∣* for the smooth g. While there can exist a notion of “optimal” smoothing distribution
for a given choice of ∣ ∙ ∣ certificate, as in part addressed earlier for the isotropic 'ι, '2 and '∞
certificates (Yang et al., 2020), this is not the focus of this paper. The choice of the smoothing
distribution in later sections is inspired by previous work for the purpose of granting anisotropic
certificates. This recipe complements randomized smoothing works based on Neyman-Pearson’s
lemma (Cohen et al., 2019) or the Level-Set and Differential Method (Yang et al., 2020).
We will deploy this framework recipe to show two specializations for anisotropic certification, namely
ellipsoids (Section 4.1) and generalized cross-polytopes (Section 4.2).1.
4.1	Certifying Ellipsoids
In this section, We consider the certification under 'ς norm, or ∣∣δ∣∑,2 = √δ>Σ-1δ, that has a dual
norm ∣δ∣Σ-1,2. Note that both ∣δ∣Σ,2 ≤ r and ∣δ∣Σ-1,2 ≤ r define an ellipsoid. Despite that the
folloWing results hold for any positive definite Σ, We assume for efficiency reasons that Σ is diagonal
throughout. First, We consider the anisotropic Gaussian smoothing distribution N (0, Σ) With the
smooth classifier defined as g∑(χ) = Ee〜N(o,∑) [f (X + e)]. Considering the classifier Φ-1(g∑(x)),
where Φ is the standard Gaussian CDF, and following Theorem 1 to grant an '∑ certificate for
Φ-1(g∑(x)), we derive the Lipschitz constant L under ∣∣ ∙ k∑-1,2, in the following proposition.
Proposition 2. Φ-1(g∑(x)) is 1 -Lipschitz (i.e. L = 1) under the ∣∣ ∙ ∣∣∑-1,2 norm.
Since Φ-1 is a strictly increasing function, by combining Proposition 2 with Theorem 1, we have:
Corollary 1. Let cA = arg maxi gΣi (x) , then arg maxi gΣi (x + δ) = cA for all δ satisfying:
∣δ∣∑,2 ≤ 2 (φ-1 (g∑A(X))- Φ-1 (maxg∑=cA(x))).
Corollary 1 holds the `2 certification from Zhai et al. (2019) as a special case for when Σ = σ2I.2
4.2	Certifying Generalized Cross-Polytopes
Here we consider certification under the 'λ norm defining a generalized cross-polytope, i.e. the
set {δ : ∣δ∣Λ,ι = 114-16111 ≤ r}, as opposed to the '1-bounded set that defines a cross-polytope,
i.e. {δ : ∣δ∣1 ≤ r}. As with the ellipsoid case and despite that the following results hold for any
positive definite Λ, for the sake of efficiency, we assume Λ to be diagonal throughout. For generalized
cross-polytope certification, we consider an anisotropic Uniform smoothing distribution U , which
defines the smooth classifier gΛ(x) = Ee〜U[-ι,i]n [f (X + Ae)]. Following Theorem 1 and to certify
under the '1Λ norm, we compute the Lipschitz constant of gΛ under the ∣ΛX∣∞ norm, which is the
dual norm of ∣ ∙ ∣∣λ,i (see Appendix B), in the next proposition.
Proposition 3. The classifier gΛ is 1/2-Lipschitz (i.e. L = 1/2) under the ∣AX∣∞ norm.
Similar to Corollary 1, by combining Proposition 3 with Theorem 1, we have that:
Corollary 2. Let cA = arg maxi gΛi (X) , then arg maxi gΛi (X + δ) = cA for all δ satisfying:
∣δ∣Λ,1 = ∣A-1δ∣1 ≤ (gΛcA(X) - mcax gΛc6=cA (X) .
Corollary 2 holds the '1 certification from Yang et al. (2020) as a special case for when A = λI .
1Our analysis also grants a certificate for a mixture of Gaussians smoothing distribution (see Appendix B.1).
2A similar result was derived in the appendix of Fischer et al. (2020); Li et al. (2020) with a more involved
analysis by extending Neyman-Pearson’s lemma.
4
Under review as a conference paper at ICLR 2022
5	Evaluating Anisotropic Certificates
With the anisotropic certification framework presented in the previous section, the question arises:
“Given two general (isotropic or anisotropic) certification regions R1 and R2 , how can one effectively
compare them?”. We propose the following definition to address this issue.
Definition 1. For a given input point x, consider the two certification regions R1 and R2 obtained
for two classifiers f1 and f2, i.e. A1 = {δ : arg maxc f1c(x) = arg maxc f1c(x + δ), ∀δ ∈ R1}
and A2 = {δ : arg maxc f2c(x) = arg maxc f2c(x + δ), ∀δ ∈ R2} where arg maxc f1c(x) =
arg maxc f2c(x). We say A1 is a "superior certificate" to A2 (i.e. A1 A2), if and only if, A1 ⊃ A2.
This definition is a natural extension from the radius-based comparison of `p -ball certificates, provid-
ing a basis for evaluating anisotropic certification. To compare an anisotropic to an isotropic region
of certification, it is not immediately clear how to (i) check that an anisotropic region is a superset to
the isotropic region, and (ii) if it were a superset, how to quantify the improvement of the anisotropic
region over the isotropic counterpart. In Sections 5.1 and 5.2, we tackle these issues for the particular
cases of ellipsoid and generalized cross-polytope certificates.
5.1	Evaluating Ellipsoid Certificates
Comparing '2-Balls to '∑ -Ellipsoids (Specialization of Definition 1). Recall that if Σ = σ2I,
our ellipsoid certification in Corollary 1 recovers as a special case the isotropic '2-ball certification
of Cohen et al. (2019); Salman et al. (2019a); Zhai et al. (2019). Consider the certified regions
Ri = {δ : kδ∣∣2 ≤ σrι} and R2 = {δ : kδ∣∣∑,2 = √δ>Σ-1δ ≤ r2} for given r1,r2 > 0. Since
we take Σ = diag({σi2}in=1), the maximum enclosed '2-ball for the ellipsoid R2 is given by the
set R3 = {δ : kδk2 ≤ mini σir2}, and thus R2 ⊇ R3. Therefore, it suffices that R3 ⊇ R1 (i.e.
mini QiY? ≥ σrι), to say that R2 is a superior certificate to the isotropic Ri as per Definition 1.
Quantifying '2Σ Certificates. The aforementioned specialization is only concerned with whether
our ellipsoid certified region R2 is “superior” to the isotropic '2-ball without quantifying it. A natural
solution is to directly compare the volumes of the certified regions. Since the volume of an ellipsoid
given by R2 is V(R2) = rn√πn∕r(n∕2+i) "二］Qi (Kendall, 200，), We directly compare the proxy
n
radius R defined for R2 as R = r2 n i Qi , since larger R correspond to certified regions With
larger volumes. Note that R, which is the nth root of the volume up to a constant factor, can be seen
as a generalization to the certified radius in the case When Qi = Q ∀i.
5.2	Evaluating Generalized Cross-Polytope Certificates
Comparing 'i -Balls to 'iΛ-Generalized Cross-Polytopes (Specialization of Definition 1).
Consider the certificates Si = {δ : kδ∣∣ι ≤ λrι}, S? = {δ : |怜限,1 = ∣∣Λ-1δk1 ≤ r2}, and
S3 = {δ : kδki ≤ mini λir2}, where we take Λ = diag({λi}in=i). Note that since S2 ⊇ S3, then as
per Definition 1, it suffices that S3 ⊇ Si (i.e. mini λir2 ≥ λri) to say that the anisotropic generalized
cross-polytope S2 is superior to the isotropic 'i -ball Si .
Quantifying 'iΛ Certificates. Following the approach proposed in the '2Σ case, we quantitatively
compare the generalized cross-polytope certification of Corollary 2 to the 'i certificate through the
volumes of the two regions. We first present the volume of the generalized cross-polytope.
Proposition 4. V ({δ : kΛ-1δ∣ι ≤ r}) = (2rn Qi λi.
n n n	n n	∕τ^^rn
Following this definition, we define the proxy radius for S2 in this case to be R = r2 n i=i λi . As
with the '2 case, larger R correspond certified regions with larger volumes. As in the ellipsoid case,
R can be seen as a generalization to the certified radius when λ% = λ ∀i.
5
Under review as a conference paper at ICLR 2022
6	AnCer: Sample-wise Volume Maximization for Anisotropic
Certification
Given the results from the previous sections, we are now equipped to certify anisotropic regions, in
particular ellipsoids and generalized cross-polytopes. As mentioned in Section 4, these regions are
generally defined as R = {δ : kδkΘ,p ≤ rp} for a given parameter of the smoothing distribution
Θ = diag ({θi}in=1), an `p-norm (p ∈ {1, 2}), and a gap value of rp ∈ R+. At this point, one could
simply take an anisotropic distribution with arbitrarily chosen parameters Θ and certify a trained
network at any input point x, in the style of what was done in the previous randomized smoothing
literature with isotropic distributions. However, the choice of Θ is more complex in the anisotropic
case. A fixed choice of anisotropic Θ could severely underperform the isotropic case - take, for
example, the anisotropic distribution of Figure 1d applied to the input of Figure 1c.
Instead of taking a fixed Θ, we generalize the framework introduced in Alfarra et al. (2020), where
parameters of the smoothing distribution are optimized per input test point (i.e. in a sample-wise
fashion) so as to maximize the resulting certificate. The goal of the optimization in Alfarra et al.
(2020) is, at a point x, to maximize the isotropic `2 region described in Section 4.1 (i.e. {δ : kδk2 ≤
σxrp (x, σx))}), where rp is the gap and a function of x and σx ∈ R+ . In the isotropic `p case,
this generalizes to maximizing the region {δ : kδkp ≤ θxrp (x, θx)}, which can be achieved by
maximizing radius θxrp (x,θx) through θx ∈ R+, obtaining ri*0 (Alfarra et al., 2020).
For the general anisotropic case, we propose AnCer, whose objective is to maximize the volume of
the certified region through the proxy radius, while satisfying the superset condition with respect to
the maximum isotropic '2 radius, 小. In the case of the ellipsoids and generalized cross-polytopes as
presented in Sections 5.1 and 5.2, respectively, AnCer’s optimization problem can be written as:
arg max rp (x, Θx) n	θχ	s.t. min θfrp (x, Θx) ≥ riso
Θx	i	i
(1)
where rp (x, Θx) is the gap value under the anisotropic smoothing distribution. We iteratively solve a
relaxed version of Equation (1) , with further details presented in Appendix C.
Memory-based Anisotropic Certification. While each of the classifiers induced by the parameter
Θx, i.e. gΘx, is robust by definition as presented in Section 4, the certification of the overall data-
dependent classifier is not necessarily sound due to the optimization procedure for each x. This
is a known issue in certifying data-dependent classifiers, and is addressed by Alfarra et al. (2020)
through the use of a memory-based procedure. In Appendix D, we present an adapted version of this
algorithm to AnCer. All subsequent results are obtained following this procedure.
Limitations of AnCer. Given ANCER uses a memorization procedure similar to the one presented
in Alfarra et al. (2020), it incurs limitations on memory and runtime complexity. The main limitations
of the memory-based certification are outlined in Appendix E of Alfarra et al. (2020). The anisotropic
case increases on the complexity of the isotropic framework by the increased runtime of specific
functions presented in Appendix D. Certification runtime comparisons are in Section 7.3.
Further, note that in memory-based data-dependent certification there is a single procedure for both
certification and inference in contrast with the fixed σ setting from Cohen et al. (2019). While the
linear runtime dependency on memory size might appear daunting for the deployment of such a
system, there are a few factors that could mitigate the cost. Firstly, in practice the models deployed
get regularly updated in deployment, and the memory should be reset in those situations. Secondly,
there are possible solutions which might attain sublinear runtime for the post-certification stage, such
as the application of k-d trees to reduce the space of comparisons and speed-up the process. As such,
we believe AnCer to be suited to applications in offline scenarios, where improved robustness is
desired and inference time is not a critical issue.
A further limitation of the memorization procedure has to do with the impact of the order in which
inputs are certified on the overall statistics obtained. Within a memory-based framework, certifying
x2 with x1 in memory can be different from certifying x1 with x2 in memory if they intersect. In
practice, given the low number of intersections observed with the original certified regions, this
effect was almost negligible in the results presented in Section 7. For fairness of comparison with
non-memory based methods, we report "worst-case" results for AnCer in which we abstain from
deciding whenever an intersection of two certified regions occurs.
6
Under review as a conference paper at ICLR 2022
0	1	2	3	4	5
h radius
CIFAR-IO-SmoothAdv
0	1	2	3	4	5
h radius
C∣FAR-10 - MACER
0	1	2	3	4	5
& radius
ImageNet - Cohen
——Fixed
Isotroplc DD
——ANCER
0	12	3	4
h rβd∣us
ImageNet- SmoothAdv
0	12	3	4
h radius
I'、 OQ
1	2	3	4	0
proxy radius
Fixed
Isotroplc DD
ANCER
( (
δ-BJn8< pel=七 3
12	3	4
iξ proxy radius
Figure 2: Distribution of top-1 certified accuracy as a function of '2 radius (top) and '∑-norm proxy
radius (bottom) obtained by different certification methods on CIFAR-10 and ImageNet.
7	Experiments
We now study the empirical performance of ANCER to obtain '∑, 'λ, '2 and '1 certificates on
networks trained using randomized smoothing methods found in the literature. In this section, we
show that ANCER is able to achieve (i) improved performance on those networks in terms of '2
and '1 certification when compared to certification baselines that smooth using a fixed isotropic
σ (Fixed σ) (Cohen et al., 2019; Yang et al., 2020; Salman et al., 2019a; Zhai et al., 2019) or
a data-dependent and memory-based isotropic one (Isotropic DD) (Alfarra et al., 2020); and (ii)
a significant improvement in terms of the '2Σ and '1Λ -norm certified region obtained by the same
methods - compared by computing the proxy radius of the certified regions - thus generally satisfying
the conditions of a superior certificate proposed in Definition 1. Note that both data-dependent
approaches (Isotropic DD and AnCer) use memory-based procedures. As such, the gains described
in this section constitute a trade-off given the limitations of the method described in Section 6.
We follow an evaluation procedure as similar as possible to the ones described in Cohen et al.
(2019); Yang et al. (2020); Salman et al. (2019a); Zhai et al. (2019) by using code and pre-trained
networks whenever available and by performing experiments on CIFAR-10 (Krizhevsky, 2009) and
ImageNet (Deng et al., 2009), certifying the entire CIFAR-10 test set and a subset of 500 examples
from the ImageNet test set. For the implementation of AnCer, we solve Equation (1) with Adam for
100 iterations, where the certification gap rp(x, Θx) is estimated at each iteration using 100 noise
samples per test point (see Appendix C) and Θx in Equation (1) is initialized with the Isotropic DD
solution from Alfarra et al. (2020). Further details of the setup can be found in Appendix E.
As in previous works, 'p certified accuracy at radius R is defined as the portion of the test set
Dt for which the smooth classifier correctly classifies with an 'p certification radius of at least R.
In a similar fashion, We define the anisotropic 'ς∕'λ certified accuracy at a proxy radius of R (as
defined in Section 5) to be the portion of Dt in which the smooth classifier classifies correctly with
an '∑∕'Λ -norm certificate of an nth root volume of at least RR. We also report average certified
radius (ACR) defined as Eχ,y〜d、[Rχl(g(χ) = y)] (Alfarra et a , 202 ; Zhai et a , 2019) as well
as average certified proxy radius (ACR) defined as Eχ,y〜d、[Rχl(g(χ) = y)], where Rx and Rx
denote the radius and proxy radius at x with a true label y for a smooth classifier g. Recall that in
the isotropic case, the proxy radius is, by definition, the same as the radius for a given 'p-norm. For
each classifier, we ran experiments on the σ values reported in the original work (with the exception
of Yang et al. (2020), see Section 7.2). For the sake of brevity, we report in this section the top-1
certified accuracy plots, ACR and ACR per radius across σ, as in Salman et al. (2019a); Zhai et al.
(2019); Alfarra et al. (2020). The performance of each method per σ is presented in Appendix G.
7.1	ELLIPSOID CERTIFICATION ('2 AND '2Σ-NORM CERTIFICATES)
We perform the comparison of '2-ball vs. '2Σ-ellipsoid certificates via Gaussian smoothing using net-
works trained following the procedures defined in Cohen et al. (2019), Salman et al. (2019a), and Zhai
et al. (2019). For each of these, we report results on ResNet18 trained using σ ∈ {0.12, 0.25, 0.5, 1.0}
for CIFAR-10, and ResNet50 using σ ∈ {0.25, 0.5, 1.0} for ImageNet. For details of the training
7
Under review as a conference paper at ICLR 2022
Table 1: Comparison of top-1 certified accuracy at different `2 radii, `2 average certified radius
(ACR) and '∑ average certified proxy radius (ACR) obtained by using the isotropic σ used for
training the networks (Fixed σ); the isotropic data-dependent (Isotropic DD) optimization scheme
from Alfarra et al. (2020); and AnCer’s data-dependent anisotropic optimization.
CIFAR-10	Certification	0.0	Acc 0.25	uracy 0.5	@ `2 r 1.0	adius ( 1.5	%) 2.0	2.5	`2 ACR	'∑ ACR
Cohen Cohen et al. (2019)	Fixed σ	86	71	51	27	14	6	2	0.722	0.722
	Isotropic DD	82	76	62	39	24	14	8	1.117	1.117
	AnCer	86	85	77	53	31	17	10	1.449	1.772
SmoothAdv Salman et al. (2019a)	Fixed σ	82	72	55	32	19	9	5	0.834	0.834
	Isotropic DD	82	75	63	40	25	15	7	1.011	1.011
	AnCer	83	81	73	48	30	17	8	1.224	1.573
MACER Zhai et al. (2019)	Fixed σ	87	76	59	37	24	14	9	0.970	0.970
	Isotropic DD	88	80	66	40	17	9	6	1.007	1.007
	AnCer	84	80	67	34	15	11	9	1.136	1.481
ImageNet	Certification	0.0	Accuracy @ `2 radius (%) 0.5	1.0	1.5	2.0	2.5					3.0	`2 ACR	'∑ ACR
Cohen Cohen et al. (2019)	Fixed σ	70	56	41	31	19	14	12	1.098	1.098
	Isotropic DD	71	59	46	36	24	19	15	1.234	1.234
	AnCer	70	70	62	61	42	36	29	1.810	1.981
SmoothAdv Salman et al. (2019a)	Fixed σ	65	59	44	38	26	20	18	1.287	1.287
	Isotropic DD	66	62	53	41	32	24	20	1.428	1.428
	AnCer	66	66	62	58	44	37	32	1.807	1.965
procedures, see Appendix E.1. Figure 2 plots top-1 certified accuracy as a function of the `2 radius
(top) and of the '∑-norm proxy radius (bottom) per trained network and dataset, while Table 1
presents an overview of the certified accuracy at various '2 radii, as well as '2 ACR and 'ς -norm
ACR. Recall that, following the considerations in Section 5.1, the '2 certificate obtained through
ANCER is the maximum enclosed isotropic '2-ball in the '2Σ ellipsoid.
First, we note that sample-wise certification (Isotropic DD and AnCer) achieves higher certified
accuracy than fixed σ across the board. This mirrors the findings in Alfarra et al. (2020), since certi-
fying with a fixed σ for all samples struggles with the robustness/accuracy trade-off first mentioned
in Cohen et al. (2019), whereas the data-dependent solutions explicitly optimize σ per sample to
avoid it. More importantly, ANCER achieves new state-of-the-art '2 certified accuracy at most radii
in Table 1, e.g. at radius 0.5 ANCER brings certified accuracy to 77% (from 66%) and 70% (from
62%) on CIFAR-10 and ImageNet, respectively, yielding relative percentage improvements in ACR
between 13% and 47% when compared to Isotropic DD. While the results are significant, it might
not be immediately clear why maximizing the volume of an ellipsoid with AnCer results in a larger
maximum enclosed '2-ball certificate in '2Σ ellipsoid when compared to optimizing the '2-ball with
Isotropic DD. We explore this phenomenon in Appendix G.3.
As expected, ANCER substantially improves 'ς ACR compared to Isotropic DD in all cases 一
with relative improvements in ACR between 38% and 63% over both datasets. The joint results,
certification with '2 and '2Σ, establish that ANCER certifies the '2-ball region obtained by previous
approaches, in addition to a much larger region captured by the '2Σ certified accuracy and ACR, and
therefore is, according to Definition 1, generally superior to the Isotropic DD one.
7.2	GENERALIZED CROSS-POLYTOPE CERTIFICATION ('1 AND '1Λ -NORM CERTIFICATES)
To investigate '1 -ball vs. '1Λ -generalized cross-polytope certification via Uniform smooth-
ing, we compare ANCER to the '1 state-of-the-art results from RS4A (Yang et al., 2020).
While the authors of the original work report best certified accuracy based on 15 networks
trained at different σ levels between 0.15 and 3.5 on CIFAR-10 (WideResNet40) and Ima-
geNet (ResNet50) and due to limited computational resources, we perform the analysis on a
subset of those networks with σ = {0.25, 0.5, 1.0}. We reproduce the results in Yang et al.
(2020) as closely as possible, with details of the training procedure presented in Appendix E.2.
8
Under review as a conference paper at ICLR 2022
Table 2: Comparison of top-1 certified accuracy at different '1 radii, '1 average certified radius
(ACR) and 'λ average certified proxy radius (ACR) obtained by using the isotropic σ used for
training the networks (Fixed σ); the isotropic data-dependent (Isotropic DD) optimization scheme
from Alfarra et al. (2020); and AnCer’s data-dependent anisotropic optimization.
CIFAR-10	Certification	0.0	Accuracy @ `1 radius (%) 0.25	0.5	0.75	1.0	1.5					2.0	`1 ACR	'Λ ACR
RS4A Yang et al. (2020)	Fixed σ Isotropic DD AnCer	92 92 92	83 89 90	75 82 84	71 76 80	46 58 63	0 6 6	0 2 2	0.775 0.946 0.980	0.775 0.946 1.104
ImageNet
RS4A
Yang et al. (2020)
Fixed σ	78	73	67	63	0	0	0	0.683	0.683
Isotropic DD	79	76	70	65	46	0	0	0.729	0.729
AnCer	78	76	70	66	48	0	0	0.730	1.513
Figure 3 shows the top-1 certified accuracy as a function
of the 'ι radius (top) and of the d-norm proxy radius
(bottom) for RS4A, and Table 2 shows an overview of the
certified accuracy at various `1 radii, as well as `1 ACR
and 'Λ AcR.
As with the ellipsoid case, we notice that AnCer out-
performs both Fixed σ and Istropic DD for most `1 radii,
establishing new state-of-the-art results in CIFAR-10 at
radii 0.5 and 1.0, and ImageNet at radii 0.5 (compared
to previous results reported in Yang et al. (2020)). Once
more and as expected, AnCer significantly improves the
'λ ACRR for all radii, pointing to substantially larger cerfi-
cates than the isotropic case. These results also establish
that ANCER certifies the `1 -ball region obtained by previ-
ous work, in addition to the larger region obtained by the
'λ certificate, and thus We can consider it superior (with
respect to Definition 1) to Isotropic DD.
0.0
0	1	2
<ι proxy radius
——Fixed
Isotroplc DD
——ANCER
ImageNet- RS4A
n8B Pe=BJeO
0.0
0	1	2
iɪ proxy volume
Figure 3: Distribution of top-1 certified
accuracy as a function of `1 radius (top)
and 'Λv-norm proxy radius (bottom) ob-
tained by different certification methods
on CIFAR-10 and ImageNet.
7.3	Certification Runtime
The certification procedures of Isotropic DD and AnCer
tradeoff improved certified accuracy for runtime, since
they require a sample-wise optimization to be run prior to
the Certify step described in Cohen et al. (2019), and
a memory-based step as per Alfarra et al. (2020). The
runtime of the optimization and certification procedures
is roughly equal for '1, '2, 'ς and 'λ certification, and
mostly depends on network architecture. As such, we
report the average certification runtime for a test set sample
Table 3: Average certification time for
each sample per architecture used: (a)
ResNet18 ('2, '∑ on CIFAR-10), (b)
WideReSNet40 ('1, 'Λ on CIFAR-10),
and (c) ResNet50 (ImageNet).
	Fixed σ	Isotropic DD	AnCer
(a)	1.6s	1.8s	2.7s
(b)	7.4s	9.5s	11.5s
(c)	109.5s	136.0s	147.0s
on an NVIDIA Quadro RTX 6000 GPU for Fixed σ, Isotropic DD and ANCER (including the isotropic
initialization step) in Table 3. We observe that the overall run time overhead for AnCer is not
significant as compared to its certification gains.
8	Conclusion
In this work, we lay the theoretical foundations for anisotropic certification through a simple analysis,
propose a metric for comparing general robustness certificates, and introduce AnCer, a certification
procedure that estimates the parameters of the anisotropic smoothing distribution to maximize the
certificate. Our experiments show that ANCER achieves state-of-the-art '1 and '2 certified accuracy
on CIFAR-10 and ImageNet. Our anisotropic analysis enables further insights about the boundary of
the safe region around an input sample, as compared to its isotropic counterpart.
9
Under review as a conference paper at ICLR 2022
9	Reproducibility Statement
We provide complete proofs for each of the theoretical results presented in Section 4 in Appendix B.
Details on the practical implementation of the AnCer optimization algorithm is presented in
Appendix C, while the memory-based procedure is detailed in Appendix D. An overview of the
experimental setup used to obtain the results in Section 7 can be found at the top of that section, while
details on the hyperparameters and network training are presented in Appendix E. We include source
code in Python and instructions to reproduce our results as part of the supplementary material.
10	Ethics S tatement
We confirm that no results in this paper involved studies on human subjects, and all experiments used
open-source datasets.
References
Motasem Alfarra, Adel Bibi, Philip HS Torr, and Bernard Ghanem. Data dependent randomized
smoothing. arXiv preprint arXiv:2012.04351, 2020. 2, 3, 6,7, 8, 9, 13, 17, 18, 19, 23, 25
Rudy Bunel, Ilker Turkaslan, Philip HS Torr, Pushmeet Kohli, and M Pawan Kumar. A unified view
of piecewise linear neural network verification. In Advances in Neural Information Processing
Systems (NeurIPS), 2018. 2
Jeremy M Cohen, Elan Rosenfeld, and J Zico Kolter. Certified adversarial robustness via randomized
smoothing. In International Conference on Machine Learning (ICML), 2019. 1, 2, 3, 4, 5, 6, 7, 8,
9, 17, 23, 24
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2009. 1, 2,7, 22
Krishnamurthy Dj Dvijotham, Jamie Hayes, Borja Balle, Zico Kolter, Chongli Qin, Andrgs Gyorgy,
Kai Xiao, Sven Gowal, and Pushmeet Kohli. A framework for robustness certification of smoothed
classifiers using f-divergences. In International Conference on Learning Representations (ICLR),
2020. 2
Marc Fischer, Maximilian Baader, and Martin Vechev. Certified defense to image transformations via
randomized smoothing. In Advances in Neural Information Processing Systems (NeurIPS), 2020.
4
Igor Gilitschenski and Uwe D Hanebeck. A robust computational test for overlap of two arbitrary-
dimensional ellipsoids in fault-detection of kalman filters. In 2012 15th International Conference
on Information Fusion, 2012. 19, 20
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations (ICLR), 2015. 1, 2
Sven Gowal, Krishnamurthy Dj Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan
Uesato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. Scalable verified training for
provably robust image classification. In IEEE International Conference on Computer Vision
(ICCV), 2019. 2
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In IEEE Conference on Computer Vision and Pattern Recognition, 2016. 22
Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. Safety verification of deep neural
networks. In International Conference on Computer Aided Verification (CAV), 2017. 2
Jongheon Jeong and Jinwoo Shin. Consistency regularization for certified robustness of smoothed
classifiers. In Advances in Neural Information Processing Systems (NeurIPS), 2020. 2
10
Under review as a conference paper at ICLR 2022
Matt Jordan and Alexandros G Dimakis. Exactly computing the local lipschitz constant of relu
networks. In Advances in Neural Information Processing Systems (NeurIPS), 2020. 3
Hamid Karimi, Tyler Derr, and Jiliang Tang. Characterizing the decision boundary of deep neural
networks. arXiv preprint arXiv:1912.11460, 2019. 3
Maurice G Kendall. A Course in the Geometry ofn Dimensions. Courier Corporation, 2004. 5, 15
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009. 2, 7,
13, 22
Aounon Kumar, Alexander Levine, Tom Goldstein, and Soheil Feizi. Curse of dimensionality on
randomized smoothing for certifiable robustness. In International Conference on Machine Learning
(ICML), 2020. 1
Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified
robustness to adversarial examples with differential privacy. In IEEE Symposium on Security and
Privacy (SP), 2019. 1, 2
Guang-He Lee, Yang Yuan, Shiyu Chang, and Tommi S Jaakkola. Tight certificates of adversarial
robustness for randomly smoothed classifiers. arXiv preprint arXiv:1906.04948, 2019. 3
Alexander Levine and Soheil Feizi. Robustness certificates for sparse adversarial attacks by ran-
domized ablation. In Association for the Advancement of Artificial Intelligence (AAAI), 2020.
2
Alexander Levine and Soheil Feizi. Improved, deterministic smoothing for l1 certified robustness.
arXiv preprint arXiv:2103.10834, 2021. 1
Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Certified adversarial robustness with
additive noise. In Advances in Neural Information Processing Systems (NeurIPS), 2019. 2
Linyi Li, Maurice Weber, Xiaojun Xu, Luka Rimanic, Tao Xie, Ce Zhang, and Bo Li. Provable robust
learning based on transformation-specific smoothing. arXiv preprint arXiv:2002.12398, 2020. 4
Jeet Mohapatra, Ching-Yun Ko, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu, and Luca Daniel. Higher-
order certification for randomized smoothing. Advances in Neural Information Processing Systems
(NeurIPS), 2020. 2, 27, 28
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, and Pascal Frossard. Robust-
ness via curvature regularization, and vice versa. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2019. 13
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep
learning library. In Advances in Neural Information Processing Systems (NeurIPS). 2019. 18, 23
LlUis Ros, AssUmPta Sabater, and Federico Thomas. An ellipsoidal calculus based on propagation
and fusion. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 2002. 19,
20
Hadi Salman, Jerry Li, Ilya P Razenshteyn, Pengchuan Zhang, Huan Zhang, SebaStien Bubeck,
and Greg Yang. Provably robust deep learning via adversarially trained smoothed classifiers. In
Advances in Neural Information Processing Systems (NeurIPS), 2019a. 2, 3, 5, 7, 8, 14, 23, 24, 27,
29
Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang. A convex relaxation
barrier to tight robust verification of neural networks. In Advances in Neural Information Processing
Systems (NeurIPS), 2019b. 2
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning
Representations (ICLR), 2014. 1, 2
11
Under review as a conference paper at ICLR 2022
Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed
integer programming. In International Conference on Learning Representations (ICLR), 2019. 2
Florian Tramer, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. The space of
transferable adversarial examples. arXiv preprint arXiv:1704.03453, 2017. 3
Florian Tramer, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick Mc-
Daniel. Ensemble adversarial training: Attacks and defenses. In International Conference on
Learning Representations (ICLR), 2018. 3
Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Duane Boning, Inderjit S
Dhillon, and Luca Daniel. Towards fast computation of certified robustness for relu networks. In
International Conference on Machine Learning (ICML), 2018. 2
Greg Yang, Tony Duan, J Edward Hu, Hadi Salman, Ilya Razenshteyn, and Jerry Li. Randomized
smoothing of all shapes and sizes. In International Conference on Machine Learning (ICML),
2020. 1,2,3,4,7,8,9,23,28
Runtian Zhai, Chen Dan, Di He, Huan Zhang, Boqing Gong, Pradeep Ravikumar, Cho-Jui Hsieh,
and Liwei Wang. Macer: Attack-free and scalable robust training via maximizing certified radius.
In International Conference on Learning Representations (ICLR), 2019. 2, 4, 5, 7, 8, 23, 24
Dinghuai Zhang, Mao Ye, Chengyue Gong, Zhanxing Zhu, and Qiang Liu. Filling the soap
bubbles: Efficient black-box adversarial certification with non-gaussian smoothing. https:
//openreview.net/forum?id=Skg8gJBFvr, 2019. 2
12
Under review as a conference paper at ICLR 2022
A Qualitative Motivation of Anis otropic Certification
A.1 Visualizing CIFAR- 1 0 Optimized Isotropic vs. Anisotropic Certificates
To extend the illustration in Figure 1 to a higher
dimensional input, we now analyze an exam-
ple of the isotropic `2 certification of random-
ized smoothing with N(0, σ2I), where σ is op-
timized per input Alfarra et al. (2020), against
AnCer, certifying an anisotropic region char-
acterized by a diagonal 传-norm. To do so, We
consider a CIFAR-10 Krizhevsky (2009) dataset
point x, Where the input is of size (32x32x3).
We perform the 2D analysis by considering the
regions closest to a decision boundary. To do so,
and folloWing Moosavi-Dezfooli et al. (2019),
We compute the Hessian of fy (x) With respect
to x Where y is the true label for x With f clas-
sifying x correctly, i.e. y = arg maxi fi(x).
In addition to the Hessian, We also compute its
eigenvector decomposition, yielding the eigen-
vectors {νi}, i ∈ {1, . . . , 3072} ordered in de-
scending order of the absolute value of the re-
spective eigenvalues. In Figure 4a, We shoW the
I	I Anistropic
I	I Isotropic
⑶
Figure 4: Illustration of the landscape of fy for
points around an input point x, and tWo pro-
jections of an isotropic `2 certified region and
an anisotropic ^2-norm region on a CIFAR-10
dataset example to a subset of tWo eigenvectors of
the Hessian of fy (blue regions correspond to a
higher confidence in y).
I	I Anistropic
I	I Isotropic
(b)
projection of the landscape of fy in the highest curvature directions, i.e. ν1 and ν2 . Note that the
isotropic certification, much as in Figure 1c, in these 2 dimensions is nearly optimal When compared to
the anisotropic region. HoWever, if We take the same projection With respect to the eigenvectors With
the loWest and highest eigenvalues, i.e. ν1 and ν3072, the advantages of the anisotropic certification
become clear as shoWn in Figure 4b.
A.2 Visualizing Safe Images in Optimized Anisotropic Certificates
Figure 5: Visualization of a CIFAR-10 image x and an example x + δ of an imperceptible change
that is not inside the optimal isotropic certified region, but is covered by the anisotropic certificate.
As observed from the examples in Section 3 and Appendix A.1, anisotropic certification reasons
more closely about the shape of the decision boundaries, alloWing for further insights into constant
prediction (safe) directions. In Figure 5, We present a series of test set images x, as Well as practically
indistinguishable X + δ images which are not inside the optimal certified isotropic '2 -balls for each
input sample, yet are within the anisotropic certified regions. This shoWcases the merits of using
anisotropic certification for characterizing larger safe regions.
B	Anisotropic Certification and Evaluation Proofs
Proposition 1 (restatement). Consider a differentiablefunction g : Rn → R. Ifsupχ∣Ng(x)k* ≤ L
where ∣∣∙∣∣* has a dual norm ∣∣zk = maxχ z>x s.t. kx∣∣* ≤ 1, then g is L-Lipschitz under norm
k ∙ ∣∣*, that is |g(x) — g(y)∣ ≤ Lkx - yk.
13
Under review as a conference paper at ICLR 2022
Proof. Consider some x, y ∈ Rn and a parameterization in t as γ(t) = (1 - t)x + ty ∀t ∈ [0, 1].
Note that γ(0) = x and γ(1) = y. By the Fundamental Theorem of Calculus we have:
|g(y) - g(X)I = Ig(Y(I)) - g(Y(O))I
1 dg(Y⑴)dt
dt
1
Vg>Vγdt
1
Vg>VY dt
≤ r kVg(X)k*kvY⑴kdt ≤ Lky — Xk
0
□
Theorem 1 (restatement). Let g : Rn → RK, gi be L-Lipschitz continuous under norm k ∙ k*
∀i ∈ {1, . . . , K}, and cA = arg maxi gi(X). Then, we have arg maxi gi(X + δ) = cA for all δ
satisfying:
kδk ≤ 2l (gcA(x) — maxgc=cA(x)).
Proof. Take cB = arg maxc gc6=cA (X). By Proposition 1, we get:
IgcA (x + δ) — gcA (x)∣≤ Lkδk =⇒ gcA (x + δ) ≥ gcA (x) — Lkδk
IgcB (x + δ) — gcB(x)∣≤ Lkδk =⇒ gcB (x + δ) ≤ gcB (x)+ Lkδk
By subtracting the inequalities and re-arranging terms, we have that as long as gcA (X) — Lkδk >
gcB (x) + Lkδk, i.e. the bound in the Theorem, then gcA (x + δ) > gcB (x + δ), completing the
proof.	□
Proposition 2 (restatement). Consider g∑(x) = Ee〜N(o,∑) [f (x + e)]. Φ-1(g∑(x)) is 1 -Lipschitz
(i.e. L = 1) under the k ∙ k∑-1,2 norm.
Proof. To prove Proposition 2, one needs to show that Φ-1(gΣi (x)) ∀i is 1-Lipschitz under the
k ∙ ∣∣∑-1,2 norm. For ease of notation, we drop the superscript g∑ and use only g. We want to
show that kVΦ-1 (g∑(x))k∑-ι ,2 = kΣ1∕2VΦ-1(g∑(x))k2 ≤ 1. Following the argument presented
in Salman et al. (2019a), it suffices to show that, for any unit norm direction u and p = gΣ(x), we
have:
U>∑2 Vg∑(x) ≤ √2∏ exp 卜2(Φ-1(p))2) .	(2)
We start by noticing that:
U>∑1 Vg∑(x) =	1 局 f f (t)u>Σ1 ∑-1(t — x)exp (—1(x — t)∑-1(x — t)) dnt
( 2π)n IΣI Rn	2
=Es 〜N (0,i)[f(x + ∑1 s)u>s] = Ev 〜N (0,∑)[f(x + v)u>∑-1 v]∙
We now need to find the optimal f * : Rn → [0,1] that satisfies g∑(x) = Ev〜N(o,∑) [f (x + v)] = P
while maximizing the left hand size Ev〜N(o,∑) [f (x + v)u>Σ-1 v]. We argue that the maximizer is
the following function:
f *(x + V) = 1 {u>Σ-1V ≥ —Φ-1(p)}.
To prove that f * is indeed the optimal maximizer, we first show feasibility. (i): It is clear that
f* : Rn → [0, 1]. (ii) Note that:
Ev 〜N (0,∑)) [1 {u>ς-2 V ≥ —φ-1 (p)}] = Px 〜N (0,1)(X ≥ 一①-1(P)) = I- φJφ-1(P))= P.
To show the optimality of f*, we show that it attains the right upper bound:
Ev〜N(0,Σ))[u>∑-1VI {u>∑-2V ≥ —Φ-1 (p)}i = Ex〜N(0,1) [xl {x ≥ —Φ-1(p)}]
= ∖____ / X exp ( - -ɪx2 I dx
√2∏ J-Φ-1(p)	2
=√∏exp (-2&-1(P))2)
obtaining the bound from Equation (2), and thus completing the proof.	□
14
Under review as a conference paper at ICLR 2022
Proposition 3 (restatement). Consider gχ(x) = Ee〜u[-ι,i]n [f (x + Ae)]. The classifier g∖ ∀i is
1/2-Lipschitz (i.e. L = 1/2) under the kΛxk∞ norm.
Proof. We begin by observing that the dual norm of kχkΛ,ι = IlATXkI is kχ∣∣* = ∣∣Axk∞, since:
max x>y = max y>Az = kAyk∞.
kΛ-1xk1≤1	kzk1≤1
Without loss of generality, We analyze dgi∕∂χι. Let X = [χ2,..., Xn] ∈ Rn-1, then:
空=上"Z	Z 1fi(xι + λιeι,X + A^)deιdn-i^
∂x1	(2λ)n ∂x1 [-1,1]n-1 -1
1
=而	(f (χι + 1, x + Ae) - f (χι - 1, χ + Ae))d	e
2n [-1,1]n-1
Thus,
l⅛T∣ ≤ 2n Qn=2 λj /-1,1]n-Jfi(XI + 1,X + A e) - fi(x1 - 1,x + AOldn-Z ≤ 2.
The second and last steps folloW by the change of variable t = χ1+λ1e1 and Leibniz rule. FolloWing a
symmetric argument, ∖λjdgi∕∂xj ∣ ≤ 1∕2 ∀i resulting in having ∣∣AVgi(x)k∞ = maxi λi ∣dg7∂xJ ≤
1/ 2 ∀i concluding the proof.	□
Proposition 4 (restatement). V ({δ : kA-1δk1 ≤ r}) = (2n)- Qi λi.
Proof. Take A = rA-1 = diag(1∕rλι,..., 1∕rλ-) = diag(a1,..., an).
We can re-Write the region as {x : Pi ai|xi| ≤ 1}, from Which it is clear to see that this region is
an origin centered, axis-aligned simplex With the set of vertices V = {±1∕aiei}in=1, Where ei is the
standard basis vector i.
Define the sets of vertices Vt = V \ {-1∕a-en} and Vb = V \ {1∕a-en}. Given the symmetry
around the origin, each of these sets defines an n-dimensional hyperpyramid With a shared base
Bn-1 given by the n - 1-dimensional hyperplane defined by all vertices Where xn = 0, and an
apex at the vertex 1∕a-en (or -1∕a-en in the case of Vb). The volume of each of these n - 1-
dimensional hyperpyramids is given by V(B--1)∕na- (Kendall (2004)), yielding a total volume of
Vn = n a1-V(Bn-1). The same argument can be applied to compute V(Bn-1) which is a union of
two n 一 1-dimensional hyperpyramids. This forms a recursion that completes the proof.	□
Proof. (Alternative Proof.) We consider the case that A-1 is a general positive definite matrix
that is not necessarily diagonal. Note that V ({δ : kA-1δk1 ≤ r}) = V ({δ : k(rA)-1δk1 ≤ 1})=
rn|A|V ({δ : ∣δ∣1 ≤ 1}) where |rA| denotes the determinant. The last equality follows by the
volume of a set under a linear map and noting that {δ : ∣(rA)-1δ∣1 ≤ 1} = {rAδ : ∣δ∣1 ≤ r}.
At last, {δ : ∣δ∣1 ≤ 1} can be expressed as the disjoint union of 2n simplexes. Thus, we have
V ({δ : kA-1δk1 ≤ r}) = (2r)-/n!|A| since the volume of a simplex is 1∕n! completing the proof. □
For completeness, we supplement the previous result with bounds on the volume that may be useful
for future readers.
Proposition 5. For any positive definite A-1 ∈ Rn×n, we have the following:
≤ V {δ : ∣A-1δ∣1 ≤ r}
≤ (2r)nV (Z(A))
where V (Z(Λ)) =，|A>A| which is the volume Ofthe zonotope with a generator matrix A.
15
Under review as a conference paper at ICLR 2022
Proof. Let S1 = {δ : kΛ-1δk1 ≤ r}, S∞ = {δ : kΛ-1δk∞ ≤ r} and S∞n = {δ : nkΛ-1δk∞ ≤
r}. Since kΛ-1δk∞ ≤ kΛ-1δk1 ≤ nkΛ-1δk∞, then S∞ ⊇ S1 ⊇ S∞n . Therefore, we have
V (S∞) ≥ V (Si) ≥ V (S∞). At last note that, S∞ = { r Λδ : ∣∣δ∣∣∞ ≤ 1} and that with the change of
variables δ = 2u-1n where 1n is a vector of all ones, We have S∞ = Z (ɪ A)㊉-r Λ1n where ㊉ isa
MinkoWski sum and noting that Tr A1n is a single point in Rn. Therefore, V (Z 2rA A)㊉-Alj =
(2r∕n)n V (Z(A)). The upper bound follows with a similar argument completing the proof. □
B.1 Certification under Gaussian Mixture Smoothing Distribution
We consider a general, K-component, zero-mean Gaussian mixture smoothing distribution G such
that:
K
G({αi,Σi}iK=1) := XαiN(0,Σi),	s.t. Xαi = 1,0<αi ≤ 1	(3)
i=1	i
Given f and as per the recipe described in Section 4, we are interested in the Lipschitz constant of
the smooth classifier gg(x) = (f * G)(X) = PK αig∑% = PK αi(f * N(0, Σi)) = Pi αig∑1x)
where gΣi is defined as in the Gaussian case.
Note the weaker bound when compared to Proposition 2, for each of the Gaussian components
presented in the following proposition.
Proposition 6. g∑ is ∖f2p∏ -LiPSChitz under kJ∣∑-1,2 norm.
Proof. Following a similar argument to the proof of Proposition 2, we get:
u>ς 1 Vg∑(x) ≤ (2π)n∕2p∑i' ∣u>ς-2(t - x)∣ exp (-2(x - 0>ςT(X - t)) dnt
=Es〜N(0,I) [∣u>s∣] = Ev〜N(0,1) [∣v∣] = V/2/n.
□
With Proposition 6, we obtain a Lipschitz constant for a Gaussian mixture smoothing distribution as:
Proposition 7. gg is pπ∕2-Lipschitz under ∣∣δ∣∣B-1,2 norm, where BT = PK aiΣ-i.
Proof.
∣gG (X + δ) - gG (X)∣ ≤
≤
2 kδkB,2,
Obtained by first applying the triangle inequality, then Proposition 2 followed by Jensen’s inequality.
□
Thus yielding the following certificate by combining Proposition 7 and Theorem 1.
Corollary 3. Let cA = arg maxi gG (X) , then arg maxi gGi (X + δ) = cA for all δ satisfying:
kδkB,2 ≤√2π (gGA(X)- max gG=cA
where B-1 = PiK αiΣi-1.
16
Under review as a conference paper at ICLR 2022
C AnCer Optimization
In this section we detail the implementation choices required to solving Equation (1). For ease of
presentation, we restate the ANCER optimization problem (with Θx = diag({θix}in=1)):
arg max
Θx
rp (x, Θx) n	θix
s.t. min θχrp (x, Θx) ≥ r*o,
i
where rp (x, Θx) is the gap value under the anisotropic smoothing distribution, and & is the optimal
isotropic radius, i.e, θxrp(x, θx) for θx ∈ R+. This is a nonlinear constrained optimization problem
that is challenging to solve. As such, we relax it, and solve instead:
arg max	rp (x,	Θx)	n ɪɪθX	+ K mE	θχrp	(x, Θx)	s.t.	θχ	≥	θx
given a hyperparameter K ∈ R+. While the constraint θχ ≥ θx is not explicitly required to enforce
the superset condition over the isotropic case, it proved itself beneficial from an empirical perspective.
To sample from the distribution parameterized by Θx (in our case, either a Gaussian or Uniform), we
make use of the reparameterization trick, as in Alfarra et al. (2020). The solution of this optimization
problem can be found iteratively by performing projected gradient ascent.
A standalone implementation for the AnCer optimization stage is presented in Listing 1, whereas the
full code integrated in our code base is available as supplementary material. To perform certification,
we simply feed the output of this optimization to the certification procedure from Cohen et al. (2019).
import torch
from torch.autograd import Variable
from torch.distributions.normal import Normal
class Certificate():
def Compute_proxy_gap(self, logits: torch.Tensor):
raise NOtImplementedErrOr
def Sample_noise(self, batch: torch.Tensor, repeated_theta: torch.Tensor):
raise NotImplementedError
def compute_gap(self, pABar: float):
raise NotImplementedError
class L2Certificate(Certificate):
def __init__(self, batch_size: int, device: str = "cuda:0"):
self.m = NOrmal(torch.zeros(batch_Size).to(device),
torch.ones(batch_Size).to(device))
self.device = device
self.norm = "l2"
def COmPute_proxy_gap(self, logits: torch.Tensor):
return self.m.icdf(logits[:, 0].ClamP_(0.001, 0.999)) - \
self.m.icdf(logits[:, 1].clamp_(0.001, 0.999))
def SamPle_noise(self, batch: torch.Tensor, repeated_theta: torch.Tensor):
return torch.randn_like(batch, device=self.device) * repeated_theta
def COmPute_gap(self, pABar: float):
return norm.ppf(pABar)
class L1Certificate(Certificate):
def __init__(self, device="cuda:0"):
self.device = device
self.norm = "l1"
def COmPute_proxy_gap(self, logits: torch.Tensor):
return logits[:, 0] - logits[:, 1]
def SamPle_noise(self, batch: torch.Tensor, repeated_theta: torch.Tensor):
return 2 * (torch.rand_like(batch, device=self.device) - 0.5) * repeated_theta
def COmPute_gap(self, pABar: float):
return 2 * (pABar - 0.5)
17
Under review as a conference paper at ICLR 2022
def ancer_optimization(
model: torch.nn.Module, batch: torch.Tensor,
certificate: Certificate, learning_rate: float,
isotropiJtheta: torch.Tensor, iterations: int,
samples: int, kappa: float, device: Str = "Cuda:0"):
"""Optimize batch using ANCER, assuming isotropic initialization point.
Args :
model : trained network
batch: inputs to certify around
certificate: instance of desired certification object
Iearning_rate: optimization learning rate for ANCER
isotropic_theta: initialization isotropic value per input in batch
iterations : number of iterations to run the optimization
samples : number of samples per input and iteration
kappa: relaxation hyperparameter
"""
batch_size = batch.shape[0]
img_size = np.prod(batch.shape[1:])
#	define a variable, the optimizer, and the initial sigma values
theta = Variable(isotroPiJtheta, requires_grad=True).to(device)
optimizer = torch.optim.Adam([theta], lr=learning_rate)
initial_theta = theta.detach().clone()
#	reshape vectors to have ‘‘samples ‘‘ per input in batch
new_shape = [batch_size * samples]
new_shape.extend(batch[0].shape)
new_batch = batch.repeat((1, samples, 1, 1)).view(new_ShaPe)
#	solve iteratively by projected gradient ascend
for _ in range(iterations):
theta_repeated = theta.repeat(1, samples, 1, 1).view(new_ShaPe)
#	Reparameterization trick
noise = CertifiCate.sample_noise(new_batch, theta_repeated)
out = model(
new_batch + noise
).reshape(batch_size, samples, -1).mean(dim=1)
vals, _ = torch.topk(out, 2)
gap = CertifiCate.compute_proxy_gaP(VaIS)
prod = torch.prod(
(theta.reshape(batch_size, -1))**(1/img_size), dim=1)
proxy_radius = prod * gap
radius_maximizer =- (
proxy_radius.sum () +
kappa *
(torch.min(theta.view(batch_size, -1), dim=1).values*gap).sum()
)
radius_maximizer.backward()
optimizer.step()
#	project to the initial theta
with torch.no_grad():
torch.max(theta, initial_theta, out=theta)
return theta
Listing 1: Python implementation of the AnCer optimization routine using PyTorch Paszke et al.
(2019)
D Memory-based Certification for AnCer
To guarantee the soundness of the AnCer classifier, we use an adapted version of the data-dependent
memory-based solution presented in Alfarra et al. (2020). The modified algorithm involves a
post-processing certification step that obtains adjusted certification statistics based on the memory
18
Under review as a conference paper at ICLR 2022
procedure from Alfarra et al. (2020) (see the original paper for more details). We present an adapted
version to AnCer of this post-processing memory-based step in Algorithm 1.
Algorithm 1: Memory-Based Certification
Input: input point XN +1, certified region RN +1, prediction CN +1, and memory M
Result: Prediction for xN+1 and certified region at xN+1 that does not intersect with any
certified region in M.
for (xi , Ci , Ri) ∈ M do
if CN+1 6= Ci then
if xN+1 ∈ Ri then
I return Abstain, 0
else if MaxIntersect(RN +1, Ri) and Intersect(RN +1, Ri) then
R0N +1 = LargestOutSubset(Ri, RN+1);
RN +1 - RN+1;
end
add (xN+1, CN+1, RN +1) to M;
return CN+1, RN +1;
Note that the proposed certified region RN+1 emerges from our certification bounds presented in
Sections 4.1 and 4.2. There are a few differences between our proposed Algorithm 1 with respect to
the original variant presented in Alfarra et al. (2020). The first is that we remove the computation of
the largest certifiable subset of a certified region RN+1 when there exists an i such that xN+1 ∈ Ri
with a different class prediction, i.e. (LargestInSubset in Alfarra et al. (2020)) due to the
complexity of the operation in the anisotropic case. As an example, it is generally difficult to find
the largest volume ellipsoid contained in another ellipsoid. Due to this complexity, we choose to
simply Ab stain instead. Given the high dimensionality of the data, empirically, we never found a
certificate in this situation within our experiments. Further, to ease the computational burden of the
Intersect function, we introduce and instantiate the function MaxIntersect first which checks
whether the `p-ball over-approximation of the region RN+1 intersects with a `p over-approximation
of Ri . This follows since when the `p balls over-approximation to the anisotropic regions RN+1
and Ri do not intersect, then RN+1 and Ri do not intersect either. Only in cases in which those
over-approximation regions intersect, we run the more expensive Intersect procedure. We present
practical implementations for MaxIntersect, Intersect and LargestOutSubset for the
ellipsoids and generalized cross-polytopes considered in this paper.
D.1 IMPLEMENTING MAXINTERSECT (RA, RB) IN THE ELLIPSOID AND GENERALIZED
Cross-Polytope Cases
Given the two regions RA and RB, consider 'p-ball approximations of those regions, RB = {x ∈
Rn ： kX - akp ≤ ra} and RB = {x ∈ Rn : ∣Ix - b∣∣p ≤ rb} such that RA ⊆ RA and RB ⊆RB.
Lemma 1. If ∣∣a 一 b∣p > r。+ r ,then RA ∩ RB = 0.
Proof. For the sake of contradiction, let ∣∣a 一 b∣p > r。+ rb and X ∈ RA ∩ RB. Then, We have that
∣X - a∣ ≤ ra and ∣X - b∣ ≤ rb. However:
r。 + rb < ∣a - b∣p ≤ ∣X - a∣p + ∣X - b∣p ≤ r。 + rb,
forming a contradiction. Thus, RA ∩ RB = 0, which in turn implies RA ∩ RB = 0 since RA and
RB are subsets of RA and RB, respectively.	□
This forms a fast, maximum intersection check for ellipsoids, i.e. p = 2, and generalized cross-
polytopes, i.e. p = 1. The MaxIntersect function returns False if ∣a - b∣p > r。 + rb, and
True otherwise.
D.2 IMPLEMENTING INTERSECT(RA, RB) IN THE ELLIPSOID CASE
The problem of efficiently checking if two ellipsoids intersect is not trivial. We rely on the work
of Ros et al. (2002); Gilitschenski & Hanebeck (2012) with missing proofs from Gilitschenski &
Hanebeck (2012) for completeness.
19
Under review as a conference paper at ICLR 2022
Lemma 2. Let RA = {x ∈ Rn : (x-a)>A(x-a) ≤ 1} and RB = {x ∈ Rn : (x-b)>B(x-b) ≤
1} define two ellipsoids centered at a and b, respectively. We have that R = {x : t(x - a)>A(x -
a) + (1 - t)(x - b)>B(x - b) ≤ 1} for any t ∈ [0, 1] satisfies RA ∩ RB ⊆ R ⊆ RA ∪ RB.
Proof. By considering the convex combination of the left-hand side of the inequalities defining the
regions RA and RB , it becomes obvious that x ∈ RA ∩ RB =⇒ x ∈ R, concluding the left side
of the property. As for the right side, it suffices to show that if x ∈/ RA and x ∈ R then x ∈ RB
and, similarly, that if x ∈/ RB and x ∈ R then x ∈ RA . We show the first case since the second
follows by symmetry. Without loss of generality, we assume that a = b = 0n . Now, let x be such
that x>Ax > 1 and tx>Ax + (1 - t)x>Bx ≤ 1 since x ∈/ RA and x ∈ R. Then, since x ∈ R, we
have that (1 一 t)x>Bx ≤ 1 一 tx> Ax ≤ 1 since x> Ax > 1 which implies that X ∈ Rb.	□
Note that the previous result holds without loss of generality when for the radius 1 as the radius can
be absorbed in A and B. As the following Lemma was shown by Gilitschenski & Hanebeck (2012)
without proof, we complement it below for completeness.
Lemma 3. Theset R is equivalent to thefollowing ellipsoidR = {x : (X — m)>Et(x — m) ≤ K(t)}
where Et = tA +(1 — t)B, m = E-1 (tAa +(1 — t)Bb), andK(t) = 1 — ta> Aa — (1 — t)b>Bb +
m>Etm.
Proof.
t(x — a)> A(x — a) + (1 — t)(x — b)>B(x — b) ≤ 1
⇔x> (tA + (1 — t)B) x — 2x> (tAa + (1 — t)Bb) ≤ 1 — ta> Aa — (1 — t)b>Bb
'---------{-----------}
Et
'-----------{------------}
Et m
⇔(x — m)>Et(x — m) ≤ 1 — ta>Aa — (1 — t)b>Bb + m>Etm
The last equality follows by adding and subtracting m> Et m and concluding the proof.	□
Proposition 8. The set of points satisfying R fort ∈ (0, 1) is either an empty set, a single point, or
the ellipsoid R.
Proof. We first observe that since A and B are positive definite, then Et is positive definite. Then
observe that for a choice of t ∈ (0, 1) such that K(t) < 0, the set R is an empty set, and since
R ⊇ RA ∩ RB, the two sets do not intersect. If K(t) = 0, then the only point satisfying R is the
center at m. Following a similar argument, then the two ellipsoids intersect at a point. At last for a
choice of t such that K(t) > 0, then R defines an ellipsoid.	□
As per Theorem 8, it suffices to find some t ∈ [0, 1] under which K(t) < 0 to guarantee that
the ellipsoids do not intersect. To that end, we solve the following convex optimization problem:
t* = argmint∈[o,i]K(t) and check the condition if K(t*) < 0. Moreover, as shown by Ros et al.
(2002); Gilitschenski & Hanebeck (2012) K(t) is convex in the domain t ∈ (0, 1). With several
algebraic manipulations, one can show that K(t) has the following equivalent forms:
K(t) = 1 — ta>Aa — (1 —t)b>Bb+m>Etm
K(t) = 1 —t(1 — t)(b — a)>BEt-1A(b — a)
K(t) = 1 — (b — a)>
1	1	-1
口 B-1 + t AT	(b - a)
Observe that for ANCER, we have that both A and B to be diagonals with diagonal elements
{Aii}in=1 and {Bii}in=1, respectively, resulting in the following simple form for K(t):
n
1—X(bi—ai)2
K(t)
t(1 — t) AiiBii
tAii + (1 — t)Bii
The Intersect function in the ellipsoid case returns False if there exists a t ∈ (0, 1) such that
K(t) < 0, i.e. ellipsoids do not intersect, and True otherwise.
20
Under review as a conference paper at ICLR 2022
D.3 IMPLEMENTING INTERSECT(RA, RB) IN THE GENERALIZED CROSS-POLYTOPE CASE
Let RA and RB be two generalized cross-polytopes RA = {x ∈ Rn : kA(x - a)k1 ≤ 1} and
RB = {x ∈ Rn : kB(x - b)k1 ≤ 1}, where A and B are positive definite diagonal matrices with
elements {Aii}in=1 and {Bii}in=1, respectively. We are interested in deciding whether RA and RB
intersect. However, given the conservative context in which Intersect is used in Algorithm 1, we
only need to make sure that the function only returns False if it is guaranteed that RA ∩ RB = 0.
As such, we are able to simplify the complex problem of generalized cross-polytope intersection
to the much simpler one of ellipsoid over-approximation intersection. We do this by considering
the over-approximation, i.e. superset, ellipsoids RA = {χ ∈ Rn : IlA(X — a)k2 ≤ 1} and
RB = {x ∈ Rn ： kB(X — b)∣∣2 ≤ 1}, and perform the ellipsoid intersection check presented in
Appendix D.2. If RA ∩ RB = 0, then this implies that RA ∩ RB = 0 and We can safely return
False. Otherwise, we conservatively assume the generalized cross-polytopes intersect, and return
True, triggering the reduction procedure detailed in Appendix D.5.
D.4 IMPLEMENTING LARGESTOUTSUBSET(RA, RB) IN THE ELLIPSOID CASE
Given tWo ellipsoids RA = {X ∈ Rn : (X - a)>A(X - a) ≤ 1} and RB = {X ∈ Rn : (X -
b)>B(X - b) ≤ 1} that do intersect Where A and B are positive definite diagonal matrices, the task
is to find the largest possible ellipsoid RB centered at b such that RB ⊆ RB where RA ∩ RB = 0.
Finding a maximum ellipsoid that satisfies those conditions is not trivial, so instead We consider a
maximum enclosing '2-ball of Rb, RB = {x ∈ Rn : kX — b∣2 ≤ r}, that does not intersect Ra.
To obtain this ball, we project the center of RB, b, to the ellipsoid RA. Particularly, we formulate
the problem as the projection of a vector y = b - a onto an ellipsoid with the same shape as RA
centered at 0n . This is equivalent to solving the following optimization problem for a symmetric
positive definite matrix A:
mXn1 kχ - yk2
s.t. X>AX ≤ 1.
Note that the objective function is convex, and the constraint forms a convex set. Forming the
Lagrangian to this problem, we obtain:
L(χ, λ) = 2 ∣∣χ 一 yk2 + λ (x>Ax — 1),
where λ > 0. Therefore, the global optimal solution must satisfy the KKT conditions below:
∂L
=0 → X = (2λA + I)	y,
∂X
∂L
∂λ
0 → y> (2λA+I)->A(2λA+I)-1y 一 1 = 0.
、-------------V------------}
f(λ)
Thus, to project the vector y on our region the ellipsoid characterized by A, one needs to solve
the scalar optimization f (λ) = 0 then substitute back in the formula of x*. Further, given A =
diag(A11, . . . , Ann), we can simplify the problem to:
f (λ)=X r+‰ -1=0.
Once x* is obtained, we can define the maximum radius of the '2-ball centered at b that does not
intersect RA as:
r* = k(X* + a) 一 bk2 一 ,
for an arbitrarily small e. Finally, we obtain RB as the maximum ball contained within RB that has
a radius smaller than r*, that is:
RB = {χ ∈ Rn ： kX — b∣2 ≤ min{r*, minBii}}.
Bi
Note that while choosing the radius of RB to be r* guarantees that RB ∩ RA = 0, this does not
guarantee that RB ⊆ Rb. To guarantee both properties, we take the minimum of both r* and
mini Bii. This approach finds the solution to the projection of the point to the ellipsoid {X ∈ Rn :
X>AX ≤ 1}; it does not work for the case in which b ∈ RA, since the problem would be trivially
solved by setting X* = y. Thus, our classifier must abstain in that situation.
21
Under review as a conference paper at ICLR 2022
D.5 IMPLEMENTING LARGESTOUTSUBSET(RA, RB) IN THE GENERALIZED
Cross-Polytope Case
Let RA and RB be two generalized cross-polytopes RA = {x ∈ Rn : kA(x - a)k1 ≤ 1} and
RB = {x ∈ Rn : kB(x - b)k1 ≤ 1}, where A and B are positive definite diagonal matrices with
elements {Aii}in=1 and {Bii}in=1, respectively. The task is to find the largest possible generalized
cross-polytope RB centered at b such that RB ⊆ RB where RA ∩ RB = 0.
As with the ellipsoid case, solving this problem for a generalized cross-polytope is not trivial,
so instead we consider a maximum enclosing cross-polytope (i.e., 'ι-ball) of RB = {x ∈ Rn :
||x 一 b∣∣ ι ≤ r} that does not intersect RA and is a subset of Rb. To obtain this 'ι-ball, we project
the center of RB , b, to the generalized cross-polytope RA in a similar fashion to the ellipsoid case in
Appendix D.4. We formulate the problem as the projection of the vector y = b 一 a to the 0n centered
generalized cross-polytope {x ∈ Rn : |Ax|1 ≤ 1}.
Lemma 4. Consider the hyperplane H = {x ∈ Rn : w>x 一 k = 0} and a point y ∈ Rn. The `2
projection of y on the hyperplane is the point x* = y — (w>y-k)w∕kwk2.
Proof. We define the projection problem in a similar fashion to the ellipsoid case:
12	>
min - ∣∣x — y∣2	s.t. w > x — k = 0,
and obtain the Lagrangian as L(x, λ) = 11 ∣∣x — y∣2 + λ(wτx — k), from where we get (using the
KKT conditions): x* = y — λ*w and λ* = w>y-k∕kw∣∣2; thus obtaining: x* = y — (W ∣∣y-k)w.	□
kwk2
While this formulation does not yield the closest point from a hyperplane when measured with the `1
norm, the fact that ∣x 一 x*∣∣ι ≥ IlX — x*∣∣2 implies the certification set obtained in the '1 norm via this
method is a subset of the '1-ball of the minimum projection point. Crucially, this '1 projection has
the advantage of having a closed-form solution, while an `1 one would require solving the problem
using an iterative linear programming solver. As such, for the sake of computational complexity,
we decided to use this projection, despite the sub-optimality of the result from the '1 perspective.
Empirically, we have found this does not affect our results.
Since the set of vertices of the generalized cross-polytope {x ∈ Rn : ∣Ax∣1 ≤ 1} is given by
{ei/Aii, —ei/Aii}in=1, and considering the distance between the projections and the original y, the
hyperplane that minimizes it is defined by the set of vertices {sign(yi)ei/Aii}in=1. By writing it as a
system ofn equations, we obtain the hyperplane defined by w = [—sign(y1)A11, ..., —sign(yn)Ann]
and k = 1. Finally, after computing x* as per Lemma 4, we can define the maximum radius of the
'1 -ball centered at b that does not intersect RA as:
r* = ∣(x* + a) — b∣1 — ,
for an arbitrarily small e. Finally, and similar to the ellipsoids case, we obtain RB as the maximum
generalized cross-polytope contained within RB that has a radius smaller than r*, that is:
RB = {x ∈ Rn : kX — b∣ι ≤ min{r*, minBii}}
Bi
Similar to before, to guarantee that the '1 ball RB is still a subset to Rb, we take the minimum
between r* and mini Bii to be the radius of RB. AS with the ellipsoid case, this approach does not
work for the case in which b ∈ RA , since the assumption of the closest plane to y would not hold.
Thus, our classifier must abstain in that situation.
E	Experimental Setup
The experiments reported in the paper used the CIFAR-10 Krizhevsky (2009)3 and ImageNet Deng
et al. (2009)4 datasets, and trained ResNet18, WideResNet40 and ResNet50 networks He et al. (2016).
3Available here (url), under an MIT license.
4Available here (url), terms of access detailed in the Download page.
22
Under review as a conference paper at ICLR 2022
Experiments used the typical data split for these datasets found in the PyTorch implementation Paszke
et al. (2019). The procedures to obtain the baseline networks used in the experiments are detailed in
Appendix E.1 and E.2 for ellipsoids and generalized cross-polytopes, respectively. Source code to
reproduce the AnCer optimization and certification results of this paper is available as supplementary
material.
Isotropic DD Optimization. We used the available code of Alfarra et al. (2020)5 to obtain the
isotropic data dependent smoothing parameters. To train our models from scratch, we used an adapted
version of the code provided in the same repository.
Certification. Following Cohen et al. (2019); Salman et al. (2019a); Zhai et al. (2019); Yang et al.
(2020); Alfarra et al. (2020), all results were certified with N0 = 100 Monte Carlo samples for
selection and N = 100, 000 estimation samples, with failure a probability of α = 0.001.
E.1 Ellipsoid certification baseline networks
In terms of ellipsoid certification, the baselines we considered were Cohen Cohen et al. (2019)6,
SMOOTHADV Salman et al. (2019a)7 and MACER Zhai et al. (2019)8.
In the CIFAR-10 experiments, we used a ResNet18 architecture, instead of the ResNet110 used
in Cohen et al. (2019); Salman et al. (2019a); Zhai et al. (2019) due to constraints at the level of
computation power. As such, we had to train each of the networks from scratch following the
procedures available in the source code of each of the baselines. We did so under our own framework,
and the training scripts are available in the supplementary material. For the ImageNet experiments
we used the ResNet50 networks provided by each of the baselines in their respective open source
repositories.
We trained the ResNet18 networks for 120 epochs, with a batch size of 256 and stochastic gradient
descent with a learning rate of 10-2, and momentum of 0.9.
E.2 Generalized Cross-Polytope certification baseline networks
For the certification of generalized cross-polytopes we considered RS4A Yang et al. (2020)9. As
described in RS4A Yang et al. (2020), We take λ = σ∕√3 and report results as a function of σ for
ease of comparison.
As With the baseline, We ran experiments on CIFAR-10 on a WideResNet40 architecture, and
ImageNet on a ResNet50 Yang et al. (2020). HoWever, due to limited computational poWer, We
Were not able to run experiments on the Wide range of distributional parameters the original Work
considers, i.e. σ = {0.15, 0.25, 0.5, 0.75, 1.0, 1.125, 1.5, 1.75, 2.0, 2.25, 2.5, 2.75, 3.0, 3.25, 3.5}
on CIFAR-10 andσ = {0.25, 0.5, 0.75, 1.0, 1.125, 1.5, 1.75, 2.0, 2.25, 2.5, 2.75, 3.0, 3.25, 3.5} on
ImageNet. Instead, and matching the requirements from the ellipsoid section, We choose a subset of
σ = {0.25, 0.5, 1.0} and performed our analysis at that level.
While the trained models are available in the source code of RS4A, We ran into several issues When
We attempted to use them, the most problematic of Which being the fact that the clean accuracy of
such models Was very loW in both the WideResNet40 and ResNet50 ones. To avoid these issues We
trained the models from scratch, but using the stability training loss as presented in the source code
of RS4A. All of these models achieved clean accuracy of over 70%.
FolloWing the procedures described in the original Work, We trained the WideResNet40 models With
the stability loss used in Yang et al. (2020) for 120 epochs, With a batch size of 128 and stochastic
gradient descent With a learning rate of 10-2, and momentum of 0.9, along With a step learning rate
scheduler With a γ of 0.1. For the ResNet50 netWorks on ImageNet, We trained them from scratch
5Data Dependent Randomized Smoothing source code available here
6 Cohen source code available here.
7 SmoothAdv source code available here.
8 MACER source code available here.
9RS4A source code available here.
23
Under review as a conference paper at ICLR 2022
with stability loss for 90 epochs with a learning rate of 0.1 that drops by a factor of 0.1 after each 30
epochs and a batch size of 256.
F Superset argument
The results we present in Section 7 support the argument that AnCer achieves, in general, a certificate
that is a superset of the Fixed σ and Isotropic DD ones. To confirm this at an individual test set
sample level, We compare the '2, '1, 'ς and 'λ certification results across the different methods, and
obtain the percentage of the test set in which AnCer performs at least as well as all other methods in
each certificates of the samples. Results of this analysis are presented in Tables 4 and 5.
For most netWorks and datasets, We observe that ANCER achieves a larger 'p certificate than the
baselines in a significant portion of the dataset, shoWcasing the fact that it obtains a superset of
the isotropic region per sample. This is further confirmed by the comparison With the anisotropic
certificates, in Which, for all trained netWorks except MACER in CIFAR-10, AnCer’s certificate is
superior in over 90% of the test set samples.
Table 4: Superset in top-1 '2 and '2Σ (rounded to nearest percent)
	% ANCER `2 is the best	% ANCER 'ς is the best
CIFAR-10: COHEN	83	93
CIFAR-10: SmoothAdv	73	90
CIFAR-10: MACER	50	69
ImageNet: Cohen	94	96
ImageNet: SmoothAdv	90	93
Table 5: Superset in top-1 '1 and '1Λ (rounded to nearest percent)
	% ANCER `1 is the best	% AnCer 'λ is the best
CIFAR-10: RS4A	100	100
ImageNet: RS4A	97	99
G EXPERIMENTAL RESULTS PER σ
G. 1 CERTIFYING ELLIPSOIDS - '2 AND '2Σ CERTIFICATION RESULTS PER σ
In this section We report certified accuracy at various '2 radii and '2Σ proxy radii, folloWing the metrics
defined in Section 7, for each training method (Cohen Cohen et al. (2019), SmoothAdv Salman
et al. (2019a) and MACER Zhai et al. (2019)), dataset (CIFAR-10 and ImageNet) and σ (σ ∈
{0.12, 0.25, 0.5, 1.0}). Figures 6 and 7 shoWs certified accuracy at different '2 radii for CIFAR-10
and ImageNet, respectively, Whereas Figures 8 and 9 plot certified accuracy and different '2Σ proxy
radii for CIFAR-10 and ImageNet, respectively.
G.2 CERTIFYING ELLIPSOIDS - '1 AND '1Λ CERTIFICATION RESULTS PER σ
In this section We report certified accuracy at various '1 radii and '1Λ proxy radii, folloWing the metrics
defined in Section 7, for RS4A, dataset (CIFAR-10 and ImageNet) and σ (σ ∈ {0.25, 0.5, 1.0}).
Figures 10 and 11 shoWs certified accuracy at different '1 radii for CIFAR-10 and ImageNet, respec-
tively, Whereas Figures 12 and 13 plot certified accuracy and different '1Λ proxy radii for CIFAR-10
and ImageNet, respectively.
G.3 WHY DOES ANCER IMPROVE UPON ISOTROPIC DD’S 'p CERTIFICATES ?
As observed in Sections 7.1 and 7.2, ANCER’s '2 and '1 certificates outperform the corresponding
certificates obtained by Isotropic DD. To explain this, We compare the '2 certified region obtained
24
Under review as a conference paper at ICLR 2022
>OΛ⅛OOΛ pəwəɔ⅛sn8e p0>!≡七 3。
Cohen (σ = 0.5)
Cohen (σ = 1.0)
——Fixed (ACR=0.668)
Isotropic (ACR=0.393)
——ANCER (ACR=0.561)
MACER (σ = 1.0)
——Fi)æd (ACD.272)
ISOtropiC (ACR=0.345)
——ANCER (ACR=0.363)
1	2	3
⅛ ∏adius
6 4 2
- a -
Ooo
⅛eln8e pay 一七 əɔ
——Fixed (ACR=0.429)
Isotropic (ACR=0.461)
——ANCER (ACR=0.424)
——Fi)æd (ACD67)
ISOtropiC (AC R=0,641)
——ANCER (ACR=0.56)
??|2|.1
Ooo
&el n8e P2 一七 əɔ
0.0
0.0	0.5	1-0	1.5
h radius
0.0
0.0	0.5	1-0	2.0	2.5
I2 radius
0.0	J''
0	1	2	3	4	5
∕2 radius
Figure 6:	CIFAR-10 certified accuracy as a function of `2 radius, per model and σ (used as initializa-
tion in the isotropic data-dependent case and AnCer).
1412
O O
Aanooe pəugjəɔ
Cohen (σ = 0.25)
——FiXed(AeD.47)
Isotropic (ACR=0.527)
——ANCER (ACR=0.653)
Cohen (σ = 0.5)
0.6
0.4
0.2
0.0
0.0	0.5	1-0	1.5
b radius
0-∞ 0.25 0.50 0.75 1.00
/2 radius
SmoothAdv (σ = 0.25)
Aanooe pəE七 3。
0.0	0.5
1-0	1.5
t2 radius

Figure 7:	ImageNet certified accuracy as a function of `2 radius, per model and σ (used as initialization
in the isotropic data-dependent case and AnCer).
by ANCER, defined in Section 6 as {δ : kδk2 ≤ mini σixr(x, Σx)}, to the one by Isotropic DD
defined as {δ : kδk2 ≤ σxr(x, σx)}. We observe that the radius of both of these certificates can
be separated into a σ-factor (σx vs. σmx in = mini σix) and a gap-factor (r(x, σx) vs. r(x, Σx)). We
posit the seemingly surprising result can be attributed to the computation of the gap-factor r using an
anisotropic, optimized distribution. However, another potential explanation would be that AnCer
benefits from a prematurely stopped initialization provided by Isotropic DD, thus achieving a better
σmx in than the isotropic σx when given further optimization iterations.
To investigate this, we take the optimized parameters from the Isotropic DD experiments on
SMOOTHADV for an initial σ = 0.25 on CIFAR-10, and run the optimization step of Isotropic
DD for 100 iterations more than its default number of iterations from Alfarra et al. (2020), so
as to match the total number of optimization steps between Isotropic DD and AnCer. The
histograms of σx or σmx in and the gap-factor r, i.e. the two factors from the `2 certifica-
tion results, are presented in Figure 14. While σx for Isotropic DD is similar in distribution
to ANCER’s σmx in, the distribution of the two gaps, r(x, σx) and r(x, Σx), are quite different.
25
Under review as a conference paper at ICLR 2022
&el n8e P2 一七 əɔ
Cohen (σ = 0.12)
——FiXedSCa=O263)
——Isotropic (ACR=0299)
——ANCER(AC能0.676)
⅛ prcxyvolume
⅛eln8e P2 一七 əɔ
Cohen (σ = 0.25)
、——Fi«d (ACR=0.402)
Isotropic (ΛCR=0.44)
——ANCER (ACR=0Λ73)
店 Pro 呼 volume
25?|2|.1
Uooo
⅛eln8e P2 一七 əɔ
Cohen (σ= 1.0)
——Fixed (ACR=0.497)
IsotropiC 依 CG=O.824)
'—— ANCER (ACR=1.163)
1	2	3	4	5
Il proxy volume
Aoe-n∞e ps>≡= əɔ
0.0	0.5	1-0	1.5	2.0	2.5
Zfproxyvolume
MACER (σ = 0.12)
Aanooe pəugjəɔ
0	1	2	3	4	5
/Jproxyvolume
Aoe-n∞e pəz七 əɔ
SmgthAdv (σ = 0.25)
、——Fied (ACR=0.454)
Isotropic (ACR=0.561)
\\ ——ANCER (ACA=O.896)
0.5	1-0	1.5	2.0	2.5
Il proxy volume
MACER (σ = 0.25)
S 4 2
Ooo
Aanooe pəugjəɔ
0.5	1-0	1.5
if proxy volume
>0Λ⅛08 P≡U=SJ≡O
一？？I2l.1
-OOO
Aanooe pəz七 əɔ
MACER (σ = 0.5)
0.0
0.0	0.5	1-0	2.0	25
/Jprcxyvolume
??|2|.1
Ooo
Aanooe pəugjəɔ
SmoothAdv (σ = 1.0)
——Fixed (ACR=0.565)
∖J—— Isotropic t4CR=0.694)
∖ ——ANCER (ACR=0.992)
Il proxy volume
MACER (σ= 1.0)
MFixed (ACR=0.668)
Isotropic 80=0.393)
ANCER (ACR=0.609)
1	2	3	4	5
if proxy volume
Figure 8:	CIFAR-10 certified accuracy as a function of 'ς proxy radius, per model and σ (used as
initialization in the isotropic data-dependent case and AnCer).
Cohen (σ = 0.25)
——Fixed (<4CR=0.47)
IsotropiC (ACR=0.527)
——ANCER (ACR=0.989)
0.0	0.5	1,0	1.5	2.0	2.5
tl pro>y volume
SmoothAdv (σ = 0.25)
0.0
0.0	0.5	1-0	1.5	2.0	2.5
妗 Pioxyvolume
Sm∞thAdv (σ = 0.5)
产
10.3
即2
§0.1
0.0
O
Cohen (σ = 1.0)
J———Fixed t4CR=0.864)
Isotrapic t4CR=0.995)
——ANCER (ACR=1.531)
12	3	4
proxy volume
1412
O O
⅛eln∞e pəz七 əɔ
0.0
0	12	3
蟾 Pro呼 VohJme
⅛eln∞e pay-七 əɔ
Fied (ACR=0.518)
IsotropiC(ACR=O.GI)
——ANCER (ACR=0.977)
0.2 ——Fixed (ΛCR=0.816)
IsotropiC 依媒=0.921)
——ANCER (ACR=1.268) S一
0.0
0	12	3
Iipioxyvolume
Figure 9:	ImageNet certified accuracy as a function of '∑ proxy radius, per model and σ (used as
initialization in the isotropic data-dependent case and AnCer).
In particular, the AnCer certification gap is significantly
larger when compared to Isotropic DD, and is the main
contributor to the improvement in the '2-ball certificate of
ANCER. That is to say, ANCER generates Σx that is better
aligned with the decision boundaries, and hence increases
the confidence of the smooth classifier.
H	Visual Comparison of
Parameters in Ellipsoid Certificates
Anisotropic certification allows for a better characteriza-
tion of the decision boundaries of the base classifier f. For
example, the directions aligned with the major axes of the
ellipsoids kδ kΣ,2 = r, i.e. locations where Σ is large, are,
by definition, expected to be less sensitive to perturbations
Figure 14: Histograms of the values
of the σ-factor (left) and gap r (right)
obtained by AnCer initialized with
Isotropic DD, and Isotropic DD when
allowed to run for 100 iterations more
than the baseline. Vertical lines plot the
median of the data.
26
Under review as a conference paper at ICLR 2022
0.8
0-∞ 0.25 0.50 0.75 1.00 1.25
radius
• a -
Ooo
>0Λ⅛00Λ P≡U=SJ≡O
RS4A
Agn∞e psy=e≡o
0.0	0.5	1-0	1.5
iι radius
——Fixed (ACR=0.397)
IsotropiC (ACR=0.614)
——ANCER (ACR=O .629)
0.0
0.0	0.5	1-0	1.5	2.0
/ɪ radius
Figure 10:	CIFAR-10 certified accuracy as a function of `1 radius per σ (used as initialization in the
isotropic data-dependent case and AnCer).
RS4A (σ=0∙25)
l-6l.4“
Ooo
⅛eln∞e pəz七 əɔ
——FiXed(AClm76)
Isotropic (ACR=0.233)
——ANCER(ACR=0.233)
_ __ ___
⅛e,nOOe psli=⅛≡o
RS4A(σ = 0.5)
0.0
0.0	02	0.4	0.6
t1 radius
——Fied (ACR=0.642)
Isotropic (AC R=0.681)
——ANCER (ACR=0.68)
RS4A (σ=1.0)
0.0	0-1	0-2	0.3	0.4
iι radius
0.00	0.25	0.50	0.75	1.00
tι radius
Figure 11:	ImageNet certified accuracy as a function of `1 radius per σ (used as initialization in the
isotropic data-dependent case and AnCer).
compared to the minor axes directions. To visualize this concept, Figure 15 shows CIFAR-10 images
along with their corresponding optimized '2 isotropic parameters obtained by Isotropic DD, and 'ς
anisotropic parameters obtained by AnCer. First, we note the richness of information provided by
the anisotropic parameters when compared to the `2 worst-case, isotropic one. Interestingly, pixel
locations where the intensity of Σ is large (higher intensity in Figure 15) are generally the ones
corresponding least with the underlying true class and overlapping more with background pixels.
A particular insight one can get from AnCer certification is that the decision boundaries are not
distributed isotropically around each input. To quantify this in higher dimensions, we plot in Figure 16
a histogram of the ratio between the maximum and minimum elements of our optimized smoothing
parameters for the experiments on SmoothAdv (with an initial σ = 1.0) on CIFAR-10. We note that
this ratio can be as high as 5 for some of the input points, meaning the decision boundaries in that
case could be 5 times closer to a given input for some directions than others.
I Non data-dependent Anisotropic Certification
As mentioned briefly in Section 6, it is our intuition that anisotropic certification requires a data-
dependent approach, as different points will have fairly different decision boundaries and the certified
regions will extend in different directions (as exemplified in Figure 1).
To validate this claim, we perform certification of SmoothAdv Salman et al. (2019a) with an initial
σ = 1 on CIFAR-10 using a Σ which is the average of all the optimized Σx. The results of the
certified accuracy, ACR and ACR are presented in Table 6, along with the same results for the
methods reported in the main paper. As can be observed, moving away from the data-dependent
certification in the anisotropic scenario leads to a significant performance drop in terms of robustness.
J Theoretical and Empirical Comparison with Mohapatra et al.
(2020)
In regards to the theoretical results, unfortunately the certified regions of Mohapatra et al. (2020)
do not exhibit a closed form solution similarly to ours. Thus, a direct theoretical volume bound
comparison is not possible.
As for the empirical comparison, ANCER’s performance on both `2 and `1 certificates far out-does
that of Mohapatra et al. (2020). For example, with `2 certificates at a radius of 0.5, Cohen certified
with ANCER achieves 77% certified accuracy (see Table 1) while Mohapatra et al. (2020) achieves
under 60% certified accuracy. Note that Mohapatra et al. (2020) has only a marginal improvement
27
Under review as a conference paper at ICLR 2022
RS4A(σ = 0.5)
——Fixed (4CR=0.212)
IsotropiC (ACG=O.479)
——ANCER (ACR=0.665)
RS4A (σ=0.25)
0-∞ 0.25 0.50 0.75 1.00 1.25
/Cproxywilume
Fiecl (ACR=0.397)
IsotropiC (ACR=0.614)
ANCER (ACR=0.781)
⅛eln8e pay=e30
——Fixed (ACR=0.737)
Isotropic (ACA=O.88)
——ANCεR(<4CR=1.016)
0.0
0.0	0.5	1.0	1.5	2.0	2.5
经 PrCXyV OhJme
0.0	0.5	1-0	1.5
Ifproxyvolume
Figure 12:	CIFAR-10 certified accuracy as a function of 'λ proxy radius per σ (used as initialization
in the isotropic data-dependent case and AnCer).
RS4A (σ=0∙25)
RS4A(σ = 0.5)
RS4A(σ=1.0)
——Fixed (ACR=0.176)
IsotropiC (ACR=0.233)
——ANCER(Ac能1.421)
• ∙ -
Uoo
Aoe-n∞e ps≈=e≡o
1	2
proxy volume
——Fied (ACR=0.339)
Isotrapic (ACR=0.378)
——ANCER(AC丘=1.053)
——Fixed f4CR=0Λ42)
Isotropic tACR=0.681)
——ANCER (ACR=1.075)
0.0	0.5	1-0	1.5	2.0
∕? proxy volume
0.0
0.0	0.5	1-0	1.5	2.0	2.5
Ia proxy volume
M0.2
Aoe-n∞e ps≈=e30
Figure 13:	ImageNet certified accuracy as a function of '，proxy radius per Q (used as initialization
in the isotropic data-dependent case and AnCer).
over Cohen et al. As for the `1 certificates, Mohapatra et al. (2020) uses the Gaussian distribution
of Cohen et al, resulting in worse performance than existing state-of-art in `1 Yang et al. (2020)
that uses a uniform distribution. Our approach improves further upon the performance of Yang
et al. (2020). For example, as per Table 2, RS4A with ANCER certification achieves 84% certified
accuracy at an `1 radius of 0.5, Yang et al. (2020) achieves 75% certified accuracy while Mohapatra
et al. (2020) achieves below 60%. However, we believe that the combination of both approaches,
ANCER and Mohapatra et al. (2020) can further boost the performance as also hinted on in the
abstract of Mohapatra et al. (2020) on the use of data-dependent smoothing.
28
Under review as a conference paper at ICLR 2022
Figure 15: Visualization of an input CIFAR-10 image x (top), and the optimized parameters σ
(middle) and Σ (bottom) - higher intensity corresponds to higher σ% in that pixel and channel - of the
smoothing distributions in the isotropic and anisotropic case, respectively.
0.6
0.5
0.4
0.3
0.2
0.1
0.0
2	3	4	5
Figure 16: Distribution of the maximum over the minimum ANCER σx at each dataset point for
SmoothAdv Salman et al. (2019a) on CIFAR-10 (for initial σ = 1.0)

Table 6: Comparison of different certification methods on SmoothAdv with an initial σ = 1.0 on
CIFAR-10.
CIFAR-10	SmoothAdv	0.0	Acc 0.25	uracy 0.5	@ `2 r 1.0	adius ( 1.5	%) 2.0	2.5	`2 ACR	'∑ ACR
	Fixed σ	45	40	35	25	16	9	5	0.565	0.565
	Isotropic DD	41	39	36	29	21	14	7	0.694	0.694
σ = 1.0	AnCer	44	43	41	35	26	15	8	0.871	0.992
	Average Σ	29	25	21	14	9	5	2	0.329	0.379
29