Under review as a conference paper at ICLR 2022
Dual Training of Energy-Based Models
with Overparametrized Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
Energy-based models (EBMs) are generative models that are usually trained via
maximum likelihood estimation. This approach becomes challenging in generic
situations where the trained energy is nonconvex, due to the need to sample the
Gibbs distribution associated with this energy. Using general Fenchel duality re-
sults, we derive variational principles dual to maximum likelihood EBMs with
shallow overparametrized neural network energies, both in the active (aka feature-
learning) and lazy regimes. In the active regime, this dual formulation leads to
a training algorithm in which one updates concurrently the particles in the sam-
ple space and the neurons in the parameter space of the energy at a faster rate.
We also consider a variant of this algorithm in which the particles are sometimes
restarted at random samples drawn from the data set, and show that performing
these restarts at every iteration step corresponds to score matching training. Using
intermediate parameter setups in our dual algorithm thereby gives a way to inter-
polate between maximum likelihood and score matching training. These results
are illustrated in simple numerical experiments.
1 Introduction
Energy-based models (EBMs) are explicit generative models which consider Gibbs measures de-
fined through an energy function f, with a probability density proportional to exp(-β f (x)), where
β is the inverse temperature. Such models originate in statistical physics (Gibbs, 2010; Ruelle,
1969), and have become a fundamental modeling tool in statistics and machine learning (Wain-
wright & Jordan, 2008; Ranzato et al., 2007; LeCun et al., 2006; Du & Mordatch, 2019; Song &
Kingma, 2021). Given data samples from a target distribution, the learning algorithms for EBMs
attempt to estimate an energy function f to model the samples density. The resulting learned model
can then be used to obtain new samples, typically through Markov Chain Monte Carlo (MCMC)
techniques.
The standard method to train EBMs is maximum likelihood estimation, i.e. the learned energy is the
one maximizing the likelihood of the target samples, within a certain function class. One generic
approach for this is to use gradient descent, where gradients may be approximated using MCMC
samples from the trained model. However, this is computationally difficult for highly non-convex
trained energies, due to ‘metastability’, ie the presence of large basins in the energy landscape that
trap trajectories for potentially exponential time. This has motivated a myriad of alternative losses to
learn EBM energies, such as the popular score matching; see (Song & Kingma, 2021) for a review.
All in all, such weaker losses come at the expense of a loss of statistical power, which motivates
exploring computationally efficient methods for EBM maximum-likelihood estimation.
EBMs also have structural connections with maximum entropy (maxent) models, which have been
studied for decades through Fenchel duality. Dai et al. (2019b) was the first work to leverage similar
duality arguments for maximum likelihood EBM training. However, their analysis is restricted to en-
ergies lying in RKHS balls (i.e. non-parametric linear models). Despite the appealing optimization
properties of RKHS, these spaces of functions typically only contain very smooth functions when
the dimension is large (Berlinet & Thomas-Agnan, 2004). A recent line of work—originating in
supervised learning—has considered an alternative based on shallow neural networks (Bach, 2017),
1
Under review as a conference paper at ICLR 2022
which admit a linear representation in terms of a measure over its parameters and are able to adapt to
hidden low-dimensional structures in the data. The statistical benefits of the obtained F1 or Barron
spaces have recently been studied in the context of shallow EBMs by Domingo-Enrich et al. (2021),
who show that they may outperform the RKHS models.
In this work, we focus instead on the computational aspects of training such shallow EBMs. Relying
on infinite-dimensional Fenchel duality results for the KL-regularized L2 and L∞ regression prob-
lems over probability measures (App. B), we recast the maximum likelihood training of F1 -EBMs
into a min-max problem over measures, and derive a gradient descent-ascent algorithm (Alg. 1)
in the associated metric spaces, based on the Wasserstein distance. Crucially, these schemes, de-
fined over idealised parametrisations requiring infinite number of neurons, admit a finite-particle
approximation, as in regression or classification. When viewed in terms of particle systems, the
dynamics evolve two interacting populations simultaneously: one over the neuron parameters, and
the other over the data space (Sec. 3). Moreover, our proposed algorithm naturally interpolates be-
tween the primal and dual formualtions, thanks to the relative time-scale between the minimization
and maximization steps, and is even able to interpolate between MLE and Score Matching. This
dual algorithm is evaluated experimentally in Sec. 5 in a well-calibrated high-dimensional teacher-
student environment, which allows us to assess our models against the ground-truth, and test the
effect of input dimension. Our experiments confirm that the dual algorithm converges significa-
tively faster than the primal one, suggesting that dual updates might bypass metastability despite
high-dimensional and non-convex energy landscapes.
Related work. Our work is based on general Fenchel duality results (App. B) that may be useful in
applications beyond the main focus of this paper (see App. D). These theorems are a generalization
of results stated in the compact case in Domingo-Enrich et al. (2021) in their Appendix D. Similar
duality results have been studied extensively in the area of maximum entropy (maxent) models
(reviewed in Ch. 12 of Mohri et al. (2012)). The first maxent duality principle was due to Jaynes
(1957). Maxent models have been applied since the 1990s in natural language processing and in
species habitat modeling among others, and studied theoretically especially since the 2000s (Altun
& Smola, 2006; Dud^ et al., 2007).
Recently Dai et al. (2019a) leveraged duality arguments in the context of maximum likelihood
EBMs, although in a form different from ours. Their duality result works in the more restrictive
setting of “lazy” energies lying in RKHS balls and probability measures with L2 densities, and they
derive it directly from a general theorem that works for reflexive Banach spaces (Ekeland & Temam
(1999), Ch. 6, Thm. 2.1). Our Fenchel duality results, which work for Borel probability mea-
sures and feature-learning (F1) energies, are more general because we must rely on measure spaces,
which are non-reflexive Banach spaces. Their algorithm is also different: they do not evolve gener-
ated samples, but rather use a transport parametrization of the energy. Dai et al. (2019b) expand the
work (Dai et al., 2019a) combining it with Hamiltonian Monte Carlo.
A precursor of modern machine learning EBMs were restricted Boltzmann machines (RBMs), first
trained via contrastive divergence or CD (Hinton, 2002) - which estimates the gradient of the log-
likelihood via approximate MCMC samples of the trained model. It later led to maximum likelihood
training of EBMs (see e.g. Xie et al. (2016; 2017); Du & Mordatch (2019) among many others). A
popular variant of CD is persistent contrastive divergence or PCD (Tieleman, 2008; Tieleman &
Hinton, 2009), in which the MCMC samples are evolved and reused over gradient computations
to be progressively equilibrated. Drawing a comparison with our work, our dual F1-EBM training
algorithm resembles PCD in that both evolve a set of samples over training iterations.
A vast array of EBM losses alternative to maximum likelihood have been developed in recent years
(Song & Kingma, 2021) with the goal of avoiding the MCMC procedure, which may be costly
for non-convex densities. Some successful ones are score matching (Hyvarinen, 2005) and related
methods such as denoising score matching (Vincent, 2011). Building on these, recent works have
achieved state of the art image generation (Song & Ermon, 2019; 2020; Ho et al., 2020; Song et al.,
2021). We derive the score matching algorithm for F1 energies and show a continuum of algorithms
interpolating between dual maximum likelihood and score matching training.
2
Under review as a conference paper at ICLR 2022
Finally, our work (in particular App. C) also has links with maximum mean discrepancy (MMD)
flows. MMDs are probability metrics that were first introduced in (Gretton et al., 2007; 2012) for
kernel two-sample tests, and that have enjoyed ample success with the advent of deep-learning-based
generative modeling as discriminating metrics (Li et al., 2015; Dziugaite et al., 2015; Li et al., 2017).
Among the MMD literature, the closest work to ours is (Arbel et al., 2019), which study theoretically
the convergence of unregularized MMD gradient flow (our equation (26) with β-1 = 0). In their
experiments, they observe that noisy updates (j3-1 > 0) are needed for good generalization. Our
work shows that their algorithm is exactly training maximum likelihood EBMs energies in an RKHS
ball of radius that depends on the noise level (see App. C).
2 Background and setup
In this section, we provide preliminary background on the functional spaces associated to over-
parametrized two-layer networks, and on EBMs and their training losses.
Notation. If V is a normed vector space, we use BV (β ) to denote the closed ball of V of radius
β , and BV := BV (1) for the unit ball. If K denotes a subset of the Euclidean space, P(K) is the
set of Borel probability measures, M(K) is the space of Radon (i.e. signed and finite) measures,
and M+ (K) is the set of non-negative Radon measures. If γ is a Radon measure over K, then
kγ kTV = JK d∣γ∣ is the total variation (TV) norm of γ, which turns M(K) into a Banach space.
Throughout the paper, and unless otherwise specified, σ : R → R denotes a generic non-linear
activation function. We use (•)+ : R → R to denote the ReLU activation, defined as (z)+ =
max{z, 0}. We use τ to denote a fixed base probability measure, possibly used with a subindex
to specify the space it is defined over. We use Sd ⊆ Rd+1 for the d-dimensional hypersphere
and log for the natural logarithm. We denote the Lebesgue measure by λ. Given two probability
measures ν, V0 ∈ P(K), DKL(VkV0) = JK log 怒 dν denotes the KL divergence from V0 to V and
H(v, V0) = - Rk log(%)dV is the cross-entropy.
2.1	Overparametrized two-layer neural network spaces
In this work, we will focus on dense function approximation classes generated by overparametrized
shallow neural networks. One can distinguish two canonical models, depending on the asymptotic
scaling regime. For further background on these regimes, we refer the reader to Chizat et al. (2019).
Kernel regime. Let X ⊆ Rd1, Θ ⊆ Rd2, φ : XX Θ → R, and 丁㊀ be a fixed base probability mea-
sure over Θ. We define F2 as the reproducing kernel Hilbert space (RKHS) of functions f : X → R
such that for some h ∈ L2(Θ,tθ), We have that, for all X ∈ X, f(x) = /㊀夕(x, θ)h(θ)dτθ(θ). The
RKHS norm of F2 is defined as kf kF2 = inf
khk
L2(Θ) | f (∙) = Jθ 9(∙, θ) h(θ) dτΘ(θ
where
∣∣hkL2(θ) ：= fθ ∣h(θ)∣2 dτθ(θ) (c.f. Bach (2017)). As an RKHS, the kernel of F2 is
k(x,y) = / q(x,θ)夕(x,θ) dτθ(θ).
Θ
(1)
Feature-learning regime. Set X, Θ and φ as in the previous paragraph, and define Fi as the
Banach space of functions f : X → R such that, for some Radon measure γ ∈ M(Θ),
for all x ∈ X we have f(χ)=/㊀夕(x,θ) dγ(θ). We define the norm of Fi as IIfkFI =
inf {∣YIItv f(∙) = Rθ Ψ(∙,θ) dY(θ)}
. This construction was introduced by Bach (2017), who first
used the notation Fi and focused in particular on the case X ⊆ Rd, Θ = Sd and 夕(x, θ)
ReLuk (h(x, 1), θi) for some k ∈ Z+ . This space is also known by the name of Barron space (E
et al., 2019; E & Wojtowytsch, 2020) in reference to the classic work (Barron, 1993).
Remark that since ∣∣h∣Li(θ) = rθ ∣h(θ)∣ dτθ(θ) ≤ (Rθ ∣h(θ)∣2 dτθ(θ))1/2 =1向后⑼ by the
Cauchy-Schwarz inequality, we have F2 ⊂ Fi: in particular finite-width neural networks belong
3
Under review as a conference paper at ICLR 2022
to F1 but not to F2 (Bach, 2017). The TV norm in F1 acts as a sparsity-promoting penalty, which
encourages the selection of few well-chosen neurons and may lead to favorable adaptivity properties
when the target has a low-dimensional structure.
2.2	EBMs and training losses
Consider a measurable set X ⊆ Rd1 with a fixed base probability measure τX ∈ P(X). If F is
a class of functions (or energies) mapping X to R, for any f ∈ F we can define the probability
measure νf as a Gibbs measure with density:
dνβf (x):= ZefeTf((X)	with	Zβf := f e-βf(y)dτχ(y),
dτX	X
where dνβf /dτX is the Radon-Nikodym derivative of νβf and Zβf is the partition function. The
parameter β > 0 is the inverse temperature. We could merge β into F by considering the function
class {βf |f ∈ F}, but We have decided to keep them separate to showcase the dependency on
β. Gibbs measures are the cornerstone of statistical physics since the seminal works of Boltzmann
and Gibbs. Beyond their widespread use across computational sciences, they have also found their
application in Machine Learning, by the name of energy-based models (EBMs), where the energy
function is parametrised using e.g. a neural network.
We denote by F1 -EBMs the energy-based models for which the energy class F is the unit ball
Bfi (1) of Fι. Notice that the class {βf |f ∈ F} is equal to the ball Bfi (β). Such models may
be regarded as abstractions of more complex deep EBMs, in that they incorporate feature learning,
and they were first studied by Domingo-Enrich et al. (2021), which provide statistical guarantees.
They are to be contrasted with F2-EBMs, for which F is the unit ball BF2 (1). F2-EBMs, which
we study in App. C, have fixed features and showed worse statistical performance in experiments
(Domingo-Enrich et al., 2021).
Given samples {xi}in=1 from a target measure νp, training an EBM consists in selecting the best νβf
with energy f ∈ F according to a given criterion. Two such criteria are relevant in this paper.
Maximum likelihood. The maximum likelihood estimator (MLE) is defined as f =
argmaxf∈f Qn=1 dVf (xi), or, equivalently, as the minimizer of the cross-entropy H(νn, νβf)
with the empirical measure Vn = n Pi=1 δXi :
f=arg∈min - nX log (筌(Xi)
1n
arg min- Y'f (xi) + β-1 log Zβf.
f∈F n i=1
(2)
The estimated distribution is then simply given by dVβ^
Z-1e-βfdτχ. By observing that
Dkl(v∣∣v0) = H(ν, v0) - H(ν), where H(ν) := H(ν,ν) is the entropy, minimizing the cross-
entropy is equivalent to minimizing the KL divergence when the latter is finite. However, such
equivalence is only well-defined in the population setting where the empirical measure νn is re-
placed by its population counterpart νp . An appropriate choice of function class F induces a reg-
ularization that prevents the learned Gibbs measure to overfit to the empirical data measure, and
presumably approximate νp instead. The MLE enjoys strong statistical properties (Wainwright &
Jordan, 2008; Wainwright, 2019), as well as a powerful variational principle as soon as one consid-
ers convex function classes (see App. A), but its notorious computational challenges (see Related
works section) have motivated alternative approximation metrics to be used to learn EBMs.
Since an arbitrary element f of Fi can be expressed as f (x) = Jθ 夕(x, θ) dγ(θ), with kf ∣∣Fι equal
to the infimum of kγkTV for all such γ, the maximum likelihood function for F = BF1 (1) the
problem (2) can be restated as ∕mle = JQ 夕(∙, θ)dγMLE(θ) where
YMLE = arg min — X / P(Xi, θ) dγ(θ) + - log ( / exp
γ∈M(Θ) n i=i Θ	β	X
kγkTV≤i	i=i
夕(x,θ) dγ(θ) dτχ(x) (3)
4
Under review as a conference paper at ICLR 2022
Score matching. An important instance of such weaker metrics is given by Score Matching
(SM). The SM metric between two absolutely continuous probability measures ν, ν0 is defined as
SM(ν, V 0) = Rχ |V log dd^ (x) — V log ^^0 (x)|2 dν(x). The SM metric is known in information
theory as the relative Fisher information. Note that this metric cannot be trivially extended to the
case ν = νn, because empirical measures do not have a density with respect to τX . To get around
this difficulty, note that, if the target measure νp is absolutely continuous with respect to τX and
We denote by fp(x) = —β-1 log dp(χ) its energy, learning an EBM with function class F under
dτX
the population loss corresponds to solving f = arg min f ∈f fχ |Vf (x) — Vfp(X)|2 dνρ(x). The
insight from Hyvarinen (2005) is that under regularity conditions on fp, via integration by parts we
then have	X |Vf	— Vfp|2	dνp	= E{xi}n	L(f,	νn)	+	C, where	E{xi}n	denotes expectation over
the data set, C is a constant in f which is therefore irrelevant, and
1n	1
L(f,Vn) = - Ee 1∆f (Xi) + -|Vf(Xi)∣2.
n2
i=1
In practice, we train an EBM via score matching by solving f = arg minf ∈F L(f, νn). Score
matching is computationally more tractable than maximum likelihood and performs well in practice
(see Related works section). Statistically, its main drawback is that the SM metric is weaker than
the KL divergence, and may fail to distinguish distributions in some instances.
The following proposition, proved in App. G, provides the expression for the loss L and the resulting
score matching problem for F1 -EBMs.
Proposition 1. Suppose that X ⊆ Rd1 is a manifold without boundaries. Assume that
JX ∣Vχ 2(x, θ) ∙ V ddVp (χ)∣ dτχ (x) is upper-bounded by some constant K for all θ ∈ Θ. Assume
also that supθ∈θ IlVx 夕(x, θ)k < η(x) and that JX ∣η(x)∣2 dνp (x) < ∞. The optimization problem
to train EBMS under the score matching loss over the ball Bfx (1) gives fsM = JQ 夕(∙，θ)dγsM(θ)
where
γSM = arg min
γ∈M(Θ) Θ X
kγkTV≤1
1 Vx2(x, θ) ∙Vx J]P(x, θ0) dγ(θ0) — β-1∆x2(x, θ)) dνn(x)dγ(θ)(4)
3 DUAL F1-EBM TRAINING VIA MAXIMUM LIKELIHOOD
As a corollary of the duality result from Subsec. B.2, we derive an alternative objective forF1-EBMs
trained via maximum likelihood, the original objective being (3) and we develop an algorithm to
solve this alternative problem. To this end, we make:
Assumption 1. Let φ : XX Θ → R be a continuousfunction such that either X is compact or (i)for
any fixed θ ∈ Θ,夕(x, θ) ≤ ξ(x) for some StriCtly positive ξ : X → R, and (ii) ξ(x) + log(ξ(x))
o — log ( dTX(x)) — (di + e) log ∣∣x∣2 as ∣∣x∣2 → +∞ for some e > 0.
In particular, this assumption holds for ReLU network energies when setting X = Rd1 , Θ = Rd1+1,
夕(x, θ) = σ(h(x, 1), θi)∕∣θ∣ and TX Gaussian (and in many other settings).
Theorem 1. Under Assumption 1, the problem (3) is the Fenchel dual of
min max β-1
ν∈P(X) γ∈M(Θ),
kγkTV≤1
D KL MTX) + /(
夕(x, θ) d(ν — Vn)(X) dγ(θ).
(5)
Moreover, the solution V? of (70) is precisely the Gibbs measure for the optimal γ? in (3), that is,
ddνX(X) = Zβ eχp (—β rθ 3(x, θ) dγ*(χ))
If we replace the F1 ball by the F2 ball, the analogous duality result links the maximum likelihood
problem with the entropy regularized MMD flow from Arbel et al. (2019) (see App. C).
5
Under review as a conference paper at ICLR 2022
Training dynamics on nonnegative measures. Let us write the dynamics to solve (5) that can be
discretized in terms of parameters and particles (cf Proposition 2 below). To this end we consider the
triple (γ+, γ-, ν) where the nonegative measures γ± are defined through the Hahn decomposition of
γ = γ+ - γ-. Then we introduce coupled gradient flows for this triple, in which γt+ and γt- evolve
via a Wasserstein-Fisher-Rao gradient flow (Chizat et al., 2018) and νt evolves via a Wasserstein
gradient flow (Santambrogio, 2017):
dtγσ = -ασVθ ∙ (γf VθFt(θ)) + αγσ (σFt⑹—Kt), σ = ±1, γtσ = γ±
∂tνt = Vx ∙ (Vt (Vxft(X)- β-1V log dτχ) ) + β-1∆χνt,
where α is a tunable parameter and we defined
Fte) = ],χψ(x,θ) d(Vt-Vn)(X),	ft (X) = / Ψ(x, θ) (dY+ - dY-)(θ),
Kt=1kγt+kTV+kγt-kTV≥1Z Ft(θ)(dγt+ - dγt-)(θ).
Θ
(6)
(7)
The initialization of (6) is V0 = Vn and γ0± = 0 (such that the initial energy is null). The term
Kt keeps the total variation of γt below one. The parameter α acts as a relative timescale. Notice
that different values of α can potentially lead to different behaviors of the dynamics; setting α 1
would correspond to the primal formulation of maximum likelihood with persistent MCMC samples
(as in PCD). In contrast if α 1, γt± evolves faster than Vt and if the optimization is well behaved
at all times γt = γt+ - γt- remains close to minimizing the inner maximization problem of (5) with
γ = γt. Initializing V0 = Vn is crucial to avoid the kind of metastabilities that curse the behavior of
classical (primal) maximum likelihood EBM training.
Proposition 2 below states that the solution (μt,ν∕ may be approximated using coupled particle
systems (see proof in App. F) and is the basis for Alg. 1. The link between particle systems and
measure PDEs is through a classical technique known as propagation of chaos (Sznitman, 1991) and
it has been used previously for similar coupled systems in the machine learning literature (Domingo-
Enrich et al., 2020).
Proposition 2. Let {θ0(j)}jm=1 be initial features sampled uniformly over Θ, let {σj}jm=1 be uniform
samples over {±1} and let {w0(j) = 1}jm=1 be the initial weight values, which are set to 1. Let
{X0(i)}iN=1 be the initial “generated” samples, which are chosen i.i.d. uniformly from the target
sample set {Xi}in=1. Consider the system of ODEs/SDEs:
(j)	(j)
-dt- = ασjVFt(θ(j)),	务=aw?，(QjFtMh- Kt)
dt	dt
-Xt(i = (-vft(xt% + β-1v log -TX (x(i)P) - + p2φ-1 -Wt(i)
(8)
where
1N	1n
瓦⑹=NN ∑^(x(i),θ) - - ∑^(χi,θ),
m
ft(χ) = m X σj w(j%(χ,θ(j)),
m j=1
m
Kt = 1Pm=ι Wyj ≥m m X σjw(j)Ft(θ(j)).
j=1
(9)
are the empirical counterparts of the functions in (7). Then the system (8) approximates the measure
dynamics. Namely, as m, N → ∞:
• the empirical measure Yt =* Pm=I σj Wy) 58?)converges weakly to the solution Yt = γ+ — Y-
of (6) with uniform initialization for any finite time interval [0, T], and
• the empirical measure Vt = N PN=I δχ(i) converges weakly to the solution Vt of (6) for any
finite time interval [0, T].
6
Under review as a conference paper at ICLR 2022
Importantly, the system of ODEs/SDEs in (8) may be solved via forward Euler steps on {θj}jm=1 and
{wj}jm=1 (or rather, {logwj}jm=1), and Euler-Maruyama updates on {X0(i)}iN=1. Such a discretization
yields Algorithm 1. Algorithm 1 makes use of a tunable parameter pR, which stands for the restart
probability and will be discussed in Sec. 4 as a natural way to connect maximum likelihood with
score matching. To discretize (8) we set pR = 0, i.e., there are no particle restarts.
Algorithm 1 Dual F1-EBM training (pR = 0: maximum likelihood, pR = (sα∧1): score matching)
Input: n samples {xi}in=1 of the target distribution, stepsize s, stepsize ratio α.
Initialize unif. features (θ0(j))jm=1 over Θ, weights (w0(j))jm=1 in [0, 1), signs (σj)jm=1 over {±1}.
Initialize generated samples {X0(i)}iN=1 uniformly i.i.d. from {xi}in=1.
for t = 0, . . . , T - 1 do
for i = 1, . . . , N do
With probability pR, replace Xt(i) by some uniformly chosen sample in {xi }in=1 (see Sec. 4).
Sample ζt(i) from the d1-variate standard Gaussian.
Perform Euler-Maruyama update: X(+)ι = X(i) - S(Nft (X，，) + β-1V log dTχ (Xyy) 十
p2β-1s Z(i), where f is defined in (9).
end for
for j = 1, . . . , m do
Update θ(j)ι = θj) + sασjVFt(θj) where Ft is defined in (9).
Update Wj)I = w(j)ι exp(sασ7-Ft(θj))).
Normalize if needed Wj)I = W(j1/ max (m-1 Pm=I Wg), 1).
end for
end for
Output： SamPles {xTi)}N=ι, energy fτ(x) := mm Pm=I σjWj中(x,θj).
4	Links between maximum likelihood and score matching
F1-EBMS
In this section we uncover how the score matching loss fits seamlessly as a variant of Alg. 1, in the
form of particle restarts. Interestingly, we can modify the PDE (6) in a way that allows us to make a
connection with score matching. To this end, let us introduce the following coupled measure PDE:
dtYσ = -ασvθ ∙ (γf vθFt(θ)) + αYσ (σFt(θ) - Kt) , σ = ±1, γtσ = γ± ,
∂tνt = Vx
+ β-1∆xνt - α (νt - νn) .
(10)
Remark that the only difference between this equation and the PDE (6) for dual maximum likelihood
training is the term -α(νt -νn), which draws νt closer to the empirical target measure νn. We have:
Proposition 3. In the limit α → ∞, the equations for γtσ in (10) reduce to
dtYt = σvθ	∙	(Yt vθV (γt)(θ))	-Y	(σV (γt)(θ)-V(Yt)) , σ = ±1,	Yσ	= γ±	(II)
where Yt = γ+ - Y-, V(Y) = Jθ V(γ)dγ, and V(γ)(θ) is the Frechet derivative of the score
matching loss L : M(Θ) → R defined in (4).
That is, in the large α limit, equation (10) is equivalent to the Wasserstein-Fisher-Rao gradient
flow of a loss L which, remarkably, is the score matching loss for F1-EBMs. This means that
adding the term -α (νt - νn) to the dual maximum likelihood measure dynamics and letting α →
∞ we recover the score matching dynamics. This additional term can be easily implemented at
particle level by replacing each training sample Xt(i) by some random target sample in {xi}in=1 with
probability pR = 1 - e-αt = αt + o(t) for every time interval of length t (proof in App. G).
7
Under review as a conference paper at ICLR 2022
Similar birth-death processes were used in (Rotskoff et al., 2019) in the context of neural network
regression. Hence, the score matching scheme corresponds to setting the restart probability pR = sα
in Algorithm 1. The restart probability acts as a knob that allows us to interpolate between score
matching and maximum likelihood.
In summary, score matching differs from dual maximum likelihood in that the trained measure is
being “pulled” towards the target measure at all times via particle restarting. Such constant pulling
should be useful to alleviate sampling problems due to metastability issues which may arise with
dual maximum likelihood. However, dual maximum likelihood has the upside of providing samples
of the learned EBM as a byproduct of training, which score matching does not. A good balance
between both algorithms may be to use a restart probability pR between sα ∧ 1 and 0, or even a pR
that decreases with time from one value to the other, in such away that at the beginning of training we
avoid metastability issues by restarting the particles frequently, and at an advanced phase we perform
little to no restarting to obtain faithful samples. It is also interesting to contrast our approach to score
matching with the works Sutherland et al. (2018); Arbel & Gretton (2018), which using different
techniques propose algorithms to train EBMs with RKHS energies via score matching. Finally,
notice that a particle discretization of the flow (11) yields an alternative straightforward algorithm
to train F1-EBMs via score matching; see Subsec. G.1. In Subsec. G.1 we show that this algorithm
can be linked directly to Alg. 1 with particle restarts, without recurring to measure arguments.
5	Experiments
Setup. To illustrate Alg. 1 we perform numerical experiments on simple synthetic datasets generated
by teacher models with energy f *(x) = J PJ=I w*σ(hθ*,xi), with θj ∈ Sd for all j. The training
is performed using Alg. 1 with the added detail that both the features θt(j) and the particles Xt(i) are
constrained to remain on the sphere by adding a projection step in the update of their positions. The
code, figures, and videos on the dynamics can be found in the supplementary material. In the main
text we consider two planted teacher neurons (J = 2) with negative output weights w； = Wg =
-10 in dimension d = 14 and m = 64 neurons for the student model, but we include additional
experiments and videos in App. I and supplementary material. We study setups with two different
choices of angles between the teacher neurons, which showcase different behaviors:
•	Teacher neurons θg, θg forming an angle of 2.87 rad (≈ 164 degrees), and output weights w；=
w2； = -10. The teacher neurons are almost in opposite directions, and the resulting target
distribution is bimodal, as the energy has two local minimizers around θg and θg (see Figure 5).
•	Teacher neurons θg, θ2 forming an angle of 1.37 rad (≈ 78 degrees). The teacher neurons are
almost orthogonal and the resulting target distribution is monomodal; indeed, when the angle is
less than n/2, the target energy has a unique minimizer at the geodesic average between θg and
θg (see Figure 5 in App. I).
Monitoring convergence. In all our experiments, to monitor convergence we use a testing set ofn；
data points sampled from the teacher distribution: denoting these samples by {x； }n=1, we estimate
*
the KL divergence from the student to the teacher via log(亲 ^2i=1 exp(一βft(χg) + βf ；(x；))) +
n* Pn= 1(ft(Xi) + β广(Xi)) where f (x) = m1 Pm=I w(j)σ(hθj),xi). Similarly, for the score
*
matching objective we use the estimate n* Pi=Il Vxft (Xi) - Vχf ；(x；)| .
Comparison of the primal algorithm and the dual algorithms. We defer the empirical study of
tuning the restart probability to App. I, and in this section focus on comparing the dual algorithm
for maximum likelihood F1-EBMs (i.e. with pR = 0) to the classical (primal) algorithm, which
was the algorithm used in the experiments of Domingo-Enrich et al. (2021). The primal algorithm
corresponds to Alg. 1 with α 1, while the dual algorithm uses α 1. To obtain a principled
comparison of the two settings where numerical errors do not blow up, we set s to be the step-
size for the fastest process (particle evolution for the primal, neuron evolution for the dual), and
min(α, 1∕α)s the stepsize for the slow process.
8
Under review as a conference paper at ICLR 2022
SGD iteration
SGD iteration
SGD iteration
SGD iteration
SGD iteration
SGD iteration
Figure 1: (Top) The evolution of the KL divergence, the score matching metric and the TV norm
of the trained measure (i.e., the F1 norm) during training for Algorithm 1 with X = S14, m = 64,
PR = 0, S = 0.02, n =105, N =2 ∙ 105 and a = 0.05 (primal training) or α = 20 (dual training),
showing a speedup by a factor about 10-20 of the latter over the former. The angle between the two
teacher neurons is 1.37 rad (monomodal distribution). (Bottom) Same experiments with an angle of
2.87 rad between the two teacher neurons (bimodal distribution).
The results are shown in Figure 6 for the two angle configurations between teacher neurons. We
observe that the dual algorithm is several orders of magnitude faster at reaching KL and SM val-
ues close to the final ones, which showcases the main advantage of the dual approach. For the
monomodal distribution, the final values obtained by the dual algorithm are slightly better than
for the primal algorithm; for the bimodal distribution, the converse happens and the convergence
is slower for both algorithms, most likely due to metastability. Interestingly, the decrease of the
performance metrics seems to stall as soon as the hard F1-norm threshold is reached.
6 Discussion and outlook
In this work we leverage a Fenchel duality result to recast the maximum likelihood loss for F1-
EBMs into a min-max problem on probability measures over the sample space. We provide mean-
field dynamics at the measure level to solve this problem, which lead to a dual algorithm (Alg. 1)
after discretization. We observe that if we restart particles at random target samples throughout
training, we get an algorithm which is equivalent to training under the score matching loss. We
perform experiments in which we learn planted distributions with two-layer ReLU networks, and
we observe empirically that our dual algorithm is much faster than the classical one.
At theoretical level, one direction for future work is to obtain convergence results for the dynamics
(6) and (10). Domingo-Enrich et al. (2020) study similar coupled Wasserstein-Fisher-Rao gradient
flows, but their results only work in the case of weight learning rates much larger than position
learning rates. We hypothesize that the additional term -α (νt - νn), which keeps νt close to νn,
might help in the analysis.
At the numerical level, it would be interesting to further test the variant of Algorithm 1 with an-
nealed pR decreasing in time, to understand under which parameter setup it captures the best fea-
tures of maximum likelihood and score matching. One could also test Algorithm 1 using deeper
neural architectures: while the analysis is more complicated in this case, the scheme itself can be
straightforwardly generalized to deep networks.
References
Yasemin Altun and Alex Smola. Unifying divergence minimization and statistical inference via
convex duality. In Learning Theory, pp. 139-153. Springer Berlin Heidelberg, 2006.
9
Under review as a conference paper at ICLR 2022
Michael Arbel and Arthur Gretton. Kernel conditional exponential family. In Proceedings of the
Twenty-First International Conference on Artificial Intelligence and Statistics, volume 84 of Pro-
Ceedings of Machine Learning Research ,pp. 1337-1346. PMLR, 2018.
Michael Arbel, Anna Korba, Adil Salim, and Arthur Gretton. Maximum mean discrepancy gradient
flow. In Advances in Neural Information Processing Systems, volume 32, 2019.
Francis Bach. Breaking the curse of dimensionality with convex neural networks. Journal of Ma-
chine Learning Research, 18(19):1-53, 2017.
Andrew Barron. Universal approximation bounds for superpositions of a sigmoidal function. Infor-
mation Theory, IEEE Transactions on, 39:930 - 945, 1993.
Alain Berlinet and Christine Thomas-Agnan. Reproducing Kernel Hilbert Space in Probability and
Statistics. Springer, 2004.
Jonathan Borwein and Qiji Zhu. Techniques of Variational Analysis. CMS Books in Mathematics.
Springer-Verlag New York, 2005.
Zhengdao Chen, Grant M. Rotskoff, Joan Bruna, and Eric Vanden-Eijnden. A dynamical central
limit theorem for shallow neural networks, 2020.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-
parameterized models using optimal transport. In Advances in neural information processing
systems, pp. 3036-3046, 2018.
Lenalc ChizaL EdoUard Oyallon, and Francis Bach. On lazy training in differentiable programming.
In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.
Lenalc Chizat, Gabriel Peyre, Bernhard Schmitzer, and Francois-Xavier Vialard. Unbalanced opti-
mal transport: Dynamic and kantorovich formUlations. Journal of Functional Analysis, 274(11):
3090-3123, 2018.
Bo Dai, HanjUn Dai, ArthUr Gretton, Le Song, Dale SchUUrmans, and Niao He. Kernel exponential
family estimation via doUbly dUal embedding. In Proceedings of the Twenty-Second Interna-
tional Conference on Artificial Intelligence and Statistics, volUme 89 of Proceedings of Machine
Learning Research, pp. 2321-2330. PMLR, 2019a.
Bo Dai, Zhen LiU, HanjUn Dai, Niao He, ArthUr Gretton, Le Song, and Dale SchUUrmans. Exponen-
tial family estimation via adversarial dynamics embedding. In Advances in Neural Information
Processing Systems, volUme 32. CUrran Associates, Inc., 2019b.
PrafUlla Dhariwal and Alex Nichol. DiffUsion models beat gans on image synthesis. arXiv preprint
arXiv:2105.05233, 2021.
Carles Domingo-Enrich, Samy Jelassi, ArthUr Mensch, Grant Rotskoff, and Joan BrUna. A mean-
field analysis of two-player zero-sUm games. In Advances in Neural Information Processing
Systems, volUme 33, pp. 20215-20226. CUrran Associates, Inc., 2020.
Carles Domingo-Enrich, Alberto Bietti, Eric Vanden-Eijnden, and Joan BrUna. On energy-based
models with overparametrized shallow neUral networks, 2021.
YilUn DU and Igor Mordatch. Implicit generation and generalization in energy-based models. In
Advances in Neural Information Processing Systems (NeurIPS), 2019.
Miroslav Dudlfk, Steven J. Phillips, and Robert E. Schapire. Maximum entropy density estimation
with generalized regUlarization and an application to species distribUtion modeling. J. Mach.
Learn. Res., 8:1217-1260, 2007.
Nelson Dunford and Jacob T Schwartz. Linear operators. Part I, General theory. Pure and applied
mathematics (Interscience series). Interscience, 1958.
Gintare Karolina Dziugaite, Daniel M Roy, and Zoubin Ghahramani. Training generative neural
networks via maximum mean discrepancy optimization. UAI, 2015.
10
Under review as a conference paper at ICLR 2022
Weinan E and Stephan Wojtowytsch. On the banach spaces associated with multi-layer relu net-
works: Function representation, approximation theory and gradient descent dynamics, 2020.
Weinan E, Chao Ma, and Lei Wu. A priori estimates of the population risk for two-layer neural
networks. Communications in Mathematical Sciences, 17:1407-1425,01 2019.
Ivar Ekeland and Roger Temam. Convex analysis and variational problems. Philadelphia, Pa:
Society for Industrial and Applied Mathematics, 1999.
Josiah Willard Gibbs. Elementary Principles in Statistical Mechanics: Developed with Especial
Reference to the Rational Foundation of Thermodynamics. Cambridge Library Collection - Math-
ematics. Cambridge University Press, 2010.
Arthur Gretton, Karsten M Borgwardt, Malte Rasch, Bemhard SchOlkopf, and Alex J Smola. A ker-
nel method for the two-sample-problem. In Advances in neural information processing systems,
pp. 513-520, 2007.
Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf, and Alexander Smola.
A kernel two-sample test. Journal of Machine Learning Research, 13(25):723-773, 2012.
Geoffrey E. Hinton. Training products of experts by minimizing contrastive divergence. Neural
Comput., 14(8):1771-1800, 2002.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances
in Neural Information Processing Systems, volume 33. Curran Associates, Inc., 2020.
Aapo Hyvarinen. Estimation of non-normalized statistical models by score matching. Journal of
Machine Learning Research, 6(24):695-709, 2005.
E. T. Jaynes. Information theory and statistical mechanics. Phys. Rev., 106:620-630, May 1957.
Alexia Jolicoeur-Martineau, Remi Piche-Taillefer, Remi Tachet des Combes, and Ioannis
Mitliagkas. Adversarial score matching and improved sampling for image generation. arXiv
preprint arXiv:2009.05475, 2020.
Zahra Kadkhodaie and Eero P Simoncelli. Solving linear inverse problems using the prior implicit
in a denoiser. arXiv preprint arXiv:2007.13640, 2020.
H. Kneser. Sur un theoreme fondamentale de la theorie des jeux. C. R. Acad. Sci. Paris, 234:
2418-2420, 1952.
Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and F Huang. A tutorial on energy-based
learning. 2006.
Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabas Poczos. Mmd gan:
Towards deeper understanding of moment matching network. In Advances in Neural Information
Processing Systems, volume 30, 2017.
Yujia Li, Kevin Swersky, and Richard Zemel. Generative moment matching networks. In ICML,
2015.
Henry McKean. A class of markov processes associated with nonlinear parabolic equations. Pro-
ceedings of the National Academy of Sciences of the United States of America, 56:1907-11, 01
1967.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-
layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665-E7671,
2018.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning.
The MIT Press, 2012.
J v Neumann. Zur theorie der gesellschaftsspiele. Mathematische annalen, 100(1):295-320, 1928.
11
Under review as a conference paper at ICLR 2022
Edward C. Posner. Random coding strategies for minimum entropy. IEEE Transations on Informa-
tion Theory, 21(4):388-391,1975.
Marc Ranzato, Christopher Poultney, Sumit Chopra, et al. Efficient learning of sparse representa-
tions with an energy-based model. 2007.
Grant M Rotskoff and Eric Vanden-Eijnden. Neural networks as interacting particle systems:
Asymptotic convexity of the loss landscape and universal scaling of the approximation error.
arXiv preprint arXiv:1805.00915, 2018.
Grant M Rotskoff, Samy Jelassi, Joan Bruna, and Eric Vanden-Eijnden. Global convergence of neu-
ron birth-death dynamics. In Proceedings of the 36th International Conference on International
Conference on Machine Learning, Long Beach, CA, USA, 2019.
D. Ruelle. Statistical mechanics: Rigorous results. W.A. Benjamin, 1969.
Filippo Santambrogio. {Euclidean, metric, and Wasserstein} gradient flows: an overview. Bulletin
of Mathematical Sciences, 7(1):87-154, 2017.
Maurice Sion. On general minimax theorems. Pacific J. Math., 8(1):171-176, 1958.
Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A central
limit theorem. Stochastic Processes and their Applications, 2019.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
arXiv preprint arXiv:1907.05600, 2019.
Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. In
Advances in Neural Information Processing Systems, 2020.
Yang Song and Diederik P. Kingma. How to train your energy-based models, 2021.
Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. In Interna-
tional Conference on Learning Representations (ICLR 2021), 2021.
Danica J. Sutherland, Heiko Strathmann, Michael Arbel, and Arthur Gretton. Efficient and princi-
Pled score estimation with nystrθm kernel exponential families. In Proceedings of the Twenty-
First International Conference on Artificial Intelligence and Statistics, volume 84 of Proceedings
of Machine Learning Research, pp. 652-660. PMLR, 2018.
Alain-Sol Sznitman. Topics in propagation of chaos. In PaUl-Louis Hennequin (ed.), Ecole d,Ete de
Probabilites de Saint-FlourXIX 一 1989, pp. 165-251, Berlin, Heidelberg, 1991. Springer Berlin
Heidelberg.
Tijmen Tieleman. Training restricted boltzmann machines using approximations to the likelihood
gradient. In Proceedings of the 25th International Conference on Machine Learning, ICML ’08.
Association for Computing Machinery, 2008.
Tijmen Tieleman and Geoffrey Hinton. Using fast weights to improve persistent contrastive diver-
gence. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML
’09. Association for Computing Machinery, 2009.
Pascal Vincent. A connection between score matching and denoising autoencoders. Neural Compu-
tation, 23(7), 2011.
Martin Wainwright and Michael Jordan. Graphical models, exponential families, and variational
inference. Foundations and Trends in Machine Learning, 1:1-305, 01 2008.
Martin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Series
in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019.
Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu. A theory of generative convnet. In Pro-
ceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings
of Machine Learning Research. PMLR, 2016.
12
Under review as a conference paper at ICLR 2022
Jianwen Xie, Song Zhu, and Yingnian Wu. Synthesizing dynamic patterns by spatial-temporal
generative convnet. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2017.
13