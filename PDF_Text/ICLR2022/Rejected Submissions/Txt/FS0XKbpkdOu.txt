Under review as a conference paper at ICLR 2022
Sphere2Vec: SELF-SUPERVISED LOCATION REPRESEN-
tation Learning on Spherical Surfaces
Anonymous authors
Paper under double-blind review
Ab stract
Location encoding is valuable for a multitude of tasks where both the absolute
positions and local contexts (image, text and other types of metadata) of spatial
objects are needed for accurate predictions. However, most existing approaches
do not leverage unlabeled data, which is crucial for use cases with limited labels.
Furthermore, the availability of large-scale real-world GPS coordinate data demand
representation and prediction at global scales. Existing location encoding models
assume that the input coordinates are in Euclidean space, which can lead to mod-
eling errors due to distortions introduced when mapping coordinates from other
manifolds (e.g., spherical surfaces) to Euclidean space. We introduce Sphere2Vec,
a location encoder, which can directly encode spherical coordinates while pre-
serving spherical distances. Sphere2Vec is trained with a self-supervised learning
framework which pre-trains deep location representations from unlabeled geo-
tagged images with contrastive losses, and then fine-tunes to perform supervised
geographic object classification tasks. Sphere2Vec achieves the performances of
state-of-the-art results on various image classification tasks ranging from species,
Point of Interest (POI) facade, to remote sensing. The self-supervised pretraining
significantly improves the performance of Sphere2Vec especially when the labeled
data is limited.
1 Introduction
Spatial information has become an important component to many machine learning tasks that address
the environmental, economic, and societal needs for sustainable development. Just to name a few,
these tasks include geo-aware species fine-grained recognition (Chu et al., 2019; Mac Aodha et al.,
2019), Point of Interest (POI) facade image classification (Chu et al., 2019; Yan et al., 2018), remote
sensing image classification (Christie et al., 2018; Ayush et al., 2020), poverty prediction (Jean
et al., 2016; 2019), point cloud classification and semantic segmentation (Qi et al., 2017a;b), traffic
forecasting (Li et al., 2018; Cai et al., 2020), and geographic question answering (Mai et al., 2019;
2020a). Developing a general model for vector space representations of any point in space, termed
a location encoder, would pave the way for many future applications that improves our well-being.
Such a location encoder can operate on a 2D space or on other manifolds; of particular interest is the
spherical surface of the Earth.
Here we use geo-aware image classification shown in Figure 1 and 2 as an example. During training
we assume a dataset D “ tpIi, xi, yiq|i “ 1, ..., Nu, where Ii is an image, yi P 1, ., C is the
corresponding class label, xi represents the location (longitude and latitude) and optionally the
time the image was taken1. The image information is often processed by an state-of-the-art image
classification model (e.g., InceptionV3) to predict the class labels as shown in Figure 1. Although the
image itself is the most critical for image classification, sometimes it can be insufficient - the image of
two distinct classes may look very similar both for the central object and its surrounding environment.
For example, Figure 2a and 2f shows example images of two different fox species: Arctic fox (Vulpes
lagopus) and bat-eard fox (Otocyon megalotis), for which the image model can be confused due to
the high visual similarity of the two species. Fortunately, the geo-location information of an image
may provide a clue to their real classes. Figure 2b, 2g show that these two species have distinct spatial
distribution patterns, and the image locations provide complementary information to the images. This
1In this study we focus on the location information and leave the time aspect of modeling to future.
1
Under review as a conference paper at ICLR 2022
Image I
Pretrained Image Feature
Extractor F()
Image Embedding
F(I)
p
O
…4。
AO
O
O
[Freeze
Projection
Matrix
UnsUPervised
Contrastive
Loss
Location
∖ X =（入。）一
)08OQ
ApQZlX)
Supervised Loss
LocationEncoder Location Embedding
EneQ p[x] = E,nc(x)
Location Supervised Training
Figure 1: Applying Sphere2Vec to geo-aware image classification. The supervised image classification module
(green) is a InceptionV3 network similar to that of Mac Aodha et al. (2019); the supervised location classification
module (blue) can be Sphere2Vec or any other inductive location encoders (Chu et al., 2019; Mac Aodha et al.,
2019; Mai et al., 2020b); the self-supervised training stage (orange) pre-trains the location encoder based on
unlabeled image data. The dotted lines indicates that there is no back-propergation through these lines.
(a) Image
(d) grid
(e) SphereC +
(c) wrap
(i) grid
(j) SPhereC +
(h) wrap
(f) image (g) Bat-Eared FoX
(b) Arctic FoX

Figure 2: Applying location encoders to differentiate two visually similar species: Arctic foX and bat-eared foX.
(a) and (f) are their eXample images which look very similar. (b) and (g) show their distinct patterns in image
locations. (c)-(e): The predicted distributions of Vulpes lagopus from different location based models (without
images as input). (h)-(j): The predicted distributions of OtoCyon megalotis. While wrap* (Mac Aodha et al.,
2019) produces a over-generalized species distribution, sphereC ' (our model) produces a more compact and
fine-grain distribution on the polar region and in data sparse areas such as Africa (See Figure 2g-2j). grid (Mai
et al., 2020b) is between the two. For more eXamples, please see Figure 10.
insight motivates the development of hybrid classification models, which take both the images and
their locations as input (Mac Aodha et al., 2019; Mai et al., 2020b).
In spite of their effectiveness, hybrid models have an issue when representing points in non-Euclidean
spaces. The demand on representation and prediction learning at a global scale grows dramatically
due to emerging global scale issues, such as the transition path of the latest pandemic (Chinazzi
et al., 2020), long lasting issue for malaria (Caminade et al., 2014), under threaten global biodi-
versity(Di Marco et al., 2019; Ceballos et al., 2020), and numerous ecosystem and social system
responses for climate change (Hansen & Cramer, 2015). However, when applying the state-of-the-art
2D space location encoders (Gao et al., 2019; Chu et al., 2019; Mac Aodha et al., 2019; Mai et al.,
2020b; Zhong et al., 2020) to large-scale real-world GPS coordinate datasets such as species images
taken all over the world, a map projection distortion problem (Williamson & Browning, 1973)
emerges. These 2D location encoders are designed for preserving distance in 2D (or 3D) Euclidean
space, while GPS coordinates are in fact on a spherical manifold, e.g., point x “ pλ, φq on a sphere
with longitude λ P [—π, π) and latitude φ P [—π{2, —π/2] (See Figure 1). Directly applying these
2D location encoders on spherical coordinates will yield a large distortion in the polar regions. Note
that map projection distortion is unavoidable when projecting spherical coordinates into 2D space.
This emphasizes the importance of calculating on a round planet (Chrisman, 2017). See AppendiX
9.1 for more discussion.
2
Under review as a conference paper at ICLR 2022
From a modeling perspective another key challenge is how to design the self-supervised training
objective that effectively transfers the attribute information (e.g., image features) at different locations
into a location encoder. This is very important in practice since collecting and labeling geo-tagged
images is particularly costly as annotations often require domain expertise. However, most existing
location encoding approaches (Chu et al., 2019; Mac Aodha et al., 2019; Mai et al., 2020b) are
developed and trained in a supervised learning framework while massive unlabeled geographic data
cannot be used. Ayush et al. (2020) proposed a geographic self-supervised learning framework, but
they mainly focus on training image encoders while the usage of geo-location information is limited
to data augmentation in the pre-training stage. In contrast, in this work, we focus on training location
encoders in a self-supervised manner so that geo-location information is explicitly use as part of the
model input.
In this work, we propose Sphere2Vec as shown in Figure 1, which directly encodes point coordinates
on a spherical surface while avoiding the map projection distortion. The multi-scale encoding method
utilizes Double Fourier Sphere basis (OpS2q terms) or a subset (OpSq terms) of it while still being
able to correctly measure the spherical distance. Furthermore, we develop a self-supervised learn-
ing framework which pre-trains deep location representations from unlabeled geo-tagged images
by predicting image features or image identities base on their geo locations. We compare several
self-supervised learning objectives including Mean Square Error loss (MSE), contrastive binary
classification loss (BI), and contrastive multi-classification loss (MC). The pre-trained Sphere2Vec
model can be fine-tuned (with an additional output layer) to perform supervised geographic objects
classification tasks. We demonstrate the effectiveness of Sphere2Vec on geo-aware image classifica-
tion tasks including fine-grain species recognition (Chu et al., 2019; Mac Aodha et al., 2019; Mai
et al., 2020b), POI fascade image classification (Tang et al., 2015; Mac Aodha et al., 2019), and
remote sensing image recognition (Christie et al., 2018; Ayush et al., 2020).
In summary, the contributions of our work are:
1.	We develop a unified view of distant preserving location encoding methods on spheres based
on Double Fourier Sphere (DFS) (Merilees, 1973; Orszag, 1974).
2.	We propose an effective pre-training strategy for location encoders from unlabeled geo-
tagged images by predicting image features and image identities based on their geo locations.
3.	Sphere2Vec improves the performances of state-of-the-art results on four species recognition
datasets (BirdSnap, NABirds, iNat2017, iNat2018), one POI fascade image classification
dataset (YFCC), and one remote sensing image classification dataset (fMoW). It also
demonstrate significant advantage in few shot learning settings.
2	Problem Formulation
Following previous work (Mai et al., 2020b), Euclidean location encoding can be formulated as
follows. Given a set of points (e.g., POI) P “ tpi u in L-D space (L “ 2, 3), we define a function
ep,θ PX) : RL → Rd (L ! d), which is parameterized by θ and maps any coordinate X in space to
a vector representation of d dimension. Each point (e.g., a restaurant) pi “ pxi , viq is associated
with a location Xi and attributes vi (i.e., POI features such as types, names, capacity, images, etc.).
The function eP,θPX) encodes the information about Point X which is useful for certain tasks such as
predicting a label y. Attributes and coordinates of points can be seen as analogies to words and word
positions in commonly used word embedding models.
Similarly, we can formulate spherical location encoding as follows. Given a set of points P “ tpiu
on the surface of a sphere S2, e.g., species occurrences all over the world, where pi “ PXi, vi) and
Xi “ (λi, φi) P S2 indicates a point with longitude λ% P [—∏, ∏) and latitude φi P [一∏∕2, ∏∕2].
Define a function eP,θ PX) : S2 → Rd, which is parameterized by θ and maps any coordinate X in a
spherical surface S2 to a vector representation of d dimension. See Appendix 9.1 for more discussions
about the map projection distortion problem.
3	Method
The geo-aware image classification task (Chu et al., 2019; Mac Aodha et al., 2019) can be formulated
as follows. Given an image I taken from location/point X, we estimate which category y it belongs
to by PPy|I, X). If we assume I and X are conditionally independent given y, then based on Bayes’
3
Under review as a conference paper at ICLR 2022
theorem, we have Ppy|I, xq 9 P py|xqP py|Iq (See Mac Aodha et al. (2019)). Ppy|Iq can be given
by any state-of-the-art image classifier such as Inception V3 (Szegedy et al., 2016) for the species
recognition task or MoCo-V2+TP (Ayush et al., 2020) for the RS image classification task. Figure 1
illustrates the whole training and inference process by using species recognition as the example task.
P py|Iq and P py|xq are given by the image encoder (the green box) and location encoder (the blue
box) respectively and are multiplied together for final prediction. In this work, we focus on estimating
the geographic prior distribution of class y over the spherical surface P(y |x) 9 σ(e(x)p,y) where
σ() is a sigmoid activation function. T P RdXc is a class embedding matrix where the yth column
T:,y P Rd indicates the class embedding for class y.
In the following we introduce the two main contributions of our work - location encoding on
spherical surfaces (the blue box in Figure 1), and a self-supervised training framework for location
representation model (the orange box in Figure 1). In Section 3.1, we will discuss the design of
spherical distance-kept location encoder epx), Sphere2Vec. We developed a unified view of distance-
reserving encoding on spheres based on Double Fourier Sphere (DFS) (Merilees, 1973; Orszag, 1974).
In Section 3.2 we discuss training strategies for Sphere2Vec including self-supervised pre-training
given unlabeled images, and supervised fine tuning. We use eP,θpx) to indicate the spherical location
encoding neural network with parameters θ, and will refer it by epx) for simplicity.
3.1	Sphere2Vec ENCODER
We want to find a function Gpsphereq px) which does a one-to-one mapping from each point xi “
pλi , φi ) P S2 to a multi-scale representation with S as the total number of scales such that it satisfies
the following requirement:
xGpsphereqpx1), Gpsphereqpx2)y “ f p∆D), @x1, x2 PS2,	(1)
where ∆D P r0, πRs is the spherical surface distance between x1, x2, R is the radius of this sphere,
and fpx) is a strictly monotonically decreasing function for x P r0, πRs. In other words, we expect
to find a function Gpsphereq px) such that the resulting multi-scale representation of x preserves the
spherical surface distance.
Similar to the case for Euclidean space (Mai et al., 2020b), any point x “ pλ, φ) P S2 with longitude
ππ
λ P [—∏, ∏) and latitude φ P r´-, —S, can be explicitly encoded in a multi-scale fashion in the
form of epxq (x) “ NN于切(Gp*q (x)) where NNf 加()is a learnable multi-layer perceptron with h
hidden layers and k neurons per layer. Gp*q(x) is a concatenation of multi-scale spherical spatial
features of S levels. (*) indicates different types of spherical location encoders. In the following, we
call epxq (x) location encoder and its component Gp*q (x) position encoder. Let rmin,『max be the
minimum and maximum scaling factor, and g “ rmax .2
rmin
sphereDFS Double Fourier Sphere (DFS) (Merilees, 1973; Orszag, 1974) is a simple yet successful
pseudospectral method, which has been applied to efficient analysis of large scale phenomenons such
as weather (Sun et al., 2014) and blackholes (Bartnik & Norton, 2000). Our first intuition is to use
the base functions of DFS to help decompose x “ (λ, φ) into a high dimensional vector:
S—1	S—1
G(SphereDFS)(X) “ U ,in φn, Cos φn Y U
rsinλm,cosλmsY
n“0	m“0
S—1 S—1
(2)
n“0 m“0
rcos φn cos λm, cos φn sin λm, sin φn cos λm, sin φn sin λms.
U
where Vs “ 0,1,…，S — 1, we define λs “ -λ, φs “ -φ, f “ rmιin ∙ gs{(S—1q. Y means vector
s	s	s mn
S—1	fs	fs
concatenation and sS“—01 indicates vector concatenation through different scales. It basically lets all
the scales of φ terms interact with all the scales of λ terms in the encoder. This would introduce a
position encoder whose output has O(S2) dimensions and increase the memory burden in training
and hurts generalization (Figure 3a).
2In practice we fix rmax “ 1 meaning no scaling of λ, φ.
4
Under review as a conference paper at ICLR 2022
Figure 3: Spectral representations of different encoders. Each point pλm , φnq represent an interaction terms of
trigonometric functions in the encoder. Points on the axis correspond to single terms with no interactions.
An encoder might achieve better result by only using a subset of these terms. In comparison, the state
of art encoder such as grid and wrap (Mac Aodha et al., 2019) define: 3
S—1
Gpgridq PXq “ U [sin φs, cos φs, Sin λs, cos λs].	(3)
s“0
We can see that grid employs a subset of terms from sphereDF S (Figure 3b) and performs poorly
at a global scale due to its inability to preserve spherical distances (See Appendix 9.3 for proof).
We explore different subsets of DFS terms while achieving two goals: 1) efficient representation
with OpSq dimensions 2) preserving distance measures on spheres. See Appendix 9.2 for the detail
definition of sphereC, sphereC `, sphereM, sphereM `, and Figure 3 for a unified view. We
hypothesize that encoding these terms in the multi-scale representation would make the training of
the encoder easier and the order of output dimension is still OpSq.
In location encoding, the uniqueness of encoding (no two points on sphere having the same position
encoding) is very important, Gp*q (x) in the five proposed methods are one-to-one mapping.
Theorem 1. @* P tsphereC, sphereC', sphereM, sphereM ',sphereDFS}, Gp*q(x) is an
injective function. See the proof in Appendix 9.5.
3.2 TRAINING Sphere2Vec
Self-Supervized Pretraining (the yellow box in Figure 1). We consider two contrastive objec-
tives. The first is a binary classification loss, or so called noise contrastive estimation (Gutmann &
Hyvarinen, 2010), which avoids calculation of the partition function and has been successfully used
in word embeddings (Mikolov et al., 2013) and language modeling (Mnih & Teh, 2012):
Lbi(P,N) = ´E(a,b)~p[logσ(s(a,b))] ´ E(a,b-q„N[log(1 — σ(s(a,b´)))]	(4)
where BI stands for “binary”, P is a set of positive pairs (denoted as pa, bq), N is a set of negative
pairs (denoted as (a, b´)), s(∙, ∙) is a similarity function that assigns higher values when the inputs
are more similar (such as cosine(q), and σ(vq = ev{(1 ` evq is the sigmoid function.
The second objective function is the multi-class classification loss with temperature, which has been
successfully used in unsupervised learning for images (He et al., 2020) and text (Gao et al., 2021):
LMC (P, N, τq = Epa,bq„P
exp(s(a, bq{τ q
eχp(s(a, bq{τq + Xb-PN(b|a) exP(s(a, b´q{τq
(5)
where MC stands for “multi-class”, N (b|aq obtains a negative pair with first entry being a; P and
s(∙, ∙) are defined as earlier. The temperature scaling parameter T determines how soft the softmax
is (Hinton et al., 2015). In practice it help with the trade off between top ranked classes (precision)
versus reset of the classes (recall).
In order to learn useful representations, we need to choose appropriate distributions for positive
pairs P and negative pairs N. During the location encoder pretraining stage we use a geo-tagged
unlabeled
but unlabeled image set X , each image is represented by a embedding generated from a
pretrained image extract extractor F4 such as InceptionV3 or MoCo-V2+TP (Ayush et al., 2020).
Given an unlabeled image (X, Iq P Xunlabeled, we use the following positive and negative instances:
3As a multi-scale encoder, grid degenerates to wrap when the number of scales S “ 1.
4F should not see the current image labels during pretraining stage.
5
Under review as a conference paper at ICLR 2022
•	Geo-tagged positive Px “ tpepxq, WFpIqqu where W is a linear projection layer.
•	In-batch negatives Nb “ {(e(x), WF(I´)g,l´) P B(x,I)∖{(x,I)}} where B(x,I)
represents the batch of examples for which px, Iq is part of during training.
•	Sampled negative locations NS “ {(e(x—), WF(I))} of size N, where x~ is one of the
N evenly sampled locations from the surface of the sphere for each example x.
•	Dropout positive Pd “ t(e(x), e1(x))}, where we pass the same input x to the encoder e()
twice and obtain two embeddings as “positive pairs” by applying independently sampled
dropout masks. This is a data augmentation strategy (so called SimCSE), which has been
very successful in generating sentence embeddings (Gao et al., 2021). We use e1(x) to
denote the embedding from the second mask.
•	Dropout negative Nd “ {(e(x),e1(χ-))∣(x,I-) P B(x, I)∖{(x, I)},where e1(∙) has the
same meaning as that defined in Pd and B(x, I) indicates the same as Nb.
We define two versions of contrastive losses, which both have three components: in-batch, negative
location, and SimCSE. The self-supervized binary classification loss LBI is defined as
LBI(X) “ L鬻Ch(X) + βιLBgoc(X) + β2LBmcse(X)
“ LBI(Px,Nb) + βιLBI(H,NS) + β2LBI(Pd,Nd)
where β1 and β2 control the contribution of the last two loss components.
The self-supervized multi-class classification loss LMC is defined as
LMC(X) “ LbMaCtch(X) + α1LnMeCgloc(X) + α2LSMimC cSe(X)
“ LMC(Px,Nb,τ0)+α1LMC(Px,NS,τ1)+α2LMC(Pd,Nd,τ2)
(6)
(7)
where α1 and α2 are hyper-parameters.
Supervised Fine-Turning (the blue box in Figure 1). We directly use image labels in the training
objective. Following Mac Aodha et al. (2019), we used a presence-absence loss function which
converts the multi-class labels into binary multi-labels. Given a set of training samples Xlabeled “
t(x, y)}, the loss function LSupervized (X) is defined as:
LSupervized(X) “ βLBI(Pl, H) + LBI(H,Nl) + LBI(H,NlS)	(8)
Here, β is a hyperparameter to enlarge the weight of positive samples, and the following positive and
negative samples are used:
•	Labeled positives Pl “ t(e(x), T:,y)|(x, y) P X}.
•	Labeled negatives Nl “ t(e(x), T:,i)|(x, y) P X, i P t1..c}zty}}.
•	Sampled negative locations NlS “ t(e(x´), T：,i)l(x,y) P X,i P {1..c}}, where x~ is
one of the N evenly sampled locations from the surface of the sphere for each example x.
4 Related Work
Multi-Scale Position/Location Encoding on Spheres Recent works in various applications
(Zhong et al., 2020; Mai et al., 2020b; Mildenhall et al., 2020; Tancik et al., 2020) found that
a simple sinusoidal mapping of input coordinates (“positional encoding”) allows multilayer per-
ceptrons (MLP) to represent higher frequency content. More specifically fθ is decomposed into
fθ “ NNθ(PE(x)), where PE is a deterministic function mapping x from Rd into higher dimen-
sional space and NNθ is simply a MLP. This strategy is compelling since MLPs can be orders of
magnitude more compact than grid-sampled representations in order to achieve similar order of
details (Tancik et al., 2020). They show that passing input points through a simple Fourier feature
mapping enables a MLP to learn high-frequency functions in low-dimensional problem domains.
This development is consistent with Rahaman et al. (2019a) who show that deep networks are biased
towards learning lower frequency functions, and mapping the inputs to a higher dimensional space
using high frequency functions enables better fitting of data that contains high frequency details.
In this work we extend the success of this line of work to none Euclidean spaces such as spherical
surface. We leverage truncated Fourier transformation on spheres to achieve computation efficiency
while avoiding the error caused by projection distortion.
6
Under review as a conference paper at ICLR 2022
Unsupervised Representation Learning Unsupervised text encoding models such as trans-
former (Vaswani et al., 2017; Devlin et al., 2019) has been effectively utilized in many Natural
Language Processing (NLP) tasks. At its core, a trained model encodes words into vector space
representations based on their positions and context in the text. Following the success in NLP, there
has been significen recent progress in unsupervised image pretraining (He et al., 2020; Caron et al.,
2020; Ayush et al., 2020). Interestingly almost all of them are based on certain form of contrastive
learning (Hadsell et al., 2006), which helps to construct unsupervised classification objectives from
continuous inputs such as images. He et al. (2020) proposes Momentum Contrast (MoCo) for unsu-
pervised visual representation learning. To increase the number of negative examples in contrastive
training, they uses a queue of multiple mini-batches. Similar strategy has been adopted in NLP (Gao
et al., 2021). To improve the encoding inconsistency between mini batches, they make the target
image encoder parameterizes a moving average of the query image encoder. In this work we are
focusing on the pretraining of location encoder with a frozen image encoder. Our approach is very
memory efficient (easily scaling up to 1024 batch size) and therefore avoid the need of multi-batch
training. Ayush et al. (2020) combines a geo-location classification loss and a contrastive image
loss similar to MoCo during pre-training. For geo-location classification, a deep image network is
trained to predict a coarse geo-location of where in the world the image might come from. Since the
geo-location is not part of the classifier’s input, it is hard for the model to directly leverage it in the
prediction process. Zhai et al. (2018) learn location representation from unlabeled image-location
pairs for image localization. They apply cross entropy loss to discretized location (or time), which
cannot leverage the continuity of the approximated function. One main difference and also one of our
main contributions is our contrastive self-supervised loss, which avoids the need of discretization.
5	Experiment
5.1	The Effectiveness of Spherical Location Encoding
To test the effectiveness of Sphere2Vec we first conduct geo-aware image classification experiments in
a full supervised setting (without the self-supervised pretraining step) on seven large-scale real-world
datasets including 5 (animal) species datasets, one POI dataset (YFCC), and one remote sensing
dataset (fMoW). Please refer to Mac Aodha et al. (2019) and Christie et al. (2018) for the dataset
descriptions. Table 1 compares the result of five variants of Sphere2Vec models against nine baseline
models (see their detailed descriptions in Appendix 9.6. We can see that the Sphere2Vec models
outperform baselines on all seven datasets, and the variants with linear number of DFS terms works
as well as or even better than sphereDF S. Figure 2c-2e and 2h-2j show the predicted species
distributions from different models for two fox species. We can see that wrap and grid have over-
generalization issue in data sparse area (e.g., Africa) and the North Pole area. The distributions
produced by sphereC ` are more compact and generalizable.
To help understand the impact of Sphere2Vec , we conduct detailed analysis on iNat2017 and iNat2018.
Figure 4a shows the image locations in iNat2017 validation dataset. We split this dataset into different
latitude bands and compare the ∆MRR between each model to grid. Figure 4b and 4c show the
number of samples and ∆MRR in each band while Figure 4f shows that the contrast between these
two variables for different models. We can see that Sphere2Vec models have larger ∆MRR at
the North Pole region (Figure 4c). Moreover, Sphere2Vec has advantages on bands with less data
samples, e.g. φ P [—30°, —20。). We also compare SphereC' and grid at a quantized spatial
resolution - in latitude-longitude CellS(FigUre 4d and 4e). SphereC' has more advantages in North
Pole and data sparse cells. For more details please refer to Appendix 9.12, 9.13, and 9.14.
5.2	The Effectiveness of Self-Supervised Pretraining
Next, we investigate the impact of self-supervised contrastive pretraining (see Section 3.2). For
species recognition and POI image classification task, we use the pretrained Inception-V3 model
provided by PyTorch as image feature extractor F in the yellow box of Figure 1. As for fMoW
dataset, we use the unsupervised pretrained MoCo-V2+TP (Ayush et al., 2020) as F. Both models
are supervised fine-tuned on the respective dataset to act as g(F(∙)) in the green box of Figure 1.
We consider three self-supervised training objectives - mean square error loss MSE between the
location embedding e(x) and the image feature F(I), noise contrastive binary classification loss BI
(see Equation 6), and contrastive multi-class classification loss MC (See Equation 7). In order to
7
Under review as a conference paper at ICLR 2022
Table 1: Geo-aware image classification under fully supervised settings over three tasks: species recognition,
POI image classification (YFCC), and remote sensing (RS) image classification (fMOW (Christie et al., 2018)).
tile indicates the results reported by Mac Aodha et al. (2019). wrap* indicates the original results reported
by Mac Aodha et al. (2019) while wrap is the best results we obtained by rerunning their code. See Appendix
9.6 for details about these baselines. Since the test sets for iNat2017 and iNat2018 are not open-sourced, we
report results on validation sets. The best performance of the baseline models and Sphere2Vec are highlighted as
bold. All compared models use location only while ignoring time. The original result reported by Ayush et al.
(2020) for No Prior on fMOW is 69.05. We obtain 69.84 by retraining their implementation. Here we report
Top1 accuracy. See Appendix 9.9 for more results. See Appendix 9.8.1 for hyperparameter tuning details.
Task	Species Recognition						POI	RS
Dataset	BirdSnaP BirdSnaPt NABirdSt iNat2017 iNat2018					AVg	YFCC	fMOW
P(y|x) - Prior Type	Test	Test	Test	Val	Val	-	Test	Val
No Prior (i.e. image model)	70.07	70.07	76.08	63.27	60.2	67:94	30!5	69.84
tile (Tang et al., 2015)	70.16	72.33	77.34	66.15	65.61	70.32	50.43	-
xyz	71.85	78.97	81.2	69.39	71.75	74.63	50.75	70.18
wrap* (Mac Aodha et al., 2019)	71.66	78.65	81.15	69.34	72.41	74.64	50.70	-
wrap	71.87	79.06	81.62	69.22	72.92	74.94	50.90	70.29
wrap ' ffn	71.99	79.21	81.36	69.4	71.95	74.78	50.76	70.28
rbf(Mai et al., 2020b)	71.78	79.4	81.32	68.52	71.35	74.47	51.09	70.65
rff(Rahimi et al., 2007)	71.92	79.16	81.3	69.36	71.8	74.71	50.67	70.27
grid (Mai et al., 2020b)	71.7	79.44	81.24	69.46	72.98	74.96	51.18	70.81
theory (Mai et al., 2020b)	71.88	79.24	81.59	69.47	73.01	75.04	51.16	70.82
sphereC	72.06	79.8	81.86	69.62	73.29	75:33	3T34	71.00
sphereC '	72.41	80.02	81.81	69.66	73.31	75.44	51.28	71.03
sphereM	72.02	79.75	81.69	69.69	73.25	75.28	51.35	70.98
sphereM '	72.24	80.34	81.9	69.67	73.72	75.57	51.24	71.10
sphereDF S	71.75	79.18	81.39	69.65	73.24	75.04	51.15	71.46
(b) Samples per φ band
(a) Validation Locations
(d) ∆MRR per cell
(e) ∆MRR per cell
Figure 4: iNat2017 dataset and model performance comparison: (a) Sample locations for validation set where
the dashed and solid lines indicates latitudes; (b) The number of training and validation samples in different
latitude intervals. (c) MRR difference between a model and baseline grid on the validation dataset. (d)
∆MRR “ MRRpsphereC') — MRRpgrid) for each latitude-longitude cell. Red and blue color indicates
positive and negative ∆M RR while darker color means high absolute value. The number on each cell indicates
the number of validation data points while "1K+" means there are more than 1K points in a cell. (e) The number
of validation samples v.s. ∆MRR “ M RRpsphereC ') — M RRpgrid) per latitude-longitude cell. The
orange dots represent moving averages. (b) The number of validation samples v.s. ∆MRR per latitude band.
(c) ∆MRR per φ band
(f) ∆MRR per φ band
simulate the situations with limited labeled data we first perform self-supervised pretraining for epxq
with one of the three objectives using 100% of the unlabeled dataset Xunlabeled (e.g., iNatlist 2018).
Then we perform stratified sampling to obtain Γ portion of the labeled data, denoted as XΓ , while
keeping its label distribution the same as the full training set. Finally we fine-tune the pretrained epxq
on XΓ with the supervised learning objective (Equation 8).
8
Under review as a conference paper at ICLR 2022
Model Performance Comparison on iNat2018
Ratios of Training Labels used for Supervised Training
(a) Comparing different loss functions L
Figure 5: Comparing different self-supervised learning objectives L and different encoders epxq in a label
limited setting on iNat2018. epxq is first trained on Xunlabeled with different self-supervised objectives, and then
supervised trained with different amount of training labels. Different curves indicates different self-supervised
objectives L and different epxq (denoted as “[]”) indicated by the legend. The X axis indicates different
training label ratio Γ and the Y axis indicates the Top 1 accuracy from the joint predict of image classifier
and epxq. No Prior indicates an standalone image classifier with no location encoder. Sup. indicates training
location encoders with supervised learning loss. The shaded area along each curve indicates the standard
deviations we get after running each setting for 5 times. (a) Comparison among five models with identical model
architecture but different training objectives L on iNat2018 dataset. MSE, BI ´ LbBaItch ` LnBeIgloc ` LsBiImcse,
and MC ´ LbMaCtch ` LnMeCgloc ` LsMimC cse indicate training sphereM ` with three different self-supervised
objectives before supervised training. (b) Comparing the performance of sphereM ` and theory (the 2nd best
model on iNat2018 dataset) in both supervised only (Sup.) and self-supervised plus supervised (Sup.&MC)
settings. We can see that in both settings, sphereM ` is able to outperform theory under different ratio Γ.
Model Performance Comparison on iNat2018
Ratios of Training Labels used for Supervised Training
(b) Comparing different encoders epxq
Figure 5a compares the model performance of sphereM ` on iNat2018 with different training objec-
tives L. Each curve indicates the model performance of one training process in different supervised
label ratios Γ with identical epxq. From Figure 5a, we can see that compared with Sup. setting, all
three self-supervised objectives can improve the model performances. The largest improvements hap-
pen when Γ is small (i.e., few shot learning setting), and we get a 3.9% performance improvement
when adding MC self-supervised training stage for Γ “ 0.05. BI and MC show competitive
performance while MC is slightly better. MSE is less effective than the two. Furthermore, in order
to show the impact of self-supervised training on different epxq, we compare the performance of
sphereM ` and theory (the 2nd best model on iNat2018 ) under different L (See Figure 5b). We can
see that MC can improve the performance of both epxq while sphereM ` can outperform theory in
either supervised or self-supervised + supervised settings.
We also perform a series of ablation studies on these self-supervised training objectives. Please refer
to Appendix 9.10 for detail analysis. Moreover, we also compare MC and Sup. on the fMoW dataset
and find that MC self-supervised loss is also effective in the label limited learning setting on the
fMoW dataset (more details in Figure 8 and Appendix 9.11).
6	Conclusion
In this work, we propose a multi-scale spherical location encoder - Sphere2Vec which can encode
any location on the spherical surface into a high dimension vector. Moreover, we propose an self-
supervised learning framework which pretrains location encoders based on unlabeled geo-tagged
images. Experiment results on seven large-scale geo-aware image classification datasets shows
that Sphere2Vec can outperform multiple strong baselines. Moreover, we show that by adding the
self-supervised pretraining stage, the performance of Sphere2Vec can be further improved, especially
when labeled data is limited. In the future, we would like to explore the effectiveness of Sphere2Vec
on other machine learning tasks such as image geolocalization, geographic question answering, and
so on. Moreover, we only pretrain the location encoder while keeping the image encoder freeze
during the self-supervised training stage. In the future, we would like to explore ways to pretrain dual
encoders at the same time or early fusion of image and location data.
9
Under review as a conference paper at ICLR 2022
7	Ethics Statement
All datasets utilized in this work are publicly available. We only do some data preprocessing such as
image feature extraction. No new information is collected.
We find out that the geographic coverage of most datasets used in this work such as BirdSnap, NABird,
iNat2017, iNat2018, YFCC, and fMoW are mostly concentrated on North American, Europe while
they have poor geographic coverage on other regions and countries such as Russia, China, Africa,
and so on (See Figure 4a). This kind of geographic bias will also affect the machine learning models
trained on them as shown in Figure 4d. This paper shows the our Sphere2Vec is more robust than the
state-of-the-art models for such bias. However, how to systematically handle such geographic bias
should be an interesting research direction.
8	Reproducibility Statement
The proposed models as well as preprocessed datasets in this paper will be made public available
through GitHub.
References
Benjamin Adams, Grant McKenzie, and Mark Gahegan. Frankenplace: interactive thematic mapping
for ad hoc exploratory search. In Proceedings of the 24th international conference on world wide
web, pp. 12-22. International World Wide Web Conferences Steering Committee, 2015.
Ivan Anokhin, Kirill Demochkin, Taras Khakhulin, Gleb Sterkin, Victor Lempitsky, and Denis
Korzhenkov. Image generators with conditionally-independent pixel synthesis. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14278-14287, 2021.
Kumar Ayush, Burak Uzkent, Chenlin Meng, Kumar Tanmay, Marshall Burke, David Lobell, and
Stefano Ermon. Geography-aware self-supervised learning. arXiv preprint arXiv:2011.09980,
2020.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Robert Bartnik and Andrew Norton. Numerical methods for the einstein equations in null quasi-
spherical coordinates. SIAM Journal on Scientific Computing, 22:917-950, 03 2000.
Ronen Basri, Meirav Galun, Amnon Geifman, David Jacobs, Yoni Kasten, and Shira Kritchman.
Frequency bias in neural networks for input of non-uniform density. In International Conference
on Machine Learning, pp. 685-694. PMLR, 2020.
Thomas Berg, Jiongxin Liu, Seung Woo Lee, Michelle L Alexander, David W Jacobs, and Peter N
Belhumeur. BirdSnap: Large-scale fine-grained visual categorization of birds. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2011-2018, 2014.
Ling Cai, Krzysztof Janowicz, Gengchen Mai, Bo Yan, and Rui Zhu. Traffic transformer: Capturing
the continuity and periodicity of time series for traffic forecasting. Transactions in GIS, 24(3):
736-755, 2020.
Cyril Caminade, Sari Kovats, Joacim Rocklov, Adrian M. Tompkins, Andrew P. Morse, Felipe J.
Coldn-Gonzdlez, Hans Stenlund, Pim Martens, and Simon J. Lloyd. Impact of climate change on
global malaria distribution. Proceedings of the National Academy of Sciences, 111(9):3286-3291,
2014. ISSN 0027-8424. doi: 10.1073/pnas.1302089111. URL https://www.pnas.org/
content/111/9/3286.
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. In Hugo Larochelle,
Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances
in Neural Information Processing Systems, 2020.
10
Under review as a conference paper at ICLR 2022
Gerardo Ceballos, Paul R. Ehrlich, and Peter H. Raven. Vertebrates on the brink as indicators of
biological annihilation and the sixth mass extinction. Proceedings of the National Academy of
Sciences, 2020. ISSN 0027-8424. doi: 10.1073/pnas.1922686117. URL https://www.pnas.
org/content/early/2020/05/27/1922686117.
Matteo Chinazzi, Jessica T. Davis, Marco Ajelli, Corrado Gioannini, Maria Litvinova, Stefano
Merler, Ana Pastore y Piontti, KUnPeng Mu, Luca Rossi, KaiyUan Sun, Cecile Viboud, XinyUe
Xiong, Hongjie Yu, M. Elizabeth Halloran, Ira M. Longini, and Alessandro Vespignani. The
effect of travel restrictions on the sPread of the 2019 novel coronavirus (covid-19) outbreak.
Science, 368(6489):395-400, 2020. ISSN 0036-8075. doi: 10.1126∕science.aba9757. URL
https://science.sciencemag.org/content/368/6489/395.
Nicholas R Chrisman. Calculating on a round Planet. International Journal of Geographical
Information Science, 31(4):637-657, 2017.
Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional maP of the world. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, PP. 6172-6180,
2018.
Grace Chu, Brian Potetz, Weijun Wang, Andrew Howard, Yang Song, Fernando Brucher, Thomas
Leung, and Hartwig Adam. Geo-aware networks for fine grained recognition. In Proceedings of
the IEEE International Conference on Computer Vision Workshops, PP. 0-0, 2019.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
deeP bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, PP.
4171-4186, 2019.
Moreno Di Marco, Simon Ferrier, Tom D. Harwood, Andrew J. Hoskins, and James E. M. Watson.
Wilderness areas halve the extinction risk of terrestrial biodiversity. Nature, 573(7775):582-585,
2019. ISSN 1476-4687. doi: 10.1038/s41586-019-1567-7. URL https://doi.org/10.
1038/s41586-019-1567-7.
Ruiqi Gao, Jianwen Xie, Song-Chun Zhu, and Ying Nian Wu. Learning grid cells as vector reP-
resentation of self-Position couPled with matrix rePresentation of self-motion. In International
Conference on Learning Representations, 2019.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: SimPle contrastive learning of sentence
embeddings. In EMNLP 2021, 2021.
Michael Gutmann and Aapo Hyvarinen. Noise-contrastive estimation: A new estimation principle
for unnormalized statistical models. volume 9 of Proceedings of Machine Learning Research, PP.
297-304, Chia Laguna Resort, Sardinia, Italy, 13-15 May 2010. PMLR.
Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant
mapping. CVPR ’06, pp. 1735-1742, USA, 2006. IEEE Computer Society.
Gerrit Hansen and Wolfgang Cramer. Global distribution of observed climate change impacts. Nature
Climate Change, 5(3):182-185, 2015. doi: 10.1038/nclimate2529. URL https://doi.org/
10.1038/nclimate2529.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In 2020 IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 9726-9735, 2020.
Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network.
CoRR, abs/1503.02531, 2015. URL http://arxiv.org/abs/1503.02531.
Mike Izbicki, Evangelos E Papalexakis, and Vassilis J Tsotras. Exploiting the earth’s spherical
geometry to geolocate images. In Joint European Conference on Machine Learning and Knowledge
Discovery in Databases, pp. 3-19. Springer, 2019.
Neal Jean, Marshall Burke, Michael Xie, W Matthew Davis, David B Lobell, and Stefano Ermon.
Combining satellite imagery and machine learning to predict poverty. Science, 353(6301):790-794,
2016.
11
Under review as a conference paper at ICLR 2022
Neal Jean, Sherrie Wang, Anshul Samar, George Azzari, David Lobell, and Stefano Ermon. Tile2vec:
Unsupervised representation learning for spatially distributed data. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 33,pp. 3967-3974, 2019.
Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural network:
Data-driven traffic forecasting. In International Conference on Learning Representations, 2018.
Oisin Mac Aodha, Elijah Cole, and Pietro Perona. Presence-only geographical priors for fine-grained
image classification. In Proceedings of the IEEE International Conference on Computer Vision,
pp. 9596-9606, 2019.
Gengchen Mai, Bo Yan, Krzysztof Janowicz, and Rui Zhu. Relaxing unanswerable geographic
questions using a spatially explicit knowledge graph embedding model. In AGILE: The 22nd
Annual International Conference on Geographic Information Science, pp. 21-39. Springer, 2019.
Gengchen Mai, Krzysztof Janowicz, Ling Cai, Rui Zhu, Blake Regalia, Bo Yan, Meilin Shi, and
Ni Lao. SE-KGE: A location-aware knowledge graph embedding model for geographic question
answering and spatial semantic lifting. Transactions in GIS, 2020a. doi: 10.1111/tgis.12629.
Gengchen Mai, Krzysztof Janowicz, Bo Yan, Rui Zhu, Ling Cai, and Ni Lao. Multi-scale repre-
sentation learning for spatial feature distributions using grid cells. In The Eighth International
Conference on Learning Representations. openreview, 2020b.
Philip E. Merilees. The pseudospectral approximation applied to the shallow water equations on a
sphere. Atmosphere, 11(1):13-20, 1973. doi: 10.1080/00046973.1973.9648342.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations
of words and phrases and their compositionality. In Advances in Neural Information Processing
Systems, pp. 3111-3119, 2013.
Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and
Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020.
A Mnih and YW Teh. A fast and simple algorithm for training neural probabilistic language models.
In Proceedings of the 29th International Conference on Machine Learning, ICML 2012, volume 2,
2012.
Karen A Mulcahy and Keith C Clarke. Symbolization of map projection distortion: a review.
Cartography and geographic information science, 28(3):167-182, 2001.
Tu Dinh Nguyen, Trung Le, Hung Bui, and Dinh Phung. Large-scale online kernel learning with ran-
dom feature reparameterization. In Proceedings of the Twenty-Sixth International Joint Conference
on Artificial Intelligence, IJCAI-17, pp. 2543-2549, 2017. doi: 10.24963/ijcai.2017/354.
Steven A. Orszag. Fourier series on spheres. Mon. Wea. Rev., 102:56-75, 1974.
Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. PointNet: Deep learning on point sets
for 3D classification and segmentation. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 652-660, 2017a.
Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. PointNet++: Deep hierarchical
feature learning on point sets in a metric space. In Advances in Neural Information Processing
Systems, pp. 5099-5108, 2017b.
Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua
Bengio, and Aaron Courville. On the spectral bias of neural networks. In Kamalika Chaudhuri
and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine
Learning, volume 97 of Proceedings of Machine Learning Research, pp. 5301-5310. PMLR,
09-15 Jun 2019a.
Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua
Bengio, and Aaron Courville. On the spectral bias of neural networks. In International Conference
on Machine Learning, pp. 5301-5310. PMLR, 2019b.
12
Under review as a conference paper at ICLR 2022
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. C. Platt,
D. Koller, Y. Singer, and S. T. Roweis (eds.), Advances in Neural Information Processing Systems,
pp. 1177-1184. Curran Associates, Inc., 2008.
Ali Rahimi, Benjamin Recht, et al. Random features for large-scale kernel machines. In NIPS,
volume 3, pp. 5. Citeseer, 2007.
Cheng Sun, Jianping Li, Fei-Fei Jin, and Fei Xie. Contrasting meridional structures of stratospheric
and tropospheric planetary wave variability in the northern hemisphere. Tellus A: Dynamic
Meteorology and Oceanography, 66(1):25303, 2014.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2818-2826, 2016.
Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh
Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn
high frequency functions in low dimensional domains. In H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33,
pp. 7537-7547. Curran Associates, Inc., 2020.
Kevin Tang, Manohar Paluri, Li Fei-Fei, Rob Fergus, and Lubomir Bourdev. Improving image
classification with location context. In Proceedings of the IEEE international conference on
computer vision, pp. 1008-1016, 2015.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,匕Ukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information
Processing Systems, pp. 5998-6008, 2017.
David Williamson and Gerald Browning. Comparison of grids and difference approximations for
numerical weather prediction over a sphere. Journal of Applied Meteorology, 12:264-274, 02
1973.
Bo Yan, Krzysztof Janowicz, Gengchen Mai, and Rui Zhu. xNet+SC: Classifying places based
on images by incorporating spatial contexts. In 10th International Conference on Geographic
Information Science (GIScience 2018). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2018.
Menghua Zhai, Tawfiq Salem, Connor Greenwell, Scott Workman, Robert Pless, and Nathan Jacobs.
Learning geo-temporal image features. In British Machine Vision Conference (BMVC), 2018.
Ellen D Zhong, Tristan Bepler, Joseph H Davis, and Bonnie Berger. Reconstructing continuous
distributions of 3d protein structure from cryo-em images. In International Conference on Learning
Representations, 2020.
13
Under review as a conference paper at ICLR 2022
Figure 6: An illustration for map projection distortion: (a)-(d): Tissot indicatrices for four projections. The equal
area circles are putted in different locations to show how the map distortion affect its shape.
9	Appendix
9.1	The Map Projection Distortion Problem
In fact there is no map projection, which can preserve distances in all directions. The so-called
equidistant projection can only preserve distance on one direction, e.g., the longitude direction for
the equirectangular projection (See Figure 6d), while the conformal map projections (See Figure 6a)
can preserve directions while resulting in a large distance distortion. For a comprehensive overview
of map projections and their distortions, see Mulcahy & Clarke (2001). This is a well recognized
problem in Cartography which shows the importance of calculating on a round planet Chrisman
(2017). Due to the limitations of these 2D location encoders, there is an urgent need for a location
encoding method which preserves the spherical distance (e.g., great circle distance5) between two
points. The multi-scale encoding method utilizes Double Fourier Sphere basis (OpS2q terms) or a
subset (OpSq terms) of it while still being able to correctly measure the spherical distance. This
inspires us to explore the most effective subset of Fourier bases on spheres.
9.2	sphereC,sphereC+,sphereM,sphereM+
sphereC Inspired by the fact that any point px, y, zq in 3D Cartesian coordinate can be expressed by
sin and cos basis of spherical coordinates (λ, φ plus radius) 6, we define the basic form of Sphere2Vec,
namely sphereC encoder, for scale s as
S—1
G(SPhereC) PX)= U [sin φs, CoS φs CoS λs, CoS φs Sin λs].	(9)
s“0
It can be shown that when S = 1, GpsphereCq pX) directly satisfies our expectation in Equation 1 where
f (x) = CoSpR). See more detailed analysis and comparison to the grid encoder in Appendix 9.3.
sphereM Considering the fact that many geographical features are more sensitive to either latitude
(e.g., temperature, sunshine duration) or longitude (e.g., timezones, geopolitical borderlines), we
might want to focus on increasing the resolution of either φ or λ while the other is hold relatively
at large scale. Therefore, we introduce a multi-scale position encoder sphereM , where interaction
terms between φ and λ always have one of them fixed at top scale:
S—1
GpsphereMq pX) = U rSin φs, CoS φs CoS λ, CoS φ CoS λs, CoS φs Sin λ, CoS φ Sin λss.	(10)
s“0
This new encoder ensures that the φ term interact with all the scales of λ terms and λ term interact
with all the scales of φ terms. Note that when S = 1, GpsphereMq is equivalent to GpsphereCq . Both
sphereC and sphereM are multi-scale versions of a spherical distance-kept encoder (See Equation
12) and keep that as the main term in their multi-scale representation.
5https://en.wikipedia.org/wiki/Great- circle_distance
6https://en.wikipedia.org/wiki/Spherical_coordinate_system
14
Under review as a conference paper at ICLR 2022
sphereC+ and sphereM+ From the analysis of the two proposed encoders and the state-of-the-art
grid encoders (Appendix 9.3), we know that grid pays more attention to the sum of cos difference of
latitudes and longitudes, while our proposed encoders pay more attention to the spherical distances.
In order to capture both information, we consider merging grid with each proposed encoders to get
more powerful models that encode geographical information from different angles.
GPsphereC+q(x) = GPsphereCq(Xq Y Gpgril) (χ),
GpsphereM') (χ) “ GpsphereM) (χ) Y G^gr^id) (χq
(11)
9.3	ANALYSIS OF sphereC AND ITS COMPARISON TO THE grid ENCODER
To illustrate that sphereC is good at capturing spherical distance, we take a close look at its basic
case S “ 1 (define s “ 0 and fs “ 1), where the multi-scale encoder degenerates to
GppsphereC) (Xq “ rsin(φq, cos(φq cos(λq, cos(φq sin(λqs.
(12)
These three terms are included in the multi-scale version (S > 1) and serve as the main terms at the
largest scale and also the lowest frequency (when s “ S ´ 1). The high frequency terms are added
to help the downstream neuron network to learn the point-feature more efficiently. Interestingly,
Gp psphereC) captures the spherical distance in a very explicit way:
Theorem 2. Let X1 , X2 be two points on the same sphere with radius R, then
XGpsphereC) (X1), GpSPhereC)(X2) “ cos(∆D),
where ∆D is the great circle distance between X1 and X2. Under this metric,
}GpsphereC)(XI) ´ GpsphereC)(X2)} “ 2sE(δd).
2R
Moreover, }GpsphereC)(XI) — GpsphereC)(X2)} « ɪ,when ∆D is small w.r.t. R.
(13)
(14)
See the proof in Appendix 9.4. Since the central angle ∆δ “ 簧 P [0, ∏S and Cos(X) is strictly
monotonically decrease for x P r0, πs, Theorem 2 shows that GppsphereC) (Xq directly satisfies our
expectation in Equation 1 where f (x) “ cos(R). In comparison, when S = 1, the inner product in
the output space of grid encoder is
XGpgrid)(X1), Gpgrid)(X2)〉“ cos(φι ´ 02)+ cos(λι ´ λ2),
(15)
which models the latitude difference and longitude difference of X1 and X2 separately rather than
spherical distance. This introduces problems in encoding. For instance, consider data pairs X1 “
(λ1, φq and X2 “ (λ2, φq, the distance between them in output space of grid, }Gppgrid)(X1q ´
Gppgrid)(X2q}2 “ 2 ´ 2 cos(λ1 ´ λ2q stays as a constant in terms of φ. However, when φ varies from
一∏2 to ∏2, the actual spherical distance changes in a wide range, e.g., the actual distance between the
data pair at φ “ 一 ∏2 (South Pole) is 0 while the distance between the data pair at φ = 0 (Equator),
gets the maximum value. This issue in measuring distances also has a negative impact on grid’s
ability to model distributions in areas with sparse sample points because it is hard to learn the true
spherical distance. We observe that grid reaches peak performance at much smaller rmin than that of
Sphere2Vec encodings. Moreover, sphereC outperforms grid near polar regions where grid claims
large distance though the spherical distance is small (A, B in Figure 1).
9.4	Proof of Theorem 2
Proof. Since GppsphereC) (Xi) “ rsin(φi), cos(φi) cos(λi), cos(φi) sin(λi)s for i “ 1, 2, the inner
product
xGppsphereC)(X1), GppsphereC)(X2)〉
“s
in(φ1q sin(φ2q + cos(φ1q cos(λ1q cos(φ2q cos(λ2q + cos(φ1q sin(λ1q cos(φ2q sin(λ2q (16)
in(φ1q sin(φ2q + cos(φ1q cos(φ2q cos(λ1 ´ λ2q
“s
“ cos(∆δq “ cos(∆D{Rq,
15
Under review as a conference paper at ICLR 2022
where ∆δ is the central angle between x1and x2, and the spherical law of cosines is applied to derive
the second last equality. So,
}Gppx1q ´ Gppx2q}2
“ xGppx1 q ´ Gppx2q, Gppx1q ´ Gppx2qy
“ 2 ´ 2 cosp∆D{Rq
“ 4 sin2 p∆D{2Rq.
(17)
So }G(xι) — Gg)} “ 2sm(∆D∕2R) since ∆D/2R P [0, ∏∏]. By Taylor expansion, }Gp(xι)—
GG(x2)} « ∆D∕R when ∆D is small w.r.t. R.	□
9.5	Proof of Theorem 1
@* P tsphereC, SphereC', SphereM, SphereM+}, Gp*q(xι) “ Gp*q(x2) implies
sin(φ1q “ sin(φ2q,	(18)
cos(φ1q sin(λ1q “ cos(φ2q sin(λ2q,	(19)
cos(φ1q cos(λ1q “ cos(φ2q cos(λ2q,	(20)
from S “ 0 terms. Equation 18 implies φ1 “ φ2. If φ1 “ φ2 “ π∕2, then both points are at North
Pole, λι “ λ2 equal to whatever longitude defined at North Pole. If φι “ φ2 “ —∏∕2, it is similar
case at South Pole. When φι “ φ2 P (— ∏, π∏), cos(φι) “ cos(φ2) ‰ 0. Then from Equation 19 and
20,
sin λ1 “ sin(λ2), cos(λ1) “ cos(λ2),	(21)
which shows that λι “ λ?. In summary, xι “ x2, so Gp*q is injective.
If * “ SphereDFS, Gp*q(xι) “ Gp*q(x2) implies
sin(φ1) “ sin(φ2), cos(φ1) “ cos(φ2), sin(λ1) “ sin(λ2),cos(λ1) “ cos(λ2),	(22)
which proves xι “ x2 and Gp*q is injective directly.
9.6	Baselines
In order to understand the advantage of sphereical-distance-kept location encoders, we compare
different versions of Sphere2Vec with multiple baselines:
•	No P rior indicates a image classifier without using any location information, i.e., predicting
image labels purely based on image information P (y|I).
•	tile divides the study area A (e.g., the earth surface) into grids with equal intervals along
the latitude and longitude direction. Each grid has an embedding to be used as the encoding
for every location x fall into this grid. This is a common practice by many previous work
when dealing with coordinate data (Berg et al., 2014; Adams et al., 2015; Tang et al., 2015).
•	wrap is a location encoder model introduced by Mac Aodha et al. (2019). Given a location
x “ (λ, φ), it uses a coordinate wrap mechanism to convert each dimension of x into
2numbers- Gpwrapq(X) = IsE(π λ-q, cos(π λ-q, sin(π4q, Cospn福)].Then the
180	180	90	90
results are passed through a multi-layered fully connected neural network NNpwrapq ()
which consists of an initial fully connected layer, followed by a series of h residual blocks,
each consisting of two fully connected layers (k hidden neurons) with a dropout layer in
between. We adopt the official code of Mac Aodha et al. (2019)7 for this implementation.
7http://www.vision.caltech.edu/~macaodha/projects/geopriors/
16
Under review as a conference paper at ICLR 2022
•	wrap ` ffn is similar to wrap except that it replaces NNpwrapq pq with NNffnpq, a
simple learnable multi-layer perceptron with h hidden layers and k neurons per layer as
that Sphere2Vec has. wrap ` ffn is used to exclude the effect of different NNpq on the
performance of location encoders. In the following, all location encoder baselines use
NNffnpq as the learnable neural network component so that we can directly compare the
effect of different position encoding G(∙) on the model performance.
•	xyz first converts xi “ pλi , φiq P S2 into 3D Cartesian coordinates px, y, zq centered at the
sphere center by following Equation 23 before feeding into a multilayer perceptron NNpq.
Here, we let px, y, zq to locate on a unit sphere with radius R “ 1. xyz can be treated as a
special case of sphereC where S “ 1.
Gpxyzq pxq “ rz, x, ys “ rsin φ, cos φcos λ, cos φ sin λs	(23)
•	rbf randomly samples M points from the training dataset as RBF anchor points
k x ´ xanchor k2
{xanchor, m = 1 …M}, and use gaussian kernels exp ' —	i——-m--------) on each
2σ2
anchor points, where σ is the kernel size. Each point pi has a M -dimension RBF feature
vector which is fed into NNffnpq to obtain the location embedding. This is a strong baseline
for representing floating number features in machine learning models.
•	rff, i.e., Random Fourier Features (Rahimi & Recht, 2008; Nguyen et al., 2017), first en-
codes location X into a D dimension vector - GPrff q (x) “ 夕(x) “ ?2 [cos (ωf X ' biqSD“i
where ωi i.„i.d Np0, δ2Iq and bi is uniformly sampled from r0, 2πs. I is an identity matrix.
Each component of 夕(x) first projects X into a random direction ωi and makes a shift by bi.
Then it wraps this line onto the unit cirle in R2 with the cosine function. Rahimi & Recht
(2008) show that 夕(XqT夕(x1) is an unbiased estimate of the Gaussian kernal K(x, x1).
HXq is consist of D different estimates to produce a further lower variance approximation.
To make rff comparable to other baselines, we feed 夕(x) into NN f加()to produce the
final location embedding.
•	grid is a multi-scale location encoder on 2D Euclidean space proposed by Mai et al. (2020b).
Here, we simply treat X “ (λ, φ) as 2D coordinate. It first use Gpgridq (X) shown in Equation
3 to encode location X into multi-scale representation and then feed it into NNffn() to
produce the final location embedding.
•	theory is another multi-scale location encoder on 2D Euclidean space proposed by Mai
et al. (2020b). It use a position encoder Gptheoryq (X) shown in Equation 24.. Here, Xs “
[λs,φsS = rfs, φS and aι = [1,0]T, a2 = [-1/2, ?3/2ST, a3 = [—1/2, —?3/2ST P R2
are three unit vectors which orient 2π{3 apart from each other. The encoding results are
feei.nto NNffn () to produce the final location embedding.
S—1 3
GPtheoryq(Xq = U U [sin(Xxs, aj〉), cos(<xs, aj〉)].	(24)
s“0 j“1
9.7	Model Implementation Detail
All types of Sphere2Vec as well as all baseline models we compared share the same model set up
-epxq(x) = NNffn(GP*q(x)). Here, Gp*q(∙) is a position encoder which preprocesses the input
coordinates X = (λ, φ) P S2 and outputs a position embedding Gp*q(x). There are no learnable
parameter in Gp*q (x). The position encoders Gp*q (•) used for different Sphere2Vec are discussed
in detail in Section 3.1 while Gp*q(∙) of all baselines are discussed in Appendix 9.6. We can see
that Gp*q(∙) of grid, theory and different types of Sphere2Vec encode the input coordinates in a
multi-scale fashion by using different sinusoidal functions with different frequencies. Many previous
work call this practice “Fourier input mapping” (Rahaman et al., 2019b; Tancik et al., 2020; Basri
et al., 2020; Anokhin et al., 2021). The difference is that grid and theory use the Fourier features
from 2D Euclidean space while our Sphere2Vec uses all or the subset of Double Fourier Sphere
Features to take into account the sphereical geometry and the distance distortion it brings.
17
Under review as a conference paper at ICLR 2022
Except for wrap, NNf 加(∙)used by all the other location encoders is a learnable multi-layer PerceP-
tron (MLP) with h hidden layers and k neurons per layer. We use the original MLP implementation
Provided by Mai et al. (2020b). Here, each layer of NNffnpq is a linear layer followed by a droPout
layer, a nonlinear activation function, a layer normalization layer (Ba et al., 2016), and a skiP con-
nection. Different from other location encoders, wrap uses a different imPlementation of the full
connected layer NNWfnp(∙). Please refer to Appendix 9.6 and Mac Aodha et al. (2019) for the
detailed implementation of NNwrnp(∙).
All models are implemented in PyTorch. We use the original implementation of wrap from
Mac Aodha et al. (2019) and the implementation of grid and theory from Mai et al. (2020b).
We train/evaluate each model on a Ubuntu machine with 2 GeForce GTX Nvidia GPU cores, each of
which has 10GB memory.
9.8	Model Training Detail and Hyperparameter Tuning
To train different types of Sphere2Vec as well as baseline models, we use Adam as the optimizer in
both unsupervised and supervised training phase. Basically, we first do unsupervised training with
an initial learning rate ηunsuper by following either Equation 6 or 7 as the unsupervised training
objective. In this setting, we predict the corresponding image embedding based on the input location.
Only location encoder is trained during the unsupervised training phase while we keep the image
encoder freezed as shown in Figure 1. Here, we just use a image encoder pretrained on other image
datasets such as ImageNet and use it as a image feature extractor F. After the model is convergence,
we use a new learning rate ηsuper to do supervised training by following Equation 8.
9.8.1	Full Supervised Training Setting
In order to test the effectiveness of our spherical encoding method - SPhere2Vec, We train all location
encoders in a fully supervised setting. In order to find the best hyperparameter combinations for
each model on each dataset, We use grid search to do hyperparameter tuning including supervised
training learning rate ηsuper “ r0.01, 0.005, 0.002, 0.001, 0.0005, 0.00005s, the number of scales
S “ r16, 32, 64s, the minimum scaling factor rmin “ r0.10.050.020.010.0050.0010.0001s, the
number of hidden layers and number of neurons used in NNffn(∙) - h = [1, 2, 3,4] and k “
[256,512,1024], the dropout rate in NNffn(∙) - dropout = [0.1,0.2,0.3,0.4,0.5,0.6,0.7]. We
also test multiple options for the nonlinear function used for NNffn (∙) including ReLU, LeakyReLU,
and Sigmoid. The maximum scaling factor rmax can be determined based on the range of latitude φ
and longitude λ. For grid and theory, We use rmax “ 360 and for all SPhere2Vec, We use rmax “ 1.
As for rbf and rff, We also tune their hyperparamaters including kernel size σ “ [0.5, 1, 2, 10] as
Well as the number of kernels M “ [100, 200, 500].
Based on hyperparameter tuning, We find out using 0.5 as the dropout rate and ReLU as the nonlinear
activation function for NN ffn (∙) works best for every location encoder. Based on the hyperparameter
tuning, the best hyperparameter combinations are selected for different models on different datasets.
The best results are reported in Table 1. Note that each model has been running for 5 times and
its mean Top1 score is reported. Due to the limit of space, the standard deviation of each model’s
performance on each dataset is not included in Table 1. However, we report the standard deviations
of all models’ performance on three datasets in Appendix 9.9.
Moreover, we find out ηsuper and rmin are the most important hyperparameters. Table 2 shows
the best hyperparameter combinations of different SPhere2Vec models on different species image
classification dataset. We use a smaller S for sphereDF S since it has O(S2q terms while the
other models have O(Sq terms. sphereDF S with S “ 8 yield a similar number of terms to the
other models wth S “ 32 (See Table 3). Interestingly, all first four SPhere2Vec models (sphereC,
sphereC `, sphereM, and sphereM `) shows the best performance on all five datasets with the
same hyperparamter combinations. Note that compared with other datasets, iNat2017 and iNat2018
are more up-to-date datasets with more training samples and better geographic coverage. This indi-
cates that the proposed 4 SPhere2Vec models show similar performance over different hyperparameter
combinations.
18
Under review as a conference paper at ICLR 2022
Table 2: The best hyperparameter combinations of Sphere2Vec models on different image classification datasets.
The learning rate ηsuper tends to be smaller for larger datasets; The best S is 8 for sphereDF S and 32 for all
others; and we fix the maximum scale rmax as 1. Here, rmin indicates the minimum scale. h and k are the
number of hidden layers and the number of neurons in NNpq respectively.
Dataset	ηsuper	r min	k
BirdSnaP	0.001	10-6	512
BirdSnap：	0.001	io´4	1024
NABirds：	0.001	10-4	1024
iNat2017	0.0001	lo´^	1024
iNat2018	0.0005	10-3	1024
Table 3: Dimension of position encoding for different models in terms of total scales S
Model	SphereC	SphereC'	SphereM	SphereM'	SphereDFS
Dimension 3S	6S	5S	8S	4S* 1 2 + 4S^^
We also find out that using a deeper MLP as NNf fn(∙), i.e., a larger h does not necessarily lead to
better classification accuracy. In many cases, one hidden layer - h “ 1 achieves the best performance
for many kinds of location encoders. We discuss this in detail in Appendix 9.9.
9.8.2 Unsupervised Training Setting
In order to test the effectiveness of different unsupervised learning objectives, we also conduct
another set of experiments by unsupervised training different location encoders before the supervised
training phase. In addition to the hyperparameter we discussed in Section 9.8.1, we also fine tune
the hyperparameters used in the unsupervised learning phase including the unsupervised training
learning rate ηunsuper, the negative location loss weight α1, SimCSE loss weight α2, the number of
negative locations we used |Ns|, three temperature τ0, τ1, and τ2. As for unsupervised objective LBI,
we also fine tune β1 and β2. The hyperparameters we have searched and the hyperparameter tuning
results for the unsupervised learning setting are shown in Table 5, 6, 7 in Appedix 9.10.
9.9 More Experimental Results on Geo-Aware Image Classification Tasks
Table 4 is a complementary of Table 1 which compares the performance of SphereM ` with all
baseline models on the geo-aware image classification task. The results on three datasets are shown
here including BirdSnaPt NABirdsf, and iNat2018. For each model, We vary the depth of its
NNffn(q, i.e., h “ r1, 2, 3, 4s. The best evaluation results With each h are reported. Moreover, We
run each model With one specific h 5 times and report the standard deviation of the Top1 accuray,
indicated in “()”. Several important observations can be made based on Table 4:
1. Although the absolute performance improvement betWeen SphereM ` and the best baseline
model is not very large - 0.90%, 0.28%, and 0.71% for three datasets respectively, these
performance improvements are statistically significant given the standard deviations
of these Top1 scores.
2. These performance improvements are comparable to those from the previous studies
on the same tasks. In other Words, the small margin is due to the nature of these datasets.
For example, Mai et al. (2020b) shoWed that grid or theory has 0.79%, 0.44% absolute
Top1 accuracy improvement on BirdSnapf and NABirdsf dataset respectively. Mac Aodha
et al. (2019) shoWed that wrap has 0.09%, 0%, 0.04% absolute Top1 accuracy improvement
on BirdSnap, BirdSnapf and NABirdsf dataset. Here, We only consider the results of wrap
that only uses location information, but not temporal information. Although Mac Aodha
et al. (2019) shoWed that compared With tile and nearest neighbor methods, wrap has
3.19% and 3.71% performance improvement on iNat2017 and iNat2018 datatset, these large
margins are mainly because the baselines they used are rather Weak. When We consider the
typical rbf and rff (Rahimi et al., 2007) used in our study, their performance improvement
are doWn to -0.02% and 0.61%.
19
Under review as a conference paper at ICLR 2022
Table 4: The impact of the depth h of multi-layer perceptrons NNffn pq on Top1 accuracy. The numbers
in “()” indicates the standard deviations estimated from 5 independent train/test runs. We find that the model
performances are not very sensitive to NNffnpq, and, in most cases, one layer NNffnpq achieve the best result.
In other words, the larger performance gaps in fact comes from different Gp*q (∙) We use. Moreover, given the
performance’s variance of each model, we can see that sphereM ` outperforms other baseline models on all
these three datasets and the margins are statistically significant. The same conclusion can be drawn based on our
experiments on other datasets. Here, we only show results on three datasets as an illustrative example.
Dataset		BirdSnaPt NABirdSt	iNat2018
P(y|x) - Prior Type	h	-Test	Test	V0T~
xyz	丁 2 3 4	78.81 (0.10)^^81.08 (0.05)^^71.6 (0.08) 78.83 (0.10)	81.2 (0.09)	71.7 (0.02) 78.97 (0.06)	81.11 (0.06) 71.75 (0.04) 78.84 (0.09)	81.02 (0.03) 71.71 (0.03)
wrap (Mac Aodha et al., 2019)	丁 2 3 4	79.04 (0.13)^^81.60 (0.04)^^72.89 (0.08) 78.94 (0.13)	81.62	(0.04)	72.84	(0.07) 79.08 (0.15)	81.53	(0.02)	72.92	(0.05) 79.06 (0.11)	81.51	(0.09)	72.77	(0.06)
wrap ` ffn	丁 2 3 4	78.97 (0.09)^^81.23 (0.06)^^71.90 (0.05) 79.02 (0.15)	81.36	(0.04)	71.95	(0.05) 79.21 (0.14)	81.35	(0.05)	71.94	(0.04) 79.06 (0.09)	81.27	(0.13)	71.93	(0.04)
rbf(Mai et al., 2020b)	丁 2 3 4	79.40 (0.13)^^81.32 (0.08)^^71.02 (0.18) 79.38 (0.12)	81.22	(0.11)	71.29	(0.20) 79.40 (0.04)	81.31	(0.07)	71.35	(0.21) 79.25 (0.05)	81.30	(0.07)	71.21	(0.19)
rff(Rahimi et al., 2007)	丁 2 3 4	78.96 (0.18)^^81.27 (0.07)^^71.76 (0.06) 78.97 (0.04)	81.28	(0.05)	71.71	(0.09) 79.07 (0.12)	81.30	(0.11)	71.80	(0.04) 79.16 (0.13)	81.22	(0.11)	71.46	(0.05)
grid (Mai et al., 2020b)	丁 2 3 4	79.44 (0.11)^^81.24(0.06)^^72.98 (0.05) 79.07 (0.05)	81.13 (0.04)	72.9 (0.03) 79.27 (0.10)	81.04 (0.05) 72.72 (0.04) 78.88 (0.02)	80.78 (0.07) 72.49 (0.03)
theory (Mai et al., 2020b)	丁 2 3 4	79.24 (0.15)^^81.23 (0.02)^^73.01 (0.09) 79.16 (0.21)	81.31	(0.14)	72.70	(0.02) 78.89 (0.21)	81.07	(0.01)	72.57	(0.07) 79.06 (0.18)	80.72	(0.12)	72.33	(0.08)
sphereM `	2 3 4	80.34 (0.08)~81.86 (0.03)~73.72 (0.07) 79.91 (0.12)	81.84 (0.04)	73.46	(0.03) 80.06 (0.06)	81.90 (0.05)	73.73	(0.02) 79.92 (0.14)	81.84 (0.10)	73.21	(0.04)
3. By comparing the performances of the same model with different depth of its NNffnpq,
i.e., h, we can see that the model performance is not sensitive to h. In fact, in most
cases, one layer NNffn pq achieve the best result. This indicates that the depth of the
MLP does not significantly affect the model performance and a deeper MLP does not
necessarily lead to a better performance. In other words, the systematic bias (i.e.,
distance distortion) introduced by grid and theory can not later be compensated by
a deep MLP. It shows the importance of designing a spherical-distance-aware location
encoder.
9.10	Ablation Study on Different Self-Supervised Loss
Both the BI (Equation 6) and MC (Equation 7) unsupervised loss have three loss component: in-
batch, negative location, SimCSE loss component. How does each of them contribute to the overall
unsupervised training? In order to answering this question, we do ablation studies on these two
20
Under review as a conference paper at ICLR 2022
(a) Ablation Study on BI
Figure 7: Ablation study on different unsupervised learning setting on iNat2018 dataset. (a) Alblation study on
BI loss. BI ´ LbBaItch ` LnBeIgloc ` LsBiImcse indicates the full BI loss while BI ´ LbBaItch ` LnBeIgloc deletes the
SimCSE component. BI ´ LbBaItch additionally deletes the negative location loss component. Comparing among
those three model settings can help us understand the effect of different loss components. Similarly, we do the
same ablation study on MC loss and show in (b).
(b) Ablation Study on MC
objectives by using iNat2018 dataset as an example. Figure 7a and 7b illustrate the ablation study
results.
From Figure 7a, we see that as for BI, adding LnBeIgloc will significantly increase the model perfor-
mance especially when Γ is large. Adding LsBiImcse also improves the model performance when Γ is
small. As for the ablation study on MC loss as shown in Figure 7b, adding LnMeCgloc can significantly
increase the model performance and adding LsMimC cse can also make the performance slightly better.
9.11	Effectiveness of Unsupervised Pretraining on fMoW dataset
Figure 8: Comparison among two models with identical model architecture but different training objectives
on fMoW dataset. Each point on each curve indicates a specific training process. We repeat each of them for
five times and show the standard deviations as shaded areas along the line. We can see that with statistically
significance, MC unsupervised loss is also effective in the few shot learning setting on the fMoW dataset.
9.12	Impact of MRR by The Number of Samples at Different Latitude Bands
See Figure 9
21
Under review as a conference paper at ICLR 2022
Table 5: Ablation Study on unsupervised loss LMC over iNat2018 dataset. We show the effect of different
hyperparematers of LMC on the performance of location encoders. We use sphereM ` as an representative
location encoder and use supervised training dataset ratio Γ as 0.5. lr indicates the learning rate used for
unsupervised LMC training. dropout indicates the dropout rate used by location encoder which will affect the
SimCSE loss as Gao et al. (2021) shows. α1 and α2 are weights for the negative location loss LnMeCglocpXq and
SImCSE loss LsMimC cse pXq component respectively. τ0, τ1, and τ2 are the temperatures used by three different
loss components. See Equation 7 for the explanation.
Γ	epxq	L	ηunsuper。1 |N s| a T0 Ti T dropout	Top1
0.5	SphereM '	LMC	0.001 0.0005 0.0002 0.00005 1	1	1	1	1	1	0.5 0.00001 0.000005 0.000001	71.31 71.86 71.78 71.76 71.83 71.7 71.69
			2 0.0005 0l5	1	1	1	1	1	0.5 0	71.8 71.86 71.71 71.42
			2 0.0005	1	1 0l5 1	1	1	0.5 0	71.71 71.86 71.81 71.82
			64 32 24 22 20 18 0.0005	1	1	1	12	1	1	0.5 8 4 2 1 0.1 0.01	71.85 71.95 71.74 71.81 72.08 71.77 72.02 71.87 71.93 71.69 71.89 71.86 71.67 71.63
			2 0.0005	1	1	1 20	1	1	0.5 0.5	71.85 72.08 71.76
			2 0.0005	1	1	1 20	1	1	0.5 0.5	71.84 72.08 71.69
			1 0.0005	1	4	1 20	1	1	0.5 8	72:08- 71.84 71.69
			017- 0.6 0.5 0.0005	1	1	1 20	1	1	0.4 0.3 0.2 0.1	71.38 71.91 72.08 71.55 71.08 69.96 68.17.
22
Under review as a conference paper at ICLR 2022
Table 6: Ablation Study on unsupervised loss LBI over iNat2018 dataset. We show the effect of different
hyperparematers of LBI on the performance of location encoders. We use sphereM ` as an representative
location encoder ad use supervised training dataset ratio Γ as 0.5. lr indicates the learning rate used for
unsupervised LMC training.
Γ		L	ηunsuper	βl	|N s|	尸2	dropout	Top1
0.5	SphereM '	LBI	-0.001 0.0008 0.0005 0.0002	ι	ι	0	0 5 0.0001	1	1	0 0.00001 0.000005 0.000002	71.76 71.87 71.85 72.02 71.67 71.62 71.48 71.56
			2 0.0002	015	1	0.5 0	71.98 72.02 71.85 71.65
			1 4 0.0002	1	4	0.5 8 16	72.02 71.82 71.94 71.84
			0.00 0.10 0.0002	1	1	0.20	0.5 0.50 1.00	72.02 72.06 71.93 71.83 71.84.
Table 7: Ablation Study on unsupervised loss MSE over iNat2018 dataset. We show the effect of different
hyperparematers of MSE on the performance of location encoders. We use sphereM ` as an representative
location encoder ad use supervised training dataset ratio Γ as 0.5. lr indicates the learning rate used for
unsupervised MSE training.
Γ	epxq	L	ηunsuper	drθpθUt Tθp1
0.5	sphereM '	MSE	-0.001	71.05 0.0005	71.41 0.0001	71.55 0.00001	0.5	71.58 0.000005	71.69 0.000002	71.71 0.000001	71.6
(p⅛) HHW ,CΦPO≡HHW
log 10(#samples in latitude band)
(a) iNat2017
(b) iNat2018
Figure 9: Impact of MRR by The Number of Samples at Different Latitude Bands.
23
Under review as a conference paper at ICLR 2022
(a) Feather duster worm
(e) White-browed wagtail
(i) Arctic fox
(m) Scarlet leafwing
(q) African pied hornbill
(u) False Tiger Moth
(g) grid
(h) SPhereC+
(f) wrap
(o) grid
(P) sphereC +
(n) wrap
(S) grid
(t) SPhereC+
(r) wrap
(w)grid
(x) SPhereC+
(v) wrap
Figure 10: Compare the predicted distributions of example species from different models. The first figure of
each row marks the data points from iNat2018 training data.
(b) wrap*
(j) wrap
(c) grid
(k) grid
(d) SPhereC+
(l) sphereC +
9.13	Predicted Distributions iNat2018
We plot the predicted species distributions from different models at different geographic regions, and
compare them with the training sample locations of the corresponding species, see Figure 10. We can
see that compared with wrap* and grid, in each geographic region with sparse training samples and
the North Pole area, the spatial distributions produced by sphereC ` are more compact while the
other two have over-generalization issue.
9.14	Embedding Clustering
We use the location encoder trained on iNat2017 or iNat2018 dataset to produce a location embedding
for the center of each small latitude-longitude cell. Then we do agglomerative clustering8 on all these
embeddings to produce a clustering map. Figure 11 and 12 show the clustering results for different
models with different hyperparameters on iNat2017 and iNat2018 dataset.
8https://scikit-learn.org/stable/modules/generated/sklearn.cluster.
AgglomerativeClustering.html
24
Under review as a conference paper at ICLR 2022
Figure 11: Embedding clusterings of iNat2017 models. (a) wrap* with 4 hidden ReLU layers of 256 neurons;
(d) rbf with the best kernel size σ “ 1 and number of anchor points m “ 200; (b)(c)(e)(f) are Space2Vec
models Mai et al. (2020b) with different min scale Tm%n = {10-6, lθ´2}.a (g)-(l) are different Sphere2Vec
models.b
a They share the same best hyperparameters: S “ 64, rmax “ 1, and 1 hidden ReLU layers of 512 neurons.
b They share the same best hyperparameters: S “ 32, rmax “ 1, and 1 hidden ReLU layers of 1024 neurons.
9.15	Experiments with Synthetic Dataset
9.15.1	Synthetic Dataset Generation
To further investigate the effectiveness of our proposed spherical location encoder Sphere2Vec, we
construct a set of synthetic datasets.
We utilize the Von Mises-Fisher distribution (VMF) (Izbicki et al., 2019), an analogy of the 2D
Gaussian distribution on the spherical surface S2 to generate synthetic data points9. The probability
density function of vMF is defined as
VMFpx; μ,κq = 2∏ Sinh(Kq exppκμTXPXqq
where χpxq “ rx, y, zs “ rcos φ cos λ, cosφsinλ, sin φs
(25)
Here, χpxq conVerts x into a coordinates in the 3D Euclidean space, and on the surface of a unit
sphere. A VMF distribution is controlled by two parameters - the mean direction μ P R3 and
concentration parameter K P R'. μ indicates the center of a VMF distribution which is a 3D unit
Vector. κ is a positiVe real number which controls the concentration of VMF. A higher κ indicates
more compact VMF distribution, while K “ 1 correspond to a VMF distribution with standard
deViation coVering half of the unit sphere.
To simulate multi-modal distributions, we assume C classes with eVen prior, and each classes follows
a VMF distribution. To create a dataset we first sample C sets of parameters {(μi, Ki)} (C = 50).
Then we draw SP samples for each class (SP “ 100). So in total, each generated synthetic dataset
has 5000 data points for 50 balanced classes.
9https://www.tensorflow.org/probability/api_docs/python/tfp/
distributions/VonMisesFisher#sample
25
Under review as a conference paper at ICLR 2022
⑴ theory Prmin “ 10 q
Figure 12: Embedding clusterings of iNat2018 models. (a) wrap* with 4 hidden ReLU layers of 256 neurons;
(d) rbf with the best kernel size σ “ 1 and number of anchor points m “ 200; (b)(c)(e)(f) are Space2Vec
models (Mai et al., 2020b) with different min scale rmin “ {10”, lθ´3 }.a (g)-(l) are Sphere2Vec models with
different min scale.b
a They share the same best hyperparameters: S “ 64, rmax “ 1, and 1 hidden ReLU layers of 512 neurons.
b They share the same best hyperparameters: S “ 32, rmax “ 1, and 1 hidden ReLU layers of 1024 neurons.
The concentration parameter κi is sampled by first drawing r from an uniform distribtuion
U pκmin , κmax q, and then take the square r2 . The square helps to avoid sampling many large
κi which yield very concentrated vMF distributions that are rather easy to be classified. We fix
κmin “ 1 and vary κmax in r16, 32, 64, 128s.
For the center parameter μi we adopt two sampling approaches:
1.	Uniform Sampling: We uniformly sample C centers (μi) from the surface of a unit sphere.
We generate four synthetic datasets (for different values of κmax) and indicate them as U1,
U2, U3, U4. See Table 8 for the parameters we use to generate these datasets.
2.	Stratified Sampling: We first evenly divide the latitude range [一∏/2, π∕2] into Nμ intervals.
Then we uniformly sample Cμ centers (μi) from the spherical surface defined by each
latitude interval. Since the latitude intervals in polar regions have smaller spherical surface
area, this stratified sampling method has higher density in the polar regions. We keep
Nμ X Cμ = C = 50 fixed and varies Nμ in [5,10, 25, 50]. Combined with the 4 Kmax
choices, this procedure yields 16 different synthetic datasets. We denote them as Si.j. See
Table 8 for the parameters we use to generate these datasets.
Figure 13a-13d visualize the data point distributions of these four synthetic datasets based on the
uniform sampling method in 2D space. Figure 13e visualized the U4 dataset in a 3D Euclidean
space. We can see that when κmax is larger, the variation of point density among different vMF
distributions becomes larger. Some vMF are very concentrated and the resulting data points are
easier to be classified. Moreover, if we treat these datasets as 2D data points as grid and theory
do, vMF distributions in the polar areas will be stretched to very extended shapes making model
learning more difficult. However, this kind of systematic bias can be avoided if we use a spherical
location encoder as Sphere2Vec.
26
Under review as a conference paper at ICLR 2022
(b) U2 dataset in 2D degree Space (κmax “ 32)
(a) U1 dataset in 2D degree Space (κmax “ 16)
Figure 13: The data distributions of four synthetic datasets (U1, U2, U3, and U4) generated from the uniform
sampling method. (e) shows the U4 dataset in a 3D Euclidean space. We can see that if we treat these datasets as
2D data points as grid and theory, the vMF distributions in the polar areas will be stretched and look like 2D
aniostropic multivariate Gaussian distributions. However, this kind of systematic bias can be avoided if we use a
spherical location encoder as Sphere2Vec.
(c) U3 dataset in 2D degree Space (κmax “ 64)
(d) U4 dataset in 2D degree Space (κmax “ 128)
(e) U4 dataset in 3D space (κmax “ 128)
Figure 14 visualizes the data distributions of four synthetic datasets with stratified sampling method.
They have different Nμ but the same κmaχ. We can see that when Nμ increases, a more fine-grain
stratified sampling is carried out. The resulting dataset has a larger data bias toward the polar areas.
9.15.2 Synthetic Dataset Evaluation Results
We evaluate all baseline models as well as sphereM ` on those generated 20 syhthetic datasets
as described above. For each model, we do grid search on their hyperparameters for each dataset
including supervised learning rate ηsuper , the number of scales S, the minimum scaling factor
rmin, the number of hidden layers and number of neurons used in NNffn(∙) - h and k. The best
performance of each model is reported in Table 8. Some observations can be made from Table 8:
27
Under review as a conference paper at ICLR 2022
⑹ S3.3 dataset in 2D degree Space (Nμ “ 25)
(d) S4.3 dataset in 2D degree Space (Nμ = 50)
Figure 14: The data distributions of four synthetic datasets (S1.3, S2.3, S3.3, and S4.3) generated from the
stratified sampling method With Kmax = 64. We can see that when Nμ increases, a more fine-grain stratified
sampling is carried out. The resulting dataset has a larger data bias toward the polar areas.
1.	sphereM ` is able to outperform all baselines on all 20 synthetic datasets. The absolute
Top1 improvement can go up to 2% and the error rate deduction can go up to 30.8%. This
shows the robustness of sphereM `.
2.	When the dataset is fairly easy to classify (i.e., all baseline models can produce 95+% Top1
accuracy), sphereM ` is still able to further improve the performance and gives a very
large error rate reduction (up to 30.8%). This indicates that sphereM ` is very robust and
reliable for datasets with different distribution characteristics.
3.	Comparing the error rate of different stratified sampling generated datasets (S1.j - S4.j)
we can see that when we keep Kmax fixed and increase Nμ, the relative error reduction
ER become larger. Increasing Nμ means we do a more fine-grain stratified sampling.
The resulting datasets should sample more vM F distributions in the polar regions. This
phenomenon shows that when the dataset has a larger data bias towards the polar area,
we expect sphereM ` will be more effective.
28
Under review as a conference paper at ICLR 2022
Table 8: Compare sphereM ` to baselines on synthetic datasets. U1 - U4 indicate 4 synthetic datasets generated
based on the uniform sampling approach (see Appendix 9.15.1). S1.1 - S4.4 indicate 16 synthetic datasets
generated based on the stratified sampling apprach. For all datasets have C “ 50 and SP “ 100. For each
model, we perform grid search on its hyperparameters for each dataset and report the best Top1 accuracy. The
∆T op1 column shows the absolute performance improvement of sphereM ` over the best baseline model
(bolded) for each dataset. The ER column shows the relative reduction of error compared to the best baseline
model (bolded). We can see that sphereM ` can outperform all other baseline models on all of these 20
synthetic datasets. The absolute Top1 accuracy improvement can be as much as 2.0% for datasets with lower
precisions, and the error rate deduction can be as much as 30.8% for datasets with high precisions.
ID	Method	Nμ Cμ	Kmin KmaX	Xyz wrap wrap+ffn rff rbf grid theory	SPhereM '	∆Top1	ER
^U1			16-	67.2 67.0^^66.9^^66.8 46.6 68.6 67.8	-692-	0.6	^19^
U2			1	32	73.1 75.1	73.9	72.3 58.4 76.2 76.5	77.4	0.9	-3.8
	uniform	--	1				
U3			1	64	86.1 90.1	88.3	89.0 91.7 92.3 92.7	93.3	0.6	-8.2
U4			128	91.8 94.9	92.3	92.5 97.4 97.5 97.7	98.0	0.3	-13.0
S11			16-	68.7 69.7	688^^68.6 70.5 69.5 69.4	-723-	1.8	"-6?T
S1.2			32	76.7 79.1	78.1	78.4 81.1 81.2 79.2	82.9	1.7	-9.0
S1.3		5 10	1	64	91.2 92.5	92.9	92.6 94.7 94.8 94.9	95.4	0.5	-9.8
S1.4			128	86.5 91.6	88.3	92.4 93.5 95.2 94.9	96.1	0.9	-18.7
^SΣ1			16-	70.5 71.3	707^^70.4 46.6 72.0 70.7	-74.0-	2.0	^Tr
S2.2			32	76.1 79.7	78.2	78.6 61.2 80.9 80.5	82.3	1.4	-7.3
S2.3		10 5	1	64	88.0 89.9	88.2	88.5 80.0 92.5 91.9	93.3	0.8	-10.7
S2.4	stratified		128	94.4 96.6	96.7	95.5 94.0 97.6 97.6	98.1	0.5	-20.8
S3.1			16-	66.2 66.3	647^^65.6 67.1 66.7 66.7	-683-	1.2	^3£~
S3.2			32	80 82.5	80.7	81.6 83.4 84.5 82.1	85.9	1.4	-9.0
S3.3		25 2	1	64	85.4 86.0	85.7	86.2 89.1 89.6 88.6	91.0	1.4	-13.5
S3.4			128	93.2 96.0	94.8	95.7 97.2 97.3 97.4	98.0	0.6	-23.1
S4.1			16-	64.8 67.4	66.0^^66.3 66.9 67.1 64.5	-68.4-	-1-	^31T
S4.2			32	75.6 78.2	77.4	77.4 78.4 80.1 78.3	81.0	0.9	-4.5
S4.3		50 1	1	64	91.3 93.9	93.7	93.8 95.0 95.2 94.0	96.1	0.9	-18.7
S4.4			128	94.3 95.5	94.4	94.7 95.4 97.4 96.5	98.2	0.8	-30.8
29