Under review as a conference paper at ICLR 2022
A Geometric Perspective on Variational Au-
TOENCODERS
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we propose a geometrical interpretation of the Variational Autoen-
coder framework. We show that VAEs naturally unveil a Riemannian structure of
the learned latent space. Moreover, we show that using these geometrical con-
siderations can significantly improve the generation from the vanilla VAE which
can now compete with more advanced VAE models on four benchmark datasets.
In particular, we propose a new way to generate samples consisting in sampling
from the uniform distribution deriving intrinsically from the Riemannian manifold
learned by a VAE. We also stress the proposed method’s robustness in the low data
regime which is known as very challenging for deep generative models. Finally,
we validate the method on a complex neuroimaging dataset combining both high
dimensional data and low sample sizes.
1	Introduction
Variational Autoencoders (VAE) (Kingma & Welling, 2014; Rezende et al., 2014) are powerful
generative models that map complex input data into a much lower dimensional space referred to
as the latent space while driving the latent variables to follow a given prior distribution. Their
simplicity to use in practice has made them very attractive models to perform various tasks such as
high-fidelity image generation (Razavi et al., 2020), speech modeling (Blaauw & Bonada, 2016),
clustering (Yang et al., 2019) or data augmentation (Chadebec et al., 2021).
Nonetheless, when taken in their simplest version, it was noted that these models produce most of
the time blurry samples. This undesired behavior may be due to several limitations of the VAE
framework. First, the training of a VAE aims at maximizing the Evidence Lower BOund (ELBO)
which is only a lower bound on the true likelihood and so does not ensure that we are always
actually improving the true objective (Burda et al., 2016; Alemi et al., 2016; Higgins et al., 2017;
Cremer et al., 2018; Zhang et al., 2018). Second, the prior distribution used as regularization and
for sampling may be too simplistic (Dai & Wipf, 2018) leading to poor data generation and there
exists no guarantee that the actual distribution of the latent code will match a given prior distribution
inducing over-regularization (Connor et al., 2021). Hence, trying to tackle those limitations through
richer posterior distributions (Salimans et al., 2015; Rezende & Mohamed, 2015) or better priors
(Tomczak & Welling, 2018) represents a major part of the proposed improvements over the past few
years. However, the tractability of the ELBO constrains the choice in distributions and so finding a
trade-off between model expressiveness and tractability remains crucial.
In this paper, we take a rather different approach and focus on the geometrical aspects a vanilla VAE
is able to capture in its latent space. In particular, we propose the following contributions:
•	We show that VAEs unveil naturally a latent space with a structure that can be modeled as a
Riemannian manifold through the learned covariance matrices in the posterior distributions.
•	We propose a natural sampling scheme consisting in sampling from a uniform distribution
defined on the learned manifold and given by the metric. We show that this procedure im-
proves significantly the generation process from a vanilla VAE and makes it able to perform
as well as more advanced VAE models in terms of Frechet Inception Distance (Heusel et al.,
2017) and Precision and Recall (Sajjadi et al., 2019) scores on four benchmark datasets.
•	We also show that the propose method appears more robust to dataset size changes and
outperforms even more strongly peers when only smaller sample sizes are considered.
1
Under review as a conference paper at ICLR 2022
•	We validate the method on complex neuroimaging data from OASIS (Marcus et al., 2007).
2	Variational Autoencoders
Considering that we are given x ∈ X a set of data points deriving from an unknown distribution
p(x), a VAE aims at inferring such a distribution with a parametric model {pθ, θ ∈ Θ} using a
maximum likelihood estimator. A key assumption behind the VAE is to assume that the generation
process also involves latent variables z living in a lower dimensional space such that the generative
model writes
Z 〜qprior(z)	； X 〜Pθ(x∣z),
where qprior is a prior distribution over the latent variables often taken as a standard Gaussian and
Pθ(x|z) is referred to as the decoder and is most of the time taken as a parametric distribution the
parameters of which are estimated using neural networks. Hence, the likelihood pθ writes:
Pθ(X)= ∕pθ(x∣z)q(z)dz .	(1)
Z
As this integral is most of the time intractable so is pθ(z|x), the posterior distribution. Hence,
Variational Inference (Jordan et al., 1999) is used and a simple parametrized variational distribution
qφ(z|x) is introduced to approximate the posterior pθ(z|x). qφ(z∣x) is referred to as the encoder
and, in the vanilla VAE, qφ is chosen as a multivariate Gaussian whose parameters μφ and Σφ are
again given by neural networks. An unbiased estimate of the likelihood pθ (X) can then be derived
using importance sampling with qφ(z∣x) and the ELBO objective follows using Jensen,s inequality:
logPθ(x) = logEZ〜qφ [pθ] ≥ EZ〜qφ [logP] = EZ〜qφ logpθ(x∣z) - Dkl(qφ(z∣x)kp(z))
'-----------------{Z----------------} (2)
ELBO
The Evidence Lower BOund (ELBO) is now tractable since both pθ(x∣z) and qφ(z∣x) are
parametrized and so can be optimized with respect to the encoder and decoder parameters.
Remark 1 In practice, pθ(x∣z) is chosen depending on the modeling of the input data but is often
taken as a simple distribution (e.g multivariate Gaussian, Bernoulli ...). Hence, the ELBO can also
be seen as a two terms objective (Ghosh et al., 2020). The first one is a reconstruction term given
by pθ (x|z) while the second one is a regularizer corresponding to the KL divergence between the
posterior and the prior. For instance, in the case of a multivariate Gaussian we have
LREC = ∣∣X - μθ(Z)Il2,	LREG = DκL(qφ(z∣x)kp(z)) .	(3)
3	Related Work
A natural way to improve the generation from VAEs consists in trying to use more complex priors
(Hoffman & Johnson, 2016) than the standard Gaussian distribution used in the initial version such
that they better match the true distribution of the latent codes. For instance, using a Mixture of
Gaussian (Nalisnick et al., 2016; Dilokthanakul et al., 2017) or a Variational Mixture of Posterior
(VAMP) (Tomczak & Welling, 2018) as priors was proposed. In the same vein, hierarchical latent
variable models (S0nderby et al., 2016; Klushyn et al., 2019) or prior learning (Chen et al., 2016;
Aneja et al., 2020) have recently emerged and aimed at finding the best suited prior distribution
for a given dataset. Acceptance/rejection sampling method was also proposed to try to improve the
expressiveness of the prior distribution (Bauer & Mnih, 2019). Some recent works linking energy-
based models (EBM) and VAEs (Xiao et al., 2020) or modeling the prior as an EBM (Pang et al.,
2020) have demonstrated promising results and are also worth citing .
On the ground that the latent space must adapt to the data as well, geometry-aware latent space mod-
elings as hypershpere (Davidson et al., 2018), torus (Falorsi et al., 2018) or Poincare disk (Mathieu
et al., 2019) or discrete latent representations (Razavi et al., 2020) were proposed. Other recent con-
tributions proposed to see the latent space as a Riemannian manifold where the Riemannian metric
is given by the Jacobian of the generator function (Arvanitidis et al., 2018; Chen et al., 2018; Shao
et al., 2018). This metric was then used directly within the prior modeled by Brownian motions
2
Under review as a conference paper at ICLR 2022
(Kalatzis et al., 2020). Others proposed to learn the metric directly from the data throughout train-
ing thanks to geometry-aware normalizing flows (Chadebec et al., 2020) or learn the latent structure
of the data using transport operators (Connor et al., 2021). While these geometry-based methods
show interesting properties of the learned latent space they either require the computation of a time
consuming model-dependent function, the Jacobian, or add further parameters to the model to learn
the metric or transport operators adding some computational burden to the method.
Arguing that VAEs are essentially autoencoders regularized with a Gaussian noise, Ghosh et al.
(2020) proposed another interesting interpretation of the VAE framework and showed that other
types of regularization may be of interest as well. Since the generation process from these autoen-
coders is no longer relying on the prior distribution, the authors proposed to use ex-post density
estimation by fitting simple distributions such a Gaussian mixture in the latent space. While this
paves the way for consideration of other ways to generate data, it mainly reduces the VAE frame-
work to an autoencoder while we believe that it can also unveil interesting geometrical aspects.
Another widely discussed improvement of the model consists in trying to tweak the approximate
posterior in the ELBO so that it better matches the true posterior using MCMC methods (Sali-
mans et al., 2015) or normalizing flows (Rezende & Mohamed, 2015). For instance, methods using
Hamiltonian equations in the flows to target the true posterior (Caterini et al., 2018) were proposed.
Finally, while discussing the potential link between PCA and autoencoders some intuitions arose on
the impact of both the intrinsic structure of the variance of the data (Rakowski & Lippert, 2021) and
the shape of covariance matrices in the posterior distributions (Rolinek et al., 2019) on disentangle-
ment in the latent space. We also believe that these covariance matrices indeed play a crucial role
in the modeling of the latent space but in this paper, we instead propose to see their inverse as the
value of a Riemannian metric evaluated at the embedding points μ%.
4	Proposed Method
In this section, we argue that a vanilla VAE shows naturally a Riemannian structure of the latent
space through the learned covariance matrices in the posterior distributions. We then propose a new
natural generation scheme guided by this estimated geometry and consisting in sampling from a
uniform distribution deriving intrinsically from the learned Riemannian manifold.
4.1	A Word on Riemannian Geometry
First, we briefly recall some basic elements of Riemannian geometry needed in the rest of the paper.
A more detailed discussion on integration and probability densities on manifolds may be found in
Appendix A. A d-dimensional manifold M is a manifold which is locally homeomorphic to a d-
dimensional Euclidean space. If the manifold M is further connected and differentiable it possesses
a tangent space Tz at any z ∈ M composed of the tangent vectors of the curves passing by z. IfM
is equipped with a smooth inner product g = h∙∣∙)z defined on its tangent space Tz for any Z ∈ M
then M it is called a Riemannian manifold and g is the associated Riemannian metric. Since g is
an inner product, a local representation of g at any z ∈ M is given by the positive definite matrix
G(z). The notion of length of curves γ : R → M traveling in M can be defined as follows
1	1
L(Y)=/ qhγ⑴ιγ⑴ iγ(t)dt=/ /γ⑴ >G(Y(U)Y⑴ dt.
00
Curves minimizing L are geodesics and a Riemannian distance between z1 , z2 ∈ M can be defined
distG(z1,z2) = inf L(Y)	s.t.	z1 = Y(0),z2 = Y(1) .	(4)
γ
The manifold M is said to be geodesically complete if all geodesic curves can be extended to R. In
an Euclidean space, G reduces to the Id and the distance becomes the classic Euclidean one.
Remark 2 A simple extension of this Euclidean framework consists in assuming that the metric
is given by a constant positive definite matrix Σ different from Id. In such a case the induced
Riemannian distance is the well-known Mahalanobis distance which writes
distΣ
(z2 - z1)>Σ(z2 -z1).
3
Under review as a conference paper at ICLR 2022
4.2	The Riemannian Gaussian Distribution
The notion of measure and so of probability distribution can be extended to geodesically complete
Riemannian manifolds as well (Pennec, 2006). Given the Riemannian manifold M endowed with
the Riemannian metric G and a chart z, an infinitesimal volume element may be defined on each
tangent space Tz of the manifold M as follows
dMz = Pdet G(z)dz,	(5)
with dz being the Lebesgue measure. Hence, a Riemannian Gaussian distribution on M can be
defined and consists in using the Riemannian distance defined in Eq. 4 instead of the Euclidean one
Nriem(z∣σ, μ) = 3xp ( -	^ ), C = ZexP ( - di*z, ")2 ) dMz ,(6)
C	2σ	2σ
M
where dMz is the volume element defined by Eq. 5. Hence, the multivariate normal distribution is
only a specific case of the Riemannian distribution with σ = 1, defined on the manifold M = Rd
endowed with the constant Riemannian metric G(z) = Σ-1, ∀z ∈ M.
4.3	Geometrical Interpretation of the VAE Framework
Within the VAE framework, the variational distribution qφ(z∣x) is voluntarily chosen as a sim-
ple multivariate Gaussian distribution defined on Rd with d being the latent space dimension.
Hence, as explained in the previous section, given an input data point xi, the posterior qφ(z∣x) =
N(μ(xi), Σ(xi)) can also be seen as a Riemannian Gaussian distribution where the Riemannian
distance is simply the distance with respect to the metric tensor Σ-1 (xi). Hence, the VAE frame-
work can be seen as follows. As with an autoencoder, the VAE provides a code μ(xi) which is a
lower dimensional representation ofan input data point xi. However, it also gives a tensor Σ-1(xi)
depending on Xi which can be seen as the value of a Riemannian metric G at μ(xi) i.e.
G(μ(χi)) = ∑-1 (Xi).
This metric is crucial since it impacts the notion of distance in the latent space now seen as the Rie-
mannian manifold M = (Rd, G) and so changes the directions that are favored in the sampling from
the posterior distribution qφ(z∣x). Then, a sample z is drawn from a standard (i.e. σ = 1 in Eq. 6)
Riemannian Gaussian distribution and fed to the decoder. As first approximation and since we only
have access to a finite number of metric tensors Σ-1(Xi), the VAE model assumes that the metric is
locally constant close to μ(xi) and so the Riemannian distance reduces to the Mahalanobis distance
in the posterior distribution. This simplified drastically the training process since now Riemannian
distances have closed form and so are easily computable. Interestingly, the VAE framework will
impose through the ELBO expression given in Eq. 3, that Z gives a sample X 〜pθ(x∣z) close to Xi
when decoded. Since z has a probability density function imposing higher probability for samples
having the smallest Riemannian distance to μ, the VAE imposes in a way that latent variables that
are close in the latent space with respect to the metric G will also provide samples that are close
in the data space X in terms of L2 distance as noticed in Remark. 1. Noteworthy is that the latter
distance can be amended through the choice of the decoderpθ(X∣z). This is aan interesting property
since it allows the VAE to directly link the learned Riemannian distance in the latent space to the
distance in the data space. The regularization term in Eq. 3 ensures that the covariance matrices do
not collapse to 0d and constraints the latent codes to remain close to the origin easing optimization.
Finally, at the end of training, we have a lower dimensional representation of the training data given
by the means of the posteriors μ(xi) and a family of metric tensors (Gi = ∑-1(xi)) corresponding
to the value of a Riemannian metric defined locally on the latent space. Inspired from Hauberg et al.
(2012), we propose to build a smooth continuous Riemannian metric defined on the entire latent
space by performing the following interpolation:
G(Z) = XX ∑-1(xi) ∙ ωi(z) + λ ∙ Id,	ωi(z)=exp (-2-：产,μi)2 ) ,	(7)
where dist∑-13)(z, μ%) = (z - μi)>Σ-1(xi)(z - μi) is the Riemannian distance between Z
and μi with respect to the locally constant metric G(μ(xi)) = Σ-1(xi). Since the sum in Eq. 7
4
Under review as a conference paper at ICLR 2022
is made on the total number of training samples N, the number of reference metric tensors can be
decreased for huge datasets by selecting only k < N metric tensors1 and increasing ρ to reduce
memory usage. We provide an ablation study on the impact of λ, the number of centroids k and
their choice along with a discussion on the choice for ρ in Appendix G. Then, we have:
Proposition 1 The Riemannian manifold M = (Rd, G) is geodesically complete.
Prop. 1 (proved in Appendix B) allows now to refer to probability densities on M. Rigorously, the
metric defined in Eq. 7 should have been used during the training process. Nonetheless, this would
have made the training longer and trickier since it would involve i) the computation of Riemannian
distances that have no longer close form and so make the resolution of the optimization problem
in Eq. 4 needed, ii) the sampling from Eq. 6 which is not trivial and iii) the computation of the
regularization term. Instead, by approximating the value of the metric during training by its value
at μi (i.e. ∑-1(xi)), the VAE training remains unchanged, stable and computationally reasonable
since Riemannian Gaussians become multivariate Gaussians in qφ(z∣x). Noteworthy is the fact that,
likewise (Ghosh et al., 2020), in our vision of the VAE, the prior distribution is only seen as a
regularizer though the KL term and other latent space regularization schemes may have been also
envisioned. In the following, we keep the proposed vision and do not amend the training process.
4.4	Geometry- Aware Sampling
Assuming that the VAE has learned a latent representation of the data in a space seen as a Rieman-
nian manifold, we propose to exploit this strong property to enhance the generation procedure. A
natural way to sample from such a latent space would consist in sampling from the uniform distribu-
tion intrinsically defined on the learned manifold. Similar to the Gaussian distribution presented in
Sec. 4.2, the notion of uniform distribution can indeed be extended to Riemannian manifolds. Given
a bounded set A ⊂ M, the uniform distribution writes (Pennec, 2006)
pA(z)
IA(Z)
Vol(A)
IA(Z)
R lA(z)dMz
M
This density is taken with respect to dMz, the Riemannian measure but using Eq. 5 anda coordinate
system z allows to obtain a pdf now defined with respect to the Lebesgue measure:
URiem(Z) Y VZdet G(Z).
Since the Riemannian metric has a closed form expression given by Eq. 7, sampling from this
distribution is quite easy and may be performed using the HMC sampler (Neal, 2005) for instance.
Now we are able to sample from the intrinsic uniform distribution which is a natural way of exploring
the estimated manifold and the sampling is guided by the geometry of the latent space. A discussion
on practical outcomes can be found in Appendix. C.
4.5	Illustration on a Toy Dataset
The usefulness of such sampling procedure may be easily appended in Figure 1 where a vanilla
VAE was trained with a toy dataset composed of binary images of disks and rings of different size
and thickness (example inspired by Chadebec et al. (2021)). On the left is presented the learned
latent space along with the embedded training points given by the colored dots. The log of the
metric volume element is given in gray scale. In this example, we clearly see a geometrical structure
appearing since the disks and rings seem to wrap around each other. Obviously, sampling using
the prior (taken as a N(0, Id)) in such a case is far from being optimal since the sampling will
be performed regardless of the underlying distribution of the latent variables and so will create
irrelevant samples. To further illustrate this, we propose to interpolate between points in the latent
space using different cost functions. Dashed lines represent affine interpolations while the solid ones
show interpolation aiming at minimizing the potential V(Z) = (，det G(Z))T all along the curve
i.e. solving the minimization problem
inf
γ
1
V(γ(t))dt
0
s.t. γ(0) = Z1, γ(1) = Z2 .
(8)
1This may be performed with k-medoids algorithm for instance.
5
Under review as a conference paper at ICLR 2022
(a)
(b)
Figure 1: Top: Visualization and interpolation in a 2D latent space learned by a vanilla VAE trained
with binary images of rings and disks. The log of the metric volume element，det G(Z) (also
proportional to the log of the density we propose to sample from) is represented in gray scale. Top
right: The Riemannian distance from a starting point is presented with the color maps. The dashed
lines are affine interpolations between two points in the latent space and the solid ones are obtained
by solving Eq. 8. Bottom: Decoded samples along the interpolation curves.
Below are presented the decoded samples all along the interpolation curves. Thanks to those interpo-
lations we can see that i) the latent space seems to really have a specific geometrical structure since
decoding all along the interpolation curves obtained by solving Eq. 8 leads to qualitatively satisfying
results, ii) certain locations of the latent space must be avoided since sampling there will produce
irrelevant samples (see red frames and corresponding red dashes). Using the proposed sampling
scheme will allow to sample in the white areas and so ensure that the sampling remains close to the
data i.e. where information is available and so does not produce irrelevant images when decoded.
5	Experiments
In this section, we conduct a comparison with other VAE models using other regularization schemes,
more complex priors, richer posteriors, ex-post density estimation or trying to take into account
geometrical aspects including our method. In the following and to ensure a fair comparison, all the
models share the same auto-encoding neural network architectures described in Appendix E.
5.1	Generation with Benchmark Datasets
First, we compare the proposed sampling method to several VAE variants such as a Wasserstein
Autoencoder (WAE) (Tolstikhin et al., 2018), Regularized Autoencoders (Ghosh et al., 2020) with
either L2 decoder’s parameters regularization (RAE-L2), gradient penalty (RAE-GP), spectral nor-
malization (RAE-SN) or simple L2 latent code regularization (RAE), a vamp-prior VAE (VAMP)
(Tomczak & Welling, 2018), a Hamiltonian VAE (HVAE) (Caterini et al., 2018), a geometry-aware
VAE (RHVAE) (Chadebec et al., 2020) and an Autoencoder (AE). We elect these models since they
use different ways to generate the data using either the prior or ex-post density estimation. For the
latter, we use the approach of Ghosh et al. (2020) and fit a 10-component mixture of Gaussian in
the latent space after training. The models are trained on MNIST (LeCun, 1998), SVHN (Netzer
et al., 2011), CIFAR 10 (Krizhevsky et al., 2009) and CELEBA (Liu et al., 2015) and we keep the
model achieving the best validation loss. See Appendix E for the comprehensive experimental set-
up. Figure 2 shows a qualitative comparison between the resulting generated samples for MNIST
and CELEBA, the same plots are made available in Appendix D for SVHN and CIFAR 10. Interest-
6
Under review as a conference paper at ICLR 2022
MNIST
CELEBA
AE - N
VAE - N
WAE
VAMP
HVAE
RHVAE
AE - GMM
VAE - GMM
RAE
VAE - Ours
Figure 2: Generated samples with different models and generation processes. Generated samples
with RAE variants are also provided in Appendix D.
Gen. Near. train Near. rec. Gen. Near. train Near. rec. Gen. Near. train Near. rec. Gen. Near. train Near. rec.
Figure 3: Nearest train image (near. train) and nearest image in all reconstructions of train images
(near. recon) to the generated one (Gen.) with the proposed method. Note: the nearest reconstruction
may be different from the reconstruction of the nearest train image.
ingly, using the non-prior based methods seems to produce qualitatively better samples (rows 7 to
end). Nonetheless, the resulting samples seem even sharper when the sampling takes into account
geometrical aspect of the latent space as we propose (last row). Additionally, even though the exact
same model is used, we clearly see that using the proposed method represents a strong improvement
of the generation process from a vanilla VAE when compared to the samples coming from a normal
prior (second row). This insists on the fact that even the simplest VAE model actually contains a lot
of information in its latent space but the limited expressiveness of the prior impedes to access to it.
Hence, using more complex prior such as the VAMP may be a tempting idea. However, one must
keep in mind that the ELBO objective in Eq. 2 must remain tractable and so using more expressive
priors may be impossible. These observations are even more supported by Table 1 where we report
the Frechet Inception Distance (FID) and the precision and recall (PRD) score against the test set
to assess the sampling quality and diversity. Again, fitting a mixture of Gaussian (GMM) in the
latent space appears to be an interesting idea since it allows for a better expressiveness and latent
space prospecting. For instance, on MNIST the FID falls from 40.7 with the prior to 13.1 when
using a GMM. Nonetheless, with the proposed method we are able to make it even smaller (8.5) and
PRD scores higher without changing the model and performing post processing. This can also be
observed on the 3 other datasets. Impressively, in almost all cases, the proposed generation method
7
Under review as a conference paper at ICLR 2022
Table 1: FID (lower is better) and PRD score (higher is better) for different models and datasets.
In the first section are presented results using the prior distribution while in the second one, we use
ex-post density estimation by fitting a 10-component mixture of Gaussian in the latent space.
Model	MNIST (16)		SVHN (16)		CIFAR 10 (32)		Celeba (64)	
	FID J	PRD ↑	FID J	PRD ↑	FID J	PRD ↑	FID J	PRD ↑
AE - N (0, 1)	46.41	0.86/0.77	119.65	0.54/0.37	196.50	0.05/0.17	64.64	0.29/0.42
WAE	20.71	0.93/0.88	49.07	0.80/0.85	132.99	0.24/0.52	54.56	0.57/0.55
VAE - N (0, 1)	40.70	0.83/0.75	83.55	0.69/0.55	162.58	0.10/0.32	64.13	0.27/0.39
VAMP	34.02	0.83/0.88	91.98	0.55/0.63	198.14	0.05/0.11	73.87	0.09/0.10
HVAE	15.54	0.97/0.95	98.05	0.64/0.68	201.70	0.13/0.21	52.00	0.38/0.58
RHVAE	36.51	0.73/0.28	121.69	0.55/0.41	167.41	0.12/0.22	55.12	0.45/0.56
AE - GMM	9.60	0.95/0.90	54.21	0.82/0.83	130.28	0.35/0.58	56.07	0.32/0.48
RAE (GP)	9.44	0.97/0.98	61.43	0.79/0.78	120.32	0.34/0.58	59.41	0.28/0.49
RAE (L2)	9.89	0.97/0.98	58.32	0.82/0.79	123.25	0.33/0.54	54.45	0.35/0.55
RAE (SN)	11.22	0.97/0.98	95.64	0.53/0.63	114.59	0.32/0.53	55.04	0.36/0.56
RAE	11.23	0.98/0.98	66.20	0.76/0.80	118.25	0.35/0.57	53.29	0.36/0.58
VAE - GMM	13.13	0.95/0.92	52.32	0.82/0.85	138.25	0.29/0.53	55.50	0.37/0.49
VAE - Ours	8.53	0.98/0.97	46.99	0.84/0.85	93.53	0.71/0.68	48.71	0.44/0.62
can either compete or outperform peers both in terms of FID and PRD scores. Finally, we check
if the proposed method does not overfit the training data and is able to produce diverse samples by
showing the nearest neighbor in the train set and the nearest image in all the reconstructions of the
train images to a generated image in Figure 3. This experiment shows that the generated samples are
not only resampled train images and that the sampling prospects quite well the manifold. To support
even more this claim we provide in Appendix G an analysis in a case where only two centroids are
selected in the metric. This also shows that the generated samples are not only an interpolation be-
tween the k selected centroids since some generated images contain attributes that are not present in
the images of the decoded centroids. The outcome of such an experiment is that using post training
latent space processing such as ex-post density estimation or adding some geometrical considera-
tion to the model allows to strongly improve the sampling without adding more complexity to the
model. Generating 1000 samples on CELEBA takes approx. 5.5 min for our method vs. 4 min for a
10-component GMM and 0.6s for prior based methods on a single GPU V100-16GB.
RAE-GP - GMM
RAE-L2 - GMM
VAE-GMM
VAE-Ours
RAE-SN - GMM
WAE- N (0,1)
RAE-GP - N(0,1)
RAE-L2 - N(0,1)
RAE-SN- N (0,1)
VAE-N (0,1)
WAE - GMM
Figure 4: Evolution of the FID score according to the number of training samples.
5.2	Investigating Generation Robustness in Low Data Regime
We perform a comparison using the same models and datasets as before but we decide to progres-
sively decrease the size of the training set to see the robustness of the different sampling methods
according to the number of samples. This experiment is rarely performed in most generative models
related papers even though it is well known that such a context may be very challenging for these
models. Nonetheless, it appears to us very important since in day-to-day applications collecting
such large databases may reveal costly if not impossible (think of medicine for instance). Hence,
we consider MNIST, CIFAR10 and SVHN and use either the full dataset size, 10k, 5k or 1k training
samples. For each experiment, the best retained model is again the one achieving the best ELBO
8
Under review as a conference paper at ICLR 2022
Table 2: Classification results averaged on 20 independent runs. For the generative models, the
classifier is trained on 2K generated samples per class.
Generation method	Balanced Accuracy	F1 AD	CN
Original (unbalanced)	66.2 ± 7.6	47.6 ± 15.8	87.3 ± 2.0
Original (resampled)	81.8 ± 2.6	72.1 ± 3.6	88.0 ± 2.3
AE-N(0,Id)	50.0 ± 0.0	0.0 ± 0.0	84.1 ± 0.0
WAE	57.4 ± 9.7	21.0± 24.5	84.4 ± 2.3
VAE - N (0, Id)	51.8 ± 3.8	6.1 ± 11.8	84.6 ± 1.1
VAMP	83.1 ± 2.6	70.4 ± 3.6	82.2 ± 4.7
HVAE	56.3 ± 7.9	19.6 ± 21.7	85.4 ± 1.7
RHVAE	68.0 ± 10.9	47.0 ± 24.2	85.1 ± 3.3
AE - GMM	82.4 ± 2.3	69.5 ± 3.1	82.0 ± 3.6
RAE (GP)	63.9 ± 9.8	46.5 ± 15.9	70.6 ± 19.6
RAE (L2)	74.1 ± 6.0	60.6 ± 9.5	82.1 ± 5.9
RAE (SN)	62.3 ± 8.9	37.8 ± 22.6	80.1 ± 7.9
RAE	69.3 ± 8.1	53.8 ± 12.9	80.0 ± 10.7
VAE - GMM	83.0 ± 3.6	71.4 ± 4.3	85.3 ± 3.0
VAE - Ours	85.4 ± 2.5	74.7 ± 3.5	87.3 ± 2.7
on the validation set the size of which is set as 20% of the train size. See Appendix E for further
details about experiments set-up. Then, we report the evolution of the FID against the test set in
Figure 4. Results obtained on SVHN are presented in Appendix F. Again, the proposed sampling
method appears quite robust to the dataset size since it outperforms the other models’ FID even
when the number of training samples is smaller. This is made possible thanks to the proposed metric
that allows to avoid regions of the latent space having poor information. Finally, our study shows
that although using more complex generation procedures such as ex-post density estimation seems
to still enhance the generation capability of the model when the number of training samples remains
quite high (≥5k), this gain seems to worsen when the dataset size reduces as illustrated on CIFAR.
5.3	Generation with Complex Data
Finally, we also propose to stress the proposed generation procedure in a day-to-day scenario where
the limited data regime is more than common. To stress the model in such condition, we consider the
publicly available OASIS database composed of 416 MRI of patients, 100 of whom were diagnosed
with Alzheimer disease (AD). Since both FID and PRD scores are not relevant and reliable in such
low data regime due to the lack of a large test set, we propose to assess quantitatively the generation
quality with a data augmentation task. Hence, we split the dataset into a train set (70%), a validation
set (10%) and a test set (20%). Each model is trained on each label of the train set and used to
generate 2k new samples per class. Then a simple CNN classifier is trained on i) the original train
set and ii) the 4k generated samples from the generative models and tested on the test set. Table 2
shows the mean balanced accuracy and F1 scores across 20 runs. These metrics provide a good way
to assess i) if the generative model can generate data that are not too far from the test set and add
information to the data that is relevant for classification and ii) allows to quantify the amount of
overfitting. The proposed method is the only one to be able to outperform the original (unbalanced)
data both in terms of balanced accuracy and F1 scores for both labels meaning that generated samples
are relevant to the classifier. This is also the sign of a good generalization power since the classifier
achieves classification results that outperform the one observed on the original data.
6	Conclusion
In this paper, we provided a geometric understanding of the latent space learned by a VAE and
showed that it can actually be seen as a Riemannian manifold. Then, we proposed a new natural
generation process consisting in sampling from the intrinsic uniform distribution defined on this
learned manifold. It showed to be competitive with more advanced versions of the VAEs using either
more complex priors, ex-post density estimation, normalizing flows or other regularization schemes.
Interestingly, the proposed method revealed good robustness properties in complex settings such as
high dimensional data or low sample sizes. Future work would consist in trying to use this method
to perform data augmentation in those challenging contexts and compare its reliability for such a
task with state of the art augmentation methods.
9
Under review as a conference paper at ICLR 2022
Reproducibility S tatement
In order to make the method and the proposed experiments reproducible, we provide in Appendix
E the complete experimental set-up and in Appendix C pseudo-code algorithms detailing the imple-
mentation from a practical point of view. We also provide an implementation in the supplementary
material.
10
Under review as a conference paper at ICLR 2022
References
Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information
bottleneck. arXiv preprint arXiv:1612.00410, 2016.
Jyoti Aneja, Alexander Schwing, Jan Kautz, and Arash Vahdat. NCP-VAE: Variational autoencoders
with noise contrastive priors. arXiv:2010.02917 [cs, stat], 2020.
Georgios Arvanitidis, Lars Kai Hansen, and Soren Hauberg. Latent space oddity: On the curvature
of deep generative models. In 6th International Conference on Learning Representations, ICLR
2018, 2018.
Matthias Bauer and Andriy Mnih. Resampled priors for variational autoencoders. In The 22nd
International Conference on Artificial Intelligence and Statistics, pp. 66-75. PMLR, 2019.
Merlijn Blaauw and Jordi Bonada. Modeling and transforming speech using variational autoen-
coders. Morgan N, editor. Interspeech 2016; 2016 Sep 8-12; San Francisco, CA.[place un-
known]: ISCA; 2016. p. 1770-4., 2016. Publisher: International Speech Communication As-
sociation (ISCA).
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders.
arXiv:1509.00519 [cs, stat], 2016.
Manfredo Perdigao do Carmo. Riemannian Geometry. Birkhauser, 1992.
Anthony L Caterini, Arnaud Doucet, and Dino Sejdinovic. Hamiltonian variational auto-encoder.
In Advances in Neural Information Processing Systems, pp. 8167-8177, 2018.
Clement Chadebec, Clement Mantoux, and Stephanie Allassonniere. Geometry-aware hamiltonian
variational auto-encoder. arXiv:2010.11518, 2020.
Clement Chadebec, Elina Thibeau-Sutre, Ninon Burgos, and Stephanie Allassonniere. Data AUg-
mentation in High Dimensional Low Sample Size Setting Using a Geometry-Based Variational
Autoencoder. arXiv preprint arXiv:2105.00026, 2021.
Nutan Chen, Alexej Klushyn, Richard Kurle, Xueyan Jiang, Justin Bayer, and Patrick Smagt. Met-
rics for deep generative models. In International Conference on Artificial Intelligence and Statis-
tics, pp. 1540-1550. PMLR, 2018.
Xi Chen, Diederik P Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya
Sutskever, and Pieter Abbeel. Variational lossy autoencoder. arXiv preprint arXiv:1611.02731,
2016.
Marissa Connor, Gregory Canal, and Christopher Rozell. Variational autoencoder with learned latent
structure. In International Conference on Artificial Intelligence and Statistics, pp. 2359-2367.
PMLR, 2021.
Chris Cremer, Xuechen Li, and David Duvenaud. Inference suboptimality in variational autoen-
coders. In International Conference on Machine Learning, pp. 1078-1086. PMLR, 2018.
Bin Dai and David Wipf. Diagnosing and Enhancing VAE Models. In International Conference on
Learning Representations, 2018.
Tim R Davidson, Luca Falorsi, Nicola De Cao, Thomas Kipf, and Jakub M Tomczak. Hyperspher-
ical variational auto-encoders. In 34th Conference on Uncertainty in Artificial Intelligence 2018,
UAI 2018, pp. 856-865. Association For Uncertainty in Artificial Intelligence (AUAI), 2018.
Nat Dilokthanakul, Pedro A. M. Mediano, Marta Garnelo, Matthew C. H. Lee, Hugh Salimbeni,
Kai Arulkumaran, and Murray Shanahan. Deep unsupervised clustering with gaussian mixture
variational autoencoders. arXiv:1611.02648 [cs, stat], 2017.
Simon Duane, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth. Hybrid monte carlo.
Physics Letters B, 195(2):216-222, 1987.
11
Under review as a conference paper at ICLR 2022
LUca Falorsi, Pim de Haan, Tim R. Davidson, Nicola De Cao, Maurice Weiler, Patrick Forre, and
Taco S. Cohen. Explorations in homeomorphic variational auto-encoding. arXiv:1807.04689 [cs,
stat], 2018.
Partha Ghosh, Mehdi SM Sajjadi, Antonio Vergari, Michael Black, and Bernhard Scholkopf. From
variational to deterministic autoencoders. In 8th International Conference on Learning Represen-
tations, ICLR 2020, 2020.
Mark Girolami and Ben Calderhead. Riemann manifold langevin and hamiltonian monte carlo
methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73(2):
123-214, 2011.
S0ren Hauberg, Oren Freifeld, and Michael Black. A Geometric take on Met-
ric Learning. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger
(eds.), Advances in Neural Information Processing Systems, volume 25. Curran Asso-
ciates, Inc., 2012. URL https://proceedings.neurips.cc/paper/2012/file/
ec5aa0b7846082a2415f0902f0da88f2-Paper.pdf.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in Neural Information Processing Systems, 2017.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a
constrained variational framework. ICLR, 2(5):6, 2017.
Matthew D Hoffman and Matthew J Johnson. Elbo surgery: yet another way to carve up the varia-
tional evidence lower bound. In Workshop in Advances in Approximate Bayesian Inference, NIPS,
volume 1, pp. 2, 2016.
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction
to variational methods for graphical models. Machine Learning, 37(2):183-233, 1999.
Dimitrios Kalatzis, David Eklund, Georgios Arvanitidis, and Soren Hauberg. Variational autoen-
coders with riemannian brownian motion priors. In International Conference on Machine Learn-
ing, pp. 5053-5066. PMLR, 2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. arXiv:1312.6114 [cs, stat],
2014.
Alexej Klushyn, Nutan Chen, Richard Kurle, and Botond Cseke. Learning Hierarchical Priors in
VAEs. Advances in neural information processing systems, pp. 10, 2019.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Yann LeCun. The MNIST database of handwritten digits. 1998.
Jun S Liu. Monte Carlo strategies in scientific computing. Springer Science & Business Media,
2008.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), December 2015.
Maxime Louis. Computational and statistical methods for trajectory analysis in a Riemannian
geometry setting. PhD Thesis, Sorbonnes universite´s, 2019.
Daniel S. Marcus, Tracy H. Wang, Jamie Parker, John G. Csernansky, John C. Morris, and Randy L.
Buckner. Open access series of imaging studies (OASIS): Cross-sectional MRI data in young,
middle aged, nondemented, and demented older adults. Journal of Cognitive Neuroscience, 19
(9):1498-1507, 2007.
12
Under review as a conference paper at ICLR 2022
Emile Mathieu, Charline Le Lan, Chris J Maddison, Ryota Tomioka, and Yee Whye Teh. Contin-
UoUs hierarchical representations with Poincare variational auto-encoders. In Advances in neural
information processing Systems, pp. 12565-12576, 2019.
Eric Nalisnick, Lars Hertel, and Padhraic Smyth. Approximate inference for deep latent gaUssian
mixtUres. In NIPS Workshop on Bayesian Deep Learning, volUme 2, pp. 131, 2016.
Radford M Neal. Hamiltonian importance sampling. In talk presented at the Banff International
Research Station (BIRS) workshop on Mathematical Issues in Molecular Dynamics, 2005.
Radford M Neal and others. MCMC Using hamiltonian dynamics. Handbook of Markov Chain
Monte Carlo, 2(11):2, 2011.
YUval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo WU, and Andrew Y Ng. Reading
digits in natUral images with UnsUpervised featUre learning. 2011.
Bo Pang, Tian Han, Erik Nijkamp, Song-ChUn ZhU, and Ying Nian WU. Learning latent space
energy-based prior model. Advances in Neural Information Processing Systems, 33, 2020.
Xavier Pennec. Intrinsic statistics on riemannian manifolds: Basic tools for geometric measUre-
ments. Journal of Mathematical Imaging and Vision, 25(1):127-154, 2006. ISSN 0924-9907,
1573-7683. doi: 10.1007/s10851-006-6228-4.
Alexander Rakowski and Christoph Lippert. Disentanglement and local directions of variance. In
Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp.
19-34. Springer, 2021.
Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with
vq-vae-2. Advances in Neural Information Processing Systems, 2020.
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Interna-
tional Conference on Machine Learning, pp. 1530-1538. PMLR, 2015.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and ap-
proximate inference in deep generative models. In International conference on machine learning,
pp. 1278-1286. PMLR, 2014.
Michal Rolinek, Dominik Zietlow, and Georg MartiUs. Variational aUtoencoders pUrsUe pca direc-
tions (by accident). In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 12406-12415, 2019.
MSM Sajjadi, O Bachem, M LUcic, O BoUsqUet, and S Gelly. Assessing generative models via
precision and recall. In 32nd Conference on Neural Information Processing Systems (NeurIPS
2018), pp. 5228-5237, 2019.
Tim Salimans, Diederik Kingma, and Max Welling. Markov chain monte carlo and variational
inference: Bridging the gap. In International Conference on Machine Learning, pp. 1218-1226,
2015.
Hang Shao, Abhishek KUmar, and P. Thomas Fletcher. The riemannian geometry of deep generative
models. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops
(CVPRW), pp. 428-4288. IEEE, 2018. ISBN 978-1-5386-6100-0. doi: 10.1109/CVPRW.2018.
00071.
Casper Kaae S0nderby, Tapani Raiko, Lars Maal0e, S0ren Kaae S0nderby, and Ole Winther. Ladder
variational aUtoencoder. In 29th Annual Conference on Neural Information Processing Systems
(NIPS 2016), 2016.
I Tolstikhin, O Bousquet, S Gelly, and B Scholkopf. Wasserstein auto-encoders. In 6th International
Conference on Learning Representations (ICLR 2018), 2018.
Jakub Tomczak and Max Welling. Vae with a vampprior. In International Conference on Artificial
Intelligence and Statistics, pp. 1214-1223. PMLR, 2018.
13
Under review as a conference paper at ICLR 2022
Zhisheng Xiao, Karsten Kreis, Jan Kautz, and Arash Vahdat. Vaebm: A symbiosis between varia-
tional autoencoders and energy-based models. In International Conference on Learning Repre-
sentations, 2020.
Linxiao Yang, Ngai-Man Cheung, Jiaying Li, and Jun Fang. Deep clustering by gaussian mixture
variational autoencoders with graph embedding. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pp. 6440-6449, 2019.
Cheng Zhang, Judith Butepage, Hedvig Kjellstrom, and StePhan Mandt. Advances in variational
inference. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(8):2008-2026,
2018.
14
Under review as a conference paper at ICLR 2022
A Further Elements on Riemannian Geometry
In the field of differential geometry, a Riemannian manifold M can be defined as a connected
and differentiable manifold endowed with a Riemannian metric g . The metric g is a smooth inner
product g : P → h∙∣∙ip defined on each tangent space TpM of the manifold with P ∈ M. A chart (or
coordinate system) (U, x) is a homeomorphism mapping an open set U of the manifold to an open
set V ofan Euclidean space. The manifold M is further called a d-dimensional manifold if for each
chart V ⊂ Rd . This means that there exists a neighborhood U of each point P ∈ M such that U is
homeomorphic to Rd. Given P ∈ U, a chart φ : (x1, . . . , xd) induces a basis
∂
∂
on the
p
∂x1 , . . ., ∂xd
tangent space TpM. Hence, the metric of a Riemannian manifold can be locally represented in the
chart φ as a positive definite matrix G(P) = (gi,j)p,0≤i,j≤d = (h ∂fi 1 ∂Xj ip)0≤i,j≤d for each point P
of the manifold. That is for v, w ∈ TpM andP ∈ M, the inner product writes hu|wip = u>G(P)w.
Two ways of apprehending manifolds exist. The first one is the extrinsic view and it assumes that
the manifold is embedded within a higher dimensional Euclidean space. A simple example is the
2-dimensional sphere S2 seen as a subspace of R3 . The second one which is adopted in this paper
is the intrinsic view. In the intrinsic view the manifold is studied using its underlying structure and
so the length of a curve γ : R → M traveling in the manifold cannot be interpreted using the
Euclidean distance but requires to use the metric defined onto the manifold itself. Let z1, z2 ∈ M
be two points of the manifold, and γ be a curve traveling in M parametrized by t ∈ [0, 1] such that
γ(0) = z1 and γ(1) = z2. Then, the length ofγ is given by
11
L(Y) = / kγ(t)kγ(t)dt = hY(t)|Y(t)iY(t)dt
00
Curves Y that are minimizer(s) of this criteria are called geodesic curves. A distance dist on the
manifold M can then be derived and writes:
dist(z1,z2) = minL(γ)	s.t. γ(0) = z1, γ(1) = z2
γ
(9)
The manifold M is said to be geodesically complete if all geodesic curves can be extended to R.
For any P ∈ M, the exponential map at P maps a vector v of the tangent space TpM to a point of
the manifold P ∈ M such that the geodesic starting at P with initial velocity V reached P at time 1.
Exp : ( TpM → M
pp	v → Expp(v) = γ(p,v) (1) ,
where γ(p,v)⑴ means Y(0) = P and γ(0) = v. Saying that the manifold M is geodeSically
complete means that the exponential maps is defined on the entire tangent space TpM for each
element P ∈ M.
In Pennec (2006), the author discusses the statistical framework that can be developed on geodesi-
cally complete manifolds using an intrinsic point of view. In particular, given a positively oriented
Riemannian manifold M and a chart φ = (xi,…,xd), a volume form dVolg can be defined as the
d-form:
dVolg = dddet(gi,j) dx1 ∧ ∙∙∙∧ dxd
This represents an infinitesimal volume element on each tangent space and so a measure on the
manifold M	_______________
dM(φ-1(x)) = ʌ/detg(φ-1(x)) dx
It follows that we are able to integrate functions f : M ⊃ U → R on a given chart (U, φ).
f f(P)dM(P)= J f (φ-1(x)) pdet g(φ-1(x)) dx1 …dxd
U	φ(U)
This notion can then be extended to the whole manifold M using partition of unity. In particular,
such a property allows us to define probability distributions whose density is defined with respect to
the measure on the manifold. We recall such definition from Pennec (2006) below
15
Under review as a conference paper at ICLR 2022
Definition 1 Let B(M) be the Borel σ-algebra ofM. The random point p has a probability density
function ρp if:
∀X ∈ B(M), P(p ∈ X) = ρ(p)dM(p)
X
and	ρ(p)dM(p) = 1
M
Finally, given a chart φ = (zι,…，zn) defined on the whole manifold M and a random point P
on M, the point z = φ(p) is a random point whose density ρ0z may be written with respect to the
Lebesgue measure as such (Pennec, 2006):
PZ(Z) = PP(OT(Z))VZdet g(。-1(Z))
(10)
16
Under review as a conference paper at ICLR 2022
B Proof of Prop. 1
We adapt the proof in Louis (2019) and Chadebec et al. (2021) to our specific metric. We will show
that given the manifold M = Rd and the Riemannian metric whose local representation is given by
Eq. 7, any geodesic curve γ :]a, b[→ M is actually extensible to R. Let us consider a geodesic curve
γ such that γ cannot be extended to R. There exists I =]a, b[ such that I is the maximum definition
domain of γ. We will show that with such an assumption we will end up with a contradiction. We
recall the shape of the Riemannian metric:
N
G(Z) = X Σ-1(xi) ∙ ωi(z) + λ ∙ Id ,
i=1
Since Σi are positive definite matrices we have z> Σiz > 0, ∀ z ∈ M - {0}. We further have
ωi(z) > 0, ∀ z ∈ M since the manifold is geodesically complete. Let t0 ∈]a, b[ we therefore have
for any t ∈]a, b[.
N
λ ∙ kγ(t)k2 ≤ λ ∙kγ(t)k2 + Xγ(t)>∑(Xi)-1γ(t) ∙ 3H
i=1
≤kγ(t)kY(t) = kγ(to)kY(to),
where the last equality comes for the constant speed of geodesic curves (Carmo, 1992). Hence we
have:
kγ(t) - γ(t0)k2 ≤ kγ("t0) ∙∣t- to|.
λ
This means that for any t ∈]a, b[ the geodesic curve γ remains within a compact set. We show now
that the curve can actually be extended. Let us define the sequence tn -→ b. Since the geodesic
n→∞
curves have a constant speed the set I = {(tn, Y(tn)}n∈N is compact. Moreover, using CaUchy-
Lipchitz theorem, we can find ε > 0 such that for any n ∈ N, the geodesic γ can be extended to
]tn - ε, tn + ε[. Since, tn can be as close as b as desired we can assure that the curve definition
domain can be extended to ]a,b + ε [.
17
Under review as a conference paper at ICLR 2022
C The Generation Process Algorithm - Implementation Details
In this appendix, we provide pseudo-code algorithms explaining how to build the metric from a
trained VAE and how to use the proposed sampling process. Noteworthy is the fact that we do not
amend the training process of the vanilla VAE which remains pretty simple and stable.
C.1 Building the Metric
In this section, we explain how to build the proposed Riemannian metric. For the sake of clarity, we
recall the expression of the metric in Eq. 7 below
N
G(Z) = X Σ-1(xi) ∙ ωi(z) + λ ∙ Id ,
i=1
where
ωi(z) = exp ( - distxT(x2)(Z，μi)2 ! = eχp ( — (Z-",尸三—：(Xi)(Z - μ^),
Algorithm 1 Building the Metric from a Trained Model
Input: A trained VAE model m, the training dataset X, λ
for xi ∈ X do
μi, ∑i = m(xi)	. Retrieve training embeddings and covariance matrices
end for
Select k centroids Ci in the μ%	. e.g. with k-medoids
Get corresponding covariance matrices Σi
P J max min ∣∣Ci - Cjk 2	. Set P to the max distance between two closest neighbors
i	j6=i
Build the metric using Eq. 7
N
G(Z) = X Σ-1 ∙ ωi(z) + λ ∙ Id
i=1
Return G
. Return G as a function
As is standard in VAE implementations, we assume that the covariance matrices Σi given by the
VAE are diagonal and that the encoder outputs a mean vector and the log of the diagonal coefficients.
In the implementation, the exponential is then applied to recover the Σi so that no singular matrix
arises.
C.2 Sampling Process
Further to the description performed in the paper, we provide here a detailed algorithm stating the
main steps of the generation process.
C.2.1 The HMC Sampler
In the sampling process we propose to rely on the Hamiltonian Monte Carlo sampler to sample from
the Riemanian uniform distribution. In a nutshell, the HMC sampler aims at sampling from a target
distribution ptarget (Z) with Z ∈ Rd using Hamiltonian dynamics. The main idea behind such a
sampler is to introduce an auxiliary random variable V 〜N(0, Id) independent from Z and mimic
the behavior of a particle having Z (resp. v) as location (resp. velocity). The Hamiltonian of the
particle then writes
H(Z, v) = U(Z) + K(v) ,
where U(Z) is the potential energy of such a particle and K(v) is its kinetic energy both given by
U (z) = - log Ptarget(Z),	K (v) = 1 V> V
18
Under review as a conference paper at ICLR 2022
The following Hamilton’s equations govern the evolution in time of the particle.
∂ ∂H(z,v)
∂ CdV 、
1 ∂H(z,v)
I -^∂z-
v,
-Vz logPtarget(Z).
(11)
In order to integrate these equations, recourse to the leapfrog integrator is needed and consists in
applying nlf times the following equations.
v(t + εf )	= v(t)	+	εf	∙ Vz	logPtarget (Xt)),
z(t + εif)	= z(t)	+	εif	∙ v(t	+ εf) ,	(12)
v(t +	εlf )	= v(t	+	εf )	+	εf	∙	V z	log Ptarget(Z(t	+	εlf)) ,
where εlf is called the leapfrog step size. This algorithm produces a proposal (ze, ve) that is accepted
with probability α where
α = min 1, exp H(Z, v) - H (Ze, ve) .
This procedure is then repeated to create an ergodic Markov chain (Zn) converging to the distribution
Ptarget (Neal & others, 2011; Duane et al., 1987; Liu, 2008; Girolami & Calderhead, 2011).
C.3 The Proposed Algorithm
In our setting the target density is given by the density of the Riemannian uniform distribution which
writes with respect to Lebesgue measure as follows
p(z) = URiem(Z) = J Pdet G(Z) .	(13)
The log density follows
log p(z) = 21ogdet G(Z)- log C,
In such a case, the Hamiltonian writes
H(z, v) = — logP(z) + 2v>v ,
and Hamilton’s equations become
∂ ∂H(z,v)
∂	∂v
∂ ∂H(z,v)
I	∂zi
v,
∂	log p(z)
∂ Zi
-2tr
Since the covariance matrices are supposed to be diagonal as is standard in VAE implementations,
the computation of the inverse metric is straightforward. Moreover, since G(Z) is smooth and has
a closed form, it can be differentiated with respect to Z pretty easily. Now, the leapfrog integrator
given in Eq. 12 can be used and the acceptance ratio α is easy to compute. Noteworthy is the fact
that the normalizing constant C is never needed since it vanishes in the gradient computation and
simplifies in the acceptance ratio α. We provide a pseudo-code of the proposed sampling procedure
in Alg. 2. A typical choice in the sampler’s hyper-parameters used in the paper is N = 100, nlf = 10
and εlf = 0.01. The initialization of the chain can be done either randomly or on points that belong
to the manifold (i.e. the centroids Ci or μi).
19
Under review as a conference paper at ICLR 2022
Algorithm 2 Proposed Sampling Process
Input: The metric function G, hyper-parameters of the HMC sampler (chain length N, number
of leapfrog steps nlf , leapfrog step size εlf)
Initialization: z	. Initialize the chain
for i = 1 → N do
V 〜N(0, Id)	. Draw a velocity
Ho J H(z, V)	. Compute the starting Hamiltonian
zo J Z
for k = 1 J nlf do
V J v — εf NzH(z, v)
z J z + εif ∙ V	. Leapfrog step Eq. 12
e J V 一 ε2f ∙ VzH(e, V)
VJV
zJz
end for
H J H(ze, Ve)	. Compute the ending Hamiltonian
Accept ze with probability α = min 1, exp(H0 - H)
if Accepted then
zJz
else
z J z0
end if
end for
Return z
20
Under review as a conference paper at ICLR 2022
D Other Generation
D.1 Some Further Samples on CELEBA and MNIST
In this section, we provide some further generated samples using the proposed method. Figure 5 and
Figure 6 again support the fact that the method is able to generate sharp and diverse samples. We
also add the other variants of the RAE model in Figure 7.
Qs 7夕|£6511
(23Gq 7
O 4 ∕∙t7t-316s
Jd / 4543 J / 4
421，7SQ 7 夕/
1 130S7/G2 1-
夕V?夕/7/49久
/ or C ʃ I 0 5497
77N —，/33，g
2qzz*“fc∙30√
Figure 5: 100 samples with the proposed method on MNIST dataset.
Figure 6: 100 samples with the proposed method on Celeba dataset.
21
Under review as a conference paper at ICLR 2022
MNIST	CELEBA
AE - N
VAE - N
WAE
VAMP
HVAE
RHVAE
AE - GMM
VAE - GMM
RAE (GP)
RAE (L2)
RAE (SN)
RAE
VAE - Ours
Figure 7:	Generated samples with different models and generation processes.
夕。∕57q 夕 7 Mv
Ic-'c0 4c6H37QOQtκ
AJg4夕OqGeeQ /ə孑
3 3O-G9Z6S 971 夕力
7。Rq3/6。/CΓ6 平
73 Q 3。2ON‹27?/
0G∕74√7∕e5 7G /
22
Under review as a conference paper at ICLR 2022
D.2 CIFAR AND SVHN
In this appendix, we gather the resulting samplings from the different considered models for SVHN
and CIFAR 10.
AE - N
VAE - N
WAE
VAMP
HVAE
RHVAE
AE - GMM
VAE - GMM
RAE (GP)
RAE (L2)
RAE (SN)
RAE
VAE - Ours
SVHN	CIFAR 10
Figure 8:	Generated samples with different models and generation processes.
Figure 9: Closest element in the training set (Near.) to the generated one (Gen.) with the proposed
method.
23
Under review as a conference paper at ICLR 2022
E Experimental S et-up
The RAEs, VAEs and AEs are trained for 100 epochs for SVHN, MNIST2 and Celeba and 200 on
CIFAR10. Each time we use the official train and test split of the data. For MNIST and SVHN,
10k samples out of the train set are reserved for validation and 40k for CIFAR10. As to Celeba, we
use the official validation set for validation. The model that is kept at the end of training is the one
achieving the best validation loss. All the models are trained with a batch size of 100 and starting
learning rate of 1e-3 (but CIFAR where the learning rate is set to 5e-4) with an Adam optimizer
(Kingma & Ba, 2014). We also use a scheduler decreasing the learning rate by half if the validation
loss stops increasing for 5 epochs. For the experiments on the sensitivity to the training set size, we
keep the same set-up. For each dataset we ensure that the validation set is 1/5th the size of the train
set but for CIFAR where we select the best model on the train set. The neural networks architectures
can be found in Table 3 and are inspired by Ghosh et al. (2020). The metrics (FID and PRD scores)
are computed with 10000 samples against the test set (for Celeba we selected only the 10000 first
samples of the official test set). The factor ρ is set to ρ = max min kci - cj k2 to ensure some
i j6=i
smoothness of the manifold. For models coming from peers, we use the parameters provided by the
authors when available.
For the data augmentation task, the generative models are trained on each class for 1000 epochs with
a batch size of 100 and a starting learning rate of 1e-4. Again a scheduler is used and the learning
rate is cut by half if the loss does not improve for 20 epochs. All the models have the autoencoding
architecture described in Table 3. As to the classifier, it is trained with a batch size of 200 for 50
epochs with a starting learning rate of 1e-4 and Adam optimizer. A scheduler reducing the learning
rate by half every 5 epochs if the validation loss does not improve is again used. The best kept model
is the on achieving the best balanced accuracy on the validation set. Its neural network architecture
may be found in Table 4. MRIs are only pre-processed such that the maximum value of a voxel is 1
and the minimum 0 for each data point.
Table 3: Neural networks used for the encoder and decoders of VAEs in the benchmarks
	MNIST [CIFAR10]	SVHN	CELEBA	OASIS
Encoder	(1[3], 32, 32)	(3, 32, 32)	(3,64,64)	=	(1, 208, 176)
Layer 1	Conv(128, (4, 4), stride=2) Batch normalization Relu	Linear(1000) Relu	CONV(128, (5, 5), STRIDE=2) Batch normalization Relu	Conv(64, (5, 5), stride=2) Relu
Layer 2	Conv(256, (4, 4), stride=2) Batch normalization Relu	Linear(500) Relu	-Conv(256, (5, 5), STRIDE=2)- Batch normalizati on Relu	Conv( 128, (5, 5), stride=2) Relu
Layer 3	Conv(512, (4, 4), stride=2) Batch normalization Relu	LINEAR(500, 16*)	-Conv(512, (5, 5), STRIDE=2)- Batch normalizati on Relu	Conv(256, (5, 5), stride=2) Relu
Layer 4	Conv(1024, (4, 4), stride=2) Batch normalization Relu	-	-CONV(1024, (5, 5), STRIDE=2)- Batch normalizati on Relu	Conv(512, (5, 5), stride=2) Relu
Layer 5	LINEAR(4096, 16*)	-	LINEAR(16384, 64*)	Conv(1024, (5, 5), stride=2) Relu
Layer 6	-	-	-	LINEAR(4096, 16*)
Decoder	(16 [32])	(16)	(64)	=	(16)
Layer 1	Linear(65536) RESHAPE(1024, 8, 8)	Linear(500) Relu	Linear(65536) RESHAPE(1024, 8, 8)	Linear(65536) RESHAPE(1 024, 8, 8)
Layer 2	ConvT(512, (4, 4), stride=2) Batch normalization Relu	Linear ( 1000) Relu	CONVT(512,(5, 5), STRIDE=2) Batch normalizati on Relu	CONVT(51 2, (5, 5), STRIDE=(3, 2)) Relu
Layer 3	ConvT(256, (4, 4), stride=2) Batch normalization Relu	Linear(3072) RESHAPE(3, 32, 32) Sigmoid	CONVT(256,(5, 5), STRIDE=2) Batch normalizati on Relu	ConvT(256, (5, 5), stride=2) Relu
Layer 4	ConvT(3, (4, 4), stride= 1) Batch normalization Sigmoid	-	CONVT(128,(5, 5), STRIDE=2) Batch normalizati on Relu	ConvT(1 28, (5, 5), stride=2) Relu
Layer 5	-	-	CONVT(3, (5, 5), STRIDE=1) Batch normalizati on SIGMOID	ConvT(64, (5, 5), stride=2) Relu
Layer 6	-	-	-	ConvT(1, (5, 5), stride=1) Relu
2MNIST images are re-scaled to 32x32 images with a 0 padding.
24
Under review as a conference paper at ICLR 2022
Table 4: Neural Network used for the classifier in Sec. 5.3
	OASIS Classifier
Input Shape	(1, 208,176)
Laye r 1	~CONV(8, (3, 3), STRIDE=1)- Batch normalization LeakyRelu MAXPOOL(2, stride=2)
Laye r 2	CONV(16, (3, 3), STRIDE=1) Batch normalization LeakyRelu MaxPool(2, stride=2)
Laye r 3	Conv(32, (3, 3), STRIDE=2) Batch normalization LeakyRelu MaxPool(2, stride=2)
Laye r 4	Conv(64, (3, 3), STRIDE=2) Batch normalization LeakyRelu MaxPool(2, stride=2)
Laye r 5	LINEAR(256,100) Relu
Laye r 6	LINEAR(100, 2) S oftmax
25
Under review as a conference paper at ICLR 2022
F Dataset Size Sensibility on SVHN
In Figure 10, we show the same plot for SVHN as in Sec. 5.2. Again the proposed method appears
to be part of the most robust generation procedures to dataset size changes.
Figure 10: FID score evolution according to the number of training samples.
26
Under review as a conference paper at ICLR 2022
G Ablation Study
G.1 Influence of the Number of Centroids in the Metric
In order to assess the influence of the number of centroids and their choice in the metric in Eq. 7,
we show in Figure 11 the evolution of the FID according to the number of centroids in the metric
(left) and the variation of FID according to the choice in the centroids. As expected, choosing a
small number of centroids will increase the value of the FID since it reduces the variability of the
generated samples that will remain close to the centroids. Nonetheless, as soon as the number of
centroids is higher than 1000 the FID score is either competitive or better than peers and continues
decreasing as the number of centroids increases.
1
Number of centroids in the metric
3
Centroids choice
mnist
celeba
2
4

5
Figure 11: Left: FID score evolution according to the number of centroids in the metric (Eq. 7).
Right: The FID variation with respect to the choice in centroids. We generate 10000 samples by
selecting each time different centroids (k = 1000).
To assess the variability of the generated samples, we propose to analyze some generated samples
when only 2 centroids are considered. In Figure 12, we display on the left the decoded centroids
along with the closest image to these decoded centroids in the train set. On the right are presented
some generated samples. We place these samples in the top row if they are closer to the first decoded
centroid and in the bottom row otherwise. Interestingly, even with a small number of centroids the
proposed sampling scheme is able to access to a relatively good diversity of samples. These samples
are not simply resampled train images or a simple interpolation between selected centroids as some
of the generated samples have attributes such as glasses that are not present in the images of the
decoded centroids.
Decoded centroid Nearest train image	Generated samples
Figure 12: Variability of the generated samples when only two centroids are considered in the metric.
Left: The image obtained by decoding the centroids. Middle: The nearest image in the train set to
the decoded centroids. Right: Some generated samples. Each generated sample is assigned to the
closest decoded centroid (top row for the first centroid and bottom row for the second one).
27
Under review as a conference paper at ICLR 2022
G.2 INFLUENCE OF λ IN THE METRIC
In this section, we also assess the influence of the regularization factor λ in Eq. 7 on the resulting
sampling. To do so, we generate 10k samples using the proposed method on both MNIST and
Celeba datasets for values of λ ∈ [1e-6, 1e-4, 1e-2, 1e-1, 1]. Then, we compute the FID against
the test set. Each time, we consider k = 1000 centroids in the metric. As shown in Figure 13, the
influence of λ remains limited. In practice, λ is mainly here to avoid pathological cases such as the
metric collapsing to 0 far from the centroids. In the implementation, a typical choice for λ is 1e-2.
O
K
80
70
60
50
40
30
20
10
10-6	10-5	10-4	10-3	10-2	10—1.....100
Value of λ
Figure 13: FID score evolution according to the value of λ in the metric (Eq.7).
G.3 THE CHOICE OFρ
In the experiments presented, the smoothing factor ρ in Eq. 7 is set to the value of the maximum
distance between two closest centroids ρ = max min kcj - cik2. This choice is motivated by the
i j 6=i
fact that we wanted to build a smooth metric and so ensure some smoothness of the manifold while
trying to interpolate faithfully between the metric tensors Gi = Σi-1. In particular, a small value
of ρ would have allowed disconnected regions and the sampling may have not prospected well the
learned manifold and would have only become a resampling of the centroids. On the other hand,
setting a high value for P would have biased the interpolation and the value of the metric at a μ%.
As a result, G(μi) might have been very different from the one observed Σ-1 since the other μj
would have had a strong influence on its value. The proposed value for P appeared to work well in
practice.
28
Under review as a conference paper at ICLR 2022
H Other Classification Metrics
In Table 5 are presented some further classification results for each considered model while gener-
ated samples using each generation procedure are made available in Figure 14. On OASIS database
the proposed method appears to produce visually the sharpest samples. This better generation per-
formance is also supported by the classification metrics provided in Table 2 and Table 5.
Table 5: Classification results averaged on 20 independent runs. For the VAEs, the classifier is
trained on 2K generated samples per class.
Generation method	Balanced Accuracy	Prpri Qi∩n		Rprall	
		AD	CN	AD	CN
Original*	66.2 ± 7.6	74.7 ± 8.4	80.3 ± 4.0	35.7 ± 16.3	95.7 ± 1.5
Original (resampled)	81.8 ± 2.6	67.0 ± 5.3	91.4 ± 1.8	78.5 ± 5.2	85.1 ± 4.2
AE - N	50.0 ± 0.0	^^0.0 ± 0.0	72.6 ± 0.0	0.0 ± 0.0	100.0 ± 0.0
WAE	57.4 ± 9.7	48.5± 42.8	76.7 ± 6.1	19.3 ± 27.5	95.4 ± 9.3
VAE - N	51.8 ± 3.8	38.0 ± 47.3	73.4 ± 1.7	3.7 ± 7.8	99.8 ± 0.7
VAMP	83.1 ± 2.6	56.3 ± 5.2	97.5 ± 2.1	94.8 ± 4.7	71.5 ± 7.4
HVAE	56.3 ± 7.9	48.7 ± 41.7	75.5 ± 3.8	13.9 ± 17.6	98.6 ±2.2
RHVAE	68.0 ± 10.9	56.1 ± 25.3	83.0 ± 7.5	46.7 ± 30.2	89.2 ± 10.6
AE - GMM	82.4 ± 2.3	55.8 ± 4.9	96.8 ± 2.4	93.3 ± 5.6	71.5 ± 6.2
RAE (GP)	63.9 ± 9.8	45.3 ± 18.5	84.2 ± 8.6	60.9 ± 28.6	67.0 ± 24.9
RAE (L2)	74.1 ± 6.0	57.8 ± 10.1	88.3 ± 5.2	70.0 ± 18.7	78.3 ± 11.7
RAE (SN)	62.3 ± 8.9	43.1 ± 24.9	80.6 ± 6.6	41.7 ± 30.1	82.9 ± 16.4
RAE	69.3 ± 8.1	56.2 ± 13.5	85.2 ± 6.2	60.0 ± 24.0	78.5 ± 17.5
VAE - GMM	83.0 ± 3.6	60.7 ± 5.4	94.9 ± 3.7	88.0 ± 9.5	77.9 ± 5.9
VAE - Ours	85.4 ± 2.5	64.0 ± 5.3	95.8 ± 2.2	90.4 ± 5.6	80.3 ± 5.1
*unbalanced
29
Under review as a conference paper at ICLR 2022
30
Under review as a conference paper at ICLR 2022
I Link B etween the Riemannian VAE and Vanilla VAE
We assume as in (Ghosh et al., 2020) that a VAE is essentially an autoencoder regularized with
noise. Hence, a Riemannian-based VAE could also be seen as a regularized autoencoder but the
noise would be informed by the intrinsic geometry of the latent space. Indeed, the main assumption
behind the Riemannian VAE would consist in assuming that given a set of data x ∈ X ⊂ RD
there exists a lower dimensional space, namely the latent space, that has apparently no reason to be
Euclidean in which live the latent variables. In such a framework, it would be assumed that this space
is the d-dimensional Riemannian manifold M = (Rd, G) where G is an unknown Riemannian
metric. Now the goal of the Riemannian VAE would be the same as a regularized autoencoder that
is to learn a smooth representation of the data within a much lower dimensional space here seen as
the Riemannian manifold M.
To to so and similarly to autoencoder models, it would be assumed that there exist eφ : RD → M
a parametrized encoding function mapping the input data onto the manifold eφ(x) = μ ∈ M and
dθ : M → RD a parametrized decoding function that maps back the latent codes to the data space.
In such a case, the main objective would be to find φ and θ such that the reconstruction loss is
minimized
min LREC = minl(x,dθ(eφ(x)),	x ∈ X ,	(14)
where l is a function measuring the distance between the input data and the reconstructions and is
chosen depending on the problem and the data (e.g. mean square error, binary cross entropy...). In
order to learn a smooth latent space meaning that small variations in the latent space do not change
completely the output of the decoder, the decoder is also regularized using a Riemannian Gaussian
noise. That is imposing that
dφ(z) ≈ x,	Z ~NGIm(z∣μ,σ),
where
NM(Zk μ) = CIexp (-distG^), C = ZexP (-dist⅛^ MMz ,	(15)
M
and the reconstruction loss in Eq. 14 would become
minLREC = minl(x,dθ(z)),	X ∈ X, Z ~ NGem(Z∣eφ(x),σ),
φ,θ	φ,θ
Hence, We no longer decode the embedding μ but rather Z that is obtained with the Riemannian
Gaussian distribution centered on μ. Since the metric G is unknown, a Riemannian VAE would aim
at learning the metric directly from the data. Thus, the encoder function would output an embedding
μ of an input data point but also the value of the Riemannian metric at the embedding point i.e.
G(μ). Since, we would only have access to a finite number of metric tensors, a smooth metric G
could be built using Eq. 7. Now, at least theoretically, we would be able to compute the geodesic
distance involved in Eq. 15. As of now, the manifold is not regularized and so pathological cases such
as the metric collapsing to 0 may arise. To avoid such a behavior, some smoothness conditions could
be applied on the manifold by imposing for instance that the Riemannian Gaussian distribution is
not too far from a standard Gaussian. This would prevent the metric from collapsing to 0 and ensure
that the latent codes remain close to the origin as well. However, other regularization schemes could
have be envisioned as well. Since we would be working in an ambient-like manifold, there exists a
global chart z and so the density of the Riemannian Gaussian distribution can be written with respect
to the Lebesgue measure dZ in Rd (Pennec, 2006)
NGem(Zk μ) = ^ exp ( - distG(z, ">2) PdetG(Z)
C	2σ
d (	2	distG(z, μ)2λ /a . E 「
C = exp (------------------d ʌ/det G(Z) dz ,
Rd
(16)
Hence, the regularization term set as the KL divergence between the Riemannian Gaussian distribu-
tion and the standard normal would follow
LREG = DKL(NGem(ZE 4州N(O, Id))
/
Rd
PNrGem(σ,μ) (log(PNGL(σ,μ)) - Iog(PN(3Id))) d .
31
Under review as a conference paper at ICLR 2022
The final objective a Riemannian VAE would try to minimize would then write
L = LREC + LREG .
Unfortunately, this framework cannot be used in practice for at least two reasons. The first one is
the sampling from the Riemaniann distribution in Eq. 15 which is far from being trivial. MCMC
methods could have been envisioned to sample from such a distribution but this would have impeded
backpropagation since this framework would not be amenable to the reparametrization trick (Sali-
mans et al., 2015; Caterini et al., 2018). Second, the regularization term would involve computing
the density of the Riemannian Gaussian distribution which explicitly involves the computation of
the Riemannian distance and so the resolution of the optimization problem in Eq. 9. Since, this
framework is not usable in practice, it can be assumed that the value of the metric during training
can be approximated by its value at μ (i.e. G(μ)). With this approximation, Riemannian Gaussians
become multivariate Gaussians and all the terms become computable. We find back the vanilla VAE
framework if We further consider G(μ) = Σ(x)-1 where Σ(x) is the covariance matrix given by
the encoder of a vanilla VAE. We indeed have
NsmI (ZE μ) = -1 exp (- dists； (z, μ) ) √det ς-1
C	2σ
^ff	dist∑-ι (z, μ)2 ∖ f-T	(17)
C = exp (----------ς 2； , μ d √det Σ-1 dz ,
Rd
where dist∑-ι (z, μ) =，(z - μ)>Σ-1(z - μ). If we further set σ = 1 we have
NiemI(Z k μ)
exp (- (Zw)Tς2T(Zw))
R exp ( - (""U"")) dz
Rd
(18)
Assuming as is standard in the VAE framework, that Σ is diagonal makes the computation of LREG
easy and we retrieve the training ofa vanilla VAE model. Indeed, as explained in Remark. 1, the log
of the conditional distribution pθ reduces to the reconstruction loss LREC and the KL between the
variational posterior qφ(z∣χ) and the prior taken as a standard Gaussian gives Lreg.
32