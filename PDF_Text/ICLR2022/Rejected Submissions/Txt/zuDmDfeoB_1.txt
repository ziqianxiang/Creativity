Under review as a conference paper at ICLR 2022
How Does the Task Landscape Affect MAML
Performance?
Anonymous authors
Paper under double-blind review
Ab stract
Model-Agnostic Meta-Learning (MAML) has become increasingly popular for
training models that can quickly adapt to new tasks via one or few stochastic
gradient descent steps. However, the MAML objective is significantly more dif-
ficult to optimize compared to standard non-adaptive learning (NAL), and little
is understood about how much MAML improves over NAL in terms of the fast
adaptability of their solutions in various scenarios. We analytically address this
issue in a linear regression setting consisting of a mixture of easy and hard tasks,
where hardness is related to the rate that gradient descent converges on the task.
Specifically, we prove that in order for MAML to achieve substantial gain over
NAL, (i) there must be some discrepancy in hardness among the tasks, and (ii)
the optimal solutions of the hard tasks must be closely packed with the center far
from the center of the easy tasks optimal solutions. We also give numerical and
analytical results suggesting that these insights apply to two-layer neural networks.
Finally, we provide few-shot image classification experiments that support our
insights for when MAML should be used and emphasize the importance of training
MAML on hard tasks in practice.
1	Introduction
Large-scale learning models have achieved remarkable successes in domains such as computer vision
and reinforcement learning. However, their high performance has come at the cost of requiring huge
amounts of data and computational resources for training, meaning they cannot be trained from
scratch every time they are deployed to solve a new task. Instead, they typically must undergo a
single pre-training procedure, then be fine-tuned to solve new tasks in the wild.
Meta-learning aims to address this problem by extracting an inductive bias from a large set of
pre-training, or meta-training, tasks that can be used to improve test-time adaptation. Model-agnostic
meta-learning (MAML) (Finn et al., 2017), one of the most popular meta-learning frameworks,
formulates this inductive bias as an initial model for a few gradient-based fine-tuning steps. Given
that for the model can be fine-tuned on any meta-test task before evaluating its performance, MAML
aims to learn the best initialization for fine-tuning among a set of meta-training tasks. To do so, it
executes an episodic meta-training procedure in which the current initialization is adapted to specific
tasks in an inner loop, then the initialization is updated in an outer loop. This procedure has led to
impressive few-shot learning performance in many settings (Finn et al., 2017; Antoniou et al., 2018).
However, in some settings MAML’s inner loop adaptation and the second derivative calculations
resulting thereof may not justify their added computational cost compared to traditional, no-inner-loop
pre-training. Multiple works have suggested that MAML’s few-shot learning performance may not
drop when executing the inner loop adaptation for only a fraction of the model parameters (Raghu
et al., 2019; Oh et al., 2020) or, in some cases, by ignoring the inner loop altogether (Bai et al.,
2020; Chen et al., 2019). Unfortunately, the underlying reasons for MAML’s behavior are still not
well-understood, making it difficult to determine when inner loop adaptation during meta-training
yields meaningful benefit.
In this work, we investigate when and why MAML’s inner loop updates provide significant gain
for meta-test time adaptation. To achieve this, we focus on how the meta-training loss landscape
induced by the MAML objective affects adaptation performance compared to the loss landscape
induced by the classical objective without inner loop adaptation, which we term the Non-Adaptive
1
Under review as a conference paper at ICLR 2022
(a)
(b)
(I), not (II): PHPE ≪1, Small R, rh
H	Hard task solutions
・	EaSy task solutions
wM wMAML
★ WNAL
Excess Risk
(c)	(d)
Figure 1: MAML, NAL excess risks and optimal solutions in various environments.
Learning (NAL) method. Notably, MAML should always perform at least as well as NAL because its
meta-training procedure aligns with the meta-test time evaluation of performance after a few steps of
task-specific SGD. Thus, our goals are to quantify the gain provided by MAML and determine the
scenarios in which this gain is most significant. We start by studying the multi-task linear regression
setting since it allows us to compare the exact solutions and excess risks for MAML and NAL.
In doing so, we obtain novel insights on how MAML deals with hard tasks differently than NAL,
and show that MAML achieves significant gain over NAL only if certain conditions are satisfied
relating to the hardness and geography of the tasks. These observations provide a new understanding
of MAML that is distinct from the representation learning perspective considered in recent works
(Raghu et al., 2019; Du et al., 2020; Tripuraneni et al., 2020). We then give theoretical and empirical
evidence generalizing these insights to neural networks.
In particular, our main observations are best captured in the setting in which tasks are either “hard” or
“easy”. We let ρH be the hardness parameter for the hard tasks, and ρE be the hardness parameter
for the easy tasks, where ρE > ρH (smaller hardness parameter means more hard). As we measure
the hardness of a task by the rate at which gradient descent converges for the task, in this case,
the hardness parameter is the task loss function’s strong convexity parameter. For a particular task
environment, we let R be the dimension-normalized distance between the average of easy tasks’
optimal solutions and the average of hard tasks’ optimal solutions, and let rH quantify the variance,
or dispersion, of the hard tasks' optimal solutions. Assuming that 黑(1 一 黑)2》-m, where d is
the problem dimension and m is the number of samples used for adaptation, then the ratio of the
expected excess risks after task-specific adaptation of the NAL and MAML solutions is approximately
(Corollary 2): EEm(WwERM；) ≈ 1 + R∙ Thus, the largest gain for MAML over NAL occurs when the
task environment satisfies (informal):
I.	Hardness discrepancy: the hard tasks are significantly more difficult than the easy tasks
PH《1, without being impossibly hard (PH ≥ c > 0), and
ρE
II.	Benign geography: the optimal solutions of the hard tasks have small variance rH , and the
distance between the hard and easy task centers R is large.
Figure 1 summarizes observations (I) and (II) by plotting locations of the easy and hard tasks’ optimal
solutions sampled from four distinct task environments and the corresponding solutions and excess
risks for NAL and MAML. The environment in subfigure (a) violates (I) since PH = 1. Subfigures
ρE
(b)-(c) show environments with either small R or large rh, so (II) is not satisfied. In contrast, the
environment in subfigure (d) has small『h, large R, and PH =0.2, in which case as expected MAML
PE
achieves the largest gain over NAL, Em(WNAL) ≈ 3.
Em (wM AML )
Summary - Why MAML. We show theoretically and empirically that MAML outperforms standard
training (NAL) in linear and nonlinear settings by finding a solution that excels on the more-difficult-
to-optimize (hard) tasks, as long as the hard tasks are sufficiently similar. Our work thus highlights
the importance of task hardness and task geography to the success of MAML.
1.1	Related work
A few works have explored why MAML is effective. Raghu et al. (2019) posed two hypotheses
for MAML’s success in training neural networks: rapid learning, meaning that MAML finds a
set of parameters advantageous for full adaptation on new tasks, and feature reuse, meaning that
2
Under review as a conference paper at ICLR 2022
MAML learns a set of reusable features, among the tasks. The authors’ experiments showed that
MAML learns a shared representation, supporting the feature reuse hypothesis. Motivated by this
idea, Saunshi et al. (2020) proved that Reptile (Nichol et al., 2018), a similar algorithm to MAML,
can learn a representation that reduces the new-task sample complexity in a two-task, linear setting.
Conversely, Oh et al. (2020) gave empirical evidence that MAML performance does not degrade
when forced to learn unique representations for each task, and Goldblum et al. (2020) showed that
while some meta-learning algorithms learn more clustered features compared to standard training, the
same cannot be said of MAML. Moreover, a series of works have shown that removing the inner loop
and learning a distinct classifier for each class of images in the environment can yield representations
as well-suited for few-shot image classification as MAML’s (Chen et al., 2019). In this work, we
take a more general, landscape-based perspective on the reasons for MAML’s success, and show that
MAML can still achieve meaningful gain without necessarily learning a representation.
Much of the theory surrounding MAML and meta-learning more broadly has focused on linear
settings, specifically multi-task linear regression or the related linear centroid problem (Denevi et al.,
2018). Some works have studied how to allocate data amongst and within tasks (Cioba et al., 2021;
Bai et al., 2020; Saunshi et al., 2021). Denevi et al. (2018) considered meta-learning a common mean
for ridge regression, and Kong et al. (2020) studied whether many small-data tasks and few large-data
tasks is sufficient for meta-learning. Other works have examined the role of the inner loop SGD
step size in meta-learning approaches (Bernacchia, 2021; Charles & Konecny, 2020; Wang et al.,
2020b), while Gao & Sener (2020) studied the trade-off between the accuracy and computational
cost of the NAL and MAML training solutions. However, unlike our work, these works either did not
consider or did not provide any interpretable characterization of the types of task environments in
which MAML is effective. Other theoretical studies of MAML and related methods have focused on
convergence rates in general settings (Fallah et al., 2020; Rajeswaran et al., 2019; Zhou et al., 2019;
Ji et al., 2020a;b; Collins et al., 2020; Wang et al., 2020a) and excess risk bounds for online learning
(Finn et al., 2019; Balcan et al., 2019; Khodak et al., 2019). Like the current work, Fallah et al. (2020)
noticed that the MAML solution should be closer than the NAL solution to the hard tasks’ global
minima, but the authors neither quantified this observation nor further compared MAML and NAL.
2	Problem Setup: Training to Adapt
We aim to determine when and why MAML yields models that are significantly more adaptable
compared to models obtained by solving the traditional NAL problem in multi-task environments. To
this end, we consider the gradient-based meta-learning setting in which a meta-learner tries to use
samples from a set of tasks observed during meta-training to compute a model that performs well
after one or a few steps of SGD on a new task drawn from the same environment at meta-test time.
Specifically, we follow Baxter (1998) by considering an environment p which is a distribution over
tasks. Each task Ti is composed of a data distribution μi over an input space X and a label space
Y. We take our model class to be the family of functions {hw : X → Y : w ∈ RD} where hw is
a model parameterized by w. The population loss fi(w) on task i is the expected value of the loss
of hw on samples drawn from μi, namely fi(w) ：= E(x,y)〜*稔['(hw(x),y)], where ' is some loss
function, such as the squared or cross entropy loss.
During training, the meta-learner samples T tasks from P and n points {(xj ,yj )}n=ι 〜μnn for
each sampled task Ti . The meta-learner uses this data to compute an initial model w, which is
then evaluated as follows. First, the meta-learner samples a new task Ti 〜P and m labeled points
{(^j,yj)}j=ι 〜μm Next, it updates w with one step of stochastic gradient descent (SGD) on the
loss function fi using those m samples and step size a. Namely, letting XXi ∈ Rm×d and yi ∈ Rm
denote the matrix and vector containing the m feature vectors and their labels, respectively, the update
of W is given by Wi := W - α^wfi(w; XXi, yi), where fi(w; XXi, yi) := mm Pm=I '(hw(xj), yj) is
the empirical average of the loss on the m samples in (XXi, yi). The test loss of W is the expected
population loss of wi, where the expectation is taken over tasks and the m samples, specifically
Fm(Wy= EiE(Xi,yi)[fi(w-aVfi(w; XXi, yi))]	(1)
3
Under review as a conference paper at ICLR 2022
where We have used the shorthand Ei := ETi〜P and E(XXa ^) ：= E(XXa ^)〜μm ∙ For fair evaluation, We
measure solution quality by the excess risk
Em (w) := Fm (w) - inf Fm(w0)	(2)
w0 ∈RD
The excess risk is the difference betWeen the average performance of w after one step of task-specific
adaptation from the performance of the best possible initialization for fast adaptation. To find a model
With small excess risk, one can solve one of tWo problems during training, NAL or MAML.
NAL. NAL minimizes the loss fi (w) on average across the training tasks, Which may yield small
excess risk Em (w) With less computational cost than MAML (Gao & Sener, 2020). Denoting the n
training examples for the i-th task as Xi ∈ Rn×d and their labels as yi ∈ Rn , NAL solves
min FNAL(W) = τ pi=ι fi(w; Xi, yi),	⑶
w∈RD
Which is a surrogate for the expected risk minimization problem defined as minw∈RD Ei [fi (W)].
We let WNal denote the unique solution of the expected risk minimization problem, and let WNAL
denote the unique solution to (3). We emphasize that in our study We evaluate the solution of NAL
by its expected error after running one step of SGD using the m samples from a neW task that are
released at test time, and this error is captured by the excess risk Em(WNAL), defined in (2).
MAML. In contrast to NAL, MAML minimizes a surrogate loss of (1) during training. According
to the MAML frameWork, the n samples per task are divided into τ training “episodes”, each With
n2 “inner” samples for the inner SGD step and n1 “outer” samples for evaluating the loss of the
fine-tuned model. Thus, We have that τ(n1+ n2) = n. We denote the matrices that contain the outer
samples for the j-th episode of the i-th task as Xio,ujt ∈ Rn1×d and yio,ujt ∈ Rn1, and the matrices that
contain the inner samples as Xii,nj ∈ Rn2×d and yii,nj ∈ Rn2. The MAML objective is then
min
w∈RD
Tτ
FMAML(Wy= TT XX fi(w -Wfi (w; Xin, yij); Xout, yθ,ut).
i=1 j=1
(4)
We denote the unique solution to (4) by WMAML and the unique solution to the population version (1)
by WMAML. We expect Em(WMAML) ≤ Em(WNAL) since (4) is a surrogate for the true objective
of minimizing (1). HoWever, the gain of MAML over NAL may not be significant enough to justify
its added computational cost (Gao & Sener, 2020), necessitating a thorough understanding of the
relative behavior of MAML and NAL.
3	Multi-Task Linear Regression
We first explore the relative behaviors of MAML and NAL in a setting in Which We can obtain
closed-form solutions: multi-task linear regression. Here, the model hw maps inputs x to predicted
labels by taking the inner product With W, i.e. hw(x) = hW, xi. The loss function ` is the squared
loss, therefore fi(W)= 1 E(χi,yi)〜,“i [(hw, x>-yi)2] and fi(W； Xi, yi) = 2∣∣XiW-yik2 for all i. We
consider a realizable setting in Which the data for the i-th task is Gaussian and generated by a ground
truth model Wi产 ∈ Rd. That is, points (Xi, yi) are sampled from μ% by first sampling Xi 〜N(0, ∑i),
then sampling y% 〜N (hWi/, xj,ν2), i.e. y% =(Wi/, Xii+ Zi where Zi 〜N (0,ν2). In this setting
the population-optimal solutions for NAL and MAML are given by:
WNal = Ei[∑i]-1Ei[∑iW",	and	WM AML =叫心，2)]-1冉心，2$汴	(5)
where for any S ∈ N, we define Qis) := (Id — α∑i)∑i(Id-α∑i) + 哼(tr(∑2)∑i + ∑3). Note that
these Qi(s) matrices are composed of two terms: a preconditioned covariance matrix Σi (Id - αΣi)2,
and a perturbation matrix due to the stochastic gradient variance. We provide expressions for the
empirical solutions WNAL and WMAML for this setting and show that they converge to WNAL and
WMAML as n,T,τ → ∞ in Appendix D. Since our focus is ultimately on the nature of the solutions
sought by MAML and NAL, not on their non-asymptotic behavior, we analyze WNAL and WMAML
in the remainder of this section, starting with the following result.
It is most helpful to interpret the solutions WMAML and WNAL and their corresponding excess risks
through the lens of task hardness. In this strongly convex setting, we naturally define task hardness
4
Under review as a conference paper at ICLR 2022
as the rate at which gradient descent converges to the optimal solution for the task, with harder
tasks requiring more steps of gradient descent to reach an optimal solution. For step size α fixed
across all tasks, the rate with which gradient descent traverses each fi is determined by the minimum
eigenvalue of the Hessian of fi, namely λmin (Σi). So, for ease of interpretation, we can think of
the easy tasks as having data with large variance in all directions (all the eigenvalues of their Σi are
large), while the hard tasks have data with small variance in all directions (all λ(Σi) are small).
Note that both WMAML and WNAL are normalized weighted sums of the task optimal solutions,
with weights being functions of the ∑i's. For simplicity, consider the case in which m and n are
large, thus the Qi matrices are dominated by ∑i(Id — α∑i)2. Since the weights for WNAL are
proportional to the ∑i matrices, WNAL is closer to the easy task optimal solutions, as ∑i has large
energy for easy tasks and small energy for hard tasks. Conversely, the MAML weights are determined
by ∑i(Id — α∑i)2, which induces a relatively larger weight on the hard tasks, so WMAML is closer
to the hard task optimal solutions.
Note that easy tasks can be approximately solved after one step of gradient descent from far away,
which is not true for hard tasks. We therefore expect (I) MAML to perform well on both hard and
easy tasks since WMAML is closer to the optimal solutions of hard tasks, and (II) NAL to perform
well on easy tasks but struggle on hard tasks since WNAL is closer to the optimal solutions of easy
tasks. We explicitly compute the excess risks of WNAL and WMAML as follows.
Proposition 1. The excess risks for WN AL and WMAML are：
Em(WNAL) = 1 Ei∣∣Ei0 [∑i0]-1Ei0 [Σi0 (WXW"晨m)	(6)
Em(WMAML) = 2Ei∣∣Ei0 [Q(n2)]-1Ei0 [Q(n2)(W— W*)]∣∣Q(m)	⑺
We next formally interpret these excess risks, and develop intuitions (I) and (II), by focusing on two
key properties of the task environment: task hardness discrepancy and task geography.
3.1	Hardness discrepancy
We analyze the levels of task hardness that confer a significant advantage for MAML over NAL in
this section. To do so, we compare WMAML and WNAL in an environment with two tasks of varying
hardness. We let n2 =m, Σ1=ρHId, and Σ2 = ρEId, where ρH <ρE, thus task 1 is the hard task.1
In this setting, the NAL and MAML solutions defined in (5) can be simplified as
wN AL
PH
ρH + ρE
* I
Wl +
PE
ρH + ρE
*
W2*,
*	aH * aE *
WMAML = aH+∑EW1 + aH+∑E w2,
(8)
where aE := (PE(1 — αρE)2 + dm1 α2ρE) and aH := (PH(1 — αρH)2 + d+1 α2PH). Note that the
natural choice of α is the inverse of the largest task smoothness parameter (ɪ in this case). Setting
ρE
α = 1- yields aE = d+1 PE and a∏ = PH(1 一 PH)2 + (d+13pH. As a result, we can easily see that
ρE	m	ρE	mρE
for sufficiently large values of m, we have aH > aE . This observation shows that the solution of
MAML is closer to the solution of the harder task, i.e., W1*. On the other hand, WNAL is closer to
W2* , the solution to the easy task, since PH < PE .
Considering these facts, we expect the performance of MAML solution after adaptation to exceed
that of NAL. Using Proposition 1, the excess risks for NAL and MAML in this setting are
Em(WNAL)
aE PH + aH PE
(PE + PH )2
Em(WMAML)
aE aH
aE + aH
(9)
Recalling that aE ≈ 0 for m d, we conclude that MAML achieves near-zero excess risk in the
case of large m. In particular, we require that PH(1 一 PH)2》dm, otherwise the O(d∕m) terms
1The effects of task hardness could be removed by scaling the data so that it would have covariance α-1Id.
However, the current setting is useful to build intuition. Further, one can imagine a similar setting in which the
first dimension has variance α-1 and the rest have variance ρH, in which scaling would not be possible (as it
would result in gradient descent not converging in the first coordinate).
5
Under review as a conference paper at ICLR 2022
-0.25
-0.50
-0.75
-1.00
,-00-70'0's0-0
UO-SUeE-P-lil U- 3eu-p」。。。
-S-S⅛i≈SS30XLU
Excess Risk: m = n 2 = 2000
----Wmaml *
----Wnal *
Location of Solutions: m = n 2 = 100
UO-SUeE-Ps」y U- 3-eu-p」。。。
0.0	0.2	04	06	0.8	1.0
PH
Excess Risk: m = n 2 = 100
-i∙7 *ssSS30XLU
0.0	02	0.4	06	0.8	1.0
PH
(a)	(b)	(c)	(d)
Figure 2: First coordinates of wMaml，WNAL and their excess risks for various PH for large m (in
subplots (a)-(b) for m = 2000) and small m (in subplots (c)-(d) for m= 100).
in aE and aH are non-negligible. Meanwhile, for NAL We have Em(WNAL) =(p；+E)2, which
may be significantly larger than zero if PH《1, i.e. the harder task is significantly harder than the
ρE
easy task. Importantly, the error for NAL is dominated by poor performance on the hard task, which
MAML avoids by initializing close to the hard task solution. Thus, these expressions pinpoint the
level of hardness discrepancy needed for superior MAML performance: PH must be much smaller
than 1, but also larger than 0, such that PH (1 一 PH )2》m.
Figure 2 visualizes these intuitions in the case that W1 = 1d, w2 = -1d, d = 10 and σ2 = 0.01.
Subfigures (a) and (c) show the locations of the first coordinates of WNAL and WMAML for varying
PH, and m = 2000 and m = 500, respectively. Subfigures (b) and (d) show the corresponding excess
risks. We observe that unlike NAL, MAML initializes closer to the optimal solution of the harder
task as long as PH is not close to zero or one, which results in significantly smaller excess risk for
PE
MAML compared to NAL in such cases, especially for large m.
Figure 2 further shows that the MAML and NAL excess risks go to zero with PH . This is also shown
in (9) and the definition of aH, and points to the fact that too much hardness discrepancy causes no
gain for MAML. The reason for this is that PH → 0 corresponds to the hard task data going to zero
(its mean), in which case any linear predictor has negligible excess risk. Consequently, both NAL
and MAML ignore the hard task and initialize at the easy task to achieve near-zero excess risk here.
Remark 1. The condition PH (1 — PH)2》m requires that m》d. However, this condition arises
due to the simplification tr(Σi) = O(d), where tr(Σi) can be thought of as the effective problem
dimension (Kalan et al., 2020). In realistic settings, the effective dimension may be o(d), which would
reduce the complexity of m accordingly.
3.2	Task geography
The second important property of the task environment according to Proposition 1 is the location, i.e.
geography, of the task optimal solutions. In this section, we study how task geography affects the
MAML and NAL excess risks by considering a task environment with many tasks. In particular, the
task environment μ is a mixture over distributions of hard and easy tasks, with mixture weights 0.5.
The optimal solutions w* ∈ Rd for hard tasks are sampled according to w* 〜N(R1d,『hId) and
for easy tasks are sampled as w* 〜N(0d,『eId). Therefore R is the dimension-normalized distance
between the centers of the hard and easy tasks’ optimal solutions, and rH and rE capture the spread
of the hard and easy tasks’ optimal solutions, respectively. The data covariance is Σi = PHId for the
hard tasks and Σi = PEId for the easy tasks, recalling that PH and PE parameterize hardness, with
smaller PH meaning harder task. In this setting the following corollary follows from Proposition 1.
Corollary 1. In the setting described above, the excess risks of wN* AL and w*MAML are:
Em(WNAL) = 4(pe - )2 [(aEPE + 2aEPEPH)rE + aEPH(rE + R2)
+ (aH P2H + 2aH PE PH)『H + aH P2E (『H + R2)	(10)
Em(WMAML) = 4(0,二 二 一 )2 [(aE + 2aEaH)rE + (aH + 2aEaH)rH
+ aE a2H (『E + R2 ) + a2E aH (『H + R2)	(11)
6
Under review as a conference paper at ICLR 2022
25 -
Model Performance
Ratio £m (WNAL)/£m ( Wmaml )
Model Performance
-≥ )E3 *s≈SSQ3xuj
OinOin
-iE.3 *s≈SSQ3xu
—w— WMAML
-∙- WMAML
----WNAL
----wAL
Ratio £m ( Wnal Ygm ( Wmaml )
0.5
R2
rE
1	2	3	4	5
R
(d) RH=10,
0	1	2	3	4	5
R
1	2	3	4	5
R
⑶ R=0.5,
R2
rE
10
(b) RH"
R2
rE
10
0	1	2	3	4	5
R
(c) RH=10, R=0.5
Figure 3: Theoretical and empirical excess risks for NAL and MAML and ratios of the NAL to
MAML excess risk in the setting described in Section 3.2.
where °e := PE(1 ― αρE)2 + d+1 α2ρE and °h := PH(1 ― αρH)2 + d+1 α2ρH-.
Each excess risk is a normalized weighted sum of the quantities rE , rH and R2 . So, the comparison
between MAML and NAL depends on the relative weights each algorithm induces on these task
environment properties. If 黑(1 一 PH)2》今,then aH》。石 and the dominant weight in
Em(WMaml) is on the yh term, while the dominant weights in Em(WNAL) are on the yh + R2 and
rH terms. This observation leads us to obtain the following corollary of Proposition 1.
Corollary 2. In the above setting, with α = 1/pe and PH (1 — PH)2》mm, h relative excess risk
for NAL compared to MAML satisfies
Em(WNAL)	≈ ι + R2
Em(WM AML)	rH
(12)
Corollary 2 shows that MAML achieves large gain over NAL when: (i) the hard tasks’ solutions are
closely packed (rH is small) and (ii) the hard tasks’ solutions are far from the center of the easy tasks’
solutions (R is large). Condition (i) allows MAML to achieve a small excess risk by initializing in
the center of the hard task optimal solutions, while condition (ii) causes NAL to struggle on the hard
tasks since it initializes close to the easy tasks’ solutions. These conditions are reflected by the fact
that the MAML excess risk weighs rH (the spread of the hard tasks) most heavily, whereas the NAL
excess risk puts the most weight on R2 (distance between hard and easy task solutions) as well as rH .
Note that the above discussion holds under the condition that aH aE. In order for aH aE, we
must have 黑(1 一 PH )2》m,i.e. m》d and 0 < PH《1, as we observed in our discussion on
hardness discrepancy. Now, we see that even with appropriate hardness discrepancy, the hard tasks
must be both closely packed and far from the center of the easy tasks in order for MAML to achieve
significant gain over NAL. This conclusion adds greater nuance to prior results (Balcan et al., 2019;
Jose & Simeone, 2021), in which the authors argued that gradient-based meta-learning (e.g. MAML)
is only effective when all of the tasks’ optimal solutions are close to each other (in our case, all of
rE, rH and R are small). Crucially, our analysis shows that NAL would also perform well in this
scenario, and neither rE nor R need to be small for MAML to achieve small excess risk.
To further explain these insights, we return to Figure 1 in which easy and hard task optimal solutions
are each sampled from 10-dimensional Gaussian distributions of the form described above, and
100 random task optimal solutions are shown, along with the population-optimal MAML and NAL
solutions. In subfigures (b), (c) and (d), we set ρE = 0.9, ρH = 0.1, and m = 500. Among these
plots, the largest gain for MAML is in the case that the hard tasks’ optimal solutions are closely
packed (small rH), and their centers are far from each other (large R), demonstrating the the primary
dependence of relative performance on R2 /rH.
We plot more thorough results for this setting in Figure 3. Here, we vary R and compare the
performance of WNAL and WMAML in settings With relatively large YH and small/石,specifically
choosing yh and ye such that R = 0.5 and R = 10 in (a)-(b), and
rH	rE
R- = 10 and R = 0.5 in
rH
rE
(c)-(d). Here we also plot the empirical solutions wNAL and wMAML. Again, MAML significantly
outperforms NAL when R2 /rH is large (subfigures (c)-(d)), but not otherwise.
4 Two-layer Neural Network
In this section, we consider a non-linear setting in which each task is a regression problem with a
two-layer neural network with a fixed second layer. The k-th neuron in the network maps Rd → R
7
Under review as a conference paper at ICLR 2022
via σ(hwk, xi), where σ : R → R is some activation function and wk ∈Rd is the parameter vector.
The network contains M neurons for a total of D = Md parameters, which are contained in the
matrix W := [w1, . . . , wM] ∈ Rd×M. The predicted label for the data point x is the sum of
the neuron outputs, namely hW (x) := PkM=1 σ(hwk , xi). The loss function is again the squared
loss, i.e. fi(W) =2E(Xi,yi)〜μi[(σ(PM=ιhwk, Xii) - yi)2]. Ground-truth models Wi,* generate
the data for task i. We sample (Xi, yi)〜μ% by first sampling Xi 〜N(0d, ∑i) then computing
yi = PkM=1 σ(hwik,*, Xii). The following result demonstrates an important property of the MAML
objective function in the two-task version of this setting.
Theorem 1. Suppose that in the setting described above, the task environment is the uniform
distribution over two tasks, and σ is the ReLU, Softplus, Sigmoid, or tanh function. Let Σi = Id
ΠM sik
and define Sik as the k-th singular value of Wi *, Ki := kWi *∣∣2∕siM, and λi := k=M , for
si,M
i ∈ {1, 2}. Further, define the regions Si := {W : kW - Wi,* k2 ≤ c1∕(Sic,1λiκi2M2)} and the
parameters βi := τc⅛ and Li := c3Ms2C for i ∈ {1, 2} and absolute constants c, c1,c2, and c3.
λiκi	i,1
Let β1 <β2 and L1 < L2. For any stationary point WM* AM L of the population MAML objective (1)
with full inner gradient step (m = ∞), such that WM* AM L ∈ S1 ∩ S2, must satisfy for all α ≤ 1∕L2:
kvf1(WMAML)∣∣2 ≤「- 2:β22 ∣∣vf2(WMAML)∣∣2+os”	(13)
(1 - αL1 )2
Interpretation: MAML prioritizes hard tasks. In the setting above, βi and Li are strong convexity
and smoothness parameters, respectively, of the function fi on the region Si . Here, task 1 is
the harder task since β1 and β2 control the rate with which gradient descent converges to the
ground-truth solutions (Zhong et al., 2017), with smaller βi implying slower convergence, and
β1 < β2 . Thus, Theorem 1 shows that, any stationary point of the MAML objective in S1 ∩ S2
has smaller gradient norm on the hard task than on the easy task as long as there is sufficient
hardness discrepancy, specifically β2 > L1 . Physically, this condition means that the curvature of
the loss function for the easy task is more steep than the curvature of the hard task in all directions
around their ground-truth solutions. The smaller gradient norm of MAML stationary points on the
harder task suggests that MAML solutions prioritize hard-task performance. In contrast, we can
easily see that any stationary point wNAL of the NAL population loss must satisfy the condition
kvf1 (wN AL)k2 = kvf2 (wN AL)k2, meaning that NAL has no such preference for the hard task.
Figure 4 demonstrates the importance of task hardness in comparing NAL and MAML in this setting.
Here, for ease of visualization we consider the case in which M = 1 and d = 2, i.e. the learning
model consists of one neuron (with Softplus activation) mapping from R2 → R. Each subfigure plots
the NAL or MAML population loss landscape for different task environments with m=250, as well
as the ground-truth neuron for each task. In light of prior work showing that the number of gradient
steps required to learn a single neuron diminishes with the variance of the data distribution (Theorem
3.2 in Yehudai & Ohad (2020)), we again control task hardness via the data variance. In all plots,
Σi = 0.5I2 for hard tasks and Σi = I2 for easy tasks. Note that the MAML loss (evaluated after one
step of adaptation) is the evaluation metric we ultimately care about.
We observe that when all tasks are equally hard, the MAML and NAL solutions are identical
(subfigures (a)-(b)), whereas when one task is hard, MAML initializes closer to the hard task and
achieves significantly better post-adaptation performance than the NAL solution, which is closer to
the centroid of the easy tasks (c)-(d). This supports our intuition that MAML achieves significant
gain over NAL in task environments in which it can leverage improved performance on hard tasks.
5 Experiments
We next experimentally study whether our observations generalize to problems closer to those seen
in practice. We consider image classification on the Omniglot (Lake et al., 2019) and FS-CIFAR100
(Oreshkin et al., 2018) datasets. Following convention, tasks are N -way, K-shot classification
problems, i.e., classification problems among N classes where K labeled images from each class
are available for adapting the model. For both the Omniglot and FS-CIFAR100 experiments, we use
the five-layer CNN used in Finn et al. (2017); Vinyals et al. (2016). NAL is trained equivalently to
MAML with no inner loop and n=n1+n2. Further details and error bounds are in Appendix C.
8
Under review as a conference paper at ICLR 2022
0 NAL Loss Landscape - SOftPlUS
Easy Tasks
MAML Loss Landscape - SOftPlUS
,0 -∣---------------------------1
-0
-1
-1
-2
-2-10	1	2
(a)
101
,5
,0
- 6×100
,5
,0
- 4×100 -0
,5
- 3×100
-1
,0
-1
,5
-2
,0
★ NAL Solution
K MAML Solution
-2	-1	0	1	2
100
-1
10-1
NAL Loss Landscape - SOftPlUS
.0 -I-------------'---------
,5∙
.0∙
.5∙
.0∙
-0
.5∙
-1
.0∙
-1
.5∙
-2
.0∙
・ Easy Tasks
■ Hard Task
-2-10	1	2
101
-6× 100
4×100
3× 100
2× 100
MAML Loss Landscape - Softplus
.0 ------------------'----------1
NAL Solution
MAML Solution
,5∙
,0∙
-0
-1
,0∙
-1
-2
-2	-1	0	1	2
(b)	(c)	(d)
2
2
2
2
1
5 -
1
1
1
,5 -
1
0 -
0
5 -
0
0 -
5 -
0 -
5 -
0 -
1
0
0
1
0
0
1
,0 -
0
0
,5 -
,5 -
,0 -
Figure 4: Loss landscapes and ground-truth neurons for NAL (a,c) and MAML (b,d) for two distinct,
four-task environments and Softplus activation.
Table 1: Omniglot accuracies.					
Setting		Train Tasks		Test Tasks	
rH	Alg.	Easy	Hard	Easy	Hard
Large	MAML	-^992^^	96.0	98.0	81.2
Large	NAL-1	69.4	41.5	57.8	45.2
Large	NAL-10	70.0	45.3	67.2	47.9
Small	MAML	-^992^^	99.1	98.1	95.4
Small	NAL-1	69.2	46.0	55.8	45.8
Small	NAL-10	70.2	44.0	67.8	48.9
Table 2: FS-CIFAR100 accuracies.
Setting		Train Tasks		Test Tasks	
p	Alg.	Easy	Hard	Easy	Hard
0.99	NAL	78.2	9.9	74.8	9.5
	MAML	92.9	50.1	84.6	21.7
0.5	NAL	81.9	9.5	76.5	8.6
	MAML	93.6	41.1	84.3	17.9
0.01	NAL	82.4	7.6	76.9	8.2
	MAML	94.0	15.7	85.0	11.8
Omniglot. Omniglot contains images of 1623 handwritten characters from 50 different alphabets.
Characters from the same alphabet typically share similar features, so are harder to distinguish
compared to characters from differing alphabets. We thus define easy tasks as classification problems
among characters from distinct alphabets, and hard tasks as classification problems among characters
from the same alphabet, consistent with prior definitions of semantic hardness (Zhou et al., 2020).
Here we use N = 5 and K = 1. For NAL, we include results for K = 10 (NAL-10) in addition to
K = 1 (NAL-1). We split the 50 alphabets into four disjoint sets: easy train (25 alphabets), easy test
(15), hard train (5), and hard test (5). During training, tasks are drawn by first choosing ‘easy’ or
‘hard’ with equal probability. If ‘easy’ is chosen, 5 characters from 5 distinct alphabets among the
easy train alphabets are selected. If ‘hard’ is chosen, a hard alphabet is selected from the hard train
alphabets, then N = 5 characters are drawn from that alphabet. After training, we evaluate the models
on new tasks drawn analogously from the test (unseen) alphabets as well as the train alphabets.
Table 1 gives the average errors after completing training for two experiments: the first (Large) when
the algorithms use all 5 hard train and test alphabets, and the second (Small) when the algorithms use
only one hard train alphabet (Sanskrit) and one hard test (Bengali). The terms ‘Large’ and ‘Small’
describe the hypothesized size of rH in each experiment: the optimal solutions of hard tasks drawn
from ten (train and test) different alphabets are presumably more dispersed than hard tasks drawn
from only two (train and test) similar alphabets. By our previous analysis, we expect MAML to
achieve more gain over NAL in the Small rH setting. Table 1 supports this intuition, as MAML’s
performance improves significantly with more closely-packed hard tasks (Small), while NAL’s does
not change substantially. Hence, MAML’s relative gain over NAL increases with smaller rH.
FS-CIFAR100. FS-CIFAR100 has 100 total classes that are split into 80 training classes and 20
testing classes. We further split the 600 images in each of the training classes into 450 training
images and 150 testing images in order to measure test accuracy on the training classes in Table 2.
Here we use N and K as proxies for hardness, with larger N and smaller K being more hard as the
agent must distinguish more classes with fewer training samples/class. Specifically, easy tasks have
(N, K) = (2, 10), and the hard tasks have (N, K) = (20, 1). During training, hard tasks are sampled
with probability p and easy tasks with probability 1 - p. Observe that the largest performance gains
for MAML in Table 2 are on the hard tasks, consistent with our conclusion that MAML outperforms
NAL primarily by achieving relatively high accuracy on the hard tasks. However, the improvement
by MAML on the hard tasks disappears when the hard tasks are scarce (p = 0.01), supporting the
idea of oversampling hard tasks during training in order to improve MAML performance (Zhou et al.,
2020; Sun et al., 2020).
9
Under review as a conference paper at ICLR 2022
6	Ethics Statement
We believe that our paper does not have any potential ethical concerns.
7	Reproducibility Statement
The proofs for all theoretical results are in the Appendix. Experimental details can also be found in
the Appendix, and we have provided our code, as well as instructions on how to use it, in a zip file as
part of the supplementary material.
References
Antoniou, A., Edwards, H., and Storkey, A. How to train your maml. In International Conference on
Learning Representations, 2018.
Bai, Y., Chen, M., Zhou, P., Zhao, T., Lee, J. D., Kakade, S., Wang, H., and Xiong, C. How important
is the train-validation split in meta-learning? arXiv preprint arXiv:2010.05843, 2020.
Balcan, M.-F., Khodak, M., and Talwalkar, A. Provable guarantees for gradient-based meta-learning.
In International Conference on Machine Learning, pp. 424-433. PMLR, 2019.
Baxter, J. Theoretical models of learning to learn. In Learning to learn, pp. 71-94. Springer, 1998.
Bernacchia, A. Meta-learning with negative learning rates. arXiv preprint arXiv:2102.00940, 2021.
Charles, Z. and Konecny, J. On the outsized importance of learning rates in local update methods.
arXiv preprint arXiv:2007.00878, 2020.
Chen, W.-Y., Liu, Y.-C., Kira, Z., Wang, Y.-C. F., and Huang, J.-B. A closer look at few-shot
classification. arXiv preprint arXiv:1904.04232, 2019.
Cioba, A., Bromberg, M., Wang, Q., Niyogi, R., Batzolis, G., Shiu, D.-s., and Bernacchia, A. How to
distribute data across tasks for meta-learning? arXiv preprint arXiv:2103.08463, 2021.
Collins, L., Mokhtari, A., and Shakkottai, S. Task-robust model-agnostic meta-learning. In Advances
in Neural Information Processing (NeurIPS), 2020.
Denevi, G., Ciliberto, C., Stamos, D., and Pontil, M. Learning to learn around a common mean.
Advances in Neural Information Processing Systems, 31:10169-10179, 2018.
Du, S. S., Hu, W., Kakade, S. M., Lee, J. D., and Lei, Q. Few-shot learning via learning the
representation, provably. arXiv preprint arXiv:2002.09434, 2020.
Fallah, A., Mokhtari, A., and Ozdaglar, A. On the convergence theory of gradient-based model-
agnostic meta-learning algorithms. In International Conference on Artificial Intelligence and
Statistics, pp. 1082-1092, 2020.
Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta-learning for fast adaptation of deep
networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pp. 1126-1135. JMLR. org, 2017.
Finn, C., Rajeswaran, A., Kakade, S., and Levine, S. Online meta-learning. In International
Conference on Machine Learning, pp. 1920-1930, 2019.
Gao, K. and Sener, O. Modeling and optimization trade-off in meta-learning. Advances in Neural
Information Processing Systems, 33, 2020.
Goldblum, M., Reich, S., Fowl, L., Ni, R., Cherepanova, V., and Goldstein, T. Unravel-
ing meta-learning: Understanding feature representations for few-shot tasks. arXiv preprint
arXiv:2002.06753, 2020.
Ji, K., Lee, J. D., Liang, Y., and Poor, H. V. Convergence of meta-learning with task-specific
adaptation over partial parameters. arXiv preprint arXiv:2006.09486, 2020a.
10
Under review as a conference paper at ICLR 2022
Ji, K., Yang, J., and Liang, Y. Theoretical convergence of multi-step model-agnostic meta-learning.
arXiv e-prints,pp. arXiv-2002, 2020b.
Jose, S. T. and Simeone, O. An information-theoretic analysis of the impact of task similarity on
meta-learning. arXiv preprint arXiv:2101.08390, 2021.
Kalan, S. M. M., Fabian, Z., Avestimehr, A. S., and Soltanolkotabi, M. Minimax lower
bounds for transfer learning with linear and one-hidden layer neural networks. arXiv preprint
arXiv:2006.10581, 2020.
Khodak, M., Balcan, M.-F. F., and Talwalkar, A. S. Adaptive gradient-based meta-learning methods.
In Advances in Neural Information Processing Systems, pp. 5915-5926, 2019.
Kong, W., Somani, R., Song, Z., Kakade, S., and Oh, S. Meta-learning for mixed linear regression.
In International Conference on Machine Learning (ICML), 2020.
Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. The omniglot challenge: A 3-year progress
report. Current Opinion in Behavioral Sciences, 29:97-104, 2019.
Nichol, A., Achiam, J., and Schulman, J. On first-order meta-learning algorithms. arXiv preprint
arXiv:1803.02999, 2018.
Oh, J., Yoo, H., Kim, C., and Yun, S.-Y. Does maml really want feature reuse only? arXiv preprint
arXiv:2008.08882, 2020.
Oreshkin, B. N., Rodriguez, P., and Lacoste, A. Tadam: task dependent adaptive metric for improved
few-shot learning. In Proceedings of the 32nd International Conference on Neural Information
Processing Systems, pp. 719-729, 2018.
Raghu, A., Raghu, M., Bengio, S., and Vinyals, O. Rapid learning or feature reuse? towards
understanding the effectiveness of maml. In International Conference on Learning Representations,
2019.
Rajeswaran, A., Finn, C., Kakade, S. M., and Levine, S. Meta-learning with implicit gradients. In
Advances in Neural Information Processing Systems, pp. 113-124, 2019.
Saunshi, N., Zhang, Y., Khodak, M., and Arora, S. A sample complexity separation between non-
convex and convex meta-learning. In International Conference on Machine Learning (ICML),
2020.
Saunshi, N., Gupta, A., and Hu, W. A representation learning perspective on the importance of
train-validation splitting in meta-learning. In International Conference on Machine Learning, pp.
9333-9343. PMLR, 2021.
Schudy, W. and Sviridenko, M. Concentration and moment inequalities for polynomials of in-
dependent random variables. Proceedings of the Twenty-Third Annual ACM-SIAM Sympo-
sium on Discrete Algorithms, Jan 2012. doi: 10.1137/1.9781611973099.37. URL http:
//dx.doi.org/10.1137/1.9781611973099.37.
Sun, Q., Liu, Y., Chen, Z., Chua, T.-S., and Schiele, B. Meta-transfer learning through hard tasks.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.
Tripuraneni, N., Jin, C., and Jordan, M. I. Provable meta-learning of linear representations. arXiv
preprint arXiv:2002.11684, 2020.
Tropp, J. A. An introduction to matrix concentration inequalities. arXiv preprint arXiv:1501.01571,
2015.
Vershynin, R. High-Dimensional Probability: An Introduction with Applications in Data Science,
volume 47. Cambridge University Press, 2018.
Vinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K., and Wierstra, D. Matching networks for
one shot learning. arXiv preprint arXiv:1606.04080, 2016.
11
Under review as a conference paper at ICLR 2022
Wang, H., Sun, R., and Li, B. Global convergence and induced kernels of gradient-based meta-
learning with neural nets. arXiv preprint arXiv:2006.14606, 2020a.
Wang, X., Yuan, S., Wu, C., and Ge, R. Guarantees for tuning the step size using a learning-to-learn
approach. arXiv preprint arXiv:2006.16495, 2020b.
Yehudai, G. and Ohad, S. Learning a single neuron with gradient methods. In Conference on Learning
Theory,pp. 3756-3786. PMLR, 2020.
Zhong, K., Song, Z., Jain, P., Bartlett, P. L., and Dhillon, I. S. Recovery guarantees for one-hidden-
layer neural networks. In International conference on machine learning, pp. 4140-4149. PMLR,
2017.
Zhou, P., Yuan, X., Xu, H., Yan, S., and Feng, J. Efficient meta learning via minibatch proximal
update. In Advances in Neural Information Processing Systems, pp. 1532-1542, 2019.
Zhou, Y., Wang, Y., Cai, J., Zhou, Y., Hu, Q., and Wang, W. Expert training: Task hardness aware
meta-learning for few-shot classification. arXiv preprint arXiv:2007.06240, 2020.
12
Under review as a conference paper at ICLR 2022
Appendix
A Proofs for Section 3
A.1 Proof of Proposition 1
For all i, let yin = Xinw** + Zin and yθut = XoutWi,* + Zout. In other words, Zin ∈ Rn2 is the
vector containing the additive noise for the inner samples and ziout ∈ Rn1 is the vector containing the
additive noise for the outer samples.
From (41) and (43), we have that
w*NAL = Ei [Σi]-1Ei [Σiwi*]	(14)
w*MAML = Ei[Qi(n2)]-1Ei[Qi(n2)wi*],	(15)
Next, we compute a closed-form expression for Fm(W) (defined in (1)). For all i, let ^i = Xiwi,* +Zi
and yi = xi>wi,* + zi. In other words, Ziin ∈ Rm is the vector containing the additive noise for the
inner samples and zi ∈ R is the scalar additive noise containing the outer (evaluation) sample. Then
we have
F (W) = 2 EiE(Xi,yi)E(X i ,yi)
=2 EiE(Xi,yi)E(X i ,yi)
=2 EiE(Xi,yi)E(X i ,yi)
=2 EiE(Xi,yi)E(X i ,yi)
2Ei ]exi kPi(w -
i
(Dχi, W - mX >(X iw - yi)E - %)]
(hχi, Pi(W - w*)i+(zi+mm χ>X >zi))]
hχi, Pi(w -w* )i2+(Zi+mα χ>X > Zi)]
2
hxi, Pi(W - w*)i2 + σ2 + mtr(x>X>ZiZ>XiXi)
22
w*)k∑i+ σ2 + / tr(∑2)
一 一 _______ -ʌ ɪ ^-
where Pi := Id — mX>Xi. Hence,
22
inf Fm(W)= σ2 +--tr(Σ2)
w∈Rd	m
(16)
Thus we have
Em(wNAL)
Fm(WNAL) - 2Ei
EiEXi kPi(WNAL - W*)k∑i
EiEXi [(wnal - Wi*)>Pi>ΣiPi(WNAL - Wi* )
EiEXi [ (Ei，[∑io]-1Eio[∑io(w*o - w*)])>
Pi>ΣiPiEi0 [Σi0]-1Ei0 [Σi0(Wi*0 - Wi*)
Ei [ (Ei，[∑i0]-1Ei0[∑i0(W*, - w*)])>
EXi [P>∑iPi] (Ei，[Σi,]-1Ei,[∑i,(w*0 - w*)])]
Eih(Eio [∑i,]-1Ei,[∑i,(W*, - w*)])>
Qim)(Ei，[Σi,]-1Ei,[Σi,(w*, - w*)])]	(17)
EiII(Ei，[Σi,]-1Ei,[Σi,(w*, - w*)])∣∣Q(m)	(18)
13
Under review as a conference paper at ICLR 2022
where (17) follows from Lemma 2. Similarly, for MAML we have the following chain of equalities
for Em (wMAML ) :
Em(wMAML)
1	2 α2σ2	2
=Fm (WMAML ) - 2Ei σ +--------m~tr(2 )
=EiEXi ∣∣Pi(wMAML - W*)k∑i
=EiEXi [(WMAML - w">P>∑iPi(wMAML - W* )]
=EiEXi	(Ei，[Q(n2)]TEi0[Q(n2)w*0] - w*)> P>∑iPi (Ei，[Q(n2)]TEi0[Q(n2)w*0] - W*)]
=EiEXi	(Ei0[Q(n2)]TEi0[Q(n2)(w*o - w*)])>
Pi>ΣiPi (Ei，[Qi(，n2)]-1Ei，[Qi(，n2)(Wi*， -Wi*)])
=Ei(Ei，[Qi(，n2)]-1Ei，[Qi(，n2)(Wi*， -Wi*)])>
EXi [P>∑iPi] (Ei0[Q(n2)]-1Ei0[Q(n2)(w*o - w*)])
=Ei (Ei，[Qi(，n2)]-1Ei，[Qi(，n2)(Wi*， - Wi*)])> Qi(m) (Ei，[Qi(，n2)]-1Ei，[Qi(，n2)(Wi*， -Wi*)])
(19)
=Ei(Ei，[Qi(，n2)]-1Ei，[Qi(，n2)(Wi*， - Wi*)])2 (m)	(20)
Qi
where (19) follows from Lemma 2.
A.2 Proof of Corollary 1
We first write out the dimension-wise excess risks for NAL and MAML, using the results in Propo-
sition 1. Let wi*,l denote the l-th element of the vector Wi* . Since each Σi is diagonal, we have
that Qi(m) is diagonal. Let λi,l denote the l-th diagonal element of the matrix Σi and let qi,l denote
α2
the l-th diagonal element of the matrix Q(m), thus qi,ι = λi,ι (1 - αλi,ι)2 +-(tr(∑2)λi,ι + λ3j.
Recall that we assume n2 = m. The error for NAL in the l-th dimension is:
Em)(WNAL) = 1 Ei
Cm「、12 Ei [Ei0 [Ei00 [qi,lλi0,lλi00,l (w*0,l - Wi,l) (Wi，，,l - w*,l)]]]
2Ei [λi,l]
Likewise, the error for MAML in the l-th dimension is:
Em)(W"2 T (	)]
=2E⅛Ei [Ei0 [Ei00 ®®iWOl (wi0,l- w3(*l- w3]]]
14
Under review as a conference paper at ICLR 2022
Next we use the definitions of λi,l for the easy and hard tasks. For the easy tasks, note that
2
qeasy = λi,l (I- αλi,l)2 + m (M£2)%,l + λ3,l)
=PE (1 — αρE )2 +(d +1)α2ρE
m
=: aE	(21)
and similarly for the hard tasks, namely
qhard = PH(1 - αρH)2 + (d +1)α2ρH =: aH	(22)
,m
Next, recall that the task distribution is a mixture of the distribution of the hard tasks and the
distribution of the easy tasks, with mixture weights (0.5, 0.5). Let iH be the index of a task drawn
from the distribution of hard tasks, and iE be the index of a task drawn from the distribution of easy
tasks. Then using the law of total expectation, the excess risk for MAML in the l-th dimension can
then be written as:
Em(l) (wMAML)
_1	(Ei0 [%0,ι (W0,ι- w"八
=2Ei	qi,l I	ET所	)
…1	,2 Ei
8Ei [qi,l]
qi,lEiH haH (WiH,l - wi,l)i + qi,lEiΕ haE (WE,l
+ 2qi,ι EiH aHH (WiH ,ι — Wij E EiEbE (WE ,ι
-w,l)]2
-W*,ι)i
16E；q JEiH HHEiH haH (WiH,1 — WiH,ι)i + HHEiEhaE (WiE,1 — WiH,ι)i
+ 2aHEi0H aH Wii0H,l - WiiH,l	Ei0E aE Wii0E,l - WiiH,l
+ 16E	1q『EiE	HEEiH haE	(WiH,1 — WiE,1)i	+ HEEiE	haE	(WiE,1 —	WiE,1)i
+ 2aEEi0H haH (Wii0H,1 —WiiE,1i Ei0E haE (Wii0E,1 —WiiE,1i
16叫[.]2 (aH th + HH HE (rH + R2) + 2aH HE th
+ aE a2H (rE + R2) + a3E rE + 2a2E aH rE
(23)
4(H―+ H )2 (HHTH + HHHE(th + R2) + 2aHHETH
+ HE H2H (tE + R2) + H3E tE + 2H2E HH tE
15
Under review as a conference paper at ICLR 2022
where (23) follows by the properties of the distributions of the hard and easy tasks. Likewise, for
NAL we have
Em(l)(wNAL)
2 Ei
(Eio [λi,,l (wi0,1- wi,，D1
qi,l I	Ei0 [λi0,ι]	J
1
16Ei [λi,ι]2
EiH aHEiH [ph (WX
H,
I- WiH,l)i + aHEiEhPE (WE,l- WiH,l)i
+ 2aH Ei0H	ρH	Wii0H,l - WiiH,l	Ei0E	ρE	Wii0E,l - WiiH,l
+ 16E；，]2 EiE aEEiH [pε (WiH,1- wiE,ι)i + aEEiEhPE (WiE,1- wiE,ι)i
+ 2aEEi0H	aH	Wii0H,l - WiiE,l	Ei0E	aE	Wii0E,l - WiiE,l
1
4(aE + a H )2
aH P2H rH + aH P2E (rH + R2) + 2aHPHPErH+
aE P2H (rE + R2 ) + aE P2E rE + 2aE PH PE
Note that in the this setting the excess risks are symmetric across dimension. Thus multiplying by d
completes the proof.
A.3 Proof of Corollary 2
For the case that α = 1/pe and PH (1 — PH)2》-m, We can compare MAML and NAL by the
weights that are placed on rH, rE, rH + R2 and rE + R2 in their excess risks. Using the fact that
α = 1/PE, the expressions in (21) and (22) can be simplified as
aE = ≡>E
m
aH = PH(I - pH)2 +
PE
(d + I)PH
mρE
(24)
Under the assumption that PH(1 — PH)2》*,the bias term PH(1 — PH)2 dominates the gradient
variance term (d+m)PE, thus a，H dominates。石.As a result, we can ignore。石 in the expressions.
Furthermore, PH(1 — PH)2 dominates ("^；2。笈 within the expression for a，H, meaning we can also
drop the latter term. Thus we have
da3	dPH (1 — PH )2
Em(WMAML) ≈ 而HrH =	4 PE 『H
H
(25)
and
Em (WNi AL) ≈
daH P2H + 2daH PE PH
4(PH + PE)2
dPH (1 — PE )2
——rH +
+	daHPE	(	+ R2)
rH + 4(ph + PE)2(rH + R )
dPH PE (1 — PE )2fi,2
-K----:---3-R
4(PH +PE)2
(26)
16
Under review as a conference paper at ICLR 2022
Dividing (26) by (25) yields
EmwNAL ≈ ddPH(1 - PE)2r + dPHPE(1 - PE)2 必！ ,(dPH(1 - PE)2r !
Em(WMAML)	\	4 H	4(PH + PE )2	) / \	4 H)
="+ (PH‰ R2) /rH
= 1+(PH +EPE)2 RH
R2
≈ 1 + F	(27)
rH2
2
where the last approximation follows since 1 ≤ (PE；：H)2 ≤ 1.
B	Proof of Theorem 1
In this section we prove Theorem 1. We first show a general result.
Lemma 1. Let WMAML∈ RD be a stationary point of the MAML objective 1 with m = ∞ whose
Hessians of the task loss functions are bounded and positive definite, i.e. WMAMLsatisfies
β1lD W V2f1(wMAML) W L1Id, β2lD W %?f2(wmaml) W L2ID	(28)
for some L1 ≥ β1 > 0 and L2 ≥ β2 > 0. Suppose that β2 > β1. Define W1 := WMAML-
αVf1(wMAML) and w2 := wMAML- αVf2 (wMAML). Then the following holds for any α <
1/ max(L1, L2):
kVfι(wι)k2.1-αβ2kVf2(w2)k + O(α2).	(29)
1 - αL1
Proof. For some vector wMAML ∈ RD, let H1(wMAML) := V2f1(wMAML) and
H2(wMAML) = V2f2(wMAML) and g1(wMAML) = Vf1(w1) and g2(wMAML) = Vf2(w2),
recalling that w1 = wMAML-αVf1(wMAML) and w2 = wMAML-αVf2 (wMAML).
Recall that the MAML objective in this case is given by
F∞(w) = 1 f1(w - αVfι(w)) + 2f2(w - αVf2(w)).	(30)
After setting the gradient of F∞ (.) equal to zero at wMAML, we find that any stationary point
wMAMLof the MAML objective must satisfy
(ID -αH1(wMAML))g1(w1) = -(ID - αH2(wMAML))g2(w2)	(31)
Since H2(wMAML) W L2Id andα ≤ 1/max(L1, L2), Id -αH2 (wMAML) is positive definite
with minimum eigenvalue at least 1 -αL2 > 0. Thus we have
(ID -αH2 (wMAML))-1(ID -αH1(wMAML))g1(w1) = -g2 (w2)	(32)
Taking the norm of both sides and using the fact that kAvk2 ≥ σmin(A)kvk2 for any matrix A and
vector v, along with the facts that β1ID W H1(wMAML) W L1ID and β2ID W H2 (wMAML) W
L2ID by assumption, yields
kg2(w2)k2 = k(ID -αH2 (wMAML))-1(ID -αH1(wMAML))g1(w1)k2
≥ σm-1ax(ID - αH2(wMAML))σmin(ID -αH1(wMAML))kg1(w1)k2
1 -αL1
≥ ---葭kgι(wι)k2
1 -αβ2
Next, note that using Taylor expansion,
g1(w1) = Vf1(wMAML) - αV2f1(wMAML)Vf1(wMAML) + O(α2)
= (ID -αH1(wMAML))g1(wMAML)
g2(w2) = Vf2(wMAML) - αV2f2(wMAML)Vf2(wMAML) + O(α2)
= (ID -αH2 (wMAML))g2 (wMAML)
(33)
(34)
(35)
17
Under review as a conference paper at ICLR 2022
⑶	(b)	(C)	(d)
Figure 5: Task loss functions f1, f2, f3, f4 for task environment in Figure 4 (a-b).
Therefore
kg2(wMAML)k2 + O(α2)
& σm-a1x(ID - αH2 (wMAML))σmin (ID - αH1 (wMAML))
1 - αL1
1 - αβ2
kg1(w1)k2
(1 - aL1 y
V - αβ2 J
kg1(wMAML)k2,
(36)
&
i.e.,
2
(⅛⅛) kVf2(wMAML)k2 + O(α2).
□
Now we are ready to prove Theorem 1.
Proof. We first show that βiIχd W V2fi(W) W LiINd for all W ∈ Si for i ∈ {1,2}, where
▽2fi(W) ∈ RNd×Nd is the Hessian of the loss of the vectorized W and each βi and Li is defined
in Theorem 1. In other words, we will show that each fi is βi-strongly convex and Li -smooth within
Si. This will show that any stationary point WMAML that lies within S1 ∩ S2 satisfies the conditions
of Lemma 1.
For any i ∈ {1, 2} and for any W ∈ Rd×N we have by Weyl’s Inequality
V2fi(Wi,*) - (V2fi(W) - V2fi(Wi,*)k2INd
W V2fi(W) W V2f1(Wi,*) + kV2fi(W) - V2fi(Wi,*)k2INd	(37)
Thus, we will control the spectrum of V2fi(W) by controlling the spectrum of V2fi(Wi *) and by
upper bounding kV2fi(W) - V2fi(Wi,*)∣∣2.	’
To control the spectrum of V2fi(Wi,*) we use Lemma D.3 from Zhong et al. (2017), which shows
that for absolute constants c1 and c2 and each of the possible σ functions listed in Theorem 1,
(c1∕(κ2λ))INd W V2f1(Wi,*) W (c2NsiX)lNd.	(38)
Next, we apply Lemma D.10 from Zhong et al. (2017), which likewise applies for all the mentioned
σ, to obtain
kV2fi(W)-V2fi(Wi,*)∣∣2 ≤ c3N2sC,1∣∣W - Wi,*∣∣2	(39)
for a constant c3. Next, for any W ∈ Si, we have ∣∣W 一 Wi,*∣∣2 = O(1∕(scc,ιλiK2N2)). Therefore,
by combining (37), (38) and (39), we obtain that βiINd W V2fi(W) W LiINd for all W ∈ Si.
Finally we apply Lemma 1 to complete the proof.	□
Note that the constant c used in Theorem 1 is the homogeneity constant for which σ satisfies Property
3.1 from Zhong et al. (2017), namely, 0 ≤ σ0 (z) ≤ a|z|c ∀z ∈ R.
18
Under review as a conference paper at ICLR 2022
0
MAML Loss Landscape - Sigmoid
5
0
5
0
5
0
5
0
(d)
100
10-1
Figure 6: Loss landscapes for NAL (a,c) and MAML (b,d) for two distinct task environments and
Sigmoid activation.
Table 3: Effect of up-weighting hard tasks for NAL.
NAL NAL, ν = 2 NAL, ν = 5 NAL, ν = 10 MAML
Avg. coord.	0.18 ±	.02	0.33 ± .02	0.61 ±	.03	0.87	± .03	1.58	± 0.01
Test error	0.78 ±	.11	0.64 ± .08	0.51 ±	04	0.39	± .05	0.26	± 0.10
C Additional Experiments and Details
C.1 Linear regression
In all of the linear regression experiments, to run SGD on the MAML and NAL objectives, we sample
one task from the corresponding environment on each iteration for 5,000 iterations. Each task has
n1 = 25 outer loop samples and varying n2 inner loop samples for MAML, and n = n1 + n2 samples
for NAL. We appropriately tuned the ‘meta-learning rates’, i.e. the learning rate with which wMAML
and wNAL are updated after each full iteration, and used n2/10000 for MAML and 0.025 for NAL.
After 5,000 iterations, the excess risks of the final iterates were estimated using 3,000 randomly
samples from the environment. We repeated this procedure ten times to obtain standard deviations.
We also ran an experiment to test whether up-weighting the hard tasks improves NAL performance,
in light of our observation that MAML achieves performance gain by initializing closer to the hard
task solutions. We use a similar environment as in Section 3.2. Tasks are 10-dimensional linear
regression problems with n2 = 25 inner loop samples and n1 = 500 outer loop samples, and noise
variance 0.01. To implement up-weighting for NAL, we introduce a parameter ν which is the ratio of
the weight placed on the hard tasks to the weight placed on the easy tasks within each batch of tasks.
We normalize the weights to sum to 1. For example, if a task batch consists of 6 hard tasks and 4
easy tasks, then NAL with V = 2 places a weight of ^ν+- = 8 on the hard task loss functions and
6V1+4 = ι6 on the easy task loss functions (as opposed to 击 on all tasks for standard NAL).
Easy tasks have hardness parameter ρE = 1 and optimal solution drawn from N (0d, Id), and hard
tasks have hardness parameter ρH = 0.1 and optimal solution drawn from N (21d, Id). We run NAL
and MAML for 4000 iterations and use a task-batch size of 10 tasks per iteration, sampling easy
and hard tasks with equal probability. We report the average coordinate value for the final solutions
wNAL and wMAML and their test error (averaged across randomly sampled hard and easy tasks),
plus or minus standard deviation over 5 independent random trials.
Note that average coordinate value closer to 2 means the solution is closer to optimal solutions of
the hard tasks, while closer to 0 means it is closer to the optimal solutions of the easy tasks. Indeed
we see that when NAL places more emphasis on the hard tasks, i.e. ν is large, its performance
correspondingly increases and approaches that of MAML. Indeed, it illustrates that MAML can be
interpreted as a reweighing of tasks based on their level of hardness for GD.
19
Under review as a conference paper at ICLR 2022
Figure 7: Logistic regression results in analogous setting to m = 500 column in Figure 2 (T = 2
tasks, d = 10 dimensions). Recall that ρH is the strong convexity parameter (data variance) for the
hard task, which determines its hardness, while the strong convexity parameter of the easy task is 1.
MAML again initializes closer to the harder task, and has smaller excess risk for appropriate ρH .
m = 50, n2 = 50
1.0-
0.5 -
0.0-
-0.5 -
-1.0 -
0.0	0.2	0.4	0.6	0.8	1.0
4.0-
3.5 -
3.0-
2.5 -
2.0-
1.5 -
1.0-
0.5 -
0.0-
0.0	0.2	0.4	0.6	0.8	1.0
Figure 8: Version of Figure 2 with corresponding empirical results, including 95% confidence
intervals. The hardness parameter ρH varies along the x-axis.
C.2 Logistic regression
We also experimented with logistic regression, please see Figure 7 for details.
C.3 One-layer neural networks
We approximate loss landscapes for two types of activations: Softplus (Figures 4 and 5) and Sigmoid
(Figure 6). To approximate each landscape, we sample Gaussian data as specified in Section 4 and
compute the corresponding empirical losses as in equation (3) for NAL and (4) for MAML. We use
n = 500 for NAL and n1 = 20 and τ = 25 for MAML in all cases. We use n2 = m = 250 for
Softplus and n2 = m = 80 for Sigmoid. Figure 6 shows that when the hard and easy task solutions
have similar centroids (R is small, as in subfigures (a)-(b)), then the NAL and MAML solutions are
close and achieve similar post-adaptation loss (b). On the other hand, if the centroids are spread
and the hard tasks are close (large R, small rH as in subfigures (c)-(d)), then the NAL and MAML
solutions are far apart and MAML obtains significantly smaller post-adaptation loss (d).
20
Under review as a conference paper at ICLR 2022
S etting		Train Tasks		Test Tasks	
rH	Alg	Easy	Hard	Easy	Hard
Large	MAML	99.2±.2	96.0±.6^^	98.0 ±.2	81.2 ±.3
Large	NAL- 1	69.4 ±.4	41.5 ±.3	57.8 ±.4	45.2 ±.8
Large	NAL-10	70.0 ± .8	45.3 ± .9	67.2 ±.3	47.9 ±.7
Small	MAML	99.2 ± .5	99.1 ± .2	98.1 ±.3	95.4 ±.3
Small	NAL- 1	69.2 ± .5	46.0 ± .6	55.8 ±.5	45.8 ± 1.0
Small	NAL-10	70.2 ± .6	44.0 ± .8	67.8 ±.8	48.9 ±.8
Table 4: Omniglot accuracies with 95% confidence intervals.
Setting	Train Tasks	Test Tasks
p	Alg.	Easy	Hard	Easy	Hard
0.99	NAL	78.2 ±.4	9.9 ± 1.0	74.8 ±.4	9.5 ± .6
	MAML	92.9 ±.3	50.1 ± 1.0	84.6 ±.4	21.7 ±.3
0.5	NAL	81.9 ±.3	9.5 ± 1.0	76.5 ±.4	8.6 ± .3
	MAML	93.6 ±.5	41.1 ± .3	84.3 ±.6	17.9 ±.4
0.01	NAL	82.4 ± 1.4	7.6 ± .2	76.9 ±.9	8.2 ± .4
	MAML	94.0 ± 1.2	15.7 ±.3	85.0 ±.2	11.8 ±.3
Table 5: FS-CIFAR100 accuracies with 95% confidence intervals.
C.4 Omniglot
The full version of Table 1, with error bounds, is given in Table 4.
We use the same 4-layer convolutional neural network architecture as in (Finn et al., 2017), using
code adapted from the code that implements in PyTorch the experiments in the paper (Antoniou et al.,
2018). We ran SGD on the MAML and NAL objectives using 10 target samples per class for MAML,
i.e. n1 = 5 × 10. Likewise, n = n1 +5 × n2 samples were used in each task to update wNAL on each
iteration, for n2 = 5 × K. Eight tasks were drawn on each iteration for a total of 20,000 iterations.
The outer-loop learning rate for MAML was tuned in {10-2, 10-3, 10-4, 10-5, 10-6} and selected
as 10-3. Similarly, the learning rate for NAL was tuned in {10-2, 10-3, 10-4, 10-5, 10-6} and
selected as 10-6. Both MAML and NAL used a task-specific adaptation (inner) learning rate of 10-1
as in Antoniou et al. (2018). To select the alphabets corresponding to hard tasks, we ran MAML on
tasks drawn from all 50 Omniglot alphabets, and took the 10 alphabets with the lowest accuracies.
The train/test split for the other (easy) alphabets was random among the remaining alphabets.
To compute the accuracies in Table 1, we randomly sample 500 tasks from the training classes and
500 from the testing classes, with easy tasks and hard tasks being chosen with equal probability, and
take the average accuracies after task-specific adaption from the fixed, fully trained model for each
set of sampled tasks. MAML uses 1 step of SGD for task-specific adaptation during both training
and testing, and NAL uses 1 for testing. The entire procedure was repeated 5 times with different
random seeds to compute the average accuracies in Table 1 and the confidence bounds in Figure 4.
C.5 FS-CIFAR100
First, we note a typo from the main body: we used 5x as many samples for task-specific adaptation
as stated in the main body for both easy and hard tasks, that is, (N, K) = (2, 50) for easy tasks and
(N, K ) = (20, 5) for hard tasks.
The full version of Table 2 with error bounds is given in Table 5.
We use the same CNN as for Omniglot but with a different number of input nodes and channels
to account for the larger-sized CIFAR images, which are 32-by-32 RGB images (Omniglot images
are 28-by-28 grayscale). We ran MAML and NAL (SGD on the MAML and NAL objectives,
respectively) using 10 target (outer loop) samples per class per task for MAML, and 15 samples per
class per task for NAL (recalling that NAL has no inner loop samples). Eight tasks were drawn on
21
Under review as a conference paper at ICLR 2022
each iteration for a total of 20,000 iterations. The outer-loop learning rate for MAML was tuned
in {10-2, 10-3, 10-4, 10-5, 10-6} and selected as 10-3. Similarly, the learning rate for NAL was
tuned in {10-2, 10-3, 10-4, 10-5,10-6} and selected as 10-6. Both MAML and NAL used a
task-specific adaptation (inner loop) learning rate of 10-1 as in Antoniou et al. (2018).
We trained for 10,000 iterations with a task batch size of 2, with hard tasks being chosen with
probability p and easy tasks with probability 1-p. As in the Omniglot experiment, after completing
training we randomly sample 500 tasks from the training classes and 500 from the testing classes, with
easy tasks and hard tasks being chosen with equal probability. We then take the average accuracies
after task-specific adaption from the fixed, fully trained model for each set of sampled tasks. NAL
uses 5 steps of task-specific adaptation for testing and MAML uses 1 for both training and testing.
The entire procedure was repeated 5 times with different random seeds to compute average accuracies
in Table 2 and the confidence bounds in Figure 5.
D Multi-task Linear Regression Convergence Results
We motivate our analysis of the NAL and MAML population-optimal solutions for multi-task linear
regression by showing that their respective empirical training solutions indeed converge to their
population-optimal values.
First note that the empirical training problem for NAL can be written as
1T
min" EkXi(W - wi,*) - zik2,	(40)
w∈Rd T
i=1
where the j-element of zi ∈ Rn contains the noise for the j-th sample for task Ti . Taking the
derivative with respect to W and setting it equal to zero yields that the NAL training solution is:
TT
WNAL = (X X>Xi)-1 X(X>XiW* + X>Zi),	(41)
i=1	i=1
assuming PiT=1 Xi> Xi is invertible. Similarly, using ziout and ziin to denote noise vectors, as well
as Q i,j = Pi,j (Xiut)τX0utPi,j where Pij = Id — 言(Xinj)τ Xj We have that the empirical
training problem for MAML is
1Tτ
mRd TTEEkXoutPij(w - W,,*) - Zout- nαXout(Xij)>zijk2,	(42)
w∈R	i=1 j=1	2
therefore
wMAML
T τ	-1 T τ
= (XXQij)-XX Qijw:+pij(Xout)τ zout - n2 P i,j(Xout )τ Xout (Xij) τ zij
i=1 j=1	i=1 j=1
(43)
To show that wNAL and wMAML indeed converge to their population-optimal values as
T, n, n1, τ → ∞, we first make the following regularity assumptions.
Assumption 1. There exists B > 0 s.t. kwi* k ≤ B ∀i.
Assumption 2. There exists β, L > 0 s.t. βId Σi LId ∀i.
Assumption 1 ensures that the task optimal solutions have bounded norm and Assumption 2 ensures
that the data covariances are positive definite with bounded spectral norm.
Remark 2. We would like to note that Gao & Sener (2020) achieve a similar results as our Theorem
2 and 3. However, we arrive at our results using distinct techniques from theirs. Moreover, our MAML
convergence result (Theorem 3) accounts for convergence over task instances to the population-
optimal solution for MAML when a finite number of samples are allowed for task-specific adaptation
(a stochastic gradient step), whereas the analogous result in Gao & Sener (2020) (Theorem 2) does
22
Under review as a conference paper at ICLR 2022
not: it assumes τ = 1 and shows convergence as n1 = n2 → ∞. Since their result relies on n2 → ∞,
it shows convergence to the population-optimal MAML solution when an infinite amount of samples
are allowed for the inner task-specific update, i.e. a full gradient step. Our dimension-dependence
is significantly worse, than theirs, which suggests the extra complexity of the MAML objective with
finite samples allowed for task-specific adaptation.
D.1 NAL convergence
DefineVar(Σi):= ∣∣Ei[(∑i - Ei，区，])2]k andVar(∑iWi,*) := ∣∣Ei[(∑iWi,* - Ei,[∑iowi,*])2]∣∣.
Theorem 2. (NAL Convergence) Under Assumptions 1 and 2, the distance of the NAL training
solution (41) to its population-optimal value (5) is bounded as
ll	* ll	(c√d + βqdKK log(200n) + KK log(200n)	piog(200T)L2B∖
kwNAL-WNALk2 ≤ I	β2√n	+ 齐 J
+ PVM∑iWi,*) log(200d) + LB P½iτ(∑i)log(200d)	(44)
β√T	β2√T	(
with probability at least 0.96, where K is the maximum sub-exponential norm of pair-
wise products of data and noise samples, for some absolute constants c, c0 , and n ≥
4 (cLV+qc L d+^cLMlog，20®! and τ > l2B2 (Var(∑i) + %r(∑iw*))∕9. Informally,
IIwNAL - wNAL∣2 ≤ O (√n + √Tj	(45)
with probability at least 1 — o(1), as long as n = Ω(d) and T = Ω( Var (∑i) + Var (∑iw*)), where
O and Ω exclude log factors.
Proof. We first introduce notation to capture the dependency of the empirical training solution on T
and n. In particular, for any T, n ≥ 1, we define
T	T	T	-1 T
wNAL:=(TnXX>Xi)-1 TnXX>Xiw* + (TnXχ>χi) TnXx>zi∙
i=1	i=1	i=1	i=1
We next fix the number of tasks T and define the asymptotic solution over T tasks as the number of
samples approaches infinity, i.e., n → ∞, namely we define
TT
w(NTA)*L := (XΣi)-1XΣiwi*.
Using the triangle inequality we have
(T,n)	*	(T,n)	(T)*	(T)*	*
kwNAL - wNAL k = kwNAL - wNAL + wNAL - wNAL k
≤ kw(NTA,nL) - wN(TA)*Lk + kwN(TA)*L -w*NALk	(46)
We will first bound the first term in (46), which we denote as θ = kwN(TA,nL) - w(NTA)*Lk for convenience.
For this part we implicitly condition on the choice of T training tasks to obtain a bound of the
form P(kθk ≥ |{Ti}i) ≤ 1 - δ. Since this holds for all {Ti}i, we will obtain the final result
P(kθk ≥ ) ≤ 1 - δ by the Law of Total Probability. We thus make the conditioning on {Ti}i
implicit for the rest of the analysis dealing with θ.
23
Under review as a conference paper at ICLR 2022
We use the triangle inequality to separate θ into a term dependent on the data variance and a term
dependent on the noise variance as follows:
T
X
i=1
θ
Tn (E TnX>XJ X>Xi- (T X ς)	Tsi w*
≤
+ (Tn Xχ>X)T( Tn XχX>zi
T
X
i=1
i1	i1
τ1n (£ τ1nXi0Xi) x>χi- (T X ς)	Tκi
一 *
Wi
F X X>Xi
(47)
where (47) follows from the triangle inequality. We analyze the two terms in (47) separately, starting
with the first term, which We denote by 机 namely
斤：
T
X
i=1
(48)
We define the matrix Ai for each i:
T X ς"!	T 'i,
i0=1
(49)
which implies that H := ∣∣ PT=I AiW* k. We proceed to bound the maximum singular value of Ai
with high probability. Note that if we define C as
1T
C = ψ- E Xi0 Xi0
Tn
i0=1
then Ai can be written as
Ai = CT (TnX>Xi - (Tn X X> Xi0! (T X Ei0)	TEi) = CTBi	(50)
where, defining Λ := T PT=1 £* for notational convenience,
Bi=(Tn X>Xi- (Tn X X> χij λ-1 T ς)
=(Tn x> “LA - (Tn X X> Xi)AT T ς)
=(T2n X XjXiATCi’ -	X X>Xi) ATEi
n i0=1	i0=1
=T2n X (X>XiAT Ei0 - X> Xi0 ATEi)
i0=1
(51)
24
Under review as a conference paper at ICLR 2022
Adding and subtracting terms, we have
1T
Bi/X
T2n
i0=1
Λ-1Σi0 - nΣiΛ-1Σi0 + nΣi0Λ-1Σi - Xi>0Xi0Λ-1Σi
1T
+ τ2 £ ( ∑iΛ-1∑io - ∑ioΛ-1∑i
i0=1
T(1 χ>χi—ς) A—1 (T x ς)+T x Qi，- n χ> Xi)AT &
+ TEiA-1 (T X %	X Ei) ATEi
i0=1	i0=1
T
=τ (n x>xi- ς) + T2 X (EiO- n x> Xi)ATEi
where the last line follows by the definition of A.
Next, define
Zi=(Ei-1 x>x) = 1 X (Ei-Xih)(X(h))>)
for all i = 1, ..., T, where xi(h) ∈ Rd is the h-th sample for the i-th task (the h-th row of the matrix
Xi). Using this expression we can write
Bi = T (-Zi + T X Zi0 ATEi)
Note that each Zi is the sum of n independent random matrices with mean zero.
Using Lemma 27 from Tripuraneni et al. (2020) with each ai = 1, we have that
P (IlZiIl ≤ cιλi,maχmax(C2(pd/n + ti∕n),c2(pd∕n + ti/n)2)) ≥ 1 - 2exp(-t2)
for any ti > 0, and for each i ∈ [n].
(52)
Next, considering the expressions for H, Ai, Bi, C, and Zi, We can write that
T
H = X Ai w；
i=1
T
X C-1 Bi wi；
i=1
T
C-1 XBiwi；
i=1
CT X T (-Zi + T X Zi0 ATEi) w；
i=1
Next replace A by its definition 1
1T
CT T X
i=1
1T
CT T X
i0=1
i0=1
PiT00=1 Ei00 to obtain
一	τ	、-1	.
-Zi+XZi0	Ei00	Ei
i0=1
；
wi；
-1 T
Zi0	-wi；0 +	Ei00 X Eiwi；
where in equation 54 we have swapped the summations. This implies
i0=1
1T
CT T X
i0=1
T
Ei)	X Ei(w； - w；o1|
-1 T
Ei	XEi(wi； -wi；0)
i=1
(53)
(54)
(55)
H
H
25
Under review as a conference paper at ICLR 2022
where (55) follows by the Cauchy-Schwarz and triangle inequalities. Using these inequalities and
Assumptions 1 and 2 We can further bound H as:
H ≤kc-1kT XXX kZi0 k
i0=1
≤kc-1k T XXX kZi0 k
i0=1
≤kc-1k T XXX kZi0 k
i0=1
T
X k∑i(wr - WQk
i=1
T
X k∑iklW- W k
i=1
2TLB
Next, by the dual Weyl inequality for Hermitian matrices, we have λma(PT=ι Σ, ≥
PT=I λmin (∑i). Thus by Assumption 2, we have λmin(P3 Σ, ≥ Tβ, so
TT
1 T	2LB T
H ≤ kC-1k T E kZik2LB∕β = kC-1k万 E kZik	(56)
By a union bound and Lemma 27 from Tripuraneni et al. (2020) with each ai = 1, the probability
that any
IlZill ≥ cιλmɑχ(∑i)max g(Pd∕n + ti∕n),c2(Pd∕n + ti∕n)2)	(57)
is at most 2 PiT=1 exp(-ti2). Thus, with probability at least 1 - 2 PiT=1 exp(-ti2)
C	1..2LB 二、 一 / , k ,、入 k ,
H ≤∣∣C 1k~rβ~ 罪 cιλmaχ(∑i) max (c2(v^d∕n + ti∕n),c2(v^d∕n + ti∕n)2)
Let t := maxi ti , then using Assumption 2 we have
1∙∙2L2B
H ≤ kC 1k ——ci max 卜2( VZdTn + t∕n),c2( VZdTn + t∕n)2)	(58)
with probability at least 1 - 2T exp (-t2) for some absolute constants c1 and c2 and any t > 0.
Next we bound kC-1k, where C is the random matrix
1T
C T X x>Xi
Tn
i=1
Using the dual Weyl inequality again, we have
(59)
(60)
λmin(C)
Next, using again using Lemma 27 from Tripuraneni et al. (2020) with each ai = 1, as well as Weyl’s
Inequality (Theorem 4.5.3 in Vershynin (2018)), we have
λmin (1 X>Xi) ≥ λmin (∑i) - ci ∣∣∑ik max 卜2(pd∕n + s∕n),c2(Pd∕n + t∕n)2)
≥ β — ciLmax (c2(pd∕n + t∕n), c2(pd∕n + t∕n)2)
=β — CL(PdPn + t∕n) =: φ	(61)
with probability at least1 - 2 exp (-t2 ) for any t > 0 and sufficiently large n such that φ > 0,
where ci, c2, and C are absolute constants (note that since L ≥ β, in order for φ to be positive n
must be such that c2( ，d∕n + t∕n) ≤ 1 assuming ci ≥ 1, so we can eliminate the maximization. In
26
Under review as a conference paper at ICLR 2022
particular, we must have n ≥
union bound over i, we have
cL√d+ √ c2 L2d+4βcLt
2
2
. Now combining (61) with (60) and using a
kC-1k = -— ≤ -ʒ.-------------=--------
λmin(C)	Pi β 一 cL(Pd∕n + s/n)
_	1
β — CL(Pdpn + t/n)
(62)
for any t > 0 and n sufficiently large, where c is an absolute constant, with probability at least
1 — 2T exp (—t2). Using (62) with (58), and noting that both inequalities are implied by the same
event so no union bound is necessary, and n ≥
(cL√d+√c2L2d+4βcLt \	广「∙ j ι	1
I ------------------J sufficiently large, We have
H ≤	C(Pdln + t/n)	L2B
一 β — cL(pd∕n + t/n) B
(63)
With probability at least 1 — 2T exp (—t2) for some absolute constants c and c0 and any t > 0.
So far, We derived an upper bound for the first term in (47) Which We denoted by H. Next,
We consider the second term of (47), Which is due to the effect on the additive noise on
the empirical solution. To be more precise, We proceed to provide an upper bound for
PT=I X>Xi)	(PT=I TnX>Zi) U . Using the Cauchy-Schwarz inequality, we can bound
this term as
T X x>Xi
i=1
-1
≤ U(TX n X>Xi
TX x>zi) U
TUU τX n x>zi
TU
≤kC-1kT X -X>Zi
i=1
(64)
We have already bounded IlCTl∣, so we proceed to bound the term T PT=I k nnX>Zi∣∣. Note that
each element of the vector Xi>zi is the sum of products of a Gaussian random variable with another
Gaussian random variable. Namely, denoting the (h, s)-th element of the matrix Xi as xi(h)(s) and
the s-th element of the vector zi as zi(s), then the s-th element of Xi>zi isPsd=1 xi(h) (s)zi(s). The
products xi(h) (s)zi(s) are each sub-exponential, since the products of subgaussian random variables
is sub-exponential (Lemma 2.7.7 in Vershynin (2018)), have mean zero, and are independent from
each other. Thus by the Bernstein Inequality,
P
xi(h)(s)zi(s)
≥b
00	b2	b
≤2exp --c min(Pd:1KS, mxsκs)
(65)
for some absolute constant c00 and any b > 0, where Ks is the sub-exponential norm of the random
variable xi(h)(s)zi(s) (for any h, since the above random variables indexed by h are i.i.d.). Define
K := maxs Ks Using a union bound over h ∈ [n], we have
P kX>Zik≥ b)= P (X (Xx(h)(s)zi(s)! ≥ nb2
≤ X P X xi(h) (s)zi(s) ≥ b
≤ 2n exp —c00 min
b2 b
dκ,K
27
Under review as a conference paper at ICLR 2022
for any b > 0. Thus We have §∣∣X>Zik ≤ b∕√n with probability at least 1 -
2n exp (—C0 min(呆,K)). Combining this result with (64) with (47) and (63), we obtain
θ≤
c0(pd∕n + t/n) L2 Bb	1
β — cL(pd/n + t∕n) β √n β — CL(PdPn + t∕n)
1	C c0 √d + βb tL2B
β — cL(pd/n + t∕n) I β√n βn
(66)
with probability at least
1 — 2T exp (—t2) — 2n exp
—c00 min
(67)
for any t,b > 0 and n ≥ (。2+山2片+4产吆)2.
Now that we have bounds for the terms in (47), we proceed to bound the second term in (46). We
have
IlwTTAL - WNALk
TlG XX ∑i) T XX ∑iWi — Ei[∑i]-1Ei[∑iW*]
T i=1	T i=1
=∣∣G X ς)-1 TX E-
i=1	i=1
1 T -1
T ∑∑i	Ei[∑iw"
i=1
1 T -1
+ (T E∑i)	Ei[∑iW*] — Ei[∑i]-1Ei[∑iW*]
i=1
≤∣(TXς) 1TX£iW；—(Xς) 七∑iW"∣
+
1 T -1
T £ Σi)	Ei[∑iW" — Ei[∑i]-1Ei[∑iW*]
i=1
1 T -1
T ∑∑i	— Ei[∑i]-1
T i=1
∣Ei[∑iW"k
ι T E £iW； — Ei[£iW"
β lT
i=1
+ ∣( T X Σi) 1 - Ei[∑i]-1
LB
(68)
(69)
(70)
+
1
T
where in equations (68) and (69) we have used the triangle and Cauchy-Schwarz inequalities,
respectively, and in (70) we have used the dual Weyl’s inequality and Assumption 2. We first consider
the second term in (70). We can bound this term as
1 T -1
T ∑∑i	— Ei[∑i]-1
i=1
1
≤ β2
1 T -1	1 T	l
T ∑ ∑i)	b ∑(∑i — Ei0[Σi0]))Ei[∑i]-1
TX∑) JiTX(∑i-Ei，[∑io])∣Ei[∑i]-1
l1T	l
T 工(总-Ei0 [Ei0D
(71)
28
Under review as a conference paper at ICLR 2022
using Assumptions 1 and 2.
We use the matrix Bernstein inequality (Theorem 6.5 in TroPP (2015)) to bound ∣∣ 1 PT=1(∑i -
Ei0 [Σi0])k, noting that each Σi - Ei0 [Σi0] is an iid matrix with mean zero. Recall that Var(Σi) :=
∣Ei[(Σi - Ei0 [Σi0])2]∣. By matrix Bernstein and equation (71) we obtain
1 T -1
T ∑∑i	- Ei[∑i]-1
T i=1
≤
1 δ Var(Σi )
β
M	Tt
(72)
with Probability at least 1 - 2d exp
(-δ2 Var(Σi*), for any δ > 0. Similarly, we have that
∖	1 + LO/(3V T ) J
T
β∣τXK^-Ei[&w』≤ β	ar√τi,*
with probability at least 1 - 2d exp (- ；；Vr("(Wrf2 )
Thus (70) reduces to:
Il(T)*	*	∣∣∕ 1 δ Var(∑iWi,*)	LB δ Var(∑i)
kwNAL - WNALk ≤ β —√ — + 铲 √
with PrObabilityatleast 1 - 2d exp (- δ+Vδ∕⅞√⅞) - 2d exp (-Utxiwr/2).
We combine this result with (47) and (66) via a union bound to obtain
ll	*	1	fc0√d + βb ɪ tL2B
kwNAL - wNALk ≤ β - cL(p/n + t/n)[ β√ + ~jn
+ δ Var(∑iWi,*) + δ Var(LB∑i)
β√T	~β 2√T-
(73)
(74)
(75)
with Probability at least
1 - 2T exp(-t2 ) - 2n exp -c00 min
也 2口- 2deχp (- δ2 Var(Ki)/2
dK K)) PI 1 + Lδ∕(3√T)
- 2d exp
(- δ2 Var(∑iW*)∕2 )
C1 + LBδ∕(3√T))
(76)
as |on„ as n ≥	(cL^ZcLd+"*Lt Y
as long as In ^≥	∣	2 β	).
Finally, choose t = y4og(200T), b
J竽 log(200n) + -K log(200n) and δ = (PVar(∑i) + Var(∑iW*))-1 log(200d) and re-
strict n ≥ 4 (cL√d+√c2L2"+4"-") such that ------------1==----- ≤ 2 and T >
一 ∖	2β	J	β-cL( √d∕n+t∕n)	一 β
L2B2 log(200d)/(9(Var(∑i) + Var(∑iW*))). This ensures that each negative term in the high
probability bound (124) is at most 0.01 and thereby completes the proof.	□
29
Under review as a conference paper at ICLR 2022
D.2 MAML convergence
We first state and prove the following lemma.
Lemma 2. Let A be a fixed symmetric matrix in Rd×d, and let X be a random matrix in Rn×d
whose rows are i.i.d. multivariate Gaussian random vectors with mean 0 and diagonal covariance Σ.
Then
EX 1χX>X)A 1χX>x)] = ∑A∑ + 1 (tr(∑A)Id + ΣA) Σ
(77)
Proof. Letting xk denote the k-th row ofX, we have
EX (1 X>X)A (1 X>X)
=EX ](ι X χ>χk!A
n
这x> χk)l
1 n n
Exkl n2∑ E	χ>χkAχ>0χk01 + Ex
k=1 k0=1,k0 6=k
n-1	1 n
~n~ ςaς + n EExk [χ>χkAX>χk ]
n	n k=1
1n
n Eχ>χk Aχ>χk
k=1
(78)
k
Let Ck = χk>χkAχk>χk for k ∈ [n], and let λi be the i-th diagonal element of Σ for i ∈ [d]. Then
for any k, using the fact that the elements of χk are independent, have all odd moments equal to 0,
and have fourth moment equal to 3λi2 for the corresponding i, it follows that
E[C ] = (λr Pj=1 λj aj,j + 2λr2Ar,r ,
λr λs (Ar,s + As,r),
if r = s
otherwise
(79)
Using (79), we can write
E[Ck] = tr(ΣA)Σ + Σ(A+ A>)Σ
= tr(ΣA)Σ + 2ΣAΣ	(80)
where (80) follows by the symmetry of A. Plugging (80) into (78) completes the proof.	□
Now we have the main convergence result. Analogously to Theorem 2, we define Var(Qi) :=
∣∣Ei[(Qi - Ei0[Qi0])2]k2 andVar(QiWi,*):=IlEi[(QiWi,* - Ei，[Qi，Wi,*])2]∣∣2.
Theorem 3. (MAML Convergence, General Statement) Define β := β(1 — αL)2 + α 力]；+1) and
L := L(1 — αβ)2 + α LJ，+1) ∙ If Assumptions 1 and 2 hold, the distance of the MAML training
solution (4) to its population-optimal value (5) is bounded as
*	16LBd Jy log3(100Td)
IlWMAML - wMAMLIl ≤	加 L
β2 τ
ι VaVar(QiW*)log(200d)	LB^rar(Qi) log(200d)
+	β√T	+	β2√T
(81)
with probability at least 0.96, for some absolute constants c and c0, and any τ >
32d3α4L6 log6(100Td)∕(cβ) and T > L2B2 log(200d)∕(9(Var(Qi) + Var(QiW*))).
Proof. The proof follows the same form as the proof of Theorem 2. Here the empirical covariance
matrices JX>Xi and their means ∑i are replaced by the empirical preconditioned covariance
matrices Qi,j and their means Qi, respectively. As before, a critical aspect of the proof will be to
30
Under review as a conference paper at ICLR 2022
show concentration of the empirical (preconditioned) covariance matrix to its mean. To do so, we
re-define the perturbation matrices Zi :
1τ
Zi = ----ɪ2 Qij- Qi	(82)
τn1	,
j=1
where
Q^'	. — d> (outut ∖> voutŋ
i,j	:= Pi,j	(Xi,j	) Xi,j Pi,j
and
1τ
Qi := E —q i,j
τn1
1j =1
=(τn1 X Epj (XoUt)>χoUtPi,j)	(83)
=τn1 E E [(Id - nα2 (Xij)>χin∙)号(Id- nα2 (Xij)
1 τ	α2
=-E 0- 2α∑2 + -2 E [(χij)>χij (XoUt)>xout (Xij)>χij]
τn1 j=1	n2
2
= (Id - aE^EiUd - -ςJ +-----(tr(£2)£i + ς"	(84)
n2
where (84) follows by Lemma 2.
Note that here Zi has higher-order matrices than previously. Lemma 4 nevertheless gives the key
concentration result for the Zi ’s, which we will use later. For now, we argue as in Theorem 2: for any
T, τ ≥ 1, we define
w
(T,τ)
MAML
Tτ	Tτ
(T⅛ EE Qi,j )-1 T⅛ E E(Qij W+Pi,jX>jZout
i=1 j=1	i=1 j=1
--PijXLx，"X>jzij)∙
…	.CΕ-CJ	. ..	Ε , I	I	(T )*
We next fix T and define the asymptotic solution over T tasks as τ → ∞, namely wMAML :=
(T PT=I Qi)- 11 PT=ι Qiw，Again using the triangle inequality We have
kW(T,τ)	W*	k kW(T,τ) W(T)* + W(T)* W*	k
kwMAML - wMAML k = kwMAML - wMAML + wMAML - wMAML k
≤ kw(MTA,τM) L - wM(TA)*ML k + kwM(TA)*ML - w*MAML k	(85)
The first term in (85) captures the error due to have limited samples per task during training, and the
second term captures the error from having limited tasks from the environment during training. We
first bound the first term in (85), Which We denote as θ = kw(MTA,τM) L - wM(TA)*MLk for convenience.
Note that by Assumption 2, We have
βId W Qi W LId	∀ i	(86)
where β := β(1 - -L)2 + α e/，+1) and L := L(1 一 αβ)2 + α Ln(d+1) ∙ Thus, Using the argument
from (47) to (56) in the proof of Theorem 2, with 1 X>-Xi,j replaced by + P；=i Qij and ∑i
replaced by Qi, we obtain
θ ≤ "+" E EQijT
TTnI E E (Pij(XoUt)>zout - nα2Pij(XoUt)>χout(Xij)>zij)[	(87)
31
Under review as a conference paper at ICLR 2022
where
χ[G X n⅛ X QjT Qij-(T X Q)IT Qi
i=1	i0=1	1 j=1	i0=1
Defining C = T PT=I n1τ PT=I Qiz,j, We have,
「一 1
X TCTQ
i=1
1 T	-11
!i,j - (T ΣS Qij T Qi
一 *
Wi
*
Wi*
T
≤kC-1k -Tβ X kZik
βi=1
(88)
少:
斤
T
Where to obtain the inequality We have sWapped the order of the summations as in (54). Next We
bound each kZi k With high probability, by first shoWing element-Wise convergence to 0.
Lemma 3. Consider a fixed i ∈ [T], k ∈ [d], and s ∈ [d], and let Qi,j (k, s) be the (k, s)-th element
of the matrix Qi,j defined in (93). The following concentration result holds for the random variable
T⅛ Pj=I Qij(k, S)Qij(k, S):
P ( I Tn- ^X Qij (k, S)- Qi(k, S)
(89)
for some any γ > 0, where Qi = E[Tn^ Pj=ι Qij] as defined in e.g. (83).
Proof. We start by computing the (k, S)-th element of Zi. First We compute the (k, l)-th element
of the matrix Dij := (Id - nα(XinjyXij)(Xout)TXout. Here, we define x(rj ∈ Rn 1 as the
n1 -dimensional vector of the r-th-dimensional elements from all of the n1 outer samples for the j-th
(k)
instance of task i, and xi,j ∈ Rn2 as the n2-dimensional vector of the k-th-dιmensional elements
from all of the n2 outer samples for task i. Then we have
d
Dij (k,l) = X(I r=k - n hxkj ,明加牖，x(ji
r=1	n2
d
_ R (k) Y(Di _X α 峡(k) <>(r)ih (r) (l) i	(90)
=hXij，Xi,ji -	n hxi,j，Xi,j ihxij，Xi,ji	(90)
r=1 n2
Then, the (k, S)-th element of Qi,j is
Qi,j (k, S)
= Pi>,jXi>,jXi,jPi,j (k, S)
d
=XDij(h,i)(iI=S- αhx(j,X(Sj)i)
l=1	n2
dd
=Xd I=S - n2 hx(j, Xis)i) Gk), Xil)i- X n2 婚,X 曲成j, Xj
=(hx(5, x(j)i - X n 嫣,X(r)ihxi3, Xij)〉!
dd
-n Xhx(j, x(j)i 2, x(ji -X n2 hx(,j), &j,hXj Xiji)
d	2d d
=Mj Xij)i - If XhXj XK Xij)i + W XXhXij, jj x(r)ihx(r;, x(ji
2 r=1	2 l=1 r=1
(91)
32
Under review as a conference paper at ICLR 2022
Therefore the (k,s) element of / PJ=1 Qi,j is
d
χ(k) x(s))_ 2α Xh玄(k)文(r)ihX(r) x(§)i
Xi,j , Xij i- n^ I^hXij , Xij ihxi,j , Xij i
n2 r=1
2d d
+ % XXhX(j, XjXj XjXj X(jii	(92)
2 l=1 r=1
As we can see, this is a polynomial in the independent Gaussian random variables
{xir)(q)}j∈[τ],r∈[d],q∈[n2] and {x(r)(q)}j∈[τ],r∈[d],q∈[nι], for a total of dτ(n1 + n2) random vari-
τ
ables. Moreover,房 Zj=ι Qij (k, S) is the average of T i.i.d. random variables indexed by j.
Define these random variables as uj forj ∈ [τ], i.e.,
Uj- hX(,j), X(j)i-% X hX(,j), Xjxj Xj
n1	n1 n2 r=1
2dd
+ 袅 XXXj, XjXj 端乂Xj X(ji
n1 n2 l=1 r=1
such that
1 τ	1τ
为 EQ ij (k,s) = - Euj
τn1 j =1	τ j =1
Then the variance of Tn^ Pj=I Qij (k, S) decreases linearly with T, since
Var( Tn X Qij的s)=E]( Tn7 X Qij (k,s) - Qi(k,s) j
1τ 1τ
=E[(T Euj- T ∑E[uj])2]
1τ
=T2 EE[(uj - E[uj])]
T j=1
1τ τ
+ T2 ∑S	E(	E[(uj - E[uj])]E[(uj0 - E[uj])]
j=1 j0=1,j06=j
=1 E[(uι - E[u1])2]
T
≤ TE[u2]
(93)
(94)
(95)
where (95) follows from the independence of the uj ’s. Next, recall the definition of u1 given in (93):
uι = n1 h淄,X(SI) i-n2⅛ X hX(,% XfMXi?, XisI)i
n1	n1 n2
r=1
2 dd
+% X XhX(I)，XisI)MX% XfMXi?, Xii) i	(96)
n1n2 l=1 r=1
The second moment of u1 is dominated by the higher-order terms in u21 , since these terms have
the largest expectation (being of the highest order) and are the most populous. To see that they are
the most populous, note that u1 has d2n1 n22 monomials with six random variables, and only dn1 n2
monomials with four random variables and n1 monomials with two random variables. Thus E[u12]
is at most a constant times the expectation of the d4n21 n42 monomials of 12 variables in u12. Each of
33
Under review as a conference paper at ICLR 2022
these monomials in 12 variables is the product of 12 one-dimensional, mean-zero Gaussian random
variables, 8 corresponding to inner-loop samples and 4 corresponding to outer-loop samples. The
maximum expected value of each of these monomials is thus the eighth moment of the inner loop
random variable with maximum variance times the fourth moment of the outer-loop random variable
with maximum variance. Since the variables are Gaussian with maximum variance L, the maximum
expected value of each monomial is 105L4 × 3L2 = 315L6. This yields our upper bound on the
variance. The following analysis formalizes this argument:
Var (+X Qi,j (k,s)
4	n1	n1	d d d d
Ca E XXXXXXxc(h)x⑷(h)hx⑷爻(S)ihx(k) x(r)i
n2 n4τ E L L L χ-j2h) x, xi,j (h)xi,j (h)hxi,j, xi,j ihxi,j, xi,j i
n1n2τ	h=1h0=1 l=1 r=1 l0=1 r0=1
4	n1 n1 d d d d	n2
Ca E X X XXXX Tm(h)T(I(h) X^l((J)》(S)((J)
n2n4τ E	乙乙乙	χ-j2-)	χ-j xi,j	(h)xi,j (h)	2^xi,j	(q)xi,j (q)
n1n2τ	h=1h0=1 l=1	r=1 l0=1	r0=1	q=1
n2	n2	n2
X XS)⑷Xirj)⑷点) ⑺x(lj) ⑺	X xi,j) (q)x(sj	⑷	X	XS)⑷xij)	⑷
q=1	q=1	q=1
(97)
By independence, each term in the above summation has expectation zero unless (i) r = l and r0 = l0,
or (ii) r = r0, l = l0 and h = h0. There are n21d2 distinct values of (h, h0, r, r0, l, l0) that satisfy (i),
and n1d2 ≤ n12d2 distinct values of (h, h0, r, r0, l, l0) that satisfy (ii). For simplicity we treat each
term in the summations over n2 as having non-zero mean, although this can tightened. Since the
expectation of each non-zero-mean monomial is at most 315L6, we have
1 τ	730Cd2a4
Var I τnι 毕ij (k,s) J ≤ —T—
(98)
for some absolute constant c. With this bound on Var(圭 Pj=ι Qi,j(k, s)), the result directly
follows fromTheorem 1.9in Schudy & Sviridenko (2012), noting that 圭 P；=i Qij(k, s)-Qi(k, S)
is a degree 6 polynomial in centered independent Gaussian random variables. Note that by the
argument in Schudy & Sviridenko (2012), the bound on the elements of Zi given in Lemma 3 is tight
up to logarithmic factors.	□
Next, we show that this result implies a high probability bound on each kZi k.
Lemma 4. For all task indices i, any γ > 0, and some absolute constant C, the following holds:
P(kZik ≤γ) ≥1 - e2d2 exp
2
τγ2
ca4d2L6
(99)
34
Under review as a conference paper at ICLR 2022
Proof. By Lemma 3, we have a high-probability bound on the size of the elements of Zi.
Using this bound, we can bound the spectral norm of Zi using the Gershgorin Circle Theorem, which
says that every eigenvalue of Zi lies within one of the Gershgorin disks Dk defined by
Dk ：= {λ : λ ∈ [Zi(k, k) - Rk, Zi(k, k) + Rk]}	(100)
where Rk = Psd=1,s6=k |Zi(k, s)|. This implies that
kZik ≤maxZi(k,k)+Rk	(101)
k∈d
so we attempt to bound the RHS of the above. By a union bound over k, we have that for any γ > 0,
d
≤ X e2 exp
s=1,s6=k
_ ,_ , _ , _ , ,. ~ 、 _ ,_ 、、
P (Rk ≥ (d - 1)γ) ≤ P(∪s=k{∣Qi(k, S)- Q(k, s)| ≥ γ})
τγ2	γ∕6!
ca4d2L6))
=(d - 1)e2exP (~ (
cα4d2L6
(102)
where (102) follows from Lemma 3. We also have that
P (|Zi(k, k)| ≥ γ) ≤ e2 exp
（-（高丁）
(103)
again using Lemma 3. Combining (102) and (103) via a union bound yields for some particular s
P(Zi(k,k)+Rk ≤γ) ≥P({Rk ≤ (d - 1)γ} ∩ {Zi(k, k) ≤γ})
=1-P({Rk ≥ (d-1)γ}∪{Z(k,k) ≥γ})
≥ 1-P({Rk ≥ (d - 1)γ}) - P ({Z(k, k) ≥γ})
τγ2 Y/6!
ca4d2L6))
(104)
Therefore, via a union bound over k, we have
P max{Zi(k, k) + Rk} ≤ Y) = 1 - P (∪k∈[d] {Zi (k, k) + Rk ≥ Y})
k∈[d]
≥ 1-e2d2exp (- (c0‰
(105)
for any γ > 0. As a result, using (101) we have
P(kZik ≤γ) ≥ 1-e2d2exp
2
τγ2
cα4d2L6
(106)
□
We continue with the proof of Theorem 3 by bounding kC-1 k. Using an analogous argument as in
the proof of Theorem 2 based on Weyl’s Inequality (Theorem 4.5.3 in (Vershynin, 2018)), we have
1T
λmin(C) ≥ λmin(T Qi) -k Zi k
T i=1
ʌ
≥ β - Y
(107)
where (107) follows by (86) and Lemma 4, with high probability, for any Y > 0. Thus choosing
一 A .
Y < β , we have
kC-1k≤ ；	(108)
β-Y
35
Under review as a conference paper at ICLR 2022
Using this with (88) and Lemma 4, and using a union bound over i ∈ [T], we have
2	2LBγ 1
P H ≥
∖ 一 β β-Y
(109)
Next, we turn to bounding the second term in (87), which is the error term due to the additive noise in
the linear regression setting. We will do this by again making an argument based on the concentration
of a polynomial in independent, centered Gaussian random variables around its mean. First, note that
the term we must bound is
1 Tτ
i⅛ XX Pi,j (Xi,j ) Xi,j Pi,j
n1τ i=1 j =1
-1
(Tn1τ X X ' XtFt- n ' (Xout)>χout(Xij)T Zij)I
≤∣ TnIT (X X % (Xout)Ku"，1
II 1 T τ
T- XX Pij (XiU干 ZoUt- £ Pij (Xout)>χout(Xij)>zij
∣ Tn1 τ i=1 j =1	n2
Pi,j (Xout)>zout - nα Pi,j (Xout)>χout(χinj)>zinj
(110)
(111)
where (110) follows from the Cauchy-Schwarz Inequality. Define
g(k) ：= TnIT(X X Pij (Xout)>zout - n Pij (Xout)>χout (Xij)>zij) (k)
(112)
for each k ∈ [d], i.e. g(k) is the k-th element of the d-dimensional vector
(t⅛ PT=I PT=I Pi,j(Xout)>zout-急Pi,j(Xout)>Xout(Xij)>zij), and let g :=
[g(k)]1≤k≤d. Note that each g(k) is a degree-6 polynomial in independent, centered Gaus-
sian random variables. One can see that it is degree-6 because after expanding the Pi,j matrices,
there are five data matrices and a noise vector (six total matrices) in the highest-order product. Also
note that this polynomial has mean zero since the noise has mean zero and is independent of the data.
Its variance E[(g(k) - E[g(k)])2] can be upper bounded using a similar argument as in Lemma 3.
We will not write the full calculations and argument since they are very similar to those in Lemma 3.
We again use the fact that the polynomial in question is the sum of n1T i.i.d. random variables to
obtain variance decreasing linearly in n1T. The differences are that in the highest-order monomial of
g(k)2, there are there are six inner-loop samples, four outer-loop samples, and two noise samples,
for a maximum expectation of 15L3 × 3L2 × σ2. There are T2T2n21d4n42 of these terms, but as
before, only T2Td4n21n42 of these terms have nonzero expectation due to independence, and the n21n42
coefficient cancels. Thus, the variance is upper bounded as
α4d2PiT 1Pτ 1 L3σ2
Var(g(k)) ≤C-⅛j—
α4d2L5σ2
C---------
T
(113)
for some absolute constant C, which implies, via Theorem 1.9 in Schudy & Sviridenko (2012), that
P (|g(k)| ≥ b) ≤ e2 exp
（-（二厂）
(114)
thus we have via a union bound over k ∈ [d],
P (kgk ≤ √db) ≥ 1 一 e2dexp
τb2
ca4 d2L5σ2
(115)
36
Under review as a conference paper at ICLR 2022
for any b > 0. Combining this with (111) via a union bound, we have that
T τ	-1
Pi,j(Xio,ujt)>Xio,ujtPi,j
1 Tτ
T— XXPi,j(Xout)>zθ,ut - £Pi,j(Xout)>χo,Ut(Xij)>zij
Tn1τ i=1 j=1	n2
< √db
一 β - Y
with probability at least
(116)
≥ 1 - edexP (一 ( c'jd )	! - e2Td2 exP (一 ( (XTpL6 )	!
cα d L σ	cα d L
(117)
This completes the bound on θ. Next, We must bound ∣IwMTAfML - WMAMbk- We have
(T)f	f
kwMAML - wMAML k
T X Qi)	T X QiWf- Ei [Qi]-1Ei [Qiwf]
i=1	i=1
T X Qi)	T X QiWf- (T X Qi)	Ei[Qiwf ]
+
-1
Ei [QiWif] - Ei [Qi]-1Ei [QiWif]
1 T -1 1 T	T
T X Qi)	T X QiWf-(X Qi)TEi[QiWf ]
i=1	i=1	i=1
+
-1
Ei [QiWif] - Ei [Qi]-1Ei [QiWif]
(118)
(T
X Qi) HU T X Qiwf - Ei[QiWf] +
- Ei [Qi]-1
kEi [QiWif]k
(119)
≤
≤
11T
^h T EQiWi - Ei[QiWi]	+
β	i=1
(120)
Where in equations (118) and (119) We have used the triangle and Cauchy-SchWarz inequalities,
respectively, and in (120) We have used the dual Weyl’s inequality and Assumption 2. We first
consider the second term in (120). Continuing to argue as in the proof of Theorem 2, We have
kW(MTA)fML - WMf AMLk ≤
1T
T E QiWf- Ei[QiWf]
T i=1
ʌ
LB
+--X-
β2
1T
T E Qi - Ei[Qi]
T i=1
(121)
1
β
As in the proof of Theorem 2, Will use the matrix Bernstein inequality (Theorem 6.1.1 in Tropp (2015))
to upper bound ∣∣T PT=I QiWf — Ei[QiWf]∣ and ∣∣T PT= Qi- Ei[Qi]∣ with high probability.
Doing so yields
∣WM(TA)fML - WMf AML∣ ≤
1 δ Var(QiWf)	LB δ Var(Qi)
β^	√T	+ 下一√T
(122)
with probability at least 1 - 2d exp (--δ 1+LQ∕iw√k ) - 2d exp (- 1fLB(Q3蜂))for any δ > 0.
37
Under review as a conference paper at ICLR 2022
Combining (122) with our bound on θ (from (87), (109), and (116)) via a union bound and rescaling
Y = Ynew = Yold√τ yields
ʌ 一 _ , _ _、 ʌ _ _ , 、
II	* I，. 2LBγ	1	, √db	1	. 1 δ Var(QiWji)	LB δ Var(Qi)
IlWERM - WERM k ≤ F~产  --------+= H------------+= + K---------+--------1 τ----T=----
μ√τ μ 一 γ∕√τ	√τ μ 一 γ∕√τ	β	Tt	β2	Tt
(123)
with probability at least
1 - e2Td2 exp (-(OfL11)- e2dexp (-(OdcLσ11)
- 2d exp
δ2 Var(QiWi,*)/2
1 + Lδ∕(3√T)
- 2d exp
δ2 Var(Qi)/2
1 + LBδ∕(3√T)
(124)
for some absolute constants C and C, and any b and δ > 0, and Y ∈ (0, √Tμ).
Finally, choose Y = 4d QaL log3(100Td), b = 2d J。4;：。2 log3(100d), and
δ = (PVar(Qi) + Var(QiW*))TPlog(200d) and restrict τ > 32d2a4L6 log6(100Td)∕(cβ) such
that	√7 ≤ 2 and T > L2B2 log(200d)∕(9(Var(Qi) + Var(QiW*))). This ensures that each
negative term in the high probability bound (124) is at most 0.01 and thereby completes the proof.
□
38