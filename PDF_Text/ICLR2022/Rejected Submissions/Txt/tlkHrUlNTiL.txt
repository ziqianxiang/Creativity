Under review as a conference paper at ICLR 2022
Disentangling deep neural networks with rec-
TIFIED LINEAR UNITS USING DUALITY
Anonymous authors
Paper under double-blind review
Ab stract
Despite their success deep neural networks (DNNs) are still largely considered
as black boxes. The main issue is that the linear and non-linear operations are
entangled in every layer, making it hard to interpret the hidden layer outputs.
In this paper, we look at DNNs with rectified linear units (ReLUs), and focus
on the gating property (‘on/off’ states) of the ReLUs. We extend the recently
developed dual view in which the computation is broken path-wise to show that
learning in the gates is more crucial, and learning the weights given the gates is
characterised analytically via the so called neural path kernel (NPK) which depends
on inputs and gates. In this paper, we present novel results to show that convolution
with global pooling and skip connection provide respectively rotational invariance
and ensemble structure to the NPK. To address ‘black box’-ness, we propose a
novel interpretable counterpart of DNNs with ReLUs namely deep linearly gated
networks (DLGN): the pre-activations to the gates are generated by a deep linear
network, and the gates are then applied as external masks to learn the weights in
a different network. The DLGN is not an alternative architecture per se, but a
disentanglement and an interpretable re-arrangement of the computations in a DNN
with ReLUs. The DLGN disentangles the computations into two ‘mathematically’
interpretable linearities (i) the ‘primal’ linearity between the input and the pre-
activations in the gating network and (ii) the ‘dual’ linearity in the path space in
the weights network characterised by the NPK. We compare the performance of
DNN, DGN and DLGN on CIFAR-10 and CIFAR-100 to show that, the DLGN
recovers more than 83.5% of the performance of state-of-the-art DNNs, i.e., while
entanglement in the DNNs enable their improved performance, the ‘disentangled
and interpretable’ computations in the DLGN recovers most part of the performance.
We conclude by identifying several interesting future directions based on DLGN.
1 Introduction
Despite their success deep neural networks (DNNs) are still largely considered as black boxes. The
main issue is that in each layer of a DNN, the linear computation, i.e., multiplication by the weight
matrix and the non-linear activations are entangled. Such entanglement has its pros and cons. The
commonly held view is that such entanglement is the key to success of DNNs, in that, it allows DNNs
to learn sophisticated structures in a layer-by-layer manner. However, in terms of interpretability, such
entanglement has an adverse effect: only the final layer is linear and amenable to a feature/weight
interpretation, and the hidden layers are non-interpretable due to the non-linearities.
Prior works (Jacot et al., 2018; Arora et al., 2019; Cao & Gu, 2019) showed that training an infinite
width DNN with gradient descent is equivalent to a kernel method with the so called neural tangent
kernel matrix. As a pure kernel method, the NTK matrix performed better than other pure kernel
methods. However, in relation to ‘black box’-ness, there are two issues with NTK theory: (i) Issue I:
Infinite width NTK matrix does not explain fully the success of DNNs because it was observed that
finite width DNNs outperform their infinite width NTK counterparts, and it was an open question to
understand this performance gap (Arora et al., 2019), and (ii) Issue II: Since the NTK is based on the
gradients, it does not offer further insights about the inner workings of DNNs even for infinite width.
A dual view for DNNs with rectified linear units (ReLUs) was recently developed by Lakshmi-
narayanan & Singh (2020) who exploited the gating property (i.e., ‘on/off’ states) of the ReLUs. The
1
Under review as a conference paper at ICLR 2022
Deep Gated Network
Deep Linearly Gated Network
Input
Input =1
Figure 1: DGN is a setup to understand the role of gating in DNNs with ReLUs. The DLGN setup completely
disentangles and re-arranges the computations in an interpretable manner. The surprising fact that a constant 1
input is given to weight network of DLGN is justified by theory and experiments in Sections 3.1 and 3.2.
dual view is essentially linearity in the path space, i.e., the output is the summation of path contri-
butions. While the weights in a path are the same for each input, whether or not a path contributes
to the output is entirely dictated by the gates in the path, which are ‘on/off’ based on the input. To
understand the role of the gates, a deep gated network (DGN) (see Figure 1) was used to disentangle
the learning in the gates from the learning in weights. In a DGN, the gates are generated (and learnt)
in a ‘gating network’ which is a DNN with ReLUs and are applied as external signals and the weights
are learnt in a ‘weight network’ consisting of gated linear units (GaLUs) (Fiat et al., 2019). Each
GaLU multiplies its pre-activation and the external gating signal. Using the DGN, two important
insights were provided: (i) learning in the gates is the most crucial for finite width networks to
outperform infinite width NTK; this addresses Issue I, and (ii) in the limit of infinite width, learning
the weights with fixed gates, the NTK is equal to (but for a scalar) a so called neural path kernel
(NPK) which is a kernel solely based on inputs and gates. This shifts Issue II on interpretability to
that of interpretability of the gates as opposed to interpretability of the gradients.
Our Contribution. We extend the dual view to address ‘black box’-ness by completely disentangling
the ‘gating network’ and the ‘weight network’. Our contributions are listed below.
•	Disentangling Gating Network. For this, we propose a novel Deep Linearly Gated Network
(DLGN) as a mathematically interpretable counterpart of a DNN with ReLUs (see Figure 1). In a
DLGN, the gating network is a deep linear network, i.e., there is disentanglement because of the
absence of non-linear activations. The gating network is mathematically interpretable, because, the
transformations from input to the pre-activations are entirely linear; we call this primal linearity.
•	Dual View (Section 3.1). We present an unnoticed insight in prior work on fully connected networks
that the NPK is a product kernel and is invariant to layer permutations. We present new results
to show that (i) the NPK is rotationally invariant for convolutional networks with global average
pooling, and (ii) the NPK is an ensemble of many kernels in the presence of skip connections.
•	Disentangling Weight Network. We then argue via theory and experiments that the weight network
is disentangled in the path space, i.e., it learns path-by-path and not layer-by-layer. For this, in
Section 3.2 we show via experiments that destroying the layer-by-layer structure by permuting the
layers and providing a constant 1 as input (see DLGN in Figure 1) do not degrade performance. These
counter intuitive results are difficult to reconcile using the commonly held ‘sophisticated structures
are learnt in layers’ interpretation. However, these experimental results follow from the theory in
Section 3.1. In other words, it is useful to think that the learning in the weight network happens
path-by-path; we call this dual linearity, which (for infinite width) is interpreted via the NPK.
Message. The DLGN is not an alternative architecture per se, but a disentanglement and an inter-
pretable re-arrangement of the computations in a DNN with ReLUs. The DLGN disentangles the
computations into two ‘mathematically’ interpretable linearities (i) the ‘primal’ linearity and (ii)
the ‘dual’ linearity interpreted via the NPK. Using the facts that the NPK is based on input and the
gates, and in a DLGN, the pre-activations in the gating network are ‘primal’ linear, we have complete
disentanglement. We compare the performance of DNN, DGN and DLGN on CIFAR-10 and CIFAR-
100 to show that, the DLGN recovers more than 83.5% of the performance of state-of-the-art
DNNs, i.e., while entanglement in the DNNs enable their improved performance, the ‘disentangled
and interpretable’ computations in the DLGN recovers most part of the performance.
Related Works. We now compare our work with the related works.
•	Kernels. Several works have examined theoretically as well as empirically two important kernels
associated with a DNN namely its NTK based on the correlation of the gradients and the conjugate
2
Under review as a conference paper at ICLR 2022
kernel based on the correlation of the outputs (Fan & Wang, 2020; Geifman et al., 2020; Liu et al.,
2020; Chen et al., 2020; Xiao et al., 2020; Jacot et al., 2018; Arora et al., 2019; Novak et al., 2018;
Lee et al., 2017; 2020b). In contrast, the NPK is based on the correlation of the gates. We do not
build pure-kernel method with NPK, but use it as an aid to disentangle finite width DNN with ReLUs.
• ReLU, Gating, Dual Linearity. A spline theory based on max-affine linearity was proposed in
(Balestriero et al., 2018; Balestriero & Baraniuk, 2018) to show that a DNN with ReLUs performs
hierarchical, greedy template matching. In contrast, the dual view exploits the gating property to
simplify the NTK into the NPK. Gated linearity was studied in (Fiat et al., 2019) for single layered
networks, along with a non-gradient algorithm to tune the gates. In contrast, we look at networks
of any depth, and the gates are tuned via standard optimisers. The main novelty in our work in
contrast to the above is that in DLGN the feature generation is linear. The gating in this paper refers
to the gating property of the ReLU itself and has no connection to (Srivastava et al., 2015) where
gating is a mechanism to regulate information flow. Also, the soft-gating used in our work and in
(Lakshminarayanan & Singh, 2020) enables gradient flow via the gating network and is different
from Swish (Ramachandran et al., 2018), which is the multiplication of pre-activation and sigmoid.
• Finite vs Infinite Width. Lee et al. (2020a) perform an extensive comparison of finite versus
infinite width DNNs. An aspect that is absent in their work, but present in the dual view is the
disentanglement of gates and weights, and the fact that the learning in gates is crucial for finite width
network to outperform infinite width DNNs. In our paper, we make use of theory developed for
infinite width DNNs to provide empirical insights into inner workings of finite width networks.
•	Capacity. Our experiments on destruction of layers, and providing constant 1 input are direct
consequences of the insights from dual view theory. These are not explained by mere capacity based
studies showing DNNs are powerful to fit even random labelling of datasets (Zhang et al., 2016).
2	Prior Work : Neural Tangent Kernel and Dual View
In this section, we will focus on the dual view (Lakshminarayanan & Singh, 2020) and how the dual
view helps to address the open question in the NTK theory. We begin with a brief summary of NTK.
NTK. An important kernel associated with a DNN is its neural tangent kernel (NTK), which, for a
pair of input examples x, x0 ∈ Rdin , and network weights Θ ∈ Rdnet , is given by:
NTK(χ,χ0)	=	hVθy(χ), Vθy(χ0)i, where
yθ(∙) ∈ R is the DNN output. Prior works (Jacot et al., 2018; Arora et al., 2019; Cao & Gu, 2019)
have shown that, as the width of the DNN goes to infinity, the NTK matrix converges to a limiting
deterministic matrix NTK∞, and training an infinitely wide DNN is equivalent to a kernel method
with NTK∞. While, as a pure kernel NTK∞ performed better than prior kernels by more than 10%,
Arora et al. (2019) observed that on CIFAR-10:
CNTK-GAP: 77.43% ≤ CNN-GAP: 83.30%
where, CNN-GAP is a convolutional neural network with global average pooling and CNTK-GAP is
its corresponding NTK∞ matrix. Due to this performance gap of about 5 - 6%, they concluded that
NTK∞ does not explain fully the success of DNNs, and explaining this gap was an open question.
2.1	Dual View For DNNs with ReLUs: Characterising the role of gates
In the dual view, the computations are broken down path-by-path. The input and the gates (in each
path) are encoded in a neural path feature vector and the weights (in each path) are encoded in a
neural path value vector, and the output is the inner product of these two vectors. The learning in the
gates and the learning in the weights are separated in a deep gated network (DGN) setup, which leads
to the two main results of dual view presented in Section 2.1.2 and Section 2.1.3, wherein, the neural
path kernel, the Gram matrix of the neural path features will play a key role.
2.1.1	Neural Path Feature, Value, Kernel and Deep Gated Network
Consider a fully connected DNN with ‘d’ layers and ‘w’ hidden units in each layer. Let the DNN
accept input X ∈ Rdin and produce an output yθ (x) ∈ R.
3
Under review as a conference paper at ICLR 2022
DGN (prior work)
CNN-GAP : 80.32%
x -> Ci -> ∣-> C2 -> ∣-> C3 ÷ I -> C4 -> I → £ > FCi ■> FC2 → y(x)
CNN-GAP-DGN
(FL: 79.68%, Standalone: 77.12%, FR: 67.09%)
XACf
^>∣→⅛* fci -* fc2 → ^f(χ)
I* GaLU
X
GlG=
3 I
Gl GaLU
vC3
→ > * FCv -> FCv —> yDGN(X)
Figure 2: Shows the DGN on the left. Training: In the case of fixed learnt gates, the feature network is
pre-trained using yf as the output, and then the feature network is frozen, and the value network is trained with
i^DGN as the output. In the case of fixed random gates, the feature network is initialised at random and frozen,
and the value network is trained with Jdgn as the output. In the case of fixed gates, hard gating G(q) = H{q>o}
is used. Standalone Training: both feature and value network are initialised at random and trained together
with Jdgn as the output. Here, soft gating G(q) = ι+eχp(-β∙q) is used to allow gradient flow through feature
network. On the right side is the CNN-GAP and its DGN used in (Lakshminarayanan & Singh, 2020). In
CNN-GAP, C1, C2, C3, C4 are convolutional layers, FC1, FC2 are fully connected layers. In CNN-GAP-DGN,
Gl, l = 1, 2, 3, 4 are the gates of layers, the superscripts f, and v stand for feature and value network respectively.
Definition 2.1. A path starts from an input node, passes through a weight and a hidden unit
in each layer and ends at the output node. We define the following quantities for a path p:
Activity	:	AΘ(x,p) is the product of the ‘d - 1’ gates in the path.
Value	:	vΘ(p) is the product of the ‘d’ weights in the path.
Feature :	φΘ(x,p) is the product of the signal at the input node of the path and AΘ(x,p).
The neural path feature (NPF) given by φθ(x) = (φθ(x,p),p = 1,...,Pfc , ∈ RPfc and the neural
path value (NPV) given by vθ = (vθ(p),P = 1,..., Pfc), ∈ RPPf, where Pfc = dinw(d-1) is the
total number of paths.
Proposition 2.1. The output of the DNN is then the inner product of the NPF and NPV:
yθ(X)=〈。㊀(X),vθi = E φθ(X,P)vθ(P)	(I)
p∈[P]
Subnetwork Interpretation of DNNs with ReLUs. A path is active only if all the gates in the path
are active. This gives a subnetwork interpretation, i.e., for a given input X ∈ Rdin, only a subset of the
gates and consequently only a subset of the paths are active, and the input to output computation can
be seen to be produced by this active subnetwork. The following matrix captures the correlation of
the active subnetworks for a given pair of inputs X, X0 ∈ Rdin .
Definition 2.2 (Overlap of active sub-networks). The total number of ‘active’ paths for both X and
X0 that pass through input node i is defined to be:
overlapΘ(i, X, X0) =∆ |{P: P starts at node i , AΘ (X, P) = AΘ(X0,P) = 1}|
Lemma 2.1 (Neural Path Kernel (NPK)). Let D ∈ Rdin be a vector of non-negative entries and for
u, u0 ∈ Rdin , let hu, u0iD = Pid=in 1 D(i)u(i)u0(i). Then the neural path kernel (NPK) is given by:
NPKθ(χ,χ ) = hφΘ(X),φΘ(X )i = hχ,χ ioverlapθ(∙,x,x0)
Deep Gated Network (DGN) is a setup to separate the gates from the weights. Consider a DNN
with ReLUs with weights Θ ∈ Rdnet . The DGN corresponding to this DNN (left diagram in Figure 2)
has two networks of identical architecture (to the DNN) namely the ‘gating network’ and the ‘weight
network’ with distinct weights Θf ∈ Rdnet and Θv ∈ Rdnet. The ‘gating network’ has ReLUs which turn
‘on/off’ based on their pre-activation signals, and the ‘weight network’ has gated linear units (GaLUs)
(Fiat et al., 2019; Lakshminarayanan & Singh, 2020), which multiply their respective pre-activation
inputs by the external gating signals provided by the ‘gating network’. Since both the networks
have identical architecture, the ReLUs and GaLUs in the respective networks have a one-to-one
correspondence. Gating network realises φΘf (X) by turning ‘on/off’ the corresponding GaLUs in the
weight network. The weight network realises v@v and computes the output 0dgn(x) = hΦθf (x),vθvi.
The gating network is also called as the feature network since it realises the neural path features, and
the weight network is also called as the value network since it realises the neural path value.
4
Under review as a conference paper at ICLR 2022
2.1.2	Learning Weights With Fixed Gates = Neural Path Kernel
During training, a DNN learns both φΘ (x) as well as vΘ simultaneously, and a finite time char-
acterisation of this learning in finite width DNNs is desirable. However, this is a hard problem.
An easier problem is to understand in a DGN, how the weights in the value network are learnt
when the gates are fixed in the feature network, i.e., how Odgn (x) = hφgf (x), vΘv〉is learnt by
learning vgv with fixed φθf(x). While Odgn(x) = hφθf (x), vθvi is linear in the dual variables, it
is still non-linear in the value network weights Θv. However, Lakshminarayanan & Singh (2020)
showed that the dual linearity is characterised by the NPK in the infinite width regime. We state
the assumption followed by Theorem 5.1 in (Lakshminarayanan & Singh, 2020), wherein, the
NTK(X,x0) = h▽㊀VOθdgn (x), ▽㊀VOθdgn (x0)i is due to the gradient of Odgn with respect to the value
network weights, and the NPK(x, x0) = hφΘf (x), φΘf (x0)i is due to the feature network weights.
Assumption 2.1. Θ0 Iw Bernoulli(2) over {—σ, +σ} and statistically independent of Θ0.
Theorem 2.1 (Theorem 5.1 in (Lakshminarayanan & Singh, 2020)). Under Assumption 2.1 for a
fully connected DGN :
NTKFC(x, x0) → d ∙ σ2(d-1) ∙ NPKFC(x, x0),	as W → ∞
=d ∙ σ2(d-1) ∙ hx, x0i ∙ overlap(x, x0)
Remark. In the fully connected case, overlap(i, x, x0) is identical for all i = 1, . . . , din, and hence
hx, x0ioveriap(∙,x,χθ) in Lemma 2.1 becomeshx, x0i ∙ overlap(x, x0) in Theorem 2.1. It follows from
NTK theory that an infinite width DGN with fixed gates is equivalent to kernel method with NPK.
2.1.3	Learning in Gates Key For Finite Width To Be Better Than Infinite Width
The fixed gates setting is an idealised setting, in that, it does not theoretically capture the learning of
the gates, i.e., the neural path features φΘ(x). However, the learning in the gates can be empirically
characterised by comparing fixed learnt (FL) gates coming from a pre-trained DNN and fixed
random (FR) gates coming from randomly initialised DNN, and the infinite width NTK. Using a
CNN-GAP and its corresponding DGN, Lakshminarayanan & Singh (2020) showed on CIFAR-10
that (see Figure 2 for details on DGN training and the CNN-GAP architecture):
FR Gates : 67.09% ≤ CNTK-GAP: 77.43% ≤ FL Gates: 79.68% ≈ CNN-GAP: 80.32%
based on which it was concluded that learning in the gates (i.e., neural path features) is crucial for
finite width CNN-GAP to outperform the infinite width CNTK-GAP. It was also shown that the DGN
can be trained standalone (as shown in Figure 2) and is only marginally poor to the DNN.
3	Deep Linearly Gated Networks: Complete Disentanglement
Primal	lifting	Dual
DLGN : X →Linear→ Pre-activations → Gates → φgf(x)→ Linear: y(x) = hφθf(x),vθvi
The deep linearly gated network (DLGN) has two ‘mathematically’ interpretable linearities, the
‘primal’ and the ‘dual’ linearities. The primal linearity is ensured in via construction and needs no
theoretical justification. Once the pre-activations triggers gates, φΘf (x) gets realised in the value
network by activating the paths. Now, the value network itself is ‘dual’ linear, i.e., it simply com-
putes/learns the inner product y(x) = hφθf (x),vθv). Gating lifts the 'primal, linear computations in
the feature network to ‘dual’ linear computations in the value network. Dual linearity is characterised
by the NPK (for infinite width) which in turn depends on the input and gates, and the fact that the
pre-activations to the gates are primal linear implies complete disentanglement and interpretability.
Dual linearity is mathematically evident due to the inner product relationship, however, adopting it
has the following conceptual issue: it is a commonly held view that ‘sophisticated features are learnt
in the layers’, that is, given that the input x ∈ Rdin is presented to the value network (as in Figure 2),
it could be argued that the GaLUs and linear operations are entangled which in turn enable learning
of sophisticated features in the layers. In what follows, we demystify this layer-by-layer view via
theory (infinite width case) in Section 3.1 , and experiments (on finite width networks) in Section 3.2,
and then study the performance of DLGN in Section 3.2. The layer-by-layer view is demystified by
5
Under review as a conference paper at ICLR 2022
showing that (i) a constant 1 input can be given to the value network, (ii) layer-by-layer structure
can be destroyed. The constant 1 input is meant to show that if the input is not given to the value
network then it is not possible to learn sophisticated structures ‘from the input’ in a layer-by-layer
manner. In terms of the dual linearity, providing a constant 1 input has only a minor impact, in that,
the neural path feature becomes φ(χ,p) = 1 ∙ A(χ,p), i.e., it still encodes the path activity which is
still input dependent. Since φ(x) depends only on gates, the NPK will depend only on the overlap
matrix; results in Section 3.1 captures this in theory. Now, it could be argued that, despite a constant
1 input, the gates are still arranged layer-by-layer, due to which, the value network is still able to
learn sophisticated structures in its layers. Section 3.1 has theory that points out that as long as the
correlation of the gates is not lost, the layer-by-layer structure can be destroyed.
3.1	Dual Linearity: New Insights and New Results
We saw in Section 2.1.2 that dual linearity is characterised by the NPK for infinite width case. In this
section, we: (i) cover standard architectural choices namely convolutions with global-average-pooling
and skip connections in Theorems 3.2 and 3.3; the prior result Theorem 2.1 is only for the fully
connected case, (ii) present new insights on Theorem 2.1 by restating it explicitly in terms of the gates
in Theorem 3.1, and (iii) discuss how the NPK structure helps in demystifying the layer-by-layer
view. Note: Results in this section are about the value network and hold for both DGN and DLGN.
3.1.1	Fully Connected: Product of LayerWise Base Kernels
Theorem 3.1. Let Gl (x) ∈ [0, 1]w denote the gates in layer l ∈ {1, . . . , d - 1} for input x ∈ Rdin.
Under Assumption 2.1 (σ = Ccw) as W → ∞, we have forfully connected DGN/DLGN:
NTKFC(x, x0) → d ∙ σ2(d-1) ∙ NPKFC(x, x0) = d ∙ C∙ (hx, x0i ∙ Πd-11 hGl ⑺*"))
•	Product Kernel : Role of Depth and Width. Theorem 3.1 is mathematically equivalent to
Theorem 2.1, which follows from the observation that overlap(x, x0) = Πl(=d-1 1)hGl(x), Gl(x0)i.
While this observation is very elementary in itself, it is significant at the same time; Theorem 3.1
provides the most simplest kernel expression that characterises the information in the gates. From
Theorem 3.1 it is evident that the role of width is averaging (due to the division by w). Each layer
therefore corresponds to a base kernel hG"x'(x " which measures the correlation of the gates.
The role of depth is to provide the product of kernels. To elaborate, the feature network provides
the gates Gl(x), and the value network realises the product kernel in Theorem 3.1 by laying out the
GaLUs depth-wise, and connecting them to form a deep network. The depth-wise layout is important:
for instance, if We were to concatenate the gating features as 夕(x) = (Gi(χ),l = 1,...,d - 1) ∈
{0, i}(dT)W, it would have only resulted in the kernel (^(χ), ^(χ0)i = Pd-1 (Gι(x), Gi(x0)i, i.e.,
a sum (not product) of kernels.
•	Constant 1 Input. This has a minor impact, in that, the expression on right hand side of Theorem 3.1
becomes d ∙ c2cd-I) ∙ din ∙ ∏d-1 (GMx)WG((X ”, i.e., the kernel still has information of the gates.
•	Destroying structure by permuting the layers. ∏d-1 hGl(X)WGl(X )i is permutation invariant, and
hence permuting the layers has no effect.
3.1.2	Convolution Global Average Pooling: Rotationally Invariant Kernel
We consider networks with circular convolution and global average pooling (architecture and notations
are in the Appendix). In Theorem 3.2, let the circular rotation of vector x ∈ Rdin by ‘r’ co-ordinates
be defined as rot(x, r)(i) = x(i + r), if i + r ≤ din and rot(x, r)(i) = x(i + r - din) if i + r > din.
Theorem 3.2. Under Assumption 2.1, for a suitable βcv (see Appendix for expansion ofβcv):
β	din -1
NTK	(X, X ) -→ d 2 ∙〉: hx, rot (X , r)ioverlap(∙ ,x,rot(x0 ,r)), as W -→ ∞
•	Prd=in-01hx, rot(x0, r)ioverlap(
∙ ,x,rot(x0 ,r))= Prd=in-01 Pid=in 1 X(i)rot(X0, r)(i)overlap(i, X, rot(X0, r)),
where the inner 'Σ' is the inner product between X and rot(x0, r) weighted by overlap and the outer
6
Under review as a conference paper at ICLR 2022
'Σ' covers all possible rotations, which in addition to the fact that all the variables internal to the
network rotate as the input rotates, results in the rotational invariance. It was observed by Arora et al.
(2019) that networks with global-average-pooling are better than vanilla convolutional networks. The
rotational invariance holds for convolutional architectures only in the presence of global-pooling.
So, this result explains why global-average-pooling helps. That said, rotational invariance is not a
new observation; it was shown by Li et al. (2019) that prediction using CNTK-GAP is equivalent to
prediction using CNTK without GAP but with full translation data augmentation (same as rotational
invariance) with wrap-around at the boundary (same as circular convolution). However, Theorem 3.2
is a necessary result, in that, it shows rotational invariance is recovered in the dual view as well.
•	The expression in Theorem 3.2 becomes 患∙ Pd=-I Pd= 1 overlap(i, x, rot(x0, r)) for a constant
1 input. The key novel insight is that the rotational invariance is not lost and overlap matrix measures
the correlation of the paths which in turn depends on the correlation of the gates.
•	Destroying structure by permuting the layers does not destroy the rotational invariance in
Theorem 3.2. This is because, due to circular convolutions all the internal variables of the network
rotate as the input rotates. Permuting the layers only affects the ordering of the layers, and does not
affect the fact that the gates rotate if the input rotates, and correlation in the gates is not lost.
3.1.3	Residual Networks With Skip Connections (ResNet): Ensemble Of Kernels
We consider a ResNet with ‘(b + 2)’ blocks and ‘b’ skip connections between the blocks. Each block
is a fully connected (FC) network of depth ‘dblk’ and width ‘w’. There are 2b many sub-FCNs within
this ResNet (see Definition 3.1). Note that the blocks being fully connected is for expository purposes,
and the result continue to hold for any kind of block.
Definition 3.1. [Sub FCNs] Let 2[b] denote the power set of [b] and let J ∈ 2[b] denote any subset
of [b]. Define the‘J th ’ sub-FCN of the ResNet to be the fully connected network obtained by (i)
including blockj, ∀j ∈ J and (ii) ignoring blockj, ∀j ∈/ J.
Theorem 3.3. Let NPKFJC be the NPK of the Jth sub-FCN, and βfJc (see Appendix for expansion of
βfJc ) be the associated constant. Under Assumption 2.1, we have:
NTKRES → X βfJc NPKFJC, as w → ∞
J ∈2[b]
•	Ensemble. To the best of our knowledge, this is the first theoretical result to show that ResNets
have an ensemble structure, where each kernel in the ensemble, i.e., NPKFJC corresponds to one of
the 2b sub-architectures (see Definition 3.1). The ensemble behaviour of ResNet and presence of 2b
architectures was observed by Veit et al. (2016), however without any concrete theoretical formalism.
•	Effect of constant 1 input is as before for kernels NPKFJC and translates to the ensemble NTKRES.
•	Destroying structure. The ResNet inherits the invariances of the block level kernel. In addition, the
ensemble structure allows to even remove layers. Veit et al. (2016) showed empirically that removing
single layers from ResNets at test time does not noticeably affect their performance, and yet removing
a layer from architecture such as VGG leads to a dramatic loss in performance. Theorem 3.3 can be
seen to provide a theoretical justification for this empirical result. In other words, due to the ensemble
structure a ResNet is capable of dealing with failure of components. While failure of component itself
does not occur unless one makes them fail purposefully as done in (Veit et al., 2016), the insight is
that even if one or many of the kernels in the ensemble are corrupt and the good ones can compensate.
3.2	Numerical Experiments
Section 3.1 presented theoretical results which demystified the layer-by-layer view in value network,
in this section we will verify these theoretical results in experiments. We then show that DLGN
recovers major part of performance of state-of-the-art DNNs on CIFAR-10 and CIFAR-100.
Setup Details. We consider 3 DNN architectures, C4GAP, VGG-16 and Resnet-110, and their
DGN and DLGN counterparts. Here C4GAP is a simple model (achieves about 80% accuracy on
CIFAR-10), mainly used to verify the theoretical insights in Section 3.1. VGG-16 and Resnet-110 are
7
Under review as a conference paper at ICLR 2022
chosen for their state-of-the-art performance on CIFAR-10 and CIFAR-100. All models are trained
using off-the-shelf optimisers (for more details, see Appendix D). The DGN and DLGN are trained
from scratch, i.e., both the feature and value network are initialised at random and trained. In DGN
and DLGN, we use soft gating (see Figure 2) so that gradient flows through the feature network
and the gates are learnt (we chose β = 10). In what follows, we use the notation DGN(xf, xv) and
DLGN(xf, xv) where xf and xv denote the input to the value and feature networks respectively. For
instance, DLGN(x, x) means that both the value and feature network of the DLGN is provided the
image as input, and DLGN(x, 1) will mean that the feature network is given the image as input and
the value network is given a constant 1 as input. DGN(x, x) and DGN(x, 1) notation works similarly.
Disentangling Value Network. We show that destroying the layer-by-layer structure via permutations
and providing a constant 1 input do not degrade performance. Since our aim here is not state-of-the-art
performance, we use C4GAP with 4 convolutional layers which achieves only about 80% test accuracy
on CIFAR-10, however, enables us to run all the 4! = 24 layer permutations. The C4GAP, DGN
and DLGN with layer permutations are shown in Figure 3. Once a permutation is chosen, it is fixed
during both training and testing. The results in Table I of Figure 3 show that there is no significant
difference in performance between DGN(x, x) vs DGN(x, 1), and DLGN(x, x) vs DLGN(x, 1), i.e.,
constant 1 input does not hurt. Also, there is no significant difference between the models without
permutations and the models with permutations. These counter intuitive and surprising results are
difficult to explain using the commonly held ‘sophisticated features are learnt layer-by-layer’ view.
However, neither the permutations or the constant 1 input destroys the correlation in the gates, and
are not expected to degrade performance as per the insights in Section 3.1. This verifies Claim I.
DLGN Performance. For this we choose VGG-16 and Resnet-110. The results in Table II of Figure 3
show that the DLGN recovers more than 83.5% (i.e., 83.78% in the worst case) of the performance of
the state-of-the-art DNN. While entanglement in the DNNs enable their improved performance, the
‘disentangled and interpretable’ computations in the DLGN recovers most part of the performance.
C4GAP
ReLU →∙ C4
t	Ψ
C3	ReLU
t	Ψ
ReLU	GAP
t	Ψ
C2	FC
ReLU
↑	y(X)
C1
t
x
C4GAP-DGN
Xv
Gi1	Gi2	Gi3	Gi4
C4GAP-DLGN
Layer Permutation
y(χ)
y(χ)
Cvf ∈ -> Cv -> 苴->Cv-> ∈ -> C4 -> ∈ —>
Xv	↑
Gi1	Gi2	Gi3	Gi4	FC
I	ψ	ψ	ψ	↑
Cv^*^ ∈ -> C2 -> p -> C3 -> ∈ -> Cv -> ∈ —> W
	Table I								
Dataset	Permute	C4GAP	DGN(X, X)	DGN(X, 1)	DLGN(X, X)	DLGN(X, 1)	DLGN(X,1) DNN
CIFAR10	No	80.5±0.4	77.4±0.3	77.5±0.2	75.4±0.3	75.4±0.2	93.66
	Yes	-	77.3±0.5	77.9±0.6	75.9±0.5	76.0±0.5	94.40
CIFAR100	No	51.8±0.4	47.4±0.2	47.3±o.3	47.4±0.1	48.0±0.2	92.66
	Yes	-	48.4±0.8	49.2±0.9	47.5±1.0	48.4±0.9	93.43
	TabIe II								
Dataset	Model	DNN	DGN(X, X)	DGN(X, 1)	DLGN(X, X)	DLGN(X, 1)	DLGN(x,1) DNN
CIFAR10	VGG16	93.6±0.2	93.0±0.1	93.0±o.ι	87.0±0.1	87.0±0.2	92.94
	ResNet110	94.0±0.2	93.3±0.2	93.2±0.1	87.9±0.2	87.8±0.1	93.40
CIFAR100	VGG16	73.4±0.3	70.3±0.1	70.5±o.2	61.5±0.2	61.5±0.1	83.78
	ResNet110	72.7±0.2	70.8±0.2	70.8±0.4	62.3±0.2	62.7±0.3	86.24
Figure 3: Here the gates G1 , G2, G3, G4 are generated by the feature network and are permuted as
Gi1 , Gi2 , Gi3 , Gi4 before applying to the value network. C1 , C2 , C3 , C4 have 128 filters each. Table I
and II: All columns (except the last) show the % test accuracy on CIFAR-10 and CIFAR-100, and % of DNN
performance recovered by DLGN is in the last column. Table I: For each dataset, the top row has results for
vanilla models without permutations (the results are averaged over 5 runs) and the bottom row has results of
4! - 1 = 23 permutations (except the identity) for each model (the results are averaged over the 23 permutations).
8
Under review as a conference paper at ICLR 2022
C4GAP-DLGN-SF
. H
如
C1GAP
GAP
f4
■*
x
G lψ
f3
x
G Iv
f2
x
GaLU
↑
vC4
↑
GaLU
↑
v3
i
GaLU
→
v2
i
GaLU
→
v1
GAP
→
ReLU
Table In
Dataset C1GAP CIGAP-16-ENS C1GAP CIGAP-16-ENS C4GAP-DLGN-SF VGG-16-DLGN-SF
(epochs)	(200)	(200)	(2000)	(2000)	(50)	(32)
CIFAR-10	62.2±0.2	63.81	70.0±0.2	72.2	75.1±0.4	84.9±0.2
CIFAR-100	36.3±0.2	38.23	42.2±0.3	45.47	47.3±0.6	56.3±0.2
Figure 4: C1GAP and C4GAP have width = 512 to make them comparable to VGG-16 whose maximum width
is 512. The ensemble size is 16 to match the 16 layers of VGG-16. Note that C4GAP in Figure 3 has width=128.
The last two columns show only DLGN-SF(x, 1). We observed the performance ofDLGN-SF(x, x) to be 〜2%
lesser and left it out from Table III for sake brevity. All results except for ENS are averaged over 5 runs.
3.3	Interesting Future Directions
Motivated by the success of DLGN, we further break it down into DLGN-Shallow Features (DLGN-
SF), wherein, the feature network is a collection of shallow single matrix multiplications. We compare
a shallow DNN with ReLU called C1GAP against DLGN-SF of C4GAP (see Figure 4) and VGG-16
(see Appendix). The results are in Figure 4, based on which we observe the following:
•	Power of depth in value network and lifting to dual space. Both C1GAP and C4GAP-DLGN-SF
were trained with identical batch size, optimiser and learning rate (chosen to be the best for C1GAP).
The performance of C1GAP at 200 epochs is 〜10% lower than that of C4GAP-DLGN-SF. After
2000 epochs of training and ensembling 16 such C1GAPs as C1GAP-16-ENS closes the gap within
〜 3% on C4GAP-DLGN-SF. Yet, a deeper architecture VGG-16-DLGN-SF is 〜 10% better than
C1GAP-16-ENS. Note that both VGG-16-DLGN-SF and C1GAP-16-ENS have gates for 16 layers
produced in a shallow manner. While in a C1GAP-16-ENS, ‘16’ C1GAPs are ensembled, in VGG-
16-DLGN-SF these gates for 16 layers are used as gating signals to turn ‘on/off’ the GaLUs laid
depth-wise as 16 layers of the value network, which helps to lift the computations to the dual space.
Thus, using the gates to lift (instead of ensembling) the computations to the dual space in the value
network is playing a critical role, investigating which is an important future work.
•	Power of depth in feature network. By comparing CIFAR-100 performance of VGG-16-DLGN-
SF in Figure 4 and that of VGG-16-DLGN in Figure 3, we see 〜 6% improvement if we have a deep
linear network instead of many shallow linear networks as the feature network. This implies depth
helps even if the feature network is entirely linear, investigating which is an important future work.
•	DGN vs DLGN In Table I and II of Figure 3, the difference between DGN and DNN is minimal
(about 3%), however, the difference between DLGN and DNN is significantly large. Thus, it is
important to understand the role of the ReLUs in the feature network of DGN. It is interesting to
know whether this is simpler than understanding the DNN with ReLUs itself.
•	Is DLGN a Universal Spectral Approximator? The value network realises the NPK which in
general is an ensemble (assuming skip connections). The NPK is based on the gates whose pre-
activations are generated linearly. It is interesting to ask whether the DLGN via its feature network
learns the right linear transformations to extract the relevant spectral features (to tigger the gates) and
via its value network learns the ensembling of kernels (based on gates) in a dataset dependent manner.
4 Conclusion
Entanglement of the non-linear and the linear operation in each layer of a DNN makes them uninter-
pretable. This paper proposed a novel DLGN which disentangled the computations in a DNN with
ReLUs into two mathematically interpretable linearities, the ‘primal’ linearity from the input to the
pre-activations that trigger the gates, and the ‘dual’ linearity in the path space. DLGN recovers more
than 83.5% of performance of state-of-the-art DNNs on CIFAR-10 and CIFAR-100. Based on this
success of DLGN, the paper concluded by identifying several interesting future directions.
9
Under review as a conference paper at ICLR 2022
References
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. In Advances in Neural Information Processing
Systems,pp. 8139-8148, 2019.
Randall Balestriero and Richard G Baraniuk. From hard to soft: Understanding deep network
nonlinearities via vector quantization and statistical inference. arXiv preprint arXiv:1810.09274,
2018.
Randall Balestriero et al. A spline theory of deep learning. In International Conference on Machine
Learning, pp. 374-383, 2018.
Anindya Basu and Alexander Kuhnle. PyGLN: Gated Linear Network implementations for NumPy,
PyTorch, TensorFlow and JAX, 2020. URL https://github.com/aiwabdn/pygln.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and deep
neural networks. In Advances in Neural Information Processing Systems, pp. 10835-10845, 2019.
Zixiang Chen, Yuan Cao, Quanquan Gu, and Tong Zhang. A generalized neural tangent kernel
analysis for two-layer neural networks. Advances in Neural Information Processing Systems, 33,
2020.
Brian Cheung, Alex Terekhov, Yubei Chen, Pulkit Agrawal, and Bruno Olshausen. Superposition of
many models into one. arXiv preprint arXiv:1902.05522, 2019.
Zhou Fan and Zhichao Wang. Spectra of the conjugate kernel and neural tangent kernel for linear-
width neural networks. arXiv preprint arXiv:2005.11879, 2020.
Jonathan Fiat, Eran Malach, and Shai Shalev-Shwartz. Decoupling gating from linearity. CoRR,
abs/1906.05032, 2019. URL http://arxiv.org/abs/1906.05032.
Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, and Ronen Basri. On the
similarity between the laplace and neural tangent kernels. arXiv preprint arXiv:2007.01580, 2020.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, pp.
8571-8580, 2018.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014.
Chandrashekar Lakshminarayanan and Amit Vikram Singh. Neural path features and neural path
kernel: Understanding the role of gates in deep learning. Advances in Neural Information
Processing Systems, 33, 2020.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165,
2017.
Jaehoon Lee, Samuel S Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak,
and Jascha Sohl-Dickstein. Finite versus infinite neural networks: an empirical study. arXiv
preprint arXiv:2007.15801, 2020a.
Jaehoon Lee, Samuel S Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak,
and Jascha Sohl-Dickstein. Finite versus infinite neural networks: an empirical study. arXiv
preprint arXiv:2007.15801, 2020b.
Zhiyuan Li, Ruosong Wang, Dingli Yu, Simon S Du, Wei Hu, Ruslan Salakhutdinov, and Sanjeev
Arora. Enhanced convolutional neural tangent kernels. arXiv preprint arXiv:1911.00809, 2019.
Chaoyue Liu, Libin Zhu, and Mikhail Belkin. On the linearity of large non-linear models: when and
why the tangent kernel is constant. Advances in Neural Information Processing Systems, 33, 2020.
10
Under review as a conference paper at ICLR 2022
Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A Abolafia,
Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with many
channels are gaussian processes. arXiv preprint arXiv:1810.05148, 2018.
Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions, 2018. URL
https://openreview.net/forum?id=SkBYYyZRZ.
RuPesh Kumar Srivastava, Klaus Greff, and Jurgen Schmidhuber. Highway networks. arXiv preprint
arXiv:1505.00387, 2015.
Andreas Veit, Michael Wilber, and Serge Belongie. Residual networks behave like ensembles of
relatively shallow networks. arXiv preprint arXiv:1605.06431, 2016.
Joel Veness, Tor Lattimore, Avishkar BhooPchand, David Budden, ChristoPher Mattern, Agnieszka
Grabska-Barwinska, Peter Toth, Simon Schmitt, and Marcus Hutter. Gated linear networks. arXiv
preprint arXiv:1910.01526, 2019.
Lechao Xiao, Jeffrey Pennington, and Samuel Schoenholz. Disentangling trainability and generaliza-
tion in deep neural networks. In International Conference on Machine Learning, pp. 10462-10472.
PMLR, 2020.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
11
Under review as a conference paper at ICLR 2022
path： pi (on)
value: 0.1 ∙ 0.2 ∙ 0.7
activity: 1
O On gate
• Off gate
path: p2 (on)
value: 0.1.0,3 ∙ 0.6
activity: 1
feature: X ■ 1
path: p3 (off)
value: -0.1 . 0.4 . 0.7
activity: 0
feature: X ■ 0
path: p4 (off)
value: -0.1 . 0.4 . 0.6
activity: 0
feature: X ■ 0
+ — 0.1 ■ 0.4 . 0.7 . 0 . X + -0.1 ■ 0.5 . 0.6 . 0 . X
^ = 0.1 . 0.2 . 0.7 . 1 . X + 0.1 . 0.3 . 0.6 . 1 . X
Figure 5: Illustration of Definition 2.1 and Proposition 2.1 in a toy network with 2 layers, 2 gates
per layer and 4 paths. Paths pi andp are 'on' and paths p3 andp4 are 'off’. The value, activity and
feature of the individual paths are shown. y is the summation of the individual path contributions.
A Fully Connected
Here, we present the formal definition for the neural path features and neural path values for the fully
connected case in Definition A.1. The layer-by-layer way of expressing the computation in a DNN of
width 'w’ and depth 'd’ is given below.
Input Layer	zx,Θ (∙, 0)	二	二 X
Pre-Activation	:	qx,Θ (iout, l)	二	Eiin Θ(iin,iout,l) ∙ Zχ,θ(iin,l-1)
Gating	:	Gx,Θ(iout, l)	-1{qx,θ(iout,l)>0}
Hidden Layer Output	:	zx,Θ (iout, l)	二	qx,θ(iout, I) ∙ Gx,θ(iout, I)
Final Output	yθ(χ)	二	二	Eiin Θ(iin, iout, d) ∙ Zχ,θ(iin, d - 1)
Table 1: Information flow in a FC-DNN with ReLU. Here, 'q’s are pre-activation inputs, 'z’s are output of the
hidden layers, 'G’s are the gating values. l ∈ [d - 1] is the index of the layer, iout and iin are indices of nodes in
the current and previous layer respectively.
Notation A.1. Index maps identify the nodes through which a path p passes. The ranges of index
maps Ilf, Il, l ∈ [d - 1] are [din] and [w] respectively. Id(p) = 1, ∀p ∈ [Pfc].
Definition A.1. Let x ∈ Rdin be the input to the DNN. For this input,
(i)	AΘ(x, p) =∆ Πld=-11Gx,Θ (Il (p), l) is the activity of a path.
(ii)	φΘ(x) =∆ x(I0f (p))AΘ(x,p),p ∈ [Pfc] ∈ RPfc is the neural path feature (NPF).
(iii)	vθ = (∏d=1Θ(I1-1(p), Zl(P),l),p ∈ [Pfc]) ∈ RPPfiS the neural Path value (NPV).
B Convolution With Global Average Pooling
In this section, we define NPFs and NPV in the presence of convolution with pooling. This requires
three key steps (i) treating pooling layers like gates/masks (see Definition B.2) (ii) bundling together
the paths that share the same path value (due to weight sharing in convolutions, see Definition B.3),
and (iii) re-defining the NPF and NPV for bundles (see Definition B.4). Weight sharing due to
convolutions and pooling makes the NPK rotationally invariant Lemma B.1. We begin by describing
the architecture.
Architecture: We consider (for sake of brevity) a 1-dimensional1 convolutional neural network with
circular convolutions, with dcv convolutional layers (l = 1, . . . , dcv), followed by a global-average-
Pooling layer (l = dcv + 1) and dfc (l = dcv + 2, . . . , dcv + dfc + 1) fully connected layers. The
convolutional window size is wcv < din, the number of filters per convolutional layer as well as the
width of the FC is w.
Indexing: Here iin/iout are the indices (taking values in [w]) of the input/output filters. icv denotes
the indices of the convolutional window taking values in [wcv]. ifout denotes the indices (taking values
1The results follow in a direct manner to any form of circular convolutions.
12
Under review as a conference paper at ICLR 2022
in [din], the dimension of input features) of individual nodes in a given output filter. The weights
of layers l ∈ [dcv] are denoted by Θ(icv , iin, iout, l) and for layers l ∈ [dfc] + dcv are denoted by
Θ(iin, iout, l). The pre-activations, gating and hidden unit outputs are denoted by qx,Θ(ifout, iout, l),
Gx,Θ (ifout, iout, l), and zx,Θ (ifout, iout, l) for layers l = 1, . . . , dcv.
Definition B.1 (Circular Convolution). For x ∈ Rdin, i ∈ [din] and r ∈ {0, . . . , din - 1}, define :
(i)	i ㊉ r = i + r, for i + r ≤ din and i ㊉ r = i + r - dn for i + r > dn
(ii)	rot(x, r)(i) = x(i ㊉ r),i ∈ [din].
(iii)	qx,Θ (ifout, iout, l) = Picv ,iin Θ(icv , iin ,
iout, l) ∙ zx,Θ(ifout㊉(i cv — 1),i in, l - 1).
Definition B.2 (Pooling). Let Gpxo,Θol (ifout, iout, dcv + 1) denote the pooling mask, then we have
zx,Θ (iout, dcv + 1) = Pifout zx,Θ (ifo
Ut, iout, dcv ) ∙ GX,θ(ifout, iout, dcv + 1),
where in the case of global-average-pooling G隰(ifout, iout, dcv + 1) =肃,∀iout ∈ [w],ifout ∈ [din].
Input Layer	Zx,θ(∙, 1,0)	二 X
Convolutional Layers, l ∈ [dcv]		
Pre-Activation Gating Values Hidden Unit Output	qx,Θ (ifout, iout, l) Gx,Θ(ifout, iout, l) zx,Θ (ifout, iout, l)	二	∑2icv,iin θ(icv, iin, iout, I) ∙ zx,Θ (ifout ㊉(icv - I) ,iin,l - I) 1{qx Θ (ifout ,iout,l) >0} 二	qx,Θ (ifout,iout,l) ∙ Gx,Θ (ifout,iout,l)
GAP Layer, l = dcv + 1		
Hidden Unit Output	zx,Θ (iout, dcv + 1)	二	^^ifOUt zx,Θ (ifout,iout,dCV) ∙ GX,Θ (ifout,iout,dcv + I)	
Fully Connected Layers, l ∈ [dfc] + (dcv + 1)
Pre-Activation	:	qx,Θ (iout, l)	=	Eiin Θ(iin,iout,l) ∙ Zχ,θ(iin,l-1)
Gating Values	:	Gx,Θ(iout, l)	=	1{(qχ,θ(iout,l))>0}
Hidden Unit Output	:	zx,Θ (iout, l)	=	qx,Θ (iout, l) ∙ Gx,Θ (iout, l) =	Piin Θ(iin,iout,d) ∙ Zχ,θ(iin,d - 1)	
Final Output	yθ(χ)	
Table 2: Shows the information flow in the convolutional architecture described at the beginning of
Appendix B.
B.1 Neural Path Features, Neural Path Value
Proposition B.1. The total number of paths in a CNN is given by Pcnn = din(wcvw)dcv w(dfc-1).
Notation B.1 (Index Maps). The ranges of index maps Ilf, Ilcv, Il are [din], [wcv] and [w] respectively.
Definition B.3 (Bundle Paths of Sharing Weights). Let PCnn = Pnn, and {Bι,...,Bpcnn} be a
cnn
collection ofsets such that ∀i, j ∈ [Pcnn], i = j we have Bi ∩ Bj = 0 and ∪P=ι Bi = [Pcnn]. Further,
if paths p,p0 ∈ Bi, then Ilcv(p) = Ilcv(p0), ∀l = 1, . . . , dcv andIl(p) = Il (p0), ∀l = 0, . . . , dcv.
Proposition B.2. There are exactly din paths in a bundle.
Definition B.4. Let x ∈	Rdin be the input to the CNN. For this input,
Aθ(χ,P)	=	(∏=+1Gx,θ(If(p),Il(P),1))∙ (∏d: +d++1Gχ,θ(iι(p),i))
φχ,Θ (P)	= Pp∈Bp X(IO(P))AΘ(X,P)
Vθ(Bp)	=	(∏也Θ(Icv(p),ILI(P)Il(P),l))∙ (∏=+d+ +1Θ(I1-1(P),Il(P),l))
NPF	φx,θ = (φχ,θ(Bp),P ∈ [Pcnn]) ∈ RPCnn
NPV	vθ = (vθ(Bp),P ∈ [Pcnn]) ∈ RPCnn
13
Under review as a conference paper at ICLR 2022
B.2 Rotational Invariant Kernel
Lemma B.1.
din -1
NPKΘ	(x, x ) =): hx,rot(x , r)ioverlapθ(∙,x,rot(x0,r))
r=0
din -1
=〉:hrot(x, r), x ioverlapθ(∙,rot(x,r),x0)
r=0
Proof. For the CNN architecture considered in this paper, each bundle has exactly din number of
paths, each one corresponding to a distinct input node. For a bundle b^, let b^(i), i ∈ [din] denote the
path starting from input node i.
E ( E x(i)x0(i0)Aθ (x,bp(i)) Aθ (x0,bp(i0))
p∈[P] ∖i,i0∈[din]
Σ	Σ	x(i)x0(i ㊉ r)Aθ (x, bp(i)) Aθ (x0, bp(i ㊉ r))
p∈[P] ∖i∈[din],i0=i㊉r,r∈{0,…,din-1}
Σ	Σ	x(i)rot(x0, r)(i)Aθ (x, bp(i)) Aθ (rot(X, r), bp(i))
p∈[P] ∖i∈[din],r∈{0,…,din-1}
din-1
E E χ(i)rot(χ0,r)(i) E aθ (χ,bp(i)) aθ (rot(χ0,r),bp(i)))
r=0 i∈[din]
din -1
P∈[P]
Σ Σ x(i)rot(x0, r)(i)overlapΘ (i, x, rot(x0, r))
r=0	i∈[din]
din -1
):hx, rot(x ,r)ioverlapθ(∙,x,rot(x0,r))
r=0
□
In what follows we re-state Theorem 3.2.
Theorem B.	1. Let σcv = √cwW^for the convolutional layers and σf = c√le for FC layers. Under
Assumption 2.1, as w → ∞, with βcv = dcvσc2v(dcv-1)σf2cdfc + dfc σc2vdcv σf2c(dfc -1) we have:
CONV	βcv	CONV
NTKΘ0GN →	d~2 ∙ NPKΘ
din	0
Proof. Follows from Theorem 5.1 in [13].	□
C Residual Networks with Skip connections
As a consequence of the skip connections, within the ResNet architecture there are 2b sub-FC
networks (see Definition 3.1). The total number of paths Pres in the ResNet is equal to the summation
of the paths in these 2b sub-FC networks (see Proposition C.1). Now, The neural path features and
the neural path value are Pres dimensional quantities, obtained as the concatenation of the NPFs and
NPV of the 2b sub-FC networks.
Proposition C.1. The total number ofpaths in the ResNet is Pres = d^ ∙ Pb=0 (7) w(i+2)dblk-1.
14
Under review as a conference paper at ICLR 2022
Lemma C.1 (Sum of Product Kernel). Let NPKRΘES be the NPK of the ResNet, and NPKJΘ be the
NPK of the sub-FCNs within the ResNet obtained by ignoring those skip connections in the set J.
Then,
NPKRΘES = X NPKJΘ
J ∈2[b]
Proof. Proof is complete by noting that the NPF of the ResNet is a concatenation of the NPFs of the
2b distinct SUb-FC-DNNs within the ResNet architecture.	□
We re-state Theorem 3.3
Theorem C.	1. Let σ = c√W. Under Assumption 2.1, as W → ∞, for βJS = (|J| + 2) ∙ dbik ∙
σ2 ((lJl+2)dblk-1)
NTKRΘED0SGN → X βrJesNPKJΘf
J ∈2[b]	0
Proof. Follows from Theorem 5.1 in [13].	□
D	Numerical Experiments: Setup Details
We now list the details related to the numerical experiments which have been left out in the main
body of the paper.
•	Computational Resource. The numerical experiments were run in Nvidia-RTX 2080 TI GPUs and
Tesla V100 GPUs.
•	All the models in Table I of Figure 3 we used Adam (Kingma & Ba, 2014) with learning rate of
3 × 10-4 , and batch size of 32.
•	We tried several values of β in the range from 1 to 100, and found the range 4 to 10 to be suitable.
We have chosen β = 10 throughout the experiments.
•	In Section 3.2, the codes for experiments based on VGG-16 and Resnet-110 were refactored from
following repository: “https://github.com/gahaalt/resnets-in-tensorflow2".
•	For VGG-16-DLGN in Figure 3 and DLGN-SF in Figure 4, the max-pooling were replaced by
average pooling so as to ensure that the feature network is entirely linear. For the comparison to
be fair, we replaced the max pooling in VGG-16 reported in Figure 3 by average pooling. Batch
normalisation layers were retained in VGG-16, VGG-16-DLGN and VGG-16-DLGN-SF (all three
are shown in Figure 8).
•	DLGN of Resnet-110 was derived from Resnet-110 in a similar manner.
•	All the VGG-16, Resnet-110 (and their DGN/DLGN) models in Table II of Figure 3 we used
SGD optimiser with momentum 0.9 and the following learning rate schedule (as suggested in
“https://github.com/gahaalt/resnets-in-tensorflow2") : for iterations [0, 400) learning rate was 0.01,
for iterations [400, 32000) the learning rate was 0.1, for iterations [32000, 48000) the learning rate
was 0.01, for iterations [48000, 64000) the learning rate was 0.001. The batch size was 128. The
models were trained till 32 epochs.
•	The VGG-16-DLGN-SF in Table III of Figure 4 uses the same optimiser, batch size and learning
rate schedule as the models in Table II of Figure 3 as explained in the previous point.
•	For C1GAP and C4GAP in Table III of Figure 4, we used Adam (Kingma & Ba, 2014) with learning
rate of 10-3, and batch size of 32. This learning rate is best among the set {10-1, 10-2, 10-3, 3 ×
10-4} for C1GAP.
15
Under review as a conference paper at ICLR 2022
E Models Used in Figure 3 and Figure 4
Xv → Cv ->
GaLU
GAP
GLU
Xv -C Cv	P Cv P -> C3
GaLU
G3
Gl
G2
Gl	G2
Xv —
Xf ->
C1v
GaLU
GLU
GLU
f4
GAP
GLU
GLU
GLU
GLU
GAP
GLU
Xv →
Xf 一
C1v
GaLU
GaLU
GaLU
GAP
→
GaLU
Xv →
Xf →
GaLU
GaLU
-÷≡⅛→∙CV->≡⅛→J∣→∙FC→∙ y(x)
Xv →
Xf —>
GLU
C3 -> 件->C4-►件 →∙,→∙fc→∙ y(x)
GLU
Xf → C1
xv ―*- Cv -> p -> Cv
GaLU
cV-> ^→∙c4->^→>→∙fc →∙ y(x)
Xv -C Cv	P Cv P -> C3
3
G
3
--—> GaLU
y(χ)
xv → CV
Xf —> CI
C2f
G1	G2
GLU
GaLU
GLU
GLU
GLU
→ !> → FC → y(x)
Xf —> C1
Xf —> Cl

CV，一
HI-
S 3 , > :
i GaLU
xv → CV -> 处、CV，处、C3
Xf —> Cl
Figure 6: Shows the permutations 1 - 12 C4GAP-DLGN in Table I of Figure 3. The top left is the
identity permutation and is the vanilla model.
16
Under review as a conference paper at ICLR 2022
Xv →
Xf —>
Cv ->
C2v
GaLU
->C4->5⅛->!∣->FC-> y(x)
Xv ―* Cv -> P -> Cv P -> C3
GaLU
Cv -> , →∙ W →∙ FC →∙ y(x)
Cv -> r1 -> C4
Xf → C1
VC2
Gl
Gi2
Xv ―*- Cv -> P -> Cv
GaLU
GAP
→
GaLU
C2f
Xv → Cv
Xf —> C1
Cv -> r1 -> C3
VC3
G2
V GaLU
y(χ)
Xv —
Xf ->
C1v
GaLU
GaLU
GaLU
GaLU
Xv →
Xf →
C1v
GaLU
GaLU
C3v
GAP
→
GaLU
→
v4
→
GaLU

C1f
C3f
xv → Cv
Xf —C1
Xv →
Xf —>
Cv，一
C2v
GaLU
C4、■ — W →∙ FC →∙ y(x)
Xv →
C1f
Cv -> P ->
Gi1	Gi2	Gi3
Gi4
Cv ^^> Γ, C, CvΓ, Cv C4
GAP
→
GaLU
Figure 7: Shows the permutations 13 -
GaLU
GaLU
GaLU
GaLU
GAP
)
s
GaLU
GaLU
GaLU
GaLU
∣→ FC → y(X)
xv → Cv -> Cv ->	C3
Xf → C1
VC2
C3f
GaLU
G1	G2
xv ―C Cv -> p -*■ C2
GaLU
Cv	■ →∙ W →∙ FC →∙ y(x)
Cv →∙ ^→∙CI->^→>→∙FC→ y(x)
24 C4GAP-DLGN in Table I of Figure 3.
17
Under review as a conference paper at ICLR 2022
y(χ)
↑
FC
↑
Global Avg. Pool
↑
ReLU+Avg. Pool (1/2)
↑
BN
↑
(3 × 3, 512) C13
↑
ReLU
↑
BN
↑
(3 × 3, 512) C12
↑
ReLU
↑
BN
↑
(3 × 3, 512) C11
↑
ReLU+ Avg. Pool (1/2)
↑
BN
↑
(3 × 3, 512) C10
↑
ReLU
↑
BN
↑
(3 × 3, 512) C9
↑
ReLU
/ G13
BN
↑
C13
LG12
BN
↑
C12
IXGII
BN
↑
C11
↑
Avg. Po少 G io
BN
↑
C10
G9
BN
↑
C9
G8
y(χ)
↑
FC
↑
Global Avg.
T
→ GaLU+Avg.
↑
BN
↑
C13
↑
----->GaLU
↑
BN
↑
C12
↑
----->GaLU
↑
BN
↑
C11
↑
→ GaLU+ Avg.
↑
BN
↑
C10
↑
------>GaLU
↑
BN
↑
C9
↑
------>GaLU
Pool
Pool
Pool
G10
Avg. Pool
BN
↑
(3 × 3, 512) C8
↑
ReLU+Avg. Pool (1/2)
↑
BN
↑
(3 × 3, 256) C7
↑
ReLU
↑
BN
↑
(3 × 3, 256) C6
↑
ReLU
↑
BN
↑
(3 × 3, 256) C5
↑
ReLU+Avg. Pool (1/2)
↑
BN
↑
(3 × 3, 128) C4
↑
ReLU
↑
BN
↑
(3 × 3, 128) C3
↑
ReLU+Avg. Pool (1/2)
↑
BN
↑
(3 × 3, 64) C2
↑
ReLU
↑
BN
↑
(3 × 3, 64) C1
BN
↑
C8
↑
Avg. Po少 Gl
BN
↑
G6
BN
↑
。6
BN
↑
C5
↑
Avg. Po少 G4
BN
↑
C4
PG3
BN
↑
C3
↑
Avg. PooPF G2
BN
↑
C2
x
U
BN
↑
C1
Xf
G1
BNtcsτAVtBNt αtaLtBNtc6taLtBNtaτ
↑^↑ gtaL•*—BNtcs*-toTBNt gtaLtBNt G-↑ v
Pool
Pool
Pool
C10
G9
G8
BN
↑
C9
BN
↑
C8
Avg. Pool
G6
C6
G5
BN
↑
C5
Avg. Pool
G7
BN
↑
C4
BN
↑
C3
Avg. Pool
G4
BN
↑
・ C
G2
y(χ)
↑
FC
↑
Global Avg.
t
→ GaLU+Avg.
↑
BN
↑
C13
↑
----->GaLU
↑
BN
↑
C12
↑
----->GaLU
↑
BN
↑
C11
↑
→ GaLU+ Avg.
↑
BN
↑
C10
↑
------>GaLU
↑
BN
↑
C9
↑
------>GaLU
↑
BN
↑
C8
t
→ GaLU+Avg.
↑
BN
↑
C7
↑
------>GaLU
G3
BNf αfaLItBNfat dfBNf αfaLItBNfcsf dfBNf αfaLItBNt Gtv
Pool
Pool
Pool
Pool
Pool
Pool
Figure 8: Shows VGG-16 (left), VGG-16-DLGN (middle), VGG-16-DLGN-SF(right).
18
Under review as a conference paper at ICLR 2022
F Continual Learning
In this section we compare DLGN and Gated Linear Network (GLN) of Veness et al. (2019). We
highlight two closely related aspects of the GLN (as a result of the context based gating) namely
(i) context induced increased capacity(CIIC) and (ii) context induced sparse activity (CISA).
These two aspects were present in the work by Veness et al. (2019), yet they were not highlighted
because these two become apparent only from the equivalent flattened computational graph (see
Appendix F.3) of the GLN (which was not present in Veness et al. (2019)). While Veness et al. (2019)
attributed ‘cosine similarity’ for the success of GLN in continual learning tasks, we show that cosine
similarity itself is not sufficient and context induced increased capacity and context induced sparse
activity are also necessary. We show via experiments that :
(i)	GLN with CIIC and CISA performs well in continual learning.
(ii)	GLN without CIIC and CISA performs poorly in continual learning.
(iii)	DLGN-SF with increased capacity and sparse activity performs well in continual learning.
(iv)	DLGN-SF without increased capacity and sparse activity performs poorly in continual learning.
Continual Learning Task. We compare the continual learning capabilities of the GLN and the
DLGN-SF on the permuted MNIST dataset. We first generate 8 different permuted MNIST datasets
and then use the same set of 8 tasks to study the continual learning capabilities of all the models.
F.1 Non-Comparable Aspects of DLGN and GLN
We first list and discuss the non-comparable aspects between the two works.
•	Training Algorithm: The DLGN in our paper is a close cousin of the DNN with ReLUs and is
trained by backpropagation. The GLN on the other hand is conceived and motivated from the experts
setting and every node of the network is trained in a backpropagation free manner.
•	Learning in the Gates: In our paper, the parameters of the feature network (whose layers trigger
the gates) are learnt. In the GLN, the gating is fixed and not learnt. This is a very important difference
because it was shown by Lakshminarayanan & Singh (2020) that the learning in the gates is a very
important aspect of the DNNs with ReLUs.
•	Gating Mechanism: In the case of DLGN, gating mechanism exists for fully connected, convolu-
tion, residual networks. In the GLN, there are no (equivalents of) convolutional/residual architectures,
and as such we can say the GLN is equivalent to a fully connected architecture. Now, even in the
case of fully connected architecture, the gating between DLGN and GLN differ quite a bit. In the
case of DLGN, each unit is controlled by a gate which corresponds to a hyperplane. In the case
of GLN, for each unit, the number of hyperplanes is equal to the so called context-dimension, and
the binary encoding of these hyperplanes is mapped into one of 2context-dimension contexts which is
activated/selected.
F.2 Comparable Aspects of DLGN and GLN
Despite the aforementioned differences, DLGN and the GLN share the following commonalities (1)
separate gating, (2) both models are data dependent linear networks. For the purpose of comparison
to be close enough, let us consider fully connected DLGN-SF (shallow features) whose feature
network is initialised at random and not trained. This way the DLGN-SF is similar to the GLN in that
(a) the gates are fixed and (b) gates are generated via random hyperplanes. We now look at the data
dependent linearity part by comparing the computational graphs of the DLGN-SF and GLN.
F.3 Flatting the computational graph of GLN and comparing it with DLGN-SF
We now compare the computational graphs of GLN and the value network of DLGN-SF in Figure 9.
For the sake of lucidity, in the Figure 9 as well in the discussion below, we have chosen input
dimension to be 2, layers to be 2 and units per layer (i.e., width) to be 2. In Figure 9 we have omitted
the gating part which will be discussed in the text below, along with other relevant details.
19
Under review as a conference paper at ICLR 2022
GLN
11
力2 %3
吗：吗12 喟 喟
,13
,14
GLN : Flattened Computational Graph
Figure 9: Shows a comparison of the gated linear network (GLN) and the value network of DLGN.
For sake of clarity we have ommitted the details about gating which we have described in the text.
Here x = (x(1), x(2)) ∈ R2 is the input. Each of the ‘w’s is a weight vector with 2 components. The
bold lines connecting the units with their weight vectors denote their one-to-one correspondence. The
2 thin lines emanating from each w are its 2 components (bold lines denote the entire weight vector).
Left Most Diagram (Figure 9) depicts (without gating) the GLN in a manner similar to Figure 1 of
Veness et al. (2019). Here, i1 and j1 are the 2 units in layer 1 and i2 and j2 are the two units in layer
2. The ‘w’s are weight vectors in R2. The figure does not depict the 4 context maps Ci1 , Cj1 , Ci2, Cj2,
where each C∙ : X → {cι, c2, c3, c4}. For example, if Cji (x) = c2 then unit jι uses weight w}j, and
its output will be wjc2 x.
Middle Diagram (Figure 9) is a flattened version of the GLN in the left most diagram. The
aim of flattening is to depict the diagram in the left in the form of a standard computation graph.
Flattening is achieved by expanding the units i1,j1, i2, j2 context wise into corresponding sub-units
ic1k, j1ck, ic2k, j2ck, k = 1, 2, 3, 4. The function of the context maps can be achieved by one-hot encoded
gating signals (this is not shown in Figure 9), Gick (x), Gjck (x), Gick (x), Gjck (x), k = 1, 2, 3, 4.
For instance, if Ci2 (x) = c3, then we will have Gic3 (x) = 1 and Gick (x) = 0, ∀k 6= 3. The output
of unit if2 is G12 (x) ∙ wiC2x. The output of unit iι is the summation of the outputs of i?, if2, if3, if4.
Right Most Diagram (Figure 9) shows the value network of a DLGN. The gating signals
Gii, Gji, Gi2, Gj2 have been left out in the diagram. The output of unit iι is G^ (x) ∙ Wii x.
F.4 Context Induced Increase In Capacity and Sparse Activity
Two notable aspects about the GLN are the following:
• Context Induced Increase In Capacity. Note that due to the presence of contexts, after flatten-
ing, we can see that the width of the GLN gets multiplied by a factor equal to no.of.contexts =
2context-dimension. In Figure 9, the width of the GLN is 2 (in the left more diagram) and after flattening
it becomes 2 × 22 (in the middle diagram). The number of parameters in a GLN is equal to the
number of parameters of the DLGN-SF times the same factor no.of.contexts = 2context-dimension, i.e.,
say in the first layer, instead of wii and wji in the right most diagram, in the middle diagram we have
wifik,wjfik,k= 1,2,3,4.
20
Under review as a conference paper at ICLR 2022
• Context Induced Sparse Activity: Eventhough the flattened width of the GLN gets multiplied
by the factor equal to no.of.contexts = 2context-dimension, in each of the flattened layers only 口。oflogt
units are active for any given input.
F.5 Continual learning in GLN with and without CIIC and CISA
The main reason for the success of GLN in continual learning (in permuted MNIST task) that is
highlighted by Veness et al. (2019) is “inputs close in terms of cosine similarity will give rise to
similar data dependent weight matrices. Since each task-specific cluster of examples is far from
each other in signature space, the amount of interference between tasks is significantly reduced, with
the gating essentially acting as an implicit weight hashing mechanism". However, we show in the
experiments below that cosine similarity is not sufficient, context induced increased capacity and
context induced sparse activity are also necessary.
Experimental Setup. We trained two GLN models namely GLNM-CD4 and GLNM-CD1 each of
which have 10 one-vs-all GLNs each with 4 layers with 128 units in the first 3 layers, and one unit
in the final layer which serves as the output unit. The context dimension of GLNM-CD4 is equal
to 4 and the context dimension of GLNM-CD1 is equal to 1. The code for the GLN (GLN training)
is from Basu & Kuhnle (2020). While it is mentioned in Veness et al. (2019) that a 98% accuracy
is achieved (on a single MNIST taksk), we obtain only 94.32%. There are two reasons for this (i)
in order to keep the comparison between GLN and DLGN-SF fair, we did not de-skew the dataset
(which is the default option while using the GLN in Basu & Kuhnle (2020)), and if skewing is added
the accuracy improves to about 96.5% and (ii) we also confirmed with one of the authors of Veness
et al. (2019) that 128 units might not be sufficient to achieve 98%. However, since our aim here is to
understand the trend in continual learning we accept 94.32% as a reasonable ballpark performance.
We now compare GLNM-CD4 and GLNM-CD1 in terms of CIIC and CISA.
CIIC (GLNM-CD4 vs GLNM-CD1). In GLNM-CD4, the flattened width is 128 × 16. In GLNM-
CD1, the flattened width is 128 × 2.
CISA (GLNM-CD4 vs GLNM-CD1). GLNM-CD4 the only 128 out of the 128 × 16 units are active
for any given input, i.e., only 焉 of the units are active. In GLNM-CDI, 128 out of the 256 units are
active, i.e., 2 of the units are active.
Thus we can see GLNM-CD4 is a GLN with CIIC and CISA, and GLNM-CD1 is a GLN without
CIIC and CISA. The results are in Tables A, and B where the entry in ith row and jth column stands
for the test accuracy in Task-j while training Task-i. All the results are based on 3 runs.
Table A: GLNM-CD4
	Test 1	Test 2	Test 3	Test 4	Test 5	Test 6	Test 7	Test 8
Train 1	94.32±0.16	-	-	-	-	-	-	-
Train 2	92.98±0.21	94.27±0.07	-	-	-	-	-	-
Train 3	92.37±0.15	93.63±0.06	94.52±0.07	-	-	-	-	-
Train 4	91.41±0.05	92.55±0.07	93.15±0.17	94.48±0.1	-	-	-	-
Train 5	89.53±0.23	91.70±0.27	92.33±0.22	93.41±0.21	94.31±0.1	-	-	-
Train 6	86.99±1.02	90.94±0.95	91.07±0.68	91.84±0.21	93.14±0.5	94.20±0.34	-	-
Train 7	84.91±0.73	89.94±0.49	90.14±0.26	90.41±0.32	91.21±0.5	93.06±0.28	94.12±0.29	-
Train 8	82.86±0.39	85.53±1.27	88.22±0.97	89.00±0.5	90.49±0.57	92.11±0.5	92.6±0.32	93.98±0.31
								
	Table B: GLNMD-CDI									
	Test 1	Test 2	Test 3	Test 4	Test 5	Test 6	Test 7	Test 8
Train1	90.28±1.21	-	-	-	-	-	-	-
Train2	81.89±1.45	90.29±1.09	-	-	-	-	-	-
Train3	75.19±4.45	84.18±2.45	89.4±0.24	-	-	-	-	-
Train4	72.69±0.71	80.95±0.34	84.69±1.57	91.3±0.5	-	-	-	-
Train5	61.02±2.79	73.16±2.53	76.06±2.09	83.14±0.74	91.42±0.7	-	-	-
Train6	53.22±2.47	70.77±2.65	73.63±1.01	68.56±0.52	84.92±0.68	91.66±0.36	-	-
Train7	49.21±4.6	61.20±1.99	64.35±1.14	72.85±4.11	79.13±2.18	83.25±0.91	91.29±0.29	-
Train8	42.49±7.31	49.30±3.02	61.62±2.97	59.35±3.19	73.58±5.2	78.97±6.56	81.87±3.04	90.19±2.31
Observation. From Tables A and B, we can observe that the GLNM-CD4 with CIIC and CISA
performs well and GLNM-CD1 without CIIC and CISA performs poorly, from which we can conclude
21
Under review as a conference paper at ICLR 2022
that cosine similarity is not sufficient, context induced increased capacity and context induced sparse
activity are also necessary.
F.6 Continual Learning in DLGN-SF with and without capacity and sparse
ACTIVITY
Cosine Similarity holds in DLGN-SF. Even in DLGN-SF, the gates are nothing but hyperplanes
and similar inputs should trigger similar gates and activate similar data dependent linear networks.
Therefore cosine similarity holds true for DLGN-SF as well.
Increasing Capacity in DLGN-SF. In order to make capacity of DLGN-SF comparable to that of
GLN with wGLN and a given context-dimension, we let the width of the DLGN-SF to be wDLGN-SF =
wGLN × 2context-dimension. Thus we consider two models namely DLGN-SF-128x16 which is comparable
to GLNM-CD4 and DLGN-SF-128x2 which is comparable to GLNM-CD1. DLGN-SF-128x16 has
10 one-vs-all DLGN-SFs, each with 4 layers with 128 × 16 units in the first three layers, and one
unit in the last layer which serves as the output unit. DLGN-SF-128x2 has 10 one-vs-all DLGN-SFs,
each with 4 layers with 128 × 2 units in the first three layers, and one unit in the last layer which
serves as the output unit.
Inducing Sparse Activity in DLGN-SF. The gates in a DLGN-SF are triggered by the feature network
which contains wDLGN-SF hyperplanes. Since we are considering DLGN-SF with randomly initialised
(from a symmetric distribution) and fixed feature network, it is reasonable to expect that 〜 WDLGN-SF
of the gates are active/on. Thus in order to induce sparse activity we sort the pre-activation signals
of the gates and activate the gates corresponding to the top 2context1imension pre-activation signals. Thus,
for the case of DLGN-SF-128x16 to ensure sparse activity we activate only the top 12：616 = 128
gates. For the case of DLGN-SF-128x2, since we are comparing it with GLNM-CD1 whose context
WDLGN-SF
dimension is 1, we need 〜 ——2——to be active, and hence wejust let the pre-activations to directly
trigger the gates.
Experimental Setup. We trained with SGD, batch size of 32 and learning rate of 0.32 (this is
equivalent to a learning rate of 10-2 and batch size of 1). All the results are based on 3 runs. Table C
shows DLGN-SF-128x16 with sparse activiy, and Table D shows DLGN-SF-128x16 without sparse
activity. Table E shows DLGN-SF-128x2 which does not sparse activity.
Table C: DLGN-SF-128x16 Sparse Activity: Top 128 Gates Active								
	Test 1	Test 2	Test 3	Test 4	Test 5	Test 6	Test 7	Test 8
Train 1	96.08±0.03	-	-	-	-	-	-	-
Train 2	95.79±0.25	96.52±0.07	-	-	-	-	-	-
Train 3	94.61±0.27	96.45±0.13	96.55±0.24	-	-	-	-	-
Train 4	93.64±0.67	96.12±0.18	96.45±0.26	96.73±0.09	-	-	-	-
Train 5	91.43±0.39	95.93±0.15	96.32±0.23	96.41±0.09	96.78±0.2	-	-	-
Train 6	90.13±1.00	95.66±0.04	96.08±0.25	95.94±0.18	96.50±0.19	96.95±0.1	-	-
Train 7	89.34±1.18	95.03±0.15	95.72±0.09	95.73±0.19	96.07±0.21	96.70±0.05	96.92±0.03	-
Train 8	88.87±0.61	94.27±0.30	95.46±0.09	95.77±0.15	95.98±0.24	96.72±0.04	96.57±0.06	96.79±0.14
Table D: DLGN-SF-128x16
No SParSe Activity:〜128×16 GateS are ACtiVe
	Test 1	Test 2	Test 3	Test 4	Test 5	Test 6	Test 7	Test 8
Train 1	96.87±0.02	-	-	-	-	-	-	-
Train 2	96.07±0.02	97.04±0.08	-	-	-	-	-	-
Train 3	90.60±0.94	96.35±0.35	97.06±0.01	-	-	-	-	-
Train 4	81.85±0.60	91.89±0.47	96.21±0.40	96.97±0.04	-	-	-	-
Train 5	62.41±4.27	84.61±3.75	93.19±0.41	95.72±0.16	96.92±0.01	-	-	-
Train 6	59.02±4.92	66.48±2.49	80.74±1.89	91.73±1.28	96.45±0.15	97.12±0.08	-	-
Train 7	46.57±10.13	55.19±2.36	66.82±1.87	87.26±1.54	94.36±0.29	96.63±0.10	97.2±0.09	-
Train 8	29.15±8.43	54.17±4.26	62.69±3.32	76.67±4.23	82.23±3.96	90.83±4.01	92.94±4.33	93.96±4.54
22
Under review as a conference paper at ICLR 2022
Table E: DLGN-SF-128x2
No Sparse Activity:〜128 Gates Active
	Test 1	Test 2	Test 3	Test 4	Test 5	Test 6	Test 7	Test 8
Train1	96.29±0.13	-	-	-	-	-	-	-
Train2	94.85±0.87	96.62±0.03	-	-	-	-	-	-
Train3	87.44±0.89	94.96±0.68	96.42±0.2	-	-	-	-	-
Train4	76.54±1.47	87.62±0.25	93.74±0.62	96.59±0.17	-	-	-	-
Train5	60.69±3.13	80.16±2.18	87.55±4.35	94.58±0.13	96.44±0.14	-	-	-
Train6	40.89±7.04	59.1±3.65	71.62±8.58	83.75±1.13	94.46±1.01	96.51±0.2	-	-
Train7	37.86±2.74	46.3±3.14	61.75±5.84	76.75±3.18	87.41±1.39	94.5±0.59	96.55±0.17	-
Train8	29.7±4.05	42.08±1.87	55.73±6.33	63.63±0.56	70.29±3.85	82.15±4.48	90.7±1.42	96.36±0.06
Observation. From Tables C with Tables D and E, we can observe that the DLGN-SF-128x16
with CIIC and CISA performs well and DLGN-SF-128x16 without CIIC and CISA as well as
DLGN-SF-128x2 without CIIC and CISA perform poorly.
F.7 Improving Continual Learning in DLGN-SF by Introducing Sparse Activity
We now show that DLGN-SF-128x2 can be made to perform well if we introduce sparse activity. For
this we sort the pre-activations to the gates and then trigger only the top 16 gates. The results are
shown in Table F.
Table F: DLGN-SF-128x2
Sparse Activity: Top 16 Gates Active
	Test 1	Test 2	Test 3	Test 4	Test 5	Test 6	Test 7	Test 8
Train1	94.48±0.16	-	-	-	-	-	-	-
Train2	92.86±0.41	95.18±0.08	-	-	-	-	-	-
Train3	91.61±0.75	94.26±0.48	95.56±0.13	-	-	-	-	-
Train4	91.35±0.68	93.8±0.48	95.14±0.14	95.47±0.14	-	-	-	-
Train5	89.65±0.98	93.41±0.48	94.6±0.15	94.87±0.31	95.44±0.14	-	-	-
Train6	88.71±0.69	92.5±0.82	93.58±0.14	94.02±0.3	94.92±0.15	95.54±0.05	-	-
Train7	88.24±0.59	90.9±0.77	92.82±0.28	93.66±0.34	94.41±0.05	95.01±0.05	95.33±0.07	-
Train8	87.54±0.63	88.84±0.69	92.41±0.33	93.57±0.28	94.03±0.13	94.83±0.19	94.65±0.29	95.68±0.11
Observation The difference between Table E and Table F is that the model in Table F has sparse
activity, and we observe that it sparse activity ensures good performance.
F.8 Conclusion
We conclude this section on continual learning by saying DLGN-SF with increased capacity and
sparse activity performs well in continual learning. A comparison of the continual learning capabilities
of GLN and DLGN to find out which is the optimal model out of the two will require a more thorough
study which we defer to future work. Further, to perform well in continual learning, the models
should be capable of learning many tasks, which naturally requires the models to have more capacity.
In the models that perform well above, we note that capacity has been built in two ways (i) via the 10
models due to ‘one-vs-all’ and (ii) the context in the GLN increases the capacity by a factor equal to
no.of.contexts, and in the DLGN-SF we ensure capacity increase by increasing the width. Sparsity
helps to ensure that the different parts of networks are responsible for different contexts and they do
not interfere with one another. High capacity and interference minimisation have also been mentioned
in Cheung et al. (2019) in the context of continual learning.
23