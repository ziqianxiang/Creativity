Under review as a conference paper at ICLR 2022
NeuroSED: Learning Subgraph Similarity via
Graph Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
Subgraph similarity search is a fundamental operator in graph analysis. In this
framework, given a query graph and a graph database, the goal is to identify
subgraphs of the database graphs that are structurally similar to the query. Subgraph
edit distance (SED) is one of the most expressive measures of subgraph similarity.
In this work, we study the problem of learning SED from a training set of graph
pairs and their SED values. Towards that end, we design a novel siamese graph
neural network called NeuroSed, which learns an embedding space with a rich
structure reminiscent of Sed. With the help of a specially crafted inductive bias,
NeuroSed not only enables high accuracy but also ensures that the predicted Sed,
like true SED, satisfies the triangle inequality. The design is generic enough to also
model graph edit distance (GED), while ensuring that the predicted GED space is
metric, like the true GED space. Extensive experiments on real graph datasets, for
both SED and GED, establish that NEUROSED achieves ≈ 2 times lower RMSE
than the state of the art and is ≈ 18 times faster than the fastest baseline. Further,
owing to its pair-independent embeddings and theoretical properties, NeuroSed
allows up to 3 orders of magnitude faster graph/subgraph retrieval.
1	Introduction and Related work
Graphs are used to model data in a wide variety of domains. Examples include chemical com-
pounds (Sankar et al., 2017), protein-protein interaction networks (PPI) (Alon et al., 2008), knowledge
graphs (Ebsch et al., 2020), and social networks (Kempe et al., 2003). A distance function on any
dataset, including graphs, is a fundamental operator. Among several distance measures on graphs,
edit distance is one of the most powerful and popular mechanisms (Liang & Zhao, 2017; Zhao et al.,
2013; Bougleux et al., 2017; Daller et al., 2018; Fankhauser et al., 2011; Riesen & Bunke, 2009;
Zeng et al., 2009; He & Singh, 2006). Edit distance can be posed in two forms: graph edit distance
(GED) and subgraph edit distance (SED). Given two graphs G1 and G2, GED(G1 , G2) returns the
minimum cost of edits needed to convert G1 to G2. i.e., for G1 to become isomorphic to G2. An edit
can be the addition or deletion of edges and nodes, or replacement of edge or node labels, with an
associated cost. In SED(G1, G2), the goal is to identify the minimum cost of edits so that G1 is a
subgraph (subgraph isomorphic) of G2 . For examples, see Fig. 5a in the appendix.
While the applications of Ged and Sed are beyond doubt, their applicability is constrained by their
computation costs. Specifically, both Ged and Sed are NP-complete (Zeng et al., 2009; Neuhaus et al.,
2006; He & Singh, 2006). To mitigate this computational bottleneck, several heuristics (Bougleux
et al., 2017; Daller et al., 2018; Fankhauser et al., 2011; Riesen & Bunke, 2009; Neuhaus et al., 2006)
and index structures (He & Singh, 2006; Zeng et al., 2009; Liang & Zhao, 2017; Zhao et al., 2013)
have been proposed. Recently, graph neural networks have been shown to be effective in learning and
predicting Ged (Bai et al., 2019; Wang et al., 2021; Li et al., 2019; Bai et al., 2020; Zhang et al.,
2021; Xiu et al., 2020) and subgraph isomorphism (Rex et al., 2020). The basic goal in all these
algorithms is to learn a neural model from a training set of graph pairs and their distances, such that,
at inference time, given an unseen graph pair, we are able to predict its distance accurately. Wang
et al. (2020) designed an algorithm to characterize graphs through sampling random walks and then
embedding them in a feature space. While Wang et al. (2020) has not been studied in the context of
Ged or Sed, the authors have shown that they can detect graph isomorphism. While this progress in
the area of graph querying is undoubtedly impressive, there is scope to do more.
•	Modeling Sed: Existing neural approaches to learning GED cannot easily be adapted to learn
Sed. While Ged is symmetric, Sed is not. Most neural architectures for Ged have the assumption
of symmetry at its core and hence modeling Sed is non-trivial. In App. A, we provide a detailed
analysis of the limitations of prior architectures in failing to model Sed.
1
Under review as a conference paper at ICLR 2022
•	Exponential Search Space: Computing SED(G1, G2) conceptually requires us to compare the
query graph G1 with the exponentially many subgraphs of the target graph G2 . Therefore, it is
imperative that the model has an efficient and effective mechanism to prune the search space
without compromising on the prediction accuracy. We note that while several index structures and
heuristics exist for Ged (He & Singh, 2006; Zeng et al., 2009; Bai et al., 2019; Wang et al., 2021),
none exist for Sed. Thus, scalability of Sed on large graphs remains an unsolved problem.
•	Preservation of theoretical properties: GED is a metric distance function. While SED is not
metric due to being asymmetric, it satisfies the triangle inequality, non-negativity, and subgraph-
identity. Several higher-order tasks such as clustering and indexing rely on such properties (Hadi,
1991; Dohnal et al., 2003; Hjaltason & Samet, 2003; Samet, 2005; Uhlmann, 1991; Yianilos, 1993;
Zezula et al., 2006; Brin, 1995; Chgvez et al., 2001). Existing neural approaches do not preserve
these properties, which limits their usability for these higher order tasks.
•	Pair-independent computation: There is little scope for pre-computation in existing approaches
(either neural or non-neural), as the major computations are pair-dependent, i.e., both G1 and G2
need to be known. In a typical graph querying framework, the graph database is known apriori;
only the query graph is provided at runtime. If we can generate pair-independent embeddings,
and make efficient predictions directly in the embedding space, then retrieval can be sped up
tremendously by pre-computing and indexing the embeddings of the database graphs beforehand.
•	Learning with low-volume data: Generating a large amount of high quality training data is
computationally prohibitive for Sed/Ged. So, an effective model should encode powerful priors
for Sed/Ged prediction, to enable generalization even with scarce training data.
We address these challenges by a novel architecture called NeuroSed. Our key contributions are:
•	Novel formulation: We formulate the problem of learning subgraph edit distance (SED) for graphs.
Since Sed is more general than subgraph isomorphism and Ged, the proposed theory extends to
prediction of Ged, graph isomorphism and subgraph isomorphism.
•	Neural architecture: NEURO S ED utilizes a siamese graph isomorphism network (Xu et al.,
2018a) to embed graphs in a pair-independent fashion to an embedding space over which a simple
function can predict the SED (or GED). The same embedding model is used for both G1 and G2,
which captures the prior that similar topological properties need to be considered for both graphs.
The carefully crafted prediction function serves as another inductive bias for the model, which, in
addition to enabling high generalization accuracy, preserves the key properties of Sed/Ged, which
is a major contribution of our work over existing neural approaches.
•	Indexable embeddings: The prediction function satisfies the triangle inequality over the em-
bedding space for both Sed and Ged. This allows utilization of the rich literature on index
structures (Elkan, 2003; Kwedlo & Czochanski, 2019; Ciaccia et al., 1997) to boost efficiency.
•	Accurate, Fast and Scalable: Extensive experiments on real graph datasets containing up to a
million nodes establish that NeuroSed is more accurate in both Ged and Sed when compared to
the state of the art and is more than 3 orders of magnitude faster in range and k-NN queries.
2	Preliminaries and Problem Formulation
We denote a labeled undirected graph as G = (V, E, L) where V is the node set, E is the edge set and
L : V ∪ E → Σ is the labeling function over nodes and edges. Σ is the universe of all labels and
contains a special empty label . L(v) and L(e) denote the labels of node v and edge e respectively.
G1 ⊆ G2 denotes that G1 is a subgraph of G2 . All notations used in our work are summarized in
Table 6 in the appendix. For the definitions of subgraph and graph isomorphism, refer to App. B in
the supplementary. The computation of GED relies on a graph mapping.
Definition 1 (Graph Mapping) Given two graphs G1 and G2, let G1 = (V1, E1, L1) and G2 =
(V2, E2, L2) be obtained by adding dummy nodes and edges (labeled with ) to G1 and G2 respectively,
such that |V1 | = |V2| and |E1 | = |E2|. A node mapping between G1 and G2 is a bijection π : G1 → G2
where (i) ∀v ∈ Vι, π(v) ∈ V2 and at least one of v and π(v) is not a dummy; (ii) ∀e = (vι, v2) ∈
E1, π(e) = (π(v1), (π(v2))) ∈ E2 and at least one of e and π(e) is not a dummy.
Example 1	Fig. 5b shows a graph mapping. Edge mappings can be trivially inferred, so are omitted.
Definition 2 (Graph Edit Distance (Ged) under mapping π) GED between G1 and G2 under π is
GEDπ(G1, G2) = X d(L(v), L(π(v))) + X d(L(e), L(π(e)))	(1)
v∈V1	e∈E1
2
Under review as a conference paper at ICLR 2022
where d : Σ × Σ → R0+ is a distance function over the label set.
d('ι, '2) models an insertion if '1 = e, deletion if '2 = e and replacement if '1 = '2 and neither '1
nor '2 is a dummy. We assume d tobea binary function, where d('1,'2) = 1 if '1 = '2, otherwise,
0. Our framework easily extends to more general distance functions (details in App. D).
Example 2	GED for the shown mapping in Fig. 5b is 3. The red mappings incur a cost of 1.
Definition 3 (Graph Edit Distance (Ged)) GED is the minimum distance under all mappings.
GED(G1, G2) = min	GEDπ(G1,G2)	(2)
∀π∈Φ(G1,G2)
where Φ(G1, G2) denotes the set of all possible node maps from G1 to G2.
Definition 4 (Subgraph Edit Distance (Sed)) SED is the minimum GED over all subgraphs of G2.
SED(G1, G2) = minGED(G1,S)	(3)
S⊆G2
Example 3	Revisiting Fig. 5b, GED(G1, G2) = 3. The shown mapping incurs the minimum cost.
SED(G1 , G2 ) = 1. Refer to Fig. 5a for more examples.
Problem 1 (Learning Sed) Given a training set of tuples of the form hG1, G2, SED(G1, G2)i, learn
a neural model to predict SED(Q1, Q2) on unseen graphs Q1 and Q2.
Problem 2 (Learning Ged) Given a training set of tuples of the form hG1, G2, GED(G1, G2)i, learn
a neural model to predict GED(Q1, Q2) on unseen graphs Q1 and Q2.
2.1 Properties of Sed and Ged
Observation 1 (i) GED(G1, G2) ≥ 0, (ii) SED(G1, G2) ≥ 0.
Observation 2 (i) GED(G1, G2) = 0 iff G1 is isomorphic to G2, (ii) SED(G1, G2) = 0 iff G1 is
subgraph isomorphic to G2.
Observation 3 GED is a metric if the distance function d over label set Σ is metric (He & Singh,
2006). d as defined in § 2 is metric (He & Singh, 2006). Hence, GED from § 2 is metric.
We next prove that Sed satisfies the triangle inequality in Theorem 1.
Theorem 1 SED(G1, G3) ≤ SED(G1, G2) + SED(G2, G3).
Proof: See App. D.1 for details.
3 NeuroSed
Fig. 1 presents the architecture of NeuroSed. The input to our learning framework is a pair of
graphs GQ (query), GT (target) along with the supervision data SED(GQ, GT). Our objective is to
train a model that can predict Sed on unseen query and target graphs. The design of our model
must be cognizant of the fact that computing Sed is NP-hard and high quality training data is scarce.
Thus, we use a Siamese architecture in which the weight sharing between the embedding models
boosts learnability and generalization from low-volume data by imposing a strong prior that the same
topological features must be extracted for both graphs.
Siamese Networks: In these models, there are two networks with shared parameters applied to two
inputs independently to compute representations. These representations are then passed through
another module to compute a similarity score.
3.1 Siamese Graph Neural Network
As depicted in Fig. 1a, we use a siamese graph neural network (Gnn) with shared parameters to
embed both GQ and GT . Fig. 1b focuses on the GNN component of NEUROSED. We next discuss
each of its individual components.
Pre-Mlp: The primary task of the Pre-MLP is to learn representations for the node labels (or
features). Towards that end, let xv denote the initial feature set of node v. The MLP learns a hidden
representation μG = MLP (Xv). In our implementation, Xv is a one-hot encoding of the categorical
node labels. We do not explicitly model edge labels in our experiments. NeuroSed can easily be
extended to edge labels by strategies such as using Gine (Hu et al., 2019) layers instead of Gin
layers (see App. C).
3
Under review as a conference paper at ICLR 2022
(a) Siamese architecture
Figure 1: The architecture of NeuroSed.
Graph Isomorphism Network (Gin): GIN (Xu et al., 2018b) consumes the information from the
Pre-Mlp to learn hidden representations that encode both the graph structure as well as the node
feature information. GIN is as powerful as the Weisfeiler-Lehman (WL) graph isomorphism test
(Leman & Weisfeiler, 1968) in distinguishing graph structures. Since our goal is to accurately
characterize graph topology and learn similarity, Gin emerges as the natural choice. Gin develops
its expressive power by using an injective aggregation function. Specifically, in the initial layer,
each node V in graph G is characterized by the representation learned by the MLP, i.e., hG 0 = μG.
Subsequently, in each hidden layer i, we learn an embedding through the following transformation.
hG,i = MLP I (1 + ei) ∙ hG.-i +	X hG,i-i∣	(4)
u∈NG (v)
Here, i is a layer-specific learnable parameter, NG(v) is one-hop neighbourhood of the node v, and
hG,o = μG. The k-th layer embedding is hG k, where k is final hidden layer.
Concatenate
from all
layers
NODE
FEATURE
VECTORS
(b) Gnn component
Concatenation, Pool and Post-MLP: Intuitively, hvG,i captures a feature-space representation of the
i-hop neighborhood of v. Typically, GNNs operate on node or edge level predictive tasks, such as
node classification or link prediction, and hence, the node representations are passed through an
Mlp for the final prediction task. In our problem, we need to capture a graph level representation.
Furthermore, the representation should be rich enough to also capture the various subgraphs within
the input graph so that Sed can be predicted accurately. To fulfil these requirements, we first
concatenate the representation of a node across all hidden layers, i.e., the final node embedding
is ZG = CONCAT (hGi,∀i ∈ {1,2,…,k}). This allows US to capture a multi-granular view of
the subgraphs centered on v at different radii in the range [1, k]. Next, to construct the graph-level
representation, we perform a sum-pool, which adds the node representations to give a single vector.
This information is then fed to the Post-Mlp to enable post-processing. Mathematically:
ZG = MLP(zG) = MLP X zvG
v∈V
(5)
Sed Prediction: The final task is to predict the SED as a function of query graph embedding ZGQ
and target graph embedding ZGT . The natural choice would be to feed these embeddings into another
MLP to learn SED(ZGQ, ZGT ). This MLP can then be trained jointly with the graph embedding model
in an end-to-end fashion. However, an MLP prediction does not have any theoretical guarantees.
To ensure preservation of original space properties, we introduce an inductive bias in the form of
a simple fixed computation function F(ZGQ, ZGT ) to predict the SED. By training the model to
produce embeddings ZGQ and ZGT such that F(ZGQ, ZGT ) ≈ SED(ZGQ, ZGT ), we enforce a rich
structure on the embedding space. We show that these embeddings satisfy many key properties of the
Sed (and Ged) function that existing neural algorithms fail to do (Bai et al., 2019; Wang et al., 2021;
Bai et al., 2020; Zhang et al., 2021). F is defined as follows:
F(ZGQ,ZGT)= kReLU(ZGQ -ZGT)k2= kmax {0, ZGQ - ZGT}k2
(6)
Intuitively, for those co-ordinates where the value of ZGQ is greater than ZGT ; a distance penalty is
accounted by F in terms of how much those values differ, otherwise F considers 0. This follows the
intuition that the SED accounts for those features of GQ that are not in GT . Moreover, consistent with
SED, the additional features in GT that are not in GQ , do not incur any cost. Finally, the parameters
of the entire model are learned by minimizing the mean squared error1 (here T is the training set).
L =击 X	(F (Zgq, ZGT )-Sed(Gq, Gt))2
∀hGQ,GTi∈T
(7)
1Since Sed computation is NP-hard, in App. G, we discuss a loss function based on Sed lower and upper
bounds, which are faster to compute.
4
Under review as a conference paper at ICLR 2022
Adaptation for Ged (NeuroSedG): The proposed architecture naturally extends to GED (denoted
by NeuroSedG) with a simple modification of the computation function. Specifically, instead of
Eq. 6, we use	Fg(ZGQ,ZGT)= kZGQ - ZGT k2	(8)
3.2	Theoretical Characterization
Lemma 1 The following properties hold on predicted SED F(ZGQ, ZGT ):
1.	F(ZGQ,ZGT)≥0
2.	F (ZGQ, ZGT )=0 O ZGQ ≤ ZGT
3.	F(ZGQ,ZGT) ≤F(ZGQ,ZGT0)+F(ZGT0,ZGT)
PROOF. Properties (1) and (2) follow from the definition of F itself. Property (3) follows from the
fact that we take the L2 norm (holds for any monotonic norm, details are available in App. D).
We see that F satisfies the analogue of properties of SED (§ 2.1). Conditions 1, 2, and 3 captures
non-negativity, the subgraph relation (Rex et al., 2020), and triangle inequality respectively.
Furthermore, we show in App. E a complete constructive characterization for any F which can
model Sed in the embedding space, while being translation-invariant and non-negative homogeneous.
Specifically, we show that the general form for F involves solving a constrained optimization problem
(intractable in general), but under a mild technical assumption a closed form can be obtained which
coincides with equation 6. This asserts that equation 6 is the natural choice for F, modulo the choice
of norm. L2 norm is a natural choice due to its isotropy (direction/rotation invariance) and good
empirical performance.
Lemma 2 The predicted GED, i.e., Fg is metric. (i) Non-negativity: Fg (ZGQ, ZGT) ≥ 0, (ii)
Identity: Fg (ZGQ, ZGT) =。^⇒ ZGQ =(iii) Symmetry： Fg(ZGQ, ZGT) = Fg(ZGT, ZGQ)，
(iv) Triangle Inequality: Fg (ZGQ, ZGT) ≤ Fg (ZGQ, ZT0) + Fg(ZT0, ZGT).
PROOF. Follows trivially from the fact that Fg forms a Euclidean space.
Similar to F, the form for Fg also comes naturally from some mild technical assumptions for any
embedding space function which models Ged (see App. E).
Theorem 2 The properties of SED (and GED) in § 2.1 are satisfied by the prediction functions
NEUROSED(G1, G2) (and the GED version NEUROSEDG(G1, G2)).
PROOF SKETCH. (For SED) Let E be the embedding function learned by NEUROSED’s GNN. Then
NEUROSED(G1, G2) = F(E(G1), E(G2)). Now, the relevant properties follow from Lemma 1 using
the substitutions ZGQ 7→ E(G1), ZGT 7→ E(G2), and ZGT0 7→ E(G3).
We note here that if different embedding models are used for G1 and G2, or if the distance
computations are pair-dependent (Li et al., 2019), then Theorem 2 would not hold. It is easy to verify
that the proof fails if EQ and ET are different embedding functions corresponding to the query and
the target, or if E is a function of both G1 and G2 . Hence, the siamese architecture is crucial.
Complexity analysis: Inference is linear in query and target sizes (see App. F for details).
3.3	Comparison with NeuroMatch (Rex et al., 2020):
Both NeuroSed and NeuroMatch use siamese GNNs. However, there are important differences
in (1) their objectives, (2) training and (3) inference procedures and (4) theoretical characterization.
1.	Objective: While NEUROSED is trained to predict GED and SED directly, NEUROMATCH studies
the simpler task of anchored subgraph isomorphism. Thus, NeuroSed subsumes the objective of
NeuroMatch.
2.	Training: NEUROMATCH trains the model to construct order embeddings wherein if G1 ⊆ G2,
then ∀i, ZG1 [i] ≤ ZG2 [i], where ZG denotes the embedding of graph G. To inject this bias, the
model is trained using max-margin loss. NeuroSed does not enforce order embeddings. Rather
it learns embeddings that make the proposed prediction functions (Eq. 6 and Eq. 8) effective
through RMSE loss. In the context of Sed, Eq. 6 is used in the loss function of both NeuroSed
and NeuroMatch. However, the loss function themselves are different, which results in the
proposed embedding space being considerably richer in its ability to preserve properties from the
original space. In addition, in App. E, we characterize the class of functions that may act as a
substitute for Eq. 6, and thereby perform a thorough theoretical characterization of our work.
5
Under review as a conference paper at ICLR 2022
3.	Inference: NEUROSED creates a single embedding of the query graph, based on which the SED
and Ged predictions are made. In contrast, NeuroMatch generates embeddings for each node
neighborhood in the query and target graph and makes |VQ||VT | predictions corresponding to
each pair of neighborhoods. The final decision of whether the query is a subgraph of the target is
performed using a voting mechanism.
4 Empirical Evaluation
In this section, we establish the following:
•	Subgraph Search: NEUROSED outperforms the state of the art approaches for SED prediction.
•	Graph Search: NEUROSEDG , the variation to predict GED, outperforms the state of the art
approaches for Ged prediction.
•	Scalability: NEUROSED and NEUROSEDG are orders of magnitude faster than existing approaches
and scale well to graphs with millions of nodes.
The code base and datasets are available at https://anonymous.4open.science/r/NeuroSED/.
4.1 Experimental Setup
Details on the hardware and software platform and parameters are provided in App. I.
Datasets: Table 4.1 lists the datasets used for benchmarking. Further details on the dataset se-
mantics are provided in the App. H. We include a mixture of both graph databases (#graphs
>1), as well as single large graphs (#graphs = 1). Linux and IMDB contain unlabeled graphs.
I Name ∣ Avg.∣V∣ ∣ Avg.∣E∣ ∣ ∣Σ∣ | #GraPhS ∣ Avg.∣VQ∣ ∣ ∣Avg.EQ∣ |
Dblp
Amazon
PubMed
CiteSeer
Cora_ML
Protein
AIDS
AIDS’
Linux
IMDB
1.66M
334k
19.7k
4.2k
3k
38
14
9
8
13
7.2M	8
925k	1
44.3k	3
5.3k	6
8.2k	7
70	3
15	38
9	29
7	1
65	1
1
1
1
1, 071
1, 811
700
1, 000
1, 500
15
12
12
12
11
9
7
9
8
13
14
16
11
12
11
11
7
9
7
65
25
20
15
10
5
SED in NeuroSED
5
4
0
SED in H2MN-RW
Table 1: Datasets used for benchmarking.
1
1
Figure 2: Heat Map of Sed error against query
size in Dblp. Darker means higher error.
Training (and Test) Data Generation: For GED, we uSe hquery, targeti graPh PairS from IMDB,
AIDS’, and Linux. Our SetuP iS identical to SimGnn (Bai et al., 2019) and H2MN-RW (Zhang
et al., 2021). For Sed, the Procedure iS more intricate. The detailS of dataSetS can be found in APP. H.
•	Queries: In Single large graPhS, querieS are SamPled by Performing a random BFS traverSal (dePth
uP to 5) on a randomly choSen target graPh. In addition, for AIDS, we uSe known functional
groups aS teSt querieS (See APP. G for detailS). ThiS allowS uS to benchmark NEUROSED on both
natural aS well Synthetic querieS. The average query SizeS (|VQ|, |EQ|) are liSted in Table 4.1.
•	Ground-truth: We uSe mixed integer programming method F2 (Lerouge et al., 2017) imPlemented
in Gedlib (Blumenthal et al., 2019) with a large time limit to generate ground-truth data.
Train-Validation-Test Split: We uSe 100K query-target PairS for training and 10K PairS each for
validation and teSt.
Baselines: To evaluate Performance in Ged, we comPare with SIMGNN2 (Bai et al., 2019), GENN-
A (Wang et al., 2021) and H2MN(Zhang et al., 2021). H2MN has two versions based on random
walkS (H2MN-RW) and k-hoP neighborhood (H2MN-NE). We include both.
For Sed, no neural aPProaches exist. However, H2MN and SIMGNN can be trained by rePlacing
Ged with Sed along with minor modifications in training. While NeuroMatch (Rex et al., 2020)
cannot Predict SED, it generates a violation score which can be interPreted as the likelihood of the
query being subgraPh isomorPhic to the target. The violation score can be used as a Proxy for Sed
and used in ranking of k-NN (k-NearestNeighbour) queries. Thus, NEUROMATCH comParisons are
limited to k-NN queries on SED. The changes required in to adaPt to SED are in APP. I.
2We use the PyTorch imPlementation of SimGnn given on paperswithcode. The code of all other
neural algorithms have been obtained from the resPective authors.
6
Under review as a conference paper at ICLR 2022
I Methods	I Dblp	Amazon	PubMed	CiteSeer	Cora_ML	Protein	AIDS	I I	Methods ∣	AIDS'	Linux	IMDB I
I NeuroSed	I 0.964	0.495	0.728	0.519	0.635	0.524	0.512	^∣ I NeuroSedg ∣	0.796	0.415	6.734 I
I H2MN-RW	I 1.470	1.294	1.213	1.502	1.446	0.941	0.749	^∣ I H2MN-RW I	0.994	0.734	86.077 I
									 I	ττ 2 n ∣τ、T -1-w-. I	1.000	0.319	87.594 I
I H2MN-NE	I 1.552	0.971	1.326	1.827	1.229	0.755	0.657	,I H2MN-NE I			
								」I Genn-A* I	0.907	0.267	NA I
I SimGnn	I 1.482	2.810	1.322	1.781	1.289	1.223	0.696	I I	L			
								I I	SimGnn I	1.037	0.666	66.250 I
I Branch	I 2.917	4.513	2.613	3.161	3.102	2.391	1.379	I '	l _l I	Branch I	3.322	2.474	6.875 I
I~MIP-F2	I 3.427	5.595	3.399	4.474	3.871	2.249	1.537	I I	MIP-F2 I	2.929	1.245	82.124 I
(a) Prediction of Sed.	(b) Prediction of Ged
Table 2: RMSE scores (lower is better) in (a) SED and (b) Ged. Genn-A* does not scale on
graphs beyond 10 nodes and hence the results in IMDB are not reported.
In the non-neural category, we use mixed integer programming based method MIP-F2 (Lerouge
et al., 2017) with a time bound of 0.1 seconds per pair for both GED and SED. MIP-F2 provides
the optimal solution given infinite time. In addition, we also compare with Branch (Blumenthal,
2019), which achieves an excellent trade-off between accuracy and runtime (Blumenthal et al., 2020).
B RANCH uses linear sum assignment problem with error-correction (LSAPE) to process the search
space. We use Gedlib’s (Blumenthal et al., 2019) implementation of these methods.
4.2	Prediction Accuracy of Sed and Ged
Tables 2a and 2b present the accuracy of all techniques on SED and GED in terms of Root Mean
Square Error (RMSE). In App. J, we evaluate the same performance using several other metrics such
as R2 and Mean Absolute Error (MAE). NEUROSED outperforms all other techniques in 9 out of 10
settings. The gap in accuracy is the highest in IMDB for GED, where NEUROSED is more than 10
times better than the neural baselines. A deeper analysis reveals that IMDB graphs are significantly
denser and larger than AIDS’ or Linux. Thus, computing the optimal Ged is harder. While all
techniques have higher errors in IMDB, the deterioration is more severe in the baselines indicating
that NeuroSed scales better with graph sizes.
To gain insight on whether the same patterns also hold in SED, in Fig. 2, we plot the heat map of
RMSE against query graph size in Dblp. Specifically in this plot, each dot corresponds to a query
graph GQ. The co-ordinate of a query is (SED(GQ, GT ), |VQ|). The color of a dot is the RMSE; the
darker the color, the higher is the RMSE. When we compare the heat maps of NeuroSed with the
most recent baseline H2 MN-rw, we observe that H2MN-RW is noticeably darker. Furthermore,
the dark colors are clustered on higher Sed values and large query sizes (upper-right corner). This
indicates that NeuroSed scales better with query size and higher Sed. The heatmaps of all
techniques are provided in App. L.
Range and k-NN queries: Range and k-NN queries are two of the most common database queries.
We next evaluate the performance of the various algorithms on these queries. In a range query, given
a distance threshold θ, the goal is to retrieve all database graphs that are within θ distance from the
query graph. In a k-NN query, given the query graph, we identify the k nearest neighbors from the
search space in distance-ascending order. For Sed, the search space includes all target graphs of
the database graphs, whereas for Ged, the search space constitutes of the database graphs. The
performance measures are F1-score (Range query) and Kendalls’s tau (k-NN) (Kendall, 1938) of
the predicted answer set, when compared against the ground truth.
As visible in Figs. 3a-3h, NeuroSed consistently outperforms all baselines in F1-score. Similar
trend is also visible in k-NN queries. Specifically, in Kendall’s tau (Tables 3a and 3b), barring the
exception of Ged in Linux, NeuroSed consistently scores the highest indicating best preservation
of the ranking order. Overall, this shows that accurate prediction of distance also transfers to accurate
querying accuracy. In App. K, we also report k-NN performance based on Precision@k. In this
analysis, we do not include Genn-A∗ since it is orders of magnitude slower than all neural-baselines
and provides a comparable distance accuracy to H2MN. Next, we discuss efficiency in detail.
I	Methods ∣	PubMed	CiteSeer	Cora_ML	Protein	AIDS I I	Methods	I AIDS'	Linux	IMDB I
I	NeuroSed ∣	0.90	0.90	0.91	0.75	0.80 I Γ	NeuroSedG	I 0.78	0.90	0.87 I
I	H2MN-rw I	0.87	0.88	0.88	0.70		；I				
					0.72 I r	H2MN-RW	I 0.72	0.89	0.80 I
I	H2MN-NE I	0.87	0.87	0.87	0.72	0.73 I L				
						I I	H2MN-NE	I 0.73	0.93	0.82 I
I	SimGnn ∣	0.85	0.87	0.86	0.63	0.73 I l_				
I NeuroMatch ∣	0.70	0.75	0.73	0.57	~0.59 I |_	SimGnn	I 0.70	0.84	0.70 I
(a) Ranking in Sed.	(b) Ranking in GED
Table 3: Kendall’s tau scores (higher is better).
7
Under review as a conference paper at ICLR 2022
I Methods ∣	Dblp	Amazon	PubMed	CiteSeer	Cora_ML	Protein	AIDS	I I	Methods I	AIDS'	Linux	IMDB I
I NEUROSED I	6.84	1.46	1.30	1.28	1.25	0.86	0.84	I I NEUROSEDG ∣	-0.49	0.70	0.63 I
I H2MN-RW I	44.68	23.2	25.79	27.54	29.04	19.33	9.63	^∣ I	H2MN-RW I	9.50	8.74	8.83 I
I	I									10.38	9.80	10.69 I
I H2MN-NE I	56.82	40.34	50.64	54.46	70.59	28.99	15.76	,I H2MN-NE I			
								」I	GENN-A* I	12190	1340	NA^
I	SimGnn ∣	109.56	47.68	39.80	39.40	40.73	39.02	43.83				
								_l I	SimGnn ∣	39.21	38.62	38.98 I
I Branch ∣	626.489	79.25	99.11	155.09	132.98	52.26	12.93	I I	Branch ∣	10.70	8.24	-127.90 I
I	MIP-F2 I	1979.185	861.95	606.01	827.65	790.01	881.77	360.12	I I	MIP-F2 I	593.34	191.88	1173.548 I
(a) SED.	(b) GED
Table 4: Running times of all methods in seconds per 10k pair (lower is better).
DataSetS	Range (θ = 2)				10-NN				Sampler	PubMed	CiteSeer	Amazon
	CPU		GPU		CPU		GPU					
									BFS	0.728	0.519	0.495
	L-SCan	M-Tree	L-Scan	H2MN	L-Scan	M-Tree	L-Scan	H2MN				
									Rw	0.508	0.770	0.490
PubMed	0.693	0.56	0.004	26.6	1.01	0.49	0.004	27.5				
Amazon	9.09	5.07	0.025	371	11.3	4.75	0.027	372	Rwr	0.545	0.754	0.299
Dblp	48	20.9	0.070	696	50.4	18.6	0.126	698	SHADOW	0.966	0.753	0.830
(a) Scalability	(b) Query generalization
Table 5: (a) Querying time (s) for Sed in the three largest datasets. L-Scan indicates time taken
by linear scan in NeuroSed (times differs based on whether executed on a CPU or in GPU).
M-Tree indicates time taken by NeuroSed when indexed using an adapted Metric Tree. We
only present the time of H2MN-RW since H2 MN-ne is exorbitantly slow. The full table is
provided in Table 9 in Appendix. (b) Generalization to unseen query distributions. BFS (seen)
acts as the baseline to compare against. The numbers represent RMSE.
4.3	Efficiency
Table 4 presents the inference time per 10K graph pairs. As visible, NEUROSED is up to 1800 times
faster than the non-neural baselines and up to 10 to 20 times faster than H2MN-RW, the current state
of the art in GED prediction. Also note that GENN-A* is exorbitantly slow (Table 4b). GENN-A*
is slower since it not only predicts the GED but also the alignment via an A* search. While the
alignment information is indeed useful, computing this information across all graphs in the database
may generate redundant information since an user is typically interested only on a small minority of
graphs that are in the answer set. In App. K.2, we discuss this issue in detail.
Scalability: Here, we showcase how pair-independent embeddings, and ensuring triangle inequality
leads to further boost in scalability. For this experiment, we use the three largest datasets of PubMed,
Amazon and Dblp. For each dataset, we pre-compute NeuroSed embeddings of all database
graphs by exploiting pair-independent embeddings. Such pre-computation is not possible in the neural
or non-neural baselines. Furthermore, since the predictions of NeuroSed satisfy triangle inequality,
we index the pre-computed embeddings of the database graphs using the Metric Tree (Uhlmann,
1991) index structure adapted to asymmetric distance functions. Consequently, for NeuroSed, we
only need to embed the query graph and evaluate F to make predictions at query time. Table 5a
presents the results on range and 10-NN queries. When computations are done on a GPU, NEUROSED
is more than 1000 times faster than both H2MN-RW and H2MN-NE. In the absence of a GPU,
H2MN is practically infeasible since expensive pair-dependent computations are done at query time.
In contrast, even on a CPU, with tree indexing, NEUROSED is ≈ 50 times faster than GPU-based
H2MN-RW. Note that indexing enables up to 3-times speed-up on NEUROSED over linear scan,
which demonstrates the gain from ensuring triangle inequality. These results show the benefits of
using an easily computable and parallelisable prediction function F with theoretical guarantees.
4.4	Ablation Study
In this study, we explore the impact of our inductive biases in learning from low-volume data.
We create two variants of NEUROSED: (1) NEUROSED-Dual trains the two parallel GNN models
separately without weight-sharing, and (2) NEUROSED-NN uses an MLP instead of F. Both have
strictly better representational capacity than NeuroSed, so are expected to match the performance
with infinite data. Figs. 4a-4c present the results. The RMSE of NeuroSed is consistently better
than NeuroSed-Dual, with the difference being more significant at low volumes. This indicates
that siamese structure helps. Compared to NeuroSed, NeuroSed-NN achieves marginally better
performance at larger train sizes in PubMed and CiteSeer. However, in Dblp, NeuroSed is
consistently better. The number of subgraphs in a dataset grows exponentially with the node set size.
Hence, an MLP needs growing training data to accurately model the intricacies of this search space.
In Dblp, even 100k pairs is not enough to improve upon F. Overall, these trends indicate that F
enables better generalization and scalability with respect to accuracy. Furthermore, given that its
8
Under review as a conference paper at ICLR 2022
5 O
9 9
(求)aɪoɔs 二工
(a) PubMed
10	20
Range (%)
9080
10	20
Range(%)
95
Range(%)
8060
10	20
Range(%)
8060
10	20
Range(%)
(b) CiteSeer (c) Cora_ML (d) Protein
(e) AIDS
Ooo
0 8 6
1
(％)J00LH
(f) AIDS’
ιoo
98
96
94
92
Ooo
0 9 8
1
5	10	15	20
Range(%)
(h) IMDB
今 $ 2
ωSXH
NeuroSED-NN
NeuroSED-Dual
NeuroSED
Figure 3: F1-score in range queries on Sed (a-e) and Ged (f-h). The range threshold is set as
a percentage of the max distance observed in the test set.
0.5
⅛ iδi io5
Training Pairs
ιo3 ιo4 ιo5
Training Pairs
IO3 IO4 IO5
Training Pairs
(a) Dblp
(b) PubMed
(c) PubMed
Figure 4: Ablation study to analyze the impact of siamese architecture and function F.
performance is close to an MLP, and it enables indexing, the benefits outweigh the marginal reduction
in accuracy. More ablations studies are provided in App. K.1.
4.5 Generalization to Unseen Query Distributions
We train the model by sampling queries from the graph database through BFS enumerations. How
does NEUROSED generalize to unseen distributions? Towards that end, we generate queries from
the three unseen distributions of (1) Random Walks (RW), (2) Random Walks with Restarts (RWR),
and (3) SHADOW (Zeng et al., 2021). Table 5b presents the results. As visible, the errors remain
low. Even more surprisingly, the errors on Rw and Rwr are better than the train distribution of BFS
itself. This indicates good generalization to unseen distributions. See App. O for further details on
the sampling strategies.
5	Conclusions, Limitations and Future Work
Similarity search is a fundamental operator for graph analytics. Edit distance is one of the most
popular similarity measures defined over graphs. The applicability of Sed and Ged, however, is
constrained by its exponential computation complexity. Our work is motivated by the following
question: Is it possible to design a neural framework that generalizes to both SED and GED, preserves
their key properties and is capable of learning from a low-volume training data? We show that this is
indeed possible. The efficacy and efficiency of NeuroSed stems from three algorithmic innovations:
a siamese architecture to reduce dependence on high-volume training data, a carefully designed
function to introduce an inductive bias and mimic important properties of the original space, and
pair-independent embeddings to enable massive scalability.
One limitation of our work (and all neural approaches(Rex et al., 2020; Zhang et al., 2021; Wang
et al., 2021; Li et al., 2019; Bai et al., 2019; 2020)) is the ability to predict on large query graph sizes.
While the deterioration in prediction quality with larger query sizes is manageable if the same query
sizes are seen in the train data, generalizing from small query sizes to large query sizes remain an
unsolved problem (See App. M). The ability to generalize to unseen larger query sizes is important
since computing Ged and Sed are NP-complete. Hence, generating training data on large query
graphs is computationally intractable. To further advance the field of neural graph distance learning,
this generalization ability .
9
Under review as a conference paper at ICLR 2022
6 Reproducibility Statement
Please find all code, experiments and generated data at the following anonymous link: https://
anonymous.4open.science/r/NeuroSED/.
In Appendix, we have given details that are needed to make our work more reproducible. In particular,
we have (1) additional proofs in App. D, (2) descriptions of datasets in App. H, (3) additional
definitions in App. B, (4) analyses of computation cost for Sed and Ged computation in App.
F, (5) additional details of the pipeline of our architecture in App. G, (6) additional details of the
experimental setup in App. I, and (7) additional experimental results in App. J.
References
Noga Alon, Phuong Dao, Iman Hajirasouliha, Fereydoun Hormozdiari, and SUleyman Cenk Sahinalp.
Biomolecular network motif counting and discovery by color coding. In Proceedings 16th
International Conference on Intelligent Systemsfor Molecular Biology (ISMB), pp. 241-249, 2008.
Yunsheng Bai, Hao Ding, Song Bian, Ting Chen, Yizhou Sun, and Wei Wang. Simgnn: A neural
network approach to fast graph similarity computation. In Proceedings of the Twelfth ACM
International Conference on Web Search and Data Mining, WSDM ’19, pp. 384-392, 2019.
Yunsheng Bai, Hao Ding, Ken Gu, Yizhou Sun, and Wei Wang. Learning-based efficient graph
similarity computation via multi-scale convolutional set matching. Proceedings of the AAAI Con-
ference on Artificial Intelligence, 34(04):3219-3226, Apr. 2020. doi: 10.1609/aaai.v34i04.5720.
URL https://ojs.aaai.org/index.php/AAAI/article/view/5720.
David B Blumenthal. New techniques for graph edit distance computation. arXiv preprint
arXiv:1908.00265, 2019.
David B Blumenthal, Sebastien Bougleux, Johann Gamper, and Luc Brun. Gedlib: a c++ library for
graph edit distance computation. In International Workshop on Graph-Based Representations in
Pattern Recognition, pp. 14-24. Springer, 2019.
David B Blumenthal, Nicolas Boria, Johann Gamper, Sebastien Bougleux, and Luc Brun. Comparing
heuristics for graph edit distance computation. The VLDB Journal, 29(1):419-458, 2020.
Aleksandar Bojchevski and Stephan GUnnemann. Deep gaussian embedding of graphs: Unsupervised
inductive learning via ranking. arXiv preprint arXiv:1707.03815, 2017.
SebaStien Bougleux, Luc Brun, Vincenzo Carletti, Pasquale Foggia, Benoit GaUzEe, and Mario
Vento. Graph edit distance as a quadratic assignment problem. Pattern Recognition Letters, 87:
38-46, 2017. ISSN 0167-8655. Advances in Graph-based Pattern Recognition.
Sergey Brin. Near neighbor search in large metric spaces. In Proceedings of the 21th International
Conference on Very Large Data Bases, VLDB ’95, pp. 574-584, San Francisco, CA, USA, 1995.
Morgan Kaufmann Publishers Inc. ISBN 1558603794.
Edgar Chdvez, Gonzalo Navarro, Ricardo Baeza-Yates, and Jose Marroquin. Searching in metric
spaces. ACM Comput. Surv., 33:273-321, 09 2001. doi: 10.1145/502807.502808.
Paolo Ciaccia, Marco Patella, and Pavel Zezula. M-tree: An efficient access method for similarity
search in metric spaces. In VLDB, 1997.
Evariste Daller, SebaStien Bougleux, Benoit GaUzEe, and Luc Brun. Approximate Graph Edit
Distance by Several Local Searches in Parallel. In 7th International Conference on Pattern
Recognition Applications and Methods, 2018.
Vlastislav Dohnal, Claudio Gennaro, Pasquale Savino, and Pavel Zezula. D-index: Distance searching
index for metric data sets. Multim. Tools Appl., 21(1):9-33, 2003. doi: 10.1023/A:1025026030880.
URL https://doi.org/10.1023/A:1025026030880.
Christopher L. Ebsch, Joseph A. Cottam, Natalie C. Heller, Rahul D. Deshmukh, and George Chin.
Using graph edit distance for noisy subgraph matching of semantic property graphs. In 2020 IEEE
International Conference on Big Data (Big Data), pp. 2520-2525, 2020.
10
Under review as a conference paper at ICLR 2022
Charles Elkan. Using the triangle inequality to accelerate k-means. In Proceedings of the Twenti-
eth International Conference on International Conference on Machine Learning, ICML’03, pp.
147-153,2003.
Stefan Fankhauser, Kaspar Riesen, and Horst Bunke. Speeding up graph edit distance computation
through fast bipartite matching. In Proceedings of the 8th International Conference on Graph-
Based Representations in Pattern Recognition, pp. 102-111, 2011.
Ali S. Hadi. Finding groups in data: An introduction to chster analysis. Technometrics, 34:111-112,
1991.
Huahai He and Ambuj K Singh. Closure-tree: An index structure for graph queries. In 22nd
International Conference on Data Engineering (ICDE), pp. 38-38. IEEE, 2006.
Gisli R. Hjaltason and Hanan Samet. Index-driven similarity search in metric spaces (survey article).
ACM Trans. Database Syst., 28(4):517-580, December 2003.
Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec.
Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265, 2019.
David Kempe, Jon Kleinberg, and Eva Tardos. Maximizing the spread of influence through a social
network. In Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pp. 137-146, 2003.
M. G. Kendall. A new measure of rank correlation. Biometrika, 30(1/2):81-93, 1938. ISSN 00063444.
URL http://www.jstor.org/stable/2332226.
Wojciech Kwedlo and Pawel J. Czochanski. A hybrid mpi/openmp parallelization of $k$ -means
algorithms accelerated using the triangle inequality. IEEE Access, 7:42280-42297, 2019.
AA Leman and B Weisfeiler. A reduction of a graph to a canonical form and an algebra arising
during this reduction. Nauchno-Technicheskaya Informatsiya, 2(9):12-16, 1968.
Julien Lerouge, Zeina Abu-Aisheh, Romain Raveaux, Pierre HeroUx, and SebaStien Adam. New
binary linear programming formulation to compute the graph edit distance. Pattern Recognition,
72:254-265, 2017. ISSN 0031-3203. doi: https://doi.org/10.1016/j.patcog.2017.07.029. URL
https://www.sciencedirect.com/science/article/pii/S003132031730300X.
Jure Leskovec and Rok Sosic. Snap: A general-purpose network analysis and graph-mining library.
ACM Transactions on Intelligent Systems and Technology (TIST), 8(1):1, 2016.
Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, and Pushmeet Kohli. Graph matching networks
for learning the similarity of graph structured objects. In Proceedings of the 36th International
Conference on Machine Learning, pp. 3835-3845, 2019.
Yongjiang Liang and Peixiang Zhao. Similarity search in graph databases: A multi-layered indexing
approach. In 2017 IEEE 33rd International Conference on Data Engineering (ICDE), pp. 783-794,
2017. doi: 10.1109/ICDE.2017.129.
Michel Neuhaus, Kaspar Riesen, and Horst Bunke. Fast suboptimal algorithms for the computation of
graph edit distance. In Dit-Yan Yeung, James T. Kwok, Ana Fred, Fabio Roli, and Dick de Ridder
(eds.), Structural, Syntactic, and Statistical Pattern Recognition, pp. 163-172, Berlin, Heidelberg,
2006. Springer Berlin Heidelberg. ISBN 978-3-540-37241-7.
Sayan Ranu, Bradley T Calhoun, Ambuj K Singh, and S Joshua Swamidass. Probabilistic substructure
mining from small-molecule screens. Molecular Informatics, 30(9):809-815, 2011.
Rex, Ying, Zhaoyu Lou, Jiaxuan You, Chengtao Wen, Arquimedes Canedo, and Jure Leskovec.
Neural subgraph matching, 2020.
Kaspar Riesen and Horst Bunke. Approximate graph edit distance computation by means of bipartite
graph matching. Image Vision Comput., 27(7):950-959, June 2009. ISSN 0262-8856. doi:
10.1016/j.imavis.2008.04.004. URL https://doi.org/10.1016/j.imavis.2008.04.004.
11
Under review as a conference paper at ICLR 2022
Hanan Samet. Foundations of Multidimensional and Metric Data Structures (The Morgan Kaufmann
Series in Computer Graphics and Geometric Modeling). Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA, 2005. ISBN 0123694469.
Aravind Sankar, Sayan Ranu, and Karthik Raman. Predicting novel metabolic pathways through
subgraph mining. Bioinformatics, 33(24):3955-3963, 2017.
Jeffrey K. Uhlmann. Satisfying general proximity/similarity queries with metric trees. Inf. Process.
Lett., 40:175-179, 1991.
Lichen Wang, Bo Zong, Qianqian Ma, Wei Cheng, Jingchao Ni, Wenchao Yu, Yanchi Liu, Dongjin
Song, Haifeng Chen, and Yun Fu. Inductive and unsupervised representation learning on graph
structured objects. In International Conference on Learning Representations, 2020. URL https:
//openreview.net/forum?id=rkem91rtDB.
Runzhong Wang, Tianqi Zhang, Tianshu Yu, Junchi Yan, and Xiaokang Yang. Combinatorial learning
of graph edit distance via dynamic embedding. In IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pp. 5241-5250, 2021.
Haibo Xiu, Xiao Yan, Xiaoqiang Wang, James Cheng, and Lei Cao. Hierarchical graph matching
network for graph similarity computation, 2020.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? arXiv preprint arXiv:1810.00826, 2018a.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? arXiv preprint arXiv:1810.00826, 2018b.
Peter N. Yianilos. Data structures and algorithms for nearest neighbor search in general metric spaces.
In Proceedings of the Fourth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’93,
pp. 311-321, USA, 1993. Society for Industrial and Applied Mathematics. ISBN 0898713137.
Hanqing Zeng, Muhan Zhang, Yinglong Xia, Ajitesh Srivastava, Andrey Malevich, Rajgopal Kannan,
Viktor K. Prasanna, Long Jin, and Ren Chen. Deep graph neural networks with shallow subgraph
samplers. In NeurIPS, 2021. URL https://arxiv.org/abs/2012.01380.
Zhiping Zeng, Anthony K. H. Tung, Jianyong Wang, Jianhua Feng, and Lizhu Zhou. Comparing
stars: On approximating graph edit distance. Proc. VLDB Endow., 2(1):25-36, August 2009.
Pavel Zezula, Giuseppe Amato, Vlastislav Dohnal, and Michal Batko. Similarity Search: The
Metric Space Approach, volume 32 of Advances in Database Systems. Springer, 2006. ISBN
0-387-29146-6.
Zhen Zhang, Jiajun Bu, Martin Ester, Zhao Li, Chengwei Yao, Zhi Yu, and Can Wang. H2mn: Graph
similarity learning with hierarchical hypergraph matching networks. In KDD, pp. 2274-2284,
2021.
Xiang Zhao, Chuan Xiao, Xuemin Lin, Qing Liu, and Wenjie Zhang. A partition-based approach to
structure similarity search. Proc. VLDB Endow., 7(3):169-180, November 2013. ISSN 2150-8097.
doi: 10.14778/2732232.2732236. URL https://doi.org/10.14778/2732232.2732236.
Appendix
A Inability of existing neural approaches Ged in modeling Sed
A key difference between Sed and Ged is that while Ged is symmetric, Sed is not. Hence, if the
architecture or training procedure relies heavily on the assumption of symmetry, then the modeling
capacity is compromised.
12
Under review as a conference paper at ICLR 2022
SED matrix
(a) Ged and Sed
Figure 5: (a) A sample set of graphs and their corresponding Ged and Sed matrices. (b)
Example of a graph mapping. The dashed nodes and edges represent dummy nodes and edges.
The red arrows denote either insertion or change of label.
(b) Graph Mapping
•	GMN (Li et al., 2019): The loss function of GMN computes the euclidean distance between the
embeddings and then this signal is used to learn the weight parameters (See Eq. 12 in (Li et al.,
2019)). The assumption of euclidean distance prohibits the model from learning an asymmetric
distance function. Certainly, the loss function can be changed to incorporate an asymmetric
distance measure and the performance of the architecture following this change may be studied.
However, the selection of an appropriate loss function that works well with the GMN architecture
is a research problem in itself. Apart from the loss function, the proposed architecture along with
the attention mechanism is also motivated by symmetry (note the quantifications for i and j in
Eqns. (4-8) in (Li et al., 2019)).
•	GraphSim (Bai et al., 2020): GraphSim computes a square similarity matrix across node pairs
of two graphs G1 = (V1, E1), G2 = (V2, E2) on which CNNs are applied to predict the similarity.
Since G1 and G2 may be of different sizes, the smaller graph is padded with ||V1 | - |V2|| zeros.
When the similarity matrix has a large number of zeros, it confuses the model into thinking that the
two graphs are dissimilar. The authors of GraphSim acknowledge this themselves while justifying
a padding of ||V1 | - |V2|| zeros instead of a fixed maximum size limit. We quote an exact line
from the paper:
“the similarity matrix between two small but isomorphic graphs may be padded with a lot of zeros,
potentially misleading the CNNs to predict they are dissimilar.”
In our problem, even if the query graph is significantly smaller than the target graph (tens of nodes
vs. hundreds), Sed can be 0. Thus, the above mentioned design of relying on node-to-node square
similarity matrix is not well suited.
•	SimGNN (Bai et al., 2019): SIMGNN relies on a neural tensor network and a pairwise node
comparison module to compute similarity between two graphs. The pairwise node comparison
module assumes the comparison matrix to be symmetric. Hence, in our adaptation of SimGnn for
Sed, we remove this particular module and only use the neural tensor network.
B Definitions
Definition 5 (Graph Isomorphism) Graph G1 = (V1, E1, L1) is isomorphic to G2 = (V2, E2, L2)
if there exists a bijection π : V1 → V2 such that (i) ∀v ∈ V1 : L(v) = L(π(v)), (ii) ∀e = (v1 , v2) ∈
E1 : π(e) = (π(v1), π(v2)) ∈ E2, and (iii) ∀e ∈ E1 : L(e) = L(π(e)).
Subgraph isomorphism is defined analogously by using an injection instead of a bijection.
Definition 6 (Nearest Subgraph) S is a nearest subgraph to G1 in G2 if SED(G1, G2) =
GED(G1,S).
C Handling Edge Labels
A straight-forward extension of NeuroSed to handle edge labels is to use Gine (Hu et al., 2019)
layers instead of Gin layers. Equation 4 can be modified as follows:
hV,i = MLP I (1 + d) ∙ hG,i-i + ∑ ReLU(hG,τ + eG,u)1	(9)
u∈NG (v)
where evG,u denotes the vector representation of the edge label for edge (v, u).
13
Under review as a conference paper at ICLR 2022
It is also possible to represent edge labels in the current architecture by constructing a new graph
G0 with a vertex wv,u for every edge (v, u) and edges from v to wv,u and from wv,u to u. The node
representation for wv,u is a vector representation for the label on the edge (v, u) (suitably padded
for uniformity). Furthermore a new dimension is introduced which separates these new nodes from
the old ones and this dimension is set to 1 if the node is wv,u for some (v, u) ∈ EG and 0 otherwise.
NeuroSed (and NeuroSedG) can then be trained with such modified graphs as inputs, but using
the ground truth Sed (respectively Ged) for the original graphs in the loss function.
D Additional Proofs
D.1 The Triangle Inequality of Sed: Proof of Theorem. 1
Our proof relies on two lemmas.
+
Lemma 3 Let d : Σ X Σ → Rj be a distancefunction over Σ, where (i) d('1,'2) = 0 if '1 = e, and
(it) d('1,'2) = d('ι, '2) otherwise; thefollowιng holds: SED(G1, G2) = GED(G1, G2), where GED
denotes GED with d as the label set distance function. In simple words, the SED between two graphs
is equivalent to GED with a label set distance function where we ignore insertion costs.
PROOF. It suffices to prove (i) SED(G1, G2) ≥ GdED(G1, G2) and (ii) GdED(G1, G2) ≥ SED(G1, G2).
(i)	Let S = (VS, ES, LS) ⊆ G2 be the subgraph minimizing SED(G1, S) (Recall Eq. 3). Consider the
mapping π from G1 to S corresponding to SED(G1, S) (and hence GED(G1, S) as well). We extend
π to define a mapping πb from G1 to G2 by mapping of all nodes in set V2 \ VS to dummy nodes in
G1 ; the edge mappings are defined analogously.
Under this construction, SED(G1, S) = GED(G1, G2) = GdEDπb(G1, G2) ≥ GdED(G1, G2). This
follows from the property that under d, insertion costs are zero, that is d(e, `) = 0. Thus, the
additional mappings introduced in πb do not incur additional costs under d.
(ii)	Consider S ⊆ G2 and a mapping π from G1 to S such that GEDπ(G1, S) = GED(G1, G2). The
existence of such a subgraph is guaranteed (See Lemma 5 in Supplementary). From the definition
of GED, GEDπ(G1, S) ≥ GED(G1, S). Furthermore, since S ⊆ G2, GED(G1, S) ≥ SED(G1, G2).
Combining all these results, we have GED(G1, G2) ≥ GED(G1, S) ≥ SED(G1, G2).
Hence, the claim is proved.
Lemma 4 d, as defined in Lemma 3, satisfies the triangle inequality.
Proof. We need to show d('ι, '3) ≤ d('ι, '2) + d('2,'3). We divide the proof into four cases:
(i)	None of '1, '2, '3 is e. Hence, d('ι, '3) = d('ι, '3) and the triangle inequality is satisfied.
(ii)	`1 = e. The LHS is 0 and hence the triangle inequality is satisfied.
(iii)	'1 6= e and '2 = e. LHS≤ 1 and RHS= 1. Hence, satisfied.
(iv)	only '3 = e. Here, LHS= 1 and RHS≥ 1.
These four cases cover all possible situations and hence, the triangle inequality is established.
From Lemma 3, we know SED(G1, G2) = GdED(G1, G2). Combining obs. 3 with Lemma 4, GdED
satisfies the triangle inequality. Thus, Sed satisfies the triangle inequality.
D.2 Proof of Lemma 5
Lemma 5 There exists a subgraph S ofG2 and a node map π from G1 to S such that GEDπ (G1, S)
GED(G1, G2).
ɪ-ʌ     ɪ , / 1	F	Γ-	C . C	T ,	/ C C ∖ T = 1	7 1 , F
Proof. Let ∏ be a node map from G1 to G corresponding to GED(G1, G2). Let h∖, ∙∙∙ , 0 be the
nodes of G which are inserted in ∏0. Construct subgraph S of G by removing nodes h∖,…，0 and
their incident edges from G2. Let π be the node map from G1 to S which is obtained by removing
001, ∙∙∙ ,h∣ from the domain and h∖,…，0 from the co-domain of π. Since insertion costs are 0 in d
and π contains only non-insert operations, then GEDπ (G1, S) = GED(G1, G2). Hence, the claim is
proved.

Corollary 1 There exists a nearest subgraph (Def. 6) S to G1 in G2 such that S ⊆ G1 .
14
Under review as a conference paper at ICLR 2022
Symbols	Descriptions
V	The set of nodes
E	The set of edges
L	The labeling function over nodes and edges
Σ	The universe of all labels
e	The special empty label
L(V)	The label of node v
Le	The label of edge e
π	The node mapping (bijection)
-Ged-	The graph edit distance
SED	The subgraph edit distance
d	The distance function over the label set
GQ	The query graph
GT	The target graph
Table 6: Frequently used symbols
PROOF. The node map π in the above proof contains only non-insert operations. So, S in the above
proof is a subgraph of G1 ignoring node and edge labels. Since equality holds in Lemma 3, this π
gives an optimal edit distance. Thus, S in the above proof is also a nearest subgraph to G1 in G2 .
D.3 More General Distance Functions
Ged and Sed can be viewed as error-tolerant graph and subgraph matching respectively. In practice,
the edit costs are used to model the acceptable deviations from ideal graph patterns. The more tolerant
we are towards a certain kind of distortion, the smaller is its corresponding edit cost. Concrete
values for the edit costs are application-dependent. In this paper we have discussed binary distance
functions which impose a cost of 1 for unequal labels and 0 for equal labels as these are the most
popular distance functions used in literature. However our framework is also suitable for more general
distance functions.
The results for GED directly extend to any metric distance function over the label set. The relevant
proofs mentioned in the paper are directly applicable.
The results for SED can be extended to metric distance functions d over Σ for which replacement is
no more expensive than deletion, i.e. d('ι, '2) ≤ d('ι, e)V'ι, '2 ∈ Σ. A replacement can be seen
as a compound edit operation consisting of deletion(s) followed by insertion(s). If a replacement
cost is too high, replacement can be better achieved by deletion(s) followed by insertion(s), of which
insertion(s) do not contribute to the cost in Sed. Hence, this assumption is general enough to capture
a large class of interesting distance functions which might be used for the Sed prediction task. We
show a generalization of Lemma 4 to such distance functions d. The other proofs mentioned in the
paper are directly applicable.
Lemma 6 Let d : Σ X Σ → R+ be a metric and d('1,'2) ≤ d('ι, e)∀'1, '2 ∈ ∑. As in Lemma
+
3, let d : Σ × Σ → R0+ be a distance function over Σ, where (i) d('1, '2) = 0 if '1 = e, and (ii)
d('1 , '2 ) = d('1 , '2 ) otherwise. Then, d satisfies the triangle inequality.
PROOF. We need to show d('1, '3) ≤ d('1, '2) + d('2, '3). Consider the three cases:
(i)	'1 = e. LHS = 0, RHS = d('2, '3) ≥ 0 by non-negativity of d. Hence, LHS ≤ RHS.
(ii)	'1 6= e and '2 = e. LHS = d('1, '2), RHS = d('1, e). Hence, LHS ≤ RHS by assumption.
(iii)	'1 6= e and '2 6= e. LHS = d('1, '3), RHS = d('1, '2) + d('2, '3). LHS ≤ RHS by triangle
inequality for d.
These cases cover all possible situations. Hence, triangle inequality is established.
D.4 Data Augmentation
Observation 4 Let S be a nearest subgraph (Def. 6) of G1 in G2. Let S0 be a subgraph of G2
which is also a supergraph of S. Then S is also a nearest subgraph of G1 in S0 and SED(G1, S0) =
SED(G1,G2).
15
Under review as a conference paper at ICLR 2022
PROOF. The set of subgraphs of S0 is a subset of the set of subgraphs of G2. S achieves the minimum
GED(G1, S) among all subgraphs of G2 and so also the minimum GED(G1, S) among all subgraphs
ofS0. Thus,SED(G1,S0) =SED(G1,G2) =GED(G1,S).
Observation 4 gives a natural strategy for data augmentation. Once we have SED(G1, G2) and
a nearest subgraph S from the expensive non-neural computation of GED(G1, G2), we can use
(G1, S0, SED(G1, G2)) for all S ⊆ S0 ⊆ G2 as training examples.
D.5	THE TRIANGLE INEQUALITY FORF
This is referred to in the proof for Lemma 1.
Lemma 7 F(ZGQ, ZGT) ≤ F(ZGQ, ZGT0) + F(ZGT0, ZGT), where F(x,y) = kReLU(x - y)k
for any monotonic norm k.k.
PROOF. Let x, y ∈ Rn. We observe that (ReLU(x) + ReLU(y))i = ReLU(x)i + ReLU(y)i =
ReLU(xi) + ReLU(yi) ≥ ReLU(xi + yi) = (ReLU(x + y))i. Since k.k is monotonic, this implies
kReLU(x) + ReLU(y)k ≥ kReLU(x + y)k.
Using the triangle inequality for k.k, we get kReLU(x)k + kReLU(y)k ≥ kReLU(x)+ReLU(y)k ≥
kReLU(x + y)k).
Substituting x = ZGQ - ZGT0,y = ZGT0 - ZGT, we get, kReLU(ZGQ - ZGT0)k + kReLU(ZGT 0 -
ZGT)k ≥ kReLU(ZGQ - ZGT)k
This implies that F (ZGQ , ZGT0 ) + F(ZGT0 , ZGT ) ≥ F (ZGQ , ZGT ). Our claim is proved.
E FURTHER CHARACTERIZATION OF F
Let E : G → Rd , where G is the space of graphs, be the learned embedding function and F :
Rd × Rd → R be the embedding space prediction function. We aim to have the graph space
prediction function P : G × G → R, defined by P(G, H) = F (E(G), E(H)), to be able to
approximate SED : G × G → R in the best possible way, while preserving the key theoretical
properties listed below:
SED(G, H) ≥ 0	(10)
Sed(G, H) = 0 q⇒ G ⊆ H	(11)
SED(G, H) ≤ SED(G, T) + SED(T, H)	(12)
We allow E to be arbitrary. Then, P satisfies the analogous properties
P(G,H) ≥0	(13)
P(G,H) ≤ P(G,T)+P(T,H)	(14)
if and only if F satisfies
F(g,h) ≥0	(15)
F(g, h) ≤ F(g, t) + F(t, h)	(16)
For the analogue to property in Eq. 11, we first define the subgraph prediction function PS : G × G →
{0, 1} as PS (G, H) = [[∀i : E(G)[i] ≤ E (H)[i]]]. PS is motivated by NEUROMATCH (Rex et al.,
2020) where a subgraph prediction function of this form satisfies the key properties of subgraph
isomorphism: transitivity, anti-symmetry, intersection set and non-trivial intersection. Hence, P must
satisfy:
P(G, H)=0 o PS(G, H) = 1	(17)
Again, E being arbitrary, Eq. 17 holds if and only if, F satisfies:
F(g, h)=0 ^⇒ ∀i : g[i] ≤ h[i]	(18)
Any F that satisfies Eqs. 15, 16 and 18 is acceptable in our framework. However, it is not trivial to
characterize such functions. To show a complete constructive characterization we add two important
properties in our desiderata for F. To motivate these properties, we first discuss about Fg in GED
and then Sed.
16
Under review as a conference paper at ICLR 2022
E.1	GED:
For Ged, we require the following three properties:
Fg(g,h) ≥0	(19)
Fg (g, h) = 0 Q⇒ ∀i : g[i] = h[i]	(20)
Fg(g,h) ≤Fg(g,t)+Fg(t,h)	(21)
With these properties in Eqs. 19, 20, and 21; Fg is a metric. However, giving a complete constructive
characterization of metrics is not trivial. To achieve this, we establish an important connection of
metrics on vector spaces to norms. Every norm k.k gives a metric (x, y) 7→ kx - yk. Moreover for
a metric, there exists a norm k.k such that the metric can be expressed as (x, y) 7→ kx - yk, if and
only if the metric is translation invariant and homogeneous. Thus, we add these properties to the
desiderata for Fg :
Fg(g+k,h+k) =Fg(g,h),∀k ∈Rd	(22)
Fg(rg, rh) = |r|Fg(g, h), ∀r ∈ R	(23)
Observation 5 The followings give a complete constructive characterization of Fg satisfying Eqs.
19-23:
1.	Fg is (x, y) 7→ kx - yk for some norm k.k over the vector space Rd.
2.	For any norm k.k over the vector space Rd, (x, y) 7→ kx - yk is a valid Fg.
Hence, the L2 norm is a natural choice because it is isotropic (i.e. direction/rotation invariant).
E.2 Sed:
For SED, we add translation invariance and non-negative homogeneity to the desiderata, to make
a complete constructive characterization feasible:
F(g+k,h+k) = F(g,h),∀k ∈ Rd	(24)
F(rg, rh) = rF (g, h), ∀r ∈ R0+	(25)
Note that non-negativity of r in Eq. 25 is necessary to be consistent with Eq. 11. If r is negative,
then with r = -1, we will have x ≤ y =⇒ F(x, y) = 0 =⇒ F (-x, -y) = 0 =⇒ -x ≤
-y =⇒ x ≥ y, which is a contradiction for x 6= y .
For these two properties, translation invariance and non-negative homogeneity, we provide intuitive
interpretations in the context of SED. The translation invariance would imply that if graphs G and
H are modified in the same way then the SED(G, H) will be preserved. The positive homogeneity
would mean that if the costs are scaled by a positive factor, then the Sed is also scaled by the same
positive factor.
From Eq. 24, we have F(g, h) = F(g - h, 0) = p(g - h), where p : Rd → R is defined as
p(x) = F(x, 0). Then Eqs. 16 and 25 are satisfied if and only if:
p(x + y) ≤ p(x) + p(y)	(26)
p(rx) = rp(x), ∀r ∈ R0+	(27)
Here, Eq. 26 reflects subadditivity property and Eq. 27 is nonnegative homogeneity. With these
properties, p is an asymmetric seminorm. Additionally, Eq. 18 is satisfied if and only if:
p(x) = 0 ^⇒ ∀i ： x[i] ≤ 0	(28)
We call this Eq. 28 as rectification property and refer to the seminorms satisfying this property as
rectified seminorms.
Our claim is that F satisfies all the desired properties presented earlier, if and only if p is a rectified
asymmetric seminorm. Thus, we have the following complete characterization for F.
Observation 6 The followings give a complete characterization for F:
17
Under review as a conference paper at ICLR 2022
1.	F is (g, h) 7→ p(g - h) for some rectified asymmetric seminorm p.
2.	For any rectified asymmetric seminorm p, (g, h) 7→ p(g - h) is a valid F.
Now, we describe the following characterization for p. Recall that in d-dimensional vector spaces,
asymmetric seminorms are in one-to-one correspondence with convex sets containing the origin.
Specifically, every asymmetric seminorm p corresponds to a convex set B, containing the origin,
called the dual unit ball of p. Furthermore, p and B are related as follows:
p(x) = sup Z ∙ X	(29)
z∈B
Additionally, p is symmetric if and only if B is symmetric about the origin and p is positive definite
(i.e. p(x) = 0 =⇒ x = 0) if and only if B contains the origin in its topological interior. Note that
monotonic norms are also asymmetric seminorms.
Lemma 8 (Rectified dual unit ball) Eq. 28 holds if and only if B satisfies the following:
∀z ∈ B : ∀i : z[i] ≥ 0	(30)
In other words, the necessary and sufficient conditions say that B lies completely in the positive
orthant.
Proof.
1.	( =⇒ ) If z [i] < 0 for some i and some z ∈ B, then taking x in the negative orthant, with
χ[i] having a sufficiently large magnitude, gives positive Z ∙ x, which leads to a positive p(x),
contradicting the requirement that p(x) = 0 when ∀i : x[i] ≤ 0.
2.	( ^= ) If ∀z ∈ B : ∀i : z[i] ≥ 0, then for any X in the negative orthant, Z ∙ X ≤ 0, ∀z ∈ B.
0 ∈ B, so the upper bound of 0 is achievable, hence p(x) = 0.
Note that, Lemma 8 implies that the origin is at the boundary of B, which negates the positive
definiteness of p. Hence, every valid p can be constructed by first choosing a convex set B in the
positive orthant of Rd and then defining p(X) using Eq. 29. Conversely, any such construction
gives a valid p. However, depending on B , it may not be always be feasible to simplify or solve the
optimization problem of Eq. 29. In the special case where B is restricted to polytopes, Eq. 29 is a
Linear Program. With recently proposed differentiable Linear Programming layers, B could be learnt
in an end-to-end fashion via backpropagation (with non-negativity constraints on the variables to
enforce the positive orthant). However, this adds more parameters to learn and hyperparameters to
tune, and makes training and inference significantly costlier. Instead we add a different, and arguably
milder, restriction on B, to get a closed form for Eq. 29 in terms of norms. Specifically, we assume
that B is such that mirroring it on all orthants results in a convex set. Not surprisingly, this closed
form coincides with the form of F used in the paper. This establishes that our embedding space
prediction function is a natural choice for modelling Sed.
First, we show a characterization for the dual unit ball of a monotonic norm. Recall that a norm k.k
on the vector space	Rd is said to be monotonic if (∀i	:	|X0[i]|	≥ |X[i]|)	=⇒	(kX0k	≥	kXk).	This
characterization allows us to construct the dual unit ball of a rectified asymmetric seminorm p from
the dual unit ball of a monotonic seminorm k.k, and vice versa, such that p(X) = kmaX(0, X)k.
Lemma 9 Let k.k be a norm over Rd. Let B0 be its dual unit ball. k.k is monotonic if and only if the
following holds: ∀x : ∀i : z[i]x[i] ≥ 0, where, Z ∈ arg maXz∈B0 Z ∙ X.
Proof.
1.	( =⇒ ) Assume that Z [i]X[i] < 0 for some i and X. Wlog, assume X[i] > 0. Then, by
assumption, Z[i] < 0. Construct X0 by replacing X[i] in X0 with X[i] + ∆, with ∆ > 0. Let
Z0 ∈ arg maXzθ∈Bo Z ∙ x0. By continuity, for sufficiently small ∆, there is a z0, which is arbitrarily
close to Z. In particular, for such ∆, Z0[i] < 0. This gives Z ∙ x0 < z0 ∙ x. But monotonicity gives
Z0 ∙ x0 ≥ Z ∙ x and maximality of Z for X gives Z ∙ X ≥ z0 ∙ x, which on chaining give z0 ∙ x0 ≥ z0 ∙ x.
This contradicts the previous inequality.
2.	( ^= ) Consider X and x0 with |x[i]| ≤ ∣x0[i]∣, ∀i. Let Z ∈ argmaXz∈B0 Z ∙ x, such that
Z[i]x[i] ≥ 0,∀i. Then ∣∣x0k ≥ Z ∙ x0 ≥ Z ∙ X = ∣∣xk. Hence, k∙∣∣ is monotonic.
18
Under review as a conference paper at ICLR 2022
Lemma 10 Let k.k be a monotonic norm. Then x 7→ k max(0, x)k is a rectified asymmetric
seminorm.
PROOF. Let B0 be the dual unit ball of a monotonic norm k.k. Let B be the positive orthant of B0.
Clearly B is convex and contains the origin, so let B be the dual unit ball of a rectified asymmetric
seminorm p (see Lemma 8). We show that k max(0, x)k = p(x), which completes the proof.
k max(0, x) k = sup Z ∙ max(0, x)
=sup z ∙ max(0, x)[max(0, x) ≥ 0]
z∈B
=sup z ∙ X
z∈B
= p(x)
Here, max(0, x) ≥ 0, as by Lemma 9 it suffices to consider z’s in the positive orthant, where B
coincides with B0. Additionally, for x[i] < 0 as the supremum is at z[i] = 0; max(0, x) coincides
with x at i : x[i] ≥ 0.
Lemma 11 Let p be a rectified asymmetric seminorm with a dual unit ball B such that mirroring B
on all orthants results in a convex set. Then p is x 7→ k max(0, x)k for some monotonic norm k.k.
PROOF. Let B be the dual unit ball of p. Let B0 = {z : |z| ∈ B}. B0 is convex (by mirroring
assumption), symmetric about the origin, and contains the origin in its topological interior (except
in degenerate cases which we ignore), so B0 is the dual unit ball for a norm k.k. For any x, let
z ∈ arg maxz∈b,Z ∙ x, and consider z0 obtained from Z by keeping the magnitudes of Z [i]'s but taking
the sign of x[i]'s. By construction z0 ∈ B0, and z0 ∙ X ≥ Z ∙ x, which gives z0 ∈ argmaXz∈B0 Z ∙ x.
So by Lemma 9, k.k is a monotonic norm. Now, B is the positive orthant of B0, so by the reasoning
of Lemma 10, p(x) = k max(0, x)k, and it completes the proof.
Discussion: Lp norms are natural choices for the monotonic norms k.k. For L1 norm, the dual unit
ball is a hypercube with side length 2 units and centered at the origin. For L2 norm, the dual unit
ball is a hypersphere of unit radius centered at the origin. The dual unit balls of the corresponding
rectified asymmetric seminorms are simply the positive orthants of the hypercube and the hypersphere
respectively, which easily satisfy the mirroring assumption. We choose L2 norm because of it’s
isotropy, although other Lp norms are also likely to work as well as the L2 norm. In fact, any rectified
asymmetric seminorm whose dual unit ball is obtained by taking the positive orthant of a monotonic
norm’s dual unit ball will satisfy the mirroring assumption, and will therefore be expressible in the
form k max(0, x)k.
F Computation Cost of Sed and Ged Inference
For this analysis, we make the simplifying assumption that the hidden dimension in the Pre-Mlp,
GIN and Post-MLP are all d. The average density of the graph is g. The number of hidden layers in
Pre-MLP, and Post-MLP are L, and k in GIN.
The computation cost per node for each of these components are as follows.
•	Pre-MLP: The operations in the MLP involve linear transformation over the input vector xv of
dimension ∣Σ∣, followed by non-linearity. This results in O(∣V∣(∣Σ∣∙ d + d2L)) cost.
•	Gin: GIN aggregates information from each of the neighbors, which consumes O(d ∙ g) time. The
linear transformation consumes an additional O(d2) time. Applying non-linearity takes O(d) time
since it is a linear pass over the hidden dimensions. Finally these operations are repeated over each
of the k hidden layers, results in a total O(k(d2 + dg)) computation time per node. Across, all
nodes, the total cost is O(|V|kd2 + |E |kd) time. The degree g terms gets absorbed since each edge
passes message twice across all nodes.
•	Concatenation: This step consumes O(kd) time per node.
•	Pool: Pool iterates over the GIN representation of each node requiring O(|V|dk) time.
•	Post-Mlp: The final MLP takes dk dimensional vector as input and maps it to a d dimensional
vector over L layers. This consumes O(kd2 + d2L) time.
19
Under review as a conference paper at ICLR 2022
Combining all these factors, the total inference complexity for a graph is O(∣V∣(∣Σ∣∙ d + d2L + kd2) +
|E|kd). This operation is repeated on both the query and target graphs to compute their embeddings,
on which distance function F is operated. Thus, the final cost is O(n(∣∑∣∙ d + d2L + kd2) + mkd),
where n = |VQ | + |VT | and m = |EQ | + |ET |.
G Pipeline Details
Additional Model Details: We use residual connections across blocks of two GIN convolution layers.
This eases the flow of gradients to the earlier layers by skipping the intermediate non-linearities. We
observed about 5x speedup in convergence and a marginal improvement in the generalization error
with residual connections. We use 8 layers, 64 hidden dimensions and 64 embedding dimensions.
We use a single layer for pre-MLP and 2 layers with ReLU non-linearity for the other MLP’s.
Data generation: We generate training data under two different settings (described in § 4). SED
computation is NP-hard, and to the best of our knowledge, there is no existing method which
computes Sed directly. Therefore, we generate the ground truth Sed using existing methods on Ged
by leveraging Lemma 3 which establishes the relationship between Sed and Ged. More specifically,
we use time limited mixed integer programming method MIP-F2 (Lerouge et al., 2017), which
provides us with the best so far lower bound and upper bound for SED. The time limit is kept at 60
seconds per pair. Each pair is run with 64 threads on a 64 core machine. This gives sufficiently tight
bounds (with exact values for a large majority of pairs). For evaluation, we use SED = (LB + UB)/2
as the ground truth, where Lb and Ub are the lower and upper bounds respectively. Fig. 6 presents a
visualization of tightness of Sed bounds.
0.05
0.00
0	20	40
SED
(a) PubMed
Figure 6: Mean Relative Error in Generated Data. Here Range = UB - LB, and SED = (LB + UB)/2.
(e) Cora_ML
Real queries for AIDS: The queries for AIDS constitute of known functional groups. They have
been compiled from Table 1 in ‘‘Mining statistically significant molecular substructures for efficient
molecular classification S Ranu, AK Singh - Journal of chemical information and modeling, 2009.”
Loss Function: While ZGQ and ZGT be the embeddings for the query graph GQ and target graph
GT respectively, we use the following function (for training):
((F(Zgq, ZGT) - u)2, F(Zgq, ZGT) >u
H(ZGQ,ZGT,l,u)= 0,	l≤F(ZGQ,ZGT) ≤u
[(l-F(Zgq, ZGT))2,	F(Zgq, ZGT) <l
where l and u are the lower and upper bounds to SED in our generated data. With H, the parameters
of the entire model are learned by minimizing the mean squared error.
L = ɪ X	H(Zgq, Zgt,l,u)
∀hGQ,GT,l,ui∈T
(31)
Here, T is the set of all training samples.
Training details: NEUROSED, NEUROSED-NN and NEUROSED-Dual were trained using the
AdamW optimizer with a weight decay of 10-3. A batch-size of 200 graph pairs per mini-batch
was used. The learning rate schedule was cyclic. In each cycle, learning rate was increased from 0
to 10-3 for 2000 iterations and then decreased back to 0 for 2000 iterations. Training was stopped
when the best validation loss did not improve for 5 cycles. We observe that a cyclic learning rate
schedule usually gives faster and better convergence than the other attempted training schemes. A
cyclic learning rate schedule allows using higher learning rates and thus speeds up training. It offers
some regularization due to the high learning rates seen in each cycle encouraging the optimizer to find
flatter regions of minima, giving better generalization. It is also robust to hyper-parameter settings.
20
Under review as a conference paper at ICLR 2022
Subgraph search via neighborhood decomposition:
For large target graphs, it is infeasible to compute the SED. In this paper, we proposed a neighborhood
decomposition scheme to handle subgraph similarity queries in the following setting: compute the
SED between the query and the k-hop neighborhood centered at each node in the target graph for
some suitable k. The minimum SED thus computed can be considered as an approximation for the
true Sed. Ideally, such an Sed would be an upper bound to the true Sed. In our experiments the
minimum Sed is usually small. This is expected in general as the large targets graphs are likely
to contain small query graphs (or their small variations). Thus the neighborhood decomposition is
satisfactory for Sed computation between small queries and large targets in practice. One practical
advantage of using neighborhood decomposition is that it facilitates locating the region of the target
graph where the nearest subgraph is found. We rank these neighbourhoods and that allows us to
consider regions of the target graph in order of relevance. This generates much smaller candidate sets
and traditional expensive methods can be used to refine these candidate sets further.
In this section, we seek to give further insight into the efficacy of neighborhood decomposition and
design guidelines for choosing the right value of k depending on the expected query distribution. In
practice, we are concerned only with connected subgraphs. Assuming that all the nearest subgraphs
(Def. 6) are connected leads to the following result:
Theorem 3 Assuming that the nearest subgraph to the query is a connected graph, a nearest
subgraph S to G1 in G2 can be found in an l/2-hop neighborhood Nv, centered at some v ∈ VG2,
where l is the length of the longest path in G1 .
PROOF. By Corollary 1, there exists a nearest subgraph S of G1 in G2 which is also a subgraph of
G1 ignoring node and edge labels. So, the diameter of S is ≤ the length l of the longest path in G1.
Since S ⊆ G2, S is contained in some l/2 neighborhood of G2.
Finding the length of the longest path is NP-hard. However, we do not need to find the exact length of
the longest path: any upper bound suffices. Moreover we do not even need the length of the longest
path for l. The nearest subgraph having a diameter equal to l is rare. In practice, the diameter of the
nearest subgraph is unlikely to be much higher than the diameter of the query itself.
In conclusion, subgraph similarity search suffers from the problem of an exponential search space of
subgraphs. We resolve this issue in two ways: the neighborhood decomposition allows us to consider
a number of subgraphs linear in the number of nodes in the target, all of which are much smaller than
the target graph itself. Second, our neural model learns to predict the Sed directly with the small
neighborhood sizes which make training data generation as well as neural processing using GNNs
computationally feasible.
H Datasets
Dblp: Dblp is a co-authorship network where each node is an author and two authors are connected
by an edge if they have co-authored a paper. The label of a node is the venue where the authors has
published most frequently. The dataset has been obtained from https://www.aminer.org/citation.
Amazon: Each node in Amazon represents a product and two nodes are connected by an edge if
they are frequently co-purchased. The graph is unlabeled and hence equivalent to a graph containing
a single label on all nodes. The dataset has been downloaded from (Leskovec & SosiC, 2016).
PubMed: PubMed dataset is a citation network which consists of scientific publications from
PubMed database pertaining to diabetes classified into one of three classes.
Protein: Protein dataset consists of protein graphs. Each node is labeled with a one of three
functional roles of the protein.
AIDS: AIDS dataset consists of graphs constructed from the AIDS antiviral screen database. These
graphs representing molecular compounds with Hydrogen atoms omitted. Atoms are represented as
nodes and chemical bonds as edges.
CiteSeer: CiteSeer is a citation network which consists of scientific publications classified
into one of six classes. Generally a smaller version is used for this dataset, but we use the larger
version from (Bojchevski & Gunnemann, 2017).
21
Under review as a conference paper at ICLR 2022
Cora_ML: Cora dataset is a citation dataset consisting of many scientific publications classified into
one of seven classes based on paper topic. Cora_ML is a smaller datset extracted from Cora (Bo-
jchevski & Gunnemann, 2017).
AIDS’: AIDS’ dataset is another collection of graphs constructed from the AIDS antiviral screen
database. The graphs and their properties differ from those in AIDS. These graphs also represent
chemical compound structures.
Linux: Linux dataset is a collection of program dependence graphs, where each graph is a function
and nodes represent statements while edges represent dependency between statements.
IMDB: IMDB dataset is a collection of ego-networks of actors/actresses that have appeared together
in any movie.
We use the same versions of AIDS’, Linux, and IMDB as used in (Bai et al., 2019).
I Experimental Setup
Hardware Details: We use a machine with an Intel Xeon Gold 6142 processor and GeForce GTX
1080 Ti GPU for all our experiments.
Baselines: Here we describe the changes made to neural baselines to adapt to SED problem.
•	SIMGNN (Bai et al., 2019): We train the model with LB+UB as the ground truth value of SED (LB
and UB are lower and upper bounds for SED respectively). Now let G1 be the query graph, and G2
be the target graph, we use nSED(Gι, G2) = SED∣G1,G2) instead of nGED(G1, G2)=(雷耳普/? as
the normalized value of similarity in SimGnn. This is because the absolute value of Sed scales
with the size of the query, as opposed to Ged, which scales with size of the query as well as the
target.
•	NEUROMATCH (Rex et al., 2020): We use the un-anchored version of NEUROMATCH trained on
its original task (subgraph isomorphism decision problem) using its own training framework. For
ranking, we use the violation scores it produces directly rather than thresholding the violation and
predicting subgraph isomorphism as intended. We change the size limits for sampling query/target
graphs in training to better fit our data distribution.
Querying for Sed: We embed the neighbourhoods of the nodes in the given graph (or set of graphs)
and store the embeddings (denoted by set ZGT ) accordingly. For a new query GQ , we first obtain
its embedding ZGQ. Based on the stored embeddings ZGT , we compute SED(ZGQ , ZGT ) for all
ZGT ∈ ZGT using the function F(ZGQ, ZGT ). We sort the targets based on the obtained SED values
and it can trivially answer top-k or range queries. From the selected targets one can extract the nearest
subgraph by running a traditional algorithm to compute Sed and use the returned node mapping to
interpret it. An alternative is to choose a larger (k0, where k0 > k) top set and use traditional methods
to further filter these. Note that usually the graphs required in the top-k or range queries are much
smaller than the total number of targets, so using our model to filter out the unimportant targets from
the target candidate set can save significant amount of resources.
Software details: We used Pytorch and Pytorch-Geometric for implementation. All experiments
were done using Jupyter Notebooks. The full code, notebooks for all experiments and the generated
data are available on an anonymous github repository 3.
License: The provided code and data are licensed under CC-BY-SA.
J Additional Results
J.1 Other measures for predicting Sed and Ged
In Section 4.2, we present the accuracy of all techniques on SED and GED in terms of Root Mean
Square Error (RMSE). Here we evaluate the same performance using several other metrics such as
Mean Absolute Error (MAE) and R2. Tables 7a and 7b show the MAE scores for SED and GED
respectively. Tables 8a and 8b present the results based on R2 results. Consistent with the previous
results, NeuroSed outperforms all techniques both in Sed and Ged in these two measures except
3https://anonymous.4open.science/r/NeuroSED/
22
Under review as a conference paper at ICLR 2022
I Methods ∣	PubMed	Protein	AIDS	CiteSeer	Cora_ML	Amazon	DblP Il L	Methods ∣	AIDS’	Linux	IMDB I)
								NEUROSEDG I	0.629	0.318	3.612 Il
I NEUROSED I	0.474	0.377	0.401	0.332	0.432	0.207	0.626 Il l_				
I H2MN-RW I	0.764	0.575	0.566	0.882	0.824	0.480	0.926 Il |_	H2MN-RW I	0.777	0.534	28.486 I)
I H2MN-NE I	0.831	0.518	0.510	1.094	0.797	0.465	0.966 Il I	H2MN-NE I	0.779	0.176	28.182 I)
I SimGnn ∣	0.853	0.835	0.545	1.068	0.841	1.257	0.911 K F	SIMGNN I	0.816	0.489	28.082 Il
I Branch I	1.692	1.986	1.092	2.275	2.277	3.697	2.156 I) F	Branch I	2.895	2.017	3.303 Il .
I MIP-F2 I	1.528	1.264	0.684	1.940	1.834	3.303	2.165 K -	I MIP-F2 I	2.184	0.480	36.460 Il
							I				
(a) Prediction of Sed.	4 n ， ，. ΣZ
(b) Prediction of Ged
Table 7: MAE scores (lower is better) in (a) Sed and (b) Ged.
	 I								Methods ∣	AIDS’	Linux	IMDB
Methods ∣	PubMed	Protein	AIDS	CiteSeer	Cora_ML	Amazon	Dblp Il L				
								NEUROSEDG I	.9F	.97	.99
NeuroSed ∣	.99	.98	^^.98	.99	.99	.99	~.97 Il L				
H2MN-RW I	.97	.94	.95	.98	.97	.97	.94 Il |_	H2MN-RW I	.86	.92	.66
H2MN-NE I	.97	.96	.96	.97	.98	.98	.93 Il I	H2MN-NE I	.86	.98	.65
SimGnn ∣	.97	.9	.95	.97	.97	.85	.94 Ii F	SimGnn ∣	.84	.93	.80
Branch I	.88	.64	.82	.91	.86	.63	^^.75 Il ∣^	Branch I	~^-.55	.14	.98
MIP-F2 I	.80	.68	.78	.83	.79	.44		I			
							.65 K -	MIP-F2 I	-.20	.78	.69
							L				
		(a) Prediction of Sed.						(b) Prediction of Ged			
Table 8: R2 scores (higher is better, 1 is maximum) in (a) Sed and (b) Ged.
B ranch is competitive only on IMDB for Ged prediction and H2MN-NE is better in Linux only for
Ged prediction.
K PRECISION@k ON k-NN QUERIES
Fig. 7 shows Precision@k on k-NN queries. As visible, either NEUROSED comfortably performs
better than all baselines or is among the best performing algorithms.
K. 1 Additional Results on Ablation Study
Impact of Gin: To highlight the importance of GIN, we conduct ablation studies by replacing the
GIN convolution layers in the model with several other convolution layers. As visible in the Table 10,
Gin consistently achieves the best accuracy. This is not surprising since Gin is provably the most
expressive among Gnns in distinguishing graph structures (essential to Sed or Ged computation)
and is as powerful as the Weisfeiler-Lehman Graph Isomorphism test (Xu et al., 2018b).
I	Methods ∣	CiteSeer (Sed)	PubMed (S ed)	Amazon (Sed)	IMDB(GED) I
I	NeuroSed (Gin) ∣	0.519	0.728	0.495	6.734 I
I	NeuroSed-GCN ∣	0.556	0.756	0.532	12.151 I
I Neurosed-GraphSage ∣	1.364	1.156	1.841	91.312 I
I	NeuroSed-GAT ∣	1.294	1.259	1.843	89.034 I
Table 10: Ablation studies: GIN vs others. RMSE produced by different methods are shown
and NeuroSed with Gin produces the best results.
Impact of sum-pool: To substantiate our choice of the pooling layer, we have performed ablation
studies with various pooling functions as replacements for sum-pool. It is clear from Table 11 that
sum-pool is the best choice among the considered alternatives.
23
Under review as a conference paper at ICLR 2022
Datasets	Range (θ = 2)					10-NN					
	CPU		GPU			CPU		GPU			
	L-SCan	M-Tree	L-Scan	H2MN-RW	H2MN-NE	L-Scan	M-Tree	L-Scan	H2	MN-rw	H2MN-NE
PubMed	0.693	0.56	0.004	26.6	35.2	1.01	0.49	0.004		27.5	355
Amazon	9.09	5.07	0.025	371	8550	11.3	4.75	0.027		372	8760
Dblp	48	20.9	0.070	696	8910	50.4	18.6	0.126		698	9790
Table 9: Querying time (s) for Sed in the three largest datasets. L-Scan indicates time taken
by linear scan in NeuroSed (times differs based on whether executed on a CPU or in GPU).
M-Tree indicates time taken by NeuroSed when indexed using an adapted Metric Tree.
I	Pool functions ∣	CiteSeer (Sed)	PubMed (SED)	Amazon (Sed)	IMDB (Ged) I
I	NeuroSed (Sum) ∣	0.519	0.728	0.495	6.734 I
I	NEUROSED-MaX I	0.795	0.709	0.603	52.519 I
I	NEUROSED-Mean ∣	0.922	0.732	0.846	52.483 I
I NEUROSED-Attention ∣	0.914	0.797	0.868	130.47 I
Table 11: Ablation studies: sum-pool vs others. The sum-pool is the best choice among the
considered alternatives.
K.2 Alignment
In real-world applications of subgraph similarity search, alignments are of interest only for a small
number of similar subgraphs. Our framework is intended to serve as a filter to retrieve this small set
of similar subgraphs from a large number of candidates. To elaborate, a graph database may contain
thousands or millions of graphs (or alternatively, thousands or millions of neighborhoods of a large
graph) which need to be inspected for similar subgraphs. A user is typically interested in only a
handful of these subgraphs that are highly similar to the query. Since the filtered set is significantly
smaller, a non-neural exact algorithm suffices to construct the alignments (Lemma 1 allows us to
adapt general cost Ged alignment techniques for Sed alignment). Computing alignments across the
entire database is unnecessary and slows down the query response time.
To substantiate our claim, we show the average running time for answering 10-NN queries. We
break up the running time into two components: (i) k-NN retrieval time by NEUROSED, (ii) exact
alignment time using MIP-F2 for the 10-NN neighborhoods retrieved by NEUROSED. We observe
that exact alignment by existing methods on the 10-NN neighborhoods completes in reasonable time.
In contrast, GENN-A* does not scale on either PubMed or Amazon since it computes alignments
across all (sub)graphs.
I	Methods ∣	PubMed	Amazon	
I NeuroSed Retrieval ∣	0.373	6.471	
I	MIP-F2 Alignment ∣	52.8	68.4	Il
Table 12: The average running times in seconds per top-10 query. Our technique is much faster
than MIP-F2 alignment.
L Heat Maps for Prediction Error
In Figures 8 to 17, we show the variation of the errors on Sed and Ged prediction with query
sizes and ground truth values for NeuroSed (NeuroSedG for Ged) and the baselines on all
the corresponding datasets. This experiment is an extension of the heat-map results in Fig. 2 in
the main paper. These datasets show variations in the distributions of Sed and Ged values. It is
interesting to observe that among the baselines, different methods perform well on different regions
(e.g., combinations of high/low Sed and high/low query sizes). In the figures, darker means higher
error. The baselines do not show good performance on all regions. However, for NeuroSed and
NeuroSedG, we see a much better coverage for all types of regions in the domain. Furthermore, for
every region, our models outperform (or are at least competitive with) the best performing baseline
for that region.
24
Under review as a conference paper at ICLR 2022
70-
Ooo
0 9 8
七°Uo-SPTd
10	20
k
0 5 0 5
0 7 5 2
1
a∕°Uo-sp①」d
10	20
k
0 5 0 5
0 7 5 2
1
七uoPOJd
Ooo
0 8 6
七©u。叵Do」d
20	10	20
k
(d) CiteSeer-SED
(a) PubMed-SED
(b) Protein-SED
100
100
5 0 5 0
2 2 11
①zA」①∏0
Ooo
8 6 4
七ouop9Jd
k
(e) aids,-Ged
70
20
9080
Wouo∞p①」d
10
k
(f) Linux-GED
10
k
(C) AIDS-SED
100-
O O
8 6
W0uop9Jd
20	10
k
(g) IMDB-GED
20
♦ NeuroSED
-⅛- SimGNN
T- H2MN-RW
T- H2MN-NE
■ NeuroMatch
(h) Legend
Figure 7: PreCision@k on k-NN queries.
0	20	40
SED
(d)	SIMGNN
0	20	40	0	20	40
SED	SED
(e)	BRANCH	(f) MIP-F2
0	20	40	0	20	40	0	20	40
SED	SED	SED
(a) NEUROSED (b) H2MN-RW(c) H2MN-NE
0
Figure 8: Heat Maps of Sed error against query size and Sed values for Dblp. Darker means
higher error.
M Scalability with query size and generalizability to unseen larger query
SIZES
In this analysis, we aim to analyze the following aspeCts:
1.	PerformanCe on larger query sizes.
2.	Generalizability to unseen larger query sizes
In Table 13 sheds light on the above objeCtives. We notiCe that although there is some deterioration
in the quality for query sizes in the range [25, 50] when Compared to the entire set, it is not severe
(NEUROSED-50 in Table 13). However, if we the train set only Contains queries till size 25 and we
deploy the learned model to infer on queries of larger unseen sizes, the drop in quality is signifiCant
(NeuroSed-25 in Table 13). Nonetheless, NeuroSed remains superior to the optimal non-neural
approaCh (MIP-F2) when run with a generous time limit of 60 seConds per query. Overall, this
experiment highlights one direCtion that needs further study and improvement.
Method	PubMed		CiteSeer		Amazon	
	VQ ∈ [0, 50]	VQ ∈ [25, 50] I	VQ ∈ [0, 50]	VQ ∈ [25, 50] I	VQ ∈ [0, 50]	VQ ∈ [25, 50]
NeuroSed-50	1294	1.917	0.728	0.948	0.638	0.782
NeuroSed-25	2.824	4.999	4.740	9.052	1.152	1.724
MIP-F2	3.507	6.278	4.831	8.505	6.454	10.293
Table 13: RMSE of prediCtion quality. NeuroSed-50 indiCates NeuroSed trained on a dataset
Containing queries of size up to 50. NEUROSED-25 is defined analogously.
25
Under review as a conference paper at ICLR 2022
°,z∞AJ°,no
(a) NEUROSED (b) H2MN-RW (c) H2MN-NE (d) SIMGNN (e) BRANCH (f) MIP-F2
Figure 9: Heat Maps of Sed error against query size and Sed values for Amazon. Darker
means higher error.
O 20	40 O 20	40 O 20	40 O 20	40 O 20	40 O 20	40
SED	SED	SED	SED	SED	SED
(a) NEUROSED (b) H2MN-RW (c) H2MN-NE (d) SIMGNN (e) BRANCH	(f) MIP-F2
Figure 10:	Heat Maps of Sed error against query size and Sed values for PubMed. Darker
means higher error.
N Visualization and Application of Sed
Searching for molecular fragment containment is a routine task in drug discovery (Ranu et al., 2011).
Motivated by this, we show the top-5 matches to an SED query on the AIDS dataset produced by
NeuroSed in Fig. 18. The query is a functional group (Hydrogen atoms are not represented).
NeuroSed is able to extract chemical compounds that contain this molecular fragment (except for
ranks 3 and 5, which contain this group with 1 edit) from around 2000 chemical compounds with
varying sizes and structures. This validates the efficacy of NeuroSed at a semantic level.
O Alternative Query Distributions
In Rwr, we perform fixed length random walks, where the length of a walk is the average diameter
size of the queries generated through BFS during training. Next, we merge the walks to form a graph.
In Rw, we perform random walks, till the diameter of the resultant graph is the same as the average
diameter of the BFS sampled train graphs.
The details of the shaDow sampler is explained in Zeng et al. (2021).
26
Under review as a conference paper at ICLR 2022
°,z∞AJ°,no
0	20	40	0	20	40	0	20	40	0	20	40	0	20	40	0
SED	SED	SED	SED	SED
(a) NEUROSED (b) H2MN-RW (c) H2MN-NE (d) SIMGNN (e) BRANCH
20	40
SED
(f) MIP-F2
Figure 11:	Heat Maps of Sed error against query size and Sed values for CiteSeer. Darker
means higher error.
20	40	0	20	40	0	20	40	0	20	40	0
SED	SED	SED	SED
20	40	0
SED
20	40
SED
(a) NEUROSED (b) H2MN-RW (c) H2MN-NE (d) SIMGNN (e) BRANCH	(f) MIP-F2
Figure 12:	Heat Maps of Sed error against query size and Sed values for Cora_ML. Darker
means higher error.
20	40	0
SED
20	40	0	20	40	0
SED	SED
20	40	0	20	40	0
SED	SED
(a) NEUROSED (b) H2MN-RW (c) H2MN-NE (d) SIMGNN (e) BRANCH
20	40
SED
(f) MIP-F2
Figure 13:	Heat Maps of Sed error against query size and Sed values for Protein. Darker
means higher error.
4 2 0 8-
111
zAJno
D
E
S
D
E
S
20
D
E
S
D
E
S
D
E
S
D
E
S
(a) NEUROSED (b) H2MN-RW (c) H2MN-NE (d) SIMGNN (e) BRANCH (f) MIP-F2
Figure 14:	Heat Maps of Sed error against query size and Sed values for AIDS. Darker means
higher error.
27
Under review as a conference paper at ICLR 2022
8 6 4 2
°z∞AJ°no
20
20
20
20
20
D
I E
1G
(a) NEUROSED G (b) H2MN-RW (c) H2MN-NE (d) SIMGNN (e) BRANCH
(f) MIP-F2
Figure 15:	Heat Maps of Ged error against query size and Ged values for AIDS’. Darker
means higher error.
♦ ♦
°,z∞AJ°,no
10
8
0	5	10	15
GED
0	10
GED
	
0	10
GED
(a) NEUROSED G (b) H2MN-RW (c) H2MN-NE
0	10
GED
(d)	SIMGNN
0	10
GED
(e)	BRANCH
0	5	10	15
GED
(f)	MIP-F2
Figure 16:	Heat Maps of Ged error against query size and Ged values for Linux. Darker
means higher error.
28
Under review as a conference paper at ICLR 2022
°,-s ΛJ°,no
O 20	40 O 20	40 O 20	40 O 20	40 O 20	40 O
GED	GED	GED	GED	GED
20	40
GED
(a) NEUROSED G (b) H2MN-RW (c) H2MN-NE (d) SIMGNN (e) BRANCH	(f) MIP-F2
Figure 17:	Heat Maps of Ged error against query size and Ged values for IMDB. Darker
means higher error.
29
Under review as a conference paper at ICLR 2022
(a) Query (b) Rank 1	(c) Rank 2	(d) Rank 3	(e) Rank 4	(f) Rank 5
Figure 18:	Visualizations of query and resulting matches produced by NeuroSed. Red, Green
and Yellow colors indicate Carbon, Nitrogen and Oxygen atoms respectively. The actual and
predicted Sed for the target graphs are (b) 0, 0.4, (c) 0, 0.5, (d) 1, 0.6, (e) 0, 0.6 and (f) 1, 0.6.
30