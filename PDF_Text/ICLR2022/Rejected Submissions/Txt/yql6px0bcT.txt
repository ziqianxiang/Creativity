Under review as a conference paper at ICLR 2022
Decentralized Cross-Entropy Method for
Model-Based Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
Cross-Entropy Method (CEM) is a popular approach to planning in model-based
reinforcement learning. It has so far always taken a centralized approach where
the sampling distribution is updated centrally based on the result of a top-k oper-
ation applied to all samples. We show that such a centralized approach makes
CEM vulnerable to local optima and impairs its sample efficiency, even in a
one-dimensional multi-modal optimization task. In this paper, we propose De-
centralized CEM (DecentCEM) where an ensemble of CEM instances run inde-
pendently from one another and each performs a local improvement of its own
sampling distribution. In the exemplar optimization task, the proposed decentral-
ized approach DecentCEM finds the global optimum much more consistently than
the centralized CEM approaches that use either a single or a mixture of Gaussian
distributions. Also, we show that DecentCEM is theoretically sound. Further, we
extend the decentralized approach to sequential decision-making problems where
we show in several continuous control benchmark environments that it provides
an effective mechanism to improve the performance of CEM algorithms, under
the same sample budget for planning.
1	Introduction
Model-based reinforcement learning (MBRL) uses a model as a proxy of the environment for plan-
ning actions in multiple steps. This paper studies planning in MBRL with a specific focus on the
Cross-Entropy Method (CEM) (De Boer et al., 2005; Mannor et al., 2003), which is popular in
MBRL due to its ease of use and strong empirical performance (Chua et al., 2018; Hafner et al.,
2019; Wang & Ba, 2020; Zhang et al., 2021; Yang et al., 2020). CEM is a stochastic, derivative-
free optimization method. It uses a sampling distribution to generate imaginary trajectories of
environment-agent interactions with the model. These trajectories are then ranked based on their
returns computed from the rewards given by the model. The sampling distribution is updated to
increase the likelihood of producing the top-k trajectories with higher returns. These steps are it-
erated and eventually yield an improved distribution over the action sequences to guide the action
execution in the real environment.
Despite the strong empirical performance of CEM for planning, it is prone to two problems: (1)
lower sample efficiency as the dimensionality of solution space increases, and (2) the Gaussian dis-
tribution that is commonly used for sampling may cause the optimization to get stuck in local optima
of multi-modal solution spaces commonly seen in real-world problems. Previous works addressing
these problems either add gradient-based updates of the samples to optimize the parameters of CEM,
or adopt more expressive sampling distributions, such as using Gaussian Mixture Model (Okada &
Taniguchi, 2020) or masked auto-regressive neural network (Hakhamaneshi et al., 2020). Neverthe-
less, all CEM implementations to date are limited to a centralized formulation where the ranking
step involves all samples. As analyzed below and in Section 3, such a centralized design makes
CEM vulnerable to local optima and impairs its sample efficiency.
We propose Decentralized CEM (DecentCEM) to address the above problems. Rather than ranking
all samples, as in the centralized design, our method distribute the sampling budget across an en-
semble of CEM instances. These instances run independently from one another, and each performs
a local improvement of its own sampling distribution based on the ranking of its generated samples.
1
Under review as a conference paper at ICLR 2022
The best action is then aggregated by taking an arg max among the solution of the instances. It
recovers the conventional CEM when the number of instance is one.
We hypothesize that by shifting to this decentralized
design, CEM can be less susceptible to premature
convergence caused by the centralized ranking step.
As illustrated in Fig. 1, the centralized sampling dis-
tribution exhibits a bias toward the sub-optimal so-
lutions near top right, due to the global top-k rank-
ing. This bias would occur regardless of the family
of distributions used. In comparison, a decentralized
approach could maintain enough diversity thanks to
its local top-k ranking in each sampling instance.
Through a one-dimensional multi-modal optimiza-
tion problem in Section 3, we show that DecentCEM
empirically finds the global optimum more consis-
tently than centralized CEM approaches that use ei-
ther a single Gaussian or a mixture of Gaussian dis-
tributions. Also we show that DecentCEM is the-
oretically sound that it converges almost surely to
a local optimum. We further apply DecentCEM to
sequential decision making problems and use neu-
ral networks to parameterize the sampling distribu-
tion in each CEM instance. Empirical results in sev-
eral continuous control benchmarks suggest that De-
centCEM offers an effective mechanism to improve
(a) Centralized CEM (b) Decentralized CEM
Figure 1: Illustration of CEM approaches
in optimization. Shades of red indicate rela-
tive value of the 2D optimization landscape:
brighter is better. Optimal solutions are near
bottom left corner of the solution space. Blue
dots ∙ are top-k samples, and black dots ∙
are other samples. Open dots O represent the
sampling distributions with size of dots indi-
cating number of generated samples.
the sample efficiency over the baseline CEM methods under the same sample budget for planning.
2	Preliminaries
We consider a Markov Decision Process (MDP) specified by (S,A,R,P,γ,d0,T). S ⊂ Rds is the
state space, A ⊂ Rda is the action space. R : S × A → R is the reward function that maps a state
and action pair to a real-valued reward. P(s0|s, a) : S × A × S → R+ is the transition probability
from a state and action pair s, a to the next state s0. γ ∈ [0, 1] is the discount factor. d0 denotes the
distribution of the initial state s0 . At time step t, the agent receives a state st 1 and takes an action
at according to a policy ∏(∙∣s) that maps the state to a probability distribution over the action space.
The environment transitions to the next state st+ι 〜P(∙∣st, at) and gives a reward r = R(st, a.
to the agent 2. The return Gt = PiT=0 γirt+i, is the sum of discounted reward within an episode
length of T . The agent aims to find a policy π that maximizes the expected return. We denote the
learned model in MBRL as fω (∙∣s, a), which is parameterized by ω and approximates P(∙∣s,a).
Planning with the Cross Entropy Method Planning in MBRL is about leveraging the model to
find the best action in terms of its return. Model-Predictive-Control (MPC) performs online planning
at each time step up to a horizon to find the optimal action sequence:
πMPC(st) = arg max E[ΣiH=-01γir(st+i, at+i) + γHV (sH)]	(1)
at:t+H-1
where H is the planning horizon, at:t+H-1 denotes the action sequence from time step t to t+H- 1,
and V (sH) is the terminal value function at the end of the planning horizon. The first action in this
sequence is executed and the rest are discarded. The agent then re-plans at the next time step.
The Cross-Entropy Method (CEM) is a gradient-free optimization method that can be used for solv-
ing Eq. (1). The workflow is shown in Fig. 2. CEM planning starts by generating N samples
{τj}N=ι = {(aj,o, ^j,ι,…，aj,H-1)}N=1 from an initial sampling distribution gφ(τ) parameterized
1We assume full observability, i.e., we assume that the agent has access to the state.
2We assume that the agent receives the true reward. This makes the problem easier but is unfair to MBRL
methods that do not assume this. Thus, we will limit comparison to methods that also make this assumption.
2
Under review as a conference paper at ICLR 2022
distribution
parameters
at time t
Sampling	NSamPIeS	_
Distribution 产 Ta &---------------；^Γ* Model - Top-kSamples ʌɪ ʃ=____________________________,
______τ;=ι-(⅛ ⅛	⅛→)	______aɪ, ....ali.1↑
CEM Internal Loop
-----------------------------------------------------------------------------------1 CEM
action
ɑt =
Figure 2: Cross Entropy Method (CEM) for Planning in MBRL
by φ, where each sample τj is an action sequence from the current time step up to the planning
horizon H. The domain of gφ(τ) has a dimension of dτ = daH.
Using a model f, CEM generates imaginary rollouts based on the action sequence {τj } (in the
case of a stochastic model) and estimate the associated value v(τj) = E[ΣiH=-01γir(sj,i, aj,i)] where
sj,o is the current state S and sj,i+ι 〜 f (sj,i,aj,i). The terminal value YHV(sj,H) is omitted
here following convention in the CEM planning literature but the MPC performance can be further
improved if paired with an accurate value predictor (Bertsekas, 2005; Lowrey et al., 2019). The
sampling distribution is then updated by fitting to the current top-k samples in terms of their value
estimates v(τj), using the Maximum Likelihood Estimation (MLE) which solves:
N
Φ0 = argmaXEI(V(Tj) ≥ vth)loggφ(τ)	(2)
φ j=1
where Vth is the threshold equal to the value of the k-th best sample and 1 (∙) is the indicator function.
In practice, the update to the distribution parameters are smoothed by φl+1 = αφ0+ (1 -α)φl where
α ∈ [0, 1] is a smoothing parameter that balances between the solution to Eq. (2) and the parameter
at the current internal iteration l. CEM repeats this process of sampling and distribution update in an
inner-loop, until it reaches the stopping condition. In practice, it is stopped when either a maximum
number of iterations has been reached or the parameters have not changed for a few iterations. The
output of CEM is an action sequence, typically set as the expectation3 of the most recent sampling
distribution for uni-modal distributions such as Gaussians μ = E(gφ) = (ao, &i,…，^h-i).
Choices of Sampling Distributions in CEM A common choice of the sampling distribution is a
multivariate Gaussian distribution under which Eq.(2) has a straight-forward analytical solution. But
the uni-modal nature of Gaussian makes it inadequate in solving multi-modal optimization that often
occur in MBRL. To increase the capacity of the distribution, a Gaussian Mixture Model (GMM) can
be used (Okada & Taniguchi, 2020). We denote such an approach as CEM-GMM. Going forward,
we use CEM to refer to the vanilla version that uses a Gaussian distribution. Computationally,
the major difference between CEM and CEM-GMM is that the distribution update in CEM-GMM
involves solving for more parameters. Detailed steps can be found in Okada & Taniguchi (2020).
3 Decentralized CEM
In this section, we first introduce the formulation of the proposed decentralized approach called the
Decentralized CEM (DecentCEM). Then we illustrate the intuition behind the proposed approach
using a one-dimensional synthetic multi-modal optimization example where we show the issues of
the existing CEM methods and how they can be addressed by DecentCEM.
Formulation of DecentCEM DecentCEM is composed of an ensemble of multiple CEM in-
stances indexed by i, each having its own sampling distributions gφi . They can be described by
a set of distribution parameters Φ = {φi}iM=1. Each instance i manages its own sampling and dis-
tribution update by the steps described in Section 2, independently from other instances. Note that
the number of samples and elites are evenly split among the M instances. The top-M sample sets
are decentralized and managed by each instance independently whereas the centralized approach
only keeps one set of top-k samples regardless of the distribution family used. After the stopping
3Other options are discussed in Appendix A.2
3
Under review as a conference paper at ICLR 2022
condition is reached for all instances, the final sampling distribution is taken as the best distribution
in the set Φ according to (the arg max uses a deterministic tie-breaking):
N
M
φDecentCEM = arg max Eφi [v(x)] ≈ arg max	v(τi,j)	(3)
φi∈Φ	φi∈Φ j=1
where Eφi [v(x)] denotes the expectation with respect to the distribution gφi , approximated by the
sample mean. When M = 1, it recovers the conventional CEM method.
Figure 3: Left: The objective function in a 1D optimization task. Right: Comparison of our proposed
DecentCEM method to CEM and CEM-GMM, wherein the line and the shaded region denote the
mean and the min/max cost from 10 independent runs. X: resulting solution of each method.
Motivational Example Consider a one dimensional multi-modal optimization problem shown in
Fig.3(Left). There are eight local optima, including one global optimum f (x*) = -1.9 where x* =
5.146. This objective function mimics the RL value landscape that has many local optima, as shown
by Wang & Ba (2020). This optimization problem is “easy” in the sense that a grid search over the
domain can get us a solution close to the global optimum. However, only our proposed DecentCEM
method successfully converges to the global optimum consistently under varying population size
(i.e., number of samples) and random runs, as shown in Fig.3 (Right)4.
Both CEM-GMM and the proposed DecentCEM are equipped with multiple sampling distributions.
The fact that CEM-GMM is outperformed by DecentCEM may appear surprising. To gain some
insights, We illustrate in Fig. 4 how the sampling distribution evolves during the iterative update
(more details in Fig. 9 in Appendix). CEM updated the unimodal distribution toward a local op-
timum despite seeing the global optimum. CEM-GMM appears to have a similar issue. During
MLE on the top-k samples, it moved most distribution components towards the same local optimum
which quickly lead to mode collapse. On the contrary, DecentCEM successfully escaped the local
optima thanks to its independent distribution update over decentralized top-k samples and was able
to maintain a decent diversity among the distributions.
DecentCEM iter=O DecentCEM iter=4
Figure 4: How the sampling distributions evolve in the 1D optimization task, after the specified
iteration. Symbols include samples ∙, elites ∙, local optima ∙, global . 2nd row in each figure
shows the weighted p.d.f of individual distribution. Population size: 200.
GMM suits density estimation problems like distribution-based clustering where the samples are
drawn from a fixed true distribution that can be represented by multi-modal Gaussians. However,
4For a fair comparison, hyperparameter search has been conducted on all three methods for each population
size (Appendix A).
4
Under review as a conference paper at ICLR 2022
in CEM for optimization, exploration is coupled with density estimation: the sampling distribution
in CEM is not fixed but rather gets updated iteratively toward the top-k samples. And the “true”
distribution in optimization puts uniform non-zero densities to the global optima and zero densities
everywhere else. When there is a unique global optimum, it degenerates into a Dirac measure that
assigns the entire density to the optimum. Density estimation of such a distribution only needs one
Gaussian but the exploration is challenging. In other words, the conditions for GMM to work well
are not necessarily met when used as the sampling distribution in CEM. CEM-GMM is subject to
mode collapse during the iterative top-k greedification, causing premature convergence, as observed
in Fig 4. In comparison, our proposed decentralized approach takes care of the exploration aspect by
running multiple CEM instances independently, each performing its own local improvement. This
is shown to be effective from this optimization example and the benchmark results in Section 6.
CEM-GMM only consistently converge to the global optimum when we increase the population size
to the maximum 1,000 which causes expensive computations. Our proposed DecentCEM runs more
than 100 times faster than CEM-GMM at this population size, shown in Table A.3 in Appendix.
Convergence of DecentCEM We state the convergence result of DecentCEM in Theorem 3.1. We
show that the previous convergence result of CEM (Hu et al., 2011) applies to DecentCEM under the
same sample budget. The key observation is that the convergence property of each CEM instance
still holds since the number of samples in each instance is only changed by a constant factor (the
number of instances). We leave the detailed proof to appendix H.
Theorem 3.1 (Convergence of DecentCEM). If Assumptions 1-5 hold for a CEM instance described
in Algorithm 3 and we decentralize itby evenly dividing its sample size Nk into M CEM instances in
DecentCEM algorithm that satisfies Assumption 6, then the sequence of iterates {ηi,k} generated by
each CEM instance indexed by i converges almost surely to an internally chain recurrent set (Hirsch
et al., 2001) of Equation 7. Furthermore, the solution of DecentCEM {ηo,k} converges almost surely
to the best solution of the individual instances in terms of the expected value of Em-1 (η) [V (x)].
4 DecentCEM for Planning in MBRL
In this section, we develop two instantiations of DecentCEM for planning in MBRL where the
sampling distributions are parameterized by policy networks.
CEM Planning with a Policy Network In MBRL, CEM is applied to every state separately to
solve the optimization problem stated in Eq. (1). The sampling distribution is typically initialized
to a fixed distribution at the beginning of every episode (Okada & Taniguchi, 2020; Pinneri et al.,
2020), or more frequently at every time step (Hafner et al., 2019). Such initialization schemes are
sample inefficient since there is no mechanism that allows the information of the high-value region
in the value space of one state to generalize to nearby states. Also, the information is discarded
after the initialization. It is hence difficult to scale the approach to higher dimensional solution
spaces, present in many continuous control environments. Wang & Ba (2020) proposed to use a
policy network in CEM planning that helped to mitigate the issues above. They developed two
methods: POPLIN-A that plans in the action space, and POPLIN-P that plans in the parameter space
of the policy network. In POPLIN-A, the policy network is used to learn to output the mean of a
Gaussian sampling distribution of actions. In POPLIN-P, the policy network parameters serve as the
initialization of the mean of the sampling distribution of parameters. The improved policy network
can then be used to generate an action. They show that when compared to the vanilla method of
using a fixed sampling distribution in the action space, both modes of CEM planning with such a
learned distribution perform better. The same principle of combining a policy network with CEM
can be applied to the DecentCEM approach as well, which we will describe next.
DecentCEM Planning with an Ensemble of Policy Networks For better sample efficiency in
MBRL setting, we extend DecentCEM to use an ensemble of policy networks to learn the sampling
distributions in the CEM instances. Similar to the POPLIN paper, we develop two instantiations of
DecentCEM, namely DecentCEM-A and DecentCEM-P. The architecture of the proposed algorithm
is illustrated in Fig. 5.
DecentCEM-A plans in the action space. It consists of an ensemble of policy networks fol-
lowed by CEM instances. Each policy network takes the current state st as input, out-
5
Under review as a conference paper at ICLR 2022
puts the parameters θi of the sampling distribution for CEM instance i. There is no fun-
damental difference from the DecentCEM formulation in Section 3 except that the initializa-
tion of sampling distributions is learned by the policy networks rather than a fixed distribution.
The second instantiation DecentCEM-P
plans in the parameter space of the policy
network. The initial sampling distribution
is a Gaussian distribution over the policy
parameter space with the mean at the cur-
rent parameter values. In the arg max op-
eration in Eq. (3), the sample τi,j denotes
the parameters of the policy network. Its
value is obtained by computing the value
of the action sequence generated from the
policy network with the parameters τi,j .
The ensemble of policy networks in
both instantiations DecentCEM-A and
DecentCEM-P are initialized with random
Training	Inference
Figure 5: DecentCEM planning architecture. ψi = φi
for planning in action space and ψi = θi for planning
in policy network parameter space.
weights, which is empirically found to be
adequate to ensure that the output of the
networks do not collapse into the same distribution (Sec. 6 and Appendix F).
Training the Policy Network in DecentCEM When planning in action space, the policy networks
are trained by behavior cloning, similar to the scheme in POPLIN (Wang & Ba, 2020). Denote the
first action in the planned action sequence at time step t by the i-th CEM instance as at,i, the i-th
policy network is trained to mimic ^t,i and the training objective is min&i Es%at,i〜Di∣∣aθi(St)-
^t,ik2 where Di denotes the replay buffer with the state and action pairs (st, ^t,i). aθi(St) is the
action prediction at state st from the policy network parameterized by θi .
While the above training scheme can be applied to both planning in action space and parameter
space, we follow the setting parameter average (AVG) (Wang & Ba, 2020) training scheme when
planning in parameter space. The parameter is updated as θi = θi+Pδ.∈d. δi where Di = {δi}
is a dataset of policy network parameter updates planned from the i-th CEM instance previously. It
is more effective than behavior cloning based on the experimental result reported by Wang & Ba
(2020) and our own preliminary experiments.
Note that each policy network in the ensemble is trained independently from the data observed by
its corresponding CEM instance rather than from the aggregated result after taking the arg max.
This allows for enough diversity among the instances. More importantly, it increases the size of the
training dataset for the policy networks compared to the approach taken in POPLIN. For example,
with an ensemble of M instances, there would be M training data samples available from one real
environment interaction, compared to the one data sample in POPLIN-A/P. As a result, DecentCEM
is able to use larger policy networks than is otherwise possible, shown in Sec. 6 and Appendix F. 5
5 Related Work
We limit the scope of related works to CEM plannig methods. Vanilla CEM planning in action space
with a single Gaussian distribution has been adopted as the planning method for both simulated and
real-world robot control (Chua et al., 2018; Finn & Levine, 2017; Ebert et al., 2018; Hafner et al.,
2019; Yang et al., 2020; Zhang et al., 2021). Among previous attempts to improve the performance
of CEM-based planning, we see two types of approaches. The first type includes CEM in a hybrid
of CEM+X where “X” is some other component or algorithm. POPLIN (Wang & Ba, 2020) is a
prominent example where “X” is a policy network that learns a state conditioned distribution that
initializes the subsequent CEM process. This addition of the policy network allows the CEM to
search in the network parameter space which is shown to have a smoother landscape and better
exploration. Another common choice of “X” is gradient-based adjustment of the samples drawn in
CEM. GradCEM (Bharadhwaj et al., 2020) adjusts the samples in each iteration of CEM by taking
6
Under review as a conference paper at ICLR 2022
gradient ascent of the return estimate w.r.t the actions. The benefit that this method brings is not
significant on benchmark control tasks. CEM-RL (Pourchot & Sigaud, 2019) also combines CEM
with gradient based updates from RL algorithms but the samples are in the parameter space of the
actor network. To improve computational efficiency, Lee et al. (2020) proposes an asynchronous
version of CEM-RL where each CEM instance updates the sampling distribution asynchronously
without waiting for other instances to finish. The downside with both versions of CEM-RL methods
are that they rely on model-free RL algorithms. One can imagine reversing the order of CEM and
RL and using CEM to update the policies/actors of RL agents. The approach taken in (Khadka et al.,
2019) was along similar lines but they used a genetic algorithm instead of CEM.
The second type of approach aims at improving CEM itself. Amos & Yarats (2020) proposes a
fully-differentiable version of CEM called DCEM. The key is to make the top-k selection in CEM
differentiable such that the entire CEM module can be trained in an end-to-end fashion. Despite
cutting down the number of samples needed in CEM, this method does not beat the vanilla CEM in
benchmark test. GACEM (Hakhamaneshi et al., 2020) increase the capacity of the sampling distri-
bution by replacing the Gaussian distribution with an auto-regressive neural network. This change
allows CEM to perform search in multi-modal solution space but it is only verified in toy examples
and its computation seems too high to be scaled to MBRL tasks. Another method that increases
the capacity of the sampling distribution is PaETS (Okada & Taniguchi, 2020) that uses a GMM
with CEM. It is the approach that we followed for our CEM-GMM implementation. It is not clear
how well it performs in benchmark tasks since their environment setup is modified to have a range
of actions 5 times larger than the original. Also the running time results in the optimization task
in Sec.3 shows that it is computationally heavier than the CEM and DecentCEM methods, limiting
its use in complex environments. Overall, this second type of approach did not outperform vanilla
CEM, a situation that motivated our move to a decentralized formulation. Macua et al. (2015) pro-
posed a “distributed” CEM that is similar in spirit to our method in that they used multiple sampling
distributions and applied the top-k selection locally to samples from each instance. However, their
instances are cooperative as opposed to being independent as in our work. They applied “collabora-
tive smoothed projection steps” to update each sampling distribution as an average of its neighboring
instances including itself. The updating procedure is more complicated than our proposed method
and proper network topology of the instances is needed: a naive approach of updating according to
all instances will lead to mode collapse since the resulting sampling distributions will be identical.
The method was tested in toy optimization examples only.
6 Experiments
We evaluate the proposed DecentCEM methods in simulated environments with continuous action
space. The experimental evaluation is mainly setup to understand if DecentCEM improves the per-
formance and sample efficiency over conventional CEM approaches.
Benchmark Setup We benchmark the algorithms in several continuous-action control environ-
ments in OpenAI Gym.
Environments We run the benchmark in a set of 13 environments commonly used in the MBRL
literature: Pendulum, InvertedPendulum, Cartpole, Acrobot, FixedSwimmer5, Reacher, Hopper,
Walker2D, HalfCheetah, PETS-Reacher3D, PETS-HalfCheetah, PETS-Pusher, Ant. The three en-
vironments prefixed by “PETS” are proposed by Chua et al. (2018). Note that MBRL algorithms
often make different assumptions about the dynamics model or the reward function. Their bench-
mark environments are often modified from the original OpenAI gym environments such that the
respective algorithm is runnable. Whenever possible, we inherit the same environment setup from
that of the respective baseline methods. This is so that the comparison against the baselines is fair.
More details on the environments and their reward functions are in Appendix B.
Algorithms The baseline algorithms are PETS (Chua et al., 2018) and POPLIN (Wang & Ba, 2020).
PETS uses CEM with a single Gaussian distribution for planning. The POPLIN algorithm com-
bines a single policy network with CEM. As described in Sec.4, POPLIN comes with two modes:
POPLIN-A and POPLIN-P with the suffix “A” denotes planning in action space and “P” for the 5
5a modified version of the original Gym Swimmer environment where the velocity sensor on the neck is
moved to the head. This fix was proposed by Wang & Ba (2020)
7
Under review as a conference paper at ICLR 2022
InvertedPenduIum
0	10∞	2∞0	30∞	4000	50∞
Steps
Hopper
Acrobot
20∞	4000	¢000	80∞	100∞
Steps
u∙l3,uκυπEυ><
O 20∞	40∞	6000	8000 IOOOO
Steps
Walker2D
IOOO
o∙
-IOOO
-Mt®
-3000
Ant
O 200∞	400∞	β∞∞	80∞0	l∞0∞	O 20∞0	400∞	¢0000	800∞	1000∞	O 200∞	400∞	β∞∞	8∞∞	l∞0∞
Steps	Steps	Steps
---PETS ----- POPLlN-A - POPLIN-P - DecentCEM-A -- DecentCEM-P - SAC —— SAC at convergence
Figure 6:	The learning curves of the proposed DecentCEM methods and the baseline methods on
continuous control environments. The line and shaded region shows the mean and standard error of
evaluation results from 5 training runs using different random seeds. Each run is evaluated per train-
ing episode in an environment independent from training and reports average return of 5 episodes.
network parameter space. We reuse the default hyperparameters for these algorithms from the orig-
inal papers if not mentioned specifically. The detailed hyperparameters are listed in the Appendix
D.2. For our proposed methods, we include two variations DecentCEM-A and DecentCEM-P as
described in Sec. 4 where the suffix carries the same meaning as in POPLIN-A/P. All MBRL algo-
rithms studied in this benchmark uses the same ensemble networks proposed by Chua et al. (2018)
for the dynamics model learning. We also include a Model-Free RL baseline SAC (Haarnoja et al.,
2018) and show its finite-time and asymptotic result.
Evaluation Protocol The learning curve shows the mean and standard error of the test performance
out of 5 independent training runs. The test performance is an average return of 5 episodes of
the evaluation environment, evaluated at every training episode. At the beginning of each training
run, the evaluation environment is initialized with a fixed random seed such that the evaluation
environments are consistent across different methods and multiple runs to make it a fair comparison.
All experiments were conducted using Tesla V100-PCIE-16GB GPUs.
Results The learning curves of the algorithms are shown in Fig. 6 for InvertedPendulum, Ac-
robot, Reacher, Hopper, Walker2D and Ant, sorted by the difficulty of task. The full results for all
environments are included in Appendix E.
We can observe two main patterns from the results. One pattern was that in most environments,
the DecentCEM methods either matched or outperformed their counterpart that took a centralized
approach. In fact, DecentCEM can be seen as a generalization of POPLIN by adding a dimension of
policy ensemble size, with size one recovering POPLIN. We also included negative results shown in
Hopper where neither DecentCEM modes outperformed the baselines. It can be interpreted that set-
ting the policy ensemble size to one is better than 5 in this environment. By varying this additional
parameter, CEM can be fine-tuned for individual domains. Also all model-based methods underper-
formed the model-free method SAC, suggesting the difficulty of model learning. The other pattern
was that using policy networks to learn the initial sampling distribution in general helped improv-
ing the performance of CEM with both centralized and decentralized formulation. This is expected
as discussed in Sec.4 since the policy network allows the sampling distribution to “resume” from
high-value region seen before and to generalize to similar states.
Ablation Study A natural question to ask about the DecentCEM-A/P methods is whether the in-
creased performance is from the larger number of neural network parameters. We added two vara-
8
Under review as a conference paper at ICLR 2022
----POPLIN-A -------- PoPLIN-A-BiggerPoIicyNet ------- DecentCEM-A -------- POPUN-P --------- POPLIN-P-BiggerPoIiCyNet ------ DecentCEM-P
Figure 7:	Ablation study on the policy network size where POPLIN-A&P have a bigger policy
network equivalent in the total number of neural network weights to their DecentCEM counterparts.
For better visual clarity, curves are smoothed with a sliding window of size 10.
tions of the POPLIN baselines where a bigger policy network was used. The number of the network
parameters was equivalent to that of the ensemble of policy networks in DecentCEM-A/P. We show
the comparison using three environments in Fig. 7: Pendulum(1), Reacher(2) and PETS-Pusher(7)
(action dimension in parenthesis). In both action space and parameter space planning, a bigger
policy network in POPLIN either did not help or significantly impaired the performance (see the
POPLIN-P results in reacher and PETS-Pusher). This is expected since unlike DecentCEM, the
training data in POPLIN do not scale with the size of the policy network, as explained in Sec. 4.
Figure 8 (Left) shows the cumulative selection
ratio of each CEM instance during training of
DecentCEM-A with an ensemble size of 5. It
suggests that the random initialization of the
policy network is sufficient to avoid mode col-
lapse. We also plot the action statistics of the
instances in Figure 8 (Right). The line and
shaded area represent the mean and max/min
action of the instances, respectively. For visual
clarity, we show a time segment toward the end
of the training rather than all the 10k steps. De-
centCEM has maintained enough diversity in
the instances even toward the end of the train-
ing. DecentCEM-P is excluded from both plots
since it shows a similar trend as DecentCEM-A.
0'i=eκu-9sφ&S
Figure 8: Ablation of ensemble diversity. Left:
Cumulative selection ratio of each CEM instance.
Right: Action statistics of the instances.
More ablations results are included in Appendix F.
Pendulum
7 Conclusion and Future Work
In this paper, we study CEM planning in the context of continuous-action MBRL. We propose a
novel decentralized formulation of CEM named DecentCEM, which generalizes CEM to run multi-
ple independent instances and recovers the conventional CEM when the number of instances is one.
We illustrate the intuition and the strengths of the proposed DecentCEM approach in a motivational
one-dimensional optimization task and show how it fundamentally differs from the CEM approach
that uses a Gaussian or GMM. We also show that DecentCEM has almost sure convergence to a lo-
cal optimum. We extend the proposed approach to MBRL by instantiating two decentralized CEM
methods that combine with policy networks. We show the efficacy of the proposed methods in
benchmark control tasks and ablations studies.
There is a gap between the convergence result and practice that the theory assumes that the number
of samples grow polynomially with the iterations whereas a constant sample size is commonly used
in practice including our work. Investigating the convergence properties of CEM under a constant
sample size makes an interesting direction for future work. Another interesting direction to pursue
is finite-time analysis of both CEM and DecentCEM.
9
Under review as a conference paper at ICLR 2022
Reproducibility Statement
We have included the implementation details in Appendix D and the source code in the supplemen-
tary materials.
References
Brandon Amos and Denis Yarats. The differentiable cross-entropy method. In International Con-
ference on Machine Learning,pp. 291-302. PMLR, 2020.
Dimitri P Bertsekas. Dynamic programming and optimal control 3rd edition, volume i. Belmont,
MA: Athena Scientific, 2005.
Homanga Bharadhwaj, Kevin Xie, and Florian Shkurti. Model-predictive control via cross-entropy
and gradient-based optimization. In Learning for Dynamics and Control, pp. 277-286. PMLR,
2020.
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement
learning in a handful of trials using probabilistic dynamics models. In Proceedings of the 32nd
International Conference on Neural Information Processing Systems, NIPS’18, pp. 4759-4770,
2018.
Pieter-Tjerk De Boer, Dirk P Kroese, Shie Mannor, and Reuven Y Rubinstein. A tutorial on the
cross-entropy method. Annals of operations research, 134(1):19-67, 2005.
Frederik Ebert, Chelsea Finn, Sudeep Dasari, Annie Xie, Alex Lee, and Sergey Levine. Visual fore-
sight: Model-based deep reinforcement learning for vision-based robotic control. arXiv preprint
arXiv:1812.00568, 2018.
Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In 2017 IEEE
International Conference on Robotics and Automation (ICRA), pp. 2786-2793. IEEE, 2017.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International confer-
ence on machine learning, pp. 1861-1870. PMLR, 2018.
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. In International Conference on
Machine Learning, pp. 2555-2565. PMLR, 2019.
Kourosh Hakhamaneshi, Keertana Settaluri, Pieter Abbeel, and Vladimir Stojanovic. Gacem: Gen-
eralized autoregressive cross entropy method for multi-modal black box constraint satisfaction.
arXiv preprint arXiv:2002.07236, 2020.
Morris W Hirsch, Hal L Smith, and Xiao-Qiang Zhao. Chain transitivity, attractivity, and strong
repellors for semidynamical systems. Journal of Dynamics and Differential Equations, 13(1):
107-131, 2001.
Jiaqiao Hu, Ping Hu, and Hyeong Soo Chang. A stochastic approximation framework for a class of
randomized optimization algorithms. IEEE Transactions on Automatic Control, 57(1):165-178,
2011.
Shauharda Khadka, Somdeb Majumdar, Tarek Nassar, Zach Dwiel, Evren Tumer, Santiago Miret,
Yinyin Liu, and Kagan Tumer. Collaborative evolutionary reinforcement learning. In Interna-
tional Conference on Machine Learning, pp. 3341-3350. PMLR, 2019.
Kyunghyun Lee, Byeong-Uk Lee, Ukcheol Shin, and In So Kweon. An efficient asynchronous
method for integrating evolutionary and gradient-based policy search. Advances in Neural Infor-
mation Processing Systems, 33, 2020.
Kendall Lowrey, Aravind Rajeswaran, Sham Kakade, Emanuel Todorov, and Igor Mordatch. Plan
online, learn offline: Efficient learning and exploration via model-based control. In International
Conference on Learning Representations, ICLR, 2019.
10
Under review as a conference paper at ICLR 2022
Sergio Valcarcel Macua, Santiago Zazo, and Javier Zazo. Distributed black-box optimization of
nonconvex functions. In 2015 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP),pp. 3591-3595.IEEE, 2015.
Shie Mannor, Reuven Y Rubinstein, and Yohai Gat. The cross entropy method for fast policy search.
In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pp. 512-
519, 2003.
Masashi Okada and Tadahiro Taniguchi. Variational inference mpc for bayesian model-based rein-
forcement learning. In Conference on Robot Learning, pp. 258-272. PMLR, 2020.
Cristina Pinneri, Shambhuraj Sawant, Sebastian Blaes, Jan Achterhold, Joerg Stueckler, Michal
Rolinek, and Georg Martius. Sample-efficient cross-entropy method for real-time planning. arXiv
preprint arXiv:2008.06389, 2020.
Alols PoUrchot and Olivier Sigaud. CEM-RL: combining evolutionary and gradient-based methods
for policy search. In 7th International Conference on Learning Representations, ICLR 2019, New
Orleans, LA, USA, May 6-9, 2019.
Tingwu Wang and Jimmy Ba. Exploring model-based planning with policy networks. In 8th In-
ternational Conference on Learning Representations, ICLR, Addis Ababa, Ethiopia, April 26-30,
2020.
Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi
Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking model-based reinforce-
ment learning. CoRR, abs/1907.02057, 2019.
Yuxiang Yang, Ken Caluwaerts, Atil Iscen, Tingnan Zhang, Jie Tan, and Vikas Sindhwani. Data
efficient reinforcement learning for legged robots. In Conference on Robot Learning, pp. 1-10.
PMLR, 2020.
Baohe Zhang, Raghu Rajan, Luis Pineda, Nathan Lambert, Andre Biedenkapp, Kurtland Chua,
Frank Hutter, and Roberto Calandra. On the importance of hyperparameter optimization for
model-based reinforcement learning. In International Conference on Artificial Intelligence and
Statistics, pp. 4015-4023. PMLR, 2021.
11
Under review as a conference paper at ICLR 2022
A Details of the Motivational Example
A.1 Setup and Running Time
For a fair comparison of the three methods CEM, CEM-GMM and DecentCEM, we performed a
hyperparameter search for all. The list of hyperparmeters are summarized in Table A.1 and the best
performing hyperparameters for each method under each population size are shown in Table A.2.
These hyperparameters were what the data in Fig. 3 were based on. Note that the top percentage of
samples “Elite Ratio” (in Table A.1) was used in the implementation instead of top-k but they are
equivalent. The running time are included in Table A.3.
TableA.1: Hyperparameters
Algorithm	Parameter	Value
Shared Parameters	Total Population Size Elite Ratio α: Smoothing Ratio : Minimum Variance Threshold Maximum Number of Iterations	100, 200, 500, 1000 0.1 0.1 1e-3 100
CEM-GMM	M : Number of Mixture Components κ: Weights of Entropy Regularizer r: Return Mode	3,5,8,10 0.25, 0.5 ‘s’: mean of the mixture component sampled based on their weights ‘m’: mean of the component that achieves the minimum cost
DecentCEM	E : Number of Instances in the Ensemble	3,5,8,10
Table A.2: Best Hyper-Parameter
Total Population Size
100	200	500	1000
CEM-GMM	M	= 10	M	=8	M	=8	M	=8
	κ=	0.25,	κ=	0.5	κ=	0.25	κ=	0.5
	r=	‘m’	r=	‘m’	r=	‘m’	r=	‘s’
DecentCEM	E	10	E 二	10	E	10	E=	8
Table A.3: Total Time of 10 RUns (in Seconds)
	Total Population Size			
	100	200	500	1000
CEM	0.079	0.093	0.165	0.318
CEM-GMM	7.322	11.500	24.431	59.844
DecentCEM	0.407	0.420	0.506	0.545
A.2 Output of CEM Approaches
In terms of the oUtpUt of CEM approaches, there exist different options in the literatUre. The most
common option is to retUrn the sample in the domain that corresponds to the highest probability
density in the final sampling distribUtion. It is the mean in the case of GaUssian and the mode
with the highest probability density in the case of GMM. One can also draw a sample from the
final sampling distribUtion (Okada & TanigUchi, 2020) and retUrn it. Another option is to retUrn the
best sample observed (Pinneri et al., 2020). The best option among the three may be application
dependent. It has been observed that in many applications, the seqUence of sampling distribUtions
nUmerically converges to a deterministic one (De Boer et al., 2005), in which case the first two
options are identical.
12
Under review as a conference paper at ICLR 2022
B Benchmark Environment Details
In this section, we go over the details of the benchmark environments used in the experiments.
Table B.1 lists the environments along with their properties, including the dimensionality of the
observation and action spaces and the maximum episode length. Table B.2 provides a list of the
reward function for each environment. Whenever possible, we reuse the original implementations
from the literature as noted in Table B.1 so as to avoid confusion. The environments that start with
“PETS” are from the PETS paper (Chua et al., 2018), which is one of the baseline methods. Most
other environments are from Wang et al. (2019) where the dynamics are the same as the OpenAI
gym version and the reward function in Table B.1 is exposed to the agent. For more details of the
environments, the readers are referred to the original paper.
Note that FixedSwimmer is a modifed version of the original Gym Swimmer environment where
the velocity sensor on the neck is moved to the head. This fix was originally proposed by Wang &
Ba (2020). For the Pendulum environment, we use the OpenAI Gym version. The modified version
in (Wang et al., 2019) uses a different reward function which we have found to be incorrect.
Table B.1: The setup of the environments. The number in the bracket in the “Environment Name”
column denotes the source of this environment: [1] refers to the benchmark paper from Wang et al.
(2019); [2] denotes PETS (ChUa et al., 2018).______________________________________________
Environment Name	Observation Dim	Action Dim	MaximUm Episode Length
PendUlUm	3	1	200
InvertedPendUlUm [1]	4	1	100
Cartpole [1]	4	1	200
Acrobot [1]	6	1	200
FixedSwimmer [1]	9	2	1000
Reacher [1]	11	2	50
Hopper [1]	11	3	1000
Walker2d [1]	17	6	1000
HalfCheetah [1]	17	6	1000
PETS-Reacher3D [2]	17	7	150
PETS-HalfCheetah [2]	18	6	1000
PETS-PUsher [2]	20	7	150
Ant [1]	27	8	1000
Table B.2: Reward Functions. dt denotes the vector between the end effector to the target position.
zt denotes the height of the robot. kvk1 and kvk2 denote the 1-norm and 2-norm of vector v,
respectively. In PETS-PUsher, d1,t is the vector between the object position and the goal and d2,t
denotes the vector between the object position and the end effector.
Environment Name	Reward FUnction
PendUlUm	θ2 +0.lθt2 +0.001a2
InvertedPendUlUm	-θt2
Cartpole	cosθt - 0.01xt2
Acrobot	-cosθ1,t - cos(θ1,t + θ2,t)
FixedSwimmer	Xt - 0.0001IIatk2
Reacher	-kdtk - katk22
Hopper	Xt- 0.1katk2- 3(Zt- 1.3)2
Walker2d	X t- 0.1katk2- 3(Zt- 1.3)2
HalfCheetah	X t- 0.1katk2
PETS-Reacher3D	-kdtk22 - 0.01katk22
PETS-HalfCheetah	X t- 0.1 Ilatk 2
PETS-PUsher	-1.25kd1,tk1 - 0.5kd2,t k1 - 0.1katk22
Ant	Xt- 0.1kat∣∣2- 3(Zt- 0.57)2
13
Under review as a conference paper at ICLR 2022
C Algorithms
In this section, we give the pseudo-code of the proposed algorithms DecentCEM-A and
DecentCEM-P in Algorithm 1 and 2 respectively. We only show the training phase. The algo-
rithm at inference time is simply the same process without the data saving and network update. For
the internal process of CEM, we refer the readers to De Boer et al. (2005); Wang & Ba (2020).
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
Algorithm 1: DecentCEM-A Training
Initialize the policy networks Pi With θi,i = 1,2, •一,M where M is the ensemble size. Planning
horizon H . Initialize the dynamics network fω parameterized by ω. Empty Datasets Dm and Dp
// Episode 1, warmup phase
Rollout using a random policy, fill the dataset Dm with the transition data {(st, at, st+1)}
Update ω using Dm by Mean-Squared Loss	// Train the dynamics network with Dm
// Episode 2 onwards
repeat
t = 0, Dp = {}	// Each episode, reset time and dataset
repeat
foreach policy network pi in the ensemble do
generate reference mean of action sequence distribution μi using Pi and the model fω.
// Apply CEM to refine the action distribution.
// μi, Vi are the mean action sequence of the refined distribution and its expected value
μi,Vi = CEM(μi)
^t,i = μi[0]
end
at = arg max即 Ii Vi	// Pick best distribution
st+1 = steP(at)	// Execute the first action in the mean sequence
Append the transition (st, at, st+1) to Dm
Append the data {(st,at,i)}M=ι to Dp
t = t + 1	// Update time step
until Either reached the maximum episode length or terminal state
Update the model parameter ω using dataset Dm
Update the policy network weights {θi}iM=1 using dataset Dp by the behavior cloning objective
until bored
Algorithm 2: DecentCEM-P Training
Initialize the policy networks Pi with θi,i = 1,2, •一,M where M is the ensemble size. Planning
horizon H . Initialize the dynamics network fω parameterized by ω. Empty Datasets Dm and Dp
// Episode 1, warmup phase
Rollout using a random policy, fill the dataset Dm with the transition data {(st , at , st+1)}
Update ω using Dm by Mean-Squared Loss	// Train the dynamics network with Dm
// Episode 2 onwards
repeat
t = 0, Dp = {}	// Each episode, reset time and dataset
repeat
foreach policy network Pi in the ensemble do
// Apply CEM to refine the distribution of the neural network weight.
// μi, Vi are the mean of the refined weight distribution sequence and its expected value
μi,Vi = CEM(θi)
δi = μi [0]	// Keep the weight at the first step and discard the rest
end
θt = arg maxθ +δ Vi	// Pick the best distribution of weight sequence
at = Pθt (st)
st+1 = steP(at )	// Execute the action returned by the policy network Pθt
Append the transition (st , at , st+1) to Dm
Append the data {δi}iM=1 to Dp
t = t + 1	// Update time step
until either reached the maximum episode length or terminal state
Update the model parameter ω using dataset Dm
Update the policy network weights {θi}iM=1 using dataset Dp by the AVG training objective
until bored
14
Under review as a conference paper at ICLR 2022
D Implementation Details
D.1 Reproducibility
Our implementation is fully reproducible by identifying the sources of randomness and controlling
the random seeds as summarized in Table D.1. The seeds are set once at the beginning of the
experiments.
Table D.1: Random Seed. The set {1,2,3,4,5} refers to the seeds for five runs. Note that we control
the random seed for the environments since there is a random number generator in openai gym
environments independent from other sources
Source of randomness	Random Seed
deep learning framework (tensorflow in our case)
numpy	{1,2,3,4,5 }
Python random module
the training environment	1234
the evaluation environment	0
D.2 Hyperparameters
This section includes the details of the key hyperparameters used in the proposed DecentCEM al-
gorithm (Table D.5) and the baseline algorithms PETS (Table D.3), POPLIN (Table D.4) and SAC6
(Table D.2). For the neural network architecture for the dynamics model, the DecentCEM methods
exactly follow the original one in PETS and POPLIN for a fair comparison, which is an ensemble
of fully connected networks.
Table D.2: Hyperparameters of SAC
Parameter	Value
Actor learning rate Critic learning rate Actor network architecture Critic network architecture	0.0001 0.0001 [dim(observation), 64, 64, 2× dim(action)] [dim(observation)+dim(action), 64, 64, 1]
Table D.3: Hyperparameters of PETS
Parameter	Value
Model learning rate	0.001
Warmup episodes	1
Planning Horizon	30
CEM population size	500 (except in PETS-reacher3D: 400)
CEM proportion of elites	10%
CEM initial distribution variance	0.25
CEM max number of internal iterations	5
6Our SAC implementation used network architectures that are similar to the policy network in our method.
The results of our implementation either matches or surpasses the ones reported in PETS, POPLIN and the
benchmark by Wang et al. (2019)
15
Under review as a conference paper at ICLR 2022
Table D.4: Hyperparameters of POPLINA and POPLINP
Parameter	Value
Model learning rate
Warmup episodes
Planning Horizon
CEM population size
CEM proportion of elites
CEM initial distribution variance
CEM max number of internal iterations
Policy network architecture (A)
Policy network architecture (P)
Policy network learning rate
Policy network activation function
0.001
1
30
500 (except in PETS-reacher3D: 400)
10%
0.25
5
[dim(observation), 64, 64, dim(action)]
[dim(observation), 32, dim(action)]
0.001
tanh
Table D.5: Hyperparameters of the proposed DecentCEM-A/P
Parameter	Value
Model learning rate
Warmup episodes
Planning Horizon
Ensemble Size
CEM population size in each instance
CEM proportion of elites
CEM initial distribution variance
CEM max number of internal iterations
Policy network architecture (A)
Policy network architecture (P)
Policy network learning rate
Policy network activation function
0.001
1
30
5
100 (except in PETS-reacher3D: 80)
10%
0.25
5
[dim(observation), 64, 64, dim(action)]
[dim(observation), 32, dim(action)]
0.001
tanh
16
Under review as a conference paper at ICLR 2022
Figure 9: The iterative sampling process in the 1D optimization task.
E	Full Results
E.1 Detailed visualization of the iterative updates in the 1 D optimization task
Figure 9 is a version of Figure 4 with more iterations. It shows the iterative sampling process of
CEM methods in the 1D optimization task and how the sampling distribution evolve over time.
E.2 Full Learning Curves
In Figure 10, we report the learning curves in all 13 environments that we listed in Appendix B.
The algorithms used in the benchmark are: PETS, POPLIN-A, POPLIN-P and the two modes of the
proposed method DecentCEM-A and DecentCEM-P. We also included the interim and asymptotic
performance of a model-free algorithm SAC as a reference.
The learning curves in some environments can be noisy. We apply smoothing with 1D uniform filter
to the results of the following environments for easier interpretation: Cartpole, FixedSwimmer,
Hopper, Walker2d, HalfCheetah, PETS-Reacher3D, PETS-Pusher. The window size of the filter
was 10 for all but Cartpole, where 30 was used due to its high noise level for all algorithms.
Note that the performance of the baseline methods may be different from the results reported in their
original paper. Specifically, in the paper by Wang & Ba (2020), PETS, POPLIN-A and POPLIN-P
have been evaluated in a number of environments that we use for the benchmark. Our benchmark
results may not be consistent with theirs due to differences in the implementation and evaluation
protocol. For example, our results of PETS, POPLIN-A and POPLIN-P in the Acrobot environment
17
Under review as a conference paper at ICLR 2022
2000	4000	6000	8000 IOOOO	O IOOO 2000	3000	4000	5000	O IOOOO 20000	30000	40000	50000
Steps	Steps	Steps
s∙o*"j"3
O IOOOO 20000	30000	40000	50000
Steps
O 20∞	4000	6000 SOOO IOOOO
Steps
O 2000	4000	6000	8000 IOOOO
Steps
PETS-Pusher
PETS-Reacher3D
PETS-HaIfCheetah
O 1∞∞	20000	30000	40000	50000
Steps
2000	4000	6000 SOOO IOOOO 12000 IWOO
Steps
O IOOO 2000	3000	4000	5000	6000	7000
Steps
Ant
25« 4h--------------
O 20000	400∞	60000	80000 IOOOOO
Steps

号8
----PETS ------ POPLIN-A ------ POPLIN-P ----- DecentCEM-A ------ DecentCEM-P ------ SAC —— SAC at convergence
Figure 10: The learning curves of the proposed DecentCEM methods and the baseline methods on
continuous control environments. The line and shaded region shows the mean and standard error
of evaluation results from 5 training runs using different random seeds. Each run is evaluated in an
environment independent from training and reports average return of 5 episodes at every training
episode. To ensure that the evaluation environments are the same across different methods and
multiple runs, we set a fixed random seed in the evaluation environment of each task.
18
Under review as a conference paper at ICLR 2022
are all better than the results in Wang & Ba (2020). We have identified a bug in the POPLIN code
base that causes the evaluation results to be on a wrong timescale that is much slower than what it
actually is. Hence the results of our implementation look far better, reaching a return of 0 at about
4k steps as opposed to 20k steps reported in Wang & Ba (2020).
E.3 Analysis
Let’s group the environments into two categories based on how well the decentralized methods
perform in them:
1.	Pendulum, InvertedPendulum, Acrobot, Cartpole, Reacher, Walker2D, HalfCheetah,
PETS-Pusher, PETS-Reacher3D, Ant,
2.	FixedSwimmer, Hopper, PETS-HalfCheetah
The first category is where the best performing method is one of the proposed DecentCEM al-
gorithms: DecentCEM-A or DecentCEM-P. In environments where the baseline POPLIN-A or
POPLIN-P could reach near-optimal performance such as pendulum and invertedPendulum, ap-
plying the ensemble method would yield similar performance as before. In Acrobot, Reacher,
Walker2D and PETS-Pusher, applying the decentralized approach increases the performance in both
action space planning (“A”) and parameter space planning (“P”). In Pendulum, InvertedPendulum,
Cartpole, HalfCheetah, PETS-Reacher3D and Ant, ensemble helps in the action space planning but
either has no impact or negative impact on the parameter space planning.
The second category is where it is better not to use a decentralized approach with multiple instances
(note that by using one instance in the decentralized methods, we can recover one of POPLIN-
A, POPLIN-P). In Hopper, the issue might lie in the MBRL approach in general since all MBRL
baselines performed worse than the model-free baseline SAC. One possible issue is that the true dy-
namics is difficult to approximate with our model learning approach. Another possibility is that it’s
necessary to learn the variance of the sampling distribution, which none of these MBRL approaches
do. To be clear, the variance is adapted online by CEM but it is not learned. In FixedSwimmer,
the ensemble approach performs worse in both action space and parameter space planning. One
potential reason for the result is that the optimization landscape is complex and needs more samples
to estimate the expected return than what was used in each instance of the ensemble. In this case, it
is better not to distribute the population into several instances. An alternative is to increase the total
population size, with a downside of increasing computation. PETS-HalfCheetah is slightly different
in that the ensemble does improve the performance significantly when used for action space plan-
ning. However, POPLIN-P performs significantly better than all other algorithms. This suggests
that the parameter space planning has been able to successfully find a high return region using a
single Gaussian distribution. In this case, distributing the population size would not be able to trade
the estimation accuracy for better global search.
F More Ablation
Pendulum
OTC0B< ‰0 *93ubSIa BsIMJ5d
9400
9500 9600 9700
Steps
(a) Action Distance
9800
EnsH 9≈e9λv
Pendulum
——POPUN-A
---DecentCEM-A-E2
---DecentCEM-A-ES
---DecentCEM-A-E4
---DecentCEM-A-ES
Pendulum
0	2000	4000	6000 BOOO 10000
Steps
(C) Policy Control
0	2000	4000	6000 BOOO 10000
Steps
(b) Ensemble Size
Figure 11: More Ablation (a) Pairwise distance between the actions of 5 instances in DecentCEM-
A during training (b) Ensemble size ablation: E2 denotes an ensemble size of 2 (c) Policy control
performance where the policy network is directly used for control without CEM policy improvement
19
Under review as a conference paper at ICLR 2022
Figure 11 (a) is an additional plot for the ensemble diversity ablation. It shows the statistics of the
pairwise distance between the output actions by the CEM instances at each time step. The same as
in Fig. 8 (b), we only show a time window toward the end of training for visual clarity and the line
and shaded region represent the mean and min/max distances.
Figure 11 (b) shows the performance of the DecentCEM-A algorithm under different ensemble size
(where we keep the population size of each instance the same). Adjusting the ensemble size has
an impact on the performance of DecentCEM-A. An ensemble size of 3 or 4 could improve the
result reported in Fig. 10 to reach a near-optimal performance. We did not show the results for the
parameter space planning mode “P” since all variations achieve near-optimal performance and their
curves overlap. We fixed an ensemble size of 5 throughout our benchmark but tuning the ensemble
size for each environment could improve the performance further.
We also study the performance of policy control where the policy network is directly used for control
without the CEM step, denoted by the extra suffix “-PC”. The result is shown in Fig.11 (c). Without
the policy improvement from CEM, all algorithms perform worse than their counter-part of using
CEM. POPLIN-P-PC and DecentCEM-P-PC both get stuck in local optima and do not perform
very well. This makes sense since the premise of planning in parameter space is that CEM can
search more efficiently there. The policy network is not designed to be used directly as a policy.
Interestingly, DecentCEM-A-PC achieves a high performance from about 6k steps (30 episodes) of
training. The ensemble of policy networks adds more robustness to control than using a single one.
G Overhead of the Ensemble
The sample efficiency is not impaired when going from one policy network to the multiple policy
networks used in DecentCEM-A and DecentCEM-P. This is because that the generation of the train-
ing data only involve taking imaginary rollouts with the model, rather than interacting with the real
environment, as discussed in Section 4.
In terms of the population size (number of samples drawn in CEM), the DecentCEM based methods
do not impose additional cost. We show in both the motivational example (Sec. 3) and the benchmark
experiments (Appendix E) that the proposed methods work better than CEM under the same total
population size.
The additional computational cost is reasonable in DecentCEM compared to POPLIN. Each branch
of policy network and CEM instance runs independently from the others, allowing for a parallel
implementation. The instances have to be synchronized (arg max) but its additional cost is minimal.
One caveat with our current implementation though is that it is serial, which slows down the speed.
This is a limitation that calls for future work of a parallel implementation.
H	Convergence Analysis of Decentralized CEM
This section analyzes the convergence of the proposed DecentCEM algorithm in optimization.
Consider the following optimization problem:
x* ∈ arg max V(x)	(4)
x∈X
where X ⊂ Rn is a non-empty compact set and V(∙) is a bounded, deterministic value function to be
maximized. We assume that this problem has a unique global optimal solution x* but the objective
function V(∙) may have multiple local optimum and may not be continuous.
We will show that the existing convergence result of CEM in continuous optimization established
in (Hu et al., 2011) also applies to DecentCEM. It assumes that the sampling distribution gφ(x)
in CEM is in the natural exponential families (NEFs) which subsumes Gaussian distribution (with
known covariance). We restate the definition of NEFs for completeness (definition 2.1 from (Hu
et al., 2011)):
Definition H.1 (Natural Exponential Family). A family of parameterized distributions {gφ(∙),φ ∈
Φ ⊂ Rd} on X ⊂ Rn is called a Natural Exponential Family (NEF) if there exists continuous map-
pings Γ : Rn → Rd, h : Rn → R and K : Rd → R such that gφ(x) = exp(φ>Γ(x) - K (φ))h(x),
20
Under review as a conference paper at ICLR 2022
where the parameter space Φ = {φ ∈ Rd : |K(φ)∣ < ∞}, K(φ) = ln JX exp(φ>Γ(x))h(x)ν(dx)
and ν is the Lebesgue measure of X.
The mean vector function is denoted as m(φ) = Eφ[Γ(x)] where the expectation Eφ is with respect
to gφ and it can be shown that m(∙) is invertible. Note that the expression of the densities can be
simplified when restricted to a multivariate Gaussian distribution (with known diagonal covariance)
where the natural sufficient statistics Γ(x) is the identify function.
We then present the CEM algorithm below to fix notations. It follows Algorithm 2 in (Hu et al.,
2011) but is modified to align with some notations introduced in previous sections in our paper.
1
2
3
4
5
6
7
8
9
10
Algorithm 3: CEM
Choose the family of distributions gφ(x), x ∈ X from NEFs defined in H.1 and the initial parameter
φ0 ∈ int(Φ) where int denotes the interior of the parameter space Φ.
Specify elite ratio ρ ∈ (0, 1) and step size sequence {αk} and {λk } where k denotes the time step. Set
k = 0. Specify > 0 which is the parameter in the thresholding function defined in Equation 5.
[1, if X ≥ Y
l(x,γ) =	x-γ+, if γ - e<x< 1	(5)
∣0, if x ≤ γ — e
repeat
Step 1: Draw Nk i.i.d samples Λk = {x1 , x2, ..., xNk } from the distribution gφk (x)
Step 2: Calculate the sample (1 — ρ)-quantile Yk = V(d(i-ρ)Nk]) where「ae is the ceiling function
that gives the smallest integer greater than a and V(i) is the ith-order statistics of the sequence
{V(xj)}N=kι where V(∙) is the objective function to be maximized.
Step 3: Compute the new parameter φk+1 = m-1 (ηk+1), where η0 = m(φ0) = Eφ0 (Γ(x)) and
Px∈Λk L(V(x),Yk)r(x) “	√ λk Xrd …∖ 仆
ηk+1 = αk	P∈ΛfcKV(x),Yk)	+(1 -αk) I N ∑ γ(X) + (1 -λk)ηk I ⑹
x∈Λk	x∈Λk
Step 4: k = k + 1
until a stopping condition is reached
return φk
The convergence results will require the following assumptions from (Hu et al., 2011):
Assumption 1. The parameter φk+1 computed at step 3 of Algorithm 3 satisfies φk+1 ∈ int(Φ) for
all k.
Assumption 2. The step size sequence {αk} satisfies: αk > 0 ∀ k , limk→∞ αk = 0 and
∞
k=0 αk = ∞.
Assumption 3. The sequence {λk} satisfies λk = O(k-λ) for some constant λ ≥ 0 and the sample
size Nk = Θ(kβ ) where β > max{0, 1 - 2λ}.
Assumption 4. The (1 一 ρ)-quantile of {V(x),x 〜gφ(x)} is Uniquefor each φ ∈ Φ.
We know from Hu et al. (2011) that the sequence {ηk}k∞=0 from equation 6 asymptotically ap-
proaches the solution set of the ODE:
呼=L(η)	(7)
dt
L(η) = ∆φlnEφ[l(V(x),γ(m-1(η)))] ∣φ=m-i(η)	(8)
where γ(m-1(η)) is the true (1 一 ρ)-quantile of V(x) under gm-1(η).
Assumption 5. The function L(η) defined in equation 8 has a unique integral curve for a given
initial condition.
The above assumptions 1-5 are the assumptions required by the previous convergence result of CEM.
To show the convergence of DecentCEM, we only require one additional mild condition:
21
Under review as a conference paper at ICLR 2022
Assumption 6. Let M be the number of instances in DecentCEM and each instance has a sample
size of NMk where Nk is the total number of samples that satisfies the assumption 3. M is constant
and 0 < M < Nk ∀ k.
Now we restate the convergence result of DecentCEM from the main text and show the proof:
Theorem 3.1 (Convergence of DecentCEM). If Assumptions 1-5 hold for a CEM instance described
in Algorithm 3 and we decentralize itby evenly dividing its sample size Nk into M CEM instances in
DecentCEM algorithm that satisfies Assumption 6, then the sequence of iterates {ηi,k} generated by
each CEM instance indexed by i converges almost surely to an internally chain recurrent set (Hirsch
et al., 2001) of Equation 7. Furthermore, the solution of DecentCEM {ηo,k} converges almost surely
to the best solution of the individual instances in terms of the expected value of Em-1(η) [V (x)].
Proof. Each individual CEM instance has a sample size of M. Under Assumption 3, Nk = Θ(kβ).
Since Assumption 6 holds, M is constant and gets absorbed into the Θ and We have M = Θ(kβ).
Hence the conditions of Theorem 3.1 in (Hu et al., 2011) holds for each CEM instance indexed by
i and can be directly applied to shoW the almost sure convergence of their solutions {ηi,k} to an
internally chain recurrent set of Equation 7. If the recurrent sets are isolated equilibrium points, then
{ηi,k} converges almost surely to a unique equilibrium point.
Due to the fact that the instances in DecentCEM run independently from each other, their solutions
{ηi,k}iM=1 (or equivalently {φi,k}iM=1 = {m-1(ηi,k)}iM=1) might converge to identical or different
solutions denoted as {噌}M=ι. DecentCEM computes the final solution by applying an arg max
over all individual solutions: ηo,k = arg maxη∈{η }M Em-1 (η) [V (x)] (equivalent to Equation 3).
Here the expectation is approximated by the sample mean With respect to the distribution gm-1 (η) :
N PN=I V(xj), which converges almost surely to the true expectation according to the strong law
of large numbers. Hence We have that ηo,k converges almost surely to the best solution in the set
{η*}M=ι found by the individual CEM instances, in terms of the expected value of Em-1(η)[V (x)].
□
Note that the theorem implies that the solution of CEM / DecentCEM assigns the maximum proba-
bility to a locally optimal solution to Equation 4. It does not suggest whether this local optimum is
a global optimum or not. To the best of our knowledge, almost sure convergence to a local optimum
is the only convergence result that has been established about CEM in continuous optimization.
22