Under review as a conference paper at ICLR 2022
Explaining Off-Policy Actor-Critic From A
Bias-Variance Perspective
Anonymous authors
Paper under double-blind review
Ab stract
Off-policy Actor-Critic algorithms have demonstrated phenomenal experimental
performance but still require better explanations. To this end, we show its policy
evaluation error on the distribution of transitions decomposes into: a Bellman er-
ror, a bias from policy mismatch, and a variance term from sampling. By compar-
ing the magnitude of bias and variance, we explain the success of the Emphasizing
Recent Experience sampling and 1/age weighted sampling. Both sampling strate-
gies yield smaller bias and variance and are hence preferable to uniform sampling.
1	Introduction
A practical reinforcement learning (RL) algorithm is often in an actor-critic setting (Lin, 1992;
Precup et al., 2000) where the policy (actor) generates actions and the Q/value function (critic)
evaluates the policy’s performance. Under this setting, off-policy RL uses transitions sampled from a
replay buffer to perform Q function updates, yielding a new policy π. Then, a finite-length trajectory
under π is added to the buffer, and the process repeats. Notice that sampling from a replay buffer
is an offline operation and that the growth of replay buffer is an online operation. This implies
off-policy actor-critic RL lies between offline RL (Yu et al., 2020; Levine et al., 2020) and on-
policy RL (Schulman et al., 2015; 2017). From a bias-variance perspective, offline RL experiences
large policy mismatch bias but low sampling variance, while on-policy RL has a low bias but high
variance. Hence, with a careful choice of the sampling from its replay buffer, off-policy actor-critic
RL may achieve a better bias-variance trade-off. This is the direction we explore this work.
To reduce policy mismatch bias, off-policy RL employs importance sampling with the weight given
by the probability ratio of the current to behavior policy (the policy that samples the trajectories)
(Precup et al., 2000; Xie et al., 2019; Schmitt et al., 2020). However, because the behavior policy is
usually not given in practice, one can either estimate the probability ratio from the data (Lazic et al.,
2020; Yang et al., 2020; Sinha et al., 2020) or use other reasonable quantities, such as the Bellman
error (Schaul et al., 2016), as the sampling weight. Even using a naive uniform sampling from the
replay buffer, some off-policy actor-critic algorithms can achieve a nontrivial performance (Haarnoja
et al., 2018; Fujimoto et al., 2018). These observations suggest we need to better understand the
success of off-policy actor-critic algorithms, especially in practical situations where a fixed behavior
policy is unavailable.
Our contributions are as follows. To understand the actor-critic setting without a fixed behavior
policy, we construct a non-stationary policy that generates the averaged occupancy measure. We use
this policy as a reference and show the policy evaluation error in an off-policy actor-critic setting
decomposes into the Bellman error, the policy mismatch bias, and the variance from sampling.
Since supervised learning during the Q function update only controls the Bellman error, we need
careful sampling strategies to mitigate bias and variance. We show that the 1/age weighting or its
variants like the Emphasizing Recent Experience (ERE) strategy (Wang & Ross, 2019) are preferable
because both their biases and variances are smaller than that of uniform weighting.
To ensure the applicability of our explanation to practical off-policy actor-critic algorithms, we adopt
weak but verifiable assumptions such as Lipschitz Q functions, bounded rewards, and a bounded ac-
tion space. We avoid strong assumptions such as a fixed well-explored behavior policy, concentra-
tion coefficients, bounded probability ratios (e.g., ratios of current to behavior policy), and tabular
or linear function approximation. Hence our analysis is more applicable to practical settings. In
1
Under review as a conference paper at ICLR 2022
addition, our analysis suggests that 1/age-based samplings (e.g., ERE, ERE_apx, 1/age) have advan-
tages in bias and variance. Our experiments verify that SAC (Haarnoja et al., 2018) with 1/age-based
samplings outperform the prior work. Thus, our results not only provide theoretical foundations for
practical off-policy actor-critic RL algorithms but also achieve better performances.
2	Preliminaries
2.1	Reinforcement learning
Consider an infinite-horizon Markov Decision Process (MDP) hS, A, T, r, γi, where S, A are
finite-dimensional continuous state and action spaces, r(s, a) is a deterministic reward function,
γ ∈ (0, 1) is the discount factor, and T (s0 |s, a) is the state transition density; i.e., the density of
the next state s0 given the current state and action (s, a). Given an initial state distribution ρ0 , the
objective of RL is to find a policy π that maximizes the γ-discounted cumulative reward when the
actions along the trajectory follow π:
∞
max J(∏) = maxE X Yir(Si,ai)卜0 〜Po, ai 〜∏(∙∣Si), Si+1 〜T(∙∣Si,ai) .	(1)
π	π	i=0
Let ρi(s∣ρo, ∏, T) be the state distribution under ∏ at trajectory step i. Define the normalized state
occupancy measure by
∞
P∏0(S) , (I-Y) XYiPe(SWo, π, t).	⑵
i=0
Ideally, the maximization in (1) is achieved using a parameterized policy πθ and policy gradient
updates with (Sutton et al., 1999; Silver et al., 2014):
VeJ(πθ) = (I-Y)TE(s,a)〜ρ^ [(Vθlogπθ(als))Qπθ(s,a)].	⑶
Here pρθ (s, a) = Pn (s)∏θ(a|s) is given in (2), and Qπθ is the Q function under policy ∏:
∞
Qπ(s,a) = E X YIr(Si, ae)卜0 = s, ao = a, ae 〜∏(∙∣se), si+1 〜T(∙∣se,ae) .	(4)
i=o
Off-policy RL estimates Qπby approximating the solution of the Bellman fixed-point equation:
(Bn Qπ )(s,a) = r(s,a) + YEsO 〜T (∙∣s,α), a0 〜π(a∣s)Qπ (s0,aO)= QK (s,a).
It is well-known that Qπ is the unique fixed point of the Bellman operator Bπ. Hence if
(BπQ)(s, a) ≈ Q(s, a), then Q may be a “good” estimate of Qπ. In the next subsection, we
will see that an off-policy actor-critic algorithm encourages this to hold for the replay buffer.
2.2 Off-policy actor-critic algorithm
We study an off-policy actor-critic algorithm of the form shown in Alg. 1. In line 2, for episode
index e, πe samples one trajectory of length L. The transitions in this trajectory are then added to
the replay buffer. Since the policies for different episodes are distinct, the collection of transitions
in the replay buffer are generally inconsistent with the current policy. Notice the trajectory is simply
collected by πe, not a perturbed version ofπe (e.g., gaussian corrupted version of pie).
Line 3 is a supervised learning that minimizes the Bellman error of Q using gradient descents,
e
making (Bπ Q)(s, a) ≈ Q(s, a) for (s, a) in the replay buffer. When this holds, we will prove that
e
the “distance” between Q and Qπ become smaller over the replay buffer. This is a crucial step for
line 4 to be truly useful. In line 4, Qπe is replaced by Q, and the policy is updated accordingly.
In practice, line 3 is replaced by mini-batch updates (Fujimoto et al., 2018; Haarnoja et al., 2018)
where the summation of J(Qφ) is approximated by a sum over the mini-batches. In section 5.2,
we show that a uniform-weighted mini-batch sampling biases learning towards older samples; this
motivates the need for a countermeasure.
2
Under review as a conference paper at ICLR 2022
Algorithm 1 Off-policy Actor-critic Algorithm
Require: Parameterized policy πe = πθe and Q function estimate Qφ . Learning rate α.
1:	for episode e = 1, 2, ... do
2:	SamPlealength-L trajectory {(se, ae)}L=-01 with initial distribution ρ0(s) and policy πe(a∣s).
3:	Evaluate the Bellman error as J(Qφ) = P；= PL-o1 (Qφ(sj, aj) - (BKeQφ)(sj,°j)/and
update Qφ as φ  φ - aVφ J(Qφ).
,	A	∙	. 1 ■ ∕c∖	χ-7 ʃ / ∖ ∕τ	∖一1 ττn	Γ ∕χ-7 1	/ I ∖ ∖ ʌ /	∖ 1	1
4:	Approximate Eq. (3) as Vθ J(∏θ) = (1 - Y) 1E(s,a)〜。部[(Vθ log ∏θ(a∣s))Q(s,a)] and up-
date as θe+1 = θe + αVθJ(πθ).
5:	end for
Note that Alg. 1 seems to be inconsistent in the horizon because the Q function, Eq. (4), is defined
in the infinite horizon but the trajectories are finite-length. In Corollary 1, we will address this
inconsistency by approximating the Q function using finite-length trajectories.
2.3 The construction of our behavior policy
Although Alg. 1 doesn’t have a fixed behavior policy, in Lemma 1, we construct a non-stationary
policy that generates the averaged occupancy measure at every step. This describes the averaged-
over-episode behavior at every trajectory step i of the historical trajectory {(sie, aie)}iL,e-=10,N,1. We
hence define it to be our behavior policy and use it as a reference when analyzing Alg. 1.
The construction is as follows. Denote the state distribution at trajectory step i in episode e as
Pe(S) = ρi(s∣ρ0, ∏e, T). By Eq. (2), ρ∏e is the state occupancy measure generated by (ρ0, ∏e, T).
e
Intuitively, ρρπ0 describes the discounted state distribution starting at trajectory step 0 in episode e.
More generally, define ρρπee as the state occupancy measure generated by (ρie, πe, T). Then, ρρπee
describes the discounted state distribution starting at trajectory step i in episode e.
∞
ρρπiee(s) , (1 - γ) Xγj-iρje(s)
j=i
(5)
Behavior policy through averaging. Since Alg. 1 considers trajectories from all episodes, the
average-over-episodes distribution %(i) * * * = N-1 Pe ρρπee will be of interest. Namely, %(i) is the
average of all occupancy measures starting at step i. To describe the average-over-episodes behavior
at step i, we want a policy that generates %(i), and Eq. (5) helps construct such a policy. Concretely,
let Pi, ∏D(a|s) be the averaged state distribution and the averaged policy at step i, respectively.
NN
%⑴(S) , NT XPne(s),	Pi(S) , NT X Pns)
PILI ne(a|S)Pne(S)
PL Pne ⑶
(6)
Lemma 1 shows ∏D in Eq. (6) is a notion of behavior policy in the sense that (Pi, ∏D, T) generates
%(i); i.e., ∏D generates the averaged occupancy measures when the initial state follows Pi. Since ∏D
describes the averaged discounted behavior starting at step i, we define it to be our behavior policy.
This is a key to analyzing the policy evaluation error in Alg. 1.
∏D	_
Lemma 1. Let PPi (s) be the normalized state occupancy measure generated by (Pi, ∏D, T). Then
%Ci)(S) = Pni (s) a.e. and hence from Eq. (6), N T PN=I Pne (S)πe(a∣S) = Pni (s)πd (a∣S) a.e.
3 Related work
There are two main approaches to off-policy RL: importance sampling (IS) (Tokdar & Kass, 2010)
and regression-based approach. IS has a low bias but high variance, while the opposite holds for the
regression-based approach. Below, we briefly review these techniques.
3
Under review as a conference paper at ICLR 2022
Importance Sampling. Standard IS uses behavior policy to form an importance weight from the
probability ratio of the current to the behavior policy. For a fully accessible behavior policy, exam-
ples of this approach include: the naive IS weight (Precup et al., 2000), importance weight clipping
(Schmitt et al., 2020) and the marginalized importance weight (Xie et al., 2019; Yin & Wang, 2020;
Yin et al., 2021). Alternatively, one can use the density ratio of the occupancy measures of the
current to the behavior policies (Liu et al., 2018). This is estimated using a maximum entropy ap-
proach (Lazic et al., 2020), a Lagrangian approach (Yang et al., 2020), or a variational approach
(Sinha et al., 2020). A distinct approach emphasizes experience without considering probability
ratios. Examples include emphasizing samples with a higher TD error (Schaul et al., 2016; Horgan
et al., 2018), emphasizing recent experience (Wang & Ross, 2019) or updating the policy towards
the past and discarding distant experience (Novati & Koumoutsakos, 2019). It is also shown that IS
on replay buffer is equivalent to weighting on loss functions (Fujimoto et al., 2020).
Regression-based. A regression-based approach can achieve strong experimental results using
proper exploration and function approximation (Fujimoto et al., 2018; Haarnoja et al., 2018). It also
admits strong theoretical results such as generalization error using a concentration coefficient (Le
et al., 2019), policy evaluation error using bounded probability ratios (Agarwal et al., 2019)[Chap
5.1], minimax optimal bound under linear function approximation Duan et al. (2020), confidence
bounds constructed by kernel Bellman loss Feng et al. (2020). However, these settings require a
fixed behavior policy and bounded probability ratios, which rarely hold in practice. We hence con-
struct a non-stationary behavior policy to avoid this issue.
Combined. Prior work also suggests that combining IS and the regression-based yields robust
results (Dudik et al., 2011; Jiang & Li, 2016; Thomas & BrUnskilL 2016; KallUs & Uehara, 2020).
It is also common to consider some refined contractions to improve stability. Examples include n-
step Q-learning (Hessel et al., 2018), Retrace (Munos et al., 2016), Peng’s Q (Kozuno et al., 2021),
and other correction techniques (Harutyunyan et al., 2016; Tang et al., 2020; Rowland et al., 2020).
4	Policy evaluation error of off-policy actor-critic algorithms
N
Let Q*, Qn be the Q function of the optimal policy and πN, respectively. Let Q be the estimated
NN
Q function. The performance error |Q - Q*| decomposes into |Q - Qn | + ∣Qπ - Q*|. The first
N
term |Q - Qπ | is the policy evaluation error and is the focus in the off-policy evaluation literature
(Duan et al., 2020). The second term is the policy’s optimality gap and to bound this term currently
requires strong assumptions such as tabular or linear MDPs (Jin et al., 2018; 2020). In an off-policy
actor-critic setting, the policy evaluation error has not been analyzed adequately since most analysis
requires a fixed behavior policy. This is the focus of our analysis.
Suppose we are given the trajectories sampled in the past episodes (the replay buffer). We ana-
lyze the policy evaluation error over the expected distribution of transitions. We express this error
in terms of the Bellman error of Q, the bias term in 1-Wasserstein distance between the policies
(πN , πiD), and the variance term in the number of trajectories N. Note πiD is the behavior policy at
trajectory step i defined in Eq. (6). The use of 1-Wasserstein distance (Villani, 2008) makes the re-
sults applicable to both stochastic and deterministic policies. Since supervised learning only makes
the Bellman error small, we need good sampling strategies to mitigate the bias and variance terms.
We hence investigate sampling techniques from a bias-variance perspective in the next section.
4.1	Problem setup
Notation. In episode e, a length-L trajectory {(sie , aie)}iL=-01 following policy πe is sampled
(Alg. 1, line 2). Then, Q is fitted over the replay buffer (line 3). Because the error of Q at step
i depends on the states sampled at steps i, i + 1, . . . , the importance of these samples (s, a) depends
on the trajectory step i. Also, due to the discount factor, the importance of step j > i is discounted
by γj-i relative to step i. Hence, we will use a Bellman error and a policy mismatch error that
reflects the dependency on the trajectory step and the discount factor. For f : S × A → R and
g : S → R, define an averaging-discounted operator over the replay buffer in N episodes:
N L-1	N L-1
ELf(∙,∙) , NN XX (1-γ)γj-i f (sj,aj) and ELg(∙)，NN XX(I-Y )γ Ig(Se
e=1 j=i	e=1 j=i
4
Under review as a conference paper at ICLR 2022
Using EL, the Bellman error of Q and the distance between the policies (∏N, ∏D) on the replay
buffer are written as
eQL = ELIQ(∙,∙)-(BnN Q)(∙,",
Wi,L = ELW1(∏N ∣∣∏D )(∙).
(7)
W1(∏N∣∣∏D)(s) = Wi(∏n(∙∣s)∣∣∏D(∙∣s)) is the I-WaSSerStein distance between two policies at
state s, which can be viewed as a function of (s, a). Both the Bellman error e^L and the policy
Q
mismatch error W1i,L depend on trajectory step i and are both discounted by γ.
Assumptions. We now relate the Bellman error and the policy mismatch error defined in Eq. (7)
to the policy evaluation error |Q - QπN |. First, to control the error of Q by policy mismatch error in
N
W1 distance, we assume that for every state, Q and Qπ are LA-Lipschitz over actions. We provide
reasoning for this assumption in the later discussion. Next, observe that both quantities in Eq. (7)
are random with sources of randomness from initial states, policies at different episodes, and the
state transitions. To control this randomness, we need assumptions on the Q functions and apply
a concentration inequality. Because Alg. 1 only samples one trajectory in each episode, higher-
order quantities (e.g., variance) is unavailable. This motivates us to use first-order quantities (e.g., a
uniform bound on the Q functions) and apply Hoeffding’s inequality. Hence, we assume that Q(s, a)
N
and Qπ (s, a) are bounded in the interval [0, rmax/(1 - γ)] and that the action space is bounded
with the diameter diamA . A justification of these assumptions is provided in Appendix A.2.
4.2	Policy evaluation error
Observe that the Q function, Eq. (4), is defined on infinite-length trajectories while the errors, Eq. (7),
are evaluated on length-L trajectories. Discounting makes it possible to approximate the Q functions
using finite-length L. To approximate the Q function at trajectory step i up to a constant error, we
need samples at least to step i + Ω((1 - γ)-1). Hence, We first prove the main theorem with i = 0
and L = ∞. Then, we generalize the result to i ≥ 0 and finite L in Corollary 1.
Theorem 1. Let N be the number of episodes. For f : S × A → R, define the operator E0∞f ,
N PN=I P∞=o(1 — Y)γif (Se, aɛ). Denote the policy mismatch error and the Bellman error as
W0,∞ = E∞W](∏N ∣∣∏D )(∙) and e0^,∞ = E∞∣Q(∙, ∙) — (BnN Q)(∙, ∙)∣ ,respectively. Assume
Q
N
(1)	For each s ∈ S, Q(s, a) and Qn (s, a) are LA-Lipschitz in a.
N
(2)	Q(s, a) and Qn (s, a) are bounded and take values in [0, rmax/(1 - γ)].
(3)	The action space is bounded with diameter diamA < ∞.
Then, with probability at least 1 - δ,
E(so ,a。)〜ρo(s)∏D (a|s) IQ(S0, aO) - Q	(s0,aO)I
≤ Fax rɪ^θgɪ+ (户2 + 2LAdiamA) Z+ɪ k0τ + 2LaW°,∞).
1 - γ 2N	δ (1 - γ )2	1 - γ N δ 1 - γ Q
Theorem 1 expresses the error of Q as the sum of Bellman error, bias, and variance terms. To be
more specific, the first two terms are understood as the “variance from sampling” because these
decrease in the number of episodes N. On the other hand, W1O,∞ is the “policy mismatch bias” w.r.t.
πN. Because the behavior policy πOD is a mixture of the historical policies {πe }eN=1 , Eq. (6), we
expect it to increase in N until πN begins to converge.
Theorem 1 only indicates the difference between Q and QnN at i = 0 and infinite-length trajectories.
We can generalize it to i ≥ 0 and finite-length trajectories as follows. Recall from Eq. (6) and
Lemma 1, the average state distribution at the i-th step, ρi, and the behavior policy at the i-th step,
5
Under review as a conference paper at ICLR 2022
∏D, generate the average state occupancy measure, i.e., ρ∏i = N-1 PN=I Pne a.e. Therefore, by
restricting attention to the states sampled at time steps i, i + 1, . . . the “initial state distribution”
and the behavior policy become Pi and ∏D, which generalizes Theorem 1 from i = 0 to i ≥ 0.
In addition, due to γ-discounting, we may use length-L trajectories to approximate infinite-length
ones, provided that L ≥ i + Ω((1 - γ)-1). These observations lead to the following corollary.
Corollary 1. Fix assumptions (1) (2) (3) of Theorem 1. Rewrite the policy mismatch error and the
Bellman error as Wi,L = ELWι(πN∣∣πD )(∙) and GL = EL ∣Q(∙, ∙) — (BKN Q)(∙, ∙)∣ ,respectively.
Note EL is defined in Eq. (7). Then, with probability at least 1 — δ,
E(Si ,ai )7ρi(s)∏D (a|sj Q(Si, ai) - Qn (si,ai)| ≤
max rɪ^ogf+(1+aA )(∏κι+γL-i)+J (GL+2law∩
1 - γ 2N	δ (1 - γ )2	1 - γ N δ	1 - γ Q	1
Moreover, if i ≤ L — IgY with 0 < e < 1 and the constants are normalized as LA = c/(1 — Y),
GL = ξi,L∕(1 — Y), then, with probability at least 1 — δ,
E(si,ai)〜Pi(s)πD(a∣s)lQ(Si,aJ - Qn (Si, ai)|
≤O((T-Yy ((rmaχ + C ∙ diamA)Q∕√ + ≡) + ξQ + C ∙ Wi'L))
where O(∙) is a variant of O(∙) that ignores logarithmic factors.
Note that if i = 0 and L = ∞, Corollary 1 is identical to Theorem 1. Because the Bellman error
of Q and the bias of the policy are both evaluated using the averaging-discounted operator EL,
N
Corollary 1 implies the difference of Q and Qπ at trajectory step i mainly depends on states at
trajectory steps ≥ i. Since the Q function is a discounted sum of rewards from the current step to
the future, the error at step i should mainly depend on the steps ≥ i.
Normalization. Although the first conclusion in Corollary 1 gives a bound on the error of Q, the
constants may implicitly depend on the expected horizon (1 —Y)-1. Hence its interpretation requires
care. For instance, LA, the Lipschitz constant of the Q functions w.r.t. actions, is probably the most
tricky constant. While it is used extensively in the prior work (Luo et al., 2019; Xiao et al., 2019; Ni
et al., 2019; Yu et al., 2020), its magnitude is never properly addressed in the literature. Intuitively,
if a policy π is good enough, it should quickly correct some disturbance on actions. In this case,
the rewards after the disturbance only differ in a few trajectory steps, so the Lipschitzness of Qπ in
actions is sublinear in (1 — Y)-1. On the other hand, if the policy π fails to correct a disturbance
δ, due to error propagation, the error δ propagates to every future step, leading to a linear error
dependency to the horizon O(δH). Therefore, the Lipschitzness of Qπ in actions can be as large
as O((1 — Y)T). In addition to La, the Bellman error βi^L should scale linearly in (1 — Y)T
because the Q function represents the discounted cumulative reward, Eq. (4), which scales linearly
in (1 — γ)-1. These observations suggest that e^L and LA in Corollary 1 are either linear in the
Q
horizon or lie between sublinear and linear. To better capture the dependency on the horizon, we
normalize the constants and get the second conclusion.
Interpretation. The second conclusion shows the approximation error from infinite to finite-
Taylor
length trajectories is bounded by a constant for i ≤ L — ɪog^ ≈ L — Ω((1 — Y)-1), and will
become harder to control for the higher i due to the lack of samples. Besides, the variance term
dominates when N is small, while the bias term dominates at large N. Therefore, one may imagine
that the training of Alg. 1 has two phases. At phase 1, the variance term dominates and decreases
in N, so the learning improves quickly as more trajectories are collected. At phase 2, the bias term
dominates, so the policy evaluation error becomes harder to improve and πN tends to converge.
6
Under review as a conference paper at ICLR 2022
5	Practical sampling strategies
As previously mentioned, the supervised learning during the Q function update fails to control the
bias and variance. We need careful sampling techniques during the sampling from the replay buffer
to mitigate the policy evaluation error. In particular, Wang & Ross (2019) proposes to emphasize
recent experience (ERE) because the recent policies are closer to the latest policy. We show below
that the ERE strategy is a refinement of 1/age weighting and that both methods help balance the
expected selection number of each training transition (s, a). Balanced selection numbers reduce
both the policy mismatch bias and sampling variance. Hence, this suggests the potential usefulness
of ERE and 1/age, which we verify through experiments in the last subsection.
5.1	Emphasizing recent experience
In Wang & Ross (2019), the authors use a length-K trajectory (K may differ across episodes) and
perform K updates. In the k-th (1 ≤ k ≤ K) update, a mini-batch is sampled uniformly from the
most recent Ck = max(N0ηk~κ0, Cmin) samples, where No is the current size of the replay buffer,
L0 is the maximum horizon of the environment, η is the decay parameter, and cmin is the minimum
coverage of the sampling. For MuJoCo (Todorov et al., 2012) environments, the paper suggests the
values: (L0, η, Cmin) = (1000, 0.996, 5000). One can see that η = 1 does a uniform weighting
over the replay buffer, and the emphasis on the recent data becomes larger as η becomes smaller. To
see how the ERE strategy affects the mini-batch sampling, we prove the following result.
Proposition 1. ERE is approximately equivalent (Taylor Approx.) to the non-uniform weighting:
Wt (X --_1 二 C	- ɪ+ 1(t	≤ Cmin)	max (ln (m+,	0),
max(t, Cmin, NonL0) No	Cmin	N°ηL0
(8)
where t is the age of a data point relative to the newest time step; i.e., w1 is the newest sample.
Note that Prop. 1 holds for η 6= 1 because it is derived from the geometric series formula: (1 -
nn)∕(1 一 n), which is valid when n = 1. Despite this discontinuity, We may still claim that the
ERE strategy performs a uniform weighting when η is close to 1. This is because when η ≈ 1,
Eq. (8) suggests wt is proportional to 1∕(NoηL0 ) - 1∕No for all 1 ≤ t ≤ No, which is a uniform
weighting. The emphasis on the recent experience (indicated by l(t ≤ Cmin)) is also evident from
Eq. (8). Precisely, the second term increases logarithmically ln 1 when n becomes smaller, so the
smaller η indeed gives more weight on the recent experience.
Time step
Discussion. A key distinction between the orig-
inal ERE and Prop. 1 is that the original ERE
considers the trajectory length K while Prop. 1
doesn’t. Intuitively, the disappearance of K’s de-
pendency results from the aggregation of all ef-
fects in 1 ≤ k ≤ K updates. We verify that
Eq. (8) tracks the original ERE well in the next
subsection.
Another feature of Prop. 1 is an implicit 1/age
weighting in Eq. (8). Although the original ERE
samples uniformly from the recent Ck points, the
ie6 aggregate effect ofall 1 ≤ k ≤ K updates appears
to be well approximated by a 1/age weighting.
Figure 1: Expected selection numbers (aggregate
weights) over a million time steps.
To understand the effect of 1/age, recall from section 2.2 that in practice, Q is updated using mini-
batch samples. Define a point (Se, 0e)'s time step as i + 1 + L ∙ (e 一 1). Then the expected number
of times in all batch samples that a point at a certain time step is selected (the expected selection
number) gives the aggregate weight over the time steps. As shown in Figure. 1, 1/age weighting
and ERE_apx, Eq. (8), give almost uniform expected selection numbers across time steps while
uniform weighting is significantly biased toward the old samples. Therefore, 1/age weighting and
its variants help balance the expected selection number.
7
Under review as a conference paper at ICLR 2022
5.2	Policy evaluation error under non-uniform weights
Recall the expected selection numbers are the aggregate weights over time. To understand the
merit of balanced selection numbers, we develop an error analysis under non-uniform aggregate
weights in the Appendix A.5. In particular, Corollary 2 generalizes Corollary 1 to non-uniform
weights {wi > 0} such that the policy evaluation error is bounded by three terms: Hoeffiding error
i	i . i	iL	ITTi.L
O( w2/ w ) (weighted variance), weighted bias W1i,,Lw , and normalized weighted bellman
error ξi,L. Since the weighted bellman error is well-controlled by supervised learning, in the follow-
ing, we discuss the weighted biases and variances in ERE, 1/age weighting, and uniform sampling.
We conclude that the ERE and 1/age weighting are better because both of their biases and variances
are smaller than that of uniform sampling.
For the bias from policy mismatch, Figure 1 shows the uniform sampling (for each sampling from
the replay buffer) makes the aggregate weights (expected selection number) bias toward old samples.
Because the old policies tend to be distant from the current policy, the uniform sampling induces a
larger weighted bias W1i,,wL than ERE and 1/age do.
As for the variance from sampling, we generalize Corollary 1's uniform case: O(1∕√N) to Corol-
lary 2's weighted case: O( VzPi Wi/ Pi wi). That is, the variance under non-uniform weight is
bounded by the Hoeffding error O (ʌ/pɪw2 / Pi Wi) and is reduced to O(1 / √N) when the weights
are equal. Furthermore, one can prove that the Hoeffding error is minimized under uniform weights:
Proposition 2. Let {wt > 0}tN=1 be the weights of the data indexed by t. Then the Hoeffding error
PtN=1 wt2/PtN=1 wt is minimized when the weights are equal: wt = c > 0, ∀ t.
Therefore, the variance from sampling is large under non-uniform aggregate weights and is mini-
mized by uniform aggregate weights. Since the uniform sampling leads to more non-uniform aggre-
gate weights than ERE and 1/age weighting, its variance is also larger.
Because the ERE and 1/age weighting have balanced selection numbers (i.e., balanced aggregate
weights), their biases and variance are smaller. They should perform better than uniform weighting
for off-policy actor-critic RL. We will verify this in the next subsection.
5.3	Experimental verification
Since we,ve established a theoretical explanation for 1/age-based samplings (ERE, ERE_apx, 1/age),
we will explore two main propositions from the preceding subsections: (1) Are 1/age-based sam-
plings better than uniform weighting? (2) Does the approximated ERE proposed in Eq. (8) track the
original ERE well? In addition, since prioritized experience replay (Schaul et al., 2016) (PER) is a
popular sampling method, a natural question is (3) Do 1/age-based samplings outperform PER?
We evaluate five sampling methods (ERE, ERE_apx, 1/age weighting, uniform, PER) on five
MuJoCo continuous-control environments (Todorov et al., 2012): Humanoid, Ant, Walker2d,
HalfCheetah, and Hopper. All tasks have a maximum horizon of 1000 and are trained using a Py-
torch Soft-Actor-Critic (Haarnoja et al., 2018) implementation on Github (Tandon, 2018). Because
the standard SAC implementation uses the uniform sampling, by comparing uniform sampling with
the other four methods, we can deduce ways to boost SAC from a sampling’s perspective.
Most hyper-parameters of the SAC algorithm are the same as that in Tandon (2018) except for the
batch size, where we find a batch size of 512 tends to give more stable results. Our code is available
at https://github.com/sunfex/weighted-sac. The SAC implementation and the Mu-
JoCo environment are licensed under the MIT license and the personal student license, respectively.
The experiment is run on a server with an Intel i7-6850K CPU and Nvidia GTX 1080 Ti GPUs.
In Figure 2, 1/age-based samplings (ERE, ERE_apx, 1/age) perform better than the uniform weight-
ing does in all tasks. This verifies our preceding assertion that 1/age-based samplings are superior
because their biases and variances of the estimated Q function are smaller. Moreover, ERE and
ERE_apx mostly coincide with each other, so Eq. (8) is indeed a good approximation of the ERE
strategy. This also explains the implicit connection between ERE and 1/age weighting strategies:
ERE is almost equivalent to ERE_apx and 1/age is the main factor in ERE_apx, so ERE and 1/age
8
Under review as a conference paper at ICLR 2022
le3
Humano∣d-v2
8 6 4 2 0
p∙IBM9J">Q-nuJnu
0.0	0.2	0.4	0.6
time step
le4
1.50
P∙∣bm9j"~--nuJnu
le3
Ant-v2
0.4	0.6
time step
ie3
P∙∣bm9j"~--nuJnu
le3
Walker2d-v2
76543210
0.4	0.6	0.8	1.0
time step	le6
1.0	0.0
ie6
HalfCheetah-v2
5 0 5 0 5 0
2 0 7 5 2 0
LL
>-一 nujnu
0.4	0.6
time step
1.0	0.0
le6
Hopper-v2
3 2 10
">-"一 nujnu
1.0
le6
0.4	0.6
time step
1.0	0.0
le6
Figure 2: ERE, ERE_apx, 1/age weighting, PER, and uniform weighting on MuJoCo environments
in a million steps. The solid lines and shaded areas are the means and one standard deviations.
weighting should produce similar results. Finally, ERE and 1/age generally outperform PER, so the
1/age-based samplings that we study achieve nontrivial performance improvements.
Finally, we provide performance comparison with the prior work Fujimoto et al. (2020); Kozuno
et al. (202i) in Table 1). Table 1 shows the SAC with 1/age-based samplings (ERE, ERE_apx,
1/age) outperform the prior work. This suggests 1/age-based samplings not only possess simplicity
and theoretical explanations but also achieve better performances.
Methods	Hopper	HalfCheetah	Ant	Walker2D	Humanoid
SAC+ERE	"3402~~	13591	6516	5530	7147
SAC+ERE_apx	3477	13890	6757	5460	7088
SAC+1/age	3496	14328	5950	5774	6664
TD3+LAP (Fujimoto et al., 2020)	3364	10769	5593	4203	5445
TD3+PAL (Fujimoto et al., 2020)	3055	10584	4662	4458	5328
TD3+Peng (Kozuno et al., 2021)	X	X	4196	3675	X
Table 1: Performance comparison on MuJoCo environments at step 1 million. The numbers are
measured from Fujimoto et al. (2020)[Figure 1] and Kozuno et al. (2021)[Figure 4]. The X symbol
means the data are unavailable from the paper.
6	Conclusion
To understand off-policy actor-critic algorithms, we show the policy evaluation error on the expected
distribution of transitions decomposes into the Bellman error, the bias from policy mismatch, and
the variance from sampling. We use this to explain that a successful off-policy actor-critic algorithm
should have a careful sampling strategy that controls its bias and variance. Motivated by the empir-
ical success of Emphasizing Recent Experience (ERE), we prove that ERE is a variant of the 1/age
weighting. We then explain that 1/age-based samplings (e.g., ERE, ERE_apx, 1/age) have smaller
bias and variance and are preferable over uniform sampling. Our experiments verify that soft actor-
critic with 1/age-based samplings outperforms the prior work. We hence conclude that a simple but
careful design in the sampling of off-policy actor-critic RL can lead to better performances.
9
Under review as a conference paper at ICLR 2022
7	Ethics Statement
Our work explains off-policy actor-critic RL algorithms and designs simple but effective training
techniques. The results apply to real domains where past trajectories are abundant while new sam-
ples are costly, e.g., robotics, recommender systems, and power grid management. RL algorithms
may have positive economic effects by boosting efficiency and lowering risk. But inappropriate
use can have negative societal impacts. These include job loss due to automation, the ethical chal-
lenges of delegating important decisions to a machine, and data privacy issues (e.g., recommendation
agents). These implications apply to most control and RL studies and are not associated with any
specific work.
8	Reproducibility Statement
To ensure reproducibility of the experiments, we’ve provided hyper-parameters in Appendix A.1
and the source code at an anonymous Github page https://github.com/sunfex/
weighted-sac. Discussions on the assumptions are in section 4.1 and justifications are in Ap-
pendix A.2. A complete proof of the theorems and lemmas can be found in Appendix.
References
Alekh Agarwal, Nan Jiang, Sham M Kakade, and Wen Sun. Reinforcement learning: Theory and
algorithms. CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep, 2019.
Yaqi Duan, Zeyu Jia, and Mengdi Wang. Minimax-optimal off-policy evaluation with linear func-
tion approximation. In Proceedings of the 37th International Conference on Machine Learning,
volume 119 of Proceedings of Machine Learning Research, pp. 2701-2709, Virtual, 13-18 JUl
2020. PMLR.
Miroslav Dudik, John Langford, and Lihong Li. Doubly robust policy evaluation and learning.
In Proceedings of the 28th International Conference on International Conference on Machine
Learning, ICML’11, pp. 1097-1104, Madison, WI, USA, 2011. Omnipress.
Yihao Feng, Tongzheng Ren, Ziyang Tang, and Qiang Liu. Accountable off-policy evaluation with
kernel Bellman statistics. In Hal Daume In and Aarti Singh (eds.), Proceedings of the 37th
International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning
Research, pp. 3102-3111. PMLR, 13-18 Jul 2020.
Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th Inter-
national Conference on Machine Learning, ICML2018, Stockholmsmassan, Stockholm, Sweden,
July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 1582-1591.
PMLR, 2018.
Scott Fujimoto, David Meger, and Doina Precup. An equivalence between loss functions and non-
uniform sampling in experience replay. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell,
Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual, 2020.
Yasuhiro Fujita and Shin-ichi Maeda. Clipped action policy gradient. In Jennifer Dy and Andreas
Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80
of Proceedings of Machine Learning Research, pp. 1597-1606. PMLR, 10-15 Jul 2018.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of wasserstein gans. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fer-
gus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 30. Curran Associates, Inc., 2017.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Jennifer G. Dy and
10
Under review as a conference paper at ICLR 2022
Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning,
ICML2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings
ofMachine Learning Research, pp.1856-1865. PMLR, 2018.
Anna Harutyunyan, Marc G. Bellemare, Tom StePleton, and Remi Munos. Q(λ) with off-policy
corrections. In ALT, pp. 305-320, 2016.
Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan
Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in
deep reinforcement learning. Proceedings of the AAAI Conference on Artificial Intelligence, 32
(1), Apr. 2018.
Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado van Hasselt,
and David Silver. Distributed prioritized experience replay. In International Conference on Learn-
ing Representations, 2018.
Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning.
In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd International
Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp.
652-661, New York, New York, USA, 20-22 Jun 2016. PMLR.
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably effi-
cient? In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc.,
2018.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement
learning with linear function approximation. In Conference on Learning Theory, pp. 2137-2143.
PMLR, 2020.
Nathan Kallus and Masatoshi Uehara. Doubly robust off-policy value and gradient estimation for
deterministic policies. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.),
Advances in Neural Information Processing Systems, volume 33, pp. 10420-10430. Curran As-
sociates, Inc., 2020.
Tadashi Kozuno, Yunhao Tang, Mark Rowland, Remi Munos, Steven Kapturowski, Will Dabney,
Michal Valko, and David Abel. Revisiting peng’s q(λ) for modern reinforcement learning. In Ma-
rina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine
Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine
Learning Research, pp. 5794-5804. PMLR, 2021.
Nevena Lazic, Dong Yin, Mehrdad Farajtabar, Nir Levine, Dilan Gorur, Chris Harris, and Dale
Schuurmans. A maximum-entropy approach to off-policy evaluation in average-reward mdps.
Advances in Neural Information Processing Systems, 33, 2020.
Hoang Le, Cameron Voloshin, and Yisong Yue. Batch policy learning under constraints. In Inter-
national Conference on Machine Learning, pp. 3703-3712. PMLR, 2019.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. Sample complexity of asynchronous
q-learning: Sharper analysis and variance reduction. In H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33,
pp. 7031-7043. Curran Associates, Inc., 2020.
Long Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching.
Machine Learning, 8:293-321, 1992.
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Infinite-
horizon off-policy estimation. In Advances in Neural Information Processing Systems, volume 31,
pp. 5356-5366. Curran Associates, Inc., 2018.
11
Under review as a conference paper at ICLR 2022
Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorithmic
framework for model-based deep reinforcement learning with theoretical guarantees. In Interna-
tional Conference on Learning Representations, 2019. URL https://openreview.net/
forum?id=BJe1E2R5KX.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. In International Conference on Learning Representations,
2018.
Remi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare. Safe and efficient off-policy
reinforcement learning. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (eds.),
Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016.
Chengzhuo Ni, Lin F. Yang, and Mengdi Wang. Learning to control in metric space with optimal
regret. In 2019 57th Annual Allerton Conference on Communication, Control, and Computing
(Allerton),pp. 726-733, 2019.
Guido Novati and Petros Koumoutsakos. Remember and forget for experience replay. In Kamalika
Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on
Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 4851T860,
Long Beach, California, USA, 09-15 JUn 2019. PMLR.
Doina Precup, Richard S. Sutton, and Satinder P. Singh. Eligibility traces for off-policy policy
evaluation. In Proceedings of the Seventeenth International Conference on Machine Learning,
ICML ,00, pp. 759-766, San Francisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc.
ISBN 1558607072.
Mark Rowland, Will Dabney, and Remi Munos. Adaptive trade-offs in off-policy learning. In Silvia
Chiappa and Roberto Calandra (eds.), The 23rd International Conference on Artificial Intelligence
and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy], volume 108 of
Proceedings ofMachine Learning Research, pp. 34U4. PMLR, 2020.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In
ICLR, 2016.
Simon Schmitt, Matteo Hessel, and Karen Simonyan. Off-policy actor-critic with shared experience
replay. In Hal Daume In and Aarti Singh (eds.), Proceedings Ofthe 37th International Conference
on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 8545-
8554, Virtual, 13-18 Jul 2020. PMLR.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International
Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp.
1889-1897, Lille, France, 07-09 Jul 2015. PMLR.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms, 2017. arXiv:1707.06347.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In Eric P. Xing and Tony Jebara (eds.), Proceedings of
the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine
Learning Research,pp. 387-395, Bejing, China, 22-24 Jun 2014. PMLR.
Samarth Sinha, Jiaming Song, Animesh Garg, and Stefano Ermon. Experience replay with
likelihood-free importance weights, 2020. arXiv:2006.13169.
Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods
for reinforcement learning with function approximation. In Proceedings of the 12th International
Conference on Neural Information Processing Systems, NIPS'99, pp. 1057-1063, Cambridge,
MA, USA, 1999. MIT Press.
Pranjal Tandon.	Pytorch soft actor critic, 2018.	Github Reposi-
tory, https://github.com/pranz24/pytorch-soft-actor-critic.
12
Under review as a conference paper at ICLR 2022
Yunhao Tang, Michal Valko, and Remi Munos. Taylor expansion policy optimization. In Hal Daume
III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learn-
ing, volume 119 of Proceedings of Machine Learning Research, pp. 9397-9406. PMLR, 13-18
Jul 2020.
Philip Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement
learning. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd
International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning
Research, pp. 2139-2148, New York, New York, USA, 20-22 Jun 2016. PMLR.
E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In 2012
IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033, Oct 2012.
Surya T Tokdar and Robert E Kass. Importance sampling: a review. Wiley Interdisciplinary Reviews:
Computational Statistics, 2(1):54-60, 2010.
C Villani. Optimal transport - Old and new, volume 338, pp. xxii+973. 01 2008.
Che Wang and Keith Ross. Boosting soft actor-critic: Emphasizing recent experience without for-
getting the past, 2019. arXiv:1906.04009.
Chenjun Xiao, Yifan Wu, Chen Ma, Dale Schuurmans, and Martin Muller. Learning to combat
compounding-error in model-based reinforcement learning, 2019. arXiv:1912.11206.
Tengyang Xie, Yifei Ma, and Yu-Xiang Wang. Towards optimal off-policy evaluation for rein-
forcement learning with marginalized importance sampling. In Advances in Neural Information
Processing Systems, volume 32, pp. 9668-9678. Curran Associates, Inc., 2019.
Mengjiao Yang, Ofir Nachum, Bo Dai, Lihong Li, and Dale Schuurmans. Off-policy evaluation via
the regularized lagrangian. Advances in Neural Information Processing Systems, 33, 2020.
Ming Yin and Yu-Xiang Wang. Asymptotically efficient off-policy evaluation for tabular reinforce-
ment learning. In Silvia Chiappa and Roberto Calandra (eds.), Proceedings of the Twenty Third
International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of
Machine Learning Research, pp. 3948-3958, Online, 26-28 Aug 2020. PMLR.
Ming Yin, Yu Bai, and Yu-Xiang Wang. Near-optimal provable uniform convergence in offline
policy evaluation for reinforcement learning. In Arindam Banerjee and Kenji Fukumizu (eds.),
Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume
130 of Proceedings of Machine Learning Research, pp. 1567-1575. PMLR, 13-15 Apr 2021.
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn,
and Tengyu Ma. Mopo: Model-based offline policy optimization. In H. Larochelle, M. Ranzato,
R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems,
volume 33, pp. 14129-14142. Curran Associates, Inc., 2020.
A Appendix
A. 1 Hyper-parameters
To train the SAC agents, we use deep neural networks to parameterize the policy and Q functions.
Both networks consist of dense layers with the same widths. Table 2 presents the suggested hyper-
parameters. As mentioned in the experiment section, the hyper-parameters are similar as the imple-
mentation in Tandon (2018).
A.2 Justification of assumptions
In section 4.1, we introduce three main assumptions in this work. Below is a validation for each.
13
Under review as a conference paper at ICLR 2022
Variable	Value
Optimizer	Adam
Learning rate	3E-4
Discount factor	0.99
Batch size	512
Model width	256
Model depth	3
Table 2: Suggested hyper-parameters for SAC.
Environment	Hopper	HalfCheetah	Walker2D	Ant	Humanoid
Temperature	0.2	0.2	―	0.2	0.2	0.05
Table 3: Temperature parameters for SAC in MuJoCo environments.
N
1.	For each s ∈ S, Q(s, a) and Qπ (s, a) are LA-Lipschitz in a. As mentioned in the
paragraph ”Normalization” of section 4.2, the Lipschitzness of QπN is sublinear or linear in the
N
horizon, which quantifies the magnitude of Qπ ’s Lipschitz constant. Since Q approximates
N
Qπ , Q should have a similar property as long as the training error is well controlled. The
practitioner can also enforce the Lipschitzness of Q using gradient penalty (Gulrajani et al.,
2017) or spectral normalization (Miyato et al., 2018).
N
2.	Q(s, a) and Q (s, a) are bounded and take values in [0, rmax∕(1 — γ)]. This is a
standard assumption in the RL literature. If the bound is violated, one can either clip, translate,
or rescale to obtain new Q functions that satisfy the constraint. Note a bounded reward r(s, a) ∈
[0, rmax] has implied QπN ∈ [0, rmax/(1 -γ)].
3.	The action space is bounded with diameter diamA < ∞. This is a standard assumption in
continuous-control environments and is usually satisfied in practice. It is also common to use
clipping to ensure the bounds of the actions generated by the policy (Fujita & Maeda, 2018).
A.3 The construction of our behavior policy
We first discuss some important relations between state occupancy measures and Bellman flow op-
erator. Similar results about Fact 1 and 2 be found in Liu et al. (2018)[Lemma 3].
Definition 1 (Bellman flow operator). The Bellmanflow operator Bρ0,∏,τ(∙) generated by (ρo, π, T)
with discount factor γ is defined as
Bρo,∏,τ(ρ)(s) , (1 - Y)ρo(s) + Y /T(s∣s0,a0)∏(a0∣s0)ρ(s0)ds0da0.
Fact 1. Bρ0,π,T is a Y-contraction w.r.t. total variational distance.
Proof. Let p1(s), p2(s) be the density functions of some state distributions.
DTV (BP0,∏,T(PI)llBP0,∏,T(Pz)) = 2 / IBPo ,∏,T(PI(S))- Bρo,π,T(P2(S))Ids
=ɪ J γ∣ J T(s∣s0,a0)π(a0∣s0) (pι(s0) — p2(s0))ds0da0∣ds
≤ Y/T (s∣s0, a0)π(a0∣s0) ∣p1(s0) — p2(s0)∣ds0da0ds
=2 / ∣P1(SO)- P2(s0)∣ds0 = Ydtv (piM).
□
Fact 2. The normalized state occupancy measure ρPπ0 generated by (ρ0, π, T) with discount factor
Y is a fixed point of BP0,π,T; i.e., BP0,π,T (ρPπ0)(S) = ρPπ0(S).
14
Under review as a conference paper at ICLR 2022
Proof.
∞
pPo (S)=(I-Y) X Y,fi(s∖P0,π,T)
i=0
∞
= (I-Y)f0(SlP0,π,T) + Y(I- Y) X Yifi+1(SlP0,π,τ)
i=0
=(1 - Y)ρ0(S) + Y(1 - Y)
∞
XYi
i=0
T (s∣s0,a0)π(a0∣s0)fi(s0∣po,π,T )ds0da0
=(1 - Y)ρ0(S) + Y
/ T(s∣s0, a0)π(a0∣s0)(1
∞
-Y) X Yifi(S0∣Po,∏,T)ds0da0
i=0
= (I-Y)PO(S)+ Y / T(sls,a)π(ais)pPo (S0)ds0da0 = BP0,P,T(Pn )(S).
Thus, the Bellman flow operator is useful to analyze the state occupancy measures, and we have the
following lemma to construct the behavior policy πiD .
e
Lemma 1. Let Pie(S) the state distribution at trajectory step i in episode e. Let Pπe (S) be the
Pi
normalized occupancy measure starting at trajectory step i in episode e. Then
PD	PD
PPi (s) a.e., where PPi is the normalized state occupancy measure
Moreover, we have N PN=I PPe(S)∏e(a∣S) = PPi(S)πD(a∣S) a.e.
NN PL PPe (S)
is generated by (Pi, πD, T).
Proof. Precisely, Pie(S) = Pi(S|P0, πe, T) is the state distribution at trajectory step i following the
laws of (P0, πe, T). Since PPPee is the normalized occupancy measure generated by (Pie, πe, T), each
e
PPPe is the fixed-point of the Bellman flow equation:
PPe(S) = (I-Y)Pe(S) + Y /T(S|S0,a0)ne(a0|S0)PPe(s')ds'da0, ∀ e ∈ [1,…,n].
This implies the average normalized occupancy measure is the fixed point of the Bellman flow
equation characterized by (Pi, ∏iD ,T):
1N	1N
N X PPe (S)=N X
(I-Y)Pe(S)+ y∕t (S1s0，a0)ne(a01s0)PPe(S0)ds0da
(1 - Y)Pi(S) + Y/
1N
T(S∣S0,a0)NN X [∏e(ao∣S0)PPe3)] dS0da0
(I- Y)pi(s) + Y / T(S|S0,a0)
e=1
PN=1 ∏e(a0∣S0)PPe(s0) PN=ι PPe(S0)
pN=ι PPe (S0)
dS0da0
N
□
1N e
(I-Y)Pi(s) + γ I T(SIS ,a )πD(a |s)NEPPe (s )ds da,
e=1
,	D I、△ PN=I PeSIs)Pne(S) ∙ ʌ	一 ∙ ι∙ …… ，iʌ n∣ι~ a
where ∏D(a∣S)，——PN——∏e(-i——is the average behavior policy at step i. Sincethe BenmanfloW
e= ρi
operator is a Y-contraction in TV distance and hence has a unique fixed point in TV distance, denoted
as PPi (s), we arrive at NN PN=I PPe (s) = PPi (s) almost everywhere. Also, by construction, we
have
1N
N XPPie(s)πe(a|s) = PPi (S)nD(a|s) a.e.
□
15
Under review as a conference paper at ICLR 2022
A.4 Policy Evaluation Error
Lemma 2. If Q(s, a) is LA-Lipschitz in a for any s, then, for any state distribution ρ,
Es 〜PW 〜∏ι(∙∣S)Q(S,a) - Ea 〜∏2(∙∣s)QGa)I ≤ LA Es 〜ρW√π1(IS)||n2 (IS)).
Proof. For any fixed S, we have
Ea〜∏ι(∙∣s)Q(S, a) - Ea〜∏2(∙∣s)Q(S, a) = LA
Q(S,a)
LA
- Ea〜∏2(∙∣s) ¾a2i
≤LA SUp	Ea〜∏ι(∙∣s)f(a) - Ea〜∏2(∙∣s)
kfkLip≤1
LAW1(n1(・|S)|忻2(・[S)),
where the second line follows from Kantorovich-Rubinstein duality (Villani, 2008). Since the 1-
Wasserstein distance W1 is symmetric, we can interchange the roles of π1 and π2, yielding
Fa 〜∏ι(∙∣s)Q(s, a) - Ea 〜n2(.|s)Q(S,a)| ≤ LA W1 (π1 (1 s) ||n2 (1 s)) .
Taking the expectation Es〜P on both sides completes the proof.	□
Lemma 3. If Qπ (S, a) is LA-Lipschitz in a for any S, then
E(SO ,ao)〜po(s)πD (a∣s)IQπ0 (S0,aθ) - Qn (S0, a0)| ≤ ] A E	∏D W1(πD (IS)IIn(I s)).
1	1	1 - Y S 〜ρP0
Proof. For any (S, a), we have
IQπ0D (S, a) - Qπ (S, a)I
=γ∣Es0〜T(∙∣s,a)E∏DQnD (s0,∏d(s0)) - E∏Qn(s0,π(s0))∣
≤γEs，〜T(∙∣s,a)∣E∏D,∏QnD(S0,∏D(S0)) - Qπ(s0,∏D(s0)) + Qπ(s0,∏D(s0)) - Qπ(s0,∏(s0))∣
≤γEs0 〜T (∙∣s,a)(∣E∏D QnD (S0,∏D (S0))- Qπ (S0,∏D (s0 )) + ∣E∏d ,∏ Qπ (s0,∏D (s0)) - Qπ (s0,∏(s0))∣)
≤γEs，〜T (∙∣s,a) (EnDIQnD (s0, nD (SO))- Qn (s0, nD (SO)) + LAW1(πD (ISO) ||n(・]SO))),
where the last line follows from Lemma 2. Let ρiπ0 be the state distribution at step i following the
laws of (po, nD ,T). Take expectation over po and expand the recursive relation. We arrive at
∞
E(s0 ,ao)〜po(s)nD (a∣s)∣QnO (S0,aθ)- Qn (s0,aθ)∣ ≤ LAX YiEs .〜PnD Wι(nD (1Si)IIn(1Si))
=aE	∏D Wi(nD(∙∣S)∣∣∏(∙∣S)) - LAEs〜ρWι(nD(∙∣s)∣∣n(∙∣s))
1	- Y s〜P八0
/	ρ0Q
≤4E	∏D Wi(nD(∙∣s)∣∣n(∙∣s)),
1	- Y s 〜P^0
where the second line follows from ρ∏0 = (1 一 Y) P∞=0 Yip：0 .	□
Lemma 4. If Q(S, a) is LA-Lipschitz in a for any S, then
E(so,ao)〜po(s)∏D(a|s) Q(S0,aθ) - Q 0 (s0,aθ)∣
E , ∏Dc D一、Q(S,a) -BnQ(S,a)∣ + LAWi(n(・[s)||nD“S))
(s,a)〜ρ^0 (s)nD(als)	I
≤	1-^	.
16
Under review as a conference paper at ICLR 2022
Proof. For any (s,α) ∈ S ×A, we have a recursive relation:
∣Q(s,α) - QH (s,α) I = I Q(s,α) -BπD QπD (ST
≤ ∣ Q(s,a) — Bπ° Q(s, a) ∣ + ∣ Bπ° Q(s,α) — Bπ° Qπ° (s,a) ∣
=Q(s, a) - Bπ° Q(S,a) + y| Es/〜T(∙∣s,a)W〜πD(∙∣s0)Q(s0,a0) - QnD (s0,a0)
≤ Q(S, a) - Bπ° Q(S, a) + YEs0〜T(∙∣s,a),a0〜∏D(∙∣s0) 1 Q(Sl aZ)- Q"° (Sl a0)
Expand the recursive relation. We have
E(s°,a°)〜p°(s)∏D (a∣s) | Q(s0, a0) - Q ° (s0, a0) 1
≤Es°〜^°Eɑ°~∏d(∙∣s°) ∣ Q(S0,a0) -BnDQ(S0,a0) ∣
∞
+ ES°~Q° X YiEa°,sι ,aɪ ,...,Si-1,ai-1 Esa~T(TSi-ι ,ai-1 ),aa~∏DG∣Si)∣Q(Si,ai) -BπDQ(Si,ai)
i=1
∞
=ES° 〜^° X YiEaC1,sι,aι,...,Si,ai〜T,πD 1 Q(Si ,ai) - BH Q(Si,ai)]
i=0
1 — Y (s,a)~ρπ: (s)πD(a∣s)
∣Q(S,a) -BnDQ(S,a) ∣ .
(9)
The last line uses a fact for the normalized occupancy measure :
E πD
∞
(1 - Y ) [Es°~ρ° Ea °~π D(Ts°) + X I Y Es°~ρ° Ea °~π D(Ts° )∙∙∙Esi~T (∙∣ s-ɪ ,a-ɪ )g~πD (∙∣s∕ .
i=1
We are almost done once the BπD in Eq. (9) is replaced by Bπ. Note that
E 一	∏DCD(J QGa)-BnD Q(S,a) ∣
(s,a)〜嗫 : (s)π D(a∣s)l	I
≤E, 、∏D∕∖D∕J Q(S,a) -Bπ Q(S,a)∣ + ∣ Bπ Q(S,a)-BnD Q(S,a) ∣
(s,a)〜嗫 : (s)π D(a∣s)∣	I I	I
=E πD d	I Q(S,a) -Bn Q(S,a)+ Y I ES0 〜T (∙∣s,a)Eπ,πD Q(SZ,π(SZ))- Q(SZ,πD (SZ)) |
(s,a)〜嗫 ° (s)nD(a∣s)∣	I I	°	I
Lem. 2	i	i
≤ E.	.	*、Dl, l、1Q(S,a) - Bn Q(S,a)∣+ YLAES0 〜T (∙∣s,a)W1(π(∙∣SZ)IInD ∙∣SZ)
(s,a)〜。^° (s)nD(a∣s) I	I
U	l—	-,
=E	)∏DCD, J Q(S,a)-BnQ(S,a) ∣
(s,a)〜嗫 ° (s)nD(a∣s)l	I
+ LAE	∏D W1(π(∙lS)IInD3S))-(I- Y)LAES〜ρ°W1(π(∙lS)IInD(1S))
S〜嚎0
≤E	、*、D，J Q(S,a) -Bπ Q(S,a)∣+ LA W1(n(∙IS)II∏D (忖).
(S,a)〜嗫 : (s)π D(a∣s)∣	I
(10)
The second-last line follows from that the distribution of sz is ρ∏0 I(SZ) = JSa T(szIs, a)ρ∏0 (s, a),
which satisfies the identity
ρP^° (SZ) = (I-Y)ρ0(SZ) + Y /T(SZIs, a)ρP^° (s, a)dsda = (1 - Y)p0(SZ) + 7啜I(SZ).
Combining Eq. (9) and Eq. (10), the result follows.	□
Lemma 5. Let 0 ≤ f (s, a) ≤ ∆ be a bounded function for (s, a) ∈ S × A. Let E∞ be
the averaging-discounted operator with infinite-length trajectories in N episodes; i.e., E∞f =
17
Under review as a conference paper at ICLR 2022
N PL P∞=0 (i -γ)γif(sie, aie). Then, with probability greather than 1 - δ,
(e∞o - E ∏D	) f ≤ ∆A∕-2log1.
V 0	Pp0 (s)∏D(a|s)厂—VN δ
Proof. Let δse (s) be the delta measure at the initial state in episode e. Because the empir-
ical distribution ρo is composed of the realization of the trajeCtories's initial states, We know
Po = Nn Pe=1 δs0 . Then, Lemma 1 implies ρ∏0 is an average of the normalized occupancy mea-
sures in N episodes.
1N e
(S)πD(a|S) a= NEPnse(S)πe(alS)
e=1
def. of ρδπee 1 N ∞
=so N7∑∑(i -Y)YiPe(Slδs0,πe, T)ne(a|s),
(11)
where Pe is the state density at trajectory step i in episode e.	Since (See, ai)	〜
Pie(S∣δso,∏e, T)∏e(a∣S), we have
1	- γ)γif(Se,ae) - EPnee (s)∏e(a∣s)f
N∞
(=) N X X(i - γ)γi f(Se, ae) - E[f(Se,。£)葡]
e=1 i=0
1N
—X Me.
N乙
e=1
(12)
We claim that {Me}eN=1defined as Me = Pi∞=0(1 - Y)Yi f(Sie, aie) - E[f(Sie, aie)|Se0] is a martin-
gale difference sequence. To see this, let Fe be the filtration of all randomness from episode 1 to e,
with F0 being a null set. Clearly, we have Me ∈ Fe, E[Me|Fe-1] = 0 and Me ∈ [-∆, ∆] since
f(S, a) ∈ [0, ∆] by assumption, which proves {Me}eN=1is a martingale difference sequence.
Finally, since Me is bounded in [-∆, ∆], by Azuma-Hoeffding inequality, we conclude that with
probability greater than 1 - δ,
Eq. (12) ≤ N TsPmR W = ∆J ^log ；.
2	δ Nδ
□
Theorem 1. Let N be the number of episodes. For f : S × A → R, define the operator E0∞ f ,
N PN=I P∞=0(1 — γ)γif(Se, aɛ). Denote the policy mismatch error and the Bellman error as
W0,∞ = E∞Wι(∏N ∣∣∏d )(∙) and eQ∞ = E∞∣Q(∙, ∙) 一 (BFN Q)(∙, ∙)∣ ,respectively. Assume
N
(1)	For each S ∈ S, Q(S, a) and Qπ (S, a) are LA-Lipschitz in a.
N
(2)	Q(S, a) and Qπ (S, a) are bounded and take values in [0, rmax/(1 - Y)].
(3)	The action space is bounded with diameter diamA < ∞.
Note Wι(πN ∣∣πd )(s) means Wι(πN (∙∣s)∣∣∏d (∙∣s)) , which is afunction of S. Then, with probability
greater than 1 - δ, we have
E(SO ,ao)〜Po(s)∏D (a|s) IQ(S0，aO) - Q	(s0, a0) |
≤ Imax JT^l +(二+% dam A)Jξ^l 十 ɪ 侵 ∞ 十 2Law°,∞).
1 — γ V 2N δ V (1 — γ)2	1 — Y)NN δ 1 — γ V Q	)
18
Under review as a conference paper at ICLR 2022
Proof. The proof basically combines Lemma 3, 4 and 5. To start with, the objective is decomposed
as:
E(so,ao)〜ρo(s)∏D(a|s) IQ(S0, aO) - Q	(s0, a0) |
=HI + E(so,ao)〜po(s)πD(a|sjQ(S0, aO)- Qn (S0, aO)I
≤H1 + E(s,a)〜po(s)πD(a∣s) JQ(S0, aO)- Qπ0 (S0, aO) 1 + JQ"0 (s0, aO)- Qn (s0, aO)
4≤3H1 + 1-Y E(s"D (s)∏D(a∣s) hQ(S，a) - (BnN Q)(S，a)| + 2LAW1 (nN (忖l|nD (IS))
=HI + 占 + 1-YE ]|Q(.，∙)-(BnNQ)(.，』+ 2LAWl(πNllπD)(∙)
ATpHi + -H^ + ɪ &Of + 2LaWO,∞ ).
1-	γ 1- γ Q
(13)
N	max	N	max
Because ∣Q(S,a)- Qn (S，a)| ∈ [0，r1-γ ], we know Ea 〜∏D (∙∣s)∣Q(s, a) - Qπ (S，a)| ∈ [0，r1-γ ],
too. Suppose Po have N independent samples. By Hoeffding's inequality, with probability ≥ 1 - δ,
N	rmax	1	1
HI = (EsO 〜ρo - EsO 〜po )Eao 〜∏D (∙∣so)1Q(SO，a0)- Q (S0，。。" ≤ ] - Y N ^N log $， (14)
Also, let f (s，a) = |Q(s，a)-(BnN Q)(s，a)|+2L/Wi(nN (∙∣s)∣∣πd (∙∣s)). Then f (s，a) is bounded
in [0，r1m-χ + 2LAdiam/]. Thereby, Lemma 5 implies that with probability greater than 1 - δ,
H2=(e	∏d	E»[|Q(s，a)-(BnN Q)(s，a)|+2LAWi(nN (∙∣s)∣∣∏D (∙∣S))
× (s,a)〜P^0 (s)nD(a|s)	× L1	1	_
r rmax	ʌ ∣~2	1
≤ ( ；----+ 2LAdiamA ) ∖ — log -
1-γ	N δ
(15)
Combining Eq. (13), (14) and (15), a union bound implies with probability greater than 1 - 2δ,
|	nN |
E(so,ao)~po(s)∏D(a|s) |Q(s0, a0)	Q	(s0, a0) |
≤	rɪ^θgɪ+ (二 + 2LAdiamA)+J b + 2LaWi0,∞)
1 - γ 2N δ (1 - γ)2	1 - γ N δ 1 - γ Q
Finally, rescaling δ to δ∕2 finishes the proof.	□
Corollary 1. Let EL be the operator defined as ELf = N PN=I PL-(1 - Y)Yj-if (se，ae)∙
Fix assumptions (1) (2) (3) of Theorem 1. Rewrite the policy mismatch error and the Bellman
error as W；,L = ELWι(πN ||nD )(∙) and EiL = EL|Q(・，∙) — (BKN Q)(.，.)∣ ,respectively, where
Q
Wι(πN ||nD )(s) = Wι(πN (∙∣s)∣∣∏D (∙∣s)) . Let Pi(S) be the average state density at trajectory Step
i over all N episodes. Then, with probability greater than 1 - δ, we have
E(si,ai)〜Pi(s)nD (a|s)|Q(si，ai ) - Qn (si，ai)| ≤
rmax ∏~λ^^2	r rmax	2La diam AW /ɪ^^2	l λ 1 iiL cλt iLi l∖
^√^log-ξ+ ∩V2 + -rA	ʌ/^logl+ Y +1*L + 2L∕Wi,L
1 - Y	2N	δ (1 - Y)2	1 - Y	N δ	1 - Y Q	1
Moreover, if i ≤ L — Iogl and the constants are normalized as LA = c/(1 — γ), GL = ξi,L∕(1 — γ),
then, with probability greater than 1 - δ, we have
E(si,ai)~Pi(s)∏D (a|s) | Q(Si，ai) - Qπ (Si，ai ) |
≤O((1-lY)2 ((rmax + C ∙ diama)(1∕√N + e) + ξQL + C ∙ Wi,L))
19
Under review as a conference paper at ICLR 2022
Proof. Notice that Theorem 1 is the situation when i = 0 and L = ∞. To push this to i ≥ 0
and L = ∞, recall that Lemma 1 defines the behavior policy πiD , the average state distribution
_	∏D
Pi and the state occupancy measure ρρ at step i. Also, since the trajectories in each episode are
initialized using the same initial state distribution, We have ρo = p0. Therefore, the objective of
N
Theorem 1 is actually E(s0,a0)〜ρ0(s)∏D(a∣s) ∣Q(so, ao) - Q	(so,ao)∣. This is generalized to i ≥ 0
using substitutions: (p0, πD, E∞) → (pi, π?, JE∞), yielding
E(si,ai)〜Pi(s)πD(a∣s)l Q(Si, ai) - Qn (Si ai)|
≤ Imax rɪʒθgɪ +(二+2LAdiamA)rɪiθgɪ+ɪ L+2L∕wi,∞)
1 — YV 2N	δ	V(1 — γ)2	1 — Y / V N δ 1 — γ∖ Q	)
Finally, observe that for any bounded f :
E∞f ≤ ELf + γL-ikfk∞
We arrive at
E
(si,ai
max
r
≤----
—1— Y
)〜Pi(s)πD (a|s) |Q(Si, ai) — Qn (Si, ai)|
/ 1 I 2 r rmax	2LAdi	2	2 L-'	1 ii,L^oτ Wi,L∖
V2Nlog δ + (L7 + F	Nlog δ + Y )+「«Q +2LAW1 J
As for the second conclusion, With the substitutions in the statement, We knoW YL-i ≤ . Hence,
E(si,ai)^Pi(s)∏D (a|s) |Q(Si, ai) — Q	(Si ai)
rmax / 1 1	2	( rmax	2c ∙ diam∕∖( / 2	2 ʌ 1	(洛L,,)	Wi,l∖
≤ 1-y V2Nlog δ +	(Ly + (1—γ)2	XvNlog δ+eJ	+	(1f	(ξQ	+2c ∙WI	)
=O((T-Y7 ((rmαx + C ∙ diamA)(1∕√N + e) + ξQL + C ∙ Wi,L))
□
A.5 Policy Evaluation Error under Non-uniform Weights
Lemma 6. Following from Lemma 1, let w = {we > 0}eN=1 be the (unnormalized) weights
e	πD
for the episodes. We have for the weighted case, P 1仅 Pe WePne (S) = Pρ.' (S) a.e., where
nD,/、	，	，，	e	；	i …
Pp： (s) is the normalized state occupancy measure is generated by (Pi,w ,∏DW, T). Also,
,w	e	πD
Pewe Pe WePne (S)ne(a|S)= Ppi,W(S)πDw (a|S)a.e.
Proof. The proof is basically a generalization of Lemma 1. Since Pρnee is the normalized occupancy
measure generated by (Pie, πe, T), We knoW each Pρnee is the fixed-point of the Bellman floW equation:
e
Pρnie (S)
(1 — Y)Pie(S) +Y
T(s∣s0, a0)πe(a0∣S0)P∏e (s0)dS0dα0,
∀e ∈ [1, ..., N].
20
Under review as a conference paper at ICLR 2022
Taking a weighted average, we get
1e
Pewe X MePPP (S)
Pewe XXWe
(I-Y )Pe(s) + γJ'T(SIS0"厅”0“(S0)ds0da
=(I - Y )ρi,w (S) + Y / T(Sls0,αO) P 1w X Wehne (α0ls0)P∏e (SO)i
dSOdaO
=(1
Y )ρi,w(S) + Y / T(S|S0,
aO)
Pe We∏e(a0∣S0)ρρP (so) Pe WePPne (SL 0〃 0
-Pe WePne (S0)	Pe We	S ”
—
=(1
-Y)ρi,w (s)+ Y T T(SIS0,α0)πDW (a0|S0) P 1,
e We
WePPπee (S0)dS0da0,
e
e
.	D/ I ∖ △ Ee Wpn (OIs)PPe(S) ∙ ,.	. L, TLlʌ .	1∙	. .	e∙ .. ŋ 11
where ∏D(a∣s) ，——P---------∏eri——is the weighted behavior policy at step i. Since the Bell-
e wePρπe (s)
man flow operator is a Y-contraction in TV distance and hence has a unique (up to difference
πD	e
in some measure zero set) fixed point, denoted as ρρi (s), We arrive at P 1仅 Pe WePne (s)=
πD	,w	e e e
Pρi,w (s) α.e. Finally, by definition of n£(α∣s), we conclude that P [ Pe WePne(s)πe(α∣s)=
nD,w	e e
Pρi,,w (S)nDw (a|s) a.e.	口
Theorem 2. Let W = {We > 0}N=I be the (unnormalized) weights for the episodes. Let E∞W
be the operator defined as E∞ωf = P 1	PN=I P∞=0(l — Y)yiWef (sɛ, aɛ). Fix assump-
,	e we
tions (1) (2) (3) of Theorem 1. Rewrite the policy mismatch error and the Bellman error as
w0,,∞ = EE∞WWι(nN||nDw)(∙) and eQ∞ = EE∞WlQ(∙, ∙) - (BnNQ)(∙, ∙)1，respectively, where
W1 (nN ∣∣∏Dw )(s) = W1 (nN (∙∣s)∣∣nDw (∙∣s)). With probability greater than 1 — δ, we have
N	rmax P	W2	2
’0,aO)~Ρ0(S)PDw(aIs)IQ(S0, a0) — Q	(S0, a0) I	≤ 1 — Y	X 2(P	We)2	log δ
(rmax	2LadiamA ∖ :
十 ((1-Y2 +	ι — Y)V
2	e We2
(	e We)2
log 2 + 占 KQ∞+2LA W0,,∞)
Proof. The proof basically follows from Theorem 1. Decompose the objective using Lemma 3, 4
as:
I	NI
E(s0,ao)~p0(s)nDw (a∣s) IQ(S0, a0) — Q	(s0, a0) ∣
=HI + E(s0,a0)~p0,w(s)nDw (a∣sjQ(S0,a0)- Qn(S0,/
≤H1 + E(s,a)~po,w(s)nDw(a|s)|Q(s0, a0)— QP0,w (s0, a0)| + |QP0,w(s0, a0) — Qn(S0, a0)|
4≤3Hi + ɪ E	πDw	D	[∣Q(s,a) — (BnN Q)(s,a)∣ +2L∕Wι(nN (∙∣s)∣∣nDw(∙∣s))
1	一 Y (s,a)~Pρ0,,W (s)∏Dw (a∣s) L1	1	,	.
=Hi + 1H2Y + 占E0∞w ]∣Q(∙, ∙) — (BnNQ)(∙, ∙)∣+ 2LaWi(nN∣∣∏Dw)(∙)
"PHI + ⅛ + 占(以 + 2LAWι0,,Wj,
(16)
where po,w(s) = Eewe^-%) is the weighted empirical initial state distribution. Because
N	max	N	max
IQ(S,a) — QF	(S,a)| ∈	[0,	1—γ],	we know Ea~nDw(.|s) |Q(S,	a)	—	QF	(S,a)|	∈	[0,	I-Y],	too.
21
Under review as a conference paper at ICLR 2022
By Hoeffding’s inequality, with probability greater than 1 - δ,
H1=(Es0~p0 - Es0~ρ0,w )Eao〜∏Dw(∙∣so) IQ(S0, aO)- Qn(S0, αO)I
≤ Pw t
∑r rmax、2	, ——---------
e (We I-YJ 1	1 rmax Pe w2	1	1
2 log δ = L X 2(Pe We)2 log δ .
(17)
Also, let f(s,a) = ∣Q(s,a) - (BnNQ)(s, a)1 + 2L∕Wι(∏N(∙∣s)∣∣∏D,w(∙∣s)). Then f(s,a) is
bounded in [0,1-X + ZL/diam/]. Using a weighted version of AzUma-Hoeffding in Lemma 5,
we have that with probability greater than 1 - δ,
H2 = E	πD
∖ (s,a)〜Pρ0,,W (S)nDw (Ws)
—
忸S, a) - (BnNQ)(s,a)∣ +2LaW1(πN(∙∣s)∣∣∏Dw(∙∣s))
rmax
≤( L + 2LAdi
2 Ee We log1
(Pe We)2 g δ
(18)
Combining Eq. (16), (17) and (18), a union bound implies with probability greater than 1 - 2δ,
i ʌ	N	i	rmax I ^p~w2	1
E(s0,ao)〜ρo(s)∏Dw(a|s) |Q(s0, a0) - Q	(s0, a0) 1 ≤ 1 - Y V 2(P We)2 log δ
(rmax	2L∕diam/ ∖ J
+ (Qf +	1- Y N
Finally, rescaling δ to δ∕2 finishes the proof.
2 Ee W2
(	eWe)2
log 1 + ± fe： + 2LAW0,,Wj
□
Corollary 2. Let ELw be the operator defined as ELw f = P ∖ Pe PjL-1(1 —
Y)Yj-iWef(Sje, aje). Fix assumptions (1) (2) (3) of Theorem 1. Rewrite the policy mismatch error and
the BelIman error asWi,w = ELwWι(πN∣∣πDW)(∙) and GLw = ELw∣q(∙, ∙) - (BnNq)(∙, ∙)∣, re-
SPectiVely, where Wι(πN ∣∣∏Dw )(s) = Wι(πN (∙∣s)∣∣∏DW (∙∣s)) .Let ρi,w(s) be the average weighted
state density at trajectory step i. Then, with probability greater than 1 - δ, we have
口	k/ 、	cπN∕	`1	rmax / Pe w2—^―2
E(Sig)〜Pi,w(S)∏Dw(a|S)IQ(Si,ai) - Q	(si,ai)∣ ≤ γ-γ V 2(P We)2 log δ
,r rmax	, 2L/diam/∖ ( / 2Pew2 ^^2	l-Λ	1	( i,L	以、
+ ((T-可 + 丁丁 N EeA log δ + Y ) +「«Q,w + 2LA %,w)
Moreover, if i ≤ L — IoggY and the constants are normalized as LA = c/(1 — γ), ^^Lw = ξi,L/(1 —
Y), then, with probability greater than 1 - δ, we have
E(si,aQ~ρi,w (S)KDw (a|s)
Q(Si, ai) - Qn	(Si, ai)
≤O ((1 -1 γ)2 ((rmax + C ∙ diamA) (、j(X w2)∕(X we)2 + e
+ ξQLw + C ∙ Wi,w))
Proof. Observe that for any bounded f :
E∞wf ≤ ELwf + γL-ikfk∞
Thus, We can start from Theorem 2 and prove with the same argument in Corollary 1.	□
22
Under review as a conference paper at ICLR 2022
A.6 Proof of Emphazing Recent Experience
Proposition 1. The ERE strategy in Wang & Ross (2019) is equivalent to a non-uniform sampling
with weight wt:
Wt (X	1/ K (	，1 -、- ɪ) + 1(t ≤ cmin)K max (1 - ln cmin/N0 , 0),
1 - nL0/K lmax(t,Cmin,NoηL0)	No)	Cmin	'	Lolnη	)
where t is the age of a data point relative to the newest time step; i.e., wo is the newest sample.
No is the size of the experience replay. Lo = 1000 is the maximum horizon of the environment.
K is the length of the recent trajectory. η ≈ 0.996 is the decay parameter. cmin ≈ 5000 is the
minimum coverage of the sampling. Moreover, the ERE strategy can be approximated (by Taylor
Approximation) as
Wt X -----乙~~1 入T LC、- 1Γ + 1(t ≤ Cmm) max (ln τcmin-, 0
max(t, cmin, NoηL0)	No	cmin	NoηL0
Proof. Recall that Wang & Ross (2019) assume a situation of doing K updates in each episode.
In the kth update, the data is sampled uniformly from the most recent Ck = max(NoηkK0, Cmin)
points.
To compute the aggregrated weight Wt over these K updates, observe that a data point of age t is in
the most recent Ck points if Ck ≥ t and that the weight in each uniform sample is 1/Ck. Therefore,
Wt should be proportional to
Wt x	X	1-.	(19)
Ck
k: 1≤k≤K,
ck ≥t
Because Ck is designed to be lower bounded by Cmin, we shall discuss Eq. (19) by cases.
(1) When t > Cmin, we know Ck > Cmin because Ck ≥ t is a constraint in the sum. This means
Ck = NOnkLOK and hence Ck ≥ t is equivalent to k ≤ ln N/ ln nL0/K. Eq. (19) becomes
Σ
k: 1≤k≤K,
ck ≥t
Ck
min(K, ln N / ln nLo/K)
X
k=1
1_ rΓLok∕K =)
No ∕
min(Κ, ln N / ln ξ)
X
k=1
Noξ-k
ξ-1 ι-ξ-logξ N
-No -1-ξ-1
ξ-1 1-ξ-κ _
No 1-ξ-1 -
1/t-l/No
1∕t-1∕No
——i-nL0/K
n-L0 /No-1/N0
1-nLo/K
1/(NOnL )-1/No
i-nLo/K
i-nLo/K-
ift > Noη
if t ≤ Noη
if K > ln N / ln n
if K ≤ ln N / ln n
L0/K
L0/K
L0
L0
1 — ηL0/K ∖max(t, NonL0)
—
(
1
1
1
where (*) is a substitution: ξ = nLo/K
(2)	When t ≤ Cmin, we know Ck ≥ t for all k because Ck ≥ Cmin by definition. Eq. (19)
becomes
Xck
k: 1≤k≤K,
ck ≥t
K1
XCk
min(K,
(*)
ln cm0n / ln ξ)
X
k=1
Noξ-k+max(K- ln CNn /ln ξ,0) Cmn
(H)	1	(	1
1 一 nLo/K ImaX(Cmin, NonL0)
+ max(K - K
ln Cmin/No
Lo ln n
θ)ɪ
Cmin
K (	」…、-ɪ) + 工 max (1-3n&o
1	- nLo/K max(Cmin, NonLo )	No
Cmin	Lo ln n
23
Under review as a conference paper at ICLR 2022
where (*) does a substitution: ξ = ηL0K and split the sum. (**) reuses the analysis in
case (1).
Combining cases (1) and (2), we arrive at the first conclusion. As for the approximation, since
η ≈ 0.996, let η = 1 - κ. We have
1 — nLo/K = 1 — (1 — K)L0/K ≈ 1 — 1 + κLo∕K = κɪ ≈
-KIn(I - K) = -Klnη
Thus the first term in the conclusion of Prop. 1 is proportional to K. The second term is also
proportional to K. Since wt is only made to be proportional to the RHS and both terms on the RHS
become proportional to K, we can remove K on the RHS:
1	(________1_____________L) + 1(t ≤ Cmin) max(1 - ln Cmin/N0, 0
-Lo ln η Vmax(t,Cmin,N0ηL0)	No	Cmin	〈 Lo ln η , ∙
Finally, because 0 < η < 1, -L0lnη is a positive number, the above expression can be further
simplified by timing -Lo ln η on the RHS, yielding the result.	□
Proposition 2. Let {wt > 0}tN=1 be the weights of the data indexed by t. Then the Hoeffding error
PtN=1 wt2/PtN=1 wt is minimized when the weights are equal: wt = C > 0, ∀ t.
Proof. Let W = [wι,…, WN]τ be the weight vector and f (W) = wττw∕(ITw) be the Hoeffding
error. Observe that f(w) is of the form:
f (W) = llwk/( ITW) = kw/( ITW)k,
where ∣∣wk = √wτw is the 2-norm of w. Thereby, let Z = w/(ITW) be the normalized vector.
That f (w) is minimized is equivalent to that g(z) = ∣∣z∣ is minimized for ITz = 1. By the lagrange
multiplier, this happens when
z
Vg = jj—1∣- = λl, for some λ ∈ R.
This can be achieved by zt = C for some C > 0. Therefore, we know f is minimized when
Wt
zt = 「- = c for some c.
ITW
Since ITW does not depend on t, We conclude that the minimizer happens at Wt = c > 0, ∀t.	□
24