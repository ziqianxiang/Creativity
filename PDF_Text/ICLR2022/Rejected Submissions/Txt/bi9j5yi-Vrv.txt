Under review as a conference paper at ICLR 2022
A General Theory of Relativity in Reinforce-
ment Learning
Anonymous authors
Paper under double-blind review
Ab stract
We propose a new general theory measuring the relativity between two arbitrary
Markov Decision Processes (MDPs) from the perspective of reinforcement learn-
ing (RL). Considering two MDPs, tasks such as policy transfer, dynamics model-
ing, environment design, and simulation to reality (sim2real), etc., are all closely
related. The proposed theory deeply investigates the connection between any two
cumulative expected returns defined on different policies and environment dynam-
ics, and the theoretical results suggest two new general algorithms referred to as
Relative Policy Optimization (RPO) and Relative Transition Optimization (RTO),
which can offer fast policy transfer and dynamics modeling. RPO updates the
policy using the relative policy gradient to transfer the policy evaluated in one
environment to maximize the return in another, while RTO updates the parame-
terized dynamics model (if there exists) using the relative transition gradient to
reduce the gap between the dynamics of the two environments. Then, integrat-
ing the two algorithms offers the complete algorithm Relative Policy-Transition
Optimization (RPTO), in which the policy interacts with the two environments
simultaneously, such that data collections from the two environments, policy and
transition updates are all completed in a closed loop to form a principled learning
framework for policy transfer. We demonstrate the effectiveness of RPO, RTO
and RPTO in the OpenAI gym’s classic control tasks by creating policy transfer
problems.
1	Introduction
Deep reinforcement learning (RL) has demonstrated its great successes in recent years, including
breakthrough of solving a number of challenging problems like Atari (Mnih et al., 2015), GO (Silver
et al., 2016; 2017), DOTA2 (Berner et al., 2019) and StarCraft II (Vinyals et al., 2019), with human-
level performance or even beyond. These successes demonstrate that current deep RL methods are
capable to explore and exploit sufficiently in huge observation and action spaces, as long as sufficient
and effective data samples can be generated for training, such as the cases in games. For example,
AlphaGo Zero (Silver et al., 2017) costs 3 days of training over 4.9 millions self-play games, and
OpenAI Five (Berner et al., 2019) and AlphaStar (Vinyals et al., 2019) spend months of training
using thousands of GPUs/TPUs over billions of generated matches. However, for environments
that prohibit infinite interactions, e.g., robotics, real life traffic control and autopilot, etc., applying
general RL is difficult because generating data is extremely expensive and slow. Even if parallel data
collection is possible, for example, by deploying multiple robots or vehicles running simultaneously,
the scale of collected data is still far below that in virtual games. Worse still, exploration in these
environments is considerably limited for safety reasons, which further reduces the effectiveness of
the generated data. Due to the above challenges, similar significant advances like in solving virtual
games have not been witnessed in these applications yet.
Generally, there are three tracks of approaches targeting to alleviate the aforementioned situation to
promote the widespread application of RL. They are improving the data efficiency, transfer learning
and simulator engineering.
To improve data efficiency, many recent efforts have been paid on investigating offline RL algo-
rithms (Siegel et al., 2020; Fujimoto et al., 2019; Kumar et al., 2019; Wu et al., 2019; Wang et al.,
2018). Compared to standard on-policy or off-policy RL, offline RL (also known as batch RL) aims
1
Under review as a conference paper at ICLR 2022
to effectively use previously collected experiences stored in a given dataset, like supervised learning,
without online interactions with the environment. The stored experiences may not be generated by a
fixed or known policy, so offline RL algorithms can leverage any previously collected data and learn
a provably better policy than those who generated the experiences in the dataset. Although offline
RL can effectively take advantage of finite data samples, solving a complex real-world task still
requires huge amount of high quality offline experiences. Another way to increase data efficiency is
to adopt model-based RL. Compared to model-free methods, model-based RL (Kaiser et al., 2019;
Janner et al., 2019; Moerland et al., 2020) learns a dynamics model that mimics the transitions in
the true environment, and then the policy can feel free to interact with the learned dynamics instead
of the true environment. It has been proved that the true return can be improved by interacting with
the learned dynamics model when the model error is bounded (Janner et al., 2019). However, learn-
ing an accurate dynamics model still requires sufficient transition data by interacting with the true
environment, specifically for complex dynamics with noisy transitions.
Transfer learning in RL (Zhu et al., 2020) is practically useful to adapt a policy learned in a source
environment to solve another task in the target environment. In the context of this paper, we consider
the case where the policy can feel free to explore in the source environment, while the amount of
collected data in the target environment should be as small as possible. When the source environment
is a simulated one while the target environment takes place in reality, the transfer problem is also
known as the simulation to reality (sim2real) problem. The simplest way to do transfer is to train
the policy in the source environment and then use the converged parameters as warm start for a
new policy or part of its parameters in the target environment, so that the amount of interactions
with the target is expected to be largely reduced, as long as the tasks and dynamics in the two
environments are closely related. Training a shared or partially shared policy in both the source
and target environments is an alternative method which also belongs to the multi-task reinforcement
learning scope (Hessel et al., 2019). Domain adaptation has been demonstrated to be another useful
technique (Ibarz et al., 2021). Such methods try to bridge the gap between the source and target
environments using some adaptation networks. For example, adapter networks were introduced to
convert the input in simulation to be close to the real-world observation, by utilizing the generative
adversarial model (James et al., 2017; Shrivastava et al., 2017; Bousmalis et al., 2017; 2018; Rao
et al., 2020), or on the contrary an inverse network was trained to convert real-world observation to
that in simulation (James et al., 2019). Using such adapter networks, the policy only needs to be
trained in the source environment, and then it can directly be applied in the target environment.
An important concept in transfer learning is that instead of directly deploying RL in the target
environment, a source environment is considered as a proxy. Sharing this spirit, the last track of
approaches tries to build a proxy simulator that is as close as possible to the target environment, and
hence we refer to such methods as simulator engineering. For example, in robotics control problems,
there are many mature toolboxes can offer simulation engineering, including MuJoCo, PyBullet,
Gazebo, etc. Model-based RL can also be viewed as a specific form of simulator engineering that the
simulator is composed by a pure neural network, which is trained to approach the target environment
as with lower model error as possible, while this might require a large amount of dynamics data
in the target environment as mentioned above. Actually, to achieve more efficient and accurate
simulator engineering, one recent rising direction is to integrate differentiable programming and
physical systems to build a trainable simulator, which follows the physical laws as in reality and
also whose key factors, such as the mass, length and friction of some objects, are trainable like the
parameters in neural networks. Representative examples include the DiffTaichi (Hu et al., 2020),
Brax (Freeman et al., 2021) and Nimble (Werling et al., 2021).
Overall, the existing methods focus on either directly improving the data efficiency in the target
environment or bridging/reducing the gap between a proxy environment and the target environment,
and there lacks a principled theory that can incorporate the learning in the two environments through
an unified framework, and explain the intrinsic relationship between the expected returns in the two
environments from the perspective of RL. In this paper, we inherit the spirit in transfer learning and
consider two environments, where one is free to interact and another is the goal to solve, and the
number of interactions in the goal environment should be as small as possible. We believe that there
exist some explicit connections between the expected returns in the two environments, given two
different policies, from the very fundamental perspective of RL. To verify this, we formally define
two Markov Decision Processes (MDPs) and then explicitly derive the difference between the ex-
pected returns in the two MDPs. In the following context, with the RL convention, an environment
2
Under review as a conference paper at ICLR 2022
is equivalent to an MDP. Specifically, let P(s0|s, a) and P0(s0|s, a) denote two dynamics transition
functions in any two arbitrary MDPs sharing the same state and action spaces, where (s, a, s0) is the
tuple of the state, action and next state. Let ∏0(a∣s) and ∏(a∣s) denote two arbitrary policies, and
denote J(P, π) as the cumulative expected return given P and π. Then, we aim to investigate the
difference J(P0, π) - J(P, π0), which is referred to as the relativity gap between the two MDPs. It
turns out that the relativity gap has a very interesting and compact form that integrates the interac-
tions in both environments. Now, suppose P and P0 are the dynamics functions in the source and
target MDPs respectively, and J(P, π0) has been maximized by optimizing π0. Then, with fixed P,
P0 and π0, maximizing the relativity gap over π by constraining π to be close to π0 will also improve
the return J(P0, π) in the target MDP; on the other hand, for trainable P, minimizing the relativity
gap by optimizing P given a fixed policy π = π0 will reduce the dynamics gap, similar to what is
done by conventional model-based RL methods. Based on the above two principles, our theoretical
results suggest two general algorithms referred to as Relative Policy Optimization (RPO) and Rela-
tive Transition Optimization (RTO), respectively. RPO updates the policy using the relative policy
gradient to transfer the policy evaluated in the source environment to maximize the return in the
target environment, while RTO updates a dynamics model using the relative transition gradient to
reduce the value gap in the two environments. Then, applying RPO and RTO simultaneously offers
a complete algorithm named Relative Policy-Transition Optimization (RPTO), which can transfer
the policy from the source to the target smoothly. RPO, RTO and RPTO interact with the two en-
vironments simultaneously, so that data collections from two environments, policy and/or transition
updates are completed in a closed loop to form a principled learning framework. In the experimental
section, we show how to practically apply RPO, RTO and RPTO algorithms. We demonstrate the
effectiveness of these methods in the classic control problems in OpenAI gym with both discrete and
continuous actions, by varying the physical variables like mass, length and gravity of the objects to
create policy transfer problems. At the last section, we discuss a few new directions based on the
proposed relativity theory, which are worthy future investigations.
2	Preliminaries
2.1	Reinforcement Learning
A standard RL problem can be described by a tuple hE, A, S, P, r, γ, πi, where E indicates the
environment that is an MDP with dynamics transition probability P; at each time step t, st ∈ S is
the global state in the state space S, and at ∈ A is the action executed by the agent at time step t
from the action space A; the dynamics transition function P(st+1 |st, at) is the probability of the
state transition (st, at) -→ st+1; for the most general case, the reward r(st, at, st+1) can be written
as a function of st, at and st+1, while in many tasks it only relies on one or two of them, or it is even
a constant in sparse rewards problem. For notation simplicity, we usually write r(st, at, st+1) as
rt; Y ∈ [0,1] is a discount factor and π(a∕st) denotes a stochastic policy. The following equations
define some important quantities in reinforcement learning. The objective of RL is to maximize the
expected discounted return
∞
J(P,∏) = Es0,a0,…〜p,∏ EYtrt , where so 〜P(so), at 〜∏(at∣st), st+ι 〜P(st+ι∣st,at).
t=0
At time step t, the state-action value QP,π, value function V P,π, and advantage AP,π are defined
as QPK(St,at) = Est+ι ,at+ι,…〜P,π [P∞=o Yl rt+l], V P,π (st) = Eat,st+ι,…〜P,∏ [P∞=0 Ylrt+l],
AP,π(s, a) = QP,π (s, a) - VP,π(s). In the above standard definitions, we explicitly show their
dependence on both the dynamics P and policy π, since we will analyze these functions defined on
variant dynamics and policies. This convention will be kept throughout the paper.
2.2	TRPO and PPO
Given two arbitrary policies π0 and π, a well-known policy improvement theorem (Kakade & Lang-
ford, 2002; Schulman et al., 2015) is the fact revealed by the following equation
∞
J(P,∏0) = J(P,∏)+ Es0,a0,…〜P,∏0 X YtAP,π (St,at) .	⑴
t=o
3
Under review as a conference paper at ICLR 2022
Based on this theorem, some widely adopted RL algorithms such as TRPO (Schulman et al., 2015)
and PPO (Schulman et al., 2017) are developed. In TRPO, the following objective is optimized
maximizes E	p,∏θ	πθ(Is) AP,πθoid (s,a),
S〜d	θold,a^πθoid ∏θθdld(a|s)	_
subject to E [KL (∏θ0id(∙∣s)ll∏θ(∙∣s))] ≤ δ,
where the policy is parameterized by θ, and θold is the parameter since last update. dP,π is the
discounted visitation probability given P and ∏. KL(∙∣∣∙) is the KL-divergence and δ is a small
constant restricting the policy update step size. To simplify the optimization, PPO removes the
constraint in TRPO and maximizes the following clipped version
Es〜dP,∏θoid a5ft	[min (R(e)AP'n°old (s, a), cliP (R(θ), 1 - e, 1 + e) Ap,πθold (s, a))],
s	,a πθold
where R(θ) =「(少))and E is a hyperparameter controlling the proportion of clipped data. We
πθold (a|s)
review the TRPO and PPO algorithms here to allow readers to conveniently compare and see the
connection between them and our proposed algorithms in the following sections.
3 The Theory of Relativity in RL
With awareness of the most fundamental RL definitions in Section 2.1, it is sufficient to proceed the
theory in this section. To be straightforward, we directly give the most important theorem revealing
the theory of relativity in RL below. All the proofs are provided in the appendix.
Theorem 1 (The Theory of Relativity in RL) Given two Markov Decision Processes (MDPs) de-
noted by E 0 and E, who share the same state and action spaces S and A, their dynamics transition
probabilities are P 0 (st+1 |st, at) and P (st+1 |st, at) for any transition (st, at) -→ st+1 in E0 and E,
respectively. Assume the initial state distributes in the two MDPs identically that P0(s0) = P(s0).
Let J(P, π) denote the expected return defined on dynamics P and policy π. Then, the relativity gap
between any two expected returns under different dynamics and policies is defined as
J(P0,π)-J(P,π0)=J(P0,π)-J(P,π)+J(P,π)-J(P,π0),	(2)
'-------V--------}	'--------V--------}	'-------V--------}
relativity gap	dynamics-induced gap	policy-induced gap
such that the dynamics-induced gap has an explicit form as
∞
J(P0,∏) - J(P,∏) = Es0,a0,…〜P0,∏X γt [r(st,at,st+ι) + YVP,π(st+ι) - QP,π(st, at)],⑶
t=0
and the policy-induced gap is revealed by the policy improvement theorem (Kakade & Langford,
2002) introduced in Eq. (1), rewritten by inverting π and π0 as
∞
J(P,∏) - JPnO) = Es0,a0,…〜P,∏ X γtAp,π0(st, at).	(4)
t=0
In Theorem 1, it is surprising that the dynamics-induced gap has a very compact formulation like
the policy-induced gap. Below, we emphasize a few important points implied in Eqs. (3) and (4):
•	In Eq. (3), the expectation is taken over the trajectory so, a0, ∙∙∙ sampled from (P0, ∏),
while the value and state-action value functions in the expectation, i.e., VP,π (st+1) and
QP,π(st, at), are value functions defined on (P, π). This reveals a very practically useful
conclusion that given a fixed policy π, the dynamics-induced gap can be calculated by
measuring the value functions VP,π(st+1) and QP,π(st, at) in the dynamics P (imagining
this is the source environment, where infinite data can be generated to accurately evaluate
the value functions), while collecting probably a few data samples in P0 (imagining this is
the target environment) to estimate the expectation.
•	In Eq. (3), Est+1 [r(st, at, st+1) + γVP,π(st+1)] 6= QP,π(st, at), because the transition
(st , at ) -→ st+1 takes place in P0 instead of P, and hence Eq. (3) is not zero whereas
P0(st+1|st, at) 6= P(st+1|st, at) happens for non-zero r(st, at, st+1) and VP,π (st+1)
with high probability, especially for high-dimensional deep neural networks. Of course,
ifP0 = P, we immediately have J(P0, π) = J(P, π) from Eq. (3).
4
Under review as a conference paper at ICLR 2022
•	The dynamics-induced gap, i.e., J(P0, π) - J(P, π), is related to what was analyzed in
model-based policy optimization (MBPO) (Janner et al., 2019). Unfortunately, MBPO
bounds the value gap with the dynamics model error at very early derivations to get their
main theorem, which provides a basic theoretical support for that the commonly adopted
model-based RL algorithms (alternatively updating the model and policy) can guarantee
policy improvement, while it does not suggest new algorithms. Instead, with much deeper
investigation of the fundamental dynamics-induced value gap, we can finally get the ex-
plicit identity equation in Eq. (3), instead of a bound. As we will show later, Theorem 1
suggests two thoroughly new algorithms for policy transfer and transition update. Details
of the superiority of Theorem 1 over MBPO can be found in the proofs in appendix.
So far, Theorem 1 provides important results on both dynamics-induced and policy-induced gaps,
while it is still not clear how these results can be applied empirically. In the following sections, we
will introduce two new practical algorithms derived from Theorem 1, where one algorithm is for
fast policy transfer from the source environment to the target environment, and another algorithm
updates the parameterized dynamics in the source environment to be close to the dynamics in the
target environment. Then, by combining the two algorithms, we obtain the complete algorithm to
fast transfer a policy from source to target.
4 Relative Policy Optimization (RPO)
As discussed previously, Eq. (3) in Theorem 1 suggests a way of estimating the dynamics-induced
value gap by evaluating QP,π(st, at) and V P,π(st+1) in P while sampling data in P0, given π.
Practically, it is of less interest to estimate the exact dynamics-induced gap. Instead, if we have
trained a policy ∏* in P (source environment) that maximizes J(P, ∏), then We are interested in
finding another π such that π = argmax∏[J(P0, π) - J(P, π*)], i.e., π maximizes the dynamics-
induced gap, and also indirectly maximizes J(P0, π). Normally, as long as P0 is not far from P,
finding ∏ can use ∏* as a warm start. Based on the above motivation, we propose the following
theorem to get a loWer bound of the dynamics-induced value gap.
Theorem 2 Define DmVX(p, q) = maxχ DTV(p(∙∣x)∣∣q(∙∣x)) as the total variation divergence be-
tween two distributions p(∙∣x) and q(∙∣x), where DTV(p(∙∣x)∣∣q(∙∣x)) = 1 Ey |p(y|x) — q(y∣x)∣.
Define C = max§,a ∣AP,π(s, a)|, where AP,π(s, a) = QP,π(s, a) — VP,π(s) is the advantage. Let
δι = DmVx(P 0 (∙∣s,a), P (∙∣s,α))
for any (s, a) ∈ S, A and let
δ2 = DmVx(∏0(∙∣s),∏(∙∣s))
for any s ∈ S, and any two policies π0 and π. Let rmax = maxs,a,s0 r(s, a, s0) be the max reward
for all (s, a, s0). Now, let ∆P0,P(π) = J(P0, π) - J(P, π) denote the dynamics-induced gap as a
function of P0, P and π. Now, we import a new policy π0 and define the following function
∞
L∏0 (π) = EYt Eso,ao,…,st 〜P0,π0 52n(at|st) ΣS P 0(St+1|St，at)
t=0	at	st+1
hr(st, at, st+1) + γV P,π (st+1) - QP,π (st, at)i
as an approximation of ∆0,p(π) by sampling so, ao, ∙… using π0 and evaluating VP,π0 and QP,π0
using π0. Then, we have the following lower bound
P0 P	2γδ1C	4rmaxδ1δ2γ
δ , (π) ≥L∏0 (π) - (i-Y)2 -	(1-Y)3 .
Based on Theorem 2, we can further obtain the following lower bound of the entire relativity gap.
Proposition 1 The entire relativity gap in Eq. (2) has the following lower bound
J(P0,∏) - J(P,∏0) ≥ Es〜dPθ,∏o,a,sθ〜P，,n，∏α⅛[r(s,a,s0)+ YV*03) - VP,π0(s)] - C
5
Under review as a conference paper at ICLR 2022
where S is the next state that (s, a) → S, and C = 27[1-+4『2 + 4rma-Y)δ2γ is a constant relying
on the dynamics difference δ1 and policy difference δ2.
Now, it becomes clear that by taking π = πθ and π0 = πθold for some policy parameters θ and its
old version θold since last update, Proposition 1 suggests the following empirical objective
maximizes E	po ∏ 习	πθ (als) x [r(s, a, s0) + YV P ,πθoid (s0) — V P ,πθoid (s)],
θ S 〜dP,πθold ,a,s0 〜P0,∏θoid ∏θoid (a∣s)[(, , )+Y	( )	(5)
subject to Es 〜dP 0,∏θοld [DTV (πθoid (IS)llπθ (IS))] ≤ ε,
for some small ε. At the first glance, the objective in Eq. (5) is very similar to the standard RL
problem considered in TRPO or PPO (Schulman et al., 2015; 2017). However, again, by noting
where the data is sampled from and how V and Q values are evaluated, we can realize the important
difference that in Eq. (5), r(s, a, s0) + γVP,πθold (s0) - VP,πθold (s) 6= AP,πθold (s, a), i.e., it is not
the general advantage function, because the transition (s, a) -→ s0 takes place in P0. To be more
accurate, we call r(s, a, s0) + γVP,πθold (s0) - VP,πθold (s) the Relative Advantage.
That is, Eq. (5) suggests an empirical algorithm that we sample (s, a, s0) from E0 using πold and
compute the values VP,πold (s0) and VP,πθold (s) using πθold in E to update θ. Therefore, we refer
to the optimization of Eq. (5) as the Relative Policy Optimization (RPO) algorithm.
The objective in Eq. (5) contains a constraint on the policy update size, and directly solving this
objective requires line search, similar to what was proposed in TRPO (Schulman et al., 2015). To
simplify the optimization, we use a clipped version, which has been demonstrated to be effective in
PPO (Schulman et al., 2017).
Proposition 1 implies that as long as we can improve Eq. (5) by at least the constant C, we can
guarantee improvement on J(P0, πθ) over the constant J(P, πθold ) at the current step. Gener-
ally, RPO optimizes θ to maximize the relativity gap J(P0, πθ) - J(P, πθold ) in the direction of
maximizing J(P0, πθ), because J(P, πθold) is a constant at the current step. However, starting
from a well trained policy ∏θ* in P, as RPO updates θ, ∏θold will be gradually far from ∏θ* , and
therefore J(P, πθold ) might decrease. If this happens, continuing maximizing the relativity gap
J(P0, πθ) - J(P, πθold ) can not guarantee the increase of J(P0, πθ). One possible solution is to
optimize the RPO loss plus the standard RL loss, e.g., the PPO loss, defined on P to keep πθ always
performing well in E . As we can imagine, in such case, πθ will be updated towards a robust policy
that performs well in both E0 and E. Indeed, as we will show in our experiments, as long as P0 is not
too far away from P, optimizing RPO + PPO is often able to obtain such a robust policy; however,
once P0 differs from P too much, RPO will fail to transfer the policy to the target environment. This
gives a hint that in addition to RPO, we need further to reduce the gap between P0 and P , if P0
and P are far away from each other. This is possible when P (the dynamics in source environment)
is trainable, which has been considered in physical dynamics modeling (Hu et al., 2020; Freeman
et al., 2021; Werling et al., 2021) and model-based RL methods (Janner et al., 2019).
5 Relative Transition Optimization (RTO)
In this section, given fixed ∏ and P0, We consider a trainable P. Suppose Pφ(s0∣s, a) is parameter-
ized by φ for any transition (s, a) -→ s0. In the following theorem, we will import three dynamics
quantities P0, Pφ and Pφ0, where P0 still can be imagined as the dynamics in a target environment,
and Pφ and Pφ0 are two variant dynamics functions parameterized by φ and φ0, respectively.
Theorem 3 Using the definitions in Theorem 2, the following function
∞
Lφ0(φ) =	Y Es0,aο,…,at〜P0,π Est+ι〜P0 r(st, at, st+1) + γVPφ0,π(st+1) -
t=0
Est+ι〜PΦ [r(St, at, st+1) + YVPφ0,π(st+1)]
is an approximation of ∆P0,Pφ(π) by evaluating the value using Pφ0 instead of Pφ, and we have
∣∆P0,Pφ (∏) - Lφο (φ)∣≤
4δ2γrmaχ
(1 - Y)3 .
6
Under review as a conference paper at ICLR 2022
In order to reduce the dynamics-induced gap by updating φ, We have to minimize ∣∆p0,Pφ(n)|.
Based on Theorem 3, we have ∣∆p0,Pφ (∏) | ≤ ∣Lφo (φ) | + 4；1-max. Therefore, we can alternatively
minimize | L0;(φ) |. Empirically, if we consider Pφ as a probability function, then by noting that
∞
Lφ0 (φ) = X XXY ES0,…,at~P0,πX (P0(st+ι∣st,at)-Pφ(st+ι∣st,at)) (r(st,at,st+ι)+γVPφ0,π(st+ι)),
t=0	st+1
and taking φ0 = φoid, minimizing ∣Lφo (φ)∣ is equivalent to optimizing the following least square
minimize。Es〜dP0,∏,°〜∏ X (P0(s0∣s, a) - Pφ(s0∣s, a))2 (r(s, a, s0) + γVPφold,π(s0))2 .	(6)
s0
That is, we sample (s, a) in E0 using π and optimize Eq. (6). We refer to the above
optimization as Relative Transition Optimization (RTO). It is easy to see that RTO implies
the standard model-based RL methods, who directly train φ using supervised learning, i.e.,
minimize。Es〜dpo,∏,0〜∏ Pso (P0(s0∣s, a) - Pφ(s0∣s, a))1 2 3 4 5 6 7. Indeed, the objective ofRTO in Eq. (6)
can be viewed as a weighted form of supervised learning, with the weight r(s, a, s0)+γVPφold,π(s0)
showing that transitions happened with larger values evaluated by Pφiold and ∏ should be optimized
more aggressively. This is fairly reasonable from the perspective of fitting values, instead of fitting
the dynamics directly in supervised learning. Therefore, RTO provides a theoretical explanation for
the standard model-based RL methods by reducing the dynamics-induced value gap, and absorbs
standard model-based RL as a special case in RTO.
In practice, the transition function P。is usually treated as a deterministic model, i.e., ^0 = φ(s, a).
Under such case, RTO optimizes
minimize。Es〜dP0,∏, (r(s, a, s0) + YVPφold,π (s0) - r(s, a, φ(s, a)) - YVPφoid,π (φ(s, a)))2 ,⑺
α,s0 〜P0 ,∏
and the common model-based methods solve minimize。Es〜dpo,∏ a so〜Po ∏ (s0 - φ(s, a))2. Com-
paring the two objectives, Eq. (7) is more general by noting that fitting value function can distinguish
the importance of states, and it is not necessary to learn a perfect dynamics model over all transitions.
In the experiments, we use deterministic dynamics model. More precisely, we adopt deterministic
physical dynamics model, similar to recent proposed differentialable simulators (Hu et al., 2020;
Freeman et al., 2021; Werling et al., 2021), which take advantage of the physical processes. Such
model is much more efficient than pure neural network based dynamics model, because only a few
scalars such as mass, length, gravity, etc., are trainable parameters in the physical systems.
6	Relative Policy-Transition Optimization (RPTO) Algorithm
Combining RPO and RTO algorithms, we finally obtain the complete Relative Policy-Transition
Optimization (RPTO) algorithm in Algorithm 1. As we observe in Algorithm 1, in the main loop of
Algorithm 1: Relative Policy-Transition Optimization (RPTO)
1. Give the source and target environments Esource and Etarget, and their dynamics Psource and Ptarget,
where the source dynamics Pφsource is parameterized by φ; give a well-trained policy πθ0 in Eφso0urce,
where φ0 perfectly describes the initial source dynamics;
2. Create two empty replay buffers Dsource and Dtarget;
3. Initialize θ = θ0 and φ = φ0 ;
while True do
4. Using πθ to interact with Eφsource and push the generated trajectories into Dsource;
5. Using πθ to interact with E target and push the generated trajectories into Dtarget;
source
6. Sample a mini-batch {(s,a,s0)}source 〜 DsOUrCe, and update VP φ ,πθ by minimizing the
TD-error;
7. Sample a mini-batch {(s,a,s0)}target 〜Dtarget, and apply the relative policy gradient in RPO to
update πθ ; at the same time, update Pφsource according to RTO;
end while
7
Under review as a conference paper at ICLR 2022
Table 1: An overview of the studied pairs of environments.
CartPole-v0	MOUntainCarCOntinUOUS-V0
Source Env Target Env Pole Length	0.5	0.6	Source Env Target Env GraVity	0.0025	0.006
Acrobot-v1	Pendulum-v0
Source Env Target Env Link Mass1	1.0	0.1 Link Mass2	1.0	0.1	Source Env	Target Env Gravity 10.0 (Earth)	3.72 (Mars)
RPTO, the policy πθ interacts with the two environments simultaneously and pushes the data into
two buffers separately. In step 6, we sample a mini-batch from Dsource to update the value function
in Eq. (5) from RPO. In step 7, we sample a mini-batch from Dtarget to update the policy parameter θ
according to Eq. (5) in RPO, and also update the dynamics φ according to Eqs. (6) or (7) from RTO.
Therefore, RPTO combines data collection from two environments, RPO and RTO in one closed
loop, and this offers a principled learning framework for policy transfer. Indeed, steps 6 and 7 can
Psource π
be parallelized, as long as the value function Vμ φ , θ (suppose it is parameterized by μ) does
not share parameters with ∏θ , i.e., μ and θ are independent. Also, μ can be updated more frequently
because it only requires data that can be generated infinitely from the source environment, and the
more accurate the value function in the source environment is, the more accurate the relative policy
gradient in step 7 will be estimated. For the dynamics parameter φ, the case becomes different, and
it is often not a good choice to update φ as fast as we can, for example, using a larger step size for
φ. To understand this, we need to explain how RPTO transfers the policy to the target environment
in advance.
As we can imagine, as Pφsource approaches Ptarget gradually by updating φ in step 7, the policy πθ
is able to interact with a sequence of smoothly varying source environments. During this training
period, the policy sees much more diverse transitions that lie between the initial source environment
(with dynamics Pφsource) and the target environment. These diverse transitions are very helpful to
encourage the agent to explore a robust policy. On the other hand, since the policy is initialized with
a well-trained θ0 in Pφsource, smoothly varying Pφsource to reach P target generates a sequence of
environments that naturally provide a curriculum learning scheme, and this is very similar to what is
considered in the recently emerged Environment Design methods (Dennis et al., 2020). Now, we are
ready to answer the question in the last paragraph that why φ should not be updated aggressively:
a slowly and smoothly varying φ provides more chances for the agent to see diverse transitions.
Actually, if Pφsource approaches Ptarget too fast, RPTO will be similar to directly training PPO in
the target environment with a warm start θ0 . In our experiments, we simply keep the learning rate
of φ the same as that in RPO.
7	Experiments
We experiment with all the OpenAI gym’s classic control tasks, including CartPole-v0, Mountain-
CarContinuous-v0, Acrobot-v1, and Pendulum-v0, because in these tasks the physical systems are
explicitly coded, which allows us to easily build physical dynamics model with only a few trainable
factors, e.g., pole length, gravity and link mass, as indicated in Table 1. Among these, CartPole-
v0 and Acrobot-v1 are of discrete action space and the other two are continuous control problems.
For all the tasks, the default settings in OpenAI gym are treated as the source environments, while
for each task, we arbitrarily modify some of its physical factors to create the corresponding target
environments. Details of the source and target environments are shown in Table 1.
For all tasks, we first pre-train a converged policy with PPO in the source environment, and the
pre-trained policy will be used as a warm start when transferring to the target environment. For
the policy transfer stage, PPO with the pre-trained policy as initialization, denoted as PPO-warm,
will always be used as the baseline method. Other implementation details, such as hyper-parameters
and neural network structures, are provided in the appendix. All the experiments in this section are
repeated 10 times to plot the mean curve with standard derivation region.
8
Under review as a conference paper at ICLR 2022
(d) RPTO Perforτnan∞
(a) RPO Perforπιan∞
Enβα>uw-≡BJBl eBΘJΘΛ4
0.00	0.05	0.10	0.15	0.20
Training Steps	Iee
200
(b) RPO vs. Varying Pole Length
Figure 1: Tutorial experiments in CartPole-v0.
0.00	0.05	0.10	0.15	0.20
TraInIngSteps	Iee
(c) RTO Perforπιan∞
0.00	0.05	0.10	0.15	0.20
TraInIngSteps	Iee
Enβα>cω∙sffBJ. βαse><
---PPO-≈*ro
PPO-wβ∏n
——RPO
——RPTO
0.00	0.05	0.10	0.15	0.20
TraInIngSteps	Iee
MountainCarContinuous-vO
Oa 02	0.4 OS 0.8	1.0
Training Steps	1e6
Erqaa ʌu 川-spel-⅛sω><
Acrobot-v1
Figure 2: Overall performance in other tasks.
0.0	02	0.4	0.6	0.8	1-0
Training Steps	1e6
-0o-1oo-12o-14o-1eo
Erqaa ʌu 川-spel-⅛sω><
Pendulum-vO
Oa 0.5	1.0	15	2.0 25 3Λ
Training Steps	1θ6
We first use CartPole-v0 as a tutorial environment to show how the proposed algorithms are practi-
cally applied. Fig. 1 reports the results for various evaluations of RPO, RTO and RPTO. Fig. 1(a)
demonstrates that RPO (without RTO) is sufficient to successfully transfer the policy to the target en-
vironment with pole length of 0.6, and RPO transfers the policy faster than PPO-warm. In Fig. 1(b),
by varying the pole length to test more target environments, we find that when the pole length differ-
ence becomes larger, RPO fails to obtain a stable policy in the target environment. The curves also
imply that for the case where RPO succeeds, the learned policy can perform well in both the source
and target environments, i.e., RPO finds a robust policy; while for the failed cases, the learned policy
deteriorates in both environments either. These results are consistent with our analysis at the end
of Section 4. Fig. 1(c) evaluates the performance of RTO. As we can observe, RTO can optimize φ
to converge to the true pole length in the target environment. Finally, Fig. 1(d) reports the RPTO’s
performance, where the curves of PPO-zero, PPO-warm and RPO are duplicated from Fig. 1(a) as
baselines. As we can observe, RPTO transfers the policy much faster than all the other methods.
So far, we have demonstrated how to apply RPO, RTO and RPTO in CartPole-v0 and what char-
acteristics of these algorithms can help us better understand their performance. Now, we directly
report all the performance curves for the other tasks in Fig. 2. In all the tasks, RPTO shows its supe-
riority over PPO-warm in terms of both fast policy transfer and even better asymptotic convergence,
because RPTO sees much more diverse dynamics that promotes exploration. Generally, RPO also
shows its capability on fast policy transfer, while it usually suffers from higher variance and unstable
performance, because it is limited by the dynamics gap between the source and target environments
as revealed by Theorem 2 by noting the dynamics gap δ1 is a fixed constant in RPO. For the case in
RPTO, benefitting from RTO, δ1 approaches zero gradually and so RPTO is stable and efficient.
8	Discussion and Future Work
We have proposed the general theory of relativity in RL. The theory shows its significance in creating
new algorithms that are empirically demonstrated effective and efficient. In addition to RPO, RTO
and RPTO, the relativity theory also opens a few new future directions in RL. For example, as we
have discussed at the end of Section 6, controlling the update step size or frequency of RTO can
provide better curriculum learning or environment design (although in this paper we simply fix the
learning coefficient of RTO as the same as RPO. Please see appendix). Connecting this with meta-
RL and current environment design algorithms is a promising future direction. Moreover, the theory
and algorithms in this paper are orthogonal to other techniques commonly used in policy transfer,
such as domain adaptation, domain randomization, augmented observation, etc., and all of these
methods can be integrated together in policy transfer applications. It is of great interests to apply all
these techniques to solve complex real-world problems like robotics in future research.
9
Under review as a conference paper at ICLR 2022
Reproducibility
According to the author guide, we provide a Reproducibility Statement here. For theoretical results
in this paper, we have provided all complete proofs in the appendix. For the algorithms, we have
attached the codes of RPO, RTO and RPTO in the submitted .zip supplementary material.
References
Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy
Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large
scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.
Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, and Dilip Krishnan.
Unsupervised pixel-level domain adaptation with generative adversarial networks. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3722-3731,
2017.
Konstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai, Matthew Kelcey, Mrinal Kalakrish-
nan, Laura Downs, Julian Ibarz, Peter Pastor, Kurt Konolige, et al. Using simulation and domain
adaptation to improve efficiency of deep robotic grasping. In 2018 IEEE international Conference
on Robotics and Automation (ICRA), pp. 4243-4250, 2018.
Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch,
and Sergey Levine. Emergent complexity and zero-shot transfer via unsupervised environment
design. Advances in Neural Information Processing Systems (NeurIPS), 33:13049-13061, 2020.
C. Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, and Olivier Bachem.
Brax - a differentiable physics engine for large scale rigid body simulation, 2021. URL http:
//github.com/google/brax.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning (ICML), pp. 2052-2062, 2019.
Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, and Hado van
Hasselt. Multi-task deep reinforcement learning with popart. In Proceedings of the AAAI Confer-
ence on Artificial Intelligence (AAAI), volume 33, pp. 3796-3803, 2019.
Yuanming Hu, Luke Anderson, Tzu-Mao Li, Qi Sun, Nathan Carr, Jonathan Ragan-Kelley, and
Fredo Durand. Difftaichi: Differentiable programming for physical simulation. In International
Conference on Learning Representations (ICLR), 2020.
Julian Ibarz, Jie Tan, Chelsea Finn, Mrinal Kalakrishnan, Peter Pastor, and Sergey Levine. How to
train your robot with deep reinforcement learning: lessons we have learned. The International
Journal of Robotics Research, 40(4-5):698-721, 2021.
Stephen James, Andrew J Davison, and Edward Johns. Transferring end-to-end visuomotor control
from simulation to real world for a multi-stage task. In Conference on Robot Learning (CoRL),
pp. 334-343. PMLR, 2017.
Stephen James, Paul Wohlhart, Mrinal Kalakrishnan, Dmitry Kalashnikov, Alex Irpan, Julian Ibarz,
Sergey Levine, Raia Hadsell, and Konstantinos Bousmalis. Sim-to-real via sim-to-sim: Data-
efficient robotic grasping via randomized-to-canonical adaptation networks. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 12627-12637,
2019.
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-
based policy optimization. Advances in Neural Information Processing Systems (NeurIPS), 32:
12519-12530, 2019.
Eukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad
Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model
based reinforcement learning for atari. In International Conference on Learning Representations
(ICLR), 2019.
10
Under review as a conference paper at ICLR 2022
Sham Kakade and John Langford. Approximately optimal ap- proximate reinforcement learning. In
International Conference on Machine Learning (ICML), volume 2, pp. 267-274, 2002.
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-
learning via bootstrapping error reduction. In Advances in Neural Information Processing Systems
(NeurIPS), pp. 11761-11771, 2019.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, and Georg Ostrovski. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Thomas M Moerland, Joost Broekens, and Catholijn M Jonker. Model-based reinforcement learn-
ing: A survey. arXiv preprint arXiv:2006.16712, 2020.
Kanishka Rao, Chris Harris, Alex Irpan, Sergey Levine, Julian Ibarz, and Mohi Khansari. Rl-
cyclegan: Reinforcement learning aware simulation-to-real. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 11157-11166, 2020.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning (ICML), pp. 1889-1897,
2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Joshua Susskind, Wenda Wang, and Russell Webb.
Learning from simulated and unsupervised images through adversarial training. In Proceedings
of The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2107-2116,
2017.
Noah Y Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Ne-
unert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing
what worked: Behavioral modelling priors for offline reinforcement learning. arXiv preprint
arXiv:2002.08396, 2020.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, and Marc Lanctot. Mastering
the game of go with deep neural networks and tree search. Nature, 529(7587):484, 2016.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, and Adrian Bolton. Mastering the game of go without
human knowledge. Nature, 550(7676):354, 2017.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, JUny-
oung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster
level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.
Qing Wang, Jiechao Xiong, Lei Han, Han Liu, Tong Zhang, et al. Exponentially weighted imita-
tion learning for batched historical data. In Advances in Neural Information Processing Systems
(NeurIPS), pp. 6288-6297, 2018.
Keenon Werling, Dalton Omens, Jeongseok Lee, Ioannis Exarchos, and C Karen Liu. Fast and
feature-complete differentiable physics for articulated rigid bodies with contact. arXiv preprint
arXiv:2103.16021, 2021.
Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.
arXiv preprint arXiv:1911.11361, 2019.
Zhuangdi Zhu, Kaixiang Lin, and Jiayu Zhou. Transfer learning in deep reinforcement learning: A
survey. arXiv preprint arXiv:2009.07888, 2020.
11
Under review as a conference paper at ICLR 2022
Appendix
A Proof of Theorem 1
Define the expected cumulative return as
∞
J(P, π) = Eso,ao,…〜P,π EYtr(St, at, st+1)
t=0
where
S0 〜ρ(P0), at 〜∏(at∣St), St+1 〜P(st+1∣St, at).
To better understand the expected cumulative return, we have
∞
J(P, π) =	γtE
st ,at ,st+1 r(St, at, St+1)
t=0
∞
γ γ γ Est~p(st),at~∏(at∣st),st+ι~P(st+ι ∣st,at)r(St, at, st+1)
t=0
∞
γt	p(St)	π(at |St)	P(St+1|St, at)r(St, at, St+1),
t=0	st	at	st+1
where p(St ) indicates the probability that St is visited. With a fixed policy π, we investigate the
value difference between two MDPs as
J(P0, π) - J(P, π)
∞
∑>t[Esi，⑶),at~π(at∣sjst+ι~P 0(st+1 ∣st,αt)r(st, at, st+1)-
t=0
Est~p(st),st+i~P(st+i|st,at),at~n(at|st)r(st, at, st+1)]
∞
γt	p0(St)	π(at∣St) E (P0(st+ι∣St, at) - P(st+ι∣St, at)) r(st, at, St+ι)+
t=0
st
at
st+1
E(PO(St)-P(St))En(at|st)EP(st+ιιst,at)r(st,at,st+ι).	(8)
st	at	st+1
It is worth mentioning that MBPO (Janner et al., 2019) just stops here at Eq. (8) and starts to bound
the transition difference P0(St+1|St, at) - P (St+1 |St, at) and the visitation difference P0 (St) -P(St)
with the dynamics model error, which is similar to δ1 defined in our Theorem 2. Then, it obtains a
lower bound of Eq. (8) to conclude the main theorem in MBPO. As we will show below, this is still
far away to reach our main theoretical results in Theorem 1.
Continuing from Eq. (8), the first term in Eq. (8) derives as
P0(St)	∏(at∣St) E (P0(St+1∣St, at) - P(St+ι∣St, at))r(St, at, St+ι)
st
at
st+1
X P((S)) P(St) X n(at|St) X
st	t	at	st+1
(P0(St+1|St, at) - P (St+1 |St, at))
P (St+1 |St, at)
P (St+1|St, at)r(St, at, St+1)
=Est~p(st),at~π(at∣St),St+ι~Ρ (st+ι∣st,αt)
P0(St) P0(St+1|St, at) - P(St+1|St, at)
P(St)
P(St+1|St,at)
=Est,at,st+ι~;
P0(St) P0(St+1|St, at) - P(St+1|St, at)
P (St+1 |St, at)
r(St, at, St+1).
r(St, at, St+1)
12
Under review as a conference paper at ICLR 2022
In the second term of Eq. (8), the difference of the marginal probabilities Pj(St) - P(St) can be
expanded similarly as
P(St) - p(st)
=ESt-ι~p0(st-ι),αt-ι~π(αt-ι∣st-ι) P (StISt-1 ,at-1) —
ESt-ι~p(st-ι),at-ι~π(at-ι∣st-ι) P(StlSt-1, at-1)
=E P (St-1) E π(at-i∣St-i) (P 0(St∣St-i, a-ι) - P (StISt-i, a-ι)) +
St-I	ɑt-ɪ
E(p0(St-1) -P(St-I)) E π(at-1∣St-1)P(StISt-1, at-1)
st-ɪ	ɑt-ɪ
=ESt-ι~p(st-ι),at-ι~π(at-ι
PZ(St-1)
IJ)部二T
(PZ(StISt-1, at-1) - P(StISt-1, at-1)) +
E(PZ(St-I) - P(St-I))Eat-ɪ 〜P(at[∣st-1)P (StISt-1 ,at-1 ) ∙
St-1
(9)
Plugging Eq. (9) back into the term ofEq. (8), we have
E (PZ(St)- P(St)) Eπ(at]St) E P(St+“St, at)r(st, at, St+1)
St	at	St+1
E(PZ (St) - P(St)) Eat 〜π(at ∣St),St+1 〜P(St+ι∣St,at)r(st,at,St+1)
St
一ESt-ɪ ,at-ɪ ,St,at ,St十ι~P,π
PZ(St-I) PZ(StISt-1,at-1) 一 P(StISt-1,at-1)
P(St-I)	P(StISt-1, at-1)
r(St,at,St+1)+
E(PZ(St-I) - P(St-I))Eat-,St42,+ ^(%, at, St+1).
St-ɪ
(10)
Note that the second term in Eq. (10) has an identical form as that in Eq. (8) by expanding prior
states and actions. Therefore, by recursively expanding Eq. (10) backward, we finally have
J(Pz,π)- J(P,π)
∞
X Y
t=0
PZ(St) P Z(St+1〔St ,at) -P (St+1〔St, at)
,at ,St+】〜P ,π ^S)	P (St+1ISt,at)
r(St,at,St+1) +
ESt-ɪ,at-ɪ,St,at,St十ɪ〜Ρ ,π
PZ(St-I) PZ(StISt-1,at-1) - P(StISt-1,at-1)
P(St-I)	P(StISt-1, at-1)
r(St, at, St+1) +
1∞π7	tPZ(Si) PZ(Si+1^, ai) - P(Si+1ISi,ai) /	ʌ
=N EiP,π g Y PST------------------P(Si+1ISi,ai)---------r(St,at,St+1)
κ	Vλ t(	、三 PZ(Si) PZ(Si+1ki, ai) - P(Si+1ISi,ai)	z11λ
=5"工 IMSt'a'，StQ ∑ K---------------------------P (Si+1ISi,ai)--------,	(II)
where T indicates the trajectory s0, a0, ∙ ∙ ∙.
Let Xt= Ytr(St, at, St+1) and yi = ⅛⅛ P'⑶昨晨?-Pa：十"Se), We have
∞ t
EXt Eyi = x0y0 + x1(y0 + y1) + x2(y0 + y1 + y2) +--------
t=0	i=0
∞	∞
=y0(x0 + X1 + X2 +-----) + y1(x1 + X2 +----) +----= Eyi £xt,
i=0	t=i
13
Under review as a conference paper at ICLR 2022
and	t∞=i xt	= γi	t∞=i	γt-ir(st, at,	st+1) =	γiRi,	where Ri	=	t∞=i γt-ir(st,	at,	st+1) is the
empirical cumulative future reward. Continuing from Eq. (11),
J(P0,π)-J(P,π)
∞
EiP ,∏ X YtRt
t=0
PO(St) P0(st+1|st, at) - P(st+1|st, at)
P(St)	P(st+1 ∣St, at)
P(St) P0(st+1∣St,at) - P(st+1∣St,at)
p(st)	P(st+1∣St, at)
∞
X
t=0
YtET 〜P ,∏ Rt
∞ XYt t=0	E	PO(St) P0(St+1|St,at) -P(St+1|St,at) E	ɑt+ι,st+2,…~P,∏ Rt
	s0,a0,…,st+1 〜P,π 肃T	P (St+ι∣St ,at)	
∞ XYt t=0	E	PO(St) P0(St+i|St, at) - P(St+i|St, at)E ……,st+1 〜P,π 又Sr	P (St+ι∣St ,at)	ɑt+ι,st+2,…〜P,π [r(St, at, St+1) + YRt+1]
S tκ	PO(St) P 0 (st+1|st, at) - P (St+1lst,at) / /	、	vP,π∕
YEs0 Es0,a0,…，st+1~p,π 7(Sty	P(St+ι∣St,at)	(r(St,at,St+i) + YV	(St+1))
X YtEs0,a0,…,st,at 〜po,π Est+ι 〜P,π P(SsQPJt -P(St+1 Ma/ (r ( " ,电⑼+])+ YV P,π ⑶十]))
t=0	St+1 St, at
∞
Y 0 γ* {Ego,。。，…，st,αt,st+ι~p0,∏ [r (St,at, St+1) + YV ""(%+1)] - Es。,。。，…，st ,at~P0,π Q。K(S t, at) }
t=0
∞
ET〜P0,π ^X Yt [r(St,at, St+1) + YVP,π(St+1) - QP,π(St, at)] .	(12)
t=0
Note that in Eq. (12), Est+1 r(St, at, St+1)+YV (St+1) 6= Q(St, at), because the transition (St, at) -→
St+1 takes place in PO instead of P. Now, we complete the proof of the relativity theory.
B	Proof of Theorem 2
Denote the dynamics-induced value gap as a function
∆P0,P(π) =J(PO,π)-J(P,π)
∞
=ET〜P0,π ^X Yt [r(St, at, St+1) + YVP,π (St+1) - QP,π (St, at)].
t=0
We have
∆P0,P(π) - ∆P0,P(πO)
∞
=ET〜P0,π ^X Yt [r(St, at, St+1) + YVP,π (St+1) - QP,π (St, at)]-
t=0
∞
ET〜P0,∏0 X Yt hr(St, at, St+1) + YVP,π (St+1) - QP,π (St, at)i .
t=0
Let
∞
L∏0(π) =E Yt ESO ,ao,…，st 〜P 0,π0 En(at|St) EP 0(St+1|St,at)
t=0	at	st+ι
hr(St, at, St+1) + YVP,π (St+1) - QP,π (St, at)i
14
Under review as a conference paper at ICLR 2022
be the approximation of ∆P',P(π) by sampling (s0, a1, ∙∙∙ , St) and evaluating values using π0.
Then, we have
∆P，P (π) - L∏ (π)
∞
=ET〜P0,π ^X Tt [r(st,αt, st+1) + TVP,π(st+1)- QP,π(st, at)]-
t=0
∞
ET 〜P 0,π XTt 卜(st, «t, St+1) + TVp,π'(st+ι) - QIPN(St, «t)] +
t=0
∞
ET〜P0,π X Tt	[r(St,	αt,	st+1)	+ TVp,π	(st+1)-	QP,π	(st,	Ot)i	-
t=0
∞
ETtES0,a0,…,st〜P，,n，Eπ(αt∣st) EP'(st+1∣st,at)
t=0	at	st+ι
[r(st, at, st+1) + TVP,π (St+1) - QRF(St, αt)] .
Let
∞
DI= ET〜P0,π ^X Tt [r(st, at, st+1) + TVP,π (st+1) - QP,π (st, at)]-
t=0
∞
ET〜P0,π ^X Tthr(St, at, st+1) + TVP,π (st+1) - QP" (st, at)],
t=0
and
∞
D2 = ET〜P0,π ^X Tt b(st, at, st+1) + TVP" (st+1) - QP" (st, at)]-
t=0
∞
TtEs
0,α0,…，st〜P0,π， Σ π(at∣st) EP 0(st+1∣st,at)
t=0	at	st+ι
[r(st, at, st+1) + TvP,π (st+1) - qP,f (st, at)] ∙
For D1, we have
∞
D1 = ET〜P0,π X Tt
t=0
T 卜p,π (st+1) - VP,n，(st+1))-
〜P [r(st, at, st+1) + TVP,π(st+1)] - ESt十1 〜P [r(st, at, st+1) + TVP,π (st+1)]
∞
X Tt+1Es0,...,αt〜P，,π
t=0
(VP,π(st+1) - VPk，(st+1)) - Est+1 〜Pkp,π(st+1) - VP,n，(st+1))
∞
X Tt+1ES0,…,at〜P,,π	X (P0(st+1∣st, at) - P(st+1∣st, at)) (Vp,π(st+1) - VP" (St+1))
t=0	St+1
From the Lemma 1 in TRPO (Schulman et al., 2015), we have
∞
∣Vp,π(st) - VP，n，(st)∣ = ET〜P,∏ XTi-t∣Aπ，(st, at)∣
-i=t	.
€
≤ ----
—1 - t
Therefore, we have
∞
∣D1∣≤ XTt+1
t=0
2δ1e
I- t
2tδ16
(ɪ-ʒ)ɪ .
(13)
15
Under review as a conference paper at ICLR 2022
For D2 , we have
∞
D2 = X Yt X(Pn(St)- p∏0(St))X n(at|St) X
t=0	st	at	st+1
P0 (St+1|St, at) hr(St, at, St+1) + γV P,π (St+1) - QP,π (St, at)i
∞
=X γt X(Pn(St)- p∏o (St))X n(at|St) X
t=0	st	at	st+1
(P0 (St+1|St, at) - P (St+1 |St, at)) r(St, at, St+1) + γV P,n (St+1) .
Using the Lemma B.2 in MBPO, we have DTV (P0n(St)||P0n0(St)) ≤ tδ2. Also, we can bound
∞
∣r(St,at, St+ι) + YVP,π (St+ι)∣ = ∣r(St, at, St+ι) + YE X YLt-%(Si,ai, Si+ι)∣ ≤ 鲁”.
i=t+1	- γ
Finally, we have
∞
4rmax δ1 δ2	t	4rmaxδ1δ2Y
lD2 l≤	t=0 snɪɪ .	(14)
Combining Eqs. (13) and (14), we finally get the lower bound
P0 P	2Yδ1	4rmaxδ1δ2Y
δ , (π) ≥ Lndn)- (1-γ)2 - (i-γ)3 .	(15)
Now, we complete the proof.
C Proof of Proposition 1
Recall that J(P0 , π) = J(P, π) + ∆P0,P (π) and
∞
Ln0(π) =	Y ES0,αo,…,st〜P0,n0 Σn(at∣St) Σ P0 (St+1 |St, at)
hr(St, at, St+1) + YV P,n (St+1) - QP,n (St, at)i
=X dp0,n0(s) X n(a∣S) X P0(S0∣S,a)[r(S,a, S0) + YVPn(Sr)- Qp,n0(S,a)]
s	a	s0
=Es〜dP0,∏0,a,s，〜P0,∏0Πa⅛[r(S, a, C + YV"0⑺ - QP,n0(s,明
Directly maximizing Ln0 (π) is insufficient, because once π varies, J(P, π) varies (probably de-
creases) accordingly, which can not guarantee monotonic increasing in J(P0, π). The case is that
we have to maximize J(P, π) + ∆P0,P(π). The policy improvement theorems in TRPO suggest
∞
J(P,n)- J(P,n0) ≥ EiP,n，X YtAP,n0 (St, at)
t=0
4Yδ22
(1-Y2.
16
Under review as a conference paper at ICLR 2022
Therefore, we have the final bound
J(P0,π) - JP 吟
=J(P; π) - J(P, π) + J(P, π) - J(P, π0)
∞	一
≥ Lπ0 (π) + Eτ-P,π0 X YtAP" (St,at) - C
_t=0	_
=EgdPO,∏0,a,s0〜P0,π ∏as)[r(s, a, C + YVPMg- QPB(s, a) + APB (s, a)] - C
=ES〜dp,"As，〜P0,π 5⅛⅜[r(s,α,C + YVPK ⑺-VPK (s)] - C	(16)
where C = 27；I-YF近 + 4r隹：$2Y. Now, we complete the proof.
D Proof of Theorem 3
Continuing from Eq. (12), we have
J(P0,π) - J(P,∏)
∞
=ET〜P，^X γt [r(st, at, st+1) + YVP,π (st+1)- QP,π(st, at)]
t=0
∞
=EYtES0,a0,…，at 〜P 0,π [ESt+ι 〜P，(r(st,at,St+1) + YV PK (St+1)) - ESt+1 〜P (r ( st, at, st+1)+ YV P (St+1))]
t=0
∞	"
=	YtES
0,a0,…，at〜P0,π EP0(st+1∣st,at) (r(st,at,st+ι) + YVP,π(st+ι))
t = 0	L St+1
-P(St+ι∣st, at) (r(st, at, st+1) + yvp,π(st+1)).
St+1
Now, considering a parameterized source dynamics Pφ, we have
∆pPy J(P0,π) - J(Pφ,π)
∞	"
=EYtES0 ,a。,…，at 〜P0,π EP '(st+1∣st,at) (r(st,at, st+1) + YV Pφ，π (St+1 ))
t=0	St+1
-pΦ(St+1∣st, at) (r(st, at, st+1) + Yvpφ,π(St+1)).
St+1
Let
LΦp (O)=
∞	"
YtES
0,a0,∙∙∙ ,at~Pp,π	P2 P0(st+1 ∣St, at) (r(st, at, St+1) + YVPφ p,π(st+1))
t=0	St+1
-ɪ2 pΦ(St+1∣st, at) (r(st, at, st+1) + Yvpφp,π(st+1)).
St+1
Then,
∆p/,Pφ (π) - Lφ p(φ) =
∞
X Yt+1ES0,a0,…，at〜P P,π X (PZ(St+1∣st, at) - pφ(st+1∣st, at)) (VPφ,π(st+1) - VPφ''"(51))
t=0	St+1
17
Under review as a conference paper at ICLR 2022
For V Pφ,π(st+1) - V Pφ0,π(st+1), we have
VPφ,π(st+1)-VPφ0,π(st+1)
∞
=E2φ,∏ E Yi-t-1 E (Pφ(si+ι∣Si,ai) - Pφ0(si+ι∣Si, ai)) (r(si,ai, Si+ι) + YVPφ0,π(si+ι))
i=t+1	si+1
V 2δ1rmax
≤ W-Y)2.
Finally, we obtain the lower bound
∣∆(Φ)-Lφ0(Φ)∣≤ 4δ2γrma3x.
(1- Y)3
E	The RPO and RTO Algorithms
Due to space limitation, we are not able to put the RPO and RTO algorithms in the main text. These
two algorithms can be tailored from the RPTO algorithm in Algorithm 1, and we put them here.
Algorithm 2: Relative Policy Optimization (RPO)
1.	Give the source and target environments Esource and Etargetand their dynamics Psource and Ptarget；
give a well-trained policy πθ0 in Esource;
2.	Create two empty replay buffers Dsource and Dtarget ;
3.	Initialize θ = θ0 ;
while True do
4.	Using πθ to interact with Eφsource and push the generated trajectories into Dsource ;
5.	Using πθ to interact with E target and push the generated trajectories into Dtarget ;
0	P source π
6.	Sample a mini-batch {(s,a,s )} source 〜 Dsource, and update V P φ , θ by minimizing the
TD-error;
7.	Sample a mini-batch {(s, a, S')}target 〜Dtarget, and apply the relative policy gradient in RPO to
update πθ ;
end while
Algorithm 3: Relative Transition Optimization (RTO)
1.	Give the source and target environments ESoWy and E target, and their dynamics pssource and P target,
where the source dynamics Pφsource is parameterized by φ; give an arbitrary policy πθ ;
2.	Create two empty replay buffers Dsource and Dtarget ;
3.	Initialize φ = φ0 ;
while True do
4.	Using πθ to interact with Eφsource and push the generated trajectories into Dsource ;
5.	Using πθ to interact with E target and push the generated trajectories into Dtarget ;
P source π
6.	Sample a mini-batch {(s,a,s )} source 〜 Dsource, and update V P φ , θ by minimizing the
TD-error;
7.	Sample a mini-batch {(s,a, S')}target 〜Dtarget, and update p^ource according to RTO or SL;
end while
F	Other Experimental Details
For all the environments used in our experiments, the policy neural network and value neural net-
work are of the same structure. The state is fed into three fully connected layers with ReLU activa-
tion function, and then the output embedding is fed into a policy head and a value head, respectively.
That is, the policy network and value network share the bottom embeddings. For tasks with discrete
action space, the policy head is a softmax layer to output multinomial distribution; for continuous
control problems, the policy head is a diagonal Gaussian distribution that outputs a mean and a std.
18
Under review as a conference paper at ICLR 2022
The value head consists of two fully connected layers and finally outputs a scalar value. All the
layers are of the same size of 64 in our experiments. For the physical dynamics model, each envi-
ronment has its own physical systems coded in OpenAI gym. In CartPole-v0, only the pole length is
treated as the trainable parameter, i.e., φ in CartPole-v0 only contains one free parameter. Similarly,
in MountainCarContinuous-v0, only gravity is trainable; in Acrobot-v1, two physical factors link
mass1 and link mass2 are trainable; in Pendulum-v0, the trainable parameter in φ is also the gravity.
For all algorithms, we choose their learning rate from [5e-4, 1e-4, 1e-5] with the best performance.
For RPTO, its loss consists of the RPO loss and the RTO loss, and we simply fix both the loss
coefficients as 1.0 in all experiments. For RPO, as mentioned in the main text, we clip the objective
in Eq. (5) in a similar way as PPO did, and the clip range is set to 0.2, which is suggested by PPO.
For all the environments, we first pre-train a converged policy in the source environment using PPO-
zero, and then use the trained parameters as warm start for all algorithms in the policy transfer
experiments. However, for MountainCarContinuous-v0, PPO-zero fails to learn a good policy that
can solve the source task. This phenomenon has also been observed in many previous approaches
considering deep exploration, where MountainCarContinuous-v0 has been considered as a typical
task for deep exploration. Nevertheless, it is not our focus in this paper, and as long as we can
obtain a good policy in the source environment, this policy can be used as a warm start for the policy
transfer stage, no matter what method is adopted to generate this policy. Therefore, to solve the
source MountainCarContinuous-v0, we first handcraft a rule that can succeed in the source task,
and then employ imitation learning to obtain a parameterized policy, which is further fine-tuned
using PPO-zero in the source MountainCarContinuous-v0 environment. The final converged policy
is used as the warm start at the policy transfer stage. For other environments, PPO-zero can provide
a good policy in the source task.
19