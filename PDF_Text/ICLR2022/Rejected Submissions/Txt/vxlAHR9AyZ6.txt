Under review as a conference paper at ICLR 2022
α-WEIGHTED FEDERATED ADVERSARIAL TRAINING
Anonymous authors
Paper under double-blind review
Ab stract
Federated Adversarial Training (FAT) helps us address the data privacy and gover-
nance issues, meanwhile maintains the model robustness to the adversarial attack.
However, the inner-maximization optimization of Adversarial Training can exac-
erbate the data heterogeneity among local clients, which triggers the pain points
of Federated Learning. This makes that the straightforward combination of two
paradigms shows the performance deterioration as observed in previous works.
In this paper, we introduce an α-Weighted Federated Adversarial Training (α-
WFAT) method to overcome this problem, which relaxes the inner-maximization
of Adversarial Training into a lower bound friendly to Federated Learning. We
present the theoretical analysis about this α-weighted mechanism and its effect
on the convergence of FAT. Empirically, the extensive experiments are conducted
to comprehensively understand the characteristics of α-WFAT, and the results on
three benchmark datasets demonstrate α-WFAT significantly outperforms FAT
under different adversarial learning methods and federated optimization methods.
1	Introduction
To handle the data privacy and governance issues, Federated Learning (McMahan et al., 2017) as
one promising paradigm of distributed training has drawn the increasing attention (McMahan et al.,
2017; Kairouz et al., 2019). However, training locally in Federated Learning also introduces the
vulnerability from the adversarial attacks (Goodfellow et al., 2015; Kurakin et al., 2016), which drives
us to consider the model robustness in this framework. Thus, recent advances (Kairouz et al., 2019)
explore to apply the Adversarial Training methods (Madry et al., 2018) into Federated Learning.
However, the straightforward combination of Adversarial Training and Federated Learning presents
some potential challenges due to the communication cost and the hardware requirement. For example,
Shah et al. (2021) pointed out that the communication in Federated Learning may be a constraint to
Adversarial Training, and proposed a dynamic schedule on the local training epochs to achieve the
expected robustness in a short communication budget. Hong et al. (2021) considered the hardware
constraint where some clients are not able to participate Adversarial Training, and they proposed a
federated robust propagation method to share the adversarial robustness among the clients. Although
these previous works indeed addressed some realistic problems, one critical issue in the way is the
performance deterioration in the combination of two paradigms as observed in (Zizzo et al., 2020).
As shown in the left panel of Figure 1, one typical phenomenon in Federated Adversarial Training
(FAT) is the robust accuracy of FAT (Zizzo et al., 2020) based on FedAvg (McMahan et al., 2017)
will decrease significantly at the later stage of learning compared with the centralized Adversarial
Training (Madry et al., 2018) that does not. Actually, it exists in many variants of Federated Learning
methods (Shah et al., 2021) under different communication rounds and different local training epochs.
However, it is still lack of the sufficient algorithmic breakthrough to overcome this issue, since almost
all previous works (Zizzo et al., 2020; Shah et al., 2021; Hong et al., 2021) consistently apply the
conventional update framework of Federated Learning with Adversarial Training.
We dive into this phenomenon and attribute it to the inner-maximization of Adversarial Training.
Compared with the centralized Adversarial Training (Madry et al., 2018), the training data of FAT is
distributed to each client, which leads to the Adversarial Training in each client unaware of the data in
the others. Therefore, the adversarial examples generated by the inner-maximization of Adversarial
Training tend to be highly biased to each local distribution, yielding the radical optimization to pursuit
the model robustness (as shown in Figure 2, it has a severe local bias to the global optimum). In
1
Under review as a conference paper at ICLR 2022
Aoalnoo4
FAT： Natural   FAT： Robust
AT: Natural ---------- AT: Robust
0 5 0 5 0
3 2 2 1 1
Aoalnooq lsnqoα
0	20	40	60	80	100	0	200	400	600	800	1000
Communication	Communication
Figure 1: Left: comparison between centralized Adversarial Training and Federated Adversarial
Training based on FedAvg. Right: comparison between FAT and α-WFAT. All the experiments are
conducted on CIFAR-10 dataset (Non-IID) with 5 clients, and use AT (Madry et al., 2018) to train as
well as PGD-20 to evaluate the Robust Accuracy. Note that, the notation “method-A-B" in the right
panel means the method with A local training epochs and B communication rounds.
short, the inner-maximization of Adversarial Training exacerbates the data heterogeneity among local
clients, which actually triggers the pain points of Federated Learning that are exploring urgently.
To handle this problem, we propose a new learning framework based on a simple but effective re-
weighting mechanism, namely, α-Weighted Federated Adversarial Training (α-WFAT). Concretely,
we relax the objective of the inner-maximization in Adversarial Training into a lower bound by an α-
weighting mechanism (as Eq. (3) in Section 4.2). Similar to the bias-variance trade-off (Kohavi et al.,
1996), we introduce the small bias to the original objective via a low-bound relaxation, facilitating
a friendly optimization in the combination of Adversarial Training and Federated Learning. This
constructs a conservative optimization for Adversarial Training that weights different population based
on their adversarial losses. Then, the harsh heterogeneous update can be down-weighted with this
flexible weighting mechanism, and the convergence could be accelerated at the same time. Applying
the similar idea to FAT, we propose α-WFAT that emphasizes the robust clients more compared with
other non-robust clients to alleviate the heterogeneous bias caused by the local adversarial generation.
The right panel of Figure 1 gives a simple comparison between α-WFAT and FAT. Empirically, we
conducted extensive experiments to provide a comprehensive understanding of the proposed α-WFAT,
and the results of α-WFAT in the context of different adversarial learning methods and federated
optimization methods demonstrate its superiority to improve the model performance.
Main Contributions
•	We derive an α-weighted relaxation for Adversarial Training to relax the inner-maximization
by a lower bound, which builds a mediating function to alleviate the potential radical
optimization in its straightforward combination with Federated Learning (in Section 4.2).
•	We propose a new learning framework, i.e., α-Weighted Federated Adversarial Training
(α-WFAT), to realize the relaxation of inner-maximization in FAT, which is simple and com-
patible with various Federated Learning or Adversarial Training methods (in Section 4.3).
•	We conduct extensive experiments to comprehensively understand the characteristics of the
α-WFAT, and confirm its effectiveness on improving the model performance for both IID
and Non-IID settings in the context of several federated optimization methods (in Section 5).
2	Related work
Federated Learning The representative work in Federated Learning is FedAvg (McMahan et al.,
2017), which has been proved effectiveness during the distributed training to maintain the data privacy.
To further address the heterogeneous issues, several optimization approaches have been proposed
e.g., FedProx (Li et al., 2018), FedNova (Wang et al., 2020b) and Scaffold (Karimireddy et al., 2020).
FedProx introduced a proximal term for FedAvg to constrain the model drift cause by heterogeneity;
FedNova proposed a general framework that eliminated the objective inconsistency and preserved
the fast convergence; Scaffold utilized the control variates to reduce the gradient variance in the
2
Under review as a conference paper at ICLR 2022
local updates and accelerate the convergence. MOON (Li et al., 2021a) alleviated the heterogeneity
by maximizing the agreement between the representation of the local model and that of the global
model, which helps correct the local training of individual parties. Reisizadeh et al. (2020) developed
a robust federated learning algorithm to against distribution shifts in clients samples. Our α-WFAT
introduces the relaxation into federated adversarial training, which is orthogonal to and compatible
with the previous optimization methods.
Adversarial Training As one of the defensive methods (Papernot et al., 2016; Gao et al., 2021),
Adversarial Training (Madry et al., 2018; Zhang et al., 2019; Jiang et al., 2020; Chen et al., 2021)
is to improve the robustness of machine learning models. The classical AT (Madry et al., 2018)
is built upon on a min-max formula to optimize the worst case, e.g., the adversarial example near
the natural example (Goodfellow et al., 2015). Zhang et al. (2019) decomposed the prediction
error for adversarial examples as the sum of the natural error and the boundary error, and proposed
TRADES to balance the classification performance between the natural and adversarial examples.
Wang et al. (2020c) further explored the influence of the misclassified examples on the robustness,
and proposed MART that emphasizes the minimization of the misclassified examples to boost the AT.
Zhang et al. (2020) investigated the “benign adversarial examples” in AT and further improve the
natural performance of robust model. In this work, our α-WFAT framework leverages the client-level
measure to alleviate the heterogeneous issue in the straightforward combination of adversarial training
and federated learning. It is compatible to further incorporate those centralized adversarial training
methods to improve the model performance.
Federated Adversarial Training. Recently, several works have made the exploration on the Ad-
versarial Training in the context of Federated Learning, which consider the data privacy and the
robustness in one framework. To our best knowledge, Zizzo et al. (2020) take the first trial to study
the feasibility of extending Federated Learning (McMahan et al., 2017) with the standard AT on both
IID and Non-IID settings. Empirically, they found that there was a large performance gap existing
between the distributed and the centralized adversarial training, especially on the Non-IID data. Shah
et al. (2021) designed a dynamic schedule for the local training to pursue a larger robustness under
the constrained communication budget of Federated Learning. Hong et al. (2021) explored how to
effectively propagate the adversarial robustness when only limited clients in Federated Learning have
the sufficient computational budget to afford AT. Although previous works have investigated to solve
the challenges about the constrained communication or computational budget, one critical issue that
affects the performance when combined Adversarial Training with Federated Learning has received
only few discussion. In this work, we consider such a basic issue (see Figure 1) in the straightforward
combination of AT with Federated Learning and introduce our solution to this problem.
3	Preliminary
In this section, we will briefly formalize the notations of Adversarial Training (Goodfellow et al.,
2015; Madry et al., 2018) and Federated Learning as well as FedAvg (McMahan et al., 2017).
3.1	Adversarial Training
Let (X, d∞) denote the input feature space X with the infinity distance metric d∞(x, X) = ∣∣x - X∣∣∞,
and Be[χ] = {X ∈ X | d∞(x, X) ≤ e} be the closed ball of radius > 0 centered at x in X. Dataset
S = {(xn, yn)}nN=1, where xn ∈ X and yn ∈ Y = {0, 1, ..., C - 1}. The objective function of the
standard adversarial training (AT) (Madry et al., 2018) is defined as follows,
1N
min - max Wθ (Xn),yn),
fθ ∈F N n=1 "n∈B∕xn]
(1)
where X is the most adversarial data within the e-ball centered at x, fθ(∙) : X → RC is a score
function, ' : RC ×Y → R is a composition of a base loss 'b : ∆C-1 × Y → R (e.g., the Cross-
Entropy loss) and an inverse link function 'l : RC → ∆C-1 (e.g., the Softmax). Here, ∆C-1
is the corresponding probability simplex that yields '(fθ(∙),y) = h&(于8(∙)),y). For the inner-
maximization of Eq. (1), the multi-step projected gradient descent (PGD) (Madry et al., 2018) is
usually applied to find the most adversarial samples, which are then used for the outer-minimization.
3
Under review as a conference paper at ICLR 2022
Class 1, Class 2
Adversarial data
Unseen data
Misclassified data
Decision boundary
Globally optimal
decision boundary
Figure 2: Left panel: locally learned decision boundary on Client A; Middle left panel: locally
learned decision boundary on Client B; Middle right panel: globally aggregated decision boundary
based on FedAvg; Right panel: globally aggregated decision boundary based on α-WFAT. Note that,
the distance between the correctly classified adversarial examples and the decision boundary (i.e.,
the bold line) can approximately reflect the client loss, and shows that 'client A > 'client B. Then,
selectively treating two client models in the aggregation can acquire a better global model (e.g., the
fourth panel), which is consistent with the intuition of Eq. (3).
3.2	Federated Learning
Let Dk denotes a finite set of samples from the k-th client, and in each round, a set of datasets
{Dk }kK=1 from K clients are involved into the training. The objective of Federated Learning is to
learn a machine learning model without any exchange of the training data between the clients and
the server. The current popular strategy, namely FedAvg, is introduced by McMahan et al. (2017),
where the clients collaboratively send the locally trained model weights θk to the server for the global
average aggregation. Concretely, each client runs on a local copy of the global model (parameterized
by θt in the t-th round) with its local data to optimize the objective like Eq. (1). Then, the server
receives their updated model weights {θkt }kK=1 of all clients and performs the following aggregation
1K
θt+1 = N X Nk θk,
k=1
(2)
where Nk denotes the number of the samples in Dk and N = PkK=1 Nk . Then, the weights θt+1 for
the global model will be sent back to each client for another lifecycle. After the sufficient rounds of
such a periodic synchronization and aggregation, we expect the stationary point of Federated Learning
will approximately approach to or have a small gap with that from the centralized counterpart.
4	α-WEIGHTED FEDERATED ADVERSARIAL TRAINING
In the following sections, we will first introduce our motivation for Federated Adversarial Training
through analyzing the challenges brought by the straightforward combination. Then, we relax the
inner-maximization problem of Adversarial Training as a lower bound by an α-weighted decompo-
sition, and present the theoretical understanding of such a relaxation. Finally, we will present the
α-Weighted Federated Adversarial Training and the corresponding analysis.
4.1	Motivation
For the general FAT (Zizzo et al., 2020), in each round, the clients will receive the latest model from
the server. Then, they conduct Adversarial Training with the local data and send their optimized
model parameters to the server. The server will aggregate the parameters of all clients into a global
model with FedAvg or other methods in Federated Learning. However, considering the characteristics
of Federated Learning and Adversarial Training, it naturally brings the following critical challenge.
The inner-maximization can enlarge the data heterogeneity in Federated Learning. In terms of the
combination of adversarial training and federated learning, the key point is that such adversarial
examples generated by the inner-maximization exacerbate the heterogeneity, which induces the
performance deterioration under the model aggregation methods. As the Figure 2 shows, the client A
with large adversarial loss might make a dominating effect on the convergence of the global model,
given the current model aggregation methods treat all client models indiscriminately. Thus, it might
be better to selectively weight the client models in the model aggregation.
4
Under review as a conference paper at ICLR 2022
4.2	α-WEIGHTED RELAXATION OF DECOMPOSED ADVERSARIAL TRAINING
As previous analysis, the inner-maximization of Adversarial Training is not very compatible with
Federated Learning due to the exacerbation of heterogeneity when combing them together. An idea
to alleviate this problem is building a mediating function that keeps the original goal but is friendly
to two paradigms. In this section, we present one possible solution to this intuition, which tries to
find a relaxation of the inner-maximization in Adversarial Training. Formally, we decompose the
inner-maximization objective into the independent K populations that corresponds to the K clients
in Federated Learning, and relaxes it into a lower bound by the α-weighted mechanism as follows,
1N
LAT = N X
n=1
max
xn ∈Be [xn]
'(f(Xn),yn
max
xn GBe [xn]
{^"
Lk
'(f(xn),yk)
Kb N	K	N	K
≥ (I + a) X N	Lφ(k) + (I -	a)	X N	Lφ(k)	s.t. α ∈	[0,	I),	Kb	≤ ɪ
k=1	k=Kb +1
=. Lα(Kb),
(3)
where φ(∙) is a function which maps the index to the original population sorted by {NkLk} in an
ascending order. The following theorems provide us more analysis about the α-weighted relaxation.
Theorem 4.1. Lα(K ) is monotonically decreasing w.r.t. both α and K , i.e., Lα1 (K ) < Lα2 (K )
.	一 _ _ z ^	.	^	.	.	^	^	..	.	..	一 -，^ .	.	.
if α1 > α2 and Lα(Kb1) < Lα(Kb2) if Kb1 > Kb2. Specifically, Lα(Kb) recovers L of adversarial
training when α achieves 0, and Lα(K ) relaxes L to a lower bound objective by increasing K and α.
We can flexibly emphasize the importance of partial populations by setting the proper hyperparameters,
alleviating the evenly averaging of harsh heterogeneous updates in FedAvg (McMahan et al., 2017).
Theorem 4.2. Assume the loss function '(∙, ∙) in Eq. (3) satisfies the Lipschitzian smoothness
condition w.r.t. the model parameter θ and the training sample x, and is λ-strongly concave for all
x, and E [∣∣VLα(K) 一 Vθ'(f(X), y)∖∖2^ ≤ δ2, where x is the adversarial example. Then, after the
sufficient T-step optimization i.e., T ≥ 骨,for α-Weighted relaxation ofdecomposed Adversarial
Training with the constant stepsize VZLTσ2 in PGD, we have the following convergence property,
1T
T X E
t
(4)
LL	α	α
where L = Lθθ + L<x defined by the Lipschitzian constraints, ∆ ≥ L (K)∣θo — infθ L (K) and
b
ξ(t) = Pk Nφ(k) 一 PKb +1 Nφ(k) meaning the accumulative counting difference of the t-th step.
When α = 0, Eq.(3) recovers the original loss of Adversarial Training, and the first part in the RHS of
Eq. (4) goes to 1 that recovers the convergence rate of Adversarial Training (Sinha et al., 2018). When
α → 1, Eq.(3) becomes more biased, while simultaneously the straightforward benefit is that we
can achieve a faster convergence in Eq. (4) if T PT ξ(I) < 0, i.e.,(1 + α 7T PN *()) < 1, Actually,
this is possible when the sample number is approximately similar among all clients and the top-1
choice easily has -N < ξ(t) = Nφ(t()1) 一 PkK=2 Nφ(t()k) < 0 in each optimization step. Especially, a
larger α has a faster convergence in this case. Therefore, the α-weighted relaxation of decomposed
Adversarial Training provides us a trade-off between maintaining the robustness from the standard
biased approximation by α and Kb .
robust training and achieving the faster convergence with some
4.3	α-WEIGHTED FEDERATED ADVERSARIAL TRAINING
Inspired by the previous analysis of α-weighted relaxation, we propose an α-Weighted Federated
Adversarial Training to combine Adversarial Training and Federated Learning. The intuition is
5
Under review as a conference paper at ICLR 2022
Algorithm 1 α-Weighted Federated Adversarial Training (α-WFAT)
Input: number of clients: K, number of communication rounds: T, number of client training epochs
per round: E, initial server’s model parameter: θ0 , hyper-parameter for aggregation: α, number
of enhanced clients: Kb ;
Output: a globally robust model with parameter θT ;
1:	for t = 0, ..., T 一 1 do
2:	Clients: [ perform adversarial training]
3:	for client k = 1, ..., K do
4:	θkt,Lk = AT(θkt, E) (Madry et al., 2018)
5:	end for
6:	Server: [ performs aggregation over weight updates]
7:	Lall J [NNL1, N2L2,..∙, NKLK]
8:	Lsorted J Ascending_Sort(Laii)	ʌ	ʌ
9:	∀k, Pk J (I + Q) ∙ 1( Nk Lk ≤ LsortedKD + (I- Q) ∙ 1( NN Lk > Lsorted[Kb ]);
10:	θt+1 = PK 1p N PK=I PkNkθk;	. α-weighted mechanism
k=1 Pk IN
11:	end for
applying the Q-weighted mechanism into the inner-maximization in FAT, formalized as follows,
min Lα-W F AT
min ---------
fθ∈F Pk PkNk
K	1 Nk
EPkNk ɪ max m max '(fθ(Xn),yn)
M	VNkn=I "B/""	/
X-------------{z-------------}
(5)
1
Lk
where Pk = (1 + α) ∙ 1(NNLk ≤ Lsorted[K]) + (1 - α) ∙ 1(NNLk > Lsorted[Kb]) denotes the weight
assigned to the k-th client based on the ascending sort of weighted client losses compared with the
Kb-th one. We summarize the procedure of Q-WFAT in Algorithm 1, which consists of multi-round
iterations between the local training on the client side and the global aggregation on the server side.
Concretely, on the client side, after downloading the global model parameter from the server, each
client will perform the Adversarial Training on its local data. At the same time, the client loss on the
adversarial examples is also recorded, which acts as the soft-indicator of the local bias induced by the
radical adversarial generation. Then, when the training steps reach to the condition, the client will
upload its model parameter and the loss to the server. On the server side, after collecting the model
parameters {θk}kK=1 and the losses {Lk}kK=1 of all clients, it will first sort the losses in an ascending
order to find the
top-Kb clients. Based
on that, the global model parameters will be aggregated by
^
the Q-weighted mechanism in which the model parameters of the top-K clients are upweighted with
(1 + Q) and the remaining is downweighted by (1 - Q). For some atypical layers (Li et al., 2021b)
e.g., BN, it is outside the scope of this paper and we keep the aggregation same as FedAvg. Note that,
one interesting point in Q-WFAT is the top-K clients with the higher weights are not fixed, and they
can be routing among all clients. In Figure 3, we trace this dynamic of Q-WFAT in one experiment.
In the following, we provide the theoretical analysis of Q-WFAT on the convergence in the context of
Federated Learning (Li et al., 2019), which is slightly different from previous centralized counterpart.
Theorem 4.3. Assume the loss function '(∙, ∙) in Eq. (5) is L-smooth and λ-strongly concave w.r.t.
the model parameter θ, and the expected norm and the variance of the stochastic gradient in each
client respectively satisfy E [∣∣Vθ[(f(Xk),yk)||2] ≤ ς2 andE [∣∣Vθ'(f(Xk),yk) — VθLk||2] ≤ δk.
Let K = L, Y = max{8κ, E} where E is the iteration numberofthe local Adversarial Training with
the learning rate η =。(匕).Then, after the sufficient T-step communication roundsfor α-WFAT,
we have the following asymptotics to the optimal point,
ElLa-WFAT] -L ≤ γ + t - ι (ɪ + -2E [∣∣θ0 - θ*U2]),	⑹
where L* is the minimum value of La-WFAT, θ* is the optimal model parameter, and
B=X (⅛) 2 (N δ) 2+6L --XX ⅛ N Lk!+8(E - 1)2ς2
6
Under review as a conference paper at ICLR 2022
5 I Il I Illll I I III I Il Il
4 I I I Illl I I III I III III I I I III
，Il III I Il Il I I Il Il
O	20	40	60	80	OT
Communication
Figure 3: The index of the top-K clients with the small losses in α-WFAT (α = 1/6, K = 1) in each
communication round on CIFAR-10. We can see that it is dynamically routing among all clients.
When α = 0, we have Pk(t) = 1 and Eq.( 6) becomes the convergence rate of FedAvg on non-IID
data (Li et al., 2019). Different from Theorem 4.2 that concludes in the centralized training setting,
when α → 1, the convergence is indefinite compared to the standard FAT, since the emerging terms in
B, i.e.,(」k
1+α
T)	2	P (T)
(T)y	and ——k^r), are acted as the scalar timing by the personalized variance bound
N )	1+α N
δk and the local optimum Lk of each client. One possible case is when the optimization approaches
to the optimal parameter θ*, δ2 can be in a smaller scale relative to the scale of Ll. In this case, the
increment of the first term of B can be totally counteracted by the loss of second term of B so that in
sum B becomes smaller. Then, we can have a tighter upper bound for α-WFAT in Eq.( 6) to achieve
a faster convergence than FAT. The completed proof of Theorem 4.3 can refer to Appendix C and the
experiments in the following section will confirm α-WFAT can reach to a more robust optimum.
5	Experiments
In this section, we will provide an in-depth analysis of α-WFAT and empirically verify its efficiency
compared with the current methods on a range of IID and non-IID datasets.
5.1	Experimental Setup
Dataset. We conduct the experiments on three benchmark datasets, i.e., CIFAR-10, CIFAR-
100 (Krizhevsky, 2009) and SVHN (Netzer et al., 2011) for Federated Adversarial Training. For
the IID scenario, we just randomly and evenly distribute the samples to each client. For the Non-
IID scenario, we follow McMahan et al. (2017); Shah et al. (2021) to partition the training data
based on their labels. To be specific, a skew parameter s is utilized in the data partition introduced
by Shah et al. (2021), which enables K clients to get a majority of the data samples from a subset
of classes. We denote the set of all classes in a dataset as Y and create Yk by dividing all the class
labels equally among K clients. Accordingly, we split the data across K clients that each client has
(100 - (K - 1) × s)% of data for the class in Yk and s% of data in other split sets. The detailed
training and evaluation settings can refer to Appendix E.
5.2	Ablation Study
In this part, we conduct various experiments on CIFAR-10 to visualize the characteristics of α-WFAT.
Non-AT vs. AT. In the left two panels of Figure 4, we respectively apply our α-weighted mechanism
to Federated Standard Training (α-WFST) and Federated Adversarial Training (α-WFAT) under
Kb = 1. We also consider both FedAvg and FedProx in this experiment to guarantee the universality.
From the curves, we can see that α-WFST has the negative effect on the natural accuracy, while
α-WFAT consistently improve the robust accuracy both based on FedAvg or FedProx. This indicates
our α-weighted mechanism is tailored for the inner-maximization of Federated Adversarial Training
instead of the outer-minimization considered by other federated optimization methods.
Impact of α and Kb . To study the effect of hyperparameter in α-WFAT, we conduct several ablation
experiments to verify the model performance. Regarding the experiments of α, we set the client
7
Under review as a conference paper at ICLR 2022
Aɔe.lnɔzra-lnHN
O 20	40	60	80	1∞
Communication
3530252015
AORJno。WSnqO£t
0	20	40	60	80	1∞
Communication
0 5 0 5 0 5
6 5 5 4 4 3
AOeJnOS
6055504540353025
^osτ-oo<
Figure 4: Ablation study on α-WFAT. Left two panels: comparison between Federated Standard
Training and Federated Adversarial Training respectively in combination with the α-weighted mecha-
nism, i.e., (α-WFST) vs. (α-WFAT). Right two panels: the natural accuracy and the robust accuracy
of α-WFAT with different α
and different Kb on CIFAR-10.
number K
5 and Kb
1 to upweight/downweight the client models in each communication rounds.
The right middle panel of Figure 4 shows α ∈ (0, 1/6] can significantly improve the robust accuracy
and the natural accuracy, while a larger α might be inappropriate to the natural accuracy. Regarding the
choice of K , we specially set K = 8 in this experiment to span the range of K due to the constraint
K <= K/2. The right panel of Figure 4 presents the accuracy of α-WFAT with increasing K .
According to the plot, both the natu-
ral accuracy and the robust accuracy are
improved even with larger Kb , which
shows the effect of Kb on the relax-
ation of inner-maximization in the pro-
posed α-WFAT. In addition, we conduct
the experiments about emphasizing/de-
emphasizing the client with smallest ad-
versarial loss by adjusting the α, the re-
sults (in Appendix E.1) confirmed that the
relaxation introduced by our α-WFAT is
needed to improve performance, and is
consistent with our previous analysis.
Table 1: Test accuracy (%) on CIFAR-10 (Non-IID) with
different Adversarial Training methods on the local client.
Methods		Natural	PGD-20	CW∞
AT	FAT	57.45%	32.58%	30.52%
	α-WFAT	62.34%	35.59%	33.06%
TRADES	FAT	64.00%	31.64%	28.95%
	α-WFAT	65.26%	35.10%	31.80%
MART	FAT	56.29%	36.27%	32.41%
	α-WFAT	58.41%	38.90%	34.67%
Different AT methods. In Table 1, we
validate the combination of α-weighted mechanism and different Adversarial Training methods (i.e.,
TRADES (Zhang et al., 2019) and MART (Wang et al., 2020c)), where we switch different local
Adversarial Training methods on the client side. Through the comparison with FAT, the results show
that α-WFAT can consistently improve both the natural performance and the robust performance, and
is general to the state-of-the-art Adversarial Training works under Federated Learning scenarios.
5.3 Performance Evaluation
In this section, we compare our α-WFAT with FAT on various benchmark datasets to demonstrate its
effectiveness. Specifically, we validate both the non-IID and IID settings with three represenative
Federated optimization methods i.e., FedAvg, FedProx and Scalffold. Besides, a centralized AT
baseline is provided as the reference of the IID setting. Note that, there is no such a baseline for the
non-IID setting, since the centralized case is one distribution which is incomparable and meaningless.
Considering the sensitivity of data selection in Non-IID settings, we also report the results with Mean
and Std values in Appendix E.2 after running experiments for multiple times.
According to Table 2 on CIFAR-10, we can find that α-WFAT significantly outperforms FAT on the
Non-IID data in terms of both the natural accuracy (〜2%-6%) and the robust accuracy (〜2%-5%).
For the IID data, our method acquires a similar improvement on the robust accuracy without the
deterioration of the natural accuracy. The reason might be because even the data is IID, Adversarial
Training can still drive the independently-initialized overparameterized network (Allen-Zhu & Li,
2020) on each client side towards at the robust overfitting of different directions, yielding the model
heterogeneity. Thus, the proper relaxation to the inner-maximization makes Adversarial Training more
compatible with Federated Learning. Another interesting observation is that Federated Adversarial
Training shows better performance than centralized Adversarial Training in the case of the IID setting.
This gain could be from the distributed training paradigm that helps Adversarial Training converge
8
Under review as a conference paper at ICLR 2022
Table 2: Performance on three benchmark datasets under different federated optimization methods.
											
Setting		Non-IID					IID				
CIFAR-10		Natural	FGSM	PGD-20	CW∞	AA	Natural	FGSM	PGD-20	CW∞	AA
Centralized AT		-	-	-	-	-	66.47%	47.68%	38.18%	37.04%	34.48%
FedAvg	FAT	57.45%	39.44%	32.58%	30.52%	29.20%	69.35%	48.45%	37.43%	35.72%	33.96%
	α-WFAT	63.44%	45.13%	37.17%	33.99%	32.36%	67.43%	50.33%	42.78%	37.91%	36.20%
FedProx	FAT	60.44%	41.59%	33.84%	31.29%	30.02%	66.91%	46.70%	37.14%	34.54%	32.68%
	α-WFAT	62.51%	44.29%	36.75%	33.82%	31,98%	68.31%	48.40%	42.41%	37.25%	35.97%
Scaffold	FAT	62.81%	43.61%	34.13%	32.53%	30.95%	68.27%	49.25%	39.33%	37.31%	35.30%
	α-WFAT	64.12%	46.05%	37.35%	34.78%	33.32%	71.36%	50.42%	43.83%	39.12%	35.47%
CIFAR-100		Natural	FGSM	PGD-20	CW∞	AA	Natural	FGSM	PGD-20	CW∞	AA
Centralized AT		-	-	-	-	-	35.81%	23.09%	18.64%	16.48%	15.42%
FedAvg	FAT	31.07%	19.60%	16.16%	13.37%	12.47%	38.35%	23.37%	18.44%	16.63%	15.45%
	α-WFAT	35.17%	21.26%	16.72%	13.91%	12.83%	38.43%	23.76%	18.82%	16.71%	15.62%
FedProx	FAT	33.33%	20.20%	16.08%	13.76%	12.72%	37.18%	22.29%	18.16%	16.33%	15.29%
	α-WFAT	34.30%	20.82%	16.74%	13.84%	12.88%	37.37%	23.11%	18.43%	16.36%	15.45%
Scaffold	FAT	41.17%	25.17%	20.01%	16.74%	15.49%	42.42%	26.79%	21.18%	18.89%	17.63%
	α-WFAT	41.07%	25.55%	20.40%	16.79%	15.59%	42.08%	27.18%	22.26%	19.34%	18.03%
SVHN		Natural	FGSM	PGD-20	CW∞	AA	Natural	FGSM	PGD-20	CW∞	AA
Centralized AT		-	-	-	-	-	92.39%	89.75%	72.73%	72.31%	70.93%
FedAvg	FAT	91.24%	87.95%	68.87%	67.89%	66.54%	93.52%	90.68%	72.24%	71.22%	70.08%
	α-WFAT	91.25%	88.28%	71.72%	69.79%	68.62%	92.75%	90.06%	74.37%	72.34%	71.27%
FedProx	FAT	90.92%	87.50%	68.44%	67.18%	65.94%	93.54%	90.66%	72.53%	71.42%	70.21%
	α-WFAT	91.25%	88.15%	71.54%	69.53%	68.47%	93.59%	90.80%	74.66%	72.67%	71.48%
Scaffold	FAT	89.95%	87.23%	68.66%	67.23%	66.65%	93.80%	91.00%	73.26%	72.05%	70.80%
	α-WFAT	90.20%	87.81%	71.39%	68.81%	67.88%	93.92%	91.28%	75.96%	74.05%	72.88%
to the more robust optimum by the divide-and-conquer mechanism. This might enlighten the more
explore in Adversarial Training to improve the model robustness via Federated Learning.
On CIFAR-100 and SVHN, we can find the similar improvement in Table 2 as that of CIFAR-10 under
three types of federated optimization methods. Nevertheless, α-WFAT only becomes superior in
terms of the robust accuracy but comparable with FAT in terms of the natural accuracy especially
on CIFAR-100. It indicates that the inner-maximization of Adversarial Training when combined
with Federated Learning mainly affects the model robustness, and thus the α-weighted relaxation
correspondingly helps the model converge to a more robust optimum.
6	Conclusion
In this work, we explore the performance deterioration in the straightforward combination of Adver-
sarial Training with Federated Learning. To alleviate the potential radical optimization, we apply
an α-weighted relaxation into Adversarial Training to relax the inner-maximization. Based on this
α-weighted mechanism, we further propose α-Weighted Federated Adversarial Training (α-WFAT).
We provide the theoretical analysis and empirical evidences to understand the proposed simple but
effective method. The experimental results under different settings confirm the effectiveness of
α-WFAT. Nevertheless, we only move a small step on the heterogeneous issue in the combination of
two paradigms and more issues in their cross field could be further explored in the future.
9
Under review as a conference paper at ICLR 2022
7	Ethics Statement
This paper does not raise any ethics concerns. This study does not involve any human subjects,
practices to data set releases, potentially harmful insights, methodologies and applications, potential
conflicts of interest and sponsorship, discrimination/bias/fairness concerns, privacy and security
issues, legal compliance, and research integrity issues.
8	Reproducibility Statement
To ensure the reproducibility of experimental results, we will provide a link for an anonymous
repository about the source codes of this paper in the discussion forums.
References
Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and
self-distillation in deep learning. arXiv preprint arXiv:2012.09816, 2020.
Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. In
Symposium on Security and Privacy (SP), 2017.
Tianlong Chen, Sijia Liu, Shiyu Chang, Yu Cheng, Lisa Amini, and Zhangyang Wang. Adversarial
robustness: From self-supervised pre-training to fine-tuning. In CVPR, 2020.
Tianlong Chen, Zhenyu Zhang, Sijia Liu, Shiyu Chang, and Zhangyang Wang. Robust overfitting
may be mitigated by properly learned smoothening. In ICLR, 2021.
Ruize Gao, Feng Liu, Jingfeng Zhang, Bo Han, Tongliang Liu, Gang Niu, and Masashi Sugiyama.
Maximum mean discrepancy test is aware of adversarial attacks. In ICML, 2021.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In ICLR, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In CVPR, 2016.
Junyuan Hong, Haotao Wang, Zhangyang Wang, and Jiayu Zhou. Federated robustness propagation:
Sharing adversarial robustness in federated learning. arXiv preprint arXiv:2106.10196, 2021.
Ziyu Jiang, Tianlong Chen, Ting Chen, and Zhangyang Wang. Robust pre-training by adversarial
contrastive learning. In NeurIPS, 2020.
Peter Kairouz, H Brendan McMahan, Brendan Avent, AUrelien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Ad-
vances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In
ICML, 2020.
Ron Kohavi, David H Wolpert, et al. Bias plus variance decomposition for zero-one loss functions.
In ICML, 1996.
Alex Krizhevsky. Learning multiple layers of features from tiny images. In arXiv, 2009.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv
preprint arXiv:1611.01236, 2016.
Qinbin Li, Bingsheng He, and Dawn Song. Model-contrastive federated learning. In CVPR, 2021a.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 2018.
10
Under review as a conference paper at ICLR 2022
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of
fedavg on non-iid data. In ICLR, 2019.
Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. Fedbn: Federated learning
on non-iid features via local batch normalization. arXiv preprint arXiv:2102.07623, 2021b.
Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. In ICLR, 2014.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In ICLR, 2018.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In AISTATS, 2017.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. In NeurIPS Workshop on Deep
Learning and Unsupervised Feature Learning, 2011.
Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a
defense to adversarial perturbations against deep neural networks. In 2016 IEEE symposium on
security and privacy (SP), 2016.
Amirhossein Reisizadeh, Farzan Farnia, Ramtin Pedarsani, and Ali Jadbabaie. Robust federated
learning: The case of affine distribution shifts. In NeurIPS, 2020.
Devansh Shah, Parijat Dube, Supriyo Chakraborty, and Ashish Verma. Adversarial training in
communication constrained federated learning. arXiv preprint arXiv:2103.01319, 2021.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with
principled adversarial training. In ICLR, 2018.
Haotao Wang, Tianlong Chen, Shupeng Gui, Ting-Kuei Hu, Ji Liu, and Zhangyang Wang. Once-
for-all adversarial training: In-situ tradeoff between robustness and accuracy for free. In NeurIPS,
2020a.
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective
inconsistency problem in heterogeneous federated optimization. In NeurIPS, 2020b.
Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan Gu. On the
convergence and robustness of adversarial training. In ICML, 2019.
Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving
adversarial robustness requires revisiting misclassified examples. In ICLR, 2020c.
Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust general-
ization. In NeurIPS, 2020.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan.
Theoretically principled trade-off between robustness and accuracy. In ICML, 2019.
Jingfeng Zhang, Xilie Xu, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and Mohan Kankan-
halli. Attacks which do not kill training make adversarial learning stronger. In ICML, 2020.
Yuchen Zhang, John C Duchi, and Martin J Wainwright. Communication-efficient algorithms for
statistical optimization. The Journal of Machine Learning Research, 14(1):3321-3363, 2013.
Giulio Zizzo, Ambrish Rawat, Mathieu Sinn, and Beat Buesser. Fat: Federated adversarial training.
arXiv preprint arXiv:2012.01791, 2020.
11
Under review as a conference paper at ICLR 2022
Appendix
A Proof of Eq. (3) and Theorem 4.1
We proof the Eq. (3) and Theorem 4.1 in this section.
Recall the α-weighted relaxation for the inner-maximization objective decomposition with K inde-
pendent populations as follows,
1 N	K N 1 Nk
-N n= J xn ∈Be [xn]'(f (xn),yn) = X Nk (MXxn∈Be[xn]
一 '-----------------{z--------------}
Lk
Kb K	(7)
≥ (I + α) X φ(k)LLφ(k) + (I - α) X -k)LLφ(k)	s.t. α ∈ [0, I), KK ≤ y
k=1	k=Kb +1
=. Lα(Kb ),
where φ(∙) is a function which maps the index to the original population group sorted by {NkLk} in
an ascending order.
proof of Eq. (3). The deduction of the inequality in Eq. (7) can be formulated in the following. Given
α ∈ [0,1) and K ≤ 与 with the population sorted by { NNk Lk} in an ascending order, We have
b
Pk=I Nk) Lφ(k) ≤ Pk=K +ι Nk) Lφ(k). Then, We have the following relationship by subtraction,
Kb K	Kb K
X NφkLφ(k) +	X Nφ(k) Lφ(k)	- (1 + α)	X Nφ(k) Lφ*) -	(1 - α)	X Nφ(k)	L。®
N φ(k)	N φ(k)	N φ(k)	N φ(k)
k=1	k=Kb +1	k=1	k=Kb +1
=α ∙( X ^^NNk)Lφ(k) - X -ɪ-Lφ(k) ≥ 0.	⑻
k=Kb +1	k=1
□
proof of Theorem 4.1. It can be naturally proved by Eq. (8). If α1 > α2, then we have,
Kb N	K N
LaI(K) - La2 (K) = (1 + αI) X N Lφ(k) + (I - α1) X N Lφ(k)
k=1	k=Kb +1
Kb	K
-(1 + α2) X -ɪɪLφ(k) - (I - α2) X -ɪɪLφ(k)	⑼
k=1	k=Kb +1
= (αι- α2) (X N≡LMk)- X Nφ(k)LMk)] ≤ 0
1	2	N	φ(k)	N	φ(k)
k=1	k=Kb +1
Similarly, we can prove La(Kι) < La(K2) if K1 > K2.	□
B Proof of Theorem 4.2
Based on the convergence of Adversarial Training (Sinha et al., 2018), we proof Theorem 4.2 in this
section.
proof of Theorem 4.2. Let c : X × X → R+ ∪ {∞}, where c(x, x0) is the “cost” for an adversary to
perturb x0 to x. Let f(θ, x; x0) = `(θ; x) - γc(x, x0), noting that the gradient steps is preformed as
12
Under review as a conference paper at ICLR 2022
χt), where X is an approximate maximizer of f (仇 x; χt) in x, and θt+1
亡 in the rest of the proof, which is satisfied for the constant step size μ
gt = V0 f (θt,X;
We assume μt ≤
and T ≥ l⅛Δl .
we have
θt - μtgt.
/ ∆L
V Lψ Tδ2
By a Taylor expansion using the Lψ-smoothness of the objective Lk for k-th client,
Lk∣θt 十1	≤	LkJ + <VLk"θt+1 -	θt>	+	Lψ I∣θt+1 -	θt∣∣2	(10)
=Lk I θt -μt∣∣VLk I θt∣∣2 + 宇 I∣gt∣∣2 + μt (VLk ∣ /, VLk I 3-g∖
=Lki 仍一μt (I- 2Lψμ2) ∣∣VLk i θt∣∣2
+ μt(l - Lψμ)(VLk 1 θt, VLk 1 θt - gt) +-ψ2^- ∣∣gt - VLk I θt ∣∣2
Consider the function φ7(θ; x0) = SUPxeZ f (θ,x; x0), we define the potentially biased errors
Zt = gt - Vθφγ(θt; xt). Then we have the following relationship,
Lk 1 θt+ι ≤Lk | θt - μt (I- 2 LΨ μ2) ∣∣VLk 1 θt ∣∣2	(II)
+ μt(l - Lψμ)(VLk ∣ θt, VLk ∣ θt - vθφγ(θt; xt))
-μt(I- Lψμt)〈VLk | θt,ζt) +—ψ2^-∣∣vθφγ(θt;xt) + Z2 -VLk i θt∣∣2
=Lk i θt - μt (I- 2Lψμ2) ∣∣VLk i θt∣∣2
+ μt(I- Lψμ)(VLk I θt, VLk I θt - vθφγ(θt; xt))
-μt(I- Lψμt)〈VLk I θt, ζt)
+ 粤(∣∣Zt∣∣2 + ∣∣Vθφγ(θt;xt) - VLk I θt∣∣2 + 2 (Vθφγ(θt;xt) - VLk I θt,ζt>).
Since ±(a, b) ≤ ɪ (∣∣α∣∣2 + ∣∣b∣∣2), We have
Lk 1 θt+ι ≤Lk 1 θt -与 ∣∣VLk 1 θt ∣∣2 + μt ((I- Lψ α))〈 VLk 1 θt, VLk 1 θt - vθ φγ (θt; xt))	(12)
+ “t((I* L μ" ∣∣Z ∣∣2 + Lψ M2∣∣Vθ φγ (θt; xt) - VLk I θt ∣∣2.
Then, letting x： = arg maxx f (θt, x; xt), the error Zt satisfies,
∣∣Z ∣∣2 =∣∣Vθ φγ (θt; xt) - Vf (θ,xt; xt)∣∣2 = ∣∣Vθ '(θ,x) -Vθ '(θ,xt)∣∣2	(i3)
2L2
≤ Lθχ∣∣x - x*∣∣2 ≤ λ X e,	(14)
where the final inequality utilize the λ = Y - Lxx strong-concavity of x → f (θ, x; xo). For conve-
2L2
nience, let e = 7_/ 匕 Taking conditional expectations in Eq. (12) and using E [Vθφ7(θt; xt)∣θt]=
VLk i θt, we have,
E [Lk ∣ θt+ι - Lk ∣ θt∣θt] ≤-μ∣∣VLk ∣ θt∣∣2 + 〃'((I+2Lψ〃"^ + Lψμ2∣∣Vθφγ(θt;xt) - VLk ∣ >∣∣2
(15)
≤ -μ∣∣VLk ∣ θt∣∣2+ μt"Lψμ2∣∣vθφγ(θt;Xt)-VLk | θt∣∣2
Since μt ≤ 看,taking a fixed step size μ, we have,
2
E ∣∣VLk ∣ θt∣∣2 - 2 ≤ -E Lk ∣ θt - Lk I θt+ι + 2Lψμδ2	(16)
μ
13
Under review as a conference paper at ICLR 2022
Because E [∣∣Vθφγ(θ; Z)- VLkIM|同
≤ δ2, summing over t, we have,
T
1T	2
T y^E IjIVLk lθtk] - 2e ≤ μτ (Lk Iθ0 - E[Lk lθT ]) + 2Lψμδ
≤ μτ + 2Lψ μδ2
Since μ = JDT^, and λ = Y 一 Lxx, We can get the following result,
(17)
T XXE [∣∣VLk∣θtII2] ≤ R +4δ产
t
(18)
Adopting our α-weighted relaxation, we have,
TX E"11VLa(K H
T	ru	Kb N(t)	K
=τXE II(I+a)X-ɪɪvLΦ(k)ιθt + (1—α) X
t	k=1	k=Kb +1
N(t)
Nφ(k)
~N~
VLφ(k) IIθt IIIIII
2
2
whereξ(t) = PkKb=1 Nφ(t()k) - PkK=Kb+1 Nφ(t()k) to simplify the notations.
(19)
□
C Proof of Theorem 4.3
Based on the convergence of FedAvg (Li et al., 2019), we proof Theorem 4.3 in this section.
First, we make the following assumptions and present some useful lemmas. Specifically, we make
the following assumptions. Assumption C.1 and C.2 are standard (typical examples are the '2-norm
regularized linear regression, logistic regression, or softmax classifier). Assumption C.3 and C.4 have
been made by the previous works (Zhang et al., 2013; Li et al., 2019).
Assumption C.1. L1, . . . , LK are all L-smooth: for all v and w, Lk (v) ≤ Lk (w) + (v -
W)TVLk(W) + lI∣v ― w||2.
Assumption C.2. L1, . . . , LK are all λ-strongly convex: for all v and w, Lk (v) ≥ Lk (w) + (v -
w)tVLk(w) + λI∣v 一 w||2.
Assumption C.3. Let ξtk be sampled from the k-th device’s local data uniformly at random. The
variance of stochastic gradients in each device is bounded: E||VLk (Wtk, ξtk) 一 VLk (Wtk)||2 ≤ δk2
for k = 1,…，K.
Assumption C.4. The expected squared norm of stochastic gradients is uniformly bounded, i.e.,
E∣∣VLk(wk,ξk)∣∣2 ≤ ς2 forall k = 1,…，K andt = 1,…，T — 1.
14
Under review as a conference paper at ICLR 2022
We use the following lemmas proved by Li et al. (2019). Let θtk denotes the model parameter main-
tained in the k-th client at t-th step, Θ represents an immediate result of one step SGD update from
θk. For convenience, We define ΘΘt = Pk=I 警Θt, & == PK=I 警θt, gt = PK=I Nk VLk (θk)
and gt = PK=I NkVLk(,θ1k, ξ?). Therefore, Egt = gt.
Lemma C.1 (Results of one step SGD). Assume Assumption C.1 and C.2. If η ≤ 4L, we have
E∣∣Θt+1 - θ*∣∣2 ≤ (1 - ηtλ)E∣Bt - Θ*ll2	(20)
+ η2E∣∣gt - gt∣∣2 + 6Lη2r + 2EX N||& - θk||2,
k=1
where Γ = L- PK EtNkLk ≥ 0
Lemma C.2 (Bounding the variance). Assume Assumption C.3. It follows that
E||gt - gt∣l2 ≤ X (NT-) δk,	QI)
k=1 N
Lemma C.3 (Bounding the divergence of θtk). Assume Assumption C.4, that ηt is non-increasing
and η ≤ 2ηt+E for all t > 0. It follows that
K
E X Nk∣∣θt-θkIl2	≤ 4η2(E - I)2ς2
(22)
ProofofTheorem 4.3. Let ∆t = E∣∣θt — θ*∣∣2. From Lemma C.1, Lemma C.2 and Lemma C.3, it
folloWs that
∆t+1 ≤ (1 - ηtλ)∆t + ηt2B,
(23)
and the local optimum Lk of each client.
For a diminishing stepsize, η = γ+t for some β > λ and Y > 0 SUCh that ηι ≤ min{ ɪ, 4L } = ±
and ηt ≤ 2ηt+E. We will prove that ∆t ≤ γνt, where V = max{ ；；—、, (γ + 1)∆ι}. The above
can be proved by induction. Firstly, the definition of ν ensures that it holds for t = 1. Assume the
conclusion holds for some t, it follows that,
∆t+1 ≤ (1 - ηtλ)∆t + ηt2B
≤(1-)ɪ + β2B
— t + Yt + γ	(t + γ)2
_ t + γ - 1 β2B βλ - 1	(25)
=7t+τFV + [Wτ7 - Wτ7ν ]
V V
≤ t + Y + 1.
Then by the L-smoothness of L(∙),
E[Lt] -l*≤ ILZ ≤ Lγ+t	(26)
Specifically, if we choose β = 2, γ = maχ{8 L ,E} 一 1 and denote K = L, then η = 2 /.One
can verify that the choice ofηt satisfies ηt ≤ 2ηt+E for t ≥ 1. Then we have
V = max[	Ae	Bl ,	(Y	+ 1)δi] ≤	Ae	Bl +(Y + 1)δi	≤	4BB	+ (Y +1)δi,	(27)
βλ - 1	βλ - 1	λ2
15
Under review as a conference paper at ICLR 2022
and
E[Lt] -L ≤ L^V- ≤
It	2 Y +1 - γ +1
2B λ(γ +1)
(天 + ^^δi
(28)
□
κ
D Learning Framework and Algorithm
Figure 5: A brief illustration of our α-Weighted Federated Adversarial Training (α-WFAT) framework.
On the client-side, each client will conduct adversarial training with its local data and update the
optimized model parameter (i.e., θk) with the adversarial loss (i.e., NLk)). On the server-side, after
collecting the model parameters and the loss value (information of the robustness), the server will
conduct an ascending sort and aggregate the global model with a weighted average (denoted by )
which upweights the top populations of the robust client’s model parameters with α.
Here, we provide an intuitive illustration of our proposed α-WFAT in Figure 5. Based on the α-
weighted mechanism, we provide a new flexible framework for the combination of adversarial training
with federated learning. It is orthogonal to a variety of different adversarial training (Zhang et al.,
2019; Wu et al., 2020; Zhang et al., 2020; Chen et al., 2020; 2021; Wang et al., 2020a; Jiang et al.,
2020; Wang et al., 2020c) methods and federated learning algorithms (Li et al., 2018; Kairouz et al.,
2019; Hong et al., 2021) which gain the adversarial robustness or alleviate the data heterogeneity on
the client side, and can be simply but effectively combined with other approach.
E Experimental Details
Dataset. We conduct the experiments on three benchmark datasets, i.e., SVHN (Netzer et al., 2011),
CIFAR-10 and CIFAR-100 (Krizhevsky, 2009) for federated adversarial training. For the IID scenario,
we randomly distribute these datasets to each client. For simulating the Non-IID scenario, we
follow McMahan et al. (2017); Shah et al. (2021) to distribute the training data based on their labels.
To be specific, a skew parameter s is utilized in the data partition introduced by Shah et al. (2021),
which enables K clients to get a majority of the data samples from a subset of classes. We denote the
set of all classes in a dataset as Y and create Yk by dividing all the class labels equally among K
clients. Accordingly, we split the data across K clients that each client has (100 - (K - 1) × s)%
of data for the class in Yk and s% of data in other split sets. In our experiments, we set s = 2 for
simulating the Non-IID partition with 5 clients as Shah et al. (2021) recommended.
Training and Evaluation. In the experiments, we follow the previous works to leverage the same
architectures, i.e., NIN (Lin et al., 2014) for CIFAR-10, ResNet-18 (He et al., 2016) for CIFAR-100 and
Small CNN (Zhang et al., 2020) for SVHN. For the local training batch size, we set 32 for CIFAR-10,
128 for CIFAR-100 and SVHN. For the training schedule, SGD is adopted with 0.9 momentum for
100 communication rounds under 5 clients as in (Hong et al., 2021; Shah et al., 2021), and the weight
decay = 0.0001. For adversarial training, we set the configurations of PGD respectively (Madry et al.,
2018) for different datasets. On CIFAR-10/CIFAR-100, we set the perturbation bound = 8/255,
the PGD step size 2/255 and set the PGD step number 10. On SVHN, we set the perturbation
bound = 4/255, the PGD step size 1/255, and keep the same step number 10. Regarding the
evaluation, the accuracy for the natural test data and that for the adversarial test data are computed
16
Under review as a conference paper at ICLR 2022
Table 3: Brief summary of the experimental details about α-WFAT
Dataset	Network	∣	Lχj	IJK	I α
CIFAR-10	NIN(Shahetal., 2021)	∣	rɪj	ΓH	I 1/6
CIFAR-100	ReSNet-18 (Chenetal., 2021) ∣	5	1	I 1/41
SVHN	SmallCNN (Zhang et al., 2019) ∣	5	1	I 1/11
following Wang et al. (2019); Wu et al. (2020). Note that, the adversarial test data are generated by
FGSM, PGD-20, C&W (Carlini & Wagner, 2017) attack with the same perturbation bound and step
size as the training. All the adversarial generations have a random start, i.e, the uniformly random
perturbation of [-, ] added to the natural data before attacking iterations. Besides, we also report
the robustness under a stronger AutoAttack, termed as AA for simplicity.
As for our α-WFAT, different training tasks adopt different α-weighted mechanism considering
different characteristic of local training data, we set α = 1/6 (i.e., 1+α = 1.4) for the experiments
on CIFAR-10, and α = 1/41 (i.e., 1-α = 1.05) for the experiments on CIFAR-100 and α = 1/11
(i.e., I-α = 1.2) for the experiments on SVHN. As for FedProx, We set μ = 0.01 for each dataset
and its α for α-weighted mechanism are 1/11, 1/41, and 1/11. As for Scaffold, the α adopted for
previous datasets are 1/11, 1/101 and 1/11 respectively.
As for the choice of the hyper-parameter α, one useful way to set it might be progressively probing
its effect in a value-growth manner. When the α is very small, the objective will approximately
degenerate the original objective of FAT, so does the performance with no harm. Slightly enlarging α
can improve the performance due to the benefit of the bias-variance trade-off, and then make a stop
in one point where the performance becomes drop.
E.1 EMPHASIZE/DE-EMPHASIZE IN OUR α-WFAT
We conduct an empirical comparison between α-WFAT that emphasizes (relatively de-emphasize
the other clients) the client model with the smallest loss and a contrary variant that de-emphasizes
it (relatively emphasize the other clients) as follows. We find that de-emphasizing the client with
smallest loss (relatively emphasize those with larger loss) consistently harm the model performance
across these evaluations.
Table 4: Comparison with emphasize/de-emphasize the client with smallest loss.
Setting			Non-IID			
CIFAR-10			Natural	FGSM	PGD-20	CW∞
	α-WFAT: 1+α = 1.4	emphasize	63.44%	45.13%	37.17%	33.99%
	1-α α-WFAT: 1+α = 1.2	emphasize	62.26%	44.08%	35.83%	33.31%
FedAvg	1-α	. FAT: 1+α = 1.0	original	57.45%	39.44%	32.58%	30.52%
	1-α α-WFAT: 1+α = 0.8	de-emphasize	50.45%	34.34%	27.86%	26.62%
	1-α	. α-WFAT: 1+α = 0.6 1-α	de-emphasize	40.47%	28.81%	24.36%	23.19%
SVHN			Natural	FGSM	PGD-20	CW∞
	α-WFAT: 1+α = 1.4	emphasize	90.60%	87.75%	73.12%	70.51%
	1-α α-WFAT: 1+α = 1.2	emphasize	91.25%	88.28%	71.72%	69.79%
FedAvg	1-α	. FAT: 1+α = 1.0	original	91.24%	87.95%	68.87%	67.89%
	1-α α-WFAT: 1+α = 0.8	de-emphasize	90.03%	86.12%	64.35%	64.32%
	1-α	. α-WFAT: 1+α = 0.6 1-α	de-emphasize	89.46%	84.80%	58.64%	58.96%
17
Under review as a conference paper at ICLR 2022
E.2 Mean and Std results of the Non-IID settings
Considering that the Non-IID results are sensitive to the selection of data in each client, we conduct
our experiments on Non-IID settings for multiple times and conclude the results as follows. In
summary, our α-WFAT can consistently improve the model robustness with comparable or even
better natural performance than previous federated optimization methods.
Table 5: Performance on Non-IID settings under different federated optimization methods
(Mean±Std).
Setting			Non-IID				
CIFAR-10		Natural	FGSM	PGD-20	CW∞	AA
FedAvg	FAT α-WFAT	58.13±0.68% 63.36±0.07%	40.06±0.62% 44.82±0.32%	32.56±0.01% 37.14±0.03%	30.88±0.37% 33.39±0.61%	29.17±0.03% 31.66±0.70%
FedProx	FAT α-WFAT	59.95±0.45% 62.04±0.47%	41.44±0.15% 44.21±0.08%	33.83±0.01% 36.64±0.11%	31.65±0.36% 32.62±0.20%	30.11±0.09% 31.83±0.15%
Scaffold	FAT α-WFAT	61.44±1.37% 63.16±0.96%	42.85±0.76% 45.55±0.50%	34.08±0.05% 37.33±0.02%	32.56±0.02% 34.82±0.04%	31.03±0.08% 33.32±0.01%
CIFAR-100		Natural	FGSM	PGD-20	CW∞	AA
FedAvg	FAT α-WFAT	31.93±0.85% 34.80±0.37%	20.04±0.44% 20.91±0.35%	16.34±0.18% 16.66±0.06%	13.65±0.28% 13.78±0.13%	12.70±0.03% 12.79±0.04%
FedProx	FAT α-WFAT	34.07±0.74% 33.95±0.39%	20.49±0.29% 20.86±0.40%	16.20±0.12% 16.73±0.01%	13.68±0.06% 13.80±0.04%	12.67±0.08% 12.80±0.02%
Scaffold	FAT α-WFAT	39.89±1.28% 39.80±1.27%	24.78±0.40% 25.05±0.51%	19.82±0.20% 20.27±0.13%	16.73±0.01% 16.79±0.01%	15.51±0.02% 15.58±0.01%
SVHN		Natural	FGSM	PGD-20	CW∞	AA
FedAvg	FAT α-WFAT	91.52±0.28% 91.26±0.01%	88.13±0.18% 88.27±0.02%	68.98±0.11% 72.04±0.32%	68.04±0.15% 69.96±0.16%	66.59±0.04% 68.89±0.27%
FedProx	FAT α-WFAT	91.00±0.08% 91.19±0.06%	87.65±0.15% 88.15±0.01%	68.48±0.04% 71.84±0.30%	67.16±0.02% 69.88±0.35%	65.76±0.18% 68.84±0.37%
Scaffold	FAT α-WFAT	90.82±0.87% 90.93±0.76%	87.89±0.66% 88.27±0.45%	69.51±0.84% 71.77±0.38%	68.12±0.88% 69.49±0.67%	67.19±0.54% 68.37±0.48%
18