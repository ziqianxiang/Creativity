Under review as a conference paper at ICLR 2022
Monotone deep B oltzmann machines
Anonymous authors
Paper under double-blind review
Ab stract
Deep Boltzmann machines refer to deep multi-layered probabilistic models, gov-
erned by a pairwise energy function that describes the likelihood of all variables
in the network. Due to the difficulty of inference in such systems, they have given
way largely to restricted deep Boltzmann machines (which do not permit intra-
layer or skip connections). In this paper, we propose a class of model that allows
for exact, efficient mean-field inference and learning in general deep Boltzmann
machines. To do so, we use the tools of the recently proposed monotone Deep
Equilibrium (DEQ) Model, an implicit-depth deep network that always guaran-
tees the existence and uniqueness of its fixed points. We show that, for a class of
general deep Boltzmann machine, the mean-field fixed point can be considered as
the equivalent fixed point of a monotone DEQ, which gives us a recipe for deriving
an efficient mean-field inference procedure with global convergence guarantees.
We apply this approach to simple deep convolutional Boltzmann architectures and
demonstrate that it allows for tasks such as the joint completion and classification
of images, all within a single deep probabilistic setting.
1	Introduction
This paper considers (deep) Boltzmann machines, which are pairwise energy-based probabilistic
models. Theses models specify a joint distribution over variables x given by the density
P(X) (X exp E x>ΦijXj + £ b>Xi ),	⑴
(i,j)∈E	i=1
where each X1:n denotes a discrete random variable over ki possible values, represented as a one-
hot encoding Xi ∈ {0, 1}ki ; E denotes the set of edges in the model; Φi,j ∈ Rki ×kj represents
pairwise potential; and bi ∈ Rki represents unary potential. Depending on context, these models
are typically referred to as pairwise Markov random fields (MRFs) (Koller & Friedman, 2009), or
(potentially deep) Boltzmann machines (Goodfellow et al., 2016; Salakhutdinov & Hinton, 2009;
Hinton, 2002). In the above setting each Xi may represent an observed or unobserved value, and
there can be substantial structure within the variables; for instance, the collection of variables X may
(and indeed will, in the main settings we consider in this paper) consist of several different “layers”
in a joint convolutional structure, leading to the deep convolutional Boltzmann machine (Norouzi
et al., 2009).
In this paper, we propose a new parameterization and algorithmic approach to approximate inference
of these probabilistic models. There are two main contributions: First, we define a generic param-
eterization of the pairwise kernel function Φ, that can represent a general Boltzmann machine. Our
parametrization is flexible enough to incorporate almost all operators and network topology, includ-
ing fully-connected layers, convolution operators, and skip-connections, etc. Through the lens of
the recently developed monotone DEQ (Winston & Kolter, 2020), we constraint Φ to satisfy cer-
tain monotonicity conditions through training and inference; this thereby assures that the mean-field
approximation will always have a unique, globally-optimal fixed point under this parameterization.
Second, although previous works (KrahenbuhI & Koltun, 2013; BaqUe et al., 20l6) have made ap-
proaches to parallel mean-field updates, they either require strong conditions on φ, or fail to con-
verge to the true mean-field distribution. We provide a properly-damped mean-field update method,
based upon a generic proximal operator, which is guaranteed to converge to the mean-field fixed
point, even if applied in parallel to all random variables simultaneously. Although there is no exact
1
Under review as a conference paper at ICLR 2022
Figure 1: Neural network topology of different Boltzmann machines. The general case is a complete graph.
Our proposed parameterization is equivalent to a general Boltzmann machine. Technically in our setting the
Φoo connections exist, but they are not updated due to the input injection.
closed form solution of this proximal operator, we derive a very efficient Newton-based implemen-
tation. To estimate the parameters of our model, we follow the marginal-based loss minimization
approach in Krahenbuhl & KoltUn (2013); Domke (2013), where the objective is to directly maxi-
mize the likelihood induced by the approximated mean-field distribution. Such an approach grants
us the ability to update parameters via taking gradient steps of the proposed mean-field iterations.
With the proposed approaches, we perform both learning and inference for a deep convolutional,
multi-resolution Boltzmann machine, and apply the network to model MNIST and CIFAR-10 pixels
and their classes conditioned on partially observed images. Such joint probabilistic modelling allows
us to simultaneously impute missing pixels and predict the class. While these are naturally a small-
scale problem, we emphasize that performing joint probabilistic inference over a complete model of
this type is a relatively high-dimensional task as far as traditional mean-field inference is concerned.
We also compare our inference method to previous ones and demonstrate different convergence
properties of each. We conclude by highlighting limitations and directions for future work with the
method.
2	Background and related work
This paper builds upon three main avenues of work: 1) deep equilibrium models, especially their
convergent version, the monotone DEQ; 2) the broad topic of energy-based deep model and Boltz-
mann machines in particular; and 3) work on concave potentials and parallel methods for mean-field
inference. We discuss each of these below.
Equilibrium models and their provable convergence The DEQ model was first proposed by Bai
et al. (2019). Based on the observation that a neural network zt+1 = σ(Wzt + Ux + b) with input
injection x usually converges to a fixed point, they modeled an effectively infinite-depth network
with input injection directly via its fixed point: Z* = σ(Wz* + Ux + b). Its backpropagation is
done through the implicit function theorem and only requires constant memory. Bai et al. (2020)
also showed that the multiscale DEQ models achieve near state-of-the-art performances on many
large-scale tasks. Winston & Kolter (2020) later presented a parametrization of the DEQ (denoted
as monDEQ) that guarantees provable convergence to a unique fixed point, using monotone op-
erator theory. Specifically, they parameterize W in a way that I - W mI (called m-strongly
monotone) is always satisfied during training for some m > 0; they convert nonlinearities into prox-
imal operators (which include ReLU, tanh, etc.), and show that using existing splitting methods like
forward-backward and Peaceman-Rachford can provably find the unique fixed point.
Markov random field (MRF) and its variants MRF is a form of energy-based model, which
model joint probabilities of the formpθ(x) = exp (-Eθ(x)) /Zθ for an energy function Eθ. A com-
mon type of MRF is the Boltzmann machine, the most successful variant of which is the restricted
Boltzmann machines (RBM) (Hinton, 2002) and its deep (multi-layer) variant (Salakhutdinov &
Hinton, 2009). Particularly, RBMs define Eθ(v, h) = -a>v - b>h - v>Wh, where θ = {W, a, b},
v is the set of visible variables, and h is the set of latent variables. It is usually trained using the
2
Under review as a conference paper at ICLR 2022
contrastive-divergence algorithm, and its inference can be done efficiently by a block mean-field
approximation. However, a particular restriction of RBMs is that there can be no intra-layer connec-
tions, that is, each variable in v (resp. h) is independent conditioned on h (resp. v). A deep RBM
allows different layers of hidden nodes, but there cannot be intra-layer connections. By contrast,
our formulation allows intra-layer connections and is therefore is more expressive in this respect.
See fig. 1 for the network topology of RBM, deep RBM, and general BM (we also use the term
general deep BM interchangeably to emphasize the existence of deep structure). Wu et al. (2016)
proposed a deep parameterization of MRF, but their setting only considers a grid of hidden variables
h and the connections among hidden units are restricted to the neighboring nodes. Therefore, it
is a special case of our parameterization (although their learning algorithm is orthogonal to ours).
Numerous works also try to combine deep neural networks with conditional random fields (CRF)
(Krahenbuhl & Koltun, 2013; Zheng et al., 2015; Schwartz et al., 2017) These models either train a
pre-determined kernel as an RNN or use neural networks for producing either inputs or parameters
of their CRFs.
Parallel and convergent mean-field It is well-known that mean-field updates converge locally
using a coordinate ascent algorithm (Blei et al., 2017). However, local convergence is only guaran-
teed if the update is applied sequentially. Nonetheless, several works have proposed techniques to
parallelize updates. Krahenbuhl & Koltun (2013) proposed a concave-convex procedure (CCCP) to
minimize the KL divergence between the true distribution and the mean-field variational family. To
achieve efficient inference, they use a concave approximation to the pairwise kernel, and their fast
update rule only converges if the kernel function is concave. Later, Baque et al. (2016) derived a
similar parallel damped forward iteration to ours that provably converges without the concave po-
tential constraint. However, unlike our approach, they do not use a parameterization which ensures a
global mean-field optimum, and their algorithm therefore may not converge to the actual fixed point
of the mean-field updates. This is because Baque et al. (2016) used the Proxf proximal operator
(described below), whereas we derive the proxfα operator to guarantee global convergence when
doing mean-field updates in parallel. What,s more, Baque et al. (2016) focused only on inference
over prescribed potentials, and not on training the (fully parameterized) potentials as we do here.
Le-Huu & Alahari (2021) brought up a generalized Frank-Wolfe based framework for mean-field
updates which include the methods proposed by Baque et al. (2016); Krahenbuhl & Koltun (2013).
Their results only guarantee global convergence to a local optimal.
3 Monotone deep B oltzmann machines and approximate inference
In this section, we present the main technical contributions of this work. We begin by presenting a
parameterization of the pairwise potential in a Boltzmann machine that guarantees the monotonicity
condition. We then illustrate the connection between a (joint) mean-field inference fixed point and
the fixed point of our monotone Boltzmann machine and discuss how deep structured networks can
be implemented in this form practically; this establishes that, under the monotonicity conditions on
Φ, there exists a unique globally-optimal mean-field fixed point. Finally, we present an efficient
parallel method for computing this mean-field fixed point, again motivated by the machinery of
monotone DEQs and operator splitting methods.
3.1	A monotone parameterization of general B oltzmann machines
In this section, we show how to parameterize our probabilistic model in a way that the pairwise
potentials satisfy I - Φ mI, which will be used later to show the existence of a unique mean-
field fixed point. Additionally, since Φ defines a graphical model that has no self-loop, we further
require Φ to be a block hollow matrix (that is, the ki × ki diagonal blocks corresponding to each
variable must be zero). While both these conditions on Φ are convex constraints, in practice it would
be extremely difficult to project a generic set of weights onto this constraint set under an ordinary
parameterization of the network.
Thus, we instead advocate for a non-convex parameterization of the network weights, but one which
guarantees that the monotonicity condition is always satisfied, without any constraint on the weights
in the parameterization. Specifically, define the block matrix
A =[ Al A2 …An ]	(2)
3
Under review as a conference paper at ICLR 2022
with Ai ∈ Rd×ki matrices for each variables, and where d can be some arbitrarily chosen dimension.
Then let Ai be a spectrally-normalized version of Ai
Ai = Ai ∙ min{√Γ-m∕kAik2,1}	(3)
i.e., a version of Ai normalized such that its largest singular value is at most √1 - m (note that
we can compute the spectral norm of Ai as kAi k2 = kAiT Ai k21/2, which involves computing the
singular values of only a k X k matrix, and thus is very fast in practice). We define the A matrix
analogously as the block version of these normalized matrices.
Then we propose to parameterize Φ as
Φ = blkdiag(AT A) — AT A
(4)
where blkdiag denotes the block-diagonal portion of the matrix along the ki × ki block. Put another
way, this parameterizes Φ as
Φij
—AT Aj
0
ifi 6=j,
ifi=j.
(5)
As the following simple theorem shows, this parameterization guarantees both hollowness of the Φ
matrix and monotonicity ofI — Φ, for any value of the A matrix.
Theorem 3.1. For any choice of parameters A, under the parametrization equation 4 above, we
have that 1) Φii = 0 for all i = 1, . . . , n, and 2) I — Φ mI.
Proof. Block hollowness of the matrix follows immediately from construction. To establish mono-
tonicity, note that
I — Φ 占 ml y⇒ I + ATA — blkdiag(ATA)占 ml
^= I — blkdiag(ATA)占 ml
T	(6)
y⇒ I — Ai Ai 占 ml, ∀i
^⇒ IlAiIl2 ≤ √1 — m, ∀i.
This last property always holds by the construction of Ai.	□
3.2	Mean-field inference as a monotone DEQ
In this section, we formally present how to formulate the mean-field inference as a DEQ update.
Recall from before that we are modelling a distribution of the form eq. (1). We are interested
in approximating the conditional distribution p(xh|xo), where o and h denote the observed and
hidden variables respectively, with a factored distribution q(xh) = Qi∈h qi (xi). Here, the standard
mean-field updates (which minimize the KL divergence between q(xh) andp(xh|xo) over the single
distribution qi(xi)) are given by the following equation,
qi(xi) := softmax	Φijqj(xj) + bi	(7)
∖j<i,j)∈E	)
where overloading notation slightly, we let qj(xj) denote a one-hot encoding of the observed value
for any j ∈ o (see e.g., Koller & Friedman (2009) for a full derivation).
The essence of the above updates is a characterization of the joint fixed point to mean-field inference.
For simplicity of notation, defining q = [q1(x1) q2(x2) . . .]T .
We see that qh is a joint fixed point of all the mean-field updates if and only if
qh = softmax (Φhhqh + Φhoxo + bh)	(8)
where xo analogously denotes the stacked one-hot encoding of the observed variables.
We briefly recall the monotone DEQ framework of Winston & Kolter (2020). Given input vector x,
a monotone DEQ computes the fixed point z?(x) that satisfies the equilibrium equation
z?(x) = σ(W z?(x) + U X + b).	(9)
4
Under review as a conference paper at ICLR 2022
Figure 2: Illustration of a possible deep convolutional Boltz-
mann machine, where the monotonicity structure can still be
enforced.
30-
25-
2 20-
£ 15-
*
10-
5-
0	2500 5000 7500 IOTOO 12500 15000 17500
Taining steps
Figure 3: Convergence of the forward-
backward splitting.
Winston & Kolter (2020) showed that if: 1) σ is given by a proximal operator1 σ(x) = prox1f (x)
for some convex closed proper (CCP) f, and 2) if we have the monotonicity condition I - W mI
(in the positive semidefinite sense) for some m > 0, then for any x there exists a unique fixed
point z?(x), which can be computed through standard operator splitting methods, such as forward-
backward splitting.
We now state our main claim of this subsection, that under certain conditions the mean-field fixed
point can be viewed as the fixed point of an analogous DEQ. This is formalized in the following
proposition.
Proposition 3.1. Suppose that the pairwise kernel Φ satisfies I - Φ mI 2 for m > 0. Then the
mean-field fixed point
qh = softmax (Φhhqh + Φhoxo + bh)	(10)
corresponds to the fixed point of a monotone DEQ model. Specifically, this implies that for any xo,
there exists a unique, globally-optimal fixed point of the mean-field distribution qh.
As the monotonicity condition of the monotone DEQ is assumed in the proposition, the proof of the
proposition rests entirely in showing that the softmax operator is given by proxf1 for some CCP f .
Specifically, as shown in (KrahenbuhI & Koltun, 2013), this is the case for
f (Z) = XZilogZi- 2Ilzk2 + I
i
zi
1, zi ≥ 0
(11)
i.e., the restriction of the entropy minus squared norm to the simplex (note that even though we are
subtracting a squared norm term it is straightforward to show that this function is convex, since the
second derivatives are given by 1/Zi - 1, which is always non-negative over its domain).
3.3	Practical considerations when modelling monotone B oltzmann machines
The construction in section 3.1 guarantees monotonicity of the resulting pairwise probabilistic
model. However, instantiating the model in practice, where the variables represent hidden units
of a deep architecture (i.e., representing multi-channel image tensors with pairwise potentials de-
fined by convolutional operators), requires substantial subtlety and care in implementation. In this
setting, we do not want to actually represent A explicitly, but rather determine a method for multi-
plying Av and ATv for some vector v (as we see in section 3.2, this is all that is required for the
parallel mean-field inference method we propose). This means that certain blocks ofA are typically
parameterized as convolutional layers, with convolution and transposed convolution operators as the
main units of computation.
More specifically, we typically want to partition the full set of hidden units into some K distinct
sets
q = [q1 q2	. . . qK]T	(12)
1A proximal operator is defined by ProXa(X) = argmin, 1 ∣∣x — z『+ αf (z).
2Technically speaking, we only need I - Φhh	mI, but since we want this to hold for any choice of h,
we need the condition to apply to the entire Φ matrix.
5
Under review as a conference paper at ICLR 2022
where e.g., qi would be best represented as a height × width × groups × cardinality tensor (i.e., a
collection of a multiple hidden units corresponding to different locations in a typical deep network
hidden layer). Note that here qi is not the same as qi(xi), but rather the collection of many different
individual variables. These qi terms can be related to each other via different operators, and a
natural manner of parameterizing A in this case is as an interconnected set of a convolutional or
dense operators. To represent the pairwise interactions, we can create a similarly-factored matrix A,
e.g., one of the form
「Aii 0	…	0	-
A21	A22	…	0
A =	.	.	.	.	(13)
.	..	.
_ Aki Aκ2	…AKK .
where e.g., Aij is a (possibly strided) convolution mapping between the tensors representing qj and
qi . In this case, we emphasize that Aij is not the kernel matrix that one “slides” along the variables.
Instead, Aij is the linear mapping as if we write the convolution as a matrix-matrix multiplication.
For example, a 2D convolution with stride 1 can be expressed as a doubly block circulant matrix (the
case is more complicated when different striding is allowed). This parametrization is effectively a
general Boltzmann machine, since each random variable in eq. (12) can interactive with any other
variables except for itself. Varying Aij, the formulation in eq. (13) is rich enough for any types of
architectures including convolutions, fully-connected layers, and skip-connections, etc.
An illustration of one possible network structure is shown in fig. 2. The precise details of how
one computes the block diagonal elements of AT A, and how one normalizes the proper diagonal
blocks (which, we emphasize, still just requires computing the singular values of matrices whose
size is the cardinality of a single qi (xi)) are somewhat involved, so we defer a complete descrip-
tion to the Appendix (and accompanying code). The larger takeaway message, though, is that it is
possible to parameterize complex convolutional multi-scale Boltzmann machines, all while ensuring
monotonicity.
3.4	Efficient parallel s olving for the mean-field fixed point
Although the monotonicity of Φ guarantees the existence of a unique solution, it does not necessarily
guarantee that the simple iteration
qh(t) = softmax(Φhhqh(t-i) + Φhoxo + bh)
(14)
will converge to this solution. Instead, to guarantee convergence, one needs to apply the damped
iteration (see, e.g. (Winston & Kolter, 2020))
qh(t) = proxfα (1 - α)qh(t-i) + α(Φhhqh(t-i) + Φhoxo + bh) .	(15)
The damped forward-backward iteration converges linearly to the unique fixed point if α ≤ 2m/L2,
assuming I - Φ is m-strongly monotone and L-Lipschitz (Ryu & Boyd, 2016). Crucially, this
update can be formed in parallel over all the variables in the network: we do not require a coordinate
descent approach as is typically needed by mean-field inference.
The key issue, though is that while proxif (x) = softmax(x) for f defined as in eq. (11), in general
this does not hold for α 6= 1. Indeed, for α 6= 1, there is no closed form solution to the proximal
operation, and computing the solution is substantially more involved. Specifically, computing this
proximal operator involves solving the optimization problem
ProXa(x) = arg min 1 ∣∣x - z∣∣2 + α X Zi log Zi - α ∣∣zk2
z2	2
i
subject to Zi = 1, Z ≥ 0.
i
(16)
The following theorem, proved in the Appendix, characterizes the solution to this problem for α ∈
(0, 1) (although it is also possible to compute solutions for α > 1, this is not needed in practice, as
it corresponds to a “negatively damped” update, and it is typically better to simply use the softmax
update in such cases).
6
Under review as a conference paper at ICLR 2022
Theorem 3.2. Given f as defined in eq. (11), α ∈ (0, 1), and x ∈ Rk, the proximal operator
proxfα(x) is given by
Proxa(X)i = ι-αW( 1-α eχp (X^ ))，
where λ ∈ R is the unique solution chosen to ensure that the resulting i proxfα (xi) = 1, and
where W (∙) is the principal branch of the Lambert W function.
In practice, however, this is not the most numerically stable method for computing the proximal
operator, especially for small α, owing to the large term inside the exponential. Computing the
proximal operation efficiently is somewhat involved, though briefly, we define the alternative func-
tion
g(y) = log -^W 1-_α exp (- - 1))	(17)
1-	α	α	α
and show how to directly compute g(-) using Halley’s method (note that Halley’s method is also the
preferred manner to computing the Lambert W function itself numerically (Corless et al., 1996)).
Finding the prox operator then requires that we find λ such that Pik=1 exp(g(Xi+λ)) = 1. This can
be done via (one-dimensional) root finding with Newton’s method, which is guaranteed to always
find a solution here, owing to the fact that this function is convex monotonic for λ ∈ (-∞, 1).
We can further compute the gradients of the g function and of the proximal operator itself via im-
plicit differentiation (i.e., we can do it analytically without requiring unrolling the Newton or Halley
iteration). We describe the details in the Appendix, and include an efficient PyTorch function imple-
mentation in the supplementary material.
Comparison to Winston & Kolter (2020) Although this work uses the same monotonicity con-
straint as in Winston & Kolter (2020), our result further requires the linear module Φ to be hollow,
and extend their work to the softmax nonlinear operator as well. These extensions introduce signif-
icant complications, but also enable us to interpret our network as a probabilistic model, while the
network in Winston & Kolter (2020) cannot.
3.5 Training considerations
Finally, we discuss approaches for training these monotone Boltzmann machines, exploiting their
efficient approach to mean-field inference. Probabilistic models are typically trained via approxi-
mate likelihood maximization, and since the mean-field approximation is based upon a particular
likelihood approximation, it may seem most natural to use this same approximation to train param-
eters. In practice, however, this is often a suboptimal approach. Specifically, because our forward
inference procedure ultimately uses mean-field inference, it is better to train the model directly to
output the correct marginals, when running this mean-field procedure. This is known as a marginal-
based loss (Domke, 2013). In the context of monotone Boltzmann machines, this procedure has a
particularly convenient form, as it corresponds roughly to the “typical” training of DEQ.
In more detail, suppose we are given a sample x ∈ X (i.e., at training time the entire sample is
given), along with a specification of the “observed” and “hidden” sets, o and h respectively. Note
that the choice of observed and hidden sets is potentially up to the algorithm designer, and can
effectively allows one to train our model in a “self-supervised” fashion, where the goal is to predict
some unobserved components from others. In practice, however, one typically wants to design
hidden and observed portions congruent with the eventual use of the model: e.g., if one is using the
model for classification, then at training time it makes sense for the label to be “hidden” and the
input to be “observed.”
Given this sample, we first solve the mean-field inference problem to find qh?(xh) such that
qh? = softmax (Φhhqh? + Φhoxo + bh) .
(18)
For this sample, we know that the true value of the hidden states is given by xh . Thus, we can apply
some loss function '(q?, Xh) between the prediction and true value, and update parameters of the
model θ = {A, b} using their gradients
d'(q?, Xh) = d'(q?, Xh) ∂qh = d'(q?, Xh) (I _ dg(q?)「dg(q?)
∂θ	∂qh	∂θ	∂q?	∖	∂qh ) ∂θ
(19)
7
Under review as a conference paper at ICLR 2022
with g(qh) ≡ Proxa ((1 - α)q力 + α(Φhhqh + ΦhoX0 + bh)). and where the last equality comes
from the standard application of the implicit function theorem as typical in DEQs or monotone
DEQs. This backward pass can also be computed via an iterative approach, and here the details
exactly mirror that of Winston & Kolter (2020).
As a final note, we also mention that owning to the restricted range of weights allowed by the
monotonicty constraint, the actual output marginals qi(xi) are often more uniform in distribution
than desired. Thus, we typically apply the loss to a scaled marginal
扇(Xi) H qi(xi)τi	(20)
where τi ∈ R+ is a variable-dependent learnable temperature parameter. Importantly, we emphasize
that this is only done after convergence to the mean-field solution, and thus only applies to the
marginals to which we apply a loss: the actual internal iterations of mean-field cannot have such a
scaling, as it would violate the monotonicity condition.
4	Experimental evaluation
We evaluate the performance of our model primarily on the MNIST dataset. Our model is able to
approximate any conditional distribution. Here we demonstrate how to model missing pixels and
the class digits conditioned on the observed pixels, as well as model missing pixels conditioned
on class digits and observed pixels. Although MNIST is of course a small-scale problem, the goal
here is to demonstrate joint inference and learning over what is still a reasonably-sized joint model,
considering the number of hidden units. Nonetheless, the current experiment is admittedly largely a
demonstration of the proposed method rather than a full accounting of its performance, a point we
highlight in the subsequent section as well. After describing the MNIST experiments, we detail a
similar experiment on CIFAR-10 to demonstrate the potential for scaling the approach.
We also show how our inference method differs from the previous ones. On the joint imputation
and classification task, We train models using our updates and the updates in KrahenbuhI & Koltun
(2013); BaqUe et al. (2016), and inference each using all three update methods, with and without
the monotonicity constraint. We show that our inference method is superior in both performance
and convergence speed. Numerically, we also demonstrate that the other two methods could either
diverge or not converging to the actual mean-field fixed point. All deferred experiments and details
can be found in the appendix.
Experiment setup The original MNIST dataset has one channel representing the gray-scale inten-
sity, ranging between 0 and 1. Here we adopt the strategy of Van Oord et al. (2016) to convert this
continuous distribution to a discrete one. We bin the intensity evenly to 4 categories {0, . . . , 3}, and
for each channel uses a one-hot encoding of the category so that the input data has shape 4 × 28 × 28.
We remark that the number of categories is chosen arbitrarily and can be any integer. For image with
missing data, if the pixels are randomly masked, we always mask each pixel off independently with
probability 60%, such that in expectation only 40% pixels are observed. If a whole patch of pixels
is masked, we randomly pick a 14 × 14 patch. The patches/masks are chosenly differently for every
image, similar to the query training in Lazaro-Gredilla et al. (2020). To make the model class richer,
in the patch case we lift the monotonicity constraint, and the model converges regardless. We also
conduct the same set of MNIST experiments using a 3-layer DBM and include the results in the
appendix.
Results on imputation and classification Training the above described model for 40 epochs with
40% missing pixels leads to a 92.95% test accuracy, and the image reconstruction result is shown
in fig. 4d. Based on the reconstructed images and our classification results, our model successfully
recovers the unimodal distribution conditioned on the observed pixels. Despite working simulta-
neously on two different tasks, our model can simply be trained jointly, taking full advantages of
the existing auto-differentiation frameworks without additional burden. It is worth noting that while
the autoregressive models of Van Oord et al. (2016) are comparable to ours, they require sampling
pixels one-by-one in a sequential order at inference time, whereas our model does not assume an
underlying sequence and can generate all pixels at once. Comparing to the differently-parameterized
monDEQ in Winston & Kolter (2020), whose linear module suffers from drastically increasing con-
dition number (hence in later epochs taking around 20 steps to converge, even with tuned α), our
8
Under review as a conference paper at ICLR 2022
(a) Test data with 60% pixels randomly
masked
7 乙 /。ɑ /，A
。亍。。夕0 t 5
"6"63S
□840)31
"7 2Q I λ I
(b) Test data with a 14 × 14 patch masked
A 5Γ1，
¥/ J?•，^
/ O /ɔ Λ/ ?
4夕夕9 7
今以一夕2
/ O ⅛ 3 7
乙？7o∆
7<c* 4 4
7乙/。q / y外
^m。幻夕。J 5
47七H夕GSS
4 6 彳 0 1 3 1
"7 2Q I "
(c) Original test data
R 5LJ1 I
夕！ I 3 Cʌ
∕∙ Zb J f
q 9 夕。7
Ob 什 42
/0377
乙夕 7o∆r
7 -5 g 4 3
(d)	Imputation with 60% pixels randomly
masked
(e)	Imputation with a 14 × 14 patch
masked, inference without injection labels
(f)	Imputation with a 14 × 14 patch
masked, inference with injection labels
(a) Test data has 50% pixels randomly
masked
Figure 4: MNIST pixel imputation
(b) Imputed pixel inference (without injec-
tion labels)
解目■邑户NQG
(c) Original image
Figure 5: CIFAR-10 pixel imputation
parameterization produces a much nicer convergence pattern: the average number of forward iter-
ations over the 40 training epochs is less than 6 steps, see fig. 3. When the missing pixels form
consecutive patches, our model reconstructs readable digits despite potentially large chunk of miss-
ing pixels (fig. 4e). Meanwhile, if the model is given the image labels as input injections, our model
performs conditionaly generation fairly well (fig. 4f). These results demonstrate the flexibility of
our parameterization for modelling different conditional distributions.
CIFAR-10 Experiment We additionally conduct an experiment on the simultaneous tasks of im-
age pixel imputation and label prediction given partially observed features. Model architecture and
training details are given in the appendix. With 50% of the pixels observed, the model obtains 58%
test accuracy, and can impute the missing pixels effectively (see fig. 5).
5 Conclusion
In this work, we give a monotone parameterization for general Boltzmann machines, and connect its
mean-field fixed point to a monotone DEQ model. We provide a mean-field update method that is
proven to be globally convergent. Our parameterization allows for full parallelization of mean-field
updates without restricting the potential function to be concave, thus addressing issues with prior
approaches. Moreover, we allow complicated and hierarchical structures among the variables and
show how to efficiently implement them. For parameter learning, we directly optimize the marginal-
based loss over the mean-field variational family, circumventing the intractability of computing the
partition function. Our model is evaluated on the MNIST and CIFAR-10 dataset for simultaneously
predicting with missing data and imputing the missing data itself. There are several potential future
directions. First, the monotone parameterization in theorem 3.1 is not an if-and-only-if statement,
hence potentially making our model more restrictive than necessary. The second direction is that
although we have a fairly efficient implementation of proxfα, it is still slower than normal nonlinear-
ities like ReLU or softmax. It is an interesting direction to more efficiently scale these models.
9
Under review as a conference paper at ICLR 2022
References
Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. arXiv preprint
arXiv:1909.01377, 2019.
Shaojie Bai, Vladlen Koltun, and J Zico Kolter. Multiscale deep equilibrium models. arXiv preprint
arXiv:2006.08656, 2020.
Pierre Baque, TimUr Bagautdinov, Francois FleUreL and Pascal Fua. Principled parallel mean-field
inference for discrete random fields. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 5848-5857, 2016.
David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisti-
cians. Journal of the American statistical Association, 112(518):859-877, 2017.
Robert M Corless, Gaston H Gonnet, David EG Hare, David J Jeffrey, and Donald E Knuth. On the
lambertw function. Advances in Computational mathematics, 5(1):329-359, 1996.
Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on
effective number of samples. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 9268-9277, 2019.
Justin Domke. Learning graphical model parameters with approximate marginal inference. IEEE
transactions on pattern analysis and machine intelligence, 35(10):2454-2467, 2013.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT press Cambridge, 2016.
Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural
computation, 14(8):1771-1800, 2002.
Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques. MIT
press, 2009.
Philipp KrahenbUhl and Vladlen Koltun. Parameter learning and convergent inference for dense
random fields. In International Conference on Machine Learning, pp. 513-521. PMLR, 2013.
Miguel Lazaro-Gredilla, Wolfgang Lehrach, Nishad Gothoskar, Guangyao Zhou, Antoine Dedieu,
and Dileep George. Query training: Learning a worse model to infer better marginals in undi-
rected graphical models with hidden variables. arXiv preprint arXiv:2006.06803, 2020.
KhUe Le-HUU and Karteek Alahari. Regularized frank-wolfe for dense crfs: Generalizing mean field
and beyond. Advances in Neural Information Processing Systems, 34, 2021.
Mohammad Norouzi, Mani Ranjbar, and Greg Mori. Stacks of convolutional restricted boltzmann
machines for shift-invariant feature learning. In 2009 IEEE Conference on Computer Vision and
Pattern Recognition, pp. 2735-2742. IEEE, 2009.
Ernest K Ryu and Stephen Boyd. Primer on monotone operator methods. Appl. Comput. Math, 15
(1):3-43, 2016.
Ruslan Salakhutdinov and Geoffrey Hinton. Deep boltzmann machines. In Artificial intelligence
and statistics, pp. 448-455. PMLR, 2009.
Idan Schwartz, Alexander G Schwing, and Tamir Hazan. High-order attention models for visual
question answering. arXiv preprint arXiv:1711.04323, 2017.
Aaron Van Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In
International Conference on Machine Learning, pp. 1747-1756. PMLR, 2016.
Homer F Walker and Peng Ni. Anderson acceleration for fixed-point iterations. SIAM Journal on
Numerical Analysis, 49(4):1715-1735, 2011.
Ezra Winston and J Zico Kolter. Monotone operator equilibrium networks. arXiv preprint
arXiv:2006.08591, 2020.
10
Under review as a conference paper at ICLR 2022
Zhirong Wu, Dahua Lin, and Xiaoou Tang. Deep markov random field for image modeling. In
European Conference on Computer Vision, pp. 295-312. Springer, 2016.
Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Da-
long Du, Chang Huang, and Philip HS Torr. Conditional random fields as recurrent neural net-
works. In Proceedings of the IEEE international conference on computer vision, pp. 1529-1537,
2015.
11
Under review as a conference paper at ICLR 2022
A Appendix
A.1 Deferred proofs
Proof of theorem 3.2. By definition, the proximal operator induced by f (the same f in eq. (11))
and α solves the following optimization problem:
mzn 1 kx - zk2+ɑ X Zi log Zi—2 Iizk2
i
s.t.	Zi ≥ 0, i = 1, . . . , d,
X Zi = 1
i
of which the KKT condition is
—Xi + Zi + α + α log Zi — az% + λ — μ% = 0, for i ∈ [d]
μi ≥ 0
Zi ≥ 0
E μiZi = 0
i∈[d]
d
X Zi = 1
i=1
We have that μi = 0 is feasible and the first equation of the above KKT condition can be massaged
as
—Xi + Zi + a + a log Zi — azi + λ — μi = 0
^⇒
^⇒
^⇒
^⇒
^⇒
^⇒
(1 — a)Zi + alog Zi = Xi — a — λ
(1 — a)Zi + alog Zi	Xi — a — λ
aa
(1 — a)Zi + a log Zi	Xi — a — λ
exp(-------J=exp (—α一)
Xi — a — λ
——Zi I = exp -
aa
1 — a 1 — a Xi — a — λ
a	a	a ”「a
1 — a	1 — a	Xi — a — λ
W	exp
Zi exp
a
1—a
1—a
----Zi exp
Zi
——Zi
a
where W is the lambert W function. Notice here Zi > 0. Our primal problem is convex and Slater’s
condition holds. Hence, we conclude that
Zi
α 1 - α	xi - α - λ
W	exp
□
A.2 Convolution network
It is clear that the monotone parameterization in section 3 directly applies to fully-connected net-
works, and all the related quantities can be calculated easily. Nonetheless, the real power of the DEQ
model comes in when we use more sophisticated linear operators like convolutions. In the context
of Boltzmann machines, the convolution operator gives edge potentials beneficial structures. For ex-
ample, when modeling the joint probability of pixels in an image, it is intuitive that only the nearby
pixels depend closely on each other.
12
Under review as a conference paper at ICLR 2022
Let A ∈ Rk×k×r×r denote a convolutional tensor with kernel size r and channel size k, let x denote
some input. For a convolution with stride 1, the block diagonal elements of ATA simply form a
1 × 1 convolution. In particular, we apply the convolutions
-AT(A(x)) + A(X)	(21)
where A is a 1 X 1 convolution given by
A[:,:] = X A[:, :,i,j]TA[:, :,i,j].	(22)
i,j
We can normalize by the spectral norm of A term to ensure strong monotonicity. Since A can be
rewritten as a k × k matrix and k is usually small, its spectral norm can be easily calculated.
It takes more effort to work out convolutions with stride other than 1. Specifically, the block diagonal
terms do not form a 1 × 1 convolution anymore, instead, the computation varies depending on the
location. It is easier to see the computation directly in the accompanying code.
Grouped channels It is crucial to introduce the concept of grouped channels, which allows us to
represent multiple categorical variables in a single location, such as the three categorical variables
representing the three (binned) color channels of an RGB pixel. In this case, each of the three
RGB channels will be represented by a different group of k channels representing the k bins. The
grouping is achieved by having the nonlinearity function (softmax) applied to each group separately.
We remark that the convolutions themselves are not grouped, otherwise none of the red pixels would
interact with green or blue pixels, etc. Instead, we want all RGB channels to interact with each other
(except that channel i at position (j, k) does not interact with itself). That means in eq. (4), the
blkdiag(ATA) is grouped in the following way. Recall that this block diagonal term has element
of size ki × ki for i ∈ [n]. This parameterization has only 1 group. With g groups, the element of
the block diagonal matrix then has size ki1	×	ki1 , . . . ,kig	×	kig	fori ∈	[n],	where	j∈[g] kij	=	ki.
We also observe empirically that grouping the latent variables improves the performance.
A.3 EFFICIENT COMPUTATION OF proxfα
The solution to the proximal operator in damped forward iteration given in theorem 3.2 involves the
Lambert W function, which does not attain an analytical solution. In this section, we show how to
efficiently calculate the nonlinearity σ(xi), as well as its Jacobian matrix for backward iteration.
Let f(y) = ι-αaW (Ioaa exp (y — 1)),and we have
χ = log f (y) = log Va——+ log ey/a-1 + log1α
1 — a	a
where the last equality uses the identity log(W (x))	= log x — W (x). Rewrite
W (1-αα σxP (y — 1)) = f (y) 1-αa and massage the terms, we have that solving log f (y) is equiv-
alent to finding the root of
h(x) = y — a — ex(1 — a) — ax.
Direct calculation shows that h0(x) = —a — (1 — a)ex and h00(x) = —(1 — a)ex. Note here y is
the input and it is known to us, and x is a scalar. Hence we can efficiently solve the root finding
problem using Halley,s method. For backpropagation, we need dx, which can be computed by
dy
implicit differentiation:
h(x) = y — a — ex(1 — a) — ax = 0
dx	1	1
dy a +(1 — a)ex	y — ax
Now we can find λ s.t Pi zi = 1 using Newton’s method on g(λ) = Pi elog(f(xi+λ)) — 1 = 0.
Note this is still a one-dimensional optimization problem. A direct calculation shows that 祭 =
Pi elog(f (χi+λ))dlog(dλxi+λ), and above we have already calculated that
d log(f (Xi + λ)	dx*	1
dλ	dy y + λ — ax
13
Under review as a conference paper at ICLR 2022
For backward computation, by the chain rule, we have:
delog f(xi+λ) = elog f(xi+λ) d logf (Xi + X))
dxi	dxi
=elog f(χi+λ)1 + dλ∕dχi
Xi + λ - α log(f (Xi + λ))，
where the last step is derived by implicit differentiation. Now to get dλ∕dxi, notice that by applying
the implicit function theorem on p(X, λ(X)) = Pi elog(f (xi +λ)) - 1 = 0, we get
dλ	dpp∖ 1 dp
dXi dλ dXi .
Thus we have all the terms computed, which finishes the derivation.
B	Additional Experiments and Details
Here we provide the model architectures and experiment details omitted in the main text.
Model architecture For MNIST experiments, using the notation in eq. (13), we design a 20-layer
deep monotone DEQ with the following structure:
-Aιι	0	0	0 一
A21	A22	0	0
A31	A32	A33	0
0	0	A43	A44
where A11 is a 20×20×3×3 convolution, A22 is a 40×40×3×3 convolution, A21 is a 40×20×3×3
convolution with stride 2, A33 is a 80 × 80 × 3 × 3 convolution, A31 is a 80 × 20 × 3 × 3 convolution
with stride 4, A32 is a 80 X 40 X 3 X 3 convolution with stride 2, A43 is a (80 ∙ 7 ∙ 7) X 10 dense
linear layer, and A44 is a 10 × 10 dense linear layer. The corresponding variable q as in eq. (12)
then has 4 elements of shape (20 X 28 X 28), (40 X 14 X 14), (80 X 7 X 7), (10 X 1). When applying
the proximal operator to q, we use 1, 10, 20, 1 as their number of groups, respectively.
Training details and hyperparameters Treating the image reconstruction as a dense classifica-
tion task, We use cross-entropy loss and class weights 1—with β = 0.9999 (CUi et al., 2019),
where ni is the number of times pixels with intensity i appear in the hidden pixels. For classifica-
tion, we use standard cross-entropy loss. To enable joint training, we put equal weight of 0.5 on
both task losses and backpropagate through their sum. For both tasks, we put τiΦqi into the cross-
entropy loss as logits, as described in eq. (20). Since mean-field approximation is (conditionally)
unimodal, the scaling grants us the ability to model more extreme distributions. To achieve faster
damped forward-backward iteration, we implement Anderson acceleration (Walker & Ni, 2011),
and stop the fixed point update as soon as the relative difference between two iterations (that is,
kqt+1 - qtk∕kqtk) is less than 0.01, unless we hit a maximum number of 50 allowed iterations. For
proxfα and the damped iteration, we set α = 0.125 (Although one can tune down α whenever the
iterations do not converge, empirically this never happens on our task). We use the Adam optimizer
with learning rate 0.001. Our models are trained on one GeForce GTX 2080 Ti GPU.
Comparison to past inference methods We conduct numerical experiments to compare our in-
ference updating method to the ones proposed by KrahenbUhI & Koltun (2013); BaqUe et al. (2016),
denoted as Krahenbuhl's and Baque's respectively. Krahenbuhl's fast concave-convex procedure
(CCCP) essentially decomposes to eq. (14), the un-damped mean-field update with softmax. This
update only converges provably when Φ is concave. Baque's inference method can be written as
qh(t) = softmax (1 - α) log qh(t-1) + α(Φhhqh(t-1) + Φhoxo + bh) .	(23)
This algorithm provably converges despite the property of the pairwise kernel function. However,
this procedure converges in the sense that the variational free energy keeps decreasing. Therefore
their fixed point may not be the true mean-field distribution eq. (10). In this experiment, we train the
14
Under review as a conference paper at ICLR 2022
Table 1: Relative update residual when monotonicity is enforced
InferenCe Train	Krahenbuhl	BaqUe	Our
Krahenbuhl	-0.0004-	0.0061	0.0024
Baque	1.250	0.0059	0.0024
OUr	1.144	—	0.0057	0.0017
Table 2: Relative update residual when monotonicity is not enforced
InferenCe Train	KrahenbUhl	Baque	Our
Krahenbuhl	-0.0005-	0.0065	0.0024
BaqUe	-1.0924-	0.0119	0.0042
OUr	1.1286 —	0.0065	0.0022
models using three different updating methods, and perform inference using three methods as well,
with and without the monotonicity condition.
KrahenbUhl's and Baque's methods often do not converge in the backward pass (there's no theo-
retical guarantees neither). To rule out the impact of the backward iteration, during training we
directly update use the gradient of the forward pass, instead of using a backward gradient hook to
compute eq. (19). Figure 7 and fig. 8 demonstrate how the three update methods impute missing
pixels when trained with different update rules, with and without the monotonicity condition, re-
SPectively. KrahenbUhl's usually does not converge when the model is trained with our method or
Baque's, whereas the other two methods impute the missing pixels well. The classification results
are presented in table 3 and table 4. Notice that when trained with our method or Baque's, the
convergence issue OfKrahenbuhl's leads to horrible classification accuracy. Our method is superior
to other inference methods when the model is trained in a different update fashion. For example,
if the model is trained by using Krahenbuhl's, it makes sense that the model performs the best if
the inference is also Krahenbuhl's since the parameters are biased toward that particular inference
method. However, our method in this case outperforms Baque's.
After these methods halt and return qhT , we run one more iteration of
qT+1 = Softmax (ΦhhqT + ΦhoXo + bh) ,	(24)
and record the relative update residual kqhT +1 - qhT k/kqhT k for randomly selected 4000 MNIST
images. The results are listed in table 1 and table 2. To alleviate the effect of numerical issues, we
strength the convergence condition to either the relative residual is less than 10-3 or the number of
iterations exceeds 100 steps.
It appears in table 1 and table 2 that although our method has a much lower residual compare to
Baque's, both of them seem small and convergent. This is because the “optimal” fixed point in
this setting on MNIST might be unique and both methods happen to converge to the same point.
However, this is in general not true. We compare our method vs Baque's on 400 randomly se-
lected MNIST test images with 40% pixels observed, and perform mean-field update until the rel-
ative residual of [0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001] is reached (without step constraint),
respectively. Then we measure the TV distance between the distributions computed by these two
methods on the remaining 60% pixels, as well as the convergence speed. The results are demon-
strated in fig. 6. One can see that when the model is trained (using Krahenbuhl's, fig. 9a), the TV
distance converges to 0 as the tolerance decreases. However, when the model is just initialized
(but still constrained to be monotone), the TV distance remains large (fig. 6c). Even though in this
case the optimal fixed point may be unique, our method is still superior to Baque's: it takes us less
iterations till convergence, despite whether the model is trained or not..
15
Under review as a conference paper at ICLR 2022
Tolerance (decreasing)
(b) Number of iteration till convergence when trained
(d) Number of iteration till convergence at initialization
Figure 6: TV distance and convergence speed
(c) TV distance at initialization
7乙/ G
r，。匕
9 7”
O与
3 " 7 2
6 3，If
, fv3‰
∕* Λo t4 H-
“夕夕07
ð.Sqq 2
/ O ‰ Q 7
二夕70口
7≤^q d
IqnqU墟上
K 5;1，
，1 Q 5 〃
/• /b *l⅝∙ K
4夕夕◎ 7
OQ 4 9 2
/ O % ɔ 7
乙 m7cU
7 < 0» ɑ ʒ
K 5 f \，
，1 U 3 B
∕∙ /0 Ir. K
q夕夕O 7
ofo4 4 2
/ O ‰ ɔ 7
乙CK 7。々
7 ‹ 9ɑʒ
0nbpm
A5s∖ /
vt S 3 多
/061 —
q夕夕。7
OOqqN
/ O ‰ T-7
乙亍7。XJ
7 < Q- a ʒ
A 5s∖ /
⅛-t S 3 2
/f Λ^v Λ⅛v 4U <1»
“夕夕。7
ObyqN
/ O % ɔ 7
乙夕7。L
7 G 夕 ” 3
≡
O
(a) Krahenbuhl
(c) Our
(b) Baque
Figure 7: Training and inference using all three update rules with 40% observed pixels with the monotonicity
condition. The labels on each row represent the training update rule, and the labels on the columns represent
the inference update rule.
16
Under review as a conference paper at ICLR 2022
。■S£.- f
γj S 5z,
// Λo /O .1 if
√?夕。7
。修一夕2
/ O ¾ 07
乙彳7。乙
7 G q q 3
Z,	i	b	iA	l	ii	∖
O	fe	7	0	1	5
7	3	"	6	S	g
U	7	2	7	I	V	i
65£、f
，■^ S3。
∕Λ- Zb Ii Hr
q 夕 7 O 7
ofo"夕 2
/ O ‰ ŋ 7
乙夕7。乙
7g q q to
A ^ 5171 i
7l S 3 2
/OGSI
q夕夕07
Os q2
/ O ⅛ ɔ 7
乙 CK70ZJ
7 Gqe, 3
a∙ 5Γl /
V 1 ⅛ 3 i
/。6 1 I
“夕夕07
。金42
/ O ‰ ɔ* 7
乙 7702J
7G 0 4 3
堂军圈.流M辘港凝
辍重薄懿敏谶鑫黛
IqnqU墟立'onbEm
K 3 6、1
，—S 3 〃
«/ 1. H
4夕夕O 7
ofoH q2
/。<pr∙7
乙 77024
Zq 9 a 3
N、56 1 —
⅛ i ⅛- 3
/Ot 1 I
⅛ 7 </ O 7
<5foH q2
/。冬 r- 7
乙77&4Z
7 G 3
(a) Krahenbuhl

(c) Our
(b) Baque
Figure 8: Training and inference using all three update rules with 40% observed pixels without the monotonicity
condition. The labels on each row represent the training update rule, and the labels on the columns represent
the inference update rule.
Table 3: Classification error (standard deviation) when monotonicity is enforced
InferenCe Train	KrahenbUhl	Baque	Our
Krahenbuhl	0.042 (0.0013)	0114(0.0019)	0.0498 (0.0014)
Baque	0.958 (0.0013)	0038(0.0010)	0.034 (0.0012)
OUr	0.946 (0.0024)~	0.0425 (0.00T6T	0.0412 (0.0017Γ
CIFAR-10 model architecture Architecture is the same as for the MNIST experiments with the
following exceptions: A11 is a 20 × 20 × 3 × 3 convolution, A22 is a 24 × 24 × 3 × 3 convolution,
A21 is a 24 × 20 × 3 × 3 convolution with stride 2, A33 is a 48 × 48 × 3 × 3 convolution, A31 is a
48 × 20 × 3 × 3 convolution with stride 4, A32 is a 48 × 24 × 3 × 3 convolution with stride 2, A43 is a
(48 ∙ 8 ∙ 8) X 10 dense linear layer, and A44 is a 10 X 10 dense linear layer. The corresponding variable
q as in eq. (12) then has 4 elements of shape (60 × 32 × 32), (24 × 16 × 16), (48 × 8 × 8), (10 × 1).
When applying the proximal operator to q, we use 1, 6, 12, 1 as their number of groups, respectively.
CIFAR-10 training details Training details are the same as for the MNIST experiments with
the following exceptions: The model is trained for 100 epochs using standard data augmentation.
During the first 10 epochs, the weight on the reconstruction loss is ramped up from 0.0 to 0.5 and
the weight on the classification loss ramped down from 1.0 to 0.5. Also during the first 20 epochs,
the percentage of observation pixels is ramped down from 100% to 50%.
C Comparison to RBM/DBM
We conduct the same set of experiments on MNIST using DBM for comparison. We use a 3-
layer DBM where the first hidden layer has 300 neurons, and the last hidden layer (representing
the digits) has 10 neurons, amounting to in total 238,200 parameters. Our proposed model in the
MNIST experiments uses 165,300 parameters.
For image imputation, we randomly mask off 60% pixels, or a randomly selected 14 X 14 patch;
the results are shown in fig. 9. In the experiment with 60% of pixels randomly masked, we also test
17
Under review as a conference paper at ICLR 2022
Table 4: Classification error (standard deviation) when monotonicity is not enforced
Inference Train	Krahenbuhl	Baque	Our
Krahenbuhl	0.035 (0.0017)	0.189 (0.0023)	0.051 (0.0015)
Baque	0.762 (0.0013)	0.041 (0.0013)	0.055 (0.0012)
Our	0.90 (0.0002)~	0.063 (0.0021)~	0.036 (0.0017F
0 5 0 5 0 5 0
2 5 7 0 2 5
Ill
K 5，\ 〜
，1 S 3 &
/. /0 Ii H
“夕夕P 7
ofoq 4 2
/ O ‰ T- 7
乙77。"
7 < 9 4 3
0 5 0 5 0 5 0
2 5 7 0 2 5
Ill
外s6 1，
Y1 4 3 2
/ O 6 1-*
q夕夕O 7
。foy 夕2
/ O ¾ T7
乙770。
Zqerq 5
0 5 0 5 0 5 0
2 5 7 0 2 5
Ill
(a)	60% pixels are randomly masked. From left to right: imputed image, true image, masked image.
25
50
75
100
125
150
0	50	100	150	200
外 5F1，
γl V 3 rʌ
/r tɔ Λo H H
Q彳夕O 7
e>s 4 4 2
/ O¾Γ- 7
乙夕7。打
7 < 9 4 ʒ
25
50
75
100
125
150
0	50	100	150	200
S 5，1 1
，— S 3 力
/• /ŋ U H
“9707
。foτr 42
/ O % T 7
CSj7ɔz-CpZJ
7 C 9 4∙ ʒ
25
50
75
100
125
150
0	50	100	150	200
(b)	14 × 14 patches are randomly masked. From left to right: imputed image, true image, masked image.
Figure 9: DBM for image imputation
the model on predicting the actual digit simultaneously. The test accuracy is 93.58%. Our model
achieves comparable test accuracy (92.95%) and imputation compared to this DBM, given fewer
parameters and despite the the monotonicity constraint of our model.
The DBM is trained using CD-1 algorithm for 100 epochs with a batch size of 128 and learning rate
of 0.01. For imputation and classification, the DBM uses Gibbs sampling of 100 steps, although the
quality of the imputed image and test accuracy are insensitive to the number of steps.
18