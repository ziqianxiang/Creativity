Under review as a conference paper at ICLR 2022
Lagrangian Method for Episodic Learning
Anonymous authors
Paper under double-blind review
Ab stract
This paper considers the problem of learning optimal value functions for finite-
time decision tasks via saddle-point optimization of a nonlinear Lagrangian func-
tion that is derived from the Q-form Bellman optimality equation. Despite a long
history of research on this topic in the literature, previous works on this general
approach have been focusing on a linear special case known as the linear program-
ming approach to RL/MDP. Our paper brings new perspectives to this general ap-
proach in the following aspects: 1) Inspired by the usually-used linear V -form La-
grangian, we proposed a nonlinear Q-form Lagrangian function and proved that it
enjoys strong duality property in spite of its nonlinearity. The Lagrangian duality
property immediately leads to a new imitation learning algorithm, which we ap-
plied to Machine Translation and obtained favorable performance on standard MT
benchmark. 2) We pointed out a fundamental limit in existing works which ex-
clusively seeks to find the minimax-type saddle points of the Lagrangian function.
We however proved that another class of saddle points, the maximin-type ones,
turn out to have better optimality property. 3) In contrast to most previous works,
our theory and algorithm are oriented to the undiscounted episode-wise reward,
which is practically more relevant than the usually considered discounted-MDP
setting, thus have filled a gap between theory and practice on the topic.
1	Introduction
Episodic learning Terry (2017) is a general learning paradigm in which the agent learns based on
data collected from a sequence of episodes of environmental interactions. Each episode consists of
a finite number of decision steps, and the goal is to maximize an episode-wise performance. With
one-step episodes, episodic learning subsumes supervised learning and bandit problems as special
cases; for sequential decision making problems, an episode may contain multiple (and possibly
varying number of) steps, in which case episode learning encompasses most reinforcement learning
and imitation learning problems encountered in current AI practice.
A pervasive idea of episodic learning, often called the value-based approach Sutton and Barto
(2018), seeks to find a value function such that greedily selecting the actions with the best values (as
prescribed by this function) gives good performance. For example, in most classification tasks, most
deep neural networks are learned (and used) to represent such a value function. Finding good value
functions can be highly challenging, especially for tasks with multi-step episodes. In this case, the
classic Bellman optimality equation Bertsekas (2019) gives a sufficient condition of optimal value
function (i.e. value functions with which greedy decisions have optimal performance).
In this paper, we study a Lagrangian method which learns optimal value functions by characterizing
them as saddle points of the Lagrangian function of a variational (re-)formulation of the Bellman
equation. The method reveals an elegant thoery on policy-value duality in machine learning. A
restricted form of this high-level idea has been known as the Linear Programming (LP) approach in
the literature of dynamic programming Puterman (1994); Si et al. (2004); Chen and Wang (2016);
Wang (2017), which has also been introduced to reinforcement learning in recent years Dai et al.
(2018); Lee and He (2018); Nachum et al. (2019); Yang et al. (2020); Nachum and Dai (2020).
This paper provides a new perspective to the foundation of this elegant idea through the following
contributions:
Firstly, previous works are mostly limited to the discounted-reward setting, which bypass some
technical difficulties Nachum and Dai (2020), but deviate from the practical settings of many (if not
1
Under review as a conference paper at ICLR 2022
most) real-world tasks. This paper fills this well-known gap between theory and practice by estab-
lishing a value-based theory (and algorithm) that targets at directly learning optimal value functions
and policies for the undiscounted episode-wise reward (see (1) in Section 2).
Secondly, previous works focused on linear formulations, which enjoys the strong duality property
and generic LP techniques, but are consequently limited to treatments for the state-value functions
(i.e. V-functions) in policy optimization setting. However, the optimal state-value function cannot
be directly used to derive optimal policy in model-free learning. In contrast, we proved a minimax
theorem for the nonlinear Lagrangian functions associated with the action-value functions (i.e. Q-
functions). The theorem opens a door to nonlinear variational treatment to the Bellman equation,
which now enjoys the strong duality property too (see Section 3).
In particular, we developed a simple imitation learning algorithm based on the Lagrangian duality
thus established, and applied the algorithm to Machine Translation (MT) as case study. Transformer
models trained by our algorithm achieves beam-search-level performance Vaswani et al. (2017) with
only greedy decoding, and leads to 1.4-BLEU improvement when also equipping with beam search
(see Section 4).
Last but not the least, previous works have exclusively focused on the primal-form optimization
formulation of the Bellman equation, thus are limited to solving the minimax-type saddle points of
the corresponding Lagrangian function. We however showed that the minimax saddle points are not
necessarily optimal value functions in learning settings. Instead, another class of the Lagrangian
saddle points - the maximin-type saddle points which are derived from the dual-form optimization
formulation - turn out to have rigorous guarantee on optimality. This observation points to new
directions for this decades-old topic (see Section 5).
2	Episodic Learning Process and B ellman Optimal Value
We start with introducing the mathematical formulation of episode learning problems, as well as the
formal definitions of related concepts.
An infinite-horizon MDP is a tuple (S, A, R, P, ρ0), where S is the state space, A the action space,
R(s) ∈ [rmin , rmax] is a bounded reward (possibly negative) associated to each state s ∈ S, 1
P(s0|s, a) specifies action-conditioned transition probabilities between states, and ρ0 is the initial
distribution with So 〜ρo. A MDP is finite if both S and A are finite sets. A policy function
π : S × A → [0, 1] specifies the action selection probabilities under each state, which induces
a markov chain Pn[St+ι = s0∣St = s] = P°∈aP(s0∣s,a) ∙ π(a∣s). Let Π denote the policy
space, i.e., the set of all policies. Without loss of generality, we assume every state s in the state
space S is reachable from the initial state under at least one policy, 2 where reachable means ∃π ∈
Π, Pt∞=0 Pπ [St = s] > 0.
An Episodic Learning Process (ELP) White (2017); Bojun (2020) is an infinite-horizon MDP that
repeatedly encounters into, and is reset by, a group of terminal states S⊥ (despite its name, a
terminal state does not terminate the ELP). Formally, an infinite-horizon MDP is an ELP if there is
a non-empty subset S⊥ ⊆ S such that (1) all terminal states have homogeneous and action-agnostic
outbound probabilities: P(s0|s1, a1) = P(s0|s2, a2), ∀s1, s2 ∈ S⊥, ∀a1, a2 ∈ A, ∀s0 ∈ S, (2) the
initial state is a terminal state: ρ0(s) = 0, ∀s 6∈ S⊥, and (3) the average episode length is finite under
any policy: ∀π ∈ Π, Eπ [T] < ∞, where T =. inf{t ≥ 1 : St ∈ S⊥} is called the termination time.
Bojun (2020) proved that in any ELP, every policy π ∈ Π has a unique stationary distribution
ρπ (s, a) such that Pπ [St+1 = s, At+1 = a] = ρπ (s, a) if Pπ [St = s, At = a] = ρπ (s, a).
Following White (2017) and Bojun (2020), we use the ELP formalism to study finite-time decision
tasks, which arguably account for most AI tasks encountered in current practice. The ELP model
formulates the learning process of such task, which consists of an infinite number of concatenated
episodes, each starting and ending at an terminal state. In general, an episode can be arbitrarily long,
but will terminate in finite steps with probability 1. The learning agent may choose to use a different
behavior policy βt for each step t of the episodic learning process (based on data collected from
1Our state-based reward formulation follows Schulman et al. (2015) and Bojun (2020), and is equivalent to
action-based reward formulations in terms of expressiveness. Refer to Appendix A of Bojun (2020) for details.
2Unreachable states are irrelevant to the real world whatsoever, so are excluded from our treatment.
2
Under review as a conference paper at ICLR 2022
previous steps), and the goal is to have βt converge (as t goes to infinity) towards an optimal policy
that maximizes the expected episode-wise cumulative reward:
J ⑺=ZEnhXT(Z)R(St)i
(1)
where ζ = (S0 , S1 , S2 , . . . ) is an infinite trajectory of the markov chain induced by policy π. Note
that the termination time T is a random variable whose value may vary for different trajectory ζ .
See Section D.3 for the ELP formulation of machine translation as an example.
A value function Q : S × A → R assigns a real number to each state-action pair (s, a) as the
“perceived benefit” of doing action a under state s. We call the set of all possible value functions,
the value space, denoted by Q. A value function Q ∈ Q is called an optimal value if (any of the)
Q-greedy policy with ∏Q(a∣s) > 0 ⇒ Q(s, a) = max« Q(s, a) is an optimal policy.
Bellman optimal value is a special optimal value function that is characterized by the Bellman opti-
mality operator. In its general form Degris et al. (2012); Sutton et al. (2011; 2014; 2016), a gener-
alized Bellman optimality operator Bγ : Q → Q transforms a value function Q ∈ Q into another
value function BγQ ∈ Q such that for any (s, a) ∈ S × A,
BY Q(s, a) = ^X P(s0∣s, a) ∙(R(s0) + γ(s0) ∙ max Q(s0, a0))
s0∈S
(2)
where γ : S → [0, 1] is a discounting function over the states.
As a classic result, if the discounting function equals a constant γc < 1 on every state, the corre-
sponding Bellman optimality operator Bγc has a unique fixed point in the value space of any MDP.
However, the fixed point of Bγc is generally not an optimal value with respect to the undiscounted
objective (1). In the following, we present a new theorem that guarantees the uniqueness of Bellman
fixed point for a more general class of discounting functions, among which a particular “episodic
discounting function” gives optimal value w.r.t. objective (1). Both the uniqueness and the opti-
mality are fundamentally rooted from an inherent graph property of ELP-MDPs. See the proof in
Appendix A for detailed elaboration.
Theorem 1. In any finite Episodic Learning Process (S, A, P, R, ρ0), let γ be any discounting
function such that γ(s) < 1 for all terminal state s ∈ S⊥, then
(1)	Bγ has a unique fixed point, i.e., the equation Q = BγQ has a unique solution.
(2)	The fixed point of Bγ is the limiting point of repeatedly applying Bγ to any Q ∈ Q.
(3)	The fixed point of Bγ is an optimal value w.r.t. objective (1) if γ is the following episodic
discounting function:
γ epi (s)	=.	1[s	6∈	S⊥]	=	01	ffoorr	3 * ss	6∈∈ SS⊥⊥.	(3)
Since we focus on optimizing objective (1), in the rest of the paper: Bellman optimality operator,
denoted by B =. Bγ epi , refers to the specific operator (2) that uses the particular episodic discounting
function (3); accordingly, Bellman optimal value, denoted by Q*, refers to the unique fixed point
of B; and similarly, Bellman optimality equation refers to the fixed-point equation Q = BQ under
episodic discounting, or more explicitly, refers to the following system of non-linear equations:
Q(s, a) = X max P(s0∣s, a) ∙(R(s0) + Yepi (s0) ∙ Q(S, a0))	, ∀(s, a) ∈ S ×A (4)
s0∈S
It is worthwhile noting that although the Bellman optimal value function is unique, there can be
many optimal value functions in an episodic learning problem. In particular, any value function that
gives the same preference order with the Bellman optimality value is an optimal value function.
3	Lagrangian Duality and Minimax Theorem
In this section we study a minimization-based variational treatment to the Bellman optimality equa-
tion (4). We will derive a practical episodic learning algorithm based on the theory presented here,
and will apply the algorithm to standard machine translation benchmarks in the next section.
3
Under review as a conference paper at ICLR 2022
Our idea is inspired by the long-known linear programming re-formulation of the closely related
optimality equation for state-value functions V : S → R,
V(S) = maxXP(s0∣s,a) ∙(R(s0) + Yepi (s0) ∙ V(s0)) ,	∀s.	(5)
s0
Both the Q-form optimality equation (4) and the V -form optimality equation (5) are nonlinear due
to the max operation inside, but the V -form equation (5) admits a linear re-formulation Puterman
(1994):
min X Po(s)∙ V(S) s.t. V (S) ≥ X P(SlS,a)∙(R(s')+ Y epi (s0)∙ V(s')) ,	∀(s, a) (6)
s∈S	s0
Thanks to its linearity, (6) enjoys the standard LP duality properties, and in particular has minimax
equality for its corresponding Lagrangian function, which is the basis for a recently revived thread
of research on the LP approach to MDP and RL Chen and Wang (2016); Wang (2017); Cho and
Wang (2017); Dai et al. (2018); Nachum and Dai (2020).
In model-free learning settings, however, the agent does not know the transition function P, thus
cannot directly use a state-value function V to compare the values of different actions (as we do
not know which states an action would lead to). For this reason, most value-based algorithms focus
on learning the optimal Q functions Watkins (1989); Hasselt (2010); Mnih et al. (2015); Haarnoja
et al. (2018). It is thus natural to ask if we can develop a variational approach directly for the Q-form
optimality equations (4), similar to what we did to the V -form optimality equations.
Indeed, the Q-form optimality equations (4) can be similarly recast into a constrained optimization
problem as follows:
min
Q
s.t.
E	Q(ST,AT)
Z~π L	」
(7)
Q(s, a) ≥ X max P(s0∣s, a) ∙ (R(SO) + Yepi (s0) ∙ Q(s0, a0))	,	∀(s, a)
s0
where π ∈ Π can be an arbitrary policy, and T =. inf{t ≥ 1 : St ∈ S⊥} is the termination time.
Unfortunately, unlike the V -form optimality equation for which the nonlinear operation maxa can
be “unpacked” into |A| linear constraints (cf. (5) and (6)), the variational formulation (7) of the
Q-form optimality equation is still nonlinear, due to the maxa0 “wrapped” inside Ps0. As a result,
although (7) can still be written into the Lagrangian form
min max E [q(St ,At )] + ^X λ(S,a) ∙ (bQ(s,o) 一 Q(S,a)),	(8)
s,a
it is unclear if the nonlinear Q-form Lagrangian still enjoys strong duality, a key property for de-
signing effective and principled learning algorithms Wang (2017); Dai et al. (2018).
In the following, we give an affirmative answer to this open question by proving a minimax theorem
for the nonlinear Lagrangian of the Q-form Bellman optimality equation (4). As with Theorem 1,
the strong Lagrangian duality is also rooted from inherent structures of the episodic learning process.
Definition. Given a finite Episodic Learning Process (S, A, P, R, ρ0), a function Lπ : Q ×
R≥S0∙lAl → R is called a Lagrangian function with conjugate policy ∏ if
L∏(Q,λ)= E. [q(St ,At )] + X λ(S,a) ∙(BQ(S,a)- Q(S,a))	(9)
s,a
where π ∈ Π can be an arbitrary policy, and T =. inf {t ≥ 1 : St ∈ S⊥ } is the termination time.
Note that the conjugate policy π only determines the distribution of terminal states (and actions) in
the first term of Lπ . With any conjugate policy π, the second term of Lπ always uses the Bellman
optimality operator B. Let Us first confirm that Q* is a minimax solution of the Lagrangian function
(9), as Lemma 2.1 states. The proof can be found in Appendix B.1.
Lemma 2.1. In any finite ELP, for any conjugate policy π, Q* ∈ arg min max Lπ (Q, λ).
4
Under review as a conference paper at ICLR 2022
Now, we observe that the Lagrangian Lπ has an equivalent form when the Lagrangian multiplier
vector is a special vector λπ that is proportional to the stationary distribution of the conjugate
policy π (and proportional to the average episode length of π too).
Lemma 2.2. In any finite Episodic Learning Process (S, A, P, R, ρ0), for any conjugate policy π,
let Lπ be the corresponding Lagrangian, and let λπ be the particular Lagrangian multipliers with
λ∏(s, a) = ρ∏(s, a) ∙ EZ〜∏ [T], where ρ∏ is the stationary distribution of π, then
Lπ(Q,λπ)=J(π)+XX
λ∏(s, a) ∙ (m@xQ(s, α) 一 Q(s, a))	(10)
s6∈S⊥ a∈A
Proof idea: Applying a known ergodic theorem of ELP, we can transform the first term of (9) from
an average over trajectories to an average over the state-action space (see Appendix B.2 for details):
ZEnhQ(ST ,AT)]= ZE∏[T]∙ SRh(I-Y epi (S)) ∙ Q(S，A)i
Then substituting the above equation to (9), yields
Ln (Q, λn )= E∏[T ] ∙ E h E [R(S0) + γ epi (S0) ∙ max Q(S0,a0)] - Y epi (S)Q(S,A)]
S,Α^ρπ Ls0-P(∙∣s,α)	a0	」
=En [T ] ∙ E	[r(S)+ Y epi (S) ∙ max Q(S,a0) - Y epi (S)Q(S,A)]
S,A^ρ∏	a0
=J(∏)	+ X	E∏ [T]	∙	ρ∏(s,	a)	∙ 1[s	∈	S⊥]	∙ (maxQ(s,	a0) — Q(s, a))
s∈S,a∈A
In above, We have again used the ergodic theorem of ELP to transform E∏ [T] ∙ ES〜ρπ [R(S)] back
to the trajectory space, so as to derive J(∏). See Appendix B.2 for the complete proof.	□
The first term in the dual form of the Lagrangian, i.e. in (10), is the true performance of the conjugate
policy π (in terms of objective (1)). Utilizing this fact, We can prove the strong duality property for
Lagrangians conjugating With optimal policies, as the folloWing theorem states.
Theorem 2 (ELP Minimax Theorem). In any finite Episodic Learning Process (S, A, P, R, ρ0), if
μ ∈ Π is an optimal policy, then its conjugate Lagrangian Lμ has strong duality property, with
min max	Lμ(Q,	λ)	= max	min Lμ(Q, λ)	= J(μ)	(11)
Q∈Q λ≥0	μ3 ' λ≥0	Q∈Q μ3 ' W	' '
Proof idea: Let ∏* denote a Q *-greedy policy, which is thus an optimal policy.
For any conjugate policy ∏, since Q* is a minimax solution of Ln (Lemma 2.1), we have
minQmaxλ≥oLn(Q, λ) = maxλ≥o L∏(Q*,λ) = E∏[Q*(St,at)] = E∏[Q∏*(Sτ,at)]. Be-
cause J(π*) = Qn* (s⊥, a) for any terminal state s⊥ ∈ S⊥ and any action a ∈ A (see Appendix
A.3), we have En [Qn* (St, AT)] = J(π*), and so minQ maxλ≥o Ln(Q, λ) = J(π*).
Again for any conjugate policy π, due to Lemma 2.2, the Lagrangian dual Ln(Q, λn) for the par-
ticular multiplier λn attains its minimum when Q achieves complementary slackness with λn in the
second term of (10), in which case minQ∈Q Ln (Q, λn ) = J(π).
So now, when the conjugate policy ∏ is an optimal policy μ, as assumed in the theorem, we have
max min Lμ(Q, λ) ≥ min Lμ(Q, λμ) = J(μ) = J(π*) = min max Lμ(Q, λ).
λ≥0 Q∈Q μ	Q∈Q	Q λ≥0	μ
The inequality above must actually be an equality because of the universally-held weak duality of
the Lagrangian. See Appendix B.3 for a more detailed proof.	□
From the proof above we can also see that (Q*, π*), as the fixed point of Bellman optimality operator
B and the corresponding Q*-greedy policy (resp.), forms a minimax saddle point of the Lagrangian
Lμ. In fact, we can derive general conditions for all saddle points of this kind by combining the
complementary slackness conditions in (9) and (10). See the proof in Appendix B.4.
Proposition 3. In any finite ELP, for any value function Q ∈ Q and any policy π ∈ Π, letλn(s, a) =
Pn(s,a) ∙ EZ 〜n [T ] ,then Ln (Q, λ) ≤Ln(Q,λn) ≤Ln (Q, λn) , ∀Q, λ, ifandonlyif
(1)	BQ(s, a) - Q(s, a) ≤ 0	, ∀(s, a) ∈ S × A
(2)	Pn(s,	a)	∙(BQ(s, a) — Q(s, a)) = 0	,	∀(s, a) ∈ S	×A
(3)	Pn(s,	a)	∙	( maxa Q(s, α) - Q(s, a))	= 0	,	∀s ∈ S⊥, a	∈ A
5
Under review as a conference paper at ICLR 2022
4 Algorithmic Application to Machine Translation
From last section We know that the Q-form Lagrangian Lμ has strong duality where μ is optimal
policy, and that the special Lagrangian multiplier λμ forms a minimax saddle point (or minimax
equilibrium) with the solution of (7). Formally,
max J(π) = min max Lμ(Q, λ) = max min Lμ(Q, λ) = min Lμ (Q, λμ)
∏	Q λ≥0 μ	λ≥0 Q	Q
A simple idea to find such minimax saddle point is to minimize the rightmost side of the above
equation, that is, to minimize Lμ(Q, λμ) over the value space Q. Although the closed form of the
objective function Lμ(Q, λμ) depends on an optimal policy μ (which might appear paradoxical at
a first glance as coming up with such an optimal policy was our original goal of learning), note that
estimating the gradient of Lμ(Q, λμ) only requires knowledge about a “trace” of the optimal policy
μ, instead of explicit knowledge about how μ is constructed. Specifically, for parameterized value
function Q(s, a; w), we want to estimate RWLμ(Q(w), λμ), that is,
Vw (Eμ[Q(Sτ ,At ; w)] + Eμ[T L 4 E	[R(So) + Y epi (S0) max Q(S,“； W) - Q(S, A; w)]) (12)
in which the optimal policy μ only plays a role in data weighting - it determines the population
distribution of the terminal state-actions (ST, AT), of the terminal time T, and of the transition
variable (S, A, S0). This observation inspires a general imitation learning approach, named LA-
grangian MINimization (LAMIN) here, in which we try to collect some demonstration data from
an optimal policy, based on which we construct an estimator of the Lagrangian gradient (12), then
apply standard stochastic gradient procedures to approximate the value function that minimizes
Lμ(Q(w), λμ), which corresponds to a minimax point of the original Lagrangian function (9).
One may design different estimators of (12) to form different algorithms under the LAMIN idea. In
the following we discuss two, and they both perform reasonably well in our validation experiment.
LAMIN1: A technical challenge of estimating (12) is to deal with the max operator in it. One idea
is to relax the Lagrangian function into a smoothed version:
Le(Q(W), λμ) = Eμ[Q(Sτ, AT ； w)]+Eμ[T ]∙
S,a,S0 〜Pμ A0 〜∏Q (w)(S0)
δ(S, A, S0, A0; w) (13)
where πQβ (W)(a|s) =.
exp(Q(s,a;w)/e)
Pb exp (Q(S,b;W)∕β)
is the Boltzmann distribution with temperature β, and
δ(s, a, s0, a0; W) =. R(s0) + γepi (s0)Q(s0, a0; W) - Q(s, a; W) is the temporal-difference error.
The smoothed Lagrangian L1 subsumes the original Lagrangian Lμ as the limiting case β → 0,
while is readily differentiable. Thus, the LAMIN1 algorithm seeks to minimize the smoothed La-
grangian L? via SGD with unbiased gradient estimator of (13). See the pseudo-code as well as
more nuanced discussion on this algorithm in Appendix D.1. Among others, it turns out that the
β-smoothing trick used in LAMIN1 is more than an approximation heuristic, but may potentially
serve as a practical correction to an inherent bias of the minimax points of the Lagrangian that we
will discuss in Section 5.
LAMIN2: Another idea is to stick to the exact Lagrangian L* but seek to construct “local” estimator
of its gradient in a per-step manner. Specifically, let Wt be the parameter vector of Q that is to be
updated at a gradient step t, then We can estimate the value of RwLμ(Q(w), λμ) IW=WJ for this
particular paramter Wt , with
Vw (Eμ[Q(Sτ, At; w)] + Eμ[T]	E	E
∖	S,A,S0 〜Pμ A0 〜∏Q(w*)(S0)
hδ(S, A, S0, A0; w)i
(14)
w=wt
E
E
where ∏q(w*) is the Q(w*)-greedy policy (See Section 2). Note that the greedy policy ∏q(w*) in
(14) does not depend on W and thus is invariant to RW; in contrast, the Boltzmann policy ∏Q(W)
used by LAMIN1 (see (13)) will take part in the gradient computation.
The consistency of the estimator (14) is characterized by the following mathematical fact.
Proposition 4. Let A be a finite action space, Q(s, a; W) a parameterized value function.
If with a given parameter vector w*, Q(w*) suggests a unique best action amaχ(s; w*) =
arg maXa∈A Q(s, a; w*) for state S , then
RW max Q(s, a; w) I = E	RW Q(s, a;
a∈A	I w=w*	a〜∏Q(w*)(s; W* ) L
W*
6
Under review as a conference paper at ICLR 2022
See the proof in Appendix D.2. In practice, the condition in Proposition 4 - i.e. Q(w*) gives unique
best actions - may be expected for large-scale continuously-valued Q functions, as it is unlikely that
the real-numbered values of two actions happen to be identical under stochastically generated w.
Moreover, the local gradient estimator (14) can be combined with the β-smoothing trick by replac-
ing the greedy policy ∏q(w*) in (14) with Boltzmann policy πQ(句*), forming the smoothed local
gradient estimator that the LAMIN2 algorithm will use. See the pseudo-code in Appendix D.2.
Empirically, we found that higher temperatures help slightly improve the performance of LAMIN2.
We now apply LAMIN algorithms to Machine Translation (MT) as a case study. Learning to trans-
late is a highly impactful AI application Wu et al. (2016), and is also an excellent example of episodic
learning problem: A translation episode starts with a given sentence in a source language, and the
agent takes actions to generate translation tokens one by one in a sequential manner. The episode
terminates when the agent outputs a special end-of-sentence (EOS) token, at which point the trans-
lation quality is evaluated based on the source sentence and the generated translation sentence. Note
that in MT, the episode length (i.e. the length of the translation sentence) is a variable controlled by
the agent policy, and the episode-wise reward (i.e. the translation quality) is generally not discounted
for long episodes. Just the opposite, standard MT metrics such as BLEU Papineni et al. (2002) uses
brevity penalty factor to discount the reward of short (instead of long) translations/episodes. See
Appendix D.3 for the exact episodic learning formulation of MT.
Given a source sentence X, most MT metrics measure the quality of a generated translation Y by
comparing the similarity between Y and some reference translation Z of X, where Z is typically
provided by human expert given X . It follows that the translation policy used by the human expert
(which maps X to Z) must be an optimal policy under such metric. Importantly, a trace of the
optimal translation policy, in the form of a collection of source-reference sentence pairs, is indeed
widely available in standard MT benchmarks, which can be readily used to power LAMIN algo-
rithms. In general, the same idea applies to all machine learning problems with imitation reward
where the performance metric is based on similarity comparison to reference/ground-truth outputs.
We tested our algorithmic ideas using the WMT English→German (en2de) dataset, one of the most
influential MT benchmarks. We parameterized value function Q with the standard TransformerBase
neural network Vaswani et al. (2017), and trained the Transformer-based Q-function using LAMIN1
and LAMIN2, with varying temperature β, then tested the Q-greedy policy with standard BLEU
metric using the WMT’14 NewsTest data. See Appendix D.4 for details in experimentation setup.
二二一」：一 一0.0
050505050
520752075
工，，6.6.6.&5.5.
222222222
①」OUS nτlmSnd
temperature (beta)
Figure 1: Corpus BLEU scores of LAMIN1
and LAMIN2 under different Boltzmann
temperature β . Greedy policy is used in both
cases. The gray dashed line indicates the
baseline performance of 27.3 achieved by a
beam-search policy in Vaswani et al. (2017).
# of gradient updates
Figure 2: Learning curves of some LAMIN
variants. Beam search with a beam size of 4
is used for the orange line, which matches the
same setting with the baseline result of 27.3
(gray dashed line). Other lines use greedy
policy which is about 2x faster.
Figure 1 illustrates the performance of LAMIN1 and LAMIN2 under a spectrum of Boltzmann
temperatures, from 0.01 (in which case the algorithm is effectively minimizing the exact Lagrangian
function) to 1.0 (which corresponds to the softmax distribution). Table 1 and 2 in Appendix D.4
reports the numerical scores. We see that LAMIN1 has slightly better performance than LAMIN2
7
Under review as a conference paper at ICLR 2022
under most temperatures. Interesting, higher temperatures tend to help the performance of LAMIN2
but hurt that of LAMIN1 (albeit with limited margins in both cases).
Notably, LAMIN1 with β = 0.01 attains a BLEU score of 27.4 with a greedy policy. In comparison,
when standard supervised learning is used, the same neural network famously attains 27.3 only if
further combined with a systematic beam search Vaswani et al. (2017). So, our algorithm achieves
comparable performance with the state-of-the-art result without time-consuming search procedure
at decision/translation time. On the other hand, when incorporating with the same beam search
procedure, the value function trained by LAMIN1 (with β = 0.01) attains 28.7, or 1.4 BLEU score
higher. Figure 2 shows the learning curves of the algorithm, with and without beam search.
5 Minimax Values vs Maximin Values
The Lagrangian method we explored so far has focused on a particular class of Lagrangian saddle
points, the minimax saddle points arg minQ maxλ Lπ, which correspond to exactly the set of op-
timal solutions for the minimization-form (or primal-form) variational problem (7). Let’s call such
a value function, a minimax value function. It seems that existing research on the Lagrangian
method have been exclusively focusing on studying minimax value functions (or minimax points of
the Lagrangian); see Puterman (1994); Cho and Wang (2017); Dai et al. (2018); Nachum and Dai
(2020) for examples. Indeed, as Lemma 2.1 confirmed, the Bellman-optimal value Q*, as an optimal
value function, is a minimax value function. However, a minimax value function, or equivalently, a
minimax solution of Lπ (for any π), is not necessarily an optimal value function.
Figure 3 shows an example, where state 4 and 5 are terminal states, from which any action leads to
state 0. Choosing action 1, 2, 3 under state 0 determin-
istically transits to state 1, 2, 3, respectively. All actions
under state 1 lead to state 4, and all actions under state
2 and 3 lead to state 5. The agent only receives non-zero
rewards at terminal states, with R(4) = 1, R(5) = 2. The
initial state is set to state 4. Clearly, an optimal policy of
this ELP should only choose action 2 or 3 (or both), but
not action 1, under state 0.
One can verify that for this ELP, the constant value func-
tion Qmin(s, a) ≡ 2 is a minimax value function. In par-
ticular, We have 2 = Qmin (0,1) = 0+1 ∙ Qmin (1, a) >
1+0 ∙ Qmin (4,a) = 1, which does satisfy the Con-
straints of (7) (that Q ≥ BQ). But the constant value
function Qmin is certainly not optimal as it assigns the
same value to all actions under state 0. Moreover, be-
r = l
r = 2
Figure 3: An ELP example
cause Qmin(1, a) > BQmin(1, a) = 1 for all a, the λ
corresponding to Qmin needs to have λ(1, a) = 0 for all a. Such a λ cannot encode any policy.
The problem with minimax value functions is that they only guarantee optimality for optimal actions,
but do not enforce sub-optimality for sub-optimal actions (such as the action 1 under state 0). This
inadequacy of minimax value functions may explain our experimental observation in last section
that adding a softmax temperature in the Lagrangian leads to better value functions (because the
softmax averaging motivates, to some extent, the optimizer to downgrade sub-optimal actions to
further lower the Lagrangian). Moreover, we remark that this is not only a problem for Q-functions
or for episodic discounting, but the minimax solutions of the more popular V -form Lagrangian in
discounted-MDPs Dai et al. (2018) suffer from the same issue too (see Appendix C.4 for a counter-
example for the minimax V -functions in discounted setting).
Interestingly enough, it turns out that value optimality is indeed guaranteed for another class of
saddle points of the Lagrangian - the maximin saddle points. Specifically, with the same argument
as in Lemma 2.1 one can show that Q*, the fixed point of B, is also the optimal solution of the
following “mirrored” problem to (7). See Appendix C.1 for the proof.
Lemma 5.1. In any finite ELP, Q* is an optimal solution of
max E	Q(ST,AT)	s.t. Q(s, a) ≤ BQ(s, a) , ∀(s, a)
Q Z 〜π L
(15)
8
Under review as a conference paper at ICLR 2022
for any conjugate policy π ∈ Π. Equivalently, Q* ∈ arg max min Ln (Q, λ).
We call an optimal solution of (15), a maximin value function (with respect to the Lagrangian Lπ).
Different from minimax value functions, a maximin value function enforces sub-optimality for all
(truly) sub-optimal actions, and at the same time can guarantee optimality for at least one (truly)
optimal action. Consequently, a maximin value function always and only induces optimal policy.
Theorem 5. In any finite ELP, for any conjugate policy π, let Qmax be an maximin value function
ofthe Lagrangian Ln - i.e. let QmaX be an optimal solution of (15) - then QmaX is an optimal value
function, in the sense that Qmax-greedy policy maximizes the episodic-reward objective (1).
Proof idea: First observe that QmaX(s, a) ≤ Q* at all (s, a). This is because QmaX, as a feasible
solution of (15), has QmaX ≤ BQmaX ≤ BBQmax …≤ Q*. NoW QmaX ≤ Q* implies that for
all actions sub-optimal to Q* (under a state), they can only have even lower values in QmaX . So,
it is enough to prove that maxa QmaX(s, a) = maxa Q* (s, a) at every non-terminal state s that is
reachable by QmaX-greedy policy - in that case the footprint of a QmaX-greedy policy will be a
subset of the footprint of a Q*-greedy policy, in Which only Q*-optimal actions are selected. Note
that QmaX ’s values on terminal states do not affect its footprint, due to the ELP conditions.
For an arbitrary state-action pair (s, a), let (S × A)next denote the set of all the non-terminal (s0, a0)
pairs that can directly follow (s, a) under the QmaX-greedy policy. We can prove that
Q maX(s, a) = Q*(s, a)
⇒QmaX(S	, a ) = Q (s,a ) and	max QmaX(S	,	a)	=	max Q (s , a) ,	∀(s	, a ) ∈ (SX	A) next
a	a
which would enable a proof-by-induction, starting from some terminal state where QmaX and Q*
equal to each other (such terminal state exists because both QmaX and Q* are optimal solutions of
(15)). See Appendix C.2 for the complete proof.	口
In light of the symmetry breaking between the minimax-type and the maximin-type Lagrangian
saddle points (in terms of optimality), we believe the latter, as well as minimax value functions and
the corresponding dual-form variational problem (15), deserve more attentions from the community.
6	Related works
A main challenge of this work comes from the non-linearity in both the Bellman optimality equation
and the associated Q-form Lagrangian being studied. In contrast, most related research focused on
linear treatments to related objects. For a linear policy-specific Bellman operator Bn (which replaces
maxa with a linear average), White (2017) proved that Bn has unique fixed point in episodic learning
setting. The linear operator Bn leads to a LP (re-)formulation, based on which the “DICE” family of
policy evaluation algorithms were developed Nachum et al. (2019); Yang et al. (2020). On the policy
optimization side, an active thread of research used saddle-point optimization to solve the linear V -
form Lagrangian, again relying on the generic LP duality inherited from the linear treatment Chen
and Wang (2016); Wang (2017); Cho and Wang (2017); Dai et al. (2018); Chen et al. (2018); Serrano
and Neu (2020); Si et al. (2004). The underlying techniques in these linear settings are not directly
applicable to our nonlinear treatment in this work.
The disparity between discounted formalism and episodic learning practice is well recognized in
reinforcement learning literature. The RL textbook Sutton and Barto (2018) devoted its Section
10.4 to the issue of deprecating the discounted formalism. The DP textbook Bertsekas and Tsitsiklis
(1996) subsumed the discounted setting as a special case of an finite-termination setting. A special
case of Theorem 1 dedicated to episodic discounting was proved by Bertsekas and Tsitsiklis (1991).
The WMT machine translation benchmark used in this paper is a fruitful driver of the rapid techni-
cal advances in Neural Machine Translation in recent years Wu et al. (2016); Koehn and Knowles
(2017); Vaswani et al. (2017). MDP-based techniques have been actively studied as a promising
method for this problem Ranzato et al. (2016); Edunov et al. (2018); Bahdanau et al. (2017); Wu
et al. (2018), but with relatively limited effectiveness observed so far Choshen et al. (2020). To our
best knowledge, the LAMIN algorithm is one of the first MDP-based solutions that is able to train
Transformer-scale neural networks independently (without pretraining or ensemble learning with
the aid of other techniques) to attain competitive performance on the WMT benchmark.
9
Under review as a conference paper at ICLR 2022
7	Reproducibility Statement
Every theorem, lemma, proposition in the main text or in the appendix of this paper has been given
complete proof (unless already proved in other papers in which case the reference was given). For the
experiment part, detailed experimentation setting was described in Appendix D.4. Experimentation
code was attached in Supplementary Material.
References
Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron
Courville, and Yoshua Bengio. An actor-critic algorithm for sequence prediction. 2017.
Dimitri P Bertsekas and John N Tsitsiklis. An analysis of stochastic shortest path problems. Math-
ematics ofOperations Research,16(3):580-595,1991.
Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic programming. Athena Scientific, 1996.
D.P. Bertsekas. Reinforcement Learning and Optimal Control. Athena Scientific optimization and
computation series. Athena Scientific, 2019. ISBN 9781886529397. URL https://books.
google.co.jp/books?id=ZlBIyQEACAAJ.
Huang Bojun. Steady state analysis of episodic reinforcement learning. In Advances in Neural
Information Processing Systems (NeurIPS), volume 33, pages 9335-9345, 2020.
Yichen Chen and Mengdi Wang. Stochastic primal-dual methods and sample complexity of rein-
forcement learning. arXiv preprint arXiv:1612.02516, 2016.
Yichen Chen, Lihong Li, and Mengdi Wang. Scalable bilinear pi learning using state and action
features. In Proceedings of the 35th International Conference on Machine Learning, volume 80,
pages 834-843. PMLR, 10-15 Jul 2018.
Woon Sang Cho and Mengdi Wang. Deep primal-dual reinforcement learning: Accelerating actor-
critic using bellman duality. arXiv preprint arXiv:1712.02467, 2017.
Leshem Choshen, Lior Fox, Zohar Aizenbud, and Omri Abend. On the weaknesses of reinforce-
ment learning for neural machine translation. In International Conference on Learning Represen-
tations, 2020.
Bo Dai, Albert Shaw, Niao He, Lihong Li, and Le Song. Boosting the actor with dual critic. In
International Conference on Learning Representations, 2018. URL https://openreview.
net/forum?id=BkUp6GZRW.
Thomas Degris, Martha White, and Richard S Sutton. Off-policy actor-critic. arXiv preprint
arXiv:1205.4839, 2012.
Sergey Edunov, Myle Ott, Michael Auli, David Grangier, and Marc’Aurelio Ranzato. Classical
structured prediction losses for sequence to sequence learning. In Proceedings of the 2018 Con-
ference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long Papers), pages 355-364, 2018.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http:
//www.deeplearningbook.org.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Confer-
ence on Machine Learning, pages 1861-1870, 2018.
Hado V Hasselt. Double q-learning. In Advances in Neural Information Processing Systems, pages
2613-2621, 2010.
Philipp Koehn and Rebecca Knowles. Six challenges for neural machine translation. In Proceedings
of the First Workshop on Neural Machine Translation, pages 28-39, 2017.
10
Under review as a conference paper at ICLR 2022
Donghwan Lee and Niao He. Stochastic primal-dual q-learning. arXiv preprint arXiv:1810.08298,
2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
Ofir Nachum and Bo Dai. Reinforcement learning via fenchel-rockafellar duality. arXiv preprint
arXiv:2001.01866, 2020.
Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of
discounted stationary distribution corrections. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alche-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing
Systems, volume 32, 2019. URL https://proceedings.neurips.cc/paper/2019/
file/cf9a242b70f45317ffd281241fa66502-Paper.pdf.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association
for Computational Linguistics, pages 311-318, 2002.
Matt Post. A call for clarity in reporting bleu scores. In Proceedings of the Third Conference on
Machine Translation (WMT), pages 186-191, 2018.
Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming.
1994.
Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level train-
ing with recurrent neural networks. In International Conference on Learning Representations,
2016.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, pages 1889-1897, 2015.
Joan Bas Serrano and Gergely Neu. Faster saddle-point optimization for solving large-scale markov
decision processes. In Alexandre M. Bayen, Ali Jadbabaie, George Pappas, Pablo A. Parrilo,
Benjamin Recht, Claire Tomlin, and Melanie Zeilinger, editors, Proceedings of the 2nd Confer-
ence on Learning for Dynamics and Control, volume 120 of Proceedings of Machine Learning
Research, pages 413-423, The Cloud, 10-11 Jun 2020.
Jennie Si, Andrew G. Barto, Warren Buckler Powell, and Don Wunsch. The Linear Program-
ming Approach to Approximate Dynamic Programming, pages 153-178. 2004. doi: 10.1109/
9780470544785.ch6.
Felix Stahlberg and Bill Byrne. On nmt search errors and model errors: Cat got your tongue?
In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),
pages 3347-3353, 2019.
Rich Sutton, Ashique Rupam Mahmood, Doina Precup, and Hado Hasselt. A new q (lambda) with
interim forward view and monte carlo equivalence. In International Conference on Machine
Learning, pages 568-576, 2014.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Richard S Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M Pilarski, Adam White,
and Doina Precup. Horde: a scalable real-time architecture for learning knowledge from unsu-
pervised sensorimotor interaction. In The 10th International Conference on Autonomous Agents
and Multiagent Systems-Volume 2, pages 761-768, 2011.
Richard S Sutton, A Rupam Mahmood, and Martha White. An emphatic approach to the problem
of off-policy temporal-difference learning. The Journal of Machine Learning Research, 17(1):
2603-2631, 2016.
11
Under review as a conference paper at ICLR 2022
W Scott Terry. Learning and memory: Basic principles, processes, and procedures. Routledge,
2017.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L
ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st Interna-
tional Conference on Neural Information Processing Systems, pages 6000-6010, 2017.
Mengdi Wang. Randomized linear programming solves the discounted markov decision problem in
nearly-linear (sometimes sublinear) run time. Mathematics of Operations Research, 2017.
CJCH Watkins. Learning from delayed rewards. PhD thesis, King’s College, University of Cam-
bridge, 1989.
Martha White. Unifying task specification in reinforcement learning. In Proceedings of the 34th
International Conference on Machine Learning-Volume 70, pages 3742-3750. JMLR. org, 2017.
Lijun Wu, Fei Tian, Tao Qin, Jianhuang Lai, and Tie-Yan Liu. A study of reinforcement learning
for neural machine translation. In Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing, pages 3612-3621, 2018.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin John-
son, Xiaobing Liu, L ukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa,
Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa,
Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google’s neural
machine translation system: Bridging the gap between human and machine translation. CoRR,
abs/1609.08144, 2016. URL http://arxiv.org/abs/1609.08144.
Mengjiao Yang, Ofir Nachum, Bo Dai, Lihong Li, and Dale Schuurmans. Off-policy evaluation via
the regularized lagrangian. Advances in Neural Information Processing Systems, 33, 2020.
12
Under review as a conference paper at ICLR 2022
A Proofs of B ellman Optimality
In this section we prove the uniqueness and optimality of the solution of generalized Bellman equa-
tion in episodic learning setting.
A. 1 Properties of Episodic Learning Process
This section presents a series of mathematical properties of ELP; the first three is known, the rest
are new. Most of our theoretical results in this paper are based on these properties.
Proposition 6. For any policy π in any ELP, the Markov chain induced by policy π is irreducible:
∀s, s0 ∈ Sπ, Pτ∞=1 Pπ [St+τ = s0|St = s] > 0. (Bojun (2020), Lemma 1.1)
Proposition 7. For any policy π in any ELP, the Markov chain induced by policy π is positive
recurrent: ∀s ∈ Sπ, Eπ[Ts] < ∞, where Ts is the recurrent time of s. (Bojun (2020), Lemma 1.2)
Proposition 8. For any policy π in any ELP, let f : S → R be a real-valued function over the states,
we have (Bojun (2020), Theorem 4)
T
SEUf(S)I= ZEJX f(St)] / ZEJTi	(16)
We write Q1 ≥ Q2 for two value functions Q1, Q2 ∈ Q iff Q1(s, a) ≥ Q2(s, a), ∀(s, a) ∈ S × A.
First observe that the generalized Bellman optimality operator (2) is monotonic.
Proposition 9. For any discounting function γ : S → [0, 1], the generalized Bellman optimality
operator Bγ is a monotonic operator: Q1 ≥ Q2 ⇒ BγQ1 ≥ BγQ2 , for all Q1, Q2 ∈ Q.
Proof. Rewrite (2) as BYQ(s,a) = £§, (P(Sls,a)∙γ(s0)) ∙maxɑo Q(s0,a0)+E§/P(Sls,a)R(s0).
As Q1(s0, a0) ≥ Q2(s0, a0) for all (s0, a0), we have maxa0 Q1 (s0, a0) ≥ maxa0 Q2(s0, a0) for all s0,
and thus Pso (P(s0∣s,a) ∙ γ(s0)) ∙maxα, Qι(s0,a0) ≥ Pso (P(s0∣s,a) ∙ γ(s0)) ∙maxα, Q2(s0,a0)
because P(s0∣s, a) ∙ γ(s0) ≥ 0 for all s0.	□
Now, as a well-known special case, if the discounting function is a constant less than 1, the corre-
sponding Bellman optimality operator is a contraction mapping with respect to the maximum-norm
distance over the value space Q, which guarantees, by the Banach fixed-point theorem, that there
is a special value function Qc which is both the unique fixed point (Qc = BcQn and the unique
limiting point of the Bellman optimality operator (Qc = lim (Bc)nQ , ∀Q ∈ Q).
n→∞
Unfortunately, Bγ loses the above contraction property under general discounting.
Proposition 10. In any ELP where there is a single state s* and a single action a* such that doing a*
under s* only goes to non-terminal StateS s0, i.e. 52so∈sx P(s0∣s*,a*) = 1, the Bellman operator
B with episodic discounting (3) is not a contraction mapping with respect to the maximum-norm
distance: For some Q1, Q2 ∈ Q, maxs,a |Q1(s, a) - Q2(s, a)| = maxs,a |BQ1(s, a) - BQ(s, a)|.
Proof. Consider two value functions with constant difference everywhere: Q1(s, a) ≡ Q2(s, a) +δ,
for some δ > 0. Clearly, maxs,a |Q1(s, a) - Q2(s, a)| = δ. On the other hand, for any (s, a) we
have ∣BQι(s, a) - BQ2(s, a)| = Pso∈s P(s0∣s, a) ∙ γ(s0) ∙ δ ≤ δ, which equals δ at the particular
(s*,a*) because Pso∈s P(s0∣s*, a*) ∙ γ(s0) ∙ δ = Pso∈s⊥ P(s0∣s*, a*) ∙ 1 ∙ δ = δ.	□
Proposition 10 seems to be a negative result for most non-trivial ELPS — it says B is not a contraction
mapping unless we always has a chance to immediately terminate an episode no matter where we
are (even when we are at terminal states, at which point the episode has not effectively started yet).
But interestingly, it turns out that the episodic Bellman optimality operator B still has unique fixed
and limiting point in all finite ELPs, not because of the contraction property as in discounted-MDPs,
but because ofa graph property dedicated to the family of ELPs, as will be shown next.
13
Under review as a conference paper at ICLR 2022
Lemma 2.1. In any Episodic Learning Process (S, A, P, R, ρ0), for any subset of states Ω ⊆ S
and any policy π, let C∏(Ω) = { s0 : ∃s ∈ Ω, Pn [St+ι = s0∣St = s] > 0 } be the set of all
successor states that are one-step reachable from Ω under π, and let Sn be the set ofstates that are
ever reachable under π (from initial states, infinite steps), then Cn (Ω) ⊆ Ω only if Sn ⊆ Ω.
Proof. For contradiction suppose Cn (Ω) ⊆ Ω and yet there is a ∏-reachable state s* ∈ Sn that is
outside the given subset Ω. We will show that in this case, it is possible to construct a policy μ (that
is possibly different from ∏) such that s* is also reachable under μ, and that μ admits an infinite
trajectory that passes through s* and never return back to s* (see Figure 4 below). This would
contradict with Proposition 7 above which asserts that a μ-reachable state s* must have finite mean
recurrence time under μ.
Figure 4: An μ-admissible trajectory that goes through s* but never returns.
Specifically, first observe that Cn (Ω) ⊆ Ω means Ω is a absorbing subset under ∏ so that once We
get into Ω We would never get out.
Then observe that the existence of a ∏-reachable state s* ∈ Ω entails that Ω cannot contain all initial
states, as otherwise from any of the initial states (in Ω) We cannot go outside the absorbing subset Ω
to reach s*. Let so be such an initial state that is outside Ω, from which we can reach s* under ∏ (as
assumed) without reaching any state in Ω in the middle (otherwise we never reach s*).
Now, pick an arbitrary state s+ in Ω, there must be some policy β under which we can reach s+ from
s0 (as states unreachable under any policy should not show up in S in the first place, see Section 2).
Without loss of generality we can again assume that we can reach s+ from s0 under β without going
through any other state in Ω in the middle because otherwise we can simply re-define s+ to be the
first state in Ω that we have encountered on the path (from so to the “old s+”).
So far we have obtained an initial state so outside Ω, from which there is an admissible path so →→ s*
for policy π, and an admissible path so →β s+ for policy β . Both paths only contain states outside
Ω (except s+). Now we construct policy μ as follows: we ask μ to copy ∏ on states in the path
so →→ s*, and ask μ to copy β on states in the path s0 →→ s+. If a state shows up in both paths, we
ask μ to be a (probability) mixture of both ∏ and β on that state. Clearly, we can reach both s* and
s+ from so under μ (note that the policy mixing only decreases the probabilities to reach s* and s+
but does not change their reachability).
By Proposition 6, since both s* and s+ are in Sμ, we must be able to go from s* to s+ under μ.
Note that so far we have only prescribed μ's behavior outside Ω. Our final step is to ask μ to copy
∏ for all states in Ω, so that Ω is also an absorbing subset for μ, meaning that once we reach s+
(from s*), we will be stuck in Ω without going back to s* (which is outside Ω by our assumption at
the beginning of the proof). In this way, we have constructed a policy μ, under which we can first
go from so to s*, then go from s* to s+, and then be stuck in Ω forever without returning to s*. A
possibility of such an infinite trajectory under μ directly contradicts with Proposition 7.	口
A.2 Proof of Theorem 1 (1) (2)
Lemma 2.1 says that in ELPs, any absorbing subset of S under a policy must contain all reachable
states of this policy. In particular, there cannot exist an absorbing subset outside the reachable set
Sn. Utilizing this fact, we can prove the first two statements of Theorem 1.
14
Under review as a conference paper at ICLR 2022
(Theorem 1 (1) (2)). In any ELP (S, A, P, R, ρ0) with finite state space S and finite action space
A, let γ be any discounting function such that γ(s) < 1 for all terminal state s ∈ S⊥, then
(1)	Bγ has a unique fixed point, i.e., the equation Q = BγQ has a unique solution.
(2)	The fixed point of Bγ is also the limiting point of repeatedly applying Bγ to any Q ∈ Q.
Proof. For any two value functions Q1 and Q2 , consider their L∞-distance
d(Q1, Q2) =. max ds(Q1,Q2)
s∈S
where
ds(Q1, Q2) =. max |Q1(s, a) - Q2(s,a)|.
a∈A
As usual, we have
d(Bγ Q1, Bγ Q2) = max
s∈S
max
a∈A
P( P (s0∣s, a) ∙ γ(s0) ∙ ( max Q1(s0, a；) - max Q2(s0, a2)) I
s0∈S	a1	a2
≤ max
s∈S
max
a∈A
≤ max
s∈S
max
a∈A
p(s P(s0∣s, a) ∙ γ(s0) ∙ I max Q1(s0, a；) — max Q2(s0, a2)
s0∈S	a1	a2
X P(s0∣s,a) ∙ γ(s0) ∙ maX ∣ Q1(s0,a0) — Q2(s0,a0) ∣
s0∈S	a
max
s∈S
max
a∈A
E P(s0|s,a) ∙ Y(SO) ∙ ds0 (Q1,Q2).
s0∈S
(17)
Traditionally, it was assumed that γ(s0) ≡ γc < 1 for all states, thus the γ(s0) term in (17) can be
readily moved out of the sum, immediately yielding d(BQ1, BQ2) ≤ Yc ∙ d(Q1, Q2). When γ(s0) is
not constant and is allowed to be 1 for non-terminal states, applying the operator Bγ to Q； and Q2
cannot guarantee to reduce d(Q；, Q2), as discussed in Proposition 10.
However, by utilizing the graph property as proved in Lemma 2.1, we can show that Bγ guarantees
to reduce the per-state distance ds (Q；, Q2 ) at some “support dimension” s, so that if we repeat-
edly apply Bγ , the set of “support states” will become smaller and smaller and eventually become
empty at which point the overall L∞-distance gets reduced (by the composite operator of repeatedly
applying Bγ ).
Specifically, for given Q；, Q2 ∈ Q, we will identify a sequence of proper subsets of states
S = d-support(0) ⊃ d-support⑴ ⊃ d-support(2) ⊃ ∙∙∙ ⊃ d-support(|S|) (18)
such that
S ∈ d-support(k) ⇒ ∀i ≥ k, ds ((BY)iQ1, (BI)iQ2) < d(Q1,Q2)	(19)
for all k ≥ 0. The construction of the subsets is by induction, and is based on the following insight:
Lemma 2.2. Under the condition of Theorem 1 , if (19) holds for k - 1, then
∃s* ∈ d-support(k), such that ∀i ≥ k, ds* ((BY)iQ1, (Bγ)iQ2) < d(Q1, Q2)
Proof. We first refactor (17) a little bit, which actually holds in a per-state sense, so
ds(B〔Qi, BYQ2) ≤ max X P(SiS,a) ∙ γ(s0) ∙ ds，(Q1,Q2)
a∈A
s0∈s	(20)
≤ max ds，(Q；,Q2) = d(Q；,Q2).
s0∈S
Recursively applying (20) gives
ds ((BY)kQι,(Bγ)kQ2) ≤ max X P(Sls, a) ∙ γ(s0) ∙ ds，((BY)k-1Qι, (Bγ)k-lQ2)
a	s0∈S
=X P(s0∣s, amaχ(S)) ∙ Y(S0) ∙ ds，((BY)k-1Qι, (By)k-lQ2)	(21)
s0∈S
≤ X P(s0∣s, amax(S)) ∙ y(s[ ∙ d(Qi, Q2 ≤ d(Q1,Q2)
s0∈S
15
Under review as a conference paper at ICLR 2022
where amax(s) =. arg maxa Ps0∈S P(Sls, a) ∙ γ(s0) ∙ ds0 ((BY)k-1Qι, (BY)k-lQ2).
Now We prove that the inequality (21) must be strict for some support-state s* ∈ d-support(k).
In particular, we will prove
X P(SlS*, amax(s*)) ∙ γ(s0) ∙ ds，((BY)k-1Qι, (BY)k-lQ2) < d(Q1, Q2	(22)
s0∈S
Case 1: There is an s* ∈ d-support(k) with Pso∈d-suppθrt(k) P(S0∣s*, amαχ(s*)) < 1. In this
case it,s possible to go from s* to some s0 outside the subset of d-support(k). For such s0 we
have γ(s0) ∙ ds，((BY)k-1Q1, (BY)k-1Q2) < d(Q1,Q2) (because (19) holds for k - 1 as assumed),
thus
X P(SlS*, amɑx) ∙ γ(s0) ∙ ds，((BY)k-1Qι, (BY)k-lQ2)
s0∈S
< X	P(SlS*,amaχ) ∙ d(Ql,Q2) +
s， 6∈d-support(k)
X	P(SlS*, amɑx) ∙ Y(s0) ∙ ds，((BY)k-1Qι, (By)k-lQ2)
s， ∈d-support(k)
≤ X	P(s0∣S*, amɑx) ∙ d(Ql, Q2) + X	P(S0∣S*, 0max) ∙ d(Ql, Q2)
s， 6∈d-support(k)	s， ∈d-support(k)
= d(Q1, Q2).
CaSe 2： FOrallS ∈ d-support(Qi,Q2), Ps0∈d-support(Q1,Q2) P(SlS, amax (S)) = L ThiSiS
equivalent to say that there exists a policy - which would choose amax(S)under the correspond-
ing s - such that it is impossible to move from any state in d-support(k) to a state outside
d-support (k) under this policy. In other words, let μ be such a policy, we have
Cμ (d-support(k)) ⊆ d-support(k)
which, by Lemma 2.1, entails
Sμ ⊆ d-support(k),	(23)
(23) literally says that the set of support-states d-support(k), as an absorbing subset under μ as
assumed in case 2, must contain all reachable states under μ. By the definition of ELP, these μ-
reachable states must in turn contain at least one terminal state (otherwise we would not have finite
episode under μ at all). Let s⊥ ∈ Sμ ⊆ d-support (k) be such a reachable terminal state under μ.
Since s⊥ is reachable under μ (and μ chooses amαχ(s) under each s), there must also be an s* ∈ Sμ
such that P(S⊥ |S*, amax(S*)) > 0. Because γ(S⊥) < 1 as assumed as the general condition of
Theorem 1, we have
X P(s0∣s*, amɑχ) ∙ γ(s0) ∙ ds，((BY)k-1Qι, (By)k-lQ2)
s0∈S
≤ X P(s0∣s*, αmɑχ) ∙ γ(s0) ∙ d(Qι, Q2)
s'∈S
=(P (s⊥∣S*, αmɑχ) ∙ Y (s⊥) +	X	P(SlS* ,αmɑχ) ∙ γ (s0) ) ∙ d(Ql, Q2)
s，ES\{sQ
< (P(s⊥∣S*,amɑχ) +	X	P(s0∣S*,amɑχ) ∙ γ(s0) ) ∙ d(Ql,Q2)
s，ES\{s,}
≤ ( P (s⊥∣S*,amɑχ)+	X	P (s0∣S*,amɑχ) ) ∙ d(Ql,Q2)
s'∈S∖{s⊥}
= d(Q1, Q2).
16
Under review as a conference paper at ICLR 2022
Now we have proved that ∃s* ∈ d-support(k), d§* ((BY)kQi, (Bγ)kQ2j < d(Q1, Q2). It is
straightforward to verify that the same proof idea applies to all i > k too (in case 1, we still have
γ(s0) ∙ ds0 ((BY)i-1Q1, (BY)i-iQ2) < d(Q1,Q2) because (19) holds for k - 1; in case 2, the
existence of s⊥ is a graph property that is independent of how many times BY is applied). □
With Lemma 2.2, we can construct each subset d-support(k) in (18) by removing the s* from
d-support(k - 1). It’s clear that (19) will hold for the sequence of support subsets thus con-
structed. In particular, note that Lemma 2.2 holds for k = 0 without the inductive condition (that
(19) holds for k - 1) because in this case it’s impossible to go outside d-support(0) = S as in
Case 1, so only Case 2 is possible (and in this case the proof does not need the inductive condition).
Now, (18) implies that d-support(|S|) is empty set. Substituting this observation into (19), yields
ds((BY)|S|Q1, (BY)|S|Q2 < d(Q1,Q2) for all s ∈ S, which means d((BY)|S|Q1, (BY)|S|Q2 <
d(Q1, Q2). In other words, the existence of the d-support sequence satisfying (18) and (19) means
that the composite operator of “repeatedly applying BY for |S| times” (i.e. (BY)|S|) guarantees to
reduce the L∞-distance of every value function pair in Q.
Moreover, note that the initial values of Q1 and Q2 do not determine how much percentage the
distance between them will reduce - the values of Qi and Q2 only affect what amaχ is in (22),
There are |S| ∙ |A| ∙ |S| possible transition probabilities in total, and |S| possible γ-values in (22),
so no matter how amax (which is a policy) and arg maxd change over iterations, they just select a
different subset from the |S|3 ∙ |A| possible terms. There are a finite number of such subsets, thus
when the sum of the subset is less than 1, there must be an absolute upper bound 1 - rmin for the
subset sum. This upper bound ratio of distance reduction can be extremely close to 1, especially
after repeating the process for |S| times, but still, it is a fixed number smaller than 1, and thus the
operator (BY)|S| is a contraction mapping, and thus it has a unique fixed point (by the Banach fixed
point theorem).
Our last step is to prove that the unique fixed point of the composite operator (BY)|S| is also the
unique fixed (and limiting) point of the original Bellman operator BY. It is easy to see that BY cannot
have two fixed points, as otherwise their distance could not get reduced by repeatedly applying BY
for |S | times. To show that BY does have a fixed point, we prove the following slightly stronger
result (in the following lemma we use B to denote the generalized Bellman operator for brevity).
Lemma 2.3. Under the condition of Theorem 1, let Q* be the unique fixed point of (B)1S1,
Q* = lim (B)nQ , ∀Q ∈ Q	(24)
n→∞
and
Q* = BQ*	(25)
Proof. We first prove that (24) holds for all Q- with Q- ≤ BQ-. 3 Because B is monotonic
operator, for such Q- we have
Q- ≤ BQ- ≤ (B)2Q- ∙∙∙≤ (B)1S1Q- ∙∙∙≤ (B)21S1Q- ∙∙∙≤ Q*	(26)
where Q* being the limit of the above sequence is guaranteed by the contraction property
of B|S| . Recall that Qi ≤ Q2 means ∀s, a Qi(s, a) ≤ Q2(s, a) and that d(Qi, Q2) =
maxs,a |Qi(s, a) - Q2(s, a)|, we see that the value functions in the above (monotonic) se-
quence must have non-increasing distances to Q*. On the other hand, as the subsequence
Q-, (B)1S1Q-, (B)2∙lSlQ-, (B)3∙lSlQ-,... converges to Q*, we know that for any e > 0, there
exists an i* such that d((B)i*"SlQ-, Q*) < G and thus d((B)iQ-,Q*) < E for all integer i > i*
due to the monotonicity, which literally means that the overall sequence (26) also converges to Q*.
Now because Q* = lim (B)nQ-, we must have BQ* = B lim (B)nQ- = lim (B)nQ- = Q*,
n→∞	n→∞	n→∞
thus (25) holds, and that Q* is a fixed point of B.
3 Such Q- guarantees to exist. In fact, every on-policy value function is such a Q- ; see A.3 for details.
17
Under review as a conference paper at ICLR 2022
Finally, because Q* = BQ*, for any Q ∈ Q (not necessarily with Q ≤ BQ this time), We have
d(BQ,Q*) = d(BQ, BQ*) ≥ d(Q,Q*), where the inequality is by (17). Again, d(BQ,Q*) ≥
d(Q, Q*) means the sequence (26) has non-increasing distances to Q*, this time for all Q ∈ Q. So,
by the same logic as above, Q* must be the limit of the sequence Q, BQ, (B)2Q,... - again, for all
Q ∈ Q this time (not necessarily with Q ≤ BQ).	□
Note that the specific form of the γ-function does not play a role in the proof of Lemma 2.3 as
presented above, so (25) and (24) apply to all Bγ that complies with the condition of Theorem 1.
We thus have completed the proof of statement (1) and (2) in Theorem 1.	□
In comparison, the classic Bellman optimality property requires γ(s) < 1 at every state s ∈ S, while
Theorem 1 only requires γ (s) < 1 at terminal states. On the other hand, the classic result applies
to all MDPs, while Theorem 1 is fundamentally based on unique structures of episodic learning
process. Importantly, the episodic discounting function that we use in ELP — i.e. (3) 一 does satisfy
the condition in Theorem 1.
A.3 Proof of Theorem 1 (3)
Finally, we prove statement (3) of Theorem 1 which asserts that Q*, as the unique fixed point of the
Bellman operator under episodic discounting, is indeed an optimal value function, in the sense that
a Q*-greedy policy is an optimal policy with respect to the undiscounted objective (1).
(Theorem 1 (3)). In any finite ELP, let Q* be the fixed point of B (i.e. the solution of (4)), let π*
be a policy such that π*(a∣s) > 0 only if Q*(s, a) = max« Q*(s, a) ,then J (π*) = max∏∈∏ J (π),
where J is the episodic-reward objective (1).
Proof. Every policy π is coupled with a (unique) on-policy value function Qπ which assigns values
to (s, a) pairs according to the conditional expectations of the episode-wise cumulative reward under
the policy π . Formally,
Qn Ga) = REM
T
R(St)S0
a
∀(s, a) ∈ S × A
(27)
Comparing (27) with the definition of the episodic-reward objective J (i.e. with (1)), and by the
ELP conditions, we see that for any policy π, its performance score equals its on-policy terminal
values:
J(π) = Qπ (s⊥, a) ,	∀s⊥ ∈ S⊥ , ∀a ∈ A.	(28)
We will prove that for the Q *-greedy policy ∏*, we have Q∏* ≥ Q∏ for all ∏ ∈ Π, which would
entail that J(π*) ≥ J(π).
First observe that Qπ* = Q*, that is, the value function Q* which induced the greedy policy π* is
also the on-policy value function of π*. Specifically, with the episodic γ-function (3), for any (s, a),
by the definition of π * we have
Q*(s, a) = BQ*(s, a)
E
S0 〜P (s,a)
max
a0
E
S0 〜P (s,a)
E
{S1,A1}〜π*
A，〜EIS，) [R(S0)+ Y(SO) ∙ Q",A)]
」R(Si) + γ(Sι) ∙ Q*(S1,A1)∣S0 = s,A0 = a]
(29)
“I舄2}5 [R(S1)+ Y(SI)R(S2)+ Y(SI)Y(S2)∙ Q*(S2,A2)| S0 = s,A0 = a]
E
{St ,At}〜∏*
T
R(St)∣∣S0 = s, A0 = a
Qπ* (s, a)
(30)
18
Under review as a conference paper at ICLR 2022
Connecting (29) and (30) and generalizing to arbitrary π yields that for any policy π ∈ Π, we obtain
Qπ ≤ BQπ ; specifically,
Qπ (s, a) = E
S0 〜P (s,a)
≤E
S0 〜P (s,a)
= BQπ(s, a)
A，—?,,) [R(SO)+ Y(SO) ∙ Qn(S0,A0)]
max
a0
[r(so) + γ(S0) ∙ Q∏ (S0,a0)]
In other words, the set {Q : ∃π ∈ Π, Q = Qπ} is a subset of the set {Q : Q ≤ BQ}.
Now, because Q* is a maximum value-function of the set {Q ≤ BQ} which contains all on-policy
value functions as a subset, and because Q* itself is also an on-policy value function (as Q* = Q∏*),
it must follow that Q* is also a maximum value-function of the subset {Qπ}. Thus we have proved
that Q∏* = Q* ≥ Qn for all ∏ ∈ Π, as desired.	□
B Proofs of Minimax Duality
B.1 Proof of Lemma 2.1
(Lemma 2.1). In any finite ELP, for any conjugate policy π, Q* ∈ arg min
max Lπ (Q, λ).
λ≥0 π
Q* is an optimal solution of (7) because B is monotonic, so for Q with Q ≥ BQ we have Q ≥
Bq ≥ (B)2Q ∙∙∙ ≥ Q*. The objective function of (7) is some probabilistic average over the values
at terminal states (and actions), which clearly attains minimum for the per-state-action minimum
value function Q*.
Due to standard Lagrangian duality, a value function is an optimal solution of (7) if and only
if it is a minimax solution of (8). In particular, only value functions with Q ≥ BQ can
prevent maxλ Lπ(Q, λ) from being arbitrarily large, and for those Q’s, maxλ Lπ(Q, λ) =
EZ〜∏ [q(St, AT)] under the complementary slackness condition, whose value attains minimum
at Q* as proved.
B.2 Proof of Lemma 2.2
(Lemma 2.2). In any finite Episodic Learning Process (S, A, P, R, ρ0), for any conjugate policy π,
let Lπ be the corresponding Lagrangian, and let λπ be the particular Lagrangian multipliers with
λ∏(s, a) = ρ∏(S) ∙ π(a∣s) ∙ EZ〜∏ [T], where ρ∏ is the stationary distribution of π, then
Lπ(Q,λπ)=J(π)+XX
λ∏(s, a) ∙ (maxQ(s, a) — Q(s, a))
s6∈S⊥ a∈A
By Proposition 8, with f(s) = EA〜冗⑶[l[s ∈ S⊥] ∙ Q(s, A)], we have
T
ZEJQ(ST ,AT )]= S1.E 〜n [χ f (St)] = Eζ~∏ [T]∙ SEPn A 〜E(S) [1[S ∈S⊥]∙ Q(S，A)]
19
Under review as a conference paper at ICLR 2022
So, for any Q ∈ Q, we have
Lπ(Q,λπ)
=EZ〜∏[T] ∙	E	1 [S	∈	S⊥]	∙ Q(S,A)	+	EZ〜∏ [T]	∙ E	BQ(S, A)- Q(S,
S,A^ρ∏ L	」	S,A^ρ∏ L
=EZ〜∏ [T] . $ 出P [l [S ∈ S⊥] ∙ Q(S, A)- Q(S, A)] +
EZ〜∏[T] ∙ E h E	∣^R(S0)+ γ(S0) ∙ max Q(S0,a0)ii
s,a〜ρ∏ L S0〜P(S,A) L	a0	」」
-EZ〜∏ [T].
E
S,A^P∏
[1[S∈S⊥]∙ Q(S，A)]+EZ~n[T]. soEρ∏
[r(S0) + γ(S0) ∙ max Q(S0, a0)]
=EZ 〜∏ [T ] ∙ SE [r(S)] + EZ 〜∏ [T ] ∙s 也 [l[S∈S⊥] ∙ (max Q(S,a) - Q(S,A))]
=J(π) + EZ〜∏[T] ∙ X ρ∏(s) ∙π(a∣s) ∙ 1[s∈S⊥]∙ ^maxQ(s, α) - Q(s,a))
s∈S,a∈A
=J(n) + EZ〜π [T] •XX
ρ∏(S) ∙ π(a∣s) ∙ (mɑxQ(s, a) — Q(s, a))
s∈S∖S⊥ a∈A
Note that in above for EZ〜∏ [T] ∙ ES〜ρπ
Proposition 8, with f(s) = R(s).
R(S) = J(π)
we have again used the transformation of
B.3	Proof of Theorem 2
(ELP Minimax Theorem). In any finite Episodic Learning Process (S, A, P,R,ρo) ,if μ ∈ Π is an
optimal policy, then its conjugate Lagrangian Lμ has strong duality property, with
min max	LU(Q,	λ) =	max	min	LU(Q, λ)	= J(μ)	(31)
Q∈Q λ≥0	μ'”	)	λ≥0	Q∈Q μ' %	1
Let ∏* be a Q *-greedy policy, which is thus an optimal policy.
For any conjugate policy ∏, since Q* is a minimax solution of Ln (Lemma 2.1), We have
minQ maxλ≥0 Lπ(Q, λ) = maxλ≥0 Lπ(Q*, λ) = Eπ[Q*(ST,AT)]. By (30) in A.3 we have
En[Q*(St,At)] = En[Qn*(ST,At)]. By (28) in A.3 we further have E∏[Q∏*(ST,At)]=
J(π*). Connecting these results together gives
min max
Q λ≥0
Ln(Q,λ)=J(π*).
(32)
Again for any conjugate policy π, due to Lemma 2.2, the Lagrangian function has the dual form (10)
under the particular multiplier λn , as copied below
Ln(Q,λn)=J(π)+XX
λ∏(s, a) ∙ (m@XQ(s, a) — Q(s, a))
s6∈S⊥ a∈A
The first term above is a constant that does not change with Q, and in the second term both λn (s, a)
and maxs Q(s, a) 一 Q(s, a) are nonnegative (for any Q and π), so the sum of the two terms, i.e.
Ln (Q, λn ), attains its minimum when Q achieves complementary slackness with λn, in which case
min Ln(Q, λn) = J(π).
Q∈Q
(33)
So now, when the conjugate policy ∏ is an optimal policy μ, as assumed in the theorem, we have
max min Lμ(Q, λ) ≥ min Lμ(Q, λμ) = J(μ) = J(π*) = min max Lμ(Q, λ).
λ≥0 Q∈Q μ	Q∈Q	Q λ≥0	μ
Due to weak minimax	duality,	maxλ≥o minQ∈Q	Lμ(Q, λ)	≤ minQ maxλ≥o	Lμ(Q, λ),	which
universally holds, thus the inequality above must actually be an equality, as desired.
20
Under review as a conference paper at ICLR 2022
B.4	Proof of Proposition 3
(Proposition 3). In any finite ELP, for any value function Q ∈ Q and any policy π ∈ Π, let
λ∏(s, a) = ρ∏(s, a) ∙ EZ〜∏ [T], where ρ∏(s, a) = ρ∏(S) ∙ π(a∣s), then
L∏(Q, λ) ≤L∏(Q, λ∏) ≤L∏(Q,λ∏)	,	∀Q, λ
if and only if
(1)	BQ(s, a) - Q(s, a) ≤ 0	, ∀(s, a) ∈ S × A
(2)	ρ∏(s, a) ∙(BQ(s, a) - Q(s, a)) = 0	, ∀(s, a) ∈ S ×A
(3)	ρ∏(s, a) ∙ (maxa Q(s, α) - Q(s, a)) = 0	, ∀s ∈ S⊥, a ∈ A
The “if” part is straightforward: Condition (1) and (2) immediately gives Lπ (Q, λπ) =
EZ 〜∏ [Q(St ,At )] = maxλ ≥o Ln (Q, λ). On the other hand, condition (3)entails that the Sec-
ond term in dual-form Lagrangian (10) is zero, so Lπ (Q, λπ) = J(π). By (33) in B.3 we have
minQ L∏(Q,λ∏) = J(π), thus L∏(Q,λ∏) =minQ L∏(Q,λ∏).
Now we prove the “only if” part, for which we resort to the general saddle-point condition: Under
given conjugate policy π, a (Q, λ) pair is a minimax equilibrium (or minimax saddle-point) of
function Lπ(Q, λ) if and only if
(i)	min max LnIQ, λ) = max min Ln(Q, λ) = Ln (O, λ),
Q∈Q λ≥0	λ≥0 Q∈Q
(ii)	Q ∈ arg min max Ln(O, λ),
Q∈Q λ≥0
(iii)	λ ∈ arg max min Ln (O, λ).
λ≥0 Q∈Q
By condition (ii), if (Q, λn) form a minimax equilibrium, then Q must be a minimax value function.
From B.1 we know that such minimax value function must have Q ≥ BQ (condition (1)).
By condition (i) and by (32) in B.3 We know that Ln(O, λn) = minQ∈Q maxλ≥o Ln(O, λ)=
J(π*) = EZ〜n [Q(St, AT)]. In other words, Ps 0 λn(s, a) ∙ (BQ(s, a) - Q(s, a)), as the second
term in Ln (O, λn), needs to be zero. Because λn (s, a) = Pn(s, a) ∙ EZ〜n [T] where EZ〜n [T] > 0,
and because BQ ≤ Q as proved, the only way to make the term zero is to have Pn (s, a) ∙ (BQ(s, a)-
Q(s, a) = 0 for each and every (s, a) (condition (2)).
Moreover, since Ln(O, λn) = minQ Ln(Q, λn) as assumed, by (33) in B.3 we have Ln (O, λn)=
J(π), which means Ps∈s, Pa∈A Pn(s, a) ∙ EZ〜n[T] ∙ (maxa Q(s,a) - Q(s,a)), as the second
term of the dual-form Lagrangian (10), must be zero. Again, because EZ〜n [T] > 0 and because
maxa Q(s, a) - Q(s, a) ≥ 0 for all (s, a), the only possibility is to have Pn(s, a) ∙ (max% Q(s, a)-
Q(s, a) = 0 for each (s, a) ∈ S \ S⊥ × A (condition (3)).
C Maximin saddle points vs Minimax saddle points
C.1 Proof of Lemma 5.1
(Lemma 5.1). In any finite ELP, Q* is an optimal solution of
max E Q(ST, AT)
s.t. Q(s, a) ≤ BQ(s, a) , ∀(s, a)
for any conjugate policy π ∈ Π. Equivalently, Q* ∈ arg max min Ln (Q, λ).
The proof is by symmetric argument with the one in B.1: Because B is monotonic, for Q with Q ≤
BQ we have Q ≤ BQ ≤ (B)2Q …≤ Q*, thus Q* maximizes the objective EZ 〜∏[Q(St ,At )] in
a per-state-action manner.
21
Under review as a conference paper at ICLR 2022
C.2 Proof of Theorem 5
(Theorem 5). In any finite ELP, for any conjugate policy π, let Qmax be an maximin value function
ofthe Lagrangian Ln - i.e. let QmaX be an optimal solution of (15) - then QmaX is an optimal value
function, in the sense that Qmax-greedy policy maximizes the episodic-reward objective (1).
As described in the proof idea, first observe that QmaX(s, a) ≤ Q* for all (s,a) ∈ S × A because
Qmax, as a feasible solution of (15), has QmaX ≤ BQmaX ≤ BBQmaX …≤ Q*.
Let μ be a QmaX-greedy policy, so μ(a∣s) > 0 only if QmaX(s, a) = maxa QmaX(s, a). We will
prove that
maxQmaX(S,4)= m_axQ*(s, α) , ∀s ∈ Sμ \ S⊥.	(34)
a	a
Note that (34) would necessarily imply that
arg max QmaX(s,a) ⊆ arg max Q*(s,a) , ∀s ∈Sμ \ S⊥.	(35)
aa
This is because QmaX ≤ Q* guarantees that for any action a sub-optimal to Q*, it can only have
even lower value in QmaX, so QmaX(s, a) ≤ Q*(s, a) < maxs Q*(s, a) = max% QmaX(s, a), in
which QmaX(s, a) < maxa QmaX(s, a) literally says that such an a cannot be QmaX-OPtimaI either.
(35) guarantees that μ, a QmaX-greedy policy, will only choose Q*-optimal actions at every non-
terminal state it may encounter since time t ≥ 1. Such a μ is clearly an optimal policy in terms of
episodic reward. Note that μ's choices on terminal states does not matter here as state-transitions in
terminal steps are action-agnostic, due to the ELP condition.
Now, to prove (34), we first prove the following induction rule:
Proposition 11. Under the context of Theorem 5, let (s, a) be an arbitrary state-action pair, and let
(S X A)next = {(s0,	a0)	∈ S ×A :	s0	∈	S⊥	and	P(s0∣s, a)	∙	μ(a0∣s0)	> 0}
denote the set of all the non-terminal (s0, a0) pairs that can directly follow (s, a) under the QmaX-
greedy policy μ, then
Q maX(s, a) = Q*(s, a)
⇒ QmaX (s , a ) = Q (s , a ) and max QmaX (s , αa) = max Q (s , aα) , ∀(s , a ) ∈ (S × A)next
a	a
Proof. Because QmaX ≤ Q* and QmaX ≤ BQmaX, we have
QmaX(s, a) ≤ BQmaX(s, a)
=Es, [R(S0)] + X P(SlS,a) ∙ γ(s0)
s0∈S
≤ Es, [R(S0)] + X P(s0∣s,a) ∙ γ(s0)
s0∈S
• max QmaX(S , a)
aa∈A
• max Q* (S0, aα)
aa∈A
(36)
=BQ*(S,a)
= Q* (S, a)
So, the premise QmaX(S, a) = Q*(S, a) entails that the two inequalities in above must be equality,
among which the second one - i.e. (36) - can be equality only if
XP(S0|S, a) • max QmaX(S0, αa) =	P (S0 |S, a) • max Q* (S0, aα)	(37)
aa∈A	aa∈A
s,∈(S×A)next	s,∈(S×A)next
where S0 ∈ (S × A)next is a slight abuse of notation which means S0 shows up in (S × A)next with
some coupled a0 (or equivalently, S0 6∈ S⊥ and P(S0|S, a) > 0).
Now observe that for (37) to hold, the only possibility is that
max QmaX(S0, aα) = max Q*(S0, αa) , ∀S0 ∈ (S × A)next	(38)
aa∈A	aa∈A
as otherwise for those S0 on which (38) do not hold, it can only be maxaa∈A QmaX(S0, αa) <
maxaa∈A Q* (S0, aα) (because QmaX ≤ Q*); those S0 must all have positive weights in (37) (by
22
Under review as a conference paper at ICLR 2022
definition of (S × A)next), and thus will cause a real loss at the LHS of (37) (and importantly, no
other state in (S X A)next could claim a “gain” to compensate this loss, again because QmaX ≤ Q*).
Next, to prove QmaX(S0, a0) = Q*(s0, a0) for all (s0, a0) ∈ (S × A)next (given the premise), notice
that for any of such (s0, a0) we have
maχ QmaX(S0, a) = QmaX(S0, a0) ≤ Q*(s0, a0) ≤ maχ Q*(s0, a)	(39)
a∈A	a∈A
in which maxa∈A QmaX(S0, a) = QmaX(S0, a0) is because α0 is by definition a Qmax-greedy action
under s0, and QmaX(S0, a0) ≤ Q*(s0, a0) is (once again) because QmaX ≤ Q*.
By (38) we know that the two ends of (39) actually equal to each other, so the inequalities in between
must also be equality, and in particular QmaX(S0, a0) = Q*(s0, a0).	口
Proposition 11 as proved above enables us to prove (34) by induction (which is enough to prove the
whole theorem, as argued above). Specifically, because both QmaX and Q* are optimal solutions
of (15), and because the objective in (15) is a distribution over only the terminal states (see B.1), it
follows that QmaX and Q* must be equal on at least one terminal state S⊥. Starting from this terminal
state s⊥ 一 as well as an arbitrary action a⊥ under it - We have QmaX(S⊥, a⊥) = Q*(s⊥, a⊥),
thus by the induction rule of Proposition 11 we obtain maxs QmaX(S0, a) = maxa Q*(s0, a) and
QmaX(S0, a0) = Q* (S0, a0) for all (S0, a0) in the (S × A)next set with respect to (S, a) = (S⊥, a⊥);
the latter enables us to expand the induction proof to all non-terminal states that are reachable by μ.
r = l
r = 2
Figure 5: A copy of Figure 3
C.3 AN COUNTER-EXAMPLE FOR THE Q-FORM MINIMAX VALUES IN ELPS
In this subsection we elaborate more about the counter-example as illustrated by Figure 3 in Sec-
tion 5 (which is copied above). In this ELP, S = {0, 1, 2, 3, 4, 5}, A = {1, 2, 3}. State 4 and 5
are terminal states, from which any action leads to state 0. Choosing action 1, 2, 3 under state 0
deterministically transits to state 1, 2, 3, respectively. All actions under state 1 lead to state 4, and
all actions under state 2 and 3 lead to state 5. The agent only receives non-zero rewards at terminal
states, with R(4) = 1, R(5) = 2. The initial state is set to state 4 (i.e. ρ0(S) > 0 only if S = 4).
The Bellman fixed-point Q* for this ELP is as follows:
•	Q*(0, 1) = 1, Q*(0, 2) = 2, Q*(0,3)=2
•	Q*(1,a) = 1, ∀a
•	Q*(2,a) = Q* (3, a) = 2, ∀a
•	Q* (4, a) = Q* (5, a) = 2, ∀a
An optimal policy of this ELP should only choose action 2 or 3, but not action 1, under state 0.
For minimax value functions, denoted by Qmin, as they are optimal solutions of (7), we have
Qmin(0, 1)	≥
Qmin (4, a)	=	2	≥ max	Qmin(0, 2)	≥
I Qmin(0, 3)	≥
maxa Qmin (1, a)	≥	1
maxa Qmin(2, a)	≥	2
maxa Qmin(3, a)	≥	2
23
Under review as a conference paper at ICLR 2022
The above minimax condition only imposes tight bounds for Q *-optimal actions (e.g. action 2 and
3 under state 0), but leaves “flexibility” for actions sub-optimal to Q* (e.g. action 1 under state 0)
as well as for all state-action pairs that follow an sub-optimal action (e.g. all actions under state
1). The consequence is that even constant value function Qmin (s, a) ≡ 2 can be a minimax value
function in this example, which is clearly sub-optimal, as discussed in Section 5.
In contrast, for maximin value functions, denoted by Qmax, they are optimal solutions of (15), thus
(qmax(0, 1)	≤	maxa Q
max(1, a)	≤	1
Qmax (0, 2)	≤	maxa Qmax (2, a)	≤	2
Q max (0, 3)	≤	maxa Q
max (3, a)	≤	2
We see that the maximin condition manages to imposes tight bounds for at least one Q*-optimal
action while at the same time can enforce all Q*-sub-optimal actions to be still suboptimal to Qmax.
For example under state 0, Qmax(0, 1) cannot exceed 1, while either Qmax(0, 2) or Qmax(0, 3)
needs to be tight (i.e. Qmax (0, 3) = 2, or Qmax (0, 2) = 2, or both) so as to keep the maximum of
the three no less than 2, as required.
Note that for both Qmin and Qmax, the Lagrangian multiplier λ that forms equilibrium/saddle points
with each of them (resp.) may not encode a policy, in general. In this example, for the constant
Qmin, we have 2 = Qmin(0, I)= 0 + 1 ∙ Qmin(1, a) > 1+0 ∙ Qmin(4, a) = 1, so Qmin(I, a) >
BQmin(1, a) = 1 for all a, in which case its equilibrium multiplier λ has to have λ(1, a) = 0 for
all a, due to the equilibrium condition (2) proved in Proposition 3. Such an “all-zero” λ (on state 1)
cannot be normalized into a policy.
Similarly, for the following maximum value function
Qmax
(4, a) = Qmax(5, a) = 2
•	Q
max(0, 1) = Qmax(1, a) = 1
•	Qmax(0, 2) = Qmax(2, a) = 2
•	Qmax(0, 3) = 1
•	Qmax(3, a) = 1.5
We have 1 = Qmaχ(0, 3) < 0+1 ∙ Qmaχ(3,a) < 2 + 0 ∙ Qmaχ(5,a) = 2, so Qmaχ(3,a) <
BQmax(3, a) = 2 again for all a, so the multiplier corresponding to this Qmax still needs to be all
zero at state 3 due to the complementary slackness condition, thus cannot be normalized at state 3.
C.4 A COUNTER-EXAMPLE FOR THE V -FORM MINIMAX VALUES IN DISCOUNTED-MDPS
Moreover, the problems With minimax value functions as demonstrated above are not limited to Q-
functions or to ELPs only, but seem to be fundamental issues rooted from the minimax structure. To
see this, consider the discounted-MDP as shoWn in Figure 6 beloW.
Tobe strictly align With the related literature Dai et al. (2018); Cho and Wang (2017), the reWards are
assigned to state-action pairs in this example. In this discounted-MDP, S = {0, 1, 2, 3}, A = {1, 2}.
The initial state is set to state 0 (i.e. ρ0(s) = 1 if s = 0, otherWise ρ0(s) = 0). From state 0, taking
Figure 6: An example of discounted-MDP for studying saddle points of the V -form Lagrangian.
24
Under review as a conference paper at ICLR 2022
action 1, 2 will deterministically goes to state 1, 2, respectively, with zero reward obtained in this
step. From state 1, any action leads to the absorbing state 3, with reward R(1, a) = 1 obtained (for
all a). From state 2, any action leads to the same absorbing state 3, but with reward R(2, a) = 2
obtained (for all a). The absorbing state 3 will loop into itself forever, with zero reward obtained.
An optimal policy in this discounted-MDP should choose action 2, not action 1, under state 0. The
discounting constant Y is set to 0.5, so the optimal discounted-reward performance is V * (0) = 1.
The V -form Lagrangian of the discounted-MDP above is: Dai et al. (2018); Cho and Wang (2017)
(1- Y)	∙ E	[v (So)]	+ X	λ(s,a)	∙(R(s,a)+ Y ∙,E	[v (S0)]	- V(S))	(40)
S0~p0 L 」(s,a)∈S×A	'	S-P(s,a) L 」	)
A minimax state-value function Vmin of the Lagrangian is an optimal solution of the Linear Pro-
gramming problem (6), which has inspired some recently proposed RL algorithms (see Section 3).
In this example, the LP can be more explicitly written as
min 0.5 ∙ V(0)
s.t. V(0) ≥ 0.5 ∙ V⑴
V	(0) ≥ 0.5 ∙ V(2)
V	(1) ≥ 0.5 ∙ V(3) + 1
V	(2) ≥ 0.5 ∙ V(3) + 2
V	(3) ≥ 0.5 ∙ V(3)
for which a possible optimal solution is: Vmin(0) = 1, Vmin(1) = 2, Vmin(2) = 2, Vmin(3) = 0,
which assigns the same value to action 1 and 2 under state 0, thus is not an optimal value function.
D Lagrangian Minimization for Machine Translation
D.1 The LAMIN 1 Algorithm
As mentioned in Section 4, the idea OfLAMINI is to minimize the smoothed Lagrangian £2(with
a small yet definite β) based on an unbiased gradient estimator of (13). For convenience, we copy
(13) below:
Lμ(Q(w), λμ) = Eμ[Q(Sτ, AT; w)] + Eμ[T] ∙
SAs~Pμ AfQ(w)(S0)
δ(S, A, S0, A0; w)
where πQβ (w)(a|S) =.
exp(Q(s,a;w)/e)
Pb exp (Q(S,b；w)/e)
is the Boltzmann distribution with temperature β, and
δ(S, a, S0, a0; w) =. R(S0) + Yepi (S0)Q(S0, a0; w) - Q(S, a; w) is the temporal-difference error. Al-
gorithm 1 gives the psuedo-code of such an algorithm.
E
E
Algorithm 1: The LAMIN1 algorithm.
Input: A piece of rollout data made by an optimal policy, in the form of {St, at, rt}0,1,...,n,
where t = T1 , T2, . . . , Tk are termination steps; A parameterized value function Q(w)
with initial weight vector w0 ; Learning rate α; Boltzmann temperature β.
for gradient update i = 0, 1, 2, . . . do
△w — 1 Pt={Tι …Tk}Nw Q(St,at; W)I	+
k	w=wi
n∙ 1 nPYepi (St+1) Vw (Pa ∏W(st+1,a)∙Q(st+1,a； W)) -VwQ(st,at; w) I
t=0	w=wi
_ Wi+ι J Wi — α ∙ ∆w
Output: A Q(w)-greedy policy.
Note that Algorithm 1 samples the stationary distribution ρμ in (13) by averaging over the rollout
data of a bunch of episodes, which is unbiased thanks to the ergodicity of episodic learning Bojun
25
Under review as a conference paper at ICLR 2022
(2020)	. The overall LAMIN1 algorithm is thus a standard unbiased SGD procedure, which shares
the generic convergence property of all SGD procedures (i.e. convergence to local minimum of (13)
is guaranteed under properly annealed learning rate; see Goodfellow et al. (2016)).
As an implementation trick, the gradient computation in Algorithm 1 can utilize the following fact
(for brevity and clarity, we omit the argument st+1 in Q and π in the following):
e	eQ(a;w)/e 、	e	eQ(a;w)/e
▽nw(a) = ▽ exp (log Pb eQ(b；w)/e) = πw(a) •▽ (log Pb eQ(b;w)/e
=∏w(a) ∙ (vQ(a; W)∕β - V log Xb eQ(b;w)/e)
=∏w (a) ∙ (VQ(a； w)∕β- PbeQ(；Q(：Q/(：; M)
Ce eQ(CW)/e
=β ∙ (πW(a) vQ(a;w) -πW(a) XbπW(b) vQ(b; W))
and so
v(X πW (a) ∙ Q(a; W))
a
=X πW (a) ∙ vQ(a;w) + X Q(a;w) ∙ v∏W (a)
aa
=X ∏W(a) ∙VQ(a; W) + 1 ∙ X ∏W(a) Q(a; W) VQ(a; W)
aa
Finally, we remark that the β-smoothing trick used in LAMIN1 is more than just an approxima-
tion heuristic, but may potentially play a role in correcting (to some extent) the sub-optimality
bias of minimax value functions as discuss in Section 5. Specifically, the original Lagrangian
function Lμ(Q, λμ) fundamentally cannot distinguish optimal minimax-values from sub-optimal
minimax-values, as the function attains its global minimum in both cases. In contrast, the smoothed
Lagrangian L%(Q, λμ) tends to reach lower value at optimal minimax-values than at sub-optimal
minimax-values.
As an illustration, consider the special case where there is only one non-terminal state, under which
the agent can only choose between two actions, 1 and 2, and suppose action 1 is the truly optimal.
In this simple case, a value function can be represented by a tuple (Q1, Q2), which indicating the
value of action 1 and 2 respectively. A sub-optimal minimax-value function may have Q1 = Q2, as
we showed in Section 5; in this case the smoothed Lagrangian L『1 (Q, λμ) would still equal J(μ),
which can be seen from the dual form of the smoothed Lagrangian:
Lμ=1 (Q,λμ) = J (μ)+1 ∙ ((eQι + QQQ2+ eQι + QQQ)- Q1)
On the other hand, an optimal minimax-value can make L肝'(Q, λμ) lower than J(μ). For exam-
ple, when J(μ) = Q1 = 0, we have L『1(Q, λμ) = ι+QQ2 ∙Q2, whichattains -0.28 at QQ = -1.3.
In other words, L肝,(Q, λμ) < J(μ) = 0 under the optimal minimax-value (0, -1.3), which is
thus distinguished from the sub-optimal minimax-value (0, 0) in smoothed Lagrangian minimization
as LAMIN1 does.
D.2 The LAMIN2 Algorithm
The idea OfLAMIN2 is to minimize the original Lagrangian function Lμ based on the “local” gradi-
ent estimator (14). The consistency of the LAMIN2 estimator (with respect to VL*) is characterized
by Proposition 4, whose proof is given below.
26
Under review as a conference paper at ICLR 2022
(Proposition 4). Let A be a finite action space, and let Q(s, a; w) be a parameterized and differen-
tiable value function that suggests a single best action °maχ(w*) = arg maXa∈A Q(s, a; w*) when
evaluating the actions under a given state S with a g^ven parameter vector w*, then
Vw max Q(s,a; W) I = E	Vw Q(s,a; W) I	(41)
a∈A	lw=w*	a 〜∏max(s[W*) L	」∣w = w*
where πmax(W*) denote the Q(W*)-greedypolicy.
Proof. We will prove that for any component of W,
∂Wi m∈Ax QGa； W) L=w* =西 QGamax(W*)； W) L=w*	(42)
which readily gives (41).
To prove (42), notice that
二 max Q(s,a; W) I = lim Q(s, amax(W* + △)； w* 匕勺-Q(s, amax(W*)； w*)
∂wi a∈A	w=w*	∆→0	∆
When there are a finite number of possible actions, there must be a non-zero gap between the best
action and the second best action under W*. On the other hand, since Q is differentiable, the change
of action-values becomes infinitely small as ∆→ 0, thus the order between the two best actions will
remain the same for small enough ∆, that is, amax(W* +∆) = amax (W*) when ∆→ 0. Therefore,
lim Q(s,am&x(W* +∆); W* +∆) — Q(s,amax(W*); W* )
∆→0	∆
lim Q(s,
amax(w*); W* +∆) — QkS,
amax(W*); W*)
∆→0	∆
∂	*I
M  Q(s, amax(W ); w) I
∂wi	w=w*
□
Note that when Q(W) is a sophisticated real-valued function, such as a deep neural network with
scalar output, the chance that two actions have precisely the same value under Q(W) should be rare.
Also, continuous action space can be densely quantized into a finite action space with arbitrarily
small quantizing error, thus (41) should still approximately hold even for continuous action spaces.
Algorithm 2: The LAMIN2 algorithm.
Input: A piece of rollout data made by an optimal policy, in the form of {St, at, rt}0,1,...,n,
where t = T1 , T2, . . . , Tk are termination steps; A parameterized value function Q(W)
with initial weight vector W0 ; Learning rate α; Boltzmann temperature β.
for gradient update i = 0, 1, 2, . . . do
∆w J 1 Pt={Tι …Tk} VwQ(st,at; W) lw=w +
k ∙ n P Y epi (st+1)(Pa πW i (st+1, a)VwQ(St+1, a; W)—VwQ(St, at; W) III
t=0	w=wi
_ Wi+ι J Wi — α ∙ ∆w
Output: A Q(W)-greedy policy.
Algorithm 2 gives the pseudo-code of LAMIN2 in a further generalized form, where the greedy pol-
icy πwi is replaced with the Boltzmann policy πwβ i. As mentioned in Section 4, higher temperatures,
such as β = 1.0, can slightly improve performance in our experiment results (for 0.5 — 0.8 BLEU
score).
27
Under review as a conference paper at ICLR 2022
D.3 An Episodic Learning Formulation of Machine Translation
Many AI tasks are sequence generation problems, where we are given a context X, and are then
asked to generate a sequence Y = (bos, y1 . . . yL, eos) using tokens chosen from a given to-
ken space. Machine Translation (MT) is an example of such tasks, where the token space is the
vocabulary of a target language, and the context X is a sentence (or a sequence of sentences) in
a source language. The choice of each token yt is conditioned on X and on the partial output
Y<t =. (bos, y1 . . . yt-1). In particular, Y<1 = (bos), and Y<L+1 = (bos, y1 . . . yL), conditioned
on which the algorithm will generate the first token y1 and the last token yL+1 =. eos, respectively.
The ELP formulation exactly captures the real-world MT tasks as described above. In the MT
context, an episode is the translation of a given sentence. The first episode effectively starts with
S1 = (X(1), bos) where X(1) is a full source sentence. A learning agent then chooses a token
A1 = y1(1) ∈ Σtarget ∪ {eos}, after which the environment state transits, deterministically, to S2 =
(X(1) , Y<(12)) = (X (1) , bos, y1(1)). The agent keeps generating actions At = yt(1) under each St =
(X(1) , Y<(1t)) until it outputs AT-1 = eos at some step T - 1, leading to terminal state ST =
(X(1), Y (1)) = (X (1), bos, y1(1)... yT(1-) 2, eos). The agent will then make a normal action AT as in
previous steps, which however makes no effect other than resetting the environment into ST+1 =
(X(2), bos) from which the second translation episode begins. The process goes on episode after
episode, generating a (theoretically infinite) sequence of translations (X(1), Y (1)), (X(2), Y (2)), . . . ,
which collectively serve as the training data for the agent to learn better translation policies.
An episode of length T as above results in a translation sentence Y = (bos, y1 . . . yL, eos) which
contains L = T - 2 “normal” tokens from Σtarget. As a common and necessary practice, most real-
world MT systems impose a maximum translation length H so that if an eos action did not show up
after H steps, the environment will transit to the terminal state SH+2 = (X, bos, y1 . . . yH, eos)
even if the agent continues to output normal token AH+1 ∈ Σtarget at step H + 1. When maximum
translation length is applied, the corresponding episodic learning model ofMT has bounded episode
length, thus satisfies the ELP Condition (3) above. Such a formulation considers the mechanism of
maximum translation length as a fundamental part of learning-based MT task specification that is
essential in helping learning agents (which may not master when to output eos) to escape from long
and meaningless translation episodes which may otherwise lead to ill-conditioned training data.
To comply with ELP Condition (2), we prescribe ρ0 to be an arbitrary distribution over the set
of terminal states, i.e. over all source-target sentence pairs (X, Y ) where Y is complete sentence
ending with eos. As with other terminal steps, no matter what S0 and A0 are, the next state S1 will
follow the same distribution, denoted as ρ1 , which specifies the distribution of the source sentences
that the agent will receive in each episode (ELP Condition (1)).
Finally, the agent receives a scalar reward R(X, Y ) ∈ [0, 100] at each terminal state, based on the
translation quality of Y (with respect to X).The reward is zero at all non-terminal states (which has
only partial translations). With this reward function, the expected episodic-reward objective J(π) of
a translation policy π corresponds exactly to its average scores under the chosen translation metric.
The ELP formulation of MT as discussed above can be formally summarized as follows:
•	S =(夕SoUrce)" × {bos} × (夕1罪60"X {- , eos}
•	A = Σtarget ∪ {eos}
•	R(S) = Imetric( X(S), Y(S)) S ∈ S⊥ , where S⊥ = {(X, Y) : Y ends with eos}
0	s 6∈ S⊥
ρ1(S0)
•	P(S0|S, a) =	1[ S0 = (S, a) ]
[l[ s0 = (s, eos)]
if S ∈ S⊥
if S 6∈ S⊥ and |Y (S)| < H
if S 6∈ S⊥ and |Y (S)| = H
•	ρ0(S) > 0 only if S ∈ S⊥
Note that in above both S and A are finite sets, in which case the model is a finite episodic learning
process. For real-world machine translation, we typically have |S | < 400002048 × 400002048 × 2
and |A| < 40000 + 1.
28
Under review as a conference paper at ICLR 2022
The experimentation code in Supplementary Material contains a faithful implementation in Python
of the formulation presented here.
D.4 Experimentation Details
We tested our algorithmic idea using the WMT’14 NewsTest English→German (en2de) dataset 4.
The data was pre-processed and post-processed using the BPE tokenizer provided by YouToken-
ToMe 5, with shared vocabulary of size 37000. We used SacreBLEU Post (2018) as the translation
metric and the standard TransformerBase neural network Vaswani et al. (2017), which is known to
achieve a BLEU score of 27.3 on the WMT’14 dataset under the state-of-the-art method of MLE-
based learning Vaswani et al. (2017).
We trained the model on the same 4.5 millions sentence pairs in the WMT’14 data set for 100, 000
gradient updates on a V100 GPU, with the same mini-batch size (and token-padding strategy) and
learning rate schedule as recommended by Vaswani et al. (2017). A dropout rate of 0.1 is applied.
The learned model is then used as search heuristic in the vanilla-beam-search decoding (e.g. see
Algorithm 1 in Stahlberg and Byrne (2019)), with a beam size of4. Empirically, we found that some
more performance gain can be obtained by adding more tricks, such as modestly increasing the beam
size (say, to 10), adding length penalty factor in the search heuristic (see Wu et al. (2016)), and model
averaging (see Vaswani et al. (2017)), but we chose to exclude these tricks in our performance report
so as to keep our algorithm simple and easy to implement. Our experimentation code and scripts
can be found in Supplementary Material for reproduction.
The following tables give the numerical values of the performance scores shown in Figure 1.
β	BLEU@100k
0.01	27.4
0.25	27.0
0.5	26.7
0.75	26.6
1.0	26.6
Table 1:	Corpus BLEU scores of LAMIN1 (i.e. Algorithm 1) under different temperature β.
β	BLEU@100k
0.01	260
0.2	26.2
0.4	26.8
0.6	26.2
0.8	26.3
1.0	26.8
Table 2:	Corpus BLEU scores of LAMIN2 (i.e. Algorithm 2) under different temperature β.
4https://nlp.stanford.edu/projects/nmt/
5https://github.com/VKCOM/YouTokenToMe
29