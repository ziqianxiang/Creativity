Under review as a conference paper at ICLR 2022
Action-Sufficient State Representation Learn-
ing for Control with Structural Constraints
Anonymous authors
Paper under double-blind review
Ab stract
Perceived signals in real-world scenarios are usually high-dimensional and noisy,
and finding and using their representation that contains essential and sufficient
information required by downstream decision-making tasks will help improve
computational efficiency and generalization ability in the tasks. In this paper, we
focus on partially observable environments and propose to learn a minimal set
of state representations that capture sufficient information for decision-making,
termed Action-Sufficient state Representations (ASRs). We build a generative
environment model for the structural relationships among variables in the system
and present a principled way to characterize ASRs based on structural constraints
and the goal of maximizing cumulative reward in policy learning. We then develop
a structured sequential Variational Auto-Encoder to estimate the environment model
and extract ASRs. Our empirical results on CarRacing and VizDoom demonstrate
a clear advantage of learning and using ASRs for policy learning. Moreover, the
estimated environment model and ASRs allow learning behaviors from imagined
outcomes in the compact latent space to improve sample efficiency.
1	Introduction
State-of-the-art reinforcement learning (RL) algorithms leveraging deep neural networks are usually
data hungry and lack interpretability. For example, to attain expert-level performance on tasks such
as chess or Atari games, deep RL systems usually require many orders of magnitude more training
data than human experts (Tsividis et al., 2017). One of the reasons is that our perceived signals in
real-world scenarios, e.g., images, are usually high-dimensional and may contain much irrelevant
information for decision-making of the task at hand. This makes it difficult and expensive for an agent
to directly learn optimal policies from raw observational data. Fortunately, the underlying states that
directly guide decision-making could be much lower-dimensional (Scholkopf, 2019; Bengio, 2019).
One example is that when crossing the street, our decision on when to cross relies on the traffic lights.
The useful state of traffic lights (e.g., its color) can be represented by a single binary variable, while
the perceived image is high-dimensional. It is essential to extract and exploit such lower-dimensional
states to improve the efficiency and interpretability of the decision-making process.
Recently, representation learning algorithms have been designed to learn abstract features from
high-dimensional and noisy observations. Exploiting the abstract representations, instead of the
raw data, has been shown to perform subsequent decision-making more efficiently (Lesort et al.,
2018). Representative methods along this line include deep Kalman filters (Krishnan et al., 2015),
deep variational Bayes filters (Karl et al., 2016), world models (Ha & Schmidhuber, 2018), PlaNet
(Hafner et al., 2018), DeepMDP (Gelada et al., 2019), stochastic latent actor-critic (Lee et al., 2019),
SimPLe (Kaiser et al., 2019), Bisimulation-based methods (Zhang et al., 2021), Dreamer (Hafner
et al., 2019; 2020), and others (Srinivas et al., 2020; Shu et al., 2020). Moreover, if we can properly
model and estimate the underlying transition dynamics, then we can perform model-based RL or
planning, which can effectively reduce interactions with the environment (Ha & Schmidhuber, 2018;
Hafner et al., 2018; 2019; 2020).
Despite the effectiveness of the above approaches to learning abstract features, current approaches
usually fail to take into account whether the extracted state representations are sufficient and necessary
for downstream policy learning. State representations that contain insufficient information may lead
to sub-optimal policies, while those with redundant information may require more samples and
more complex models for training. We address this problem by modeling the generative process
and selection procedure induced by reward maximization; by considering a generative environment
1
Under review as a conference paper at ICLR 2022
model involving observed states, state-transition dynamics, and rewards, and explicitly characterizing
structural relationships among variables in the RL system, we propose a principled approach to
learning minimal sufficient state representations. We show that only the state dimensions that have
direct or indirect edges to the reward variable are essential and should be considered for decision
making. Furthermore, they can be learned by maximizing their ability to predict the action, given
that the cumulative reward is included in the prediction model, while at the same time achieving
their minimality w.r.t. the mutual information with observations as well as their dimensionality. The
contributions of this paper are summarized as follows:
•	We construct a generative environment model, which includes the observation function, transition
dynamics, and reward function, and explicitly characterizes structural relationships among variables
in the RL system.
•	We characterize a minimal sufficient set of state representations, termed Action-Sufficient state
Representations (ASRs), for the downstream policy learning by making use of structural constraints
and the goal of maximizing cumulative reward in policy learning.
•	In light of the characterization, we develop Structured Sequential Variational Auto-Encoder (SS-
VAE), which explicitly encodes structural relationships among variables, for reliable identification
of ASRs.
•	Accordingly, policy learning can be done separately from representation learning, and the policy
function only relies on a set of low-dimensional state representations, which improve both model
and sample efficiency. Moreover, the estimated environment model and ASRs allow learning
behaviors from imagined outcomes in the compact latent space, which effectively reduce possibly
risky explorations.
2	Environment Model with Structural Constraints
In order to characterize a set of minimal sufficient state representations for downstream policy
learning, we first formulate a generative environment model in partially observable Markov decision
process (POMDP), and then show how to explicitly embed structural constraints over variables in the
RL system and leverage them.
Suppose we have sequences of observations {hot, at, rti}tT=1, where ot ∈ O denotes perceived
signals at time t, such as high-dimensional images, with O being the observation space, at ∈ A is the
performed action with A being the action space, and rt ∈ R represents the reward variable with R
being the reward space. We denote the underlying states, which are latent, by ~st ∈ S, with S being
the state space. We describe the generating process of the environment model as follows:
ot = f ( ~st, et),
rt = g(~st-1, at-1, t),	(1)
~st = h(~st-1, at-1,ηt),
where f, g, and h represent the observation function, reward function, and transition dynamics,
respectively, and et, t, and ηt are corresponding independent and identically distributed (i.i.d.)
random noises. The latent states ~st form an MDP: given ~st-1 and at-1, ~st are independent of states
and actions before t - 1. Moreover, the action at-1 directly influences latent states ~st, instead of
perceived signals ot , and the reward is determined by the latent states (and the action) as well. The
perceived signals ot are generated from the underlying states ~st , contaminated by random noise et .
We also consider noise t in the reward function to capture unobserved factors that may affect the
reward, as well as measurement noise.
It is commonplace that the action variable at-1 may not influence every dimension of ~st, and the
reward rt may not be influenced by every dimension of ~st-1 as well, and furthermore there are
structural relationships among different dimensions of ~st . Figure 1 gives an illustrative graphical
representation, where s3,t-1 influences s2,t, at-1 does not have an edge to s3,t, and among the
states, only s2,t-1 and s3,t-1 have edges to rt. We use Rt = Pτ∞=t γτ-trτ to denote the discounted
cumulative reward starting from time t, where γ ∈ [0, 1] is the discounted factor that determines how
much immediate rewards are favored over more distant rewards.
To reflect such constraints, we explicitly encode the graph structure over variables, including the
structure over different dimensions of ~s and the structures from at-1 to ~st, ~st-1 to rt, and ~st to ot.
2
Under review as a conference paper at ICLR 2022
Figure 1: A graphical illustration
of the generative environment
model. Grey nodes denote ob-
served variables and white nodes
represent unobserved variables.
Here, at-1 does not have an edge
to s3,t, and only s2,t-1 and s3,t-1
have edges to rt , and moreover,
we take into account the struc-
tural relationships among differ-
ent dimensions of latent states ~st .
Accordingly, we re-formulate (1) as follows:
ot = f (Ds÷o ® st, et),
rt = g(Ds÷r Θ st-1, Da÷r Θ at—1, J),
si,t = hi(D~(∙,i) Θ st—1, Da=~(∙,i) Θ at—1, ηi,t),
(2)
for i = 1, ∙ ∙ ∙ ,d, where st = (sι,t,…, sd,t)>, Θ denotes element-wise product, and D(.)are binary
matrices indicating the graph structure over variables. Specifically, D~.o ∈ {0,1}d×1 represents
the graph structure from d-dimensional st to ot, D~^ ∈ {0,1}d×1 the structure from ~t-ι to the
reward variable rt, Da÷r ∈ {0,1} the structure from the action variable at-ι to the reward variable
rt, D~s ∈ {0, 1}d×d denotes the graph structure from d-dimensional ~st—1 to d-dimensional ~st and
D~(∙,il is its i-th column, and Da.~ ∈ {0,1}1×d corresponds to the graph structure from at-ι to st
with Da,~(.,i) representing its i-th column. For example, D~j,i) = 0 means that there is no edge
from sj,t—1 to si,t. Here, we assume that the environment model, as well as the structural constraints,
is invariant across time instance t.
2.1 Minimal Sufficient State Representations
Given observational sequences {hot, at, rti}tT=1, we aim to learn minimal sufficient state representa-
tions for the downstream policy learning. In the following, we first characterize the state dimensions
that are indispensable for policy learning, when the environment model, including structural relation-
ships, is given. Then we provide criteria to achieve sufficiency and minimality of the estimated state
representations, when only {hot, at, rti}tT=1, but not the environment model, is given.
Finding minimal sufficient state dimensions with a given environment model. RL agents learn
to choose appropriate actions according to the current state vector ~st to maximize the cumulative
reward, in which some dimensions may be redundant for policy learning. Then how can we identify
a minimal subset of state dimensions that are sufficient to choose optimal actions? We call such state
dimensions Action-Sufficient state Representations (ASRs), and denote it by ~stASR. We can leverage
the (conditional) independence/dependence relations among the quantities, under the condition
of maximizing cumulative reward, which policy learning aims to achieve, to determine which
dimensions of the states are the ASRs. Moreover, such independence/dependence relations can be
directly seen from the graphical representation of the environment model, under the Markov condition
and faithfulness assumption (Pearl, 2000; Spirtes et al., 1993). Below, we give the graphical condition
that ASRs are expected to satisfy.
Proposition 1. Given the graphical representation corresponding to the environment model, such
as the representation in Figure 1, which is assumed to be Markov and faithful to the measured data,
each dimension in ~stASR has a direct or indirect edge to rt+τ, for τ > 0.
Proposition 1 can be shown based on the global Markov condition and the faithfulness assumption,
which connects d-separation1 to conditional independence/dependence relations. A proof is given in
Appendix. If si,t has a (direct or indirect) edge to rt+τ , the action variable at is dependent on si,t
when the cumulative reward is maximized, which can be thought of as a data selection procedure
depending on the cumulative reward, and thus si,t is needed for action determination. On the other
hand, if si,t does not have an edge to rt+τ , then the action variable at is (conditionally) independent
1A path p is said to be d-separated by a set of nodes Z if and only if (1) p contains a chain i → m → j or a
fork i — m → j such that the middle node m is in Z, or (2) P contains a collider i → m — j such that the
middle node m is not in Z and such that no descendant of m is in Z.
3
Under review as a conference paper at ICLR 2022
on si,t, and thus si,t is not needed for action determination. According to the above proposition, it is
easy to see that for the graph given in Figure 1, we have ~stASR = (s2,t, s3,t)>. That is, we only need
(s2,t, s3,t)>, instead of ~st, for the downstream policy learning.
Minimal sufficient state representation learning from observed sequences. In practice, we
usually do not have access to the latent states or the environment model, but instead only the observed
sequences {hot, at, rti}tT=1. Then how can we learn the ASRs from the raw high-dimensional inputs
such as images? We denote by ~ the estimated whole latent state representations and sasr ⊆ ~ the
estimated minimal sufficient state representations for policy learning.
We collected the data, that are used to learn the environment model and ASRs, with random actions.
As discussed above, action and ASRs are dependent given the cumulative reward—this is a type of
dependence relationship induced by selection on the effect (reward). We can then learn the ASRs
by maximizing their dependence with the action given the cumulative reward, I(~stASR; at | Rt+1),
where I denotes mutual information. Since I(靖SR; at | Rt+ι) = H(at | Rt+ι)-H(at | 靖SR, Rt+ι),
where H (∙) denotes the conditional entropy, and the first term H (at | Rt+ι) does not contain ASRs,
we can estimate ASRs by minimizing H (at | 靖SR, Rt+ι), with
TT
H (at I ~ASR,Rt+ι) = -E Eqφ,α{log Pα(at∣~ASRRt+ι)} = -E Eqφjog Pα(at∣D AsRΘ~,Rt+ι)},
where pα denote the probabilistic predictive model of at with parameters α, qφ,α is the joint distri-
bution over ~ and at with q@a = qψpα, and qφ⑶同-ι, yi：t, aLt-ι) is the probabilistic inference
z7∖∕
model of St with parameters φ and Nt = (0T, rT), and D asr ∈ {0,1}d×1 is a binary vector indicating
which dimensions of St are in SASR, so D asr Θ St gives ASRs SASR.
Moreover, we achieve minimality of the representation by minimizing conditional mutual information
between observed high-dimensional signals Yt and the ASR sasr at time t given data at previous
time instances, and meanwhile minimizing the dimensionality of ASRs with sparsity constraints:
λ1 Xt=2 I (yt； ~ASR 卜 1:t-1,a1:t-1, ~t-1) + λ2kD ASRll1,
where the conditional mutual information can be upper bound by a KL-divergence:
I (yt; MASR|yi：t-i, a1.t-ι,~t-ι) ≤R qφ(~ASR~-ι, y1：t, aιj)log pγ⅛⅞~t-,a,y1ιtD;DaL) d~ASR
=KL(qφ(~ASR∣~t-i, yi：t,ai：t-i)∣Pγ(~ASR|~t-i,at-i； D~,Da.~)),
with pγ being the transition dynamics of ~St with parameters γ.
Furthermore, Proposition 1 shows that given the (estimated) environment model, only those state
dimensions that have a direct or indirect edge to the reward variable are the ASRs. In our learning
procedure, we also take into account the relationship between the learned states ~ and the reward,
1∖∕
and leverage such structural constraints for learning the ASRs. Denote by DASR ∈ {0,1}d×1 a
binary vector indicating whether the corresponding state dimension in ~St has a direct or indirect edge
to the reward variable. Consequently, we enforce the similarity between DASR and DASR by adding
an Li norm on DASR 一 Dasr. Therefore, the ASRs can be learned by maximizing the following
function:
Lmin & suff
入 3 ^Xt ɪ Eqφ,α {lθg Pα(>t∣DASR Θ ~t, Rt + l)} —λ4∣∣D ASR - DD ASR ∣∣1
।	—	=	}
|	{z	}
Sufficiency
-λι Pt=2 KL(qφ(DASR Θ ~t∣~t-ι, yi：t,ai：t—i)∣Pγ(DASR Θ ~t∣~t-1,at-1; D~, Da,~)) 一 λ2∣DASR∣∣
।	、z
1,
}
^™^{^^^™
Minimality
(3)
where λ's are regularization terms, and note that DASR can be directly derived from the estimated
structural matrices D*r and D~( i). The constraint in (3) provides a principled way to achieve
minimal sufficient state representations, related to the information bottleneck method (Tishby et al.,
1999). Notice that it is just part of the objective function to maximize, and it will be involved in the
complete objective function in (4) to learn the whole environment model.
Remarks. By explicitly involving structural constraints, we achieve minimal sufficient state repre-
sentations from the view of generative process underlying the RL problem and the selection procedure
induced by reward maximization, which enjoys the following advantages. 1) The structural infor-
4
Under review as a conference paper at ICLR 2022
mation provides an interpretable and intuitive picture of the generating process. 2) Accordingly,
it also provides an interpretable and intuitive way to characterize a minimal sufficient set of state
representations for policy learning, which removes unrelated information. 3) There is no information
loss when representation learning and policy learning are done separately, which is computationally
more efficient. 4) The generative environment model is fixed, independent of the behavior policy
that is performed. Furthermore, based on the estimated environment model and ASRs, it is flexible
to use a wide range of policy learning methods, and one can also perform model-based RL, which
effectively reduce possibly risky explorations.
3 Structured Sequential VAE for the Estimation of ASRs
In this section, we give estimation procedures for the environment model and ASRs, as well as the
identifiability guarantee in linear cases.
Identifiability in Linear-Gaussian Cases. Below, we first show the identifiability guarantee in the
linear case, as a special case of Eq. (2). In the linear case (see the environment model given in Eq.
(5) in Appendix D), D~÷0, D~÷r, Da”., D~, and。。,~ are linear coefficients, indicating corresponding
graph structures and also the strength. Denote the covariance matrices of et and t by Σe and
Σe, respectively. Further let D~,。:= (D~^o, D›)>. The following proposition shows that the
environment model in the linear case is identifiable up to some orthogonal transformation on certain
coefficient matrices from observed data {hot, at, rti}tT=1.
Proposition 2 (Identifiability). Suppose the perceived signal ot, the reward rt, and the latent states
St follow a linear environment model. Ifassumptions A1 〜A4 (given in Appendix C) hold and with the
second-order statistics of the observed data {hot, at, rti}T=ι, the noise variances ∑e and Σe, D。..,
k
D~.oD~ Dα.~ (with k ≥ 0), and D~.oD>.o are uniquely identified.
This proposition shows that in the linear case, with the second-order statistics of the observed
data, we can identify the parameters up to orthogonal transformations. In particular, suppose
the linear environment model with parameters (D~.o, Ds.r, Dα.r, Ds, Da.~, ∑e, Σe) and that with
〜
(D灵o, D~.r, Da.., D~, Da.~, Σg, Σ∈) are observationally equivalent. Then We have D~.o = D~.oU,
pʌ	pʌ T r-Γ TΛ T T T^A	T-A T T V^Λ X~∖	1 V-∖	X-∖	1	T T ∙	. 1	1,.
Da÷r=Da÷r, Ds = U > DSU, Da.~ = Da.~U, ∑巨= ∑e, and Σg = Σe, where U is an orthogonal matrix.
General Nonlinear Cases. To handle general nonlinear cases with the generative process given in
Eq. (2), we develop a Structured Sequential VAE (SS-VAE) to learn the model (including structural
constraints) and infer latent state representations st and ASRs 耍SR, with the input {hot, at, rti}tT=1.
Specifically, the latent state dimensions are organized with structures, captured by D~s, to achieve
conditional independence. The structural relationships over perceived signals, latent states, the action
variable, and the reward variable are also embedded as free parameters (i.e., D~.o, D~.., Da.r, Da.s)
into SS-VAE. Moreover, we aim to learn state representations ~sst and ASRs ~sstASR that satisfy the
following properties: (i) ~st should capture sufficient information of observations ot, rt, and at, that is,
it should be enough to enable reconstruction. (ii) The state representations should allow for accurate
predictions of the next state and also the next observation. (iii) The transition dynamics should follow
an MDP. (iv) ~sstASR are minimal sufficient state representations for the downstream policy learning.
Let y1:T = {(ot> , rt> )> }tT=1. To achieve the above properties, we maximize the following objective
function:	T-2
L(yi：T ； (θ,φ,γ,α,D(.))) = P Eqφ{ log pθ (ot∣~t; D~+°) + log pθ (rt+ι∣~t,at; D~+r ,Da”)
t=ι	`-----------------------V----------------------}
Reconstruction
+ log Pθ (ot+ι∣~t) + log Pθ(rt+2∣~t,at+1) } + λ P Eqφa {log Pα(at ∣~t,Rt+ι; D asr)}
S---------------------V--------------------}	t=ι ,	、--------------V---------------}
Prediction	Sufficiency
T
-λι P KL(qφ国∣~t-ι,yi：t,ai：t-i
t=2
)k PY (st∣st-1,at-1, Ds, Days') ) — λ2 ∣∣D ASRkI
、	、z	.
z
Transition
Conditional disentanglement & Minimality
—(λ5∣∣D弟o∣i + λ6∣D~yr ∣1 + λ7∣∣D~ki + λ8∣Da + ~∣1 + λ4 ∣D ASR — D ASRk1),
、 — 一一 ,
Sparsity
(4)
which contains the reconstruction error at each time instance, the one-step prediction error of
observations, the KL divergence to constrain the latent space, and moreover, the MDP restrictions
on transition dynamics, the sufficiency and minimality guarantee of state representations for policy
5
Under review as a conference paper at ICLR 2022
learning, as well as sparsity constraints on the graph structure. We denote by pθ the generative model
with parameters θ and structural constraints D(.), q@ the inference model with parameters φ, pγ the
transition dynamics, and pα action prediction with ASRs given the cumulative reward Rt+1. Each
factor in pγ and qφ is modeled with a mixture of Gaussians (MoGs), to approximate a wide class of
continuous distributions.
Below are the details of each component in the above objective function:
•	Reconstruction and prediction components: These two parts are commonly used in sequential VAE.
They aim to minimize the reconstruction error and prediction error of the perceived signal ot and
the reward rt .
•	Transition component: To achieve the property that state representations satisfy an MDP, we
explicitly model the transition dynamics: logpγ(st∣st-ι,at-ι; D~, Da.~). In particular, st∣st-ι IS
modelled with a mixture of Gaussians: PK=InkN(μk(~t-ι,at-ι), ∑k (1~t-ι,at-ι)), where K is
the number of mixtures,代卜(∙) and ∑k (∙) are given by multi-layer perceptrons (MLP) with inputs
st-ι and at-ι, parameters γ, and structural constraints Ds and Da.~. ThiS explicit constraint
on state dynamics is essential for establishing a Markov chain in latent space and for learning a
representation for long-term predictions. Note that unlike in traditional VAE (Kingma & Welling,
2013), We do not assume that different dimensions in 豆 are marginally independent, but model
their structural relationships explicitly to achieve conditional independence.
•	KL-divergence constraint: The KL divergence is used to constrain the state space with multiple
purposes: (1) It is used in the lower bound of log P (y1:T) to achieve conditional disentanglement
between q@Ri,t∣∙) and qφ(Sj,t∣∙) for i = j, (2) and also to achieve minimality of ASRs.
•	Sufficiency & minimality constraints: We achieve minimal sufficient state representations for
the downstream policy learning by leveraging action prediction given the cumulative reward, the
conditional mutual information between y and 靖sr, and structural constraints. For details, please
refer to Section 2.1.
•	Sparsity constraints: According to the edge-minimality property (Zhang & Spirtes, 2011), we
additionally put sparsity constraints on structural matrices to achieve better identifiability. In
particular, we use L1 norm of the structural matrices as regularizers in the objective function to
achieve sparsity of the solution.
Figure 2 gives the diagram of the neural network architecture in model training. We use SS-VAE
to learn the environment model and ASRs. Specifically, the encoder, which is used to learn the
inference model qφ(st∖st-ι, yi：t, aLt-ι), includes a Long Short-Term Memory (LSTM (Hochreiter
& Schmidhuber, 1997)) to encode the sequential information with output ht and a Mixture Density
Network (MDN (Bishop, 1994)) to output the parameters of MoGs. At each time instance, the input
hot+1, rt+1, ati is projected to the encoder and a sample of sst+1 is inferred from qφ as output. The
generated sample further acts as an input to the decoder, together with at+1 and structural matrices
Ds+o, Ds+r, and Daw. Then the decoder outputs ^t+ι and rt+2. Moreover, the state dynamics which
satisfies a Markov process and is embedded with structural constraints Ds and Da+~, is modeled
with an MLP and MDN, marked with red in Figure 2. The action prediction part (denoted by AP),
which helps sufficient state representations, uses MLP and is marked with blue. During training, we
approximate the expectation in L by sampling and then jointly learn all parameters by maximizing L
using stochastic gradient descent.
4 Policy Learning with ASRs
After estimating the generative environment model, we are ready to learn the optimal policy, where
the policy function only depends on low-dimensional ASRs, instead of high-dimensional images.
The entire procedure roughly contains the following three parts: (1) data collection with a random
policy, (2) environment model estimation (with details in Section 3), and (3) policy learning with
ASRs. Notably, the generative environment model is fixed, regardless of the behavior policy that is
used to generate the data, and after learning the environment model, as well as the inference model
for ASRs, our framework is flexible for both model-free and model-based policy learning.
Model-Free Policy Learning. For model-free policy learning, we make use of the learned
environment model to infer ASRs ssstASR from past observed sequences {o≤t, r≤t, a≤t-1} and then
predict the action with the estimated low-dimensional ASRs. Our method is flexible to use a wide
range of model-free methods; for example, one may use deep Q-learning for discrete actions (Mnih
6
Under review as a conference paper at ICLR 2022
Figure 2: Diagram of neural net-
work architecture to learn state
representations. The correspond-
ing structural constraints are in-
volved in “Deconv” and “MLP”,
and “AP" represents the action pre-
diction part for sufficient state rep-
resentation learning.
et al., 2015) and deep deterministic policy gradient (DDPG) for continuous actions (Lillicrap et al.,
2015). Algorithm 1 in Appendix F gives the detailed procedure of model-free policy learning with
ASRs in partially observable environments.
Model-Based Policy Learning. The downside of model-free RL algorithms is that they are
usually data hungry, requiring very large amounts of interactions. On the contrary, model-based
RL algorithms enjoy much better sample efficiency. Hence, we make use of the learned generative
environment model, including the transition dynamics, observation function, and reward function,
for model-based policy optimization. Based on the generative environment model, one can learn
behaviors from imagined outcomes to increase sample-efficiency and mitigate heavy and possibly
risky interactions with the environment. We present the procedure of the classic Dyna algorithm
(Sutton, 1990; Sutton & Barto, 2018) with ASRs in Algorithm 2 in Appendix F.
5	Experiments
To evaluate the proposed approach, we conducted experiments on both CarRacing (Klimov, 2016)
and VizDoom (Kempka et al., 2016) environments, following the setup in the world model (Ha &
Schmidhuber, 2018) for a fair comparison. Itis known that CarRacing is very challenging—the recent
world model (Ha & Schmidhuber, 2018) is the first known solution to achieve the score required to
solve the task. Without stated otherwise, all results were averaged across five random seeds, with
standard deviation shown in the shaded area.
5.1	CarRacing Experiment
CarRacing is a continuous control task with three continuous actions: steering left/right, acceleration,
and brake. Reward is -0.1 every frame and +1000/N for every track tile visited, where N is the
total number of tiles in track. It is obvious that the CarRacing environment is partially observable: by
just looking at the current frame, although we can tell the position of the car, we know neither its
direction nor velocity that are essential for controlling the car. For a fair comparison, we followed the
same setting as in Ha & Schmidhuber (2018). Specifically, we collected a dataset of 10k random
rollouts of the environment, each consisting of 1000 time steps. The dimensionality of latent states ~st
was set to d = 32, determined by hyperparameter tuning.
Analysis of ASRs. To demonstrate the structures over observed frames, latent states, actions, and
rewards, We visualized the learned Ds>o, Ds>r, Ds, and Da>~, as shown in Figure 9 in Appendix
G. Intuitively, We can see that Ds>r and Da>~ have many values close to zero, meaning that the
reward is only influenced by a small number of state dimensions, and not many state dimensions are
influenced by the action. Furthermore, from Ds, We found that there are influences from ~i,t to si,t+1
(diagonal values) for most state dimensions, which is reasonable because we want to learn an MDP
over the underlying states, While the connections across states (off-diagonal values) are much sparser.
Compared to the original 32-dim latent states, ASRs have only 21 dimensions. BeloW, We empirically
shoWed that the loW-dimensional ASRs significantly improve the policy learning performance in
terms of both efficiency and efficacy.
Comparison Between Model-Free and Model-Based ASRs. We applied both model-free
(DDPG) (Lillicrap et al., 2015) and model-based (Dyna and Prioritized SWeeping) algorithms (Sutton,
1990) to ASRs (With 21-dims). As shoWn in Figure 3, interestingly, by taking advantage of the
learned generative model, model-based ASRs is superior to model-free ASRs at a faster rate, Which
7
Under review as a conference paper at ICLR 2022
Oooooo
2 0 8 6 4 2
1 1
PJM ①① >q-nEn□
500	1000	1500	2000
Episodes
Figure 3: Cumulative rewards of
model-based ASRs, model-free ASRs,
700λ
0
ω⊃-ro> ssuw
100
Generation
VRL, SLAC, PlaNet, DBC and
Dreamer evaluated on CarRacing.
Figure 4: Fitness Value of ASRs
compared to world models evaluated
on CarRacing, including mean score,
max score, and the best average score.
demonstrates the effectiveness of the learned model. It also shows that with the estimated environment
model and ASRs, we can learn behaviors from imagined outcomes to improve sample-efficiency.
Comparison with VRL, SLAC, PlaNet, DBC, and Dreamer. We also compared the proposed
framework of policy learning with ASRs (with 21-dims) with 1) the same learning strategy but with
vanilla representation learning (VRL, implemented without the components for minimal sufficient
state representations as in Eq. (3)), 2) SLAC (Lee et al., 2019), 3) PlaNet (Hafner et al., 2018), 4)
DBC (Zhang et al., 2021), and 5) Dreamer (Hafner et al., 2019). For a fair comparison, the latent
dimensions of VRL, PlaNet, SLAC, DBC and Dreamer are set to 21 as well, and we require all of
them to have the model capacity similar to ours (i.e., similar model architectures). From Figure 3, we
can see that our methods, both model-free and model-based, obviously outperform others. It is worth
noting that the huge performance difference between ASRs and VRL shows that the components for
minimal sufficient state representations play a pivotal role in our objective.
Comparison with World Models. In light of the fact that world models (Ha & Schmidhuber, 2018)
achieved good performance in CarRacing, we further compared our method (with 21-dim ASRs)
with the world model. For a fair comparison, following Ha & Schmidhuber (2018), we also used
the Covariance-Matrix Adaptation Evolution Strategy (CMA-ES) (Hansen, 2016) with a population
of 64 agents to optimize the parameters of the controller. In addition, following the same setting as
in Ha & Schmidhuber (2018) (where the agent’s fitness value is defined as the average cumulative
reward of the 16 random rollouts), we show the fitness values of the best performer (max) and the
population (mean) at each generation (Figure 4). We also took the best performing agent at the end
of every 25 generations and tested it over 1024 random rollout scenarios to record the average (best
avg score). It is obvious that our method (denoted by ASR-*) has a more efficient and also efficacy
training process. The best average score of ASRs is 65 higher than that of world models.
Comparison with Dreamer and DBC with Background Distraction. We further compared
ASRs (with 21-dims) with Dreamer and DBC when there are natural video distractors in CarRacing;
we chose Dreamer and DBC, because their performance are relatively better than other comparisons
when there are no distractors. Specifically, we followed Zhang et al. (2021) to incorporate natural
video from the Kinetics dataset (Kay et al., 2017) as background in CarRacing. Similarly, for a fair
comparison, we require all of them to have the same latent dimensions and have the similar model
capacity. As shown in Figure 5, we can see that our method outperforms both Dreamer and DBC.
Ablation Study. We further performed ablation studies on latent dynamics prediction; that is, we
compared with the case when the transition dynamics in Eq. (4) is not explicitly modeled, but is
replaced with a standard normal distribution. Figure 10 in Appendix G shows that by explicitly
modelling the transition dynamics (denoted by with LDP), the cumulative reward has an obvious
improvement over the one without modelling the transition dynamics (denoted by without LDP).
5.2	VizDoom Experiment
We also applied the proposed method to VizDoom take cover scenario (Kempka et al., 2016), which
is a discrete control problem with two actions: move left and move right. Reward is +1 at each
time step while alive, and the cumulative reward is defined to be the number of time steps the agent
manages to stay alive during an episode.
Considering that in the take over scenario the action space is discrete, we applied the widely used
DQN (Mnih et al., 2013) on ASRs for policy learning. In addition to the comparisons with VRL (as
in CarRacing) and DQN on raw observations, we further compared with another common approach to
8
Under review as a conference paper at ICLR 2022
Model
Cumulative
Rewards
Dreamer
DBC
Model-free ASRs
Model-based ASRs
621±124.5
803±112.5
938±87.2
954±98.6
p∙lEMα,κα,>4-nEnu
Ob
Figure 5: Comparisons with
Dreamer and DBC in CarRacing
with natural video distractors, af-
ter 2000 training episodes, with
standard error.
500	1000	1500
Episodes
Figure 6: Comparing ASRs and
SOTA methods evaluated on Viz-
Doom.
DQN on ASRS
DQN on VRL
DRQN on Observations
DQN on Observations
Model-based ASRs
ω≡-ra> ss∙,u~zl
Figure 7: Fitness value of ASRs
(with CMA-ES) compared to world
models evaluated on VizDoom.
POMDPs: DRQN (Hausknecht & Stone, 2015). As shown in Figure 6, DQN on ASRs achieve a much
better performance than all other comparisons, and in particular, DQN on ASRs outperforms DRQN
on observations by around 400 on average in terms of cumulative reward. Similarly, we applied
model-based (Dyna) algorithms (Sutton, 1990) to ASRs (with 21-dims). As shown in Figure 6, we
can draw the same conclusion that by taking advantage of the learned generative model, model-based
ASRs is superior to model-free ASRs at a faster rate. We also applied ASRs to world models, where
Figure 7 shows that our method with ASRs (denoted by ASR-*) achieves a better performance.
6	Related Work
In the past few years, a number of approaches have been proposed to learn low-dimensional Markovian
representations, which capture the variation in the environment generated by the agent’s actions,
without direct supervision (Lesort et al., 2018; Krishnan et al., 2015; Karl et al., 2016; Ha &
Schmidhuber, 2018; Watter et al., 2015; Zhang et al., 2018; Kulkarni et al., 2016; Mahadevan &
Maggioni, 2007; Gelada et al., 2019; Gregor et al., 2018; Ghosh et al., 2019; Zhang et al., 2021).
Common strategies for such state representation learning include reconstructing the observation,
learning a forward model, or learning an inverse model. Furthermore, prior knowledge, such as
temporal continuity (Wiskott & Sejnowski, 2002), can be added to constrain the state space.
Recently, much attention has been paid to world models, which try to learn an abstract representation
of both spatial and temporal aspects of the high-dimensional input sequences (Watter et al., 2015;
Ebert et al., 2017; Ha & Schmidhuber, 2018; Hafner et al., 2018; Zhang et al., 2019b; Gelada et al.,
2019; Kaiser et al., 2019; Hafner et al., 2019; 2020). Based on the learned world model, agents can
perform model-based RL or planning. Our proposed method is also in the class of world models,
which models the generative environment model, and additionally, encodes structural constraints
and achieves the sufficiency and minimality of the estimated state representations from the view of
generative and selection process. In contrast, Shu et al. (2020) makes use of contrastive loss, as an
alternative of reconstruction loss; however, it only focuses on the transition dynamics and also fails to
ensure the sufficiency and minimality. Another line of approaches of state representation learning
is based on predictive state representations (PSRs) (Littman & Sutton, 2002; Singh et al., 2004). A
recent approach generalizes PSRs to nonlinear predictive models, by exploiting the coarsest partition
of histories into classes that are maximally predictive of the future (Zhang et al., 2019a). Moreover,
bisimulation-based methods have also attracted much attention (Castro, 2020; Zhang et al., 2021).
On the other hand, our work is also related to Bayesian network learning and causal discovery
(Spirtes et al., 1993; Pearl, 2000; Huang* et al., 2020). For example, Strehl et al. (2007) considers
factorized-state MDP with structures being modeled with dynamic Bayesian network or decision
trees. Incorporating such structure information has shown benefits in several machine learning tasks
(Zhang* et al., 2020; Huang et al., 2019), and in this paper, we show its advantages in POMDPs.
7	Conclusions and Future Work
In this paper, we develop a principled framework to characterize a minimal set of state representations
that suffice for policy learning, by making use of structural constraints and action predictions.
Accordingly, we propose SS-VAE to reliably extract such a set of state representations from raw
observations. The estimated environment model and ASRs allow learning behaviors from imagined
outcomes in the compact latent space, which effectively reduce sample complexity and possibly
risky interactions with the environment. The proposed approach shows promising results on complex
environments-CarRacing and Vizdoom. The future work along this direction include investigating
identifiability conditions in general nonlinear cases and extending the approach to cover heterogeneous
environments, where the generating processes may change over time or across domains.
9
Under review as a conference paper at ICLR 2022
Reproducibility S tatement
Proofs of all our theoretical results are given in Appendix A, B, C, D with disclosure of all assumptions.
More details about the model estimation and policy learning are given in Appendix E and Appendix
F, respectively. Appendix G provides more experimental details (including a description of the
hyperparameters) and results. All datasets used are publicly available or instructions are provided on
how to generate them in Appendix G. A description of the network architectures used is included in
Appendix H. Appendix I introduces the platform and license. Source code for all our experiments
will be made available on GitHub after deanonymization, providing also a complete description
of our experimental environment, configuration files and instructions on the reproduction of our
experiments.
References
Y. Bengio. The consciousness prior. arXiv preprint arXiv:1709.08568, 2019.
C.	M. Bishop. Mixture density networks. In Technical Report NCRG/4288, Aston University,
Birmingham, UK, 1994.
Pablo Samuel Castro. Scalable methods for computing state similarity in deterministic Markov
decision processes. AAAI, 2020.
F. Ebert, C. Finn, A. X. Lee, and S. Levine. Self-supervised visual planning with temporal skip
connections. ArXiv Preprint ArXiv:1710.05268, 2017.
C. Gelada, S. Kumar, J. Buckman, O. Nachum, and M. G. Bellemare. Deepmdp: Learning continuous
latent space models for representation learning. In International Conference on Machine Learning
(ICML), 2019.
D. Ghosh, A. Gupta, and S. Levine. Learning actionable representations with goal conditioned
policies. ICLR, 2019.
K. Gregor, G. Papamakarios, F. Besse, L. Buesing, and T. Weber. Temporal difference variational
auto-encoder. arXiv preprint arXiv:1806.03107, 2018.
D. Ha and J. Schmidhuber. World models. In Advances in Neural Information Processing Systems,
2018.
D. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and J. Davidson. Learning latent
dynamics for planning from pixels. arXiv preprint arXiv:1811.04551, 2018.
D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi. Dream to control: Learning behaviors by latent
imagination. arXiv preprint arXiv:1912.01603, 2019.
D.	Hafner, T. Lillicrap, M. Norouzi, and J. Ba. Mastering atari with discrete world models. arXiv
preprint arXiv:2010.02193, 2020.
N. Hansen. The CMA evolution strategy: A tutorial. arXiv preprint arXiv:1604.00772, 2016.
M. Hausknecht and P. Stone. Deep recurrent q-learning for partially observable mdps. In 2015 AAAI
Fall Symposium Series, 2015.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780,
1997.
B. Huang, K. Zhang, M. Gong, and C. Glymour. Causal discovery and forecasting in nonstationary
environments with state-space models. In International Conference on Machine Learning (ICML),
2019.
B. Huang*, K. Zhang*, J. Zhang, J. Ramsey, R. Sanchez-Romero, C. Glymour, and B. Scholkopf.
Causal discovery from heterogeneous/nonstationary data. JMLR, 21(89), 2020.
L.	Kaiser, M. Babaeizadeh, P Milos, B. Osinski, R. H. Campbell, K. Czechowski, •…，and
H. Michalewski. Model-based reinforcement learning for Atari. arXiv preprint arXiv:1903.00374,
2019.
10
Under review as a conference paper at ICLR 2022
M.	Karl, M. Soelch, J. Bayer, and P. van der Smagt. Deep variational bayes filters: Unsupervised
learning of state space models from raw data. arXiv preprint arXiv:1605.06432, 2016.
W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan, F. Viola, T. Green,
T. Back, P. Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950,
2017.
M. KemPka, M. Wydmuch, G. Runc, J. Toczek, and W. ja´kowski. ViZDoom: A Doom-based
AI research platform for visual reinforcement learning. In IEEE Conference on Computational
Intelligence and Games, pp. 341-348, Santorini, Greece, Sep 2016. IEEE. URL http：//arxiv.
org/abs/1605.02097. The best PaPer award.
D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,
2013.
O. Klimov. Carracing-v0. http://gym.openai.com/, 2016.
R.G. Krishnan, U. Shalit, and D. Sontag. Deep kalman filters. arXiv preprint arXiv:1511.05121,
2015.
T. D. Kulkarni, A. Saeedi, S. Gautam, and S. J. Gershman. Deep successor reinforcement learning.
arXiv preprint arXiv:1606.02396, 2016.
A. X. Lee, A. Nagabandi, P. Abbeel, and S. Levine. Stochastic latent actor-critic: Deep reinforcement
learning with a latent variable model. arXiv preprint arXiv:1907.00953, 2019.
T. Lesort, N. Diaz-Roddguez, J. F. Goudou, and D. Filliat. State representation learning for control:
An overview. Neural Networks, 108:379-392, 2018.
T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous
control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
M. L. Littman and R. S. Sutton. Predictive representations of state. In In Advances in neural
information processing systems, pp. 1555-1561, 2002.
S. Mahadevan and M.. Maggioni. Proto-value functions: A laplacian framework for learning
representation and control in markov decision processes. Journal of Machine Learning Research
(JMLR), 8:2169-2231, 2007.
V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller.
Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Ried-
miller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King,
D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforce-
ment learning. Nature, 518(7540):529-533, 2015.
J. Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, Cambridge,
2000.
B. Scholkopf. Causality for machine learning. arXiv preprint arXiv:1911.10500, 2019.
R.	Shu, T. Nguyen, Y. Chow, T. Pham, K. Than, M. Ghavamzadeh, S. Ermon, and H. Bui. Predictive
coding for locally-linear control. In Proceedings of the 37th International Conference on Machine
Learning, volume 119 of Proceedings of Machine Learning Research. PMLR, 2020.
S.	Singh, M. R James, and M. R Rudary. Predictive state representations: A new theory for
modeling dynamical systems. In In Proceedings of the 20th conference on Uncertainty in artificial
intelligence, pp. 512-519, 2004.
P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search. Spring-Verlag Lectures
in Statistics, 1993.
A. Srinivas, M. Laskin, and P. Abbeel. Curl: Contrastive unsupervised representations for reinforce-
ment learning. ICML, 2020.
11
Under review as a conference paper at ICLR 2022
A. L. Strehl, C. Diuk, and M. L. Littman. Efficient structure learning in factored-state mdps. In AAAI,
2007.
R. S. Sutton. Integrated architectures for learning, planning, and reacting based on approximating
dynamic programming. In Machine learning proceedings 1990, pp. 216-224. Elsevier, 1990.
R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018.
N. Tishby, F. C. Pereira, and W. Bialek. The information bottleneck method. The 37th annual Allerton
Conference on Communication, Control, and Computing, 1999.
P. A. Tsividis, T. Pouncy, J. L. Xu, J. B. Tenenbaum, and S. J. Gershman. Human learning in atari. In
AAAI Spring Symposium Series, 2017.
M. Watter, J. Springenberg, J. Boedecker, and M. Riedmiller. Embed to control: A locally linear
latent dynamics model for control from raw images. NeurIPS, 2015.
L.	Wiskott and T. J. Sejnowski. Slow feature analysis: Unsupervised learning of invariances. Neural
Computation, 14(4):715-770, 2002.
A. Zhang, Z. C. Lipton, L. Pineda, K. Azizzadenesheli, A. Anandkumar, L. Itti, J. Pineau, and
T. Furlanello. Learning causal state representations of partially observable environments. arXiv
preprint arXiv:1906.10437, 2019a.
A. Zhang, R. McAllister, R. Calandra, Y. Gal, and S. Levine. Learning invariant representations for
reinforcement learning without reconstruction. ICLR, 2021.
J.	Zhang and P. Spirtes. Intervention, determinism, and the causal minimality condition. Synthese,
182(3):335-347, 2011.
K.	Zhang and A. Hyvarinen. A general linear non-gaussian state-space model: Identifiability,
identification, and applications. In Asian Conference on Machine Learning, pp. 113-128, 2011.
K. Zhang*, M. Gong*, P. Stojanov, B. Huang, Q. Liu, and C. Glymour. Domain adaptation as a
problem of inference on graphical models. In NeurIPS, 2020.
M. Zhang, S. Vikram, L. Smith, P. Abbeel, M. J. Johnson, and S. Levine. Solar: deep structured
representations for model-based reinforcement learning. arXiv preprint arXiv:1808.09105, 2018.
M.	Zhang, S. Vikram, L. Smith, P. Abbeel, M. Johnson, and S. Levine. Self-supervised visual
planning with temporal skip connections. ICML, 2019b.
12
Under review as a conference paper at ICLR 2022
Appendices for “Action-Sufficient State Representation
Learning for Control with Structural Constraints "
A Proof of Proposition 1
We first give the definitions of the Markov condition and the faithfulness assumption, which will be
used in the proof.
Definition 1 (Global Markov Condition (Spirtes et al., 1993; Pearl, 2000)). The distribution p over V
satisfies the global Markov property on graph G if for any partition (A, B, C) such that B d-separates
A from C,
p(A,C|B) = p(A|B)p(C|B).
Definition 2 (Faithfulness Assumption (Spirtes et al., 1993; Pearl, 2000)). There are no independen-
cies between variables that are not entailed by the Markov Condition.
Below, we give the proof of Proposition 1.
Proof. We first show that if si,t ∈ ~stASR, then it has a direct or indirect edge to rt+τ.
We prove it by contradiction. Suppose that si,t does not have a direct or indirect edge to rt+τ .
According to the Markov assumption, si,t is independent of at given R. Hence, si,t is not necessary
for decision making, and thus si,t is not a dimension in ~stASR, which contradicts to the assumption.
Since we have a contradiction, it must be that si,t has a direct or indirect edge to rt+τ.
We next show that if si,t has a direct or indirect edge to rt+τ , then si,t ∈ ~stASR.
Similarly, by contradiction suppose that si,t is not a dimension in ~stASR. It means that si,t is
independent on at given R and some other variables. Then according to the faithfulness assumption,
si,t does not have a direct or indirect edge to rt+τ, which contradicts to the assumption.
□
B Minimality of the Representation
In this section, we give the detailed derivation of the minimality of the state representation given in
Section 2.1.
We achieve minimality of the representation by minimizing conditional mutual information between
observed high-dimensional signals yt, where y = {oT, W}, and the ASR 耍SR at time t given data
at previous time instances, and meanwhile minimizing the dimensionality of ASRs with sparsity
constraints:
T
λ1 Xt=2 I(yt; ~t	卜 1:t-1,a1:t-1, ~t-I) + λ2kD l∣ι.
Note that in the above conditional mutual information, we need to conditional on the previous states
~t-ι, instead of <~asR, which two give different conditional mutual information. It can be shown
by contradiction. Suppose I(yt ; ~st |y1:t-1 , a1:t-1 ,~t-ι) = I(yt； ~ASR卜i：t-i,ai：t-i, ~asr), and
denote ~s C = ~s\~s ASR. Then the equivalence implies that ~stC-1 is independent of ot (where ot ∈ yt)
given {yi：t—i,ai：t—i, ∙~asR}. It is obviously violated for the example given in Figure 1, where
~sC = s1 and ~s ASR = {s2, s3}, and s1,t-1 is dependent on ot given {y1:t-1, a1:t-1, s2,t-1, s3,t-1}.
Hence, conditioning on ~t-ι and ~t-SR give different conditional mutual information. Therefore, in
the above conditional mutual information, we need to condition on the previous states ~st-1.
13
Under review as a conference paper at ICLR 2022
Moreover, the conditional mutual information I(yt；碎ASR 卜上t_1,。上-1,可-1) can be upper bound
by a KL-divergence:
I(yt；靖SR卜i：t-i,ɑi:t-i,st-i)	~
=I(yt；靖SR, {yi：t-i,ai：t-i, St-1}) - I(yt； {yi：t-i,ai：t-i, St-1})	~
=[H(yt) - H(yt∣FASRJ y1：t-1, a1：t-1j St-1)] - [H(yt) -~H(yt∣yLt-1, art-1, Ft-1)]
=H (yt 卜 1：t-1 ,a1：t-1,豆-1) - H(yt∣SASR, y1：t-1, a1：t-1, st-1)
~ ~
-p(yt|y1：t-1,a1：t-1, st-1) logp(yt 卜 1：t-1, art-1, st-1)
+ p(yt|sASR, y1：t-1, a1：t-1,§t-1) logp(yt∣FASR, y1：t-1,«1：t-1,豆-1)
≤ -p(yt∣ 靖SR, yrt-1,αrt-1, st-1) logp(yt 卜 1：t-1, «!：1, St-1)
+ p(yt∣ FASR, y1：t-1,«1：t-1, St-I) log p(yt∣ SASR, y1：t-1,a1：t-1, Ft-I)
J qφ( fasr∣ Ft-1, y1：t, «1：t-1)log
KL("( FASR
qφ(s 优S RlSt-I ,yi：t,ai：t—1)	“F ASR
PY (^ASRFt-i ,at-i；Ds,Daas)	t
同一1, y1：t, a1：t-1)kPγ ( fasr∣ Ft-1,at-1；D~ ,u),

with PY being the transition dynamics of St with parameters Y.
C Assumptions of Proposition 2
To show the identifiability of the model in the linear case, we make the following assumptions:
A1. do + dr ≥ ds, where ∣ot∣ = do, ∣rt∣ = dr, and ∣st∣ = ʤ.
A2. (D>o, D>『)> is full column rank and DS is full rank.
A3. The control signal at is i.i.d. and the state St is stationary.
A4. The process noise has a unit variance, i.e., var(ηt) = I.
D PROOF OF PROPOSITION 2
Proof. The proof of the linear case without control signals has been shown in Zhang & Hyvarinen
(2011). Below, we give the identifiability proof in the linear-Gaussian case with control signals:
Ot = D S÷o st + et,
rt+1 = D S÷r st + Da÷r αt + et+1,
st = DSst-1 + Da+Sat-1 + ηt ∙
(5)
Let yt+1 = [o>,r"T, DS⅛O = [D>o, D>r]τ, Dα÷r = [0>, DL]T, and et =归1,e.F. Then
the above equation can be represented as:
(__ R → ɪ r∖	, ∙.∙
yt = DSaO st + Da÷rat + et,
ISt = D SSt-I + Da-Sat-1 + ηt.
(6)
Because the dynamic system is linear and Gaussian, we make use of the second-order statistics of the
observed data to show the identifiability. We first consider the cross-covariance between yt+k and at:
f Cov(yt+k ,at) = D s- ODk-IDa-S ∙ Var(at), if k > 0.
I Cov(yt+k,at) = Da-r ∙ Var(at),	if k = 0.
r-ɪ-ɪ-l	Γ∙	,1	∙	1	1	∙ 1	. ∙ Γ∙ TΛ 7~Λ	TΛ
Thus, from the cross-covariance between yt+k and at, we can identify DS-°Da-s, Da-
⑺
L- r, and
Ds- oDkDaS for k > 0.
Next, we consider the auto-covariance function of s. Define the auto-covariance function of s at
lag k as Rs(k) = E[ st s>k], and similarly for Ry(k). Clearly, Rs(-k) = Rs(k)τ and Ry(-k)=
Ry(k)τ. Then we have
Rs(k) = r s(k - I) ∙ D>,
if k > 0,
Rs(k) = R>(1) ∙ DT + Da+sVar(at-1)DT S +1, if k = 0.
(8)
ɪʌ 1	C *	∙ 1 .Λ	1	7	,	7	T τ,~	τ∖ →	~	, τ∖	,
Below, we first consider the case where d° + dr = d§. Let Yt = Ds-ost, so Yt = Yt + Da-rat-1 + et
and Ry (k) = Ds-oRst (k)D>。. Ry (k) satisfies the recursive property:
Ry(k) = Ry(k — 1) ∙ Ωτ,	if k > 0,
Ry(k) = RT (1) ∙ Ωτ + DSUDa-SVar(at-1)D>S + I)DT。, if k = 0,
(9)
14
Under review as a conference paper at ICLR 2022
1
where Ω = DsOD~D~,o.
Denote Sk = D弟oDk-1Dα.~ ∙ Var(at). Then we can derive the recursive property for Ry (k):
(Ry(k)	=	Ry(k — 1) ∙ Ω> — Da+rSi1Ω> + Do.rS>,	if k > 1,
J	Ry(k)	=	Ry(k — 1) ∙ Ω> — Da+rVarτ(at)DLΩ> — ∑eΩτ	+ D。”S>,	if k = 1,
(Ry (k) = Ry (I) ∙ QT + (Da+rVar(at)DD>+r + ^e)
I	+ D弟O(Da+ SVar(at)Dl S + 1)D τ+o,	if k = 0∙
When k = 2, we have
Ry (2) = Ry (1) ∙ Ωτ — Da+r STΩτ + Da+r S> .
The above equation can be re-organized as
(Ry(2) — D a+r ∙ ST) = (Ry(1) — D a+r，ST) ∙ Ω>.
Because Da+r and Sk are identifiable, and suppose (Ry(1) — Da+r ∙ ST) is invertible, Ω =
Ds+oDSD弟O is identifiable.
We further consider Ry(0) and Ry(1) and write down the following form:
-Ry (0) — D s+o(Da+ SVar(at-ι)Dls + / )D>√
[	Ry (1)	]
=-Ry(1)] oτ+ -	Da+rVar(at)DL	] +ς - I 一
=	Ry(O)J + [—Da+rVarT(at)DLΩτ + D。+,S> + e [—ω> .
From the above two equations we can then identify Σe and D s+o(Da+ sVar(at-1)D(T S + I)DT0, and
because DS+oDa+S is identifiable, DS+oDT。is identifiable.
ɪ	F	F	,1 ∙1 .∙f~ 1∙1∙, C- τ∖	τ∖ TΛ τ∖ T-A k T-A	T-∖ τ∖ ~Γ	1
In summary, we have shown the identifiability of Da+r, Ds+oD(+s, Ds+oDkD(+s, Ds+ oDT+ o, and
Σe. Furthermore, Ds+ o, DS, and Da+ S are identified up to some orthogonal transformations. That
is, suppose the model in Eq. (3) with parameters (D s+o, D济 r, Da+r, Ds, Das, Σe, Σe) and that with
〜
(Ds+ o, Ds+ r, Da+r, Ds, D(+s, Σ巨,Σe) are observationally equivalent, we then have D居 o = DS+oU,
7-∖	7-∖ T T-Γ ^ΓΛ T T 7-∖	TΛ T T vS X~∖	F vS X~∖ F	T T ∙	,1	1
Da+r = Da+r, DS = U 1 D SU, D(+s = Da+ SU, Σ巨= Σe, and Σ∈ = Σe, where U is an orthogonal
matrix.
Next, we extend the above results to the case where do+dr > ds. Let I)∑o(i _)be the i-th row of Ds+o.
Recall that Ds+ o IS of full column rank. Then for any i, one can show that there always exist ⅛ — 1 rows
of Ds+o, such that they, together with D^ 〃, •)，form a ds × ds full-rank matrix, denoted by Ds+o(i,∙).
S 二
Then from the observed data corresponding to D 弟 o(i,∙), D弟 o(i,∙) is determined up to orthogonal
transformations. Thus, Ds+ o is identified up to orthogonal transformations. Similarly, Da+r, Ds,
and Da+S are identified up to orthogonal transformations. Furthermore, Cov(D 弟 o~t + Da+rat) is
1	∙	11	T-∖	τ∖	TΛ	FC	ɪʌ	r~∖	/	、	r-∖	/ T-∖	→	.	^t-∖	∖	X-∖	X~∖	∙
determined by Ds+ 0, D。+r, DS, and D。+s. Because Cov(yt) = Cov(D s+ oSt + D。+rat) + Σν，ΣV is
identifiable.
One may further add sparsity constraints on D弟 o, Ds+ r, Ds, and D(+s, to select more sparse structures
among the equivalent ones. For example, one may add sparsity constraints on the columns of Ds+o.
Note this corresponds to the mask on the elements of St in Eq. 2; if the full column is 0, then the
corresponding dimension of St is not selected.
□
E More Estimation Details for General Nonlinear Models
The generative model pθ can be further factorized as follows:
log Pθ (yi:T |f 1:T, ai:T-1； D s+o, D S+ r, Da+r )
=logPθ(oi：t| si：t； D弟o) + logPθ(ri：T| nT, ai：T-1； D s+ r, Da+r)	(IO)
T
= y?t-1 lθgpθ (ot1 st； DS+ O) + logpθ (rt1 st-1, at-1; DS+ r, Da+ r ),
15
Under review as a conference paper at ICLR 2022
where both pθ(ot∣st; D~÷o) and pθ⑺同— - ; D~”, Dα÷r) are modelled by mixture of Gaussians,
with Ds+o indicating the existence of edges from St to ot and Ds.丁 indicating the existence of edges
from ~St-1 to rt .
The inference model q@(s1：T|yi：T, ai：T-ι) is factorized as
logqφ(si:T|yi：T, ai：T-i)
T
= log qφ ( ~si |yi , a0) + P log qφ ( ~st |~st-i , yi：t, ai：t-i ),
t=2
where both qφ(~1∣y1,a0) and qφ(St∖~t-ι, yi：t, aLt-ι) are modelled with mixture of Gaussians.
The transition dynamics pγ is factorized as
〜
T
~ . ~
logPγ(~1:T∖ai:T-i; Ds(∙,i),Dα+s(∙,i)) = E logPY(~t∖~t-i, at-1； D~(∙,i),Dα+~(∙,i)),
t=i
with ~st∖~st-i modelled with mixture of Gaussians.
Thus, the KL divergence can be represented as follows:
KL(q0(~i：T∖yi:T, ai：T-i)kPγ(~i：T))
T
=KL(qφ(~i∖yι,a0)kPγ(~ι)) + P Eqφ [KL(qφ(~t∖~t-ι, yLt,aLt-ι)kPγ(~t∖~t-ι))]∙
t=2
(11)
(12)
In practice, KL divergence with mixture of Gaussians is hard to implement, so instead, we used the
following objective function:
T
KL(qφ(~i∖yι,ao)kPγ0(~i)) + P Eqφ [KL(qφ(~t∖~t-i, yrt,au)kPγo(~t∖~t-i))]
T	t=2	(13)
+λ ∑ log PY (StlSt-i, at-1; Ds(∙,i), Da+s(.,i))
t=i
where PY0 is a standard multivariate Gaussian N (~0, Id).
F More Details for Policy Learning with ASRs
Algorithm 1 gives the procedure of model-free policy learning with ASRs in partially observable
environments. Specifically, it starts from model initialization (line 1) and data collection with a random
policy (line 2). Then it updates the environment model and identifies the set of ASRs with the collected
data (line 3), after which, the main procedure of policy optimization follows. In particular, because
we do not directly observe the states ~st, on lines 8 and 12, we infer qφ(~stA+SiR∖o≤t+i, r≤t+i, a≤t)
and sample ~stA+SiR from the posterior. The sampled ASRs are then stored in the buffer (line 13).
Furthermore, we randomly sample a minibatch of N transitions to optimize the policy (lines 14 and
15). One may perform various RL algorithms on the ASRs, such as deep deterministic policy gradient
(DDPG (Lillicrap et al., 2015)) or Q-learning (Mnih et al., 2015).
Algorithm 2 presents the procedure of the classic Dyna algorithm with ASRs. Lines 17-22 make
use of the learned environment model to predict the next step, including ~stA+SiR and rt+i, and update
the Q function n times. Specifically, in our implementation, the hyper-parameter n is 20. Based on
the learned model, the agent learns behaviors from imagined outcomes in the compact latent space,
which helps to increase sample efficiency.
G	Additional Experiments and Details
G. 1 CarRacing Experiment
CarRacing (with an illustration in Figure 8) is a continuous control task with three continuous actions:
steering left/right, acceleration, and brake. Reward is -0.1 every frame and +1000/N for every track
tile visited, where N is the total number of tiles in track. It is obvious that the CarRacing environment
16
Under review as a conference paper at ICLR 2022
Algorithm 1 Model-Free Policy Learning with ASRs in Partially Observable Environments
1:	Randomly initialize neural networks and initialize replay buffer B .
2:	Apply random control signals and record multiple rollouts.
3:	Estimate the model given in (2) with the recorded data (according to Section 3).
4:	Identify indices of ASRs according to the learned graph structure and the criteria in Prop. 1.
5:	for episode = 1, . . . , M do
6:	Initialize a random process N for action exploration.
7:	Receive initial observations o1 and r1.
8:	Infer the posterior qφ(~ASR∣o1 ,r1) and sample ~ASR.
9:	for t = 1, . . . , T do
10:	Select action at = π (~stASR) + Nt according to the current policy and exploration noise.
11:	Execute action at and receive reward rt+1 and observation ot+1.
12:	Infer the posterior qφ(st+SR∣o≤t+1 ,r≤t+1, a≤t) and sample St+SR.
13:	Store transition (~stASR, at, rt+1, ~stA+S1R) in B.
14:	Sample a random minibatch of N transitions (~siASR, ai, ri+1, ~siA+S1R) from B.
15:	Update network parameters using a specified RL algorithm (e.g., DQN or DDPG).
16:	end for
17:	end for
Algorithm 2 Model-Based Policy Learning with ASRs in Partially Observable Environments
1:	Randomly initialize neural networks and initialize replay buffer B .
2:	Apply random control signals and record multiple rollouts.
3:	Estimate the model given in (2) with the recorded data (according to Section 3).
4:	Identify indices of ASRs according to the learned graph structure and the criteria in Prop. 1.
5:	for episode = 1, . . . , M do
6:	Initialize a random process N for action exploration.
7:	Receive initial observations o1 and r1.
8:	Infer the posterior qφ(~asr|o1 ,r1) and sample ~asr.
9:	for t = 1, . . . , T do
10:	Select action at = π (~stASR) + Nt according to the current policy and exploration noise.
11:	Execute action at and receive reward rt+1 and observation ot+1.
12:	Infer the posterior qφ(StASR∣o≤t+1, r≤t+1, a≤t) and sample StASR.
13:	Store transition (~stASR, ~st,at,rt+1, ~stA+S1R, ~st+1,ot+1) in B.
14:	Sample a random minibatch of N transitions (~SiASR, ai, ri+1, ~SiA+S1R) from B.
15:	Update network parameters using a specified RL algorithm (e.g., DQN or DDPG).
16:	Update the model given in (2) with the recorded data from B (according to Section 3).
17:	for p = 1, . . . , n do
18:	Sample a random minibatch of pairs of (~St , at ) from B.
19:	Predict (~StA+S1R, rt+1) according to the model given in (2).
20:	Update network parameters using a specified RL algorithm (e.g., DQN or DDPG).
21:	end for
22:	end for
23:	end for
FH
Figure 8: An illustration of Car Racing environment.
17
Under review as a conference paper at ICLR 2022
is partially observable: by just looking at the current frame, although we can tell the position of the
car, we know neither its direction nor velocity that are essential for controlling the car.
For a fair comparison, we followed the same setting as in Ha & Schmidhuber (2018). Specifically,
we collected a dataset of 10k random rollouts of the environment, each consisting of 1000 time steps,
for model estimation. The dimensionality of latent states ~st was set to d = 32, and regularization
parameters was set to λ1 = 1, λ2 = 1, λ3 = 1, λ4 = 1, λ5 = 1, λ6 = 6, λ7 = 10, λ8 = 0.1, which are
determined by hyperparameter turning.
Analysis of ASRs. To demonstrate the structures over observed frames, latent states, actions, and
rewards, We visualized the learned D~÷0, D~÷r, D~, and Da.~, as shown in Figure 9. Intuitively, We
can see that。弟/and D0.~ have many values close to zero, meaning that the reward is only influenced
by a small number of state dimensions, and not many state dimensions are influenced by the action.
Furthermore, from D~, we found that there are influences from ~i,t to <¾,t+ι (diagonal values) for
most state dimensions, which is reasonable because we want to learn an MDP over the underlying
states, while the connections across states (off-diagonal values) are much sparser. Compared to the
original 32-dim latent states, ASRs have only 21 dimensions. Below, we empirically showed that
the low-dimensional ASRs significantly improve the policy learning performance in terms of both
efficiency and efficacy.
-0.4
-0.2
0.0
Figure 9: Visualization of estimated structural matrices D~.o, D~.r, D&+~, and D~ in Car Racing.
Ablation Study. We further performed ablation studies on latent dynamics prediction; that is, we
compared with the case when the transition dynamics in (4) are not explicitly involved. Figure 10
shows that by explicitly modelling the transition dynamics (denoted by with LDP), the cumulative
reward has an obvious improvement over the one without modelling the transition dynamics (denoted
by without LDP).
Difference between our SS-VAE and Planet, Dreamer. Both our method and Planet (Hafner
et al., 2018) and Dreamer (Hafner et al., 2019) are world model-based methods. The differences are
mainly in two aspects: (1) our method explicitly considers the structural relationships among variables
in the RL system, and (2) it guarantees minimal sufficient state representations for policy learning.
Previous approaches usually fail to take into account whether the extracted state representations are
sufficient and necessary for downstream policy learning. Moreover, as for the component of recurrent
networks, SS-VAE uses LSTM that only contains the stochastic part, while PlaNet and Dreamer use
RSSM that contains both deterministic and stochastic components.
G.2 VizDoom Experiment
We also applied the proposed method to VizDoom (Kempka et al., 2016). VizDoom provides many
scenarios and we chose the take cover scenario (Figure 11). Unlike CarRacing, take cover is a discrete
control problem with two actions: move left and move right. Reward is +1 at each time step while
alive, and the cumulative reward is defined to be the number of time steps the agent manages to stay
alive during a episode. Therefore, in order to survive as long as possible, the agent has to learn how
to avoid fireballs shot by monsters from the other side of the room. In this task, solving is defined as
attaining the average survival time of greater than 750 time steps over 100 consecutive episodes, each
running for a maximum of 2100 time steps.
Following the same setting as in Ha & Schmidhuber (2018), we collected a dataset of 10k random
rollouts of the environment, each consisting of 500 time steps. The dimensionality of latent state ~st
18
Under review as a conference paper at ICLR 2022
PJeMωα φ>-^m-⊃E⊃u
1200
1000
800
600
400
200
00	500	1000 1500 2000
Episodes
Figure 10: Ablation study of latent dynamics
prediction (LDP) evaluated on Car Racing
with model-free ASR.
Figure 11: An illustration of VizDoom
take cover scenario.
•	, , V erʌ -rɪ T 1	, ʌ	ι A	ι A	ι A	ι A	ι A	r. ∖	-1 zʌ ʌ	CrC
is set to d = 32. We also set λ1 = 1, λ2 = 1, λ3 = 1, λ4 = 1, λ5 = 1, λ6 = 6, λ7 = 10, λ8 = 0.1. By
tuning thresholds, we finally reported all the results on the 21-dim ASRs, which achieved the best
results in all the experiments.
H	Detailed Model Architectures
In the car racing experiment, the original screen images were resized to 64 × 64 × 3 pixels. The
encoder consists of three components: a preprocessor, an LSTM, and an MDN. The preprocessor
architecture is presented in Figure 12, which takes as input the images, actions and rewards, and its
output acts as the input to LSTM. We used 256 hidden units in the LSTM and used a five-component
Gaussian mixture in the MDN. The decoder also consists of three components: a current observation
reconstructor (Figure 13), a next observation predictor (Figure 14), and a reward predictor (Figure
15). The architecture of the transition/dynamics is shown in Figure 16, and its output is also modelled
by an MDN with a five-component Gaussian mixture. The architecture of the action prediction is
given in Figure 17, which is a two-layer MLP taking states and rewards as input and predicted action
as output. In the VizDoom experiment, we used the same image size and the same architectures
except that the LSTM has 512 hidden units and the action has one dimension. It is worth emphasising
that we applied weight normalization to all the parameters of the architectures above except for the
structural matrices D(.).
In DDPG, both actor network and critic network are modelled by two fully connected layers of size
300 with ReLU and batch normalisation. Similarly, in DQN (Mnih et al., 2013) on both ASRs and
SSSs, the Q network is also modelled by two fully connected layers of size 300 with ReLU and batch
normalisation. However, in DQN on observations, it is modelled by three convolutional layers (i.e.,
relu conv 32 × 8 × 8 -→ relu conv 64 × 4 × 4 -→ relu conv 64 × 3 × 3) followed by two additional
fully connected layers of size 64. In DRQN (Hausknecht & Stone, 2015) on observations, we used
the same architecture as in DQN on observations but padded an extra LSTM layer with 256 hidden
units as the final layer.
I Platform and License
We run all the experiments on the servers with 4 NVidia V100 GPUs. We used the Car Racing in
OpenAI gym and VizDoom environments and we have cited the creators. In our code, we have used
the following libraries: Tensorflow (Apache License 2.0), OpenAI Gym (MIT License), VizDoom
(MIT License), OpenCV (Apache 2 License), Numpy (BSD 3-Clause "New" or "Revised" License)
and NVIDIA-DALI libraries (Apache 2 License).
19
Under review as a conference paper at ICLR 2022
Figure 12: Network architecture of preprocessor.
Figure 13: Network architecture of observation
reconstruction.
Figure 14: Network architecture of observation
prediction.
Figure 15: Network architecture of reward.
Figure 16: Network architecture of transi-
tion/dynamics.
20
Under review as a conference paper at ICLR 2022

1.00
0.75
^s→o
0.25
0	5 10 15 20 25 30
Dimension Index
1.00

Figure 17: Network architecture of action prediction.
0.00
0.75
0.50
0.25
Ds→r
0	5 10 15 20 25 30
Dimension Index
0 5 10 15 20 25 30
Sf+ι
1.0
0.8
0.6
0.4
0.2
0.0
Figure 18:	Visualization of estimated structural matrices Ds⅛o, D~+r, Da+~, and Ds in Car Racing,
without the explicit sparsity constraints.
3-e>
0.00
1.00
0.75
^s→o
0.25
Dimension Index
0	5 10 15 20 25 30
θa→s
Dimension Index
0
5
10
15
20
25
30
D§
0 5 10 15 20 25 30
I：
-0.4
-0.2
-0.0

Figure 19:	Visualization of estimated structural matrices D~>o, D~>r, Da+~, and Ds in VizDoom.
21