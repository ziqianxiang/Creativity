Under review as a conference paper at ICLR 2022
Scaling-up Diverse Orthogonal Convolu-
tional Networks by a Paraunitary Framework
Anonymous authors
Paper under double-blind review
Ab stract
Enforcing orthogonality in convolutional neural networks is an antidote for gra-
dient vanishing/exploding problems, sensitivity to perturbation, and bounding
generalization errors. Many previous approaches for orthogonal convolutions
enforce orthogonality on its flattened kernel, which, however, do not lead to the
orthogonality of the operation. Some recent approaches consider orthogonality
for standard convolutional layers and propose specific classes of their realizations.
In this work, we propose a theoretical framework that establishes the equivalence
between diverse orthogonal convolutional layers in the spatial domain and the
paraunitary systems in the spectral domain. Since 1D paraunitary systems admit
a complete factorization, we can parameterize any separable orthogonal convolu-
tion as a composition of spatial filters. As a result, our framework endows high
expressive power to various convolutional layers while maintaining their exact
orthogonality. Furthermore, our layers are memory and computationally efficient
for deep networks compared to previous designs. Our versatile framework, for the
first time, enables the study of architectural designs for deep orthogonal networks,
such as choices of skip connection, initialization, stride, and dilation. Consequently,
we scale up orthogonal networks to deep architectures, including ResNet and Shuf-
fleNet, substantially outperforming their shallower counterparts. Finally, we show
how to construct residual flows, a flow-based generative model that requires strict
Lipschitzness, using our orthogonal networks.
1 Introduction
Convolutional neural networks (CNNs), whose deployment has witnessed extensive empirical success,
still exhibit a range of limitations that are not thoroughly studied. Firstly, deep convolutional networks
are in general difficult to learn, and their high performance heavily relies on techniques that are not
fully understood, such as skip-connections (He et al., 2016a), batch normalization (Ioffe & Szegedy,
2015), specialized initialization (Glorot & Bengio, 2010). Secondly, they are notoriously sensitive
to imperceptible perturbations, including adversarial attacks (Goodfellow et al., 2014) or geometric
transformations (Azulay & Weiss, 2019). Finally, a precise characterization of their generalization
property is still under active investigation (Neyshabur et al., 2018; Jia et al., 2019).
Orthogonal networks, which have a “flat” spectrum with all singular values of the layer being 1 (each
layer’s output norm kyk equals its input norm kxk, ∀x), alleviate all problems above. As shown in
recent works, by enforcing orthogonality in neural nets, we obtain (1) easier optimization (Zhang
et al., 2018a; Qi et al., 2020): since each orthogonal layer preserves the gradient norm during
backpropagation, an orthogonal network is free from gradient vanishing/ exploding problems; (2)
robustness against adversarial perturbation (Anil et al., 2019; Li et al., 2019b; Trockman & Kolter,
2021): since each orthogonal layer is 1-Lipschitz, an orthogonal network can not amplify any
perturbation to the input to flip the output prediction; (3) better generalizability as proved in (Jia et al.,
2019): a network’s generalization error is positively related to the standard deviation of each linear
layer’s singular values, thus encouraging orthogonality in the network lowers its generalization error.
Despite the benefits, enforcing orthogonality in convolutional networks is challenging. To avoid the
strict constraint, orthogonal initialization (dynamical isometry) (Pennington et al., 2017; Xiao et al.,
2018) and orthogonal regularization (Wang et al., 2019; Qi et al., 2020) are adopted to address the
gradient vanishing/exploding problems. However, these methods are not suitable for applications that
1
Under review as a conference paper at ICLR 2022
require strict Lipschitzness, such as adversarial robustness (Anil et al., 2019) and residual flows (Chen
et al., 2019), as they do not enforce strict orthogonality (and Lipschitzness) during training.
Our goal is to enforce exact orthogonality in state-of-the-art convolutional networks without expensive
computations. We identify three main challenges in doing so. Challenge I: Achieving exact orthog-
onality throughout the training process. Prior works such as orthogonal regularization (Wang
et al., 2019) and reshaped kernel orthogonality (Jia et al., 2017; Cisse et al., 2017), while enjoying
algorithmic simplicity, do not meet the requirement of exact orthogonality. Note that enforcing
orthogonality during training is necessary as a post-training orthogonalization of the weights can
ruin the performance. Challenge II: Avoiding expensive computations. An efficient algorithm is
crucial for scalability to large networks. Existing work based on projected gradient descent (Sedghi
et al., 2019), however, requires expensive projection after each update. For instance, the projection
step in Sedghi et al. (2019) computes an SVD and flattens the spectrum to enforce orthogonality,
which costs O(Size(feature) ∙ channels3) for a convolutional layer. Challenge III: Scaling-up to
state-of-the-art deep convolutional networks. There are many variants to the standard convolu-
tional layer, including dilated, strided, group convolutions, which are essential for state-of-the-art
deep convolutional networks. However, none of the existing methods proposes mechanisms to
orthogonalize these variants. The lack of techniques, as a result, limits the broad applications of
orthogonal convolutional layers to state-of-the-art deeper convolutional networks.
We resolve challenges I, II, and III by proposing complete parameterizations for orthogonal 1D-
convolutions and separable orthogonal 2D-convolutions. First, using the convolution theorem (Op-
penheim et al., 1996) (convolution in the spatial domain is equivalent to multiplication in the spectral
domain), we reduce the problem of designing orthogonal convolutions to constructing unitary ma-
trices for all frequencies, i.e., a paraunitary system (Vaidyanathan, 1993). Therefore, we obtain a
parameterization for all separable orthogonal 2D-convolutions, guaranteeing exact orthogonality and
high expressive power. Note that there is no previous approach that achieves exact orthogonality (up
to machine precision). For the first time, our versatile framework enables the study in the designs of
deep orthogonal networks, such as skip connection, initialization, stride, and dilation. Consequently,
we scale orthogonal networks to deep architectures, including ResNet and ShuffleNet, substantially
outperforming their shallower counterparts. Our proposed method serves as gaining insight into the
trade-off between orthogonality and expressiveness. We observe that exact orthogonality is essential
in these deeper architectures with more than 10 layers (detailed discussion of exact orthogonality in
Appendix E.3). Finally, we show how to deploy our orthogonal networks in Residual Flow (Chen
et al., 2019), a flow-based generative model that requires strict Lipschitzness (Appendix F).
Summary of Contributions:
1.	We establish the equivalence between orthogonal convolutions in the spatial domain and paraunitary
systems in the spectral domain. Consequently, we can interpret existing approaches for orthogonal
convolutions as implicit designs of paraunitary systems.
2.	Based on a complete factorization of 1D paraunitary systems, we propose the first exact and
complete design for orthogonal 1D-convolutions as well as separable orthogonal 2D-convolutions,
ensuring exact orthogonality and high expressive power. None of the existing works achieve a
complete parameterization of all orthogonal 2D-convolutions.
3.	We prove that orthogonality for various convolutional layers (strided, dilated, group) are also
entirely characterized by paraunitary systems. Consequently, our design easily extends to these
variants, ensuring both completeness and exactness of the orthogonal convolutions.
4.	We study the design considerations for orthogonal networks (choices of skip connection, initializa-
tion, depth, width, and kernel size), and show that orthogonal networks can scale to deep architectures
including ResNet, WideResNet, and ShuffleNet.
2 Orthogonal Convolutions via Paraunitary Systems
Designing orthogonal convolutional layer {ht,s : yt = ht,s * Xs}TS s=1 (s,t index input/output
channels) in the spatial domain is challenging. When a convolution layer is written as a matrix-vector
product, where the matrix is block-circulant and its (t, s)th takes a circulant form Cir (ht,s) as:
-ht,s[1]
ht,s[2]
Cir (ht,s)=	.
ht,s[N]
ht,s[1]
ht,s[N]
ht,s[2]~
• ∙ ∙
.∈ RN×N
(2.1)
ht,s[N]	•••	ht,s[2]	ht,s[1]
2
Under review as a conference paper at ICLR 2022
Therefore, the layer is orthogonal if the block-circulant matrix [Cir (ht,s)]tT=,S1,s=1 is orthogonal.
However, it is not obvious how to enforce orthogonality in a block-circulant matrix.
2.1	Achieving Orthogonal Convolutions by Paraunitary Systems
We propose a novel design of orthogonal convolutions from a spectral domain perspective, motivated
by the convolution theorem (Theorem 2.1). For simplicity, we group the entries at the same locations
into a vector/matrix, e.g., we denote {xs[n]}sS=1 as x[n] ∈ RS and {ht,s[n]}tT=,S1,s=1 as h[n] ∈ RT×S.
Theorem 2.1 (Convolution theorem (Oppenheim et al., 1996)). For a standard convolution layer
h: y[i] = n h[n]x[i - n], the convolution in the spatial domain is equivalent to a matrix-
vector multiplication in the spectral domain, i.e., Y (z) = H(z)X(z), ∀z ∈ C. Here, X(z) =
PnN-01 x[n]z-n, Y(Z) = PnN-01 y[n]z-n, H(Z) = PL=-L h[n]z-n denote the Z-transforms of
input, output, kernel respectively, where N is the length of x, y and [-L, L] is the span ofthe filter h.
The convolution theorem states that a standard convolution layer is a matrix-vector multiplication in
the spectral domain. As long as the transfer matrix H(Z) is unitary at Z = ejω for all frequencies
∀ω ∈ R (j is the imaginary unit), the convolutional layer h is orthogonal.
Therefore, as a major novelty of our paper, we design orthogonal convolutions through designing
unitary transfer matrix H(ejω) at all frequencies ω ∈ R, which is known as a paraunitary sys-
tems (Vaidyanathan, 1993; Strang & Nguyen, 1996). We prove in Theorem B.5 that a convolutional
layer is orthogonal in the spatial domain if and only if it is paraunitary in the spectral domain.
Benefits through paraunitary systems. (1) The spectral representation simplifies the designs of
orthogonal convolutions and avoids the analysis of block-circulant structure. (2) Since paraunitary
systems are necessary and sufficient for orthogonal convolutions, it is impossible to find an orthogonal
convolution whose transfer matrix is not paraunitary. (3) There exists a complete factorization of
paraunitary systems: any paraunitary H(Z) can be realized through multiplications in the spectral
domain, as will show in Equation (2.2a). (4) Since multiplications in spectral domain correspond to
convolutions in spatial domain, any orthogonal convolution can be realized as cascaded convolutions
of multiple sub-layers, each parameterized by an orthogonal matrix. (5) There are mature methods that
represent orthogonal matrices via unconstrained parameters. Consequently, we can learn orthogonal
convolutions using standard optimizers on a model parameterized via our design.
Interpretation of existing methods. Since paraunitary system is a necessary and sufficient condition
for orthogonal convolution, all existing approaches, including singular value clipping and masking
(SVCM) (Sedghi et al., 2019), block convolution orthogonal parameterization (BCOP) (Li et al.,
2019b), Cayley Convolution (CayleyConv) (Trockman & Kolter, 2021), skew orthogonal convolution
(SOC) design paraunitary systems implicitly. We discuss these interpretations in Appendix C.3.3, and
show that our SC-Fac has the lowest computational complexity among all approaches.
2.2	Realizing Paraunitary Systems via Re-parameterization
After reducing the problem of orthogonal convolutions to paraunitary systems, we are left with the
question of how to realize paraunitary systems. We use a complete factorization form of paraunitary
systems to realize any paraunitary systems. According to Theorem C.7, any paraunitary system H(Z)
can be written as the form in Equation (2.2a) and any H(Z) in this form is a paraunitary system:
H(Z) = V(z; U(-L))…V(z; U(T))QV(z-1; U⑴)…V(z-1; U(L)),	(2.2a)
where V(z; U(')) = (I - U(')U(')>) + U(')U(')>z, ∀' ∈ {-L,…，-1} ∪ {1,…，L}.	(2.2b)
Here Q is an orthogonal matrix and each U (`) is a column-orthogonal matrix whose number of
columns is sampled uniformly from {1,…，T}. As multiplications in the spectral domain are
equivalent to convolutions in the spatial domain, this complete spectral factorization of paraunitary
systems in Equation (2.2a) allows us to parameterize any orthogonal convolution in the spatial domain
as cascaded convolutions of V(z; U('))'s spatial counterparts and the orthogonal matrix Q.
Model design in the spatial domain. We obtain a complete design of orthogonal 1D-convolutions:
using learnable (column)-orthogonalmatrices ({U(')}-=1-L, Q, {U(')}L=1), we parameterize a size
(L + L + 1) convolution as cascaded convolutions of the following filters in the spatial domain
{[i - U⑶U(')>, U⑶U(')]}—、，Q，{[u⑶U(')>，I — U⑶U(')>i}L .	(2.3)
3
Under review as a conference paper at ICLR 2022
Figure 1 provides a visualization of our proposed design of orthogonal convolution layers; each block
denotes a convolution and the form of the filter is displayed in each of the block. In practice, We
compose all (L + L + 1) filters into one for orthogonal convolution, which not only increases the
computational parallelism but also avoids storing intermediate outputs between filters.
x
[I - U1 7)(U1 7 ))τ ,U1 工)(U(工))τ]	—►…—	[I - U ⑴(U ll))τ ,U⑴(U(1))τ]	—►	Q	—►	[U(T)(U(T))T, I -U(T)(U(T))T]	-►	►	[UH)(U L L T, I - UH)(U ɪ-L))τ]		y 	A
Figure 1: Complete design of orthogonal convolutional layer as a cascade of convolutions, whose filter
coefficients are depicted in each block. The matrix Q is orthogonal and U(')'s are column-orthogonal.
Using this 1D-convolution, we obtain a complete design for separable orthogonal 2D-convolutions,
which can be represented as a convolution of two orthogonal 1D-convolutional layers. With separabil-
ity, we obtain a design of orthogonal 2D-convolutions by composing two complete designs of orthogo-
nal 1D-convolutions. As a result, we can parameterize separable orthogonal 2D-convolution with filter
size (Li + Li + 1) X (L2 + L + 1) as a convolution of two orthogonal 1D-convolutions with learnable
(column-)orthogonal matrices ({u1(''}-'=-Lι, Qi, {U(')}L= i) and ({。2力-^一旦?，Q2, {u2')}L2ι)∙
With a complete factorization of paraunitary systems (1D and separable 2D), we reduce the problem
of designing orthogonal convolutions to the one for orthogonal matrices.
Parameterization for orthogonal matrices. In Appendix C.4, we perform a comparative study on
different parameterizations of orthogonal matrices, including the Bjorck orthogonalization (Anil
et al., 2019; Li et al., 2019b), the Cayley transform (Helfrich et al., 2018; Maduranga et al., 2019),
and the exponential map (Lezcano-Casado & Martinez-Rubio, 2019). In our implementation, we
adopt a modified exponential map due to its efficiency, exactness, and completeness. The exponential
map is a surjective mapping from a skew-symmetry matrix A to a special orthogonal matrix U (i.e.,
det(U) = 1) with U = exp(A) = I + A + A2/2 T---------, where the infinite series is computed up
to machine-precision (Higham, 2009). To parameterize all orthogonal matrices, we introduce an
additional orthogonal matrix Q in U = Q exp(A), where Q is (randomly) generated at initialization
and fixed during training (Lezcano Casado, 2019) .
Now we have an end-to-end pipeline for orthogonal convolutions as shown in Figure 2 (See Algorithm
1 in Appendix E for a pseudo implementation). Since our method relies on separability and complete
factorization for 1D paraunitary systems, we call it Separable Complete Factorization (SC-Fac).
Figure 2: SC-Fac: A pipeline for designing orthogonal convolutional layer.⑴ An orthogonal
convolution h[n] is equivalent a paraunitary system H(z) in the spectral domain (Theorem 2.1). (2)
The paraunitary system H(Z) is multiplications of factors characterized by (column-)orthogonal ma-
trices ({U(')}-=-L, Q, {U(')}L=ι) (Equation (2.2a), Theorem B.5). (3) These orthogonal matrices
are parameterized by skew-symmetric matrices using exponential map.
3	Unifying Orthogonal Convolutions Variants
Various convolutional layers (strided, dilated, and group convolution) are widely used in neural
networks. However, it is not apparent how to enforce their orthogonality, as the convolution theorem
(Theorem 2.1) only holds for standard convolutions. Previous approaches only deal with standard
convolutions (Sedghi et al., 2019; Li et al., 2019b; Trockman & Kolter, 2021), thus orthogonality for
state-of-the-art architectures has never been studied before.
We address this limitation by modifying convolution theorem for each variant of convolution layer,
which allows us to design these variants using paraunitary systems.
Theorem 3.1 (Convolution and paraunitary theorems for various convolutions). Strided, dilated,
and group convolutions can be unified in the spectral domain as Y(z) = H(Z)X(z), where Y(z),
H(z), X(z) are modified Z-transforms of y, h, X. We instantiate H(z) for strided convolutions in
Proposition C.4, dilated convolution in Proposition C.5, and group convolution in Proposition C.6.
Furthermore, a convolution is orthogonal if and only if H (z) is paraunitary.
4
Under review as a conference paper at ICLR 2022
In Table 1, we formulate strided, di-
lated, and group convolutions in the
spatial domain, interpreting them as
up-sampled or down-sampled variants
of a standard convolution. Now, we
introduce the concept of up-sampling
and down-sampling precisely below.
Given a sequence x, we introduce its
up-sampled sequence x↑R with sam-
pling rate R as x↑R [n] , x[n/R] for
n ≡ 0 (mod R). On the other hand,
its (r,R)-polyphase component xr|R
indicates the r-th down-sampled se-
quence with sampling rate R, defined
as xr|R[n] , x[nR + r]. We illustrated an example of x↑R and xr|R in Figure 3 when sampling
rate R = 2. The Z-transforms of x↑R, xr|R are denoted as X↑R(z), Xr|R(z) respectively. Their
relations to X(z) are studied in Appendix C.1.
Table 1: Variants of convolutions. We present the modified Z-transforms, Y(z), H(z), and X(Z) for each
convolution such that Y(Z) = H(Z)X(Z) holds. In the table, X[R](z)，[X0lR(z)>,..., XR-1lR(z)>]>
and X[R] (Z) = [X-0lR(z),..., X-(RT)IR(z)]. For group convolution, hg is the filter for the gth group with
Hg (Z) being its Z-transform, and Mkdiag (∙) stacks multiple matrices into a block-diagonal matrix.
Figure 3: Up and down sampling. In (a), the sequence x[n] is
up-sampled into x↑2[n]. In (b), x[n] is down-sampled into x0|2 [n]
with even entries (red) and x1|2[n] with odd entries(blue).
Convolution Type	Spatial Representation	Y(Z)	Spectral Representation H (Z)	X (Z)
Standard	y[i] = Pn∈z h[n∖x[i - n]	Y (Z)	H(Z)	X(Z)
R-Dilated	y[i] = Pn∈z h↑R[n∖x[i - n]	Y (Z)	H(ZR)	X(Z)
] R-Strided	y[i] = Pn∈z h[n∖χ[Ri - n∖	Y (Z)	Hf[R](Z)	X[R](Z)
↑ R-Strided	y[i∖ = Pn∈Z h[n∖x↑R[i - n∖	Y[R] (z)	H[R] (Z)	X(Z)
G-Group	y[i∖ = PneZ blkdiag ({hg[n∖})χ[i - n∖	Y (Z)	blkdiag({Hg(Z)})	X(Z)
Now we are ready to interpret convolution variants. (1) Strided convolution is used to adjust the
feature resolution: a strided convolution (] R-strided) decreases the resolution by down-sampling
after a standard convolution, while a transposed strided convolutional layer (↑ R-strided) increases
the resolution by up-sampling before a standard convolution. (2) Dilated convolution increases the
receptive field of a convolution without extra parameters: an R-dilated convolution up-samples its
filters before convolution with the input. (3) Group convolution reduces the parameters and compu-
tations, thus widely used by efficient architectures: a G-group convolution divides the input/output
channels into G groups and restricts the connections within each group. In Appendix C.2, we prove
that a convolution is orthogonal if and only if its modified Z-transform H(Z) is paraunitary.
4	Learning Deep Orthogonal Networks with Lipschitz Bounds
In this section, we switch our focus from layer design to network design. In particular, we aim to
study how to scale-up deep orthogonal networks with Lipschitz bounds.
Lipschitz networks (Anil et al., 2019; Li et al., 2019b; Trockman & Kolter, 2021), whose Lipschitz
bounds are imposed by their architectures, are proposed as competitive candidates to guarantee
robustness in deep learning. A Lipschitz network consists of orthogonal layers and GroupSort
activations (Anil et al., 2019) — both are 1-Lipschitz and gradient norm preserving. See Appendix D
for more discussions on the properties of GroupSort and Lipschitz networks. Given a Lipschitz
constant L, a Lipschitz network f can compute a certified radius for each input from its output margin.
Formally, denote the output margin of an input x with label c as
Mf (x) , max(0, f (x)c - maxf(x)i),	(4.1)
i6=c
i.e., the difference between the correct logit and the second largest logit. Then the output is robust to
perturbation such that f(x + e) = f(x) = c, ∀e : |©| < Mf(X)I√2L.
5
Under review as a conference paper at ICLR 2022
Despite the benefit, existing architectures for Lipschitz networks remain simple and shallow, and a
Lipschitz network is typically an interleaving cascade of orthogonal layers and GroupSort activa-
tions (Li et al., 2019b). More advanced architectures, such as ResNet and ShuffleNet, are still out of
reach. While orthogonal layers supposedly substitute the role of batch normalization (Pennington
et al., 2017; Xiao et al., 2018; Qi et al., 2020), other critical factors, including skip-connections (He
et al., 2016a;b) and proper initialization (Glorot & Bengio, 2010) are lacking. In this section, we
explore skip-connections and initialization methods toward addressing this problem.
Skip-connections. Two general types of skip-connections are widely used in deep networks, one
based on addition and another on concatenation. The addition-based connection is proposed in
ResNet (He et al., 2016a), and adopted in SE-Net (Hu et al., 2018) and EffcientNet (Tan & Le,
2019). The concatenation-based connection is proposed in flow-based generative models (Dinh
et al., 2014; 2016; Kingma & Dhariwal, 2018), and adopted in DenseNet (Huang et al., 2017)
and ShuffleNet (Zhang et al., 2018b; Ma et al., 2018). In what follows, we propose Lipschitz
skip-connections with these two mechanisms, illustrated in Figure 6 (in Appendix D).
Proposition 4.1 (Lipschitzness of residual blocks). Suppose f1, f2 are L-Lipschitz (for resid-
ual/shortcut branches) and α ∈ [0, 1] is a learnable scalar, then an additive residual block
f : f(x) , αf1(x) + (1 - α)f2 (x) is L-Lipschitz. Alternatively, suppose g1, g2 are L-Lipschitz and
P is a channel permutation, then a concatenative residual block g : g(x) , P g1(x1); g2(x2) is L-
Lipschitz, where [∙; ∙] denotes channel concatenation, and X is split into xι and x2,i.e., X =[xι, x2].
Initialization. Proper initialization is crucial in training deep networks (Glorot & Bengio, 2010;
He et al., 2016a). Various methods are proposed to initialize orthogonal matrices, including the
identical/permutation and torus initialization in the context of orthogonal RNNs (Henaff et al.,
2016; Helfrich et al., 2018; Lezcano-Casado & Mardnez-Rubio, 2019). However, initialization of
orthogonal convolutions was not systematically studied, and all previous approaches inherit the
initialization from the underlying parameterization (Li et al., 2019b; Trockman & Kolter, 2021). In
Proposition D.1, we study the condition when a paraunitary system (represented in Equation (2.2a))
reduces to an orthogonal matrix. This reduction allows us to apply the initialization methods for
orthogonal matrices (e.g., uniform, torus) to orthogonal convolutions.
In the experiments, we will evaluate the impact of different choices of skip-connections and initializa-
tion methods to the performance of deep Lipschitz networks.
5 Related Work
Dynamical isometry (Pennington et al., 2017; 2018; Chen et al., 2018; Pennington et al., 2018)
aims to address the gradient vanishing/exploding problems in deep vanilla networks with orthogonal
initialization. These works focus on understanding the interplay between initialization and various
nonlinear activations. However, these approaches do not guarantee Lipschitzness after training, thus
are unsuitable for applications that require strict characterization of Lipschitz constants, such as
adversarial robustness (Anil et al., 2019) and residual flows (Behrmann et al., 2019).
Learning orthogonality has three typical approaches: regularization, parameterization (representing
the feasible set with unconstrained parameters), and projected gradient descent (PGD) / Riemannian
gradient descent (RGD). While regularization is approximate, the latter two learn exact orthog-
onality. (1) For orthogonal matrices, various regularizations are proposed in Xie et al. (2017);
Bansal et al. (2018). Alternatively, numerous parameterizations exist, including Householder reflec-
tions (Mhammedi et al., 2017), Given rotations (Dorobantu et al., 2016), Cayley transform (Helfrich
et al., 2018), matrix exponential (Lezcano-Casado & Mardnez-Rubio, 2019), and algorithmic Un-
rolling (Anil et al., 2019; Huang et al., 2020). Lastly, Jia et al. (2017) propose PGD via singular
value clipping, and Vorontsov et al. (2017); Li et al. (2019a) consider RGD. (2) For orthogonal
convolutions, some existing works learn orthogonality for the flattened matrix (Jia et al., 2017; Cisse
et al., 2017; Bansal et al., 2018) or each output channel (Liu et al., 2021). However, these methods
do not lead to orthogonality (norm preserving) of the operation. Sedghi et al. (2019) propose to
use PGD via singular value clipping and masking, which is expensive and can result in inexact
orthogonality. Recent works adopt parameterizations, using block convolutions (Li et al., 2019b),
Cayley transform (Trockman & Kolter, 2021), or convolution exponential (Singla & Feizi, 2021).
Note that network deconvolution (Ye et al., 2020) aims to whiten the activations (i.e., orthogonalize
their distribution), but the added whitening operations do not necessarily preserve orthogonality.
6
Under review as a conference paper at ICLR 2022
Paraunitary systems are extensively studied in filter banks and wavelets (Vaidyanathan, 1993; Strang
& Nguyen, 1996; Lin & Vaidyanathan, 1996). Classic theory shows that 1D-paraunitary systems
are fully characterized by a spectral factorization (see Chapter 14 of Vaidyanathan (1993)), but not
all MD-paraunitary systems admit a factorized form (see Chapter 8 of Lin & Vaidyanathan (1996)).
While the complete characterization of MD-paraunitary systems is known in theory (which incurs a
difficult problem of solving a system of nonlinear equations) (Venkataraman & Levy, 1995; Zhou,
2005), most practical constructions use separable paraunitary systems (Lin & Vaidyanathan, 1996)
and special classes of non-separable paraunitary systems (Hurley & Hurley, 2012). The equivalence
between orthogonal convolutions and paraunitary systems thus opens the opportunities to apply these
classic theories in designing orthogonal convolutions.
6 Experiments
In the experiments, we achieve the following goals. (1) We demonstrate in Section 6.1 that our
separable complete factorization (SC-Fac) achieves precise orthogonality (up to machine-precision),
resulting in more accurate orthogonal designs than previous ones (Sedghi et al., 2019; Li et al., 2019b;
Trockman & Kolter, 2021). (2) Despite the differences in preciseness, we show in Section 6.2 that
different realizations of paraunitary systems only have a minor impact on the adversarial robustness
of Lipschitz networks. (3) Due to the versatility of our convolutional layers and architectures, in
Section 6.3, we explore the best strategy to scale Lipschitz networks to wider/deeper architectures.
(4) In Appendix F, we further demonstrate in a successful application of orthogonal convolutions in
residual flows (Chen et al., 2019). Training details are provided in Appendix E.1.
6.1	Exact Orthogonality
We evaluate the orthogonality of our SC-Fac layer verse previous approaches, including Cayley-
Conv (Trockman & Kolter, 2021), BCOP (Li et al., 2019b), SVCM (Sedghi et al., 2019), RKO (Cisse
et al., 2017), OSSN (Miyato et al., 2018). Our experiments are based on a convolutional layer with 64
input channels and 16 × 16 input size. We orthogonalize the layer using each approach, and evaluate
it with Gaussian inputs. For our SC-Fac layer, We initialize all orthogonal matrices uniformly, while
we use built-in initialization for others. We evaluate the difference between 1 and the ratio of the
output norm to the input norm — a layer is exactly orthogonal if the number is close to 0.
Table 2: (Left) Orthogonality evaluation of different designs for standard convolution. The number
kConv(x)k/kxk - 1 indicates the difference between the output and input norms of a layer. A layer is more
precisely orthogonal if the number is closer to 0. As shown, our SC-Fac achieves orders of magnitude more
orthogonal on standard convolution. (Right) Orthogonality evaluation of our SC-Fac design for various
convolutions. The numbers kConv(x)k/kxk - 1 displayed are in the magnitude of 10-8. As shown, our
SC-Fac achieves machine epsilon orthogonality on variants of convolution.
Conv.	kConv(X)k/ixk-1
SC-Fac	(+3.14 ± 7.38) X 10-8
CayleyConv	(+2.88 ± 1.90) X 10-4
BCOP	(+2.59 ± 6.14)X 10-3
SVCM	-0.429 ± 3.31 X 10-3
RKO	-0.666 ± 1.74 X 10-3
OSSN	-0.422 ± 3.44 X 10-3
Groups TyPe^^^^	1	4	16
1	+3.14 ± 7.38	+1.94 ± 6.87	+1.44 ± 6.29
R-Dilated	2	+3.65 ± 7.87	+1.41 ± 6.77	+1.02 ± 6.46
4	+3.18 ± 7.46	+1.79 ± 6.87	+1.54 ± 6.21
2 D QfrirlArl	-4.69 ± 5.10	+4.38 ± 6.30	+1.79 ± 5.78
ψ R-Slrided 4	+ 10.39±5.15	+6.35 ± 6.04	+3.05 ± 5.79
2 个 R-SfrirIarI	+3.67 ± 7.96	+1.38 ± 6.70	+1.43 ± 6.23
↑ R-Strided 4	+3.86 ± 7.09	+1.12 ± 6.81	N/A
(1)	Standard convolution. We show in Table 2 (Left) that our SC-Fac is orders of magnitude more
precise than all other approaches. The SC-Fac layer is in fact exactly orthogonal up to machine
epsilon, which is 2-24 ≈ 5.96 × 10-8 for 32-bits floats. While RKO and OSSN are known not
to be orthogonal, we surprisingly find that SVCM is far from orthogonal due to its masking step.
(2) Convolutions variants. In Section 3, we show that various orthogonal convolutions can be
constructed using paraunitary systems. We verify our theory in Table 2 (Right): our SC-Fac layers
are exactly orthogonal (up to machine precision) for various convolutions.
6.2	Adversarial Robustness
In this subsection, we evaluate the adversarial robustness of Lipschitz networks. Following the setup
in (Trockman & Kolter, 2021), we adopt KW-Large, ResNet9, WideResNet10-10 as the backbone
architectures, and evaluate their robust accuracy on CIFAR-10 with different designs of orthogonal
7
Under review as a conference paper at ICLR 2022
convolutions. We extensively perform a hyper-parameter search and choose the best hyper-parameters
for each approach based on the robust accuracy. The details of the hyper-parameter search is in
Appendix E. We run each model with 5 different seeds and report the best accuracy.
(1)	Certified robustness. Following Li et al. (2019b), we use the raw images (without normalization)
for network input to achieve the best certified accuracy. As shown in Table 3 (Top), different
realizations of paraunitary systems, SC-Fac, CayleyConv and BCOP achieve comparable performance
— CayleyConv performs < 1% better in clean accuracy, but the difference in robust accuracy are
negligible. (2) Practical robustness. Trockman & Kolter (2021) shows that the certified accuracy is
too conservative, and it is possible to increase the practical robustness (against PGD attacks) with a
standard input normalization. Notice that the normalization increases the Lipschitz bound, thus lower
the certified accuracy. Our experiments in Table 3 (Bottom) are based on ResNet9, WideResNet10-
10 (Trockman & Kolter, 2021) and a deeper WideResNet22. For the shallow architectures (ResNet9,
WideResNet10-10), our SC-Fac, CayleyConv, and BCOP again achieve comparable performance —
CayleyConv is slightly ahead in robust accuracy. For the deeper architecture, our SC-Fac has a
clear advantage in both clean and robustness accuracy, and the clean accuracy to only 5% lower
than a traditional ResNet 32 trained with batch normalization. Surprisingly, we find that RKO also
performs well in robust accuracy while not exactly orthogonal. In summary, our experiments show
that various paraunitary realizations provide different impacts on certified and practical robustness.
While exact orthogonality provides tight Lipschitz bound, there is a trade-off between the exact
orthogonality and the practical robustness (especially with the shallow architectures).
Table 3: (Top) Certified robustness for plain convolutional networks (without input normalization). We
use KW-Large introduced by Wong et al. (2018). The results for RKO, OSSN, and SVCM are produced by
Trockman & Kolter (2021). (Bottom) Practical robustness for residual networks (with input normalization).
For 22 layers, the width of SC-Fac is multiplied with 10, CayleyConv with 6, and BCOP and RKO with 8. We
are unable to scale CayleyConv, BCOP, and RKO due to memory constraint. As shown, deeper architectures
perform better than shallow ones for all orthogonal convolution types, and our SC-Fac has a clear advantage.
	Test Acc.	KW-Large					
		SC-Fac	CayleyConv BCOPlRKO			OSSN SVCM	
0	Clean	74.69	75.57	74.81	74.47	71.69	72.43
36	Certified	58.68	59.03	58.83	57.50	55.71	52.11
255	PGD	67.72	67.78	67.47	68.32	65.13	66.43
I	I	ResNet9	∣							WideReSNet10-10	∣				WideResNet22-max		
	I Test Acc. I	SC-Fac CayleyConv BCOP RKO ∣ SC-Fac					CayleyConv BCOP		RKO I SC-Fac		CayleyConv	BCOP	RKO
0	I Clean ∣	82.19	84.26	83.20	84.07 I	84.09	82.99	84.29	84.511	87.82	85.85	84.50	84.55
36 255	I PGD I	71.21	73.47	73.05	75.03 I	74.29	76.02	74.60	77.14 I	76.46	74.81	75.00	76.41
6.3	Scaling-up Deep Orthogonal Networks with Lipschitz Bounds
All previously proposed Lipschitz networks (Li et al., 2019b; Trockman & Kolter, 2021) consider
only shallow architectures (≤ 10 layers). In this part, we investigate various factors to scale Lipschitz
networks to deeper architectures: skip-connection, depth-width, receptive field, and down-sampling.
(1)	Skip-connections. Conventional wisdom suggests that skip-connections mainly address the
gradient vanishing/exploding problem; thus, they are not needed for orthogonal networks. To
understand their role, we perform an experiment that trains deep Lipschitz networks without skip-
connection and with skip-connections based on addition/concatenation (see Section 4). As shown in
Table 4 (left), the network with additive skip-connection substantially outperforms the other two, and
the one without skip-connections performs the worst. Therefore, we empirically show that (additive)
skip-connection is crucial in deep Lipschitz networks. (2) Depth and width. Exact orthogonality
is criticized for harming the expressive power of neural networks. We show that the decrease of
expressive power can be alleviated by increasing the network depth/width. In Table 3 (Bottom)
and Table 7 (Appendix E), we observe that deeper/wider architectures increase both the clean and
robust accuracy. (3) Initialization methods. We try different initialization methods, including
identical, permutation, uniform, and torus (Henaff et al., 2016; Helfrich et al., 2018). We find that
identical initialization works the best for deep Lipschitz networks (> 10 layers), while all methods
perform similarly in shallow networks as shown in Table 6 (Appendix E). (4) Receptive field and
down-sampling. Previous works (Li et al., 2019b; Trockman & Kolter, 2021) use larger kernel
8
Under review as a conference paper at ICLR 2022
Skip type	Test Acc.	
	Clean	PGD
ConvNet (w/o skip)	69.59	59.22
ShuffleNet (concat)	75.21	66.00
ResNet (add)	87.82	76.46
Table 4: (Left) Comparisons of various skip connection types on WideResNet22-10 (kernel size 5). (Right)
Comparisons of various receptive field and down-sampling types on WideResNet10-10. The symbols ✓,
X indicate whether average pooling or Strided convolution is used for down-sampling. For “slim” in Strided
convolution, we set kernel_size = stride; and for for “wide”, kernel_size = stride * kernel_size’ (where kernel_
size, is the kernel size for the main branch.
Receptive Field		Down-Sampling		Test Acc.	
Kernel	Dilation	Pool	Stride	Clean	PGD
3	1	X	slim	80.70	68.81
3	1	X	wide	82.36	70.36
3	1	✓	X	84.54	71.71
3	2	✓	X	81.53	70.07
5	1	✓	X	84.09	74.29
5	2	✓	X	81.28	70.58
00000
00000
54321
)s( emiT niarT
05 05 05 0
32211
)s( emiT ecnerefnI
s°)frouκ≡
size and no stride for Lipschitz networks. In Table 4 (Right), we perform a study on the effects
of kernel/dilation size and down-sampling types for the orthogonal convolutions. We find that an
average pooling as down-sampling consistently outperforms strided convolutions. Furthermore, a
larger kernel size helps to boost the performance. (5) Run-time and memory comparison. We find
that previously proposed orthogonal convolutions such as CayleyConv, BCOP, and RKO require
more GPU memory and computation time than SC-Fac. Therefore, we could not to scale them due
to memory constraints (for 22 and 32 layers using Tesla V100 32G). In order to scale up Lipschitz
networks, economical implementation of orthogonal convolution is crucial. As shown in Figure 4, for
deep and wide architectures, our SC-Fac is the most computationally and memory efficient method
and the only method that scales to a width increase of 10 on WideResNet22. Missing numbers in
Figure 4 and Table 7 (Appendix E) are due to the large memory requirement.
-∙- Normal →- SC-FaC →- CayIey →- BCOP →- RKO ∣
15
10
5
0
1	3	6	8	10	1	3	6	8	10	1	3	6	8	10
Width	Width	Width
Figure 4: Run-time and memory comparison using WideResNet22 on Tesla V100 32G. x-axis indicates the
width factor (channels = base_channels × factor). Our SC-Fac is the most computationally and memory-efficient
for wide architectures and is the only method that scales to width factor to 10 on WideResNet22. We also
compare with an ordinary network with regular convolutions and ReLU activations. Note that SC-Fac has the
same inference speed as a regular convolution — the overhead is from the GroupSort activations.
In summary, additive skip-connections are still essential for learning deep orthogonal networks. Due
to the orthogonal constraints, it is helpful to increase the depth/width of the network. However,
this significantly increases the memory requirement; thus, a cheap implementation (like SC-Fac) is
desirable. Finally, we find that a larger kernel size and down-sampling based on average pooling is
helpful, unlike standard practices in deep networks.
7 Conclusion
In this paper, we present a paraunitary framework for orthogonal convolutions. Specifically, we estab-
lish the equivalence between orthogonal convolutions in the spatial domain and paraunitary systems
in the spectral domain. Therefore, any design for orthogonal convolutions is implicitly constructing
paraunitary systems. We further show that the orthogonality for variants of convolution (strided,
dilated, and group convolutions) is also fully characterized by paraunitary systems. In summary,
paraunitary systems are all we need to ensure orthogonality for diverse types of convolutions.
Based on the complete factorization of 1D paraunitary systems, we develop the first exact and
complete design of separable orthogonal 2D-convolutions. Our versatile design allows us to study
the design principles for orthogonal convolutional networks. Consequently, we scale orthogonal
networks to deeper architectures, substantially outperforming their shallower counterparts. In our
experiments, we observe that exact orthogonality plays a crucial role in learning deep Lipschitz
networks. In the future, we plan to investigate other use cases that exact orthogonality is essential.
9
Under review as a conference paper at ICLR 2022
Ethics S tatement
Our work lies in the foundational research on neural information processing. Specifically, we establish
the equivalence between orthogonal convolutions in neural networks and paraunitary systems in signal
processing. Furthermore, our presented orthogonal convolutional layers are plug-and-play modules
that can replace various convolutional layers in neural networks. Consequently, our modules are
applicable in Lipschitz networks for adversarial robustness, recurrent networks for learning long-term
dependency, or flow-based networks for effortless reversibility.
The vulnerability of neural networks raises concerns about their deployment in security-sensitive
scenarios, such as healthcare systems or self-driving cars. In our experiment, we demonstrate a
successful application of orthogonal convolutions in learning robust networks. Furthermore, these
networks achieve higher robust accuracy without additional techniques such as adversarial training or
randomized smoothing. Therefore, our research contributes to the robust learning of neural networks
and potentially leads to their broader deployment.
Furthermore, we reduce the inference cost of our layer to be the same as a traditional convolution
layer, significantly lowering the expense for deployment compared with other methods for orthogonal
networks. However, our layers are memory and computationally more expensive during training than
traditional layers. The overhead to the already expensive training cost exacerbates the concerns on
the efficacy of learning neural networks. Therefore, balancing between robustness and efficiency
is an important research topic that requires more research in the future. We develop more efficient
implementation than previous approaches, narrowing the gap between these two conflicting goals.
Reproducibility S tatement
We provide detailed proofs to our theoretical claims in Appendices B to D. We describe experimental
setups and training details of our models in Appendix E. We include our code for experiments in the
supplementary material.
References
Cem Anil, James Lucas, and Roger Grosse. Sorting out lipschitz function approximation. In
International Conference on Machine Learning, pp. 291-301, 2019.
Aharon Azulay and Yair Weiss. Why do deep convolutional networks generalize so poorly to small
image transformations? Journal of Machine Learning Research, 20(184):1-25, 2019.
Nitin Bansal, Xiaohan Chen, and Zhangyang Wang. Can we gain more from orthogonality regular-
izations in training deep cnns? In Proceedings of the 32nd International Conference on Neural
Information Processing Systems, pp. 4266-4276. Curran Associates Inc., 2018.
Jens Behrmann, Will GrathWohL Ricky TQ Chen, David Duvenaud, and Jorn-Henrik Jacobsen.
Invertible residual networks. In International Conference on Machine Learning, pp. 573-582.
PMLR, 2019.
Ake Bjorck and Clazett Bowie. An iterative algorithm for computing the best estimate of an
orthogonal matrix. SIAM Journal on Numerical Analysis, 8(2):358-364, 1971.
Minmin Chen, Jeffrey Pennington, and Samuel Schoenholz. Dynamical isometry and a mean field
theory of rnns: Gating enables signal propagation in recurrent neural networks. In International
Conference on Machine Learning, pp. 873-882. PMLR, 2018.
Ricky TQ Chen, Jens Behrmann, David K Duvenaud, and Joern-Henrik Jacobsen. Residual flows for
invertible generative modeling. Advances in Neural Information Processing Systems, 32, 2019.
Artem Chernodub and Dimitri Nowicki. Norm-preserving orthogonal permutation linear unit activa-
tion functions (oplu). arXiv preprint arXiv:1604.02313, 2016.
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval
networks: Improving robustness to adversarial examples. In International Conference on Machine
Learning, pp. 854-863. PMLR, 2017.
10
Under review as a conference paper at ICLR 2022
Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components
estimation. arXiv preprint arXiv:1410.8516, 2014.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv
preprint arXiv:1605.08803, 2016.
Victor Dorobantu, Per Andre Stromhaug, and Jess Renteria. Dizzyrnn: Reparameterizing recurrent
neural networks for norm-preserving backpropagation. arXiv preprint arXiv:1612.04035, 2016.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256.JMLR Workshop and Conference Proceedings, 2010.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. Ffjord:
Free-form continuous dynamics for scalable reversible generative models. arXiv preprint
arXiv:1810.01367, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European Conference on Computer Vision, pp. 630-645. Springer, 2016b.
Kyle Helfrich, Devin Willmott, and Qiang Ye. Orthogonal recurrent neural networks with scaled
cayley transform. In International Conference on Machine Learning, pp. 1969-1978. PMLR,
2018.
Mikael Henaff, Arthur Szlam, and Yann LeCun. Recurrent orthogonal networks and long-memory
tasks. In International Conference on Machine Learning, pp. 2034-2042, 2016.
Nicholas J Higham. The scaling and squaring method for the matrix exponential revisited. SIAM
review, 51(4):747-764, 2009.
Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 7132-7141, 2018.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In CVPR, volume 1, pp. 3, 2017.
Lei Huang, Li Liu, Fan Zhu, Diwen Wan, Zehuan Yuan, Bo Li, and Ling Shao. Controllable
orthogonalization in training dnns. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 6429-6438, 2020.
Barry Hurley and Ted Hurley. Paraunitary matrices. arXiv preprint arXiv:1205.0703, 2012.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International conference on machine learning, pp. 448-456.
PMLR, 2015.
Kui Jia, Dacheng Tao, Shenghua Gao, and Xiangmin Xu. Improving training of deep neural networks
via singular value bounding. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 4344-4352, 2017.
Kui Jia, Shuai Li, Yuxin Wen, Tongliang Liu, and Dacheng Tao. Orthogonal deep neural networks.
IEEE transactions on pattern analysis and machine intelligence, 2019.
Li Jing, Yichen Shen, Tena Dubcek, John Peurifoy, Scott Skirlo, Yann LeCun, Max Tegmark, and
Marin Soljacic. Tunable efficient unitary neural networks (eunn) and their application to rnns. In
International Conference on Machine Learning, pp. 1733-1741. PMLR, 2017.
11
Under review as a conference paper at ICLR 2022
Jaroslav Kautsky and Radka Turcajova A matrix approach to discrete wavelets. In WaveletAnalysis
and Its Applications, volume 5, pp. 117-135. Elsevier, 1994.
Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In
Advances in neural information processing systems, pp. 10215-10224, 2018.
Ivan Kobyzev, Simon Prince, and Marcus Brubaker. Normalizing flows: An introduction and review
of current methods. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.
Mario Lezcano Casado. Trivializations for gradient-based optimization on manifolds. Advances in
Neural Information Processing Systems, 32:9157-9168, 2019.
Mario Lezcano-Casado and Davidd Martinez-Rubio. Cheap orthogonal constraints in neural networks:
A simple parametrization of the orthogonal and unitary group. In International Conference on
Machine Learning, pp. 3794-3803, 2019.
Jun Li, Fuxin Li, and Sinisa Todorovic. Efficient riemannian optimization on the stiefel manifold via
the cayley transform. In International Conference on Learning Representations, 2019a.
Qiyang Li, Saminul Haque, Cem Anil, James Lucas, Roger B Grosse, and Jorn-Henrik Jacobsen.
Preventing gradient attenuation in lipschitz constrained convolutional networks. In Advances in
neural information processing systems, pp. 15390-15402, 2019b.
Yuan-Pei Lin and PP Vaidyanathan. Theory and design of two-dimensional filter banks: A review.
Multidimensional Systems and Signal Processing, 7(3-4):263-330, 1996.
Sheng Liu, Xiao Li, Yuexiang Zhai, Chong You, Zhihui Zhu, Carlos Fernandez-Granda, and Qing
Qu. Convolutional normalization: Improving deep convolutional network robustness and training.
Advances in Neural Information Processing Systems, 34, 2021.
Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2: Practical guidelines for
efficient cnn architecture design. In Proceedings of the European conference on computer vision
(ECCV), pp. 116-131, 2018.
Kehelwala DG Maduranga, Kyle E Helfrich, and Qiang Ye. Complex unitary recurrent neural
networks using scaled cayley transform. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 33, pp. 4528-4535, 2019.
Alexander Mathiasen, Frederik Hvilsh0j, Jakob R0dsgaard J0rgensen, Anshul Nasery, and Davide
Mottin. What if neural networks had svds? arXiv preprint arXiv:2009.13977, 2020.
Zakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, and James Bailey. Efficient orthogonal
parametrisation of recurrent neural networks using householder reflections. In Proceedings of the
34th International Conference on Machine Learning-Volume 70, pp. 2401-2409, 2017.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. In International Conference on Learning Representations, 2018.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to spectrally-
normalized margin bounds for neural networks. In International Conference on Learning Repre-
sentations, 2018.
Alan V. Oppenheim, Alan S. Willsky, and S. Hamid Nawab. Signals & Systems (2nd Ed.). Prentice-
Hall, Inc., 1996.
George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji
Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. arXiv preprint
arXiv:1912.02762, 2019.
Jeffrey Pennington, Samuel S Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep
learning through dynamical isometry: theory and practice. In Proceedings of the 31st International
Conference on Neural Information Processing Systems, pp. 4788-4798, 2017.
12
Under review as a conference paper at ICLR 2022
Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. The emergence of spectral universality in
deep networks. In International Conference on Artificial Intelligence and Statistics, pp.1924-1932.
PMLR, 2018.
Haozhi Qi, Chong You, Xiaolong Wang, Yi Ma, and Jitendra Malik. Deep isometric learning for
visual recognition. In International Conference on Machine Learning, pp. 7824-7835. PMLR,
2020.
Hanie Sedghi, Vineet Gupta, and Philip M Long. The singular values of convolutional layers. In
International Conference on Learning Representations, 2019.
Sahil Singla and Soheil Feizi. Skew orthogonal convolutions. arXiv preprint arXiv:2105.11417,
2021.
Gilbert Strang and Truong Nguyen. Wavelets and filter banks. SIAM, 1996.
Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks.
In International Conference on Machine Learning, pp. 6105-6114. PMLR, 2019.
Asher Trockman and J Zico Kolter. Orthogonalizing convolutional layers with the cayley transform.
In International Conference on Learning Representations, 2021.
PP Vaidyanathan. Multirate systems and filter banks. Prentice-Hall, Inc., 1993.
Rianne Van Den Berg, Leonard Hasenclever, Jakub M Tomczak, and Max Welling. Sylvester
normalizing flows for variational inference. In 34th Conference on Uncertainty in Artificial
Intelligence 2018, UAI 2018, pp. 393-402. Association For Uncertainty in Artificial Intelligence
(AUAI), 2018.
Shankar Venkataraman and Bernard C Levy. A comparison of design methods for 2-d fir orthogonal
perfect reconstruction filter banks. IEEE Transactions on Circuits and Systems II: Analog and
Digital Signal Processing, 42(8):525-536, 1995.
Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. On orthogonality and learning
recurrent networks with long term dependencies. In International Conference on Machine Learning,
pp. 3570-3578, 2017.
Jiayun Wang, Yubei Chen, Rudrasis Chakraborty, and Stella X Yu. Orthogonal convolutional neural
networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.
Eric Wong, Frank R Schmidt, Jan Hendrik Metzen, and J Zico Kolter. Scaling provable adversarial
defenses. In Proceedings of the 32nd International Conference on Neural Information Processing
Systems, pp. 8410-8419, 2018.
Eric Wong, Leslie Rice, and J Zico Kolter. Fast is better than free: Revisiting adversarial training. In
International Conference on Learning Representations, 2020.
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and Jeffrey Pennington.
Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla convolu-
tional neural networks. In International Conference on Machine Learning, pp. 5393-5402. PMLR,
2018.
Di Xie, Jiang Xiong, and Shiliang Pu. All you need is beyond a good init: Exploring better solution
for training extremely deep convolutional neural networks with orthonormality and modulation. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6176-6185,
2017.
Chengxi Ye, Matthew Evanusa, Hua He, Anton Mitrokhin, Tom Goldstein, James A Yorke, Cornelia
Fermuller, and Yiannis Aloimonos. Network deconvolution. In International Conference on
Learning Representations, 2020.
Jiong Zhang, Qi Lei, and Inderjit Dhillon. Stabilizing gradients for deep neural networks via efficient
svd parameterization. In International Conference on Machine Learning, pp. 5806-5814, 2018a.
13
Under review as a conference paper at ICLR 2022
Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient
convolutional neural network for mobile devices. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 6848-6856, 2018b.
Jianping Zhou. Multidimensional Multirate Systems: Characterization, Design, and Applications.
Citeseer, 2005.
14
Under review as a conference paper at ICLR 2022
Appendices: Scaling-up Diverse Orthogonal Convolutional
Networks by a Paraunitary Framework
Notations. We use non-bold letters for scalar (e.g., x) and bold ones for vectors or matrices (e.g., x).
We denote sequences in the spatial domain using lower-case letters (e.g., x[n], x[n]) and their spectral
representations using upper-case letters (e.g., X(z), X(z)). For a positive integer, say R ∈ Z+, we
abbreviate the set {0,1,…，R - 1} as [R], and whenever possible, We use its lower-case letter, say
r ∈ [R], as the corresponding iterator.
Assumptions. For simplicity, we assume all sequences are L2 with range Z = {0, ±1, ±2, ∙∙∙} (a
sequence x = {x[n], n ∈ Z} is L2 if Pn∈Z kx[n]k2 < ∞). Such assumption is common in the
literature, which avoids boundary conditions in signal analysis. To deal with periodic sequences
(finite sequences with circular padding), one can either adopt the Dirac function in the spectral domain
or use discrete Fourier transform to compute the spectral representations. In our implementation, we
address the boundary condition case by case for each convolution type, with which we achieve exact
orthogonality in the experiments (Section 6.1).
A	Pseudo Code for Our SC-Fac Algorithm
We include the pseudo-code for separable complete factorization (Section 2) in Algorithm 1 and
diverse orthogonal convolutions (Section 3) in Algorithm 2. The pseudo-code in Algorithm 1 consists
of three parts: (1) First, we obtain orthogonal matrices from skew-symmetric matrices using matrix
exponential. We use GeoTorch library (Lezcano Casado, 2019) for the function matrix_exp in our
implementation; (2) Subsequently, we construct two 1D paraunitary systems using these orthogonal
matrices; (3) Lastly, we compose two 1D paraunitary systems to obtain one 2D paraunitary systems
The pseudo-code in Algorithm 2 consists of two parts: (1) First, we reshape each paraunitary system
into an orthogonal convolution depending on the stride; and (5) second, we concatenate the orthogonal
kernels for different groups and return the output.
B Orthogonal Convolutions via Paraunitary Systems
In this section, we prove the convolution theorem and Parseval’s theorem for standard convolutional
layers. Subsequently, we prove the paraunitary theorem which establishes the equivalence between
orthogonal convolutional layers and paraunitary systems.
B.1 Spectral Analysis of Standard Convolution Layers
Standard convolutional layers are the default building blocks for convolutional neural networks.
One such layer consists of a filter bank with T × S filters h = {hts [n], n ∈ Z}t∈[T],s∈[S] , where
S, T are the number of input and output channels respectively. The layer maps an S-channel input
x = {xs[i], i ∈ Z}s∈[S] to a T -channel output y = {yt[i], i ∈ Z}t∈[T] according to
yt[i]=	hts [n]xs [i - n],
s∈[S] n∈Z
(B.1)
where i indexes the output location to be computed, and n indexes the filter coefficients. Alternatively,
we can rewrite Equation (B.1) in matrix-vector form as
y[i] =	h[n]x[i - n],
n∈Z
(B.2)
where each h[n] ∈ RT×S is a matrix, and each x[i - n] ∈ RS or y[i] ∈ RT is a vector.
Notice that in Equation (B.2), we group entries from all channels into a vector or matrix (e.g.,
from {x0[n]}s∈[S] to x[n]), different from a common notation that groups entries from all locations
into a vector or matrix (e.g., from {xs [n], n ∈ Z} into xs). In the matrix-vector form, a standard
convolutional layer computes a vector sequence y = {y[i] ∈ RT, i ∈ Z} with a convolution between
a matrix sequence h = {h[n] ∈ RT×S, n ∈ Z} and a vector sequence x = {x[i] ∈ RS, i ∈ Z}.
15
Under review as a conference paper at ICLR 2022
Algorithm 1: Separable Complete Factorization (SC-Fac)
Input: Number of channels C, kernel size K = 2L + 1, and
Skew-symmetric matrices {Ad')} with A%)∈ Rc×c,∀' ∈ [-L,L], d ∈ {1, 2}.
Output: A paraunitary system H ∈ RC×C×K×K.
Initialization: Sample Nd' from {1,…，C} uniformly ∀' ∈ [-L, L], d ∈ {1, 2}
/* Iterate for vertical/horizontal dimensions	*/
for d = 1 to 2 do
/* 1) Compute orthogonal matrices from skew-symmetric matrices */
/* Iterate for filter locations	*/
for ` = -L to L do
if ` = 0 then
Qd — matrix_exp(AdO)) // Use matrix_exp() in GeoTorch
(Lezcano Casado, 2019)
else
Ud) J select(matrix_exp(Ad')), cols = Nd)) // selects the first cols
columns of the matrix
end if
end for
/* 2) Compose 1D paraunitary systems from orthogonal matrices */
Hd J Qd
for ` = 1 to L do
Hd J ConVId(Hd, U'前 ), I - U,U「
Hd J conv1d( I- UdT)UdTL, ud-''ud-')> ,Hd)
end for
end for
/* 3) Compose a 2D paraunitary systems from two 1D paraunitary	*/
H J Compose(H1, H2) // i.e., H:,:,i,j = (H2):,:,j (H1):,:,i where the 1D
paraunitary systems H1 and H2 are of size C × C × K
return H
Algorithm 2: Construct Diverse Orthogonal Convolutions from Paraunitary Systems
Input: Number of base channels C, kernel size K = R(2L + 1),
stride R, dilation D, number of groups G
Output: An orthogonal kernel W ∈ RT×S×K×K
Set K0 J K/R, number of input channels S J GC/R2 and output channels T J GC
for g = 0 to G - 1 do
/* 1) Construct orthogonal convolutions from paraunitary systems */
Initialize skew-symmetric matrices {{Ad',g)}L=_L}d=ι for the current g
Hg J Algorithm 1: SC-FaC(C, K0, {{4尸—]=1)
Hg J reshape(Hg, (C,C,K0,K0) → (C/R2, C, K, K))
end for
/* 2) Concatenate orthogonal convolutions from different groups	*/
W J concatenate({Hg}gG=-01, dim = 0)
return W (where the filter for input channel s and output channel t is W t,s,:,: ∈ RK×K)
Let us first define the Z-transform and various types of Fourier transform in Definition B.1 before
proving the convolution theorem (Theorem 2.1).
16
Under review as a conference paper at ICLR 2022
Definition B.1 (Z-transform and Fourier transforms). Fora sequence (of scalars, vectors, or matrices)
x = {x[n], n ∈ Z}, its Z-transform X(z) is defined as
X(z) =	x[n]z-n,
n∈Z
(B.3)
where z ∈ C is a complex number such that the infinite sum is convergent. If z is restricted to the
unit circle z = ejω (i.e., |z | = 1), the z-transform X (z) reduces to a discrete-time Fourier transform
(DTFT) X (ejω). If ω is further restricted to a finite set ω ∈ {2πk∕N, k ∈ [N ]} ,the DTFT X (ejω)
reduces to an N -points discrete Fourier transform (DFT) X (ej2nk/N).
The celebrated convolution theorem states that the convolution in spatial domain (Equation (B.2)) is
equivalent to a multiplication in the spectral domain, i.e., Y (z) = H(z)X(z), ∀z ∈ C.
Proof of Theorem 2.1. The proof follows directly from the definitions of standard convolution (Equa-
tion (B.2)) and Z-transform (Equation (B.3)).
Σ Σ
i∈Z n∈Z
h[n]z-n
n∈Z
Y(z)=	y[i]z-i	(B.4)
i∈Z
x[i - n] z-i	(B.5)
X x[i - n]z-(i-n)	(B.6)
i∈Z
Xh[n]z-n	Xx[k]z-k	(B.7)
= H(z)X(z),	(B.8)
where Equations (B.4) and (B.8) use the definition of Z-transform, Equation (B.5) uses the definition
of convolution, and Equation (B.7) makes a change of variable k = i - n.	□
Next, we introduce the concepts of inner product and Frobenius norm for sequences. We then
prove Parseval’s theorem, which allows us to compute the sequence norm in the spectral domain.
Definition B.2 (Inner product and norm for sequences). Given two sequences x = {x[n], n ∈ Z}
and y = {y[n], n ∈ Z} with x[n], y [n] having the same dimension for all n, the inner product of
these two sequences is defined as
hx, yi ,	hx[n], y[n]i
n∈Z
(B.9)
where hx[n], y[n]i denotes the Frobenius inner product between x[n] and y[n]. Subsequently, we
can define the Frobenius norm ofa sequence using inner product as
kxk, VZhxXy
(B.10)
Theorem B.3 (Parsavel’s theorem). Given a sequence x = {x[n], n ∈ Z}, its sequence norm kxk
can be computed by X(ejω) in the spectral domain as
kxk2
1π
Ekx[n]k2 = 2π J JX(ejω)k2dω,
(B.11)
where ∣∣X (ejω )k2 = X (ejω )*X (ejω) is an inner product between two COmpIex arrays.
17
Under review as a conference paper at ICLR 2022
Proof of Theorem B.3. The theorem follows from the definitions of convolution and discrete-time
Fourier transform (DTFT).
21 Z IX x[n]e-jωn, X x[m]e-jωm∖ dω
2π -π n∈Z	m∈Z
1π
EEhxH x[m]i 2π	e-jω(m-n)dω
n∈Z m∈Z	π -π
ΣΣ hx[n], x[m]i lm=n
n∈Z m∈Z
X hx[n], x[n]i =X kx[n]k2,
n∈Z	n∈Z
(B.12)
(B.13)
(B.14)
(B.15)
(B.16)
where Equation (B.14) is due to the bi-linearity of inner products, and Equation (B.15) makes uses of
the fact that Rπ∏ e-jωk dω = 0 fθr k = 0 and ^∏ e-jωk dω = Un dω = 2π fθr k N S	口
B.2 Equivalence between Orthogonal Convolutions and Paraunitary Systems
With the sequence norm introduced earlier, we formally define orthogonality for convolutional layers.
Definition B.4 (Orthogonal convolutional layer). A convolution layer is orthogonal if the input norm
kxk is equal to the output norm ky k for arbitrary input x, that is
kyk ,
X
n∈Z
ky[n]k2
/Xkx[nk2 , 同,
(B.17)
where kxk (or kyk) is defined as the squared root of nZ kx[n]k2 (or nZky[n]k2).
This definition of orthogonality not only applies to standard convolutions in Equation (B.2) but also
variants of convolutions in Appendix C.3. In this section, however, we first establish the equivalence
between orthogonality for standard convolutions and paraunitary systems.
Theorem B.5 (Paraunitary theorem). A standard convolutional layer (in Equation (B.2)) is orthogo-
nal (by Definition B.4) if and only if its transfer matrix H(z) is paraunitary, i.e.,
H(z)tH(Z) = I, ∀∣z∣ = 1 0 H(ejω)tH(ejω) = I, ∀ω ∈ R.	(B.18)
In other words, the transfer matrix H(ejω) is unitary for all frequencies ω ∈ R.
Proof of Theorem B.5. We first prove that a convolutional layer is orthogonal if its transfer matrix
H(z) is paraunitary (i.e., H(ejω) is unitary for any frequency ω ∈ R).
kyk2
2∏ ∕π W(ejω升2 dω
π -π
ɪ 「 ∣∣H(ejω)X(ejω)『dω
2π -π
2∏	X(ejω升2 dω
π -π
kxk2
(B.19)
(B.20)
(B.21)
(B.22)
where Equations (B.19) and (B.22) are due to Parseval’s theorem (Theorem B.3), Equations (B.19)
and (B.20) follows from the convolution theorem (Theorem 2.1), and Equation (B.21) utilizes that
H(ejω) is unitary for any frequency ω ∈ R (thus kH(ejω)X(ejω)k = kX(ejω)kfor any X(ejω)).
The ‘only if’ part also holds in practice. We here prove by contradiction using periodic inputs (e.g.,
finite inputs with circular padding). Suppose there exists a frequency ω and H(ejω) is not unitary.
18
Under review as a conference paper at ICLR 2022
Since H(ejω) is continuous (due to h ∈ L2 and dominated convergence theorem), there exist integers
N, k, such that ω ≈ 2kπ∕N and H(ej2nk/N) is also not unitary. As a result, there exists a complex
vector U such that V = H(ej2nk/N)u while ∣∣vk = ∣∣uk. Therefore, We can construct two periodic
sequences x = {x[n], n ∈ [N]} and y = {y[n], n ∈ [N]} such that
x[n] = UAkkN =⇒ y[n] = vej2nnk/N.	(B.23)
Now the input norm ∣∣x∣ = JPn∈[n] ∣∣x[n]∣∣2 = √NIlUk is not equal to the output norm ∣∣y∣ =
JPn∈[n] ky[n]k2 = √N∣∣v∣∣, i.e., the layer is not orthogonal, which leads to a contradiction. □
C A Paraunitary Framework for Orthogonal Convolutions
Section B.2		Section B.3		Section B.4		
Orthogonal	Spectral Transformation	Paraunitary	Spectral Factorization	Orthogonal	Parameterization	Unconstrained
Conv. Layers	‘ ■ • Standard Conv. • Dilated Conv.	Systems	• Separable • BCOP	Matrices	• Mat. Exp. • Cayley	Parameters
	• Strided Conv. • Group Conv.			Parameterization	• Bjorck • ■ ■ ■	
				• CayleyConV • SOC		
Figure 5: A framework for designing orthogonal convolutional layers. In Appendix C.2, we unify
variants of orthogonal convolutions in the spectral domain and show that their designs reduce to con-
structing paraunitary systems. In Appendix C.3, we show that a paraunitary system can be constructed
with different approaches: our approach and BCOP (Li et al., 2019b) represent the paraunitary using
orthogonal matrices, while CayleyConv (Trockman & Kolter, 2021) and SOC (Singla & Feizi, 2021)
directly parameterizes it using unconstrained parameters. In Appendix C.4, we investigate various
parameterizations for orthogonal matrices, such as matrix exponential, Cayley transform, and BjGrCk
orthogonalization.
C.1 Multi-resolution Analysis
Multi-resolution operations are essential in various convolutional layers, in particular strided and
dilated convolutions. In order to define and analyze these convolutions rigorously, we first review the
concepts of up-sampling, down-sampling, and polyphase components.
(1)	Up-sampling. Given a sequence (of scalars, vectors, matrices) x = {x[n], n ∈ Z}, its up-
sampled sequence x↑R = {x↑R [n], n ∈ Z} is defined as
x↑R[n], x[n/R] n≡0(modR),	(C.1)
0	otherwise
where R ∈ Z+ is the up-sampling rate. Accordingly, we denote the Z-transform of x↑R as
X↑R(z) = Xx↑R[n]z-n.	(C.2)
n∈Z
The following proposition shows that X↑R(z) is easily computed from X(z).
Proposition C.1 (Z-transform of up-sampled sequence). Given a sequence x and its up-sampled
sequence x↑R, their Z-transforms X(z) and X↑R(z) are related by
X↑R(z) = X(zR).
(C.3)
19
Under review as a conference paper at ICLR 2022
Proof of Proposition C.1. The proof makes use of the definition of Z-transform (Equation (B.3)).
X↑R(z) =	x↑R[n]z-n	(C.4)
n∈Z
= X x↑R[mR]z-mR	(C.5)
m∈Z
= X x[m](zR)-m = X(zR),	(C.6)
m∈Z
where Equation (C.5) makes a change of variables m = n/R since x↑R [n] = 0, ∀n = mR.	□
(2)	Down-sampling and polyphase components. Different from the up-sampled sequence, there
exist multiple down-sampled sequences, depending on the phase of down-sampling. These sequences
are known as the polyphase components. Specifically, given a sequence (of scalars, vectors, or
matrices) x = {x[n], n ∈ Z}, its rth polyphase component xr|R = {xr|R[n], n ∈ Z} is defined as
xr|R[n] , x[nR + r],	(C.7)
where R ∈ Z+ is the down-sampling rate. We further denote the Z-transform of xr|R as
Xr|R(z) = Xxr|R[n]z-n,	(C.8)
n∈Z
Note that r ∈ Z is an arbitrary integer, which does not necessarily take values from [R]. In fact, we
have x(r+kR)|R[n] = xr|R[n + k] and Xr+kR|R (z) = zkXr|R(z). In Proposition C.2, we establish
the relation between H(Z) and {HrlR(z)}r∈[R], i.e.,to represent H(Z) in terms of {HrlR(z)}r∈[R].
Proposition C.2 (Polyphase decomposition). Given a sequence x and its polyphase components
xr|R 's, theZ-transform X(Z) can be represented by {XrlR(z)}r∈[R] as
X(Z) =	Xr|R(ZR)Z-r.	(C.9)
r∈[R]
Proof of Proposition C.2. We start with X(Z), and try to decompose it into its polyphase components
X01R(z),…,XR-1lR(z).
X(Z) = Xx[n]Z-n	(C.10)
n∈Z
= XX x[mR + r]Z-(mR+r)	(C.11)
r∈[R] m∈Z
= X	X x[mR + r]Z-mR Z-r	(C.12)
r∈[R] m∈Z
= X	X xr|R[m](ZR)-m Z-r	(C.13)
r∈[R] m∈Z
= X Xr|R(ZR)Z-r,	(C.14)
r∈[R]
where Equation (C.11) makes a change of variables n = mR+r, and Equation (C.13) is the definition
of polyphase components xr | R [m] = X [mR + r].	□
For simplicity, we stack R consecutive polyphase components into polyphase matrices as
Γ X01R(Z)]
X[R](z) =	.	, f[R](z) = [x-0lR(z);…;X-(RT)lR(z)] .	(C.15)
XR-1|R(Z)
The following proposition extends the Parseval’s theorem in Theorem B.3 and shows that the sequence
norm kxk can also be computed in terms of the polyphase matrix X [R] (Z) (or Xf[R] (Z)).
20
Under review as a conference paper at ICLR 2022
Proposition C.3 (Parseval’s theorem for polyphase matrices). Given a sequence x, its sequence
norm kxk can be computed by X[R] (ejω) (or X[R] (ejω)) in the spectral domain as
kxk2 = ⅛
2π
X[R] (ejω)∣∣2 dω = ɪ ∕∏ f [R](ejω)∣∣2 dω.
(C.16)
Proof of Proposition C.3. The proof follows the standard Parseval’s theorem in Theorem B.3. We
only prove the first part of the proposition (using X[R] (ejω )) as follows.
kxk2 =	kx[n]k2
n∈Z
= X X kx[mR + r]k2
r∈[R] m∈Z
=XX∣∣mrh∣∣2
r∈[R] m∈Z
=：XZ π∣∣X r|R (eω "∣2 dω
π r∈[R] -π
=1 /∏ ∣∣x [Ru "2 dω,
(C.17)
(C.18)
(C.19)
(C.20)
(C.21)
where Equation (C.17) follows the definition of sequence norm, Equation (C.18) changes variables
as n = mR + r, Equation (C.19) is the definition of polyphase components, and Equation (C.20)
applies ParseVal's theorem to xr|R's. The second part (using X[R] (ejω)) can be proved similarly. □
C.2 Unifying Various Convolutional Layers in the Spectral Domain
In Appendix B, the convolution theorem states that a standard convolutional layer is a matrix-vector
product Y (z) = H(z)X(z) in the spectral domain, and the layer is orthogonal if and only if H(z)
is paraunitary (Theorem B.5). However, the canonical convolution theorem does not hold for variants
of convolutions, thus enforcing a paraunitary H(z) may not lead to orthogonal convolution. In this
subsection, we address this limitation by showing that various convolutions can be uniformly written
as Y(Z) = H(Z)X(z), where Y(z), H(z), X(Z) are some spectral representations of y, h, x.
Subsequently, we prove that any of these layers is orthogonal if and only if its H(Z) is paraunitary.
(1)	Strided convolutional layers are widely used in neural networks to adjust the feature resolution:
a strided convolution layer decreases the resolution by down-sampling after a standard convolution,
while a transposed convolution increases the resolution by up-sampling before a standard convolution.
Formally, a strided convolutional layer with stride R (abbrev. as ] R-strided convolution) computes
its output following
y[i] = X h[n]x[Ri - n].	(C.22)
n∈Z
In contrast, a transposed strided convolutional layer with stride R (abbrev. as ↑ R-strided convolution)
computes its output according to
y[i] = X h[n]x↑R[i - n].	(C.23)
n∈Z
Proposition C.4 (Orthogonality of strided convolutional layers). For a ] R-strided convolution, the
spatial convolution in Equation (C.22) leads to the following spectral representation:
Y(Z) = Hf[R] (Z)X[R] (Z)	(C.24)
And for an ↑ R-strided convolution, the spatial convolution is represented in spectral domain as:
Y[R] (Z) = H[R] (Z)X(Z)	(C.25)
Furthermore, a ] R-strided convolution is orthogonal if and only if H[R](z) is paraunitary, and an
↑ R-strided convolution is orthogonal if and only ifH[R] [Z] is paraunitary.
21
Under review as a conference paper at ICLR 2022
Proofof Proposition C.4. (1a) 1 R-Strided convolutions. We first prove the spectral representation
of ]R-strided convolution in Equation (C.24).
Y (z) = X y[i]z-i = X X h[n]x[Ri - n] z-i	(C.26)
i∈Z	i∈Z n∈Z
Σ ΣΣh[mR - r]x[(i - m)R + r]	z-i
i∈Z	r∈[R] m∈Z
XX
h[mR - r]z-m	X x[(i - m)R + r]z-(i-m)
r∈[R]	m∈Z	i
h[mR - r]z-m
r∈[R]	m∈Z
i0R + r]z-i0
(C.27)
(C.28)
(C.29)
=	H-r|R(z)Xr|R(z),	(C.30)
r∈[R]
where Equation (C.26) follows from the definitions of the ] R-strided convolution (Equation (C.22))
and the Z-transform (Equation (B.3)), Equation (C.27) makes a change of variables n = mR - r,
Equation (C.29) further changes i0 = i - m, and Equation (C.30) is due to the definition of polyphase
components (Equation (C.7)). Now We rewrite the last equation concisely as
Γ X0lR(Z)-
Y(Z)= [H-0lR(z);…；H-(RT)(z)]	.	,	(C.31)
'	^RzTT	} X RTIR (Z)
H[R](z)	J ,∖
X [R](z)
which is the spectral representation of ] R-strided convolutions in Equation (C.24).
Now we prove the orthogonality condition for ]R-strided convolutions.
kyk2 = 2∏ /ɔp(ej"消 dω
=ɪ ʃ∏ f [R](ejω)X[R] (ejω)∣∣2 dω
=2∏ /：llX "ω "2 dω
=kxk2,
(C.32)
(C.33)
(C.34)
(C.35)
where Equations (C.32) and (C.35) are due to Parseval’s theorems (Theorem B.3 and Proposition C.3),
Equation (C.33) follows from the spectral representation of the ] R-strided convolution (EqUa-
tion (C.24)), and Equation (C.34) utilizes that the transfer matrix is unitary at each frequency. The
“only if” part can be proved by contradiction similar to Theorem B.5.
(1b) ↑ R-strided convolutions. According to Proposition C.1, the Z-transform of x↑R is X(ZR).
Therefore, an application of the convolution theorem (Theorem 2.1) on Equation (C.23) leads us to
Y(Z) = H(Z)X(ZR)	(C.36)
Expanding Y(Z) and H(Z) using polyphase decomposition (Proposition C.2), we have
X Y r|R(ZR)Z-r = X Hr|R(ZR)Z-r X(ZR)	(C.37)
r∈[R]	r∈[R]
X Y r|R(ZR)Z-r = XHr|R(ZR)X(ZR)Z-r	(C.38)
r∈[R]	r∈[R]
Yr|R(ZR) = Hr|R(ZR)X(ZR), ∀r ∈ [R]	(C.39)
Y r|R(Z) = Hr|R(Z)X(Z), ∀r ∈ [R],	(C.40)
22
Under review as a conference paper at ICLR 2022
where Equation (C.39) is due to the uniqueness of Z-transform, and Equation (C.40) changes the
variables from zR to z. Again„we can rewrite the last equation in concisely as
一 Y01R(Z) 一 Y11R (Z) . .	=	一 H01R(z) 一 H 11R(z) . .	X (z),	(C.41)
. YR-1|R(Z)		. HR-1|R(Z)		
{	}|	{Z	}	
Y[R](Z)		H[R](Z)		
which is the spectral representation of ↑ R-strided convolutions in Equation (C.25).
Lastly, we prove the orthogonality condition for ↑ R-strided convolutions.
kyk2 = 2∏	Y q* “2dω	(C.42)
=2Π Z JlH[R](ejω)XB)1dω	(C.43)
=2∏ ∕π IlX ®" )∣ι2 dω π -π	(C.44)
= kxk2,	(C.45)
where Equations (C.42) and (C.45) are due to Parseval’s theorems (Theorem B.3 and Proposition C.3),
Equation (C.43) follows from the spectral representation of the ↑ R-strided convolution (Equa-
tion (C.25)), and Equation (C.44) uses the fact that the transfer matrix is unitary for each frequency.
The “only if" part can be proved by contradiction similar to Theorem B.5.	□
(2)	Dilated convolutional layer is proposed to increase the receptive field of a convolutional layer
without extra parameters and computation. The layer up-samples its filter bank before convolution
with the input. R-dilated convolutional layer) computes its output with the following equation:
y[i] = X h↑R [n]x[i - n]	(C.46)
n∈Z
Proposition C.5 (Orthogonality of dilated convolutional layer). For an R-dilated convolution, the
spatial convolution in Equation (C.46) leads to a spectral representation as
Y (z) = H(zR)X(z),	(C.47)
Furthermore, an R-dilated convolutional layer is orthogonal if and only if H (zR) is paraunitary.
Proof of Proposition C.5. According to Proposition C.1, the Z-transform of h↑R is H(zR). There-
fore, the “if” part follows directly from the convolution theorem. The “only if” part can be proved by
constructing a counterexample similar to Theorem B.5.	□
Notice that H(zR) is paraunitary if and only if H(ejω) is unitary for all frequency ω ∈ R, which is
the same as X Hz being paraunitary. In other words, any filter bank that is orthogonal for a standard
convolution is also orthogonal for a dilated convolution and vice versa.
(3)	Group convolutional layer is proposed to reduce the parameters and computations and used in
many efficient architectures, including MobileNet, ShuffleNet. The layer divides both input/output
channels into multiple groups and restricts the connections within each group.
Formally, a group convolutional layer with G groups (abbrev. as G-group convolutions) is parame-
terized by G filter banks {hg}g∈[G], each consists of (T /G) × (S/G) filters. The layer maps an S
channels input x to a T channels output y according to
y[i] = X blkdiag ({hg[n]}g∈[G]) x[i - n],	(C.48)
n∈Z
where blkdiag({∙}) computes a block diagonal matrix from a set of matrices.
23
Under review as a conference paper at ICLR 2022
Proposition C.6 (Orthogonality of group convolutional layer). For a G-group convolution, the
spatial convolution in Equation (C.48) leads a spectral representation as , their z-transforms satisfy
Y(Z) = blkdiag ({Hg(z)}g∈[G]) X(z),	(C.49)
Furthermore, a G-group convolutional layer is orthogonal if and only if the block diagonal matrix is
paraunitary, i.e., each hg (z) is paraunitary.
Proof of Proposition C.6. Due to the convolution theorem, it suffices to prove that the Z-transform
of a sequence of block diagonal matrices is also block diagonal in the spectral domain.
h0 [n]
-Pn∈Z h0[n]z-n
Σ
n∈Z
I
hG-1[n]
-----------V-----------'
blkdiag {hg[n]}g∈[G]
-n
Pn∈Z hG-1 [n]z-n
(C.50)
z
H0(z)
(C.51)
|
HG-1(z)
}
blkdiag {Hg(Z)}g∈[G]
As a result, we can write the orthogonality condition as
-H0(Z)
hG-1
-H 0(z)tH 0(z)
HG-1(z)
I, ∀lzl = 1.
(C.52)
_	H GT(z)t H GT(Z)
The equation implies Hg(z>Hg(z) = I, ∀∣z∣ = 1, ∀g ∈ [G], i.e., each Hg(z) is ParaUnitary.	口
C.3 Realizations of Paraunitary Systems
In this subsection, we first prove that all finite-length 1D-paraunitary systems can be represented in a
factorized form. Next, we show how we can construct MD-paraunitary systems using 1D systems.
Lastly, we study the relationship of existing approaches to paraunitary systems.
C.3.1 Complete Factorization of 1 D - Paraunitary Systems
The classic theorem for spectral factorization of paraunitary systems is traditionally developed for
causal systems (Vaidyanathan, 1993; Kautsky & Turcajova, 1994). Given a causal paraunitary system
of length L (i.e., polynomial in Z-1), there always exists a factorization such that
H(Z) = QV(z-1; U⑴)…V(z-1; U(LT)),	(C.53)
where Q is an orthogonal matrix, U⑶ is a column-orthogonal matrix, and V(z; U) is defined as
V(Z; U) = (I- UU>) + UU>Z.	(C.54)
In Theorem C.7, we extends this theorem from causal systems to finite-length (but non-causal) ones.
Theorem C.7 (Complete factorization for 1D-paraunitary systems). Suppose that a paraunitary
system H(Z) is finite-length, i.e., it can be written as n h[n]Z-n for some sequence {h[n], n ∈
[—L, L]}, then it can befactorized in thefollowingform:
H(Z) = V(z; U(-L)) ∙∙∙ V(z; U(T))QV(z-1; U⑴)∙∙∙ V(z-1; U(L)),	(C.55)
where Q is an orthogonal matrix, U(') is a column-orthogonal matrix, and V(z; U) is defined
in Equation (C.54). Consequently, the paraunitary System H(z) is parameterized by L + L + 1
(COlUmn-)orthogonal matrices Q and U(') 's.
24
Under review as a conference paper at ICLR 2022
Proof for Theorem C.7. Given a non-causal paraunitary system H(z), we can always find a causal
counterpart H(Z) such that H(Z) = ZLH(Z) (This can be done by shifting the causal system
backward by L steps, which is equivalent to multiplying ZL in the spectral domain). Since the causal
system H(Z) admits a factorization in Equation (C.55), we can write the non-causal system H(Z) as
H(Z) = ZLQV(z-1; U⑴)…V(z-1; U(L+L)).	(C.56)
Therefore, it suffices to show that for an orthogonal matrix Q and any column-orthogonal matrix U,
we can always find another column-orthogonal matrix U such that
ZQV(z-1; U) = V(z; U)Q.	(C.57)
If the equation above is true, we can set U(') = U('-1-L) for ' < 0 and U(') = U('-L) for ' > 0,
which will convert Equation (C.56) into Equation (C.55).
Now we start to prove Equation (C.57). Note that any column-orthogonal U has a complement U
such that [U, U] is orthogonal and I = UU> + UU> . We then rewrite Equation (C.57) as
ZQV (z-1; U) = ZQ(I - U U > + U U >z-1)	(C.58)
=Q(I - UU> + UU>z)	(C.59)
=(I - QUU>Q> + QUU>Q>z)Q	(C.60)
= (I-UU> + UU>Z)Q	(C.61)
= V(Z; U)Q,	(C.62)
where in Equation (C.61) we set U = QU. This completes the proof.	□
C.3.2 Multi-dimensional (MD) Paraunitary Systems
If the data are multi-dimensional (MD), we will need MD-convolutional layers in neural networks.
Analogously, we can prove the equivalence between orthogonal MD-convolutions in the spatial
domain and MD-paraunitary systems in the spectral domain, i.e.,
H(z)*H(z) = I, Z = (zι,…，zd), %| = 1,∀d ∈ [D],	(C.63)
where D is the data dimension. In this work, we adopt a parameterization based on separable systems.
Definition C.8 (Separable MD-paraunitary system). A MD-paraunitary system H(z) is separable if
there exists D 1D-paraunitary Systems H1(z1),…,HD(ZD) such that
H(z) = H(zι,…，zd)，Hι(zι)…HD(ZD).	(C.64)
Therefore, we can construct an MD-paraunitary system with D number of 1D-paraunitary systems,
each of which is represented in Equation (C.55). Notice that not all MD-paraunitary systems are
separable, thus the parameterization in Equation (C.64) is not complete (see Section 5 for a discussion).
However, we can guarantee that our parameterization realizes all separable MD-paraunitary systems
— each separable paraunitary system admits a factorization in Equation (C.64), where each 1D-system
admits a factorization in Equation (C.55).
C.3.3 Interpretations of Previous Approaches
In Theorem B.5, we have shown that a paraunitary transfer matrix is both necessary and sufficient for
a convolution to be orthogonal. Therefore, we can interpret all approaches for orthogonal convolutions
as implicit constructions of paraunitary systems, including singular value clipping and masking
(SVCM) (Sedghi et al., 2019), block convolution orthogonal parameterization (BCOP) (Li et al.,
2019b), Cayley convolution (CayleyConv) (Trockman & Kolter, 2021), skew orthogonal convolution
(SOC) (Singla & Feizi, 2021). Furthermore, we prove how orthogonal regularization (Wang et al.,
2019; Qi et al., 2020) encourages the transfer matrix to be unitary for all frequencies.
(1) Singular value clipping and masking (SVCM) (Sedghi et al., 2019) clips all singular values
of H(ejω) to ones for each frequency ω after gradient update. Since the clipping step can arbitrarily
25
Under review as a conference paper at ICLR 2022
enlarge the filter length, SVCM subsequently masks out the coefficients outside the filter length.
However, the masking step breaks the orthogonality, as we have seen in the experiments (Section 6.1).
(2)	Block convolution orthogonal parameterization (BCOP) (Li et al., 2019b) tries to generalize
the spectral factorization of 1D-paraunitary systems in Equation (C.55) to 2D-paraunitary systems.
H(z1,z2) = zK-1 zK-1 QV(z1,z2; U(1), U21))…V(zi,Z2； U(KT), U2KT)),	(C.65)
where V(z1,z2; U('), UC = V(zi； U('))V(z2; U(f)). In other words, this approach makes
each V -block, instead of the whole paraunitary system, separable. This factorization in BCOP is
incomplete for 2D-paraunitary system — unlike 1D-paraunitary systems, not every 2D-paraunitary
system admits a factorized form (Lin & Vaidyanathan, 1996).
(2)	Calyey convolution (CayleyConv) (Trockman & Kolter, 2021) aims to generalize the Cayley
transform for orthogonal matrices in Equation (C.88) to 2D-paraunitary systems H(z1, z2):
H(z1, z2) = (I - A(z1, z2)) (I + A(z1, z2))-1 ,	(C.66)
where A(z1,z2) is a SkeW-Hermitian matrix for |zi| = 1,包| = 1 (i.e., A(ejωι, ejω2)* =
A(ejω1, ejω2 ) for any ω1, ω2). Since Cayley transform cannot parameterize a matrix with singu-
lar value -1 for any frequency, the CayleyConv is not a complete parameterization.
(4)	Skew orthogonal convolution (SOC) (Singla & Feizi, 2021) aims to generalize the matrix
exponential for orthogonal matrices (Equation (C.90)) to convolution exponential for 2D-paraunitary
systems H(z1, z2):
H(z1,z2) = exp (A(z1,z2))，X A(Z]z2)= I + A(z1,z2) + A(Z:2)+——,(C.67)
k!	2
k=0
where A(Z1, Z2) is skew-Hermitian matrix for any matrix for |Z1 | = 1, |Z2 | = 1 (in other words,
A(ejωι, ejω2 1=A(ejωι, ejω2) for any ωι, ω2). It is not resolved whether all 2D-paraunitary systems
can be represented in terms of convolution exponential.
(5)	Orthogonal regularization (Ortho-Reg) (Wang et al., 2019; Qi et al., 2020) is developed
to encourage orthogonality in convolutional layers. We show that such orthogonal regularization
is equivalent to a unitary regularization of the paraunitary system with uniform weights on all
frequencies.
Σ
i∈Z
2	π
X h[n]h[n - Ri]> - δ[i]	=~ [ IlH [R] (ejω )tH [R](ejω) - I『dω,
n∈Z	2π -π
(C.68)
where δ [0] = I is an identity matrix, δ [n] = 0 is a zero matrix for n 6= 0. We prove the equivalence
more generally in Proposition C.9. However, this approach cannot enforce exact orthogonality and in
practice requires hyperparameter search for a proper regularizer coefficient.
Proposition C.9 (Parseval’s theorem for ridge regularization). Given a sequence of matrices h
{h[n], n ∈ Z}, the following four expressions are equivalent:
ɪ Z ∣∣H[R] (ejω)tH[R](ejω) - I∣∣2 dω,
I	I2
X ∣∣∣X h[n]>h[n - Ri] - δ[i]∣∣∣ ,
i∈Z ∣n∈Z	∣
ɪ ∏ ∣∣H[R] (ejω)H[R](ejω)t - I∣∣2 dω,
∣	∣2
X ∣∣∣X h[n]h[n - Ri]> - δ [i]∣∣∣ ,
i∈Z ∣n∈Z	∣
(C.69a)
(C.69b)
(C.69c)
(C.69d)
where k ∙ k denotes the Frobenius norm ofa matrix.
26
Under review as a conference paper at ICLR 2022
Proof of Proposition C.9. We first prove the equivalence between Equations (C.69a) and (C.69c).
∣∣H[R](ejω)tH[R] (ejω) - I∣∣2
=tr ((H[R](ejω)tH[R](ejω) - IJ (H[R](ejω)tH[R](ejω) - I))	(C.70)
=tr H[R](ejω)tH[R](ejω)H[R](ejω)tH[R](ejω) - 2tr H[R](ejω)tH[R](ejω) +I (C.71)
=tr (H[R](ejω)H[R](ejω)tH[R](ejω)H[R](ejω)t) - 2tr (H[R](ejω)H[R](ejω)t) +I (C.72)
=tr ((H[R](ejω)H[R](ejω)t -I)t (H[R](ejω)H[R](ejω)t - I))	(C.73)
= ∣∣∣H[R](ejω)H[R](ejω)t - I∣∣∣2 ,	(C.74)
where Equations (C.70) and (C.74) make use of kAk2 = tr(AtA), Equations (C.71) and (C.73) are
due to the linearity of tr(∙), and Equation (C.72) utilizes tr(AB) = tr(BA).
Next, we prove the equivalence between Equations (C.69a) and (C.69b).
H[R](ejω)tH[R](ejω) - I = X HrlR(ejω)tHrlR(ejω) - I	(C.75)
r∈[R]
=XXX
hr|R[m]>hr|R[m - i] - δ[i] e-jωi	(C.76)
r∈[R] i∈Z m∈Z
=XXX
h[Rm + r]>h[R(m - i) + r] - δ[i] e-jωi	(C.77)
r∈[R] i∈Z m∈Z
=X XX
h[Rm + r]>h[Rm + r - Ri] - δ[i] e-jωi	(C.78)
i∈Z r∈[R] m∈Z
= X X h[n]>h[n - Ri] -δ[i] e-jωi,	(C.79)
i∈Z n∈Z
where Equation (C.75) follows from the definition of polyphase matrix in Equation (C.15). Equa-
tion (C.76) uses a number of properties of Fourier transform: a Hermitian in the spectral domain is a
transposed reflection in the spatial domain, a frequency-wise multiplication in the spectral domain is
a convolution in the spatial domain, and an identical mapping in the spectral domain is an impulse
sequence in the spatial domain. Equation (C.77) follows from the definition of polyphase components
in Equation (C.7), and Equation (C.79) makes a change of variables n = Rm + r. In summary, we
show that the LHS (denoted D(ejω)) is a Fourier transform of the RHS (denoted as d[i]):
H[R](ejω)tH[R](ejω) - I = X X h[n]>h[n - Ri] - δ[n] e-jωi.
、	二TTT	} i∈Z ∖n∈Z
D(ejω)	、--------------{------------}
d[i]
(C.80)
Applying Parseval’s theorem (Theorem B.3) to the sequence d = {d[i], i ∈ Z}, we have
ɪ Z∏ ∣∣H[R] (ejω)tH[R](ejω) - I∣∣2 dω
Σ
i∈Z
∣2
X h[n]>h[n - Ri] - δ[i]∣∣
n∈Z	∣
(C.81)
which proves the equivalence between Equations (C.69a) and (C.69b). With almost identical argu-
ments, we can prove the equivalence between Equations (C.69c) and (C.69d), that is
2∏ J ∣∣H[R](ejω)H[R](ejω)t- I∣∣2 ʤ = X
2π	-π	i∈Z
∣2
X h[n]h[n - Ri]> - δ[i]∣∣∣
n∈Z	∣
(C.82)
which completes the proof.
□
27
Under review as a conference paper at ICLR 2022
Table 5: Computational complexities of different approaches for orthogonal convolutions.
Approach	Computational Complexity
Normal	O (L2N2C2)
SVCM	O (L2N2C2 + N2C3)
Ortho-Reg	O (L2N2C2 + L4C2)
CayleyConv	O (N2 log(N)C2 + N2C3)
SOC	O (KL2N2C2)
BCOP	O (L2N2C2 + KL2C3)
SC-Fac	O (L2N2C2 + L2C3)
In Table 5, we compare the computational complexities (forward pass) of orthogonal convolutions
against normal convolution. For simplicity, we assume the feature maps have size N × N, the
convolution filters have size L × L, and the maximum of input/output channels is C . We use K to
denote the number of iterations for BjOrck's algorithm in BCOP, or Taylor's series order in SOC.
•	For SC-Fac, BCOP, SVCM, or Ortho-Reg, the first term O(L2N2C2) is the base cost ofa normal
convolution, and the second term is the overhead for reconstruction, projection, or regularization.
Note that the overhead in SC-Fac is comparable to Ortho-Reg, which is lower than SVCM or
BCOP. If C < N2, the overhead in SC-Fac is negligible compared to the base cost.
•	For CayleyConv, the first term O(N2 log(N)C2) is the cost for fast Fourier transform (FFT), and
the second term O(N2C3) is the cost for matrix inversion for all frequencies. The cost of SOC
is exactly K times as a normal convolution. The computational complexities of CayleyConv and
SOC are significantly higher than the one of normal convolution.
•	For those approaches whose filters can be explicitly obtained (SC-Fac, BCOP, SVCM, and Ortho-
Reg), the inference time of an orthogonal convolution is no different from a normal convolution,
i.e., O(L2N 2C2). On the other hand, for those approaches whose filters are implicitly defined
(CayleyConv and SOC), the inference time is the same as the forward pass in training.
C.4 Constrained Optimization over Orthogonal Matrices
In Theorem C.7, we have shown how orthogonal matrices characterize paraunitary systems. Our
remaining goal, therefore, is to parameterize orthogonal matrices using unconstrained parameters. In
this part, we review popular parameterization methods: Householder reflections (Mhammedi et al.,
2017; Mathiasen et al., 2020), Givens rotations (Dorobantu et al., 2016; Jing et al., 2017), Bjorck
orthogonalization (Anil et al., 2019), Cayley transform (Helfrich et al., 2018; Maduranga et al., 2019),
and exponential map (Lezcano-Casado & Mardnez-Rubio, 2019; Lezcano Casado, 2019).
(1)	Householder reflections (Mhammedi et al., 2017; Mathiasen et al., 2020). The parameteriza-
tion represents an orthogonal matrix using a product of Householder matrices. Given N ∈ N and
n ∈ {1, ∙∙∙ , N}, We define H(n) as a mapping from a vector V ∈ Rn (a scalar V ∈ R for n = 1) to
a block-diagonal matrix H(n)(v) ∈ RN×N:
H(n)(v) = [IN-n I 2 vv> 1 , H(I)(V)=FNT J ,	(C.83)
L	In- 2k^2J	L vJ
Where In ∈ Rn×n denotes an identity matrix. For n ≥ 2, H(n) (v) is the Householder matrix
that represents the reflection With respect to the hyperplane orthogonal to concat(0N-n, v) ∈ RN
and passing through the origin. For n = 1, the scalar V takes values {-1, +1}, Which makes
H(1)(V) either an identity matrix (for V = 1) or a Householder matrix (for V = -1). This method
parameterizes an orthogonal matrix as a product of H(n)'s:
U = HS)(VS))…H⑵(V⑵)H⑴(V(I)),	(C.84)
Where each v(n) ∈ Rn is a learnable vector for n ≥ 2, and V(1) is a fixed constant that is generated
at initialization and fixed afterWard. Observing that Equation (C.84) is serial (unfriendly to parallel
computing), Mathiasen et al. (2020) proposes to increase its parallelism using WY transform.
28
Under review as a conference paper at ICLR 2022
(2) Givens rotations (Dorobantu et al., 2016; Jing et al., 2017). The parameterization represents
a special orthogonal matrix using a product of Given rotations matrices. Given N ∈ N and i, j ∈
{1,…，N} with i = j, We define G(i,j) as a mapping from an angle θ ∈ R to the corresponding
rotation matrix G(i,j) ∈ RN×N:	1	∙ . .	・	0	… .	•	0	・ .	•	0' .	
	.. . 0	∙	. .. • cos θ ∙ ∙	. . •	- sin θ •	. . ••	0	
G(i,j) (θ) =	. . .	..	.. .	. . ..	. . .	.	(C.85)
	0 ∙ .	• sin θ • • .	• cos θ • ..	••	0 .	
	. . 0 ∙	. . •	0	••	. . •	0	•	.. ••	1	
This method parameterizes a special orthogonal matrix U as a product of G(i,j) ’s:
U = G(0,1)(θ(0,1)) G(O⑵(θ(O⑵)G(1,2)(0(I⑵)…G(N-2,n-I)(θ(N-2,N-I)),
(C.86)
where θ(i,j)'s are N(N - 1) unconstrained parameters. Since the determinant of each rotation
matrix is +1, their product U also has +1 determinant, thus is a special orthogonal matrix. Again,
Equation (C.86) is highly serial, thus both Dorobantu et al. (2016) and Jing et al. (2017) propose to
group N/2 Given rotations into a packed rotation to increase the parallelism.
(2)	Bjorck orthogonalization (Anil et al., 2019). The algorithm was first introduced in Bjorck &
Bowie (1971) to compute the closest orthogonal matrix of a given matrix. Given an initial matrix UO,
this algorithm iteratively approaches its closest orthogonal matrix as:
Uk+1 = Uk(I + 1 Pk + …+ (-1)p (12) pp) ,Vk ∈ [K],	(C.87)
where K is the iterative steps, Pk =I - Uk>Uk, and p controls the trade-off between efficiency and
accuracy at each step. When the algorithm is used for parameterization, it maps an unconstrained
matrix UO to an approximately orthogonal matrix UK in K steps. Although Bjorck parameterization
is complete (since any orthogonal matrix Q can be represented by UO = Q), it is inexact due to the
iterative approximation.
(3)	Cayley transform (Helfrich et al., 2018; Maduranga et al., 2019). The transform provides
a bijective parameterization of orthogonal matrices without -1 eigenvalue with skew-symmetric
matrices (i.e., A> = -A)
U= (I - A)(I + A)-1,	(C.88)
where the skew-symmetric matrix A is represented by its upper-triangle entries. Since orthogonal
matrices with -1 eigenvalue are out of consideration, the parameterization is incomplete. Helfrich
et al. (2018); Maduranga et al. (2019) overcome this difficulty by a scaled Cayley transform:
U= D(I - A)(I + A)-1,	(C.89)
where D is a diagonal matrix with ±1 non-zero entries, which is (randomly) generated at initialization
and fixed during training.
(4)	Exponential map (Lezcano-Casado & Martinez-Rubio, 2019; Lezcano Casado, 2019). The
mapping provides a surjective parameterization of all special orthogonal matrices (with +1 determi-
nant) with using skew-symmetry matrices (i.e., A> = -A).
∞ Ak	1
U = eχp(A) , X Ir = I + A + 2a + …，
(C.90)
k=O
where the infinite sum can be computed exactly up to machine-precision (Higham, 2009). Lezcano-
Casado & Martinez-RUbiO (2019) derives an efficient backpropagation algorithm for the mapping,
making it an exact and efficient parameterization in neural networks. To support a complete parame-
terization for all orthogonal matrices, Lezcano Casado (2019) extends the mapping as:
U = Q exp(A),
(C.91)
where Q is an orthogonal matrix, which is generated at initialization and fixed during training.
In principle, we can use any of these approaches to parameterize the orthogonal matrices. In this
work, we choose exponential map due to its exactness, efficiency, and completeness.
29
Under review as a conference paper at ICLR 2022
D	Learning Deep Orthogonal Networks with Lipschitz Bounds
In this section, we first discuss the properties of GroupSort and Lipschitz networks. Subsequently, we
prove Proposition 4.1, which exhibits two approaches to construct Lipschitz residual blocks. Lastly,
we prove in Proposition D.1 when a paraunitary system (represented by a complete factorization as in
Theorem C.7) reduces to an orthogonal matrix. The reduction allows us to apply the initialization
methods for orthogonal matrices to paraunitary systems.
GroupSort and orthogonality. The GroupSort activation separates inputs into groups and sorts
each group into ascending order (Anil et al., 2019). It guarantees two properties: (1) The activation
is norm preserving in the forward pass — a sorting function does not change the norm of any
input vector. (2) The activation is gradient norm preserving in the backward pass — a sorting
function acts as a permutation, which is orthogonal. GroupSort with the group size of two is a spacial
case that we use in the paper followed by (Chernodub & Nowicki, 2016; Anil et al., 2019).
A Lipschitz network with orthogonal layers and GroupSort activations is locally orthogonal. Given
any input x to the network, there exists a neighborhood N (x, r) with radius r such that the sorting
order in each activation does not change. In this neighborhood N(x, r), each GroupSort layer operates
as a constant permutation (thus orthogonal); consequently, the whole network is locally orthogonal.
Lipschitz residual blocks. In Proposition 4.1, we prove the Lipschitzness of two types of residual
blocks, one based on additive skip-connection and another based on concatenative one (See Figure 6).
Proof for Proposition 4.1. We first prove the Lipschitzness for the additive residual block f : f(x) ,
αf1(x) + (1 - α)f2(x). Let x, x0 be two inputs to f and f (x), f(x0) be their outputs, we have
kf (x0) - f (x)k	= Il (αf 1(x0) + (1- α)f2(x0))	— («f 1(x) + (1 — α)f2(x)) ∣∣	(D.1)
=I|a (f1(XO)- f 1(X)) + (I-	a) (f2(XO)- f 2 (X))	∣l	(Dz
≤ α IIf1(x0) - f1(x)II + (1 - α) IIf2(x0) - f2(x)II	(D.3)
≤ αL kXO - Xk + (1 - α)L kXO	- Xk	(D.4)
= LkXO - Xk,	(D.5)
where Equation (D.3) makes uses of the triangle inequality, and Equation (D.4) is due to the L-
Lipschitzness of both f1, f2. Therefore, we have shown that kf(XO) - f (X)k ≤ LkXO - Xk.
Similarly, we prove the Lipschitzness for the concatenative residual block g : g(X) ,
P g1(X1); g2(X2) . Let X, XO be two inputs to g and g(X), g(XO) be their outputs, we have
kg(XO) - g(X)k2	=	IIIP	[g1(X1O);g2(X2O)]	-	[g1(X1); g2(X2)]III	(D.6)
= II[g1(XO); g2(XO)] - [g1(X); g2(X)]II2	(D.7)
= IIIg1(X1O) - g1(X)III + IIIg2(X2O) - g2(X2)III	(D.8)
≤ L2 IIIX1O - X1 III + L2 IIIX2O - X2 III	(D.9)
= L2 III[X1O; X2O] - [X1; X2]III	(D.10)
= L2 kXO - Xk2 ,	(D.11)
where Equation (D.7) utilizes	kPXk = kXk, ∀X, and Equation (D.9) is due	to the L-Lipschitzness of
g1, g2.	The	equations above implies that kg(x0) 一 g(x)k ≤ L∣∣x0 — x∣∣.	□
Reduction of a paraunitary system to an orthogonal matrix. In Proposition D.1, we prove a
special case when a paraunitary system reduces to an orthogonal matrix. The reduction allows us to
apply the initialization methods for orthogonal matrices to paraunitary systems.
Proposition D.1 (Reduction of a paraunitary matrix to an orthogonal matrix). Suppose a Paraunitary
system H (Z) takes the complete factorization in Equation (C.55), and assume L = L with U(-`)=
QU (`) for all `, then the paraunitary matrix H(z) reduces to an orthogonal matrix Q,
H(Z) = V(z; U(-L)) ∙∙∙ V(z; U(T))QV(z-1; U⑴)…V(z-1; U(L)) = Q.	(D.12)
30
Under review as a conference paper at ICLR 2022
(a) Basic additive block. (b) Strided additive block.
(c) Basic shuffling block.
(d) Strided shuffling block.
Figure 6: Variants of residual blocks. In our experiments, we combine (a) & (b) to construct an
orthogonal ResNet, and (c) & (d) to construct an orthogonal ShuffleNet. In Proposition 4.1, we prove
the Lipschitzness of these building blocks. Since composition of Lipschitz functions is still Lipschitz,
it implies that a network constructed by these building blocks is also Lipschitz.
Proof for Proposition D.1. In order to prove Equation (D.12), it suffice to show that
V(z; U(-'))QV(z-1; U⑶)=Q,	(D.13)
and Equation (D.12) will reduce recursively to the orthogonal matrix Q. For simplicity, we rewrite
U (-') as L and U(')as R, by which We have L = QR (or R = Q>L) and We aim to prove
V(z; L)QV(z-1; R) = Q. By the definition of V(z; ∙) in Equation (C.54), We expand it as
V (z; L)QV (z-1; R) = I - LL> +LL>z Q	I - RR> +RR>z-1 =
LL>Q(I - RR>) z '	{z	} c[-1]	+ (I - LL>)Q(I - RR>) + LL>QRR> + (I - LL>)QRR> z-			1 (D.14)
		V	 	 c[0]			V	 c[1]	
Therefore, We Will need to shoW that		c[-1] = 0, c[1] = 0 and c[0] = Q.		
We first shoW that both	c[-1] for z and c[1] for z-1 are zero matrices.			
	c[-1] =	=LLτQ (I - RRr)		(D.15)
		LL>Q - LL>QRR>		(D.16)
		L(QτL)τ - L(QτL)τRRτ		(D.17)
		LRτ - L(RτR)Rτ		(D.18)
		LRτ - LRτ = 0,		(D.19)
	c[1]	(I - LLτ)QRRτ		(D.20)
		QRRτ - LLτQRRτ		(D.21)
		(QR)Rτ - LLτ (QR)Rτ		(D.22)
		LRτ - L(LτL)Rτ		(D.23)
		LRτ - LRτ = 0.		(D.24)
31
Under review as a conference paper at ICLR 2022
Table 6: Comparisons of various initialization methods on WideResNet (kernel size 5).
Initialization	WideReSNet10-10		WideResNet22-10	
	Clean (%)	PGD (%)	Clean (%)	PGD (%)
	 uniform	83.58	73.20	87.55	75.71
torus	82.40	72.50	88.12	75.43
permutation	83.18	73.16	87.82	76.46
identical	83.29	73.49	87.82	75.49
Lastly, we show that the constant coefficient c[0] is equal to Q.
c[0] = (I - LL>)Q(I - RR>) + LL>QRR>	(D.25)
= Q -LL>Q - QRR> + 2LL>QRR>	(D.26)
= Q -LR> - LR> + 2LR> = Q	(D.27)
which completes the proof.	□
E S upplementary Materials for Experiments
E.1 Experimental Setup
Network architectures. For fair comparisons, we follow the architectures by Trockman & Kolter
(2021) for KW-Large, ResNet9, WideResNet10-10 (i.e., shallow networks). We set the group size for
GroupSort activations as 2 in all experiments. For networks deeper than 10 layers, we implement their
architectures modifying from the Pytorch official implementation of ResNet. It is crucial to replace
the global pooling before fully-connected layers with an average pooling with a window size of 4.
For the average pooling, we multiply the output with the window size to maintain its 1-Lipschitzness.
Other architectures, including ShuffleNet and plain convolutional network (ConvNet), are further
modified from the ResNet, where only the skip-connections are changed or removed. We use the
widen factor to indicate the channel number: we set the number of channels at each layer as base
channels multiplied by the widen factor. The base channels are 16, 32, 64 for three groups of residual
blocks. More details of the ResNet architecture can be found in the official Pytorch implementation.1
Learning strategies. We use the CIFAR-10 dataset for all our experiments. We normalize all
input images to [0, 1] followed by standard augmentation, including random cropping and horizontal
flipping. We use the Adam optimizer with a maximum learning rate of 10-2 coupled with a piece-wise
triangular learning rate scheduler. We initialize all our SC-Fac layers as permutation matrices: (1) we
select the number of columns for each pair U('), U(-') uniformly from {1,…，T} at initialization
(the number is fixed during training); (2) for ' > 0, we sample the entries in U(') uniformly with
respect to the Haar measure; (3) for ` < 0, we set U (-`) = QU (`) according to Proposition D.1.
E.2 Additional Empirical Results on Hyper-parameters Selection
Multi-class hinge loss. Following previous works on Lipschitz networks (Anil et al., 2019; Li et al.,
2019b; Trockman & Kolter, 2021), we adopt the multi-class hinge loss in training. For each model,
we perform a grid search on different margins 0 ∈ {1 × 10-3, 2 × 10-3, 5 × 10-3, 1 × 10-2, 2 ×
10-2, 5 × 10-2, 0.1, 0.2, 0.5} and report the best performance in terms of robust accuracy. Notice
that the margin 0 controls the trade-off between clean and robust accuracy, as shown in Figure 7.
Initialization methods. In Proposition D.1, we have shown how to initialize our orthogonal convolu-
tional layers as orthogonal matrices. In Table 6, we perform a study on different initialization methods,
including identical, permutation, uniform, and torus (Henaff et al., 2016; Helfrich et al., 2018). We
find that permutation works the best for WideResNet22-10, while all methods are similar in shallower
WideResNet10-10. Therefore, we use permutation initialization for all other experiments.
1 https://github.com/pytorch/vision/blob/master/torchvision/models/
32
Under review as a conference paper at ICLR 2022
)%( ycarucc
—×- Train
—×- Test
→-PGD
Margin 0 (×10-3)
Figure 7: Effect of the Lipschitz margin 0 for WideResNet22-10. It shows a trade-off between
clean and robust accuracy with different margins for multi-class hinge loss. As shown, the training
and test accuracy become higher with larger margin, but the robust accuracy decreases after 0 = 0.1.
Table 7: Comparison of different depth and width on WideResNet (kernel size 5). Some numbers
are missing due to the large memory requirement (on Tesla V100 32G). The notation width factor
indicates (channels = base channels × factor).
	10 layers									
	 Width	1	3	6	8	10	1	3	6	8	10
			Clean (%)				PGD with		= 36/255 (%)		
Ours	79.96	84.17	84.96	84.61	84.09	65.92	69.70	72.18	72.51	74.29
Cayley	77.88	82.14	82.56	85.53	85.01	66.65	73.06	74.33	75.66	76.13
RKO	81.37	83.55	84.67	85.18	84.62	70.55	74.44	76.41	76.65	77.02
	22 layers									
Width	1	3	6	8	10	1	3	6	8	10
			Clean (%)				PGD with		= 36/255 (%)		
Ours	79.90	82.22	87.21	88.10	87.82	67.95	70.88	74.30	75.12	76.46
Cayley	79.11	84.82	85.85	-	-	69.79	65.61	74.81	-	-
RKO	82.71	84.19	84.33	84.55	-	72.40	74.36	75.66	76.41	-
	34 layers									
Width	1	3	6	8	10	1	3	6	8	10
			Clean (%)				PGD with		= 36/255 (%)		
Ours	81.24	88.17	88.92	-	-	69.21	71.85	75.09	-	-
Cayley	82.46	84.29	-	-	-	71.27	74.73	-	-	-
RKO	81.51	83.24	83.92	-	-	71.38	73.84	75.03	-	-
Network depth and width. Exact orthogonality is criticized for harming the expressive power
of neural networks, and we find that increasing network depth/width can partially compensate for
such loss. In Table 7, we perform a study on the impact of network depth/width on the predictive
performance. As shown, deeper/wider architectures consistently improve both the clean and robust
accuracy for our implementation. However, the best robust accuracy is achieved by a 22-layer network
since we can afford a wide architecture for 34-layer architecture.
Comparison against normal convolutional networks. In Table 8, We perform a comparison
between our orthogonal networks and normal convolutional networks. Their architecture are identical
except for the activation function (GroupSort for ours and ReLU for normal convolutional networks).
Since batch normalization is common in normal convolutional networks but not in Lipschitz networks,
33
Under review as a conference paper at ICLR 2022
Table 8: Comparison of orthogonal convolutions and normal convolutions on WideResNet (ker-
nel size 5). The notation width factor indicates (channels = base channels × factor).
	22 layers	
	 Width	1	3	6	8	10	1	3	6	8	10
	Clean (%)	PGD with = 36/255 (%)
Ortho. (SC-Fac)	79.90 82.22 87.21 88.10 87.82	67.95 70.88 74.30 75.12 76.46
Normal (w/o BN)	88.81 90.71 91.59 91.64 91.57	54.33 66.36 69.35 69.94 74.10
Normal (with BN)	88.52 91.74 91.20 92.29 92.40	51.53 66.90 73.18 73.89 73.02
	Training (s)	Inference (s)
Ortho. (SC-Fac)	145.3 173.4 250.1 323.1 434.0	1.47	4.35	9.72 12.65 17.56
Normal (w/o BN)	13.77 30.71 69.35 99.94 153.5	1.01	3.03	6.77	9.12 13.03
Normal (with BN)	16.56 34.16 87.49 106.9 167.4	1.14	3.34	7.33	9.69 13.39
Table 9: Practical robustness against '∞ adversarial examples (WideResNet kernel size 5, '∞
perturbation radius of = 8/255). BCOP+ and SOC (Singla & Feizi, 2021) results with ResNet-18
are reported by Singla & Feizi (2021).
Model	Method I Clean (%)		PGD (%)
Resnet-18	BCOP+	79.26	34.85
	SOC	82.24	43.73
	BCOP	77.57	46.35
WideResNet22-max	Cayley	78.27	45.21
	Ours	76.28	46.27
we provide both results for normal convolutional networks with or without batch normalization. In
the table, we report the clean/robust accuracy, train time for epoch, and inference time for the test set.
Robustness against '∞ attacks using adversarial training. Since orthogonality only guarantees '2
Lipschitzness, Lipschitz networks with orthogonal layers are not naturally robust to '∞ perturbations.
To further guard Lipschitz networks against '∞ attacks, We follow the approach of adversarial training
in Wong et al. (2020). For training, we use a FGSM variant with step size 10/255; for evaluation,
we use 50 PGD iterations with step size 2/255 and 10 random restarts. We report the experimental
results in Table 9. We observe that different orthogonal convolution methods achieve similar '∞
robustness on WideResNet-22. Furthermore, the Lipschitz networks with WideResNet22 architecture
is consistently better than ResNet-18, which is previously used in Singla & Feizi (2021).
E.3 On the Necessity of Exact Orthogonality
Due to the benefits like generalizability and robustness, achieving exact orthogonality in convolutions
is the primary goal ofa current research line (Sedghi et al., 2019; Li et al., 2019b; Trockman & Kolter,
2021; Singla & Feizi, 2021). However, until our work, no previous approach achieves orthogonality
up to machine precision. Therefore, our proposed method serves as an extreme case in gaining insight
into the trade-off between orthogonality and expressiveness.
In Section 6, we have seen that exact orthogonality is not critical in shallow Lipschitz networks
for robustness, and various orthogonal convolutions (with different precision) achieve comparable
results. However, our method is more favorable in deeper networks (with more than 10 layers) — we
show the results in Table 7. It indicates that exact orthogonality is crucial in learning deep Lipschitz
networks. However, without our implementation of exact orthogonality (which does not exist before),
it is unclear whether exact orthogonality up to machine precision is needed in Lipschitz networks.
Moreover, exact orthogonality is essential for other important and timely applications. For example,
reversible networks/normalizing flows (Kingma & Dhariwal, 2018; Van Den Berg et al., 2018) require
exact orthogonality to compute inverse transform and determinants accurately.
34
Under review as a conference paper at ICLR 2022
F Orthogonal Convolutions for Residual Flows
In this section, we first review the class of flow-based generative models (Papamakarios et al., 2019;
Kobyzev et al., 2020). We focus on invertible residual network (Behrmann et al., 2019), a flow-based
model that relies on Lipschitz residual block, and its extended version Residual Flow (Chen et al.,
2019). We then show how to construct improved Residual Flow using our orthogonal convolutions.
Flow-based models. Given an observable vector x ∈ RD and a latent vector z ∈ RD, we define a
bijective mapping f : RD → RD from the latent vector z to an observation x = f (z). We further
define the inverse of f as F = f-1, with which we represent the likelihood of x by the one of z as:
ln pX (x) = ln pZ (z) + ln|det JF (x)|,	(F.1)
where pX is the data distribution, pZ is the base distribution (usually a normal distribution), and
JF (x) is the Jacobian of F at x. In practice, the bijective mapping f is composed by a sequence
of K bijective mapping such that f = fκ ◦•••◦ fι, where each fk is named as a flow. Since the
inverse mapping F = Fi ◦•∙• FK transforms the data distribution PX into a normal distribution PZ,
flow-based models are also known as normalizing flows. Accordingly, we rewrite Equation (F.1) as:
K
ln PX (x) = lnPZ (z) +	ln|det JFk (x)|,
k=1
(F.2)
In a practical flow-based model, we require efficient computations of (a) each bijective mapping fk,
(b) its inverse mapping Fk = f-i, and (C) the corresponding log-determinant ln∣det JF(∙) |.
Invertible residual networks (i-ResNets). Behrmann et al. (2019) proposes a flow-based model
based on residual network (ResNet). Note that a block in ResNet is defined as F(x) = x + g(x),
where g is a convolutional network. In (Behrmann et al., 2019), the authors prove that F is a bijective
mapping if g is 1-Lipschitz, and its inverse mapping can be computed by fixed-point iterations:
xk+1 = y - g(xk),
(F.3)
where y = g(x) is the output of F and the initialization of the iterative algorithm is x0 := y. From
the Banach fixed-point theorem, we have
kx - xkk2 = ILip(g(g) kx1 - x0k ,	(F.4)
i.e., the convergence rate is exponential in the number of iterations and smaller Lipschitz constant
will yield faster convergence. Furthermore, the log-determinant can be computed as:
ln PX (x) = lnPZ(z) +tr(ln(I + Jg(x)))
lnPZ(z)+ tr (XX (一> [Jg(x)]k
k=1	k
(F.5)
(F.6)
where the infinite sum is approximated by truncation and the trace is efficiently estimated using the
HUtChinSon trace estimator tr(A) = Ev〜N(o,i)[v>Av].
To constrain the Lipschitz constant, i-ResNet uses spectral normalization on each linear layer in the
block. Moreover, to improve optimization stability, i-ResNet changes the activation function from
ReLU to ELU, ensuring nonlinear activations have continous derivatives.
As summarized in the conclusion of Behrmann et al. (2019), there are two remaining problems in this
model: (1) The estimator of the log-determinant is biased and inefficient; (2) Designing and learning
networks with a Lipschitz constraint are challenging — one needs to constrain each linear layer in
the block instead of being able to control the Lipschitz constant of a block.
Residual Flow. Chen et al. (2019) address problem {(1) by proposing an unbiased Russian roulette
estimator for Equation (F.6):
tr (ln (I + Jg(x))) = En,v
n
X
k=1
(-1)k v Jg(x)k v
k P(N ≥ k)
(F.7)
35
Under review as a conference paper at ICLR 2022
Table 10: Comparisons of various flow-based models on the MNIST dataset. We report the
performance in bits per dimension (bpm), where a smaller number indicates a better performance.
Model	MNIST
Glow (Kingma & Dhariwal, 2018)	1.05
FFJORD (Grathwohl et al., 2018)	0.99
i-ResNet (Behrmann et al., 2019)	1.05
Residual Flow (Chen et al., 2019)	0.97
SC-Fac Residual Flow (Ours)	0.896
where n 〜P(N) and V 〜N(0, I). Residual Flow further changes the activation function from ELU
to LipSwish. The LipSwich activation avoids derivative saturation, which occurs when the second
derivative is zero in a large region. However, problem (2) remains unresolved.
Residual flows with orthogonal convolutions. We propose to address problem (2) by replacing
the spectral normalized layers by our orthogonal convolutional layers (SC-Fac). Note that orthogonal
convolutions directly control the Lipschitz constant of a ResNet block. We keep all other components
unchanged — in particular, we use LipSwish activation instead of GroupSort, as GroupSort suffers
from derivative saturation. We experiment our model on MNIST dataset. As shown in Table 10,
our model substantially improve the performance over the original Residual Flow. We display some
images generated by our model in Figure 8.
Figure 8: Random samples from SC-Fac Residual Flow trained on MNIST.
36