Under review as a conference paper at ICLR 2022
Layer-wise Adaptive Model Aggregation for
Scalable Federated Learning
Anonymous authors
Paper under double-blind review
Ab stract
In Federated Learning, a common approach for aggregating local models across
clients is periodic averaging of the full model parameters. It is, however, known
that different layers of neural networks can have a different degree of model dis-
crepancy across the clients. The conventional full aggregation scheme does not
consider such a difference and synchronizes the whole model parameters at once,
resulting in inefficient network bandwidth consumption. Aggregating the parame-
ters that are similar across the clients does not make meaningful training progress
while increasing the communication cost. We propose FedLAMA, a layer-wise
model aggregation scheme for scalable Federated Learning. FedLAMA adap-
tively adjusts the aggregation interval in a layer-wise manner, jointly considering
the model discrepancy and the communication cost. The layer-wise aggregation
method enables to finely control the aggregation interval to relax the aggregation
frequency without a significant impact on the model accuracy. Our empirical study
shows that FedLAMA reduces the communication cost by up to 60% for IID data
and 70% for non-IID data while achieving a comparable accuracy to FedAvg.
1	Introduction
In Federated Learning, periodic full model aggregation is the most common approach for aggregat-
ing local models across clients. Many Federated Learning algorithms, such as FedAvg (McMahan
et al. (2017)), FedProx (Li et al. (2018)), FedNova (Wang et al. (2020)), and SCAFFOLD (Karim-
ireddy et al. (2020)), assume the underlying periodic full aggregation scheme. However, it has been
observed that the magnitude of gradients can be significantly different across the layers of neural net-
works (You et al. (2019)). That is, all the layers can have a different degree of model discrepancy.
The periodic full aggregation scheme does not consider such a difference and synchronizes the en-
tire model parameters at once. Aggregating the parameters that are similar across all the clients does
not make meaningful training progress while increasing the communication cost. Considering the
limited network bandwidth in usual Federated Learning environments, such an inefficient network
bandwidth consumption can significantly harm the scalability of Federated Learning applications.
Many researchers have put much effort into addressing the expensive communication issue. Adap-
tive model aggregation methods adjust the aggregation interval to reduce the total communication
cost (Wang & Joshi (2018); Haddadpour et al. (2019)). Gradient (model) compression (Alistarh
et al. (2018); Albasyoni et al. (2020)), sparsification (Wangni et al. (2017); Wang et al. (2018)),
low-rank approximation (Vogels et al. (2020); Wang et al. (2021)), and quantization (Alistarh et al.
(2017); Wen et al. (2017); Reisizadeh et al. (2020)) techniques directly reduce the local data size.
Employing heterogeneous model architectures across clients is also a communication-efficient ap-
proach (Diao et al. (2020)). While all these works effectively tackle the expensive communication
issue from different angles, they commonly assume the underlying periodic full model aggregation.
To break such a convention of periodic full model aggregation, we propose FedLAMA, a novel layer-
wise adaptive model aggregation scheme for scalable and accurate Federated Learning. FedLAMA
first prioritizes all the layers based on their contributions to the total model discrepancy. We present
a metric for estimating the layer-wise degree of model discrepancy at run-time. The aggregation
intervals are adjusted based on the layer-wise model discrepancy such that the layers with a smaller
degree of model discrepancy is assigned with a longer aggregation interval than the other layers.
The above steps are repeatedly performed once the entire model is synchronized once.
1
Under review as a conference paper at ICLR 2022
Our focus is on how to relax the model aggregation frequency at each layer, jointly considering the
communication efficiency and the impact on the convergence properties of federated optimization.
By adjusting the aggregation interval based on the layer-wise model discrepancy, the local mod-
els can be effectively synchronized while reducing the number of communications at each layer.
The model accuracy is marginally affected since the intervals are increased only at the layers that
have the smallest contribution to the total model discrepancy. Our empirical study demonstrates
that FedLAMA automatically finds the interval settings that make a practical trade-off between the
communication cost and the model accuracy. We also provide a theoretical convergence analysis of
FedLAMA for smooth and non-convex problems under non-IID data settings.
We evaluate the performance of FedLAMA across three representative image classification bench-
mark datasets: CIFAR-10 (Krizhevsky et al. (2009)), CIFAR-100, and Federated Extended MNIST
(Cohen et al. (2017)). Our experimental results deliver novel insights on how to aggregate the local
models efficiently consuming the network bandwidth. Given a fixed number of training iterations, as
the aggregation interval increases, FedLAMA reduces the communication cost by up to 60% under
IID data settings and 70% under non-IID data settings, while having only a marginal accuracy drop.
2	Related Works
Compression Methods - The communication-efficient global model update methods can be catego-
rized into two groups: structured update and SketChed update (KoneCny et al. (2016)). The structured
update indicates the methods that enforce a pre-defined fixed structure of the local updates, such as
low-rank approximation and random mask methods. The sketched update indicates the methods
that post-process the local updates via compression, sparsification, or quantization. Both directions
are well studied and have shown successful results (Alistarh et al. (2018); Albasyoni et al. (2020);
Wangni et al. (2017); Wang et al. (2018); Vogels et al. (2020); Wang et al. (2021); Alistarh et al.
(2017); Wen et al. (2017); Reisizadeh et al. (2020)). The common principle behind these methods is
that the local updates can be replaced with a different data representation with a smaller size.
These compression methods can be independently applied to our layer-wise aggregation scheme
such that the each layer’s local update is compressed before being aggregated. Since our focus is on
adjusting the aggregation frequency rather than changing the data representation, we do not directly
compare the performance between these two approaches. We leave harmonizing the layer-wise
aggregation scheme and a variety of compression methods as a promising future work.
Similarity Scores - Canonical Correlation Analysis (CCA) methods are proposed to estimate the
representational similarity across different models (Raghu et al. (2017); Morcos et al. (2018)). Cen-
tered Kernel Alignment (CKA) is an improved extension of CCA (Kornblith et al. (2019)). While
these methods effectively quantify the degree of similarity, they commonly require expensive com-
putations. For example, SVCCA performs singular vector decomposition of the model and CKA
computes Hilbert-Schmidt Independence Criterion multiple times (Gretton et al. (2005)). In addi-
tion, the representational similarity does not deliver any information regarding the gradient differ-
ence that is strongly related to the convergence property. We will propose a practical metric for
estimating the layer-wise model discrepancy, which is cheap enough to be used at run-time.
Layer-wise Model Freezing - Layer freezing (dropping) is the representative layer-wise technique
for neural network training (Brock et al. (2017); Kumar et al. (2019); Zhang & He (2020); Goutam
et al. (2020)). All these methods commonly stop updating the parameters of the layers in a bottom-
up direction. These empirical techniques are supported by the analysis presented in (Raghu et al.
(2017)). Since the layers converge from the input-side sequentially, the layer-wise freezing can re-
duce the training time without strongly affecting the accuracy. These previous works clearly demon-
strate the advantages of processing individual layers separately.
3	Background
Federated Optimization - We consider federated optimization problems as follows.
m
min
x∈Rd
F (x) :=	piFi (x)
i=1
(1)
2
Under review as a conference paper at ICLR 2022
Algorithm 1: FedLAMA: Federated Layer-wise Adaptive Model Aggregation.
Input: T0: base aggregation interval, φ: interval increasing factor, pi,i ∈ {1,…,m}.
1	Tl — T0, ∀l ∈ {1,…,L};
2	for k = 1 to K do
3	SGDstep: Xk = xik-1 - Nf(Wii-ι,ξk )；
4	for l = 1 to L do
5	if k mod Tl is 0 then
6	Synchronize layer l: u(i,k)- Pm=IPixi(I,k)；
7	di — Pm=I (PikU(i,k) - x(ι,k)k2) /(TI(dim(Uak)));
8	if k mod φT0 is 0 then
9	L Adjust aggregation interval at all L layers (Algorithm 2).;
10	Output: UK;
where Pi = ni/n is the ratio of local data to the total dataset, and Fi(X) = ： Pξ∈D fi(x,ξ) is the
local objective function of client i. n is the global dataset size and ni is the local dataset size.
FedAvg is a basic algorithm that solves the above minimization problem. As the degree of data het-
erogeneity increases, FedAvg converges more slowly. Several variants of FedAvg, such as FedProx,
FedNova, and SCAFFOLD, tackle the data heterogeneity issue. All these algorithms commonly
aggregate the local solutions using the periodic full aggregation scheme.
Model Discrepancy - All local SGD-based algorithms allow the clients to independently train their
local models within each communication round. The variance of stochastic gradients and heteroge-
neous data distribution can lead the local models to different directions on parameter space during
the local update steps. We formally define such a discrepancy among the models as follows.
m
XPikU- xik2,
i=1
where m is the number of clients, U is the synchronized model, and xi is client i’s local model.
This quantity bounds the difference between the local gradients and the global gradients under a
smoothness assumption on objective functions.
4 Layer-wise Adaptive Model Aggregation
Layer Prioritization - In theoretical analysis, it is common to assume the smoothness of objective
functions such that the difference between local gradients and global gradients is bounded by a
scaled difference of the corresponding sets of parameters. Motivated by this convention, we define
‘layer-wise unit model discrepancy’, a useful metric for prioritizing the layers as follows.
di
PNIPilIUi-Xik2
Tl (dim(uι))
l ∈ {1, ∙∙∙ ,L}
(2)
where L is the number of layers, l is the layer index, U is the global parameters, xi is the client i’s
local parameters, T is the aggregation interval, and dim(∙) is the number of parameters.
This metric quantifies how much each parameter contributes to the model discrepancy at each iter-
ation. The communication cost is proportional to the number of parameters. Thus, Pim=1 Pi kUl -
xli k2/dim(Ul ) shows how much model discrepancy can be eliminated by synchronizing the layer
at a unit communication cost. This metric allows prioritizing the layers such that the layers with a
smaller dl value has a lower priority than the others.
Adaptive Model Aggregation Algorithm - We propose FedLAMA, a layer-wise adaptive model
aggregation scheme. Algorithm 1 shows FedLAMA algorithm. There are two input parameters: T0
is the base aggregation interval and φ is the interval increase factor. First, the parameters at layer
l are synchronized across the clients after every Tl iterations (line 6). Then, the proposed metric
3
Under review as a conference paper at ICLR 2022
1
2
3
4
5
6
7
8
9
10
11
12
13
Algorithm 2: Layer-wise Adaptive Interval Adjustment.
Input: d: the observed model discrepancy at all L layers, τ0 : the base aggregation interval, φ:
the interval increasing factor.
Sorted model discrepancy: d J sort (d);
Sorted index of the layers: i J argsort (d);
Total model size: λ J Pl* L=1 dim(ul);
Total model discrepancy: δ J EL=I di * dim(uι);
for l = 1 to L do
δι J (Pi=I di * dim(ui))∕δ;
λι J (Pi=Idim(Ui))“；,
Find the layer index: i Jii ;
if δi < λi then
LTi J φτ0;
else
LTi J T0;
Output: τ: the adjusted aggregation intervals at all L layers.;
di is calculated using the synchronized parameters Ui (line 7). At the end of every φT0 iterations,
FedLAMA adjusts the model aggregation interval at all the L layers. (line 9).
Algorithm 2 finds the layers that can be less frequently aggregated making a minimal impact on the
total model discrepancy. First, the layer-wise degree of model discrepancy is estimated as follows.
A	Pi=I di * dim(Ui)	小
δι =	L,	(3)
Pi=I di * dim(ui)
where di is the ith smallest element in the sorted list of the proposed metric d. Given l layers with
the smallest di values, δi quantifies their contribution to the total model discrepancy. Second, the
communication cost impact is estimated as follows.
λι=Pi=I dim(Ui)	⑷
PiL=1 dim(Ui)
λi is the ratio of the parameters at the l layers with the smallest di values. Thus, 1 - λi estimates
the number of parameters that will be more frequently synchronized than the others. As l increases,
δi increases while 1 - λi decreases monotonically. Algorithm 2 loops over the L layers finding the
l value that makes δi and 1 - λi similar. In this way, it finds the aggregation interval setting that
slightly sacrifices the model discrepancy while remarkably reducing the communication cost.
Figure 1 shows the δi and 1 - λi curves collected from a) CIFAR-10 (ResNet20) training and b)
CIFAR-100 (Wide-ResNet28-10) training. The x-axis is the number of layers to increase the aggre-
gation interval and the y-axis is the δi and 1 - λi values. The cross point of the two curves is much
lower than 0.5 on y-axis in both charts. For instance, in Figure 1.a), the two curves are crossed when
x value is 9, and the corresponding y value is near 0.2. That is, when the aggregation interval is
increased at those 9 layers, 20% of the total model discrepancy will increase by a factor of φ while
80% of the total communication cost will decrease by the same factor. Note that the cross points are
below 0.5 since the δi and 1 - λi are calculated using the di values sorted in an increasing order.
It is worth noting that FedLAMA can be easily extended to improve the convergence rate at the cost
of having minor extra communications. In this work, we do not consider finding such interval set-
tings because it can increase the latency cost, which is not desired in Federated Learning. However,
in the environments where the latency cost can be ignored, such as high-performance computing
platforms, FedLAMA can accelerate the convergence by adjusting the intervals based on the cross
point of 1 - δi and λi calculated using the list of di values sorted in a decreasing order.
Impact of Aggregation Interval Increasing Factor φ -In Federated Learning, the communication
latency cost is usually not negligible, and the total number of communications strongly affects the
4
Under review as a conference paper at ICLR 2022
pnu6eE pz--eE.10U
0.0
0	5	10	15	20
# of layers with increased interval
a) CIFAR-10 (ResNet20)
pnu6eE pz=eE.10U
0.0
0	5	10	15	20	25
# of layers with increased interval
b) CIFAR-100 (WRN28-10)
Figure 1: The comparison between the model discrepancy increase factor δι and the communication
cost decrease factor 1 - λι for a) CIFAR-10 and b) CIFAR-100 training.
scalability. When increasing the aggregation interval, Algorithm 2 multiplies a pre-defined small
constant φ to the fixed base interval τ0 (line 10). This approach ensures that the communication
latency cost is not increased while the network bandwidth consumption is reduced by a factor of φ.
FedAvg can be considered as a special case of FedLAMA where φ is set to 1. When φ > 1, Fed-
LAMA less frequently synchronize a subset of layers, and it results in reducing their communication
costs. When increasing the aggregation interval, FedLAMA multiplies φ to the base interval τ0 . So,
it is guaranteed that the whole model parameters are fully synchronized after φτ0 iterations. Because
of the layers with the base aggregation interval τ0, the total model discrepancy of FedLAMA after
φτ0 iterations is always smaller than that of FedAvg with an aggregation interval of φτ0.
5 Convergence Analysis
5.1	Preliminaries
Notations - All vectors in this paper are column vectors. X ∈ Rd denotes the parameters of one
local model and m is the number of clients. The stochastic gradient computed from a single training
data point ξ is denoted by g(X, ξ). For convenience, we use g(X) instead. The full batch gradient is
denoted by VF (x). We use k ∙ k and ∣∣∙∣∣op to denote 12 norm and matrix operator norm, respectively.
Assumptions - We analyze the convergence rate of FedLAMA under the following assumptions.
1.	(Smoothness). Each local objective function is L-smooth, that is, kVFi(X) - VFi (y)k ≤
LkX - yk,∀i ∈ {1,…，m}.
2.	(Unbiased Gradient). The stochastic gradient at each client is an unbiased estimator of the
local full-batch gradient: Eξ [gi (X, ξ)] = VFi (X).
3.	(Bounded Variance). The stochastic gradient at each client has bounded variance:
Eξ[kgi(x,ξ)-VFi(x)k2 ≤ σ2],∀i ∈{1,…，m},σ2 ≥ 0.
4.	(Bounded Dissimilarity). For any sets of weights {pi ≥ 0}im=1, Pim=1 pi = 1, there exist
constants β2 ≥ 1 and κ2 ≥ 0 such that Pim=1 pikVFi(x)k2 ≤ β2 k Pim=1 piVFi(x)k2+κ2.
If local objective functions are identical to each other, β2 = 1 and κ2 = 0.
5.2	Analysis
We begin with showing two key lemmas. All the proofs can be found in Appendix.
Lemma 5.1. (Framework) Under Assumption 1 〜3, ifthe learning rate satisfies η ≤ 在,FedLAMA
ensures
1K	2	m
K ∑E [kVF(Uk)『]≤ — E [F(uι) - F(u*)] + 2ηLσ2 I(Pi)2
2K m
+K XXPi EhlIUk-Xk『].
(5)
5
Under review as a conference paper at ICLR 2022
Lemma 5.2. (Model Discrepancy) Under Assumption 1 〜4, if the learning rate satisfies η <
2(T~~1 -i)L, FedLAMA ensures
1 GS	Bl^∣∣	i ∣∣2] / 2η2 (Tmax - 1)σ2 ,	Aκ2
K∑∑piEWUk-Xk∣∣ ] ≤ —E— + LnF
k=1 i=1
Aβ2 K
+ KL2(T-A) XE [kvF(Uk)k ],
(6)
where A = 4η2(τmax - 1)2L2 and τmax is the largest averaging interval across all the layers.
Based on Lemma 5.1 and 5.2, we analyze the convergence rate of FedLAMA as follows.
Theorem 5.3. Suppose all m local models are initialized to the same point U1. Under As-
Sumption 1 〜 4, if FedLAMA runs for K iterations and the learning rate satisfies η ≤
min < E 1-TW,——/	1 、 =卜，FedLAMA ensures
12(TmaxT)L , Lv∕2Tmαχ(Tmαχ-1)(2β2 + 1) J，
1K
E K ∑kVF(uk)k2
m
≤ K( (E[F(uι) - F(u*)]) + 4ηXp2Lσ2
η	i=1
+ 3η (τmax - 1)L2σ2 + 6η2τm
ax (τm
ax - 1)L2 κ2 ,
(7)
where u* indicates a local minimum and Tmax is the largest averaging interval across all the layers.
Remark 1. (Linear Speedup) With a sufficiently small diminishing learning rate and a large number
of training iterations, FedLAMA achieves linear speedup. If the learning rate is η = √m and
Pi = m1 ,∀i ∈{1,…，m},wehave
E
1K
K EkVF(Uk)k2
K i=1
(8)
If K > m3, the first term on the right-hand side becomes dominant and it achieves linear speedup.
Remark 2. (Impact of Interval Increase Factor φ) The worst-case model discrepancy depends on the
largest averaging interval across all the layers, Tmax = φT0 . The larger the interval increase factor φ,
the larger the model discrepancy terms in (7). In the meantime, as φ increases, the communication
frequency at the selected layers is proportionally reduced. So, φ should be appropriately tuned to
effectively reduce the communication cost while not much increasing the model discrepancy.
6 Experiments
Experimental Settings - We evaluate FedLAMA using three representative benchmark datasets:
CIFAR-10 (ResNet20 (He et al. (2016))), CIFAR-100 (WideResNet28-10 (Zagoruyko & Komodakis
(2016))), and Federated Extended MNIST (CNN (Caldas et al. (2018))). We use TensorFlow 2.4.3
for local training and MPI for model aggregation. All our experiments are conducted on 4 compute
nodes each of which has 2 NVIDIA v100 GPUs.
Due to the limited compute resources, we simulate Federated Learning such that each process se-
quentially trains multiple models and then the models are aggregated across all the processes at
once. While it provides the same classification results as the actual Federated Learning, the train-
ing time is serialized within each process. Thus, instead of wall-clock time, we consider the total
communication cost calculated as follows.
LL
C = X Cl = X dim(uι) * κι,	⑼
l=1	l=1
where κl is the total number of communications at layer l during the training.
6
Under review as a conference paper at ICLR 2022
Table 1: (IID data) CIFAR-10 classification results. The number of workers is 128 and the local
batch size is 32 in all the experiments. The epoch budget is 300.
LR	Base aggregation interval: τ0	Interval increase factor: φ	Validation acc.	Comm. cost
0.8	6	1 (FedAvg)	88.37 ± 0.02%	100% 二
0.8	12	1 (FedAvg)	84.74 ± 0.05%	50%
0.4	6	2 (FedLAMA)	一	88.41 ±0.01%	-62.33%-
0.6	24	1 (FedAvg)	80.34 ± 0.3%	25%
0.6	6	4(FedLAMA)	一	86.21 ±0.1%	42.17% 一
Table 2: (IID data) CIFAR-100 classification results. The number of workers is 128 and the local
batch size is 32 in all the experiments. The epoch budget is 250.
LR	Base aggregation interval: τ0	Interval increase factor: φ	Validation acc.	Comm. cost
0.6	6	1 (FedAvg)	76.50 ± 0.02%	100%	,
0.6	12	1 (FedAvg)	66.97 ± 0.9%	50%	,
0.5	6	2 (FedLAMA)	一	76.02 ±0.01%	-66.01%-
0.6	24	1 (FedAvg)	45.01 ± 1.1%	25%	,
0.5	6	4(FedLAMA)	一	76.17 ±0.02%	39.91% —
Hyper-Parameter Settings - We use 128 clients in our experiments. The local batch size is set to 32
and the learning rate is tuned based on a grid search. For CIFAR-10 and CIFAR-100, we artificially
generate heterogeneous data distributions using Dirichlet’s distribution. When using Non-IID data,
we also consider partial device participation such that randomly chosen 25% of the clients participate
in training at every φτ0 iterations. We report the average accuracy across at least three separate runs.
6.1	Classification Performance Analysis
To evaluate the proposed model aggregation scheme, we keep all the other factors the same, such as
optimizer, the number of clients, the degree of heterogeneity, and compare the performance across
different model aggregation schemes. We compare the performance across three different model
aggregation settings as follows.
•	Periodic full aggregation with an interval of τ 0
•	Periodic full aggregation with an interval of φτ 0
•	Layer-wise adaptive aggregation with intervals of τ0 and φ
The first setting provides the baseline communication cost, and we compare it to the other settings’
communication costs. The third setting is FedLAMA with the base aggregation interval τ 0 and the
interval increase factor φ. Due to the limited space, we present a part of experimental results that
deliver the key insights. More results can be found in Appendix.
Experimental Results with IID Data 一 We first present CIFAR-10 and CIFAR-100 classification
results under IID data settings. Table 1 and 2 show the CIFAR-10 and CIFAR-100 results, respec-
tively. Note that the learning rate is individually tuned for each setting using a grid search, and
we report the best settings. In both tables, the first row shows the performance of FedAvg with
a short interval τ 0 = 6. As the interval increases, FedAvg significantly loses the accuracy while
the communication cost is proportionally reduced. FedLAMA achieves a comparable accuracy to
FedAvg with τ0 = 6 while its communication cost is similar to that of FedAvg with φτ 0 . These
results demonstrate that Algorithm 2 effectively finds the layer-wise interval settings that maximize
the communication cost reduction while minimizing the model discrepancy increase.
Experimental Results with Non-IID Data - We now evaluate the performance of FedLAMA using
non-IID data. FEMNIST is inherently heterogeneous such that it contains the hand-written digit
pictures collected from 3, 550 different writers. We use random 10% of the writers’ training samples
in our experiments. Table 3 shows the FEMNIST classification results. The base interval τ0 is set
to 10. FedAvg (φ = 1) significantly loses the accuracy as the aggregation interval increases. For
example, when the interval increases from 10 to 40, the accuracy is dropped by 2.1% 〜2.7%.
In contrast, FedLAMA maintains the accuracy when φ increases, while the communication cost
is remarkably reduced. This result demonstrates that FedLAMA effectively finds the best interval
setting that reduces the communication cost while maintaining the accuracy.
7
Under review as a conference paper at ICLR 2022
Table 3: (Non-IID data) FEMNIST classification results. The number of workers is 128 and the
local batch size is 32 in all the experiments. The number of training iterations is 2,000.
LR	Base aggregation interval: τ0	Interval increase factor: φ	active ratio	Validation acc.	Comm. cost
0.04	10	1 (FedAvg)	25%	86.04 ± 0.01%	100% 二
	20	1 (FedAvg)		85.38 ± 0.02%	50%
	10	2 (FedLAMA)		86.01 ±0.01%	-52.83%-
	40	1 (FedAvg)		83.97 ± 0.02%	25%
	10	4(FedLAMA)	一		85.61 ±0.02%	-29.97%-
0.04	10	1 (FedAvg)	50%	86.59 ± 0.01%	100% 二
	20	1 (FedAVg)		85.50 ± 0.02%	50%
	10	2 (FedLAMA)		86.07 ±0.02%	-53.32%-
	40	1 (FedAVg)		83.92 ± 0.02%	25%
	10	4(FedLAMA)	一		85.77 ±0.02%	-29.98%-
0.04	10	1 (FedAvg)	100%	85.74 ± 0.03%	100% 二
	20	1 (FedAVg)		85.08 ± 0.01%	50%
	10	2 (FedLAMA)		85.40 ±0.02%	-51.86%-
	40	1 (FedAVg)		83.62 ± 0.02%	25%
	10	4 (FedLAMA)	一		84.67 ±0.02%	29.98% ―
Table 4: (Non-IID data) CIFAR-10 classification results. The number of workers is 128 and the
local batch size is 32 in all the experiments. The number of training iterations is 6, 000.
LR	BaSe aggregation interval: T0	Interval increase factor: φ	active ratio	Dirichlet’s coeff.	Validation acc.	Comm. cost
0.4	6	1 (FedAvg)	25%	0.1	84.02 ± 0.1%	100%	,
	24	1 (FedAVg)			76.27 ± 0.08%	25%
	6	4(FedLAMA)	一			-83.06 ±0.1%-	-39.52%-
0.8	6	1 (FedAvg)	25%	0.5	87.59 ± 0.2%	100% 二
	24	1 (FedAVg)			83.36 ± 0.4%	25%
	6	4(FedLAMA)	一			86.57 ±0.02%	-42.40%-
0.8	6	1 (FedAvg)	100%	0.1	89.52 ± 0.05%	100%	,
	24	1 (FedAVg)			84.82 ± 0.06%	25%
	6	4(FedLAMA)	一			-87.47 ±0.1%-	-42.49%-
0.8	6	1 (FedAvg)	100%	0.5	90.53 ± 0.08%	100% 二
	24	1 (FedAVg)			85.68 ± 0.1%	25%
	6	4 (FedLAMA)	一			87.45 ±0.05%	42.73% ―
Table 5: (Non-IID data) CIFAR-100 classification results. The number of workers is 128 and the
local batch size is 32 in all the experiments. The number of training iterations is 6, 000.
LR	Base aggregation interval: τ0	Interval increase factor: φ	active ratio	Dirichlet's coeff.	Validation acc.	Comm. cost
0.4	6	1 (FedAvg)	25%	0.1	79.15 ± 0.02%	100%	,
	12	1 (FedAVg)			76.16 ± 0.05%	50%
	6	2 (FedLAMA)	一			78.63 ±0.03%	-63.14%-
0.4	6	1 (FedAvg)	25%	0.5	78.81 ± 0.1%	100% 二
	12	1 (FedAVg)			76.11 ± 0.05%	50%
	6	2 (FedLAMA)	一			77.86 ±0.04%	-63.20%-
0.4	6	1 (FedAvg)	100%	0.1	79.77 ± 0.04%	100%	,
	12	1 (FedAVg)			77.71 ± 0.08%	50%
	6	2 (FedLAMA)	一			-79.07 ±0.1%-	-60.48%-
0.4	6	1 (FedAvg)	100%	0.5	80.19 ± 0.05%	100% 二
	12	1 (FedAVg)			77.40 ± 0.06%	50%
	6	2 (FedLAMA)	一			78.88 ±0.05%	61.73% ―
Table 4 and 5 show the non-IID CIFAR-10 and CIFAR-100 experimental results. We use Dirichlet’s
distribution to generate heterogeneous data across all the clients. The detailed settings regarding
Dirichlet’s distribution can be found in Appendix. The base aggregation interval τ0 is set to 6. The
interval increase factor φ is set to 2 for FedLAMA. Likely to the IID data experiments, we observe
that the periodic full averaging significantly loses the accuracy as the model aggregation interval
increases, while it has a proportionally reduced communication cost. For both datasets, FedLAMA
achieves a comparable accuracy to the periodic full averaging with the interval of τ0 while having
the communication cost that is close to the periodic full averaging with the increased interval of
φτ0. Especially, FedLAMA works effectively even when the Dirichlet’s coefficient is set to 0.1.
The coefficient of 0.1 represents an extremely high degree of data heterogeneity in terms of not
only the number of samples per client but also the balance of the classes assigned to each client.
These results imply that FedLAMA is a practical algorithm for Federated Learning applications
with highly heterogeneous data distributions.
8
Under review as a conference paper at ICLR 2022
1500
1000
500
1	3	5	7	9	11	13	15	17	19 21	23
layer ID
a) CIFAR-10 (ReSNet20)
FedAvg	FedLAMA
200
0
150
100
50
b) CIFAR-100 (WRN28-10)
ii
2	3
layer ID
c) FEMNIST (CNN)
1
4
Figure 2: The number of communications at the individual layers. The communications are counted
during the whole training (non-IID data).
I I FedAVg O FedLAMA
① Z-SsEP -ss
a) CIFAR-10 (ReSNet20)
b) CIFAR-100 (WRN28-10)
Figure 3: The total data size (communication cost) that correspond to Figure 2. The data size
comparison clearly shows where the performance gain OfFedLAMA comes from.
layer ID
c) FEMNIST (CNN)
6.2 Communication Efficiency Analysis
We analyze the total number of communications and the accumulated data size to evaluate the com-
munication efficiency of FedLAMA. Figure 2 shows the total number of communications at the
individual layers. The τ 0 is set to 6 and φ is 2 for FedLAMA. The key insight is that FedLAMA
increases the aggregation interval mostly at the output-side large layers. This means the dl value
shown in Equation (2) at the these layers are smaller than the others. since these large layers take
up most of the total model parameters, the communication cost is remarkably reduced when their
aggregation intervals are increased. Figure 3 shows the layer-wise local data size shown in Equation
9. FedLAMA shows the significantly smaller total data size than FedAvg. The extra computational
cost of FedLAMA is almost negligible since it calculates dl after each communication round only.
Therefore, given the virtually same computational cost, FedLAMA aggregates the local models at a
cheaper communication cost, and thus it improves the scalablity of Federated Learning.
We found that the amount of the reduced communication cost was not strongly affected by the
degree of data heterogeneity. As shown in Table 4 and 5, the reduced communication cost is similar
across different Dirichlet’s coefficients and device participation ratios. That is, FedLAMA can be
considered as an effective model aggregation scheme regardless of the degree of data heterogeneity.
7	Conclusion
We proposed a layer-wise model aggregation scheme that adaptively adjusts the model aggregation
interval at run-time. Breaking the convention of aggregating the whole model parameters at once,
this novel model aggregation scheme introduces a flexible communication strategy for scalable Fed-
erated Learning. Furthermore, we provide a solid convergence guarantee of FedLAMA under the
assumptions on the non-convex objective functions and the non-iiD data distribution. our empirical
study also demonstrates the efficacy of FedLAMA for scalable and accurate Federated Learning.
9
Under review as a conference paper at ICLR 2022
Harmonizing FedLAMA with other advanced optimizers, gradient compression, and low-rank ap-
proximation methods is a promising future work.
10
Under review as a conference paper at ICLR 2022
8	Code of Ethics
Our work does not deliver potentially harmful insights or conflicts of interests. We also do not find
any potential inappropriate application or privacy/security issues. The datasets we used in our study
are all public benchmark datasets, and our source code will be opened once the paper is accepted.
9	Reproducibility Statement
The software versions, implementation details, hyper-parameter settings can be found in the first
two paragraphs of Section 6. The entire source code used in our experiments will be published as
an open source once the paper is accepted. We believe one can exactly reproduce our experimental
results following the provided descriptions.
References
Alyazeed Albasyoni, Mher Safaryan, Laurent CondaL and Peter Richtarik. Optimal gradient Com-
pression for distributed and federated learning. arXiv preprint arXiv:2010.03246, 2020.
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. Qsgd:
Communication-efficient sgd via gradient quantization and encoding. Advances in Neural In-
formation Processing Systems, 30:1709-1720, 2017.
Dan Alistarh, Torsten Hoefler, Mikael Johansson, Sarit Khirirat, Nikola Konstantinov, and Cedric
Renggli. The convergence of sparsified gradient methods. arXiv preprint arXiv:1809.10505,
2018.
Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Freezeout: Accelerate training
by progressively freezing layers. arXiv preprint arXiv:1706.04983, 2017.
Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub KoneCny, H Brendan McMa-
han, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for federated settings. arXiv
preprint arXiv:1812.01097, 2018.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist
to handwritten letters. In 2017 International Joint Conference on Neural Networks (IJCNN), pp.
2921-2926. IEEE, 2017.
Enmao Diao, Jie Ding, and Vahid Tarokh. Heterofl: Computation and communication efficient
federated learning for heterogeneous clients. arXiv preprint arXiv:2010.01264, 2020.
Kelam Goutam, S Balasubramanian, Darshan Gera, and R Raghunatha Sarma. Layerout: Freezing
layers in deep neural networks. SN Computer Science, 1(5):1-9, 2020.
Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Scholkopf. Measuring statistical de-
pendence with hilbert-schmidt norms. In International conference on algorithmic learning theory,
pp. 63-77. Springer, 2005.
Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, and Viveck R Cadambe. Lo-
cal sgd with periodic averaging: Tighter analysis and adaptive synchronization. arXiv preprint
arXiv:1910.13598, 2019.
Chaoyang He, Songze Li, Jinhyun So, Xiao Zeng, Mi Zhang, Hongyi Wang, Xiaoyang Wang, Pra-
neeth Vepakomma, Abhishek Singh, Hang Qiu, et al. Fedml: A research library and benchmark
for federated machine learning. arXiv preprint arXiv:2007.13518, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
11
Under review as a conference paper at ICLR 2022
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In
International Conference on Machine Learning, pp. 5132-5143. PMLR, 2020.
Jakub Konecny, H Brendan McMahan, Felix X Yu, Peter Richtarik, Ananda Theertha Suresh, and
Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv
preprint arXiv:1610.05492, 2016.
Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural
network representations revisited. In International Conference on Machine Learning, pp. 3519-
3529. PMLR, 2019.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Adarsh Kumar, Arjun Balasubramanian, Shivaram Venkataraman, and Aditya Akella. Accelerat-
ing deep learning inference via freezing. In 11th {USENIX} Workshop on Hot Topics in Cloud
Computing (HotCloud 19), 2019.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 2018.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial intelli-
gence and statistics, pp. 1273-1282. PMLR, 2017.
Ari S Morcos, Maithra Raghu, and Samy Bengio. Insights on representational similarity in neural
networks with canonical correlation. arXiv preprint arXiv:1806.05759, 2018.
Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. Svcca: Singular vector
canonical correlation analysis for deep learning dynamics and interpretability. arXiv preprint
arXiv:1706.05806, 2017.
Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Ramtin Pedarsani.
Fedpaq: A communication-efficient federated learning method with periodic averaging and quan-
tization. In International Conference on Artificial Intelligence and Statistics, pp. 2021-2031.
PMLR, 2020.
Thijs Vogels, Sai Praneeth Karimireddy, and Martin Jaggi. Practical low-rank communication com-
pression in decentralized deep learning. In NeurIPS, 2020.
Hongyi Wang, Scott Sievert, Zachary Charles, Shengchao Liu, Stephen Wright, and Dimitris Pa-
pailiopoulos. Atomo: Communication-efficient learning via atomic sparsification. arXiv preprint
arXiv:1806.04090, 2018.
Hongyi Wang, Saurabh Agarwal, and Dimitris Papailiopoulos. Pufferfish: Communication-efficient
models at no extra cost. arXiv preprint arXiv:2103.03936, 2021.
Jianyu Wang and Gauri Joshi. Adaptive communication strategies to achieve the best error-runtime
trade-off in local-update sgd. arXiv preprint arXiv:1810.08313, 2018.
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective in-
consistency problem in heterogeneous federated optimization. arXiv preprint arXiv:2007.07481,
2020.
Jianqiao Wangni, Jialei Wang, Ji Liu, and Tong Zhang. Gradient sparsification for communication-
efficient distributed optimization. arXiv preprint arXiv:1710.09854, 2017.
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Tern-
grad: Ternary gradients to reduce communication in distributed deep learning. arXiv preprint
arXiv:1705.07878, 2017.
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan
Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep
learning: Training bert in 76 minutes. arXiv preprint arXiv:1904.00962, 2019.
12
Under review as a conference paper at ICLR 2022
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint
arXiv:1605.07146, 2016.
Minjia Zhang and Yuxiong He. Accelerating training of transformer-based language models with
progressive layer dropping. arXiv preprint arXiv:2010.13369, 2020.
13
Under review as a conference paper at ICLR 2022
A Appendix
A. 1 Convergence Analysis
Herein, we provide the proofs of the lemmas and theorem shown in Section 5.
A.1.1 Preliminaries
FedLAMA periodically chooses a few layers that will be less frequently synchronized. We call these
layers Least Critical Layers (LCL) for short.
Notations - All vectors in this paper are column vectors. X ∈ Rd denotes the parameters of one
local model and m is the number of workers. The stochastic gradient computed from a single
training data point ξ is denoted by g(X, ξ). For convenience, we use g(X) instead. The full batch
gradient is denoted by VF(x). We use ∣∣ ∙ ∣∣ and ∣∣ ∙ ∣∣op to denote 12 norm and matrix operator norm,
respectively.
Objective Function - In this paper, we consider federated optimization problems as follows.
min
x∈Rd
m
F(X) := XpiFi(X)
i=1
(10)
where Pi = njn is the ratio of local data to the total dataset, and Fi(X) =; Pξ∈D fi(x,ξ) is the
local objective function of client i. n is the global dataset size and ni is the local dataset size. Note
that, by definition, Pim=1 pi = 1.
Averaging Matrix - We define a time-varying averaging matrix Wk ∈ Rdm×dm as follows.
Wk=F
if k mod τmin is 0
if k mod τmax is 0
otherwise
(11)
I is an identity matrix, P is also a time-varying averaging matrix, and J is a full averaging matrix.
First, Pi1 is a d × d diagonal matrix that has 1 for the diagonal elements that correspond to the LCL
parameters and pi for all the other diagonal elements. Likewise, Pi0 is another d × d diagonal matrix
that has 0 for the diagonal elements that correspond to the LCL parameters and pi for all the other
diagonal elements. Then, P is defined as follows.
P = P1, for m diagonal blocks
P0, for all the other blocks
(12)
The ith block column of P consists of Pi1 and Pi0 following the above definition.
Here we present an example ofP where m = 2 and d = 2. In this example, p0 = 1/3 and p1 = 2/3.
Saying the LCL is the second parameter, P is defined as follows.
P10
00
2-3o
=
0-
P
,
O01
2-3o
=
--
P
,
O00
l-3o
=
00
P
,
O01
l-3o
-
P10
P00
P01
P11
0 3 0]
100
0 3 0
001
(13)
(14)
P
The full-averaging matrix J is defined as follows. First, Ji is a d × d diagonal matrix that has pi for
the diagonal elements. Then, J consists of m × m blocks of Ji such that each column block is m of
Ji blocks. Here we present an example of J where m = 2 and d = 2 as follows.
J0
O 2-3
2-3o
=
1
J
,
O 1-3
l-3o
- 」
(15)
14
Under review as a conference paper at ICLR 2022
J
J0	J1
J0	J1
O 2-30 2-3
2-30 2-30
O 1-30 1-3
1-30 1-30
(16)
The averaging matrix P and J have the following properties:
1.
P1dm
1dm ,
J1dm
1dm .
2.	The product of any two averaging matrices consists only of diagonal block matrices because
all the blocks in P and J are diagonal.
3.	PJ = JP = J regardless of which layers are chosen as the LCL.
4.	PP = P regardless of which layers are chosen as the LCL.
Vectorization - We define a vectorized form of m local model parameters Xk ∈ Rdm, its stochastic
gradients gk ∈ Rdm , and the full gradients fk ∈ Rdm as follows
Xk = Vec {χk, Xk,…，Xm}
gk = Vec{gι(xk),g2(xk),…，gm(xm)}	(17)
fk = Vec {VF1(xk), VF2(Xk),…，VFm(Xm)}.
The full model aggregation can be written using the vectorized form of local models Xk and the
averaging matrix J as follows.
JXk
O 2-30 2-3
2-30 2-30
O 1-30 1-3
1-30 1-30
I I
(1,1)
xk
丁(1,2)
Xk
τ(2,1)
xk
(2,2)
xk
Γ(χk1,1)
(xk1,2)
(xk1,1)
(x(k1,2)
+ 2xk2,1)"3]
+ 2xk2,2))∕3
+ 2xk2,1))∕3
+ 2x(k2,2))/3
(18)
where x(ki,j) is the jth model parameter of local model i at iteration k.
We also define the following additional vectorized forms of the weighted model parameters and
gradients for convenience.
Xk = VeC {√pιxk, √p2Xk,…，√pmxm}
gk = VeC {√P1g1(xk), √P2g2(xk),…，√pmgm(xm)}	(19)
fk = vec {√p1VFι(xk), √p2VF2(xk),…，√pmVFm(xm)}
Assumptions - We analyze the convergence rate of FedLAMA under the following assumptions.
1.	(Smoothness). Each local objective function is L-smooth, that is, kVFi(x) - VFi (y)k ≤
LkX - yk,∀i ∈ {1,…，m}.
2.	(Unbiased Gradient). The stochastic gradient at each client is an unbiased estimator of the
local full-batch gradient: Eξ [gi (x, ξ)] = VFi (x).
3.	(Bounded Variance). The stochastic gradient at each client has bounded variance:
Eξ[kgi(x,ξ) — VFi(x)k2 ≤ σ2],∀i ∈ {1,…，m},σ2 ≥ 0.
4.	(Bounded Dissimilarity). For any sets of weights {pi ≥ 0}im=1, Pim=1 pi = 1, there exist
constants β2 ≥ 1 and κ2 ≥ 0 such that Pim=1 pikVFi(x)k2 ≤ β2k Pim=1 piVFi(x)k2+κ2.
If local objective functions are identical to each other, β2 = 1 and κ2 = 0.
15
Under review as a conference paper at ICLR 2022
A.1.2 Proofs
Theorem 5.1. Suppose all m local models are initialized to the same point u1. Under As-
sumption 1 〜 4, f FedLAMA runs for K iterations and the learning rate satisfies
1
η≤
min
_____1
2(τmax
T)L , L√2Tmaχ(Tmaχ-1)(2β2 + 1)
, FedLAMA ensures
1K
E KK EkVF(uk)k
i=1
m
≤ κ( (e [F(UI)- F(u*用 +4ηXp2Lσ2
η	i=1
+ 3η2(τmax-1)L2σ2 + 6η2τmax(τmax-1)L2κ2,
where u* indicates a local minimum.
(20)
Proof. Based on Lemma 5.1 and 5.2, we have
1K	2
κ X EuVF (uk)k ] ≤ ηκ
m
(E[F (UI)- F (UJD+ 2η X p2Lσ2
+ L2
η2(τmax - 1)σj2
i=1
Aβ2
1-A
KL2(1 - A)
K
XE kVF(uk)k2 +
k=1
Aκ2	∖
L2(1- A)'
K
2
+
After re-writing the left-hand side and a minor rearrangement, we have
1K	2	m
K ∑E [kVF(Uk)k2] ≤ ηKK (E [F(ui) - F(u*)]) + 2*p2Lσ2
+ K X I-A EhkVF(Uk)k2i
k=1
l r2 η η2(τmax - 1)σ2 ,	Aκ2	∖
+ L 1 -Γ-Λ — + L2(1- A)).
By moving the third term on the right-hand side to the left-hand side, we have
K X (1- T-A) EhkVjF (Uk )k2i≤ ηK
k=1
m
(E [F(UI)- F(UJD+ 2ηXp2Lσ2
2 η2 (τmax - 1)σ
+	I —U-
i=1
2 Aκ2 A
^+ L2(1- A) J
(21)
If A ≤ 2.1+1, then AeA ≤ 2. Therefore, (21) can be simplified as follows.
1K
K XEuVF(Uk)k2]
m
≤	(E[F(UI)- F(UJD + 4ηXp2Lσ
ηK	i=1 i
Aκ2
(22)
+ 2L2
η2(Tmaχ — 1)σ2
1 - A
+ 21-Λ.
2
The learning rate condition A ≤ 2e1+1
also ensures that ɪ-A ≤ 1 + 壶.Based on Assumption 4,
表 ≤ 3,andthus 1-A ≤ 3. Therefore, we have
Km
K X EUVF(Uk)k2] ≤ ηKK (E [F(ui) - F(u*)]) + 4η Xp2Lσ2
+ 3η2(τmax - 1)L2σ2 + 6η2 τmax (τmax - 1)L2κ2.
We complete the proof.
□
16
Under review as a conference paper at ICLR 2022
Learning Rate Constraints - In Theorem 5.3, We have two learning rate constraints, one from (22)
and the other from (51) as follows.
A< 2β⅛	from(22)
A < 1 from (51)
After a minor rearrangement, we have a unified learning rate constraint as follows.
η ≤ min
1	1
2(Tmax - I)L Lp2τmaχ(Tmax - 1)(2/2 + 1)
)
Lemma 5.1. (Framework) UnderAssumption 1 〜3, ifthe learning rate satisfies η ≤ 克,FedLAMA
ensures
1K	2	m
K ∑E [kVF(Uk)k2] ≤ — E [F(uι) - F(u*)] + 2ηLσ? ^(Pi)2
2K m
+K XX pi EhIlUk-Xk『].
(23)
Proof. Based on Assumption 1, we have
E [F (Uk+1) - F(Uk)] ≤ -ηE
J
m	η2L
WFUk), Epigi(Xk )i +	2 E
{	/	J
Ti
(24)
m
Xpigi (Xik)
i=1
_____ - /
{^^^^^^^^^^^^
T2
First, T1 can be rewritten as follows.
m
Ti= E NF(Uk), Xpi (gi(Xk)-VFi(Xk))〉
i=1
m
+E hVF(Uk),XpiVFi(Xik)i
i=i
m
E hVF(Uk),XpiVFi(Xik)i
i=i
XPiVFi(Xk)| j — 1 E [vf(Uk) - XPiVFi(Xk)
=	=	(25)
where the last equality holds based on a basic equality: 2a>b = kak2 +kbk2-ka-bk2.
2 kVF(Uk)k2 + 1E
Then, T2 can be bounded as follows.
|m	m
T2 = E I ∣XPi (gi(Xk) - E [gi(Xk)]) + XPi E [gi(Xk)]
|m	m
E I IXPi (gi(Xk) - VFi(Xk)) + XPiVFi(Xik)
≤ 2E ]|XPi (gi(Xk) -VFi(Xk))| j +2E ||XPiVFi(Xk)
m
=2XPi2E ||gi(Xik)-VFi(Xik)||2 +2E
i=i
≤ 2σ2 Xp2 + 2E ]∣XPiVFi(Xk)|]
||||Xm PiVFi(Xik)
(26)
17
Under review as a conference paper at ICLR 2022
where the last equality holds because gi(xk) - NFi(Xk) has 0 mean and is independent across i,
and the last inequality follows Assumption 3.
By plugging in (25) and (26) into (24), we have the following.
2
m
E [F(uk+ι) - F(Uk)] ≤ -2 kVF(Uk)k2 - 2 E WXPiVFi(Xk)
i=1
i
m
m
+ 2 E MVF (Uk)- X PiVFi (xk “| + η2Lσ2 X Pi
i=1
+ η2L EhX PNFi(Xik )11
i=1
2
m
-2 kVF(Uk)k2 - 2(1 - 2ηL) E WXPNFi(Xk)
m
i=1
2
m
+ 2 E MVF (Uk)- X PiVFi (xk “| + η2Lσ2 X pi
i=1
i=1
If η ≤ 2L, it follows
E [F(Uk+1) - F(Uk)]
≤-2 kVF(Uk)k2
m
+ ηLσi X Pii
i=1
m
VF(Uk) -	PiVFi(Xik)
i=1
1m
≤-2 kVF(Uk)k2 + ηLσ2 XPI
m
+ 2 XPi E[∣∣VFi(Uk) -VFi(xk)『]
i=1
m Li m
≤ -2 IIvf(Uk)k + ηLσ	XPi + ɪXPiE [∣∣Uk-χk∣∣ i,
i=1	i=1
(27)
η
i
+ 2 E
where (27) holds based on the convexity of `2 norm and Jensen’s inequality.
By taking expectation and averaging across K iterations, we have.
1 E [F(Uk+1)- F(Uk )]
K S η
Km
≤-而 X kVF(Uk)k2 + ηLσ2 XPI
2K
k=1	i=1
+
Pi E
k-1 i=1
Uk
18
Under review as a conference paper at ICLR 2022
After a minor rearrangement, we have a telescoping sum as follows.
1K	2	m
IK ∑E [kVF(Uk)k2] ≤ ηKK E [F(Ui) - F(uk+ι)] + 2ηLσ2 >
2K m
+ K XX pi EhIlUk-Xk『]
m
≤ KE E [F(UI)- F(U*)] + 2ηLσ2 Xp2
η	i=1
2K m
+ EXXpiE h∣∣uk -xk『],
where u* indicates the local minimum. Here, We complete the proof.
□
Lemma 5.2. (Model Discrepancy) Under Assumption 1 〜 4, if the learning rate satisfies η <
2(T^^1 —i)L, FedLAMA ensures
1 GG	Bl^∣∣	i ∣∣2] / 2η2 (Tmax - I)σ2 l	Aκ2
K∑∑piEWUk-XkIl ] ≤ —口一 + E-0
Aβ2	K
+ KLAβ-A) X e"f(Uk)k2]，
where A = 4η2(τmax - 1)2L2 and τmax is the largest averaging interval across all the layers.
(28)
Proof. We begin with rewriting the weighted average of the squared distance using the vectorized
form of the local models as follows.
mm
X pi IlUk-Xk∣∣2 = X∣∣√pi (Uk-Xk )∣∣2
i=1	i=1
=kJXk - Xkk2	(29)
=k(J - I)^k k2，
where (29) holds by the commutative property of multiplication.
Then, according to the parameter update rule, we have
(J - I)Xk = (J - I)Wk-1(^k-1 - ηgk-1)
=(J - I)Wk-1Xk-i -(J - Wk-I)ηgk-i,	(3O)
where (30) holds because JW = J based on the averaging matrix property 3, and IW = W.
Then, expanding the expression of Xk-1, we have
(J - I)Xk = (J - I)Wk-1(Wk-2(Xk-2 - ηgk-2)) - (J - Wk-i)ηgk-i
=(J - I)Wk-1Wk-2Xk-2 - (J - Wk-1Wk-2)ηgk-2 - (J - Wk-1 )ηgk-1.
Repeating the same procedure for Xk-2, Xk-3, •…，X2,we have
k-1	k-1	k-1
(J - I)^k = (J - I) Y WsX1 - η X(J - Y Wι)gs
s=1	s=1	l=s
k-1	k-1
=-η X(J - Y Wι)^s,	(31)
s=1	l=s
where (31) holds because x1 is the same across all the workers and thus (J - I)X1 = 0.
19
Under review as a conference paper at ICLR 2022
Based on (31), we have
K
KK X(Ehk(J - Dxk k2i)
k=1
k-1	k-1
X(J - Y Wι)^s
s=1	l=s
K
X E
k=1
∖
k-1	k-1	k-1	k-1
X(J	-	Y WMgs-	fs)	+	X(J	-	Y Wι)fs
s=1	ι=s	s=1	ι=s
V 2η2
≤ ---
[∣∣k-1	k-1
IX(J - Y WMgs- fs)
∣s=1	ι=s
_ - /
{z
T3
K	∣k-1	k-1
+XE ∣∣∣X(J - Y Wifs
k=1	∣s=1	ι=s
S-------------7-----------
T4
(32)
K
where (32) holds based on the convexity of `2 norm and Jensen’s inequality. Now, we focus on
bounding T3 and T4 , separately.
Bounding T3
XE ]|X(J - ∏ Wι)(gs - fs)∣2j
K k-1	「|| k-1
=XXE ∣∣(j - Y Wι)(^s - fs)
k=1 s=1	∣	ι=s
K k-1	「
≤ XX E Ms- W
k=1 s=1
k-1
(J-YWι)
ι=s
(33)
(34)
op
2
2
where (33) holds because gs - fs has 0 mean and independent across s, and (34) holds based on
Lemma A.1.
20
Under review as a conference paper at ICLR 2022
Without loss of generality, we replace k with aτmax + b, where a is the communication round index
and b is the iteration index within each communication round. Then, we have
K∕τmax - 1 Tmax aτmax+b- 1	2
xxx E Ms- Wl
a=0	b=1	s=1
K/Tmax - 1 Tmax aτ	2
x	xxEMs-训∣
a=0	b=1 s=1
k-1
(J-YWl)
l=s
op
aTmax +b-1
(J- Y Wl)
l=s
K/Tmax -1 Tmax aTmax +b-1	2
+ xxx E Ms- W
a=0	b=1 s=aTmax +1
K/Tmax -1 Tmax aTmax +b-1
xxxE
a=0	b=1 s=aTmax+1
aTmax +b-1
(J - Y Wl)
l=s
op
aTmax +b-1
(J- Y Wl)
l=s
op
op
K/Tmax -1 Tmax aT +b-1
xxxE
a=0	b=1 s=aTmax +1
K/Tmax -1 Tmax aTmax +b-1 m
XXX	XPiEh∣∣(gi(χS)-vFiX))∣∣2i
a=0	b=1 s=aTmax+1 i=1
K/Tmax -1 Tmax aTmax +b-1 m
≤ x x x xpiσ2
a=0 b=1 s=aTmax+1 i=1
K/Tmax -1 Tmax
x	x(b-1)σ2
a=0	b=1
K/Tmax -1
x
a=0
τmax (τmax - 1) 2
2 σ
≤ κ (Tmax - I) σ2
≤	2
(35)
(36)
(37)
(38)
2
2
2
2
Remember FedLAMA synchronizes the whole parameters at least once after every τmax iterations.
Thus, (35) holds because Qla=Tms ax+b-1 Wl is J when s ≤ aτmax, and thus J - Qla=Tms ax+b-1 Wl
becomes 0. (36) holds based on Lemma A.2. (37) holds based on Assumption 3.
Bounding T4
21
Under review as a conference paper at ICLR 2022
K-Tmax	「||k-1	k-1	"2
x E IiX(J - Y WT
k=1	"s=1	l=s	"
2
≤
≤
≤
≤
≤
K/Tmax -1 Tmax
X XE
a=0
b=1
aT +b-1
X (J-
s=1
aTmax +b-1
Y	Wlfs
l=s
K/Tmax -1 Tmax
X XE
aτmax +b-1
a=0
b=1
(J-
s=aTmax +1
aTmax +b-1
Y
l=s
^
Pl)fs
K/Tmax -1 Tmax
a=0
b=1
(b-1)
aτmax +b-1
(J
s=aτmax +1
aTmax +b-1
Y
l=s
^
Pl)fs
K/Tmax -1 Tmax
a=0
b=1
(b-1)
aτmax +b-1
s=aτmax +1
K/Tmax -1 Tmax
a=0
b=1
(b-1)
aτmax +b-1
s=aτmax +1
1
τmax (τmax - 1)
KlTmax - 1
X
a=0
aTmax +Tmax
X
s=aτmax +1
τmax (τmax - 1)
K
XE ifk
k=1
Km
2 mac	1) XXPiE [∣∣VFi(xk)∣∣2i,
k=1 i=1
(J-
aTmax +b-1
Y
l=s
Pl)
op
(39)
(40)
(41)
(42)
(43)
E
E
E
^

—
2
2
2
2
2
2
2
2
2
E
where (39) holds because J - Qla=Tms ax+b-1 Pl becomes 0 when s ≤ aτ. (40) holds based on the
convexity of`2 norm and Jensen’s inequality. (41) holds based on Lemma A.1. (42) holds based on
Lemma A.2.
Final Result
By plugging in (38) and (43) into (32), we have
Km
K XXpi E
k=1 i=1
≤ 誓卜(Tma2- 1)σ2 + TmuTmax- 1) (XXPi E h∣VFi(xk)∣∣20)
=η2(τmaχ - 1)σ2 + η2Tmak- 1) (X Xpi E h∣∣VFi(xk)∣∣2i)
(44)
The local gradient term on the right-hand side in (44) can be rewritten using the following inequality.
Eh∣∣VFi(xik)∣∣2i =Eh∣∣VFi(xik)-VFi(uk)+VFi(uk)∣∣2i
≤ 2E h∣∣VFi(xik) - VFi(uk)∣∣2i +2E hkVFi(uk)k2i	(45)
≤ 2L2 E h∣∣uk -xik∣∣2i +2EhkVFi(uk)k2i ,	(46)
22
Under review as a conference paper at ICLR 2022
where (45) holds based on the convexity of `2 norm and Jensen’s inequality.
Plugging in (46) into (44), we have
Km
⅛ XX Pi Eh辰-Xk『i
k=1 i=1
≤ η2(τmax - 1)σ2 + 2η2τmax(Tmax- I)L2 X XPi EhM-Xk『］
k=1 i=1
+ 2η⅛φαχ-J) X X pi EhkVFi(Uk )k2i	(47)
k=1 i=1
After a minor rearranging, we have
⅛ X X Pi EhM-Xk U2i≤ 1- Jτ± 1)L2
+N -m2)XXPiEhkVFi(Uk)k2i
(48)
Let us define A = 2η2 τmax (τmax - 1)L2. Then (48) is simplified as follows.
Km
K XX Pi EhM-Xk『］
k=1 i=1
≤ η2(τma- A1"2 + KL⅛Λ) X X Pi EhkVFi(Uk)k2i
k=1 i=1
Based on Assumption 4, we have
Km
K XX Pi EhM-Xk『］
k=1 i=1
≤ η2(Tmax - 1)σ2 +
-	1 - A +
Aβ2
KL2 (1 - A)
η2(Tmax - 1)σ2	Aβ2
1 - A	+ KL2(1 - A)
E	PiVFi (Uk)
k=1 i=1
K
XE kVF(Uk)k2 +
k=1
2I	Aκ2
I + L2(1- A)
Aκ2
L2(1 - A),
(49)
(50)
K
m
where (50) holds based on the definition of the objective function (10).
Note that (49) is true only when 1 - A > 0. Thus, after a minor rearrangement, we have a learning
rate constraint as follows.
1
η <  ----------
2(τmax - 1)L
(51)
Here, we complete the proof.
□
A.1.3 Proof of Other Lemmas
Lemma A.1. Consider a real matrix A ∈ Rmdj ×mdj and a real vector b ∈ Rmdj. If b 6= 0mdj,
we have
kAbk ≤ kAkopkbk	(52)
23
Under review as a conference paper at ICLR 2022
Proof.
kAbk2=kAbbkf kbk2
≤ kAko2pkbk2	(53)
where (53) holds based on the definition of operator norm.	□
Lemma A.2. Suppose an md × md averaging matrix P and the full-averaging matrix J, then
kJ-Pko2p= 1.	(54)
regardless of which layers are chosen as the LCL.
Proof. First, by the definition of averaging matrix P, all the columns that do not correspond to the
LCL are zeroed out in J - P. Then, based on the averaging matrix property 1 and 2, the remaining
columns in P has 1 at all different rows. By the definition of J, all the non-zero elements in ith
column are the same pi,i ∈ {1,…，m}. Consequently, the remaining columns in J - P are
always orthogonal regardless of which layers are chosen as the LCL, and thus the eigenvalues of
J - P are either 1 or -1. Finally, by the definition of the matrix operator norm, kJ - Pko2p =
max{∣λ(J - P) ∣} = 1, where λ(∙) indicates the eigenvalues of the input matrix.	□
24
Under review as a conference paper at ICLR 2022

2'"151'"
(XelUUoS) SSo- 6u,ll
(XelUUoS) SSo- 6u=i,ll
100	200	300
1.0
(％) AOeJnae uo-p->
100	200
(％> AOeJnae uo-sp-i>
300
3"
Epoch	Epoch
a) CIFAR-10 IID setting learning curves
(XelUUoS) SSo- 6U-U-E1
0	2000	4000	6000
Iteration
(XelUUoS) SSo- 6U-U-E1
2.0
1.5
1.0
0
2.5
2.0
1.5
1.0
0
2000	4000	6000
Iteration
(％) AOEnae uo-p->
2000	4000
Iteration
6000
b) CIFAR-10 non-IID setting learning curves
Figure 4: The learning curves of CIFAR-10 (ResNet20) training (128 clients). a): The curves for
IID data distribution. b): The curves for non-IID data distribution (α = 0.1). FedAvg (x) indicates
FedAvg with the interval of x. FedLAMA (x, y) indicates FedLAMA with the base interval of x
and the interval increase factor ofy. As the aggregation interval increases, FedAvg rapidly loses the
convergence speed, and it results in achieving a lower validation accuracy within the fixed iteration
budget. In contrast, FedLAMA effectively increases the aggregation interval while maintaining the
convergence speed.
(XeEUOS) ss。- 6u-ui-ll
(XeEUOS) ss。- 6u-ui-ll
100	200
Epoch
a) CIFAR-100
FedAVg ⑹
FedAVg (12)
——FedAvg (24：
(％) Aoe-Inooe uo-ep-i>
100
Epoch
IID setting learning curves
(％) AOeJnOOe uo-ep=e>
(XeEUOS) ss。- 6u=-e-ll
0	2000	4000	6000
Iteration
(XeEUOS) ss。- 6u=-e-ll
2000	4000
Iteration
b) CIFAR-100
6000
0.2
0
0
0.8
0.6
0.4
(％) AOeJnOOe uo-ep-i>
2000
4000
Iteration
(％) AOeJnOOe uo-ep-i>
2000	4000	6000
Iteration
non-IID setting learning curves

a

Figure 5: The learning curves of CIFAR-100 (WideResNet28-10) training (128 clients). a): The
curves for IID data distribution. b): The curves for non-IID data distribution (α = 0.1). FedAvg
(x) indicates FedAvg with the interval of x. FedLAMA (x, y) indicates FedLAMA with the base
interval of x and the interval increase factor ofy. While FedAvg significantly loses the convergence
speed as the aggregation interval increases, FedLAMA has a marginl impact on it which results in a
higher validation accuracy.
A.2 Additional Experimental Results
In this section, we provide extra experimental results with extensive hyper-parameter settings. We
commonly use 128 clients and a local batch size of 32 in all the experiments. The gradual learning
rate warmup (Goyal et al. (2017)) is also applied to the first 10 epochs in all the experiments. Overall,
the learning curve charts and the validation accuracy tables deliver the key insight that FedLAMA
achieves a comparable convergence speed to the periodic full aggregation with the base interval
25
Under review as a conference paper at ICLR 2022
O
(XelUUOS) SSO- 6U-U-B1
O
(XeiUUOs) SSo- 6ue,ll
O
8 6 4 2
。 。 。 。
(％) Aɔe,inɔɔe Uo-p=e>
S 6 4 20
(％) Aɔe,inɔɔe Uo-p=e>
Iteration	Iteration	Iteration	Iteration
Figure 6: The learning curves of FEMNIST (CNN) training. FedAvg (x) indicates FedAvg with
the interval of x. FedLAMA (x, y) indicates FedLAMA with the base interval of X and the interval
increase factor of y. FedLAMA curves are not strongly affected by the increased aggregation interval
while FedAvg significantly loses the convergence speed as well as the validation accuracy.
(τ,) while having the communication cost that is similar to the periodic full aggregation with the
increased interval (φτ0).
Artificial Data Heterogeneity 一 For CIFAR-10 and CIFAR-100, we artificially generate the het-
erogeneous data distribution using Dirichlet,s distribution. The concentration coefficient α is set to
0.1, 0.5, and 1.0 to evaluate the performance of FedLAMA across a variety of degree of data hetero-
geneity. Note that the small concentration coefficient represents the highly heterogeneous numbers
of local samples across clients as well as the balance of the samples across the labels. We used the
data distribution source code provided by FedML (He et al. (2020)).
CIFAR-10 - Figure 4 shows the full learning curves for IID and non-IID CIFAR-10 datasets. The
hyper-parameter settings correspond to Table 4 and 1. First, as the aggregation interval increases
from 6 to 24, FedAvg suffers from the slower convergence, and it results in achieving a lower val-
idation accuracy, regardless of the data distribution. In contrast, FedLAMA learning curves are
marginally affected by the increased aggregation interval. Table 6 and 7 show the CIFAR-10 classi-
fication performance of FedLAMA across different φ settings. As expected, the accuracy is reduced
as φ increases. The IID and non-IID data settings show the common trend. Depending on the system
network bandwidth, φ can be tuned to be an appropriate value. When φ = 2, the accuracy is almost
the same as or even slightly higher than FedAvg accuracy. If the network bandwidth is limited, one
can increase φ and slightly increase the epoch budget to achieve a good accuracy. Table 8 shows the
CIFAR-10 accuracy across different τ0 settings. We see that the accuracy is significantly dropped as
τ0 increases.
CIFAR-100 - Figure 5 shows the learning curves for IID and non-IID CIFAR-100 datasets. Likely
to CIFAR-10 results, FedAvg learning curves are strongly affected as the aggregation interval in-
creases from 6 to 24 while FedLAMA learning curves are not strongly affected. Table 9 and 10
show the CIFAR-100 classification performance of FedLAMA across different φ settings. Fed-
LAMA achieves a comparable accuracy to FedAvg with a short aggregation interval, even when
the degree of data heterogeneity is extreamly high (25% device sampling and Direchlet’s coefficient
of 0.1). Table 11 shows the FedAvg accuracy with different τ0 settings. Under the strongly het-
erogeneous data distributions, FedAvg with a large aggregation interval (τ ≥ 12) do not achieve a
reasonable accuracy.
FEMNIST - Figure 6 shows the learning curves of CNN training. Likely to the previous two
datasets, the periodic full aggregation suffers from the slower convergence as the aggregation in-
terval increases. FedLAMA learning curves are not much affected by the increased aggregation
interval, and it results in achieving a higher validation accuracy after the same number of iterations.
Table 12 shows the FEMNIST classification performance of FedLAMA across different φ settings.
FedLAMA achieves a similar accuracy to the baseline (FedAvg with τ0 = 10) even when using a
large interval increase factor φ ≥ 4. These results demonstrate the effectiveness of the proposed
layer-wise adaptive model aggregation method on the problems with heterogeneous data distribu-
tions.
26
Under review as a conference paper at ICLR 2022
Table 6: (IID data) CIFAR-10 classification results of FedLAMA with different φ settings.
# of clients	Local batch size	LR	Averaging interval: τ0	Interval increase factor: φ	Validation acc.
128	32	0.8	6	1 (FedAvg)	88.37 ± 0.1%
		0.5		2	88.41 ± 0.04%
				4	86.33 ± 0.2%
				8	.	85.08 ± 0.04%
Table 7: (Non-IID data) CIFAR-10 classification results of FedLAMA with different φ settings.
# of clients	Local batch size	LR	T0-	Active ratio	Dirichlet coeff.	φ	Validation acc.
128	32	0.8	6	100%	1	1 (FedAvg)	90.79 ± 0.1%
						2	89.01 ± 0.04%
						4	~	87.84 ± 0.01%
				100%	0.5	1 (FedAVg)	90.53 ± 0.18%
						2	89.21 ± 0.2%
						4	86.68 ± 0.12%
				100%	0.1	1 (FedAVg)	89.52 ± 0.11%
						2	89.00 ± 0.1%
						4	84.82 ± 0.08%
				50%	1	1 (FedAVg)	90.34 ± 0.12%
						2	89.56 ± 0.13%
						4	87.48 ± 0.21%
				50%	0.5	1 (FedAVg)	89.86 ± 0.13%
						2	88.44 ± 0.15%
						4	87.29 ± 0.18%
				50%	0.1	1 (FedAVg)	87.83 ± 0.2%
						2	87.40 ± 0.17%
						4	~	85.92 ± 0.21%
		0.6		25%	1	1 (FedAVg)	88.97 ± 0.03%
						2	87.89 ± 0.2%
						4	86.61 ± 0.1%
				25%	0.5	1 (FedAVg)	87.59 ± 0.05%
						2	87.12 ± 0.08%
						4	86.57 ± 0.02%
		0.3		25%	0.1	1 (FedAVg)	84.02 ± 0.04%
						2	83.55 ± 0.02%
						4	一	83.06 ± 0.03%
Table 8: (Non-IID data) CIFAR-10 classification results of FedAvg with different T0 settings.
# of clients	Local batch size	LR	-T0-	Active ratio	Dirichlet coeff.	φ	Validation acc.
128	32	0.8	~6~	100%	0.1	1 (FedAvg)	89.52 ± 0.11%
			~2T~			1 (FedAvg)	87.29 ± 0.05%
			~4A~			1 (FedAvg)	84.82 ± 0.1%
128	32	0.3	~6~	25%	0.1	1 (FedAvg)	84.02 ± 0.1%
			~2τΓ			1 (FedAvg)	82.48 ± 0.2%
			~24~			1 (FedAvg)	76.72 ± 0.1%
Table 9: (IID data) CIFAR-100 classification results of FedLAMA with different φ settings.
# of clients	Local batch size	LR	Averaging interval: τ0	Interval increase factor: φ	Validation acc.
128	32	0.6	6	1 (FedAvg)	76.50 ± 0.02%
				2	75.99 ± 0.03%
				4	76.17 ± 0.2%
				8	.	76.15 ± 0.2%
27
Under review as a conference paper at ICLR 2022
Table 10: (Non-IID data) CIFAR-100 classification results of FedLAMA with different φ settings.
# of clients	Local batch size	LR	T0-	Active ratio	Dirichlet coeff.	φ	Validation acc.
128	32	0.4	6	100%	1	1 (FedAvg)	80.34 ± 0.01%
						2	78.92 ± 0.01%
						4	77.16 ± 0.05%
				100%	0.5	1 (FedAVg)	80.19 ± 0.02%
						2	78.88 ± 0.1%
						4	~	78.03 ± 0.08%
		0.2		100%	0.1	1 (FedAVg)	79.78 ± 0.02%
						2	79.07 ± 0.02%
						4	79.32 ± 0.01%
		0.4		50%	1	1 (FedAVg)	79.94 ± 0.1%
						2	78.98 ± 0.01%
						4	77.50 ± 0.02%
				50%	0.5	1 (FedAVg)	79.95 ± 0.05%
						2	78.37 ± 0.05%
						4	76.93 ± 0.1%
		0.2		50%	0.1	1 (FedAVg)	79.62 ± 0.06%
						2	78.76 ± 0.02%
						4	77.44 ± 0.02%
		0.4		25%	1	1 (FedAVg)	78.78 ± 0.02%
						2	78.10 ± 0.02%
		0.2				4	76.84 ± 0.03%
		0.4		25%	0.5	1 (FedAVg)	78.81 ± 0.01%
						2	77.86 ± 0.04%
						4	77.01 ± 0.1%
				25%	0.1	1 (FedAVg)	79.06 ± 0.03%
						2	78.63 ± 0.02%
		0.2				4	一	77.17 ± 0.01%
Table 11: (Non-IID data) CIFAR-100 classification results of FedAvg with different T0 settings.
# of clients	Local batch size	LR	-τ0-	Active ratio	Dirichlet coeff.	φ	Validation acc.
128	32	0.4	~6~	100%	0.1	1 (FedAvg)	79.78 ± 0.02%
			~2τΓ			1 (FedAvg)	77.71 ± 0.1%
			~4A~			1 (FedAvg)	69.63 ± 0.1%
128	32	0.4	~6~	25%	0.1	1 (FedAvg)	79.06 ± 0.03%
			~2T~			1 (FedAvg)	76.16 ± 0.05%
			~24~			1 (FedAvg)	67.43 ± 0.1%
Table 12: FEMNIST classification results of FedLAMA with different φ settings.
# of clients	Local batch size	LR	Averaging interval: τ0-	Active ratio	Interval increase factor: φ	Validation acc.
128	32	0.04	12	100%	1 (FedAvg)	85.74 ± 0.21%
					2	85.40 ± 0.13%
					4	84.67 ± 0.1%
					8	-	84.15 ± 0.18%
				50%	1 (FedAVg)	86.59 ± 0.2%
					2	86.07 ± 0.1%
					4	85.77 ± 0.15%
					8	85.31 ± 0.03%
				25%	1 (FedAVg)	86.04 ± 0.2%
					2	86.01 ± 0.1%
					4	85.62 ± 0.08%
					8	.	85.23 ± 0.1%
28