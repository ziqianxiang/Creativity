Under review as a conference paper at ICLR 2022
Learning to Pool in Graph Neural Networks
for Extrapolation
Anonymous authors
Paper under double-blind review
Ab stract
Graph neural networks (GNNs) are one of the most popular approaches to using
deep learning on graph-structured data, and they have shown state-of-the-art perfor-
mances on a variety of tasks. However, according to a recent study, a careful choice
of pooling functions, which are used for the aggregation and readout operations
in GNNs, is crucial for enabling GNNs to extrapolate. Without proper choices of
pooling functions, which varies across tasks, GNNs completely fail to generalize to
out-of-distribution data, while the number of possible choices grows exponentially
with the number of layers. In this paper, we present GNP, a Lp norm-like pooling
function that is trainable end-to-end for any given task. Notably, GNP generalizes
most of the widely-used pooling functions. We verify the effectiveness of GNNs on
thirteen tasks using seven different GNN architectures and eight real-world graphs
with up to one million edges. Specifically, we demonstrate that simply using GNP
for every aggregation and readout operation enables GNNs to extrapolate well on
many node-level, graph-level, set-related tasks; and GNP sometimes performs even
better than the best-performing choices among existing pooling functions.
1	Introduction
Many real-world data, such as relationships between people in social networks or chemical bonds
between atoms, can naturally be represented as graphs. Finding models with proper inductive biases
to better describe such graph data has been a common goal for many researchers, and Graph Neural
Networks (GNNs)(Scarseni et al., 2009; KiPf & Welling, 2017; Hamilton et al., 2017; VelickoVic
et al., 2018; Xu et al., 2019; Maron et al., 2019; Xu et al., 2020) are considered to be the most
successful model. They haVe ProVed effectiVe for a Variety of tasks, including recommendation (Ying
et al., 2018a), drug discoVery (Stokes et al., 2020), and chiP design (Mirhoseini et al., 2020).
An imPortant design choice for a GNN often oVerlooked is the sPecification of pooling functions,
the functions used for the aggregation or readout oPeration in GNNs. They are usually required
to be inVariant w.r.t. the Permutation of nodes in a graPh, and common choices are element-wise
summation (sum), maximum (max), minimum (min), or aVerage (mean). Some recent works also
ProPosed to use Parametric models and learn them from data as well (Ying et al., 2018b; Lee et al.,
2019b; Gao & Ji, 2019; Yuan & Ji, 2020).
While most of the PreVious works on this line focused on imProVing PredictiVe Performance for their
own tasks, recently, Xu et al. (2021) studied the imPact of the choice of Pooling functions on the
ability of a neural network to extrapolate. SPecifically, Xu et al. (2021) highlighted the imPortance
of the choice of Pooling functions in order to make GNNs generalize oVer the data lying outside of
the suPPort of the training data distribution, and they argued that the sPecification of the Pooling
functions acts as an imPortant inductiVe bias that can make GNNs either comPletely fail to extraPolate
or gracefully generalize to out-of-distribution data. As a motiVating examPle, consider the Problem of
counting the number of nodes in a graPh. If we are to solVe this Problem with a single-layer GNN
haVing one readout layer, Probably the best Pooling function would be sum, and the corresPonding
model will readily generalize to graPhs with a much larger number of nodes than the ones seen during
training. On the other hand, if we choose the Pooling function as max instead, it may still fit the
training data well but comPletely fail to Predict the number of nodes in out-of-distribution graPhs.
The findings in Xu et al. (2021) raise a natural question; which Pooling functions should be used
for a giVen Problem in order to make GNNs constructed with them successfully extraPolate for
1
Under review as a conference paper at ICLR 2022
out-of-distribution data? Xu et al. (2021) did not present any guide but empirically showed that we
do have the “right” pooling function for each problem tested, and when a pooling function is not
properly selected, GNNs completely fails to extrapolate. The caveat here is that we do not know
which pooling function is the right choice before actually training and validating the model.
To this end, in this paper, we present a generic learning-based method to find proper pooling
functions for a given arbitrary problem. Our method, entitled Generalized Norm-based Pooling
(GNP), formulates the pooling functions as a generic Lp norm-like function (including negative p
as well), and learns the parameters inside the pooling functions in an end-to-end fashion. Unlike
previous learning-based pooling methods that are usually tailored for specific tasks or focused on
improving predictive performances, GNP can be applied to arbitrary tasks, and it improves the
extrapolation ability of GNNs constructed with it. Also, GNP includes most of the pooling functions
being used for GNNs as special cases. Despite the enhanced flexibility, GNP incurs minimal overhead
in GNN in terms of the model complexity. A naive application of GNP to GNNs is likely to fail
because of some difficulty in training, so we propose a simple remedy to this. Using nine graph-level,
node-level, and set-related tasks, we demonstrate that GNNs with GNP trained by our training scheme
extrapolate for out-of-distribution data comparably and sometimes even better than those with pooling
functions that are carefully chosen among of widely-used ones. In addition, we demonstrate the
effectiveness of GNP on four real-world tasks (graph classification, influence maximization, node
classification, and graph regression) using six additional GNN architectures (GCN (Kipf & Welling,
2017), GAT (VelickoVic et al.,2018), PNA (Corso et al., 2020), hierarchical SAGPool(Lee et al.,
2019b), ASAPool (Ranjan et al., 2020), and MONSTOR (Ko et al., 2020)) and eight real-world
graphs. We summarize our contributions as follows:
•	Generalized pooling function: We propose GNP, a simple yet flexible pooling function that can
readily be applied to arbitrary tasks involving GNNs, with minimal parameter overhead.
•	Effective training methods: We propose effective training methods for GNP.
•	Extensive experiments: We empirically demonstrate that GNNs with GNP generalize to out-of-
distribution data on nine extrapolation tasks. We also show successful application of GNP to six
GNN architectures on four real-world tasks in eight real-world graphs with up to one million edges.
2	Related work
Aggregation functions Various aggregation functions have been appeared to enhance the perfor-
mance of GNNs. Hamilton et al. (2017) proposed GraphSAGE with four different aggregation
methods; max, mean, GCN (Kipf & Welling, 2017), and LSTM (Hochreiter & Schmidhuber, 1997).
Velickovic et al. (2018) proposed Graph Attention neTworks (GArS) including attention-based aggre-
gation functions (Vaswani et al., 2017). Xu et al. (2019) proposed Graph Isomorphism Networks
(GINs) and proved that GNN can satisfy the 1-Weisfeiler-Lehman (WL) condition only with sum
pooling function as aggregation function. Recently, Li et al. (2020) proposed a trainable softmax
and power-mean aggregation function that generalizes basic operators. Compared to these methods
designed to improve interpolation performance on specific tasks, ours can improve extrapolation
performance for generic tasks.
Readout functions Zhang et al. (2018) suggested SortPooling that chooses top-k values from the
sorted list of the node features to construct outputs. Another popular idea is hierarchical pooling,
where outputs are obtained by iteratively coarsening nodes in graphs in a hierarchical fashion (Ying
et al., 2018b; Gao & Ji, 2019; Lee et al., 2019b; Yuan & Ji, 2020). Although demonstrated to be
effective for the tasks they have been designed for, most of these methods require heavy computation
and it is not straightforward to extend them for aggregation functions. On the other hand, our GNP
can be applied to both aggregation and readout functions with minimal overhead.
Pooling functions in generic context Vinyals et al. (2015) proposed Set2Set to get a representation
of set-structured data with a LSTM-based pooling function. Lee et al. (2019a) proposed to use an
attention-based pooling function to get summaries of set data. For convolutional neural networks,
there were some approaches to generalize average pooling and max pooling widely used for many
neural network architectures. Gulcehre et al. (2014) proposed a normalized learnable Lp norm
function that generalizes average pooling and max pooling. Lee et al. (2016) further extended those
pooling functions with learnable tree-structured pooling filters.
2
Under review as a conference paper at ICLR 2022
Norm-based pooling functions There have been several works to employ norm-based pooling
functions. Gulcehre et al. (2014) proposed a learnable Lp norm function of the form
1 |v|	1/p
f (V)=(E XIviIp)
to substitute max pooling or average pooling used in convolutional neural networks. Similar norm-
based pooling functions were used for acoustic modeling (Swietojanski & Renals, 2016) and text
representation (Wu et al., 2020). Compared to GNP, these pooling methods cannot express the sum
pooling. Li et al. (2020) further generalized this by multiplying IvIq to include sum pooling as well,
but not considered the case where p is positive and the case where p is negative at the same time. GNP
is the most generic norm-based pooling function, compared to all aforementioned approaches, and
more importantly, no other works studied their usefulness in the context of learning to extrapolate.
Extrapolation Trask et al. (2018) pointed out that most of the feed-forward neural networks fail
to extrapolate even for the simplest possible identity mapping, and suggested using alternative
computation units mimicking the behavior of arithmetic logic units. The ability to extrapolate is also
important in the GNN context, for instance, many combinatorial optimization problems involving
graphs often require extrapolation. Selsam et al. (2019); Prates et al. (2019) tackled the extrapolation
problem by performing large iterations of message passing. Using various classical graph algorithms,
VeliCkovic et al. (2020) showed that the extrapolation performance of GNNs depends heavily on the
choice of the aggregation function. Similarly, Xu et al. (2021) demonstrated that choosing the right
non-linear function for both MLPs and GNNs is crucial for the extrapolation.
3 Main Contribution: Generalized Norm-based Pooling
In this section, we present our Generalized Norm-based Pooling (GNP) and discuss its expressiveness.
Then, we describe some difficulties in training GNP and our remedy. Lastly, we present a task on
which a GNN with GNP can extrapolate, while that equipped with the basic pooling functions cannot.
3.1	Generalization of Basic Pooling Functions
While GNP is motivated by the Lp-norm function, which includes the sum and max functions as
special cases, further ingredients are added to make GNP more flexible than the Lp-norm function.
Specifically, we allow p to be negative to let GNP express a wider class of functions than the previous
norm-based or learning-based pooling functions.
Let V = {vi}in=1 be a set of node features with vi ∈ Rd for i = 1, . . . , n. We define GNP to be an
element-wise function where the output for each jth element is
GNPj(V) = g (X Ivi,jIp),
where p ∈ R \ {0} and q ∈ R are learnable parameters. GNP includes the basic pooling functions
(sum, mean, max, and min) as special cases.
Proposition 1. Suppose all the entries of v are non-negative in equation 3.1. Then, GNP includes
sum, max, mean as special cases. If we further restrict v to be positive, GNP includes min.
Proof. GNPj (V) is equivalent to elementwise sum when (p, q) = (1, 0) and elementwise mean
when (p, q) = (1, 1). When q = 0, we have
lim GNPj (V) = max vi lim
p→∞	i	p→∞
e,小
maxi Ivi,j
p 1/p
max ∣vi,jI ∙ 1 = max Ivij I,
ii
(1)
so GNP converges to max. Similarly, we can obtain min as a limit forp → -∞.
□
3
Under review as a conference paper at ICLR 2022
3.2	HANDLING OF NEGATIVE p
The GNP function in equation 3.1 is not continuous and even not defined at p = 0. Hence, directly
learning GNP in the original form as in equation 3.1 (even with p = 0 ignored) can cause instability,
especially when an algorithm is trying to move from a positive p value to a negative p value. Instead,
we suggest splitting the GNP function into two parts, GNP+ with positive p and GNP- with negative
p, and let the model choose the right balance between them. Specifically, define
1 n	1/p+	1 n	-1/p-
GNP+(v)= nq+ (XMijIp]	, GNP-(V) = nq- (XEj∣-pj	,⑵
where p+ > 0, q+ , p- > 0, and q- are learnable parameters. Given a set of node features V, we first
split the feature dimension into two, and compute the output from GNP+ for the first half and from
GNP- for the second half. Then we mix two outputs with a single linear layer to get the final output.
y= hGNP1+(V) ...	GNPb+d/2c(V) GNPb-d/2c+1(V) ...	GNPd- (V)i ,	(3)
GNP(V) =Wy+b,	(4)
where [∙[ is the floor function, W ∈ Rd×d and b ∈ Rd are learnable parameters. Note that Widely-
used GNN layers have a linear layer or MLP after message-passing between nodes. Instead of using
an additional linear layer, GNP concatenates the outputs of GNP+ and GNP- and passes them to
the linear layer or MLP. Therefore, we have only four extra parameters (p+ , p- , q+ and q- ) for each
GNN layer of typical GNN architectures. With this design, GNP can easily switch between positive p
and negative p, choosing proper values according to tasks.
3.3 Stabilization of Training Processes
Unfortunately, even with the above design to split the positive and negative parts, GNP still suffers
from a training instability issue. In this section, we introduce our remedy for such an issue. With our
remedy, as we will empirically demonstrate, GNP can be applied to arbitrarily complex deep GNNs
as a drop-in replacement for the existing pooling functions.
Negative or near-zero inputs GNP first processes inputs to be non-negative values by taking
absolute values. In practice, in many GNN architectures, inputs are passed through ReLU before
being fed into the pooling functions, so in such a case, we do not explicitly take the absolute values.
If not, we explicitly put the ReLU activation function before every GNP to make inputs non-negative.
For the positive part of GNP, when the inputs are close to zero, the gradient w.r.t. the parameter p+
may be exploded, as one can see from the following equation.
∂ GNP+(V)	GNP+(V)
∂p+	p+
Tog(GNP+(V))+PGNi(Vgp+ij)
Hence, we add a small tolerance term to every input element to prevent gradient explosion. This
works well for positive p, but we need more care for negative p. When p is negative, even small
can be amplified by the term (vi,j + )p- to dominate the other values. Hence, when a specific
input vi,j is smaller than , we replace it with q/ to mask out the effect of that input for the output
computation. An exceptional case is when every input element is below . For such a case, we fix the
output of GNP to be zero by default.
v = vi,j +	if vi,j >
i,j / otherwise ,
0
GNP-(V" n1- (Pn= Vpj)1/p-
if vi,j < ε for i = q, . . . , n
otherwise
Even with these treatments, still, the algorithm can diverge especially when p is large. To resolve this,
we clipped all p values to be contained in [0, 50] and used the log-sum-exp trick. That is,
GNPp+ (V) = nq+ exp (p+ log (Xexp(p+log(vij + e)))).
4
Under review as a conference paper at ICLR 2022
Table 1: Extrapolation performance in terms of MAPE on large graphs with different structures. On
two tasks (invsize and harmonic), GNP significantly outperformed the second best one.
Types	invsize		harmonic		maxdegree	
	GNP	Best Baseline (sum, max)	GNP	Best Baseline (SAGPool)	GNP	Best Baseline (sum, max)
BA	0.9±0.3	92.5±10.5	2.5±0.9	78.4±40.8	2.1±1.1	0.0±0.0
Expander	1.9±1.0	35.4±7.8	0.9±0.5	11.9±18.6	2.3±1.1	0.0±0.0
4regular	0.8±0.3	205.6±36.8	1.9±1.3	1179.3±310.6	3.4±3.7	0.0±0.0
Tree	0.8±0.3	202.3±11.9	14.7±6.3	149.4±34.9	1.9±0.6	0.0±0.0
Ladder	0.8±0.3	195.4±53.6	2.4±2.4	1138.4±283.3	30.7±16.9	0.1±0.1
Also, similar to Gulcehre et al. (2014), we reparameterized p+ and p- with the softplus activation
function, i.e., p+ = 1 + log(1 + exp(t+)) for some t+ ∈ R.
Another important trick was to use different learning rates for training (p+,p-) and (q+, q-). Since
the parameters (q+, q-) have much larger impact on the GNP, if we use the same learning rates for
(p+,p-) and (q+, q-), the model can converge to unwanted local minimum that are not faithfully
tuned for (p+,p-). Hence, we used larger learning rates for (p+,p-) to balance training.
3.4 Extrapolation Ability of GNP
As stated in Theorem 1, we prove that a GNN equipped with GNP can extrapolate on the harmonic
task, which we define in Section 4.2. However, that equipped with the basic pooling functions cannot
extrapolate on the task, as we show empirically in Section 4.2 and theoretically in Appendix A.
Theorem 1. (Informal) Assume all the nodes in G have the same scalar feature 1. Then, a one-layer
GNN equipped with GNP and trained with squared loss in the NTK regime learns the harmonic
task function, and thus it can extrapolate.
Proof. See Appendix A for detailed analysis.	□
4	Experiments
In this section, we review our experiments on various extrapolation tasks.
4.1	Experimental Setups
Machines We performed all experiments on a Linux server with RTX 3090 GPUs.
GNN models For graph-level tasks, we used one GIN (Xu et al., 2019) layer with a hidden
dimension of 32 and two FC layers as MLP, and we fed only the outputs of the GIN layer into the
readout function. Note that this simple model is expressive enough for obtaining exact answers to all
considered graph-level tasks. For node-level tasks, we used three of the aforedescribed GIN layers,
without readout functions, so that nodes at most three hops away from the target node can be taken
into consideration. For set-level tasks, we used one FC layer with a hidden dimension of 32 before
the pooling function and used another FC layer for the final output after the pooling function.
Baseline Commonly for all tasks, we considered sum, max, mean, and min, all of which are
generalized by GNP, as baseline aggregation and/or readout functions. For graph-level tasks, we
additionally considered SortPooling (Zhang et al., 2018) with k = 20 and Set2Set (Vinyals et al.,
2015) as baseline readout functions, and we considered the hierarchical pooling version of SAGPool
(Lee et al., 2019b) as a whole as a baseline model. For set-level tasks, we additionally considered
Set2Set (Vinyals et al., 2015) as a baseline pooling function and Set Transformer (Lee et al., 2019a)
as a whole as a baseline model.
Evaluation We compared evaluation metrics on the test set when validation loss was minimized,
and in each setting, we reported mean and standard deviation over 5 runs, unless otherwise stated.
5
Under review as a conference paper at ICLR 2022
GNP (Proposed)	(sum, max)	(sum, mean)	SortPooIing	SAGPool
100
75
50
25
0
50	60	70	80	90	100
Number of nodes
Number of nodes	Number of nodes
(b) harmonic
(c) maxdegree
Figure 1: Extrapolation performances depending on the number of nodes in test graphs on three tasks
(invsize, harmonic, and maxdegree). Only GIN equipped with GNP performed consistently
well on all the tasks. We tested 19 competitors and reported the results of the most successful ones.
(a) invsize
4.2	Extrapolation Performances on Graph-level Tasks
In this section, we consider three graph-level tasks. Given a graph, the first task is to find the
maximum node degree (maxdegree), and the second task is to compute the harmonic mean node
degree divided by the number of nodes (harmonic). The last task is to compute the inverse of the
number of nodes (invnode), which does not depend on the topology of the given graph. For details
of the synthetic datasets we used, see Appendix B.1.
For maxdegree, whose objective is maxv∈v (Pu∈n(v)1), where N (V) is the set of neighbors of
v, the reasonable choice is to use sum and max as aggregation and readout functions, respectively,
For harmonic, whose objective is (Pv∈v(Pu∈n(v)1)-1)-1 the reasonable combination of aggre-
gation and readout functions are sum and GNP with (p, q) = (-1,0), respectively. For invnode,
whose objective is (Pv∈vIT)-1, any of mean, max, and min is reasonable as the aggregation
function, and GNP with (p, q) = (-1, 0) is reasonable as the readout function.
We trained all models for 200 epochs, and we compared their test MAPE1 for evaluation in Figure 1.
GIN with GNP showed near-perfect extrapolation performances on all three tasks, and especially
for harmonic and invnode, GIN with GNP was the only successful model. Among the 16
combinations of sum, max, mean, and min, using sum and max as the aggregation and readout
functions, respectively, showed near-perfect extrapolation performance on maxdegree. For the
same task, another combination (mean, max) showed reasonably good performance. For the other
tasks, however, none of the 16 combinations was successful. sortPool and set2set as the readout
function were tested, while fixing the aggregation function to the aforementioned reasonable one for
each task. While they performed almost perfectly for maxdegree, they failed at the other tasks.
Lastly, sAGPool was not successful in any of the tasks.
We also tested the extrapolation performance using large test graphs with distinctive structures.
As seen in Table 1, GIN with GNP showed near-perfect performance only except for harmony
on random trees, and maxdegree on ladder graphs. Especially, on invsize and harmony, it
significantly outperformed the best baseline. We further tested the extrapolation performance of
GNP and the baseline approaches using real-world graphs in Appendix C.1, graphs with different
structures in Appendix C.2, graphs with different node feature distributions in Appendix C.3, and
various activation functions in Appendix C.4.
4.3	Extrapolation Performance on Node-level Tasks
We further evaluated the extrapolation performance of GNP on two node-level tasks considered in
VeIiCkOViC et al. (2020). The first task is to decide whether each node is within 3 hops from the
target node or not. (bfs). We formulate the task as a regression problem and the label is 1 within 3
hops and 0 outside 3 hops. The second task is to find the minimum distance from each node to the
target node on a graph with non-negative weights (shortest). only the nodes within 3 hops from
the target node were taken into consideration. As discussed in (VeIiCkOViC et al., 2020), one of the
optimal models for the tasks imitates the parallel breadth-first search and the parallel Bellman-Ford
algorithm (Bellman, 1958) for bfs and shortest, respectively. In such cases, the reasonable
aggregators for bfs and ShOrteSt are max and min, respectively.
1MAPE scales the error by the actual value, and it has been considered as a proper measure of extrapolation
performance (Xu et al., 2021).
6
Under review as a conference paper at ICLR 2022
Table 2: Extrapolation performances in terms of MAE on two node-level tasks (shortest and bfs).
GNP and all baseline methods were near perfect on bfs; GNP was second best on shortest.
Aggregation	sum	max	mean	min	GNP
bfs shortest	0.000±0.000^^0.000±0.000^^0.000±0.000^^0.000±0.000^^0.000±0.001 1.323±0.162 0.762±0.395	1.316±0.330 0.141±0.007 0.332±0.105
(a) Extrapolation Performance on Large Graphs with Homogeneous Structures.
T	I	bfs	I	shortest
Types 1________________________1_________________________________
I GNP	max ∣ GNP	min	max
BA	0.001±0.001	0.000±0.000	0.546±0.168	0.275±0.015	1.268±0.642
Expander	0.000±0.000	0.000±0.000	0.159±0.068	0.019±0.004	0.334±0.225
4regular	0.003±0.003	0.000±0.000	1.911±0.257	1.188±0.182	5.178±1.218
Tree	0.003±0.002	0.000±0.000	1.579±0.289	1.057±0.256	4.584±0.980
Ladder	0.002±0.001	0.000±0.000	1.217±0.278	0.701±0.160	3.400±1.056
(b) Extrapolation Performance on Large Graphs with Heterogeneous Structures.
Table 3: Extrapolation performance in terms of MAPE on set-related tasks. Only the basic model
equipped with GNP performed consistently well on all tasks. Especially for σpost and σMap，it
significantly outperformed all competitors, including Set Transformer.
Model	Pooling	μpost	丁 2 σpost	“MAP	σM AP
	sum	135.0 ± 9.3	390.7 ± 99.1	126.8 ± 18.2	369.1 ± 12.1
	max	119.2 ± 31.7	120.6 ±4.0	118.6 ± 31.6	108.8 ± 2.0
Basic	mean	1.9 ± 0.2	134.2 ± 6.0	1.9 ± 0.2	107.1 ± 2.2
	min	95.8 ± 15.6	126.2 ± 3.9	118.6 ± 31.6	108.0 ± 2.4
	Set2Set	2.1 ± 0.2	135.7 ± 4.0	1.9 ± 0.2	106.1 ± 2.6
	sum	136.3 ± 6.8	119.3 ± 16.1	100.0 ± 0.0	381.9 ± 10.8
	max	100.0 ± 0.0	123.8 ± 2.1	98.9 ± 2.6	109.6 ± 3.3
Deep	mean	2.2 ± 0.2	135.2 ± 4.0	2.2 ± 0.4	109.2 ± 2.9
	min	83.0 ± 10.2	99.5 ± 2.1	90.8 ± 6.0	108.8 ± 5.3
	Set2Set	1.9 ± 0.3	131.1 ± 8.5	1.9 ± 0.2	106.0 ± 1.5
Set Transformer		^^1.9 ± 0.2^^	25.0 ± 9.0	^^1.9 ± 0.1 ^^	40.8 ± 9.5
Basic	GNP	1.5 ± 0.6	^^0.7 ± 0.3^^	1.5 ± 0.6	3.1 ± 0.5
We considered five GINs equipped with sum, max, mean, and min, and GNP, respectively, as
aggregation functions. Note that the readout operation is not used for node-level tasks. For description
of the datasets, see Appendix B.2. We trained all of them for 100 epochs for bfs and for 200 epochs
for shortest; and we compared their test MAE2 in Table 2. GNP and all baseline methods were
near perfect on bfs, regardless of graph types, and GNP was second best on shortest. As
expected, GIN with min performed best on shortest.
4.4	Extrapolation Performance on Set-related Tasks
We also applied our proposed approach to three set-related tasks. They are all related to estimating
posterior distributions when the likelihood function is Gaussian. Specifically, the tasks are to find
closed-form posterior hyperparameters μpost and σp°st, the MAP estimate "map of μ when σ2 is
known, and the MAP estimate σM1 AP if σ2 when μ is known. Note that ground-truth values of μpost
and "map are identical, while we used different loss functions for them. For description of the
datasets, see Appendix B.3.
We trained for 300 epochs (a) the basic model (see Section 4.1) with GNP, (b) Set Transformer (Lee
et al., 2019a) (c) the basic and deep3 models with one among sum, max, mean, min, and Set2Set
(Vinyals et al., 2015). We compared their MAPE in Table 3. The basic model equipped with GNP
showed near-perfect extrapolation performance on all four tasks, even though the formula for σMAP
cannot be exactly expressed by gNp, and it was the only such model. For μpost and "map, whose
ground-truth values are approximated by the average of the elements, Set Transformer and those
equipped with mean or Set2Set were comparable to the basic model with GNP, while they were not
on the other tasks.______________
2MAPE was not applicable since the ground-truth value for some nodes can be 0.
3The deep model has an additional FC layer before the pooling function.
7
Under review as a conference paper at ICLR 2022
2.00-
1.75-
1.50-
p+
1.25-
1.00-
0.75-
J二一一
0 50 100150200
Training Epochs
0.4
0.2q+
0.0
40-
30-
p+ 20-
10-
0-
q+
0 50 100 150 200
Training Epochs
1.00
0.75
0.50 q+
0.25
0.00
2.00-1
1.00-
1.75-
p+ 1.50-
1.25-
-0.75
(C) mean (μpost)
-0.50 q+
-0.25
-0.00
0 100 200 300
Training Epochs
I-1.00
0.75
0.00
(d) min (shortest)
0.50
q-
0.25
50-
40-
p-30-
20-
10-
0-
0 50 100 150 200
Training Epochs
(a)	sum (maxdegree)
(b)	max (maxdegree)
Figure 2: EmpiriCal behavior of GNP. We showed how the parameters p and q of GNP Changed
during training. For eaCh task, GNP imitated the ideal pooling funCtions if suCh pooling funCtions
exist. For example, for maxdegree, GNP as aggregation and readout funCtions approximated sum
(i.e., p+ ≈ 1 and q+ ≈ 0) and max (i.e., p+	1 and q+ ≈ 0), respeCtively, whiCh performed best.
Table 4: EffeCtiveness of GNP- . The extrapolation performanCe of GNP degraded without GNP-.
Tasks	gnp	GNp+	Graphs	gnp	GNp+
harmonic	1.1 ± 0.8	2.1 ± 0.6	BA	2.5 ± 0.9	31.5 ± 1.2
shortest	0.332 ± 0.105	0.774 ± 0035	tree	14.7 ± 6.3	26.1 ± 7.4
σ2 σpost	0.7 ± 0.3	0.6 ± 0.2	ladder	2.4 ± 2.4	19.3 ± 21.1
(a) Test Error on ErdoS-Renyi Random Graphs
(b) Test Error on harmonic on Other Graphs
4.5	Empirical Behavior of GNP
As we disCussed in SeCtion 3, GNP generalizes sum, max, mean, and min. In order to Confirm the
faCts experimentally, we showed in Figure 2 how the learnable parameters p and q in GNP Changed
during training. For maxdegree, GNP as aggregation and readout funCtions approximated sum
(i.e., p+ ≈ 1 and q+ ≈ 0) and max (i.e., p+ 1 and q+ ≈ 0), respeCtively, whiCh performed best
on the task. For μpost and shortest, GNp approximated mean (i.e., p+ ≈ 1 and q+ ≈ 1) and min
(i.e., p-	0 and q- ≈ 0), respeCtively, whiCh were the best performing baseline for the tasks. To
sum up, empiriCally, GNP imitated the ideal pooling funCtions for eaCh task if suCh pooling funCtions
exist. We also observed that either GNP+ or GNP- tends to dominate the other side in all Considered
graph-level tasks. Detailed results are provided in Appendix C.6.
4.6	Ablation Study: Effectiveness of GNP-
In order to demonstrate the effeCtiveness of GNP- for extrapolation, we Compared the model
equipped only with GNP and the model only with GNP+ on eaCh of three tasks (harmony,
shortest, and σp2ost) in Table 4. The detailed settings for eaCh task were the same as in previous
experiments. The model only with GNP+ performed well only on the task for σp2ost. The extrapolation
performanCe of GNP degraded signifiCantly without GNP- on harmony and shortest.
4.7	Effectiveness of GNP on Real-world Tasks
Graph classification We Compared the graph ClassifiCation aCCuraCy of hierarChiCal SAGPool (Lee
et al., 2019b) and ASAPool (Ranjan et al., 2020), and their variants with GNP. For the variant of
SAGPool, we replaCed all pooling funCtions before, inside, and between graph pooling operations.
For the variant of ASAPool, we replaCed all pooling funCtions exCept for those inside LEConv. SinCe
we used GNP, instead of the ConCatenation of global average pooling and max pooling funCtions, the
input dimension of the first fully-ConneCted layer after them was reduCed by half. For the variants,
exCept for the additional hyperparameters of GNP, all hyperparemters were set the same.
We used three datasets from TUDataset (Morris et al., 2020). D&D (Dobson & Doig, 2003; Sher-
vashidze et al., 2011) and PROTEINS (Dobson & Doig, 2003; Borgwardt et al., 2005) Contain
protein-interaCtion graphs, and NCI1 (Wale & Karypis, 2006) Contains the graphs representing Chem-
iCal Compounds. For ConsistenCy with the original SAGPool, we performed 10-fold Cross validation
with 20 different random seeds. For ASAPool, we performed 10-fold Cross validation with the 20
random seeds speCified in its implementation. We report the test aCCuraCy with standard deviation in
Table 5. SAGPool and ASAPool equipped with GNP Consistently outperformed the original models
with a Carefully Chosen pooling funCtions.
8
Under review as a conference paper at ICLR 2022
Table 5: Graph classification accuracy. Replacing the carefully chosen pooling functions in SAGPool
and ASAPool with GNP improved their accuracy on graph-classification tasks.
Model	Aggregation	Readout	D&D	PROTEINS	NCI1
SAGPool (original)	GCN	mean, max	0.765 ± 0.009	0.722 ± 0.008	0.688 ± 0.013
SAGPool (with GNP)	GNP	GNP	0.774 ± 0.010	0.728 ± 0.013	0.695 ± 0.015
(a) SAGPool					
Model	Aggregation	Readout	D&D	PROTEINS	NCI1
ASAPool (original)	GCN	mean, max	0.764 ± 0.009	0.738 ± 0.008	0.711 ± 0.004
ASAPool (with GNP)	GNP	GNP	0.772 ± 0.007	0.739 ± 0.006	0.725 ± 0.007
(b) ASAPool
Table 6: Influence maximization performance. The influences of 100 seed nodes produced by
MONSTOR and its variants in graphs unseen during training are reported. The variant equipped with
GNP outperforms original MONSTOR (with max) and the other variant (with sum).
Aggregation	BT	Extended JI	LP	BT	WannaCry JI	LP	BT	Celebrity JI	LP	
max	1222.5±0.4	706.9±0.1	3259.6±0.7	2746.5±1.4	1646.6±2.1	9090.2±3.8	155.2±0.1	140.5±0.0	5665.0±1.4
sum	1216.6±1.7	706.5±0.2	3189.2±6.9	2742.6±0.9	1645.8±0.2	9030.1±2.0	153.9±0.4	140.5±0.0	5666.9±0.6
GNP	1223.0±0.3	707.3±0.2	3262.1±1.7	2753.4±0.1	1648.3±0.1	9098.4±2.2	155.3±0.8	140.4±0.0	5666.1±1.8
Influence maximization We compared the performance of MONSTOR (Ko et al., 2020) and
its variants with GNP on the influence maximization task (Kempe et al., 2003), which has been
extensively studied due to its practical applications in viral marketing and computational epidemiology.
The objective of the task is to choose a given number of seed nodes so that their collective influence
(i.e., degree of spread of information through a given social network) is maximized.
For experimental details, we followed (Ko et al., 2020): (a) we used three real-world social networks
with up to 85, 202 edges (Extended, WannaCry, and Celebrity) with three kinds of realistic
activation probabilities (BT, JI, and LP), (b) we used the same training methods and hyperparameters
except for the additional parameters of GNP, and (c) we compared MONSTOR and its variants in an
inductive setting. For example, we used the model trained using the Celebrity and WannaCry
datasets to test the performance on the Extended dataset. For details of the influence maximization
problem, MONSTOR, and the real-world social networks that we used, see Appendix E.
We performed three runs and reported the influence maximization performance with standard devia-
tions in Table 6. As seen in the results with sum and max aggregations, the performances heavily
depended on the choice of the aggregation function. In most of the cases, MONSTOR equipped with
GNP outperformed the original MONSTOR with max aggregation and also a variant of MONSTOR
with sum aggregation.
Two additional tasks The experimental results on node classification and graph regression are
provided in Appendix C.7. For node classification, we used a real-world graph with over one
million edges and compared the accuracy of GCN and GAT, and their variants with GNP. For graph
regression, we compared the MAE of PNA (Corso et al., 2020) and its variants with GNP.
5	Conclusion
In this work, we proposed GNP, a learnable norm-based pooling function that can readily be applied
to arbitrary GNNs or virtually to any neural network architecture involving permutation-invariant
pooling operation. The key advantages of GNP are its generality and ability to extrapolate. We
showed that GNP includes most of the existing pooling functions and can express a broad class
of pooling functions as its special cases. More importantly, with various synthetic and real-world
problems involving graphs and sets, we demonstrated that the networks with GNP as aggregation or
readout functions can correctly identify the pooling functions that can successfully extrapolate. We
also introduced some non-trivial design choices and techniques to stably train GNP. The limitation of
our work is that, although we have empirically demonstrated the excellent extrapolation performance
on various tasks, we have not developed theoretical arguments regarding under what condition models
constructed with GNP will extrapolate well. It would be an interesting future work to rigorously
study the class of problems that GNP can solve.
9
Under review as a conference paper at ICLR 2022
Reproducibility Statement
We provided the source code used in our experiments in main paper, including the implementations
of GNP and the GIN model, in the supplementary materials. The provided supplementary matarials
also include example synthetic datasets and the pretrained weights used in our experiments.
References
Albert-Laszlo Barabasi and Reka Albert. Emergence of scaling in random networks. Science, 286
(5439):509-512,1999.
Richard Bellman. On a routing problem. Quarterly ofapplied mathematics, 16(1):87-90, 1958.
Karsten M. Borgwardt, Cheng Soon Ong, Stefan Schonauer, S. V. N. Vishwanathan, Alex J. Smola,
and Hans-Peter Kriegel. Protein function prediction via graph kernels. Bioinformatics, 21(SUPPL1):
i47-i56, 2005.
Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Lio, and Petar Velickovic. Principal
neighbourhood aggregation for graph nets. In NeurIPS, 2020.
Paul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes without
alignments. Journal of molecular biology, 330(4):771-783, 2003.
Simon S Du, Kangcheng Hou, Russ R Salakhutdinov, Barnabas Poczos, Ruosong Wang, and Keyulu
Xu. Graph neural tangent kernel: Fusing graph neural networks with graph kernels. In NeurIPS,
2019.
Paul ErdoS and Alfred Renyi. On the evolution of random graphs. Publications ofthe Mathematical
Institute of the Hungarian Academy of Sciences, 5(1):17-60, 1960.
Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In
ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.
Hongyang Gao and Shuiwang Ji. Graph u-nets. In ICML, 2019.
Caglar Gulcehre, Kyunghyun Cho, Razvan Pascanu, and Yoshua Bengio. Learned-norm pooling for
deep feedforward and recurrent neural networks. In ECML/PKDD, 2014.
William L Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In NeurIPS, 2017.
Sepp Hochreiter and JUrgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):
1735-1780, 1997.
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,
and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv
preprint arXiv:2005.00687, 2020.
John J Irwin, Teague Sterling, Michael M Mysinger, Erin S Bolstad, and Ryan G Coleman. Zinc: a
free tool to discover chemistry for biology. Journal of chemical information and modeling, 52(7):
1757-1768, 2012.
Arthur Jacot, Clement Hongler, and Franck Gabriel. Neural tangent kernel: Convergence and
generalization in neural networks. In NeurIPS, 2018.
David Kempe, Jon Kleinberg, and Eva Tardos. Maximizing the spread of influence through a social
network. In KDD, 2003.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
In ICLR, 2017.
Jihoon Ko, Kyuhan Lee, Kijung Shin, and Noseong Park. Monstor: An inductive approach for
estimating and maximizing influence over unseen networks. In ASONAM, 2020.
10
Under review as a conference paper at ICLR 2022
Chen-Yu Lee, Patrick W Gallagher, and Zhuowen Tu. Generalizing pooling functions in convolutional
neural networks: Mixed, gated, and tree. In AISTATS, 2016.
Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set
transformer: A framework for attention-based permutation-invariant neural networks. In ICML,
2019a.
Junhyun Lee, Inyeop Lee, and Jaewoo Kang. Self-attention graph pooling. In ICML, 2019b.
Jure Leskovec, Andreas Krause, Carlos Guestrin, Christos Faloutsos, Jeanne VanBriesen, and Natalie
Glance. Cost-effective outbreak detection in networks. In KDD, 2007.
Guohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem. Deepergcn: All you need to train
deeper gcns. arXiv preprint arXiv:2006.07739, 2020.
Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph
networks. In NeurIPS, 2019.
Azalia Mirhoseini, Anna Goldie, Mustafa Yazgan, Joe Jiang, Ebrahim Songhori, Shen Wang, Young-
Joon Lee, Eric Johnson, Omkar Pathak, Sungmin Bae, et al. Chip placement with deep reinforce-
ment learning. arXiv preprint arXiv:2004.10746, 2020.
Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion
Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. In ICML
Workshop on Graph Representation Learning and Beyond, 2020. URL www.graphlearning.
io.
Marcelo Prates, Pedro HC Avelar, Henrique Lemos, Luis C Lamb, and Moshe Y Vardi. Learning to
solve np-complete problems: A graph neural network for decision tsp. In AAAI, 2019.
Ekagra Ranjan, Soumya Sanyal, and Partha Talukdar. Asap: Adaptive structure aware pooling for
learning hierarchical graph representations. In AAAI, 2020.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The
graph neural network model. IEEE transactions on neural networks, 20(1):61-80, 2009.
Daniel Selsam, Matthew Lamm, Benedikt Bunz, Percy Liang, Leonardo de Moura, and David L Dill.
Learning a sat solver from single-bit supervision. In ICLR, 2019.
Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn, and Karsten M
Borgwardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(9), 2011.
Jonathan M Stokes, Kevin Yang, Kyle Swanson, Wengong Jin, Andres Cubillos-Ruiz, Nina M
Donghia, Craig R MacNair, Shawn French, Lindsey A Carfrae, Zohar Bloom-Ackermann, et al. A
deep learning approach to antibiotic discovery. Cell, 180(4):688-702, 2020.
Pawel Swietojanski and Steve Renals. Differentiable pooling for unsupervised acoustic model
adaptation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 24(10):1773-
1784, 2016.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-31,
2012.
Andrew Trask, Felix Hill, Scott E Reed, Jack Rae, Chris Dyer, and Phil Blunsom. Neural arithmetic
logic units. In NeurIPS, 2018.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Eukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In ICLR, 2018.
Petar Velickovic, Rex Ying, Matilde Padovano, Raia Hadsell, and Charles Blundell. Neural execution
of graph algorithms. In ICLR, 2020.
11
Under review as a conference paper at ICLR 2022
Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for sets.
In ICLR, 2015.
Nikil Wale and George Karypis. Comparison of descriptor spaces for chemical compound retrieval
and classification. In ICDM, 2006.
Minjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mufei Li, Xiang Song, Jinjing Zhou, Chao Ma,
Lingfan Yu, Yu Gai, Tianjun Xiao, Tong He, George Karypis, Jinyang Li, and Zheng Zhang.
Deep graph library: A graph-centric, highly-performant package for graph neural networks. arXiv
preprint arXiv:1909.01315, 2019.
Chuhan Wu, Fangzhao Wu, Tao Qi, Xiaohui Cui, and Yongfeng Huang. Attentive pooling with
learnable norms for text representation. In ACL, 2020.
Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan. Inductive representa-
tion learning on temporal graphs. In ICLR, 2020.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In ICLR, 2019.
Keyulu Xu, Mozhi Zhang, Jingling Li, Simon S Du, Ken-ichi Kawarabayashi, and Stefanie Jegelka.
How neural networks extrapolate: From feedforward to graph neural networks. In ICLR, 2021.
Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec.
Graph convolutional neural networks for web-scale recommender systems. In KDD, 2018a.
Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L Hamilton, and Jure Leskovec.
Hierarchical graph representation learning with differentiable pooling. In NeurIPS, 2018b.
Hao Yuan and Shuiwang Ji. Structpool: Structured graph pooling via conditional random fields. In
ICLR, 2020.
Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An end-to-end deep learning
architecture for graph classification. In AAAI, 2018.
Chuan Zhou, Peng Zhang, Jing Guo, Xingquan Zhu, and Li Guo. Ublf: An upper bound based
approach to discover influential nodes in social networks. In ICDM, 2013.
12
Under review as a conference paper at ICLR 2022
A Theoretical Analysis (Related to Section 3.4)
Similarly to Xu et al. (2021), we present an analysis of the extrapolation ability of GNNs with our
GNP pooling function. Specifically, we show that a one-layer GNN with the GNP pooling function
can extrapolate on the harmonic task. Let f(θ, G) be a one-layer GNN defined as follows:
f(θ, G) = W(2) GNP
W(1)xv
v∈N (u)
(5)
where θ is the parameters of the GNN, G = (V, E) is the input graph, xv is the initial feature of node
v ∈ V , and N(u) ⊆ V is the set of the neighbors of node u ∈ V .
When two graphs G and G0 are given, the Graph Neural Tangent Kernel (GNTK) (Du et al., 2019) is
computed as
GNTK(GC)= Eθ~n (0,I) KW，fG } ]-
A.1 Feature map of the GNTK
We first compute the GNTK for the network defined as equation 5 and derive the corresponding feature
map using a general framework presented in Jacot et al. (2018); Du et al. (2019); Xu et al. (2021).
Let Σ(1), Σ(2) be the covariance for the first linear layer and the second linear layer, respectively.
Also, let Θ(1) be the kernel value after the first linear layer, respectively.
From the framework, Σ(1) and Θ(1) are determined as follows:
hΣ(1)(G,G0)i	= hΘ(1)(G,G0)i	=xu>xu.
uu0	uu0
Also, Σ(2) can be computed as
∑(2) (G, GO) = E(f(v),f(v0))〜N(0,[Λ(1)(G,G0)]vvθ)
GNP
f(v)
v∈N(u)	u∈V
GNP
f(v0)
v0∈N(u0)	u0∈V0
where
[Λ(1)(G G0)]	=	[Σ(1)(G,G)]vv	[Σ(1)(G,G0)]vv0	= xv>xv	xv>x0v
[Λ (G, G)]vv0 =	[Σ(1)(G0, G)]v0v	[Σ(1)(G0, G0)]v0v0 = x0v>xv	x0v>x0v .
By a simple algebraic manipulation, one can easily see that the feature map φ(G) is computed as
Φ(G) = C ∙(GNP ({w(Mhv}v∈v),
lV1lq X (w(k)>hu )p-1(X (w(k)>hv )p)p ∖(w(k)>hu > 0) ∙ hu,...∖	(6)
u∈V	v∈V
where w(k)〜N(0, I), C is a constant, and h is the sum of the initial features of the neighbors
N(u) of node u, i.e. Pv∈N(u) xv.
13
Under review as a conference paper at ICLR 2022
A.2 Analysis on the HARMON I C task
We analyze the extrapolation ability of GNNs on the harmonic task, whose exact functional form
is given as
f?(G) =	X X 1-1!-1.	(7)
v∈V u∈N (v)
Following Xu et al. (2021), we assume linear algorithmic alignment; if a neural network can simulate
a target function f by replacing MLP modules with linear functions, (i.e., the nonlinearities of the
neural network is well matches with the target function, so the neural network only has to learn the
linear (MLP) part), than it can correctly learn the target function, and thus can extrapolate well. With
this hypothesis, we proceed as follows. We assume that a GNN is in the NTK regime, that is, the
GNN is initialized in a specific way called NTK parameterization, trained via gradient descent with
small step size, and the widths of the network tend to infinty. In such case, the GNN behaves as a
solution to kernel regression with GNTK kernel. Then we convert the kernel regression problem
into a constrained optimization problem in the feature space induced from GNTK kernel, and show
that the solution for the constrained optimization problem aligns with the functional form of the
harmonic task (equation 7).
We first state the following Lemma from Xu et al. (2021) showing that a NTK kernel regression
solution can be viewed as a constrained optimization problem in the feature space.
Lemma 1 (Lemma 2 in Xu et al. (2021)). Suppose NTKtrain is the n × n kernel for training data,
NTK(x, xi) is the kernel value between test data x and training data xi, and Y is the training labels.
Let φ(x) be a feature map induced by a neural tangent kernel, for any x ∈ Rd. The solution to kernel
regression
(NTK(x, xι),..., NTK(x, Xn)) ∙ NTK-a1nY
is equivalent to φ(x)>βNTK, where βNTK is
mβin kβ k2
s.t. φ(xi)>β = yi,	for i = 1, . . . , n.	(8)
Proof. See Xu et al. (2021).
□
Theorem 1.	Assume all the nodes in G have the same scalar feature 1. Then, a GNN defined
as equation 5 trained with squared loss in the NTK regime learns the harmonic task function
(equation 7).
Proof. Assume (p+,p-, q+, q-) = (∞, 1, ∞, 0). Then every output of GNP+ goes to zero regard-
less of inputs and GNP- aligns with the target function equation 7. The feature map of GNTK in
this case can be simplified as follows:
Φ(G) = C ∙(GNP ({w(k)>%}v∈v),
X (W(Mhu)-2 (X	! 2 I(w(k) > hu > 0) ∙ hu,...
u∈V	v∈V w( ) hv
By Lemma 1, we know that in the NTK regime, the GNN f (θ, G) behaves as the solution to the
constrained optimization problem equation 8 with feature map φ(G) and coefficients β. Let βw ∈ R
be a coefficient corresponding to GNP({w>hu}u∈V) and βw0 ∈ R be a coefficient corresponding
to the other term in φ(G). Similar to Lemma 3 in Xu et al. (2021), we can combine the effect of
coefficients for w's in the same direction. For each W 〜Unif(unit sphere), we can define βw and
βw0 as the total effect of weights in the same direction with considering scaling.
14
Under review as a conference paper at ICLR 2022
βw=Z βuI (kW⅛u⅛ = 1) ∙kWk P(u),
βw=Zβu i (k⅛ = 1) ∙k⅛ P(u).
Since the dimension of the input features is 1, we only need to consider two directions of w. To get
min-norm solution, we compute the Lagrange multiplier as
min / βW + βWdP(w)
β,β0 J
s.t. G GNP ({w>hv}v∈v) ∙ βw + X (w>hu)-2
u∈V
=Xhu-1!	∀i∈ [n],
u∈Vi
-2
I(w>hu > 0) ∙ βW ∙ hudP(w)
where Gi = (Vi,Ei) is the i-th training data and W 〜N(0, 1). By KKT condition, taking the
derivative for each variable, we can get the following conditions:
n
β+ = C ∙ ∑λi ∙
i=1
∀i ∈ [n],
where β+, β+0 are the combined weights of w’s in the positive direction, β-, β-0 are the combined
weights of w ’s in the negative direction, and c is a constant.
The above conditions can be satisfied with proper λi's, so the model can fit all training data. Moreover,
since the solution φ(G)>βNTK = Pu∈V hu-1 -1 is equivalent to the functional form of the target
function equation 7, GNN defined as in equation 5 can learn the harmonic task.	□
Below, we prove GNNs with sum-aggregation and max-readout trained with squared loss in the NTK
regime cannot extrapolate well on the harmonic task.
Theorem 2.	Assume all nodes have the same scalar feature 1. Then, one-layer GNNs with sum-
aggregation and max-readout trained with squared loss in the NTK regime do not extrapolate well in
the harmonic task.
Proof. The target function of harmonic task is
f?(G)
15
Under review as a conference paper at ICLR 2022
so in order for one-layer GNNs with sum-aggregation and max-readout of the form
MLP max hv , hv is the hidden vector for the node v,
v∈V
to match the target function, MLP must learn some non-linear transform between max and the
inverse function. However, as shown in Xu et al. (2021), MLP converges to a linear function along
directions from the origin. Hence, there always exist domains for which the GNN cannot learn the
target function.	□
Similarly, we can show that one-layer GNNs with sum-aggregation and min/sum/mean-readout
cannot learn the target function for some domain, meaning that they cannot extrapolate.
B Training Details
We used the open-source implementations of Set Transformer provided by the authors. We used the
open-source implementation of SAGPool in Pytorch Geometric (Fey & Lenssen, 2019) provided by
the authors with the reported hyperparameter settings. For all other models, we used the open-source
implementations provided by the DGL framework (Wang et al., 2019).
For all models, we used the mean squared loss (MSE) as training and validation loss functions,
unless otherwise stated. We performed a grid search to find the combination of hyperparameters
that minimize the validation loss. In all experiments, we used the RMSprop optimizer (Tieleman &
Hinton, 2012) to train all models with GNP, and for all baseline models, we additionally considered
the Adam optimizer (Kingma & Ba, 2015) with default parameters (i.e., β = (0.9, 0.999)) and
β = (0.5, 0.999).
B.1 Extrapolation on Graph-level Tasks (Related to Section 4.2)
For each task, We generated Erdos-Renyi (ErdOs & Renyi, 1960) random graphs with probabilities
ranging from 0.1 to 0.9. We trained and validated our model using such graphs with at least 20 and at
most 30 nodes, and we tested on such graphs with at least 50 and at most 100 nodes, following the
procedure in Xu et al. (2021). We generated 5, 000 graphs for training, 1, 000 graphs for validation,
and 2, 500 graphs for test. For all nodes, we used the scalar 1 as the node feature.
For further experiments with different structures, we generated 2, 500 graphs of each type
among ladder graphs, 4-regular random graphs,4 random trees, expanders,5 and
BarabaSi-Albert (BA)(BarabaSi & Albert, 1999) random graphs6. They all have at least 50 and at
most 100 nodes.
Table 7 describes the hyperparameter search space for all graph-level tasks.
B.2 Extrapolation on Node-level Tasks (Related to Section 4.3)
We created 5, 000 graphs for training, 1, 000 graphs for validation, and 2, 500 graphs of each type for
test in the way described in Section 4.2. Graphs for training and validation have at least 20 and at
most 40 nodes, while those for test are larger with at least 50 and at most 70 nodes. Target nodes is
sampled uniformly at random among all nodes in each graph.
For shortest, we used the scalar 0 as the feature of the target node, and used the scalar 10 × |V |
as the feature of the other nodes. The weight of each edge is drawn uniformly at random from U (0, 5)
in training and validation graphs, and from U(0, 10) in test graphs. For bfs, we used the scalar 1 as
the feature of the target node and used the scalar 0 as the feature for the other nodes. For both tasks,
we added self-loop with edge weight 0 to every node.
TableS 8 describes the hyperparameter search space for all node-level tasks.
4The degree of every node is 4.
5We created Erdos-Renyi random graphs with probability 0.8, following the procedure in Xu et al. (2021).
6The number of edges to attach from a new node to existing nodes ranged from 0.05 × |V | to 0.4 × |V |.
16
Under review as a conference paper at ICLR 2022
Table 7: Search space for maxdegree, harmonic, and invsize tasks
Hyperparameter	Selection pool
Optimizer	RMSprop
Learning rate for p	3e-2, 1e-2, 3e-3
Learning rate for the other parameters	3e-2, 1e-2, 3e-3, 1e-3
Norm clipping	1e2, 1e4
(a) Search space for GIN with GNP
Hyperparameter	Selection pool
Optimizer	Adam, Adam with β = (0.5,0.999), RMSprop
Learning rate	3e-2, 1e-2, 3e-3, 1e-3
Norm clipping	1e2, 1e4
Number of iterations (for Set2Set)	1, 2	
(b) Search space for GIN with baseline aggregation & readout functions
Hyperparameter	Selection pool
Optimizer	Adam, Adam with β = (0.5,0.999), RMSprop
Learning rate	1e-2, 3e-3, 1e-3, 3e-4
Norm clipping	1e2, 1e4
(c) Search space for SAGPool
Table 8: Search space for bfs, shortest tasks
Hyperparameter	Selection pool
Optimizer	RMSprop
Learning rate for p	3e-2, 1e-2, 3e-3
Learning rate for the other parameters	1e-2, 3e-3, 1e-3
Norm clipping	1e2, 1e4
(a) Search space for GIN with GNP
Hyperparameter	Selection pool
Optimizer	Adam, Adam with β = (0.5,0.999), RMSprop
Learning rate	3e-2, 1e-2, 3e-3, 1e-3
Norm clipping	1e2, 1e4
(b) Search space for GIN with baseline aggregation functions
B.3 Extrapolation on Set-related Tasks (Related to Section 4.4)
For each task, we generated 4, 000 sets for training, 500 sets for validation, and 500 sets for test.
For each set, the number of elements is sampled uniformly at random from [20, 40) for training
and validation sets, and from [50,100) for test sets. For μpost, σp°st, and "map, We sampled el-
ements from N(μ, 12) where μ 〜N(0,12). For σMap，we sampled elements N(5,σ2) where
σ 〜InVGamma(1,15). As loss functions, we used MSE for μpost and σpost and used the negative
logarithm of the product7 of the likelihood and the prior for "map and σMAp.
Tables 9 describes the hyperparameter search space for all set-related tasks.
B.4 Graph Classification (Related to Section 4.7)
For original SAGpool (Lee et al., 2019b), we used the optimal hyperparameter settings shared by the
authors8. For SAGpool equipped with GNp, we used gradient clipping with a maximum gradient
norm of 1000 for the parameters of GNp, and we used a different learning rate for p of GNp. For
DD , PROTEINS, and NCI1, we used 1×, 20×, and 10× larger learning rates forp than the original
Iearning rates, respectively.____
7This product is proportional to the posterior probability
8https://docs.google.com/spreadsheets/d/1JXGNOCQkRHDCQqNarteYpEuWnkNzNq_
WFiQrIY276i0/edit?usp=sharing
17
Under review as a conference paper at ICLR 2022
Table 9: Search space for μpost, σp°st, "map, and σMAP
Hyperparameter	Selection pool
Optimizer	RMSprop
Learning rate for p	3e-2, 1e-2, 3e-3
Learning rate for the other parameters 3e-2, 1e-2, 3e-3
Norm clipping	1e4
(a)	Search space for GNP
Hyperparameter	Selection pool
Optimizer Adam, Adam with β = (0.5,0.999), RMSprop
Learning rate	3e-2, 1e-2, 3e-3, 1e-3
Norm clipping	1e4
(b)	Search space for basic operators
Hyperparameter	Selection pool
Optimizer	Adam, Adam with β = (0.5,0.999), RMSprop
Learning rate	1e-2, 1e-3, 1e-4
Norm clipping	1e4
Number of iterations (for Set2Set)	1, 2
Encoder design (for Set transformer)	2 SAB blocks, 2 ISAB blocks
(c)	Search space for Set2Set and Set Transformer
Table 10: Statistics of real-world datasets
Dataset	Number of graphs	Average number of nodes	Average number of edges
D&D	1178	284.3	715.7
PROTEINS	1113	39.06	72.82
NCI1	4110	29.87	32.30
For original ASAPool (Ranjan et al., 2020), we used the optimal hyperparameter settings shared
by the authors9. For ASAPool equipped with GNP, we used a different learning rate for p and q of
GNP. For DD and NCI1, we used 3e-2 and 3e-3 for the learning rate forp and q, respectively. For
PROTEINS, we used 1e-1 for the learning rate for p, and 1e-2 for the learning rate for q.
B.5 Influence Maximization (Related to Section 4.7)
For original MONSTOR (Ko et al., 2020), we used the optimal hyperparameter settings provided in
the paper. For the parameters of GNP, we used the RMSprop optimizer, and the learning rates were
set to 3e-2 for p and 3e-3 for q.
C Additional Experiments and Results
C.1 Graph-level Extrapolation on Real-world Datasets
(Related to Section 4.2)
We further tested the extrapolation performances of GNP and baseline approaches using real-world
graphs. For real-world graphs, we used D&D, PROTEINS, and NCI1, which were also used for graph
classification tasks in the paper. Table 10 describes statistics of datasets. For evaluation, we ignored
graphs with nodes with zero in-degrees.
In this experiment, we used a model trained using the Erdos-Renyi graphs described in Section
4.2. As seen in the Table 11, GNP showed near-perfect extrapolation performance only except for
the maxdegree task on the NCI1 dataset. Even though the average number of nodes in the D&D
dataset is approximately 10 times larger than that of the training dataset, the models trained with
GNP performed well. One of the possible reasons for the relatively high MAPE on the NCI1 dataset
is its extremely low average degree of nodes, which is roughly 2.17. Note that the training dataset
COntainS Erdos-Renyi random graphs with edge probabilities ranging from 0.1 to 0.9.
9https://github.com/malllabiisc/ASAP
18
Under review as a conference paper at ICLR 2022
Table 11: Extrapolation performances of GNP on real-world datasets. Only except for the maxdegree
task on the NCI1 dataset, GNP showed near-perfect performance.
Task	D&D	PROTEINS	NCI1
invsize	1.7±0.6	0.5±0.2	0.3±0.1
harmonic	3.4±1.1	2.3±0.3	2.4±0.7
maxdegree	3.4±1.3	2.8±1.1	22.4±12.4
Table 12: Extrapolation performance of baseline approaches on real-world datasets. Except for the
maxdegree task, there was no combination of simple pooling functions that extrapolated well.
Task	D&D	PROTEINS	NCI1
invsize (best combination) harmonic (best combination) maxdegree (ideal combination) maxdegree (2nd best combination)	100.0±0.0 (SortPooling) 552.7±1012.4 (sum, mean) 0.0±0.0 (sum, max) 10.5±0.9 (sum, mean)	93.6±2.8 (SAGPool) 110.2±3.6 (sum, max) 0.0±0.0 (sum, max) 30.8±3.9 (sum, mean)	37.9±0.0 (set2set) 39.5±1.5 (sum, max) 0.0±0.0 (sum, max) 63.9±15.0 (sum, mean)
We also measured the test error of the baseline approaches, and we reported the test MAPE of the best-
performing one in Table 12. Except for the maxdegree task, there was no combination of simple
pooling functions that extrapolated well. These results are consistent with the experiment results in
the paper. On the maxdegree task, the second best combination (among the 16 combinations of
sum, max, mean, and min) showed significantly worse extrapolation performance than the GIN
model equipped with GNP.
C.2 Graph-level Extrapolation on Graphs with Different Structure Types
(Related to Section 4.2)
We trained a GNN using graphs of one structure type at a time then measured extrapolation error on
the other structure types. In Table 13, each row denotes the test MAPEs of the model trained using
the same graph. While the model trained using ER graphs or BA graphs extrapolated well on all three
tasks, the model trained using the other graphs showed poor extrapolation performance. According
to Xu et al. (2021), the distribution of training graphs can affect the extrapolation performance, and
this can be one of the possible reasons why the model trained using 4regular, expander, tree,
ladder graphs showed poor extrapolation performance.
C.3 Graph-level Extrapolation on Graphs with Different Node Feature
Distributions (Related to Section 4.2)
In the paper, we investigated the extrapolation performances in graph-level and node-level tasks on
graphs with different sizes and structures. We also performed experiments on graphs with different
edge feature distributions for the shortest task.
We additionally performed graph-level experiments for testing extrapolation to out-of-distribution
node features. As in Xu et al. (2021), 3-dimensional node features drawn from U (0, 5) were used in
training and validation data, and those drawn from U(0, 10) were used in test data.
We reported the test error in Table 14. As shown in the table, the error was slightly larger than that in
the original settings without node features. However, the error was still reasonably low, and GNP
outperformed baseline approaches especially on the invsize and harmonic tasks.
C.4 Graph-level Extrapolation with Various Activation Functions
(Related to Section 4.2)
We performed an additional graph-level experiment with a variant of GNP for handling negative inputs
and a wider range of activation functions. Since the original GNP can only take non-negative inputs,
we replaced ReLU to the absolute function for processing the inputs and then used an activation
function. We considered ReLU, ELU, and LeakyReLU as the activation function. We compared the
extrapolation error in each setting in Table 15, and GNP with the aforementioned changes showed
performance comparable to original GNP.
19
Under review as a conference paper at ICLR 2022
Table 13: Test error on heterogeneous structures. Each row denotes the test MAPEs of the model
trained using the same graph.
	ER	BA	4regular	Expander	Tree	Ladder
ER	1.2±0.3	0.9±0.3	0.8±0.3	1.9±1.0	0.8±0.3	0.8±0.3
BA	1.4±1.3	1.1±0.7	1.1±0.5	1.9±2.7	1.0±0.5	1.1±0.5
4regular	9.5±13.7	6.0±8.6	0.6±0.4	15.8±22.9	0.8±0.2	0.7±0.3
Expander	1.2±0.4	1.3±0.4	1.9±1.8	1.0±0.5	5.8±7.3	2.8±2.8
Tree	6.1±5.9	3.7±3.7	0.9±0.4	11.2±10.5	0.9±0.4	0.9±0.4
Ladder	5.1±2.7	3.1±1.6	1.4±0.5	7.0±5.1	2.7±3.0	1.1±0.6
(a) invsize
	ER	BA	4regular	Expander	Tree	Ladder
ER	1.1±0.8	2.5±0.9	1.9±1.3	0.9±0.5	14.7±6.3	2.4±2.4
BA	8.0±2.2	2.9±0.5	2.7±0.6	13.7±4.0	7.2±3.1	2.8±2.0
4regular	80.4±0.8	62.1±0.8	1.5±1.6	92.5±0.7	165.3±19.2	39.2±5.2
Expander	162.3±62.2	382.4±152.9		1202.6±526.6	7.8±2.2	3218.2±1491.6	1658.4±741.2	
Tree	119.5±57.8	116.4±66.7	66.7±22.5	126.0±61.7	4.1±5.0	45.0±15.1
Ladder	85.8±0.4	72.4±0.6	24.8±0.9	94.8±0.2	99.8±7.3	4.0±0.9
(b) harmonic						
	ER	BA	4regular	Expander	Tree	Ladder
ER	2.5±0.4	2.1±1.1	3.4±3.7	2.3±1.1	1.9±0.6	30.7±16.9
BA	3.9±2.0	2.0±0.8	4.7±5.6	3.6±1.4	2.1±1.6	15.9±20.1
4regular	86.7±3.5		89.2±2.1	5.0±8.1	92.3±3.3	15.5±1.5	38.8±8.5
Expander	13.2±9.2		23.5±4.6	238.6±138.7	7.4±2.6	131.6±96.7	306.7±174.7
Tree	78.8±28.2	57.7±21.3	41.3±13.2	126.9±44.7	2.6±0.5	31.0±22.2
Ladder	91.2±0.2		92.6±0.1	24.1±1.3	95.4±0.0	35.3±1.1	2.1±1.5
(c) maxdegree
Table 14: Test error on graph-level tasks with different node feature distributions.
Task invsize harmonic maxdegree
Test MAPE 0.8±0.6	2.4±1.7	4.7±1.4
C.5 Test MAPE on Graph-level Tasks (Related to Section 4.2)
In Table 16, we reported test MAPEs and standard deviations for all 19 competitors and GNP on the
graph-level tasks.
C.6 Behaviors of GNP+ and GNP- for Graph-level Tasks
(Related to Section 4.5)
We analyzed the behavior of the negative GNP on three graph-level tasks that we performed in the
paper. In all experiments, we found that either GNP+ or GNP- tends to dominate the other side.
To validate the observation, we masked the output of GNP+ and GNP- for readout to 0 on the
graph-level tasks.
As seen in Table 17, masking the output of GNP- on the maxdegree task and masking the output
of GNP+ on the other tasks do not significantly affect the extrapolation performance. When we
masked the opposite part of GNP, however, the test MAPE was near 100. These results imply that
the effect of the dominated part on the output of the model is negligible. That is, when the optimal
pooling function is max, the negative GNP has almost no effect on determining the output. Similarly,
when the optimal function is GNP with (p, q) = (-1, 0), the positive GNP has almost no effect on
determining the output.
C.7 Effectivenes s of GNP on Node Classification and Graph Regression
(Related to Section 4.7)
Node classification To show the scalability of GNP to million-scale graphs, we performed node
classification on the OGBN-Arxiv dataset (Hu et al., 2020), which contains 169, 343 nodes and
1, 166, 243 edges. We compared the node classification accuracy of GCN (Kipf & Welling, 2017)
20
Under review as a conference paper at ICLR 2022
Table 15: Test error on graph-level tasks with different activation functions.
Task	invsize	harmonic	maxdegree
ReLU	0.7±0.5	5.1±0.9	7.3±2.1
LeakyReLU	0.3±0.2	4.6±1.4	5.4±1.6
ELU	0.2±0.2	5.8±0.7	6.2±3.1
Table 16: Extrapolation performances on three graph-level tasks. We reported test MAPEs and
standard deviations. Near-perfect scores are in bold, and scores significantly better than those in
completely failed cases are underlined.
Readout ∣_____________________Aggregation_____________________Il Readout I TestMAPE
sum	max	mean	min ∣∣
sum	376.1±378.0	257.1±351.3	257.1±351.3	257.1±351.3	SortPool	100.0±0.0
max	101.0±7.6	179.1±44.2	179.1±44.2	179.1±44.2	Set2Set	198.8±0.5
mean	116.9±6.6	179.9±44.7	179.1±44.2	179.9±44.7	SAGPool	178.7±10.4
min	139.6±54.2	179.1±44.2	179.1±44.2	179.1±44.2	GNP	1.2±0.3
	(a) invsize
Readout ∣	 I	sum		AggregatiOn	:	Il Readout Test MAPE max	mean	min	∣∣	∣
sum	109.6±19.0	121.1±28.9	151.5±73.1	121.1±28.9	SortPool	76.8±13.0
max	73.0±2.3	76.0±3.6	76.4±3.5	76.0±3.6	Set2Set	78.2±4.0
mean	95.7±9.5	75.9±3.6	76.3±3.5	75.9±3.6	SAGPool	26.9±21.0
min	91.1±12.2	76.0±3.6	76.3±3.5	76.0±3.6	GNP	1.1±0.8
	(b) harmonic
Readout ∣	 I	sum		AggregatiOn	:	Il Readout Test MAPE max	mean	min	∣∣	∣
sum	60.5±22.1	50.5±2.1	49.9±0.5	50.5±2.1	SortPool	0.0±0.0
max	0.0±0.0	59.7±0.3	59.7±0.3	59.7±0.3	Set2Set	0.0±0.0
mean	16.3±2.4	59.7±0.3	59.7±0.3	59.7±0.3	SAGPool	51.4±1.8
min	25.5±3.2	59.7±0.3	59.7±0.3	59.7±0.3	GNP	2.5±0.4
(c) maxdegree
and GAT (VelickOVic et al., 2018), and their variants equipped with GNP. For implementations, We
used open source implementations: GCN+linear+labels10 and GAT+FLAG11. For the ones with GNP,
since those implementations do not contain a linear layer after the last message-passing between the
nodes, we used an additional linear layer only for the last GNN layer. The increase in the number of
parameters is negligible compared to the total number of the parameters (see Table 18a). For GAT
with GNP, we reduced the hidden dimension from 256 to 128.
We performed 10 runs for each model with the designated train/val/test split and reported the test
accuracy and the standard deviation. As seen in Table 18a, GCN with GNP and GAT with GNP
showed slightly better accuracy compared to the original models.
Graph regression We compared the graph regression performance of PNA (Corso et al., 2020) and
its variant with GNP. We replaced all max, sum, mean and min aggregation in PNA with GNP. We
used the ZINC (Irwin et al., 2012) dataset and performed experiments with and without edge features.
For the implementation and the hyperparameters, we followed the implementation by the authors.12
We performed 5 runs for each model with the designated train/val/test split and reported the test
MAE and the standard deviation in Table 18b. Without edge features, PNA equipped with GNP
outperformed the original model. With edge features, however, the original model showed slightly
lower test MAE than PNA equipped with GNP.
10https://github.com/dmlc/dgl/tree/master/examples/pytorch/ogb/ogbn-arxiv
11https://github.com/devnkong/FLAG
12https://github.com/lukecavabarrett/pna
21
Under review as a conference paper at ICLR 2022
Table 17: Test error with different masking schemes
Task	without masking	masking GNP+	masking GNP-
invsize	1.2±0.3	1.1±0.1	99.6±1.0
harmonic	1.1±0.8	1.0±0.7	100.1±0.1
maxdegree	2.5±0.4	100.0±0.2	2.5±0.4
Table 18: Node classification and graph regression performance. The values in parentheses are the
reported test accuracies/MAEs and the reported standard deviations.
Model	Test accuracy	Number of parameters
GCN (original) GCN (with GNP)	0.7310±0.0014 (0.7306±0.0024) 0.7324±0.0014	238,632 240,244
GAT (original) GAT (with GNP)	0.7366±0.0017 (0.7371±0.0013) 0.7377±0.0019	1,628,440 521,084
(a) Node classification results on OGBN-Arxiv
Model	Test MAE	Number of parameters
PNA (without edge features, original) PNA (without edge features, with GNP)	0.3033±0.0116 (0.320±0.032) 0.2857±0.0360	433,395 433,443
PNA (with edge features, original) PNA (with edge features, with GNP)	0.1843±0.0090 (0.188±0.004) 0.1910±0.0122	95,111 95,163
(b) Graph regression results on ZINC
D Closed-form Solutions for S et- related Tasks
(Related to Section 4.4)
In Table 19, we provided the closed-form solutions for each task.
E Details about Influence Maximization and MONSTOR
(Related to Section 4.7)
Influence Maximization (IM) (Kempe et al., 2003) is one of the most extensively studied NP-hard
problems on social networks due to its practical applications in viral marketing and computational
epidemiology. The goal of the problem is to choose a given number of seed nodes (i.e., a set of
initially activated nodes) that maximize the influence through a given graph under a diffusion model.
In this experiment, we used the Independent Cascade (IC) model as the diffusion model. In the IC
model, each link (u, v) has an activation probability puv . When a node u is newly activated and a
neighbor v is not activated yet, the node u has exactly one chance to activate the node v with the
probability puv , and the diffusion process ends when every activated node fails to activate any new
node. In the model, the influence is the number of activated nodes after the diffusion process ends.
MONSTOR estimates the influence given a graph and a seed set. To train the model, we generated a
dataset consisting of pairs of an input graph and a set of randomly chosen seed nodes. To generate
ground-truth answers, we ran 10, 000 Monte-Carlo simulations and recorded the probability πu,i that
each node u is activated until the i-th step. We first trained the base model M to estimate πi given
πi-1 , . . . , πi-d. MONSTOR is constructed by stacking s times the base model M, and s is chosen
to minimize squared loss between the ground-truth influences and the estimated influences on the
validation set. Since influence maximization is a submodular maximization problem, we used UBLF
(Zhou et al., 2013) or CELF (Leskovec et al., 2007) equipped with MONSTOR, which greedily
selects seed nodes.
For the real-world datasets we used, we provided the statistics in Table 20.
22
Under review as a conference paper at ICLR 2022
Table 19: A closed-form solution for each task when the input set S = {χ1,χ2,…，Xn} is given.
Task Closed form solution
μpost	(σ⅛ + ； / 1	σ⅛)	∙(： -1	μ2 + σ12 Pi=ι Xi)
2 σpost	(竭+ ;	σ) 1	
μMAP	(σ12 + ；	σ)(	μf+ σ12 Pn=ι Xi)
端AP	(α + n	+ 1)-1	Ie + 2 P2ι(Xi-μ)2)
Table 20: Statistics of real-world social networks used for the influence maximization task.
Dataset		Number of nodes	Number of edges
Extended	Train	5636	31826
	Test	5413	27146
Celebrity	Train	7848	28839
	Test	7336	27699
WannaCry	Train	16246	84217
	Test	19381	85202
F Code & Data
All assets used in the paper, including the training/evaluation code and the trained models with GNP,
are contained in the supplemental material. All assets we used from DGL13 (Wang et al., 2019)
and Pytorch Geometric14 (Fey & Lenssen, 2019) are available under the Apache license 2.0 and
MIT license, respectively. The implementation of Set Transformer15 (Lee et al., 2019a) that we
used is available under the MIT License. The implementation of ASAPool16 17 18 19 (Ranjan et al., 2020)
that we used is available under the Apache license 2.0. For the other assets, we were unable to
find their licenses. For the SAGPool (Lee et al., 2019b) implementation in Pytorch Geometric, the
dataset generators for the graph-level and node-level tasks (Xu et al., 2021), and the MONSTOR (Ko
et al., 2020) implementation in DGL, we used the code on the GitHub repositories17,18,19 shared by
the authors of the original papers. We accessed TUDataset20 (Morris et al., 2020) using PyTorch
Geometric.
13https://github.com/dmlc/dgl
14https://github.com/rusty1s/pytorch_geometric
15https://github.com/juho-lee/set_transformer
16https://github.com/malllabiisc/ASAP
17https://github.com/inyeoplee77/SAGPool
18https://github.com/jinglingli/nn-extrapolate
19https://github.com/jihoonko/asonam20-monstor
20https://chrsmrrs.github.io/datasets/
23