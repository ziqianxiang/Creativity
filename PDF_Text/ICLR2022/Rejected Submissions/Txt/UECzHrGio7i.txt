Under review as a conference paper at ICLR 2022
Robust Imitation
Learning from Corrupted
Demonstrations
Anonymous authors
Paper under double-blind review
Ab stract
We consider offline Imitation Learning from corrupted demonstrations where a
constant fraction of data can be noise or even arbitrary outliers. Classical ap-
proaches such as Behavior Cloning assumes that demonstrations are collected by
an presumably optimal expert, hence may fail drastically when learning from cor-
rupted demonstrations. We propose a novel robust algorithm by minimizing a
Median-of-Means (MOM) objective which guarantees the accurate estimation of
policy, even in the presence of constant fraction of outliers. Our theoretical analy-
sis shows that our robust method in the corrupted setting enjoys nearly the same er-
ror scaling and sample complexity guarantees as the classical Behavior Cloning in
the expert demonstration setting. Our experiments on continuous-control bench-
marks validate that existing algorithms are fragile under corrupted demonstration
while our method exhibits the predicted robustness and effectiveness.
1	Introduction
Recent years have witnessed the success of using autonomous agent to learn and adapt to complex
tasks and environments in a range of applications such as playing games (e.g. Mnih et al., 2015; Sil-
ver et al., 2018; Vinyals et al., 2019), autonomous driving (e.g. Kendall et al., 2019; Bellemare et al.,
2020), robotics (Haarnoja et al., 2017), medical treatment (e.g. Yu et al., 2019) and recommendation
system and advertisement (e.g. Li et al., 2011; Thomas et al., 2017).
Previous success for sequential decision making often requires two key components: (1) a careful
design reward function that can provide the supervision signal during learning and (2) an unlimited
number of online interactions with the real-world environment (or a carefully designed simulator) to
query new unseen region. However, in many scenarios, both components are not allowed. For ex-
ample, it is hard to define the reward signal in uncountable many extreme situations in autonomous
driving; and it is dangerous and risky to directly deploy a learning policy on human to gather infor-
mation in autonomous medical treatment (Yu et al., 2019). Therefore an offline sequential decision
making algorithm without reward signal is in demand.
Offline Imitation Learning (IL) offers an elegant way to train intelligent agents for complex task
without the knowledge of reward functions or using a simulator. Since the offline imitation learning
does not interact with the environment, in order to guide intelligent agents to correct behaviors,
it is crucial to have high quality expert demonstrations. The well-known Behavior Cloning (BC)
algorithm (Pomerleau, 1988) requires that the demonstrations given for training are all presumably
optimal and it aims to learn that mapping from state to action from expert demonstration data set.
However in real world scenario, since the demonstration is often collected from human, we cannot
guarantee that all the demonstrations we collected have high quality. An human expert can make
mistakes by accident or due to the hardness of a complicated scenario (e.g., medical diagnosis). Fur-
thermore, even an expert demonstrates a successful behavior, the recorder or the recording system
can have a chance to contaminate the data by accident or on purpose (e.g. Eykholt et al., 2018; Neff
& Nagy, 2016).
1
Under review as a conference paper at ICLR 2022
Figure 1: Reward vs. fraction of corruptions in Lunar Lander environment. Shaded region represents
one standard deviation for 50 trials. We fix the sample size N = 4000 for the demonstration data set,
and vary the fraction of corruptions up to 30%. Our algorithm Robust Behavior Cloning (RBC) on
corrupted demonstrations has nearly the same performance as BC on expert demonstrations (this is
the case when = 0), which achieves expert level. Also, it barely changes when grows larger. By
contrast, the performance of vanilla BC on corrupted demos fails drastically.
This leads to the central question of the paper:
Can the optimality assumption on expert demonstrations be weakened or even tolerate arbitrary
outliers under offline imitation learning settings?
More concretely, we consider corrupted demonstrations setting where the majority of the demon-
stration data is collected by an expert policy (presumably optimal), and the remaining data can be
even arbitrary outliers (the formal definition is presented in Definition 2.1). This has great signifi-
cance in many applications, such as automated medical diagnosis for healthcare (Yu et al. (2019))
and autonomous driving (Ma et al., 2018), where the historical data (demonstration) is often com-
plicated and noisy which requires robustness consideration.
However, the classical offline imitation learning approaches such as Behavior Cloning (BC) fails
drastically under this corrupted demonstration settings. We illustrated this phenomenon in Figure 1.
We use BC on a continuous control environment, and the performance of the policy learned by BC
drops drastically as the fraction of corruptions increases in the offline demonstration data set. How-
ever, our proposed algorithm - Robust Behavior Cloning (Algorithm 1) - is resilient to corruptions
in the offline demonstrations. The detailed experimental setup is included in Section 5. We now
summarize our contributions as follows.
1.1	Main Contributions
•	(Algorithm) We consider robustness in offline imitation learning where we have corrupted demon-
strations. Our definition for corrupted demonstrations significantly weakens the presumably opti-
mal assumption on demonstration data, and can tolerate a constant -fraction of state-action pairs
to be arbitrarily corrupted. We refer to Definition 2.1 for a more precise statement.
To deal with this issue, we propose a novel algorithm Robust Behavior Cloning (Algorithm 1)
for robust imitation learning. Our algorithm works in the offline setting, without any further
interaction with the environment. The core ingredient of our robust algorithm is using a novel
median of means objective in policy estimation compared to classical Behavior Cloning. Hence,
it’s simple to implement, and computationally efficient.
•	(Theoretical guarantees) We analyze our Robust Behavior Cloning algorithm when there exists a
constant fraction of outliers in the demonstrations under the offline setting. We show that our RBC
achieves nearly the same error scaling and sample complexity compared to vanilla BC with expert
2
Under review as a conference paper at ICLR 2022
demonstrations. To this end, our algorithm guarantees robustness to corrupted demonstrations at
no cost of statistical error. This is the content of Section 4.
•	(Empirical support) We validate the predicted robustness and show the effectiveness of our al-
gorithm on different high-dimensional continuous control benchmarks - the vanilla BC is fragile
indeed with corrupted demonstrations, and our Robust Behavior Cloning achieves nearly the same
performance compared to vanilla BC with expert demonstrations. This is the content of Section 5.
2	Problem Setup
2.1	Reinforcement Learning and Imitation Learning
Markov Decision Process and Reinforcement Learning. We start the problem setup by intro-
ducing the Markov decision process (MDP). An MDP M =(S, A, r, P, μo, Yi consists of a state
space S, an action space A, an unknown reward function r : S × A → [0, Rmax], an unknown
transition kernel P : S × A → ∆(S), an initial state distribution μo ∈ ∆(S), and a discounted
factor γ ∈ (0, 1). We use ∆ to denote the probability distributions on the simplex.
An agent acts in a MDP following a policy ∏(∙∣s), which prescribes a distribution over the action
space A given each state S ∈ S. Running the policy starting from the initial distribution si 〜 μo
yields a stochastic trajectory T := {st, at, rt}1≤t≤∞, where st, at, rt represent the state, action, re-
ward at time t respectively, with at 〜∏(∙∣st) andthe next state st+i follows the unknown transition
kernel st+i 〜 P(∙∣st, at). We denote ρ∏,t ∈ ∆(S X A) as the marginal joint stationary distribution
for state, action at time step t, and we define ρπ = (1 - γ) Pi∞=i γtρπ,t as visitation distribution
for policy π. For simplicity, we reuse the notation ρπ (s) = a∈A ρπ(s, a)da to denote the marginal
distribution over state.
The goal of reinforcement learning is to find the best policy π to maximize the expected cumulative
return Jn = ET〜∏ [P∞=ι γtrt]. Common RL algorithms (e.g., please refer to SzePeSVari (2010))
requires online interaction and exploration with the environments. However, this is prohibited in the
offline setting.
Imitation Learning. Imitation learning (IL) aims to obtain a policy to mimic expert’s behavior
with demonstration data set D = {(si, ai)}iN=i where N is the sample size of D. Note that we do
not need any reward signal. Tradition imitation learning assumes perfect (or near-optimal) expert
demonstration - for simplification we assume that each state-action pair (si, ai) is drawn from the
joint stationary distribution of an expert policy πE :
(Si, ai)〜ρ∏E	(1)
Learning from demonstrations with or without online interactions has a long history (e.g., Pomerleau
(1988); Ho & Ermon (2016)). The goal of offline IL is to learn a policy πbIL = A(D) through an IL
algorithm A, given the demonstration data set D, without further interaction with the unknown true
transition dynamic P.
Behavior Cloning. The Behavior Cloning (BC) is the well known algorithm (Pomerleau, 1988)
for IL which only uses offline demonstration data without any interaction with the environment.
More specifically, BC solves the following Maximum Likelihood Estimation (MLE) problem, which
minimizes the average Negative Log-Likelihood (NLL) for all samples in offline demonstrations D:
bBC = argminɪ X -log(π(a∣s))	(2)
π∈Π N
(s,a)∈D
Recent works (Agarwal et al., 2019; Rajaraman et al., 2020; Xu et al., 2021) have shown that BC
is optimal under the offline setting, and can only be improved with the knowledge of transition
dynamic P in the worst case. Also, another line of research considers improving BC with further
online interaction of the environment (Brantley et al., 2019) or actively querying an expert (Ross
et al., 2011; Ross & Bagnell, 2014).
3
Under review as a conference paper at ICLR 2022
2.2	Learning from corrupted demonstrations
However, it is sometimes unrealistic to assume that the demonstration data set is collected through
a presumably optimal expert policy. In this paper, we propose Definition 2.1 for the corrupted
demonstrations, which tolerates gross corruption or model mismatch in offline data set.
Definition 2.1 (Corrupted Demonstrations). Let the state-action pair (si, ai)iN=1 drawn from the
joint stationary distribution of a presumably optimal expert policy πE. The corrupted demonstration
data D are generated by the following process: an adversary can choose an arbitrary -fraction
( < 0.5) of the samples in [N] and modifies them with arbitrary values. We note that is a constant
independent of the dimensions of the problem. After the corruption, we use D to denote the corrupted
demonstration data set.
This corruption process can represent gross corruptions or model mismatch in the demonstration data
set. To the best of our knowledge, Definition 2.1 is the first definition for corrupted demonstrations
in imitation learning which tolerates arbitrary corruptions.
In the supervised learning, the well-known Huber’s contamination model (Huber (1964)) considers
(x,y) iiid (1-e)P+eZ, where X ∈ Rd is the explanatory variable (feature) and y ∈ R is the response
variable. Here, P denotes the authentic statistical distribution such as Normal mean estimation or
linear regression model, and Z denotes the outliers.
Dealing with corrupted x and y in high dimensions has a long history in the robust statistics com-
munity (e.g. Rousseeuw, 1984; Chen et al., 2013; 2017; Yin et al., 2018). However, it’s only until
recently that robust statistical methods can handle constant e-fraction (independent of dimensional-
ity Rd) of outliers in x and y (Klivans et al., 2018; Prasad et al., 2020; Diakonikolas et al., 2019; Liu
et al., 2019; 2020; Shen & Sanghavi, 2019; LUgoSi & Mendelson, 2019; LecUe & Lerasle, 2020; Jalal
et al., 2020). We note that in Imitation Learning, the data collecting process for the demonstrations
does not obey i.i.d. assUmption in traditional sUpervised learning dUe to the temporal dependency.
Notations. ThroUghoUt this paper, we Use {ci}i=1,2,3 to denote the Universal positive constant.
We Utilize the big-O notation f(n) = O(g(n)) to denote that there exists a positive constant c1 and
a natUral nUmber n0 sUch that, for all n ≥ n0, we have f(n) ≤ c1g(n).
3	Our Algorithms
It is well known that the Median-of-Means (MOM) estimator achieves sUb-GaUssian concentration
boUnd for one-dimensional mean estimation even thoUgh the Underlying distribUtion only has sec-
ond moment boUnd (heavy tailed distribUtion) (interested readers are referred to textbooks sUch as
Nemirovsky & YUdin (1983); JerrUm et al. (1986); Alon et al. (1999)).
The vanilla MOM estimator for one-dimensional mean estimation works like following: (1) ran-
domly partition N samples into M batches; (2) calcUlates the mean for each batch; (3) oUtpUts the
median of these batch mean. Very recently, MOM estimators are Used for high dimensional robUst
regression (Brownlees et al., 2015; HsU & Sabato, 2016) by applying MOM estimator on the loss
function of empirical risk minimization process.
3.1	Robust B ehavior Cloning
Motivated by Using MOM estimators on the loss fUnction, we propose Definition 3.1 which Uses a
MOM objective to handle arbitrary oUtliers in demonstration data set (s, a) ∈ D.
Definition 3.1 (RobUst Behavior Cloning). We split the corrupted demonstrations D into M batches
randomly1: {Bj}M=ι, With the batch size b ≤ *.The Robust Behavior Cloning solves thefollowing
optimization
πbRBC = arg min max median (`j (π) - `j (π0)) ,	(3)
π∈Π π0∈Π 1≤j≤M
1Without loss of generality, We assume that M exactly divides the sample size N, and b = M is the batch
size.
4
Under review as a conference paper at ICLR 2022
Algorithm 1 Robust Behavior Cloning.
1:	Input: Corrupted demonstrations D
2:	Output: Robust policy πbRBC
3:	Initialize π and π0 .
4:	for t = 0 to T - 1, do
5:	Randomly partition D to M batches with the batch size b ≤ *.
6:	For each batch j ∈ [M], calculate the loss `j (π) - `j (π0) by eq. (4).
7:	Pick the batch with median loss within M batches
median ('j(π) — 'j(∏0)),
1≤j≤M	j	j
and evaluate the gradient for π and π0 using back-propagation on that batch
(i)	perform gradient descent on π.
(ii)	perform gradient ascent on π0 .
8:	end for
9:	Return: Robust policy πbRBC = π .
where the loss function `j (π) is the average Negative Log-Likelihood in the batch Bj:
`j(π) = b X Tog(WaIS)).	⑷
(s,a)∈Bj
The workhorse of Definition 3.1 is eq. (3), which uses a novel variant of Median-of-Means (MOM)
tournament procedure (Le Cam (2012); Lugosi & Mendelson (2019); LecUe & Lerasle (2020); Jalal
et al. (2020)). In eq. (4), we calculate the average Negative Log-Likelihood (NLL) for a single
batch, and πbRBC is the solution of a min-max formulation based on the batch loss `j (π). Though
our algorithm minimizes the robust version of NLL, we do not utilize the traditional iid assumption
in the supervised learning.
To gain some intuition of the formulation eq. (3), if we replace the median operator by the mean
operator, then RBC is equivalent to BC which just minimizes the empirical average of Negative
Log-Likelihood. This is due to the linearity of using the mean operator. However, this is not robust
to corrupted demonstration. Hence, we use the median operator on the loss function.
The intuition behind solving this min-max formulation is that the inner variable π0 needs to get
close to πE to maximize the difference of loss function, and the outer variable π also need to get
close to πE . Hence we can guarantee that πbRBC will be close to πE . In Section 4, we show that
under corrupted demonstrations, πbRBC in eq. (3) has the same error scaling and sample complexity
compared to πE .
In Section 4, we provide rigorous statistical guarantees for Definition 3.1. However, the objective
function eq. (3) in Definition 3.1 is not convex (in general), hence we use Algorithm 1 as a compu-
tational heuristic to solve it.
In each iteration of Algorithm 1, we randomly partition the demonstration data set D into M batches,
and calculate the loss 'j(∏) — 'j (∏0) by eq. (4). We then pick the batch BMed With the median loss,
and evaluate the gradient on that batch. We use gradient descent on π for the arg min part and
gradient ascent on π0 for the arg max part. In Section 5, we empirically show that this gradient-
based heuristic Algorithm 1 is able to minimize this objective and has good convergence properties.
As for the time complexity, when using back-propagation on one batch of samples, our RBC incurs
overhead costs compared to vanilla BC, in order to evaluate the loss function for all samples via
forward propagation.
5
Under review as a conference paper at ICLR 2022
4	Theoretical Analysis
In this section, we provide theoretical guarantees for our RBC algorithm. Since our method (Defi-
nition 3.1) directly estimates the conditional probability π(a∣s) over the offline demonstrations, our
theoretical analysis provides guarantees on Es 〜PnEllbRBC(∙∣s) - ∏e(∙∣s)∣∣tv , which upper bounds
the total variation norm compared to ∏e under the expectation of S 〜PnE. The ultimate goal of the
learned policy is to maximize the expected cumulative return, thus we then provide an upper bound
for the sub-optimality JπE - JπbRBC .
We begin the theoretical analysis by Assumption 4.1, which simplifies our analysis and is common
in literature (Agarwal et al., 2019; 2020). By assuming that the policy class Π is discrete, our upper
bounds depend on the quantity log(∣Π∣)∕N, which matches the error rates and sample complexity
for using BC with expert demonstrations (Agarwal et al., 2019; 2020).
Assumption 4.1. We assume that the policy class Π is discrete, and realizable, i.e., πE ∈ Π.
We first present Theorem 4.1, which shows that minimizing the MOM objective via eq. (3) guaran-
tees the closeness of robust policy to optimal policy in total variation distance.
Theorem 4.1.	Suppose we have corrupted demonstration data set D with sample size N from
Definition 2.1, and there exists a constant corruption ratio < 0.5. Under Assumption 4.1, let τ to
be the output objective value with bRBC in the optimization eq. (3) with the batch size b ≤ 圭,then
with probability at least 1 - c1 δ, we have
Es〜ρ∏E ∣∣bRBC(∙∣s) - ∏e(∙∣s)∣∣Tv = O (log(N≡ + T)	(5)
The proof is collected in Appendix A. We note that the data collection process does not follow the
iid assumption, hence we use martingale analysis similar to (Agarwal et al., 2019; 2020). The first
part of eq. (5) is the statistical error log(?1/'). The second part is the final objective value in the
optimization eq. (3) T which includes two parts - the first part scales with O( b), which is equivalent
to the fraction of corruption O(). The second part is the sub-optimality gap due to the solving the
non-convex optimization.
Our main theorem - Theorem 4.1 一 guarantees that a small value of the final objective implies an
accurate estimation of policy and hence we can certify estimation quality using the obtained final
value of the objective.
Next, we present Theorem 4.2, which guarantees the reward performance of the learned robust
policy πbRBC .
Theorem 4.2.	Under the same setting as Theorem 4.1, we have
T 1	<c( 1 r iog(∣∏∣∕δ)	6
JnE - JbRBC ≤ O I(1 - γ)2 V N +τ I,	(6)
with probability at least 1 - c1 δ.
The proof is also collected in Appendix A. We note that the error scaling and sample complexity of
Theorem 4.1 and Theorem 4.2 matches the vanilla BC with expert demonstrations (Agarwal et al.,
2019; 2020).
Remark 4.1. The quadratic dependency on the effective horizon ((J7)2 in the discounted setting
or H2 in the episodic setting) is widely known as the compounding error or distribution shift in
literature, which is due to the essential limitation of offline imitation learning setting. Recent work
(Rajaraman et al., 2020; Xu et al., 2021) shows that this quadratic dependency cannot be improved
without any further interaction with the environment or the knowledge of transition dynamic P.
Hence BC is actually optimal under no-interaction setting. Also, a line of research considers im-
proving BC by further online interaction with the environment or even active query of the experts
(Ross et al., 2011; Brantley et al., 2019; Ross & Bagnell, 2014). Since our work, as a robust coun-
terpart of BC, focuses on the robustness to the corruptions in the offline demonstrations setting, it
can be naturally used in online the online setting such as DAGGER (Ross et al., 2011) and Brantley
et al. (2019).
6
Under review as a conference paper at ICLR 2022
5	Experiments
In this section, we study the empirical performance of our Robust Behavior Cloning. We eval-
uate the robustness of Robust Behavior Cloning on several continuous control benchmarks simu-
lated by PyBullet Coumans & Bai (2016) simulator: HopperBulletEnv-v0, Walker2DBulletEnv-v0,
HalfCheetahBulletEnv-v0 and AntBulletEnv-v0. Actually, these tasks have true reward function al-
ready in the simulator. We will use only state observation and action for the imitation algorithm, and
we then use the reward to evaluate the obtained policy when running in the simulator.
For each task, we collect the presumably optimal expert trajectories using pre-trained agents from
Standard Baselines32. In the experiment, we use Soft Actor-Critic (Haarnoja et al. (2018)) in the
Standard Baselines3 pre-trained agents, and we consider it to be an expert.
For the continuous control environments, the action space are bounded. Hence we generate cor-
rupted demonstration data set D as follows: we first randomly choose fraction of samples, and
corrupt the action to the boundary (normally -1 or +1). We note that Definition 2.1 allows for arbi-
trary corruptions, and we choose these outliers’ action since it has the maximum effect, and cannot
be easily detected.
We compare our RBC algorithm (Algorithm 1) to a number of natural baselines: the first baseline
is directly using BC on the corrupted demonstration D without any robustness consideration. The
second one is using BC on the expert demonstrations with the same sample size. In different settings,
we fix the policy network as 2 hidden layer feed-forward Neural Network of size {500, 500} with
ReLU activation, which is standard in the baselines.
5.1	Convergence of our algorithm
We illustrate the convergence and the performance of our algorithm to support our theoretical analy-
sis. We track the performance metric of different algorithms vs. epoch number in the whole training
process. More specifically, we then evaluate current policy in the simulator for 20 trials, and obtain
the mean and standard deviation of cumulative reward for each epoch. This metric corresponds to
theoretical bounds in Theorem 4.2.
We focus on four continuous control environments, where the observation space has dimensions
around 30, and the action space has boundary [-1, 1]. We fix the sample size as 60000, and vary
the corruption fraction to be 10%, 20%. Figure 2 validates our theory that our Robust Behavior
Cloning nearly matches the performance ofBC on expert demonstrations for different environments
and corruption ratio.
5.2	Performance under different setups
This is the experiment we have shown in Section 1. In the Lunar Lander control environment, we fix
the sample size N = 4000, and then vary the fraction of corruptions . As expected, Figure 1 shows
that our RBC is resilient to a constant-fraction of outliers in the demonstrations ranging from 0 to
30%, and it achieves nearly the same performance as the BC on expert demonstrations. In contrast,
directly using BC on corrupted demonstrations obtains worse reward performance as the fraction of
outliers grows.
6	Discussions
6.1	Related Work
Imitation Learning. Behavior Cloning (BC) is the most widely-used imitation learning algorithm
(Pomerleau, 1988; Osa et al., 2018) due to its simplicity, effectiveness and scalability, and has been
widely used in practice. From a theoretical viewpoint, it has been showed that BC achieves infor-
mational optimality in the offline setting (Rajaraman et al., 2020) with no further online interactions
or the knowledge of the transition dynamic P.
2The pre-trained agents were cloned from the following repositories: https://github.com/
DLR-RM/stable-baselines3, https://github.com/DLR-RM/rl-baselines3-zoo.
7
Under review as a conference paper at ICLR 2022
With online interaction, there,s a line of research focusing on improving BC in different scenarios -
for example, Ross et al. (2011) proposed DAgger (Data Aggregation) by querying the expert policy
in the online setting. Brantley et al. (2019) proposed using an ensemble ofBC as uncertainty measure
and interacts with the environment to improve BC by taking the uncertainty into account, without
the need to query the expert. Very recently, (Xu et al., 2021; Rajaraman et al., 2021) leveraged the
knowledge of the transition dynamic P to eliminate compounding error/distribution shift issue in
BC.
Besides BC, there are other imitation learning algorithms: Ho & Ermon (2016) used generative ad-
versarial networks for distribution matching to learn a reward function; Reddy et al. (2019) provided
a reinforcement learning framework to deal with imitation learning by artificially setting the re-
ward; Ghasemipour et al. (2020) unified several existing imitation learning algorithm as minimizing
distribution divergence between learned policy and expert demonstration, just to name a few.
Offline RL. Reinforcement learning leverages the signal from reward function to train the policy.
Different from IL, offline RL often does not require the demonstration to be expert demonstration
(e.g. Fujimoto et al., 2019; Fujimoto & Gu, 2021; Kumar et al., 2020) (interested readers are referred
to (Levine et al., 2020)), and even expects the offline data with higher coverage for different sub-
optimal policies (Buckman et al., 2020; Jin et al., 2021; Rashidinejad et al., 2021). Behavior-agnostic
setting (Nachum et al., 2019; Mousavi et al., 2020) even does not require the collected data from a
single policy.
The closest relation between offline RL and IL is the learning of stationary visitation distribution,
where learning such visitation distribution does not involve with reward signal, similar to IL. A
line of recent research especially for off-policy evaluation tries to learn the stationary visitation
distribution of a given target policy (e.g. Liu et al., 2018; Nachum et al., 2019; Tang et al., 2020;
Mousavi et al., 2020; Dai et al., 2020). Especially Kostrikov et al. (2020) leverages the off-policy
evaluation idea to IL area.
Robustness in IL and RL. There are several recent papers consider corruption-robust in either RL
or IL. In RL, Zhang et al. (2021b) considers that the adversarial corruption may corrupt the whole
episode in the online RL setting while a more recent one (Zhang et al., 2021a) considers offline RL
where -fraction of the whole data set can be replaced by the outliers. However, the dependency
scales with the dimension in Zhang et al. (2021a), yet can be a constant in this paper for robust
offline IL. Many other papers consider perturbations, heavy tails, or corruptions in either reward
function (Bubeck et al., 2013) or in transition dynamic (Xu & Mannor, 2012; Tamar et al., 2014;
Roy et al., 2017).
The most related papers follow a similar setting of robust IL are (Wu et al., 2019; Tangkaratt et al.,
2020; 2021; Brown et al., 2019; Sasaki & Yamashina, 2020), where they consider imperfect or noisy
observations in imitation learning. However, their algorithms cannot handle outliers in the demon-
strations, and (Wu et al., 2019; Tangkaratt et al., 2020; 2021) require additional online interactions
with the environment. Our algorithm achieves robustness guarantee from purely offline demonstra-
tion, without the potentially costly or risky interaction with the real world environment.
6.2	Summary and Future Works
In this paper, we considered the corrupted demonstrations issues in imitation learning, and pro-
posed a novel robust algorithm, Robust Behavior Cloning, to deal with the corruptions in offline
demonstration data set. The core technique is replacing the vanilla Maximum Likelihood Estima-
tion with a Median-of-Means (MOM) objective which guarantees the policy estimation and reward
performance in the presence of constant fraction of outliers. Our algorithm has strong robustness
guarantees and works well in practice.
There are several avenues for future work: since our work focuses on the corruption in offline data
set, any improvement in online imitation learning which utilizes Behavior Cloning would benefit
from the corruption-robustness guarantees by our offline Robust Behavior Cloning. Also, it would
also be of interest to apply our algorithm for real-world environment, such as automated medical
diagnosis and autonomous driving.
8
Under review as a conference paper at ICLR 2022

0	25	50	75	100	125	150	175	200
Epochs
(a) Hopper environment with = 0.1.

Epochs
(b) Hopper environment with = 0.2.
3000-
2000-
1000-
-1000-
PJeM3α
0	25	50	75	100	125	150	175	200
Epochs
(c) HalfCheetah environment with = 0.1.
PJeM3α
(d) HalfCheetah environment with = 0.2.
0	25	50	75	100	125	150	175	200	0	25	50	75	100	125	150	175	200
Epochs	Epochs
(e) Walker2D environment with = 0.1.	(f) Walker2D environment with = 0.2.

(g) Ant environment with = 0.1.
Figure 2: Offline Imitation Learning on four different continuous control tasks with demonstration
data of size 60000. We vary the corruption ration = 10%, 20%. For every 5 epochs, we evaluate
the current policy in the environment for 20 trials, and the shaded region represents the standard
deviation. Vanilla BC on corrupted demonstrations fails to converge to expert policy. Using the
robust counterpart Algorithm 1 on corrupted demonstrations has good convergence properties. Sur-
prisingly, our RBC on corrupted demonstrations has nearly the same reward performance of using
BC on expert demonstrations.
(h) Ant environment with = 0.2.
9
Under review as a conference paper at ICLR 2022
References
Alekh Agarwal, Nan Jiang, S. Kakade, and Wen Sun. Reinforcement learning: Theory and algo-
rithms. 2019.
Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Structural complex-
ity and representation learning of low rank mdps. In Advances in Neural Information Processing
Systems, volume 33,pp. 20095-20107, 2020.
Noga Alon, Yossi Matias, and Mario Szegedy. The space complexity of approximating the frequency
moments. Journal of Computer and system sciences, 58(1):137-147, 1999.
Marc G Bellemare, Salvatore Candido, Pablo Samuel Castro, Jun Gong, Marlos C Machado, Sub-
hodeep Moitra, Sameera S Ponda, and Ziyu Wang. Autonomous navigation of stratospheric bal-
loons using reinforcement learning. Nature, 588(7836):77-82, 2020.
Kiante Brantley, Wen Sun, and Mikael HenafL Disagreement-regularized imitation learning. In
International Conference on Learning Representations, 2019.
Daniel Brown, Wonjoon Goo, Prabhat Nagarajan, and Scott Niekum. Extrapolating beyond sub-
optimal demonstrations via inverse reinforcement learning from observations. In International
conference on machine learning, pp. 783-792. PMLR, 2019.
Christian Brownlees, Emilien Joly, and Gabor Lugosi. Empirical risk minimization for heavy-tailed
losses. The Annals of Statistics, 43(6):2507-2536, 2015.
Sebastien Bubeck, Nicolo Cesa-Bianchi, and Gabor Lugosi. Bandits with heavy tail. IEEE Trans-
actions on Information Theory, 59(11):7711-7717, 2013.
Jacob Buckman, Carles Gelada, and Marc G Bellemare. The importance of pessimism in fixed-
dataset policy optimization. In International Conference on Learning Representations, 2020.
Yudong Chen, Constantine Caramanis, and Shie Mannor. Robust sparse regression under adversarial
corruption. In International Conference on Machine Learning, pp. 774-782, 2013.
Yudong Chen, Lili Su, and Jiaming Xu. Distributed statistical machine learning in adversarial set-
tings: Byzantine gradient descent. Proceedings of the ACM on Measurement and Analysis of
Computing Systems, 1(2):44, 2017.
Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games,
robotics and machine learning. 2016.
Bo Dai, Ofir Nachum, Yinlam Chow, Lihong Li, Csaba Szepesvari, and Dale Schuurmans. Coindice:
Off-policy confidence interval estimation. In Advances in Neural Information Processing Systems,
2020.
Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Jacob Steinhardt, and Alistair Stew-
art. Sever: A robust meta-algorithm for stochastic optimization. In International Conference on
Machine Learning, pp. 1596-1606, 2019.
Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul
Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep learning
visual classification. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 1625-1634, 2018.
Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning.
arXiv preprint arXiv:2106.06860, 2021.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052-2062. PMLR, 2019.
Seyed Kamyar Seyed Ghasemipour, Richard Zemel, and Shixiang Gu. A divergence minimization
perspective on imitation learning methods. In Conference on Robot Learning, pp. 1259-1277.
PMLR, 2020.
10
Under review as a conference paper at ICLR 2022
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. In International Conference on Machine Learning, pp. 1352-1361.
PMLR, 2017.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International confer-
ence on machine learning, pp. 1861-1870. PMLR, 2018.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in neural
information processing systems, 29:4565-4573, 2016.
Daniel Hsu and Sivan Sabato. Loss minimization and parameter estimation with heavy tails. The
Journal of Machine Learning Research, 17(1):543-582, 2016.
P. J. Huber. Robust estimation of a location parameter. Annals of Mathematical Statistics, 35:
492-518, 1964.
Ajil Jalal, Liu Liu, Alexandros G Dimakis, and Constantine Caramanis. Robust compressed sensing
using generative models. Advances in Neural Information Processing Systems, 2020.
Mark R Jerrum, Leslie G Valiant, and Vijay V Vazirani. Random generation of combinatorial struc-
tures from a uniform distribution. Theoretical Computer Science, 43:169-188, 1986.
Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In
International Conference on Machine Learning, pp. 5084-5096. PMLR, 2021.
Alex Kendall, Jeffrey Hawke, David Janz, Przemyslaw Mazur, Daniele Reda, John-Mark Allen,
Vinh-Dieu Lam, Alex Bewley, and Amar Shah. Learning to drive in a day. In 2019 International
Conference on Robotics and Automation (ICRA), pp. 8248-8254. IEEE, 2019.
Adam Klivans, Pravesh K Kothari, and Raghu Meka. Efficient algorithms for outlier-robust regres-
sion. In Conference On Learning Theory, pp. 1420-1430. PMLR, 2018.
Ilya Kostrikov, Ofir Nachum, and Jonathan Tompson. Imitation learning via off-policy distribution
matching. In International Conference on Learning Representations, 2020.
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline
reinforcement learning. arXiv preprint arXiv:2006.04779, 2020.
Lucien Le Cam. Asymptotic methods in statistical decision theory. Springer Science & Business
Media, 2012.
GUillaUme LecUe and MatthieU Lerasle. Robust machine learning by median-of-means: theory and
practice. The Annals of Statistics, 48(2):906-931, 2020.
Sergey Levine, Aviral KUmar, George TUcker, and JUstin FU. Offline reinforcement learning: TUto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
Lihong Li, Wei ChU, John Langford, and XUanhUi Wang. Unbiased offline evalUation of contextUal-
bandit-based news article recommendation algorithms. In Proceedings of the 4th International
Conference on Web Search and Data Mining (WSDM), pp. 297-306, 2011.
LiU LiU, Tianyang Li, and Constantine Caramanis. High dimensional robUst m-estimation: Arbitrary
corrUption and heavy tails. arXiv preprint arXiv:1901.08237, 2019.
LiU LiU, Yanyao Shen, Tianyang Li, and Constantine Caramanis. High dimensional robUst sparse
regression. In International Conference on Artificial Intelligence and Statistics, pp. 411-421.
PMLR, 2020.
Qiang LiU, Lihong Li, Ziyang Tang, and Dengyong ZhoU. Breaking the cUrse of horizon: Infinite-
horizon off-policy estimation. In Advances in Neural Information Processing Systems, pp. 5356-
5366, 2018.
Gabor LUgosi and Shahar Mendelson. Risk minimization by median-of-means toUrnaments. Journal
of the European Mathematical Society, 22(3):925-965, 2019.
11
Under review as a conference paper at ICLR 2022
Xiaobai Ma, Katherine Driggs-Campbell, and Mykel J Kochenderfer. Improved robustness and
safety for autonomous vehicle control with adversarial reinforcement learning. In 2018 IEEE
Intelligent Vehicles Symposium (IV), pp. 1665-1671. IEEE, 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529-533, 2015.
Ali Mousavi, Lihong Li, Qiang Liu, and Denny Zhou. Black-box off-policy estimation for infinite-
horizon reinforcement learning. In International Conference on Learning Representations, 2020.
Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of
discounted stationary distribution corrections. In Advances in Neural Information Processing
Systems, pp. 2318-2328, 2019.
Gina Neff and Peter Nagy. Automation, algorithms, and politics— talking to bots: Symbiotic agency
and the case of tay. International Journal of Communication, 10:17, 2016.
Arkadii Semenovich Nemirovsky and David Borisovich Yudin. Problem complexity and method
efficiency in optimization. 1983.
Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J Andrew Bagnell, Pieter Abbeel, Jan Peters, et al.
An algorithmic perspective on imitation learning. Foundations and Trends® in Robotics, 7(1-2):
1-179, 2018.
D. Pomerleau. Alvinn: An autonomous land vehicle in a neural network. In Advances in Neural
Information Processing Systems, 1988.
Adarsh Prasad, Arun Sai Suggala, Sivaraman Balakrishnan, and Pradeep Ravikumar. Robust estima-
tion via robust gradient estimation. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 82(3):601-627, 2020.
Nived Rajaraman, Lin Yang, Jiantao Jiao, and Kannan Ramchandran. Toward the fundamental limits
of imitation learning. Advances in Neural Information Processing Systems, 33, 2020.
Nived Rajaraman, Yanjun Han, Lin F Yang, Kannan Ramchandran, and Jiantao Jiao. Provably
breaking the quadratic error compounding barrier in imitation learning, optimally. arXiv preprint
arXiv:2102.12948, 2021.
Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offline rein-
forcement learning and imitation learning: A tale of pessimism. arXiv preprint arXiv:2103.12021,
2021.
Siddharth Reddy, Anca D Dragan, and Sergey Levine. Sqil: Imitation learning via reinforcement
learning with sparse rewards. In International Conference on Learning Representations, 2019.
Stephane Ross and J Andrew Bagnell. Reinforcement and imitation learning via interactive no-regret
learning. arXiv preprint arXiv:1406.5979, 2014.
StePhane Ross, Geoffrey J. Gordon, and J. Bagnell. A reduction of imitation learning and structured
prediction to no-regret online learning. In AISTATS, 2011.
Peter J Rousseeuw. Least median of squares regression. Journal of the American statistical associ-
ation, 79(388):871-880, 1984.
Aurko Roy, Huan Xu, and Sebastian Pokutta. Reinforcement learning under model mismatch. In
Advances in Neural Information Processing Systems, volume 30, 2017.
Fumihiro Sasaki and Ryota Yamashina. Behavioral cloning from noisy demonstrations. In Interna-
tional Conference on Learning Representations, 2020.
Yanyao Shen and Sujay Sanghavi. Learning with bad training data via iterative trimmed loss mini-
mization. In International Conference on Machine Learning, PP. 5739-5748. PMLR, 2019.
12
Under review as a conference paper at ICLR 2022
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement
learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140-
1144, 2018.
Csaba Szepesvari. Algorithms for reinforcement learning. Synthesis lectures on artificial intelli-
gence and machine learning, 4(1):1-103, 2010.
Aviv Tamar, Shie Mannor, and Huan Xu. Scaling up robust mdps using function approximation. In
International conference on machine learning, pp. 181-189. PMLR, 2014.
Ziyang Tang, Yihao Feng, Lihong Li, Dengyong Zhou, and Qiang Liu. Doubly robust bias reduction
in infinite horizon off-policy estimation. In International Conference on Learning Representations
(ICLR), 2020.
Voot Tangkaratt, Bo Han, Mohammad Emtiyaz Khan, and Masashi Sugiyama. Variational imitation
learning with diverse-quality demonstrations. In International Conference on Machine Learning,
pp. 9407-9417. PMLR, 2020.
Voot Tangkaratt, Nontawat Charoenphakdee, and Masashi Sugiyama. Robust imitation learning
from noisy demonstrations. In International Conference on Artificial Intelligence and Statistics,
pp. 298-306. PMLR, 2021.
Philip S. Thomas, Georgios Theocharous, Mohammad Ghavamzadeh, Ishan Durugkar, and Emma
Brunskill. Predictive off-policy policy evaluation for nonstationary decision problems, with appli-
cations to digital marketing. In Proceedings of the 31st AAAI Conference on Artificial Intelligence,
pp. 4740-4745, 2017.
A. Tsybakov. Introduction to nonparametric estimation. In Springer series in statistics, 2009.
Sara van de Geer. Empirical Processes in M-estimation, volume 6. Cambridge university press,
2000.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, JUny-
oung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster
level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.
Yueh-Hua Wu, Nontawat Charoenphakdee, Han Bao, Voot Tangkaratt, and Masashi Sugiyama. Imi-
tation learning from imperfect demonstration. In International Conference on Machine Learning,
pp. 6818-6827. PMLR, 2019.
Huan Xu and Shie Mannor. Distributionally robust markov decision processes. Mathematics of
Operations Research, 37(2):288-300, 2012.
Tian Xu, Ziniu Li, and Yang Yu. Error bounds of imitating policies and environments for reinforce-
ment learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.
Dong Yin, Yudong Chen, Kannan Ramchandran, and Peter Bartlett. Byzantine-robust distributed
learning: Towards optimal statistical rates. In Proceedings of the 35th International Conference
on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 5650-5659.
PMLR, 10-15 Jul 2018.
Chao Yu, Jiming Liu, and Shamim Nemati. Reinforcement learning in healthcare: A survey. arXiv
preprint arXiv:1908.08796, 2019.
Tong Zhang. From -entropy to kl-entropy: Analysis of minimum information complexity density
estimation. The Annals of Statistics, 34(5):2180-2210, 2006.
Xuezhou Zhang, Yiding Chen, Jerry Zhu, and Wen Sun. Corruption-robust offline reinforcement
learning. arXiv preprint arXiv:2106.06630, 2021a.
Xuezhou Zhang, Yiding Chen, Xiaojin Zhu, and Wen Sun. Robust policy gradient against strong
data corruption. arXiv preprint arXiv:2102.05800, 2021b.
13
Under review as a conference paper at ICLR 2022
A Proofs
The analysis of maximum likelihood estimation is standard in i.i.d. setting for the supervised learn-
ing setting (van de Geer, 2000). In our proofs of the robust offline imitation learning algorithm,
the analysis for the sequential decision making leverages the martingale analysis technique from
(Zhang, 2006; Agarwal et al., 2020).
Our Robust Behavior Cloning (Definition 3.1) solves the following optimization
πbRBC = arg min max median (`j (π) - `j (π0)) ,	(7)
π∈Π π0∈Π 1≤j≤M
where the loss function 'j (∏) is the average Negative Log-Likelihood in the batch Bj:
`j(π) = b X TOg(WaIS)).	⑻
(s,a)∈Bj
This can be understood as a robust counterpart for the maximum likelihood estimation in sequential
decision process.
With a slight abuse of notation, we use xi and yi to denote the observation and action, and the
underlying unknown expert distribution is yi 〜p(∙∣Xi) andp(y∣x) = f *(x, y). Following Assump-
tion 4.1, We have the realizable f * ∈ F, and the discrete function class satisfies ∣F∣ < ∞.
Let D denote the data set and let D0 denote a tangent sequence {x0i, yi0}|iD=|1. The tangent sequence is
defined as Xi 〜 Di(xi：i-i,yi：i-i) and yi 〜p(∙∣χi). Note here that Xi follows from the distribution
Di , and depends on the original sequence, hence the tangent sequence is independent conditional on
D.
For this martingale process, we first introduce a decoupling Lemma from Agarwal et al. (2020).
Lemma A.1. [Lemma 24 in Agarwal et al. (2020)] Let D be a dataset, and let D0 be a tangent se-
quence. Let Γ(f, D) = (xy)∈D φ(f, (X, y)) be any function which can be decomposed additively
across samples in D. Here, φ is any function of f and sample (X, y). Let f = f(D) be any estimator
taking the dataset D as input and with range F. Then we have
ED hexp Γ(fb, D) - lOg ED0 exp(Γ(fb, D0)) - lOg |F |i ≤ 1.
Then we present a Lemma which upper bounds the TV distance via a loss function closely related
to KL divergence. Such bounds for probabilistic distributions are discussed extensively in literature
such as Tsybakov (2009).
Lemma A.2. [Lemma 25 in Agarwal et al. (2020)] For any two conditional probability densities
f1 , f2 and any state distribution D ∈ ∆(X ) we have
Ex〜Dkfι(x,∙)- f2(x, ∙)kTv
≤ -2 log Ex〜D,y〜f2(∙∣x)exp
-2log
f2(x,y)∖
fι(χ,y)J
A.1 Proof of Theorem 4.1
With these Lemmas in hand, we are now equipped to prove our main theorem (Theorem 4.1), which
guarantees the solution πbRBC of eq. (3) is close to the optimal policy πE in TV distance.
Theorem A.1 (Theorem 4.1). Suppose we have corrupted demonstration data set D with sample
size N from Definition 2.1, and there exists a constant corruption ratio < 0.5. Under Assump-
14
Under review as a conference paper at ICLR 2022
tion 4.1, let τ to be the output objective value with πbRBC in the optimization eq. (3) with the batch
size b ≤ 圭,then with probability at least 1 一 cιδ, we have
Es〜PnEIlbRBC(∙∣s) - ∏e(∙∣s)∣∣Tv = O flog≡≡ + T
Proof of Theorem 4.1. En route to the proof of Theorem 4.1, we keep using the notations in
Lemma A.1 and Lemma A.2, where the state observation is x, the action is y, and the discrete
function class is F .
Similar to Agarwal et al. (2020), we first note that Lemma A.1 can be combined with a simple
Chernoff bound to obtain an exponential tail bound. With probability at least 1 一 c1 δ, we have
-log Edo exp(Γ(f, D0)) ≤ -Γ(f, D) + log |F| + log(1∕δ).	⑼
Our proof technique relies on lower bounding the LHS of eq. (9), and upper bounding the RHS
eq. (9).
Let the batch size b ≤ +, which is a constant in Definition 3.1, then the number of batches M ≥
3N such that there exists at least 66% batches without corruptions.
In the definition of RBC (Definition 3.1), we solve
πbRBC = arg min max median (`j (π) - `j (π0)) .	(10)
π∈Π π0∈Π 1≤j≤M
Notice that since πE is one feasible solution of the inner maximization step eq. (10), we can
choose π0 = πE . Now we consider the objective function which is the difference of Negative
Log-Likelihood between f and f *, i.e., 'j (f) 一 'j (f *), defined in eq. (4) where
'j(π) = b X -log(n(a|s)).
(s,a)∈Bj
Hence, we choose Γ(f, D) in Lemma A.1 as
Γj (f,D) = NN X 一 2 log
f *(χi,yi)
f (χi,yi)
i∈Bj
N
2b工
i∈Bj
(logf(xi,yi) 一 logf*(xi,yi)) ,
which is the difference ofNegative Log-Likelihood N (' (f *) —' (f ))/2 evaluated on a single batch
Bj, j ∈ [M]. This is actually the objective function on a single batch appeared in eq. (3).
Lower bound for the LHS of eq. (9). We apply the concentration bound eq. (9) for such uncor-
rupted batches, hence the majority of all batches satisfies eq. (9). For those batches, the LHS of
15
Under review as a conference paper at ICLR 2022
eq. (9) can be lower bounded by the TV distance according to Lemma A.2.
- log ED0
exp IT X -2l°g 公 PD
i∈Bj	f(xi, yi)
(i)	N	11	f*(x,y)
=-Ti⅛ log 叽，。〜%exp -- 2log J^
(ii)	N	2
≥ 2b EEx~DJf (X, ∙)- f (χ, ∙)∣∣τv
i∈Bj	TV
(11)
1	… r ,1	∙	1	1	1	,	公…，F , ,I F	1∙	,	1	∙	1 z∙∙x
where (i) follows from the independence between f and D0 due to the decoupling technique, and (ii)
follows from Lemma A.2, which is an upper bound of the Total Variation distance.
Upper bound for the RHS of eq. (9). Note that the objective is the median of means of each
batches and f * is one feasible solution of the inner maximization step eq. (10). Since T is the output
objective value with bbRBC in the optimization eq. (3), this implies that 'Med(∏) - 'Med(∏0) ≤ T for
the median batch BMed, which is equivalent to -ΓMed(f, D) ≤ Nt/2.
Hence for the median batch BMed, the RHS of eq. (9) can be upper bounded by
-ΓMed(f, D) + log |F| + log(1∕δ) ≤ log |F| + log(1∕δ) + Nt/2.	(12)
Putting together the pieces eq. (11) and eq. (12) for BMed, we have
Es〜p∏e 版RBC(∙∣s) - ∏e(∙∣s)∣∣Tv = O (IOg(F”δ) + τ),
with probability at least 1 - c1δ.
□
A.2 Proof of Theorem 4.2
With the supervised learning guarantees Theorem 4.1 in hand, which provides an upper bound for
Es 〜PnJbRBC《⑸-πe (1S)IITV, we are now able to present the suboptimality guarantee of the
reward for πbRBC. This bound directly corresponds to the reward performance ofa policy.
Theorem A.2 (Theorem 4.2). Under the same setting as Theorem 4.1, we have
J J <o(	1 rlog(∣F∣∕δ)	!
JnE - JnRBC ≤ O 1(1 - γ)2 V N + T),
with probability at least 1 - c1δ.
Proof of Theorem 4.2. This part is similar to Agarwal et al. (2019), and we have
(1 - γ)(JπE
-JnRBC ) = Es~ρ∏E Ea〜∏E(∙∣s) a	(s, a)
≤ 占1EgpnE WRBC(忖-凝(忖口2
=占qE
~pnE忻 RBC(∙∣s)-∏E(∙∣s)kTv,
16
Under review as a conference paper at ICLR 2022
where We use the fact that su-Ps,a,∏ | Aπ (s, a) | ≤ ι-1γ for the advantage function and the reward is
always bounded between 0 and 1.
Combining Theorem 4.1, we have
J _ J ≤ O(	1	Jlog(∣F∣∕δ)’
JπE	JnRBC ≤ O I (1 - γ)2 V	N +
with probability at least 1 - c1δ.
□
17