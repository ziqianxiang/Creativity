Under review as a conference paper at ICLR 2022
Neural Tangent Kernel Empowered
Federated Learning
Anonymous authors
Paper under double-blind review
Ab stract
Federated learning (FL) is a privacy-preserving paradigm where multiple partic-
ipants jointly solve a machine learning problem without sharing raw data. Un-
like traditional distributed learning, a unique characteristic of FL is statistical het-
erogeneity, namely, data distributions across participants are different from each
other. Meanwhile, recent advances in the interpretation of neural networks have
seen a wide use of neural tangent kernel (NTK) for convergence and generaliza-
tion analyses. In this paper, we propose a novel FL paradigm empowered by the
NTK framework. The proposed paradigm addresses the challenge of statistical
heterogeneity by transmitting update data that are more expressive than those of
the traditional FL paradigms. Specifically, sample-wise Jacobian matrices, rather
than model weights/gradients, are uploaded by participants. The server then con-
structs an empirical kernel matrix to update a global model without explicitly per-
forming gradient descent. We further develop a variant with improved communi-
cation efficiency and enhanced privacy. Numerical results show that the proposed
paradigm can achieve the same accuracy while reducing the number of communi-
cation rounds by an order of magnitude compared to federated averaging.
1 Introduction
Federated learning (FL) has emerged as a popular paradigm involving a large number of workers col-
laboratively solving a machine learning problem (Kairouz et al., 2021). In a typical FL framework,
a server broadcasts a global model to selected workers and collects model updates without needing
to access the raw data. One popular algorithm is known as federated averaging (FedAvg) (McMa-
han et al., 2017), in which workers perform stochastic gradient descent (SGD) to update the local
models and upload the weight vectors to the server. A new global model is constructed on the server
by averaging the received weight vectors.
Li et al. (2020) characterized some unique challenges of FL. First, client data are generated lo-
cally and remain decentralized, which implies that they may not be independent and identically
distributed (IID). Prior works have shown that statistical heterogeneity can negatively influence the
convergence of FedAvg (Zhao et al., 2018). This phenomenon may be explained that local updating
under data heterogeneity will cause cost-function inconsistency (Wang et al., 2020). More chal-
lengingly, the learning procedure is susceptible to system heterogeneity, including the diversity of
hardware, battery power, and network connectivity. Local updating schemes often exacerbate the
straggler issue caused by heterogeneous system characteristics.
Recent studies have proposed various strategies to alleviate the statistical heterogeneity. One possi-
ble solution is to share a globally available dataset with participants to reduce the distance between
client-data distributions and the population distribution (Zhao et al., 2018). In practice, though, such
a dataset may be unavailable or too small to meaningfully compensate for the heterogeneity. Some
researchers replaced the coordinate-wise weight averaging strategy in FedAvg with nonlinear ag-
gregation schemes (Wang et al., 2020; Chen & Chao, 2021). The nonlinear aggregation relies on
a separate optimization routine, which can be elusive, especially when the algorithm does not con-
verge well. Another direction is to modify the local objectives or local update schemes to cancel the
effects of client drift (Li et al., 2020; Karimireddy et al., 2020). However, some studies reported that
these methods are not consistently effective, and may perform worse than FedAvg when evaluated
in various settings (Reddi et al., 2021; Haddadpour et al., 2021; Chen & Chao, 2021).
1
Under review as a conference paper at ICLR 2022
In this work, we present a neural tangent kernel empowered federated learning (NTK-FL) paradigm.
Given a fixed number of communication rounds, NTK-FL outperforms state-of-the-art methods in
terms of test accuracy. We summarize our contributions as follows.
•	We propose a novel FL paradigm without requiring workers to perform gradient descent. To the
best of our knowledge, this is the first work using the NTK method to replace gradient descent in
FL algorithms.
•	Our scheme inherently solves the non-IID data problem of FL. Compared with FedAvg, it is robust
to different degrees of data heterogeneity and has a consistently fast convergence speed. We verify
the effectiveness of the paradigm theoretically and experimentally.
•	We add communication-efficient and privacy-preserving features to the paradigm and develop
CP-NTK-FL by combining strategies such as random projection and data subsampling. We show
that some strategies can also be applied to traditional FL methods. Although such methods cause
performance degradation when applied to FedAvg, they only slightly worsen the model accuracy
when applied to the proposed CP-NTK-FL.
2 Related Work
Neural Tangent Kernel. Jacot et al. (2018) showed that training an infinitely wide neural net-
work with gradient descent in the parameter space is equivalent to kernel regression in the function
space. Lee et al. (2019) used a first-order Taylor expansion to approximate the neural network output
and derived the training dynamics in a closed form. Chen et al. (2020) established the generalization
bounds for a two-layer over-parameterized neural network with the NTK framework. The NTK com-
putation has been extended to convolutional neural networks (CNNs) (Arora et al., 2019), recurrent
neural networks (RNNs) (Alemohammad et al., 2021), and even to neural networks with arbitrary
architectures (Yang & Littwin, 2021). Empirical studies have also provided a good understanding
of the wide neural networks training (Lee et al., 2020).
Federated Learning. FL aims to train a model with distributed workers without transmitting local
data (McMahan et al., 2017; Kairouz et al., 2021). FedAvg has been proposed as a generic solution
with many theoretical analyses and implementation variants. Recent studies have shown a growing
interest in improving its communication efficiency, privacy guarantees, and robustness to hetero-
geneity. To reduce communication cost, gradient quantization and sparsification were incorporated
into FedAvg (Reisizadeh et al., 2020; Sattler et al., 2019). From the security perspective, Zhu et al.
(2019) showed that sharing gradients may cause privacy leakage. To address this challenge, differen-
tially private federated optimization and decentralized aggregation methods were developed (Girgis
et al., 2021; Cheng et al., 2021). Other works put the focus on the statistical heterogeneity issue and
designed various methods such as adding regularization terms to the objective function (Li et al.,
2020; Smith et al., 2017). In this work, we focus on a novel FL paradigm where the global model
is derived based on the NTK evolution. We show that the proposed NTK-FL is robust to statistical
heterogeneity by design, and extend it to a variant with improved communication efficiency and
enhanced privacy.
Kernel Methods in Federated Learning. The NTK framework has been mostly used for con-
vergence analyses in FL. Seo et al. (2020) studied two knowledge distillation methods in FL and
compared their convergence properties based on the neural network function evolution in the NTK
regime. Li et al. (2021) incorporated batch normalization layers to local models, and provided the-
oretical justification for its faster convergence by studying the minimum nonnegative eigenvalue of
the tangent kernel matrix. Huang et al. (2021) directly used the NTK framework to analyze the con-
vergence rate and generalization bound of two-layer ReLU neural networks trained with FedAvg. Su
et al. (2021) studied the convergence behavior of a set of FL algorithms in the kernel regression set-
ting. In comparison, our work does not focus on pure convergence analyses of existing algorithms.
We propose a novel FL framework by replacing the gradient descent with the NTK evolution.
2
Under review as a conference paper at ICLR 2022
3 Background and Preliminaries
We use lowercase nonitalic boldface, nonitalic boldface capital, and italic boldface capital letters to
denote vectors, matrices, and tensors, respectively. For example, for column vectors aj∈ RM, j ∈
{1, . . . , N}, A = [a1, . . . , aN] is an M × N matrix. A third-order tensor A ∈ RK×M×N can be
viewed as a concatenation of such matrices. We use a slice to denote a matrix in a third-order tensor
by varying two indices (Kolda & Bader, 2009). Take tensor A, for instance: Ai:: is a matrix of the
ith horizontal slice, and A:j: is its jth lateral slice (Kolda & Bader, 2009). Finally, the indicator
function of an event is denoted by 1 (∙).
3.1	Federated Learning Model
Consider an FL architecture where a server trains a global model by indirectly using datasets
distributed among M workers. The local dataset of the mth worker is denoted by Dm =
{(Xm,i, ym,i)}iN=m1, where (Xm,i, ym,i) is an input-output pair, drawn from a distribution Pm. The
local objective can be formulated as an empirical risk minimization over Nm training examples:
Fm(W) = N- PNm R(w; Xm,j, ym,i), where R is a sample-wise risk function quantifying the er-
ror of model with a weight vector w ∈ Rd estimating the label ym,i for an input Xm,i. The global
objective function is denoted by F (w), and the optimization problem may be formulated as:
min F(W) = 1-f X Fm(W).
w∈Rd	M
m=1
(1)
3.2	Linearized Neural Network Model
Let (xi, yi) denote a training pair, with xi ∈ Rd1 and yi ∈ Rd2, where d1 is the input dimension and
d2 is the output dimension. X , [x1, . . . , xN]> represents the input matrix andY , [y1, . . . , yN]
represents the label matrix. Consider a neural network function f : Rd1 → Rd2 parameterized by
>
a
vector w ∈ Rd, which is the vectorization of all weights for the multilayer network. Given an input
Xi,the network outputs a prediction y = f (w; xj. Let '(yi, yi) be the loss function measuring the
dissimilarity between the predicted result y and the true label y We are interested in finding an
optimal weight vector w? that minimizes the empirical loss over N training examples:
w
1N
argmin L(w； X, Y)，Nf。闻，』♦.
w	i=1
(2)
One common optimization method is the gradient descent training. Given the learning rate η, gra-
dient descent updates the weights at each time step as: w(t+1) = w(t) - ηVwL. To simplify the
notation, let f(t) (X) be the output at time step t with an input X, i.e., f(t) (X) , f(w(t); X). Following
Lee et al. (2019), we use the first-order Taylor expansion around the initial weight vector w(O) to
approximate the neural network output given an input X, i.e.,
f (t) (x) ≈f(0)(x)+J(0)(x)(w(t) -w(0)),
(3)
where J(O)(X) = [Vf(0)(x),..., Vfd0)(x)]>, with Vf(t)(x)，[∂yjt)∕∂w(t),..., ∂yjt)∕∂wdt)]>
being the gradient of the jth component of the neural network output with respect to w(t). Consider
the halved mean-squared error (MSE) loss ', namely, ' = a Pd= 1 2 (yj 一 y7-)2. Based on the
continuous-time limit, one can show that the dynamics of the gradient flow are governed by the
following differential equation:
df = -ηH(O) f (t)(X) - Y),
(4)
where f(t) (X) ∈ RN ×d2 is a matrix of concatenated output for all training examples, and H(O) is
the neural tangent kernel at time step 0, with each entry (H(O))ij equal to the scaled Frobenius inner
product of the Jacobian matrices: (H(O)) j = dl(J(O)(Xi), J(O)(Xj% . The differential equation
(4) has the closed-form solution:
f (t) (X)=(I - e-ηt H(O)) Y + e-ηtH(O) f (0)(X).
(5)
3
Under review as a conference paper at ICLR 2022
The neural network state f(t) (X) can thus be directly obtained from (5) without running the gradient
descent algorithm.
4 Proposed FL Paradigm via the NTK Framework
In this section, we present the NTK-FL paradigm (Figure 1) and then extend it to the variant CP-
NTK-FL (Figure 2) with improved communication efficiency and enhanced privacy. The detailed
algorithm descriptions are presented as follows.
4.1 NTK-FL Paradigm
NTK-FL follows four steps to update the global
model in each round. First, the server will
select a subset Ck of workers and broad-
cast to them a model weight vector w(k)
from the kth round. Here, the superscript
k is the communication round index, and it
should be distinguished from the gradient de-
scent time step t in Section 3.2. Second,
each worker will use its local training data
Dm to compute a Jacobian tensor J (mk) ∈
RNm×d2×d, ∀ m ∈ Ck, which is a concate-
nation of Nm sample-wise Jacobian matrices
(J mk) K = [vf(k) (Xgi),..., Vfdk (Xgi)]>，
i ∈ {1, . . . , Nm }. The worker will then up-
load the Jacobian tensor J(mk) ， labels Ym ， and
initial condition f(k) (Xm ) to the server. The
transmitted information corresponds to the vari-
ables in the state evolution of f(t) in (5). Third，
the server will construct a global Jacobian ten-
sor J(k) ∈ RN ×d2 ×d based on received J(mk) ’s，
with each worker contributing Nm horizontal
slices to J(k) .
aggregation server
Figure 1: Schematic of NTK-FL. Each worker
first receives the weight w(k) ， and then uploads
the Jacobian tensor J(mk) ， label Ym ， and initial
condition f(k) (Xm ). The server builds a global
kernel H(k) and performs the weight evolution
with {t1, . . . , tΨ}. We use (9) to find the best tj
and update the weight accordingly.
We use a toy example to explain the process as follows. Suppose the server selects worker 1 and
worker 2 in a certain round. Workers 1 and 2 will compute the Jacobian tensors J (1k) and J (2k) ，
respectively. The global Jacobian tensor is constructed as:
J(k) = ( J(1k,i):: , ifi∈{1,...,N1},
i::	J(2k,j):: ,j=i-N1, ifi ∈ {N1+1,...,N1+N2}.
(6)
After obtaining the global Jacobian tensor J(k) ， the (i, j)th entry of the global kernel H(k) is cal-
culated as the scaled Frobenius inner product of two horizontal slices of J(k)， i.e.， (H(k) )ij =
d2 hJ (3 J j〉f. For simplicity， Fourth， the server will perform the NTK evolution to obtain the
updated neural network function f(k+1) and weight vector w(k+1). With a slight abuse of notation，
let f (k,t) denote the neural network output at gradient descent step t in communication round k. The
neural network function evolution dynamics and weight evolution dynamics are given by:
f (k,t) = (I - e-NtH(k) ) Y(k) + e-NtH(k) f (k)
d2
w(k,t) =X(J:(jk:))>R:(jk,t) + w(k),
j=1
(7a)
(7b)
4
Under review as a conference paper at ICLR 2022
where J:(jk:) is the jth lateral slice of J(k), and R:(jk,t) is the jth column of the residual matrix R(k,t)
defined as follows:
t-1
R(k,t) , JL_ X [γ(k) - f(k,u)(χ(k))]
Nd2 u=0
(8)
Note that χ(k) = [χ1> , . . . , χC> ]> denotes a concatenation of worker training examples, and
Y(k) = [Y1> , . . . , YC> ]> denotes a concatenation of worker labels. The weight evolution in (7b)
is derived by unfolding the gradient descent steps. To update the global weight, the server performs
the evolution with various integer steps {t1, . . . , tΨ} and selects the best one with the smallest loss
value. Our goal is to minimize the training loss with a small generalization gap (Nakkiran et al.,
2020). The updated weight is decided by the following procedure:
w(k+1) , w(k),	t(k) = argmin L(f(w(k，j); X(k), Y(k))).
tj
(9)
Alternatively, if the server has an available validation dataset, the optimal number of update steps
can be selected based on the model validation performance. In practice, such a validation dataset
can be obtained from held-out workers (Wang et al., 2021). Based on the closed-form solution in
(7b), the search of t(k) over the grid {t1, . . . , tΨ} can be completed in O(Ψ) time.
Comparison of NTK-FL and Huang et al. (2021). Huang et al. (2021) presented the details
of FedAvg by letting clients use local updates and upload gradients to train a two-layer neural net-
work. In contrast, NTK-FL let each client transmit Jacobian matrices without performing local SGD
steps. The model weight is updated via NTK evolution in (7b). The main differences include: (1)
clients transmit more expressive Jacobian matrices to improve model performance in the non-IID
FL setting; (2) the computation is shifted to the server.
Robustness Against Statistical Heterogeneity. In essence, statistical heterogeneity comes from
the decentralized data of heterogeneous distributions owned by individual workers. If privacy is not
an issue, the non-IID challenge can be readily resolved by mixing all workers’ datasets and training
a centralized model. In NTK-FL, instead of building a centralized dataset, we use Jacobian matrices
to construct a global kernel H(k), which is a concise representation of gathered data points from
all selected workers. This representation is yet more expressive/less compact than that of a tradi-
tional FL algorithm. More precisely, the update being sent for NTK-FL regarding the ith training
example of the mth worker for NTK-FL is Jm = [Vfι(xm,i),..., Vfd2 (xm,i)]>, whereas the gra-
dient update being Sent for FedAvg is VL(w; xm,i, ymi) = d2 Pd= 1 (y∕m,i,j — ym,i,j) Vfj(xm,i),
a weighted sum of coordinates ofJm. By sending Jacobian matrices Jm and jointly processing them
on the server, NTK-FL delays the more aggressive data aggregation step after the communication
stage and therefore better approximates the centralized learning setting than FedAvg does.
4.2 CP-NTK-FL variant
Compared to FedAvg, NTK-FL does not incur additional client computational overhead since cal-
culating the Jacobian tensor enjoys the same communication efficiency with computing aggregated
gradients. Without locally updating weight vectors, NTK-FL is faster than FedAvg on the client
side. In this section, we focus on the perspectives of the communication efficiency and security in
terms of data confidentiality and membership privacy.
For communication, we follow the widely adopted analysis framework in wireless communication
to examine only the client uplink overhead, assuming that the downlink bandwidth is much larger
and the server will have enough transmission power (Tran et al., 2019). In NTK-FL, the client up-
link communication cost and space complexity are dominated by a third-order tensor J(mk), i.e., an
O (Nmd2d) complexity compared to O (d) in FedAvg. For security, we investigate a threat model
where a curious server may perform membership inference attacks (Nasr et al., 2018) or data re-
construction attacks (Zhu et al., 2019). Compared to the averaged gradient, sample-wise Jacobian
matrices are more expressive, which may facilitate such attacks from the aggregation server. We
extend NTK-FL by combining various tools to solve the aforementioned problems without jeopar-
dizing the performance severely. Although it is possible to incorporate these tools into FedAvg, we
will show that overall it will lead to more severe accuracy drop.
5
Under review as a conference paper at ICLR 2022
shuffling server
worker gets Zm
sends C(Jmk),
fm (Zm), Ym
aggregation server
shuffling server
performs
permutation
trusted key server
、①
key server
encrypts P
& transmits
Bm ⊂Dm
~o
the mth worker
(k)	worker receives
F ),ρ weight w(k) &
decrypts to get
the Seed P
aggregagtion
server builds
kernel H(k) &
obtains ∆w(k)
Figure 3: Training results of 300 workers via
NTK-FL and FedAvg, along with variants with
the local dataset subsampling and random pro-
jection, denoted as NTK-FL0 and FedAvg0 , re-
spectively. We train a two-layer multilayer per-
ceptron on the Fashion-MNIST dataset. The
joint effect causes more accuracy degradation in
FedAvg (red) than in NTK-FL (black).
Figure 2: Schematic of CP-NTK-FL. A trusted
key server (orange) sends an encrypted seed
E(k+m , P) with the public key k+m for random pro-
jection. The client transmits the required message
to the shuffling server (blue) for permutation.
↑
③
S
②
Jacobian Dimension Reduction. First, we let the mth worker sample a subset Bm from its dataset
Dm uniformly for the training. Let β ∈ (0, 1) denote the sampling rate, Bm contains Nm0 = βNm
data points, with the training pairs denoted by (X0m, Ym0 ). Next, we consider using a random pro-
jection to preprocess the input data via a seed shared by a trusted key server. Formally, the sampled
training examples are projected into Z0m, i.e., Z0m = X0mP, where P ∈ Rd1 ×d01 is a projection
matrix generated based on a seed P with IID standard Gaussian entries. In general, we have d01 < d1
and an non-invertible projection operation. The concept of trusted key server follows the trusted
third party in cryptography (Van Oorschot, 2020), and we assume it will not be compromised.
These two steps can already reduce the uplink communication overhead and enhance privacy. We
first examine the current Jacobian tensor J0m(k) ∈ RNm0 ×d2 ×d0. Compared with its original version
J(mk), it has reduced dimensionality at the cost of certain information loss. Meanwhile, the random
projection will defend against the data reconstruction attack, as the Jacobian tensor is now evaluated
at the projected data Z0m . We empirically verify their impact on the test accuracy in Figure 3. We
set d01 = 100 and sampling rate β = 0.4, and train a multilayer perceptron with 100 hidden nodes
on the Fashion-MNIST dataset (Xiao et al., 2017). The joint effect of these strategies is a slight
accuracy drop in NTK-FL and a nonnegligible performance degradation in FedAvg.
Jacobian Compression and Shuffling. We use a compression scheme to reduce the size of the Ja-
cobian tensor by zeroing out the coordinates with small magnitude (Alistarh et al., 2018). In addition
to the communication efficiency, this compression scheme is empirically effective against the data
reconstruction attack (Zhu et al., 2019). To further ensure the confidentiality and membership pri-
vacy, we introduce a shuffling server, inspired by some recent frameworks (Girgis et al., 2021; Cheng
et al., 2021), to permute Jacobian tensors J(mk)’s, neural network states fm(k)’s, and labels Ym’s.
Based on (7b), we denote the model update by ∆w(k) , w(k+1) - w(k) = Pjd=2 1(J:(jk:))>R:(jk,t),
which is a sum of matrix products. If rows and columns are permuted in synchronization, the weight
update ∆w(k) will remain unchanged. Considering the high dimensionality of the neural network
weight, the reconstruction attack becomes computationally infeasible. As provable differential pri-
vacy guarantee does not explicitly protect against the reconstruction attack (Zhang et al., 2020), we
leave a thorough privacy study for future work.
5 Analysis of Algorithm
In this section, we analyze the loss decay rate between successive communication rounds in NTK-
FL and make comparisons with FedAvg. Similar to Du et al. (2019) and Dukler et al. (2020), we
6
Under review as a conference paper at ICLR 2022
consider a two-layer neural network f : Rd → R of the following form to facilitate our analysis:
1n
f(x; V, c) = √n ɪ^erσ(v>x),	(10)
r=1
where X ∈ Rd1 is an input, Vr ∈ Rd1 is the weight vector in the first layer, V = [vι, ∙ ∙ ∙ , Vn],
Cr ∈ R is the weight in the second layer, and σ(∙) is the rectified linear unit (ReLU) function,
namely σ(z) = max(z, 0), applied coordinatewise. We state two assumptions as prerequisites.
Assumption 1 The first layer Vr ’s are sampled from N(0, α2 I). The second layer cr ’s are sampled
from {-1, 1} with equal probability and are kept fixed during training.
Assumption 1 gives the initial distribution of the neural network parameters. Similar assumptions
can be found in Dukler et al. (2020). We add restrictions to the input data in the next assumption.
Assumption 2 (Normalized input). The input data are normalized, i.e., kxik2 6 1, ∀ i.
For this neural network model, the (i, j)th entry of the empirical kernel matrix H(k) given in (3.2)
can be calculated as: (H(k))j = n x> Xj £；=1 l(k)ljk), where l(k)，1{(v*, Xii > 0}, and the
term cr2 is omitted according to Assumption 1. Define H∞, whose (i, j)th entry is given by:
(H∞)ij , Ev〜N(0,α2i) [x>Xj l(vτXi > 0) l(v>Xj > 0)] .	(11)
Let λ0 denote the minimum eigenvalue of H∞, which is restricted in the next assumption.
Assumption 3 The kernel matrix H∞ is positive definite, namely, λ0 > 0.
In fact, the positive-definite property of H∞ can be shown under certain conditions (Dukler et al.,
2020). For simplicity, we omit the proof details and directly assume the positive definiteness ofH∞
in Assumption 3. Next, we study the residual term kf (k)(X)-yk22 in communication round k, where
X = [X1τ, . . . , XτM]τ ∈ RN×d1 denote a concatenation of client inputs and y = [y1τ, . . . , yMτ ]τ ∈
RN denote a concatenation of client labels. We give the convergence result by analyzing how the
residual term decays between successive rounds.
Theorem 1 For the NTK-FL scheme under Assumptions 1 to 3, let the learning rate η = O (N)
and the neural network width n = Ω (卓 ln N^), then with probability at least 1 一 δ, the one-round
loss decay of NTK-FL is
t(k)
Ilf (k+1)(X) 一 y∣∣2 6 (1 一 2Nθ)	IIf (k)(X) 一 y∣∣2,	(12)
where t(k) is the number of NTK update steps defined in (9).
The proof of Theorem 1 can be found in Appendix B. By studying the asymmetric kernel matrix
caused by local update (Huang et al., 2021), we have the following theorem for FedAvg, where the
proof can be found in Appendix C.
Theorem 2 For FedAvg under Assumptions 1 to 3, let the learning rate η = O (TNICk |) and the
neural network width n = Ω (卓 ln 警)，then with probability at least 1 一 δ, the one-round loss
decay of FedAvg is
Ilf(k+I)(X)- y∣ι2 6 (ι - 2ητ⅛) ∣∣f (k)(X)一 y∣∣2,	(13)
where τ is the number of local iterations, and |Ck | is the cardinality of the worker set in round k.
Remark 1 (Fast Convergence of NTK-FL). The convergence rate of NTK-FL is faster than FedAvg.
To see this, we compare the Binomial approximation of the decay coefficient in Theorem 1 with the
decay coefficient in Theorem 2, i.e., 1 — N；Nλ0 + O (η2) < 1 一 2^)：|, where η《1 for a large
N 1 . The number of NTK update steps t(k) is chosen dynamically in (9), which is on the order of
102 to 103, whereas τ is often on the order of magnitude of10 in literature (Reisizadeh et al., 2020;
Haddadpour et al., 2021). One can verify that ηιt(k)λo is larger than η2τλ0∕∣Ck | and draw the
conclusion in (1).
1For example, if we have 100 clients, each of which has more than 100 data points, then N is on the order
of 104 . Considering the choice of the learning rate η1 , the Binomial approximation holds in (1).
7
Under review as a conference paper at ICLR 2022
(a)
(b)	(c)
Figure 4: Test accuracy versus communication round of different methods evaluated on: (a) EM-
NIST dataset, where the heterogeneity comes from feature skewness. (b) non-IID MNIST dataset
with label skewness, where the Dirichlet distribution parameter α = 0.5. (c) non-IID Fashion-
MNIST dataset with label skewness, where the Dirichlet distribution parameter α = 0.5. NTK-FL
outperforms all baseline FL algorithms in different scenarios, and achieves similar test performance
compared with the ideal centralized training case.
6 Experimental Results
Federated Settings. We use three datasets, namely, MNIST (LeCun et al., 1998), Fashion-
MNIST (Xiao et al., 2017), and EMNIST (Cohen et al., 2017) digits. All of them contain C = 10
categories. For MNIST and Fashion-MNIST, we follow Hsu et al. (2019) to simulate non-IID data
with the symmetric Dirichlet distribution (Good, 1976). Specifically, for the mth worker, we draw
a random vector qmι 〜 Dir(α), where qmι = [qm,ι,..., qm,c]> belongs to the (C - 1)-standard
simplex. Images with category k are assigned to the mth worker in proportional to (100 ∙ qm,k)%.
The heterogeneity in this setting mainly comes from label skewness. For the EMNIST dataset, it
has a federated version that splits the dataset into shards indexed by the original writer of the digits
(Kairouz et al., 2021). The heterogeneity mainly comes from feature skewness. A multilayer per-
ceptron model with 100 hidden nodes is chosen as the target neural network model. We consider a
total of 300 workers and select 20 of them with equal probability in each round.
Convergence. We empirically verify the convergence rate of the proposed method. For FedAvg,
we use the number of local iterations from {10, 20, . . . , 50} and report the best results. For NTK-FL,
we choose t(k) over the set {100, 200, . . . , 2000}. We use the following methods that are robust to
the non-IID setting as the baselines: (i) Data sharing scheme suggested by Zhao et al. (2018), where
a global dataset is broadcasted to workers for local training; the size of the global dataset is set to be
10% of the total number of local data points. (ii) Federated normalized averaging (FedNova) (Wang
et al., 2020), where the workers transmit normalized gradient vectors to the server. (iii) Central-
ized training simulation, where the server collects the data points from subset Ck of workers and
performs gradient descent to directly train the global model. Clearly, scheme (iii) achieves the per-
formance that can be considered as an upper bound of all other algorithms. The training curves over
three repetitions are shown in Figure 4. More implementation details and the results on CIFAR-10
(Krizhevsky, 2009) can be found in Appendix A. Our proposed NTK-FL method shows consistent
advantages over other methods in different non-IID scenarios.
Degree of Heterogeneity. In this experiment, we select the Dirichlet distribution parameter
α from {0.1, 0.2, 0.3, 0.4, 0.5} and simulate different degrees of heterogeneity on Fashion-MNIST
dataset. A smaller α will increase the degree of heterogeneity in the data distribution. We evaluate
NTK-FL, DataShare, FedNova, and FedAvg model test accuracy after training for 50 rounds. The
mean values over three repetitions are shown in Figure 5, where each point is obtained over five
repetitions with standard deviation less than 1%. It can be observed that NTK-FL achieves stable
test accuracy in different heterogeneous settings. In comparison, FedAvg and FedNova show a
performance drop in the small α region. NTK-FL has more advantages over baselines methods
when the degree of heterogeneity is larger.
Effect of Hyperparameters. We study the effect of the tunable parameters in CP-NTK-FL. We
change the local data sampling rate β and dimension d01, and evaluate the model test accuracy on the
non-IID Fashion-MNIST dataset (α = 0.1) after 10 communication rounds. The results are shown
in Figure 6. A larger data sampling rate β or a larger dimension d01 will cause less information loss,
8
Under review as a conference paper at ICLR 2022
Figure 5: Test accuracy versus the Dirichlet distri-
bution parameter α for different methods evaluated
on the non-IID Fashion-MNIST dataset. Reducing
the value of α will increase the degree of hetero-
geneity in the data distribution. NTK-FL is robust
to different heterogeneous data distributions, and
shows more advantages over FedAvg and FedNova
when the degree of heterogeneity is larger.
r- - 6	82.9	84.1	84.3	84.7	84.8	84.6
	83.1	83.8	84.8	84.4	84.5	85.0
ιη - 6	82.8	83.9	84.4	84.6	84.4	84.9
`t -	82.4	83.8	84.3	84.4	84.4	84.5
cn -	82.4	83.5	84.2	84.4	84.1	84.2
(N -	81.9	I 83.7		84.2	84.5	84.6
IOO		200	300			
82
83
Figure 6: CP-NTK-FL test accuracy for dif-
ferent hyperparams. A larger data sampling
rate β and a larger dimension d01 are expected
to give a higher test accuracy. In general, the
scheme is robust to different combinations of
hyperparameters.
and are expected to achieve a higher test accuracy.
to different combinations of hyperparameters.
Uplink Communication. We evaluate the
uplink communication efficiency of CP-NTK-
FL (d01 = 200, β = 0.3) by measuring the num-
ber of rounds and cumulative uplink commu-
nication cost to reach a test accuracy of 85%
on non-IID Fashion-MNIST dataset (α = 0.1).
The results over three repetitions are shown
in Table 1. Compared with federated learn-
ing with compression (FedCOM) (Haddadpour
et al., 2021), quantized SGD (QSGD) (Alis-
tarh et al., 2017), and FedAvg, CP-NTK-FL
achieves the goal within an order of magnitude
fewer iterations, which is particularly advanta-
geous for applications with nonnegligible en-
coding/decoding delays or network latency.
The results also show that the scheme is robust
Table 1: Uplink communication cost to reach 85%
on non-IID Fashion-MNIST dataset (α = 0.1).
CP-NTK-FL can achieve the target goal within
the fewest communication rounds without incur-
ring communication cost significantly.
optimization algorithms	comm. rounds	comm. cost (MB)
CP-NTK-FL	26	386
FedCOM	250	379
QSGD (4 bit)	614	465
FedAvg	284	1720
7 Conclusion and Future Work
In this paper, we have proposed an NTK empowered FL paradigm. It inherently solves the statistical
heterogeneity challenge. By constructing a global kernel based on the local sample-wise Jacobian
matrices, the global model weights can be updated via NTK evolution in the parameter space. Com-
pared with traditional algorithms such as FedAvg, NTK-FL has a more centralized training flavor by
transmitting more expressive updates. The effectiveness of the proposed paradigm has been verified
theoretically and experimentally.
In future work, it will be interesting to extend the paradigm for other neural network architectures,
such as CNNs, residual networks (ResNets) (He et al., 2016), and RNNs. It is also worthwhile to
further improve the efficiency of NTK-FL and explore its savings in wall-clock time. We believe the
proposed paradigm will provide a new perspective to solve federated learning challenges.
References
Sina Alemohammad, Zichao Wang, Randall Balestriero, and Richard Baraniuk. The recurrent neural
tangent kernel. In International Conference on Learning Representations, 2021.
9
Under review as a conference paper at ICLR 2022
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD:
Communication-efficient SGD via gradient quantization and encoding. Advances in Neural In-
formation Processing Systems, 2017.
Dan Alistarh, Torsten Hoefler, Mikael Johansson, Nikola Konstantinov, Sarit Khirirat, and Cedric
Renggli. The convergence of sparsified gradient methods. In Advances in Neural Information
Processing Systems, 2018.
Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang.
On exact computation with an infinitely wide neural net. In Advances in Neural Information
Processing Systems, 2019.
Hong-You Chen and Wei-Lun Chao. Fedbe: Making Bayesian model ensemble applicable to feder-
ated learning. In International Conference on Learning Representations, 2021.
Zixiang Chen, Yuan Cao, Quanquan Gu, and Tong Zhang. A generalized neural tangent kernel
analysis for two-layer neural networks. In Advances in Neural Information Processing Systems,
2020.
Pau-Chen Cheng, Kevin Eykholt, Zhongshu Gu, Hani Jamjoom, KR Jayaram, Enriquillo Valdez,
and Ashish Verma. Separation of powers in federated learning. arXiv preprint arXiv:2105.09400,
2021.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. EMNIST: Extending
MNIST to handwritten letters. In International Joint Conference on Neural Networks, 2017.
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2019.
Yonatan Dukler, Guido Montufar, and Quanquan Gu. Optimization theory for ReLu neural networks
trained with normalization layers. In International Conference on Machine Learning, 2020.
Antonious M. Girgis, Deepesh Data, Suhas N. Diggavi, Peter Kairouz, and Ananda Theertha Suresh.
Shuffled model of differential privacy in federated learning. In International Conference on Arti-
ficial Intelligence and Statistics, 2021.
Irving J Good. On the application of symmetric dirichlet distributions and their mixtures to contin-
gency tables. The Annals of Statistics ,4(6):1159-1189, 1976.
Farzin Haddadpour, Mohammad Mahdi Kamani, Aryan Mokhtari, and Mehrdad Mahdavi. Feder-
ated learning with compression: Unified analysis and sharp guarantees. In International Confer-
ence on Artificial Intelligence and Statistics, 2021.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In International Conference on Computer Vision and Pattern Recognition, 2016.
Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data
distribution for federated visual classification. arXiv preprint arXiv:1909.06335, 2019.
Baihe Huang, Xiaoxiao Li, Zhao Song, and Xin Yang. FL-NTK: A neural tangent kernel-based
framework for federated learning convergence analysis. arXiv preprint arXiv:2105.05001, 2021.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in Neural Information Processing Systems, 2018.
Peter Kairouz, H Brendan McMahan, Brendan Avent, AUrelien Bellet, Mehdi Bennis, et al. Ad-
vances and open problems in federated learning. Foundations and Trends in Machine Learning,
2021.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In
International Conference on Machine Learning, 2020.
10
Under review as a conference paper at ICLR 2022
Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review, 51(3):
455-500, 2009.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Master thesis, Dept. of
Comput. Sci., Univ. of Toronto, Toronto, Canada, 2009.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in Neural Information Processing Systems, 2019.
Jaehoon Lee, Samuel S. Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak,
and Jascha Sohl-Dickstein. Finite versus infinite neural networks: an empirical study. In Advances
in Neural Information Processing Systems, 2020.
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges,
methods, and future directions. IEEE Signal Processing Magazine, 37(3):50-60, 2020.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. Proceedings of Machine Learning and Sys-
tems, 2020.
Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. FedBN: Federated learn-
ing on non-iid features via local batch normalization. In International Conference on Learning
Representations, 2021.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial Intelli-
gence and Statistics, 2017.
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. In ICLR 2020 : Eighth International
Conference on Learning Representations, 2020.
Milad Nasr, Reza Shokri, and Amir Houmansadr. Machine learning with membership privacy us-
ing adversarial regularization. In ACM SIGSAC Conference on Computer and Communications
Security, pp. 634-646, 2018.
Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny,
Sanjiv Kumar, and Hugh Brendan McMahan. Adaptive federated optimization. In International
Conference on Learning Representations, 2021.
Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Ramtin Pedarsani.
Fedpaq: A communication-efficient federated learning method with periodic averaging and quan-
tization. In International Conference on Artificial Intelligence and Statistics, 2020.
Felix Sattler, Simon Wiedemann, Klaus-Robert Muller, and Wojciech Samek. Robust and
communication-efficient federated learning from non-iid data. IEEE Transactions on Neural Net-
works and Learning Systems, 31(9):3400-3413, 2019.
Hyowoon Seo, Jihong Park, Seungeun Oh, Mehdi Bennis, and Seong-Lyun Kim. Federated knowl-
edge distillation. arXiv preprint arXiv:2011.02367, 2020.
Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet Talwalkar. Federated multi-task
learning. In Advances in Neural Information Processing Systems, 2017.
Lili Su, Jiaming Xu, and Pengkun Yang. Achieving statistical optimality of federated learning:
Beyond stationary points. arXiv preprint arXiv:2106.15216, 2021.
Nguyen H Tran, Wei Bao, Albert Zomaya, Minh NH Nguyen, and Choong Seon Hong. Federated
learning over wireless networks: Optimization model design and analysis. In IEEE INFOCOM
2019-IEEE Conference on Computer Communications, pp. 1387-1395. IEEE, 2019.
11
Under review as a conference paper at ICLR 2022
Paul C Van Oorschot. Computer Security and the Internet: Tools and Jewels. Springer Nature, 2020.
Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni.
Federated learning with matched averaging. In International Conference on Learning Represen-
tations, 2020.
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H. Vincent Poor. Tackling the objective in-
consistency problem in heterogeneous federated optimization. In Advances in Neural Information
Processing Systems, 2020.
Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat,
Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A field guide to feder-
ated optimization. arXiv preprint arXiv:2107.06917, 2021.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a novel image dataset for bench-
marking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
Greg Yang and Etai Littwin. Tensor programs iib: Architectural universality of neural tangent kernel
training dynamics. In International Conference on Machine Learning, 2021.
Yuheng Zhang, Ruoxi Jia, Hengzhi Pei, Wenxiao Wang, Bo Li, and Dawn Song. The secret re-
vealer: Generative model-inversion attacks against deep neural networks. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 253-261, 2020.
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated
learning with non-iid data. arXiv preprint arXiv:1806.00582, 2018.
Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. In Advances in Neural
Information Processing Systems, 2019.
12
Under review as a conference paper at ICLR 2022
A Additional Experimental Results
We give the detailed setting of the learning rate
and batch size. For the learning rate η, we
search over the set {10-3, 3 × 10-3, 10-2, 3 ×
10-2 , 10-1 }. The learning rate is fixed dur-
ing the training. For the client batch size,
we set it to 200 for all datasets. We eval-
uate different methods, including the central-
ized training simulation, data sharing method
(Zhao et al., 2018), FedNova (Wang et al.,
2020), FedAvg (McMahan et al., 2017), and the
proposed NTK-FL on the non-IID CIFAR-10
dataset (Krizhevsky, 2009) and present the re-
sults in Figure 7. NTK-FL outperforms other
FL algorithms and shows test accuracy close to
the centralized simulation. The observation is
consistent with the results in Figure 4.
Figure 7: Test accuracy versus communication
round of different methods evaluated on the non-
IID CIFAR-10 dataset, where the Dirichlet distri-
bution parameter α = 0.1.
B Proof of Theorem 1
Let Im denote a set of indices such that for i ∈ Im, (xi, yi) ∈ Dm. We first present some lemmas
to facilitate the convergence analysis. In communication round k, define Si(k) as the set of indices
corresponding to neurons whose activation pattern is similar to its initial state for an input xi :
Sikk , {r ∈{1,...,n}∣∃ v, k v - VrO) k2 6 R, 1(0) = 1 (v>Xi > 0)}.	(14)
We upper bound the cardinality of Si in Lemma 1.
Lemma 1 Under Assumption 1 to 2, with probability at least 1 - δ, we have
..	2> nR
|Si| 6 ∖ ~ ~^~∀ V i ∈ {1,...,N}.	(15)
π δα
n
Proof. To bound |Si| = P 1 (r ∈ Si), consider an event Air defined as follows:
r=1
Air , {∃ V, kv — vr0)k2 6 R, 1(0) = 1 (v>Xi > 0)}.	(16)
Clearly, 1 (r ∈ Si) = 1 (Air). According to AssUmPtion 2, |乂|| 6 1, it can be shown that the event
Air happens if and only if |(v(r0))>Xi| 6 R based on a geometric argument. Based on Assumption 1,
We have (VrO))>Xi 〜N(0, α2). The probability of event Air is
P[Air] = P h|(v(r0))>Xi| 6 Ri	(17a)
=erf (√Ra )6 W r.	(17b)
By Markov’s inequality, we have with probability at least 1 - δ,
Xi (r ∈ Si) 6 r2 nR.	(18)
π δα
r=1
The proof is complete.	□
We bound the perturbation of the kernel matrix H(k,t) in Lemma 2.
Lemma 2 Under Assumption 1 to 2, if∀ r ∈ {1, . . . , n}, kv(rk,t) - vr(0) k2 6 R, then
kH(k，t)- H(0)k2 6 2√2NR	(19)
πδα
13
Under review as a conference paper at ICLR 2022
Proof. We have
kH(k't) - H(0)k22 6 kH(k't) - H(0)k2F
NN	2
=XXh(H(k't))ij-(H(0))iji
NN
=n xχgχj )2
i=1 j=1
'tt -1(0) ι(r)!
(20a)
(20b)
(20c)
Consider the event Air defined in (16). Let φ(kr,t)，Iik't)ljk't) - 1(0)1(?. If -Air and -Ajr
happen, clearly we have ∣φ(krt) | = 0. Therefore, the expectation of ∣φ(krt) | can be bounded as
E
P(Air ∪ Ajr) ∙ 1
6 P(Air)+P(Ajr)
① C 2R R
6 2∖ --,
(21a)
(21b)
πα
where ① comes from (17b). By Markov,s inequality, We have with probability at least 1 - δ,
礴曲6 2r∏R.
(21c)
(22)
Plugging (22) into (20c) yields
kH(k,t) - H(0) k2 6 N 8n2R2 = 8N2R2 .
2、n2 πδ2ɑ2	πδ2α2
Taking the square root on both sides completes the proof.
Lemma 3 With probability at least 1 - δ,
(23)
□
kH(0)-H∞k26N
ln(2N 2∕δ)
2n
(24)
Proof. We have
NN	2
kH(0) - H∞k22 6 kH(0) -H∞k2F = XX
(H(0))ij -(H∞)ij 2.
i=1 j=1
(25)
n
Note that (H(0))j = nx>Xj P l(0)ljr), (H(0))j ∈ [-1,1]. By Hoeffding,s inequality, we have
r=1
with probability at least 1 - δ∕n2,
(H(0))ij-(H∞)ij 6
Applying the union bound over i, j ∈ [N] yields
ln (2N 2∕δ)
2n
(26)
kH(0)-H∞k26N
ln (2N2∕δ)
2n
(27)
The proof is complete.
□
Now we are going to prove Theorem 1.
Theorem 1 FortheNTK-FLschemeunderAssumPtionsI to 3, let the learning rate η = O (λ0) and
the neural network width n = Ω (卓 ln 2N∙), then with probability at least 1 — δ, the one-round
loss decay of NTK-FL is
kf(k+1)(X)-yk22
t(k)
kf(k)(X)-yk22.
(28)
14
Under review as a conference paper at ICLR 2022
Proof. Taking the difference between successive terms yields
f(k,t+1)(xi) - f (k,t)(xi) = √1n X [crσ ((vrk,t+1))>xj - Crσ ((Vrk㈤)>x，i .	(29)
We decompose the difference term to the sum of dIi and dIiI, based on the set Si :
di , √ X [crσ ((vrk,t+1))>Xi) - Crσ ((vrk,t))>Xi)i ,	(30a)
AZn r∈Si
di1 , √ X [crσ ((vrk,t+1))>Xi) - Crσ ((vrk,t))>Xi)i .	(30b)
n r∈Si
Consider the residual term
f(k,t+1)(X) - y22	(31a)
= f(k,t+1)(X)-f(k,t)(X)+f(k,t)(X) - y22	(31b)
= f(k,t)(X)-y22+2DdI+dII,f(k,t)(X) - yE + f(k,t+1)(X) - f(k,t)(X)22.	(31c)
We will give upper bounds for the inner product terms dI, f (k,t) (X) - y , dII, f (k,t) (X) - y ,
and the difference term f (k,t+1)(X) - f (k,t)(X)22, separately. Based on the property of the set
Si , we have
d = -√ηn X Cr BvrL, Xii l(k,t)	(32a)
AZn r/Si
N
=-nN X (f (k,t)(Xj) - %) X>Xi X Cr 1(" ιjk,t)	(32b)
j=1	r/Si
N
=-N X (f(k,t)(Xj) - yj) (CH(k,t))ij- - (H⊥(k,t))ij) ,	(32c)
j=1
where (H⊥(k,t))ij is defined as
n
(H⊥Wt))ij , —X>Xj X Ii"ljr,tL	(33)
n	r∈Si
For the inner product term dI, f(k,t) (X) - y , we have
(dI,f(^)(X)- y = -N(f Wt)(X)- y)>(HS)- H⊥Wt))(f Wt)(X)- y).	(34)
Let T1 and T2 denote the following terms
T1 , -(f(k,t)(X) - y)>H(k,t) (f(k,t) (X) -y),	(35a)
T2 , (f(k,t) (X) - y)>H⊥(k,t) (f(k,t) (X) - y).	(35b)
With probability at least — - δ, T1 can be bounded as:
T1 = -(f(k,t)(X) - y)>(H(k,t) - H(0) + H(0) -H∞ +H∞)(f(k,t)(X) -y)	(36a)
6 -(f(k,t)(X) - y)>(H(k,t) - H(0))(f(k,t)(X) - y)
-(f(k,t)(X) -y)>(H(0) - H∞)(f(k,t)(X) - y) - λ0f(k,t)(X) - y22	(36b)
6 (2√∏NR + N ； - λjf (k,t)(χ)τ2,	(36c)
15
Under review as a conference paper at ICLR 2022
where ① comes from Lemma 2 and Lemma 3. To bound the term T2, consider the '2 norm of the
matrix H⊥(k,t). With probability at least 1 - δ, we have:
kH⊥(k,t) k2 6 kH⊥(k,t)kF
=(X XX (n X χ>χji(k,t)ι jk,t)!)2
i=1 j=1	r∈Si
N…①l~2 NR
6 n |Si|6 vnia，
where ① comes from Lemma 1. Therefore, with probability at least 1 - δ, we have
T 6 r2NRf (k,t)(x)-y∣∣2∙
πδα	2
Combine the results of (36c) and (38):
DdI,f (k,t)(χ) - y〉6 η ('等+r ln(2Na-λ! f (k,t)(X)-W
For the inner product term dII, f (k,t)(X) - y , we first bound kdIIk22 as follows:
kdIIk2 = X (√n X 卜σ ((vrk,t+1))>x) - Crσ ((vrk*>Xi)i)
i=1	r∈Si
①	η2 N
6 η E∣Si∣ E(crhVvrL, Xii)2
②	112 N
6 + E∣Si∣∑kVvr Lk 2 kχik2
i=1	r∈Si
6 n—~ ιSi∣2 max IlVvr L∣∣2
n	r∈[n]	r 2
6 η2兽∣∣fM)(X)-y∣∣2,
(37a)
(37b)
(37c)
(38)
(39)
(40a)
(40b)
(40c)
(40d)
(40e)
where ① comes from the LiPSchitz continuity of the ReLU function σ(∙),② holds due to CaUchy-
Schwartz inequality. Plug (18) into (40e), we have with probability at least 1 -δ :
kdIIk2 6 ∏δ2R2∣f (k,t)(X) - y∣2.	(41)
The inner product term dII, f(k,t)(X) - y can be bounded as
(d∏,fWt)(X) -y〉6 √∏δR∣∣fWt)(X)-y∣∣2.	(42)
Finally, the bound for the difference term is derived as
Nn	2
∣∣f(D(X)- fWt)(X)∣∣2 6 X (√nXCrEvrL,Xii) 6 η2∣∣f(k,t)(X) -y∣2.	(43)
Combine the results of (39), (42) and (43):
∣∣f (k,t+1)(X)-y∣∣2 6 "1 + 8√2ηR + 2ηj *N® - 2N0 + n2# ∣∣f (k，t)(X)-y∣∣2. (44)
Let R = O (δN0), n = Ω (N ln N), and n = O(N), We have
∣∣f(k,t+1)(X) - y∣∣2 6 (1 - nN) ∣∣f(k,t)(X) - y∣∣2.	(45)
Summing up over the selected number t(k) iterations completes the proof.	□
16
Under review as a conference paper at ICLR 2022
C Proof of Theorem 2
Theorem 2 For FedAvg under Assumptions 1 to 3, let the learning rate η = O (TN⅛) and the
neural network width n = Ω (卓 ln 2N∙), then with probability at least 1 一 δ, the one-round loss
decay of FedAvg is
IIf(k+I)(X)-y∣∣2 6 (1 - 岛)f (k)(χ) - y俏.	(46)
Proof. We first construct a different set of kernel matrices {Λ(k), Λ(mk,τ)} similar to Huang et al.
(2021). Let l(m^U)，l{hv(⅛,u), Xii > 0}, the (i,j)th entry of Λku and Λ(k,u) is defined as
n
(A(k，U))∙∙, -χ>χ X n(k,0)ι∣(k,u)	(47a)
(1 mn )ij	Xi Xj / v ɪimr JLjmr ,	(t+∕a)
n r=1
(Λ(k,u))ij ,(Λ(mk,u))ij,	if(Xj,yj) ∈Dm.	(47b)
Taking the difference between successive terms yields
f(k+I)(Xi)- f ⑻(Xi) = √n X &rσ ((vrk+1))>Xi) -Crσ ((vrk))>x,i .	(48)
We decompose the difference term to the sum of dIi and dIiI, based on the set Si and its complement:
dI , √n X hcrσ ((vrk+1))>Xi)-Crσ ((Vrk))>Xi)i ,
Vn r∈Si
diI , √ X hcrσ ((vrk+1))>Xi)-Crσ ((Vrk))>Xi)i .
n r∈Si
(49a)
(49b)
Consider the residual term
IIf(k+1)(X)-yII22	(50a)
= IIf(k+1)(X)-f(k)(X)+f(k)(X)-yII22	(50b)
= IIf(k)(X) -yII22+2DdI+dII,f(k)(X) -yE+IIf(k+1)(X)-f(k)(X)II22.	(50c)
We will give upper bounds for the inner product terms dI, f(k) (X) - y , dII, f(k) (X) - y , and
the difference term IIf (k+1)(X) - f (k)(X)II22, separately. For an input X ∈ Rd1, let fm(k,u)(X) ,
√n P Crσ(hv(k,u)), Xi). By the update rule ofFedAvg, the relation between the weight vector Vrk)
r=1
in successive communication rounds is:
τ-1
v(k+1) = v(k)
一舟 XX NX
m∈Ck u=0
(k,u)
r
vr(k)
ηCr
N √n∣Ck |
τ-1
X X X (燎U)(Xj)-yj)XjIjmu).
m∈Ck u=0 j∈Im
Based on the property of the set Si , we have
τ-1
di=-√n XXX cr (W+I)-Vrk), XiE 以
m m∈Ck u=0 r∈Si
τ-1
Nn⅛ XXXX (燎U)(Xj) - y)X>Xj1(k)1jmU)
m∈Ck u=0 r/Si j∈Im
τ-1
Nnn XXX (fa”) (Xj) -yj) [(Λm,u))ij - smWu))ij]
k m∈Ck U=0 j∈Im
(51a)
(51b)
(52a)
(52b)
(52c)
—
—
17
Under review as a conference paper at ICLR 2022
For the inner product term dI, f(k) (X) - y , we have
dI,f(k)(X)-y = -
τ-1
Nη-∣ X(f(k)(X) - y)>(Λ(k,u) - Λ~k,U))(燎U)(X)- y).	(53)
Let T1 and T2 denote the following terms
T1 ,-(f(k)(X) - y)>Λ(k,U)(fg(k,U)(X) -y),	(54a)
T2 , (f (k)(X) - y)>Λ⊥(k,U)(fg(k,U)(X) - y),	(54b)
where fgk,u)(X)，[f(k,u)(Xι)>,…，虑U)(XICk∣)>]>. We are going to bound Ti and T2 SePa-
rately. T1 can be written as:
T1 = -(f (k) (X) - y)> (Λ(k,U) - H(0) + H(0) - H∞ + H∞)(fg(k,U) (X) - y)	(55a)
= -(f (k)(X) - y)>(Λ(k,U) - H(0))(fg(k,U)(X) -y)
-	(f(k)(X)-y)>(H(0) - H∞)(fg(k,U)(X) - y)
-	(f(k)(X) - y)>H∞(f(k)(X) - y)
-	(f (k)(X) - y)>H∞(fg(k,U)(X) - f (k)(X)).	(55b)
First, we bound the norm of fg(k,U) (X) - y. It can be shown that
kfm(k,U)(Xm )-ym	k2=	kfm(k,U)(Xm	) - fm(k,U-1)(Xm	) + fm(k,U-1)(Xm )-ym k2	(56a)
6	kfm(k,U)(Xm	) - fm(k,U-1)(Xm	)k2 + kfm(k,U-1)(Xm )-ym	k2	(56b)
①
6 (1 + η)kfm(k,U-1)(Xm ) -ym k2,	(56c)
where ① holds based on the derivation of (43). Applying (56c) recursively yields
kfm(k,U)(Xm )-ym k2 6 (1+η)Ukf(k)(Xm )-ym k2.	(57)
The bound for kfg(k,U)(X) - yk22 can thus be derived as
kfg(k,U)(X) - yk22 = XN hfg(k,U)(xi)-yii2	(58a)
i=1
= X fm(k,U)(Xm )-ym 22	(58b)
m∈Ck
6 (1 + η)2Uf (k)(X) - y22.	(58c)
Second, following the steps in Lemma 2, it can be shown that with probability at least 1 - δ,
kΛ(k,t) -小叫2 6 2√2NR	(59)
πδα
We also bound the difference between fg(k,U)(X) and f(k) (X) as follows:
① U
kfg(k,U)(X) - f (k)(X)k2 6	kfg(k,v)(X) - fg(k,v-1)(X)k2	(60a)
v=1
②ʌ	工、	τ∖
6	ηkfg(k,v-1)(X) -yk2	(60b)
v=1
③ U
6	η(1 + η)v-1kf (k)(X) -yk2	(60c)
v=1
= [(1+η)U-1]kf(k)(X) - yk2,	(60d)
18
Under review as a conference paper at ICLR 2022
where ① holds due to triangle inequality,② comes from (43),③ comes from (58c). Plugging the
results from (58c), (59), and (60d) into (55b), we have with probability at least 1 - δ,
T1 6 (1 +η)u
2√2NR V /
谓a+ NJ-
ln(2N 2∕δ)
2n
+ κλ0 - (1 + κ)λ0 kf (k) (X) - yk22, (61)
where κ is the condition number of the matrix H∞ . Next, consider the bound for T2 . The `2 norm
of Λ⊥(k,u) can be bounded as
kΛ⊥(k,u)k2 6 kΛ⊥(k,u)kF
(62a)
=(X X X nn XX>χji(k"1
i=1 m∈Ck j∈Im	r∈Si
N…①l~2 NR
6	|Si | 6	,
n	π δα
where ① comes from Lemma 1. Therefore, We have with probability at least 1 一 δ,
T 6 (1 + η)u r2NR
π δα
Combine the results of (61) and (63):
DdIf (k)(X)-yE 6 卤"。+」η2 +
(62b)
(62c)
—
(1 + κ)ηλo
N
f(k)(X) - y22.
(63)
κλ0
ɪ) (64)
For the inner product term dII, f (k)(X) - y , we first bound kdIIk22 with probability at least 1 - δ:
N
kdIIk22 = X
i=1
2
6
6
6
6
①
6
②
6
③
6
④
6
X crσ (vr(k+1))>Xi - crσ (v(rk))>Xi
r∈Si
N2
n X |Si| X (cr hvr+ ) - vr), xi i)
i=1	r∈Si
1N
n X∣Si∣X
ηcr
i=1
2
η2
r∈Si
N
(65a)
(65b)
N√n∣Ck |
2
XXX (fm(k,u)
(Xj) - yj) ⅛u) I
m∈Ck u=0 j∈Im
(65c)
τ-1
2
N2n2 |Ck |2
2
η2
N2n2 |Ck |2
2
η2
N2n2 |Ck |2
1
N 2n2∖Ck ∖2
1
Nn2∖Ck I2
2R2
X 国X(XXXI小)(Xj )-yj
(65d)
i=1
N
r∈Si	m∈Ck u=0 j∈Im
τ-1
2
X|Si| X XX|Im |fm(k,u)(Xm)-ym 2
i=1	r∈Si m∈Ck u=0
N	τ-1
7XISiI X ( X X(1 + η)u∣im∣ f (k)(Xm) -ym∣L
N
XISiI X X ((1+η)τ-1)IIm I f (k)(Xm) - ym
N2
XISiI X((1+η)τ - 1) f (k)(X) - y2
(65e)
2
2
1
(65f)
(65g)
(65h)
πδ2 α2 |Ck |2
τ(τ - 1) 2
τη +---2~η η2+
2
f(k)(X)-y22.
(65i)
19
Under review as a conference paper at ICLR 2022
where ① comes from (57),② holds due to ∣∣akι 6 ∣∣a∣∣2,③ holds due to ∣∣akι 6，dim(a)ka||2,④
is from Lemma 1. With probability at least 1 - δ, the inner product term can thus be bounded as
DdII,f(k)(X) - yE 6 √gT⅛ (η + ɪη2 + OW)) f (k)(X) - 丫修
The bound for the difference term is derived as
Nn	2
∖∖f(k+1)(X)- f(k)(χ)∣∣2 6 χ	√nxCrhvrk+I)-Vrk),Xii
i=1	r=1
6
3 (τη + T(T-1)η2 + o(η2)) ∖∖f(k)(X)- y∖∖2∙
Combine the results of (64), (66) and (67b):
∖∖f (k+1)(X) - y∖∖22 6
-≡Nκ)λ0] + 2 + o(η2)}∖∖f (k)(x)-y∖∖2.
Let R = O (δαNλ0), n = Ω (等 ln 卒),and η = O (TNICkJ, we have
∖∖f (k+1)(X) - y∖∖22 6 1-
ητλo
2N ∣Ck |
∖∖f(k+1)(X) - y∖∖22.
(66)
(67a)
(67b)
(68)
(69a)
□
20