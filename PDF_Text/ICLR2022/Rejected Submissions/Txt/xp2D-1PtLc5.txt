Under review as a conference paper at ICLR 2022
ClsVC: Learning Speech Representations with
two different classification tasks.
Anonymous authors
Paper under double-blind review
Ab stract
Voice conversion(VC) aims to convert one speaker’s voice to generate a new
speech as it is said by another speaker. Previous works focus on learning latent
representation by applying two different encoders to learn content information
and timbre information from the input speech respectively. However, whether
they apply a bottleneck network or vector quantify technology, it is very difficult
to perfectly separate the speaker and the content information from a speech sig-
nal. In this paper, we propose a novel voice conversion framework, ’ClsVC’, to
address this problem. It uses only one encoder to get both timbre and content in-
formation by dividing the latent space. Besides, some constraints are proposed to
ensure the different part of latent space only contains separating content and tim-
bre information respectively. We have shown the necessity to set these constraints,
and we also experimentally prove that even if we change the division proportion
of latent space, the content and timbre information will be always well separated.
Experiments on the VCTK dataset show ClsVC is a state-of-the-art framework in
terms of the naturalness and similarity of converted speech.
1	Introduction
Voice conversion (VC) is an exciting topic committed to converting one utterance ofa source speaker
into another utterance of a target person by keeping the content in the original utterance while
replacing it with the vocal features from the target speaker. Up to now, many methods have been
applied in VC successfully. Commonly, these methods can be roughly categorized into two classes,
i.e., parallel VC and non-parallel VC Mohammadi & Kain (2017). Specifically, parallel VC means
that model training requires parallel corpus, which is unnecessary for non-parallel VC. Recently,
more researchers have focused on the solutions of non-parallel VC since it is not easy for us to
collect so many paired source-target speech datasets.
Early VC systems, like Gaussian Mixture Model (GMM), Stylianou et al. (1998); Toda et al. (2007)
needed a lot of parallel data for model training, and the generated speech quality was not good
enough. With the advance of deep learning, a variety of novel VC methods have been proposed
in recent years. Among them, GAN-based model is one of the most popular methods, Goodfellow
et al. (2014); Hsu et al. (2017a); Kaneko & Kameoka (2018); Kaneko et al. (2019a); Kameoka
et al. (2018); Kaneko et al. (2019b) which could learn a global generative distribution of the target
speech without explicit approximation. These GAN-based models jointly train a generator and a
discriminator. An adversarial loss derived from the discriminator is used to encourage the generator
outputs to build indistinguishable from real speech. Thanks to the cycle consistency training, GAN-
based VC models can be trained with non-parallel speech datasets.
Besides, learning discrete speech representations has also attracted much attention. Vector Quan-
tization (VQ), an extremely important signal compression method, which can quantify continuous
data into discrete data. Previous studies have confirmed that the quantized discrete data produced
by the continuous speech data is closely related with the phoneme information Chorowski et al.
(2019). Recently, VQVC Wu & Lee (2020) has been proposed to learn to disentangle the con-
tent and speaker information with reconstruction loss only. Then, VQVC+ Wu et al. (2020) was
soon presented to improve the conversion performance of VQVC by adding the U-Net architecture
within an auto-encoder-based VC system. To far improve the performance of disentangling content
and speaker information, many other existing studies were introduced to combine with VQ, such as
1
Under review as a conference paper at ICLR 2022
VQ-Wav2Vec, VQ-VAE and VQ-CPC. Baevski et al. (2019); Ding & Gutierrez-Osuna (2019); van
Niekerk et al. (2020)
There is also another line of research focus on learning latent representations with Autoencoder.
In particular, Variational Auto Encoder(VAE) is the most famous. The network structure of VAE
contains an encoder and a decoder and the core idea is very clear: the encoder learns a specific latent
space from input speech and the decoder outputs a reconstructed speech from this latent space.
In this process, VAE focuses on how to force the encoder to learn a specific latent space. So far,
many VAE-based models have been successfully applied to VC Hsu et al. (2016); van den Oord et al.
(2017); Hsu et al. (2017b); Ding & Gutierrez-Osuna (2019); Hsu et al. (2017a). In addition, AutoVC
is another successful application of Autoencoder. Qian et al. (2019) Through ingenious experimental
design, AutoVC uses two different encoders to learn content and speaker information, respectively,
so that this model can achieve distribution-matching style transfer by only on a self-reconstruction
loss.
Unfortunately, in the field of VC, all the models mentioned above have their inherent disadvan-
tages. For example, GAN-based models can usually achieve a good conversion effect and ensure
the matching between the generated data and the input data, but it is recognized that the training of
GAN is very unstable. On the contrary, the training of VQVC is simple and fast enough, but the
quantity of audio produced by this method is very poor. This may be because the discrete speech
representations will inevitably lose some content information. In addition, although the VAE-based
model also has a great conversion effect, it can not guarantee distribution matching. AutoVC is a
great study, the training is very simple and achieves state-of-the-art results. However, in order to
realize style conversion, it has to introduce a pre-trained speaker encoder.
Based on these existing methods, we naturally wonder if there is a new solution that can achieve
the distribution matching as AutoVC and GAN, that trains as easily as VQVC and VAE, that can
disentangle content and speaker information by only one encoder as VQ does, and also has better
performance in voice conversion or in decoupling linguistic and timbre information from speech?
In this paper, we proposed a novel voice conversion framework to meet all the above requirements.
Specifically, our model is similar to VAE, Autoencoder is the main framework of our model, and
two different types of classification tasks are applied to force our model to separate the content
and speaker information correctly. Here, the two classification tasks refer to general classification
tasks and adversarial classification tasks respectively. The goal of the general classification task is
to identify the features related to the speaker as accurately as possible, that is, the speaker informa-
tion. While the latter is designed to eliminate speaker information in latent space to get speaker-
independent features, that is, content information. Experiment results are carried out on the VCTK
dataset. Objective and Subjective evaluations demonstrate that the proposed method outperforms
VQVC, AutoVC, VQ-VAE and StarGAN-VC in terms of naturalness and speaker similarity.
2	Background
In mathematical statistics, if we already know the joint probability density functions of X and Y ,
we can easily find the marginal probability density functions of X and Y respectively. Formally,
if (x, y)〜p(x, y) is known, We can get the marginal distributions of X and Y by the following
formula:
fX (x) =	p(x, y)dy	fY(y) = p(x, y)dx	(1)
Further, under the constraints of some setting conditions, although the closed-form of joint probabil-
ity density functions p(x, y) in Eq. (1) is generally unknown, it is still feasible for a neural network
to learn the marginal distributions of x and y from input samples z when each z corresponds to a
unique pair of (x, y).
Mutual information (MI), a crucial indicator to measure the dependence between two different
variables. Recently, many MI estimators have been successfully applied to constrain neural networks
to disentangle different components of the input data. which can be formulated as
I(χ,Y)=Z Z P(X,Y)log PPXYY)	⑵
X Y	P(X)P(Y)
2
Under review as a conference paper at ICLR 2022
(a) Training of ClsVC
Figure 1: Framework of ClsVC. Z is the latent variable, which is divided into two parts, namely CX
and SX . Here, we assume that CX represents content information, which is speaker independent,
and SX represents speaker information, which is closely related to speaker identity.
(b) Conversion of ClsVC
Where P (X) and P (Y ) are the marginal distributions of X and Y respectively, and P (X, Y ) de-
notes the joint distribution of X and Y . Since it is hard to obtain the required distribution formula
P (X, Y ), many studies focus on proposing a sample-based MI lower bound (or upper bound) to get
an approximation that can be calculated. Moon & Hero (2014); Hjelm et al. (2018); Cheng et al.
(2020)
Recently, Yuan et al. Yuan et al. (2020) , introduced a new MI estimator to learn content and style
information from speech for voice conversion. Specifically, they proposed a novel MI-based learning
objective to encourage a content encoder to output the content embedding and guide a speaker
encoder to output the speaker embedding. Inspired by this, we proposed a new, simple and more
effective framework for learning latent speech representation.
Gradient Reversal Layer (GRL) Gradient Reversal Layer (GRL) was first proposed to address the
issue of domain adaption Ganin & Lempitsky (2015), which aims to force model to output domain-
shared features which are independent with domains. Specifically, GRL is often located between an
encoder and a domain classifier. During the forward propagation, GRL acts as an identity transform.
During the backpropagation, GRL takes the gradient from the subsequent level, multiplies it by
-1 and passes it to the preceding layer so that the encoder and domain classifier have completely
opposite optimization objectives.
3	Method
Firstly, for every speech x , we use content embedding Cx to represent linguistic information and
speaker embedding Sx is proposed to represent timbre and style information. And, U means the set
of speakers. The following two theorems are the premise of our framework:
Theorem 3.1 The content embedding Cx and speaker identity U are independent for each other,
In addition, the probability of each speaker’s speech being selected is the same, Formally, P (U =
u|Cx) = P(U = u) = constant regardless of the speaker identity u .
Theorem 3.2 The speaker embedding Sx and speaker identity U are in one-to-one correspondence.
That is, for a speaker u who produced speech x, P(U = u|Sx) = 1 and for other speakers
v(v 6= u) , P(U = v|Sx) = 0 .
3.1	Problem Formulation
Given a dataset of multi speakers and their audio recordings X , where speaker u has Nu audio
recordings. Formally, for each speaker u, Xu = {xui }iN=u1. For every input speech x ∈ X, we
use T to represent the number of frames of speech x, noted that T is constant, which means that
3
Under review as a conference paper at ICLR 2022
for any speech segments longer than T , we randomly select T frames, at the same time, for those
speech segments with a length shorter than T , we pad them with constant. In this case, x can also
expressed as x(1 : T) which is a random process randomly sampled from the speech distribution
PX (∙∣CX = Cx, SX = Sχ). Here, We assume that every speech X can be expressed as a function
f (∙) of content embedding Cx and speaker embedding Sx . That is, X = f (Cx, Sx) . And, we also
assume that x is uniquely determined by Cx and Sx . Formally, x1 = x2 only if Cx1 = Cx2 and
Sx1 = Sx2 . Based on this assumption, for ∀ Xi , Xj ∈ X , the following formula must be true:
I(Sxi,f(Cx,Sxi))=I(Sxj,f(Cx,Sxj)), I(Cxi,f(Cxi,Sx))=I(Cxj,f(Cxj,Sx)).
(3)
Now, assume two speech X1 and X2 from speaker u and speaker v respectively, X1 = f(Cx1, Sx1 )
and X2 = f(Cx2, Sx2) . Our goal is to design a speech conversion framework to generate a new
speech Xb1→2 which preserves the content information ofX1 but the speaker information is matched
with X2 . Formally, an ideal converted speech should satisfy the following forms:
I(Sx1,X1) =I(Sx2,Xb1→2),	I(Cx2,X2) = I(Cx1, Xb1→2).	(4)
Based on the above assumption, the formula in Eq. (4) can be equivalent to Xb1→2 = f(Cx1, Sx2)
. This conclusion is quite strong, and the formal proof will be presented in the appendix.
3.2	The Proposed Framework
In this section, we will introduce the core component of our proposed framework. As is illustrated in
Figure 1(a), our framework contains four modules. The first module is an encoder E, which learns
a latent variable Z from input speech X. Here we expect Z to be a specific function of content
embedding Cx and speaker embedding Sx . Which can be formulated as:
Z = E(X) = E(f(Cx, Sx)) = Sx ㊉ Cx	(5)
where ㊉ means concat operation. In this case, Z contains both linguistic information and timbre
information and when we divide Z into two parts, the first part is the estimated speaker embedding
Sx while the second part is the estimated content embedding Cx .
However, without any constraints, the encoder may learn meaningless speech representation. To
address this problem, two different types of classifiers are used to encourage our encoder to output
the target Z. Specifically, when we divide Z into two parts, the first part is put into a common
speaker classifier C1 to identify the speaker as correctly as possible. At the same time, the latter
part is put into an adversarial classifier C2 . Noted that C2 also expects to correctly recognize the
speaker’s identity like C1, the only difference is that a GRL is included in C2, which will make the
encoder expect to fool the adversarial classifier so that it cannot classify correctly. This two different
classifiers will finally converge when the first part of Z is very closely related with the speaker
information and the second part of Z is sufficiently speaker-independent such that the adversarial
classifier C2 is not able to identify the speaker. Then we regard the first part of Z as estimated
speaker embedding Sx and the second part of Z is regarded as estimated content embedding Cx .
The last module in our framework is a decoder D, which will output a reconstruct speech Xb from
input latent variable Z0 . It is worth noting that here the latent variable Z0 is not same with Z
since Z0 is produced by concating with Cx and the vector norm of Sx rather than original speaker
embedding Sx .
3.3	How and Why does it work
Here we will discuss how and why our model can induce style embedding and content embedding
into independent representation spaces simultaneously.
In training phase, a speech Xu produced by a speaker u is selected to feed into the encoder E
to output a latent variable Zu = E(Xu, θe) , where θe are the parameters of the encoder. After
that, the output of encoder is then divided into two parts, the vector norm of the first part of Zu
and the speaker identity u are fed into a common speaker classifier C1 while the estimated content
embedding Cxu which is the second part of Zu and the speaker identity u are fed into an adversarial
speaker classifier C2 to predict the probability of the speaker identity according to estimated speaker
4
Under review as a conference paper at ICLR 2022
embedding Sxu and content embedding Cxu respectively, i.e. Pu = C1(Sxu , u, θc1), Pu0 =
C2(Cxu , u, θc2 ) . Where θc1 and θc2 denote the parameters of common speaker classifier and
adversarial speaker classifier respectively. Subsequently, new latent variable Zu0 is then fed into
decoder D to output a reconstruct speech x0u = D(Zu0 , θd). Where θd refer to the parameters of
decoder.
Our goal is to disentangle the content embedding Cx and speaker embedding Sx from a speech
x. Formally, the ideal content embedding and speaker embedding should satisfy I(Cx, Sx) = 0
. In addition, since the process from a speaker u to his speech xu to speaker embedding Sxu is a
MarKov Chain, we can say that I(Cxu , Sxu ) ≥ I(Cxu , xu) ≥ I(Cxu , u) based on the MI-data
processing inequality Cover (1999) . So, we use I(Cxu , u) instead of I(Cxu , Sxu) . according
to the illustration of Thm3.1, for the ideal content embedding, I (Cxu , u) = 0. So our goal is to
minimize I (Cxu , u) and we use the adversarial-classification loss function to minimize the upper
bound. At the same time, the common-classification loss function was designed to encourage the
speaker embedding Sxu to be as closely related to speaker identity u as possible. They can be
expressed as:
K
Lcom-cls (θe, θc1) = -	I(u == k) log pk	(6)
k=1
K
Ladv-cls(θe, θc2) = - X I(u == k) log p0k	(7)
k=1
Where I(∙) is the indicator function, K is the number of speakers and U denotes speaker Who Pro-
duced speech x, and pk is the probability of speaker k . During training, for Lcom-cls , θe and θc1 are
all optimized to minimize the classification loss to better identify the corresponding speaker. But for
Ladv-cls , θc2 are still optimized to minimize the classification loss, Whereas θe are optimized to max-
imize the classification loss to fool the classifier. Ideally, under these tWo constraints, the first part
of the output of encoder Will be more closely related to speaker information While the second part of
the output of encoder Will be sufficiently speaker-independent so that the classifier can not identify
the speaker. And then, We can easily get the ideal content embedding and speaker embedding at the
same time.
Unfortunately, the truth is, When there are only the above tWo classification constraints, three differ-
ent results may occur, as is illustrated in Figure 2, When estimated speaker embedding Sx contain
some content information, like Figure 2(a), correspondingly, for estimated content embedding Cx,
these content information Will be lost. In this case, the estimated content embedding Sx are still
speaker-independent and the common classifier can still correctly identify the speaker from esti-
mated speaker embedding. The only draWback is that some content information Will eventually be
lost because We replace Sx With it’s vector norm Sbx , Which Will lead to imperfect reconstruction.
That is Why We propose the reconstruct-loss function betWeen input speech x and the reconstruct
speech x0 . That is, Lrecon = E[kx0 - xk22]. The key logic behind setting this loss function is
clear: to minimize the reconstruction loss, the estimated content embedding Cx Will be encouraged
to carry all the content information to avoid the results of Figure 2(a).
Or, consider such a case, the estimated content embedding Cx may contain some speaker infor-
mation that can not be completely recognized by the adversarial speaker classifier C2 . When it
happens, both the common classifier C1 and the adversarial classifier C2 can still Work Well, and
even our model can perform Well in the speech reconstruction task. In other Words, the above three
objective functions can not restrict the occurrence of such a situation as Figure 2(b). But, in this
case, the decoder may ignore Sx because the estimated content embedding Cx has provided both
content information and speaker information. As a result, our model can only reconstruct speech
but can not convert any speech. To address this problem, We provide the code-reconstruction loss
function betWeen the latent variable Z and reconstruct latent variable Zb Which is produced from
the reconstruct speech x0 , Lcode-recon = E[kZb - Z k22] . This objective function Will constrain
the decoder to focus on the estimated speaker embedding Sx . At the same time, since the speaker
information is provided by the estimated speaker embedding, Sx Were also encouraged to carry all
the speaker information to minimize Lcode-recon . The full objective function can be computed as:
L(θe,θd)
Lrecon + αLcode-recon + βLcom-cls - λLadv-cls
(8)
5
Under review as a conference paper at ICLR 2022
Figure 2: Three different case of disentangle different parts. Here Z are the latent variable output
from the input speech, Cx denote the estimated content embedding and Sx refer to the estimated
speaker embedding, both Cx and Sx are a part of the latent variable Z, in addition, x0 is the
reconstruct speech and zb are regarded as reconstruct latent variable. zb = E(x0) . Whether Cx and
Sx are properly selected may lead to the above three different situations
Where α , β, and λ refers to the weight of Lcode-recon, Lcom-cls , and Ladv-cls respectively. With this
objective loss function, an ideal case as Figure 2(c) will eventually appear. Specifically, in this case,
two important assumptions will be met.
•	The estimated content embedding Cx carry all content information but not contain any
speaker information. On the contrary, the estimated speaker embedding Sx contain all
speaker information but have no content information. In other words, now Cx are the real
content embedding and Sx are the real speaker embedding. So that for any input speech
x, there must be X = f(Cχ, Sx)〜PX(∙∣Cχ = Cx,Sχ = Sx).
•	With Cx and Sx , an perfect reconstruction is achieved. i.e. x = x0 . Where x0 is the
reconstruction of x through the likelihood pθd (x|Cx, Sx) .
Futher, based on the above two conditions and our previous assumptions, we then say for any index
i, there must be Xi = f (Cxi, Sxi) = Xi 〜pxo (∙∣Cxi, Sxi) . That is, in the process of VC, if
we define source utterance x1 = f (Cx1 , Sx1), target utterance x2 = f (Cx1 , Sx2), and x1→2
is regarded as the converted speech. Then We can finally find the best encoder parameters θe and
decoder parameters θd to satisfy the following formula: L(θe ,θd) = -λLadv-cls = -λlog K and
KL(PX1→2 (∙∣Cxι ,Sx2 )kpχ (∙∣Cχ = Cxi ,Sx = Sx2)) = 0	(9)
where K is the number of speakers, KL(∙∣∣∙) is the KL-divergence.
Which means the optimizer of loss function in Eq.(8) would imperceptibly satisfy the ideal speech
conversion in Eq.(4). That’s why we say our method can achieve distribution-matching style transfer
as GAN and AutoVC.
4	Experiments
In this section, we will evaluate the performance of our ClsVC on traditional many-to-many VC tasks
and one-shot VC tasks. Specifically, many-to-many VC means that in conversion phase, both the
selected source speaker and the target speaker are all appeared in training process. In contrast, one-
shot VC focuses on some more difficult tasks, in which both the source speaker and target speaker
are unseen in training dataset, and only one utterance of source and target speakers are required.
Besides, we also empirically validate the convenience and robustness of the ClsVC framework. We
will present the audio demo and further details may be found in our implementation code.
4.1	Datasets and Configurations
To evaluate the performance of different model in VC tasks, comparative experiments are conducted
on VCTK Veaux et al. (2016), a high-fidelity multi-speaker English speech corpus. This corpus
6
Under review as a conference paper at ICLR 2022
contains 46 hours of speech data produced by 109 English speakers from different countries. In our
work, the entire dataset is randomly divided into 2 sets: 100 speaker’s recordings for training and
other 9 speaker’s recordings for testing. Before training, the sampling rate of all recordings is re-
sampled to 16KHz, and the mel-spectrograms are computed through a short-time Fourier transform
(STFT) with Hann windowing, where 1024 for FFT size, 1024 for window size and 256 for hop
size. The STFT magnitude is transformed to the mel scale using 80 channel mel filter bank spanning
90 Hz to 7.6 kHz.
ClsVC is trained with batch size of sixteen for 200K steps on one NVIDIA V100 GPU, using the
ADAM optimizer with β1 = 0.9, β2 = 0.98. The weight in Eq.(8) are set to α = 0.1, β = 0.1,
λ = 0.5. StarGAN-VC21, AutoVC2, VAE3, and VQVC+4 are chosen as the baseline models.
4.2	Comparison
To compare the performance of different models in VC tasks, we use both objective and subjective
tests. Specifically, the Mel-Cepstral Distortion(MCD) between converted speech and the ground
truth target speech is used as our objective evaluation to measure the similarity of the transferred
voice and the real voice from the target speaker. And, we invited 12 humans (7 male and 5 female)
participants to evaluate the quality of some converted speech generated from different models. After
hearing every speech, the subjects are asked to choose a score from 1-5 points of the naturalness of
the converted speech. The higher the score, the better the audio quality of converted speech, which
we called the Mean Opinion Score(MOS) test. In addition, all participants are also asked to take
Voice Similarity Score(VSS) test. Where groups of utterances are rated with a score of 1-5 on the
voice similarity, in each group, there are five converted utterances generated from AutoVC, VQVC+,
VAE, StarGAN-VC2 and ClsVC respectively, and one real utterance from the target speaker. In VSS
test, we calculate the score according to the timbre similarity given by the tester. Specifically, the
similarity score of 5 corresponds to the converted speech most similar to the ground truth speech,
while the similarity score of 1 indicates that the tester does not think that the converted speech and
the ground truth speech come from the same speaker. The results appear as shown below:
Table 1: Comparison of different models in traditional VC and one-shot vc
Methods	Traditional VC			One-Shot VC		
	MCD	MOS	VSS	MCD	MOS	VSS
VQVC+	7.08 ± 0.22	3.31 ± 0.90	3.42 ± 0.85	8.41 ± 0.08	2.75 ± 0.84	3.11 ± 0.88
AutoVC	4.34 ± 0.12	3.81 ± 1.14	3.45 ± 0.76	7.66 ± 0.17	2.61 ± 0.73	2.91 ± 0.72
VAE	5.63 ± 0.21	3.33 ± 0.87	3.07 ± 0.84	—	—	—
StarGAN-VC2	6.28 ± 0.09	3.45 ± 1.01	3.59 ± 0.87	—	—	—
ClsVC	4.06 ± 0.11	4.09 ± 0.86	4.03 ± 0.82	4.81 ± 0.13	4.18 ± 0.73	3.75 ± 0.74
As quoted in Table 1, in the traditional VC task, compared with other baseline models, our ClsVC
has achieved the best results in both subjective and objective tests, which indicates that the speech
produced by our method is much better than baselines’. The results of VSS test show that compared
with VQVC+, AutoVC, VAE and StarGAN-VC2, our method makes the converted speech learn
better timbre information, which improves the conversion effect.
For one-shot VC task, since VAE and StarGAN-VC2 can not achieve voice conversion for unseen
speakers, we just compare our method with VQVC+ and AutoVC. The result shows that with only
one utterance available for unseen speakers, the performance of AutoVC is greatly reduced, previous
studies have reported this phoneme Tan et al. (2021). By comparison, even for unseen speakers and
only one utterance is available, our ClsVC still achieved amazing results.
It is worth noting to say that from Table 1, the objective and subjective results are not completely
consistent. For example, the mean score of MCD of VAE is 5.63, which is lower than that of
StarGAN-VC2, which means that the quality of speech generated from VAE should be higher than
1https://github.com/SamuelBroughton/StarGAN-Voice-Conversion-2
2https://github.com/auspicious3000/autovc
3https://github.com/vsimkus/voice-conversion
4https://github.com/ericwudayi/SkipVQVC
7
Under review as a conference paper at ICLR 2022
StarGAN-VC2, but the MOS results are just the opposite. This indicates that sometimes a speech
with a lower score of MCD may still have inferior audio quality. To issue the problem, we add an-
other objective experiment. By applying a well-known open-source speech detection toolkit, Resem-
blyzer (https://github.com/resemble-ai/Resemblyzer), we conduct a fake speech
detection test to compare the similarity of 11 unknown utterances (6 real ones, 5 fakes which are
generated from different models respectively) against ground truth reference audio. The results are
shown in Figure 5.
(a) Female to Female conversion
Figure 3: Results of fake speech detection. The prediction threshold in our test is 0.84
(b) Female to Male conversion
The experimental results support our conclusion, for both same-gender VC (Female to Female) and
cross-gender VC (Female to Male), our ClsVC always outperforms other baseline models. All these
objective and subjective experiments show that ClsVC is a state-of-the-art framework for VC under
both traditional and one-shot conditions.
4.3	Necessity of some loss functions
In this section, we will prove experimentally that the loss function we proposed is necessary for our
framework. Specifically, we design an ablation experiment to observe the effect of Lcode-recon on
our framework. We retrain ClsVC without Lcode-recon, called ’ClsVCs’. Based on our assumption,
without the constrain of the code-reconstruction loss function, the estimated speaker embedding Sx
may be ignored by the decoder, and our model can finally achieve a good self reconstruction but can
not convert any speech. To test this hypothesis, we reuse Resemblyzer to do Speaker diarization test.
In this test, we need prepare some ground truth speech from the source speaker and target speaker
respectively, then we input a converted speech produced by ’ClsVC’ and ’ClsVCs’ respectively,
and this toolkit will automatically identify the probability of who the input speech belongs to. For
further comparison, we divide the converted audio into two gender groups, the one is same-gender
conversion, including male to male and female to female VC, the other one is cross-gender con-
version, including female to male and male to female VC, results of Speaker classification test are
summarized in Figure 4(a).
Besides, We also select some self reconstruction speech of the source speaker to compare the per-
formance of ’ClsVC’ and ’ClsVCs’ in the speech reconstruction task. In practice, 12 human par-
ticipants were invited to hear a real speech and two reconstructed speech produced by ’ClsVC’ and
’ClsVCs’ respectively, they need to evaluate the similarity between the reconstructed speech and
real speech and choose one that was more similar to the ground truth. In addition, if it is difficult to
judge, they can also choose the ’Fair’ option. We draw the result into the following Figure 4(b) .
The Results show that in the speech reconstruction task, ’ClsVCs’ performs slightly worse than our
’ClsVC’, but in the VC task, the performance of ’ClsVCs’ is quite poor. As shown in Figure 4(a),
the converted speech produced by ’ClsVCs’ still has high similarity with the source speaker and
low similarity with the target speaker, which indicates that ClsVCs cannot achieve VC without the
constraints of Lcode-recon .
8
Under review as a conference paper at ICLR 2022
Figure 4: Results of ablation experiment. 1:ClsVC; 2:ClsVCs; Same: same-gender conversion;
Cross: cross-gender conversion;
4.4	Dimensions of two parts of latent variable
Here we will discuss how to devide the content embedding Cx and speaker embedding Sx from
the latent space Z. In AutoVC, it is important to select the size of bottleneck carefully to make the
estimated content embedding contain all content information but have no timbre information. But
in our model, as we discussed before, no matter how we divide it, the first part of Z tends to be the
ideal content embedding, and the second part tends to be the ideal speaker embedding. In this case,
we can determine the dimensions of Cx and Sx at will and it will be very convenient for us to get
content embedding and speaker embedding with only one encoder.
In order to verify that the decoupling ability of our model is equivalent under different partition
modes, we retrain ClsVC by changing the dimensions of Cx and Sx . Specifically, in ClsVC, the
dimensions of Cx and Sx are 32 and 256, respectively. Now we retrain ClsVC by changing them
to 32 and 64, called ’ClsVC-dim1’, or, to 64 and 32, clled ’ClsVC-dim2’. In addition, we trained
another ’clsvc-dim3’ with both the dimensions of Cx and Sx in this model are 64. We select some
unseen speakers’ utterances(100 utterances per speaker) to these models to obtain the estimated
speaker embedding Sx , then we plotted Sx in 2-D space with t-SNE in Figure 5.
(a) ClsVC
(b) ClsVC-dim1
(c) ClsVC-dim2
(d) ClsVC-dim3
Figure 5: The visualization of speaker embedding. None of these speaker appeared in training.
Results shown in Figure 5 indicate that the content and timbre information will always be well
separated even we change the division proportion of latent space.
5	Conclusion
In this paper, we proposed ClsVC, a novel VC system learning latent speech representation. During
training, a common speaker classifier is proposed to encourage the estimated speaker embedding to
become more and more related to the speaker identity and an adversarial classifier will focus the es-
timated content embedding more speaker-independent. We also introduce other objective functions
to make the encoder learn the ideal latent space. All subjective and objective experimental results
show that the method we proposed is state-of-the-art.
9
Under review as a conference paper at ICLR 2022
References
Alexei Baevski, Steffen Schneider, and Michael Auli. vq-wav2vec: Self-supervised learning of
discrete speech representations. In International Conference on Learning Representations, 2019.
Pengyu Cheng, Weituo Hao, Shuyang Dai, Jiachang Liu, Zhe Gan, and Lawrence Carin. Club: A
contrastive log-ratio upper bound of mutual information. In International Conference on Machine
Learning ,pp.1779-1788. PMLR, 2020.
Jan Chorowski, Ron J Weiss, Samy Bengio, and Aaron van den Oord. UnsUPervised speech repre-
sentation learning using wavenet autoencoders. IEEE/ACM transactions on audio, speech, and
language processing, 27(12):2041-2053, 2019.
Thomas M Cover. Elements of information theory. John Wiley & Sons, 1999.
Shaojin Ding and Ricardo GUtierrez-OsUna. GroUp latent embedding for vector qUantized variational
aUtoencoder in non-parallel voice conversion. In INTERSPEECH, pp. 724-728, 2019.
Yaroslav Ganin and Victor Lempitsky. UnsUpervised domain adaptation by backpropagation. In
International conference on machine learning, pp. 1180-1189. PMLR, 2015.
Ian Goodfellow, Jean PoUget-Abadie, Mehdi Mirza, Bing XU, David Warde-Farley, Sherjil Ozair,
Aaron CoUrville, and YoshUa Bengio. Generative adversarial nets. Advances in neural information
processing systems, 27, 2014.
R Devon Hjelm, Alex Fedorov, SamUel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and YoshUa Bengio. Learning deep representations by mUtUal information estimation
and maximization. In International Conference on Learning Representations, 2018.
Chin-Cheng HsU, Hsin-Te Hwang, Yi-Chiao WU, YU Tsao, and Hsin-Min Wang. Voice conver-
sion from non-parallel corpora Using variational aUto-encoder. In 2016 Asia-Pacific Signal and
Information Processing Association Annual Summit and Conference (APSIPA), pp. 1-6. IEEE,
2016.
Chin-Cheng HsU, Hsin-Te Hwang, Yi-Chiao WU, YU Tsao, and Hsin-Min Wang. Voice conver-
sion from Unaligned corpora Using variational aUtoencoding wasserstein generative adversarial
networks. In Francisco Lacerda (ed.), Interspeech 2017, 18th Annual Conference of the Interna-
tional Speech Communication Association, Stockholm, Sweden, August 20-24, 2017, pp. 3364-
3368. ISCA, 2017a.
Wei-Ning HsU, YU Zhang, and James Glass. UnsUpervised learning of disentangled and interpretable
representations from seqUential data. In Proceedings of the 31st International Conference on
Neural Information Processing Systems, pp. 1876-1887, 2017b.
HirokazU Kameoka, TakUhiro Kaneko, KoU Tanaka, and NobUkatsU Hojo. Stargan-vc: Non-parallel
many-to-many voice conversion Using star generative adversarial networks. In 2018 IEEE Spoken
Language Technology Workshop (SLT), pp. 266-273. IEEE, 2018.
TakUhiro Kaneko and HirokazU Kameoka. Cyclegan-vc: Non-parallel voice conversion Using cycle-
consistent adversarial networks. In 2018 26th European Signal Processing Conference (EU-
SIPCO), pp. 2100-2104. IEEE, 2018.
TakUhiro Kaneko, HirokazU Kameoka, KoU Tanaka, and NobUkatsU Hojo. Cyclegan-vc2: Improved
cyclegan-based non-parallel voice conversion. In ICASSP 2019-2019 IEEE International Confer-
ence on Acoustics, Speech and Signal Processing (ICASSP), pp. 6820-6824. IEEE, 2019a.
TakUhiro Kaneko, HirokazU Kameoka, KoU Tanaka, and NobUkatsU Hojo. Stargan-vc2: Rethinking
conditional methods for stargan-based voice conversion. Proc. Interspeech 2019, pp. 679-683,
2019b.
Seyed Hamidreza Mohammadi and Alexander Kain. An overview of voice conversion systems.
Speech Communication, 88:65-82, 2017.
10
Under review as a conference paper at ICLR 2022
Kevin R Moon and Alfred O Hero. Ensemble estimation of multivariate f-divergence. In 2014 IEEE
International Symposium on Information Theory, pp. 356-360. IEEE, 2014.
Kaizhi Qian, Yang Zhang, Shiyu Chang, Xuesong Yang, and Mark Hasegawa-Johnson. Autovc:
Zero-shot voice style transfer with only autoencoder loss. In International Conference on Machine
Learning, pp. 5210-5219. PMLR, 2019.
Yannis Stylianou, Olivier Cappe, and Eric Moulines. Continuous probabilistic transform for voice
conversion. IEEE Transactions on speech and audio processing, 6(2):131-142, 1998.
Zhiyuan Tan, Jianguo Wei, Junhai Xu, Yuqing He, and Wenhuan Lu. Zero-shot voice conver-
sion with adjusted speaker embeddings and simple acoustic features. In IEEE International
Conference on Acoustics, Speech and Signal Processing, ICASSP 2021, Toronto, ON, Canada,
June 6-11, 2021, pp. 5964-5968. IEEE, 2021. doi: 10.1109/ICASSP39728.2021.9414975. URL
https://doi.org/10.1109/ICASSP39728.2021.9414975.
Tomoki Toda, Alan W Black, and Keiichi Tokuda. Voice conversion based on maximum-likelihood
estimation of spectral parameter trajectory. IEEE Transactions on Audio, Speech, and Language
Processing, 15(8):2222-2235, 2007.
Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learn-
ing. In Proceedings of the 31st International Conference on Neural Information Processing Sys-
tems, pp. 6309-6318, 2017.
Benjamin van Niekerk, Leanne Nortje, and Herman Kamper. Vector-quantized neural networks
for acoustic unit discovery in the zerospeech 2020 challenge. In Helen Meng, Bo Xu, and
Thomas Fang Zheng (eds.), Interspeech 2020, 21st Annual Conference of the International Speech
Communication Association, Virtual Event, Shanghai, China, 25-29 October 2020, pp. 4836-
4840. ISCA, 2020.
Christophe Veaux, Junichi Yamagishi, Kirsten MacDonald, et al. Superseded-cstr vctk corpus: En-
glish multi-speaker corpus for cstr voice cloning toolkit. 2016.
Da-Yi Wu and Hung-yi Lee. One-shot voice conversion by vector quantization. In ICASSP 2020-
2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp.
7734-7738. IEEE, 2020.
Da-Yi Wu, Yen-Hao Chen, and Hung-yi Lee. Vqvc+: One-shot voice conversion by vector quanti-
zation and u-net architecture. Proc. Interspeech 2020, pp. 4691-4695, 2020.
Siyang Yuan, Pengyu Cheng, Ruiyi Zhang, Weituo Hao, Zhe Gan, and Lawrence Carin. Improving
zero-shot voice style transfer via disentangled representation learning. In International Confer-
ence on Learning Representations, 2020.
6	Appendix
Here we will present the formal proof of some equations which are appeared in the text of this paper.
About Eq.(3):
Given I(X, Y) = RX Ry P(X, Y) log PPWPYY) , we can infer that
I(Sxi ,f(Cχ, Sxi))= Z Z	P(Sxi ,f (Cx, Sχi))log ；( (Sxi Pf W Sxi))
Sxi f(Cx,Sxi )	P(Sxi)P(f(Cx, Sxi))
Where
P(Sxi ,f(Cx, Sxi)) = P(SX = Sxi)P(f(Cχ, SX ) = f(Cx, Sxi)∣Sχ = Sxi)
P(Sxi)P(f(Cx, Sxi)) =	P(Sχ = Sxi)P(f (Cχ, Sx) = f (Cx, Sxi))
=P(f (CX, SX) = f(Cx, Sxi)∣Sχ = Sxi)
=	P(f (Cχ, Sχ) = f (Cx, Sxi))
11
Under review as a conference paper at ICLR 2022
Note that We have assumed that the value of f (∙) is unique. It is easily to refer that the probability
off(CX, SX) = f(Cx, Sxi) is equivalent to CX = Cx, SX = Sxi, so that
P(f(CX,SX)=f(Cx,Sxi))=P(CX =Cx,SX=Sxi)
P(f(CX,SX) =f(Cx,Sxi)|SX =Sxi) =P(CX =Cx,SX =Sxi|SX =Sxi) =P(CX =Cx)
In this case
P(Sxi ,f(Cχ, Sxi)) =	P(CX = Cx)
P(Sxi)P(f(Cx, Sxi)) = P(CX = Cx SX = Sxi)
So the folloWing formula must be true
I(Sxi ,f(Cx, Sxiy)=I	[	P (Cχ = Cx, Sx = Sxi)log PSP(CX = CL、
Sxi f(Cx,Sxi)	P(CX = Cx, SX = Sxi )
according to the conclusion of Thm.3.1 and Thm.3.2 , We can say that P (CX = Cx, SX = Sxi) =
P(CX = Cx, SX = Sxj ) for any index i and j . So there must be
I(Sxi,f(Cx,Sxi))=I(Sxj,f(Cx,Sxj))
Similarly
I(Cxi,f(Cxi,Sx))=I(Cxj,f(Cxj,Sx))
About Eq.(4):
We have shoWn When xb1→2 = f(Cx1, Sx2) , there must be
I(Sx1,x1) =I(Sx1,f(Cx1,Sx1))=I(Sx2,f(Cx1,Sx2)) =I(Sx2,xb1→2)
I(Cx2,x2) =I(Cx2,f(Cx2,Sx2)) =I(Cx1,f(Cx1,Sx2))=I(Cx1,xb1→2)
NoW We assume that xb1→2 = f(Cx1, Sxj ) Where Sxj 6= Sx2 . In this case
I(Sx2, xb1→2) =I(Sx2,f(Cx1,Sxj))
/ /
Sx2 f (Cx1,Sxj)
P(Sx2,f(Cx1,Sxj))log
P (Sx2 ,f (Cxi, Sxj))
P (Sx2 )P (f (Cxi, Sxj))
Where
P(Sx2,f(Cxi, Sxj))	_ P(Sχ = Sx2)P(f (Cχ, Sx) = f (Cxi, Sxj )∣Sχ = Sx?)
P(Sx2)P(f (Cxi, Sxj)) =	P(SX = Sx2)P(f(Cχ, SX) = f(Cxi, Sxj))
Since the probability of f(CX, SX) = f(Cxi, Sxj )|SX = Sx2 is zero, We conclude that
P(Sx2 ,f (Cxi, Sxj))	P(Sx2 ,f(Cxi, Sx2)) = P(CX = Cxi, SX = Sx2)
P(Sx2)P(f(Cxi,Sxj)) =	Whe P(Sx2)P(f(Cx2,Sx2)) =	P(Cχ = Cxi)
Similarly, ifWe assume that xb1→2 = f(Cxi, Sx2) Where Cxi 6= Cxi . In this case
I(Cxi,X1-2)= I (Cxi ,f (Cxi, Sx2 ))= /	/	P (Cxi ,f (Cxi, Sx2 ))log PP(CxIPffCxi, SSS))
Cxi f (Cxi ,Sx2 )	xi	xi , x2
12
Under review as a conference paper at ICLR 2022
Where
P(Cxι,f(Cxi, Sχ2)) = P(CX = Cχι)P(f (Cχ, SX) = f (Cxi, Sχ2)∣Cχ = Cxi)
P(CxjP(f(Cxi, Sx2)) =	P (Cx = Cxi )P (f (C X, Sx ) = f (Cxi, Sx?))
Then we can get the same conclusion, That is
P (Cxi,f(Cxi, Sx2))	P (Cxι,f(Cxi, Sx2)) = P (CX = Cxi, SX = Sx2)
P(CxJP(f(Cxi, Sx2)) =	W hl P (Sx2 )P (f (Cx2, Sx2)) =	P (Sx = Sx2)
Which indicates that the formula in Eq.(4) is equivalent to xb1→2 = f(Cxi, Sx2)
13