Under review as a conference paper at ICLR 2022
Multi-Agent Constrained Policy Optimisation
Anonymous authors
Paper under double-blind review
Ab stract
Developing reinforcement learning algorithms that satisfy safety constraints is be-
coming increasingly important in real-world applications. In multi-agent rein-
forcement learning (MARL) settings, policy optimisation with safety awareness
is particularly challenging because each individual agent has to not only meet its
own safety constraints, but also consider those of others so that their joint be-
haviour can be guaranteed safe. Despite its importance, the problem of safe multi-
agent learning has not been rigorously studied; very few solutions have been pro-
posed, nor a sharable testing environment or benchmarks. To fill these gaps, in
this work, we formulate the safe MARL problem as a constrained Markov game
and solve it with policy optimisation methods. Our solutions—Multi-Agent Con-
strained Policy Optimisation (MACPO) and MAPPO-Lagrangian—leverage the
theories from both constrained policy optimisation and multi-agent trust region
learning. Crucially, our methods enjoy theoretical guarantees of both monotonic
improvement in reward and satisfaction of safety constraints at every iteration. To
examine the effectiveness of our methods, we develop the benchmark suite of Safe
Multi-Agent MuJoCo that involves a variety of MARL baselines. Experimental
results justify that MACPO/MAPPO-Lagrangian can consistently satisfy safety
constraints, meanwhile achieving comparable performance to strong baselines.1
1	Introduction
In recent years, reinforcement learning (RL) techniques have achieved remarkable successes on a
variety of complex tasks (Silver et al., 2016; 2017; Vinyals et al., 2019). Powered by deep neural
networks, deep RL enables learning sophisticated behaviours. On the other hand, deploying neu-
ral networks turns the optimisation procedure from policy space to parameter space; this enables
gradient-based methods to be applied (Sutton et al., 1999; Lillicrap et al., 2015; Schulman et al.,
2017). For policy gradient methods, at every iteration, the parameters of a policy network are up-
dated in the direction of the gradient that maximises return.
However, policies that are purely optimised for reward maximisation are rarely applicable to real-
world problems. In many applications, an agent is often required not to visit certain states or take
certain actions, which are thought of as “unsafe” either for itself or for other elements in the back-
ground (Moldovan & Abbeel, 2012; Achiam et al., 2017). For instance, a robot carrying materials
in a warehouse should not damage its parts while delivering an item to a shelf, nor should a self-
driving car cross on the red light while rushing towards its destination (Shalev-Shwartz et al., 2016).
To tackle these issues, Safe RL (Moldovan & AbbeeL 2012; Garcia & Fernandez, 2015) is proposed,
aiming to develop algorithms that learn policies that satisfy safety constraints. Despite the additional
requirement of safety on solutions, algorithms with convergence guarantees have been proposed (Xu
et al., 2021; Wei et al., 2021).
Developing safe policies for multi-agent systems is a challenging task. Part of the difficulty comes
from solving multi-agent reinforcement learning (MARL) problems itself (Deng et al., 2021); more
importantly, tackling safety in MARL is hard because each individual agent has to not only consider
its own safety constraints, which already may conflict its reward maximisation, but also consider the
safety constraints of others so that their joint behaviour is guaranteed to be safe. As a result, there are
very few solutions that offer effective learning algorithms for safe MARL problems. In fact, many
of the existing methods focus on learning to cooperate (Foerster et al., 2018; Rashid et al., 2018;
1Paper homepage including Videos and Code: https://sites.google.com/view/macpo
1
Under review as a conference paper at ICLR 2022
Yang & Wang, 2020). However, they often require certain structures on the solution; for example,
Rashid et al. (2018) and Yang et al. (2020) adopt greedy maximisation on the local component of
a monotonic joint value function, and Foerster et al. (2018) estimates the policy gradient based on
the counterfactual value from a joint critic. Therefore, it is unclear how to directly incorporate
safety constraints into these solution frameworks. Consequently, developing agents’ collaborations
towards reward maximisation under safety constraints remains an unsolved problem.
The goal of this paper is to increase practicality of MARL algorithms through endowing them with
safety awareness. For this purpose, we introduce a general framework to formulate safe MARL
problems, and solve them through multi-agent policy optimisation methods. Our solutions leverage
techniques from both constrained policy optimisation (Achiam et al., 2017) and multi-agent trust
region learning (Kuba et al., 2021a). The resulting algorithm attains properties of both monotonic
improvement guarantee and constraints satisfaction guarantee at every iteration during training. To
execute the optimisation objectives, we introduce two practical deep MARL algorithms: MACPO
and MAPPO-Lagrangian. As a side contribution, we also develop the first safe MARL bench-
mark within the MuJoCo environment, which include a variety of MARL baseline algorithms. We
evaluate MACPO and MAPPO-Lagrangian on a series of tasks, and results clearly confirm the ef-
fectiveness of our solutions both in terms of constraints satisfaction and reward maximisation. To
our best knowledge, MACPO and MAPPO-Lagrangian are the first safety-aware model-free MARL
algorithms that work effectively in the challenging MuJoCo tasks with safety constraints.
2	Related Work
Considering safety in the development of AI is a long-standing topic (Amodei et al., 2016). When
it comes to safe reinforcement learning (Garcia & Fernandez, 2015), a commonly used framework
is Constrained Markov Decision Processes (CMDPs) (Altman, 1999). In a CMDP, at every step,
in addition to the reward, the environment emits costs associated with certain constraints. As a
result, the learning agent must try to satisfy those constraints while maximising the total reward.
In general, the cost from the environment can be thought of as a measure of safety. Under the
framework of CMDP, a safe policy is the one that explores the environment safely by keeping the
total costs under certain thresholds. To tackle the learning problem in CMDPs, Achiam et al. (2017)
introduced Constrained Policy Optimisation (CPO), which updates agent’s policy under the trust
region constraint (Schulman et al., 2015) to maximise surrogate return while obeying surrogate
cost constraints. However, solving a constrained optimisation at every iteration of CPO can be
cumbersome for implementation. An alternative solution is to apply primal-dual methods, giving
rise to methods like TRPO-Lagrangian and PPO-Lagrangian (Ray et al., 2019). Although these
methods achieve impressive performance in terms of safety, the performance in terms of reward is
poor (Ray et al., 2019). Another class of algorithms that solves CMDPs is by Chow et al. (2018;
2019); these algorithms leverage the theoretical property of the Lyapunov functions and propose
safe value iteration and policy gradient procedures. In contrast to CPO, Chow et al. (2018; 2019)
can work with off-policy methods; they also can be trained end-to-end with no need for line search.
Safe multi-agent learning is an emerging research domain. Despite its importance (Shalev-Shwartz
et al., 2016), there are few solutions that work with MARL in a model-free setting. The majority
of methods are designed for robotics learning. For example, the technique of barrier certificates
(Borrmann et al., 2015; Ames et al., 2016; Qin et al., 2020) or model predictive shielding (Zhang
et al., 2019) from control theory is used to model safety. These methods, however, are specifically
derived for robotics applications; they either are supervised learning based approaches, or require
specific assumptions on the state space and environment dynamics. Moreover, due to the lack of a
benchmark suite for safe MARL algorithms, the generalisation ability of those methods is unclear.
The most related work to ours is Safe Dec-PG (Lu et al., 2021) where they used the primal-dual
framework to find the saddle point between maximising reward and minimising cost. In particular,
they proposed a decentralised policy descent-ascent method through a consensus network. How-
ever, reaching a consensus equivalently imposes an extra constraint of parameter sharing among
neighbouring agents, which could yield suboptimal solutions (Kuba et al., 2021a). Furthermore,
multi-agent policy gradient methods can suffer from high variance (Kuba et al., 2021b). In contrast,
our methods employ trust region optimisation and do not assume any parameter sharing.
HATRPO (Kuba et al., 2021a) introduced the first multi-agent trust region method that enjoys
theoretically-justified monotonic improvement guarantee. Its key idea is to make agents follow a
2
Under review as a conference paper at ICLR 2022
sequential policy update scheme so that the expected joint advantage will always be positive, thus
increasing reward. In this work, we show how to further develop this theory and derive a protocol
which, in addition to the monotonic improvement, also guarantees to satisfy the safety constraint at
every iteration during learning. The resulting algorithm (Algorithm 1) successfully attains theoreti-
cal guarantees of both monotonic improvement in reward and satisfaction of safety constraints.
3	Problem Formulation: Constrained Markov Game
We formulate the safe MARL problem as a constrained Markov game〈N, S, A, p, p0, γ, R, C, c〉.
Here, N = {1,. ..,n} is the set of agents, S is the state space, A = ∏J=ι A1 is the product of
the agents, action spaces, known as the joint action space, p : S × A ×S → R is the probabilistic
transition function, p0 is the initial state distribution, γ ∈ [0,1) is the discount factor, R : S×A → R
is thejoint reward function, C = {Ci. }i-∈≤≤N.<mi is the set of sets of cost functions (every agent i has ml
cost functions) of the form C∖ : S×Al → R, and finally the set of corresponding cost-constraining
values is given by C = {c'}1<N^. At time step t, the agents are in a state st, and every agent i takes
an action a； according to its policy πl (al |st). Together with other agents, actions, it gives a joint
action a； = (a1,..., a?) and thejoint policy π (a|s) = ∏l=ι πl (al |s). The agents receive the reward
R(s；,a；), meanwhile each agent i pays the costs Cl.(s；,a；), ∀j = 1,...,ml. The environment then
transits to a new state s；+1 ~ p(*|s；, a；). In this paper, We consider a fully-cooperative setting where
all agents share the same reward function, aiming to maximise the expected total reward of
∞
j(π), 旧 s0~Q0, >0：8 ~∏, s L∞ ~p 12 γt R (s；, a；)],
；=0
meanwhile trying to satisfy every agent i,s safety constraints, written as
∞
JKn)，Es0~p0,a0g~∏,sL∞~p∣ 2 T； Cij(St, a；)] < c≈,	V j = 1,...,ml.	⑴
；=0
We define the state-action value and the state-value functions in terms of reward as
∞
Q∏(S, a),旧si:∞~P,at∞~∏	T；R(s；,a；)s0= S,a0 =a
；=0
and V∏ (S)，Ea~∏ [Q∏ (s, a)].
The joint policies π that satisfy the Inequality (1) are referred to as feasible. Notably, in the above
formulation, although the action a； of agent i does not directly influence the costs {C/ (s；, a，)}mj=ι
of other agents k ≠ i, the action a； will implicitly influence their total costs due to the dependence
on the next state s；+i 2. For the Jth cost function of agent i, we define the Jth state-action cost value
function and the state cost value function as
∞
QIj,∏ (s, al) ,旧a-i~π-i,si：8~p,ai：8~n I 2 T；C) (St, a；) I 2 * * s0 = S, a0 = &[,
；=0
∞
and, Vj,∏ (s)，Ea~∏,SL∞~p,aL∞~∏ ∣ J^T；C)(s；, a；) ∣ S0 = s],	respectively.
；=0
Notably, the cost value functions Qlj,π and Vjl,π, although similar to traditional Qπ and Vπ, involve
extra indices i and j; the superscript i denotes an agent, and the subscript j denotes its Jth cost.
Throughout this work, we pay a close attention to the contribution to performance from different
subsets of agents, therefore, we introduce the following notations. We denote an arbitrary subset
2We believe that this formulation realistically describes multi-agent interactions in the real-world; an action
of an agent has an instantaneous effect on the system only locally, but the rest of agents may suffer from its
consequences at later stages. For example, consider a car that crosses on the red light, although other cars may
not be at risk of riding into pedestrians immediately, the induced traffic may cause hazards soon later.
3
Under review as a conference paper at ICLR 2022
{iι,..., ih } of agents as 人4;We write -人4 to refer to its complement. Given the agent subset 人力,
we define the multi-agent state-action value function :
Qi∏h (s, a'i") , Ea-%“ ~∏-%” [β∏(s, a，L”, a-i1^)].
On top of it, the multi-agent advantage function 3 * is defined as follows,
A∏" (s, a J'1*, ai1")，Q∏*,i1" (s, a 九,ail:") - Q∏* (ʌ, a 八*).	(2)
An interesting fact about the above multi-agent advantage function is that any advantage A∏-h can
be written as a sum of sequentially-unfolding multi-agent advantages of individual agents, that is,
Lemma 1 (Multi-Agent Advantage Decomposition, KUba et al. (2021b)). For any state S ∈ S,
subset of agents iι力 ⊆ N, and joint action ai1:h, thefollowing identity holds
h
A∏-h (s, ai1:h) = 2 An (s, ai1:j-1, αlj).
'=1
4 Multi-Agent Constrained Policy Optimisation
In this section, we first present a theoretically-justified safe multi-agent policy iteration procedure,
which leverages multi-agent trust region learning and constrained policy optimisation to solve con-
strained Markov games. Based on this, we propose two practical deep MARL algorithms, enabling
optimising neural-network based policies that satisfy safety constraints. Throughout this work, we
refer the symbols π and π to be the “current” and the “new”joint policies, respectively.
4.1	Multi-agent Trust Region Learning With Constraints
Kuba et al. (2021a) introduced the first multi-agent trust region method—HATRPO—that enjoys
theoretically-justified monotonic improvement guarantee. Specifically, it relies on the multi-agent
advantage decomposition in Lemma 1, and the “surrogate” return that is given as follows.
Definition 1. Let π be a joint policy, ∏i1:h-1 be some other joint policy of agents zih_1, and 登ih be
a policy of agent ih. Then we define
理h (∏i1：h-1,7^ih) , Es~p∏,a%h-1 ~∏Uh-I ,a，h~A，h [An (s, ai1:h-1, aih)].
With the above definition, we can see that Lemma 1 allows for decomposing the joint surrogate
return Ln(∏) ,旧S〜0π,a〜n [A∏(s, a)] into a sum over surrogates of L∏h (∏i1:h-1 ,亓ih), for h =
1,...,机 This can be used to justify that if agents, with a joint policy π, update their policies
by following a sequential update scheme, that is, if each agent in the subset i1:h sequentially solves
the following optimisation problem:
τrih = max L∏:" (∏i1:h-1 ,介ih) - VDmLx (πih ,πih),
Alh
where V = "maχs,a |”(s,a)|, and Dm?x(πih,τ^ih) , maxDKL(^ih(∙∣s),τ^ih(∙∣s)),
(1 - γ)2	kl	S
then the resulting joint policy π will surely improve the expected return, i.e., J(π) ≥ J(π) (see the
proof in Kuba et al. (2021a, Lemma 2)). We know that due to the penalty term DmLx(冗Ih，戈Ih), the
new policy nIil will stay close (w.r.t max-KL distance) to 小.
For the safety constraints, we can extend Definition 1 to incorporate the “surrogate” cost, thus al-
lowing us to study the cost functions in addition to the return.
Definition 2. Let ∏ be a joint policy, and 亓i be some other policy of agent i. Then, for any of its
costs ofindex j ∈ {1,...,mi}, we define
L},n(亓i)= 旧S〜0∏,al〜Al hAin (s, ai)i .
3We would like to highlight that these multi-agent functions of Qin1:h and Ain1:h , although involve agents in
superscripts, describe values w.r.t the reward rather than costs since they do not involve cost subscripts.
4
Under review as a conference paper at ICLR 2022
By generalising the result about the surrogate return in Equation (1), we can derive how the expected
costs change when the agents update their policies. Specifically, we provide the following lemma.
Lemma 2. Let π and ∏ be joint policies. Let i ∈ N be an agent, and j ∈ {1,...,ml} be an index
of one of its costs. The following inequality holds
JL	4γmax. αt |Al. (s,αl)|
4(n) ≤ 4 (π) + Llj,∏ (亓1) + V) £ DKax(π ,τf ), where K =------------(r-^)2-----------.
See proof in Appendix A. The above lemma suggests that, as long as the distances between the
policies πh and 亓h, ∀h ∈ N, are sufficiently small, then the change in the Jth cost of agent i, i.e.,
J； (∏) - JIj (π), is controlled by the surrogate LIj ∏ (亓1). Importantly, this surrogate is independent of
other agents’ new policies. Hence, when the changes in policies of all agents are sufficiently small,
each agent i can learn a better policy 亓1 by only considering its own surrogate return and surrogate
costs. To summarise, we provide the following algorithm that guarantees both safety constraints
satisfaction and monotonic performance improvement.
Algorithm 1: Safe Multi-Agent Policy Iteration with Monotonic Improvement Property
1:
2:
3:
4:
5:
6:
7:
8:
Initialise a safe joint policy ∏o = (πj,...,瑶).
for k = 0,1,... do
Compute the advantage functions A∏k (s, a) and Aj ∏ (s, ai), for all state-(joint)action pairs
(s, a), agents i, and constraints j ∈ {1,...,ml}.
Compute V = 4>max；-；y Ga)I, and 匕.=4，吟；*小,凉)l
Draw a permutaion i1:L of agents at random.
,∀i ∈N ,j = 1,...,mi.
for h = 1 : n do
Compute the radius of the KL-constraint δlh // see Appendix B for the setup of δlh.
Make an update πtι=argmaχ #h ∈∏% h Lin 卜加/Cj)- VD max Hh,∙ )i,
where Πlh is a subset of safe policies of agent ih, given by
Πlh = Kh ∈ Πlh I DmLx(4，拒h) ≤ δlh, and
h-1
Jjh (∏*)+LjhnMih)+Vjh Dmax (戏，万lh) ≤ Cjh- ∑ V 沙 max (碎，小),Vj=ι,...,ml}.
'	Z=I
9:	end for
10:	end for
In the above algorithm, in addition to sequentially maximising agents’ surrogate returns, the agents
must assure that their surrogate costs stay below the corresponding safety thresholds. Meanwhile,
they have to constrain their policy search to small local neighbourhoods (w.r.t max-KL distance).
As such, Algorithm 1 demonstrates two desirable properties: reward performance improvement and
satisfaction of safety constraints, which we justify in the following theorem.
Theorem 1. Ifa sequence of joint policies (π⅛) ∞=0 is obtained from Algorithm 1, then it has the
monotonic improvement property, J(π⅛+ι) ≥ J(πQ, as well as it satisfies the safety constraints,
JIj (∏k) ≤ Cjr for all k ∈ N,i ∈ N, and j ∈ {1,...,ml}.
See proof in Appendix B. The above theorem assures that agents that follow Algorithm 1 will only
explore safe policies; meanwhile, every new policy will be guaranteed to result in performance
improvement. These two properties hold under the conditions that only restrictive policy updates
are made; this is due to the KL-penalty term in every agent,s objective (i.e., vDmax(k?, πlh)), as well
as the constraints on cost surrogates (i.e., the conditions in ∏lh). In practice, it can be intractable to
evaluate DKL(瞰(∙∣s),小(∙∣S)) at every state in order to compute DmLx(瞰,小).In the following
subsections, we describe how we can approximate Algorithm 1 in the case of parameterised policies,
similar to TRPO/PPO implementations (Schulman et al., 2015; 2017).
5
Under review as a conference paper at ICLR 2022
4.2 MACPO: Multi-Agent Constrained Policy Optimisation
Here we focus on the practical settings where large state and action spaces prevent agents from des-
ignating policies πl(∙∣S) for each state separately. To handle this, We parameterise each agent's πlgi
by a neural network 俨.Correspondingly, thejoint policies ∏θ are parametrised by θ = (θ 1,...,θn).
Let,s recall that at every iteration of Algorithm 1, every agent i卜 maximises its surrogate return with
a KL-penalty, subject to surrogate cost constraint. Yet, direct computation of the max-KL constraint
is intractable in practical settings, as it would require computation of KL-divergence at every single
state. Instead, one can relax it by adopting a form of expected KL-constraint DKL (戏,小)≤ δ
where DKL(嗯,小)，旧s~Qπ^ [□kl(k? (∙∣s),πlħ (∙∣s))]. Such an expectation can be approximated
by stochastic sampling. As a result, the optimisation problem solved by agent i⅛ is written as
%=*m ax ESi H-寸--i -⅛.	(s, al1 j' al")i
"k+1
s.t.	JT	(πθ j	+ ES ~pπ°	,aih 〜πih.	h a 7∏θjt	(s' al")i ≤	c lj,,	∀ J	∈ {1,…，m l" },
θ
and DKL 威，小)≤ 6.	(3)
We can further approximate Equation (3) by Taylor expansion of the optimisation objective and cost
constraints up to the first order, and the KL-divergence up to the second order. Consequently, the
optimisation problem can be written as
θ3 = arg max (glh )，(θih - θg)
s.t.
and
d；h + (bi;)τ (θlh - θi") ≤ 0, J= 1,...,rn
1 (θl'h - θ? )τ Hl'h (θl'h - θ?) ≤ 6，
(4)
where glh is the gradient of the objective of agent i⅛ in Equation (3), dlj = Jj (∏ej - C., and
Hlh = V，i D KL (冗 Ihi ,πlh )∣ i % is the Hessian of the average KL divergence of agent i⅛, and blh
" h	屋	P h = Dk	J
is the gradient of agent of the Jth constraint of agent Zz.
Similar to Chow et al. (2017) and Achiam et al. (2017), one can take a primal-dual optimisation
approach to solve the linear quadratic optimisation in Equation (4). Specifically, the dual form can
be written as:
max 三[(gih)τ(Hlh)-1 gih - 2(rlh)τVih + (Vih)τSl'h] + (Vih)τclh - T,
Λih ≥0,vih >o 2λlh	2
where rih，(gih)τ(Hih)-1Bih,Bih = ∣bh,...,b⅛] andSih，(Bih)τ(Hih)-1Bih.	(5)
Given the solution to the dual form in Equation (5), i.e., 4；h and v：h, the solution to the primal
problem in Equation (4) can thus be written by
θ≈h = θ? + ɪ (Hih)-1 (gih - Bihvh).
In practice, we use backtracking line search starting at 1∕%h to choose the step size of the above
update. Furthermore, we note that the optimisation step in Equation (4) is an approximation to the
original problem from Equation (3); therefore, it is possible that an infeasible policy KDih will be
generated. Fortunately, as the policy optimisation takes place in the trust region of k：” , the size of
update is small, and a feasible policy can be easily recovered. In particular, for problems with one
safety constraint, i.e.,mlh = 1, one can recover a feasible policy by applying a TRPO step on the
cost surrogate, written as * *
叱1 = θ K- a 'J=τ,:∖-1to, (Hih)-1bih
bih (Hih)-1bih
(6)
6
Under review as a conference paper at ICLR 2022
where aJis adjusted through backtracking line search. To PUt it together, We refer to this algorithm
as MACPO, and provide its pseudocode in Appendix D.
4.3 MAPPO-Lagrangian
In addition to MACPO, one can use Lagrangian multiPliers in Place of oPtimisation with linear
and quadratic constraints to solve Equation (3). The Lagrangian method is simPle to imPlement,
and it does not require computations of the Hessian HIh whose size grows quadratically with the
dimension of the parameter vector θlh.
Before we proceed, let us briefly recall the optimisation procedure with a Lagrangian multiplier.
Suppose that our goal is to maximise a bounded real-valued function f (x) under a constraint g (x);
maxx f (ɪ), s.t. g(x) ≤ 0. We can introduce a scalar variable λ and reformulate the optimisation by
max min f ⑴ -λg (x).	(7)
X A ≥0
Suppose that ɪ+ satisfies g(x+) > 0. This immediately implies that -λg(x+) → -∞, as λ → +∞,
and so Equation (7) equals -∞ for X = x+. On the other hand, if X- satisfies g(x-) ≤ 0, we have
that -λg(x-) ≥ 0, with equality only for λ = 0. In that case, the optimisation objective,s value
equals f (x-) > -∞. Hence, the only candidate solutions to the problem are those X that satisfy the
constraint g (x) ≤ 0, and the objective matches with f (x).
We can employ the above trick to the constrained optimisation problem from Equation (3) by sub-
suming the cost constraints into the optimisation objective with Lagrangian multipliers. As such,
agent Zz computes λ'% and θ3 to solve the following min-max optimisation problem
min max
A3h ≥0 附
E	,-
s”ne"1:h-1 5
fe
?1:h-1
J1:h-1
fc+1
/h
,a，h ~嗡 hRM，al1：h-1，alh )]
-Σ%"	E.
u=1	'
,s-P∏θ ,a,h“hj缁3(s‘ a'")] + 心
K	y h
θ
m
s.t.万KL卜%，吸h)≤ 6.
(8)
Although the objective from Equation (8) is affine in the Lagrangian multipliers λ^ (u = 1,..., mlh),
which enables gradient-based optimisation solutions, computing the KL-divergence constraint still
complicates the overall process. To handle this, one can further simplify it by adopting the PPO-clip
objective (Schulman et al., 2017), which enables replacing the KL-divergence constraint with the
clip operator, and update the policy parameter with first-order methods. We do so by defining
m,h
Any (s, a'1:h-1 ,小)，A^k (ʌ, al1:h-1 ,aih) - X λ%	(s, /) + 明,
u=1
and rewriting the Equation (8) as
∕hm'max ES %” ,ai1-1 ~∏%h-1∕h ~ 叱 hAih, T) (s, ai1:h-1, aih )i,
A h . ≥0 6 h	θK	θ,Lh-1	θlh
1:mlh -	K+1
s.t.方KL卜%,吸h) ≤ 6.
(9)
The objective in Equation (9) takes a form ofan expectation with quadratic constraint on the policy.
Up to the error of approximation of KL-constraint with the clip operator, it can be equivalently
transformed into an optimisation ofa clipping objective. Finally, the objective takes the form of
E
θK
,al1:h-1
l1:h-1
〜∏ .
l1:h-1
θK+1
,alh
min f ： A∏hθT)
6K
(s, ai1:h-1, aih),
clM二,1 ±少甯)(s,…,a)!|. (IO)
7
Under review as a conference paper at ICLR 2022
(a)	(b)	(c)
Figure 1: Example tasks in Safe Multi-Agent MuJoCo Environment. (a): Safe 2x4-Ant, (b): Safe
4x2-Ant, (c): Safe 2x3-HalfCheetah. Body parts of different colours are controlled by different
agents. Agents jointly learn to manipulate the robot, while avoiding crashing into unsafe red areas.
The clip operator replaces the policy ratio with 1 - £, or 1 + e, depending on whether its value is
below or above the threshold interval. As such, agent i扭 can learn within its trust region by updating
仍拉 to maximise Equation (10), while the Lagrangian multipliers are updated towards the direction
opposite to their gradients of Equation (8), which can be computed analytically. We refer to this
algorithm as MAPPO-Lagrangian, and give a detailed pseudocode of it in Appendix E.
5	Experiments
Although MARL researchers have long had a variety of environments to test different algorithms,
such as StarCraftII (Samvelyan et al., 2019) and Multi-Agent MuJoCo (Peng et al., 2020), no public
safe MARL benchmark has been proposed; this impedes researchers from evaluating and bench-
marking safety-aware multi-agent learning methods. As a key contribution of this paper, we intro-
duce Safe Multi-Agent MuJoCo Benchmark, a safety-aware extension of the MuJoCo environment
that is designed for safe MARL research. We show example tasks in Figure 1, in our environment,
safety-aware agents have to learn not only skilful manipulations of a robot, but also to avoid crashing
into unsafe obstacles and positions. For more details of the setup, please refer to Appendix F.
We use Safe MAMuJoCo to examine if the MACPO/MAPPO-Lagrangian agents can satisfy
their safety constraints and cooperatively learn to achieve high rewards, compared to existing
MARL algorithms. Notably, our proposed methods adopt two different approaches for achieving
safety. MACPO reaches safety via hard constraints and backtracking line search, while MAPPO-
Lagrangian maintains a rather soft safety awareness by performing gradient descents on the PPO-
clip objective. Figure 2 shows cost and reward performance comparisons between MACPO,
MAPPO-Lagrangian, MAPPO (Yu et al., 2021), IPPO (de Witt et al., 2020), and HAPPO (Kuba
et al., 2021a) algorithms on three challenging tasks. Figure 2 should be interpreted at three-folds;
each subfigure represents a different robot, within each subfigure, three task setups in terms of multi-
agent control are considered, for each task, we plot the cost curves (the lower the better) in the upper
row, and plot the reward curves (the higher the better) in the bottom row. Detailed hyperparameter
settings are described in Appendix H.
The experiments reveal that both MACPO and MAPPO-Lagrangian quickly learn to satisfy safety
constraints, and keep their explorations within the feasible policy space. This stands in contrast to
IPPO, MAPPO, and HAPPO which largely violate the constraints thus being unsafe. Furthermore,
our algorithms achieve comparable reward scores; both methods are often better than IPPO. In
general, the performance (in terms of reward) of MAPPO-Lagrangian is better than of MACPO;
moreover, MAPPO-Lagrangian outperforms the unconstrained MAPPO on challenging Ant tasks.
We note that on none of the tasks the reward of HAPPO was exceeded though it is unsafe.
6	Conclusion
In this paper, we tackled multi-agent policy optimisation problems with safety constraints. Central
to our findings is the safe multi-agent policy iteration procedure that attains theoretically-justified
monotonic improvement guarantee and constraints satisfaction guarantee at every iteration during
learning. Based on this, we proposed two practical algorithms: MACPO and MAPPO-Lagrangian.
To demonstrate their effectiveness, we introduced a new benchmark suite of Safe Multi-Agent Mu-
JoCo and compared our methods against strong MARL baselines. Results show that both of our
methods can significantly outperform existing state-of-the-art methods such as IPPO, MAPPO and
HAPPO in terms of safety, meanwhile maintaining comparable performance in terms of reward.
8
Under review as a conference paper at ICLR 2022
ManyAgent Ant 6xl
ManyAgent Ant 2x3
ɪ - 1 J IR I ,
Is。。∙,po-d 山∙,6 2」∙,><
0.0	02	04	0«	0∙β	1.0
Environment steps lβ7
ManyAgent Ant 3x2
HAPPO
MAPPO
——MACPO (ours)
MAPPO4. (ours)
IPPO
OO 02 0Λ 0.6 Og
Environment steps
1.0
1∙7
02	04	0«	0∙β	1.0
Environment steps lβ7
0.0	0.2	0.4 O.e 0.8	1.0
Environment steps lβ7
ManyAgent Ant 6×1
□.1",aλq,m∙,□o2du vraEV>⅞
HAPPO
MAPPO
——MACPO (ours)
MAPPO4. (ours)
IPPO
0.0	02	04	0«	0∙β
Environment steps
400-
t⅛35°
8 3∞
(υ
P 250-
O
0.200
LU
(U 150-
6
E 100
(υ
2 so
O
■ ⅝ ■ ⅝ •， •，
PJ2M∙,≈∙,po-d 山∙,6 2」∙,><
(a)	Safe ManyAgent Ant: 2x3-Agent (left), 3x2-Agent (middle), 6x1-Agent (right)
Ant 2x4
Ant 2x4 d
4000-
E 3500-
lŋ
[3000-
fl> 2500-
m 2∞0-
∣S-1500-
(υ
ra1∞o
0.0	0.2	0.4 O.e 0.8
Environment steps
Ant 4x2
4S。。∙,po--dw∙,6 2」∙,>v
350
t⅛
8 300
■S 2∞
200-
£■
(U «0-
6
P ioo
50- Z
O-
OX)
HAPPO
MAPPO
MACPO (ours)一
MAPPO-L (ours)
IPpO
Al/ ZV∖JE‰A√⅝⅜K
02	04	0«	0∙β
Environment steps
■g 35∞∙
F 30∞
2500
2000
1500
ω ,
8:
g, ιooo
lŋ
5∞
O-
MAPPO
MACPO (ours)
MAPPO-L (ours)
IPPO
HAPPO
Ant 2x4 d
0.0	02	04	04 Og
Environment steps
(b)	Safe Ant: 2x4-Agent (left), 4x2-Agent (middle), 2x4d-Agent (right)
Is。。∙,po--dw ωɑEω><
HaIfCheetah 3×2
Environment steps
HaIfCheetah 6×1
Environment steps
p∙leM∙,α∙,po--dw∙,6 2」∙,>v
HalfCheetah 3x2
HaIfCheetah 6×1
0.0	02	04	0«	0∙β	1.0	0.0	0.2	0.4	0.C	0.8	1.0	0.0	02	04	0«	0∙β	1.0
Environment steps	lβ7	Environment steps	lβ7	Environment steps	lβ7
(c) Safe HalfCheetah: 2x3-Agent (left), 3x2-Agent (middle), 6x1-Agent (right)
Figure 2: Performance comparisons on tasks of Safe ManyAgent Ant, Safe Ant, and Safe HalfChee-
tah in terms of cost (the first row) and reward (the second row). The safety constraint values are: 1
for ManyAgent Ant, 0.2 for Ant, and 5 for HalfCheetah. Our methods consistently achieve almost
zero costs, thus satisfying safe constraints, on all tasks. In terms of reward, our methods outperform
IPPO and MAPPO on some tasks but underperform HAPPO, which is also an unsafe algorithm.
9
Under review as a conference paper at ICLR 2022
References
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In
International Conference on Machine Learning, pp. 22-31. PMLR, 2017.
Eitan Altman. Constrained Markov decision processes, volume 7. CRC Press, 1999.
Aaron D Ames, Xiangru Xu, Jessy W Grizzle, and Paulo Tabuada. Control barrier function based
quadratic programs for safety critical systems. IEEE Transactions on Automatic Control, 62(8):
3861-3876, 2016.
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mane. Con-
crete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.
Urs Borrmann, Li Wang, Aaron D Ames, and Magnus Egerstedt. Control barrier certificates for safe
swarm behavior. IFAC-PapersOnLine, 48(27):68-73, 2015.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained re-
inforcement learning with percentile risk criteria. The Journal of Machine Learning Research, 18
(1):6070-6120, 2017.
Yinlam Chow, Ofir Nachum, Edgar Duenez-Guzman, and Mohammad Ghavamzadeh. A lyapunov-
based approach to safe reinforcement learning. arXiv preprint arXiv:1805.07708, 2018.
Yinlam Chow, Ofir Nachum, Aleksandra Faust, Edgar Duenez-Guzman, and Mohammad
Ghavamzadeh. Lyapunov-based safe policy optimization for continuous control. arXiv preprint
arXiv:1901.10031, 2019.
Christian Schroeder de Witt, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip HS
Torr, Mingfei Sun, and Shimon Whiteson. Is independent learning all you need in the starcraft
multi-agent challenge? arXiv preprint arXiv:2011.09533, 2020.
Xiaotie Deng, Yuhao Li, David Henry Mguni, Jun Wang, and Yaodong Yang. On the complex-
ity of computing markov perfect equilibrium in general-sum stochastic games. arXiv preprint
arXiv:2109.01795, 2021.
Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 32, 2018.
Javier Garcia and Fernando Fernandez. A comprehensive survey on safe reinforcement learning.
Journal of Machine Learning Research, 16(1):1437-1480, 2015.
Jakub Grudzien Kuba, Ruiqing Chen, Munning Wen, Ying Wen, Fanglei Sun, Jun Wang, and
Yaodong Yang. Trust region policy optimisation in multi-agent reinforcement learning. arXiv
preprint arXiv:2109.11251, 2021a.
Jakub Grudzien Kuba, Muning Wen, Yaodong Yang, Linghui Meng, Shangding Gu, Haifeng Zhang,
David Henry Mguni, and Jun Wang. Settling the variance of multi-agent policy gradients. arXiv
preprint arXiv:2108.08612, 2021b.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Songtao Lu, Kaiqing Zhang, Tianyi Chen, Tamer Basar, and Lior Horesh. Decentralized policy
gradient descent ascent for safe multi-agent reinforcement learning. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 35, pp. 8767-8775, 2021.
Teodor Mihai Moldovan and Pieter Abbeel. Safe exploration in markov decision processes. In Pro-
ceedings of the 29th International Coference on International Conference on Machine Learning,
pp. 1451-1458, 2012.
10
Under review as a conference paper at ICLR 2022
Bei Peng, Tabish Rashid, Christian A Schroeder de Witt, Pierre-Alexandre Kamienny, Philip HS
Torr, Wendelin Bohmer, and Shimon Whiteson. Facmac: Factored multi-agent centralised policy
gradients. arXiv preprint arXiv:2003.06709, 2020.
David Pollard. Asymptopia: an exposition of statistical asymptotic theory. 2000. URL http://www.
stat. yale. edu/pollard/Books/Asymptopia, 2000.
Zengyi Qin, Kaiqing Zhang, Yuxiao Chen, Jingkai Chen, and Chuchu Fan. Learning safe multi-agent
control with decentralized neural barrier certificates. In International Conference on Learning
Representations, 2020.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and
Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforce-
ment learning. In International Conference on Machine Learning, pp. 4295-4304. PMLR, 2018.
Alex Ray, Joshua Achiam, and Dario Amodei. Benchmarking safe exploration in deep reinforcement
learning. arXiv preprint arXiv:1910.01708, 7, 2019.
Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas
Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson.
The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889-1897. PMLR,
2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement
learning for autonomous driving. arXiv preprint arXiv:1610.03295, 2016.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484-489, 2016.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. nature, 550(7676):354-359, 2017.
Richard S Sutton, David A McAllester, Satinder P Singh, Yishay Mansour, et al. Policy gradient
methods for reinforcement learning with function approximation. In NIPs, volume 99, pp. 1057-
1063. Citeseer, 1999.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, JUny-
oung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster
level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.
Honghao Wei, Xin Liu, and Lei Ying. A provably-efficient model-free algorithm for constrained
markov decision processes. arXiv preprint arXiv:2106.01577, 2021.
Tengyu Xu, Yingbin Liang, and Guanghui Lan. Crpo: A new approach for safe reinforcement
learning with convergence guarantee. In International Conference on Machine Learning, pp.
11480-11491. PMLR, 2021.
Yaodong Yang and Jun Wang. An overview of multi-agent reinforcement learning from game theo-
retical perspective. arXiv preprint arXiv:2011.00583, 2020.
Yaodong Yang, Ying Wen, Jun Wang, Liheng Chen, Kun Shao, David Mguni, and Weinan Zhang.
Multi-agent determinantal q-learning. In International Conference on Machine Learning, pp.
10757-10766. PMLR, 2020.
Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising
effectiveness of mappo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955,
2021.
11
Under review as a conference paper at ICLR 2022
Moritz A. Zanger, Karam DaaboUL and J. Marius Zollner. Safe continuous control with constrained
model-based policy optimization, 2021.
Wenbo Zhang, Osbert Bastani, and Vijay Kumar. Mamps: Safe multi-agent reinforcement learning
via model predictive shielding. arXiv preprint arXiv:1910.12639, 2019.
12
Under review as a conference paper at ICLR 2022
Appendices
A Proofs of preliminary results	14
B Auxiliary Results for Algorithm 1	15
C Auxiliary Results for Implementation of MACPO	17
D MACPO	18
E	MAPPO-Lagrangian	19
F	Safe Multi-Agent MuJoCo	20
G	Experiments in Safe Many-Agent Ant Environments	21
H	Details of Settings for Experiments	22
13
Under review as a conference paper at ICLR 2022
A Proofs of preliminary results
Lemma 1 (MUlti-Agent Advantage Decomposition, Kuba et al. (2021b)). For any state s ∈ S,
subset of agents 人力 ⊆ N, and joint action al1h, thefollowing identity holds
h
A∏-h (s, al1:h) = 2 An (s, al1:j'-1 ,αl■>).
J=ι
Proof. We write the multi-agent advantage as in its definition, and expand it in a telescoping sum.
Λi∏-h (s, all:h) = β⅛h (s, all:h) - V∏ (S)
h
=2 IQn，(s, ail--碎T (s, ai"1)]
J=1
h
=2 Λ∏ (s, ai1:∕τ,αi∕)∙
J=1
Lemma 2. Let π and ∏ be joint policies. Let i ∈ N be an agent, and j ∈ {1,...,ml} be an index
of one of its costs. The following inequality holds
Al	,	,	4γmax. ai IΛl. (s,αl)|
JHn) ≤ J}(π)+ L；,n (亓1) + Vlj 2 DmL Sh,亓h), where Vj=	(1 - y),2-----.
Proof. From the proof of Theorem 1 from Schulman et al. (2015) (in particular, equations (41)-(45)),
applied to joint policies π and π, We conclude that
J(n) ≤ Jij(n) + Es~p∏,a~n |A；n(s,ai)i
4α2γ max$,a，IΛijπ(s,αi)|
+	(1 - 7)2
where a = DmVx (π, ∏) = maxDTV (∏(∙∣s),∏(∙∣s)).
Using the inequality DTV(p, q)2 ≤ DKL(p, q) (Pollard, 2000; Schulman et al., 2015), we obtain
Jij (∏) ≤ Jij (∏) + 旧s% ,a〜π h ”n (s, ai)i
4γmax5 a IΛl,∙ Λs,αl)∣
+	ɑ - 7,∏—D mLx(∏,∏),
where	DKLx(n, ∏) = maxDKL (∏(∙∣s), ∏(∙∣s)).
Notice now that we have Es~0∏,a~n [Λj,n(s, ai)]=旦~日,八7f≈ [Λj,n(s, ai)], and
DmLx(n,∏) = maxDKL (∏(∙∣s),∏(∙∣s)) = max ∣2 DKL 卜'(∙∣s),亓l(∙∣s)
S	S	l=1
n	n
≤ 2 mfxDKL k'(∙∣s),亓l(∙∣S)) = 2 DmLx k',町.
Z=1 S	Z=I
(11)
.	4y max ? I A", (s,α") I
Setting Vlj =--------4-产-------------,We finally obtain
n
JiHn) ≤ Jijg + L≈,n (亓i) + Vij 2 DmLx 口，亓z).
Z=1

14
Under review as a conference paper at ICLR 2022
B Auxiliary Results for Algorithm 1
Remark 1. In Algorithm 1, we compute the size of KL constraint as
.ʃ ..
= min min min
Z ≤h-1 1≤J ≤mi

4Y (∏*) - LM (婢+1)Y Eh-1D KL (咪碟+1)
il
v J
min min
Z ≥h+ι ι≤ j ≤m
4Y (∏k)-vi: 2h-1 D KL (碟,碟+1)
il
Vj
Note that δlχ (i.e., h = 1) is guaranteed to be non-negative if π左 satisfies safety constraints; that is
because then cij ≥ J； (π⅛) for all / and j, and the set {/ | ∕<h} is empty.
This formula for δlh, combined with Lemma 2, assures that the policies πlh within δlh max-KL
distance from π/ will not violate other agents' safety constraints, as long as the base joint policy
∏k did not violate them (WhiCh assures δl1 ≥ 0). To see this, notice that for every / = 1,...,h - 1,
and j = 1,...,ml,
max
D KL
πlh ) ≤ δlh ≤
成
CiI-J: (∏Q - LInM 口 -V 2 h-1 D KL (π%, πQ)
li
Vj
h-1
implies J(∏k)+ L‰(π11) + vil X DKxg,πQ) + ViiDmL或,πlh) ≤ c：.
u=1
By Lemma 2, the left-hand side of the above inequality is an upper bound of J； (∏+h- ,πlh, n-l1:h),
which implies that the update of agent 力h doesn't violate the constraint of Jl. The fact that the
constraints of Jl； for / ≥ h + 1 are not violated, i.e.,
h-1
Jll (∏k) + V X DmL (π%, π%+1) + V DmL (π≈h ,πlh) ≤ C,	(12)
M=1
is analogous.
Theorem 1.	Ifa sequence of joint policies (∏⅛) ∞=0 is obtained from Algorithm 1, then it has the
monotonic improvement property, J(∏⅛+1) ≥ J(∏Q, as well as it satisfies the safety constraints,
Jlj (∏k) ≤ J, for all k ∈ N, i ∈ N, and j ∈ {1,...,ml}.
Proof. Safety constraints are assured to be met by Remark 1. It suffices to show the mono-
l	l'h
tonic improvement property. Notice that at every iteration k of Algorithm 1, π j ∈ Π . Clearly
DmLx(K ,πljh) = 0 ≤ δih. Moreover,
h-1
Jjh(∏k) + LM(πih)+ VjhDmLx成,π) = JjhE) ≤ Cjh-Vjh XDmLx位,π3),
Z=1
15
Under review as a conference paper at ICLR 2022
where the inequality is guaranteed by updates of previous agents, as described in Remark 1 (Inequal-
ity 12). By Theorem 1 from Schulman et al. (2015), we have
J(∏k+l) ≥ J(∏k) + 旧s”心,a-∏fc+ι [Ank (s, a)] -V。KLx(∏k, ∏k+l),
which by Equation 11 is lower-bounded by
n
≥ J(∏k) + Es~∕ ,a 〜πk+1 [Ank (S, a) ] - 2 VDKLx (πk ,唠 +1)
h=1
which by Lemma 1 equals
n	n
=J(∏k) + ∑ Es~	,a，i：“	[AnJS, a'1"-1,迪)]-X VDKLx(πkMi)
h=1	+	h=1
n
=J(∏k) + ∑ (Ln (∏k1T /3) -VDKLx(砂,瘴+1)) ,	(13)
h=1
and as for every h, n^ is the argmax, this is lower-bounded by
n
≥ J(∏k) + ∑ (Ln (∏k1+1-1 ,霏)-VDKLx(砂，砂))，
h=1
which, as follows froK Definition 1, equals
n
= J(∏k) +	0 = J(∏k), which finishes the proof.
h=1
16
Under review as a conference paper at ICLR 2022
C Auxiliary Results for Implementation of MACPO
Theorem 2.	The solution to the following problem
p* = min gr X
X
s.t. br x + C ≤ 0
Xr Hx ≤ δ,
where g, b, X ∈ RN, c, δ ∈ R,δ > 0, H ∈ SN, and H A 0. When there is at least one Strictlyfeasibk
point, the optimal point x* satisfies:
x* = --j-HT (gr + v*b
where λ* and v* are defined by
λ*
v * 一
(
+
arg max
A ≥0
fα (λ) , 2A (Zr - q) + A (⅛" - δ)-与
fb (λ)，-1 (A+λδ)
if λc 一 r > 0
otherwise
where q = gr H -1g, r = gr H-1b, and s = br H-1b.
Furthermore, let Λα，{λ | λc 一 r > 0,λ ≥ 0}, and Λ
satisfies
{λ | λC - r ≤ 0, λ ≥ 0} . The value of λ*
λ* ∈'λα, Proj Isiif, Aa) ,λ*, Proj
ι∖
δ, q
(14)
where λ* = λa if fα (λa) > fb (λ?) and λ* = λ* otherwise, and Proj (a, S) is the projection of a
point x on to a set S. Note the projection of a point x ∈ R onto a convex segment of R, [a, b], has
value Proj(x, [α,b]) = max(a, min(b,x)).
Proof. See Achiam et al. (2017) (Appendix 10.2).

17
Under review as a conference paper at ICLR 2022
D MACPO
Algorithm 2: MACPO
1:	Input: Stepsize a, batch size B, number of: agents ? episodes K, steps per episode T, possible
steps in line search L.
2:	Initialize: Actor networks {稣，∀i ∈ N}, Global V-value network {φo}, Individual Vl-cost
networks {φj. 0}i=1：n,j=1：m, Replay buffer B
3:	for k = 0,1,.,..,K - 1 do
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
Collect a set of trajectories by running thejoint policy ∏θk = (π^,...,喂n).
Push transitions {(o；, alt, 0.1，rt),∀z ∈ N,t ∈ T} into B.
Sample a random minibatch of M transitions from B.
Compute advantage function A(s, a) based on global V-value network with GAE.
Compute cost-advantage functions AIj (s, at) based on individual Vt-cost critics with GAE.
Draw a random permutation of agents ii：n.
Set Mj1 (s, a) = A (s, a).
for agent Z 卜=Zi,..., in do
Estimate the gradient of the agent’s maximisation objective
gk = ⅛ W ∑ %? log πihih (αth | oth) Mt1:h (”, a；).
for j =1,..., mlh do
Estimate the gradient of the agent's 严 cost
bjh = ⅛ ∑ι K吸 log π⅛h (aih∖oih) Aih (St,/").
17:
18:
end for
Set Bih = [b 1h,..., bM.
i
Compute Hkih , the Hessian of the average KL-divergence
磊 M ∑ Dkl*" (∙∖oth),π' (∙∖oth) 1.
b=1t=1	%h	”
Solve the dual (5) for %h, v：h.
Use the conjugate gradient algorithm to compute the update direction
19:
th
Xk
Update agent Z 心,s policy by
(Hk)-1 (gk - B认V
20:
21:
22:
% =碟+,谭,
where j ∈ {0,1,..., L} is the smallest such j which improves the sample loss, and
satisfies the sample constraints, found by the backtracking line search.
if the approximate is not feasible then
Use equation (6) to recover policy。3 from unfeasible points.
end if
23:
24:
25:
党 (aih ∖oih )
Compute Mi1:h+1 (s, a) = §晨 ,---Mi1:h (St, at). //Unless h = n.
ih
θk
26:
27: end for
end for
Update V-value network by following formula:
.	.1 1 N I	2
Φk+1 = argminφ Nʃ ∑ ∑ (v0(St) - Rt)
n=1 t=0
18
Under review as a conference paper at ICLR 2022
E MAPPO-Lagrangian
Algorithm 3: MAppO-Lagrangian
1:	Input: Stepsizes aS, a冗，batch size B, number of: agents n, episodes K, steps per episode T,
discount factor γ.
2:	Initialize: Actor networks {θl0, ∀i ∈ N}, Global V-value network {Φ0},
V-COStnetWorks {φ) 0}1≤N≤mi, Replay buffer B.
3:	for k = 0,1,...,K - 1 do"
4:
5
6
7
8
9:
10:
11:
12:
13:
14:
15:
Collect a set of trajectories by running thejoint policy ∏θk =(冗%,...,喂n).
Push transitions {(o；, a；,。；+「rt), ∀ ∈ N" ∈ T} into B.
Sample a random minibatch of B transitions from B.
Compute advantage function A(s, a) based on global V-value network With GAE.
Compute cost advantage functions Nl.(s, al) for all agents and costs,
based on V-cost networks with GAE.
Draw a random permutation of agents z'i：n.
Set Ml1 (s, a) = N (s, a).
for agent i卜=力i,..., in do
Initialise a policy parameter θlh = θlkh ,
and Lagrangian multipliers 岑 =0, ∀ j = 1,..., mlh.
Make the Lagrangian modification step of objective construction
n
Mlh,⑷(St,at) = Mlh(St,at) - £ 儿;NJh(St,a/).
JT
for e = 1,..., 6ppo do
Differentiate the Lagrangian PPO-Clip objective
δ θih =
B T
V 少h B E E min
b=1t=0
⅛⅛⅛hjMIhe)(St,at),clip
θk
一
,1 土 E
-
Mlh，")(St, at).
-
16:
17:
18:
Update temprorarily the actor paramaters
的 J θlh + a β Δ θ ιh.
for J =1,..., mlh do
Approximate the constraint violation
B T
Ch = BT ∑ 3” lh (St)-qh.
19:
Differentiate the constraint
B ©	T
Mh = ⅛ MJf + ∑o
20:
21:
22:
∖
Jh
θih
h
∖
(a；h|o；h)入.	.
带诬 N；”(St ,"t") .
23:
24:
25:
end for
for j = 1, . . . , mlh do
Update temporarily the Lagrangian multiplier
* j ReLU (碑-aλΔΛih^
/
26:
27:
28:
29:
end for
end for
Update the actor parameter θ3 = θlh.
"”
Compute Mlh+1 (s, a) = -^+
"θkh
(a% ∣o% )
--------Mlh(s, a). //Unless h = n.
(小h ∣o% )
30: end for
end for
Update V-value network (and V-cost networks analogously) by following formula:
B T	2
Φk+1 = arg minψ BT EE (y≠ (St) - Rt)
b=1 t=0
19
Under review as a conference paper at ICLR 2022
F Safe Multi-Agent MuJoCo
Safe MAMuJoCo is an extension of MAMuJoCo (Peng et al., 2020). In particular, the background
environment, agents, physics simulator, and the reward function are preserved. However, as oppose
to its predecessor, Safe MAMuJoCo environments come with obstacles, like walls or bombs. Fur-
thermore, with the increasing risk of an agent stumbling upon an obstacle, the environment emits
cost (Brockman et al., 2016). According to the scheme from Zanger et al. (2021), we characterise
the cost functions for each task below.
ManyAgent Ant & Ant
The width of the corridor set by two walls is 9m(ManyAgent Ant), The width of the corridor set by
three folding line walls with an angle of 30 degrees is 10m(Ant). The environment emits the cost of
1 for an agent, if the distance between the robot and the wall is less than 1.8m, or when the robot
topples over. This can be described as
C = ∣0,	for 0.2 ≤ Ztorso ,t+1 ≤ 1.0 and ∣∣Xtorso ,t+1 - Xwall ∣∣2 ≥ 1.8
t ]l, otherwise.
where ztorso ,t+1 is the robot,s torso,s z-coordinate, and xtorso ,t+1 is the robot,s torso,s X-coordinate,
at time t + 1; xwall is the X-coordinate of the wall.
Figure 3: ManyAgent Ant 3x2 with a corridor and Ant 4x2 with three corridors.
HalfCheetah & Couple HalfCheetah
In these tasks, the agents move inside a corridor (which constraints their movement, but does not
induce costs). Together with them, there are bombs moving inside the corridor. If an agent finds
itself too close to a bomb, the distance between an agent and a bomb is less than 9m, a cost of 1 will
be emitted.
0, for	∣ytorso ,t+1 - yobstacle ∣2 ≥ 9
1, otherwise .
where ytorso ,t+1 is the y-coordinate of the robot,s torso, and yobstacle is the y-coordinate of the
moving obstacle.
Figure 4: HalfCheetah 2x3 and Couple HalfCheetah 1P1.
20
Under review as a conference paper at ICLR 2022
G Experiments in Safe Many-Agent Ant Environments
We provide additional results on the Safe Many-Agent ant tasks.
The width of the corridor is 12m; its walls fold at the angle of 30 degrees. The environment emits
the cost of 1 for an agent, if the distance between the robot and the wall is less than 1.8m, or when
the robot topples over. This can be described as
∫0,	for	0.2 ≤ ztorso ,t+1 ≤ 1.0 and IIxtOrSO ,t+1 - Xwall ∣∣2 ≥ 1.8
Ct =
1,	otherwise .
where ztorso ,t+1 is the robot,s torso,s z-coordinate, and xtorso ,t+1 is the robot,s torso,s X-coordinate,
at time t + 1; xwall is the X-coordinate of the wall.
-sφ><
Figure 5: Many-Agent Ant 3x2 with two folding line walls.
ManyAgent Ant 6xl
0.0	0.2	0.4	0.6	0.8
Environment steps
1.0
le7
I ■ , ɪ ■ I -
p」roM ①α ① po-d 山 ΦCT2Φ><
Environment steps
0.0	0.2	0.4	0.6	0.8	1.0
0.0	0.2	0.4	0.6	0.8	1.0
le7
Environment steps
Figure 6: Performance comparisons on tasks of Safe ManyAgent Ant in terms of cost (the first row)
and reward (the second row). The safety constraint values is set to 10. Our algorithms are the only
ones that learn the safety constraints, while achieving satisfying performance in terms of the reward.
21
Under review as a conference paper at ICLR 2022
H Details of Settings for Experiments
In this section, we introduce the details of settings for our experiments.
The code is available at https://github.com/Anonymous-ICLR2022/
Multi- Agent- Constrained- Policy- Optimisation
hyperparameters	value	hyperparameters	value	hyperparameters	value
critic lr	5e-3	optimizer	Adam	num mini-batch	40
gamma	0.99	optim eps	1e-5	batch size	16000
gain	0.01	hidden layer	1	training threads	4
std y coef	0.5	actor network	mlp	rollout threads	16
std x coef	1	eval episodes	32	episode length	1000
activation	ReLU	hidden layer dim	64	max grad norm	10
Table 1: Common hyperparameters used for MAPPO-Lagrangian, MAPPO, HAPPO, IPPO, and
MACPO in the Safe Multi-Agent MuJoCo domain
Algorithms	MAPPO-Lagrangian	MAPPO	HAPPO	IPPO	MACPO
actor lr	9e-5	9e-5	9e-5	9e-5	/
ppo epoch	5	5	5	5	/
kl-threshold	/	/	/	/	0.0065
ppo-clip	0.2	0.2	0.2	0.2	/
Lagrangian coef	0.78	/	/	/	/
Lagrangian lr	1e-3	/	/	/	/
fraction	/	/	/	/	0.5
fraction coef	/	/	/	/	0.27
Table 2: Different hyperparameters used for MAPPO-Lagrangian, MAPPO, HAPPO, IPPO, and
MACPO in the Safe Multi-Agent MuJoCo domain.
task	value	task	value	task	value
Ant(2x4)	0.2	Ant(4x2)	0.2	Ant(2x4d)	0.2
HalfCheetah(2x3)	5	HaIfCheetah(3x2)	5	HalfCheetah(6x1)	5
ManyAgent Ant(2x3)	1	ManyAgent Ant(3x2)	1	ManyAgent Ant(6x1)	1
Table 3: Safety bound used for MACPO in the Safe Multi-Agent MuJoCo domain
22