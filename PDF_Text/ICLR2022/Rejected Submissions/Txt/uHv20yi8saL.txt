Under review as a conference paper at ICLR 2022
Monotonic Improvement Guarantees under
Non-stationarity for Decentralized PPO
Anonymous authors
Paper under double-blind review
Ab stract
We present a new monotonic improvement guarantee for optimizing decentralized
policies in cooperative Multi-Agent Reinforcement Learning (MARL), which
holds even when the transition dynamics are non-stationary. This new analysis
provides a theoretical understanding of the strong performance of two recent
actor-critic methods for MARL, i.e., Independent Proximal Policy Optimization
(IPPO) (Schroder de Witt et al., 2020) and MUIti-Agent PPO(MAPPO)(YU et al.,
2021), which both rely on independent ratios, i.e., computing probability ratios
separately for each agent’s policy. We show that, despite the non-stationarity that
independent ratios caUse, a monotonic improvement gUarantee still arises as a
resUlt of enforcing the trUst region constraint over joint policies. We also show
this trUst region constraint can be effectively enforced in a principled way by
boUnding independent ratios based on the nUmber of agents in training, providing
a theoretical foUndation for proximal ratio clipping. Moreover, we show that the
sUrrogate objectives optimized in IPPO and MAPPO are essentially eqUivalent
when their critics converge to a fixed point. Finally, oUr empirical resUlts sUpport
the hypothesis that the strong performance of IPPO and MAPPO is a direct resUlt
of enforcing sUch a trUst region constraint via clipping in centralized training, and
the good valUes of the hyperparameters for this enforcement are highly sensitive to
the nUmber of agents, as predicted by oUr theoretical analysis.
1	Introduction
In cooperative mUlti-agent reinforcement learning (MARL), a team of agents mUst coordinate their
behavior to maximize a single cUmUlative retUrn (Panait & LUke, 2005). In sUch a setting, partial
observability and/or commUnication constraints necessitate the learning of decentralized policies that
condition only on the local action-observation history of each agent. In a simUlated or laboratory
setting, decentralized policies can often be learned in a centralized fashion, i.e., Centralized Training
with Decentralized ExecUtion (CTDE)(Oliehoek & Amato, 2016), which allows agents to access
each other’s observations and Unobservable extra state information dUring training.
Actor-critic algorithms (Konda & Tsitsiklis, 2000) are a natUral approach to CTDE becaUse critics
can exploit centralized training by conditioning on extra information not available to the decentralized
policies (Lowe et al., 2017; Foerster et al., 2017). UnfortUnately, sUch actor-critic methods have long
been oUtperformed by valUe-based methods sUch as QMIX (Rashid et al., 2018) on MARL benchmark
tasks sUch as Starcraft MUlti-Agent Challenge (SMAC) (Samvelyan et al., 2019). However, two
recent actor-critic algorithms (Schroder de Witt et al., 2020; YU et al., 2021) have Upended this
ranking by oUtperforming previoUsly dominant MARL methods, sUch as MADDPG (Lowe et al.,
2017) and valUe-decomposed Q-learning (SUnehag et al., 2017; Rashid et al., 2018). Both algorithms
are mUlti-agent extensions of Proximal Policy Optimization (PPO) (SchUlman et al., 2017) bUt one
Uses decentralized critics, i.e., independent PPO (IPPO) (Schroder de Witt et al., 2020), and the other
Uses centralized critics, i.e., mUlti-agent PPO (MAPPO) (YU et al., 2021).
One key featUre of PPO-based methods is the Use of ratios (between the policy probabilities before
and after Updating) in the objective. Both IPPO and MAPPO extend this featUre of PPO to the
mUlti-agent setting by compUting ratios separately for each agent’s policy dUring training, which we
call independent ratios. UnfortUnately, Until now there has been no theoretical jUstification for the
Use of sUch independent ratios. In this paper we show that the analysis that Underpins the monotonic
1
Under review as a conference paper at ICLR 2022
policy improvement guarantee for PPO (Schulman et al., 2015) does not carry over to the use of
independent ratios in IPPO and MAPPO. Instead, a direct application of this analysis leads to a joint
policy optimization and suggests the use of joint ratios, i.e., computing ratios between joint policies.
The difference is crucial because, based on the existing trust region analysis for PPO, only a joint
ratios approach enjoys a monotonic policy improvement guarantee. Moreover, as independent ratios
consider only the change in one agent’s policy and ignore the fact that the other agents’ policies also
change, the transition dynamics underlying these independent ratios are non-stationary (Papoudakis
et al., 2019), breaking the assumptions in the monotonic improvement analysis (Schulman et al.,
2015). While some studies attempt to extend the monotonic improvement analysis to MARL (Wen
et al., 2021; Li & He, 2020), they primarily consider optimizing policies with joint ratios, rather than
independent ratios as in our paper, and are thus not applicable to IPPO or MAPPO.
To address this gap, we provide a new monotonic improvement analysis that holds even when the
transition dynamics are non-stationary. We show that, despite this non-stationarity, a monotonic
improvement guarantee still arises as a result of enforcing the trust region constraint over joint
policies, i.e., a centralized trust region constraint. In other words, constraining the update of joint
policies in centralized training addresses the non-stationarity of learning decentralized policies. This
new analysis implies that independent ratios can also enjoy the same performance guarantee as joint
ratios if the centralized trust region constraint is properly enforced by bounding independent ratios.
In this way both IPPO and MAPPO can guarantee monotonic policy improvement. We also provide
a theoretical foundation for proximal ratio clipping by showing that centralized trust region can
be enforced in a principled way by bounding independent ratios based on the number of agents in
training. Furthermore, we show that the surrogate objectives optimized in IPPO and MAPPO are
essentially equivalent when their critics converge to a fixed point.
Finally, we provide empirical results that support the hypothesis that the strong performance of IPPO
and MAPPO is a direct result of enforcing such a trust region constraint via clipping in centralized
training. Particularly, we show that good values of the hyperparameters for the clipping range are
highly sensitive to the number of agents, as these hyperparameters, together with the number of
agents, effectively determine the size of the centralized trust region. Moreover, we show that IPPO
and MAPPO have comparable performance on SMAC maps with varied difficulty and numbers of
agents. This comparable performance also implies that the way of training critics could be less crucial
in practice than enforcing a trust region constraint.
2	Related Work
The use of trust region optimization in MARL traces back to parameter-sharing TRPO (PS-
TRPO) (Gupta et al., 2017), which combines parameter sharing with TRPO for cooperative multi-
agent continuous control but provides no theoretical support. Our analysis showing that a trust region
constraint is pivotal to guarantee performance improvement in MARL applies to PS-TRPO, among
other algorithms. Multi-agent trust region learning (MATRL) (Wen et al., 2021) uses a trust region
for independent learning with a game-theoretical analysis in the policy space. MATRL considers in-
dependent learning and proposes to enforce a trust region constraint by approximating the stable fixed
point via a meta-game. Despite the improvement guarantee for joint policies, solving a meta-game
itself can be challenging because its complexity increases exponentially in the number of agents.
We instead consider centralized learning and enforce the trust region constraint in a centralized and
scalable way. Multi-Agent TRPO (MATRPO) directly extends TRPO to the multi-agent case (Li &
He, 2020) and divides the trust region by the number of agents. However, the analysis assumes a
private reward for each agent, which yields different theoretical results from ours. Non-stationarity
has been discussed in multi-agent mirror descent with trust region decomposition (Li et al., 2021),
which first decomposes the trust region for each decentralized policy and then approximates the
KL divergence through additional training. However, this method needs to learn a fully centralized
action-value function and thus becomes becomes impractical when there are many agents.
3	Background
Dec-MDPs. We consider a fully cooperative multi-agent task in which a team of cooperative agents
choose sequential actions in a stochastic environment. It can be modeled as a decentralized Markov
2
Under review as a conference paper at ICLR 2022
decision process (Dec-MDP), defined by a tuple {N , S, A, P, r, ρ0 , γ}, where N , {1, . . . , N}
denotes the setofN agents and s ∈ S , S1 ×S2 ×...×SN describes the joint state of the environment.
The initial state so 〜ρo is drawn from distribution ρo, and at each time step t, all agents k ∈ N
choose simultaneously one action atk ∈ Ak, yielding a joint action at , at1 × at2 × ... × atN ∈ A ,
A1 × A2 X ... X AN. After executing the joint action at in state st, the next state st+ι 〜P (st, at)
is drawn from transition kernel P and a collaborative reward rt = r(st, at) is returned. In a Dec-
MDP, each agent k ∈ N has a local state stk ∈ Sk, and chooses its actions with a decentralized
policy ak 〜∏k(∙∣sk) based only on its local state. The collaborating team of agents aims to
learn a joint policy, π(at∣st)，Qk=I ∏k(ak | sk), that maximizes their expected discounted return,
η(π) , E(st,at) [Pt∞=0 γtrt], where γ ∈ [0, 1) is a discount factor.
Policy optimization methods. For single-agent RL that is modeled as an infinite-horizon dis-
counted Markov decision process (MDP) {S, A, P, r, ρo, Y}, the performance for a policy π(a∣s) is
defined as: η(π) = E(st,at) Pt∞=0 γtr(st, at) . The action-value function Qπ and value function
Vπ are defined as:
∞
Qn (st, at) = Est+1 〜p(∙∣st,at), [ ^X Yr(St+1, at+l R ,	Vπ (St)= Eat〜π(∙∣st)
at+1 〜π(Tst+1)	l=0
st, at) .
Let the advantage function be Aπ(s, a) = Qπ(s, a) - Vπ(s); the following useful identity expresses
the expected return of another policy ∏ in terms of the advantage over ∏ (Kakade & Langford,
2002): η(π) = η(π) + Ps ρ∏(s) Pa Π(a∣s)A∏(s, a), where ρ∏(s) is the state distribution induced
by ∏. The complex dependency of ρ∏ (s) on ∏ makes the righthand side difficult to optimize directly.
Schulman et al. (2015) proposed to consider the following surrogate objective
rπ(a∣s)	]
Ln(π) = η(π) + XPn(S)Xπ(als)A∏(s,a) = η(π) + E(s,α)〜ρ∏ [ (I)An(s,a)],
VV	lπ(a∣s)	」
where the Pn is replaced with Pn. Define DmVax(π, π)，maxs DTV(∏(∙∣s), ∏(∙∣s)), where DTV is
the total variation (TV) divergence.
Theorem 1. (Theorem 1 in Schulman et al. (2015)) Let α = DmVx(∏, ∏) ∙ Then the following bound
holds
η(π) ≥ Ln (π) 一门 4'γ α2,
(1 - γ)2
where = maxs,a |An (s, a)|.
This theorem forms the foundation of policy optimization methods, including Trust Region Policy
Optimization (TRPO) (Schulman et al., 2015) and Proximal Policy Optimization (PPO) (Schulman
et al., 2017). TRPO suggests a robust way to take large update steps by using a constraint, rather than
a penalty, on the TV divergence, and considers the following practical optimization problem,
TRPO: max E(s,0)〜ρ∏ [；(；：)An(s,a)],	s.t. DmVx(∏(∙∣s),∏(∙∣s)) ≤ α. (1)
This constrained optimization is complicated as it requires using conjugate gradient algorithms with
a quadratic approximation to the constraint. PPO simplifies the above optimization by clipping
probability ratios λn = ；(：|：) to form a lower bound of Ln (∏):
PPO: max E(s,a)〜ρ∏ [min (λnAn(s,a), clip(λn, 1 - e, 1 + E)An(s, a))],	(2)
n
where is a hyperparameter to specify the clipping range.
Independent PPO and Multi-Agent PPO. Both IPPO (Schroder de Witt et al., 2020) and
MAPPO (Yu et al., 2021) optimize decentralized policies with independent ratios. In particular, the
main objective IPPO and MAPPO optimize is
maxE(sk,ak)〜ρ∏k [min (λn% Ak(sk,ak), clip(λn%, 1 - e, 1 + e)Ak(sk,ak))] ∀k ∈ {1, 2,…，Ν},
k	(3)
3
Under review as a conference paper at ICLR 2022
where λ∏k = ；：(：/：( denotes the ratio between the decentralized policy probabilities of agent k
before and after updating. The difference between IPPO and MAPPO lies in how they estimate the
advantage function: IPPO learns a decentralized advantage function Ak(sk, ak) , Pt∞=0[r(stk, atk)] -
V (sk) based on the local information (sk, ak) for each agent, while MAPPO uses a centralized critic
that conditions on centralized state information s: Ak(sk, ak) , Es-k Pt∞=0[r(stk, atk)] - V (s) ,
where -k refers the set of all agents except agent k. Both methods use parameter sharing between
agents. Consequently, as all agents share the same actor and critic networks, one can ignore the agent
specifier k in the objective and use experience from all agents to update the actor and critic networks:
maxXE(sk,ak)〜ρ∏θ [min (λθAφ(sk,ak),clip(λθ, 1 - e, 1 + e)Aφ(sk,ak))],	(4)
θk
where λθ = ；；(：/：(, and θ, φ are shared parameters for policy and advantage networks. The use
of independent ratios together with parameter sharing has shown strong empirical results in various
MARL benchmark tasks (Schroder de Witt et al., 2020; Yu et al., 2021).
4	Trust Region Analysis for MARL
In this section, we first directly apply TRPO’s trust region analysis to cooperative MARL, which
yields joint ratios rather than the independent ratios adopted in IPPO and MAPPO. We then show that
optimization with independent ratios induces non-stationarity in MARL, which breaks the stationarity
assumption in the trust region analysis. Finally, we provide a new analysis that shows how monotonic
policy improvement can still arise from non-stationary transition dynamics with independent ratios.
4.1	Optimization with Joint Ratios
Consider the joint policy π(a∣s) and the centralized advantage function An(s, a) = Qn(s, a)-
Vπ(s). Then, the trust region analysis for single-agent RL carries over directly to MARL with the
surrogate objective as Ln(∏) = η(π) + Ps Pn(S) Pa Π(a∣s)A∏(s, a). One can consider the same
optimization problem for TRPO shown in equation 1,
max E(s,a)〜ρ∏h π(als) An (S, a)i，	s.t. DmVx(π(∙∣s), π(∙∣s)) ≤ α. (5)
∏	lπi a∣s)
The trust region constraint is enforced over joint policies, which we refer as a centralized trust region
constraint. With joint ratios defined as λn = ：(：|：) = Qk=I [；：(：/：(], one can simplify the
above optimization as PPO to have the following objective,
JR-PPO:	max	E(s,a)〜ρ∏ [ min (λn An (s, a), clip(λn, 1 - e, 1 + c)A∏ (s, a))].	(6)
n
We call the resulting algorithm Joint Ratio PPO (JR-PPO) (see Algorithm 1 in the appendix).
Unlike IPPO and MAPPO, JR-PPO consider joint ratios over the joint policies, rather than independent
ones. This difference is crucial, as joint ratios naturally enjoy the monotonic improvement guarantee
carried over from the single-agent trust region analysis:, i.e., Theorem 1. Furthermore, the objective
used in IPPO and MAPPO is not equivalent to the above objective as they are lower bounds of
different objectives. Thus, Theorem 1 does not imply any guarantees for IPPO and MAPPO.
4.2	Optimization with Independent Ratios
Optimization with independent ratios, however, induces non-stationarity in MARL. When optimizing
decentralized policies, the environment is non-stationary from the perspective of a single agent
since the other agents also change their policies during training. To analyze the non-stationarity in
decentralized policy optimization, we first consider the Markov chain for the local state sk induced by
the underlying MDP for agent k. When all agents, policies are updated from ∏ι,…,∏n to ∏ι,..., ∏n,
the state transition distribution of this Markov chain also shifts. We denote such transition shift from
sk to sk for agent k as
∆π,…,πn (Sk ∣sk)，X [t)~ ( (sk ∣sk ak)∏lJak ∣sk) -v	(Sk∖sk αk )π, (αk Isk)]
δλ∏1 ,…,∏n (S ∖s ) — 7 Jp∏1,∙∙∙,∏N (S ∖s , a )πk (a ∖s ) p∏1,…,∏N(S ∖s , a )πk (a ∖s )J ,
ak
4
Under review as a conference paper at ICLR 2022
wherep∏i,...,∏n andp∏ι,…,∏n refer to the transition dynamics before and update ∏ is updated. The
state transition shift consists of two parts: an exogenous part, which is caused by the update of other
agents, policies (i.e., the change of transition dynamics from p∏k to p∏Q, and an endogenous part,
which is contributed by the update of the agent’s own policy (i.e., the change of agent k’s policy from
∏k to ∏k). See the appendix A.2 for detailed analysis. The exogenous shift breaks the assumption in
the monotonic improvement guarantee (Schulman et al., 2015) that the MDP is stationary, i.e., the
state transition shift arises only endogenously from the agent’s policy changes. As a result, Theorem 1
no longer holds if one optimizes with independent ratios as in IPPO and MAPPO.
4.3	Monotonic Policy Improvement for Independent Ratios
We now provide a new analysis for optimization with independent ratios. As the above analysis
suggests that the exogenous transition shift breaks the trust region analysis in TRPO, we instead
consider how to handle this exogenous shift in training. Specifically, since the exogenous shift is
caused by the changes of other agents’ policies, it can be controlled by constraining the update of
other agents’ policies in centralized training.
Proposition 1. InaDec-MDP the transition shift ∆∏1,二,∏N (Sk ∣sk) decomposes as follows:
∆π1,...,πN ( kk | Gk) — ∆πi,π2,…,πN ( k∣ | Gk) + ∆πi,π2,π3,…,πN ( Xk | Gk) + + 入五1 ,…EN-I ,及N ( Zk | Gk)
δ∏i,...,∏n (S |S ) = δ∏i,∏2,...,∏n (S |s ) + δ∏i,∏2,∏3,...,∏n (S |s ) + …+ δ∏i,...,∏n-i,∏n (S |s ).
The proof is given in the appendix A.3.1. This proposition implies that the state transition shift at
local observation Sk is caused by the shifts arising from all decentralized policies. This decompo-
sition inspires the derivation of a new monotonic improvement guarantee for decentralized policy
optimization by enforcing the trust region over joint policies.
Define the surrogate objective for decentralized policy πk as
L∏k),∏2,...,∏N go)= X P∏1,∏2,...,∏N (Sk) X Π H 垓 )A∏ko(sk , ak ),
sk	ak
and the expected return of decentralized policy πk as
η∏ι,…,∏N (πk) = E(sk,ak)〜ρ∏ι,…，∏N (sk,ak) [rk (Sk , ak )] ∙
In practice, rk(Sk, ak) is usually unknown. As the state transition shift decomposes into the sum of
transition shifts caused by each decentralized policy, we can bound this state transition shift with a
centralized trust region constraint as in equation 5.
Theorem 2.	Let α = DTmVax(π, πS ). Then the following bound holds :
N	4Cγ
η∏ι,…,∏N (πk) ≥ η∏ι,…,∏n (πk )+〉: LΠ1,…,∏n (πk0) - (J -^)2 α2 ∀k,
k0=1	(1 - γ)
where C 二 maXk∈N max§k,ak ∣A∏k (sk, ak)∣.
The proof is given in the appendix A.3.2. This theorem implies that, for sufficiently small α, the
performance increase of a decentralized policy πk is lower bounded by the sum of surrogate objectives
for each decentralized policy with respect to the samples generated by πk. In other words, if the
trust region is enforced, the sum of surrogate objectives yields an approximate lower bound for
η∏1,∏2,…,∏n (∏k), which holds for any decentralized policy ∏k.
Theorem 2 differs from Theorem 1 in three respects. First, the lower bound for one decentral-
ized policy effectively relies on surrogate objectives for all agents, since the update of one agent’s
policy affects all other agents’ transition probability. Therefore, to improve the performance for
policy πk, we can simultaneously maximize L(πk1),...,πN (πS1) + L(πk1),...,πN(πS2) + ... + L(πk1),...,πN (πSN).
Second, unlike the surrogate objective in Theorem 1, the new surrogate objective implicitly con-
tains an independent ratio λ∏k，；：(：/：( as it can be rewritten as follows: L1,∏2,…,∏n (∏k')=
E(sk,ak)[∏kθ(aJSk) Ank (Sk, ak)]. Third, the additional term (I-Y)2α2 requires computing the total
variation between joint policies π(a∣s) and ∏(a∣s), rather than the policies that are directly opti-
mized. We show in the next section that, in centralized training, this requirement is easily satisfied as
the magnitude of the constraint on the update is proportional to the number of agents.
5
Under review as a conference paper at ICLR 2022
5 Realizing trust regions via bounding independent ratio s
Theorem 2 indicates that the centralized trust region is crucial to guarantee monotonic improvement.
In this section, we show that bounding independent ratios is an effective way to enforce such a
centralized trust region constraint, and this enforcement requires taking into account the number of
agents. To achieve this, we first present two lemmas about DTV divergence.
Proposition 2. In a Dec-MDP DmVX(π, ∏) ≤ PN=I DmVX(∏k,∏)∙
This proposition is a direct result of the fact that the joint policy in a Dec-MDP factors as a product
of decentralized polices, i.e., π = QkN=1 πk .
Proposition 3. DmVX(∏k,∏k) =maxs∈s P亓k(ak∣sk)≥πk(ak∣sk) [∏k(ak∣sk) - ∏k(ak∣sk)]∙
This useful identity follows from a property of DTV: Dτv(μ(x), ν(x)) = ∑μ(χ)>ν(x)[μ(x) - ν(x)]
where μ and V are two distributions. This proposition indicates that a decentralized trust region is
also defined by the sum of probability differences over a special subset. We use this to upper bound
the trust region in the following analysis.
Assumption 1. Assume the advantage function Aπk (sk , ak ) converges to a fixed point for ∀k.
Theorem 3.	Ifindependent ratios λk，；：(：/：( are within the range [1 — ek, 1 + Ek] for ∀k ∈ N,
then thefollowing bound holds: DmVX(∏k(∙∣s), ∏k(∙∣s)) < Ek; DmVX(∏(∙∣s), ∏(∙∣s)) < PN=I Ek∙
This theorem comes from that fact that optimizing ∏(s, a) with respect to a converged A(s, a) leads
to π(a∣s) > π(a∣s) if A(s, a) > 0, ∀s, a in actor-critic algorithms (Konda & Tsitsiklis, 2000). Thus,
based on Proposition 3, we have the following
DmVX (∏k ,∏k) = max
sk
E	[∏k(ak ∣sk) - ∏k(ak ∣sk)] ≤ max
ak∈Ak
Ak(sk,ak)>0
sk
E	[Ek∏k(ak∣sk)] <Ek∙
a∈Ak
Ak(sk,ak)>0
The equation is from Proposition 3 by considering A(s, a) > 0 such that π(a∣s) > π(a∣s). The
first inequality is a result of bounded ratios and the second is from Pa∈^A(S a)>o} [∏(a∣s)] < 1
(considering the lower bound yields the same analysis.) Furthermore, the trust region constraint over
joint policies is a direct result of Proposition 2. The detailed proof is given in the appendix A.3.3. As
DτV is a bounded divergence between [0, 1], the ratio guarantee makes sense when Ek ≤ 1∙0.
Theorem 3 implies that bounding independent ratios ；：(：/：( with [1 - Ek, 1 + Ek] amounts to
enforcing a trust region constraint with size E over decentralized policies. Thus, to enforce the
centralized trust region constraint over joint policies, i.e., DmVX(∏, ∏) ≤ α as in Theorem 2, one
can consider bounding independent ratios according to the number of agents, e.g., λk，；：(：/：( ∈
[1 - Na, 1 + Na]. In the next section, we present practical implementations of these ratio constraints.
6 Instantiating Ratio Constraints
In this section, we show that IPPO and MAPPO effectively satisfy the conditions of Theorem 2 and
Theorem 3. Specifically, the independent clipping and parameter sharing used by IPPO and MAPPO
are useful ways to approximate the ratio constraints in Theorem 3 and to optimize the surrogate
objective in Theorem 2. Furthermore, we show that the surrogate objectives optimized in IPPO and
MAPPO are essentially equivalent when their critics converge to a fixed point.
6.1 Optimizing surrogate objectives
The objective is to update all agents’ policies simultaneously with the experience from all agents,
which can be further simplified with parameter sharing (Gupta et al., 2017):
max XXP∏i,∏2,...,∏n(sk) X∏θ(uk∣sk)Aφ(sk,uk),	⑺
k sk	uk
s.t. DmVX(∏(∙∣s),∏(∙∣s)) ≤ α,	(8)
6
Under review as a conference paper at ICLR 2022
where θ and φ are the shared parameters for decentralized policies and critics. Furthermore, to
effectively optimize the surrogate objective, we can clip the probability ratios of each decentralized
policies to form a lower bound of the objective in equation 7, similar to PPO (Schulman et al., 2017).
Namely, with independent ratios λk = ；；(：/：( ∀k ∈ N, We can optimize the following objective:
max	EE(Sk,uk)〜ρ(sk,uk)[min (λkAk, clip(λk, 1 - e, 1 + e)Ak)],
k
which is exactly the objective used by IPPO and MAPPO.
6.2	Enforcing the trust region constraint
Proposition 2 implies that, in centralized training, one way to enforce trust region constraint is to
delegate the centralized trust region constraint to each agent, such that the update of each decentralized
policy ∏k(ak∣sk) is bounded. Therefore, to enforce the centralized trust region constraint, one can
impose a sufficient condition as follows,
DmVx(∏θ(∙∣sk),∏θ(∙∣sk)) ≤ α ∀k ∈N,	(9)
which suggests that enforcement of the centralized trust region constraint translates to a decentralized
trust region constraint if the trust region is specified properly according to the number of agents.
Furthermore, based on Theorem 3, bounding independent ratios is an effective way to enforce the
trust region constraint. One can thus impose a sufficient condition to constrain independent ratios
λk such that λk ∈ [1 - Na, 1 + Na], where N is the number of agents in training. Clipping is one of
many ways to achieve this sufficient condition but itself is a heuristic approximation so often fails to
bound ratios exactly within the ranges. In practice, one would need to tune the the clipping range
and the number of epochs so the ratios can be properly bounded. We show in the experiment section
that good values of the hyperparameters for the clipping range are highly sensitive to the number of
agents, as these hyperparameters, together with the number of agents, effectively determine the size
of the centralized trust region.
6.3	Learning advantage functions
We now look at the training of the advantage function, where IPPO and MAPPO differ. IPPO trains a
decentralized advantage function, while MAPPO trains a centralized one that incorporates centralized
state information. Assume a stationary distribution of (sk, ak) exists. From Lyu et al. (2021), we
have the following:
Proposition 4. (Lemma 1 & 2 in Lyu et al. (2021)) Training of centralized critic and k-th decen-
tralized critic admits unique fixed points Qπ(sk, s-k, ak, a-k) and Es-k,a-k [Qπ (sk, s-k, ak, a-k)]
respectively, where Qπ is the true expected return under the joint policy π.
Accordingly, based on the definition, the centralized value function is V (s) =
V (sk, s-k) = Eak,a-k [Qπ(sk, s-k, ak, a-k)] and the decentralized one is V (sk) =
Es-k,ak,a-k [Qπ(sk, s-k, uk, a-k)] = Es-k [V (sk, s-k)] = Es-k [V (s)]. Thus, we have
AIPPO (sk, ak) = AMAPPO (sk, ak) (and so IPPO and MAPPO objectives are equivalent) given that the
underlying critics converge to a fixed point.
7 Experiments
We consider the StarCraft Multi-Agent Challenge (SMAC) (Samvelyan et al., 2019) for our empirical
analysis as it provides a wide range of multi-agent tasks with varied difficulty and numbers of agents.
We first show that clipping is an effective way to constraint ratios when the number of optimization
epochs and the learning rate are properly specified. Furthermore, we show that clipping also requires
taking into account the number of agents such that the centralized trust region can be properly
enforced. We then empirically demonstrate that bounding independent ratios in effect enforces the
trust region over joint policies. Finally, we present results showing that IPPO and MAPPO perform
equivalently on SMAC maps with varied difficulty and numbers of agents.
7
Under review as a conference paper at ICLR 2022
_ ,
8q!BJ,δe?小
—I- Mod⅛p⅛a
C⅛Φ⅛sO-5
C⅛Φ⅛sO-*
-i- OPP⅛g 0.3
—C⅛Φ⅛sO2
-I- C⅛Φ⅛sO.1
M
10	20	30	40
Number of optimization epochs
(a)
____ ^^1
aββ⅞uaojad
M M
aɔa,fe)
(b)
____ ^^1
aββ⅞uaojad £mnuno
M M
tW*<Π,β
(c)
——1»
—»
——»
——»
—S
——»
——2
Figure 1: Trust region with respect to the clipping range, the number of agents and the number
of optimization epochs: (a) Ratio ranges for 5 agents with the number of optimization epochs; (b)
Cumulative percentage of decentralized trust region as the clipping value varies; (c) Cumulative
percentage of centralized trust region as the number of agents varies (clipping at 0.1); and (d)
Cumulative percentage of centralized trust region with optimization epochs (clipping at 0.1).
Clipping and ratio ranges. Theorem 3 indicates that bounding the independent ratios amounts to
enforcing a trust region constraint over joint policies. We empirically show that independent ratio
clipping approximately bounds the independent ratios in the training if some hyperparameters are
properly set. We train decentralized policies on one map, 2s3z, and clip the independent ratios when
optimizing the surrogate objective. Figure 1a shows how the the max and min of the ratios changes
according to the number of optimization epochs with different clipping values. Independent ratio
clipping can effectively constrain the range of ratios only when the number of optimization epochs and
the clipping range are properly specified. In particular, the range of independent ratios grows as the
number of optimization epochs increases. This growth is slower when the clipping range is smaller,
e.g., = 0.1. Furthermore, the clipping range may not strictly bound ratios between [1 - , 1 + ]:
when the clipping range is 0.1, the independent ratios can exceed 1.2; and the independent ratios can
even grow up to 1.6 when the clipping range is 0.3.
Ratio clipping and trust region constraint. Next, we show that the trust region defined by the
total variation is empirically bounded by independent ratio clipping, and this bound is also propor-
tional to the number of agents. Specifically, we compute the maximum total variation divergence
DTmVax over empirical samples collected by the behavior policy during the first round of actor update,
which contains 100 optimization epochs, and report the distribution of DTmVax . Figure 1b shows the
distribution of DTmVax over decentralized policies when clipping range varies. For clipping at 0.1,
all DTmVax values are smaller than 0.2, meaning that the trust region is effectively enforced to be
small. As the clipping range increases, more DTmVax values exceed 0.3. For the case without clipping,
DTmVax almost uniformly distributes among [0.0, 0.8], implying trust region is no longer enforced.
Figure 1c presents the distribution of DTmVax over joint polices for clipping at 0.1, on maps with
different number of agents. See appendix Table 1 for more details. The DmVx(π, ∏) is estimated by
summing up the empirical total variation distances DmVx(∏k,∏k) over all agents. The DmVx(π, ∏)
grows almost proportionally with the number of agents, indicating that enforcing the centralized trust
region with independent ratios clipping also requires considering the number of agents. Figure 1d
presents the distribution of DTmVax over joint polices with different numbers of epochs for clipping at
0.1. Compared to the number of agents, the number of epochs has less impact on the trust region.
Independent ratios clipping on SMAC Figure 2 shows the empirical returns and trust region
estimates with different ratio clipping values across different maps in SMAC. 1 Notably, when the
clipping value is small, e.g., = 0.1, the joint total variation distance, i.e., the centralized trust region,
can be effectively bounded, as in the second row in Figure 2. The empirical returns corresponding to
= 0.1 are thus improved monotonically, outperforming all other returns consistently in four maps.
Moreover, as the number of agents increases, the trust region enforced by clipping value = 0.1 in
the initial training phase also grows from less than 0.3 to more than 0.5. In contrast, for clipping at
1Trained via decentralized advantage, i.e., IPPO. Results with centralized advantage are similar, as presented
in the appendix A.4. Unlike Yu et al. (2021), the value function is not clipped in our experiments.
8
Under review as a conference paper at ICLR 2022
ε-B-s<t5
IwWurmlU.L
27nl_w_30ni
27m-v¾-30πι
-ft'P) I
一M(S)
一M(S)
一W(S)
——m>d⅛⅜l(⅞
2π⅛wb
t⅛∏⅛w
Figure 2: Empirical returns and trust region estimates for independent ratios clipping.
ε-B-s<t5
AlHUFWm
1Om W 11m
<sι
P*>τ⅛e
⅝ssv∙p
ιβ⅛fe
c^a-SΛ≈e^?
OΛ
α κι> Xano IInJ KKn ɪwo <τso aɪm	o ≡o m »sj ιαn KKn ɪwo <τso Bm	o ≡o ∞ ?sj <om t:n ɪwo Im ≈∞	o ssj m jsj ιom KKn ɪwa m> aɪaa
Naτtι∙ro∏hwβt∙cn K)	Naτtι∙rtfftτwβt∙cnK)	Naτ*∙rtfftτwβt∙cnK)	J*∙τ*∙rtfftτwβt∙cnK)





Figure 3: Contrasting IPPO and MAPPO across different maps.
0.5 and 1.0, the learning quickly plateaus at local optima, especially on maps with many agents, e.g.,
10m_vs_11m and 27m_vs_30m, which shows that the policy performance η(πk) is closely related
to the enforcement of trust region. We also apply the same clipping values to joint clipping and
independent clipping, see Appendix A.6 for more analysis.
IPPO and MAPPO We show that the empirical performance of IPPO and MAPPO are very similar
despite the fact that the advantage functions are learned differently. We evaluate IPPO and MAPPO
on maps of varied difficulty and numbers of agents. We heuristically set the clipping range based
on the number of agents. Specifically, we set the clipping range for 3s5z, 1c3s5z, 10m_vs_11m,
and bane_vs_bane, as 0.1, 0.1, 0.1, and 0.05, respectively. The results are presented in Figure 3. On
the four maps considered, IPPO and MAPPO perform similarly. This phenomenon can be observed
in Yu et al. (2021), which provides more empirical comparisons between IPPO and MAPPO. Such
comparable performance also implies that, for actor-critic methods in MARL, the way of training
critics could be less crucial than enforcing the trust region constraint.
8 Conclusion
In this paper, we present a new monotonic improvement guarantee for optimizing decentralized
policies in cooperative MARL. We show that, despite the non-stationarity in IPPO and MAPPO, a
monotonic improvement guarantee still arises from enforcing the trust region constraint over joint
policies. This guarantee provides a theoretical understanding of the strong performance of IPPO and
MAPPO. Furthermore, we provide a theoretical foundation for proximal ratio clipping by showing
that a trust region constraint can be effectively enforced in a principled way by bounding independent
ratios based on the number of agents in training. Finally, our empirical results support the hypothesis
9
Under review as a conference paper at ICLR 2022
that the strong performance of IPPO and MAPPO is a direct result of enforcing such a trust region
constraint via clipping in centralized training.
References
Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual Multi-Agent Policy Gradients. arXiv:1705.08926 [cs], December 2017. URL
http://arxiv.org/abs/1705.08926. arXiv: 1705.08926.
Jayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer. Cooperative multi-agent control using
deep reinforcement learning. In International Conference on Autonomous Agents and Multiagent
Systems, pp. 66-83. Springer, 2017.
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In In
Proc. 19th International Conference on Machine Learning. Citeseer, 2002.
Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information
processing systems, pp. 1008-1014, 2000.
Hepeng Li and Haibo He. Multi-agent trust region policy optimization. arXiv preprint
arXiv:2010.07916, 2020.
Wenhao Li, Xiangfeng Wang, Bo Jin, Junjie Sheng, and Hongyuan Zha. Dealing with non-
stationarity in multi-agent reinforcement learning via trust region decomposition. arXiv preprint
arXiv:2102.10616, 2021.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-
critic for mixed cooperative-competitive environments. arXiv preprint arXiv:1706.02275, 2017.
Xueguang Lyu, Yuchen Xiao, Brett Daley, and Christopher Amato. Contrasting centralized and
decentralized critics in multi-agent reinforcement learning. arXiv preprint arXiv:2102.04402,
2021.
Frans A. Oliehoek and Christopher Amato. A Concise Introduction to Decentralized POMDPs.
SpringerBriefs in Intelligent Systems. Springer International Publishing, 2016. ISBN 978-3-
319-28927-4. doi: 10.1007/978-3-319-28929-8. URL https://www.springer.com/gp/
book/9783319289274.
Liviu Panait and Sean Luke. Cooperative Multi-Agent Learning: The State of the Art. Autonomous
Agents and Multi-Agent Systems, 11:387-434, November 2005. doi: 10.1007/s10458-005-2631-2.
Georgios Papoudakis, Filippos Christianos, Arrasy Rahman, and Stefano V Albrecht. Dealing with
non-stationarity in multi-agent deep reinforcement learning. arXiv preprint arXiv:1906.04737,
2019.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shi-
mon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement
learning. In International Conference on Machine Learning, pp. 4295-4304. PMLR, 2018.
Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas Nardelli,
Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The
starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.
Christian Schroder de Witt, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip H. S.
Torr, Mingfei Sun, and Shimon Whiteson. Is independent learning all you need in the starcraft
multi-agent challenge? CoRR, abs/2011.09533, 2020. URL https://arxiv.org/abs/
2011.09533.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889-1897. PMLR,
2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
10
Under review as a conference paper at ICLR 2022
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi,
Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl Tuyls, and Thore Graepel.
Value-Decomposition Networks For Cooperative Multi-Agent Learning. arXiv:1706.05296 [cs],
June 2017. URL http://arxiv.org/abs/1706.05296. arXiv: 1706.05296.
Ying Wen, Hui Chen, Yaodong Yang, Zheng Tian, Minne Li, Xu Chen, and Jun Wang. A game-
theoretic approach to multi-agent trust region optimization. arXiv preprint arXiv:2106.06828,
2021.
Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising
effectiveness of mappo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021.
11
Under review as a conference paper at ICLR 2022
A Appendix
A.1 Joint Ratio PPO
Algorithm 1 Joint Ratio PPO (JR-PPO)
for iteration i = 0, 1, 2, . . . do
Roll out decentralized policies [π1, π2, ..., πN] in environment;
Compute centralized advantage estimates A(s, a);
Computejoint ratios λ∏ = ：(：|：)
∙k(ak |sk)」;
Optimize the surrogate objective max∏ E[min (λ∏A(s, a), clip(λ∏, 1 - e, 1 + e)A(s, a))]
end for
A.2 S tationarity assumption in TRPO
The single-agent TRPO relies on the following analysis:
L∏(Π) - L∏(π)= Xρ(s) X (∏(a∣s) - π(a∣s))A∏(s, a)
sa
=XP(S) X (π⑷s) - π⑷S))Ir(S) + Xp(s0ls,a)γv(s0) - v(s)
s	a	s0
=X P(S) XX (π⑷S) -∏(a∣S))p(S0∣S, a)γv(S0)
s	s0	a
=EP(S)E £ (π⑷S)P(SlS,a) 一 ∏(a∣S)p(S0∣S, a))γv(S0)
s	s0	a
=X P(S) XSn(SlS)- Pn(SlS))Yv(S0).
s	s0
This analysis is based on the assumption that P(S0|S, a) remains the same before and after π is
updated, such that transition shift Pn(SlS) - Pn(s0∣s) is only caused by the agent,s policy update,
i.e., endogenously. Such analysis no longer holds when the transition dynamics P(S0|S, a) are
non-stationary: Pn(s0∣s, a) = Pn(s0∣s, a).
A.3 Proofs
A.3.1 Proof of Proposition 1
Proof. Assume agent k’s policy πk is executed independently of other agents policies π-k, we have
δ∏1,...,∏N (Sk ∣sk)
n ,...,n
X P(SSk,SS-k|Sk,S
S-k,s-k
ak,a-k
-k, ak, a-k)[∏k(ak∣sk)∏-k(a-k∣s-k) - πk(ak∣sk)π-k(a-k∣s-k)]
E P(5k,S-k ∣sk ,s-k,ak ,a^
|
Ξ-k c-k
s ,s
ak,a-k
+ ∏k(ak ∣zk )π-k (a-k ∣s-k) - πk (ak∣sk )π-k (a-k ∣s-k)].
}
^^^{^^^™
exogenous
|
^^■^{^^^^~
endogenous
}
The above decomposition can be repeated such that the exogenous part can be translated into
endogenous parts that are specific to each agent. Specifically, repeat the decomposition for the
12
Under review as a conference paper at ICLR 2022
exogenous part by considering agent k0 (k0 6= k):
∏k(ak∣sk)∏-k(a-k∣s-k) - ∏k(ak∣sk)π-k(a-k∣s-k)
∏ (ak ∣sk )[∏k0 (ak0|sk0)n-{k，k0}(a-{k，k0 }∣s-{k,k0}) - ∏k0 (ak0 ∣sk0 )∏-{k,k0 }(a-{k,k0} ∣s-{k,k0})]
∏ k (ak ∣sk)[ ∏ k0 (ak0 ∣sk0 )∏-{k,k0 }(a-{k,k0} ∣s-{k,k0}) - ∏k0 (ak0 ∣sk0 )∏-{k,k0} (a-{k,k0} ∣s-{k,k0})
、------------------------------------------------------{------------------------------------------}
πk -exogenous
+ ∏k0(ak0 ∣sk0)∏-{k,k0} (a-{k，k0}|s-{k，k0}) - ∏k0(ak0 ∣sk0)n-{k，k0}(a-{k，k0}|s-{k，k0}).
'-----------------------------------------------------------------------------------}
πk -endogenous
C	ICT	1	A ∏1 .....ΠN ∕~k k、 C ”
So on and so forth, one can decompose ∆ 1， ，N(sk∣sk) as follows:
'	∏	πj- ,...,nɪ' × 1 /
∆∏1,..* (Sk ∣sk) = ∆π1,∏2,…,∏N (Sk ∣sk)+∆∏1,∏2,∏3,…,∏N (Sk ∣sk)+ …+∆∏1,…，黑-1，裳(Sk ∣sk),
，l,...,，l	，l,，l,...,，l	，l,，l,，l,...,，l	，l,...,，l ,，i
which implies that the state transition shift at local observation sk is caused by the shifts arising from
all decentralized policies.	口
A.3.2 Proof of Theorem 2
Proof. This proof is based on the perturbation theory. Let Gsi = (1 + γPπs1i ,π2 ,...,πN +
(YPni,∏2,...,∏N )2 + …= (I-YPni,∏2,...,∏N )-1 and GG Si = (1+ YPΠi,∏2,...,∏N +(7^ ,∏2 ,...,∏N ) 2 +
…=(1-YPni ,∏2,.. ∏n)-1 denote the distribution of state Si under ∏1,∏2,…，∏n and ∏1,∏2,…,∏n,
We will use the convention that ρ (a density on state space) is a vector and r (a reward function
on state space) is a dual vector (i.e., linear functional on vectors), thus rρ is a scalar meaning the
expected reward under density ρ. Note that η(π) = rGρ0, and η(πS) = rGSρ0. Denote the state shift
of Si as ∆(si) = Psi ∏2 njv - P∏i,∏2, ∏N. Using the perturbation theory, We have the following
[Gsi ]-1 - [<Gsi ]-1 = YPΠi ,∏2,...,∏N - YPΠi ,∏2,...,∏N =Yδ(sJ
Left multiply by Gsi and right multiply by Gsi :
GSsi = Gsi +YGsi∆(Si)GSsi.
Substituting the right-hand side into Gsi gives
GSsi - Gsi = YGsi∆(Si)Gsi +Y2Gsi∆(Si)Gsi∆(Si)GSsi).
Consider decentralized policy πi :
η∏1,∏2 ,…,∏N (πi)-
ηn1 ,n2 ,...,nN
(πi)
=rGS siρ0 - rGsi ρ0 = r(GS si - Gsi)ρ0
=YrGsi∆(Si)Gsiρ0 + Y2rGsi∆(Si)Gsi∆(Si)GSsiρ0.
Let us first consider the leading term YrGsi ∆(Si)Gsi ρ0,
YrGsi∆(Si)Gsiρ0
X c(s∙) X(n-(s0 Is-) 一 n (s0 Is-)) Zv(S ) — X C(S ∙) X ∆nι,…,nN (ς0 ∣ς .) Zv(S )
•乙 P(Si) ʌ,(pn (SiISi) Pn (SiISi))Yv(Si)=乙 PH) ʌ, δ∏i,...,∏n (SiISi)Yv(Si)
si	s0i	si	s0i
X c(s∙) X ∣^∆πi,π2,∙∙∙,πN (s0Is∙) + ∆πi,π2,n3,…,πN ( ς0 Iς .) +	+ 人立 1 ,…EN-IEN ( J %.)] v(ς0 )
一 ? JP(Si) / j [δ∏i,∏2,…,∏n (SiISi)十 δ∏i,∏2,∏3,…，∏n (SiISi ) 十…十 δ∏i ,∙∙∙,∏N-1 ,∏n (SiISi)I Yv(Si).
si	s0i
13
Under review as a conference paper at ICLR 2022
For one of these summation terms, we have the following 2:
XP(Si) X h∆∏1,…,∏j-1 ,∏j,...,∏N(s0∣si)iγv(s0)	(10)
ρ i	π1,...,πj-1,πj,...,πN i i γ i
si	s0i
〉：P(Si)〉：〉： SnI,…,Πj-ι,∏j,…,∏n (si |si, ai )πj (ai 1 Si) - p∏ι,…,Πj-ι,∏j,…,∏n (Si |si, ai)πj (ai 1 Si)) Yv(Si)
s0i	ai
(11)
EP(Si)E (πj (ai|Si) - πj (ai|Si )) EpnI,…,∏j-i,∏j,…,∏N (SiISi,ai YYv(Si	(12)
si	ai	s0i
EP(Si) E (πj(ai|Si) - πj(ai|Si)) Ir(Si) + EpnI,…,∏j-1,∏j ,…,∏N
si	ai	s0i
(SiISi,ai)γv(Si) - v∏ι,...,∏j-ι,(Si)]
nj ,...,nN
(13)
=EP(Si) E (∏j(ai∣Si) - ∏j(ai∣Si))Anj (s* a/	(14)
si	ai
= LniI),∏2,..,∏N (Π ) - LniI),∏2,..,∏N (∏j ).	(15)
The derivation from line 13 to 14 is based on the following definition:
Anj (si,ai) , r(si) +〉：pni,…,nj-ι,nj,…，∏n(S0iISi, ai)γv(S0i) -
s0
i
v∏i,…,nj∙-ι,(Si),
nj ,...,nN
Where vnι,..,nj-ι,(Si) , r(Si) + γEsipnι
,...,nj — 1 ,nj ,...,nN ('0iISi, ai)	ai πj(aiISi)v(S0i). Which
nj ,...,nN
will result in LnLn2,…,∏n(∏j) = 0 for ∀j. In fact, for line 13, r(si) and v∏i,…,njy,(Si) can be
nj,...,nN
interpreted as functions over Si, which will be zero if integrated with Paa (∏j(a∕si) - π7- (a∕si)).
Note that the advantage function also conditions on the πj .
Note that the advantage of πj with respect to Si , ai in multi-agent RL is defined differently from the
advantage function in the single agent case.
We can thus repeat the above decomposition iteratively and have the following:
YrGsi∆(Si)GsiPo = LniI),∏2,...,∏n(∏ι) - LniI)g…八(∏ι)
+ Lni),n2,...,nN (π2) - LnI),n2,…,∏n (π2)
+ ... + LnI),n2,…,∏n (πN) - LnI),n2,…,∏n (πN).
Note that L(ni1),n2,...,nN (πj) = 0 for∀j. Thus,
YrGsi ∆(Si)Gsi Po = LniI),n2,...,nN (∏ι) + LniI)m,…,∏n (∏2) + …+ LniI)g…,∏n (∏n ).
TL T	F	1	.1	1	.	9	- ʌ /	∖	- ʌ /	\ 片。∙	τ-τ∙	♦∙1,∙1	1
Next we bound the second term Y2rGsi ∆(Si)Gsi ∆(Si)Gsi Po. First we consider the product
YrGsi∆(Si) = Yv∆(Si). Consider the component of this dual vector:
I(YrGsi δ(Si)SI = |X(pni (SiISi)- Pni(SiISi))Yvi(Si)I
s0
i
NN
=I	p(S01,...,SNIsi, ..., SN,aι, ...,aN)vi(si)[ ɪɪ ∏j@民)-ɪɪ ∏j@同)]I
s01 ,...,s0N s1 ,...,si-1 ,si+1 ,sN a1 ,...,aN	j=1	j=1
NN
=I	Ai(si,ai)[" ∏j(a∕si) - ɪɪ ∏j(a/si)]1.
s1 ,...,si-1,si+1 ,sN a1,...,aN	j=1	j=1
Thus, we have
NN
k(YrGsi∆(Si)sk ≤XX
IAi(Si,ai)"Yπj(adSi)- Yπj(aiISi)I ≤ 2α.
s1,...,sN a1 ,...,aN	j=1	j=1
2 Note that Anj (si, ai) inthe analysis is defined with the transition dynamics P 亓1,…，亓 j-ι,∏j,…，∏n (si∣si,ai).
14
Under review as a conference paper at ICLR 2022
We bound the other portion Gsi∆(si)Gsiρo using the lι operator norm:
~
kGsi∆(si)GSsPoki ≤
k∆(si)kι
(i-γ)2
The k∆(si)k1 can be bound as follows:
Hδ(Si)ki = £|pni(SiISi)- Pni(SiISi)I
s0i,si
NN
p(S01, ...,S0N
s1 ,...,sN s1 ,...,sN a1 ,...,aN
∣S1,..., SN, ai, ∙∙∙, aN)lɪɪ ∏j(ai∣Si) - ɪɪ ∏j(ai∣Si)∣
j=1	j=1
NN
X X Yπj(ai|Si)- Yπj(ai|Si)i≤ 2α.
s1,...,sN a1,...,aN j=1	j=1
So we have that
C.	_	~	_	..... ~ _
Y2IrGsi∆(Si)Gsi∆(Si)GSiρo∣ ≤ Y∣∣rGsi∆(Si)k∞∣∣Gsi△3)GSiPoki ≤
4Y
0-77
α2.
A.3.3 Proof of Theorem 3
Proof. Given the assumption that the advantage function A(S, a) converges to a fixed point. we have
the fact that optimizing Π(s, a) with respect to A(s, a) leadsto π(a∣S) > π(a∣S) if A(s, a) > 0, ∀s, a.
Consider independent ratios λk，；：(：/：( which are within the range [1 - Ek, 1 + Ek] for ∀k ∈ N,
Thus, based on Proposition 3, we have the following
DmVx (∏k ,∏k) = max
sk
≤ max
sk
≤ max
sk
E	[∏k(ak∣Sk) - ∏k(ak∣Sk)]
ak∈Ak
∏(a∣s)>π(a∣s)
X [(1 + Ek)πk(akISk) - πk(akISk)]
ak∈Ak
Ak(sk,ak)>0
X	[Ekπk(akISk)]<Ek.
a∈ Ak
Ak(sk,ak)>0
□
The first inequality is a result of bounded ratios and the second is from Ea∈{a∙A(s a)>o} [∏(a∣S)] < 1.
Furthermore, the trust region constraint over joint policies is a direct result of Proposition 2. One can
also consider A(s, a) < 0, which leads to π(a∣S) < π(a∣S) in optimization. Given that independent
ratios are also lower bounded by 1 - e, the same conclusion can be reached for DmVx.	□
A.4 Experiment details and more results
The number of agents in each is given in Table 1.
Test battle win mean of IPPO on maps with varied difficulty and numbers of agents is presented in
Figure 4.
Empirical test battle win mean, test returns and trust region estimates of MAPPO on maps with varied
difficult and numbers of agents are presented in Figure 5.
A.5 Ablations on small clipping values
We also present the ablation results for small clipping values, i.e., < 0.1, in Figure 6. It is true that a
small clipping value results in a small trust region, and thus small clipping values, e.g., 0.08, 0.05
and 0.03, would be preferred for maps with a large number of agents, e.g., maps 10m_vs_11m (10
15
Under review as a conference paper at ICLR 2022
Table 1: Number of agents on maps.
SMAC Map	Number of agents
2s_vs_1sc	2
3s_vs_5z	3
2s3z	5
6h_vs_8z	6
1c3s5z	9
10m_vs 11m	10
-ft'(S) I
一M(S)
一M(S)
一W(S)
No-rg 烟
-H(S)
-M(S)
一αs(5)
—<Λ(S)
Noc⅛e⅛(5)
SJCS ∙l-Bμ
OSs)SW
SJCS ∙l-Bμ
Figure 4:	Test battle win mean of IPPO on maps with varied difficulty and numbers of agents
agents) and 27m_vs_30m (27 agents). However, when the clip value is too small, e.g., = 0.01 in
maps with 5 and 8 agents, the resultant trust region is also small and the update step in each iteration
can thus be too small to improve the policy. Thus, one would need to trade off between the trust
region constraint, to ensure monotonic improvement, and the policy update step, to ensure a sufficient
parameter update at each iteration.
A.6 Comparison between joint ratio clipping and independent ratio clipping
We apply the same clipping values to these two types of clipping, and use maps with many agents,
i.e., 10m_vs_11m and 27m_vs_30m, to make the difference more salient (based on the theoretical
results in the paper). The results are presented in Figure 7 and 8.
Compared to joint ratio clipping, the independent ratio clipping is more sensitive to the number of
agents. In particular, for a small clipping value, e.g., = 0.1, joint ratio clipping consistently produces
better performance than independent ratio clipping, even when the number of agents changes from
10 to 27. As the clipping value increases to 0.5, the performance gap between these two types of
clipping becomes larger, which is also aligned with our theoretical analysis.
16
Under review as a conference paper at ICLR 2022
∙⅛re-
Ionl_阿_IIlnl
Z7ni_v»_30ni
o.s -a <4
-M«
—6s<4	M
»7 -- 1.0(4
—Nodta*g(⅞	Λ A
一#•，(，)
一»∙»(«)
一Mffl
一<•»(«)
NOc⅜χ>g<4
M
as
-
一
一
一
NOeB^E
2s∞o Soam τsmo ImmKaXa)ιsmcoi7smoxmm
o 2soαx> soææ 75oαχ> 1 oooæo 12sooα> ι axo∞ι 7smoxaxa>
O ææoo axχjχ> τaxxs ImXa),25∞x>'saxa)w5am2axax)
27nt_w_30m
-
-
一
一
NOeB^E
≥,l8⅛√a
Q ≈WW SWaW 75WWIwSW∣≈WB∣SolSWI»2WW»
25800 a∞∞ 7SαXX> IO∞8,2S∞55088175883∞88
½ww Swow zaww 1 ww»i	awɪwɪ TswwawiMW
Figure 5:	Empirical test battle win mean (first row), test returns (second row) and trust region
estimates (third row) of MAPPO on maps with varied difficult and numbers of agents
17
Under review as a conference paper at ICLR 2022
I Γ> I
AilnJMVMer
Ionl_S_IIE
S5β?
SJCSS-EU.
~8H
o aso sæ 7® <o∞ «» iso ιτso ax»
MineerVMiestepsg
(K'
O 25» a» 7S0 lθæ ISSO 15» 17® 2∞
⅛∙⅛errfUmwt∞⅛Q
IOm W 11m
t> 256 500 7» 10∞ 125» 1«0 1750 XfB
MineerVEratepsg
⅛s ⅛m2
O 25» a» 756 1IMI> 1250 t5C0 17® 2∞
⅛∙⅛errfUmwt∞βQ
Figure 6:	Empirical returns, trust region estimates and test battle win rate for small values of
independent ratios clipping.
18
Under review as a conference paper at ICLR 2022

o :so seo no wen >≡o 1s00 na> 2aa0
Nι∙τ*∙∙“ftn∙st∙ρsg
---EeOendMt<，)
——>»«(*) I
Λlxad⅛M0r
0 S5o san iso ιcno «KB ιsoo ιτso 20 no
Nιπt>∙r crumnt∙ps(ς
10rn-v^11m
---nd«»nMK(<)
—2丽 |
__ ,
e∙s⅛I
0 :so seo no ion ，:Stl ιsoo na> 2aa0
N ι∙rtxtf SnwWpsOO
27in_w_30m
10rn_¥^_11m
-------HVHndM (<)
-11咽
<λvwΛ√vvAW
a.gnA
Λχxω*⅛W
0 ZiOSaOTSo icm t:so tso ιτs> WX
Γ*∙rtwtf HnwWrano
IM
ιac
7.S
1&C
0 :so san JSO toao >≡o ，sra nso ≡ea
NnrtwrtferosMrano
sc
10m 3 11m
1Ckn_w_11m
27m_vw_30in
27m ∙n 30n
Z7mjfvJ0n




(a) Joint TV divergence
(b) Empirical returns
Figure 7:	Joint divergence estimates and empirical returns for two types of ratio clipping at different
clipping values: 0.1 (first row), 0.3 (first row) and 0.5 (first row).
19
Under review as a conference paper at ICLR 2022
27m_”_30In
n∣H∣∞ιMιt(<∣
o ZiOSaOTSo totD t:so tsco ιτs> WX
J⅛τ*∙ror⅛n∙st∙ps(∣ς
10rn-v^11m
o ZiOSaOTSo toαι t:so tsco ιτs> Slm
HirtertIDnMrt(K)
Figure 8:
row), 0.3
Test battle win rate for two types of ratio
(first row) and 0.5 (first row).
clipping at different clipping values: 0.1 (first
20