Under review as a conference paper at ICLR 2022
Generalization of GANs and overparameter-
ized models under Lipschitz continuity
Anonymous authors
Paper under double-blind review
Ab stract
Generative adversarial networks (GANs) are so complex that the existing learning
theories do not provide a satisfactory explanation for why GANs have great suc-
cess in practice. The same situation also remains largely open for deep neural net-
works. To fill this gap, we introduce a Lipschitz theory to analyze generalization.
We demonstrate its simplicity by showing generalization and consistency of over-
parameterized neural networks. We then use this theory to derive Lipschitz-based
generalization bounds for GANs. Our bounds show that penalizing the Lipschitz
constant of the GAN loss can improve generalization. This result answers the
long mystery of why the popular use of Lipschitz constraint for GANs often leads
to great success, empirically without a solid theory. Finally but surprisingly, we
show that, when using Dropout or spectral normalization, both truly deep neural
networks and GANs can generalize well without the curse of dimensionality.
1	Introduction
In Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), we want to train a discrimi-
nator D and a generator G by solving the following problem:
min max Ex〜Pdlog(D(X)) + Ez〜Pzlog(I - D(G(Z)))	(1)
where Pd is a data distribution that generates real data, and Pz is some noise distribution. G is a
mapping that maps a noise z to a point in the data space. After training, G can be used to generate
novel but realistic data.
Since its introduction (Goodfellow et al., 2014), a significant progress has been made for developing
GANs and for interesting applications (Hong et al., 2019). Some recent works (Brock et al., 2019;
Zhang et al., 2019; Karras et al., 2020b) can train a generator that produces synthetic images of
extremely high quality. To explain those success, one popular way is to analyze generalization of the
trained players. There are many existing theories (Mohri et al., 2018) for analyzing generalization.
However, they suffer from various difficulties since the training problem of GANs is unsupervised
in nature and contains two players competing against each other. Such a nature is entirely different
from traditional learning problems. Neural distance (Arora et al., 2017) was introduced for analyzing
generalization of GANs. One major limitation of existing distance-based bounds (Arora et al., 2017;
Zhang et al., 2018; Jiang et al., 2019; Husain et al., 2019) is the strong dependence on the capacity
of the family, which defines the distance between two distributions, sometimes leading to trivial
bounds. This limitation prevents us from fullly understanding and identifying the key factors that
contribute to the generalization of GANs. For example, it has long been theoretically unclear why
Lipschitz constraint empirically can lead to great success in GANs?
The standard learning theories suffer from various difficulties when analyzing overparameterized
neural networks (NNs). For example, Radermacher-based bounds (Bartlett & Mendelson, 2002;
Golowich et al., 2020) can be trivial (Zhang et al., 2021); algorithmic stability (Shalev-Shwartz
et al., 2010) and robustness (Xu & Mannor, 2012) may not be directly used since instability is
the well-known issue when training GANs (Salimans et al., 2016; Arjovsky & Bottou, 2017; Xu
et al., 2020). Those examples are among the reasons for why the theoretical study of modern deep
learning is still in its infancy (Fang et al., 2021). Although some studies show generalization for
shallow networks with at most one hidden layer (Arora et al., 2021; Mianjy & Arora, 2020; Mou
et al., 2018; KUzborskij & Szepesvari, 2021; Ji et al., 2021; HU et al., 2021; Jacot et al., 2018), it has
1
Under review as a conference paper at ICLR 2022
long been theoretically unknown why deeper NNs can generalize better? Many great successes of
deep learning often need huge datasets, but it has long been theoretically unknown whether or not
the generalization of deep NNs suffers from the curse of dimensionality?
This work has the following contributions:
B We introduce a Lipschitz theory to analyze generalization of a learned function. This theory is
surprizingly simple to analyze various complex models in general settings (including supervised,
unsupervised, and adversarial settings).
B We show that Dropout or spectrally-normalized neural networks avoid the curse of dimensional-
ity. The number of layers required to ensure good generalization is logarithmic in sample size. It
also suggests that deeper NNs can generalize better. We further show consistency and indentify a
sufficient condition to guarantee high performance of DNNs. These results provide a significant step
toward answering the open theoretical issues of deep learning (Zhang et al., 2021; Fang et al., 2021).
B Using Lipschitz theory, we provide a comprehensive analysis on generalization of GANs which
resolves the two open challenges in the GAN community: (i) Our bounds apply to any particular D
or G, and hence overcome the major limitation of existing works; In particular, for the first time in
the literature, we show that Dropout and spectral normalization can help GANs to avoid the curse of
dimensionality. (ii) Lipschitz constraint is used popularly through various ways including gradient
penalty (Gulrajani et al., 2017), spectral normalization (Miyato et al., 2018), dropout, and data
augmentation. Our analysis provides an unified explanation for why imposing a Lipschitz constraint
can help GANs to generalize well in practice.
Organization: We will review related work in the next section. Section 3 presents the theory con-
necting Lipschitz continuity with generalization, and some analyses about deep neural networks. In
Section 4, we analyze the generalization of GANs in various aspects. Section 5 concludes the paper.
2	Related work
Generalization in GANs: There are few efforts to analyze the generalization for GANs us-
ing the notion of neural distance, dD (Pd, Pg), which is the distance between two distribu-
tions (Pd, Pg), where D is the discriminator family.1 Arora et al. (2017) analyze generaliza-
tion by bounding the quantity |dD(Pd, Pg) - dD(Pd, Pg)|, where (Pd, Pg) are empirical ver-
sions of (Pd, Pg). For a suitable choice of the loss V (Pd, Pz, D, G) in GANs, we can write
|dD(Pd,Pg) -dD(Pbd,Pbg)| = | maxD∈D V (Pd, Pz, D, G) - maxD∈D V (Pbd, Pbz, D, G)|, where Pg
is the induced distribution by putting samples from Pz through generator G. Both Arora et al. (2017)
and Husain et al. (2019) analyze | maxD∈D V (Pd, Pz, D, Go) - maxD∈D V (Pd, Pz, D, Go)| to see
generalization of a trained Go, while (Zhang et al., 2018; Jiang et al., 2019) provide upper bounds
for | maxD∈D V (Pd, Pz, D, G) - minG∈G maxD∈D V (Pd, Pz, D, G)|. Note that those quantities
of interest are non-standard in terms of learning theory.
A major limitation of those distance-based bounds (Arora et al., 2017; Zhang et al., 2018; Jiang
et al., 2019; HUsain et al., 2019) is the dependence on the notion of distance dp(∙, ∙) which relies
on the best D ∈ D for measuring proximity between two distributions. The distance between two
given distributions (μ, V) may be small even when the two are far away (Arora et al., 2017). This is
because there exists a perfect discriminator D, whenever μ and V do not have overlapping supports
(Arjovsky & Bottou, 2017). In those cases, a distance-based bound may be trivial. As a result,
existing distance-based bounds are insufficient to understand generalization of GANs.
Qi (2020) shows a generalization bound for their proposed Loss-Sensitive GAN. Nonetheless, it
is nontrivial to make their bound to work with other GAN losses. Wu et al. (2019) show that the
discriminator will generalize if the learning algorithm is differentially private. Their concept of
differential privacy basically requires that the learned function will change negligibly if the training
set slightly changes. Such a requirement is known as algorithmic stability (Xu et al., 2010) and is
nontrivial to assure in practice. Note that this assumption cannot be satisfied for GANs since their
training is well-known to be unstable in practice.
1In general, D can be replaced by another family of functions to define the neural distance. However, for
the ease of comparison with our work, the discriminator family is used.
2
Under review as a conference paper at ICLR 2022
Lipschitz continuity, stability, and generalization: Lipschitz continuity naturally appears in the
formulation of Wasserstein GAN (WGAN) (Arjovsky et al., 2017). It was then quickly recognized
as a key to improve various GANs (Fedus et al., 2018; Lucic et al., 2018; Mescheder et al., 2018;
Kurach et al., 2019; Jenni & Favaro, 2019; Wu et al., 2019; Zhou et al., 2019; Qi, 2020; Chu et al.,
2020). Gradient penalty (Gulrajani et al., 2017) and spectral normalization (Miyato et al., 2018) are
two popular techniques to constraint the Lipschitz continuity ofD or G w.r.t their inputs. Some other
works (Mescheder et al., 2017; Nagarajan & Kolter, 2017; Sanjabi et al., 2018; Nie & Patel, 2019)
suggest to control the Lipschitz continuity of D or G w.r.t their parameters. Data augmentation is
another way to control the Lipschitz constant of the loss, and is really beneficial for training GANs
(Zhao et al., 2020a;b; Zhang et al., 2020; Tran et al., 2021). Those works empirically found that
Lipschitz continuity can help improving stability and generalization of GANs. However, it has long
been a mystery of why imposing a Lipschitz constraint can help GANs to generalize well. This work
provides an unified explanation.
3	Lipschitz continuity and Generalization
In this section, we will present the main theory that connects Lipschitz continuity and generalization.
We then discuss why deep neural networks can avoid the curse of dimensionality, and why deeper
networks may generalize better.
Notations: Consider a learning problem specified by a function/hypothesis class H, an instance set
X with diameter at most B, and a loss function f : H × X → R which is bounded by a constant
C. Given a distribution Px defined on X, the quality of a function h(x) is measured by its expected
loss F(Pχ, h) = Ex〜px [f (h, x)]. Since Px is unknown, We need to rely on a finite training sample
S = {xι,…，xm} ⊂ X and often work With the empirical loss F(Px, h) = Ex〜P [f (h,x)]=
mm Px∈s f (h, x), where Px is the empirical distribution defined on S. A learning algorithm A will
pick a function hm ∈ H based on input S, i.e., hm = A(H, S).
We first establish the following result whose proof appears in Appendix A.
Theorem 1 (Lipschitz continuity ⇒ Generalization). If a loss f(h, x) is L-Lipschitz continuous
w.r.t input x in a compact set X ⊂ Rnx, for any h ∈ H, and Px is the empirical distribution defined
from m i.i.d. samples from distribution Px, then sup |F(Px, h) - F(Px, h)| is upper-bounded by
___________________________________________ h∈H
1.	Lλ + Cy∕(∖Bnx λ-nxe log4 — 2log δ)∕m with probability at least 1 一 δ, for any constants
δ ∈ (0, 1] and λ ∈ (0, B].
2.	(LB + 2C)m-a/nx With probability at least 1 — 2exp(-0.5mα), for any α ≤ nx∕(2 + nx).
The assumption about Lipschitzness is natural. When learning a bounded function (e.g. a classifier),
the assumption will satisfy if choosing a suitable loss (e.g. square loss, hinge loss, ramp loss, logistic
loss, tangent loss, pinball loss) and H which has Lipschitz members with bounded ouputs. Cross-
entropy loss can satisfy if we require every h ∈ H to have outputs belonging to a closed interval
in (0,1) or use label smoothing. Some recent works (Miyato et al., 2018; Gouk et al., 2021; Pauli
et al., 2021) propose to put a penalty on the Lipschitz constant of h only. However, leaving open the
Lipschitzness of f w.r.t h may not ensure a small Lipschitz constant of the loss.
This theorem tells that Lipschitz continuity is the key to ensure a function to generalize. Its gen-
eralization bounds can be better as the Lipschitz constant of the loss decreases. Note that there is
a tradeoff between the Lipschitz constant and the expected loss F(Px, h) of the learnt function. A
smaller L means that both f and h are getting simpler and flatter, and hence may increase F(Px, h).
In contrast, a decrease of F(Px, h) may require h to be more complex and hence may increase L.
Theorem 1 presents generalization bounds, in a general setting, which suffer from the curse of
dimensionality. This limitation is common for any other approaches without further assumptions
(Bach, 2017). For some special classes, we can overcome this limitation as discussed next.
3.1 Deep neural networks that avoid the curse of dimensionality
We consider the two families of neural networks: one with bounded spectral norms for the weight
matrices, and the other with Dropout. The following theorem whose proof appears in Appendix A.1
provides sharp bounds for the Lipschitz constant.
3
Under review as a conference paper at ICLR 2022
Theorem 2.	Let fixed activation functions (σ1, . . . , σK), where σi is ρi-Lipschitz continuous. Let
hW (x) := σK(WKσK-1(WK-1 . . . σ1(W1x) . . . )) be the neural network associated with weight
matrices (W1, . . . , WK), and Lh be the Lipschitz constant of h. Let the bounds (s1, . . . , sK) and
(b1, . . . , bK) be given.
Spectrally-normalized networks (SN-DNN): Let Hsn = {hW : W = (W1, . . . , WK), kWikσ ≤
Si}, where ∣∣ ∙ ∣∣σ is the spectral norm. Then ∀h ∈ Hsn, Lh ≤ QK=I PkSk∙
Dropout DNN: Let Hdr = {hW,q : hW,q = DrT (hW, q), kWikF ≤ bi}, where DrT is the usual
dropout training (Srivastava etal., 2014) with drop rate q for network hw, and ∣∣∙∣f is the Frobenius
norm. Then ∀h ∈ Hdr, Lh ≤ qK QkK=1 ρkbk.
Most popular activation functions (e.g., ReLU, Leaky ReLU, Tanh, SmoothReLU, Sigmoid, and
Softmax) have small Lipschitz constants (ρk ≤ 1). This theorem suggests that the Lipschitz constant
can be exponentially small as a neural network is deep (large K) and uses Dropout at each layer,
since q < 1 is a popular choice in practice. On the other hand, the Lipschitz constant will be
small if we control the spectral norms of weight matrices, e.g. by using spectral normalization. The
Lipschitz constant will be exponentially smaller as the neural network is deeper and the spectral
norm at each layer is smaller than 1. This case often happens as observed by Miyato et al. (2018).
The generalization of SN-DNNs and Dropout DNNs can be seen by combining Theorems 1 and 2.
One can observe that, for the same norm bound on weight matrices, a network with smaller Lipschitz
constant can provide a better bound. An interesting implication from Theorem 2 is that deeper
networks (larger K) will have smaller Lipschitz constants and hence lead to better generalization
bounds. This answers the second question of Section 1.
A trivial combination of Theorems 1 and 2 will result in a bound of O (m-1/nx) which suffers from
the curse of dimensionality. The following theorem shows stronger results in Appendix A.1.
Theorem 3	(Generalization of DNNs). Given the assumptions in Theorems 1 and 2, let Lf be the
Lipschitz constant of the loss f(h, x) w.r.t h, and δ ∈ (0, 1] be any given constant.
1.	SN-DNNs: assume that there exist p ∈ (0, 1) and constant Csn such that CsnpK ≥ QkK=1 ρksk.
Ifthe number of layers K ≥ — 2 logp m, then thefollowing holds with probability at least 1 一 δ:
suPh∈Hsn |F(Pχ,h) — F(Pχ,h)∣	≤ (CsnLfB + C√log4 - 2logδ) m-0.5
2.	Dropout DNNs: For Hdr with drop rate q ∈ (0, 1), let Cdr = QkK=1 ρkbk. If the number of
layers K ≥ — 2 logq m, then thefollowing holds with probability at least 1 — δ:
suPh∈Hdr ∣F(Pχ, h) - F(Pχ, h)∣ ≤	'CdrLfB + C√log4 - 2logδ) m-0.5
The assumption of K ≥ — ɪ logq m is naturally met in practice. For example, when training from
1.2M images, constant q = 0.5 requires K ≥ 10, and q = 0.1 requires K ≥ 3. Note that Alexnet
(Krizhevsky et al., 2012) has 8 layers and the generator of StyleGAN (Karras et al., 2021) has 18
layers. The assumption about SN-DNNs can be satisfied when choosing activations with Lipschitz
constant ρk < 1 or ensuring the spectral bound sk < 1 at any layer k. As mentioned before, such
conditions are often satisfied in practice (Miyato et al., 2018) when using spectral normalization.
Comparison with state-of-the-art: Some recent studies (Bartlett et al., 2017; Neyshabur et al., 2018)
provide generalization bounds for SN-DNNs for classification problems, using Radermacher com-
plexity or PAC-Bayes. One major limitation of their works is that the sample size depends poly-
nomially/exponentially on depth K. For SN-DNNs using ReLU, Golowich et al. (2020) improved
the dependence to be linear in K if provided assumptions comparable with ours. In another view,
when fixing m, their results require K = O(m) to get a meaningful generalization bound. This
is impractical. In contrast, our result shows that it is sufficient to choose K which is logarithmic
in m. Another limitation of the bounds in (Bartlett et al., 2017; Neyshabur et al., 2018; Golowich
et al., 2020) is the dependence on 1∕γ, where Y is the margin of the classification problem. Note that
practical data may have a very small margin or may be inseparable. Hence those bounds are really
limited and inapplicable to inseparable cases. On the contrary, Theorem 3 holds in general settings,
including inseparable classification and unsupervised problems.
Our result for Dropout DNNs holds in general settings including unsupervised learning. This is
significant since state-of-the-art studies about Dropout (Arora et al., 2021; Mianjy & Arora, 2020;
4
Under review as a conference paper at ICLR 2022
Mou et al., 2018) obtain efficient bounds only for networks with no more than 3 layers and for
supervised learning. To the best of our knowledge, this work is the first showing that Dropout can
help DNNs avoid the curse of dimensionality in general settings.
3.2 Consistency of overparameterized models
We have discussed generalization of a function by bounding the difference between the empirical and
expected losses. In some situations, those bounds may not be enough to explain a high performance,
since both losses may be large despite their small difference. Next we consider consistency (Shalev-
Shwartz et al., 2010) to see the goodness of a function compared with the best in its family.
Definition 1. A learning algorithm A is said to be Consistent with rate cons(m) under distribution
Px if for all m, ES 〜Pm |F (Pχ, A(H, S)) — F (Pχ, h*)| ≤ Ccons(m), where Ccons(m) must satisfy
tcons(m) → 0 as m → ∞, h = argminh∈H F(Px, h).
Consistency says that, for any (but fixed) m, the learned function hm = A(H, S) is required to be
(in expectation) close to the optimal h*. The closeness is measured by |F(Px, hm) 一 F(Px, h*)|.
By considering this quantity, optimization error will naturally appear. We first show the following
observation in Appendix A.2.
Lemma 1. Denote h* = arg minh∈H F(Px, h) and Pbx is the empirical distribution defined from a
sample S of size m. For any ho ∈ H, letting o = F(Px, ho) 一 minh∈H F(Px, h), we have:
|F(Px, ho) 一 F(Px, h )| ≤ o + 2 suph∈H |F(Px, h) 一 F(Pbx, h)|
This lemma shows why the optimization error o and capacity of family H control the goodness of
a function. Combining Theorem 1 with Lemma 1 will lead to the following.
Theorem 4	(General family). Given the assumptions in Theorem 1, consider any function ho ∈ H.
Let h* = arg minh∈H F(Px, h), and o = F(Px, ho) 一 minh∈H F(Px, h) be the optimization
error of ho on a sample S of size m. For any α ≤ nx/(2 + nx), with probability at least 1 一
2 exp(—0.5mα): |F(Px, ho) — F(Px, h*) | ≤ Co + 2(LB + 2C)m-a/nx.
Corollary 1. Given the assumptions in Theorem 4, consider a learning algorithm A and family
H. A is consistent if, for any given sample S of size m, the learned function ho = A(H, S) has
optimization error at most Co(m) which is a decreasing function ofm, i.e., Co(m) → 0 as m → ∞.
The assumption about optimization error Co(m) is naturally satisfied when the training problem is
convex. Indeed, it is well-known (Allen-Zhu, 2017; Schmidt et al., 2017) that gradient descent (GD)
with T iterations can find a solution with error O(T -1) whereas stochastic gradient descent (SGD)
with T iterations can find a solution with error O(T -0.5). Therefore, GD and SGD with T = O(m)
iterations will satisfy this assumption. Note that convex training problems appear in many traditional
models (Hastie et al., 2017), e.g., linear regression, support vector machines, kernel regression.
For DNNs, the training problems are often nonconvex and hence the assumption may not always
hold. Surprisingly, overparameterized models can lead to tractable training problems. Indeed,
(Allen-Zhu et al., 2019; Du et al., 2019; Zou et al., 2020; Nguyen & Mondelli, 2020; Nguyen,
2021) show that GD and SGD can find global solutions of the training problems for popular DNN
families. For T iterations, GD and SGD can find a solution with error O(T -0.5). Those results
suggests that T = O(m) iterations are sufficient to ensure our assumption about Co(m). Allen-Zhu
et al. (2019) show that T = O(log m) iterations are sufficient to ensure Co(m) = O(m-1).
Combining Theorems 3 with Lemma 1 will lead to the following for Dropout DNNs. Similar results
can be shown for SN-DNNs.
Theorem 5	(Dropout family). Given the assumptions in Theorem 3, consider any ho ∈ Hdr. Let
h* = arg minh∈Hdr F(Px, h), and Co = F(Px, ho) — minh∈Hdr F(Px, h) be the optimization
error ofho on a sample S of size m. For any constant δ ∈ (0, 1], with probability at least 1 — δ:
|F(Px, ho) — F(Px, h*)∣≤ Co + 2 (CdrLfB + C√log4 - 2log δ) m-0.5
Corollary 2 (Consistency of Dropout DNNs). Given the assumptions in Theorem 5, consider a
learning algorithm A and family Hdr. If, for any given sample S of size m, the learned function
ho = A(Hdr, S) has optimization error at most Co(m) which is a decreasing function ofm, then A
is consistent with rate c°(m) + 2 (Cdr Lf B + Cʌ/log 4 — 2 log δ) m-0∙5.
5
Under review as a conference paper at ICLR 2022
Connection to overparameterization: Contrary to classical wisdoms about overfitting, modern ma-
chine learning exhibits a strange phenonmenon: very rich models such as neural networks are trained
to exactly fit (i.e., interpolate and o = 0) the data, but often obtain high accuracy on test data (Belkin
et al., 2019; Zhang et al., 2021). Those models often belong to overparameterization regime where
the number of parameters in a model is far larger than m. Such a strikingly strange behavior could
not be explained by traditional learning theories (Zhang et al., 2021). Some works try to under-
stand overparameterization in linear regression (Bartlett et al., 2020) and kernel regression (Liang
et al., 2020). Some recent results (KUzborskij & Szepesvari, 2021; Ji et al., 2021; HU et al., 2021;
Jacot et al., 2018) on consistency hold only for shallow neural networks with no more than 3 layers.
However, consistency of deep neUral networks remains largely open.
For overparameterized NNs with a sUitable width, T = O(m) iterations are sUfficient for GD and
SGD to achieve optimization error o(m) = O(m-0.5) as discUssed before. Combining this ob-
servation with Corollary 2 will reveal consistency with rate O(m-0.5) for DropoUt DNNs and SN-
DNNs. To the best of oUr knowledge, this is the first consistency resUlt for overparameterized DNNs
which are trUly deep and avoid the cUrse of dimensionality.
Why are small consistency rates for high-capacity families sufficient to guarantee high gener-
alization? To see why, consider GapB(ho, η) = F (Px, ho) - F (Px, η) which is the Bayes gap
of an ho = A(H, S), where η denotes the (Unknown) trUe fUnction we are trying to learn. Note
that GapB (ho, η) = Cons(ho, m) + F(Pχ, h*) — F(Pχ, η), where Cons(h°, m) = F(Px, h。)一
F(Px, h*) denotes the consistency rate. This decomposition suggests that a requirement of both
Cons(ho, m) and F(Px, h*) to be small will ensure a small GapB (ho, η), since F(Px, η) is inde-
pendent ofH. In other words, a small consistency rate for high-capacity H is sufficient to guarantee
high performance of ho on test data.
Overparameterized NNs often have a very high capacity. Some regularization methods can help
us localize a subset Hg of the chosen NN family so that Hg has a small generalization gap. For
example, in Theorem 3, we originally need to work with family H = {hW : kWikF ≤ bi}, but
Dropout localizes a subset Hdr ⊂ H having a small generalization gap. One should ensure that
Hg still has a high capacity to produce a small optimization error. Interestingly, a small (even zero)
optimization error is frequently observed in practice for overparameterized NNs (Zhang et al., 2021).
In those cases, we can achieve a small consistency rate as shown in Corollary 2. Our work shows
this property for Dropout and SN. Combining these arguments with the above sufficient condition
will provide an answer for why those overparameterized NNs can work well on test data.
4 Generalization of GANs
This section presents a comprehensive analysis on generalization of GANs. We then discuss why
Lipschitz constraint succeeds in practice.
Notations: Let S = {x1, ..., xm, z1, ..., zm} consist of m i.i.d. samples from real distribution Pd
defined on a compact set Zx ⊂ Rnx and m i.i.d. samples from noise distribution Pz defined on a
.，一 E” A	.....	....	……	C	JlC	.
compact set Zz ⊂ Rn, Px and Pz be the empirical distributions defined from S respectively. Denote
D as the discriminator family and G as the generator family. Let v(D,G,χ,z) = ψι(D(χ))+ψz(1-
D(G(Z))) be the loss defined from a real example X 〜 Pd, a noise Z 〜 Pz, a discriminator D ∈D,
and a generator G ∈ G. Different choices of the measuring functions (ψ1, ψ2) will lead to different
GANs. For example, saturating GAN (Goodfellow et al., 2014) uses ψ1(x) = ψ2(x) = log(x);
WGAN (Arjovsky et al., 2017) uses ψ1(x) = ψ2(x) = x; LSGAN (Mao et al., 2017; 2019) uses
ψ1(x) = —(x + a)2, ψ2(x) = —(x + b)2 for some constants a, b; EBGAN (Zhao et al., 2017) uses
ψ1(x) = x, ψ2(x) = max(0, r—x) for some constant r. We will often work with: V (Pd, Pz, D, G) =
_ , , .. _ , ... ^ _ , , .. _ , _ , ,...
Ex〜Pdψι(D(x))+Ez〜Pzψ2(l-D(G(z))); V(Pd, Pz,D,G) = Ex〜Pdψι(D(x))+Ez〜Pzψ2(l-D(G(z)))
.^ . . .. . ...................................... . ^ ^ . ...
V(Pd,Pz,D,G) = Ex〜PdΨι(D(x)) + Ez〜Pzψ2(1 - D(G(Z)))； V(Pd,Pz,D,G) = Ex〜PdΨι(D(x)) +
Ez〜PzΨ2(l- D(G(Z)))
In practice, we only have a finite sample S and an optimizer will solve
minG∈G maxD∈D V (Pd, Pz, D, G) and return an approximate solution (Do, Go), which can
be different from the training optimum (DO, G*) and Nash solution (D*, G*), where
,_ , _.,, __,^ ^ _ 一
(Do,Go) = arg min max V (Pd, Pz ,D, G),
G∈G D∈D
(D*, G*) = argminmax V (Pd, Pz, D, G) (2)
G∈G D∈D
6
Under review as a conference paper at ICLR 2022
T 1	♦	.1	C	, ♦	/τr∕7-⅛ I ->	7-Λ	z->∖	τr∕τ^ iɔ t^λ z->∖∖.	i
In learning theory, we often estimate (V (Pd, Pz , Do, Go) - V (Pd, Pz , Do, Go)) to see general-
ization. However a small bound on this quantity may not be enough, since V (Pd, Pz , Do, Go)
can be far from the best V (Pd, Pz, D*, G*). Another way (Bousquet et al., 2004) is to see How
good is (Do, Go) compared to the Nash solution (D*, G*)? In other words, we basically need
to estimate the difference |V (Pd,Pz ,Do,Go) - V (Pd, Pz ,D*,G*)∣ = |V (Pd, Pz ,Do, Go)-
minG∈G maxD∈D V(Pd, Pz, D, G)| where V(Pd, Pz , Do, Go) shows the quality of the fake dis-
tribution induced by generator Go .
We first make the following error decomposition:
V (Pd, Pz, Do, Go)- V (Pd, Pz, D*,G*) = [V (Pd, Pz, Do, Go)- V (Pd, Pz, Do, Go)]+
[V (Pd, Pz, Do, Go) - V(Pd, Pz, Do,G：)] + [V(Pd, Pz, Do,G：) - V (Pd, Pz, D*,G*)] (3)
一 _ __________.. — — — —、 _ _, ^ ^ __ .. . 一 .一 一 一 .一 _ ... 一
The first term (V(Pd, Pz, Do, Go) - V(Pbd, Pbz, Do, Go)) in the right-hand side of (3) shows the
difference between the population and empirical losses of a specific solution (Do, Go). The sec-
ond term (V(Pd, Pz, Do, Go) - V(Pd, Pz, Do, G*)) is in fact the Optimization error incurred by
the optimizer. This error depends strongly on the capacity of the chosen optimizer. The third term
___≤>≤≥一______________— — 一.~.一∙ .一 一 一 . 一 一
(V(Pbd, Pbz, Do*, Go*) -V(Pd, Pz, D*, G*)) is optimizer-independent and strongly depends on the ca-
pacity of both families (D, G), since both V(Pbd, Pbz, Do*, Go*) and V(Pd, Pz, D*, G*) are optimizer-
independent. We call this term Joint error of (D, G). In the next subsections, we will provide upper
bounds on both the error of (Do, Go) and joint error of (D, G), and then generalization bounds that
take the optimization error into account.
In the later discussions, we will often use the following assumptions and notation L = LψLdLg
which upper bounds the Lipschitz constant of the loss v(D, G, x, z).
Assumption 1. ψ1 and ψ2 are Lψ -Lipschitz continuous w.r.t. their inputs on a compact domain and
upper-bounded by constant C ≥ 0.
Assumption 2. Each generator G ∈ G is Lg -Lipschitz continuous w.r.t its input z over a compact
set Zz ⊂ Rn with diameter at most Bz.
Assumption 3. Each discriminator D ∈ D is Ld -Lipschitz continuous w.r.t its input x over a
compact set Zx ⊂ Rnx with diameter at most Bx.
These assumptions are reasonable and satisfied by various GANs. For example, WGAN, LSGAN,
EBGAN naturally satisfy Assumption 1, while saturating GAN will satisfy it if we constraint the
output of D to be in [α, β] ⊂ (0, 1) as often used in practice. Spectral normalization and gradient
penalty are popular techniques to regularize D and are crucial for large-scale generators (Zhang
et al., 2019; Karras et al., 2020b). Therefore Assumptions 3 and 2 are natural.
4.1 Error bounds
The following result readily comes from Theorem 1.
Corollary 3. Given the assumptions (1, 2, 3), for any δ ∈ (0, 1], λ ∈ (0, Bz], with probability at
least 1 一 δ, we have
SUPD∈D,G∈G |V (Pd, Pz ,D,G) - V (Pd, Pz, D,G)∖ ≤ Lλ + 1 PdBn λ-ne log4 - 2log δ
This corollary tells the generalization of any generator G ∈ G, and can be further tighten by using
Theorem 3 when using SN or Dropout. To see generalization of both players (Do, Go), observe that
., ^ ^ .. , ^ ^ ..
∖V(Pd, Pz, Do, Go) -V(Pbd,Pbz,Do,Go)∖ ≤ supD∈D,G∈G ∖V (Pbd, Pbz, D, G) -V(Pd,Pz,D,G)∖.
The following theorem provides an upper bound whose proof appears in Appendix B.
___	-	, _	_____O O - —八	_____ —	一
Theorem 6. Denote (D, G) = supD∈D,G∈G ∖V(Pbd, Pbz, D, G) - V(Pd, Pz, D, G)∖. Given the
assumptions (1, 2, 3), for any constants δ, δx ∈ (0, 1],
(General family) for any λ ∈ (0, Bz], λx ∈ (0, Bx], with probability at least 1 - δ - δx:
e(D, G) ≤ Lλ+√m PdBnλ-ne log4 - 2iogδ+Lψ Ldλx+√m q d Bnx h nx e log4 -2 log ⅛r
(D with spectral norm) given the assumptions in Theorem 3, with probability at least 1 — 2δ:
e(Hsn, G) ≤ [CsnLψLgBz + 2C√log 4 — 2 log δ + CsnLψBχ]m 0.5
7
Under review as a conference paper at ICLR 2022
(D with Dropout) given the assumptions in Theorem 3, with probability at least 1 一 2δ:
E(Hdr, G) ≤ [Cdr LψLgBz + 2C√log 4 - 2 log δ + CdrLψBχ]m 0.5
For many models, such as WGAN, the measuring functions and D are Lipschitz continuous w.r.t
their inputs. Note that the generator in WGAN, LSGAN, and EBGAN will be Lipschitz continuous
w.r.t z, if we use some regularization methods such as gradient penalty or spectral normalization
for both players. Theorem 6 also suggests that penalizing the zero-order (C) and first-order (L)
informations of the loss can improve the generalization. This provides a significant evidence for
the important role of gradient penalty or spectral normalization for the success of some large-scale
generators (Zhang et al., 2019; Brock et al., 2019; Karras et al., 2020b).
It is worth observing that a small Lipschitz constant of the loss not only requires that both discrimi-
nator and generator are Lipschitz continuous w.r.t their inputs, but also requires Lipschitz continuity
of the loss w.r.t both players. Most existing efforts focus on the players in GANs, and leave the loss
open. Constraining on either D or G may be insufficient to ensure Lipschitz continuity of the loss.
One advantage of the generalization bounds in Theorem 6 is that the upper bounds on
|V (Pbd, Pbz, D, G) 一 V (Pd, Pz, D, G)| hold true for any particular (D, G) in their families. Mean-
while, the existing generalization bounds (Arora et al., 2017; Zhang et al., 2018; Jiang et al., 2019;
Wu et al., 2019; Husain et al., 2019) hold true conditioned on the best discriminator. Hence the
bounds in Theorem 6 are more practical than existing ones, since D is not trained to optimality
before training G in practical implementations of GANs.
__,^ ^ _ . _... __, _ _ _ . _... . , _
Next We consider the joint error V (Pd, Pbz, DO, G*) 一 V (Pd, Pz, D*,G*) of both families (D, G).
Such a quantity also shows the goodness of the training optimum (DO, GO) compared with the
,_ . _... . ___________________________ ^ ^ _ . _...	__, _ _ _ . _....
Nash solution (DO, GO). It is worth observing that |V(Pbd, Pbz, DOO, GOO) 一 V(Pd, Pz, DO, GO)| =
| minG∈G maxD∈D V(Pd, Pz, D, G) 一 minG∈G maxD∈D V(Pd, Pz, D, G)| measures the quality of
the best players given a finite number of samples only, and such error does not depend on any
optimizer. Hence it represents the Joint capacity of both generator and discriminator families. The
following theorem provides an upper bound whose proof appears in Appendix B.
Theorem 7. Given the assumptions (1, 2, 3), for any constants δ, δx ∈ (0, 1], λ ∈ (0, Bz], λx ∈
, _— . _ _ ____________________________________ ^ ^ _ . . . . __ , _ _ _ . _ . . . . _ .
(0, Bχ], with probability at least 1 — δ — δχ: |V(Pbd, Pbz, DO, GO) — V(Pd, Pz, D , Go)∣ ≤ Lλ +
√CmP∖Bnλ~ne log4 - 2logδ + LψLdλx + √CmqdBxxλxnxelog4 - 2logδχ.
This bound on joint capacity of (D, G ) is loose, since few informations about those families are
used. We can tighten this bound when using SN or Dropout for D, similar with Theorem 6.
4.2	From optimization error to generalization
Finally we make a bidge between optimization error and generalization. The decomposition (3)
contains three components, for which the first component is bounded in Theorem 6 while the third
component is bounded in Theorem 7. Combining those observations will lead to the following result.
Theorem 8 (Generalization bounds for GANs). Assume the assumptions (1, 2, 3) and the opti-
■	. ■	ITΛ∕τ-i τ~i 1-Λ	∖	♦	TT/A ATA x>∖ I -
mization error |V(Pd, Pz, DO, GO) — min max V(Pd, Pz, D, G)| ≤ EO
G∈G D∈D
Denote EcOns (D, G)
|V(Pd, Pz, DO, GO) — V(Pd,Pz, DO, GO)|. For any constants δ, δx ∈ (0, 1],
1.	(General family) for any λ ∈ (0, Bz], λx ∈ (0, Bx], with probability at least 1 — δ — δx:
Econs(D, G)	≤ Eo + 2Lλ + √√= PdBnλ-ne l0g4 - 2log δ + 2LψLdλχ +
√m q ∖BX λ-nxe log4 - 2log δχ.
2.	(Spectral norm) g^ven the assumptions in Theorem 3, D ≡ Hsn, with probability at least
1 - 2δ: Econs(Hsn, G) ≤ Eo + 2[CsnLψLgBz + 2C√log4 - 2logδ + CsnLψBχ]m 0.5
3.	(Dropout) given the assumptions in Theorem 3, D ≡ Hdr, With probability at least 1 - 2δ:
Econs(Hdr , G) ≤ Eo + 2[Cdr Lψ Lg Bz + 2C √log4 - 2log δ + CdrLψ Bχ]m-0.5
Theorems 6 and 8 provide us a comprehensive view about generalization of GANs. Note that their
assumptions are naturally met in practice as pointed out before. For the first time in the GAN litera-
8
Under review as a conference paper at ICLR 2022
ture, our work reveals that GANs can avoid the curse of dimensionality when choosing appropriate
(D, G ). Furthermore, a logarithmic (in m) number of layers are sufficient for each player. Although
this work shows this property for DNNs with spectral norm or Dropout. We believe that this property
can hold for many other DNN families.
One important implication from Theorem 8 is that GANs can be consistent under suitable conditions.
An example condition is overparameterization, for which the optimization error can be zero. Our
experiments in Appendix F provide a good evidence for this conjecture as the well-trained discrim-
inators often reach Nash equilibria. A recent investigation about optimization of overparameterized
GANs appears in (Balaji et al., 2021). We leave this door open for the readers.
4.3	Why a Lipschitz constraint is crucial
Various works (Guo et al., 2019; Jenni & Favaro, 2019; Qi, 2020; Arjovsky et al., 2017; Gulrajani
et al., 2017; Roth et al., 2017; Miyato et al., 2018; Zhou et al., 2019; Thanh-Tung et al., 2019; Jiang
et al., 2019; Tanielian et al., 2020; Xu et al., 2020) try to ensure Lipschitz continuity of the discrim-
inator or generator. The most popular techniques are gradient penalty (Gulrajani et al., 2017) and
spectral normalization (SN) (Miyato et al., 2018). Those techniques are really useful for different
losses (Fedus et al., 2018) and high-capacity architectures (Kurach et al., 2019). From a large-scale
evaluation, Kurach et al. (2019) found that gradient penalty can help the performance of GANs but
does not stabilize the training, whereas using SN for G only is insufficient to ensure stability (Brock
et al., 2019). Some recent large-scale generators (Brock et al., 2019; Zhang et al., 2019; Karras
et al., 2020b) use gradient penalty or SN to ensure their successes. Data augmentation (Zhao et al.,
2020a;b; Tran et al., 2021; Karras et al., 2020a) also contributes to the excellent performance of
GANs in practice, due to implicitly penalizing the Lipschitz constant of the loss (see Appendix D
for explanation). Those empirical observations without a theory poses a long mystery of why can
imposing a Lipschitz constraint help GANs to perform well? This work provides an answer:
. Theorems 6 and 8 show that a Lipschitz constraint on one player (D or G) can help, but may be
not enough. A penalty on the first-order (L) information of the loss can lead to better generalization.
. Spectral normalization (Miyato et al., 2018) is a popular technique to regularize GANs. Using
SN, the spectral norms of the weight matrices are often small in practice, and hence the Lipschitz
constant of D (or G) can be exponentially small when using SN. In those cases, the assumptions of
Theorem 8 are satisfied. Therefore the generalization bound in Theorem 8 is tight and supports well
the success of spectrally-normalized GANs (Miyato et al., 2018; Zhang et al., 2019).
. Dropout and SN are really efficient to control the complexity of the players and provide tight
generalization bounds.
. WGAN (Arjovsky et al., 2017) naturally requires D to be 1-Lipschitz continuous. Weight clipping
is used so that every element of network weights belongs to [-c, c] for some constant c. For some
choices, e.g. c = 0.01 in (Arjovsky et al., 2017), the spectral norm of the weight matrix at each
layer can be smaller than 1.2 In those cases the Lipschitz constant of D can be exponentially small,
leading to tight bounds in Theorem 8 and better generalization.
. SN, gradient penalty, and data augmentation are crucial parts of large-scale GANs (Brock et al.,
2019; Zhang et al., 2019; Karras et al., 2020b). As a result, Theorems 6 and 8 provide a strong
support for their success in practice.
. Our experiments with SN in Appendix F indeed show that SN can reduce the Lipschitz constants
of the players and the loss. However, when SN is overused, the trained players can get underfitting
and may hurt generalization. A reason is that an underfitted model can have a bad population loss
and high optimization error.
5 Conclusion
We have presented a simple way to analyze generalization of various complex models that are hard
for traditional learning theories. Some successful applications were done and made a significant
step toward understanding DNNs and GANs. One limitation of our bounds is that the optimization
aspect is left open.
2For c = 0.01, if the number of units at each layer is no more than 100, then the Frobenius norm of the
weight matrice at each layer is smaller than 1, and so is for the spectral norm.
9
Under review as a conference paper at ICLR 2022
References
Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. The
Journal of Machine Learning Research ,18(1):8194-8244, 2017.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252. PMLR, 2019.
Martin Arjovsky and Leon Bottou. Towards principled methods for training generative adversarial
networks. In International Conference on Learning Representations, 2017.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In Proceedings of the 34th International Conference on Machine Learning, 2017.
Raman Arora, Peter Bartlett, Poorya Mianjy, and Nathan Srebro. Dropout: Explicit forms and
capacity control. In International Conference on Machine Learning, pp. 351-361. PMLR, 2021.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium
in generative adversarial nets (gans). In International Conference on Machine Learning, pp. 224-
232, 2017.
Haim Avron and Sivan Toledo. Randomized algorithms for estimating the trace of an implicit sym-
metric positive semi-definite matrix. Journal of the ACM (JACM), 58(2):1-34, 2011.
Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal of
Machine Learning Research, 18(1):629-681, 2017.
Yogesh Balaji, Mohammadmahdi Sajedi, Neha Mukund Kalibhat, Mucong Ding, Dominik Stoger,
Mahdi Soltanolkotabi, and Soheil Feizi. Understanding over-parameterization in generative ad-
versarial networks. In International Conference on Learning Representations, 2021.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. Advances in Neural Information Processing Systems, 30:6240-6249, 2017.
Peter L Bartlett, Philip M Long, Gabor Lugosi, and Alexander Tsigler. Benign Ovefitting in linear
regression. Proceedings of the National Academy of Sciences, 117(48):30063-30070, 2020.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-
learning practice and the classical bias-variance trade-off. Proceedings of the National Academy
of Sciences, 116(32):15849-15854, 2019.
Olivier Bousquet, StePhane Boucheron, and Gabor Lugosi. Introduction to statistical learning the-
ory. In Machine Learning 2003, LNAI, volume 3176, pp. 169-207. Springer, 2004.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural
image synthesis. In International Conference on Learning Representations, 2019.
Casey Chu, Kentaro Minami, and Kenji Fukumizu. Smoothness and stability in gans. In Interna-
tional Conference on Learning Representations, 2020.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-
1685. PMLR, 2019.
Cong Fang, Hanze Dong, and Tong Zhang. Mathematical models of overparameterized neural
networks. Proceedings of the IEEE, 109(5):683-703, 2021.
William Fedus, Mihaela Rosca, Balaji Lakshminarayanan, Andrew M Dai, Shakir Mohamed, and
Ian Goodfellow. Many paths to equilibrium: Gans do not need to decrease a divergence at every
step. In International Conference on Learning Representations, 2018.
10
Under review as a conference paper at ICLR 2022
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. Information and Inference: A Journal of the IMA, 9(2):473-504, 2020.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems, pp. 2672-2680, 2014.
Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael J Cree. Regularisation of neural net-
works by enforcing lipschitz continuity. Machine Learning, 110(2):393-416, 2021.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of wasserstein gans. In Advances in Neural Information Processing Systems, pp.
5767-5777, 2017.
Tianyu Guo, Chang Xu, Boxin Shi, Chao Xu, and Dacheng Tao. Smooth deep image generator
from noises. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp.
3731-3738, 2019.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning: Data
Mining, Inference, and Prediction. Springer, New York, NY, 2017.
Yongjun Hong, Uiwon Hwang, Jaeyoon Yoo, and Sungroh Yoon. How generative adversarial net-
works and their variants work: An overview. ACM Computing Surveys (CSUR), 52(1):1-43,
2019.
Tianyang Hu, Wenjia Wang, Cong Lin, and Guang Cheng. Regularization matters: A nonparamet-
ric perspective on overparametrized neural network. In International Conference on Artificial
Intelligence and Statistics, pp. 829-837. PMLR, 2021.
Hisham Husain, Richard Nock, and Robert C Williamson. A primal-dual link between gans and
autoencoders. In Advances in Neural Information Processing Systems, volume 32, pp. 415-424,
2019.
Michael F Hutchinson. A stochastic estimator of the trace of the influence matrix for laplacian
smoothing splines. Communications in Statistics - Simulation and Computation, 18(3):1059-
1076, 1989.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: convergence and gener-
alization in neural networks. In Advances in Neural Information Processing Systems, pp. 8580-
8589, 2018.
Simon Jenni and Paolo Favaro. On stabilizing generative adversarial training with noise. In Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 12145-12153,
2019.
Ziwei Ji, Justin D Li, and Matus Telgarsky. Early-stopped neural networks are consistent. arXiv
preprint arXiv:2106.05932, 2021.
Haoming Jiang, Zhehui Chen, Minshuo Chen, Feng Liu, Dingding Wang, and Tuo Zhao. On com-
putation and generalization of generative adversarial networks under spectrum control. In Inter-
national Conference on Learning Representations, 2019.
Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training
generative adversarial networks with limited data. In Advances in Neural Information Processing
Systems, 2020a.
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Ana-
lyzing and improving the image quality of stylegan. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 8110-8119, 2020b.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.
doi: 10.1109/TPAMI.2020.2970919.
11
Under review as a conference paper at ICLR 2022
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems, volume 25, pp.
1097-1105, 2012.
Karol Kurach, Mario LuCiC, Xiaohua Zhai, Marcin Michalski, and Sylvain Gelly. A large-scale study
on regularization and normalization in gans. In International Conference on Machine Learning,
pp. 3581-3590, 2019.
Ilja Kuzborskij and Csaba Szepesvari. Nonparametric regression with shallow overparameterized
neural networks trained by gd with early stopping. In Conference on Learning Theory, pp. 2853-
2890. PMLR, 2021.
Tengyuan Liang, Alexander Rakhlin, and Xiyu Zhai. On the multiple descent of minimum-norm
interpolants and restricted lower isometry of kernels. In Conference on Learning Theory, pp.
2683-2711. PMLR, 2020.
Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans
created equal? a large-scale study. In Advances in Neural Information Processing Systems, pp.
700-709, 2018.
Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley.
Least squares generative adversarial networks. In Proceedings of the IEEE International Confer-
ence on Computer Vision, pp. 2794-2802, 2017.
Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. On
the effectiveness of least squares generative adversarial networks. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 41(12):2947-2960, 2019.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of gans. In Advances in
Neural Information Processing Systems, pp. 1825-1835, 2017.
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do
actually converge? In International Conference on Machine Learning, pp. 3481-3490, 2018.
Poorya Mianjy and Raman Arora. On convergence and generalization of dropout training. In Ad-
vances in Neural Information Processing Systems, volume 33, 2020.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. In International Conference on Learning Representations,
2018.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning.
MIT Press, 2018.
Wenlong Mou, Yuchen Zhou, Jun Gao, and Liwei Wang. Dropout training, data-dependent reg-
ularization, and generalization bounds. In International Conference on Machine Learning, pp.
3645-3653. PMLR, 2018.
Vaishnavh Nagarajan and J Zico Kolter. Gradient descent gan optimization is locally stable. In
Advances in Neural Information Processing Systems, pp. 5585-5595, 2017.
Vaishnavh Nagarajan and J Zico Kolter. Uniform convergence may be unable to explain generaliza-
tion in deep learning. In Advances in Neural Information Processing Systems, pp. 11615-11626,
2019.
Jeffrey Negrea, Gintare Karolina Dziugaite, and Daniel Roy. In defense of uniform convergence:
Generalization via derandomization with an application to interpolating predictors. In Interna-
tional Conference on Machine Learning, pp. 7263-7272. PMLR, 2020.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to
spectrally-normalized margin bounds for neural networks. In International Conference on Learn-
ing Representations, 2018.
12
Under review as a conference paper at ICLR 2022
Quynh Nguyen. On the proof of global convergence of gradient descent for deep relu networks with
linear widths. In International Conference on Machine Learning, 2021.
Quynh Nguyen and Marco Mondelli. Global convergence of deep networks with one wide layer fol-
lowed by pyramidal topology. In Advances in Neural Information Processing Systems, volume 33,
2020.
Weili Nie and Ankit Patel. Towards a better understanding and regularization of gan training dy-
namics. In Conference on Uncertainty in Artificial Intelligence (UAI), 2019.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural sam-
plers using variational divergence minimization. In Advances in Neural Information Processing
Systems,pp. 271-279, 2016.
Patricia Pauli, Anne Koch, Julian Berberich, Paul Kohler, and Frank Allgower. Training robust
neural networks using lipschitz bounds. IEEE Control Systems Letters, 2021.
Guo-Jun Qi. Loss-sensitive generative adversarial networks on lipschitz densities. International
Journal of Computer Vision, 128(5):1118-1140, 2020.
Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, and Thomas Hofmann. Stabilizing training of
generative adversarial networks through regularization. In Advances in Neural Information Pro-
cessing Systems, pp. 2018-2028, 2017.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in Neural Information Processing Systems,
pp. 2234-2242, 2016.
Maziar Sanjabi, Jimmy Ba, Meisam Razaviyayn, and Jason D Lee. On the convergence and ro-
bustness of training gans with regularized optimal transport. In Advances in Neural Information
Processing Systems, pp. 7091-7101, 2018.
Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic
average gradient. Mathematical Programming, 162(1-2):83-112, 2017.
Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability
and uniform convergence. The Journal of Machine Learning Research, 11:2635-2670, 2010.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine
Learning Research, 15(1):1929-1958, 2014.
Ugo Tanielian, Thibaut Issenhuth, Elvis Dohmatob, and Jeremie Mary. Learning disconnected man-
ifolds: a no gan’s land. In International Conference on Machine Learning, 2020.
Hoang Thanh-Tung, Truyen Tran, and Svetha Venkatesh. Improving generalization and stability of
generative adversarial networks. In International Conference on Learning Representations, 2019.
Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-
encoders. In International Conference on Learning Representations, 2018.
Ngoc-Trung Tran, Viet-Hung Tran, Ngoc-Bao Nguyen, Trung-Kien Nguyen, and Ngai-Man Che-
ung. On data augmentation for gan training. IEEE Transactions on Image Processing, 30:1882-
1897, 2021.
Bingzhe Wu, Shiwan Zhao, Chaochao Chen, Haoyang Xu, Li Wang, Xiaolu Zhang, Guangyu Sun,
and Jun Zhou. Generalization in generative adversarial networks: A novel perspective from pri-
vacy protection. In Advances in Neural Information Processing Systems, pp. 307-317, 2019.
Huan Xu and Shie Mannor. Robustness and generalization. Machine learning, 86(3):391-423,
2012.
Huan Xu, Constantine Caramanis, and Shie Mannor. Robust regression and lasso. IEEE Transac-
tions on Information Theory, 56(7):3561-3574, 2010.
13
Under review as a conference paper at ICLR 2022
Kun Xu, Chongxuan Li, Huanshu Wei, Jun Zhu, and Bo Zhang. Understanding and stabilizing gans’
training dynamics with control theory. In Proceedings of the 37th International Conference on
Machine Learning, 2020.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning (still) requires rethinking generalization. Communications oftheACM, 64(3):107-
115, 2021.
Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative
adversarial networks. In International Conference on Machine Learning, pp. 7354-7363, 2019.
Han Zhang, Zizhao Zhang, Augustus Odena, and Honglak Lee. Consistency regularization for
generative adversarial networks. In International Conference on Learning Representations, 2020.
Pengchuan Zhang, Qiang Liu, Dengyong Zhou, Tao Xu, and Xiaodong He. On the discrimination-
generalization tradeoff in gans. In International Conference on Learning Representations, 2018.
Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial networks. In
International Conference on Learning Representations, 2017.
Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han. Differentiable augmentation for
data-efficient gan training. In Advances in Neural Information Processing Systems, 2020a.
Zhengli Zhao, Zizhao Zhang, Ting Chen, Sameer Singh, and Han Zhang. Image augmentations for
gan training. arXiv preprint arXiv:2006.02595, 2020b.
Zhiming Zhou, Jiadong Liang, Yuxuan Song, Lantao Yu, Hongwei Wang, Weinan Zhang, Yong Yu,
and Zhihua Zhang. Lipschitz generative adversarial nets. In International Conference on Machine
Learning, pp. 7584-7593, 2019.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-
parameterized deep relu networks. Machine Learning, 109(3):467-492, 2020.
14
Under review as a conference paper at ICLR 2022
A LIPSCHITZ CONTINUITY ⇒ GENERALIZATION
This section provides the proofs for the theorems of Section 3. Let X = SiN=1 Xi be a partition of
X into N disjoint subsets. We use the following definition from Xu & Mannor (2012).
Definition 2 (Robustness). An algorithm A is (N, )-robust, for : Xm → R, if the following holds
for all S ∈ Xm:
∀s ∈	S,∀x ∈ X,∀i	∈	{1,..., N}, ifs,x ∈	Xi	then |f (A(H,	S),	s) -	f (A(H,	S), x)| ≤	(S).
Basically, a robust algorithm will learn a hypothesis which ensures that the losses of two similar data
instances should be the same. A small change in the input leads to a small change in the loss of the
learnt hypothesis. In other words, the robustness ensures that each testing sample which is close to
the training dataset will have a similar loss with that of the closest training samples. Therefore, the
hypothesis A(H, S) will generalize well over the areas around S.
Theorem 9 (Xu & Mannor (2012)). Ifa learning algorithm A is (N, )-robust and the training data
S is an i.i.d. sample from distribution Px, then for any δ ∈ (0, 1] we have the following with proba-
bility at least 1 一 δ: |F(Pχ, A(H, S)) 一 F(Pχ, A(H, S))| ≤ E(S) + C'(Nlog4 - 2log δ)∕m.
This theorem formally makes the important connection between robustness of an algorithm and
generalization. If an algorithm is robust, then its resulting hypotheses can generalize. One important
implication of this result is that we should ensure the robustness of a learning algorithm. However,
it is nontrivial to do so in general.
Let us have a closer look at robustness. E(S) in fact bounds the amount of change in the loss with
respect to a change in the input given a fixed hypothesis. This observation suggests that robustness
closely resembles the concept of Lipschitz continuity. Remember that a function y : X → Y is
said to be L-Lipschitz continuous if dy (y(x), y(x0)) ≤ Ldx (x, x0) for any x, x0 ∈ X, where dx is a
metric on X , dy is a metric on Y, and L ≥ 0 is the Lipschitz constant. Therefore, we establish the
following connection between robustness and Lipschitz continuity.
Lemma 2. Given any constant λ > 0, consider a loss f : H × X → R, where X ⊂ Rnx is
compact, B = diam(X) = maXχ,χθ∈χ ||x — x0∣∣∞, N =「Bnxλ-nx]. Ifforany h ∈ H, f(h, x) is
L-Lipschitz continuous w.r.t input x, then any algorithm A that maps Xm to H is (N, Lλ)-robust.
Proof: It is easy to see that there exist N = d(B∕λ)nxe disjoint nx-dimensional cubes, each with
edge length of λ, satisfying that their union covers X completely since X is compact. Let Ck be one
of those cubes, indexed by k, and Xk = X ∩ Ck. We can write X = SkN=1 Xk.
Consider any s, x ∈ X. If both s and x belong to the same Xk for some k, then we have
∣f (A(H, S),s) - f(A(H, S), x)| ≤ L||s - χ∣∣∞ ≤ Lλ for any algorithm A and any S ∈ Xm
due to the Lipschitz continuity of f, completing the proof.
Proof of Theorem 1: For any h ∈ H and dataset S, there exists an algorithm A that maps S to h,
i.e., h = A(H, S). Lemma2tells that A is (「Bnx λ-nx e, Lλ)-robust for any λ ∈ (0, B]. Theorem 9
implies ∣F(Pχ, h) - F(Pχ, h)| ≤ Lλ + √Cm PdBnx λ-nxe log4 - 2log δ with probability at least
1 - δ, for any constants δ ∈ (0, 1] and λ ∈ (0, B]. Since this bound holds true for any h ∈ H, we
conclude
一C ,、……… C K—：——τ----------------------------
hUHIF R J F (Px,h)1 ≤ Lλ+√m pdBnx λ-nx e log4 - 2log δ
15
Under review as a conference paper at ICLR 2022
The second statement is an application 2 exP(-0.5mα). Indeed,	of the firs	t one by taking λ = Bm-a/nx and δ
	Lλ + √C m	
suP |F (Px, h) - F (Pbx , h)|	≤ h∈H		二 pdBnx λ-nx e log 4 — 2 log δ
≤		 α LBm-nx	C	r~~	：	 +———y∕∖mcxo log4 — log4 + mα m
≤		 α LBm-nx	C	，~：	 +——j= mma log 4 + mα m
≤ ≤	α	α	/	-∣ _i nχ +2 LBm-nx + Cm-nx γ (1+ log4)m	+ nχ ɑ (LB + 2C )m-nx	
I n	-∣ I In X -1-2
The last inequality holds because -1 + nx+2 α ≤ 0 and hence m- + nχ α ≤ 1, completing the
nx
proof.
A.1 Deep neural networks that avoid the curse of dimensionality
Proof of Theorem 2: Denote h0(x) = x, h1 (x) = σ1 (W1h0(x)), ..., hK (x) = σK(WKhK-1 (x)).
By definition, the LiPschitz constant of a function g(h) is defined to be IlgkLip = SuP σ(Vg(h)),
khk2≤1
where σ(B) is the spectral norm of matrix B. For a linear function we have kW hkLip =
suP σ(W h) = kW kσ. Since σi is ρi-Lipschitz for any i, we have
khk2≤1
khK (x)kLip	≤	ρKkWKhK-1(x)kLip	(4)
≤	ρKkWKkσkhK-1(x)kLip	(5)
≤	ρKρK-1kWKkσkWK-1kσkhK-2(x)kLip	(6)
...	(7)
K
≤	Y ρkkWkkσ	(8)
k=1
K
≤	Y ρksk	(9)
k=1
which proves the first statement.
Next consider a neural network hA trained with Dropout (Srivastava et al., 2014). At each minibatch
t of the training phase, we randomly sample a thin sub-network of hW, compute the gradients g(t) (x)
given the minibatch data, and then update each weight matrice as
W(t) := Normalize(W(t))c where W(t) := W(t-1) — ηg(t)(x)	(10)
where Normalize(Wi(t))c is the normalization so that k W(t) ∣∣f ≤ Ci ≤ b for some tuning constant
ci and any i, and η is the learning rate.
(T)	(T)
After training (with T minibatchs), the network weights are scaled as Wi := qWi for any i,
where q is the drop rate. This implies that after training, we obtain a neural network hW,q with all
weight matrices satisfying kWi kF ≤ qbi . By using the same arguments as above, we have
16
Under review as a conference paper at ICLR 2022
K
khW,q(x)kLip ≤ YρkkWkkσ	(11)
k=1
K
≤ Y ρkkWkkF	(12)
k=1
K
≤ Y ρkqbk	(13)
k=1
where we have used the fact that kB kσ ≤ kB kF for any B, completing the proof.
Proof of Theorem 3: Let L be the Lipschitz constant of loss f (h, x) w.r.t x. The basic property
of Lipschitz functions and composition shows that L ≤ LfLh. For any h ∈ Hdr, we have Lh ≤
qK Cdr owing to Theorem 2. Hence L ≤ LfqKCdr ≤ LfCdrm-0.5.
Taking λ = B, Theorem 1 tells that
sup |F(Px, h)-F(Pχ, h)| ≤ LB+CP(log4 - 2logδ)∕m ≤ BLf Cdrm-0.5+Cp(log4 - 2log δ)∕m
h∈Hdr
Similar arguments can be used for family Hsn , completing the proof.
A.2 Consistency proof
Proof of Lemma 1: We have
|F(Pχ,ho) - F(Px,h*)∣
|F(Px, ho) - F(Pbx, ho) + F(Pbx, ho) - minF(Pbx, h) + minF(Pbx, h) - minF(Px, h)|
h∈H	h∈H	h∈H
|F(Px,ho) -F(Pbx,ho)|+|F(Pbx,ho) -
≤
≤
≤
minF(Pbx, h)| + | minF(Pbx, h) - minF(Px,(1h4))|
|F(Px, ho) - F(Pbx, ho)| + o + sup |F(Pbx, h) - F(Px, h)|	(15)
h∈H
o + 2 sup |F(Pbx, h) - F(Px, h)|
h∈H
where we have used Lemma 3 to derive (15) from (14).
B	Proofs of the main theorems for GANs
Proof of Corollary 3: Observe that
^ . . ...................................................................
SuP	|V (Pd, Pz ,D,G) - V (Pd, Pz ,D,G)∣	= SuP	|Ez 〜Pz ψ2(l - D(G(Z)))-
D∈D,G∈G	D∈D,G∈G
Ez〜P Ψ2(l - D(G(Z)))∣. Since Pz is an empirical version of Pz, applying Theorem 1 will provide
the generalization bounds for suPd∈d,g∈g |Ez〜Pzψ2(l - D(G(z))) - Ez〜P ψ2(l - D(G(Z)))|.
The same arguments can be done for SuPD∈D,G∈G |V(Pd, Pz, D, G) - V(Pd, Pz, D, G)|, complet-
ing the proof.
17
Under review as a conference paper at ICLR 2022
Proof of Theorem 6: Observe that
|V (Pbd, Pbz, D, G) -V(Pd,Pz,D,G)|
=IEx〜Pdψι(D(x))+ Ez〜Pzψ2(l- D(G(Z)))- Ex〜Pdi。v(D,G,x,z)∣	(16)
≤ IEz〜Pzψ2(1 - D(G(Z)))- Ez〜Pzψ2(1 - D(G(Z)))1 + ∣Ex〜PdΨι(D(x)) — Ex〜pdΨι(D(x))∣
Therefore
supG∈G,D∈D |V (Pbd, Pbz, D, G) - V (Pd, Pz, D, G)|
≤ SUp	IEz〜Pzψ2(1 - D(G(Z)))- Ez〜P ψ2(1- D(G(Z)))|
G∈G,D∈D	z
+ SUp	IEx〜PdΨι(D(x)) - Ex〜PdΨι(D(x))I	(17)
G∈G,D∈D	d
Theorem 1 shows that sup	|Ez〜Pzψ2(l - D(G(Z))) - Ez〜P ψ2(l - D(G(Z)))| ≤ Lλ +
G∈G,D∈D	~ z
C,(dBnλ-ne log4 - 2log δ)∕m, With probability at least 1 - δ, for any constants δ ∈ (0,1] and
λ ∈ (θ,Bz]. Similarly, we have sup	|Ex〜Pdψι(D(x)) - Ex〜P ψι(D(x))∣ ≤ LψLdλx +
G∈G,D∈D	d
C (dBxnx λx-nx e log 4 - 2 log δx)∕m, with probability at least 1 - δx, for any constants δx ∈ (0, 1]
and λx ∈ (0, Bx]. Combining these bounds with (17) and the union bound will lead to the first
statement of the theorem.
For the second and third statements, we choose λ = Bz , λx = Bx, δ = δx . Using the bounds for
the Lipschitz constant of D in Theorem 3 will complete the proof.
Proof of Theorem 7: By definition, (D0,G0) = argmmG∈g max°∈D V(Pd,Pz, D, G) and
(D*,G*) = argminG∈G max°∈D V(Pd,Pz,D,G).
Therefore
.,^ ^ ..
IV(Pd, Pbz, DO, GO) - V (Pd, Pz, D*, G*)|
, __ , ^ ^ _ 一 __ , _ _ _ _...
I min max V(Pbd, Pbz, D, G) - min max V(Pd, Pz, D, G)I
G∈G D∈D	z	G∈G D∈D	z
≤
≤
. _ _ , ^ ^ _ 一 _ _ , _...
max I max V(Pbd, Pbz, D, G) - max V(Pd, Pz, D, G)I
G∈G D∈D	z	D∈D	z
______________  ^ ^ _ 一	_ _ ,	_...
max max IV(Pbd,Pbz, D, G) - V(Pd,Pz, D, G)I
G∈G D∈D
(18)
(19)
(20)
≤ Lλ + CP(dBnλ-ne log 4 — 2 log δ)∕m + LψLdλx + C J( dBXx λx nx] log 4 — 2 log δx(21)
where we have used Lemma 3 to derive (20) from (19) and (21) from (20). The last inequality comes
from Theorem 6, completing the proof.

Lemma 3. Assume that h1 and h2 are continuous functions defined on a compact set Zx. Then
I max h1 (x) - max h2 (x)I ≤ max Ih1 (x) - h2(x)I
I min h1 (x) - min h2 (x)I ≤ max Ih1 (x) - h2(x)I
Proof: Denote x1O = arg maxx∈Zx h1(x), x2O = arg maxx∈Zx h2(x). It is easy to see that
h1(xO2) - h2(x2O) ≤ h1(xO1) - h2(x2O) ≤ h1(x1O) - h2(xO1)
Therefore
I max h1 (x) - max h2 (x)I = Ih1 (xO1) - h2 (x2O)I ≤ max Ih1(x) - h2 (x)I.
We can rewrite I minx∈Zx h1(x) - minx∈Zx h2(x)I	= I - maxx∈Zx (-h1(x)) +
maxx∈Zx(-h2(x))I ≤ maxx∈Zx Ih1(x) - h2(x)I, completing the proof.
18
Under review as a conference paper at ICLR 2022
C GANs and Autoencoders
C.1 Tightness of the bounds for GANs
Note that our bounds	in Theorems 6 and	8 in general	are	not tight in terms
of sample complexity	and dimensionality.	Taking λ	=	Bz m-1/(n+2) ,	δ =
2 exp(-0.5mn/(n+2)), λx = Bxm-1/(nx+2), δx = 2 exp(-0.5mnx /(nx +2)), Theorem 6 provides
supD∈D,G∈G |V (Pbd, Pbz, D, G) -V(Pd,Pz,D,G)| ≤ O(m-1/(n+2) + m-1/(nx+2)). This bound
O(m-1/(n+2) + m-1/(nx+2) ) surpasses the previous best bound O(m-1/(1.5n) + m-1/(1.5nx) ) in
the GAN literature (Husain et al., 2019).
C.2 Sample-efficient bounds for Autoencoders
Husain et al. (2019) did a great job at connecting GANs and Autoencoder models. They showed
that the generator objective in f -GAN (Nowozin et al., 2016) is upper bounded by the objective of
Wasserstein Autoencoders (WAE) (Tolstikhin et al., 2018). Under some suitable conditions, the two
objectives equal. They further showed the bound:
|maxV(Pd,Pz,D,G)-maxV(Pbd,Pbz,D,G)| ≤ O(m-1/sd +m-1/sg),
where Sd > d*(Pd) (the I-UPPer Wasserstein dimension of Pd) and Sg > d*(Pg). We show in
Appendix C.3 that sd > 1.5nx, sg > 1.5n even for a simple distribution, where nx is the dimen-
sionality of real data, and n is the dimensionality of latent codes. Therefore their boUnd becomes
O(m- 1.5nχ + m- 15n), which is significantly worse than our bound O(m-1/(n+2) + m-1/(nx+2)).
As a resUlt, oUr work Provides tighter generalization boUnds for both GANs and AUtoencoder mod-
els. More imPortantly, our results for DNNs with DroPout or sPectral norm translate directly to
Autoencoders, leading to significant tighter bounds.
C.3 How large is 1-upper Wasserstein dimension?
This Part Provides an examPle of why 1-uPPer Wasserstein dimension is not small. Before that we
need to take the following definitions from Husain et al. (2019).
Definition 3 (Covering number). For a set S ⊂ Rn, we denote Nη (S) be the η-covering number of
S, which is the smallest non-negative integer m such that there exists closed balls B1 , B2, ..., Bm of
radius η with S ⊆ im=1 Bi.
For any distribution P, the (η,τ)-dimension is dη (P,τ) := Iog-Nogp,τ), where Nn (P,τ):=
inf{Nη (S) : P(S) ≥ 1 - τ}.
Definition 4 (1-uPPer Wasserstein dimension). The 1-upper Wasserstein dimension of distribution
P is
s
d (P):= inf{s ∈ (2, ∞) : limsup dn(P, ηs-2) ≤ s}
η→0
Consider the simPle case of the unit Gaussian distribution P ≡ N (x; 0, I) defined in the n-
dimensional space Rn. We will show that the 1-upper Wasserstein dimension of P is d* (P) ≥ 1.5n.
s
First of all, we need to see the region S such that P(S) ≥ 1 - ηs-2. Since P is a Gaussian, the
Birnbaum-Raymond-Zuckerman inequality tells that Pr(∣∣x∣∣2 ≥ nη-s-2) ≤ ηs-2. It implies that
Pr(∣∣x∣∣2 ≤ nη-s-2) ≥ 1 - ηs-2. In other words, S is the following ball:
S = {x ∈ Rn : ∣∣x∣∣2 ≤ nη-s-2
As a consequence, we can lower bound the covering number of S as
Nη (S) ≥
/ q ~~- f
Vnn s-2
n
∖
n
n
2
'	-3s+4
nn s-2
19
Under review as a conference paper at ICLR 2022
By definition we have
/	、 n
Nn(P,ηs-2) =inf{N(S) : P(S) ≥ 1 - ηs-2} ≥ (nη-3-+4)2
Next we observe that
s
dn (P,η s-2)=
≥
≥
1
-log η
1
nn
-log n + -
- log η 2	2
-3s + 4λ
工ɪ POg η
n 3s - 4	n
2 Ly) - 2io访log n
(22)
(23)
(24)
Therefore
s
limsup dn (P,η s-2) ≥
η→0
n 3s - 4
2 (S - 2 )
(25)
s
The definition of d* (P) requires lim supn→0 dn (P, ηs-2) ≤ S and S ∈ (2, ∞). Those requirements
imply 2 (3S—4) ≤ s, and thus S ≥ 1(4 + 3n + P(4 + 3n)2 - 32n) > 3n. As a result, d* (P) >
1.5n.
D	Why does data augmentation impose a Lipschitz constraint?
In this section, we study a perturbed version of GANs to see the implicit role of data augmentation
(DA). Consider the following formulation:
minmaxEx〜PdEe[logD(X + e)] + Ez〜pzEe[log(1 - D(G(Z) + e))]	(26)
where = σu and u follows a distribution with mean 0 and covariance matrix I, σ is a non-negative
constant. Note that when u is the Gaussian noise, the formulation (26) turns out to be the noisy
version of GAN (Arjovsky & Bottou, 2017).
Noise penalizes the Jacobian norms: Adding noises to the discriminator inputs corresponds to mak-
ing a convolution to real and fake distributions (Roth et al., 2017; Arjovsky & Bottou, 2017). Let
pd*(x) = E[pd(x + )], pg* (x) = E[pg(x + )] be the density functions of the convoluted distri-
butions Pd*e, Pg*e, respectively. Werewrite Ve(D,G) = Ex〜Pd[EelogD(X + e)] + Ez〜pz[Eclog(1 -
D(G(z) + e))] = Ex〜Pd*」og D(x) + Ex〜Pg*e log(1 - D(χ)). Given a fixed G, the optimal discrimina-
tor is D*(x) = Pd pd*+Px) (χ) according to Arjovsky & Bottou (2017). Training G is to minimize
V(D*, G). By using the same argument as Goodfellow et al. (2014), one can see that training G is
equivalent to minimizing the Jensen-Shannon divergence dJS (Pd*, Pg*). Appendix D.2 shows
qdJ^JS (Pd*e,Pg ) ≤ q djs (Pd*e, Pg*e) + P。(。)	(27)
《djS (Pd, Pg*e) ≤ q djS (Pd*e, Pg*e) + P θ(σ)	(28)
where o(σ) satisfies lim o(σ- = 0. They suggest that for a fixed σ, minimizing djs(Pd*e,Pg*e)
σ→0 σ
implies minimizing both dJS(Pd, Pg*) and dJS(Pd*, Pg). The same behavior can be shown for
many other GANs.
Lemma 4. Let Jx(f) be the Jacobian of f(x) w.r.t its input x. Assume the density functions pd and
pg are differentiable everywhere in Zx. For any x ∈ Zx,
1.	[pd*(x) -pg(x)]2+ O(σ2)	=	[pd(x)	-pg(x)	+ o(σ)]2 + σ2Eu	uTJxT(pd)Jx(pd)u.
2.	[pd(x) - pg*(x)]2+ O(σ2)	=	[pd(x)	-pg(x)	- o(σ)]2 + σ2Eu	uTJxT(pg)Jx(pg)u.
3.	[pd*(x)-pg*(x)]2 +O(σ2) = [pd(x)-pg(x)+o(σ)]2 +σ2Eu uT JxT (pd - pg)Jx(pd -pg)u .
Lemma 5. If U 〜N (0, I) then Eu [uτ AT Au] = traCe(AT A) = ∣∣A∣∣F for any given matrix A.
20
Under review as a conference paper at ICLR 2022
The proof of Lemma 4 appears in Appendix D.3, while Lemma 5 comes from (Avron & Toledo,
2011; Hutchinson, 1989). Lemmas 4 and 5 are really helpful to interpret some nontrivial implica-
tions.
When training G, we are trying to minimize the expected norms of the Jacobians of the densities
induced by D and G. Indeed, training G will minimize djs(pʤ PgS, and thus also minimize
djs(Pd, PgS and djs(Pd*e,Pg) according to (27) and (28). Because √djs is a proper distance,
minimizing djs(Pd, Pg*e) leads to minimizing Ex〜pd [(pd(x) - Pg*e(x))2]. Combining this obser-
vation with Lemma 4, We find that training G requires both Ex〜pd [(pd(x) - Pg(x) + o(σ))2] and
Ex〜PdEu[uT JT(pg)Jx(Pg)u] to be small. As a result, Ex〜pg [|| Jx(pg)∣∣F] should be small due to
Lemma 5. A larger σ encourages a smaller Jacobian norm, meaning the flatter learnt distribution. A
small σ enables us to learn complex distributions. The optimal D*(x) = Pd (xd+x) ⑺ suggests
that a penalty on Jx(pg) will lead to a penalty on the Jacobian of D.
It is also useful to observe that adding noises to real data (x) only will require djs(Pd*e,Pg) to
be small, whereas adding noises to fake data (G(z)) only will require djs(Pd, PgQ to be small.
Lemma 4 suggests that adding noises to real data only does not make any penalty on pg . Further, if
noises are used for both real and fake data, we are making penalties on both Jx(pg) and Jx(pd -pg).
Note that a small Jx (pd - pg) implies Jx (pd) u Jx (pg). As a consequence, training GAN by the
loss (26) will require both the zero-order (pg) and first-order (Jx (pg)) informations of the fake
distribution to match those of the real distribution. This is surprising. The (implicit) appearance of
the first-order information ofpd can help the GAN training to converge faster, due to the ability to
use more information from pd . On the other hand, the use of noise in (26) penalizes the first-order
information of the loss, and hence can improve the generalization of D and G, following Theorem
8.
Connection to data augmentation: Note that each input for D in (26) is perturbed by an . When
has a small norm, each x0 = x + is a local neighbor of x. Noise is a common way to make
perturbation and can lead to stability for GANs (Arjovsky & Bottou, 2017). Another way to make
perturbation is data augmentation, including translation, cutout, rotation. The main idea is to make
another version x0 from an original image x such that x0 should preserve some semantics of x. By
this way, x0 belongs to the neighborhood of x in some senses, and can be represented as x0 = x +
for some . Those observations suggest that when training D and G from a set of original and
augmented images (Zhao et al., 2020a;b), we are working with an empirical version of (26). Note
that our proofs for inequalities (27, 28) and Lemma 4 apply to a larger contexts than Gaussian noise,
meaning that they can apply to different kinds of data augmentation.
Some recent works (Karras et al., 2020a; Tran et al., 2021) show that data augmentation (DA) for
real data only will be problematic, meanwhile using DA for both real and fake data can significantly
improve GANs (Zhao et al., 2020a;b; Karras et al., 2020a). Lemma 4 agrees with those observations:
DA for fake data only poses a penalty on Jacobian of pg only, while DA for real data only does no
penalty on pg . Differrent from prior works, Lemma 4 shows that DA for both real and fake data
poses a penalty on Jx(pg) and requires Jx(pg) u Jx(pd). In other words, DA requires the zero- and
first-order informations of pg to match those of pd, while also penalizes the first-order information
of the loss for better generalization of D and G. This is surprising.
Appendix E presents our simulation study. The results show that both DA and Gaussian noise can
penalize the Jacobians of D, G and the loss, hence confirming the above theoretical analysis.
D.1 Local linearity
Consider a function f : Rn → R which is differentiable everywhere in its domain. f is also called
locally linear everywhere. Let = σu, where u follows a distribution with mean 0 and covariance
matrix I, σ ≥ 0, Jx(f) be the Jacobian off w.r.t its input x. Considering f(x+) = f(x+σu) as a
function ofσ, Taylor’s theorem allows us to write f(x +σu) = f(x) + σJx(f)u + o(σ). Therefore,
Ee[f(x + )] = Ee [f (x) + σJx(f)u + o(σ)]	(29)
= f (x) + o(σ) + σEu [Jx (f)u]	(30)
= f(x) + o(σ),	(31)
21
Under review as a conference paper at ICLR 2022
where we have used Eu[Jx(f)u] = 0 due to Eu [u] = 0 and the independence of the elements of u.
As σ → 0, we have E [f (x + )] → f(x).
D.2 Proofs for inequalities (5, 6)
Consider the Jensen-Shannon divergence djs(pʤ PgS. Since √djs is a proper distance, We
have the following triangle inqualities:
JdJS (PdgPg ) ≤ JdJS (PdgPgS + JdJS (Pg-Pg )	(32)
q djs (Pd,Pg*e) ≤ J djs (Pd*e,Pg*e) + P djS (Pd*e, Pd )	(33)
Next we will show that djs(Pg^e, Pg) = o(σ). The following expression comes from a basic
property of Jensen-Shannon divergence:
djS (Pgg Pg ) = H ( P"； Pg ) — 1H (Pg*e )- | H (Pg ),	(34)
where H(P) denotes the entropy of distribution P.
Denote o(∙), oι(∙), 02(∙), 03(∙) be some functions of σ satisfying lim o(σ) = 0. Appendix D.1
σ→0
suggests thatPg^e(x) = Ee[pg(x+e)] = Pg(x)+01(σ) andlog(pg(x)+01(σ)) =log(pg(x))+02(σ)
by using Taylor’s theorem for σ. Therefore:
- 2 H(Pg)
2 / Pg (x) logPg (x)dx.
(35)
- 2 H (Pg*e)
2 /Pg*e(x)logPgmx)dx = 2 /Pg*e(x)log[Pg(x) + 01(σ)]dx
1 / Pg*e(x)[lθg Pg (x) + 02(σ)]dx
11
2	Pg*e(x)lθgPg (x)dx +$。2。).
H
(36)
Pg*e + Pg
2
-
-Z
-Z
-Z
Pg*e(x) + Pg (x)
2
Pg*e(x) + Pg (x)
2
Pg*e(x)+ Pg (x)
2
Pg*e(x) + Pg (x)
- 1/ [Pg*e (x)+ Pg
— 2 / [Pg*e (x)+ Pg
log
Pg*e(x)+ Pg (X) ) dx
log
2
2
+01 (σ)) dx
log 卜g(X) + 1 ol
logPg (x) + 203(
(x) log Pg (x)dx -
dx
dx
o3(σ)
/ [Pg*e(x) + Pg (x)]dx
(x)] logPg(x)dx 一 103(σ)
2
4
一1 /Pg*e(x)logPg(x)dx - 2 /Pg(x)logPg(x)dx - 203(σ)	(37)
From equations (34, 35, 36, 37) we can conclude djs(Pg*e, Pg) = o(σ). Similar arguments can be
done to prove djs(Pd*e, Pd) = o(σ). Combining those with (32) and (33), we arrive at
22
Under review as a conference paper at ICLR 2022
d/djs (Pd.e,Pg ) ≤ d<SjS (Pd.e,Pg,e) + PoF)	(38)
，djs(Pd,Pg*e) ≤ qdJS‰,Pg∑) + PoF).	(39)
D.3 Proof of Lemma 13
For any X ∈ Zx, it is worth remembering thatpd*e(x) = Ee[pd(x + e)]. Consider
Y = pd(x + ) -pg(x) = pd(x +σu) -pg(x),
which is a function of u. Since u follows a distribution with mean 0 and covariance which is the
identity matrix, Y is a random variable. Due topd(x+) = pd(x) +σJx (pd)u+o(σ) from Appendix
D.1, we can express the variance of Y as
Var(Y) = Eu(Y2)-(Eu(Y))2	(40)
= Eu h(pd(x + ) - pg(x))2i - [Eu(pd(x + ) -pg(x))]2	(41)
=Eu [[pd(x) -Pg(x) + o(σ) + σJx(pd)u]2] - [pd*e(x) -Pg(x)]2	(42)
= Eu (pd(x) -pg(x) + o(σ))2 + 2σ[pd(x) -pg(x) + o(σ)]Eu[Jx(pd)u]
+σ2Eu [uτJT(Pd)Jx(Pd)UI - [pd*e(x) -Pg(x)]2	(43)
=[Pd(X)- Pg(x) + o(σ)]2 + σ2Eu[uT JT(Pd)Jx(Pd)u] - ∣Pd*e(x) - Pg(x)]2 (44)
Since Pg(x) does not depend on = σu, we have Var(Y ) = Var(Pd(x + )) = Var(Pd(x + σu))
which is bounded above by CVar(σu) = Cσ2, for some C ≥ 0. Combining this with (44) will
result in
[Pd*e(x) - Pg(x)]2 + Cσ2 ≥ [Pd(x) - Pg(x) + o(σ)]2 + σ2Eu[uT JT(Pd)Jx(Pd)u]	(45)
completing the first statement. The second and third statements can be proven similarly.
E	Evaluation of data augmentation for GANs
There is a tradeoff in data augmentation. Making augmentation from a larger region around a given
image implies a larger σ. Lemmas 13 and 14 suggest that the Jacobian norms should be smaller,
meaning the flatter learnt distributions. Hence, too large region for augmentation may result in
underfitting. On the other hand, augmentation in a too small region (a small σ) allows the Jacobian
norms to be large, meaning the learnt distributions can be complex. As σ → 0, no regularization is
used at all.
This section will provide some empirical evidences about those analyses. We first evaluate the
role of σ when doing augmentation by simple techniques such as translation. We then evaluate the
case of augmentation by adding noises. Two models are used in our evaluations: Saturating GAN
(Goodfellow et al., 2014) and LSGAN (Mao et al., 2017).
E.1 Experimental setups
The architectures of G and D are specified in Figure 1, which follow http://github.com/
eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.
py. We use this architecture with Spectral normalization (Miyato et al., 2018) for D in all
experiments of GAN and LSGAN. Note that, for LSGAN, we remove the last Sigmoid layer in D .
We use MNIST dataset which has 60000 images for training and 5000 images for testing. During the
testing phase, 5000 new noises are sampled randomly at every epoch/minibatch to compute some
metrics. For the derivative of D with respect to its input, the input includes 2500 fake images and
2500 real images. Before fetching into D, both real and fake images are converted to tensor size
(1, 28, 28), rescaled to (0, 1) and normalized with mean = 0.5 and std = 0.5. The noise input of
G has 100 dimensions and is sampled from normal distribution N (0, I). We use Adam optimizer
with β1 = 0.5, β2 = 0.999, lr = 0.0002, batchsize = 64.
23
Under review as a conference paper at ICLR 2022
Generator Network
n=128	n=256
n=512	n=1024	n=784
φs°N ~nd-
,leeu-l
,leeu-l
2E.IONqoaω
,leeu-l
2E.IONqoaω
,leeu-l
2E.IONqoaω
,leeu-l
ɪUel
Discriminator Network
Fake
Image
po5s
-eeun
nVHAXe2
-eeun
nVHAXe2
-eeun
usse
Figure 1: The architectures of G and D with the negative slope of LeakyRuLU is 0.2
GAN - IlW∕az∣∣>
GAN - ∣∣∂D∕∂x∣∣f
LSGAN - ∣∣∂V7∂z∣∣p
0	90	180	270
Epoch
0	90	180	270
Epoch
0	90	180	270
Epoch
=XWQ-
LSGAN - ∣∣9D∕9x∣∣r
90	180	270
Epoch
Figure 2: Some behaviors of GAN (first two subfigures) and LSGAN (last two subfigures) with
different σ for augmentation. Both 架 and ⅞Z are measured along the training process. ∣∣ ∙ ∣∣f
denotes the Frobenious norm.




E.2 THE ROLE OF σ FOR DATA AUGMENTATION
In this experiment, the input of D which includes real and fake images are augmented using transla-
tion. The shifts in horizontal and vertical axis are sampled from discrete uniform distribution within
interval [-s, s], where S = 2 corresponds to σ = √2, S = 4 corresponds to σ = √6.67, and S = 8
corresponds to σ = √24.
Jacobian norms of D and loss V : Figure 2 shows the results. It can be seen from the figure that the
higher σ provides smaller Frobenius norms of Jacobian of both D and V . Such behaviors appear in
both GAN and LSGAN, which is consistent with our theory.
Jacobian norms of G: To see the effect of data augmentation on G, we need to fix D when training
G. Therefore we did the following steps: (i) train both G and D for 100 epochs, (ii) then keeping
D fixed, We further train G to measure its Jacobian norm along the training progress. We chose
σ ∈ {√6.67, √14, √24} and augmented 64,96,128 times for each image respectively.
Figure 3	shows the results. We observe that a higher σ provides smaller Jacobian norm of G.
Interestingly, as the norm decreases as training G more, suggesting that G gets simpler.
24
Under review as a conference paper at ICLR 2022
GAN - ∣∣aG∕az∣∣r
35
Rr∕9--
250	500	750
Minibatch
.... σ = V6.67	---- σ = V14	---- σ = V24
Figure 3: Some behaviors of GAN and LSGAN with different σ for augmentation. ∂∂Z is measured
along the training process.
Gan - ∣∣au∕az∣M
GAN - HθD∕ax∣∣r
LSGAN - ∣∣aWθz∣∣P
0	160	320	4S0	0	160	320	4S0	0	160	320	4S0	0	160	320	4S0
Epoch	Epoch	Epoch	Epoch
.... σ= 0.01	---- σ= 0.1	---- σ = 0.5
LSGAN - ∣∣3D∕ax∣∣r
Figure 4:	Some behaviors of GAN (first two subfigures) and LSGAN (last two subfigures) when
augmenting images by adding noises. Both ∂∂X and ∂∂Z are measured along the training process.
E.3 Augmentation by adding noises
In this experiment, the input of D which includes real and fake images are augmented by adding
Gaussian noise N(0, σ). We choose σ ∈ {0.01, 0.1, 0.5} and augment {16, 64, 128} times for each
image respectively.
Jacobian norms of D and loss V : Figure 4 shows the results after 500 epochs. It can be seen from
the figure that the higher σ provides smaller Jacobian norms. This is consistent with our theoretical
analysis. In comparison with using translation, adding Gaussian noise makes the Jacobian norms in
both GAN and LSGAN more stable.
Jacobian norms of G: We did the same procedure as for the case of image translation to see how
large the norm of ∂G∕∂z is. We choose σ ∈ {0.5, 2,4}. Figure 5 show the results. The same
behaviour can be observed. Larger σ often leads to smaller norms. It is worth noting that the
Jacobian norm will be zero as σ is too large. In this case both D and G may be over-penalized.
Those empirical results support well our theory.
F	Evaluation of spectral normalization
This section presents an evaluation on the effect of Lipschitz constraint by using spectral normal-
ization (SN) (Miyato et al., 2018). We use Saturating GAN and LSGAN with four scenerios: no
penalty; SN for G only; SN for D only; SN for both D and G. The setting for our experiments
appears in subsection E.1.
The results appear in Figure 6. When no penalty is used, we observe that the gradients of the loss
tend to increase in magnitude while both D and G are hard to reach optimality. When SN is used
25
Under review as a conference paper at ICLR 2022
GAN - ðɑ/ðz k	LSGAN - ∂G∕∂z r
128 .
--ZW9--
0
250	500	750
Minibatch
60
40
20
0
250	500	750
Minibatch
....σ = 0.5	----- σ= 2	---- σ = 4
Figure 5:	Some behaviors of GAN and LSGAN when augmenting images by adding noises. 祟 is
measured along the training process.
o 3∞ e∞ 9∞
Epoch
-ze∕5∖d
o 3∞ e∞ 9∞
Epoch
SQ
o 3∞ e∞ 9∞
Epoch
(s9s
o 3∞ e∞ 900
Epoch
0	3∞ e∞ 900
Epoch
No penalty ------- Penalty on G
0	3∞	6∞	9∞
Epoch
4 2
--Ze/5、-
0	3∞	6∞	9∞
Epoch
SQ
-Penalty on D
0	3∞	6∞	9∞
Epoch
(s9s
-Penalty on G&D
0	3∞	6∞	900
Epoch
O 3∞	6∞	900
Epoch


No penalty ------ Penalty on G -------- Penalty on D ------ Penalty on G&D
Figure 6:	Some behaviors of Saturating GAN (top row) and LSGAN (bottom row) in different
situations. Vg is the loss for training the generator, and FID measures the quality of generated
images, the lower the better.
for G, it seems that G has been over-penalized since the gradient norms are almost zero, meaning
that G may be underfitting. This behavior appears in both GAN and LSGAN, and was also observed
before (Brock et al., 2019). The most sucessful case is the use of SN for D only. We observe that
both players seem to reach the Nash equilibrium. The gradient norms of the loss are relatively stable
and small in the course of training, while the quality of fake image (measured by FID) can be better
than the other cases. Furthermore both the loss Vg and k∂Vg /∂z kF of the generator are stable and
belong to small domains, suggesting that the use of SN for D can help us to penalize the zero- and
first-order informations of the loss.
Our experiments suggest three messages which agree well with our theory in Theorem 8. Firstly,
when no penalty is used, the Lipschitz constant of a hypothesis may be large in order to well fit
the training data. In this case the generalization may not be good. Secondly, we can get stuck at
underfitting if a penalty on Lipschitzness is overused. The reason is that a heavy penalty can result
in a small Lipschitz constant (thus simpler hypothesis), meanwhile a too simple hypothesis may
cause a large optimization error. Hence, the generalization is not good in this case. Thirdly, when
an appropriate penalty is used, we can obtain both a small Lipschitz constant and small optimization
error which lead to better generalization.
26
Under review as a conference paper at ICLR 2022
G	Further discussion
We have discussed both generalization and consistency in Section 3. We next provide some inter-
pretations from our theoretical results which may be helpful in practice.
• We may want to find an unknown (measurable) function η(x) based on a training set S
of size m.3 A popular way is to select a family H (e.g., an NN architecture) and then do
training on S to obtain a specific ho ∈ H. The quality of ho can be seen from different
levels (Bousquet et al., 2004):
Ei. Optimization error: erro(ho, hm) = F(Px, ho) 一 F(Pχ,hn) for comparing With
hm = arg mmh∈H F(Px, h) which is the best in H for the training data;
L Xt	J -	/ 7 ∖	IL ∕7~⅛7∖	T-l /	7 ∖ I .	.1 「CC	F
E2. Generalization gap: errg(ho) = |F(Px, ho) 一 F(Px, ho)| to see the difference be-
tween the empirical and expected losses of ho ;
E3. Consistency rate: err∕ho,h*) = F(Px,h0) — F(Px, h*) for comparing with the best
function h* = argminh∈H F (Px, h) in H;
E4. Bayes gap: errB(ho, η) = F(Px, ho) 一 F(Px, η) for comparing with the truth.
•	A small optimization error may not always lead to good generalization.
•	A small generalization gap is insufficient to explain a high success in practice. Indeed,
errg (ho) can be small although both empirical and expected losses are high. The use of
this quantity poses a long debate (Nagarajan & Kolter, 2019; Negrea et al., 2020).
•	From Theorem 1, one may try to penalize the Lipschitz constant of the loss as small as pos-
sible to ensure a small generalization gap. However, as explained before, such a naive appli-
cation may not lead to good performance. The reason is that family H may be much smaller
and the members of H will have lower capacity as L decreases. Note that a large decrease
of capacity easily leads to underfitting, and hence F(Px, ho) will be high. Our experiments
in Appendix F provide a further evidence when spectral normalization is overused.
•	Those observations suggest that making only optimization or generalization gap small is
not enough. Both should be small, and so is consistency rate due to Lemma 1.
•	When does a small consistency rate still lead to bad generalization? In those bad cases, the
Bayes gap errB(ho, η) will be large. Note that errB (ho, η) = errc(ho, h*) + erra (H),
where erra (H) = F(Px, h*) 一 F(Px, η) is often known as the approximation error and
measures how well can functions in H approach the target (Bousquet et al., 2004). There-
fore erra(H) represents the capacity of family H. A stronger family with higher-capacity
members will lead to smaller F(Px, h*) and hence a smaller erra (H). Those observations
imply that, provided loss f is not a constant function, a bad generalization with a small
consistency rate happens only when H has low capacity.
•	When working with a high-capacity family H, a small consistency rate is sufficient to ensure
good generalization. Lemma 1 suggests that it is sufficient to ensure good generalization
by making both optimization error and generalization gap to be small.
•	For overparameterized NNs, we often observe small (even zero) optimization error. Our
results in Theorem 3 shows that Dropout and spectral normalization can produce small
generalization gap. By combining those observations, we can conclude that Dropout DNNs
and SN-DNNs can generalize well.
3For simplicity, we limit the discussion to measurable functions.
27