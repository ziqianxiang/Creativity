Under review as a conference paper at ICLR 2022
AI-SARAH: Adaptive and Implicit Stochastic
Recursive Gradient Methods
Anonymous authors
Paper under double-blind review
Ab stract
We present AI-SARAH, a practical variant of SARAH. As a variant of SARAH, this
algorithm employs the stochastic recursive gradient yet adjusts step-size based on
local geometry. AI-SARAH implicitly computes step-size and efficiently estimates
local Lipschitz smoothness of stochastic functions. It is fully adaptive, tune-free,
straightforward to implement, and computationally efficient. We provide technical
insight and intuitive illustrations on its design and convergence. We conduct
extensive empirical analysis and demonstrate its strong performance compared
with its classical counterparts and other state-of-the-art first-order methods in
solving convex machine learning problems.
1	Introduction
We consider the unconstrained finite-sum optimization problem
minw∈Rd [P(w) =f1 Pn=Ifi(w)].	(1)
This problem is prevalent in machine learning tasks where w corresponds to the model parameters,
fi (w) represents the loss on the training point i, and the goal is to minimize the average loss
P (w) across the training points. In machine learning applications, (1) is often considered the loss
function of Empirical Risk Minimization (ERM) problems. For instance, given a classification or
regression problem, fi can be defined as logistic regression or least square by (xi , yi ) where xi is
a feature representation and yi is a label. Throughout the paper, we assume that each function fi ,
i ∈ [n] = {1,…,n}, is smooth and convex, and there exists an optimal solution w* of (1).
1.1	Main Contributions
We propose AI-SARAH, a practical variant of stochastic recursive gradient methods (Nguyen et al.,
2017) to solve (1). This practical algorithm explores and adapts to local geometry. It is adaptive at
full scale yet requires zero effort of tuning hyper-parameters. The extensive numerical experiments
demonstrate that our tune-free and fully adaptive algorithm is capable of delivering a consistently
competitive performance on various datasets, when comparing with SARAH, SARAH+ and other
state-of-the-art first-order method, all equipped with fine-tuned hyper-parameters (which are selected
from ≈ 5, 000 runs for each problem). This work provides a foundation on studying adaptivity (of
stochastic recursive gradient methods) and demonstrates that a truly adaptive stochastic recursive
algorithm can be developed in practice.
1.2	Related Work
Stochastic gradient descent (SGD) (Robbins & Monro, 1951; Nemirovski & Yudin, 1983; Shalev-
Shwartz et al., 2007; Nemirovski et al., 2009; Gower et al., 2019) is the workhorse for training
supervised machine learning problems that have the generic form (1).
In its generic form, SGD defines the new iterate by subtracting a multiple of a stochastic gradient
g(wt ) from the current iterate wt . That is,
wt+1 = wt - αtg(wt).
In most algorithms, g(w) is an unbiased estimator of the gradient (i.e., a stochastic gradient),
E[g(w)] = VP(W), ∀w ∈ Rd. However, in several algorithms (including the ones from this paper),
1
Under review as a conference paper at ICLR 2022
g(w) could be a biased estimator, and convergence guarantees can still be well obtained.
Adaptive step-size selection. The main parameter to guarantee the convergence of SGD is the
step-size. In recent years, several ways of selecting the step-size have been proposed. For example,
an analysis of SGD with constant step-size (αt = α) or decreasing step-size has been proposed
in Moulines & Bach (2011); Ghadimi & Lan (2013); Needell et al. (2016); Nguyen et al. (2018);
Bottou et al. (2018); Gower et al. (2019; 2020b) under different assumptions on the properties of
problem (1).
More recently, adaptive / parameter-free methods (Duchi et al., 2011; Kingma & Ba, 2015; Bengio,
2015; Li & Orabona, 2018; Vaswani et al., 2019; Liu et al., 2019a; Ward et al., 2019; Loizou et al.,
2020) that adapt the step-size as the algorithms progress have become popular and are particularly
beneficial when training deep neural networks. Normally, in these algorithms, the step-size does not
depend on parameters that might be unknown in practical scenarios, like the smoothness parameter
or the strongly convex parameter.
Random vector g(wt) and variance reduced methods. One of the most remarkable algorithmic
breakthroughs in recent years was the development of variance-reduced stochastic gradient algorithms
for solving finite-sum optimization problems. These algorithms, by reducing the variance of the
stochastic gradients, are able to guarantee convergence to the exact solution of the optimization
problem with faster convergence than classical SGD. In the past decade, many efficient variance-
reduced methods have been proposed. Some popular examples of variance reduced algorithms are
SAG (Schmidt et al., 2017), SAGA (Defazio et al., 2014), SVRG (Johnson & Zhang, 2013) and SARAH
(Nguyen et al., 2017). For more examples of variance reduced methods, see Defazio (2016); Konecny
et al. (2016); Gower et al. (2020a); Khaled et al. (2020); HOrVgth et al. (2020).
Among the variance reduced methods, SARAH is of our interest in this work. Like the popular SVRG,
SARAH algorithm is composed of two nested loops. In each outer loop k ≥ 1, the gradient estimate
vo = VP(wk-ι) is set to be the full gradient. Subsequently, in the inner loop, at t ≥ 1, a biased
estimator vt is used and defined recursively as
vt = Vfi(wt) - Vfi(wt-1) + vt-1,	(2)
where i ∈ [n] is a random sample selected at t.
A common characteristic of the popular variance reduced methods is that the step-size α in their
update rule wt+1 = wt - αvt is constant (or diminishing with predetermined rules) and that depends
on the characteristics of problem (1). An exception to this rule are the variance reduced methods
with Barzilai-Borwein step size, named BB-SVRG and BB-SARAH proposed in Tan et al. (2016)
and Li & Giannakis (2019) respectively. These methods allow to use Barzilai-Borwein (BB) step
size rule to update the step-size once in every epoch; for more examples, see Li et al. (2020); Yang
et al. (2021). There are also methods proposing approach of using local Lipschitz smoothness to
derive an adaptive step-size (Liu et al., 2019b) with additional tunable parameters or leveraging BB
step-size with averaging schemes to automatically determine the inner loop size (Li et al., 2020).
However, these methods do not fully take advantage of the local geometry, and a truly adaptive
algorithm: adjusting step-size at every (inner) iteration and eliminating need of tuning any
hyper-parameters, is yet to be developed in the stochastic variance reduced framework. This
is exactly the main contribution of this work, as we mentioned in previous section.
2 Motivation
With our primary focus on the design of a stochastic recursive algorithm with adaptive step-size, we
discuss our motivation in this chapter.
A standard approach of tuning the step-size involves the painstaking grid search on a wide range
of candidates. While more sophisticated methods can design a tuning plan, they often struggle for
efficiency and/or require a considerable amount of computing resources.
More importantly, tuning step-size requires knowledge that is not readily available at a starting
point w0 ∈ Rd, and choices of step-size could be heavily influenced by the curvature provided
V2P(w0). What if a step-size has to be small due to a "sharp" curvature initially, which becomes
"flat" afterwards?
To see this is indeed the case for many machine learning problems, let us consider logistic regression
for a binary classification problem, i.e., fi(w) = log(1 + exp(-yixTW)) + 2∣∣w∣∣2, where Xi ∈ Rd
is a feature vector, yi ∈ {-1, +1} is a ground truth, and the ERM problem is in the form of (1). It is
2
Under review as a conference paper at ICLR 2022
(a)
(b)
(c)
(d)
Figure 1: AI-SARAH vs. SARAH: (a) evolution of the optimality gap P(W) — P and (b) the squared
norm of stochastic recursive gradient kvtk2; AI-SARAH: (c) evolution of the step-size, upper-bound,
local Lipschitz smoothness and (d) distribution of Si of stochastic functions. Note: in (a), P is a
lower bound of P (w*); in (c), the white spaces suggestfull gradient computations at outer iterations;
in (d), bars represent medians of si ’s.
easy to derive the local curvature of P (w), defined by its Hessian in the form
V1 2 *P(W) = nlP乙［i+XPP-⅛%2 XixT + λi.
、-----V----}
si(w)
(3)
Given that ,+。产 ≤ 0.25 for any a ≥ 0, one can immediately obtain the global bound on Hessian,
i.e. ∀w ∈ Rd We have V2P(W) W 1 n Pii=ι XixT + λI. Consequently, the parameter of global
Lipschitz smoothness is L = 4λmaχ(n Pin=ι XiXT) + λ. It is well known that, with a constant
step-size less than (or equal to) L, a convergence is guaranteed by many algorithms.
However, suppose the algorithm starts at a random w0 (or at 0 ∈ Rd), this bound can be very tight.
With more progress being made on approaching an optimal solution (or reducing the training error),
it is likely that, for many training samples, -yixiTwt 0. An immediate implication is that si(wt)
defined in (3) becomes smaller and hence the local curvature will be smaller as well. It suggests
that, although a large initial step-size could lead to divergence, with more progress made by the
algorithm, the parameter of local Lipschitz smoothness tends to be smaller and a larger step-size can
be used. That being said, such a dynamic step-size cannot be well defined in the beginning, and a
fully adaptive approach needs to be developed.
For illustration, we present the inspiring results of an experiment on real-sim dataset1 with
`2 -regularized logistic regression. Figures 1(a) and 1(b) compare the performance of classical
SARAH with AI-SARAH in terms of the evolution of the optimality gap and the squared norm of
recursive gradient. As is clear from the figure, AI-SARAH displays a significantly faster convergence
per effective pass2.
Now, let us discuss why this could happen. The distribution of si as shown in Figured 1(d) indicates
that: initially, all si’s are concentrated at 0.25; the median continues to reduce within a few effective
passes on the training samples; eventually, it stabilizes somewhere below 0.05. Correspondingly, as
presented in Figure 1(c), AI-SARAH starts with a conservative step-size dominated by the global
Lipschitz smoothness, i.e., 1∕λmaχ(V2P(wo)) (red dots); however, within 5 effective passes, the
moving average (magenta dash) and upper-bound (blue line) of the step-size start surpassing the red
dots, and eventually stablize above the conservative step-size.
For classical SARAH, we configure the algorithm with different values of the fixed step-size, i.e.,
{2-2, 2-1, ..., 24}, and notice that 25 leads to a divergence. On the other hand, AI-SARAH starts
with a small step-size, yet achieves a faster convergence per effective pass with an eventual (moving
average) step-size larger than 25 .
1The dataset is available at https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/
datasets/
2The effective pass is defined as a complete pass on the training dataset. Each data sample is selected once
per effective pass on average.
3
Under review as a conference paper at ICLR 2022
3 Algorithm
We present AI-SARAH in Algorithm 1. This algorithm implicitly computes αt-1 at any t ≥ 1 through
approximately solving the sub-problem, i.e., minα>0 ξt(α), and estimating the local Lipschitz
smoothness of a stochastic function; the upper-bound of step-size makes the algorithm stable, and
it is updated with exponential smoothing on harmonic mean (of approximate solutions to the sub-
problems), which also keep tracks of the local Lipschitz smoothness of a finite sum function, i.e.,
P (w).
This algorithm is fully adaptive and requires no efforts of tuning, and can be implemented
easily. Notice that β is treated as a smoothing factor in updating the upper-bound of the step-size,
and the default setting is β = 0.999. There exists one hyper-parameter in Algorithm 1, γ, which
defines the early stopping criterion on Line 8, and the default setting is Y = 击.We will show later
in this chapter that, the performance of this algorithm is not sensitive to the choices of γ, and this
is true regardless of the problems (i.e., regularized/non-regularized logistic regression and different
datasets.)
Algorithm 1 AI-SARAH
1	Parameter: 0 < γ < 1 (default 击),β = 0.999	
2	Initialize: Wo	
3	: Set: αmax = ∞	
4	: for k = 1, 2, ... do	
5	wo = Wk-1	
6	:	v0 = VP(W0)	
7	: t=1	
8	: while ∣vt∣2 ≥ γ∣vo∣2 do	
9	:	Select random mini-batch St from [n] uniformly with |St|	b
10	αt-i ≈ argminα>oξt(α)	
11	:	if k = 0 and t = 1 then	
12	δk =	1 t	αt-ι	
13	:	else	
14	δ = βδk-ι + (1- β) α⅛	
15	:	end if	
16	α = 1 αmax	δk	
17	αt-1 = min{αt-1, amaχ}	
18	:	Wt = Wt-1 - αt-1vt-1	
19	:	vt = VfSt(Wt) - VfSt(Wt-1) + vt-1	
20	:	t=t+1	
21	: end while	
22	Set Wk = wt.	
23	: end for	
3.1 Estimate Local Lipschitz Smoothness
In the previous chapter, we showed that AI-SARAH adapts to local Lipschitz smoothness and yields
a faster convergence than classical SARAH. Then, the question is how to estimate the parameter of
local Lipschitz smoothness in practice.
Can we use line-search? The standard approach to estimate local Lipschitz smoothness is to
use backtracking line-search. Recall SARAH’s update rule, i.e., wt = wt-1 - αt-1vt-1, where
vt-1 is a stochastic recursive gradient. The standard procedure is to apply line-search on function
fit (wt-1 - αvt-1). However, the main issue is that -vt-1 is not necessarily a descent direction.
AI-SARAH sub-problem. Define the sub-problem (as shown on line 10 of Algorithm 13) as
minξt(α) = min ∣∣Vfit (Wt-I- αvt-i) - Vfit(wt-i) + vt-ik2,	(4)
α>0	α>0
where t ≥ 1 denotes an inner iteration and it indexes a random sample selected at t. We argue that,
by (approximately) solving (4), we can have good estimate of the parameters of the local Lipschitz
3For sake of simplicity, we use fit instead of fSt .
4
Under review as a conference paper at ICLR 2022
smoothness.
To illustrate this setting, we denote Lti the parameter of local Lipschitz smoothness prescribed by
fit at wt-ι. Let Us focus on a simple quadratic function 九(W) = 1 (XTW - yit)2. Let a be the
optimal step-size along direction -vt-ι, i.e. α = argminα f^ (Wt-I - ɑvt-ι). Then, the closed
T
form solution of α can be easily derived as a = 'T-】it, whose value can be positive, negative,
bounded or unbounded.	t
On the other hand, one can compute the step-size implicitly by solving (4) and obtain αit-1, i.e.,
αit-1 = arg minα ξt(α). Then, we have
αt-1 = x⅛,
which is exactly 表 and recall Lt is the parameter of local Lipschitz smoothness of f跳.
Simply put, as quadratic function has a constant Hessian, solving (4) gives exactly *.For
general (strongly) convex functions, if V2fit (Wt-1), does not change too much locally, We can
still have a good estimate of Lti by solving (4) approximately.
Based on a good estimate of Lit, we can then obtain the estimate of the local Lipschitz smoothness of
P (Wt-1). And, that is
Lt = 1 Pn=ILt = n Pn=ι αi-ι
Clearly, if a step-size in the algorithm is selected as 1/Lt, then a harmonic mean of the sequence of the
step-size’s, computed for various component functions could serve as a good adaptive upper-bound
on the step-size computed in the algorithm. More details of intuition for the adaptive upper-bound
can be found in Appendix A.2.
3.2	Compute Step-size and Upper-bound
On Line 10 of Algorithm 1, the sub-problem is a one-dimensional minimization problem, which
can be approximately solved by Newton method. Specifically in Algorithm 1, we compute one-step
Newton at α = 0, and that is
αt-1
ξ (0)
∣ξ00(0)l.
(5)
Note that, for convex function in general, (5) gives an approximate solution; for functions in particular
forms such as quadratic ones, (5) gives an exact solution.
The procedure prescribed in (5) can be implemented very efficiently, and it does not require any
extra (stochastic) gradient computations if compared With classical SARAH. The only extra cost
per iteration is to perform two backward passes, i.e., one pass for ξt0(0) and the other for ξt00(0); see
Appendix A.2 for implementation details.
As shown on Lines 11-16 of Algorithm 1, αmax is updated at every inner iteration. Specifically, the
algorithm starts without an upper bound (i.e., αmaχ = ∞ on Line 3); as ɑt-ι being computed at
every t ≥ 1, we employs the exponential smoothing on the harmonic mean of {αt-ι} to update the
upper-bound. For k ≥ 0 and t ≥ 1, we define ɑmaχ = -1k, where
k = 0, t = 1
otherWise
and 0 < β < 1. We default β = 0.999 in Algorithm 1; see Appendix A.2 for details on the design of
the adaptive upper-bound.
3.3	CHOICE OFγ
We perform a sensitivity analysis on different choices of γ. Figures 2 shows the evolution of
the squared norm of full gradient, i.e., kVP (W)k2, for logistic regression on binary classification
problems; see extended results in Appendix A. It is clear that the performance of γ,s, where,
γ ∈ {1/8, 1/16, 1/32, 1/64}, is consistent with only marginal improvement by using a smaller value.
We default γ = 1/32 in Algorithm 1.
5
Under review as a conference paper at ICLR 2022
Figure 2: Evolution of ∣∣VP(w)k2 for Y ∈ {焉,击,门,8, 4, 2}: regularized (top row) and non-
regularized (bottom row) logistic regression on ijcnn1, rcv1, real-sim, news20 and covtype.
3.4	Convergence Analysis
In this section, we provide a convergence analysis of AI-SARAH (Algorithm 1) with a i) modified
line 10 to ɑt-1 ≈ argminα∈[ak. ,αk ] ξt(α), and ii) replacing the while loop with a for loop
t ∈ [m], where αkmin and αkmax are step-size bounds picked in each outer iteration and m be a
hyper-parameter.
For brevity, let us just hint that for many problems (e.g., the one mentioned in Chapter 2), as the
solution Wt is approaching w* the local curvature of P(W) is getting flatter and hence。大ɑ乂 could be
chosen as a fraction of reciprocal of a smoothness parameter of fi(w)’s over set
Wk := {w ∈ Rd | kwk-1 - wk ≤ m ∙ αmaχkv0k},	⑹
which can be much larger than the fraction of reciprocal of a global smoothness parameter. Let us
just remark that for k-th outer loop, all iterates Wt will stay inside of Wk and hence αkmax is well
defined. The parameter 0 < &大® ≤ @器a乂 can be chosen arbitrary (e.g., 0α也=Omax/2 or even
αkmin = αkmax). We defer the technical details and proofs in Appendix B and C. Let us just state the
main convergence theorem here.
Theorem 3.1. Suppose that the functions fi(W) are convex and smooth with parameter Lkmax over
Wk and P is μ-strongly convex. Let Us define
σk =	1	+ Omax αmaχLmax
m	μαm1in(m+1) + amɪin 2-α⅛ιaχLmax ,
and select m and αmk ax such that σmk < 1, ∀k ≥ 1. Then, modified Algorithm 1 converges as follows:
E[∣VP(Wk)k2] ≤ (n3σm) kVP(W0)k2.
Remark: SARAH algorithm is a special case of the modified Algorithm 1 when ∀k: αkmin = αkmax =
α ≤ 21L .
4 Numerical Experiment
In this chapter, we present the empirical study on the performance of AI-SARAH. For brevity, we
present a subset of experiments in the main paper, and defer the full experimental results and imple-
mentation details4 in Appendix A.
The problems we consider in the experiment are '2 -regularized logistic regression for binary classifi-
cation problems; see Appendix A for non-regularized case. Given a training sample (xi, yi) indexed
by i ∈ [n], the component function f is in the form fi(w) = log(1 + exp(-y%xTW)) + λ∣w∣2,
4Code will be made available upon publication.
6
Under review as a conference paper at ICLR 2022
Table 1: Summary of Datasets from Chang & Lin (2011).
Dataset	# features	n (# Train)	# Test	% Sparsity
ijcnn11	22	49,990	91,701	40.91
rcv11	47,236	20,242	677,399	99.85
real-sim2	20,958	54,231	18,078	99.76
news202	1,355,191	14,997	4,999	99.97
2 covtype2	54	435,759	145,253	77.88
1 dataset has default training/testing sanples.
2 dataset is randomly split by 75%-training & 25%-testing.
where λ = * for the '2 -regularized case and λ = 0 for the non-regularized case.
The datasets chosen for the experiments are ijcnn1, rcv1, real-sim, news20 and covtype. Table 1 shows
the basic statistics of the datasets. More details and additional datasets can be found in Appendix A.
We compare AI-SARAH with SARAH, SARAH+, SVRG (Johnson & Zhang, 2013), ADAM (Kingma
& Ba, 2015) and SGD with Momentum (Sutskever et al., 2013; Loizou & RiCht疝ik, 2020; 2017).
While AI-SARAH does not require hyper-parameter tuning, we fine-tune each of the other al-
gorithms, which yields ≈ 5, 000 runs in total for each dataset and case.
To be specific, we perform an extensive search on hyper-parameters: (1) ADAM and SGD with
Momentum (SGD w/m) are tuned with different values of the (initial) step-size and schedules to
reduce the step-size; (2) SARAH and SVRG are tuned with different values of the (constant) step-size
and inner loop size; (3) SARAH+ is tuned with different values of the (constant) step-size and early
stopping parameter. (See Appendix A for detailed tuning plan and the selected hyper-parameters.)
Figure 3 shows the average and total wall clock running time of AI-SARAH and the other algorithms.
While any individual run of AI-SARAH could be 2-5x more time consuming than the other algorithms,
its running time is negligible if comparing the total wall clock time. The reason is that AI-SARAH
does not require any tuning effort, but we have ≈ 5, 000 runs to fine-tune the other algorithms. Figure
4 shows the minimum ∣∣VP(w)k2 achieved at a few points of effective passes and wall clock time
horizon. It is clear that, AI-SARAH’s practical speed of convergence is faster than the other algorithms
in most cases. Here, we argue that, if given an optimal implementation of AI-SARAH (just as that of
ADAM and other built-in optimizer in Pytorch5 ), it is likely that our algorithm can be accelerated.
By selecting the fine-tuned hyper-parameters of all other algorithms, we compare them with AI-
SARAH and show the results in Figures 5-7. For these experiments, we use 10 distinct random seeds
to initialize w and generate stochastic mini-batches. And, we use the marked dashes to represent the
average and filled areas for 97% confidence intervals.
Figure 5 presents the evolution of ∣VP (w)∣2. Obviously from the figure, AI-SARAH exhibits the
strongest performance in terms of converging to a stationary point: by effective pass, the consis-
tently large gaps are displayed between AI-SARAH and the rest; by wall clock time, we notice that
AI-SARAH achieves the smallest ∣VP (w)∣2 at the same time point. This validates our design, that is
to leverage local Lipschitz smoothness and achieve a faster convergence than SARAH and SARAH+.
In terms of minimizing the finite-sum functions, Figure 6 shows that, by effective pass, AI-SARAH
consistently outperforms SARAH and SARAH+ on all of the datasets with a possible exception on
covtype dataset. By wall clock time, AI-SARAH yields a competitive performance on all of the
datasets, and it delivers a stronger performance on ijcnn1 and real-sim than SARAH.
For completeness of illustration on the performance, we show the testing accuracy in Figure 7. Clearly,
fine-tuned ADAM dominates the competition. However, AI-SARAH outperforms the other variance
reduced methods on most of the datasets from both effective pass and wall clock time perspectives,
and achieves the similar levels of accuracy as ADAM does on rcv1, real-sim and covtype datasets.
Having illustrated the strong performance of AI-SARAH, we continue the presentation by showing
the trajectories of the adaptive step-size and upper-bound in Figure 8.
This figure clearly shows that why AI-SARAH can achieve such a strong performance, especially on
the convergence to a stationary point. As mentioned in previous chapters, the adaptivity is driven by
the local Lipschitz smoothness. As shown in Figure 8, AI-SARAH starts with conservative step-size
and upper-bound, both of which continue to increase while the algorithm progresses towards a
stationary point. After a few effective passes, we observe: the step-size and upper-bound are stablized
5Please see https://pytorch.org/docs/stable/optim.html for Pytorch built-in optimizers.
7
Under review as a conference paper at ICLR 2022
AI-SARAH
SVRG
SGD w/m
ijcnnl
SARAH
Jill
Algorithm
SARAH+
hill
Algorithm
Adam
Jill
Algorithm
(U3S)}POD =BM<5SF
10*
10s
Algorithm
Figure 3: Average (top row) and total (bottom row) running time of AI-SARAH and other algorithms
for the regularized case.
---AI-SARAH
SARAH
SARAH+
SVRG
Adam
SGD w/m
ijcnnl	rcvl	real-sim
news20
covtype
10.0	29.0	64.0	10.0	42.0 SSO	10.0	45.0	103.0	10.0	66.0	147.0	10.0	245.0	957.0
Wall Clock (sec)	Wall Clock (sec)	Wall Clock (sec)	Wall Clock (sec)	Wall Clock (sec)
Figure 4: Running minimum per effective pass (top row) and wall clock time (bottom row) of
∣∣VP(w)k2 between other algorithms with all hyper-parameters configurations and AI-SARAH for the
regularized case. Note: the horizontal dashes in blue represent the minimum ∣∣VP(w)∣2 achieved
by AI-SARAH at certain effective pass or time point.
-∙- Al-SARAH	SARAH -A- SARAH+	SVRG	Adam -∙- SGD w/m
Z=3rA= Z=3rA=
ijcnnl
real-sim
news20
CoVtyPe
10^2-
10-4-
10^β-
10^*-
10^w-
0	25	50	75	100 125	0	20	40	60 so	0	25	∞	75	100	0	50	100	150	0 200 400 «0 800 IoOo
Wall Clock (sec)	Wall Clock (sec)	Wall Clock (sec)	Wall Clock (sec)	Wall Clock (sec)
Figure 5: Evolution of ∣∣VP(w)∣∣2 for the regularized case by effective pass (top row) and wall
clock time (bottom row).
8
Under review as a conference paper at ICLR 2022
-A- SARAH+	SVRG	Adam -∙- SGD w/m
-∙- AI-SARAH	SARAH
Figure 6: Evolution of P(w) for the regularized case by effective pass (top row) and wall clock time
(bottom row).
-∙- Al-SARAH	SARAH -A- SARAH+	SVRG	Adam -∙- SGD w/m
0.960-r
0.955 -
0.950 -
0.945-
rea Isim
news20
0.935 -
0.930 -
0.925-
0.920-
0.915-
0.910-
0.905-
0.97-
0.96-
0.95-
0.94-
0.93-
0.92-
0.91-
Figure 7: Running maximum of testing accuracy for the regularized case by effective pass (top row)
and wall clock time (bottom row).
ijcnnl	rcvl	rea l-sim	news20	covtype

0	5	10	15	20	0	10	20	30	0	5	10	15	20	0	10	20	34>	40	0	5	10	15	20
Effective Pass	Effective Pass	Effective Pass	Effective Pass	Effective Pass
Figure 8: Evolution of AI-SARAH’s step-size α and upper-bound αmax for the regularized case.
due to λ (and hence strong convexity). In Appendix A, we can see that, as a result of the function
being unregularized, the step-size and upper-bound could be continuously increasing due to the fact
that the function is likely non-strongly convex.
5 Conclusion
In this paper, we propose AI-SARAH, a practical variant of stochastic recursive gradient methods.
The idea of design is simple yet powerful: by taking advantage of local Lipschitz smoothness, the
step-size can be dynamically determined. With intuitive illustration and implementation details,
we show how AI-SARAH can efficiently estimate local Lipschitz smoothness and how it can be
easily implemented in practice. Our algorithm is tune-free and adaptive at full scale. With extensive
numerical experiment, we demonstrate that, without (tuning) any hyper-parameters, it delivers
a competitive performance compared with SARAH(+), ADAM and other first-order methods, all
equipped with fine-tuned hyper-parameters.
9
Under review as a conference paper at ICLR 2022
Ethics S tatement
This work presents a new algorithm for training machine learning models. We do not foresee any
ethical concerns. All datasets used in this work are from the public domain and are commonly used
benchmarks in ML papers.
Reproducibility S tatement
We uploaded all the codes used to make all the experiments presented in this paper. We have used
random seeds to ensure that one can start optimizing the ML models from the same initial starting
point as was used in the experiments. We have used only datasets that are in the public domain,
and one can download them from the following website https://www.csie.ntu.edu.tw/
~cjlin/libsvmtools/datasets/. After acceptance, we will include a link to the GitHub
repository where we will host the source codes.
10
Under review as a conference paper at ICLR 2022
References
Yoshua Bengio. Rmsprop and equilibrated adaptive learning rates for nonconvex optimization. corr
abs/1502.04390, 2015.
Leon Bottou, Frank E Curtis, and Jorge NocedaL Optimization methods for large-scale machine
learning. SIAMReview, 60(2):223-311, 2018.
Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM
transactions on intelligent systems and technology (TIST), 2(3):1-27, 2011.
A. Defazio. A simple practical accelerated method for finite sums. In NeurIPS, 2016.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives. In Advances in Neural Information
Processing Systems, volume 27, pp. 1646-1654. Curran Associates, Inc., 2014.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of machine learning research, 12(Jul):2121-2159, 2011.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic
programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Robert M Gower, Peter RichEik, and Francis Bach. Stochastic quasi-gradient methods: Variance
reduction via jacobian sketching. Mathematical Programming, pp. 1-58, 2020a.
Robert M Gower, Othmane Sebbouh, and Nicolas Loizou. Sgd for structured nonconvex functions:
Learning rates, minibatching and interpolation. arXiv preprint arXiv:2006.10311, 2020b.
Robert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Peter
Richtdrik. Sgd: General analysis and improved rates. In International Conference on Machine
Learning, pp. 5200-5209, 2019.
Samuel Horvdth, Lihua Lei, Peter Richtdrik, and Michael I. Jordan. Adaptivity of stochastic gradient
methods for nonconvex optimization. arXiv preprint arXiv:2002.05359, 2020.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in Neural Information Processing Systems, volume 26, pp. 315-323. Curran
Associates, Inc., 2013.
Ahmed Khaled, Othmane Sebbouh, Nicolas Loizou, Robert M. Gower, and Peter Richtdrik. Unified
analysis of stochastic gradient methods for composite convex and smooth optimization. arXiv
preprint arXiv:2006.11573, 2020.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
J. Konecny, J. Liu, P Richtdrik, and M. Takdc. Mini-batch semi-stochastic gradient descent in the
proximal setting. IEEE Journal of Selected Topics in Signal Processing, 10(2):242-255, 2016.
Bingcong Li and Georgios B Giannakis. Adaptive step sizes in variance reduction via regularization.
arXiv preprint arXiv:1910.06532, 2019.
Bingcong Li, Lingda Wang, and Georgios B. Giannakis. Almost tune-free variance reduction.
In Proceedings of the 37th International Conference on Machine Learning, volume 119, pp.
5969-5978. PMLR, 2020.
Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive
stepsizes. arXiv preprint arXiv:1805.08114, 2018.
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265,
2019a.
Yan Liu, Congying Han, and Tiande Huo. A class of stochastic variance reduced methods with an
adaptive stepsize. 2019b. URL http://www.optimization-online.org/DB_FILE/
2019/04/7170.pdf.
11
Under review as a conference paper at ICLR 2022
Nicolas Loizou and Peter Richtarik. Linearly convergent stochastic heavy ball method for minimizing
generalization error. arXiv preprint arXiv:1710.10737, 2017.
Nicolas Loizou and Peter Richtarik. Momentum and stochastic momentum for stochastic gradi-
ent, newton, proximal point and subspace descent methods. Computational Optimization and
Applications,77(3):653-710, 2020.
Nicolas Loizou, Sharan Vaswani, Issam Laradji, and Simon Lacoste-Julien. Stochastic polyak step-
size for sgd: An adaptive learning rate for fast convergence. arXiv preprint arXiv:2002.10542,
2020.
Eric Moulines and Francis R Bach. Non-asymptotic analysis of stochastic approximation algorithms
for machine learning. In Advances in Neural Information Processing Systems, pp. 451-459, 2011.
D. Needell, N. Srebro, and R. Ward. Stochastic gradient descent, weighted sampling, and the
randomized kaczmarz algorithm. Mathematical Programming, Series A, 155(1):549-573, 2016.
Arkadi Nemirovski and David B. Yudin. Problem complexity and method efficiency in optimization.
Wiley Interscience, 1983.
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic
approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574-
1609, 2009.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2003.
Lam Nguyen, Phuong Ha Nguyen, Marten van Dijk, Peter Richtarik, Katya Scheinberg, and Martin
Takac. SGDandhogwild! Convergence without the bounded gradients assumption. In Proceedings
of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pp. 3750-3758. PMLR, 2018.
Lam M. Nguyen, Jie Liu, Katya Scheinberg, and Martin Takdc. Sarah: A novel method for machine
learning problems using stochastic recursive gradient. In Proceedings of the 34th International Con-
ference on Machine Learning (ICML 2000), volume 70, pp. 2613-2621, International Convention
Centre, Sydney, Australia, 2017. PMLR.
H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statistics,
pp. 400-407, 1951.
M. Schmidt, N. Le Roux, and F. Bach. Minimizing finite sums with the stochastic average gradient.
Math. Program., 162(1-2):83-112, 2017.
Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. Pegasos: primal estimated subgradient
solver for SVM. In 24th International Conference on Machine Learning, pp. 807-814, 2007.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization
and momentum in deep learning. In International conference on machine learning, pp. 1139-1147.
PMLR, 2013.
Conghui Tan, Shiqian Ma, Yu-Hong Dai, and Yuqiu Qian. Barzilai-borwein step size for stochastic
gradient descent. In Proceedings of the 30th International Conference on Neural Information
Processing Systems, pp. 685-693, 2016.
Sharan Vaswani, Aaron Mishkin, Issam Laradji, Mark Schmidt, Gauthier Gidel, and Simon Lacoste-
Julien. Painless stochastic gradient: Interpolation, line-search, and convergence rates. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alch6-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural
Information Processing Systems, volume 32, pp. 3732-3745. Curran Associates, Inc., 2019.
Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp convergence over nonconvex
landscapes. In International Conference on Machine Learning, pp. 6677-6686, 2019.
Zhuang Yang, Zengping Chen, and Cheng Wang. Accelerating mini-batch sarah by step size rules.
Information Sciences, 2021. ISSN 0020-0255. doi: https://doi.org/10.1016/j.ins.2020.12.075.
12
Under review as a conference paper at ICLR 2022
Appendix
The Appendix is organized as follows. In Chapter A, we present extended details on the design,
implementation and results of our numerical experiments. In Chapter B, we present the theoretical
analysis of AI-SARAH. In Chapter C, we provide the basic definitions, some existing technical
preliminaries that are used in our results, and the proofs of the main lemmas and theorems from
Chapter B.
A	Extended details on Numerical Experiment
In this chapter, we present the extended details of the design, implementation and results of the
numerical experiments.
A. 1 Problem and Data
The machine learning tasks studied in the experiment are binary classification problems. As a
common practice in the empirical research of optimization algorithms, the LIBSVM datasets6 are
chosen to define the tasks. Specifically, we selected 10 popular binary class datasets: ijcnn1, rcv1,
news20, covtype, real-sim, a1a, gisette, w1a, w8a and mushrooms (see Table 2 for basic statistics
of the datasets).
Table 2: Summary of Datasets.
Dataset	d - 1 (# feature)	n (# Train)	ntest (# Test)	% Sparsity
ijcnn11	22	~^49,990^^	91,701	40.91
rcv11	47,236	^^20,242^^	677,399	99.85
news202	1,355,191	-^14997^^	4,999	99.97
covtype2	54	435,759	145,253	77.88
real-sim2	20,958	-^54231 ^^	18,078	99.76
a1a1	123	1,605^^	30,956	88.73
gisette1	5,000	~~6,000~~	1,000	0.85
w1a1	300	~^2,477^^	47,272	96.11
w8a1	300	-^49749^^	14,951	96.12
mushrooms2	112	6,093	2,031	81.25
1 dataset has default training/testing samples.
2 dataset is randomly split by 75%-training & 25%-testing.
A.1.1 Data Pre-Processing
Let (χi , yi) be a training (or testing) sample indexed by i ∈ [n] (or i ∈ [ntest]), where χi ∈ Rd-1
is a feature vector and yi is a label. We pre-processed the data such that χi is of a unit length in
Euclidean norm and yi ∈ {-1, +1}.
A.1.2 Model and Loss Function
The selected model, hi : Rd 7→ R, is in the linear form
hi(ω, ε) = χiTω + ε, ∀i ∈ [n],	(7)
where ω ∈ Rd-1 is a weight vector and ε ∈ R is a bias term.
For simplicity of notation, from now on, we let xi d=ef [χiT 1]T ∈ Rd be an augmented feature vector,
w d=ef [ωT ε]T ∈ Rd be a parameter vector, and hi(w) = xiTw for i ∈ [n].
6LIBSVM datasets are available at https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/
datasets/.
13
Under review as a conference paper at ICLR 2022
Given a training sample indexed by i ∈ [n], the loss function is defined as a logistic regression
fi(w) =log(1 + exp(-yihi(w)) + 2 ∣∣wk2.	(8)
In (8), 2 ∣∣w∣2 is the '2-regularization of a particular choice of λ > 0, where We used λ = 1 in the
experiment; for the non-regularized case, λ was set to 0. Accordingly, the finite-sum minimization
problem we aimed to solve is defined as
min
w∈Rd
{P(W)=f 1 X fi(w)].
i=1
(9)
Note that (9) is a convex function. For the '2-regularized case, i.e., λ = 1/n in (8), (9) is
μ-strongly convex and μ = 1. However, without the λ, i.e., λ = 0 in (8), (9) is μ-strongly convex if
and only if there there exists μ > 0 such that V2P(w)占 μI for W ∈ Rd (provided VP(W) ∈ C).
A.2 Algorithms
This section provides the implementation details7 of the algorithms, practical consideration, and
discussions.
A.2.1 TUNE-FREE AI-SARAH
In Chapter 3 of the main paper, we introduced AI-SARAH, a tune-free and fully adaptive algo-
rithm. The implementation of Algorithm 1 was quite straightforward, and we highlight the
implementation of Line 10 with details: for logistic regression, the one-dimensional (constrained
optimization) sub-problem minα>0 ξt (α) can be approximately solved by computing the Newton
step at α = 0, i.e., αt-ι = 一 ∣ξto(0)∣. This can be easily implemented with automatic differentiation in
Pytorch8, and only two additional backward passes w.r.t α is needed. For function in some particular
form, such as a linear least square loss function, an exact solution in closed form can be easily derived.
As mentioned in Chapter 3, we have an adaptive upper-bound, i.e., αmax, in the algorithm. To be
specific, the algorithm starts without an upper-bound, i.e., αmax = ∞ on Line 3 of Algorithm 1.
Then, αmax is updated per (inner) iteration. Recall in Chapter 3, αmax is computed as a harmonic
mean of the sequence, i.e., {&1}, and an exponential smoothing is applied on top of the simple
harmonic mean.
Having an upper-bound stabilizes the algorithm from stochastic optimization perspective. For
example, when the training error of the randomly selected mini-batch at Wt is drastically reduced
or approaching zero, the one-step Newton solution in (5) could be very large, i.e. αt-ι》0,
which could be too aggressive to other mini-batch and hence Problem (1) prescribed by the batch.
On the other hand, making the upper-bound adaptive allows the algorithm to adapt to the
local geometry and avoid restrictions on using a large step-size when the algorithm tries to make
aggressive progress with respect to Problem (1). With the adaptive upper-bound being derived by an
exponential smoothing of the harmonic mean, the step-size is determined by emphasizing the
current estimate of local geometry while taking into account the history of the estimates. The
exponential smoothing further stabilizes the algorithm by balancing the trade-off of being locally
focused (with respect to fSt ) and globally focused (with respect to P).
It is worthwhile to mention that Algorithm 1 does not require computing extra gradient of
fSt with respect to W if compared with SARAH and SARAH+. At each inner iteration, t ≥ 1,
Algorithm 1 computes VfSt (Wt-1 一 αvt-1) with α = 0 just as SARAH and SARAH+ would compute
VfSt (Wt-1), and the only difference is that α is specified as a variable in Pytorch. After the adaptive
step-size αt-1 is determined (Line 17), Algorithm 1 computes VfSt (Wt-1 一 αt-1vt-1) just as
SARAH and SARAH+ would compute VfSt (Wt).
In Chapter 3 of the main paper, we discussed the sensitivity of Algorithm 1 on the choice of γ .
Here, we present the full results (on 10 chosen datasets for both '2-regularized and non-regularized
7Code will be made available upon publication.
8Fordetailed description of the automatic differentiation engine in Pytorch, please see https://pytorch.
org/tutorials/beginner/blitz/autograd_tutorial.html.
14
Under review as a conference paper at ICLR 2022
0.70-
news20
0.65-
0.60-
0.55-
0.50-
0.45-
0.40-
0.35-
0.700-
covtype
0.675 -
0.650-
0.625-
0.600 -
IO-10 -
ɪo-12-
IO-4
10^5∙
U) Y.
10-,∙
U) T .
0.575 -
0.550-
0.525-
5	10	15	20
Effective Pass
5	10	15	20
Effective Pass
5	10	15	20
Effective Pass
Figure 9: '2-regularized case ijcnn1, rcv1, real-sim, news20 and covtype withY∈
{ 64, 32,16, 8, 4, 1 }: evolution of P (W) (top row) and ∣∣VP (w)∣2 (middle row) and running maxi-
mum of testing accuracy (bottom row).
5	10	15	20
Effective Pass
5	10	15	20
Effective Pass
0.85 -
0.80 -
0.75-
0.70-
0.65-
0.60 -
055-
0.5<J-
O 5	10	15	20	O 5	10	15	20	O 5	10	15	20	O 5	10	15	20	O 5	10	15	20
Effective Pass	Effective Pass	Effective Pass	Effective Pass	Effective Pass
Figure 10: '2-regularized case of a1a, gisette, w1a, w8a and mushrooms with Y ∈
{614, 312,16, 8, 4,1}: evolution of P (W) (top row) and ∣∣VP (w)k2 (middle row) and running maxi-
mum of testing accuracy (bottom row).
ZHA=
ʌɔsnuɔv
cases) in Figures 9, 10, 11, and 12. Note that, in this experiment, We chose Y ∈ {64, 32,1-6,81, 4,21},
and for each γ, dataset and case, we used 10 distinct random seeds and ran each experiment for 20
effective passes.
A.2.2 Other Algorithms
In our numerical experiment, we compared the performance of TUNE-FREE AI-SARAH (Algorithm
1) with that of 5 FINE-TUNED state-of-the-art (stochastic variance reduced or adaptive) first-order
15
Under review as a conference paper at ICLR 2022
Z--i⅛A--
10-10 -
0.7:
>V23VM≤
0	5 U) 15	20
Effective Pass
0	5	10	15	20
Effective Pass
5	10	15	20
Effective Pass
5	10	15	20	0
Effective Pass
5	10	15	20
Effective Pass
Figure 11:	Non-regularized case ijcnn1, rcv1, real-sim, news20 and covtype with γ ∈
{ 64, 32,16, 8, 4,1}: evolution of P (W) (top row) and ∣∣VP (w)∣2 (middle row) and running maxi-
mum of testing accuracy (bottom row).
0.70-
oæs-
0.60-
W 055
⅛ 050-
045-
OΛ1-
035-
ala	qisette	wla
w8a
X3sn84
1.0 ■
0.9-
o.a
0.7
o.e
0.5-
0.4-
0.3-
mushrooms
O 5	10	15	20 O 5	10	15	20 O 5 ɪθ 15	20 O 5 ɪθ 15	20 O 5	10	15	20
Effective Pass	Effective Pass	Effective Pass	Effective Pass	Effective Pass
Figure 12:	Non-regularized case a1a, gisette, w1a, w8a and mushrooms withγ∈
{ 64 , 32, ιi6,8, 4,1}: evolution of P (W) (top row) and ∣∣VP (w)k2 (middle row) and running maxi-
mum of testing accuracy (bottom row).
16
Under review as a conference paper at ICLR 2022
Table 3: Tuning Plan - Choice of Hyper-parameters.
Method	# Configuration	Step-Size	Schedule (%)1	Inner Loop Size (# Effective Pass)	Early Stopping (γ)
SARAH	160	{0.1, 0.2,..., 1}/L	n/a	{0.5, 0.6,..., 2}	n/a
SARAH+	50	{0.1, 0.2,..., 1}/L	n/a	n/a	1/{2, 4, 8, 16, 32}
SVRG	160	{0.1, 0.2,..., 1}/L	n/a	{0.5, 0.6,..., 2}	n/a
ADAM2	300	[10-3, 10]	{0,1,5,10,15厂	n/a	n/a
SGD w/m3	300	[10-3, 10]	{0,1,5,10,15}	n/a	n/a
1 Step-size is scheduled to decrease by X% every effective pass over the training samples.
2 β1 = 0.9, β2 = 0.999.
3 β = 0.9.
Table 4: Running Budget (# Effective Pass).
Dataset	Regularized	Non-regularized
ijcnn1	20	20
rcv1	30	40
news20	40	50
covtype	20	20
real-sim	20	30
a1a	30 二	40
gisette	30	40
w1a	40	50
w8a	30	40
mushrooms	30	40
methods: SARAH, SARAH+, SVRG, ADAM and SGD with Momentum (SGD w/m). These algorithms
were implemented in Pytorch, where ADAM and SGD w/m are built-in optimizers of Pytorch.
Hyper-parameter tuning. For ADAM and SGD w/m, we selected 60 different values of the (initial)
step-size on the interval [10-3, 10] and 5 different schedules to decrease the step-size after every
effective pass on the training samples; for SARAH and SVRG, we selected 10 different values of
the (constant) step-size and 16 different values of the inner loop size; for SARAH+, the values of
step-size were selected in the same way as that of SARAH and SVRG. In addition, we chose 5 different
values of the inner loop early stopping parameter. Table 3 presents the detailed tuning plan for these
algorithms.
Selection criteria:
We defined the best hyper-parameters as the ones yielding the minimum ending value of the loss
function, where the running budget is presented in Table 4. Specifically, the criteria are: (1) filtering
out the ones exhibited a "spike" of the loss function, i.e., the initial value of the loss function is
surpassed at any point within the budget; (2) selecting the ones achieved the minimum ending value
of the loss function.
Hightlights of the hyper-parameter search:
•	To take into account the randomness in the performance of these algorithms provided different
hyper-parameters, we ran each configuration with 5 distinct random seeds. The total number of
runs for each dataset and case is 4, 850.
•	Tables 5 and 6 present the best hyper-parameters selected from the candidates for the regularized
and non-regularized cases.
•	Figures 13, 14, 15 and 16 show the performance of different hyper-parameters for all tuned
algorithms; it is clearly that, the performance is highly dependent on the choices of hyper-
parameter for SARAH, SARAH+, and SVRG. And, the performance of ADAM and SGD
w/m are very SENSITIVE to the choices of hyper-parameter.
Global Lipschitz smoothness of P (w). Tuning the (constant) step-size of SARAH, SARAH+ and
SVRG requires the parameter of (global) Lipschitz smoothness of P (w), denoted the (global) Lipschitz
17
Under review as a conference paper at ICLR 2022
Table 5: Fine-tuned Hyper-parameters - '2-regularized Case.
Dataset	ADAM (α0, x%)	SGD w/m (α0, x%)	SARAH (α, m)	SARAH+ (α, γ)	SVRG (α, m)
ijcnn1	(0.07, 15%)	(0.4, 15%)	(3.153, 1015)	(3.503, 1/32)	(3.503, 1562)
rcv1	(0.016, 10%)	(4.857, 10%)	(3.924, 600)	(3.924, 1/32)	(3.924, 632)
news20	(0.028, 15%)	(6.142, 10%)	(3.786, 468)	(3.786, 1/32)	(3.786, 468)
covtype	(0.07, 15%)	(0.4, 15%)	(2.447, 13616)	(2.447, 1/32)	(2.447, 13616)
real-sim	(0.16, 15%)	(7.428, 15%)	(3.165,762)	(3.957, 1/32)	(3.957, 1694)
a1a	(0.7, 15%)	(4.214, 15%)	(2.758, 5O)=	(2.758, 1/32)	(2.758, 50)
gisette	(0.028, 15%)	(8.714, 10%)	(2.320, 186)	(2.320, 1/16)	(2.320, 186)
w1a	(0.1, 10%)	(3.571, 10%)	^^(3.646, 60)^^	(3.646, 1/32)	(3.646, 76)
w8a	(0.034, 15%)	(2.285, 15%)	(2.187, 543)	(3.645, 1/32)	(3.645, 1554)
mushrooms	(0.220, 15%)	(3.571, 0%)	(2.682,190) 一	(2.682, 1/32)	(2.682, 190)
Table 6: Fine-tuned Hyper-parameters - Non-regularized Case.
Dataset	ADAM (α0, x%)	SGD w/m (α0, x%)	SARAH (α, m)	SARAH+ (α, γ)	SVRG (α, m)
ijcnn1	(0.1, 15%)	(0.58, 15%)	(3.153, 1015)	(3.503, 1/32)	(3.503, 1562)
rcv1	(5.5, 10%)	(10.0, 0%)	(3.925, 632)	(3.925, 1/32)	(3.925, 632)
news20	(1.642, 10%)	(10.0, 0%)	(3.787, 468)	(3.787, 1/32)	(3.787, 468)
covtype	(0.16, 15%)	(2.2857, 15%)	(2.447, 13616)	(2.447, 1/32)	(2.447, 13616)
real-sim	(2.928, 15%)	(10.0, 0%)	(3.957, 1609)	(3.957, 1/16)	(3.957, 1694)
a1a	(1.642, 15%)	(6.785, 1%)	(2.763, 5O)=	(2.763, 1/32)	(2.763, 50)
gisette	(2.285, 1%)	(10.0, 0%)	(2.321, 186)	(2.321, 1/32)	(2.321, 186)
w1a	(8.714, 10%)	(10.0, 0%)	^^(3.652, 76)^^	(3.652, 1/32)	(3.652, 76)
w8a	(0.16, 10%)	(10.0, 5%)	(2.552, 543)	(3.645, 1/32)	(3.645, 1554)
mushrooms	(10.0, 0%)	(10.0, 0%)	(2.683, 190) 一	(2.683, 1/32)	(2.683, 190)
rcvl
15.0
12.5
10.0
7.5
5.0
2.5
0.0
real-sιm
14
12
10
-8
6
4
2
0
newszθ
70
€0
50
40
30
20
10
0
covtype
1.6
1.4
1.2
1.0
0.8
0.6
0.930
A 0.925
U
IO
⅛ 0.920
U
W 0915
0.910
0.905
LF
0.8	)
0∙7	♦
.
0.6
10^l
10-9
IO-5
10^7
0.95
0.90
0.85
0.80
0.75
0.70
0.925
0.900
0.875
0.850
0.825
0.800
0.775
WMagS
+is
HVWS
E¾αos
+is
HVWS
E¾αos
+is
HVWS
E¾αos
+is
HVWS
E¾αos
+is
HVwS
Figure 13:	Ending loss (top row), ending squared norm of full gradient (middle row), maximum
testing accuracy (bottom row) of different hyper-paramters and algorithms for the '2-regularized
case on ijcnn1, rcv1, real-sim, news20 and covtype datasets.
18
Under review as a conference paper at ICLR 2022
ala	gιsette	wla
w8a	mushrooms
⅛)d
Z=≥)d=
0.976
0.975
0.974
0.973
0.972
0.971
0.970
1.00
0.98
0.96
0.94
0.92
0.90
0.88
Ao.82
U
E
8 0®
LJ
<
0.78
0.76
C⅛sα8
SUAS
+HVHVS
HVtIVS
Figure 14:	Ending loss (top row), ending squared norm of full gradient (middle row), maximum
testing accuracy (bottom row) of different hyper-paramters and algorithms for the '2-regularized
case on a1a, gisette, w1a, w8a and mushrooms datasets.
real-sim	news20	covtype
ijcnnl
rcvl
0.8
δ^oβ
0.4
0.930
0.925
0.920
0.915
0.910
0905
-l^eɪɪʃ 一.，
T士
TnT
+ɪss
HWVS
+ɪss
HWVS

Figure 15:	Ending loss (top row), ending squared norm of full gradient (middle row), maximum
testing accuracy (bottom row) of different hyper-paramters and algorithms for the non-regularized
case on ijcnn1, rcv1, real-sim, news20 and covtype datasets.
19
Under review as a conference paper at ICLR 2022
ala	gιsette	wla	w8a	mushrooms
0.84
A 0.82
U
E
8 0®
⅛
0.78
0.76
0.985
0.980
0.975
0.970
1.00
0.98
0.96
0.94
0.92
0.90
0.88
WMQgS
9UΛS
+Hvuvs
HVWS
E⅜QOS
9UΛS
+Hvuvs
HVWS
E⅜QOS
9UΛS
+Hvuvs
HVWS
E⅜QOS
9UΛS
+Hvuvs
HVWS
E⅜QOS
9UΛS
+Hvuvs
HVWS
Figure 16:	Ending loss (top row), ending squared norm of full gradient (middle row), maximum
testing accuracy (bottom row) of different hyper-paramters and algorithms for the non-regularized
case on a1a, gisette, w1a, w8a and mushrooms datasets.
Table 7: Global Lipschitz Constant L
Dataset	Regularized	Non-regularized
ijcnn1	0.285408	0.285388
rcv1	0.254812	0.254763
news20	0.264119	0.264052
covtype	0.408527	0.408525
real-sim	0.252693	0.252675
a1a	0.362456 =	0.361833
gisette	0.430994	0.430827
w1a	0.274215	0.273811
w8a	0.274301^^	0.274281
mushrooms	0.372816	0.372652
constant L, and it can be computed as, given (8) and (9),
1	1n T
L = 4 λmax( 一)： Xixi ) + λ,
i=1
where λmaχ(A) denotes the largest eigenvalue of A and λ is the penalty term of the '2-regularization
in (8). Table 7 shows the values of L for the regularized and non-regularized cases on the chosen
datasets.
20
Under review as a conference paper at ICLR 2022
Z=sHm=
w8a	_ mushrooms
ala	„ gisette	wla
0	20 40 60 SO 18
Percentile
0 20 40 60 80 100
Percentile
0	20 40 60 80 100	0	20 « 60 80 100	0	20 « 60 80 100
Percentile	PerCentne	Percentile
Z=SHA=
UΓ"
Figure 17: Average ending ∣∣VP(w)∣∣2 for '2-regularized case - AI-SARAH vs. Other Algorithms:
AI-SARAH is shown as the horizontal lines; for each of the other algorithms, the average ending
∣VP (w)∣2 from different configurations of hyper-parameters are indexed from 0 percentile (the
worst choice) to 100 percentile (the best choice); see Section A.2.2 for details of the selection criteria.
Z=SHA=
工
O 20 40 60 80 IOO
Percentile
Figure 18: Average ending ∣∣VP(w)k2 for non-regulariZedcase- AI-SARAH vs. Other Algorithms.
A.3 Extended Results of Experiment
In Chapter 4, we compared tune-free & fully adaptive AI-SARAH (Algorithm 1) with fine-tuned
SARAH, SARAH+, SVRG, ADAM and SGD w/m. In this section, we present the extended results of
our empirical study on the performance of AI-SARAH.
Figures 17 and 18 compare the average ending ∣VP (w)∣2 achieved by AI-SARAH with the other
algorithms, configured with all candidate hyper-parameters.
It is clear that,
•	without tuning, AI-SARAH achieves the best convergence (to a stationary point) in practice on
most of the datasets for both cases;
•	while fine-tuned ADAM achieves a better result for the non-regularized case on a1a, gisette, w1a
and mushrooms, AI-SARAH outperforms ADAM for at least 80% (a1a), 55% (gisette), 50% (w1a),
and 50% (mushrooms) of all candidate hyper-parameters.
Figure 19 shows the results of the non-regularized case for ijcnn1, rcv1, real-sim, news20 and covtype
datasets. Figures 20 and 21 present the results of the '2 -regularized case and non-regularized case
respectively on a1a, gisette, w1a, w8a and mushrooms datasets. For completeness of presentation,
we present the evolution of AI-SARAH’s step-size and upper-bound on a1a, gisette, w1a, w8a and
mushrooms datasets in Figures 22 and 23. Consistent with the results shown in Chapter 4 of the main
paper, AI-SARAH delivers a competitive performance in practice.
21
Under review as a conference paper at ICLR 2022
ijcnnl
rea∣-s∣m
0.30
0.25
β2β
0.15
0.10
0.05 ∙
0.00
0	10	20	30
Effective Pass
rcvl
real-sim	news20	covtype
Z--(Msr>-
0	5	10	15 2β
Effective Pass
IoT
ιo-4
IoT
I(IY
IOT
IL
0	10 2β 30	«
Effective Pass
IL
10^3
IoT
IoT
I(IY
IOT
IOT
0	10 2β 30
Effective Pass
0	10	20	30 .W so
Effective Pass
0	5	10	15 2β
Effective Pass
ijcnnl
j--∙- AI-SARAH
SARAH
SARAH+
0.960 ∙
0.955 ∙
0.956
Q.945
0∙94<∣∙
Effective Pass
0.925-
0.S20-
0.915 -
0.910 -
0.905 -
'→- SVRG
-+- Adam
--∙- SGD w/m
Effective Pass
Figure 19: Non-regularized case: evolution ofP(W) (top row), ∣VP (W)∣2 (middle row), and running
maximum of testing accuracy (bottom row).
0.96-
0.94-
0.92-
0.90-
0.88-
0.86-
0.84-
0.82-
0.80-
Xoe-Inuuq
Figure 20: '2-regularized case: evolution of P (W) (top row), kVP (W) ∣∣2 (middle row), and running
maximum of testing accuracy (bottom row).
22
Under review as a conference paper at ICLR 2022
10^2-
10^4-
10^,-
Itre -
10-m-
10-n-
10^m -
Z=i⅛>-
IOT
IoT
IoT
IOT
IoT
O IO 2β 30	«
Effective Pass
O 10	20	30	40
Effective Pass
10	20	30	40 sβ
Effective Pass
Figure 21: Non-regularized case: evolution of P(W) (top row), kVP(W) ∣∣2 (middle row), and running
maximum of testing accuracy (bottom row).
0.985 ∙
0.980 ∙
0.975 ■
0.975
0.965 ∙
0.960
0.955 ■
O 10 2β 30	«
Effective Pass
O 10	20	30	«
Effective Pass
ala___________ gisette WIa
Effective Pass
w8a
Figure 22: '2-regularized case: evolution of AI-SARAH's step-size α and upper-bound amaχ.
mushrooms
wla	w8a	mushrooms
Figure 23: Non-regularized case: evolution of AI-SARAH’s step-size α and upper-bound αmax.
23
Under review as a conference paper at ICLR 2022
B Theoretical Analysis
In this chapter, we provide a convergence analysis of AI-SARAH (Algorithm 1) with a i) modified
line 10 to ɑt-ι ≈ argminα∈[ak. ,αk ] ξt(α), and ii) replacing the while loop with a for loop
t ∈ [m], where αkmin and αkmax are step-size bounds picked in each outer iteration and m be a
hyper-parameter.
B.1	AI-SARAH AND SARAH
Like SVRG and SARAH, AI-SARAH’s loop structure of this algorithm is divided into the outer loop,
where a full gradient is computed, and the inner loop, where only stochastic gradient is computed.
However, unlike SVRG and SARAH, the step-size is computed implicitly. In particular, at each
iteration t ∈ [m] of the inner loop, the step-size is chosen by approximately solving a simple one-
dimensional constrained optimization problem. Define the (modified) sub-problem (optimization
problem) at t ≥ 1 as
min	ξt(α),	(10)
α∈[αkmin,αkmax]
where ξt(α) := ∣∣vtk2 = ∣∣Vfst (wt-ι - avt-i)-VfSt (wt-ι) + vt-ιk2,。大皿 and αk1 aχ are lower-
bound and upper-bound of the step-size respectively. These bounds do not allow large fluctuations of
the (adaptive) step-size. We denote αt-1 the approximate solution of (10). Now, let us present some
remarks regarding AI-SARAH.
Remark B.1. As we will explain with more details in the following subsections, the values of αkmin
and αkmax cannot be arbitrarily large. To guarantee convergence, we will need to assume that
aAax ≤ L2aχ, where Lmax = maxi∈[n] Lk. Here, Lk is the local smoothness parameter of f
k
defined on a working-set for each outer loop (see Definition B.6).
Remark B.2. SARAH Nguyen et al. (2017) can be seen as a special case of AI-SARAH, where
αkmin = αkmax = α for all outer loops (k ≥ 1). In other words, a constant step-size is chosen for the
algorithm. However, if αkmin < αkmax, then the selection of the step-size in AI-SARAH allows a faster
convergence of ∣vt∣2 than SARAH in each inner loop.
Remark B.3. At t ≥ 1, let us select a mini-batch of size n, i.e., |St| = n. In this case, AI-SARAH is
equivalent to deterministic gradient descent with a very particular way of selecting the step-size, i.e.
by solving the following problem
min	ξt (α),
α∈[αkmin,αkmax]
where ξt(α) = ∣VP (wt-1 - αVP(wt-1)) ∣2. In other words, the step-size is selected to minimize
the squared norm of the full gradient with respect to wt.
B.2	Definitions / Assumptions
First, we present the main definitions and assumptions that are used in our convergence analysis.
Definition B.4. Function f : Rd → R is L-smooth if: f(x) ≤ f (y) +〈Vf (y), X — yi+ L∣∣x 一
y ∣2 , ∀x, y ∈ Rd,
and it is LC -smooth if:
f(x) ≤ f(y) + hVf(y),x — yi + LC∣∣X — y∣2,∀χ,y ∈ C.
Definition B.5. Function f : Rd → R is μ-strongly convex if: f(x) ≥ f (y) +〈Vf (y), X — y +
2 ∣∣x — y∣2, ∀x,y ∈ Rd. If μ = 0 thenfunction f is a (non-strongly) convex function.
Having presented the two main definitions for the class of problems that we are interested in, let us
now present the working-set Wk which contains all iterates produced in the k-th outer loop of (the
modified) Algorithm 1.
Definition B.6 (Working-Set Wk). For any outer loop k ≥ 1 in (the modified) Algorithm 1, starting
at Wk-I we define
Wk := {w ∈ Rd ∣∣∣Wk-i 一 W∣∣≤ m ∙ amaxIWoll}.	(11)
24
Under review as a conference paper at ICLR 2022
Note that the working-set Wk can be seen as a ball of all vectors w’s, which are not further away
from Wk-ι than m ∙。大ɑ乂|忖0口. Here, recall that m is the total number of iterations of an inner loop,
αkmax is an upper bound of the step-size αt-1, ∀t ∈ [m], and kv0k is simply the norm of the full
gradient evaluated at the starting point Wk-I in the outer loop.
By combining Definition B.4 with the working-set Wk, we are now ready to provide the main
assumption used in our analysis.
Assumption 1. Functions fi, i ∈ [n], of problem (1) are LiW -smooth. Since we only focus on the
working-set Wk, we simply write Lik -smooth.
Let us denote Li the smoothness parameter of function fi, i ∈ [n], in the domain Rd. Then, it is
easy to see that Lik ≤ Li, ∀i ∈ [n]. In addition, under Assumption 1, it holds that function P is
Lk -smooth in the working-set Wk, where Lk = ɪ PN1 Lk.
As we will explain with more details in the next section for our theoretical results, we will assume
that αkmax ≤
Lmax , where Lkmax = maxi∈[n] Lik.
k
B.3	Convergence Guarantees
Now, we can derive the convergence rate of AI-SARAH. Here, we highlight that, all of our theoretical
results can be applied to SARAH. We also note that, some quantities involved in our results, such as
Lk and Lkmax, are dependent upon the working set Wk (defined for each outer loop k ≥ 1). Similar
to Nguyen et al. (2017), we start by presenting two important lemmas, serving as the foundation of
our theory.
The first lemma provides an upper bound on the quantity Pm=O E[kVP(Wt)k2]. Note that it does not
require any convexity assumption.
Lemma B.7. Fix a outer loop k ≥ 1 and consider Algorithm 1 with @大&乂 ≤ 1/Lk. Under
k
Assumption 1, PmL0 E[kVP(wt)k2] ≤ 后E[P(w°) - P(w*)] + * PmL0 E[kVP(wt) - vtk2].
αmin	αmin
The second lemma provides an informative bound on the quantity E[kVP (Wt) - vtk2]. Note that it
requires convexity of component functions fi, i ∈ [n].
Lemma B.8. Fix a outer loop k ≥ 1 and consider Algorithm 1 with αkmax < 2/Lkmax. Suppose fi is
convex for all i ∈ [n]. Then, under Assumption 1, for any t ≥ 1 :
E[kVP(wt) - vtk2] ≤ (⅛⅛χ) E[kvok2].
Equipped with the above lemmas, we can then present our main theorem and show the linear
convergence of (the modified) Algorithm 1 for solving strongly convex smooth problems.
Theorem B.9. Suppose that Assumption 1 holds and P is strongly convex with convex component
functions fi, i ∈ [n]. Let us define
σk =	1	+ αmαx amaxLmax
m μαm1in(m+1) + Omin 2-amaxLmax,
and select m and αkmax such that σmk < 1, ∀k ≥ 1. Then, Algorithm 1 converges as follows:
E[kVP(Wk)k2] ≤ (Q3σm) kVP(W0)k2.
As a corollary of our main theorem, it is easy to see that we can also obtain the convergence of
SARAH Nguyen et al. (2017). Recall, from Remark B.2, that SARAH can be seen as a special case of
AI-SARAH if, for all outer loops, αkmin = αkmax = α. In this case, we can have
σk =	J . + - 0Lmax
σm	μα(m+1) + 2-αLmax .
If we further assume that all functions fi, i ∈ [n], are L-smooth and do not take advantage of the
local smoothness (in other words, do not use the working-set Wk), then Lkmax = L for all k ≥ 1.
Then, with these restrictions, we have
σ~	σk	---1ς--T + —αL— < 1
σm	σm	μa(m+1) + 2-aL < 1.
As a result, Theorem B.9 guarantees the following linear convergence: E[∣∣VP(Wk)k2] ≤
(σm)k∣∣VP(Wo)k2, which is exactly the convergence of classical SARAH provided in Nguyen et al.
(2017).
25
Under review as a conference paper at ICLR 2022
C	Technical Preliminaries & Proofs of Main Results
Let us start by presenting some important technical lemmas that will be later used for our main proofs.
C.1 Technical Preliminaries
Lemma C.1. Nesterov (2003) Suppose that function f is convex and L-Smooth in C ⊆ Rn. Then
for any w, w0 ∈ C :
(Vf(W)- Vf(w0), (w - w0)i ≥ 1 kVf(w) - Vf(w0)k2.	(12)
L
Lemma C.2. Let Assumption 1 hold for all functions fi of problem (1). That is, let us assume that
function f is Lk-smooth ∀i ∈ [n]. Then, function P(W) d=f n1 PZi fi(w) is Lk-smooth, where
几—1 Pn Ti
Lk = n 乙i=1 Lk .
Proof. For each function fi, we have by definition of Lik-local smoothness,
fi(x) ≤ fi(y) + hvfi(y),χ — yi + Lkx - yk2,∀x,y ∈ Wk.
Summing through all i0s and dividing by n, we get
P(x) ≤ P(y) + (VP(y),x — yi + LkIlx — yk2,∀x,y ∈ Wk.
□
The next Lemma was first proposed in Nguyen et al. (2017). We add it here with its proof for
completeness and will use it later for our main theoretical result.
Lemma C.3. Nguyen et al. (2017) Consider vt defined in (2). Then for any t ≥ 1in Algorithm 1, it
holds that:
tt
E[kVP(Wt)-vtk2]=XE[kvj-vj-ik2]-XE[kVP(Wj)-VP(Wj-i)k2].	(13)
j=i	j=i
Proof. Let Ej denote the expectation by conditioning on the information W0, Wi, . . . , Wj as well as
v0, vi, . . . , vj-i. Then,
Ej [kVP (Wj) -vjk2] =Ej k (VP(Wj-i) -vj-i) + (VP (Wj) - VP(Wj-i)) - (vj -vj-i)k2
= Ej[kVP (Wj-i) - vj-ik2] + Ej[kVP (Wj) - VP (Wj-i)k2] + Ej[kvj - vj-ik2]
+ 2 (VP(Wj-i) - vj-i)T (VP(Wj) - VP(Wj-i))
-	2 (VP(Wj-i) - vj-i)T Ej[vj - vj-i]
-	2 (VP (Wj) - VP(Wj-i))T Ej [vj - vj-i]
=	Ej[kVP (Wj-i) - vj-ik2] - Ej[kVP (Wj) - VP (Wj-i)k2] + Ej[kvj - vj-ik2],
where the last equality follows from
Ej [vj - vj-i] = Ej [Vfij (Wj) - Vfij (Wj-i)] = VP(Wj) - VP(Wj-i).
By taking expectation in the above expression, using the tower property, and summing over j =
1, ..., t, we obtain
tt
E[kVP(Wt)-vtk2]=XE[kvj-vj-ik2]-XE[kVP(Wj)-VP(Wj-i)k2].
j=i	j=i
□
26
Under review as a conference paper at ICLR 2022
C.2 Proofs of Lemmas and Theorems
For simplicity of notation, we use |S | = 1 in the following proofs, and a generalization to |S | > 1 is
straightforward.
C.2.1 Proof of Lemma B.7
By Assumption 1, Lemma C.2 and the update rule wt = wt-1 - αt-1vt-1 of Algorithm 1, we obtain:
L
P(Wt) ≤ P(Wt-I)- αt-1hVP(Wt-I),vt-ii +~ ~2αt-ιkvt-1k2
= P(Wt-I) - t--ιk-l∣VP(Wt-I)k2 + ^t-I∣∣VP(Wt-I) - vt-ιk2 - (^t-I - -Ikα2-ι) ∣∣vt-ι∣∣2,
where, in the equality above, We use the fact that(a, b)= ɪ(Ilak2 + ∣∣b∣∣2 — ∣∣a 一 b∣2).
By rearranging and using the lower and upper bounds of the step-size αt-- in the outer loop k
(αkmin ≤ αt-- ≤ αkmax), we get:
kk
-mnkVP(Wt--)k2 ≤ [P(Wt-i) - P(Wt)] + -maxkVP(Wt--) - vt--k2 - -t-- (1 - Lkαt--) ∣vt--k2.
By assuming that -maχ ≤ 尚,it holds that -t-- ≤ 尚 and(1 - Lk-t--) ≥ 0, ∀t ∈ [m]. Thus,
k	kk
-mnkVP(Wt--)k2 ≤ [P(Wt--) - P(Wt)] + -maxkVP(Wt--) - vt--k2 - -fn (1 - Lk-max) kvt--k2.
By taking expectations and multiplying both sides with ^k-:
αmin
2	-k
E[∣VP(Wt--)k2] ≤ —[E[P(Wt--)] - E[P(Wt)]] + -maxE[kVP(Wt--) - vt--k2] -(1 - Lk-max) E[∣vt--k2]
-min	-min
2	-k
≤ — [E[P(Wt--)] - E[P(Wt)]] + -faxE[kVP(Wt--) - vt--k2],
--
min	min
where the last inequality holds as -fax ≤ *.Summing over t = 1, 2,...,m + 1, we have
m+-	m+-	k	m+-
X E[kVP(Wt--)k2]	≤	—	X E[P(Wt--)-	P(Wt)]	+	-max	X E[kVP(Wt--)	- vt--k2]
t=-	-min	t=-	-min	t=-
2	-k	m+-
= E[P(wo) - P(Wm+-)] + -fax E E[kVP(Wt--) - vt--k2
-k	-k
fin	fin t=-
2	-k	m+-
≤ — E[P(wo) - P(W*)] + -fax E E[kVP(wt--) - vt--k2],
-k	-k
fin	fin t=-
where the last inequality holds since w* is the global minimizer of P.
The last expression can be equivalently written as:
m	2	-k m
]TE[kVP(wt)k2]	≤ — E[P(wo) - P(w*)] + -fax ]TE[kVP(Wt) - vtk2],
t=o	-fin	-fin t=o
which completes the proof.
C.2.2 Proof of Lemma B.8
Ejkvjk2
≤
Ej [kvj-- - (Vfij (Wj-I ) - Vfij (Wj )) k2 ]
kvj--k2 + Ej kVfij (Wj--) - Vfij (Wj)k2
2
-Ej	T-〈Vfit (Wj-I) - Vfij (Wj),Wj-- - Wj)
(-2)
≤
kvj--k2 + Ej kVfij (Wj--) - Vfij (Wj)k2] - Ej
2
-j--Lkj
kVfij (Wj--) - Vfij (Wj)k2
27
Under review as a conference paper at ICLR 2022
For each outer loop k, it holds that αj-1 ≤ αkmax and Lik ≤ Lkmax. Thus,
2
Ej[kvjIl ]	≤ kvj-ιk + Ej [IVfij(Wj-I)-Nfij(Wj)k ] — -k~~LmaxEj [INfj(Wj-I)-Nfij(Wj)k ]
αmax Lk
=llvj-ιk2 + (1 - ~k^^Lmax) Ej [kNfij(Wj-I) - Nfij(Wj)k2]
αmaxLk
=Ilvj-Ik2 + (1 - αk 2Lmaχ ) Ej [kvj - vj-1k2] .
max k
By rearranging, taking expectations again, and assuming that αkmax < 2/Lkmax:
αk Lmax
E[kvj - vj-1k2] ≤ D 黑 LmaX	[E[kvj-1k2] - E[kvj k2]].
2 - αmaxLk
By summing the above inequality over j = 1, . . . , t (t ≥ 1), we have:
t
X E[kvj -vj-1k2]	≤
j=1
k max
amaxLk____
2 - amaχLmax
≤
k max
amax Lk___
2 _ ak	Lmax
- amax k
[E[kv0k2]-E[kvtk2]] .
(14)
Now, by using Lemma C.3, we obtain:
(13)
E[kNP(Wt) -vtk2]	≤
t
XE[kvj-vj-1k2]
j=1
(14)
≤
k max
αmaxLk____
2 _ ak	Lmax
- αmax k
k max
αmaxLk____
2 - amaχLmax
[E[kv0k2]-E[kvtk2]]
E[kv0k2].
(15)
≤
C.2.3 Proof of Theorem B.9
Proof. Since v0 = NP(W0) implies kNP(W0) - v0k2 = 0, then by Lemma B.8, we obtain:
m	mak Lmax
X E[kNP(Wt) - vtk2] ≤ 2	(JXLmaX E[kv0k2].	(16)
t=0	2 - amaxLk
Combine this with Lemma B.7, we have that:
m	2	ak	m
ΕE[kNP(Wt)k2]	≤	— Ε[P(wo) - P(w*)] + -max £E[kNP(Wt) - vtk2]
aa
t=0	amin	amin t=0
(16)	2	ak mak	Lmax
≤ F E[P (WO)	- P (W*)] +	十	0	maX 3ax	ElkvOlW。)
akmin	akmin	2 -	akmax Lkmax
Since We are considering one outer iteration, with k ≥ 1, we have vo = NP (wo) = NP (Wk-I) and
Wk = Wt, where t is drawn uniformly at random from {0, 1,..., m}. Therefore, the following holds,
E[kNP (Wk )k2]=
(17)
≤
1
m+1
m
XE[kNP(Wt)k2]
t=o
2	ak	ak	Lmax
----------E[P(Wk-ι) - P(w*)] + -max ―max-k——
-min(m+1)	-min V2 - -maxLmax
E[kNP(Wk-ι)k2]
(μ-min (m + 1)
+ -Sax
+ 0m^
k max
-maxLk____
2 - ak LmaX
- -max k
E[kNP (Wk-ι )k2].
≤
28
Under review as a conference paper at ICLR 2022
αk	αk Lmax
Let US use σk = -—τ +——max ∙ ° maχ k max, then the above expression can be written as:
m μαmin(m+1) αmin αmax k
E[kVP(Wk )k2] ≤ σm E[∣∣VP(Wk-ι)k2 ].
By expanding the recurrence, we obtain:
E[kVP(Wk)k2] ≤	(YYσm) kVP(W0)k2.
This completes the proof.	□
29