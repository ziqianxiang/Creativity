Under review as a conference paper at ICLR 2022
Multiresolution Equivariant Graph Varia-
tional Autoencoder
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we propose Multiresolution Equivariant Graph Variational Autoen-
coders (MGVAE), the first hierarchical generative model to learn and generate
graphs in a multiresolution and equivariant manner. At each resolution level, MG-
VAE employs higher order message passing to encode the graph while learning
to partition it into mutually exclusive clusters and coarsening into a lower res-
olution that eventually creates a hierarchy of latent distributions. MGVAE then
constructs a hierarchical generative model to variationally decode into a hierarchy
of coarsened graphs. Importantly, our proposed framework is end-to-end permuta-
tion equivariant with respect to node ordering. MGVAE achieves competitive re-
sults with several generative tasks including general graph generation, molecular
generation, unsupervised molecular representation learning to predict molecular
properties, link prediction on citation graphs, and graph-based image generation.
1	Introduction
Understanding graphs in a multiscale and multiresolution perspective is essential for capturing the
structure of molecules, social networks, or the World Wide Web. Graph neural networks (GNNs)
utilizing various ways of generalizing the concept of convolution to graphs (Scarselli et al., 2009)
(Niepert et al., 2016b) (Li et al., 2016) have been widely applied to many learning tasks, including
modeling physical systems (Battaglia et al., 2016), finding molecular representations to estimate
quantum chemical computation (Duvenaud et al., 2015) (Kearnes et al., 2016) (Gilmer et al., 2017b)
(Hy et al., 2018), and protein interface prediction (Fout et al., 2017). One of the most popular
types of GNNs is message passing neural nets (MPNNs) that are constructed based on the message
passing scheme in which each node propagates and aggregates information, encoded by vectorized
messages, to and from its local neighborhood. While this framework has been immensely successful
in many applications, it lacks the ability to capture the multiscale and multiresolution structures that
are present in complex graphs (Rustamov & Guibas, 2013) (Chen et al., 2014) (Cheng et al., 2015)
(Xu et al., 2019).
Ying et al. (2018) proposed a multiresolution graph neural network that employs a differential pool-
ing operator to coarsen the graph. While this approach is effective in some settings, it is based on
soft assigment matrices, which means that (a) the sparsity of the graph is quickly lost in higher layers
(b) the algorithm isn’t able to learn an actual hard clustering of the vertices. The latter is important
in applications such as learning molecular graphs, where clusters should ideally be interpretable as
concrete subunits of the graphs, e.g., functional groups.
In contrast, in this paper we propose an arhictecture called Multiresolution Graph Network (MGN),
and its generative cousin, Multiresolution Graph Variational Autoencoder (MGVAE), which explic-
itly learn a multilevel hard clustering of the vertices, leading to a true multiresolution hierarchy. In
the decoding stage, to “uncoarsen” the graph, MGVAE needs to generate local adjacency matrices,
which is inherently a second order task with respect to the action of permutations on the vertices,
hence MGVAE needs to leverage the recently developed framework of higher order permutation
equivariant message passing networks (Hy et al., 2018; Maron et al., 2019b).
Learning to generate graphs with deep generative models is a difficult problem because graphs are
combinatorial objects that typically have high order correlations between their discrete substruc-
tures (subgraphs) (You et al., 2018a) (Li et al., 2018) (Liao et al., 2019) (Liu et al., 2019) (Dai
et al., 2020). Graph-based molecular generation (Gmez-Bombarelli et al., 2018) (Simonovsky &
1
Under review as a conference paper at ICLR 2022
Komodakis, 2018) (Cao & Kipf, 2018) (Jin et al., 2018) (Thiede et al., 2020) involves further chal-
lenges, including correctly recognizing chemical substructures, and importantly, ensuring that the
generated molecular graphs are chemically valid. MGN allows us to extend the existing model of
variational autoencoders (VAEs) with a hierarchy of latent distributions that can stochastically gen-
erate a graph in multiple resolution levels. Our experiments show that having a flexible clustering
procedure from MGN enables MGVAE to detect, reconstruct and finally generate important graph
substructures, especially chemical functional groups.
2	Related work
There have been significant advances in understanding the invariance and equivariance properties of
neural networks in general (Cohen & Welling, 2016a) (Cohen & Welling, 2016b), of graph neural
networks (Maron et al., 2019b), of neural networks learning on sets (Zaheer et al., 2017) (Serviansky
et al., 2020) (Maron et al., 2020), along with their expressive power on graphs (Maron et al., 2019c)
(Maron et al., 2019a). Our work is in line with group equivariant networks operating on graphs and
sets. Multiscale, multilevel, multiresolution and coarse-grained techniques have been widely applied
to graphs and discrete domains such as diffusion wavelets (Coifman & Maggioni, 2006); spectral
wavelets on graphs (Hammond et al., 2011); finding graph wavelets based on partitioning/clustering
(Rustamov & Guibas, 2013); graph clustering and finding balanced cuts on large graphs (Dhillon
et al., 2005) (Dhillon et al., 2007) (Chiang et al., 2012) (Si et al., 2014); and link prediction on social
networks (Shin et al., 2012). Prior to our work, some authors such as (Zhou et al., 2019) proposed
a multiscale generative model on graphs using GAN (Goodfellow et al., 2014), but the hierarchical
structure was built by heuristics algorithm, not learnable and not flexible to new data that is also
an existing limitation of the field. In general, our work exploits the powerful group equivariant
networks to encode a graph and to learn to form balanced partitions via back-propagation in a data-
driven manner without using any heuristics as in the existing works.
In the field of deep generative models, it is generally recognized that introducing a hierarchy of
latents and adding stochasticity among latents leads to more powerful models capable of learning
more complicated distributions (Blei et al., 2003) (Ranganath et al., 2016) (Ingraham & Marks,
2017) (Klushyn et al., 2019) (Wu et al., 2020) (Vahdat & Kautz, 2020). Our work combines the
hierarchical variational autoencoder with learning to construct the hierarchy that results into a gen-
erative model able to generate graphs at many resolution levels.
3	Multiresolution graph network
3.1	Construction
An undirected weighted graph G = (V, E, A, Fv, Fe) with node set V and edge set E is represented
by an adjacency matrix A ∈ N∣V∣×∣V∣, where Aij > 0 implies an edge between node vi and vj
with weight Aij (e.g., Aij ∈ {0, 1} in the case of unweighted graph); while node features are
represented by a matrix Fv ∈ R∣V ∣×dv, and edge features are represented by a tensor Fe ∈ R∣V∣×∣V∣×de.
The second-order tensor representation of edge features is necessary for our higher-order message
passing networks described in the next section. Indeed, Fv can be encoded in the diagonal of Fe .
Definition 1. A K-cluster partition of graph G is a partition of the set of nodes V into K mutually
exclusive clusters V1, .., VK. Each cluster corresponds to an induced subgraph Gk = G[Vk].
Definition 2. A coarsening of G is a graph G of K nodes defined by a K -cluster partition in which
node Vk of G corresponds to the induced subgraph Gk. The weighted adjacency matrix A ∈ NK×K
.~
of G Is
A {2 ∑Vi,Vj∈Vk Aij,	if k = k',
kk' = {∑Vi∈Vk,Vj∈Vk'Aij, if k ≠ k',
where the diagonal of A denotes the number of edges inside each cluster, while the off-diagonal
denotes the number of edges between two clusters.
Fig. 3.1 shows an example of Defs. 1 and 2: a 3-cluster partition of the Aspirin C9H8O4 molecular
graph and its coarsening graph. Def. 3 defines the multiresolution of graph G in a bottom-up manner
2
Under review as a conference paper at ICLR 2022
Figure 1: Aspirin C9H8O4, its 3-cluster partition and the corresponding coarsen graph
in which the bottom level is the highest resolution (e.g., G itself) while the top level is the lowest
resolution (e.g., G is coarsened into a single node).
Definition 3. An L-level coarsening of a graph G is a series of L graphs G(1) , .., G(L) in which
1.	G(L) isG itself.
2.	For 1 ≤ ' ≤ L - 1, G(') is a coarsening graph of G('+1) as defined in Def. 2. The number
of nodes in G (') is equal to the number ofclusters in G ('+1).
3.	The top level coarsening G(1) is a graph consisting ofa single node, and the corresponding
adjacency matrix A(1) ∈ N1×1.
Definition 4. An L-level Multiresolution Graph Network (MGN) ofa graph G consists ofL- 1 tu-
ples of five network components {(c('), e 像1, di?al, dg'Ll, P("}L=2. The '-th tuPle encodes G⑷
and transforms it into a lower resolution graph G('-1) in the higher level. Each of these network
components has a separate set of learnable parameters (θ('), θ2'), θ('), θ('), θ5')). For simplic-
ity, we collectively denote the learnable parameters as θ, and drop the superscript. The network
components are defined as follows:
1.	Clustering procedure C(G(`); θ), which partitions graph G(') into K clusters
Vf),..., VK). Each cluster is an induced subgraph G((') of G(')with adjacency matrix
Ak .
2.	Local encoder eloca(Gk); θ), which is a permutation equivariant (see Defs. 5, 6) graph
neural network that takes as input the subgraph Ga), and outputs a set of node latents
Z(')represented as a matrix ofsize ∣V["∣ X dz.
3.	Local decoder d.a(Z(' ； θ), which is a permutation equivariant neural network that tries
to reconstruct the subgraph adjacency matrix Aa) for each clusterfrom the local encoder's
output latents.
4.	(Optional) Global decoder dglobal (Z(')； θ), which is a permutation equivariant neural net-
work that reconstructs thefull adjacency matrix A(') from all the node latents of K clusters
Z(')= ㊉ k z[')represented as a matrix ofsize ∣V(')∣ × dz.
5.	Pooling network P(Zke); θ), which is a permutation invariant (see Defs. 5, 6) neural net-
work that takes the set of node latents Zk) and outputs a single cluster latent Za) ∈ dz.
The coarsening graph G('-1) has adjacency matrix A('-1) built as in Def. 2, and the cor-
responding node features Z ('-1)= ㊉ k Za) represented as a matrix ofsize K × dz.
Algorithmically, MGN works in a bottom-up manner as a tree-like hierarchy starting from the high-
est resolution graph G(L), going to the lowest resolution G(1) (see Fig. 3.1). Iteratively, at `-th level,
MGN partitions the current graph into K clusters by running the clustering procedure c('). Then,
the local encoder e^oc^ and local decoder dgKl operate on each of the K subgraphs separately, and
can be executed in parallel. This encoder/decoder pair is responsible for capturing the local struc-
tures. Finally, the pooling network p(`) shrinks each cluster into a single node of the next level.
3
Under review as a conference paper at ICLR 2022
Figure 2: Hierarchy of 3-level Multiresolution Graph Network on Aspirin molecular graph
Optionally, the global decoder dg?bai makes sure that the whole set of node latents Z ⑹ is able to
capture the inter-connection between clusters.
In terms of time and space complexity, MGN is more efficient than existing methods in the field.
The cost of global decoding the highest resolution graph is proportional to ∣V∣2. For example, while
the encoder can exploit the sparsity of the graph and has complexity O(∣E ∣), a simple dot-product
decoder dglobal(Z) = sigmoid(ZZT) has both time and space complexity of O(∣V∣2) which is infea-
sible for large graphs. In contrast, the cost of running K local dot-product decoders is O(∣V∣2∕K),
which is approximately K times more efficient.
3.2	Higher order message passing
In this paper we consider permutation symmetry, i.e., symmetry to the action of the symmetric
group, Sn. An element σ ∈ Sn is a permutation of order n, or a bijective map from {1, . . . , n} to
{1, . . . , n}. The action of Sn on an adjacency matrix A ∈ Rn×n and on a latent matrix Z ∈ Rn×dz
are
[σ ∙ A]iι,i2 = Aσ-1(iι),σ-1(i2),	[σ，Z]i,j = Zσ-1(i),j,	σ ∈ Sn.
Here, the adjacency matrix A is a second order tensor with a single feature channel, while the latent
matrix Z is a first order tensor with dz feature channels. In general, the action of Sn on a k-th order
tensor X ∈ Rn ×d (the last index denotes the feature channels) is defined similarly as:
[σ ∙ X ]il,..,ik,j = Xσ-'(iι),..,σ-'(ik),j,
σ ∈ Sn .
Network components of MGN (as defined in Sec. 3.1) at each resolution level must be either equiv-
ariant, or invariant with respect to the permutation action on the node order of G('). Formally, We
define these properties in Def. 5.
Definition 5. An Sn -equivariant (or permutation equivariant) function is a function
f ：Rnk×d → Rnk ×d' that satisfies f(σ ∙ X) = σ ∙ f (X) for all σ ∈ Sn and X ∈ Rnkxd. Similarly,
we say that f is Sn-invariant (or permutation invariant) if and only iff(σ ∙ X) = f(X).
Definition 6. An Sn-equivariant network is afunction f : Rnk×d → Rnk ×d' defined as a composition
of Sn-equivariant linear functions f1, .., fT and Sn-equivariant nonlinearities γ1, .., γT:
f = YT o fτ o .. o Yi o fι.
On the another hand, an Sn-invariant network is a function f : Rnk×d → R defined as a composition
ofan Sn-equivariant network f′ and an Sn-invariantfunction on top of it, e.g., f = f′′ o f′.
In order to build higher order equivariant networks, we revisit some basic tensor operations: the
tensor product A ⑥ B and tensor contraction AJxl X (details and definitions are Sec. A). It can be
shown that these tensor operations respect permutation equivariance (Kondor et al., 2018). Based
on these tensor contractions and Def. 5, we can construct the second-order Sn -equivariant networks
as in Def. 6 (see Example 1): f = Y o MT o .. o Y o M1. The second-order networks are par-
ticularly essential for us to extend the original variational autoencoder (VAE) (Kingma & Welling,
4
Under review as a conference paper at ICLR 2022
2014) model that approximates the posterior distribution by an isotropic Gaussian distribution with
a diagonal covariance matrix and uses a fixed prior distribution N(0, 1). In constrast, we generalize
by modeling the posterior by N(μ, Σ) in which Σ is a full covariance matrix, and We learn an adap-
tive parameterized prior N(^, Σ) instead of a fixed one. Only the second-order encoders can output
a permutation equivariant full covariance matrix, while lower-order networks such as MPNNs are
unable to. See Sec. 4.2, B and C for details.
Example 1. The second order message passing has the message H0 ∈ R∣V ∣×∣V ∣×(dv +de) initialized
by promoting the node features Fv to a second order tensor (e.g., we treat node features as self-loop
edge features), and concatenating with the edge features Fe. Iteratively,
Ht = γ(Mt), Mt = Wt ㊉(A 区 Ht-ι)ii,j ,
i,j
where A 0 Ht-ι results in a fourth order tensor while Ji,j contracts it down to a second order
tensor along the i-th and j-th dimensions,㊉ denotes concatenation along thefeature channels, and
Wt denotes a multilayer perceptron on the feature channels. We remark that the popular MPNNs
(Gilmer et al., 2017b) is a lower-order one and a special case in which Mt = D-1AHt-1Wt-1
where Dii = ∑j Aij is the diagonal matrix of node degrees. The message HT of the last iteration is
still second order, so we contract it down to the first order latent Z = ㊉i HT Ji.
3.3	Learning to cluster
Definition 7. A clustering of n objects into k clusters is a mapping π : {1,.., n} → {1,.., k} in which
π(i) = j if the i -th object is assigned to the j -th cluster. The inverse mapping ∏-1(j) = {i ∈ [1, n]：
π(i) = j } gives the set of all objects assigned to the j -th cluster. The clustering is represented by an
assignment matrix Π ∈ {0, 1}n×k such that Πi,π(i) = 1.
Definition 8. The action of Sn on a clustering π of n objects into k clusters and its corresponding
assignment matrix Π are
[σ ∙ π](i) = π(σ-1(i)),	[σ ∙ π]i,j = πσT(i),j,	σ ∈ Sn.
Definition 9. Let N be a neural network that takes as input a graph G of n nodes, and outputs a
clustering π ofk clusters. N is said to be equivariant if and only ifN(σ ∙ G) = σ ∙ N(G) for all
σ ∈ Sn.
From Def. 9, intuitively the assignement matrix Π still represents the same clustering ifwe permute
its rows. The learnable clustering procedure c(G('); θ) is built as follows:
1.	A graph neural network parameterized by θ encodes graph G(') into a first order tensor of K
(`)	∣V (`) ∣×K
feature channels (`) ∈ R∣V ∣×K
eaure cannes p ∈	.
2.	The clustering assignment is determined by a row-wise maximum pooling operation:
∏⑹(i) = arg maxPTk
k∈[1,K]	,
that is an equivariant clustering in the sense of Def. 9.
(1)
A composition of an equivariant function (e.g., graph net) and an equivariant function (e.g., maxi-
mum pooling given in Eq. 1) is still an equivariant function with respect to the node permutation.
Thus, the learnable clustering procedure C(G('); θ) is permutation equivariant.
In practice, in order to make the clustering procedure differentiable for backpropagation, we replace
the maximum pooling in Eq. 1 by sampling from a categorical distribution. Let π(') (i) be a categor-
ical variable with class probabilities p((') ..,p('K computed as softmax from pi?. The Gumbel-max
trick (Gumbel, 1954)(Maddison et al., 2014)(Jang et al., 2017) provides a simple and efficient way
to draw samples ∏(')(i):
Πi') = one-hot ( arg max [g,k + log pi')]),
k∈[1,K]	,
where gi,1, .., gi,K are i.i.d samples drawn from Gumbel(0, 1). Given the clustering assignment
matrix Π('), the coarsened adjacency matrix A('-1) (see Defs. 1 and 2) can be constructed as
∏(')τAi')∏(').
5
Under review as a conference paper at ICLR 2022
It is desirable to have a balanced K-cluster partition in which clusters V('),.., V^) have similar sizes
that are close to ∣ V(') ∣∕K. The local encoders tend to generalize better for same-size subgraphs. We
want the distribution of nodes into clusters to be close to the uniform distribution. We enforce
the clustering procedure to produce a balanced cut by minimizing the following Kullback-Leibler
divergence:
K	P (k)	∕∣V (')∣	∣V (')∣∖	1 1	1∖
DKL(PIQ) = ∑1 P(k)log两 where P = (西，..，西)，Q = (K,..,K).⑵
The whole construction of MGN is equivariant with respect to node permutations of G. In the case
of molecular property prediction, we want MGN to learn to predict a real value y ∈ R for each graph
G while learning to find a balanced cut in each resolution to construct a hierarchical structure of
latents and coarsen graphs. The total loss function is
LL
LMGN (G, y) = ∣∣f (㊉ R(Z ⑻))-y∣∣2 + ∑ λ⑶Dkl(P(')||Q(')),	⑶
'=1	'=1
where f is a multilayer perceptron,㊉ denotes the vector concatenation, R is a readout function that
produces a permutation invariant vector of size d given the latent Z∣V(') ∣×d at the '-th resolution,
λ(') ∈ R is a hyperparamter, and DKL(P⑷ ∣∣Q⑹)is the balanced-cut loss as defined in Eq. 2.
4	Hierarchical generative model
In this section, we introduce our hierarchical generative model for multiresolution graph generation
based on variational principles.
4.1	Background on graph variational autoencoder
Suppose that we have input data consisting of m graphs (data points) G = {G1, .., Gm}. The stan-
dard variational autoencoders (VAEs), introduced by Kingma & Welling (2014) have the following
generation process, in which each data graph Gi for i ∈ {1, 2, .., m} is generated independently:
1. Generate the latent variables Z = {Z1, .., Zm}, where each Zi ∈ R∣Vi∣×dz is drawn i.i.d. from a
prior distribution p0 (e.g., standard Normal distribution N(0, 1)).
2.	Generate the data graph Gi Z pθ (GiIZi) from the model conditional distribution pθ.
We want to optimize θ to maximize the likelihood pθ (G) = ∫ pθ(Z)pθ(G∣Z)dZ. However, this re-
quires computing the posterior distribution pθ (G IZ) = ∏im=1pθ(GiIZi), which is usually intractable.
Instead, VAEs apply the variational principle, proposed by Wainwright & Jordan (2005), to approxi-
mate the posterior distribution as qφ(ZIG) = ∏im=1 qφ(ZiIGi) via amortized inference and maximize
the evidence lower bound (ELBO) that is a lower bound of the likelihood:
LELBO(φ, θ) = Eqφ(Z∣G)[log pθ(GIZ)] - DKL(qφ(ZIG)IIp0(Z))
m	(4)
=∑ [Eqφ(Zi∣Gi)[logpθ(GiIZi)]-DKL(qφ(ZiIGi)IIp0(Zi))].
i=1
The probabilistic encoder qφ(ZIG), the approximation to the posterior of the generative model
pθ(G, Z), is modeled using equivariant graph neural networks (see Example 1) as follows. As-
sume the prior over the latent variables to be the centered isotropic multivariate Gaussian pθ (Z) =
N(Z; 0, I). We let qφ(ZiIGi) be a multivariate Gaussian with a diagonal covariance structure:
log qφ (ZiIGi) = log N (Zi； μi,σiI),	(5)
where μ%, σi ∈ R∣Vi∣×dz are the mean and standard deviation of the approximate posterior output by
two equivariant graph encoders. We sample from the posterior qφ by using the reparameterization
trick: Zi = μi + σ% Θ e, where E Z N(0, I) and Θ is the element-wise product.
On the another hand, the probabilistic decoder pθ (Gi IZi) defines a conditional distribution over the
entries of the adjacency matrix Ai: pθ(GiIZi) = ∏(u,v)∈Vi2 pθ(Aiuv = 1IZiu, Ziv). For example,
Kipf & Welling (2016) suggests a simple dot-product decoder that is trivially equivariant: pθ(Aiuv =
1IZiu, Ziv) = γ(ZiTuZiv), where γ denotes the sigmoid function.
6
Under review as a conference paper at ICLR 2022
4.2	Multiresolution VAEs
Based on the construction of multiresolution graph network (see Sec. 3.1), the latent vari-
ables are partitioned into disjoint groups, Zi = {zf),Z(2),..,Z(L)} where Z(') = {[Z(')]k ∈
R∣[V(')]k∣×dz }k is the set of latents at the '-th resolution level in which the graph Gi(') is partitioned
into a number of clusters [G(') ] k.
In the area of normalzing flows (NFs), Wu et al. (2020) has shown that stochasticity (e.g., a chain
of stochastic sampling blocks) overcomes expressivity limitations of NFs. In general, our MGVAE
is a stochastic version of the deterministic MGN such that stochastic sampling is applied at each
resolution level in a bottom-up manner. The prior (Eq. 6) and the approximate posterior (Eq. 7) are
represented by
LL
P(Zi) = ∏∏ p(zi(')) = ∏∏ ∏ p([z(') ]k),	⑹
qφ(Zi∣Gi) = qφ(Z(L) ∣G(L)) ∏ qφ(Z(')∣Z('+1), G(')),	⑺
'=L-1
in which each conditional in the approximate posterior are in the form of factorial Normal distribu-
tions, in particular
qφ(Z(')∣Z('+1), Gie)) = ∏ qφ([Z(')]k∣Zi('+1), [G(')]k),
k
where each probabilistic encoder qφ([Zi')]k∣Zi'+1), [Gi')[k) operates on a subgraph [Gi')]k as
follows:
•	The pooling network p('+1) shrinks the latent Zi'+1) into the node features of Gi') as in
the construction of MGN (see Def. 4).
•	The local (deterministic) graph encoder d(ocal encodes each subgraph [Gi')] k into a mean
vector and a diagonal covariance matrix (see Eq. 5). A second order encoder can produce a
positive semidefinite non-diagonal covariance matrix, that can be interpreted as a Gaussian
Markov Random Fields (details in Sec. B). The new subgraph latent [Zi(')]k is sampled by
the reparameterization trick.
The prior can be either the isotropic Gaussian N(0, 1) as in standard VAEs, or be implemented
as a parameterized Gaussian N(^, Σ) where μ and Σ are learnable equivariant functions (details in
Sec. C). The reparameterization trick for conventional N (0, 1) prior is the same as in Sec. 4.1, while
the new one for the generalized and learnable prior N(μ, Σ) is given in Sec. B. On the another hand,
the probabilistic decoderpθ(Gi(1), .., Gi(L)∣Zi(1), .., Zi(L)) defines a conditional distribution over all
subgraph adjacencies at each resolution level:
pθ(Gi(1),..,Gi(L)∣Zi(1),..,Zi(L))=∏pθ(Gi(')∣Zi('))=∏∏pθ([Ai(')]k∣[Zi(')]k).
'	'k
Extending from Eq. 4, we write our multiresolution variational lower bound LMGVAE(φ, θ) on
log p(G ) compactly as
LMGVAE(φ, θ) = ∑∑ [Eqφ(z(')∣G('))[logPΘ(Gi(')∣Zi('))] - Dkl(qφ(Zi')∣Gi'))∣∣P0(Z(I))],⑻
i' φi i
where the first term denotes the reconstruction loss (e.g., ∣∣A(') - Ai') ∣∣ where A(L) is Gi itself,
Ai('<L) is the adjacency produced by MGN at level ',and Ai') are the reconstructed ones by the de-
coders); and the second term is indeed DKL(N(μi'), ∑i'))∣∣N(μi'), ∑i'))) where μi') ∈ R∣V(')∣×d
and Σi') ∈ R∣V*×∣V*×d are the mean and covariance tensors produced by the `-th encoder for
graph Gi, while //i')and Σi') are learnable ones in an equivariant manner as in Sec. C. In general,
the overall optimization is given as follows:
min	LMGVAE(Φ,θ; {μi'),∑(')}') + ∑λ(')DκL(p(')∣∣Qi')),	(9)
φ,θ,{μ⑷,ς ⑷}'	i,'
where φ denotes all learnable parameters of the encoders, θ denotes all learnable parameters of the
decoders, and DKL(Pii')∣∣Qii')) is the balanced-cut loss for graph Gi at level ` as defined in Sec. 3.3.
7
Under review as a conference paper at ICLR 2022
Figure 3: MGVAE generates molecules on QM9 (4 on the left) and ZINC (the rest) equivariantly.
There are many more examples of generated molecules in Sec. D.2. Both equivariant MGVAE and
autoregressive MGN generate high-quality molecules with complicated structures such as rings.
Dataset	Method	Training size	Input features	Validity	Novelty	Uniqueness
QM9	GraphVAE	Z 100K	Graph	61.00%	85.00%	40.90%
	CGVAE			100%	94.35%	-98.57%-
	MolGAN			98.1%	94.2%	-TO%
	Autoregressive MGN	10K		100%	95.01%	-97.44%-
	All-at-once MGVAE			100%	100%	-95.16%-
ZINC	GraphVAE	Z 200K	Graph	14.00%	100%	31.60%
	CGVAE			100%	100%	99.82% 一
	JTVAE			100%	-	-
	Autoregressive MGN	1K		100%	99.89%	-99.69%-
	All-at-once MGVAE	10K	—	ChemiCaI	99.92%	100%	99.34% 一
Table 1: Molecular graph generation results. GraphVAE results are taken from (Liu et al., 2018).
5	Experiments
Many more experimental results and details are presented in the Sec. D of the Appendix.
5.1	Molecular graph generation
We examine the generative power of MGN and MGVAE in the challenging task of molecule genera-
tion, in which the graphs are highly structured. We demonstrate that MGVAE is the first hierarchical
graph VAE model generating graphs in a permutation-equivariant manner that is competitive against
autoregressive results. We train on two datasets that are standard in the field:
1.	QM9 (Ruddigkeit et al., 2012) (Ramakrishnan et al., 2014): contains 134K organic
molecules with up to nine atoms (C, H, O, N, and F) out of the GDB-17 universe of
molecules.
2.	ZINC (Sterling & Irwin, 2015): contains 250K purchasable drug-like chemical compounds
with up to twenty-three heavy atoms.
We only use the graph features as the input, including the adjacency matrix, the one-hot vector
of atom types (e.g., carbon, hydrogen, etc.) and the bond types (single bond, double bond, etc.)
without any further domain knowledge from chemistry or physics. First, we train autoencoding
task of reconstructing the adjacency matrix and node features. We use a learnable equivariant prior
(see Sec. C) instead of the conventional N(0, 1). Then, we generate 5, 000 different samples from
the prior, and decode each sample into a generated graph (see Fig. 3). We implement our graph
construction (decoding) in two approaches:
1.	All-at-once: We reconstruct the whole adjacency matrix by running the probabilistic de-
coder (see Sec. 4). MGVAE enables us to generate a graph at any given resolution level `.
In this particular case, we select the highest resolution ` = L. This approach of decoding
preserves equivariance, but is harder to train. On ZINC, we extract several chemical/atomic
features from RDKit as the input for the encoders to reach a good convergence in training.
2.	Autoregressive: The graph is constructed iteratively by adding one edge in each iteration,
similarly to (Liu et al., 2018). But this approach does not respect permutation equivariance.
In our setting for small molecules, L = 3 and K = 2'-1 for the '-th level. We compare our methods
with other graph-based generative models including GraphVAE (Simonovsky & Komodakis, 2018),
CGVAE (Liu et al., 2018), MolGAN (Cao & Kipf, 2018), and JT-VAE (Jin et al., 2018). We evaluate
the quality of generated molecules in three metrics: (i) validity, (ii) novelty and (iii) uniqueness as
8
Under review as a conference paper at ICLR 2022
COMMUNITY-SMALL	EGO-SMALL
Model	Degree	Cluster	Orbit	Degree	Cluster	Orbit
GRAPHVAE	0.35	0.98	0.54	0.13	0.17	0.05
DeepGMG	0.22	0.95	0.4	0.04	0.10	0.02
GRAPHRNN	0.08	0.12	0.04	0.09	0.22	0.003
GNF	0.20	0.20	0.11	0.03	0.10	0.001
GraphAF	0.06	0.10	0.015	0.04	0.04	0.008
MGVAE	0.002	0.01	0.01	1.74e-05	0.0006	6.53e-05
Table 2: Graph generation results depicting MMD for various graph statistics between the test set
and generated graphs. MGVAE outperforms all competing methods.
the percentage of the generated molecules that are chemically valid, different from all molecules
in the training set, and not redundant, respectively. Because of high complexity, we only train on a
small random subset of examples while all other methods are trained on the full datasets. Our models
are equivalent with the state-of-the-art, even with a limited training set (see Table 1). Admittedly,
molecule generation is a somewhat subject task that can only be evaluated with objective numerical
measures up to a certain point. Qualitatively, however the molecules that MGVAE generates are as
good as the state of the art, in some cases better in terms of producing several high-quality drug-like
molecules with complicated functional groups and structures. Many further samples generated by
MGVAE and their analysis can be found in the Appendix.
5.2 General graph generation by MGVAE
We further examine the expressive power of hierarchical latent structure of MGVAE in the task of
general graph generation. We choose two datasets from GraphRNN paper (You et al., 2018a):
1.	Community-small: A synthetic dataset of 100 2-community graphs where 12 ≤ ∣V ∣ ≤ 20.
2.	Ego-small: 200 3-hop ego networks extracted from the Citeseer network (Sen et al., 2008)
where 4 ≤ ∣V ∣ ≤ 18.
The datasets are generated by the scripts from the GraphRNN codebase (You et al., 2018b). We keep
80% of the data for training and the rest for testing. We evaluate our generated graphs by computing
Maximum Mean Discrepancy (MMD) distance between the distributions of graph statistics on the
test set and the generated set as proposed by (You et al., 2018a). The graph statistics are node
degrees, clustering coefficients, and orbit counts. As suggested by (Liu et al., 2019), we execute 15
runs with different random seeds, and we generate 1,024 graphs for each run, then we average the
results over 15 runs. We compare MGVAE against GraphVAE (Simonovsky & Komodakis, 2018),
DeepGMG (Li et al., 2018), GraphRNN (You et al., 2018a), GNF (Liu et al., 2019), and GraphAF
(Shi et al., 2020). The baselines are taken from GNF paper (Liu et al., 2019) and GraphAF paper (Shi
et al., 2020). In our setting of (all-at-once) MGVAE, we implement only L = 2 levels of resolution
and K = 2' clusters for each level. Our encoders have 10 layers of message passing. Instead of
using a high order equivariant network as the global decoder for the bottom resolution, we only
implement a simple fully connected network that maps the latent Z(L) ∈ R∣V∣×dz into an adjacency
matrix of size ∣V∣ × ∣V∣. For the ego dataset in particular, we implement the learnable equivariant
prior as in Sec. B and Sec.C. Table 2 includes our quantitative results in comparison with other
methods. MGVAE outperforms all competing methods. Figs. 10 11 show some generated examples
and training examples on the 2-community and ego datasets.
6 Conclusion
We introduced MGVAE built upon MGN, the first generative model to learn and generate graphs in
a multiresolution and equivariant manner. The key idea of MGVAE is learning to construct a series
of coarsened graphs along with a hierarchy of latent distributions in the encoding process while
learning to decode each latent into the corresponding coarsened graph at every resolution level.
MGVAE achieves state-of-the-art results from link prediction to molecule and graph generation,
suggesting that accounting for the multiscale structure of graphs is a promising way to make graph
neural networks even more powerful.
9
Under review as a conference paper at ICLR 2022
References
Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, and Koray Kavukcuoglu.
Interaction networks for learning about objects, relations and physics. In Advances in Neural
Information Processing Systems, volume 29. Curran Associates, Inc., 2016.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent dirichlet allocation. J. Mach. Learn.
Res., 3(null):9931022, March 2003. ISSN 1532-4435.
Nicola De Cao and Thomas Kipf. Molgan: An implicit generative model for small molecular graphs,
2018.
Xu Chen, Xiuyuan Cheng, and Stephane Mallat. Unsupervised deep haar scattering on graphs. In
Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014.
XiUyUan Cheng, XU Chen, and StePhane Mallat. Deep haar scattering networks. CoRR,
abs/1509.09187, 2015.
Kai-Yang Chiang, Joyce JiyoUng Whang, and Inderjit S. Dhillon. Scalable clUstering of signed
networks Using balance normalized cUt. In Proceedings of the 21st ACM International Conference
on Information and Knowledge Management, CIKM ’12, pp. 615624, New York, NY, USA, 2012.
Association for CompUting Machinery. ISBN 9781450311564. doi: 10.1145/2396761.2396841.
Taco S. Cohen and Max Welling. GroUp eqUivariant convolUtional networks. Proceedings of The
33rd International Conference on Machine Learning, 48:2990-2999, 2016a.
Taco S. Cohen and Max Welling. Steerable cnns. ICLR’17, 2016b.
Ronald R. Coifman and MaUro Maggioni. DiffUsion wavelets. Applied and Computational Har-
monic Analysis, 21(1):53-94, 2006. ISSN 1063-5203. doi: https://doi.org/10.1016/j.acha.
2006.04.004. URL https://www.sciencedirect.com/science/article/pii/
S106352030600056X. Special IssUe: DiffUsion Maps and Wavelets.
HanjUn Dai, Azade Nazi, YUjia Li, Bo Dai, and Dale SchUUrmans. Scalable deep generative model-
ing for sparse graphs. In Proceedings of the 37th International Conference on Machine Learning,
volUme 119 of Proceedings of Machine Learning Research, pp. 2302-2312. PMLR, 13-18 JUl
2020.
Inderjit Dhillon, YUqiang GUan, and Brian KUlis. A fast kernel-based mUltilevel algorithm for graph
clUstering. In Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge
Discovery in Data Mining, KDD ’05, pp. 629634, New York, NY, USA, 2005. Association for
CompUting Machinery. ISBN 159593135X. doi: 10.1145/1081870.1081948.
Inderjit S. Dhillon, YUqiang GUan, and Brian KUlis. Weighted graph cUts withoUt eigenvectors a
mUltilevel approach. IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(11):
1944-1957, 2007. doi: 10.1109/TPAMI.2007.1115.
Adji B. Dieng, Francisco J. R. RUiz, David M. Blei, and Michalis K. Titsias. Prescribed generative
adversarial networks, 2019.
David K DUvenaUd, DoUgal MaclaUrin, Jorge IparragUirre, Rafael Bombarell, Timothy Hirzel, Alan
AspUrU-GUzik, and Ryan P Adams. ConvolUtional networks on graphs for learning molecUlar fin-
gerprints. In Advances in Neural Information Processing Systems, volUme 28. CUrran Associates,
Inc., 2015.
Vijay Prakash Dwivedi, Chaitanya K. Joshi, Thomas LaUrent, YoshUa Bengio, and Xavier Bresson.
Benchmarking graph neUral networks. CoRR, abs/2003.00982, 2020.
Jack Edmonds and Richard M. Karp. Theoretical improvements in algorithmic efficiency for net-
work flow problems. Journal of the ACM (JACM), 1972. doi: 10.1145/321694.321699.
Alex FoUt, Jonathon Byrd, Basir Shariat, and Asa Ben-HUr. Protein interface prediction Using graph
convolUtional networks. In Proceedings of the 31st International Conference on Neural Informa-
tion Processing Systems, NIPS’17, pp. 65336542, Red Hook, NY, USA, 2017. CUrran Associates
Inc. ISBN 9781510860964.
10
Under review as a conference paper at ICLR 2022
J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for
quantum chemistry. 70, 2017a.
Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference on
Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 1263-1272.
PMLR, 06-11 Aug 2017b.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and
Statistics, volume 9 of Proceedings of Machine Learning Research, pp. 249-256, Chia Laguna
Resort, Sardinia, Italy, 13-15 May 2010. PMLR.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems, volume 27. Curran Associates, Inc., 2014.
E. J. Gumbel. Statistical theory of extreme values and some practical applications: a series of
lectures. US Govt. Print. Office, Number 33, 1954.
Rafael Gmez-Bombarelli, Jennifer N. Wei, David Duvenaud, Jos Miguel Hernndez-Lobato, Ben-
jamn Snchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D. Hirzel,
Ryan P. Adams, and Aln Aspuru-Guzik. Automatic chemical design using a data-driven con-
tinuous representation of molecules. ACS Central Science, 4(2):268-276, 2018. doi: 10.1021/
acscentsci.7b00572. PMID: 29532027.
David K. Hammond, Pierre Vandergheynst, and Rmi Gribonval. Wavelets on graphs via spec-
tral graph theory. Applied and Computational Harmonic Analysis, 30(2):129-150, 2011.
ISSN 1063-5203. doi: https://doi.org/10.1016/j.acha.2010.04.005. URL https://www.
sciencedirect.com/science/article/pii/S1063520310000552.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.
P. Hohenberg and W. Kohn. Inhomogeneous electron gas. Phys. Rev., 136:864-871, 1964.
Truong Son Hy, Shubhendu Trivedi, Horace Pan, Brandon M. Anderson, , and Risi Kondor. Pre-
dicting molecular properties with covariant compositional networks. The Journal of Chemical
Physics, 148, 2018.
John Ingraham and Debora Marks. Variational inference for sparse and undirected models. In Pro-
ceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings
of Machine Learning Research, pp. 1607-1616. PMLR, 06-11 Aug 2017.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In
ICLR, 2017.
Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for
molecular graph generation. In Proceedings of the 35th International Conference on Machine
Learning, volume 80 of Proceedings of Machine Learning Research, pp. 2323-2332. PMLR,
10-15 Jul 2018.
Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecular graph
convolutions: moving beyond fingerprints. Journal of Computer-Aided Molecular Design, 30(8):
595608, Aug 2016. ISSN 1573-4951. doi: 10.1007/s10822-016-9938-8.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd Inter-
national Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,
2015, Conference Track Proceedings, 2015.
Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International
Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014,
Conference Track Proceedings, 2014.
11
Under review as a conference paper at ICLR 2022
Thomas N. Kipf and Max Welling. Variational graph auto-encoders, 2016.
Alexej Klushyn, Nutan Chen, Richard Kurle, Botond Cseke, and Patrick van der Smagt. Learning
hierarchical priors in vaes. In Advances in Neural Information Processing Systems, volume 32.
Curran Associates, Inc., 2019.
Daphne Koller and Nir Friedman. In Probabilistic Graphical Models: Principles and Techniques.
MIT Press, 2009.
Risi Kondor, Truong Son Hy, Horace Pan, Brandon M. Anderson, and Shubhendu Trivedi. Covariant
compositional networks for learning graphs, 2018.
Nils M. Kriege, Pierre-Louis Giscard, and Richard C. Wilson. On valid optimal assignment kernels
and applications to graph classification. In Proceedings of the 30th International Conference on
Neural Information Processing Systems, NIPS’16, pp. 16231631, Red Hook, NY, USA, 2016.
Curran Associates Inc. ISBN 9781510838819.
Yann LeCun, Corinna Cortes, and Christopher J.C. Burges. The mnist database of handwritten
digits. URL http://yann.lecun.com/exdb/mnist/.
Yujia Li, Richard Zemel, Marc Brockschmidt, and Daniel Tarlow. Gated graph sequence neural
networks. In Proceedings of ICLR’16, April 2016.
Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter W. Battaglia. Learning deep gener-
ative models of graphs. ICML’18, abs/1803.03324, 2018.
Renjie Liao, Yujia Li, Yang Song, Shenlong Wang, Will Hamilton, David K Duvenaud, Raquel
Urtasun, and Richard Zemel. Efficient graph generation with graph recurrent attention networks.
In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.
Zinan Lin, Ashish Khetan, Giulia Fanti, and Sewoong Oh. Pacgan: The power of two samples in
generative adversarial networks. In Advances in Neural Information Processing Systems, vol-
ume 31. Curran Associates, Inc., 2018.
Jenny Liu, Aviral Kumar, Jimmy Ba, Jamie Kiros, and Kevin Swersky. Graph normalizing flows. In
Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.
Qi Liu, Miltiadis Allamanis, Marc Brockschmidt, and Alexander Gaunt. Constrained graph varia-
tional autoencoders for molecule design. In Advances in Neural Information Processing Systems,
volume 31. Curran Associates, Inc., 2018.
Jianxin Ma, Peng Cui, Kun Kuang, Xin Wang, and Wenwu Zhu. Disentangled graph convolutional
networks. In Proceedings of the 36th International Conference on Machine Learning, volume 97
of Proceedings ofMachine Learning Research, pp. 4212-4221. PMLR, 09-15 JUn 2019.
Chris J Maddison, Daniel Tarlow, and Tom Minka. A* sampling. In Advances in Neural Information
Processing Systems, volUme 27. CUrran Associates, Inc., 2014.
Haggai Maron, Heli Ben-HamU, Hadar Serviansky, and Yaron Lipman. Provably powerfUl graph
networks. In Advances in Neural Information Processing Systems, volUme 32. CUrran Associates,
Inc., 2019a.
Haggai Maron, Heli Ben-HamU, Nadav Shamir, and Yaron Lipman. Invariant and eqUivariant graph
networks. In International Conference on Learning Representations, 2019b.
Haggai Maron, Ethan Fetaya, Nimrod Segol, and Yaron Lipman. On the Universality of invariant
networks. In Proceedings of the 36th International Conference on Machine Learning, volUme 97
of Proceedings of Machine Learning Research, pp. 4363-4371. PMLR, 09-15 JUn 2019c.
Haggai Maron, Or Litany, Gal Chechik, and Ethan Fetaya. On learning sets of symmetric elements.
In Proceedings of the 37th International Conference on Machine Learning, volUme 119 of Pro-
ceedings of Machine Learning Research, pp. 6734-6744. PMLR, 13-18 JUl 2020.
12
Under review as a conference paper at ICLR 2022
F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. M. Bronstein. Geometric deep learn-
ing on graphs and manifolds using mixture model cnns. In 2017 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR),pp. 5425-5434, Los Alamitos, CA, USA,jul 2017.IEEE
Computer Society. doi: 10.1109/CVPR.2017.576.
Kevin P. Murphy. Chapter 19: Undirected graphical models (markov random fields). In Machine
Learning: A Probabilistic Perspective, pp. 663-707. MIT Press, 2012.
M. Niepert, M. Ahmed, and K. Kutzkov. Learning convolutional neural networks for graphs. In
Proceedings of the International Conference on Machine Learning, 2016a.
Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural net-
works for graphs. In Proceedings of The 33rd International Conference on Machine Learning,
volume 48 of Proceedings of Machine Learning Research, pp. 2014-2023, New York, New York,
USA, 20-22 Jun 2016b. PMLR.
B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. Pro-
ceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining (KDD), pp. 701-710, 2014.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks, 2016.
Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole von Lilienfeld. Quantum
chemistry structures and properties of 134 kilo molecules. Scientific Data, 1, 2014.
Rajesh Ranganath, Dustin Tran, and David Blei. Hierarchical variational models. In Proceedings of
The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine
Learning Research, pp. 324-333, New York, New York, USA, 20-22 Jun 2016. PMLR.
Lars Ruddigkeit, Ruud van Deursen, Lorenz C. Blum, and Jean-Louis Reymond. Enumeration
of 166 billion organic small molecules in the chemical universe database gdb-17. Journal of
Chemical Information and Modeling, 52(11):2864-2875, 2012. doi: 10.1021/ci300415d. PMID:
23088335.
Havard Rue and Leonhard Held. Gaussian markov random fields: Theory and applications. In
Monographs on Statistics and Applied Probability, volume 104, London, 2005. Chapman & Hall.
Raif Rustamov and Leonidas J Guibas. Wavelets on graphs via deep learning. In Advances in Neural
Information Processing Systems, volume 26. Curran Associates, Inc., 2013.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2009.
doi: 10.1109/TNN.2008.2005605.
Maximilian Seitzer. pytorch-fid: FID Score for PyTorch. https://github.com/mseitzer/
pytorch-fid, August 2020. Version 0.1.1.
P. Sen, G. M. Namata, M. Bilgic, L. Getoor, B. Gallagher, , and T. Eliassi-Rad. Collective classifi-
cation in network data. AI Magazine, 29(3):93-106, 2008.
Hadar Serviansky, Nimrod Segol, Jonathan Shlomi, Kyle Cranmer, Eilam Gross, Haggai Maron,
and Yaron Lipman. Set2graph: Learning graphs from sets. In Advances in Neural Information
Processing Systems, volume 33, pp. 22080-22091. Curran Associates, Inc., 2020.
Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M.
Borgwardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(77):
2539-2561, 2011.
Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang. Graphaf:
a flow-based autoregressive model for molecular graph generation. In International Confer-
ence on Learning Representations, 2020. URL https://openreview.net/forum?id=
S1esMkHYPr.
13
Under review as a conference paper at ICLR 2022
Donghyuk Shin, Si Si, and Inderjit S. Dhillon. Multi-scale link prediction. In Proceedings of
the 21st ACM International Conference on Information and Knowledge Management, CIKM
’12, pp. 215224, New York, NY, USA, 2012. Association for Computing Machinery. ISBN
9781450311564. doi: 10.1145/2396761.2396792.
Si Si, Donghyuk Shin, Inderjit S Dhillon, and Beresford N Parlett. Multi-scale spectral decompo-
sition of massive graphs. In Advances in Neural Information Processing Systems, volume 27.
Curran Associates, Inc., 2014.
Martin Simonovsky and Nikos Komodakis. Graphvae: Towards generation of small graphs using
variational autoencoders. CoRR, abs/1802.03480, 2018.
Akash Srivastava, Lazar Valkov, Chris Russell, Michael U. Gutmann, and Charles Sutton. Vee-
gan: Reducing mode collapse in gans using implicit variational learning. In Advances in Neural
Information Processing Systems, volume 30. Curran Associates, Inc., 2017.
Teague Sterling and John J. Irwin. Zinc 15 ligand discovery for everyone. Journal of Chemical
Information and Modeling, 55(11):2324-2337, 2015. doi: 10.1021∕acs.jcim.5b00559. PMID:
26479676.
L. Tang and H. Liu. Leveraging social media networks for classification. Data Mining and Knowl-
edge Discovery, 23(3):447-478, 2011.
Erik Henning Thiede, Truong Son Hy, and Risi Kondor. The general theory of permutation equiv-
arant neural networks and higher order graph variational encoders. CoRR, abs/2004.03990, 2020.
Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical variational autoencoder. In Advances in
Neural Information Processing Systems, volume 33, pp. 19667-19679. Curran Associates, Inc.,
2020.
Petar Velikovi, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li, and Yoshua Ben-
gio. Graph attention networks. In International Conference on Learning Representations, 2018.
Martin J. Wainwright and Michael I. Jordan. A variational principle for graphical models. New
Directions in Statistical Signal Processing, 2005.
Hao Wu, Jonas Kohler, and Frank Noe. Stochastic normalizing flows. In Advances in Neural
Information Processing Systems, volume 33, pp. 5933-5944. Curran Associates, Inc., 2020.
Bingbing Xu, Huawei Shen, Qi Cao, Yunqi Qiu, and Xueqi Cheng. Graph wavelet neural network.
In International Conference on Learning Representations, 2019.
Yiding Yang, Zunlei Feng, Mingli Song, and Xinchao Wang. Factorizable graph convolutional
networks. In Advances in Neural Information Processing Systems, volume 33, pp. 20286-20296.
Curran Associates, Inc., 2020.
Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. Hi-
erarchical graph representation learning with differentiable pooling. In Advances in Neural Infor-
mation Processing Systems, volume 31. Curran Associates, Inc., 2018.
Jiaxuan You, Rex Ying, Xiang Ren, William Hamilton, and Jure Leskovec. GraphRNN: Generat-
ing realistic graphs with deep auto-regressive models. In Proceedings of the 35th International
Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp.
5708-5717. PMLR, 10-15 Jul 2018a.
Jiaxuan You, Rex Ying, Xiang Ren, William L. Hamilton, and Jure Leskovec. Code for graphrnn:
Generating realistic graphs with deep auto-regressive model. 2018b.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and
Alexander J Smola. Deep sets. In Advances in Neural Information Processing Systems, vol-
ume 30. Curran Associates, Inc., 2017.
Dawei Zhou, Lecheng Zheng, Jiejun Xu, and Jingrui He. Misc-gan: A multi-scale generative model
for graphs. Frontiers in Big Data, 2:3, 2019. ISSN 2624-909X. doi: 10.3389/fdata.2019.00003.
URL https://www.frontiersin.org/article/10.3389/fdata.2019.00003.
14
Under review as a conference paper at ICLR 2022
A Basic tensor operations
In order to build higher order equivariant networks, we revisit some basic tensor operations: tensor
product (see Def. 10) and tensor contraction (see Def. 11). It can be shown that these tensor opera-
tions respect permutation equivariance. Based on them, we build our second order message passing
networks.
Definition 10. The tensor product of A ∈ Rna with B ∈ Rnb yields a tensor C = A 0 B ∈ Rna+b
where
Ci1,i2,..,ia+b = Ai1,i2,..,iaBia+1,ia+2,..,ia+b
Definition 11. The contraction of A ∈ Rna along the pair of dimensions {x, y} (assuming x < y)
yields a (a - 2)-th order tensor
Ci1,..,ix-1,j,ix+1,..,iy-1,j,iy+1,..,ia = ∑ Ai1,..,ia
ix ,iy
where we assume that ix and iy have been removed from amongst the indices of C. Using Einstein
notation, this can be written more compactly as
C{i1,i2,..,ia}∖{ix,iy} = Ai1,i2,..,iaδix,iy
where δ is the Kronecker delta. In general, the contraction ofA along dimensions {x1, .., xp} yields
a tensor C = AlT T ∈ Rna P where
x1 ,..,xp
Alx1,..,xp = ∑ ∑... ∑ Ai1,i2,..,i
ix1 ix2	ixp
a
or compactly as
Alx x = Ai1,i2,..,iaδix1,ix2,..,ixp
x1 ,..,xp	1 , 2 ,.., a
B Markov Random Fields
Undirected graphical models have been widely applied in the domains spatial or relational data,
such as image analysis and spatial statistics. In general, k-th order graph encoders encode an undi-
rected graph G = (V, E) into a k-th order latent z ∈ Rnk ×dz, with learnable parameters θ, can be
represented as a parameterized Markov Random Field (MRF) or Markov network. Based on the
Hammersley-Clifford theorem (Murphy, 2012) (Koller & Friedman, 2009), a positive distribution
p(z ) > 0 satisfies the conditional independent properties of an undirected graph G iff p can be
represented as a product of potential functions ψ, one per maximal clique, i.e.,
p(z∣θ)=焉 ∏C ψc(zc∣θc)
(10)
where C is the set of all the (maximal) cliques of G, and Z(θ) is the partition function to ensure the
overall distribution sums to 1, and given by
Z(θ) = ∑∏ ψc(zc∣θc)
z c∈C
Eq. 10 can be further written down as
p(z∣θ) J ∏ ψv (zv∣θ) ∏ ψst(zst∣θ)…	∏	ψc(zc∣θ)
v∈V	(s,t)∈E	c=(i1,..,ik)∈Ck
where ψv , ψst, and ψc are the first order, second order and k-th order outputs of the encoder, cor-
responding to every vertex in V, every edge in E and every clique of size k in Ck, respectively.
However, factorizing a graph into set of maximal cliques has an exponential time complexity, since
the problem of determining if there is a clique of size k in a graph is known as an NP-complete
problem. Thus, the factorization based on Hammersley-Clifford theorem is intractable. The second
order encoder relaxes the restriction of maximal clique into edges, that is called as pairwise MRF:
p(z∣θ) J ∏ψst(zs, zt)
s~t
15
Under review as a conference paper at ICLR 2022
Our second order encoder inherits Gaussian MRF introduced by (Rue & Held, 2005) as pairwise
MRF of the following form
P(ZIe) IX ∏ψst(Zs,Zt) ∏ψt(Zt)
s~t	t
where ψst(zs,zt) = exp ( — 2ZsΛstZt) is the edge potential, and ψt(zt) = exp ( — 1 Λttz2 + ηtzt)
is the vertex potental. The joint distribution can be written in the information form of a multivariate
Gaussian in which
Λ=Σ-1
p(z∣θ)
η = Λμ
X exp(ηTz-
2 ZT Az)
(11)
Sampling Z fromp(z∣θ) in Eq. 11is the same as sampling from the multivariate GaussianN(μ, Σ).
To ensure end-to-end equivariance, We set the latent layer to be two tensors μ ∈ Rn×dz and Σ ∈
Rn×n×dz that corresponds to dz multivariate Gaussians, whose first index, and second index are
first order and second order equivariant with permutations. Computation of Σ is trickier than μ,
simply because Σ must be invertible to be a covariance matrix. Thus, our second order encoder
produces tensor L as the second order activation, and set Σ = LLT . The reparameterization trick
from Kingma & Welling (2014) is changed to
z = μ + La e Z N(0,1)
C Equivariant learnable prior
The original VAE published by Kingma & Welling (2014) limits each covariance matrix Σ to be
diagonal and the prior to be N(0, 1). Our second order encoder removes the diagonal restriction on
the covariance matrix. Furthermore, we allow the prior N(μ, Σ) to be learnable in which μ and
Σ are parameters optimized by back propagation in a data driven manner. Importantly, Σ cannot
be learned directly due to the invertibility restriction. Instead, similarly to the second order encoder,
T
a matrix L is optimized, and the prior covariance matrix is constructed by setting Σ = LL . The
Kullback-Leibler divergence between the two distributions N(μ, Σ) and N(μ, Σ) is as follows:
DKL(N(μ,∑)∣∣N(μ, ∑))=
2卜Mς ς) + (μ-从)丁£ (μ-μ)-n + ln(：：：个))
2	det Σ
(12)
Even though Σ is invertible, but gradient computation through the KL-divergence loss can be nu-
merical instable because of Cholesky decomposition procedure in matrix inversion. Thus, we add
neglectable noise = 10-4 to the diagonal of both covariance matrices.
Importantly, during training, the KL-divergence loss breaks the permutation equivariance. Suppose
the set of vertices are permuted by a permutation matrix Pσ for σ ∈ Sn. Since μ and Σ are the first
order and second order equivariant outputs of the encoder, they are changed to Pσ μ and Pσ ∑P：
accordingly. But
Dkl(N(μ, ∑)∣∣N(μ, ∑)) ≠ DKL(N(Pσμ,Pσ∑PT)∣∣N(μ, ∑))
To address the equivariance issue, we want to solve the following convex optimization problem that
is our new equivariant loss function
min DKL(N(Pσμ,Pσ∑PT)∣∣N(μ, ∑))	(13)
σ∈Sn
However, solving the optimization based on Eq. 13 is computationally expensive. One solution
is to solve the minimum-cost maximum-matching in a bipartite graph (Hungarian matching) with
the cost matrix C j = ∣∣μi 一 μ^j∣∣ by O(n4) algorithm published by Edmonds & Karp (1972), that
can be still improved further into O(n3). The Hungarian matching preserves equiariance, but is
still computationally expensive. In practice, instead of finding a optimal permutation, we apply
a free-matching scheme to find an assignment matrix Π such that: Πj = 1 if and only if j* =
argminj ∣∣μi — μj∣∣, for each i ∈ [1, n]. The free-matching scheme preserves equivariance and can
be done efficiently in a simple O(n2) algorithm that is also suitable for GPU computation.
16
Under review as a conference paper at ICLR 2022
D	Experiments
D. 1 Link prediction on citation graphs
We demonstrate the ability of the MGVAE models to learn meaningful latent embeddings on a link
prediction task on popular citation network datasets Cora and Citeseer (Sen et al., 2008). At training
time, 15% of the citation links (edges) were removed while all node features are kept, the models are
trained on an incomplete graph Laplacian constructed from the remaining 85% of the edges. From
previously removed edges, we sample the same number of pairs of unconnected nodes (non-edges).
We form the validation and test sets that contain 5% and 10% of edges with an equal amount of
non-edges, respectively.
We compare our model MGVAE against popular methods in the field:
1.	Spectral clustering (SC) (Tang & Liu, 2011)
2.	Deep walks (DW) (Perozzi et al., 2014)
3.	Variational graph autoencoder (VGAE) (Kipf & Welling, 2016)
on the ability to correctly classify edges and non-edges using two metrics: area under the ROC curve
(AUC) and average precision (AP). Numerical results of SC and DW are experimental settings are
taken from (Kipf & Welling, 2016). We reran the implementation of VGAE as in (Kipf & Welling,
2016).
For MGVAE, we initialize weights by Glorot initialization (Glorot & Bengio, 2010). We repeat the
experiments with 5 different random seeds and calculate the average AUC and AP along with their
standard deviations. The number of message passing layers ranges from 1 to 4. The size of latent
representation is 128. The number of coarsening levels is L ∈ {3, 7}. In the `-th coarsening level, we
partition the graph G⑹ into 2' (for L = 7) or 4' (for L = 3) clusters. We train for 2,048 epochs using
Adam optimization (Kingma & Ba, 2015) with a starting learning rate of 0.01. Hyperparameters
optimization (e.g. number of layers, dimension of the latent representation, etc.) is done on the
validation set. MGVAE outperforms all other methods (see Table 3).
We propose our learning to cluster algorithm to achieve the balanced K-cut at every resolution level.
Besides, we also implement two fixed clustering algorithms:
1.	Spectral: It is similar to the one implemented in (Rustamov & Guibas, 2013).
•	First, we embed each node i ∈ V into Rnmax as (ξι(i)∕λι(i),.., ξnw,aχ ⑴/》η小.，(i)),
where {λn, ξn}nn=m0ax are the eigen-pairs of the graph Laplacian L = D-1 (D-A) where
Dii = ∑j Aij. We assume that λ0 ≤ .. ≤ λnmax. In this case, nmax = 10.
•	At the `-th resolution level, we apply the K-Means clustering algorithm based on the
above node embedding to partition graph G(').
2.	K-Means:
•	First, we apply PCA to compress the sparse word frequency vectors (of size 1,433 on
Cora and 3,703 on Citeseer) associating with each node into 10 dimensions.
•	We use the compressed node embedding for the K-Means clustering.
Tables 4 and 5 show that our learning to cluster algorithm returns a much more balanced cut on
the highest resolution level comparing to both Spectral and K-Means clusterings. For instance, we
Table 3: Citation graph link prediction results (AUC & AP)
Dataset	Cora		Citeseer	
Method	AUC (ROC)	AP	AUC (ROC)	AP
SC	84.6 ± 0.01	88.5 ± 0.00	80.5 ± 0.01	85.0 ± 0.01∏
-DW	83.1 ± 0.01	85.0 ± 0.00	80.5 ± 0.02	83.6 ± 0.01
VGAE	90.97 ± 0.77	91.88 ± 0.83	89.63 ± 1.04	91.10 ± 1.02
MGVAE (Spectral)	91.19 ± 0.76	92.27 ± 0.73	90.55 ± 1.17	91.89 ± 1.27
MGVAE (K-Means)	93.07 ± 5.61	92.49 ± 5.77	90.81 ± 1.19	91.98 ± 1.02
MGVAE	95.67 ± 3.11一	95.02 ± 3.36一	93.93 ± 5.87一	93.06 ± 6.33一
17
Under review as a conference paper at ICLR 2022
Method	Min	Max	STD	KL divergence
SPeCtral	1~	2020	177.52	3.14 —
K-MeanS	-Γ~	364	40.17	0.84
Learn to cluster		36	4.77	0.02	—
Table 4: Learning to cluster algorithm returns balanced cuts on Cora.
Method	Min	Max	STD	KL divergence
SPeCtral	Γ~	3320	292.21	4.51	-
K-MeanS	-Γ~	326	41.69	074
Learn to cluster		38	4.93	0.01	—
Table 5: Learning to cluster algorithm returns balanced cuts on Citeseer.
have L = 7 resolution levels and We partition the '-th resolution into K = 2' clusters. Thus, on the
bottom levels, we have 128 clusters. If we distribute nodes into clusters uniformly, the expected
number of nodes in a cluster is 21.15 and 25.99 on Cora (2, 708 nodes) and Citeseer (3, 327 nodes),
respectively. We measure the minimum, maximum, standard deviation of the numbers of nodes in
128 clusters. Furthermore, we measure the KUllback-Leibler divergence between the distribution of
nodes into clusters and the uniform distribution. Our learning to cluster algorithm achieves loW KL
losses of 0.02 and 0.01 on Cora and Citeseer, respectively.
D.2 Molecular graph generation
In this case, MGVAE and MGN are implemented with L = 3 resolution levels, and the `-th resolution
graph is partitioned into K = 2'-1 clusters. On each resolution level, the local encoders and local
decoders are second-order Sn-equivariant networks with up to 4 equivariant layers. The number of
channels for each node latent dz is set to 256. We apply two approaches for graph decoding:
1.	All-at-once: MGVAE reconstructs all resolution adjacencies by equivariant decoder net-
works. Furthermore, we apply learnable equivariant prior as in Sec. C. Our second order
encoders are interpreted as Markov Random Fields (see Sec. B). This approach preserves
permutation equivariance. In addition, we implement a correcting process: the decoder
network of the highest resolution level returns a probability for each edge, we sort these
probabilities in a descending order and gradually add the edges in that order to satisfy all
chemical constraints. Furthermore, we investigate the expressive power of the second order
Sn -equivariant decoder by replacing it by a multilayer perceptron (MLP) decoder with 2
hidden layers of size 512 and sigmoid nonlinearity. We find that the higher order decoder
outperforms the MLP decoder given the same encoding architecture. Table 6 shows the
comparison between the two decoding models.
2.	Autoregressive: This decoding process is constructed in an autoregressive manner simi-
larly to (Liu et al., 2018). First, we sample each vertex latent z independently. We randomly
select a starting node v0, then we apply Breath First Search (BFS) to determine a particular
node ordering from the node v0, however that breaks the permutation equivariance. Then
iteratively we add/sample new edge to the existing graph Gt at the t-th iteration (given a
randomly selected node v0 as the start graph G0) until completion. We apply second-order
MGN with gated recurrent architecture to produce the probability of edge (u, v ) where one
vertex u is in the existing graph Gt and the another one is outside; and also the probability
of its label. Intuitively, the decoding process is a sequential classification.
We randomly select 10,000 training examples for QM9; and 1,000 (autoregressive) and 10,000 (all-
at-once) training examples for ZINC. It is important to note that our training sets are much smaller
comparing to other methods. For all of our generation experiments, we only use graph features as
the input for the encoder such as one-hot atomic types and bond types. Since ZINC molecules are
larger then QM9 ones, it is more difficult to train with the second order Sn-equivariant decoders
(e.g., the number of bond/non-bond predictions or the number of entries in the adjacency matrices
are proportional to squared number of nodes). Therefore, we input several chemical/atomic features
18
Under review as a conference paper at ICLR 2022
Dataset	Method	Validity	Novelty	Uniqueness
QM9	MLP decoder	100%	99.98%	77：62% 1
	-Sn decoder-	100%	100%	95.16% ―
Table 6: All-at-once MGVAE with MLP decoder vs. second order decoder.
Feature	Type	Number	Description
GetAtOmicNum	Integer	1	Atomic number
IsInRing	Boolean	1	Belongs to a ring?
IsInRingSize	Boolean	9	Belongs to aring of size k ∈ {1,.., 9}?
GetIsArOmatic	Boolean	1	Aromaticity?
GetDegree	Integer	1	Vertex degree
GetExplicitValance	Integer	1	Explicit valance
GetFOrmalCharge	Integer	1	Formal charge
GetIsotope	Integer	1	Isotope
GetMass	Double	1	Atomic mass
GetNoImplicit	Boolean	1	Allowed to have implicit Hs?
GetNumExplicitHs	Integer	1	Number of explicit Hs
GetNumImplicitHs	Integer	1	Number of implicit Hs
GetNumRadicalElectrons	Integer	1	Number of radical electrons
GetTotalDegree	Integer	1	Total degree
GetTotalNumHs	Integer	1	Total number of Hs
GetTotalValence	Integer	1	Total valance
Table 7: The list of chemical/atomic features used for the all-at-once MGVAE on ZINC. We denote
each feature by its API in RDKit.
computed from RDKit for the all-at-once MGVAE on ZINC (see Table 7). We concatenate all these
features into a vector of size 24 for each atom.
We train our models with Adam optimization method (Kingma & Ba, 2015) with the initial learning
rate of 10-3. Figs. 4 and 5 show some selected examples out of 5,000 generated molecules on QM9
by all-at-once MGVAE, while Fig. 6 shows the molecules generated by autoregressive MGN. Qual-
itatively, both the decoding approaches capture similar molecular substructures (bond structures).
Fig. 9 shows an example of interpolation on the latent space on ZINC with the all-at-once MGVAE.
Fig. 7 shows some generated molecules on ZINC by the all-at-once MGVAE. Fig. 8 and table 8
show some generated molecules by the autoregressive MGN on ZINC dataset with high Quantita-
tive Estimate of Drug-Likeness (QED) computed by RDKit and their SMILES strings. On ZINC,
the average QED score of the generated molecules is 0.45 with standard deviation 0.21. On QM9,
the QED score is 0.44 ± 0.07.
D.3 General graph generation by MGVAE
Figs. 10 11 show some generated examples and training examples on the 2-community and ego
datasets, respectively.
D.4 Unsupervised molecular properties prediction on QM9
Density Function Theory (DFT) is the most successful and widely used approach of modern quan-
tum chemistry to compute the electronic structure of matter, and to calculate many properties of
molecular systems with high accuracy (Hohenberg & Kohn, 1964). However, DFT is computa-
tionally expensive (Gilmer et al., 2017a), that leads to the use of machine learning to estimate the
properties of compounds from their chemical structure rather than computing them explicitly with
DFT (Hy et al., 2018). To demonstrate that MGVAE can learn a useful molecular representations
and capture important molecular structures in an unsupervised and variational autoencoding manner,
we extract the highest resolution latents (at ` = L) and use them as the molecular representations
for the downstream tasks of predicting DFT’s molecular properties on QM9 including 13 learning
19
Under review as a conference paper at ICLR 2022
Figure 4: Some generated examples on QM9 by the all-at-once MGVAE with second order Sn-
equivariant decoders.
Figure 5: Some generated examples on QM9 by the all-at-once MGVAE with a MLP decoder
instead of the second order Sn-equivariant one. It generates more tree-like structures.
20
Under review as a conference paper at ICLR 2022
Figure 6: Some generated examples on QM9 by the autoregressive MGN.
Figure 7: Some generated examples on ZINC by the all-at-once MGVAE with second order Sn -
equivariant decoders. In addition of graph features such as one-hot atomic types, we include several
chemical features computed from RDKit (as in Table 7) as the input for the encoders. A generated
example can contain more than one connected components, each of them is a valid molecule.
21
Under review as a conference paper at ICLR 2022
QED = 0.711
QED = 0.715
QED = 0.756
QED = 0.751
QED = 0.879
QED = 0.805
QED = 0.742
QED = 0.769
QED = 0.710
QED = 0.790
QED = 0.850
QED = 0.859
QED = 0.730
QED = 0.703
QED = 0.901
QED = 0.855
QED = 0.786
QED = 0.895
QED = 0.729
QED = 0.809
Figure 8: Some generated molecules on ZINC by the autoregressive MGN with high QED (drug-
likeness score).
22
Under review as a conference paper at ICLR 2022
Row	Column	SMILES
1	1	O=CINC(CCCF)C2[nH]nnc21
	2	OCC(OSBr)C1ccc(-c2cccc(Cl)c2Cl)[nH]1
	3	C=CCI=CC=C2c(cc(=C3ONC(Cl)=C3Cl)[nH]c2=O)O1
	4	COC(=CN1NC=CN1)C=C1C=CC(Cl)=CO1
2	1	[NH-]C(CNS1(=O)=NNc2c(F)cccc21)C1CC1
	2	CS(=O)N1CC[SH](C)C(CNCc2ccccc2)C1
	3	C=C(Cl)[SH](=O)(NC)C1c2ccc(Cl)c(n2)CC1O
	4	CC(F)C(=C1C[NH2+]C([O-])N1)S(=O)Cc1ccccc1
3	1	CCI(NC2=CONN2c2ccccc2)C=C(C=O)N[N-]1
	2	CC(=O)NN1N=C(C(O)c2cccc3ccοc23)C(=O)C1=O
	3	C=CC(C)=CIC(F)=CC(C=C2ONN=NS2=O)=C1SCl
	4	CCNION=C(C=C(Cl)C2ccco2)C(F)(F)C1=O
4	1	O=C(CCN(C1[nH+]cc(S)s1)c1ccc2cc1SC2)C1=NCC=C1Cl
	2	CC=CNC1=C2Oc3ccccc3C(C)S2=S=N1
	3	O=C(SCI=CC=NS1(=O)=O)CICCC(Cl)CC1S1=NN=NN=N1
	4	COCCNCC1CC2CCCCC2[nH]1
5	1	ClC=CICON=C(C2nCno2)N1CC(Cl)(Br)Br
	2	CS(=O)(=O)dCCC[nH+]C1SNCdCCCCC1Cl
	3	O=S1(=O)CNS(=O)(N(c2ccccc2F)c2ccccc2C1)=N1
	4	O=CINS(C2ccccc2C1)=S2(=NSN=N2)O1
Table 8: SMILES of the generated molecules included in Fig. 8. Online drawing tool: https:
//pubchem.ncbi.nlm.nih.gov//edit3/index.html
Figure 9: Interpolation on the latent space: we randomly select two molecules from ZINC and we
reconstruct the corresponding molecular graphs on the interpolation line between the two latents.
Figure 10: The top row includes generated examples and the bottom row includes training examples
on the synthetic 2-community dataset.
23
Under review as a conference paper at ICLR 2022
Generated examples
Training examples
Figure 11: EGO-SMALL.
24
Under review as a conference paper at ICLR 2022
Target	Unit	Mean	STD	Description
α	bohr3	75.2808	8.1729	Norm of the static polarizability
,^CV	cal/mol/K	31.6204	4.0674	Heat capacity at room temperature
G	eV	-70.8352	9.4975	Free energy of atomization
gap	eV	6.8583	1.2841	Difference between HOMO and LUMO
H	eV	-77.0167	10.4884	Enthalpy of atomization at room temperature
HOMO	eV	-6.5362	0.5977	Highest occupied molecular orbital
LUMO	eV	0.3220	1.2748	Lowest unoccupied molecular orbital
上		D	2.6729	1.5034	Norm of the dipole moment
ωι	CmT	3504.1155	266.8982	Highest fundamental vibrational frequency
^R2	bohr2	1189.4091	280.4725	Electronic spatial extent
-U	eV	-76.5789	10.4143	Atomization energy at room temperature
^U0	eV	-76.1145	10.3229	Atomization energy at0K
ZPVE	eV	4.0568	0.9016	Zero point vibrational energy
Table 9: Description and statistics of 13 learning targets on QM9.
	alpha	Cv	G	gap	H	HOMO	LUMO	mu	omega1	R2	U	U0	ZPVE
IWL	T75^	^Σ39"	T8Γ	^O2"	ʒɪ	0.38	0.89	τ0r	192	154	T4Γ	ʒɪ	0.51
'^NGF	3.51	T9T	136	^O6"	T92"	0.34	0.82	^094	-168-	137"	T89"	lɪ	0.45
'PSCN-	1.63	T09"	TΓ3"	^077"	336"	0.30	0.75	^08Γ	152	^6T	334	^3.50^	0.38
CCN 2D	T.3F	^093"	TTT	^069"	TTT	0.23	0.67	T72"	120	^53^	T02"	T99	0.35
MGVAE	2.83	T91	T78	T66	T87	0.34	0.58	^O5^	195	-90^	T89	T90^	0.14
Table 10: Unsupervised molecular representation learning by MGVAE to predict molecular prop-
erties calculated by DFT on QM9 dataset.
targets. For the training, we normalize all learning targets to have mean 0 and standard deviation 1.
The name, physical unit, and statistics of these learning targets are detailed in Table 9.
The implementation of MGVAE is the same as detailed in Sec. D.2. MGVAE is trained to reconstruct
the highest resolution (input) adjacency, its coarsening adjacencies and the node atomic features. In
this case, we do not use any chemical features: the node atomic features are just one-hot atomic
types. After MGVAE is converged, to obtain the Sn-invariant molecular representation, we average
the node latents at the L-th level into a vector of size 256. Finally, we apply a simple Multilayer
Perceptron with 2 hidden layers of size 512, sigmoid nonlinearity and a linear layer on top to predict
the molecular properties based on the extracted molecular representation. We compare the results
in Mean Average Error (MAE) in the corresponding physical units with four methods on the same
split of training and testing from (Hy et al., 2018):
1.	Support Vector Machine on optimal-assignment Weisfeiler-Lehman (WL) graph kernel
(Shervashidze et al., 2011) (Kriege et al., 2016)
2.	Neural Graph Fingerprint (NGF) (Duvenaud et al., 2015)
3.	PATCHY-SAN (PSCN) (Niepert et al., 2016a)
4.	Second order Sn-equivariant Covariant Compositional Networks (CCN 2D) (Kondor et al.,
2018) (Hy et al., 2018)
Our unsupervised results show that MGVAE is able to learn a universal molecular representation in
an unsupervised manner and outperforms WL in 12, NGF in 10, PSCN in 8, and CCN 2D in 8 out
of 13 learning targets, respectively (see Table 10). There are other recent methods in the field that
use several chemical and geometric information but comparing to them would be unfair.
D.5 Supervised molecular properties prediction on ZINC
To further demonstrate the comprehensiveness of MGN, we apply our model in a supervised regres-
sion task to predict the solubility (LogP) on the ZINC dataset. We use the same split of 10K/1K/1K
25
Under review as a conference paper at ICLR 2022
Method	MLP	GCN	GAT	MoNet	DiscenGCN	FactorGCN	GatedGCNE	MGN
MAE	0.667	0.503	0.479	0.407	0.538	—	0.366 一	0.363 —	0.290
Table 11: Supervised MGN to predict solubility on ZINC dataset.
for training/validation/testing as in (Dwivedi et al., 2020). The implementation of MGN is almost
the same as detailed in Sec. D.2, except we include the latents of all resolution levels into the pre-
diction. In particular, in each resolution level, we average all the node latents into a vector of size
256; then we concatentate all these vectors into a long vector of size 256 × L and apply a linear layer
for the regression task. The baseline results are taken from (Yang et al., 2020) including:
1.	Multilayer Perceptron (MLP),
2.	Graph Convolution Networks (GCN),
3.	Graph Attention Networks (GAT) (Velikovi et al., 2018),
4.	MoNet (Monti et al., 2017),
5.	Disentangled Graph Convolutional Networks (DisenGCN) (Ma et al., 2019),
6.	Factorizable Graph Convolutional Networks (FactorGCN) (Yang et al., 2020),
7.	GatedGCNE (Dwivedi et al., 2020) that uses additional edge information.
Our supervised result shows that MGN outperforms the state-of-the-art models in the field with a
margin of 20% (see Table 11).
D.6 Graph-based image generation by MGVAE
In this additional experiment, we apply MGVAE into the task of image generation. Instead of matrix
representation, an image I ∈ RH×W is represented by a grid graph of H ∙ W nodes in which each
node represents a pixel, each edge is between two neighboring pixels, and each node feature is the
corresponding pixel’s color (e.g., R1 in gray scale, and R3 in RGB scale). Fig. 12 demonstrates an
exmaple of graph representation for images. Since images have natural spatial clustering, instead of
learning to cluster, we implement a fixed clustering procedure as follows:
•	For the '-th resolution level, We divide the grid graph of size H(') × W (') into clusters
of size h × W that results into a grid graph of size H(') × W('), SUPPoSingly h and W are
divisible by H⑹ and W ('), respectively. Each resolution is associated with an image I(')
that is a zoomed out version of I ('+1).
•	The global encoder e(`) is implemented with 10 layers of message passing that operates
on the whole H ⑹ × W (') grid graph. We sum up all the node latents into a single latent
vector Z ⑹ ∈ Rdz. The global decoder d(') is implemented by the convolutional neural
network architecture of the generator OfDCGAN model (Radford et al., 2016) to map Z(')
into an approximated image I⑹.The Sn-invariant pooler p(') is a network operating on
each small h × W grid graph to produce the corresponding node feature for the next level
` + 1. MGVAE is trained to reconstruct all resolution images. Fig. 13 shows an example of
reconstruction at each resolution on a test image of MNIST (after the network converged).
We evaluate our MGVAE architecture on the MNIST dataset (LeCun et al.) with 60,000 training
examples and 10,000 testing examples. The original image size is 28 × 28. We pad zero pixels to get
the image size of 25 × 25 (e.g., H(5) = W(5) = 32). Each cluster is a small grid graph of size 2 × 2
(e.g., h = W = 2). Accordingly, the image sizes for all resolutions are 32 × 32, 16 × 16, 8 × 8, etc. In
this case, the whole network architecture is a 2-dimensional quadtree. The latent size dz is selected
as 256. We train our model for 256 epochs by Adam optimizer (Kingma & Ba, 2015) with the initial
learning rate 10-3. In the testing process, for the `-th resolution, we sample a random vector of size
dz from prior N (0, 1) and use the decoder d(`) to decode the corresponding image. We generate
10,000 examples for each resolution. We compute the Frechet Inception Distance (FID) proposed by
(Heusel et al., 2017) between the testing set and the generated set as the metric to evaluate the quality
26
Under review as a conference paper at ICLR 2022
Method	FID； (32 X 32)	FIDl (16 X 16)	FID; (8 X 8)
DCGAN	113.129 =	N/A	N/A
VEEGAN	68.749		
PACGAN	58.535		
PresGAN	42.019		
MGVAE	39.474 —	64.289 —	39.038 —
Table 12: Quantitative evaluation of the generated set by FID metric for each resolution level on
MNIST. It is important to note that the generation for each resolution is done separately: for the `-th
resolution, we sample a random vector of size dz = 256 from N (0, 1), and use the global decoder
d(`) to decode into the corresponding image size. The baselines are taken from (Dieng et al., 2019).
Figure 12: An image of digit 8 from MNIST (left) and its grid graph representation at 16 × 16
resolution level (right).
of our generated examples. We use the FID implementation from (Seitzer, 2020). We compare our
MGVAE against variants of Generative Adversarial Networks (GANs) (Goodfellow et al., 2014)
including DCGAN (Radford et al., 2016), VEEGAN (Srivastava et al., 2017), PacGAN (Lin et al.,
2018), and PresGAN (Dieng et al., 2019). Table 12 shows our quantitative results in comparison
with other competing generative models. The baseline results are taken from Prescribed Generative
Adversarial Networks paper (Dieng et al., 2019). MGVAE outperforms all the baselines for the
highest resolution generation. Figs. 14 and 15 show some generated examples of the 32 × 32 and
16 × 16 resolution, respectively.
27
Under review as a conference paper at ICLR 2022
32 X 32	16 X 16	8 X 8
Target
Reconstruction
Figure 13: An example of reconstruction on each resolution level for a test image in MNIST.
/ ^ / 1 …，「7 7 d 2 3
H6;3^7Γ3lT77
CC 3 932894，AS
4201 )。 5?夕彳49
74∕^5∂^9∕33∕
3 3 I (jOga 夕 374
施。；ZSmqeg夕多7以
9 6g∕Q53Z75 彳 S
夕 3。453«39/。彳
弓乡6&Z邑5，。097
Figure 14: Generated examples at the highest 32 X 32 resolution level.
28
Under review as a conference paper at ICLR 2022
Figure 15: Generated examples at the 16 × 16 resolution level.
29