Under review as a conference paper at ICLR 2022
Towards	understanding how momentum im-
PROVES GENERALIZATION IN DEEP LEARNING
Anonymous authors
Paper under double-blind review
Ab stract
Stochastic gradient descent (SGD) with momentum is widely used for training
modern deep learning architectures. While it is well understood that using momen-
tum can lead to faster convergence rate in various settings, it has also been observed
that momentum yields higher generalization. Prior work argue that momentum
stabilizes the SGD noise during training and this leads to higher generalization. In
this paper, we take the opposite view to this result and first empirically show that
gradient descent with momentum (GD+M) significantly improves generalization
comparing to gradient descent (GD) in many deep learning tasks. From this obser-
vation, we formally study how momentum improves generalization in deep learning.
We devise a binary classification setting where a two-layer (over-parameterized)
convolutional neural network trained with GD+M provably generalizes better than
the same network trained with vanilla GD, when both algorithms start from the
same random initialization. The key insight in our analysis is that momentum is
beneficial in datasets where the examples share some features but differ in their
margin. Contrary to the GD model that memorizes the small margin data, GD+M
can still learn the features in these data thanks to its historical gradients. We also
empirically verify this learning process of momentum in real-world settings.
1 Introduction
It is commonly accepted that adding momentum to an optimization algorithm is required to optimally
train a large-scale deep network. Most of the modern architectures maintain during the training
process a heavy momentum close to 1 (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014;
He et al., 2016; Zagoruyko & Komodakis, 2016). Indeed, it has been empirically observed that
architectures trained with momentum outperform those which are trained without (Sutskever et al.,
2013). Several papers have attempted to explain this phenomenon. From the optimization perspective,
Defazio (2020) assert that momentum yields faster convergence of the training loss since, at the
early stages, it cancels out the noise from the stochastic gradients. On the other hand, Leclerc &
Madry (2020) empirically observes that momentum yields faster training convergence only when
the learning rate is small. While these works shed light on how momentum acts on neural network
training, they fail to capture the generalization improvement induced by momentum (Sutskever et al.,
2013). Besides, the noise reduction property of momentum advocated by Defazio (2020) seems to
even contradict the observation that, in deep learning, having a large noise in the training improves
generalization (Li et al., 2019; HaoChen et al., 2020). To the best of our knowledge, there is no
existing work which theoretically explains how momentum improves generalization in deep learning.
Therefore, this paper aims to close this gap and addresses the following question:
Is the higher generalization induced by momentum tied to the stochastic noise of the gradient? If not,
what is the underlying mechanism of momentum improving generalization in deep learning?
In this paper, we empirically verify that the generalization improvement induced by momentum is
not tied to the stochasticity of the gradient. Indeed, as reported in Figure 1, momentum improves
generalization more significantly for full batch GD than for SGD in CIFAR object recognition tasks.
Motivated by this empirical observation and the fact that the stochastic noise influences generalization,
we theoretically study how gradient descent with momentum (GD+M) can generalize better than
vanilla gradient descent (GD). We therefore only focus on the contribution of momentum of the true
gradient on generalization.
The question we address concerns algorithmic regularization which characterizes the generalization
of an optimization algorithm when multiple global solutions exist in over-parameterized deep learning
1
Under review as a conference paper at ICLR 2022
	CIFAR-10		CIFAR-100	
	Test	Ratio	Test	Ratio
R18	75.83/84.68	1.11	43.32/51.99	1.20
WR16	75.02/84.48 —	1.12	42.95/51.33 —	1.20
(a)
	CIFAR-10		CIFAR-100	
	Test	Ratio	Test	Ratio
R18	86.15/85.91	0.99	53.81/58.01	1.08
WR16	84.83/87.85 —	1.04	55.09/60.83 —	1.10
(b)
(c)
Number of epochs
(d)
Figure 1: Test accuracy obtained with Resnet-18 (R18) and WideResnet16 (WR16) on CIFAR-10 and CIFAR-
100. The architectures are trained using GD/GD+M (a) and SGD/ SGD+M (b) for 300 epochs to ensure zero
training error. (c)-(d) respectively display the training loss and test accuracy by R18 with GD/GD+M on
CIFAR-10. To isolate the effect of momentum, we turn off data augmentation, dropout and batch normalization.
GD and SGD respectively refer to stochastic gradient descent with batch sizes 50k (full batch) and 128. We grid
searched the best (scheduled) learning rate and weight decay for each individual algorithm separately. Results
are averaged over 3 runs and we only report the mean (see Appendix for complete table).
model Soudry et al. (2018); Lyu & Li (2019); Ji & Telgarsky (2019); Chizat & Bach (2020); Gunasekar
et al. (2018); Arora et al. (2019). This regularization arises in deep learning mainly due to the non-
convexity of the objective function. Indeed, this latter can create multiple global minima scattered
in the space that vastly differ in terms of generalization. Algorithmic regularization is induced by
and depends on many factors such as learning rate and batch size (Goyal et al., 2017; Hoffer et al.,
2017; Keskar et al., 2016; Smith et al., 2018), initialization Allen-Zhu & Li (2020), adaptive step-size
(Kingma & Ba, 2014; Neyshabur et al., 2015; Wilson et al., 2017), batch normalization (Arora et al.,
2018; Hoffer et al., 2019; Ioffe & Szegedy, 2015) and dropout (Srivastava et al., 2014; Wei et al.,
2020). However, none of these works theoretically analyzes the regularization induced by momentum.
We therefore start our investigation by raising the following question:
Does momentum unconditionally improve generalization in deep learning?
This question could be positively answered given the success of momentum for learning distinct
architectures such as ResNets (He et al., 2016) or BERT (Devlin et al., 2018). However, we here
empirically give a negative answer through the following synthetic example in deep learning. We
consider a binary classification problem where data-points are generated from a standard normal
distribution and labels are outputs of teacher networks. Starting from the same initialization, we
train different over-parametrized student networks using GD and GD+M. Based on Table 1, whether
the target function is simple (linear) or complex (neural network), momentum does not improve
generalization even when using a non-linear neural network as learner. The same observation holds
for SGD/SGD+M as shown in the Appendix. Therefore, momentum does not always lead to a higher
generalization in deep learning. Instead, such benefit seems to heavily depend on both the structure
of the data and the learning problem.
On which data set does momentum help generalization? In this paper, in order to deter-
mine the underlying mechanism produced by momentum to improve generalization, we de-
sign a binary classification problem with a simple data structure where training a two-layer (over-
parameterized) convolutional network with momentum provably improves generalization in deep
learning. It is built upon a data distribution that relies on the concepts of feature and margin. Infor-
mally, each example in this distribution is a 1D image having P patches. One of the patches (the
signal patch) contains a feature we want to learn and all the others are Gaussian random noise with
small variance.
Mathematically, one can think of a feature as a vector w* ∈ Rd. We assume that our training examples
are divided into large margin data where the signal is αw* with α constant and small margin data
where the signal is βw* with β 1. Intuitively, the second type of data is inherently noisier as the
margin is small and therefore, a classifier would struggle more to generalize on this type of data. We
2
Under review as a conference paper at ICLR 2022
Figure 2: DataSet equation (D) 2D. EaCh data-point is Xi = [ci ∙ w*, di ∙ n] ∈ R4 for some Ci ,d ∈ R. We
project these points in the 2D space (SPan(w*), span(n)). The feature is w* and the noisy patch is in span(n).
The large margin data (squares) have large Component along w* and relatively small noise Component and are
thus roughly equal to αw*. The small margin data (circles) have relatively large noise component and thus,
these data are well-spread on the span of n.
^^^^^^ TeaCher StUdenr^^^^_	Linear	1-MLP	2-MLP	1-CNN	2-CNN
1-MLP	93.48/93.25 二	92.32/92.18 二	84.3/83.68 二	94.18/94.12 二	76.04/76.12 二
2-MLP	93.45/92.85	91.02/91.78	83.82/83.25	94.14/94.20	75.50/75.56
1-CNN	92.21/92.34	92.31/92.33	83.39/83.44	94.39/94.39	79.44/78.32
2-CNN	91.04/91.22 -	91.51/91.56「	82.44/82.12 一	93.91/93.79 一	80.86/78.56 一
Table 1: Test accuracy obtained using GD/GD+M on a Gaussian synthetic dataset trained using neural network
with ReLU activations. The training dataset consists in 500 data points in dimension 30 and test set in 5000
points. The student networks are trained for 1000 epochs to ensure zero training error. The results are averaged
over 3 runs and we only report the mean (see Appendix for complete table).
underline that all the examples share the same feature but differ in the intensity of the signal. We
consider a training dataset of size N with the following split for μ 冬 1 :
(1 — μ)N datapoints are with large margin,
μN datapoints are with small margin data.
(D)
Figure 2 sketches equation (D) in a 2D setting. We emphasize that datasets having similar features
and different margins are common in the real-world. Examples include object-recognition datasets
such as CIFAR (Krizhevsky et al., 2009) or Imagenet (Deng et al., 2009) (for example, the “wheel
feature” of a car can be strong or weak depending on the orientation of the car). More specifically, we
believe that the dataset (D) can be viewed as a simplified model of these object-recognition datasets.
In this context, the following informal theorems characterize the generalization of the GD and GD+M
models. They dramatically simplify Theorem 3.1 and Theorem 3.2 but highlight the intuitions behind
our results.
Theorem 1.1 (Informal, GD+M). There exists a dataset of the form (D) with size N such that a
two-layer (over-parameterized) convolutional network trained with GD+M:
1.	initially only learns large margin data from the (1 — μ)N examples.
2.	has large historical gradients that contain the feature w* present in small margin data.
3.	keeps learning the feature in the small margin data using its momentum historical gradients.
The model thus reaches zero training error and perfectly classify large and small margin data at test.
Theorem 1.2 (Informal, GD). There exists a dataset of the form (D) with size N such that a two-layer
(over-parameterized) convolutional network trained with GD:
1.	initially only learns large margin data from the from the (1 — μ)N examples.
2.	has small gradient after learning these data.
3.	memorizes the remaining small margin data from the μN examples using the noises.
The model thus reaches zero training and manages to classify the large margin data at test. However,
it fails to classify the small margin data because of the memorization step during training.
Why does GD+M generalize better than GD? Since the large margin data are dominant, GD
focus in priority on these examples to decrease its training loss. However, after fitting this data, it
significantly lowers its gradient. The gradient is thus not large enough for learning the small margin
data. Similarly, GD+M fits the large margin data and subsequently gets a small gradient. However,
3
Under review as a conference paper at ICLR 2022
(a)	(b)	(c)
Figure 3: (a): Training loss (b) test accuracy on large margin data and (c) test accuracy on the small margin data
in the synthetic setting. While GD and GD+M get zero training loss, GD+M generalizes better on small margin
data than GD. Setting: 20000 training data, 2000 test data, d=30, number of neurons=5, number of patches=5.
O	IOO 200	300	O 50 IOO 150 200 250 300
Number of epochs	Number of epochs
(a)	(b)
Figure 4: Training (a) and test (b) accuracy obtained with Resnet-18 on CIFAR-10 dataset with artificially
generated small margin data. The architectures are trained using GD/GD+M for 300 epochs to ensure zero
training error. Data augmentation, dropout and batch normalization are turned off. (SM) stands for the test
accuracy obtained by the algorithm on the small margin data. Results are averaged over 5 runs with best
scheduled learning rate and weight decay for each individual algorithm separately.
contrary to GD, GD+M has large historical gradients in his momentum gradient. These gradients
helped to learn the feature in the large margin data. They also help to learn small margin data since all
the examples share the same feature. GD+M therefore uses his momentum to learn the small margin
data. We name this process historical feature amplification and believe that it is key to understand
why momentum improves generalization.
Empirical justification. We also provide an empirical justification that such phenomenon does
happen in a real-world setting as reported in Figure 4. In this experiment, we create small margin
data in the CIFAR-10 dataset by respectively lowering the resolution of 10% of the training and
test images, adding Gaussian noise of variance 0.005 and randomly shuffling the RGB channels.
Figure 4 shows that even though both algortihms reach zero training error and 100% training accuracy,
GD+M gets higher generalization than GD on this decimated dataset. Above all, at test, GD+M
performs as well on small and large margin data while GD does relatively worse on small margin
data.Indeed, the relative accuracy drop for GD+M is 80.36/83.32 = 0.97 while for GD is equal to
65.14/73.69 = 0.88.
Our paper is organized as follows. In Section 2, we formally define the data distribution equation (D),
the model and algorithms we use to learn it. Lastly, Section 3 presents our main theorems and provide
a proof sketch in Section 4 and Section 5. Additional experiments can be found in the Appendix.
More related Work
Momentum in convex setting. GD+M (a.k.a. heavy ball or Polyak momentum) consists in calcu-
lating the exponentially weighted average of the past gradients and using it to update the weights.
For convex functions near a strict twice-differentiable minimum, GD+M is optimal regarding local
convergence rate Polyak (1963; 1964); Nemirovskij & Yudin (1983); Nesterov (2003). However, it
may fail to converge globally for general strongly convex twice-differentiable functions Lessard et al.
(2015) and is no longer optimal for the class of smooth convex functions. In the stochastic setting,
GD+M is more sensitive to noise in the gradients; that is, to preserve their improved convergence
rates, significantly less noise is required d’Aspremont (2008); Schmidt et al. (2011); Devolder et al.
(2014); Kidambi et al. (2018). Finally, other momentum methods are extensively used for convex
functions such as Nesterov’s accelerated gradient Nesterov (1983). Our paper focuses on the use of
GD+M and contrary to the aforementioned papers, our setting is non-convex and we mainly focus on
4
Under review as a conference paper at ICLR 2022
the generalization of the model learned by GD and GD+M when both methods converge to global
optimal. We underline that contrary to the non-convex world, generalization is typically disentangled
with optimization for (strictly) convex functions.
Non-convex optimization with momentum. A long line of work consists in understanding the
convergence speed of momentum methods when optimizing non-convex functions. Mai & Johansson
(2020); Liu et al. (2020); Cutkosky & Mehta (2020); Defazio (2020) show that SGD+M reaches
a stationary point as fast as SGD under diverse assumptions. Besides, Leclerc & Madry (2020)
empirically shows that momentum accelerates neural network training for small learning rates and
slows it down otherwise. Our paper differs from these works as we work in the batch setting and
theoretically investigate the generalization benefits brought by momentum (and not the training ones).
Generalization with momentum. Momentum-based methods such as SGD+M, RMSProp (Tiele-
man & Hinton, 2012) and Adam (Kingma & Ba, 2014) are standard in deep learning training since
the seminal work of Sutskever et al. (2013). Although its well accepted that Momentum improve
generalization in deep learning, only a few works formally investigate the role of momentum in
generalization. Leclerc & Madry (2020) empirically reports that momentum yields higher general-
ization when using a large learning rate. However, they assert that this benefit can be obtained by
applying an even larger learning rate on vanilla SGD. We suspect that this observation is due to batch
normalization (BN) which is known to dramatically bias the algorithm’s generalization (Lyu & Li,
2019). In Appendix, we report that BN reduces the generalization gain of momentum comparing to
without BN. To our knowledge, our work is first that theoretically investigate the generalization of
momentum in deep learning.
2 Setting and algorithms
In this section, we first introduce a formal definition of the data distribution equation (D) and the
neural network model we use to learn it. We finally present the GD and GD+M algorithms.
General notations. For a matrix W ∈ Rm×d, we denote by wr its r-th row. For a function
f: Rm×d → R, We denote by Vwrf (W) the gradient of f with respect to Wr and Vf (W) the
gradient with respect to W. For an optimization algorithm updating a vector w, w(t) represents its
iterate at time t. We use Id for the d × d identity matrix and 1m the all-ones vector of dimension
m. Finally, we use the asymptotic complexity notations when defining the different constants in the
paper. We use O, Θ, Ω to hide logarithmic dependency on d.
Data distribution. We define our data distribution D as follows.
Each sample from D consists in an input data X and a label y that are generated as:
1.	The label y is uniformly sampled from {-1, 1}.
2.	Each data-point X = (X[1], . . . , X[P]) consists in P patches where each X[j] ∈ Rd.
3.	Signal patch: for one patch P(X) ∈ [P], we have X[P(X)] = cw*, where C ∈ R,
w* ∈ Rd and ∣∣w*∣∣2 = 1.	(D)
4.	The distribution of c satisfies that
αy with probability 1
—μ
c=	.
[βy with probability μ
5.	Noisy patches: for all the other patches j ∈ [P]\{P(X)}, X[j]〜N(0, (I-w* w*>)σ2Id).
We precise that we sample the noisy patches in the orthogonal complement of w* to have a simpler
analysis. To present the simplest result, we assume that the values in equation (D) satisfy α = d0.49,
β = 一 ]、. α, σ = -U and P ∈ [2, Polylog(d)].
polylog(d) d	d
Using this model, we generate a training dataset Z = {(Xi, yi)}i∈[N] where Xi = (Xi [j])j∈[P] . We
focus on the case where μ = 1∕poly(d) and N = Θ (1. We let Z to be partitioned in two sets Zi
and Z2 such that Z1 gathers the large margin data while Z2 the small margin ones. Lastly, we define
μ = lzN2l the fraction of small margin data.
Learner model. We use a two-layer convolutional neural network with cubic activation to learn
the training dataset Z. This model is the simplest non-linear network since a quadratic activation
5
Under review as a conference paper at ICLR 2022
would only output positive labels and mismatch our labeling function. The first layer weights are
W ∈ Rm×d and the second layer is fixed to 1m. Given a input data X, the output of the model is
mP
fW(X) =XXhwr,X[j]i3.	(CNN)
r=1 j=1
The number of neurons is set as m = polylog(d) to ensure that (CNN) is mildly over-parametrized.
Training objective. We fit the training dataset Z using (CNN) and solve the logistic regression
problem	N
min , X Xlog(1+ exp(-yifw(Xi))) + IkW∣∣2 := L(W).	(P)
W ∈Rm×d N	2
i=1
(P) sheds light on our choice of cubic activation in (CNN). Indeed, it is the smallest polynomial
degree that makes the training objective (P) non-convex and compatible with our dataset. Linear or
quadratic activations would respectively make the problem convex or all the labels positive. Here, we
Pick λ ∈ [0, poly(d)/ .
Importance of non-convexity. When λ > 0, if the loss N PN=Ilog (1 + exp (-yifw(Xi))) is
convex, then there is a unique global optimal solution, so the choice of optimization algorithm does
not matter. In our case, due to the non-convexity of the training objective, GD + M converges to a
different (approximate) global optimal comparing to GD, with better generalization properties.
Test error. We assess the quality of a predictor W using the classical 0-1 loss used in bi-
nary classification. Given a sample (X, y), the individual test (classification) error is defined
as L (X, y) = 1{fWc(X)y < 0}. While L measures the error of fW on an individual data-point,
we are interested in the test error that measures the average loss over data points generated from (D)
and defined as
L(fw) ：= E(X,y)〜D[L(fw(X),y)].
(TE)
Algorithms. We solve the training problem equation (P) using GD and GD+M. GD is defined by
W(t+1) = W⑴-ηVL(W(t)), for t ≥ 0,	(GD)
where η > 0 is the learning rate. On the other hand, GD+M is defined by the update rule
(g(t+1)	= γg(t) + (1 - γ)VLb(W (t)) fort≥0	(GD+M)
W (t+1) = W(t) - ηg(t+1)	, fort ≥ 0.	(GD+M)
where γ ∈ (0, 1) is momentum factor. We now detail how to set parameters in (GD) and (GD+M).
Parametrization 2.1. When running GD and GD+M on equation (P), the number of iterations is
T ∈ [poly(d)N∕(η), d°Qog d)∕(η)]. For both algorithms, the weights W(O),..., Wm) are initialized
using independent samples from a normal distribution N(0,σ0Id) where σ2 = Polydog⑷.The
learning rate is set as:
1.	GD: the learning rate may take any reasonable value η ∈ (0, O(1)].
2.	GD+M: the learning rate is a large learning rate: η = Θ(1).1
Lastly, the momentum factor in GD+M is set to be Y = 1 一 Polydog⑷.
Our Parametrization 2.1 matches with the parameters used in practice as the weights are generally
initialized from Gaussian with small variance and momentum is set close to 1 (Sutskever et al., 2013).
3 Main results
We now formally state our main theorems regarding the generalization of models trained using
equation (GD) and equation (GD+M) on the training set Z generated by equation (D). As announced
in the introduction, we show that the GD+M model incurs a generalization error that is dramatically
smaller than the GD model. Before introducing the main result, we define some notations:
1This is consistent with the empirical observation that only momentum with large learning rate improves
generalization (Sutskever et al., 2013)
6
Under review as a conference paper at ICLR 2022
Main objects. Let r ∈ [m], i ∈ [N], j ∈ P\{P (Xi)}, γ ∈ (0, 1) and t ≥ 0. We are mainly
interested in Wrt), the r-th weight of the network, Nwr L(W⑴)the gradient of the training loss w.r.t.
wr, gr(t) the momentum gradient defined by gr(t+1) = γgr(t) + (1 - γ)NwrLb(W(t)). The analysis lies
on the projection of these objects on the feature w* and on noisy patches Xi[j]. We introduce the
following notations for the component of the learned weights along feature and noise directions:
-	Projection on w*: Crt) =(wSt^w*).
-	Projection on Xij] : Ξ(tj,r = hwtt, Xi [j]).
-	TOtalnoise: m(t)= Pm=I Pj∈[p]∖{p(Xi)}hwrt),Xij]i3.
-	Maximum signal: let rmax = argmaxr∈[m] c(rt), c(t) = c(rtm)ax
Theorem 3.1.	Assume that we run GD on (P) for T iterations with parameters set as in Parametriza-
tion 2.1. With probability at least 1 - o(1), the weights learned by GD
1.	Partiallylearnthefeature: for all r ∈ [m], ∣crτ )| ≤ O(1∕α).
2. memorize from small margin data: for all i ∈ Z2 ,
Ξi(t)
~ ..
≥ Ω(1).
C	.1 .1 . ∙ ∙	♦	11 .1	/ɑ t / 1 t 1∖ ∖	1.1	∙ ..J 1 . .. . J K / ∖
Consequently, the training error is smaller than O(μ∕poly(d)) and the test error is at least Ω(μ).
Intuitively, the training process of the GD model is described as follows. Since the large margin
data are dominant in Z, the gradient points mainly in the direction of the feature w*. Therefore, GD
eventually learns the feature in Z1 (Lemma 4.1) and the gradients from Z1 quickly become small.
Afterwards, the gradient is dominated by the gradients from Z2 (Lemma 4.2). Because Z2 has small
margin, the full gradient is now directed by the noisy patches. It implies that GD memorizes noise in
Z2 (Lemma 4.4). Since these gradients also control the amount of remaining feature to be learned
(Lemma 4.3), we conclude that the GD model partially learns the feature and introduces a huge noise
component in the learned weights. We provide a proof sketch of Theorem 3.1 in Section 4.
Theorem 3.2.	Assume that we run GD+M on equation (P) for T iterations with parameters set as in
Parametrization 2.1. With probability at least 1 - o(1), the weights learned by GD+M
1.	(at least for one of them) is highly correlated with thefeature: C(T) > Ω(1∕β).
(T)
2.	are barely correlated with noise: for all r ∈ [m] ,for all i ∈ [N ] and j ∈ [P ]. |二» ≤ O(σ0).
Consequently, the training loss and the test error are at most O(μ∕poly(d)).
Intuitively, the GD+M model follows this training process. Similarly to GD, it first fits the Z1
(Lemma 5.1). Contrary to GD, the momentum gradient is still highly correlated with w* after this
step (Lemma 5.2). Indeed, the key difference is that momentum accumulates historical gradients.
Since these gradients were accumulated when learning large margin data, the direction of momentum
gradient is highly biased towards w*. Therefore, the GD+M model amplifies the feature from these
historical gradients to learn the feature in small margin data (Lemma 5.3). Subsequently, the gradient
becomes small (Lemma 5.4) and the weights are no longer updated. Therefore, the GD+M model
manages to ignore the noisy patches (Lemma 5.5) and learns the feature from both Z1 and Z2 . We
provide a proof sketch of Theorem 3.2 in Section 5.
To state the proof, we further decompose the gradients along signal and noise directions.
-	Projection on w*: Gr(t) = hNwr Lb(Wt), w*i and Gr(t) = hgr(t), w*i.
-	Projection on Xi[j] : Gi(,tj),r = hNwr Lb(W (t)), Xi[j]i, Gi(,tj),r = hgr(t),Xi[j]i.
-	Maximum signal: let rmax = argmaxr∈[m]C(rt), C(t) = C(rtm)ax andG(t) = Gr(tm)ax.
Signal and noise iterates. Our analysis is build upon a decomposition of the updates equation (GD)
and equation (GD+M) on w* and Xi [j ]. These decompositions are respectively defined as follows:
C(t+1) = C(t) G(t) (GD-S)	Ξ(t+1) = Ξ(t)	G(t)	(GD-N)
Cr	= Cr - ηGr (GD-S)	Ξi,j,r = Ξi,j,r - ηGi,j,r (GD-N)
7
Under review as a conference paper at ICLR 2022
Gr(t+1) = γGr(t) + (1 - γ)Gr(t)
cG(rrt+1)==cγ(rtG)r-η+G(r(1t+-1)γ)Gr(GDM-S)
Gi(,tj+,r1) = γGi(,tj),r + (1 - γ)Gi(,tj),r
(t+1)	(t)	(t+1)
Ξi,j,r = Ξi,j,r - Gi,j,r
(GDM-N)
We detail how to use these dynamics to analyze GD+M and GD in Section 4 and Section 5. Our anal-
ysis heavily depends on the gradients of the training loss which involve sigmoid(x) = (1 + e-x)-1 .
We define the derivative of a data-point i as `i(t) = sigmoid(-yifW(t) (Xi)), the derivatives
Vkt) = N Pi∈zfc '(t) for k ∈ {1,2} and the full derivative V(t) = Vf) + ν(t.
4 Analysis of GD
In this section, we provide a proof sketch for Theorem 3.1 that reflects the behavior of GD with
λ = 0. A more detailed proof (extending to λ > 0) can be found in the Appendix.
Step 1: Learning Z1. At the beginning of the learning process, the gradient is mostly dominated
by the gradients coming from the Z1 samples. Since these data have large margin, the gradient is
thus highly correlated with w* and Crt) increases as shown in the following Lemma.
Lemma 4.1. For all r ∈ [m] and t ≥ 0, equation (GD-S) is simplified as:
c9+1) ≥ C?) + Θ(η)α3(crt))2 ∙ Sigmoid(- P：= α3(cSt))3).
Consequently, after To = Θ (ηθrσ^) iterations,for all t ∈ [To, T], we have c(t) ≥ ΩΩ(1∕α).
Intuitively, the increment in the update in Lemma 4.1 is non-zero when the sigmoid is not too small
which is equivalent to c(t) ≤ O(1∕α). Therefore, c(t) keeps increasing until reaching this threshold.
After this step, the Z1 data have small gradient and therefore, GD has learned these data.
Lemma 4.2. Let To = Θ (ηα3σc,). After t ∈ [T0, T] iterations, the Zi derivative is bounded as
ν(t) ≤ O (η(t-To+i)α) + O (β03) ν(t. ThefiMderive疝eis ν(t) ≤ O (η(t4+i)α + (1 + β03) ν2t)).
By our choice of parameter, Lemma 4.2 indicates that the full gradient is dominated by the gradients
from Z2 data after To =C (焉a) ∙ Consequently, ν2(t) also rules the amount of feature learnt by GD.
Lemma 4.3. Let To = Θ (.=3.° )∙ FOrt ∈ [To, T], equation (GD-S) becomes c(t+1) ≤ O(1∕α) +
O(ne3/a)PT=TO ν2τ)∙
Lemma 4.3 implies that quantifying the decrease rate of V2(t) provides an estimate on the quantity
of feature learnt by the model. We remark that V2(t) = sigmoid(β3 Psm=i(C(st))3 + Ξi(t)) for some
i ∈ Z2. We thus need to determine whether the feature or the noise terms dominates in the sigmoid.
Step 2: Memorizing Z2. We now show that the total correlation between the weights and the noise
in Z2 data increases until being large.
Lemma 4.4. Let t ≥ 0 and i ∈ Z2. Assume that Ξ(t) ≤ O(1). Then, equation (GD-N) can be
simplified as:
yiΞ(t+i) ≥ yiΞ(°) + θ(ησ2d) XX(Ξ(τ) )2 - O
yi i,j,r — yi i,j,r 1 N j∖ i,j,r>
τ=o
Let Ti = O (. σ√dσ2d). Therefore, Ξ(t) ≥ Ω(1), for t ∈ [Ti, T] and thus GD memorizes.
By Lemma 4.4, in the gradient of Z2 data, the noise term dominates the feature term (which scales as
O(β3)). Consequently, the algorithm memorizes the Z2 data which implies a fast decay of ν(t).
Pσ2√d
α
8
Under review as a conference paper at ICLR 2022
Lemma 4.5. Let TI = O (σ0σ√dσ2d). For t ∈ [Tι,T ], we have PT=0 ν2τ) ≤ O (η⅛).
Combining Lemma 4.5 and Lemma 4.3, we prove that GD partially learns the feature.
Lemma 4.6. For t ≤ T, the signal component satisfies c(t) ≤ O(1∕α).
Lemma 4.4 and Lemma 4.6 respectively yield the first two items in Theorem 3.1. Bounds on the
training and test errors are respectively obtained by plugging these results in (P) and (TE).
5	Analysis of GD+M
In this section, we provide a proof sketch for Theorem 3.2 that reflects the behavior of GD+M with
λ = 0. A more detailed proof (also extending to λ > 0) can be found in the Appendix.
Step 1: Learning Z1. Similarly to GD, by our initialization choice, the early gradients and so,
momentum gradients are large. They are also spanned by the feature w* and therefore, the GD+M
model also increases its correlation with w*.
Lemma 5.1. For all r ∈ [m] and t ≥ 0, as long as c(t) ≤ O(I∕α), the momentum update
equation (GDM-S) is simplified as:
-Gr(t+1) = -γGr(t)+(1-γ)Θ(α3)(c(rt))2
Consequently, after T = Θ (σ⅛^ + I--Y) iterations, for all t ∈ [To, T], we have c(t) ≥ Ω(1∕α).
Step 2: Learning Z2. Contrary to GD, GD+M has a large momentum that contains w* after Step 1.
Lemma 5.2. Let T0 = Θ
+ ι--γ). For t ∈ [To,T], we have G⑴ ≥ Ω(√1 - Y∕α).
Lemma 5.2 hints an important distinction between GD and GD+M: while the current gradient along
w* is small at time To, the momentum gradient stores historical gradients that are spanned by w*. It
amplifies the feature present in previous gradients to learn the feature from small margin data.
Lemma 5.3. Let To = Θ ( σ⅛3 + ι-1γ ). After Ti = To + Θ (ι--γ j iterations, for t ∈ [TI, T], we
have Ctt ≥ Ω (√ι-γ仪).Our choice ofparameter in Section 2, this implies Ctt ≥ ΩΩ(1∕β).
Lemma 5.3 states that at least one of the weights that is highly correlated with the feature compared
to GD where C⑴=O(1). This result implies that V(t) converges fast.
Lemma 5.4. Let To = Θ (ησ⅛3 + I-LY). After Ti = To + Θ (I-LY) iterations, for t ∈ [Ti, T],
V(t) ≤ O (η(t-Tι + i)β).
With this fast convergence, Lemma 5.4 implies that the correlation of the weights with the noisy
patches does not have enough time to increase and thus, remains small.
Lemma 5.5. Let i ∈ [N], j ∈ [P]\{P (Xi)} and r ∈ [m]. For t ≥ 0, equation (GDM-N) can be
rewritten as[G?+：) | ≤ Y|G(tj,/ + (1 一 γ)O(σ2σ4d2)ν(t). As a consequence, after t ∈ [TL,T]
iterations, we thus have ∣Ξ(tj,r | ≤ O(σoσ√d).
Lemma 5.3 and Lemma 5.5 respectively yield the two first items in Theorem 3.2.
6	Discussion
Our work is a first step towards understanding the algorithmic regularization of momentum and leaves
room for improvements. We constructed a data distribution where historical feature amplification
may explain the generalization improvement of momentum. However, it would be interesting to
understand whether this phenomenon is the only reason or whether there are other mechanisms
explaining momentum’s benefits.An interesting setting for this question is NLP where momentum is
used to train large models as BERT (Devlin et al., 2018). Lastly, our analysis is in the batch setting
to isolate the generalization induced by momentum. It would be interesting to understand how the
stochastic noise and the momentum together contribute to the generalization of a neural network.
9
Under review as a conference paper at ICLR 2022
References
Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and
self-distillation in deep learning. arXiv preprint arXiv:2012.09816, 2020.
Sanjeev Arora, Zhiyuan Li, and Kaifeng Lyu. Theoretical analysis of auto rate-tuning by batch
normalization. arXiv preprint arXiv:1812.03981, 2018.
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. arXiv preprint arXiv:1905.13655, 2019.
Anthony Carbery and James Wright. Distributional and lq norm inequalities for polynomials over
convex bodies in Rn. Mathematical research letters, 8(3):233-248, 2001.
Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks
trained with the logistic loss. In Conference on Learning Theory, pp. 1305-1338. PMLR, 2020.
Ashok Cutkosky and Harsh Mehta. Momentum improves normalized sgd. In International Conference
on Machine Learning, pp. 2260-2268. PMLR, 2020.
Alexandre d’Aspremont. Smooth optimization with approximate gradient. SIAM Journal on Opti-
mization, 19(3):1171-1183, 2008.
Aaron Defazio. Understanding the role of momentum in non-convex optimization: Practical insights
from a lyapunov analysis. arXiv preprint arXiv:2010.00406, 2020.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Olivier Devolder, FrangOiS Glineur, and Yurii Nesterov. First-order methods of smooth convex
optimization with inexact oracle. Mathematical Programming, 146(1):37-75, 2014.
Priya Goyal, Piotr Dolldr, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,
Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Srebro.
Implicit regularization in matrix factorization. In 2018 Information Theory and Applications
Workshop (ITA), pp. 1-10. IEEE, 2018.
Jeff Z HaoChen, Colin Wei, Jason D Lee, and Tengyu Ma. Shape matters: Understanding the implicit
bias of the noise covariance. arXiv preprint arXiv:2006.08680, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the gen-
eralization gap in large batch training of neural networks. arXiv preprint arXiv:1705.08741,
2017.
Elad Hoffer, Ron Banner, Itay Golan, and Daniel Soudry. Norm matters: efficient and accurate
normalization schemes in deep networks, 2019.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International conference on machine learning, pp. 448-456.
PMLR, 2015.
Ziwei Ji and Matus Telgarsky. The implicit bias of gradient descent on nonseparable data. In
Conference on Learning Theory, pp. 1772-1798. PMLR, 2019.
10
Under review as a conference paper at ICLR 2022
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016.
Rahul Kidambi, Praneeth Netrapalli, Prateek Jain, and Sham Kakade. On the insufficiency of existing
momentum schemes for stochastic optimization. In 2018 Information Theory and Applications
Workshop (ITA),pp.1-9.IEEE, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep con-
VolUtional neural networks. Advances in neural information processing systems, 25:1097-1105,
2012.
Guillaume Leclerc and Aleksander Madry. The two regimes of deep network training. arXiv preprint
arXiv:2002.10376, 2020.
Laurent Lessard, Benjamin Recht, and Andrew Packard. Analysis and design of optimization
algorithms via integral quadratic constraints, 2015.
Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large
learning rate in training neural networks. arXiv preprint arXiv:1907.04595, 2019.
Yanli Liu, Yuan Gao, and Wotao Yin. An improved analysis of stochastic gradient descent with
momentum. arXiv preprint arXiv:2007.07989, 2020.
Shachar Lovett. An elementary proof of anti-concentration of polynomials in gaussian variables.
Electron. Colloquium Comput. Complex., 17:182, 2010.
Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks.
arXiv preprint arXiv:1906.05890, 2019.
Vien Mai and Mikael Johansson. Convergence of a stochastic gradient method with momentum
for non-smooth non-convex optimization. In International Conference on Machine Learning, pp.
6630-6639. PMLR, 2020.
Arkadij Semenovic Nemirovskij and David Borisovich Yudin. Problem complexity and method
efficiency in optimization. 1983.
Yurii Nesterov. A method for unconstrained convex minimization problem with the rate of conver-
gence o (1∕k^ 2). In Doklady an USSr, volume 269, pp. 543-547,1983.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2003.
Behnam Neyshabur, Ruslan Salakhutdinov, and Nathan Srebro. Path-sgd: Path-normalized optimiza-
tion in deep neural networks. arXiv preprint arXiv:1506.02617, 2015.
Boris T Polyak. Gradient methods for the minimisation of functionals. USSR Computational
Mathematics and Mathematical Physics, 3(4):864-878, 1963.
Boris T Polyak. Some methods of speeding up the convergence of iteration methods. Ussr computa-
tional mathematics and mathematical physics, 4(5):1-17, 1964.
Mark Schmidt, Nicolas Le Roux, and Francis Bach. Convergence rates of inexact proximal-gradient
methods for convex optimization. arXiv preprint arXiv:1109.2415, 2011.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V. Le. Don’t decay the learning rate,
increase the batch size, 2018.
11
Under review as a conference paper at ICLR 2022
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit
bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):
2822-2878, 2018.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15(1):1929-1958, 2014.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization
and momentum in deep learning. In International conference on machine learning, pp. 1139-1147.
PMLR, 2013.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-31,
2012.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018.
Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-
bridge University Press, 2019.
Colin Wei, Sham Kakade, and Tengyu Ma. The implicit and explicit regularization effects of dropout,
2020.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. arXiv preprint arXiv:1705.08292, 2017.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,
2016.
12