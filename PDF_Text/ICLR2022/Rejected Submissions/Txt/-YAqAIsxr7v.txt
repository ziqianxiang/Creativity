Under review as a conference paper at ICLR 2022
OVD-Explorer:	A General Information-
theoretic Exploration Approach for Rein-
forcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
Many exploration strategies are built upon the optimism in the face of the uncer-
tainty (OFU) principle for reinforcement learning. However, without consider-
ing the aleatoric uncertainty, existing methods may over-explore the state-action
pairs with large randomness and hence are non-robust. In this paper, we explic-
itly capture the aleatoric uncertainty from a distributional perspective and propose
an information-theoretic exploration method named Optimistic Value Distribution
Explorer (OVD-Explorer). OVD-Explorer follows the OFU principle, but more
importantly, it avoids exploring the areas with high aleatoric uncertainty through
maximizing the mutual information between policy and the upper bounds of pol-
icy’s returns. Furthermore, to make OVD-Explorer tractable for continuous RL,
we derive a closed form solution, and integrate it with SAC, which, to our knowl-
edge, for the first time alleviates the negative impact on exploration caused by
aleatoric uncertainty for continuous RL. Empirical evaluations on the commonly
used Mujoco benchmark and a novel GridChaos task demonstrate that OVD-
Explorer can alleviate over-exploration and outperform state-of-the-art methods.
1	Introduction
The exploration and exploitation trade-off is critical in reinforcement learning (Thompson, 1933;
Sutton & Barto, 2018). Many exploration strategies have been proposed in the literature (Osband
et al., 2016; Lillicrap et al., 2016; Martin et al., 2017; Pathak et al., 2017; Ciosek et al., 2019).
Among these strategies, those following the Optimism in the Face of Uncertainty (OFU) principle
(Auer et al., 2002) provide efficient guidance for exploration (Chen et al., 2017; Ciosek et al., 2019).
Generally, OFU-based methods regard the uncertainty as the result of insufficient exploration for
the state-action pair, and refer to such uncertainty as epistemic uncertainty. Besides, there is another
uncertainty called aleatoric uncertainty, which captures environmental stochasticity:
•	Epistemic uncertainty (a.k.a. parametric uncertainty) represents the ambiguity of the model
arisen from insufficient knowledge, and is high at those state-action pairs seldom visited
(Dearden et al., 1998; Osband et al., 2016; Moerland et al., 2017).
•	Aleatoric uncertainty (a.k.a. intrinsic uncertainty) is the variation arisen from environment
randomness, caused by the stochasticity of policy, reward and/or transition probability, and
is characterized by return distribution. (Bellemare et al., 2017; Moerland et al., 2017).
In the heteroscedastic stochastic tasks where the environment randomness of different state-action
pairs differs, those OFU-based methods may be inefficient without properly tackling aleatoric un-
certainty (Nikolov et al., 2019). As shown in Figure 1, if the epistemic uncertainty estimation is
disturbed due to the volatility caused by aleatoric uncertainty, the exploration strategy (i.e., policy
A) optimistic about such estimated uncertainty may lead to explore areas with high aleatoric uncer-
tainty. Visiting such areas results in unstable and risky transitions. In the real world, the aleatoric
uncertainty can be caused easily, for example, unpredictable wind can shift the trajectory of an
robot’s action. If such aleatoric uncertainty is not modelled, the RL agent may be trapped because
the state transitions in such area can be wrongly considered novel and worth exploring due to the
high uncertainty. This issue, to explore overly the state-action pairs visited frequently but with high
1
Under review as a conference paper at ICLR 2022
higher	Aleatoric uncertainty	lower
PoIicy A	Policy B
agent
Figure 1: An intuitive example. Red/green
lines denote good/bad exploration policies.
aleatoric uncertainty, is referred to as the over-exploration issue. Intuitively, such issue could be
solved by avoiding exploring optimistically about aleatoric uncertainty (i.e., policy B in Figure 1).
The similar concern has been raised in the discrete
RL, where Nikolov et al. (2019) proposes to use
the Information-Directed Sampling (IDS) to avoid the
over-exploration issue in the environments with het-
eroscedastic noise. However, IDS needs to calculate
information-regret ratio for each action, thus apply-
ing IDS for continuous RL with explosive or even in-
finite action space could be non-trivial and ineffec-
tive. Meanwhile, many advanced continuous RL ap-
proaches suffer from the over-exploration issue. Soft
Actor-Critic (SAC) is a well performed continuous RL
algorithm, but does not account for efficient explo-
ration beyond maximizing policy entropy (Haarnoja
et al., 2018). OAC improves the exploration follow-
ing OFU principle, but it ignores to avoid exploration towards higher aleatoric uncertainty (Ciosek
et al., 2019). Therefore, an effective exploration method to address the over-exploration issue for
continuous RL is of urgent need.
To address the over-exploration issue, we propose that the characterization of aleatoric uncertainty
and a new exploration principle are necessary. The new principle need to properly trade-off be-
tween the epistemic and aleatoric uncertainties, so as to explore seldom visited state-action pairs
while avoiding trapped in areas with high aleatoric uncertainty. In this paper, we propose a gen-
eral information-theoretic exploration approach OVD-Explorer, which enriches the OFU exploration
principle with a complement: “avoid areas with high aleatoric uncertainty”. Furthermore, to guide
the exploration following the new principle, OVD-Explorer maximizes the mutual information be-
tween policy and corresponding upper bounds. Notable, the upper bounds represent the best return
that policy can reach, formulated using Optimistic Value Distributions (OVD), and the value distri-
bution characterises the aleatoric uncertainty. From the theoretical derivation, we show that maxi-
mizing such mutual information urges the OVD-Explorer to guide exploration towards the areas with
higher epistemic uncertainty and alleviate over-exploration issue by avoiding the areas with lower
information gain, i.e., the areas that have higher aleatoric uncertainty but low epistemic uncertainty.
To make OVD-Explorer tractable for continuous action space, we derive its closed form, and pro-
pose a scheme to incorporate OVD-Explorer with any policy-based RL algorithm. Practically, we
demonstrate the exploration benefits based on SAC. Evaluations on many challenging continuous
RL tasks, including a toy task GridChaos, tasks in Mujoco as well as their stochastic variants are
conducted, and the results verify our analysis. To the best of our knowledge, OVD-Explorer firstly
addresses the negative impact of aleatoric uncertainty for exploration in continuous RL.
2	Preliminaries
2.1	Distributional RL
To model the randomness in the observed long-term return, which characterises aleatoric uncer-
tainty, distributional RL methods (Bellemare et al., 2017; Dabney et al., 2018b;a) are used to esti-
mate Q-value distribution rather than expected Q-value (Mnih et al., 2015). In our paper, we focus
on quantile regression used in QR-DQN (Dabney et al., 2018b), where the distribution of Q-value is
represented by the quantile random variable Z. Z maps the state-action pair to a uniform probability
distribution supported on the values at all corresponding quantile fractions. Based on value distri-
bution estimator parameterized as θ, We denote the value at quantile fraction Ti as Zi(s, a, θ) given
state-action pair (s, a).
Similar to the Bellman operator in the traditional Q-Learning (Watkins & Dayan, 1992), the distri-
butional Bellman operator (Bellemare et al., 2017) TDπ under policy π is given as:
TDZ(st, at) = R(st, at) + YZ(st+ι,at+ι), at+ι 〜∏(∙∣st+ι).	(I)
2
Under review as a conference paper at ICLR 2022
Notice that TDπ operates on random variables, = denotes that distributions on both sides have equal
probability laws. Based on operator TDπ, Dabney et al. (2018b) propose QR-DQN to train quantile
estimations via the quantile regression loss (Koenker & Hallock, 2001), which is denoted as:
NN
Lqr(Θ) = NN XX[PTi(δi,j)],	⑵
i=1 j=1
where TD error δi,j = R(st, at) + γZi(st+1, at+1; θ) - Zj(st, at; θ), the quantile Huber loss
Pτ(U) = U * (T - lu<o), and Ti means the quantile midpoints, which is defined as Ti = τi+2+τi.
2.2	Distributional Soft Actor-Critic method
Distributional Soft Actor-Critic (DSAC) (Ma et al., 2020) seamlessly integrates distributional RL
with Soft Actor-Critic (SAC) (Haarnoja et al., 2018). Basically, based on the Equation 1, the distri-
butional soft Bellman operator TDπS is defined considering the maximum entropy RL as follows:
TDSZ(st,at) = R(st,at) + γ[Z(st+ι,at+ι)- αlogπ(at+ι∣St+ι)],	⑶
where at+ι 〜 π(∙∣st+ι), st+ι 〜P(∙∣st, at). Then, the quantile regression loss differs from EqUa-
tion 2 on δi,j, by extending clipped double Q-Learning (Fujimoto et al., 2018) based on the maxi-
mum entropy RL framework, to overcome overestimation of Q value estimation:
k
δkj = R(st, at) + γ[min Zi(st+ι, at+i； θk) - αlog∏(at+ι∣st+ι； φ)] 一 Zj(st, at； θk),	(4)
k=1,2
where θ and φ represents their target networks respectively. The actor is optimized by minimizing
the following actor loss, the same as SAC,
Jπ (Φ) =	E	[log ∏(f(st,j; Φ)∣st) - Q(st,f (st,∈t ； φ); θ) ],	(5)
St 〜D,et 〜N	(J)
where D is the replay buffer, f(s, ； φ) means sampling action with re-parameterized policy and is
a noise vector sampled from any fixed distribution, like standard spherical Gaussian, and Q value is
the minimum value of the expectation on certain distributions, as
Q(st, at； θ) = min Q(st, at； θk) = ɪ min X Zi(st, at； θk)∙
k=1,2	N k=1,2
i=0
(6)
3	Methodology
We introduce a general exploration method, named OVD-Explorer, to achieve efficient and robust
exploration for continuous RL. Overall, OVD-Explorer estimates the upper bounds of the policy’s
return, and uses the bounds as criteria to find a behavior policy for online exploration from the
information-theoretic perspective. Innovatively, to avoid over-exploration, OVD-Explorer particu-
larly takes the aleatoric uncertainty into account, leveraging the optimistic value distribution (OVD)
to formulate the upper bounds of policy’s return. Then, OVD-Explorer derives the exploration pol-
icy via maximizing the mutual information between policy and corresponding upper bounds for an
optimistic exploration. Meanwhile, the derived behavior policy avoids being optimistic to aleatoric
uncertainty, thus avoiding over-exploration issue.
In the following, we first give the intuition together with the theoretical derivation of OVD-Explorer.
Then we practically describe how we formulate OVD based on uncertainty estimation. Lastly, anal-
ysis are given to further illustrate why OVD-Explorer can explore effectively and robustly.
3.1	Optimistic value distribution guided exploration
Most OFU-based algorithms follow the exploration principle: “select the action that leads to areas
with high uncertainty”. However, this principle is incomplete as the high uncertainty may also be
caused by the aleatoric uncertainty, which misleads the exploration direction. Hence, we comple-
ment the principle by introducing a constrain: “Not only select the action that leads to areas with
3
Under review as a conference paper at ICLR 2022
high uncertainty, but also avoid the ones that only lead to the area with high aleatoric uncer-
tainty”. Notable, this new principle intuitively describes a good exploration ability, and we will
illustrate how to quantitatively measure a policy’s exploration ability in the later section. By follow-
ing this principle, OVD-Explorer derives the behavior policy for exploration.
Intuitively, each policy π in the policy space Π has different exploration ability (at state s), denoted
by Fπ(s). Given current state s, OVD-Explorer aims at finding the exploration policy πE with the
best exploration ability by solving the following optimization problem:
πE = arg max Fπ (s).	(7)
π∈Π
We propose to quantitatively measure the policy’s exploration ability from an information-theoretic
perspective by measuring the mutual-information of multi-variables as follows:
Fn(S)= MI(Zn(s, a。)，…，Zπ(s, a—); ∏(∙∣s)∣s),	(8)
where ∏(∙∣s) is the action random variable, and Zn(s, aj the random variable describing the upper
bounds of return that action ai can reach under policy π at state s. Here, ai ∈ A denotes any legal
action, thus k could be infinite in continuous action space. The higher Fn (s) is, the better ability
that policy π has in exploring towards higher epistemic uncertainty while simultaneously avoiding
higher aleatoric uncertainty, which satisfies the principle we raised. (see analysis in Section 3.3.)
Now we state the Theorem 1 to measure the mutual information above in continuous action space.
Theorem 1. The mutual information in Equation 8 at state s can be approximated as:
Fn (S) ≈ C E
a〜π (∙∣ S)
W(s,a)〜Zn (s,a)
ΦZ∏ (P(s,a))logφZπ 皆 a))
C
(9)
Φχ(∙) is the cumulative distribution function (CDF) of random variable X, z(s,a) is the sampled
upper bound of return from its distribution Zn (s, a) following policy π, Zn describes the current
return distribution of the policy π, and C is a constant (See proof in Appendix B.1).
Theorem 1 reveals that Fn(s) is only proportional to Φz∏ (z(s, a)), by maximizing which the Pol-
icy’s exploration ability can be improved. Concretely, given any policy πθ (parameterized by θ), the
derivative VθΦz∏ (z(s, a)) can be measured, thus gradient ascent can be iteratively performed to
derive a better exploration policy. (The complete closed form solution is given in later Section 4)
Note that, to perform above optimization procedure, We need to formulate two critical components
in Φz∏ (z(s, a)): @ the return of the policy Zn (s, a) andθ upper bound of the return of the policy
Zn(s, a). Therefore, we first details the formulations in Section 3.2, and then analyze why maxi-
mizing Φz∏ (z(s, a)) can derive a better exploration in Section 3.3.
3.2	Formulating the return of policy and corresponding upper bound
Now, we introduce the formulation of the return of policy Zn(S, a) and corresponding upper bound
Zn(s, a). As mentioned before, most OFU-based methods neglect the aleatoric uncertainty when
formulating the return and upper bound of return (Mavrin et al., 2019; Chen et al., 2017), result-
ing in the over-exploration issue. Therefore, OVD-Explorer particularly takes into account the
aleatoric uncertainty, leveraging distributional RL paradigm to characterize aleatoric uncertainty
(Bellemare et al., 2017; Dabney et al., 2018b;a). OVD-Explorer uses two value distribution estima-
tors Z(S, a; θ1) and Z(S, a; θ2) parameterized by θ1 and θ2, as ensemble estimators to formulate Zn
and Zn in different ways. Unless stated otherwise, the (S, a) is omitted hereafter to ease notation.
Formulation of Zn. The Zn denotes the upper bounds of value that policy ∏ can reach via different
actions. We propose Gaussian distribution with optimistic mean value for formulation as follows,
and accordingly refer to it as Optimistic Value Distribution (OVD):
Z (S,a) ~ NXμZ(S,a),σaleatoriC(S，a))，	(IO)
where μz(s, a) and σ2leatoric(S, a) is mean value and variance, respectively. Notable, Chen et al.
(2017) discovers the optimisticity is beneficial for better estimating the upper bound, which moti-
vates us to optimistically estimate the μz(s, a) by considering epistemic uncertainty as follows:
μ(S, a) = Ei〜U(1,N)Ek = 1,2Zi(S, a; θk)
μZ(s,a) = μ(s,a) + βσepistemic(s,a), s∙t∙	2	^	,
σ2pistemiC(S, a) = Ei〜U(1,N)vark=1,2Zi(S, a； Ok)
(11)
4
Under review as a conference paper at ICLR 2022
Table 1: The comparison about two scenarios. Note that for the ease of clarity, without causing
ambiguity, we omit part of the notation in the table.
Fig.2	Aleatoric uncertainty σ0	Epistemic uncertainty σe	Optimistic value estimation μz (s, a)	CDF value Φ(a)	Action a
(a) (b)	σa(aι) = σa(a2) σa(αι) < σa(a2)	σe(aι) > σe(a2) σeSI) = be(a2)	μz(S, aI) > μz(S,a2) μz(S, aQ = μz(S, a2)	Φ(a1) > Φ(a2) Φ(a1) > Φ(a2)	aι aι
where μ(s, a) represents the Q-value estimation, σepistemic(s, a) is the epistemic uncertainty, β is a
hyper-parameter controlling the magnitude of epistemic uncertainty, U is uniform distribution, N is
the number of quantiles, and Zi(S, a; θk) is the value of the i-th quantile drawn from Z(S, a; θk).
Moreover, the aleatoric uncertainty should be considered as it will affect the policy’s performance,
thus We propose to model it as the variance of Zπ. In practice, the aleatoric uncertainty σaieatoric(s, a)
can be captured by value distribution estimators (Clements et al., 2019) as follows:
σaleatoric(S, a) = Vari~U(1,N) [Ek=1,2Zi(S, a； θkR
(12)
Leveraging optimistic value estimations (via epistemic uncertainty) together with explicitly mod-
eling aleatoric uncertainty, the upper bound of the value Zn can be effectively formulated as an
optimistic Gaussian distribution. Such an upper bound value estimation provides a useful guidance
for OVD-Explorer to derive the behavior policy for online exploration.
Formulation ofZπ. Zπ estimates the value obtained following policy π. Inspired by TD3 (Fujimoto
et al., 2018), to alleviate overestimation, we propose to formulate Zπ in a pessimistic way. In
practice, Zn can be measured in two ways. First, similar to formulating Zn in Equation 10, Zπ can
also be formulated as Gaussian distribution as follows:
Zπ(s,a)〜N(μz∏ (S,a),σ2leatoric(s,a)), S.t. μz∏ (s,a) = μ(s,a) - βσepistemic(s, a), (13)
where μ(s, a), σaleatoric(s, a) and σepistemic(s, a) are the same defined in Equation 11. Differently,
σepistemic(s, a) is subtracted from μ(s, a), which reveals the pessimistic value estimation of Zπ.
Another way is to formulate Zπ as multivariate uniform distribution as follows:
Z π (S, a) ^U{Zπ (S, a； θ)}i=1,...,n,
s.t. Ziπ (S, a； θ)
min Zi(s,a; θk),
k=1,2
(14)
where each quantile value Ziπ(S, a； θ) equals to the minimum estimated value among ensemble esti-
mators (i.e., Zi(S, a； θk). As such, the estimated value of Zπ is relative pessimistic.
Adopting pessimistic value estimation avoids overestimation issue, thereby the value distribution
Zπ can be more accurately formulated. Additional, Gaussian distribution helps more when the
environment randomness follows a unimodal distribution, and multivariate uniform distribution can
be more flexible and suitable for scenarios with multi-modal distributions. (see more in Section 5.3.)
3.3	Analysis of OVD-Explorer
This section analyzes how OVD-Explorer optimistically explores the informative areas and avoids
the over-exploration issue. According to Theorem 1, the exploration policy OVD-Explorer derives
can maximize Fn(s), which is proportional to CDF value Φz∏ (z(s, a)). In the following, Fig-
ure 2(a) and 2(b) illustrate such CDF values for different actions (shaded area), and Table 1 shows
how uncertainties affect the exploration.
In these cases, the value distribution Zπ(S, a) is specified as a Gaussian distribution (in Equation 13),
and the sampled optimistic value z(s, a) is specified as the mean of OVD μz (s, a) (in Equation 11).
Specifically, at state S, we assume that the means of Zπ at actions a1 and a2 are the same for ease
of clarification.
Basically, Figure 2(a) shows OVD-Explorer can achieve more optimistic exploration. We assume
the aleatoric uncertainty at a1 and a2 are the same, but epistemic uncertainty is higher at a1, causing
μz(s, aι) > μz(s,a2). Thus the CDF value is larger at aι, which means the action with higher
epistemic uncertainty is preferred.
5
Under review as a conference paper at ICLR 2022
More crucially, Figure 2(b) shows that OVD-
Explorer could guide to avoid the area with
higher aleatoric uncertainty given the equal op-
timism. We assume that the epistemic un-
certainty at a1 and a2 are the same, causing
their optimistic value estimation to be equal to
μz(s, a), but aleatoric uncertainty is lower at
a1, i.e., PDF curve of Zπ(s, a1) is ”thinner and
taller”. Thus the CDF value is larger at a1 ,
meaning that the action with lower aleatoric un-
certainty is preferred.
Figure 2: How OVD-Explorer explores (a) op-
timistically about epistemic uncertainty and (b)
pessimistically about aleatoric uncertainty.
In the limit of t → ∞, the epistemic uncertainty
tend to be 0, and the aleatoric uncertainty esti-
mated in Equation 12 converges to represent the true environment randomness. This is comparable
to the second case (Figure 2(b)), and OVD-Explorer tends to choose the one with lower aleatoric
uncertainty, which is a great advantage of our method over other OFU methods.
To summarize, the exploration guided by OVD-Explorer trades off between two criteria: exploring
the areas with higher epistemic uncertainty, to ensure exploring optimistically, and avoiding the areas
with higher aleatoric uncertainty, to avoid over-exploration caused by high environment randomness.
4	OVD-Explorer for modern RL algorithms
For continuous action space, the argmax operator in Equation 7 is intractable. To address that, in
this section, we derive the behavior policy πE in closed form and state the scheme to incorporate it
with existing policy-based algorithms.
First, we denote the policy learned by any policy-based algorithm as the target policy πT . To avoid
the gap between training data collected using the behavior policy πE and the target policy πT, we
need to constrain the difference between πE and πT , thus we derive πE in the vicinity of πT .
Second, to derive exploration πE for modern RL algorithms with stochastic policy based on OVD-
Explorer, where both the exploration ∏e = N(μE, ∑e ) and target policy ∏t = N(μτ, ∑t ) are
Gaussian distributions, we introduce the following proposition:
Proposition 1. The OVD-Explorer behavior policy ∏e = N(μE, ∑e ) is asfollows:
μE = μτ + αEz∏
∂z(s, a)
m X ∂a	la="τ
(15)
and
ΣE = ΣT.	(16)
In specific, α is the step size controlling the exploration level and m = log φZπ(s,μTC(z(s,*T))+ 1
(see proof in Appendix B.2).
The expectation EZn can be estimated by sampling K samples, then Equation 15 is simplifies as:
μE
αm Ky ∂zi(s, a)
μT + ɪ ⅛ Fla=μτ.
(17)
Algorithm 1 summarizes the overall procedure of OVD-Explorer, including the formulation of Zn
and Zπ (Line 2 and line 3) and behavior policy generation (Line 4). The generated behavior pol-
icy can be integrated with any modern policy-based RL algorithms to render the stable and well-
performed algorithm. Please see Appendix C for the entire pseudo-code and details about how the
OVD-Explorer-based behavior policy is incorporated with DSAC.
5	Experiments
To reveal the consistency between our theoretical analysis and the performance of OVD-Explorer
algorithm, we conduct experiments to address the following questions:
6
Under review as a conference paper at ICLR 2022
Algorithm 1 The behavior policy (i.e., exploration policy) derived from OVD-Explorer.
Input: Current state st, current value distribution estimators θ1,θ2, current policy network φ.
Output: Behavior policy πE.
1:	Obtain target policy from policy-based RL algorithm ∏t(∙∣st; φ)〜N(μτ(st； φ),στ(st； φ))
2:	Derive OVD Zn(St,μτ(st； φ)) using Eq. 10.
3:	Construct value distribution of behavior policy Zπ (st,μτ (st； φ)) using Eq. 13 or 14.
4:	Calculate the mean of behavior policy distribution μE using Eq. 17.
5:	return ∏e 〜N(μE, στ(st； φ))
RQ1 (Exploration): Can OVD-Explorer explore optimistically while avoiding over-exploration
simultaneously?
RQ2 (Performance): Can OVD-Explorer handle complex and even stochastic tasks?
RQ3 (Sensitivity to α): Is OVD-Explorer sensitive to α that controls exploration magnitude?
5.1	Experiment Setup
Baselines include SAC (Haarnoja et al., 2018), DSAC (Ma et al., 2020), and DOAC, which is the dis-
tributional variant of OAC (Ciosek et al., 2019). We implement Gaussian and quantile formulations
of Zπ as in Equation 13 and 14, which are denoted in the following as OVDE_G and OVDE_Q,re-
spectively. We test OVD-Explorer on a novel task GridChaos and several tasks in Mujoco (Todorov
et al., 2012) including the stochastic variants.
The appendix gives more details, including environment settings, implementation details, hyper-
parameters settings, and computing infrastructure used, as well as more experiment evaluations,
including tasks with different noise scales, tasks with different episode horizon, the sensitivity to β
and ablation study on the pessimistic formulation of Zπ(s, a).
5.2	Exploration in GridChaos (RQ1)
To illustrate the exploration pattern of OVD-Explorer and show the advantage of OVD-Explorer
over DSAC and DOAC, we evaluate OVD-Explorer on a novel continuous and stochastic control
task called GridChaos.
Figure 3(a) shows the map of GridChaos, in
which we control the cyan triangle (agent) aim-
ing to reach the fixed dark blue goal. The
state is the current coordinate, and the action is
a two-dimensional vector including the move-
ment angle and distance. One episode termi-
nates when the agent reaches the goal or max-
imum steps (i.e., MAX-STEP, typically 100).
Also, it receives a +100 reward when reach-
ing the goal otherwise 0. The reason why it is
chaos is that the randomness of the transition
is heterogeneous in different parts of this envi-
ronment. Table 2 shows the environment set-
tings. Moreover, Figure 3(b) shows that OVD-
Explorer can reach the goal faster, with more
efficient exploration.
Table 2: Settings of GridChaos.		
	Value	Description
Observation[0]	[-1,1]	X-coordinate
Observation[1]	[-1, 1]	Y-coordinate
Action[0]	[-1, 1]	Degree, mapped to [-π, π]
Action[1]	[-1, 1]	Distance, mapped to [0, MAX-STEP]
Noise_0	0.5	Variance of Gaussian noise in the left half of the map (default)
Noise」	0.1	Variance of Gaussian noise in the right half of the map (default)
Figure 3(c) shows the values of uncertainty estimation and exploration objective (mutual informa-
tion) taken four different actions at the state shown in Figure 3(a) and at training epoch 1249. Ba-
sically, Figure 3(c) illustrates that the estimated aleatoric uncertainty of left is higher than that of
right, indicating that OVD-Explorer models aleatoric uncertainty properly. Further, OVD-explorer
encourages to explore the right side at that time, since the value of exploration objective (in green) is
highest. It implies that OVD-explorer tends to explore areas with higher epistemic uncertainty and
avoiding higher aleatoric uncertainty, which is in accordance with our exploration principle. On the
other hand, if high epistemic uncertainty is considered only like in OAC(Ciosek et al., 2019), the
7
Under review as a conference paper at ICLR 2022
(a)
Type
Epistemic uncertainty
Aleatoric uncertainty
Mutual information
(b)
Figure 3: GridChaos. (a) The cyan triangle can move to reach the goal at the top right. (b) The
performance on it. (c) The values about uncertainty and exploration objective (mutual information).
(c)
agent would be guided to the left. Then the agent may be trapped in the left side due to the high
aleatoric uncertainty, and it is the possible reason about why OAC fails tackling such heteroscedastic
stochastic task. In appendix, we show more analysis about the exploration pattern, please refer to
Appendix E.2 and Appendix E.10.
Besides, we conduct the evaluation when the randomness is high around the goal, as well as many
other noise settings, the results enhance the ability of OVD-Explorer. More experiments that evaluate
the performance on several other noise settings, as well as more detailed value histogram on different
epoch in training process can be found in Appendix.
5.3	Performance on Mujoco tasks (RQ2)
To demonstrate the performance of OVD-Explorer more generally, we evaluate it on several Mujoco
tasks. For 5 standard Mujoco tasks 1, the transition is deterministic and the randomness is only from
the stochastic policy. For 5 noisy tasks, the heteroscedastic Gaussian noise is added in Mujoco tasks.
Specifically, in each state transition on noisy tasks, Gaussian noise of different scales is randomly
injected following a certain probability. Overall, we have the following findings.
Basically, OVD-Explorer can perform stably in the standard tasks with slight or little randomness,
guiding efficient exploration. From the results2 in standard Mujoco tasks in table 3, itis shown that in
the relatively easy tasks, i.e., Hopper-v2, Reacher-v2, and HalCheetah-v2, OVD-Explorer does not
obtain much gain beyond the baselines, which seems that these tasks are not profoundly demanding
for exploration. In the high-dimension tasks, i.e., Ant-v2 as shown in Figure 4(a), OVD-Explorer
significantly outperforms DSAC and DOAC. It means that the aleatoric uncertainty caused by policy
indeed degrades the performance of DOAC, where the aleatoric and epistemic uncertainties are not
distinguished.
Besides, OVD-Explorer can avoid the impact of heteroscedastic aleatoric uncertainty on exploration
and thus improve robustness. As the results of noisy Mujoco tasks shown in Table 3, DOAC is worse
than DSAC in most cases, which means that heteroscedastic aleatoric uncertainty causes significant
degrades of DOAC. Simultaneously, OVD-Explorer significantly outperforms DOAC and DSAC,
especially in Noisy Ant-v2 as shown in Figure 4(b).
Furthermore, OVDE_G takes the Gaussian prior and concretely obtains better estimation of critic in
the noisy tasks, while OVDE_Q performs better in the standard tasks due to the flexibility of quantile
distribution. Generally, the difference between OVDE_G and OVDE_Q is that they use different
formulations of Zπ(s, a). Theoretically, the value function distribution in OVDE_Q is more flexible
and should perform better than OVDE_G, as the reported results on five standard Mujoco tasks show.
On the other hand, OVDE_G performs better in the noisy tasks, and it is because that the transition
probability is Gaussian in the stochastic Mujoco tasks.
1https://github.com/openai/gym/tree/master/gym/envs/mujoco
2Here, the reported results may be slightly different from previously reported results, partly due to the
statistic approach, and partly due to the implementation. Nevertheless, the patterns of these baselines (e.g.,
DSAC outperforms SAC in the vast majority of cases) are consistent with previous results. The details about
the implementation of baselines are described in the Appendix D.2.
8
Under review as a conference paper at ICLR 2022
Table 3: Comparisons of algorithms on five standard and five noisy tasks in Mujoco. We report
the averaged performance and standard deviation of 5 runs. Each trail uses the mean undiscounted
episodic return over the last 8% epoch (or at most the last 100 epoch) to avoid bias, and the total
epoch number is shoWn in column epoch. The maximum value of each roW is shoWn in bold.
Task	Epoch	SAC	DSAC	DOAC	OVD-EXPLORER-G	OVD-EXPLORER-Q
Ant-v2	2500	4706.2±1338.9	6206.9±1202.5	6586.7±1023.3	7160.6±763.2	7590.3±154.9
HalfCheetah-v2	2500	12373.8±860.9	13890.0±3424.4	12977.0±140.4	14084.5±1579.8	14792.4±997.4
Hopper-v2	1250	2751.8±775.7	2199.7±602.7	2215.1±557.1	2239.5±428.2	2619.3±457.0
Reacher-v2	250	-21.6±2.5	-11.9±0.5	-19.8±1.7	-11.6±2.4	-10.8±1.4
InvDbPendulum-v2	300	9344.0±28.4	9359.6±0.1	5109.4±3638.7	9128.0±460.6	9351.3±16.3
N-Ant-v2	2500	261.5±57.6	416.38±42.16	337.39±11.96	492.54±50.44	450.34±58.42
N-HalfCheetah-v2	1250	351.91±6.68	431.39±35.68	417.47±39.62	445.28±37.52	429.63±34.45
N-Hopper-v2	1250	207.06±19.49	244.53±4.71	242.74±7.87	252.09±7.82	237.68±13.11
N-Pusher-v2	1250	-46.92±12.12	-25.31±2.46	-39.13±9.06	-23.41±0.69	-28.51±4.53
N-InvDbPendulum-v2	300	934.36±1.91	932.78±4.02	496.61±205.8	933.67±1.54	934.64±0.95
∈ 6000
n
⅞
ω 4000-
6
ID
L.
ω
2 2000
0....................................
0	500 1000 1500 2000 2500
Number OfTraining Epoch
500
u,lnsα
0
6	500 1000 1500 2000 2500
Number OfTraining Epoch
360
Ooooo
4 2 0 8 6
3 3 3 2 2
u∙JnΦCTΛ^ω><
240-
OQ 02	0：4	0：6	0：8
alpha value
(a) Ant-v2	(b) Noisy Ant-v2	(c) Noisy Ant-v2
Figure 4: Training curves on (a) Ant-v2 and (b) Noisy Ant-v2. The x-axis indicates the number of
training epochs, while the y-axis is the evaluation result represented by the average episode return.
The shaded region denotes the half standard deviation of average evaluation over 5 seeds. Curves are
smoothed uniformly for visual clarity. (c) Sensitivity to α. The x-axis indicates different α settings,
while the y-axis is the evaluation result represented by average episode return in the last 100 epoch
before total 1250 epoch. Error bars indicate half standard deviation of average evaluation over 5
seeds. The 9 different α values are 0.0005, 0.01. 0.025, 0.05, 0.075, 0.1, 0.25, 0.4, 0.8, respectively.
5.4 SENSITIVITY TO α (RQ3)
The step size α in Equation 15 controls how much the behavior policy derived from OVD-Explorer,
i.e. πE, is far away from the target policy πT , which can be essential for exploration benefit. To
investigate the appropriate range of it, We test several α value on Noisy Ant-V2 task using OVDE,G,
and the result is shown in Figure 4(c). If α is quite small, OVD-Explorer degenerates to DSAC and
implies little exploration. In contrast, if α is larger, the performance becomes Worse because of the
huge gap betWeen behavior policy and target policy. The result demonstrates that α should be taken
in a suitable range to facilitate more adequate exploration. In our experiments, We uniformly use
α to be equal to 0.05 to shoW the performance of the OVD-Explorer. In addition, We find that a
smaller α leads to higher gains When the task is much more difficult, and the details can be found in
Appendix E.6. We also verify the sensitivity of OVD-Explorer to β in Appendix E.7 and found that
there is a broad range of settings for β, Which can lead to Well performance.
6 Conclusion
This paper proposes an information-theoretic exploration method OVD-Explorer, Which introduces
a novel measurement of exploration ability, i.e., the mutual information betWeen the policy and the
upper bounds of return. By maximizing the mutual information, OVD-Explorer is able to derive
the behavior policy, that folloWs the OFU principle, and further avoids exploring the areas With high
aleatoric uncertainty. Integrated With SAC, OVD-Explorer addresses the negative impact of aleatoric
uncertainty for exploration in continuous RL for the first time.
9
Under review as a conference paper at ICLR 2022
References
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit
problem. Mach. Learn., 47(2-3):235-256, 2002.
Syrine Belakaria, Aryan Deshwal, and Janardhan Rao Doppa. Max-value entropy search for multi-
objective bayesian optimization with constraints. CoRR, abs/2009.01721, 2020.
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, R. Devon
Hjelm, and Aaron C. Courville. Mutual information neural estimation. In Proceedings of the
35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm,
Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 530-
539. PMLR, 2018.
Marc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi
Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Infor-
mation Processing Systems, pp. 1471-1479, 2016.
Marc G. Bellemare, Will Dabney, and Remi Munos. A distributional perspective on reinforcement
learning. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017,
Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Re-
search, pp. 449-458. PMLR, 2017.
Richard Y Chen, Szymon Sidor, Pieter Abbeel, and John Schulman. Ucb exploration via q-
ensembles. arXiv preprint arXiv:1706.01502, 2017.
Pengyu Cheng, Weituo Hao, Shuyang Dai, Jiachang Liu, Zhe Gan, and Lawrence Carin. CLUB:
A contrastive log-ratio upper bound of mutual information. In Proceedings of the 37th Inter-
national Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, vol-
ume 119 of Proceedings of Machine Learning Research, pp. 1779-1788. PMLR, 2020. URL
http://proceedings.mlr.press/v119/cheng20b.html.
Kamil Ciosek, Quan Vuong, Robert Loftin, and Katja Hofmann. Better exploration with optimistic
actor critic. In Advances in Neural Information Processing Systems 32: Annual Conference on
Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver,
BC, Canada, pp. 1785-1796, 2019.
William R. Clements, Beno^t-Marie Robaglia, Bastien Van Delft, Reda Bahi Slaoui, and SebaStien
Toth. Estimating risk and uncertainty in deep reinforcement learning. CoRR, abs/1905.09638,
2019.
Will Dabney, Georg Ostrovski, David Silver, and Remi Munos. Implicit quantile networks for distri-
butional reinforcement learning. In Proceedings ofthe 35th International Conference on Machine
Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018, volume 80 of
Proceedings of Machine Learning Research, pp. 1104-1113. PMLR, 2018a.
Will Dabney, Mark Rowland, Marc G. Bellemare, and Remi Munos. Distributional reinforcement
learning with quantile regression. In Proceedings of the Thirty-Second AAAI Conference on Artifi-
cial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18),
and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New
Orleans, Louisiana, USA, February 2-7, 2018, pp. 2892-2901. AAAI Press, 2018b.
Richard Dearden, Nir Friedman, and Stuart J. Russell. Bayesian q-learning. In Proceedings of
the Fifteenth National Conference on Artificial Intelligence and Tenth Innovative Applications of
Artificial Intelligence Conference, AAAI 98, IAAI 98, July 26-30, 1998, Madison, Wisconsin, USA,
pp. 761-768, 1998.
Lior Fox, Leshem Choshen, and Yonatan Loewenstein. Dora the explorer: Directed outreaching
reinforcement action-selection. In International Conference on Learning Representations, 2018.
Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. In Proceedings of the 35th International Conference on Machine Learning,
ICML2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings
of Machine Learning Research, pp. 1582-1591. PMLR, 2018.
10
Under review as a conference paper at ICLR 2022
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the
35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm,
Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 1856-
1865. PMLR, 2018.
Geoffrey E. Hinton and Drew van Camp. Keeping the neural networks simple by minimizing the
description length of the weights. In Proceedings of the Sixth Annual ACM Conference on Compu-
tational Learning Theory, COLT 1993, Santa Cruz, CA, USA, July 26-28, 1993, pp. 5-13. ACM,
1993.
Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime:
Variational information maximizing exploration. In Advances in Neural Information Processing
Systems, pp. 1109-1117, 2016.
Hyoungseok Kim, Jaekyeom Kim, Yeonwoo Jeong, Sergey Levine, and Hyun Oh Song. EMI:
exploration with mutual information. In Proceedings of the 36th International Conference on
Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of
Proceedings of Machine Learning Research, pp. 3360-3369, 2019.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd Inter-
national Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,
2015, Conference Track Proceedings, 2015.
Johannes Kirschner and Andreas Krause. Information directed sampling and bandits with het-
eroscedastic noise. In Conference On Learning Theory, COLT 2018, Stockholm, Sweden, 6-9
July 2018, volume 75 of Proceedings of Machine Learning Research, pp. 358-384. PMLR, 2018.
Roger Koenker and Kevin F Hallock. Quantile regression. Journal of economic perspectives, 15(4):
143-156, 2001.
Shibo Li, Wei Xing, Robert M. Kirby, and Shandian Zhe. Multi-fidelity bayesian optimization
via deep neural networks. In Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In 4th
International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May
2-4, 2016, Conference Track Proceedings, 2016.
Xiaoteng Ma, Qiyuan Zhang, Li Xia, Zhengyuan Zhou, Jun Yang, and Qianchuan Zhao. Distribu-
tional soft actor critic for risk sensitive learning. CoRR, abs/2004.14547, 2020.
Jarryd Martin, Suraj Narayanan Sasikumar, Tom Everitt, and Marcus Hutter. Count-based explo-
ration in feature space for reinforcement learning. In Proceedings of the Twenty-Sixth Interna-
tional Joint Conference on Artificial Intelligence, pp. 2471-2478, 2017.
Borislav Mavrin, Hengshuai Yao, Linglong Kong, Kaiwen Wu, and Yaoliang Yu. Distributional rein-
forcement learning for efficient exploration. In Proceedings of the 36th International Conference
on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of
Proceedings of Machine Learning Research, pp. 4424-4434. PMLR, 2019.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nat., 518(7540):529-533, 2015.
Volodymyr Mnih, Adria PUigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International Conference on Machine Learning, pp. 1928-1937, 2016.
11
Under review as a conference paper at ICLR 2022
Thomas M. Moerland, Joost Broekens, and Catholijn M. Jonker. Efficient exploration with double
uncertain value networks. CoRR, abs/1711.10789, 2017.
Nikolay Nikolov, Johannes Kirschner, Felix Berkenkamp, and Andreas Krause. Information-
directed exploration for deep reinforcement learning. In 7th International Conference on Learning
Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. In Advances in Neural Information Processing Sys-
tems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10,
2016, Barcelona, Spain,pp. 271-279, 2016.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped DQN. In Advances in Neural Information Processing Systems 29: Annual Confer-
ence on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain,
pp. 4026-4034, 2016.
Georg Ostrovski, Marc G. Bellemare, Aaron van den Oord, and Remi Munos. Count-based ex-
ploration with neural density models. In Proceedings of the 34th International Conference on
Machine Learning, pp. 2721-2730, 2017.
Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration
by self-supervised prediction. In Proceedings of the 34th International Conference on Machine
Learning, pp. 2778-2787, 2017.
Deepak Pathak, Dhiraj Gandhi, and Abhinav Gupta. Self-supervised exploration via disagreement.
In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June
2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research,
pp. 5062-5071. PMLR, 2019.
Valerio Perrone, Iaroslav Shcherbatyi, Rodolphe Jenatton, Cedric Archambeau, and Matthias W.
Seeger. Constrained bayesian optimization with max-value entropy search.	CoRR,
abs/1910.07003, 2019.
Nikolay Savinov, Anton Raichuk, Damien Vincent, Raphael Marinier, Marc Pollefeys, Timothy P.
Lillicrap, and Sylvain Gelly. Episodic curiosity through reachability. In 7th International Con-
ference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. Open-
Review.net, 2019.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman,
Filip De Turck, and Pieter Abbeel. #exploration: A study of count-based exploration for deep
reinforcement learning. In Advances in Neural Information Processing Systems, pp. 2753-2762,
2017.
William R Thompson. On the likelihood that one unknown probability exceeds another in view of
the evidence of two samples. Biometrika, 25(3/4):285-294, 1933.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based con-
trol. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2012,
Vilamoura, Algarve, Portugal, October 7-12, 2012, pp. 5026-5033. IEEE, 2012.
Zi Wang and Stefanie Jegelka. Max-value entropy search for efficient Bayesian optimization. vol-
ume 70 of Proceedings of Machine Learning Research, pp. 3627-3635, International Convention
Centre, Sydney, Australia, 06-11 Aug 2017. PMLR.
Christopher J. C. H. Watkins and Peter Dayan. Technical note q-learning. Mach. Learn., 8:279-292,
1992.
Fan Zhou, Jianing Wang, and Xingdong Feng. Non-crossing quantile regression for distributional
reinforcement learning. In Advances in Neural Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual, 2020.
12
Under review as a conference paper at ICLR 2022
A Related Work
In this work, we consider the exploration strategy under the principle of Optimism in the Face of
Uncertainty (OFU) (Auer et al., 2002), especially in the heteroscedastic stochastic environment. We
aim to improve exploration efficiency, and alleviate the over exploration issue caused by aleatoric
uncertainty.
Basic exploration strategies, like -greedy (Sutton & Barto, 2018), noise perturbation (Lillicrap et al.,
2016), entropy regularization (Mnih et al., 2016) and stochastic policy (Haarnoja et al., 2018), lead
to undirected exploration through random perturbations. With the increasing emphasis on explo-
ration efficiency in RL, various exploration methods have been developed. One kind of methods
uses intrinsic motivation to stimulate agent to explore, such as count-based novelty (Martin et al.,
2017; Ostrovski et al., 2017; Bellemare et al., 2016; Tang et al., 2017; Fox et al., 2018), prediction
error (Pathak et al., 2017), reachability (Savinov et al., 2019) and information gain on environment
dynamics (Houthooft et al., 2016). Some recent methods, originating from tracking uncertainty,
guide efficient exploration under the principle of OFU, such as Thompson Sampling (Thompson,
1933; Osband et al., 2016), IDS (Nikolov et al., 2019; Clements et al., 2019) and other customized
methods (Moerland et al., 2017; Pathak et al., 2019).
The base of OFU methods is to model epistemic and aleatoric uncertainties in RL. Bootstrapped
DQN (Osband et al., 2016) has become the well-used approach for capturing epistemic uncertainty
(Kirschner & Krause, 2018; Ciosek et al., 2019), and distributional RL methods (Bellemare et al.,
2017; Zhou et al., 2020; Dabney et al., 2018a;b) are used for capturing aleatoric uncertainty. How-
ever, most traditional OFU methods do not distinguish the two types of uncertainty, which can easily
lead the naive solution to favor actions with higher variances in stochastic tasks, i.e., over-exploration
issue.
To address that, Mavrin et al. (2019) study how to take advantage of value distribution for effi-
cient exploration under both types of uncertainty, proposing Decaying Left Truncated Variance
(DLTV) based on QR-DQN. Besides, Nikolov et al. (2019) and Clements et al. (2019) propose
to use Information Direct Sampling (Kirschner & Krause, 2018) for efficient exploration in RL,
which formulates epistemic and heteroscedastic aleatoric uncertainty and maximizes information
gain on globally optimal action to explore informative state-action pairs. However, such methods
are complicated when deriving a behavior policy and is limited to discrete control.
Meanwhile, there is not any strategy that can help the well-performed continuous RL algorithms
(Haarnoja et al., 2018; Ciosek et al., 2019; Ma et al., 2020) to address aleatoric uncertainty when
exploration. OAC (Ciosek et al., 2019) proposes exploration bonus guided by the upper bound
of Q estimation to facilitate exploration based on Soft Actor-Critic (SAC) (Haarnoja et al., 2018).
Nevertheless, OAC ignores the potential impact of the aleatoric uncertainty, which may cause mis-
leading exploration. Our proposed OVD-Explorer is a novel exploration strategy, which can guide
agent to explore towards higher epistemic uncertainty, and also avoid the areas with high aleatoric
uncertainty, improving the robustness of exploration especially facing heteroscedastic aleatoric un-
certainty.
To capture aleatoric uncertainty, OVD-Explorer models the value distribution and uses mutual in-
formation to guide exploration following the principle of OFU, measuring the correlations between
the policy distribution and upper bounds distribution of return. There are some other information-
theoretic exploration strategies using mutual information, such as VIME (Houthooft et al., 2016),
which measures the information gain on environment dynamics, and EMI (Kim et al., 2019), which
generates intrinsic reward using prediction error of representation learned by mutual information.
Those methods can solve sparse reward problem very well by using intrinsic reward. Nevertheless,
those exploration methods use mutual information neither on the value distribution, nor for OFU-
based exploration. Besides, unlike the mechanisms used in measuring mutual information, such as
variational inference (Hinton & van Camp, 1993; Houthooft et al., 2016) and f-divergence (Nowozin
et al., 2016; Kim et al., 2019), we find the correlation between policy and upper bounds of return
through uncertainty as shown in Theorem 1, thus we can directly derive the close form exploration
policy.
13
Under review as a conference paper at ICLR 2022
B Proofs
B.1 Proof of Theorem 1
In order to prove the Theorem 1, we first propose the following lemma about Fπ (s).
Lemma 1. The mutual information of Zπ (s,a0), ∙∙∙ ,Zπ(s, ak-ι) and π(∙∣s) at state S is:
Fπ (∙∣s)
E
(∙∣s) W(S,a)~Zn(S,a)
p(a∣w(s, a), s)log pSlz(s,? S)
da,
(18)
L
a〜π
where p(a,(s,a),s) represents the posterior probability distribution of policy given current state S
and the sampled upper bound of return z(s, a).
Proof. For simplicity, we assume that the size of action space is k = 2, and the actions are denoted
as a0 and a1 , a0 6= a1 . Then we derive the mutual information among three random variables
MI(Zπ(s, ao), Zπ(s, aι), π(∙∣s)), where the action sampled from π(∙∣s) is either a° or a>
Considering the formula for the mutual information, Fπ (S) is derived as follows:
Fπ (∙∣s) = MI(Zπ(s,ao),Zπ(s,aι); ∏(∙∣s)∣s)
Σ
a 〜π(∙∣s)
W(s,ao)〜Zn (s,ao)
W(s,aι)〜Zn (s,aι)
p(a,z(S,aO),z(S,aI))
P(a, z(S, aO), z(S, aI))Iog / I -------「7------K
π(a∣S)p(z(S,ao),z(S,aι))
Σ
a 〜π(∙∣s)
W(s,ao)~Zn (s,ao)
W(s,aι)〜Zn (s,aι)
p(aB(S, ao), z(s, aι))p(w(S, ao), W(s, aι))log
p(a,(S,ao ),Z(S,aι))
π(a∣S)
Σ
E
Z(s,ao)〜Zn (s,ao)
z(s,αι)^Zn (s,aι)
P(OB(S,aθ),z(S,al))
p(a∣z(S, ao), z(S, aι)) log----厂「---------
where the posterior distribution p(a,(S, ao), z(s, aι)) is the probability of choosing action a on the
condition of the samples from upper bounds of action aO and a1.
Considering that in the decision-making process, the probability of action aO is independent to
the upper bound of other actions, such as z(s, aι), which means that p(ao∣z(S, ao), w(s, aι)) =
p(ao |z(s, ao)). Therefore, the above equation can be further reduced as follows.
「π∕∣、	l^ /	、 、i	p(ao∣z(S, ao), s)
F (∙∣s) = E	p(ao∣z(S,ao),S)log-------,-p-------
W(s,ao)〜Zn(s,ao) L	n(a0|S)	」
L Γ /	∣-/	、	P(OIB(S,。。3「
+ E	p(aι ∣z(s, aι), s) log---；—rʒ------ .
W(s,aι)〜Zn (s,aι) L	n(a1|s)	」
It is easy to extend to infinite-action case. Based on that for any action ai ∈ [ao, ak-1] and k → ∞,
the conditional probability p(ai∣z(s, ao), ^(s, aι),..., ^(s, ak-ι)) = p(a∕z(s,ai)), Fπ(∙∣s) can be
simplified as following,
Fπ (∙∣s)
E
a 〜π(∙∣s) W(s,a)〜Zn (s,a)
p(a⑸s,a),s)log 双小/ S)
da.
(19)
□
Lemma 1 tells that the mutual information Fπ(St) is in direct proportion to p(a∣w(s, a), s), which
measures how much it is worth acting under the current policy ∏(a∣s) when the upper bound is
known.
14
Under review as a conference paper at ICLR 2022
Next, to measure the posterior probability p(a∣w(s, a), s), we use a general and practically effective
approach (Wang & Jegelka, 2017; Belakaria et al., 2020; Perrone et al., 2019; Li et al., 2020) of
approximating the posterior probability given upper bound value.
Specifically, We approximate p(a∣z(s, a), S) using the prior that zπ(s, a) ≤ z(s, a) with given pol-
icy π(s,a), since z(s, a) is the upper bound of zπ(s,a). Hence, we use the indicator function
1z∏(s,a)≤w(s,a) to truncate the policy π(s, a), and utilize the constant C to normalize the probability,
as is shown in the following equation.
P⑷ZGa), S) ≈ 1πWaIS)Ezn
(s,a)〜Zπ (s,a) [ɪzπ (s,a)≤∑(s
C
Here, Ez∏(s,a)〜Zn⑶。)[lz∏9,a)≤z(s,a)] = Φζ∏(s,a)(z(s, a)), where Φχ is the cumulative distribu-
tion function (CDF) of x, Zn and Zπ are the random variables, whose distributions describe the
randomness of the returns, and z(s, a) is the value of random variable Zn.
Therefore, the posterior probability can be measured as follows,
p(a∣W(s,a),s) ≈ 1 π(a∣s)Φz∏ (z(s,a)).	(20)
C
In our method, we do not use the commonly used mechanisms about mutual information such as
neural network estimation (Belghazi et al., 2018) and upper bound estimation (Cheng et al., 2020).
Instead, we can find the correlation between random variables as shown in Equation 20, which helps
to derive mutual information directly.
According to Lemma 1 and Equation 20, we can give the proof of Theorem 1 in the following.
Proof. By Combining Lemma 1 and Equation 20, Fπ (S) can be further derived as follows.
Fπ(S)
E
Ja〜π(∙∣s) W(s,a)〜Zn(s,a)
p(aB(s, a), s)log pSlz(s,a), S)
≈E
Ja〜π(∙∣s) W(s,a)〜Zn(s,a) _
1∏ π(a∣s)Φz∏(z(s,a))log
C
da
π(a∣s)Φz∏ (z(s, a))
Cπ (a|S)
da
/	E	c1 π(a∣s)Φz∏ (z(s, a)) log φZn (z((s,a))
/a〜π(∙∣s) W(s,a)〜Zn(s,a) LC	C
da
1
E E
C	a 〜π(∙∣s)
W(s,a)〜Zn (s,a)
Φzn (z(s,a))logφZn(”⑼
C
Here, the last equality follows from Theorem 1.
□
B.2 Proof of proposition 1
Proof. Similar to (Ciosek et al., 2019), we set the covariance matrix of behavior policy πE is that
of target policyπT, i.e., ΣE = ΣT. Hence, the OVD-Explorer problem is simplified as:
μE = arg max F(S)
μ
一 工 /_/	、、•1	Φz∏ (Z(S,μ))
=arg max, E Φz∏ (z(S,μ))log-----------------
μ Zn	C
(21)
To ensure that the behavior policy samples actions around the target policy, we derive the πE upon
mean μτ of target policy πτ. In specific, we firstly obtain the gradient of F(s, μ) at πτ, which is
given as follows:
VaFπ (S,μ)∣μ="τ = EZn m × dz∂∣a) ∣a=μτ	(22)
15
Under review as a conference paper at ICLR 2022
Algorithm 2 OVD-EXplorer for DSAC
1:	Initialise: Value networks θ1, θ2, policy network φ and their target networks θ)1, O2, φ, quantiles
number N, target smoothing coefficient (τ), discount (γ), an empty replay pool D
2:	for each iteration do
3:	for each environmental step do
4:	at 〜πE (at, st) according to Algorithm 1
5:	D —D∪{(st ,at ,r(st ,at ),st+ι)}
6:	end for
7:	for each training step do
8:	for i = 1 to N do
9:	for j = 1 to N do
10:	calculate δik,j , k = 1, 2, following Eq. 4
11:	end for
12:	end for
13:	Calculate LQR(θk), k = 1, 2 using δik,j following Eq. 2
14:	Update θk with XILQR (θk)
15:	Calculate Jπ (φ), following Eq. 5
16:	Update φ with XJπ (φ)
17:	end for
18:	Update target value network with θOk — τθk + (1 - τ)θOk , k = 1, 2
19:	Update target policy network with φO — τφ + (1 - τ)φO
20:	end for
where r^ is given as:
ΦZπ(s,μτ)(Z(S,μτ)) l
m = φz ∏(s,μτ) (z(s,μτ ))(log ——tj^-------------+ 1),	(23)
C
and φ(x) is the probability distribution function (pdf). Hence, the μE is given as follows:
∂zO(s, a)
μE = μτ + αEz∏ m X —诟—∣a=μτ ,	(24)
where ɑ is the step size controlling exploration level and m = log φZπ	TCZzS平T"十 上 口
C Algorithm 2: OVD-Explorer for DSAC
In this section, we show the whole algorithm of our implementation of OVD-Explorer based on
DSAC in Algorithm 2. All the code can be found in the supplementary material.
D More details about the experiments
D.1 GridChaos
GridChaos is an environment built on OpenAI’s
Gym toolkit, whose map is shown as in Fig-
ure 3(a) and Figure 5. In this section we illus-
trate more details in addition to Section 5.2.
The movable cyan triangle and the fixed sym-
metric dark blue goal are two parts split from
square, and the goal is to make the triangle
embedded in the goal to recover the original
square, which is to say that it is an isosceles
triangle whose base side is equal to the height.
The triangle is always initialised randomly in
the cyan rectangle, and the black line in the map
represents the wall, where the triangle will be
Figure 5: Map of GridChaos.
16
Under review as a conference paper at ICLR 2022
Table 4: OVD-Explorer parameters
Parameter			Value
Training	Discount		0.99
	Target smoothing coefficient	τ	5e-3
	Learning rate		3e-4
	Optimizer		Adam (Kingma & Ba, 2015)
	Batch size		256
	Quantiles amount		20
	Replay buffer size		1.0 × 106 for Mujoco tasks
			1.0 × 105 for other tasks
	Environment steps per epoch		1.0 × 103 for MUjoCo tasks
			1.0 × 102 for other tasks
Exploration	Exploration ratio	α	0.05
	Uncertainty ratio	β	3.2
	Normalization factor	C	0.5
adsorbed once hits the wall. The state transition is stochastic, and we add Gaussian noise to the
action resulting in Gaussian transition probability.
To represent the location of the triangle, we establish a Cartesian coordinate system using the cen-
troid of the map as the origin, as shown in Figure 5. Then the coordinates of the triangle are repre-
sented by the midpoint of the altitude of the triangle which is shown as the red point in the triangle.
In the case shown in Figure 5, the initial coordinate of the triangle (agent) is in the negative half of
the x-axis and the task target is in the first quadrant.
D.2 Baselines
Ma et al. (2020) shows the performance of DSAC and TD4, which is the distributional extension
of TD3 (Fujimoto et al., 2018), and can also be used to capture epistemic and aleatoric uncertainty.
Moreover, DSAC outperforms TD4 on Mujoco tasks as shown in Ma et al. (2020), so we evaluate
only on SAC and DSAC, and further implement OVD-Explorer based on DSAC to develop the
exploration ability.
SAC. The SAC (Haarnoja et al., 2018) implementation is mainly based on OAC repository, and the
results in Ant-v2 and Hopper-v2 are similar to reported results by OAC. Our SAC report a better
result than OAC’s implementation for SAC on HalfCheetah-v2, which is because the high variance
of this environment as explained as in OAC.
DSAC. The DSAC (Ma et al., 2020) implementation is based on our implementation of SAC, except
that the distributional Q function in the DSAC repository is used instead of the traditional Q function
in SAC. As it is based on SAC, we set the hyper-parameters of DSAC to be consistent with SAC to
ensure the fair comparison, which also results in the different reported results from original paper of
DSAC. In our results, DSAC can guarantee an absolute advantage over SAC in most cases, which is
consistent with the previous conclusion.
DOAC. The DOAC implementation is mainly based on our implementation of DSAC as well as
the open source code of OAC. As DSAC shows great advantage due to the distributional value
estimation, to ensure a fair comparison, we extend OAC (Ciosek et al., 2019) to its distributional
version, i.e., DOAC, by replacing the exploration process of DSAC by the behavior policy derived
by OAC. We set the hyper-parameters the same as used by OAC in Mujoco3, and our results of
DOAC on Ant-v2 and HalfCheetah-v2 are significantly better than that OAC reported.
3That is given by the open source code, where βUB is 4.66 and δ is 23.53
17
Under review as a conference paper at ICLR 2022
D.3 Implementation
Our implementation of OVD-Explorer is based on the open source code of OAC 4, also refer to the
code of DSAC 5 as well as softlearning 6. All experiments are performed on NVIDIA GeForce RTX
2080 Ti 11GB graphics card.
The training process of OVD-Explorer and DOAC are the same as in DSAC, except for the different
behavior policy used, while OVD-Explorer and DOAC enrich the experience replay with the data
using the the derived exploration policies, respectively. To ensure the fair comparison, the hyper-
parameters for training process of baselines and OVD-Explorer are the same. Besides, we have three
hyper-parameters associated with OVD-Explorer as mentioned before, including α that controls the
exploration level, β that determines the magnitude of uncertainty we use, as well as C that is the
normalization factor. The hyper-parameters in our experiments are shown in Table 4.
E More Experiment S tudy
E.1 Runtime analysis
Figure 6 shows the time consumption of al-
gorithms relative to SAC. As can be seen,
the distributional value estimation used in
DSAC, DOAC and our methods introduces ex-
tra time consumption distinctly. Nevertheless,
the relative time consumption of OVDE_G and
OVDE-Q to SAC is 1.21 and 1.17, respectively,
which means that OVD-Explorer spends about
20% more time than SAC to achieve up to
nearly 100% performance gain as shown in Fig-
ure 6(b). This demonstrates the extra time con-
sumption is well worth it. Besides, the time
consumption of OVDE_Q is close to that of
DSAC, only with a larger variance, which in-
dicates that the additional time consumption of
OVDE_Q is minimal while performing better
exploration.
SAC DSAC DOAC OVDE_G OVDE_Q
Figure 6: Runtime analysis. The data is from 1
trial of each algorithm on Noisy Ant-v2 task, and
the errorbar represents half of the standard devia-
tion.
E.2 Analysis about OVD-Explorer’ s advantage in the case of GridChaos
With the heatmap of the visiting frequency of agent during exploration, and the heatmap about the
uncertainty estimation, we can visually analyze the patterns and advantages of OVD-Explorer.
(a)	(b)	(c)	(d)	(e)
(f)
Figure 7: State visiting frequency heatmap from 1.0 × 105 to 2.5 × 105 steps of one trial for (a)
OVD-Explorer, (b) DSAC and (c) DOAC. (d) Estimated aleatoric uncertainty of OVD-Explorer; (e)
Epistemic-aleatoric ratio of OVD-Explorer; (f) Estimated uncertainty for exploration in DOAC.
4https://github.com/microsoft/oac-explore
5https://github.com/xtma/dsac
6https://github.com/rail-berkeley/softlearning
18
Under review as a conference paper at ICLR 2022
The distinctly different exploration patterns can be easily found. Figure 7(a), 7(b) and 7(c) present
the state visiting frequency of OVD-Explorer, DSAC and DOAC, respectively. We can see that
OVD-Explorer explores directly to the right half, where the environmental randomness is lower,
whereas DSAC and DOAC are both stuck in the left half with higher environmental randomness.
Furthermore, we show how OVD-Explorer could explore directly without being trapped by the ran-
domness through the estimated uncertainty. In specific, Figure 7(d) shows that the aleatoric uncer-
tainty estimated by OVD-Explorer is consistent with environment settings, where the environment
noise is higher on the left half. Figure 7(e) shows the ratio of estimated epistemic uncertainty and
aleatoric uncertainty (i.e., epistemic-aleatoric ratio) of OVD-Explore, and higher ratio means higher
epistemic uncertainty or lower aleatoric uncertainty, which is exactly the direction OVD-Explorer
explores . The ratio is larger on the right half, which means that OVD-Explorer can avoid being stuck
in the left half. Meanwhile, Figure 7(f) presents the estimated uncertainty in DOAC, which is larger
on the left half. As DOAC encourages exploring area with relatively large estimated uncertainty, it
explains why DOAC is stuck in the left half.
E.3 Evaluation on several other noise scale in GridChaos
OVD-Explorer can explore efficiently in heteroscedastic stochastic environment by considering dif-
ferently about epistemic and aleatoric uncertainty for exploration as shown in Section 5.2. To further
empirically prove its strength, we test OVD-Explorer in GridChaos with different noise scales in
four quadrants, and the result is shown as Figure 8 and Table 5. We can find that OVD-Explorer can
perform well in all those different noise injection of environment.
M » U »
Eα⅛s a6eaΛ<
IOO
ε «
i 60
g
S M
I
« 20
0
« M O
« « M
Eα⅛s a6eaΛ<
30j0*°
Eα⅛s a6eaΛV
Figure 8:	Training curves on GridChaos with noise of different scale. The x-axis indicates number
of training epoch (100 environment steps for each training epoch), while the y-axis is the evaluation
result represented by average episode return. The shaded region denotes half standard deviation of
average evaluation over 5 seeds. Curves are smoothed uniformly for visual clarity. These results are
corresponding to row A to row E in Table 5.
Table 5: The results for GridChaos. Noise setup shows the different setup for environmental het-
erogeneous Gaussian noise scale, and the corresponding four columns represent the noise settings
in the four quadrants of the Cartesian coordinate system, as shown in Figure 5, in which the goal
locates in the first quadrant, and the triangle is initialised in the second or third quadrants. Average
return shows the average episodic undiscounted return with half standard derivation in the last 100
epoch before totally 1250 epochs. FRG epoch means the minimum training epoches in the trials
used to Firstly Reach the Goal before totally 1250 epochs. The row S is the standard GridChaos as
shown in Figure 3(b), the others are shown in Figure 8
	1	Noise setup			DSAC	Average return OVDE	DOAC	DSAC	FRG epoch OVDE	DOAC
		2	3	4						
S	0.1	0.5	0.5	0.1	0.00±0.00	59.30±48.42	3.02±6.03	1250+	229	1222
A	0.0	0.5	0.1	0.1	18.94±37.87	58.99±48.18	38.42±47.08	1161	180	662
B	0.0	0.05	0.01	0.01	39.78±48.72	79.52±39.77	18.71±37.41	694	144	846
C	0.05	0.1	0.1	0.05	0.05±0.10	39.64±48.55	20.59±39.66	1250+	180	309
D	0.001	0.005	0.005	0.001	20.00±40.00	40.46±48.61	39.99±48.97	284	276	321
E	0.0	0.0	0.0	0.0	0.00±0.00	20.20±39.90	14.60±29.20	1250+	185	1118
Concretely, from the results, the following observations deserve to be noticed. First, OVD-Explorer
can significantly achieve better average return in all those settings, especially when the noise is set
high, and can learn to reach the goal faster (see column about FRG). It shows the ability of OVD-
Explorer to guide agent explore against higher aleatoric uncertainty on the left side (the second
19
Under review as a conference paper at ICLR 2022
and third quadrants). Second, for the task without noise as shown in row E, which means the
state transition is deterministic, OVD-Explorer still learns quickly. The results in row E show the
inherently high difficulty of this task, not only because of the very sparse reward, but also the gate
leading to the goal is set very small (the width of the gate is only 30% of the length of agent, i.e.,
the base of the isosceles triangle, which means that at the doorway the agent can only move a very
small distance horizontally, otherwise it would be adsorbed to the wall and immobile).
E.4 Evaluation in GridChaos when the aleatoric uncertainty is high around
GOAL
In the previous experiments in GridChaos, the noise (i.e., aleatoric uncertainty) near the goal is set
lower. In such situation, OVD-Explorer, which follows the principle of OFU and further avoids
exploring areas with higher aleatoric uncertainty, could bring significant advantage. Such setup of
heterogeneous noise is reasonable, because in real life, the goal or optimal policy is always not
expected to be highly stochastic.
Nevertheless, the evaluation about tasks with the existence of high randomness in the target region
is valuable, so we conducted the following experiment in GridChaos, where the environment ran-
domness in the right half (first and fourth quadrants), where the target is located, was set larger. The
results are shown in Table 6. Note that we use OVDE(P) to denote the usual implementation that pes-
simistically estimates the value distribution (i.e., using Equation 13). Besides, OVDE(M) denotes
the implementation that does not pessimistically estimate the value distribution (i.e., we modify the
mean of Gaussian distribution Zπ in Equation 13 from the lower bound to expected value of the Q
estimation μ(s, a) as in Equation 11.).
Table 6: The results for GridChaos (additional). This shows row F to J, which are the cases where
the optimal policy would face higher aleatoric uncertainty.
	Noise setup		DSAC	Average return		DOAC	FRG epoch			
	1&4	2&3		OVDE(P)	OVDE(M)		DSAC	(p)	(M)	DoAC
F	0.5	0.1	50.10±40.96	16.61±33.22	17.01±33.86	50.52±41.43	479	1071	583	321
G	0.1	0.05	0.00±0.00	19.84±39.68	39.96±48.95	20.14±39.93	-1	188	226	233
H	0.05	0.005	0.00±0.00	20.69±39.61	20.07±39.94	0.00±0.00	-1	247	308	-1
I	0.01	0.005	0.0±0.0	40.00±48.99	60.00±48.99	39.99±48.98	-1	200	236	301
J	0.005	0.001	20.00±40.00	39.98±40.97	20.00±40.00	20.00±40.00	236	312	200	296
Our experimental findings are mainly the following three aspects.
Firstly, when facing extremely high aleatoric uncertainty around the goal (see row F), which causes
the interaction around goal to be very unstable, chaotic and disorder, OVD-Explorer would strongly
discourage exploring such a area, and thus performance would be damaged. In contrast, DSAC and
DOAC have no restriction on aleatoric uncertainty, and high randomness may instead increase the
probability of achieving the goal.
Second, in most cases (see G, H, I, J), OVD-Explorer always can guide better exploration and
achieve better performance than DSAC and DOAC, especially when the noise is negligible (see row
J). This reflects the fact that our exploration objective (the mutual information shown in Equation 8)
makes great sense, achieving an appropriate trade-off between avoiding high aleatoric uncertainty
and being optimistic about high epistemic uncertainty.
Third, an interesting finding is that OVD-Explorer may perform better by turning off the pessimistic
estimation facing higher aleatoric uncertainty around the goal (see column OVDE(M)). This sug-
gests that excessive pessimism is unnecessary if there is a need to explore areas with high aleatoric
uncertainty.
Overall, from the results in Table 5 and Table 6, OVD-Explorer is able to tackle most of the cases
quite well. When there is high randomness around the goal, OVD-Explorer has a shortcoming that it
will inevitably slow down the efficiency of reaching the goal, because it limit the exploration towards
such area. Fortunately, this shortcoming can be mitigated by turning off the pessimistic estimation.
20
Under review as a conference paper at ICLR 2022
E.5 Evaluation of statistical sense
Table 7: Comparisons of related algorithms on Ant-v2. We report the averaged performance and
standard deviation.
Task	Epoch	SEED	DSAC	DOAC	OVD-EXPLORER-G	OVD-EXPLORER-Q
Ant-v2	2500	0,1,2,3,4	6206.9±1202.5	6586.7±1023.3	7160.6±763.2	7590.3±1 54.9
Ant-v2	2500	5, 6, 7, 8, 9	6565.0±1343.0	6664.2±255.5	7190.1±813.8	7174.3±570.0
Ant-v2	2500	0-9	6385.9±1287.2	6625.4±7446.8	7175.3±789.0	7382.3±466.6
To counteract the randomness from a statistical perspective, we conduct all experiments for 5 trails
with different seeds (typically 0-5), and report the average results with standard deviation. Next,
to verify that the 5 trails are sufficient to mitigate the statistical randomness, we run other 5 runs
(seeds are set as 5, 6, 7, 8, 9, respectively) for those algorithms on Ant-v2, and show the results in
the following. Note that the first row of results is from our previously reported results, which is the
same as Table 3, and the second row show the results new. The experimental results in Table 7 show
that the results of 5 trials are sufficiently representative of the overall level, while the performance
of OVD-Explorer undoubtedly stays ahead.
E.6 Study on episodic horizon
Section 5.3 has shown great advantage of OVD-Explorer over DSAC and DOAC in stochastic Mu-
joco tasks, which limits the length for an episode to 100 steps. To further empirically verify the
efficiency of OVD-Explorer, we test on Noisy Ant-v2 task with different maximum episodic length
setup. Our results in Figure 9 show that OVD-Explorer can significantly perform better than base-
lines in different maximum episodic length (i.e., 250, 500, 750 and 1000). Noting that longer maxi-
mum episodic length renders higher difficulty of solving tasks, especially for the high-dimensional
tasks demanding exploration. In specific, we have the following two conclusions.
500
Oooo
Oooo
4 3 2 1
u∙lnωαπb><
O 500 IOOO 1500 2000 2500
Number OfTraining Epoch
u∙lntt≈36e∙!3><
1500
1250-
1000
750-
500-
250
O 250	500 750 IOOO 1250
Number OfTraining Epoch
U 2500
I 2000-
⅛1500-
E
f IOOO
3000
O 250 500 750 IOOO 1250
Number of Training Epoch
(a) 100 steps	(b) 250 steps	(c) 500 steps
(d) 500 steps
O 250 500 750 IOOO 1250
Number of Training Epoch
Enβ≈36ea><
5000τ
4000
3000
2000
IOOO
(e) 750 steps
0	250 500 750 1000 1250
Number OfTraining Epoch
6000
5000-
4000
3000
2000-
1000
Enβ≈36ea><
(f) 1000 steps
O 250 500 750 IOOO 1250
Number OfTraining Epoch
Figure 9:	Training curves on Noisy Ant-v2 tasks with different maximum episodic length setup. The
x-axis indicates number of training epoch (the number of environment steps for each training epoch
is the same as the episodic horizon), while the y-axis is the evaluation result represented by average
episode return. The shaded region denotes half standard deviation of average evaluation over 5
seeds. Curves are smoothed uniformly for visual clarity. The sub-title of each figure represents the
episodic horizon, also known as the maximum episode length.
21
Under review as a conference paper at ICLR 2022
Firstly, the exploration should be more conservative in the harder tasks, where we should set smaller
α in OVD-Explorer. In Figure 9(a), (b) and (c), α is set to 0.05 by default, while we can find
that the advantage of OVD-ExPlorejG decreases gradually with the increasing of the difficulty of
tasks. Further, if α is set to 0.005, then there is a substantial Performance imProvement as shown in
Figure 9(d). Also, as shown in Figure 9(e), both OVD-ExPlorer methods Perform well when the task
ePisodic horizon is 750 with α set to 0.005. On the hardest task we tested, i.e., the Noisy Ant-v2
with horizon 1000 as shown in Figure 9(f), OVD-ExPlorer gain remarkable Performance, while α is
set smaller as 0.001.
Secondly, OVD-ExplorejQ is more stable than OVD-Explorer-G, which is consistent with the con-
clusion in Section 5.3. We can find from Figure 9(a), (b) and (c) that OVD-ExplorejQ performs
stably better while OVD-ExplorejG degrades. OVD-ExplorejG is better in easier task with horizon
100, which is due to the Gaussian prior of noise. But when the task becomes harder, the prior helps
less, and OVD-ExplorejQ shows the advantage of more flexibly modeling aleatoric uncertainty and
thus the performance is more stable.
E.7 S TUDY ON HYPER-PARAMETERS β
OVD-Explorer is sensitive to α, as shown in Section 5.4. There is another hyper-parameter β, which
controls the scale of uncertainty quantification as shown in Equation 11 and Equation 13, further
having an impact on ZZπ and Zπ. To evaluate its sensitivity about β, we conduct the experiment
on Noisy Ant-v2 task using OVD-Explorer_G, and the result is shown in Figure 10. The results
demonstrate that there is a broad range of settings for β, which can lead to well performance.
0	2	4	6	8
β value
Figure 10:	Sensitivity to β. The x-axis indicates different β settings, while the y-axis is the evalua-
tion result represented by average episode return in the last epoch before totally 1250 epoch. Error
bars indicate half standard deviation of average evaluation over 5 seeds. The 11 different β values
are 0.05, 0.1, 0.6, 1.2, 1.8, 2.4, 3.2, 3.6, 4.8, 6.0, 8.0.
E.8 Ablation study on value distribution estimation
As mentioned in Section 3.2, we estimate Z π pessimistically to alleviate over-estimation. Also,
as mentioned in Appendix E.4, the pessimism is unnecessary if there is a need to explore areas
with high aleatoric uncertainty. In the following, to investigate the benefit of pessimistic estimation
in general case, we compare the performance of OVD-Explorer to the modified versions that use
normally estimated Z π . Our results show that pessimistic estimation can mostly be better than that
using normal estimation.
For OVDE-G (mean), we modified the mean of Gaussian distribution Zn in Equation 13 from
the lower bound to average value of the Q estimation μ(s, a) as in Equation 11. For OVDE-Q
(mean), we modified the ziπ(s, a) in Equation 14 from the minimum value to the average value, i.e.,
Zi(s,a; θ) = Ek=1,2Zi(s,a; θk).
As shown in Figure 11, both OVDE_G (mean) and OVDE_Q (mean) perform worse than the pes-
simistic version. To draw a conclusion, pessimistic estimate is indeed required in general cases.
Only when there is a need to explore areas with high aleatoric uncertainty, is such pessimistic esti-
mation worth being turned off.
22
Under review as a conference paper at ICLR 2022
(a)	(b)
Figure 11:	Training curves on Noisy Ant-v2 with different estimation of Zπ . The x-axis indicates
number of training epoch (100 environment steps for each training epoch), while the y-axis is the
evaluation result represented by average episode return. The shaded region denotes half standard
deviation of average evaluation over 5 seeds. Curves are smoothed uniformly for visual clarity.
E.9 The performance compared with RND
RND also follows OFU principle, modeling uncertainty based on network distillation and using it as
an intrinsic motivation signal to facilitate agent exploration. For the sake of fairness, we implement
RND based on DSAC, denoted by DSAC+RND in the following, and evaluate it on 3 standard
Mujoco tasks and 3 noisy Mujoco tasks. We show the results in the following.
Table 8: Comparisons with RND. We report the averaged performance and standard deviation of 5
runs. Each trail uses the mean undiscounted return over the last 100 epoch. The maximum value of
each row is shown in bold.
Task	Epoch	DSAC	DSAC+RND	OVD-EXPLORER-G	OVD-EXPLORER-Q
Ant-v2	2500	6206.9±1202.5	7308.4±641.3	7160.6±763.2	7590.3±154.9
HalfCheetah-v2	2500	13890.0±3424.4	12198.1±2338.3	14084.5±1579.8	14792.4±997.4
Hopper-v2	1250	2199.7±602.7	2077.9±344.1	2239.5±428.2	2619.3±457.0
N-HalfCheetah-v2	1250	431.39±35.68	409.48±45.88	445.28±37.52	429.63±34.45
N-Hopper-v2	1250	244.53±4.71	231.46±9.94	252.09±7.82	237.68±13.11
N-ANT-V2 (250)	1250	1275.87±172.64	1306.05±223.18	-	1384.43±84.39
For the complex task Ant-v2, RND brings much greater improvement by facilitating exploration
based on the DSAC. For the other simpler tasks, RND does not bring significant performance im-
provement. This experiment demonstrates to some extent the effectiveness of RND exploration on
several Mujoco tasks, but at the same time, our algorithm OVD-Explorer is still better.
E.10 Analysis about exploration process of OVD-Explorer
In the following, we further verify from statistical analysis that OVD-Explorer is in compliance with
our raised exploration principle, i.e., OVD-Explorer can achieve better trade-off between opti-
mistic exploration and effectively avoiding exploring the areas with high aleatoric uncertainty.
We show the values of uncertainty estimations and our exploration objective (mutual information)
at different stages during the training processes of two trials with different noise settings in figures.
In Figure 12, the environment noise is set lower around the goal, noting that the darker background
color in the map represents higher aleatoric uncertainty, and the red dot represents the coordinate of
the current state. The performance under this trail is given and the agent hardly ever reaches the goal
before the 1000th epoch. Therefore, in the early stage, the aleatoric uncertainty is inaccurate and
remains very low, as the value distribution shows little divergence. The figure also shows that our
exploration objective (in green) is high when the epistemic uncertainty is high. So before the 1000th
epoch, the exploration is guided by epistemic uncertainty, which follows the OFU principle. Later,
once the goal has been explored, the aleatoric uncertainty is properly modelled, i.e., the aleatoric
uncertainty towards left is larger than right at current state (see epoch 1240). Then the mutual
information value towards left is lower than right, although the epistemic towards left is higher. It
23
Under review as a conference paper at ICLR 2022
indicates that OVD-Explorer can property balance epistemic uncertainty and aleatoric uncertainty,
and effectively avoid to explore the areas with higher aleatoric uncertainty.
In Figure 13, the environment noise is set higher around the goal. At the stage before the 1000th
epoch, the exploration guidance is similar to Figure 12. The agent would hardly estimate the accurate
aleatoric uncertainty without obtaining any reward. In the later stage, at the 1240th epoch, OVD-
Explorer suggests exploring to the right, even though it has been recognized that the environmental
uncertainty on the right is high. This is because the epistemic unertainty dominates under the mutual
information. In contrast, at the 1249th epoch, when the action towards right has been explored much,
the significant higher aleatoric uncertainty towards right dominates. Therefore, following the mutual
information, the action towards left is preferred. This demonstrates the trade off that OVD-Explorer
make, which satisfies our raised exploration principle.
24
Under review as a conference paper at ICLR 2022
Training epoch = 1000 Training epoch = 1240 Training epoch = 1249
1.0
0.5
0.0
τypθ
Epistemic uncertainty
Aleatoric uncertainty
Mutual information
Type
Epistemic uncertainty
Aleatoric uncertainty
Mutual information
IUnl①U ΦCT2Φ><
Figure 12: The statistical analysis for the training process, With the aleatoric uncertainty around the
goal is set loWer.
τypθ
Epistemic uncertainty
Aleatoric uncertainty
Mutual information
τypθ
Epistemic uncertainty
Aleatoric uncertainty
Mutual information
Training epoch = 1000 Training epoch = 1240 Training epoch = 1249
1111 ∣ιiιιllll IJ∣IIIJi ≡
up down left right up down left right up down left right
Oooo
6 4 2
Enl①H ΦCT2Φ><
O
Number of Training Epoch
Figure 13: The statistical analysis for the training process, With the aleatoric uncertainty around the
goal is set higher.
25