Under review as a conference paper at ICLR 2022
On the One-sided Convergence of Adam-type
Algorithms in Non-convex Non-concave Min-
max Optimization
Anonymous authors
Paper under double-blind review
Ab stract
Adam-type methods, the extension of adaptive gradient methods, have shown great
performance in the training of both supervised and unsupervised machine learning
models. In particular, Adam-type optimizers have been widely used empirically as
the default tool for training generative adversarial networks (GANs). On the theory
side, however, despite the existence of theoretical results showing the efficiency
of Adam-type methods in minimization problems, the reason of their wonderful
performance still remains absent in GAN’s training. In existing works, the fast
convergence has long been considered as one of the most important reasons and
multiple works have been proposed to give a theoretical guarantee of the con-
vergence to a critical point of min-max optimization algorithms under certain
assumptions. In this paper, we firstly argue empirically that in GAN’s training,
Adam does not converge to a critical point even upon successful training: Only
the generator is converging while the discriminator’s gradient norm remains high
throughout the training. We name this one-sided convergence. Then we bridge
the gap between experiments and theory by showing that Adam-type algorithms
provably converge to a one-sided first order stationary points in min-max opti-
mization problems under the one-sided MVI condition. We also empirically verify
that such one-sided MVI condition is satisfied for standard GANs after trained
over standard data sets. To the best of our knowledge, this is the very first result
which provides an empirical observation and a strict theoretical guarantee on the
one-sided convergence of Adam-type algorithms in min-max optimization.
1 Introduction
As one of the most popular optimizers in supervised deep learning tasks like natural language
processing (Chowdhury, 2003) as well as the main workhorse of generative adversarial network
training (Goodfellow et al., 2014), Adam-type methods are widely used because of their minimal
need for learning rate tuning and their coordinate-wise adaptivity on local geometry. Starting from
AdaGrad (Duchi et al., 2011), adaptive gradient methods have evolved into a variety of different
Adam-type algorithms, such as Adam (Kingma & Ba, 2015), RMSprop, AMSGrad (Reddi et al.,
2018) and AdaDelta (Zeiler, 2012). In supervised learning, adaptive gradient methods and Adam-
type algorithms play important roles. Especially in the field of NLP (natural language processing),
Adam-type algorithms are the goto optimizer. Multiple NLP experiments show that sparse Adam
outperforms other non-adaptive algorithms like Stochastic Gradient Descent (SGD) not only on the
solution performance, but also on both the training and testing error’s convergence rates. It’s worth
mentioned that the most popular pre-training language model BERT (Devlin et al., 2018) also uses
Adam as its optimizer, which shows the power of Adam-type algorithms.
Also, Adam-type algorithms are very effective in min-max optimization. As a direct and widely used
application of min-max optimization, generative adversarial networks (GANs) are notorious for the
training difficulty. Training by SGD will easily diverge to nowhere or converge to a limiting cycle,
both of which will lead to an ill-performing solution, while Adam optimizer, as the default optimizer
for GANs (Hsieh et al., 2020), can obtain better performance. The reason why these two optimizers
have so much difference in GAN’s training has long been an open problem. Traditionally, the training
performance of min-max optimization is measured according to its first-order convergence, which
means the norm of the gradient, but is it really true in GAN’s training?
After training GAN on two relatively simple datasets, MNIST and Fashion-MNIST, we can find
that, in a practical training process of GAN, Adam optimizer does not perfectly converge since the
1
Under review as a conference paper at ICLR 2022
norm of discriminator’s gradient remains quite high through out the training process. Instead, it
only has a one-sided convergence as the norm of generator’s gradient actually converges to 0. This
(a) MNIST
(b) Fashion-MNIST
π□□□
ESlI 已
E□D□
於二
(c) MNIST (d) Fashion-MNIST
Figure 1: We train GAN on the dataset MNIST and Fashion-MNIST. The first two figures above show
us the Frobenius norm of the gradients of discriminator and generator. After 50k iterations, we obtain
(c),(d) by using Adam. Despite its one-sided convergence, the min-max training actually succeeds.
paper thus aims to explain this phenomenon by bridging the gap between theory and practice. On
one hand, we understand under which conditions Adam-type optimization algorithms have provable
convergence for min-max optimization. Towards this end, a recent work (Liu et al., 2020) designs
two algorithms, Optimistic Stochastic Gradient (OSG) and Optimistic AdaGrad (OAdaGrad) for
solving a class of non-convex non-concave min-max problems and gives theoretical guarantee on
their convergence. (Liu et al., 2020) also proposes an open problem on the convergence proof of
Adam-type algorithms, which is solved by this paper. On the other hand, we find that the MVI
condition needed for our convergence proof does not practically hold for GANs. Instead, we propose
the much milder one-sided MVI condition, which tends to hold practically and under which we
provide the theoretical guarantee of the one-sided convergence of Adam-type algorithms.
Despite some theoretical guarantee made on the convergence of Adam-type algorithms on convex
concave or non-convex concave min-max optimization, in the non-convex non-concave setting which
is most general, there is no theoretical guarantee on convergence. Comparatively speaking, proving
the convergence of Adam-type algorithms is much more difficult since they use an empirical version
of Momentum. Although it has been shown to perform well in practice, it is actually difficult to
analyze theoretically. Even in the standard convex setting, proving the convergence of Adam-type
algorithms (Reddi et al., 2018; Zou et al., 2021) is much harder than other adaptive algorithms such
as AdaGrad (Duchi et al., 2011). Actually, the original version of Adam is known not to converge in
convex settings. Therefore, to formally analyze the convergence of Adam-type algorithms in min-max
optimization, we also consider a “theoretically correct” version of Adam, which is an analog of
AMSGrad (Reddi et al., 2018).
In this paper, there are three main contributions. (1) We analyze Extra Gradient AMSGrad, which is
an Adam-type algorithm used for solving non-convex non-concave min-max optimization problems
as well as GAN’s training. We prove that, under the assumption of standard MVI condition, the Extra
Gradient AMSGrad algorithm provably converges to a ε-stationary point with O(dε-2) complexity
in deterministic setting and O(dε-4) complexity in stochastic setting. (2) Although the standard MVI
condition above is a much milder assumption than convexity, we empirically show that MVI condition
does not hold for GAN’s objective functions in reality. Instead, the one-sided MVI condition proposed
by us tends to hold, which is the mildest assumption ever used in all the convergence proofs for
min-max optimization. Under the the one-sided MVI condition, we modify the algorithm above
by using dual rate decay, and theoretically prove its convergence rate. (3) We conduct empirical
experiments on GAN’s training by the Extra Gradient AMSGrad algorithm and the Extra Gradient
AMSGrad with dual rate decay analyzed by us. We show that they have much better performance
than the Stochastic Gradient Descent Ascent (SGDA) algorithm. Also, we empirically verify that our
new one-sided MVI condition is indeed satisfied during GAN’s training while the previously
proposed standard MVI condition is not, which makes the one-sided MVI condition much closer
to reality than the standard version.
After achieving all these results, we are eventually able to understand the one-sided convergence of
Adam-type algorithms in min-max optimization as well as in GAN’s training.
2
Under review as a conference paper at ICLR 2022
2 Background and Related Works
In this section, we will introduce the background knowledge as well as related works on the following
three fields: adaptive gradient methods, min-max optimization, and the convergence properties of
multiple algorithms for min-max optimization problems.
2.1 Adaptive Gradient Methods and Adam-type Methods
We consider the simplest 1-dimensional unconstrained minimization problem:
min f(x).
x∈D⊆R
where f : D → R is a continuously differentiable function. As one of the most dominant algorithms
on the optimization problem above, Stochastic Gradient Descent (SGD) was originally proposed
by (Goodfellow et al., 2016), which has been both empirically and theoretically proved effective,
especially when facing large datasets and complicated models. To further improve the performance
of SGD, several adaptive variants of SGD have been proposed, such as RMSprop, Adam (Kingma &
Ba, 2015), AdaGrad (Duchi et al., 2011), AMSGrad (Reddi et al., 2018) and AdaDelta (Zeiler, 2012).
Distinguished from the vanilla gradient descent or its stochastic version SGD, adaptive gradient
methods use a coordinate-wise scaling of the updating direction and each iteration relies on the history
information of past gradients. In AdaGrad, we use arithmetic average when adopting history gradient
information of each iteration while in Adam, RMSprop etc., we use exponential moving average
instead because its believed that the more current gradient information is more important. Although
adaptive gradient methods and momentum based methods are two different routes on optimization,
they are combined perfectly in Adam. Now we introduce the family of adaptive gradient methods
and Adam-type, and all of them have the following form:
mt+1 = htVf (Xt) + rt ∙ mt, vt+ι = PtEf(Xt))2 + qt ∙ Vt
Xt+1 = Xt — λt ∙
mt+1
√vt+ι + ε
[Adaptive]
Here, f is the objective function to minimize. h, r, p, q are scalars depending on t, λt is the learning
rate of the t-th iteration and ε > 0 is a small constant used to protect the denominator from being close
to 0. From the formula above, we see that the momentum mt is the weighted sum of the past gradients
and vt is the weighted sum of the past squared gradients. When h = 1, r = 0, mt+1 = Vf(Xt) is
just the current gradient. We start with the original Adam.
vt+1 = αtvt + (1 - αt)(Vf (Xt))2, mt+1 = βtmt + (1 - βt)Vf(Xt)
xt+1 = χt - λ ∙
mt+1
√vt+1 + ε
[Adam]
As we can see, Adam is a combination of adaptive gradient method and momentum method. Here,
the momentum term is empirical, meaning that it does not coincide with acceleration techniques
that are theoretically sound, which creates extra difficult for the analysis. In Adam, we have
ht + rt = pt + qt = 1. When the αt = α, βt = β remains constant, there is a bias correction step
where vt+ι J l-a and mt+ι J ^^. However, We may practically ignore this bias correction
step since ɪ-ot and ɪ-1^^ rapidly approach to 1. As one of the variants of Adam, AMSGrad has the
following formulation:
Vt+1 = αtvt + (1 — at)(Vf (xt))2, Vt+ι = max(vt, Vt+ι)
mt+1 = βtvt + (1 - βt)Vf (Xt),
Xt+1 = Xt — λ ∙
mt+1
√Vt+1 + ε
[AMSGrad]
As we can see, their difference is that the velocity term vt keeps increasing in AMSGrad.
After showing the details of these traditional adaptive gradient methods and Adam-type methods, we
introduce their convergence properties as well as their further variants. Reddi et al. (2018) shows
that Adam does not converge in some settings where large gradient information is rarely encountered
and it will die out quickly because of the “short memory” property of the exponential moving
average. However, under some conditions, the convergence proofs of adaptive gradient methods
have been obtained. Basu et al. (2018) proved the convergence rate of RMSprop and Adam when
using deterministic gradients instead of stochastic gradients. Li & Orabona (2018) analyzed the
3
Under review as a conference paper at ICLR 2022
convergence rate of AdaGrad under both convex and non-convex settings. All the papers above
provide theoretical guarantee for the convergence of different types of adaptive gradient descent. After
that, Chen et al. (2019) extends Adam to a broader class of Adam-type algorithms and provides its
convergence analysis for non-convex optimization problems. In order to combine the fast convergence
of adaptive methods and better generalization with momentum based methods, a number of new
algorithms are proposed, such as SC-AdaGrad / SC-RMSprop (Mukkamala & Hein, 2017), AdamW
(Loshchilov & Hutter, 2019), AdaBound (Luo et al., 2019) etc..
2.2	Min-max Optimization
In the min-max optimization problem (or saddle point problem), we have to solve:
xm∈iXn my∈aYx φ(x, y),	[SP]
where X ⊆ Rn1 , Y ⊆ Rn2 , and φ : X × Y → R is the objective function. When φ is convex on x
and concave on y, we call it a convex-concave min-max optimization. Otherwise, it’s a more general
non-convex non-concave min-max optimization. For the brevity, we denote z = (x, y) and Z =
X × Y ⊆ Rn1+n2. We also introduce our gradient vector field: V(Z) = (-Vχφ(x, y), Nyφ(x,y)),
which are the update directions on both sides. The goal of [SP] is to find a tuple z* = (χ*,y*) such
that φ(x*,y) 6 φ(x*,y*) 6 φ(x, y*) holds for ∀x ∈ X ,y ∈ Y, which is called the solution of [SP].
If the inequality above only holds in the local neighbourhood of z*, then z* can only be called a local
solution. Notice that the necessary condition of being a solution (or even a local solution) is to be a
stationary point of φ, which means V(z*) = 0. Furthermore, if V is C1, any local solution of [SP]
must be stable, which means V2xxφ(x*, y*) 0 and V2yyφ(x*, y*) 0. Next, we will introduce
several commonly-used algorithms which are designed to solve [SP].
Stochastic Gradient Descent Ascent (SGDA) This is a simple extension of Stochastic Gradient
Descent (SGD) algorithm for minimization problems (Johnsen, 1959). In the t-th iteration:
zt+1 = Zt + Yt ∙ V(Zt; ωt),	[SGDA]
where ωι, ω2,… are the independent and identically distributed sequence of noises. V(z, ω) can be
treated as a query to the stochastic first-order oracle (SFO). In each iteration of SGDA, we need to
query SFO once. Notice that, we simultaneously update x, y in each iteration of SGDA. Therefore, if
we alternate the updates of x and y, we obtain a variant of SGDA, which is named as the alternating
stochastic gradient descent ascent (AltSGDA) algorithm. Different from original SGDA, we have to
make two queries to SFO in each iteration. One for Zt = (xt, yt), and the other for the intermediate
step (xt+1, yt). Since original SGDA is not going to work even in the convex-concave setting (such as
minx maxy f(x, y) = xy), so researchers propose the following “theoretically correct modification”.
Stochastic Extra-gradient (SEG) This is a different algorithm with the above SGDA, and it is
originally proposed for solving the convex-concave setting of min-max optimization problems by
Korpelevich (1976). Given Zt as a base, we take a virtual gradient descent ascent step and obtain a Zt,
which can be treated as the shadow of Zt. Then we use the gradient at Zt0 as the update direction of Zt.
This process can be described as:
z0 = zt + Yt ∙V(zt； ω(1))
zt+1 = zt + Yt ∙ V(zt ； ω(2)).
[SEG]
In each iteration, we need to make two queries to the SFO. One for the base zt and the other for the
shadow zt0. However, in the first step of [SEG], we can use the gradient at the previous shadow zt0-1
so that we only have to make only one query in each iteration and remember the query’s result of the
previous step. This algorithm is called Optimistic Gradient or Popov’s Extra-gradient (Popov, 1980)
which can be described as:
zt = zt + Yt ∙ V(z0-ι; ωt-ι)
zt+1 = zt + Yt ∙ V(zt； ωt).
[OG]
As a widely used algorithm, it has been applied in multiple works (Daskalakis et al., 2018; Mer-
tikopoulos et al., 2019). Under some mild assumptions, convergence rates are proved by many
theoretical works and we will summarize them in the next section.
4
Under review as a conference paper at ICLR 2022
2.3	Convergence Rates of Multiple Min-max Algorithms
In this section, we summarize the convergence rates of different algorithms as well as the assumptions
needed. For convex-concave optimization, Nesterov (2007) provided the O(1/T) convergence
guarantee of Mirror-Prox in terms of duality gap. Juditsky et al. (2011) introduced its stochastic
version where only the stochastic first order oracle can be accessed. After combining with (Darzentas,
1983), convergence rates for both deterministic and stochastic mirror-prox algorithms are shown to
be optimal. When it comes to the more challenging non-convex non-concave min-max optimization,
Dang & Lan (2015) showed that the deterministic extragradient method can converge to ε-first order
stationary point with non-asymptotic guarantee. Another interesting algorithm Inexact Proximal Point
(IPP) method (Lin et al., 2018), which is a stage-wise algorithm, performs well when the objective
function is weakly-convex weakly-concave. In each stage, we construct a strongly-convex strongly-
concave sub-problem by adding quadratic regularizers. Then, by using stochastic algorithms, we
can approximately solve the original problem. It’s known that IPP also has a first order convergence
guarantee. Also, Sanjabi et al. (2018) proposed an alternating deterministic optimization algorithm,
where multiple steps of gradient ascents are conducted before one gradient descent step. Therefore,
we can approximately make sure that the max step always reaches near optimal. However, in
order to guarantee its convergence to first order stationary point, we have to assume that the inner
maximization problem satisfies PL condition (Polyak, 1969). For the details of convergence rate, we
summarize them into Table 1. Finally, MVI condition needs to be explained. Let K : Rd → Rd be
	Assumption	IC	Guarantee
-OMD (DaskaIakis et al., 2018)- (deterministic)	bilinear	N/A	asymptotic
OG (LiU et al., 2020) (stochastic)	MVI has solution	O(ε-4)	ε-SP
OAdaGrad (Liu et al., 2020) (stochastic)	MVI has solution BCG COnditiOn	Oe ((d∕ε2)ι-1α)	ε-SP
SEG (IUSem etal., 2017) (stochastic)	pseudo-monotonicity	O(ε-4)	ε-SP
Extra-gradient (Azizian et al., 2019) (deterministic)	strong-monotonicity	O(log(1∕ε))	ε-optim
AItSGDA(GideI et al., 2019) (deterministic)	bilinear	O(log(1∕ε))	ε-optim
IPP (Lin etal.,2018) (stochastic)	MVI has solution	O(ε-6)	ε-SP
Extra Gradient AMSGrad (ours) (deterministic & stochastic)	MVI has solution	O(dε-2) & O(dε-4)	ε-SP
Extra Gradient AMSGrad with Dual Rate Decay (ours) (deterministic & stochastic)	one-sided MVI has solution	Oe(dε-2) & Oe(dε-4)	ε-SP
Table 1: Summary of different algorithms for min-max optimization. IC stands for iteration complex-
ity, ε-SP stands for ε-first order stationary point, and ε-optim stands for ε-close to the set of optimal
solutions. The last two lines are algorithms analyzed by us in this paper. BCG condition stands for
the bounded cumulative gradient assumption.
an operator and X ⊆ Rd is a closed convex domain. Hartman & Stampacchia (1966) proposed the
StamPacchia Variational Inequality (SVI), which aims to find z* ∈ X, such that(K(z*), z - z*i > 0
holds for all z ∈ X. Similarly, Minty (1962) proposed the Minty Variational Inequality (MVI)
Problem, which aims to find z* ∈ X, such that hK(z), z - z*i > 0. In Table 1, the oPerator
K(z) = (Vχφ(x,y), -Vyφ(x,y))> = -V(Z) with z = (x,y).
3 Main Results
In this section, we introduce the main results of this PaPer. We focus on two algorithms: Extra
Gradient AMSGrad (AMSGrad-EG) and Extra Gradient AMSGrad with Dual Rate Decay (AMSGrad-
EG-DRD) which inherit the idea of OAdaGrad into Adam-tyPe algorithms. With AMSGrad-EG,
we can Prove its first-order convergence under MVI condition. However, as we stated above, Adam
does not Perfectly converge in GAN’s training since MVI condition does not always hold for GAN’s
objective functions. We bridge the gaP by ProPosing one-sided MVI condition which is shown to be
5
Under review as a conference paper at ICLR 2022
more likely to hold. Under this condition, we prove that Extra Gradient AMSGrad with Dual Rate
Decay (AMSGrad-EG-DRD) converges one-sidedly, which matches our experiment results.
3.1	Problem Setting and Assumptions
Throughout the paper, we analyze the min-max optimization problems:
min max φ(x, y),	[SP]
where X ⊆ Rn1 , Y ⊆ Rn2, and φ : X × Y → R is the objective function. We denote z = (x, y) and
Z = X × Y. First, we state some useful assumptions on φ(x, y):
Assumption 1.
(1)	V := (-Vχφ, Vyφ) is L-Lipschitz continuous under ∣∣ ∙ ∣∣2 norm.
(2)	The stochastic first order gradient oracle (SFO) is unbiased and has bounded variance:
E[V(z; ξ)] = V(z) and E∣V(z; ξ) - V(z)∣2 6 σ2.
(3)	The Stochastic first-order Gradient Oracle (SFO) has bounded output: there exists G > 0 and
δ > 0 such that ∣V(z; ξ)∣2 6 G and ∣V(z; ξ)∣∞ 6 δ almost surely holds.
(4)	There exists a universal constant D > 0, such that ∣zk ∣2 6 D holds for all points zk on our
trajectory and ∣∣z*∣2 6 D. Ifthefeasible set Z is bounded, then this assumption naturally holds.
Assumption 2 (Standard MVI condition). The MVI of -V(z) has a solution, which means there
exists a z*, such that:
h—V (z),z — z*i > 0 holds for ∀z ∈ Z
3.2	Extra Gradient AMSGrad (AMSGrad-EG)
In this section, we analyze the Extra Gradient AMSGrad (AMSGrad-EG) algorithm, which is used
for non-convex non-concave min-max optimization, and we theoretically provide its convergence
rate. So far, the convergence rate of Adam-type algorithms for min-max optimization has long been
an open problem, and this work is the very first to obtain a related result. AMSGrad-EG algorithm is
described as Algorithm 1.
Algorithm 1 Extra Gradient AMSGrad
Input: The initial state zo = mo = vo = 0, a constant learning rate η, momentum parameters
β1t, β2, a Stochastic First-order Oracle (SFO) V(z; ξ), a sequence of batch sizes {Mk}.
Output: zt where t is uniformly chosen from {0, 1, . . . , N — 1}.
1:	for k = 1, . . . , N do
2:	(Gradient Evaluation 1) gk-i =e PM=I V(zk-i； ξk-ι).
3:	(Momentum Update 1) mk = βikmk-i + (1 — βik)gk-i.
4:	(Velocity Update 1) Vk = max(β2Vk-1 + (1 一 β2)g2-1,Vk-1), Hk = δI + Diag(√vk).
5:	(Shadow Update) Zk = zk-i + η ∙ H-Imk.
6:	(Gradient Evaluation 2) gk = Mk PM=I V(Zk; ξk).
7:	(Momentum Update 2) mk = βikmk + (1 — βik )gk.
8:	(Velocity Update 2) Gk = max(β2Vk + (1 —e2雇,Vk), Hk = δI + Diag(√vk)
i
9:	(Real Update) Zk = Zk-i + η ∙ Hk mk.
10:	end for
Compared to the original Adam, we just add an extra-gradient technique and a taking-max process in
velocity updates. It’s worth mentioned that if we delete the maximizing operation in velocity update
steps, then this algorithm degenerates to Extra-Gradient Adam, since the largest difference between
Adam and AMSGrad is that the latter one guarantees that the velocity term is non-decreasing.
Theorem 3.1 (Main Theorem 1). For the AMSGrad-EG algorithm, given the objective function
φ(x, y) : Rn1+n2 → R and V (Z) = (-Vχφ, Vy φ) that satisfy Assumption 1 and Assumption 2, as
well as the initial point Zo ∈ Z, the iteration number N, a sequence of batch sizes {Mk} and a
constant learning rate η 6 袅,then the output ofthe algorithm satisfies the following inequality:
6
Under review as a conference paper at ICLR 2022
EkV(z)k2 6 N
^ 6dD2(δ + G)2
η2
12dG2(δ + G)2 -
+	δ2
150σ2(δ + G) XX 1
+ Nδ	t=ι Mt
48GD(δ + G)
Nη
N
X	β1t +
t=1
108η2G2(δ + G)
Nδ
N
X	β12t .
t=1
+
Here, we analyze the conclusion above on two sides: parameter choosing on β1k and on Mk .
Discussion Here, we give some discussions on Theorem 3.1 and compare it with existing results.
(1)	There are two practical ways to choose the parameter sequence {βιt}: (1) βιt = βι ∙ λt-1
where β1, λ ∈ (0, 1) and (2) β1t = 1/t. In both settings, PtN=1 β12t = O(1) and PtN=1 β1t = Oe(1).
Therefore, we can conclude from Theorem 3.1 that: EkV (z)k2 6 O(d∕N) + O(1∕N) ∙ PN=I 1/Mt
holds after regarding D, G, δ, η as constants.
(2)	When the batch sizes Mk are constant, let Mk = Θ(1∕ε2). To guarantee EkV(z)k2 6 ε2, the
total number of iterations should be N = O(dε-2) and the total complexity is PkN=0 Mk = O(dε-4).
When the batch sizes Mk are increasing, let Mk = k + 1. To guarantee EkV(z)k22 6 ε2, the total
number of iterations should be N = Oe(dε-2 ) and the total complexity is PkN=0 Mk = Oe(d2ε-4).
Obviously, using constant batch sizes obtains a better total complexity.
(3)	In the deterministic setting, the first-order oracle directly outputs the accurate gradient V(z; ξ) =
V(z), which means σ = 0. Theorem 3.1 leads to EkV(z)k22 6 O(d/N). To guarantee EkV(z)k22 6
ε2, the total number of iterations should be N = O(dε-2).
(4)	In the AMSGrad-EG algorithm, the momentum term is a technical difficulty on the convergence
proof. Proofs in the past works always use the MVI condition or convex condition like hV(zk), zk -
z*〉6 0 ⇒ hgk,zk - z*i / 0 to control the gradient norms. However, if we replace gk with the
momentum term mk , the inequality above will no longer hold, and then we have to find another
way to control the upper bound of gradient norms. It’s also worth mentioned that our proof can’t be
extended to Optimistic Adam (OAdam) since we need to guarantee that H1 H2 . . . in our proof.
Actually, Adam may not even converge in convex case (Reddi et al., 2018).
(5)	Comparison with OAdaGrad: (Liu et al., 2020) proposes the Optimistic AdaGrad (OAdaGrad)
algorithm and gives a convergence analysis on under Assumption 1, 2 and Bounded Cumulative
Gradient Assumption (which assumes the existence of a constant 0 6 δ 6 1/2 such that the
cumulative gradients are bounded as ∣∣^±k,i∣∣2 6 δka for all k). Under these assumptions, they
conclude that:	1 N
N XEkV(Zk)kH-iι 6 OQlN 1-α).
k=1	-1
On one hand, notice that 焉 PN=I EkV(Zk )∣∣H-ι is the average of the norms of V(Zk). However,
the norm keeps changing. Since Hk--11 keeps dekc-r1easing and may limit to 0 as k → ∞, its unclear
what is the real convergence rate in terms of the size of the gradient. It would be more convincing if
we can upper bound the average of constant norms like N PN=I EkV(Zk)k2. On the other hand, the
Bounded Cumulative Assumption though widely used in related papers (Zhou et al., 2018; Reddi
et al., 2018; Duchi et al., 2011), is actually a very strong assumption: Under this assumption, it holds
that kgi:k i∣∣2 6 δkα, which naturally leads to:
1N	1N	d
N XEkV(Zk)k2 6 N XEkgkk2 6 O(NI-a),
which causes circularity on the argument. In this paper, we overcome these two shortcomings.
Standard MVI and one-sided MVI conditions From Table 1, we can see that many related
convergence proofs rely on assuming the MVI condition of -V(Z), which means hV (Z), Z - Z*i 6
0 ∀Z ∈ Z . Although MVI condition is theoretically known to be true in many standard supervised
deep learning settings (Li & Yuan, 2017; Kleinberg et al., 2018; Allen-Zhu & Li, 2020; Allen-
Zhu & Li, 2020; Li et al., 2018; 2020; Allen-Zhu & Li, 2020; 2019). However, this is a rather
unrealistic assumption for GANs: In some practical scenarios such as DCGAN (Radford et al.,
2015), it is unclear whether the training objective can satisfy the MVI condition: While the generator
might have a consistent gradient direction towards the optimal generator (which is the one that
generates the target distribution), it is very unlikely that there is a “optimal discriminator” where the
discriminator’s gradient is pointing to through the course of the training. Indeed, different generator
7
Under review as a conference paper at ICLR 2022
should in principle requires different discriminator to discriminate it from the target distribution,
which precludes the MVI condition to hold on y .
Also, in practical scenarios like GAN, we only care about the min-variable x (which refers to the
generator of GAN), and the optimally of y is not needed. Therefore, in the following part, we
propose a weaker version of MVI condition, which is the one-sided MVI condition. Recall that
Z = (x,y),z* = (x*,y*) where X ∈ X,y ∈ Y,Z = X × Y, and V(Z) = (-Vχφ(z), Nyφ(z)):=
(Vx(z),Vy(z)). Then, the one-sided MVI condition implies that hVx(z),x - x*〉6 0 ∀z ∈ Z,
which means for any y ∈ Y, the x-part of function V, -V (∙,y) satisfies the MVI condition. Now we
empirically verify that one-sided MVI is more likely to hold in practice in some simple applications
of GANs. ForZ,Z* ∈ Z:
h-V(Z),Z-Z*i = h-Vx(Z),x-x*i+h-Vy(Z),y-y*i,
where Z = (x, y), Z* = (x* , y*). We call the three terms above as total MVI, x-sided MVI and
y-sided MVI respectively. Assumption 2 requires total MVI to be non-negative, and Assumption 3
requires x-sided MVI to be non-negative. After training Wasserstein GAN on the MNIST/Fashion
MNIST dataset with AMSGrad-EG optimizer, we denote Zk := (xk, yk) as the value of Z at the k-th
iteration, and Z* := (x*, y*) as the value of Z at the last iteration. In the following Figure 2, we plot
the total MVI values h-V(Zk), Zk - Z*i, x-sided MVI values h-Vx(Zk), xk - x*i, and y-sided MVI
values h-Vy (Zk), yk - y*i along the training trajectory. We can see that x-sided MVI stays positive
while the total MVI does not, which means the one-sided MVI condition proposed in Assumption 3 is
more realistic than the original MVI condition in Assumption 2. Under the one-sided MVI condition,
(a) MNIST	(b) Fashion MNIST
Figure 2: This figure shows the MVI values along the training trajectory. As we can see, the blue
curve stays above x axis in both experiments while the red curve does not. Since we use non-linear
activations in the network architecture, this result is exciting. It’s safe to say that the one-sided MVI
condition proposed by us fits the reality since the x-sided MVI keeps positive.
we prove in our next theorem that, the conclusion of Theorem 3.1 still holds once we slightly modify
AMSGrad-EG to Extra Gradient AMSGrad with Dual Rate Decay (AMSGrad-EG-DRD). To the best
of our knowledge, this is a convergence guarantee of an adaptive min-max algorithm with the weakest
assumption ever needed. In the next section, we introduce the AMSGrad-EG-DRD algorithm and its
convergence property.
3.3 Extra Gradient AM S Grad with Dual Rate Decay
Now, we write down the one-sided MVI condition introduced above in Assumption 3, which is the
weakest assumption ever needed to obtain a convergence guarantee in min-max optimization.
Assumption 3 (One-sided MVI condition). The one-sided MVI of -V(Z) has a solution, which
means there exists a Z* = (x* , y*) ∈ Z, such that:
h-Vx(Z), x - x*i > 0 holds for ∀Z = (x, y) ∈ Z.
After slightly modifying AMSGrad-EG with a θ(l/ʌ/k) dual rate decay, we get Extra Gradient
AMSGrad with dual rate decay (AMSGrad-EG-DRD). Its pseudo-algorithm is placed in the appendix.
We propose its convergence property as follows:
Theorem 3.2 (Main Theorem 2). For the AMSGrad with Extra-Gradient and Dual Rate Decay
(AMSGrad-EG-DRD) algorithm, given the objective function φ(x, y) : Rn1+n2 → R and V(Z) =
(-Nxφ, Nyφ) := (Vx(Z), Vy(Z)) that satisfy Assumption 1 and Assumption 3, as well as the initial
point Z0 ∈ Z, the iteration number N, a sequence of batch sizes {Mk} and a constant learning rate
η 6 3δL, then the output of the algorithm satisfies the following inequality:
8
Under review as a conference paper at ICLR 2022
n□□□
[11∏□
□□□π
□DUE
E□Π≡
□□E□
□□ll□
□EJ[JE
HIirgIiii
EE□D
UHMl
QDiieJ
[]ŋm
I ιmc
L1EJ□□
K3EE]∏
ΠEI∣I0
匚E□
ΠIIΠE2
ΠΠoLl
n≡]∣ι
cαiE
【nr i j
ElEE
EJEJ□□
1〕□【】〔】
ΠUI1□
□□Γ in
linilŋ
∏!□∏∏
□Γ1ΠΠ
E□Π□
ll□∏□
□□□□
□□□□
□□□□
I if Ji j∏
QQOQ
F JL,]0Π
□ππ□
sbħ□
BDBB
ΞΞDH
QQHO
□□□□
ππππ
□□□□
□□□□
BBBQ
□□□□
ΞE1ΞΞ
□ΞΞE]
(a) A-EG
(b) A-EG-DRD
(c) SGDA
(d) A-EG
(e) A-EG-DRD
□□□□
□□□□
□□□□
□□OΞ
ΞQQ□
EIEI@0
BΞΞQ
(f) SGDA

Figure 3: Generated MNIST and Fashion-MNIST figures by the three algorithms after 10k, 20k, 50k
iterations. A-EG and A-EG-DRD stand for AMSGrad-EG and AMSGrad-EG-DRD.
EkVx(z)k22 6
(15 + 3log N )dG2(δ + G)2
Nδ2
6dD2(δ + G)2	150σ2(δ + G) XX 1
+ Nn + Nδ LM
ι 48GD(δ + G)
+ Nη
N
X	β1t +
t=1
108η2G2(δ + G)
Nδ
N
X	β12t.
t=1
Similar to Theorem 3.1, in the deterministic setting where σ = 0, the total complexity is O(dε-2).
In the stochastic setting, We have: EkVx(Z)k2 6 O(d∕N) + O(1∕N) ∙ PN=I 1/Mt. When We use
constant batch sizes Mk = Θ(1∕ε2), iteration number N should be O(dε-2) in order to guarantee
that EkVx(z)k22 6 ε2. So the total complexity should be Oe(dε-4).
4	Experimental Results
In this section, We use experiments to verify the effectiveness of AMSGrad-EG and AMSGrad-EG-
DRD algorithms by applying Wasserstein GAN (Arjovsky et al., 2017) on the MNIST (LeCun et al.,
1998) and Fashion-MNIST (Xiao et al., 2017) datasets in our experiments. More experiments Will be
shoWn in the appendix. The architectures of discriminator and generator are set to be MLP. The layer
Widths of generator MLP are 100, 128, 784 and the layer Widths of discriminator MLP are 784, 128, 1.
We set batch sizes as 64, learning rate as 1e-4 and We compare AMSGrad-EG, AMSGrad-EG-DRD
and SGDA by printing their generated figures after 10k, 20k, 50k iterations in the folloWing Figure
3. We use the TensorfloW frameWork (Abadi et al., 2016) to complete our experiments. As a result,
unlike the non-adaptive SGDA algorithm, the tWo algorithms proposed by us perform better than the
non-adaptive SGDA and their generated figures are realistic, Which shoWs their effectiveness.
5	Discussion and Future Works
This Work fills up the blank in the theory of non-convex non-concave min-max optimization as Well as
GAN’s training. We bridge the gap betWeen theory and practice and provide the theoretical guarantee
of the one-sided convergence of Adam under one-sided MVI condition, Which perfectly matches the
empirical observation. To the best of our knoWledge, it is the very first proof for the convergence of
Adam-type algorithms in non-convex non-concave min-max optimization. Future folloW-up Works
can go further on the folloWing tWo directions: (a) Figure out Which part of Adam-type algorithms
play an important role on the outstanding performance: automatic tuning of learning rate or local
geometry adaptivity. (b) With both the discriminator and generator of GANs to be overparameterized
2-layer ReLU netWorks, it Would be an influential Work to figure out the convergence property, the
converging limit and the training trajectory of min-max optimization under multiple optimizers like
Adam and SGDA so that We can get some intuition on their differences.
9
Under review as a conference paper at ICLR 2022
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-scale
machine learning. In 12th USENIX symposium on operating systems design and implementation
(OSD116),pp. 265-283, 2016.
Zeyuan Allen-Zhu and Yuanzhi Li. What can resnet learn efficiently, going beyond kernels? arXiv
preprint arXiv:1905.10337, 2019.
Zeyuan Allen-Zhu and Yuanzhi Li. Backward feature correction: How deep learning performs deep
learning. arXiv preprint arXiv:2001.04413, 2020.
Zeyuan Allen-Zhu and Yuanzhi Li. Feature purification: How adversarial training performs robust
deep learning. arXiv preprint arXiv:2005.10190, 2020.
Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and
self-distillation in deep learning. arXiv preprint arXiv:2012.09816, 2020.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In International conference on machine learning, pp. 214-223. PMLR, 2017.
Walss Azizian, Ioannis Mitliagkas, Simon Lacoste-Julien, and Gauthier GideL A tight and unified
analysis of extragradient for a whole spectrum of differentiable games. arXiv: Learning, 2019.
Amitabh Basu, Soham De, Anirbit Mukherjee, and Enayat Ullah. Convergence guarantees for
rmsprop and adam in non-convex optimization and their comparison to nesterov acceleration on
autoencoders. 2018.
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of adam-type
algorithms for non-convex optimization. In ICLR 2019 : 7th International Conference on Learning
Representations, 2019.
Gobinda G Chowdhury. Natural language processing. Annual review of information science and
technology, 37(1):51-89, 2003.
Cong D. Dang and Guanghui Lan. On the convergence properties of non-euclidean extragradi-
ent methods for variational inequalities with generalized monotone operators. Computational
Optimization and Applications, 60(2):277-310, 2015.
John Darzentas. Problem complexity and method efficiency in optimization. 1983.
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with
optimism. In ICLR 2018 : International Conference on Learning Representations 2018, 2018.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina N. Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, 2018.
John C Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12:2121-2159, 2011.
Gauthier Gidel, Reyhane Askari Hemmat, Mohammad Pezeshki, Gabriel Huang, Remi Le Priol,
Simon Lacoste-Julien, and Ioannis Mitliagkas. Negative momentum for improved game dynamics.
In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 1802-1811,
2019.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural
Information Processing Systems 27, pp. 2672-2680, 2014.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. 2016.
10
Under review as a conference paper at ICLR 2022
Philip Hartman and Guido Stampacchia. On some non-linear elliptic differential-functional equations.
ActaMathematica ,115(1):271-310,1966.
Ya-Ping Hsieh, Panayotis Mertikopoulos, and Volkan Cevher. The limits of min-max optimization
algorithms: convergence to spurious non-critical sets. arXiv preprint arXiv:2006.09065, 2020.
Alfredo N. Iusem, Alejandro Jofre, Roberto Imbuzeiro Oliveira, and Philip Thompson. Extragra-
dient method with variance reduction for stochastic variational inequalities. Siam Journal on
Optimization, 27(2):686-724, 2017.
Erik Johnsen. Arrow, hurwicz and uzawa: Studies in linear and non-linear programming, stanford
univeristy press 1958. 229 s., 7,50. Ledelse and Erhvervs0konomi, 23, 1959.
Anatoli Juditsky, Arkadii S. Nemirovski, and Claire Tauvel. Solving variational inequalities with
stochastic mirror-prox algorithm. Stochastic Systems, 1(1):17-58, 2011.
Diederik P. Kingma and Jimmy Lei Ba. Adam: A method for stochastic optimization. In ICLR 2015 :
International Conference on Learning Representations 2015, 2015.
Robert Kleinberg, Yuanzhi Li, and Yang Yuan. An alternative view: When does sgd escape local
minima? arXiv preprint arXiv:1802.06175, 2018.
G. M. Korpelevich. The extragradient method for finding saddle points and other problems. Matecon,
12:747-756, 1976.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive
stepsizes. In The 22nd International Conference on Artificial Intelligence and Statistics, pp.
983-992, 2018.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. In
Advances in Neural Information Processing Systems, pp. 597-607. http://arxiv.org/abs/1705.09886,
2017.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized
matrix sensing and neural networks with quadratic activations. In COLT, 2018.
Yuanzhi Li, Tengyu Ma, and Hongyang R Zhang. Learning over-parametrized two-layer neural
networks beyond ntk. In Conference on Learning Theory, pp. 2613-2682, 2020.
Qihang Lin, Mingrui Liu, Hassan Rafique, and Tianbao Yang. Solving weakly-convex-weakly-
concave saddle-point problems as weakly-monotone variational inequality. 2018.
Mingrui Liu, Youssef Mroueh, Jerret Ross, Wei Zhang, Xiaodong Cui, Payel Das, and Tianbao Yang.
Towards better understanding of adaptive gradient algorithms in generative adversarial nets. In
ICLR 2020 : Eighth International Conference on Learning Representations, 2020.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR 2019 : 7th
International Conference on Learning Representations, 2019.
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate. In ICLR 2019 : 7th International Conference on Learning Representations,
2019.
Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chandrasekhar,
and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going the extra
(gradient) mile. In ICLR 2019 : 7th International Conference on Learning Representations, pp.
1-23, 2019.
George J. Minty. Monotone (nonlinear) operators in hilbert space. Duke Mathematical Journal, 29
(3):341-346, 1962.
11
Under review as a conference paper at ICLR 2022
Mahesh Chandra Mukkamala and Matthias Hein. Variants of rmsprop and adagrad with logarithmic
regret bounds. In ICML’17 Proceedings of the 34th International Conference on Machine Learning
-Volume 70,pp. 2545-2553, 2017.
Yurii Nesterov. Dual extrapolation and its applications to solving variational inequalities and related
problems. Mathematical Programming, 109(2):319-344, 2007.
B.T. Polyak. Minimization of unsmooth functionals. Ussr Computational Mathematics and Mathe-
matical Physics, 9(3):14-29, 1969.
L. D. Popov. A modification of the arrow-hurwicz method for search of saddle points. Mathematical
Notes, 28(5):845-848, 1980.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In ICLR
2018 : International Conference on Learning Representations 2018, 2018.
Maziar Sanjabi, Meisam Razaviyayn, and Jason D. Lee. Solving non-convex non-concave min-max
games under Polyak-^ojasieWicz condition. arXivpreprint arXiv:1812.02878, 2018.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
MattheW D. Zeiler. Adadelta: An adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu. On the convergence of adaptive
gradient methods for nonconvex optimization. arXiv preprint arXiv:1808.05671, 2018.
Difan Zou, Yuan Cao, Yuanzhi Li, and Quanquan Gu. Understanding the generalization of adam in
learning neural netWorks With proper regularization. arXiv preprint arXiv:2108.11371, 2021.
12
Under review as a conference paper at ICLR 2022
A Proof for the Convergence of AMSGrad-EG
We recall that in the t-th iteration of Extra-Gradient AMSGrad, our update is as follows:
mt = βιtm^t-1 + (1 — βιt)gt-i, Vt = max(β2Vt-i + (1 — β2)g2-i,Vt-i)
Ht = δI + Diag(√Vt), Zt = zt—i + η ∙ H-1 mt
mt = βιtmt + (1 — βιt)gt, Vt = max(β2vt + (1 — β2)^2,vt)
Ht = δi + Diag(Pvt), zt = zt-i + η ∙ H-Imt.
(1)
Now we begin to prove Theorem 3.1 (Main Theorem 1). Before that, we prove that as the weighted
sum of stochastic gradient, the momentum terms mt, mt are also contained in l2 ball with radius G.
Lemma A.1. There exist upper boundsfor both velocity terms vt, Vt and momentum terms mt, mt:
(1) For ∀t ∈ N, almost surely, the momentum terms Ilmtk2 6 G, ∣∣mt∣∣2 6 G∙
(2) For ∀t ∈ N, almost surely, the velocity terms |vt,i| 6 G2, |Vt,i| 6 G2 hold for ∀i ∈ [d].
Proof of Lemma A.1. Actually, this lemma can be simply proved by using the method of induction.
Since we’ve assumed that kV (z)k2 6 G almost surely holds for z ∈ Z = Rd, so kgtk2 6 G and
k^tk2 6 G almost surely holds. Therefore, by knowing that mo = m0 = 0 ⇒ 0 = ∣∣m0∣∣2 =
∣∣mok 6 G and once ∣∣mk-1∣∣2 6 G, ∣r^k-1∣∣2 6 G, we have:
Ilmk ∣∣2 = kβ1km k-1 + (1 — β1k )gk-1∣∣2 6 Blk ∣∣m k-1∣∣2 + (1 — Blk )∣∣gk-1∣∣2 6 (1 — Blk )G + BikG = G
km k ∣∣2 = ∣∣β1k mk + (1 — Blk )gk ∣∣2 6 Blk ∣∣mk∣∣2 + (1 — Blk )∣∣0k ∣∣2 6 (1 — Blk )G + Blk G = G.
Similarly, the upper bound for velocity terms can also be easily proved.
□
Lemma A.2.
kzt — z*kHt 6 ∣∣zt-ι — z*kHt — kzt-ι — ZtkH古 + 1岛一ZtkH古 + 2hη ∙ εt, Zt- z*i + 8nBitGD.
Here,乱=gt — V(Zt) = M PMtI V(Zt； Q- V(Zt).
Proof of Lemma A.2. According to the update rules:
llZt — z*∣H t = kZt-i + η ∙H-Im t— Z*kHt
=llZt-i + η ∙H-Im t— Z*kHt — llZt-i + η ∙H-Im t— ZtkH t
=iiZt-i— z*∣H t— kZt-i— ZtkHt + 2hη ∙m t, Zt— Z*i
=llZt-i — z* kHt — llZt-i — Zt + Zt — Zt kHt +2hη ∙m t,Zt — Zti +2hη ∙m t,Zt — Z*i
=iiZt-i— z*∣H t— iiZt-i- ZtkHt— k^t— ZtkHt— 2hHt(Zt-i— Zt), Zt— Zt
+ 2hη ∙ m t, Zt — Zt + 2hη ∙ m t, Zt — z*i
=llzt-i	— z*kH t	—	lzt-i — ZtkHt —	kzt	— ZtkHt	+ 2hη	∙m t,	zt — z*i
.,	ʌ	,	.	ʌ	1	.
+ 2hzt — zt, Ht(zt-i — Zt + η ∙Ht mt)i
=llzt-i— z*kH t— lzt-i— ZtkHt— kzt— ZtkHt + 2hη ∙m t, zt— z*i
ʌ,...
+ 2hzt — zt, Ht(zt —々)〉
=kzt-i — z*kHt — kzt-i — ZtkHt + kzt — ZtkHt + 2hη ∙ mt, zt — z*i
Notice that gt = V(^t) + εt. Since z* is a solution of MVI which meanshV(z), z — z*i 6 0 holds
for ∀z ∈ Z, so(V(^t), ^t — z*i 6 0. Therefore:
hη ∙ mt, Zt — z*i = (η ∙ (BItmt + (1 — BIt)gt), Zt — z*i 6 (η ∙ gt, Zt — z*i + (η ∙ Bit(mt — gt), Zt — z*i
6 hη ∙(V(zt) + εt),zt — z*i + nBit ∙ kmt — gt∣∣2 ∙∣∣B — z*l∣2
(a)
6 hηεt,zt — zi + 4nBitGD.
13
Under review as a conference paper at ICLR 2022
Here, (a) holds because Ilmt — ^t∣∣2 6 IlmtIl2 + IlgtIl2 6 2G, |岛一z*∣∣2 6 2D and by using MVI
property, hV(Zt), ^t — z∖ 6 0.
Combine it with the inequality above, we obtain that:
Ilzt- z1良 6 l∣zt-ι — z* 1良 一 IIzt-I- ZtkH古 + 1岛 一 ZtkH； + 2hη ∙ εt, Zt- z*i + 8ηβιtGD.
which comes to our conclusion.	口
In the lemma above, hη ∙ εt, Zt — z*∖ has zero mean. So it can be ignoring when taking expectation.
Next, We upper bound the ∣∣^t - ZtkH term.
Lemma A.3.
I^t - ZtIHt 6 2η2 ∙ I(H-1 - H-1)mtIHt + 16η2G2β2t + 12ηL- ∙IZt - zt-iIHt
+ 12η2 ∙ (ι∣εt∣∣H -1 + ι∣εt-1∣∣H-1).
Here, εt-ι = gt-1 - V(Zt-i) and εt = gt - V(Zt).
1
ProofofLemma A.3. According to the update rules (1), we know that Zt - Zt = η ∙ (Ht Imt -
H-1 mt). Therefore, we upper bound the term ∣∣Zt - ZtkH as follows:
I^t - ZtIHt = η2 ∙ kH-1mt - H-1mtIHt = η2 ∙ kH-1(mt - mt) + (H-1- H-1)mtkHt
6 2η2 ∙ (kmt - mtkH-1 + k(H-1 - H-1)mtkHt)
= 2η2 ∙k(H-1- H-1)mt∣∣Ht + 2η2 ∙ kβ1t(mt- m t-1) + (1- β1t)(0t- gt-1)kH -ι
6 2η2 ∙k(H-1- H-1)mt∣∣H t+4η2 (β2tkmt- m t-1kH -ι + (1- β1t)2kgt- gt-1kH -ι)
It 2η2 ∙ k(H-1 - H-1)mtkHt + 16η2β2tG2 +4η2k^t - gt-1kH-1
=) 2η2 ∙ k(H-1 - H-1)mtkHt + 16η2G2β2t +4η2kv(^t)- V(Zt-I) + εt - εt-1 kH-1
(6)	2η2 ∙ k(H-1 - H-1)mtkHt + 16η2G2β2t +12η2 ∙kV(Zt) - V(Zt-1)kH-1
+ 12η2 ∙ (ι∣εt∣∣H -1 + ι∣εt-1kH -1)
(f) 9 2 11∕^-1	口一1、	∣∣2	16η2G2β2t , 12η2L2 II ʌ	∣∣2
6 2η ∙	k(Ht	-	Ht	)mt∣∣Ht	+ δ I	δ2-	∙	k2t	-	Zt-1kHt
+ 12η2 ∙ (ι∣εt∣∣H -1 + ι∣εt-1kH-1)
Here, (a) holds because mt = β1tmt-1 + (1 - β1t)gt-1,mt = β1tmt + (1 - β1t)gt, and then:
mt - mt = β1t(mt - mt-1) + (1 - β1t)(gt - gt-1).
(b) holds because ka + bk2C 6 (kakC + kbkC)2 6 2(kak2C + kbk2C) and (e) holds because of the
similar reason: kx + y + Zk2C 6 (kxkC + kykC + kZkC)2 6 3(kxk2C + kyk2C + kZk2C). (c) holds
because (1 - β1t)2 < 1 and
Il	ʌ ∣∣2	Jlmt- mt-1k2	4G2
kmt - mt-1kH-1 6 --------δ------- 6 丁.
(d) holds because gt = V(^t) + εt and gt-1 = V(Zt-1) + εt-1∙ (f) holds because of the following
fact:
ʌ ʌ ʌ
δI = Ho W Ho W H1 W H1 W ... W HN W ...,
or equivalently:
∣I 占 H-1 占 H-1 占 H-1 占 H-1 占...占 Hn1 占...，
14
Under review as a conference paper at ICLR 2022
which leads to ∣∣εt-ιkH -i 6 ∣∣εt-ιkH-「Also, V(∙) is L-LiPschitz continuous and δI W HHt, so:
L2
IlV(zt) - V(Zt-I)IlH - 6 ^j2 kzt - zt-ι∣Ht.
□
Since our learning rate η 6 言,we have 12n2^ 6 1. Now, we can combine Lemma A.2 and Lemma
A.3:
Ilzt- z*∣Ht 6 Ilzt-I- z*∣Ht - kzt-1 -切良 +2hη ∙ εt, zt - z*i + 8ηβ1tGD
+ 2η2 ∙ k(HH-1 - H-1)mtkHt + 16η2G2β2t + 12ηδ2L2 ∙k^t - zt-1kHt
+ 12η2 ∙ (kεt∣∣H — + ι∣εt-1kH-1),
Since '12炉 6 11, therefore:
Ilzt -	z*kHt 6	llzt-1	- z*kHt	- 2llzt-1	- B∣∣Ht	+ 2hη	∙	εt,	zt	-	z*i	+ 8ηβ1tGD +	" δ	’1t
+ 2η1 ∙ ιι(HH-1- H—Dm/iH t + 12η1 ∙(怕恃-i + 归-1|匕-1).
After taking exPectation and summation over t = 1, 2, . . . , N, we obtain that:
2 XXEkzt-I- ztIHt 6 XE hIzt-1 - z*IHt -Izt - z*IHti + X ∖8ββuG3D + 16η2G2β2t
t=1	t=1	t=1
NN
+ 2η1 ∙ XEk(H-1 -町1)口嗫 + 12η1 ∙ XE 卜阎恨-1 + Iεt-1 IH-i].
t=1	t=1
(2)
Since ∣∣zt-1 -ztIH =kη∙Ht mt∣∣H=andV(zt-ι) = gt-1 -εt-1 = mt-β1t(mt-1 -gt-1)-εt-1,
we know that:
IlV (zt—1)∣∣H-ι = Ilmt- β1t(m t-1- gt-1)- εt-1ι∣H-ι
Ht	Ht
63 (iimtiiH-i + ι∣β1t(m t-1- gt-1 )IH-i + ι∣εt-1 ∣∣H -)
=3 (ι∣β1t(m t-1- gt-1 )∣ι H-1 + ι∣εt-1ι∣H-1) + 3 ∙ ∣∣H-1mt IlHt	⑶
12G2 β12t	2	3	2
6 -δ+ 3即-1k-1 + 庐 ∣∣zt-1-切必
After combining Equation (2) and Equation (3), it holds that:
GlI?IlLv	、l|2	6 G 雨 ∣^∣∣	*∣∣2	∣∣	*∣∣2 1 , Gl"48β1tGD	108η1 G1β1t
ZEkV(zt-1)∣∣H-ι 6 声ZE [llzt-1 - z l∣Ht - Ilzt - z l∣H」+ X [-η- +-δ-
NN
+ 12XEk(H-I- H-1)mtIHt +75XE 帽恃-1 + Iεt-1IH』∙
t=1	t=1
(4)
In the following stePs, we will uPPer bound the four terms above on the right side one by one, and we
start from the first term.
Lemma A.4.
N
X [kzt-1 - z*kHt -kzt - z* kHt] 6 dD2 ∙ (δ + G),
t=1
which is a constant.
15
Under review as a conference paper at ICLR 2022
Proof of Lemma A.4. Notice that
N	N-1
X [kzt-1-z*kHt - kzt- z*kHJ6 kz0- z*kH0 + X (kzt-z*kHt+1 - kzt- z*kHt)
N-1
=kzo-z*kHo + X [(zt- z*)>(HHt+ι — HHt) γzt-zF
t=1
N-1
6 D2 ∙ tr(Ho) + X D2 ∙ (tr(Ht+ι) - tr(Ht)) = D2 ∙ tr(HN) 6 dD2 ∙ (δ + G),
t=1
which comes to our conclusion.	□
For the second term, it’s easy to see that:
N
X
t=1
48βιtGD 108η2G2β2J
~Γ~ +
48GD
N
X β1t +
t=1
108η2G2
N
Xβ12t,
t=1
(5)
η
δ
Next, we analyze the third term.
Lemma A.5.
N 1	1	2	dG2(δ + G)
Ek(Ht - H-1 )mtkHt 6 —⅛~~-.
t=1
Proof of Lemma A.5. For any δ 6 x < y, we notice that:
(y - x)2	y - x y - x
x2y	< x2	6 δ2
(6)
Therefore, we have:
NN
X k(H-1 - H-1)mtkHt 6 Xm>(H-I- H-1)Ht(H-1 — H-1)mt
t=1	t=1
N
6 X G2 ∙ tr ((H-1 - H-1)Ht(H-I- H-1
t=1
(a) N G2
6 X滔
t=1
- ,ʌ . .-
• [tr(Ht) - tr(Ht)]
G2	G2	dG2(δ + G)
6 "δ2^ • tr(HN) < δ2Γ ∙ d(δ + G) = -j2--.
Here, (a) holds because of Equation (6).
□
Finally, we come to the noise term PN=I E [|昌|愕-ι + |昌—1∣∣H-J, which is closely related to our
batch sizes Mt . Obviously, we have:	t	t
Ekεt∣∣H - 6 δ ∙ Ekεtk2 6 δ ∙ M.
Similarly,
Ekεt-1 kH-1 6 1 • M
Therefore, we can upper bound the expectation of the noise term as:
N
XE [kεtkH- + kεt-1kH-ι]
t=1
2σ2 N 1
6 丁 ⅛ M
(7)
16
Under review as a conference paper at ICLR 2022
Finally, after we combine Equation (4) with Equation (5), Equation (7) and Lemma A.4, Lemma A.5,
we obtain that:
N
XEkV (zt-1)k2H-1
Ht
t=1
6dD2(δ + G)	12dG2(δ + G)	150σ2 XX 1
6 —η2	+	δ2	+ ^^⅛Mt
+
48GD
η
N
X β1t +
t=1
108η2G2
-δ-
N
X	β12t .
t=1
Since for∀t, kV(z⅛-ι)kH-ι > δ+1G∣∣V(zt-ι)k2, therefore:
N
X E∣V(zt-1)∣2 6
t=1
6dD2(δ + G)2	12dG2(δ + G)2	150σ2(δ + G) XX 1
n +	δ2	+ δ ZM
48GD(δ + G)
η
N
Xβ1t+
t=1
108η2G2(δ + G)
δ
β12t,
t=1
(8)
N
which comes to our conclusion.
B Proof for the Convergence of AMS Grad-EG-DRD
In the t-th iteration of AMSGrad-EG-DRD, our update is as follows:
mt = βιtm^t-1 + (1 — βιt)gt-i, Vt = max(β2Vt-i + (1 — βz)g2-i,Vt-i)
Ht = δI + Diag(√Vt), Zt = (Xt, yt)
(xt-1 + η ∙ (Ht)-1mX, yt-1 + √ ∙ (H『mt)
mt = βιtmt + (1 — βιt)gt, Vt = max(β2Vt + (1 — β2)^2,Vt)
(9)
Ht = δI + Diag(VZvt), Zt = (xt,yt)
(χt-ι + n∙(HX)Tmx yt-ι + √ ∙ (Hty)-1mt).
Most parts of this convergence proof are similar to the convergence proof of AMSGrad-EG. Lemma
A.1 still holds.
Lemma B.1.
Ilxt- χ* kHX 6 Ilxt-I- x*kHX -kxt-ι— XtIlHX + kxt— XtIIHX + 2hη ∙ εχ,xt— x*i+ 8nβιtGD.
Here，εχ = gt - Vx(Zt) = M PM=I 匕(^t； ξi) - Vx(Zt)∙
We can use the same technique of Lemma A.2 to prove it. Next, we obtain the next lemma.
Lemma B.2.
kXt - XtkH x 6 2η2 ∙k((HX)T-(Hx)-1)mX ∣H χ +	疏 + ɪ" ∙∣^t - Zt-1∣2H t
+ 12η2 ∙ (kεtkjHX)-1 + kεt-ik2Hχ)-1).
Here, εt-ι = gt-1 - V(Zt-I) and εt = gt - V(^t)∙
We can prove it by using the same technique as Lemma A.3. Since We have 12即 6 2. Now, We
can combine Lemma B.1 and Lemma B.2:
kxt - x*kHx 6 ∣∣χt-ι - x*kHx - kxt-ι - XtkHx + 2hη ∙εχ,xt -x*i+ 8nBιtGD
+ 2η2 ∙k((HX)T-(Hx)T)mtkHx + 16n2Gβt + 2|岛-Zt-IkHt
+ 12η2 ∙ (|第&x)-ι + kεx-ik2Hx)-1),
17
Under review as a conference paper at ICLR 2022
After taking expectation and summation over t = 1, 2, . . . , N, we obtain that:
NN	N
2 XEkxt-I- XtkHX 6 XE Uxt-I-x*kHχ -kχt-χ*kHx] + X
t=1	t=1	t=1
8ηβ1tGD + T递]
δ
NN
+ 2η2 ∙ XEk((HX)T-(Hx)T)mXkHχ +12η2 ∙ XE [kεχ&x)-i + |层-1临『)-ι]
t=1	t=1
1N
+ 2 EEkyt-I - ytkH y.
t=1
SinCe kxt-i - xtkHX = ∣∣η ∙ (Hx)TmXkHX and Vx(Zt-I) = gX-1 - εX-i
gtx-1) - εtx-1, we know that:
(10)
mχ 一 βιt(m X-1 一
IlVx(Zt-I) k2Hχ)-1 = kmx - eit(mx-1 - gx-1)- εx-i∣∣2Hχ)-1
6 3 kmtx k(2HtX)-1+ kβιt(mx-1- gx-ι)k2Hχ)-ι + kεx-ιk2Hχ)-J
=3 (kβιt(mx-i - gt-i)klHχ)-ι + kεx-ik2Hχ)-1)+3」(理尸馆；^X
6 —δt~1^+3kεx-ιk2Hx)τ + η2 kxt-i- xtkH X
(11)
After Combining Equation (10) and Equation (11), it holds that:
V-^κ∣∣v 2	∖∣∣2	6 * * * VλF l^ll	*l∣2 II	*∣∣2 1 , Gi"48β1tGD	10助2@2团£
TEkVx(ZtT)kern-16 ηTE [kxt-1-x kHX- kxt-x kHx∖ +	—η—ι	δ—
t=1	t=1	t=1
NN
+ 12XEk((Hx)T-(Hx)T)mxkHx +75XE 卜用&x)-i + 1层-11混尸]
t=1	t=1
1-2 X Ekyt-I- ytkHy .	(12)
η t=1	t
In the following steps, we will upper bound the five terms above on the right side. ACtually, the
first four terms Can be upper bounded by using the same teChnique in the ConvergenCe proof of
AMSGrad-EG algorithm above.
Lemma B.3.
N
X [kxt-1 - x*kHX - kxt - x* kHx] 6 dD2 ∙ (δ + G),
t=1	t	t
which is a constant.
Lemma B.4.
X k((Hx)-i - (Hx)-1)mtkHX 6 "U+ G).
t=1
Finally, the last term can be perfectly bounded by the O(1/ʌ/t) decayed learning rate.
N	N2
X Ekyt-I- ytkH y 6 X E ηj k(Hy)-1my kH y
6 XEη2 ∙ G2 = η2G2(1 + logN).	(13)
tδ δ
t=1
To sum up, we eventually get the equation that:
EkVx(Z)k22 6
(15 + 3logN)dG2(δ + G)2	6dD2(δ + G)2	150σ2(δ + G) X 1
Nδ2	+	Nη2	+ Nδ	Mχ M
18
Under review as a conference paper at ICLR 2022
+
48GD(δ + G)
Nn
N
X β1t +
t=1
108n2 G2 (δ + G)
Nδ
N
Xβ12t,
t=1
which comes to our conclusion.
C More Experimental Results
In this section, we further use experiments to verify the effectiveness of AMSGrad-EG and AMSGrad-
EG-DRD algorithms proposed by us. Also, we show that the one-sided MVI condition is more
feasible than standard MVI condition even in a more complicated setting. Here, we use DCGAN
(Radford et al., 2015) on CIFAR10 dataset. We set our batch size as 100, learning rate as 1e-4 and we
compare AMSGrad-EG, AMSGrad-EG-DRD and SGDA by drawing their generated figures after 50,
100, 200 iterations in the following Figure 2. The two algorithms proposed by us again perform better
than the non-adaptive SGDA. After training DCGAN on the CIFAR10 dataset with AMSGrad-EG
optimizer, we plot the total MVI values h-V(Zk),zk - z*〉，X-SidedMVI values h-VX(zk),xk - x*)，
and y-sided MVI values (-V,(zM, — — &*) along the training trajectory as the following Figure 4.
iterations
Figure 4: This figure shows the MVI values along the DCGAN’s training trajectory. As we can see，
the blue curve stays above x axis in the experiment while the red curve does not.
19
Under review as a conference paper at ICLR 2022
(a) AMSGrad-EG
(b) AMSGrad-EG-DRD
(c) SGDA
Figure 5: These are the generated CIFAR10 figures by the three algorithms after 50, 100, 200
iterations.
D Stochastic Extra-Gradient and Adaptive Extra-Gradient
Algorithms
In a previous paper (Liu et al., 2020), the authors propose the Optimistic Gradient (OG) method and
Optimistic AdaGrad (OAdagrad) method. In this section, we extend them to the Extra-Gradient
type algorithms: Stochastic Extra-Gradient (SEG) and Adaptive Extra-Gradient (AEG). We put the
result in the appendix because it is just a by-product of this research and not our main result.
Before we introduce our newly-proposed Adaptive Extra-Gradient method, we slightly modify
the Stochastic Extra-Gradient (SEG) algorithm, by using different batch sizes in each iteration, as
Algorithm 2 below.
Algorithm 2 Stochastic Extra-Gradient (SEG) with batch size
Input: The initial state Z0 = z0 = 0, a constant learning rate η, a Stochastic First-order Oracle
(SFO) V (z; ξ), a sequence of batch sizes {mk}k>1.
Output: zt, t is uniformly chosen from {0, 1, 2, . . . , N - 1}.
1:	for k = 1, . . . , N do
2:	(Gradient Evaluation) gk-1 = m1k PmfeI V(ZkT; ξk-1).
3:	(Shadow Update) Zk = ∏z [zk-ι + η ∙ gk-ι]. ʌ
4:	(Gradient Evaluation) gk-i = m1k Pm=I V(Zk ； ξk- 1)
5:	(Real Update) Zk = ∏z[zk-1 + η ∙ gk-1 ]
6:	end for
Algorithm 3 is a basic idea on the design of AEG algorithm. Here, we use constant batch size m
and let gk-1 = m1 pm=1 V(Zk-1, ξk-1), gk-1 =a pm=1 V(Zk,ξk—1) be the estimated gradients
on Zk-1 and Zk in the k-th iteration. Also, g1:k is the concatenation of g1,... ,gk, and g1∙.k,i is its i-th
20
Under review as a conference paper at ICLR 2022
row vector. Similarly, g0:k is the concatenation of g0 , . . . , gk, and g0:k,i is its i-th row vector. Note
that all the matrices Hk, Sk are diagonal so they don’t require extra computation complexity.
Algorithm 3 Adaptive Extra-Gradient (AEG)
Input: The initial state Zo = zo = 0, Ho = H0 = δI, a constant learning rate η, a Stochastic
First-order Oracle (SFO) V (z; ξ), a constant batch size m.
Output: zt, t is uniformly chosen from {0, 1, 2, . . . , N - 1}.
1:	for k = 1, . . . , N do
2:	(Gradient Evaluation) gk-1 = m1 Pm=I V(Zk-1； ξk-1).
3:	(Gradient Concatenation and Norm Calculation) Update go:k = [go:k-2 gk-1], sk-1,i =
k (gθ:k-i,i gi:k-i,i) I∣2 i = 1, 2,...,d and Hk-i = δI + diag(sk-J.
4:	(Shadow Update) Zk = Zk—i + η ∙ H--Igk-i
5:	(Gradient Evaluation) gk-i = m1 Pm=I V(Zk； ξi—i).
6:	(Gradient Concatenation and Norm Calculation) Update gi：k = [gi：k-i gk], sk-i,i =
k (g0:k—1,i gl:k,i) Il2 i = 1, 2,...,d and Sk-I = δI + diag(sk-1).
7:	(Real Update) Zk = Zk-i + η ∙ S--1ιgk-ι
8:	end for
Now, we introduce the following two theorems (Theorem D.1 and Theorem D.2), which compare the
convergence rates of SEG and its adaptive variant AEG.
Theorem D.1 (Convergence of SEG). There is an algorithm (Stochastic Extra-Gradient), which
given:
• A Stochastic First-order Oracle (SFO) access to the objective function φ(x, y) : X × Y → R
where X ⊆ Rn1 , Y ⊆ Rn2, which is denoted as: V(Z； ξ) and it satisfies the following
conditions:
E[V(z； ξ)] = V(Z) =(-Vχφ, Vyφ) and EkV(z; ξ) - V(z)∣2 6 σ2.
Here, Z := (x, y) and Z := X × Y. Also: the corresponding MVI function of -V has a
solution z* ∈ Z, which means h-V (z), z 一 z*i > 0 holds for ∀z ∈ Z.
• A positive real L such that V is L-Lipschitz continuous with respect to ∣∣ ∙ ∣∣2.
• The learning rate η.
• An initial point Zo ∈ Z.
• The iteration number N.
• The sequence of batch sizes in each iteration {mk}k>i.
Then, we output a result Z ∈ Z which satisfies:
E[rη2 (Z)] 6
4∣zo-z* *k2
N
50η2
+ ɪ
where rα(z) ：= IlZ — ∏z (z + α ∙ V (z))∣2 and η 6 4L .If Z = Rn1+n2, then the projection operator
Π is the identity, andrα(z) ：= α ∙ ∣∣V(z)∣2. The inequality above becomes:
EIV(Z)I22 6
4∣zo-z*k2
η2N
Compared with Optimistic Stochastic Gradient (OSG) method proposed in (Liu et al., 2020), we can
have a larger learning rate in this theorem.
Remark. Let η =克.Ifthe batch SizeS are constant, which means mk = m for ∀k. In order
to guarantee that EkV(z)∣2 6 ε2, we have to make m = O(1∕ε2) and N = O(L2∕ε2), and the
total complexity is PN=I mk = mN = O(L2∕ε4). In another scenario where mk = k +1 is an
increasing sequence, in order to guarantee that EkV(z)∣2 6 ε2, we have to make N = O(L2∕ε2)
and then the total complexity is PkN=i mk = PN=i(k + 1) = O(L4∕ε4).
21
Under review as a conference paper at ICLR 2022
Theorem D.2 (Convergence of AEG). When our objective function φ(x, y) satisfies Assumption 1
and 2, as well as the bounded cumulative gradient condition, there exists an algorithm (AEG), given:
•	A Stochastic First-order Oracle (SFO) access to the objective function φ(x, y) : Rn1+n2 →
R, which is denoted as: V (z; ξ) and it satisfies the following conditions:
E[V(z； ξ)] = V(Z)= (-Vχφ, Vyφ) and EllV(z; ξ) - V(z)∣∣* 1 2 3 4 6 σ2.
Here, z := (x, y) and Z := Rn1+n2 = Rd where d := n1 + n2. Also: the corresponding
MVIfunction of V has a solution z* ∈ Z.
•	Positive real numbers G, such that lV(z; ξ)l2 6 G almost surely holds.
•	A positive real L such that V is L-Lipschitz continuous with respect to ∣∣ ∙ ∣∣2.
•	A universal constant D > 0 such that lzk - z* l2 6 D and for all the points on the
trajectory of our algorithm.
•	An initial point z0 ∈ Z.
•	The iteration number N.
•	A constant batch size m.
•	A constant 0 6 α 6 1/2 such that: the cumulative gradients are bounded as:
k(gi:k,i g1.k,i)k2 6 2δ ∙ ka forall i,k.
•	The constant learning rate η 6 4δL.
The output results z0, z1, . . . , zN-1 satisfies the following inequality:
1 N-1	2	16dD2 δ∕η2 + 200dδ + 40dG2∕δ	50δσ2∕m + 2G2 d∕δ
NE 工 kV(Zk)kH-16	N-O	+	N	.
k=0
If we switch the assumption of MVI condition (Assumption 2) to the one-sided MVI condition
(Assumption 3), we can slightly modify the Adaptive Extra-Gradient (AEG) method to Adaptive
Extra-Gradient with Dual Rate Decay (AEG-DRD), where the learning rate of {yk} variables set
as k. In the Algorithm 4 below, Hk = Diag(Ak, Bk) and Sk = Diag(Ck, Dk) where for each
k ∈ N, Ak, Ck ∈ Rn1×n1 and Bk,Dk ∈ Rn2×n2. Also, gk = (gX,gyy) and gk = (gk,g^y) where
gχ,^χ ∈ Rn1 andgy,gy ∈ Rn2.
Algorithm 4 Adaptive Extra-Gradient with Dual Rate Decay (AEG-DRD)
Input: The initial state Z° = z0 = 0, H0 = H0 = δI, a constant learning rate η, a Stochastic
First-order Oracle (SFO) V(z; ξ), a constant batch size m.
Output: zt where t is uniformly chosen from {0, 1, 2, . . . ,N- 1}.
1: for k = 1, . . . , N do
2:	(Gradient Evaluation) gk-1 = m1 Pm=I V(Zk-1； ξk-l).
3:	(Gradient Concatenation and Norm Calculation) Update g0:k = [g0:k-2 gk-1], sk-1,i =
k (gθ:k-i,i ^Lk-ι,i) I∣2 i = 1, 2,...,d and Hk-1 = δI + diag(sk-ι).
4:	(Shadow Update of x) Xk = Xk-ι + η ∙ A-_1igX-i.
5:	(Shadow Update of y) yk = yk-1 + η ∙ B--Igk-ι
6:	(Gradient Evaluation) gk-1 = m1 Pm=I V(Zk； ξ^k-1).
7:	(Gradient Concatenation and Norm Calculation) Update gi：k = [^Lk-ι gk], Sk-ι,i =
Il (g0:k-1,i gl:k,i) 12 i = 1, 2,...,d and S 6 7 8 9 10k-I = δI + diag(sk-1).
8:	(Real Update of x) Xk = Xk-1 + η ∙ Ck-IIgX-r
9:	(Real Update of y) y = yk-1 + η ∙ D--Igk-1∙
10: end for
22
Under review as a conference paper at ICLR 2022
Theorem D.3 (Convergence of AEG-DRD). When our objective function φ(x, y) satisfies Assump-
tion 2 and 3, as well as the bounded cumulative gradient condition, there exists an algorithm
(AEG-DRD), given:
•	A Stochastic First-order Oracle (SFO) access to the objective function φ(x, y) : Rn1+n2 →
R, which is denoted as: V (z; ξ) and it satisfies the following conditions:
E[V(z； ξ)] = V(Z) =(-Vχφ, Vyφ) and EkV(z; ξ) - V(z)k2 6 σ2.
Here, z := (x, y) and Z := Rn1+n2 = Rd where d := n1 + n2. Also: the one-sided MVI
function of V has a solution z* ∈ Z.
•	Positive real numbers G, such that kV(z; ξ)k2 6 G almost surely holds.
•	A positive real L such that V is L-Lipschitz continuous with respect to ∣∣ ∙ ∣∣2.
•	A universal constant D > 0 such that kzk - z* k2 6 D and for all the points on the
trajectory of our algorithm.
•	An initial point z0 ∈ Z.
•	The iteration number N.
•	A constant batch size m.
•	A constant 0 6 α 6 1/2 such that: the cumulative gradients are bounded as:
k(gi:k,i g1.k,i)k2 6 2δ ∙ ka forall i,k.
•	The constant learning rate η 6 £.
The output results z0, z1, . . . , zN-1 satisfy the following inequality:
1 ʌ	2	16dD2δ∕η2 + 200dδ + 40dG2∕δ	24dG2∕δ + 50δσ2∕m
N Er kVx (Zk-I)∣C--1 6------------------Ni-O---------------+------------N----------.
k=1
D.1 Proof of Theorem D.1
We recall that in the t-th iteration of Stochastic Extra-Gradient (SEG) algorithm with batch size, we
have the following updates:
mt
Zt = πz zt-ι + η ∙ m X V(Zt-1； ξi-ι)
t i=1
zt = ΠZ
mt
1
zt-1 + η •——EV(zt； ξt-l)
mt
i=1
∏z [zt-1 + η ∙ gt-ι]
∏z [zt-1 + η ∙ gt-ι]
Before we start our proof, we introduce two simple properties of the projection operation ΠZ as
follows, where Z ⊆ Rd is closed and convex.
Lemma D.1.
(1)	kx - ΠZ (x)k2 + kΠZ (x) - zk2 > kx - zk2 and hΠZ (x) - x, ΠZ (x) - zi 6 0 holds for
∀x ∈ Rd , z ∈ Z. Actually, the first inequality is a simple extension of the second one.
(2)	The projection operator ΠZ is a compression, which means for ∀x, y ∈ Rd, it holds that:
kΠZ (x) -ΠZ(y)k2 6 kx - yk2.
Now, we can start our formal proof.
Lemma D.2.
IlZk - z*k2 6 IlZk-I- z*k2 - (1 - 6η2L2) ∙ kzk-1 - zk k2 -kzk - zk k2
+ 6η2(kεk-1k2 + ∣∣εk-1 k2) + 2hzk - z*,η ∙ εk-1i.
Here, εk-ι = m^ PmkL V(zk-i； ξi-ι) — V(Zk-I) and εk-ι
m1k PmkI V (Zk; a) - V (Zk).
23
Under review as a conference paper at ICLR 2022
ProofofLemma D.2. According to the Property (1) of Lemma D.1, we know that:
Ilzk - z*∣∣2 = IlnZIzk-I + η ∙ 9k-1] - z*∣∣2 6 Ilzk-I + η ∙ gk-1 - z*∣∣2 - Ilzk-I + η ∙ gk-1 - ZkIl2
=∣∣zk-1 - z*∣∣2 - ∣∣zk-1 - zk ∣∣2 + 2hη ∙ gk-1,zk - z* i
=∣∣zk-1 -	z*∣∣2 -	∣∣zk-1	-	zk + zk - zk ∣∣2 + 2hη ∙ gk-1,	zk - z*i + 2hη	∙ gk-1,zk	- zk i
=∣∣zk-1 -	z* ∣∣2 -	∣∣zk-1	-	zk ∣∣2 - Ilzk - zk ∣∣2 - 2hzk-1	- zk, zk - zki
+ 2hη ∙ (V(zk) + εk-1),zk - z*i +2hη ∙ gk-1, zk - zki
=∣∣zk-1 - z* ∣∣2 - ∣∣zk-1 - zk ∣∣2 - Ilzk - zk ∣∣2 + 2hzk - zk, zk - zk-1 - η ∙ gk-1 i
+ 2hη ∙ V(zk),zk - z*i +2hη ∙ εk-1,zk - z*i
Since z* is a solution of MVI inequality, we know that for ∀z ∈ Z, it holds that(V(z), z - z*〉6 0,.
Therefore:
(V(zk),zk - z*〉6 0.
Also, since zk = ∏z [zk-ι + η ∙ gk-ι], according to the property (1) of Lemma D.1, we have:
hzk - zk , zk - (zk-1 + η ∙ gk-1)i 6 0∙
After combining all the three inequalities above, we have:
∣∣zk - z*∣∣2 6 ∣∣zk-1 - z*∣∣2 - ∣∣zk-1 - zk∣∣2 - Ilzk - zk∣∣2 +2hη ∙ εk-1, zk - z*i
+ 2hzk - zk, zk - zk-i - η ∙ gk-i + η ∙ gk-i - η ∙ 0k-ι)
6 ∣∣zk-1 - z*∣∣2 - ∣∣zk-1 - zk∣∣2 - Ilzk - zk∣∣2 + 2hη ∙ εk-1,zk - z*i
+ 2(zk - zk,η ∙ gk-i - η ∙ gk-ii
6 ∣∣zk-i - z*∣2 - ∣∣zk-i - zk∣∣2 - Ilzk - zk∣∣2 + 2η∣zk - zkIl ∙ Ilgk-I - gk-i∣∣
+ 2hη ∙ εk-i, zk - z*i
(b)
6 ∣∣zk-i - z*『-Ilzk-I - zk ∣∣2 -Ilzk - zk『+ 2η]∣gk-i - gk-i∣∣2
+ 2(η∙ εk-i,zk - z*〉	(14)
Here, (a) holds by Cauchy-Schwarz inequality, and (b) holds because by using the property (2) of
Lemma D.1, we know that
Ilzk - zkIl = IlnZ[zk-i + η ∙ gk-i] - ∏z[zk-i + η ∙ gk-i]∣ 6 ll(zk-i + η ∙ gk-i) - (zk-i + η ∙ gk-i)∣
=η∙ Ilgk-I — gk-i∣∣.
Now, we focus on dealing with Ilgk-I - ^k-i∣∣. In fact:
Ilgk-I - Ok-ill 2 = IlV (zk-i) - V (zk) + εk-i - εk-i ∣2
(d)
6 3 (IlV(zk-i) - V(zk)∣2 + ∣∣εk-i∣∣2 + ∣∣εk-i∣∣2
(15)
(e)
6 3L2 ∣zk-i - zk ∣∣2 + 3(∣εk-i∣∣2 + ∣∣εk-i∣∣2)
Here, (c) holds by the definition of εk-i, ɛk-i. (d) holds because by Cauchy Inequality, ∣χ +
y + z∣2 6 3(∣x∣2 + ∣∣y∣2 + ∣∣z∣2) always holds. (e) holds because we,ve assumed that V(z) is
L-Lipschitz continuous under ∣∙∣2 norm. Combine Equation (14) and (15), we obtain that:
Ilzk - z*∣2 6 ∣∣zk-i - z*∣2 - (1 - 6η2L2)∣zk-i - zkIl2 -Ilzk - zkIl2 + 6η2 ∙ (∣∣εk-iIl2 + ∣∣εk-i∣∣2)
+ 2hη ∙ εk-i, zk - z*i,
which is exactly what we want to prove in this lemma.	□
Now, we come back to our Theorem D.1. Notice that
r (zk)2 = Ilzk- ∏z[zk + η ∙ V(zk)]∣2 = Ilzk - zk+i + zk+i - ∏z[zk + η ∙ V(zk)]∣2
(a)
6 2 (Ilzk - zk + i Il2 + ∣∣zk+i - nZ [zk + η ∙ V(zk)]|[2)
=2 (∣∣zk - zk+i∣∣2 + IlnZ[zk + η ∙ gk] - ∏z[zk + η ∙ V(zk)]『)
(b)
6 2∣∣zk - zk+i∣F + 2η2 ∙ llɛk∣∣2
(16)
Here, (a) comes from Cauchy Inequality and (b) holds because of the property (2) of Lemma D.1.
24
Under review as a conference paper at ICLR 2022
ProofofTheorem D.1. Put k = 1, 2,...,N in Lemma D.2 and add them up, and we obtain that:
N-1	N
IIzN - z*ll2 6 I∣z0 - z*k2 - (1 - 6η2L2) ∙ ^X Ilzk- zk+ιIl2 - ^X Ilzk - Zk∣∣2
N-1	N
÷ 6η2 ∙ X (IIεkIl2 + l∣εkll2) +2Xhzk - z*,η ∙ εk-ιi.
k=0	k=1
Therefore, after combining with Equation (16), we have:
N-1	N-1	N-1
Xrη (zk )2 6 2 X Ilzk- zk+1∣∣2 ÷ 2η2 ∙ X ∣∣εk ∣∣2
k=0	k=0	k=0
2	(	N-1	N
6 1 - 6 2l2 ("z0 - z*12 ÷ 6η2 , £ ("εk∣∣2 ÷ llɛk∣∣2) ÷ 2 E〈zk - z*,η ∙ εk-1i
-η ∖	k=0	k = 1
N-1
÷2η2 ∙ X ITI2
k=0
Since η 6 笠,we have 1 一 6η2 L2 > 2. After taking expectation on the inequality above, it holds
that:
1 N-1
N ∑Eh(zk)2] 6
k=0
4∣∣z0-z*∣∣2 ÷48η2 NX 亡 ÷2η2 NX 口
—N —	ɪ L嬴卞L嬴
4∣∣z0-z*∣∣2
N
UC 2 N-1
50η2
÷ ɪ
k=0
which comes to our conclusion. Here, we use the fact that 旧|际 ∣∣2 6 1，EkεkIl2 6
E〈zk 一 z*,η ∙ εk-1i = 0.
σ2
mk
and
□
D.2 PROOF OF THEOREM D.2
We recall that in the t-th iteration of Adaptive Extra-Gradient, our update is as follows:
zt = zt-1 ÷ η ∙ H-LIIgt-1
zt = zt-1 ÷ η ∙ S二 13t.
Now we begin our proof by introducing several lemmas.
Lemma D.3.
Hzk — z*l∣2k-ι 6 llzk-1 一 zllLi -llzk-1 一 zkll；1 ÷ Ilzk- zkll2k-ι ÷2hη ∙ ɛk,zk 一 z*i.
Here, ^⅛ = ^⅛ 一 V(zk) = m1 Pm=I V(zk； ξi) - V(zk).
ProofofLemma D.3. According to our update rule, we know that:
Hzk - z*llSk-1 = llzk-1 ÷η ∙ Sk-Igk - z*ι∣Sk-ι
=llzk-1 ÷ η ∙ Sk-Igk - z*llSk-ι -Ilzk-I ÷ η ∙ Sk-Igk - zk IISk-I
=llzk-1 一 z*!sfc-ι -Ilzk-I 一 zkIISk-I ÷ 2hη ∙ Ok,zk 一 z*i
=llzk-1	一	z* IlSk - i	—	llzk-1 一	zk ÷ zk	一 zk ∣∣Sfc-ι ÷ 2hη ∙ gk , zk 一 zki ÷ 2hη , gk, zk 一 z* i
=llzk-1	一	z*llSk-ι	-llzk-1 -	zk ∣∣Sfc-ι	- lzk 一 zk∣∣Sk-ι - 2(Sk-1(zk-1 - zk), zk 一 zki
÷ 2hη ∙ Ok , zk 一 zki ÷ 2hη ∙ gk , zk 一 z*i
=llzk-1 一 z*!Sfc-ι -Ilzk-I 一 zk IlSk-I - lzk 一 zk∣∣Sk-ι ÷ 2hη，gk, zk 一 z* i
÷ 2(zk - zk, Sk-1 (zk-1 - zk ÷ η ∙ Sk-Iok))
=llzk-1 - z*llSk-ι -Ilzk-I - zk IlSk-I - lzk - zk∣∣Sk-ι ÷ 2hη，gk, zk - z* i
÷ 2(zk - zk , Sk-1(zk - zk ))
=llzk-1 - z*llSk-ι -llzk-1 - zk IlSk-I ÷ Ilzk - zk∣∣Sk-ι ÷ 2hη，gk, zk - z* i
25
Under review as a conference paper at ICLR 2022
Notice that ^ = V(Zk) + ^. Since z* is a solution of MVI which means(V(z),z - z*〉6 0 holds
for ∀z ∈ Z, so (V(Zk), Zk — z*〉6 0. Therefore:
hη ∙ gk,Zk - Z*〉= (η ∙ (V(Zk) + ^), Zk - Z*〉6 (η ∙ ^,Zk - z*〉.
Combine it with the inequality above, we obtain that:
Ilzk - z*kSk-ι 6 llzk-1 - z*kSk-ι -IlZk-I - ZkIlSk-1 + llzk - zk IlSk-I +2hη ∙ εk, zk - z*〉,
which comes to our conclusion.	口
In the lemma above, it,s worth mentioned that the expectation of (η ∙ ^, Zk - z*〉is 0. In the next
lemma, we are going to deal with the upper bound OfkZk - ZkIISk—. Before that, we notice the
following fact:
Ho W S0 W H1 W S1 √ ... W HN W SN.
Lemma D.4.
llzk - zk IlSk-I 6 ~22- llzk-1 - zk Ilsfc-1 + 6η2 (∣∣εk-1∣∣H-ι + ∣∣εk-1∣∣S-ι )
O	k	k — 1	k — 1
+ 2η2 ∙I(S--ι- HUI)gk-i∣∣Sk-1
ProofofLemma D.4. Since Zk = Zk-ι + η ∙ H--Igk-I and Zk = Zk-ι + η ∙ S--Igk, We have:
Izk - ZkkSk-I= η2 ∙ ISfcIigk - HAIgk-IkSk-I= η2…S--igk- S--Igk-1 + S--Igk-I- HUIgk-IkSk-I
(a)
6 2η2 ∙ (kS--ι(gk -gk-i)kSk-1 + k(Sfcli - HuI)gk-ikSk-1)
= 2η2 ∙ (kgk-gk-ikS-1 + k(S--ι- H-1ι)gk-ikSk-1)
k-1
=2η2 ∙ IIv(Zk) - V(Zk-I) + εk-1 - εk-1kS-21 +2η2 ∙ ll(Sk-i - Hk-LI)gk-1kSk-1
(b)
6 6η2 ∙ (lIV(Zk)- V(Zk-I)kS-1 + kεk-1∣∣S-1 + ∣∣εk-1∣∣S-1 )
Sk-1	Sk-1	Sk-1
+ 2η2 ∙ I(Sk--11 - Hk--11)gk-1I2Sk-1
6 6η2 ∙(於 kzk - zk-1kSk-1 + llεk-1kS-11 + llεk-1 kH-1j
+ 2η2 ∙k(S--ι- Hk:1ι)gk-ikSk-1,
which comes to our conclusion. Here, (a) holds since for any norm ∣∣ ∙ ∣a, we have ∣∣x + ykA 6
(IlxkA + kuka)2 6 2(∣∣χ∣∣A + kykA), and (b) holds for the similar reason. (c) holds because of the
following two reasons:
(1)	Since V is L-Lipschitz continuous, and OI W Sk-ι, we have:
.................C 1.......................C	L2..	...	L2..	...
kV(Zk)- V(Zk-I) kSfc2] 6 O kV(Zk)- V(Zk-I) k2 6 ɪ llzk - Zk-Ik 6 Q llzk - Zk-IkSk”
(2)	Since Hk-1 W Sk-1, we have Sk--11 W Hk--11. Therefore,
ι∣εk-ι∣∣S-6 iiɛk-ikH-iɪ.
□
Since η 6 卷,then 6啜? 6 2. According to Lemma D.3 and Lemma D.4, we have:
kzk - z*kSk-1 6 llzk-1 - z*kSk-1 - kzk-i - zk ∣∣Sk-1 + 2hη ∙ εk-1, zk - z*〉+ 2 ∣∣zk-1 - zk ∣∣Sk-1
+ 6η2 (kεk-1∣∣H-1] + kεk-1∣∣S-J + 2η2 ∙ k(S--i - Hk-LI)gk-1kSk-1
26
Under review as a conference paper at ICLR 2022
which means:
2 Ilzk-I - ZkIISk-ι 6 Ilzk-I — z*llSk-ι -IIzk - z*llSk-ι +2hη ∙ εk-ι, zk - z*i
+ 6力(∣∣εk-ι∣∣H-i + ι∣εk-1∣∣S-1) + 2η2 ∙ Il(Sk-LI- Hk-LI)gk-ι Ii Sk一
∖	j∙/k-1	ʊk-i/
(17)
NOticethat, I∣zk-1 - zkIISk-I = llη ∙ Hk-LIgk-ι∣∣Sk-ι∙ Therefore,
Ilgk-IIlS真=IS--ι9k-i∣∣Sk-ι = 1HAlgk-1 + (S--I- H^ι)gk-1 ISk-i
62 (IIHAIgk-1ISk-1 +1(S--I- HAI)gk-iISk-i)	(18)
2	..	...	..	一 ,..
=η ∙ llzk-1 - zk∣∣Sk-ι + 2 ∙ I(Sk-I- Hk-I)gk-1∣∣Sk-ι
Combine Equation (17) and (18), we obtain that:
Ilgk-IIlST 6 F ∙ (Izk-I - z"lSk-ι -Izk - z1∣Sk-ι) + 一hεk-1,zk -吟
Sk-i	η2	k 1	k 1 η
+ 24 ∙ (ι∣εk-ι∣∣H-1 + ι∣εk-1∣∣S-1)+10 ∙ I(Sk-I- Hk-LI)gk-"∣Sk-i
k-i	k-i
(19)
In the inequality above, the expectation of (ɛk-i, zk-z*i is 0, which can be ignored under expectation.
Since we are going to do the summation over k = 1,2,...,N in the future, in the following lemmas,
we anaiyze the UPPer bounds of PN=I(Izk-1 - z*∣∣Sk-i - B -羽良-)PN=I I(SUi-
HUI)gk-iISk i,andP3(Iεk-ιI3 + 党-能-1 )
k 1	Hk-I	Sk-1
Lemma D.5.
N
X(Izk-1 - z*ISk-i -Izk - z*ISk-i) 6 2dD2δ ∙ Nα.
k=L
ProofofLemma D.5. Notice that
N	N-1
X (llzk-1 - z*∣∣Sk-i -Izk - z*llSk-i) 6 IIzO - z* iS0 + X Qlzk - z* IlSk -Izk - z*llSk-i)
k=1	k=1
N-1
=lz0 - z*∣∣So + X [(zk - z*)> (Sk- Sk-I) ∙ (zk - z*)]
k=1
N-1
6 D2 ∙ tr(So) + X D2 ∙ (tr(Sk) - tr(Sk-i)) = D2 ∙ tr(SN-i) < D2 ∙ 2dδNα,
k=1
which comes to our conclusion.	□
Lemma D.6.
X ∣(S--ι - HUI)gk-ilSk-i 6 2相；N”.
k=1
ProofofLemma D.6. For any δ 6 x < y, we notice that:
(y - χ)2
x2y
y — X y — X
X2	6	δ2
(20)
<
27
Under review as a conference paper at ICLR 2022
Therefore, we have:
NN
-1	-1	2	>	-1	-1	-1	-1
k(Sk-1 - Hk-1)gk-1 kSk-1 6	gk-1(Hk-1 - Sk-1)Sk-1(Hk-1 - Sk-1)gk-1
k=1	k=1
N
6 XG2 ∙ tr ((H-11 - S-L)Si(H-11 - S--I))
k=1
(a) N G2
6 X ~2Γ ∙ [tr(Sk-I) - tr(Hk-I)]
k=1
G2	E 、	G2 …C Va	2dG2 ∙ Nα
6 δ2Γ ∙ tr(SN-I) < δ2Γ ∙ 2dδ ∙ N = -δ-.
Here, (a) holds because of Equation (20).
Lemma D.7.
N
EX (kεkτkH--ι + kεk-1kS--ι) 6
k=1
δσ2
——十 4dδ ∙ Nα.
m
The proof of this lemma can be found in (Duchi et al., 2011). Combine Lemma D.5, D.6, D.7 with
Equation (19), we obtain that:
EX kgk-ikS-1 6 42 ∙ 2dD2δNa + 24 ∙ (δσ2 + 4dδ ∙ Na) +10 ∙
Sk-1	η2	m
NoW, We replace the gk-1 With the true gradient V (zk-1) as folloWs:
2dG2 ∙ Na
δ
N
EX kV (zk-1)k2H-1
Hk-1
k=1
NN
EX kgk-1 + εk-1kH-iι 6 2 ∙ EX kgk-ιkH-iι
k=1	k-1	k=1	k-1
N
+ 2 ∙ EX kειkH-ι∣
k=1	-i
(a)	N
6 2 ∙ E X [kgk-1kS-iι + gk-1(H--1
k=1
N
6 2 ∙ EX kgk-ιk
δσ2
- Sk-I) ∙ gk-1∖ +2 ∙( — + 4dδ ∙ N
k=1
6 -8 ∙ 2dD2δNa + 50 ∙ (Q
η2	m
=-8 ∙ 2dD2δNa + 50 ∙ (Q
η2	m
δσ2	N
S-21 + 2 ∙ ( ~m^ + 4dδ ∙ Na) +2G ∙ X [tr(Hk-l) - tr(Sk-1)]
+ 4dδ ∙ Na +20 ∙
+ 4dδ ∙ Na +20 ∙
k=1
2dG2 ∙ Na
δ
2dG2 ∙ N
δ^^
+ 2G2 ∙tr(H-1)
2G2d
α
+
δ
□
Here, (a) uses the conclusion of Lemma D.7. Finally, We multiplies 焉 on both sides and finish the
proof:
1	N-1
N E X kV(zk )kH-ι 6
k=0
16dD2δ∕η2 + 200dδ + 40dG2∕δ + 50δσ2∕m + 2G2d∕δ
N1-α
N
D.3 Proof of Theorem D.3
In this proof, We are still using the Adaptive Extra-Gradient (AEG) algorithm, Which its t-th iteration
is:
Zt = zt-i+ η ∙ Ht-Igt-1
Zt = zt-i + η ∙ S-1ι^t.
For simplicity, We split the update of x variable and y variable. Recall that Hk = Diag(Ak , Bk) and
Sk = Diag(Ck, Dk) Where for each k ∈ N, Ak, Ck ∈ Rni ×ni and Bk, Dk ∈ Rn2×n2. Also, We
28
Under review as a conference paper at ICLR 2022
denote gk = (gχ, gy) and ^ = (gX, ^) where gk,^ ∈ Rn1 and gy,gy ∈ Rn2. Then, the update
above can be rewritten as:
Xt = xt-1 + η ∙ A-IIgxLI
yt = yt-ι + / ∙ B--Igy-ι
Xt = xt-1 + n ∙ Ct-Igt
yt = yt-ι + ； ∙ D=ιgt.
Similar to Lemma D.3, we only focus on the x-s, and then we can get our upper bound for ∣∣xfc -
χ*∣∣Ck i，which is the first step of our proof.
Lemma D.8.
E-x*∣Cfc-ι 6 IIXk-I- x*∣Cfc-ι - kxk-ι- XkIICk-I + IlXk-xkIlCk-I +2hη∙εx-ι,χk- x*i.
ProofofLemma D.8. According to the update rule of Xk, Xk, we know that:
llXk - X*llCk-i = |Xk-i + η ∙ c--ιgk- X*1Ck-i
= IlXk-I + η ∙ c--1gk - X*kCk-i - |Xk-1 + η，C-1igk - Xk llCk-ι
= IlXk-I- x*IICk-I - !Xk-1- XkllCk-1 + 2hη∙ gk,Xk- X*i
=IlXk-I — X*1Ck-1 — llXk-1 — Xk + Xk — XkIlCk-I ÷2hη ∙gk,Xk — Xki + 2hη ∙ gx,Xk — X*i
=||Xk-1 - x* ∣∣Ck-ι - llXk-1 - Xk ∣∣Ck-ι - llXk - XkIlCk-I - 2hCk-1 (Xk-1 - Xk ),Xk - Xki
÷ 2(η ∙ gx, Xk - Xk〉÷ 2(η ∙能,Xk - x*〉
=llχk-1- χ*llCk-1 - llχk-1- χkllCk-1 - llχk- XkkCk-I ÷2hη∙ gx,χk- χ*i
÷ 2hχk - χk,Ck-1(χk-1 - χk ÷ η ∙。/1於)〉
=llχk-1 - χ*llCk-1 - llχk-1 - χk∣∣Ck-1 - llχk - XkllCk-1 ÷2hη∙能,χk - χ*i
÷ 2hχk - χk, Ck-I(Xk - χk))
=llχk-1 - χ*llCk-1 - 1χk-1 - χk∣∣Ck-1 ÷ llχk - XkllCk-1 ÷2hη∙说,χk - χ*i
Notice that gk = V(Zk) ÷ εk-1 ⇒ gx =匕(Zk) ÷ εx-1. Since z* satisfies the one-side MVI
condition, therefore〈匕(z), X - x*〉6 0 holds for ∀z = (χ, y) ∈ Z, so(匕(Zk),Xk - x*〉6 0.
Therefore:
hη∙ gx,χk - χ*i = hη∙ (Vx(Zk )÷ εx-1),χk - χ*〉6 hη∙ εx-1,χk - X*).
Combine it with the inequality above, we obtain that:
Ilχk-χ*HCk-1 6 llχk-1-χ*HCk-1 - Hχk-1-χkllCk-1 ÷ !χk-XkkCk-I ÷2hη∙εx-1,χk-χ*i,
which comes to our conclusion.	□
Notice that the term 2(η ∙εx-1, Xk - x*〉has zero mean, which can be ignored when taking expectation.
In the next step, we upper bound the ∣∣Xk - Xk ||Cfc_ 1 term. Also, we notice the following facts:
A0 W C0 W A1 W C1 W ... W AN W CN,
B0 W D0 W B1 W D1 W ... W BN W DN.
Lemma D.9.
llχk - χk ∣∣Ck-1 6 "^^2~ (llχk-1 - χk ∣∣Ck-1 ÷ llyk-1 - yk llDk-1) ÷ 6η2 (|屈-1|匕二 ÷ ||"-1葭二
÷2η2 ∙∣(C--1- A--1)g"∣∣Ck-1
ProofofLemma D.9. Since Xk = Xk-1 ÷ η ∙ A--Igx-I and Xk = Xk-1 ÷ η ∙ Ck-1ex, We have:
∣Xk - Xk∣Ck-1 = η2 ∙ ∣CJ1 能-A--1 gx-1∣Ck-1 = η2 ∙ ICjax - CJIgx-1 ÷ CJIgx-I- A--Igx-11^-
29
Under review as a conference paper at ICLR 2022
(a)	/	、
6 2η2 ∙ (lQfc2ι(gX - g")kCi + k(C--ι - A--1)gX-1kC一)
=2η2 ∙ (Mx -g"∣∣C二+ k(Cfc21 - A--I)g"kC一)
6 2η2 ∙ (Mk- gk-1kSτ + IKCfc-11 - A--I)gx-1kCk-1)
k — 1
=2η2 ∙ kV(Zk)- V(Z k-1) + εk-1 - εk-1∣∣S-1 + 2η2 ∙ Il(CJI- A--I)g x-1kCk-1
」k—1
(c)	/	、
6 6η2 ∙(kV(Zk)- VkZk-I)IIS—1 + ∣∣εk-1kS―1 + ∣∣εk-1∣∣S-1 )
Sk—1	Sk—1	Sk—17
+ 2η2 ∙k(C--1- A--I)gx-1kCk—1
6 6η2 •(於 kzk - zk-1kSk -1 + llεk-1 kS—J】+ ll£k-1kH—iɪ^
+ 2η2 ∙k(C--11 - A--1)gx-1lCk-1
==6η2 ∙ (~2 Ilxk - xk-1||Ck —1 + ~2 Ilyk - yk-1||Dk —1 + llεk-1kS—iɪ + H£k-1kH—iɪ
+ 2η2 50--1-4--1^-111^1
(21)
which comes to our conclusion. Here, (a) holds since for any norm ∣∣ ∙ ∣∣a, we have ∣∣x + y∣∣A 6
(IlxIlA + IlyIlA)2 6 2(∣x∣A + IlyllA), and (c) holds for the similar reason. (b) holds because
II；M n, _ I∣2 一 IlGx X ∣∣2	_|_ Il ^y Cy l∣2	'll；Jx Cx l∣2 and fpʌ bnldc fnr
Ilgk - gk-1∣∣sT	= Ilgk -	gk-1Hc —1	+	Hgk	- gk-1 H d —1	>	Ilgk	- gk-1Hc —1	, and (e) holds for
k—1	k—1	k—1	k—1
the same reason. (d) holds because of the following two reasons:
(1)	Since V is L-Lipschitz continuous, and δI W Sk-1, We have:
................C	1.................C	L2..	...	L2..	...
IlV(Zk)- V(Zk-I)IlS—J] 6 δ IIV(Zk)- V(Zk-I) Il2 6 ɪ llzk - Zk-IIl 6 Q llzk - Zk-IllSk—『
(2)	Since Hk-I W Sk-1, We have Sk-I W Hk-L1∙ Therefore,
|际-1|*J16 l∣εk-1 IlH—ɪɪ ∙
□
Since we require our learning rate η 6 余,the coefficient above 尊L < 1. After combining
Lemma D.8 and Lemma D.9, we obtain that:
Ilxk - x*||Ck-1 6 llxk-1 - x*||Ck -1 - 2 llxk-1 - xk IICk- 1 + 2 llyk-1 - yk IlDk_ 1 +2hη ∙ εx-1, xk - x* i
+ 6η2 (|屈-1|匕-1 + iiεk-1lS-1)+ 2η2 ∙ Il(C--I- A--I)gx-1iiCk—1,
k—1	k—1
(22)
which means:
2 llxk-1 - xk IlCk_1
6 llxk-1 - x*||Ck-1 - lxk - x*llCk -1 + 2 llyk-1 - yk IlDk_ 1 + 2hη ∙ εx-1, xk - x* i
+ 6η2 (|际-1|匕—1 + ι∣εk-1∣∣Sτ)+ 2η2 ∙ Il(C-LI- A--I)成-1反—1,
k—1	k—1
(23)
NOtiCethat, ∣∣xk-1 -xk∣∣Ckτ = l∣η ∙ A--Igx-IkC-.Therefog
臃-1降1 = 2^1^-1层- = ∣A--1gx-1 + (Cfc-I- A--1)gx-1lCk-1
k—1
6 2 (|因--1成-1|区―1 + I(C--1 - A--1)gx-1lCk—1)
2
=η ∙ llxk-1 - xk llck _ 1 + 2 ∙ H(Ck-I - Ak-I)9^11& — 1.
(24)
30
Under review as a conference paper at ICLR 2022
Combining Equation (23) and Equation (24), we get:
4	2	8
llgk-lke-iɪ 6 η (Ilxk-I - x Ilcfc-1 - IIxk - x kCk _1) + η Ilyk-I - ykkDk_1 + 庐 hη，εk-1,xk - x i
+ 24 (∣∣εk-ι∣∣H-1 + B-1∣∣S-1 ) +10 ∙ Il(C--I- A--I)g"∣∣Ck.ι,
k—1	k—1
(25)
Then, we take summation over k = 1, 2,...,N and take expectation, and we can obtain that:
N	. N	CN
E X 1屐-1值—-16 -2E X (^xk-i- x*ICk-1- Ixk- χ*nCk-J + ∑2E X ι∣yk-1 - yk ι∣Dk-1
k=1	/ k=1	/ k=1
NN
+ 24 ∙ EX (∣∣εk-1∣∣H-1∣ + 喻-IkS-IJ +10 ∙ EX Il(CUI-若--1)成-11区-1
k=1	1	1	k=1
(26)
In the following steps, we will upper bound the four terms above on the right side. The third term
E PN=I∣∣εk-ιI2--1 + ∣∣εk-ι∣∣2-1 ) can be upper bounded by using Lemma D.7:
一∖	Hk-I	Sk-IJ
δσ2
E X (∣∣εk-1 IlH-1 + ∣∣εk-1 ∣∣S-1 ) 6	+ 4dδ ∙ No.	(27)
k = 1	-1	-1
The fourth term E PN=I Il(Ck-1i - A--1)gX-1llCkτ can be upper bounded by using Lemma D.6:
ɪN--C	ʌ	C	2dG2 ∙ Na
EX I(c--1- 4--1^-11^-1 6 EX I(S--1- HUM-IkSk-1 6 δ.
k=1	k=1
For the first term E PN=I (∣∣xk-ι — x*ICfc_ 1 - IlXk — x* ICfc_ ɪ) ,we can use the similar techniques
as Lemma D.5.
Lemma D.10.
N
X (Ixk-ι-x*ICk-1 -Ixk-x*ICk_1) 6 2dD2δ ∙ Nα.
k=1
ProofofLemma D.10. Notice that
N	N-1
X (nxk-i -x*ι∣Ck-1- Mx® -x*nCk-J 6 B - xiiCo+ X (nxk -x*ι∣Ck- Mx® -x*nCk-J
k=1	k=1
N-1
=∣∣x0 - x*∣∣Co + X [ (Xk- x* )>(Ck - Ck-I) ∙ (xk - x*)]
k=1
N-1
6 D2 ∙ tr(Co) + X D2 ∙ (tr(Ck) - tr(Ck-I)) = D2 ∙ tr(CN-1) < D2 ∙ tr(SN-1) 6 D2 ∙ 2dδN:
k=1
which comes to our conclusion.	□
For the second term E PN=I ∣∣yk-1 - yk ||Dfc_ 1,notice that:
N	N 2	N	2
X Iyk-1 - yk IDk-1 = X *^--1"^-1 = X * 成>"1 Dk-IB--1。"
k=1	k=1	k=1
NL R	wn n2	2dδka
6 X k ∙ G%r(B--IDkTB--I) < X kG2 ∙
k=1	k=1
2η2dG2 XX 1
=-δ-工 k2-α
k=1
31
Under review as a conference paper at ICLR 2022
Here, we notice that:
X k2-α < 1 + Z1 x2-adx< 1 + Z1 +dx = 1 + 2 (1-√√N) < 3.
Therefore:
X kyι- ykkDk-1 < 6η2δG2.
k=1
According to Equation (26):
N
EX kgkx-1k2C-1
k=1	-1
6 当∙ 2dD2δ ∙ Na + 4 ∙ 6η⅛G2 +24 ∙(贮 + 4dδ ∙ N。) +10 ∙ 2dG：∙ Nα
η2	η2	δ	m	δ
=Nα ∙ (8dD2δ +96dδ +20dG2) +的 + 也.
η2	δ	δ m
Finally, we replace the gkx-1 above with the actual gradient Vx(zk-1). Since:
kVx(zk-1)k2C-1 6 kgkx-1kC-1 +kεkx-1kC-1 2 62kgkx-1k2C-1 +kεkx-1k2C-1
Ck-1	k-1	k-1	Ck-1	Ck-1
we can upper bound the target term E PkN=1 kVx(zk-1)k2 -1 :
Ck-1
(28)
N	N	NN
E X kVx(zk-1)k2C-1 6EX2	kgkx-1k2C-1	+kεkx-1k2C-1	62EXkgkx-1k2C-1	+2EXkεkx-1k2C-1
Ck-1	Ck-1	Ck-1	Ck-1	Ck-1
k=1	k=1	k=1	k=1
(a)	Nα ・	(16dD2δ	+ 192dδ +	40dG2、		24dG2		48δσ2	+ 2E	N
6		I η2		δ ,	+	δ	+	m		Σ k=1
										
										
(b)	Nα ・	(16dD2δ	+ 192dδ +	40dG2、	\	24dG2		48δσ2	(	δσ2
6		I η2			+	δ	+	m	+2	m
										
		(16dD2δ		40dG2 ) -^δ- /		24dG2		50δσ2 , m		
=	Nα ・		+ 200dδ +		+	-δ-	+			
+4dδ ∙ Nα
H--ι + kεk-1kS--ι
which means that:
1N
NE X kVx(zk-1)kC-ι1 6
k=1
which comes to our conclusion.
16dD2δ∕η2 + 200dδ + 40dG2∕δ + 24dG2∕δ + 50δσ2∕m
N1α
N
(29)
32
Under review as a conference paper at ICLR 2022
E	Pseudo-Algorithm of AMS Grad-EG-DRD
Algorithm 5 Extra Gradient AMSGrad with Dual Rate Decay
Input: The initial state zo = mo = vo = 0, a constant learning rate η, a Stochastic First-order
Oracle (SFO) V (z; ξ), momentum parameters β1t, β2, a sequence of batch sizes {Mk}.
Output: zt with t uniformly chosen from {0, 1, . . . , N - 1}.
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
for k = 1, . . . , N do
(Gradient Evaluation 1) gk-1 = m1 Pm=I V(Zk-I； ξk-I).
(Momentum Update 1)mk = βikmk—i + (1 - βik)gk-ι
(Velocity Update 1) Vk = max(β2Vk-1 + (1 - β2)g2-ι,Vk-ι), Hk = δI + Diag(√vk).
(ShadowUpdate) Xk = xk-i + η ∙ (Hχ)-1 mχ,
yk = yk-i + √ (Hk)-1 my, Zk = (Xk, yk).
(Gradient Evaluation 2) gk =京 PM=L V(Zk; ξk).
(Momentum Update 2) mk = βikmk + (1 - βik )gk.
(Velocity Update 2) Vk = max(β2Vk + (1 - βz)g2,Vk), Hk = δI + Diag(√vk)
i
(Real Update) Zk = Zk-i + η ∙ Hk mk.
(Real Update) Xk = Xk-i + η ∙ (Hx)	mX，
yk = yk-1 + √ηk (Hy)	m k, Zk = (xk , yk ).
end for
33