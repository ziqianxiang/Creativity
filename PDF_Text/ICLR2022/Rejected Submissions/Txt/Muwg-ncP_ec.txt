Under review as a conference paper at ICLR 2022
Exact Stochastic Newton Method for Deep
Learning: the feedforward networks case.
Anonymous authors
Paper under double-blind review
Ab stract
The inclusion of second-order information into Deep Learning optimization has
drawn consistent interest as a way forward to improve upon gradient descent meth-
ods. Estimating the second-order update is computationally expensive, which
drastically limits its usage scope and forces the use of various truncations and
approximations. This work demonstrates that it is possible to solve the Newton
direction in the stochastic case exactly. We consider feedforward networks as
a base model, build a second-order Lagrangian which we call Sifrian, and pro-
vide a closed-form formula for the exact stochastic Newton direction under some
monotonicity and regularization conditions. We propose a convexity correction
to escape saddle points, and we reconsider the intrinsic stochasticity of the online
learning process to improve upon the formulas. We finally compare the perfor-
mance of the developed solution with well-established training methods and show
its viability as a training method for Deep Learning.
Optimization in Deep Learning is mainly dominated by first-order methods built around the central
concept of backpropagation (LeCun et al., 1988). Second-order methods have exceptional theoreti-
cal properties in deterministic optimization (Boyd et al., 2004; Nocedal & Wright, 2006), but such
properties do not translate well into the stochastic case (LeCun et al., 2012; Bottou et al., 2018).
First-order methods such as the Stochastic Gradient Descent (SGD) (Robbins & Monro, 1951) are
relatively simple and have been adaptively improved for Deep Learning (Duchi et al., 2011; Tiele-
man & Hinton, 2012; Kingma & Ba, 2014; Reddi et al., 2019; Yao et al., 2020). Inherently, second-
order methods might seems inadequate for Deep learning due to increased computational cost, poor
clock-wall performance, and the non-convex nature of Deep Learning (LeCun et al., 2012). Further-
more, Newton method might even reduce the generalization capabilities of training (Wadia et al.,
2021; Amari et al., 2020). Despite these various limitations, substantial effort was deployed to
include Hessian information into the optimization process (e.g. Byrd et al., 2011; Sohl-Dickstein
et al., 2014; Byrd et al., 2016; Agarwal et al., 2017; Berahas et al., 2019; Anil et al., 2020; Goldfarb
et al., 2020; Castera et al., 2021). Several approaches exist such as the Gauss-Newton method (e.g
Schraudolph, 2002; Botev et al., 2017), diagonal approximation of the Hessian (e.g. Bordes et al.,
2009; Schaul et al., 2013), iterative low-rank updates such as BFGS (Broyden, 1970; Fletcher, 1970;
Goldfarb, 1970; Shanno, 1970) (see also, Liu & Nocedal, 1989; Schraudolph et al., 2007; Bollapra-
gada et al., 2018), or Hessian-Free methods which combine the fast Hessian matrix multiplication
(Pearlmutter, 1994) and the conjugate gradient algorithm (e.g. Martens, 2010; Martens & Sutskever,
2012; Dauphin et al., 2014). The use of the Fisher matrix to capture curvature information in the
space of distributions, instead of the Hessian, is another approach which yields the natural gradient
(Amari, 1998), but suffers from the same computational issues as the Newton method. Within this
context K-FAC method (Martens & Grosse, 2015; Ba et al., 2016a; George et al., 2018) mitigates
some of the computational issues of the natural gradient method. In essence, several drawbacks and
flaws limits the adoption of second-order methods as a standard for neural networks training .
In this paper, we propose a novel approach to characterize the Newton update which help us derive
an exact closed-form solution for the stochastic Newton method. Our method requires a suitable
regularization of the neural network and strict monotonicity of the activation functions. We start this
paper by introducing useful notations while deriving the well-known backpropagation algorithm for
a feedforward network. Then we introduce a second-order Lagrangian which we call Sifrian, that
will serve to characterize the Newton direction. We derive four types of equations from the Sifrian,
and we provide an exact closed-form solution for the Newton direction in the stochastic case. We
1
Under review as a conference paper at ICLR 2022
further propose a saddle-free version of our method, and add a randomization process to enhance our
solution. In the last part of this paper, we show the applicability of our method for Deep Learning
through diverse classification tasks using feedforward architectures.
1 Preliminaries
Deep Learning and neural networks training could be seen as an optimization problem ofan expected
loss function ` over a distribution D of labeled samples (x, d). In general, weights and biases
(W, β) are the sought-after parameters of the network. The labeled samples distribution D is often
unknown, but a large number of samples (database D) allows the approximation of the expected loss
with an empirical risk:
Wie (E(x,d)〜D [' (W, β, x, d)]) →∣D∣ X '(W, β, X(P),d(P)) , (X(P), d(P))Pe J" D.⑴
The loss or cost function ` is typically a cross entropy function or an l2 norm of the mismatches
between the network outputs and the labels: ' (W, β, (X(P), d(P))) = 2 卜?) - d(p)∣∣ . Where
n indicates the output layer number. We present more in detail the architecture of a Feedforward
Neural Network (FNN) hereafter.
1.1	Notations and Feedforward Neural Networks (FNN)
In this section, we recall the main details of FNNs, which will be used as a standard model for Deep
Learning1. The notation used throughout this paper is similar to notations presented in LeCun et al.
(1988). The main equation governing the FNN, a.k.a. the forward model is the following:
X(kP) =F (Wk X(kP-)1 + βk) , k ∈ [1..n], p∈D.	(2)
where D is the database, p designs one single sample from the database (e.g. one single image or
audio recording), k is the layer number and n is the total number of layers in the network. The
initial input for sample p is X(0P) (e.g. the vectorized input image data). The state variable X(P) is
transformed at each layer k through a multiplication by a weight matrix Wk and an addition of a
bias vector βk. The activation function F, which is typically a sigmoid or a Rectified Linear Unit
(ReLU), is applied element wise on the resulting activation vector: a(kP) = Wk X(kP-)1 + βk, and
serves to introduce non-linearity in the neural network.
1.2	The Lagrangian and Backpropagation
The origin of backpropagation could be traced back to the early 1970s and could be derived by
casting Deep Learning as a constrained optimization problem. This section is similar to LeCun et al.
(1988). The main novelty in this section is the addition of a regularization term dependent on the
state variable R(x) to the cost function ': 'r = ' + R (x). The regularization has to verify the
admissibility criteria which we define hereafter:
Definition 1: A regularization term is admissible if the second derivative of the augmented cost
function w.r.t. to the state variable is separable and non-null, i.e.:
∀ (p, q) ∈ D, ∀ (k, m) ∈ [1..n]
∂'r
dxkP)Xm)I P,q,k,m
H δP=q.
(3)
An example of such regularization is the following function:
R(X) = 2 X X	DXkP), AkP)XkP)E	(4)
P∈D k=1..n-1 1
1Convolutional Neural Networks (CNN) are a type of FNN. The concepts of this paper apply also to CNN.
2
Under review as a conference paper at ICLR 2022
Λ(kp)	are symmetric positive matrices. Such a choice might seem atypical since the regular-
ization often concerns the network parameters, mainly the weights or the biases. This ”admissible”
regularization was introduced for the sole purpose of guaranteeing a non-null partial derivative of
the cost ` w.r.t the state variable x(kp) . This last property is essential to solve Equ. 16. We will show
later that such a regularization minimizes also the norm of the gradient (Barrett & Dherin, 2020;
Smith et al., 2021).
In order to derive the backpropagation algorithm we introduce the following Lagrangian as per the
notation of LeCun et al. (1988):
L(x, W, β, b) = 'r + X X Ekp- F (Wk X巴 + βj , b* .
p∈D k=1..n
(5)
The Lagrangian contains the original cost function, the admissible regularization term, and the prod-
uct of the forward equation with the adjoint state vectors b(kp)	. These adjoint vectors are de-
fined for each layer of the network and each sample of the database; however, their values are not
fixed yet and will be chosen in a way that simplifies the gradient computation. If the state variable
x verifies the forward equations, then the Lagrangian simplifies to the cost function:
L(x (W, β), W, β, b) = 'r (x (W, β)).	(6)
The total derivative of the previous Lagrangian w.r.t. the weights or biases is the gradient and could
be expressed in terms of partial derivatives as follows2 :
{dL(x(W,β),W,β,b) =	_dL_	+	P P	(dx⅛p)、( ∂L ∖ =	d'R
dWk	=	∂Wk	+	/^p∈D 2^m=1..n	IdWk J	IdXm) J =	dWk	,
(7)
dL(x(W,β),W,β,b) — dC_ + P P	(dxm)、( ∂L ∖ — d'R
dβk	∂βk + 2p∈D 2m=1..n〈 dβk J ∖dx(m J	dβk .
The partial derivatives of the Lagrangian w.r.t to {Wk , βk }k=1..n are straightforward to compute.
(dx(p) dx(p)
■dWm, -dm)	are non-trivial to evaluate and the core idea of back-
propagation is the selection of the adjoint state variables b(kp) which cancels the superfluous
terms. Such a simplification is achievable if:
= 0.
p∈D, k∈[1..n]
Computing the previous partial derivative yields the following backpropagation equation:
~~= = bk" - Ik=L.n-1Wk+lVF (akp+l) bfe + 1 + -^-r = 0.
∂xk	∂xk
(8)
(9)
1E is the indicator function, and it is equal to one if the underlying condition E is true and null
otherwise. The nabla operator VF is a diagonal square matrix with an element-wise derivative of
its argument (along the diagonal). The resolution of the backpropagation system could be split into
a boundary condition and a backward propagation system. Further details could be found in LeCun
et al. (1988). The particular choice of the adjoint state vectors b(kp)	yields a simple
p∈D, k=1..n
formula for the gradient of the cost function:
Gk
gk
∂L
∂Wk
∂L
- -
∂βk
-Pp VF a(kp)	b(kp)x(kp-)T1,
-Pp VF (a(kp)b(kp).
(10)
The backpropagation algorithm is fundamental for Deep Learning. It is simple and provides a
straightforward solution to an otherwise tedious problem to solve. Unfortunately, developing an effi-
cient second-order backpropagation method remains elusive. The current state-of-the-art is based on
the use of the R-operator (Pearlmutter, 1994) to compute the product of the Hessian with a given
vector without explicitly calculating or storing the Hessian (Martens, 2010; Dauphin et al., 2014;
Agarwal et al., 2017). In the following section, we provide a different framework for backpropaga-
tion which allows the characterization of the second-order Newton direction.
2A denominator layout notation for derivation is used throughout this paper.
3
Under review as a conference paper at ICLR 2022
2 The S ifrian and the Exact Newton Direction
It is legitimate to wonder if the Lagrangian multipliers method could be replicated for higher-order
optimization. We hereafter provide a solution for the second-order case and build an equivalent
Lagrangian which we call the Sifrian.
2.1 The Sifrian
The Lagrangian is useful to derive the first-order gradient, and it combines the cost function and the
forward model multiplied by adjoint variables. Our approach to extend the Lagrangian consists in
creating a function with the forward model, the backward model, and the definition of the gradient.
We choose to omit the loss function, as its derivation will generate the gradient again. Starting from
this paradigm, we introduce the Sifrian3 .
Definition 2: The Sifrian S of the feed-forward network is defined as follows:
S(x,b,W,β,G,g,γ,ζ,N,η) = Pk,p Dx(kp) -F a(kp) , γk(p) E
+ (bkp) - 1k=1..n-1Wk+lVF (akp)ι) bk+1 + ∂X(r)，z(p))
+ Pk〈Gk + PpVF (akp)) bkp)xkp-T, Nk E
(11)
+ gk + PpVF a(kp) b(kp),ηk .
The Sifrian includes all the equations generated by the first order backpropagation, multiplied
by four new adjoint parameters γk(p) , ζk(p) , Nk, ηk . While the Lagrangian describes a saddle
point (Nocedal & Wright, 2006), the Sifrian, describes an equilibrium and it has a unique null value
when all the forward, backward and gradients equations are verified i.e.:
S(x(W,β),b(W,β,x),W,β,G(W,β,x,b),g(W,β,x,b),γ,ζ,N,η)=0.	(12)
We call the previous quantity the equilibrated Sifrian, and it is critical to notice its invariance to total
derivation w.r.t. either the weights or biases (i.e., remains null). Expressing the total derivative of
the equilibrated Sifrian using partial derivatives yields the following two equations:
dS ―	∂S	+ P	dxm	∂S	+	dbim	∂S	+ P	dGm ∂S	+	dgm	∂S
dWk =	∂Wk	+ 乙m,p	dWk	∂χ(p)	+	dWk	∂b(p	+ Lm	dWk	∂Gm	+	dWk	∂gm
dS	=	∂S	+ P	dXm) ∂S	+ 如“)∂S	+ P	dGm ∂S +	dgm ∂S
dβk	=	∂βk	+ 乙m,p	dβk	∂χ(m)	+ dβk	∂b^P)	+ mm	dβk	∂Gm +	dβk ∂gm
(13)
Similar to first order backpropagation, we select the new adjoints variables
γk(p) , ζk(p) , Nk , ηk	to circumvent the calculation of any superfluous terms. We
make the following choice:
∂ ∂	∂S - C	∂S -
∀m,∀q :	∂Wm = -Gm, ∂βm = -gm,
S
---FT = O,
x(q)
xm
∂S
∂bmq)
(14)
In this case, after vectorizing the weight gradient Gk (and also Nk), and concatenating all their
values across all layers, the Equ. 13 could be cast as the following block matrix system:
dG	dg
dW	dW
dG	dg
dβ	dβ
→H
(15)
N
η
G
g
N
η
G
g
3The choice of the Sifrian appellation stems from the Arabic word ”Sifr”, which means zero. ”Rien” means
also ”nothing” in French.
4
Under review as a conference paper at ICLR 2022
The matrix that appears in the previous system (H) is the Hessian. Hence the computation of the
Newton direction could be achieved by solving the associated system of four equations in Equ. 14.
The derivation of the four types of Sifrian equations4 is provided with further details in the Ap-
pendix (A.1). We report hereafter the equations for piece-wise affine activation functions (e.g.,
ReLU or Leaky ReLU). The general case ofC2 activation function is included in the Appendix (A.1).
⑴ PpVFkp) (Ykp)Xkp)T + bkp)xkp)T + 1k=2..nbkp)Zkp)T) = 0，
⑵ Pp VFkp) (Ykp) + bkP))= 0,
(3)	Ykp)- 1k<nWk+ivFk+)iY(+1 + Pm dxf2⅛ZW + 1k<nNk+iVFk+)ibkp+i = 0,
(4)	ζk(p) - 1k=2..nVFk(p)Wkζk(p-)1 + VFk(p) (Nkx(kp-)1+ηk) =0.
(16)
Comparison with the R-operator: Characterizing the Hessian effect without explicitly comput-
ing its values has been proposed by Pearlmutter (1994), using an operator R which requires the
preselection of a specific direction to study its transformation through the Hessian. The Sifrian
formulation can achieve the same goal by relaxing the two gradient constraints from the Sifrian
equations and preselecting the values of {N, η}. The product requires a forward pass to compute
ζ, a backward pass to compute Y. Finally, the sought-after product is provided by the gradient of
the Sifrian w.r.t. to the weights and biases. The Sifrian formulation is derived differently from the
R-operator. Nevertheless, it can still fulfill the same role, and it is more suitable for inversion, which
we will conduct hereafter.
2.2 The Exact Stochastic Newton (ESN)
The resolution of the previous system is intricate in the general case. However, if we consider a
single sample, i.e., the stochastic case and a strictly monotonous activation function, then a closed-
form solution could be derived.
ESN Theorem: If the cost function is admissibly regularized, the activation function F is strictly-
monotonous piece-wise affine, the first order adjoints b(kp)	are non-null and the curvature of
the output layer { -d2`R (P) [ is invertible, then the minimal-rank stochastic NeWton second-order
∂xnp ∂xnp
update has a closed-form solution:
(17)
ηk = 1k=n [VF (anp))i τ { a ∂2'R(P) 0 - 1 普-Nkx%
∂xn ∂xn	∂xn	k-1
Proof: Under the proposed assumptions, the Sifrian second equation simplifies to Yk(p) = -b(kp),
Which starts a cascade of simplifications. The first equations becomes 1k=2..nζk(p-)1 = 0. The last
layer of the third equation yields: Ynp) + 时(P) Znp) = 0. The remaining two equations become:
∂xn ∂xn
Ik=ι..n-1 ∂XR + Ik=ι..n-1NT+1VF (akp+ι) bkp+1 = 0,
-n∂X∂⅛Foτ 嘉 + vf (akP))(NkXkp-I+ η) = 0.
4The Sifrian yields a total of 2 |D| + 2 equations with an equal number of unknowns.
(18)
5
Under review as a conference paper at ICLR 2022
The Newton weight is a matrix, and it is determined through its product with the adjoint vector.
Inverting the Newton direction is a degenerate problem, but the minimal rank solution is unique,
and it is given in Equ. 17. A major issue still remains: the results of the Newton weight do not
extend to the first layer (Equ. 18). To circumvent this shortcoming, we introduce an additional layer
before the input, which corresponds to the identity multiplication and a null bias. This layer remains
unchanged throughout the optimization process, and we call it the ”white layer”. This pre-input
extension allows the definition of an update rule for the first layer.
White layer concept The third equation of the Sifrian allows the characterization of the Newton
weight Nk only between the 2nd and the output (nth) layers. Such a problem is fundamental
and it is legitimate to wonder how is it possible for Hessian-Free methods to work for inversion.
The answer lays within the regularization/damping (Marquardt, 1970) which covers this structural
gap. The important role of the first layer for the Newton method has been highlighted recently by
Wadia et al. (2021). In our case, and in order to extend the inversion result to the first layer, we just
introduce an additional layer before the input, which has the same size as the input, and does not
bring any modification, hence the appellation ”white layer”. The white layer corresponds to a matrix
multiplication with the identity, a null bias, an identity activation function and is unchanged during
the training.
Figure 1: White layer extension before the input of the neural network.
Admissible regularization: The ESN solution relies primarily on the existence of the term
which would be null if not for the admissible regularization, whose effect revolves ul-
timately around minimizing the norm of the state variable x(kp)	. Given that the cost function `
steers the weights/biases to minimize the mismatches, and consequently the adjoints b(kp)	then
p,k
the admissible regularization is contributing to the gradient’s norm minimization (Equ. 10), which
is a desirable features for generalization purposes (Barrett & Dherin, 2020; Smith et al., 2021).
The ”granular” solution: The admissible regularisation could be removed and this would lead to
a ”granular” solution which updates only the bias of the output layer ηn . The granular solution is
not practical and it is of no use for the neural network training.
Hypotheses relaxation: Some of the hypotheses of the ESN theorem could be relaxed: notably the
[∂2'r	ʌ
Idxnp) ∂xnp) /
invertibility of the output curvature:
Moore-Penrose inverse could be used instead.
The output curvature is often positive definite and its inversion is simple for e.g. it is the identity for
an l2 cost function.
6
Under review as a conference paper at ICLR 2022
3	Adjustment of the Exact Stochastic Newton (ESN)
The ESN solution is exact and simple to compute; however, it diverges if implemented as it is. In
this section, we first address the non-convexity issue and adjust the solution to become a descent
direction. We also explore the stochasticity effect stemming from the samples randomness during
the training process and provide an enhancement to the ESN. For the remainder of this paper we
assume also that regularization is a of the same form as Equ. 4. We start this section by correcting
the concave directions of the ESN.
3.1	Escaping the S addles: a Descent Direction
The minimal rank solution (Equ. 17) has some similarities with the gradient expression (Equ. 10),
yet it is not a descent direction. To further illustrate this, we consider the inner product of the Newton
and the gradient expressions layer by layer. For the weights, the product is always positive:
hNk,Gki = Dx(kp-)1, Λ(kp-)1x(kp-)1E = x(kp-)12Λ(p) ≥ 0.
k-1
(19)
However, most of the Newton biases corresponds to ascent directions :
hηk, gki
-	x(kp-)1Λ(kp-)1
+ 1k=n
d2'R
。乂俨％Xn))
-1
(20)
The last layer bias nature depends on the values of the regularization and the error. One of the
main advantages of the Sifrian formulation is that we can treat the weights and biases separately
and we can exactly pinpoint to the origin of non-convexity: the bias update. We choose to build
our saddle-free variation by just changing the signs of the ascent direction. This manipulation is
straightforward for all layers except the output bias update. The problematic term corresponds to
the ”granular” solution (i.e., no regularization), which overfits every sample. We chose to keep the
”granular” solution as is and switch the sign of the remaining term. This guarantees that the update
is a descent direction5. The saddle-free ESN is the following:
(21)
* = 1k=n hvF (anp))i	n ∂xn⅛np) O 静 + NkX巴.
The above convexity correction is significantly simpler than the currently existing methods such
as the Gauss Newton truncation (e.g. Schraudolph, 2002; Martens, 2010) or the absolute Hessian
(Dauphin et al., 2014). The convexity correction is the first step in improving the ESN. Another
intrinsic issue in the above formulation is that we try to optimize a given sample without taking into
consideration the yet ”unknown” coming samples. The essence of stochasticity in still missing. In
the next section, we include such an aspect into the ESN method.
3.2	Randomization
The ESN solution we have proposed so far is exact but remains deterministic since it focuses on
training a single pre-selected sample p. An intrinsic source of stochasticity in online learning stems
from the randomness of the streamed samples. As such, it is preferable to use random variables to
describe the dynamics of ESN. The two fundamental equations of ESN after discarding the ”granu-
lar” solution, adding the white layer are:
(d⅛)T +	ykp)TNk=	O,	With	ykp)	=	VF	(akP))	bkp),
NkX(kp-)1 + ηk = 0.
5This point could be further justified With a first order Taylor development of the loss function.
(22)
7
Under review as a conference paper at ICLR 2022
Let's consider Ω = D as the set of possible outcomes of the training samples6 and randomize the
sample index p → p. Aside from {N , η} all the vectors in the above system become random
variables. We solve the randomized Equ. 22 using the following least-squares criteria:
2
argmin{N,η}
L
Ω=D
dΩ (P)
+ Nkx(kp-)1 +ηk22 .
2
(23)
We changed the ESN into a least-squares problem, which is desirable, given the sizeable well-
established literature about optimal unbiased estimators for this class of problems (Aitken, 1936;
Kariya & Kurata, 2004). The minimisation could be done for each layer separately, which reduces
the inverse problem dimensionality. The search for critical points is detailed in the Appendix (A.4),
and yields the following randomized ESN system which involves a Sylvester equation (for Nk) :
Ep hy(kp)yk(p)Ti Nk+NkVarx(kp-)1 = -Ep卜(∂Xk-ι )T]
NkEp hx(kp-)1i + ηk = 0.
(24)
We notice that the above system could considerably simplify if the state variables x(kp-)1 is normalized
i.e. null mean and identity variance. This aspect has been extensively explored in Deep Learning
through batch, weight, layer and group normalization (Ioffe & Szegedy, 2015; Salimans & Kingma,
2016; Ba et al., 2016b; Wu & He, 2018). Normalizing the state variable corresponds to the following
transformation:	ɪ
Skp) J (JVar (Xkp)))	(Xkp)- EPhXkp)D .	(25)
The Sylvester equation for the Nk could be entirely circumvented, by using the proposed Variance-
Invariance Theorem described hereafter.
Variance-Invariance Theorem: In addition to the hypotheses of the ESN Thereom (Section 2.2).
If ∀k ∈ [1..n] : Var X(kp-)1	0, then the randomized ESN is invariant to the variances
Var X(kp-)1 which can be substituted with the identity. The randomized ESN system becomes:
Nk = -E hI + ykp)ykp)Ti-1 e ykp) (d⅛)T
ηk = NkE hX(kp-)1i .
(26)
Proof: The demonstration builds on the invariance of the Newton method to affine transformations
and is detailed step by step in the Appendix (A.5)
3.3	Discussion
Expectations and estimators: We used the expectation E extensively in this paper to derive the
theoretical ESN solution. However, estimators would replace expectation during implementation.
The performance and the computational cost would be directly related to the estimators choice: rank
one, moving average, or even batch estimators are all possible with different compromises.
Notes on convergence: Several convergence results for the Newton method exist (e.g Bottou &
Le Cun, 2005; Byrd et al., 2011; Meng et al., 2020). The ESN convergence fits well within the gen-
eral framework developed by Sunehag et al. (2009), under some mild and non-restrictive conditions.
To batch or not to batch: After randomization, ESN could be used in batch mode. However,
there are no guarantees that such a solution solves the ”batch” Sifrian equations. Therefore, we
focus only on using spot or historical data.
6A σ-field is more appropriate, but We use only a random space Ω for the sake of simplicity.
8
Under review as a conference paper at ICLR 2022
4	Results
We validate the ESN method for FNNs, using a classification task on MNIST dataset (Lecun et al.,
1998), and also an autoencoder training with the CURVES dataset (Hinton & Salakhutdinov, 2006).
Leaky Relu and He initializer (He et al., 2016; Glorot & Bengio, 2010) are used for all the training.
Further implementation details are available in the Appendix (A.6). We report the results hereafter:
MNIST
CURVES
e)
1
0.8
00
1
0.8
f)
ssol gniniarT
102
Time (s)
104
01
10 -10
102	104
Time (s)
102	104
Time (s)
102	104
Time (s)
SGDMomentUm ADAM	ADAHessian ESN
Figure 2: Training and validation loss as a function of both epochs and wall-clock time for clas-
sification (MNIST) and autoencoding (CURVES) with one seed. For MNIST classification, the
architecture is {784, 256, 32, 10}. For CURVES, the autoencoder has the following architecture
{784, 216, 64, 6, 64, 216, 784}. Constant learning rates are used 0.001 for ADAM (Kingma & Ba,
2014) and ADAHessian (Yao et al., 2020) and 0.01 for SGD and ESN.
5	Conclusion
This paper showed that it is possible to derive an exact expression of the stochastic second-order up-
date for Deep Learning via a second-order Lagrangian which we called Sifrian, under some regular-
ization constraints. We revisited backpropagation using Lagrange multipliers method and developed
a new framework to better understand the mechanics of the Newton method at the stochastic level.
This framework yielded unexpected insights on how to correct the non-convexity, the importance
of the first layer, and how to get a closed-form solution. We further proceeded to randomize the
solution, to better align the training with its stochastic nature and avoid overfitting. We tested the
solution using simple training models; and despite the known limitations of the Newton method,
the ESN (Exact Stochastic Newton) solution was able to perform as well as adaptive methods in a
non-convex setting. ESN holds great potential but still requires further refinement for batch training,
parameters selection and estimators choice. Generalization of this work to other type of networks is
also a research venue which needs further investigation.
Reproducibility S tatement
The results of this paper are reproducible. Commented Matlab code is included in the supplementary
material. All experiments are run on a workstation with eight cores 2.6 GHz CPU.
9
Under review as a conference paper at ICLR 2022
References
Naman Agarwal, Brian Bullins, and Elad Hazan. Second-order stochastic optimization for machine
learning in linear time. The Journal ofMachine Learning Research, 18(1):4148-4187, 2017.
Alexander C Aitken. Iv.—on least squares and linear combination of observations. Proceedings of
the Royal Society of Edinburgh, 55:42-48, 1936.
Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251-
276, 1998.
Shun-ichi Amari, Jimmy Ba, Roger Grosse, Xuechen Li, Atsushi Nitanda, Taiji Suzuki, Denny
Wu, and Ji Xu. When does preconditioning help or hurt generalization? arXiv preprint
arXiv:2006.10732, 2020.
Rohan Anil, Vineet Gupta, Tomer Koren, Kevin Regan, and Yoram Singer. Scalable second order
optimization for deep learning. arXiv preprint arXiv:2002.09018, 2020.
Jimmy Ba, Roger Grosse, and James Martens. Distributed second-order optimization using
kronecker-factored approximations. 2016a.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016b.
David GT Barrett and Benoit Dherin. Implicit gradient regularization. arXiv preprint
arXiv:2009.11162, 2020.
Albert S Berahas, Majid Jahani, Peter Richtarik, and Martin Takac. QUasi-newton methods for deep
learning: Forget the past, just sample. arXiv preprint arXiv:1901.09997, 2019.
RaghU Bollapragada, Jorge Nocedal, Dheevatsa MUdigere, Hao-JUn Shi, and Ping Tak Peter Tang. A
progressive batching l-bfgs method for machine learning. In International Conference on Machine
Learning, pp. 620-629. PMLR, 2018.
Antoine Bordes, Leon Bottou, and Patrick Gallinari. Sgd-qn: Careful quasi-newton stochastic gra-
dient descent. Journal of Machine Learning Research, 10:1737-1754, 2009.
Aleksandar Botev, Hippolyt Ritter, and David Barber. Practical gauss-newton optimisation for deep
learning. In International Conference on Machine Learning, pp. 557-565. PMLR, 2017.
Leon Bottou and Yann Le Cun. On-line learning for very large data sets. Applied Stochastic models
in business and industry, 21(2):137-151, 2005.
Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. Siam Review, 60(2):223-311, 2018.
Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge uni-
versity press, 2004.
Charles George Broyden. The convergence of a class of double-rank minimization algorithms 1.
general considerations. IMA Journal of Applied Mathematics, 6(1):76-90, 1970.
R Byrd, Gillian M Chin, Will Neveitt, and Jorge Nocedal. On the use of stochastic hessian informa-
tion in unconstrained optimization. SIAM Journal on Optimization, 21(3):977-995, 2011.
Richard H Byrd, Samantha L Hansen, Jorge Nocedal, and Yoram Singer. A stochastic quasi-newton
method for large-scale optimization. SIAM Journal on Optimization, 26(2):1008-1031, 2016.
Camille Castera, Jerome Bolte, CedriC Fevotte, and Edouard Pauwels. An inertial newton algorithm
for deep learning. Journal of Machine Learning Research, 22(134):1-31, 2021.
Yann Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua
Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex op-
timization. arXiv preprint arXiv:1406.2572, 2014.
10
Under review as a conference paper at ICLR 2022
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of machine learning research, 12(7), 2011.
Roger Fletcher. A new approach to variable metric algorithms. The computer journal, 13(3):317-
322, 1970.
Thomas George, Cesar Laurent, Xavier Bouthillier, Nicolas Ballas, and Pascal Vincent. Fast
approximate natural gradient descent in a kronecker-factored eigenbasis. arXiv preprint
arXiv:1806.03884, 2018.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256. JMLR Workshop and Conference Proceedings, 2010.
Donald Goldfarb. A family of variable-metric methods derived by variational means. Mathematics
of computation, 24(109):23-26, 1970.
Donald Goldfarb, Yi Ren, and Achraf Bahamou. Practical quasi-newton methods for training deep
neural networks. arXiv preprint arXiv:2006.08877, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural
networks. science, 313(5786):504-507, 2006.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International conference on machine learning, pp. 448-456.
PMLR, 2015.
Takeaki Kariya and Hiroshi Kurata. Generalized least squares. John Wiley & Sons, 2004.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE, 86(11):2278-2324, 1998. doi: 10.1109/5.726791.
Yann LeCun, D Touresky, G Hinton, and T Sejnowski. A theoretical framework for back-
propagation. In Proceedings of the 1988 connectionist models summer school, volume 1, pp.
21-28, 1988.
Yann A LeCun, Leon Bottou, Genevieve B Orr, and Klaus-Robert Muller. Efficient backprop. In
Neural networks: Tricks of the trade, pp. 9-48. Springer, 2012.
Dong C Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization.
Mathematical programming, 45(1):503-528, 1989.
Donald W Marquardt. Generalized inverses, ridge regression, biased linear estimation, and nonlinear
estimation. Technometrics, 12(3):591-612, 1970.
James Martens. Deep learning via hessian-free optimization. In ICML, volume 27, pp. 735-742,
2010.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In International conference on machine learning, pp. 2408-2417. PMLR, 2015.
James Martens and Ilya Sutskever. Training deep and recurrent networks with hessian-free opti-
mization. In Neural networks: Tricks of the trade, pp. 479-535. Springer, 2012.
Si Yi Meng, Sharan Vaswani, Issam Hadj Laradji, Mark Schmidt, and Simon Lacoste-Julien. Fast
and furious convergence: Stochastic second order methods under interpolation. In International
Conference on Artificial Intelligence and Statistics, pp. 1375-1386. PMLR, 2020.
11
Under review as a conference paper at ICLR 2022
Jorge Nocedal and Stephen Wright. Numerical optimization. Springer Science & Business Media,
2006.
Barak A Pearlmutter. Fast exact multiplication by the hessian. Neural computation, 6(1):147-160,
1994.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. arXiv
preprint arXiv:1904.09237, 2019.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathemati-
cal statistics, pp. 400-407, 1951.
Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate
training of deep neural networks. Advances in neural information processing systems, 29:901-
909, 2016.
Tom Schaul, Sixin Zhang, and Yann LeCun. No more pesky learning rates. In International Con-
ference on Machine Learning, pp. 343-351. PMLR, 2013.
Nicol N Schraudolph. Fast curvature matrix-vector products for second-order gradient descent.
Neural computation, 14(7):1723-1738, 2002.
Nicol N Schraudolph, Jin Yu, and Simon Gunter. A stochastic quasi-newton method for online
convex optimization. In Artificial intelligence and statistics, pp. 436-443. PMLR, 2007.
David F Shanno. Conditioning of quasi-newton methods for function minimization. Mathematics
of computation, 24(111):647-656, 1970.
Samuel L Smith, Benoit Dherin, David GT Barrett, and Soham De. On the origin of implicit regu-
larization in stochastic gradient descent. arXiv preprint arXiv:2101.12176, 2021.
Jascha Sohl-Dickstein, Ben Poole, and Surya Ganguli. Fast large-scale optimization by unifying
stochastic gradient and quasi-newton methods. In International Conference on Machine Learning,
pp. 604-612. PMLR, 2014.
Peter Sunehag, Jochen Trumpf, SVN Vishwanathan, and Nicol Schraudolph. Variable metric
stochastic approximation theory. In Artificial Intelligence and Statistics, pp. 560-566. PMLR,
2009.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-
31, 2012.
Neha Wadia, Daniel Duckworth, Samuel S Schoenholz, Ethan Dyer, and Jascha Sohl-Dickstein.
Whitening and second order optimization both make information in the dataset unusable dur-
ing training, and can reduce or prevent generalization. In International Conference on Machine
Learning, pp. 10617-10629. PMLR, 2021.
Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on
computer vision (ECCV), pp. 3-19, 2018.
Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, and Michael W Ma-
honey. Adahessian: An adaptive second order optimizer for machine learning. arXiv preprint
arXiv:2006.00719, 2020.
12
Under review as a conference paper at ICLR 2022
A Appendix
A.1 C2 Sifrian equations
The inner product used in the Sifrian is either the inner product of vectors or the trace product for
matrices, which are equivalent after vectorization:
(A, B) = trace (BTA) = vec (B)T vec (A),
〈x, y)= xτy = trace (XyT).
The derivation is carried through perturbation factorization for e.g.:
S(Wk + ≡k)-S (Wk)〜(∂W ,δw).
We carry the derivation and we report hereafter the full Sifrian equations for a C2 activation func-
tions:
⑴-P”Fkp)
t + 1k=2..nb(P)必T)
k ljk
:巴 + ηk- 1k=2..nWkZk-1] Xkp)T
PP VKP) b(5T,
+ ηk - 1k=2..nWkZ(P)Ii - PPVFkP)
+
0,
⑶ Y(P)- k'.n-lW+IVFkTYM + Pm dxf^Zmb
+ 1k=ι..nT<uVFk2bk% = 0,
⑷ Z(P)- 1k=2..nVF(P)W忒也 + VKP) [NkX巴+ ηk] = 0.
A.2 ESN RESOLUTION FOR C2 ACTIVATION FUNCTIONS
The stochastic case for C2 functions has the same type of simplifications. Comparing the first and
second equations:
⑴-PPVFkP) (YkP)XMT + 1k=2..nbkPdp)T)
k ljk
：k* + IIk- 1k=2..nWkZ牝]Xk"T
PP VFkP)bkP)XkMT,
巴 + ηk - 1k=2..n WkZkPy - PPVFkP)
+
0,
13
Under review as a conference paper at ICLR 2022
This yields that 1k=1..n-1ζp(k) = 0. The fourth equation state that Nkx(kp-)1 + ηk = 0 is always
null except for the last layer. Let’s focus on the last layer. Three equations allows to characterize it:
'(2) bɑp) = (VFnp))T V2Fnp)b(p) [Nnx2-1 + η/ - YF,
<(3)Ynp)+y⅛p) Znp)=0,
∂χn，dxn，
.(4) ζnp) + VFnp) hNnxn-1 + ηni = 0.
If we assume that the C 2 has a non null second derivative everywhere then, the previous three equa-
tions yield a specific value of Nnx(np-) 1 + ηn = 0 and also ζn(p) :
Except the last layer we have: Yk(p) = -b(kp), and a similar system to the piecewise affine case could
be derived.
A.3 Example: a simple regularization
In this section we give a concrete simple example to further explain the ESN method. In the case of
an l2 cost function and regularization defined by:
R(X)=2 X x Dχkp), χkp)E.
p∈D k=1..n-1
we have the following solution:
(N	PF (akp))bkP)Xkp)T
Nk =- WpW,
<
ηk = 1k=n [vf (an))i	(d(p) - Xn)) - Nk xkp)ι.
•	The previous solution is straightforward to compute; the first order backpropagation pro-
vides all the elements.
•	It is co-linear with the gradient except for the bias of the last layer.
•	The adjoints b(kp) ) are impacted by choice of the regularization parameter λ.
•	The minimal rank Newton solution is nota descent direction. Most of the biases correspond
to an ascent direction.
•	The squared norm division is problematic and will lead to divergent solution if the norm
of the backpropagated errors is very small i.e.	VF a(kp) b(kp)	→ 0. A sufficient
condition to avoid divergence consists in defining layer by layer regularization such as:
λk-ι MlVF (akp)) bk1∣.
•	It is possible to remove the regularization i.e. λ → 0, this yields a ”granular” solution
which states that the second order optimal solution consists in just adjusting the last layer
bias with a slope corrected error: VF a(np)	d(p) - x(np) . The granular solution is
not suitable for neural network training, as it overfits every sample.
A.4 Proof: proposition Randomized ESN
We start from the randomized least-squares criteria.
argmin{N,η}
/
√Ω=D
dΩ (P)
T
∂⅛!+ ykp)T Nk
2
+ lllNkx(kp-)1 +ηk lll22 .
2
14
Under review as a conference paper at ICLR 2022
The optimum is a critical point i.e. :
∀k ∂Nk
(0,0).
Each layer could be optimized separately which is desirable feature that reduce the dimensionality
of the problem. We get the following derivatives:
∂NNk → R "ω
T +yk(p)y(kp)TNk+Nkx(kp-)1x(kp-)1T +ηkx(kp-)1T	=0,
、∂ηk → Rdω (NkXk-I+ n，= 0
Integration over Ω yields expectations and two sets of equations:
T
NkE x(kp-)1 +ηk =0.
We simplify the system by right multiplying the second equation with E x(kp-)1 and substracting,we
get:
曲)T
k-1
Nk E [xk1-ι] + ηk = 0.
The above expression corresponds toa Sylvester equation. The last two equations are the main result
of the proposition.
A.5 Invariance of ESN to normalisation:
In this section we demonstrate that the ESN is invariant to normalisation which would allow for a
simpler resolution of the Sylvester Equation. Such a result is expected for a Newton method, we
do the demonstration step by step. First let’s start by reformulating the forward problem with the
normalized state variable s:
s(kp)
Var
The original state variable is an affine transformation of the normalized one:
X(kp) = Σks(kp) + mk .
The matrix Σk is symmetric and positive. We assume further that it is definite i.e. invertible in this
context. The forward model becomes:
Σks(kp) + mk - F Wk Σk-1s(kp-)1 + mk-1 + βk .
The square-root of the variance and the expectation could be absorbed into the weights/biases:
Wk0 = WkΣk-1,
Iek = βk + Wk mk-1.
With the above notations, the forward model becomes:
Σks(kp) +mk = F Wk0 s(kp-)1 +
15
Under review as a conference paper at ICLR 2022
We use a new adjoint u. The Lagrangian becomes:
L(x, W,β, b) = 'r + X X D∑kSkp) + mk- F (Wksk-i + βk) , Ukp)E ∙
p∈D k=1..n
The derivative w.r.t. S(kp) should be null, which yields the following backpropagation equation.
-τpy = EkUkp)- 1k=ι..n-ιwk+ιTPF (akp+ι) ukp+ι + Ek J^Ry = 0.
∂Sk	∂xk
Given the definition of Wk0 +1 . The adjoint equation is the same as the original, hence u = b, and
the first order adjoint is unmodified. The gradient is slightly modified:
IGk = ∂W = - Pp PF (akp)) Ukp)Sk-T,
[gk = ∂∂L = - Pp VF (akp)) ukp).
Let’s move to the Sifrian:
S(S,U,W0,β0,G0,g0,γ0,ζ0,N0,η0) = Pk,p DEkS(kp) +mk - F (Wk0 S(kp-)1 +β0k) ,γ0k(p)E
+ (∑kUkp)- 1k=ι..n-ιW0+ιτVF (akp+ι)谥% +	4A
+ Pk DG0k + Pp PF (a(kp)) U(kp)S(kp-)1T, Nk0 E
+ Dgk0 +PpVF (a(kp)) U(kp),η0kE .
The Sifrian equations are:
(1)	Pp VFk(p) (γk0 (p)S(kp-)T1 +U(kp)S(kp-)T1 + 1k=2..nU(kp)ζk0 -1(p)T ) =0,
(2)	Pp VFk(p) (γk0 (p) + U(kp)) =0,
(3)	Σk Yk (P)- 1k<nW0+ιT VFk+½k + ι(p) + Pm EkdffdRp EmZm (P) + 4N + J VF(+)i U^1 = 0,
xk xm
(4)Ekζk0(p)-1k=2..nVFk(p)Wk0ζk(p-)1+VFk(p) (Nk0 S(kp-)1 + ηk0 ) =0.
We follow the same steps of the ESN Theorem demonstration, add the white layer and we remove
the granular solution. We finally get the following stochastic system:
[(∂χR))T Ek + (VF (akp+ι) ukp+ι)T Nk+1 = 0,
INk Sk-I + ηk = 0.
The randomization yields,
E [ykp)ykP)Ti Nk + NkVar (Sk-I) = -E ykp) (d⅛)T ∑,
NkE卜kp-1i + ηk0 = 0
16
Under review as a conference paper at ICLR 2022
The s is normalized hence:
JVar (Sk-I)= I,
[e [sk-J =0.
The randomized, ESN with the normalized state variable has the following solution:
fNk = -E hI + ykp)ykP)TiTE "ykp) (d⅛)T# 期，
lη,k = 0
We revert to the original variables from Equ. A.5, add convexity correction and we get the following
solution:
Nk = -E hI + ykP)ykP)TiTE "ykp) (章)J
ηk = NkE x(kp-)1
A.6 Implementation details:
In this section we specify technical details of our implementation. First, we remove the ”granular”
terms from our solution, since they do overfit each sample by construction.:
Nk = -E hI + ykP)ykP)TiTE Pp) (M) J
ηk = NkE hx(kP-)1i
ESN is solved for each layer independently, which means that the inverted matrices are a scale
smaller than the empirical Fisher matrix for e.g.. To further illustrate this point, let’s consider,
for e.g., the feedforward network with layers sizes [784, 256, 32, 10] (MNIST classification): the
empirical Fisher matrix would have a size of 210298x210298, while the biggest matrix inver-
sion for ESN would be only 256x256. In practice, the computation requires initially two ex-
pectations E
T
d'R J
∂X(p)7
k-1
, E x(kP-)1
which could be estimated using exponentially de-
i
caying moving averages (Similar to ADAM). The product with the non-centered precision matrix
E hI + y(kP)yk(P)Ti	, could be approximated in several ways. We list hereafter at least three meth-
ods:
•	Direct method: the non-centered covariance matrix could be estimated using an exponen-
tially decaying moving average and inverted directly given the small sizes.
•	Recursive Least-Squares (RLS): A sliding window could be used to update recursively the
estimation of N(k) using Sherman-Morrison formula with an initialization of the precision
matrix at I. Such a method is a classic of online learning and is similar to Kalman filtering.
•	Rank one method: instead of involving the historical data, the simplest/crudest estimation
of the covariance matrix is based on the spot/instantaneous information. The precision
matrix could be derived using Sherman-Morrison formula (applied once):
17
Under review as a conference paper at ICLR 2022
Probability distribution/weights: It is common to use weights in the context of least-squares
regression. For Deep Learning, we could increase the importance of samples that are not well
assimilated into the network, i.e., high errors(or vice-versa). This is possible for e.g. by choosing
ProbabiIitieS/weights proportional to errors, norms: W(P)〜IXnp) - d(p)∣∣ . The ESN regression
equations become:
Nk = -E hw(p)I + w(p)yk(p)yk(p)Ti-1 E
w(p)yk(p)
Ehw(p)x(p) i
η = Nk F)-II
Effects of the admissible regularization: We stated in this paper that the admissible regularisa-
tion R(X) reduces the norm of the state variable X. To further illustrate this statistical effect, we will
use MNIST classification as an example, with regularized and un-regularized (normal) stochastic
gradient descent method. For the sake of simplicity we consider the following form of regularisa-
R(X) = I X X λ DXkp), Xkp)E .
p∈D k=1..n-1
The architecture of the FNN is the same from the experimental section i.e. {784, 256, 32, 10}. We
select a learning rate of μ = 0.001 and various values of λ = {0,10-2,10-3,10-4,10-5 }.
Norm of the state variable x
0.5
0.45
0.4
0.35
Training Errors
——λ=0
----入=0.00001
λ =0.0001
----λ =0.001
λ=0.01
150
148
146
144
——λ=0
----λ =0.00001
A =0.0001
----A =0.001
A =0.01
SMRx
3 52
0. .2 0.
0
srorrE gniniarT
0.15 - ∖ ∖
0.ι	χj∕g=⅞⅛.,^
0.05 -	- ^Ua-k√ ■ "-ΛjL⅞¾g
0∣---1---1---1---1---1
012345
Epoch
Figure 3: Effect of the admissible regularization on the norm of the state variable P x(kp) for
different values of λ = {0,10-2,10-3,10-4,10-5}. SGD is used as the training method. The
higher values of λ reduce further the norm of the state variable x. Training cost is reported in the
right side of the figure.
18