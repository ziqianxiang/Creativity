Under review as a conference paper at ICLR 2022
Only tails matter: Average-Case Universal-
ity and Robustness in the Convex Regime
Anonymous authors
Paper under double-blind review
Abstract
Recent works have studied the average convergence properties of first-order
optimization methods on distributions of quadratic problems. The average-
case framework allows a more fine-grained and representative analysis of
convergence than usual worst-case results, in exchange for a more precise
hypothesis over the data generating process, namely assuming knowledge of
the expected spectral distribution (e.s.d) of the random matrix associated
with the problem. This work shows that a problem’s asymptotic average
complexity is determined by the concentration of eigenvalues near the edges
of the e.s.d. We argue that having a priori information on this concentra-
tion is a more grounded assumption than complete knowledge of the e.s.d.
basing our analysis on the approximate concentration is effectively a middle
ground between the coarseness of the worst-case scenario convergence and
the restrictive previous average-case analysis. We introduce the Generalized
Chebyshev method, asymptotically optimal under a hypothesis on this con-
centration, and globally optimal when the e.s.d. follows a Beta distribution.
We compare its performance to classical optimization algorithms, such as
Gradient Descent or Nesterov’s scheme, and we show that, asymptotically,
Nesterov’s method is universally nearly optimal in the average-case.
1	Introduction
The analysis of the average complexity of algorithms has a long story in computer science.
Average-case complexity, for instance, drives much of the decisions made in cryptography
(Bogdanov & Trevisan, 2006).
Despite their relevance, average-case analyses are difficult to extend to other algorithms,
partly because of the intrinsic issue of defining a typical distribution over problem instances.
Recently though, Pedregosa & Scieur (2020) derived a framework to systemically evaluate the
complexity of first-order methods when applied on distributions of quadratic minimization
problems. This is done by relating the average-case convergence rate to the expected spectral
distribution (e.s.d) of the objective function’s Hessian, which is a well-studied object on
random matrix theory. Having access to this object in practice is a much stronger hypothesis
when compared to the worst-case analysis that relies only on the values of the edges of this
distribution.
Paquette et al. (2020) extended the average-case framework by introducing a noisy generative
model for the problems. They further derived the average complexity of the Nesterov
Accelerated Method (Nesterov, 2003) on a particular distribution. They showed the strong
concentration of the metrics around a limiting value as dimensions go to infinity.
Scieur & Pedregosa (2020) showed that for a strongly convex problem with eigenvalues sup-
ported on a contiguous interval, the optimal average-case complexity converges asymptotically
to the one given by the Polyak Heavy Ball method (Polyak, 1964) in the worst-case.
1.1	Current limitations of the average-case analysis
When analyzing the state of the art of average-case methods on quadratics problems,
we observe significant limitations that we address in this paper. First, little is known
1
Under review as a conference paper at ICLR 2022
about the convergence rate on convex problems. Also, optimal average-case algorithms
require an exact estimation of the e.s.d to guarantee an optimal convergence rate, their
convergence rate under inexact e.s.d. is not known. Finally, the non-smooth is also
discussed in (Pedregosa & Scieur, 2020), but with little details.
Convex problems. The minimization of non-strongly convex problems is drastically slower
than their strongly convex counterpart, as Gradient Descent presents worst-case convergence
in Θ(t) and Nesterov is Θ(表).In the strongly convex case, both the worst-case and average-
case are asymptotically equal. However, little is known on optimal average-case rates for
convex problems, as well as the average-case complexity of classical methods such as gradient
descent or Nesterov’s method, see (Paquette et al., 2020).
Exact estimation of the e.s.d. In (Pedregosa & Scieur, 2020), the theoretical study of
optimal algorithms in the average-case requires an exact estimation of the e.s.d. of the
problem class. Such estimation may be hard, nor impossible to obtain in practical scenarios.
Despite showing good performance when the e.s.d. is estimated with empirical quantities,
there are no theoretical guarantees on the performance of the method when the e.s.d. is
poorly estimated. There is therefore a need to analyze the algorithm’s performance under
different notions of uncertainty on the spectrum. This allows a practitioner to choose the
best algorithm for a practical problem, even with imperfect a priori information.
Non-smooth. Pedregosa & Scieur (2020) briefly introduce average-case optimal rates on
non-smooth problems, when the e.s.d. is the Laguerre distribution e-λ. In this paper, we
extend the analysis to the generalized Laguerre distribution λαe-λ, α > -1.
1.2	Contributions
Our main contribution is a fine-grained analysis of the average-case complexity on convex
quadratic problems: we show that a problem’s complexity depends on the concentration
of the eigenvalues of e.s.d. around the edges of their support. From this perspective, we
propose a family of optimal algorithms in the average-case, analyze their robustness,
and finally exhibit a universality result for Nesterov’s method. More precisely,
•	(Optimal algorithms). In Section 3, we propose the Generalized Chebyshev Method
(GCM, Algorithm 1), a family of algorithms whose parameters depend on the concentration
of the e.s.d. around the edges of their support. If the parameters of the GCM method
are set properly, the algorithm converges at an optimal average-case rate (Theorem 3 for
smooth problems, Theorem 6 for non-smooth problems), a rate that we show is faster
than worst-case optimal methods like Nesterov acceleration. We show these rates to be
representative of the practical performance of the algorithms in Fig. 6, and retrieve the
classical worst-case rates as limits of the average-case (see Table 1).
•	(Robustness). Developing an optimal algorithm requires the knowledge of the exact
e.s.d. However, in practical scenarios, we only have access to an approximation of the
e.s.d. In Theorem 2 in Section 4 we analyze the rate of GCM in the presence of such a
mismatch. We also analyze the optimal average-case rates of distributions representing
the smooth convex, non-smooth convex, and strongly convex settings and compare them
with the worst-case rates (Table 1).
•	(Universality). Finally, in Theorem 4, we analyze the asymptotic average-case conver-
gence rate of Nesterov’s method. We show that its convergence rate is nearly optimal
(up to a logarithmic factor) under some natural assumptions over the data, namely a
concentration of eigenvalues around 0 similar to the Marchenko-Pastur measure. This
contributes to the theoretical understanding of the numerical efficiency of Nesterov’s
acceleration.
2	Average-Case Analysis
In this section, we recall the average-case analysis framework for random quadratic problems.
The main result is Theorem 1, which relates the expected error to the expected spectral
2
Under review as a conference paper at ICLR 2022
Figure 1: Representation of different
spectra with different concentrations of
eigenvalues around the edges of the sup-
port. The average-case rates for non-
strongly problems are determined by
these concentrations
Regime	Worst-case	Average-Case
Strongly conv.	(1 - Θ(1∕√κ))t	(1 - Θ(1∕√κ))t
Smooth conv.	1∕t2	1∕t2ξ+4
Convex	1∕√t	1∕tα + 2
Table 1: Comparison between function value worst-
case and average-case convergence. κ is the condi-
tion number in the smooth strongly convex case. In
the smooth convex case ξ > -1 is the concentration
of eigenvalues around 0 (see Assumption 1) and in
the non-smooth case We consider dμ α λαe-λ
min nf (x) := 1 (X — x?)>H(X — x?)0 .
x∈Rd	2
distribution and the residual polynomial. The one-to-one correspondence between the residual
polynomials and first-order methods applied to quadratics Will alloW us to pose the problem
of finding an optimal method as the best approximation problem in the space of polynomials.
We define a random quadratic problem:
Problem 1. Let H ∈ Rd×d be a random symmetric positive-definite matrix independent
to x? ∈ Rd, a random vector that is the solution to the problem. We define the random
quadratic minimization problem as
(OPT)
We are interested on minimizing the expected errors Ekf(xt) - f(x?)k, the expected function-
value gap, and E||Vf (xt)||2, the expected gradient norm, where Xt is the t-th update of a
first-order method starting from x0 and E is the expectation over the random variables H , x0
and x? .
The expectation We consider is over the problem and not over any randomness of the
algorithm.
In this paper, We consider the class of first-order methods (F.O.M’s) to minimize (OPT).
Methods in this class construct the iterates xt as
Xt ∈ xo + span{Vf (xo),..., Vf (xt-ι)} .	(1)
That is, xt belongs to the span of previous gradients. This class of algorithms includes
for instance gradient descent and momentum, but not quasi-NeWton methods since the
preconditioner could alloW the iterates to go outside of the span. Furthermore, We Will only
consider oblivious methods, that is, methods in Which the coefficients of the update are
knoWn in advance and do not depend on previous updates. This leaves out some methods
such as conjugate gradient or methods With line-search.
From First-Order Method to Polynomials. There is an intimate link betWeen first-
order methods and polynomials that simplifies the analysis of quadratic objectives. The
next proposition shoWs that, With this link, We can assign to each optimization method
a polynomial that determines its convergence. FolloWing Fischer (1996), We Will say a
polynomial Pt is residual if Pt(0) = 1.
Proposition 1. (Hestenes et al., 1952) Let Xt be generated by a first-order method. Then
there exists a residual polynomial Pt of degree t, that verifies
Xt - X? = Pt(H)(X0 - X?) .	(2)
3
Under review as a conference paper at ICLR 2022
Remark 1. If the first-order method is further a momentum method, i.e.
xt+ι = Xt + htVf(Xt) + mt(xt - xt-i).
We can determine the polynomials by the recurrence P0 = 1 and
Pt+1(λ) =Pt(λ)+htλPt(λ)+mt(Pt(λ) -Pt-1(λ)).
We note that while most popular F.O.M’s can be posed as a momentum method, the Nesterov
method cannot.
A convenient way to collect statistics on the spectrum of a matrix is through its empirical
spectral distribution.
Definition 1 (Expected spectral distribution (e.s.d)). . Let H be a random matrix with
eigenvalues {λι,..., λd}. The empirical spectral distribution of H, called μH, is the
probability measure
μH := d∑d=ιδλi,	⑶
where δλi is the Dirac delta, a distribution equal to zero everywhere except at λi and whose
integral over the entire real line is equal to one.
Since H is random, the empirical spectral distribution μH is a random variable in the space
of measures. Its expectation over H is called the expected spectral distribution and we
denote it
μ := EH [μH].	⑷
We can link the e.s.d. of H to the convergence of a first-order method on the distribution
of H. In the following we will consider X0 - X? and H to be independent, with X0 - X?
sampled isotropically.
Theorem 1. Let Xt be generated by a first-order method associated to the polynomial Pt ,
the measure μ the e.s.d. of H, and E[(xo — x*)(x0 - x?)T] = R2I for some constant R.
Then we can write the convergence metrics at time step t as
E[kXt-X?k2]
R2/ Pt2(λ)dμ(λ),
E[f(Xt) - f(X?)]
R2 / Pt2(λ)λdμ(λ)
and
E[∣∣Vf(xt)∣∣2] = R2 / Pt2(λ)λ2dμ(λ).
(5)
This shows that polynomials are a powerful abstraction as they allow us to write all of our
convergence metrics within the same framework . For simplicity, we set R2 = 1 and we will
refer directly to the polynomials associated to a given method. We will refer to objective l
as the one associated to the added λl term, i.e. the function-value is ob jective l = 1.
This framework is linked to the field of orthogonal polynomials by the next proposition.
We construct an optimal method w.r.t. a given distribution through a family of orthogonal
polynomials associated to it.
Proposition 2 ((Pedregosa & Scieur, 2020)). Let Ptl be defined as
Ptl :
arg minPt(0)=1	Pt2 (λ)λldν(λ).
(6)
Then (Ptl) is the family of residual orthogonal polynomials w.r.t. to λl+1dν.
This theorem further implies that the optimal first-order method is a momentum method as
Favard's theorem Marcellan & AlvareZ-Nodarse (2001) tells Us the orthogonal polynomials
w.r.t. a given distribution are related through a three term recurrence,
Pt+1 (λ) = atPt(λ) + btλPt(λ) + (1 - at)Pt-1 (λ).	(7)
Following Remark 1, the optimal method is derived from this recurrence as
Xt+1 = Xt + (at - 1)(Xt - Xt-1) + btVf(Xt) .	(8)
4
Under review as a conference paper at ICLR 2022
3 Methods
Being able to write the rates in terms of the expected spectral distribution ties the average-case
framework to the field of random matrix theory. Indeed, because of results from this field,
certain e.s.d’s are considered more natural than others. Indeed, it can be shown that the
same distribution arises when we take the gram matrix of random centered i.i.d. features
with variance σ2 : the Marchenko Pastur distribution.
Definition 2 (MP distribution). The Marchenko Pastur distribution associated with the
parameter r and with scale σ2 is given by
dμMP (λ)
1 ∙∖∕(λ+ - λ)(λ - λ-)
2πσ2	rλ
(9)
with λ+ = σ2(1 + √r)2, λ- = σ2 max(0, (1 — √r)2).
The Marchenko Pastur distribution μMp can be considered a natural first model for e.s.d's
as it arises universally from matrices with i.i.d. entries,under mild low moment assumptions,
there is no specific distribution of the matrix to be considered. It can be seen as a model for
the white-noise in the data. When r = 1, i.e. n = d, we have dμMp H λ-1/2√λ+ — λ.
Pedregosa & Scieur (2020) first derived the optimal method w.r.t. μMp, and Paquette et al.
(2020) derived Nesterov’s rates under the distribution. As we are concerned with being
robust, a natural step is to consider the Beta weights.
Definition 3. The (generalized) Beta weights with parameters τ, ξ and scale L are given by
the (non-normalized) pdf
dμ(λ) = λξ (L — λ)τ.	(10)
This family of distribution generalizes the MP distribution, and both have similar concentra-
tions near 0 when ξ ≈ —1/2.
The optimal method w.r.t. μ and objective l is associated to a shifted Jacobi polynomial
Pae with β = ξ + l + 1,α = T. When α = β = —1/2, We retrieve the Chebyshev Method
(Flanders & Shortley, 1950). As such, we name our proposed methods the Generalized
Chebyshev Method (GCM).
Algorithm 1: GCM(α, β)
Inputs: Initial vector x0, function f, smoothness parameter estimate L
X-1《-0, δo《—0
for t = 1, . . . , T do
2 (β2 +αβ+(2t+I)(a+e)+2t2 +2)(2t+α+β+1)
°t《-	2(t+1)(t+α+β+1)(2t+α+β)
(	,_ (2t+a+e+1)(2t+a+e+2)
bt λ	L(t+1)(t+α+β+1)-
〜√_	(t+a)(t+e)(2t+a+e+2)
Yt L	(t+l)(t+α+β+1)(2t+α+β)
δt L——-ɪ—
t	at +γtδt-1
xt * L xt-1 + (δtat — I)(Xt-I — xt-2) + δtbNf (xt-1)
We’ll consider the Nesterov’s method used in Paquette et al. (2020), which is defined by the
iterations:
xt+1 = y — 7 Vf (yt)	(H)
L
yt+ι = χt+ι +1+3 (Xt+ι — Xt)	(12)
α -x
We also consider the Laguerre method, which is optimal w.r.t. dμ(x) = Γ(J+i), taking a as
a parameter. This method is proposed to optimize non-smooth functions.
Both these methods are generalizations of ones that have been proposed in Pedregosa &
5
Under review as a conference paper at ICLR 2022
Scieur (2020). We show that Algorithm 1 corresponded to polynomials Ptae and derive the
Laguerre method in appendix B.
Remark 2. The Generalized Chebyshev takes the largest eigenvalue L as a parameter, but
the rates we will show are robust to an overestimation of L.
4 Robust Average-Case Rates
We will state our assumption over the spectral distributions. It effectively allows us to
parametrize all of our distributions of interest in a way that characterizes the asymptotic
convergence, diving them into equivalence classes.
Assumption 1. We will write ντ,ξ for a continuous distribution supported in (0, L] s.t.
ντ0 ,ξ (x) > 0 for x ∈ [0, L], dντ,ξ = Θ(λξ) near 0 and dντ,ξ = Θ((L - λ)τ ) near L.
Assumption 1 is quite nonrestrictive, in that, the spectral distribution of the Hessian for
any smooth convex problem can be identified with some τ, ξ in this class. It is a milder
assumption than (1) assuming complete knowledge of the spectrum of the Hessian or (2)
the specific distribution on the entries of your data. Moreover the assumption encompasses
the frequently used MP (e.g. Martin & Mahoney (2021); Pennington & Bahri (2017)) and
Uniform distributions We note there’s no need to consider eigenvalues situated at 0 as they
do not contribute to the optimization process.
The ξ works as a measure of how close we are to the worst-case scenario, as it approaches
-1. Samples in finite dimension of distributions with high values of ξ will work as strongly
convex functions in practice.
We show that ντ,ξ indeed behaves like an equivalence class when considering the asymptotics
of the convergence of the methods: only the concentrations near the edge matter. We do this
by singling out from each of these classes the beta distributions for which we can compute
the rates, then show the rates to be the same inside ντ,ξ .
Theorem 2 (GCM average-case rates). A Generalized Chebyshev Method with parameters
(α, β) applied to a problem with e.s.d. as in Assumption 1 has average-case rates
(t-1-2β	if α<τ +1/2 and β<ξ + 3/2
E[f(xt) - f (x?)] ~ L ∙ C[V t-2(ξ+2) log t	if α = T + 1/2 and β = ξ + 3/2 ,
，[t2(maχ{t-β-τ,-ξ-1}-1) if a>τ + 1/2 or β>ξ + 3/2
(13)
( t-1-2β	if α < τ + 1/2 and β < ξ + 5/2
E[∣∣Vf(xt)ll2]〜L2 ∙ C2八 t-2(ξ+3) logt	if α = τ +1/2 and β = ξ + 5/2 ,
[t2(maχ{t-β-τ,-ξ-2}-1) if a>τ + 1/2 or β > ξ + 5/2
(14)
where Cνα,β is a distribution dependent constant.
Theorem 2, which is illustrated by fig. 2 shows that overestimating β, and underestimating α
will still leave us with the optimal asymptotic rates, so a good rule of thumb for calibrating
the algorithm is to use high β and low α.
Theorem 3 shows that a proper choice of α, β can indeed make the Jacobi polynomial
asymptotically optimal w.r.t. to any ντ,ξ.
Theorem 3 (Optimal Rates). Let ν follow Assumption 1. The optimal asymptotic average-
case rates for E[f (xt) 一 f (x?)] and E[||Vf (xt)∣∣2] are attained by the GCM with parameters
(τ, ξ + 2) and (τ, ξ + 3), respectively, and read
E[f(xt) - f(x?)] = Θ(t-2(ξ+2)),	E[||Vf(xt)||22] = Θ(t-2(ξ+3)).
For the function value (l = 1), we find rates that approach t-2 as ξ → -1, showing the
worst-case as a limit (over the considered distribution) on the average-case.
6
Under review as a conference paper at ICLR 2022
Figure 3 & Table 1: The figure illustrates the robustness of the Generalized Chebyshev Method
with parameters (α, β) for a fixed problem corresponding to the Marchenko-Pastur distribution
(T = 2, ξ = -2). The color represents the exponent a of the average-case rate O(ta) of the method
for different values of α and β . The white star represents the optimal tuning and the blue area is the
set of parameters for which the method converges. Note we have a large of region that guarantee
the same optimal asymptotic rate. The table compares the asymptotic average-case rates for the
function-value for different methods with different (τ, ξ) values.
Method		Parameters (τ, ξ)	
		(2, 2)	(2,- 2)
GCM(α = 2 ,β =	5) 2 )	t-5	t-3
GCM(α = 2 ,β =	3) 2 )	t-4		t-3	
Nesterov		t-4	t-3 log t
Gradient Descent			-5	 t F		-3	 t F
We remark that the above theorems imply that, at least asymptotically, the GCM is robust
for a suboptimal choice of parameter β up to 1/2 below the optimal choice and infinitely
above.
For completeness, we also derive worst-case rates for the GCM:
Proposition 3 (GCM worst-case rates). Let f be a convex, L-smooth quadratic function.
Then, for the Generalized Chebyshev Method with parameters (α, β), we have worst-case rates
f(xt) - f (x?) ≤ Cl L <	t2(α-β) t-1-2β, 1t-2, t2(α-β)	if α > β - 1			(15)
		if if	α≤β-1 α≤β-1 if α > β	β ≤ 2 , β ≥ 2 -2	
皿(Xt) - f(x?)∣∣≤ C2L2 <	t-1-2β,	if	α≤β-2	β ≤ 3/2 .	(16)
	t-4,	if	α≤β-2	β ≥ 3/2	
For a reasonable choice of α,β, i.e. β ≥ 1, α ≤ β - 1. the function value achieves the
theoretical lower bound of t-2 .
We now analyze the convergence of the Nesterov method. Nesterov (2003) has shown that
it matches up to a constant factor a lower bound on the worst-case complexity of non
strongly convex problems. A natural question is if this performance would translate to good
average-case rates. To do so, we will extend Paquette et al. (2020) proof for the Nesterov
method rates under the MP distribution.
Theorem 4	(Nesterov average-case rates). Let ν as in Assumption 1. Then for the Nesterov
method, we have average-case rates
t-2(ξ+2)
Ef(Xt) -G 〜C1 ,νn t-3ξlog∕2)
ifξ< -1/2	,
if ξ = -1/2 ,	E[∣∣V∕(xt)||2]~ C2,νt-(ξ+9/2). (17)
if ξ> -1/2
The difference between the asymptotic average-case rates of Nesterov and the optimal ones
are tξ+l-1/2, when ξ + l > 1/2, log t when ξ + l = 1/2 and 0 otherwise. This shows that
Nesterov is almost optimal when the concentrations near 0 are relatively high, i.e. low ξ .
Theorem 5	(Gradient Descent average-case rates). Let ν as in Assumption 1. Then for
gradient descent
Ef(Xt) - f (x?)] = Θ(t-(ξ+2)),	E[∣∣V∕ (xt )||2 ] = Θ(t-(ξ+3)).	(18)
7
Under review as a conference paper at ICLR 2022
Figure 4: Above: Empirical spectrum for the covariance matrix of the features. Below:
Gradient norms throughout iterations. Left: CIFAR-10 Inception features Right: MNIST
features. Here We choose to compare gradient norms as the minimum function value is not
known. The properly tuned GCM achieves remarkable performance under these non-synthetic
spectrum,s.
Observe for the function value that the rate for Nesterov is t-2 and the rate for Gradient
Descent is t-1 when ξ — — 1.
Lastly, we consider the optimal rates for a Gamma distribution.
Theorem 6 (Laguerre method rates). Let a > —1 and μ., a > —1 be a Gamma distribution,
α - X
i.e. dμα(x) = Γ(O+i) ∙ The optimal rates are given by the Laguerre method of appropriate
tuning and
Eff (xt) — f (x?)] =Θ(t-(α+2)).	(19)
Note that this result does not have the same universality as the others because of the
non-compacity of the distribution,s support. These rates are contrasted to the worst-case
lower bound on the optimization of non-smooth functions by first-order methods, which gives
............. C
f(xk) — f(x ) ≥ √t.
These rates are not found when α → —1, indicating that the worst-case is especially
pessimistic in this scenario.
Remark 3. All of the expected rates We state are almost deterministic on the high dimensional
setting as per the concentration results shown in Paquette et al. (2020)
5	Experiments
We simulate the e.s.d’s in two ways. The Marchenko Pastur distribution, which we sample
by taking H = XXT where X has i.i.d. gaussian samples. This enables us to simulate
(τ, ξ) values of (1/2, —1/2). Other values of (τ, ξ) are simulated by sampling A ∈ Rd from the
corresponding Beta distribution and taking H = U diag(A)Ut, where U is an independently
sampled orthonormal matrix.
We let x? = 0 and sample xo from a centered Gaussian distribution, the dynamics are
the same as in the general case. In all experiments we use the problem,s instance largest
eigenvalue to calibrate each method (e.g. Gradient Descent,s stepsize is 1/L). Our theoretical
rates in Theorem 5 and Theorem 4 respectively for the Nesterov method and Gradient Descent
are precise under the approximate range —1 < ξ < 0 as we show in Figure 6. Distributions
with higher ξ need many samples otherwise they behave as strongly convex functions.
The same is not true for the Generalized Chebyshev Method. If β < β? or ξ is low the
empirical findings diverge from the theoretical. We believe this is due to numerical instability
8
Under review as a conference paper at ICLR 2022
Figure 5: Rates for a synthetic problem, simulating the Marchenko Pastur distribution. Note
that both tunings of the GCM achieve performance in function value very close to the one of
Conjugate Gradient, which is optimal for every draw of the problem.
					
					
			GCM( 1. 3)			
		∖2 2 GCM(1,5) 	CG ——GD 	NesteroV				
					
under these regimes as the metrics also have much larger variance than in the other regimes.
We’ve not been able though to pinpoint the exact source of this supposed instability. This is
shown in appendix D.
The GCM with β > β? performs corresponding to the theory, and it’s non-asymptotically
very close to the performance of β? . High values of β also perform very well on non-synthetic
data, suggesting in practice we should use these values.
Figure 6: Comparison between experiments run on synthetic Beta distribution and theoretical
asymptotic. Y-axis is the function value
6	Conclusion and further work
In this paper, we’ve established that the assymptotic convergence of first order methods
on quadratic problems in the convex regime depend on the concentration of the Hessian’s
eigenvalue near the edges of the spectrum’s support. We further contributed to the theoretic
understanding of the Nesterov’s method performance and established the contrast between
the worst-case and average-case in the main regimes considered in Optimization.
9
Under review as a conference paper at ICLR 2022
Bibliography
Andrej Bogdanov and Luca Trevisan. Average-case complexity. arXiv preprint cs/0606037,
2006.
Bernd Fischer. Polynomial based iteration methods for symmetric linear systems. SIAM,
1996.
Donald A Flanders and George Shortley. Numerical determination of fundamental modes.
Journal of Applied Physics, 21(12):1326-1332, 1950.
Magnus Rudolph Hestenes, Eduard Stiefel, et al. Methods of conjugate gradients for solving
linear systems, volume 49. NBS Washington, DC, 1952.
Francisco Marcellan and Renato AlvareZ-Nodarse. On the “favard theorem” and its extensions.
Journal of computational and applied mathematics, 127(1-2):231-254, 2001.
Charles H Martin and Michael W Mahoney. Implicit self-regulariZation in deep neural
networks: Evidence from random matrix theory and implications for learning. Journal of
Machine Learning Research, 22(165):1-73, 2021.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87.
Springer Science & Business Media, 2003.
Courtney Paquette, Bart van Merrienboer, Elliot Paquette, and Fabian Pedregosa. Halting
time is predictable for large models: A universality property and average-case analysis.
arXiv preprint arXiv:2006.04299, 2020.
Fabian Pedregosa and Damien Scieur. Acceleration through spectral density estimation. In
International Conference on Machine Learning, pp. 7553-7562. PMLR, 2020.
Jeffrey Pennington and Yasaman Bahri. Geometry of neural network loss surfaces via random
matrix theory. In International Conference on Machine Learning, pp. 2798-2806. PMLR,
2017.
B.T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR
Computational Mathematics and Mathematical Physics, 04, 1964.
Damien Scieur and Fabian Pedregosa. Universal average-case optimality of polyak momentum.
arXiv preprint arXiv:2002.04664, 2020.
Gabor Szego. Orthogonal polynomials, vol. 23. In American Mathematical Society Colloquium
Publications, 1975.
Walter Van Assche. Weak convergence of orthogonal polynomials. Indagationes Mathematicae,
6(1):7-23, 1995.
10
Under review as a conference paper at ICLR 2022
A Proofs of Section 2
Theorem 1. Let xt be generated by a first-order method associated to the polynomial Pt ,
the measure μ the e.s.d. of H, and E[(xo — x*)(x0 - x?)T] = R2I for some constant R.
Then we can write the convergence metrics at time step t as
E[kxt-x?k2]
R2/ Pt2(λ)dμ(λ),
E[f(xt) - f(x?)]
R2 / Pt2(λ)λdμ(λ)
and
E[∣∣Vf(xt)∣∣2] = R2 /Pt2(λ)λ2dμ(λ).
(5)
Proof. We remark that by the definition of the expected spectral distribution μ of H, We
have for continuous g
Eh[g(tr(H))] = / g(λ) dμ(λ)
(20)
We knoW that xt - x? = Pt (H)(x0 - x?). We can Write ||xt - x?||2 in terms of a trace and
use the independence of H and x0 - x? to connect it to the e.s.d.:
E||xt - x?||2 = E[tr((x0 - x?)TPt(H)2(x0 - x?))]	(21)
= EH,x0-x?[tr(Pt(H)2(x0 -x?)(x0 - x?)T]	(22)
=EH Pt(H)2Ex0-x? [(x0 - x?)(x0 - x?)T])	(23)
=R2Eh[Pt(tr(H))2] = R2 / Pt(λ)2 dμ(λ)	(24)
For the gradient and function value the reasoning is the same by noticing that
E[f(xt) - f(x?)] = E[tr((x0 - x?)TPt(H)HPt(H)(x0 -x?))]	(25)
= EH [(λPt)(tr(H))2],	(26)
Where λPt is also a polynomial. As Vf(xt) = H(xt - x?).
E||Vf(xt))||2 = E[tr((x0 - x?)T Pt(H)H2Pt(H)(x0 - x?))]	(27)
= EH [(λ2Pt)(tr(H))2]	(28)
□
Proposition 2 ((Pedregosa & Scieur, 2020)). Let Ptl be defined as
Ptl := arg minPt(0)=1	Pt2(λ)λldν(λ).
Then (Ptl) is the family of residual orthogonal polynomials w.r.t. to λl+1dν.
(6)
Proof. We differentiate the expression for the metrics W.r.t. to the coefficients of the
polynomials:
λ P2(λ)dμ(λ)
λ白
dak
2 ∙
akλkPt(λ)
dμ(λ)
/ λl+k Pt(λ)dμ(λ)) =0
This means that Pt(λ) is orthogonal to any polynomial of degree t - 1 W.r.t to the intern
PrOdUCt h∙,∙iλi+idμ
□
11
Under review as a conference paper at ICLR 2022
B GCM and Laguerre method derivation
We will first state two lemmas that allow us to construct the optimal polynomials. With
them in hand the procedure is trivial.
Lemma 7. Let (Pt) be a family polynomials following
~ . . ~ ~ .
Pt(λ) = (αt + βtλ)Pt-i)λ) + γtPt-2(λ),
• I 7	I I 1	∙	1	7 S / C VJI mi
with P0 a constant polynomial and Pt 6= 0, ∀t. Then
Pt(λ) = (at + bt λ)Pt-1 (λ) + (1 - at)Pt-2 (λ)	(29)
is the recurrence for Pt(λ) = Pt(λ)/Pt(0). With:
at = δt αt	(30)
bt = δtβt	(31)
δt = (αt + γtδt-1)	(δ0 = 0)	(32)
The proof of this is presented in Pedregosa & Scieur (2020). Further, we know how to
compute the recurrence for the polynomials of a shifted distribution:
Lemma 8. Let (Pt) be a family polynomials orthogonal w.r.t following
Pt(λ) = (αt + βtλ)Pt-i(λ) + γtPt-2(λ),	(33)
and define polynomials Pt s.t. :
. . .. ~
PMm(X)) = Pt (λ),
with m(λ) = aλ + b a non singular affine transform. Then Pt follows a recurrence like in eq.
(33), with:
αt0 = αt + bβt	(34)
βt0 = aβt	(35)
γt0 = γt	(36)
The lemma is self-evident by considering eq. (33) with argument m-1(λ)
These results are enough to get the recurrence relation for the residual polynomial w.r.t
xβ(L - x)α. We begin by the standard Jacobi polynomials, which are orthogonal w.r.t
(1 一 x)ɑ(1 + x)β and follow a recurrence according to at, βt, Yt below Szego (1975):
(2n + α + β)(2n + α + β - 1)
at =---------------------------
2n(n + α + β)
β	(a2 一 β2)(2n + a + β — 1)
t	2n(n + a + β)(2n + a + β — 2)
—2(n + a — 1)(n + β — 1)(2n + a + β)
Yt	2n(n + a + β)(2n + a + β — 2)
(37)
(38)
(39)
We then shift the distribution according to η(x), and then transform to the residual ones.
We slightly simplify these computations and use remark 1 to get Algorithm 1.
We know (Szego, 1975) that the Laguerre polynomials Ltα, with usual normalization, follow
the recurrence
Lαw = (2t+a -1 —，)La-。)+t+a -1LH	(40)
As we don’t have to shift the domains, we have only to apply lemma 7 to get the Laguerre
method.Further, We can get a explicit expression for δt =冒', simplifying the expression.
12
Under review as a conference paper at ICLR 2022
Algorithm 2: Laguerre(α)
Inputs: Initial vector x0 , function f
x-1 J 0
for t = 1, . . . , T do
|_ xt J xt-1 + t-1 (XtT - xt-2) - t+1ɑ▽f (Xt-I)
C Proofs of section 3
In the following we will consider shifted versions of the spectral distributions. This shift is
written as an affine transform m(λ) : [0, L] → [-1, 1] because most results in the theory of
orthogonal polynomials are stated in terms of distributions supported in [-1, 1].
This can be seen as an additional layer of abstraction because the quantities evaluated with
the shifted distributions and polynomials are proportional, i.e. if Pt (x) = Pt (m(x)) and
μ0(x) = μ0(m(x)):
J Pt2 (x)μ0(x)dx H
p P2(x)μ0(x)dx
(41)
So all the asymptotics are the same. The Jacobi polynomials Ptα,β are those orthogonal w.r.t
dμ(x) = (1 一 x)ɑ(1 + x)β. Most works use the normalization Pae(-1) = (-1)t(t+β). We
will write Pa,β for this normalization and Pae for the residual polynomials
Theorem 2	(GCM average-case rates). A Generalized Chebyshev Method with parameters
(α, β) applied to a problem with e.s.d. as in Assumption 1 has average-case rates
t-1-2β	if α < τ + 1/2	and β < ξ + 3/2
E[f(xt) - f (x?)]	~ L ∙	Ca,β	t-2(ξ+2) log t	if α = T +1/2	and β = ξ + 3/2	,
，[t2(maχ{α-β-τ,-ξ-1}-1)	if a>7 + 1/2	or β>ξ + 3/2
(13)
(t-1-2β	if α<τ + 1/2	and β<ξ + 5/2
E[∣∣Vf(xt)∣∣2]〜L2	∙	C2a,Vβ	<	t-2(ξ+3) logt	if α = T +1/2	and β = ξ + 5/2	,
[t2(maχ{a-β-τ,-ξ-2}-1)	if a > τ + 1/2	or β > ξ + 5/2
(14)
where Cνα,β is a distribution dependent constant.
Proof. We will prove that for any α and β, ξ, T > -1, l > 0 and ν following Assumption 1,
we have
Zl	( t-1-2β	if α<τ + 1/2 and β<ξ +1/2
/ Pta,β(x)2XldVT,ξ-l(x)〜LICa，e t-2(ξ+1) logt	if α = T + 1/2 and β = ξ + 1/2
[t2(maχ{a-β-τ,-ξ}-1) if a > τ + 1/2 or β > ξ + 1/2
We will first show this result for the Beta weights, then show that distributions with the
same concentration behave similarly.
The normalization of Ptα,β is s.t.[ Szego (1975) (4.3.3)]:
∕1 Pα,β	。/j、e .	2α+β+1	Γ(n + α + 1)Γ(n + β + 1)	1
LPt "(X)(I-X)(I+x)β dx =2n + α + β + 1 Γ(n +1)Γ(n + α + β + 1) =0(t ) (42)
Further, the residual polynomials are s.t. ∣Pα,β| = Θ(t-β)∣Pta,β|, from the definition of the
classical normalization.
We state the result (Exercise 91, Generalisation of 7.34.1) from Szego (1975):
13
Under review as a conference paper at ICLR 2022
Lemma 9. We have
Z1
0
(1 - x)τ Ptα,β (x)2dx
Θ(hτα)
hτα :
t-1 logt
t-1
if α > τ + 1/2
if α = τ + 1/2
if α < τ + 1/2
(43)
(44)
〜
Noting that Pae(x) = (-1)tPtβ,α(-x), we can write:
Z Pt(X)2(1—x)T(1+x)ξdx = Θ IZ (1 — x)τ∣Ptα,β(x)∣2dx)+Θ IZ (1 — x)ξpβ,α(x)∣2dx
(45)
We can then show our result for dνT,ξ-l (x) = xξ-l (L — x)α by carefully considering each of
the cases on Lemma 9 and the maximum of each term in eq. 45, and an added t-2β from
the different normalization. With this, we have the wanted result for the Beta weights
It remains to show:
Z Pae(x)2 d%,ξ(x) = Θ IZ (1 — x)τPae(x)2dx
(46)
And the rest follows from the same arguments. We do this with the help of this lemma
shown in Van Assche (1995) relating to the weak convergence of the orthogonal polynomials:
Lemma 10. Let μ be a measure and (Pt) it's family of orthonormal polynomials s.t. Pt
follow the recurrence:
xPt(x) = atPt+1(x) + btPt(x) + at-1Pt-1(x)
and at , bt converge respectively to a, b. Then for any f continuous and bounded:
Z f(X)P2(X)dμ(X) → ∏ Z √f(χ)x2 dχ
(47)
Let s.t.
X ≥ 1 — ⇒ | dνT,ξ — A(1 — X)T | ≤ B(1 — X)T	(48)
We observe that for 0 < x < 1 — e, f (x) = ([-方；：,；+.). is bounded.
We get from an application of 10, and the observation that Pae = NtpaB, with Nt 二
Θ(t-1/2):
Z (1 — x)τPae(x)2 dx
0
'----------{z----------}
Θ(hτα )
1-
I (1 — x)τPae(x)2 dx +
0
'-----------------------}
^^^{^^^™
Θ(t-1)
Z1
1-
(1 — x)τPae(x)2 dx ⇒
Z1
1-
(1 — x)τPae(x)2 dx
Θ(hτα)
Z1 Pae(x)2 dντ,ξ(x) = Z1 -
00
Ptα,e (x)2f (x)(1 —
{^^^^"
Θ(t-1)
(49)
(50)
(51)
+Θ
(1 — x)τPae(x)2 dx
_ - 1
{z	.
Θ(hτα)
/ ∖
□
14
Under review as a conference paper at ICLR 2022
Theorem 3	(Optimal Rates). Let ν follow Assumption 1. The optimal asymptotic average-
case rates for E[f(xt) — f (x?)] and E[∣∣V∕(xt)∣∣2] are attained by the GCM with parameters
(τ, ξ + 2) and (τ, ξ + 3), respectively, and read
E[f (xt) —f (x?)] = Θ(t-2(ξ+2)),	E[||Vf (xt)||22] = Θ(t-2(ξ+3)).
Proof. We will prove that for τ, ξ > —1 If α = τ and β = ξ + l + 1 (i.e., are optimal), the
rate of convergence reads
Pmin l ZPt2(λ)λldν(λ) = Θ (ZQ Ptα,β(λ)2(L — λ)τλξ+l dλ) = Θ(t-2(ξ+l+1))	(52)
Showing the second equality is easy by considering theorem 2, and that is further the
minimum asymptotic rate for the Beta distribution μτ,ξ∙
ν
By setting PV and Pt = pvpto) the optimal orthonormal and residual and polynomials w.r.t.
v we show that Pt must have the same rate on μτ,ξ as it does on V, thus the optimal rate of
v cannot be lower than the optimal rate of μτ,ξ. Indeed, setting eι, e? as in eq. 48:
I	PV(X)2dv(x) = (θ Z	PtV(x)2dμτ,ξ(X))
1-2	1-2
Zl	PV(X)2dV(X) = θ IZl	PV(X)2dμτ,ξ(X))
1-2	1-2
I	PV(X)2dv(∕) = Θ ( /	PV(X)2dμτ,ξ(x) I = Θ
-1+1	-1+1
(53)
(54)
(55)
(56)
Where the first two equations come from the fact that V = Θ(μτ,ξ) near —1 and 1 and the
third from lemma 10.
This effectively upper bounds the rates on V because the rates of Pt on μτ,ξ can,t be lower
than —2(ξ + l + 1).
□
Proposition 3 (GCM worst-case rates). Letf be a convex, L-smooth quadratic function.
Then, for the Generalized Chebyshev Method with parameters (α, β), we have worst-case rates
t2(α-β)
f (xt) —f (x?) ≤ C1L	t-1-2β,
I	t-2,
if
if
if α > β — 1
α ≤ β — 1	β ≤ 2
α ≤ β — 1	β ≥ 2
t2(α-β)
||Vf (xt)—f (x?)|| ≤ C2L2	t-1-2β, if
I	t-4,	if
if α > β — 2
α ≤ β — 2	β ≤ 3/2
α ≤ β — 2	β ≥ 3/2
(15)
(16)
Proof. rates] We will prove that: supx∈[0,L] XlPtα,β(X)2 = O(Lltv(α,β,l)). Where:
2(α — β)
v(α,β,l)=	—1 — 2β, if
I	—2l,	if
From Szego (1975), Theorem 7.32.2, if θ < 2:
Ptae (Cos θ)= J	。小
1 θ-αT∕2O(t-1∕2)
if α > β — l
α ≤ β — l	β ≤ l — 2
α ≤ β — l	β ≥ l — 2
if α < — 2
if α ≥ — 2, 0 ≤ θ ≤ ct-1
if α ≥ — 2, θ > ct-1
(57)
We observe that, from the symmetry of the Jacobi polynomials:
sup Xl Ptα,β (X)2 = Θ
x∈[0,L]
max
sup
x∈[0,1]
Ptα,β(X)2,
sup (1 — X)lPtβ,α (X)2
x∈[0,1]
))
(58)
(59)
The (1 — x)1 term, corresponds to (2sin(2))l in the variable θ, which is O(θ2l). The rest
follows from carefully considering the expressions given by eq. 58.	□
15
Under review as a conference paper at ICLR 2022
Theorem 4	(Nesterov average-case rates). Let ν as in Assumption 1. Then for the Nesterov
method, we have average-case rates
E[f(xt)- f(x*)]~ Cl,ν{
t-2(ξ+2)
t-3 logt
t-(ξ+7∕2)
if ξ < -1/2
if ξ = -1/2 ,	E[∣∣Vf(xt)∣∣2] ~ C2,νtTξ+9∕2). (17)
if ξ> —1/2
Proof. We will prove:
t-2(ξ+1)	if0 < ξ < 1/2
t-3 log t	ifξ= 1/2
t-(ξ+5∕2)	if ξ > 1/2
(60)
Paquette et al. (2020) has shown that the nesterov polynomials Pt are asymptotically, in t:
Pt(λ)〜2J1(t^)e-αλt∕2	(61)
t αλ
In the sense that:
[1 ul [Pt2^(u) - 4*«)e-ut] 4dμMP(U) = O(t-(l+25/12))	(62)
0	tu
The arguments can be easily used to show that such an integral is O(t-(α+l+31∕12)) when
evaluated Wrt a general dμ s.t μ0 = Θ(λα) near 0.
We can thus consider our integral of interest substituting PtNes by it’s Bessel asymptotic
and dividing it into three regions, i.e. [0,1] = [0, j] ∪ [∣, √] ∪ [√, 1] corresponding to two
different regimes for the Bessel function. The first region Will give us the asymptotic and the
others we will bound.
We consider first, for some > 0:
Z√t uξ 4j2(t√u) e-ut du	(63)
J且	t U
We note the asymptotic for j12 :
J2(√tV)〜一^(1 + cos(2√tv + 2γ))	(64)
π tv
Doing the change of variable v = tU, and identifying the upper limit of the interval, which is
t1∕2 to ∞:
Z √ uξ 4 JI(Y) e-ut du
Je	t2U
(65)
(66)
(67)
Where the cosinus term goes to 0 from the Riemann-Lebesgue lemma and Γ is the incomplete
Gamma function.
The term corresponding to the interval [t-1∕2 , 1] is exponentially small. Indeed, because of
the exponential e-ut it is O(e-'VZt). This shows that the integral concentrates in a region
that is closer and closer to 0 and that only the behaviour of the distribution near 0 matters.
We have for the [0, ɪ] region, doing the change of variables v = t2u:
I'e uξ 4j2(t√u) e-ut du = Θ (t-2(ξ+1) ∣'te vξ j12(√v) e- dv)	(68)
0	t2u	0	v
16
Under review as a conference paper at ICLR 2022
-v
And the eɪ is Θ(1). We have the following Bessel asymptotics:
v
So we divide this integral aswell:
t-2(ξ+1)
J2(①	1
V	4,
J2(√v) = O(v-3∕2),
∖ξ生”e-
t-2(ξ+1)
0
v
1vξ J≡ e
v→0
v→∞
(69)
(70)
dv = Θ (t-2(ξ+1) / vξ-3 dv) = Θ (lξ(t)t-ξ-5)	(71)
v dv = Θ (t-2(ξ+1) / e1vξ dv) = Θ (t-2(ξ+1)
(72)
v
Where Iξ(t) = log t if ξ = 11 and 1 otherwise.
The nesterov rate is then Iξ(t)t-ξ-2 if ξ ≥ ɪ and t-2(ξ+1) if 0 < ξ < 2
Theorem 5	(Gradient Descent average-case rates). Let ν as in Assumption 1.
gradient descent
□
Then for
E[f(xt) - f(x?)] = Θ(t-(ξ+2)),
E[∣Vf(χt 川 2 ] = θ(t-(ξ+3)).
(18)
Proof. Considering that PGD(λ) = (1 — L)t We will prove :
Z (1 — λ)2tλl dντ,ξ-l = Θ(t-(ξ+l+1)
0
We know, for the Beta weights, that:
[1(1 - λ)2t+τλξ+l dλ = γ(I + ξ + 1)r(2t + T + 1) = θ(t-(ξ+l + 1))
J0 (1	λ)	λ	dλ =	Γ(2t +1 + ξ + τ + 2)	(t	)
We can indentify this asymptotic to the interval 0 for any e because:
Z1(1 — λ)2t+τλξ+l dλ = O((1 — e)2t)
(73)
(74)
(75)
Then:
Z 1(1 — λ)2tλl dντ,ξ-l = O((1 — )2t)
Z 0(1 — λ)2tλl dντ,ξ-l = Θ Z (1 — λ)2t+τλξ+l dλ = Θ(t-(ξ+l+1))
(76)
(77)
□
Theorem 6 (Laguerre method rates). Let a > —1 and μ., a > —1 be a Gamma distribution,
i.e. dμα(x)
tuning and
xαe-x
Γ(α+1).
The optimal rates are given by the Laguerre method of appropriate
E[f(xt) - f(x?)] = Θ(t-(α+2)).
(19)
Proof. Let La be the Laguerre polynomials with the usual normalization Szego (1975):
/ La(X)2 dμa (X)= Lα(0)=(n + °)	(
We further now [Szego (1975) (5.1.13)]]:
t
XLta(X) = Lta+1(X)
k=0
(78)
(79)
17
Under review as a conference paper at ICLR 2022
Thus, letting Ptα be the residual laguerre polynomial, we consider:
E[f(Xt)- f(X?)]= / Pα+2(λ)2dμα+l(λ)= (t + ； +2) 2 / Lα+2dμα+l(λ)
=(t + Ot +2) 2 XX ]/ Lα+1 (λ)dμα+l(λ)
t+ α+2-2 Xt	k+ α+ 1 t+ α+2-2t+ α+ 2
= t + O + 2-1 = Θ(t-(α+2))
(80)
□
D Additional Experiments
Figure 7: Empirical vs Theoretical function-value performance for GCM(O?, β?) . Red lines
are given by numerical integration, shades are minimum and maximum values under 10 runs
and the blue line is the mean
Figure 8: Empirical vs Theoretical function-value performance under Marchenko Pastur
distribution. Red lines are given by numerical integration,shades are minimum and maximum
values under 10 runs and the blue line is the mean
We note that in the regimes where the empirical average performance doesn’t match the
theoretical one, we can still find samples of problems who do match. This and the much
larger variance on the function-value, this discrepancy is due to numerical unstability in
these regimes.
18