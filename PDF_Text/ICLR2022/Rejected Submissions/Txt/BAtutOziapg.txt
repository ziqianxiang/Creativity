Can Stochastic Gradient Langevin Dynamics
Provide Differential Privacy for Deep
Learning?
Anonymous authors
Paper under double-blind review
Ab stract
Bayesian learning via Stochastic Gradient Langevin Dynamics (SGLD) has been
suggested for differentially private learning. While previous research provides
differential privacy bounds for SGLD when close to convergence or at the initial
steps of the algorithm, the question of what differential privacy guarantees can be
made in between remains unanswered. This interim region is essential, especially
for Bayesian neural networks, as it is hard to guarantee convergence to the poste-
rior. This paper will show that using SGLD might result in unbounded privacy loss
for this interim region, even when sampling from the posterior is as differentially
private as desired.
1	Introduction
Machine learning and, specifically, deep learning models show state-of-the-art results in various
fields such as computer vision, natural language processing, and signal processing (e.g., Carion
et al. (2020); Devlin et al. (2019); Balevi & Andrews (2021)). Training these models requires data,
which in some problems, e.g., healthcare, finance, can include private information that should not
be made public. Unfortunately, it has been shown (Fredrikson et al. (2015); Carlini et al. (2021))
that private information from the training data can sometimes be extracted from the trained model.
One common approach to handle this issue is Differential Privacy (DP). Differential Privacy is a
framework that ensures that the distribution of training output would be the same, even if we switch
one of the training participants, thus ensuring privacy.
As privacy is usually obtained by adding random noise, it is natural to investigate whether Bayesian
inference, which uses a distribution over models, can give private predictions. Previous works have
shown that sampling from the posterior is differentially private under certain mild conditions (Wang
et al. (2015); Foulds et al. (2016); Dimitrakakis et al. (2017)). The main disadvantage of this method
is that sampling from the posterior is generally hard. The posterior usually does not have a closed-
form solution, and iterative methods such as Markov Chain Monte Carlo (MCMC) are needed.
While theoretical bounds on the convergence of MCMC methods for non-convex problems exist
(Ma et al., 2019), they usually require an infeasible number of steps to guarantee convergence in
practice.
Stochastic Gradient Langevin Dynamics (SGLD) is a popular MCMC algorithm used to approxi-
mately sample from an unnormalized distribution (Welling & Teh, 2011). The privacy guarantees
of this specific sampling algorithm are interesting as it not only returns a sample from the posterior,
which can be private, but the process itself of stochastic gradient descent with Gaussian noise mir-
rors the common Gaussian mechanism in DP. Previous work Wang et al. (2015) gives two disjoint
privacy analyses: The first is for approximate sampling from the Bayesian posterior, which is only
relevant when the SGLD almost converges. The second uses the standard DP analysis utilizing the
Gaussian mechanism and the Advanced Composition theorem (Dwork & Roth, 2014), which only
applies for a limited number of steps and is not connected to Bayesian sampling.
From these two lines of research, differential privacy bounds for SGLD are provided for its initial
steps or when close to convergence. Neither of these cases is suitable for deep learning and many
other problems, as one would limit the model’s accuracy, and the other is unattainable in a reasonable
time. Consequently, the privacy properties of SGLD in the interim region, between these two private
1
sections, are of high importance. One could speculate that since the initial steps of the algorithm
are private, and it converges to the posterior that is also private, then sampling at the interim region
will be private as well. If so, SGLD could be considered a solution for training differentially private
deep neural networks. Unfortunately, as we will show, this is not the case.
Our Contributions: This work provides a counter-example, based on a Bayesian linear regression
problem, showing that approximate sampling using SGLD might result in an unbounded loss of
privacy in the interim regime. Moreover, this loss of privacy can even occur under strong conditions
- when sampling from the posterior is as private as desired, and the problem is complex - even
stronger conditions than what we can assume for most Deep Neural Network problems. This implies
that special care should be given when using SGLD for private predictions, especially for problems
where it is infeasible to guarantee convergence.
2	Related Work
Several previous works investigate the connection between Bayesian inference and differential pri-
vacy (Wang et al. (2015); Foulds et al. (2016); Zhang et al. (2016); Dimitrakakis et al. (2017);
Geumlek et al. (2017); Ganesh & Talwar (2020)). None of these papers provide guarantees over
SGLD differential privacy in the interim regime. The closest work to ours is Wang et al. (2015) that
specifically investigates stochastic MCMC algorithms such as SGLD. As mentioned, its analysis
only covers the initial phase and when approximate convergence is achieved.
As many of the privacy bounds require sampling from the posterior, if SGLD is to be used, it requires
non-asymptotic convergence bounds. Dalalyan (2014) provided non-asymptotic bounds on the error
of approximating a target smooth and log-concave distribution by Langevin Monte Carlo. Cheng &
Bartlett (2018) studied the non-aSymPtotic bounds on the error of approximating a target density p*
where log p* is smooth and strongly convex.
For the non-convex setting, Raginsky et al. (2017) showed non-asymptotic bounds on the 2-
Wasserstein distance between SGLD and the invariant distribution solving Ito stochastic differential
equation. However, to provide (, δ) differential privacy, an algorithm should produce a distribution
that is O(δ) close to neighbouring databases. Total Variation (for details about Total Variation see
Tsybakov (2008)) is a more suitable distance for working with differential privacy. Ma et al. (2019)
examined a target distribution p*, which is strongly log-concave outside ofa region of radius R, and
where - ln p* is L-Lipschitz. They provided a bound on the number of steps needed for the Total
Variation distance between the distribution at the last step and p* to be smaller than . This bound is
proportional to O(e32LR2 备)，where d is the model dimension. This result suggests that even little
non-convexity will render running until close to convergence impractical. A conclusion from this
work is that basing the differential privacy of SGLD on the proximity to the posterior is impractical
for non-convex settings.
3	Background
3.1	Differential Privacy
Differential Privacy (Dwork et al. (2006b;a); Dwork (2011); Dwork & Roth (2014)) is a definition
and a framework that enables performing data analysis on a database while reducing one’s risk of
exposing its personal data to the database. An algorithm is differentially private ifit does not change
its output distribution by much due to a single record change in its database.
Definition 1. Approximate Differential Privacy: A randomized algorithm M : D → Range(M) is
(e, δ)-differentially private if ∀S ⊆ Range(M) and {∀D, D ∈ D : k D — D∣∣ ≤ 1} eq. 1 holds.
D, D are called neighboring databases, and while the metric can change per application, Hamming
distance is typically used.
- - , . - , ʌ. -
Pr[M(D) ∈ S] ≤ exp(e)Pr[M(D) ∈ S] + δ	(1)
Mironov (2017) suggested Renyi Differential Privacy (Definition 3), a relaxation to differential pri-
vacy, and a way to translate RDP guarantees into approximate differential privacy guarantees.
2
Definition 2. Renyi Divergence (Renyi, 1961): For two probability distributions Z and Q over R,
the Reyni divergence Oforder ν > 1 is
DV(Z||Q) δ ɪlogEx~q
ν-1
Definition 3. (ν, )-RDP: A randomized mechanism f : D → R is said to have -Ree nyi differential
privacy of order V, or (ν, e) -RDP in short, if for any adjacent databases D, D ∈ D eq. 2 holds,
where Dν is Reenyi divergence of order ν.
.........ʌ..
DV(f(D)||f(D)) ≤ e	⑵
Lemma 3.1. (Mironov (2017) Proposition 3). IffiS (ν, e)-RDP it also satisfies (e + lo-δ ,δ) Dif-
ferential Privacy for any 0 < δ < 1.
3.2 Stochastic Gradient Langevin Dynamics
Stochastic Gradient Langevin Dynamics (SGLD) is an MCMC method that is commonly used for
Bayesian Inference (Welling & Teh, 2011). The update step of SGLD is shown in eq. 3, where θj
is the parameter vector at step j, η is the step size at step j, p(θj) is the prior distribution, p(yi∣θj)
is the likelihood of sample yi given model parameterized by θj , b is the batch size, and n is the
database size. SGLD can be seen as a Stochastic Gradient Descent with Gaussian noise, where the
variance of the noise is calibrated to the step size.
b
θj+ι = θj + ^2j Mjln p(θj) + b X Jj ln Pkyijwj)] + √¾ξj
i=1	(3)
ij 〜Uniform{1,…,n}
ξj~N(0,1)
A common practice in deep learning is to use cyclic Stochastic Gradient Descent. This flavour
of SGD first randomly shuffles the database samples and then cyclically uses the samples in this
order. For optimization, there is empirical evidence that it works as well or better than SGD with
reshuffling, and it was conjectured that it converges at a faster rate (Yun et al. (2021)). Cyclic-SGLD
is the analog of cyclic-SGD for SGLD, where the difference is the use of the SGLD step instead of
the SGD step. For simplicity, we will consider cyclic-SGLD in this work.
4 Method
Our goal is to prove that even when the posterior is as private as desired, sampling using SGLD for
T steps can be as non-private as desired. This requires analysing the distribution of SGLD after T
steps, which is hard in the general case. However, we show that we can get the desired behaviour
when looking at a simple Bayesian linear regression problem where everything is a Gaussian with
closed-form expressions. Our result is summarized in theorem 1.
Theorem 1. ∀ δ < 0.5, , 0 there exists a domain anda Bayesian inference problem where a single
sample from the posterior distribution is (, δ) differentially private, but, there is a number, T, for
which performing approximate sampling by running SGLD for T steps is not (0, δ) differentially
private.
As 0 can be as big as desired, and can be as small as desired, a corollary of Theorem 1 is that
we could always find a problem for which the posterior is (, δ) differentially private, but there will
be a step in which SGLD will result in unbounded loss of privacy. Therefore, SGLD alone can not
provide any privacy guarantees in the interim regime, even if the posterior is private.
To prove our theorem, we consider a Bayesian regression problem for a linear model with Gaussian
noise, as defined in eq. 4, on domain D defined in eq. 5.
3
y=θx+ξ
ξ 〜N(0,βT)
θ 〜N(0,α-1)	⑷
logp(y∣x,θ) = -β(y - θx)2∕2 - 1 log(2π∕β)
D(n, γ1, xh, xl, c) =
yi	γ	n
{χi,yi∣∣-- c| ≤ nγ1; χi,yi,c,γι ∈ R>o; χι ≤ Xi ≤ χ九N=ι
xi	=	(5)
21
We assume that χhβ > 3 and that γι < -
n, c, xl , xh , γ1 are parameters of the problem (c, xl , xh, and γ1 are used, together with the database
size - n, to bound the database samples to a chosen region). For every , 0 and δ, we will show there
exist parameters n, c, xl, xh, γ1 that have the privacy properties required to prove Theorem 1. The
restrictions on the dataset simplify the proof but are a bit unnatural as it assumes we approximately
know c, the parameter we are trying to estimate. Later we show in subsection 4.3 that they can
be replaced with a Propose-Test-Release phase. We will address the problem of Bayesian Linear
Regression for the model described in eq. 4 on domain D as Bayesian linear regression problem
on domain D. This problem has a closed-form solution for both the posterior distribution and the
distribution at each SGLD step, thus enabling us to get tight bounds on the differential privacy in
each case.
The heart of our proof is showing that for n big enough sampling from the posterior is (, δ) differ-
2
entially private, with e 〜 O(n^), while for SGLD there exists a step in which releasing a sample
will not be (e0, δ) differentially private for e0 = Ω(n).Therefore, by considering instances of the
3
problem where C 〜O(n2 √e) and n is big enough, sampling from the posterior will be (e, δ) dif-
ferentially private, while there will be an SGLD step in which releasing a sample will not be (e0, δ)
differentially private for e0 = Ω(ne). We note that the bounds contain dependency over δ, but since
we are using a fixed and equal δ for both the posterior and SGLD privacy analysis, we omit it from
the bounds for simplicity.
¾¾l⅛ws
Figure 1: A value indicative of the distance between the distributions of samples from two
SGLD processes running on neighbouring databases for the linear Gaussian model. For details
on μs, μr, σSr see subsection 4.2. The parameters used for this experiment (n = 1149019, α =
0.1, β = 1,xh = 1.8, xl = 0.9, γ1 = 0.1, γ2 = 1.001, c = 1165165) ensure (1.6, 0.01) differential
privacy when sampling from the posterior.
Figure 1 depicts an indicative value of the distance between the distributions of samples from two
SGLD processes running on adjacent databases for the Bayesian linear regression problem. As we
4
will later show, SGLD on one of these examples is a Gaussian while the other is a mixture of n
Gaussians. We plot 1 Pi "t-)t , where μt is the mean of the single Gaussian at timestep t, μi
is the mean of the i’th Gaussian component at timestep t and (σti)2 its variance. We can see that
even though the distributions are close at the initial iterations and at convergence (which implies
differential privacy in those areas), in the interim region, they are significantly apart, which implies
a lack of differential privacy.
4.1	Posterior Sampling Privacy
To prove Theorem 1, we first need to show that ∀δ < 0.5, , there exists a domain and a Bayesian
inference problem where a single sample from the posterior distribution is (, δ) differentially pri-
vate. In order to do so, this section will consider the differential privacy guarantees provided by one
sample from the posterior for the Bayesian linear regression problem on domain D.
We begin by using a well-known result for the closed-form-solution of the posterior distribution for a
Bayesian linear regression problem (see Bishop (2006) for further details). By using the parameters
of our problem, we get Lemma 4.1.
Lemma 4.1. The posterior distribution for Bayesian linear regression problem on domain D is
p(θD) = N (θ;μ,σ2);μ = 0P+⅛=⅛β; σ2
α + pn=ι x2β∙
(6)
1
Using the posterior distribution, one can calculate the Renyi divergence between every two neigh-
bouring databases, thus getting an expression for the Renyi differential privacy, as shown in Lemma
4.2.
Lemma 4.2. For a Bayesian linear regression problem on domain D, such that n > max{1 +
22	2
10X2 β ,1+νχ2 }, one samplefrom the posterior is (ν, CI)-Renyi differentially private. eɪ 〜 O( n3)
for c >> n1+γ1 .
x2h	1
C1 = 2(n - 1)x2 +2(V -
2νβ ∙
(Xhe)(Xha + Xhe)
1) 7---2h―2-----2 + 2νβ -9-   -----2+
(n - 1)x2 - VXh	10n1-2γ1 xj
(C + nγ1)	v (Xha + Xhe )2 (C + nγ1 )2
n2-γ1	+ 2	IO X,β	n3
We can show that for C >> n1+γ2, each of the terms is bounded by O(n).The first and second
terms are bounded by O( 1). The third term is bounded by O(n2γ1-1). Noticing that n2γ1-1 =
n2(；1) < n23, we get that the third term is bounded by O(n3). As C >> nγ1, the fourth term
is bounded by O(cnγ1), and since cnγ1 = Cnn+ γ1 < n3, the term is bounded by O(nn23). Lastly,
since c >> n1+γ1, the last term is bounded by O(宗).For the full proof, see subsection A.1 in the
appendix.
Translating the Renyi differential privacy guarantees into approximate differential privacy terms can
be done according to Lemma 3.1, which gives Lemma 4.3.
Lemma 4.3. With the conditions of Lemma 4.2, one sample from the posterior is (eɪ + In-δf), δ)
differentially private.
By choosing V such that In-I) < W and then choosing n big enough such that ci < ∣, we get that
the posterior is (C, δ) differentially private.
4.2	Stochastic Gradient Langevin Dynamics Privacy
To complete the proof of Theorem 1, we need to show that even if one sample from the posterior
is (C, δ) differentially private for a Bayesian linear regression problem on domain D, it does not
5
provide any guarantees on the privacy of SGLD for that problem. In order to do so, this section will
first consider the loss in privacy when using SGLD for the Bayesian linear regression problem on
domain D, and then, together with the results of section 4.1, will prove Theorem 1.
In order to show that SGLD is not differentially private after initial steps and before convergence,
it is enough to find two neighbouring databases for which the loss in privacy is as big as desired in
those steps. We define neighbouring databases D1 and D2 in eq. 7 and consider the Bayesian linear
regression problem on Di and D?. We set the learning rate to be η = g+nX2 β)2.
DI = {xi, yi : Xi = xh, yi = C ∙ xh}i=l
D2 = {χi,yi: Xi = χh,y = c ∙ χh}n-ι1 ∪ {Xh, C ∙ Xh}
(7)
To tightly analyze the differential privacy loss when approximately sampling via SGLD at each step,
we need to get a closed-form solution for the distribution for each step. For database D1, the solution
is Normal distribution. For database D2, different shuffling of samples produces different Gaussian
distributions, therefore giving a mixture of Gaussians.
I-V T 1 1	1 ∙ n y -I T I ʌ ∙ .1 1.1	C< 1	1F∕^k∙A,l	1	.1 '，，1 n y- -1 T 1 ʌ
We look at cyclic-SGLD with a batch size of 1 and mark by θj , θj the samples on the j’th SGLD
step when using databases D1 and D2 accordingly. Since D1 samples are all equal, the update step
of the cyclic-SGLD is the same for every step (with different noise generated for each step). This
update-step contains only multiplication by a scalar, addition of a scalar, and addition of Gaussian
noise, therefore, together with a conjugate prior results in Normal distribution for θj: N(θj; μj,σj).
For D2, there is only one sample different from the rest. We mark by r the index in which this
sample is used in the cyclic-SGLD and call this order r-order. Note that there are only n different
values forr and, as such, effectively only n different samples orders. Since every order of samples is
chosen with the same probability, r is distributed uniformly in {1, .., n}. We mark by θjr the sample
on the j,th SGLD step when using r-order. Since, for a given order, θj is formed by a series of
multiplications by a scalar, addition of scalar, and addition of Gaussian noise, and since the prior is
also Gaussian, then θj is distributed Normally, N(θj; μr, (σj)2). As r is distributed uniformly, θj
distribution mass is distributed evenly between all θjr, resulting in a mixture of Gaussians.
Intuitively what will happen is that each Gaussian components ,θj as well as θj , will move towards
the similar posterior Gaussian. However, at each epoch, θj will drag a bit behind because in one
batch one gradient is smaller. While this gap can be quite small, for large n, the Gaussians are very
peaked with very small standard deviations; thus, they separate enough that we can easily distinguish
between the two distributions.
According to approximate differential privacy definition (Definition 1), it is enough to find one set
S such that p(θj ∈ S) > ep(θj ∈ S) + δ to show that releasing θj is not (, δ) private. We
choose S = {s∣s > μj} at some step j we will define later on. It is clear from symmetry that
p(θj > μj) = 1/2, and by using Chernoff bound we can boundp(θj > μj).
Lemma 4.4. p(θj > μj) ≤ n Pn=I exp(- ”溜 ).
T T -	T	Λ A	F	1 . 1	i~ Λ ∙ Γ1	1.1	1	F	IJLCC
Using Lemma 4.4, we can upper bound the mass of θj in S, and thus lower bound the difference
between θj and θj distribution masses in S for some step - j. To use Lemma 4.4, we first need to
lower bound (尢卒)for a certain step. This is done in Lemma 4.5.
Lemma 4.5. ∃k ∈ Z>0 such that
W(k + 1)n-μrk + 1)Q2
(^k + 1)n)2
2
Ω( n),forn big enough.
To prove Lemma 4.5, we first find closed-form solutions for θ(rk+1)n , θ(k+1)n distributions (Lemma
A.1). Using the closed-form solutions, We find a lower bound over (μ(k+i)n - %+、)万 as a
function of k, which applies for all k (Lemma A.2). To upper bound (3"+12)2, We find an approx-
imation to the epoch in which the data and prior effects on the variance are approximately equal,
6
1 1 ； I-V T 1	.1	.	< ∙ <	∙ 11	♦ 1 ,1	♦	1	/ Γ 7 ^l . -I ∖	1 1	.1
marked k. We choose the step in which we will consider the privacy loss as (dke +1)n and show that
(σrdk e+i)n)2 is upper bounded at this step (Lemma A.4). Using the upper bound on the difference in
means and the lower bound on the variance, Lemma 4.5 is proved. By using the lower bound from
Lemma 4.5 in Lemma 4.4, we get Lemma 4.6.
Lemma 4.6. For the Bayesian linear regression problem over database D1, such that n is big
enough, ∃T ∈ Z>0 such that approximate sampling by running SGLD for T steps will not be (, δ)
2
Privatefor E < Ω(谆),δ < 0.5.
From Lemma 4.3, we see that sampling from the posterior is (, δ) differentially private for =
2
O (nC3). From Lemma 4.6, We see that for SGLD, there exists a step in which releasing a sample will
2
not be (e0, δ) differentially private for e0 = Ω(枭).Therefore, considering instances of the problem
where C = O(n2 √e), sampling from the posterior will be (e, δ) differentially private. However,
there will be an SGLD step in which releasing a sample will not be (E0, δ) differentially private for
E0 = Ω(ne). Since we can choose n to be big as desired, we can make the lower bound over E as
big as we desire. This completes the proof of Theorem 1.
4.3 Propose Test Sample
Our analysis of the posterior and SGLD is done on a restricted domain D as defined in eq. 5. These
restrictions over the dataset simplify the proof but are a bit unnatural as they assume we approxi-
mately know c, the parameter we are trying to estimate. This section shows that these restrictions
could be replaced with a Propose-Test-Release phase (Dwork & Lei, 2009) and common practices
in deep learning.
When training a statistical model, it is common to first preprocess the data by enforcing it in a
bounded region and removing outliers. After the data is cleaned, the training process is performed.
This is especially important in DP, as outliers can significantly increase the algorithm’s sensitivity
to a single data point and thus hamper privacy.
Informally, algorithm 1 starts by clipping the input to the accepted range. It then estimates a
weighted average of the ratio yi (line 12) and throws away outliers that deviate too much from
xi
it. The actual implementation of this notion is a bit more complicated because of the requirement
to do so privately. Once the database is cleaned, algorithm 1 privately verifies that the number of
samples is big enough, so the sensitivity of p(θ∣W) to a single change in the database will be small,
therefore making sampling from p(θ∣W) (e, δ) differentially private. This method is regarded as
Propose-Test-Release, where we first propose a bound over the sensitivity, then test if the database
holds this bound, and finally release the result if so.
We define nmin in eq. 26 in the appendix to be the minimum size of W for which the algorithm
will sample from p(θ∣W) with high probability. We will show later on that this limit ensures that
sampling from p(θ∣W) is (e, δ) differentially private.
We define p(θ∣W) to be the posterior for the Bayesian linear regression problem over database W.
From Lemma 4.1, it follows thatp(θ∣W) has the form of
p(θlW) = N(θ;μ, σ2); μ
E(Xi,yi)∈W Xiyie , σ2 = ________1_________
α + P(χi,yi)∈W x2β ; σ - α + P(χi,yi)∈W x2β
Claim 4.1. Algorithm 1 is (5E, 2δ) differentially private.
By claim C.9, steps 6-13 are (3E, δ) differentially private. By corollary C.3, steps 14-19 are (2E, δ)
differentially private for given m and n. Therefore by the sequential composition theorem, the
composition is (5E, 2δ) differentially private. The claim proved by noticing that if steps 6-19 are
private with respect to the updated database (after step 5), then they are also private for the original
database.
Claim 4.2. When replacing line 19 with sampling via SGLD with step size η = (ɑ十九；方2 .尸,then
∃T(n1) : Z>0 → Z>0 such that the updated algorithm is not (E, δ) differentially private ∀E ∈
R>o, δ < 6 ifranfor T(nι) steps.
7
Algorithm 1 Propose Test Sample
Input: D = {xi, yi}in=11
Parameters: , δ < 0.5, xl > 0, xh > xl, α > 0, β ≥
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
for i = 1, 2, . . . , N do
Xi J max{xi, xι}
Xi J min{xi,xh}
yi J max{yi, 0}
end for
nι J nι - 7 log 2δ + LaP(7)
V = {χi,yi∣Xi ≤ nl1}
n2 J IV| - 7 log 2δ + Lap(7)
if n2 ≤ 1 then
return null
X3∑, ρι ∈ (1, 2), ρ2 ∈ (0, 1), γι ∈ (P2, 2)
11: end if
12∙ j j E(χi,yi)∈V Xiyi
1 乙./ m j	Pɔ	2
(xi ,yi )∈V xi
ι3: mJ m+Lap( 7 npι ^n2-n23¾⅞+Xh)
14: W J {(Xi,yi): IXi — m∣≤ nρ2}
15： nw JIW| - 7 log(2δ) + Lap(7)
16:	if nW < nmin then
17:	return null
18:	end if
19:	return sample fromp(θIW)
Proof sketch (See appendix for full proof). We first note that by choosing 1+ρ2 > ρ1, the sensitivity
of m grows slower than the bound over the distance ∣ yi — /1. Therefore for nι big enough, samples
Xi
for which Xi = m will be included in W with high probability. Consequently, databases D3, D4 ∈
D will reach, with high probability, to step 19, which from our previous analysis over SGLD (see
subsection 4.2) will cause an unbounded loss in privacy.
ρ1 > ρ3 > 1
D3 =	{xi, yi	:	xi	=	xh, yi	= nɪɜ	∙ xh}i=l	(8)
D4 =	{χi,yi	:	Xi	=	Xh,yi	= np3	∙ xh}n-ι1	∪ {Xh,np3	∙ Xh}
5 Wasserstein Distance and Differential Privacy
As we have shown in Theorem 1, one cannot give any DP guarantees for SGLD in the interim
region. That means that to get private samples using SGLD, one must limit the number of iterations,
thus utilizing the Gaussian mechanism, or run until approximate convergence. Therefore, it is of
interest to get non-asymptotic convergence bounds for SGLD so that we guarantee privacy after a
known number of steps. Previously, several works have given non-asymptotic bounds; however,
some of those do so for the 2-Wasserstein metric (Raginsky et al. (2017); Cheng et al. (2018)). This
is unfortunate as the 2-Wasserstein metric is unsuitable for differential privacy - it is easy to create
two distributions with 2-Wasserstein distance as small as desired but with disjoint support.
It is, however, interesting to ask whether combining bounds on the 2-Wasserstein metric with
Lipschitz continuous probability densities will allow us to get privacy guarantees. The intuition
why this should be enough is simple: Ifp, q are two distributions with small 2-Wasserstein distance,
then there is (under mild conditions) a mapping, f : X → X, such that the pushforward maintains
f]p = q (i.e. for each measurable set S q(S) = p(f-1(S))) and that Ep[IIX - f(X)II2] < . One can
assume that p(X) ≈ q(f(X)) and q(X) ≈ q(f (X)) as X ≈ f(X) with high probability. Unfortunately,
this intuition does not hold exactly, as the map f can change the density considerably but still be a
8
pushforward by changing the volume. For example, if we assume f is smooth and bijective, we get
the standard change of variable formula such that p(x) = q(f(x)) |det(Jf)|, so p(x) ≈ q(f (x))
only if |det(Jf)| ≈ 1. This issue becomes more severe as the dimensionality increases.
For completeness, we will share our results connecting p(x) to q(x) when W2 (p, q) is small,
and both distributions are L-Lipschitz continuous. This bound scale poorly with dimension, and
as such ill-suited for SGLD on deep networks, but can still be useful for Bayesian sampling in
low-dimensional problems.
For distribution p, we define the density pλ(x) as the average of p(x) on a ball of radius λ centered
around X - pλ(x) = voj。) JBd(0)p(x + z)dz, where Bd(X) is the ball in Rd of radius λ centered
around x, and vold(λ) is its volume.
Claim 5.1. For L-Lipschitz continuous distribution P we have |p(x) 一 Pλ(x)∣ ≤ λL.
Theorem 2. Let P, Q be absolutely continuous w.r.t the Lebesgue measure in Rd, with finite second-
moment and L-Lipschitz continuous densities p, q. If W2(p, q) < 2 then we have
pλ(X) ≤
vold(λ)
Vθld(λ 一 E)
qλ(X) +
vold(λ)
Vθld(λ 一 e)
一1
2λL +
E
Vθld(λ 一 E)
(9)
The proof is an extension of the proof of theorem 2.1 in Walker (2004) to dimensions larger than 1.
The detailed proof is in the supplementary material.
It is easy to see that as Vvold[-：)=(1+ ʌ-^) , the bounds usefulness quickly diminishes with
dimensionality as it requires extremely small E to give non-vacuous results.
This, however, can still give useful results in low-dimensional problems.
6 Conclusion
As shown in this work, while SGLD has interesting connections to privacy and some guarantees,
caution is required if one wishes to use it to get private predictions. This is especially important for
models such as deep neural networks, where it is infeasible to guarantee convergence.
9
References
Eren Balevi and Jeffrey G. Andrews. Wideband channel estimation with a generative adversarial
network. IEEE Transactions on Wireless Communications, 20(5):3049-3060, 2021. doi: 10.
1109/TWC.2020.3047100.
Christopher Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics.
Springer-Verlag New York, 2006.
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and
Sergey Zagoruyko. End-to-end object detection with transformers. In Andrea Vedaldi, Horst
Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), Computer Vision - ECCV2020, pp. 213-
229. Springer International Publishing, 2020. ISBN 978-3-030-58452-8.
Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine
Lee, Adam Roberts, Tom B. Brown, D. Song, UJ. Erlingsson, Alina Oprea, and Colin Raffel.
Extracting training data from large language models. In USENIX Security Symposium, 2021.
X. Cheng and P. Bartlett. Convergence of langevin mcmc in kl-divergence. In ALT, 2018.
Xiang Cheng, Niladri S. Chatterji, Peter L. Bartlett, and Michael I. Jordan. Underdamped langevin
MCMC: A non-asymptotic analysis. In Conference On Learning Theory COLT, 2018.
Arnak Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave
densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79, 12
2014. doi: 10.1111/rssb.12183.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, Minneapolis, Minnesota, June
2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https:
//aclanthology.org/N19-1423.
Christos Dimitrakakis, Blaine Nelson, Zuhe Zhang, Aikaterini Mitrokotsa, and Benjamin I. P. Ru-
binstein. Differential privacy for bayesian inference through posterior sampling. Journal of
Machine Learning Research, 18(11):1-39, 2017. URL http://jmlr.org/papers/v18/
15-257.html.
Cynthia Dwork. A firm foundation for private data analysis. Commun. ACM, 54(1):86-95, January
2011. ISSN 0001-0782. doi: 10.1145/1866739.1866758. URL https://doi.org/10.
1145/1866739.1866758.
Cynthia Dwork and Jing Lei. Differential privacy and robust statistics. In Proceedings of the Forty-
First Annual ACM Symposium on Theory of Computing, STOC ’09, pp. 371-380, New York,
NY, USA, 2009. Association for Computing Machinery. ISBN 9781605585062. doi: 10.1145/
1536414.1536466. URL https://doi.org/10.1145/1536414.1536466.
Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Found. Trends
Theor. Comput. Sci., 9(3-4):211-407, August 2014. ISSN 1551-305X. doi: 10.1561/0400000042.
URL https://doi.org/10.1561/0400000042.
Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our
data, ourselves: Privacy via distributed noise generation. In Advances in Cryptology - EU-
ROCRYPT 2006, 25th Annual International Conference on the Theory and Applications of
Cryptographic Techniques, volume 4004 of Lecture Notes in Computer Science, pp. 486-
503. Springer, 2006a. doi: 10.1007/1176167929. URL https://iacr.org/archive/
eurocrypt2006/40040493/40040493.pdf.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in
private data analysis. In Shai Halevi and Tal Rabin (eds.), Theory of Cryptography, pp. 265-284,
Berlin, Heidelberg, 2006b. Springer Berlin Heidelberg. ISBN 978-3-540-32732-5.
10
James R. Foulds, Joseph Geumlek, Max Welling, and Kamalika Chaudhuri. On the theory and
practice of privacy-preserving bayesian data analysis. In Uncertainty in Artificial Intelligence,
UAI, 2016.
Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confi-
dence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC Confer-
ence on Computer and Communications Security, CCS '15, pp. 1322-1333, New York, NY, USA,
2015. Association for Computing Machinery. ISBN 9781450338325. doi: 10.1145/2810103.
2813677. URL https://doi.org/10.1145/2810103.2813677.
ArUn Ganesh and KUnal Talwar. Faster differentially private samplers via renyi divergence analysis
of discretized langevin mcmc. ArXiv, abs/2010.14658, 2020.
Joseph GeUmlek, ShUang Song, and Kamalika ChaUdhUri. Renyi differential privacy mechanisms
for posterior sampling. In Advances in Neural Information Processing NeurIPS, 2017.
Alison L. Gibbs and Francis Edward SU. On choosing and boUnding probability metrics. Interna-
tional Statistical Review, 70(3):419-435, 2002.
M. Gil, F. Alajaji, and T. Linder. Renyi divergence measures for commonly used univariate Contin-
UoUs distribUtions. Information Sciences, 249:124-131, 2013. ISSN 0020-0255. doi: https://doi.
org/10.1016/j.ins.2013.06.018. URL https://www.sciencedirect.com/science/
article/pii/S0020025513004441.
Yi-An Ma, Yuansi Chen, Chi Jin, Nicolas Flammarion, and Michael I. Jordan. Sampling can be
faster than optimization. Proceedings of the National Academy of Sciences, 116(42):20881-
20885, 2019. ISSN 0027-8424. doi: 10.1073/pnas.1820003116. URL https://www.pnas.
org/content/116/42/20881.
Ilya Mironov. Renyi differential privacy. CoRR, abs/1702.07476, 2017. URL http://arxiv.
org/abs/1702.07476.
Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via stochastic
gradient langevin dynamics: a nonasymptotic analysis. In Satyen Kale and Ohad Shamir (eds.),
Proceedings of the 2017 Conference on Learning Theory, volume 65 of Proceedings of Machine
Learning Research, pp. 1674-1703. PMLR, 07-10 Jul 2017. URL https://proceedings.
mlr.press/v65/raginsky17a.html.
Alfred Renyi. On measures of entropy and information. In Proceedings of the Fourth Berkeley
Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of
Statistics, pp. 547-561. University of California Press, 1961.
Alexandre B. Tsybakov. Introduction to Nonparametric Estimation. Springer Publishing Company,
Incorporated, 1st edition, 2008. ISBN 0387790519.
Martin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Series
in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019.
Stephen Walker. New approaches to Bayesian consistency. The Annals of Statistics, 32, 2004.
Yu-Xiang Wang, Stephen Fienberg, and Alex Smola. Privacy for free: Posterior sampling and
stochastic gradient monte carlo. In Proceedings of the 32nd International Conference on Ma-
chine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 2493-2502,
Lille, France, 07-09 Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/
wangg15.html.
M. Welling and Y. Teh. Bayesian learning via stochastic gradient langevin dynamics. In ICML,
2011.
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Open problem: Can single-shuffle SGD be better than
reshuffling SGD and gd? In Conference on Learning Theory, COLT, 2021.
Zuhe Zhang, Benjamin I. P. Rubinstein, and Christos Dimitrakakis. On the differential privacy of
bayesian inference. In AAAI Conference on Artificial Intelligence, 2016.
11
A SGLD and Posterior Privacy
Proof Theorem 1. Define
1
2 > γι > 0;
3
2 > Y2 > 1 + Y1 ； Xl
Xh
^2^
ν1
2ln( 1)
+1
n1 = max{ 2θ⅛
1
xhβ,
aa
Xhe, XQ
ɪ	1
e xhβ - 2)+/}
max{1 + x2 8, 1 +
Xl2
16νβ ((Xhe)(Xha + Xhe)
n2
1+8
)(1+
(V - 1)) (16VeXh
)1—2γ1,
_2
(1 + 10 ⅜ V)
____1
------))2-γ1
γ2 -γ1
γ2
(4ν ((Xha + Xhe)2
100 x6 β
)(1+
,	χ2	,
(1 + 10X2 V)
2
------))3-2γ2 }
γ2 -γ1
—
α
(


1
1

nɜ = max{1 + 10 Xh —, 1 + V
Xl2 e
r	// /	、/C 「、、-4- ∕32X2β、2 2vι、____1 、
np = max{nι, n2, n3, ((e0 — ln(0.5 — δ))eX e (——h-)2-) 2(γ2-1) }
3a
1
vι = max{6,1 + 2eXhe }
cp = nγp2
We consider the Bayesian linear regression problem over database D1 (defined in eq. 7) with n = np
and c = cp. Since np > n1, the problem holds the constraints of lemma A.6. Consequently, there
exists a step for which one approximate sample from the posterior using SGLD is not (00, δ) private
for all 00 such that 00 ≤ e
promises that 0 ≤ e


2
x2β .a
2vι
2
x2β 2at(323β)2(np)2 + ln(0.5 - δ). From eq. 10, the choice of np
-(32X2β)2(np)2 + ln(0.5 - δ); Therefore approximate sampling from
the posterior using SGLD is not (0, δ) differentially private.
Since np > n2 and np > n3, the problem holds the constraints of Claim D.30. Therefore one
sample from the posterior is (, δ) differentially private.
e
xhβ V (3⅛ )2( np )2+ln(0∙5 - δ) "0
(红)2 ≥ (e0 - ln(0.5 - δ))exhβ (32Xhβ)2 2v1
np	3 a
n2(Y2T) ≥ © - ln(0.5 - δ))ex22β (3⅛β)2 2v1
p	3a
np ≥ ((J - ln(0.5 - δ))e干(32Xhe )2 2v1) 2M-1)
3a
(10)
□
A. 1 Posterior Sampling Privacy
Proof Lemma 4.1. eq. 11 is a known result for the Bayesian inference problem for a linear model
with Gaussian noise with known precision parameter (β) and a conjugate prior (Bishop (2006) -
3.49-4.51. for details). By choosing the basis function to be φ(X) = X, working in one dimension,
12
and choosing m0 = 0, S0 = α-1, we get the linear model defined in eq. 4 and matching posterior
described in Lemma 4.1.
p(w|t)	= N(w; mN,	SN); mN	=	SN(S0-1m0	+ βΦTt);	SN-1	= S0-1	+ βΦTΦ	(11)
□
Proof Lemma 4.2. By definition 3, for a single sample from the posterior to be (ν, 0) RDP, the
Renyi divergence of order V between any adjacent databases needs to be bounded. We consider two
adjacent databases D, D ∈ D, and w.l.o.g, define that they differ in the last sample (where it is
also allowed to be (0, 0) for one of them, which saves us the need to consider also a neighbouring
database with a size smaller by 1). To ease the already complex and detailed calculations, we use
definitions in eq .12.
D = {xi, yi}i=1 ∪ {xn, yn}, D = {xi, yi}i=1 ∪ {xn, yn}
n-1	n-1
z =	xi2 , q =	yixi
i=1	i=1
According to Lemma 4.1 and with definitions in eq. 12, the posterior distributions are
p(θ∣D)=N(θ; μ,σ2); μ =。" +吆/ 常
1
α + (z + xn)β
p(θlD)=N (θ; μ,σ2); μ = Ow+⅛⅛ ； σ2 = αr⅛n>
(12)
(13)
By Gil et al. (2013), the Reyni divergence of order ν, - DV(fι ∣∣f2) - for fι, f2, uni-variate normal
distributions with means μι, μ2 and variances σ1,σ2 accordingly, is
DV (f1llf2 )=ln σ2 + 1(V- 1)lnf⅛ + 1 V (μ(1σ-)μ2)
(σfι,f2 )V = νσ2 + (I- V)σ2 > 0
EI	Γ∙	Γ∙	/∕^kl7^Λ∖	∙>∕∕^kli∖∖,ICΓ∙ 1	♦	1	♦	Λ Λ t
Therefore, for p(θ∣D) and p(θ∣D), the Renyi divergence of order V is as shown in eq. 14, where we
omit the subscript for (σ2)V since it is clear from context to which distributions it applies.
DV(p(θlD)llp(θlD)) = ln^ + 2(V -1)ln(σy + 2 :©2；
(σ2)V = Vσ2 + (1 - V)σ2
(14)
According to claim D.25, (σ2)V > 0. Therefore the value DV(p(θ∣D),p(θ∣D)) exists. In order to
prove Renyi differential privacy, each of the terms of DV(p(θ∣D),p(θ∣D)) is bounded separately.
The bounds on each of the terms are proved at claims D.26,D.27, and D.28.	口
Proof Lemma 4.3. By Lemma 4.2, sampling from the posterior is (V, 1)-RDP, therefore by Lemma
3.1, sampling from the posterior is also (∈ι + In-JI), δ) differentially private.	口
13
A.2 Stochastic Gradient Langevin Dynamics Privacy
Proof Lemma 4.4.
n
p(θj > μj|D2) = Ep(Ij > μjlD2)p(θj = θj |D2)
r=1
n
Σp(θj - μr > μj - μr D2)p(% = θ ∣D2)
=1
1n
n Epej- μj > μj- μj ιD2) ≤
=1
1S ( (μj-μj)2
n之exp(-下厂
)
Where the inequality holds due to Chernoff bound (For further details, see Wainwright (2019)).
□
ProofLemma 4.5. BylemmaA.5, for n > max{ Xae, Xae (e干-2)+ ^^, ^χ1Tβ -熹} eq. 15
h	2h	h
holds for some k ∈ R>o. We can see that this lower bound is dominated by n, therefore proving
Lemma 4.5.
(μ(dk e + 1)n - %k ] + 1)J ≥ e-亲 α (	3 )2( C )2
(%ke+i)n)2	-	vι 32xhβ n	(15)
1
vι = max{6,1 + 2exhβ }
□
2
ProofLemma 4.6. By Lemma A.6, for n > max{凛,Xae (eXhe - 2) + 2^, 20^ -皋}, there
exists T ∈ Z>0 (Marked in Lemma A.6 as dke) such that running SGLD for the Bayesian linear
regression problem over D1 for T steps will not be (, δ) differentially for < 0, as defined in eq.
2
16, and δ < 0.5. Since e0 is dominated by 3,this proves the lemma.
,	__2— α ,	3	、。/c、0	, ,	c、
e = e x2β 厂()2(-)2 + ln(0∙5 - δ)
2v1 32x2hβ	n
1
vι = max{6,1 + 2exhβ }
(16)
□
A.3 Stochastic Gradient Langevin Dynamics Detailed Analysis
In order to ease the analysis of the SGLD process, markings in eq. 17 are used. xh,α, β,c, n are as
defined for the Bayesian linear regression problem, and η is defined in subsection 4.2.
λ = [1 - 2(a + nxhβ)], W = [1 - 2(α + n(χh)2β)], P = 2ncxhβ, P = 2nc(Xh)2β	(17)
2	2	2	2	22
Lemma A.1. The forms of θ(rk+1)n are
k	n-2	n-
θ1k+1)n = θθλk + 1λ("-1)(k+1) + X(MnT) j[ρλn-1 + ρ χ λi + √ X λiξi]
j=0	i=0	i=0
，*)n=θo(λλ--ι)k+ι+
r-1	n	k
(X(P + √ξξ)λλ"-i-1 + (P+ √ηξ)λn-r + X (ρ + pξη)λn-j)X(λλ"-1)l.
i=1	j=r+1	l=0
14
Proof Lemma A.1. Welling & Teh (2011) define the SGLD update rule as in eq. 3. This rule can be
applied to the Bayesian linear regression problem over databases D1, D2 as following
p(θj) = N(θj；0, αT) ⇒ lnp(θj) = ln( /c 1 _1) - 1 θjα ⇒ vθjp(θj) = -θjα
2πα-1	2
p(yi∣θj) = N(yj;θjχi,β-1) ⇒ lnp(y∕θj) = in(
⇒
R θj p(yi∖θ j ) = (yi - θj Xi)Xie ⇒
.
θj+1 = θj + 2 [-θjα + n(yi - θjXj)xiβ] + √ηjξi =
θj [1 - 2(α + nx2β)] + 2nyiXiβ + √ηξj =
θj [1 - 2(α + nx2β)] + 2ncχ2β + √ηξj
By using standard tools for solving first-order non-homogeneous recurrence relations with variable
coefficients, the value of θn1 can be found.
八 T	ʌ	T .
^n=^λn-1(
一 C .	—.
θoi+ρ+√⅞
X ρ + √ηξ.
+ i=2 =r)
ʌ
λ
n
θ0*λn-1 + (ρ+ √ηξ)λn-1 + (ρ + √ηξ) X λn-1-(i-1) =
i=2
n
θo*λnτ + (ρ+ √ηξ)λn-1 + (ρ + √ηξ) X λn-1-(i-1) =
i=2
n-2
θo"nτ + (ρ+ √ηξ)λn-1 + (ρ + √ηξ) X λi =
i=0
n-2	n-1
θo"nτ + ρλn-1 + P X λi + √ηξ X λi.
i=0	i=0
Now by defining a new series -6除+切 =c∖θ^kn + c2, and using the tools for solving first order
non-homogeneous recurrence relations with constant coefficients, the value of θk1n can be found
^1	k	k
θin = Ck (θn+X c2 ) = θn CkT+X c2ck-i =
c1	i=2 c1	i=2
k-2	k-2
θn1 C1k-1 + C2 X Ci1 = (θ0C1 + C2)C1k-1 + C2 X Ci1 =
i=0	i=0
n-2	n-1	k-1
θ0(Wλnτ)k + (ρλn-1 + P X λi + √ηξ X λi) XMλnτ)j.
i=0	i=0	j=0
The proof for θ^ is done in similar manner.	□
_	O	_ _ , O	, .
Corollary A.1. θrk + 1)n 〜N(θrk+1)n; μrk+1)n, (6rk+1)n)2)-
LemmaA.2. 〃2-μ^ ≥ λn-1 0⅛入协-1)*+1 - λk+1).
Proof Lemma A.2. The proof of this lemma is separated into two cases, for r = 1 and for r > 1.
For r = 1, it is easy to derive eq. 18 from lemma A.1, using E[θ0] = 0 and E[ξ] = 0.
n-2 k	k
μ1k+i)n = P X λi X(λλ(n-1))j + ρλn-1 X(^λ(I))j
i=0 j=0	j=0
n-2	k	k
μkn+n = P X λ X λjn + ρλn-1 X λrn
(18)
15
We use the sum of a geometric sequence to get
Λ1
μ(k+1)n
n—2 k	k
P X λi X(λλ(n-1))j + ρλn-1 X(λλ(n-1))j
i=0 j=0	j=0
"-λn-1.	ʌʌn-lʌ 1 - (λλn-1)k+1
(P(Fɪ )+ ρλ )	1 - Λλn-1
Therefore the difference between the means can be lower bounded:
μkn+n - μkn+n =
1 - Λ(k+1)n	1 - λn-1	1
[ρ(Tɪ)+PλnT]-
1 - (λλ(n-1)
ʌ .
1 - λλn-1
)k+1-1 - λn-1
[P(Zɪ
)+ ρλn-1] =*
1 - x(k+1)n ncχ2β (1 _ λn) _ 1 - (λλ(nT))k+1 ncχ2β (I _ λn-1(Iλ + 3))
1 — λn α + nxhβ	1 - ʌʌn-1	Q + nχh β	4	4
λ _ ∖(k+1)n∖ ncχ2β
(1 - ʌ ) α + nχhβ
1 - (Λλ(n-1))k+1 ncx2β	—1 1 - 3
―1 - »-1 — α + nχhβ (I - ʌ	(4ʌ + 4))
I-WnT))k+1 (1 - ʌn-1(1 λ +3))]=**
1 - λλn-1	'4	4〃」
—
ncχ2β [(1 - λ(k+1)n)-
Q + nχhβ [(I A )
ncχ2β (λn-132α(1 - Ak+1A(k+1)(n—1)) + A(k+1)(n—1)(Λk+1 - λk+1 )(1 - λn-1λ)
Q + nχ^β'	1 - An-1λ
ncχ2β (A(k+1)(n—1)(λk+1 - λk+1)(1 - λn-1λ))_
Q + nχhβ1	1 - An—1Λ
ncχ β A(k+1)(n—1) (^k + 1 - χk+1) =
Q + nχhβ
〉
λn-1
ncx2 β
λk(n-1)(^k+1 - λk+1)
where equality * holds from claims D.1, D.2, D.3, equality ** holds from claim D.5, and the in-
equality holds because λ < λ < 1. This proves Lemma A.2 for r = 1.
For the case of r > 1, from Lemma A.1 it is easy to see
r—2	r—2
愣 1)n = MT-1 + P X Ai + √η X λiξ""k(nT) +
i=0	i=0
k—1	n—2	n—1
X(AAn-1)j [PAn-1 + P X λi + √η X λiξi]] AAn-r + .
j = 0	i = 0	i = 0
n—r—1	n—r
PAn—r+P X λj + √η X ξλj
j=0	j=0
Therefore μk>L1 follows
kn+n
μrk+1)n
r—2	k—1	n—2	n—r—1
[[pX Ai]AkAk(n-1) + X(AAn-1)j[PAn-1 + P X Ai]]AAn-r + PAn—r + P X λj
i = 0	j = 0	i = 0	j = 0
16
Consequently the difference in means for r > 1 can be lower bounded:
μkn+n - μkn+n
r—2	k—1
n—2
λn-r [λρλkλk(n-ι'> £ λi + λ E(λλn-ι)j (ρλn-1+ρ £ λi)-
r —2
^ρ^kλk5τ) X λi
i=0
i=0	j=0	i=0
k—1	n—2
-ʌ X(λλn-1)j(ρλn-1 + P X λi)] + λn-r (ρ - P) =
j=0	i=0
r—2	k—1
λn-r λk(nτ)ρ(λk+1 - λk+1) fλi + λn-r Eλ(nTj[λnT(ρλj+1
—
i=0
n—2
j=0
P第+1) + (λj+1 -第+1)p E λi] + λn-r (p - P)
*
i=0
1 — ʌr-1
λn-rλk(nτ)ρ(λk+1 - λk+1) —l~— + λn-r
1-λ
ncx
[λ(1 - λkn)-
T1 -(λn-?)k (1 - λn(3λ-1 +1))] + λn-r(ρ - P) =**
1 - λn-1λ	4	4
λn-rλk(nτ)ρ(λk+1 - λk+1)
λn-1[4 2α(1 - λkλk(n-1))]
1 - λλn-1
TF + λn-r
1-λ
ncx
[(λ - λ)+
+ λk(nT) (λk+1 - λk+1)] + λn-r (ρ - P)
***
λn-r
ncxhβ C λk(n-1)(λk+1 - λk + 1)(1 - λr-1 ) + λn-r
Q + nxhβ
ncx
—
+
λn-1[42α(1 - *kλk(nτ))]
ʌ .
1 - λλn-1
+ λk(n-1)(λk+1 - λk+1)] + λn-r(ρ - P)=
λn-r
λn-r
λn-r
λn-r
λn-1
λn-r
λn-1
ncxhβ
Q + nxhβ
ncxhβ
Q + nxhβ
ncxhβ
Q + nx»
ncxhβ
Q + nx»
ncxhβ
Q + nxhβ
ncxhβ
Q + nxhβ
ncxhβ
Q + nx»
λk(nτ)(λk+1 - λk+1)[1 - λr-1 - 1] +
, ʌ
(λ - λ +
, ʌ
(λ - λ +
λn-1[420(1 - *λk(nτ))]
ʌ .
1 - λλn-1
-λk+1)λr-1+
)+ λn-r(ρ- P)=
λn-1[42α(1 - *λk(nτ))]
λk(n-1)(λk+1
ʌ .
1 - λλn-1
-λk+1)+
)+ λn-r(ρ- P) =
, ʌ
(λ - λ +
λn-1[42α(1 - λkλk(n-1))]
ʌk(n-1) (^k+1
ʌ .
1 - λλn-1
-λk+1)
)+ λn-r(ρ - P) >****
where equality * holds from claims D.6 and D.7, equality ** holds from claim D.10, equality ***
holds from claim D.1, and equality **** holds from claim D.11 and λ > λ.	□
17
Lemma A.3. For Xihe > 3,n > ^χτβ 一 χ1β, ∃k ∈ R+ such that upper bounds defined in eq. 19
hold for all 0 < k ≤ k:
(σ1k+1)n)2 ≤ 2(HnT)21(HnT)2k
α1	(19)
(σrk+1)n)2 ≤ 6(λλn-r)2τMnT)2k.
ProofLemmaA.3. The proof will be separated into two cases, r = 1 and r > 1.廿需十九)2 can be
easily computed from lemma A.1 using the fact that both the noise and prior are distributed normally.
A first general upper bound on 向晨十九)2 is found at eq. 20.
n-1	k
(σ1n+n)2 = 1(λλ(n-1))2(k+1) + η X λ2i X(^2λ2(nT))j
α	i=0	j=0
(^λ(n-1))2(k+1)[1 + ηXλ2iX ,("(nτ))2j ]=
α	∕⅛0	j=0 (nd))22)」
n-1	k
(^λ(n-l))2(k + l) [1 + η X λ2i X(^λ(nT))2(j-(k+1))] ≤
α	i=0	j=0
n-1	k
(^λ(n-1))2(k + 1) [1+ η X / X λ2n(j-(k+1))]=
α	i=0	j=0
(k+1)n
(HS-1))2* + 1)[1+ η X λ-2i]=
α	i=1
(^λ(nτ))2(k+1)[1 + ηλ-1 -：：；1)”]
α	1 一 λ-2
(20)
where the inequality holds because λ < λ
By claim D.12, this upper bound can be further bounded for k ≤ ɪ logλ(1十,丁富))一 1 SUch
that eq. 21 will hold, therefore proving the bound for r = 1.
r	r	、— O”_L1 ∖h	r
(λλ(nτ))2(k+1)[1 + ηλ-2--_ — ] ≤ 2("(nτ))2(k+1)-
α	1 一 λ-2	α
(21)
For r > 1,⑹：；η)2 can be bounded as following
18
(σkr>n+1 n)2 =
r-2	k-1	n-1
(λλn-r)2η[(^kλk(n-1))2 X λ2i + £次》n—1/ X λ2i] +
i=0	j=0	i=0
n-r
η X λ2i + 1(^λn-1)2k(^λn-1)2 ≤*
α
i=0
r-2	k-1	n-1
(λλn-r)2η[(^kλk(nT))2 X λ2i + £5》n-1/ X λ2i] +
i=0	j=0	i=0
n-r
η X λ2i + 1αλnT)2k (λλn-)2 =
i=0	α
r-2	k-1	n-1	n-r
(λλn-r	)2[1(^λn-1)2k + 〃*n—1产 X λ2i + η £(“n-1/ X λ2i]+ η X λ2i ≤**
α	i=0	j=0	i=0	i=0
n-1	k-1	n-1	n-r
(λλn-r)2[1(MnT)2k + η(λλn-1)2k X λ2i + ηX(MnT)2j X λ2i] + ηX λ2i ≤***
α	i=0	j=0	i=0	i=0
n-1	k-1	n-1
2(Un-r )2 [1(*λnτ )2k + ηαλn-1)2k X λ2i + η X(WλnT)2j X λ2i]
α	i=0	j=0	i=0
(22)
where inequality * follows from λ < 1and r > 1, inequality ** follows from r ≤ n, and inequality
*** holds from claim D.15.
For k ≤ 2n logλ( ι+^1ι-λ2)) — 1，this bound Can be further developed
n-1	k-1	n-1
2(*λn-r)2[1(^λn-1)2k + η(λλn-)2k X λ2i + ηX(λλn-1)2j Xλ2i] ≤
α	i=0	j=0	i=0	.	(23)
6(Tλn-r )21(λλn-1)2k
α
The inequality holds from claims D.12, D.14, which provide the bound for r > 1. All that is left is
to prove that * logλ( ι+^'-λ？)) — 1 > 0, which is done in Claim D.22.	口
Lemma A.4. Mark k = * logλ (
1
) — 1, for the conditions of Lemma A.3
1+On (1-λ2)
(σ1ken+n)2 ≤ (1 + 2e忐)(MnT)21(λλ(n-1))2dke
Wn+n)2 ≤ 6(Hn-r)2 1MλnT)2dke.
Proof Lemma A.4. This proof will be separated into two cases, for r > 1 and for r = 1. For r > 1,
the bound found in eq. 22, has no dependence on the choice of k, therefore holds also for dke. This
bound was, in turn, developed for k at eq. 23 using three claims. If these claims also hold for dke,
then the bound in eq. 23 also holds for dke, and the lemma is proved for r > 1.
Claims D.14, D.12 hold for all k ≤ 2n logλ( “’ 113)), and since dkk] ≤ k + 1 =
2n logλ( 1+, 11-λ2)), they holds fordk]. Claim D.15 was proved for all k, hence also holds for
dke.
19
For r = 1, the bound found at eq. 20 is applicable for all k, hence
r	r 、 「谓 I 1	_
(σ(dke + i)n)2 ≤ (MnT)2(dke + 1)[ 1+ ηλ-2 1 -i-λ-2	] ≤ (MnT)2(dke+1) 1(1 + 2ex1β)
where the last inequality holds from claim D.17.
Lemma A.5. For k defined in Lemma A.4,
2
max{-2⅛, -2⅛(e xhβ — 2) + 1r^}
x2hβ, x2hβ	2x2hβ
(〃「k]n+n - /]n+n)2 ≥
-(σdken+n)2- ≥
1
vι = max{6,1 + 2exhβ }.
□
the conditions of Lemma A.3, and n >
-xhβ α (ξ⅛ )2(C )2
v1 32x2hβ	n
Proof Lemma A.5.
Men+n -/en+n)2〉(λn-1 Oc^λdke(nτ)adke + 1- λdke + 1)/
°「k e n+n	≥	V1(*λnτ )2 1 (λ<λn-1)2dk e
λ2dke(n-1) (λn-1 nXhe§ (£「ke+1 - λdke+1))2
v1(λλn-r )2 α (*λnτ)2dk e
T 0+⅜ Nk e+1- λdk e+1))2
V1(*λn-r )2 1 *2dk e
αλ2(r-1)	ncxhβ	2 (Xdke+1 — λdke + 1)2	αλ2(r-1)	ncx]lβ 2∩	Xdke+'〉
V	1	α + nxhβ	^2dk e + 1	V1	α + nxh β	^dk e+1 —
αλ2(rT) ( ncxhβ )2(1 - (1___________4nxxβ___________
V	1	Q + nxhβ	(Q + nxhβ)2 — (Q + 1 nx2β)
αλ2(rT) ( ncxhβ )2(_________4nxhβ___________)2 ≥
V	1	Q + nxhβ	(Q + nxhβ)2 — (Q + 1 nx2β) -
αλ2(TT)	ncxhβ	2/	4nx2β y	Qλ2(r-1) ncx2β 22	3nx2β	2
V	1	Q + nxhβ (q + nxhβ)2	- V1	2nxhβ	(2nx2β)2
Qλ"T) (C)2(_3_)2 = Qλ"T) (_3_)2(C)2 ≥
V	1	v 2、4nxhβ	V1	' 32Xhe yn 一
Qλ2(n-1)	3	2 c 2
一v1	(32xhhβ)(n) ≥ e

χ2β Q 3 x2∕ c、2
v1 (k)(n)
where first inequality holds from A.3, A.4 and the definition of v1, the second inequality follows
claim D.17 and claim D.22, fourth inequality holds under the assumption of nxhhβ > Q ^⇒ n >
-α⅛, and last inequality holds from claim D.19.	口
xh β
Lemma A.6. For the Bayesian linear regression problem over database D1, the conditions of
Lemma A.5, and k, as defined in Lemma A.4, approximate sampling, by running SGLD for (dke+1)n
steps, will not be (, δ) differentially private for
δ < 0.5, e < e-熹 2q7(32⅛3hβ)2(C)2 + ln(0.5 - δ)
1
V1 = max{6,1 + 2eXhe }.
Proof Lemma A.6. According to definition 1, it is enough that there is one group, S, such that
p(θ(dk e + 1)n ∈ S DI) > e'p(θ(dk e+1)n ∈ SlD2) + δ, to ShOW that releasing θ(dk e + 1)n is not
(gδ) private. Consider S = {s∣s > μ°ke+1)n}. From claim D.23 and since θ0ke + 1)n ~
N(θ(dke + 1)n； μ°ke + 1)n, σ2dke+12), eq. 24 holds. The conditions for the right term to be smaller
20
than 0 (thus making the approximate sampling not (, δ) private) are found in eq. 25, therefore
proving the lemma.
-e
e p(6\「ke + 1)n ∈ S|D2) + δ - P(θ(dke + 1)n ∈ SIDI) ≤e e
()2( C )2
2v1 32xhβ	n + δ - 0.5 (24)

-e
e
2
x2β ɪ ( —J )2 (C )2
2v1 32xhβ n + δ - 0.5 < 0
F-e- X2β -α(_3^)2 (c )2
e	2v1 (32xhβ)(n) < 0.5 - δ
-e
熹高(32⅛)2(C)2 < ln(0.5 - δ)
(25)

e<e-鼎靠 (32⅛)2( C )2 +ln(0.5 - δ)
□
B Propose Test S ample S upplementary
nmin which is used in algorithm 4.3 is defined as following
nb1
2ln( 1) +1
E
X2h 8
max{1 + —2—, 1 +
Xl2 E
1+8
(V - 1)) (16VeXh
1
)1—2γ1,
32νβ ((Xhe)(Xha + xhβ)
-	1
)τm) 2-γι ,
32νβ ((Xhe)(Xha + Xhe)
1
))2—2γ1,
(8ν ((Xha9+6至)m) 3,
E	10 Xl β
(8ν ((Xha + Xhe)2)) 3⅛}
("(10 席 ))	}
X2h 10V
nb2 = max{1 + #-ɪ, 1 +
nmin = max{nbi,nb2,nι γ2 }
C Propose Test S ample Privacy
(26)
Proof Claim 4.2. We set the algorithm parameters in eq. 27, and matching databases D3, D4 defined
in eq. 8. We note that we only define a lower bound over n1, which will be updated later on.
ρ3 = 1.15; ρ2 = 0.45; ρ1 = 1.25, γ1 = 0.49;
Xl = Xh/2
nι > max{21 log *,41 log *,210ρ1,29+10ρ2}
2δ	2δ
e = 3; Xh = 1
a=1
(27)
Mark the return value of the algorithm as r, the event of the algorithm running on database D3 and
W = D3 as AD3, the event of the algorithm running on database D4 and W = D4 as AD4, and
ν
(
(



21
S = {s∣s > μi}, where μ% is the mean of the sample distribution at the SGLD i'th step given
database D3 (Similarly to as defined in subsection 4.2). We will show that ∀e ∈ R>o, δ < 1, ∃n1
such that eq. 28 holds.
P(r ∈ S∣D3) > eeP(r ∈ S∣D4) + δ
We first show that
P(r ∈ S ∧ AD3 ∣D3)=0
P(r ∈ S ∧ AD4 ∣D4)=0
(28)
(29)
Notice that the algorithm can return result in S only if it reached step 19. Consider an event where
the algorithm reached step 19 and AD3. From AD3, ∃(χi,yi) ∈ D3 such that |Xyi - m| ≥ 喟.
However, since ∀(xi, yi) ∈ D3 : Xi = nρ3 then ∀(xi,yi) ∈ D3 : |Xyi - m| > np and therefore
| W| = 0. Under the assumption that sample fromp(θ∣{}) returns null then in this case the algorithm
also returns null and therefore P(r ∈ S ∧ AcD |D3) = 0. Same arguments hold for D4.
Following eq. 29, to prove eq. 28 it is enough to prove eq. 30.
P(r ∈ S∣D3,A3)P(A3∣D3) ≥*
P(r ∈ S∣D3, A3) - 5δ >** eeP(r ∈ S∣D4, A4) 十 δ ≥
e'P(r ∈ S∣D4, A4)P(A4∣D4) + δ = eP(r ∈ S ∧ A4∣D4) 十 δ
(30)
From claim C.1 ∃nbound1 such that ∀n1 > nbound1 inequality * holds. From Lemma 4.6, for n1
big enough ∃T ∈ Z>0 such that eq. 31 hold (Where 6δ < 0.5 according to the claim conditions).
Therefore, ∃k, nbound2 ∈ R>0 such that ∀n1 > nbound2 : 0 > kn21(1-ρ3) and eq. 31 hold. As
1
P3 > 1, by choosing nι > max{nbound2, (∣) 23-1) } get that e0 > e. Consequently, by choosing
1
nι > max{nboundι, nbound2, (∣) 2(ρ3-1) }, inequalities * and ** hold, and the claim is proved.
∕ 二 Ω(n2(P3T))
P (r ∈ S∣D3, A3) > J P (r ∈ S∣D4, A4) + 6δ
(31)
□
Claim C.1. ∃nbound1 ∈ Z>0 such that the probability for algorithm 4.3 to reach step 19 with
W = D3 (marked event A) is greater or equal to 1 - 5δ for all n1 > nbound1 .
Proof. Mark the event of nw > nmin ∧ m ∈ [m — np2, m + np2 ] ∧ n1+p2-0.1 > np1 ∧ nι ≤
n1 ∧ V = D as B. Since P(A|D3, B) = 1 it follows that P(A|D3) ≥ P(A ∧ B|D3) =
P(A|B, D3)P(B|D3) = P(B|D3). Therefore if ∃nlb such that ∀n1 > nlb : P(B|D3) ≥ 1 - 5δ
the claim is proved.
P(B|D3) =
P(m ∈ [m — np2, m + n§2 ] ∧ nw > nmin∣D3, V = D, n2+p2-0.1 > np1, nι ≤ nj
P(n2+ρ2-0.1 >np1∣v = D,nι ≤ n1,D3)P(V = D,nι ≤ n1∣D3) ≥
P(m ∈ [m — np2, m + n§2] ∧ nw > nmin∣D3, V = D, n1+p2-0.1 > np1, nι ≤ nɪ) — 3δ 二
P(nw > nmin∣D3, V = D, n1+p2-0.1 > np1, nι ≤ nɪ, τm ∈ [m — n§2, m + np2])
P(m ∈ [m — ng2, m + np2]∣D3, V = D, n1+p2-0.1 > np1, nι ≤ nɪ) — 3δ ≥
P(nw > nmin∣D3, V = D, n2+p2-0.1 > np1, nι ≤ nι, m ∈ [m — ng2, m + ng2]) — 4δ ≥
1 — 5δ
(32)
22
By corollary C.1 and claim C.3 for n1 big enough first inequality holds. By claim C.4 for n1
big enough second inequality holds, and by claim C.5 for n1 big enough third inequality holds.
Therefore for n1 big enough eq. 32 holds and the claim is proved.	□
Claim C.2. For n1 > max{ ɪ 10ρι, 4 ɪ log 2δ }
P(np1 ≥ np3 八 n1 ≤ nι ∣D3) ≥ 1 - 2δ.
Proof. Mark the noise added at step 6 as lι
P(np1 ≥ np3 八 n1 ≤ n1∣D3)=
P((nι + lι - 1log71)p1 ≥ np3 八 n1 + lι - 1log ɪ ≤ n1∣D3)=
e 2δ	e	2δ
P((nι + 11 - ɪlogɪ)^1 ≥ nρ3 ∧ l1 ≤ ɪlogɪ∣D3)=
€	2δ	e	2δ
P((nι + lι - ɪlogɪ)^1 ≥ nρ3 ∧ ∣lι∣ ≤ ɪlogɪ∣D3)+
€	2δ	€	2δ
P((nι + lι - ɪlogɪ)^1 ≥ nρ3 ∧ lι ≤ -ɪlogɪ∣D3) ≥
€	2δ	€	2δ
P((n1 + l1 - - log 与P1 ≥ nl3 ∧∣l1∣ ≤ Log X7∣D3) =
€	2δ	€	2δ
P((n1 +11 - ɪ	logχτ)ρ1	≥ nι3∣∣l1∣ ≤ Log	57,d3)p(Il1∣	≤ Log	⅛∣D3)	≥
€	2δ	€	2δ	€	2δ
P((n1 - 2； log2δ)ρ1 ≥ np3 ∣D3) - 2δ =1 - 2δ
Where last inequality holds from following equation
P(∣l1∣ ≤ L log ɪ) = 1 - 2p(l1 ≤ --log ɪ) = 1 - exp(-ɪg~2δ~) = 1 - 2δ
€	2δ	€	2δ	-
e
Last equality holds since n > ɪ 10ρ1 ⇒ n[1 < (ɪ)10ρ1 ⇒ n[0∙1 < (ɪ)ρ1 and therefore
(n1 - 2ɪ log 2δ)ρ1 > (2n1)p1 > np1-0∙1 = np3	□
Corollary C.1.
Vn1 > max{3	, 4工 log W} : P(V = D3 八历 ≤ n1) ≥ 1 — 2δ.
2	€	2δ
Claim C.3. For n1 > max{ ɪ (9+10ρ2), 4ɪ ]0g _!_}
P(n1+p2-0∙1 >np1∣D3,V = D3,n1 ≤ n1) ≥ 1 - δ.
Proof.
P(n0-9+p2 > np1 ∣D3, V = D3,n1 ≤ n1) ≥ P(n0-9+p2 > np1 ∣D3, V = D3) ≥
P((n1 - 21 logl)0∙9+p2 > np1 ∣D3)p(n2 ≥ ∣V∣- 21 log ɪ) ≥
€	2δ	€	2δ
P((n1 - 2： log 2δ严+p2 > np1 ∣D3) - δ =1 - δ
where the second inequality holds since P(Lap( -) < - ɪ log __) < δ, and last equality holds since
(n1 - 2ɪ log __)0-9+ρ2 > (_m)0-9+ρ2 > n1+p2-0∙2 = np1	□
Claim C.4. ≡nzfe1 ∈ Z>0 SUCh that
Vn1 > n® : P(m ∈ [m — np2, m + np2] ∣D3, n_.9+p2 > np1) ≥ 1 — δ.
23
Proof. Mark the noise added at step 13 as l1
P(m ∈ [m — np2, m + np2]∣D3, n2.9+p2 > np1)=
P(lι ∈ [—np2,np2]D3,n0.9+ρ2 >n11) ≥
1 — 2(! exp(—nρ2----———1 2 2——)=
12	12	1 方pi 2(n2-I)Xhx2+χh '
e "1	n2(n2-l)x4
1 — exp( — —n1+0Hn2 - 1)x4，)
p( nP1(2(n2 - i)χhχ2 + Xh))
Since n0.9+P2 > nρι then nρ1 = o(n2+ρ2) and therefore for n1 big enough the exponent is smaller
than δ.	口
Claim C.5. ∃nlb2 ∈ Z>0 such that
∀ni > nib2 : p(nw > nmiη∣∣V| = D3 ∧ n2.9+p2 > np1 ∧ m ∈ [m — np2, m + n22], D3) ≥ 1 — δ.
Proof. For abbreviation mark event B as B = | V| = D3 An2.9+P2 > np1 ∧m ∈ [m—np2, m+np2].
Mark the Laplace noise used in step 8 as l1 and the Laplace noise used in step 15 as l2.
P(nW > nmin|B,D3)
P(ni----log 7T7 + l2 > nmin∣B, D3) >
2δ
P(ni — ɪlogɪ — 1log1 > nmin∣B,D3)P(l2 > — 1log1)+
2δ	δ	δ
P(ni----log 7T7 + l2 > nmin ∧ l2 <-log飞|B, D3) >
2δ	δ
P(ni - ~ log 2δ	- ~ log	j >	nminlB, D3)(1	- 2)	>
P(ni - ~ log 2j	- ~ log	J >	nmiη∣B, D3) -	2 =
2	111
P(ni - - log 777 - - log 7
2δ	δ
2	111
P(ni - - log 777 - - log T
2δ	δ
2	111
P (ni — TOg^ — TogM
2δ	δ
2	111
P (ni — TOg^ — TogM
2δ	δ
>n
>n
miη∣li < — log T, B, D3)P(II < — log T |B, D3) +
δ	δ
min ∧ li ≥ — log 7lB, D3) - % ≥
δ	2
> nmin∣li < - log J, B, D3)P(II < - log J |B, D3) - 2 ≥
> nmin∣li < 一 log T, B, D3) - δ
-J
From B it holds that |m — m| < nρ2 and therefore Tm < m + nρ2, and for the case of li < ɪ log δ
it holds that n2 < ni — ɪ log 表 + ɪ log i < ni + ɪ log ∣. Therefore m ≤ m +(ni + ɪ log δ)ρ2.
As nmin = O(max{m2, nγι }) then for the case of li < ɪ log ∣ and B, it holds that nmin =
2 P2	2ρ3	P2
O(max{(m + nρ2)3,nγι }) = O(max{ni3 , nγι }) < o(ni), therefore mn® such that ∀ni >
nib2 : ni — I log 2δ — ⅛ log i > nmin. Consequently, YnI > n® : P(ni - 2 log * 一 ⅛ log ⅜ >
nmiη∣li < I log i,B,D3) = 1.
□
Definition 4. A randomized function f(X, y) : χn1 × Rn2 → R, is (, J)-differentially private with
respect to X if ∀S ⊆ R, and ∀X, X ∈ Xn : kX — X∣∣ ≤ 1,eq. 33 holds.
, . , ʌ
P(f(X, y) ∈ S) ≤ exp(-)P(f(X, y) ∈ S) + δ	(33)
24
Claim C.6. Calculating 'n1,n2 is (2e, 0) differentially private.
Proof. Since nι can differ by UP to 1 for neighbouring databases, calculating n 1 is protected via the
Laplace mechanism. Since for a given % the value | V | can change by up to 1 for two neighbouring
databases then calculating n2 is (, 0) by the Laplace mechanism. Consequently from sequential
composition theorem the sequential composition is (2e, 0) differentially private.	□
ClaimC.7. P(n ≤ |V∣∣D,%) = 1 - δ.
Proof. Mark l 〜LaPG),
P(n2 ≤ ∣v∣∣D,nι) = P(∣V∣- ɪiogɪ +1 ≤ ∣v∣∣D,nι)=
2δ
1	1 .	1	log Try.
P(1 ≤ ɪ log2δ∣D,nι) = 1 - 2exp(- e 1 2δ ) = 1 - δ
e
□
Claim C.8. Calculating m is (3 0) differentially private with respect to D for given n 1,n2 and
n2 < |V|.
Proof. Mark by D a neighbouring database to D, and V as V induced by this database. If V = V
then the claim follows trivially. In case the V's differ, assume w.l.o.g that |V| ≥ |V|, and that
if |V| = |V| then they differ in their last sample. Define q = P(xi,yi)∈v∕{xlvI,y|V|} XiUi, Z =
(xi ,yi )∈V /{x|V | ,y|V | } i .
|
|
q + x|V | y|V|
Z + x2v |
q + x∣v∣y∣v∣
Z + x2v I
22	22
qχ∣v∣ + χ∣v∣y∣v∣χ∣v∣ + χ∣v∣y∣v∣z - qχ∣v∣ - χ∣v∣y∣v∣χ∣v∣ - χ∣v∣y∣v|z
l≤
qxh + nρ1 XhZ + niιXh ≤ nρι 2zxh + Xh = nρι( 2xh + Xh ) ≤
(z + x2)z _ 1 (z + x2)z	1 Z + Xi (z + x2)z —
nρι (JxL +	Xh	) ≤ nρι (2Xh +	Xh	) = nρι 2(n2- 1)XhX2 + Xh
1'∣V ∣X2	|V |(|V |- 1)X4，— 1 'n2 X2	n2(n2 - 1)X4'	1	n2(n2 - 1)X4
therefore by the Laplace mechanism calculating mn is (3, 0) differentially private.
□
Claim C.9. Steps 6-13 are (33, δ) differentially private.
25
Proof. Mark I) as a neighbouring database,
P (m ∈ s∣i)
I	P(m ∈ S∣D, nι = ri, n2 = r2)p(nι = ri, n2 = r2∣D)dr1dr2
^r1,r2∈R>0×R>0
I	P(m ∈ S|D, ni = ri, n2 = r2)p(n1 = ri, n2 = r2∣D)dr1dr2+
Λ1,Γ2∈R>0×[1,∣V |]
I	P(m ∈ s|D,ni = m = r2)p(n1 = m = r2∣D)dr1dr2 ≤*
Jr1,r2∈R>0×(∣v∣,∞]
P	P(m ∈ S|D, ni = ri, n2 = r2)p(n1 = ri, n2 = r2∣D)dr1dr2 + δ ≤**
Λι,r2∈R>0×[1,∣V ∣]
/
Jrιb2∈R>0×[i,∣V∣]
e2eP(τh ∈ S|D, n = ri, n2
∖/ J	I Δ∖ J J ,C,
r2)p(ni = ri,n2 = r2∣D)dridr2 + δ ≤
j
,r1,r2∈R>0×R>0
e2eP(rh ∈ S|D,ni = ri,n2
r2)p(E = ri, n2 = r2∣D)dridr2 + δ
e2eP(rh ∈ S|D) + δ
where inequality * follows claim C.7 and inequality ** follows claims C.8 and C.6.	口
Claim C.10. Steps 14-19 are (e, δ) differentially private with respect to D for |W| < nmin and
given n2, h.
Proof. Mark l ~ Lap( ɪ), and D as a neighbouring database. Eq. 34 proves the claim.
P (S ∣d,∣w I<nmin,h,n2) =
P(S ∩ {null}∣D, ∣WI < nmin,h,n2)+
P(S ∩ {null}c∣D, ∣WI < nmin,h,n2) ≤
eeP(S ∩ {null}∣D, ∣W∣ < nmin, h, n2) + δ ≤
_ . ʌ .
eeP(S∣D, ∣W∣ <d,h,n2)+ δ
(34)
where first inequality is true from eq. 35 and the Laplace mechanism for nw.
P(null∣D, ∣W∣ < nmin,h,n2)=
P(nw < nmin + ^ log(T77)|D, |W 1 < nmin, h, n2) ≥
€	2δ
P(l < ɪlog(ɪ)) ≥ 1 - δ
€	2δ
(35)
□
Claim C.11. Step 19 is (e, δ) differentially private with respect to D for ∣W∣ ≥ nmin and given
n2, h.
Proof. For a given n2, h and a neighbouring database, the group W can change by up to one sample.
Mark n = |W| and C = h. From eq. 36, it follows that W ∈ D, as defined in eq. 5.
P2
n ≥ nJ1 ⇒
n1 > nγ1 ≥ np2
(36)
As W ∈ D, n ≥ nbi, and n ≥ n% the problem of sampling from p(θ∣W) for |W| ≥ nmin holds
the constraints of claim D.29. Therefore one sample from p(θ∣W) is (e, δ) differentially private. 口
26
Claim C.12. Steps 14-18 are (, 0) differentially private with respect to D for |W| > nmin and
given m, n.
Proof. Only data released is nw, and since the sensitivity of | W | given m, n is 1, then the Laplace
mechanism ensures (e, 0) differential privacy.	□
Corollary C.2. Steps 14-19are (2, δ) differentially private with respect to D for |W | > nmin and
given m, n2.
Corollary C.3. Steps 14-19 are (2e, δ) differentially private with respect to D g^ven m, n.
D Auxiliary Claims
This subsection contains simple claims used to simplify the reading of the proofs. Claims described
in this subsection uses the marking defined in eq. 17.
ClaimD.1. P ι-λ = α+χhββ.
Proof Claim D.1.
1
P T-I =
η2	1	2
2ncxhβ 1-(1- 2(α + n(Xh尸户))=皿h
ncx
α + nxhβ α + nxhβ
1
□
Claim D2 P⅛1 + Pλn-1 = 0+^(1 - λn).
Proof Claim D.2.
1 - λn-1	n-1	1 - λn-1 + λn-1 - λn	1 - λn
P -ɪ-ɪ + ρλn 1 = P----Γ-λ-------) = P(L)
ncx2h β
α + nx2hβ
(1-λn)
where the last equality holds from Claim D.1
Claim D.3. ρ(⅛1) + Pλn-1 = 0≡¾(1 - λn(4λ-1 + 1)).
□
Proof Claim D.3.
1	ʌn-1	1	ʌn-1	ι	1 _ 3 \n-1 _ 1 ʌ n
P(T-^r) + Pλn-1 = P(1fɪ) + PTλn-1 = P(1	4	4 λ )
1-λ	1-λ 4	1-λ
ρ(
1- λn( 4 λ-1 + 4)
ncx
1-λ
α + nx2hβ
31
(1-λn(4 λ-1+4))
)
where the last equality holds from Claim D.1.
ClaimD.4. 4 λ + 3 - λ= 3 2 α.
□
Proof Claim D.4.
13	1η	3	η 1
4λ + 4 -1=4(1 - 2(α + nx β)) + 4 - (I - 2(α + 4nχ β))
2[α + 4nχ2β - 4(α + nχ2β)] = 42α
□
Claim D.5.
(1 - λkn)(1 - TλnT) - (1 - (M(nT)))k(1 - λn-1(Tλ + 3))
λn-13ηα(1 - *”k(nT)) + λk(nτ)(λk - λk)(1 - λn-1λ).
27
Proof Claim D.5.
(1 - λkn)(1 - λλn-1) - (1 - (λλ(n-1)))k(1 - λn-1(∣λ + 3))=
_ 1	3 ʌ , ʌ	_ ,	_ 1	3
λn-1 (-λ + - - λ) + λkn(λλn-1 - 1) + (λλn-1)k(1 - λn-1(-λ + /)=
λn-1 (1 λ+- - λ)+λk(n-1)(λk(λλn-1 -1)+λfc(1 - λn-1(∣λ+-)))=
λn-1 (1 λ + - - λ) + λk(n-1)(λfc(1 - λn-1(1 λ + -)) - λk(1 - λλn-1))=
λn-1 (1 λ + - - λ) + λk(n-1)(λfc(1 - λn-1(1 λ + -)) - λk(1 - λn-1λ)) =*
λn-1 η-α + λfc(n-1)(λfc(1 - λn-1(1 λ + -)) - λk(1 - λn-1λ)) =*
λn-1 η-α + λfc(n-1)(λfc(1 - λn-1(λ + -ηα) - λk(1 - λn-1λ))=
λn-1 η-α - λkλn-1 -ηα + λk(n-1)(λk(1 - λn-1λ) - λk(1 - λn-1λ))=
λn-1 -ηα(1 - λkλk(n-1)) + λk(n-1)(λk - λk)(1 - λn-1λ)
where equality * holds from claim D.4
Claim D.6. λ P- λ(n-1)jλj[λn-1ρ + P Pn:02 λi] = λ(1 - λkn) zn⅛⅛.
□
Proof Claim D.6.
k-1	n-2	kn-1	i _ ʌ kn
λ X λ(nτ)jλj [λn-1ρ + ρ X λi] = ρλ X λi = ρλ-——
1-λ
j=0	i=0	i=0
ncxhβ
α + nxhβ
(1 - λkn)
.*
Where equality * follows from claim D.1.
□
Claim D.7. λ PM λ(n-1)jλj[λn-1ρ + PPn-2 λi] = λ 1-"⅛f 0≡⅛(1 - λn(3λ-1 + 4)).
J	_L zʌ	zʌ	I h ∣~j
Proof Claim D.7.
k—1	n—2
λ X λ(n-1)j λj [λn-1ρ + P X λi]
j=0	i=0
λ∖k [λn-1ρ+P、*
ʌ 1 - (λn-1λ)k ncxhβ
λ -'	I 77	；	27)
1 - λn-1λ Q + nxhβ
-1
(1 - λ (4 λ + /)
Where equality * follows from claims D.1, D.3.
□
Claim D.8.
λλk - λk λnλ - λλk + λλkλn( 4 λ-1 +1) = (1 - λλn-1 )(λk+1 - λk+1) + λk+1λn-1( 4 2 Q).
Proof Claim D.8.
λλk - λk λn λ - λλk+λλk λn(-λ-1 +1)=
λk+1(1 - λλn-1) - λk+1(1 - λn-1(1 λ + -)) =*
λk+1(1 - λλn-1) - λk+1(1 - λn-1(λ+ -ηα))=
(1 - λλn-1)(λk+1 - λk+1) + λk+1 λn-1(-ηα)
where equality * holds from claim D.4.
□
28
Claim D.9.
λ(1 - λkn)(1 - λn-1λ) - λ(1 - (λn-1λ)fc)(1 - λn(4λ-1 +1))=
(λ - λ)(1 - λλn-1) + λn-1λ[42α(1 - *kλk5T))] + λk(n-1)[(1 - λλn-1)(λfc+1 - λk+1)].
Proof Claim D.9.
3	一 入	一，	3 TI
λ(1 - λkn)(1 - λn-1^) - λ(1 - (λn-1λ)k)(1 - λn(-λ-1 + -))=
λ - λ -	λnλ(1 - (1λ-1 +1))	-	λfc(n-ι)[λλfc - λk λnλ - λλfc + λλfc λn(- λ-1 +1)] =*
λ - λ -	λnλ(1 - (1λ-1 + 1))	-	λk(n-1)[(1 - λλn-1 )(λk+1 - λk+1) + λk+1λn-1(1ηα)]=
λ - λ -	λn-1λ(λ - (1 + -λ))	-	λk(n-1)[(1 - λλn-1)(λk+1 - λk+1) + λk+1λn-1(∣ηα)]	=**
λ - λ -	λn-1λ(λ - (λ+ ∣ηα)) - λk(n-1)[(1 - λλn-1)(λk+1 - λk+1) + λk+1λn-1(∣ηα)]=
(λ - λ)(1 - λλn-1) + λn-1λ[∣ηα(1 - *kλk(nT))] + λk(nT) [(1 - ΛΛn-1)(Λk+1 - Λk+1)]
Where equality * follows from claim D.8 and equality ** follows from claim D.4.	□
Claim D.10.
λ(1 - λkn) - λ1 - Sn 1λ)k (1 - λn(∣λ-1 +1))
1 - λn-1^	'4	4〃
(λ - λ) +
λn-1[42ɑ(1 - λkλk(nT))]
ʌ .
1 - λλn-1
+ λk(n-1)(λk+1 - λk+1).
Proof Claim D.10.
λ(1 - λkn) - λ1j;1Iy (1 - λn(∣ λ-1+1)) =[m 5.d
1 — λn-1λ	4	4
(λ - λ)(1 - λλn-1) + λn-1λ[32α(1 - *kλk(nT))] + λk(nT)[(1 - λλn-1)(λk+1 - λk+1)]
	ʌ (1 - ^λn-1)
(λ - λ	)+ λn-1∣ 32 M- λkλ"nT))I + λk(n-1)(^k+1 - λk+1) 1 - λλn-1
Claim D.11.
α+⅛ (λ - λ+
λn-1
[3 2α(1-λfcλfc(n-1)
1-λλn-1
") + (P - P)
□
> 0.
29
Proof Claim D.11.
ncx
α + nx2hβ
, ʌ
(λ - λ +
ncx
α + nx2hβ
ncx2hβ
, ʌ
(λ - λ +
λn-1[42α(1 - λkλk(n-1))]
ʌ .
1 - λλn-1
λn-1[42α(1 - λkλk(n-1))]
ʌ .
1 - ^λn-1
)+ (P - ρ)
) + 2 ncχhβ(I - 4)
α + nx2hβ
(1 — 2(α + nx2β) — (1 — 2 (α + 4 nx2β))+
λn-1[4 2α(1 - λkλk(n-1))]
ʌ .
1 - ^λn-1
ncx
α + nx2hβ
(—3 2(nχ2β)+
]1 3n 2 R
-) + 42 ncχhβ =
λn-1[42α(1 - λkλk(n-1))]
ʌ .
1 - λλn-1
) + 3 2 ncχhβ
ncxhβ λn-1[42α(1 - λkλk(n-1))]
α + nxhβ	1 - Λλn-1
ncxhβ λn-1[420(1 - *λk5τ))]
+ ncxhβ[3n - 3n-XW]
h l4 2	42 α + nx2βj
α + nx2hβ
ʌ .
1 - λλn-1
3η
+ ncx2 β~ — [1 —
h 4 2l
nx2 β
0+^x2β] > 0
where the last inequality holds because λ, λ < 1 and α > 0
□
ClaimD.12. O > λ-2η 1-λ-λ-+I)n
is truefor k ≤ * logλ(
ι+On (i-λ2)
) -1.
1
Proof Claim D.12.
1 ≥ λ-2η
1 - λ-2kn
1 - λ-2
λ211(λ-2 - 1) ≥ λ
αη
- 0 λ2 11(1 - λ-2) ≤ 1 - λ-2kn 0
αη
-2kn - 1 0 1 + λ211(λ-2 - 1) ≥ λ-2kn 0
αη
-k ≥ 2n logλ(1+On(1 - λ2)) 0k ≤ 2n logλ(
1 + On(I - λ2)
1
)
□
Claim D.13.
ɪ (λλ(n-1))2k > η Pn=o1 λ2i PM(λ2λ25-%j is true for k ≤ 2n logλ(	) ∙
Proof Claim D∙13∙ First note that the inequality can also be written as
α>n Pn-I λ2ipk-ι(M(I) )2(Lk).
Secondly, the right hand term of the inequality could be upper bound as in eq. 37. Therefore for the
claim’s inequality to holds it is enough that 1 ≥ nλ-21⅛n- , which proved by claim D.12 to be
true for k ≤ 2n logλ(
1
1+On (i-λ2)
)
30
n-1	k-1
η X λ2i X(λλ(n-1))2(j-k)
i=0	j=0
n-1	k-1
η X λ2i X———1-----
i=0	j=0 (^λ(I))2(I)
<k>j
n-1	k-1
ηXλ2iX
i=0	j=0
r=nj +i
n-1	k-1
ηXλ2iX
i=0	j=0
1
λ2n(k-j)
n-1 k-1
η Xi=0 Xj=0
nk-1
ηX
r=0
1
λ2[nk-r]
=r0=nk-r,1<r0 <nk
nk
η X ⅛T
r0=1
(37)
nk
ηXλ-2i
i=1
λ-2 - λ-2(nk+1)
η -T-I—-
ηλ-2
1 - λ-2nk
1 - λ-2
□
ClaimD.14. 1 (Mn-1)2® ≥ η(λλn-1)2k Pn-I λ2i is true for kt ≤ 21n logλ( 1+泰 11-λ2))
ProofClaim D.14. eq. 38 holds because λ, λ < 1. By multiplying both sides with Pn-c1 λ2i get
eq. 39. Then noticing that the right term equals to the right term of claim D.13, and hence smaller
than the left term of the claim, the claim is proved.
k-1
(Un-1)2® < 1 < X(λλnT)2j	(38)
i=0
-	；-	-
n-1	®-1	n-1
η(λλn-1)2k X λ2i < η X(^λn-1)2j X	λ2i	(39)
i=0	j=0	i=0
□
Claim D.15. The inequality
®-1	n-1	n-r
(^λn-r)2[1(λλn-1)2k + ηX(^λnT)2jXλ2i] > ηXλ2i
α	j=0	i=0	i=0
h。Idsfio xhβ > 3,n> 2⅛ - ⅛.
Proof Claim D.15. Left hand side can be lower bounded according to eq. 40, while right
hand side can be upper bounded according to eq. 41. Therefore it’s enough to show that
λ2n[ 1 λ2kn + η 1J：2n ] > η 1ι-λn, which according to eq. 42 is equivalent to showing that
(2nxhβ-1) 1 λ2(k+1)n+2(2λ2n-1) > 0. Since n > 20⅛j - χ1β ClaimD.19appliesandtherefore
- 2	- 2
λ2n ≥ e xhβ. Consequently it,s enough to show that (2nxhβ — 1) 1 λ2(k+1)n + 2(2e xhβ — 1) > 0,
which is true for x2hβ > 3 by claim D.16.
®-1	n-1
(λλn-r)2[1(MnT)2k + ηX(MnT)2j X λ2i] >
α	j=0	i=0
®-1	n-1
(λλnT)2[1(^λnT)2k + ηX(λλn-1)2j X λ2i] >
α	j=0	i=0
(40)
®-1	n-1
λ2n [-λ2kn + η X λ2jn X λ2i] = λ2n [-λ2kn
αα
j=0	i=0
1 - λ2kn]
+η 万k]
First inequality holds because λ < 1 and r > 1, and second inequality holds because λ < λ.
31
n-r	n-1	2n
η X λ2i<η X λ2i = η 1⅛
i=0	i=0
Inequality holds because λ < λ and r > 1.
1 - λ2kn	1 - λ2n
1 -λ2 ] >η 1 -λ2
λ2n[1 λ2kn + η
α
λ2n(i - λ2)1 λ2kn + ηλ2n(i - λ2kn) >η(i - λ2n)
α
(1 - λ2)-λ2(k+1)n + η(2λ2n - λ2(k+1)n -1) > o
α
(α + nxhβ)2(1 - λ2)1 λ2(k+1)n + 2(2λ2n - λ2(k+1)n - 1) > 0
α
(α + nxhβ)2(1 - (1--------)2)1 λ2(k+1)n + 2(2λ2n - λ2(k+1)n - 1) > 0
α + nx2hβ	α
(2(α + nxhβ) - 1)1 λ2(k+1)n + 2(2λ2n - λ2(k+1)n - 1) > 0
α
2λ2(k+1)n + (2nx2β - 1)1 λ2(k+1)n + 2(2λ2n - λ2(k+1)n - 1) > 0
α
(2nxhβ - 1)1 λ2(k+1)n + 2(2λ2n - 1) > 0
α
Claim D.16. For x2β > 3 the inequality (2e x2β - 1) > 0 holds.
ProofClaim D.16. It's easy to see that the inequality holds only if x2β ≥ 清.Since -1
claim is proved.
(41)
(42)
□
<3
□
Claim D.17. For k as defined in lemma A.4, and the conditions of claim D.19
1	2
-(e xhβ
α
2	.
(eXhe - 1)	.	、-2 1 - λ-2(dke+1)n
+ α (α + nx2β) + 1) >λ η —E—-
Proof Claim D.17.
I - λ-2(dke+1)n	λ-2(k+2)n - I	λ-2(2n logλ( 1 + μ(ι-λ2) )-1+2)” - I
η—λ2-I- ≤ η 1 - λ2	= η---------1-λ------------
λ-logλ(1+磊(1-λ2))λ-2n - 1	[1 + Οη(1 - λ2)]λ-2n - 1
η---------1-λ--------=η---------1-λ---------=
η¾2njη + ηλT⅛1 =--+ ⅛‰ ≤
2 1	1
e x β------1---α
2
(ex2β — 1)
(α + nx2β) + 8
2
1 r 2	(e χ2β — 1)
Te x β + α -----------1 ]
α	(α + nx2β) + 8
where the fourth equality holds from eq. 43 and the second inequality holds from D.19.
η	1	1
λ2 — 1 η (1 — 2 (α + nx2β ))2 — 1	"η(α + nx2β) + ( 2 (α + nx2β ))2
1	_	1
(α + nx2β) + 4 (α + nx2β )2	(α + nx2 β) + 8
(43)
□
32
Claim D.18.
∀k> 0:1 - (λ )k
^
4 nx2β
≥  ---——  ----1—.
(α + nx2β)2 — (α + 4 nx2β)
Proof Claim D.18.
1-(^)k ≥
λ
1-λ
ɪ ʌ
λ
1-
=1-
1-
1
α+nx2β
α+ 4 nx2β
(α+nx2β)2
1-
(α + nx2β)2 - (α + nx2β)
(α + nx2β)2 — (α + 4 nx2β)
α2 + 2nx2 αβ + (nx2β)2 - α - nx2β
1 - .7------7------：--7----------:---7-
a2 + 2nx2αβ + (nx2β)2 — ɑ — 4 nx2β
4 nx2β
(α + nx2β)2 — (α + 4 nx2 β)
Where first inequality holds because λ < λ.
Claim D.19. For the conditions of claim D.21,
□
1
(1 -
α + nx2 β
)2n
-2
e x2β
≥
Proof Claim D.19. The proof is easily deduced from claims D.20 and D.21
□
Claim D.20.
lim (1 -
n→∞
α + nx2β
)2n = e
2
x2β
1

ln(1-' 1 2 Q )
ProofLemma D.20. From eq. 44, it is enough to find limn→∞--α+nx β
2n
ln(1-
Since limn→∞
___1__)
α+nx2 β '
~τ~
2n
0, and both the numerator and denominator are differentiable
around ∞, the use of L,HOpital,s rule is possible as shown in eq. 45. This proves the claim.
ln(1-1~ττ7
1α+nx2β
∖2n	ln[(1--匚F )2n]	2n ln(1--匚F)	----ɪ一
(1 - -)	= e	α + nx2β/ J= e ∖	α + nx2 β，= e 2n
α + nx2β
(44)
lim
n→∞
磊In(I	α+nx2β )
d 1
dn 2n
________x2β________
lim (α+nx2β-1)(a+nx2β)
— -
2n2
2n2x2β	2
Im (nx2β)2	x2β
(45)
□
Claim D.21.
∀n >
1
2ax2β
ɪ : d-(1--------)2n < 0.
x2β dn α + nx2β
—
Proof claim D.21. First, a simplified term for the derivative is found at eq. 46.
-d (1 -
dn
1	∖2n — d 尸 In(I-0+⅛β)
f	U 1n nx^ Ie
α + nx2 β	dn
(1 -
1
α + nx2β
)2n[2ln(1 -
α + nx2β ) + 2n 1-
1
1
α+nx2β
x2β
(α + nx2β)2
(46)
(1 -
1
α + nx2β
——d +
α + nx2β
2nx2β
(α + nx2β - 1)(α + nx2β)
A lower bound for the ln term can be found using Taylor’s theorem as shown in eq .47, where
0 ≤ ξ ≤ α+⅛.
1
ln(1------1~^)
α + nx2 β
1
α + nx2 β
l(Γ⅛(0+^
)2 ≤
1
α + nx2β
- 1( α 十：2e )2
(47)
—
—
33
From equations 46 and 47 it is enough to find the terms for which
nχ2 β
(α+nx2β-1)(α+nx2β)
<	,1。C +
α+nx2 β
1 g+nX2β)2 holds. A simplified version of this inequality is found at (48), and it can be easily seen
that for α > 1 (n12e + 1) ^⇒ n > ^1^ -± this inequality holds.
nx2 β	1	1	1
(α + nx2 β - 1)(α + nx2 β)	α + nx2 β 2 (α + nx2 β)2
^⇒
0 < 2a2 + 2nx2βα — 2α — 2nx2β + a + nx2β — 1 ^⇒
(48)
0 < nx2 β(2α — 1) + α(2α — 1) — 1
□
2
Ci - ♦ _ _ τ⅛ GGL	-	"/T2Q C	1	1,1	T . •	/- I - CTC7	Ir 1 ■
Claim D.22. For n > Xae (e hβ — 2) + 石为 and the conditions of claim D.19, k, as defined in
lemma A.4, is positive.
Proof Claim D.22. The claim’s inequality is simplified at eq. 49
k > 0
2n logλ(ι + αηι 1ι - λ2)) - 1 > 0
logλ(
1
1+On(I - λ2)
) > 2n
ln(i+αη 1ι-λ2))
ln λ
> 2n
ln(1 + an 11 - λ2)) < 2n ln λ
ln⅛⅛^) < ln λ2n
∏⅛—
λ-2n < 1 + ^(1 - λ2)
αη
λ-2n - 1 < -1(1 - λ2)
αη
(49)
2	2
ByclaimD.19 λ-2n — 1 < e xhβ — 1, therefore it is enough to find terms for e xhβ — 1 < 卷(1 — λ2),
which is done at eq. 50, which proves the claim.
2	1
exhβ - 1 ‹ 一(1 - λ2)
2
αη(eXhe - 1) < (1 - λ2)
,-⅛	, η ,	C …C
ɑη(exhβ - 1) < 1 - (1 - 2(α + nxhβ))
α(eXhe — 1) < (a + nxhβ) — ɪ (α + nx2β)2
2	1
α(eXhe — 1) < (α + nxhβ)-----
2	1
a(eXhe — 2) + - < nxhβ
(50)
α / 2 Q	人、	1
—2~^ (eHh - 2)+	2,0 < n
x2hβ	2x2hβ
34
□
Claim D.23. For k as defined in lemma A.4, and the conditions of lemma A.5
pW「ke+1)n >μ(dke + 1)nD)
≤e
ɪ (—3^ )2( C )2
2vi (32x2β ) (n )
Proof claim D.23.
/八	、	∖ TΛ∖ .-
p(θ(dke+1)n > μ(dke+1)n|D) ≤
1n
n X exp(-
r=1
(〃(「ke+1)n - μrdke + 1)n)2
2(σ(dk e+1)n
)2
)≤
n2
n X exp(-e * x2β 场(32χ2β)(n)) =
n	v1	x β n
(=1
exP(-e-鼎言(32⅛)2(C)2)
Where the first inequality holds due to lemma 4.4 and second inequality holds due to lemma A.5.
□
Claim D.24. for n > 1 + 10xXh β, the inequality ɪθ (α + (z + Xn)β) > V(Xn — Xn) holds.
ProofClaim D.24. Notice that 击(α + (z+Xn)β) > ɪθzβ > ɪθ (n — 1)x2β and VXh > V(Xn — x2n),
Therefore a sufficient condition will be that ɪθ (n — 1)X2β > VXh which is equivalent to n >
2
1 + ⅞ 詈.
□
Claim D.25. For the (σ2)V as defined in eq. 14
(σ2)V > O.
Proof Claim D.25.
(σ2)V = νσ2 + (1 — ν)σ 2 = -—ν―2-- +----V2 2	=
α + (z + Xn)β	α + (z + Xn)β
v (α+(z+Xn)β)+(1—ν)(α+(z+X2n)β) = α+(z+Xn )β+V(Xn—Xn)
(α + (z + X2n)β)(α + (z + Xn) β)	(α + (z + Xn，β)(α + (z + Xn)β)
(51)
Therefore, a sufficient condition is that α + (z + X2n)β + V(Xn — Xn) > 0. Since the condition of
Lemma 4.2 dictates n
x2
> 1 + 10T V then claim D.24 holds, which satisfy this condition.
xl β
□
Claim D.26. For the Bayesian linear regression problem on domain D, and σ, σ defined in eq. 14
in σ ≤ _XL^.
σ	2(n — 1)X2
XL
Proof Claim D.26. Consider c1
c1
Xh	>
(n - 1)X2
(n-1)x2
Xn - Xn
z+x
x *lβ - ** __ α + (z + 珠阴 I
>	1
α + (z + X2n)β α + (z + X2n)β
(52)
Where eq. 52 holds trivially for Xn ≤ Xn, therefore it is assumed that Xcn > Xn. From eq. 52, by
Taylor theorem and 0 ≤ ζ ≤ c1 following inequality holds
ec1 =1 + ci + -2(cι)2 > 1 + ci >
α + (z + X^n)β
α + (z + Xt2n)β
Consequently, because the natural logarithm is monotonically increasing the following equation also
holds
1	1 1 α + (z + Xn)β 1 σ
2C1 > 2ln α +(z + Xn)β =	^
Therefore ln^ < 2(n‰	□
35
Claim D.27. For the Bayesian linear regression problem on domain D, the conditions of Lemma
4.2 and (σ2)V, σ defined in eq. 14
1,	1、，	σ2	,1,	，、	Vxh
2(V- 1)lnW ≤ 2(V- 1)2((n - 1)x2 -Vxh).
ProofClaim D.27. consider ci =((n-iVxh-VX2)
CI =	Vxh	≥*________νβχh_______〉*
(n - 1)xl2 - Vx2h	α + (n - 1)xl2β - Vβx2h
_______Vexn_______≥_______Ve(Xn- xn)_______
α + (z + x2n)β 一 Vexn - α + (z + x2n)β 一 Vβ(xn - xn )
α + (z + x2n)β
----:---~：-----~~7：--- C 、 — 1 ——
α+(z+xn)e+Vβ(xn - xn)
1	(α+(z+xn)β)(α+(z+xn)β)	1
----;---^τττ~T • --:----  -----  ----TrT— — 1
α + (z + xn )β	α + (z + xn )β + Vβ(xn — xn )
σ2	1
(σ2)V
x2
Where inequalities * holds under assumption that n > 1 + VT, and last equality holds from eq. 51.
xl
Therefore, by using Taylor theorem and 0 ≤ ζ ≤ c1 following inequality holds
ec1
eζ	2	σ2
1+ c1 + "2"(c1) > 1+ c1 ≥ 谖尸
σ2
From this inequality, and because the natural logarithm is monotonically increasing ln (σ⅞y ≤ ci,
therefore
1 / r、］ σ2	1 , r、	1 ,	_,、	Vxh
2(V - 1)lnW	≤ 2(V - 1)C1	= 2(V-	1)((n -	1)x2	-Vxh).
□
Claim D.28. For the Bayesian linear regression problem on domain D, the definitions of eq. 14,
and the conditions of Lemma 4.2, the value 2((-2? is bounded by
2,,r(	xh 、, 2.,°∕(xhβ)(xhα + xhβ)、(C + nγ1) , V/(xhα + xhβ)2、(C + nγ1)2
β 10ni-2γ1 x2 ) +	e( —10(x2β)2 —) n2-γ1	+ 2( —i0x6β 一) —n3一
Proof Claim D.28. First bound ∣μ - μ∣,
q+ xnyn
q + xnyn
|〃 - μl = βl
-------:------7Γ~~^ - ---------:------~7~~7
α + (z + xn)β	α + (z + xn)β
|
(q + xnyn )(α + (Z + xn)β) - (q + xn 0n)(α + (Z + Xn )β) ∣ =
(a +(z + xn)β)(α + (z + xn)β)
qxnβ + xnyna + xnynzβ + xnjnxjβ - 9鹫万一"。丁° - xnθnzβ - xnθnxnβ
(a+(z+xn)β)(a+(z+xn)β)
β I
xnz( q - xn )β - xnz(Z - xn )β + a(xnyn - xnyn) + xnxnβ(ynxn - Iynxn)
(a+(z+xn )β)(a+(z+xn)β)
|<
xhz(2nγ1 )β + axh(c + nγ1) + xhhβ (c + nγ1)
β ।	(a + (z + xn)β )(a + (z + *)β)	1
2xhβznγ1 + (xha + xhhβ)(c + nγ1),
βl (：+(z+xn)β)(a +h(z+xn)β) 1
36
Therefore,
V (μ - μ)2 V
2 (σ* 2)V	—
Ve2(2xhβznγι+(xhα+Xhe)(c+nγι))2 ( α+(Z+xn)e+V(Xn- Xn) )-1
2β ( (α +(z + xn)β)(α + (z + X2l)β) ) ^ ((α + (z + x2t)β)(α + (z + X2l)β))
v	β2(2X2 βznγ1 + (x2 α + x4 β)(c + nγ1 ))2
-----：--∖ h-：------÷-h-~h~^~=~'' . .-----L <*
2 (α+(z+xn)β )(α+(z+Xn)β)(α+(z+Xn )β+V(Xn - Xn))
V	β2 (2X2hβznγ1 + (X2hα + X4hβ)(c + nγ1 ))2
=
2190 (α+(z+wɔm(a+(z+*响(。+(z+磋)0
V 2 (2Xhβ)2Z2n力1 + 2(2Xhβ)(Xhbα + Xhhβ)zn71 (c + nγ1) + (Xha + Xhhβ)2(c + nγ1 )2
2	190 (a+(z+琮泗2(。+(z+Xn)β	≤
V 2	(2Xhβ)2z2n271	+	(4Xhβ)(Xhha + Xhβ)zn'1(c +	nγ1)	+	(Xha + Xhβ)2(c + nγ1 )2	<**
2β (	io ((z+Xn)β)2((z+Xn)β)	) ≤
vR 2(生包贮2 ) +
2	10 nX2β3
v β 2( (4琢0(琢。+ Xhe)nY1(c + nγ1)) + 上 β2 ((Xha + Xhe)2(c + nγ1 )2
2	190 (nX2)2e3	2	190 (nX2e)3
2“β() + 2VR(
(Xhe)(Xha + XhR) )(c +
nγ1 )
+ V ((Xha + XhR)2) (C + nγ1 )2
2	10 X6β	n3
Inequality * is true because Lemma 4.2 conditions dictates that n > 1 + 署詈,and according
to claim D.24 this promises that 击(α + (z + Xn)β) > V(Xn — Xn). Inequality ** follows from
n >> 1 ⇒ (n - 1)Xι ≈ nXι.	口
Claim D.29. For the conditions and definitions of Lemma 4.3, one sample from the posterior is
(, δ) differentially private for the following terms on n and V.
V=1+
2ln( 1)
n ≥ max{1 + g8, 1 +v^(1 + 8—),
(16VRXh)⅛
( 190 宙 )	,
(
16vR ((Xhe)(Xha + XhR)
190 (X2e)2
1
)(c + nγ1)) 2-γ1 ,

(4v((XhɪXxhβ)2 )(c + nγ1))2}
ProofClaim D.29. By Lemma 4.3, one sample from the posterior is (门 + ln-ɪ), δ) differentially
private. For each of the 6 terms of s + ln-ɪ), a lower bound on n and V is found at equations 53,
54, 55, 56, 57, 58 such that the sum of terms is upper bounded by . These bounds match the claim’s
guarantee over n and V therefore proving the claim.
ln(1)
For term -V-I
ln( 1) J 『
V-I =2 O
3 + 1 = V
(53)

37
For term
XL
_xh_ ≤ E
2(n - 1)x2 — 16
x2h 8
n ≥ 1 +-2 —
xl2 C
(54)
For term 1 (V - 1)
Vxh
(n-1)x2—VXhh
2(V
- 1)
(n - 1)x2- Vxh
≤
16
2(V
-1)16Vxh ≤ (n - 1)x2
- νx2h
(55)
n≥
1	16V x2h
1 + 2(V - 1)T
x
l
2
+ νx2 = 1 +
xl2
1 + 8(V-5)
For term 2νβ( ɪ-
10 n
Xh
.1-2Y1 X2
)
2νβ( "⅛) ≤ i
竺Ve⅛ ≤ n1-2Yi
F	∙9T2
C	10 xl
n ≥ (U)F
10 Cxl
(56)
For term 2Vβ (
(Xhe)(Xha+xhhβ) ) (c+nγ1 )
--90^γ - ) ^2—γ^
2Vβ(
(Xhe)(Xha + Xhe))(C +
-Iw-)
nγ1 )
n2-γ1
C
≤ —
一8
For term ν (
n2-γ1 ≥
n≥(
16Vβ ((Xhe)(Xha + xhβ)
C
)(c + nγ1)
(57)
16 "e ((Xhe)(Xha + Xhe)
10 (X2e )2
C
1
)(c + ηγ1)) 2-γι
(Xha+Xhe)2 ) (c+nγ1)2
告X6e n n3
V ((Xha +;he)2 )(c + nγ1 )2 ≤ C
n3 ≥ 4V((XhIXhe)2 )(c + n")2
(58)
n ≥ (4v((Xhy Xzhe)2 )(c + nYi
C	10 Xl e
))3
□
38
Claim D.30. For C = nγ2, γι < γ2 < 2, and the conditions and definitions of Lemma 4.3, one
sample from the posterior is (, δ) differentially private for following terms on n and ν.
ν
2ln( 1) +1

(ν- 1)
n ≥ max{1 + 翼-,1 + vx2(1 + 8
Xl	Xl
(16VeXh ) T-Yr
()	,
(
),

16 νe ((Xhe)(Xha + Xhe)
190 (X2e)2
)(1+----------2-------
(1 + 10 ⅛ V )γ2-γ1
___1
))2-γ 1
γ2
-

(4ν (If )(1+(1 + 10 L V …)) 3-M
Xl β
Proof Claim D.30. Claim D.29 provides general lower bounds on n for (, δ) differential privacy.
When c = nγ2 , γ2 > γ1, these bounds can be simplified.
For condition n ≥ (16νβ((xhβ9(Xh2α+χhβ) )(c + nγ1)) 2-γι,
e	io(xι β)
(
(
16νβ ((Xhe)(Xha + xhβ)
1
)(c + nγ1)) 2-γι =
16νβ ((Xhe)(Xha + Xhe)
)nγ2 (1 +
nγ2-γ1
1
))2-γ1 ≤


1
(
i6νβ ((Xhe)(Xha + Xhe)

)nγ2 (1 +---------2----------)) 2-γι
(1 + 10 Xh β )γ2-γ1
xl β
x2 ν
,where the inequality holds since Lemma 4.3 dictates that n ≥ 1 + 10 -2 V. Consequently it S
xl β
enough that
n>(
i6νβ ((Xhe)(Xha + Xhe)
)(1+
(1 +
___1
))2-γ1
γ2
1
-

Following same considerations for condition n ≥ (4ν((Xho+χββ) )(c + nγ1))2, it is enough that
> (4ν((Xha + Xhe)2
n
10贰e

、,一	1	、、	2
+ (1 + 10⅛ β )γ2-γ1	72
xl β
□
E Wasserstein distance Proof
Claim E.1. If p, q are distributions with 2-Wasserstein distance W2(p, q) = 2, then we have
p(Br(X)) ≤ q(Br+(X)) + .
Proof. If is the claim that d2P ≤ dw from Gibbs & Su (2002). Picking an optimal coupling and using
Markov inequality we get P(d(X,y) > e) ≤ ɪE[d(X,y)] = e. As {(X, y) : X ∈ B『(x)} ⊂ {(X, y):
y ∈ Br+Ky)}∪{(X,y) : d(X, y) > e} we getP(Br(X)) ≤ q(Br+e(χ)) + E (special case of Strassen
theorem).	口
39
Claim E.2. Let p, q be continuous distributions on Rd with Wasserstein distance W2 (p, q) < 2,
and let pδ,qδ be their convolutions with uniform distribution on Bδ (0). We assume both density
functions are L-Lipshitz continuous. For λ > we have
vold(λ)	vold(λ)
pλ(x) ≤ vold(λ -e) qλ(x) + vold(λ - e) + 2 (Vold(λ - e) - 1)
(59)
Proof. We have P (Bλ(x)) = P (Bλ-(x)) + P (A(x; λ - , λ)) where A(x; r1, r2) is the annulus
around x between radius r1 and r2. From continuity there exists z ∈ P (Bλ (x)) such that p(z) =
Pvol j(χ)), where VoId (r) is the volume of a ball of radius r in Rd. From LiPshitz continuity We have
P(A(x; λ - e, λ)) ≤ (vold(λ) - vold(λ - e))(p(z) + 2λL) = (1 -吗除？) P(Bλ(x)) + △,
where ∆ = (Vold(λ) - Vold(λ - ))2λL. From this, we get
P(Bλ-e(x)) ≥ W)-JP(Bλ(x)) - △.	(60)
Vold(λ)
Combining this with claim E.1, we get
P(Bλ(x)) ≤ Vold(λ) JP(Bλ-e(x)) + △) ≤	Vold(λ) √Q(Bλ(x)) + △ + e).	(61)
Vold(λ - )	Vold(λ - )
We divide by Vold(λ) to get the densities pλ, qλ.
Pλ(χ) ≤	Tld qλ(x) +	△(+，)	(62)
Vold(λ - )	Vold(λ - )
□
40