Under review as a conference paper at ICLR 2022
The limiting dynamics of SGD: modified loss,
phase space oscillations, anomalous diffusion
Anonymous authors
Paper under double-blind review
Ab stract
In this work we explore the limiting dynamics of deep neural networks trained
with stochastic gradient descent (SGD). As observed previously, long after per-
formance has converged, networks continue to move through parameter space by
a process of anomalous diffusion in which distance travelled grows as a power
law in the number of gradient updates with a nontrivial exponent. We reveal an
intricate interaction between the hyperparameters of optimization, the structure in
the gradient noise, and the Hessian matrix at the end of training that explains this
anomalous diffusion. To build this understanding, we first derive a continuous-
time model for SGD with finite learning rates and batch sizes as an underdamped
Langevin equation. We study this equation in the setting of linear regression,
where we can derive exact, analytic expressions for the phase space dynamics of
the parameters and their instantaneous velocities from initialization to stationar-
ity. Using the Fokker-Planck equation, we show that the key ingredient driving
these dynamics is not the original training loss, but rather the combination of a
modified loss, which implicitly regularizes the velocity, and probability currents,
which cause oscillations in phase space. We identify qualitative and quantitative
predictions of this theory in the dynamics of a ResNet-18 model trained on Im-
ageNet. Through the lens of statistical physics, we uncover a mechanistic origin
for the anomalous limiting dynamics of deep neural networks trained with SGD.
1	Introduction
Deep neural networks have demonstrated remarkable generalization across a variety of datasets and
tasks. Essential to their success has been a collection of good practices on how to train these models
with stochastic gradient descent (SGD). Yet, despite their importance, these practices are mainly
based on heuristic arguments and trial and error search. Without a general theory connecting the
hyperparameters of optimization, the architecture of the network, and the geometry of the dataset,
theory-driven design of deep learning systems is impossible. Existing theoretical works studying
this interaction have leveraged the random structure of neural networks at initialization [1, 2, 3]
and in their infinite width limits in order to study their dynamics [4, 5, 6, 7, 8]. Here we take a
different approach and study the training dynamics of pre-trained networks that are ready to be used
for inference. By leveraging the mathematical structures found at the end of training, we uncover an
intricate interaction between the hyperparameters of optimization, the structure in the gradient noise,
and the Hessian matrix that corroborates previously identified empirical behavior such as anomalous
limiting dynamics. Not only is understanding the limiting dynamics of SGD a critical stepping stone
to building a complete theory for the learning dynamics of neural networks, but recently there have
been a series of works demonstrating that the performance of pre-trained networks can be improved
through averaging and ensembling [9, 10, 11]. Combining empirical exploration and theoretical
tools from statistical physics, we identify and uncover a mechanistic explanation for the limiting
dynamics of neural networks trained with SGD.
2	Diffusive B ehavior in the Limiting Dynamics of SGD
A network that has converged in performance will continue to move through parameter space [12,
13, 14, 15]. To demonstrate this behavior, we resume training of pre-trained convolutional networks
while tracking the network trajectory through parameter space. Let θ* ∈ Rm be the parameter vector
for a pre-trained network and θk ∈ Rm be the parameter vector after k steps of resumed training.
1
Under review as a conference paper at ICLR 2022
We track two metrics of the training trajectory, namely the local parameter displacement δk between
consecutive steps, and the global displacement ∆k after k steps from the pre-trained initialization:
δk = θk - θk-i,	∆k = θk - θ*.	(1)
As shown in Fig. 1, neither of these differences converge
to zero across a variety of architectures, indicating that
despite performance convergence, the networks continue
to move through parameter space, both locally and glob-
ally. The squared norm of the local displacement kδk∣∣2
remains near a constant value, indicating the network
is essentially moving at a constant instantaneous speed.
This observation is quite similar to the “equilibrium” phe-
nomenon or “constant angular update“ observed in Li
etal. [17] andWanetal. [13] respectively. However, these
works only studied the displacement for parameters im-
mediately preceding a normalization layer. The constant
instantaneous speed behavior we observe is for all param-
eters in the model and is even present in models without
normalization layers.
While the squared norm of the local displacement is es-
sentially constant, the squared norm of the global dis-
placement ∣∆k 112 is monotonically growing for all net-
works, implying even once trained, the network continues
to diverge from where it has been. Indeed Fig. 1 indicates
a power law relationship between global displacement
and number of steps, given by ∣∣∆k∣∣2 8 kc. As we,ll
see in section 8, this relationship is indicative of anoma-
3 × Io-8
1.5	× Ic)-8
Pl∣2
1.6	×10-6
1.2 X 10-6
VGG-16
0	15000	30000	45000
Figure 1: Despite performance con-
vergence, the network continues to
lous diffusion where c corresponds to the anomalous dif-
fusion exponent. Standard Brownian motion corresponds
to c = 1. Similar observation were made by Baity-Jesi
et al. [14] who noticed distinct phases of the training tra-
jectory evident in the dynamics of the global displace-
ment and Chen et al. [15] who found that the exponent
of diffusion changes through the course of training. A
parallel observation is given by Hoffer et al. [18] for the
beginning of training, where they measure the global dis-
move through parameter space. We
plot the squared Euclidean norm for the
local and global displacement (δk and
∆k) of five classic convolutional neu-
ral network architectures. The networks
are standard Pytorch models pre-trained
on ImageNet [16]. Their training is re-
sumed for 10 additional epochs. We
show the global displacement on a log-
log plot where the slope of the least
squares line c is the exponent of the
power law ∣∣∆k∣∣2 H kc. See ap-
pendix H for experimental details.
placement from the initialization of an untrained network and observe a rate H log(k), a form of
ultra-slow diffusion. These empirical observations raise the natural questions, where is the network
moving to and why? To answer these questions we will build a diffusion based theory of SGD, study
these dynamics in the setting of linear regression, and use lessons learned in this fundamental setting
to understand the limiting dynamics of neural networks.
3	Related Work
There is a long line of literature studying both theoretically and empirically the learning dynamics
of deep neural networks trained with SGD. Our analysis and experiments build upon this literature.
Continuous models for SGD. Many works consider how to improve the classic gradient flow model
for SGD to more realistically reflect momentum [19], discretization due to finite learning rates
[20, 21], and stochasticity due to random batches [22, 23]. One line of work has studied the dynam-
ics of networks in their infinite width limits through dynamical mean field theory [24, 25, 26, 27],
while a different approach has used stochastic differential equations (SDEs) to model SGD directly,
the approach we take in this work. However, recently, the validity of this approach has been ques-
tioned. The main argument, as nicely explained in Yaida [28], is that most SDE approximations
simultaneously assume that ∆t → 0+, while maintaining that the learning rate η = ∆t is finite.
The works Simsekli et al. [29] and Li et al. [30] have questioned the correctness of the using the
central limit theorem (CLT) to model the gradient noise as Gaussian, arguing respectively that the
heavy-tailed structure in the gradient noise and the weak dependence between batches leads the
CLT to break down. In our work, we maintain the CLT assumption holds, which we discuss fur-
2
Under review as a conference paper at ICLR 2022
ther in appendix A, but importantly we avoid the pitfalls of many previous SDE approximations by
simultaneously modeling the effect of finite learning rates and stochasticity.
Limiting dynamics. A series of works have applied SDE models of SGD to study the limiting
dynamics of neural networks. In the seminal work by Mandt et al. [31], the limiting dynamics were
modeled with a multivariate Ornstein-Uhlenbeck process by combining a first-order SDE model for
SGD with assumptions on the geometry of the loss and covariance matrix for the gradient noise.
This analysis was extended by Jastrzebski et al. [12] through additional assumptions on the Covari-
ance matrix to gain tractable insights and applied by Ali et al. [32] to the simpler setting of linear
regression, which has a quadratic loss. A different approach was taken by Chaudhari and Soatto
[33], which did not formulate the dynamics as an OU process, nor assume directly a structure on the
loss or gradient noise. Rather, this analysis studied the same first-order SDE via the Fokker-Planck
equation to propose the existence of a modified loss and probability currents driving the limiting
dynamics, but did not provide explicit expressions. Our analysis deepens and combines ideas from
all these works, where our key insight is to lift the dynamics into phase space. By studying the
dynamics of the parameters and their velocities, and by applying the analysis first in the setting of
linear regression where assumptions are provably true, we are able to identify analytic expressions
and explicit insights which lead to concrete predictions and testable hypothesis.
Stationary dynamics. A different line of work avoids modeling the limiting dynamics of SGD
with an SDE and instead chooses to leverage the property of stationarity. These works [28, 34,
35, 36] assume that eventually the probability distribution governing the model parameters reaches
stationarity such that the discrete SGD process is simply sampling from this distribution. Yaida [28]
used this approach to derive fluctuation-dissipation relations that link measurable quantities of the
parameters and hyperparameters of SGD. Liu et al. [35] used this approach to derive properties for
the stationary distribution of SGD with a quadratic loss. Similar to our analysis, this work identifies
that the stationary distribution for the parameters reflects a modified loss function dependent on
the relationship between the covariance matrix of the gradient noise and the Hessian matrix for the
original loss.
Empirical exploration. Another set of works analyzing the limiting dynamics of SGD has taken
a purely empirical approach. Building on the intuition that flat minima generalize better than sharp
minima, Keskar et al. [37] demonstrated empirically that the hyperparameters of optimization in-
fluence the eigenvalue spectrum of the Hessian matrix at the end of training. Many subsequent
WorkS have studied the Hessian eigensPectrUm during and at the end of training. JaStrzebSki et al.
[38], Cohen et al. [39] studied the dynamics of the top eigenvalues during training. Sagun et al.
[40], Papyan [41], Ghorbani et al. [42] demonstrated the spectrum has a bulk of values near zero
plus a small number of larger outliers. Gur-Ari et al. [43] demonstrated that the learning dynamics
are constrained to the subspace spanned by the top eigenvectors, but found no special properties of
the dynamics within this subspace. In our work we also determine that the top eigensubspace of the
Hessian plays a crucial role in the limiting dynamics and by projecting the dynamics into this sub-
space in phase space, we see that the motion is not random, but consists of incoherent oscillations
leading to anomalous diffusion.
4	Modeling SGD as an Underdamped Langevin Equation
Following the route of previous works [31, 12, 33] studying the limiting dynamics of neural
networks, we first seek to model SGD as a continuous stochastic process. We consider a net-
work parameterized by θ ∈ Rm, a training dataset {x1, . . . , xN} of size N, and a training loss
L(θ) = ^N PN= ι '(θ, Xi) with corresponding gradient g(θ) = ∂L. The state of the network at the
kth step of training is defined by the position vector θk and velocity vector vk of the same dimension.
The gradient descent update with learning rate η, momentum β, and weight decayλ is given by
vk+1 = βvk - g(θk) - λθk,	θk+1 = θk + ηvk+1,	(2)
where we initialize the network such that v0 = 0 and θ0 is the parameter initialization. In order to
understand the dynamics of the network through position and velocity space, which we will refer to
as phase space, we express these discrete recursive equations as the discretization of some unknown
ordinary differential equation (ODE), sometimes referred to as a modified equation as in [44, 20].
While this ODE models the gradient descent process even at finite learning rates, it fails to account
for the stochasticity introduced by choosing a random batch B of size S drawn uniformly from the
set of N training points. This sampling yields the stochastic gradient ga(θ) = 1 Pii∈B V'(θ, Xi).
To model this effect, we make the following assumption:
3
Under review as a conference paper at ICLR 2022
Assumption 1 (CLT). We assume the batch gradient is a noisy version of the true gradient such
that gB(θ) - g(θ) is a Gaussian random variable with mean 0 and Covariance 1 Σ(θ).
Incorporating this model of stochastic gradients into the previous finite difference equation and
applying the stochastic counterparts to Euler discretizations, results in the standard drift-diffusion
stochastic differential equation (SDE), referred to as an underdamped Langevin equation,
θ	v	00
v]	[- η(Mβ) (g⑻+ λθ + (I - β)v∖l	I0 √ηS(1+β)
(3)
where Wt is a standard Wiener process. This is the continuous model we will study in this work:
Assumption 2 (SDE). We assume the underdamped Langevin equation (3) accurately models the
trajectory of the network driven by SGD through phase space such that θ(ηk) ≈ θk and v(ηk) ≈ vk.
See appendix A for further discussion on the nuances of modeling SGD with an SDE.
5 Linear Regression with SGD is an Ornstein-Uhlenbeck Process
Equipped with a model for SGD, we seek to understand its dynamics in the fundamental setting of
linear regression, one of the few cases where we have a complete model for the interaction of the
dataset, architecture, and optimizer. Let X ∈ RN ×d be the input data, Y ∈ RN be the output labels,
and θ ∈ Rd be our vector of regression coefficients. The least squares loss is the convex quadratic
loss L(θ) = 2N ∣∣Y-Xθk2 with gradient g(θ) = Hθ - b, where H = XNX and b = XNY. Plugging
this expression for the gradient into the underdamped Langevin equation (3), and rearranging terms,
results in the multivariate Ornstein-Uhlenbeck (OU) process,
θt
vt
0	-I -I θ Γθt-l	Γμ
[病询(H+λI)η⅛wIiIM-[0
--------{-------}
dt + √2κ-1
「0	0
2(1-β)
η(ι+β)
Σ(θ)
dWt,
(4)
where A and D are the drift and diffusion matrices re-
spectively, K = S(1 一 β2) is an inverse temperature con-
stant, and μ =(H + λI)-1b is the ridge regression solu-
tion. The solution to an OU process is a Gaussian process.
By solving for the temporal dynamics of the first and sec-
ond moments of the process, We can obtain an analytic
expression for the trajectory at any time t. In particular,
We can decompose the trajectory as the sum of a deter-
ministic and stochastic component defined by the first and
second moments respectively.
Deterministic component. Using the form of A We can
decompose the expectation as a sum of harmonic oscilla-
tors in the eigenbasis {qι,...,qm} of the Hessian,
B = 0.9
β = 0.99
Positti)On 叩吧 0Q6
Po部¾T
D
Figure 2: Oscillatory dynamics in
linear regression. We train a linear
network to perform regression on the
CIFAR-10 dataset by using an MSE
loss on the one-hot encoding of the la-
bels. We compute the hessian of the
loss, as well as its top eigenvectors.
The position and velocity trajectories
are projected onto the first eigenvector
of the hessian and visualized in black.
The theoretically derived mean, equa-
tion (5), is shown in red. The top and
bottom panels demonstrate the effect of
varying momentum on the oscillation
mode.
Velocity Velocity
E [[Vt]]	= [μ]	+ XX	") W	+ bi(t) 1)	. (5)
— — — ―	Ii ^1	— —	— — /
Here the coefficients ai(t) and bi(t) depend on the op-
timization hyperparameters η, β, λ, S and the respective
eigenvalue of the Hessian Pi as further explained in
appendix F. We verify this expression nearly perfectly
matches empirics on complex datasets under various hy-
perparameter settings as shown in Fig. 2.
Stochastic component. The cross-covariance of the pro-
cess between two points in time t ≤ s, is
CoVQvt , θsD= KT(B-e-AtBe-ATt)eAT(t-s),
S	(6)
d

A
∖
0
4
Under review as a conference paper at ICLR 2022
where B solves the Lyapunov equation AB + BA| = 2D. In order to gain analytic expressions for
B in terms of the optimization hyperparameters, eigendecomposition of the Hessian, and covariance
of the gradient noise, we must introduce the following assumption:
Assumption 3 (Simultaneously Diagonalizable). We assume the covariance of the gradient noise is
spatially independent Σ(θ) = Σ and commutes with the Hessian HΣ = ΣH, therefore sharing a
common eigenbasis.
6 Understanding Stationarity via the Fokker-Planck Equation
The OU process is unique in that it is one of the few SDEs which we can solve exactly. As shown in
section 5, we were able to derive exact expressions for the dynamics of linear regression trained with
SGD from initialization to stationarity by simply solving for the first and second moments. While the
expression for the first moment provides an understanding of the intricate oscillatory relationship in
the deterministic component of the process, the second moment, driving the stochastic component,
is much more opaque. An alternative route to solving the OU process that potentially provides more
insight is the Fokker-Planck equation.
The Fokker-Planck (FP) equation is a PDE describing the time evolution for the probability distribu-
tion of a particle governed by Langevin dynamics. For an arbitrary potential Φ and diffusion matrix
D, the Fokker-Planck equation (under an It6 integration prescription) is
∂tp = V ∙ (VΦp + 5 (κ-1Dp},	(7)
、----------{z---------}
-J
where p represents the time-dependent probability distribution, and J is a vector field commonly
referred to as the probability current. The FP equation is especially useful for explicitly solving for
the stationary solution, assuming one exists, of the Langevin dynamics. The stationary solution pss
by definition obeys ∂tpss = 0 or equivalently V ∙ Jss = 0. From this second definition we see that
there are two distinct settings of stationarity: detailed balance when Jss = 0 everywhere, or broken
detailed balance when V ∙ Jss = 0 and Jss = 0.
For a general OU process, the potential is a convex quadratic function Φ(x) = x|Ax defined by
the drift matrix A. When the diffusion matrix is isotropic (D α I) and spatially independent (V ∙
D = 0) the resulting stationary solution is a Gibbs distribution Pss(X) 8 e-κφ(x) determined
by the original loss Φ(x) and is in detailed balance. Lesser known properties of the OU process
arise when the diffusion matrix is anisotropic or spatially dependent [45, 46]. In this setting the
solution is still a Gaussian process, but the stationary solution, if it exists, is no longer defined by
the Gibbs distribution of the original loss Φ(x), but actually a modified loss Ψ(x). Furthermore,
the stationary solution may be in broken detailed balance leading to a non-zero probability current
Jss(x). Depending on the relationship between the drift matrix A and the diffusion matrix D the
resulting dynamics of the OU process can have very nontrivial behavior.
In the setting of linear regression, anisotropy in the data distribution will lead to anisotropy in the
gradient noise and thus an anisotropic diffusion matrix. This implies that for most datasets we should
expect that the SGD trajectory is not driven by the original least squares loss, but by a modified loss
and converges to a stationary solution with broken detailed balance, as predicted by Chaudhari and
Soatto [33]. Using the explicit expressions for the drift A and diffusion D matrices we can compute
analytically the modified loss and stationary probability current,
ψ(θ,v)=([θ]-[μD|(U2)(]θHμ])，JssGv)=-qu(小曲pss，⑻
where Q is a skew-symmetric matrix and U is a positive definite matrix defined as,
Q = [ 0、-W]	U = [ηc⅛β)WT(H + λI)	0 ]	(9)
Q = Σ(θ)	0	, u =	0	Σ(θ)-1 .	(9)
These new fundamental matrices, Q and u, relate to the original drift A and diffusion D matrices
through the unique decomposition A = (D + Q)u, introduced by Ao [47] and Kwon et al. [48].
Using this decomposition we can easily show that B = u-1 solves the Lyapunov equation and
indeed the stationary solution pss is the Gibbs distribution defined by the modified loss Ψ(θ, v)
in equation (8). Further, the stationary cross-covariance solved in section 5 reflects the oscillatory
dynamics introduced by the stationary probability currents Jss(θ, v) in equation (8). Taken together,
we gain the intuition that the limiting dynamics of SGD in linear regression are driven by a modified
loss subject to oscillatory probability currents.
5
Under review as a conference paper at ICLR 2022
7 Evidence of a Modified Loss and Oscillations in Deep Learning
Does the theory derived in the linear regression setting (sections 5, 6) help explain the empirical
phenomena observed in the non-linear setting of deep neural networks (section 2)? In order for the
theory built in the previous sections to apply to the limiting dynamics of neural networks, we must
introduce simplifying assumptions on the loss landscape and gradient noise at the end of training:
Assumption 4 (Quadratic Loss). We assume that at the end of training the loss for a neural network
can be approximated by the quadratic loss L(θ) = (θ — μ)1 (H) (θ — μ), where H 巳 0 is the
training loss Hessian and μ is some unknown mean vector, corresponding to a local minimum.
Assumption 5 (Covariance Structure). We assume the covariance of the gradient noise is propor-
tional to the Hessian of the quadratic loss Σ(θ) = σ2H where σ ∈ R+ is some unknown scalar.
Under these simplifications, then the expressions derived in the linear regression setting would apply
to the limiting dynamics of deep neural networks and depend only on quantities that we can easily
estimate empirically. Of course, these simplifications are quite strong, but without arguing their the-
oretical validity, we can empirically test their qualitative implications: (1) a modified isotropic loss
driving the limiting dynamics through parameter space, (2) implicit regularization of the velocity
trajectory, and (3) oscillatory phase space dynamics determined by the Hessian eigen-structure.
Modified loss. As discussed in section 6, due to the anisotropy of the diffusion matrix, the loss
landscape driving the dynamics at the end of training is not the original training loss L(θ), but a
modified loss Ψ(θ, v ) in phase space. As shown in equation (8), the modified loss decouples into a
term Ψθ that only depends on the parameters θ and a term Ψv that only depends on the velocities v .
Under assumption 5, the parameter dependent component is proportional to the convex quadratic,
| H-1(H+λI)
ψθ Y (θ-μ) ( η(i + β) )(θ-μ).
(10)
This quadratic function has the same mean μ as the training loss, but a different curvature. Using
this expression, notice that when λ ≈ 0, the modified loss is isotropic in the column space of H,
regardless of what the nonzero eigenspectrum of H is. This striking prediction suggests that no
matter how anisotropic the original training loss — as reflected by poor conditioning of the Hessian
eigenspeCtrUm — the training trajectory of the network will behave isotropically, since it is driven not
by the original anisotropic loss, but a modified isotropic loss.
We test this prediction by studying the limiting dynamics of a pre-trained ResNet-18 model with
batch normalization that We continue to train on ImageNet according to the last setting of its hyper-
parameters [49]. Let θ* represent the initial pre-trained parameters of the network, depicted with the
white dot in figures 3 and 4.
Training loss Ltrain	Test loss LteSt	Modified loss W
Figure 3: The training trajectory behaves isotropically, regardless of the training loss. We
resume training of a pre-trained ResNet-18 model on ImageNet and project its parameter trajec-
tory (black line) onto the space spanned by the eigenvectors of its pre-trained Hessian q1, q30 (with
eigenvalue ratio ρ1 /ρ30 ` 6). We sample the training and test loss within the same 2D subspace
and visualize them as a heatmap in the left and center panels respectively. We visualize the mod-
ified loss computed from the eigenvalues (ρ1 , ρ30) and optimization hyperparameters according to
equation (10) in the right plot. Note the projected trajectory is isotropic, despite the anisotropy of
the training and test loss.
6
Under review as a conference paper at ICLR 2022
We estimate1 the top thirty eigenvectors q1,..., q30 of the
Hessian matrix H* evaluated at θ* and project the limit-
ing trajectory for the parameters onto the plane spanned
by the top q1 and bottom q30 eigenvectors to maximize the
illustrated anisotropy with our estimates. We sample the
train and test loss in this subspace for a region around the
projected trajectory. Additionally, using the hyperparam-
eters of the optimization, the eigenvalues ρ1 and ρ30, and
the estimate for the mean μ = θ* - H-1 g* (g* is the gra-
dient evaluated at θ*), We also sample from the modified
loss equation (10) in the same region. Figure 3 shows the
projected parameter trajectory on the sampled train, test
and modified losses. Contour lines of both the train and
test loss exhibit anisotropic structure, with sharper curva-
ture along eigenvector q1 compared to eigenvector q30, as
expected. However, as predicted, the trajectory appears
to cover both directions equally. This striking isotropy
of the trajectory within a highly anisotropic slice of the
loss landscape indicates qualitatively that the trajectory
evolves in a modified isotropic loss landscape.
Implicit velocity regularization. A second qualitative
prediction of the theory is that the velocity is regulated
by the inverse Hessian of the training loss. Of course
there are no explicit terms in either the train or test losses
that depend on the velocity. Yet, the modified loss con-
tains a component, Ψv a v|HTv, that only depends on
the velocities This additional term can be understood as a
form of implicit regularization on the velocity trajectory.
Indeed, when we project the velocity trajectory onto the
plane spanned by the q1 and q30 eigenvectors, as shown
in Fig. 4, we see that the trajectory closely resembles the
curvature of the inverse Hessian H-1. The modified loss
is effectively penalizing SGD for moving in eigenvectors
of the Hessian with small eigenvalues. A similar quali-
tative effect was recently proposed by Barrett and Dherin
[21] as a consequence of the discretization error due to
finite learning rates.
Phase space oscillations. A final implication of the the-
ory is that at stationarity the network is in broken detailed
balance leading to non-zero probability currents flowing
through phase space:
v
Jss(θ,v)= [-η(1+β) (H + λI)(θ-μ)] pss.	(II)
Figure 4: Implicit velocity regulariza-
tion defined by the inverse Hessian.
The shape of the projected velocity tra-
jectory closely resembles the contours
of the modified loss Ψv.
Step
045 0
Position
Figure 5: Phase space oscillations are
determined by the eigendecomposi-
tion of the Hessian. We visualize the
projected position and velocity trajecto-
ries in phase space. The top and bottom
panels show the projections onto q1 and
q30 respectively. Oscillations at differ-
ent rates are distinguishable for the dif-
ferent eigenvectors and were verified by
comparing the dominant frequencies in
the fast Fourier transform of the trajec-
tories.
These probability currents encourage oscillatory dynamics in the phase space planes characterized
by the eigenvectors of the Hessian, at rates proportional to their eigenvalues. We consider the same
projected trajectory of the ResNet-18 model visualized in figures 3 and 4, but plot the trajectory in
phase space for the two eigenvectors q1 and q30 separately. Shown in Fig. 5, we see that both trajec-
tories look like noisy clockwise rotations. Qualitatively, the trajectories for the different eigenvectors
appear to be rotating at different rates.
The integral curves of the stationary probability current are one-dimensional paths confined to level
sets of the modified loss. These paths might cross themselves, in which case they are limit cycles, or
they could cover the entire surface of the level sets, in which case they are space-filling curves. This
distinction depends on the relative frequencies of the oscillations, as determined by the pairwise
1To estimate the eigenvectors of H* we use subspace iteration, and limit ourselves to 30 eigenvectors to
constrain computation time. See appendix H for details.
7
Under review as a conference paper at ICLR 2022
ratios of the eigenvalues of the Hessian. For real-world datasets, with a large spectrum of incom-
mensurate frequencies, we expect to be in the latter setting, thus contradicting the suggestion that
SGD in deep networks converges to limit cycles, as claimed in Chaudhari and Soatto [33].
8	Understanding the Diffusive B ehaviour of the Limiting
Dynamics
Taken together the empirical results shown in section 7 indicate that many of the same qualitative
behaviors of SGD identified theoretically for linear regression are evident in the limiting dynamics
of neural networks. Can this theory quantitatively explain the results we identified in section 2?
Constant instantaneous speed. As noted in section 2, we observed that at the end of training,
across various architectures, the squared norm of the local displacement kδt k22 remains essentially
constant. Assuming the limiting dynamics are described by the stationary solution the expectation
of the local displacement is
2
Ess [kδt k2] = q/ R2∖ σ2tr(H),	(12)
S(1 - β )
as derived in appendix G. We cannot test this prediction directly as we do not know σ2 and comput-
ing tr(H) is computationally prohibitive. However, we can estimate σ 2tr(H) by resuming training
for a model, measuring the average kδtk2, and then inverting equation (12). Using this single es-
timate, We find that for a sweep of models with varying hyperparameters, equation (12) accurately
predicts their instantaneous speed. Indeed, Fig. 6 shows an exact match between the empirics and
theory, which strongly suggests that despite changing hyperparameters at the end of training, the
model remains in the same quadratic basin.
Learning rate ∣
250	500	750	1000	0.80	0.85	0.90	0.95
Batch SiZeS	Momentum β
Figure 6: Understanding how the hyperparameters of optimization influence the diffusion. We
resume training of pre-trained ResNet-18 models on ImageNet using a range of learning rates, batch
sizes, and momentum coefficients, tracking ∣∣δtk2 and k ∆t k2. Starting from the default hyperparam-
eters, namely η = 1e 一 4, S = 256, and β = 0.9, we vary each one while keeping the others fixed.
The top row shows the measured kδtk2 in color, with the default hyperparameter setting highlighted
in black. The dotted line depicts the predicted value from equation (12). The bottom row shows the
estimated exponent c found by fitting a line to the k∆t k2 trajectories on a log-log plot. The dotted
line shows c = 1, corresponding to standard diffusion.
Exponent of anomalous diffusion. The expected value for the global displacement under the sta-
tionary solution can also be analytically expressed in terms of the optimization hyperparameters and
the eigendecomposition of the Hessian as,
2t
Ess g『]=S(Iη-β2) σ2 Ir(H) t + 2tX
m
X ρlCl(k) ,	(13)
l=1
where Ci (k) is a trigonometric function describing the velocity of a harmonic oscillator with damp-
ing ratio Zl = (1 一 β)/y∕2η(1 + β) (Pl + λ), see appendix G for details. As shown empirically in
section 2, the squared norm k∆t k2 monotonically increases as a power law in the number of steps,
suggesting its expectation is proportional to tc for some unknown, constant c. The exponent c de-
termines the regime of diffusion for the process. When c = 1, the process corresponds to standard
Brownian diffusion. For c > 1 or c < 1 the process corresponds to anomalous super-diffusion
or sub-diffusion respectively. Unfortunately, it is not immediately clear how to extract the explicit
8
Under review as a conference paper at ICLR 2022
exponent c from equation (13). However, by exploring the functional form of Cl (k) and its relation-
ship to the hyperparameters of optimization through the damping ratio ζl, we can determine overall
trends in the diffusion exponent c.
Akin to how the exponent c determines the regime of diffusion, the damping ratio ζl determines the
regime for the harmonic oscillator describing the stationary velocity-velocity correlation in the lth
eigenvector of the Hessian. When ζl = 1, the oscillator is critically damped implying the velocity
correlations converge to zero as quickly as possible. In the extreme setting of Cl (k) = 0 for all
l, k, then equation (13) simplifies to standard BroWnian diffusion, Ess [∣∣∆tk2] 8 t. When Zl > 1,
the oscillator is overdamped implying the velocity correlations dampen slowly and remain positive
even over long temporal lags. Such long lasting temporal correlations in velocity lead to faster
global displacement. Indeed, in the extreme setting of Cl (k) = 1 for all l, k, then equation (13)
simplifies to a form of anomalous super-diffusion, Ess [k∆tk2] α t2. When Zl < 1, the oscillator is
underdamped implying the velocity correlations Will oscillate quickly betWeen positive and negative
values. Indeed, the only Way equation (13) could describe anomalous sub-diffusion is if Cl (k) took
on negative values for certain l, k.
Using the same sWeep of models described previously, We can empirically confirm that the opti-
mization hyperparameters each influence the diffusion exponent c. As shoWn in Fig. 6, the learning
rate, batch size, and momentum can each independently drive the exponent c into different regimes
of anomalous diffusion. Notice hoW the influence of the learning rate and momentum on the dif-
fusion exponent c closely resembles their respective influences on the damping ratio Zl . Interest-
ingly, a larger learning rate leads to underdamped oscillations, and the resultant temporal velocities’
anti-correlations reduce the exponent of anomalous diffusion. Thus contrary to intuition, a larger
learning rate actually leads to slower global transport in parameter space. The batch size on the other
hand, has no influence on the damping ratio, but leads to an interesting, non-monotonic influence
on the diffusion exponent. Overall, the hyperparameters of optimization and eigenspectrum of the
Hessian all conspire to govern the degree of anomalous diffusion at the end of training.
9	Discussion
Through combined empirics and theory based on statistical physics, We uncovered an intricate in-
terplay betWeen the optimization hyperparameters, structure in the gradient noise, and the Hessian
matrix at the end of training.
Significance. The significance of our Work lies in (1) the identification/verification of multiple
empirical phenomena (constant instantaneous speed, anomalous diffusion in global displacement,
isotropic parameter exploration despite anisotopic loss, velocity regularization, and slower global
parameter exploration With faster learning rates) present in the limiting dynamics of deep neural
netWorks, (2) the emphasis on studying the dynamics in velocity space in addition to parameter
space, and (3) concrete quantitative as Well as qualitative predictions of an SDE based theory that
We empirically verified in deep netWorks trained on large scale datasets (indeed some of the above
nontrivial phenomena Were predictions of this theory). Of course, these contributions directly build
upon a series of related Works studying the immensely complex process of deep learning. To this
end, We further clarify the originality of our contributions With respect to some relevant Works.
Originality. The empirical phenomena We present provide novel insight With respect to the Works
of Wan et al. [13], Hoffer et al. [18], and Chen et al. [15]. We observe that all parameters in the
netWork (not just those With scale symmetry) move at a constant instantaneous speed at the end
of training and diffuse anomalously at rates determined by the hyperparameters of optimization.
In contrast to the Work by Liu et al. [35], We modeled the entire SGD process as an OU process
Which alloWs us to provide insight into the transient dynamics and identify oscillations in parameter
and velocity space. We build on the theoretical frameWork used by Chaudhari and Soatto [33] and
provide explicit expressions for the limiting dynamics in the simplified linear regression setting and
conclude that the oscillations present in the limiting dynamics are more likely to be space-filling
curves (and not limit cycles) in deep learning due to many incommensurate oscillations.
Overall, by identifying key phenomena, explaining them in a simpler setting, deriving predictions
of neW phenomena, and providing evidence for these predictions at scale, We are furthering the
scientific study of deep learning. We hope our neWly derived understanding of the limiting dynamics
of SGD, and its dependence on various important hyperparameters like batch size, learning rate, and
momentum, can serve as a basis for future Work that can turn these insights into algorithmic gains.
9
Under review as a conference paper at ICLR 2022
References
[1]	Radford M Neal. Priors for infinite networks. In Bayesian Learning for Neural Networks,
pages 29-53. Springer, 1996.
[2]	Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep infor-
mation propagation. arXiv preprint arXiv:1611.01232, 2016.
[3]	Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington,
and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint
arXiv:1711.00165, 2017.
[4]	Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. arXiv preprint arXiv:1806.07572, 2018.
[5]	Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Roman Novak, Jascha
Sohl-Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear
models under gradient descent. arXiv preprint arXiv:1902.06720, 2019.
[6]	Mei Song, Andrea Montanari, and P Nguyen. A mean field view of the landscape of two-layers
neural networks. Proceedings of the National Academy of Sciences, 115:E7665-E7671, 2018.
[7]	Grant M Rotskoff and Eric Vanden-Eijnden. Parameters as interacting particles: long time
convergence and asymptotic error scaling of neural networks. In Proceedings of the 32nd
International Conference on Neural Information Processing Systems, pages 7146-7155, 2018.
[8]	Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-
parameterized models using optimal transport. arXiv preprint arXiv:1805.09545, 2018.
[9]	Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, and Andrew Gordon Wil-
son. Loss surfaces, mode connectivity, and fast ensembling of dnns. In Proceedings of the 32nd
International Conference on Neural Information Processing Systems, pages 8803-8812, 2018.
[10]	Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon
Wilson. Averaging weights leads to wider optima and better generalization. arXiv preprint
arXiv:1803.05407, 2018.
[11]	Wesley Maddox, Timur Garipov, Pavel Izmailov, Dmitry Vetrov, and Andrew Gordon Wilson.
A simple baseline for bayesian uncertainty in deep learning. arXiv preprint arXiv:1902.02476,
2019.
[12]	StanisIaW Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua
Bengio, and Amos Storkey. Three factors influencing minima in sgd. arXiv preprint
arXiv:1711.04623, 2017.
[13]	Ruosi Wan, Zhanxing Zhu, Xiangyu Zhang, and Jian Sun. Spherical motion dynamics of deep
neural netWorks With batch normalization and Weight decay. arXiv preprint arXiv:2006.08419,
2020.
[14]	Marco Baity-Jesi, Levent Sagun, Mario Geiger, Stefano Spigler, Gerard Ben Arous, Chiara
Cammarota, Yann LeCun, Matthieu Wyart, and Giulio Biroli. Comparing dynamics: Deep
neural netWorks versus glassy systems. In International Conference on Machine Learning,
pages 314-323. PMLR, 2018.
[15]	Guozhang Chen, Cheng Kevin Qu, and Pulin Gong. Anomalous diffusion dynamics of learning
in deep neural netWorks. arXiv preprint arXiv:2009.10588, 2020.
[16]	Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, EdWard Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. Neural Information Processing Systems Workshop, 2017.
[17]	Zhiyuan Li, Kaifeng Lyu, and Sanjeev Arora. Reconciling modern deep learning With tradi-
tional optimization analyses: The intrinsic learning rate. In Advances in Neural Information
Processing Systems, volume 33, pages 14544-14555. Curran Associates, Inc., 2020.
10
Under review as a conference paper at ICLR 2022
[18]	Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the gen-
eralization gap in large batch training of neural networks. In Advances in Neural Information
Processing Systems, volume 30. Curran Associates, Inc., 2017.
[19]	Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks,
12(1):145-151,1999.
[20]	Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel LK Yamins, and Hidenori
Tanaka. Neural mechanics: Symmetry and broken conservation laws in deep learning dy-
namics. arXiv preprint arXiv:2012.04728, 2020.
[21]	David GT Barrett and Benoit Dherin. Implicit gradient regularization. arXiv preprint
arXiv:2009.11162, 2020.
[22]	Qianxiao Li, Cheng Tai, and E Weinan. Stochastic modified equations and adaptive stochastic
gradient algorithms. In International Conference on Machine Learning, pages 2101-2110.
PMLR, 2017.
[23]	Samuel L Smith, Benoit Dherin, David GT Barrett, and Soham De. On the origin of implicit
regularization in stochastic gradient descent. arXiv preprint arXiv:2101.12176, 2021.
[24]	Francesca Mignacco, Florent Krzakala, Pierfrancesco Urbani, and Lenka Zdeborovd. Dynam-
ical mean-field theory for stochastic gradient descent in gaussian mixture classification. arXiv
preprint arXiv:2006.06098, 2020.
[25]	Stefano Sarao Mannelli, Eric Vanden-Eijnden, and Lenka Zdeborovd. Optimization and gen-
eralization of shallow neural networks with quadratic activation functions. arXiv preprint
arXiv:2006.15459, 2020.
[26]	Francesca Mignacco, Pierfrancesco Urbani, and Lenka Zdeborovd. Stochasticity helps to nav-
igate rough landscapes: comparing gradient-descent-based algorithms in the phase retrieval
problem. Machine Learning: Science and Technology, 2021.
[27]	Stefano Sarao Mannelli and Pierfrancesco Urbani. Just a momentum: Analytical study of
momentum-based acceleration methods in paradigmatic high-dimensional non-convex prob-
lems. arXiv preprint arXiv:2102.11755, 2021.
[28]	Sho Yaida. Fluctuation-dissipation relations for stochastic gradient descent. arXiv preprint
arXiv:1810.00004, 2018.
[29]	Umut Simsekli, Levent Sagun, and Mert Gurbuzbalaban. A tail-index analysis of stochastic
gradient noise in deep neural networks. In International Conference on Machine Learning,
pages 5827-5837. PMLR, 2019.
[30]	Zhiyuan Li, Sadhika Malladi, and Sanjeev Arora. On the validity of modeling sgd with stochas-
tic differential equations (sdes). arXiv preprint arXiv:2102.12470, 2021.
[31]	Stephan Mandt, Matthew Hoffman, and David Blei. A variational analysis of stochastic gra-
dient algorithms. In International conference on machine learning, pages 354-363. PMLR,
2016.
[32]	Alnur Ali, Edgar Dobriban, and Ryan Tibshirani. The implicit regularization of stochastic
gradient flow for least squares. In International Conference on Machine Learning, pages 233-
244. PMLR, 2020.
[33]	Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational infer-
ence, converges to limit cycles for deep networks. In 2018 Information Theory and Applica-
tions Workshop (ITA), pages 1-10. IEEE, 2018.
[34]	Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George Dahl,
Chris Shallue, and Roger B Grosse. Which algorithmic choices matter at which batch sizes?
insights from a noisy quadratic model. In Advances in Neural Information Processing Systems,
volume 32. Curran Associates, Inc., 2019.
11
Under review as a conference paper at ICLR 2022
[35]	Kangqiao Liu, Liu Ziyin, and Masahito Ueda. Noise and fluctuation of finite learning rate
stochastic gradient descent. In International Conference on Machine Learning, pages 7045-
7056. PMLR, 2021.
[36]	Liu Ziyin, Kangqiao Liu, Takashi Mori, and Masahito Ueda. On minibatch noise: Discrete-
time sgd, overparametrization, and bayes. arXiv preprint arXiv:2102.05375, 2021.
[37]	Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping
Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp min-
ima. arXiv preprint arXiv:1609.04836, 2016.
[38]	StaniSIaW JaStrzebski, Zachary Kenton, Nicolas Ballas, ASja Fischer, Yoshua Bengio, and
Amos Storkey. On the relation between the sharpest directions of dnn loss and the sgd step
length. arXiv preprint arXiv:1807.05031, 2018.
[39]	Jeremy M Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet TalWalkar. Gra-
dient descent on neural netWorks typically occurs at the edge of stability. arXiv preprint
arXiv:2103.00065, 2021.
[40]	Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis
of the hessian of over-parametrized neural netWorks. arXiv preprint arXiv:1706.04454, 2017.
[41]	Vardan Papyan. The full spectrum of deepnet hessians at scale: Dynamics With sgd training
and sample size. arXiv preprint arXiv:1811.07062, 2018.
[42]	Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net opti-
mization via hessian eigenvalue density. In International Conference on Machine Learning,
pages 2232-2241. PMLR, 2019.
[43]	Guy Gur-Ari, Daniel A Roberts, and Ethan Dyer. Gradient descent happens in a tiny subspace.
arXiv preprint arXiv:1812.04754, 2018.
[44]	Nikola B Kovachki and AndreW M Stuart. Analysis of momentum methods. arXiv preprint
arXiv:1906.04285, 2019.
[45]	Crispin W Gardiner et al. Handbook of stochastic methods, volume 3. springer Berlin, 1985.
[46]	Hannes Risken. Fokker-planck equation. In The Fokker-Planck Equation, pages 63-95.
Springer, 1996.
[47]	Ping Ao. Potential in stochastic differential equations: novel construction. Journal of physics
A: mathematical and general, 37(3):L25, 2004.
[48]	Chulan KWon, Ping Ao, and David J Thouless. Structure of stochastic dynamics near fixed
points. Proceedings of the National Academy of Sciences, 102(37):13029-13033, 2005.
[49]	Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recogni-
tion, pages 770-778, 2016.
[50]	Tomaso Poggio, Kenji KaWaguchi, Qianli Liao, Brando Miranda, Lorenzo Rosasco, Xavier
Boix, Jack Hidary, and Hrushikesh Mhaskar. Theory of deep learning iii: explaining the non-
overfitting puzzle. arXiv preprint arXiv:1801.00173, 2017.
[51]	Levent Sagun, Leon Bottou, and Yann LeCun. Eigenvalues of the hessian in deep learning:
Singularity and beyond. arXiv preprint arXiv:1611.07476, 2016.
[52]	Valentin Thomas, Fabian Pedregosa, Bart Merrienboer, Pierre-Antoine Manzagol, Yoshua
Bengio, and Nicolas Le Roux. On the interplay betWeen noise and curvature and its effect
on optimization and generalization. In International Conference on Artificial Intelligence and
Statistics, pages 3503-3513. PMLR, 2020.
[53]	Richard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the fokker-
planck equation. SIAM journal on mathematical analysis, 29(1):1-17, 1998.
[54]	Richard Jordan, David Kinderlehrer, and Felix Otto. Free energy and the fokker-planck equa-
tion. Physica D: Nonlinear Phenomena, 107(2-4):265-271, 1997.
12
Under review as a conference paper at ICLR 2022
A	Modeling SGD with an SDE
As explained in section 4, in order to understand the dynamics of stochastic gradient descent we
build a continuous Langevin equation in phase space modeling the effect of discrete updates and
stochastic batches simultaneously.
A. 1 Modeling Discretization
To model the discretization effect we assume that the system of update equations (2) is actually a
discretization of some unknown ordinary differential equation. To uncover this ODE, we combine
the two update equations in (2), by incorporating a previous time step θk-1, and rearrange into
the form of a finite difference discretization, as shown in equation (??). Like all discretizations,
the Euler discretizations introduce error terms proportional to the step size, which in this case is
the learning rate η. Taylor expanding θk+1 and θk-1 around θk, its easy to show that both Euler
η
discretizations introduce a second-order error term proportional to 2θ.
θk+1-⅛ = θ + ηθ + O(η2),	θk-⅛≡1 = θ - ηθ + O(η2).
η2	η2
Notice how the momentum coefficient β ∈ [0, 1] regulates the amount of backward Euler incorpo-
rated into the discretization. When β = 0, we remove all backward Euler discretization leaving
just the forward Euler discretization. When β = 1, we have equal amounts of backward Euler as
forward Euler resulting in a central second-order discretization2 as noticed in [19].
A.2 Modeling Stochasticity
In order to model the effect of stochastic batches, we first model a batch gradient with the following
assumption:
Assumption 1 (CLT). We assume the batch gradient is a noisy version of the true gradient such that
gB(θ) — g(θ) is a Gaussian random variable with mean 0 and Covariance 1 Σ(θ).
The two conditions needed for the CLT to hold are not exactly met in the setting of SGD. Indepen-
dent and identically distributed. Generally we perform SGD by making a complete pass through the
entire dataset before using a sample again which introduces a weak dependence between samples.
While the covariance matrix without replacement more accurately models the dependence between
samples within a batch, it fails to account for the dependence between batches. Finite variance.
A different line of work has questioned the Gaussian assumption entirely because of the need for
finite variance random variables. This work instead suggests using the generalized central limit the-
orem implying the noise would be a heavy-tailed α-stable random variable [29]. Thus, the previous
assumption is implicitly assuming the i.i.d. and finite variance conditions apply for large enough
datasets and small enough batches.
Under the CLT assumption, we must also replace the Euler discretizations with Euler-Maruyama
discretizations. For a general stochastic process, dXt = μdt + σdWt, the Euler-Maruyama method
extends the Euler method for ODES to SDEs, resulting in the update equation Xk+ι = Xk + ∆tμ +
√∆tσξ, where ξ 〜N(0,1). Notice, the key difference is that if the temporal step size is ∆t = η,
then the noise is scaled by the square root √η. In fact, the main argument against modeling SGD
with an SDE, as nicely explained in Yaida [28], is that most SDE approximations simultaneously
assume that ∆t → 0+, while maintaining that the square root of the learning rate √η is finite.
However, by modeling the discretization and stochastic effect simultaneously we can avoid this
argument, bringing us to our second assumption:
Assumption 2 (SDE). We assume the underdamped Langevin equation (3) accurately models the
trajectory of the network driven by SGD through phase space such that θ(ηk) ≈ θk and v(ηk) ≈ vk.
This approach of modeling discretization and stochasticity simultaneously is called stochastic mod-
ified equations, as further explained in Li et al. [22].
2The difference between a forward Euler and backward Euler discretization is a second-order central dis-
cretization, (θk+n-θk) - (θk-nk-1) = η ( θk+1-^+θk-1 ) = ηθ + O肾.
13
Under review as a conference paper at ICLR 2022
B S tructure in the Covariance of the Gradient Noise
As we’ve mentioned before, SGD introduces highly structured noise into an optimization process,
often assumed to be an essential ingredient for its ability to avoid local minima.
Assumption 5 (Covariance Structure). We assume the covariance of the gradient noise is propor-
tional to the Hessian of the quadratic loss Σ(θ) = σ2H where σ ∈ R+ is some unknown scalar.
In the setting of linear regression, this is a very natural assumption. If we assume the classic genera-
tive model for linear regression data yi = x|θ^+σe where, θ ∈ Rd is the true model and E 〜 N (0, 1),
then provably Σ(θ) ≈ σ2H.
Proof. We can estimate the covariance as Σ(θ) ≈ 得 PN=I gig1 — ggl. Near stationarity ggl《
N PN=I gigl, and thus,
1N
∑(θ) ≈ NN Egig∣.
Under the generative model yi = x∣θ + σe where E 〜N(0,1) and σ ∈ R+, then the gradient gi is
gi = (xl(θ 一 θθ) 一 σe)xi,
and the matrix gigil is
gigil = (xil (θ 一 θθ) 一 σE)2 (xixil).
Assuming θ ≈ θθ at stationarity, then (xil (θ 一 θθ) 一 σE)2 ≈ σ2. Thus,
2N	2
Σ(θ) ≈ NN X XiX∣ = NNXlX = σ2H
i=1
Also notice that weight decay is independent of the data or batch and thus simply shifts the gradient
distribution, but leaves the covariance of the gradient noise unchanged.	□
While the above analysis is in the linear regression setting, for deep neural networks it is reasonable
to make the same assumption. See the appendix of JaStrzebSki et al. [12] for a discussion on this
assumption in the non-linear setting.
Recent work by Ali et al. [32] also studies the dynamics of SGD (without momentum) in the setting
of linear regression. This work, while studying the classic first-order stochastic differential equation,
made a point to not introduce an assumption on the diffusion matrix. In particular, they make the
point that even in the setting of linear regression, a constant covariance matrix will fail to capture
the actual dynamics. To illustrate this point they consider the univariate responseless least squares
problem,
1n
minimize - 5"^(xiθ)2.
θ∈R	2n	i
i=1
As they explain, the SGD update for this problem would be
θk+1 = θk - S
k
θk = Y(1 -η(1X Xi))θo,
i=1	i∈B
from which they conclude for a small enough learning rate η, then with probability one θk →
0. They contrast this with the Ornstein-Uhlenbeck process given by a constant covariance matrix
where while the mean for θk converges to zero its variance converges to a positive constant. So
is this discrepancy evidence that an Ornstein-Uhlenbeck process with a constant covariance matrix
fails to capture the updates of SGD? In many ways this problem is not a simple example, rather a
pathological edge case. Consider the generative model that would give rise to this problem,
y = 0x + 0ξ = 0.
In otherwords, the true model θθ = 0 and the standard deviation for the noise N = 0. This would
imply by the assumption used in our paper that there would be zero diffusion and the resulting SDE
would simplify to a deterministic ODE that exponentially converges to zero.
14
Under review as a conference paper at ICLR 2022
C A Quadratic Loss at the End of Training
Assumption 4 (Quadratic Loss). We assume that at the end of training the loss for a neural network
can be approximated by the quadratic loss L(θ) = (θ 一 μ)1 (H^) (θ 一 μ), where H 占 0 is the
training loss Hessian and μ is some unknown mean vector, corresponding to a local minimum.
This assumption has been amply used in previous works such as Mandt et al. [31], Jastrzebski et al.
[12], and Poggio et al. [50]. Particularly, Mandt et al. [31] discuss how this assumption makes sense
for smooth loss functions for which the stationary solution to the stochastic process reaches a deep
local minimum from which it is difficult to escape.
It is a well-studied fact, both empirically and theoretically, that the Hessian is low-rank near local
minima as noted by Sagun et al. [51], and Kunin et al. [20]. This degeneracy results in flat directions
of equal loss. Kunin et al. [20] discuss how differentiable symmetries, architectural features that
keep the loss constant under certain weight transformations, give rise to these flat directions. Im-
portantly, the Hessian and the covariance matrix share the same null space, and thus we can always
restrict ourselves to the image space of the Hessian, where the drift and diffusion matrix will be full
rank. Further discussion on the relationship between the Hessian and the covariance matrix can be
found in Thomas et al. [52].
It is also a well known empirical fact that even at the end of training the Hessian can have negative
eigenvalues [41]. This empirical observation is at odds with our assumption that the Hessian is
positive semi-definite H 0. Further analysis is needed to alleviate this inconsistency.
15
Under review as a conference paper at ICLR 2022
D Solving an Ornstein-Uhlenbeck Process with Anis otropic
Noise
We will study the multivariate Ornstein-Uhlenbeck process described by the stochastic differential
equation
dXt = A(μ — Xt)dt + √2κ-1 D dWt	Xo = xo,	(14)
where A ∈ Sm+ is a positive definite drift matrix, μ ∈ Rm is a mean vector, K ∈ R+ is some positive
constant, and D ∈ S+m+ is a positive definite diffusion matrix. This OU process is unique in that it
is one of the few SDEs we can solve explicitly. We can derive an expression for XT as,
XT = e-AT xo +
(I - e-AT)μ+Z
eA(JT) y∕2κ-1DdWt.
(15)
Proof. Consider the function f (t, x) = eAtx where eA is a matrix exponential.
Lemma3 we can evaluate the derivative of f (t, Xt) as
df (t, Xt) = (AeAtXt + eAtA(μ — Xt)) dt + eAt√2κ-1DdWt
=AeAtμdt + eAt√2κ-1DdWt
Integrating this expression from t = 0 to t = T gives
f(T,XT) — f(0,Xo)= / AeAtμdt + / eAt√2κ-1DdWt
oo
eATXT — xo = (eAT — I) μ + [ eAt√2κ-1DdWt
which rearranged gives the expression for XT .
Then by It6's
□
From this expression it is clear that XT is a Gaussian process. The mean of the process is
E [XT] = e-ATxo + (I — e-AT) μ,
and the covariance and cross-covariance of the process are
Var(XT)
κ-1 Z eA(t-T) 2DeA| (t-T)dt,
o
Cov(XT, XS)
min(T,S)
κ-1
o
A(t-T)2DeA| (t-S)dt
(16)
(17)
(18)
These last two expressions are derived by It6 Isometry4.
D. 1 The Lyapunov Equation
We can explicitly solve the integral expressions for the covariance and cross-covariance exactly by
solving for the unique matrix B ∈ S+m+ that solves the Lyapunov equation,
AB + BA| = 2D.	(19)
If B solves the Lyapunov equation, notice
dt 卜A(t-T)BeAl(t-S)) = eA(t-T)ΑBeAl(t-s) + eA(t-T)BA∣eAMt-S)
= eA(t-T)2DeA| (t-S)
Using this derivative, the integral expressions for the covariance and cross-covariance simplify as,
Var(XT) = κ-1 B — e-ATBe-A|T ,	(20)
Cov(XT, XS) = κ-1 B — e-ATBe-A|T eA| (T-S),	(21)
where we implicitly assume T ≤ S.
3It6's Lemma states that for any It6 drift-diffusion process dXt = μtdt + σtdWt and twice differentiable
scalar function f (t, x), then df (t, Xt) = fft + μtfx + σtfxx) dt + σtfχdWt.
4It6 Isometry states for any standard It6 process Xt, then E
R0tXtdWt2 = E hR0t Xt2 dti.
16
Under review as a conference paper at ICLR 2022
D.2 Decomposing the Drift Matrix
While the Lyapunov equation simplifies the expressions for the covariance and cross-covariance, it
does not explain how to actually solve for the unknown matrix B . Following a method proposed by
Kwon et al. [48], we will show how to solve for B explicitly in terms of the drift A and diffusion D.
The drift matrix A can be uniquely decomposed as,
A = (D + Q)U	(22)
where D is our symmetric diffusion matrix, Q is a skew-symmetric matrix (i.e. Q = -Q|), and U is
a positive definite matrix. Using this decomposition, then B = U-1, solves the Lyapunov equation.
Proof. Plug B = U-1 into the left-hand side of equation (19),
AU-1 + U-1A| = (D + Q)UU-1 +U-1U(D-Q)
=(D+Q)+(D-Q)
= 2D
Here We used the symmetry of A,D,U and the skew-symmetry of Q.	□
All that is left is to do is solve for the unknown matrices Q and U. First notice the following identity,
AD - DA = QA + AQ
(23)
Proof. Multiplying A = (D + Q)U on the right by (D - Q) gives,
A(D-Q) = (D+Q)U(D-Q)
=(D + Q)A|,
which rearranged and using A = AT gives the desired equation.	□
Let V ΛV | be the eigendecomposition of A and define the matrices D
These matrices observe the following relationship,
λi - λj e
Lj Dij.
V|DV and Q = V|QV.
(24)
Proof. Replace A in the previous equality with its eigendecompsoition,
V ΛV TD — DV ΛV | = QV ΛV | + V ΛV |Q.
Multiply this equation on the right by V and on the left by V|,
ΛDe - De Λ = QeΛ + ΛQe .
Looking at this equality element-wise and using the fact that Λ is diagonal gives the scalar equality
for any i, j ,
(λi - λj)Dij = (λi + λj)Qij,
which rearranged gives the desired expression.	□
Thus, Q and U are given by,
Q = VQeVT,	U = (D + Q)-1A.	(25)
This decomposition always holds uniquely when A, D » 0, as ：i-：j exists and (D + Q) is invert-
ible. See [48] for a discussion on the singularities of this decomposition.
D.3 Stationary Solution
Using the Lyapunov equation and the drift decomposition, then XT 〜PT, where
PT = N (e-ATX0 + (I - e-AT) μ, KT (UT- e-ATUTe-AIT)) .	(26)
In the limit as T → ∞, then e-AT → 0 and pT → pss where
Pss = N (μ,κ-1U-1) .	(27)
Similarly, the cross-covariance converges to the stationary cross-covariance,
Covss(XT ,Xs ) = KTBeAI(T-S).	(28)
17
Under review as a conference paper at ICLR 2022
E A Variational Formulation of the OU Process with
Anisotropic Noise
In this section we will describe an alternative, variational, route towards solving the dynamics of the
OU process studied in appendix D.
Let Φ : Rn → R be an arbitrary, non-negative potential and consider the stochastic differential
equation describing the Langevin dynamics of a particle in this potential field,
dXt = -VΦ(Xt)dt + P2κ-1D(Xt)dWt,	Xo = xo,	(29)
where D(Xt) is an arbitrary, spatially-dependent, diffusion matrix, κ is a temperature constant, and
x0 ∈ Rm is the particle’s initial position. The Fokker-Planck equation describes the time evolution
for the probability distribution p of the particle’s position such that p(x, t) = P(Xt = x). The FP
equation is the partial differential equation5,
∂tp = V ∙ (VΦ(Xt)p + KTV ∙ (D(XQp) , p(x, 0) = δ(xo),	(30)
where V∙ denotes the divergence and δ(x0) is a dirac delta distribution centered at the initialization
x0 . To assist in the exploration of the FP equation we define the vector field,
J(x,t) = -VΦ(Xt)p - V ∙ (D(Xt)P),	(31)
which is commonly referred to as the probability current. Notice, that this gives an alternative ex-
pression for the FP equation, ∂tp = -V∙ J, demonstrating that J (x, t) defines the flow of probability
mass through space and time. This interpretation is especially useful for solving for the stationary
solution pss, which is the unique distribution that satisfies,
∂tPss = -V ∙ Jss = 0,	(32)
where Jss is the probability current forpss. The stationary condition can be obtained in two distinct
ways:
1.	Detailed balance. This is when Jss(X) = 0 for all X ∈ Ω. This is analogous to reversibility
for discrete Markov chains, which implies that the probability mass flowing from a state i
to any state j is the same as the probability mass flowing from state j to state i.
2.	Broken detailed balance. This is when V ∙ Jss(x) = 0 but Jss(x) = 0 for all X ∈ Ω. This
is analogous to irreversibility for discrete Markov chains, which only implies that the total
probability mass flowing out of state i equals to the total probability mass flowing into state
i.
The distinction between these two cases is critical for understanding the limiting dynamics of the
process.
E.1 The Variational Formulation of the Fokker-Planck Equation with
Isotropic Diffusion
We will now consider the restricted setting of standard, isotropic diffusion (D = I). It is easy
enough to check that in this setting the stationary solution is
pss(X)
e-κΦ(x)
-Z^^
Z =	e-κΦ(x)dX,
Jω
(33)
where pss is called a Gibbs distribution and Z is the partition function. Under this distribution,
the stationary probability current is zero (Jss (X) = 0) and thus the process is in detailed balance.
Interestingly, the Gibbs distribution pss has another interpretation as the unique minimizer of the the
Gibbs free energy functional,
F(p) = E [Φ] - κ-1 H (p),	(34)
where E [Φ] is the expectation of the potential Φ under the distribution p and H(p) =
—hp(x)log(p(x))dx is the Shannon entropy of p.
5This PDE is also known as the Forward Kolmogorov equation.
18
Under review as a conference paper at ICLR 2022
Proof. To prove that indeed pss is the unique minimizer of the Gibbs free energy functional, consider
the following equivalent expression
F (p) =	p(x)Φ(x)dx + κ-1	p(x)log(p(x))dx
=κ-1∕ p(x)(log(p(x)) - Iog(Pss(Xy)) dx - κ-1∕ log(Z)
= κ-1DKL(p k pss) - κ-1log(Z)
From this expressions, it is clear that the KUllback-Leibler divergence is uniquely minimized when
P = Pss.	□
In other words, with isotropic diffusion the stationary solution pss can be thought of as the limiting
distribution given by the Fokker-Planck equation or the unique minimizer of an energetic-entropic
functional.
Seminal work by Jordan et al. [53] deepened this connection between the Fokker-Planck equation
and the Gibbs free energy functional. In particular, their work demonstrates that the solution P(x, t)
to the Fokker-Planck equation is the Wasserstein gradient flow trajectory on the Gibbs free energy
functional.
Steepest descent is always defined with respect to a distance metric. For example, the update equa-
tion, Xk+ι = Xk - ηVΦ(χk), for classic gradient descent on a potential Φ(χ), can be formu-
lated as the solution to the minimization problem Xk+ι = argminχηΦ(x) + 2d(x,xk)2 where
d(X, Xk) = kX - Xk k is the Euclidean distance metric. Gradient flow is the continuous-time limit of
gradient descent where we take η → 0+. Similarly, Wasserstein gradient flow is the continuous-time
limit of steepest descent optimization defined by the Wasserstein metric. The Wasserstein metric is
a distance metric between probability measures defined as,
W2 (μ1,μ2)
inf
p∈π(μi,μ2)
/
Rn×Rn
|X - y|2P(dX, dy),
(35)
where μι and μ2 are two probability measures on Rn with finite second moments and Π(μ1,μ2)
defines the set of joint probability measures with marginals μι and μ2. Thus, given an initial distri-
bution and learning rate η, we can use the Wasserstein metric to derive a sequence of distributions
minimizing some functional in the sense of steepest descent. In the continuous-time limit as η → 0+
this sequence defines a continuous trajectory of probability distributions minimizing the functional.
Jordan et al. [54] proved, through the following theorem, that this process applied to the Gibbs free
energy functional converges to the solution to the Fokker-Planck equation with the same initializa-
tion:
Theorem 1 (JKO). Given an initial condition P0 with finite second moment and an η > 0, define
the iterative scheme Pη with iterates defined by
Pk = argmi∖η (E [Φ] — KTH(P)) + W2 (p,pk-1).
As η → 0+, then Pη → P weakly in L1 where P is the solution to the Fokker-Planck equation with
the same initial condition.
See [54] for further explanation and [53] for a complete derivation.
E.2 Extending the Variational Formulation to the Setting of Anisotropic
Diffusion
While the JKO theorem provides a very powerful lens through which to view solutions to the Fokker-
Planck equation, and thus distributions for particles governed by Langevin dynamics, it only applies
in the very restricted setting of isotropic diffusion. In this section we will review work by Chaudhari
and Soatto [33] extending the variational interpretation to the setting of anisotropic diffusion.
Consider when D(Xt) is an anisotropic, spatially-dependent diffusion matrix. In this setting, the
original Gibbs distribution given in equation (33) does not necessarily satisfy the stationarity con-
dition equation (32). In fact, it is not immediately clear what the stationary solution is or if the
dynamics even have one. Thus, Chaudhari and Soatto [33] make the following assumption:
19
Under review as a conference paper at ICLR 2022
Stationary Assumption. Assume there exists a unique distribution pss that is the stationary solution
to the Fokker-Planck equation irregardless of initial conditions.
Under this assumption we can implicitly define the potential Ψ(x) = -κ-1log(pss(x)). Using this
modified potential we can express the stationary solution as a Gibbs distribution,
Pss(X) H e-κψ(x).	(36)
Under this implicit definition we can define the stationary probability current as Jss (x) =
j(x)pss (x) where
j(x) = -VΦ(x) — κ-1 V ∙ D(X) + D(x)VΨ(x).	(37)
The vector field j (x) reflects the discrepancy between the original potential Φ and the modified
potential Ψ according to the diffusion D(X). Notice that in the isotropic case, when D(X) = I,
then Φ = Ψ and j(X) = 0. Chaudhari and Soatto [33] introduce another property of j(X) through
assumption,
Conservative Assumption. Assume that theforce j(x) is conservative (i.e. V ∙ j(x) = 0).
Using this assumption, Chaudhari and Soatto [33] extends the variational formulation provided by
the JKO theorem to the anisotropic setting,
Theorem 2 (CS). Given an initial condition p0 with finite second moment, then the energetic-
entropic functional,
F(P) = Ep [Ψ(x)] — KTH(P)
monotonically decreases throughout the trajectory given by the solution to the Fokker-Planck equa-
tion with the given initial condition.
In other words, the Fokker-Plank equation (30) with anisotropic diffusion can be interpreted as mini-
mizing the expectation of a modified loss Ψ, while being implicitly regularized towards distributions
that maximize entropy. The derivation requires we assume a stationary solution Pss exists and that
the force j (x) implicitly defined by Pss is conservative. However, rather than implicitly define
Ψ(x) and j(x) through assumption, if we can explicitly construct a modified loss Ψ(x) such that
the resulting j (x) satisfies certain conditions, then the stationary solution exists and the variational
formulation will apply as well. We formalize this statement with the following theorem,
Theorem 3 (Explicit Construction). If there exists a potential Ψ(x) such that either j(x) = 0 or
V ∙ j(x) = 0 and VΨ(x) ⊥ j(x), then Pss is the Gibbs distribution H e-κψ(x) and the variational
formulation given in Theorem 2 applies.
E.3 Applying the Variational Formulation to the OU Process
Through explicit construction we now seek to find analytic expressions for the modified loss Ψ(x)
and force j(x) hypothesised by Chaudhari and Soatto [33] in the fundamental setting of an OU
process with anisotropic diffusion, as described in section D. We assume the diffusion matrix is
anisotropic, but spatially independent, V ∙ D(x) = 0. For the OU process the original potential
generating the drift is
Φ(x) = (x — μ)1 AL (x — μ).	(38)
Recall, that in order to extend the variational formulation we must construct some potential Ψ(x)
such that V ∙ j(x) = 0 and VΨ ⊥ j(x). It is possible to construct Ψ(x) using the unique decompo-
sition of the drift matrix A = (D + Q)U discussed in appendix D. Define the modified potential,
Ψ(x) = (x — μ)1 UU (x — μ).	(39)
Using this potential, the force j(x) is
j (x) = —A(x — μ) + DU (x — μ) = -QU (x — μ).	(40)
Notice that j(x) is conservative, V ∙ j(x) = V ∙ -QU (x — μ) = 0 because Q is skew-symmetric.
Additionally, j(x) is orthogonal, j(x)lVΨ(x) = (x — μ)1 UTQU (x — μ) = 0, again because Q is
skew-symmetric. Thus, we have determined a modified potential Ψ(x) that results in a conservative
orthogonal force j (x) satisfying the conditions for Theorem 3. Indeed the stationary Gibbs distri-
bution given by Theorem 3 agrees with equation (27) derived via the first and second moments in
appendix D,
e-κψ(X) HN (μ, KTU T)
20
Under review as a conference paper at ICLR 2022
In addition to the variational formulation, this interpretation further details explicitly the stationary
probability current, Jss (x) = j(x)pss, and whether or not the the stationary solution is in broken
detailed balance.
F Explicit expressions for the OU process Generated by SGD
We will now consider the specific OU process generated by SGD with linear regression. Here we
repeat the setup as explained in section 5.
Let X ∈ RN ×d, Y ∈ RN be the input data, output labels respectively and θ ∈ Rd be our vector of
regression coefficients. The least squares loss is the convex quadratic loss L(θ)=击 ∣∣Y - Xθ∣∣2
with gradient g(θ) = Hθ — b, where H = XNX and b = XNY. Plugging this expression for
the gradient into the underdamped Langevin equation (3), and rearranging terms, results in the
multivariate Ornstein-Uhlenbeck (OU) process,
d θt = A ( μ - θt ) dt + √2K-1DdW^t,
where A and D are the drift and diffusion matrices respectively,
-I
2(1—e) I ,
η(i+β) I」
2(i-β)
η(ι+β)
Σ(θ)
(41)
(42)
D
0
0
0
K = S(1 - β2) is a temperature constant, and μ =(H + λI)-1b is the ridge regression solution.
F.1 S olving for the Modified Loss and Conservative Force
In order to apply the expressions derived for a general OU process in appendix D and E, we must
first decompose the drift as A = (D + Q)U. Under the simplification Σ(θ) = σ2H discussed in
appendix B, then the matrices Q and U, as defined below, achieve this,
0	-σ2H
σ2H 0
TT 2	2H-1 (H + λI)	0
U	η(1+β)σ2
=	0	当 H-1
σ2
(43)
Using these matrices we can now derive explicit expressions for the modified loss Ψ(θ, v) and con-
servative force j(θ, v). First notice that the least squares loss with L2 regularization is proportional
to the convex quadratic,
Φ(θ) = (θ - μ)l(H + λI)(θ - μ).	(44)
The modified loss Ψ is composed of two terms, one that only depends on the position,
Ψθ (θ)=(θ-N (L 卜-μ),
and another that only depends on the velocity,
Ψv (v) = v|
(45)
(46)
The conservative force j(θ, v) is
j (θ,v)=]-ηc⅛) (h+λ∣ )(θ - μ)
and thus the stationary probability current is Jss (θ, v) = j (θ, v)pss.
(47)
F.2 Decomposing the Trajectory into the Eigenbasis of the Hessian
As shown in appendix D, the temporal distribution for the OU process at some time T ≥ 0 is,
pT vθ =N e-AT
Vθ + (I - e-AT) W , KT (u-1 - e-ATUTe-ATT
Here we will now use the eigenbasis {q1, . . . , qm} of the Hessian with eigenvalues {ρ1, . . . , ρm} to
derive explicit expressions for the mean and covariance of the process through time.
21
Under review as a conference paper at ICLR 2022
θ
v
E
Deterministic component. We can rearrange the expectation as
[μl + e-AT [θ0- μ.
0	v0
Notice that the second, time-dependent term is actually the solution to the system of ODEs
with initial condition [θo - μ vo]1. This system of ODEs can be block diagonalized by factorizing
A = OSO| where O is orthogonal and S is block diagonal defined as
q1	0
0	q1
qm	0
0 qm
一 0
η(i+β)(PI + λ)
S =
-1
2(i-β)
η(ι+β)
0	-1
2	(C _|_ λ)	2(1-β)
η(i+β)(Pm + λ)	η(i+β)
In otherwords in the plane spanned by [qi 0]| and [0
2D system
ai =	0
bi∖	[-η(i+β)(Pi + λ)
qi]| the system of ODEs decouples into the
1
2(1-β)
η(ι+β)-
ai
bi
O
This system has a simple physical interpretation as a damped harmonic oscillator. If We let b = a⅛,
then we can unravel this system into the second order ODE
1-β	2
ai+2 η(ι+β)a i+η(ι+β)(Pi+λ)ai=0
which is in standard form (i.e. X + 2γX + ω2x = 0) for Y
η⅛+⅛and ωi = η∕++β)
(Pi + λ).
Let ai(0) = hθo - μ, qii and %(0) =(v。，qii, then the solution in terms of Y and ωi is
e-γt (电(0) cosh (pγ2 - ωi21) + γ√02-ω(0) Sinh (pY2 — ωit))	Y >ω%
ai(t) =	e-γt (ai (0) + (Yai (0) + bi(0))t)	Y = ωi
e-γt Qi(0)cos (Pω22 - Y21) + 弋：；*0) Sin ('ωi2 - γ21))	γ <ωi
Differentiating these equations gives us solutions for bi (t)
e-γt Qi (0) CoSh (PY2 -染t) - ω2√γ2-ωi(O) Sinh (pγ2 - ωi21))	γ >ω%
bi(t) =	< e-γt	(bi(0) - (ω2ai(0) + )bi(0)) D	Y = ωi
e-γt	(bi(0)cos (Pω22 - Yt) - ω2√ω2+y0)	Sin ('ω2	- Y2O[	Y <ωi
Combining all these results, we can now analytically decompose the expectation as the sum,
E [[θ]] = [μ]+XX ")[qi]+bi ⑴[：]).
i=1
Intuitively, this equation describes a damped rotation (spiral) around the OLS solution in the planes
defined by the the eigenvectors of the Hessian at a rate proportional to the respective eigenvalue.
22
Under review as a conference paper at ICLR 2022
Stochastic component. Using the previous block diagonal decomposition A
simplify the variance as
OSOT we can
θ
v
Var
KT U-1
e-Aτ UTe-ATT
KT U-1
e-osoττ U-1 e-osTOTT
	
	
KTO (OTU-1O - e-ST(OtU-1O)e-STT) Ot
Interestingly, the matrix OtUTO is also block diagonal,
OtU-1O = Ot
-η(1+β22 (H + λI)-1 H
0
0
σ2H
~η(1+β)σ2 ρι
2	Pi +ʌ
0
2
σ2ρι
η(1 + β)b2	Pm
2	Pm + λ
0
0
σ2ρm
O
0
Thus, similar to the mean, we can simply consider the variance in each of the planes spanned by
[qi 0]t and [0 %]t . Ifwe define the block matrices,
ησ2	Pi
Di
2S(1-β) P^+λ
0
0
S(1⅛ Pi
Si
0
η(1+β) (Pi + λ)
1 一
2(1-β)
η(ι+β).
	
	
then the projected variance matrix in this plane simplifies as
Var
qiTθ
qiTv
R - e-SiTDie-SiTT
Using the solution to a damped harmonic osccilator discussed previously, we can express the matrix
exponential e-SiT explicitly in terms of Y
a∕∣72 - ωt2∖, then the matrix exponential is
ι-β
η(i+β)
and ωi =《η(1+β) (ρi + λ). If we let a
e-γt
e-sit = e
e-γt
cosh (ait) + — sinh (αit)
一ωi sinh (ait)
1 + Yt t
-ω2t 1 — γt
cos (ait) + — sin (ait)
—-i sin(ait)	(
O- sinh (αit)
cosh (αit) — O sinh (a*)
击 sin (ait)
cos (ait) - -Y- sin (ait)
γ >ωi
3i
Y < ωi
-Yt
Y
23
Under review as a conference paper at ICLR 2022
G Analyzing Properties of the Stationary Solution
Assuming the stationary solution is given by equation (??) we can solve for the expected value of
the norm of the local displacement and gain some intuition for the expected value of the norm of
global displacement.
G. 1 Instantaneous Speed
Ess kδkk2 =Ess kθk+1 -θkk2
= η2Ess kvk+1 k2
=ηtr (Ess [vk+1v∣+l])
= η2tr (Varss (vk+1) + Ess [vk+1] Ess [vk+1]|)
=η2tr (κ-1UT)
η2
=S(1-β2)tr(σ H)
Note that this follows directly from the definition of δk in equation (1) and the mean and variance of
the stationary solution in equation ( ??), as well as the follow-up derivation in appendix F.
G.2 Anomalous Diffusion
Notice, that the global movement ∆t = θt -θ0 can be broken up into the sum of the local movements
∆t = Pit=1 δi, where δi = θi - θi-1. Applying this decomposition,
t
Ess [k∆tk2] = Ess	X δi
i=1
tt
XEss [kδik2] +XEss[hδi,δji]
i=1	i6=j
As we solved for previously,
2
Ess [kδik 2] = η Ess [kvi k2 ] = η%r(Varss (Vi))= S(1 - β 2) tr (σ^2H).
By a similar simplification, we can express the second term in terms of the stationary cross-
covariance,
Ess [hδi , δji] = ηEss [hvi, vji] = ηtr (Covss (vi, vj )) .
Thus, to simplify this expression we just need to consider the velocity-velocity covariance
Covss(vi, vj). At stationarity, the cross-covariance for the system in phase space, zi = [θi vi]
is
CovSS(Zi,Zj) = KTU-1e-A||i-j|
where κ = S(1 - β2 ), and
TT	HT (H + λI)	0
U	η(1+β)σ2
=	0	当 H T
σ2
-I
2(1-β) I
η(i+β) I
As discussed when solving for the mean of the OU trajectory, the drift matrix A can be block
diagonalized as A = OSO| where O is orthogonal and S is block diagonal defined as
qι 0	... qm 0
O =	..
.
0 q1 . . .	0	qm
一 0
η(i+β)(PI + λ)
S =
-1	一
2(1-β)	..
η(ι+β)	.
..
..
0	-1
2	(C + λ)	2(1 —β)
η(i+β)(Pm + λ)	η(i+β)	-I
24
Under review as a conference paper at ICLR 2022
Notice also that O diagonalizes	U-1 such that,
Λ = OTU-1O =	∣>(1+β)σ2 ρι	O	] 2	Pi+λ 0	σ2p1	... .. ..	.. η(1+β)σ2 Pm	0 2	ρm+λ	0 _	0	σ2 Pm _
Applying these decompositions, properties of matrix exponentials, and the cyclic invariance of the
trace, allows US to express the trace of the cross-covariance as
tr(CoVss(a, Zj))= KTtr (U-ie-Aτli-jl^
=KTtr(UTOe-STli-jlOτ)
=KTtr (Λe-STIi-jl)
n
=KT X tr (Λke-STli-jl)
k=1
where Λk and Sk are the blocks associated with each eigenvector of H. As solved for previously in
the variance of the OU process, we can express the matrix exponential e-Sk li-jl explicitly in terms
of Y = η⅛ and 3k
matrix exponential is
Jη(i+β) (pk + λ). If we let T = ∖i - j| and αk = √∣γ2 - ω2∣, then the
e-γτ
e-ski-jl = < g-γτ
e-γτ
cosh (α") + Ok sinh (akT)	Ok Sinh S")
一Ok sinh(αkT)	cosh(αkT)一看 Sinh (αkT)
1 + YT T
∙-3kτ	1 - YTl
cos (αkT) + Ok sin (akT)	看 Sin (akT)
-ωk sin (akT)	cos (akT) - Ok sin (α6)
Y >3k
Y = 3k
Y <3k
Plugging in these expressions into previous expression and restricting to just the kth velocity com-
ponent, we see
iκ-1σ2ρke-γτ (COSh (以T) - Ok Sinh (α")) Y >3k
κ-1σ2ρke-γτ (1 - YT)	Y = 3k
κ-1σ2ρke-γτ (cos (akT) - Sin (akT))	Y <3k
Pulling it all together,
ESS [Idk2]=s(n-β 2)
m
X ρι C，(k)
l = 1
where Cι (k) is defined as
Cι(k)
e-γk
e-γk
e-γk
(cosh (αιk) — O sinh (αιk))
(I-Yk)
(cos (αιk) - O sin (αιk))
Y >3ι
Y = 3ι
Y < 3ι
for Y = η(1+β), 3ι = Jη(1+β)(PI + λ), and αι =，卜2 - 32∖.
25
Under review as a conference paper at ICLR 2022
G.3 Training Loss and Equipartition Theorem
In addition to solving for the expected values of the local and global displacements, we can consider
the expected training loss and find an interesting relationship to the equipartition theorem from
classical statistical mechanics.
The regularized training loss is Lλ(θ) = 2(θ - μ)lH(θ - μ) + 2∣∣θ∣∣2, where H is the Hessian
matrix and μ is the true mean. Taking the expectation with respect to the stationary distribution,
Ess [Lλ(θ)] = 2tr((H + λI)Ess [θθl]) - μlHEss [θ] + 2μlHμ
The first and second moments of the stationary solution are
2
Ess 网=μ	Ess [θθr] =	—(H + λI厂IH + μμl
S(1 - β) 2
Plugging these expressions in and canceling terms we get
Ess Wλ(θ)] = 4" Q"(σ2H)+ λk〃『
4S(1 - β)	2
Define the kinetic energy of the network as K(V) = 11 m∣∣v∣∣2, where m = 2(1 + β) is the per-
parameter “mass" of the network according to our previously derived Langevin dynamics. At sta-
tionarity,
Ess [K(v)]=皆包tr(Ess [vv|]) = M G "(σ2H)
4	4S(1 - β)
where We used the fact that Ess [vv|] = 5.-§2)σ2H. In otherwords, at stationarity,
Ess [Lλ(θ)] = Ess [K(v)] + 2kμk2.
This relationship between the expected potential and kinetic energy can be understood as a form of
the equipartition theorem.
26
Under review as a conference paper at ICLR 2022
H Experimental Details
H.1 Computing the Hessian eigendecomposition
Computing the full Hessian of the loss with respect to the parameters is computationally intractable
for large models. However, equipped with an autograd engine, we can compute Hessian-vector prod-
ucts. We use the subspace iteration on Hessian-vector products computed on a variety of datasets.
For Cifar-10 we use the entire train dataset to compute the Hessian-vector products. For Imagenet,
we use a subset 40,000 images sampled from the train dataset to keep the computation within rea-
sonable limits.
For experiments on linear regression, the Hessian is independent of the model (it only depends on
the data) and can be computed using any model checkpoint. For all other experiments, the Hessian
eigenvectors were computed using the model at its initial pre-trained state.
H.2 Figure 1
We resumed training for a variety of ImageNet pre-trained models from Torchvision [16] for 10
epochs, with the hyperparameters used at the end of training, shown in table 1.
We kept track of the norms of the local and global displacement, kδkk22 and k∆k k22, every 250
steps in the training process, to keep the length of the trajectories within reasonable limits. kδk k22 is
visualized directly, along with its 15 step moving average. We then fit a power law of the form αkc
to the k∆k k22 trajectories for each model, using the last 2/3 of the saved trajectories. We visualize
the k∆kk22 trajectories along with their fits on a log-log plot.
Model	Dataset	Opt.	Epochs	Batch size S	LR η	Mom. β	WD λ
VGG-16	ImageNet	SGDM	10	256	10-5	0.9	5 X 10-4
VGG-11 w/BN	ImageNet	SGDM	10	256	10-5	0.9	5 × 10-4
VGG-16 w/BN	ImageNet	SGDM	10	256	10-5	0.9	5 × 10-4
ResNet-18	ImageNet	SGDM	10	256	10-4	0.9	10-4
ResNet-34	ImageNet	SGDM	10	256	10-4	0.9	10-4
Table 1: Figure 1 experiments training hyperparameters.
H.3 Figure 2
For this figure we trained a linear regression model on Cifar-10, using MSE loss on the one-hot
encoded labels. The hyperparameters used during training are shown in table 2.
At every step in training the full set of model weights and velocities were stored. The top 30 eigen-
vectors of the hessian were computed as described in appendix H.1, using 10 subspace iterations.
The saved weight and velocity trajectories were then projected onto the top eigenvector of the hessian
and were visualized in black. Using the initial weights and velocities, the red trajectories were
computed according to equation (5).
Model	Dataset	Opt.	Epochs	Batch size S	LR η	Mom. β	WD λ
Linear Regression	Cifar-10	SGDM	4	512	10-5	0.9	0
Linear Regression	Cifar-10	SGDM	4	512	10-5	0.99	0
Table 2: Figure 2 experiments training hyperparameters.
H.4 Figure 3
For this figure we constructed an arbitrary Ornstein-Uhlenbeck process with anisotropic noise which
would help contrast the original and modified potentials. We sampled from a 2 dimensional OU
process of the form
27
Under review as a conference paper at ICLR 2022
d x1 = A ( b - x1 ) dt + ^0-4√DdWt.
x2	x2
where we set b = [-0.1, 0.05]| and arbitrarily construct A such that it’s eigenvectors are aligned
with q1 = [-1, 1]| and q2 = [1, 1]| and it’s eigenvalues are 4 and 1 as follows:
D = 40	01
V = -11 11
A = V -1DV
The background for the left panel was computed from the convex quadratic potential Φ(x) =
1 XTAx - bx. The background for the right panel was computed from the modified quadratic
Ψ(x) = 2xlUx — ux, with U = (D + Q)TA and U = UA-1b (see equation (25)). Both
were sampled in a regular 40 × 40 grid in [-0.1, 0.1] × [-0.1, 0.1].
H.5 Figures 4, 5, and 6
Starting with the ImageNet pre-trained ResNet-18 from Torchvision [16], we resumed training for
5 epochs with the hyperparameters used at the end of training, shown in table3. The top 30 Hessian
eigenvectors were computed as described in appendix H.1, using 10 subspace iterations. During
training, we tracked the projection of the weights and velocities onto eigenvectors q1 and q30 .
For Figure 3, we show the projection of the position trajectory onto eigenvectors q1 and q30 in 2D in
black. The background for the left and center panels was computed taking the ImageNet pre-trained
ResNet-18 from Torchvision [16] and perturbing its weights in the q1 and q30 directions in a region
close to the projected trajectory. The training and test loss were computed for a grid of 20 × 20
perturbed models. The background for the right panel was computed according to equation (10).
Model I Dataset ∣ Opt. ∣ Epochs ∣ Batch size S ∣ LR η ∣ Mom. β ∣ WD λ
ResNet-18 ∣ ImageNet ∣ SGDM ∣	5	∣	256	∣ 10-4 ∖	0.9	∣ 10-4
Table 3: Figures 4,5,6 experiments training hyperparameters.
H.6 Figure 7
We resumed training for an ImageNet pre-trained ResNet-18 from Torchvision [16] for 2 epochs,
using the sweeps of hyperparameters shown in table 4. We indicate a sweep in a particular hyperpa-
rameter by [A, B], which denotes 20 evenly spaced numbers between A and B, inclusive.
We kept track of the norms of the local and global displacement, kδkk22 and k∆kk22 , every step in
the training process. The value for kδkk22 at the end of the two epochs is shown in the top row of
the figure. We fitted power law of the form αkc to the k∆k k22 trajectories for each model on the full
trajectories. The fitted exponent c for each model is plotted in the bottom row of the figure.
Model	Dataset	Opt.	Epochs	Batch size S	LR η	Mom. β	WD λ
ResNet-18	ImageNet	SGDM	2	[32, 1024]	10-4	0.9	10-4
ResNet-18	ImageNet	SGDM	2	256	[10-3, 10-5]	0.9	10-4
ResNet-18	ImageNet	SGDM	2	256	10-4	[0.8, 0.99]	10-4
Table 4: Figure 7 experiments training hyperparameters.
H.7 Increasing rate of anomalous diffusion
Upon further experimentation with the fitting procedure for the rate of anomalous diffusion ex-
plained in Figure 1, we observed an interesting phenomenon. The fitted exponent c for the power
law relationship ∣∣∆k ∣∣∣ α kc increases as a function of the length of the trajectory We fit to. As can
28
Under review as a conference paper at ICLR 2022
be seen in Figure 7, c increases at a diminishing rate with the length of the trajectory. This could
be indicative of k∆k k22 being governed by a sum of power laws where the leading term becomes
dominant for longer trajectories.
Step
Figure 7: The fitted rate of anomalous diffusion increases with the length of trajectory fitted.
The left panel shows the fitted power law on training trajectories of increasing length from the pre-
trained ResNet-18 model. The right panel shows the fitted exponent c as a function of the length of
the trajectory.
29