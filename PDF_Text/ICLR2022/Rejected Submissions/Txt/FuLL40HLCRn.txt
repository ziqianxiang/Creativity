Under review as a conference paper at ICLR 2022
ST-DDPM: Explore Class Clustering for Con-
ditional Diffusion Probabilistic Models
Anonymous authors
Paper under double-blind review
Ab stract
Score-based generative models involve sequentially corrupting the data distribution
with noise and then learns to recover the data distribution based on score matching.
In this paper, for the diffusion probabilistic models, we first delve into the changes
of data distribution during the forward process of the Markov chain and explore
the class clustering phenomenon. Inspired by the class clustering phenomenon, we
devise a novel conditional diffusion probabilistic model by explicitly modeling the
class center in the forward and reverse process, and make an elegant modification
to the original formulation, which enables controllable generation and gets inter-
pretability. We also provide another direction for faster sampling and more analysis
of our method. To verify the effectiveness of the formulated framework, we conduct
extensive experiments on multiple tasks, and achieve competitive results compared
with the state-of-the-art methods (conditional image generation on CIFAR-10 with
an inception score of 9.58 and FID score of 3.05).
1	Introduction
Deep generative models such as generative adversarial networks (GANs) Goodfellow et al. (2014);
Zhu et al. (2017); Brock et al. (2018), flows Rezende & Mohamed (2015); Kingma & Dhariwal
(2018); Ho et al. (2019), variational autoencoders (VAEs) Kingma & Welling (2013); Maal0e et al.
(2019), score-based generative models Song & Ermon (2019); Ho et al. (2020); Song et al. (2020b)
and autoregressive models Van Oord et al. (2016); Oord et al. (2016); Salimans et al. (2017) have
been proposed to generate high quality samples in wide variety of data modalities. The score-based
generative models involve sequentially corruptig the data distribution with noise and then learns
to recover the data distribution based on score matching (e.g. the gradient of the log probility
density Song & Ermon (2019); Song et al. (2020b), the noise corruption Ho et al. (2020)).
Given the Markov chain that gradually corrupts the data with noise, in this paper, we first delve
into the changes of data distribution during the forward process and visualize these behaviours.
Specifically, we measure the class clustering by considering the intra-class to inter-class variance
ratio at each time step. Low values of the variance ratio demonstrate better class separation since
the samples are concentrated around their corresponding class mean. We notice that in the early
stage of forward process, the data distribution maintains stable intra-class variance and inter-class
variance, leading to a balanced variance ratio. As the noise gradually increases, the class separation
decreases sharply and the data distribution is ultimately converted into the noise distribution, as
shown in Figure 1 (Left). The class clustering phenomenon reveals the process of diffusion where the
corruption noise first corrupts the datapoints without increasing the variance ratio, and then greatly
decreases the class separation and mixes the datapoints from different classes, which generates the
noise distribution.
Inspired by the class clustering phenomenon, we formulate a novel conditional diffusion probabilistic
model by explicitly modeling the class center in the forward and reverse process, which enables
controllable generation. Concretely, as shown in Figure 1 (Right), in the early stage of forward
process, samples are gradually clustered to their corresponding class center while all the samples are
finally gathered into the noise distribution as the diffusion progresses. The reverse process is then
modulated by the class center during generation:
1
Under review as a conference paper at ICLR 2022
Figure 1: Left: An example of the class clustering phenomenon. Right: The overview of the
proposed conditional diffusion probabilistic model where Ai , Bi are samples from different classes,
and A, R are the class centers.
1.	By explicitly introducing the class center, we make an elegant modification to the original
formulation Ho et al. (2020) and further decouple the module for conditional information
encoding and the module for noise prediction. The formulation that exploits the class center
is much simpler since we don’t need to train a separate model or apply heuristics and domain
knowledge to estimate the gradient of log likelihood Song et al. (2020b).
2.	The learned class centers, which get interpretability, also reflect the common characteristics
of datapoints from the same class (e.g. template face, number).
3.	With the explicitly-modeled class centers, we can naturally speed up the reverse process
with earlier starting, which is different from strided sampling Song et al. (2020a).
Due to the guided “shift” from sample to its corresponding class center during both the forward
and reverse process, we name the conditional DDPM as ST-DDPM. To verify the effectiveness of
the formulated framework ST-DDPM, we conduct extensive experiments on the task of conditional
image generation, free-form image inpainting Yu et al. (2019); Liu et al. (2018), attribute-to-image
synthesis Yan et al. (2016) and text-to-image synthesis Reed et al. (2016); Xu et al. (2018); Zhang
et al. (2018), and achieve competitive results compared with the original formulation. We also provide
another direction for faster sampling and further explore the relationship between the guided class
centers and other score-based methods.
2	Related Work
Score-based Generative Models. Score-based generative models Song & Ermon (2019); Ho et al.
(2020); Song et al. (2020b); Tae et al. (2021); Kim et al. (2021); Tashiro et al. (2021) involve
sequentially corruptig the data distribution with noise using Markov chains and then learns to convert
the noise distribution to the data distribution based on score matching. Score matching with Langevin
dynamics (SMLD) Song & Ermon (2019) learns to estimate the gradient of the log probility density
with respect to data at each time step and samples from a sequence of decreasing noise scales for
generation. Denoising diffusion probabilistic model (DDPM) Ho et al. (2020) trains the model with
denoising score matching to predict the noise corruption at each step and then reverse the noise
distribution during generation. Song et al. Song et al. (2020b) further propose a unified framework
to generalize prior works by introducing the stochastic differential equations (SDEs). There are
other methods to learn the reverse process of Markov chains including infusion training Bordes et al.
(2017), variational walkback Goyal et al. (2017), generative stochastic networks Alain et al. (2016)
and others Salimans et al. (2015); Song et al. (2017); Levy et al. (2017); Nijkamp et al. (2019);
Lawson et al. (2019).
2
Under review as a conference paper at ICLR 2022
3	Denoising Diffusion Probabilistic Models
The DDPM Ho et al. (2020) employs a forward diffusion Markov chain to step-by-step convert a
real data distribution into a standard Gaussian distribution, and then build a parameterized Markov
chain to model the reverse process. Specifically, the forward process starts from x0 which comes
from the real data distribution q(xo), and performs T steps of diffusion to generate xι, x2, ∙∙∙ , XT
sequentially. The forward trajectory is given by
T
q(X0：T) = q(xo) ∩q(xt∣Xt-i)	q(xt∣Xt-i) = N(xt； √1 - βtXt-1, βt∣	(1)
t=1
where βι, β2, •…，βτ correspond to aforementioned diffusion rate of each forward step and are fixed
as a variance schedule. The reverse process is conducted by parameterized operators pθ(xt-1 |xt):
T
Pθ(X0:T) = P(XT) ∏Pθ(Xt-1 ∣Xt)	Pθ(xt-i∣Xt) = N(Xt-1 ； μθ(xt,t), Σθ(xt,t))	(2)
t=1
where p(xT) = N(xT; 0, I). The DDPM is trained to minimize a variational bound of negative log
likelihood Eq [- logpθ(x0)], denoted by:
Eq [- logpθ(x0)] ≤ Eq
T
- log p(xT) -	log
t=1
Pθ (xt-i∣xt)
q(xt∣xt-i)
L.
Rewrite L (3) by Bayes rule establishes a connection betweenpθ(xt-ι∣xt) and q(xt-ι∣xt, xo) for
backward operators:
T
L = Eq - log P XT、- Xlog pθ xt-1 Xt、- logPθ(x0∣x1)	(3)
q(xT |x0)	t=2	q(xt-1|xt, x0)
T
=Eq KL(q(xT |xo) k P(XT)) + £KL(q(xt-1|xt, X0) k Pθ (xt-1∣Xt)) - log Pθ (X0∣X1),
t=2
(4)
where the forward process posterior conditioned upon x0 is related to the forward process distribution
via Bayes rule, denoted by:
q(xt-1|xt,x0)
q(xt∣Xt-1, X0)q(xt-1∣X0)
q(xtlxo)
Fortunately, all right items have closed form expressions, which can exactly derive a Gaussian
distribution, denoted by:
q(xt-1∣Xt, Xo) = N(xt-1; √αιβt Xo + √αt(1 Ft-1) Xt, 1	Ql βtI)
1 — Qt	1 — Qt	1 — Qt
where Qt = 1 - βt and(y.t = Qit=1 Qi . Consequently, the KL divergences between Gaussians in
L (4) can be computed analytically. Further simplications and reparameterizations Ho et al. (2020)
come to the following variant of the variational bound:
Lsimple(θ) = Et,xo,e [k E-W (√q7x0 + √1 - Qt ∈, t) ∣∣2]
4 Measuring Class Clustering in RGB and Feature S paces
We first delve into the changes of data distribution during the forward process and visualize these
behaviours. Concretely, we measure the class clustering in RGB and feature spaces by considering
the intra-class to inter-class variance ratio at each time step, given by
σintra-class _ C Σ›i,j kxi,j	μi k2
σinter— class =N	P, kμ -μik2
(5)
3
Under review as a conference paper at ICLR 2022
T = O
T = 800
T = 500
T = 1000
Figure 2:	The changes of data distribution and class clustering in RGB space on MNIST dataset.
T = 1000
Figure 3:	The changes of data distribution and class clustering in feature space on CIFAR-10 dataset.
where C is the number of classes, N is the number of data points, xi,j is the j-th sample in class
i, μi is the mean of samples of class i and μ is the mean across all the samples. Low values of the
variance ratio demonstrate better class separation. We compute the variance ratio and visualize the
data distribution on MNIST and CIFAR-10 dataset. Note that for better visualization, the feature
vectors for CIFAR-10 dataset are computed based on pre-trained ResNet18 backbone.
As shown in Figure 2 and Figure 3 given by t-SNE visualization, in the early stage of forward
process, the data distribution maintain stable intra-class variance and inter-class variance, leading
to a balanced class clustering (T approximately from 0 to 800). As the noise gradually increases,
the class separation decreases sharply and the data distribution is ultimately converted into the noise
distribution (T = 1000). The class clustering phenomenon inspires us to explicitly introduce the class
center or class template during forward and reverse process for controllable generation.
5 The Formulation of ST-DDPM
Inspired by the class clustering phenomenon, we formulate the conditional diffusion probabilistic
model ST-DDPM by explicitly modeling the condition as the class center in the forward and reverse
process. Concretely, we introduce the condition encoding network to obtain the class center or class
template as u = uφ(c), where c is the given condition (e.g. corrupted image, class label or attributes),
and then rebuild the diffusion operators and modify the original recurrence relation Eq. (1), given by:
__	1	1
q(xt ∣Xt-i, U) = N(xt； √αtxt-i+αt4 (1-αt4 )u, βtI)
Xt = √αtXt-i+αt4 (1-αt )u+ √βtet,
(6)
4
Under review as a conference paper at ICLR 2022
where we add a condition-related item for Gaussian mean to make the forward process has a “shift”
toward the class center. With the recurrence relation, we can derive the general xt sampling formula:
1	1	1	1	1
Xt =	λ∕α^tχo	+ aj4	(1 — α4 )u + ʌ/l —	ɑtWt	= α4	α4χo +	(1 — α4 )u	+ ʌ/l — &Wt.	(7)
Algorithm 1: Training
repeat
Xo 〜q(χo)
u = uφ(c)
t 〜Uniform(1, 2,…，T)
W 〜N(0,I)
Optimize ∣∣ W - [w6(√0txo + n + √1 — &w,t) 一
until converged;
√n⅛] k2
Algorithm 2: Sampling
XT 〜N(0, I)
for t = T to 1 do
w 〜N(0, I) if t ≥ 2, else W = 0
Xt-1	= √=	∣^Xt-√βt ,	∣^wθ(xt,t)-t== i	一 mti	+ q/βt(1-at÷1)W
_ t 1	√αt	|_ t	√1-at	|_ 八 S)	√1-αt J tJ V (1-0t)
return X0
See Appendix A.1 for detailed derivation. An intuitive explanation of the proposed diffusion operators
and the weight-scheduling of U can be given as follows: Since & is closer to 1 in the early stage,
the class-guided forward process first corrupts the sample while maintains the specificity of samples.
As the diffusion procedures, the samples from the same class gather at the class center. As 国 gets
smaller and closer to 0, the data distribution is ultimately converted into noise distribution. In total,
inspired by the class clustering, the well-designed weights firstly allow the specificity of datapoints,
then gather datapoints from the same class for universality, and finally turn the distribution into pure
noise by eliminating the impact of class means.
1	1	1	1
For convenience, We denote at4 * (1 一 at4 )u = mt and at4 (1 一 %)u = nt. Then the forward process
posterior distribution q(Xt-1|Xt, X0, u) can be derived from probability density:
q(xt-i ∣Xt, Xo, U) = N(xt-1； μt(xt, Xo, u), σ2I),
(8)
where
/	ʌ	√αt(1 一 at-1)	.
μt(Xt, Xo, u)=-一-——二-Xt+
1 一 Qt
Bt√& * * 9t-ι.
1 一出
l一1 一 C^.t-1	2	βt(1 一 αt-1)
XlEnt+nt-1	σt=	(1一 at).
(9)
See Appendix A.2 for detailed derivation. Then we can directly model parameterized μe (Xt, u, t) to
predict μt(Xt, Xo, u). Similar to Ho et al. (2020), we further reparameterize Eq. (7) for Wt 〜N(0, I),
then the forward process posterior mean becomes:
Xt — nt — √1 一 QtWθ(Xt, u,t)	1 Γ	βt	f 4、
μt(Xt,--------------7≡=-------------, u) = -- Xt ——X	_ wθ(Xt, u,t) 一 mt .
√Qt	√Qt L √1 — Qt	」
We then optimize the simplified loss to predict the corruption noise W:
Lsimple(φ, θ) =Et,xo,e [|| W — wΘ(√QtXO + nt + √1 一 Qtw, u,t) ||2 *]
Since we explicitly model the class center during the forward process, from Eq. (7), we have
_ Xt - √QtXo _	nt
√	√1 一 Qt	√1 一 Qt ,
(10)
(11)
(12)
where we can extract U out of parameters and use wθ(Xt,t) to predict xt√-tχ0, and decouple
the condition encoding module uφ and the denoising module Wθ, making the final simplified loss
Lsimple (φ, θ) becomes:
Lsimple (φ, θ) = Et,xo,e j k W 一 wΘ (√QtXO + nt + √1 一 QtW,t)-H t _ k2 f	(13)
IL	√1 一 Qt」J
5
Under review as a conference paper at ICLR 2022
G
01023 456 700 9
O — XΓp4v678σ-
OI 83y5J 7Eq
Class center 四0
Figure 4: Left: Class-conditional samples on 28×28 MNIST and the trained conditional encoding
network uφ (last row). Right: Class-conditional samples on 32×32 CIFAR-10.


Algorithm 1 and Algorithm 2 separately display the complete training procedure with this simplified
objective and sampling procedure with the proposed ST-DDPM.
6	Experiments
To verify the effectiveness of the formulated framework ST-DDPM, we conduct extensive experiments
on the task of conditional image generation, free-form image inpainting Yu et al. (2019); Liu et al.
(2018) and text-to-image synthesis Reed et al. (2016); Xu et al. (2018); Zhang et al. (2018). Following
prior work Ho et al. (2020), we set T = 1000 and set the diffusion rate increasing linearly from
β1 = 10-4 to βT = 0.02 for all experiments. For the denoising network θ, we use the same
architecture as Ho et al. (2020), which is a U-Net based on a Wide ResNet since we focus on the
impact of our formulation. More experimental details can be found in the Appendix B.
6.1	Class-Conditional Image Generation
For class-conditional image generation, the con-
dition refers to an one-hot vector corresponding
to a specific class label. To consturct the condi-
tion encoding network uφ , we map the one-hot
vector to an initialized embedding and employ
stacked convolution and upsample layers to pre-
dict the class center.
Figure 4 (Left) presents the class-conditional
synthesis results and the learned class centers
on the 28×28 MNIST dataset. As shown in the
last row, the learned class centers also exhibit
the shape of corresponding numbers, further in-
dicating the class clustering phenomenon men-
tioned in the previous section. Figure 4 (Right)
presents the conditional synthesis results on the
32×32 CIFAR-10 Krizhevsky et al. (2009) dataset, also verifying the capability and effectiveness
of our formulated method for controllable generation. The results of negative log likelihoods are
presented in Appendix C and more examples can be found in the Appendix E.
To explore the sample quality, we compute the Inception scores and FID scores on CIFAR-10 dataset,
as shown in Table 1. For comparison, following Song et al. (2020b), we introduce a time-dependent
classifier and compute the gradient for class-conditional sampling (grad. DDPM). Also, we build a
simple conditional DDPM (cond. DDPM) by directly injecting a class embedding along with the
Table 1: CIFAR-10 sample quality.
Model	FIDJ	IS↑
Unconditional
NCSN Song & Ermon (2019)	25.32	8.87 ± 0.12
NCSNv2 Song & Ermon (2020)	10.87	8.40 ± 0.07
DDPM Ho et al. (2020)	3.17	9.46 ± 0.11
NCSN++ cont. (deep, VE) Song et al. (2020b)	2.20	9.89
Conditional
Projection Discriminator Miyato & Koyama (2018)	17.5	8.62
FQ-GAN Zhao et al. (2020)	5.34	8.50
BigGAN Brock et al. (2018)	14.73	9.22
grad. DDPM	4.25	9.18
cond. DDPM	3.82	9.40
ST-DDPM	3.05	9.58 ± 0.09
6
Under review as a conference paper at ICLR 2022
Right: The progressive decoding with the same intialized noise modulated by different class centers
dataset. Right: More examples and corresponding class center uφ .
timestep embedding. Details are given in the Appendix B. With the same denoising architecture
as the unconditional DDPM Ho et al. (2020), the proposed ST-DDPM even obtains better IS score
of 9.58 and FID score of 3.05. For class-conditional generation, our model achieves competitive
results with the state-of-the-art methods, and better sample quality than most methods in the literature,
indicating the effectiveness of the proposed method.
As shown in Figure 5, we further visualize the progressive diffusion and decoding of ST-DDPM
modulated by different class centers. The ST-DDPM explicitly models the class center during the
forward and reverse process, making the class clustering more significant. Figure 5 (Left) presents
the progressive diffusion where the class clustering phenomenon is marked by the red bounding box.
Figure 5 (Right) shows the progressive decoding toward different classes with the same initialized
noise, qualitatively verifying the capability of ST-DDPM for controllable generation.
6.2	Image Inpainting
Image inpainting Yu et al. (2019); Liu et al. (2018) aims to synthesize contents for the missing regions
of a given image such that the result is visually realistic and semantically correct. Different from
class-conditional generation where the conditional encoding network is confined to a limited and
discrete space (i.e. the number of class labels), the condition space of image inpainting is continuous
and non-enumerable. To further prove the effectiveness of ST-DDPM that explicitly models the
class center, we consider different images for imputation as different class and build a continuous
conditional encoding network to generate the class center. Concretely, we denote the corrupted image
as the input condition and apply a U-Net based architecture to predict its corresponding class center,
making the class centers non-enumerable. The reverse process also starts from noise and is modulated
by the predicted center.
Figure 7 separately presents the inpainting results on the 256×256 Place2 Zhou et al. (2017), 256×256
CelebA-HQ Liu et al. (2015) and 256×256 LSUN church Yu et al. (2015) dataset, which indicates
the effectiveness and scalability of the ST-DDPM method. More examples can be found in the
Appendix E. We also visualize the progressive diffusion decoding and the predicted class center in
Figure 6. Notice that the decoupled condition encoding network has learned to predict the template
face (i.e. uφ) based on uncorrupted region, and the decoding process is modulated by the class center
for inpainting, further demonstrating the effectiveness and interpretability of our proposed ST-DDPM.
The further evaluation results can be found in Appendix C.
7
Under review as a conference paper at ICLR 2022
256×256 CelebA-HQ dataset (second row) and 256×256 LSUN church dataset (third row).
,Female
,Eyewear
No eyewear、
,No Bald
Bald
Figure 8: Attribute-to-image synthesis results on the 64×64 LFW dataset. (Left: Image progression
conditioned on attribute (Interpolations on the attribute vectors). Right: Visualization of class centers
uφ for test subset.)
6.3	Attribute-to-Image Synthesis
Attribute-to-image synthesis Yan et al. (2016) re-
quires to generate object image from high-level
visual attributes (e.g. age, gender, lighting),
which are also continuous values. Similarly,
we map the attribute vectors to class center (i.e.
template face) by condition encoding network
and leverage the proposed ST-DDPM formula-
tion. By interpolating between the minimum and
maximum attribute value to modify the value
progressively, we can further explore the capa-
bilities of ST-DDPM for image synthesis in a
continuous condition space.
Hφ(cι)	λ = 0.1 λ = 0.2 λ = 0.3 λ = 0.4 λ = 0.5 λ = 0.6 λ = 0.7 λ = 0.8 λ = 0.9	%©)
Figure 9: Interpolations on the condition space for
the 64×64 LFW dataset.
Figure 8 presents the attribute-to-image synthesis results on the 64×64 LFW Huang et al. (2008)
dataset. As shown in Figure 8 (Left), samples generated by progressive condition are visually
consistent with attribute description, indicating that our method can effectively explore the continuous
condition space. Figure 8 (Right) visualizes the class centers predicted by the condition encoding
network and the uφ also shows the shape of template face, demonstrating the interpretability of our
method. Since we decouple the condition encoding network and denoising network, we can further
explore the condition space by interpretation of two different conditions, given by λ ∙ Uφ (c1 ) + (1 -
λ) ∙ Uφ (c2). Figure 9 shows the samples generated by interpolations on the condition space. The
interpretation smoothly introduces attributes from two different conditions, which can be extended to
multi-condition generation in the future.
6.4	Text-to-Image Synthesis
Text-to-image synthesis Reed et al. (2016); Xu et al. (2018); Zhang et al. (2018) aims to generate
realistic and text-consistent images according to the given natural language descriptions. We extend
the ST-DDPM to the more challenging cross-modal generation by mapping the encoded word
8
Under review as a conference paper at ICLR 2022
T = 700 (FID = 7.52)
T = 800(FID = 4.12)
T= 900 (FID = 3.10)	T = 950 (FID = 3.06)
Figure 10: Left: Speedup on the 32×32 CIFAR-10 dataset for different time steps. Right: Faster
diffusion denoising on the 32×32 CIFAR-10 dataset with different time steps.
sequences to class centers (details in the appendix B). Also, we obtain an Inception score of 4.52
and FID score of 18.18, demonstrating the effectiveness of our ST-DDPM method. More generated
examples and visualization results of class centers can be found in the Appendix E.
6.5	Faster Diffusion Denoising
Different from strided sampling Song et al. (2020a), since we decouple the denoising network and
condition encoding network and the diffusion process is modulated by the class center and datapoints
from the same class gather earlier, as shown in Figure 5 (Left), we can devise a way for faster
diffusion denoising that starts from earlier starting points XT = nτ + e, where e 〜N(0, I) (details
in the appendix B). By reducing the number of reverse steps, we enable faster sampling without
harming quality as much as possible.
Figure 10 presents the speedup and generated examples on the 32×32 CIFAR-10 dataset with
different time steps. With T = 700, our method can achieve a speedup of 1.43× and the sample
quality is slightly decreased. With T = 900, our method can achieve a speedup of 1.12× without
harming the sample quality. Also, our technique of earlier starting (ES) can be combined with strided
sampling (SS) and achieve a speedup of 11.1×, and detailed derivation and more results are given in
the Appendix D.
6.6	Score-based Methods for controllable generation
With bayes rule, Song et al. (2020b) imple-
ment controllable generation by introducing
gradient guidance, which is given by extra
time-dependent classifier or domain knowledge.
Computationally, an advantage of the proposed
approach over the guidance strategy in Song
et al. (2020b) is that no backpropagation through
a classifier or other estimation is needed. The
Figure 11: The normalized guidance for different
classes: Left: our learned class center and weight-
scheduling. Right: the gradient of classifier.
0	200 400 600	800 1000
Denoising step t
class centers, the guidance of ST-DDPM, are
predicted in a feed-forward manner and could
be cached. To further explore the relationship,
we separately visualize the normalized guidance
during the reverse process. As shown in Fig-
ure 11, the learned class centers with weight-scheduling, as a guidance without extra estimation, show
a similar trend with the gradient of classifier during the reverse process. Notice that the learned class
centers also have better class separation compared with the unstable gradient guidance.
7	Conclusion
In this paper, for the diffusion probabilistic models, we delve into the changes of data distribution
during the forward process of the Markov chain and explore the class clustering phenomenon. To
enable controllable generation, inspired by the class clustering phenomenon, we devise a novel
conditional diffusion probabilistic model ST-DDPM by explicitly modeling the class center in the
forward and reverse process, and make an elegant modification to the original formulation. The
learned class centers also reflect the common characteristics of datapoints from the same class (e.g.
template face). Extensive experiments verify the effectiveness and scalability of our method.
9
Under review as a conference paper at ICLR 2022
References
Guillaume Alain, Yoshua Bengio, Li Yao, Jason Yosinski, Eric Thibodeau-Laufer, Saizheng Zhang,
and Pascal Vincent. Gsns: generative stochastic networks. Information and Inference: A Journal
ofthe IMA, 5(2):210-249, 2016.
Navaneeth Bodla, Gang Hua, and Rama Chellappa. Semi-supervised fusedgan for conditional image
generation. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 669-683,
2018.
Florian Bordes, Sina Honari, and Pascal Vincent. Learning to generate samples from noise through
infusion training. arXiv preprint arXiv:1703.06975, 2017.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural
image synthesis. arXiv preprint arXiv:1809.11096, 2018.
Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis. arXiv preprint
arXiv:2105.05233, 2021.
Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint
arXiv:1406.2661, 2014.
Anirudh Goyal, Nan Rosemary Ke, Surya Ganguli, and Yoshua Bengio. Variational walkback:
Learning a transition operator as a stochastic recurrent net. arXiv preprint arXiv:1711.02282, 2017.
Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving flow-
based generative models with variational dequantization and architecture design. In International
Conference on Machine Learning, pp. 2722-2730. PMLR, 2019.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in
Neural Information Processing Systems, volume 33, pp. 6840-6851, 2020.
Gary B Huang, Marwan Mattar, Tamara Berg, and Eric Learned-Miller. Labeled faces in the wild:
A database forstudying face recognition in unconstrained environments. In Workshop on faces
in’Real-Life’Images: detection, alignment, and recognition, 2008.
Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, and Il-Chul Moon. Score matching
model for unbounded data score. arXiv preprint arXiv:2106.05527, 2021.
Diederik P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions.
arXiv preprint arXiv:1807.03039, 2018.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Dieterich Lawson, George Tucker, Bo Dai, and Rajesh Ranganath. Energy-inspired models: Learning
with sampler-induced distributions. arXiv preprint arXiv:1910.14265, 2019.
Daniel Levy, Matthew D Hoffman, and Jascha Sohl-Dickstein. Generalizing hamiltonian monte carlo
with neural networks. arXiv preprint arXiv:1711.09268, 2017.
Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang, Andrew Tao, and Bryan Catanzaro.
Image inpainting for irregular holes using partial convolutions. In Proceedings of the European
Conference on Computer Vision (ECCV), pp. 85-100, 2018.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of the IEEE international conference on computer vision, pp. 3730-3738, 2015.
Lars Maal0e, Marco Fraccaro, Valentin Lievin, and Ole Winther. Biva: A very deep hierarchy of
latent variables for generative modeling. arXiv preprint arXiv:1902.02102, 2019.
10
Under review as a conference paper at ICLR 2022
Takeru Miyato and Masanori Koyama. cgans with projection discriminator. arXiv preprint
arXiv:1802.05637, 2018.
Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Qureshi, and Mehran Ebrahimi. Edgeconnect: Structure
guided image inpainting using edge prediction. In Proceedings of the IEEE/CVF International
Conference on Computer Vision Workshops,pp. 0-0, 2019.
Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying Nian Wu. Learning non-convergent non-
persistent short-run mcmc toward energy-based model. arXiv preprint arXiv:1904.09770, 2019.
Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Ko-
ray Kavukcuoglu. Conditional image generation with pixelcnn decoders. arXiv preprint
arXiv:1606.05328, 2016.
Tingting Qiao, Jing Zhang, Duanqing Xu, and Dacheng Tao. Mirrorgan: Learning text-to-image
generation by redescription. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 1505-1514, 2019.
Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee.
Generative adversarial text to image synthesis. In International Conference on Machine Learning,
pp. 1060-1069. PMLR, 2016.
Yurui Ren, Xiaoming Yu, Ruonan Zhang, Thomas H Li, Shan Liu, and Ge Li. Structureflow: Image
inpainting via structure-aware appearance flow. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pp. 181-190, 2019.
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International
Conference on Machine Learning, pp. 1530-1538. PMLR, 2015.
Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to
accelerate training of deep neural networks. arXiv preprint arXiv:1602.07868, 2016.
Tim Salimans, Diederik Kingma, and Max Welling. Markov chain monte carlo and variational
inference: Bridging the gap. In International Conference on Machine Learning, pp. 1218-1226.
PMLR, 2015.
Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the
pixelcnn with discretized logistic mixture likelihood and other modifications. arXiv preprint
arXiv:1701.05517, 2017.
Jiaming Song, Shengjia Zhao, and Stefano Ermon. A-nice-mc: Adversarial training for mcmc. arXiv
preprint arXiv:1706.07561, 2017.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv
preprint arXiv:2010.02502, 2020a.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
arXiv preprint arXiv:1907.05600, 2019.
Yang Song and Stefano Ermon. Improved techniques for training score-based generative models.
arXiv preprint arXiv:2006.09011, 2020.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint
arXiv:2011.13456, 2020b.
Jaesung Tae, Hyeongju Kim, and Taesu Kim. Editts: Score-based editing for controllable text-to-
speech. arXiv preprint arXiv:2110.02584, 2021.
Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. Csdi: Conditional score-based
diffusion models for probabilistic time series imputation. arXiv preprint arXiv:2107.03502, 2021.
Aaron Van Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In
International Conference on Machine Learning, pp. 1747-1756. PMLR, 2016.
11
Under review as a conference paper at ICLR 2022
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.
Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd
birds-200-2011 dataset. 2011.
Min Wang, Congyan Lang, Liqian Liang, Gengyu Lyu, Songhe Feng, and Tao Wang. Attentive
generative adversarial network to bridge multi-domain gap for image synthesis. In 2020 IEEE
International Conference on Multimedia and ExPo (ICME),pp. 1-6. IEEE, 2020.
Daniel Watson, Jonathan Ho, Mohammad Norouzi, and William Chan. Learning to efficiently sample
from diffusion probabilistic models. arXiv PrePrint arXiv:2106.03802, 2021.
Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the EuroPean conference on
comPuter vision (ECCV), pp. 3-19, 2018.
Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He.
Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In
Proceedings of the IEEE conference on comPuter vision and Pattern recognition, pp. 1316-1324,
2018.
Xinchen Yan, Jimei Yang, Kihyuk Sohn, and Honglak Lee. Attribute2image: Conditional image
generation from visual attributes. In EuroPean Conference on ComPuter Vision, pp. 776-791.
Springer, 2016.
Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:
Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv
PrePrint arXiv:1506.03365, 2015.
Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. Generative image
inpainting with contextual attention. In Proceedings of the IEEE conference on comPuter vision
and Pattern recognition, pp. 5505-5514, 2018.
Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. Free-form image
inpainting with gated convolution. In Proceedings of the IEEE/CVF International Conference on
ComPuter Vision, pp. 4471-4480, 2019.
Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris N
Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial
networks. In Proceedings of the IEEE international conference on comPuter vision, pp. 5907-5915,
2017.
Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris N
Metaxas. Stackgan++: Realistic image synthesis with stacked generative adversarial networks.
IEEE transactions on Pattern analysis and machine intelligence, 41(8):1947-1962, 2018.
Yang Zhao, Chunyuan Li, Ping Yu, Jianfeng Gao, and Changyou Chen. Feature quantization improves
gan training. arXiv PrePrint arXiv:2004.02088, 2020.
Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10
million image database for scene recognition. IEEE transactions on Pattern analysis and machine
intelligence, 40(6):1452-1464, 2017.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference
on comPuter vision, pp. 2223-2232, 2017.
12
Under review as a conference paper at ICLR 2022
A Extended Derivations
A.1 Forward process general formula derivation
Xt = √Qxt-1 + Qt (1 — q1 )u + /∕βt∈t
=VQt [√Qt-IXt-2 + Qt-I (1 — Qt-I
______________________	1	1
)u + Pβt-1Q-1] + αt4(1 - at )u + p∕βtet
√αtαt-iXt-2 + √α αt-1(1 — ɑt-ι
)u + √ɑt pβt-i€t-1 + αt4 (1 — αt )u + Petq
√atat-iχt-2 + att (at at-1)(I - Qti-I) + αt4(I - αt )] U + /αtβt-1 + βt€
_____ IIII	1	/______________
√QtQt-1Xt-2 + Qj (α4 — Qj αj4-1 + 1 — Qt)U + \/3(1 — Qt-1) + 1 — QtW
_____ 1	11	________
√QtQt-1Xt-2 + Qt (1 — Qt Qt-I)U + 1J— — QtQt-I€
VQtXo + Qt4 (1 — Qt4 )u + ʌ/1 — Qt€ .
(14)
(15)
(16)
(17)
(18)
(19)
(20)
(21)
A.2 Forward process posterior distribution derivation
/ I	、	q(Xt∣Xt-1, Xo, u)q(Xt-1∣Xo, u)
q(Xt-1 ∣Xt, Xo, u)= ---------1T-------ʌ---------- (22)
q(Xt∣Xo, u)
__	1	1
q(Xt∣Xt-1, Xo, u) = q(Xt∣Xt-1, u) = N(Xt； √Q⅛Xt-1 + &t (1 - Q t )u,βtI)
1
q(Xt-1 ∣Xo, u) = N(Xt-15√at-1Xo + &之1(1
1
—a∕-1)u, (1 —西-1 )D
(24)
___	1	1
q(Xt∣Xo, u) = N(Xt； √^tXo + &屋1 —轨)u, (1 — &)I)
(23)
(25)
For the reason that all these multivariate Gaussian distributions have diagonal covariance matrix, We
can treat X and U as univariable, i.e., each element of X follows following derivation. By denoting
1	1	1	1
Ht (1 — Qt )u = mt and 且(1 — Ht )u = nt, we can derive q(Xt-1∣Xt, Xo, u) as follow:
/ l	、√⅛exp 卜
q(Xt-1∣Xt, Xo, u)=-----------------
(xt-√O7xt-1-mt)’
2βt
-]∙
h (Xt-I-√αt-1χo-nt-1)2 ]
exp [	2(1-αt-1)
exp卜
(xt-√0txo-nt)2
2(1-at)
(26)
1
=exp
βt(1-%-1)
I-Gt
—E
βt(1-αt-1)
(1-6t)
E =11 QtT [X2 + Q tX2-1 + m2 + 2√QfXt-Imt — 2√QfXt-IXt — 2mtXt]
1 — Ht
(27)
(28)
+ I βtH [X2-1 + Ht-1X2 + n2-1 + 2√H-Xont-1 — 2√Ht-1XoXt-1 — 2nt-1Xt-1]
t	(29)
彳；—(Q*I) [x2 + HtX2 + n2 + 2√HtXont — 2√HtXoXt — 2ntXt]
(30)
The coefficient of X2-1 as
form of [Xt-1 + (aXt + bXo + c)]2.
at-f-^α1-ɑt = 1, so we try to transform E to the
√‰∕
2
—
1i⅛l Qt + ɪ
1
1
]
13
Under review as a conference paper at ICLR 2022
We extract the coefficient of x2 as α2:
2 _ 1 - αt-1	βt (1 - αt-1)
1 - αt	(1 - at)2
=(1 - at-i)(1 - αt) - βt(1 - at-i)
一	(1- at)2
_ (1 - at-i)(1 - at - 1 + αt)
=	(1 - at)2
_ (I - αt-1)αt(1 - at-1)
=	(1 - at)2
_ αt(1 - αt-1)2
=(1 - at)2
√at(1 - αt-1)
a = ±----------------
1 - at
We extract the coefficient of x0 as b2:
b2 _ βtat-1	βt(1 - αt-1)αt
1	- α (1 - &t)2
=βtɑt-1(1 - αt) - βt(1 - αt-1)&t
(1 - at)2
_ βtɑt-1 [(1 - &t) - (1 - αt-1)αt]
=	(1 - at)2
_ βtat-i [1 — &t — αt + at]
=	(1 - at)2
_ β2ɑt-i
=(1 - at)2
b = ± βt√αt-1
1 - at
(31)
(32)
(33)
(34)
(35)
(36)
(37)
(38)
(39)
(40)
(41)
(42)
The coefficient of Xt-IXt is -2 VatII-^tT) = 2a, which means that a = - VatIl-^-1).
The coefficient of Xt-1X0 is -2 β√0t-1 = 2b, which means that b = - βt√Otj1.
The coefficient of XtX0 is 2 Vate-OIat-I) = 2 VatVO-I"!1^, which is equal to 2αb.
Note that mt = ∏t — √O7∏t-ι:
√O7∏t-ι 二	1 =αt(1	1 -at4 )u		 1 一√0tαt-1(1 —	1 at4-1)u	(43)
	1	1	111	1	
	=αt(1	-at4 )u	-αt4 αt4 ɑt-1(1 ∙	- at4-1)u	(44)
	1	1	1	1	1		
	=αt(1	-at4 )u	-at4 (αt4 - at4)	u	(45)
	1	1	1	1		
	=αt(1	-at4 -	αt4 + αt4 )u		(46)
	1 二 at(1	1 -at )u			(47)
	mt				(48)
14
Under review as a conference paper at ICLR 2022
The coefficient of xt-ι should be equal to 2c:
2 √O?
=2√αt-
1 - Qt-i
1 - Qt
1 - Qt-i
mt
-2F-nt-i
1 - Qt
(49)
=2√αt-
=2 √0?
=2√αt-
1 - Qt
1 - Qt-i
1 - Qt
1 - Qt-i
1 - Qt
1 - Qt-i
nt
nt
=2√αt-
1 - Qt
1 - Qt-i
1 - Qt
nt
nt
-EntT)- 2T-⅛nt-i
(50)
—
—
—
—
1 — Qt—1
2Qt τ-^τnt-i- 2
βt
∑———nt-i
1 - Qt
(51)
2
2
αt(l - αt-ι) + βt
1 - Qt
Qt - Qt ÷ 1 - Qt
1 - Qt
2nt-i
nt-1
nt-1
(52)
(53)
(54)
=2
1 - αt-i
1——Lnt - nt-i
1 - Qt
=2c
I-1 - Qt-i
C = √Qt-;———nt - nt-i
1 - Qt
The coefficient of Xt should be 2αc:
—
2T-Q3mt ÷ 2β(1 - Qt-I) nt
1 - Qt	t ÷	(1 - Qt)2	t
—
2∖ (nt-^nt-i)÷2 βf-Q^ nt
=2 βt(1 - Qt-I)	1 - Qt-i
_ (1 - Qt)2	1 - Qt .
=2
=2
=2
=2
=2
(55)
(56)
(57)
(58)
I_1 — Qt_1
nt ÷ 2Ft ————nt-i
1 - Qt
∙βt(1 - Qt-i) - (1 - Qt)(1 - Qt-i)
(1 - Qt)2
(1 - Qt-i)(βt - 1 ÷ Qt)
(1 - Qt)2
(1 - Qt-i)(-Qt ÷ Qt)
(1 - Qt)2
1 l-1 - Qt-i
nt ÷ 2√Qt————nt-i
1 - Qt
1 l-1 - Qt-i
nt ÷ 2√Qt ————nt-i
1 - Qt
1 l-1 - Qt-i
nt ÷ 2 ʌ/ɑt-:—∑一
1 - Qt
(1 - Qt-I)Qt(-1 ÷ Qt-I)
(1 - Qt)2
(1 - Qt-I)2
-Qt (1 - Qt)2 _
nt-i
I_1 — Qt_1
nt ÷ 2Ft ————nt-i
1 - Qt
1 l—1 - Qt-i
nt ÷ 2~λ———nt-i
1 - Qt
-2 [√ot 11~JqQ-1 nt - nt-i
√q7(1 - Qt-I)
1 - Qt
(59)
(60)
(61)
(62)
(63)
(64)
(65)
(66)
(67)
=2ca	(68)
The coefficient of x0 should be 2bc:
-2√0tβt((IL -")nt ÷ 2√QZΓɪ--nt-i
—
2
=2bc
βt √¾-τ
,1 - Qt
1-1 - Qt-i
√Qt~~.---∑-nt - nt-i
1 - Qt
(69)
(70)
(71)
15
Under review as a conference paper at ICLR 2022
The remaining items should be equal to c2:
1 - at-1
1 - at
1 - at-1
1 - at
m2
+ 产—n2-1 - βt(1- QG) n2
1 - Qt	(1 - Qt)2
/一	∖2 ,	βt	2	βt(1 - Qt-1) 2
-√atnt-1) +Ent-1 - (1 - Qt)2 nt
(72)
(73)
1 - at-1
1 - at
βt(1 - Qt-I)
(1- Qt)2
2 , 「	1 - Qt-I . βt
nt + 卜t LT + E
n2-1 - 2√at⅛tΓntnt-1
(74)
—
(1 - at)(1 - at-ι) - βt(1 - at-ι)
(1 - at)2
at — a + βt
-1 - at-
n2-1 - 2√at ⅛1 ntnt-1
(75)
(1 - at-ι)(1 - at - βt)
(1 - at)2
(1 - at-i)(1 - at -
at — Qt + 1 — at
1 - Qt
n2-1 - 2√at⅛1 ntnt-1
(76)
1 + at)
(1 - at)2
(1 - at-1)at(-at-1 + I)
2	2	/_1 — Qt_1
nt + nt-1 - 2y∕ot-———ntnt-1
1 - Qt
(77)
(1 - at)2
2	2	/_1 — at-1
nt + nt-1 - 2√at ———Lntnt-I
1 - Qt
(78)
(1 - Qt-I)2 2 ,	2	1 /-1 - Qt-I
at -γl-nt + nt-1 - 2√at -l——ntnt-1
(1 - αQt)2	t	t-1	1 - αQt
(79)
2
c2
1 - at-1
-;———∏t - ∏t-i
1 - 1y-t
(80)
(81)
Finally we have a
1-at
βt√⅛1, C =5⅛1 nt - nt-1 and
E = [xt-1 + (axt + bxo + c)]2
l /	√at(1 - Qt-1)
= xt-1 + (--------；---=------
1 - αQt
(82)
Xt
βt√Qt-I
1 - Qt
xo+√at ⅛⅞1 nt-nt-I)
(83)
—
,b
—
—
2
Now we have the forward process posterior distribution as follows:
1	-e	1	I"-(Xt-I - μ)2
q(XTXt,xo,U) = √i∏q1≡rexp [ 2!≡ I= Hexp [^σ^J
t	(84)
which is right a probability density function of some Gaussian distribution. Then we have:
σ
2
βt(1 - at-1)
(1 - at)
(85)
16
Under review as a conference paper at ICLR 2022
√at(1 - at-1)
μ =	;	-
1 - at
_ √at(1 - at-1)
xt +
Bt√αt-ι.
1 - at
l-1 - at-1	.
X0 - √at-———nt + nt-1
1 - at
(86)
xt +
βt√αt-1 Xt - nt - √1 - αtWt
1 - at
1 - at
√at(1 - at-1)
----;---二----χt +
1 - at
at(I - at-1) +
_(1 - at)√at	(1 - at)√at.
√0t
βt	Xt - nt - √1 - ate
1 - at
βt
√αt
at — at + βt	βt
-----二~~-—∙—Xt — —；—~~.	=
(I - at)√at	√at√1 - at
l-1 - at-1	.
-√at ————nt + nt-1
1 - at
-√αt-
1 - at-i
(87)
x	Bt M1 - at w
√	√at(1 - at)
βt + at — at
(I - at)√at.
1	βt	1
l——χt-.—— w-=---尸 nt + nt-1
√at	√atv 1 - at	aat
1	βt	1
-^Xt ——LK	_ W ——三(nt - √atnt-1)
a at	a atv 1 — at	a at
1	βt	1
XtW-mt
vat	ʌ/atʌ/1 — at vat
βt
√1 — at
- mt)
1 - at
βt
nt + nt-1
(88)
+ at(I - at-1)
_(I - at)√at	(1 - at)√at _
nt + nt-1
nt + nt-1
(89)
(90)
(91)
(92)
(93)
(94)
B Implementation Details
Follows the backbone of PixelCNN++ Salimans et al. (2017) and DDPM Ho et al. (2020), our
employ a U-Net architecture based on a Wide ResNet for noise prediction Wθ and replace the
weight normalization Salimans & Kingma (2016) with a group normalization Wu & He (2018) to
make the implementation simpler. We use four feature map resolutions for 32×32 models, and six
resolutions for 64×64 and 256×256 models. All models have two convolutional residual blocks per
resolution level and self-attention blocks at 16×16 resolution between convolutional blocks. The
time embedding is then specified by adding the Transformer Vaswani et al. (2017) sinusoidal position
embedding into each residual block.
To predict the class centers μφ, We stack convolutional residual blocks and upsample layers to map
feature vectors to three-channels map for CIFAR-10 and LFW dataset. For image inpainting on
CelebA-HQ, LSUN-church, Place2 datasets, We also employ a U-Net architecture for pixel-to-pixel
prediction. For text-to-image synthesis on CUB bird dataset, We stack convolutional residual blocks
and upsample layers With attention mechanism from the pre-trained Word embeddings to predict the
class center.
For image inpainting, We use the Irregular Mask Dataset collected by Liu et al. (2018), Which contains
55,116 irregular raW masks for training and 24,866 for testing. During training, for each image in
the batch, We first randomly sample a mask from 55,116 training masks, then perform some random
augmentations on the mask, finally We use it to mask the image and get our class center for training.
So the training masks are different all the time. The mask is irregular and may be 100During testing,
We use 12,000 test masks sampled and augmented from 24,866 raW testing masks. These 12,000
masks are categorized by hole size according to hole-to-image area ratios (0-20%, 20-40%, 40-60%).
FolloW the training setting of DDPM Ho et al. (2020), We set T = 1000 and set the diffusion rate
increasing linearly from β1 = 10-4 to βT = 0.02 for all experiments. The dropout rate is set to 0.1.
We use the Adam optimizer, and set the learning rate to 2 × 10-4 for 32×32 images and 2 × 10-5
for higher resolution Without any sWeeping. The batch size is set to 128 for CIFAR-10, 64 for LFW,
and 24 for high-resolution Places2, LSUN-church, CUB-bird and Celea-HQ datasets. We use EMA
on model parameters With a decay factor of 0.9999. We train the 32×32 models With four Nvidia
RTX 2080Ti GPUs and higher resolution models With four A6000 GPUs.
17
Under review as a conference paper at ICLR 2022
Table 2: CIFAR-10 results. NLL measured in bits/dim.
Model	FIDJ	NLL Test (Train)
Unconditional		
Gated PixelCNN	65.93	3.03 (2.90)
DDPM	3.17	≤3.75 (3.72)
Conditional		
cond. DDPM	3.82	≤3.75 (3.72)
ST-DDPM	3.05	≤3.74 (3.69)
Table 3: Evaluation results on the Place2 dataset.
Mask	FIDJ		
	0-20%	20-40%	40-60%
CAYuetal.(2018)	4.8586	18.4190	37.9432
EdgeConnect Nazeri et al. (2019)	3.0097	7.2635	19.003
StructureFlow Ren et al. (2019)	2.9420	7.0354	22.3803
cond. DDPM	2.0401	6.5726	17.2834
ST-DDPM	1.7498	6.1887	14.5111
For comparison, following Song et al. (2020b); Dhariwal & Nichol (2021), we introduce a time-
dependent classifier and compute the gradient for class-conditional sampling (grad. DDPM), given
by:
e = Q (xt,t) — √1 — at Vxtlog pφ (y|xt)	(95)
where φ is the parameters of the pretrained classifier, y is the class label. Also, we build the simple
conditional DDPM (cond. DDPM) by directly injecting a class embedding along with the timestep
embedding into the denoising network, given by:
= θ(xt, c, t)
(96)
where c is the given condition (e.g. class label).
C More Evaluation Results
We measure the negative log likelihoods (lossless codelengths) on CIFAR-10 dataset, given in Table 2.
Further, to evaluate the model on image inpainting, we follow prior works Yu et al. (2019); Liu et al.
(2018) by reporting the FID score on Place2 dataset. Table 3 shows the comparison results. The
ST-DDPM achieves competitive results comparable to prior GAN-based methods for the FID score,
showing the great potential of our method.
Table 4: Evaluation results of text-to-image generation.
Model	FIDJ	IS↑
StackGAN (Zhang et al. (2017))	51.89	3.70
StackGAN++ (Zhang et al. (2018))	15.30	3.82
FusedGAN (Bodla et al. (2018))	-	3.92
AttnGAN (Xu et al. (2018))	-	4.36
MirrorGAN (Qiao et al. (2019))	-	4.56
AGAN-CL (Wang et al. (2020))	-	4.97
ST-DDPM	18.18	4.52
18
Under review as a conference paper at ICLR 2022
D Faster Sampling with Strided Sampling
Given the Eq.(7), we can get a transfer relation as follows:
Xt = √0txo + nt + VZetq
As t increases, √0t get closer to zero, leading to an approximate transfer relation as follows:
Xt ≈ nt + VZetq
(97)
(98)
Therefore, we provide another thought for faster sampling that is to select a earlier starting timestep.
Also, inspired by DDIM Song et al. (2020a) with strided sampling, we adapt our formulations
into similar non-Markovian inference processes, which enables new and fast generative processes.
Specifically, we introduce a family of inference distribution indexed by σ ∈ RT≥0:
T
qσ (xi：T |xo) = qσ (XT ∣χo)[[ qσ (χt-ι |xt, χo)	(99)
t=2
where
qσ(XT∣X0) = N(XT； √0TX0 + nτ,(1 - <⅛)I)	(100)
and for all t > 1
qσ(Xt-i∣Xt, X0)= N(Xt-1； √ɑt-iX0+nt-1-ʌ/1 -国-i - σ2∙
Xt
-^^7X0 - nt
√τ-α
, σt2I). (101)
They are designed to ensure that qσ (Xt ∣xo) = N(Xt; √0tXo + nt, (1 - &)I) for all t.
Then we employ a trainable generative process pθ (X0:T) where each pθ(Xt-1 |Xt) models the reverse
conditional distribution qσ(Xt-ι∣Xt, xo). Specifically, for some xo 〜q(χo) and Et 〜N(0, I), we
can obtain xt using Eq. (7) and train a model θ to predict t from xt without x0. We can then predict
the denoised observation fθ(Xt, t) with ^t:
^t = S(Xt,t)-	，	(102)
√1 - αt
fθ (Xt,t) = Xt-nt-√f ∙皂.	(103)
Then we can define the generative process with a fixed prior pθ (XT) = N(XT; 0, I) and
PzJ (X Ix ) = ʃ N (x0 ； fθ (xIJ)ZD ift=1	(104)
pθ( t-1' t q qσ(xt-ι∣xt,fθ(Xt,t)) otherwise,	()
from which one can generate a sample Xt-1 from Xt via:
Xt-1 = √αt-i ∙
Xt - nt - √1 -出∙ ^t
√0t
+ nt-1 +
γ 1 - α^t-1 - σt ∙ Et + σtJ∙
(105)
We do these modifications to ensure the optimal solution for ST-DDIM is also the same as that for
ST-DDPM, which enables us to adopt other forward processes with smaller steps for accelerated
sampling without having to retrain the model. We take the same settings with DDIM for T and σ,
where T is a sub-sequences of [1, ∙∙∙ , T] and σ兀(η) = ηP(1 - &丁一)(1 - α" J1 - OaT^-. η is
a hyperparameter that we can directly control.
The evaluation results of FID on the CIFAR-10 dataset with strided sampling (SS) and earlier starting
points (ES) with different η are presented in Table 5. With earlier starting and strided sampling,our
method can achieve a speedup of 11.1× without significant decreasing in sample quality. Compared
with existing works Watson et al. (2021); Song et al. (2020a), our method can also achieve competitive
evaluation results with the same number of steps. Also, more generated examples with different η
and sampling steps T are presented in Figure 12.
19
Under review as a conference paper at ICLR 2022
Table 5: Evaluation results of FID on the CIFAR-10 dataset for ST-DDPM with strided sampling (SS)
and earlier starting (ES), and the complete sampling step T = 1000.
Step	SS					ES + SS	
	10	20	50	100	300	90	270
0.0	13.69	6.42	4.64	4.29	4.09	4.45	4.10
0.2	14.84	7.54	4.89	4.68	4.18	4.62	4.33
η 0.5	15.40	8.88	5.81	4.88	4.25	4.88	4.53
1.0	27.67	16.23	8.88	6.27	4.54	6.25	4.61
Figure 12: More generated samples with different sampling steps.
T = 90
T = 270
E	Additional S amples
Figure 13, 15, 16, 17, 18 present more generated examples separately on the CIFAR-10 Krizhevsky
et al. (2009), CelebA-HQ Liu et al. (2015), LSUN-church Yu et al. (2015), Place2 Zhou et al. (2017)
and LFW Huang et al. (2008) dataset. We also visualize the class center in the second column in
Figure 15, 16, 17.
Figure 14 shows the generated examples and class centers on the widely-used 256×256 CUB
bird Wah et al. (2011) dataset. Notice that the class centers are also bird-like templates, demonstrating
the effectiveness of our method.
20
Under review as a conference paper at ICLR 2022
(c) Bird
(d) Cat	(e) Deer	(f) Dog
(g) Frog	(h) Horse	(i) Ship
(j) Truck
Figure 13: More class-conditional samples on 32×32 CIFAR-10.
21
Under review as a conference paper at ICLR 2022
This bird has a brown
crown, a yellow belly,
and a pointed bill.
This bird is bluewith
black and has a long,
pointy beak.
This bird is completely
black with a short
blunt bill.
A bird with a large beak,
brown wings and a
white neck and belly.
IΛIdαα= S
A small sized bird that is The small bird has a white
mostly yellow that has a belly, breast and through
black chest marking along with a short beak.
A very large bird with
a yellow curved beak,
and gray wings.
A small bird with a grey
breast, short black bill
and brownish red crown.
Lnml PUnOJ9 IΛIdαα= S
A black crown and lighter The bird has a bright yellow
back and feathers this bird color on it's breast, belly,
has a long narrow beak. abdomen.
This bird has a yellow
belly with brown wings
and a brown crown.
This colorful bird has bright
blue wings and tail along
with a long, pointed beak.
WM PUn0」5
er! Jθrθu
Figure 14: Text-to-image synthesis results on the 256×256 CUB bird dataset.
22
Under review as a conference paper at ICLR 2022
Figure 15: More generated samples on 256×256 CelebA-HQ dataset.
23
Under review as a conference paper at ICLR 2022
Figure 16: More generated samples on 256×256 LSUN-church dataset.
24
Under review as a conference paper at ICLR 2022
Figure 17: More generated samples on 256×256 Place2 dataset.
25
Under review as a conference paper at ICLR 2022
μ 从 CI)	丸= 0.1 丸= 0.2 丸= 0.3 丸= 0.4 丸= 0.5 丸= 0.6 丸= 0.7 丸= 0.8 丸= 0,9	μφ(c2)
Figure 18: More interpolation samples on 64×64 LFW dataset.
26