Under review as a conference paper at ICLR 2022
Multi-scale Feature Learning Dynamics:
Insights for Double Descent
Anonymous authors
Paper under double-blind review
Ab stract
A key challenge in building theoretical foundations for deep learning is the
complex optimization dynamics of neural networks, resulting from the high-
dimensional interactions between the large number of network parameters. Such
non-trivial dynamics lead to intriguing model behaviors such as the phenomenon
of “double descent” of the generalization error. The more commonly studied as-
pect of this phenomenon corresponds to model-wise double descent where the
test error exhibits a second descent with increasing model complexity, beyond the
classical U-shaped error curve. In this work, we investigate the origins of the
less studied epoch-wise double descent in which the test error undergoes two non-
monotonous transitions, or descents as the training time increases. By leveraging
tools from statistical physics, we study a linear teacher-student setup exhibiting
epoch-wise double descent similar to that in deep neural networks. In this setting,
we derive closed-form analytical expressions for the evolution of generalization
error over training. We find that double descent can be attributed to distinct fea-
tures being learned at different scales: as fast-learning features overfit, slower-
learning features start to fit, resulting in a second descent in test error. We validate
our findings through numerical experiments where our theory accurately predicts
empirical findings and remains consistent with observations in deep neural net-
works.
1	Introduction
Classical wisdom in statistical learning theory predicts a trade-off between the generalization ability
of a machine learning model and its complexity, with highly complex models less likely to gener-
alize well (Friedman et al., 2001). If the number of parameters measures complexity, deep learning
models sometimes go against this prediction (Zhang et al., 2016): deep neural networks trained by
stochastic gradient descent exhibit a so-called double descent behavior (Belkin et al., 2019b) with
increasing model parameters. Specifically, with increasing complexity, the generalization error first
obeys the classical U-shaped curve consistent with statistical learning theory. However, a second
regime emerges as the number of parameters is further increased past a transition threshold where
generalization error drops again, hence the “double descent” or more accurately model-wise double
descent (Nakkiran et al., 2019).
Nakkiran et al. (2019) showed that the phenomenon of double descent is not limited to varying
model size but is also observed as a function of training time or epochs. In this case as well, the
so-called epoch-wise double descent is in apparent contradiction with the classical understanding
of over-fitting (Vapnik, 1998), where one expects that longer training of a sufficiently large model
beyond a certain threshold should result in over-fitting. This has important implications for prac-
titioners and raises questions about one of the most widely used regularization method in deep
learning (Goodfellow et al., 2016): early stopping. Indeed, while one might expect early stopping
to prevent over-fitting, it might in fact prevent models from being trained at their fullest potential.
Since the 1990s, there has been much interest in understanding the origins of non-trivial general-
ization behaviors of neural networks (Opper, 1995; Opper & Kinzel, 1996). The authors of Krogh
& Hertz (1992b) were among the first to provide theoretical explanations for (model-wise) double
descent in linear models. Summarily, at intermediate levels of complexity, where the model size is
equal to the number of training examples, the model is very sensitive to noise in training data and
hence, generalizes poorly. This sensitivity to noise reduces if the model complexity is either de-
1
UlIderreVieW as a COnferenCe PaPer at ICLR 2022
-SOfrOPiC AniSofroPiC
y :=Ty + 6 VfW
NoiSy feacher SfUdenf
MSE Generalization Error
MSE Generalization Error
o-ξ
IO2 1。3
τramg Hme
1。4
IO5 IO6
FigurerLefL The teache-∙S the data generating PrOCeSS that OPemteS on isotropic GaUSSian inputs
N∙ The StUdeiltiS trained on a dataset generated by the teachery U H(Hr虫尸一 Where 2 句TN
follow an anisotropic GaUSSiaIl distribution SUCh that the directions WithIarger/smaller VarianCe are
Ieamed faster/slower The COlldition IlUmber Of F determines how much faster SOme features are
Ieamed than the otherOlle Can think Of N as the latent factors Of Variation on WhiCh the teacher
operateWhiIe S Can be thought as the PiXeIS that the StUdentIeamS from，RighL The generalization
error as the training time proceed(py The CaSe Where OIlIy the fast—leammg feature 恰 slow—
Ieaming feature are trained，(bottomN The CaSe Where both features，FeatUreS that are Ieamed on
a faster timecale are responsible for the CIaSSiCaI uhaped generaHZation CurvWhiIe the SeCOnd
descent: Can bebured S rhe features rhare Ieameda SioWe 二 imecal
CreaSed OrinCreaSed∙ MOre recen=y=he double descent: PhenOmena has been also SrUdied for more
COmPleX models SUCh as rwo—pyer neural ne-works and random feaBre models (Ba er aΓ229; Mei
浮 MOnranarL 2019; D。ASCOIi et aΓ2020; GeraCe et a = 2020)∙
The mmori-y Of PreViOUS WOrk in Lhis direcHOnfOCUSeS On UnderStandingrhe ClsymPtotiC behavior
Of model PerfOrmanC尸 Le." Where training Hme EI ∞∙ In recenr years =here has Been an imeresr
in SrUdying the YIomasymPtotiC Bni 济 training Hme) PerfOrmanC 尸 SUgge⅛ng -har SeVeral intriguing
PrOPertieS Of neural ne-works Can be arabu 济 d toafferenr fea≡res being IeanIed arafferenr scale
AmOng rhe Hmired WOrk SrUdying rhe PaniCUIar epoch—wise double descend Nakkiran er ah (229)
introduces rhe notion Of effe dive mode - complexity and hypshesizes -har ir increases Wrh training
dme and hence UnifieS bo-h model—wise and epoch—wise double des cenLThrOUgh a COmbina一o∙n Of
Lheory and emp5:CalreSUlFHeCkeI 济 YihIaZ (202。) ALnd rha二he dynamics Of evolution Of Single
and -WO IayernerWOrkS Under gra&enr descend Can be perceived to be rhe SUPelPOSi 一o∙n OfrWO
bias/variance CUrVeS Wrh differed minima dmeFthUSleadingεnon—monsonic -esr error curves，
In this work" We build On Bo:S et aL (1993)； BOS(1998)； AdVani 浮 SaXe (2017)； Mei 浮 MOlnanari
(229) WhiCh anaIyZe modebwise double descen 二 hrough the IenS OfHnear modeFto PrObe rhe
OriginS Of epoch—wise double descenL PaniCUIarly"
♦ We introduce a linear LeaCherISrUdenr model who∙户 despi 济 i-s SimPHCi-y" exhibi-s SOme Of
intriguing PrOPeaes Of generaHZa-o∙n dynamics in deep neural nerworks∙ (section 2∙1)
♦ Inrhe Hmir Of high dimensions) we IeVerage rhe replica method developed in Sra-is-o∙al
PhySiCS to derive dosed—form expressions for rhe generaHZa-o∙n dynamics Of OUrreaCher—
SrUdenr se≡p" as a function Of training dme and regularization Streng一h∙ (SeC-o∙n 2.2)
♦ COnSiSrenr Wrh recenr ALndingf We ProVide an explanation for rhe exisrence Of epoch—wise
double descen 二hrough rhe IenS Of mum—scale fea≡re learning，(FigUre I)
♦ We PerfOrm simulation experimen-sεVaHdare OUr analytical PrediC 一o∙ns∙ We also conduce
experimenrs Wrh deep nerworky ShOWingrhar OUrreaCher—srudenr se≡p exhibi-s genera 丁
izaro∙n behavior WhiCh is qualitatively SilniIarε-har Of deep Be-Works ∙ CFigUre 2)
Under review as a conference paper at ICLR 2022
2	Analytical Results
Stochastic Gradient Descent (SGD) — the de facto optimization algorithm for neural networks —
exhibits complex dynamics arising from a large number of parameters (Kunin et al., 2020). How-
ever, it is possible to describe some aspects of the high-dimensional microscopic dynamics of neural
networks in terms of low-dimensional understandable macroscopic entities. In a series of seminal
papers by Gardner (Gardner, 1988; Gardner & Derrida, 1988; 1989), the replica method of statisti-
cal physics was adopted to derive expressions describing the generalization behavior of large linear
models trained using SGD. In this paper, we employ Gardner’s analysis to build upon an established
line of work studying linear and generalized linear models (Seung et al., 1992; Kabashima et al.,
2009; Krzakala et al., 2012). While most of previous work study the asymptotic (t → ∞) gener-
alization behavior, we adapt these methods to study transient learning dynamics of generalization
for finite training time. In the following, we first introduce a teacher-student model that exhibits
interesting characteristics of modern neural networks. We then adapt the replica method to study the
generalization performance as a function of training time and amount of regularization.
2.1	A Teacher-Student Setup
Teacher: We study a supervised linear regression problem in which the training labels y, are
generated by a noisy linear model (Figure 1),
y ：= y* + e, y* ：= ZTW,	Zi 〜N(O, √1d)),	(1)
where Z ∈ Rd is the teacher's input and y*,y ∈ R are the teacher's noiseless and noisy outputs,
respectively. W ∈ Rd represents the (fixed) weights of the teacher and e ∈ R is the noise. Both
W and e are drawn i.i.d. from Gaussian distributions with zero means and variances of 1 and σ2,
respectively.
Student: A student model is correspondingly chosen to be a similar shallow network with train-
able weights W ∈ Rd. The student model is trained on n training pairs {(xμ, yμ)}n=> with the
labels yμ being generated by the above teacher network, as,
y := XTW,	s.t. X := Ftz,	(2)
where the matrix F ∈ Rd×d is a predefined and fixed modulation matrix regulating the student's
access to the true input Z . One can think of Z as the latent factors of variation on which the teacher
operates, while X can be thought as the pixels that the student learns from.
Learning paradigm: To train our student network, we use stochastic gradient descent (SGD) on
the regularized mean squared loss, evaluated on the n training examples as,
1n	λ
LT = 2n X(yμ- yμ)2 + 2 l∣w I∣2	⑶
μ=1
where λ ∈ [O, ∞) is the regularization coefficient. Optimizing Eq. 3 with stochastic gradient descent
(SGD) yields the typical update rule,
Wt — Wt-ι - ηVv^LT + ξ,	(4)
in which t denotes the training step and η is the learning rate. Additionally, ξ 〜 N(0, σξ) models
the stochasticity noise of the optimization algorithm (Bottou et al., 1991).
Macroscopic variables: The quantity of interest in this work, is the expected generalization error
of the student, determined by averaging the student's error over all possible input-target pairs and
noise realizations, as,
Lg ：=2Ez[(y*-仍2]∙	(5)
As shown in Bos et al. (1993), if n,d → ∞ with a constant ratio d < ∞, Eq. 5 can be written as a
function of two macroscopic scalar variables R, Q ∈ R,
LG = 1(1 + Q - 2R),	(6)
3
Under review as a conference paper at ICLR 2022
where σ2 is the variance of the teacher’s output noise and,
11
R := dWTFW,	Q := dW^TFTFW,	(7)
See App. B.1 for the proof.
Both R and Q have clear interpretations; R is the dot-product between the teacher’s weights W
and the student's modulated weights FW, hence can be interpreted as the alignment between the
teacher and the student. Similarly, Q can be interpreted as the student’s modulated norm. The
negative sign of R in Eq. 6 suggests that the larger R is, the smaller the generalization error gets.
At the same time, Q appears with a positive sign suggesting the students with smaller (modulated)
norm generalize better.
As a remark, note that both R and Q are functions of W which itself is a function of training iteration
t and the regularization strength λ. Therefore, hereafter, we denote the above quantities as LG (t, λ),
R(t, λ), and Q(t, λ).
2.2	Main Results
In this Section, we present our main analytical results, with Section 2.3 containing a sketch of our
derivations. For brevity of the results, here, we only present the results for σ2 = λ = 0. See App. B
for the general case and the detailed proofs.
General matrix F. Let Z := [zμ]n=ι ∈ Rn×d and X := [χμ]n=ι ∈ Rn×d denote the input
matrices for the teacher and student such that X := ZF. For a general modulation matrix F, the
input covariance matrix has the following singular value decomposition (SVD),
XTX = FTZTZF = VΛVT,	(8)
in which the diagonal matrix Λ contains the eigenvalues of the student’s input covariance matrix.
Solving the dynamics of gradient descent as in Eq. 4, we arrive at the following exact analytical
expressions for R(t) and Q(t),
R(t) = dTr(D)	where, D :=(I - [I - ηA]t),	(9)
Q(t) = dTr(ATA) where, A := FVDVTF-1,	(10)
in which Tr(.) is the trace operator. See App. B.2 the proof.
Remark: The solution in Eqs. 9 and 10 are exact, however, they require the empirical computation of
the eigenvalues Λ. Below, we treat a special case of the dynamics that allow us to derive approximate
solutions that do not explicitly depend on Λ.
Special case: Fast and slow features. We now study a case where the modulation matrix F has
a specific structure described in Assumption 1.
Assumption 1. The modulation matrix, F, under a SVD, F := UΣVT has two sets of singular
values such that the first p singular values are equal to σ1 and the remaining d - p singular values
are equal to σ2. We let the condition number of F to be denoted by K := σ1 > 1.
By employing the replica method of statistical physics (Gardner, 1988; Gardner & Derrida, 1988),
we now derive approximate expressions for R(t) and Q(t). To begin with, we first define the fol-
lowing auxiliary variables,
αι := n, α2 := n
p d-p
d 1	- := d 1
p ησ12 t , 2	d - p ησ22 t ,
and also let,
2 入i	，、
ai = 1 +-------------------------/	, for i ∈ {1,2}.
∕~∙j	i	∕~∙j	∕~∙j
(1 — α — ʌi) + a∕(1 — α — λi)2 + 4-i
(11)
(12)
4
Under review as a conference paper at ICLR 2022
The Closed-from scalar expression for R(t) is then given by,
_ , ,	_	_	n	n	n
R(t) =	Ri	+	R2,	where,	Ri	:=——-,and,	R2	:=——-	(13)
aid	a2d
For Q(t), We accordingly define two more auxiliary variables,
bi = ~~~—,	Ci = 1 - 2Ri 一 口-----L for i ∈ {1, 2},	(14)
ai — ai	d a.
with which the closed-from scalar expression for Q(t) reads,
Q(t)= Qi + Q2, where,
Qi ：
bib2C2 + bici
1 - bib2
and,
Q2 ：
bi b2ci + b2C2
1 - bib2
(15)
By plugging Eqs. 13 and 15 into Eq. 6, one obtains a closed-form expression for LG (t) as a function
of the training time. See App. B.3 for the proof.
Remark: Eq. 11 indicates that the singular values of F,
are directly multiplied by t. That implies that the learn-
ing speed of each feature is scaled by the magnitude of
its corresponding singular value. As an illustration, the
figure on the right shows the evolution of Ri, R?, and
R = Ri + R2 for a case where P = d/2, σi = 1, and
σi = 0.01 implying a condition number of K = 100.
2.3	Sketch of derivations
In this Section, we sketch the key steps in the derivation of our main results. For the sake of sim-
plicity, here we only treat the case where σe = λ = 0. The general case with detailed proofs are
presented in App B.
Exact dynamics of SGD. Recall the gradient descent update rule in Eq. 4. For the linear model
defined in Eqs. 1-2, learning is governed by the following discrete-time dynamics,
WWtt = WWtt-i -索Wt-ιLT,
=Wt-i - η[ - XT(y - XWWtt-i)]∙
(16)
(17)
With the assumption that Wt=0 = 0, the dynamics admit the following exact closed-form solution,
Wt =(I - [I - ηXTX「)(XTX)-iXTy := WW(t).	(18)
With a SVD on XTX, Eqs. 9-10 can then be obtained by substituting Wt in Eqs. 7. As a remark,
note that one can recover the results of Advani & Saxe (2017) by setting F = I. In that case, the
eigenvalues of XTX follow a Marchenko-Pastur distribution (Marchenko & Pastur, 1967).
Induced probability density of SGD. It is well-known (Kuhn & Bos, 1993; Solla, 1995) that
probability distribution of weight configurations for network weights W trained via SGD on a loss
L(W), tend to the Gibbs distribution such that,
P(W) = ɪ e-βL(W),
Zβ
(19)
in which Ze is the partition function (R dW exp(-βL(W))) and β is called the inverse temper-
ature and is inversely proportional the stochastic noise of SGD, ξ, defined in Eq. 4. Intuitively,
for small β, the distribution of P(W) is almost uniform, while as β → ∞, P(W) becomes more
concentrated around the minimum of the training loss.
Itis important to highlight that Eq. 19 describes the equilibrium distribution of the student network’s
weights, i.e., at the end of training (t → ∞). However, we are interested in studying the trajectory
5
Under review as a conference paper at ICLR 2022
of student's weights during the course of training, i.e., for finite t. To that end, we derive the time-
dependent probability density over W,
P(W ,t) = -i-- e-βL(W ,t,	where,
'Zβ ,t
(20)
LT(W,t) : = 71 X(yμ - y"(t))2 + λl∣W此	(21)
2n	2
=；Xe" - xμT W(t))2 + λ∣W 此 0⅜) defined in Eq. 18) (22)
2n —	2
≈ LT(W) + 2 (λ + — )||W||2.
~ ʌ
Remark: LT (W,t) is a modified loss such that its min-
imum (equilibrium distribution) is achieved at the tth it-
ʌ
erate of gradient descent on L(W). The schematic dia-
gram on the right illustrates this equivalence, such that,
~	C	ʌ	ʌ
argminW LT(W,t) = Wt, where Wt is the defined in
Eq. 4.
The typical generalization error. To determine the typical generalization performance at time t,
one proceeds by first computing the free-energy of the system as,
f ：
-β EW,z
[ln Zβ,t].
(24)
Free-energy is a self-averaging property where its typical/most probable value coincides with its av-
erage over proper probability distributions Engel & Van den Broeck (2001). Therefore, to determine
the typical values of R and Q, we extremize the free-energy w.r.t. those variables.
Due to the logarithm inside the expectation, analytical computation of Eq. 24 is intractable. How-
ever, the replica method (Mezard et al., 1987) allows us to tackle this through the following identity,
Ew,z[lnZβ,t] = lim	.
r→0	r
(25)
Computation of the free-energy via replica method and its subsequent extremization w.r.t R and Q,
we arrive at Eqs. 13 and 15. See App. B.3 for more details.
To summarize, using the replica method, we are able to cast the high-dimensional dynamics of SGD
into simple scalar equations governing R and Q and, consequently, the generalization error LG .
While our analysis is limited to the specific teacher and student setup, this simple model already
exhibits dynamics qualitatively similar to those observed in more complex networks, as we now
illustrate.
3	Experimental Results
In this Section, we conduct numerical simulations to validate our analytical results and provide
clear insights on the macroscopic dynamics of generalization. We also conduct experiments on real-
world neural networks showing a close qualitative match between the generalization behavior of
neural networks and our teacher-student setup.
For real-world experiments, we train a ResNet18 (He et al., 2016) with large layer widths [64, 2 ×
64, 4 × 64, 8 × 64]. We follow the training setup of Nakkiran et al. (2019); Label noise with a
probability 0.15 randomly assign an incorrect label to training examples. Noise is sampled only
once before the training starts. We train using Adam (Kingma & Ba, 2014) with learning rate of
1e - 4 for 1K epochs. Real-world experiments are averaged over 50 random seeds. To ensure
reproducibility, we include the complete source code in a GitHub repository as well as an
anonymous Collab notebook.
6
Under review as a conference paper at ICLR 2022
0.50
0.45
0.40
0.35
0.30
0.25
0.20
J. əul= qucwjh
0.400.350.300.25
Figure 2: A qualitative comparison between a ResNet-18 and our analytical results. (a): Heat-
map of empirical generalization error (0-1 classification error) for the ResNet-18 trained on CIFAR-
10 with 15% label noise. X-axis denotes the inverse of weight-decay regularization strength and Y-
axis represents the training time. (c): Heat-map of the analytical generalization error (mean squared
error) for the linear teacher-student setup with K = 100, the condition number of the modulation
matrix. (b, d): Three slices of the heat-maps for large, intermediate, and small amounts of regu-
larization. Analysis: As predicted by Eqs. 13 and 15, κ = 100 implies that a subset of features
are learned 100 times faster that the rest. Intuitively, large amounts of regularization allow for the
fast-learning features to be learned by not to overfit. Intermediate levels of regularization result in a
classical U-shaped generalization curve but prevent slow features from learning. Small amounts of
regularization allow for both fast and slow features to be learned, leading to double descent.
Large λ
Intermediate λ
Small λ
3.1	Match between theory and real-world experiments
We conduct an experiment on the classification task of CIFAR-10 (Krizhevsky et al., 2009) with
varying amount of weight decay regularization strength λ. We monitor the generalization error (0-
1 test error) during the course of training and visualize a heat-map of the generalization error for
different λ's in Figure 2 (a).
We also conduct a similar experiment with the teacher-student setup presented in Section 2.1. We
visualize a heat-map of the generalization error which is the mean squared error (MSE) over test
distribution in Figure 2 (b). Particularly, we plot Eqs. 13 and 15 with a constant κ = 100. As a
remark, we note that a κ = 100 implies that a subset of features are learned 100 times faster than
other features.
It is observed that in both experiments, a model with intermediate levels of regularization displays
a typical overfitting behavior where the generalization error decreases first and then overfits. This is
consistent with Eq. 87 which indicates larger amounts of regularization prevent slow feature from
being learned as λ and the inverse oft are summed. In other words, learning of slow features requires
large weights, something that is penalized by the weight-decay. On the other hand, a model with
smaller amount of regularization exhibits the double descent generalization curve.
We also validate our derived analytical expressions by running numerical simulations which are
presented in Figure 4.
7
Under review as a conference paper at ICLR 2022
3.2	The Phase diagram
To further investigate the transition between the two phases of classical single descent and double
descent, we explore the phase diagram. Recall that with Eq. 6, one can fully characterize the evo-
lution of the generalization dynamics in terms of two scalar variables instead of the d-dimensional
parameter space. R and Q presented in Eq. 7 are macroscopic variables where R represents the
alignment between the teacher and the student and Q is the student’s (modulated) norm. Hence,
a better generalization performance is achieved with larger R and smaller Q.
R and Q are not free parameters and both depend on the training dynamics through Eqs. 13 and 15.
Nevertheless, it is instructive to visualize the generalization error for all pairs of (R, Q). In Figure
3, we visualize the RQ-plane for (R, Q) ∈ [0.0, 0.8] × Q ∈ [0.0, 1.6]. At the time of initialization,
(R, Q) = (0, 0) as the models are initialized at the origin. As training time proceeds, values of
R and Q follow the depicted trajectories. In Figure 3, different trajectories correspond to different
values of κ, the condition number of the modulation matrix F in Eq. 2. It is important to note that
the closer a trajectory is to the lower-right, the better the generalization error gets.
The yellow curve which corresponds to the case with large κ = 1e5 meaning that a subset of features
are extremely slower than the others that practically do not get learned. In that case, generalization
error exhibits traditional over-fitting due to over-training. On the phase diagram, the yellow trajec-
tory starts at (0, 0) and moves towards Point A which has the lowest generalization error of this
curve. Then as the training continues, Q increases and as t → ∞ the trajectory lands at Point B
which has the worse generalization error. The curves in orange, green and blue correspond to tra-
jectories with κ = 1e3, κ = 1e2, κ = 1e1, respectively. They follow the case of κ = 1e5 up to
the vicinity of Point B, but then the trajectories slowly incline towards another fixed point, Point C
signalling a second descent in the generalization error.
The phase diagram along with the corresponding generalization curves in Figure 2 illustrate that
features that are learned on a faster time-scale are responsible for the initial conventional U-shaped
generalization curve, while the second descent can be attributed to the features that are learned at a
slower time-scale.
ULIOU PujPInPOUI SLUəpms
Figure 3: Left: Phase diagram of the generalization error as a function of R(t) and Q(t) (Eqs. 13
and 15). The generalization error for all pairs of (R, Q) ∈ [0.0, 0.8] × [0.0, 1.6] is contour-plotted
in the background in shades of beige, with the best generalization performance being attained on
the lower right part of the plot. The trajectories describe the evolution of R(t) and Q(t) as training
proceeds. Each trajectory correspond to a different κ, the condition number of the modulation matrix
F in Eq. 2. κ describes the ratio of the rates at which two sets of features are learned. Right: The
corresponding generalization curves for different plotted over the training time axis. Analysis: The
trajectory with κ = 1e5 (bright yellow) starts at the origin and advances towards point A (a descent
in generalization error). Then by over-training, it converges to point B (an ascent in generalization
error). For the other trajectories with smaller κ, a first descent in generalization error occurs up to
the point A, then an ascent happens, but they no longer converge to point B . Instead, by further
training, these trajectories converge to point C implying a second descent.
8
Under review as a conference paper at ICLR 2022
4	Related Work and Discussion
Although the term double descent has been introduced rather recently (Belkin et al., 2019a), similar
behaviors had already been observed and studied in several decades-old works form a statistical
physics perspective (Krogh & Hertz,1992a; OPPer, 1995; OPPer & KinzeL 1996; Bos, 1998; Engel
& Van den Broeck, 2001). More recently, these behaviors have been investigated in the context of
modern machine learning, both from an emPirical (Nakkiran et al., 2019; Amari et al., 2020; Yang
et al., 2020) and theoretical (Belkin et al., 2019a; Geiger et al., 2019; Advani & Saxe, 2017; Mei &
Montanari, 2019; Gerace et al., 2020; d’Ascoli et al., 2020; Ba et al., 2019; d’Ascoli et al., 2021)
PersPectives.
Hastie et al. (2019); Advani et al. (2020); Belkin et al. (2020) use random matrix theory (RMT)
tools to characterize the asymPtotic generalization behavior of over-Parameterized linear and random
feature models. In an influential work, Mei & Montanari (2019) extend the same analysis to a
random feature model and theoretically derive the model-wise double descent curve for a model
with Tikhonov regularization. Jacot et al. (2020) also study double descent in ridge estimators and
show an equivalence to kernel ridge regression. Pennington & Worah (2019) used RMT to study the
curvature of single-hidden-layer neural network in an attemPt to understand the efficacy of first-order
oPtimization methods in training DNNs. In addition, Liang & Rakhlin (2020) take a similar aPProach
to investigate imPlicit regularization in high dimensional ridgeless regression with nonlinear kernels.
While most of the related work study the non-monotonicity of the generalization error as a function
of the model size or samPle size, Nakkiran et al. (2019) introduced the ePoch-wise double descent.
EPoch-wise double descent refers to the Phenomenon where the generalization error undergoes two
descents as the training time increases. There has been limited work on studying of ePoch-wise
double descent. Very recently, Heckel & Yilmaz (2020) and StePhenson & Lee (2021) have focused
on finding the roots of this Phenomenon.
Heckel & Yilmaz (2020) Provides upper bounds on the risk of single and two layer models in a
regression setting where the inPut data has distinct feature variances. Heckel & Yilmaz (2020)
demonstrate that a suPerPosition of two or more bias-variance tradeoff curves leads to ePoch-wise
double descent. The authors also show that different layers of the network are learned at different
ePochs. For that reason, ePoch-wise double descent can be eliminated by aPProPriate selection of
learning rates for individual network weights. Consistent with these findings, our work formalizes
this Phenomenon in terms of feature learning scales and Provides closed-form Predictions.
StePhenson & Lee (2021) arrives at similar conclusions. Authors in StePhenson & Lee (2021) take
a random matrix theory aPProach on a data model that exhibits ePoch-wise double descent. The
data model is constructed so that the noise is exPlicitly added only to the fast-learning features while
slow-learning features remain noise-free. Consequently, the fast-learning features are noisy and
hence show a U-shaPed generalization curve while slow-learning features are noiseless.
Our findings and those of Heckel & Yilmaz (2020) and StePhenson & Lee (2021) reinforce one
another with a common central finding that the ePoch-wise double descent results from different
features/layers being learned at different time-scale. However, we also highlight that both Heckel
& Yilmaz (2020) and StePhenson & Lee (2021) built uPon tool from random matrix theory and
study distinct data models from our teacher-student setuP. We study the same Phenomenon from a
different PersPective. By leveraging the rePlica method from statistical Physics, we characterized
the generalization behavior using a set of informative macroscoPic Parameters. While suPPorting
the notion that the interaction of different feature learning sPeeds causes ePoch-wise double descent,
our work Provides formal Predictions of the dynamics that unfold during training.
We believe our theoretical framework sets the stage for further understanding of generalization dy-
namics in neural networks beyond the double descent. A future direction to study is a case in which
the first descent is strong enough to bring down the training loss to very small values to the Point that
learning slower features is Practically imPossible or haPPens after a very large number of ePochs.
Power et al. (2021) rePorts an instance of such behavior called Grokking where the model abruPtly
learns to Perfectly generalize but long after the training loss has reached very small values.
Limitations. It should be noted that studying finer details of the dynamics would require a more
Precise model of the neural networks. Clearly, our ProPosed model is not a universal and unique
way to model the dynamics of the comPlex, over-Parameterized deeP neural networks.
9
Under review as a conference paper at ICLR 2022
Social Impact. The authors do not foresee a negative social impact specifically arising from this
rather theoretical work.
References
Madhu S Advani and Andrew M Saxe. High-dimensional dynamics of generalization error in neural
networks. arXiv preprint arXiv:1710.03667, 2017.
Madhu S Advani, Andrew M Saxe, and Haim Sompolinsky. High-dimensional dynamics of gener-
alization error in neural networks. Neural Networks,132:428-446, 2020.
Alnur Ali, J Zico Kolter, and Ryan J Tibshirani. A continuous-time view of early stopping for least
squares regression. In The 22nd International Conference on Artificial Intelligence and Statistics,
pp. 1370-1378. PMLR, 2019.
Alnur Ali, Edgar Dobriban, and Ryan Tibshirani. The implicit regularization of stochastic gradient
flow for least squares. In International Conference on Machine Learning, pp. 233-244. PMLR,
2020.
Shun-ichi Amari, Jimmy Ba, Roger Grosse, Xuechen Li, Atsushi Nitanda, Taiji Suzuki, Denny
Wu, and Ji Xu. When does preconditioning help or hurt generalization? arXiv preprint
arXiv:2006.10732, 2020.
Jimmy Ba, Murat Erdogdu, Taiji Suzuki, Denny Wu, and Tianzong Zhang. Generalization of two-
layer neural networks: An asymptotic viewpoint. In International conference on learning repre-
sentations, 2019.
Yu Bai and Jason D. Lee. Beyond linearization: On quadratic and higher-order approximation of
wide neural networks, 2020.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-
learning practice and the classical bias-variance trade-off. Proceedings of the National Academy
of Sciences, 116(32):15849-15854, 2019a.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-
learning practice and the classical bias-variance trade-off. Proceedings of the National Academy
of Sciences, 116(32):15849-15854, 2019b.
Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. SIAM
Journal on Mathematics of Data Science, 2(4):1167-1180, 2020.
Carl M Bender and Steven A Orszag. Advanced mathematical methods for scientists and engineers
I: Asymptotic methods and perturbation theory. Springer Science & Business Media, 2013.
S Bos, W KinzeL and M Opper. Generalization ability of perceptrons with continuous outputs.
Physical Review E, 47(2):1384, 1993.
Siegfried Bos. Statistical mechanics approach to early stopping and weight decay. Physical Review
E, 58(1):833, 1998.
Leon Bottou et al. Stochastic gradient learning in neural networks. Proceedings OfNeuro-Nimes, 91
(8):12, 1991.
Lin Chen, Yifei Min, Mikhail Belkin, and Amin Karbasi. Multiple descent: Design your own
generalization curve. arXiv preprint arXiv:2008.01036, 2020.
Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks
trained with the logistic loss. In Conference on Learning Theory, pp. 1305-1338. PMLR, 2020.
Stephane D,Ascoli, Maria Refinetti, Giulio Biroli, and Florent Krzakala. Double trouble in dou-
ble descent: Bias and variance(s) in the lazy regime. In Hal DaUme In and Aarti Singh
(eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of
Proceedings of Machine Learning Research, pp. 2280-2290. PMLR, 13-18 Jul 2020. URL
https://proceedings.mlr.press/v119/d-ascoli20a.html.
10
Under review as a conference paper at ICLR 2022
StePhane d'Ascoli, Levent Sagun, and Giulio Biroli. Triple descent and the two kinds of overfitting:
Where & why do they appear? arXiv preprint arXiv:2006.03509, 2020.
Stephane d'Ascoli, Marylou Gabrie, Levent Sagun, and Giulio Biroli. On the interplay between
data structure and loss function in classification problems. In Thirty-Fifth Conference on Neural
Information Processing Systems, 2021.
Stephane d'Ascoli, Maria Refinetti, Giulio Biroli, and Florent Krzakala. Double trouble in dou-
ble descent: Bias and variance (s) in the lazy regime. In International Conference on Machine
Learning,pp. 2280-2290. PMLR, 2020.
Andreas Engel and Christian Van den Broeck. Statistical mechanics of learning. Cambridge Uni-
versity Press, 2001.
Jerome Friedman, Trevor Hastie, Robert Tibshirani, et al. The elements of statistical learning,
volume 1. Springer series in statistics New York, 2001.
Elizabeth Gardner. The space of interactions in neural network models. Journal of physics A:
Mathematical and general, 21(1):257, 1988.
Elizabeth Gardner and Bernard Derrida. Optimal storage properties of neural network models.
Journal of Physics A: Mathematical and general, 21(1):271, 1988.
Elizabeth Gardner and Bernard Derrida. Three unfinished works on the optimal storage capacity of
networks. Journal of Physics A: Mathematical and General, 22(12):1983, 1989.
Mario Geiger, Stefano Spigler, Stephane d'Ascoli, Levent Sagun, Marco Baity-Jesi, Giulio Biroli,
and Matthieu Wyart. Jamming transition as a paradigm to understand the loss landscape of deep
neural networks. Physical Review E, 100(1):012115, 2019.
Stuart Geman, Elie Bienenstock, and Rene Doursat. Neural networks and the bias/variance dilemma.
Neural computation, 4(1):1-58, 1992.
Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc Mezard, and Lenka Zdeborova. Gener-
alisation error in learning with random features and the hidden manifold model. In International
Conference on Machine Learning, pp. 3452-3462. PMLR, 2020.
Sebastian Goldt, Galen Reeves, Marc Mezard, Florent Krzakala, and Lenka Zdeborova. The gaus-
sian equivalence of generative models for learning with two-layer neural networks. arXiv e-prints,
pp. arXiv-2006, 2020.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning. MIT press
Cambridge, 2016.
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-
dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Reinhard Heckel and Fatih Furkan Yilmaz. Early stopping in deep networks: Double descent and
how to eliminate it. arXiv preprint arXiv:2007.10099, 2020.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: convergence and gen-
eralization in neural networks. In Advances in neural information processing systems, pp. 8571-
8580, 2018.
Arthur Jacot, Berfin Simsek, Francesco Spadaro, Clement Hongler, and Franck Gabriel. Implicit
regularization of random feature models. In International Conference on Machine Learning, pp.
4631-4640. PMLR, 2020.
Yoshiyuki Kabashima, Tadashi Wadayama, and Toshiyuki Tanaka. A typical reconstruction limit for
compressed sensing based on lp-norm minimization. Journal of Statistical Mechanics: Theory
and Experiment, 2009(09):L09003, 2009.
11
Under review as a conference paper at ICLR 2022
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Anders Krogh and John A Hertz. Generalization in a linear perceptron in the presence of noise.
Journal of Physics A: Mathematical and General, 25(5):1135, 1992a.
Anders Krogh and John A Hertz. A simple weight decay can improve generalization. In Advances
in neural information processing systems, pp. 950-957, 1992b.
Florent Krzakala, Marc Mezard, Francois Sausset, YF Sun, and Lenka Zdeborova. Statistical-
physics-based reconstruction in compressed sensing. Physical Review X, 2(2):021005, 2012.
R Kuhn and S Bos. Statistical mechanics for neural networks with continuous-time dynamics.
Journal of Physics A: Mathematical and General, 26(4):831, 1993.
Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel LK Yamins, and Hidenori Tanaka.
Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics. arXiv
preprint arXiv:2012.04728, 2020.
Yann Le Cun, Ido Kanter, and Sara A Solla. Eigenvalues of covariance matrices: Application to
neural-network learning. Physical Review Letters, 66(18):2396, 1991.
Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel “ridgeless” regression can gen-
eralize. The Annals of Statistics, 48(3), Jun 2020. ISSN 0090-5364. doi: 10.1214/19-aos1849.
URL http://dx.doi.org/10.1214/19-AOS1849.
Vladimir Alexandrovich Marchenko and Leonid Andreevich Pastur. Distribution of eigenvalues for
some sets of random matrices. Matematicheskii Sbornik, 114(4):507-536, 1967.
Song Mei and Andrea Montanari. The generalization error of random features regression: precise
asymptotics and double descent curve. arXiv preprint arXiv:1908.05355, 2019.
Marc Mezard, Giorgio Parisi, and Miguel Virasoro. Spin glass theory and beyond: an introduction
to the Replica Method and its applications, volume 9. World Scientific Publishing Company,
1987.
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever.
Deep double descent: where bigger models and more data hurt. ICLR 2020, arXiv preprint
arXiv:1912.02292, 2019.
Brady Neal, Sarthak Mittal, Aristide Baratin, Vinayak Tantia, Matthew Scicluna, Simon Lacoste-
Julien, and Ioannis Mitliagkas. A modern take on the bias-variance tradeoff in neural networks.
arXiv preprint arXiv:1810.08591, 2018.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring general-
ization in deep learning. In Advances in Neural Information Processing Systems, pp. 5947-5956,
2017.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. Towards
understanding the role of over-parametrization in generalization of neural networks, 2018.
Manfred Opper. Statistical mechanics of learning: Generalization. The handbook of brain theory
and neural networks, pp. 922-925, 1995.
Manfred Opper and Wolfgang Kinzel. Statistical mechanics of generalization. In Models of neural
networks III, pp. 151-209. Springer, 1996.
Jeffrey Pennington and Pratik Worah. Nonlinear random matrix theory for deep learning. Journal
of Statistical Mechanics: Theory and Experiment, 2019(12):124005, 2019.
12
Under review as a conference paper at ICLR 2022
Mohammad Pezeshki, Sekou-Oumar Kaba, Yoshua Bengio, Aaron C. Courville, Doina Precup,
and Guillaume Lajoie. Gradient starvation: A learning proclivity in neural networks. CoRR,
abs/2011.09468, 2020. URL https://arxiv.org/abs/2011.09468.
Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Gener-
alization beyond overfitting on small algorithmic datasets. In ICLR MATH-AI Workshop, 2021.
Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua
Bengio, and Aaron Courville. On the spectral bias of neural networks. In International Confer-
ence on Machine Learning, pp. 53O1-53lO. PMLR, 2019.
Frederick Reif. Fundamentals of statistical and thermal physics. Waveland Press, 2009.
Hyunjune Sebastian Seung, Haim Sompolinsky, and Naftali Tishby. Statistical mechanics of learn-
ing from examples. Physical review A, 45(8):6056, 1992.
Sara A Solla. A bayesian approach to learning in neural networks. International Journal of Neural
Systems, 6:161-170, 1995.
Cory Stephenson and Tyler Lee. When and how epochwise double descent happens. arXiv preprint
arXiv:2108.12006, 2021.
Vladimir N. Vapnik. The nature of statistical learning theory. Wiley, New York, 1st edition, Septem-
ber 1998. ISBN 978-0-471-03003-4.
Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, and Yi Ma. Rethinking bias-variance trade-
off for generalization of neural networks. In International Conference on Machine Learning, pp.
10767-10777. PMLR, 2020.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Xiao Zhang and Dongrui Wu. Rethink the connections among generalization, memorization and
the spectral bias of dnns. CoRR, abs/2004.13954, 2020. URL https://arxiv.org/abs/
2004.13954.
A	Further Related Work and Discussion
If we consider plots where the generalization error on the y-axis is plotted against other quantities
on the x-axis, we find earlier works that have identified double descent behavior for quantities such
as the number of parameters, the dimensionality of the data, the number of training samples, or the
training time on the x-axis. In this paper, we studied epoch-wise double descent, i.e. we plot the
training time t, or the number of training epochs, on the x-axis. Literature displaying double descent
phenomena in generalization behavior w.r.t. other quantities do so in the limit of t → ∞.
From a random matrix theory perspective, Le Cun et al. (1991); Hastie et al. (2019); Advani et al.
(2020), and Belkin et al. (2020) are among works which have analytically studied the spectral density
of the Hessian matrix. According to their analyses, at intermediate levels of complexity, the presence
of small but non-zero eigenvalues in the Hessian matrix results in high generalization error as the
inverse of the Hessian is calculated for the pseudo-inverse solution.
Neyshabur et al. (2014) demonstrated that over-parameterized networks does not necessarily overfit
thus suggesting the need of a new form of measure of model complexity other than network size.
Subsequently, Neyshabur et al. (2018) suggest a novel complexity measure based on unit-wise ca-
pacities which correlates better with the behavior of test error with increasing network size. Chizat
& Bach (2020) study the global convergence and superior generalization behavior of infinitely wide
two-layer neural networks with logistic loss. Goldt et al. (2020) make use of the Gaussian Equiv-
alence Theorem to study the generalization performance of two-layer neural networks and kernel
models trained on data drawn from pre-trained generative models. Bai & Lee (2020) investigated
the gap between the empirical performance of over-parameterized networks and their NTK counter-
parts, first proposed by Jacot et al. (2018).
13
Under review as a conference paper at ICLR 2022
From the perspective of bias/variance trade-off, Geman et al. (1992), and more recently, Neal et al.
(2018) empirically observe that while bias is monotonically decreasing, variance could be decreasing
too or unimodal as the number of parameters increases, thus manifesting a double descent general-
ization curve. Hastie et al. (2019) analytically study the variance. More recently, Yang et al. (2020)
provides a new bias/variance decomposition of bias exhibiting double desc-nt in which the variance
follows a bell-shaped curve. However, the decrease in variance as the model size increases remains
unexplained. For high dimensional regression with random features, d’Ascoli et al. (2020) provides
an asymptotic expression for the bias/variance decomposition and identifies three sources of vari-
ance with non-monotonous behavior as the model size or dataset size varies. d’Ascoli et al. (2020)
also employs the analysis of random feature models and identifies two forms of overfitting which
leads to the so-called sample-wise triple descent. More recently, Chen et al. (2020) show that as a
result of the interaction between the data and the model, one may design generalization curves with
multiple descents.
From a statistical physics perspective, OPPer (1995); Bos et al. (1993); Bos (1998); OPPer & KinzeI
(1996) are among the first studies which theoretically observe sample-wise double-descent in a ridge
regression setuP where the solution is obtained by the Pseudo-inverse method. Most of these studies
emPloy the “Gardner analysis” (Gardner, 1988; Gardner & Derrida, 1988; 1989) for models where
the number of Parameters and the dimensionality of data are couPled and hence the observed form of
double descent is different from that observed in deeP neural networks. A beautiful extended review
of this line of work is Provided in Engel & Van den Broeck (2001). Among recent works, Gerace
et al. (2020) also aPPly the Gardner analysis but to a novel generalized data generating Process called
the hidden manifold model and derive the model-wise double-descent equations analytically.
Finally, recall that towards Providing an exPlanation for the ePoch-wise double descent, we argue
that the epoch-wise double descent can be attributed to different features being learned at different
time-scales, resulting in a non-monotonous generalization curve. In relation to the asPect of different
feature learning scales, Rahaman et al. (2019) had observed that DNNs have a tendency towards
learning simPle target functions first that can allow for good generalization behavior of various
data samPles. Pezeshki et al. (2020) also identify and Provide exPlanation for a feature learning
imbalance exhibited by over-Parameterized networks trained via gradient descent on cross-entroPy
loss, with the networks learning only a subset of the full feature sPectrum over training. More
recently though, Zhang & Wu (2020), show that certain DNNs models Prioritize learning high-
frequency comPonents first followed by the learning of slow but informative features, leading to the
second descent of the test error as observed in ePoch-wise double descent.
On the difference between model-wise and epoch-wise double descent curves. In accordance
with its name, model-wise double descent (in the test error) occurs due to an increase in model-
size (number of its Parameters), i.e., as the model transitions from an under-Parameterized to an
over-Parameterized regime. A variety of works have tried to understand this Phenomenon from
the lens of imPlicit regularization (Neyshabur et al., 2014) or defining novel comPlexity measures
(Neyshabur et al., 2017). On the other hand, ePoch-wise double descent (in the test error) as treated
in our work, is observed to occur for both over-Parameterized (Nakkiran et al., 2019) and under-
Parameterized (Heckel & Yilmaz, 2020) setuPs. As found in our work along with the latter reference,
this Phenomenon seems to be a result of different feature learning sPeeds rather than the extent of
model Parameterization. The overlaP of the test-error contributions from the different weights with
varying scales of learning henceforth leads to a non-monotonous evolution of the model test error as
exemPlified by ePoch-wise double descent.
We also note that the Peak in model-wise double descent is associated with the model’s caPacity to
Perfectly interPolate the data, we do not think an analogous notion exists for the case of ePoch-wise
double descent. Our understanding of the Peak in the latter is that it corresPonds to a training time
configuration whereby a subclass of features are already learnt (due to a larger associated signal-to-
noise-ratio) and are being overfitted uPon to fit the target. As training Proceeds further, the remaining
set of features are eventually learnt thus allowing for a lowering of the test error.
On the implicit regularization of SGD and ridge-regularized loss. The results Presented in Eqs.
20-23 have a core dePendence on the findings of Ali et al. (2019; 2020). These works first formalize
the connection between (continuous-time) GD or SGD-based training of an ordinary least squares
(OLS) setuP and that of ridge regression, Providing bounds on the test error under these algorithms
14
Under review as a conference paper at ICLR 2022
over training time t, in terms of a ridge setup with ridge parameter λ = 1/t. We utilize these results
in the sense that by evaluating the generalization error LG of our student-teacher setup with explicit
ridge regularization, we invoke the connection between the ridge coefficient λ and training time t as
described in these works, to obtain the behavior of (ridgeless) LG over training. This determination
of an expression of LG (t) is what allows us to study the epoch-wise DD phenomenon.
B	Technical Proofs
B.1 THE GENERALIZATION ERROR AS A FUNCTION OF R AND Q (EQ. 6)
Recall that the teacher is the data generator and is defined as,
y := y* + e, y* := ZTW, Zi 〜N(O, √1d)),	(26)
where Z ∈ Rd is the teacher's input and y*,y ∈ R are the teacher's noiseless and noisy outputs,
respectively. W ∈ Rd represents the (fixed) weights of the teacher and e ∈ R is the noise.
And student is defined as,
y := XTW,	s.t. X := Ftz,	(27)
where the matrix F ∈ Rd×d is a predefined and fixed modulation matrix regulating the student's
access to the true input Z .
The average generalization error of the student, determined by averaging the student's error over all
possible input-target pairs and noise realizations is given by,
LG :=2Eχ,w[(y*-仍2],	(28)
in which the variables (y*,仍 form a bi-variate Gaussian distribution with zero mean and a Covari-
ance of,
∑ _ ∣^<y*,y* >z < y*,y〉z] —「1 R
一[<y",y >z <y,y >z∖ — [r q∖ ,
in which,
R : = Ez [y*Ty] = Ez [WTZzTfW] = 1 WTFW, and,
d
Q : = Ez [yTy] = Ez [WTFTzztfW] = ；WTFTFW.
Eq. 29 implies a correlation between y* and y obstructing the calculation of the average in Eq. 28.
Following (Bos, 1998; Krogh & Hertz, 1992a), We define decoupled variables y* and y as follows,
(29)
(30)
(31)
y* =: y*,	and y=: Ry* + VZQ - R2y	(32)
The variables y* and y are independent Gaussian variables such that < y*,y >z = O. Therefore,
two expectations can be applied independently,
LG : = 2Eχ,w[(y*-仍2],	(33)
=2Ey.,y[(y* - (Ry* + PQ - Ry))2],	(34)
= 1(1 + Q - 2R).	(35)
Finally, we note that expectation w.r.t. a Gaussian variable x is defined as,
+∞ dx	x2
Eχ[f (x)] := J	√2∏ exp ( - -2)f(x)∙	(36)
15
Under review as a conference paper at ICLR 2022
B.2 The general case exact dynamics (Eqs. 9-10)
Recall that to train our student network, we use stochastic gradient descent (SGD) on the regularized
mean squared loss, evaluated on the n training examples as,
1n	λ
Lt ：=▽!>"-yμ)2 + 5IW112，	(37)
2n	2
μ=1
where 5 ∈ [0， ∞) is the regularization coefficient.
The minimum of the loss function, denoted by W, is achieved at,
VWLT = 0 = Vw[2IIy - xW∣∣2 + 2∣∣W∣∣2] =0	(38)
⇒ -XT(y — XW)+ XW = O	(39)
⇒ W := (XTX + XI)-1 Xty.	(40)
An exact gradient descent has the following dynamics,
Wt = Wt-1 - ηVwt-1 LT，	(41)
=Wt-1 - η[- XT(y - XWt-1)+ XWt-1 ]	(42)
=(1 - ηX)Wt-1 - ηXTXWt-1 + ηXTy，	(43)
=[(1 - ηX)i - ηXT X ]Wt-1 + ηX Ty	(44)
=[(1	-	ηX)I	- ηXT X ]Wt-1 + η(XT X	+	XI )(XT X + XI )-1 X t y,	(45)
=[(1	-	ηX)I	- ηXTX]Wt-1 + η(XTX	+	XI)W,	(46)
=[(1	-	ηX)I	- ηXTX]Wt-1 + (ηXTX	+	ηXI)W,	(47)
=[(1	-	ηX)I	- ηXTX]Wt-1 + (ηXTX	+	(ηX - 1)I)W + W,	(48)
which leads to,
Wt - W = [(1 - ηX)I - ηXTX](Wt-1 - W)，	(49)
=[(1 - ηX)I - ηXTX]t(Wɔ - W).	(50)
Assuming Wɔ = 0, we arrive at the following closed-form equation,
Wt =(I - [(1 - ηX)I - ηXTX]) W，	(51)
where W is defined in Eq 40.
Now back to definition of R in Eq. 84 and by substitution of Eq. 51, we have,
R(t) ： = 1WTFWt，	(52)
d
=；WTF(I - [(1 - ηX)I - ηXTX]，W，	(53)
=dWTF(I - [(1 - ηX)I - ηXTX]，(XTX + XI)-1XTy，	(54)
=；WTFV(I - [(1 - ηX)I - ηΛ]')(Λ + XI)-1VTXTy， (XTX = VΛVT)	(55)
=；WTFV(I - [(1 - ηX)I - ηΛ])(Λ + XI)-1(ΛVTFTW + Λ2e)，	(56)
=；WTFV(I - [(1 - ηX)I - ηΛ])(Λ + XI)-1(ΛVTFTW + Λ2e)，	(57)
=dτr[(I-[(1-ηX)I- ηΛ]t) ɪ+^].	(58)
16
Under review as a conference paper at ICLR 2022
Similarly for Q, let D := I - (1 - ηλ)I - ηΛt , then we have,
Q(t) :	1 =-WT T F T FW,	(59) d =d WT (I — [(i — nλ)I — nXT x]')f T F (I — [(1 — nλ)I — nXT X]')W,	(60) =1WT VDV T F T FVDV T W,	(61) d =1WTVDFTFDVTW,	(F := FV, X = UA1/2VT, "= UTe) (62) d =1(WTFTTV + A-1∕2g) . A、rDFTFD-a-(VTF-1W + A-1/2?),	(63) d	A+λI	A+λI =1(WT FTT + A-1/2g) A dFt Fd -a-(F-1W + A-1/2?),	(64) d	A+λI	A+λI 1	TA	A =dW F EDF FDEF W,	(65) + dAT2?七 DFTF D 高 A-1/2 ?,	(66) =dτrhAT Ai + σ2TrhBT Bi	(67)
where,	A := F (I — [(1 — nλ)I — nA]t) r+λIFτ and,	(68) B : = F (I — [(1 — nλ)I — nA]t) r+λIA-2.	(69)
For simplicity and brevity of the results, in the main text, we only present the results where σ2 = 0
and λ = 0. Substituting σ2 = λ = 0 leads to the following expressions,
R(t) = d Trh(I - I — nA〕'“	(70)
Q(t) = dTrhATAi	where, A := FV (I — [I — nA]')VTF-1,	(71)
and that concludes the proof.
B.3 The special case approximate dynamics (Eqs. 13 and 15)
Recall that the teacher and student are defined as,
y := y* + e,	y* := ZTW, y := XTW,	X := Ftz,	(72)
where e 〜N (0, σ2) is the label noise, F is the modulation matrix, and ∖∖z∖∖2 = ||W ||2 = 1.
The training and generalization losses are defined as,
LT :=2nX(y — y)2 + 2I∣W∣∣2,	LG :=2Ez[(y -y*)2]∙	(73)
According to Eq. 6, the generalization loss can be written in terms of two scalar variables R and Q,
LG	二 2(1 + Q — 2R), where,	(74)
R:	=Ez [y*T y] = Ez [WT ZzT fW] = ； WT FW, and,	(75)
Q:	=Ez [yT y] = Ez [WT FT zzt fW] = d WT FT FW.	(76)
17
Under review as a conference paper at ICLR 2022
Now, applying t steps of SGD on LT results in the following distribution for the student’s weights,
P(W ,t) = ɪ e-βLT(W R
Zβ,t
(77)
in which LT (W, t) is a modified loss where its equilibrium coincides with the tth iterate of SGD on
the original loss LT(W).
In Eq. 77 the scalar variable β depends on the noise of SGD and Zβ,t is the partition function which
is defined as,
Zβ,t
R∞ Qd=I d(Wi)δ(dWiFTFWi - Qo)P(Wi,t)
-R∞∞Q[1⅛Wi)δ(!WTFTFW-Q)-
(78)
in which, Qo can be perceived to be a target norm the student weights W are being constrained to
and d is the dimensionality of the data. It can be interpreted that the partition function Zβ,t counts
the students.
We are now interested in finding R and Q of the typical (most probable) students. Therefore, it
suffices to find the students that dominate the partition function (or more precisely the free-energy).
The free-energy is defined as,
f := - βdEW,z[ln Zβ,t],	(79)
where W and z are the teacher’s weight and input, respectively.
Due to the logarithm inside the expectation, analytical computation of Eq. 79 is intractable. How-
ever, the replica method (Mezard et al., 1987) allows US to tackle this through the following identity,
Ew,z [ln Zβ,t] = lim EW,z[Zβ,t] - 1.	(80)
r→o	r
The case where F = I. As a first step, we first study a case where F = I . In that case, as derived
in Bos (1998), Eq. 79 can be simplified to,
-βf
1 Q - R2
2 Qo - Q
+ 2 In(QO - Q) - 2d ln[1 + β(QO - Q)]-
nβ G - 2HR + Q
2d 1 + β(Qo - Q)
(81)
in which the scalar variables G and H are defined as,
H : = Ey* [y*Ty] = Ey* [y*T(y* + e)] = 1,	(82)
G : = Ey* [yTy] = Ey* [(y* + e)T(y* + e)] = 1 + σ2.	(83)
At this point, in order to find the most probable students, one can extremize the free-energy
f (R, Q,Qo) in Eq. 81. The solution to this extermination is derived in Bos et al. (1993) and
reads,
in which,
▽r/ = 0
▽Qf = 0
▽Q0f = 0
n1
da
n
d a2
n2-a
—n/d 1° d a
2λ
a = 1+---------------------------/	==
∕~∙j	i	∕~∙j	∕~∙j
1 — n/d — λ + y(1 — n/d — λ)2 + 4λ
a :=1 + β(Qo- Q),
and,
1
限=λ+ηt.
(84)
(85)
(86)
(87)
⇒
⇒
⇒
R
Q
1
1
18
Under review as a conference paper at ICLR 2022
The case where F follows Assumption 1.
Assumption. The modulation matrix, F, under a SVD, F := UΣVT has two sets of singular values
such that the first p singular values are equal to σ1 and the remaining d-p singular values are equal
to σ2. We let the condition number of F to be denoted by K := σ1 > 1.
Without loss of generality, we assume that U = V = I. Consequently, the (noiseless) teacher and
the student can be written as the composition of two sub-models as following,
y* = y1 + y = ZT WI + ZT W2,
y = yι + ^ = σιzT W1 + σ2ZT W2,
(teacher decomposition)
(student decomposition)
(88)
(89)
in which Z1 ∈ Rp and Z2 ∈ Rd-p.
Let yi denote the output of the ith component of the student. Also let y* and yi denote the noiseless
and noisy targets, respectively. Therefore, for the student components i ∈ 1, 2, we have,
yi = σ1zT W1,
y1* = Z1TW1,
yi = y* + ZTW2 - σ2zTW2 +e,
S----------------{z------}
y2-y2=e2(t)
T
y2 = σ2z2 W2,
y2* = Z2TW2,
y2 = y* + ZT Wi - σιzTW1 +e,
V---------------{z------}
y^-yι=∈ι (t)
in which e is the explicit noise, added to the teacher’s output while ej (t) is an implicit variable noise
which decreases as the component j = i learns to match yj and y§.
Accordingly, the variables Hi and Gi for each component i are re-defined as,
Hi = E[y*τ yi] = Ey “y*T y*] = d,
G1 = E[y1Ty1],
=E[(y* + y* - y2)T(y* + y* - y2)] + σ2,
=E[y*T y*] + E[y*T y*] + E[yT y2],
-2E[y2Ty2] + σ2,
=P + ― + Q* - 2R* + σ2,
dd
= 1 + Q2 - 2R2 + σ2,
H2 = E[y*T y2] = Eyl [y*T y2 ] = —dɪ,
G2 = E[y2Ty2],
=E[(y* + y* - yi)T(y* + y* - yi)] + σ2,
=E[y*T y*] + E[y*T y*] + E[yT yi],
-2E[y*Tyi] + σ2,
=一 + P + Qi - 2Ri + σ2,
dd
= 1 + Qi - 2Ri + σ2 ,
in which Ri and Qi are defined as,
Ri := Ez[y*Tyi] = ；WTσiWi, and, Qi := Ez[yf彷] = ；WTσ2Wi,
where σi denotes the singular values of the matrix F as defined in Assumption 1.
Rewriting Eqs. 84, 85, and 86 for each of the student’s components, we arrive at,
Ri
n1
d ai
R2
n1
d a2
Qi
n
Pa2i - n
1 + Q2 - 2R2 + σ2 -------
d ai
ai
一一	2λi
ai = 1 +	/	=
1 - P - λi + J(1 - P - λi)2 + 4λi
Q2
n
(d - P)ai2 - n
1 + Qi - 2Ri + σ2 -
n 2 - a2
d a2
a2 = 1 +
1 - n- - λ +
d-P
d1	1
12 :=:-----2 (λ + -7),
d - P σ22	ηt
^*-
2λ
n
d-p
λ)2 +4λ

19
Under review as a conference paper at ICLR 2022
where Q1 depends on Q2 and vice versa. However, with simple calculations, we can arrive at the
following standalone equation. Let,
αι = n, α2 =六，
p d-p
and also let,
αi	n 2 - ai
bi = -2	,	Ci = 1 - 2Ri -	for i ∈ {1, 2},
ai - αi	d	ai
with which the closed-from scalar expression for Q(t, λ) reads,
Q(t, λ) = Q1 + Q2, where,
b1b2c2 +b1c1
Qi := —l-7-r一, and,
1 - b1b2
b1b2c1 + b2c2
Q2 :=	1 - bib2
(90)
(91)
(92)
B.4 Replica Trick
In the following, we detail the mathematical arguments leading to the replica trick expression. For
some r → 0, we can write for any scalar x:
xr	exP(r ln x) = lim 1 + r ln x r→0
⇒ lim r ln x r→0	lim xr - 1 r→0
⇒ ln x	xr - 1	(93) 二 lim	 r→0	r
.∙. E[ln x]二	二 lim 叫X ]——-, E : averaging r→0	r
B.5 Computation of the free-energy
The self-averaged free energy (per unit weight) of our student network, is given by (Engel & Van den
Broeck, 2001),
-βf = d hhln Z iiz,w	(94)
Here, β = 1/T is the inverse temperature parameter corresponding to our statistical ensemble, d the
(teacher) student network width, and Z the partition function of the system defined as (n: number
of training examples).
As Gaussian variables (with n, d → ∞), in the partition function, to obtain,
hhZriiz,w = Y1 ∏ Z dμ(Wa)dyμd(y*)μe-βNET(ya，y*)
a=1 μ=1
× ((δ Q*μ -√dw τx*")δ: -√dwT 噌 ZW
rd
∏Π	dμ(W a)
a=1 μ=1
(95)
dyμdyμ dy*"" ee ET 3”)**“。*“^^
2π	2π
XNexP	*μWτx*μ -=或WTxμ)))
d	z,W
where in the last line above, we have expressed the inserted δ functions using their integral repre-
sentations. To make further progress, we introduce the auxiliary variables,
]Twa∆ij W *j = dRa,
ia
X Wai Γij Wbj = dQab
i ha,bi
(96)
(97)
20
Under review as a conference paper at ICLR 2022
via the respective δ functions, to arrive at,
hhZniiz,W = Y / dμ (Wa)
μ,a,b J
dyad^ dy*"dy*" e-βNET(ya,y* )**"犷“包"
2π
2π
×	P dQab	PdRa δ
E Wa∆i,jW*j - PRa I δ I E WarjWj- PQab
i,j,a	ijha,bi
× ( ∕exP (— Q0 X(yμ)
∖ ∖	μ,a
2- 2 X 或赏Qab - Xy*"yμRa - 2 X(y*μ)2
μ,ha,bi
μ,a
μ	IIW
(98)
Repeating the procedure of expressing the above δ functions using their integral representations, we
then get (α = n/d),
ʌ ʌ ʌ
n	dQ0 dQ0a dQabQab dRa Ra
hhZ i>χ,χ*，W = J ∏ √2∏丁-/diΓ石"exp
(iP X QoQ 0a + iP X QabQab
a<b
a
+ iP X RaRa) / Y dW exp (- 2 X QOaWarjWj
a	i,a	i,j,a
-i X QabWarjWj-i X Ra∆jWj)×
i,j,a<b	i,j,a
Y
μ,a
dyμdyμ Q e-βN ET 5心
2π √2∏
exp
-2 X(y*μ)2 + i X yμyμ
μ	μ,a
-1X a - Ra)(yμ)2 - 2 X yμyμ (Qab - RaRb) - i X『"赏Ra
a,μ	μ,ha,bi	μ,a
(99)
If we now, perform a singular value decomposition of the covariance matrix r as, r = UT SU =
VTV, where S: matrix of singular values of r, and we have expressed, V = S1/2U, then one can
proceed to write,
ʌ
ʌ
ʌ
n	1	dQ0 dQ0a dQabQab dRaRa	iP
hhZ iiχ,W = E J ∏ √2∏ 彳 ~∏∕d∕d~ 后"exp⅛ ∑ QOQ Oa
■"W
+ iP X QabQab + iP X RaRa) / Y * exp ( - X X QOa (WWa)2
a<b	a	i,a 2π	i,a
X
QabWaWW -
i,a<b
i X Ra Wa) ×/Y
i,j,a	μ,a
dyμdyμ dy*μ e-N ET 3)
2π	√2∏
eχp (-1 X(yμ)a + i X yμyμ -1X (ι - Ra) (yμ)a - i X y*“ 求 Ra
μ	μ,a	a,μ	μ,a
-1 X yμyμ (Qab - RaRb))
μ,ha,bi
(100)
having expressed, Wa = VWa, and identifying ∆ = S1/2U from our definitions. Now, since in
the above, the Waa integrals factorize in i, and similarly the yaμ, yμ and dy*μ factorize in μ, one can
proceed to write:
ʌ ʌ ʌ
/ / nn∖∖	1 d TT dQ0dQ0a dQabQab dRaRa	(Di^ i	C ^
hhZ iiχ,w = E J ∏-√2Π4Γ-∏∕dW~%"exp Pp匕∑QOQOa
+ i X QabQab + i X RaRa + GS(QOa,Qab, Ra) + αGE(Qab, Ra)i)
a<b	a
(101)
21
Under review as a conference paper at ICLR 2022
where,
GS (Q 0a, Q ab, R a)=ln / Y 吗 exp ( - ɪ X QoaWa VWa — X QabWaWb — i X RaW。)
a π	a	a<b	a
Ge(Qab, Ra) = InZ Y dy2∏yaKe-βNET(ya,y*) exp ( - 1(y*)2 + i Xyaya
aπ	a
-1 X O - Ra) Iy))- 2 X yy (Qab - RaRb) - ,y*μ Xy0Ra)
a	ha,bi	a
(102)
Now, in the limit d → ∞, Eq. 101 can be approximated using the saddle-point approach (Bender &
Orszag, 2013),
hhZ niiχ,W ≈ ext，Q 0 ,Q 0a ,Qab ,Qab ,Ra ,Ra exp (P h 2 X QOQOa + i X QabQab
a	a<b
+ i X RaRa + GS(Q0a,Qab, Ra) + αGE(Qab, Ra)])
a
(103)
where, extr corresponds to extremization of hhZniix,W over the respective order parameters. Per-
forming this extremization over QOa, Qab and Ra, then generates an expression of the form,
hhZniix,W = extrQo,Q,R exp n nN [ 5 ~k---+ + 5 In(QO - Q) - 5 ln[1 + β(QO - Q)]
2QO - Q 2	2

αβ 1 - 2R + Q 1
T 1+ β(Qo - Q)
(104)
where we have invoked replica symmetry in the form, Qab = Q and Ra = R, and that ET
(y* - y)2∕2. Plugging this back into Eq. ??, then finally yields,
βf = -extrQo,Q,R< - γ:--+ + - In(QO - Q) - 5 ln[1+ B(QO - Q)]
2QO - Q 2	2

αβ 1 - 2R + Q 1
T 1+ B(Qo - Q) ∫
(105)
The remaining pair of order parameters generate the following set of transcendental equations on
extremization (Bos, 1998):
R = α
a
Q =	(1- j α
a2 - α	a
1
(106)
QO = Q + β(a-i)
where, a= max[1, α] for T → 0.
Now, the above determined values of R, Q and QO can be perceived as the maximally likely values of
R, Q and QO of our teacher-student setup, for an inverse temperature β parameterizing the system.
C Extended experiments
Figure 4 presents the analytical generalization dynamics for two values of κ and provides compar-
ison between the theory and simulation results of the same model. We observe that the theory and
22
Under review as a conference paper at ICLR 2022
-64 2 Q-8B
LLLL0n
JOJJO UOlγez=BJOUOHss
lθ-ɪ	IO0	IO1	IO2
Training time
Figure 4: The teacher-student set-up in Sec. equation 2.1. We compare the analytical solutions to
simulations performed on our teacher-student setup with d = 100, P = 50, n = 150 and We plot the
error bars over 100 random seeds. The solutions and the simulations match closely and we observe
double descent over the generalization error.
simulations accurately match. Further experiments are provided in the following anonymous Colab
notebook.
Before diving into the theory, we invite the reader to recall a simple equation from thermodynam-
ics. Consider an ideal gas in a container with its large number of molecules moving around, col-
liding with each other, all while obeying Newton,s laws. While the exact dynamics of each of
such molecules is intractable, the system,s macroscopic behavior can be characterized in terms of a
handful of scalar quantities, namely, the pressure P, the volume V, and the temperature T. By av-
eraging over suitable probability measures and applying the principle of free-energy minimization,
one arrives at a remarkably simple relationship between these three macroscopic variables, i.e., the
well-known PV = nRT (n: number of moles of gas, R: gas constant) (Reif, 2009).
23