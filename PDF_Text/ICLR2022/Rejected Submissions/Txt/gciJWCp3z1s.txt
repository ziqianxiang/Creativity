Under review as a conference paper at ICLR 2022
On the Convergence of Projected Alternating
Maximization for Equitable and Optimal
Transport
Anonymous authors
Paper under double-blind review
Ab stract
This paper studies the equitable and optimal transport (EOT) problem, which has
many applications such as fair division problems and optimal transport with mul-
tiple agents etc. In the discrete distributions case, the EOT problem can be for-
mulated as a linear program (LP). Since this LP is prohibitively large for general
LP solvers, Scetbon et al. Scetbon et al. (2021) suggests to perturb the problem
by adding an entropy regularization. They proposed a projected alternating max-
imization algorithm (PAM) to solve the dual of the entropy regularized EOT. In
this paper, we provide the first convergence analysis of PAM. A novel rounding
procedure is proposed to help construct the primal solution for the original EOT
problem. We also propose a variant of PAM by incorporating the extrapolation
technique that can numerically improve the performance of PAM. Results in this
paper may shed lights on block coordinate (gradient) descent methods for general
optimization problems.
1 Introduction
Optimal transport (OT) is a classical problem that recently finds many emerging applications in
machine learning and artificial intelligence, including generative models Arjovsky et al. (2017), rep-
resentation learning Ozair et al. (2019), reinforcement learning Bellemare et al. (2017) and word
embeddings Alvarez-Melis et al. (2019) etc. More recently, Scetbon et al. Scetbon et al. (2021) pro-
posed an equitable and optimal transport (EOT) problem that targets to fairly distribute the workload
of OT when there are multiple agents. In this problem formulation, there are multiple agents work-
ing together to move mass from measures μ to V and each agent has its unique cost function. A very
important issue that needs to be considered here is the fairness, which aims at finding transportation
plans such that the workloads among all the agents are equal to each other. This can be achieved
by minimizing the largest transportation cost among all agents, which leads to a convex-concave
saddle point problem. The EOT problem has wide applications in economics and machine learning,
such as fair division or the cake-cutting problem Moulin (2003); Brandt et al. (2016), multi-type re-
source allocation Mackin & Xia (2015), internet minimal transportation time and sequential optimal
transport Scetbon et al. (2021).
We now describe the EOT problem formally. Given two discrete probability measures μn =
pn=ι aiδχi and Vn = PZi bδyi, the EOT studies the problem of transporting mass from μ to
ν by N agents. Here, {x1, x2, ..., xn} ⊂ Rd and {y1, y2, ..., yn} ⊂ Rd are the support points
of each measure and a = [a1, a2, ..., an]> ∈ ∆n, b = [b1, b2, ..., bn]> ∈ ∆n are corresponding
weights for each measure, where ∆n denotes the probability simplex in Rn . Moreover, through-
out this paper, we assume vector b > 0. For each agent k, we denote its unique cost function as
ck(x, y), k ∈ [N] = {1, . . . , N} and its cost matrix as Ck, where Cik,j = ck(xi, yj). Moreover, we
define the following coupling decomposition set
πikj ≥ 0, ∀i, j ∈ [n]	,
1
Under review as a conference paper at ICLR 2022
where r(π) = π1, c(π) = π>1 are the row sum and column sum of matrix π respectively. Mathe-
matically, the EOT problem can be formulated as
min max
π∈ΠaN,b 1≤k≤N
hπk, Cki.
(1)
When N = 1, (1) reduces to the standard OT problem. Note that (1) minimizes the point-wise
maximum of a finite collection of functions. It is easy to see that (1) is equivalent to the following
constrained problem:
N
min max '(π, λ) :=	λkhπk, Ck).
π∈ΠN λ∈∆N
π a,b +	k=1
(2)
The following proposition shows an important property of EOT: at the optimum of the minimax
EOT formulation (2), the transportation costs of the agents are equal to each other.
Proposition 1 (Scetbon et al., 2021, Proposition 1) Assume that all cost matrices Ck, k ∈ [N] have
the same sign. Let π* ∈ ΠNNb be the optimal solution of (2). It holds that
h(π*)i,Cii = h(π*)j,Cji, ∀i,j ∈ [N].	(3)
Note that Proposition 1 requires all cost matrices to have the same sign. When the cost matrices are
all non-negative, (2) solves the transportation problem with multiple agents. When the cost matrices
are all non-positive, the cost matrices are interpreted as the utility functions and (2) solves the fair
division problem Moulin (2003).
The discrete OT is a linear programming (LP) problem (in fact, an assignment problem) with a
complexity of O(n3 log n) Tarjan (1997). Due to this cubic dependence on the dimension n, it is
challenging to solve large-scale OT in practice. A widely adopted compromise is to add an entropy
regularizer to the OT problem Cuturi (2013). The resulting problem is strongly convex and smooth,
and its dual problem can be efficiently solved by the celebrated Sinkhorn’s algorithm Sinkhorn &
Knopp (1967); Cuturi (2013). This strategy is now widely used in the OT community due to its
computational advantages as well as improved sample complexity Genevay et al. (2019). Similar
ideas were also used for computing the Wasserstein barycenter Benamou et al. (2015), projection
robust Wasserstein distance Paty & Cuturi (2019); Lin et al. (2020); Huang et al. (2021a), projection
robust Wasserstein barycenter Huang et al. (2021b). Motivated by these previous works, Scetbon
et al. Scetbon et al. (2021) proposed to add an entropy regularizer to (2), and designed a projected
alternating maximization algorithm (PAM) to solve its dual problem. However, the convergence
of PAM has not been studied. Scetbon et al. Scetbon et al. (2021) also proposed an accelerated
projected gradient ascent algorithm (APGA) for solving a different form of the dual problem of the
entropy regularized EOT. Since the objective function of this new dual form has Lipschitz continuous
gradient, APGA is essentially the Nesterov’s accelerated gradient method and thus its convergence
rate is known. However, numerical experiments conducted in Scetbon et al. (2021) indicate that
APGA performs worse than PAM. We will discuss the reasons in details later.
Our Contributions. There are mainly three issues with the PAM and APGA algorithms in Scetbon
et al. (2021), and we will address all of them in this paper. Our results may shed lights on designing
new block coordinate descent algorithms. Our main contributions are given below.
(i)	The PAM algorithm in Scetbon et al. (2021) only returns the dual variables. How to find the
primal solution of (2), i.e., the optimal transport plans π, was not discussed in Scetbon et al. (2021).
In this paper, we propose a novel rounding procedure to find the primal solution. Our rounding
procedure is different from the one widely used in the literature Altschuler et al. (2017).
(ii)	We provide the first convergence analysis of the PAM algorithm, and analyze its iteration com-
plexity for finding an -optimal solution to the EOT problem (2). In particular, we show that it takes
at most O(N n2-2) arithmetic operations to find an -optimal solution to (2). This matches the rate
of the Sinkhorn’s algorithm for computing the Wasserstein distance Dvurechensky et al. (2018).
(iii)	We propose a variant of PAM that incorporates the extrapolation technique as used in Nesterov’s
accelerated gradient method. We name this variant as PAM with Extrapolation (PAME). The iter-
ation complexity of PAME is also analyzed. Though we are not able to prove a better complexity
over PAM at this moment, we find that PAME performs much better than PAM numerically.
2
Under review as a conference paper at ICLR 2022
Notation. For vectors a and b with the same dimension, a./b denotes their entry-wise division.
We denote c∞ := maxk kCk k∞ . Throughout this paper, we assume vector b > 0, and we denote
ι := minj log(bj). We use 1n to denote the n-dimensional vector whose entries are all equal to
one. We use IX (x) to denote the indicator function of set X, i.e., IX (x) = 0 if x ∈ X, and
IX (x) = ∞ otherwise. We denote ct = c(PkN=1 πk (ft+1, gt, λt)). For integer N > 0, we denote
[N] := {1, . . . ,N}. We also denote π(f, g, λ) = [πk(f,g,λ)]k∈[N].
2 Projected Alternating Maximization Algorithm
The PAM algorithm proposed in Scetbon et al. (2021) aims to solve the entropy regularized EOT
problem, which is given by
min max
π∈ΠaN,b λ∈∆+N
N
'η(∏,λ) := XPk (∏k,λ)
k=1
(4)
where η > 0 is a regularization parameter, pηk (πk, λ) := λk hπk, Cki - ηH(πk), and the entropy
function H is defined as H(π) = - Pi,j πi,j (log πi,j - 1). The entropy regularization was first
introduced into the OT problem by Cuturi (2013) and is now widely used in the OT community. By
adding an entropy regularizer, the primal problem becomes strongly convex and the dual problem
is unconstrained and is suitable for alternating maximization. This leads to the Sinkhorn’s algo-
rithm which has low per-iteration complexity and thus is scalable. The PAM algorithm proposed by
Scetbon et al. (2021) used the same idea for the EOT problem. Note that (4) is a strongly-convex-
concave minimax problem whose constraint sets are convex and bounded, and thus the Sion’s mini-
max theorem Sion (1958) guarantees that
min max
π∈ΠaN,b λ∈∆+N
'η (π,λ) = max min 'η (π,λ).
λ∈∆+N π∈ΠaN,b
(5)
NoW We consider the dual problem of minπ∈ΠN
π∈ a,b
Pk,i,j πik,j = 1 and consider the dual of
'η(π, λ). First, We add a redundant constraint
min
π∈ΠaN,b,Pk,i,j πik,j
Jη (∏,λ).
(6)
The reason for adding this redundant constraint is to guarantee that the dual objective function is
Lipschitz smooth. It is easy to verify that the dual problem of (6) is given by
N
max min	λkhπk, Cki-ηH(π)+f
f,g Pk,i,j πik,j =1, k=1
π∈(Rn+×n)N
k
π
where f and g are the dual variables and H(π) = Pk H(πk). It is noted that problem (7) admits
the following solution:
“k(f,g,λ) = PkZKfgλλ)kι,	∀k ∈ [N],
(8)
where
ζk (f, g, λ) = exp
+ InT -λkCk∖ , Vk ∈ [N].
η
By plugging (8) into (7), We obtain the folloWing dual problem of (6):
(9)
f∈Rmn,agx∈Rnhf,ai+hg,bi-ηlog
kζk(f, g, λ)k1 -η.
(10)
Plugging (10) into (5), we know that the entropy regularized EOT problem (4) is equavalent to a
pure maximization problem:
max	F(f, g, λ) := hf, ai + hg, bi - ηlog
f∈Rn, g∈Rn, λ∈∆N
kζk(f, g, λ)k1 -η. (11)
3
Under review as a conference paper at ICLR 2022
Function F (f, g, λ) is a smooth concave function with three block variables (f, g, λ). We use
(f *,g*,λ*) to denote an optimal solution of (11), and We denote F * = F (f *,g*,λ*). The PAM
algorithm proposed in Scetbon et al. (2021) is essentially a block coordinate descent (BCD) algo-
rithm for solving (11). More specifically, the PAM updates the three block variables by the folloWing
scheme:
ft+1 ∈ argmax F (f, gt, λt),	(12a)
f
gt+1 ∈ argmaxF(ft+1, g, λt),	(12b)
g
λt+1 := Proj∆N (λt + TVλF(ft+1, gt+1,λt)) .	(12c)
Each iteration of PAM consists of tWo exact maximization steps folloWed by one projected gradi-
ent step. Importantly, the tWo exact maximization problems (12a)-(12b) have numerous optimal
solutions, and We choose to use the folloWing ones:
ft+1 = ft + ηlog
a
r (PN=IZk(ft,gt,λt))
gt+1 = gt + ηlog
(13)
(14)
Furthermore, the optimiality conditions of (12a)-(12b) imply that
r (PN=ι Z "ft+1®t, »))	C (PN=IZ k (f t+1, gt+1,λt))
Pk kZk(ft+1,gt,λt)kι =0, b- Pk kZk(ft+1,gt+1,λt)kι
(15)
HoWever, We need to point out that the PAM (12) only returns the dual variables (ft, gt, λt). One
can compute the primal variable π using (8), but it is not necessarily a feasible solution. That is,
π computed from (8) does not satisfy π ∈ ΠaN,b. HoW to obtain an optimal primal solution from
the dual variables Was not discussed in Scetbon et al. (2021). For the OT problem, i.e., N = 1, a
rounding procedure for returning a feasible primal solution has been proposed in Altschuler et al.
(2017). HoWever, this rounding procedure cannot be applied to the EOT problem directly. In the
next section, We propose a neW rounding procedure for returning a primal solution based on the
dual solution (ft, gt, λt). This neW rounding procedure involves a dedicated Way to compute the
margins.
2.1	The Rounding Procedure and the Margins
Given a ∈ ∆n, b ∈ ∆n, and π = {πk}k∈[N] satisfying r(Pk πk) = a, We construct vectors
ak, bk ∈ Rn, k ∈ [N] from the procedure
(ak, bk)k∈[N] = Margins(π, a, b).	(16)
The details of this procedure is given beloW. First, We set ak = r(πk), Which immediately implies
PkN=1 ak = a. We then construct bk such that the folloWing properties hold (these properties are
required in our convergence analysis later):
(i)	bk ≥ 0;
(ii)	PkN=1 bk = b;
(iii)	Pin=1 aik = Pjn=1 bjk, ∀k∈ [N];
(iv)	For any fixed j ∈ [n], the quantities bjk - [c(πk)]j have the same sign for all k ∈ [N]. That is,
for any k and k0, We have
(bj - [c(∏k)]j) ∙ (bk0 - [c(∏k0)]j) ≥0,	(17)
4
Under review as a conference paper at ICLR 2022
which provides the following identity that is useful in our convergence analysis later:
N	N n	nN
X kbk -c(∏k )kι = XX |bk - [c(∏k )]j I = X X(bk - [c(∏k )]j)
k=1	k=1 j=1	j=1 k=1
n N N
Xbj- c Xπk =b-c Xπk
j=1	k=1	j	k=1
(18)
The procedure on constructing (bk)k∈[N] satisfying these four properties is provided in Appendix
A.
After (ak, bk)k∈[N] are constructed from (16) with π = π(fT , gT -1, λT-1), we adopt the rounding
procedure proposed in Altschuler et al. (2017) to output a primal feasible solution (∏k)k∈[N]. The
rounding procedure is described in Algorithm 2.
With this new procedure for rounding and computing the margins ak , bk , we now formally describe
our PAM algorithm in Algorithm 1. Note that the algorithm is terminated when the following criteria
are met:
kct-1 - bk1 ≤ /(6(6c∞ - ηι)),	(19a)
∣∣λt - λt-1∣∣2 ≤ ηe∕(18c∞),	(19b)
F(ft,gj,λt-1) ≤ e/6,	(19c)
ɪ , . . . . ~ ,. .. 一 ,.. ... 一 ,. 一、
where F(f, g, λ) is the suboptimality defined as: F(f, g, λ) = F(f *,g*, λ*) - F(f, g, λ).
Algorithm 1 Projected Alternating Maximization Algorithm
1:	Input: Cost matrices {C k}1≤k≤N, vectors a, b ∈ ∆n+ with b > 0, accuracy . f0 = g0 =
[1,..., 1]>, λ0 = [1/N,..., 1/N]> ∈ ∆+N. t = 0
2:	Choose parameters as
η = min{3(i0g"N) + 1),c∞}, T = η∕c∞.	QO)
3:	while (19) is not met do
4:	Compute ft+1 by (13)
5:	Compute gt+1 by (14)
6:	Compute λt+1 by (12c)
7:	t — t + 1
8:	end while
9:	Assume stopping condition (19) is satisfied at the T-th iteration. Compute (ak, bk)k∈[N] =
Margins(π(f T, gT-1, λT-1), a, b) as in Section 2.1.
10:	Output: (∏,λ) where πk = Round(πk(fT, gτ-1, λτ-1), ak,bk), ∀k ∈ [N], λ = λτ-1.
Algorithm 2 Round(π, a, b)
1	Input: π ∈ Rn×n, a ∈ R*, b ∈ R*.		
2	:X=	Diag (x) with xi =	√v八1 r(π)i
3	: π0 =	Xπ	
4	:Y=	Diag (y) with yj =	_bj—八 1 c(∏0j 八 1
5	: π00 =	π0Y	
6	: erra	= a - r(π00), errb	= b - c(π00 )
7	: Output: π00 + erraerrb>/kerrak1.		
5
Under review as a conference paper at ICLR 2022
2.2 Connections with BCD and BCGD Methods
We now discsuss the connections between PAM and the block coordinate descent (BCD) method
and the block coordinate gradient descent (BCGD) method. For the ease of presentation, we now
assume that we are dealing with the following general convex optimization problem with m block
variables:
min	J(x1,x2, . . . ,xm),	(21)
xi ∈Xi ,i=1,...,m
where Xi ⊂ Rdi and J is convex and differentiable. The BCD method for solving (21) iterates as
follows:
xit+1 = argmin J(xt1+1,xt2+1, . . . ,xit-+11,xi,xit+1, . . . ,xtm),	(22)
xi∈Xi
and it assumes that these subproblems are easy to solve. The BCGD method for solving (21) iterates
as follows:
xt+1 = argmin BxiJ (x1+1, x2+1, ∙ ∙ ∙, x；+1, Xi, xt+ι, ∙ ∙ ∙, x>), Xi - Xti + ɪ l∣Xi — xtk2, (23)
xi∈Xi	2τ
where τ > 0 is the step size. The PAM (12) is a hybrid of BCD (22) and BCGD (23), in the
sense that some block variables are updated by exactly solving a maximization problem (the f and g
steps), and some other block variables are updated by taking a gradient step (the λ step). Though this
hybrid idea has been studied in the literature Hong et al. (2017); Xu & Yin (2013), their convergence
analysis requires the blocks corresponding to exact minimization to be strongly convex. However,
in our problem (11), the negative of the objective function is merely convex. Hence we need to
develop new convergence proofs to analyze the convergence of PAM (Algorithm 1). How to extend
our convergence results of PAM (Algorithm 1) to more general settings is a very interesting topic
for future study.
3 Convergence Analysis of PAM
In this section, we analyze the iteration complexity of Algorithm 1 for obtaining an -optimal solu-
tion to the original EOT problem (2). The -optimal solution to (2) is defined as follows.
Definition 2 (see, e.g., Nemirovski (2005)) We call (π, λ) ∈ ∏N X Δn an e-optimal solution to
the EOT problem (2) if the following inequality holds:
max '(π, λ) — min '(π, λ) ≤ e.
λ∈∆N	π∈ΠN
a,b
Note that the left hand side of the inequality is the duality gap of (2).
3.1 Main Result
We now present our main theorem, which gives the iteration complexity of PAM such that (55) is
satisfied, and as a result of Lemma 15, an e-optimal solution to the original EOT problem (2) is
obtained.
Theorem 3 Define e0 = e∕(6c∞ — η∣), and set T as
36	648c2∞	28	2 -2
T = 5+	+	I = O (c∞e ),
η√Y0e0	ηe	ηγ0e	∞	/
(24)
where Y = min{ (2c∞-η∣)2,
is a constant and we know γ0 = O(c-∞2). The output pair of
Algorithm 1 is an e-optimal solution of the EOT problem (2).
Proof. See Appendix B.

Remark 4 Though our complexity result matches the rate of the Sinkhorn’s algorithm in terms of
the dependence on e, we argue that EOT is a more difficult problem than the entropic regularized OT,
and thus our results are promising. First, EOT is a saddle-point problem while entropic regularized
6
Under review as a conference paper at ICLR 2022
OT is a minimization problem. Second, the extra variable λ in EOT requires a gradient projection
step in the PAM algorithm, which introduces significant difficulty to the analysis of the convergence
behavior. While for Sinkhorn’s algorithm it is much easier to analyze, because the dual is uncon-
strained. Third, since there are multiple agents in EOT, it is more difficult to design the rounding
procedure to obtain the primal solution. We also note that the dependence of c∞ in our result and
in the result of Sinkhorn’s algorithm Dvurechensky et al. (2018) are both c2∞.
4 Projected Alternating Maximization with Extrapolation
In this section, we discuss how to accelerate the PAM algorithm (Algorithm 1). It can be shown that
the gradient of F in (11) is Lipschitz continuous1. Therefore, Scetbon et al. Scetbon et al. (2021)
proposed to adopt Nesterov’s accelerated gradient method Nesterov (2004) to solve (11). Their
algorithm, named APGA (Accelerated Projected Gradient Ascent algorithm), iterates as follows:
(v, w, z)> ~(fj, gt-1, λt-1)> + t-2 ((f t-1, gt-1, λt-1)> - (f t-2, gt-2, λt-2)>) (25a)
t+ 1
(f t, gt)> — (V, W)> + 1 N(f,g)F(V, w,z)	(25b)
L
(λt)> J Proj ∆ N (Z + L ▽、F(V, w, Z)) ,	(25c)
where L is the LiPschitz constant of VF. Note that APGA treats the problem (11) as a generic Con-
vex and smooth problem, and does not take advantage of the special structures of (11). In particular,
f and g are updated using gradient ascent steps. This is in contrast to PAM in which f and g are
obtained by exact maximizations, which is expected to improve the function value of F more sig-
nificantly. In the following, we will design an accelerated algorithm that utilizes this property. Our
method is called PAME (PAM with Extrapolation) and it incorporates the extrapolation technique
to the gradient step for updating λ, and f and g are still updated using exact maximizations. We
note that currently we are not able to prove a better complexity for PAME. Our iteration complexity
result in Theorem 22 is in the same order as that of PAM, but numerically we have observed great
improvement of PAME over PAM. It is an interesting future topic to study other accelerations to
PAM that can provably achieve improved complexity.
A typical iteration of our PAME algorithm is given below:
f t+1 = f t + η log ( (P Jk at t - ) ,	(26a)
r( k ζk(ft, gt, λt))
gt+1 = gt + ηlog (	. J+1 t ),	(26b)
c( k ζk(f t+1, gt, λt))
yt+1 = PrOjδn (λt + (1 - θ)(λt - λt-1)),	(26c)
λt+1 = Proj∆N (yt+1 + TVλF(ft+1, gt+1, yt+1)) .	(26d)
Here θ ∈ (0, 1) is a given parameter for the extrapolation step. We see that steps (26a)-(26b) are the
same as (13)-(14) and they are solutions to the exact maximizations (12a)-(12b). Steps (26c)-(26c)
give extrapolation to the gradient step for λ, similar to Nesterov’s accelerated gradient method.
Note that PAME (26) solves the dual entropy-regularized EOT problem (11). We use the same
rounding procedure in Section 2.1 to generate a primal solution to the original EOT problem (1).
The complete PAME algorithm is described in Algorithm 3. Note that the algorithm is terminated
when the following criteria are met:
kct-1 -bk1 ≤ /(6(6c∞ - ηι),	(27a)
kλt-1 - λt-2k2 ≤ ηe∕(60(1 - θ)c∞),	(27b)
kλt- ytk2 ≤ ηe∕(42c∞),	(27c)
F(ft,gt-1,λtT) ≤ e∕6.	(27d)
1In Lemma 8 we proved that VλF is Lipschitz continuous. The Lipschitz continuity of Vf F and NgF
can be proved similarly.
7
Under review as a conference paper at ICLR 2022
Algorithm 3 Projected Alternating Maximization with Extrapolation Algorithm
1:	Input: Cost matrices {Ck}1≤k≤N, accuracy , θ ∈ (0, 1).
2:	Initialization: f0 = g0 = [1, ..., 1]>, λ0 = [1/N, ..., 1/N]> ∈ ∆N.
3:	Choose parameters as
3(log(n2N) + 1) ,c∞}, τ=2⅛.
(28)
4:	while (27) is not met do
5:	Compute ft+1 by (26a)
6:	Compute gt+1 by (26b)
7:	Compute yt+1 by (26c)
8:	Compute λt+1 by (26d)
9:	t <— t + 1
10:	end while
11:	Assume the stop condition (27) is satisfied at the T-th iteration. Compute (ak , bk)k∈[N] =
Margins(π(fT, gT-1, λT-1), a, b) as in Section 2.1.
12:	Output: (π,5) where πk = Round(πk(fT,gT-1,λTT),ak,bk), ∀k ∈ [N], λ = λτ-1.
Figure 1: Computational time comparison between PAM, PAME and APGA algorithms on Gaussian
distributions. Upper Left: N = 10, n = 100, η = 0.1, Upper Right: N = 10, n = 500, η = 0.1,
Bottom Left: N = 10,n = 100, η = 0.5, Bottom Right: N = 5,n = 100, η = 0.1.
4.1 Convergence Analysis of PAME Algorithm
In this section, we analyze the iteration complexity of PAME (Algorithm 3) for obtaining an -
optimal solution to the original EOT problem (1). The proof for PAME is different from that of
PAM, and here we need to analyze the behavior of the following Hamiltonian, inspired by Jin et al.
Jin et al. (2018).
E (f,g,λ1,λ2) = F (f,g,λ1) - 21τ kλ1 - λ2k2.
(29)
8
Under review as a conference paper at ICLR 2022
Theorem 5 Define 0 = /(6c∞ - ηι), and set T to be
T=8+
48
η√γ1e0
3600(1 - θ)2 + 882 c2∞
ηe2
s+券=O (c∞e-2),
(30)
+
where Y1 =min{ (2c∞-η∣)2，
2(2θ-θ2)
(7-5θ)2c∞ ,
and we know γ1 = O(c-∞2). At least one of the
iterations in Algorithm 3, after rounding, is an e-saddle point of the EOT problem (2).
Proof. See Appendix C.

Remark 6 We are not able to analytically prove that PAME has an improved complexity bound
at this moment yet. The APGA proposed in Scetbon et al. (2021) in fact has better complexity
than PAM and PAME. However, as demonstrated in Scetbon et al. (2021) and in our numerical
experiments (Sections 5 and D), APGA performs worse than PAM. We believe the reason is that
APGA takes gradient step for the variables f and g, while PAM exactly minimizes the subproblems
corresponding to these two variables. It is the exact minimization step that led to the improvement.
Developing a provably better algorithm is definitely important and interesting, and we will work on
it in the future.
5 Numerical Experiments
We compare the performance of PAME with PAM and APGA (25) Scetbon et al. (2021) on a syn-
thetic dataset: the Gaussian distributions. We also conduct numerical comparison on another syn-
thetic dataset: the fragmented hypercube dataset, and the results will be given in the Appendix.
Gaussian Distribution: Consider the case when two sets of discrete support {xi}i∈[n] , {yj }j∈[n]
are independently sampled from Gaussian distributions
N	11	,	110	110	andN	22	,	-01.2	-10.2	(31)
respectively. The base cost matrix Cbase is computed by Cib,ajse = kxi - yj k22 . Assume we have
N agents. The cost matrix of each agent can be obtained by adding Gaussian noise sampled from
N (0, 10) to each element of the base cost. For instance, for the k-th agent with a cost matrix Ck,
we have Cik,j = |Cib,ajse +N (0, 10)|.
We then set a = b = [1/n,..., 1/n] for all experiments. For all algorithms, We set T = ∙5η and We
c∞
set θ = 0.1 for the PAME algorithm. We consider the EOT error as a measure of optimality. The
EOT error at iteration t is defined by
Error = ∣'(∏(f t,gt,λt),λt)- '",
(32)
where ' is the approximated optimal value of EOT (2) obtained by running the PAM algorithm for
20000 iterations. Figures 1 plots the EOT error against the execution time for Gaussian distributions.
We run each algorithm for 2000 iterations for different parameter settings. In all cases, the PAME
and PAM perform significantly better than APGA, and PAME also shoWs significant improvement
over PAM.
6 Conclusion
In this paper, We provided the first convergence analysis of the PAM algorithm for solving the EOT
problem. Specifically, We have shoWn that it takes at most O(e-2) iterations for the PAM algorithm
to find an e-saddle point. We proposed a PAME algorithm Which incorporates the extrapolation
technique to PAM. The PAME shoWs significant numerical improvement over PAM. Results in this
paper might shed lights on designing neW BCD type algorithms.
9
Under review as a conference paper at ICLR 2022
References
Jason Altschuler, Jonathan Niles-Weed, and Philippe Rigollet. Near-linear time approximation algo-
rithms for optimal transport via Sinkhorn iteration. In Advances in neural information processing
systems ,pp.1964-1974, 2017.
David Alvarez-Melis, Stefanie Jegelka, and Tommi S Jaakkola. Towards optimal transport with
global invariances. In The 22nd International Conference on Artificial Intelligence and Statistics,
pp. 1870-1879. PMLR, 2019.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In International conference on machine learning, pp. 214-223. PMLR, 2017.
Marc G Bellemare, Will Dabney, and Remi Munos. A distributional perspective on reinforcement
learning. In International Conference on Machine Learning, pp. 449-458, 2017.
Jean-David Benamou, Guillaume Carlier, Marco Cuturi, Luca Nenna, and Gabriel Peyre. Iterative
Bregman projections for regularized transportation problems. SIAM Journal on Scientific Com-
puting, 37(2):A1111-A1138, 2015.
Felix Brandt, Vincent Conitzer, Ulle Endriss, Jerome Lang, and Ariel D Procaccia. Handbook of
computational social choice. Cambridge University Press, 2016.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in
neural information processing systems, pp. 2292-2300, 2013.
Pavel Dvurechensky, Alexander Gasnikov, and Alexey Kroshnin. Computational optimal transport:
Complexity by accelerated gradient descent is better than by Sinkhorn’s algorithm. In Interna-
tional Conference on Machine Learning, pp. 1367-1376. PMLR, 2018.
Aude Genevay, LenaiC Chizat, Francis Bach, Marco Cuturi, and Gabriel Peyre. Sample complexity
of sinkhorn divergences. In The 22nd International Conference on Artificial Intelligence and
Statistics, pp. 1574-1583, 2019.
M. Hong, X. Wang, M. Razaviyayn, and Z.-Q. Luo. Iteration complexity analysis of block coordi-
nate descent method. Mathematical Programming Series A, 163(1):85-114, 2017.
Minhui Huang, Shiqian Ma, and Lifeng Lai. A Riemannian block coordinate descent method for
computing the projection robust Wasserstein distance. In Proceedings of the 38th International
Conference on Machine Learning, volume 139, pp. 4446-4455. PMLR, 2021a.
Minhui Huang, Shiqian Ma, and Lifeng Lai. Projection robust Wasserstein barycenters. In Proceed-
ings of the 38th International Conference on Machine Learning, volume 139, pp. 4456-4465.
PMLR, 2021b.
Chi Jin, Praneeth Netrapalli, and Michael I Jordan. Accelerated gradient descent escapes saddle
points faster than gradient descent. In Conference On Learning Theory, pp. 1042-1085. PMLR,
2018.
Tianyi Lin, Chenyou Fan, Nhat Ho, Marco Cuturi, and Michael Jordan. Projection robust Wasser-
stein distance and Riemannian optimization. In NeurIPS, volume 33, 2020.
Erika Mackin and Lirong Xia. Allocating indivisible items in categorized domains. arXiv preprint
arXiv:1504.05932, 2015.
HerVe Moulin. Fair division and collective welfare. MIT press, 2003.
A. Nemirovski. Prox-method with rate of convergence O(1/t) for variational inequalities with
Lipschitz continuous monotone operators and smooth convex-concave saddle point problems.
SIAM Journal on Optimization, 15(1):229-251, 2005.
Y. E. Nesterov. Introductory lectures on convex optimization: A basic course. Applied Optimization.
Kluwer Academic Publishers, Boston, MA, 2004. ISBN 1-4020-7553-7.
10
Under review as a conference paper at ICLR 2022
Sherjil Ozair, Corey Lynch, Yoshua Bengio, Aaron Van den Oord, Sergey Levine, and Pierre Ser-
manet. Wasserstein dependency measure for representation learning. In Advances in Neural
Information Processing Systems,pp. 15604-15614, 2019.
Francois-Pierre Paty and Marco CUtUrL SUbsPace robust Wasserstein distances. In International
Conference on Machine Learning, pp. 5072-5081, 2019.
Meyer Scetbon, LaUrent MeUnier, Jamal Atif, and Marco CUtUri. EqUitable and optimal transport
with mUltiple agents. In International Conference on Artificial Intelligence and Statistics, pp.
2035-2043. PMLR, 2021.
R. Sinkhorn and P. Knopp. Concerning nonnegative matrices and doUbly stochastic matrices. Pacific
J. Math., 21:343-348, 1967.
Richard Sinkhorn. Diagonal eqUivalence to matrices with prescribed row and colUmn sUms. The
American Mathematical Monthly, 74(4):402-405, 1967.
MaUrice Sion. On general minimax theorems. Pacific Journal of mathematics, 8(1):171-176, 1958.
Robert E Tarjan. Dynamic trees as search trees via EUler toUrs, applied to the network simplex
algorithm. Mathematical Programming, 78(2):169-177, 1997.
Y. XU and W. Yin. A block coordinate descent method for regUlarized mUlti-convex optimization
with applications to nonnegative tensor factorization and completion. SIAM Journal on Imaging
Sciences, 63(3):1758-1789, 2013.
11
Under review as a conference paper at ICLR 2022
A CONSTRUCTING bk IN THE MARGINS PROCEDURE (16)
In the section, we show how to construct (bk)k∈[N] in the Margins procedure (16) such that the four
properties in Section 2.1 are satisfied.
First, we set
bk= C (πk (f T ,gT-1,λT T)) +
b-c Pk πk(fT, gT-1, λT-1)
N
It is easy to verify that properties (ii)-(iv) are satisfied. But it is possible that (i) is violated. We now
describe a procedure to iteratively update bk to achieve (i) while keeping (ii)-(iv) satisfied. If (i) does
not hold, then there exist k and j, such that bjk < 0, which further implies bjk - [c(πk)]j < 0. Since
bjk = kakk1, and	[c(πk)]j =	πikj = kakk1,
j	j	ij
there must exist an j0 such that bjk0 - [c(πk)]j0 > 0, which further implies bjk0 > 0. Moreover, since
Pk bjk = bj > 0, there must also exists an k0 such that bjk0 > 0. We then update the following
quantities:
bkk 一 bkk + θ
珞 一 bko-θ
bk0- bk0-θ
bk0 一 畤：+ θ,
where
θ = min{∣bk ∣,∣bk0∣,∣bko-[c(∏k )]j0∣}.
Note that this update maintains that (ii)-(iv) are satisfied. From our discussion above, itis guaranteed
that θ > 0. Therefore, bjk is improved, i.e., it is getting closer to 0, if not equal. Repeating this
procedure leads to bk, k ∈ [N] such that (i) is also satisfied.
B Proof of Theorem 16
B.1 Technical Preparations
We first give the partial gradients of F.
Pk,jexp((fi+gj-λkCikj)∕η)
[f (f,g, )]i = ai	Pk kζk(f,g,λ))kι =ai
Pk,i exp((fi + gj - λkCikj)∕η)
VgF(f，g,λ)]j = bj - -, Pk kζk(f,g,λ))kι- = bj -
r Xπk(f, g, λ)	,
i(33a)
c Xπk(f,g,λ)	,
j(33b)
[VλF (f,g,λ)]k
PijCj exp((fi + gj- λkCj)∕η)
k kζk(f, g, λ))k1
(33c)
Since (13) and (14) renormalize the row sum
immediately have
N
and column sum of Pk ζk (f, g, λ) to be a and b, we
k=1
kζk(ft+1,gt,λt)k1=1,	kζk(ft+1,gt+1,λt)k1=1,∀t,
(34)
k=1
N
which, combined with (8), yields
πk(ft+1,gt,λt) = ζk(ft+1,gt,λt), πk(ft+1,gt+1,λt)=ζk(ft+1,gt+1,λt),∀t.
(35)
The following lemma gives an error bound for Algorithm 2 (see Altschuler et al. (2017)).
12
Under review as a conference paper at ICLR 2022
Lemma 7 (Rounding Error) Let a, b ∈	Rn+	with Pin=1	ai	=	Pjn=1 bj	=	q,	π ∈	Rn+×n,	and
π = Round(π, a, b). Thefollowing inequality holds:
kπ - πkι ≤ 2(IlNn)- akι + ∣∣c(π) - bkι).
Proof. The proof is a slight modification from (Altschuler et al., 2017, Lemma 7). Note that Lines
2-5 in Algorithm 2 renormalize the row sum and column sum that are larger than the corresponding
ai and bj. It is easy to verify that π, π00, err。and err are nonnegative with IIerra ∣∣ι = IIerrb∣∣ι =
q - ∣π00∣1 and
r(∏) = r(π00) + r(err@err>/∣erra∣ι) = r(∏00) + err a = a ,
and likewise c(∏) = b. Denote ∆ = ∣∣n∣ι 一 k∏00∣∣ι. Since We remove mass from a row of n when
ri(π) ≥ ai, and from a column when cj (π0) ≥ bj, we have
nn
∆= X(ri(π) -ai)+ +X(cj(π0) -bj)+.
Firstly, a simple calculation shows
n1
EerMn) 一 ai)+ = 2 [kr(π) 一 a|i + |n|i 一 q] .
i=1
Secondly, the fact that the vector c(n) is entrywise larger than c(n0) leads to
nn
X(cj(n0) 一 bj)+ ≤ X(cj(n) 一 bj)+ ≤ Ic(n) 一 bI1.
j=1	j=1
Therefore we conclude
k∏ ― n∣i ≤ △+ IIerraerr>∣∣ι∕∣errakι = △ + q — k∏00kι = 2△ + q -∣∣n∣∣ι
≤ kr(n) — a|1 + 2kc(n) — b11 ≤ 2hkr(n) — ak1 + kc(n) — bkι]∙
The following lemma shows that VλF is LiPSchitz continuous.
Lemma 8 For any f, g ∈ Rn and λ1, λ2 ∈ △N, the following inequality holds
∣VλF(f,g,λ1) -VλF(f,g,λ2)∣2 ≤ c∞∣λ1 — λ2∣2∕η,	(36)
which immediately implies
F(f,g,λ1) ≥ F(f,g,λ2) + hVλF(f,g,λ2),λ1 — X2〉一 c∞∣λ1 — λ2∣2.	(37)
Proof. The Proof essentially follows Scetbon et al. (2021). It is easy to verify that the (q, k)-th entry
of the Hessian of F(f, g, λ) with resPect to λ is
∂ 2F
∂λq∂λk
ην2 [σq,1(λ)σk,1(λ) 一
ν(σk,2(λ)11k=q)]
where 11k=q = 1 iff k = q and 0 otherwise, for all k ∈ {1, ..., N} and p ≥ 1
N
ν=XXexp
fi + gj —
η
fi + gj 一
η
13
Under review as a conference paper at ICLR 2022
Let V ∈ RN satisfying ∣∣vk2 = 1, and by denoting VλF the Hessian of F with respect to λ for fixed
where the last three inequalities come from Cauchy Schwartz inequality. Moreover we have
1
ην2
N
- ν	vk2σk,2
k=1
〉PN=I v2σk,2	c∞
≥≥
ην	η
v
T
kXN=1vkσk,1(λ)
which completes the proof.

The next lemma gives a bound for g .
Lemma 9 Let (ft, gt, λt) be the sequence generated by Algorithm 1. For any t ≥ 0, it holds that
max
j
gjt
- min gjt ≤ c∞ - ηι,
max
j
gj -
min gj ≤
c∞ - ηι.
(38a)
(38b)
Proof. We prove (38a) first. When t = 0, (38a) holds because of the initialization g0. When t ≥ 1,
from (15) we have
N
N
N
X e-λk-1cj/η ≥ X e-λk-1kckk∞∕η ≥ X e-∣ckk∞∕η ≥ Νe-∞/η,
(39)
k=1
k=1
k=1
where the second inequality is due to 0 ≤ λtk-1 ≤ 1. Combining (39) and (34) we get
j ∙ Ne-c∞∕η(1,£产/〉≤ X j (X e"C加)e5 = bj ≤ 1,
i	k=1
which leads to
maxgj ≤ c∞ 一 ηlog(N〈1,eft/n)).
(40)
Moreover, note that e-λk 1Ckj/η ≤ 1, therefore NN PN=
ej Pd'/η i≥ X est / ( X e
k=1
1 e-λk -Cikj/η ≤ 1. This fact leads to:
λk-1 Cij∕η[ efi∕η = Bbj,
i

which gives
mingj ≥ η∣ 一 ηlog(N〈1, eft/η〉).
j
(41)
Combining (40) with (41) yields (38a). The bound for gj (38b) can be obtained similarly, by noting
that πj ∈ ΠaN,b. We omit the details for brevity.

Lemma 10 Let {ft, gt, λt} be generated by PAM (Algorithm 1). The following equality holds.
N
X k∏k(ft+1,gt+1,λt) - ∏k(ft+1,gt,λt)k1 = I∣ct - b∣∣1,∀t.
k
14
Under review as a conference paper at ICLR 2022
Proof. By (35), we have
N
X kπk(ft+1, gt+1, λt) - πk(ft+1, gt, λt)k1
k
N
=XX∣e(ft+1+gj+1-λkCkj)/η - e(ft+1+gj-MCkj)/η∣
k i,j
N
= X X[πk(ft+1, gt, λt)]i,j bj/ctj - 1 = X ctj|bj/ctj - 1| = ct - b1.
k i,j	j
B.2 Key Lemmas
In this subsection, we provide afew useful lemmas that will lead to our main theorem on the iteration
complexity of PAM (Algorithm 1). These lemmas yield the following results: the function F is
monotonically increasing (Lemmas 11), the suboptimality of the dual problem can be upper bounded
(Lemma 12-14), and the PAM returns an -optimal solution under conditions (55) (Lemma 15). In
Theorem 16 we will show that these conditions can indeed be satisfied.
Lemma 11 [Increase of F] Let {ft , gt , λt} be generated by PAM (Algorithm 1). The following
inequalities hold:
F(ft+1,gt,λt)-F(ft,gt,λt) ≥0	(42a)
F(ft+1,gt+1,λt) - F(ft+1,gt,λt) ≥ 2∣∣ct - b∣∣2	(42b)
F(ft+1,gt+1,λt+1)- F(ft+1,gt+1,λt) ≥ c∞kλt+1 - λtk2∕(2η).	(42c)
Proof.
First, (42a) is a direct consequence of (12a).
Next, we prove (42b). We have
F(ft+1,gt+1,λt)-F(ft+1,gt,λt)
=hgt+1-gt,bi-ηlog XN kζk(ft+1,gt+1,λt)k1 +ηlog XN kζk(f t+1, gt, λt)k1
n
=hgt+1 -gt,bi = ηXbjιog(bj/Cj) = ηK(bllct) ≥ 2Ilct -bk1,
j=1
where K(x||y) denotes the KL divergence of x and y, the second equality is due to (34), the third
equality is due to (14), and the last inequality follows the Pinsker’s inequality.
Finally, we prove (42c). From the optimality condition of (12c), we know that there exists
h(λt+1) ∈ ∂I∆N (λt+1)
such that
VλF(ft+1, gt+1,λt) - 1(λt+1 - λt) - h(λt+1) = 0.
τ
(43)
(44)
From (37) we have
F(ft+1,gt+1, λt+1) - F(ft+1,gt+1,λt) ≥ (VλF(ft+1, gt+1,λt), λt+1 - λti- c∞kλt+1 - λtk2
2η
h1(λt+1 - λt) + h(λt+1), λt+1 - λti -
τ
≥ h1(λt+1 - λt),λt+1 - λti -
τ
= c2∞kλt+1 - λtk2∕(2η),
c2
c∞ kλt+1 - λtk2
2η
c2
c∞ kλt+1 - λtk2
2η
15
Under review as a conference paper at ICLR 2022
where the first equality is due to (44), the second inequality is due to (43), and the last equality is
due to the definition of τ in (20).
Before we bound the suboptimality gap, we need the following lemma.
Lemma 12 Let {ft , gt , λt} be generated by PAM (Algorithm 1). For any λ ∈ ∆N, the following
inequality holds:
J - λt, VλF(ft+1,gt, λt)> ≤ 3c∞kλt+1 - λtk2∕η + c∞ M- b∣∣1 .	(45)
Proof. The optimality condition of (12c) is given by:
hλ - λt+1,1(λt+1 - λt)-VλF(ft+1,gt+1,λt)i ≥ 0,	∀λ ∈ ∆N,
τ
which implies that
hλt+1 - λ, -VλF (f t+1, gt+1, λt)i
≤hλ - λt+1,1(λt+1 - λt)i ≤ 1 kλ - λt+1k2kλt+1 - λtk2 ≤ 2c∞kλt+1 - λtk2∕η,
ττ
(46)
(47)
where the last inequality is due to the fact that the diameter of ∆N is bounded by √2 ≤ 2. Moreover,
we have
hλt - λ, VλF (f t+1, gt+1, λt) - VλF (f t+1, gt, λt)i
N
=XX -λk) ∙ h∏k(ft+1,gt+1, λt) - πk(ft+1,gt, λt), Cki
k
N
≤ X kπk(ft+1, gt+1, λt) - πk(ft+1, gt, λt)k1kCkk∞
k
≤ c∞ kct - bk1 ,
where the equality is due to (33c), and the last inequality is due to Lemma 10. Finally, we have
λt - λ, -VλF(ft+1, gt, λt)
= λt - λt+1, -VλF(ft+1, gt+1, λt) + λt+1 - λ, -VλF(ft+1, gt+1, λt) +
λt - λ, VλF(ft+1, gt+1, λt) - VλF(ft+1, gt, λt)
≤kλt- λt+1k2 ∙kVλF(ft+1,gt+1,λt)k2 +2c∞kλt+1 - λtk2∕η + c∞kct-bkι,
(48)
(49)
where the first inequality is due to (47) and (48). From (33c) we have kVλF(ft+1, gt+1, λt)k2 ≤
c∞, which, combined with (49) and the fact that η ≤ c∞, yields the desired result.
The suboptimality of (11) is defined as: F(f,g,λ) = F(f*,g*,λ*) - F(f, g, λ). Note that
F(f,g,λ) ≥ 0, ∀f,g,λ ∈ δn.
Lemma 13 Let (ft, gt, λt) be generated by PAM (Algorithm 1). The following inequality holds:
F(ft+1, gt, λt) ≤ (2c∞ - η∣)kct - bkι + 3c∞kλt+1 - λtk2∕η.
Proof. Denote ut = (maxj gjt + minj gjt)∕2, u* = (maxj gj* + minj gj*)∕2. From (35) we get
nn
h1, ct - bi = Xai -Xbj = 0,
which further implies
gt - g*, ct - b = (gt - ut1) - (g* - u*1), ct - b	(50)
≤ (kgt - ut1k∞ + kg* - u*1ll∞) ∣∣ct - b∣∣ι ≤ (C∞ - ηI) ∣∣ct - b∣∣ 1,
16
Under review as a conference paper at ICLR 2022
where the last inequality is due to Lemma 9. Now We set λ = λ* in (45), and We obtain
<λt - λ*,-VλF(ft+1,#, λt)> ≤ 3c∞kλt+1 - λtk2∕η + c∞ ∣∣ct - b∣∣1.	(51)
Since F(f, g, λ) is a concave function, we have
F(f*,g*,λ*) ≤ F(ft+1, gt, λt) + hVF(ft+1, gt, λt), (f*,g*2-(ft+1,gt, λt)i,
which, combining with (33) yields
F(f t+1,gt, λt) = F(f*，g*，λ*) - F(ft+1,gt, λt)
≤ hft+1 - f *, r(PN=ι ∏k(f t+1,gt, λt))- a + hgt - g*,ct - bi
+ 3- λ*,-VλF(ft+1,gt,λt))
≤ (2c∞ - ηι)kct - bk1 + 3c2∞kλt+1 - λtk2∕η,
where the last inequality follows from (15), (35), (50) and (51).
The next lemma shows that the suboptimality gap F(f, g, λ) can be bounded by O(1∕t).
Lemma 14 Let (ft, gt, λt) be generated by PAM (Algorithm 1). The following inequality holds:
F(f t+1,gt+1 ,λt+1)
≤_________45γ0)__________
-t + 1 + 4∕(ηγ0F(f 0,g0,λ0)),
where γ0 = min 4 TTt-1―72,77⅛-
0	(2c∞ -ηι)2 , 9c2∞
is a constant.
Proof. Combining (42b) and (42c), we have
F(ft+1,gt+1,λt+1)- F(ft+1,gt,λt) ≥ 2∣∣ct - b∣∣2 + c∞∣∣λt+1 - λt∣∣2/(2η).	(52)
Therefore, we have
F1(ft+1,gt+1,λt+1) - F'(ft+1,gt,λt)
≤-2∣∣Ct- b∣∣2 -c∞∣∣λt+1- λt∣∣2 /(2η)
≤ - 2Y0 ∙ (((2cg -ηI)kct - bkI)2 + (3c∞kλt+1 -11|2加)2)	(53)
≤ - 4γθ((2c∞ - ηI)kct - bk1 + 3c∞kλt+1 - λ1∣2∕η)2
≤-4 γ0F(f'+1,g' ,λt)2,
where the last inequality is from Lemma 13. Dividing both sides of (53) by F(f t+1, gt+1, λt+1) ∙
F(ft+1, gt, λt), we have
1	、	1	l η	F1(ft+1,gt,λt)
≥	+ Y0 ∙
F1(ft+1,gt+1, λt+1) — F(ft+1, gt, λt)	4 / F(ft+1, gt+1,λt+1)
1	η	1η
≥	+ Y0 ≥	+ Y
-]^t+t+1,gt,λt)	4' - F^(ft,gt,λt)	4'
(54)
where the second inequality is due to (53) and the last inequality is from (42a). Summing (54) from
0 to t leads to
1	〉	1	, η(t + 1L
------------------- ≥ -------------+-----------Y0,
F(ft+1,gt+1,λt+1) — F(f0,g0,λ0)	4	' ,
which implies the desired result.
The next lemma gives sufficient conditions for the PAM algorithm to return an -optimal solution to
the original EOT problem (2).
17
Under review as a conference paper at ICLR 2022
Lemma 15 Assume PAM terminates at the T -iteration, i.e.,
kcT -1 - bk1 ≤ /(6(6c∞ - ηι)),	(55a)
∣∣λτ - λτ-1∣∣2 ≤ ηe∕(18c∞),	(55b)
F(fT,gT-1,λTT) ≤ e/6.	(55c)
Then theoutput (∏, W) ofPAM(Algorithm 1), i.e., πk = Round(πk (f T, gτ-1,λτ-1), ak, bk), ∀k ∈
[N ],^=λτ-1, is an E-optimal solution of the original EOT problem (2).
Proof. According to Definition 2, it is sufficient to show that the output (∏, W) ∈ ∏Nb X Δn satisfies
the following two inequalities:
E
max ' (π, λ) — '(π, λ) ≤ -,	(56a)
E
'(π,λ) — mm '(π, λ) ≤ -.	(56b)
π∈ΠaN,b	2
We prove (56a) first. For ease of presentation, We denote π = π(fT,gτ-1,λT-1), π*
π(f *,g*,λ*). Note that πk = Round(∏k, ak,bk),∀k ∈ [N]. We also denote
X(π):
argmax
λ∈∆N
N
'(π,λ) = X λk hπk,Cki
k=1
(57)
(
)
Note that the term on the left hand side of (56a) can be rewritten as
' (∏, λ(τr))— ' 卜,只)
=('(∏,λ(∏^)) — '(∏, λ(π))) + (['(∏, λ(π)) — ηH (∏)] — ['(π*,λ*) — ηH (π*)])
、-----------{-----------} 、---------------------{-----------------------}
(I)	(II)
一.^ .. .., -.	ʌ. .,.	, ʌ. ʌ..
+ (['(π*,λ*) —	ηH(π*)]	— ['(∏,	λ)	— ηH(∏)]) +	('(∏, ^)	— '(π, ^)).
x--------------------{--------------------} X--------{---------}
(III)	(IV)
(58)
We now provide upper bounds for these four terms. Denote
Q = argmaXhnk, Ck〉， Q = argmaxh∏k, Ck〉.	(59)
k∈[N]	k∈[N]
Since (1) and (2) are equivalent, we have the following for the term (I):
(I)	= XN(∏)]kh∏k ,Ck i — X[λ(∏)]k h∏k ,Ck i = h∏ Q,C Q、-贫,c Ck)
kk
ʌ , ʌ , ʌ , ʌ , ʌ , ʌ , _______________________
≤h∏ ®,c®i— hn” C D≤w — n® kikek k∞ ≤ c∞ X k∏k — πk kι ⑼)
k
≤2c∞ X(kr(nk) — akki + ∣∣c(∏k) — bkki) = 2c∞∣∣cT-1 — b∣∣ι,
k
where the first inequality follows from the definition of Q in (59), the fourth inequality is from
Lemma 7, and the last equality follows from (15) and (17).
For the term (II), recall that H(π)	=	— PkijnTj(log∏kj _ 1) and ∏k	=
fT 1>+1(gT-1)>-λT-1Ck	T-1	maxj gjT -1 +minj gjT -1
exp ( ---------η√----k---) due to (35), and define UI 1 = --------j——2------j-.	We
18
Under review as a conference paper at ICLR 2022
have
(II)	= X λ(∏) k h∏k ,Ck i + η X 潸,j f fT + gj—_ λk	嘴-1) - F *
k	k,i,j ∖	η	)
=X(λ∏)k- λk )h∏k,ck〉+ XnkJ (fT+gT-1 -η) - F *
k	k,i,j
=Q∏) - λ, VlF f, gT-1,Λt-1)〉+〈产,* + ⑹-1, CTT〉- η Pi,j,k 稔-F*
=(λ(π) - ʌ, ViF (f t ,gτ-1,λT-1))+ (f t ,a) + ST-1,ct-1〉- log (X 忻k ∣∣J - η - F *
=(入⑺-λ, ViF(fτ, gτ-1,λτ-1)〉+ STT ctT- bi + F(fτ, gτ-1,λτT)- F*
≤ (λ(∏) - λ, ViF(fτ, gτ-1,λτ-1)〉+ STτ, ctT- Ibb
≤c∞∣∣ctT- b∣∣ι + 3c∞∣∣λT- λττ∣∣2∕η + STT-UT-11, ctT- bi
≤c∞∣∣ctT- b∣∣ι + 3c∞∣∣λT- λττ∣∣2∕η + IIgTT-UTTIhkCTT- b∣∣1
43c∞∕2 - ηι∕2州CT-1 - bkι + 3c∞kAτ - λτ-l∣∣2∕η,
(61)
where the third equality uses (35), (33c) and (15), the second inequality follows from Lemma 12 by
setting λ = λ(τr) and t = T - 1, and the last inequality uses Lemma 9.
For the term (III), we have
(III)	≤ X λk h∏k ,ck i + η X ∏k,j (--%----------k--------1) - F *
k	ij '	η	)	(62)
= KgT-1,ct T-bi + F (fτ ,gτ-1,λτ T)-F *|
≤(c∞∕2 - ηι∕2)∣cτ-1-bkι + |F(fτ,gτ-1,λτT)- F*|,
where the last inequality follows from Lemma 9.
Finally, for the term (IV), we have
(IV )= Ehnk - ∏k ,九。k i ≤ E k∏k - ∏k kι∣Qk k∞
kk
≤2c∞ X(∣∣r(∏k) - αk∣1 + ∣∣c(∏k) - bk∣1) = 2c∞∣∣ctT- b∣1,
k
(63)
where the first inequality uses |黑 | ≤ 1, the second inequality uses Lemma 7 and (17). Plugging
(60) - (63) into (58), and using (55), we obtain (56a).
Now we prove (56b). For ease of presentation, we denote
τr(λ) := argmin'(π,λ).	(64)
π∈ΠN,
a,b
We also denote b = ct-1 = Pk C (∏k) and π0k = Round(π(λ)k, αk, bk), where
(ak,bk)k∈[N]:= MarginS(π(λ), α, b),
as defined in (16). From (18) we know that
XIlC ((π(λ))k)-bk∣∣ = XC((π(Λ))k)-Xbk	= kb-汕=kb-cτ-1k1,	(65)
k	k	k 1
where the second equality is due to τr(λ) ∈ ∏N^ and thus C(Pk(∏(*))k) = b, and the fact that
Pkbk = b due to Property (ii) of the Margins procedure in Section 2.1. By the Sinkhorn,s theorem
19
Under review as a conference paper at ICLR 2022
Sinkhorn (1967), π is the unique optimal solution of mιn∏∈∏N 'η(π, λ). Therefore
a,b
∑λkh∏k ,Ck i-ηH (∏) ≤ E 九 hπ0k ,Ck i-ηH(∏0).
kk
Now, note that the left hand side of (56b) can be arranged into three parts:


,. ". , ,". ".
'(Π,^)-'(∏(^),^)
:(Xλkh∏k,cki-Xλkh∏k,cki) +(Xλkh∏k,cki-Xλkhπ0k,cki
kk	kk
'------------{--------------} '-------------{------------
(V)	(VI)
+
|
}
{z^^^
(V II)
(66)
(67)
}
We now upper bound these three terms. First note that the term (V) is the same as the term (IV) and
thus has the same upper bound in (63). Since 0 ≤ H(π) ≤ log(n2N) + 1, from (66) we have that
(VI ) = X λk h∏k ,ck i-X λk hπ0k ,c ki ≤ η∣H (∏) - H (∏0)∣ ≤ 1 的
kk
where the last step uses the definition of η in (20).
For the term (VII), we have
(VII )= X λkhπ0k ,Ck i-X λ h(π(λ))k ,Ck i≤ X kπ0k - (π(λ))k kιkCk k∞
kk	k
≤2c∞ X(kr((π(λ))k) - akkι + kc((π(λ))k) - bkki) = 2c∞∣∣cτ-1 - b∣∣ι,
k
(68)
(69)
where the second inequality follows from Lemma 7, the second equality uses (65) and the fact that
r((π(λ))k) = ak due to the property of the Margins procedure in (16).
Finally, plugging (63) (note (V)=(IV)), (68) and (69) into (67), and using (55a) and noting ι < 0,
we obtain (56b). This completes the proof.

B.3 Main Result
Theorem 16 Define 0 = /(6c∞ - ηι), and set T as
36	648c2∞	28	2 -2
T = 5+	-j+^ +	I = O (c∞e ),
η√Y0e0 η	η70e
(70)
where γ0 is defined in Lemma 14 and we know γ0 = O(c-∞2). The output pair of Algorithm 1 is an
-optimal solution of the EOT problem (2).
Proof. According to Lemma 15, we only need to show that (55) holds after T iterations as defined in
(70). To guarantee (55a) and (55b), we follow the ideas of Dvurechensky et al. Dvurechensky et al.
(2018) and construct a switching process. We first reduce Fa from Fa(f0, g0, λ0) to a constant s by
running t1 steps. In this process, Lemma 14 indicates
44
ti ≤ 1 +--H------
ηγ0s	ηγ0Fa(f0, g0, λ0)
(71)
Secondly, starting from s, we continue running the algorithm, and assume that there are t2 iterations
in which (55a) fails. By (42b) we have
t2
72s
≤1 +	.
20
Under review as a conference paper at ICLR 2022
Therefore, we know that the total iteration number that (55a) fails is upper bounded by
72s	4
TI=t1+12 ≤ 2+不 + ηγos -
4
ηγoF(f 0,g0,λ0)
iterations. By choosing S = e0∕(6√70), We know that
(2 +	12	I	24	_	_4_____ ≤	2 +	36
十 η√Y0e0 十 η√T0e0 - ηγ0F(f0,g0,λ0)— 十 η√Y0e0
2 +	12	+	_^4_____________4	≤	2+	12
^r	η√γ0e0	^r	η√γ0e0	ηγ0F(f0,g0,λ0)	—	^r	η√γ0e0
otherwise.
Therefore, We have Ti ≤ 2 +——√6^7. Similarly, starting from s, the number of iterations that (55b)
fails can be bounded by
648Sc2
t3 ≤ 1 + -2∞
η2
where we apply (42b). By choosing S = , we know that the total iteration number that (55b) fails
is upper bounded by
T2 = t1 + t3 ≤ 2 +
648c∞
η
44
+ -- - -H-----
ηγoe	ηγ0F(f0,g0,λ0)
iterations. Finally, by letting S = /6 in (71), we know that
F(fT3T,gT3T,λT3-1) ≤ e/6
after
T3
24
1 +——
ηγ0
iterations. From (42a) we know that after T3 iterations, we have
F(fτ3,gτ3τ,λτ3T) ≤ F(fτ3T,gτ3T,λτ3T) ≤ e/6,
i.e., (55c) holds. Combining the above discussions, we know that after T = T1 + T2 + T3 + 1
iterations, there must exist at least one iteration such that (55) holds, and thus the output of PAM is
an -optimal solution to the original EOT problem (2).
C Proof of Theorem 22
The following simple fact is useful for our analysis later.
kyt+1 - λtk2 = kProj∆N (λt + (1 - θ)(λt - λt-1)) -PrOj∆N (λt) l∣2 ≤ (1 - θ)kλt - λt-1k2,
(72)
where the equality follows from the definition of yt+1 in (26c), and the inequality is due to the
non-expansiveness of the projection operator.
The following lemma shows that the Hamiltonian E(ft, gt, λt, λt-1) is monotonically increasing
when updating λ in Algorithm (3).
Lemma 17 [Sufficient increase in λ] Let {ft, gt, yt, λt} be generated by PAME (Algorithm 3). The
following inequality holds:
E(ft+1,gt+1,λt+1,λt) - E(ft+1,gt+1,λt,λt-i) ≥ 2θ-τθ2∣λt- λt-1k2 + 4lτkλt+1-yt+1 k2.
(73)
Note that since θ ∈ (0, 1), the right hand side of (73) is always nonnegative.
Proof. From the optimality condition of (26d) we know that, there exists h(λt+1) ∈ ∂I∆N (λt+1)
such that
VλF(f t+1,gt+1, yt+1) - 1(λt+1 - yt+1) - h(λt+1) = 0.	(74)
τ
21
Under review as a conference paper at ICLR 2022
By the convexity of the indicator function Iδn (λt+1), we have
hyt+1 - λt+1, h(λt+1)i ≤ 0,	(λt - λt+1, h(λt+1)i ≤ 0.	(75)
Moreover, we have the following inequality:
kλt+1 - λtk2 = kλt+1 - yt+1 + yt+1 - λtk2
=kyt+1 - λtk2 + 2(λt+1 - yt+1,yt+1 - λti + ∣∣λt+1 - yt+1∣∣2	(76)
≤ (1 - θ)2∣∣λt - TT∣∣2 + 2(λt+1 - yt+1,yt+1 - λti + ∣∣λt+1 - yt+1∣∣2,
where the inequality is from (72).
We then have the following inequality:
F(ft+1,gt+1,λt) - F(ft+1,gt+1,λt+1)
≤ (F(ft+1, gt+1,yt+1) + "1F(ft+1, gt+1, yt+1), λt - yt+1))-
(F(ft+1,gt+1,yt+1) + ”ιF(ft+1,gt+1 ,yt+1),λt+1 - yt+1)- c∞kλt+1 - yt+1k2∕(2η))
=(VλF(ft+1,gt+1, yt+1), λt - λt+1i + c∞kλt+1 - yt+1k2/(2η)
≤hVιF(ft+1,gt+1,yt+1) - h(λt+1),λt - λt+1i + c∞∣∣λt+1-yt+1∣∣2∕(2η)
=1 (λt+1 - yt+1,λt - λt+1i + ɪ∣∣λt+1 - yt+1∣∣2
T	4τ
=1 hλt+1 - yt+1, λt - yt+1 + yt+1 - λt+1i + ɪ∣λt+1 - yt+1∣2
T	4T
=-1 hλt+1 -yt+1,yt+1 -λti- -3∣λt+1 -yt+1∣2,
T	4T
(77)
where the first inequality is from the concavity of F with respect to λ and (37), the second inequality
is due to (75), the second equality is due to (74). Combining (76) and (77) leads to
E(ft+1 ,gt+1,λt+1,λt) = F(ft+1,gt+1,λt+1) - 2T∣λt+1 - λt∣2
1	3
≥ F(ft+1,gt+1, λt) + 1 hλt+1 - yt+1 ,yt+1 - λti + 4T∣λt+1 - yt+1k2
-(ɪfθɪ∣λt - λt-1∣2 - 1 hλt+1 - yt+1, yt+1 - λti- ɪ∣λt+1 - yt+1∣2
2T	T	2T
=F(ft+1,gt+1, λt) - 2T∣λt - λt-1∣2 + 2θ2τθ2∣λt - λt-1∣∣2 + 4T∣λt+1 - yt+1∣2
=E(ft+1 ,gt+1,λt, λt-1) + 2θ2τθ2∣λt - λt-1∣∣2 + 4T∣λt+1 - yt+1∣2,
which completes the proof.
□
Now We define the following function E, and later we will prove that E(ft, gt, λt, λt 1) can be
upper bounded by O(1∕t).
E(f, g, λ1, λ2) = F(f*,g*, λ*) - E(f, g,λ1 ,λ2).
The next lemma is useful for obtaining the upper bound for E(ft, gt, λt, λt 1). Moreover, it is
noted that E(f,g,λ1,λ2) ≥ 0, ∀f, g, λ1,λ2,and E(f,g, λ,λ) = F(f, g,λ), ∀f,g, λ.
Lemma 18 Let {ft, gt, yt, λt} be generated by PAME (Algorithm 3). For any λ ∈ ∆N,Ihefollow-
ing inequality holds
<λ - λt, VlF(ft+1, gt, λt)> ≤ c∞∣ct - b∣1 + 7c∞∣λt+1 - yt+1∣∣2∕η + 5(1 - θ)c∞∣λt - λ一心/小
(78)
Proof. From the optimality condition of (26d), we have the following inequality:
(λ - λt+1,1(λt+1 - yt+1) -VλF(ft+1,gt+1,yt+1)) ≥ 0, ∀λ ∈ ∆N.	(79)
22
Under review as a conference paper at ICLR 2022
(83)
The left hand side of (78) can be rearranged to three terms.
hλ - λt, VλF(ft+1,gt,λt)i
=hλt- λ,-Vλ F (ft+1,gt+1,yt+1)i + hλt- λ, VλF(ft+1,gt+1D-VλF (f t+1 ,gt+1 ,λt)i
`---------------{z-------------} `-------------------------------------------}
(I)	(II)
+ hλt - λ, VλF(ft+1, gt+1, λt) - VλF(ft+1, gt, λt)i.
`---------------------{---------------------}
(III)
(80)
We now bound these three terms one by one. To bound the term (I), we first note that from (33c)
and (15), we have
kVλF(ft+1,gt+1,λt)k2 ≤ c∞ ≤ c∞∕η,	(81)
where the second inequality is due to the definition of η (28). Now we can bound the term (I) as
follows:
(I)	= λt - λt+1, -VλF (ft+1, gt+1,	λt)	+	λt	-	λt+1, VλF (ft+1,	gt+1,	λt)	-	VλF (ft+1, gt+1, yt+1)
+ λt+1 - λ, -VλF(ft+1, gt+1, yt+1)
c2
≤ 同-λt+1∣∣2 ∙ mF(ft+1,gt+1,λt)∣∣2 + ∙∞ in - λt+1∣∣2 ∙ ^λt -yt+1∖∖2
+1∖∖λt+1-λ∖∖2 ∙∖∖λt+1-yt+1∖∖2
≤3c2∞kλt -λt+1k2∕η+4c2∞kλt+1 -yt+1k2∕η,
(82)
where the first inequality uses Lemma 8 and (79), the second inequality uses (81) and the facts that
∖∖λt - yt+1∖∖2 ≤2andkλt-λk2 ≤2.
For the term (II), Lemma 8 yields:
(II)	≤2 ∖∖VλF (f t+1, gt+1, yt+1) - VλF (f t+1, gt+1, λt)∖∖2 ≤ 2c2∞ ∖∖yt+1 - λt∖∖2 ∕η.
For the term (III), it can be bounded as:
N
(III)	= X(λk - λk) ∙h∏k(ft+1, gt+1,λt) - πk(ft+1,gt, λt), Cki
k=1
N
≤ Xkπk(ft+1,gt+1,λt)-πk(ft+1,gt,λt)k1kCkk∞≤c∞kct-bk1,
k=1
where the last inequality is due to Lemma (10). Plugging (82) - (84) into (80) and applying the
triangle inequality, we obtain
λ - λt, VλF(ft+1, gt, λt) ≤ c∞kct - bk1 + 7c2∞kλt+1 - yt+1k2∕η + 5c2∞kyt+1 - λtk2∕η,
which immediately implies (78) by noting (72).
Lemma 19 Let (ft, gt, yt, λt) be generated by PAME (Algorithm 3). The following inequality
holds:
E(ft+1,gt,λt,λtT) ≤ (2c∞-η∣)kct-bkι + 7c∞kλt+1-yt+1k2∕η+(7-5θ)c∞kλt-λt-lk2∕η.
Proof. Since F(f, g, λ) is a concave function, we have
F(f*,g*, λ*) ≤ F(ft+1, gt, λt) + (VF(ft+1,gt, λt), (f*,g*, λ*) - (ft+1, gt, λt)),
which implies that
F(f t+1,gt, λt) ≤hg -g*,c - bi + hλt - λ*, -VλF(ft+1,gt, λt)i
≤(c∞ - ηι)kct - bk1 + c∞kct - bk1 + 7c2∞kλt+1 - yt+1k2∕η	(85)
+ 5(1 - θ)c2∞kλt - λt-1 k2∕η.
(84)
23
Under review as a conference paper at ICLR 2022
where in the first inequality we have used (35), and the second inequality follows from (50) and
setting λ = λ* in (78). From (85) We immediately get
E(ft+1, gt, λt, λt-1) = F(ft+1, gt, λt) + 2τkλt - λt-1k2
≤ c∞kλt - Xt-1k2/n + (2c∞ - ηI) kct - bk1 + 7c∞kλt+1 - yt+1k2/n + 5(1 - θ)c∞kλt - XtTk2/n
≤ 2c∞ llλt - λt-1k2/η + (2c∞ - ηι)kct - bk1 + 7c∞kλt+1 - yt+1k2/n + 5(1 - θ)c∞kλt - λt-1 k2/n,
Where the second inequality is due to kλt - λt-1 k2 ≤ 2. This completes the proof.
The following lemma bounds E(ft+1,gt+1, λt+1,λt) by O(1/t).
Lemma 20 Let {ft, gt, ytλt} be generated by PAME (Algorithm 3). The following inequality holds:
E^(ft+1 ,gt+1,λt+1,λt)
V__________6/mI)___________
` ., ~ 一
t +1 + 6/(nYilF(f 0,g0,λ0))
where we assume λ-1 = λ0, and
γ1 = min{(⅛7,
2(2θ — θ2)
(7 - 5θ)2c∞
(86)
is a constant.
Proof. Combining (42b) and Lemma 17, we have
E(ft+1,gt+1,λt+1,λt)-E(ft+1,gt,λt,λt-1)
=(E(ft+1, gt+1,λt+1, λt) - E(ft+1,gt+1, λt, λt-1)) + (F(ft+1, gt+1,λt) - F(ft+1,gt, λt))
≥ 2kct- bk1 + 2⅛θ2w- λt-1∣∣2 + 41τN+1-yt+1∣∣2,
which implies that
E(ft+1, gt+1,λt+1,λt) - E(ft+1,gt, λt, λt-1)
η	c2	c2
≤ - ηkct - bk2 - (2θ - θ2)-∞kλt - λt-1k2 -铲kλt+1 - yt+1k2
2	η	2η
≤ - 2γ1 h((2-∞ - ηI) ∣∣-t - b∣∣1)2 + ((7 - 5θ)-∞kλt - λt-1k2∕η)2 + (7-∞ kλt+1 - yt+1k2/n)2i
≤ - ηγ1 [(2-∞ - ηI) ll-t - bill + (7 - 5θ)-∞kλt - λt-1k2∕η + 7-∞kλt+1 - yt+1k2/n]2
≤ - ηY1E(ft+1,gt,λt,λt-1)2,
6
(87)
where the last inequality applies Lemma 19. We then divide both sides of (87) by
E(ft+1,gt+1,λt+1, λt) ∙ E(ft+1, gt, λt, λt-1), and we obtain
1	、	1	l η	E(ft+1,gt,λt,λt-1)
---------------------≥ -----------------十一Yi ∙----------------------
E(ft+1,gt+1,λt+1,λt) -E?(ft+1,gt,λt,λt-1)	6 ʃ	E(ft+1,gt+1,λt+1,λt)
≥ ；_______1________+ % ≥ ；_1________________+ 4,
一E(ft+1,gt,λt,λt-1)	67 - E(ft,gt,λt,λt-1)	C
where the second inequality holds because (87) implies that E(ft+1,gt,λt,λt-1) ≥
E(ft+1,gt+1,λt+1,λt), and the last inequality follows from (42a). Summing (88) from 0 to t
leads to
；_______1_______≥ ；_1____________+ gU Yi =」_ + η≡υ Yi
E(ft+1,gt+1,λt+1,λt) — E(f0,g0,λ0,λ-1)	6 / F(f0,g0,λ0)	6 /
24
Under review as a conference paper at ICLR 2022
which immediately leads to the desired result.

Similar to Lemma 15, the following lemma provides some sufficient conditions for the PAME algo-
rithm to return an -optimal solution to the original EOT problem (2).
Lemma 21 Assume PAME terminates at the T -iteration, i.e.,
kcT -1 - bk1 ≤ /(6(6c∞ - ηι),	(89a)
kλTT- λτ-2k2 ≤ ηe∕(60(1 - θ)c∞),	(89b)
l∣λτ - yτk2 ≤ ηe∕(42c∞),	(89c)
F(fT,gT-1,λτT) ≤ e∕6.	(89d)
Then the output	(π, λ)	of PAME (Algorithm 3),	i.e.,	πk	=
Round(πk(fτ, gτ-1,λτ-1),ak,bk), ∀k ∈ [N], λ = λτT, is an E-optimal solution of the
original EOT problem (2).
Proof. The proof is essentially the same as that of Lemma 15. More specifically, we again need to
show that the output of PAME (π, λ) satisfies (56). The proof of (56b) is exactly the same as the
proof of Lemma 15. The proof of (56a) only requires to develop a new bound for
”-λ, VλF(fτ,gτ-1,λτ-1)》	(90)
that is used in (61). Other parts are again exactly the same as the ones in Lemma 15. The new bound
of (90) can be obtained by applying Lemma 18 with λ = λ(∏) and t = T 一 1, which yields
hλ(∏)- λ, VλF(fτ,gτ-1,λτT)i
≤ c∞lcτ-1	-	bl1 + 5(1 -	θ)c2∞lλτ-1 -	λτ-2l2∕η +	7c2∞lλτ	- yτl2∕η.	(91)
By combining (91) with (60)-(63), we can bound the left hand side of (56a) by
' (∏, λ(∏)) — '(π, λ)
≤(6c∞-ηι)cτ-1-b1+5(1-θ)c2∞λτ-1-λτ-22∕η+7c2∞lλτ-yτl2∕η
+ ∣F(fτ,gτ-1,λτT)- F*∣	(92)
1111	1
≤ U+ 12 + 12 + 6； e =2e,
where in the last inequality we have used all the sufficient conditions (89a)-(89d).
Theorem 22 Define E0 = E∕(6c∞ - ηι), and set T to be
T=8+
48
η√γTe0
3600(1 - θ)2 + 882 c2∞
ηE2
S+券=O (Ci),
(93)
+
where γ1 is defined in (86) and we know γ1 = O(c-∞2). At least one of the iterations in Algorithm 3,
after rounding, is an E-saddle point of the EOT problem (2).
Proof. According to Lemma 21, we only need to show that (89) holds after T iterations as defined
in (93).
We follow the same idea as the proof of Theorem 16. First we reduce E(f t+1,gt+1, λt+1,λt) from
E(f0, g0, λ0, λ-1) = F(f0, g0, λ0) to a constant S by running tι steps. By Lemma 20, We have
6
tι ≤ 1 +------------
ηγ1S
6
~ , 〜 ~ 一
ηγιF(f 0,g0,λ0)
(94)
Secondly, starting from S, we continue running the algorithm, and assume that there are t2 iteration
in which (89a) fails. By (42b) we have
t2
72S
≤ 1 +	.
25
Under review as a conference paper at ICLR 2022
Therefore, we know that the total iteration number that (89a) fails can be upper bounded by
72s
TI = t1 + t2 — 2 +------02 +
ηe02
0
iterations. By choosing S = ^√γ^, we know that
6
6
—
ηγ1s
ηγιF(f 0,g0,λ0)
0
2+	12	+	36
T	2+ η√Y1e0 + η√Y1e0
T1 ―	2+	12	+	36
2 + η√Y1J + η√Y1e0
~	≤ 2 +
ηγ1F(f0 ,g0,λ0)—乙 +
——.............. —	2 +
ηγιF(f0 ,g0,λ0) — 2 +
48
η√γ1∈0
12
η√γ1e0
if F(f0,g0,λ0) ≥ 6√γ1,
otherwise.
—
—
6
Therefore, We have Ti — 2 + . 48V
η γ1
. Similarly, from Lemma 17 we know that, starting from s, the
number of iterations that (89b) and (89c) fail can be respectively bounded by
3600(1 - θ)2c2 s	3528c2 s
t3 — 1+ =DA [2∞ , and t4 — 1 + 一千∙
ηe2(2θ - θ2)	ηe2
By choosing s = e, we have the total iteration numbers that (89b) and (89c) fail can be respectively
bounded by
3600(1 - θ)2c2
TT= tι + t3 — 2+ ηe(∖θ -吟
6
+--------
ηγ1
——=--------—2 +
ηγιF'(f0,g0,λ0)-
3600(1 - θ)2c∞	6
ηe(2θ - θ2) 十 ηγιe
6
and
T3 = tι + t4 — 2+3528c∞
η
6
+ 一
ηγ1
——工--------—2 +
ηγιF(f0,g0,λ0)一
3528c∞	6
---------1-----.
η	ηγ1
—
6
Finally, by letting s = /6 in (94), we know that
E(f τ4-l,gτ4-l,λτ4-l,λτ4-2) — e/6
(95)
after
T4 = 1 +工
ηγ1
iterations. From (95) We knoW that
F(fτ4T,gτ4T,λτ4T) — e/6,
which implies that (89d) holds with T = T4 by noting (42a).
Combining the above discussions, we know that after T = T1 + T2 + T3 + T4 + 1 iterations, there
must exist at least one iteration such that the sufficient condition (89) holds, and thus the output of
PAME is an -optimal solution to the original EOT problem (2).

D Additional Numerical Results
D.1 Gaussian Distribution
Figure 2 shows the optimal couplings obtained from the standard OT and EOT of two Gaussian
distributions under three different metrics: the Euclidean cost (k ∙ k2), the square Euclidean cost
(k ∙ ∣∣2) and the L95 norm (k ∙ ||1.5) respectively. We set n = 4, η = 0.05 and generate samples
independently according to (31). For the EOT problem, we consider three agents with cost matrices
computed by the three metrics mentioned above. Note that the entropy regularized models lead to a
dense transportation plan and Figure 2 only plots the couplings with a probability larger than 10-3.
We see that all the agents have the same total cost in the EOT model, and as expected, the cost is
smaller than the other three OT costs obtained by using the same metric. The sub figures in the first
row imply that if we split the workload to three parts evenly, then the three agents will each have
costs 5.935/3, 2.158/3 and 5.030/3, which is not fair because they have different costs. But the
EOT model can indeed guarantee the fairness.
We further compare the computational time for Gaussian distributions. We generate the data as in
Section 5 and set the parameters as η = 0.5, τ = 5η∕c∞. We stop all the algorithms when the EOT
error (32) is less than 10-4. Tables 1 and 2 show the CPU time (in seconds) for different (n, N)
pairs. The reported computational time is averaged over 5 runs. In Table 1, the APGA algorithm
fails to reach an error of 10-4 in 500000 iterations when n = 100 and n = 500. We conclude
that the APGA algorithm converges much slower than PAM and PAME algorithms. The PAME
algorithm performs the best among all three algorithms.
26
Under review as a conference paper at ICLR 2022
Figure 2: Optimal couplings of standard OT (first row) and EOT (second row). OT Square Euclidean
Cost: 5.935; OT Euclidean Cost: 2.158; OT L11.5 Cost: 5.030; EOT Cost: 0.906.
Table 1: CPU time (in seconds) comparison for Gaussian Distributions. Fixed N = 3.
Algorithms	n= 10	n=20	n = 50	n=100	n=500
PAM	0.038283	0.096039	0.177209	1.038785	1.768560
PAME	0.025210	0.065552	0.091593	0.564618	1.340104
APGA	1.125673	12.364775	106.768840	-	-
D.2 Numerical Results on Fragmented Hypercube Dataset
In this section, we compare the performance of PAME with PAM and APGA (25) Scetbon et al.
(2021) on the fragmented hypercube dataset.
Fragmented Hypercube: We now consider transferring mass between a uniform distribution over
a hypercube μ = U([-1,1]d) and a distribution V obtained by a pushforward V = T]μ defined
by T(X) = X + 2sign(X) Θ (Pm=I em)
Here sign(∙) is taken elementwisely, m* ∈ [d] and
ei, i ∈ [d] is the canonical basis of Rd. In our experiments, We set d = 10, m* = 2 and sample
two base support sets {xbase}i∈[n], {yjas}j∙∈[n] independently from μ, V. To obtain the cost matrix
for one agent, we first add Gaussian noise sampled from N (0, 1) to the base support sets to get
{Xinoisy}i∈[n] , {yjnoisy}j ∈[n] and compute the cost using the noisy support sets. For instance, for
the k-th agent, we have (Xinoisy)k = Xibase + N (0, 1) , (yjnoisy)k = yjbase + N (0, 1) and Cik,j =
k(Xinoisy)k - (yjnoisy)kk22.
Figures 3 plots the EOT error versus the CPU time for Fragmented Hypercube dataset. We run PAM
for 20000 iterations to get an approximate optimal `* and run all algorithms for 2000 iterations
for different parameter settings. In all cases, the PAME and PAM perform significantly better than
APGA, and PAME also shows significant improvement over PAM.
We then compare the computational time for Fragmented Hypercube dataset. We set the parameters
as η = 0.2, τ = 5η∕c∞. We stop all the algorithms when the EOT error (32) is less than 10-4.
Tables 3 and 4 show the CPU time (averaged over 5 runs) for different (n, N) pairs. We see that the
PAME algorithm still performs the best among all three algorithms. Note that in Table 3 the APGA
algorithm fails to reach an error of 10-4 in 500000 iterations when n = 50, n = 100 and n = 500.
27
Under review as a conference paper at ICLR 2022
Table 2: CPU time (in seconds) comparison for Gaussian Distributions. Fixed n = 50.
Algorithms	N=2	N=3	N = 5	N=10	N=20
PAM	0.180343	0.177209	1.021598	0.719909	0.903429
PAME	0.105775	0.091593	0.560785	0.385495	0.618381
APGA	70.959300	106.768840	169.213121	178.637889	212.697866
Figure 3: CPU time comparison between PAM, PAME and APGA algorithms on the Fragmented
Hypercube dataset. Upper Left: N = 5, n = 100, η = 0.2, Upper Right: N = 5, n = 500, η =
0.2, Bottom Left: N = 5,n = 100, η = 0.1, Bottom Right: N = 10,n = 100, η = 0.2.
Table 3: CPU time (in seconds) comparison for Fragmented Hypercube. Fixed N = 3.
Algorithms n = 10 n = 20 n = 50 n = 100 n = 500
PAM	0.165165
PAME	0.112527
APGA	13.771911
0.101363	0.177209	1.154193	3.840804
0.068529	0.091593	0.588553	2.007653
22.017804	-	-	-
Table 4: CPU time (in seconds) comparison for Fragmented Hypercube. Fixed n = 20.
Algorithms	N=2	N = 3	N=5	N=10	N=20
PAM	0.003180	0.101363	0.172696	0.253926	0.231646
PAME	0.040302	0.068529	0.110080	0.150513	0.129156
APGA	1.007166	22.017804	13.801154	6.304324	3.749495
28