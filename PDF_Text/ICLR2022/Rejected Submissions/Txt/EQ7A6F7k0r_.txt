Under review as a conference paper at ICLR 2022
QTN-VQC: An End-to-End Learning frame-
work for Quantum Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
The advent of noisy intermediate-scale quantum (NISQ) computers raises a cru-
cial challenge to design quantum neural networks for fully quantum learning tasks.
To bridge the gap, this work proposes an end-to-end learning framework named
QTN-VQC, by introducing a trainable quantum tensor network (QTN) for quan-
tum embedding on a variational quantum circuit (VQC). The architecture of QTN
is composed of a parametric tensor-train network for feature extraction and a ten-
sor product encoding for quantum embedding. We highlight the QTN for quan-
tum embedding in terms of two perspectives: (1) we theoretically characterize
QTN by analyzing its representation power of input features; (2) QTN enables an
end-to-end parametric model pipeline, namely QTN-VQC, from the generation of
quantum embedding to the output measurement. Our experiments on the MNIST
dataset demonstrate the advantages of QTN for quantum embedding over other
quantum embedding approaches.
1	Introduction
The state-of-the-art machine learning (ML), particularly based on deep neural networks (DNN),
has enabled a wide spectrum of successful applications ranging from the everyday deployment of
speech recognition (Deng et al., 2013) and computer vision (Sermanet et al., 2014) through to the
frontier of scientific research in synthetic biology (Jumper et al., 2021). Despite rapid theoretical
and empirical progress in DNN based regression and classification (Goodfellow et al., 2016), DNN
training algorithms are computationally expensive for many new scientific applications, such as
new drug discovery (Smalley, 2017), which requires computational resources that are beyond the
computational limits of classical hardwares (Freedman, 2019). Fortunately, the imminent advent
of quantum computing devices opens up new possibilities of exploiting quantum machine learning
(QML) (Biamonte et al., 2017; Schuld et al., 2015; Schuld & Petruccione, 2018; Schuld & Killoran,
2019; Saggio et al., 2021; Dunjko, 2021) to improve the computational efficiency of ML algorithms
in the new scientific domains.
Although the exploitation of quantum computing devices to carry out QML is still in its initial
exploratory stages, the rapid development in quantum hardware has motivated advances in quantum
neural networks (QNN) to run in noisy intermediate-scale quantum (NISQ) devices (Preskill, 2018;
Huggins et al., 2019; Huang et al., 2021; Kandala et al., 2017). A NISQ device means that not
enough qubits could be spared for quantum error correction, and the imperfect qubits have to be
directly used at the physical layer. Even though, a compromised QNN approach is proposed by
employing hybrid quantum-classical models that rely on the optimization of variational quantum
circuits (VQC) (Benedetti et al., 2019; Mitarai et al., 2018). The resilience of the VQC based
models to certain types of quantum noise errors and high flexibility concerning coherence time and
gate requirements (McClean et al., 2018) admit many practical implementations of QNN on NISQ
devices (Chen et al., 2020b; Yang et al., 2021; Du et al., 2020; 2021; Skolik et al., 2021; Dunjko
et al., 2016; Jerbi et al., 2021; Ostaszewski et al., 2021). One notable limitation in the current QNN
training pipeline is that the quantum embedding is not fully realizable in a quantum computer, which
may impede the learning of the QNN. Hence, this work proposes QTN-VQC to enable an end-to-
end trainable QNN, including data embedding to quantum measurements, that are easily realizable
in quantum devices, where QTN stands for the quantum tensor network (Orus, 2019; HUckle et al.,
2013; Biamonte et al., 2017; Murg et al., 2010) for generating quantum embedding.
1
Under review as a conference paper at ICLR 2022
Classical data X
10〉
10〉
10〉
>
›hθ 1 ψX〉= 1 gθ(x))
*E3zι
*Ez2
*d I ：
叱三IzS
Quantum Embedding Generation
Hx
Variational Quantum Circuit Measurement
Hθ	ɪ( I gθ (x)))
Figure 1: An illustration of QNN based on VQC.
10〉
10〉
I 0〉
InPUt y J
Dense
-jzt4x
Input y (->
-TTTN 九x
(a) A dense layer for dimension reduction
(b) A Tensor-Train layer for dimension reduction
Figure 2: Different paradigms for quantum embedding. (a) a dense layer is used to generate low-
dimensional vector x from a high-dimensional one y; (b) a TTN is usedfor dimension reduction.

Λ
：UX10 声 S = | ψX
√
√
As shown in Figure 1, our QNN builds a unitary linear operator that consists of three main Com-
ponents: (1) quantum embedding generation; (2) variational quantum circuit; (3) measurement.
Quantum embedding generation, also known as quantum encoding, applies a fixed unitary linear
operator Hx transforming classical vectors X to quantum states ∣ψχi in a Hilbert space. This step
is an important aspect of designing quantum algorithms that directly impact the entire computation
cost of VQC and owns a characteristic of quantum superposition. Moreover, the VQC comprises
two types of quantum gates: (1) Controlled-NOT (CNOT) gates; (2) learnable parametric quantum
gates. The CNOT gates ensure the property of quantum entanglement through mutually connecting
the qubits, and the parametric quantum gates can be adjustable to best fit the quantum input states.
The model parameters of VQC should be optimized by employing variants of gradient descent al-
gorithms during the training process. Those parametric quantum gates of VQC are similar to the
weights assigned to DNN, and such quantum circuits have been justified to be resilient to quantum
noises (Farhi et al., 2014; Kandala et al., 2017; McClean et al., 2016). Besides, the measurement
M(∣gθ(x)i) aims at projecting the quantum output states ∣gθ(x)i to one classical output z%.
This work focuses on quantum embedding generation because it is quite related to the practical usage
in machine learning applications in terms of computational cost and representation capability of
classical input features. In particular, we design a novel quantum tensor network (QTN) for quantum
embedding generation. More specifically, the QTN consists of a tensor-train network (TTN) for
dimension reduction anda quantum tensor encoding framework for outputting quantum embeddings.
The dimension reduction is a necessary procedure before the quantum encoding because only a
small number of qubits could be supported on available NISQ computers at this moment. A typical
approach for dimension reduction relies on a classical fully-connected layer, also known as a dense
layer, to convert high-dimensional input vectors y into low-dimensional ones x. However, since a
dense layer cannot be physically mapped on a quantum computer, much overhead has to be incurred
by frequently communicating between classical and quantum devices during the end-to-end training
pipeline.
2
Under review as a conference paper at ICLR 2022
As shown in Figure 2 (b), one of our contribution is to leverage a tensor train network (TTN)
to replace the dense layer in Figure 2 (a). The benefits of applying TTN arise from two aspects:
(1) TTN can maintain the representation power of the dense layer, which will be justified in our
theorems; (2) TTN is a tensor network and can be flexibly placed in quantum computers, which
enables an end-to-end training process fully conducted in a quantum computer. Moreover, in this
work, a tensor product encoding (TPE) is delicately designed for generating quantum embedding,
which builds the relationship between a classical vector x and the corresponding quantum state |xi;
Besides, we further investigate the representation of QTN-VQC in terms of model size and non-
linear activation function used in TTN. We denote a QTN as the combination of TTN and TPE and
utilize QTN-VQC as a genuine end-to-end learning framework for QNN.
2	Related Work
The work (Schuld & Petruccione, 2018; Biamonte et al., 2017; Dunjko & Briegel, 2018) demonstrate
that VQC shows great promise in surpassing the performance of classical ML. Prominent examples
of VQC based models include quantum approximate optimization algorithm (QAOA) (Farhi et al.,
2014), and quantum circuit learning (QCL) (Mitarai et al., 2018). Various architectures and geome-
tries of VQC have been shown in tasks ranging from image classification (Henderson et al., 2020;
Chen et al., 2020a; Kerenidis et al., 2020) to reinforcement learning (Chen et al., 2020b).
As for quantum embedding, basis encoding is the process of associating classical input data in the
form of binary strings with the computational basis state of a quantum system (Leymann & Barzen,
2020). Similarly, amplitude encoding is a technique involving encoding data into the amplitudes of
a quantum state (Soklakov & Schack, 2006). Unfortunately, the computational cost of both quantum
embedding and amplitude encoding becomes exponentially expensive with the increasing number of
qubits (Schuld & Killoran, 2019). A new technique of angle embedding makes use of the quantum
gates to generate quantum states (Fu et al., 2011), but it cannot deal with the high-dimensional
feature inputs. Therefore, this work exploits the use of TTN for dimension reduction followed by a
TPE for generating quantum embedding.
In particular, this work employs the TTN for dimensionality reduction. The TTN model based
on TT decomposition in neural networks was first proposed in (Oseledets, 2011), and it could be
flexibly extended the convolutional neural network (CNN) (Garipov et al., 2016) and recurrent neural
network (RNN) (Tjandra et al., 2017). The empirical study of TTN on machine learning tasks
shows that TTN is capable of maintaining the DNN baseline results (Qi et al., 2020a; Yu et al.,
2017; Yang et al., 2017; Jin et al., 2020). However, to our best knowledge, no existing works have
applied TTN to QML. Besides, since the tensor network-based machine learning model like TTN is
closely related to quantum machine learning in terms of their model structures (Liu & Wang, 2018;
Gao et al., 2017), the QTN-VQC model can be directly regarded as the classical simulation of the
corresponding quantum machine learning. In addition to a classical dense layer, more complicated
architectures like AlexNet (Lloyd et al., 2020) could be used for dimension reduction, and we also
compare the performance between TTN and AlexNet-based models.
3	Notations
We denote RI as a I-dimensional real coordinate space, and RI1 ×12×1K refers to a space of K-
order tensors. The symbol W ∈ RI1 ×I2 ×∙∙∙×IK represents a K-order multi-dimensional tensor in
RI1 × I2 × IK, and the symbols V ∈ RI and W ∈ RI × J represent a vector and a matrix, respectively.
For the notations of quantum computing, ∀ v ∈ RI, the symbol |vi denotes a quantum state associ-
ated with a 2I -dimensional vector in a Hilbert space. Particularly, |0i = [1 0]T and |1i = [0 1]T.
The quantum gate RY (θ) means a Pauli-Y gate with a unitary operator as defined in Eq. (1), which
implies a qubit rotates the Bloch sphere along the Y-axis by a given angle θ.
RY (θ)
θ
CoS θ
2θ
一i Sm 2
θ
一i Sin ⅞
θ2
CoS 2
(1)
Moreover, the operator 0 is a tensor product. Given the vectors Vi ∈ RI, the tensor product of I
vectors is defined as 0I=1Vi, which is a 2I-dimensional vector and can provide a compact represen-
3
Under review as a conference paper at ICLR 2022
I1	I2	I3	IK-2	IK-1	IK
(a) Tensor-Train Network: given a set of TT-ranks {r卜 r2,..., rκ}, a circle
represents a core tensor and each line is related to a dimension.
(b) Tensor Product Encoding: Rγ( πx) denotes the PaUIi-Y rotation gate
Figure 3: A demonstration of quantum tensor network for quantum embedding.
tation for vι 0 v 名一 ∙名 VI. Similarly, the symbol |0i0S means a tensor product of S quantum
states of |0). Furthermore, for a scalar v, the quantum state |v〉can be written as:
|vi = cos v|0)+ Sin v|1)
CoS V
sin v
(2)
4	QTN-VQC: Our Proposed End-to-End Learning Framework
This section introduces our proposed end-to-end learning framework, namely QTN-VQC in this
work. As shown in Figure 3, the QTN model includes two components (a) TTN and (b) TPE, which
will be separately introduced in Section 4.1 and Section 4.2. Moreover, Figure 4 illustrates the
framework of VQC and Section 4.3 is devoted to discussing the details of VQC.
4.1	Tensor Train Network for Dimension Reduction
We leverage TTN (Novikov et al., 2015) for the dimension reduction of input features. TTN re-
lies on the TT decomposition (Oseledets, 2011) and has been commonly employed in machine
learning tasks like speech processing (Qi et al., 2020b) and computer vision (Yang et al., 2017).
The TT decomposition assumes that given a set of TT-ranks {r0, r1, ..., rK}, a K-order tensor
W ∈ RI1×I2× ×IK is factorized into the multiplication of 3-order tensors Xk ∈ Rrk-1×Ik×rk.
In more detail, given a set of indices {i1, i2, ..., iK}, X(i1, i2, ..., iK) is decomposed as:
K
X(i1, i2, ..., iK) = Y Xk(ik),	(3)
k=1
where ∀ik ∈ [Ik], Xk(ik) ∈ Rrk-1 ×rk. Since r0 = r1 = 1, the term QkK=1 Xk(ik) is a scalar value.
TTN employs the TT decomposition in a dense layer and is explicitly demonstrated in Figure 3 (a).
I	]R*I1 × I2 × × IK	Ji × J2 × × JK
In more detail, for an input tensor X ∈ R	and an output tensor Y ∈ R	,
we achieve
I1 I2	IK
Y(j1,j2,…,jK) = XX ,- X W ((i1,j1), (i2,j2), ..., (iK,jK)) XHl,i2,…,iK)
i1=1 i2=1	iK=1
I1 I2	IK	K	K
=XX XYWk(ik,jk) 1 ∙ Y Xk(ik)
i1=1 i2=1	iK=1	k=1	k=1
K	Ik
=	Wk(ik, jk)Xk(ik)
k=1 ik=1
K
= Y Yk (jk ),
k=1
(4)
4
Under review as a conference paper at ICLR 2022
where Xk(ik) ∈ Rrk-1×rk, and Yk(jk) ∈ Rrk-1×rk which results in a scalar QkK=1 Yk(jk) be-
cause of the ranks r0 = r1 = 1; W((i1, j1), (i2, j2), ..., (iK,jK)) is closely associated with
W(m1, m2, ..., mK) as defined in Eq. (3), if each index mk = ik × jk is set. The multi-dimensional
tensor W is decomposed into the multiplication of 4-order tensors Wk ∈ Rrk-1 ×Ik ×Jk ×rk. A non-
linear activation function, e.g., Sigmoid, Tanh, and ReLU, is imposed upon the tensor Y. Compared
with a dense layer with QkK=1 IkJk parameters, a TTN owns as few as PkK=1 rkrk-1IkJk trainable
parameters.
When a TTN is utilized for the dimension reduction, the high-dimensional input vector x ∈ RI is
first reshaped into a tensor X ∈ RI1 × I2 × × IK, and then We can represent X as a TT format that
goes through TTN. The outputs of TTN can be converted back to a tensor Y ∈ RJ1× J2× × JK,
Which is further reshaped to a loWer dimensional vector y ∈ RJ. Here, We define QkK=1 Ik = I and
QkK=1 Jk = J. Moreover, the computational complexities of TTN and the related dense layer are in
the same scale, Which is discussed in (Yang et al., 2017).
Eq. (4) suggests that TTN is a multi-dimensional extension of a dense layer, Where the trainable
Weight matrix ofa dense layer is changed to the learnable core tensors. Additionally, many empirical
studies demonstrate that a TTN is capable of maintaining the baseline results of the dense layer (Qi
et al., 2020b; Yang et al., 2017; Novikov et al., 2015; Qi et al., 2020a). More significantly, since
TTN can be flexibly mapped into a quantum circuit, the quantumness inherent in TTN brings great
advantages over other architectures like the dense layer. In other Words, although TTN is treated
classically, itis possible to substitute equivalent quantum circuits for TTN When more qubits become
available (Du et al., 2020), Which implies that QTN-VQC stands for a genuine end-to-end QNN
learning architecture on a quantum computer.
Furthermore, since the gradient exploding and diminishing problems are serious issues in the TTN
training. To avoid those training problems, We only consider 3-order core tensors and small TT-ranks
to configure a simple TTN in our experimental simulations. Our theoretical analysis of QTN-VQC
based on Theorem 3 in Section 5 suggests that the representation poWer is not related to TT-ranks
and the tensor order K, thus small TT-ranks and the tensor order K are preferred. In particular, a
loWer K can significantly reduce the computational cost and speed up the convergence rate.
4.2 Tensor Product Encoding
In this subsection, We first introduce Theorem 1, and then We derive our TPE associated With the
circuits in Figure 3 (b).
Theorem 1. Given the classical vector x = [x1, x2, ..., xI]T ∈ RI, a TPE as shown in Figure 3 (b)
can result in a quantum state |xi with the following complete vector representation as:
I	a I CoS xι CoS X2	CoS xι
(渴 =IRY(2Xi)) 10产=Rinx；J 函—χ2]函…函—,I] = 1XXi	⑸
Proof. Since each element xi in the vector x can be Written as |xii = CoS xi |0i + Sinxi|1i, the
quantum state |xi can be Written as:
CoS x1	CoS x2	CoS xI
|xi =	.	1 ③.2 ③一③.i .	(6)
Sin x1	Sin x2	Sin xI
When the vector x goes through the quantum tensor netWork, Which implies the folloWing as:
RY(2xi)|0i = cosxi|0i + sinxi|1i = |xii.	(7)
The preceding equation, in turn, implies that Eq. (5).	□
Theorem 1 builds a connection betWeen the vector x and the quantum state |xi, and the resulting
|xi is taken as the quantum embedding as the inputs to VQC. Since ③I=IRY(2xi) is a reversely
unitary linear operator, there is no information loss incurred during the stage of quantum encoding.
Furthermore, if the input is multiplied with a constant 2, we obtain the following term as:
自I RV^-))∣ox0I —	kos(πxn]农	rcos(πχ2)ι	》...农	kos(πχI)1	(8)
咯=IRY(nxi〃 ∣0i =	^sin(∏χι)J ⑤[sin(∏χ2)J	⑤	⑤[sin(πxI)]	,	(8)
which corresponds to Figure 3 (b).
5
Under review as a conference paper at ICLR 2022
Rγ (β)=
10〉
10〉
10〉
10〉
(a) VariationaI quantum circuit: the dashed square indicates repeated model with CNOT gates for entangling
quantum states, and RX(α), Rγ(β), RZ(γ) represent Pauli-X, Y Z gates with free parameters α, β, γ
β . β
cos- -sinɪ
. β	β
Sin - cos -
Pauli-Y
C	10 0 0
= 0100
= 0 0 0 1
---①	|_0 0 1 0
CNOT gate
Rχ(α)=
cos a	一 i sin a
2	2
-i sin a	cos a
2	2 .
ex
RZ (Y)=	0
0
exp(iY
Pauli-X
Pauli-Z
(b) Matrix representation for the Quantum gates applied in the VQC
Figure 4: Aframework of variational quantum circuit.
4.3 The Framework of Variational Quantum Circuit
The framework of VQC is shown in Figure 4 (a), where 4 qubit wires are taken into account, and
the CNOT gates aim at mutually entangling the channels such that |x1i, |x2i, |x3i and |x4i lie
in the same entanglement state. The Pauli-X, Y, Z gates RX(∙), RY(∙) and RZ(∙) with learnable
parameters (α1, β1, γ1), (α2, β2, γ2), (α3, β3, γ3), (α4, β4, γ4) are built to set up the learnable part.
Being similar to the unitary operators of RY (α), RX(β) and RZ(γ), which are defined in Figure 4
(b), are separately associated with the rotations along X-axis and Z-axis by the given angles of β and
γ. Besides, the quantum circuits in the dash square can be repeatedly copied to compose a deeper
architecture. The outputs of VQC are connected to the measurement which projects the quantum
states into a certain quantum basis that becomes a classical scalar zi.
As for the end-to-end training paradigm for QTN-VQC, the learnable parameters come from the
VQC and TTN models, and they should be updated by applying the back-propagation algorithm
based on the Adam optimizer. Given D qubits and H depths, there are totally 3DH trainable
parameters for VQC. Consequently, there are PkK=1 rk-1rkIkJk + 3DH parameters for QTN-
VQC. On the other hand, the Dense-VQC model possesses more model parameters than QTN-VQC
(QkK=1 rk-1rkIkJk + 3DH vs. PkK=1 rk-1rkIkJk + 3DH).
5	Characterizing Representation Power of QTN-VQC
This section focuses on analyzing the representation power of QTN-VQC. As shown in Figure 5,
given d qubits and a target quantum state |z)=因D=ι %)，since He is known as a linear operator and
Tx is defined as a definite mapping from input x to the unitary matrix Ux , the representation power
of QTN-VQC is determined by how TTN can approximate the classical vector TxT(H-1∣zi). To
understand the expressiveness of TTN, we first start with the discussion on the expressive capability
of Dense-VQC (a dense layer is taken for dimension reduction) and then generalize it to QTN-VQC.
Based on the universal approximation theorem (Cybenko, 1989; Barron, 1994) for a feed-forward
neural network, we derive the following theorem as:
Theorem 2.	Given a target vector Tx-1 (Hθ-1 |zi), there exists a feed-forward neural network fdense
with a dense layer connecting to D qubits, then
C
||T-1(Hθ1∣ Zi)- fdense Cy) ∣∣ι ≤ √√^,	(9)
where the activation function tanh(∙) is imposed upon the dense layer, and C is a constant associ-
ated with the target vector Tx-1(Hθ-1 |zi).
6
Under review as a conference paper at ICLR 2022
Figure 5: An illustration ofanalyzing the representation PowerofQTN-VQC.
Since TTN is a compact TT representation of a dense layer, by modifying Theorem 2 for TTN, We
can also derive the upper bound on the approximation error as follows:
Theorem 3.	Given a target vector T-1(H-I |z)), there exists a TTN, denoted as fττN, with a TT
layer connecting to D qubits, then
KC
11TX (HeIZi) - fTTNCy) ||1 ≤ ɪɪ √pj=,	(10)
k=1 Dk
where QkK=1 Dk = D, the Sigmoid activation function is imposed upon the TTN model, K denotes
the multi-dimensional order, C is a ConStant associated with the target vector T-1(H-1∣z)).
Comparing the two upper bounds, it is observed that TTN can attain an identical upper bound as
the dense layer on the approximation error because QkK=1 Dk = D. That implies that TTN can
at least maintain the representation power of a dense layer. Besides, the number of qubits D is a
key factor determining the upper bound on the approximation error. However, D is a small fixed
number on a NISQ device, and a larger number of qubits D is expected to further improve the
representation power of QTN-VQC. However, the computational costs of classical simulation may
grow exponentially with the increasing number of qubits, and a small number of qubits have to be
considered in practice.
6	Experiments and Results
6.1	Experimental setups
We assess our QTN-VQC based end-to-end learning system on the standard MNIST. MNIST is a
dataset for the task of 10 digit classification, where there are 50000 and 10000 28 × 28 image data
assigned for training and testing, respectively. The full MNIST dataset is challenging for quantum
machine learning algorithms, and many works only consider 2-digit classification on the MNIST
task (Wang et al., 2021; Chen et al., 2020a). Moreover, the image data are separately reshaped
into 784 dimensional input vectors. Dense-VQC and PCA-VQC are taken as our experimental
baselines to compare with our QTN-VQC model. Dense-VQC denotes that a dense layer is used for
dimension reduction, and PCA-VQC refers to using principal component analysis (PCA) to extract
low-dimensional features before training the VQC parameters.
As for the experiments of QTN-VQC, the image data are reshaped into 3-order 7 × 16 × 7 tensors.
We set small TT-ranks as {1, 2, 2, 1} to reduce the computational cost of TTN. the image data are
represented as the TT format according to Eq. (3) before going through the TTN model. Since 8
qubits are used for the quantum encoding, the output of TTN needs to configure the tensor format
as 2 × 2 × 2, which results in 8 dimensional output vectors. Besides, the model parameters of
QTN-VQC are randomly initialized based on the Gaussian distribution, and the back-propagation
algorithm is applied to train the models. The Sigmoid function is utilized for the hidden layers of
TTN.
To be consistent with QTN-VQC, the weight of the dense layer for Dense-VQC is configured as the
shape of 784 × 8. Although Dense-VQC is a hybrid classical-quantum model, the training process of
Dense-VQC can also be set as an end-to-end pipeline and the weights of the dense layer are updated
during the training stage. The Sigmoid function is used for the dense layer. On the other hand, PCA
7
Under review as a conference paper at ICLR 2022
is employed to reduce the feature dimension to 8, and the resulting low-dimensional features are
further encoded into quantum states. Consequently, PCA-VQC admits the VQC parameters solely
to be updated during the training stage. A standard AlexNet (Iandola et al., 2016) is employed to
constitute an AlexNet-VQC to compare the performance.
Moreover, 6 VQC layers are constructed to form a deep model, and the outputs of the VQC model
are connected to 10 classes with a non-trainable matrix. The back-propagation algorithm based on
the Adam optimizer with a learning rate of 0.001 is employed for the model training. The loss of
cross-entropy (CE) is utilized as the objective function during the training stage, and it is also taken
as the metric to evaluate the model performance. We leverage the tools of Pennylane (Bergholm
et al., 2018) and PyTorch (Paszke et al., 2019) to simulate the model performance. In particular,
we separately simulate the model performance with noiseless quantum circuits and noisy quantum
circuits corrupted by quantum noises from IBM quantum machines.
6.2	Experimental Results of Noiseless Quantum Circuit
Table 1 shows the final results of the models on the test dataset. QTN-VQC owns much fewer model
parameters than Dense-VQC (328 vs. 6416) and attains even higher classification accuracy than
Dense-VQC (91.43% vs. 88.54%) and lower loss values than Dense-VQC (0.3090 vs. 0.4132).
However, PCA-VQC with 144 trainable VQC parameters attains the worst performance by all met-
rics, which implies that a trainable quantum embedding is of significance to boost experimental
performance. Although our empirical results cannot reach the state-of-the-art classification perfor-
mance of classical ML algorithms, our empirical results demonstrate the advantages of QTN-VQC
over the PCA-VQC and Dense-VQC counterparts. With the development of more powerful quan-
tum devices supporting more qubits, the representation power of QTN-VQC can be improved and
better experimental results could be attained. Moreover, AlexNet-VQC achieves better results than
QTN-VQC (92.81%vs.91.43%), but it involves more model parameters than QTN-VQC.
Table 1: Empirical results on the MNIST test dataset under the noiseless quantum circuit setting.
Models	Params	CE	Acc (%).
-PCA-VQC-	144	0.5877	82.48 ± 1.02
Dense-VQC	6416	0.4132	88.54 ± 0.73
AlexNet-VQC	3.25 X 106	0.2562	92.81 ± 0.47
QTN-VQC~~	328	0.3090	91.43 ± 0.51一
6.3	Experimental Results of Noisy Quantum Circuit
To empirically validate the effectiveness of our proposed VQC algorithm, we proceed with the
simulation of the practical experiments with noisy quantum circuits. More specifically, we follow
an established noisy circuit experiment with the NISQ device suggested by (Chen et al., 2020b). One
major advantage of the setups is to observe the robustness and preserve the quantum advantages of
a deployed VQC with physical settings being close to quantum processing unit (QPU) experiments
without an executive queuing time. As for the detailed setup, we first use an IBM Q 20-qubit
machine to collect channel noise in the real scenario for a deployed VQC and upload the machine
noise into our Pennylane-Qiskit simulator (denoted as Accq20 . We provide a depolarizing noisy
circuit simulation (denoted as Accdepo) based on a depolarizing channel attained from (Nielsen &
Chuang, 2010) with a noise level of 0.1. As shown in Table 2, the quantum noise brings about
the performance degradation of all models, but our proposed QTN-VQC consistently outperforms
PCA-VQC and Dense-VQC in the condition of noisy quantum circuits. In particular, QTN-VQC
can even outperform the AlexNet-VQC counterpart in noisy circuit conditions.
6.4	Further Discussions
The above experimental results show the advantages of QTN-VQC over Dense-VQC and PCA-
VQC in the scenarios with noiseless and noisy quantum circuits. Next, we will further discuss the
representation power of QTN-VQC based on two factors: (1) the activation function used in TTN;
(2) the number of qubits.
8
Under review as a conference paper at ICLR 2022
Table 2: Empirical results on the MNIST test dataset under the noisy quantum circuit setting.
Models	Params	AcCq20 (%)	Accdepo (%)
-PCA-VQC-	144	81.23 ± 1.34	83.12 ± 1.17
Dense-VQC	6416	84.55 ± 1.22	86.09 ± 1.04
AlexNet-VQC	3.25 X 106	87.46 ± 1.34	87.86 ± 1.08
QTN-VQC~~	328	88.12 ± 1.09一	89.32 ± 1.07一
6.4.1	The activation function used in TTN
Table 3 compares the results of QTN-VQC based on different activation functions. Our simula-
tion on noiseless quantum circuits shows that the non-linear activation functions can bring more
performance gain than a linear one, but the Sigmoid function attains a better performance than the
Tanh and ReLU counterparts in our experiments. Our experiments also correspond to the universal
approximation theory for QTN-VQC in Theorem 3.
Table 3: Comparing performance of QTN-VQC with and without activation function.
Models	CE	Acc (%).
QTN-VQC (Linear)	0.4958	86.16 ± 0.65
-QTN-VQC (Tanh)-	0.4792	87.12 ± 0.51
QTN-VQC (ReLU)	0.3764	89.56 ± 0.49
QTN-VQC (Sigmoid)	0.3090	91.43 ± 0.54一
6.4.2	The number of qubits
Finally, we investigate the effects of the number of qubits on the performance of QTN-VQC by
increasing the qubits from 8 to 12 and 16. Accordingly, the output of TTN is configured as a
tensor format of 2 × 3 × 2, and the model size is increased from 328 to 464 and 600, respectively.
Our experiments show that the baseline performance of QTN-VQC can be further improved by
increasing the number of qubits, which implies that more qubits are likely to possess higher accuracy.
Table 4: Comparing performance of QTN-VQC with fewer qubits.
Models	Params	CE	Acc (%)
QTN-VQC (8 qubits)	-328-	0.3090	91.43 ± 0.51
QTN-VQC (12 qubits)	-464-	0.2679	92.36 ± 0.62
QTN-VQC (16 qubits)	600	0.2355	92.98 ± 0.52一
7	Conclusions
This work proposes a genuine end-to-end learning framework for quantum neural networks based on
QTN-VQC. QTN consists of a TTN for dimension reduction and a TPE framework for generating
quantum embedding. The TTN model is a compact representation of a dense layer to classically
simulate quantum machine learning algorithms. Our theorem on the representation of QTN-VQC
shows that the number of qubits is inversely related to the approximation error of QTN-VQC and the
non-linear activation plays an important role. Our experiments compare our proposed QTN-VQC
with Res-VQC, Dense-VQC, and PCA-VQC. Our simulated results demonstrate that QTN-VQC
obtains better experimental performance than Dense-VQC and PCA-VQC with both noiseless and
noisy quantum circuits, and it achieves marginally worse performance than AlexNet-VQC. Besides,
our results justify our theorem on the representation power of QTN-VQC.
9
Under review as a conference paper at ICLR 2022
References
Andrew R Barron. Approximation and Estimation Bounds for Artificial Neural Networks. Machine
Learning ,14(1):115-133,1994.
Marcello Benedetti, Erika Lloyd, Stefan Sack, and Mattia Fiorentini. Parameterized Quantum Cir-
cuits as Machine Learning Models. Quantum Science and Technology, 4(4):043001, 2019.
Ville Bergholm, Josh Izaac, Maria Schuld, Christian Gogolin, M Sohaib Alam, Shahnawaz
Ahmed, Juan Miguel Arrazola, Carsten Blank, Alain Delgado, Soran Jahangiri, et al. Pen-
nylane: Automatic Differentiation of Hybrid Quantum-Classical Computations. arXiv preprint
arXiv:1811.04968, 2018.
Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth Lloyd.
Quantum Machine Learning. Nature, 549(7671):195-202, 2017.
Samuel Yen-Chi Chen, Chih-Min Huang, Chia-Wei Hsing, and Ying-Jer Kao. Hybrid Quantum-
Classical Classifier Based on Tensor Network and Variational Quantum Circuit. arXiv preprint
arXiv:2011.14651, 2020a.
Samuel Yen-Chi Chen, Chao-Han Huck Yang, Jun Qi, Pin-Yu Chen, Xiaoli Ma, and Hsi-Sheng
Goan. Variational Quantum Circuits for Deep Reinforcement Learning. IEEE Access, 8:141007-
141024, 2020b.
George Cybenko. Approximation by Superpositions of A Sigmoidal Function. Mathematics of
Control, Signals and Systems, 2(4):303-314, 1989.
Li Deng, Jinyu Li, Jui-Ting Huang, Kaisheng Yao, Dong Yu, Frank Seide, Michael Seltzer, Geoff
Zweig, Xiaodong He, Jason Williams, et al. Recent Advances in Deep Learning for Speech
Research at Microsoft. In Proc. IEEE International Conference on Acoustics, Speech and Signal
Processing, pp. 8604-8608, 2013.
Yuxuan Du, Min-Hsiu Hsieh, Tongliang Liu, and Dacheng Tao. Expressive Power of Parametrized
Quantum Circuits. Physical Review Research, 2(3):033125, 2020.
Yuxuan Du, Min-Hsiu Hsieh, Tongliang Liu, Dacheng Tao, and Nana Liu. Quantum Noise Protects
Quantum Classifiers Against Adversaries. Physical Review Research, 3(2):023153, 2021.
Vedran Dunjko. Inside Quantum Black Boxes. Nature Physics, pp. 1-2, 2021.
Vedran Dunjko and Hans J Briegel. Machine Learning & Artificial Intelligence in the Quantum
Domain: A Review of Recent Progress. Reports on Progress in Physics, 81(7):074001, 2018.
Vedran Dunjko, Jacob M Taylor, and Hans J Briegel. Quantum-Enhanced Machine Learning. Phys-
ical Review Letters, 117(13):130501, 2016.
Edward Farhi, Jeffrey Goldstone, and Sam Gutmann. A Quantum Approximate Optimization Algo-
rithm. arXiv preprint arXiv:1411.4028, 2014.
David H Freedman. Hunting for New Drugs with AI. Nature, 576(7787):S49-S53, 2019.
Yangguang Fu, Mingyue Ding, and Chengping Zhou. Phase Angle-Encoded and Quantum-Behaved
Particle Swarm Optimization Applied to Three-Dimensional Route Planning for UAV. IEEE
Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans, 42(2):511-526,
2011.
Xun Gao, Zhengyu Zhang, and Luming Duan. An Efficient Quantum Algorithm for Generative
Machine Learning. arXiv preprint arXiv:1711.02038, 2017.
Timur Garipov, Dmitry Podoprikhin, Alexander Novikov, and Dmitry Vetrov. Ultimate Tensoriza-
tion: Compressing Convolutional and FC Layers Alike. arXiv preprint arXiv:1611.03214, 2016.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep Learning, volume 1.
MIT Press, 2016.
10
Under review as a conference paper at ICLR 2022
Maxwell Henderson, Samriddhi Shakya, Shashindra Pradhan, and Tristan Cook. Quanvolutional
Neural Networks: Powering Image Recognition with Quantum Circuits. Quantum Machine In-
telligence, 2(1):1-9, 2020.
Hsin-Yuan Huang, Michael Broughton, Masoud Mohseni, Ryan Babbush, Sergio Boixo, Hartmut
Neven, and Jarrod R McClean. Power of Data in Quantum Machine Learning. Nature Communi-
cations, 12(1):1-9, 2021.
Thomas Huckle, Konrad Waldherr, and Thomas Schulte-HerbrUggen. Computations in Quantum
Tensor Networks. Linear Algebra and its Applications, 438(2):750-781, 2013.
William Huggins, Piyush Patil, Bradley Mitchell, K Birgitta Whaley, and E Miles Stoudenmire.
Towards Quantum Machine Learning with Tensor Networks. Quantum Science and Technology,
4(2):024001, 2019.
Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt
Keutzer. SqueezeNet: AIexNet-level Accuracy with 50x Fewer Parameters andj 0.5 MB Model
Size. arXiv preprint arXiv:1602.07360, 2016.
Sofiene Jerbi, Casper Gyurik, Simon Marshall, Hans J Briegel, and Vedran Dunjko. Variational
Quantum Policies for Reinforcement Learning. arXiv preprint arXiv:2103.05577, 2021.
Xuanyu Jin, Jiajia Tang, Xianghao Kong, Yong Peng, Jianting Cao, Qibin Zhao, and Wanzeng Kong.
CTNN: A Convolutional Tensor-Train Neural Network for Multi-Task Brainprint Recognition.
Proc. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 29:103-112, 2020.
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,
Kathryn Tunyasuvunakool, RUSS Bates, AUgUStm Zidek, Anna Potapenko, et al. Highly Accurate
Protein Structure Prediction with AlphaFold. Nature, 596(7873):583-589, 2021.
Abhinav Kandala, Antonio Mezzacapo, Kristan Temme, Maika Takita, Markus Brink, Jerry M
Chow, and Jay M Gambetta. Hardware-Efficient Variational Quantum Eigensolver for Small
Molecules and Quantum Magnets. Nature, 549(7671):242-246, 2017.
Iordanis Kerenidis, Jonas Landman, and Anupam Prakash. Quantum Algorithms for Deep Convo-
lutional Neural Networks. In Proc. International Conference on Learning Representations, 2020.
Frank Leymann and Johanna Barzen. The Bitter Truth About Gate-Based Quantum Algorithms In
the NISQ Era. Quantum Science and Technology, 5(4):044007, 2020.
Jin-Guo Liu and Lei Wang. Differentiable Learning of Quantum Circuit Born Machines. Physical
Review A, 98(6):062324, 2018.
Seth Lloyd, Maria Schuld, Aroosa Ijaz, Josh Izaac, and Nathan Killoran. Quantum Embeddings for
Machine Learning. arXiv preprint arXiv:2001.03622, 2020.
Jarrod R McClean, Jonathan Romero, Ryan Babbush, and Alan Aspuru-Guzik. The Theory of
Variational Hybrid Quantum-Classical Algorithms. New Journal of Physics, 18(2):023023, 2016.
Jarrod R McClean, Sergio Boixo, Vadim N Smelyanskiy, Ryan Babbush, and Hartmut Neven. Bar-
ren Plateaus in Quantum Neural Network Training Landscapes. Nature Communications, 9(1):
1-6, 2018.
Kosuke Mitarai, Makoto Negoro, Masahiro Kitagawa, and Keisuke Fujii. Quantum Circuit Learning.
Physical Review A, 98(3):032309, 2018.
Valentin Murg, Frank Verstraete, Ors Legeza, and Reinhard M Noack. Simulating Strongly Corre-
lated Quantum Systems with Tree Tensor Networks. Physical Review B, 82(20):205105, 2010.
Michael A Nielsen and Isaac Chuang. Quantum Computation and Quantum Information. Cam-
bridge, 2010.
Alexander Novikov, Dmitry Podoprikhin, Anton Osokin, and Dmitry Vetrov. Tensorizing Neural
Networks. In Proc. Advances in Neural Information Processing Systems, 2015.
11
Under review as a conference paper at ICLR 2022
Roman Orus. Tensor Networks for Complex Quantum Systems. Nature Reviews Physics, 1(9):
538-550, 2019.
Ivan V Oseledets. Tensor-Train Decomposition. SIAM Journal on Scientific Computing, 33(5):
2295-2317, 2011.
Mateusz Ostaszewski, Lea M Trenkwalder, Wojciech Masarczyk, Eleanor Scerri, and Vedran Dun-
jko. Reinforcement Learning for Optimization of Variational Quantum Circuit Architectures.
arXiv preprint arXiv:2103.16089, 2021.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An Imperative Style,
High-Performance Deep Learning Library. In Proc. Advances in Neural Information Processing
Systems, volume 32, pp. 8026-8037, 2019.
John Preskill. Quantum Computing in the NISQ Era and Beyond. Quantum, 2:79, August 2018.
ISSN 2521-327X.
Jun Qi, Hu Hu, Yannan Wang, Chao-Han Huck Yang, Sabato Marco Siniscalchi, and Chin-Hui
Lee. Exploring deep hybrid tensor-to-vector network architectures for regression based speech
enhancement. arXiv preprint arXiv:2007.13024, 2020a.
Jun Qi, Hu Hu, Yannan Wang, Chao-Han Huck Yang, Sabato Marco Siniscalchi, and Chin-Hui
Lee. Tensor-to-Vector Regression for Multi-Channel Speech Enhancement Based on Tensor-Train
Network. In Proc. IEEE International Conference on Acoustics, Speech and Signal Processing,
pp. 7504-7508, 2020b.
Valeria Saggio, Beate E Asenbeck, Arne Hamann, Teodor Stromberg, Peter Schiansky, Vedran DUn-
jko, Nicolai Friis, Nicholas C Harris, Michael Hochberg, Dirk Englund, et al. Quantum Speed-ups
in Reinforcement Learning. In Quantum Nanophotonic Materials, Devices, and Systems 2021,
volume 11806, pp. 118060N. International Society for Optics and Photonics, 2021.
Maria Schuld and Nathan Killoran. Quantum Machine Learning in Feature Hilbert Spaces. Physical
Review Letters, 122(4):040504, 2019.
Maria Schuld and Francesco Petruccione. Supervised Learning with Quantum Computers, vol-
ume 17. Springer, 2018.
Maria Schuld, Ilya Sinayskiy, and Francesco Petruccione. An Introduction to Quantum Machine
Learning. Contemporary Physics, 56(2):172-185, 2015.
Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, and Yann LeCun. Over-
feat: Integrated Recognition, Localization and Detection Using Convolutional Networks. In Proc.
International Conference on Learning Representations, 2014.
Andrea Skolik, Sofiene Jerbi, and Vedran Dunjko. Quantum Agents in The Gym: A Variational
Quantum Algorithm for Deep Q-Learning. arXiv preprint arXiv:2103.15084, 2021.
Eric Smalley. AI-Powered Drug Discovery Captures Pharma Interest. Nature Biotechnology, 35(7):
604-606, 2017.
Andrei N Soklakov and Rudiger Schack. Efficient State Preparation for A Register of Quantum Bits.
Physical review A, 73(1):012307, 2006.
Andros Tjandra, Sakriani Sakti, and Satoshi Nakamura. Compressing Recurrent Neural Network
with Tensor-Train. In Proc. International Joint Conference on Neural Networks, pp. 4451-4458,
2017.
Hanrui Wang, Yongshan Ding, Jiaqi Gu, Yujun Lin, David Z Pan, Frederic T Chong, and Song
Han. Quantumnas: Noise-Adaptive Search for Robust Quantum Circuits. arXiv preprint
arXiv:2107.10845, 2021.
12
Under review as a conference paper at ICLR 2022
Chao-Han Huck Yang, Jun Qi, Samuel Yen-Chi Chen, Pin-Yu Chen, Sabato Marco Siniscalchi,
Xiaoli Ma, and Chin-Hui Lee. Decentralizing Feature Extraction with Quantum Convolutional
Neural Network for Automatic Speech Recognition. In Proc. IEEE International Conference on
Acoustics, Speech and Signal Processing, pp. 6523-6527, 2021.
Yinchong Yang, Denis Krompass, and Volker Tresp. Tensor-Train Recurrent Neural Networks for
Video Classification. In Proc. International Conference on Machine Learning, pp. 3891-3900,
2017.
R Yu, S Zheng, A Anandkumar, and Y Yue. Long-term Forecasting Using Tensor-Train RNNs.
arXiv preprint arXiv:1711.00073, 31, 2017.
13
Under review as a conference paper at ICLR 2022
A	Appendix
The section of appendix includes the proofs for Theorem 2 and Theorem 3.
A.1 Proof for Theorem 2
Proof. Theorem 2 is derived from the modification of the universal approximation theory proposed
by Barron (1994); Cybenko (1989). The universal approximation theory is shown in Lemma 1,
which suggests that a feed-forward neural network with d neurons can approximate any continuous
function with arbitrarily small .
Lemma 1. Given a continuous targetfunction f : RI → R, we can employ a 2-layerneural network
with a non-linear activation f : RI → R, such that
1
∣f - f∣≤H,	(11)
where J denotes the number of neurons, and Cf is a constant associated with f. In particular, for
r ≥ 1, Cf satisfies the following condition as:
llfll∞ + X	l∣Dkfll∞ ≤ Cf,	(12)
k,1≤ k(k-1) ≤r
where Dkf= [vf, V2f,…,Vk f]∖ .
To associate Lemma 1 with our Theorem 2, the target function is replaced with the target vector
HX-1Hθ-1 |zi, then there is a neural network with a dense layer connected to D qubits such that
∣∣Hχ1H-1∣zi- fdense(y)∣∣1 ≤ √Dc,	(13)
where C is related to the target vector HXIH-1∖Z.	□
A.2 Proof for Theorem 3
Proof. Assume that X = fττN (y), X = HXIH-1 |ziand the TT decomposition of target vector is
C 4 -C，	IiA>	，1	Λ .
{X1 , X2 , ..., XK }, then we obtain
K
l∣Hχ1 H-1Izi-fTTN(y)l∣ι = l∣X-x∣∣ι ≤ Y IXk-XkIli	(14)
k=1
On the other hand, we denote vec(Yk) and vec(Xk) as the vectorization of the tensors Yk and Xk,
respectively. We also define QkK=1 Ik = I, Wk ∈ RDk ×Ik ×rk-1 ×rk as the TTN parameters, and
also define Wk ∈ RIk×rk-1rkDk as the matricization of Wk. Moreover, σ refers to a non-linear
activation function.
Since vec(Xk) = σ(WT vec(Yk)) that corresponds to a dense layer, we can obtain that
1
IlXk -Xk ∣∣1 ≤ -τ==C.	(15)
Dk
In sum, we can further obtain
K	K1
llHXIHθ 1|zi- fTTN (y)||1 ≤ Y UXk-Xk ll1 ≤ Y √∩- C,	(16)
k=1	k=1 Dk
where QkK=1 Dk
D.
□
14
Under review as a conference paper at ICLR 2022
Figure 6: A comparison of convergence rates for different models.
B	Appendix
This section includes additional experimental simulations. First, we assess the settings of TT-ranks,
and then we compare the convergence rates of QTN-VQC and Dense-VQC in the experiments.
B.1	Experiments on TT-ranks for QTN-VQC
Table 5 corresponds to the experiments of QTN-VQC with 8 qubits and the Sigmoid function. The
empirical results suggest that the larger TT-ranks cannot result in better results than the smaller ones.
The main reason is that the TT-ranks can correspond to a manifold, and there may potentially exist
an optimal manifold with smaller TT-ranks that corresponds to the best performance.
Table 5: Comparing performance of different TT-ranks for QTN-VQC
TT-ranks	ParamS	CE	ACC (%)
{1,2, 2,1}	328	0.3090	91.43 ± 0.51
{1,4, 4,1}	768	0.3082	91.46 ± 0.53
{1,6, 6,1}	1464	0.3079	91.47 ± 0.52-
B.2	A comparison of convergence rates
Next, we analyze the computational complexity for TTN for QTN-VQC. In more detail, given the
TT-ranks {r1, r2, ..., rK}, a multi-dimensional tensor W is factorized into several K -order tensors
Wk ∈ Rrk-1 ×Ik×Jk ×rk, the computational complexity of the feed-forward process is in the scale of
O(K maxk Ik maxk Jk (maxk rk)K). In contrast, the computational overhead for a dense layer is
in the scale of O( k Ik k Jk). It means that smaller TT-ranks can reduce the computational cost
for QTN-VQC, which explains that smaller TT-ranks {1, 2, 2, 1} is configured in our experiments
of QTN-VQC.
Empirically, we compare the convergence rates of different models on the test data in our experi-
ments. In our experimental settings with the Tanh activation function and 8 qubits, the QTN-VQC
model consistently attains a faster convergence rate than the Dense-VQC and PCA-VQC counter-
parts. Moreover, Table 6 compares the absolute running time of QTN-VQC with Dense-VQC and
AlexNet-VQC. Since our experiments are conducted on the same GPUs and CPUs, the training time
of all models can be comparable. Our evaluation shows that QTN-VQC is marginally slower than
Dense-VQC, but it is much faster than AlexNet-VQC.
15
Under review as a conference paper at ICLR 2022
Table 6: Comparing performance of different TT-ranks for QTN-VQC
Models	Dense-VQC	AlexNet-VQC	QTN-VQC
Time/epochs (mins)	58	一	75	61
C Experiments of Labeled Faces in the Wild (LFW)
C.1 Experimental setups
The LFW is a dataset for the task of unconstrained face recognition, which is composed of 13000
images with the shape of [154, 154, 3]. The shape of We randomly split all the datasets into 11000
training data, 2000 test data. 16 qubits are used for VQC, and the shape of the input tensor is set as
22 × 147 × 22. The other settings are kept the same as the configurations for the MNIST task.
C.2 Experimental results
Table 6 presents the simulation results under the noiseless quantum circuit condition, while Table
7 demonstrates the empirical results in the setting of noisy quantum circuits. The QTN-VQC out-
performs the Dense-VQC counterpart (92.15 vs. 91.27), and it owns much fewer model parameters
(2816 vs. 1.1 × 106). Although
The experimental results on the LFW dataset also highlight the advantages of QTN-VQC in terms
of fewer model parameters and better empirical performance.
Table 7: Simulation results on the LFW test dataset under a noiseless circuit condition.
Models	Params	CE	Acc (%)
Dense-VQC	1.1 X 106	0.3011	91.27 ± 0.25
AlexNet-VQC	2.3 X 107	0.2875	93.21 ± 0.36
QTN-VQC~~	2816	0.2910	92.15 ± 0.43一
Table 8: Empirical results on the LFW test dataset under the noisy quantum circuit setting.
Models	Params	A8q20 (%)	Accdepo (%)
DenSe-VQC	1.1 X 106	88.65 ± 1.22	87.23 ± 1.04
AlexNet-VQC	2.3 X 107	89.76 ± 1.34	88.66 ± 1.08
QTN-VQC~~	2816	89.93 ± 1.09一	89.64 ± 1.07一
16