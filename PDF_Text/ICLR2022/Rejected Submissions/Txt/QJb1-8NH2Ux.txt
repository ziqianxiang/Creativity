Under review as a conference paper at ICLR 2022
Detecting Adversarial Examples Is (Nearly)
As Hard As Classifying Them
Anonymous authors
Paper under double-blind review
Ab stract
Making classifiers robust to adversarial examples is challenging. Thus, many de-
fenses tackle the seemingly easier task of detecting perturbed inputs.
We show a barrier towards this goal. We prove a general hardness reduction be-
tween detection and classification of adversarial examples: given a robust detector
for attacks at distance (in some metric), we show how to build a similarly robust
(but inefficient) classifier for attacks at distance /2—and vice-versa.
Our reduction is computationally inefficient, and thus cannot be used to build
practical classifiers. Instead, it is a useful sanity check to test whether empiri-
cal detection results imply something much stronger than the authors presumably
anticipated.
To illustrate, we revisit 14 empirical detector defenses published over the past
years. For 12/14 defenses, we show that the claimed detection results imply an
inefficient classifier with robustness far beyond the state-of-the-art— thus casting
some doubts on the results’ validity.
Finally, we show that our reduction applies in both directions: a robust classi-
fier for attacks at distance /2 implies an inefficient robust detector at distance .
Thus, we argue that robust classification and robust detection should be regarded
as (near)-equivalent problems, if we disregard their computational complexity.
1	Introduction
Building models that are robust to adversarial examples (Szegedy et al., 2014; Biggio et al., 2013) is
a major challenge and open-problem in machine learning. Due to the inherent difficulty in building
robust classifiers, researchers have attempted to build techniques to at least detect adversarial exam-
ples, a weaker task that is largely considered easier than robust classification (Xu et al., 2018; Pang
et al., 2021; Sheikholeslami et al., 2021).
Yet, evaluating the robustness of empirical detector defenses is challenging. This is in part due to
a lack of strong evaluation guidelines and benchmarks—akin to those developed for robust classi-
fiers (Carlini et al., 2019; Croce et al., 2020)—as well as to a lack of long-standing comparative
baselines such as adversarial training (Madry et al., 2018).
To illustrate, consider the following (fictitious) claims about two defenses against adversarial exam-
ples on CIFAR-10:
•	defense A is a classifier that achieves robust accuracy of 90% under '∞-perturbations
bounded by = 4/255;
•	defense B also has a “rejection" option, and achieves robust accuracy of 90% under '∞-
perturbations bounded by = 8/255 (we say that defense B is robust for some example
if it classifies that example correctly, and either rejects/detects or correctly classifies all
perturbed examples at distance .)
Which of these two (empirical) claims are you more likely to believe to be correct?
Defense A claims much higher robustness than the current best result achieved with adversarial
training (Madry et al., 2018; Rebuffi et al., 2021), the only empirical defense against adversarial
examples that has stood the test of time. Indeed, the state-of-the-art '∞ robustness for E = 4∕255
1
Under review as a conference paper at ICLR 2022
on CIFAR-10 (without external data) is ≈ 79% (Rebuffi et al., 2021). Thus, the claim of defense A
would likely be met with some initial skepticism and heightened scrutiny, as could be expected for
such a claimed breakthrough result.
The claim of defense B is harder to assess, due to a lack of long-standing baselines for robust
detectors (many detection defenses have been shown to be broken (Carlini & Wagner, 2017; Tramer
et al., 2020)). On one hand, detection of adversarial examples has largely been considered to be an
easier task than classification (Xu et al., 2018; Pang et al., 2021; Sheikholeslami et al., 2021). On
the other hand, defense B claims robustness to perturbations that are twice as large as defense A
( = 8/255 vs. = 4/255).
In this paper, we show that the claims of defenses A and B are, in fact, equivalent! (up to computa-
tional efficiency.)
We prove a general hardness reduction between classification and detection of adversarial examples.
Given a detector defense that achieves robust risk α for attacks at distance (under any metric), we
show how to build an explicit but inefficient classifier that achieves robust risk α for classifying
attacks at distance /2. The reverse implication also holds: a classifier robust at distance /2 implies
an explicit but inefficient robust detector at distance .
To the authors knowledge, there is no known way of leveraging computational inefficiency to build
more robust models. We should thus be as “surprised” by the claim made by defense B as by the
claim made by defense A.
Our reduction provides a way of assessing the plausibility of new robust detection claims, by con-
trasting them with results from the more mature literature on robust classification. To illustrate, we
revisit 14 published detection defenses across three datasets, and show that in 12/14 cases the de-
fense’s robust detection claims would imply an inefficient classifier with robustness far superior to
the current state-of-the-art. Yet, none of these detection papers make the claim that their techniques
should imply such a breakthrough in robust classification.
Using our reduction, it is obvious that many detection defenses are claiming much stronger robust-
ness than we believe feasible with current techniques. And indeed, many of these defenses were
later shown to have overestimated their robustness (Carlini & Wagner, 2017; Tramer et al., 2020).
Remarkably, we find that for certified defenses, the state-of-the-art results for provable robust clas-
sification and detection perfectly match the results implied by our reduction. For example, Sheik-
holeslami et al. (2021) recently proposed a certified detector on CIFAR-10 with provable robust
error that is within 3% of the provable error of the inefficient detector obtained by combining our
result with the state-of-the-art robust classifier of Zhang et al. (2020a).
In summary, we prove that giving classifiers access to a detection option does not help robustness (or
at least, not much). Our work provides, to our knowledge, the first example ofa hardness reduction
between different approaches for robust machine learning. As in the case of computational com-
plexity, we believe that such reductions can be useful for identifying research questions or areas that
are unlikely to bear fruit (bar a significant breakthrough)—so that the majority of the community’s
efforts can be redirected elsewhere.
On a technical level, our reduction exposes a natural connection between robustness and error cor-
recting codes, which may be of independent interest.
2	Hardness Reductions Between Robust Classifiers and
Detectors
In this section, we prove our main result: a reduction between robust detectors and robust classifiers,
and vice-versa. We first introduce some useful notation and define the (robust) risk of classifiers
with and without a detection option.
2.1	Preliminaries
We consider a classification task with a distribution D over examples x ∈ Rd with labels y ∈
[C]. A classifier is a function f : Rd → [C]. A detector is a classifier with an extra “rejection”
2
Under review as a conference paper at ICLR 2022
or ”detection” option ⊥, that indicates the absence of a classification. We assume for simplicity
that classifiers and detectors are deterministic. Our results can easily be extended to randomized
functions as well. The binary indicator function 1{a} is 1 if and only if the predicate A is true.
We first define a classifier’s risk, i.e., its classification error on unperturbed samples.
Definition 1 (Risk). Let f : Rd → [C] ∪ {⊥} be a classifier (optionally with a detection output ⊥).
The risk of f is the expected rate at which f fails to correctly classify a sample:
R(f)=(χ,y‰ [i{f (X)=揖
(1)
Note that for a detector, rejecting an unperturbed example sampled from the distribution D is counted
as an error.
For classifiers without a rejection option, we define the robust risk as the risk on worst-case adver-
sarial examples (Madry et al., 2018). Given an input X sampled from D, an adversarial example X
is constrained to being within distance d(χ, X) ≤ E from x, where d is some distance measure.
Definition 2 (Robust risk). Let f : Rd → [C] be a classifier. The robust risk at distance is:
Radv(f) :
E
(X,y)〜D
max
d(x,X)≤e
^kf(x)=y}
(2)
Thus, a sample (X, y) is robustly classified if and only if every point within distance E ofX (including
X itself) is correctly classified as y .
For a detector (a classifier with an extra detection/rejection output), we analogously define the robust
risk with detection. The classifier is now allowed to reject adversarial examples.
Definition 3 (Robust risk with detection). Let f : Rd → [C] ∪ {⊥} be a classifier with an extra
detection output ⊥. The robust risk with detection at distance E is:
Radv-det(f) =(x,yE〜d [d(maχ≤eIf(X)=y ∨ f (x)∈{y,⊥}}
(3)
That is, a detector defense f is robust on a natural input X if and only if the defense classifies the
natural input X correctly, and the defense either rejects or correctly classifies every perturbed input
X within distance E from x. The requirement that the defense correctly classify natural examples
eliminates pathological defenses that reject all inputs.
2.2	Robust Detection Implies Inefficient Robust Classification
We are now ready to introduce our main result, a reduction from a robust detector for adversarial
examples at distance E, to an inefficient robust classifier at distance E/2. We later prove that this
reduction also holds in the reverse direction, thereby demonstrating the equivalence between robust
detection and classification—up to computational hardness.
Theorem 4 (e-robust detection implies inefficient e/2-robust classification). Let d(∙, ∙) be an ar-
bitrary metric. Let f be a detector that achieves risk R(f) = α, and robust risk with detection
Radv-det (f) = β. Then, we can construct an explicit (but inefficient) classifier g that achieves risk
R(g) ≤ α and robust risk Rad/v2 (g) ≤ β.
The classifier g is constructed as follows on input X:
•	Run the detector model y J f (x). Ifthe input is not rejected, i.e., y = ⊥, then output the label
y that was predicted by the detector.
•	Otherwise, find an input X0 within distance E/2 ofX that is not rejected, i.e., d(X, X0) ≤ E/2 and
f(X0) 6= ⊥. If such an input X0 exists, output the label y J f(X0). Else, output a uniformly
random label y ∈ [C].
An intuitive illustration for our construction, and for the proof of the theorem (see below) is in
Figure 1.
3
Under review as a conference paper at ICLR 2022
£
Figure 1: Illustration of the construction of a robust classifier from a robust detector in Theorem 4.
The outer blue circle represents all inputs at distance at most from the input x. For a detector f,
the areas in green correspond to correctly classified inputs, and ratcheted gray areas correspond to
rejected inputs. The detector f is thus robust on x up to distance . The classifier g classifies a
perturbed input x, at distance e/2 from x, by finding any input within distance e/2 from X (the red
dashed circle) that is not rejected by f . Such an input necessarily exists and is correctly labeled by
f . The classifier g is thus robust on x up to distance e/2.
Our construction can be viewed as an analog of minimum distance decoding in coding theory. We
can view a clean data point sampled from D as a codeword, and an adversarial example X as a noisy
message with a certain number of errors (where the error magnitude is measured using an arbitrary
metric on Rd rather than the Hamming distance that is typically used for error correcting codes).
A standard result in coding theory states that if a code can detect α errors, then it can correct a∕2
errors. This result follows from a “ball-packing” argument: if α errors can be detected, then any
two valid codewords must be at least at distance α from each other, and therefore α∕2 errors can be
corrected via minimum distance decoding.
Proof of Theorem 4. First, note that the natural accuracy of our constructed classifier g is at least as
high as that of the detector f, since g always mimics the output of f whenever f does not reject an
input sampled from D. Thus, R(g) ≤ R(f) = α.
Now, for the sake of contradiction, consider an input (x, y)〜D for which the constructed classifier
g is not robust at distance e/2. By construction, this means that there exists some input X at distance
e/2 from X such that X is misclassified, i.e., g(X) = y = y. We will show that the detector f is not
robust with detection for x either (for attacks at distance up to e).
By definition of the classifier g, if g(X) = y = y then either:
•	The detector f also misclassifies X, i.e., f (X) = y.
So f is not robust with detection for X at distance e.
•	There exists an input X0 within distance e/2 of X, such that the detector f misclassifies X0,
i.e. f (x0) = y.
Note that by the triangular inequality, d(x, x0) ≤ d(x, X) + d(X, x0) ≤ e/2 + e/2 = e, and
thus f is not robust with detection for X at distance e.
•	The detector f rejects all inputs χ0 within distance e/2 of X (and thus g has output y by
sampling a label at random).
Since d(χ,X) ≤ e/2, this implies that the detector also rejects the clean input χ, i.e.,
f(X) = ⊥, and thus f is not robust with detection for X.
4
Under review as a conference paper at ICLR 2022
In summary, whenever the constructed classifier g fails to robustly classify an input x up to dis-
tance /2, the detector f also fails to robustly classify x with detection up to distance . Taking
expectations over the entire distribution D concludes the proof.	□
Note that the classifier g constructed in Theorem 4 is computationally inefficient. Indeed, the second
step of the defense consists in finding a non-rejected input within some metric ball. If the original
detector f is a non-convex function (e.g., a deep neural network), then this step consists in solving
an intractable non-convex optimization problem. Our reduction is thus typically not suitable for
building a practical robust classifier. Instead, it demonstrates the existence of an inefficient but
explicit robust classifier. We discuss the implications of this result more thoroughly in Section 3.
A corollary to our reduction is that many “information theoretic” results about robust classifiers can
be directly extended to robust detectors. For example, Tsipras et al. (2019) prove that there exists a
formal tradeoff between a classifier’s clean accuracy and robust accuracy for certain natural tasks.
Since their result applies to any classifier (including inefficient ones), combining their result with our
reduction implies that a similar accuracy-robustness tradeoff exists for detectors. More precisely,
Tsipras et al. (2019) show that for certain classification tasks and suitable choices of parameters
α, β, , any classifier g which achieves risk R(g) ≤ α must have robust risk at least Radv(g) ≥ β
against '∞-perturbations bounded by U By our reduction, this implies that any detector f with
risk at most R(f) ≤ α must also have robust risk with detection at least Rad/v2-det(f) ≥ β against
'∞-perturbations bounded by u∕2.
Similar arguments can be applied to show, for instance, that the increased data complexity of robust
generalization from Schmidt et al. (2018), or the tradeoff between robustness to multiple perturba-
tion types from Tramer & Boneh (2019), also apply to robust detectors.
Our reduction does not apply for “computational” hardness results that have been shown for robust
classification. For example, Garg et al. (2020) and Bubeck et al. (2018) show (“unnatural”) distri-
butions where learning a robust classifier is computationally hard—under standard cryptographic
assumptions. We cannot use Theorem 4 to conclude that learning a robust detector is hard for these
distributions, since the existence of such a detector would only imply an inefficient robust classifier
which does not contradict the results of Garg et al. (2020) or Bubeck et al. (2018).
2.3 Robust Classification Implies Inefficient Robust Detection
A similar argument as in Theorem 4 can be used in the opposite direction, to show that a robust clas-
sifier at distance U∕2 implies an inefficient robust detector at distance U. Taken together, Theorem 4
and Theorem 5 show that robust detection and classification are equivalent, up to a factor 2 in the
norm bound and up to computational constraints.
Theorem 5 (e∕2 robust-classification implies inefficient e-robust detection). Let d(∙, ∙) be an arbi-
trary metric. Let g be a defense that achieves robust risk Rad/v2 (f) = β. Then, we can construct
an explicit (but inefficient) defense f that achieves risk R(f) ≤ β and robust risk with detection
Radv-det(f) ≤ β.
The defense f is constructed as follows on input x:
•	Run the classifier y J g(x).
•	Find a perturbed input x0 withing distance e∕2 of x that is classified differently, i.e.,
d(x, x0) ≤ e∕2 and g(x0) 6= y. If such an input x0 exists, reject the input and output
⊥. Else, output the class y.
We provide the proof of Theorem 5 in Appendix A.
A main distinction between Theorem 4 and Theorem 5 is that the construction in Theorem 4 pre-
serves clean accuracy, but the construction in Theorem 5 does not. That is, the constructed robust
detector in Theorem 5 has clean accuracy that is equal to the robust classifier’s robust accuracy.
The construction in Theorem 5 can be efficiently (but approximately) instantiated by a certifiably
robust classifier (Wong & Kolter, 2018; Raghunathan et al., 2018). These defenses can certify that
a classifier’s output is constant for all points within some distance e of the input. For an adversarial
5
Under review as a conference paper at ICLR 2022
example X for g, the certification always fails and thus the constructed detector f will reject X. If g
is robust and the certification succeeds, the detector f copies the output of g. However, a certified
defense may fail to certify a robust input (a false negative), and thus the detector f may reject more
inputs than with the “optimal” construction in Theorem 5. This reduction from a certified classifier
to a detector is implicit in (Wong & Kolter, 2018, Section 3.1).
3	What Are Detection Defenses Claiming?
We now survey 14 detection defenses, and consider the robust classification performance that these
defenses implicitly claim (via Theorem 4). As we will see, in 12/14 cases, the defenses’ detection
results imply an inefficient classifier with far better robust accuracy than the state-of-the-art.
Before presenting our experimental setup and the explicit results from the reduction, we first discuss
how we believe these results should be interpreted.
Interpreting our reduction. Suppose that some detector defense claims a robust accuracy that
implies—via our reduction—an inefficient classifier with much higher robustness that the state-of-
the-art (e.g., the defense A described in the introduction of this paper).
A first possible interpretation of our reduction is that this robust detector implies the existence of a
robust classifier. This interpretation is rather weak however, since it is typically presumed that robust
classification is possible, and that human perception is one concrete example of a robust classifier.
The mere existence ofa robust classifier is thus typically already assumed to be true.
Our reduction yields a stronger result. It provides an explicit construction of an (inefficient) robust
classifier from a robust detector. The question then is whether we should expect the construction of
inefficient robust classifiers to be easier than the construction of efficient ones. That is, do we expect
that we can leverage computational inefficiency to build more robust classifiers that the current
state-of-the-art?
We do not know of a positive answer to this question, and there is evidence to suggest that the answer
may be negative.1 For example, the work of Schmidt et al. (2018) proves that fora synthetic classifi-
cation task between Gaussian distributions, building more robust classifiers requires additional data
regardless of the amount of computation power. Their results are corroborated by current state-of-
the-art robust classifiers based on adversarial training (Madry et al., 2018), which do not appear to
be limited by computational constraints. On CIFAR-10 for example, adversarial training achieves
100% robust training accuracy (Schmidt et al., 2018). Thus, it is unclear how computational ineffi-
ciency could be leveraged to build more robust classifiers using existing techniques.
Candidate approaches could be to train much larger models (e.g., with an exponential number of
parameters), or to perform an exhaustive architecture search to find more robust models. Yet, note
that the robust classifier constructed in our reduction only uses its unbounded computational power
at inference time. That is, the classifier that is built in Theorem 4 uses a trained detector model as a
subroutine (which is presumed to be efficient), and then solves a non-convex optimization problem
at inference time. The classifier built in our reduction is thus presumably weaker than a robust
classifier that can be trained with unbounded computational power.
To summarize, when a detector defense claims a certain robust accuracy, this implies the existence of
a concretely instantiatable robust classifier with an inefficient inference procedure. If this inefficient
classifier is much more robust than the current state-of-the-art, this does not necessarily mean that
the defense’s claims is wrong. But given how challenging robust classification is proving to be,
we have reason to be skeptical of such a major breakthrough (even for inefficient classifiers). To
compound this, many proposed detection defenses are quite simple, and reject adversarial inputs
based on some standard statistical test over a neural network’s features. It would thus be particularly
surprising if such simple techniques could yield robust classifiers, given that “simple” approaches to
adversarial robustness (denoising, compression, randomness, etc.) are ineffective (He et al., 2017).
1Some works have shown that for certain “unnatural distributions”, computational inefficiency is necessary
to build robust classifiers (Garg et al., 2020; Bubeck et al., 2018). Yet, since we presume that the human
perceptual system is robust to small perturbations on natural data (e.g., such as CIFAR-10), there must exist
some efficient natural process to achieve robustness on such data.
6
Under review as a conference paper at ICLR 2022
As a result, itis not too surprising that a number of the detector defenses that we survey have already
been broken by stronger attacks (Carlini & Wagner, 2017; Tramer et al., 2020). Our reduction would
have already suggested that such a break was likely to happen.
Experimental setup. We choose 14 detector defenses from the literature (see Table 1). Our se-
lection of these defenses was partially motivated by a pragmatic consideration on the easiness of
translating the defenses’ claims into a bound on the robust risk with detection Radv-det. Indeed, some
defenses simply report a single AUC score for the detector’s performance, from which we cannot
derive a useful bound on the robust risk. We thus focus on defenses that either directly report a
robust error akin to Definition 3, or that provide concrete pairs of false-positive and false-negative
rates (e.g., a full ROC curve). In the latter case, we compute a “best-effort” bound on the robust risk
with detection2 as:
Radv-det(f) ≤FPR+FNR+R(f) ,	(4)
where FPR and FNR are the detector’s false-positive and false-negative rates for a fixed detection
threshold, and R(f) is the defense’s standard risk (i.e., the test error on natural examples).
The above union bound in Equation (4) is quite pessimistic, as we may over-count examples that lead
to multiple sources of errors (e.g., a natural input that is misclassified and erroneously detected). The
true robustness claim made by these detector defenses might thus be stronger than what we obtain
from our bound. We encourage future detection papers to report their adversarial risk with detection,
Radv-det, to facilitate direct comparisons with robust classifiers using our reduction.
The 14 detector defenses use three datasets: MNIST, CIFAR-10 and ImageNet, and consider ad-
versarial examples under the '∞ or '2 norms. Given a claim of robust detection at distance e, we
contrast it to a state-of-the-art robust classification result for distance /2:
•	On MNIST with '∞ attacks, We use the adversarially-trained TRADES classifier (Zhang
et al., 2019) and measure robust error with the Square attack (Andriushchenko et al., 2020).
•	On MNIST with '2 attacks, We use the adversarially-trained classifier from Tramer &
Boneh (2019) and measure robust error with PGD (Madry et al., 2018).
•	On CIFAR-10, for both '∞ and '2 attacks we use the adversarially-trained classifier of Re-
buffi et al. (2021) (trained without external data), and attack it using the APGD-CE attack
from AutoAttack (Croce & Hein, 2020).
•	For ImageNet, for both '∞ and '2 attacks we use adversarially-trained classifiers and PGD
attacks from Engstrom et al. (2019).
We also consider two certified defenses for '∞ attacks on CIFAR-10: the robust classifier of Zhang
et al. (2020a), and a recent certified detector of Sheikholeslami et al. (2021).
Results. As we can see from Table 1, most defenses claim a detection performance that implies a
far greater robust accuracy than our current best robust classifiers. To illustrate with a concrete exam-
ple, the CIFAR-10 detector of Miller et al. (2019) claims to achieve robust accuracy with detection
of 75% for '2 attacks with = 2.9. Using Theorem 4, this implies an inefficient classifier with
robust accuracy of 75% for '2 attacks with = 2.9/2 = 1.45. Yet, the current state-of-the-art robust
accuracy for such a perturbation budget is only 30% (Rebuffi et al., 2021). If this detector defense’s
robustness claim were correct, it would imply a remarkable breakthrough in robust classification.
Why do many of these defenses claim robust accuracies that appear “too good to be true”? A pri-
mary reason is that the vast majority of the above detector defenses do not consider evaluations
against adaptive attacks (Carlini et al., 2019; Athalye et al., 2018; Tramer et al., 2020). That is,
these defenses show that they can detect some fixed attacks, and thereafter conclude that the detec-
tor is robust against all attacks. As in the case of robust classifiers, such an evaluation is clearly
insufficient! Some defenses do evaluate against adaptive adversaries, but fail to build a sufficiently
strong attack to reliably approximate the worst-case robust risk. Because of the lack of a strong
comparative baseline, it is not always immediately clear that these results are overly strong.
2Many detector defenses report performance against a set of fixed (non-adaptive) attacks. We interpret these
results as being an approximation of the worst-case risk.
7
Under review as a conference paper at ICLR 2022
Table 1: For each detector defense, we compute a (best-effort) bound on the claimed robust risk
with detection Radv-det using Equation (4), and report the complement (the robust accuracy with
detection), 1 - Radv-det. For each detector’s robustness claim (at distance ), we report the state-
of-the-art robust classification accuracy for attacks at distance /2, denoted 1 - Rad/v2 . Detection
defense claims that imply a higher robust classification accuracy than the current state-of-the-art are
highlighted in red.
Dataset	Defense	Norm		1 - Radv-det	1 - Rad/v2
	Grosse et al. (2017)	'∞	0.5	≥ 98%	94%
MNIST	Ma et al. (2018)	'2	4.2	≥ 99%	72%
	Raghuram et al. (2021)	'2	8.9	≥ 74%	0%
	Yin et al. (2020)	'2	1.7	≥ 90%	66%
	Feinman et al. (2017)	`2	2.7	≥ 43%	36%
	Miller et al. (2019)	'2	2.9	≥ 75%	30%
CIFAR-10	Raghuram et al. (2021)	'2	4.0	≥ 56%	10%
	Ma & Liu (2019)	'∞	4/255	≥ 96%	85%
	Roth et al. (2019)	'∞	8/255	≥ 66%	79%
	Lee et al. (2018)	'∞	20/255	≥ 81%	59%
	Li et al. (2019)	'∞	26/255	≥ 80%	44%
	Xu et al. (2018)	'2	1.0	≥ 67%	54%
ImageNet	Ma & Liu (2019)	'∞	2/255	≥ 68%	55%
	Jha et al. (2019)	'∞	2/255	≥ 30%	55%
	Hendrycks & Gimpel (2017)	'∞	10/255	≥ 76%	30%
	Yu et al. (2019)	'∞	26/255	≥ 7%	5%
For example, the recent work of Raghuram et al. (2021, ICML Long Talk) builds a detector on
MNIST with a FNR of ≤ 5% at a FPR of ≤ 20%, for adaptive `2 attacks bounded by = 8.9.
Yet, this perturbation bound is much larger than the average distance between an MNIST image
and the nearest image from a different class! Thus, an attack within this perturbation bound can
trivially reduce the detector’s accuracy to chance. On CIFAR-10, the same detector achieves 95%
clean accuracy, and a FNR of ≤ 19% at a FPR of ≤ 20% for adaptive `2 attacks bounded by = 4.
Using Equation (4), this yields a bound on the robust accuracy with detection of 1 - Radv-det (f) ≥
1 - (5% + 19% + 20%) = 56%. In contrast, the best robust classifier we are aware of for `2
attacks bounded by = 2 achieves robust accuracy of only 10% (Rebuffi et al., 2021). In summary,
the adaptive attack considered in this detector defense’s evaluation is highly unlikely to be good
approximation of a worst-case attack, and this defense can likely be broken by stronger attacks.
Certifiably robust detection and classification. In Table 2, we look at the robust accuracy with
detection, and standard robust accuracy achieved by certified defenses (for which the claimed ro-
bustness numbers are necessarily mathematically correct).
We note that our reduction is not as meaningful in the case of certified defenses, since it is highly
plausible that computational inefficiency can be leveraged to build better certified classifiers. In-
deed, given any robust classifier (e.g., an adversarially trained model), the classifier’s robustness can
always be certified inefficiently (by enumerating over all points within an -ball). Thus, the existence
of an inefficient classifier with higher certified robustness than the state-of-the-art is to be expected.
Nevertheless, we find that existing results for certified classifiers and detectors perfectly match what
is implied by our reduction (up to ±2% error). For example, Zhang et al. (2020a) follow a long line
of results on robust classifiers and achieve 39% robust accuracy on CIFAR-10 for perturbations of
'∞-norm below 4∕255. Together with Theorem 5, this implies an inefficient detector with 39% robust
8
Under review as a conference paper at ICLR 2022
Table 2: Certified robust accuracy 1 - Rad/v2 for the defense of Zhang et al. (2020a), and certified
robust accuracy with detection 1 - Radv-det for the defense of Sheikholeslami et al. (2021).
e 1 - RlIv-det	1 - R；/V
8/255	37%	39%
16/255	32%	33%
detection accuracy for perturbations of '∞-norm below 8/255. The recent work of Sheikholeslami
et al. (2021) nearly matches that bound (37% robust accuracy with detection), with a defense that
has the advantage of being concretely efficient.
These results give additional credence to our thesis: with current techniques, robust classification
is indeed approximately twice as hard (in terms of the perturbation bounds covered) than robust
detection.
Extensions and open problems. The main open problem raised by our work is of course whether
it could be possible to show an efficient reduction between classification and detection of adversarial
examples, but this seems implausible (at least with our minimum distance decoding approach).
Another interesting question is whether a similar reduction can be shown for robustness to less
“structured” perturbations than `p balls and other metric spaces. For example, there has been a line
of research on defending against adversarial patches (Brown et al., 2017), using empirical (Hayes,
2018; Naseer et al., 2019; Chou et al., 2020) and certifiable techniques (Chiang et al., 2020; Zhang
et al., 2020b; Xiang et al., 2021). To use our result, we would have to define some metric to measure
the size of an adversarial patch’s perturbation. Yet, the size of a patch is typically defined by the
number of contiguously perturbed pixels, which does not define a metric (in particular, it does not
satisfy the triangular inequality which our reduction relies on).
Finally, similar hardness reductions might exist between other candidate approaches for building
robust classifiers. For example, the question of whether (test-time) randomness can be leveraged to
build more robust models is also intriguing .Empirical defenses that use randomness can be noto-
riously hard to evaluate (Athalye et al., 2018; Tramer et al., 2020), so a reduction similar to ours
might be useful in showing that we should not expect such approaches to bare fruit.
4	Conclusion
We have shown formal reductions between robust classification with, and without, a detection op-
tion. Our results show that significant progress on one of these two tasks implies similar progress
on the other—unless computational inefficiency can somehow be leveraged to build more robust
models. This raises the question on whether we should spend our efforts on studying both of these
tasks, or focus our efforts on a single one.
On one hand, the two tasks represent different ways of tackling a common goal, and working on
either task might result in new techniques or ideas that apply to the other task as well. On the
other hand, our reductions show that unless we make progress on both tasks, work on one of the
tasks can merely aim to match the robustness of our inefficient constructions, whilst improving their
computational complexity.
We believe our reduction will serve as a useful sanity-check when assessing the claims of future de-
tector defenses. Detector defenses’ robustness evaluations have received less stringent scrutiny than
robust classifiers over the past years, perhaps in part due to a lack of strong comparative baselines.
Instead of having to wait until some detector defense’s claims pass the test-of-time, we show that
detection results can be directly contrasted against long-standing results for robust classification.
When applying this approach to past detector defenses, we find that many make robustness claims
that imply significant breakthroughs in robust classification. We believe our reduction could have
been useful in highlighting the suspiciously strong claims made by many of these defenses—before
they were explicitly broken by stronger attacks.
9
Under review as a conference paper at ICLR 2022
Ethics statement. Our paper demonstrates a fundamental barrier towards detecting against adver-
sarial examples, under the assumption that our current techniques are insufficient to achieve strong
(inefficient) robust classification. We do not however explicitly break any existing defenses (our
results merely strongly suggest that many existing detector defenses’ claims are suspiciously high).
Our paper therefore cannot lead to any explicit harms, but aims to further our understanding of the
hardness of robust classification and detection.
Reproducibility statement. Our paper’s contribution is mainly of theoretical nature. Section 2
is self-contained and clearly states our assumptions, results and proofs (except for the proof of
Theorem 5 in Appendix A). The experiments in Section 3 use only public datasets and pre-trained
models, with clearly indicates hyper-parameters for all attacks that we evaluate.
References
Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square at-
tack: a query-efficient black-box adversarial attack via random search. In European Conference
on Computer Vision, 2020.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of se-
curity: Circumventing defenses to adversarial examples. In International Conference on Machine
Learning, 2018.
Battista Biggio, Igmo Corona, Davide Maiorca, Blame Nelson, Nedim Srndic, Pavel Laskov, Gior-
gio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Euro-
Pean Conference on Machine Learning and Knowledge Discovery in Databases, pp. 387-402.
Springer, 2013.
Tom B Brown, Dandelion Mane, Aurko Roy, Martin Abadi, and Justin Gilmer. Adversarial patch.
arXiv preprint arXiv:1712.09665, 2017.
Sebastien Bubeck, Yin Tat Lee, Eric Price, and Ilya Razenshteyn. Adversarial examples from cryp-
tographic pseudo-random generators. arXiv preprint arXiv:1811.06418, 2018.
Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten
detection methods. In AISec, pp. 3-14. ACM, 2017.
Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris
Tsipras, Ian Goodfellow, and Aleksander Madry. On evaluating adversarial robustness. arXiv
preprint arXiv:1902.06705, 2019.
Ping-yeh Chiang, Renkun Ni, Ahmed Abdelkader, Chen Zhu, Christoph Studer, and Tom Goldstein.
Certified defenses for adversarial patches. arXiv preprint arXiv:2003.06693, 2020.
Edward Chou, Florian Tramer, and Giancarlo Pellegrino. Sentinet: Detecting physical attacks
against deep learning systems. In Workshop on Deep Learning Security, 2020.
Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble
of diverse parameter-free attacks. In ICML, 2020.
Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Nicolas Flammarion, Mung Chiang,
Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness bench-
mark. arXiv preprint arXiv:2010.09670, 2020.
Logan Engstrom, Andrew Ilyas, Hadi Salman, Shibani Santurkar, and Dimitris Tsipras. Robustness
(python library), 2019. URL https://github.com/MadryLab/robustness.
Reuben Feinman, Ryan R Curtin, Saurabh Shintre, and Andrew B Gardner. Detecting adversarial
samples from artifacts. arXiv preprint arXiv:1703.00410, 2017.
Sanjam Garg, Somesh Jha, Saeed Mahloujifar, and Mahmoody Mohammad. Adversarially robust
learning could leverage computational hardness. In Algorithmic Learning Theory, pp. 364-385.
PMLR, 2020.
10
Under review as a conference paper at ICLR 2022
Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick McDaniel. On
the (statistical) detection of adversarial examples. arXiv preprint arXiv:1702.06280, 2017.
Jamie Hayes. On visible adversarial perturbations & digital watermarking. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition Workshops ,pp.1597-1604, 2018.
Warren He, James Wei, Xinyun Chen, Nicholas Carlini, and Dawn Song. Adversarial example
defenses: Ensembles of weak defenses are not strong. In USENIX Workshop on Offensive Tech-
nologies, 2017.
Dan Hendrycks and Kevin Gimpel. Early methods for detecting adversarial images. In International
Conference on Learning Representations, 2017.
Susmit Jha, Sunny Raj, Steven Lawrence Fernandes, Sumit Kumar Jha, Somesh Jha, Gunjan Verma,
Brian Jalaian, and Ananthram Swami. Attribution-driven causal analysis for detection of adver-
sarial examples. arXiv preprint arXiv:1903.05821, 2019.
Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting
out-of-distribution samples and adversarial attacks. In Advances in Neural Information Process-
ing Systems, 2018.
Yingzhen Li, John Bradshaw, and Yash Sharma. Are generative classifiers more robust to adversarial
attacks? In International Conference on Machine Learning, pp. 3804-3814. PMLR, 2019.
Shiqing Ma and Yingqi Liu. Nic: Detecting adversarial samples with neural network invariant
checking. In Proceedings of the 26th Network and Distributed System Security Symposium (NDSS
2019), 2019.
Xingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wijewickrema, Grant Schoenebeck,
Dawn Song, Michael E Houle, and James Bailey. Characterizing adversarial subspaces using
local intrinsic dimensionality. In International Conference on Learning Representations, 2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018.
David Miller, Yujia Wang, and George Kesidis. When not to classify: Anomaly detection of attacks
(ada) on dnn classifiers at test time. Neural computation, 31(8):1624-1670, 2019.
Muzammal Naseer, Salman Khan, and Fatih Porikli. Local gradients smoothing: Defense against
localized adversarial attacks. In 2019 IEEE Winter Conference on Applications of Computer
Vision (WACV), pp. 1300-1307. IEEE, 2019.
Tianyu Pang, Huishuai Zhang, Di He, Yinpeng Dong, Hang Su, Wei Chen, Jun Zhu, and Tie-Yan
Liu. Adversarial training with rectified rejection. arXiv preprint arXiv:2105.14785, 2021.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial exam-
ples. In International Conference on Learning Representations, 2018.
Jayaram Raghuram, Varun Chandrasekaran, Somesh Jha, and Suman Banerjee. A general frame-
work for detecting anomalous inputs to dnn classifiers. In International Conference on Machine
Learning, 2021.
Sylvestre-Alvise Rebuffi, Sven Gowal, Dan A Calian, Florian Stimberg, Olivia Wiles, and Tim-
othy Mann. Fixing data augmentation to improve adversarial robustness. arXiv preprint
arXiv:2103.01946, 2021.
Kevin Roth, Yannic Kilcher, and Thomas Hofmann. The odds are odd: A statistical test for detecting
adversarial examples. In International Conference on Machine Learning, 2019.
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adver-
sarially robust generalization requires more data. In Advances In Neural Information Processing
Systems, pp. 5019-5031, 2018.
11
Under review as a conference paper at ICLR 2022
Fatemeh Sheikholeslami, Ali Lotfi, and J Zico Kolter. Provably robust classification of adversarial
examples with detection. In International Conference on Learning Representations, 2021.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. In International Conference on
Learning Representations, 2014.
Florian Tramer and Dan Boneh. Adversarial training and robustness for multiple perturbations. In
Advances In Neural Information Processing Systems, 2019.
Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to
adversarial example defenses. In Conference on Neural Information Processing Systems, 2020.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness may be at odds with accuracy. In International Conference on Learning Representa-
tions, 2019.
Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In International Conference on Machine Learning, pp. 5283-5292, 2018.
Chong Xiang, Arjun Nitin Bhagoji, Vikash Sehwag, and Prateek Mittal. Patchguard: A provably ro-
bust defense against adversarial patches via small receptive fields and masking. In 30th {USENIX}
Security Symposium ({USENIX} Security 21), 2021.
Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep
neural networks. In Network and Distributed System Security Symposium, 2018.
Xuwang Yin, Soheil Kolouri, and Gustavo K Rohde. Gat: Generative adversarial training for ad-
versarial example detection and robust classification. In International Conference on Learning
Representations, 2020.
Tao Yu, Shengyuan Hu, Chuan Guo, Wei-Lun Chao, and Kilian Q Weinberger. A new defense
against adversarial images: Turning a weakness into a strength. In Advances in Neural Informa-
tion Processing Systems, 2019.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan.
Theoretically principled trade-off between robustness and accuracy. In International Conference
on Machine Learning, 2019.
Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Duane Boning, and Cho-Jui Hsieh. Towards
stable and efficient training of verifiably robust neural networks. In International Conference on
Learning Representations, 2020a.
Zhanyuan Zhang, Benson Yuan, Michael McCoyd, and David Wagner. Clipped bagnet: Defending
against sticker attacks with clipped bag-of-features. In 2020 IEEE Security and Privacy Work-
shops (SPW), pp. 55-61. IEEE, 2020b.
12
Under review as a conference paper at ICLR 2022
A	Proof Of Theorem 5.
We recall Theorem 5:
Theorem 5 (e/2 robust-classification implies inefficient e-robust detection). Let d(∙, ∙) be an arbi-
trary metric. Let g be a defense that achieves robust risk Rad/v2 (f) = β. Then, we can construct
an explicit (but inefficient) defense f that achieves risk R(f) ≤ β and robust risk with detection
Radv-det(f) ≤ β.
The defense f is constructed as follows on input x:
•	Run the classifier y J g(x).
•	Find a perturbed input x0 withing distance e/2 of x that is classified differently, i.e.,
d(x, x0) ≤ e/2 and g(x0) 6= y. If such an input x0 exists, reject the input and output
⊥. Else, output the class y.
Proof of Theorem 5. Note that for any input (x, y) for which the classifier g is robust at distance
e/2, no input x0 above exists and so f(x) = y. Thus, the risk of f is at most the robust risk of g, so
R(f) ≤ β.
Now, consider an input (x, y)〜D for which f is not robust with detection at distance e. That is,
either f (x) = y, or there exists an input X at distance d(x, x) ≤ e such that f (X) = y ∈ {y, ⊥}. We
will show that the defense g is not robust for x either (for attacks at distance up to e/2.)
If f(x) 6= y, then by the same argument as above it cannot be the case that g is robust at distance
e/2 for x.
So let us consider the case where f (X) = y ∈ {y, ⊥}. By the definition of f, this means that for all
χ0 at distance at most e/2 from X, we have g(χ0) = y. But, note that there exists a point x* that is at
distance at most e/2 from both X and x. Since we must have g(χ*) = y, we conclude that g is not
robust at distance e/2 for X.
Taking expectations over the distribution D concludes the proof.	□
13