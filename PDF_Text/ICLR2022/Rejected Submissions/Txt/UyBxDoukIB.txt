Under review as a conference paper at ICLR 2022
Teamwork makes von Neumann work:
Min-Max Optimization in Two-Team Zero-Sum
Games
Anonymous authors
Paper under double-blind review
Ab stract
Motivated by recent advances in both theoretical and applied aspects of multi-
player games, spanning from e-sports to multi-agent generative adversarial net-
works, we focus on min-max optimization in team zero-sum games. In this class
of games, players are split into two teams with payoffs equal within the same
team and of opposite sign across the opponent team. Unlike the textbook two-
player zero-sum games, finding a Nash equilibrium in our class can be shown to
be CLS-hard, i.e., itis unlikely to have a polynomial-time algorithm for computing
Nash equilibria. Moreover, in this generalized framework, we establish that even
asymptotic last iterate or time average convergence to a Nash Equilibrium is not
possible using Gradient Descent Ascent (GDA), its optimistic variant, and extra
gradient. Specifically, we present a family of team games whose induced utility is
non-multilinear with non-attractive per-se mixed Nash Equilibria, as strict saddle
points of the underlying optimization landscape. Leveraging techniques from con-
trol theory, we complement these negative results by designing a modified GDA
that converges locally to Nash equilibria. Finally, we discuss connections of our
framework with AI architectures with team competition structures like multi-agent
generative adversarial networks.
1 Introduction
Team competition has played a central role in the development of Game Theory (Marschak, 1955;
von Stengel & Koller, 1997; Bacharach, 1999; Gold, 2005), Economics (Marschak, 1955; Gottinger,
1974) and Evolutionary Biology (Nagylaki, 1993; Nowak et al., 2004), however, the behavior of the
underlying dynamics within the teams are usually sidelined. Either for reasons of mathematical
convenience or bigger picture understanding, “teams” in literature are typically modeled as if they
were unitary actors, i.e., single individuals without unveiling the internal decision-making of the
team members (see Kim et al. (2019)).
For instance, in the biology setting of weak selection model (Nagylaki, 1993; Chastain et al., 2014;
Mehta et al., 2015) species are modeled to compete as teams, while at the crux of the matter the
genes of each species are the actual players and their alleles are the actions in the survival game.
Similarly, it was the social-media collaboration of the Reddit retail trading crowd as a team that
touches off the last year’s GameStop frenzy of short squeeze (Umar et al., 2021; Hasso et al., 2021)
transforming the markets into a tug of war game against the team of Wall Street hedge funds.
Recently, these intrinsic details behind the competition among teams have attracted renewed interest
in the Machine Learning community, motivated by the advent of multi-agent systems that are used
for generative tasks or playing complex games like CTF (Jaderberg et al., 2019) or Starcraft (Vinyals
et al., 2019). So as to win this kind of games, self-training AI systems have to develop both col-
laborative attributes (coordination within each team) as well as contesting ones (competition across
the teams). Moreover, following the complementary thread of multi-agent generative adversarial
network research, the creation of a pool by efficient incumbent agents, either in generators (Arora
et al., 2017; Hoang et al., 2017; 2018; Zhang et al., 2018; Tang, 2020), or discriminators (Hardy
et al., 2019; Albuquerque et al., 2019) has been tested providing significant statistical and computa-
1
Under review as a conference paper at ICLR 2022
tional benefits. In this direction, researchers strive to harness the efficacy of distributed processing,
utilizing shallower networks that can learn all the while more diverse datasets 1.
In order to shed some light on this persistent strain of research, the main premise of the theoretical
scaffolding developed in this paper is that
The “unitary two-players” min-max approach misses the critical component
of the collective strategy making within each competing team.
Our class of games. In this regard, we turn our attention to Two-Team Zero-Sum games, proposed
by Schulman & Vazirani (2019b), a quite general class of min-max optimization problems that in-
clude bilinear games as well as a wide range of non-convex non-concave games. In this class, the
players fall in two teams of size k1 , k2 and submit their own probabilistic strategy vector indepen-
dently, akin to a general normal form multi-player game. Following the econometric common value
assumption of Marschak (1955), what makes a group of players a team is that in any outcome the
players of each team receive an identical payoff. Thus, to build some intuition, it is easy to see
that if perfect coordination existed within each team, the interaction between the teams is merely
a zero-sum game between two “virtual” players. To streamline our presentation here, we defer the
more precise description of our model to Section 2.
Challenges behind Two-Team Zero-Sum games. In the archetypical case of two players, i.e.,
(k1 = k2 = 1), min-max strategies are typically thought of as the axiomatically correct predic-
tions thanks to the seminal Von Neumann’s minmax theorem (Von Neumann, 1928). Unfortu-
nately, min-max optimization for case k > 1 is a much more tenuous affair: Schulman & Vazi-
rani (2019b) preclude the existence of unique value by presenting a family of team games where
min max 6= max min together with bounds about this duality gap, which quantifies exactly the ef-
fect of exchanging the order of strategy commitment either between the teams or the players thereof.
If defining the correct figure of merit for Team games is rife with frustration, what is even more
demanding is understanding what kind of algorithms/dynamics are able to solve this problem when
a game-theoretically meaningful solution exists: Firstly, computing local Nash Equilibria (NE) in
general non-convex non-concave games is PPAD-complete (Daskalakis et al., 2009; 2021). Thus,
all well-celebrated first-order methods, like gradient descent-ascent (Lin et al., 2020; Daskalakis &
Panageas, 2019), its optimistic (Popov, 1980; Daskalakis & Panageas, 2018; Mertikopoulos et al.,
2019) and extra gradient variant (Korpelevich, 1976) would require an exponential number of steps
in the parameters of the problem to find an appoximate NE under Nemirovsky-Yudin (Nemirovskij
& Yudin, 1983) oracle optimization model. Secondly, even if a regret notion could be defined, no-
regret methodology is guarranteed to attract only to the set of coarse correlated equilibria (CCE)
(Fudenberg, 1991; Hannan, 2016; Flokas et al., 2020; Giannou et al., 2021), a weaker notion that
may be exclusively supported on strictly dominated strategies, even for simple symmetric two-player
games (See also Viossat & Zapechelnyuk (2013)).
Whilst the aforementioned intractability failures for the general case of non-convex non-concave
min-max problems provides significant insights, they can not a fortiori answer the fundamental
question, restricted in the model of Two-Team Zero-Sum Games:
Can we compute Nash equilibria in Two-Team Zero-Sum Games and
ultimately are there first-order methods that converge to them under tangible guarantees?
Our results. To the best of our knowledge, the following contributions are the first-of-its-kind
type of results for the case of Two-Team Zero-Sum games:
•	For the case of the computational complexity of approximate (possibly mixed) NE we establish a
sweeping negative result proving that it is CLS-hard (Theorem 3.1), i.e., is computationally harder
than finding pure NE in a congestion game or finding approximate gradient descent fixed points.
•	From an optimization perspective, we settle these questions with a resounding “no” for all the
well-known discrete gradient flow variations. Specifically, we present a simple family of two-
team with two-players zero-sum games where Projected-GDA, Optimistic-GDA, and Extra Gra-
dient fail even to stabilize around a mixed NE, when they are initialized nearby (Theorem 3.5).
1Indeed, from the training perspective, it’s more computationally preferable to back-propagate through two
equally sized neural networks with smaller capacity rather than through a giant single one that would be twice
as deep.(Tang, 2020)
2
Under review as a conference paper at ICLR 2022
Additionally, for the category GDA in the non-degenerate team games with unique mixed NE,
one could acquire an even stronger result for any high-dimensional configuration of actions and
players. (Theorem 3.2)
•	In order to make some substantial headway under the burden of the above instability re-
sults, we shift our attention to adaptive control generalizations of the celebrated Washout fil-
ters-traditionally used for stabilizing the Dutch-roll motion of an aircraft during a flight (Has-
souneh et al., 2004; Grant & Reid, 1997). Inspired by this framework, we propose the modified
KPV-GDA2 which consists of a tandem combination of GDA together with a stabilizing feedback
introduces by Bazanella et al. (1997).
state(k+1) =	state(k) + ηGDA(state(k+1)) + ηK(state(k) - stress(k)) (KPV GDA)
stress(k+1) = stress(k) + ηP (state(k) - stress(k))
The main linchpin of KPV-GDA method is the Simon’s and Theil’s (Simon, 1956; Theil, 1957)
certainty equivalence principle, a widely used methodology in Control theory in developing ap-
plied dynamic rational expectations models. According to this principle, the feedback law is
split into two optimization steps whereby K-step attracts quickly the state to the stress while
P -step converges slowly to the fixed points of GDA. Compared with the plethora of the proposed
dynamics for min-max problems, the crucial advantage of the afore-described technique is that
does not introduce any extra fixed points than GDA’s ones. In Section 2.2, we provide some il-
lustrative examples of KPV-GDA technique, while in Theorem 3.7 we prove the existence of such
control feedback for our class of games.
•	Finally, in Section 4 we provide a series of experiments in simple two-team zero-sum games
showcasing both the messy behaviors of traditional methods like GDA,OGDA and the power of
KPV-GDA method in these optimization environments. Additionally, we show that multi-agent
GAN architectures achieve better performance than the single-agent ones, in terms of network
capacity, when they are trained in synthetic or real-world datasets like CIFAR10.
2 Preliminaries
2.1	Definitions
Our setting. Formally, a two-team game in normal form is defined as a tuple Γ = Γ(N , A, u)
consisting of (i) a finite set of players N , split into two teams A, B with kA and kB players cor-
respondingly such that: N = NA ∪ NB = {Aι, ∙ ∙ ∙ ,AkA,Bi,…，BkB}; (ii) a finite set of
actions (or pure strategies) Ai = {α1, . . . , αni} per player i ∈ N; (iii) each team’s payoff func-
tion uA, uB : A → R, where A := Qi Ai denotes the ensemble of all possible action profiles
α = (αA1 , . . . , αAk , αB1 , . . . , αBk ) while the individual utility of a player is identical to her
teammates, i.e., ui = uA & uj = uB ∀(i, j) ∈ NA × NB. In this general context, players could also
adhere mixed strategies, i.e, probability distributions sk ∈ ∆(Ak) over the pure strategies αk ∈ Ak.
Correspondingly, We define the product distributions X = sai 0∙∙∙0 SAkA, y = sbi 0∙∙∙0 SBkB
as the teams’ strategies. Collectively, we will write X := i∈NA Xi = i∈NA ∆(Ai), Y :=
Qi∈N Yi = Qi∈N ∆(Ai) the space of mixed strategy profiles of teams A, B.
Similarly with the bilinear two-player games, the teams’ utility functions can be expressed via the
payoff-tensors A, B ∈ Rτ with τ = Qi∈N |Ai| and acquire the form3:
uA = Ayx & uB = Byx
(2.1)
2The name “KPV-GDA” is an initialism from (K, P)-Vaned Gradient Descent Ascent method; Just like
the tail section of an aircraft where the vanes are flight control surfaces that control the unstable yaw, (K, P)
control feedback aims to stabilize the unstable arrows of Gradient flow around a mixed NE.
3Figuratively, the latter form denotes what is known as a tensor contraction given: Ayx =
Pi,...,j,k...,l xi,...,j Ai,...,j,k...,lyk...,l If x, y have shapes (i, j) and (k, l) that would be equivalent to u =
einsum(’ijkl,ij,kl’, A, x, y)
3
Under review as a conference paper at ICLR 2022
In terms of solutions, we focus on the per player Nash Equilibrium (NE), i.e., a state strategy profile
s* = (X, y) = ((SA1,..., 4 SAkA), (SB1,..., SBkB))SUCh that
Ui(s*) ≥ Ui(Si； s-i)4 for all Si ∈ ∆(Ai) and all i ∈N	(NE)
The state strategy profile s* is called pure if every player of both teams chooses a single action;
otherwise we say that it is mixed. Finally, a two-team game is Called two-team zero-sum if uA =
-uB or eqUivalently A + B = O.
Remark 2.1. A qUite technical prereqUisite for the rest of this work, we will assUme that a sUccinct
representation of the Utility tensors of the game is available or eqUivalently that a payoff oracle
provides efficiently both the valUe of the Utility fUnction and its derivatives for a specific inpUt,
which is consistent with the vast majority of the applications that are described in the literatUre (von
Stengel & Koller, 1997).
A first approach on computing Nash equilibria in Two-Team Zero-Sum games. Given the
existence of the dUality-gap between the min max and max min, in lieU of the two-player zero-sUm
game, an eqUilibriUm in oUr setting can not be compUted via linear programming. For the goal of
compUting Nash eqUilibria in two-team zero-sUm games, we have experimented with a selection of
first-order methods that have been Utilized with varying sUccess in the setting of the two-person zero-
sUm case. Namely, we analyze the following methods: i) Gradient Descent-Ascent ii) Optimistic
Gradient Descent-Ascent iii) Extra Gradient Method iv) Optimistic Multiplicative Weights Update
Method We defer their precise definitions in Appendix B. The below remark will play a key role in
the seqUel.
Remark 2.2. Any fixed point of the aforementioned discrete-time dynamics on the Utility fUnction
corresponds necessarily to the Nash eqUilibria of the game.
Hence, an important testbed for the long-rUn behavior of GDA, OGDA, and EG methods is to exam-
ine whether these methods stabilize aroUnd their fixed points, which effectively constitUte the Nash
eqUilibria of the game. In Section 3.2, we show that in lack of pUre Nash eqUilibria, all the above
methods fail to stabilize on their fixed points even for a simple class of (2, 2)-players game, and as
a conseqUence to the mixed Nash eqUilibria of the game.
The presence of these resUlts showcases the need for a different approach that lies oUtside pUrely
optimization-based ideas. Inspired by the applications of washoUt filters to stabilize highly sUs-
ceptible systems and their adaptive control generalizations, we design a new incarnation of GDA
vaned by two matrices-control feedback. SUrprisingly, in contrast with the aforementioned tradi-
tional methods, oUr proposed techniqUe accomplishes last-iterate stabilization on its fixed point, i.e.,
the mixed Nash eqUilibria of the team game.
(K, P )-Vaned GDA Method. After concatenating the vectors of the minimizing and the maxi-
mizing agents z(k) = (X(k), y(k)) we can write oUr method, for appropriate matrices K, P:
z(k+1) = ΠZ
θ(k+1) = ΠZ
Z⑹ + η(-ζffZZk)))) + ηK (z(k) — θ(k))}
θ(k) + ηP (z(k) - θ(k))
(2.2)
IntUitively, the added variable θ(k) holds an estimate of the fixed point, and throUgh the feedback
ηK(z(k) - θ(k)) the vector z stabilizes aroUnd that estimate which slowly moves towards the real
fixed point of the plain GDA dynamic. It is crUcial to note that no additional fixed points are intro-
dUced to the system.
2.2	Two Illustrative Examples
OUr first example sUpports a doUble role: Firstly, it exemplifies how oUr two-team min-max compe-
tition can captUre the formUlation of mUlti-agent GANs’ architectUres. Secondly, it hints also at an
early separation between the optimization methods, since as we will see GDA will not converge to
the Nash EqUilibriUm/groUnd-trUth distribUtion.
4We are Using here the standard shorthand (s1, . . . , si, . . . , s|N|) to highlight the strategy of a given player
i ∈ N versUs the rest of players N \ {i}.
4
Under review as a conference paper at ICLR 2022
Figure 1: Parameter training of the configuration under different algorithms
2.2	. 1 Learning a mixture of Gaussians with multi-agent GANs
Consider the case of M, a mixture of gaussian distribution With two components, C1 〜N(μ, I) and
C2 〜N(-μ, I) and mixture weights π1, π2 to be positive such that π1 + π2 = 1 and π1, π2 = 2.
To learn the distribution above, we utilize an instance of a Team-WGAN in which there exists a
generating team of agents Gp : R → R, Gθ : Rn → Rn , and a discriminating team of agents
Dv : Rn → R, Dw : Rn → R, all described by the following equations:
Generators: Gp(ζ) = p + ζ , Gθ(z) = z + θ
Discriminators: Dv(y) = hv, yi , Dw(y) = Pi wiyi2
(2.3)
The generating agent Gθ maps random noise Z 〜N(0, I) to samples while generating agent Gp(Z),
utilizing an independent source of randomness Z 〜N(0,1), probabilistically controls the sign of
the output of the generator Gθ. The probability of ultimately generating a sample y = z+θ is equal
to ζ + p, while the probability of the sample being y = -z - θ is equal to 1 - (p + ζ).
On the other end, there stands the discriminating team of Dv , Dw. Discriminators, Dv (y), Dw (y)
map any given sample y to a scalar value accounting for the realness or fakeness of it - negative
meaning fake, positive meaning real. The discriminators are disparate in the way they measure the
realness of samples as seen in their definitions.
We follow the formalism of the Wasserstein GAN to form the optimization objective:
max min
Ez~N (0,I ),ζ ~N (0,1)
Ey~real [Dv (y) + Dw (y)]
-
Gp(Z) ∙ (Dv(Gθ(y)) + Dw(Gθ(y)))	、
+
(1 - Gp (Z)) ∙ (Dv (-Gθ (y)) + Dw (-Gθ (y)))
Equation equation 2.4 yields the simpler form:
(2.4)
n
max min(π1 -∏2)vTμ - 2ρvTθ + VTθ + X Wi (μ2 - θ2)	(2.5)
v,w θ,p
i
It is easy to check that Nash equilibria of Equation equation 2.4 must satisfy:
θ θ =. μ, P = 1 - ∏2 = ∏ι ]
[θ = -μ, p = 1 - ∏ι = ∏2J
Figure 1 demonstrates both GDA’s failure and OGDA, EG, and our KPV-GDA method’s success to
converge to the above Nash equilibria and simultaneously to discover the groud truth mixture.
5
Under review as a conference paper at ICLR 2022
2.2.2 Multiplayer Matching Pennies
Interestingly enough, there are non-trivial instances of two-team competition settings that even Op-
timistic GDA and EG Method fail to converge. Such is the case for a team version of the well-known
game of matching pennies. The game can be shortly described as such: “coordinate with your team-
mates to play a game of matching pennies against the opposing team, coordinate not and pay a
penalty”. The penalty is set to 2. For the interested reader, We defer to appendix C.2 the precise
description of the game in a contracted tensor/table. Since every player has simply two actions, their
probability vector can be represented by a single variable in [0, 1]. Considering the minimizing team
x its players are x1 , x2 , While the players of the maximizing team y are y1 , y2 . The multiplayer of
matching pennies is described by the utility function:
u(x1,x2,y1,y2) = -x1x2-x1y1-x1y2+1.5(x1+x2)-x2y1-x2y2+y1y2+0.5(y1+y2)-1 (2.6)
As We can see in Figures 2 and 3, multiplayer matching pennies game consists an excellent bench-
mark Where all traditional gradient floW discretizations fail under perfect competition setting. Inter-
estingly, We are not aWare of a similar example in min-max literature and it has been our starting
point for seeking neW optimization techniques inspired by Control theory. Indeed, KPV-GDA varia-
tion With (K, P) = (-1.1I, 0.3I) achieves to converge to the unique mixed Nash Equilibrium of the
game. In the folloWing sections, We provide theorems that explained formally this long-run behavior
of the examined dynamics.
Figure 2: Multiplayer matching pennies under Figure 3: Projected Trajectory of Team A under
different algorithms	different algorithms
3	Our main results
3.1	On the complexity of Two-Team Zero-Sum Games
We start this section by shoWing that computing a Nash equilibrium in tWo-team zero-sum games is
computationally hard and thus getting a polynomial-time algorithm that computes a Nash equilib-
rium is unlikely.
Theorem 3.1 (CLS-hard). Computing a Nash equilibrium in two-team zero-sum games is CLS-hard.
The main idea of the proof of Theorem 3.1 relies on a reduction of approximating Nash equilibria
in congestion games, Which has been shoWn to be complete for the interesting class of CLS, Which
contains the problem of continuous optimization. For concision, We defer the proof of the above
theorem to the paper’s supplement.
3.2	First-order methods fail to stabilize
The negative computational complexity result We proved for tWo-team zero-sum games (Theorem
3.1) does not conclude the prospect of having algorithms (learning dynamics, first-order methods)
that converge to Nash equilibria and thus can approximate them Well enough. Unfortunately, We can
even prove negative results about convergence to Nash equilibria in tWo-team zero-sum games of
Well-established methods broadly used in classic tWo-player zero-sum games.
In this section, We are going to construct a family of tWo-team zero-sum games With the property
that GDA, OGDA, EG, and OMWU fail to stabilize to Nash equilibria. This result indicates hoW
challenging and rich the setting of team zero-sum games can be and Why provable guarantees about
6
Under review as a conference paper at ICLR 2022
convergence have not been established yet. Before defining the family of two-team zero-sum games,
we prove an important theorem which states that GDA does not stabilize around mixed Nash equi-
libria. This fact is a stepping stone in constructing the family of team-zero sum games later. We
present the proof of all of the below statements in detail in the paper’s appendix.
Weakly-stable Nash equilibrium (Kleinberg et al., 2009; Mehta et al., 2015). Consider the set
of Nash equilibria with the property that if any single randomizing agent of one team is forced to
play any strategy in her current support with probability one, all other agents of the same team must
remain indifferent between the strategies in their support. This type of Nash equilibria is called
weakly-stable. Note that trivially pure Nash equilibria are weakly-stable. It has been shown that
mixed Nash equilibria are not weakly-stable in generic games5 (Kleinberg et al., 2009). We can
show that Nash equilibria that are not weakly-stable Nash are actually unstable for GDA moreover,
through standard dynamical systems machinery, that the set of initial conditions that converges to
Nash equilibria that are not weakly-stable should be of measure zero. Formally, we prove that:
Theorem 3.2 (Non weakly-stable Nash are unstable). Consider a two-team zero-sum game with
utility function of Team B (y vector) being U(x, y) and Team A (x vector) being -U (x, y). More-
over, assume that (x*, y*) is a Nash equilibrium of full support that is not weakly-stable. ItfOllOws
that the set of initial conditions so that GDA converges to (x*, y*) is of measure zero for step size
η < L where L is the Lipschitz constant of VU.
3.3	Generalized Matching Pennies (GMP)
Inspired by Theorem 3.2, in this section we construct a family of team zero-sum games so that GDA,
OGDA, and EG methods fail to converge (if the initialization is a random point in the simplex, the
probability of convergence of the aforementioned methods is zero). The intuition is to construct a
family of games, each of which has only mixed Nash equilibria (that are not weakly-stable), i.e.,
the constructed games should lack pure Nash equilibria; using Theorem 3.2, it would immediately
imply our claim for GDA. It turns out that OGDA and EG also fail to converge for the same family.
Definition of GMP. Consider a setting with two teams (Team A, Team B), each of which has
n = 2 players. Inspired by the standard matching pennies game and the game defined in Schulman
& Vazirani (2019a), we allow each agent i to have two strategies/actions that is S = {H, T } for
both teams with 24 possible strategy profiles. In case all the members of a Team choose the same
strategy say H or T then the Team “agrees” to play H or T (otherwise the Team “does not agree”).
Thus, in the case both teams “agree”, the payoff of each team is actually the payoffs for the two-
player matching pennies. If one team “agrees” and the other does not, the team that “agrees” gets
payoff ω ∈ (0, 1) and the other team gets penalty ω. If both teams fail to “agree”, both teams
get payoff zero. Let xi with i ∈ {1, 2} be the probability that agent i of Team A chooses H and
1 - xi the probability that she chooses T . We also denote x the vector of probabilities for Team
A. Similarly, we denote yi for i ∈ {1, 2} be the probability that agent i of Team B chooses H and
1 - yi the probability that she chooses T and y the probability vector.
The first fact about the game that we defined is that for ω ∈ (0, 1), there is only one Nash equilibrium
(x*, y*), which is the uniform, i.e., x； = x2 = y； = y2 = 2 for all agents i.
Lemma 3.3 (GMP has a unique Nash). The Generalized Matching Pennies game exhibits a unique
Nash equilibrium which is (x；, y；) = ((1, 1), (2, 2)).
Remark 3.4. The fact that the game we defined has a unique Nash equilibrium that is in the interior
of [0, 1]4 is really crucial for our negative convergence results later in the section as we will show
that it is not weakly-stable Nash equilibrium and the negative result about GDA will be a corollary
due to Theorem 3.2. Please also note that ifω = 1 then there are more Nash equilibria, in particular
the (0, 0), (1, 0), (0, 1), (1, 1) are also Nash equilibria (which are pure).
The following Theorem is the main (negative) result of this section.
Theorem 3.5 (GDA, OGDA, EG, and OMWU fail). Consider GMP game with ω ∈ (0, 1). Assume
that ηGDA < 4, ηOGDA < min(ω, 1), ηEG < ω , and ηomwu < min (1, ¾2) (bound on the stepsize
5Roughly speaking, games in which we add small Gaussian noise on every payoff.
7
Under review as a conference paper at ICLR 2022
Figure 4: GDA, OGDA, & EG fail to converge to a Nash Equilibrium even in average
for GDA, OGDA, EG, and OMWU methods respectively). It holds that the set of initial conditions
so that GDA, OGDA, EG, OMWU converge (stabilize to any point) is of measure zero.
Remark 3.6 (Average iterate also fails). One might ask what happens when we consider average
iterates instead of the last iterate. It is well-known fact Syrgkanis et al. (2015) that the average
iterate of no-regret algorithms converges to coarse correlated equilibria (CCE) so we expect that the
average iterate stabilizes. Nevertheless, CCE might not be Nash equilibria. Indeed we can construct
examples in which the average iterate of GDA, OGDA, and EG experimentally fail to stabilize to
Nash equilibria. In particular, we consider a slight modification of GMP; players and strategies are
the same but the payoff matrix has changed and can be found below (table on the right):
HH	HTlTH	TT
HH HT/TH TT	1,-1	ω, 一ω	-1,1
	-ω, ω	0, 0	-ω, ω
	-1,1	ω, 一ω	1,-1
	HH	HT/TH	TT
HH	2, -2	2,- 2	-2, 2
HT/TH	1 1 		 — 2 , 2	0, 0	1 1 		 — 2 , 2
TT	-1,1	1 _1 2,	2	1,-1
Table 1: GMP configurations of Theorem 3.5 (left) and Remark 3.6 (right)
Figure 4 illustrates that the average iterates of GDA, OGDA, and EG stabilize to points that are not
Nash equilibria. Note that since our method (see next subsection) converges locally, the average
iterate should converge locally to a Nash equilibrium.
3.4	Wash-out Filters & Adaptive Control
The aforementioned results indicate that to answer the tantalizing question of finding NE in two-
team zero-sum games, our machinery should be broadened outside the limits of textbook optimiza-
tion arsenal. The mainstay of this effort and our positive result is KPV-GDA method defined in
(2.2), inspired by the adaptive control toolbox and washout filters. Our main statement shows that
KPV-GDA stabilizes around any Nash equilibrium for appropriate choices of matrices K, P . The
formal theorem is given below:
Theorem 3.7 (KPV-GDA stabilizes). Consider a team zero-sum game so that the utility of Team
B is U(x, y) and hence the utility of Team A is -U(x, y) and a Nash equilibrium (x*, y*) of the
game. Moreover we assume
-V2 Xx U (x*, y*)
V2 yx U (x*, y*)
-V22yxyyUU((xx**,,yy**))	is invertible.
For any fixed step size η > 0, we can always find matrices K, P so that KPV-GDA method defined
in (2.2) converges locally to (x* , y*).
The latter statement concerns the existence of matrices K, P . Below, we provide a sufficient condi-
tion under which a simple parametrization of K, P (provably) guarantees convergence.
8
Under review as a conference paper at ICLR 2022
Theorem 3.8. Consider a two-team zero-sum game so that the utility of Team B is U (x, y) and
hence the utility of Team A is -U (x, y) and a Nash equilibrium (x*, y*) of the game. Moreover let
H. _( FxU (x*, y*) -VXy U (x*, y*) ʌ
H ∙=l VyxU(x*, y*)	VyyU(x*, y*)/
and E be the set of eigenvalues ρ of H with real part positive, that is E = {H0s eigenvalues ρ :
Re(ρ) > 0}. We assume that H is invertible and moreover
β = min R(P) + Im(P) > max Re (P = α.	(3.1)
ρ∈E	Re(ρ)	ρ∈E
We set K = k ∙ I, P = P ∙ I. There exist small enough step size η > 0 and scalar p > 0 and for
any k ∈ (-β, -α) so that KPV-GDA method defined in (2.2) with chosen K, P converges locally to
(x*,y*).
4 Experiments
In this section, we perform a series of numerical experiments to validate our theoretical findings. Our
experiment setting includes a 2-D Gaussian Mixture Model with 8 modes. Our architecture includes
8 “shallow” generators and discriminators with 2 layers of 2-16-2 ReLUs activations, compared with
a giant single-agent GAN with 4 layers of 2-128-256-1024-2 activations. Interestingly, the giant one
fails in a double sense; It demonstrates both mode-collapsing and mode-drop phenomena without
stabilizing. On the other hand, our architecture with a small number of neurons achieves to fit the
data well. We defer the discussion of the experiments with multi-generators multi-discriminator
architectures for CIFAR-10 again to the paper’s supplement.
Real data
GeTIerated data
Keal data
Qpnprnfpd data
(a) Each generator of MGAN (b) Mode Collapse of single-(c) Single-agent GAN can’t dis-
learns one mode of 8-GMM agent GANs	criminate between the modes
5 Conclusions and Open Problems
In this paper, we have presented a number of negative results about the problem of finding a Nash
equilibrium in team zero-sum games and moreover about the inability of commonly used methods
for min-max optimization such as GDA, OGDA, and EG to stabilize. We also presented a method
(called KPV-GDA) that manages to stabilize around Nash equilibria. Given these results, a number
of interesting open questions emerge.
Open Questions. One question for future consideration is the global convergence and the rates
of convergence of KPV-GDA method. We believe that the KPV-GDA converges globally for an
appropriate choice of matrices K, P . One other possible direction is to find a systematic way to get
the matrices K, P.
9
Under review as a conference paper at ICLR 2022
Reproducibility Statement
In our submission folder, we provide all the necessary additional technical materials and complete
proofs of the main draft’s statements in the appendix section. We also uploaded the code of our
experiments (Python/PyTorch/Tensorflow).
References
Isabela Albuquerque, Joao Monteiro, Thang Doan, Breandan Considine, Tiago Falk, and Ioannis
Mitliagkas. Multi-objective training of generative adversarial networks with multiple discrimina-
tors. In International Conference on Machine Learning, pp. 202-211. PMLR, 2019.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 214-
223, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium
in generative adversarial nets (gans). In International Conference on Machine Learning, pp. 224-
232. PMLR, 2017.
Yakov Babichenko and Aviad Rubinstein. Settling the complexity of nash equilibrium in congestion
games. In Samir Khuller and Virginia Vassilevska Williams (eds.), STOC ’21: 53rd Annual ACM
SIGACT Symposium on Theory of Computing, Virtual Event, Italy, June 21-25, 2021, pp. 1426-
1437. ACM, 2021.
Michael Bacharach. Interactive team reasoning: A contribution to the theory of co-operation.
Research in Economics, 53(2):117-147, 1999. ISSN 1090-9443. doi: https://doi.org/10.
1006/reec.1999.0188. URL https://www.sciencedirect.com/science/article/
pii/S1090944399901886.
Nicola Basilico, Andrea Celli, Giuseppe De Nittis, and Nicola Gatti. Computing the team-maxmin
equilibrium in single-team single-adversary team games. Intelligenza Artificiale, 11(1):67-79,
2017a.
Nicola Basilico, Andrea Celli, Giuseppe De Nittis, and Nicola Gatti. Team-maxmin equilibrium: ef-
ficiency bounds and algorithms. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 31, 2017b.
A. S. Bazanella, P. V. Kokotovic, and A. S. e Silva. On the control of dynamic systems with unknown
operating point. In 1997 European Control Conference (ECC), pp. 3434-3439, 1997. doi: 10.
23919/ECC.1997.7082644.
Yongcan Cao, Wenwu Yu, Wei Ren, and Guanrong Chen. An overview of recent progress in the
study of distributed multi-agent coordination. IEEE Transactions on Industrial informatics, 9(1):
427-438, 2012.
Andrea Celli and Nicola Gatti. Computational results for extensive-form adversarial team games.
In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.
Erick Chastain, Adi Livnat, Christos Papadimitriou, and Umesh Vazirani. Algorithms, games, and
evolution. Proceedings of the National Academy of Sciences, 111(29):10620-10623, 2014.
Constantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic) gradient descent in
min-max optimization. Advances in Neural Information Processing Systems, 31, 2018.
Constantinos Daskalakis and Ioannis Panageas. Last-iterate convergence: Zero-sum games and
constrained min-max optimization. Innovations in Theoretical Computer Science, 2019.
Constantinos Daskalakis and Christos Papadimitriou. Continuous local search. In Proceedings of
the twenty-second annual ACM-SIAM symposium on Discrete Algorithms, pp. 790-804. SIAM,
2011.
10
Under review as a conference paper at ICLR 2022
Constantinos Daskalakis, Paul W Goldberg, and Christos H Papadimitriou. The complexity of
computing anash equilibrium. SIAM Journal on Computing, 39(1):195-259, 2009.
Constantinos Daskalakis, Stratis Skoulakis, and Manolis Zampetakis. The complexity of constrained
min-max optimization. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory
of Computing, pp. 1466-1478, 2021.
Ishan Durugkar, Ian Gemp, and Sridhar Mahadevan. Generative multi-adversarial networks. arXiv
preprint arXiv:1611.01673, 2016.
Alex Fabrikant, Christos Papadimitriou, and Kunal Talwar. The complexity of pure nash equilibria.
In Proceedings of the thirty-sixth annual ACM symposium on Theory of computing, pp. 604-612,
2004.
John Fearnley, Paul W Goldberg, Alexandros Hollender, and Rahul Savani. The complexity of gra-
dient descent: CLS = PPAD ∩ PLS. In Proceedings of the 53rd Annual ACM SIGACT Symposium
on Theory of Computing, pp. 46-59, 2021.
Lampros Flokas, Emmanouil Vlatakis-Gkaragkounis, Thanasis Lianeas, Panayotis Mertikopoulos,
and Georgios Piliouras. No-regret learning and mixed nash equilibria: They do not mix. In
NeurIPS’20: The 34th International Conference on Neural Information Processing Systems,
2020.
Lampros Flokas, Emmanouil-Vasileios Vlatakis-Gkaragkounis, and Georgios Piliouras. Solving
min-max optimization with hidden structure via gradient descent ascent. In Advances in Neu-
ral Information Processing Systems 35: Annual Conference on Neural Information Processing
Systems 2021, NeurIPS 2021, December 2021, virtual, 2021.
Drew Fudenberg. Jean tirole game theory, 1991.
Angeliki Giannou, Emmanouil Vasileios Vlatakis-Gkaragkounis, and Panayotis Mertikopoulos. Sur-
vival of the strictest: Stable and unstable equilibria under regularized learning with partial in-
formation. In Mikhail Belkin and Samory Kpotufe (eds.), Proceedings of Thirty Fourth Con-
ference on Learning Theory, volume 134 of Proceedings of Machine Learning Research, pp.
2147-2148. PMLR, 15-19 Aug 2021. URL https://proceedings.mlr.press/v134/
giannou21a.html.
Natalie Gold. Introduction: Teamwork in theory and in practice. In Teamwork, pp. 1-21. Springer,
2005.
Hans W Gottinger. J. marschak and roy radner,” economic theory of teams”(book review). Theory
and Decision, 5(3):349, 1974.
Peter R Grant and Lloyd D Reid. Motion washout filter tuning: Rules and requirements. Journal of
aircraft, 34(2):145-151, 1997.
James Hannan. 4. approximation to rayes risk in repeated play. In Contributions to the Theory of
Games (AM-39), Volume III, pp. 97-140. Princeton University Press, 2016.
Corentin Hardy, Erwan Le Merrer, and Bruno Sericola. Md-gan: Multi-discriminator generative
adversarial networks for distributed datasets. In 2019 IEEE international parallel and distributed
processing symposium (IPDPS), pp. 866-877. IEEE, 2019.
Tim Hasso, Daniel Muller, Matthias Pelster, and Sonja Warkulat. Who participated in the gamestop
frenzy? evidence from brokerage accounts. Finance Research Letters, pp. 102140, 2021.
Munther A Hassouneh, Hsien-Chiarn Lee, and Eyad H Abed. Washout filters in feedback control:
Benefits, limitations and extensions. In Proceedings of the 2004 American control conference,
volume 5, pp. 3950-3955. IEEE, 2004.
Quan Hoang, Tu Dinh Nguyen, Trung Le, and Dinh Phung. Multi-generator generative adversarial
nets. arXiv preprint arXiv:1708.02556, 2017.
11
Under review as a conference paper at ICLR 2022
Quan Hoang, Tu Dinh Nguyen, Trung Le, and Dinh Phung. Mgan: Training generative adversarial
nets with multiple generators. In International conference on learning representations, 2018.
Max Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia
Castaneda, Charles Beattie, Neil C Rabinowitz, Ari S Morcos, Avraham Ruderman, et al. Human-
level performance in 3d multiplayer games with population-based reinforcement learning. Sci-
ence, 364(6443):859-865, 2019.
Jeongbin Kim, Thomas R Palfrey, and Jeffrey R Zeidel. A theory of games played by teams of
players. 2019.
R. Kleinberg, G. Piliouras, and E. Tardos. Multiplicative updates outperform generic no-regret
learning in congestion games. In STOC, 2009.
GM Korpelevich. The extragradient method for finding saddle points and other problems. Matecon,
12:747-756, 1976.
Jason D. Lee, Ioannis Panageas, Georgios Piliouras, Max Simchowitz, Michael I. Jordan, and Ben-
jamin Recht. First-order methods almost always avoid strict saddle points. Math. Program.,
176(1-2):311-337, 2019. doi: 10.1007/s10107-019-01374-3. URL https://doi.org/10.
1007/s10107-019-01374-3.
Joel Z Leibo, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, and Thore Graepel. Multi-agent
reinforcement learning in sequential social dilemmas. arXiv preprint arXiv:1702.03037, 2017.
Dan Li, Dacheng Chen, Baihong Jin, Lei Shi, Jonathan Goh, and See-Kiong Ng. Mad-gan: Multi-
variate anomaly detection for time series data with generative adversarial networks. In Interna-
tional Conference on Artificial Neural Networks, pp. 703-716. Springer, 2019.
Tianyi Lin, Chi Jin, and Michael Jordan. On gradient descent ascent for nonconvex-concave mini-
max problems. In International Conference on Machine Learning, pp. 6083-6093. PMLR, 2020.
Jakob Marschak. Elements for a theory of teams. Management science, 1(2):127-137, 1955.
H Brendan McMahan, Geoffrey J Gordon, and Avrim Blum. Planning in the presence of cost
functions controlled by an adversary. In Proceedings of the 20th International Conference on
Machine Learning (ICML-03), pp. 536-543, 2003.
R. Mehta, I. Panageas, and G. Piliouras. Natural selection as an inhibitor of genetic diversity:
Multiplicative weights updates algorithm and a conjecture of haploid genetics. In ITCS, 2015.
Panayotis Mertikopoulos, Houssam Zenati, Bruno Lecouat, Chuan-Sheng Foo, Vijay Chan-
drasekhar, and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going
the extra (gradient) mile. In ICLR’19-International Conference on Learning Representations, pp.
1-23, 2019.
Mohammad Sal Moslehian. Ky fan inequalities. CoRR, abs/1108.1467, 2011.
Thomas Nagylaki. The evolution of multilocus systems under weak selection. Genetics, 134(2):
627-647, 1993.
Arkadij SemenoVic NemiroVskij and David BorisoVich Yudin. Problem complexity and method
efficiency in optimization. 1983.
Martin A Nowak, Akira Sasaki, Christine Taylor, and Drew Fudenberg. Emergence of cooperation
and eVolutionary stability in finite populations. Nature, 428(6983):646-650, 2004.
Christos H Papadimitriou. The complexity of the lin-kernighan heuristic for the traVeling salesman
problem. SIAM Journal on Computing, 21(3):450-465, 1992.
Georgios Piliouras and Leonard J Schulman. Learning dynamics and the co-eVolution of competing
sexual species. In 9th Innovations in Theoretical Computer Science Conference (ITCS 2018),
Volume 94, pp. 59. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2018.
12
Under review as a conference paper at ICLR 2022
Leonid Denisovich Popov. A modification of the arrow-hurwicz method for search of saddle points.
Mathematical notes ofthe Academy ofSciences ofthe USSR, 28(5):845-848, 1980.
R.W. Rosenthal. A class of games possessing pure-strategy Nash equilibria. International Journal
of Game Theory, 2(1):65-67, 1973.
Tim Roughgarden. Intrinsic robustness of the price of anarchy. In Proceedings of the forty-first
annual ACM symposium on Theory of computing, pp. 513-522, 2009.
Leonard J. Schulman and Umesh V. Vazirani. The duality gap for two-team zero-sum games. Games
Econ. Behav., 115:336-345, 2019a. doi: 10.1016/j.geb.2019.03.011. URL https://doi.
org/10.1016/j.geb.2019.03.011.
Leonard J Schulman and Umesh V Vazirani. The duality gap for two-team zero-sum games. Games
and Economic Behavior, 115:336-345, 2019b.
Hassam Ullah Sheikh, Mina Razghandi, and Ladislau Boloni. Learning distributed cooperative
policies for security games via deep reinforcement learning. In 2019 IEEE 43rd Annual Computer
Software and Applications Conference (COMPSAC), volume 1, pp. 489-494. IEEE, 2019.
Herbert A Simon. Dynamic programming under uncertainty with a quadratic criterion function.
Econometrica, Journal of the Econometric Society, pp. 74-81, 1956.
Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E Schapire. Fast convergence of regu-
larized learning in games. In Advances in Neural Information Processing Systems, pp. 2989-2997,
2015.
Shichang Tang. Lessons learned from the training of gans on artificial datasets. IEEE Access, 8:
165044-165055, 2020.
Henri Theil. A note on certainty equivalence in dynamic planning. Econometrica: Journal of the
Econometric Society, pp. 346-349, 1957.
Zaghum Umar, Mariya Gubareva, Imran Yousaf, and Shoaib Ali. A tale of company fundamentals
vs sentiment driven pricing: The case of gamestop. Journal of Behavioral and Experimental
Finance, 30:100501, 2021.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, JUny-
oung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster
level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.
Yannick Viossat and Andriy Zapechelnyuk. No-regret dynamics and fictitious play. Journal of
Economic Theory, 148(2):825-842, 2013.
Emmanouil-Vasileios Vlatakis-Gkaragkounis, Lampros Flokas, and Georgios Piliouras. Poincare
recurrence, cycles and spurious equilibria in gradient-descent-ascent for non-convex non-
concave zero-sum games. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelz-
imer, Florence d,Alche-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in NeU-
ral Information Processing Systems 32: Annual Conference on Neural Information Pro-
cessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada,
pp. 10450-10461, 2019. URL https://proceedings.neurips.cc/paper/2019/
hash/6c7cd904122e623ce625613d6af337c4-Abstract.html.
John Von Neumann. Zur theorie der gesellschaftsspiele. Math, 100:295-320, 1928.
Bernhard von Stengel and Daphne Koller. Team-maxmin equilibria. Games and Economic Behavior,
21(1-2):309-321, 1997.
Hongyang Zhang, Susu Xu, Jiantao Jiao, Pengtao Xie, Ruslan Salakhutdinov, and Eric P Xing.
Stackelberg gan: Towards provable minimax equilibrium via multi-generator architectures. arXiv
preprint arXiv:1811.08010, 2018.
Youzhi Zhang and Bo An. Converging to team-maxmin equilibria in zero-sum multiplayer games.
In International Conference on Machine Learning, pp. 11033-11043. PMLR, 2020.
Youzhi Zhang, Bo An, and Jakub Cerny. Computing ex ante coordinated team-maXmin equilibria
in zero-sum multiplayer extensive-form games. arXiv preprint arXiv:2009.12629, 2020.
13
Under review as a conference paper at ICLR 2022
A	Appendix
A. 1 Further Related Work
The properties of team games have given rise to a vast corpus of literature which we cannot hope to
review here; for an appetizer, we refer the reader to Kim et al. (2019); Gold (2005) and reference
therein. On the other hand, the convergence of first-order methods to NE, like GDA, OGDA, or
Extragradient in team games (even finite ones) is nowhere near as well understood.
A notable exception to this is the case of team-maxmin equilibria (TME), a notion introduced by
von Stengel & Koller (1997). In this category of two-team zero-sum games, a team of n-players
plays defensively against a single adversarial player. TMEs provide crucial information about the
robustness since they give the team players the highest payoff they employ uncoordinated strategies.
Unfortunately, TMEs correspond to the solution of a non-linear non-convex program. (Basilico
et al., 2017b) provided heuristics with iterative LP and support enumeration methods with proved
approximation guarantees. For the case of extensive or large-sized normal form games, Basilico
et al. (2017a); Celli & Gatti (2018) introduced a relaxed notion of team-maxmin equilibrium with
coordination device (TMECor). In TMECor solutions, the team of players shares their strategies ex-
ante6 against the adversary. While TMECor shares some properties of NEs in zero-sum two-player
games (e.g., exchangeability), the proposed algorithms by (Celli & Gatti, 2018) leverages tools like
the Mixed-Integer Linear Program (MILP) that involves a large number of integer variables. To
tackle heuristically these hurdles, it (Zhang et al., 2020; Zhang & An, 2020) had been proposed
modified versions of incremental strategy generation (ISG) algorithm (See (McMahan et al., 2003))
for large-sized games.
Describing the history and the literature surrounding multi-agent GANs would take us too far afield,
so we do not attempt it. Scratching only the surface, we mention that: a) In MGAN (Hoang et al.,
2018) and MAD-GAN (Li et al., 2019), we have a mixture of generative models with asymptotically
infinite capacity networks. b) In the seminal case of MIX+GAN (Arora et al., 2017), an extra
regularization term is typically added to discourage the weights ofa mixture of generators being too
far away from uniform. c) In Stackelberg GAN, (Zhang et al., 2018) exploit the leader-followers
relation of discriminator VS generators to achieve smaller minimax gap and better Frechet metrics.
For the case of multiple-discriminators: d) In GMAN (Durugkar et al., 2016), an increased number
of agents are used to achieve higher quality images, while e) MD-GAN (Hardy et al., 2019) is
designed to generalize over different datasets and multiple tasks that are spread on multiple workers.
Interestingly, in the continuous regime, gradient flow has been recently studied under the perspec-
tive of dynamical systems. In the setting of Hidden Games, two competitive non-convex operators
join in a two-player min-max game. In Piliouras & Schulman (2018); Vlatakis-Gkaragkounis et al.
(2019), the authors exploit the bi-linear form of the hidden game to prove antithetical to conver-
gence, like spurious fixed points, cycles, and generalized high-dimensional recurrence phenomena.
Additionally, in the case of general convex-concave settings, Flokas et al. (2021) proved that local
stability can be achieved around the von-Neumann equilibria while a global convergent result can
be described for strictly convex hidden games. A crucial comparison of the hidden model with the
team games as described in our setting is the capacity & independence of the player’s choice.
While the main contribution of this work is to highlight the inherent difficulties that the above heuris-
tic architectures are cursed, our theoretical findings in team competition hint also at new optimiza-
tion schemes like KPV-GDA whose performance is illustrated in Section 4 with delicate network
architectures that contain even multiple generators and discriminators simultaneously.
A.2 Discussion about different solution concepts
In this subsection, we would like to stress some differences between the different equilibrium notions
( TMECor, TME, and per-player NE ) explaining the game-theoretic rationalism of our choice.
Initially, it is easy to see that TMECor similarly with the Coarse correlated equilibria can achieve
better social welfare but as in the case of congestion games, found in literature, their price of anarchy
6That is, the team members are allowed to discuss and agree on tactics before the game starts, but they
cannot communicate during the game.
14
Under review as a conference paper at ICLR 2022
can become significantly worse Roughgarden (2009). In order to give a better result, TMECor
requests ana priori knowledge of the game. For example, as Zhang et al. (2020) mentioned: For
example, in multiplayer poker games, a team may play against an adversary player, but they cannot
communicate and discuss their strategy during the game due to the rule.
However in many nowadays challenging AI models, the agents/players of each team do not neces-
sarily have knowledge of the actual game. Thus, the team players can only decide in advance only
about their own dynamics. Thus, the main conceptual reason that we focus on NE per player in com-
parison with the aforementioned TME relaxations is to study the interplay between the individual
and team incentives in competitive tasks. It is also a way of reasoning about games in which players
do not necessarily need to know in which team they fall into in advance.
Problems of this kind are prevalent in modern ML applications like strategic conflict resolution Leibo
et al. (2017), coordination between autonomous vehicles Cao et al. (2012) , or collaboration of agents
in defensive escort teams Sheikh et al. (2019). In this kind of games each agent is simultaneously
working towards maximizing its own payoff (local reward) as well as the collective success of the
team (global reward).
As it is stated again in Zhang et al. (2020), Celli and Gatti (2018) show that the “ex ante” coordi-
nation can be modeled using a coordination device, assuming that the adversary does not observe
any signal from the device. The team members agree on a planned strategy (e.g., a mixed strategy)
in the planning phase, and then, just before the game starts, the coordination device randomly picks
a pure joint strategy (from the planned strategy) for the team members to act upon.
Therefore, this TMECor interpretation would critically correspond to a high-level two-metaplayer
zero-sum game, where the metaplayer is the team as a whole entity with every individual player in
perfect coordination with its teammates.
Finally, an additional reason, the notion of “per player Nash equilibria” can be seen as a smoother
figure of merit for the performance of a defensive team against an adversary. Notably, in the vast
majority of the aforementioned literature, a single superpowerful adversary has been used. However,
using a single-agent adversary can describe a universally fair model.By contrast, the notion of per
player NE expresses the model hurdles of both simultaneous team intra-collaboration and inter-
competition.
B	Game Dynamics
The following algorithms are ubiquitous in the literature of min max optimization and have known
variable success under different settings. Gradient descent-ascent (GDA) is the prototypical method
upon which Optimistic Gradient Descent (OGDA), Extra Gradient (EG), and our proposed method
K, P -vaned Gradient Descent-Ascent (KPV-GDA) are built. Optimistic Multiplicative Weights Up-
date Method is the optimistic variant of the Multiplicative Weights Update Method.
B.1	Gradient Descent-Ascent
]χ(k+1) = ∏Xi{χ(k) -ηVxif(X(k),y(k))0
[y(k+r) = ∏Yj nyjk) + ηVyjf (X(R y(k))o
B.2	Optimistic Gradient Descent-Ascent
fx(k+1) = ∏Xi [x(k) — 2ηVxif (x(k), y(k)) + ηVxif (x(I), y(kT))O
[yjk+1) = ∏Yj [yjk +2ηVyjf(χ(k,y(k)) -ηVyjf(X(I),y(k-1))O
B.3	Extra Gradient Method
卜ik+2) = ∏Xi{χik) — ηVxif(χ(k),y(k))O，χ(k+1) =∏Xi{x(k) -ηVxif(χ(k+2),y(k+1 ))∣
[yjk+2) = ∏Yj{yjk) + nVyjf(X(Je),y(k))}, yjk+1) = ∏γ,{yjk) + ηVyjf(χ(k+2),W+1 ))∣
15
Under review as a conference paper at ICLR 2022
B.4	Optimistic Multiplicative Weights Update Method
(k)
xi
(k)
yj
exp (-2ηVχif(x(k),y(k))+ηVχif(x(kτ),y(kτ)))
Pj Xjk) exp ^-2ηVχi f (x(k),y(k))+ηVχi f (x(k-1) ,y(k-1)
exp "" f(x(k ,y(k))-ηVyj f (x(k-1),y(k-1)/)
Pj yjk) exp "^yj f (χ(k),y(k) )-ηVyj f (x(k-1) ,y(k-1)))
B.5	K, P -VANED GRADIENT DESCENT-ASCENT
z(k+1)
θ(k+1)
∏Z(z(k) + ηGff瑞)) + nK(z(k) - θ(k))}
ΠZnθ(k)+ηP(z(k) - θ(k))o
Notations xi(k) ( or yj(k)) stand for the strategy vector of the i-th minimizing (or j-th maximizing)
agent at time-step k. The step size is denoted as η. Operators ΠXi , ΠYj , ΠZ are the projection
operators to the corresponding simplices. The projection operator is important since the context of
operation is that of constrained optimization and remaining inside the set of feasible solutions is not
guaranteed at every step except for the case of the OMWU method.
C	Derivation of the Min-Max Objective in equation 2.5
By the definition of Variance, We get that: Var [xi] = E[x2] - (E[x∕)2 ⇔ E[x2] = Var [xi] +
(E[xi]) . More precisely for any X 〜N(μ, I), we get E[χ2] = 1 + μ].
Additionally, after calculations, We can get that:
Ez~N (0,i)[(zi + θi)2] = Var[zi + θi] +(E[& + θi])2 ===⇒
Ez~N(0,I) [(zi + θi)2] = Var [zi] + (Elzi] + E[θi]) ⇒
Ez~N (0,I )[(zi + θi)2] = Var[zi] + (0 + θi) ⇒
Eζ~N(0,I) [(zi + θi)2] = 1 + θ2
Moreover, it is easy to check that for a mixture D(μ1, σ1I, μ2, σ2I, π1, π2) of two multi-
dimensional normal distributions N(μ1, σ1I),N(μ1, σ1I) with corresponding weights π1, π2:
Va”~D[xi] = ∏ισ2 + ∏2σ2 + ∏ιμ1 + ∏2μ2,i - (∏1μ1,i + ∏2μ2,i)2
Specifically for the case ofμ1 = μ = -μ2, this equivalent with:
Varlxi] = 1 + μi2 - (π1μi - π2μi)2
Hence:
Elxi2] = Varlxi] + (π1μi - π2μi)2 = 1 + μi2
So we can apply the following manipulations in the objective (The notation lxi2] stands for the vector
that has as entries the values of the vector X squared):
max min
v,w θ,p
∙~real Dv(y) + D
W
—
-Eζ~N(0,i),ζ~N(0,1) hGp(ζ) ∙ (Dv( - Gθ(y)) + DW(Gθ(y))) +
+ (1 - Gp(Z)) ∙ (Dv(Gθ (y)) + Dw(Gθ (y)))i =
16
Under review as a conference paper at ICLR 2022
max min
v,w θ,p
∙~real hV, yi + hw, [yi2]i —
-Ez~N(0,I),Z~N(0,1) h(P + Z) ∙ (hv,-(Z + θ)i + hw, [(θi + Zi)2]〉) +
+ (1 - (p + Z)) ∙ (hv, -(Z + θ)i + hw, [( - (θi + Zi))2]i)i =
maXms Ey~real[hv, yi] + Ey~real [w, [y2]i] -
-Ez-N(0,i),ζ~N(0,1) [(p + Z) ∙ (hv, z + θi + hw, [(θi + zi)2]i) +
+ (1 - (P + Z)) ∙
-(z + θ)i + hw, [(θi + zi)2]i	=
max min V, Ey
~real y + w, Ey
~real [[yi ]] ) -
-	p ∙ (Dv, Ez~N(0,I) h(z + θ)iE + Dw, Ez~N(0,I) h[(θi + Zi)2]iE) -
-	(1 - p)( ∙ Dv, Ez~N(0,I)h-(z+θ)iE+ Dw, Ez~N(0,I) h[(θi + Zi)2]iE)=
max min 卜,∏πμι + ∏2(-μ,) +(w, [1 + μ2])
—
-	P ∙ (Dv, θE+Dw, [(θ2+I)]iE) - a - P) ∙ (Dv, -θE+Dw, [(θ2+I)]E =
max min (∏ι — ∏2)vτμ — 2pvTθ + VTθ +	Wi(μ2 — θ2)
v,w θ,p
i
C.1 Proof of Theorem 3.1
We will reduce the problem of finding a Nash equilibrium in congestion games to the problem of
finding a Nash equilibrium in two-team zero-sum games. The result then will follow since comput-
ing Nash equilibria in congestion games is CLS-hard (Babichenko & Rubinstein (2021)).
As a recent result (Fearnley et al., 2021) shows CLS is equal to the intersection to PLS and PPAD,
two important classes of total problems. PPAD captures diverse problems in combinatorics and (non-
)cooperative game theory, like the ε-approximation of a mixed Nash Equilibrium in a graphical game
or the computation of market equilibria. PLS, for “Polynomial Local Search”, captures problems of
finding a local minimum of an objective function f, in contexts where any candidate solution x has a
local neighbourhood within which we can readily check for the existence of some other point having
a lower value of f. Many diverse local optimization problems have been shown complete for PLS,
attesting to its importance. Examples include searching for a local optimum of the TSP according to
the Lin-Kernighan heuristic (Papadimitriou, 1992), and finding pure Nash equilibria in many-player
congestion games (Fabrikant et al., 2004). The complexity class CLS (“Continuous Local Search”)
was introduced by Daskalakis and Papadimitriou (Daskalakis & Papadimitriou, 2011) to classify
various important problems that lie in both PPAD and PLS. CLS is seen as a strong candidate for
capturing the complexity of some of those important problems, like the general versions of Banach’s
fixed point theorem, computation of KKT points, computation of gradient descent fixed points etc.
In our reduction, a congestion game is defined by the tuple (N; E; (Si)i∈N; (ce)e∈E) where N is
the set of agents, E is a set of resources (also known as edges or facilities), and each player i has
a set Si of subsets of E. Each strategy si ∈ Si is a set of edges (a path), and ce is a cost (negative
utility) function associated with facility e. For a strategy profile s = (s1, s2, . . . , sN), the cost of
player i is given by a(s) = Pe∈sa ce('e(s)), where 'e(s) is the number of players using e in S (the
load of edge e). It is a well-known result Rosenthal (1973) that congestion games exhibit a potential
function Φ(s), that
`e(s)
Φ(s)=XXce(j)
e∈E j=1
with the property that if any agent i changes her strategy to s0i it holds that
Φ(s0i, s-i) - Φ(si, s-i) = ci(s0i, s-i) - ci(si, s-i).
17
Under review as a conference paper at ICLR 2022
Reduction. Consider a congestion game (N; E; (Si)i∈N; (ce)e∈E) with n = |N| players and
potential function Φ. We define a team zero-sum game as follows: Team A has n players, in which
each agent i chooses strategies from Si . Team B has n players, with each agent j having only one
possible choice (singleton set of actions) call it d, i.e., these are dummy players. If players from
Team A choose strategy profile s (Team B has only one choice) then they get utility uA (s, d) =
-Φ(s). The utility members of Team B get is uB (s, d) = Φ(s).
Let x* ≡ (x；,…，xn) and (d,...,d) be a (possibly mixed) Nash equilibrium in the team zero-sum
game we defined. We shall show that (x*,…,x：) is a Nash equilibrium of the original congestion
game and the reduction will be complete. Aiming for contradiction, suppose (x1*, ..., x*n) is not a
Nash equilibrium of the original congestion game. Then there exists an agent i that can deviate from
strategy x; to Xi and decrease her expected cost. Hence We have that
0 < Es~x* [ci(s)]-Es~(.,x-i)[ci(s)]
=Es~χ* [Φ(s)] - Es~..,χ 二.)[Φ(s)] (Property of potential).
We conclude that Es~χ* [ua(s, d)]	=	-Es~χ* [Φ(s)]	<	-Es~(x*,x*.)[Φ(s)]=
Es~(x*,χ*.)[ua(s, d)] which is a contradiction since (x*,..., x*a) is a Nash equilibrium for
the team zero-sum game hence if player i deviates, her payoff (i.e., the payoff of her Team) should
not increase.
C.2 Multiplayer Matching Pennies
Below, we present the exact definition of the (2x2) two-team of two-players matching pennies that
we discuss in Sec
Team B
HH HTlTH TT
HH	-1,1	-1/2,1/2	1, -1
Team A HT/TH	1/2, -1/2	0, 0	1/2, -1/2
TT	1,-1	-1/2,1/2	-1,1
C.3 Proof of Theorem 3.2
Since (x*, y*) is not weakly-stable, there exist players i, j from the same team (say B without loss
of generality) and strategies k, l, l0 so that ifi is forced to play k, then j’s best response is l and that
gives larger payoff than another strategy l0 in her support. Formally it holds that (by multi-linearity
ofU)
∂2U(x* y*)	∂2U(x* y*)
(payoff if i, j choose k,l)   -----> >  ---------- (PayOffif i,j choose k,l0),	(C.1)
∂yik∂yjl	∂yik ∂yjl0
and also "Udy： ) = "Udxjy ) (*). We shall show that NQyyU(x*, y*) has a strictly positive
eigenvalue tangent in the product of simplices (note that if we were working with A, we would
show that NQxxU(x*, y*) has a strictly negative eigenvalue). Consider a vector of size Pi |Si|,
(where |Si| is the cardinality of the strategy space of agent i in Team B) which has 1 at coordinates
(i, k), (j, l), -1 at coordinate (j, l0) and from which we subtract yi*; we denote by v the resulting
vector. We shall show that v>NQxxU (x*, y*)v < 0.
18
Under review as a conference paper at ICLR 2022
By multilinearity of U it follows that dydU- = 0 (**) for all agents j and strategies s, s0 and the
same is true for x variables (team A). We conclude that
1	>v2 U( *	*) = a2U(χ*,y： _ IU(χ*, y) _ X * (d2U(x*,y*Q _ IU(χ*,y： A
2	yy( , y ) =	∂yik∂yji	∂yik∂j	与 yis V ∂yis∂yji	∂yia∂yjv
=∂2U(x*, y*) - ∂2U(x*, y*) - (∂U(x*, y*) - ∂U(x*, y*))
∂yik∂yji ∂yik ∂yjio 卜 ∂yji	dy"	)
(=)∂2U(x*, y*) - ∂2U(x*, y*) (c>1) 0
∂yik ∂yji	∂yik ∂yji	'
Therefore Vyy U (x*, y*) has a positive eigenvalue and as a result
R._( -VXxU(x*, y*)	0	)	(C2
R :=	0	V2yyU(x*,y*)	(C.2)
must have a positive and a negative eigenvalue (since the trace is zero).
We consider the Jacobian of the GDA dynamics at (x*, y*). The corresponding matrix is the fol-
lowing:
JGDA = I + η
-V2xxU (x*, y*)
V2yxU(x*,y*)
-V2xyU(x*,y*) )
V2yyU(x*,y*)
(C.3)
We will show that JGDA has an eigenvalue (possible complex) with an absolute value greater than
one. It suffices to show that JGDA - I has an eigenvalue with positive real part (because then JGDA
would have an eigenvalue with real part greater than 1 and hence magnitude greater than one). Due
to (**), we get that JGDA - I has trace zero. To reach contradiction suppose that no eigenvalue of
JGDA - I has positive real part, then all eigenvalues of JGDA - I should be imaginary or zero. But an
imaginary eigenvalue (that is not zero) also results in an eigenvalue with magnitude greater than one
for JGDA, therefore all eigenvalues of JGDA - I should be zero. We use Ky Fan inequalities which
states that the sequence (in decreasing order) of the eigenvalues of 2 (H + H>) majorizes the real
part of the sequence of the eigenvalues of H (see Moslehian (2011), page 4) for any matrix H. We
choose H = 1 ∙ ( Jgda - I). Since R has both a negative and a positive eigenvalue, We get that H
has an eigenvalue with negative real part. The claim follows since H has trace zero, thus it should
have an eigenvalue with positive real part as well.
We conclude that JGDA has an eigenvalue with an absolute value greater than one. Using Theorem
2.2 in Daskalakis & Panageas (2018), it occurs that the set of initial conditions so that GDA converge
to (x* , y* ) is of measure zero (for the particular choice of the step size).
D	Proof of Lemma 3.3
Firstly, we start with the min-max formulation of GMP game:
min max -x1x2y1y2 - (1 - x1)(1 - x2)(1 - y1)(1 - y2) + x1x2(1 - y1)(1 - y2)+
x∈[0,1]2 y∈[0,1]2
+ (1 - x1)(1 - x2)y1y2 +ω (1 - x1x2 - (1 - x1)(1 - x2)) (y1y2 + (1 - y1)(1 - y2)) -
- ω (1 - y1y2 - (1 - y1)(1 - y2)) (x1x2 + (1 - x1)(1 - x2))
or after simplification, it is equivalent with
min max (ω+1)(x1+x2)+(1-ω)(y1+y2) -(x1+x2)(y1+y2) -2ωx1x2 +2ωy1y2. (D.1)
x∈[0,1]2 y∈[0,1]2
Remark D.1. Please note that the objective in the min-max is multi-linear and the degree of each
variable in every summand is at most one (total degree is 2). Moreover note that due to the non
convexity-concavity of the function above, the max-min is not equal to the min-max.
Let (x1*, x2*, y1*, y2*) be a Nash equilibrium. Assuming x1*, x*2, y1*, y2* ∈ (0, 1) from (D.1) and first-
order conditions we get the system of equations
1. ω + 1 - y1* - y2* - 2ωx*2 = 0,
19
Under review as a conference paper at ICLR 2022
2.	ω + 1 — y； — y2 — 2ωx1 = 0,
3.	1 — ω — x； — x； + 2ωyg = 0,
4.	1 — ω — x； — x； + 2ωyj = 0.
Combining the first two equations, we have x1； = x；2 and combining the last two it follows y1； = y2； .
Dividing equation one by ω and subtracting three, it follows that ω- — 2y^ + ω — 2ωy； = 0. Hence
We conclude that y； = 1. As a result y2 = 2 and substituting in first equation x； = x2 =1.
•	Assume now that x；- = 0 and x2；, y-；, y2； ∈ (0, 1). Following the same idea, now only
equations 2, 3,4 hold and instead of the first we have the constraint ω+1—y； — y； —2ωx2 ≥
0. From 3, 4 we conclude that y； = y； and using 2 it holds that y； = y； = 1+ω. Using
3 follows that 1 — ω — x；2 + ω(ω + 1) = 0. Thus x2； = 1 + ω2 > 1 (this is not possible
because x；； ∈ [0, 1]).
•	Consider the case that x1； = 1 and x；；, y1；, y；； ∈ (0, 1). Only equations 2, 3, 4 hold and
instead of the first we have the constraint ω + 1 — y； — y； — 2ωx； ≤ 0. From 3, 4 we
conclude that y- = y； and using 2 it holds that y； = y； = --ω. Using 3 follows that
—ω — x； + ω(1 — ω) = 0. Thus x； = —ω2 < 0 (this is not possible because x； ∈ [0,1]).
By symmetry the same happens when x；； = 0 or x；； = 1 and x1；, y1；, y；； ∈ (0, 1).
•	Case X- = x； = 0 and y；,y； ∈ (0,1). Using 3, 4 we get y； = y； = ω2-1 < 0 (this is not
possible).
•	Case X- = x； = 1 and y；,y； ∈ (0,1). Using 3, 4 we get y； = y； = ω2+- > 1 (this is not
possible).
•	Case X- = 0 and x； = 1 and y；,y； ∈ (0,1). Using 3, 4 we get y； = y； = 1. Moreover
one becomes ω — 2sx； ≥ 0 and two ω — 2ωx- ≤ 0, that is X- ≥ ； and x； ≤ ；
(contradiction). The case x1； = 1 and x；； = 0 and y1；, y；； ∈ (0, 1) is symmetric.
Similarly one can consider the case where the y team plays pure and x1；, x；； ∈ (0, 1). One can also
check that all possible pure strategy profiles are not Nash equilibria.
E Proof of Theorem 3.5
We split the proof in 3 parts. Before we start the proof, note that the Hessian of U (E.4) has infinity
norm less than 4 (since ω ∈ (0, 1)), so U has gradient Lipschitz with L ≤ 4. Thus for the rest of the
proof for GDA, we choose ηOGDA < 4.
GDA. For GDA the proof will be straightforward. We will show that (x1；, x；；, y1；, y；；) is a
weakly Nash equilibrium. Then the claim about GDA will follow because of Lemma 3.3, Theorem,
3.2 and Remark 2.2.
Assume that player x1 fixes his strategy to x- = 0. and yι, y； keep their strategy (；, 1). We shall
show that x； is not indifferent in his support and would like to change his mixed strategy x； = ； to
pure. When x1 = 0 and y- = y； = ； the payoff ofTeam A (x variables) becomes —sx； — 1 + ；.
Since ω ∈ (0,1), x； prefers to play x； = 0 (instead of - she had). We conclude that (；, 1, 1, -) is
not a weakly-stable Nash equilibrium.
OGDA. The Jacobian of the update rule of OGDA dynamics (use the same machinery of
Section 3 in Daskalakis & Panageas (2018)) is the following:
	(	I — 2ηOGDAVXχU	—2ηOGDAVXy U I + 2ηOGDAVyy U 0	ηOGDAVXχU	ηOGDAVXyU ∖	
JOGDA =		2ηOGDAVyχU I		一ηOGDAV,χU 0	—ηOGDA VyyU 1 0	J	, (E.1)
	∖	0	I	0	0	
20
Under review as a conference paper at ICLR 2022
where U is the payoff of Team B (max) and -U is the payoff of team A. Substituting for MPG
payoff at Nash equilibrium, we have
vXxU =(-2ω -2ω), /U =(2)
2(ω), Vxy U =(-1
-1
-1
The corresponding Jacobian matrix becomes:
JOGDA
	(	0 4ω	4ω 0	2 2	2 2	0 -2ω	-2ω 0	-1 -1	-1 -1	∖
/ I 0 0 0 ∖		-2	-2	0	4ω	1	1	0	-2ω	
0 I 0 0 I		-2	-2	4ω	0	1	1	-2ω	0	
I 0 0 0	+ ηoGDA		0	0	0	0	0	0	0	0	
∖ 0 I 0 0 /		0	0	0	0	0	0	0	0	
		0	0	0	0	0	0	0	0	
	∖	0	0	0	0	0	0	0	0	/
(E.2)
It turns out that matrix (E.2) has characteristic polynomial (help of Mathematica)
π(λ) = (4(1 + ω2)ηO2GDA(1 - 2λ)2 + (λ - 1)2 λ2 - 4ωηOGDA λ(1 - 3λ + 2λ2))
∙((λ - 1)λ + 2ωηoGDA(2λ - 1))2.
(E.3)
A root of π(λ) is 1(1 + 4ηoGDAj + 4ωηoGDA + P — 16*GDA + 32jωηOGDA + 16ω2ηOGDA),
which is in absolute value greater than one for 0 < ηOGDA ≤ ω It is also easy to see the Hessian of
U, that is:
/	0	-2ω	-1
(VxxU	VxyU	A	_	-2ω	0	-1
IVyxU	VyyU	厂-1	-1	0
-1	-1	2ω
」	(E.4)
is invertible for ω ∈ (0,1) (this is true as the eigenvalues are -2ω, 2ω, -2√1 + ω2, 2√Γ+^ω2,
non of which is zero). From Theorem 3.2 in Daskalakis & Panageas (2018) follows that the
initial conditions so that OGDA converges to the Nash equilibrium is of measure zero for step size
ηOGDA < 2L ≤ 1 (where L is the LiPSchitz constant of VU). We choose "ogda ≤ min( 1 ,ω) and
the claim follows.
EG. The Jacobian of EG dynamics comPuted at the Nash equilibrium (fixed Point) is given
below (made use of chain rule):
J —I" ( -VxxU(χ*,y*)
JEG = I+ηEG ( VyxU(χ*, y*)
-VxyU(x*, y*) V n2 ( -VxxU(x*, y*)
VyyU (x*, y*) +ηEG VyxU (x*, y*)
-V2xyU(x*, y*)
V2yyU(x*,y*)
(E.5)
We substitute with the values and we get
		(	0	2ω	1	1	∖	(	4ω2 - 2	-2	4ω	4ω
JEG	二 I+ηEG		2ω -1 -1	0 -1 -1	1 0	2ω	I +η2G		-2 -4ω	4ω2 - 2 -4ω	4ω -2+4ω2	4ω -2
		∖			2ω	0	/	∖	-4ω	-4ω	-2	-2+4ω2
2
(E.6)
The eigenvalues of JEG are 1 + ηEG(-2ω + 4ηEGω2), 1 + ηEG(-2ω + 4ηEGω2),
1 + ηEG (-4^eg + 2ω + 4ηEGω2 - 2，一1 一 8nEGω - 16忧6分2) , and
1 + ηEG (-4%g + 2ω + 4ηEGω2 + 2，-1 - 8%Gω - 16^6-2) . If -4%g + 2ω + 4%Gω2 > 0
then JEG will have an eigenvalue with absolute value greater than one. A sufficient condition for that
is ηEG ≤ ω2. Finally, for the same choice of stepsizes We have that JEG is invertible for all (x, y),
hence the EG dynamics is a local diffeomorPhism. From Theorem 2 in arxiv version of Lee et al.
(2019) the claim for EG method follows.
OMWU. The Jacobian of the update rule of OMWU dynamics (use the same machinery of Section
3 in Daskalakis & Panageas (2019)) is the following:
21
Under review as a conference paper at ICLR 2022
	/	1	ηω	η	η
		ηω	1	η	η
		-η	-η	1	ηω
JOMWU =		-η 1	-η 0	ηω 0	1 0
		0	1	0	0
		0	0	1	0
	∖	0	0	0	1
O22220 Ooo
巴 21 1 1 1 1
O22220 Ooo
-η \
2 ʌ
—~
ηω
---
2
0
0
0
0
(E.7)
—
—
22^- 2
0
It turns out that matrix (E.7) has characteristic polynomial (help of Mathematica)
π(λ) = — ((4 + ω2)η2(1 - 2λ)2 + 4(λ - 1)2λ2 - 4ωηλ(1 - 3λ + 2λ2)) •
(2(λ - 1)λ + ωη(2λ - 1))2 .
(E.8)
One root of the polynomial π(λ) above is 1 (1 + 2ηj + ηω + ʌ/l - 4η2 + 4η2ωj + η2ω2). As in
the analysis of OGDA, it turns out that for 0 < η < ω the aforementioned root has absolute value
greater than one.
Therefore We conclude that the Nash equilibrium where each agent plays (1,11) is repelling (and
thus the fact that the set of initial conditions so that OMWU converges to that particular fixed point
is of measure zero is derived from standard arguments from Lee et al. (2019). To conclude the proof
we need to exclude that OMWU will stabilize in other fixed points. Note that OMWU can stabilize
to points that are not Nash equilibria (e.g., all pure strategy profiles are fixed points). To exclude
such stabilization, the trick is that if OMWU stabilizes to a point, it should be (approximate coarse
correlated equilibrium where the approximation depends on the size of the stepsize). However, none
of the pure strategy profiles is a ω∕2- approximate coarse correlated equilibrium, so OMWU does
not stabilize. Hence if we choose 0 < η < ω∕2, OMWU does not stabilize.
F Proof of Theorem 3.8
We first compute the Jacobian of KPV-GDA dynamics (2.2) where we have eliminated one variable
per agent (to avoid using the projection operator). The Jacobian has the following form:
k k I+ ( FxUX,y*) -vyχUX,y*) ʌ
JKPV-GDA = I + η	∙ I +l VxyU(x*, y*)	<U(x*, y*))
P ∙ I
(F.1)
It suffices to show that the spectral radius of JKPV-GDA is less than one for step size η small enough.
We first show the claim below
Claim F.1. Let J = I+ηM where I is the identity matrix. Suppose that M has all of its eigenvalues
with a real part that is negative. Then, there exists an interval (0, η0) so that if η ∈ (0, η0) then J
has all of its eigenvalues with an absolute value less than one.
Proof. Let a% + bi ∙ j be an eigenvalue of M such that a% < 0. Assume that η < 一 a⅛2, then the
ai + i
corresponding eigenvalue of J is 1 + η(ai + bi ∙ j), the magnitude of which is (1 + ηai)2 + η2b2 =
1 + 2ηai + η2(ai2 + bi2) < 1 since -2ai > η(ai2 + bi2) (by assumption). Hence we can choose η0 to
be minia⅛ >0.
□
Using Claim F.1, we conclude that as long as the matrix M below has eigenvalues with real part
negative then (x*,y*) is attracting:
k k I , ( -VxxU(x*,y*) -VyxU(x*,y*) λ I
M := I k ∙ I +〈 VxyU(x*, y*)	VyyU(x*, y*) ) -k . I
p ∙ I	-p ∙ I
(F.2)
22
Under review as a conference paper at ICLR 2022
By setting M11, M12, M21, M22 as the corresponding block matrices we compute the characteristic
polynomial of M . It holds
det(M - λI)	det	M11	M12	- λI	= det	M11 — λI	-kI
		M21	M22			pI	-pI -λI
det	(M11-λI-p+λ Il	-kI	-
	0	-pI — λI
+ pkpλ) i) det(Tp + λH).
det
(F.3)
(F.4)
(F.5)
det(M - λI) has roots at λ = -p and when
det
Therefore, if M has all eigenvalues with real part negative, we must have p > 0. Let ρ be an
eigenvalue of
-VxxU (x*, y*)
Vxy U (x*, y*)
-VyxU (x*, y*)
V2yyU(x*,y*)
. It must hold that
λ + pk+λ - k = ρ.
(F.6)
We need to investigate under what assumptions λ will have real part negative. We expand (F.6) and
we get
λ2 + λ(p - k - ρ) - ρp = 0.	(F.7)
We solve Equation (F.7) and we get
-p + k + P ± '(p - k - Py + 4ρp
λι,2 = ---------------5----------------
(F.8)
We provide sufficient conditions so that Re(λ1,2) < 0. We consider the following cases:
• P is real and negative. In this case observe that forp = 0 we have that λ1,2 = k + P and 0.
Note that for k < 0, the first eigenvalue has real part negative and the second eigenvalue is
zero. We compute the first derivative at p = 0 and this gives 2 (T + |k+p1)= _______k__P ^<
0. Therefore the λ1,2 as a function ofp is strictly decreasing at zero. We conclude that for
p sufficiently small and positive, both eigenvalues of M will be real and negative. Hence
if an eigenvalue of H is real and negative then for p sufficiently small positive and k < 0
both eigenvalues of M will be negative.
• P is complex with |Im(P)| 6= 0. In this case observe that for p = 0 we have that λ1,2 =
k + P and 0. Ifwe choose min(0, -Re(P)) > k, then the first eigenvalue will have real part
negative. Now using the same idea as for the first case, we compute the first derivative of
the real part of λ1,2 as a function ofp and we get
1	( — I ± k2 - Re(P)2 — Im(P)2 )
2	1	(k + Re(P))2 + Im(P)2 4
The expression above is negative as long as -Re(P)2 -Im(P)2 - kRe(P) < 0. The equation
is trivially satisfied when Re(P) < 0 since k is chosen to be negative. Assume Re(P) > 0
(if it is zero then the above is trivially true since the imaginary part is non-zero). It occurs
IpI2
that the inequality above is true when k > 一 Re(P). Since k is chosen to be smaller than
min(0, -Re(P)), it suffices to show that
|P|2
-Re(P)) > - R()is satisfied by our assumptions.
The above is true as long as Im(P) 6= 0. Let E be the set of eigenvalues P of H with real
part positive (and non-zero imaginary part by assumption). There exists a choice for η, k, p,
in which η, p > 0 and sufficiently small and k is negative and chosen to be
|P|2
min ——-- > —k > max Re(0).
ρ∈E Re(P)	ρ∈E
23
Under review as a conference paper at ICLR 2022
G CIFAR- 1 0
CIFAR-10 is a well-established testbed for various GAN architectures. It contains 10 balanced
classes of images of different objects.
Just to further illustrate the merits of multi-agent GANs, we offer a selection of images generated
by WGAN-GP Arjovsky et al. (2017) and the MGAN Hoang et al. (2017).
In the pictures provided by the WGAN-GP we see that across iterations, the generated samples tend
to cover only certain classes of the dataset while it takes longer for the samples to become realistic.
On the other hand, the MGAN architecture from an early stage provides with diverse samples that
tend to be more realistic from early on.
Figure 6: MGAN after 300 iterations
24
Under review as a conference paper at ICLR 2022
Figure 7: WGAN after 10000 iterations
Figure 8: WGAN after 15000 iterations
25
Under review as a conference paper at ICLR 2022
Figure 9: WGAN after 25000 iterations
26