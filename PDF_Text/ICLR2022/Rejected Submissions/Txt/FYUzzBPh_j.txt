Under review as a conference paper at ICLR 2022
Communicating via
Markov Decision Processes
Anonymous authors
Paper under double-blind review
Ab stract
We consider the problem of communicating exogenous information by means of
Markov decision process trajectories. This setting, which we call a Markov cod-
ing game (MCG), generalizes both source coding and a large class of referen-
tial games. MCGs also isolate a problem that is important in decentralized con-
trol settings in which cheap-talk is not available—namely, they require balancing
communication with the associated cost of communicating. We contribute a theo-
retically grounded approach to MCGs based on maximum entropy reinforcement
learning and minimum entropy coupling that we call greedy minimum entropy
coupling (GME). We show both that GME is able to outperform a relevant base-
line on small MCGs and that GME is able to scale efficiently to extremely large
MCGs. To the latter point, we demonstrate that GME is able to losslessly commu-
nicate binary images via trajectories of Cartpole and Pong, while simultaneously
achieving the maximal or near maximal expected returns, and that it is even capa-
ble of performing well in the presence of actuator noise.
1	Introduction
This work introduces a novel problem setting called Markov coding games (MCGs). MCGs are
two-player decentralized Markov decision processes (Oliehoek et al., 2016) that proceed in four
steps. In the first step, one agent (called the sender) receives a special private observation (called
the message), which it is tasked with communicating. In the second step, the sender plays out an
episode of a Markov decision process (MDP). In the third, the other agent (called the receiver)
receives the sender’s MDP trajectory as its observation. In the fourth, the receiver estimates the
message from the received trajectory. The shared payoff to the sender and receiver is a weighted
sum of the cumulative reward yielded by the MDP and an indicator specifying whether or not the
receiver correctly decoded the message.
Among the reasons that MCGs are of interest is the fact that they generalize other important settings.
The first of these is referential games. In a referential game, a sender attempts to communicate a
message to a receiver using cheap talk actions—i.e., communicatory actions that do not have exter-
nalities on the transition or reward functions. Referential games have been a subject of academic
interest dating back at least as far as Lewis’s seminal work Convention (Lewis, 1969). Since then,
various flavors of referential games have been studied in game theory (Skyrms, 2010), artificial life
(Steels, 2013), evolutionary linguistics (Smith, 2002), cognitive science (Spike et al., 2017), and
machine learning (Lazaridou et al., 2018). MCGs can be viewed as a generalization of referential
games to a setting where we drop the often unrealistic assumption that the sender’s actions do not
incur costs.
A second problem setting generalized by MCGs is source coding (MacKay, 2002). In source coding
(also known as data compression) the objective is to construct an injective mapping from a space of
messages to the set of sequences of symbols (for some finite set of symbols) such that the expected
output length is minimized. Source coding has a myriad of real world applications involving the
compression of images, video, audio, and genetic data. MCGs can be viewed as a generalization
of the source coding problem to a setting where the cost of an encoding may involve complex
considerations, rather than simply being equal to the sequence length.
Yet another reason to be interested in MCGs is that they isolate an important subproblem of decen-
tralized control. In particular, achieving good performance in an MCG requires the sender’s actions
1
Under review as a conference paper at ICLR 2022
to simultaneously perform control in an MDP and communicate information (i.e., to communicate
implicitly). This presents a challenge due to the fact that approximate dynamic programming, the
foundation for preeminent approaches to constructing control policies (Sutton & Barto, 2018), is ill
suited to constructing communication protocols because their values depend on counterfactuals. In
other words, the information conveyed by an action depends on the policy at other contemporaneous
states, violating the locality assumption of approximate dynamic programming approaches.
To address MCGs, we propose a theoretically grounded algorithm called greedy minimum entropy
coupling (GME). GME leverages a union of maximum entropy reinforcement learning (MaxEnt
RL) (Ziebart et al., 2008) and minimum entropy coupling (MEC) (Kovacevic et al., 2015). The
key insight is that maximizing the returns of the MDP can be disentangled from learning a good
communication protocol by realizing that the entropy of a policy corresponds (in an informal sense)
to its capacity to communicate. GME leverages this insight in two steps. In the first step, GME
constructs a MaxEnt policy for the MDP, balancing between maximizing expected return and max-
imizing cumulative conditional entropy. In the second step, which occurs at each decision point,
GME uses MEC to pair messages with actions in such a way that the sender selects actions with the
same probabilities as the MaxEnt RL policy (thereby guaranteeing the same expected return from
the MDP) and the receiver’s uncertainty about the message is greedily reduced as much as possible.
To demonstrate the efficacy of GME, we present experiments for MCGs based on a gridworld,
Cartpole, and Pong (Bellemare et al., 2013), which we call CodeGrid, CodeCart, and CodePong, re-
spectively. For CodeGrid, we show that with a message space in the 10s or 100s, GME significantly
is able to outperform a relevant baseline. For CodeCart and CodePong, we use a message space of
binary images and a uniform distribution over messages, meaning that a randomly guessing receiver
has an astronomically small probability of guessing correctly. Remarkably, we show that GME is
able to achieve an optimal expected return in Cartpole and Pong while simultaneously losslessly
communicating images to the receiver, demonstrating that GME has the capacity to be scaled to
extremely large message spaces and complex control tasks. Moreover, we find that the performance
of GME decays gracefully as the amount of actuator noise in the environment increases.
2	Related Work
The works that are most closely related to this one can be taxonomized as coming from literature on
referential games, source coding, multi-agent reinforcement learning, and diverse skill learning.
Referential Games Among work on referential games, Foerster et al. (2016)’s work is perhaps most
similar in that it is concerned with directly optimizing the performance ofa communication protocol.
They propose DIAL, an algorithm that optimizes the sender’s protocol by performing gradient ascent
through the parameters of the receiver. Foerster et al. show that DIAL outperforms methods based
on independent Q-learning on a variety of communication tasks. However, DIAL-based approaches
are not directly applicable to MCGs, as they would require differentiating through trajectories.
Coding Another body of related work concerns extensions of the source coding problem. Length
limited coding (Larmore & Hirschberg, 1990) considers a problem setting in which the objective
is to minimize the expected sequence length (as before), subject to a maximum length constraint.
Coding with unequal symbol costs (Golin et al., 2002; Iwata et al., 1997) considers the problem in
which the goal is to minimize the expected cumulative symbol cost of the sequence to which the
message is mapped. The cost of a symbol may differ from the cost of other symbols arbitrarily,
making it a strictly more general problem setting than standard source coding (which can also be
thought of as minimizing cumulative symbol cost with equally costly symbols). Both length limited
coding and coding with unequal costs are subsumed by Markov coding games. And while existing
algorithms for both standard source coding and the extensions above are well-established and widely
commercialized, they are unable to address the more general MCG setting.
MCGs are also related to finite state Markov channel settings (Wang & Moayeri, 1995). In such
settings, the fidelity of the channel by which the sender communicates to a receiver is controlled by
a Markov process, which, in contrast to our work, transitions independently of the sender’s decisions.
Another related setting is intersymbol interference, where the sender’s previously selected symbols
(i.e., actions) may cause interference with subsequently selected symbols, making them less likely
to be faithfully transmitted to the receiver (Lathi, 1998). MCGs differ from both Markov channel
2
Under review as a conference paper at ICLR 2022
and intersymbol interference settings in that the Markov system controls the cost paid by the sender,
rather than interfering with the quality of the channel. MCGs are more resemblant of a setting in
which the channel is reliable, but subject to natural variation in costs, such as based on weather or
third party usage, as well as variation based on the sender’s own usage.
Multi-Agent Reinforcement Learning A third related area comes from MARL literature. Strouse
et al. (2018) investigate directly embedding a reward for taking actions with high mutual information
into policy gradient objectives. They find that this approach can improve expected return in coop-
erative settings with asymmetric information. The baseline for our CodeGrid experiments loosely
resembles Strouse et al.’s algorithm. More recently, Bhatt & Buro (2021) investigate an alternative
approach whereby the sender’s behavioral policy deterministically selects the action that maximizes
the receiver’s posterior probability of the correct message, when computed using the target policy.
They show that this modification empirically yields significantly improved convergence properties
as compared to other variations of independent reinforcement learning. However, this approach is
not directly applicable to settings in which a single action must be used for both communication and
control.
Diverse Skill Learning A fourth area of related research is that of diverse skill learning (Eysenbach
et al., 2019). Eysenbach et al. (2019) propose an unsupervised learning method for discovering
diverse, identifiable skills. Their objective, called DIAYN, seeks to learn diverse, discriminable
skills. This paradigm resembles our work in the sense that skills can be interpreted as messages and
discriminability can be interpreted as maximizing the mutual information between the skill and the
state. The baseline used in our CodeGrid experiments can also be viewed as an adaptation of an
idealized version of DIAYN to the MCG setting.
3	Background and Notation
We will require the following background and notation material to introduce Markov coding games
and greedy minimum entropy coupling.
Markov Decision Processes To represent our task formalism, we use finite Markov decision pro-
cesses (MDPs). We notate MDPs using tuples hS, A, R, Ti where S is the set of states, A is the
set of actions, R : S × A → R is the reward function, and T : S × A → ∆(S) is the transition
function. An agent’s interactions with an MDP are dictated by a policy π : S → ∆(A) mapping
states to distributions over actions. We focus on episodic MDPs, meaning that after a finite number
of transitions have occurred, the MDP will terminate. The history of states and actions is notated
using h = (s0, a0, . . . , st). We use the notation R(h) = Pj R(sj, aj) to denote the amount of
reward accumulated over the course of a history. When a history is terminal, we use z to notate it,
rather than h. The objective of an MDP is to determine a policy arg maxπEπR(Z) yielding a large
cumulative reward in expectation.
Entropy To help us quantify the idea of uncertainty, we introduce entropy. Symbolically, the entropy
of a random variable X is H(X) = -E logP(X). Because the logarithm function is concave, the
entropy of X is maximized when the mass of PX is spread as evenly as possible and minimized
when the mass of PX is concentrated at a single point.
In the context of decision-making, entropy can be used to describe the uncertainty regarding which
action will be taken by an agent. When a policy spans multiple decision-points, the uncertainty
regarding the agent’s actions given that the state is known is naturally described by conditional
entropy. Conditional entropy is the entropy of a random variable, conditioned upon the fact that the
realization of another random variable is known. More formally, conditional entropy is defined by
H(X | Y) = H(X,Y) -H(Y) where the joint entropy H(X, Y) = -E logP(X,Y) is defined as
the entropy of (X, Y ) considered as a random vector.
In some contexts, it is desirable for a decision-maker’s policy to be highly stochastic. In such cases,
an attractive alternative to the expected cumulative reward objective is the maximum entropy RL
objective Ziebart et al. (2008) maxπ Eπ [Pt R(St, At) + αH(At | St)], which trades off between
maximizing expected return and pursuing trajectories along which its actions have large cumulative
conditional entropy, using the temperature hyperparameter α.
3
Under review as a conference paper at ICLR 2022
Figure 1: An approximate minimum entropy coupling. Given marginal distributions PX and PY ,
minimum entropy coupling constructs the a joint distribution PX,Y having minimal joint entropy.
In other words, it finds a joint distribution which allows to encode as much information as possible
about X into a given marginal distribution PY .
Mutual Information A closely related concept to entropy is mutual information. Mutual informa-
tion describes the strength of the dependence between two random variables. The greater the mutual
information between two random variables, the more the outcome of one affects the conditional
distribution of the other. Symbolically, mutual information is defined by I(X; Y ) = H(Y )-H(Y |
X) = H(X)-H(X | Y ). From this definition, we see explicitly that the mutual information of
two random variables can be interpreted as the amount of uncertainty about one that is eliminated
by observing the realization of the other.
Mutual information is important for communication because we may only be able to share the real-
ization of an auxiliary random variable, rather than that of the random variable of interest. In such
cases, maximizing the amount of communicated information amounts to maximizing the mutual
information between the auxiliary random variable and the random variable of interest.
The Data Processing Inequality The independence relationships among random variables play an
important role in determining their mutual information. If random variables X and Z are con-
ditionally independent given Y (that is, X ⊥ Z | Y ), the data processing inequality states that
I(X; Y ) ≥ I(X; Z). Less formally, the data processing inequality states that if Z does not provide
additional information about X given Y , then the dependence between X and Z cannot be stronger
than the dependence between X and Y .
Minimum Entropy Coupling In some cases, we may wish to maximize the mutual information
between two random variables subject to fixed marginals. That is, we are tasked with determin-
ing a joint distribution PX,Y that maximizes the mutual information I(X; Y ) between X and Y
subject to the constraints that PX,Y marginalizes to PX and PY , where PX and PY are given
as input. Invoking the relationship between mutual information and joint entropy I(X; Y ) =
H(X) + H(Y ) - H(X, Y ), we see that this problem is equivalent to that of minimizing the joint
entropy of X and Y . As a result, this problem is often referred to as the minimum entropy coupling
problem. A visual example is shown in Figure 1. While minimum entropy coupling is NP-hard
KovaceVic et al. (2015), Cicalese et al. (2019) recently showed that there exists a polynomial time
algorithm that is suboptimal by no more than one bit.
4	Markov Coding Games
We are now ready to introduce Markov coding games (MCGs). An MCG is a tuple
h(S, A, T, R), M, μ, Zi, where (S, A, T, R) is a Markov decision process, M is a set of messages,
μ is a distribution over M, and Z is a non-negative real number. An Markov coding game proceeds
in the following steps:
1.	First, a message M 〜 μ is sampled from the prior over messages and revealed to the sender.
2.	Second, the sender uses a message conditional policy to generate a trajectory Z 〜(T, ∏∣m).
3.	Third, the sender’s terminal trajectory Z is given to the receiver as an observation.
4.	Fourth, the receiver uses a trajectory conditional policy to estimate the message M 〜∏∣z(Z).
The objective of the agents is to maximize the expected weighted sum of the return and the accuracy
of the receiver,s estimate E [r(Z) + ZI[M = M]]. Optionally, in cases in which a reasonable
distance function is available, we allow for the objective to be modified to minimizing the distance
4
Under review as a conference paper at ICLR 2022
between the message and the guess d(M, M), rather than maximizing the probability that the guess
is correct.
Figure 2: A depiction of the structure of MCGs. First, the sender is given a message. Second,
the sender is tasked with a MDP, unrelated to the message. Third, the receiver observes the sender’s
trajectory. Fourth, the receiver estimates the message.
4.1	An Example
A payoff matrix for a simple MCG is shown in
Figure 3. In this game, the sender is given one
of two messages m or m0, with equal probabil-
ity. It then chooses among three actions a1, a01
and a010 , for which the rewards are 4, 3, and 0,
respectively. The receiver observes the sender’s
trajectory (which is equivalent to the sender’s
action in MDPs with one state) and estimates
the message using actions m and rh0, corre-
sponding to guessing to m and m0, respectively.
The receiver accrues a reward of 4 for guess-
ing correctly. The payoff entries in table denote
(R(Z),ZI[M = M]) for each outcome.
Figure 3: A payoff matrix for a simple MCG.
Message	Sender	Receiver 0 m	m
m	aι a1 a10	-(4T)~(4,^oΓ (3, Z)	(3, 0) (0, Z)	(0,0)
m0	aι a； a?	^^(4j0)~(4,ττ (3, 0)	(3, Z) (0, 0)	(0, Z)
As is generally true of MCGs, this MCG is difficult for independent approximate dynamic
programming-based approaches because their learning dynamics are subject to local optima. Con-
sider a run in which the sender first learns to maximize its immediate reward by always selecting
a1 . Now, the receiver has no reason to condition its guess on the sender’s action because the sender
is not communicating any information about the message. As a result, thereafter, the sender has
no incentive to communicate any information in its message, because the receiver would ignore it
anyways. This outcome, sometimes called a babbling equilibrium, leads to a total expected return
of 4 + Z/2 (sender always selects aι, receiver guesses randomly). In cases in which Z is small
(communication is unimportant), the babbling equilibrium performs well. However, it can perform
arbitrarily poorly as ζ becomes large.
4.2	Special Cases
We can express both (a large class of) referential games and various source coding settings as special
cases of the MCG formalism by describing the MDPs to which they correspond.
(A Large Class of) Referential Games We can express a T step referential game as follows.
•	The state space S = {s0, s1, . . . , sT}.
•	The transition function deterministically maps st 7→ st+1 and terminates at input sT .
•	The reward function maps to zero for every state action pair.
Standard Source Coding We can express the standard source coding problem as follows.
•	The state space S = {s}.
•	The action space A = A ∪ {0}.
•	The transition function deterministically maps to S to S for a ∈ A and terminates the game on 0.
•	The reward function maps to -1 for a ∈ A and maps 0 to 0.
Length Limited Source Coding Length limited source coding can be captured in the same way
as standard source coding with the modifications that S = {S0, S1, . . . , ST}, the transition function
terminates on ST, and the reward function yields 0 from ST, where T is the length limit.
5
Under review as a conference paper at ICLR 2022
Source Coding with Unequal Symbol Costs Source coding with unequal symbol costs can be
captured in the same way as standard source coding with the modification that R(∙,a) returns the
negative symbol cost of a, rather than returning -1, for a ∈ A.
5	Greedy Minimum Entropy Coupling
To address MCGs, we propose a novel algorithm we call greedy minimum entropy coupling (GME).
GME (and more broadly, any algorithm geared toward MCGs) is faced with two competing incen-
tives. On one hand, it needs to maximize expected reward R(Z) generated by the MDP. On the other
hand, it needs to maximize the amount of information I(M; Z) communicated to the receiver about
the message, so as to maximize the probability ofa correct guess. GME handles this trade-off using
a two step process for constructing the sender’s policy. In the first step, it computes an MDP policy
that balances between high cumulative reward and large cumulative entropy. In the second step, it
couples the probability mass of this policy with the posterior over messages in such a way that the
expected return does not decrease and the amount of mutual information between the message and
trajectory is greedily maximized. We describe these steps below. Thereafter, we show this procedure
possesses desirable guarantees and discuss intuition for the method.
5.1	Method Description
Step One In the first step, GME uses MaxEnt RL to construct an MDP policy π. This policy is
an MDP policy, not an MCG policy, and therefore does not depend on the message. Note that this
policy depends on the choice of temperature α used for the MaxEnt RL algorithm.
Step Two In the second step, at execution time, GME constructs a message-conditional policy on-
line. Say that, up to time t, the sender is in state st, history ht and has played according to the
message conditional policy n:M. Let
bt = P(M | ht,∏M)
be the posterior over the message, conditioned on the history and the historical policy. GME per-
forms a MEC between the posterior over the message bt and distribution over actions π(st), as
determined by the MDP policy. Let ν = MEC(bt, π(st)) denote joint distribution over messages
and actions resulting from the coupling. Then GME sets the sender to act according to the message
conditional distribution
∏M(st，m) = ν(At | M = m)
of the coupling distribution MEC(bt, π(st)).
Given the sender’s MDP trajectory, GME’s receiver uses the the sender’s MDP policy and MEC
procedure to reconstruct the sender’s message conditional policy along the trajectory; thereafter, the
receiver computes the posterior and guesses the maximum a posteriori message.
Pseuodocode for GME is included in the appendix.
5.2	Method Analysis
GME possesses guarantees both concerning the return R(Z) generated by the MDP and concerning
the amount of information communicated I(M; Z).
Proposition 1. At each state of the MDP, the message conditional policy π∣m Selects actions with
the same probabilities as the MDP policy π.
Proof. Fix an arbitrary state s. Let bbe a posterior over the message induced by the sender’s message
conditional policy on the way to s. Then recall that GME uses the distribution PMEC(b,π(s)) induced
by a MEC between b and π(s) to select its action. Because MECs guarantee that the resultant joint
distribution marginalizes correctly, it follows directly that GME must select its actions with the same
probabilities as ∏(s).	□
Proposition 2. The expected return generated from the MDP by the message conditional policy π∣M
is equal to that of the MaxEnt policy π. That is,
EM 〜μE∏∣M R(Z) = En R(Z).
6
Under review as a conference paper at ICLR 2022
Figure 4: Results for GME and RL+PR on CodeGrid with varying message space sizes.
Proof. It follows from Proposition 1 that all trajectories are generated with the same probabilities
and therefore that the expected returns are the same.	□
Proposition 3. At each decision point, GME greedily maximizes the mutual information
I(M; Ht+1 | bt, ht) between the message M the history at the next time step Ht+1, given the
contemporaneous posterior and history, subject to Proposition 1.
Proof. For conciseness, we leave the conditional dependence on bt and ht implicit in the argu-
ment. First, we claim that I(M; Ht+1) = I(M; At). To see this, first consider that Ht+1 ≡
(ht, At, St+1). This means we have I(M; Ht+1) = I(M; (At, St+1)) since we are conditioning
on ht. Now, consider that because the message influences the next state only by means of the action,
we have the causal graph M → At → (At, St+1), which implies that M ⊥ (At, St+1) | At. Also,
we trivially have M ⊥ At | (At, St+1). These conditional independence properties allow us to
apply the data processing inequality: X ⊥ Z | Y ⇒ I(X; Y ) ≥ I(X; Z). Applying it in both
directions yields I(M; (At, St+1)) = I(M; At).
Now consider that we can rewrite mutual information using the equality
I(M;At) = H(M) + H(At) -H(M,At).	(1)
The first term H(M) is exogenous by virtue of being determined by bt . The second term is exoge-
nous by virtue of being subject to Proposition 1. The third term is the joint entropy between M and
At, which is exactly What a minimum entropy coupling minimizes.	□
5.3	Method Discussion
One aspect of GME that went uncommented upon in the analysis section is the choice of MaxEnt RL
for step one. The reason that GME uses MaxEnt RL here is to control the H(At) term in equation
(1). If the temperature α is large, GME places more emphasis on maximizing H(At) (and thereby
I(M; Z)); if the temperature α is small, GME places more emphasis on maximizing R(Z). The
appropriate choice ofα will depend on the value ofζ. Code for GME that illustrates the functionality
of α applied to the example from Figure 3 can be found at https://bit.ly/36I3LDm.
A second aspect of GME that we have yet to discuss is its scalability. Performing an approxi-
mate MEC takes O(N logN) time, making it expensive to scale to very large message spaces. To
accommodate this fact, for large message spaces we recommend using a factored representation
M ⊂ Mi × ∙∙∙ × Mk and a corresponding factored belief (bi,..., bk), where each bj tracks the
posterior over Mj . At each time step, we suggest performing a MEC between π(st) and the block
bj = arg maxb H(bj ) having the largest entropy. By doing so, we can substantially reduce the
size the of the space over which we need to perform a minimum entropy coupling at any one time,
allowing us to scale to extremely large message spaces.
6	Experiments
We investigate the efficacy of GME on MCGs based on a gridworld, Cartpole and Pong. For all
three we use MaxEnt Q-learning for our MDP policy. Details regarding our implementation can be
found in the supplementary material and in the appendix.
7
Under review as a conference paper at ICLR 2022
actuator noise
(P=O.O)
pixel error
s∞re
actuator noise
(p = 0.05)
pixel error
s∞re
actuator noise
(P=O.I)
pixel error
s∞re
actuator noise
(p = 0.25)
pixel error
s∞re
actuator noise
(p=0∙5)
pixel error
s∞re
Figure 5:	Results for GME on CodeCart with varying amounts of actuator noise and temperatures.
CodeGrid In our gridworld MCG, which we call CodeGrid, the sender is placed on a 4 X 4 grid in
which it can move left, right, up and down. The sender starts at position (1, 1) and must to move to
(4, 4) within 8 time steps to achieve the maximal MDP return of 1. Otherwise, the game terminates
after 8 time steps and the sender receives a payoff of 0.
For our baseline, we trained an RL agent to play as the MCG sender. We paired this RL agent with
an perfect receiver, meaning that, for each episode, the receiver computed the exact posterior over
the message based on the sender’s current policy and guessed the MAP, both during training and
testing. We abbreviate this baseline as RL+PR (where PR stands for perfect receiver). RL+PR is
one way to adapt related work such as (Strouse et al., 2018; Eysenbach et al., 2019) to the MCG
setting. Pseudocode for the RL+PR baseline can be found in the appendix.
We show results for GME and RL+PR across a variety of settings in Figure 7. The column of the
figure corresponds to the cardinality of the message space; the exact size is listed in the green bubble.
We use a uniform marginal over messages in each case. The x-axis corresponds to the proportion of
the time that the receiver correctly guesses the message. The y-axis corresponds to the MDP payoff
(in this case whether the sender reached the opposing corner of the grid within the time limit). Both
GME and RL+PR possess mechanisms to trade-off between these two goals. GME can adjust its
temperature α, while RL+PR can adjust the value ofζ used during training. Figure 7 show the Pareto
curve for each. For GME, the size of the circle corresponds to the inverse temperature β = 1∕α.
For the settings with 8 messages and 32 messages, we observe that both GME and RL+PR achieve
strong performance—achieving optimal or nearly optimal MDP returns and optimal or nearly opti-
mal transmission. However, as the size of the message space increases, the performance of RL+PR
falls off sharply. Indeed, for the 128 message setting, RL+PR is only able to correctly transmit the
message slightly more than half the time. RL+PR’s inability to achieve strong performance in these
cases may be a result of the fact that communication protocols violate the locality assumption of ap-
proximate dynamic programming approaches. In contrast, we observe that GME, which constructs
its protocol using MEC, remains optimal or near optimal both the 64 and the 128 message settings.
CodeCart and CodePong While the CodeGrid experiments that GME can outperform an obvious
approach to MCGs, it remains to be determined whether GME can perform well at scale. Toward the
end of making this determination, we consider MCGs based on Cartpole and Pong. In these MCGs,
the message spaces are the sets of 16 × 16 and 22 × 22 binary images, respectively, each with a
uniform prior. The cardinality of these spaces (> 1077 and > 10145) is astronomical. In fact, it is
not clear how an RL+PR-like approach could even be scaled to this setting. On the other hand, GME
is easily adaptable to this setting, using the factorization scheme suggested in the method section.
We also include results with variable amounts of actuator noise, i.e., with some probability p, the
environment executes a random action, rather than the one it intended. Actuator noise models the
probability of error during transmission and arises naturally in many settings involving communica-
tion, such as noisy channel coding. In this setting, the receiver may only observe the action executed
8
Under review as a conference paper at ICLR 2022
actuator noise
(P = O.O)
pixel error
score
actuator noise
(p = 0.05)
pixel error
score
actuator noise
(P = 0.1)
pixel error
score
actuator noise
(p = 0∙25)
pixel error
score
actuator noise
(P = OS)
pixel error
score
log 8 =5.0
log β= 6.0
log β = 7.0
1231.4 ± 7.11
^=|21.0±0.0|
12365 ±152 I
∣240Q±8∙7∣
卜3.6 ± 12.11
1250.4 ±6.8 I
卜 15.9 ±361
12403 ±104 I
卜 18.9 ±2.11
Figure 6:	Results for GME on CodePong with varying amounts of actuator noise and temperatures.
by the environment, rather than the one intended by the sender. While this setting is not technically
an MCG, GME can still be directly applied. And while the derivation of Proposition 3 no longer
holds, we suggest that GME nevertheless offers a strong heuristic in such settings.
We show the results in Figure 5 and Figure 6 for CodeCart and CodePong, respectively. For both
plots, the y-axis corresponds to the amount of actuator noise (lower is more noise). The x-axis
corresponds the inverse temperature value β = 1∕α (further right is colder temperature, meaning
there is more emphasis on MDP expected return). Each entry contains a decoded yin-yang symbol
with the corresponding temperature and actuator noise. Each entry also lists the `1 pixel error (blue)
and the MDP expected return (green), along with corresponding standard errors over 10 runs; a
brighter color corresponds to better performance.
Remarkably, for both CodeCart and CodePong, we observe that, when there is no actuator noise,
GME is able to perfectly transmit images while achieving the maximum expected return in the
MDP. For CodeCart, this occurs at logβ ∈ {-10, -1, 0.1}; for CodePong, it occurs at logβ ∈
{4, 5}. Interestingly, as the amount of actuator noise increases to a non-zero value, the effect on
performance differs between CodeCart and CodePong. In CodeCart, GME continues to achieve
maximal performance in the MDP, but begins to accumulate errors in the transmission. In contrast, in
CodePong, GME continues to transmit the message with perfect fidelity, but begins to lose expected
return from the MDP. This suggests that accidentally taking a random action is costlier in Pong than
it is in Cartpole, but that, in an informal sense, Pong has more bandwidth to transmit information.
However, in both cases, the decay in performance is graceful—for example, with p = 0.05, the
decrease in the visual quality of the transmission for Cartpole (log β = -1) is difficult to even
perceive, while the CodePong sender still manages to win games roughly 80% of the time (log β =
5). The performance in both settings continues to deteriorate up to p = 1/2, at which point GME is
neither able to perform adequately on the MDP nor to transmit a clear symbol.
Videos of the agent playing are included in the supplemental material.
7	Conclusions
This work introduces a new problem setting called Markov coding games, which generalize referen-
tial games and source coding and are related to channel coding problems and decentralized control.
We contribute an algorithm for this setting called GME and show that GME possesses provable
guarantees regarding both the return generated from the MDP and the amount of information con-
tent communicated, subject to some constraints. On the experimental side, we show that GME
significantly outperforms an RL baseline with a perfect receiver. Finally, we show that GME is able
to scale to extremely large message spaces and transmit these messages with high fidelity, even with
some actuator noise, suggesting that it could be robust in real world settings.
9
Under review as a conference paper at ICLR 2022
8	Reproducibility
Our codebase is included in the supplemental material.
References
M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An
evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279,
Jun 2013. ISSN 1076-9757. doi: 10.1613/jair.3912. URL http://dx.doi.org/10.1613/
jair.3912.
Varun Bhatt and Michael Buro. Inference-based deterministic messaging for multi-agent communi-
cation. Proceedings of the AAAI Conference on Artificial Intelligence, 35(13):11228-11236, May
2021. URL https://ojs.aaai.org/index.php/AAAI/article/view/17339.
F. Cicalese, L. Gargano, and U. Vaccaro. Minimum-entropy couplings and their applications. IEEE
Transactions on Information Theory, 65(6):3436-3451, 2019. doi: 10.1109/TIT.2019.2894519.
Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need:
Learning skills without a reward function. In International Conference on Learning Representa-
tions, 2019. URL https://openreview.net/forum?id=SJx63jRqFm.
Jakob Foerster, Ioannis Alexandros Assael, Nando de Freitas, and Shimon Whiteson. Learning to
communicate with deep multi-agent reinforcement learning. In D. Lee, M. Sugiyama, U. Luxburg,
I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29,
pp. 2137-2145. Curran Associates, Inc., 2016. URL https://proceedings.neurips.
cc/paper/2016/file/c7635bfd99248a2cdef8249ef7bfbef4-Paper.pdf.
Mordecai J. Golin, Claire Kenyon, and Neal E. Young. Huffman coding with unequal letter costs.
In Proceedings of the Thiry-Fourth Annual ACM Symposium on Theory of Computing, STOC
’02, pp. 785-791, New York, NY, USA, 2002. Association for Computing Machinery. ISBN
1581134959. doi: 10.1145/509907.510020. URL https://doi.org/10.1145/509907.
510020.
Kenichi Iwata, Masakatu Morii, and Tomohiko Uyematsu. An efficient universal coding algorithm
for noiseless channel with symbols of unequal cost. IEICE Transactions on Fundamentals of
Electronics, Communications and Computer Sciences, 80:2232-2237, 1997.
Mladen Kovacevic, Ivan Stanojevic, and VoJm Senk. On the entropy of couplings. Informa-
tion and Computation, 242:369-382, 2015. ISSN 0890-5401. doi: https://doi.org/10.1016/j.
ic.2015.04.003. URL https://www.sciencedirect.com/science/article/pii/
S0890540115000450.
Lawrence L. Larmore and Daniel S. Hirschberg. Length-limited coding. In Proceedings of the First
Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’90, pp. 310-318, USA, 1990.
Society for Industrial and Applied Mathematics. ISBN 0898712513.
B. P. Lathi. Modern Digital and Analog Communication Systems 3e Osece. Oxford University Press,
Inc., USA, 3rd edition, 1998. ISBN 0195110099.
Angeliki Lazaridou, Karl Moritz Hermann, Karl Tuyls, and Stephen Clark. Emergence of linguistic
communication from referential games with symbolic and pixel input. In International Confer-
ence on Learning Representations, 2018. URL https://openreview.net/forum?id=
HJGv1Z-AW.
David K. Lewis. Convention: A Philosophical Study. Wiley-Blackwell, 1969.
David J. C. MacKay. Information Theory, Inference Learning Algorithms. Cambridge University
Press, USA, 2002. ISBN 0521642981.
10
Under review as a conference paper at ICLR 2022
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529-533, FebrUary 2015. ISSN 1476-4687. doi: 10.1038∕nature14236. URL
https://www.nature.com/articles/nature14236. Number: 7540 Publisher: Na-
tUre PUblishing GroUp.
Vinod Nair and Geoffrey E. Hinton. Rectified linear Units improve restricted boltzmann machines.
In Proceedings of the 27th International Conference on International Conference on Machine
Learning, ICML’10, pp. 807-814, Madison, WI, USA, JUne 2010. Omnipress. ISBN 978-1-
60558-907-7.
Frans A Oliehoek, Christopher Amato, et al. A concise introduction to decentralized POMDPs,
volUme 1. Springer, 2016.
Adam Paszke, S. Gross, SoUmith Chintala, G. Chanan, E. Yang, Zachary Devito, Zeming Lin, Alban
Desmaison, L. Antiga, and A. Lerer. AUtomatic differentiation in PyTorch, 2017.
Brian Skyrms. Signals: Evolution, Learning, and Information. Oxford University Press, 2010.
Kenny Smith. The cUltUral evolUtion of commUnication in a popUlation of neUral networks. Con-
nection Science, 14:65 - 84, 2002.
Matthew Spike, Kevin Stadler, Simon Kirby, and Kenny Smith. Minimal reqUirements for the emer-
gence of learned signaling. Cognitive Science, 41(3):623-658, 2017. doi: https://doi.org/10.
1111/cogs.12351. URL https://onlinelibrary.wiley.com/doi/abs/10.1111/
cogs.12351.
LUc Steels. Self-organization and selection in cUltUral langUage evolUtion. 03 2013. doi: 10.1075/
ais.3.02ste.
D J StroUse, Max Kleiman-Weiner, Josh TenenbaUm, Matt Botvinick, and David Schwab. Learning
to share and hide intentions Using information regUlarization. In Proceedings of the 32nd Interna-
tional Conference on Neural Information Processing Systems, NIPS’18, pp. 10270-10281, Red
Hook, NY, USA, 2018. CUrran Associates Inc.
Richard S. SUtton and Andrew G. Barto. Reinforcement Learning: An Introduction. A Bradford
Book, Cambridge, MA, USA, 2018. ISBN 978-0-262-03924-6.
Hong Shen Wang and N. Moayeri. Finite-state markov channel-a UsefUl model for radio com-
mUnication channels. IEEE Transactions on Vehicular Technology, 44(1):163-171, 1995. doi:
10.1109/25.350282.
Brian D. Ziebart, Andrew Maas, J. Andrew Bagnell, and Anind K. Dey. MaximUm entropy inverse
reinforcement learning. In Proceedings of the 23rd national conference on Artificial intelligence
- Volume 3, AAAI’08, pp. 1433-1438, Chicago, Illinois, JUly 2008. AAAI Press. ISBN 978-1-
57735-368-3.
11
Under review as a conference paper at ICLR 2022
A Experimental Details
For CodeGrid, we use a policy parameterized by neural network with two fully-connected layers of
hidden dimension 64, each followed by a ReLu activation (Nair & Hinton, 2010). For CodePong and
CodeCart, we use a convolutional encoder with three layers of convolutions (number of channels,
kernel size, stride) as follows: (32,8,4), (64,4,2), (64,3,1). This is followed by a fully connected
layer (Mnih et al., 2015). We use ReLu activations after each layer, note that we do not use any
max-pooling. For CodeGrid and CodePong, layer weights are randomly initialized using PyTorch
1.7 (Paszke et al., 2017) defaults. For CodeCart, we initialise weights according to an optimally-
trained DQN policy included in rl-baselines3-zoo1 and individually finetune for each β for 15k
steps. For all environments, we use the Adam optimizer with learning rate 10-4, β1 = 0.9, β2 =
0.999, = 10-8 and no weight decay.
CodeGrid
Figure 7: An illustration of two possible CodeGrid trajectories, one of length 6 (left) and one of
length 8 (right).
CodeCart For CodeCart we use the standard action space (move left and move right). There are no
redundant actions that the agent can use to communicate.
CodePong For CodePong we use a reduced action space of size two (move up and move down).
There are no redundant actions that the agent can use to communicate.
1 https://github.com/DLR- RM/rl- baselines3- zoo
12
Under review as a conference paper at ICLR 2022
B Algorithmic Details
B.1 GME
Algorithm 1 GME
// Step 1: Compute MDP policy
Input: MDP GMDP, temperature α
MaxEnt MDP policy π — MaxEntRL(Gmdp, α)
// Step 2: Play Sender’s Part of MCG episode
Input: MDP policy π, MCG GMCG
message m J reset(GMCG) 〃 observe message from environment
belief b J Gmcg μ
state s J GMCG .s
while sender’s turn do
joint distribution V J minimum_entropy.coupling(b, π(s))
decision rule π∣m J V(A | M)
sender action a 〜π∣m(m)
new belief b J POSterior_update(b, π∣m, a)
next state s J GMCG.step(a)
end while
// Step 3: Play Receiver’s Part of MCG episode
Input: MDP policy π, MCG GMCG
MDP trajectory Z J Gmcg .receiver.ObServation()
belief b J- Gmcg ∙μ
for s, a ∈ z do
joint distribution V J minimum_entropy.coupling(b, π(s))
decision rule π∣m J V(A | M)
new belief b J POSterior_update(b, π∣m, a)
end for
estimated message m J arg maxm，b(m0)
GMCG.step(m)
13
Under review as a conference paper at ICLR 2022
B.2	FACTORED GME
Algorithm 2 Factored GME
// Step 1: Compute MDP policy
Input: MDP GMDP, temperature α
MaxEnt MDP policy π — MaxEntRL(Gmdp, α)
// Step 2: Play Sender’s Part of MCG episode
Input: MDP policy π, MCG GMCG
message m J reset(GMCG) 〃 observe message from environment
belief b J- Gmcg ∙μ
state s J GMCG .s
while sender’s turn do
active block index i J arg maxj {H(bj)|bj ∈ b}
joint distribution V J minimum_entropy.coupling(bi, π(s))
decision rule π∣m J V(A | M)
sender action a 〜π∣m(mi)
new belief b J posterior .update (bi, ∏∣m , a)
next state s J GMCG.step(a)
end while
// Step 3: Play Receiver’s Part of MCG episode
Input: MDP policy π, MCG GMCG
MDP trajectory Z J Gmcg .receiver.ObServation()
belief b J GmcgW
for s, a ∈ z do
active block index i J arg maxj {H(bj)|bj ∈ b}
joint distribution V J minimum_entropy.coupling(bi, π(s))
decision rule π∣m J V(A | M)
new belief b J POSterior_update(b, π∣m, a)
end for
for i do
estimated ith message block mi J arg maxm∕bi(mi)
end for
GMCG.step(m)
B.3	RL+PR baseline
Algorithm 3 RL+PR baseline
// Play Episode
Input: MCG GMCG
state s J s0
message m J sample(μ)
belief b J μ
while True do
action a J sample(π(s, m))
belief b J posterior_update(b, π, a)
new state s0 J sample(T (s, a))
if s0 non-terminal then
add_to_buffer(s, a, R(s, a), s0)
state s J s0
else
break
end if
end while
add_to_buffer(s, a, R(s, a) + Zmaxmιo∈M b(m0), 0)
14
Under review as a conference paper at ICLR 2022
B.4	Min Entropy Joint Distribution Algorithm outputting a sparse
REPRESENTATION OF M CICALESE ET AL. (2019)
Algorithm 4 Min Entropy Joint Distribution
Require: prob. distributions p = (p1, . . . ,pn) and q = (q1, . . . , qn)
Ensure: A Coupling M = [mij] of p and q in sparse representation L = {(mij, (i, j)) |mij 6= 0}
if P = q, let i = max {j ∣Pj = q§} ； if Pi < q% then swap P 什 q.
Z = (zι,...,Zn) - p ∧ q, L JQ
CreatePriorityQUeUe(Q(Tow)), qrowsum J 0
CreatePriorityQueue Q(col) , qcolsum J 0
for i = n downto 1 do
(d)	(r)
zi J zi, zi J 0
if qcolsum + zi > qi then
(z(d),z(r),I, qcolsum j Lemma3-Sparse (Zi, q” Q^coll),qcolsum)
else
while Q(col) 6= Q do
(m, l) J ExtractMin(Q(col)),
qcolsum J qcolsum - m,
L J L ∪ {(m, (l, i))}
end while
end if
if qrowsum + zi > pi then
(zi(d), zi(r), I, qrowsum) J Lemma3-Sparse(zi, pi, Q(row), qrowsum)
for each (m, l) ∈ I do L → L ∪ {(m, (i, l))}
if zi(r) > 0 then Insert Q(row), (zi(r), i)
(r)
qrowsum J qrow sum+ zi
else
while Q(row) 6= Q do
(m, l) J ExtractMin(Q(row))
qrowsum J qrowsum - m
L J L ∪ {(m, (i, l))}
end while
end if
LJL∪ (zi(d),(i,i))
end for
Algorithm 5 Lemma3-Sparse
Require: real z > 0, x ≥ 0, and priority qUeUe Q s.t. P(m,l)∈Q m = qsum and qsum = x ≥ z
Ensure: z(d), z(r) ≥ 0, and I ⊆ Q s.t. z(d) + z(r) = z, and z(d) + P(m,l)∈I m= x
I J Q, sumJ 0
while Q 6= Q and sum+ M in(Q) < x do
(m, l) J ExtractMin(Q), qsum J qsum - m
I J I ∪ {(m, l)} , z(r) J z - z(d)
end while
z(d) J x - sum, z (r) J z - z(d)
return z(d), z(r) , I, qsum
15