Under review as a conference paper at ICLR 2022
Metrics Matter: A Closer Look on Self-Paced
Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
Curriculum reinforcement learning (CRL) allows to solve complex tasks by gen-
erating a tailored sequence of learning tasks, starting from easy ones and subse-
quently increasing their difficulty. However, the generation of such task sequences
is largely governed by application assumptions, often preventing a theoretical in-
vestigation of existing approaches. Recently, Klink et al. (2021) showed how
self-paced learning induces a principled interpolation between task distributions
in the context of RL, resulting in high learning performance. So far, this interpola-
tion is unfortunately limited to Gaussian distributions. Here, we show that on one
side, this parametric restriction is insufficient in many learning cases but that on
the other, the interpolation of self-paced RL (sprl) can be degenerate when not
restricted to this parametric form. We show that the introduction of concepts from
optimal transport into sprl prevents aforementioned issues. Experiments demon-
strate that the resulting introduction of metric structure into the curriculum allows
for a well-behaving non-parametric version of sprl that leads to stable learning
performance across tasks.
1	Introduction
Reinforcement learning (RL) (Sutton & Barto, 1998) has celebrated great successes as a framework
for autonomous acquisition of desired behavior. With ever-increasing computational power, this
framework and the algorithms developed under it have allowed to create learning agents capable of
solving non-trivial long-horizon planning (Mnih et al., 2015; Silver et al., 2017) and control tasks
(Akkaya et al., 2019). However, these successes have also highlighted the need for certain forms
of regularization, such as leagues in the context of boardgames (Silver et al., 2017), a gradual di-
versification of simulated training environments for robotic manipulation (Akkaya et al., 2019) or
a tailored training pipeline in the context of humanoid control for soccer (Liu et al., 2021). These
regularizations help to overcome shortcomings of modern RL agents, such as poor exploratory be-
havior - a problem that is an active topic of research (Bellemare et al., 2016; Ghavamzadeh et al.,
2015; Machado et al., 2020).
One can view aforementioned regularizations under the umbrella term of curriculum reinforcement
learning (Narvekar et al., 2020), where the idea is to avoid shortcomings of modern (deep) RL agents
such as aforementioned poor exploration by learning on a tailored sequence of tasks. Such curric-
ula can materialize in a variety of ways and are motivated from many perspectives in the literature
(AndrychoWicz et al., 2017; Florensa et al., 2017; Wohlke et al., 2020). Although the resulting Cur-
ricula can often be interpreted as a sequence of task distributions, these sequences typically lack a
formal connection to the reinforcement learning objective of maximizing the expected reWard under
a given target task distribution. In a recent line of Work, Klink et al. (2021) proposed the idea of
self-paced reinforcement learning (sprl), borroWing from the concept of self-paced learning that
has been established in the supervised learning literature (Kumar et al., 2010; Jiang et al., 2015;
Meng et al., 2017). Klink et al. shoWed a connection betWeen a regularized RL objective and a
sequence of task distributions that trade-off betWeen yielding high expected reWard and tasks likely
under the target distribution. This interpolant has, hoWever, so far been restricted to Gaussian distri-
butions (Klink et al., 2020a;b; 2021). While successful in experimental evaluations, this Gaussian
assumption clearly imposes a limitation on the flexibility of the curriculum and disconnects the al-
gorithmic implementation from the established theory. This disconnect raises the question Whether
the observed performance of sprl is due to the Gaussian approximation.
1
Under review as a conference paper at ICLR 2022
Contribution: The key insight presented in this paper is that the Gaussian approximation of existing
sprl implementations is indeed important for their empirical performance. We show that
Parametric assumptions in sprl hinder the learning performance in task spaces with non-
Gaussian target distributions.
sprl can, however, fail to facilitate learning on the target task distributions when leaving
these parametric assumptions behind.
Equipping sprl with Wasserstein metrics allows for a flexible, particle-based represen-
tation of the task distribution that ensures a meaningful interpolation in aforementioned
failure cases, providing higher performance.
2	Related Work
The main focus of this work is on self-paced reinforcement learning (sprl, Klink et al. (2020a;b;
2021)) that takes the concept of self-paced curriculum learning (Kumar et al., 2010) from supervised-
to reinforcement learning (RL). Opposed to supervised learning, where there is ongoing discussion
about the mechanics of curricula and their effect in different situations (Weinshall & Amir, 2020; Wu
et al., 2021), the mechanics seem to be more agreed upon in RL. In RL, curricula improve learning
performance of an agent by adapting the training environments to its proficiency, and with that e.g.
bypass poor exploratory behaviour of non-proficient agents. Applications are by now widely spread
and different terms have been established. Adaptive Domain Randomization (Akkaya et al., 2019)
uses curricula to gradually diversify training parameters of a simulator to facilitate sim-to-real trans-
fer. Unsupervised environment discovery (Dennis et al., 2020; Jiang et al., 2021b;a) similarly aims
to efficiently train an agent which is robust to variations in the environment. Automatic curriculum
learning methods (Florensa et al., 2017; Sukhbaatar et al., 2018; Florensa et al., 2018; Portelas et al.,
2019; Zhang et al., 2020; Racaniere et al., 2020; Eimer et al., 2021), to which sprl belongs to, par-
ticularly focus on improving the learning speed and/or performance of an agent on a set of desired
tasks. Curricula are often generated as distributions that maximize a certain surrogate objective, such
as learning progress (Baranes & Oudeyer, 2010; Portelas et al., 2019), intermediate task difficulty
(Florensa et al., 2018), regret (Jiang et al., 2021b) or disagreement between Q-functions (Zhang
et al., 2020). Curriculum generation can also be interpreted as a two-player game (Sukhbaatar et al.,
2018). The work by Jiang et al. (2021a) even hints to a link between surrogate objectives and two-
player games. Opposed to these interpretations, sprl has been shown to perform an interpolation
between task distributions by Klink et al. (2021), allowing to formally relate the effect of sprl to the
concept of annealing in statistics (Neal, 2001) and homotopic continuation methods in optimization
(Allgower & Georg, 2003). We wish to add to this formal understanding of sprl by investigating
the interpolation that it produces more closely.
As this investigation will lead us to the problem of optimal transport, we wish to point out important
literature in this field. Dating back to the work by Monge in the 18th century, optimal transport
has been understood as an important fundamental concept touching upon many fields in both the-
ory and application (LiU et al., 2019; Peyre et al., 2019; Chen et al., 2θ21). In probability theory,
optimal transport translates to the so-called Wasserstein metric (Kantorovich, 1942) that compares
two distribUtions Under a given metric on the sample space. From a compUtational perspective,
entropy-regUlarized Wasserstein metrics (CUtUri, 2013) have led to tangible speed-Ups in compUta-
tions revolving aroUnd optimal transport and are hence widely applied (Feydy et al., 2019).
3	Preliminaries
This section serves to introdUce the necessary backgroUnd on (contextUal) RL, self-paced RL and
optimal transport.
3.1	Contextual Reinforcement Learning
ContextUal reinforcement learning (Hallak et al., 2015) can be seen as a conceptUal extension to the
(single task) reinforcement learning (RL) problem
∞
m∏ax J(π) = m∏axEpo(so),p(st+1 ∣st,at),π(at|st)
t=0
γtr(st, at)
(1)
2
Under review as a conference paper at ICLR 2022
which aims to maximize the above expected reward objective by finding an optimal a policy
π : S×A 7→ R for a given MDP M = hS, A,p, r,p0i with initial state distribution p0 and transition
dynamics p. Contextual RL extends this objective to a space of MDPs M(c) = hS, A, pc, rc, p0,ci
equipped with a distribution μ6 → R over contextual variables C ∈ C
max J(π, μ) = max Eμ(c) [J(π, c)] .	(2)
ππ
The policy π : S ×C ×A 7→ R is conditioned on the contextual parameter c. The distribution
μ(c) encodes the tasks M(c) that the agent is expected to encounter. Objective J(π, c) in Eq. (2)
corresponds to the objective J(π) in Eq. (1) where, however, the initial state distribution p0, the
transition dynamics p as well as the reward function r of M are replaced by their counterparts in
M(c). This contextual model of optimal decision making is well-suited for learning in multiple
related tasks as is the case in multi-task (Wilson et al., 2007), goal-conditioned (Schaul et al., 2015)
or curriculum RL (Narvekar et al., 2020).
3.2	Self-Paced Reinforcement Learning
Self-paced reinforcement learning (sprl) has been introduced by Klink et al. (2020a;b; 2021) as a
curriculum RL algorithm that alters the context distribution μ(c) in the contextual RL objective (2)
to increase the learning performance of an agent and/or make it less susceptible to local optima of
the objective function. SPRL computes a surrogate distribution p : C 7→ R under which to train the
RL agent, i.e. optimize J(π, p).
This surrogate distribution is found by optimizing the KL divergence to the target distribution μ(c)
subject to two constraints (see Klink et al. (2021, Section 8))
min DKL (p(c) k μ(c)) s.t. J(π,p) ≥ δ	DKL (p(c) k q(c)) ≤ .	(3)
p
The distribution p(c) balances between tasks likely under the (target) distribution μ(c) and tasks in
which the agent currently obtains large rewards. The KL divergence constraint w.r.t. the previous
context distribution q(c) prevents large changes in p(c) during subsequent iterations, making the
curriculum robust against errors in the estimates of the expected agent performance J(π, c).
A particularly interesting aspect of this work is that objective (3) can be interpreted to perform
a specific interpolation between the distributions μ(c), q(c) and a maximum entropy distribution
PJ(C) 8 exp(ηJ(π, C)) encoding high reward tasks. This interpolation is given by
Pα,η(C) H μ(c) 1+a q(c) 1αa eχp(ηJ(∏, C)) 1+α.	(4)
The two parameters α and η controlling the interpolation are the Lagrangian multipliers of the two
constraints in objective (3). So far, Klink et al. (2020a;b; 2021) restricted the distribution pα,η(C)
to a Gaussian distributions PV(C) = N(C∣μ, Σ). In this case, optimizing (3) w.r.t. μ and Σ of PV
corresponds to performing an I-projection of the analytic optimal distribution (4) to the Gaussian
restriction, i.e. minimizing DKL (PV (C) k Pα,η (C)) w.r.t. ν.
In this work, we are interested in investigating the distribution Pα,η outside of this parametric re-
striction PV, i.e. truly employing the distribution (4) instead of its I-projection to a Gaussian.
3.3	Optimal Transport
The problem of optimally transporting density between two distributions has been initially investi-
gated by Monge (1781). As of today, generalizations established by Kantorovich (1942) have led
to so called Wasserstein distances as metrics between probability distributions defined on a metric
space M = (d, C) with metric d : C × C 7→ R≥0
Wp(P1,P2) = γ Γi(npf,p ) Z	d(C1, C2)p dγ(C1, C2)	, P ≥ 1
Γ(P1,P2)
γ : C × C 7→ R≥0P1(C1) =	γ(C1, C2) dC2, P2(C2) =	γ(C1, C2) dC1
The distance between P1 and P2 results from solving an optimization problem that finds a so-called
plan γ. This plan encodes how to equalize P1 and P2 taking into account the cost of moving density
between between parts of the space C . This cost is encoded by the metric d. In the following, we
3
Under review as a conference paper at ICLR 2022
Algorithm 1 Self-Paced Reinforcement Learning Implementations
Input: Context dist. po(c), target context dist. μ(c), performance threshold δ, distance bound E
for k = 0 to K do
Agent Improvement:
Sample contexts Ci 〜Pk(c), i ∈ [1, M]
Train policy π under ci and observe episodic rewards Ri = Pt∞=1 rci (st, at), i ∈ [1, M]
Estimate J(π, c) from the dataset D = {(ci, Ri)|i ∈ [1, M]}
Context Distribution Update:
G-SPRL Optimize (3) w.r.t. μk+ι and ∑k+ι, wherepk+ι(c) = N(c∣μk+ι, ∑k+ι)
NP-SPRL Optimize (3) w.r.t ɑ and η using a discrete approximation pa,η ≈ Pα,η (C) (4)
WB-SPRL Optimize (7) w.r.t. β to obtain pβ(c)
end for
will always assume to work with 2-Wasserstein distances under euclidean metric, i.e. p = 2 and
d(C1, C2) = kC1 - C2 k2. Particularly interesting for the investigation conducted in this paper are so
called Wasserstein barycenters
pW2 (C) = arg minp PkK=1 wkW2(p,pk),	PkK=1 wk = 1,	(5)
that perform a weighted interpolation between a set of distributions pk by computing a distribu-
tion p that minimizes the above sum of Wasserstein distances. As illustrated in appendix A, these
barycenters allow to smoothly interpolate between complex distributions w.r.t. the defined metric d.
4	Non-Parametric Self-Paced Reinforcement Learning
To highlight the importance of a non-parametric variant of sprl that, nonetheless, respects the
metric structure of the context space C, We investigate three versions of sprl: G-SPRL- the Gaussian
version from Klink et al. (2021), NP-SPRL- a faithful implementation of the SPRL objective (3) that
computes and uses the distribution pα,η (C) (4) by discretizing the context space C, and WB -SPRL-
an instantiation of Wasserstein barycenters in the sprl framework. As shown in Algorithm 1, all
implementations only differ in the computation of the context distributions.
4.1	np-sprl
Computing the analytic distribution pα,η(C) and adjusting the parameters α and η via the SPRL
objective (3) will require approximations in the general case, as sampling arbitrary continuous dis-
tributions is an open research problem (Brooks et al., 2011; Liu & Wang, 2016; Liu et al., 2019;
Wibisono, 2018). Using approximate methods resulting from this research would, however, directly
interfere with our intent of evaluating the behavior of pα,η (C) as exactly as possible. Consequently,
we discretize the continuous context spaces in our experiments in order to faithfully sample and
evaluate expectations w.r.t. pα,η(C). This discretization is schematically shown in Figure 1 for one
of the evaluation environments. Although such a discretization will clearly not scale gracefully to
higher dimensions, it allows us to investigate the “analytic” behavior of sprl, which, as we will
show, can be sub-optimal even for low-dimensional context spaces.
With a context space C ⊆ Rd discretized into N cells, we can represent pα,η as a vector Pα,η∈RN0.
To sample a continuous context C ∈ C, we can then first sample an index of a cell from Pα,η and
then sample uniformly within this cell to obtain a context C. Further, the KL divergences in the SPRL
objective (3) are straightforward to evaluate when working with Pα,η. To evaluate the expected per-
formance, We simply evaluate the performance measure J(∏, c) at the N cell centers Cn to obtain
the vector J(∏) ∈ RN. With that we obtain J(∏,Pα,η) = P晨J(∏).
4.2	wb-sprl
To leverage optimal transport in an sprl style algorithm, we realize the interpolation between the
current distribution q(c), target distribution μ(c) and “value distribution” PJ(c) 8 exp(ηJ(π, c))
by Wasserstein barycenters
Pe (c) = argmin(l-β1-β2)W (p(c),μ(c)) + βιW (P(C), Pj (c)) + β2W (p(c),q(c)).	(6)
p
4
Under review as a conference paper at ICLR 2022
阚回■同
(a)	Maze Environment
(b)	Context Space C
(c)	Function f on C
(d)	f discretized on C
Figure 1: (a) The first environment for evaluation of sprl is a maze task simulated in MuJoCo
(Todorov et al., 2012), in which a point-mass needs to move in a maze of circular shape to reach
desired target positions. (b) The target position is encoded via the contextual variable c ∈ C⊆R2 .
The area highlighted in red visualizes the initial position of the point mass and the walls of the
environment are shown in black. (c + d) In order to faithfully evaluate expectations over functions
or compute KL divergences e.g. in the SPRL objective (3), we discretize the context space C.
The weights β of the interpolation are adjusted with the goal of minimizing W2 (pβ, μ) while ensur-
ing a constraint on expected performance and distance to the previous distribution q(c)
mβn W2(Pβ(c),μ(c)) s.t J(∏,Pβ) ≥ δ W2(pβ(c),q(c)) ≤ J	(7)
A difference w.r.t. SPRL is that the parameter η of the value distribution pJ (c) is not adjusted in
optimization problem (7). Instead we adjust η before optimizing (7) such that J(π, pJ) ≥ δH,
where δH > δ is another performance threshold. This ensures that with β1 → 1, it holds that
J(π, pβ) ≥ δ. We resort to particle-based representations of the distributions when implementing
(6) and (7). This allows us to make use of Monge maps to compute pβ(c) efficiently. More details
on the implementation of wb-sprl are provided in appendix C.
5	Experiments
The experiments in this section serve to show the need for a realization of sprl without parametric
restrictions on pα,η but to also highlight that NP-SPRL is not necessarily well-suited for this endeav-
our due its ignorance w.r.t. the metric on C. Experimental details can be found in appendix D. Code
is provided in the supplementary material and will be made available upon acceptance. To situate
our method in the field of curriculum RL and to showcase how our method performs w.r.t. the cur-
rent state-of-the art, we further evaluate acl, goalgan, alp-gmm, vds and plr (Graves et al.,
2017; Florensa et al., 2018; Portelas et al., 2019; Zhang et al., 2020; Jiang et al., 2021b) alongside
the investigated sprl variants.
5.1	MAZE
We first turn toa sparse-reward, maze-like environment depicted in Figure 1, in which an agent needs
to reach a desired goal. Such environments have e.g. been investigated by Florensa et al. (2018).
The contexts c ∈ C of this environment encode the goal position to be reached and hence contains
UnsolVable contexts (goals inside of a wall or in the inner circle of the maze). Defining μ(c) to be
uniform over the context space, the curriculum needs to identify and train the agent on the subspace
of feasible tasks in order to achieVe a good learning performance. This subspace of feasible tasks
is highly non-Gaussian, making it an interesting testbed for np- and wb-sprl. We discretize the
context space C ⊆ R2 of goal positions into a 50 × 50 grid. Figure 2 compares the performance of
the different CRL algorithms. We see that both np- and wb-sprl perform better than g-sprl, since
the performance constraint Ep(c) [J(π, c)] ≥ δ in objectiVe (3) at some point preVents the Gaussian
context distribution of g-sprl from expanding, as otherwise too many infeasible tasks would be
encoded in pν (c) and hence the performance constraint Violated. As shown in Figures 2 and 7, the
non-parametric Versions of sprl can flexibly assign probability to feasible contexts, resulting in
high learning performance.
5.2	Point-Mass
We now consider the point-mass enVironment inVestigated by (Klink et al., 2020a;b; 2021). As
shown in Figure 3a, a point mass needs to be steered through a narrow gate to reach a goal position
5
Under review as a conference paper at ICLR 2022
G-SPRL Random GoalGAN	ACL PLR
NP-SPRL WB-SPRL ALP-GMM VDS
0.8
W 0.6
取0.4
o 0.2
0
0	100	200	300	400
Epoch
(a) Learning Performance (Maze)
回回回

(b) G-SPRL context distributions pν over iterations
(c) NP-SPRL context distributions pα,η over iterations
Figure 2: a) Achieved success rate of different curricula and a uniform sampling baseline (Ran-
dom) over iterations. Mean and standard error are computed from 10 runs. b) Parametric and non-
parametric context distributions pν (c) and pα,η (c) for a run of G-SPRL and NP-SPRL respectively.
The distributions are represented by 2000 samples drawn from them.
on the other side of a wall. While Klink et al. only considered a narrow gate at one specific position
as the target task, we will investigate a version in which the gate is located at one of two opposing
positions cι = [-3 0.5] and C2=[3 0.5], making μ(c) a bi-modal distribution. This again challenges
the Gaussian restriction of pν (c) in the G-SPRL algorithm. We again discretize the context space
C ⊆ R2 into a grid of 50 bins on each axis. We investigate two target distributions with different
log-likelihoods, that nonetheless produce very similar samples
μι(c) = 2N (ci, 10-4I) + 2N(C2,10-4I)	μ2(c)
2	, if C ∈ {cι, c2}
≈0 , else.
Note that “≈0” corresponds to a value of exp(-1000), as this ensures that μ(c) is absolutely Con-
tinuous w.r.t. pα,η(c) and pν (c), which is required to compute the KL divergence between them.
Figure 3 shows that the performance of g- and np-sprl depends drastically on the choice of tar-
get distribution. Further, we see that np-sprl does not outperform g-sprl in this environment,
although it should be able to match the bi-modal target distributions, which g-sprl cannot. Finally,
we see that wb-sprl outperforms both other versions of sprl and achieves a consistent perfor-
mance across target distributions. The illustrations in Figures 4 and 5 serve to illustrate the underly-
ing problem. As evident in Figure 4, NP-SPRL is indeed able to match the target distribution μι(c)
correctly while G-SPRL ultimately covers only one mode of the target density μι(c). As shown in
appendix B, NP-SPRL still only learns to solve one of the two tasks likely under μι(c). Investigating
Figure 4b more closely reveals that np-sprl only generates a “proper” curriculum for one of the
two target tasks (the left one in the images) by gradually interpolating between easy tasks and the
target task, while simply incorporating the second target task into pα,η without generating an appro-
priate curriculum for the agent to learn this task. Figure 5 shows that the curricula generated by g-
and NP-SPRL for μ2(c) are even more problematic. G-SPRL increases the variance of the Gaussian
context distribution to match the constant (negligible) likelihood that is assigned to the non-target
C⊆ R2 pg
*
-3
一G-SPRL - Random — WB-SPRL — ALP-GMM - PLR
—NP-SPRL - Default - GoalGAN — ACL	— VDS
0	50	100	150	200
Epoch
0	50	100	150	200
Epoch
μ2(C)
(b) Learning Performance (Point-Mass)
(a) Environment

6
3
Figure 3: (a) The point mass environment with its two-dimensional context space. The target distri-
butions μι(c) and μ2(c) encode the two gates with width Wg =0.5, in which the agent (black dot)
is required to navigate through a narrow gate at different positions to reach the goal (red cross). (b)
Discounted cumulative return over iterations obtained under different curricula for both distribu-
tions. Statistics (mean and standard error) are computed from 10 seeds.
6
Under review as a conference paper at ICLR 2022
(b) Pα,η (C) for different iterations of NP-SPRL (Point-Mass)
■ I I
G-SPRL NP-SPRL
(a) DKL (P(C) k μι(c))	(C) PV (C) for different iterations of G-SPRL (Point-Mass)
Figure 4: (a) Mean KL divergences between μι(c) and the final context distribution computed by
G- and NP-SPRL as well as the minimum and maximum over 10 seeds. (b) and (c) visualize pα,η(c)
of NP-SPRL and pν (c) of G-SPRL for increasing iterations of the algorithms (left to right).
tasks, while np-sprl now interpolates between none of the target tasks. Consequently, neither the
agent under g- nor np-sprl learns to solve any of the target tasks. The observed shortcomings of g-
and np-sprl are caused by their use of the KL divergence to measure similarity between distribu-
tions. Figure 6 shows that the KL divergence generates interpolations that are highly dependent on
the likelihood function of the distributions, as the KL divergence does not take the metric structure
of the sample space C into account. Indeed Gaussian distributions encode a (Euclidean) metric via
their log-likelihood, explaining the acceptable performance of G- and NP-SPRL for μι(c). Looking
at Figure 6b and the curricula generated by wb-sprl in Figure 7, we see that the explicit notion of
a (Euclidean) metric in wb -sprl leads to a gradual change in the tasks encoded by the curriculum
for both target distributions. This leads to higher learning performance as visualized in Figure 3.
5.3	Pick and Place
Next, we consider the pick-and-place task in the OpenAI gym environment suite (Brockman et al.,
2016) in which a robot is tasked to grasp a block on a table and move it to a desired position (see Fig-
ure 8). The sparse reward of this environment, only rewarding the robot upon completing the desired
task, makes it a very challenging exploration problem, as can be seen in Figure 8 in which (default)
sac does not learn this pick and place task. One way to alleviate such challenging exploration prob-
lems is to learn the task via a curriculum of starting states. Such starting states can be obtained from
an expert demonstration, i.e. a trajectory τ EXPERT : [0, 1] 7→ S. A curriculum over this trajectory is
then formally defined by choosing the context space to be the unit interval C = [0, 1] ⊆ R. The con-
textual parameter c only influences the initial state distribution p0,c = δs=τEXPERT(c) . We investigate
the np- and wb-sprl algorithms in this setting by recording one execution of a hand-crafted con-
troller that first moves the end-effector above the block to be grasped, then lowers the end-effector,
grasps the block and moves to the target. We slightly randomize the position of the goal as well as
the block to enforce that the agent learns a robust policy. In a realistic scenario, we only record the
expert demonstration at a discrete set of states {ti|i = 1, . . . , N} and hence we have a truly discrete
CRL setting in this experiment very well suited for np-sprl. Furthermore, the finite number of
①ɔu①沙①AlajY
500 -
400 -
300 -
200 -
100 -
0 -
(a) DKL (p(c) k μ2(c))
G-SPRLNP-SPRL
(b) pα,η (c) for different iterations of NP-SPRL (Point-Mass)
(c) pν (c) for different iterations of G-SPRL (Point-Mass)
Figure 5: (a) Mean KL divergences between μ2(c) and the final context distribution computed by
G- and NP-SPRL as well as the minimum and maximum over 10 seeds. (b) and (c) visualize pα,η(c)
of NP-SPRL and pν (c) of G-SPRL for increasing iterations of the algorithms (left to right).
7
Under review as a conference paper at ICLR 2022
(a) KL divergence interpolation: arg minp PkK=1 wk DKL (p k pk)
(b) Wasserstein barycenter (Equation 5)
Figure 6: Interpolations between unimodal (left) and bi-model (right) distributions p1 (c) and p2(c)
via a KL divergence-based interpolation and Wasserstein barycenters (5). In one case, all distribu-
tion are Gaussians or mixture of Gaussians (blue). In the other case, the distributions are uniform
distributions (orange). The Wasserstein barycenters are computed using a particle-based approxima-
tion (Feydy et al., 2019). The visualized PDFs are then estimated using a kernel density estimation.
This results in a small amount of smoothing for the uniform distribtions.
contexts avoids the necessity for function approximators to approximate the expected performance
J (π, c) in a context c, as we can simply estimate the performance in each (discrete) context via a
sliding window. We again investigate two target context distributions, one being a narrow Gaussian
target distribution with mean at C = 0 and negligible variance (μι(c)) and the other being again a
Dirac-delta with a negligible amount of probability in any context to be absolutely continuous w.r.t.
any distribution (μ2(c)). The results in Figure 8 show that WB-SPRL generates a curriculum that
allows the agent to learn a reliable policy by smoothly moving probability from later steps of the ex-
pert demonstration towards earlier ones. This learned policy is more reliable (25% vs. 100% success
rate) and faster ( 69 vs. 13 steps for task completion) than the demonstration. Both default learning
(always starting from the initial state) and np-sprl do not lead to successful policies. In appendix
B, we show that the non-effectiveness of np-sprl is again grounded in the degenerate interpolation
that puts no probability mass at intermediate time-steps regardless of the target distribution.
6	TeachMyAgent B enchmark
A final environment for evaluation is the TeachMyAgent benchmark proposed by (Romac et al.,
2021). In their evaluations accompanying the benchmark, Romac et al. reported a poor performance
of g-sprl compared to other automatic curriculum RL methods. Consequently, we want to inves-
tigate the improvement of wb-sprl in this benchmark to verify the observed benefits and identify
further directions for improving the sprl framework. Figure 9 shows that wb-sprl significantly
improves upon G-SPRL, achieving top performance across the methods in the mostly unfeasible sce-
nario. We see that WB -S PRL particularly lacks behind ALP-GMM in the mostly trivial setting. Figure
9c shows that WB -S PRL quickly reduces to uniform sampling in this setting (on average after 60 out
of 370 iterations), as it can incorporate all tasks while fulfilling the desired performance threshold.
This observation explains the similar performance between random sampling and wb-sprl in the
mostly trivial setting and further highlights an important conceptual difference between SPRL and
(a) wb-sprl (Maze)
I	I
(b) WB-SPRL (Point Mass μι (C))
(c) WB-SPRL (Point Mass μ2(c))


Figure 7: Visualizations of the empirical distributions pβ generated by WB-SPRL in the maze- (a)
and point-mass environments for both target distributions μι(c) and μ2(c) ((b) + (c)).
8
Under review as a conference paper at ICLR 2022
(a) wb-sprl Curriculum (Pick and Place)
Figure 8: (a) A wb-sprl curriculum for the pick-and-place task. The vertical dotted lines and
small images visualize the states corresponding to different time steps of the expert trajectory. The
colored bars indicate the context distributions pβ(c) for different iterations of WB-SPRL. The corre-
spondence between color and algorithm is shown via the colorbar on the right. (b) Success rates over
iterations for different learning algorithms in the pick and place task for target distribution μι (c) and
μ2(c). Mean and standard error is computed from 10 algorithm runs.
—Random — GoalGAN — ACL — VDS - Default
—WB-SPRL - ALP-GMM - PLR - NP-SPRL
Epoch	Epoch
(b) Learning Performance (Pick and Place)
alp-gmm: The maximization of learning progress in alp - gmm leads to a prioritized sampling of
the target (uniform distribution), efficiently avoiding tasks the agent has already learned. sprl does
not have a notion of avoiding “too easy” tasks as of now. Introducing such a prioritized sampling of
p(c) is hence an interesting future research direction and may allow to combine the benefits of the
sprl framework with other successful curriculum RL methods.
7 Conclusion
We investigated self-paced reinforcement learning (sprl) outside of its current parametric restric-
tions, revealing important shortcomings of the KL divergence for measuring the similarity of (task)
distributions in curriculum reinforcement learning. We showed that replacing the KL divergences,
originally employed in sprl, with Wasserstein distances alleviates the observed problems of sprl.
Apart from achieving higher empirical performance in the sprl framework, our results indicate
that introducing a notion of metric structure on the task space may be an important next step for
deriving a principled, yet practical, understanding of curriculum RL. Our findings motivate a vari-
ety of future investigations, such as more elaborate implementations of wb-sprl that e.g. directly
move the individual particles of the context distribution via the gradient of the performance mea-
sure VcJ(∏, C) instead of using the value distribution PJ(C) as a proxy. As highlighted in Section
6, importance-sampling of the context distributions are expected to further improve performance.
Finally, investigating other metrics than the Euclidean one considered in this paper is expected to
allow for principled interpolations in non-Euclidean spaces.
Student that
forgets
★★ /	**-
Mostly trivial
task space
★★
Mostly unfeasible
task space
cova Covar-GMM
ALP-GMM
R^nd Random
G-SPRL
Rugged
difficulty
★★★ '''∖
Variety of
students
★★★★
RIAC
WB-SPRL
(c) Evolution of pβ in the mostly trivial scenario
(a) Results in the TeachMyAgent benchmark
Q
0
Figure 9: a) Performance on the TeachMyAgent benchmark in the no expert knowledge setting.
Baseline results are taken from (Romac et al., 2021). Please refer to (Romac et al., 2021) for a
detailed explanation of the setup. b) + c) Visualization of the WB-SPRL curricula in the mostly
unfeasible and the mostly trivial task space scenario. The x-axis encodes the height of the obstacles
that the agents encounters while the y-axis encodes the distance between them.
9
Under review as a conference paper at ICLR 2022
References
Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron,
Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving rubik’s cube with a
robot hand. arXiv preprint arXiv:1910.07113, 2019.
Eugene L Allgower and Kurt Georg. Introduction to numerical continuation methods. SIAM, 2003.
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob
McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In
Neural Information Processing Systems (NeurIPS), 2017.
Adrien Baranes and Pierre-Yves Oudeyer. Intrinsically motivated goal exploration for active motor
learning in robots: A case study. In International Conference on Intelligent Robots and Systems
(IROS), 2010.
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.
Unifying count-based exploration and intrinsic motivation. In Neural Information Processing
Systems (NeurIPS), 2016.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng. Handbook of markov chain monte
carlo. CRC press, 2011.
Yongxin Chen, Tryphon T Georgiou, and Michele Pavon. Stochastic control liaisons: Richard
sinkhorn meets gaspard monge on a Schrodinger bridge. SIAM Review (SIREV), 63(2):249-313,
2021.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Neural Informa-
tion Processing Systems (NeurIPS), 2013.
Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch,
and Sergey Levine. Emergent complexity and zero-shot transfer via unsupervised environment
design. In Neural Information Processing Systems (NeurIPS), 2020.
Theresa Eimer, Andre Biedenkapp, Frank Hutter, and Marius Lindauer. Self-paced context evalu-
ation for contextual reinforcement learning. In International Conference on Machine Learning
(ICML), 2021.
Jean Feydy, ThibaUlt SejoUrne, Francois-Xavier Vialard, Shun-ichi Amari, Alain Trouve, and
Gabriel Peyre. Interpolating between optimal transport and mmd using sinkhorn divergences.
In International Conference on Artificial Intelligence and Statistics (AISTATS), 2019.
Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, and Pieter Abbeel. Reverse cur-
riculum generation for reinforcement learning. In Conference on Robot Learning (CoRL), 2017.
Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation for
reinforcement learning agents. In International Conference on Machine Learning (ICML), 2018.
Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, and Aviv Tamar. Bayesian reinforcement
learning: A survey. Foundations and Trends® in Machine Learning, 8(5-6):359-483, 2015.
Alex Graves, Marc G Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu. Auto-
mated curriculum learning for neural networks. In International Conference on Machine Learning
(ICML), 2017.
Assaf Hallak, Dotan Di Castro, and Shie Mannor. Contextual markov decision processes. arXiv
preprint arXiv:1502.02259, 2015.
Ashley Hill, Antonin Raffin, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, Rene Traore,
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Rad-
ford, John Schulman, Szymon Sidor, and Yuhuai Wu. Stable baselines. https://github.
com/hill-a/stable-baselines, 2018.
10
Under review as a conference paper at ICLR 2022
Lu Jiang, Deyu Meng, Qian Zhao, Shiguang Shan, and Alexander G Hauptmann. Self-paced cur-
riculum learning. In AAAI Conference on Artificial Intelligence (AAAI), 2015.
Minqi Jiang, Michael Dennis, Jack Parker-Holder, Jakob Foerster, Edward Grefenstette, and Tim
RocktascheL Replay-guided adversarial environment design. 2021a.
Minqi Jiang, Edward Grefenstette, and Tim RocktaScheL Prioritized level replay. In International
Conference on Machine Learning (ICML), 2021b.
Leonid Kantorovich. On the transfer of masses (in russian). Doklady Akademii Nauk, 37(2):227—
229, 1942.
Pascal Klink, Hany Abdulsamad, Boris Belousov, and Jan Peters. Self-paced contextual reinforce-
ment learning. In Conference on Robot Learning (CoRL), 2020a.
Pascal Klink, Carlo D’ Eramo, Jan R Peters, and Joni Pajarinen. In H. Larochelle, M. Ranzato,
R. Hadsell, M. F. Balcan, and H. Lin (eds.), Neural Information Processing Systems (NeurIPS),
2020b.
Pascal Klink, Hany Abdulsamad, Boris Belousov, Carlo D’Eramo, Jan Peters, and Joni Pajarinen.
A probabilistic interpretation of self-paced learning with applications to reinforcement learning.
Journal of Machine LearningResearch (JMLR), 22(182):1-52, 2021.
M Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable
models. In Neural Information Processing Systems (NeurIPS), 2010.
Chang Liu, Jingwei Zhuo, Pengyu Cheng, Ruiyi Zhang, Jun Zhu, and Lawrence Carin. Understand-
ing and accelerating particle-based variational inference. In International Conference on Machine
Learning (ICML), 2019.
Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference
algorithm. In Neural Information Processing Systems (NeurIPS), 2016.
Siqi Liu, Guy Lever, Zhe Wang, Josh Merel, SM Eslami, Daniel Hennes, Wojciech M Czarnecki,
Yuval Tassa, Shayegan Omidshafiei, Abbas Abdolmaleki, et al. From motor control to team play
in simulated humanoid football. arXiv preprint arXiv:2105.12196, 2021.
Marlos C Machado, Marc G Bellemare, and Michael Bowling. Count-based exploration with the
successor representation. In AAAI Conference on Artificial Intelligence (AAAI), 2020.
Deyu Meng, Qian Zhao, and Lu Jiang. A theoretical understanding of self-paced learning. Informa-
tion Sciences, 414:319-328, 2017.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Gaspard Monge. Memoire SUrIatheoriedeS deblais et des remblais. DerImprimerieRoyale, 1781.
Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E. Taylor, and Peter Stone.
Curriculum learning for reinforcement learning domains: A framework and survey. Journal of
Machine Learning Research (JMLR), 21(181):1-50, 2020.
Radford M Neal. Annealed importance sampling. Statistics and Computing, 11(2):125-139, 2001.
Gabriel Peyre, Marco Cuturi, et al. Computational optimal transport: With applications to data
science. Foundations and Trends® in Machine Learning, 11(5-6):355-607, 2019.
Remy Portelas, Cedric Colas, Katja Hofmann, and Pierre-Yves Oudeyer. Teacher algorithms for
curriculum learning of deep rl in continuously parameterized environments. In Conference on
Robot Learning (CoRL), 2019.
Sebastien Racaniere, Andrew K Lampinen, Adam Santoro, David P Reichert, Vlad Firoiu, and
Timothy P Lillicrap. Automated curricula through setter-solver interactions. In International
Conference on Learning Representations (ICLR), 2020.
11
Under review as a conference paper at ICLR 2022
Clement Romac, Remy Portelas, Katja Hofmann, and Pierre-Yves Oudeyer. Teachmyagent: a
benchmark for automatic curriculum learning in deep rl. International Conference on Machine
Learning (ICML), 2021.
Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approxima-
tors. In International Conference on Machine Learning (ICML), 2015.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. Nature, 550(7676):354, 2017.
Sainbayar Sukhbaatar, Zeming Lin, Ilya Kostrikov, Gabriel Synnaeve, Arthur Szlam, and Rob Fer-
gus. Intrinsic motivation and automatic curricula via asymmetric self-play. In International
Conference on Learning Representations (ICLR), 2018.
Richard S Sutton and Andrew G Barto. Introduction to Reinforcement Learning. MIT Press, 1998.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In International Conference on Intelligent Robots and Systems (IROS), 2012.
Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Courna-
peau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stefan J. van der
Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nel-
son, Eric Jones, Robert Kern, Eric Larson, C J Carey, Ilhan Polat, Yu Feng, Eric W. Moore,
Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero,
Charles R. Harris, Anne M. Archibald, Antonio H. Ribeiro, Fabian Pedregosa, Paul van Mul-
bregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing
in Python. Nature Methods,17:261-272, 2020.
Daphna Weinshall and Dan Amir. Theory of curriculum learning, with convex loss functions. Jour-
nal of Machine Learning Research (JMLR), 21(222):1-19, 2020.
Andre Wibisono. Sampling as optimization in the space of measures: The langevin dynamics as a
composite optimization problem. In Conference on Learning Theory (COLT), 2018.
Aaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli. Multi-task reinforcement learning: a
hierarchical bayesian approach. In International Conference on Machine Learning (ICML), 2007.
Jan Wohlke, Felix Schmitt, and Herke van Hoof. A performance-based start state curriculum frame-
work for reinforcement learning. In International Conference on Autonomous Agents and Multi-
agent Systems (AAMAS), pp. 1503-1511, 2020.
Xiaoxia Wu, Ethan Dyer, and Behnam Neyshabur. When do curricula work? In International
Conference on Learning Representations (ICLR), 2021.
Yunzhi Zhang, Pieter Abbeel, and Lerrel Pinto. Automatic curriculum learning through value dis-
agreement. In Neural Information Processing Systems (NeurIPS), 2020.
12
Under review as a conference paper at ICLR 2022
(a) Gaussian p0(c) centered in the middle of the spiral target p1 (c).
(b) Gaussian p0(c) located top-left of the spiral target p1 (c).
(c) Sine wave with Gaussian noise p0 (c).
Figure 10: Wasserstein barycenters between two distributions p0(c) and p1 (c). The above fig-
ures visualize the barycenters pw(c) = arg minp wW2(p, p1) + (1 - w)W2(p,p0) for w =
0, 0.2, 0.4, 0.6, 0.8, 1 (from left to right). The target p1 (c) is shown in green, while the respec-
tive barycenter pw (c) is shown in blue.
A	Illustration of Wasserstein Barycenters
This short appendix serves to showcase the capacity of Wasserstein barycenters to interpolate be-
tween complicated target distributions. Figure 10 visualizes different barycenters that interpolate
between different source distributions and a target distribution encoding samples contained in a spi-
ral. Modelling such a spiral, e.g. via Gaussian mixture models would require many components
to accurately model the narrow, curved line to which the samples are constrained. As we represent
the distributions via a set of particles (see appendix C), the barycenter computation does not require
knowledge of the log-likelihood function of the target distribution but only requires samples from
it, which is beneficial is the specification of the likelihood function is challenging. We see that
the barycenters gradually displace the particles to match the target distribution. For the sinusoidal
source distribution in Figure 10c, we can see that the idea of minimum action, which is inherent to
Wasserstein metrics, creates non-trivial interpolations that are still smooth w.r.t. the defined metric
(in this case the euclidean metric on R2).
B Qualitative Results
Figure 11 shows trajectories that have been generated by agents trained with different curricula. We
see that directly learning on the two target tasks prevents the agent from finding the gates in the
wall to pass through. Consequently, the agent minimizes the distance to the goal by moving right
in front of the wall (but not crashing into it) to accumulate reward over time. We see that random
learning indeed generates meaningful behavior. This behavior is, however, not precise enough to
pass reliably through the wall. As mentioned in the main paper, g-sprl and np-sprl only learn
to pass through one of the gates. We see that in the particular run displayed in Figure 11, np-sprl
stayed closer to the wall than g-sprl in those contexts in which it could not pass the wall. This
better behavior is probably a result of np-sprl ultimately sampling both target tasks during training
while g-sprl only focuses on one. Finally, wb-sprl learns a policy that can pass through both
gates reliably, showing that the gradual interpolation towards both target tasks allowed to learn both
of them. alp-gmm, plr and vds also learn good policies. The generated trajectories are, however,
not as precise as the ones learned with wb-sprl. The reason for this lack of precision is probably
that these methods have no notion of target distribution and hence cannot focus learning on the target
13
Under review as a conference paper at ICLR 2022
(a) Default
(b) Random
(d) np-sprl
(g) goalgan	(h) acl
(c) g-sprl
(i) plr	(j) vds
Λ
X
Figure 11: Final trajectories generated by the different investigated curricula in the point mass en-
vironment. The color encodes the context: Blue represents gates positioned at the left and red gates
positioned at the right.
tasks at later iterations. goalgan partly creates meaningful behavior. However, this behavior is
unreliable and hence leads to low returns due to the agent frequently crashing into the wall. acl
does not learn to solve the task. The reason for the poor performance of acl in this task is probably
explained by the fact that acl models the curriculum generation as a bandit problem, which in the
50 × 50 grid used in the point mass experiment needs to choose between 2500 arms.
Figure 12 shows the distribution pα,η (c) generated by NP-SPRL for different iterations in the pick
and place environment. We see that regardless of the target distribution, the interpolation skips
all intermediate states of the expert demonstration. This is surprising, given that the interpolation
worked at least somewhat desirably in the point mass environment for a Gaussian target distribution.
Comparing Figures 12a and 12b, we see that the Gaussian target distribution leads to pα,η (c) putting
probability mass on a few more states than only the initial one. However, these states are still so
close to the initial state that no learning takes place.
C Implementation Details of wb-sprl
The implementation of WB-SPRL relies on the computation of the Wasserstein barycenter pβ (c). To
compute this barycenter, We represent the distributions μ(c), PJ (C) and q(c) via a set of particles.
For a distribution p(c), we denote the particle-based representation as P(C)=& PN=I 6斗(c) where
Cn〜P(C) and 6片 is a Dirac delta located at % This representation allows to compute the so-called
Monge maps Tp1 →p% : C → C that encode how to move the particles from the distribution pi to Pk.
In the case of the Wasserstein-2 distances, we can use a linear combination of these Monge maps as
a reasonable approximation to the exact barycenter Pβ(C)
Pβ(c) ≈ (βiTq→pj + β2Tq→μ)# 贸C)
Iteration 24			Iteration 26	
Iteration 28			Iteration 30 [			
(a) Target μι (C)
Iteration 24				Iteration 26 Γ		
Iteration 28 lI				Iteration 30		
(b) Target μ2(c)
Figure 12: Visualization of the curriculum distribution pα,η (c) of NP-SPRL for different iterations
in the pick and place tasks. (a) shows the curriculum generated for the Gaussian target distribution
μι(c). (b) shows the curriculum in case of using the Dirac-delta-like distribution μ2(c).
14
Under review as a conference paper at ICLR 2022
	General			g-sprl			np-sprl	wb-sprl
Env.	δ	KL |M	nSTEP	nOFFSET	σLB	DKLLB	σLB	DKLLB	δH
Maze	.65	.25|1	10000	5	-	-	--	.9
Point-Mass	4	.25|.25	4096	5	[.2 .1875]	8000	4	1000	6
PNP	.5	.5|.13	3000	25	-	-	--	.8
TMA	160	-|.25	50 EP.	0	-	-	--	270
Table 1: Hyperparameters of the investigated sprl algorithms in the different learning environ-
ments, as described in appendix D. Note the differentiation between for G- and NP-SPRL (KL) and
WB-SPRL (M).
where T#q is the push-forward of q by T, i.e. the application of the weighted Monge maps to the
particles of q
T#q(c) = N PN=1 δτ (Cn)(C).
The benefit of this approximation is that the maps Tq→pj and T于→口 only need to be computed once
in order to compute Pe (c) for arbitrary β. This fast computation of pβ (c) allows to quickly optimize
β via objective (7).
To make the optimization of Pe(c) even faster, We replace the two Wasserstein distances in (7) by
approximations, resulting in
min X ι∣τ,j→μ(Cn)- Cne k2 s∙t 1 X J(n, Cne) ≥δ
Wμ,Wj	N
n=1	n=1
n=1
The objective term of above constrained optimization problem is a computationally cheap replace-
ment for W2 (Pe, μ). Instead of computing the Wasserstein distance to μ, We simply ask the trans-
ported particles of Pe to follow the transport plan Tq→ as closely as possible. This avoids the
(comparatively) expensive computation of W2(pe, μ) for each choice of β. Finally, we also avoid
to compute W2(pe, q) but rather replace it with a pessimistic substitute NN PN=IkCne — Cnk2 ≤ &
This substitute is guaranteed to be larger than W2(Pe, q)2 since when using the particle representa-
tion, the Wasserstein distance W2(Pe, q) corresponds to finding a permutation π : [1, N] 7→ [1, N]
that optimizes
min∏《N PN=IkCnW - C∏(n)k2∙
Note that the approximations to the Wasserstein distances in (7) do not alter the transport plan and
with that Pe(c). The approximations may simply alter the optimal parameters β* to (7). However,
since the approximations have the same asymptotic behavior, these differences do not hinder good
learning performance as can be seen in the experimental section. For the computations of the Monge
maps, we use the GeomLoss library (Feydy et al., 2019) which allows to solve entropy regularized
optimal transport problems (Cuturi, 2013). We then use the SciPy library (Virtanen et al., 2020) to
adjust the weights β subject to the performance and distance constraints.
D Experimental Details
This section discusses hyperparameters and additional details of the conducted experiments that
could not be provided in the main text due to space limitations.
D.1 Algorithm Hyperparameters
The two main parameters of the three SPRL algorithms are the performance threshold δ (and δH) as
well as the allowed distance between subsequent distributions (note that we say “distance” as the
interpretation is different for wb-sprl than for g- and np-sprl). We did not perform an extensive
hyperparameter search for these parameters but proceeded as follows: The performance threshold
δ is chosen such that it is around 50% of the maximally achievable reward and the parameter M
is chosen such that around 10 iterations are required to move particles across the whole context
space. We then evaluated a larger and a lower value of the parameters and chose the best. For g-
and NP-SPRL, we initialized KL with value of 0.05 used in the initial experiments by Klink et al.
15
Under review as a conference paper at ICLR 2022
However, we realized that larger values slightly improved performance. Additionally, the number of
steps per epoch nstep (i.e. between an update of the context distribution) and the number of epochs
that take place before any context distribution update is allowed noffset need to be chosen. In our
experiments, the nstep parameter is chosen such that at least 20 episodes are completed per context
distribution update. This seemed enough for the two- and one-dimensional context spaces consid-
ered. For higher-dimensional context spaces, we assume that it is advisable to either increase the
number of completed episodes or to reduce the trust-region parameter, as the regression problem
for the expected performance J(π, c) will probably require more data. The noffset parameter mostly
serves to give the policy π some time to become proficient on easy tasks and the estimator of J(π, c)
to get somewhat accurate. Nonetheless, sprl can also works without this offset, as can be seen in
the TMA benchmark. Hence, noffset can be rather seen as a “safety” parameter that interacts with the
trust-region parameters M and KL as well as nstep.
For G-SPRL, Klink et al. employed a lower bound on the standard deviation σlb of the context
distribution that needs to be respected until the KL divergence w.r.t. μ(c) falls below a threshold
DKL. This avoids a collapse of the context distribution in the case of a very narrow target context
distribution μ(c). In NP-SPRL, we translated this into a lower bound on the entropy of the context
distribution. For wb-sprl, this lower bound is not required. Only, the additional performance lower
bound δH > δ for computing the “value distribution” pJ (c). Aforementioned parameters are listed
in Table 1 for the different environments.
For alp-gmm, the relevant hyperparameters are the percentage of random samples drawn from
the context space prand, the number of completed learning episodes between the update of the
context distribution nrollout as well as the maximum buffer size of past trajectories to keep sbuffer.
Similar to Klink et al. (2021), we chose them by a grid-search over (prand, nrollout, sbuffer) ∈
{0.1, 0.2, 0.3} × {50, 100, 200} × {500, 1000, 2000} for the maze and pick and place environment.
For the point-mass environment, we took the values from Klink et al. (2021).
For goalgan, we again took the parameters from Klink et al. (2021) for the point-mass environ-
ment and tuned the amount of random noise that is added on top of each sample δnoise, the number
of policy rollouts between the update of the context distribution nrollout as well as the percentage of
samples drawn from the success buffer psuccess for the maze and pick and place task via a grid search
over(δnoise,nrollout,psuccess) ∈ {0.025, 0.05, 0.1} × {50, 100, 200} × {0.1, 0.2, 0.3}.
Finally, for acl, the Exp3.S algorithm that ultimately realizes the curriculum requires two hy-
perparameters to be chosen: the scale factor for the updates of the arm probabilities η and the
parameter of the -greedy exploration strategy. We combine ACL with the absolute learn-
ing progress (ALP) metric also used in alp-gmm and conduct a hyperparameter search over
(η, ) ∈ {0.05, 0.1, 0.2} × {0.01, 0.025, 0.05}. In the discrete pick and place (or discretized maze
and point-mass) environment, the absolute learning progress in a context c can be estimated by keep-
ing track of the last reward obtained in c and then computing the absolute difference between the
return obtained from the current policy execution and the stored last reward. Implementing the acl
algorithm by Graves et al. (2017), we had numerical issues due to the normalization of the ALPs
via quantiles computed from a representative sample. This is probably due to the sparse nature of
the task. Consequently, we normalized via the maximum and minimum ALP seen over the entire
history of tasks.
For PLR, the staleness coefficient ρ, the score temperature β as well as the replay probability p
need to be chosen. We did a grid-search over (ρ, β, p) ∈ {0.15, 0.3, 0.45} × {0.15, 0.3, 0.45} ×
{0.55, 0.7, 0.85} and chose the best configuration for each environment.
For VDS, the parameters for the training of the Q-function ensemble, i.e. the learning rate lr, the
number of epochs nep and the number of minibatches nbatch, need to be chosen. Just as for PLR, we
conducted a grid-search over (lr, nep, nbatch) ∈ {10-4, 5×10-4, 10-3} × {3, 5, 10} × {20, 40, 80}.
We chose the same number of environment steps between training of the Q-function as for the SPRL
algorithms. The parameters of all employed baselines are given in tables 2 and 3.
D.2 Task Descriptions
We now detail details of the individual experiments, such as context-, state- and action spaces as well
as the employed RL algorithms. As RL agents, we use ppo and sac implemented in the Stable
Baselines library (Hill et al., 2018). In all tasks, we represent policy- and/or value function
networks using neural networks with three hidden layers of 128 neurons and tanh activations.
16
Under review as a conference paper at ICLR 2022
Env.	alp-gmm			GOALGAN			ACL	
	pRAND	nROLLOUT	sBUFFER	δNOISE	nROLLOUT	pSUCCESS	η	
Maze	.2	100	500	.1	100	.3	0.01	0.2
Point-Mass	.2	100	500	.1	200	.3	0.01	0.2
PNP	.2	50	500	.1	50	.2	.05	.05
Table 2: Hyperparameters of the investigated baseline algorithms in the different learning environ-
ments, as described in appendix D.
D.2.1 MAZE
The maze task is simulated using MuJoCo (Todorov et al., 2012) by defining a sphere with radius
.15 that can move along two prismatic joints along x- and y-direction. The goal (i.e. the context)
can be chosen within C = [-3, 15] × [-3, 15] ⊆ R. The actually reachable space of positions (and
with that goals) is a subset of [-1, 13] × [-1, 13] due to the “hole” because of the inner walls of the
maze. The state of the environment is given by the x- and y-position and -velocity of the agent. The
reward is sparse, only returning a reward of one if the goal is reached and zero otherwise. A goal
is considered reached if the Euclidean distance between goal and position of the point-mass falls
below 0.15.
We use the PPO algorithm for learning in this task, updating the policy every 10.000 environment
steps for 10 epochs with 20 mini-batches. The parameter λ for the generalized advantage estimation
is set to 0.995 and the entropy coefficient is set to zero. All other parameters are set to the default of
the Stable Baselines implementation.
For algorithms that support the specification of an initial context distribution (sprl variants and
GOALGAN), we initialize the context distribution to be uniform over [-1, 1] × [-1, 1] for GOALGAN,
NP- SPRL and WB-SPRL and N(0, .06I) for G-SPRL. This initial context distribution samples goals
around the initial position of the agent.
D.3 Point-Mass
The environment setup is the same as the one investigated by Klink et al. (2020b; 2021) with the
only difference in the target context distributions μι(c) and μ2(c). We use PPO with 4.096 steps
per policy update, four update epochs with 8 mini-batches in each policy update, λ=0.99 and an
entropy coefficient of zero. All other parameters are left to the implementation defaults of the
Stable Baselines implementation.
D.4 Pick and Place
The investigated environment is a slightly modified version of the FetchPickAndPlace envi-
ronment from the OpenAI Gym Brockman et al. (2016) environment suite. The dynamics and
reward functions are exactly the same as the ones from the original environment. We only modify
the initial position of the cube as well as the goal to which the cube needs to be moved. The initial
x- and y-position of the cube on the table is sampled uniformly within [1.31, 1.37] × [0.72, 0.78]
and the goal position is sampled within [1.31, 1.37] × [0.87, 0.93] × [0.59, 0.65]. If the environment
is reset to a state in which the cube is in contact with the gripper, its position is not randomized as
otherwise the cube may e.g. fall out of the gripper due to randomization, which would counteract
the idea behind the curriculum.
Env.	PLR			VDS		
	ρ	β	P	LR	nEP	nBATCH
Maze	.3	.3	.7	5×10-4	10	40
Point-Mass	.3	.3	.7	5×10-4	3	40
PNP	.3	.45	.7	5×10-4	10	40
Table 3: Hyperparameters of the investigated baseline algorithms in the different learning environ-
ments, as described in appendix D.
17
Under review as a conference paper at ICLR 2022
The demonstration is generated from a simple controller that first aligns the gripper with a position
10cm above the cube, then lowers the gripper until the cube is between its two fingers, closes the
gripper and then moves the endeffector to the goal position. The control command for the robot
endeffector is computed via
U = sign(∆χ) max(0.2, ∣∆χ∣),	∆χ = Xdes - x,
where xdes is the desired- and x the current endeffector position. Lower bounding the control com-
mand by 0.2 avoids the movements becoming very small as the endeffector approaches the target
position, which would lead to unnecessarily long demonstrations. As soon as we are within a 0.01
distance to the target position we switch to the next step in the previously outlined sequence.
As an RL agent, we use SAC with 5 steps per policy update, a batch size of 512, a maximum replay
buffer size of 100.000 and a learning rate 3×10-4. All other parameters are left to the implementa-
tion defaults of the Stable Baselines implementation.
As in the maze task, we specify initial context distributions for goalgan and the investigated sprl
variants. These context distributions assign uniform probability to the last 4 time-steps of the trajec-
tory.
18