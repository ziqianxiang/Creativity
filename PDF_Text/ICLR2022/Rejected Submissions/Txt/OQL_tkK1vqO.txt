Under review as a conference paper at ICLR 2022
ZARTS: On Zero-order Optimization for Neu-
ral Architecture Search
Anonymous authors
Paper under double-blind review
Ab stract
Differentiable architecture search (DARTS) has been a popular one-shot paradigm
for NAS due to its high efficiency. It introduces trainable architecture parameters
to represent the importance of candidate operations and proposes first/second-
order approximation to estimate their gradients, making it possible to solve NAS
by gradient descent algorithm. However, our in-depth empirical results show that
the approximation will often distort the loss landscape, leading to the biased ob-
jective to optimize and in turn inaccurate gradient estimation for architecture pa-
rameters. This work turns to zero-order optimization and proposes a novel NAS
scheme, called ZARTS, to search without enforcing the above approximation.
Specifically, three representative zero-order optimization methods are introduced:
RS, MGS, and GLD, among which MGS performs best by balancing the accuracy
and speed. Moreover, we explore the connections between RS/MGS and gradient
descent algorithm and show that our ZARTS can be seen as a robust gradient-free
counterpart to DARTS. Extensive experiments on multiple datasets and search
spaces show the remarkable performance of our method. In particular, results on
12 benchmarks verify the outstanding robustness of ZARTS, where the perfor-
mance of DARTS collapses due to its known instability issue. Also, we search
on the search space of DARTS to compare with peer methods, and our discovered
architecture achieves 97.54% accuracy on CIFAR-10 and 75.7% top-1 accuracy
on ImageNet, which are state-of-the-art performance.
1	Introduction
Despite their success, neural networks are still designed mainly by humans (Simonyan & Zisserman,
2014; He et al., 2016; Howard et al., 2017). It remains open to automatically discover effective and
efficient architectures. The problem of neural architecture search (NAS) has attracted wide attention,
which can be modeled as bi-level optimization for network architectures and operation weights.
One-shot NAS (Bender et al., 2018) is a popular search framework that regards neural architec-
tures as directed acyclic graphs (DAG) and constructs a supernet with all possible connections and
operations in the search space. DARTS (Liu et al., 2019) further introduces trainable architecture pa-
rameters to represent the importance of candidate operations, which are alternately trained by SGD
optimizer along with network weights. It proposes a first-order approximation to estimate the gra-
dients of architecture parameters, which is biased and may lead to the severe instability issue shown
by (Bi et al., 2019). Other works (Zela et al., 2020b; Chen & Hsieh, 2020) point out that architecture
parameters will converge to a sharp local minimum resulting in the instability issue and introduces
extra regularization items so that architecture parameters converge to a flat local minimum.
In this paper, we empirically show that the first-order approximation of optimal network weights
sharpens the loss landscape and results in the instability issue of DARTS. It also shifts the global
minimum, misleading the training of architecture parameters. To this end, we discard such approx-
imation and turn to zero-order optimization algorithms, which can run without the requirement that
the search loss is differentiable w.r.t. architecture parameters. Specifically, we introduce a novel
NAS scheme named ZARTS, which outperforms DARTS by a large margin and can discover effi-
cient architectures stably on multiple public benchmarks.
In a nutshell, this paper sheds light on the frontier of NAS in the following aspects:
1
Under review as a conference paper at ICLR 2022
1)	Establishing zero-order based robust paradigm to solve bi-level optimization for NAS. Dif-
ferentiable architecture search has been a well-developed area (Liu et al., 2019; Xu et al., 2020b;
Wang et al., 2020b) which solves the bi-level optimization of NAS by gradient descent algorithms.
However, this paradigm suffers from the instability issue during search since biased approximation
for optimal network weights distorts the loss landscape, as shown in Fig. 1 (a) and (b). To this end,
we propose a flexible zero-order optimization NAS framework to solve the bi-level optimization
problem, which is compatible with multiple potential gradient-free algorithms in the literature.
2)	Uncovering the connection between zero-order architecture search and DARTS. This work
introduces three representative zero-order optimization algorithms without enforcing the unverified
differentiability assumption for search loss w.r.t. architecture parameters. We reveal the connections
between the zero-order algorithms and gradient descent algorithm, showing that two implementa-
tions of ZARTS can be seen as gradient-free counterparts to DARTS, being more stable and robust.
3)	Strong empirical performance and robustness. Experiments on four datasets and five search
spaces have been conducted to evaluate the performance of our method. Unlike DARTS that suffers
the severe instability issue shown by (Zela et al., 2020b; Bi et al., 2019), ZARTS can stably dis-
cover effective architectures on various benchmarks. In particular, the searched architecture achieves
75.7% top-1 accuracy on ImageNet, outperforming DARTS and most of its variants.
2	Related Work
One-shot Neural Architecture Search. Bender et al. (2018) construct a supernet so that all can-
didate architectures can be seen as its sub-graph. DARTS (Liu et al., 2019) introduces architecture
parameters to represent the importance of operations in the supernet and update them by gradient
descent algorithm. Some works (Xu et al., 2020b; Wang et al., 2020b; Dong & Yang, 2019) reduce
the memory requirement of DARTS in the search process. Other works (Zela et al., 2020b; Chen &
Hsieh, 2020) point out the instability issue of DARTS, i.e., skip-connection gradually dominates the
normal cells, leading to performance collapse during the search stage.
Bi-level Optimization for NAS. NAS can be modeled as a bi-level optimization for architecture pa-
rameters and network weights. DARTS (Liu et al., 2019) proposes first/second-order approximations
to estimate gradients of architecture parameters so that they can be trained by gradient descent algo-
rithms. However, we show that such approximation will distort the loss landscape and mislead the
training of architecture parameters. Amended-DARTS (Bi et al., 2019) derives an analytic formula
of the gradient w.r.t. architecture parameters that includes the inverse of Hessian matrix of network
weights, which is even unfeasible to compute. In contrast, this work discards the approximation in
DARTS and attempts to solve the bi-level optimization by gradient-free algorithms.
Zero-order Optimization. Unlike gradient-based optimization methods that require the objective
differentiable w.r.t. the parameters, zero-order optimization can train parameters when the gradient
of objective is unavailable or difficult to obtain, which has been widely used in adversarial robust-
ness for neural networks (Chen et al., 2017; Ilyas et al., 2018), meta learning (Song et al., 2020), and
transfer learning (Tsai et al., 2020). Liu et al. (2020b) aim at AutoML and utilize zero-order opti-
mization to discover optimal configurations for ML pipelines. In this work, (to our best knowledge)
we make the first attempt to apply zero-order optimization to NAS and experiment with multiple
algorithms, from vanilla random search (Flaxman et al., 2004) to more advanced and effective direct
search (Golovin et al., 2020), showing its great superiority against gradient-based methods.
3	B i-level Optimization in DARTS
Following one-shot NAS (Bender et al., 2018), DARTS constructs a supernet stacked by normal cells
and reduction cells. Cells in the supernet are represented by directed acyclic graphs (DAG) with N
nodes {xi}iN=1, which represents latent feature maps. Each edge ei,j contains multiple operations
{oi,j , o ∈ O}, whose importance is represented by architecture parameters αio,j . Therefore, NAS
can be modeled as a bi-level optimization problem by alternately updating the operation weights ω
(parameters within candidate operations on each edge) and the architecture parameters α:
α
ω
min Lvai(ω*(α), α);	s.t. ω*(α) = argminLtrain(ω, α).
(1)
2
Under review as a conference paper at ICLR 2022
(a) Landscape w.r.t. a with g；St	(b) Landscape w.r.t. a with ω*	(C) Tracks of optimization path
Figure 1: Loss landscapes w.r.t. architecture parameters α where the red star indicates the global
minimum. (a) illustrates the landscape with ω%t. (b) illustrates the landscape with ω*, which is
obtained by training ω for 10 iterations. To fairly compare the landscapes in (a) and (b), we utilize
the same model and candidate α points. We observe the first-order approximation sharpens the
landscape. (c) displays the tracks of optimization path of DARTS and our ZARTS. Starting at the
same initial point, ZARTS can converge to the global minimum but DARTS fails.
3.1	Fundamental Limitations in the DARTS Framework
By enforcing an unverified (and in fact difficult to verify) assumption that the search loss
Lvai(ω*(α), α) is differentiable w.r.t. α, DARTS (Liu et al., 2019) proposes a second-order ap-
proximation for the optimal weights ω*(α) by applying one-step gradient descent:
ω*(α) ≈ ω2nd(α) = ω - ξVωLtrain(ω, α) = ω0,	(2)
where ξ is the learning rate to update network weights. Thus the gradient of the loss function w.r.t. α,
VaLval(ω*(α), α), can be computed by the chain rule: VaLval(ω*(α), α) ≈ VɑLvai(ω0, α) 一
ξV2α,ωLtrain(ω, α)Vω0 Lval (ω0, α). Nevertheless, the second-order partial derivative is hard to
compute, so the authors adopt the difference method, which is proved in Appendix A.1.
To further reduce the computational cost, first-order approximation is introduced by assuming
ω*(α) being independent of α, as shown in Eq. 3, which is much faster and widely used in many
variants of DARTS (Chen et al., 2019; Wang et al., 2020b; Zela et al., 2020b).
ω*(α) ≈ ω"∕α) = w.	(3)
The gradient is then simplified as: VaLvaι(ω*(α), α) ≈ VaLvaι(ω, α), which, however, further
exacerbates the estimation bias.
Reexamining the definition of ω*(α) in Eq. 1, one would note that it is intractable to derive a
mathematical expression for ω*(α), making Lvai(ω*(α), α) even non-differentiable w.r.t. α. Yet
DARTS has to compromise with such approximations as Eq. 2 and Eq. 3 so that differentiability is
established and SGD can be applied. However, such sketchy estimation of optimal operation weights
can distort the loss landscape w.r.t. architecture parameters and thus mislead the search procedure,
which is shown in Fig. 1 and analyzed in the next section.
3.2	Distorted Landscape and Incorrect Optimization Process in DARTS
Fig. 1 illustrates the loss landscape with perturbations on architecture parameters α, showing how
different approximations of ω* affect the search process. We train a supernet for 50 epochs and ran-
domly select two orthonormal vectors as the directions to perturb α. The same group of perturbation
directions is used to draw landscapes in Fig. 1(a) and (b) for a fair comparison. Fig. 1(a) shows the
loss landscape with the first-order approximation in DARTS, ω%∕α) = ω, while Fig. 1(b) shows
the loss landscape with more accurate ω*(α), which is obtained by fine-tuning the network weights
ω for 10 iterations for each α. Landscapes (contours) are plotted by evaluating L at grid points
ranged from -1 to 1 at an interval of 0.02 in both directions. Global minima are marked with stars on
the landscapes, from which we have two observations: 1) The approximation ω[(α) = ω shifts
the global minimum and sharpens the landscape 1, which is the representative characteristic of in-
stability issue as pointed out by (Zela et al., 2020b). 2) Accurate estimation for ω* leads to a flatter
landscape, indicating that the instability issue can be alleviated. Moreover, we display the landscape
with second-order approximation ω^d in Appendix A.2, which is also sharp but slightly flatter than
1A “sharp” landscape has denser contours than a “flat” one.
3
Under review as a conference paper at ICLR 2022
Algorithm 1: ZARTS: Zero-order Optimization Framework for Architecture Search
Hyper-parameters:
Operation weights ω, architecture parameters α, sampling number N , iteration number M ,
update estimation function φ(∙).
while not converged do
Sample candidates: {ui}N=ι, and estimate optimal operation weights ω*(α±) by
descending VωLtrain(ω, α±) for M iterations, where α± = α±g;
Compute descent direction: u* = φ ({ui, ω*(α±)}N=J;
Update architecture parameters: α - α + u*;
? The sampling strategies and update estimation functions φ(∙) for three different zero-order
optimization algorithms are detailed in Table 1.
Table 1: Configuration of three methods used in the ZARTS scheme. The main difference lies in the
meaning of function φ(∙): RS follows the traditional gradient estimation algorithms, MGS estimates
the update according to the improvement of loss function, while GLD uses direct search. Note that
the ZARTS framework is general and can support more configurations besides the listed ones.
Algorithm	Sampling strategy	Update estimation function φ ({ui, ω*(αi)}N=ι)
ZARTS-RS	Ui 〜 q(u∣α), any spherically symmetric distribution.	U* = -ξ ∙ 21(N) Pg] [L(a + μUi) - L(a - μui)] Ui (Eq. 6)	
ZARTS-MGS	Ui 〜 q(u∣α), any proposal distribution.	U* = PN=JPN(Ue(Uj)|°严[画10)	
ZARTS-GLD	Ui 〜Sd-1, a UnifOrm distri- bution on a unit sphere.	U* = arg mini {L(α)∣α = α, & = α + Ui} (Eq. 12)
Fig. 1 (a). Consequently, we discard the first/second-order approximation in DARTS and instead
use more accurate ω* coordinated with zero-order optimization.
Figure 1 (c) shows the optimization paths of DARTS and three methods of ZARTS, illustrating how
the approximation in DARTS affects the search process. Starting from the same randomly generated
position, we update architecture parameters α for 10 iterations by DARTS and ZARTS and draw
the tracks of the optimization path. ZARTS can gradually converge to the global minimum, while
DARTS converges to an incorrect point.
4	Zero-order Optimization for Architecture Search
This paper goes beyond the restrictive first/second-order approximation in DARTS and proposes to
train architecture parameters α in a zero-order optimization manner, allowing for more accurate esti-
mation for ω*(α). The generic form of our ZARTS framework is outlined in Alg. 1. Specifically, we
select three representative methods, including a vanilla zero-order optimization algorithm, random
search (RS) (Liu et al., 2020a), and two advanced algorithms: Maximum-likelihood Guided Param-
eter Search (MGS) (Welleck & Cho, 2020) and GradientLess Descent (GLD) (Golovin et al., 2020).
They are presented in Sec. 4.1, 4.2, 4.3 also as preliminaries. Further, we theoretically establish the
connection between ZARTS and DARTS, showing that ZARTS with RS and MGS optimizer can be
seen as an expansion of DARTS. In the following, we use L(α)，Lvai(ω*(α), α) to denote the
objective w.r.t. architecture parameters α ∈ Rd (Eq. 1), and L(α + U)，Lvai (ω* (α + u), α + u),
where u is the update for α.
4.1	ZARTS -RS via Random Search
Typical zero-order optimization methods have no access to first-order gradient information and con-
struct gradient estimators based on zero-order information, typically, the function evaluation. As
discussed in (Liu et al., 2020a), gradient estimation techniques can be categorized into different
types based on the required number of function evaluations. One-point gradient estimator is one of
the most popular algorithms (Liu et al., 2020a):
VaL(α):="® L(α + μu)u,	(4)
μ
4
Under review as a conference paper at ICLR 2022
where U 〜q is sampled from a spherically symmetric distribution q, μ > 0 is a smoothing param-
eter, and φ(d) is a dimension-dependent factor related to q. Specifically, q can either be a standard
multivariate normal distribution N(0, I) with 夕(d) = 1, or a multivariate uniform distribution on a
unit sphere Sd-1 with 夕(d) = d. Intuitively, ZARTS-RS samples nearby points from α and yields
a large updating step if the function value is high, consistent with the definition of the gradient.
Two-point gradient estimator is a natural extension of one-point estimator:
弋aL(α) := ^F)[L(α + μu) - L(α - μu)] u.
2μ
(5)
We can also sample and average a batch of gradient estimations as Eq. 6 to reduce the variance of
gradient estimators, resulting in the multi-point estimator:
ʌ —	9(d) J 一 . 一 一
VaL(α) ：= -~^ ^X [L(α + μUi) - L(a - μUi)] ui.
2μN i=1
(6)
Following the first-order gradient descent algorithms, the architecture parameters α are updated
by α - α -东aL(α) With the learning rate ξ. Our implementation is compatible with all the
estimators above, and we use multi-point estimator (Eq. 6) by default.
4.2	ZARTS -MGS via Maximum-likelihood Guided Parameter Search
Maximum-likelihood guided parameter search (MGS) is an advanced zero-order optimization algo-
rithm for machine translation (Welleck & Cho, 2020). We make the attempt to apply it to the NAS
task. We first define a distribution for the update of architecture parameters, U, as follows:
P(u∣α)
p(u∣α)
-z(oτ
,	Lg + u)-L(a)
Z (α) exp(	T
(7)
where je(u∣ɑ) = exp (-[L(α + U) -L(α)]∕τ) is an unnormalized exponential distribution, and
Z(α) = ∕e(u∣α)du is its normalization coefficient. T is a temperature parameter controlling the
variance of the distribution.
Intuitively, U with higher probability makes a more significant improvement on the objective
function. Therefore, the optimal update of architecture parameters can be estimated by u* =
Eu〜p(u∣α) [u]. However, since the probability p(u∣α) is an implicit function relying on L(α + u),
making it impractical to obtain the expectation, we refer to (Welleck & Cho, 2020) and apply im-
portance sampling to sample from a proposal distribution q(u∣α) with known probability function:
u* = Eu~P(u|a)[u] = / pZuaudu = / q(u|a) ]z(p((UX)°) u] du
—E	I' P(UIa) 1J	〜ɪ X	Γ P(UiIa)	,	*V	Z81
=Eu~q(u1a)	[z(a)q(U∣a)uJ	≈ N =	|_Z(a)q(Ui∣a)Ui] ,	U ,	⑻
where {Ui}iN=1 are sampled from the proposal distribution q(UIa). Similarly, the normalization
coefficient Z(a) can be computed as follows:
Z (a)=Z e(u|a)du ≈ N X [ pu⅛y ].	⑼
For convenience, we define a ratio representing the weight on each sample as ce(UIa) =
Consequently, the optimal update for architecture parameters in Eq. 8 can be computed by:
e(u∣α)
q(uIa)
N
X
i=1
ce(UiIa)
u	Ui
Pj=I e(ujIa).
N
X
i=1
exp(-[L(a + Ui) - L(a)]∕τ) ∕q(ui∣a)
PN=I exp (-[L(a + Uj) - L(a)]∕τ) /q(uj|a)
(10)
Finally, the architecture parameters are updated by a J a + U*. Additional importance sampling
diagnostics are conducted to verify the effectiveness of ZARTS-MGS (see results in Appendix A.3).
U *
5
Under review as a conference paper at ICLR 2022
4.3	ZARTS -GLD via GradientLess Descent
Unlike the above two algorithms that estimate gradient or the update for α, Golovin et al. (2020)
propose the so-called GradientLess Descent (GLD) algorithm, which falls into the category of truly
gradient-free (or direct search) methods. This work provides solid theoretical proof on the efficacy
and efficiency of this approach and suggestions on the choice of search radius boundaries. Particu-
larly, the authors prove the distance between the optimal minimum and the solution given by GLD
is bounded and positively correlated with the condition number of the objective function, where the
condition number Q is defined as:
Q = max ∫ IL(a + A)-L(a)HH ]
Q = ιmaxd	k∆ik∙∣L(α)∣	.	(11)
We notice the loss landscape w.r.t. α is pretty flat, as shown in Fig. 1(b), implying a low condition
number, thus the high efficiency of ZARTS-GLD.
Specifically, at each iteration, with a predefined search radius boundary [r, R], we indepen-
dently sample candidate updates {ui } for architecture parameters on spheres with various radii
{2-kR}lko=g(0R/r) and perform function evaluation at these points. By comparing L(α) and {L(α +
ui)}, α steps to the point with minimum value, or stay at the current point if none of them makes
an improvement. The architecture parameters are then updated by a J α + u*.
u* = argmin{L(α)∣α = α, c^ = α + u∣}	(12)
i
4.4	Connection between DARTS and ZARTS
The similarity between gradient-estimation-based zero-order optimization and SGD builds an es-
sential connection when the objective function is differentiable. Recall the smoothing parameter μ
defined in Eq. 4, leading to the following definition of the smoothed version of L:
Lμ(α) := Eu〜q0[L(α + μu)],	(13)
where q0 = N(0, I) ifq = N(0, I), and q0 = Bd (a multivariate uniform distribution on a unit ball)
if q = Sd-1. The unbiasedness of Eq. 4 with respect to VaLμ(α) is assured by:
Eu〜q [▽ αL(α)i = VαLμ(α),	(14)
as proved by (Nesterov & Spokoiny, 2017; Berahas et al., 2021). The two-point and multi-point
estimates have a similar unbiasedness condition when Eu〜q [u] = 0, which is satisfied in our case.
The bias between VaL(α) and VɑL(α) is also bounded (Berahas et al., 2021; Liu et al., 2018b):
E [kVαL(α) - VαL(α)∣∣2] = O(d)kV°L(α)k2 + O (μ2[/d) ,	(15)
where d,μ,夕(d) have the same meanings as those in Sec. 4.1. Consequently, if L(α) is indeed
differentiable w.r.t α and the iteration number M is set to 1, ZARTS-RS degenerates to second-
order DARTS with bounded error.
Next, we theoretically show that MGS (Welleck & Cho, 2020) will degenerate to gradient descent al-
gorithm if the first-order Taylor approximation is applied. Then we analyze the relationship between
ZARTS-MGS and DARTS.
Proposition 1. Assuming that L(α) in Eq. 1 is differentiable w.r.t. α, MGS algorithm (Welleck &
Cho, 2020) degenerates to SGD (used in vanilla DARTS) by the first-order Taylor approximation for
L(α), i.e., u* H -VaLIo).
Proof. Denote g , VαL(α) as the gradient of L. The Taylor series of L at α up to the first order
gives L(α +u) - L(α) ≈ u>g. Applying this approximation to the distribution ofu (Eq. 7) yields:
P(u∣α)
e-u>g"
^gr
Z(g) = kuk≤ε
e-u>g"du.
(16)
Here, the magnitude of u is constrained within ε to make sure the rationality of first-order Taylor
approximation. The optimal update u* then becomes:
u*
RIuk≤ε U ∙ e-uτg"du	VgZ(g)
--------=T—:------ - =T--------
Z (g)	TZ (α, g)
-T Vgln Z (g).
(17)
6
Under review as a conference paper at ICLR 2022
Note that u>g = -kukkgkcosη, where η is the angle between u and g. According to the symmetry
of integral, Z(g) is determined once kgk is given. Therefore, we can formulate Z(g) as a function
of kgk: Z(g) = Z(kgk). According to the chain rule:
* -	1 V ln ZM	_	1V ln Z	WgkZ(Hgk)	ʊ	5gkZ(Hgk) 俳、
U = -TVgInZ(g)	= -TVgInZ(IIgk) =	- TZ(kgk)	VgIlgIl	= -TZ(kgk)kgkg.	(18)
Since Z(kgk), Vkgk Z(kgk), kgk are all scalars, we have
U* b—g = -VaL(α) = -VɑLvai(ω*(α), α).	(19)
That is, the optimal update u* in MGS algorithm shares a common direction with the negative
gradient -VaL(α), as used by gradient descent.	□
Based on Proposition 1, ZARTS-MGS can be seen as an expansion of DARTS, and it degrades
to first-order and second-order DARTS when ω * is estimated by ω1*st and ω2*nd, respectively. In
general, ZARTS-RS/-MGS can degenerate to DARTS, given the differentiability assumption.
However, unlike DARTS, which has to estimateω*(α) by ω1*st or ω2*nd to satisfy the differentiablity
property of Lval and update α by gradient descent algorithm, ZARTS, without such assumptions,
can compute ω*(α) by training network weights ω for arbitrary numbers of iterations, leading to
more robust and effective training for architecture parameters, as shown in the next section.
5	Experiments
In this section, we first verify the stability of our ZARTS (with its three variants RS, MGS, GLD) on
the four popular search spaces of R-DARTS (Zela et al., 2020b) on three datasets including CIFAR-
10 (Krizhevsky et al., 2009), CIFAR-100 (Krizhevsky et al., 2009), and SVHN (Netzer et al., 2011).
Note that though ZARTS-GLD performs best in Table 2, and it falls into the category of direct
search methods, requiring more sampling for candidate updates U and thus more search cost (2.2
GPU-days on the search space of DARTS) than ZARTS-MGS (1.0 GPU-days). Considering the
trade-off between accuracy and speed, ZARTS-MGS is chosen by default if not otherwise specified.
We then follow Amended-DARTS (Bi et al., 2019) and empirically evaluate the convergence ability
of our method by searching for 200 epochs. Performance trends of the discovered architectures
along with the search process are drawn in Fig. 2(a). Finally, we search and evaluate on the search
space of DARTS (Liu et al., 2019) to compare with peer methods and show the efficacy of our
method. All the experiments are conducted on NVIDIA 2080Ti. The details of search spaces and
experiment settings are given in Appendix B, due to space limitation.
5.1 Stability Evaluation
The instability issue of gradient-
based NAS methods has re-
cently drawn increasing attention.
Amended-DARTS (Bi et al.,
2019) shows that after search-
ing by DARTS for 200 epochs,
skip-connection gradually domi-
nates the discovered architectures.
R-DARTS (Zela et al., 2020b)
proposes four search spaces, S1-S4,
which amplify the instability of
DARTS, as such dominance occurs
after only 50 epochs of searching.
These studies expose the instability
of gradient-based methods. To
verify the stability of our method,
we search on S1-S4 proposed by
R-DARTS and conduct convergence
analysis following Amended-DARTS.
Table 2: Test error (%) with DARTS and its variants on
different search spaces. We adopt the same settings as R-
DARTS (Zela et al., 2020a). The best and second best is
underlined in boldface and in boldface, respectively.
DARTS
R-DARTS
DP L2
DARTS ZARTS (ours)
ES ADA RS MGS GLD
1234
SSSS
01RAFIC
4540
8832
3.4.3.7.
3590
.8.3.5.9
2.3.2.4.
0594
.1.3.5.8
3.3.2.4.
1641
.0.2.7.7
3323
8116
2323
1838
.1.4.9.5
3.3.2.3.
5460
.6.2.5.7
2.3.2.3.
Ooixvhid
9682
2222
1234
SSSS
6505
25.93	24.25	28.37	24.03	23.64	23.16	23.33
22.30	22.24	23.25	23.52	21.54	20.91	21.13
22.36	23.99	23.73	23.37	22.62	22.33	21.90
22.18	21.94	21.26	23.20	23.33	21.31	21.00
1234
SSSS
NHVS
8315
.5.5.4.0
4.3.3.3.
5291
.5.5.4.6
2222
9180
.7.5.4.5
4222
2001
.7.6.5.5
2222
340
.5.5.5
222
1528
∙5∙4
2.2.2.2.
8843
.4.4.4.5
2222
Performance on S1-S4. We first search on the four spaces on CIFAR-10, CIFAR-100, and SVHN.
Our evaluation settings are the same as R-DARTS (Zela et al., 2020b). Specifically, for CIFAR-10,
7
Under review as a conference paper at ICLR 2022
(a) Trend of Top-1 accuracy
Figure 2: Trends of accuracy and model size in the search process of DARTS and ZARTS for 200
epochs on CIFAR-10. The top-1 accuracy is obtained by training models for 600 epochs.
(b) Trend of Number of Parameters
Table 3: Performance on CIFAR. The top block reports the accuracy of the best model. The bottom
block gives the mean of four independent searches as recommended by (Zela et al., 2020b; Chen &
Hsieh, 2020; Yu et al., 2020). Reported by (Dong & Yang, 2019). ? by (Zela et al., 2020b).
CIFAR-10	Params (M) J	Error (%)J	GPU Cost (days) J	CIFAR-100	Params (M)J	Error (%)J	GPU Cost (days) J
DARTS (1st) ( 019)	3.3	3.00	0.4	AmoebaNet(201 )	3.1	18.93	3150
DARTS (2nd) (2019)	3.4	2.76	1.0	PNAS ( 01 )	3.2	19.53	150
P-DARTS ( 19)	3.4	2.50	0.3	ENAS ( 01 )	4.6	19.43	0.45
GDAS ( 201 )	3.4	2.93	0.2	P-DARTS ( 19)	3.6	17.49	0.3
ISTA-NAS ( 020)	3.3	2.54	0.05	GDAS ( .01 )	3.4	18.38	0.2
PR-DARTS (2020)	3.4	2.32	0.17	ROME(2020a)	4.4	17.33	0.3
DARTS- ( 02 )	3.5	2.50	0.4	PR-DARTS (2020)	3.4	16.45	0.17
ZARTS (ours)	3.5	2.46	1.0	DARTS- ( 02 )	3.4	17.16	0.4
				ZARTS (ours)	4.0	15.46	1.0
DARTS(1st) ( 20)	-	3.38±0.23	0.4				
MergeNAS ( 020b)	2.9	2.68±0.01	0.6	DARTS ( 019)	-	20.58±0.44?	0.4
SGAS (Cri.2) ( 020)	3.9	2.67±0.21	0.3	R-DARTS ( 020b)	-	18.01±0.26?	1.6
R-DARTS ( 020 )	-	2.95±0.21	1.6	ROME(2020a)	4.4	17.41±0.12	0.3
SDARTS-ADV ( 020)	3.3	2.61±0.02	1.3	DARTS- ( 021)	3.3	17.51±0.25	0.4
Amended-DARTS ( 019)	3.3	2.71±0.09	1.7	ZARTS (ours)	4.1±0.13	16.29±0.53	1.0
DARTS- ( 02 )	3.5±0.1	2.59±0.08	0.4		—		
ZARTS (ours)	3.7±0.3	2.54±0.07	1.0				
the discovered models on S1 and S3 are constructed by 20 cells with 36 initial channels; models
on S2 and S4 have 20 cells with 16 initial channels. For CIFAR-100 and SVHN, all the models
on S1-S4 have 8 cells and 16 initial channels. Four parallel tests are conducted on each bench-
mark, among which the best is reported in Table 2. We observe that ZARTS achieves outstanding
performance with great robustness on 12 benchmarks. All three zero-order algorithms outperform
DARTS by a significant margin. Even the vanilla zero-order optimization algorithm ZARTS-RS
achieves similar robust performance as R-DARTS, which verifies our analysis in Fig. 1, i.e., the
coarse estimation ωjSt in DARTS distorts the landscape and causes instability. Additionally, to
compare with SDARTS (Chen & Hsieh, 2020), we follow its experiment settings by increasing the
number of cells and initial channels to 20 and 36 with results reported in Appendix B.4.
Convergence Analysis. The convergence ability of NAS methods describes whether a search
method can stably discover effective architectures along the search process. For an effective NAS
method with great convergence ability, the discovered architectures’ ultimate performance (top-1
accuracy) should converge to a high value. Amended-DARTS (Bi et al., 2019) empirically shows
that DARTS has a poor convergence ability: accuracy of the supernet increases but the ultimate per-
formance of the searched network drops. Following Amended-DARTS, we run ZARTS and DARTS
for 200 epochs and show the trend of performance and number of parameters in Fig. 2. Specifically,
we derive one network every 25 epochs during the search process and train each network for 600
epochs to evaluate its ultimate performance. We observe that the networks searched by ZARTS per-
form stably well (around 97.40% accuracy), while the performance of networks searched by DARTS
gradually drops. Moreover, the parameter number of networks searched by DARTS decreases sig-
nificantly after 50 epochs, indicating that parameterless operations dominate the topology and the
instability issue (Zela et al., 2020a) occurs. On the contrary, ZARTS consistently discovers effective
networks with about 4.0M parameters, showing the great stability of our method.
8
Under review as a conference paper at ICLR 2022
5.2 Comparison with Peer Methods on the Search Space of DARTS
To show the efficacy and effectiveness of our method, we search and evaluate on DARTS’s search
space on both CIFAR-10 and CIFAR-100. Additionally, the models searched on CIFAR-10 are
transferred to ImageNet to evaluate the transferability of our method. The search space and settings
follow DARTS (Liu et al., 2019), as is introduced in Appendix B.2.
Results on CIFAR-10. We conduct four parallel runs by searching with different random seeds
and separately training the searched architectures for 600 epochs. The best and average accuracy of
four parallel tests are reported in Table 3. In particular, ZARTS achieves 97.46% average accuracy
and 97.54% best accuracy on CIFAR-10, outperforming DARTS and its variants. Also, compared
with Amended-DARTS that approximates optimal operation weights ω*(α) by Hessian matrix, our
method can stably discover effective architectures in fewer GPU days.
Results on CIFAR-100. Table 3 shows our method achieves 83.71% and 84.54% for mean and best
accuracy (i.e. 1- error%), on CIFAR-100, outperforming the compared methods by more than 1%.
Results on ImageNet. For transfer- ability test, we follow the settings of DARTS to transfer the network discov- ered on CIFAR-10 to ImageNet. Mod- els are constructed by stacking 14 cells with 48 initial channels. We train 250 epochs with a batch size of 1024 by SGD with a momentum of 0.9 and a base learning rate of 0.5. We utilize the same data pre-processing strategies and auxiliary classifiers as DARTS. Table 4 shows the performance of our searched networks, wit two models evaluated. ZARTS (5.6M) has 5.6M parameters and achieves 75.7% top-1 accuracy on the validation set of Im- ageNet, and ZARTS (5.0M) has 5.0M parameters and achieves 75.5% accu- racy. Their structure details are given in Appendix B.6, which has fewer skip connection operations than DARTS.	Table 4: Performance on ImageNet in DARTS’s search space by two architectures. t direct search on ImageNet.				
	Models	FLOPs Params (M) J (M) J		Top-1 Err. (%) J	GPU Cost (days) J
	AmoebaNet-A (2019) NASNet-A (2018) PNAS (2018a) MdeNAS (2019) DARTS (2nd) (2019) P-DARTS (2019) PC-DARTS (2020a) FairDARTS-B (2020) FairNAS-Ct (2019) SNAS (2019) GDAS (2019) SPOSt (2019) ProxylessNASt (2019) FBNet-Ct (2019) Amended-DARTS (2019)	555 564 588 - 574 557 586 541 321 522 581 323 465 375 586	5.1 5.3 5.1 6.1 4.7 4.9 5.3 4.8 4.4 4.3 5.3 3.5 7.1 5.5 5.2	25.5 26.0 25.8 25.5 26.7 24.4 25.1 24.9 25.3 27.3 26.0 25.6 24.9 25.1 24.7	3150 1800 225 0.16 1.0 0.3 0.1 0.4 12 1.5 0.2 12 8.3 9 1.7
	ZARTS (5.6M params) ZARTS (5.0M params)	647 573	56 5.0	24.3 24.5	1.0 1.0
					
6	Further Discussion and Conclusion
The proposed ZARTS has opened new possible space for future work at least in the following as-
pects: i) We have incorporated the three adopted zero-order solvers in Table 1, which suggests new
solvers may also be readily reused to improve ZARTS, e.g., in a way of AutoML that automatically
determines the suited solver given specific tasks or datasets. In contrast, this feature is not allowed
in DARTS as there is little option for the gradient-descent solver. ii) Since ZARTS can be seen as a
gradient-free counterpart to DARTS, which in fact also requires the same exhaustive GPU memory
as DARTS, memory-efficient techniques, e.g., single-path NAS ROME (Wang et al., 2020a) can also
be adopted to reduce the memory cost as well as the computation cost in the search process.
DARTS has been a dominant paradigm in NAS, while its instability issue has received increasing
attention (Bi et al., 2019; Zela et al., 2020b; Chen & Hsieh, 2020). In this work, we have empiri-
cally shown that the instability issue results from the first-order approximation for optimal network
weights and the optimization gap in DARTS, which is also raised in the recent study (Bi et al., 2019).
To step out of such a bottleneck, this work proposes a robust search framework named ZARTS,
allowing for higher-order approximation for ω*(α) and supporting multiple combinations of Zero-
order optimization algorithms. Specifically, we adopt three representative methods for experiments
and reveal the connection between ZARTS and DARTS. Extensive experiments on various bench-
marks show the effectiveness and robustness of ZARTS. To our best knowledge, this is the first
work that manages to apply zero-order optimization to one-shot NAS, which provides a promising
paradigm to solve the bi-level optimization problem for NAS.
9
Under review as a conference paper at ICLR 2022
References
Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc Le. Understand-
ing and Simplifying One-Shot Architecture Search. In ICML, pp. 549-558, 2018.
Albert S Berahas, Liyuan Cao, Krzysztof Choromanski, and Katya Scheinberg. A theoretical and
empirical comparison of gradient approximations in derivative-free optimization. Foundations of
Computational Mathematics, pp. 1-54, 2021.
Kaifeng Bi, Changping Hu, Lingxi Xie, Xin Chen, Longhui Wei, and Qi Tian. Stabilizing darts
with amended gradient estimation on architectural parameters. arXiv preprint arXiv:1910.11831,
2019.
Han Cai, Ligeng Zhu, and Song Han. ProxylessNAS: Direct Neural Architecture Search on Target
Task and Hardware. In ICLR, 2019.
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. ZOO: zeroth order op-
timization based black-box attacks to deep neural networks without training substitute models.
In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, AISec@CCS
2017, Dallas, TX, USA, November 3, 2017, pp. 15-26, 2017. doi: 10.1145/3128572.3140448.
Xiangning Chen and Cho-Jui Hsieh. Stabilizing differentiable architecture search via perturbation-
based regularization. In ICML, 2020.
Xin Chen, Lingxi Xie, Jun Wu, and Qi Tian. Progressive Differentiable Architecture Search: Bridg-
ing the Depth Gap between Search and Evaluation. In ICCV, 2019.
Xiangxiang Chu, Bo Zhang, Ruijun Xu, and Jixiang Li. FairNAS: Rethinking Evaluation Fairness
of Weight Sharing Neural Architecture Search. arXiv preprint. arXiv:1907.01845, 2019.
Xiangxiang Chu, Tianbao Zhou, Bo Zhang, and Jixiang Li. Fair darts: Eliminating unfair advantages
in differentiable architecture search. ECCV, 2020.
Xiangxiang Chu, Xiaoxing Wang, Bo Zhang, Shun Lu, Xiaolin Wei, and Junchi Yan. DARTS-
: robustly stepping out of performance collapse without indicators. In ICLR. OpenReview.net,
2021.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A Large-Scale
Hierarchical Image Database. In CVPR, pp. 248-255. IEEE, 2009.
Xuanyi Dong and Yi Yang. Searching for a Robust Neural Architecture in Four GPU Hours. In
CVPR, pp. 1761-1770, 2019.
Abraham D Flaxman, Adam Tauman Kalai, and H Brendan McMahan. Online convex optimization
in the bandit setting: gradient descent without a gradient. arXiv preprint cs/0408007, 2004.
Daniel Golovin, John Karro, Greg Kochanski, Chansoo Lee, Xingyou Song, and Qiuyi Zhang. Gra-
dientless descent: High-dimensional zeroth-order optimization. In ICLR. OpenReview.net, 2020.
Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and Jian Sun.
Single Path One-Shot Neural Architecture Search with Uniform Sampling. arXiv preprint.
arXiv:1904.00420, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image
Recognition. In CVPR, pp. 770-778, 2016.
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient Convolutional Neural Networks for
Mobile Vision Applications. arXiv preprint. arXiv:1704.04861, 2017.
Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with
limited queries and information. In Proceedings of the 35th International Conference on Machine
Learning, ICML2018, Stockholmsmassan, Stockholm, Sweden, July 10-15,2018, pp. 2142-2151,
2018.
10
Under review as a conference paper at ICLR 2022
Alex Krizhevsky, Geoffrey Hinton, et al. Learning Multiple Layers of Features from Tiny Images.
Technical report, Citeseer, 2009.
GUohao Li, GUocheng Qian, Itzel C Delgadillo, Matthias Muller, Ali ThabeL and Bernard Ghanem.
Sgas: Sequential greedy architecture search. In CVPR, 2020.
Chenxi LiU, Barret Zoph, Maxim NeUmann, Jonathon Shlens, Wei HUa, Li-Jia Li, Li Fei-Fei, Alan
YUille, Jonathan HUang, and Kevin MUrphy. Progressive NeUral ArchitectUre Search. In ECCV,
pp.19-34, 2018a.
Hanxiao LiU, Karen Simonyan, and Yiming Yang. DARTS: Differentiable ArchitectUre Search. In
ICLR, 2019.
Sijia LiU, Bhavya KailkhUra, Pin-YU Chen, PaishUn Ting, ShiyU Chang, and Lisa Amini.
Zeroth-order stochastic variance redUction for nonconvex optimization. arXiv preprint
arXiv:1805.10367, 2018b.
Sijia LiU, Pin-YU Chen, Bhavya KailkhUra, GaoyUan Zhang, Alfred O Hero III, and Pramod K
Varshney. A primer on zeroth-order optimization in signal processing and machine learning:
Principals, recent advances, and applications. IEEE Signal Processing Magazine, 37(5):43-54,
2020a.
Sijia LiU, Parikshit Ram, Deepak Vijaykeerthy, Djallel BoUneffoUf, Gregory Bramble, Horst SamU-
lowitz, DakUo Wang, Andrew Conn, and Alexander Gray. An ADMM based framework for
aUtoml pipeline configUration. In Proceedings of the AAAI Conference on Artificial Intelligence,
pp. 4892-4899, 2020b.
YUrii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex fUnctions.
Foundations of Computational Mathematics, 17(2):527-566, 2017.
YUval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo WU, and Andrew Y. Ng. Reading
digits in natUral images with UnsUpervised featUre learning. In NIPSW, 2011.
Art B. Owen. Monte Carlo theory, methods and examples. 2013.
HieU Pham, Melody Y GUan, Barret Zoph, QUoc V Le, and Jeff Dean. Efficient NeUral ArchitectUre
Search via Parameter Sharing. In ICML, 2018.
Esteban Real, Alok Aggarwal, Yanping HUang, and QUoc V Le. RegUlarized evolUtion for image
classifier architectUre search. In AAAI, volUme 33, pp. 4780-4789, 2019.
Karen Simonyan and Andrew Zisserman. Very deep convolUtional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
XingyoU Song, Wenbo Gao, YUxiang Yang, Krzysztof Choromanski, Aldo Pacchiano, and YUn-
hao Tang. ES-MAML: simple hessian-free meta learning. In 8th International Conference on
Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.
YUn-YUn Tsai, Pin-YU Chen, and TsUng-Yi Ho. Transfer learning withoUt knowing: Repro-
gramming black-box machine learning models with scarce data and limited resoUrces. CoRR,
abs/2007.08714, 2020.
Xiaoxing Wang, Xiangxiang ChU, YUda Fan, Zhexi Zhang, Xiaolin Wei, JUnchi Yan, and Xiaokang
Yang. ROME: robUstifying memory-efficient NAS via topology disentanglement and gradients
accUmUlation. CoRR, abs/2011.11233, 2020a.
Xiaoxing Wang, Chao XUe, JUnchi Yan, Xiaokang Yang, Yonggang HU, and Kewei SUn. Mergenas:
Merge operations into one for differentiable architectUre search. In International Joint Conference
on Artificial Intelligence, 2020b.
Sean Welleck and KyUnghyUn Cho. Mle-gUided parameter search for task loss minimization in
neUral seqUence modeling. arXiv preprint arXiv:2006.03158, 2020.
11
Under review as a conference paper at ICLR 2022
Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian,
Peter Vajda, Yangqing Jia, and Kurt Keutzer. FBNet: Hardware-Aware Efficient ConvNet Design
via Differentiable Neural Architecture Search. In CVPR, 2019.
Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. SNAS: Stochastic Neural Architecture
Search. In ICLR, 2019.
Yuhui Xu, Lingxi Xie, Xiaopeng Zhang, Xin Chen, Guo-Jun Qi, Qi Tian, and Hongkai Xiong.
Pc-darts: Partial channel connections for memory-efficient architecture search. In ICLR, 2020a.
Yuhui Xu, Lingxi Xie, Xiaopeng Zhang, Xin Chen, Guo-Jun Qi, Qi Tian, and Hongkai Xiong.
PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search. In ICLR,
2020b.
Yibo Yang, Hongyang Li, Shan You, Fei Wang, Chen Qian, and Zhouchen Lin. ISTA-NAS: efficient
and consistent neural architecture search by sparse coding. In NeurIPS, 2020.
Kaicheng Yu, Christian Sciuto, Martin Jaggi, Claudiu Musat, and Mathieu Salzmann. Evaluating
the search phase of neural architecture search. In ICLR, 2020.
Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, and Frank Hutter.
Understanding and Robustifying Differentiable Architecture Search. In ICLR, 2020a.
Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, and Frank Hutter.
Understanding and robustifying differentiable architecture search. In ICLR, 2020b.
Xiawu Zheng, Rongrong Ji, Lang Tang, Baochang Zhang, Jianzhuang Liu, and Qi Tian. Multinomial
Distribution Learning for Effective Neural Architecture Search. In ICCV, pp.1304-1313, 2019.
Pan Zhou, Caiming Xiong, Richard Socher, and Steven Chu-Hong Hoi. Theory-inspired path-
regularized differential network architecture search. In NeurIPS, 2020.
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning Transferable Architec-
tures for Scalable Image Recognition. In CVPR, volume 2, 2018.
12
Under review as a conference paper at ICLR 2022
A	Appendix
A.1 Estimation for Second-order Partial Derivative in DARTS
Liu et al. (2019) introduce second-order approximation to estimate optimal network weights,
i.e., ω* ≈ ω0 = ω - ξVωLtrain(ω,α), So that PaLval3 (α), α) ≈ PaLval 3, α)-
ξVa,ωLtrain(ω, α)VωoLval 3, α). However, the second-order partial derivative is hard to Com-
pute, so the authors estimate it as follows:
vα,ω Ltrain(ω,即 3, Lval3, α) ≈ VaLtrain(ω+ , α)| VaLtrain (ω-, a) ,	q°)
where ω± = ω ± eVω,Lval(ω0, α), and e = 帖 工0.0(30 二川2 ∙ Here, we prove that the above
approximation in Eq. 20 is difference method. ω
Proof. First of all, to simplify the writing, we make the following definitions:
f(ω, α) = Va Ltrain (ω, α),	g(ω, α) = Lval (ω, α).	(21)
Then the left term in Eq. 20 can be simplified as:
Vα,ωLtrain(ω, α]V3,Lval 3； α) = Vωf (ω, α) ∙ Vω0g(ω0, α)	(22)
=Vω f(3, α) ∙ Ha： α) ∙ kVω, g(ω0, α)∣∣2 = V f(ω, α) ∙ l ∙∣∣Vωο g(ω0, α)∣∣2, (23)
kV3, g(ω0, α)k2
where l = /嚼30 O), is a Unit vector. We notice Vωf (ω, α) ∙ l is the directional derivative of
f(ω, α) along direction l, which can be estimated by difference method with a small perturbation
e0 = 0.01:
Vω f(3, α) ∙ l ∙kVω, g(ω0, α)∣∣2 ≈ f(ω + 工 α)2-0fω-L a)/Va，g(ω0, α)∣∣2	(24)
Moreover, we define e = 帖 ,∖ /川2. Then ω ± e0l = ω ± eVω,g(ω0, a)，ω±, so Eq. 24 can
be simplified as:	ω
f(ω + e0l, a)- f(ω - e0l, a)	0	f(ω+, a)- f(ω-,a)
------------2p-------------I∣v30g(ω ,a)k2 =----------2e---------.	(25)
SUbstitUting f(ω, a) in Eq. 21 with Eq. 25 resUlts in Eq. 20. Therefore second-order approximation
in DARTS utilizes difference method, which is also a zero-order optimization algorithm. □
A.2 Loss Landscape w.r.t. Architecture Parameters
To draw loss landscapes w.r.t. a, we train a supernet for 50 epochs and randomly select two or-
thonormal vectors as the directions to perturb a. The same group of perturbation directions is used
to draw landscapes for a fair comparison. Landscapes are plotted by evaluating L at grid points
ranged from -1 to 1 at an interval of 0.02 in both directions. Fig. 3 illustrates landscapes (contours)
(a) Landscape w.r.t. a with 3；St	(b) Landscape w.r.t. a with ω^nd	(C) Landscape w.r.t. a with 3*
Figure 3: Loss landscapes w.r.t. architecture parameters a. In (a), we illustrate the landscape with
first-order approximation. In (b), we illustrate the landscape with second-order approximation. In
(c), we obtain ω* by training network weights ω for 10 iterations, and illustrate the landscape w.r.t.
a with ω*. To fairly compare the landscapes, we utilize the same model and candidate a points.
We observe the first/second-order approximations both sharpen the landscape.
13
Under review as a conference paper at ICLR 2022
w.r.t. α under different order of approximation for optimal network weights, showing that both first-
and second-order approximation sharpen the landscape and in turn lead to incorrect global mini-
mum. In this work, We obtain ω*(α) by fixing α and fine-tuning network weights for M iterations
(Fig. 3 (c)). Selection of M is also analyzed in Appendix B.3, showing that M = 10 iterations is
accurate enough to estimate optimal network weights.
A.3 Details of ZARTS-MGS Algorithm
Selection of the Proposal Distribution q(u∣α). Since the probability function of distribution P
is intractable, we sample from a proposal distribution q and approximate the optimal update of
architecture parameters by Eq. 10. The proposal distribution q affects the efficiency of sampling.
Specifically, an ideal q should be as close to p as possible when the sampling number is limited.
Following Welleck & Cho (2020), we set the proposal distribution q to a mixture of two Gaussian
distributions, one of which is centered at the negative gradient of the loss function with current
weights:
q(u∣α) = (1 - λ)N(-VαLvai(ω, α),σ2) + λN(0,σ2),
(26)
where σ is the standard deviation. Intuitively, first-order DARTS (Liu et al., 2019) gives a hint:
it updates the architecture parameters α in the direction of -VαLval (ω, α). The gradient is an
imperfect but workable direction with easy access.
Importance Sampling Diagnostics. To demonstrate that importance sampling and our choice of
the proposal distribution is indeed appropriate in our case, we evaluate the effectiveness of q (the
proposal distribution) quantitatively by the following experiments. According to Owen (2013), ef-
fective sample size (ESS) Ne is a popular indicator defined as Ne = p⅛2. For N samples, a
larger Ne ∈ [1,N] usually indicates a more effective sampling. On the contrary, small Ne implies
imbalanced sample weights and therefore is unreliable (Owen, 2013).
To evaluate the effectiveness of sampling in ZARTS-MGS, we set the sampling number N to various
values and plot Ne versus epochs in each case. As is shown in Fig. 4(a), Ne gradually approaches
Nin all settings, indicating that different sampling numbers in our setting are meaningful, including
larger ones (otherwise Ne may “saturate”).
As a further exploration, we define effective sample ratio (ESR) Re as the ratio of effective sampling
to all samples:
Re=N
(27)
The value of Re denotes the bias between the target distribution p and proposal distribution q, and a
smaller value indicates a greater difference. Re is plotted against epochs for various N in Fig. 4(b).
On the one hand, Re at epoch 50 stabilizes at 0.7 as N increases, which is an acceptable level of
bias between p and q and supports our choice of q; on the other hand, we notice Re has already
converged when N ≥ 4. Considering the trade-off between estimation accuracy and speed, we set
N = 4 as default, which is further discussed in Appendix B.3.
S ΦN-∞ 6u=dEes 8Nt58t山
LLlnnr
,10	15	20	25	30	35	40	45	50
Epoch
(b) ESR Re versus epochs over sampling number N .
Figure 4: ESS Ne and ESR Re versus epochs with different sampling numbers N in the search stage
on CIFAR-10. We fix iteration number M = 10 for all settings.
1.0
£ 0.8-
ra
M 0.6-
⅛
ŋ 0.4-
S
ω
⅛ 0.2-
14
Under review as a conference paper at ICLR 2022
B S upplementary Experiments
B.1	Details of Search Spaces
DARTS’s Standard Search Space. The operation set O contains 7 basic operations: skip con-
nection, max pooling, average pooling, 3 × 3 separable convolution, 5 × 5 separable convolution,
3 × 3 dilated separable convolution, and 5 × 5 dilated separable convolution. Though zero operation
is included in the origin search space of DARTS (Liu et al., 2019), it will never be selected in the
searched architecture. Therefore, we remove zero operation from the search space. We search and
evaluate on the CIFAR-10 (Krizhevsky et al., 2009) dataset in this search space, and then transfer
the searched model to ImageNet (Deng et al., 2009). Additionally, in our convergence analysis, we
search by DARTS and ZARTS in this search space for 200 epochs.
RDARTS’s Search Spaces S1-S4. To evaluate the stability of search algorithm, RDARTS (Zela
et al., 2020a) designs four search spaces where DARTS suffers from instability severely, i.e. normal
cells are dominated by parameter-less operations (such as identity and max pooling) after searching
for 50 epochs. In S1, each edge in the supernet only has two candidate operations, but the candidate
operation set for each edge differs; in S2, the operation set O only contains 3 × 3 separable convolu-
tion and identity for all edges; in S3, O contains 3 × 3 separable convolution, identity, and zero for
all edges; in S4, O contains 3 × 3 separable convolution and noise operation for all edges. Please
refer to Zela et al. (2020a) for more details of the search spaces.
B.2	Experiment Settings
Search Settings. Similar to DARTS, we construct a supernet by stacking 8 cells with 16 initial
channels. We apply Alg. 1 to train architecture parameters α for 50 epochs. Two hyper-parameters
of ZARTS, sampling number N and iteration number M, are set to 4 and 10 respectively. Ablation
studies of the two hyper-parameters are analyzed in Appendix B.3. The setup for training ω follows
DARTS: SGD optimizer with a momentum of 0.9 and a base learning rate of 0.025. Our experiments
are conducted on NVIDIA 2080Ti. ZARTS-MGS algorithm is used in supplementary experiments
by default.
Evaluation Settings. We follow DARTS (Liu et al., 2019) and construct models by stacking 20
cells with 36 initial channels. Models are trained for 600 epochs by SGD with a batch size of 96.
Cutout and auxiliary classifiers are used as introduced by DARTS.
B.3	Ablation Studies
There are two hyper-parameters in our method: sampling number N and iteration number M . As
introduced in Section 4, N samples of update step of architecture parameters ui are drawn to esti-
mate the optimal update u*. For each sampled Ui, We approximate the optimal weights ω*(α + Ui)
for each sample by training ω for M iterations. To evaluate the sensitivity of our method to the two
hyper-parameters above, we conduct ablation studies on the standard search space of DARTS on
CIFAR-10 dataset.
Sensitivity to the Sampling Number
N . For various sampling numbers N,
the average performance of three paral-
lel searches with different random seeds
is reported in Table 5. In this experi-
ment, iteration number M is fixed to 10.
When N = 2, ZARTS achieves 97.37%
accuracy with 2.87M parameters. The
number of parameters of searched net-
work increases as N increases, and the
performance of searched network gets
stable when N ≥ 4. Our method per-
forms better than DARTS when N = 4,
Table 5: Comparison of different sampling numbers to
approximate the optimal update for architecture parame-
ters on the standard search space of DARTS on CIFAR-
10 dataset. For each N, three parallel tests are conducted
by searching on different random seeds and the mean and
standard deviation of top-1 accuracy are reported.
Sampling number	N=2	N=4	N=6	N=8
Error (%)	2.63	2.54	2.51	2.57
STD	±0.12	±0.07	±0.09	±0.11
Params (M)	2.87	3.71	3.53	4.31
Cost (GPU days)	0.5	1.0	1.1	1.5
with similar search cost (1.0 GPU days). When N = 6, ZARTS achieves its best accuracy (97.49%)
and costs 1.1 GPU days. When N continues to increase, our method attempts to find more complex
architectures (with 4.31M and 4.26M parameters).
15
Under review as a conference paper at ICLR 2022
Table 7: Comparison with peer methods under the settings of SDARTS. (left) Test error of other
methods are obtained from SDARTS (Chen & Hsieh, 2020), indicating the best performance among
four replicate experiments with different random seeds. Note that ‘RS’ in SDARTS indicates random
smoothing technique, while ‘RS’ in ZARTS indicates random search, a zero-order optimization
algorithm. The best and second best is underlined in boldface and in boldface, respectively. (right)
We report the average error and standard deviation of our method among four replicate experiments.
Since PC-DARTS and SDARTS do not provide the average performance, we only compare the three
implementations of our ZARTS.
	PC-DARTS	SDARTS		ZARTS					ZARTS (avg.±std)		
		RS	ADV	RS	MGS GLD				RS	MGS	GLD
S1 S2 S3 S4	3.11	2.78	2.73	2.83	2.65	2.50	o 2 I U	S1	3.10±0.20	2.79±0.14	2.73±0.07
	3.02	2.75	2.65	2.41	2.39	2.60		S2	2.60±0.14	2.65±0.17	2.67±0.08
	2.51	2.53	2.49	2.59	2.56	2.56		S3	2.89±0.30	2.74±0.10	2.70±0.09
	3.02	2.93	2.87	3.35	2.74	2.63		S4	4.11±1.07	2.99±0.21	3.35±0.93
S1 S2 S3 S4	18.87	17.02 16.88 17.38 17.62 17.40					O T-I X I U	S1	18.07±0.55	18.20±0.48	17.83±0.44
	18.23	17.56 17.24 16.05 16.41 16.69						S2	17.04±0.70	17.35±0.75	17.14±0.39
	18.05	17.73 17.12 17.22 17.03 16.58						S3	18.14±0.72	17.72±0.46	16.99±0.38
	17.16	17.17 15.46 18.23 16.57 15.97						S4	19.08±1.01	17.33±0.73	16.63±0.75
S1	2.28	2.26	2.16	2.09	2.13	2.14	Z H	S1	2.21±0.10	2.17±0.03	2.20±0.04
S2 S3	2.39	2.37	2.07	2.06	2.06	2.15		S2	2.16±0.10	2.10±0.03	2.22±0.07
	2.27	2.21	2.05	2.17	2.20	2.07		S3	2.27±0.14	2.25±0.05	2.13±0.07
S4	2.37	2.35	1.98	2.49	2.04	2.15		S4	2.56±0.09	2.20±0.11	2.26±0.09
Sensitivity to the Iteration Number
M. DARTS and its variants (Xu et al.,
2020b; Zela et al., 2020b) assume that
the optimal operation weights ω*(α) is
differentiable w.r.t. architecture param-
eters α, which has not been theoreti-
cally proved. In this work, we relax the
above assumption and adopt zero-order
optimization to update α. As introduced
in Section 4, we perform multiple iter-
ations of gradient descent on operation
Table 6: Comparison of different iteration numbers to
approximate the optimal operation weights in DARTS’s
standard search space on CIFAR-10. We conduct three
parallel tests for each M and report the mean and stan-
dard deviation of top-1 accuracy.
Iteration number	M=2	M=5	M=8	M=10
Error (%)	2.62	2.60	2.57	2.54
STD	±0.15	±0.09	±0.03	±0.07
Params (M)	2.91	3.40	3.52	3.71
weights to accurately estimate ω*(α). To further confirm our analysis on the impact of iteration
number M , we search with various values of M and report the average performance of three paral-
lel searches with different random seeds in Table 6. In this experiment, we set the sampling number
N to 4. The results reveal that the performance of searched model improves as M increases and the
highest accuracy is achieved at M = 10, which supports our analysis that inaccurate estimation for
optimal operation weights ω* (α) can mislead the search procedure.
B.4	Comparision with Peer Methods on S1-S4
Unlike R-DARTS (Zela et al., 2020b) that constructs models by stacking 8 cells and 16 inital chan-
nels, SDARTS (Chen & Hsieh, 2020) builds models by stacking 20 cells and 36 initial channels. To
compare with SDARTS for fair, we follow its settings and report our results in Table 7. Specifically,
we conduct four parallel tests on each benchmark by searching with different random seeds. Tabel 7
reports the best and average performance of our method. Note that other methods in Tabel 7 only
report the best performance of four parallel tests. According to the results, we observe our ZARTS
achieves state-of-the-art on 7 benchmarks and SDARTS-ADV slightly outperforms our ZARTS on
5 benchmarks.
16
Under review as a conference paper at ICLR 2022
8 6 4 2 0
do ss~Is3UΓeτetJ。.XnnN
(a) Trend of Number of Parameterless Operations
dο>s33sJ。.日nz
DARTS
PC-DARTS
S-DARTS (ADV)
S-DARTS (RS)
ZARTS-RS
ZARTS-MGS
ZARTS-GLD
，2S
(b) Trend of Number of Identity Operations
Figure 5: Trends of number of parameterless operations and identity operations in each normal cell
searched by different NAS methods on CIFAR-10 for 200 epochs. The parameterless operations
include max pooling, average pooling, and identity operation.
手
B.5	Convergence Analysis
The convergence ability of NAS methods describes whether a search method can stably discover
effective architectures along the search process. A robust and effective NAS method should be
able to converge to exemplary architectures with high performance. This work follows Amended-
DARTS (Bi et al., 2019) and evaluates the convergence ability by searching for an extended period
(200 epochs). However, since it is time-consuming to train every derived architecture along the
search process, we illustrate the trend of number of parameterless operations (pooling and iden-
tity operations) in each normal cell to represent the performance of architectures (Fig. 5). Recent
works (Chen et al., 2019; Bi et al., 2019; Zela et al., 2020b) show that architecture with more than 4
parameterless operations (especially identity operations) usually has a bad performance, which is a
typical phenomenon of the instability issue. Here, we show the number of parameterless operations
of our ZARTS in Fig. 5 and compare with another three methods, including DARTS, PC-DARTS
and S-DARTS. We observe that the architectures discovered by DARTS, PC-DARTS and S-DARTS
will be gradually dominated by parameterless operations (especially identity operation), implying
that the instability issue occurs. In contrast, our ZARTS can stably control the number of parame-
terless operations.
B.6	Visualization of Architectures
Note that in all our experiments, we directly utilize the architecture at the final epoch (epoch 50) as
the inferred network. No model selection procedure is needed.
We visualize the architectures of normal and reduction cells searched by ZARTS in DARTS’s search
space on CIFAR-10, as is shown in Fig. 6. The architecture searched on CIFAR-100 dataset is illus-
trated in Fig. 7. We also conduct experiments in the four difficult search spaces of RDARTS (Zela
et al., 2020a) on CIFAR-10 (Krizhevsky et al., 2009), CIFAR-100 (Krizhevsky et al., 2009), and
SVHN (Netzer et al., 2011). The searched architectures are illustrated in Fig. 8, Fig. 9, and Fig. 10.
SeP-Conv_3x3
(a) The normal cell searched by ZARTS
SeP-Conv_3X3
dil conv 5
skp_ccnnect________________
SeP-Conv_3x3
sep_conv_3x3
SkiP-ConneCt
SeP-Conv_3x3
-------1 skip_connect
J{k-1} -,
_________L_max_po ol_3x3
skip_connect
(b) The reduction cell searched by ZARTS
Figure 6: The architectures of normal and reduction cell searched by ZARTS on CIFAR-10 in
DARTS’s search space. Model constructed by the above cells achieves 97.54% accuracy on the
CIFAR-10 dataset with 3.5M parameters.
avg_pool_3x3
_conv_
_conv_
max_pool_3x3
avg_pool_3x3
17
Under review as a conference paper at ICLR 2022
avg_pool_3x3
"{k-1} maX_pooi_3X3
skip_connect
Skip_Conn
"{k-2}
skip_connect
avg_pool_3x3
skip_connect
avg_pool_3x3
(a)	The normal cell searched by ZARTS
(b)	The reduction cell searched by ZARTS
Figure 7:	The architectures of normal and reduction cell searched by ZARTS on CIFAR-100 in
DARTS’s search space. Model constructed by the above cells achieves 84.54% accuracy on the
CIFAR-100 dataset with 4.0M parameters.
skip ConneCt
skip_connect
dil Conv 3x3
dιl Conv 3x3
Sep-Conv_3x3
SkiP-Connect
Sep-Conv_3x3
J(k-1}
J(k-2]
dil Conv 5x5
[{k-1}
I________I avg_pool_3x3
IC_{k-2} max_pool_3x3
m
c_{k}
dil Conv 5x5
max_pool_3x3
avg_pool_3x3
sep_conv_3x3
skip_connect
-------1 sep conv 3x3
c_{k-2} ^~E=---二
____Jsep_conv_3x3
SeP-Conv_3x3
sep_conv_3x3
J(k-2]
SkiP-Ccnnect
SeP-Conv_3x3
SeP-Conv_3x3
SkiP-CameCt
skip_ccnnect
SeP-CaIv_3x3
SeP-Conv_3x3
Skip-ConneCt
SeP-Conv_3x3
SeP=Conv=3x3
c-{k-1]
(c) S3
Q{k-1}
sep_conv_3x3
I c_{k-2}
sep_conv_3x3
sep_conv_3x3
SeP Conv 3x3
sep_conv_3x3
sep_conv_3x3
sep_conv_3x3
sep_conv_3x3
c_{k-1} sep_conv_3x3
sep_conv_3x3_______________
sep_conv_3x3
sep_conv_3x3
sep_conv_3x3
c_{k-2}
noise
_sep^_--r'''--sep_conv_3x3
sep_conv_3x3 ^	^^^
c_{k}
0
2
1
3
(d) S4
Figure 8:	The architectures of normal and reduction cells searched by ZARTS on CIFAR-10 in the
four difficult search space of RDARTS. The left column shows the normal cells, while the right
column shows the reduction cells.
18
Under review as a conference paper at ICLR 2022
k_{k-1}	dU_conv_3x3
I c_{k-2}
skip_connect
dɪl conv 3x3
skip connect
dil COnv 5x5
sep_conv_3x3
sep_conv_3x3
__________skip_connect
(a)
J{k-1}
sep_conv_3x3
c_{k-2}
sep_conv_3x3
sep_conv_3x3
sep_conv_3x3
sep_conv_3x3
c_{k}
skip_connect
sep_conv_3x3
sep_conv_3x3
I c_{k-2} ∣-
[{k-1}
S1
max_pool_3x3
dil_conv_3x3
c_{k}
max_pool_3x3
skip_connect
avg_pool_3x3
avg_pool_3x3
dil_conv_5x5
gk-1}1
gk-2}
sep_conv_3x3
sep_conv_3x3
Se
c_{k}
sep_conv_3x3
skip_connect
skip_connect
sep_conv_3x3
skip_connect
(b) S2
gk-2}卜
Q{k-1}
sep_conv_3x3
skip_connect
Ski
c_{k}
sep_conv_3x3
sep_conv_3x3
skip_connect
sep_conv_3x3
sep_conv_3x3
skip_connect
skip_connect
sep_conv_3x3
skip_connect
c_{k-1}
skip_connect
sep_conv_3x3
(c) S3
I c_{k-2}
I-------1 sep_conv_3x3
I - - IWeP_conv_3x3
c_{k}
3x3
Sep Conv 3x3
sep_conv_3x3
sep_conv_3x3
sep_conv_3x3
sep_conv_3x3
sep_conv_3x3
(d) S4
Figure 9:	The architectures of normal and reduction cells searched by ZARTS on CIFAR-100 in
the four difficult search space of RDARTS. The left column shows the normal cells, while the right
column shows the reduction cells.
19
Under review as a conference paper at ICLR 2022
k_{k-1}「seP-Conv_3x3
max_pool_3x3
I c_{k-2}	avg_pool_3x3
c_{k}
max_pool_3x3
dil Conv 5x5
max_pool_3x3
skip_connect
skip_connect
sep_8nv_3x3^| 2
c_{k-2}
sep_conv_3x3
0
sep_conv_3x3
J{k-1}
"'"''\_^skip_connect	.	_
Sep conv 3x3	^^-----Ji~--------- CC
—-------=-----------------=StI 1 Lsep_Conv_3x3 I___
sep_conv_3x3 '	r^^^^^-ʌ~	. J 3
J{k}
skip_connect
skip_connect
sep_conv_3x3
sep_conv_3x3
skip_connect
skip_connect
c_{k-1}
skip_connect
skip_connect
sep_conv_3x3
noise
Q{k-1}
I c_{k-2} sep_conv_3x3
c_{k}
sep_conv_3x3
sep_conv_3x3
sep_conv_3x3
I________I sep_conv_3x3
Ic_{k-2} sep_conv_3x3
3x3
sep_conv_3x3
∣j{k-1}
noise
sep_conv_3x3
c_{k}
sep_conv_3x3
noise
sep_conv_3x3
(d) S4
Figure 10:	The architectures of normal and reduction cells searched by ZARTS on SVHN in the four
difficult search space of RDARTS. The left column shows the normal cells, while the right column
shows the reduction cells.
20
Under review as a conference paper at ICLR 2022
口k-1}
sep_conv_3x3
max_pool_3x3
J{k2}
dil Conv 3x3
sep_conv_3x3
sep_conv_5x5
sep_conv_3x3
sep_conv_5x5
sep_conv_3x3
I c_{k-2}
sep_c onv_3x3
Sep conv 5x5
Sep ColV 3x3
dil Conv 5x5
sep_conv_3X3
sep conv 5x5
max_pool 3x3
max pool 3x3
(a)	The normal cell derived at 10 epoch
(b)	The normal cell derived at 15 epoch
gk-2}
I C_{k-2} Lskip_con*
Sep-Conv 3x3
Skip-ConneCt
---------1 _______Sep-Conv_3x3_________________________
C-{k-1} [ .sep_conv_3x3	Skip-Connect
----------≈-τr-oΓ3^^l
sep-Conv 3x3 J I
sep-Co nv_5χ5J^2~∣~-^*
sep_conv_5x5
sep_conv_3x3
max_pool_3x3
sep_conv_5x5
sep_conv_5x5
匚而Ypqnv_3x3
sep_conv_3x3
(c)	The normal cell derived at 20 epoch
(d)	The normal cell derived at 25 epoch
max_po ol_3x3
C {k-2}T SkiP-Connect
sep_conv_3x3
sep_conv_5x5
sep_conv_3x3
max_pool_3x3
c_{k-11
sep_conv_3x3
sep_conv_3x3
Sep-Conv_5x5
Sep-Conv_5x5
Skip-Connect
Skip-ConneCt
dil_conv_3x3
sep_conv_5x5
2
Sep_ConV_
sep_Conv_3x3
C_{k-H
(e)	The normal cell derived at 50 epoch
(f)	The normal cell derived at 75 epoch
(g)	The normal cell derived at 100 epoch
(h)	The normal cell derived at 125 epoch
sep_conv_5x5
sep conv 5x5
sep_conv_3x3
sep conv 3x3
c {k-1l
sep_conv_3x3
skip_connect
Sep-Conv_3x3
c_|k-2} sep conv 3x3
sep_conv_5x5
sep_conv_5x5
_conv_3x3
sep conv 3x3
sep conv 3x3
sep_conv_3x3
sep_conv_3x3
sep conv 3x3
(i)	The normal cell derived at 150 epoch
(j)	The normal cell derived at 175 epoch
sep_conv_5x5
sep conv 3x3
sep_c onv_3x3
sep conv 3x3
sep conv 3x3
sep conv 3x3
sep_c onv_3x3
sep conv 3x3
(k) The normal cell derived at 200 epoch
Figure 11:	The derived architectures of normal cell every 25 epochs, which are searched by ZARTS
on CIFAR-10 for 200 epochs.
21
Under review as a conference paper at ICLR 2022
Skip-Connect
Sep-Conv_3x3
Sep_ConV_5x5
Sep-Conv_5x5
max_pool_3x3
avg_pool_3x3
skip_connect
sep_conv_3x3
sep_conv_5x5
|c{k-1} p^sep_conv_5x5
∖. skip_connect
max_pool_3x3
(a)	The reduction cell derived at 10 epoch
(b)	The reduction cell derived at 15 epoch
max_pool_3x3
"{k-2}
_pool_3x3
s ep_c onv_3x3_jj 1
avg_pool_3x3
sep_conv_3x3
dil_conv_3x3
sep_conv_5x5
I ., ,. fk skip connect
∣j{k-1}
sep_conv_3x3
sep_conv_3x3
I---k-2}_J	max_pool_3x3
'..-----l-⅛max_poo l_3x3
avg_pool_3x3
max_pool_
c_{k-1l
sep_conv_5x5
sep_conv_5x5
(c)	The reduction cell derived at 20 epoch
(d)	The reduction cell derived at 25 epoch
--------1 dil_conv_5x5
c_{k-2} ~~—二
_______L_max_poo l_3x3
dil_conv_5x5
dil_conv_5x5
sep_conv_5x5
C {k-1}
max_pool_3x3
max pool 3x3
c_{ k-2 }
max_pool_3x3
avg_pool_3x3
max_pool_3x3
sep_conv_5x5
skip_connect
max_pool_3x3
max_pool_3x3
c_{ k-1!
(e)	The reduction cell derived at 50 epoch
(f)	The reduction cell derived at 75 epoch
口以｝
max_pool_3x3
∣j{k-2} J
X^max_pool_3x3
pool_3x3
skip connect
sep_conv_5x5
max_pool_3x3
sep_conv_5x5
[Uk2i maX-PooL3X3
I___2___I max_pool_3x3
c_{k-1l
max_pool_3x3
SkiP-Connec
maX-pool 3x3
SkiP-Connec
dil_conv_5x5	SeP-ConV_5x5
(g)	The reduction cell derived at 100 epoch
(h)	The reduction cell derived at 125 epoch
^Jk2}	maχ-p∞l-3χ3
I_______I avg_pool_3x3
sep_conv_3x3
skip_connect
c_{ k-1}
sep_conv_5x5
skip_connect
max_pool_3x3
max_pool_3
PJk2} avg-pool-3χ3
ɪ—------a avg-Pool_3x3
SkiP-Connea
SkiP-Connect
C {k-H
sep_conv_5x5
skip-connect
max_pool_3x3
max pool 3x
(i)	The reduction cell derived at 150 epoch
(j)	The reduction cell derived at 175 epoch
PJk21 avgJooL3x3
ɪ—------a avg_pool_3x3
Skip-Connect
0
c-{k}
J{k-1}
sep_conv_5x5
3
、\Skip_connect
max_po ol_3x3	J-J—— —
max_pool_3X3
」skip-connect
3
(k) The reduction cell derived at 200 epoch
Figure 12:	The derived architectures of reduction cell every 25 epochs, which are searched by
ZARTS on CIFAR-10 for 200 epochs.
22