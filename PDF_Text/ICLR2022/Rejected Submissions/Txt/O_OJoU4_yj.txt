Under review as a conference paper at ICLR 2022
Stabilized Self-training with Negative Sam-
pling on Few-labeled Graph Data
Anonymous authors
Paper under double-blind review
Ab stract
Graph neural networks (GNNs) are designed for semi-supervised node classifica-
tion on graphs where only a small subset of nodes have class labels. However,
under extreme cases when very few labels are available (e.g., 1 labeled node per
class), GNNs suffer from severe result quality degradation.
Specifically, we observe that existing GNNs suffer from unstable training process
on few-labeled graph data, resulting to inferior performance on node classifica-
tion. Therefore, we propose an effective framework, Stabilized self-training with
Negative sampling (SN), which is applicable to existing GNNs to stabilize the
training process and enhance the training data, and consequently, boost classifica-
tion accuracy on graphs with few labeled data. In experiments, we apply our SN
framework to two existing GNN base models (GCN and DAGNN) to get SNGCN
and SNDAGNN, and evaluate the two methods against 13 existing solutions over 4
benchmarking datasets. Extensive experiments show that the proposed SN frame-
work is highly effective compared with existing solutions, especially under set-
tings with very few labeled data. In particular, on a benchmark dataset Cora with
only 1 labeled node per class, while GCN only has 44.6% accuracy, SNGCN
achieves 62.5% accuracy, improving GCN by 17.9%; SNDAGNN has accuracy
66.4%, improving that of the base model DAGNN (59.8%) by 6.6%.
1 Introduction
Graph is an expressive data model, representing objects and the relationships between objects as
nodes and edges respectively. Graph data are ubiquitous with a wide range of real-world appli-
cations, e.g., social network analysis (Qiu et al., 2018; Li & Goldwasser, 2019), traffic network
prediction (Guo et al., 2019; Li et al., 2019), protein interface prediction (Fout et al., 2017), recom-
mendation systems (Fan et al., 2019; Yang et al., 2020a). Among these applications, an important
task is to classify the nodes in a graph into various classes. However, one tough situation commonly
existing is the lack of sufficient labeled data, which are also expensive to collect.
To ease the situation, semi-supervised node classification on graphs has attracted much attention
from both industry (Qiu et al., 2018; Li & Goldwasser, 2019) and academia (Defferrard et al., 2016;
Hamilton et al., 2017; Velickovic et al., 2018; Liu et al., 2020; Li et al., 2018; Klicpera et al., 2019).
It aims to leverage a small amount of labeled nodes and additionally a large amount of unlabeled
nodes in a graph to train an accurate classifier. There exists a collection of graph neural networks
for semi-supervised node classification (Kipf & Welling, 2017; Velickovic et al., 2018; Monti et al.,
2017; Hamilton et al., 2017; Klicpera et al., 2019; Liu et al., 2020). For instance, Graph convolution
networks (GCNs) rely on a message passing scheme called graph convolution that aggregates the
neighborhood information of a node, including node features and graph topology, to learn node
representations, which can then be used in downstream classification tasks (Kipf & Welling, 2017).
Despite the great success of GCNs, under the extreme cases when very few labels are given (e.g.,
only one labeled node per class), the shallow GCN architecture, typically with two layers (Kipf
& Welling, 2017), cannot effectively propagate the training labels over the input graph, leading to
inferior performance. In particular, as shown in our experiments, on a benchmark dataset Cora with
1 labeled node per class (Cora-1), GCN is even less accurate than some unsupervised methods, such
as DGI(Velickovic et al., 2019) and G2G (Bcjchevski & Gunnemann, 2018). Recently, several latest
studies try to improve classification accuracy by designing deeper GNN architectures, e.g., DAGNN
1
Under review as a conference paper at ICLR 2022
40
302010
(兴)su8Jd
100	200
Epoch
(a) GCN
40
300
Ooo
3 2 1
(兴)su8Jd
100	200	300
Epoch
(b) DAGNN
40302010
(兴)u8Jd
Oooo
4 3 2 1
(兴)u8Jd
(C) SNGCN	(d) SNDAGNN
Figure 1: The distribution of predicted labels in different classes in Cora-1.
(Liu et al., 2020), whiCh also address the over-smoothing issue identified in (Xu et al., 2018; Li et al.,
2018; Chen et al., 2020a). However, these deep GNNs are still not direCtly designed to taCkle the
sCarCity of labeled data, espeCially when only very few labels are available.
After ConduCting an in-depth study, we have an important finding that existing GNNs suffer from
unstable training proCess, when labeled nodes are few. In partiCular, on Cora dataset with 7 Classes,
for eaCh run, we randomly seleCt 1 labeled node per Class as the training data for both GCN and
DAGNN, and repeat 100 runs with 300 epoChs per run, to get the average number of prediCted labels
in perCentage per Class at eaCh epoCh and also the standard deviation. The statistiCal results of GCN
and DAGNN are shown in Figures 1(a) and 1(b) respeCtively. x-axis is the epoCh from 0 to 300, and
y-axis is the perCentage of a Class in the prediCted node labels. There are 7 Colored lines representing
the average perCentage of the prediCted labels of the respeCtive Classes, as the epoCh inCreases. The
dashed lines are the ground-truth perCentage of eaCh Class in the Cora dataset. The shaded areas
in Colors represent the standard deviation. Observe that in Figure 1(a), GCN has high varianCe at
different runs when prediCting node labels, and the varianCe keeps large at late epoChs, e.g., 300,
whiCh indiCates that GCN is quite unstable at different runs with 1 training label per Class sampled
randomly, leading to inferior ClassifiCation aCCuraCy as illustrated in our experiments. Moreover,
as shown in Figure 1(b), DAGNN also suffers from unstable training proCess. The varianCe of
DAGNN is relatively smaller than that of GCN, whiCh provides a hint about why DAGNN performs
better than GCN. Nevertheless, both GCN and DAGNN yield unstable training proCess with large
varianCe. SinCe there is only 1 labeled node per Class in Cora-1, at different runs, the randomly
sampled training nodes Can heavily influenCe the message passing proCess in GCN and DAGNN,
depending on the ConneCtivity of the training nodes to their neighborhoods over the graph topology,
whiCh result to the unstable training proCess observed above.
To address the unstable training proCess of existing GNNs when only very few labeled data are
available, we propose a framework, Stabilized self-training with Negative sampling (SN), whiCh is
readily appliCable to existing GNNs to improve ClassifiCation aCCuraCy via stabilized training pro-
Cess. In the proposed SN framework, at eaCh epoCh, we seleCt a set of nodes with prediCted labels
of high ConfidenCe as pseudo labels and add suCh pseudo labels into training data to enhanCe the
training of next epoCh. To taCkle the unstable issue of existing GNNs, we develop a stabilizing
teChnique in self-training to balanCe the training. We then design a negative sampling regularization
teChnique over pseudo labels to further improve node ClassifiCation aCCuraCy. In experiments, we
apply our SN framework to GCN and DAGNN, denoted as SNGCN and SNDAGNN respeCtively.
2
Under review as a conference paper at ICLR 2022
Figures 1(c) and 1(d) report the average percentage and standard variance of the predicted labels
per class per epoch of SNGCN and SNDAGNN on Cora-1 respectively. With our stabilized self-
training technique, obviously, the variance of SNGCN in Figure 1(c) decreases quickly and becomes
stable as epoch increases, compared with Figure 1(a) of GCN. SNDAGNN is also more stable than
DAGNN as shown in Figures 1(d) and 1(b) respectively. As reported later in experiments, with the
proposed SN framework, SNGCN achieves 62.5% node classification accuracy on Cora-1, signifi-
cantly improving GCN (44.6%) by 17.9%, and SNDAGNN obtains 66.4% accuracy on Cora-1 and
outperforms DAGNN (59.8%) by a substantial margin of 6.6%. We conduct extensive experiments
on 4 benchmarking datasets, and compare with 13 existing solutions, to evaluate the performance
of the proposed SN framework. Experimental results demonstrate that our SN framework is able to
significantly improve classification accuracy of existing GNNs when only few labels are available,
and is also effective when training labels are sufficient.
2 Related Work
In literature, there are two directions to address the scarcity of labeled data for semi-supervised
node classification: (i) explore multi-hop graph topological features to propagate the labels in L
over the input graph, e.g., GCN (Kipf & Welling, 2017) and DAGNN (Liu et al., 2020); (ii) enhance
the training data by pseudo labels (self-training) (Li et al., 2018) or augmenting graph data by new
edges and features (Kong et al., 2020; Zhao et al., 2021). Note that these two directions are not
mutually exclusive, but can work together on few-labeled graph data. Here we review the existing
studies that are most relevant to this paper.
GNNs. There exist a large collection of GNNs, such as GCN, DAGNN, GAT, MoNet, and APPNP
(Kipf & Welling, 2017; Liu et al., 2020; Bruna et al., 2014; Henaff et al., 2015; Defferrard et al.,
2016; Velickovic et al., 2018; Monti et al., 2017; Chen et al., 2020b; Klicpera et al., 2019). We
introduce the details of GCN (Kipf & Welling, 2017) and DAGNN (Liu et al., 2020) here. GCN
learns the representation of each node by iteratively aggregating the representations of its neighbors.
Specifically, GCN consists of k > 0 layers, each with the same propagation rule defined as follows.
At the l-th layer, the representations H(IT) of previous layer are aggregated to get H(l).
H(I) = σ(AH(IT)W(I)), l = 1, 2,..., k.	(1)
1 1 1
A = D2 AD 2 is the graph laplacian, where A = A + I is the adjacency matrix of G after
adding self-loops (I is the identity matrix) and D is a diagonal matrix With Dii = Ej Aj. W(I)
is a trainable weight matrix of the l-th layer, and σ is a nonlinear activation function. Initially,
H(0) = X. Note that GCN usually achieves superior performance with 1-layer or 2-layer models
(Kipf & Welling, 2017). When applying multiple layers to leverage large receptive fields, the per-
formance degrades severely, due to the over-smoothing issue identified in (Xu et al., 2018; Li et al.,
2018; Chen et al., 2020a). A recent deep GNN architecture, DAGNN, tackles the over-smoothing
issue and achieves state-of-the-art results by decoupling representation transformation and propaga-
tion in GNNs (Liu et al., 2020). Then it utilizes an adaptive adjustment mechanism to balance the
information from local and global neighborhoods of each node. Specifically, the mathematical ex-
pression of DAGNN is as follows. DAGNN uses a learnable parameter s ∈ Rc×1 to adjust the weight
of embeddings at different propagation level (from 1 to k). It processes data in the following way.
Z = MLP(X) ∈ Rn×c, H1 = Al ∙ Z ∈ Rn×c,l = 1,2,…，k , S1 = H1 ∙ S ∈ Rn×1,l = 1,2,…，k,
S l = [Si, Si,..., Si] ∈ Rn×c,l = 1, 2,...,k, Xout = SOftmaX(Ek=I Hi Θ S ι), where Al is the l-th
power of matrix A, Θ is the Hadamard product, ∙ is dot product, MLP is the Multilayer Perceptron
and softmax operation is on the second dimension.
Data Augmentation. Another way to address the situation of limited labeled data is to add pseudo
labels to training dataset by self-training (Li et al., 2018), or enhance the graph data by adding new
edges and features (Zhao et al., 2021; Kong et al., 2020). Self-training itself is a general methodology
(Scudder, 1965) and is used in various domains in addition. It is used in word-sense disambiguation
(Yarowsky, 1995; Hearst, 1991), bootstrap for information extraction and learning subjective nouns
(Riloff & Jones, 1999), and text classification (Nigam et al., 2000). In (Zhou et al., 2012), it suggests
that selecting informative unlabeled data using a guided search algorithm can significantly improve
performance over standard self-training framework. Buchnik & Cohen (2018) mainly consider self-
training for diffusion-based techniques. Recently, self-training has been adopted for semi-supervised
3
Under review as a conference paper at ICLR 2022
tasks on graphs. For instance, Li et al. (2018) propose self-training and co-training techniques for
GCN. This self-training work selects the top-k confident predicted labels as pseudo labels. The
co-training technique co-trains a GCN with a random walk model to handle few-labeled data. Com-
pared with existing self-training work, our framework are different as shown later. In particular, our
framework has a different strategy to select pseudo labels and also has a stabilizer to address the
deficiencies of existing GNNs; moreover, we propose a negative sampling regularization technique
to further boost accuracy. Besides, in existing work, if a node is selected as a pseudo label, it will
never be moved out even if the pseudo label becomes obviously wrong in later epochs. On the other
hand, in our framework, we update pseudo labels in each epoch to avoid such an issue. There also
exist studies to augment the original graph data, which is different from self-training. For instance,
Zhao et al. (2021) utilize link prediction to promote intra-class edges and demote inter-class edges in
a given graph. Kong et al. (2020) iteratively augment node features with gradient-based adversarial
perturbations to enhance the performance of GNNs.
3 The Framework
3.1	Problem Formulation
Let G = (V , E , X) be a graph consisting of a node set V with cardinality n, a set of edges E of
size m, each connecting two nodes in V , a feature matrix X ∈ Rn×d , where d is the number of
features in G . For every node vi ∈ V , it has a feature vector Xi ∈ Rd , where Xi is the i-th row of
X. Let c be the number of classes in G. We use L to denote the set of labeled nodes, and obviously
L ⊆ V. LetU be the set of unlabeled nodes andU = V \ L. Each labeled node vi ∈ L has a one-hot
vector Yi ∈ {0, 1}c, indicating the class label of vi. Under the few-labeled setting, |L| ≪ |U|. A
high-level definition of the semi-supervised node classification problem is as follows.
Definition 1. Given a graph G = (V , E , X), a set of labeled nodes L ⊆ V, and a groundtruth
class label Yi ∈ {0, 1}c per node vi ∈ L, assuming that each node belongs to exactly one class,
Semi-Supervised Node Classification predicts the labels of the unlabeled nodes.
In particular, the aim is to leverage the graph G with the labeled nodes in L, and to train a forward
predicting classification model/function f(G, θ) that takes as input the graph G and a set of trainable
parameters θ. The output of f is a matrix F ∈ Rn×c, with each i-th row Fi ∈ [0, 1]c representing
the output probability vector of node vi ∈ V (the 1-norm of Fi is normalized to 1).
We adopt the widely used cross-entropy loss. For a node vi , its loss of Fi with respect to its true
class label Yi, L(Yi, Fi), is defined as follows.
c
L(Yi, Fi) = -	Yi,jln(Fi,j)
j=1
where Yi,j is the j-th value in Yi and Fi,j is the j-th value in Fi.
3.2	Stabilized Self-training
Recall that existing GNNs suffer from unstable training process as shown in Figure 1. In this section,
we present the stabilized self-training technique that not only augments training data with pseudo
labels but also stabilizes the training process. We first explain how to choose pseudo labels and then
introduce loss function of stabilized self-training.
At a certain epoch, given the matrix F ∈ Rn×c, with each i-th row Fi ∈ [0, 1]c representing the
output probability vector of node vi ∈ V. Let Yi,j satisfying the following Eq. (2) be the predicted
label of node vi. We say that, with confidence Fi,j , node vi has class label Cj ; i.e., the largest
element Fi,j in vector Fi is called the confidence of node vi .
Y = ( 1 if j = argmaXj′ Fi,j′,
i,j 0 otherwise,
(2)
Then for every unlabeled node vi ∈ U , we can get Ni , the number of nodes with the same predicted
label as vi, in Eq. (3). Recall that in Figure 1, in existing GCN and DAGNN, the distribution of
4
Under review as a conference paper at ICLR 2022
the predicted labels is unstable with large variance during the training process; we also observe that
most predicted labels are in the same class, especially at the early epochs. To reduce such unstable
situation of existing GNNs, we use Ni as a stabilizer in our framework to be explained shortly.
Ni = ∣{vj ∈u IYi = Yj} I	⑶
For every unlabeled nodes vi in U , it can have a predicted label. However, the confidence
argmaxj' Fij' might be low. We do not want to add such low-confidence labels into the training
of the next epoch. Therefore, we only choose those unlabeled nodes with high-confidence predicted
labels as pseudo labels to be augmented into the training data of next epoch. In particular, an unla-
beled node vi is selected to be a node with pseudo label in next epoch, if its confidence satisfies a
threshold β, as shown below. We use U' to denote all unlabeled nodes selected with pseudo labels.
U'
{vi ∈ U∣ max Fij > β}
(4)
where β ∈ [0, 1] is a threshold controlling the extent of cautious selection for self-training. A bigger
threshold means stricter selection of the pseudo labels.
After explaining how to choose pseudo labels above, we present the loss of our stabilized self-
training technique in Eq. (5). In particular, we design N17τ as the stabilizer of the training process,
Ni +1
to overcome the deficiencies of existing GNNs illustrated in Figure 1. The intuition is that, if an
unlabeled node vi is in a pseudo label class with many nodes, its importance in the loss function is
reduced. In other words, our stabilized self-training loss reduces the impact of classes with many
pseudo labeled nodes, which is especially useful to rectify the training process when the predictions
in the early epochs are incorrect or less confident, compared with ground truth.
Lsst = E Ni + 1 ∙ L(Yi，Fi)
∀Vi∈U'	i
(5)
Compared with existing self-training techniques (Li et al., 2018), our stabilized self-training tech-
nique has major differences. First, we develop the stabilizer to re-weight the importance of pseudo
labels in the loss function, so as to address the unstable issue of existing GNNs. Second, we select
only those nodes with high-confidence pseudo labels satisfying β threshold, and adaptively update
the pseudo labels per epoch, meaning that a pseudo label in previous epoch will be removed in the
next epoch if its confidence becomes low. On the other hand, existing methods keep a pseudo label
once it is selected and never remove it in later epochs (Li et al., 2018), which may harm the training
quality if the pseudo label is wrong compared with ground truth.
3.3	Negative Sampling Regularization
Under extreme cases with very few labeled nodes (e.g., 1 labeled node per class), we further design
a negative sampling regularization technique for better performance. In existing studies, negative
sampling is used as an unsupervised technique over node embeddings in network embedding meth-
ods (Yang et al., 2020b; Velickovic et al., 2019; Yang et al., 2020b). Here we customize it to the
semi-supervised node classification task, and apply negative sampling over labels instead of embed-
dings.
Intuitively, the label of a node v should be distant to the label of another node u if these two nodes
are faraway on the input graph G. Specifically, a positive sample is a node Vi in L or U'. We sample
a set I of positive samples from L ∪ U' uniformly at random. The negative samples of a positive
sample vi are the nodes that are not directly connected to vi in graph G. For each positive sample vi
in I, we sample a fixed-size set Ji of negative samples uniformly at random.
For a positive-negative pair " Vj), compared with the Yi of Vi ∈ L ∪ U', the intention is to let
the output vector Fj of Vj to be as different as possible. Here we use the symbol Yi to represent
the pseudo label Yi for node Vi in U' or ground-truth label Yi of node Vi in L to avoid ambiguity.
Denote 1 as the all-one vector in Rc. Then we have the following loss of all positive-negative pairs.
Lneg
Σ Σ
∀vi∈I ∀vj ∈Ji
1
∣ι∣∙∣Jil
,~ .
L(Yi, 1 — Fj)
(6)
5
Under review as a conference paper at ICLR 2022
1
2
3
4
5
6
7
8
9
10
11
12
13
Algorithm 1: SN Framework Over GNNs
Input: Graph G = (V, E, X) with labeled node set L and unlabeled node set U
Output: the learned classifier f (∙, θ).
Generate initial parameter θ for model f (∙, ∙).
for each epoch t = 0, 1, 2, ..., T do
Use GNN to compute prediction F — f(G,θ)
Get high confidence set U' and its stabilizing factor N17τ per node Vi(Section 3.2)
Ni +1
Get positive samples and corresponding negative samples using L∪U' and G (Section 3.3)
Get Ltotal of current epoch by Eq. (7) (Section 3.4)
Update model parameters by θ J Adam OPtimizer(θ, gradient = VθLtotal).
if Convergence then
Break
end
end
3.4	Objective Function and Algorithm
Our final loss function is as follows, and it combines the stabilized self-training loss and negative
sampling loss in Eq. (5) and Eq. (6) respectively.
Ltotal =TTTT ∙	L(Yi, Fi ) + λ1Lsst + λ2Lneg,
| | ∀vi∈L
(7)
where λ1 and λ2 are factors controlling the impact of these two losses.
Algorithm 1 shows the pseudo-code of our SN framework over GNNs, and it takes as input a graph
G with labeled nodes L and unlabeled nodes U. Note that SN can be instantiated over either a
shallow or a deep GNN, e.g., GCN and DAGNN introduced in Section 2. The output of Algorithm 1
is the learned classification model f with trainable parameters θ. At Line 3, SN initializes the
trainable parameters θ by Xavier (Glorot & Bengio, 2010). Then from Lines 4 to 12, SN trains the
classification model per epoch t iteratively, until convergence or the max number T of iterations is
reached. Specifically, at Line 5, SN first use a GNN to obtain the forward prediction output F. Then
at Line 6, SN detects the pseudo-labeled set U' and obtain the stabilizer N17τ of each node Vi in
Ni +1
U', after which, at Line 7 We perform negative sampling to obtain positive samples I and negative
samples Ji . At Line 8, SN computes loss Ltotal of current epoch according to Eq. (7). And at Line
9, SN updates model parameters θ for next epoch by Adam optimizer (Kingma & Ba, 2015).
4 Experiments
We evaluate SN against 13 competitors for semi-supervised node classification on 4 benchmark
graph datasets. All experiments are conducted on a machine powered by an Intel(R) Xeon(R) E5-
2603 v4 @ 1.70GHz CPU, 131GB RAM, 16.04.1-Ubuntu, and 4 Nvidia Geforce 1080ti Cards with
Cuda version 10.2. Source codes of all competitors are obtained from the respective authors. Our
SN framework is implemented in Python, using libraries including PyTorch (Paszke et al., 2019) and
PyTorch Geometric (Fey & Lenssen, 2019). An anonymous link1 of our source code is provided.
4.1	Datasets and Competitors
Datasets. Table 1 shows the statistics of the 4 real-world graphs used in our experiments. We list
the number of nodes, edges, features and classes in each graph dataset respectively. Specifically, the
4 datasets are Cora (Sen et al., 2008), Citeseer (Sen et al., 2008), Pubmed (Sen et al., 2008), and
Core-full (Bojchevski & GUnnemann, 2018), all of which are widely used for benchmarking node
classification performance in existing studies (Sun et al., 2020; Li et al., 2018; Liu et al., 2020).
Notice that every node in these graphs has a ground-truth class label.
1https://anonymous.4open.science/r/e7aca211-0d8d-4564- 8f3f-0ef24b01941e/
6
Under review as a conference paper at ICLR 2022
Table 1: Datasets
	Cora	Citeseer ∣ PUbmed ∣ Cora-full		
	 #of Nodes	2708	3327	19717	19793
# of Edges	5429	4732	44338	65311
# of Features	1433	3703	500	8710
# of Classes	7	6	3	67
Competitors. We compare with 13 existing solutions, including LP (Label Propagation) (Wu et al.,
2012), DeePWalk (Perozzi et al., 2014), LINE (Tang et al., 2015), G2G (Bcjchevski & Gunnemann,
2018), DGI (Velickovic et al., 2019), GCN (Kipf & Welling, 2017), GAT (Velickovic et al., 2018),
MoNet (Monti et al., 2017), APPNP (KlicPera et al., 2019), DAGNN (Liu et al., 2020), STs (Li
et al., 2018), LCGCN and LCGAT in (Xu et al., 2020). In Particular, GCN, GAT, MoNet, APPNP,
and DAGNN are GNNs. DeePWalk, DGI, LINE, and G2G are unsuPervised network embedding
methods. STs rePresents the four variants in (Li et al., 2018), including Self-Training, Co-Training,
Union, and Intersection; we summarize the best results among them as the results of STs.
4.2	Experimental Settings
We evaluate our framework and the comPetitors on semi-suPervised node classification tasks with
various settings. In Particular, for each graPh dataset, we rePeat exPeriments on 100 random data
sPlits as suggested in (Liu et al., 2020; Li et al., 2018) and rePort the average Performance. For
each graPh dataset, we vary the number of labeled nodes Per class in {1, 3, 5, 10, 20}, where 1, 3, 5
rePresent the very few-labeled settings. Following convention in existing work (Liu et al., 2020),
we exPlain what a random data sPlit is, as follows. For examPle, when the number of labeled nodes
Per class on Cora is 3 (denoted as Cora-3), since Cora has 7 classes, we randomly Pick 3 nodes Per
class, combining together as a training set of size 21 (i.e., the labeled node set L), and then, among
the remaining nodes, we randomly select 500 nodes as a validation set, and 1000 nodes as a test set.
Each data sPlit consists of a training set, a validation set, and a test set as mentioned above. We use
the classification accuracy on test set as evaluation metric. SPecifically, accuracy is defined as the
fraction of the testing nodes whose class labels are correctly Predicted by the learned classifier.
4.3	Implementation Details
We instantiate SN framework over the classic GCN model with 2 layers and a recent deeP GNN
architecture DAGNN to demonstrate the effectiveness and aPPlicability of SN. The instantiation of
SN over GCN and DAGNN are dubbed as SNGCN and SNDAGNN resPectively. SNGCN and
SNDAGNN has Parameters (i) inherited from GCN and DAGNN and (ii) develoPed in SN. Hence,
we first tune the best Parameters of the base models under each classification task setting on each
dataset and rePort this result for them for a fair comParison.
Base models (GCN and DAGNN). In base models, we tune the Parameters: a L2 regularization
rate with search sPace in {1e-2, 5e-3, 1e-3, 5e-4, 1e-4, 5e-5, 0}, a droPout rate in {0.5, 0.8}. For
DAGNN, the level k of ProPagation after MLP is searched in {10, 15, 20}. We have the follow-
ing Parameters for base models in exPeriments: the number of hidden units of GCN and MLP (in
DAGNN) is 64 units without bias; the number of layers of GCN and MLP (in DAGNN) is 2 layers;
the learning rate of Adam OPtimizer is 0.01; the activation function is RELU; the maximum number
of training ePochs is 1000. Moreover, early stoPPing is triggered when the validation loss is smaller
than the average validation loss of Previous 100 ePochs, and the current ePoch is beyond 500 ePochs.
SN over GCN and DAGNN (SNGCN and SNDAGNN). After finding the best hyPer Parameters
of the base models, we then tune the Parameters in SN. λ1 is searched in {0.1, 1} and λ2 is searched
in {0, 0.1, 1}. Stabilizing enabler searches in {True, False}. The number of Positive and negative
samPles (|I|, |Ji|) is searched in {(1, 10), (2, 5), (5, 2), (10, 1)}. For instance, (2, 5) means that we
samPle 2 Positive nodes and then for each Positive node, we samPle 5 negative nodes.
Competitors. We use the Parameters suggested in the original PaPers of the comPetitors to tune
their models, and rePort the best results of the comPetitors. Notice that for unsuPervised network
embedding methods, including DeePWalk, DGI, LINE, and G2G, after obtaining the embeddings,
we use logistic regression to train a node classifier over the embedding (Velickovic et al., 2019).
7
Under review as a conference paper at ICLR 2022
Table 2: Accuracy results (in percentage) on Cora and CiteSeer respectively, averaged over
100 random data splits. (The best accuracy is in bold.)
# of Labels	Cora	CiteSeer	∣
per class	1	3	5	10	20	1	3	5	10	20
GCN	44.6 63.8 71.3 77.2 81.4	40.4 53.5 61.0 65.8 69.5
SNGCN (ours)	62.5 72.8 75.8 80.7 82.5	56.2 66.4 68.0 70.2 72.1
DAGNN	59.8 72.4 76.7 80.8 83.7	46.5 58.8 63.6 67.9 71.2
SNDAGNN (ours)	66.4 77.6 79.8 82.2 84.1	48.5 65.9 67.9 69.8 72.1
LP	51.5 60.5 62.5 64.2 67.3	30.1 37.0 39.3 41.9 44.8
DeepWalk	40.4 53.8 59.4 65.4 69.9	28.3 34.7 38.1 42.0 45.6
LINE	49.4 62.6 63.4 71.1 74.0	28.0 34.7 38.0 43.1 48.5
G2G	54.5 68.1 70.9 73.8 75.8	45.1 56.4 60.3 63.1 65.7
DGI	55.3 70.9 72.6 76.4 77.9	46.1 59.2 64.1 67.6 68.7
STs	53.1 67.3 72.5 76.2 79.8	37.2 51.8 60.7 67.4 70.2
GAT	41.8 61.7 71.1 76.0 79.6	32.8 48.6 54.9 60.8 68.2
MoNet	43.4 61.2 70.9 76.1 79.3	38.8 52.9 59.7 64.6 66.9
APPNP	44.7 66.3 74.1 79.0 81.9	34.6 52.2 59.4 66.0 71.8
LCGCN	63.6 74.4 77.5 80.4 82.4	55.3 59.0 68.4 70.3 72.1
LCGAT	58.7 74.5 77.5 79.7 82.6	50.9 66.3 68.5 70.9 71.5
4.4	Overall Results
Table 2 reports the classification accuracy (in percentage) of all methods on Cora and CiteSeer,
when varying the number of labeled nodes per class in {1,3,5,10,20}. The first and second rows
report the performance of GCN and SNGCN. The third and fourth rows report the performance of
DAGNN and SNDAGNN. Observe that SNGCN (resp. SNDAGNN) enhanced by our SN frame-
work significantly outperforms GCN (resp. DAGNN) under all task settings, and the performance
gain of SNGCN is especially significant when the labels per class are few. For instance, on Cora-1,
SNGCN has accuracy 62.5%, while the accuracy of GCN is 44.6%, indicating that the proposed
framework improves GCN by 17.9%. This demonstrates the power of the proposed SN framework
to boost classification performance. Compared with other competitors, observe that SNDAGNN has
the best performance under all settings of Cora (in bold), and SNGCN has the best performance on
CiteSeer-1 and 3 and 20, while achieving similar performance compared with LCGCN and LCGAT
on CiteSeer-5 and 10. Apart from the superior performance of our SN framework over GCN and
DAGNN, in Table 2, we can observe two interesting findings. First, under extremely-few-labels
settings (e.g., Cora-1), unsupervised methods G2G (54.5%) and DGI (55.3%) achieve better perfor-
mance than GCN (44.6%). One reason is that the unsupervised methods are good at cases when no
labeled data are available, while GCNs still require a sufficient amount of labeled data. This finding
demonstrates the intuition of the negative sampling regularization in Section 3.3, and also sheds
light on possible future research to use unsupervised techniques to further enhance the performance
of semi-supervised learning. Second, the performance gap between our methods and competitors
enlarges as the number of labels per class decreases, which further illustrates the effectiveness of the
proposed SN framework under extreme settings on graphs with very few-labeled nodes per class.
Table 3 presents the classification accuracy of all methods under all settings in {1,3,5,10,20} on
PubMed and Cora-full datasets. We exclude from this table, the inaccurate competitors (e.g., Deep-
Walk and LINE) that are obviously outperformed by other competitors. Observe that on PubMed and
Cora-full, SNDAGNN and SNGCN achieve the higher accuracy consistently than their respective
base models GCN and DAGNN. For instance, on Cora-full-1, SNGCN achieves 30.8% accuracy,
6.3% better than GCN. Moreover, observe that on PubMed, SNDAGNN consistently outperforms
all other competitors, e.g., LCGCN; on Cora-full, SNGCN outperforms all competitors on 1 and 3
settings, and SNDAGNN has the best accuracy on 5, 10, and 20 settings.
In summary, the experimental results presented in Tables 2 and 3 validate the effectiveness of the
proposed SN framework over GCN and DAGNN to boost classification performance, especially
when only very few labeled nodes are available.
8
Under review as a conference paper at ICLR 2022
Table 3: Accuracy results (in percentage) on PubMed and Cora-full respectively, averaged
over 100 random data splits. ( The best accuracy is in bold.)
# of Labels	PubMed	Cora-full
per class	1	3	5	10	20	1	3	5	10	20
GCN	55.5 66.0 70.4 74.6 78.7	24.5 41.4 48.1 55.8 60.2
SNGCN (ours)	60.8 67.8 71.6 76.1 79.4	30.8 44.9 49.4 56.6 60.9
DAGNN	59.4 69.5 72.0 76.8 80.1	27.3 43.2 49.8 55.8 60.4
SNDAGNN (ours)	61.0 72.1 74.9 78.2 80.6	27.6 44.4 51.1 56.8 61.2
LP	55.7 61.9 63.5 65.2 66.4	26.3 32.4 35.1 38.0 41.0
G2G	55.2 64.5 67.4 72.0 74.3	25.8 36.4 43.3 49.3 54.3
DGI	55.1 63.4 65.3 71.8 73.9	26.2 37.9 46.5 55.3 59.8
STs	55.1 65.4 69.7 74.0 78.5	29.2 43.6 48.9 53.4 60.8
APPNP	54.8 66.9 70.8 76.0 79.4	24.3 41.5 48.5 55.3 60.1
GAT	52.7 64.4 69.4 73.7 73.5	24.8 41.0 47.5 54.7 59.9
LCGCN	56.6 69.2 72.6 74.6 80.0	26.7 43.9 49.2 55.9 60.5
LCGAT	49.5 59.2 62.3 70.2 65.3	27.4 43.2 48.4 55.0 60.1
b baseline o baseline+S o baseline+N X baseline+SN
Number of labels per class
(a)	ablation on GCN
Number of labels per class
(b)	ablation on DAGNN
Figure 2: Ablation study of SN on Cora.
4.5 Ablation Study
We conduct ablation study to evaluate the contributions of the techniques of SN presented in Sec-
tion 3. Denote baseline+SN as the method with the whole SN framework enabled, baseline+S as
the method with only stabilized self-training loss in Eq. (5) enabled, and baseline+N as the method
with only negative sampling loss in Eq. (6) enabled. Figures 2a and 2b report the ablation results on
baselines GCN and DAGNN respectively, on Cora when varying the number of labels per class in
{1, 3, 5, 10, 20}. Observe that the accuracies of baseline+S and baseline+N are always better than
the baseline model, i.e., GCN and DAGNN. Also the accuracy of baseline+SN is almost always the
highest under all settings. The ablation study demonstrates the power of our proposed techniques to
improve classification accuracy.
5 Conclusion
This paper presents Stabilized self-training with Negative sampling (SN), an effective framework for
semi-supervised node classification on few-labeled graph data. SN achieves superior performance
on graphs with extremely few labeled nodes, through two main designs: a stabilized self-training
technique that adaptively selects and re-weights high-confidence pseudo labels based on the confi-
dence distribution of current epoch to enhance the training process, and a negative sampling regu-
larization that further fully utilizes unlabeled data to train a high-quality classifier. The effectiveness
of SN is extensively evaluated on 4 real graphs, compared against 13 existing solutions. Regard-
ing future work, we plan to enhance SN by investigating other unsupervised techniques, and also
implement SN on top of more GNN architectures to further demonstrate its applicability.
9
Under review as a conference paper at ICLR 2022
References
Aleksandar Bojchevski and StePhan Gunnemann. Deep gaussian embedding of graphs: UnsUPer-
vised inductive learning via ranking. In ICLR, 2018.
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally
connected networks on graphs. In ICLR, 2014.
Eliav Buchnik and Edith Cohen. Bootstrapped graph diffusions: Exposing the power of nonlinear-
ity. In Abstracts of the 2018 ACM International Conference on Measurement and Modeling of
Computer Systems, pp. 8-10, 2018.
Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the over-
smoothing problem for graph neural networks from the topological view. In AAAI, pp. 3438-3445,
2020a.
Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph
convolutional networks. In ICML, pp. 1725-1735, 2020b.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. In NeurIPS, pp. 3837-3845, 2016.
Wenqi Fan, Yao Ma, Qing Li, Yuan He, Yihong Eric Zhao, Jiliang Tang, and Dawei Yin. Graph
neural networks for social recommendation. In WWW, pp. 417-426, 2019.
Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric.
arXiv preprint arXiv:1903.02428, 2019.
Alex Fout, Jonathon Byrd, Basir Shariat, and Asa Ben-Hur. Protein interface prediction using graph
convolutional networks. In NeurIPS, pp. 6530-6539, 2017.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In AISTATS, pp. 249-256, 2010.
Shengnan Guo, Youfang Lin, Ning Feng, Chao Song, and Huaiyu Wan. Attention based spatial-
temporal graph convolutional networks for traffic flow forecasting. In AAAI, pp. 922-929, 2019.
William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In NeurIPS, pp. 1024-1034, 2017.
Marti Hearst. Noun homograph disambiguation using local context in large text corpora. Using
Corpora, pp. 185-188, 1991.
Mikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured
data. arXiv preprint arXiv:1506.05163, 2015.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In ICLR, 2017.
Johannes Klicpera, Aleksandar Bojchevski, and Stephan GUnnemann. Predict then propagate:
Graph neural networks meet personalized pagerank. In ICLR, 2019.
Kezhi Kong, Guohao Li, Mucong Ding, Zuxuan Wu, Chen Zhu, Bernard Ghanem, Gavin Taylor,
and Tom Goldstein. FLAG: adversarial data augmentation for graph neural networks. CoRR,
2020.
Chang Li and Dan Goldwasser. Encoding social information with graph convolutional networks
forpolitical perspective detection in news media. In ACL, pp. 2594-2604, 2019.
Jia Li, Zhichao Han, Hong Cheng, Jiao Su, Pengyun Wang, Jianfeng Zhang, and Lujia Pan. Predict-
ing path failure in time-evolving graphs. In SIGKDD, pp. 1279-1289, 2019.
Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for
semi-supervised learning. In AAAI, pp. 3538-3545, 2018.
10
Under review as a conference paper at ICLR 2022
Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In SIGKDD,
pp. 338-348, 2020.
Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola,Jan Svoboda, and Michael M.
Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In CVPR,
pp. 5425-5434, 2017.
Kamal Nigam, Andrew McCallum, Sebastian Thrun, and Tom M. Mitchell. Text classification from
labeled and unlabeled documents using EM. Mach. Learn., pp. 103-134, 2000.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep
learning library. In NeurIPS, pp. 8024-8035, 2019.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: online learning of social representa-
tions. In SIGKDD, pp. 701-710, 2014.
Jiezhong Qiu, Jian Tang, Hao Ma, Yuxiao Dong, Kuansan Wang, and Jie Tang. Deepinf: Social
influence prediction with deep learning. In SIGKDD, pp. 2110-2119, 2018.
Ellen Riloff and Rosie Jones. Learning dictionaries for information extraction by multi-level boot-
strapping. In AAAI, pp. 474-479, 1999.
H. J. Scudder. Probability of error of some adaptive pattern-recognition machines. IEEE Trans. Inf.
Theory, pp. 363-371, 1965.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Gallagher, and Tina Eliassi-Rad.
Collective classification in network data. AI Mag., pp. 93-106, 2008.
Ke Sun, Zhouchen Lin, and Zhanxing Zhu. Multi-stage self-supervised learning for graph convolu-
tional networks on graphs with few labeled nodes. In AAAI, pp. 5892-5899, 2020.
Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. LINE: large-scale
information network embedding. In WWW, pp. 1067-1077, 2015.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In ICLR, 2018.
Petar Velickovic, William Fedus, William L. Hamilton, Pietro Lio, Yoshua Bengio, and R. Devon
Hjelm. Deep graph infomax. In ICLR, 2019.
Xiao-Ming Wu, Zhenguo Li, Anthony Man-Cho So, John Wright, and Shih-Fu Chang. Learning
with partially absorbing random walks. In NeurIPS, pp. 3086-3094, 2012.
Bingbing Xu, Junjie Huang, Liang Hou, Huawei Shen, Jinhua Gao, and Xueqi Cheng. Label-
consistency based graph neural networks for semi-supervised node classification. In SIGIR, 2020.
Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie
Jegelka. Representation learning on graphs with jumping knowledge networks. In ICML, pp.
5449-5458, 2018.
Gang Yang, Xiaofeng Zhang, and Yueping Li. Session-based recommendation with graph neural
networks for repeat consumption. In ICCPR, pp. 519-524, 2020a.
Zhen Yang, Ming Ding, Chang Zhou, Hongxia Yang, Jingren Zhou, and Jie Tang. Understanding
negative sampling in graph representation learning. In SIGKDD, pp. 1666-1676, 2020b.
David Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods. In ACL,
pp. 189-196, 1995.
Tong Zhao, Yozen Liu, Leonardo Neves, Oliver J. Woodford, Meng Jiang, and Neil Shah. Data
augmentation for graph neural networks. In AAAI, 2021.
Yan Zhou, Murat Kantarcioglu, and Bhavani M. Thuraisingham. Self-training with selection-by-
rejection. In ICDM, pp. 795-803, 2012.
11