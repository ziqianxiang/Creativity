Under review as a conference paper at ICLR 2022
Nonparametric Learning of Two-Layer ReLU
Residual Units
Anonymous authors
Paper under double-blind review
Ab stract
We describe an algorithm that learns two-layer residual units using rectified linear
unit (ReLU) activation: suppose the input x is from a distribution with support
space Rd and the ground-truth generative model is a residual unit of this type, given
byy = B* [(A*x)+ + x] , where ground-truth network parameters A* ∈ Rd×d
represent a nonnegative full-rank matrix and B* ∈ Rm×d is full-rank with m ≥ d
and for c ∈ Rd, [c+]i = max{0, ci}. We design layer-wise objectives as function-
als whose analytic minimizers express the exact ground-truth network in terms
of its parameters and nonlinearities. Following this objective landscape, learning
residual units from finite samples can be formulated using convex optimization
of a nonparametric function: for each layer, we first formulate the corresponding
empirical risk minimization (ERM) as a positive semi-definite quadratic program
(QP), then we show the solution space of the QP can be equivalently determined by
a set of linear inequalities, which can then be efficiently solved by linear program-
ming (LP). We further prove the statistical strong consistency of our algorithm, and
demonstrate its robustness and sample efficiency through experimental results.
1 Introduction
Neural networks have achieved remarkable success in various fields such as computer vision (LeCun
et al., 1998; Krizhevsky et al., 2012; He et al., 2016a) and natural language processing (Kim, 2014;
Sutskever et al., 2014). This success is largely due to the strong expressive power of neural networks
(Bengio & Delalleau, 2011), where nonlinear activation units, such as rectified linear units (ReLU)
(Nair & Hinton, 2010) and hyperbolic tangents (tanh) play a vital role to ensure the large learning
capacity of the networks (Maas et al., 2013). However, the nonlinearity of neural networks makes
them significantly more difficult to train than linear models (Livni et al., 2014). Therefore, with the
development of neural network applications, finding efficient algorithms with provable properties to
train such nontrivial neural networks has become an important and a relatively new goal.
Residual networks, or ResNets (He et al., 2016a), are a class of deep neural networks that
adopt skip connections to feed values between nonadjacent layers, where skipped layers
may contain nonlinearities in between. Without loss of expressivity, ResNets avoid the ∣	]
vanishing gradient problem by directly passing gradient information from previous layers	∣-A一∣
to current layers where otherwise gradients might vanish without skipping. In practice, ∣ ReLU ∣
ResNets have shown strong learning efficiency in several tasks, e.g. achieving at least	*~
93% test accuracy on CIFAR-10 classification, lowering single-crop error to 20.1% on	ɪ
the 1000-class ImageNet dataset (Russakovsky et al., 2015; He et al., 2016b).	I B I
Common ResNets are often aggregated by many repeated shallow networks, where each
network acts as a minimal unit with this kind of skip propagation, named a residual unit
(He et al., 2016b). Given the flexibility and simplicity of residual units, much theoretical work
has been devoted to study them and develop training algorithms for them in a way that sidesteps
from the standard backpropagation regime and provides guarantees on the quality of estimation (see
subsection 1.1). In this paper, we propose algorithms that can learn a general class of single-skip two-
layer residual units with ReLU activation as shown on the right by the equation: y = B
(Ax)+ +x ,
where for a scalar c, c+ = max{0, c} (for a vector, this maximization is applied coordinate-wise), x
1
Under review as a conference paper at ICLR 2022
is a random vector as the network input with support space Rd, and A ∈ Rd×d and B ∈ Rm×d are
weight matrices of layer 1 and layer 2, respectively.
Compared to previous work (Ge et al., 2018; 2017; Zhang et al., 2018; Wu et al., 2019; Tian, 2017;
Du et al., 2017; Brutzkus & Globerson, 2017; Soltanolkotabi, 2017; Li & Yuan, 2017; Zhong et al.,
2017), the introduction of residual connections simplifies the recovery of the network parameters by
removing the permutation and scaling invariance. However, the naive mean square error minimization
used in estimating the parameters remains nonconvex. Unlike most previous work, we do not assume
a specific input distribution nor that the distribution is symmetric (Ge et al., 2018; Du & Goel, 2018).
We show that under regularity conditions on the weights of the residual unit, the problem of learning
the unit can be formulated through quadratic programming (QP). We use nonparametric estima-
tion (Guntuboyina et al., 2018) to estimate the ReLU function values in the networks. We further
rewrite our constructed quadratic programs to linear programs (LPs). The LP formulation is simpler
to optimize and has the same solution space as the QP for the network parameters.
1.1	Related Work
Provable learning of neural networks has become an important topic of research. Arora et al. (2014)
recover a multi-layer generative network with sparse connections and Livni et al. (2014) study the
learning of multi-layer neural networks with polynomial activation. Goel et al. (2018) learns a
one-layer convolution network with a perceptron-like rule. They prove the correctness of an iterative
algorithm for exact recovery of the target network. Rabusseau et al. (2019) describe a spectral
algorithm for two-layer linear networks. Recent work has connected optimization and two-layer
network learning (Ergen & Pilanci, 2021; Sahiner et al., 2020; Ergen & Pilanci, 2020; Pilanci &
Ergen, 2020) and has showed how to optimize networks layer by layer (Belilovsky et al., 2019).
For learning a one-layer ReLU network, Wu et al. (2019) optimize the norm and direction of neural
network weight vectors separately, and Zhang et al. (2018) use gradient descent with a specific
initialization. For learning a two-layer ReLU network, Ge et al. (2017) redesign the optimization
landscape such that it is more amenable to theoretical analysis and Ge et al. (2018) use a moment-
based method for estimating a neural network Janzamin et al. (2015). Many others have studied ReLU
networks in various settings (Tian, 2017; Du et al., 2017; Brutzkus & Globerson, 2017; Soltanolkotabi,
2017; Li & Yuan, 2017; Zhong et al., 2017; Goel & Klivans, 2017).
The study of ReLU networks with two hidden layers has also been gaining attention (Goel & Klivans,
2017; Allen-Zhu et al., 2019), with a focus on PAC learning (Valiant, 1984). In relation to our work,
Allen-Zhu & Li (2019) examined the PAC-learnable function of a specific three-layer neural network
with residual connections. Their work differs from ours in two aspects. First, their learnable functions
include a smaller (in comparison to the student network) three-layer residual network. Second, the
assumptions they make on their three-layer model are rather different than ours.
In relation to nonparametric estimation, Guntuboyina et al. (2018) treat the final output of a shape-
restricted regressor as a parameter, placing some restrictions on the type of function that can be
estimated (such as convexity). They provide solutions for estimation with isotonic regression (Brunk,
1955; Ayer et al., 1955; van Eeden, 1956), convex regression (Seijo et al., 2011), shape-restricted
additive models (Meyer, 2013; Chen & Samworth, 2016) and shape-restricted single index mod-
els (Kakade et al., 2011; Kuchibhotla et al., 2017).
1.2	Main Results
We design quadratic objective functionals with a linear bounded feasible domain that take network
parameters and functions as variables to estimate the ground-truth network parameters and nonlinear-
ities. The values of the objectives are moments over the input distribution. Thm. 1.1 summarizes the
landscapes of the objectives.
Theorem 1.1	(objective landscape, informal). Suppose a ground-truth residual unit has nondegener-
ate weights in both layers and nonnegative weights in layer 1. Then there exist quadratic functionals
defined in linear-constrained domains whose minimizers a) are unique, and are the exact ground-truth,
or b) are not unique, but can be adjusted to the exact ground-truth.
2
Under review as a conference paper at ICLR 2022
In practice, the exact moments over unobserved variables are not available. We can only construct
the empirical risk minimization (ERM) of the moment-valued objectives by generated samples. With
functions as variables in moment-valued objectives being optimized nonparametrically, the empirical
objectives become quadratic functions with linear constraints (QP). We further show the convexity of
our QP which guarantees its solution in polynomial time w.r.t. sample size and dimension. With the
solution to the QP available, the strong consistency of our network learner is guaranteed:
Theorem 1.2	(strong consistency, informal). Suppose samples are generated by a ground-truth
residual unit that has nondegenerate weights in both layers and nonnegative weights in layer 1. Then
there exists an algorithm that learns a network -a-.→s. the exact ground-truth as sample size grows.
Roadmap: Assume A and B are student network weights of layer 1 and 2. We first give a warm-up
vanilla linear regression approach only knowing A is in Rd×d and B is in Rm×d (section 3). We
then move to the details of our nonparametric learning for layer 2 (section 4), and similarly, layer
1 (section 5). We formalize Thm. 1.1, showing how nonparametric learning allows us to select the
values of A and B from reduced spaces that are seeded by A* and B*, under which LR is faster
than vanilla LR. In section 6, we describe the strong consistency of our methods for respective layers,
formalizing Thm. 1.2 using the continuous mapping theorem (Mann & Wald, 1943).
2	Preliminaries
We describe the notations used in this paper, introduce the model and its underlying assumptions, and
state conditions which simplify the problem but which can be removed without loss of learnability.
Notation The ReLU residual units we use take a vector x ∈ Rd as input and return a vector
y ∈ Rm. We use A* ∈ Rd×d and B* ∈ Rm×d to denote the ground-truth network parameters for
layer 1 and 2, respectively. We use circumflex to denote predicted terms (e.g. an empirical objective
function f, estimated layer 1 weights A). We use n to denote the number of i.i.d. samples available
for the training algorithm, {x(i), y(i)} to denote the i-th sample drawn. For an integer k, We define
[k] to be {1, 2, . . . , k}, and e(j) as the standard basis vector with a 1 at position j. All scalar-based
operators are element wise in the case of vectors or matrices unless specified otherwise. We use x
and y to refer to the input and output vectors, respectively, as random variables.
Linear Regression: Linear regression (LR) in this paper refers to the problem of estimating L
for unbiased model y = Lx. In this case, estimation is done by minimizing the empirical risk
R (L) = 21n Pi∈[n] ∣∣Lx(i) - y(i) Il - using linear least squares (LLS). Its solution has a closed
form which we use as given. We refer the reader to Hamilton (1994) for more information.
Models, Assumptions and Conditions Following previous work (Livni et al., 2014), we assume a
given neural network structure specifies a hypothesis class that contains all the networks conforming
to this structure. Learning such a class means using training samples to find a set of weights such
that the neural networks predictions generalize well to unseen samples, where both the training
and the unseen samples are drawn from an unobserved ground-truth distribution. In this paper, the
hypothesis class is given by ReLU residual units and we assume it has sufficient expressive power to
fit the ground-truth model. More specifically, we discuss the realizable case of learning, in which the
ground-truth model is set to be a residual unit taken from the hypothesis class with the form:
y=B* h(A*x)+ + xi ,	(1)
and is used to draw samples for learning. Unlike other multi-layer ReLU-affine models which do not
apply skip connections, we cannot permute the weight matrices of the residual unit and retain the
same function because of skip-adding x (it breaks symmetry). This helps us circumvent issues of
identifiability1, and allows us to precisely estimate the ground-truth weight matrices A* and B*.
Our general approach for residual unit layer 2 learns a scaled ground-truth weight matrix that also
minimizes the layer 2 objective. The existence of such scaled equivalence of our layer 2 approach
comes from what is defined below.
1Here, we are referring to the ability to identify the true model parameters using infinite samples.
3
Under review as a conference paper at ICLR 2022
Definition 2.1 (component-wise scale transformation). A matrix A ∈ Rd×d is said to be a scale
transformation w.r.t. the j-th component if (Aj,： )> = Aj,j ∙ e(j).
Additionally, estimation is more complex when the layer 2 weights B* is nonsquare. For simplicity
of our algorithm presentation for layer 2, we stick to Cond. 2.1 in the following sections2.
Condition 2.1 (layer 2 objective minimizer unique). A* is not a scale transformation w.r.t. any
components and B* is a square matrix, i.e. m = d.
3	Warm-Up: Vanilla Linear Regression
Consider a ground-truth two-layer residual unit. If we assume that the inputs only contain vectors
with negative entries, i.e. x < 0, the effect of the ReLU function in the residual unit then disappears
because of the nonnegativity of A*. The residual unit turns into linear model y = B*x. Thus, direct
LR on samples with negative inputs can learn the exact ground-truth layer 2 parameter B * with at
least d samples, when formulating the LR as a solvable full-rank linear equation system.
On the contrary, if the inputs only contain vectors with positive entries, i.e. x > 0, all the neurons in
the hidden layer are then activated by the ReLU and the nonlinearity is eliminated. The residual unit
in this case turns into y = B* (A* + Id ) x. Taking the value that left-multiplies x as a single weight
matrix D*, this is also a linear model. Direct LR on at least d samples with positive inputs by the
residual unit can learn the exact D*. Since we have the access to the exact B*, solving for the exact
A* can be accomplished through solving a full-rank linear equation system B* ∙ A = D*, where the
unique solution A = A* + Id.
While simple, this vanilla LR approach requires a large number of redundant samples, since sampled
inputs usually have a small proportion of fully negative/positive vectors. Taking random input
vectors i.i.d. with respect to each entry as an example, the probability of sampling a vector with
all negative entries is pd- , where p- is the probability of sampling a negative vector entry, then
the expected number of samples to get one negative vector is 1/pd-. Denoting p+ similarly, 1/pd+
samples are expected for a positive vector. Besides, each LR step in this approach requires d such
samples respectively to make the linear equation system full-rank, which implies the expected sample
size to be d-exponential d ∙(1/p- + l/p；). For other common random vectors like Gaussian
samples, the proportions of fully negative/positive vectors in sampled inputs are also expected to
decrease exponentially as d grows, as high-dimensional random vectors like Gaussian are essentially
concentrated uniformly in a sphere (Johnstone, 2006). Technical and experimental details about
sample size expectations and the vanilla LR algorithm are further discussed in Appendix F.
4	Nonparametric Learning: Layer 2
We present how we learn a residual unit layer 2 under Cond. 2.1 (estimating B*): We first design an
objective functional with the arguments being a matrix and a function. The objective uses expectation
of a loss over the true distribution generating the data and is uniquely minimized by [B*]-1 and a
rectifier function (ReLU). We then formulate its ERM using nonparametric estimation as a standard
convex QP, further simplified as an LP that has the same capability as the QP to learn layer 2.
4.1	Objective Design and Landscape
Consider the formulation of a residual unit as in Eq. 1. It is possible to rewrite the model as equation:
C*y = (A*x)+ + x, where the output of the hidden neuron with skip addition is on both sides of
the equation, and C*B* = Id. We aim to estimate the inverse of B* by matrix variable C and the
nonlinearity x 7→ (A*x)+ by a function variable h. The objective is formulated as risk functional by
the L2 error between values respectively computed by C and h
G2 (C,h) = 2Ex [kh (x)+ X - Cyk2i,	⑵
2In Appendix G, we show that estimation of B* remains solvable without satisfying Cond. 2.1.
4
Under review as a conference paper at ICLR 2022
where the estimator C ∈ Rd×m , the domain of h is the nonnegative3 continuous4 Rd → Rd
function space, written as C0≥0 in shorthand. This objective is quadratic because the forward mapping
x 7→ h(x) + x and the backward mapping y 7→ Cy are both linear w.r.t. C and h, and the two are
linearly combined in a L2 norm. The objective in Eq. 2 is minimized by the ground-truth, i.e. C * and
X → (A*x)+, is one of its minimizers. However, it is not simple to describe other variable values
that minimize the objective if any exist. We give that detail under Cond. 2.1, the minimizer of G2 is
unique, as the exact ground-truth in the given domain.
Theorem 4.1 (objective minimizer, layer 2). Define G2(C, h) as Eq. 2, where C ∈ Rd×m, h ∈ C0≥0.
Then under Cond. 2.1, G2(C, h) reaches its zero minimum iff C = [B*]-1 and h : x 7→ (A*x)+.
Technical details are in Appendix H, where we use Lem. 4.2 (in subsection 4.3) to prove a more
general theorem which does not require Cond. 2.1 and is sufficient for Thm. 4.1. In the next subsection,
we construct the ERM of G2 and present our convex QP formulation.
4.2	ERM with Nonparametric Estimation is Convex QP
Consider the second layer objective (Eq. 2) with nonnegative continuous function space as the domain
of h. We follow Vapnik (1992) and define its standard empirical risk functional:
G2(c,h) = 2n X ∣∣h(χ(i)) + x(i) - Cy(i)∣∣2.	⑶
i∈[n]
The function variable h ∈ C0≥0 can be optimized either parametrically or nonparametrically. If we
parameterize h, and show that the nonlinearity w.r.t. its parameters would make G2 lose its quadratic
form. Instead, we estimate h nonparametrically: for each sample input x(i), we introduce variables
ξ(i) that estimates mapped values by h. This avoids introducing nonlinearity to the objective and
keeps G2 quadratic. On the other hand, the domain of h, i.e. nonnegative continuous function space,
turns into a set of linear inequalities as constraints when optimizing nonparametrically. In this sense,
learning the second layer of the residual unit can be formulated as the following QP:
miΞGNPE	(C,	Ξ)	:=2^	X ∣∣ξ⑴	+	X⑴-Cy⑴ ∣∣2, s.t. ξ⑺ ≥ 0, ∀i ∈	[n].	(4)
,	i∈[n]
where Ξ = {ξ(i)}in=1 is the nonparametric estimator of X 7→ (A*X)+.
Nonparametric Estimation Validation: A solution to the ERM with nonparametric estimation, i.e.
the QP, is guaranteed to be sufficient for minimizing the standard empirical risk functional (Eq. 3).
More specifically, assuming C and Ξ = {ξ(i)}in=1 are a solution to layer 2 QP (Eq. 4), it is clear that
C and h ∈ C0≥0 such that h(X(i)) = ξ(i) minimize the empirical risk functional Eq. 3. Conversely, a
minimizer of the standard empirical risk Eq. 3, C and h, corresponds to a solution to layer 2 QP as
we set ξ(i) = h(x(i)). Therefore, minimizing G and solving layer 2 QP are empirically equivalent.
Convexity: The convexity of the QP: Eq. 4 is also guaranteed. First, constraints are linear. Second,
for each sample with index i ∈ [n], the L2 norm wraps linearity w.r.t. C and ξ(i). Such formulation
ensures the quadratic coefficient matrix is positive semidefinite. Thus, with the sum of convex
functions still being convex, the QP objective (Eq. 4) becomes convex. Even without the knowledge
of how samples are generated, this QP would be a convex program. Strict proofs are in Appendix I.
4.3	LP Simplification
Consider single-sample error written as g2 (C, h; x, y) = 1 ∣∣h (x) + X - Cy∣∣2. If there is a
feasible C such that Cy - X ≥ 0 holds for all X ∈ Rd , then C and h : X 7→ Cy - X always
minimize g2 as zero, and thereby minimize the layer 2 objective (Eq. 2). Thus, we obtain a condition
that is equivalent to G2 reaching the minimum in Thm. 4.1 and avoids randomness (see Lem. 4.2).
3Setting h as nonnegative ensures that a) only ReLU nonlinearity minimizes G2 (see Thm. 4.1), and b) h’s
nonparametric estimator is linearly constrained (explained in subsection 4.2).
4If h is not a continuous function, only a null set of discontinuities is possible to make G2 reach zero as its
minimum. Setting h as continuous simplifies our theoretical results that still strictly support empirical discussion.
5
Under review as a conference paper at ICLR 2022
Algorithm 1 Learn a ReLU residual unit, layer 2.
1:	Input: {(x(i), y(i))}in=1, samples drawn by
Eq. 1.
+
2:	Output: B, ξ, a layer 2 and X → (A*x)
estimate.
3:	Go to line 4 if QP, line 5 if LP.
4:	Solve QP: Eq. 4 and obtain a GNPE minimizer,
denoted by C, Ξ. Go to line 6.
5:	Solve LP: Eq. 5 and obtain a minimizer C,
then assign ξ(i) — Cy⑴一X⑴.
6:	return C-1, Ξ.
Algorithm 2 Learn a ReLU residual unit, layer 1.
1:	Input: {(x(i), h(i))}n=ι, layer 1 samples.
2:	Output: A, a layer 1 estimate.
3:	Solve QP: Eq. 7 or LP: Eq. 8 and obtain a
G^NPE minimizer, denoted by A, Φ. {Φ is no
longer needed.}
4:	for all j ∈ [d] do
5:	M — LR{hji), Aj,:x(叫h(i)>0.
6:	Aj,： — Aj,∙Jkj. {Rescale A.}
7:	end for
8:	return A.
Lemma 4.2. G2 (C, h) reaches its zero minimum iff Cy - X ≥ 0 holds for any X ∈ Rd and its
corresponding residual unit output y, and h : X 7→ Cy - X.
The pointwise satisfaction of the inequality in Lem. 4.2 describes the solution space for the mini-
mization of G2 . The sufficiency of satisfying this inequality to minimize G2 is directly obtained
by assigning h : X 7→ Cy - X where C complies with Cy - X ≥ 0 for all X in the support space
Rd. The necessity of satisfying this inequality comes from its contraposition: If a C violates the
inequality, there must be a non-null set of X that yield the violation due to the continuity of Cy - X.
In this sense, the resulting G2 value becomes nonzero.
Empirically speaking, we cannot solve an inequality that holds w.r.t. to every point in the support
space if we only observe finite samples. We can only estimate C by solving the inequality that holds
w.r.t. each sample. Following this, we formulate such estimation as to find a feasible point in the space
defined by a set of linear inequalities, each of which corresponds to a sample: Cy(i) - X(i) ≥ 0. Each
point in the feasibility defined by the inequalities has a one-to-one correspondence in the solution
space to layer 2 QP (Eq. 4): C 什(C, {Cy(i) - x(i)}i∈[n]). The set of inequalities can be solved by
a standard LP with a constant objective and with constraints that are inequalities, i.e.
min const, s.t. Cy(i) - X(i) ≥ 0, ∀ i ∈ [n].	(5)
With the one-to-one correspondences, LP: Eq. 5 and QP: Eq. 4 have equivalent solution spaces.
Moreover, LP: Eq. 5 is also a convex program. Alg. 1 summarizes the layer 2 estimator: Simply solve
the QP/LP5 and return the inverse of C as the estimate of layer 2 weights and Ξ as x → (A*x)+
estimate. Regardless of time complexity, QP and LP in Alg. 1 work equivalently since their solution
spaces are equivalent to each other.
Our nonparametric learning directly finds a unique layer 2 estimate under Cond. 2.1. In the general
case without Cond. 2.1 (discussed in Appendix G), nonparametric learning essentially reduces the
possible values of B from Rm×d to a B* scale-equivalent matrix space, where LR uses sampled data
much more efficiently than vanilla LR on layer 2 (section 3).
5	Nonparametric Learning: Layer 1
With layer 2 learned, outputs by hidden neurons become observable. The two-layer problem is
thereby reduced to single-layer. Consider a ground-truth single-layer model: h = (A*x)+. To
construct a learning objective for this model, we rewrite the model as a nonlinearity plus a linear
mapping by A*: h = (-A*x)+ + A*x, where on both sides of the equation is the output of layer 1,
and the nonlinearity is X 7→ (-A*X)+. The objective of layer 1 is formulated as:
Gi (A,r) = 2Exhkr(x) + AX - h∣∣1 ,	(6)
5We use CVX (Grant & Boyd, 2014; 2008) that calls SDPT3 (Toh et al., 1999) (a free solver under GPLv3
license) and solves our convex QP/LP in polynomial time. See Appendix C for technical details.
6
Under review as a conference paper at ICLR 2022
where A ∈ Rd×d is of the layer 1 weights estimator, the domain of r is also C0≥0. The minimizer A
of the risk G1 falls into a matrix space such that for any matrix A in the space, each row of A is a
scaled-down version of the same row of A* without changing the direction (Thm. 5.1).
Theorem 5.1 (objective minimizer space, layer 1). Define G1(A, r) as Eq. 6, where A ∈ Rd×d,
r ∈ C0≥0. Then G1(A, r) reaches its zero minimum iff for each j ∈ [d], Aj,: = kj Aj*,: where
0 ≤ kj ≤ 1, and r : x → (A*x)+ — diag(k) ∙ A*x.
The scale equivalence in the solution space is derived from a ReLU inequality: (x)+ ≥ kx where
0 ≤ k ≤ 1. Let the j-th row of A be a scaled-down version of A*, i.e. Aj,: = kj Aj*,: where
0 ≤ kj ≤ 1. According to the inequality, we have(A*,：x)+ ≥ kjA*,：x, which indicates (A*x)+ ≥
diag(k) ∙ A*x. Thus, r minimizing Gi in Thm. 5.1 lies in feasibility C≥° when A = diag(k) ∙ A*.
Due to the existence of scale equivalence, we must compute the scale factor to obtain the ground-truth
weights A*. The scale factor kj is sufficiently obtainable with (Aj-,：x)+ and(A*,：x)+ observable:
Conditioned on nonnegative ReLU input, we have a linear model Aj,:x = kjAj*,:x where Aj,:x
and Aj*,:x are observed. Thm. 5.2 summarizes layer 1 scale factor property, allowing us to correct a
minimizer of G1 to the ground-truth weights A* by computing a scalar for each row.
Theorem 5.2 (scale factor, layer 1). Assume A is a minimizer of G1. Then for any j ∈ [d],
(Aj；x)+ /(A;,：x)+ is always equal to the scale factor kj given that (Aj,：x)+ > 0.
The elimination of randomness for G1 follows the same pattern as layer 2. If there is a feasible A
such that (A*x)+ — Ax ≥ 0 holds for all x ∈ Rd, such A is a solution to our objective by Eq. 6.
We can also have the following proposition that avoids randomness.
Lemma 5.3. G1(A, r) reaches its zero minimum iff h — Ax ≥ 0 holds for any x ∈ Rd and its
corresponding hidden output h, and r : x 7→ h — Ax.
We now turn into empirical discussion. Similar to layer 2, we formulate layer 1 QP by its empirical
objective with nonparametric estimation and linear constraints representing the nonnegativity ofr:
minGNPE (A, Φ) := ɪ X ∣∣φ(i) + Ax(i) — h(i)∣∣2 , s.t. φ(i) ≥ 0, ∀i ∈ [n].	(7)
A, Φ	2n i∈[n]
where Φ = {φ(i)}n=ι is the function estimator of x → (A*x)+ — diag(k) ∙ A*X where k refers
to the scaling equivalence. Similarly, the solution space of QP: Eq. 7 can be represented by a set of
linear inequalities as constraints of the following efficiently solvable LP
min const, s.t. h(i) — Ax(i) ≥ 0, ∀ i ∈ [n].	(8)
Alg. 2 describes how a layer 1 is learned: First, a Gi minimizer estimate A is obtained by solving
the QP/LP. Then for the j-th row, the scale factor kj is estimated by running LR on hji) and Aj,：x(i)
s.t. hji) > 0 to correct A, as Aj；X = kjhj is an unbiased linear model given that hj > 0. By
nonparametric learning, the value space of A is reduced from Rd×d to {diag(k) ∙ A* | 0 ≤ kj ≤ 1},
where LR uses sampled data much more efficiently than vanilla LR layer 1 (section 3).
6	Full Algorithm and Analysis
The full algorithm concatenates Alg. 1 and 2 in a layerwise fashion, with observations of input/output
by the ground-truth network: a) Estimates layer 2 and nonlinearity: x 7→ (A*x)+ by Alg. 1.
b) Estimates layer 1 by running Alg. 2 on input samples and the nonlinearity estimate. It has provable
guarantees. For empirical analysis, we use Cn to denote the estimation of C* from n random samples.
Similar notations are applied to other estimations. First of all, our methods to solve respective layers,
Alg. 1 and 2, are strongly consistent if any convex QPs/LPs involved can be solved exactly.
Lemma 6.1 (layer 2 strong consistency). Under Cond. 2.1, C n -→ C * and B n —→ B*, n → ∞.
For Cn,: In Appendix J, we prove its more general a.s. convergence without satisfying Cond. 2.1,
where the solution space to layer 2 objective is a non-compact continuous set where all the elements
7
Under review as a conference paper at ICLR 2022
are scaling equivalences. We use Hausdorff distance (Rockafellar & Wets, 2009) as metric and prove
that the empirical solution space a.s. converges to the theoretical solution space. Then the more
general a.s. convergence holds with the strong consistency of layer 2 scale factor estimator where we
use LR to estimate the scale factors.
For Bn： Using the continuous mapping theorem (Mann & Wald, 1943), We directly propagate the
strong consistency of C * estimator to its inverse B*,s estimator.
According to full algorithm description, layer 1 estimation uses Cny(i) - x(i) as the outputs where
i ∈ [n]. Thus, the strong consistency of the hidden neuron estimator is also guaranteed by the
continuous mapping theorem. Following the proof sketch of the Cn a.s. convergence, we obtain the
strong consistency of layer 1 estimator (See Lem. 6.2).
Lemma 6.2 (layer 1 strong consistency). An -→ A*, n → ∞.
By the continuous mapping theorem, the strong consistency of the full algorithm (Thm. 6.3, formal
version of Thm. 1.2), which is commonly defined by a loss function that is continuous on network
weights, is implied by the strong consistency of network weights estimators (Lem. 6.1 and 6.2). See
Appendix J for proofs of strong consistency discussions in this section.
Theorem 6.3 (strong consistency, formal). Define L as L2 output loss. L(An Bn -→ 0, n → ∞.
7	Experiments
We provide experimental analysis to demonstrate the effectiveness and robustness of our approach
in comparison to stochastic gradient descent (SGD) on L2 output loss: L(A, B) = 1 Ex ∣∣y - y『,
where we parameterize the output prediction by y = B [(Ax)+ + x]. Our proposed methods
outperform SGD in terms of sample efficiency and robustness to different network weights and noise
strengths, which indicates a poor optimization landscape of L2 output loss for ReLU residual units.
Setup: The ground-truth weights are generated through i.i.d. folded standard Gaussian6 and standard
Gaussian for layer 1 and 2 respectively, i.e. A* i%' |N| (0,1), B* i削. N(0,1). The input distribution
is set to be an i.i.d. zero mean Gaussian-uniform equal mixture N (-0.1,1)- U (-0.9,1.1). SGD
is conducted on mini-batch empirical losses of L(A, B) with batch size 32 for 256 epochs in
each learning trial. We apply time-based learning rate decay η = η0/ (1 + Y ∙ T) with initial rate
η0 = 10-3 and decay rate γ = 10-5, where T is the epoch number. The above hyperparameters are
tuned to outperform other hyperparameters in learning ReLU residual units in terms of output errors.
Evaluation: We use relative errors to measure the accuracy of our vector/matrix estimates: For a
network with weights A and B and its teacher network with weights A* and B * , a) layer 1 error
refers to ∣∣A - A*∣ / ∣∣A*∣, similar to layer 2. b) output error refers to E [∣y - yk / ky∣] by test
data. Due to the equivalence between the solution spaces of our QP and LP without label noise, we
choose LP in noiseless experiments, referred to as “ours”. In addition, to reduce variance, the results
of learning the same ground-truths are computed as means across 16 trials.
Sample Efficiency: Consider Fig. 1. It depicts the variation in the prediction errors (warmer color,
larger error) as a function of d (input dimension) and the number of samples the learning algorithm is
using. We compare SGD against our algorithm. We observe that our approach to the estimating the
neural network is more sample efficient. For SGD, the estimation is relatively easy with only up to
10 dimensions. As expected, once the dimension grows, the sample size required for the same level
of error as our method is larger. Still, overall, our method is capable of learning robustly with small
sample sizes and more efficiently than SGD even for larger sample sizes.
Network Weight Robustness: This experiment aims to verify whether our method can learn a
broader class of residual units. In Tab. 1, our method shows a light-tailed distribution with nearly
zero means and standard deviations for layers 1 and 2, and output errors across various ground-truth
networks, whereas SGD is less robust in the same context. Our method shows strong robustness to
network weight changes, indicating its applicability across the whole hypothesis class.
6A folded Gaussian is the absolute value of a Gaussian, with p.d.f. p(|x|) where X 〜N, denoted as |N|. We
use folded Gaussian to ensure layer 1 weights are nonnegative.
8
Under review as a conference paper at ICLR 2022
160 192 224 256 288 320 352 384
Sample Size
Figure 1: Output errors by SGD and our
method for different dimensions and sample
Table 1: Means / Standard deviations of net-
work estimate errors for different network
weights. Values are computed from the process
of learning 128 different ground-truth networks
with d = 16. 512 training samples are drawn
for each learning trial.
Layer 1
Layer 2
Output
Mean Std Mean Std Mean Std
Noise Strength
Noise Strength
Noise Strength
Figure 2: Respective errors of layers 1 and 2 and outputs of different label noise strengths for
SGD, LP and QP. We fix the ground-truth weights with d = 10 and only the noise strength varies.
512 training samples are drawn for each learning trial.
Noise Robustness: Fig. 2 confirms the robustness of our methods when output noise exists. Samples
are generated by a ground-truth residual unit with output noise being i.i.d. zero-mean Gaussian in
different strengths (i.e. standard deviations). We try both QP and LP because in noisy setting the two
approaches are not equivalent w.r.t. the solution space7. First, SGD always gives larger errors than
our methods, even though it is hardly affected by tuning the noise strength. For QP/LP, all the errors
for layer 1, 2 and output grow almost linearly as noise strength increases, indicating that both QP/LP
learn the optima robustly when output noise is present, where QP slightly outperforms LP.
8 Conclusion
In this paper, we address the problem of learning a general class of two-layer residual units and
propose an algorithm based on landscape design and convex optimization: We demonstrate firstly
that minimizers of our objective functionals can express the exact ground-truth network. Then, we
show that the corresponding ERM with nonparametric function estimation can be solved using convex
QP/LP, which indicates polynomial-time solvability w.r.t. sample size and dimension. Moreover, our
algorithms that are used to estimate both layers as well as the whole networks are strongly consistent,
with very weak conditions on input distributions.
Our work opens the door to a variety of open problems to explore. We provide a strong consis-
tency result without assuming a particular input distribution. It would be interesting to explore the
sample complexity of our algorithm when stronger assumptions are placed on input distribution.
Another extension could be to dispose of the limitations on ground-truth weights, e.g. m ≥ d,
non-degeneration, nonnegativity of A*. In addition, given that our algorithm solves ReLU by using
nonparametric estimation through convex optimization, this might provide an inspiration to solving
learning problems with other nonlinearities using nonparametric methods that improve or change the
optimization landscape.
7See Appendix B for noisy model discussion.
9
Under review as a conference paper at ICLR 2022
Reproducibility Statement
The full set of assumptions of all theoretical results in this paper are stated - section 2, Appendix F
and G. The complete proofs of all theoretical results are all included - Appendix F, G, H, I and J.
Code in MATLAB is zipped as supplementary file. The numerical simulation details, including data
sizes and splits, hyperparameters and how they were chosen, clear definitions of the specific measures
or statistics used to report results, are discussed in detail - the Setup and Evaluation paragraphs
in section 7. Results with central tendency (e.g. mean) and variation (e.g. stddev) are reported -
Tab. 1. The use of external computing infrastructure, CVX, is clarified, and the consent obtaining and
licensing of CVX and SDPT3 solver used in this work exactly follows the original author instructions:
http://cvxr.com/cvx/doc/citing.html - footnote 5.
References
Zeyuan Allen-Zhu and Yuanzhi Li. What can ResNet learn efficiently, going beyond kernels? In
Advances in Neural Information Processing Systems,pp. 9015-9025, 2019.
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized
neural networks, going beyond two layers. In Advances in neural information processing systems,
pp. 6155-6166, 2019.
Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning some deep
representations. In International Conference on Machine Learning, pp. 584-592, 2014.
Miriam Ayer, H Daniel Brunk, George M Ewing, William T Reid, and Edward Silverman. An empir-
ical distribution function for sampling with incomplete information. The annals of mathematical
statistics, pp. 641-647, 1955.
Eugene Belilovsky, Michael Eickenberg, and Edouard Oyallon. Greedy layerwise learning can scale
to imagenet. In International conference on machine learning, pp. 583-593. PMLR, 2019.
Yoshua Bengio and Olivier Delalleau. On the expressive power of deep architectures. In International
Conference on Algorithmic Learning Theory, pp. 18-36. Springer, 2011.
M Emile Borel. Les Probabilites d6nombrables et leurs applications arithm6tiques. Rendiconti del
Circolo Matematico di Palermo (1884-1940), 27(1):247-271, 1909.
Hugh D Brunk. Maximum likelihood estimates of monotone parameters. The Annals of Mathematical
Statistics, pp. 607-616, 1955.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a ConvNet with Gaussian
inputs. ArXiv, abs/1702.07966, 2017.
Yining Chen and Richard J Samworth. Generalized additive and index models with shape constraints.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 78(4):729-754, 2016.
Simon S. Du and Surbhi Goel. Improved learning of one-hidden-layer convolutional neural networks
with overlaps. ArXiv, abs/1805.07798, 2018.
Simon S. Du, Jason D. Lee, Yuandong Tian, Barnabds P6czos, and Amrendra Kumar Singh. Gra-
dient descent learns one-hidden-layer cnn: Don’t be afraid of spurious local minima. ArXiv,
abs/1712.00779, 2017.
Tolga Ergen and Mert Pilanci. Revealing the structure of deep neural networks via convex duality.
arXiv preprint arXiv:2002.09773, 2020.
Tolga Ergen and Mert Pilanci. Implicit convex regularizers of cnn architectures: Convex optimization
of two-and three-layer networks in polynomial time. In International Conference on Learning
Representations (ICLR), 2021.
J Farkas. Ober die theorie der einfachen ungleichungen. J. Reine Angew. Math, 124:1-24, 1902.
Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape
design. arXiv preprint arXiv:1711.00501, 2017.
10
Under review as a conference paper at ICLR 2022
Rong Ge, Rohith Kuditipudi, Zhize Li, and Xiang Wang. Learning two-layer neural networks with
symmetric inputs. arXiv preprint arXiv:1810.06793, 2018.
Surbhi Goel and Adam R. Klivans. Learning neural networks with two nonlinear layers in polynomial
time. In COLT, 2017.
Surbhi Goel, Adam R. Klivans, and Raghu Meka. Learning one convolutional layer with overlapping
patches. ArXiv, abs/1802.02547, 2018.
Michael Grant and Stephen Boyd. Graph implementations for nonsmooth convex programs. In
V. Blondel, S. Boyd, and H. Kimura (eds.), Recent Advances in Learning and Control, Lecture
Notes in Control and Information Sciences, pp. 95-110. SPringer-Verlag Limited, 2008. http:
//stanford.edu/~boyd/graph_dcp.html.
Michael Grant and Stephen Boyd. CVX: Matlab software for disciplined convex programming,
version 2.1. http://cvxr.com/cvx, March 2014.
Adityanand Guntuboyina, Bodhisattva Sen, et al. Nonparametric shape-restricted regression. Statisti-
cal Science, 33(4):568-594, 2018.
James Douglas Hamilton. Time series analysis, volume 2. Princeton university press Princeton, NJ,
1994.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pp. 630-645. Springer, 2016b.
Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity:
Guaranteed training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473,
2015.
Florian Jarre. On the convergence of the method of analytic centers when applied to convex quadratic
programs. Mathematical Programming, 49(1-3):341-358, 1990.
Robert I Jennrich. Asymptotic properties of non-linear least squares estimators. The Annals of
Mathematical Statistics, 40(2):633-643, 1969.
Iain M Johnstone. High dimensional statistical inference and random matrices. arXiv preprint
math/0611589, 2006.
Sham M. Kakade, Adam Tauman Kalai, Varun Kanade, and Ohad Shamir. Efficient learning of
generalized linear and single index models with isotonic regression. In NIPS, 2011.
Narendra Karmarkar. A new polynomial-time algorithm for linear programming. In Proceedings of
the sixteenth annual ACM symposium on Theory of computing, pp. 302-311, 1984.
Yoon Kim. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882,
2014.
Mikhail K Kozlov, Sergei P Tarasov, and Leonid G Khachiyan. The polynomial solvability of convex
quadratic programming. USSR Computational Mathematics and Mathematical Physics, 20(5):
223-228, 1980.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
tional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Arun K Kuchibhotla, Rohit K Patra, and Bodhisattva Sen. Efficient estimation in convex single index
models. Preprint. Available at, 2017.
Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
11
Under review as a conference paper at ICLR 2022
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with ReLU activation.
ArXiv, abs/1705.09886, 2017.
Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural
networks. In Advances in neural information processing Systems, pp. 855-863, 2014.
David G Luenberger. Optimization by vector space methods. John Wiley & Sons, 1997.
Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural
network acoustic models. In ICML Workshop on Deep Learning for Audio, Speech and Language
Processing, 2013.
Henry B Mann and Abraham Wald. On stochastic limit and order relationships. The Annals of
Mathematical Statistics, 14(3):217-226, 1943.
Mary C. Meyer. A simple new algorithm for quadratic programming with applications in statistics.
Communications in Statistics - Simulation and Computation, 42:1126-1139, 2013.
Renato DC Monteiro and Ilan Adler. Interior path following primal-dual algorithms. part ii: Convex
quadratic programming. Mathematical Programming, 44(1-3):43-66, 1989.
Katta G Murty. Computational complexity of parametric linear programming. Mathematical
programming, 19(1):213-219, 1980.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In
Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807-814,
2010.
Mert Pilanci and Tolga Ergen. Neural networks are convex regularizers: Exact polynomial-time
convex optimization formulations for two-layer networks. In International Conference on Machine
Learning, pp. 7695-7705. PMLR, 2020.
Guillaume Rabusseau, Tianyu Li, and Doina Precup. Connecting weighted automata and recurrent
neural networks through spectral learning. In The 22nd International Conference on Artificial
Intelligence and Statistics, pp. 1630-1639. PMLR, 2019.
R Tyrrell Rockafellar and Roger J-B Wets. Variational analysis, volume 317. Springer Science &
Business Media, 2009.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition
challenge. International journal of computer vision, 115(3):211-252, 2015.
Arda Sahiner, Tolga Ergen, John Pauly, and Mert Pilanci. Vector-output ReLU neural network
problems are copositive programs: Convex analysis of two layer networks and polynomial-time
algorithms. arXiv preprint arXiv:2012.13329, 2020.
Emilio Seijo, Bodhisattva Sen, et al. Nonparametric least squares estimation of a multivariate convex
regression function. The Annals of Statistics, 39(3):1633-1657, 2011.
Pranab K Sen and Julio M Singer. Large sample methods in statistics: an introduction with applica-
tions, volume 25. CRC press, 1994.
Alexander Shapiro, Darinka Dentcheva, and Andrzej RUszczynski. Lectures on stochastic Program-
ming: modeling and theory. SIAM, 2014.
Mahdi Soltanolkotabi. Learning ReLUs via gradient descent. In NIPS, 2017.
Ilya SUtskever, Oriol Vinyals, and QUoc V Le. SeqUence to seqUence learning with neUral networks.
In Advances in neural information processing systems, pp. 3104-3112, 2014.
YUandong Tian. An analytical formUla of popUlation gradient for two-layered ReLU network and its
applications in convergence and critical point analysis. In ICML, 2017.
12
Under review as a conference paper at ICLR 2022
Kim-ChUan Toh, Michael J Todd, and Reha H Tutuncu. Sdpt3—a matlab software package for
Semidefinite programming, version 1.3. Optimization methods and software, 11(1-4):545-581,
1999.
Leslie G. Valiant. A theory of the learnable. Commun. ACM, 27:1134-1142, 1984.
Constance van Eeden. MaximUm likelihood estimation of ordered probabilities, 2. Stichting
Mathematisch Centrum. Statistische Afdeling, (S 196/56), 1956.
Vladimir Vapnik. Principles of risk minimization for learning theory. In Advances in neural
information processing systems, pp. 831-838, 1992.
Shanshan WU, Alexandros G. Dimakis, and SUjay Sanghavi. Learning distribUtions generated by
one-layer ReLU networks. In NeurIPS, 2019.
YinyU Ye and Edison Tse. An extension of karmarkar’s projective algorithm for convex qUadratic
programming. Mathematical programming, 44(1-3):157-179, 1989.
Xiao Zhang, Yaodong YU, Lingxiao Wang, and QUanqUan GU. Learning one-hidden-layer ReLU
networks via gradient descent. ArXiv, abs/1806.07808, 2018.
Yin Zhang, Richard A Tapia, and John E Dennis, Jr. On the sUperlinear and qUadratic convergence of
primal-dUal interior point linear programming algorithms. SIAM Journal on Optimization, 2(2):
304-324, 1992.
Kai Zhong, Zhao Song, Prateek Jain, Peter L. Bartlett, and Inderjit S. Dhillon. Recovery gUarantees
for one-hidden-layer neUral networks. In ICML, 2017.
Appendices
We inclUde an oUtline of the appendices:
•	Appendix A: An empirical analysis of the rUnning time of oUr algorithm and an analysis of
the effect of condition nUmber on the estimation accUracy.
•	Appendix B: A brief discUssion of oUr learning algorithms in a noisy context, referring to
the LP slack variable techniqUe that is Used in oUr experiments with noise in the main paper.
Also, this section provides an insight to potential fUtUre direction of this work.
•	Appendix C: A detailed explanation of the compUtational complexity and the methods
throUgh which the QPs/LPs in this paper are solved.
•	Appendix D: A preliminarily proposed general optimization formUla for the extension of
learning mUlti-layer ResNets, to show the scalability of oUr algorithm.
•	Appendix E: A jUstification of how oUr QP/LP approaches get rid of the exponentiality that
vanilla LR approach (section 3) has.
•	Appendix F: A formal resUlt and an empirical validation of the example given in the main
paper in the context of the vanilla linear regression method (section 3).
•	Appendix G: A generalization of layer 2 learning to the case where Cond. 2.1 is not satisfied.
•	Appendix H: Proofs regarding the minimizers of the objective fUnctions we Use.
•	Appendix I: Proofs that jUstify the convexity of oUr QPs.
•	Appendix J: Proofs that show oUr estimation algorithm is strongly consistent.
A Running Time and Sensitivity to Condition Number
Following the setUp in section 7, we report additional experiments testing the rUnning time of
our algorithm in comparison to SGD and the feasibility of estimating the inverse of B* when
ill-conditioned.
13
Under review as a conference paper at ICLR 2022
d = 16
(OIbooɪ.s) Jot 闰号⅛no
------SGD-GTI
------SGD-GT2
SGD-GT.
3
------SGD-GT4
SGD-GT5
0.5	1	1.5	2	2.5
Running Time (s)
LP-GTI
+ LP-GT2
« LP-GT3
•	LP-GT4
LP-GT5
-GT-GT-GT-GT-GT
DDDDD
SGSGSGSGSG
d = 32

Figure 3: Output errors against running time by SGD and our LP algorithm for different
dimensions of networks. For each dimension size of d = 8, d = 16 and d = 32, we report the
learning curves of SGD and the final output error of our LP algorithm against time used on 5 different
ground-truth networks. We used 512 training samples, drawn for learning each ground-truth network.
(CPU specification: 2.8 GHz Quad-Core Intel Core i7.)
o d = 8
d = 16
d = 32
11
10
4
o d = 8
-→-d = 16
d = 32
9 8 7 6 5
JOJJaN jəXda
2
100	101
102	103	104	105
MB*)
100	101	102	103	104	105
κ(B*)
Figure 4: Layer 2 errors against layer 2 ground-truth weight condition numbers for different
dimensions of networks by SGD and our LP algorithm. Each data point is the mean across
learning 32 different ground-truth networks with the same pair of d and a condition number for B *.
The condition number is denoted by K(B*). We used 512 training samples, drawn for learning each
ground-truth network.
A.1 Running Time Efficiency versus SGD
In Fig. 3, we compare the running time of our algorithm against the running time of SGD for
ground-truth networks sampled at random. The running time of our algorithm is significantly lower,
with an error level which is also significantly lower. Furthermore, this is demonstrated across input
dimensions, where the difference between the running time of SGD until convergence and our
algorithm’s running time increases as d increases. In addition, the running time of our algorithm is
fixed, and does not depend on the ground-truth network, while the running time of SGD varies based
on the network sampled.
A.2 Layer 2 Weights Condition Number Robustness
Our algorithm depends on a matrix-inversion step to obtain the estimated parameters. This affects the
estimation of B*. Due to this, we further investigate the effect of the condition number of B* in the
ground-truth parameters on the accuracy of estimation. For this experiment, to obtain ground-truth
network parameters with different condition number, we follow the procedure of Ge et al. (2018),
and multiply a diagonal matrix with exponentially-dropping values on the diagonal (λ-i for the i-th
element) by two random orthonormal matrices, U and V>, on the left and on the right respectively:
B = U diag (λT,...,λ-d )V>.
14
Under review as a conference paper at ICLR 2022
In Fig. 4, We compare the effect of the condition number of B* on SGD and on our algorithm.
SGD turns out to be quite sensitive to the condition number, especially for lower dimensions. Our
algorithm, on the other hand, is almost not affected by the condition number, With a final estimation
error consistently close to 0, recovering the matrix B*.
B Discussion: Noisy Model
Here, We discuss our learning methods in the noisy case. We introduce output noise to our model,
namely
y= B* h(A*x)+ +xi +z,	(9)
Where the label noise z ∈ Rd is an i.i.d. random vector With respect to each component, satisfying
E [z] = 0. In addition z and x are statistically independent.
Taking layer 2 as an example, our original objective functional does not reach zero by substituting
the ground-truth: G2(C*, x 7→ (A*x)+) = Ez[kC*zk2] = σ2 Tr C*C*> Where σ is the noise
strength. HoWever, if layer 2 is Well conditioned, the ground-truth Will still assign a value close
to zero to the objective. In this sense, With G2 ’s continuity, the ground-truth can approximately
minimize G2 , Which validates our QP approach in terms of learning noisy residual units.
Our original LP (Eq. 5) fails to give feasible solutions due to possible violation of the inequality
C*y-x ≥ 0, since C*y - x = (A*x)+ + C*z is not necessarily an entryWise nonnegative vector
because the term C*z might have negative entries. So We introduce slack variables ζ(i) to soften the
constraints:
min 1 X 1> ∙ Z⑺，	(10)
C,Z n
i∈[n]
s.t. Cy(i) - x(i) ≥ -ζ(i), ζ(i) ≥ 0.	(11)
For a sample (x(i), y(i)) with noise z(i), if (A*x(i))+ + C*z(i) < 0, then its L1 norm would be
added to the objective. This method remedies violations to the inequality C*y - x ≥ 0. With access
to a sufficiently large sample and with the “stability” assumption, our solution C would be close to
C * since large deviations seldom rise and their penalties are diluted in the objective.
C Discussion: Solving Convex Programs
The theoretical foundation of solving convex QP and LP has been driven to maturity in terms of
computational complexity (Kozlov et al., 1980; Murty, 1980) and convergence analysis (Jarre, 1990;
Zhang et al., 1992). The time complexity for a convex QP/LP is analyzed in terms of the number
of scalar variables N and the number of bits L in the input (Ye & Tse, 1989). For example, under
Cond. 2.1, N = d2 + nd for the QP because the matrix variables have d2 scalars and the function
estimators have nd scalars. Similarly we have N = d2 for the noiseless LP and N = d2 + nd for the
noisy LPIt is guaranteed that primal-dual interior point methods can solve the convex QP/LP in a
polynomial number of iterations O( VNL), where each iteration costs at worst O(N2.5) arithmetic
operations from the Cholesky decomposition for the needed matrix inversion (Monteiro & Adler,
1989; Karmarkar, 1984). This indicates that our QPs/LPs are guaranteed to be solvable in O(N 3L)
arithmetic operations, which is generally an O(poly(n, m, d, L)) complexity.
To solve convex QPs/LPs in this paper, we use CVX, a commonly used package for specifying and
solving convex programs (Grant & Boyd, 2014; 2008). For both QPs and LPs, CVX calls a solver,
SDPT3 (Toh et al., 1999), which is specified for semidefinite-quadratic-linear programming and
applies interior-point methods with the computational complexity mentioned above. Experimentally,
SDPT3 indeed specifies and solves our programs fast and makes our numerical results robust and
stable.
15
Under review as a conference paper at ICLR 2022
D	Discussion: Extension to Multi-Layer
In this appendix, we propose a generalized optimization formula of nonparametric learning of multi-
layer residual networks to show that our algorithm is scalable. Assume we have an L-layer network
of the form
r0 = x
ri = {Wl-^rι-ι)+ + rι-ι, l =1,...,L - 1
y = W1-irL-i.
We can scale the objective to use nonparameter variables {ξl(i) }l∈[L-1] to estimate every ReLU
activation (Wj rι)+ in this network. Generally, for the l-th ReLU We can have the outer layer
objective
min Mi) + …+ ξ(i) + x(i) — Vh(i) ∣∣, s∙t∙ ξ(? ≥ 0
ξ[(li]),Vl
where hat denotes estimated values and hL-1 = y(i), and its corresponding inner layer objective
min ∣φ(i) + W1-1 (ξ(-)ι + …+ ξ(i) + x(i)) — ξ(i) ∣∣ , s.t. φ(i) ≥ 0, ξ(i) ≥ 0.
φl(i),Wl-1,ξl(i)
We can apply techniques, e.g., tune the combinations of those objectives with respect to layers or
whether to have a hat or not, or even get them weighted, to keep its convex quadratic form and at the
same time improve its optimization landscape. Specifically when L = 2, this becomes the two-layer
ResNet learning discussed in the main paper.
E S ample Efficiency: Vanilla LR versus Ours
In this appendix, we describe an example to justify how our core QP/LP approaches eliminate
the exponential sample efficiency compared to the vanilla LR approach. In layer 2 learning, C’s
optimization is to find a feasible point in the space determined by n inequalities each of which
corresponds to a sample. Taking the j-th row of the inequality, i.e. Cj,:y ≥ xj. Every time we get a
new sample x(i), y(i) in,
•	The inequality Cj,:y(i) ≥ x(ji) eliminates the solution space of Cj,: by one of the spaces
divided by plane Cj,:y(i) = x(ji). This property guarantees fast convergence speed at early
phase since Cj,:’s feasibility starts from Rd.
•	In fact, when ReLU does not activate A*x(i) at the j-th row, i.e. A*x(i) ≤ 0, the theoretical
solution of Cj,： that Cj,： B* = (0,..., 0,1,0,..., 0) (a 1 at index j) directly lies on the
divisive plane Cj,:y(i) = x(ji):
Cj,:y(i) = Cj,:B*	A*x(i)+ + x(i) = x(ji).
This property remarkably speeds up further constraints on the solution space of Cj,: to the
correct estimate and is with high probability, since it directly depends on the sign of A*x(i).
With the vanilla LR approach mentioned in section 3 all the dimensions need to have the correct
sign at the same time. This is the reason for the exponential complexity of this approach. Our main
algorithm only requires one dimension to get the correct sign for a single sample, which avoids the
exponential sample size expectation. The convergence speed is determined by the actual probability
of each dimension not getting activated by ReLU, and such probability is determined by specified
input distribution.
16
Under review as a conference paper at ICLR 2022
Algorithm 3 Learn a ReLU residual unit by LR.
1:	Input: {(x(i), y(i))}in=1, n samples drawn by Eq. 1.
2:	Output: A, B, estimated weight matrices.
3:	B J LR{(x, y) ∈{(x⑴,y⑶)}b=12c | X < 0}.
4:	DD — LR{(x, y) ∈ {(x(i), y(i))}n=[n∕2c + ι | X > 0}. {theestimationof B* (A* + Id)}
5:	Solve full-rank linear equation system B ∙ A = D
6:	return A - Id, B.
F Vanilla Linear Regression: More Details
Alg. 3 gives the full list of steps of learning two-layer residual units by LR, where we first split the
drawn samples into two halves for the respective two key steps, then for both halves we filter negative
and positive vectors, respectively. By running LR on both filtered sets we obtain an estimation of the
ground-truth network.
In the following sections, we formalize this intuition and describe the exponential sample complexity
of this approach under entry-wise i.i.d. setting as an example of the inefficiency of this method.
Theorem F.1 (exponential sample complexity, vanilla LR). Assume the input vectors are i.i.d. with
respect to each component. Then Alg. 3 learns a neural network from a ground-truth residual unit
with at least O (d ∙ 2d+1) expected number ofsamples.
Proof. For each j ∈ [d], let Pj be the marginal probability of xj being positive, i.e. Pj = P (xj >
0) > 0. Thus, the probability that a sample is positive P (x > 0) = Qj∈[d] Pj . Then the expected
number of sampling trials to obtain d positive samples isd/ Qj∈[d] Pj . Similarly, the expected
number of sampling trials to obtain d negative samples isd/ Qj∈[d] [1 - Pj - P(xj = 0)]. Let n
be a random natural number s.t. with n samples the network has learned. The expected number of
samples that guarantees successful learning is the sum of the two expectations
E[n] = d] Qj⅛
+
1
Qj∈[d] [1- Pj- P(χj = 0)]
)
≥ d Qj∈d]PP+ + Qj∈[d](i- Pj)
≥d ∙2 jYd]ppj (I-Pj)
≥d ∙2 jU (Pj+i—Pj)/
=d ∙ 2d+1.
Thus E[n] ≥ O(d ∙ 2d+1).
□
With an exponential sample complexity, the time complexity of the LR approach is thereby also
exponential because filtering through all the samples costs time with the same complexity as the
number of samples, even though the LR itself only costs polynomial time in the number of samples.
To summarize, this vanilla LR approach learns exact ground-truth residual units but with exponential
complexity in terms of both computational cost and sample size. While this approach to learning a
residual unit such as ours is simple and intuitive, we aim to improve on this approach, making full
use of all samples available.
F.1 Experiments: Sample Efficiency
We present the results of the vanilla LR in learning the residual units. As discussed in section 3, LR
requires full-rank linear systems parameterized by samples to learn the exact ground-truth parameters.
17
Under review as a conference paper at ICLR 2022
Table 2: Learning success rate of vanilla LR on residual units with input X L吧.N (0,1). The
success rates shows an exponentially decreasing trend with the same number of samples. The sample
sizes to achieve close to the same rate grows exponentially as the number of dimensions grows
linearly.
x''∖ n d'	1e1	1e2	5e2	1e3	5e3	1e4	5e4	1e5
4	0.002	0.137	0.999	1	1	1	1	1
6	0	0	0.041	0.614	1	1	1	1
8	0	0	0	0	0.579	0.998	1	1
10	0	0	0	0	0	0.002	1	1
12	0	0	0	0	0	0	0.001	0.322
Table 3: Learning success rate of vanilla LR on residual units with input X ' N (0.1,1). With
the non-zero Gaussian mean, the success rates show an overall decline compared with the rates shown
in Tab. 2 for zero-mean Gaussian inputs.
∖. n d∖	1e1	1e2	5e2	1e3	5e3	1e4	5e4	1e5
4	0.001	0.122	0.996	1	1	1	1	1
6	0	0	0.030	0.346	1	1	1	1
8	0	0	0	0	0.116	0.792	1	1
10	0	0	0	0	0	0	0.619	1
12	0	0	0	0	0	0	0	0.001
However, with degenerate linear systems, LR is completely incapable of learning the parameters.
Therefore, there exists a hard threshold for the number of samples required by LR that makes the
linear equation system full-rank. With such a dichotomous constraint on LR learning, we take the
learning success rate among 1000 trials as the metric to evaluate the performance ofLR with different
sample sizes and number of dimensions.
Tab. 2 shows the learning success rates with zero-mean Gaussian inputs. For each fixed number of
dimensions, it appears there is a hard threshold that switches the learnability of LR. The exponential
sample complexity is also reflected there as the number of dimensions grows linearly. Tab. 3 shows the
rates with input mean non-zero, where an overall decline in success rates happens. This observation
is explainable because the bottleneck of LR learning is the lower value found between the probability
of sampling a positive and a negative vector. A positive mean reduces the probability of the latter,
and thereby increases the sample size required.
G Generalization of Layer 2 Learning
In this appendix, we discuss how layer 2 is learned without satisfying Cond. 2.1. It is a fact that
A* being a scale transformation w.r.t. some components causes scaling equivalence to our layer 2
objective functional minimizers just as in layer 1. To be more specific, we give a general version
of Thm. 4.1 that handles the case without Cond. 2.1 and describes the scaling equivalence of G2
minimizers. See Thm. G.1 for a formal description.
Theorem G.1 (objective minimizer space, layer 2, general).
Let G2 be IExhkh (x) + X — Cy『]
as a functional, where C ∈ Rd×m, h ∈ C0≥0. Then G2 (C, h) reaches its zero minimum iff
CB* = diag(k) where for each j ∈ [d],
a)	.A* ≤ kj ≤ Lf A* is a scale transformation w.r.t. the j-th component.
j,j
b)	kj = 1, ifA* is not a scale transformation w.r.t. the j-th component.
18
Under review as a conference paper at ICLR 2022
As in layer 1, the scaling equivalence in layer 2 can also be obtained by assigning A* as a scale
transformation w.r.t. some component and using the properties of the ReLU nonlinearity. See
subsection H.1 for a detailed explanation of Thm. G.1. To obtain the scale factors k and correct a
scale-equivalent G2 minimizer C to the ground-truth, we observe linear models parameterized by the
scale factors, where for each j ∈ [d], [Cy]j = kjxj given that xj < 0. See Thm. G.2 and its proof
for justifications of the linear models that are used to compute k.
Theorem G.2 (scale factor, layer 2, general). Assume C is a minimizer of G1 in the context of
Thm. G.1. Then for any j ∈ [d], the following three propositions are equivalent:
a)	A* is a scale transformation w.r.t. the j-th component.
b)	[Cy]j /xj	is a constant	cjn	given that xj	< 0.
c)	[Cy]j /xj	is a constant	cjp	given that xj	> 0.
Additionally, if one of the above three propositions is true, then kj = cjn.
Proof. For j ∈ [d], we first prove a) =⇒ b), c): From (A*,)> = A*,j ∙ e(j) we have
[Cy]j	kje(j)> h(A*x)+ + xi
_ kj [(A*,：x)+ + xj _ kj [(AL-Xj)+ + xj
=	xj	=	xj
kj ,	xj < 0
=Ikj (A*,j + 1), xj> 0.
Then we prove  a) =⇒  b),  c):  a) =⇒ ∃ j0 ∈ [d] and j0 6= j s.t. Aj*,j0 6= 0. Recall
[C y]j	kj[(A*,*)+ + Xji
	=	
.
xj----------------xj
The value of xj0 affects the value of [Cy]j /xj because Aj*,j0 6= 0. In fact, from supp p(x) = Rd we
have supp p(xj0 | xj ) = R, indicating that the value of [Cy]j /xj can never be kept as a constant
when given both Xj < 0 and Xj > 0 because Xj，can be any real number.	□
With the exact derivations above, we are able to obtain a left inverse of B*, namely C, that satisfies
CB* = Id. Consider Eq. 1 left multiplied by B*C
B*Cy = y.	(12)
Eq. 12 is also a noiseless unbiased linear model where Cy and y are observable, and thereby B * is
computable due to the easy solvability of its linearity.
Alg. 5 describes how a residual unit second layer is learned without satisfying Cond. 2.1: We first
solve the QP/LP and obtain a scaling equivalence to an estimated left inverse of B * and the function
estimate, namely C and Ξ. Then we compute the scale factor estimate k by running Alg. 4, where
for each component index j ∈ [d], we first use a tolerance parameter as a threshold to determine
whether ground-truth layer 1 is a scale transformation, and if so, we run LR to estimate the model
[Cy]j = kjxj, otherwise the scale factor is directly assigned by 1. Upon correcting C and Ξ by
k, we obtain a layer 2 estimate B by running LR to estimate the linear model Eq. 12. The strong
consistency of our results in this appendix is justified in Appendix J.
H Exact Derivation of Objective Functional Minimizers
H.1 Layer 2
Lem. 4.2 is proved as follows.
19
Under review as a conference paper at ICLR 2022
Algorithm 4 Rescale a GNPE minimizer.
1:	Parameters: £应 > 0, LR objective tolerance.
2:	Input: {(x(i), y(i))}n=ι, samples drawn by Eq. 1; C, a GNPE minimizer.
3:	Output: k, a layer 2 scale factor estimate.
4:	for each j ∈ [d] do
5:	kkj - LR{χ(i),Qy(i)]jOx(i)<0.
6:	if the LR objective optimal Rj (kj ) > εtol then
7:	kj V— 1.
8:	end if
9:	end for
10:	return k.
Algorithm 5 Learn a ReLU residual unit layer 2.
1:	Input: {(x(i), y(i))}in=1, samples drawn by Eq. 1.
+
2:	Output: B, ξ, a layer 2 and X → (A*x) estimate.
3:	Go to line 4 if QP, line 5 if LP.
4:	Solve QP: Eq. 4 and obtain a GNPE minimizer, denoted by C, Ξ. Go to line 6.
5:	Solve LP: Eq. 5 and obtain a minimizer C, then assign ξ(i) V Cy(i) - x(i) for each i ∈ [n].
6:	Run Alg. 4 on {(x(i), y(i))}i∈[n], C and obtain k.
7:	B — LR {diagT∕) ∙ Cy⑴，y⑺}. ∣「
8:	ξ(i - diag-1(k) [ξ(i) + x(i)] 一 x(i) for each i ∈ [n]. {Correct Ξ to the function estimation
of (A*x)+.}
一	A ɪ
9:	return B, Ξ.
Proof. “ ^= ”: Since h(x) = Cy 一 X ≥ 0, random vector h(x) + X — Cy is always a zero vector,
which implies G2(C, h) = 0. Hence " ^= ” holds.
“	=⇒ ”: Since r.v. kh(X) + X 一 Cyk2 ≥ 0 we have
G2(C,h) = 0 =⇒ λ nX ∈ Rd kh(X) +X 一 Cyk2 > 0o = 0	(13)
where λ is the Lebesgue measure on Rd .
Proof by contradiction: Assume that ∃ X0 ∈ Rd, ∃ i ∈ [d], s.t. (Cy0 一 X0)i < 0, where y0 is the
corresponding network output. Let f (X) = (Cy 一 X)i which is continuous on Rd. Therefore for
E = —f (x0) > 0, ∃ δ > 0, ∀x ∈ B(x; δ) i.e. ∣∣x — x0k < δ,
|f (X) 一 f(X0)| <	=⇒ 2f(X0) < f(X) < 0 =⇒ [X 一 Cy]i > 0
=⇒ [h(X) + X 一 Cy]i > 0 =⇒ kh(X) + X 一 Cyk2 > 0.
∏d∕2
--------δd
Γ(d∕2+1)
Since λ (B(X; δ))
> 0 and B(X; δ) is a subset of the measured set in Eq. 13,
we have a contradiction. Thus Cy 一 X ≥ 0 holds in a pointwise manner in Rd , indicating
X ∈ Rd | h(X) 6= Cy 一 X must be a null set. With h’s continuity, h must be X 7→ Cy 一 X.
Therefore “ =⇒ ” holds.	□
Lemma H.1. Cy — X ≥ 0 holdsfor any X ∈ Rd and its corresponding output y only if CB* is a
diagonal matrix.
Proof. Let D = CB*. We rewrite Cy — X ≥ 0 as
D h(A*X)+ + Xi — X ≥ 0.
(14)
20
Under review as a conference paper at ICLR 2022
For further use, we substitute x by —x and the resulting inequality D [(-A*x)+ - x] + x ≥ 0
still holds. Added by Eq. 14 we have
D (⑷,：• x∣)dχ1 ≥0.	(15)
Proof by contradiction: Assume D is not diagonal, then ∃ i = j, such that Dij = 0. Consider the
following two cases:
a)	Dij > 0. We take the i-th row of Eq. 14 as follows
d
XDi,k [(4,: ∙x)+ + Xk] ≥ Xi.	(16)
k=1
Let x-j = 0 and Xj	<	0.	Then Pk=ι Di,k	](Ak ：	∙ x)	+ Xk	=	Dij	∙叼 < 0	=⇒	⊥.
b)	Di,j < 0. We take the i-th row of Eq. 15 as follows
d
X Di,k∣ Ak,： ∙ x ∣ ≥ 0.	(17)
k = 1
Let X = (Ak)Tv, where v = e(j). Then Pk=ι Di,k ∣ Ak ： ∙ x ∣ = Di,j < 0 =⇒ ⊥.
□
With Lem. 4.2 and Lem. H.1, we prove Thm. G.1 as follows.
Proof. With Lem. 4.2, we only need to prove V X ∈ Rd, Cy — X ≥ 0 ^⇒ CB* = diag(k).
“ b "： We have
Cy — X = CBkkAkx)+ + Xi — X = diag(k) [(Akx)+ + Xi — x.	(18)
For the i-th row ofEq. 18, consider the following two cases:
a)	Ak ： ∙ x = Ai,iXi, i.e. Ak is a scale transformation w.r.t. the i-th row. With 1,A^ ≤ ki ≤ 1
,	丁 ʃɪi,i
we have
[Cy — x]i = ki [(Ak,ixi)+ + xi] — xi = ki (Ak,ixi)+ + (ki — I) xi
For Xi ≥ 0, [Cy — x]i = (kiA^ + ki — 1) Xi ≥ 0.
For Xi < 0, [Cy — x]i = (ki — 1) Xi ≥ 0.
b)	a*,： ∙ x = Ai,iXi. With ki = 1 we have
[Cy — x]i = ki [(Ak,：x)+ + Xii- xi = (Ak,：x)+ ≥ 0.
Hence " ^= ” holds.
“ =⇒ "： Let D = CB*. With Lem. H.1, D is diagonal. Consider the following two cases:
a)	A*,： ∙ x = Ai,iXi. The i-th inequality can be written as
[Cy — x]i = Di,i [(Ak,ixi)+ + xi] — xi
=Di,i (Ak,ixi)+ + (Di,i - I) xi ≥ 0.	(19)
Proof by contradiction: we need to find X ∈ Rd which contradicts with Eq. 19 in the
following three cases:
a)	If Di,i	≤	0, then, Xi	> 0	=⇒	Dy	(Ak,iXi)+	+ (Di,i	— 1) Xi	< 0 =⇒	⊥.
b)	If Di,i	>	1, then, Xi	< 0	=⇒	⊥.
21
Under review as a conference paper at ICLR 2022
c)	If 0 <	Di,i	<	*A*	, then ∃ a >	0,	s.t.	Di,i	=[十^；十@. Letting	Xi	> 0, we have
Di,i (Ai,ixi)+ + (Di,i - 1) Xi
(Ai,ixi)	(Ai,i + α) Xi
------：------------------：-------
1 + Ai,i + a 1 + Ai,i + a
< 0 =⇒ ⊥.
Hence 1+A* ʃ ≤ Dy ≤ 1.
b)	Ai,. ∙ x = Ai,iXi, i.e. ∃ j = i, s.t. Aij > 0. The i-th inequality can be written as
[Cy - x]i = Di,i [(A].x)+ + χj - Xi
=Di,i (Ai,：x)+ + (Di,i - 1) Xi ≥ 0.
(20)
Proof by contradiction: we need to find X ∈ Rd which contradicts with Eq. 20 in the
following three cases:
a)	If Di,i	≤	0,	then, Xi	> 0 =⇒ D^	(A*,.x)+	+ (D^	- 1) Xi	< 0 =⇒	⊥.
b)	If Di,i	>	1,	then, Xi	< 0 八 x— ≤ 0	=⇒ ⊥.
c)	If 0 < Di,i < 1, then, Xi > 0 八 Xj ≤ -
Hence Di,i = 1.
A*
A*iXi 八 Xk ≤ 0 =⇒ ⊥, where k = i, j.
Ai,j
Hence D = CB* = diag(k), and thereby “ =⇒ ” holds.
□
H.2 LAYER 1
The proof of Lem. 5.3 is similar to that of Lem. 4.2 because the two lemmas follow the same idea,
which is to link objective functional minimization with always-hold inequalities. With Lem. 5.3 , we
prove Thm. 5.1 as follows.
Proof. With Lem. 5.3, we only need to prove VX ∈ Rd, (A*x)+ — Ax ≥ 0 ^⇒ Vi ∈
[d], Ai,： = kiAi,：, where 0 ≤ ki ≤ 1. This is equivalent to what it is for a single row, i.e. VX ∈
Rd, (a*>x) — ɑ>x ≥ 0 ^⇒	a = ka*, where 0 ≤ k ≤ 1.
“ ^= "：	The case where a* = 0	is clear. If a* is not a zero vector	and	a =	ka*, we have
a)	If a*>x ≥ 0, (a*>x)	— aτx = a*>x — ka*τx =	(1	—	k) a*>x ≥	0.
b)	If a*τx < 0, (a*τx)	— aτx = —ka*τx ≥ 0.
Hence " ^= ” holds.
“ =⇒ "： If a* = 0, a must be a zero vector, otherwise let X = a, then —aτx < 0 =⇒ ⊥. If a* is
not a zero vector, consider two cases below:
a)	If a = ka* where k ≥ 0. Let X = kaj- ∙ a — a*, then X is not a zero vector, and
a*τx = ∣∣a*∣∣2 (cosθ — 1) < 0 ,]
aτx = ∣∣a∣∣ ∣∣a*∣∣ (1 — cosθ) > 0 ʃ
a*τx)	— aτx < 0 =⇒ ⊥
where θ denotes the angle between a and a*.
b)	If a = ka* where k > 1, then let X = a* we have
(a*τx) — aτx = (1 — k) ∣∣a*∣∣2 < 0 =⇒ ⊥.
Hence a = ka* where 0 ≤ k ≤ 1 if a* is not zero, and thereby " =⇒ " holds.
□
I QP Convexity
The LPs in the main paper are trivially convex. So in this appendix, we only justify the convexity of
our QPs: We first prove the convexity of single-sample objectives, then the convexity of the empirical
objectives with nonparametric estimation, i.e. GNPE and GNPE is obtained by the convexity of convex
function summations.
22
Under review as a conference paper at ICLR 2022
Lemma I.1. Suppose f (U) = 1 ∣∣Tu 一 b∣∣2 where U is a real matrix. Then f is convex w.r.t. U.
Lem. I.1 is easily obtained since the Hessian f00 (T) = T>T is positive semidefinite. In the following,
we demonstrate and justify the convexity of the QPs of both layers by rewriting their single-sample
objectives into the formulation of f and summing them without loss of convexity.
Theorem I.2. QP: Eq. 4, and QP: Eq. 7 are convex optimization.
Proof. First of all, constraints of both QPs are trivially linear and convex. Thus, we only need to
justify the convexity of the two empirical objectives, GNPE and GNPE (See Eq. 7 and 4). Consider
the single-sample version of Gnpe, namely gNPE(A, φ; x, h) = 2 ∣∣φ + Ax 一 h∣2, which, in the
formulation of f, can be rewritten with
T
which guarantees the convexity of g1NPE w.r.t.
2 ∣∣ξ + X — Cy∣∣2, we have
A and φ by Lem. I.1. For g2NPE(C, ξ; x, y)
(21)
-y>
-y>
-y>
(22)
1
1
which guarantees the convexity of g2NPE w.r.t. C and ξ by Lem. I.1. Now we consider the summation.
Taking layer 1 as an example, by definition we have
GNPE(A, Φ) = E gNPE(A, φ⑺;X⑴,h⑴).	(23)
i∈[n]
For each i ∈ [n], equivalently, we take Φ as a variable instead of φ(i) in g1NPE, but with only φ(i) ∈ Φ
determining the value of g1NPE. In this sense, g1NPE is convex w.r.t. A and Φ for each i ∈ [n]. Thus,
the sum GNPE is convex w.r.t. A and Φ. Similarly, GNPE is convex w.r.t. C and Ξ.	□
J	Strong Consistency
In this appendix, we justify the strong consistency of our estimators for the residual unit layer 1/2
learning and the whole network.
According to Thm. G.1 and Thm. 5.1, the solutions to our objective functionals are continuous sets.
In addition, there are theoretical intermediate results that are also represented as continuous sets,
e.g. possible left-inverse matrices for B* in the results for layer 2. Thus, to analyze the consistency
of our learning algorithm, we define distances between sets, so that the convergence of sets can be
well defined. Further point convergence results, i.e. layer 1/2 estimator strong consistency, are based
on the set convergence we define.
Definition J.1 (deviation). Let A ⊆ M and B ⊆ M be two non-empty sets from a metric space
(M, d). The deviation of set A from the set B, denoted by D(A, B), is
D(A, B) = sup d(a, B) = sup inf d(a, b),	(24)
a∈A	a∈A b∈B
where sup and inf represent supremum and infimum, respectively.
Definition J.2 (Hausdorff distance). Let A ⊆ M and B ⊆ M be two non-empty sets from a metric
space (M, d). The Hausdorff distance between A and B, denoted by DH(A, B), is
DH(A, B) = max{D(A, B), D(B, A)}.	(25)
Remark. In the following, we use the Frobenius norm to define the distance between matrices,
i.e.d(X,Y)= ∣X-Y∣F.
23
Under review as a conference paper at ICLR 2022
J.1 Layer 2
In this subsection, we prove the strong consistency of the layer 2 estimator.
J.1 .1 Objective Minimizer Space Estimator
For simplicity of notation, We use Sn to denote our layer 2 QP/LP solution space by n random
samples8 as a random set
Sn := {C	∈	Rd×m	: Cy(i)	- X⑴	≥	0,	∀ i ∈	[n]}	(26)
and S* to denote the value space of C in Lem. 4.2 which minimizes layer 2 objective functional
S* := {C ∈ Rd×m : Cy - X ≥ 0, ∀ X ∈ Rd and its corresponding output y}.	(27)
For further use, we name Pn as the set of n sampled inputs which define Sn i.e. Pn := {x(i)}i∈[n]
where each X(i) is the same random variable in Eq. 26. By the definitions above, we describe the
strong consistency of our QP/LP as Lem. J.1.
Lemma J.1 (QP/LP strong consistency, layer 2). DH(Sn S*) —→ 0 as n → ∞.
Proof. First we prove that DH(Sn S*) → 0 as n → ∞. Recall Thm. G.1, Cy - X ≥ 0 only if
CB* = diag(k). We inherit the notation as defining D = CB*. Theorem G.1 is based on Lem. H.1,
and we prove them by raising points that show contradiction, i.e. violate the inequality that holds in a
pointwise fashion:
a)	In the proof of Lem. H.1, we use d points: -e(i), for i ∈ [d], to make
Pd=I Di,k ](A* : ∙ x) + Xk < xi, so that Dij (i = j) cannot be positive; and another
d points: (A*)-1e(i), to show Pkd=1 Di,k A*k,: ∙ x∣ < 0, so that Dij (i = j) cannot
be negative. For each -e(i) we use here, since the violations follow strict inequalities,
we know there exists a neighborhood of -e(i), Ni = N(-e(i)), such that ∀z ∈ Ni,
Pd=I Di,k ](Ak : ∙ Z) + Zk < zi. We can similarly find such neighborhood of each
(A*)-1e(i) that the strict inequality holds within the neighborhood respectively. We index
them as Nd+1 to N2d.
b)	In the proof of Thm. G.1, we further construct d points: for each i ∈ [d], we take a point x
such that xi > 0 ∧ xj ≤ -
A"∙	-F	, , . . i.	'F
AFLXi ∧ Xk ≤ 0, where k = i, j. This counterexample shows
Ai,j
[Cy - X]i < 0, and eliminates the possibility of 0 < Di,i < 1 when Ai*,: is not a scale
transformation. We can similarly find neighborhood of each point and index them as N2d+1
to N3d. Note that we omit some cases in the proof of Thm. G.1, because the first 2d points
are sufficient to use in those cases to show contradiction.
In the sampling procedure, if we sample at least one point in each neighborhood Ni, Thm. G.1
assures the solution we get Cn would lie in the true optimal set S*. The probability that the sampling
procedure “omits” any of the neighborhoods is
3d
P (Pn \ Nl= 0 or Pn \ N2 = 0 or ... or P n \ N3d = 0)≤ X P (Pn \ Ni= 0)
i=1	(28)
≤ 3d[1 - min P(Ni)]n
i∈[3d]
Since the measure on each neighborhood P(Ni) = x∈N p(X) > 0,
P (Pn \ Ni = 0 or Pn \ N2 = 0 or ... or Pn \ N3d = 0)→ 0, as n → ∞.	(29)
8Here, we take samples as random variables for empirical analysis.
24
Under review as a conference paper at ICLR 2022
Here We obtain DH(Sn, S*) → 0 as n → ∞. Now We take the infinite sum over the both sides of
Eq. 28
X P (Pn \ Nl = 0 or Pn \ N2 = 0 or ... or Pn \ N3d = 0)
n∈[∞]
≤ 3d X 1 - min P(Ni)	= 3d  ---------1 »N、- 1 < +∞.
n∈[∞]	i∈[3d]	mini∈[3d]P(Ni)
By the Borel-Cantelli lemma (Borel,1909), DH&n, S*) —→ 0 as n → ∞.	□
J.1 .2 Scale Factor Estimator
To avoid ambiguity, We use nsf to denote the number of samples used in Alg. 4. The samples pairs are
{(x(i), y(i))}in=sf1. Without loss of generality, the folloWing discussion focuses on some fixed index
j ∈ [d]. In Alg. 4, we plug in our estimator Cn and use LR to estimate kj given that Xji) < 0
knf(Cn) = argmin ɪ X Il [Cny(i)]j - kxji)∣∣2 = Pi∈[nsf] XjJCny^j.	(30)
k	2nsf i∈[nsf]	Pi∈[nsf] (x(ji)2
We first give the strong consistency of layer 2 scale factor estimator for as nsf → ∞, as described in
Lem. J.2.
Lemma J.2 (scale factor estimator strong consistency, layer 2). Suppose A* is a scale transformation
w.r.t. the j-th component, and knsf (Cn) is the nsf -sample estimator of kj via LR: Eq. 30 given that
(i)
xj < 0. Define sets
1
Unsf,n := {knsf(C) ： C ∈ Sn}, and U :=	∕* , 1 .	(31)
1 + Aj,j
Then lim lim DH(Unsfn,U*) =a.=s. 0.
nsf →∞ n→∞	,
Proof. Following our notation, Thm. G.1 and Thm. G.2 ensure that if A* is a scale transformation
w.r.t. the j-th component, for any C belonging to the true optimal set S*, knsf(C) ∈ “；* , 1 .
j,j
And the “iff” statement strengthens that U* = {knsf (C) : C ∈ S*} for any nsf ∈ Z+. Note that since
as
S* ⊂ Sn, we have U* ⊂ Unsf,n. We only need to prove D(UnSf,n, U*) ―→ 0 as n → ∞.
ʌ ʌ
∀Cn ∈ ^n and∀C ∈ S*,
," ,
knsf (C n) — knsf (C )
1
Pi∈[nsf] Ei))2
X	X(ji)
i∈[nsf]
X "(&)> (Cn-C) B*∣∣2(∣∣A*χ(i)∣∣2 + ∣∣χ(i)∣∣J
i∈[nsf]
≤--------------
Pi∈[nsf]	Xj
≤
- C ∣∣F
(kA*kF + 1) ∣∣x(i)∣∣2
1
25
Under review as a conference paper at ICLR 2022
Then
inf ∣∣Cn - C∣∣
n C∈S*"	11F
(32)
(33)
which implies
D(Unsf,n, U) ≤
Pi∈[nsf] h∣xji)∣kB*kF (kA*kF + 1)∣∣x(i)∣∣2i
Pi∈[nsf] (xji))2
,ʌ ..
D(^n, S*).
(34)
Take the nsf → ∞ limit over both sides of Eq. 34. With the strong law of large numbers9 we have
lim D(Unsf,n, U) ≤
nsf →∞
E [x(12
D(Sn, S*),w.p. 1.	(35)
Since D(Sn S*) —→ 0 as n → ∞, we have D(Unsf,n, U*) -→ 0 as n → ∞ then n$f → ∞.	□
J.1 .3 Layer 2 Weights Estimator
In Alg. 5, We solve B via LR. Let z(i) = diag-1(k) ∙ Cny(i) ∈ Rd, where k is obtained through
Alg. 4 with input Cn Assume we are using sample size of n to do the LR. The optimization
problem is
min ^X ∣∣y(i) — BZ(i)∣∣	(36)
i∈[nw]
Now we present the strong consistency of layer 2 estimator, as described in Thm. J.3.
Theorem J.3 (strong consistency, layer 2). Suppose BnSf is the solution to Eq. 36. Then BnW —→ B*
as n, nsf, nw → ∞.
Proof. Let β denote VecB (flattening B into a vector), then Bz(i) = (z(i))> 0 Im. Here the
operation 0 denotes the Kronecker product. Then we can define an equivalent optimization problem
min ɪ X y(i) — (Zei)) 0 Im β
β 2nw i∈K])」
Take the derivatives of β, we obtain
—2 X	[(z(i))> 0 Iml	(y(i)-
i∈[nw]
(37)
(38)
9Here we assume that the Kolmogorov’s strong law assumption on moments (Sen & Singer, 1994) is met as
is commonly done in empirical analysis.
26
Under review as a conference paper at ICLR 2022
Then the optimal solution ∕3nw of this optimization can be written in closed form:
-1
βnw
T
X(Z(i))	0 Im
i∈[nw]
Z(i))T 0 Im
X	(Z(i))T 0 Im
i∈[nw]
T
y(i)
-1
X [Z(i) 0 Im](Z(i))
i∈[nw]
T	-
0 Im
-1
X W) 0 Im] y(i)
i∈[nw]
Σ :
.2∈[nw] L
T
Zei)(Z ⑴)
I I E ZCi) 0 Im
i∈[nw]
-1
T
0 Im
y⑴
X Z(i) (Z(i))T
i∈[nw]
E Zi 0 Im
i∈[nw]
0 Im
y⑴
We inherit the notation from the last two subsections. By Lem. J.1, d(Cn, S*) —→ 0
as n → ∞.
ʌ
ʌ
Thus ∀ε > 0, ∃ N such that ∀n ≥ N, d(Cn, S*) ≤ ε w.p. 1, i.e. ∃ Cn ∈ S* s.t. d(Cn, Cn) ≤ ε
w.p. 1. Then by Lem. J.2, ∃ K > 0, for ε that is small enough, ∃ NSf such that ∀ nsf ≥ Nsf, We
have ∣knsf (Cn) - knsf (Cn)I ≤ Kε w.p.1. For simplicity, we omit the under-script n$f of kw in the
following discussion. In fact,
Zji)-ZC 1 = lk⅛ (eCi))T C nyCi)-k⅛ (eCi))T CnyCi)
≤
+
1
,ʌ . , .
k(C n)k(Cn)
1
,ʌ ., .
k(C n)k(Cn)
∣1
k(Cn) (e(i))T Cn - k(Cn) (e)
Cn y(i)
k(Cn) (e(i))T Cny(i) - k(Cn) (e(i))T CnyCi)
,ʌ . , .
k(C n)k(Cn)
k(Cn) (e(i))T Cny(i) - k(Cn) (e(i))T Cnyi
≤ 1 JK+(A*jA* ) ( I I y(i)|| I∣Cn - Cn∣∣ + ∣k(Cn) - k(Cn)∣ ∣∣CnB*∣∣F (kA*h + 1)卜⑺
ɪ	1 Cjj	S
≤ 1-(K+(：+； .)( i i y(i)H+dK (M*kF+1)MA
∖	j,j，
(39)
Then
ZCi)(Z⑴)T - Z(i)(Z⑴)T
F
ZCi)(Z(i))T - Z(i)(Z(i))T
+ ZCi) (ZCi))	- ZCi) (ZCi))
F
≤ MZ(i) - Z(i)
Z(i)、T
≤ Il [Z(i) - Z(i)] [Z(i)
F+
-z(i)]T
Zei))T
z(i))T
+ 2 zl
F
(i)
Zei))T
F
Z(MIF
铲
—
—
≤ X(Zji)-Zji))2 +2 ||Zei)IM)-Zci)
j∈[d]
27
Under review as a conference paper at ICLR 2022
Itfollowsthat ∣∣z(i) (z(i))> - z(i) (z(i))>∣∣ is also bounded by O(ε). With similar techniques we
can prove	F
X Z⑴(Z⑴)> -X Z⑴(Z⑴)>	≤ O(ε)	(40)
∣i∈[nw]	i∈[nw]	∣F
and
∣∣z(i) 乳 Im - Z⑺㊈ Im∣∣f ≤O(ε)	(41)
DenOte [Pi∈[nw] z⑴(Z⑺)>]as P and Pi∈[nw] Z(i)
♦	1	-r∖	1 z-ʌ ɪ ɪ
expression we have P and Q. Hence,
as Q. Substitute Z(i) with ^(i) in the above
-β*∣∣F = ∣hP ㊈ Imi-1 (h<Q ㊈ Imi y(i)) - [P ㊈ Im]-1 ([Q ㊈ Im] 丫⑴)
≤ M ㊈ Imi-1 (h<Q ㊈ Imi 丫⑴)-[P ㊈ ImiT(Q ㊈ Im] N
+ ∣∣hP ㊈ ImiT ([Q ㊈ Im] y⑺)-[P ㊈ Im]-1 ([Q ㊈ Im] y⑺)∣∣
∣∣[Q ㊈ Im] y ⑴ ∣∣f
∣∣[(Q - Q)㊈ Imi y⑴ ∣∣f + ∣∣[P ㊈ ImiT-P ㊈ Im]
-1
F
In the first part, by triangle inequality,
∣∣[P 0 Imi 1
So we only need to prove
[P 0 Imi-1 -[P 0 Im]-1	+∣∣[P 0 Im]-1∣∣F
(42)
∣∣[P 0 ImiT
- [P0Im]-1
≤ O(ε) to claim 忸nw - β*∣∣^ ≤ O(ε).
Denote P - P = ∆P. From Eq. 39, we know every entry of ∆P 0 Im can be bounded by O(ε).
By simple calculation we have
≤
F
Then we have
(P+ ∆P)-1 = P-1 - P-1∆PP-1 + O(ε2).	(43)
P-1 - P-1 = P-1∆PP-1 + O(ε2) = O(ε).	(44)
□
J.2 Layer 1
In this subsection, we justify the strong consistency of layer 1 objective functional minimizer estimator
in detail, i.e. the layer 1 QP/LP solution space. We will omit the detailed proof of Alg. 2 line 4 to 7
strong consistency since it is similar to the proof of Lem. J.2. Additionally, we also omit the strong
consistency of the X → (A*x)+ function estimator because it can be directly obtained by Lem. J.1
and J.2 and the continuous mapping theorem.
We use a new optimization problem equivalent to the optimization of G1 . Before that, we first define
the equivalence between two optimization problems as follows.
Definition J.3. Let opt1 and opt2 be two optimization problems, with f1, f2 as the respective
objective functions. Then opt1 and opt2 are said to be equivalent if given a feasible solution
to opt1, namely x1, a feasible solution to opt2 is uniquely corresponded, namely x2, such that
f1 (x1 ) = f2 (x2 ), and vice versa.
The new optimization problem and its equivalence to the optimization of G1 is described in Lem. J.4.
Lemma J.4. The optimization of G1 (Eq. 6) is equivalent to
mAnf(A) = 2Ex ∣∣(Ax - h)+∣∣ ] ∙	(45)
28
Under review as a conference paper at ICLR 2022
Proof. To see this, suppose A1 is one optimal solution to Eq. 45, then we can construct r1 (x) =
(A1x - h)+ so that G1 (A1, r1) = f(A1) and the optimality implies
min G1 (A, r) ≤ min f (A).
On the other hand, suppose (A2, r2) is an optimum of G1. Let r3(x) = (h - A2x)+, then ∀x ∈ Rd
and h be the corresponding hidden output, if [h - A2x]j ≥ 0, then [r3 (x) + A2x - h]j = 0,
otherwise [r3 (x) + A2x - h]j2 = [A2x - h]2 ≤ [h2 (x) + A2x - h]j2 since h2 is nonnegative. So
that we know that
min G1(A, r) = G1(A2,r2) = G1(A2,r3) = f(A2)
A, r
From the optimality, we further have
min G1 (A, r) ≥ min f (A)
From the simple calculation above, we can see that one optimal solution to Eq. 45 has a one-to-one
correspondence to an optimal solution to Gι.	□
Similarly, the empirical version of the two problems are equivalent, which indicates their consistency
in the empirical estimation being equivalent. In the following, we justify the strong consistency of
empirical Eq. 45 instead of G1
+2
min fn (A)=诟 X(Ax(i) - h(i))	^	(46)
i∈[n]
Denote T* := {diag(k) ∙ A* | 0 ≤ kj ≤ 1,j ∈ [d]} as the true optimal solution set, and Tn as
the optimal solution set corresponding to the n-sample problem. In the following, we justify four
conditions in a row that hold for f to derive the strong consistency of its optimal solution estimator.
Lemma J.5. Let Tn0 be the layer 1 QP/LP solution space byn0 samples. Then there exists a compact
set C determined by A*, namely C(A*), s.t. Tn0 ⊂ C(A*) w.p. 1 asn → ∞.
Proof. ∀l ∈ [d], let a* be the l-th row of A*, and aι be the l-th row of An. We'd like first to prove
that the set
Tln0 = {al : al is the l-th row of A, where A ∈ Tn0 }	(47)
is compact w.p.1.
Suppose n0 > d, and among the n0 samples, we classify them into two folds. To avoid ambiguity, let
u(i) be the points such that (a*)>u(i) > 0, i ∈ [q]; and v(j) be the points such that (a*)>v(j) < 0,
j ∈ [n - q]. From the analysis of Thm. 5.1, we have (a*)>u(i) ≥ a>u(i), ∀i ∈ [q] and a>v(j) ≤ 0,
∀j ∈ [n - q]. It follows that we can rewrite TInO as a polyhedron
TInO = {a ∈ Rd : a>u(i) ≤ (a*)>u(i), a>v(j) ≤ 0}	(48)
We are going to show the polyhedron Tn is bounded by contradiction. If it is not bounded, then
∃d ∈ Rd, d = 0 and a ∈ TInO, such that ∀λ > 0, a + λd ∈ TInO. Then
(a + λd)> u(i) = a>u(i) + λd>u(i) ≤ (a*)> u(i) o λd>u(i) ≤ (a*)> u(i) - a>u(i) (49)
similarly,
(a + λd)> v(i) = a>v(j) + λd>v(j) ≤ 0	(50)
From the definition, λ can be arbitrarily big, then d>u(i) ≤ 0, ∀i ∈ [q], and d>v(j) ≤ 0, ∀j ∈ [n-q].
Since we know span{u(i)} = Rd w.p. 1, then there ∃ some i* such that d>u(i*) < 0, w.p. 1.
(Otherwise if d>u(i) = 0 for all i ∈ [q], then either span{u(i)} 6= Rd or d = 0.) Under our
assumption that, TInO is not bounded, we know the following system (w.r.t x) has a feasible solution
w.p. 1
X ≥ 0, x>u(i*) < 0	(I)
-U
-V
29
Under review as a conference paper at ICLR 2022
where every row of U and V is (u(i))> and (v(j))> respectively. By Farkas'lemma (Farkas, 1902),
the system
[-U>, -V>] ∙ X = u(i*), X ≥ 0	(II)
is not feasible (w.p. 1). We claim that u(i*)lies in the conic hull of-v(j)′s w.p. 1. So that the second
system actually has a feasible solution and thus it raises the contradiction.
Denote the conic hull as
H = t ∈ Rd : t = X λj -v(j) , λj ≥ 0for∀j ∈ [n - q]
[	j∈[n-q]
Now suppose u(i*) ∈ H, by the supporting hyperplane theorem (Luenberger, 1997), ∃b ∈ Rd, b = 0,
such that b>u(i*) ≤ b>t for ∀t ∈ H. Then by definition,
-Vj) ∈ {t ： b>t ≤ b>u(i*)}, for ∀j ∈ [n — q]
Denote the hyperplane J = {t : b>t ≤ b>u(i*)}, then
P	u(i*)	∈/	H ≤ P	-v(j)	∈	J,	for ∀j	∈	[n	- q]	= P	-v(j)	∈ Jn-q
Since we have in this case a geometric sequence, we know its infinite sum is bounded. By Borel-
Cantelli lemma (Borel, 1909), we conclude that u(i*) ∈ H w.p. 1. Then system II is feasible w.p. 1.
So that T% is compact w.p. 1.
Now we prove that there exists a compact set C(A*), s.t. Tn0 ⊂ C(A*) w.p. 1 as n → ∞. Similarly,
We focus on the analysis of one row. As discussed above, T% is compact w.p. 1. Let W% be the set
of all u(i) sampled in estimating Tn, and Wno be the set of all v(j) sampled. Now for another set
Tnoo, similarly define sample point sets W%； and W%,. We claim that ∀u(i) ∈ W', u(i) lies in the
conic hull of Wn〃. Actually, this part of the proof is very similar to the way we prove u(i*) ∈
w.p. 1, so we will omit it here.
∀a	∈ Tnoo,	let	x(1)	and	x(2)	be two different points in Wn〃.	Then	a>(λιx⑴ +	λ2X(2))
(a*)>(λ1X(1) + λ2X(2)) for ∀λ1 ≥ 0 and λ2 ≥ 0. This simple calculation reveals a>u(i)
(a*)>u(i) for ∀u(i) ∈ Wn (from the claim we made). This implies that T“ ⊂ C(A*) w.p. 1
n → ∞, too.
H
≤
≤
as
□
LemmaJ.6. Theminimizerspaceof f(A), i.e. T* = {diag(k) ∙ A* | 0 ≤ kj ≤ 1,j ∈ [d]}, is
contained in C(A*).
Lemma J.7. f(A) is finite valued and continuous on C(A*).
Lem. J.6 and J.7 are easily obtained by the formulation of f (see Eq. 45) and Lem. J.5.
Lemma J.8 (uniform a.s. convergence). fn(A) -a→ f (A) as n → ∞, uniformly in A ∈ C(A*).
Proof. Name single-sample objective g(x, A) = ɪ ∣∣(Ax - h)+1∣ . The uniform a.s. convergence
is guaranteed by the uniform law of large numbers (Jennrich, 1969):
a)	By Lem. J.5, C(A*) is a compact set.
b)	g is continuous w.r.t. Aby its formulation and measurable over X at each A ∈ C(A*).
c)	In fact,
g(x, A) = 2∣(Ax - h)+『
≤ kAXk2 + kA*Xk2
≤ (kAkF + kA*kF) kXk2.
Since A ∈ C(A*) is in a compact set,
g(X, A) ≤	sup	kAkF + kA*kF kXk2.	(51)
A∈C(A*)
30
Under review as a conference paper at ICLR 2022
Thus the dominating function exists10.
□
By Lem. J.5, J.6, J.7 and J.8, all of the conditions are satisfied in (Shapiro et al., 2014, Thm. 5.3).
Thus, we have the strong consistency of layer 1 objective optima estimator as described in Lem. J.9.
Lemma J.9 (QP/LP strong consistency, layer 1). DH(Tn, T*) -→ 0 as n → ∞.
Similar to Lem. J.2, we have the strong consistency of the layer 1 scale factor estimator as described
in Lem. J.10.
Lemma J.10 (scale factor estimator strong consistency, layer 1). Let kn0 (Cn) be the n0sf-sample
estimator of kj via LR: Alg. 2 line 5. given that hj(i) > 0. Define sets
__ --	，…	.	ʌ	___,	-	r
VnSf,n = {knS,(A) : A ∈ Tn }, and V := [0,1].	(52)
Then lim lim DH(VnO n, V*) == 0.
ns0f →∞ n0 →∞	sf,
Remark. In case kj = 0, suppose the algorithm finds a solution over a continuous distribution with
[0, 1] as support and the probability that it finds a solution with scale factor 0 is 0.
With Thm. 5.2 and the continuous mapping theorem, the strong consistency of layer 1 estimation is
guaranteed.
a.s.
Theorem J.11 (strong consistency, layer 1). Suppose An is scaled by kn,. Then An -→→ A* as
n0 , n0sf → ∞.
By Thm. J.3 and J.11, Thm. 6.3 is guaranteed by the continuous mapping theorem.
10Here, we assume E[kxk2] < +∞ as is commonly done in empirical analysis.
31