Under review as a conference paper at ICLR 2022
f -MUTUAL INFORMATION CONTRASTIVE LEARNING
Anonymous authors
Paper under double-blind review
Ab stract
Self-supervised contrastive learning is an emerging field due to its power in pro-
viding good data representations. Such learning paradigm widely adopts the In-
foNCE loss, which is closely connected with maximizing the mutual information.
In this work, we propose the f -Mutual Information Contrastive Learning frame-
work (f -MICL), which directly maximizes the f -divergence-based generalization
of mutual information. We theoretically prove that, with mild assumptions, our
f -MICL naturally attains the alignment for positive pairs and the uniformity for
data representations, the two main factors for the success of contrastive learn-
ing. We further provide theoretical guidance on designing the similarity function
and choosing the effective f -divergences for f -MICL. Using several benchmark
tasks from both vision and natural language, we empirically verify that our novel
method outperforms or performs on par with state-of-the-art strategies.
1	Introduction
Contrastive learning has attracted a surge of attention recently due to its success in learning in-
formative representation for image recognition, natural language understanding, and reinforcement
learning (Chen et al., 2020; He et al., 2020; Logeswaran & Lee, 2018; Srinivas et al., 2020). Such
learning paradigm is fully unsupervised by encouraging the contrastiveness between similar and
dissimilar sample pairs. Specifically, the feature embeddings of similar sample pairs are expected
to be close while those of dissimilar sample pairs are expected to be far apart. To attain this goal,
a softmax cross-entropy loss, a.k.a. InfoNCE, has been widely used (Wu et al., 2018; van den Oord
et al., 2018; Chen et al., 2020; Henaff et al., 2020; He et al., 2020), which aims to maximize the
probability of picking a similar sample pair among a batch of sample pairs.
InfoNCE can be interpreted as a lower bound of the mutual information (MI) between two views of
data samples (van den Oord et al., 2018; Bachman et al., 2019; Tian et al., 2020a; Tschannen et al.,
2020). This explanation is consistent with the well-known “InfoMax principle” (Linsker, 1988).
Nevertheless, it has been shown that maximizing a tighter bound on the MI can result in worse
representations (Tschannen et al., 2020); and reducing the MI between views while only keeping
task-relevant information can improve the downstream performance (Tian et al., 2020b). These
observations suggest that maximizing the MI may be insufficient in contrastive learning and thus a
better objective design is required.
To attain the aforementioned goal, we propose a novel contrastive learning framework, coined as
f -MICL. In a nutshell, leveraging the fact that MI can be formulated as the Kullback-Leibler (KL)
divergence between the joint distribution and the product of the marginal distributions, we replace
the KL divergence with the general f -divergence family (Ali & Silvey, 1966; Csiszar, 1967). Doing
so, We obtain a generalization of mutual information, called f -mutual information ( f -ML Csiszar,
1967). Notably, by maximizing a lower bound of f -mutual information we naturally decompose
the objective function into two terms, which correspond to the properties of the alignment and the
uniformity. Such characterization has been revealed in Wang & Isola (2020, Theorem 1) for the
InfoNCE loss. Compared with Wang & Isola (2020), our result applies to a wide range of the f-
divergence family such as KL, Jensen-Shannon (JS), Pearson χ2 and Vincze-Le Cam. and it does
not rely on the limit of an infinite number of dissimilar samples. This allows us to explore the space
of f-MI and improve the performance of InfoNCE-based contrastive learning.
The similarity function is crucial for the evaluation of the contrastiveness of similar and dissimilar
sample pairs. Commonly used similarity functions include the cosine similarity (Chen et al., 2020;
1
Under review as a conference paper at ICLR 2022
Figure 1: Network architecture of our proposed f -MICL. imagei : the ith image in the current
batch; f: the function used in the f -mutual information (§2); g: feature embedding; t, t1, t2:
augmentation functions drawn from the same family T of augmentations; f 0: the derivative; f *: the
Fenchel conjugate. The symbol ◦ denotes function composition. The sum of the two terms gives the
variational lower bound of f -mutual information. See equation 10 for more details.
He et al., 2020), the bilinear functions (van den Oord et al., 2018; Tian et al., 2020a; Henaff et al.,
2020), and the neural network based scores (Hjelm et al., 2018). While most aforementioned sim-
ilarity functions for contrastive learning are heuristic and pre-designed, in this work, we provide a
principled way to design the similarity function. By assuming that the joint feature distribution of
two similar samples is proportional to a Gaussian kernel, we derive an optimal similarity function
for practical use, which resembles the well-known radial basis functions (Powell, 1987).
Figure 1 gives a high-level summary of our f-MICL framework. Given a batch of samples (e.g.,
images) we generate positive pairs via data augmentation and negative pairs using other augmented
samples in the current batch. With the optimization of our f -mutual information objective the pos-
itive pairs are aligned with each other and the data representations are uniformly distributed. Our
contributions can be summarized as follows:
•	We propose a novel framework for contrastive learning (called f -MICL) by encouraging
the contrastiveness of positive and negative pairs with a general f -divergence family.
•	With an assumption on the joint feature distribution we provide an optimal design for the
similarity function with Gaussian kernels, which shows interesting connection between
contrastive learning and kernel methods.
•	We characterize the properties of alignment and uniformity for f-MICL and provide guid-
ance on choosing proper f -divergences. Our f -MICL objective can be estimated with finite
samples and we give the corresponding error bound.
•	Experimentally, our framework is better or on par with popular baselines, by simply replac-
ing, e.g., the InfoNCE loss function with our theoretically grounded f -MICL objectives.
Notations We assume a dominating measure λ (e.g. Lebesgue) is given and all other probability
measures are represented as some density w.r.t. λ. We denote Df (pkq) as the f -divergence between
two densities functions p and q. Given the joint density p(x, y), we denote p(x) = p(x, y)dλ(y)
and p(y) = / p(x, y)dλ(x) as the marginals. We use supp(∙) to denote the support of a distribution,
and f * to denote the Fenchel conjugate of a convex function f. Every norm presented is Euclidean.
2	Preliminaries
We provide some preliminaries for our framework in the context of contrastive learning. Contrastive
learning is a popular unsupervised method for learning data representations. In contrastive learning,
2
Under review as a conference paper at ICLR 2022
Table 1: A summary of common f -divergences. KL: Kullback-Leibler; JS: Jensen-Shannon; and
SH: Squared Hellinger. ForJS, We define 夕(U) = -(U + 1) log 1++u + U log u. For the Pearson χ2,
We take f *(t) = -1 if t ≤ -2. The Tsallis-α divergence is defined in Tsallis (1988) and we have
α > 1 for f -divergences. The Vincze-Le Cam (VLC) divergence can be found in Le Cam (2012,
p.47), Which is closely related to the Pearson χ2 and Hellinger divergences. For the Vincze-Le Cam
divergence we require -3 <t < 1 and f *(t) = -1 if t ≤ -3.
Divergence	f(u)	f *(t	f0(U)	f * ◦	f0(U)
KL	u log u	exp(t - 1)	log U + 1	U	g ɪ g 1+u
JS	夕(U)	- log(2 - et)	log2 + log ɪ	- lo	
Pearson χ2	(U- 1)2	t2/4+t	2(U- 1)	U2 -	1
SH	(√u - I)2	t 1-t	1 - U-1/2	U1/2	-1
Tsallis-α	Uα /(α - 1)	((a - l)t/a)"-1)	auα-1 a —1	Uα	
Vincze-Le Cam	(UT)2 u+1	4 - t - 4√Γ-i	1	4 (u+1)2	3-	4 u + 1
We expect that similar sample pairs to be close to each other in the embedding space While uncorre-
lated pairs tobe far away. Denote ppos as the distribution of positive pairs, i.e., two samples that have
similar representations. Assume that this distribution is symmetric w.r.t. the two random variables,
then the resultant two marginals both follow the data distribution pdata (Wang & Isola, 2020).
In this work we propose the f -mutual information framework for contrastive learning. First recall
the f -mutual information (f -MI) between a pair of random variables X and Y :
Definition 1 (f -mutual information, Csiszar 1967). Consider a pair ofrandom variables (X, Y)
with density function p(x, y). The f -mutual information If between X and Y is defined as
If(X； Y):= Df (p(x,y)kp(x)p(y)) = / f (Ppxxpyy)) p(x)p(y) ∙dλ(x,y),	(1)
where f : R+ → R is (closed) convex with f(1) = 0, and recall that p(x) and p(y) are the marginal
densities of p(x, y), whereas λ is a dominating measure (e.g. Lebesgue).
Common choices of f can be found in Table 1 and Table 4 (Appendix A), which we will discuss in
more details in §3. It is well-known that f -mutual information is non-negative and symmetric, and
provided that f is strictly convex, If(X; Y) = 0 iffX and Y are independent (Ali & Silvey, 1966).
When X and Y are of high dimension, it is quite challenging to estimate the f -divergence directly.
Instead, Nguyen et al. (2010) derived a variational method by maximizing the dual problem:
If(X ； Y ) ≥ sup if (X ； Y )：= E(x,y)〜ppos[T (x,y)] - E(x,y)〜PdataEata [f " (x,y))],⑵
T∈T
where f *(t) := suPχ∈R十(xt - f (x)) is the (monotone) Fenchel conjugate of f, and is always
monotonically increasing. Here T is a class of functions T : supp(pdata) X supp(pdata) → dom f *.
Nguyen et al. (2010) showed that the bound in eq. (2) is tight if there exists T* ∈ T such that
T *(x,y) = f0 (p(x,y)∕(p(x)p(y))) ,for any (x,y) ∈ supp(pdata) × supp(pdata),	(3)
and in particular, if T comprises of all (measurable) functions.
3	DESIGN OF f-MICL
Based on the f-MI introduced in §2, we propose a novel framework for contrastive learning. Fur-
thermore, we characterize the properties of alignment and uniformity theoretically for general f-
divergences. Following Chen et al. (2020), we design the strcture of function T as follows:
T(x, y) := k(g(x), g(y)), where kg(x)k = 1 for any sample x.	(4)
The function g produces a d-dimensional normalized feature encoding on the hypersphere Sd-1 and
k is a similarity function that measures the similarity between two embeddings g(x) and g(y). With
the above interpretation, we can rewrite our objective of f -mutual information, equation 2, as:
SuP if (X ； Y) := E(χ,y)~ppos [k(g(x),g(y))] - E(x,y)~Pdata 蜜 Pdata [f (k(g(x), g(y)))],	(5)
g∈G,k∈K
3
Under review as a conference paper at ICLR 2022
where G and K are the function classes of the feature encoder g and the similarity function k. We can
treat the first term as the similarity score between positive pairs in the feature space, and the second
term as the similarity score between two random samples, a.k.a. negative pairs, in the feature space.
As f * is increasing, maximizing f -MI is equivalent to simultaneously maximizing the similarity
between positive pairs and minimizing the similarity between negative pairs.
3.1	Optimized similarity function and implementation
Let us now study how to search for the optimal similarity function k . To our best knowledge, there
has been no theoretical study on the choice of similarity functions. Most existing contrastive learning
methods (e.g. Chen et al., 2020; Tsai et al., 2020; He et al., 2020) adopt a pre-designed similarity
function, such as the cosine similarity. For the ease of notation, from now on we define xg := g(x)
and yg := g(y). Suppose (x,y)〜ppos, then we denotePgos as the distribution of (Xg,yg), andPgata
as the marginal feature distribution of xg or yg . The corresponding density functions are written as
Pg(xg),Pg(yg) andPg(xg, yg). We remind the reader of the following result:
Lemma 2 (e.g., Nguyen et al. 2010, Lemma 1). Suppose f is differentiable, and the encoder function
g is fixed. The similarity function
k*(χg,yg) = f ( Pg (χg ,yg)))	⑹
Pg(xg)Pg(yg)
maximizes if (X; Y ) in eq. (5) as long as it is contained in the function class K.
Equation 6 provides an optimal similarity function, which nevertheless depends on the density func-
tions. Comparing equation 6 with equation 5, we realize that the optimal k * in fact gives the f-MI
on the feature space, If(g(X), g(Y )), which is a low bound of the original f -MI, If(X; Y ). To use
k* practically we make the following assumption on the joint density:
Assumption 3. The joint feature distribution is proportional to a radial basis function (RBF), i.e.,
Pg(Xg, yg) H P(∣∣xg — yg∣∣2) for a real-valuedfUnction 夕.
Radial basis functions are widely used in kernel methods (Powell, 1987; Murphy, 2012), and the
Gaussian kernel is perhaps the most well-known RBF. Throughout this work We mainly consider 夕
as a Gaussian kernel:
3(kxg — yg k2) = Gσ (kxg — yg ∣2) := μ exp (—kxg-? k2 ),	⑺
with μ a constant left to be determined. Fixing yg, then Pg(∙,yg) is known as the von Mises-Fisher
distribution (von Mises, 1918; Fisher, 1953; Bingham & Mardia, 1975), since Xg and yg are unit
vectors. With Assumption 3 on the joint density, the resultant marginal feature distribution Pgdata
is uniform on the hypersphere Sd-1, where d is the dimension of the feature space (see Prop. 8 in
App. B). Additionally, for positive pairs the distance in the feature space, ∣Xg — yg ∣, is more likely
to be small. If the variance σ2 → 0, then the Gaussian kernel becomes the Dirac delta distribution,
δxg =yg. This requires that the two features Xg and yg to be the same, which is desirable. In general,
the radial basis function 夕 should be decreasing since a positive pair should be more likely to be
adjacent in the feature space. For example, with 夕(t) = 1 一 t/2 we obtain the cosine similarity.
Based on Assumption 3 we propose the following similarity function between pairs of features:
Theorem 4 (Gaussian similarity). Under Assumption 3 with Gaussian kernels and the same set-
tings as Lemma 2, the optimal similarity function k* satisfies that for any Xg, yg ∈ Sd-1:
k*(Xg, yg) = f0(CGσ(∣Xg — yg∣2)),	(8)
where d is the feature dimension and C is an absolute constant.
For simplicity we will rewrite k* (Xg, yg) = f0 ◦ Gσ(∣Xg — yg ∣2) by absorbing the constant C
into Gσ, since we have left some flexibility in equation 7. Although Assumption 3 with Gaussian
kernels may not always reflect the real feature distribution, we can still use the similarity function
in equation 8, even if it might not be optimal. In our experiments in §5, the Gaussian similarity
equation 8 consistently outperforms the default cosine similarity in contrastive learning.
4
Under review as a conference paper at ICLR 2022
Algorithm 1: f -mutual information contrastive learning (f -MICL)
Input: batch size N, function f, weighting parameter a, constant μ (in Gσ), variance σ* 2 * * S * * *,
optimizer
1	for each batch {zi}iN=1 do
2	forall k ∈ [1, N] do
3	randomly sample two augmentation functions t`,t2
4	_ yk J t1(zk ), Xk J t2(zk )
5	COmPUte if = N Pi=I [f 0 ◦ Gσ (IIxg - yg ||2)] - N (Na-I) Pi= f * ◦ f 0 ◦ Gσ (IIxg -Xj k)
6	update g by taking a step to maximizing if using the optimizer
Bringing the optimal k* in equation 8 into our objective equation 5 we have the following objective:
SUP E(χ,y)〜p [f 0 ◦ Gσ (Ilxg - yg 12)] - E(x,y)〜PdataRPdata [f"。f'。Gb (IIxg 7 12)],⑼
g∈G
where Gσ is defined in equation 7. With a similar sampling method of positive and negative pairs in
Chen et al. (2020), given a batch of N samples we can estimate the objective in equation 9 as:
1N	1
bf (X; Y ) = N X f 0 ◦ Gσ (kxg -y k2) - N	X f * ◦ f 0 ◦ Gσ (kxg -xj k2),	(10)
N i=1	N(N-1) i6=j
where xi and yi are two different kinds of data augmentation of the i-th sample, and xi and xj are
different samples of the same kind of data augmentation.
With the objective in equation 10 we propose our algorithm for contrastive learning in Algorithm 1.
Note that we treat μ and σ2 in our Gaussian kernel equation 7 as hyperparameters. To balance the
two terms in our objective, we additionally include a weighting parameter α in front of the second
term. We can prove that rescaling the second term with the factor α is equivalent to changing the
function f to another convex function fα (see Prop. 7 in Appendix A).
3.2 Alignment and Uniformity
Notably, ifwe choose the f-divergence to be the KL divergence, the objective in equation 9 becomes:
1	g g 2	Ixg - yg I2
SuP - 2σ2 E(x,y)〜Ppos [kx，— y9 k ] - μ E(x,y)〜PdataeWata exP (	^2 jj， (II)
which retrieves the objective of the alignment and uniformity in Wang & Isola (2020). Specifically,
in equation 11 the first expectation is the same as Lalign, and the second expectation is the same
as Luniform in Wang & Isola (2020) (up to the logarithmic transformation). Therefore, under our
Assumption 3, we naturally decompose the objective for the KL into two: the first term leads to the
alignment of positive pairs while the second term results in the uniformity of data representations.
Compared to Theorem 1 of Wang & Isola (2020), our conclusion is based on the lower bound of
f-MI instead of the InfoNCE loss, and does not rely on the limit of infinite negative samples. More
importantly, besides the KL divergence we can extend our conclusion to various f -divergences.
Alignment Consider the objective in eq. (9). For the first term, since f is convex, the derivative f0 is
monotonic increasing. Therefore, in the ideal case, maximizing the first term would yield xg = yg
for all (x, y)〜ppos, i.e., similar sample pairs should have aligned representations.
Uniformity In Wang & Isola (2020) it is proved that for the KL divergence minimizing the second
expectation in equation 11 gives the uniform distribution, if we have infinite samples. Here we
discuss the general f -divergences with finite samples by considering equation 10, i.e., our empirical
estimation of the objective. We list some common choices of f -divergences in Table 1, and a more
detailed version can be found in Table 4 in Appendix A.1. We have the following theorem:
Theorem 5 (uniformity). Suppose that the batch size N satisfies 2 ≤ N ≤ d + 1, with d the
dimension of the feature space. If the real function
h(t) = f* ◦ f0 ◦ Gσ (t) is strictly convex on [0, 4],	(12)
5
Under review as a conference paper at ICLR 2022
then all minimizers of the second term of equation 10, i.e., Ei=j f * ◦ f 0 ◦ Gσ (∣xg - Xj ∣∣2), satisfy
the following condition: the feature representations of all samples are distributed uniformly on the
unit hypersphere Sd-1. AU f -divergences in Table 1 satisfy equation 12.
Here by “distributed uniformly” we mean that the feature vectors
form a regular simplex (see Figure 2), and thus the distances be-
tween all sample pairs are the same. It reflects our intuition that the
feature embeddings are evenly distributed. Although minimizing
the negative term gives uniformity, the positive term is also needed
for aligning similar pairs, as we observe in our experiments in §5.
Therefore, there is a tradeoff between alignment and uniformity. In
Theorem 5 we assumed N ≤ d+1, with d as the feature dimension.
This assumption is always satisfied in our experiments in §5. For
instance, for CIFAR-10 experiments we choose N = d = 512. Figure 2: A regUIar SimPleX
on a hypersphere.
Not all f -divergences lead to the property of uniformity. In Ap-
pendix A.1 we also listed some f -divergences that do not satisfy equation 12 and Theorem 5, such as
the Reversed Kullback-Leibler (RKL) and the Neynman χ2 divergences. Experimentally, We found
that these divergences generally result in feature collapse (i.e., all feature vectors are the same) and
thus poor performance in downstream applications. Compared to the existing literature (Nowozin
et al., 2016; Acuna et al., 2021) where no suggestion is given for the choice of f -divergences theo-
retically, with Theorem 5 we provide guidance for choosing desirable f -divergences.
4 ESTIMATION OF f -MUTUAL INFORMATION
In §3.1 we have provided the empirical estimation of our objective in equation 10. However, it
remains a question whether our estimation off-mutual information is consistent. In this part we give
an upper bound for the estimation error. Recall that our f -MICL objective is if(X; Y ) (equation 2)
and its empirical estimation isbif(X; Y ) (equation 10), with T (X, y) = f0 ◦ Gσ (∣Xg - yg ∣2).
Theorem 6 (estimation error). Suppose that the function T is taken from a function class T and
define Tx as the function class of T(x, ∙) given some X ∈ Supp(Pdata). Denote RN to be the
Rademacher complexity w.r.t. the distribution P with N i.i.d. drawn samples. Then for any T ∈ T,
the estimation error |if(X; Y ) - if(X; Y )| is upper bounded with probability at least 1 - δ:
2RpNos (T)+ 2μ
Ex~PdataRNata(Tx) + N XRN-aι(Txj)	+(rτ + 2rf)
i=1
l log6∕δ
V 2(N - 1),
(13)
with the constants rτ = f (μ) — f (μe-2∕σ2),rf = f* ◦ f (μ) — f* ◦ f (μe-2∕σ2).
Here the constant μ is from our Gaussian kernel in equation 7. Rademacher complexity evaluates
the richness of a class of real-valued functions regarding a probability distribution, and its formal
definition can be found in Koltchinskii (2001) (see also Definition 11 in Appendix B).
Note that the function class T depends on the class of the feature encoder g and the f -divergence.
The estimation error eq. (13) is composed of three parts: (1) the Rademacher complexity of the
function class T. In general, if T is richer then its Rademacher complexity is also larger. (2) the
expected Rademacher complexity of the one-side function class Tx and its empirical estimation; and
(3) an error term that decreases with the number of samples. Since the feature embeddings are usu-
ally neural networks, we can use existing theory (Bartlett et al., 2019) to give more detailed bounds
for the Rademacher complexities of T. Specifically, if the Vapnik-Chervonenkis (VC) dimension
of T is finite, then our estimation error in eq. (13) goes to zero as N → ∞ (Mohri et al., 2018).
Approximation and estimation tradeoff In order to minimize the estimation error in equation 13,
we should choose a simpler function class T to reduce the Rademacher complexities. However, T
should also be rich enough so that equation 3 can be satisfied, since our objective if(X; Y ) should
approximate the f -mutual information If(X; Y ) if we choose the optimal T. Therefore, there is a
natural tradeoff between approximation and estimation errors when we change the complexity of T.
It is worth mentioning that our conclusion in Theorem 6 is theoretically non-trivial since our sample
pairs are non-i.i.d.: although the individual samples are assumed to be i.i.d., the negative pairs are
not independently drawn (e.g., (X1, X2) and (X1, X3)), which makes the derivation challenging.
6
Under review as a conference paper at ICLR 2022
5 Experiments
We compare our framework with various frameworks on several popular vision and language
datasets, and show the wide applicability of our method. In particular, our f -MICL gives state-
of-the-art performance compared to popular baseline algorithms, such as SimCLR (van den Oord
et al., 2018; Chen et al., 2020), MoCo (He et al., 2020), Uniformity (Wang & Isola, 2020), and RPC
(Tsai et al., 2020). Additional experimental results can be seen in Appendix C.
Specifically, our results confirm the following:
•	Our f -MICL encourages alignment between positive pairs, and encourages dissimilar sam-
ple pairs to be equally far apart and thus leads to uniformity.
•	By replacing the cosine similarity with the Gaussian kernels, the performance is consis-
tently better across a variety of f -divergences in our f -MICL framework.
5.1	Experimental settings
For vision tasks, we use SGD with momentum as our optimizer, and apply the cosine learning
rate schedule (Loshchilov & Hutter, 2017). Regarding the neural network architecture, we employ
ResNets (He et al., 2020) as the feature encoders. For the language dataset, we use the BERT models
(Devlin et al., 2019) and Adam with weight decay as the optimizer (Loshchilov & Hutter, 2018). To
achieve fair comparison, we keep the network architecture and optimization the same, while only
changing the objective accordingly in our comparison. See Appendix C for more detailed settings.
5.2	Comparison with benchmarks
We compare with several state-of-the-art benchmarks in Table 2. Note that SimCLR and RPC use
the cosine similarity while we use the proposed Gaussian similarity.
Vision task Our vision datasets include CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009), STL-10
(Coates et al., 2011), TinyImageNet (Chrabaszcz et al., 2017) and ImageNet (Deng et al., 2009) for
image classification. After learning a feature embedding, we evaluate the quality of representation
using the test classification accuracies via a linear classifier. We observe from Table 2 that our
proposed f-MICL consistently outperforms the benchmarks across all datasets. Specifically, we
find that the JS divergence is superior in general, especially on the large datasets.
Language task To show the efficacy of our f -MICL framework, we also conduct experiments on a
natural language dataset: English Wikipedia (Gao et al., 2021). We follow the experimental setting
of Gao et al. (2021), which applies SimCLR with BERT models (Devlin et al., 2019; Liu et al.,
2019). The application task is semantic textual similarity (STS, Agirre et al. 2013) and we report
the averaged the Spearman’s correlation in Table 2 for comparison. We can see that our f -MICL
performs better or on par with the benchmarks, especially for the KL divergence.
5.3	Ablation study
To develop a thorough understanding of f -MICL we also perform the ablation study regarding the
choice of the similarity function, the design of the f -divergences, and the batch size. More ablation
study can be found in Appendix C.
Cosine similarity vs. Gaussian In Table 3 We compare the cosine and Gaussian similarities for
different f -divergences on CIFAR-10. It can be seen that under our f -MICL framework the Gaus-
sian similarity consistently outperforms the cosine similarity for various f -divergences. This agrees
with our Theorem 2 and equation 8, and also implies the validity of Assumption 3.
Non-satisfying f -divergences We have shown analytically that not all f -divergences are proper in
our f-MICL framework (see the proof of Theorem 5, Appendix B). For example, in our experiments
on CIFAR-10 and CIFAR-100, the RKL and Neyman χ2 do not perform well: the test classification
accuracies on these two datasets are 10.00% and 1.00%, respectively, which means that the classifier
simply outputs the random guess. Moreover, we observe from Figure 3 that for the non-satisfying
f -divergences such as the RKL, the features collapse to a constant. These observations emphasize
the importance of choosing proper f -divergences and confirms our conclusion in Theorem 5.
7
Under review as a conference paper at ICLR 2022
Table 2: Test classification accuracy (%) on the vision datasets. For the Wikipedia dataset we
evaluate the semantic textual similarity (STS) via the Spearman’s correlation. For ImageNet we
train for 100 epochs with batch size 256 due to computation limit. See Appendix C for detailed
experimental settings.
Dataset	Baselines				f -MICL			
	MoCo	SimCLR	Uniformity	RPC	KL	JS	Pearson	VLC
CIFAR-10	90.30±0.19	89.71±0.37	90.41 ±0.26	90.39±0.25	90.61±0.47	89.66 ±0.28	89.35 ±0.52	89.13 ±0.33
CIFAR-100	62.77±0.17	62.75±0.45	62.51±0.36	62.66±0.39	63.00 ±0.44	63.11±0.33	61.69±0.57	61.19±0.29
STL-10	83.69±0.22	82.97±0.32	84.44±0.19	82.41 ±0.14	85.33±0.39	85.94±0.17	82.64±0.37	83.27±0.72
TinyImageNet	35.72±0.17	30.56±0.28	41.20±0.19	34.95±0.25	39.46±0.20	42.98±0.18	38.45±0.54	38.65±0.45
ImageNet	58.59	57.66	59.12	56.11	58.91	61.11	55.33	54.26
Wikipedia	77.88±0.15	77.40 ±0.12	77.95±0.08	68.32 ±0.23	78.02±0.13	76.76±0.09	77.59 ±0.12	55.07±0.13
Table 3: Comparison between the cosine similarity and our Gaussian similarity on CIFAR-10 using
the test classification accuracy (%).
Similarity	KL	JS	Pearson	SH	Tsallis	VLC
Cosine	88.95	87.06	87.79	87.06	88.55	10.00
Gaussian	89.34 89.12		89.44	88.13	89.18	89.15
Sensitivity to batch size We study the sensitivity to the batch size of our f -MICL framework on
CIFAR-10. On the right panel of Figure 3, we evaluate the classification accuracy by varying the
batch size for different f -divergences and SimCLR. We can see that for all different batch sizes
and with proper choice of f -divergences, our performance is always better than SimCLR. In other
words, we require less negative samples to achieve the same performance.
5.4	Uniformity Test
To check the uniformity of feature vectors (Theorem 5) we plot the pairwise distance kxig - xjg k of
the feature representations within the same batch on CIFAR-10 and CIFAR-100. We compute the
distances between the normalized features of every pair from a random batch, and then sort the pairs
with the increasing order. From Figure 3 we can see that f -MICL gives nearly uniform distances for
dissimilar pairs (orange regions) on both datasets with various proper f -divergences. In contrast, a
random initialized model gives a less uniform distribution for dissimilar pairs. Besides, for f -MICL
we observe small pairwise distances for similar pairs (green regions). On CIFAR-100 there are less
similar pairs compared to CIFAR-10 as there are more classes.
Figure 3: (left and middle) Distances between pairs of normalized features within a batch. Green
region: similar pairs. Orange region: dissimilar pairs. f -MICL gives nearly uniform distances for
dissimilar pairs for the f -divergences in Table 1. For non-satisfying f -divergences such as the RKL,
the features collapse to a constant and thus the distances are zero. (right) The test classification
accuracy v.s. the batch size after training 200 epochs for all algorithms.
8
Under review as a conference paper at ICLR 2022
6 Related works
Contrastive learning Self-supervised contrastive learning learns representations by encouraging
the contrastiveness between positive pairs and negative pairs. Recently it has been shown analyt-
ically that improving the contrastiveness can benefit the downstream applications (Saunshi et al.,
2019; Tosh et al., 2021). For popular contrastive learning methods such as Contrastive Predictive
Coding (CPC) (van den Oord et al., 2018), SimCLR (Chen et al., 2020), and MoCo (He et al.,
2020), their loss functions can be interpreted as a lower bound of mutual information, which is
essentially the KL divergence between the joint distribution ppos and the product of margin distribu-
tions Pdata 0 Pdata. Besides the KL divergence, other statistical divergences or distances have been
individually studied under the context of contrastive learning, e.g., the Wasserstein distance (Ozair
et al., 2019), Pearson χ2 divergence (Tsai et al., 2021), and JenSen-Shannon divergence (Hjelm
et al., 2018). Our paper differs from Hjelm et al. (2018); Tsai et al. (2021) in the following ways:
•	Instead of treating f -divergences as estimators of mutual information, we directly consider
f -mutual information, which builds solid foundation for our objective.
•	Instead of using the default cosine similarity, we challenge this routine and construct the
optimal design with the Gaussian kernel, which consistently performs better (Table 3).
•	We show that optimizing the f -MICL objective of the contrastive learning yields alignment
and uniformity for proper f -divergences with finite samples.
Uniformity Our work is inspired by Wang & Isola (2020), which shows the alignment and uni-
formity in contrastive learning. However, Wang & Isola (2020) focuses on the InfoNCE loss and
the loss function Luniform they consider is somewhat ad hoc. In contrast, our f -MICL objective is
directly built from the f -mutual information theory. Another difference is that Wang & Isola (2020)
proves uniformity in the limit of infinite samples, while our Theorem 5 only needs finite samples.
f -divergences have been widely used in, e.g., generative models (Nowozin et al., 2016), variational
inference (Wan et al., 2020), and domain adaptation (Acuna et al., 2021), for measuring the discrep-
ancy of two distributions. Compared to the existing work such as f -GAN (Nowozin et al., 2016)
which evaluates the similarity of two marginal distributions (i.e., the real and the modeled data dis-
tributions), our f -MICL objective measures the similarity between the joint distribution and the
product of data distributions. Furthermore, while f -GAN aims to learn a generative model and thus
minimizes the f -divergence, our f -MICL intends to encourage the contrastiveness and maximizes
the f -divergence. Finally, we provide a theoretical criterion for choosing proper f -divergences.
Metric learning Our work is closely related to metric learning (Kaya & Bilge, 2019; Suarez-Dlaz
et al., 2018), which aims to learn a distance metric bringing similar objects closer and distancing
dissimilar objects further. Distance metrics that are designed heuristically or independently of the
problem may not lead to a satisfactory performance in applications. In contrastive learning, a pre-
defined similarity metric, e.g., the cosine similarity (Chen et al., 2020; He et al., 2020) or a bilinear
function (van den Oord et al., 2018; Tian et al., 2020a; Henaff et al., 2020) is commonly employed to
measure the similarity of sample pairs. Comparably, we have derived an optimal similarity function
assuming that the joint positive distribution is proportional to a Gaussian kernel. 7
7 Conclusion and future work
In this work we proposed f -MICL, a novel contrastive learning framework with a generalization
of mutual information, called f -mutual information (f -MI). Our objective corresponds to the vari-
ational bound of f -divergences. By making the assumption that the joint feature distribution is
proportional to a Gaussian kernel, we naturally characterized the properties of the alignment and
uniformity. Our f-MI based objective can be well estimated with finite samples. Experimentally,
we showed the efficacy of f -MICL across a wide array of datasets and the advantage of using the
Gaussian similarity. Our results imply the following: (1) even though mutual information is widely
used in contrastive learning, generalization to f-MI can bring us better performance; (2) the cosine
similarity, the de facto option in contrastive learning, can be replaced with more effective similar-
ity functions such as the Gaussian similarity. As our work shows interesting connection between
contrastive learning and kernel methods, it would be promising to explore the usage of more RBF
kernels in contrastive learning.
9
Under review as a conference paper at ICLR 2022
Reproducibility S tatement
Our novel f -MICL objectives are implemented in anonymous downloadable source code as in the
supplementary. For theoretical results, clear explanations of any assumptions and complete proofs
are included in Appendix A and Appendix B. For detailed experimental settings and complete ex-
perimental results, the complete descriptions can be found in Appendix C.
References
David Acuna, Guojun Zhang, Marc T Law, and Sanja Fidler. f -domain-adversarial learning: Theory
and algorithms. In International Conference on Machine Learning, 2021.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo. *SEM 2013 shared
task: Semantic textual similarity. In Second joint conference on lexical and computational seman-
tics (*SEM), volume 1: proceedings of the Main conference and the shared task: semantic textual
similarity,pp. 32-43, 2013.
Syed Mumtaz Ali and Samuel D Silvey. A general class of coefficients of divergence of one distri-
bution from another. Journal of the Royal Statistical Society: Series B (Methodological), 28(1):
131-142, 1966.
Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximiz-
ing mutual information across views. Advances in Neural Information Processing Systems, 32:
15535-15545, 2019.
Peter L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight VC-dimension
and pseudodimension bounds for piecewise linear neural networks. The Journal of Machine
Learning Research, 20(1):2285-2301, 2019.
MS Bingham and KV Mardia. Maximum likelihood characterization of the von Mises distribution.
In A Modern Course on Statistical Distributions in Scientific Work, pp. 387-398. Springer, 1975.
Sergiy V Borodachov, Douglas P Hardin, and Edward B Saff. Discrete energy on rectifiable sets.
Springer, 2019.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In Proceedings of the 37th International Conference
on Machine Learning, pp. 1597-1607, 2020. URL http://proceedings.mlr.press/
v119/chen20j.html.
Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. A downsampled variant of ImageNet as an
alternative to the CIFAR datasets. arXiv preprint arXiv:1707.08819, 2017.
Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised
feature learning. In Proceedings of the fourteenth international conference on artificial intelli-
gence and statistics, pp. 215-223. JMLR Workshop and Conference Proceedings, 2011.
Imre Csiszar. Information-type measures of difference of probability distributions and indirect ob-
servation. studia scientiarum Mathematicarum Hungarica, 2:229-318, 1967.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hier-
archical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,
pp. 248-255. IEEE, 2009.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, 2019.
Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components
estimation. arXiv preprint arXiv:1410.8516, 2014.
10
Under review as a conference paper at ICLR 2022
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. In
International Conference on Learning Representations, 2017.
Ronald Aylmer Fisher. Dispersion on a sphere. Proceedings of the Royal Society of London. Series
A. Mathematical and Physical Sciences, 217(1130):295-305, 1953.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence
embeddings. In Empirical Methods in Natural Language Processing (EMNLP), 2021.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 9729-9738, 2020.
Olivier J Henaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, SM Eslami, and
Aaron van den Oord. Data-efficient image recognition with contrastive predictive coding. In
International Conference on Machine Learning, pp. 4182-4192. PMLR, 2020.
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. In International Conference on Learning Representations, 2018.
Mahmut Kaya and Hasan Sakir Bilge. Deep metric learning: A survey. Symmetry, 11(9):1066,
2019.
Diederik P Kingma and Prafulla Dhariwal. Glow: generative flow with invertible 1× 1 convolutions.
In Proceedings of the 32nd International Conference on Neural Information Processing Systems,
pp. 10236-10245, 2018.
Vladimir Koltchinskii. Rademacher penalties and structural risk minimization. IEEE Transactions
on Information Theory, 47(5):1902-1914, 2001.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images,
2009. Technical report.
Lucien Le Cam. Asymptotic methods in statistical decision theory. Springer Science & Business
Media, 2012.
Ralph Linsker. Self-organization in a perceptual network. Computer, 21(3):105-117, 1988.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pre-
training approach. arXiv preprint arXiv:1907.11692, 2019.
Lajanugen Logeswaran and Honglak Lee. An efficient framework for learning sentence representa-
tions. In International Conference on Learning Representations, 2018.
Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. Interna-
tional Conference on Learning Representations, 2017.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-
ence on Learning Representations, 2018.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning.
MIT press, 2018.
Kevin P Murphy. Machine learning: a probabilistic perspective. MIT press, 2012.
XuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals
and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory,
56(11):5847-5861, 2010.
11
Under review as a conference paper at ICLR 2022
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f -GAN: Training generative neural sam-
plers using variational divergence minimization. In Proceedings of the 30th International Con-
ference on Neural Information Processing Systems, pp. 271-279, 2016.
Sherjil Ozair, Corey Lynch, Yoshua Bengio, Aaron van den Oord, Sergey Levine, and Pierre Ser-
manet. Wasserstein dependency measure for representation learning. In Advances in Neural
Information Proceeding Systems, pp. 15604-15614, 2019.
M. J. D. Powell. Radial Basis Functions for Multivariable Interpolation: A Review, pp. 143-167.
Clarendon Press, USA, 1987. ISBN 0198536127.
Ralph Rockafellar. Characterization of the subdifferentials of convex functions. Pacific Journal of
Mathematics, 17(3):497-510, 1966.
Igal Sason and Sergio Verdu. f -divergence inequalities. IEEE Transactions on Information Theory,
62(11):5973-6006, 2016.
Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar.
A theoretical analysis of contrastive unsupervised representation learning. In Proceedings of
the 36th International Conference on Machine Learning, pp. 5628-5637, 2019. URL http:
//proceedings.mlr.press/v97/saunshi19a.html.
Aravind Srinivas, Michael Laskin, and Pieter Abbeel. CURL: Contrastive unsupervised representa-
tions for reinforcement learning. In International Conference on Machine Learning, 2020.
Juan Luis SuOrez-Diaz, Salvador Garcia, and Francisco Herrera. A tutorial on distance metric learn-
ing: Mathematical foundations, algorithms, experimental analysis, prospects and challenges (with
appendices on mathematical background and detailed algorithms explanation). arXiv preprint
arXiv:1812.05944, 2018.
Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In Computer
Vision-ECCV2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings,
Part XI 16, pp. 776-794. Springer, 2020a.
Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What
makes for good views for contrastive learning. In Advances in Neural Information Processing Sys-
tems 33 (NeurIPS 2020), 2020b. URL https://proceedings.neurips.cc//paper_
files/paper/2020/hash/4c2e5eaae9152079b9e95845750bb9ab-Abstract.
html.
Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive learning, multi-view redun-
dancy, and linear models. In Algorithmic Learning Theory, pp. 1179-1206. PMLR, 2021.
Yao-Hung Hubert Tsai, Han Zhao, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhut-
dinov. Neural methods for point-wise dependency estimation. arXiv preprint arXiv:2006.05553,
2020.
Yao-Hung Hubert Tsai, Martin Q Ma, Muqiao Yang, Han Zhao, Louis-Philippe Morency, and Rus-
lan Salakhutdinov. Self-supervised representation learning with relative predictive coding. In
International Conference on Learning Representations, 2021.
Constantino Tsallis. Possible generalization of Boltzmann-Gibbs statistics. Journal of statistical
physics, 52(1):479-487, 1988.
Michael Tschannen, Josip Djolonga, Paul K. Rubenstein, Sylvain Gelly, and Mario Lucic. On
mutual information maximization for representation learning. In International Conference on
Learning Representations, 2020.
Jean-Baptiste Hiriart Urruty and Claude Lemarechal. Convex analysis and minimization algorithms.
Springer-Verlag, 1993.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018.
12
Under review as a conference paper at ICLR 2022
Richard von Mises. Uber die “ganzzahligkeit” der atomgewicht und verwandte fragen. Phys. Z., 19:
490-500,1918.
Neng Wan, Dapeng Li, and Naira Hovakimyan. f -divergence variational inference. Advances in
Neural Information Processing Systems, 33, 2020.
Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through align-
ment and uniformity on the hypersphere. In International Conference on Machine Learning, pp.
9929-9939. PMLR, 2020.
Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-
parametric instance discrimination. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 3733-3742, 2018.
Guojun Zhang, Han Zhao, Yaoliang Yu, and Pascal Poupart. Quantifying and improving transfer-
ability in domain generalization. arXiv preprint arXiv:2106.03632, 2021.
13
Under review as a conference paper at ICLR 2022
Table 4: A summary of common f -divergences. KL: Kullback-Leibler; JS: Jensen-Shannon; SH:
Squared Hellinger. For JS, We define 夕(U) = -(U + 1) log 1+u + U log u. For Pearson χ2, We
take f (t) = -1 if t ≤ -2. For Jeffrey, C = W + W-1 and W(∙) is the Lambert-W product log
function. The Tsallis-α divergence is defined in Tsallis (1988) and We have α > 1 for f -divergences.
We ignore constant addition -1∕(α - 1) because it does not change the optimization problem. The
Vincze-Le Cam divergence can be found in (p.47, Le Cam, 2012) Which is closely related to χ2 and
Hellinger divergences. For the Vincze-Le Cam divergence we require -3 < t < 1 and f *(t) = -1
ift ≤ -3.
Divergence	f(u)	f *(t	f0(U)	f * ◦ f0(u)	
KL	u log u	exp(t - 1)	log U + 1	U
Reverse KL	- log u	-1 - log(-t)	—1/U	log U — 1
JS	夕(U)	- log(2 - et)	log2 + log ι+Uu	— log 2 + log(1 + U)
Pearson χ2	(U- 1)2	t2/4+t	2(U—1)	U2 — 1
SH	(√u - I)2	t 1-t	1 — U-1/2	U1/2 — 1
Neyman χ2	(1-u)2 u	2 — 2√T-1	1 — U-2	2 — 2U—1
Jeffrey	(U - 1) log U	Wc(e1-t) + t- 2	1 — U-1 + log U	c(e1/u/u)+log u — 1+u
Tsallis α	uα∕(α — 1)	((α — 1)t/a)a/(a-1)	auα-1 a —1	Uα
Vincze-Le Cam	(UT)2 u+1	4 — t — 4√Γ-i	(u—1)(u + 3) (u+1)2			3	4- 3	u + 1
A Additional theoretical results
In this appendix We provide additional theoretical results, including additional f -divergences and
the theory for Weighting parameters.
A.1 ADDITIONAL f-DIVERGENCES
We expand Table 1 and give more examples of f -divergences in Table 4. As We Will see in the proof
of Theorem 5, Table 1 gives a special class of f -divergences that guarantees uniformity. A detailed
description of f -divergences can be found in e.g. Sason & Verdu (2016).
A.2 Weighting parameters
In Algorithm 1 We added a Weighting parameter α to balance the alignment and uniformity. We
prove that even after adding this parameter We are still maximizing the f -mutual information, al-
though With respect to a different f.
Proposition 7 (weighting parameter). Given α > 0 and a closed convex function f : R+ → R
such that f (1) = 0, define fα : adom f → R with fα(x) = αf (x∕α) — αf (1∕α) for any X ∈
dom f. Then Ifα is still a valid f -mutual information (see Definition 1). Besides, by replacing f
with fα in equation 9 we have the following optimization problem:
SUp E(χ,y)〜Ppos [f0 (Gσ (Ilxg-yg k2"α)] - αE(x,y)〜PdataOPdata [f"。f' (Gb (IIxg - W F"。)],
g∈G
where Gσ(Ilxg — ygk2) = μ exp (- kxg-ygk ) is the Gaussian kernel.
Note that α dom f means the scalar multiplication of a set Which is applied element-Wisely. Ac-
cording to Definition 1, fα is also a valid f -divergence. This proposition tells us that rescaling the
second term With factor α is equivalent to changing the function f to another convex function fα .
The transformation from f to αf (x∕α) is also knoWn as right scalar multiplication (e.g. Chapter X,
Urruty & Lemarechal, 1993). Let us now move on to our proof:
Proof. By definition We knoW that fα is convex and closed With fα(1) = 0, and thus Ifα is a valid
f -mutual information according to Definition 1. Moreover, we have fα0 (x) = f0(x∕α) for any
14
Under review as a conference paper at ICLR 2022
x ∈ α dom f and
fα (t) =	SUp Xt- fα(x)
x∈dom fα
= sup Xt — af (x∕ɑ) + αf (1∕α)
x∈αdom f
= sup (x∕α) ∙ (at) — αf (x∕α)+ αf(1∕α)
x∕α∈dom f
=α sup ((x∕a) ∙ t — f (x∕α)) + αf(1∕α)
x∕α∈dom f
=αf *(t) + αf (1∕α),	(A.1)
where in the last line We used the definition of f * (t). Plugging f∖ and fα into equation 9 yields the
desired result.	口
A.3 Uniform distributions
In the following we prove that under Assumption 3 the marginal feature distribution pdgata is uniform.
Proposition 8. Under Assumption 3, the marginal feature distribution pdgata is uniform on the hy-
persphere Sd-1, with d the dimension of the feature space.
Proof. Under Assumption 3 we have:
Pg (Xg ,yg ) = C0 exp bkx⅛"),
where C0 is a normalizing constant. Integrating yg we have the marginal distribution for Xg :
Pg(Xg) = £ ɪ Co exp (—kxg-σy k2 ) dyg.
(A.2)
(A.3)
It suffices to show that Pg (Xg1) = Pg(X2g) for any Xg1, X2g ∈ Sd-1. Suppose Q is the orthogonal matrix
such that:
QX1g = Xg2 .	(A.4)
Such a matrix Q always exists and constructing Q is not difficult. For example, assume that {Xg1, Xg2}
span a plane with an orthonormal basis e1, e2, and
xg = cos θι ∙ eι + Sin θι ∙ e2,
cos θ2 ∙ eι + sin θ2 ∙ e2,
then Q can take the following form:
Q = [e1 e2]
cos(θ2 — θ1 )
sin(θ2 — θ1)
— sin(θ2 — θ1 ) e1>
cos(θ2 — θ1) e2>
(A.5)
(A.6)
such that Q is orthogonal and satisfies QXg1 = Xg2. Hence we have:
Pg (Xg) = ZSd-I C0 exp (-kx1 -y I')dyg
C0 exp
Sd-1
dyg
∣∣Q>x2 — Q>zg∣∣2
2σ2
C0 exp
Sd-1
d(Q>zg)
C0 exp
Sd-1
∣x2 — Zg I2)
2σ2	)
Pg(Xg2),
(A.7)
where in the second line we used equation A.4; in the third line we made the transformation yg =
Q>zg with zg a unit vector; in the fourth line we used the fact that applying a orthogonal matrix
does not change the norm and that the corresponding Jacobian determinant is one.
In fact, the proof above can be generalized from the Gaussian kernel to any radial basis functions,
by replacing the Gaussian kernel with 夕(IlXg — yg ∣∣2), and repeating the same proof. Here 夕 can be
any function such that the integral JSd-I 夕(IlXg — yg∣2)dyg is finite.	口
15
Under review as a conference paper at ICLR 2022
B Proofs
Lemma 2 (e.g., Nguyen et al. 2010, Lemma 1). Suppose f is differentiable, and the encoder function
g is fixed. The similarity function
k*(χg ,yg)=f 0 (P (x； ,yg)))	(6)
pg(xg)pg(yg)
maximizes if (X; Y ) in eq. (5) as long as it is contained in the function class K.
Proof. From Definition 1, we are computing the following supremum:
sup Z p P (XJy L、k(xg ,yg )- f ◦ k(xg ,yg )) dpgata 0 Pgata.	(B∙D
g,k	pg (xg)pg (yg )	data data
Suppose k is unconstrained and we fix g. The optimal solution should satisfy:
PPgX)；Rg) ∈ (∂f*)(k*(xg,yg)),	(B.2)
pg (x )pg (y )
almost surely for (x, y)〜Pdata 0 Pdata. From (3.11) of Rockafellar (1966) this is equivalent to:
k*(xg,yg) ∈ ∂f ( Pg(Xg,yg) ) .	(B.3)
Pg(Xg)Pg (yg)
If f is differentiable, then for any U ∈ dom f, ∂f (u) = {f 0(u)} is a singleton.	□
Theorem 4	(Gaussian similarity). Under Assumption 3 with Gaussian kernels and the same set-
tings as Lemma 2, the optimal similarity function k* satisfies thatfor any Xg, yg ∈ Sd-1:
k*(χg,yg) = f(CGσ(kxg - ygk2)),	(8)
where d is the feature dimension and C is an absolute constant.
Proof. Simply combine Proposition 8 with Lemma 2.
□
Theorem 5	(uniformity). Suppose that the batch size N satisfies 2 ≤ N ≤ d + 1, with d the
dimension of the feature space. If the real function
h(t) = f * ◦ f 0 ◦ Gσ (t) is strictly convex on [0,4],	(12)
then all minimizers of the second term Ofequation 10, i.e., Ei=j f * ◦ f 0 ◦ Gσ (Ilxg 一 Xj ∣∣2), satisfy
the following condition: the feature representations of all samples are distributed uniformly on the
unit hypersphere Sd-1. All f -divergences in Table 1 satisfy equation 12.
Proof. From the definition of h itis clear that h is decreasing since f* and f0 are both monotonically
increasing white Gσ is decreasing. Using h we rewrite the second term of equation 10 as
min
g g ∈Sd-1
x1 ,...,xN ∈
h(∣Xig-Xjg∣2).
i,j
(B.4)
When N ∈ [2, d + 1], there exists a neat characterization of the minimizers, see e.g. Borodachov
et al. (2019, Theorem 2.4.1). We include the proof below for completeness.
16
Under review as a conference paper at ICLR 2022
Apply Jensen’s inequality, we have:
N X h(kxi - Xj k2) ≥ h ( N X kxi - xj F
i,j	i,j
≥h(2),
(B.5)
where in the first line we used Jensen’s inequality; in the third line we used kxi k = kxj k = 1 for
any i, j ∈ [N]; in the last line we note that k PiN=1 xi k ≥ 0 and h is a decreasing function. When
h is strictly convex and decreasing, it is in fact strictly decreasing, and hence the two inequalities
above can be attained iff
i
and kxi - xj k2 ≡ c for all i 6= j,
(B.6)
namely that {x1, . . . , xN} form a regular simplex with its center at the origin. We remark that
when h is merely convex, points forming a centered regular simplex may form a strict subset of the
minimizers.
To see the necessity of N ≤ d + 1, let us note that
>	1, i=j
xi xj = 1-N-1, i= j,
since
2N	2
E kxi - xjk = 2N = N(N - I)C =⇒ C = N - 1 = 2 + N - 1
ij
(B.7)
(B.8)
Performing simple Gaussian elimination we note that the matrix X>X has rank N - 1 where
X = [x1, . . . , xN] ∈ Rd×N. Therefore, we must have N - 1 ≤ d.
Lastly, we need to show when h is a (strictly) convex function, which may not always be true
depending on the f -divergences. We give the following characterization (We ignore the constants μ
and 2σ2 in equation 7 as they do not affect convexity):
•	h strictly convex: hKL(t) = e-t, hJS(t) = log(1 + e-t) - log 2, hPearson(t) = e-2t - 1,
hsH(t) = e-t/2 — 1, hτsaiiis(t) = e-αt, hvLC = 3 — ι+e-t；
•	h convex but not strictly convex: Arkl (t) = -t 一 1 (RKL stands for Reversed KUllbaCk-
Leibler, see Appendix A.1);
•	h concave: hNeyman(t) = 2 - 2et (Neyman stands for Neyman χ2, see Appendix A.1).
Only for the last case we do not have the guarantee that the minimizing configurations could form
a regular simplex. For RKL, in fact, any configuration that centers at the origin suffices since h is a
linear function.	□
Theorem 6 (estimation error). Suppose that the function T is taken from a function class T and
define Tx as the function class of T(x, ∙) given some x ∈ Supp(Pdata). Denote RN to be the
17
Under review as a conference paper at ICLR 2022
Rademacher complexity w.r.t. the distribution P with N i.i.d. drawn samples. Then for any T ∈ T,
.1	. ∙	. ∙	I ■ / ΛΛ ΛΛ∖ ^ / ΛΛ ΛΛ∖ I -	1	77 ■ . 1	1 1 ∙1∙ .	. 1	. t	Γ
the estimation error |if(X; Y ) - if(X; Y )| is upper bounded with probability at least 1 - δ:
2RpNos (T ) + 2μ
Ex 〜Pdata RNr (Tx) + NN X RN-i(TXj) + (TT + 2rf )j2l0N-δ)
(13)
with the constants rτ = f0(μ) — f0(μe-2∕σ2),Tf = f* ◦ f(μ) — f* ◦ f0(μe-2∕σ2).
Proof. Our proof uses the following notion of Rademacher complexity:
Definition 11 (Rademacher complexity, Koltchinskii (2001)). Let F be a family of functions from a
subset of Euclidean space W to [a, b] and S = (w1, . . . , wm) a fixed sample of size m with elements
in W. The empirical Rademacher complexity of F w.r.t. the sample set S is defined as:
R S (F )= Eσi
1m
sup —兄 σf (Wi)
f∈F mi=1
(B.9)
where σi ’s are independent uniform random variables taking values {—1, +1}. Let P be the un-
derlying distribution of samples. The Rademacher complexity Rm (F) of function class F is the
expectation over sample sets:
Rm(F )= ES 〜P m R S (F)].
(B.10)
With the definition of Rademacher complexity we have the following lemma:
Lemma 12. Let F be a family of functions from a subset of Euclidean space W to [a, b] with b > a.
Then for any 0 < δ < 1 with probability at least 1 — δ, the following holds for any f ∈ F:
|Ew〜Pw [f(w)] — ɪ XX f (Wi)I ≤ 2Rmw (F) + (b — aUlogW,	(B.11)
w	m	m	2m
i=1
with Wi i.i.d. examples from any distribution Pw.
Proof. Our proof follows from Lemma 11 of Zhang et al. (2021). For any function f ∈ F we can
construct f = (f — a)/(b — a) such that f : W → [0, 1]. The rest follows from the linearity of
Rademacher complexity.
Let us first recall our definition equation 4: T (x, y) := k(g(x), g(y)). From the definitions of
■	/	，广、	t ^	/	，广、	∙ . .ι Cll	♦
if (X; Y ) and if(X; Y ) we write the following:
.	,   ^ ,	___ -	- - ,	、r	_	—	________ 一 r
|if (X; Y) — if (X; Y )| = E(x,z)〜Ppos[T (x,z)] — Ex~Pdata Ey~Pdata[f*(T(x, y))]—
-H XXT(Xi,Zi) - N(NTI) X f*(T(Xi,xj)))|
1	N
≤ E(x,z)〜Ppos[T (X,z)] — N^JT (Xi,zi) +
+ Ex 〜PdataEy 〜Pdata [f * (T (x,y))] - N-	X f * (T (Xi,Xj )),
i6=j
(B.12)
where we used the triangle inequality. With Lemma 12, we can upper bound the first term of
equation B.12 as:
2RpNos (T) + (bτ — aτ)ʌ/"BNr)，WpI- δ,	(B.13)
18
Under review as a conference paper at ICLR 2022
where [aT, bT] is the range of T. Now we upper bound the second term of equation B.12. Note
that we cannot apply Lemma 12 to equation B.12 directly since (xi, xj)’s are not i.i.d. samples. For
instance, the pairs (x1, x2) and (x1, x3) are not independent since they share the same x1. However,
all xi’s are i.i.d. sampled. We can upper bound the second term of equation B.12 as:
Ex 〜Pdata Ey 〜Pdata f YT W) - N (N- I) X f " Mj ))
i6=j
1N
Ex~Pdata Ey~Pdata f"(T (X,y)) - Ex~Pdata N ^^f " (T (X,xj )) +
j=1
11
+Ex〜PdataN Ef"(T(x,Xj)) - N(N- I) Ef"(T(Xi,Xj)) ≤
1 N
Ex 〜Pdata Ey 〜Pdata f " (T (x,j)) - Ex 〜Pdata Nff (T (x,Xj )) +
j=1
1 N 1
+ Ex〜PdataN Ef"(T(x,Xj))- N(N- I) Ef"(T(Xi,Xj)).
j=1	i6=j
The first term of equation B.14 can be upper bounded as:
Ex〜Pdata	Ey〜Pdataf " (T V X f " (TK
≤ 2Ex〜PdataRNata(f" ◦ Tx) + (bf - af)∕ogNδ)
≤ 2LfEx〜PdataRNata(Tx) + (bf - af )∕ogNδ)
(B.14)
(B.15)
with probability at least 1 - δ, where we used Lemma 12 in the first line and Talagrand’s lemma (e.g.,
Lemma 5.7 of Mohri et al. (2018)) in the second line. We denote Lf as the Lipschitz constant of f"
and [af, bf] as the range of f" ◦ T. Similarly, we can upper bound the second term of eq. (B.14) as:
N XRN-i(Txj) + (bf- af )j!N斗
(B.16)
with probability at least 1 - δ.
Absolute constants Let us compute the constants af, bf, aT , bT, Lf. First note that our feature
embeddings are normalized. For any Xg , yg ∈ Sd-1 we have:
0≤kXg-ygk2≤2(kXgk2+kygk2)=4.	(B.17)
From the expression of Gaussian similarity equation 7 and the monotonicity of f0 and f", we have:
bτ = f0(μ),aτ = f'Ge-2"?),bf = f" ◦ f0(μ),af = f" ◦ f'Ge-2").	(B.18)
For simplicity we will write rT = bT - aT and rf = bf - af. We can also compute the Lipschitz
constant of f" , which is bounded by the norm of the gradient:
Lf ≤ SUp (f")0 ◦ f0 ◦ Gσ(kχg - ygk2) = SUp	Gσ(∣∣χg - ygk2) = μ, (B.19)
xg,yg ∈Sd-1	xg,yg ∈Sd-1
where we used the equality (f")' ◦ f' = id (Rockafellar, 1966), with id the identity function.
Combining equation B.13, equation B.15 and equation B.16 we can use the union bound to finish
the proof.	□
19
Under review as a conference paper at ICLR 2022
C Additional experimental results
We present additional experiment details in this appendix, to further support our experiments in the
main paper.
C.1 Implementation details
In this paper, we follow the implementations in SimCLR (https://github.com/sthalles/
SimCLR). For vision tasks, we use ResNet (He et al., 2016) as the feature encoder, and we adopt
the similar procedure of SimCLR for sampling. For the language dataset, we follow the exact
experimental setting of Gao et al. (2021) and only change the objective. Our experimental settings
are detailed below:
•	Hardware and package: We train on a GPU cluster with NVIDIA T4 and P100. The plat-
form we use is pytorch. Specifically, the pairwise summation can be easily implemented
using torch.nn.functional.pdist from pytorch.
•	Datasets: the datasets we consider include CIFAR-10, CIFAR-100 (Krizhevsky et al.,
2009), STL-10 (Coates et al., 2011), TinyImageNet (Chrabaszcz et al., 2017), ImageNet
(Deng et al., 2009) and English Wikipedia (Gao et al., 2021).
•	Augmentation method: For each sample in a dataset we create a sample pair, a.k.a. posi-
tive pair, using two different augmentation functions. For image samples, we choose the
augmentation functions to be the standard ones in contrastive learning, e.g., in Chen et al.
(2020) and He et al. (2020). The augmentation is a composition of random flipping, crop-
ping, color jittering and gray scaling. For text samples, following the augmentation method
of Gao et al. (2021) we use dropout masks.
•	Neural architecture: For CIFAR-10 and CIFAR-100 we use ResNet-18 (He et al., 2016);
for STL-10, TinyImageNet and ImageNet we use ResNet-50 (He et al., 2016); for the
Wikipedia dataset we use BERTbase (Devlin et al., 2019).
•	Batch size and embedding dimension: for experiments in CIFAR-10 and CIFAR-100 we
choose batch size 512, and for STL-10 we choose batch size 64 to accommodate one GPU
training. Finally, for TinyImageNet and ImageNet, we choose batch size 256. For all the
vision datasets, we choose the embedding dimension to be 512. Regarding the language
dataset, the batch size is 64 with the feature dimension 768. In all of these cases, our
assumption N ≤ d + 1 in Theorem 5 is satisfied.
•	Hyperparameters: in all our experiments We fix the constant factor μ = 1. We find that
in practice the weight parameter α often needs to be large (e.g., in the Wikipedia dataset),
Which requires moderate tuning. Note that We also implement RPC (Tsai et al., 2021) in our
paper. For all the datasets, We folloW Tsai et al. (2021) and choose the relative parameters
α = 1.0, β = 0.005 and γ = 1.0 for all datasets.
•	Optimizer and learning rate scheduler: For vision tasks, We use SGD With momentum for
optimization and the cosine learning rate scheduler (Loshchilov & Hutter, 2017). For the
natural language task, We use Adam With Weight decay (Loshchilov & Hutter, 2018) and
the linear decay scheduler.
•	Evaluation metric: for vision tasks, We use k-nearest-neighbor (kNN) and linear evaluation
to evaluate the performance, based on the learned embeddings. For the NLP task, We use
the Spearman’s correlation to evaluate the averaged semantic textual similarity score (Gao
et al., 2021).
•	Baseline methods: for the four baseline methods, We folloW the implementations in:
-	MoCo: https://github.com/facebookresearch/moco
-	SimCLR:https://github.com/sthalles/SimCLR
-	Uniformity: https://github.com/SsnL/align_uniform
-	RPC: https://github.com/martinmamql/relative_predictive_
coding.
For fair comparison We use the experimental settings in Table 5 for all the baseline methods,
Which might differ from the original settings.
20
Under review as a conference paper at ICLR 2022
Table 5: Detailed experimental settings. arch: the neural network architecture used. N: batch
size; d: the dimension of the feature representation; lr: learning rate; μ: the constant factor in μ;
1∕(2σ2) and α follow from Algorithm 1; epoch: the number of epochs we run; k: the number of
nearest neighbors in kNN evaluation.
Dataset	arch	N	d	lr	μ	(2σ2)-1	α	epoch	k
CIFAR-10	ResNet-18	512	512	0.1	1	1	40	800	200
CIFAR-100	ResNet-18	512	512	0.1	1	1	40	1000	200
STL-10	ResNet-50	64	512	0.1	1	1	40	800	200
TinyImageNet ResNet-50		256	512	0.1	1	1	40	800	200
ImageNet	ResNet-50	256	512	0.1	1	1	40	100	n/a
Wikipedia	BERTbase	64	768	3e-5	1	20	409600	1	n/a
Table 5	gives common choices of hyperparameters for different datasets. Note that we may need to
further finetune α and σ for different f -divergences. See our supplementary code for more details.
Table 6:	Ablation study on weighting parameter α for KL and JS divergences on CIFAR-10. We
compare test classification accuracies (%) for different choices of α using kNN evaluation.
α 0.1	1	10	20	30	40	50
KL 13.16	77.60	83.53	83.77	81.39	84.19	82.77
JS 8.84	73.31	81.39	83.21	83.49	84.06	82.61
Table 7:	Ablation study on weighting parameter α for KL and Pearson χ2 divergences on Wikipedia.
We compare the semantic textual similarity (STS) via the Spearman’s correlation for different
choices of weighting parameter α.
α 1	10	102	103	104	105	409600	106
KL 67.52	70.47	72.43	75.12	76.90	77.78	78.02	77.78
Pearson 64.58	67.78	71.58	74.03	74.95	74.40	77.59	76.47
C.2 Additional ablation study on weighting parameter
We provide additional ablation study on the weighting parameter α. We perform experiments using
a vision dataset (CIFAR-10) and a language dataset (Wikipedia). For CIFAR-10, we vary α from
0.1 to 50 for KL and JS divergences and run for 200 epochs. Table 6 justifies our choice of α in
Table 5, where the downstream test classification accuracy indicates the optimal performance when
choosing α = 40. For the Wikipedia dataset, we observe that a much bigger α is desirable for
maximum performance. We vary α from 1 to 106 for KL and Pearson χ2 divergences and run for
1 epoch, as there is a large number of samples (106) in the language dataset. Table 7 justifies our
choice of α in Table 5, where the best performance is reached at α = 409600. Such an α is found
by starting from α = 100 and doubling iteratively.
C.3 Additional experiments
Our final experiments show that f -MICL is stable in terms of training and the variation of perfor-
mance is well controlled.
Training stability We depict the training loss curves of different divergences on CIFAR-10 in
Figure 4. This figure shows that our methods exhibit stable training dynamics with fast convergence.
kNN evaluation and additional f -divergences We show more detailed results of Table 2 in Table
8, including experiments using k-nearest neighbour (kNN) evaluation. Additionally, we have added
experiments on other f -divergences such as Squared Hellinger and Tsallis-α divergences.
21
Under review as a conference paper at ICLR 2022
Figure 4: The training loss curves of various f -divergences on CIFAR-10 with 200 epochs.
KL-divergence
0.000	0.001	0.002	0.003	0.004	0.005
squared distance
Figure 5: Experiment for verifying Assumption 3. We draw the relation between the squared dis-
tances kxg - yg k2 and the averaged log pg with RealNVP. The features are learned by different
algorithms trained on CIFAR-10. (left) SimCLR; (right) f-MICL with the KL divergence.
SimCLR
6700
6600
6500
6400
0.000	0.002	0.004	0.006	0.008
squared distance
A⅛fsu ① P 6。_
22
Under review as a conference paper at ICLR 2022
Table 8: Test classification accuracy (%) on the vision datasets. For the Wikipedia dataset we
evaluate the semantic textual similarity (STS) via the Spearman’s correlation. For each method, we
take three separate runs, and show the mean and stand derivation.
Evaluation	Dataset		Baselines				f-MICL				
		MoCo	SimCLR	Uniformity	RPC	KL	JS	Pearson	SH	Tsallis	VLC
	CIFAR-10	90.30	89.71	90.41	90.39	90.61	89.66	89.35	89.52	89.15	89.13
		±0.19	±0.37	±0.26	±0.25	±0.47	±0.28	±0.52	±0.25	±0.42	±0.33
Linear	CIFAR-100	62.77	62.75	62.51	62.66	63.00	63.11	61.69	61.47	60.55	61.19
		±0.17	±0.45	±0.36	±0.39	±0.44	±0.33	±0.57	±0.23	±0.44	±0.29
	STL-10	83.69	82.97	84.44	82.41	85.33	85.94	82.64	82.80	84.79	83.27
		±0.22	±0.32	±0.19	±0.14	±0.39	±0.17	±0.37	±0.27	±0.34	±0.72
	TinyImageNet	35.72	30.56	41.20	34.95	39.46	42.98	38.45	40.83	32.99	38.65
		±0.17	±0.28	±0.19	±0.25	±0.20	±0.18	±0.54	±0.67	±0.49	±0.45
	ImageNet	58.59	57.66	59.12	56.11	58.91	61.11	55.33	52.37	53.11	54.26
	CIFAR-10	88.70	84.92	89.42	84.21	89.34	89.12	89.44	88.13	89.18	89.15
		±0.22	±0.39	±0.18	±0.24	±0.57	±0.38	±0.60	±0.18	±0.62	±0.23
kNN	CIFAR-100	59.21	53.47	61.07	50.02	59.12	61.36	58.68	57.66	58.12	59.17
		±0.15	±0.47	±0.38	±0.34	±0.37	±0.35	±0.57	±0.33	±0.45	±0.28
	STL-10	78.77	74.34	79.57	73.27	79.99	80.45	76.64	78.31	76.11	79.34
		±0.25	±0.14	±0.52	±0.40	±0.47	±0.19	±0.26	±0.33	±0.24	±0.62
	TinyImageNet	36.22	29.60	37.44	24.25	36.17	38.20	35.14	35.56	33.11	35.21
		±0.20	±0.39	±0.27	±0.35	±0.29	±0.26	±0.63	±0.77	±0.52	±0.33
STS	Wikipedia	77.88	77.40	77.95	68.32	78.02	76.76	77.59	73.60	72.68	55.07
		±0.15	±0.12	±0.08	±0.23	±0.13	±0.09	±0.12	±0.10	±0.09	±0.13
Verification of Assumption 3 Throughout our paper we made an assumption (Assumption 3)
that the joint feature distribution is a Gaussian kernel. However, is it a valid assumption? In this
experiment we try to show some empirical evidence that this assumption approximately holds in
practice. Recall that Assumption 3 says that the joint feature distribution of positive pairs is:
Pg (Xg ,yg) Y exp (-M ；Jk )	(C.1)
if the RBF kernel is Gaussian. In order to estimate the joint density of positive pairs, we use nor-
malizing flows (Dinh et al., 2017), which is a popular method for the density estimation. Popular
normalizing flow models include RealNVP (Dinh et al., 2017), NICE (Dinh et al., 2014) and Glow
(Kingma & Dhariwal, 2018). Equation C.1 is equivalent to the following:
log pg (Xg, yg) = - k* ;Jk + const,	(C.2)
and thus it suffices to show that the log likelihood is linear w.r.t. the distances between each positive
pair. In Figure 5, we plot the relation between logpg, estimated by RealNVP1, and the squared
distances kXg - yg k2. The representations are learned by SimCLR, and f -MICL with the KL
divergence on the CIFAR-10 dataset. To alleviate the estimation error in the flow model, we divide
the distances into small intervals and compute the average log likelihood within each interval. From
this figure we can see that the log likelihood is roughly linear w.r.t. the squared distance, and thus
verifying our Assumption 3.
1Code available at https://github.com/ikostrikov/pytorch-flows.
23