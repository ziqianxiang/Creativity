Under review as a conference paper at ICLR 2022
Neuron-Enhanced Autoencoder based Col-
lab orative filtering: Theory and Practice
Anonymous authors
Paper under double-blind review
Ab stract
This paper presents a novel recommendation method called neuron-enhanced au-
toencoder based collaborative filtering (NE-AECF). The method uses an addi-
tional neural network to enhance the reconstruction capability of autoencoder.
Different from the main neural network implemented in a layer-wise manner, the
additional neural network is implemented in an element-wise manner. They are
trained simultaneously to construct an enhanced autoencoder of which the acti-
vation function in the output layer is learned adaptively to approximate possibly
complicated response functions in real data. We provide theoretical analysis for
NE-AECF to investigate the generalization ability of autoencoder and deep learn-
ing in collaborative filtering. We prove that the element-wise neural network is
able to reduce the upper bound of the prediction error for the unknown ratings,
the data sparsity is not problematic but useful, and the prediction performance is
closely related to the difference between the number of users and the number of
items. Numerical results show that our NE-AECF has promising performance on
a few benchmark datasets.
1	Introduction
Recommendation system aims to provide personalized recommendation based on various informa-
tion such as user purchase records, social networks, user features, and item (or product) features.
With the fast growth of E-commence, social media, and content provider, recommendation systems
play more and more important roles in our daily life and have changed our life both explicitly and
implicitly. In general, recommendation systems can be organized into three categories (Adomavi-
cius & Tuzhilin, 2005; Zhang et al., 2019): content based method, collaborative filtering, and hybrid
methods. The content based methods recommend similar items to a user or recommend one item
to similar users, where the similarity is usually obtained from side information such as genre, oc-
cupation, and age. Collaborative filtering (CF) assumes that there exist potential correlations within
both users and items, which can be implicitly used to predict unknown ratings. Hybrid methods are
combinations of content based methods and CF methods (Adomavicius & Tuzhilin, 2005; Zhang
et al., 2019; Su & Khoshgoftaar, 2009). CF is at the cores of many recommendation systems.
Early CF methods (Resnick et al., 1994) compute the similarity between users or items directly
from the ratings to make prediction. This kind of method is also called memory based CF, which
is easy to implement and has high interpretability. One limitation is that the similarity computed
from the ratings is not informative owing to the high sparsity of the rating. Another line of CF is
model based method (Ungar & Foster, 1998; Shani et al., 2002) that utilizes historical data to train
a machine learning model such as Bayesian network (Breese et al., 1998; Miyahara et al., 2000) for
recommendation. Model based methods are more effective than content based methods in learning
complex hidden preference and handling the sparsity problem. Note that both content based and
model based methods do not work when there are new items or users without ratings, which is
known as the cold start problem. A popular strategy for solving the problem is to incorporate side
information into CF methods (Adams et al., 2010; Welling et al., 2012; Zhang et al., 2017).
In the past decades, matrix factorization (Billsus & Pazzani, 1998; Mnih & Salakhutdinov, 2008; Ko-
ren et al., 2009) and matrix completion (Candes & Recht, 2009; Shamir & Shalev-Shwartz, 2014;
Sun & Luo, 2015; Chen et al., 2016; Fan et al., 2019) have been extensively studied and used in
CF. These methods usually exploit the potential low-rank structure of the incomplete rating matrix
1
Under review as a conference paper at ICLR 2022
via embedding items and users into a latent space of reduced dimension, where the observed rat-
ings are approximated by the inner products of the user feature vectors and item feature vectors.
The low-rankness is usually obtained by low-rank factorization (Koren et al., 2009), nuclear norm
minimization (Candes & Recht, 2009), or Schatten-P quasi norm minimization (Fan et al., 2019).
Particularly, Lee et al. (2016) proposed a local low-rank matrix approximation (LLORMA) that
approximates the rating matrix as a weighted sum of a few low-rank matrices. LLORMA outper-
formed vanilla low-rank matrix completion methods in collaborative filtering, which indicates that
the rating matrices in real applications may have more complicated structures rather than a single
low-rank structure.
The success of neural networks and deep learning in computer vision and natural language process-
ing inspired researchers to design neural networks for CF (Salakhutdinov et al., 2007; Dziugaite &
Roy, 2015; Sedhain et al., 2015; Wu et al., 2016; Zheng et al., 2016; He et al., 2017; van den Berg
et al., 2017; Fan & Cheng, 2018; Yi et al., 2020). For instance, Salakhutdinov et al. (2007) proposed
a restricted Boltzmann machines (Hinton et al., 2006) based CF method called RBM-CF, which
showed high performance in the Netflix challenge (Bennett & Lanning, 2007). Sedhain et al. (2015)
proposed AutoRec, an autoencoder (Hinton & Salakhutdinov, 2006; Bengio et al., 2007) based CF
method, which predicts unknown ratings by an encoder-decoder model X = W2σ(W1x), where
x denotes the incomplete ratings on one item or of one user and W1 , W2 are weight matrices to
optimize. Unlike RBM-CF, which is probabilistic and generative, AutoRec provides a discrimina-
tive approach. AutoRec outperformed LLORMA slightly on several benchmark datasets (Sedhain
et al., 2015). In addition, adding depth is able to improve the performance of AutoRec (Sedhain
et al., 2015). Inspried by Neural Autoregressive Distribution Estimator (NADE) (Larochelle &
Murray, 2011) and RBM-CF (Salakhutdinov et al., 2007), Zheng et al. (2016) proposed a method
called CF-NADE, in which parameters are shared between different ratings and achieved promising
performance in several benchmarks. Muller et al. (2018) proposed a kernel based reparametrized
neural network, in which the weight between two units is set to be a weighted kernel-function of
the location vectors. The method works well in data visualization and recommendation systems.
Interestingly, Yi et al. (2020) found that the expected value of the output layer of a neural network
depends on the sparsity of the input data. They proposed a simple yet effective method called spar-
sity normalization to improve the performance of neural networks with sparse input data such as the
highly incomplete rating matrices in CF.
It is worth mentioning that existing autoencoder based CF methods such as (Sedhain et al., 2015;
Wu et al., 2016; Muller et al., 2018; Yi et al., 2020) use linear activation function in the output of the
decoder, i.e., X = WLhL-ι, where WL denotes the weights of the output layer and h∙L-ι denotes
the features given by the last hidden layer. Thus, these methods are under the assumption that the
ratings are linear interactions between user features and item features, though the features can be
nonlinear. Such an assumption may not be true or not optimal in real problems, especially when
the data are bounded (e.g. images) or are collected by sensors (e.g. medical and chemical sensors)
with nonlinear response functions. We suspect that the rating values given by users on items are
from some nonlinear response functions because humans have complex emotion or decision curves
(LeDoux, 2000; Baker, 2001). A naive method to incorporate nonlinear interaction is using nonlin-
ear activation functions such as sigmoid function (with rescaling) in the output layer of the decoder,
which however has much lower performance than using a linear activation function. That’s why ex-
isting autoencoder based CF methods use only linear activation function. Note that a pre-specified
activation function for the output layer of the decoder may work on specific data but may be far
away from the possible optimal choice. On the other hand, the theoretical analysis for autoencoder
and deep learning based CF is very limited, while there have been many works on the theory of
low-rank matrix completion based CF (Srebro & Shraibman, 2005; Candes & Recht, 2009; Shamir
& Shalev-Shwartz, 2014; Fan et al., 2019).
Contribution. In this paper, we present a novel neural network CF method named NE-AECF, an en-
hanced autoencoder approach for recommendation system. NE-AECF is composed of two different
neural networks, one is an autoencoder to reconstruct the incomplete rating matrix, while the other is
an element-wise neural network to learn an activation function adaptively for the output layer of the
autoencoder. We provide theoretical analysis for NE-AECF, which explains the superiority of our
method. Specifically, we prove that the element-wise neural network can reduce the upper bound of
the prediction error for the unknown ratings. We also prove that the data sparsity is not problematic
but useful and the prediction performance is closely related to the difference between the number of
2
Under review as a conference paper at ICLR 2022
users and the number of items. Further, we demonstrate empirically our NE-AECF on benchmarks:
MovieLen-100k and MovieLen-1M, achieving state-of-the-art results.
Notation. We use x (or X), x, and X to denote scalar, vector, and matrix respectively. We use
kxk to denote the Euclidean norm of vector x, use kX kF and kX k2 to denote the Frobenius norm
and spectral norm of matrix X respectively. The `21 norm of matrix is denoted by kXk2,1 :=
Pikxi k, where Xi denotes the i-th column of X. The '∞ norm of matrix is denoted by ∣∣X ∣∣∞ :=
maxij |Xij |. We use |S | to denote the cardinality of set S. The symbol 00 denotes the Hadamard
product between vectors or matrices. The symbol 0 ◦0 denotes function composition.
2	Neuron-Enhanced AECF
Suppose We have an incomplete rating matrix X = (xι, x2,..., xn) ∈ Rm×n, where m is the
number of users and n is the number of items (without loss of generality). Xij ≥ 0 denotes the
rating given by user i on item j and Xij = 0 indicates an unobserved rating. S denotes the set of
observed ratings. We have Xij = Xij for all (i, j) ∈ S. Our goal is to predict the unobserved
ratings Xij, (i, j) ∈ [m] × [n]\S, from X.
We want to learn a nonlinear function f : Rm 7→ Rm such that
n
XIlsi θ (xi - f (xi))∣∣2	(1)
i=1
is as small as possible, where si is a binary vector denoting whether the the corresponding element
in xi is zero (unknown) or not. The motivation is predicting the missing entries of xi using its
observed entries, though the missing entries of xi are filled by zeros before performing f. More
formally, we consider the following problem
minimize || S Θ (X - f(X))∣∣j	(2)
where S = (s1 , s2, . . . , sn), f is performed on each column of X separately, and F denotes a
hypothesis set of m to m functions. We have infinite choices for F . For example, F can be a set
of functions in the form of neural network with some parameters W ∈ W, where W denotes a set
of matrices under some constraints. In this case, problem (2) defines a denoising autoencoder or
stacked denoising autoencoders (Vincent et al., 2010), where the noises are introduced by filling the
missing ratings with zeros.
Let f be an autoencoder with linear activation function in the output layer. Then (2) becomes
minimize ∣∣S Θ (X - W2σ(W1X)) ||： + λ (∣∣ WJ∣F + ∣∣W2∣∣F),	(3)
where W1 ∈ Rd×m and W2 ∈ Rm×d are weights matrices to learn and λ is a nonnegative constant
to control the strength of weight decay. We have omitted the bia terms for simplicity. σ denotes an
activation function such as
ReLU σ(x) = max(x, 0) and
Sigmoid σ(x) = 1/(1 + exp(-x)).
Note that (3) is exactly the basic model considered by Sedhain et al. (2015),Wu et al. (2016),Muller
et al. (2018), and Yi et al. (2020). Once (3) is used, the following assumption is made implicitly.
Assumption 1. There exist two matrices A ∈ Rm×d and B ∈ Rd×n such that ∣SΘ (X - AB) ∣F
is small enough.
The assumption indicates that ifd is much smaller than min(m, n), X can be well approximated by
a low-rank matrix, which however may not always hold in real applications. Consider the following
data generating model
X = h(A0B0),	(4)
3
Under review as a conference paper at ICLR 2022
where h : R1 7→ R1 is an element-wise nonlinear function and A0 ∈ Rm×d , B0 ∈ Rd×n may
be generated by some nonlinear functions. If the nonlinearity of h is high, X cannot be well ap-
proximated by a rank-d matrix. These analysis indicates that if the element-wise nonlinearity in
generating X is strong, (3) should use a large d to ensure a small enough training error.
The element-wise nonlinearity widely exists in real data. For example, in imaging science, the
intensity of pixels are nonlinear responses of photoelectric element to spectrum. In chemical en-
gineering, many sensors have nonlinear responses. In biomedical engineering, the dose-responses
are often nonlinear curves. Hence, in collaborative filtering, the ratings may be nonlinear responses
to some latent values, according to the studies on response curve in neuroscience and psychology
(LeDoux, 2000; Baker, 2001).
Therefore, instead of (3), one may consider the following problem
minimize U S Θ (X - h(W2σ(W1X))) H + λ (∣∣W1∣∣F + ∣∣W2∣∣F),	⑸
where h should be determined beforehand. A naive approach to determining h is choosing a bounded
or partially bounded nonlinear function according to the range of the data. For example, if the data
are image pixels within [0, 1], one may use Sigmoid function. If the data are nonnegative, one may
use ReLU. However, such choices only considered the range of the data, which is just a small portion
of the nonlinearity. Within the range, the true response functions are not necessarily linear (ReLU)
or related to exponential (Sigmoid), and can be much more complicated.
As it is difficult to choose a suitable nonlinear function h in advance, we propose to learn h from the
data adaptively, i.e.,
Wm,Wmh∈eJsΘ (X - h(W2σ(W1X)))j + λ(∣∣W1kF + kW2kF),	(6)
where H denotes a hypothesis set of nonlinear functions from R1 to R1. We have different ap-
proaches to learning h. The first approach is combining various activation functions, i.e.,
k
hθ(z) = X θiσi(z),	(7)
i
where σi(∙) are different activation functions and θ = (θι,..., θk)> are parameters to estimate.
However, it is not clear whether (7) is able to approximate a wide range of nonlinear functions. The
second approach is using polynomial functions, i.e.,
k
hθ(z) = Xθzk.	(8)
i
It is a k-order polynomial function and can well approximate any smooth functions provided that k
is sufficiently large. Another approach is using a neural network, i.e.,
hθ(z) = Θlθ (σθ(θLθ-iσθ(∙∙∙σθ(Θιz) ∙∙∙))),	(9)
where Θ1 and ΘLΘ are vectors, Θ2, . . . , ΘLΘ-1 are matrices, and σΘ is a fixed activation function.
According to the universal approximation theorems (Pinkus, 1999; Sonoda & Murata, 2017; Lu
et al., 2017), (9) is able to approximate any continuous functions provided that the network is wide
enough or deep enough.
Since (9) is more flexible than (7) and (8) in function approximation, we propose to solve the fol-
lowing problem
2	LW	LΘ
minimize U S Θ (X - h©(gw (X)))U + λw X ∣∣Wι kF + λθ X |@|自,	(10)
W,Θ	F	l=1	l=1
where W = {W1, . . . , WLW}, Θ = {Θ1, . . . , ΘLΘ}, and
gw(X) = WLW (σw (WLW-iσw(…σw(WιX)…))).	(11)
4
Under review as a conference paper at ICLR 2022
In addition, we assume Wl ∈ Rdl×dl-1 , l ∈ [LW], and Θl ∈ Rpl×pl-1, l ∈ [LΘ]. Note that
d0 = dLW = m and p0 = pLΘ = 1. Comparing (10) with (2), we see that we have replaced f
by hW ◦ gΘ with Frobenius-norm constrained weight matrices. Model (10) is exactly our neuron-
enhanced autoencoder based collaborative filtering (NE-AECF) method. There are two different
neural networks. The first one is an autoencoder defined by hΘ ◦gW, which is to learn an contraction
map from the incomplete rating matrix X to itself or its observed entries more precisely. The
second neural network is performed in an element-wise manner to learn an activation function h
adaptively for the output layer of the autoencoder or stancked autoencoders. Figure 1 shows an
example schematic of NE-AECF, where LW = LΘ = 2 and Z = gW (X).
Figure 1: An example schematic of NE-AECF. The left part demonstrates a rating matrix where
users and items are represented by each row and column. Every square in the left part corresponds
to a rating. Observed ratings are colored and unobserved rating are left white. Target rating being
predicted is marked with question mark.
3 Generalization error bound of NE-AECF
In this section, we analyze the capability of NE-AECF in predicting the unknown ratings of X. Note
that S IlS Θ (X — X)IlF = S IlS Θ (X — X)IlF := LS ,where X = hθ(gw (X)).Wehave
1
LS = |S| E '(Xij ,Xij ),
(i,j)∈S
1 c/	-χr ∖	/ -χr 令 ∖ 9	, , ι	, ι t~ . ι	ι	,ι t~
where '(Xj, Xij) = (Xij 一 Xij )2. Note that instead of the square loss, We may use other functions
such as |Xij 一 Xij |. In the remainder of this paper, '(Xj ,Xj) denotes a general loss. Let Sc ,
[m] × [n]\S, the generalization error of NE-AECF is quantified by
1
M
LSc
E	`(Xij,Xij),
(i,j)∈Sc
which is a measurement of the prediction error of NE-AECF for the unknown ratings in X . We
have the following generalization bound.
Theorem 1. Suppose a set S of ratings of X ∈ Rm×n are observed uniformaly and randomly,
which results in an incomplete matrix X with unknown ratings replaced by some values such as
zero. Suppose mn 一 |S| > |S| > 50. Let X = hΘ(gW(X)), where hΘ is defined by (9) and gW is
defined by (11). Suppose IlWlk 2 ≤ a, k W1∣∣2,1 ≤ α∣, l ∈ [Lw ], d := max (di,...,也叩-J < m,
and IΘl I2 ≤ bl, IΘl IF ≤ b0l, l ∈ [LΘ]. Suppose the Lipschitz constants of σW and σΘ are ρ and
% respectively. Suppose supij ∣' (Yij,Xj) | ≤ t`, ' is n`-Lipschitz, and ∣∣X∣∣∞ ≤ μ. Then with
5
Under review as a conference paper at ICLR 2022
probability at least 1 - δ over the random sampling S,
where v1
焉 X ` (Xij,χij)-iSy	X '(xij,Xij)
(i,j)∈Sc	(i,j)∈S
Cin'Vi in |S|
|S|
ρLW -1
… IV2 in v3	11τ'mn	C / mn 1 1
+ C2η'μV 下厂 + PWi + 3τ'V MISi	δ,
%LΘ-1
kX k
v2
PL=i P1P1-1, V3 = LθPLwτ%LθTμ-1kX∣∣f(qLW al) (qL=i bl) maxi Ib,
some absolute constants.
(12)
and C1 , C2 are
≤
First, let’s show that the bound is non-trivial. Since activation functions are often at most 1-Lipschitz,
we let P = % = 1. Suppose aι =…=。工皿=1 and bi =…=b^ = 1. Since a，ai ≤ d1-1, We
have (PLW (ala-1)”3) ≤ LW maxi aja-1 ≤ L3W2d. In addition, maxi b∣/bi ≤ maxi √pl.
Then the bound in Theorem 1 becomes
Lsc ≤Ls + O (! + O 卜产Fj
+
11τ'mn	I mn 1
PrnSi	7 ISciiSi	δ
(13)
Note that kX∣∣f ≤ √mnmaxj |Xj|. If ∣S∣ > C3 max (LW^d√mnmaxj |Xj∣, P= PiPi-I)
holds for some constant C3 , the bound is non-trivial. Obviously, the condition holds if n is much
larger than m and ∣S∣ is sufficiently large. A smaller dleads to a tighter bound.
More specifically, Theorem 1 provides the following results.
A.	The error bound of prediction for the unknown ratings can be reduced via including the
additional (element-wise) neural network.
Given a fixed autoencoder, denote by L0S the training error without the element-wise neural network
of NE-AECF. As discussed in Section 2, the additional neural network of NE-AECF aims to learn
an activation function adaptively for the output layer of the decoder. Hence, the training error L0S
can be reduced to LS. Denote LS -LS = ∆s. In (13), %Lθ-i, QL=I bi, and C2ηgμjv jgv3 are
introduced by the additional neural network. Denote v10 by the v1 without %LΘ-1 and QiL=Θ1 bi. Then
we obtain L0Sc ≤ B0 while
LΘ
LSc ≤ B0 -∆S + (%LΘ-1 Y bi - 1)
i=1
Ci n`v0 ln iSi∣L [
一网一+C2η'μf
^^^{^^^^^^^^^^^^^^^^^^^^^^^^™
∆
v2 in v3
iSi	.
|
Note that %LΘ-1 QiL=Θ1 bi can be very close to 1 and v2 is much smaller than iSi (provided that the
additional network is not too large). Therefore, ∆ can be negative, which means the element-wise
neural network can reduce the upper bound of the prediction error. On the other hand, if we do not
use the additional neural network but still want reduce B to B0 - ∆S, we have to increase the depth
or width of the main neural network, which will raise the value of v1 and hence increase the upper
bound. These results verified the superiority of our NE-AECF over classical autoencoder based CF
methods such as the AutoRec of (Sedhain et al., 2015).
B.	Filling the unknown ratings with zeros reduces the upper bound of prediction error.
6
Under review as a conference paper at ICLR 2022
In Theorem 1, if the unknown ratings are replaced by zeros, kX kF will decrease. Hence the bound
become tighter. Specifically, (13) becomes
LSc ≤Ls + O ηη'LW2Nmaxj XjI] + O
|S|
s	SP曾 PlPi-I ∖
W -^I-)
11τ'mn
PmSI
mn 1
+3τ'V5网δ
(14)
+
The bound is tight if ∖∕∖S∖ is much larger than d, the maximum size of the hidden layers.
C.	Increasing n reduces the upper bound of prediction error.
Let the sampling rate ^|Sn|，Z and network structures be fixed. Then (14) becomes
LSC ≤LS + O ηη'Lp maxjIXjl! + O (ηeμ∖lPyPiT]
y	√Zmn∕t^2	J 〈V Zmn )
11τ'	1	1 ^Γ
+ P(1 - Z)Z2mn +	' y (1 - Z)Zmn δ
(15)
Therefore, when n increases, the bound becomes tighter. In addition, LSc ≤ LS when n → ∞. In
real application, if there are more users than items, we need to construct a neural network such that
the input is a vector of each user’s rating, where items correspond to features and users correspond
samples. In other word, larger difference between the number of users and the number of items leads
to tighter upper bound of the prediction error, because we can construct the autoencoder along the
smaller size of the rating matrix.
4 Connection with previous work
The element-wise neural network of NE-AECF can be regarded as an activation function adaptively
learned from the data. It is closely related to the previous work on adaptive activation functions
such (Lin et al., 2013; Agostinelli et al., 2014; Hou et al., 2017; Goyal et al., 2019). For instance,
Lin et al. (2013) proposed to use micro neural networks to improve the convolution operator in
convolutional neural networks. Hou et al. (2017) showed that applying adaptive activation functions
in the regression (second-to-last) layer of a neural network can significantly decrease the bias of
the regression. Their adaptive activation function is in the form of piece-wise polynomials. We
found that, empirically, in NE-AECF, the improvement given by polynomials (8) and piece-wise
polynomials (Hou et al., 2017) are not significant, which may be caused by the unboundedness of
polynomials.
As mentioned in Introduction, theoretical study for autoencoder and deep learning based collabo-
rative filtering is very limited. In recent years, a few researchers have studied the generalization
ability or sample complexity of deep neural networks (Bartlett et al., 2017; Neyshabur et al., 2018;
Golowich et al., 2018) but their results do not apply to autoencoder based CF and our AE-NECF.
Our proof for Theorem 1 has taken advantage of the result of (Bartlett et al., 2017). There are two
major differences. First, our setting is collaborative filtering, in which the training samples are ma-
trix elements. Hence we utilized the transductive Rademacher complexity proposed by (El-Yaniv &
Pechyony, 2009). Second, computing the complexity of the element-wise neural network required a
different method rather than that of (Bartlett et al., 2017).
Shamir & Shalev-Shwartz (2014) provided the following generalization bound for nuclear norm
minimization based CF: LSC ≤ Ls + O (η'kxk*(√m+Vn)) + r, where X denotes the recovered
matrix given by nuclear norm minimization and R stands for the remainder of their result (Theorem
6). Suppose the rank of X is d. Then the term related to the nuclear norm can be as large as
("'√d^∣∣x ∣∣F (√m+√n)
k	Zmn
O
, where we have assumed m < n. According to (15),
the dominating term in our bound can be O
Note that with the same d, the
7
Under review as a conference paper at ICLR 2022
training error of our NE-AECF is less than the training error of nuclear norm minimization because
we are using neural networks. Now we conclude that when n is sufficiently large (compared to
LW Zd), our bound is tighter than that of (Shamir & Shalev-Shwartz, 2014).
5	Numerical results
5.1	Experiments on MovieLens datasets
In this section, we evaluate the proposed method NE-AECF on two benchmark datasets of collab-
orative filtering: Movielens 100K and Movielens 1M (Harper & Konstan, 2015). These datasets
contains 100 thousand (1 million) real world ratings for 1682 (3900) movies by 943 (6040) users, on
a 5-stars scale. We randomly sample 90% of the known ratings as training set, leaving the remaining
10% as the test set. Among the training set, 5% are held out for hyperparameter tuning. Following
Lee et al. (2016), test items or users without training observations are assigned with default rating
of 3. The model performance is evaluated by the root mean squared error defined as
RMSE = SPeij) JXj -Xj2,	(16)
|S c|
where Sc denotes the set of test ratings. For all the experiments, we use Adam (Kingma & Ba, 2015)
to minimize the objective function of NE-AECF.
We report the mean RMSE based on 50 random splits and compare our NE-AECF with the following
methods: BisaMF (Koren et al., 2009), NNMF (Dziugaite & Roy, 2015), AutoRec (Sedhain et al.,
2015), CF-NADE (Zheng et al., 2016), LLOMRA (Lee et al., 2016), GC-MC (van den Berg et al.,
2017), Sparse FC (Muller et al., 2018), DMF+ (Yi et al., 2019), and AutoRec W/SN (Yi et al., 2020).
In our NE-AECF, the main neural network has one hidden layer. The numbers of hidden units are
700 and 2000 for MovieLens-100k and MovieLens-1M respectively. For both datasets, the element-
wise neural network has one hidden layer, of which the size is 200. The regularization parameters
λW and λΘ were chosen from {0.01, 0.1, 1, 10, 50, 100, 200, 500}.
As shown1 in Table 1, on MovieLens-100k, LLORMA outperformed all low-rank factorization
methods, which indicates that a more sophisticated model (weighted sum of local low-rank mod-
els) fits the data better than a single low-rank model. As expected, our NE-AECF outperformed all
baseline methods. Similarly, in Table 2, the RMSE of NE-AECF is less than those of other methods.
It is worth mentioning that we have tried to increase the depth of the main neural network and the
element-wise neural network, but the improvements in terms of RMSE are not significant.
Table 1: RMSE result of NE-AECF and compared methods on MovieLens-100k dataset.
Model	ML-100k
BiaSMF (Koren et al., 2009)	0.911
GC-MC (van den Berg et al., 2017)	0.905
AutoSVD++ (Zhang et al., 2017)	0.904
AutoSVD (Zhang et al., 2017)	0.901
NNMF (Dziugaite & Roy, 2015)	0.903
DMF+ (Yi et al., 2019)	0.8889
LLORMA (Lee et al., 2016)	0.8881
AutoRec w/SN (Yi et al., 2020)	0.8816
NE-AECF (OUrS)	0.8791 ± 0.0063
5.2	Experiments on Douban and Flixster
In this section, we evaluate the proposed NE-AECF on two more datasets Douban and Flixter, which
are poplular benchmarks for recommendation systems. For these data, we use the preprocessed sub-
sets and splits provided by Monti et al. (2017). These datasets both contain 3000 users and 3000
1The RMSEs of the compared methods shown in all tables are from the original papers, in which the
experimental settings are the same or comparable.
8
Under review as a conference paper at ICLR 2022
Table 2: RMSE result of NE-AECF and compared methods on MovieLens-IM dataset
Model	ML-1M
AUtoSVD (Zhang et al., 2017)	0.864
AutoSVD++ (Zhang et al., 2017)	0.848
BiasMF (Koren et al., 2009)	0.845
NNMF (Dziugaite & Roy, 2015)	0.843
LLORMA (Lee et al., 2016)	0.833
DMF+ (Yi et al., 2019)	0.8321
GC-MC (van den Berg et al., 2017)	0.832
AutoRec (Sedhain et al., 2015)	0.831
CF-NADE (Zheng et al., 2016)	0.829
AutoRec w/SN (Yi et al., 2020)	0.8260
NE-AECF (OUrS)	0.8252 ± 0.0024
items. Douban contains 136,891 ratings with density 0.0152 on a rating scale {1, 2, 3, 4, 5}. Flixster
contains 26,173 ratings with density 0.0029 on a rating scale {0.5, 1, 1.5, ..., 5}, which is a bit dif-
ferent from the other three datasets. Among the training samples, 5% are used for hyperparameter
tuning.
We report the mean RMSE on 5 repeated experiments and compare our NE-AECF with the following
methods: PMF (Mnih & Salakhutdinov, 2008), GRALS (Rao et al., 2015), sRGCNN (Monti et al.,
2017), GC-MC (van den Berg et al., 2017), Factorized EAE (Hartford et al., 2018) and GRAEM
(Strahl et al., 2020). In our NE-AECF, the main neural network has one hidden layer, of which the
size is 500, for both datasets. The structure of the element-wise neural network is the same as that
used for the MovieLens datasets.
As shown in Table 3, our proposed NE-AECF outperforms other baseline method on both Douban
and Flixster. Note that some of the compared methods include extra content like the side information
of users and items into the model, while NE-AECF does not require extra content.
Table 3: RMSE result of NE-AECF and compared methods on DoUban and Flixster dataset
Model	Douban	Flixster
GRALS (Rao et al., 2015)	0.8326	1.245
PMF (Mnih & SalakhUtdinov, 2008)	0.7492	0.9809
sRGCNN (Monti et al., 2017)	0.801	0.926
Factorized EAE (Hartford et al., 2018)	0.738	0.908
GC-MC (van den Berg et al., 2017)	0.734	0.917
GRAEM (Strahl et al., 2020)	0.7497	0.8857
NE-AECF (OUrS)	0.7286	0.8816
6	Conclusion
This paper presented a novel collaborative filtering method called NE-AECF. We analyzed the gen-
eralization ability for NE-AECF and showed that the element-wise neural network is useful in re-
ducing the upper bound of the prediction error in CF. The theoretical analysis indicates that filling
the unknown ratings with zeros can make the error bound tighter. It also provides a guideline to
make a choice between item-based autoencoder and user-based autoencoder. These theoretical find-
ings were further validated by the numerical results, in which our NE-AECF has state-of-the-art
performance in terms of RMSE.
It is possible to incorporate side information or graph neural network into our NE-AECF, which can
be a future work.
9
Under review as a conference paper at ICLR 2022
References
Ryan P. Adams, George E. Dahl, and Iain Murray. Incorporating side information into probabilistic
matrix factorization using Gaussian processes. In Proceedings of the 26th Conference on Uncer-
tainty in Artificial Intelligence, pp. 1-9, 2010.
G. Adomavicius and A. Tuzhilin. Toward the next generation of recommender systems: a survey
of the state-of-the-art and possible extensions. IEEE Transactions on Knowledge and Data Engi-
neering, 17(6):734-749, 2005.
Forest Agostinelli, Matthew Hoffman, Peter Sadowski, and Pierre Baldi. Learning activation func-
tions to improve deep neural networks. arXiv preprint arXiv:1412.6830, 2014.
Frank B Baker. The basics of item response theory. ERIC, 2001.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran
Associates, Inc., 2017.
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training
of deep networks. In Advances in neural information processing systems, pp. 153-160, 2007.
J. Bennett and S. Lanning. The netflix prize. In Proceedings of the KDD Cup Workshop 2007, pp.
3-6, New York, August 2007. ACM.
Daniel Billsus and Michael J. Pazzani. Learning collaborative information filters. pp. 46-54, 1998.
URL http://portal.acm.org/citation.cfm?id=645527.657311.
John Breese, David Heckerman, and Carl Kadie. Empirical analysis of predictive algorithms for
collaborative filtering. In Proceedings of the 14th Conference on Uncertainty in Artificial Intelli-
gence, pp. 43-52. Morgan Kaufmann, 1998.
Emmanuel J Candes and Benjamin Recht. Exact matrix completion via convex optimization. Foun-
dations of computational mathematics, 9(6):717-772, 2009. ISSN 1615-3375.
Chao Chen, Dongsheng Li, Qin Lv, Junchi Yan, Stephen M. Chu, and Li Shang. Mpma: Mixture
probabilistic matrix approximation for collaborative filtering. In Proceedings of the Twenty-Fifth
International Joint Conference on Artificial Intelligence, IJCAI’16, pp. 1382-1388. AAAI Press,
2016. ISBN 9781577357704.
Gintare Karolina Dziugaite and Daniel M. Roy. Neural network matrix factorization. CoRR,
abs/1511.06443, 2015.
Ran El-Yaniv and Dmitry Pechyony. Transductive rademacher complexity and its applications.
Journal of Artificial Intelligence Research, 35:193-234, 2009.
Jicong Fan and Jieyu Cheng. Matrix completion by deep matrix factorization. Neural Networks, 98:
34-41, 2018.
Jicong Fan, Lijun Ding, Yudong Chen, and Madeleine Udell. Factor group-sparse regularization for
efficient low-rank matrix recovery. In Advances in Neural Information Processing Systems, pp.
5104-5114, 2019.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. In Conference On Learning Theory, pp. 297-299. PMLR, 2018.
Mohit Goyal, Rajan Goyal, and Brejesh Lall. Learning activation functions: A new paradigm for
understanding neural networks. arXiv preprint arXiv:1906.09529, 2019.
F Maxwell Harper and Joseph A Konstan. The MovieLens datasets: History and context. ACM
Transactions on Interactive Intelligent Systems, 5(4):1-19, December 2015.
10
Under review as a conference paper at ICLR 2022
Jason S. Hartford, Devon R. Graham, Kevin Leyton-Brown, and Siamak Ravanbakhsh. Deep Models
of Interactions Across Sets. In Proceedings of the 35th International Conference on Machine
Learning ,pp.1914-1923. PMLR, 2018.
Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural col-
laborative filtering. In Proceedings of the 26th international conference on world wide web, pp.
173-182, 2017.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural
networks. science, 313(5786):504-507, 2006.
Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief
nets. Neural computation, 18(7):1527-1554, 2006.
Le Hou, Dimitris Samaras, Tahsin Kurc, Yi Gao, and Joel Saltz. Convnets with smooth adaptive
activation functions for regression. In Artificial Intelligence and Statistics, pp. 430-439. PMLR,
2017.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd Interna-
tional Conference on Learning Representations,, 2015.
Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender
systems. Computer, 42(8):30-37, 2009. doi: 10.1109/MC.2009.263.
Hugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. In Geoffrey
Gordon, David Dunson, and Miroslav DUdIk (eds.), Proceedings of the Fourteenth International
Conference on Artificial Intelligence and Statistics, volume 15 of Proceedings of Machine Learn-
ing Research, pp. 29-37, Fort Lauderdale, FL, USA, 11-13 Apr 2011. PMLR.
Joseph E. LeDoux. Emotion circuits in the brain. Annual Review of Neuroscience, 23(1):155-184,
2000.
Joonseok Lee, Seungyeon Kim, Guy Lebanon, Yoram Singer, and Samy Bengio. Llorma: Local
low-rank matrix approximation. Journal of Machine Learning Research, 17(15):1-24, 2016.
Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400,
2013.
Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of
neural networks: A view from the width. In Proceedings of the 31st International Conference on
Neural Information Processing Systems, pp. 6232-6240, 2017.
Koji Miyahara, Michael J. Pazzani, and John Slaney. Collaborative filtering with the simple bayesian
classifier. In PRICAI 2000 Topics in Artificial Intelligence, pp. 679-689, Berlin, Heidelberg, 2000.
Springer Berlin Heidelberg. ISBN 978-3-540-44533-3.
Andriy Mnih and Russ R Salakhutdinov. Probabilistic matrix factorization. In J. Platt, D. Koller,
Y. Singer, and S. Roweis (eds.), Advances in Neural Information Processing Systems, volume 20,
2008.
Federico Monti, Michael Bronstein, and Xavier Bresson. Geometric matrix completion with re-
current multi-graph neural networks. In Advances in Neural Information Processing Systems,
volume 30. Curran Associates, Inc., 2017.
Lorenz Muller, Julien Martel, and Giacomo Indiveri. Kernelized synaptic weight matrices. In
Proceedings of the 35th International Conference on Machine Learning, pp. 3654-3663, 2018.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to
spectrally-normalized margin bounds for neural networks. In International Conference on Learn-
ing Representations, 2018.
Allan Pinkus. Approximation theory of the mlp model in neural networks. Acta numerica, 8:143-
195, 1999.
11
Under review as a conference paper at ICLR 2022
Nikhil Rao, Hsiang-Fu Yu, Pradeep K Ravikumar, and Inderjit S Dhillon. Collaborative filtering
with graph information: Consistency and scalable methods. In C. Cortes, N. Lawrence, D. Lee,
M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems, vol-
ume 28. Curran Associates, Inc., 2015.
Paul Resnick, Neophytos Iacovou, Mitesh Suchak, Peter Bergstrom, and John Riedl. Grouplens: an
open architecture for collaborative filtering of netnews. In CSCW ’94: Proceedings of the 1994
ACM conference on Computer supported cooperative work, pp. 175-186, New York, NY, USA,
1994. ACM Press.
Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hinton. Restricted boltzmann machines for
collaborative filtering. In ICML ’07: Proceedings ofthe 24th international conference on Machine
learning, pp. 791-798, 2007.
Suvash Sedhain, Aditya Krishna Menon, Scott Sanner, and Lexing Xie. Autorec: Autoencoders
meet collaborative filtering. In Proceedings of the 24th International Conference on World Wide
Web Companion, pp. 111-112, 2015.
Ohad Shamir and Shai Shalev-Shwartz. Matrix completion with the trace norm: Learning, bounding,
and transducing. Journal of Machine Learning Research, 15(98):3401-3423, 2014.
Guy Shani, Ronen Brafman, and David Heckerman. An MDP-based Recommender System. In
Proceedings of the Eighteenth Conference Annual Conference on Uncertainty in Artificial Intelli-
gence, UAI ’02, pp. 453-460, San Francisco, CA, 2002. Morgan Kaufmann.
Sho Sonoda and Noboru Murata. Neural network with unbounded activation functions is universal
approximator. Applied and Computational Harmonic Analysis, 43(2):233-268, 2017.
Nathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In International Conference
on Computational Learning Theory, pp. 545-560. Springer, 2005.
Jonathan Strahl, Jaakko Peltonen, Hiroshi Mamitsuka, and Samuel Kaski. Scalable probabilistic
matrix factorization with graph-based priors. In The Thirty-Fourth AAAI Conference on Artificial
Intelligence, AAAI, pp. 5851-5858. AAAI Press, 2020.
Xiaoyuan Su and Taghi M. Khoshgoftaar. A survey of collaborative filtering techniques. Adv. in
Artif. Intell., 2009:4:2-4:2, January 2009.
Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via nonconvex factorization. In 2015
IEEE 56th Annual Symposium on Foundations of Computer Science, pp. 270-289, 2015.
Lyle H. Ungar and Dean P. Foster. Clustering methods for collaborative filtering. In Workshop on
Recommender Systems at the 15th National Conference on Artificial Intelligence (AAAI’98), pp.
112-125, Madison, Wisconsin, USA, July 1998. AAAI Press.
Rianne van den Berg, Thomas N. Kipf, and Max Welling. Graph convolutional matrix completion.
abs/1706.02263, 2017.
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol, and
Leon Bottou. Stacked denoising autoencoders: Learning useful representations in a deep network
with a local denoising criterion. Journal of machine learning research, 11(12), 2010.
Max Welling, Ian Porteous, and Kenichi Kurihara. Exchangeable inconsistent priors for bayesian
posterior inference. In 2012 Information Theory and Applications Workshop, ITA 2012, San
Diego, CA, USA, February 5-10, 2012, pp. 407-414. IEEE, 2012.
Yao Wu, Christopher DuBois, Alice X. Zheng, and Martin Ester. Collaborative denoising auto-
encoders for top-n recommender systems. In Proceedings of the Ninth ACM International Con-
ference on Web Search and Data Mining, pp. 153-162, 2 2016.
Baolin Yi, Xiaoxuan Shen, Hai Liu, Zhaoli Zhang, Wei Zhang, Sannyuya Liu, and Naixue Xiong.
Deep matrix factorization with implicit feedback embedding for recommendation system. IEEE
Trans. Ind. Informatics, 15(8):4591-4601, 2019.
12
Under review as a conference paper at ICLR 2022
Joonyoung Yi, Juhyuk Lee, Kwang Joon Kim, Sung Ju Hwang, and Eunho Yang. Why not to
use zero imputation? correcting sparsity bias in training neural networks. In 8th International
Conference on Learning Representations,, 2020.
Shuai Zhang, Lina Yao, and Xiwei Xu. Autosvd++: An efficient hybrid collaborative filtering model
via contractive auto-encoders. In SIGIR,pp. 957-960. ACM, 2017.
Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. Deep learning based recommender system: A survey
and new perspectives. ACM Comput. Surv., 52(1):1-38, 2019.
Yin Zheng, Bangsheng Tang, Wenkui Ding, and Hanning Zhou. A neural autoregressive approach to
collaborative filtering. In Proceedings of the 33nd International Conference on Machine Learn-
ing, volume 48, pp. 764-773, 2016.
A Proof for the main theorem
First of all, we give the following lemmas.
Lemma 1. Let H = {H ∈ Rm×n : hj = Θlθσ (Θlθ-i(…σ(Θι%)…))，∀(i,j) ∈ [m] X
[n]; Θι ∈ Rpι×pl-1 ,∣∣Θι∣∣2 ≤ bι,∣∣Θι∣∣F ≤ bl,∀l ∈ [Lθ]; Z ∈ Z ,∣∣Z IIF ≤ Sz}, where the
Lipschitz constant of σ is %. Suppose the covering number of Z with respect to ∣∣ ∙ ∣f is upper-
bounded by Kε and ε = e(2%Lθ-1 QLΘι bl) . Then the cover covering number of H with respect
to ∣∣T∣F is bounded as
N(H, ∣H∣F,e) ≤ Kε Y (3√⅞LθT(Lθ; I)Sz QLθl bl ['Pl 1 .
Proof. See Section B.1.
□
Lemma 1 provides an upper bound of the covering number of the element-wise neural network. The
following lemma shows an upper bound of the covering number of the main neural network.
Lemma 2 (Theorem 3.3 of Bartlett et al. (2017), reformulated). Let Z =
WLWσ (WLW-I (…σ(WιX)…))，where Wl ∈ Rdl+1dl, l ∈ [Lw], and
max(m, d1 , . . . , dLW ) ≤ D. Denote the Lipschitz constant of σ by ρ. Suppose the reference
matrices (M1, . . . , MLW) are given. Define
C = {FW(X): W =(Wι,..., WLW), k Wlkσ ≤ al, k Wl - Mlkσ ≤ al,∀∈ [Lw]}∙
The for any > 0,
lnN(C,3∣H∣f) ≤
IIXIIF ln2D2
Now we can get an upper bound for the covering number of the entire neural network in NE-AECF.
Lemma 3. The covering number of Hw,θ = {Hθ (FW(X))} With respect to ∣∣ ∙ ∣f satisfies
ln N (H, k ∙ If ,e) ≤
4%2(Lθ-Dρ2(LW-1)∣χkF in2D2
+ XLΘ plpl-1 ln
l=1
6LΘ%LΘ-1ρLW-1
l=1
LW	LΘ
Yal2	Ybl2
al n3
kXkF (QLW al) (QL=θi bl) maxl * ,
2

∖

Proof. See Section B.2.
□
13
Under review as a conference paper at ICLR 2022
Now we can calculate the upper bound of the Rademacher complexity of HW,Θ via using Lemma 3
and Dudley entropy integral bound.
Lemma4. Letvι = 4ρ2(LWT)%2(Le-I)∣∣X∣∣Fln2D2 (QLW a2) (PLW (a0])(Q=1 b2
V2 = PLθιPιPι-ι, and v3 = 6LθPLW-1%Le-1kJXIIF(QLW aə (QL=i d)maxιb0b-1 ∙ Sup-
pose ∣∣hθ (gw(X)) k∞ ≤ μ. The Rademacher complexity of Hw,θ is bounded as
RS (HW,Θ ) ≤
4μ	12，vi + μ2v2 ln S	12μʌ/v2 lnμ-lv3
歹+	S	+	√S
(17)
Proof. See Section B.3.
□
The following lemma provides a sample complexity bound for transductive learning, which is con-
sistent with the objective function and evaluation metric (RMSE) widely used in collaborative filter-
ing.
Lemma 5 (Corollary 1 of (El-Yaniv & Pechyony, 2009), reformulated). Let H be a fixed hypothesis
Setandsuppose SuPi,j∣χ∈H ∣' (Yij,Xij) | ≤ t` . Suppose a fixed set S Ofdistinctindicesisuniformly
and randomly split to two subsets Strain and Stest, where2 |Stest| > |Strain| > 50. Then with probability
at least 1 - δ over the random split, we have
IS1H X ' (Yij,Xij) ≤∣S1π	X ' (Yij ,Xij ) + 4Rs (' ◦H)
test (i,j)∈Stest	train (i,j)∈Strain
(----------------------(18)
+ 11τ' (IStrain I + IStestl)+3T J(IStrain| + IStestI) 由1
p I Strain ∣∣ Stest ∣	V	IStrain " StestI	δ
Then Theorem 1 can be proved as follows.
Proof. Accoding to the Rademacher contraction property, We have RS(' ◦ H) ≤ η'Rs(H), where
n` denotes the lipschitz constant of '. Using Lemma 5 with a slightly different notation and Lemma
4 where μ2 v2《vι provided that the element-wise neural network is small enough, we have
焉 X ' (Xij，Xij) ≤iSf X ' (Xij，Xij)
c
+ n`
48 v⅞ι + μ2v2 ln ISI
^I
+ 48μv⅞2 lnμ-lv3
Pw
11τ' (ISI + IScI)	/(S + IScI)	1
+	pISIIScI	+3τ'V FScrln δ
≤iSI X ' (Xij，XXij)
(i,j)∈S
(19)
,Cιη'v1ln ISi , C	12lnv3
+ —网—+C2η'μy IS厂
11τ⅛mn	I mn 1
+ PS两	TX ISiISciln δ,
where C1 and C2 are some fixed constants, and
/Lw	、(LW 0 0 \ 2/3、3/2 LLe
V1 = P(LWT)%(LΘT)IXkF W (Y aι) (X ⑴	)(Y bl
2We use these assumptions to simplify the theorem.
14
Under review as a conference paper at ICLR 2022
and v3 = Lθμ-1γρLW-1%LeT IIX"(Q曾。，)(Q曾加)
respectively, we finish the proof.
Rename * and vɜ as vι and v3
□
B Proof for lemmas
B .1 PROOF FOR LEMMA 1
Proof. Let Sq1 := {Θι ∈ Rpl十1×pl : ∣∣Θι∣∣2 ≤ bι, ∣Θi∣f ≤ b∣}, ∀l ∈ [Lθ]. It is known that there
exists an q-net Sq obeying
N(SΘι, I ∙ ∣F,q) ≤
pιpι-ι
SUChthat ∣Θι - Θι∣F ≤ q. We have
Ihij - hij I = Ihij - hij IlF
=∣∣θLθ σ (Θlθ-1(∙∙∙ σ(Θ1zij ) ∙∙∙ )) - <9 Le σ (Θ Lθ-1(∙∙∙ σ(θ1zij )…))∣∣F
= ∣∣Θlθ σ (Θlθ-i(∙∙ ∙ σ(Θ1Zij) ∙ ∙	∙)) -	Θ lθ σ (Θlθ-i(∙ ∙ ∙ σ(Θι Zij) ∙ ∙∙))
+ ΘLθσ	(Θlθ-i(∙ ∙ ∙ σ(Θ1Zij)	∙ ∙ ∙))	- Θlθσ (Θlθ-i(∙ ∙ ∙ σ(Θ1Zj)	∙ ∙ ∙)) +-
+ 9Leσ	(θLθ-1(∙∙∙ σ(Θ1zij)	∙∙∙	))	- 9Leσ (θLθ-1(∙∙∙ σ(θ1zij)	∙∙∙	))	∣∣f
+ GLeσ	(<^Lθ-1(∙ ∙∙ σ(Q1Zij)	∙ ∙∙	))	- GLeσ (QLe-1(∙ ∙ ∙ σ(<^1 %)∙ ∙ ∙	))∣∣F
≤ I I ®Le σ (θLe-1(∙∙∙ σ(91zij)…))-G Le σ (θLe-1(∙∙∙ σ(θ1 zij)…))∣∣F
+ ||©Le σ (θLe-1(∙ ∙ ∙ σ(θ1zij) ∙ ∙ ∙ )) - GLe σ (@Le-1(…σ(θ1zij) ∙ ∙ ∙ )) ∣∣F +-
+ ∣∣(g Le σ (G Le-1(∙∙∙ σ(θ1zij >…))-G Lσ (G Le-1(∙∙∙ σ(Θ 1 zij 卜-))∣∣f
+ ||Θ Le σ (<gLθ-1(∙∙∙ σ(Θ 1zij >…))-G Le σ (QLe-1(，∙ •6仇Gij	))||F
(a)	-	Le-I
≤ %LeTIZijI II9Le- e Lθ∣∣f Π llθl∣∣2
l = 1
Le-2
+ %LeTIZij∣∣∣GLe∣∣2∣∣θLe-1- 0Lθ-1∣∣f Π 10l∣2 + …
l = 1
+%LTIZijI (Yu© 儿)∣∣θ1- g 1∣∣f
Le
+ %LeTkZij- Zij IlF ∏ 同 ∣2
l = 1
(Le	Le
Szj %	∏bl	+ Szj ≡Le-1	∏	bl	+	SR, q∏	bl	+ ••• + ∣Zij-	GijkF Π d
l=Le	l=Le-1	l = 1	l=1
(20)
15
Under review as a conference paper at ICLR 2022
In (a), we used the facts kXY kF ≤ kXk2 kY kF and kσ(X) -σ(Y )kF ≤ %kX -Y kF recursively.
It follows that
=√2%Lθ-1 ʌ
t
LΘ
Ybl
l=1
-1 Y	bl + …+ E1 Ybl I kZkF +
ELΘ	bi + ELΘ
i6=LΘ
2
kz - Z kF
l6=LΘ -1
≤ √2%Lθ-1
ELΘ	bi + ELΘ
i6=LΘ
i6=1
2
LΘ
LΘ
-1	∏ bi + ∙→ eι∏ bi	s2 +	∏bi
l6=LΘ -1
l6=1
l=1
In (a), we used the fact (x + y)2 ≤ 2(x2 + y2). Let El
e∕(√2Lθ)
√2%Lθ-1Sz Qk=ι bk
√2%LΘ-1 QL=ι bi
. We arrive at
2
E2z.
, ∀l ∈
(21)
[LΘ]. Let
kH - HkF ≤ E.
(22)
It means that HH is an E-Cover of H. Then the covering number of H is bounded as
N(H,k∙kF ,e)
LΘ
≤N(Z ,k∙kF,金)Y N(Sθι,k∙kF,El)
LΘ
≤κεY
l=1
LΘ
=κε Y
l=1
i=1
6%LΘ-1LΘszb0i Qk6=ibk)	plpl-1
6%LΘ-1LΘszb0ibi-1QkL=Θ1bk	plpl-1
≤κε
E
PlL=Θ1 plpl-1
(23)
E
where CΘ = 6%LΘ-1LΘszγ QlL=Θ1 bl and γ = maxl b0lbl-1. This finished the proof.
□
B.2 Proof for Lemma 3
Proof. It is easy to show that
Sz ≥ kZkF =IIWLW (σw(WLW-ισw(…σw(WιX)…)))L	(24)
LW
≥ρLW-1kXkFYai.	(25)
i=1
16
Under review as a conference paper at ICLR 2022
Combining Lemma 1 and Lemma 2, we have
ln N(H,k∙kF ,e)
≤ ln κε +
P1P1-1 ln( W)
4ρ2(LW -1)%2(LΘ -1)kXk2Fln2D2
≤
E2
LW
Yal2
l=1
LΘ
Ybl2
l=1
(26)
+
plpl-1 ln
’6Lθ%LeTρLWTkXkF (qL=W a)(Q曾 bl) maxi Ib '


∖
□
B.3 Proof for Lemma 4
Before proof, we give the following lemma, which is a variant of the Dudley entropy integral bound
on Rademacher complexity.
Lemma 6 (Lemma A.5 of Bartlett et al. (2017)). LetF be a real-valued function class taking values
in [0, 1] and assume that 0 ∈ F. Then
Proof. For convenience, let
R(F|S) ≤ inf
α>0
+ ~S Z	Jln S (FIS, k∙ kF, E) d).
(27)
LW
Y al2
l=1
LΘ
Ybl2	,
l=1
v2	=	PlL=Θ1 plpl-1,	and v3	=	6LΘρLW	-1%LΘ	-1kXkF	QlL=W1	al	QlL=Θ1	bl	maxl b0lbl-1.	Then
lnN(H, k ∙ kF, E) ≤ -2 + v2 ln
2
from (26). Let μ = maxH∈H kHk∞ and H = {Η : μH ∈
-2	-1
H}. It follows that lnN(H, k ∙ kF, E) ≤ ——5-ɪ + v ln (------------------3
E2	E
According to Lemma 6, we have
Rs (H) ≤ inf
α>0
(a)
≤ inf
α>0
12 √S /
+后L J
12 √S /
+W L V-
μ-2v1 1	1 μ μ-1v3∖A
丁+v2 ln — dE
μ-2vι + v2	_1
----E2-----+ V2 ln μ-1v3 dE
(b)
≤ inf
α>0
12
:L
√S fpμ-2v1+v2 + p21nμ-^)北
E
(28)
inf
α>0
12
+ E
1 + V2 ln -α + Pv2 ln μ-1 v3 (√S - &)))
(c) 4	12∙∖∕μ-2vι + V2
≤ S +
S
lnS+
12(S - 1) ,v2 ln μ-1v3
S√S
≤ 4	12√μ-¾
≤ S +	S
+ V2 ln S + 12 ʌ/V2 ln μ
-1 v3
17
Under review as a conference paper at ICLR 2022
In (a), We used the fact ln 1 ≤ 今.The inequality (b) holds according to √x + y ≤ √x + √y. In
(c), we have let α = √∣, which though may not be the best choice. We arrive at
4μ	12√vι + μ2v2 ln S	12μ√^2 lnμ-lv3
RS(HW,Θ) ≤ ~S +----------s----------1-------√s-------.
□
18