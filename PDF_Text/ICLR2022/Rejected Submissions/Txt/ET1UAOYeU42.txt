Under review as a conference paper at ICLR 2022
Edge Partition Modulated Graph Convolu-
tional Networks
Anonymous authors
Paper under double-blind review
Ab stract
Graph convolutional networks (GCNs), which propagate the node features
through the edges and learn how to transform the aggregated features under label
supervision, have achieved great success in supervised feature extraction for both
graph-level and node-level classification tasks. However, GCNs typically treat the
graph adjacency matrix as given and ignore how the edges could be formed by
different latent inter-node relations. In this paper, we introduce a relational graph
generative process to model how the observed edges are generated by aggregating
the node interactions over multiple overlapping node communities, each of which
represents a particular type of relation that contributes to the edges via a logi-
cal OR mechanism. Based on this relational generative model, we partition each
edge into the summation of multiple relation-specific weighted edges, and use the
weighted edges in each community to define a relation-specific GCN. We intro-
duce a variational inference framework to jointly learn how to partition the edges
into different communities and combine relation-specific GCNs for the end clas-
sification tasks. Extensive evaluations on real-world datasets have demonstrated
how the proposed method helps enhance the discriminative representation power
and its efficacy in learning both node and graph-level representations.
1	Introduction
The graph data structure is widely adopted to describe interconnected objects, such as users in a
social network and atoms held up by chemical bonds to form molecules. The edges in real-world
graphs usually indicate strong and logically reasonable relations between nodes. Graph convolu-
tional networks (GCNs)(Defferrard et al., 2016; KiPf & Welling, 2017; VeIickoVic et al., 2018;
Klicpera et al., 2019a;b) capture this property of graph data by introducing neighborhood aggrega-
tion into regular PercePtron layers. While this aPProach tyPically enhances the feature dePendencies
among strongly-related adjacent nodes, the absence of relational inference makes the aggregation
mechanism indifferent to the multiPle tyPes of latent inter-node relations, ignoring which limits the
ultimate Potential of the model Performance.
ExtensiVe efforts haVe been made to seek solution for Performance enhancement in dealing with
graPhs with heterogeneous relations (Schlichtkrull et al., 2018; Vashishth et al., 2020; Nickel et al.,
2015), where the common ground these methods conVerges to is to learn multiPle graPhs, each one
sPecified to one tyPe of relation. The seParation of relations-sPecific graPhs from one graPh is easy
when these relations are exPlicitly annotated by edge labels, but is non-triVial when the relations
are non-obserVable. To address this issue, some GCNs introduce techniques such as multi-head
(Velickovic et al., 2018; Gao & Ji, 2019) or multi-hop (AbU-EI-Haija et al., 2019) aggregation that
increase the model caPacity for modeling multi-relations, howeVer, the aggregation weights in these
models are either short of interpretability or too dependent on human knowledge to adapt to the
underlying relations. Another line of research aims to disentangle relation-specific node represen-
tations by task-driven graph factorization (Ma et al., 2019; Yang et al., 2020), where the relational
disentanglement is performed relying on label supervision, hence their performances would be neg-
atively impacted by the scarcity of observed labels.
In contrast to the aforementioned methods, we introduce edge partition modulated graph convo-
lutional networks (EPM-GCNs), a generative framework that models the generation of edges and
labels from latent relations. To begin with, we represent the latent relations with co-membership
of K overlapping latent communities. For instance, in a social network where people are linked
1
Under review as a conference paper at ICLR 2022
by online friendship, one person may be simultaneously affiliated with multiple social groups (e.g.,
graduating from the same school A, or working for the same company B, etc.). Then some similarity-
based relations (e.g., schoolmates ofA, colleagues in B) would naturally exist among people that are
affiliated with the same social group. Under this modeling assumption, the strength of each relation
between a pair of nodes is measured by the multiplication of the following latent quantities: (i) the
average interaction rate among co-members in the corresponding community and (ii) how strong the
two ending nodes are affiliated with this community. The edges are further modeled by the logi-
cal OR of binary latent edges independently generated through the Bernoulli-Poisson link function
given the strengths of each relation. This part of the generative model explains the shrinkage of
heterogeneous relations into binary latent edges.
We next build up the generation process of labels. Since one node simultaneously engages in multi-
ple relations, we generate its representation as a composition of relation-specific sub-representations.
These sub-representations are learned by three steps: the first step partitions the edges by normal-
izing the strengths of K community-based relations, and defines K independent GCNs with each
set of weighted edges; the second step obtains the desirable sub-representations by running the K
GCNs in parallel. The labels of nodes or graphs could be predicted with the node representations
generated in this way. This generative model allows the community-based relations to be inferred
from both labels and edges, which not only systematically solves the relation non-observable issue,
but also incorporates edges as an alternative source of information provided for the posterior infer-
ence of latent relations. Since the observed edges dominate labels in quantity, our method would not
heavily depend on label supervision, hence may suffer less from the scarcity of labels.
We summarize the major contributions of EPM-GCNs as follows:
•	We develop a novel generative model that generates labels from relation-specific sub-
representations, which are at first learned through independent neighborhood aggregations
with weighted edges that are partitioned by the strengths of latent relations;
•	We integrate the label generation process with the generation of edges, which enables the
observed edges to compensate for the scarcity of observed labels by providing extra infor-
mation for latent communities/relations;
•	The proposed EPM-GCNs are end-to-end trainable via variational inference;
•	We analyze the working mechanism of EPM-GCNs and evaluate the models over various
real-world network datasets. Empirical results show that the EPM-GCNs achieve state-of-
the-art performance in most of the node and graph-level classification tasks.
2	Edge partition modulated GCNs
In this section, we present EPM-GCNs that learn the representation of node or graph from relation-
specific sub-representations. As shown in Figure 1, the overall architecture of EPM-GCNs consists
of a generative network (parameterized with θ) and an inference network (parameterized with ω).
Section 2.1 outlines the data generation process and corresponding supportive modules in the gen-
erative network, Section 2.2 elaborates the module in the inference network as well as the inference-
learning details.
2.1	Generative task-learning with latent communities
For the generative supervised graph analytic tasks under discussion, the information of a graph G
with N nodes could be summarized by the triplet (X, A, y), where X is the design matrix whose
rows represent the features of individual nodes, A is the binary adjacency matrix of G, with each “1”
entry indicating the presence of an edge between the corresponding node pair, and y ∈ {yo, yu}
is the set of labels, with subscripts o and u denoting the observed and unobserved parts of labels,
respectively. The goal of our latent factor model is to describe the generation process of observed
edges and labels. Based on that, we are enabled to predict the holdout labels from their posterior.
The joint generation of edges A and labels y is modeled by stacking up the generation process of
edges and the generation of labels given the edges.
2
Under review as a conference paper at ICLR 2022
Figure 1: The overview of EPM-GCN’s computation graph. Φ encodes nodes’ affiliation strength to each
community, which is used to (1) enrich node features with community information (the dashed line) and (2)
partition edges via normalized rank factorization (the solid arrows). They are then passed to an array of GCNs
to generate node representations that correspond to heterogeneous community-based relations, and finally inte-
grated together to optimize the classification task.
2.1.1	The generation of edges
We adopt the generation process developed in Zhou (2015) to model the formation of edges A:
Aij 〜Ber(Pij), Pij = 1 - e- PK=I Yk φi,kφj,k,	(1)
which has an alternative representation under the Bernoulli-Poisson link as Ai,j = Im,, ≥ι, Mij 〜
Poisson(PK=ι YkΦi,kΦj k). In this expression, Φ is a positive node-community affiliation matrix,
whose entry at the ith row and kth column could be interpreted as the strength that node i is affiliated
with community k, γk (k ∈ [K]) is a positive community activation level indicator that measures the
member interaction frequencies featured by community k, and γkΦi,kΦj,k quantifies the interaction
rate between nodes i and j through community k. In our practice, We treat Y = [γι,γ2,…，Yk]0 as
a part of the trainable parameters θ .
In this paper, we focus on another equivalent representation that partitions each edge into the logical
disjunctions of K latent binary edges, expressed as
Aij = VlK=IAi,j,k, Ai,j,k 〜Ber(Pijk), Pijk ：= 1 - e-γkφi,kφj,k ∀i,j ∈ [N]1
where ∨ is the logical disjunction (i.e., logical OR) operator and Pijk denotes the probability that the
interaction of nodes i and j in community k results in an edge. The connection between nodes i and
j would establish as long as their interaction in one community forms up an edge, in other words,
nodes i and j would be disconnected only if their interactions in ALL communities fail to generate
edges. Link function 1 gives the conditional probability Pθ(A | Φ). Note that the node interaction
over each community uniquely corresponds to one type of inter-node relation, the generation of A
reflects the aggregation of these heterogeneous relations, suggesting that one would need to partition
the edges to separate these relations.
2.1.2	The generation of labels:
In consonance with the graph generative model that establishes the graph from latent communities,
we develop the generation process of labels from all relation-specific representations. To achieve
that, we first need to separate each relation from their totality, i.e., factorize the edges with the edge
partitioner module:
Edge PARTitioner (EPART). The edge partitioner takes edges A and community-affiliation
score matrix Φ as inputs and returns K positive-weighted edges: {A(1), A(2),…，A(K)},
1We use the [N] notation for the integer set {1, 2,…，N} throughout the text.
3
Under review as a conference paper at ICLR 2022
s.t. PkK=1 A(k) = A. The partition function follows the form
e(Yk φi,kφj,k "τ	=	(l-pijk) T
P e(Yk0φi,k0φj,k00/	Pk0( 1⅛7)T
k =1, 2,∙∙∙ ,K, ∀i,j ∈ [N ].
(2)
Here τ denotes the “temperature” that controls the sharpness of weights, its value is specified in
Table 6 in the appendix. The edge weights produced from edge partitioner represent the propor-
tion that each community-based relation takes in contributing to edge formation. When τ is set
sufficiently small, A(1), A(2),…，A(K) would approximate binary factors graphs of G, We hence
refer to them as “relational graph factors”. Replacing γkΦi,kΦj,k with Φi,kdiag(γk)Φ0j,k gener-
alizes Equation 2 to metacommunity-based edge partition, in this expression, the total number of
communities is greater than K, and Φi,k denotes the kth segment in node i’s community-affiliation
encoding. The generalized edge Partitioner enriches the choices of Φ's dimensionalities, which
enhances the flexibility in model implementation.
Subsequent to the edge partitioner follows the relational GCN bank:
Relational GCN BANK (RGCNBANK). The relational GCN bank is made UPof K independent
GCN components, coupling with the corresponding relational graph factors. It takes X* := X ∣∣ Φ
as input2, and outputs g((10(X*), gθ20(X*),…，gθK0(X*); gθk)(∙) := GCNe(∙, A(k)), k ∈ [K].
The function of this design is to learn relation-wise separable node representations. We now qual-
itatively justify the fulfillment of this requirement. Without loss of generality, let us treat the rela-
tional graph factors as approximately binary, this means the each observed edge is annotated with
the relation that makes the greatest contribution in its generation. Let us denote the relations as
r1,r2, ∙∙∙ ,rk which are uniquely featured with communities c1,c2, ∙∙∙ ,cκ. Assuming node U
connects to its neighbors v1 and v2 by relations r1 and r2, respectively. Without edge partition, fea-
tures of v1 would be propagated to v2 in two runs of neighborhood aggregation even if they connects
to u with different community identities. This issue is moderated by the relational GCN bank, as
the aggregation path (v1, u) would only be sent to the GCN component for relation r1 whereas the
path (u, v2 ) would only exist in the GCN component corresponds to relation r2 . As a result, in each
GCN component, nodes only share features through edges explained by the same relation, we hence
consider the outputs of relational GCN bank as relation-specific.
Finally, we learn the representations of nodes from relation-specific sub-representations. This step
involves the representation composer module:
REPresentation COMPoser (REPCOMP). Let H(k) := gθ(k)(X*) denote the node representa-
tions learned from the k th relation, k ∈ [K ], andfunction f (∙) denote the representation ComPoSer
whose functionality is to project a composite of relation-specific node representations to one rep-
resentation matrix, i.e., HV = f(H(1), H⑵,…，H(K)) := GCNe (IIK=ιH(k0, A) ∙ We could
further obtain hG, the vector representation of graph G, by sending the node level representations
HV to a graph pooling layer, i.e., hG := GRAPHPOOL(HV).
Taking softmax on the feature dimension of the representation composer’s output gives the uncer-
tainty distribution of labels, from which we are able to make predictions on the category of the object
(node or graph). Cascading the edge partitioner, the community-GCN bank with the representation
composer yields the probability pe(y | A, Φ, X).
2.2 Training EPM-GCN: inference and learning
2.2.1	The reparameterizable evidence lower bound objective
We set the prior distribution of Φ as Φi,k iid Gam(α, β), ∀(i, k) ∈ [N] X [K], and use a community
encoder3 to approximate the posterior distribution p(Φ | A, y):
Community ENCoder (CENC). The community encoder models the variational posterior qω (Φ)
by a Weibull distribution with shape K and scale Λ, both parameters are learned by a GCN, i.e.,
2The operator k denotes “concatenation”.
3The community encoder is the only module in the inference network.
4
Under review as a conference paper at ICLR 2022
K k Λ = GCNω (X, A). A random sample from qω (Φ) could be created through the inverse CDF
transformation of a uniformly distributed variable, given as follows:
Φ = A Θ ( - log(1 - U))°(1°κ), Ui,k M U(0,1), ∀(i, k) ∈ [N] X [K].
The operators , and ◦ (in the front of superscript4) denote element-wise multiplication, division
and power, respectively.
To jointly infer the latent factors Φ and use them to learn the task, we optimize the evidence lower
bound objective (ELBO) L, given by Equation 3, i.e.,
L = Eqω(Φ) logPθ(yo, A | Φ, X)- Dkl(Qφ∣∣Pφ)
= Eqω(Φ) logpθ(yo | A, Φ, X) + Eqω(Φ) logpθ(A | Φ) - DKL(QΦ kPΦ),
(3)
where the notations PΦ and QΦ represent the prior and the variational posterior distributions of Φ,
respectively. The three right hand side terms correspond to the classification task, graph reconstruc-
tion, and KL-regularization, we thereby refer to them as Ltask , Lrec and LKL in the sequel. Note that
the aforementioned specifications of Φ's prior and variational posterior, as in Zhang et al. (2018a),
yield an analytical expression ofLKL.
2.2.2	The overall training algorithm
To effectively train EPM-GCNs, we separate the overall training algorithm into an unsupervised
pretrain stage, followed by a supervised finetune stage.
The unsupervised pretrain: The ultimate goal of the community encoder is to find a commu-
nity assignment according to which edge partitions would facilitate task learning. Since the overall
likelihood of node labels involves stacking multiple modules embodied by deep neural networks,
inferring the posterior p(Φ | A, yo) is difficult from an optimization perspective. To seek better
model convergence, we break this optimization problem into two stages, namely, we first train the
community encoder to discover the latent communities that explains the graph architecture, then
finetune the communities to align them to task-learning. The first stage is the unsupervised pretrain,
at which we pretrain the community encoder until convergence by optimizing Lrec + LKL, i.e., up-
dating model parameters ω towards ω*, which optimizes Eqω@)logpθ(A | Φ) — Dkl(Qφ∣∣P⅛)∙
The variational probability qω (Φ) is learned to approximate p(Φ | A). The heuristic that the un-
supervised pretrain is generally supportive to the ultimate goal is that although communities and
node categories describe node similarity from different aspects, these two aspects of similarity are
semantically mutually informative, i.e., the discovered communities from graph architecture is in-
herently correlated with node labels, which is suggested by previous work on graph representation
learning with community information (Sun et al., 2019), in which the node classification results
clearly benefit from the node representations that hold community information. Such assumption is
further qualitatively verified in Section 4.2. We find through practice that the unsupervised pretrain
not only stabilizes model convergence, but also provides a reasonable initial state for subsequent
supervised optimization thus leads to better performances.
The supervised finetune: This stage involves iteratively running inference and learning. In the in-
ference step, we fix the generative network, and train the inference network by optimizing the ELBO
L, given in Equation 3. Each inference step runs after M learning steps, where we treat the inference
network as stationary and update the generative network by optimizing Ltask . Both L and Ltask are
computed over the training set {yo , A, X}. The supervised finetune updates the approximation tar-
get of the inference network from p(Φ | A) to p(Φ | A, yo), which finetunes community inference
with label supervision, making the subsequent edge-partition more facilitative to the classification
task.
The entire training pipeline of EPM-GCNs is summarized in Algorithm 1 (in Appendix A). Af-
ter running the unsupervised pretrain and the supervised fintune stages, the variational distribution
qω (Φ) is trained to approximate the posterior p(Φ | yo, A). We hence approximate the predictive
distribution of unobserved labels with Eqω(Φ) [pθ(yu | A, Φ, X)], and estimate it via Monte Carlo
estimation. In particular, We estimate p(yu | A, X, No) with 1 PS=Ipθ(yu | A, Φ(s), X), where
Φ(S) iid qω(Φ) for S = 1,...,S. For simplicity, we set S = 1 unless specified otherwise.
4Otherwise, the ◦ notation in the context of f ◦ g denotes function composition.
5
Under review as a conference paper at ICLR 2022
Table 1: Statistics of the datasets for node and graph-level classification..
Task	Node Classification			Graph Classification							
Dataset	Cora	Citeseer	Pubmed	IMDB-B	IMDB-M	MUTAG	PTC	NCI1	PROTEINS	RDT-B	RDT-M
Graphs	1	1	1	-1000-	1500-	188	344	4110	1113	2000	5000
Edges	5,429	4,732	44,338	96.5	65.9	19.8	26.0	32.3	72.8	497.7	594.8
Features	1,433	3,703	500	65	59	7	19	37	3	566	734
Nodes	2,708	3,327	19,717	19.8	13.0	17.9	25.5	29.8	39.1	29.6	508.5
Classes	7	6	3	2	3	2	2	2	2	2	5
3 Related Work
Deep graph generative models: Advances in this line of work include adopting deep architectures
in learning node embeddings (Kipf & Welling, 2016; Li et al., 2018) and joint optimization with
associated graph-analytic tasks such as semi-supervised node classification (Hasanzadeh et al., 2019;
Wang et al., 2020), community discovery (Sun et al., 2019; Mehta et al., 2019), etc. Similar to
these models, we regularize the node embeddings Φ by its probabilistic dependency with the graph
structure, but instead of simply passing the learned embeddings to a black-box prediction network,
we use them to develop the generative process of the labels, leading to better interpretability.
GCNs modeling multi-relational data: The general idea is to individually process node repre-
sentations aggregated from different types of relations. This logistic is widely adopted by GCN
variants handling graph data where relational annotations are either explicit (Schlichtkrull et al.,
2018; Vashishth et al., 2020) or implicit. In the latter setup, the missing relations are conventionally
completed with human knowledge (Abu-El-Haija et al., 2019; Vashishth et al., 2020), or learned via
label supervision (Velickovic et al., 2018; Gao & Ji, 2019; Ma et al., 2019; Yang et al., 2020). Con-
trary to these methods which treat edges as given, we jointly model the generation of labels as well
as the edges, and systematically model the relations by posterior inference. The inferred relations
are then used to partition edges, learn relation-specific sub-representations, and finally optimize the
downstream tasks.
4 Empirical evaluation
We compare the proposed EPM-GCNs against related baselines on two fundamental tasks: node
classification and graph classification. In the sequel, we use the suffixes -n and -g to distinguish the
variants of EPM-GCN developed for node and graph-level classification tasks, respectively.
4.1	Datasets preparation
We evaluate EPM-GCNs on 11 benchmarks. For node classification, we use 3 citation networks:
Cora, Citeseer and Pubmed. All three datasets provide bag-of-words document representations as
node features and (undirected) citations as edges. For graph classification, we use 4 bioinformat-
ics datasets (MUTAG, PTC, NCI1, PROTEINS) and 4 social network datasets (IMDB-BINARY,
IMDB-MULTI, REDDIT-BINARY and REDDIT-MULTI). The input node features are crafted in
the same way with Xu et al. (2019). We summarize the statistcis of selected datasets in Table 1.
4.2	V isualizing learned representations
To qualitatively assess the proposed model, we visualize the spatial distribution of Φ obtained
at the end of the unsupervised pretrain stage (Figures 2(a), 2(b)), and the relation-specific sub-
representations obtained at the end of the supervised finetune stage (Figures 2(c), 2(d)). Node repre-
sentation are projected to 2-D space via t-SNE (Van der Maaten & Hinton, 2008). We select Cora and
MUTAG as representatives of the node and graph classification datasets. For the MUTAG dataset,
we remove less than 10 graphs that contain node categories with less than 5 instances and randomly
sample 10 graphs for visualization. In Figures 2(a) and 2(b), the spatial clustering of nodes from the
same class indicates strong correlation between inferred latent communities and node labels, which
not only demonstrates our model’s ability in capturing community structure (even in small graphs
with only tens of nodes), but also positively supports our model designs, including (i) input feature
augmentation: X* = X ∣∣ Φ and (ii) edge partition. Figures 2(c) and 2(d) further justifies that
the sub-representations {g(11)(X*), gθ2)(X*),…,gθK)(X*)} are not repetitive, signaling that the
proposed model successfully extracts relation-specific representations, which potentially enhance
6
Under review as a conference paper at ICLR 2022
the discriminative power of node or graph representations and lead to better performances on the
downstream tasks.
(a) Φ, Cora	(b) Φ, MUTAG	(C) g/ (X*), Cora	(d) gθ') (X*), MUTAG
Figure 2: Visualization of the spatial distribution of node representations learned by EPM-GCNs on Cora
and MUTAG datasets, mapped to 2-D spaCe. (a) and (b) plot the t-SNE projeCtions of nodes’ Community-
affiliation sCores (i.e., rows in Φ) at the end of the unsupervised pretrain stage, where nodes are Colored by their
Categories. (C) and (d) plot the t-SNE projeCtions of relation-speCifiC sub-representations of all nodes over all
types of relations obtained at the end of the supervised finetune stage, the Colors annotate outputs from different
GCN Components in the relational GCN bank (i.e., eaCh Color Corresponds to a relation, eaCh node Corresponds
to K points in different Colors). Spatially Clustering of eaCh Colored group supports our Community-based edge
partition and node feature augmentation ((a), (b)), and justifies the learned relation-speCifiC sub-representations
are indeed relation-wise separable.
4.3	Node classification
Experimental settings: To make a fair Comparison, we use the train/test/validation split standard-
ized by the vanilla GCN (Kipf & Welling, 2017) for Cora, Citeseer and Pubmed datasets. For the
community encoder, we adopt a similar enCoder struCture devised by Kipf & Welling (2016) with
softplus aCtivations, whiCh outputs 16-dimensional Community-affiliation sCores for eaCh node. We
set the number of metaCommunities K = 4 for Cora and Citeseer and K = 8 for Pubmed dataset,
the values of K are seleCted through Cross-validation. The relational graph faCtors are then obtained
through the generalized edge partition. Before being sent to the relational GCN bank, these graph
faCtors are row-normalized by sparse softmax. We implement the relational GCN bank with K
single-layer GCNs with ReLU activation, following (VeliCkoViC et al., 2018), We set their output
dimensions as 64/K . The representation composer is also implemented with a single-layer GCN.
With these speCifiCations, the amount of parameters in the generative network remains Comparable
with that ofa 2-layer GCN with 64 hidden units (GCN-64) regardless of the ChoiCe ofK. We reCord
the Complete hyperparameter speCifiCations in Table 6 (in Appendix C).
Performance comparison: We use
ClassifiCation aCCuraCy as the evaluation
metriC in the node ClassifiCation tasks.
Table 2 reports EPM-GCN-n’s average
performanCes (± standard error) against
the paper reCords of the average perfor-
manCes of 3 groups of related baselines
on the three Citation network datasets,
where * marks the results reported by
VeliCkoviC et al. (2018). The first group
Table 2: Comparisons on node ClassifiCation performanCes.
Method	Cora	Citeseer	Pubmed
ChebNet (Defferrard et al., 2016)	81.2	69.8	74.4
GCN (Kipf & Welling, 2017)	81.5	70.3	79.0
GCN-64*	81.4	70.9	79.0
SIG-VAE (Hasanzadeh et al., 2019)	797~~	70.4	79.3
WGCAE (Wang et al., 2020)	82.0	72.1	79.1
-GAT* (VeIiCkoViC et al., 2018)^^	83.0~~	72.5	79.0
hGAO (Gao & Ji, 2019)	83.5	72.7	79.2
DisenGCN (Ma et al., 2019)	83.7	73.4	80.5
EPM-GCN-n (this work)	84.0 ± 0.1	72.4 ± 0.1	82.2 ± 0.2
(Defferrard et al., 2016; Kipf & Welling, 2017) are plain GCNs that aggregated node features with
only one set of weighted edges. These models are designed to handle at most two types of node
relations: ConneCted or not, so it is not surprised to find that EPM-GCN-n outperforms these models
by a wide margin. SinCe removing (i) relational inferenCe and (ii) relation-speCifiC feature learning
degenerates EPM-GCN-n to GCN-64, the improvement of our model against GCN-64 Could be ex-
plained as the joint effeCts of the mentioned two praCtiCes. The seCond group (Hasanzadeh et al.,
2019; Wang et al., 2020) are graph generative models that jointly optimized with semi-supervised
loss. Both SIG-VAE (Hasanzadeh et al., 2019) and WGCAE (Wang et al., 2020) adopt similar graph
generative model and loss struCture to ours, the differenCe is that these models direCtly projeCts the
latent representations to node labels by a deterministiC prediCtion network, whereas we sample la-
bels from a edge-partition based label generation proCess. Therefore our improvement with respeCt
to the seCond group Could be attributed to the marginal benefit from our label generative model. The
third group of models (VeliCkoviC et al., 2018; Gao & Ji, 2019; Ma et al., 2019) employ multi-head
7
Under review as a conference paper at ICLR 2022
Table 3: Comparisons on graph classification performances.
Method	IMDB-B IMDB-M MUTAG PTC NCI1 PROTEINS RDT-B RDT-M
WL subtree (Shervashidze et al., 2011)
AWL (Ivanov & Burnaev, 2018)
DCNN (Atwood & Towsley, 2016)
DGCNN (Zhang et al., 2018b)
PATCHYSAN (Niepert et al., 2016)
GIN-0 (Xu et al., 2019)
GIN- (Xu et al., 2019)
FactorGCN (Yang et al., 2020)
GCN (Kipf & Welling, 2017)
GAT (Velickovic et al., 2018)
GraphSAGE (Hamilton et al., 2017)
EPM-GCN-g (this work)
73.8 ± 3.9
74.5 ± 5.9
49.1 ± 0.0
70.0 ± 0.0
71.0 ± 2.2
75.1 ± 5.1
74.3 ± 5.1
75.3 ± 2.7
74.0 ± 3.4
70.5 ± 2.3
72.3 ± 5.3
50.9 ± 3.8
51.5 ± 3.6
33.5 ± 0.0
47.8 ± 0.0
45.2 ± 2.8
52.3 ±2.8
52.1 ± 3.6
-
51.9 ± 3.8
47.8 ± 3.1
50.9 ± 2.2
90.4 ± 5.7
87.9 ± 9.8
67.0 ± 0.0
85.8 ± 0.0
92.6 ± 4.2
89.4 ±5.6
89.0 ± 6.0
89.9 ± 6.5
85.6 ± 5.8
89.4 ± 6.1
85.1 ± 7.6
59.9 ± 4.3
-
56.6 ± 0.0
58.6 ± 0.0
60.0 ± 4.8
64.6 ± 7.0
63.7 ± 8.2
-
64.2 ± 4.3
66.7 ± 5.1
63.9 ± 7.7
86.0 ± 1.8
-
62.6 ± 0.0
74.4 ± 0.0
78.6 ± 1.9
82.7 ± 1.7
82.7 ± 1.6
-
80.2 ± 2.0
-
77.7 ± 1.5
75.0 ± 3.1	81.0 ± 3.1	52.5 ± 2.1
-	87.9 ± 2.5	54.7 ± 2.9
61.3 ±	0.0	-	-
75.5 ±	0.0	-	-
75.9 ±	2.8	86.3 ± 1.6	49.1	±	0.7
76.2 ±	2.8	92.4 ± 2.5	57.5	±	1.5
75.9 ±	3.8	922 ± 2.3	57r0	±	1.7
-	--
76.0 ± 3.2	50.0 ± 0.0 20.0 ± 0.0
-	--
75.9	±	3.2	-	-
80.5	± 2.8	90.5 ± 1.8 55.0 ± 1.5
76.7 ± 3.1	54.1 ± 2.1	93.6 ± 3.4 75.6 ± 5.9 83.9 ± 1.8
Table 4: ComParisons of EPM-GCN-g with varioUs inPUt node featUres.					
Random	Hand-crafted Community-based	IMDB-B	IMDB-M	MUTAG	PTC
X		64.7 ± 1.6	42.3 ± 1.5	84.6 ± 4.3	63.6 ± 2.0
	X	80.3 ± 2.0	53.5 ± 2.6	93.1 ± 5.0	74.7 ± 4.1
	X	74.7 ± 5.1	51.5 ± 2.0	88.9 ± 5.5	68.9 ± 3.9
	XX	76.7 ± 3.1	54.1 ± 2.1	93.6 ± 3.5	75.6 ± 5.9
and adaptively weighted feature aggregation mechanisms, which greatly enhances their ability in
handling multi-relational data. However, these models lack systematic modeling of latent relations,
which on the other hand is the strength of our model. Our performance boost against the third group
on most of the benchmarks demonstrates the marginal benefit of our relational inference model.
4.4	Graph classification
Experimental settings: For EPM-GCN-g, we implement the community encoder with a 2-layer
GCN and produces the latent community-affliation encoding with 100 dimensions. The number of
metacommunities is set to K = 4 and corresponding relational graph factors are generated through
the generalized edge partition. We choose GIN layer as the building block to define the rest of the
model, and set the total network depth as 5 (including the input layer) such that EPM-GCN-g is
comparable with the vanilla GIN (Xu et al., 2019). We define the K components in the relational
GCN bank by sharing the input layer and evenly partitioning the neuron units in the 2 top hidden lay-
ers. The 2 ending layers are treated as the representation composer. The remaining implementation
details are included in Table 6 (in Appendix C).
Performance comparison: We follow GIN (Xu et al., 2019) to perform 10-fold cross validation for
each dataset and report the average classification accuracy (± standard error). As shown in Table 3,
we compare the developed EPM-GCN-g with several state-of-the-art baselines for graph classifica-
tion, including the WL subtree kernel (Shervashidze et al., 2011), Anonymous Walk Embeddings
(AWL) (Ivanov & Burnaev, 2018) and other deep learning architectures like DCNN (Atwood &
Towsley, 2016), DGCNN (Zhang et al., 2018b), and PYTCHY-SAN (Niepert et al., 2016). We also
compare EPM-GCN-g with five supervised GNNs including GCN (Kipf & Welling, 2017), GAT
(Velickovic et al., 2018), GraPhSAGE (Hamilton et al., 2017), and two variants of GIN (XU et al.,
2019): GIN-0 and GIN-. Thanks to our uniquely designed multi-relational inference as well as
relation-sPecific graPh rePresentation learning and comPosition, EPM-GCN-g achieves state-of-the-
art graPh classification Performances on 5 oUt of8 benchmarks, and the second-best Performance on
NCI1 dataset, where it oUtPerforms all the other deeP architectUres. Notably, EPM-GCN-g achieves
75.6% accUracy on PTC dataset with a significant 7.9% imProvement over the second Place.
4.5	Ablation study
To fUrther stUdy the marginal benefit of each Practice we take in imPlementing EPM-GCN, as well
as model’s overall sensitivity to variations in hyPerParameter setUP, we evalUate EPM-GCN’s graPh
classification Performances over varioUs choices of inPUt featUres, nUmber of metacommUnities, and
network strUctUre.
Various input features: We first evalUate model Performances with foUr tyPes of inPUts: random
noise, hand-crafted featUres by XU et al. (2019), commUnity-affiliation scores, and hand-crafted
featUres concatenated with commUnity-affiliation scores. ResUlts are shown in Table 4. The classi-
fication accUracy measUres the discriminative level of graPh rePresentations learned from these foUr
tyPes of inPUts, comParing the first and second rows with the third row tells Us that commUnity-
affiliation scores are meaningfUl node rePresentations and comParably informative to the task as
8
Under review as a conference paper at ICLR 2022
70
0	2	4	6	8	10	12	14	16
Number of Communities
(a) IMDB-B
44
0	2	4	6	8	10	12	14	16
Number of Communities
(b) IMDB-M
75
0	2	4	6	8	10	12	14	16
Number of Communities
(c) MUTAG
2	4	6	8	10	12	14	16
Number of Communities
(d) PTC
Figure 3: The average graph classification accuracy of EPM-GCN-g with various settings on the numer of
metacommunities K ∈ {1, 2, 4, 8, 16}. Results on (a) IMDB-BINARY, (b) IMDB-MULTI, (c) MUTAG, and
(d) PTC datasets are shown in the subfigures.
hand-crafted node features. We could further conclude by comparing the second row with the fourth
row that the information Φ and X hold for the task enhances the discriminative level of learned
representations in a complementary manner. Both conclusions justify the quality of Φ as additional
node features.
Number of communities: We next study the variation of the model’s performances with growing
number of metacommunities. Figure 3 plots the average graph classification accuracy over 10-
fold cross-validation against different selections of K ∈ {1, 2, 4, 8, 16}. In the context of graph
classification, K denotes the number of all metacommunities with at least one presence among the
graphs in the dataset. We could see that the optimal results are typically achieved with K > 1, which
is in consistent with the model assumption that the graph contains multiple underlying relations, the
result also reflects the efficacy of the proposed model in modeling heterogeneous latent relations.
Network structures: Among our proposed results in Table 3, the network structure for the label
generation is not particularly fine-selected and hidden layers are evenly distributed to the relational
GCN bank and the representation composer, In this part, we focus on studying how the balance
between these two modules influences the model performance on downstream tasks. We use the
name convention EPM-GCN-g-{L1}-{L2} to denote the variant with L1 layers (including the input
layer) in the relational GCN bank and L2 layers in the representation composer, where L1 > 0 and
L1 + L2 = 5. As Table 5 shows, although the optimal layer assignments for each dataset are not
the same, i.e., the IMDB-B and IMDB-M datasets generally require less relation-wise locality in
node representations than MUTAG or PTC datasets, both modules are verified to be necessary for
the graph-analytic tasks, because neither EPM-GCN-g-{1}-{4} nor EPM-GCN-g-{5}-{0} triumphs
over other layer combinations in any of these experiments. Additionally, the variation of model
performances under different layer assignment demonstrates that datasets with larger amount of
graphs are generally more insensitive to the change of model architecture, so the way to increase
model robustness on datasets with less graphs would be a topic worth studying in the future.
Table 5: Comparisons of EPM-GCN-g with various network structures.
Network Structures
GIN-0 (Xu et al., 2019)
GIN- (Xu et al., 2019)
EPM-GCN-g-{1}-{4}
EPM-GCN-g-{2}-{3}
EPM-GCN-g-{3}-{2}
EPM-GCN-g-{4}-{1}
EPM-GCN-g-{5}-{0}
IMDB-B IMDB-M MUTAG PTC
75.1 ± 5.1
74.3 ± 5.1
75.6 ± 3.6
77.8 ± 2.0
76.7 ± 3.1
74.5 ± 2.2
74.2 ± 2.9
52.3 ±2.8
52.1 ± 3.6
55.4 ± 2.7
56.3 ± 2.1
54.1 ± 2.1
54.3 ± 3.2
51.2 ± 2.5
89.4 ±5.6
89.0 ± 6.0
89.8 ± 7.6
91.0 ± 4.9
93.6 ± 3.5
87.0 ± 6.8
80.4 ± 10.4
64.6 ± 7.0
63.7 ± 8.2
70.9 ± 5.3
73.3 ± 3.6
75.6 ± 5.9
66.3 ± 5.0
63.1 ± 6.2
5 Conclusion
Moving beyond treating the graph adjacency matrix as given, we develop novel EPM-GCNs to
aggregate the node interactions over multiple overlapping node communities. More specifically, we
first construct a community encoder to project each node to its community-affiliation scores, and then
partition the edges between nodes according to the edge-formation contribution of each community-
based relation with a edge partitioner. Then we obtain the relation-specific sub-representations
with the relation GCN bank and finally put them together to make predictions with a representation
composer. Extensive qualitative and quantitative experiments on both node and graph-level has been
made to demonstrate the efficacy of our model.
9
Under review as a conference paper at ICLR 2022
Reproducibility statement
The PyTorch code to reproduce the results in the paper has been included in the supplementary
material. Detailed hyperparameter settings have been provided in Table 6 in the Appendix.
References
Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Kristina Lerman, Hrayr
Harutyunyan, Greg Ver Steeg, and Aram Galstyan. Mixhop: Higher-order graph convolutional
architectures via sparsified neighborhood mixing. In International Conference on Machine Learn-
ing (ICML), Proceedings of Machine Learning Research, pp. 21-29, 2019.
James Atwood and Don Towsley. Diffusion-convolutional neural networks. In Advances in neural
information processing systems, pp. 1993-2001, 2016.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks
on graphs with fast localized spectral filtering. In Advances in Neural Information Processing
Systems (NeurIPS), volume 29, 2016.
Hongyang Gao and Shuiwang Ji. Graph representation learning via hard and channel-wise attention
networks. In ACM SIGKDD International Conference on Knowledge Discovery & Data Mining
(KDD), pp. 741-749, 2019.
William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In Advances in Neural Information Processing Systems (NeurIPS), volume 30, 2017.
Arman Hasanzadeh, Ehsan Hajiramezanali, Krishna Narayanan, Nick Duffield, Mingyuan Zhou,
and Xiaoning Qian. Semi-implicit graph variational auto-encoders. In Advances in Neural Infor-
mation Processing Systems (NeurIPS), volume 32, pp. 10711-10722, 2019.
Sergey Ivanov and Evgeny Burnaev. Anonymous walk embeddings. In International conference on
machine learning, pp. 2186-2195. PMLR, 2018.
Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint
arXiv:1611.07308, 2016.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In International Conference on Learning Representations (ICLR), 2017.
Johannes Klicpera, Aleksandar Bojchevski, and Stephan Gunnemann. Predict then propagate:
Graph neural networks meet personalized pagerank. In International Conference on Learning
Representations (ICLR), 2019a.
Johannes Klicpera, Stefan WeiBenberger, and Stephan Gunnemann. Diffusion improves graph learn-
ing. In Advances in Neural Information Processing Systems (NeurIPS), volume 32, pp. 13354-
13366, 2019b.
Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter Battaglia. Learning deep generative
models of graphs. arXiv preprint arXiv:1803.03324, 2018.
Jianxin Ma, Peng Cui, Kun Kuang, Xin Wang, and Wenwu Zhu. Disentangled graph convolutional
networks. In International Conference on Machine Learning (ICML), volume 97 of Proceedings
of Machine Learning Research, pp. 4212-4221, 2019.
Nikhil Mehta, Lawrence Carin, and Piyush Rai. Stochastic blockmodels meet graph neural net-
works. In International Conference on Machine Learning (ICML), volume 97 of Proceedings of
Machine Learning Research, pp. 4466-4474, 2019.
Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. A review of relational
machine learning for knowledge graphs. Proceedings of the IEEE, 104(1):11-33, 2015.
Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural net-
works for graphs. In International conference on machine learning, pp. 2014-2023. PMLR, 2016.
10
Under review as a conference paper at ICLR 2022
Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max
Welling. Modeling relational data with graph convolutional networks. In European semantic web
conference (ESWC), pp. 593-607. Springer, 2018.
Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn, and Karsten M Borg-
wardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(9), 2011.
Fan-Yun Sun, Meng Qu, Jordan Hoffmann, Chin-Wei Huang, and Jian Tang. vgraph: A generative
model for joint community detection and node representation learning. In Advances in Neural
Information Processing Systems (NeurIPS), volume 32, 2019.
Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
learning research, 9(11), 2008.
Shikhar Vashishth, Soumya Sanyal, Vikram Nitin, and Partha Talukdar. Composition-based multi-
relational graph convolutional networks. In International Conference on Learning Representa-
tions (ICLR), 2020.
Petar VeliCkovic, GUillem CUcUrUlL Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representations
(ICLR), 2018.
Chaojie Wang, Hao Zhang, Bo Chen, Dongsheng Wang, ZhengjUe Wang, and MingyUan ZhoU.
Deep relational topic modeling via graph poisson gamma belief network. In Advances in Neural
Information Processing Systems (NeurIPS), volUme 33, 2020.
KeyUlU XU, WeihUa HU, JUre Leskovec, and Stefanie Jegelka. How powerfUl are graph neUral
networks? In International Conference on Learning Representations (ICLR), 2019.
Yiding Yang, ZUnlei Feng, Mingli Song, and Xinchao Wang. Factorizable graph convolUtional
networks. In Advances in Neural Information Processing Systems (NeurIPS), volUme 33, pp.
20286-20296, 2020.
Hao Zhang, Bo Chen, Dandan GUo, and MingyUan ZhoU. WHAI: WeibUll hybrid aUtoencoding
inference for deep topic modeling. In International Conference on Learning Representations
(ICLR), 2018a.
MUhan Zhang, Zhicheng CUi, Marion NeUmann, and Yixin Chen. An end-to-end deep learning
architectUre for graph classification. In Proceedings of the AAAI Conference on Artificial Intelli-
gence, volUme 32, 2018b.
MingyUan ZhoU. Infinite edge partition models for overlapping commUnity detection and link pre-
diction. In International Conference on Artificial Intelligence and Statistics (AISTATS), volUme 38
of Proceedings of Machine Learning Research, pp. 1135-1143, 2015.
11
Under review as a conference paper at ICLR 2022
Appendix
A The pseudocode of training algorithm
Algorithm 1: The overall training algorithm of EPM-GCN
Data: node features X, observed edges A and observed labels Zo
Modules	: CENC, EPART, RGCN_BANK and REPCOMP
Parameters: ω in the inference network, and θ in the generative network
Initialize ω and θ;
while the community encoder is not converged do
Φ, K, Λ J CENC(A, X);
Compute Lrec and LKL, given in Equation 3;
ω J ω 一 ηunsup ∙ Vω(Lrec + LKL) ；	/* * Unsupervised pretrain */
end
while the whole model is not converged do
Φ, K, Λ J CENC(A, X);
A⑴,A⑵,…，A(K) J EPART(Φ, A);
for step J 1 to M do
pyo J SoftmaX(REPCOMP ◦ RGCN-BANK(Φ, X, A⑴,A⑵，…，A(K)));
Compute Ltask, given in Equation 3;
θ J θ 一 ηsup,θ ∙VθLtask ；	/* The learning step */
end
Compute L;
ω J ω 一 ηsup,ω ∙VωL ；	/* The inference step */
end
B	Property of Weibull distribution
• Similar PDF with Gamma Distribution
The Weibull distribution owns similar probability density functions (PDF) with a gamma one, which
makes it flexible to model sparse and nonnegative latent representations:
WeibullPDF: P(x∣k,λ) = ^kXkTe(X")k,
λβα	(4)
Gamma PDF: P(x∣α,β) =	XaTe-βx.
Γ(α)
• Easily Reparameterization
The latent variable X 〜 Weibull(k, λ) can be easily reparameterized as
x = λ(- ln(1 一 ε))1∕k, ε 〜Uniform(0,1),	(5)
leading to a similar gradient calculation with the Gaussian reparameterization.
• Analytic KL-Divergence
Moverover, the KL-divergence between the Weibull and gamma distributions has an analytic ex-
pression formulated as
KL(WeibUll(k,λ)∣∣Gamma(α,β)) = -α ln λ + Ya
1	k	(6)
+ ln k + βλΓ(1 + —) — Y — 1 — a ln β + lnΓ(a).
12
Under review as a conference paper at ICLR 2022
C Implemetation details
In our experiments, we find the initial state of inference module provided by the unsupervised pre-
train stage is essential to EPM-GCN’s performances.
The hyperparameters are either inherited from base models or selected on the basis of cross-
validation. Note that for the graph-level classification task, the parameters are shared across ex-
periments on all datasets. And the reported results are potentially improvable if dataset-specific
cross-validations are applied.
Table 6: Hyperparameters settings for EPM-GCN.		
Hyperparameters	Experiments	
	Node Classification	Graph Classification
Community ENCoder	Settings	
α	1	1
β	1	1
epoches of unsupervised pretrain	1500	1500
learning rate of unsupervised pretrain	1e-3	1e-2
batch size of unsupervised pretrain	1	32
type of GNN layers	GCN	GCN
module structure	{32}-{16}	{200}-{100}
Edge PARTitioner	Settings	
number of communities	[4,8]	4	
the temperature τ for partition	1	一	1
Relational GCN BANK	Settings	
epoches of jointly training	200	100
learning rate of jointly training	1e-2	1e-3
weight decay of jointly training	5e-4	0.0
batch size of jointly training	1	32
concat weight of Φ	3e-2	3e-4
type of GNN layers	GCN	GIN
module structure	{64}	{64}-{64}
REPresentation COMPoser	Settings	
epoches of jointly training	200	100
learning rate of jointly training	1e-2	1e-3
weight decay of jointly training	5e-4	0.0
batch size of jointly training	1	32
type of GNN layers	GCN	GIN
module structure	{class num}	{64}-{64}-{class num}
13