Under review as a conference paper at ICLR 2022
Conditional GANs with Auxiliary Discrimina-
tive Classifier
Anonymous authors
Paper under double-blind review
Ab stract
Conditional generative models aim to learn the underlying joint distribution of
data and labels, and thus realize conditional generation. Among them, auxiliary
classifier generative adversarial networks (AC-GAN) have been widely used, but
suffer from the problem of low intra-class diversity on generated samples. In
this paper, we point out that the fundamental reason is that the classifier of AC-
GAN is generator-agnostic, and therefore cannot provide informative guidance
to the generator to approximate the target distribution, resulting in minimization
of conditional entropy that decreases the intra-class diversity. Motivated by this
observation, we propose a novel conditional GAN with auxiliary discriminative
classifier (ADC-GAN) to resolve the problem of AC-GAN. Specifically, the pro-
posed auxiliary discriminative classifier becomes generator-aware by recognizing
the labels of the real data and the generated data discriminatively. Our theoretical
analysis reveals that the generator can faithfully replicate the target distribution
even without the original discriminator, making the proposed ADC-GAN robust
to the hyper-parameter and stable during the training process. Extensive experi-
mental results on synthetic and real-world datasets demonstrate the superiority of
ADC-GAN on conditional generative modeling compared to competing methods.
1	Introduction
Generative adversarial networks (GANs) (Goodfellow et al., 2014) have been gained great progress
in learning high-dimensional, complex data distribution such as natural images (Karras et al., 2019;
2020b;a; Brock et al., 2019). Standard GANs consist of a generator network that transfers a latent
code sampled from a tractable distribution in the latent space to a data point in the data space and a
discriminator network that attempts to distinguish between the real data and the generated one. The
generator is trained in an adversarial game against the discriminator such that it can replicate the data
distribution at the Nash equilibrium of the game. Remarkably, the training of GANs is notoriously
unstable to reach the equilibrium, and thereby the generator is prone to mode collapse (Salimans
et al., 2016; Lin et al., 2018; Chen et al., 2019). In addition, practitioners are interested in controlling
the properties of the generated samples (Yan et al., 2015; Tan et al., 2020) in practical applications.
A key solution to address the above issues is conditioning, leading to conditional GANs (Mirza &
Osindero, 2014).
Conditional GANs (cGANs) is a family variant of GANs that leverages the side information from
annotated labels of samples to implement and train a conditional generator, and therefore achieve
conditional image generation from class-label (Odena et al., 2017; Miyato & Koyama, 2018; Brock
et al., 2019) or text (Reed et al., 2016; Xuet al., 2018; Zhu et al., 2019). To implement the conditional
generator, the common technique nowadays injects the conditional information via conditional batch
normalization (de Vries et al., 2017). To train the conditional generator, a lot of efforts focus on
effectively injecting the conditional information into the discriminator or classifier (Odena, 2016;
Miyato & Koyama, 2018; Zhou et al., 2018; Kavalerov et al., 2021; Kang & Park, 2020; Zhou et al.,
2020). Among them, the auxiliary classifier generative adversarial network (AC-GAN) (Odena et al.,
2017) has been widely used due to its simplicity and extensibility. Specifically, AC-GAN utilizes
an auxiliary classifier that first attempts to recognize the label of data and then teaches the generator
to produce label-consistent (classifiable) data. However, it has been reported that AC-GAN suffers
from the low intra-class diversity problem on generated samples, especially on datasets with a large
number of classes (Odena et al., 2017; Shu et al., 2017; Gong et al., 2019).
1
Under review as a conference paper at ICLR 2022
In this paper, we point out that the fundamental reason for the low intra-class diversity problem of
AC-GAN is that the classifier is agnostic to the generated data distribution and thus cannot provide
informative guidance to the generator in learning the target distribution. Motivated by this obser-
vation, we propose a novel conditional GAN with an auxiliary discriminative classifier, namely
ADC-GAN, to resolve the problem of AC-GAN by enabling the classifier to be aware of the gen-
erated data distribution. To this end, the discriminative classifier is trained to distinguish between
the real and generated data while recognizing their labels. The discriminative property enables the
classifier to provide the discrepancy between the real and generated data distributions analogy to
the discriminator, and the classification property allows it to capture the dependencies between the
data and labels. We show in theory that the generator of the proposed ADC-GAN can replicate the
joint data and label distribution under the guidance of the discriminative classifier at the optima even
without the discriminator, making our method robust to hyper-parameter and stable on training. We
also discuss the superiority of ADC-GAN compared to two most related works (TAC-GAN (Gong
et al., 2019) and PD-GAN (Miyato & Koyama, 2018)) by analyzing their potential issues and lim-
itations. Experimental results clearly show that the proposed ADC-GAN successfully resolves the
problem of AC-GAN by faithfully learning the real joint data and label distribution. The advantages
over competing cGANs in experiments conducted on both synthetic and real-world datasets verify
the effectiveness of the proposed ADC-GAN in conditional generative modeling.
2	Preliminaries and Our Analysis
2.1	Generative Adversarial Networks
Generative adversarial networks (GANs) (Goodfellow et al., 2014) consist of two types of neural
networks: the generator G : Z → X that maps a latent code z ∈ Z endowed with an easily sampled
distribution PZ to a data point x ∈ X, and the discriminator D : X → [0, 1] that distinguishes
between real data that sampled from the real data distribution PX and fake data that sampled from
the generated data distribution QX = G ◦ PZ implied by the generator. The goal of the generator
is to confuse the discriminator by producing data that is as real as possible. Formally, the objective
functions for the discriminator and the generator are defined as follows:
min max V(G, D) = Ex〜PX [log D(x)] + Ex〜QX [log(1 — D(x))].
(1)
Theoretically, the learning of generator under an optimal discriminator can be regarded as minimiz-
ing the Jensen-Shannon (JS) divergence between the real data distribution and the generated data
distribution, i.e., minG JS(PX kQX). This would enable the generator to recover the real data distri-
bution at its optima. However, the training of GANs is notoriously unstable, especially when lacking
additional supervision such as conditional information. Moreover, the content of the generated im-
ages of GANs cannot be specified in advance.
2.2	AC-GAN
Learning GANs with conditional information can not only improve the training stability and gen-
eration quality of GANs but also achieve conditional generation, which has more practical value
than unconditional generation in real-world applications. One of the most representative conditional
GANs is AC-GAN (Odena et al., 2017), which utilizes an auxiliary classifier C : X → Y to learn
the dependencies between the real data X 〜Pχ and the label y 〜Pγ and then enforce the Condi-
tional generator G : Z × Y → X to synthesize classifiable data as much as possible. The objective
functions for the discriminator D, the auxiliary classifier C , and the generator G of AC-GAN are
defined as follows1:
max V(G,D) + λ ∙ (Ex,y〜Pχ,γ [log C(y|x)]),
D,C
mGn V(GD- λ ∙ (Ex,y〜Qx,y [log C(y|x)]),
(2)
where λ > 0 is a hyper-parameter, and Qχ,Y = G ◦ (PZ × PY) denotes the joint distribution of
generated data and labels implied by the generator.
1We follow the common practice in the literature to adopt the stable version instead of the original one.
2
Under review as a conference paper at ICLR 2022
Figure 1: Illustration of discriminators/classifiers of existing conditional GANs (PD-GAN (Miyato
& Koyama, 2018), AC-GAN (Odena et al., 2017), and TAC-GAN (Gong et al., 2019)) and the
proposed ADC-GAN. l indicates real (l = 1) or fake (l = 0) and y is the class-label of data x. ADC-
GAN is different from PD-GAN with explicitly predicting the label and is different with AC-GAN
and TAC-GAN that the classifier Cd also distinguishes real from fake like the discriminator.
I W)
(d) ADC-GAN
Proposition 1. The optimal classifier of AC-GAN outputs as follows:
c*(y∣χ)
p(χ,y)
p(x)
(3)
Theorem 1.	Given the optimal classifier, at the equilibrium point, optimizing the classification task
for the generator of AC-GAN is equivalent to:
minKL(QX,YkPX,Y)-KL(QXkPX)+HQ(Y|X),	(4)
where HQ(Y |X) = -	y q(x, y) log q(y|x)dx is the conditional entropy of generated data.
The proofs of Proposition 1 and Theorem 1 are referred to Appendix A.1 and A.2, respectively.
Our Theorem 1 exposes two shortcomings of AC-GAN. First, maximization of the KL divergence
between the marginal generator distribution and the marginal data distribution maxG KL(QX kPX)
contradicts the goal of conditional generative modeling that matches QX,Y with PX,Y . Although
this issue can be mitigated to some extent by the adversarial training objective between the discrimi-
nator and the generator that minimizes the JS divergence between the two marginal distributions, we
find that it still has a negative impact on the training stability. Second, minimization of the entropy
of label conditioned on data with respect to the generated distribution minG HQ(Y |X) will result in
that the label of generated data should be completely determined by the data itself. In other words,
it will force the generated data of each class away from the classification hyper-plane, explaining
the low intra-class diversity of generated samples in AC-GAN especially when the distributions of
different classes have non-negligible overlap, which is supported by the fact that state-of-the-art clas-
sifiers nor human cannot achieve 100% accuracy on real-world datasets (Russakovsky et al., 2015).
Note that the original version of AC-GAN, whose classifier is trained by both real and generated
samples, could also suffer from the same issue (see Appendix B).
3	The Proposed Method: ADC-GAN
The goal of conditional generative modeling is to faithfully approximate the joint distribution of
real data and labels regardless of the shape of the target joint distribution (whether there is overlap
between distributions of different classes). Note that the learning of the generator in AC-GAN is
affected by the classifier. In other words, the reason for the consequence of Theorem 1 originates
from Proposition 1, which indicates that the optimal classifier of AC-GAN is agnostic to the density
of the generated (marginal or joint) distribution (q(x) or q(x, y)). Therefore, the classifier cannot
provide the discrepancy between the target distribution and the generated distribution, resulting in a
3
Under review as a conference paper at ICLR 2022
Table 1: Comparison of objective of the generator under the optimal discriminator and classifier.
Method	Objective of the generator under the optimal discriminator and classifier
AC-GAN	minG JS(PXIlQX) + λ ∙ (KL(Qχ,γ∣Pχ,γ) - KL(QX∣Pχ) + HQ(Y|X))
TAC-GAN	minG JS(PXIIQX) + λ ∙ (KL(QX,y∣∣px,y) - KL(QX∣∣px))
ADC-GAN	minG JS(PXIIQx) + λ ∙ (KL(QX,y∣∣Px,y))
PD-GAN	minG JS(Qx,y∣∣PX,y)
biased learning objective to the generator. Recall that the optimal discriminator D*(x) = ?(*+)(乃
is able to be aware of the real data density as well as the generated data density (Goodfellow et al.,
2014), and thus can provide the discrepancy P(I) = 1001% between the real data distribution
and the generated data distribution to unbiasedly optimize the generator. Intuitively, the density-
aware ability on both real and generated data is caused by the fact that the discriminator attempts to
distinguish between real and fake samples. Motivated by this observation, we propose to make the
classifier to be distinguishable between real and fake samples, establishing a discriminative classifier
Cd : X → Y × {0, 1} that recognizes the label of real and fake samples discriminatively. Formally,
the objective functions for the discriminator D, the discriminative classifier Cd, and the generator G
of the proposed ADC-GAN are defined as follows:
max V(G,D) + λ ∙ (E∣,y〜PXY [log Cd(y, 1|x)] + E∣,y〜QXY [log Cd(y, 0|x)]),
0,Cd	,	,
min V(G,D) - λ ∙ (E∣,y〜Qχ,γ [log Cd(y, 1|x)] - E∣,y〜Qχ,γ [log Cd(y, 0|x)]),
(5)
where Cd(y, 1|x) (reps. Cd(y, 0|x)) denotes the probability that a data x is classified as the label y
and real (reps. fake) data simultaneously.
Proposition 2. For fixed generator, the optimal classifier of ADC-GAN outputs as follows:
Cd(y,i|x) = ~rrrτc,Cd(y,0∣x) = (TXxyy) ).	(6)
p(x) + q(x)	p(x) + q(x)
The proof is referred to Appendix A.3. Proposition 2 confirms that the discriminative classifier be
aware of the densities of the real and generated joint distributions, therefore it is able to provide the
discrepancy p(∣,y) = Cd(y，0|：) to unbiasedly optimize the generator as We prove below.
Theorem 2.	Given the optimal classifier, at the equilibrium point, optimizing the classification task
for the generator of ADC-GAN is equivalent to:
min KL(QX,Y kPX,Y ).	(7)
The proof is referred to Appendix A.4. Theorem 2 suggests that the classifier itself can guarantee
the generator to replicate the real joint distribution in theory regardless of the shape of the joint
distribution. In practice, we retain the discriminator to train the generator and share all layers but
the head of the classifier with the discriminator as illustrated in Figure 1 and Equation 5 for faster
convergence speed. Coupled with the adversarial training against the discriminator, the generator
of the proposed ADC-GAN, under the optimal discriminator and classifier, can be regarded as min-
imizing the following divergences: minG JS(PX IlQX) + λ ∙ KL(QX,γ∣∣Pχ,γ). Since the optimal
solution of conditional generative modeling belongs to the optimal solution set of generative mod-
eling, i.e., arg minG KL(QX,Y ∣PX,Y) ⊆ arg minG JS(PX ∣QX), learning with the discriminator
will not change the convergence point of the generator that approximates the joint distribution of real
data and labels regardless of the value of hyper-parameter λ > 0. Furthermore, the hyper-parameter
λ provides the flexibility to adjust the weight of conditional generative modeling.
4 Discussion on Competing Methods
In this section, we analyze the drawbacks of the two competing methods, TAC-GAN (Gong et al.,
2019) and PD-GAN (Miyato & Koyama, 2018), to demonstrate the superiority and rationality of
ADC-GAN compared to them. Before diving into the details, we show diagrams of the discriminator
and classifier of these methods in Figure 1 and summarize the theoretical learning goal for the
generator under the optimal discriminator and classifier of these methods in Table 1 for an overview.
4
Under review as a conference paper at ICLR 2022
4.1 TAC-GAN
TAC-GAN (Gong et al., 2019) addresses the low intra-class diversity issue of AC-GAN by eliminat-
ing the conditional entropy with respect to the generated data distribution HQ(Y |X) via learning of
the generator with another classifier Cmi : X → Y, which is trained on the generated samples. The
objective functions for the discriminator D, the twin classifiers C and Cmi , and the generator G of
TAC-GAN are defined as follows:
max V(G,D) + λ ∙ (Eχ,y〜Pχγ [log C(y∣x)]+ Eχ,y〜Qχγ [log Cmi(y∣x)]),
D,C,Cmi	,	,
mGn V(G,D) - λ ∙ (Eχ,y〜Qχ,γ [log C(y|x)] - Eχ,y〜Qχ,γ [log Cmi(y∣x)]).
(8)
Theorem 3.	Given the twin optimal classifiers, at the equilibrium point, optimizing the classification
tasks for the generator of TAC-GAN is equivalent to:
min KL(QX,Y kPX,Y ) - KL(QX kPX).	(9)
The proof is referred to Appendix A.5. Theorem 3 reveals that the learning objective of the generator
of TAC-GAN, under the optimal classifier, can be regarded as minimizing contradictory divergences,
i.e., minimization between joint distributions but maximization between marginal distributions. Al-
though theoretically the JS divergence or others (Nowozin et al., 2016; Arjovsky et al., 2017) intro-
duced through the adversarial training between the discriminator and the generator might remedy
this issue, the optimal discriminator and classifier are difficult to obtain in the practical optimization
to ensure that the contradiction is eliminated. We argue that the training instability of TAC-GAN
reported in the literature (Kocaoglu et al., 2018; Han et al., 2020) and founded in our experiments
can be explained by this analysis and interpretation.
4.2 PD-GAN
PD-GAN (Miyato & Koyama, 2018) injects the conditional information into the projection dis-
criminator Dp : X × Y → [0, 1] via the inner-product between the embedding of label and the
representation of data to calculate the joint discriminative score of the data-label pair. In such a
way, PD-GAN inherits the property of convergence point similar to the standard GAN such that
can avoid the low intra-class diversity issue of AC-GAN. The objective functions for the projection
discriminator Dp and the generator G of PD-GAN are defined as follows:
min max
G Dp
V(G, Dp) = Eχ,y〜Px,y[logDp(χ,y)] + Eχ,y〜Qχ,γ[log(1 - Dp(x,y))].
(10)
Based on this minimax game, the optimal projection discriminator has the following form:
1
Dp(x,y)
p(x, y)
1+exp(-d*(x,y))	p(x, y) + q(x,y)
⇒d*(x,y) =logR= logpx) + log* := r(x) + r(y∣x),
q(x, y)	q(x)	q(y|x)
(11)
where p(y|x) = KeXP(V)已？	and q(y∣x)	= Kexp(Vy六？	with K = |Y|	is the number of
' —	EK=I exp(vP ∙φ(x))	- EK=I exp(vq ∙φ(x))	1	1
labels. And they accordingly define:
r(x) := ψ(φ(x)),
K
K
r(y∣x):=(Vp -Vq) ∙ φ(x) - log£exp(vp ∙ φ(x)) - log £ exp(vq ∙ φ(x)) .	(12)
'-------{--------}	∖ k=1
r(y∣χ)	X----------
k=1
@
}
However, PD-GAN actually ignores the partition term a 2 in Equation 12, and constructs the logit
of the projection discriminator in the form of:
d(x, y) = r(χ) + r(y∣χ) = Ψ(Φ(χ)) + Vy ∙ φ(χ),	(13)
2The authors mistakenly argue that a can be merged into r(x). However, r(x) does not incorporate any
label information (vp or vq), which should be considered by a . Therefore, it is unreasonable to merge a into
r(x). PD-GAN actually discards a in implementing the projection discriminator.
5
Under review as a conference paper at ICLR 2022
Figure 2: Distribution learning results on one-dimensional synthetic data.
with vy = vyp - vyq is the difference between two learnable embeddings of label y defined in two
implicit conditional probabilities p(y|x) and q(y|x), φ(∙) is the representation extractor, and ψ(∙)
outputs a scalar based on the extracted representation. Discarding the partition term would make
PD-GAN no longer belong to probability models that model the conditional probabilities p(y|x)
and q(y|x), resulting in losing the complete dependencies between data and labels. Moreover, the
discriminator constructed according to the optimal form of the minimax GAN lacks theoretical guar-
antee when applied on other loss functions such as the hinge loss (Lim & Ye, 2017; Tran et al., 2017),
which PD-GAN actually used, and may even limit its discriminative ability. The proposed ADC-
GAN can be flexibly applied to any version of the loss function V (G, D) as we do not require the
specific form of the original discriminator.
5	Experiments
In this section, we conduct extensive experiments on both synthetic and real-world datasets to
demonstrate the effectiveness of the proposed ADC-GAN. Specifically, ADC-GAN has the advan-
tages of being robust to the hyper-parameter λ, stable during the training process, and capable of
accurately modeling dependencies between data and labels, in addition to the comparable or even
better performance on conditional generative modeling than existing cGANs.
5.1	Synthetic Data
We first experiment on a one-dimensional synthetic mixture of Gaussian to validate the distribution
learning ability of methods. As shown in the left-top of Figure 2, the real data distribution con-
sists of three classes in which there is non-negligible overlap between them. Both generator and
discriminator are multi-layer perceptrons with non-linearity of Tanh. In particular, we investigate
three different settings on the GAN loss function while keeping the learning of the generator with
the classifier fixed if it exists. The first row except the data part shows the learned distributions
that are estimated by kernel density estimation (Parzen, 1962) on the generated data of AC-GAN,
TAC-GAN, and ADC-GAN without the original GAN loss V (G, D). The second and the third rows
show the results of these methods trained with the log loss (Goodfellow et al., 2014) and the hinge
loss (Lim & Ye, 2017; Tran et al., 2017), respectively. The poor performance of PD-GAN with
hinge loss confirms that it is sensitive to the loss function. AC-GAN tends to generate classifiable
data so that it decreases the intra-class diversity in all settings, verifying the Theorem 1. TAC-GAN
without GAN loss cannot accurately reproduce the data distribution, verifying the Theorem 3. And
the worse performance of TAC-GAN with hinge loss compared with ADC-GAN confirms that the
contradiction stated in Theorem 3 is not easy to eliminate by the discriminator. Expectedly, the pro-
posed ADC-GAN can accurately replicate the data distribution even without the original GAN loss,
verifying the Theorem 2. For more quantitative results, please refer to Appendix C.1.
6
Under review as a conference paper at ICLR 2022
Figure 3: Hyper-parameter robustness results on overlapping MNIST.
5.2	Overlapping MNIST
In this subsection, we experiment on a constructed dataset to investigate the hyper-parameter ro-
bustness of the proposed ADC-GAN compared to existing classifier-based methods, i.e., AC-GAN
and TAC-GAN. We follow the practice of (Gong et al., 2019) to construct a two-class handwritten
digit dataset from MNIST. The first class contains an equal number of digits of ‘0’ and ‘1’ and the
second class contains an equal number of digits of ‘0’ and ‘2’. In other words, the support regions
of the two classes have an overlap of digits ‘0’. We change the weight of classifier from λ = 1 to
λ = 100 with multiplicative step of 10. As shown in Figure 3, AC-GAN mistakenly discards digit
‘0’ in the first class when λ = 1. When λ ≥ 10, the digit ‘0’ in the generated data of AC-GAN
totally disappears, indicating that its classifier encourages the generator to avoid learning the data
in the overlapping region. In general, AC-GAN shows a significant decrease in intra-class diversity.
TAC-GAN encounters mode collapse when λ = 100, while ADC-GAN faithfully replicates the real
data distribution regardless of the value of λ. These results suggest that the proposed method has
excellent robustness on hyper-parameters compared to existing classifier-based methods since the
guidances to the generator received from the discriminator and classifier are harmonious.
5.3	CIFAR-10, CIFAR-100, AND TINY-IMAGENET
We experiment on three real-world datasets: CIFAR-10, CIFAR-100, and Tiny-ImageNet. CIFAR-
10 consists of 50k training images and 10k validation images with resolution of 32 × 32. CIFAR-
100 runs similar samples with CIFAR-10 but has 100 classes rather than 10 classes in CIFAR-
10. Tiny-ImageNet contains 200 classes where each class contains 500 training images and 50
validation images with resolution of 64 × 64. We implement all methods based on the BigGAN-
PyTorch repository. The optimizer is Adam with learning rate of 2 × 10-4 for both the generator
and discriminator on CIFAR-10/100, 1 × 10-4 and 4 × 10-4 for the generator and discriminator,
respectively, on Tiny-ImageNet. We train all methods for 500 epochs with batch size of 50 on
CIFAR-10/100 and 100 on Tiny-ImageNet. The discriminator/classifier are updated 4 times per
generator update step on CIFAR-10/100, and 2 times on Tiny-ImageNet. We follow the practice
of (Miyato & Koyama, 2018; Gong et al., 2019) to adopt the hinge loss (Lim & Ye, 2017; Tran
Table 2: FID and Intra-FID (the averaged intra-class FID) and Accuracy (%) comparison of different
methods on CIFAR-10, CIFAR-100, and Tiny-ImageNet, respectively.
Datasets	Metrics	PD-GAN	AC-GAN	TAC-GAN	ADC-GAN
	FIDQ)	6.51	6.81	5.85	5.56
CIFAR-10	Intra-FID Q)	46.35	38.34	37.38	34.26
	Accuracy (↑)	62.21	84.56	88.05	89.19
	FIDQ)	8.47	11.63	11.37	7.98
CIFAR-100	Intra-FID Q)	136.79	161.39	158.15	132.27
	Accuracy (↑)	38.81	55.29	60.13	62.29
	FIDQ)	20.54	24.48	23.78	20.15
Tiny-ImageNet	Intra-FID Q)	94.23	154.82	124.68	97.69
	Accuracy (↑)	27.56	42.26	41.78	44.29
7
Under review as a conference paper at ICLR 2022
(a) CIFAR-10 FID on different λ0
(b) FID curves on CIFAR-10
(c) PD-GAN
(d) CIFAR-100 FID on different λ0
(e) FID curves on CIFAR-100
(f) ADC-GAN
Figure 4: (a,d) FID comparison of methods with different λ0 on CIFAR-10 and CIFAR-100. The
objective of competing methods is (1 - λ0)V (G, D) + λ0VC(G, C), where VC(G, C) is the task
between the generator and classifier. (b,e) FID curves with GAN training iterations on CIFAR-10
and CIFAR-100. (c,f) T-SNE visualization of CIFAR-10 training data, using learned representations
extracted from the penultimate layer in discriminators. Different colors represent different classes.
et al., 2017) as an implementation of V (G, D). We set the hyper-parameter of AC-GAN as λ = 0.2
on all datasets as it performs the best. As for TAC-GAN and ADC-GAN, the hyper-parameter is set
as λ = 1.0 on CIFAR-10/100 and λ = 0.5 on Tiny-ImageNet.
We report the FID (Heusel et al., 2017) and Intra-FID (Miyato & Koyama, 2018) (the FID for
each class) results of all methods on CIFAR-10, CIFAR-100, and Tiny-ImageNet in Table 2. AC-
GAN obtains the worst results and diverges on all datasets (see Figure 4(b),4(e),7). TAC-GAN also
diverges on CIFAR-100 and Tiny-ImageNet and only achieves a relatively stable FID training curve
on CIFAR-10. We here report their results at the best FID checkpoint. These unstable FID curves
implicitly reveal the drawback of the existing classifier-based cGANs that minimizes contradictory
divergences. To explicitly show this, we set the objective function of classifier-based methods as
(1 - λ0)V (G, D) + λ0VC(G, C), where VC (G, C) is the task between the generator and classifier.
As shown in Figure 4(a),4(d), ADC-GAN gains consistent FID results across different λ0 even for
λ0 = 1.0 (i.e., without the discriminator), showing strong robustness on λ0, while AC-GAN and
TAC-GAN performs substantially bad when λ0 becomes large. Table 2 also shows that ADC-GAN
achieves the superior FID and Intra-FID on CIFAR-10 and CIFAR-100 and comparable scores with
PD-GAN on Tiny-ImageNet. The reason why PD-GAN obtains slightly better Intra-FID than ADC-
GAN on Tiny-ImageNet is that Tiny-ImageNet is a more coarse-grained dataset, in which data in
different classes have weak correlations. Impressively, ADC-GAN performs much better training
stability than PD-GAN in terms of the FID curves as shown in Figure 4(b),4(e),7. We argue that the
reason is that ADC-GAN is less prone to over-fitting than PD-GAN due to that the discriminative
classifier of ADC-GAN solves a more difficult task than the projection discriminator of PD-GAN.
To investigate whether the model captures accurate dependencies between data and labels, we con-
duct image classification experiments based on the learned representations extracted from the shared
penultimate layer in the discriminator/classifier. Specifically, we utilize the logistical regression
classifier in scikit-learn library with default settings to compute the accuracy results. As reported
in Table 2, ADC-GAN significantly outperforms competing methods on all datasets, indicating that
ADC-GAN effectively learns accurate dependencies between data and labels. The reason is that the
discriminative classifier needs to distinguish between real and fake data while simultaneously recog-
8
Under review as a conference paper at ICLR 2022
Table 3: FID and IS comparison with competing methods on ImageNet. The results of competing
methods are copied from TAC-GAN (Gong et al., 2019) and SAGAN (Zhang et al., 2019).
Datasets	Metrics	PD-GAN/BigGAN	AC-GAN	TAC-GAN	ADC-GAN
ImageNet	FID α)	22.77	-	23.75	16.75
	IS (↑)	38.05 ± 0.79	28.5	28.86 ± 0.29	55.43 ± 0.90
nizing the labels of samples, which forces the classifier to have a more powerful and robust ability of
modeling data-to-class relations. Notice that PD-GAN obtains the worst accuracy results. By com-
paring the CIFAR-10 T-SNE (Van der Maaten & Hinton, 2008) visualization results of PD-GAN
and ADC-GAN using the learned features of validation data in Figure 4(c),4(f), one can clearly see
that PD-GAN is incapable of learning accurate dependencies between data and labels.
5.4	ImageNet
In this subsection, we compare the proposed ADC-GAN with competing methods on ImageNet
(128 × 128) with 1,000 classes (Deng et al., 2009), each of which contains around 1,300 images. We
adopt the BigGAN with base channels of 64 as the backbone of ADC-GAN and train one step for the
discriminator/classifier and one step for the generator, following the practices of TAC-GAN (Gong
et al., 2019). We follow the instruction of FQ-GAN (Zhao et al., 2020) to train ADC-GAN for
128k iterations. As shown in Table 3, the proposed ADC-GAN significantly outperforms competing
cGANs in terms of both Inception Score (IS) (Salimans et al., 2016) and FID metrics, showing the
effectiveness on large-scale high-resolution image datasets.
6	Related Work
Conditional generative adversarial networks (cGANs) (Mirza & Osindero, 2014) is a family of
GANs, which is capable of generating novel data depended on the given label. The research on
cGANs can be divided into two aspects. One is to study how to implement a conditional generator
network structurally. Approaches in this category are mainly concatenating the label vector with the
noise vector (Mirza & Osindero, 2014), conditional batch normalization (de Vries et al., 2017), and
conditional convolution layers (Sagong et al., 2019). The other is to study how to train the condi-
tional generator to generate label-dependent data. AC-GAN (Odena et al., 2017) leveraged an aux-
iliary classifier to determine the relationship between data and labels. MH-GAN (Kavalerov et al.,
2021) improved AC-GAN by replacing the cross-entropy loss of the classifier with the multi-hinge
loss. Shu et al. (2017) analyzed that AC-GAN learns a biased distribution from a Lagrange view.
AM-GAN (Zhou et al., 2018) extended the real class of the discriminator into K classes of the clas-
sifier, forming a discriminator with K+ 1 classes. Omni-GAN (Zhou et al., 2020) combined the dis-
criminator with the classifier into a K + 2 dimensional multi-label discriminator. TAC-GAN (Gong
et al., 2019) corrected the biased learning objective of AC-GAN by introducing another classifier,
which was the multi-class version of Anti-Labeler of CausalGAN (Kocaoglu et al., 2018). UAC-
GAN (Han et al., 2020) improved the training stability of TAC-GAN via MINE (Belghazi et al.,
2018). PD-GAN (Miyato & Koyama, 2018) injected the label information into the discriminator via
projection technique. In complementary to our work, ContraGAN (Kang & Park, 2020) modeled
the data-to-data relations as well as the data-to-class relations using a conditional contrastive loss.
7	Conclusions
In this paper, we present a novel conditional generative adversarial network with an auxiliary dis-
criminative classifier (ADC-GAN) to achieve faithful conditional generative modeling. The dis-
criminative classifier can provide the discrepancy between the joint distribution of the real data and
labels and that of the generated data and labels to the generator by discriminatively predicting the la-
bel of the real and generated data. Therefore, the generator can faithfully learn the real joint data and
label distribution at the Nash equilibrium. We also discuss the differences between ADC-GAN with
competing cGANs and analyze their potential issues and limitations. Extensive experimental results
validate the theoretical superiority of the proposed ADC-GAN compared to competing cGANs.
9
Under review as a conference paper at ICLR 2022
References
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein generative adversarial net-
works. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research,
pp. 214-223. PMLR, 06-11 Aug 2017. URL http://Proceedings .mlr.press∕v70/
arjovsky17a.html.
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and Devon Hjelm. Mutual information neural estimation. In Jennifer Dy and Andreas
Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80
of Proceedings of Machine Learning Research, pp. 531-540. PMLR, 10-15 Jul 2018. URL
https://proceedings.mlr.press/v80/belghazi18a.html.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity
natural image synthesis. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=B1xsqj09Fm.
Ting Chen, Xiaohua Zhai, Marvin Ritter, Mario Lucic, and Neil Houlsby. Self-supervised gans via
auxiliary rotation loss. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Harm de Vries, Florian Strub, Jeremie Mary, Hugo Larochelle, Olivier Pietquin, and
Aaron C Courville. Modulating early visual processing by language. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
6fab6e3aa34248ec1e34a4aeedecddc8- Paper.pdf.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Mingming Gong, Yanwu Xu, Chunyuan Li, Kun Zhang, and Kayhan Batmanghelich. Twin auxi-
lary classifiers gan. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran As-
sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
4ea06fbc83cdd0a06020c35d50e1e89a- Paper.pdf.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sher-
jil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In
Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger (eds.),
Advances in Neural Information Processing Systems, volume 27. Curran Associates,
Inc., 2014. URL https://proceedings.neurips.cc/paper/2014/file/
5ca3e9b122f61f8f06494c97b1afccf3- Paper.pdf.
Ligong Han, Anastasis Stathopoulos, Tao Xue, and Dimitris Metaxas. Unbiased auxiliary classifier
gans with mine. arXiv preprint arXiv:2006.07567, 2020.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochre-
iter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In
I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
8a1d694707eb0fefe65871369074926d-Paper.pdf.
Minguk Kang and Jaesik Park. Contragan: Contrastive learning for conditional image gen-
eration. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 21357-21369. Curran As-
sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
f490c742cd8318b8ee6dca10af2a163f- Paper.pdf.
10
Under review as a conference paper at ICLR 2022
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training
generative adversarial networks with limited data. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
12104-12114. Curran Associates, Inc., 2020a. URL https://Proceedings.neurips.
cc/paper/2020/file/8d30aa96e72440759f74bd2306c1fa3d-Paper.pdf.
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyz-
ing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), June 2020b.
Ilya Kavalerov, Wojciech Czaja, and Rama Chellappa. A multi-class hinge loss for conditional gans.
In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),
pp. 1290-1299, January 2021.
Murat Kocaoglu, Christopher Snyder, Alexandros G. Dimakis, and Sriram Vishwanath. Causal-
GAN: Learning causal implicit generative models with adversarial training. In International
Conference on Learning Representations, 2018. URL https://openreview.net/forum?
id=BJE-4xW0W.
Jae Hyun Lim and Jong Chul Ye. Geometric gan. arXiv preprint arXiv:1705.02894, 2017.
Zinan Lin, Ashish Khetan, Giulia Fanti, and Sewoong Oh. Pacgan: The power of two samples
in generative adversarial networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/
paper/2018/file/288cc0ff022877bd3df94bc9360b9c5d-Paper.pdf.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint
arXiv:1411.1784, 2014.
Takeru Miyato and Masanori Koyama. cGANs with projection discriminator. In International
Conference on Learning Representations, 2018. URL https://openreview.net/forum?
id=ByS1VpgRZ.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Curran As-
sociates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/file/
cedebb6e872f539bef8c3f919874e9d7- Paper.pdf.
Augustus Odena. Semi-supervised learning with generative adversarial networks. arXiv preprint
arXiv:1606.01583, 2016.
Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with aux-
iliary classifier GANs. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th In-
ternational Conference on Machine Learning, volume 70 of Proceedings of Machine Learning
Research, pp. 2642-2651. PMLR, 06-11 Aug 2017. URL http://proceedings.mlr.
press/v70/odena17a.html.
Emanuel Parzen. On estimation of a probability density function and mode. The annals of mathe-
matical statistics, 33(3):1065-1076, 1962.
Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee.
Generative adversarial text to image synthesis. In Maria Florina Balcan and Kilian Q. Weinberger
(eds.), Proceedings of The 33rd International Conference on Machine Learning, volume 48 of
Proceedings of Machine Learning Research, pp. 1060-1069, New York, New York, USA, 20-22
Jun 2016. PMLR. URL http://proceedings.mlr.press/v48/reed16.html.
11
Under review as a conference paper at ICLR 2022
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International journal ofcomputer vision, 115(3):211-252, 2015.
Min-Cheol Sagong, Yong-Goo Shin, Yoon-Jae Yeo, Seung Park, and Sung-Jea Ko. cgans with
conditional convolution layer. arXiv preprint arXiv:1906.00709, 2019.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and
Xi Chen. Improved techniques for training gans. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon,
and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Cur-
ran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/
file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf.
Rui Shu, Hung Bui, and Stefano Ermon. Ac-gan learns a biased distribution. In NIPS Workshop on
Bayesian Deep Learning, volume 8, 2017.
Zhentao Tan, Menglei Chai, Dongdong Chen, Jing Liao, Qi Chu, Lu Yuan, Sergey Tulyakov, and
Nenghai Yu. Michigan: Multi-input-conditioned hair image generation for portrait editing. arXiv
preprint arXiv:2010.16417, 2020.
Dustin Tran, Rajesh Ranganath, and David Blei. Hierarchical implicit models and likelihood-
free variational inference. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/
paper/2017/file/6f1d0705c91c2145201df18a1a0c7345-Paper.pdf.
Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
learning research, 9(11), 2008.
Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong
He. Attngan: Fine-grained text to image generation with attentional generative adversarial net-
works. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2018.
Xinchen Yan, Jimei Yang, Kihyuk Sohn, and Honglak Lee. Attribute2image: Conditional image
generation from visual attributes. arXiv preprint arXiv:1512.00570, 2015.
Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative
adversarial networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of
the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine
Learning Research, pp. 7354-7363. PMLR, 09-15 Jun 2019. URL http://proceedings.
mlr.press/v97/zhang19d.html.
Yang Zhao, Chunyuan Li, Ping Yu, Jianfeng Gao, and Changyou Chen. Feature quantization im-
proves GAN training. In Hal Daume In and Aarti Singh (eds.), Proceedings of the 37th In-
ternational Conference on Machine Learning, volume 119 of Proceedings of Machine Learning
Research, pp. 11376-11386. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.
press/v119/zhao20d.html.
Peng Zhou, Lingxi Xie, Bingbing Ni, Cong Geng, and Qi Tian. Omni-gan: On the secrets of cgans
and beyond. arXiv preprint arXiv:2011.13074, 2020.
Zhiming Zhou, Han Cai, Shu Rong, Yuxuan Song, Kan Ren, Weinan Zhang, Jun Wang, and Yong
Yu. Activation maximization generative adversarial nets. In International Conference on Learning
Representations, 2018. URL https://openreview.net/forum?id=HyyP33gAZ.
Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. Dm-gan: Dynamic memory generative ad-
versarial networks for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
12
Under review as a conference paper at ICLR 2022
A Proofs
A.1 Proof of Proposition 1
Proposition 1. The optimal classifier of AC-GAN outputs as follows:
c*(y∣χ)
p(χ,y)
p(x)
(3)
Proof.
max Eχ,y 〜pχ,γ [log C (y|x)] = Ex 〜PX Ey 〜PY ∣χ [log C (y|x)]
⇒ min Ex 〜PX Ey 〜PY ∣χ [-log C(y∣x)] = Ex 〜PX [H(p(y∣x)) + KL(p(y∣x)∣∣C(y∣x))]
⇒ C *(y∣x) = argmin KL(p(y∣x)∣∣C(y∣x)) = p(y∣x) = PpXxrly
C	p(x)
□
A.2 Proof of Theorem 1
Theorem 1.	Given the optimal classifier, at the equilibrium point, optimizing the classification task
for the generator of AC-GAN is equivalent to:
minKL(QX,YkPX,Y)-KL(QXkPX)+HQ(Y|X),	(4)
where HQ(Y |X) = - y q(X, y) log q(y|X)dX is the conditional entropy of generated data.
Proof.
π-1	rι λx≠∕ I m _ πτ	I	p(x,y)]_ Ilr	Γ1 p(χ,y) q(X) q(χ,y)]
maxEx旌Qx.Y [logC (y|x)] = Ex,y~Qx，Y [log -p(Xy] = Ex,y~Qx，Y [log q(χ-^yP(X) ~(χ)y∖
p(x, y)	q(x)	q(x, y)
=Ex,y〜QX,Y [l°gq(χ-^yj + Ex〜QX [logp(X)] + Ex,y〜Qχ,Y [log^(χp]
⇒ min KL(QX,Y kPX,Y ) - KL(QX kPX) + HQ(Y |X)
□
A.3 Proof of Proposition 2
Proposition 2. For fixed generator, the optimal classifier of ADC-GAN outputs as follows:
Cd(y, 1|x)=P(XXyq3 ,Cd(y, 0|x)=P(XXyq(X).	⑹
Proof.
max Ex,y〜Px,y [log C(y, 1|x)] X Ex,y〜Qx,y [log C(y, 0|x)] ⇒ max Ex,y,ι〜Pm,Y,L [log C(y, l|x)],
With l ∈ {0,1} andpm(x,y, V) = pm(x, y, 1) + pm(x,y, 0) = 1 p(x, y) + ɪq(x,y).
⇒ max Ex〜PmEy,i-PmL|X [log C(y,l∣x)] ⇒ min Ex〜PmEy,ι〜PmLIX [- log C(y,l∣x)]
⇒ min Ex 〜Pm [H (pm(y, ι∣x)) + KL(Pm (y, ι∣x)kC (y,ι∣x))]
⇒ C *(y,l∣x) = arg min KL(pm(y, l∣x)kC (y,l∣x)) = pm (y,l∣x) = P (X,y,0
C	pm (x)
Therefore, the optimal classifier of ADC-GAN has the form of C * (y, 1|x) = Ppm(y,1)
and C*(y, 0|x) = ppm(y)0) = p(x)+y)x)that concludes the proof.
p(x,y)
P(X)+q(X)
□
13
Under review as a conference paper at ICLR 2022
A.4 Proof of Theorem 2
Theorem 2.	Given the optimal classifier, at the equilibrium point, optimizing the classification task
for the generator of ADC-GAN is equivalent to:
min KL(QX,Y kPX,Y ).	(7)
Proof.
max Eχ,y〜Qx,y [log C*(y, 1|x)] - Eχ,y〜Qx,y [log C*(y, 0|x)]
⇒ min -Eχ,y 〜Qx,y [log C *(y, 1|x)] + Eχ,y 〜Qx,y [log C * (y, 0|x)]
⇒ min -Eχ,y〜Qx,y log
P(X,y)	-
p(x) + q(x)
+ Ex,y~Qχ,γ log p(Xq)+q(χ)
⇒ min Eχ,y〜Qx,y log
q(X,y厂
P(X,y)
⇒ min KL(QX,Y kPX,Y )
□
A.5 Proof of Theorem 3
Proposition 3. For fixed generator, the twin optimal classifiers of TAC-GAN output as follows:
厂” I、	p(x,y)八* / I、	q(x,y)
C(y|X) = E，Cm i(y|x) = Kxr.
(14)
Proof. The proof is similar to that of Proposition 1 in Appendix A.1 by considering C and Cmi as
two independent classifiers with respect to distribution P and Q, respectively.	□
Theorem 3.	Given the twin optimal classifiers, at the equilibrium point, optimizing the classification
tasks for the generator of TAC-GAN is equivalent to:
min KL(QX,Y kPX,Y ) - KL(QX kPX).
(9)
Proof.
max Eχ,y 〜Qx,y [log C * 3 |X)] - Eχ,y 〜Qx,y [log Cm i 3 |X)]
⇒ max Eχ,y〜Qχ,γ log
P(X,yY
P(X)
p(X,y) 一
q(X,y) 一
-Eχ,y~Qχ,γ log ^xxy\
-Ex~Qx 卜og p⅛
⇒ min KL(QX,Y kPX,Y ) - KL(QX kPX)
□
B	Issue of the Original AC-GAN
In this section, we show that original AC-GAN whose auxiliary classifier is trained with both real and
fake samples still suffer from the issue proved in Theorem 1. Formally, the full objective function
of the original AC-GAN is formulated as follows.
max V(G,D) + λ ∙ (Eχ,y〜Pχ,γ [log C(y∣X)]+ Eχ,y〜Qχ,γ [log C(y∣X)]),
D,C
min V(G,D) - λ ∙ (Eχ,y〜Qχ,γ [log C(y|x)]).
The objective function for training the classifier can be rewritten as:
(15)
14
Under review as a conference paper at ICLR 2022
maxEχ,y〜Px,y [log C(y|x)] + Eχ,y〜Qx,y [log C(y|x)] ⇒ maxEχ,y〜PmY [log C(y|x)], (16)
with Pm(X,y) = 2(p(x,y) + q(x,y)) andPm(X) = Py Pm(X,y) = 2(p(x) + q(x)). And we can
obtain the optimal classifier by the following:
max Eχ,y〜PmY [log C(y|x)] ⇒ min Ex〜pm,y〜PmX [- log C(y|x)]
⇒ ∏CnEx〜Pm[H(pm(y∣x)) + KL(Pm(y∣χ)kC(y∣χ))]
(17)
⇒ C*(y∣χ) = pm(y∣χ)
P(X,y) + q(x,y)
P(X) + q(X)
Even though the conditional generator learns the joint real data and label distribution, i.e., q(X, y) =
τ and	∏( rτ,'∖	—	τi( o''∖ th a	CntiIn a]「1。CeififIr fɔ* ( ,, IT、—	P(X,y)+q(X,y)	—	P(X,y)[ ]	Ctill nrc\;i da
P(χ, y) and	q(χ)	=	P(χ), the	opiumal Classifier C (y∣χ)	—	-p(x)+q(x)=	P(X)	will	Still provide
the objective stated in Theorem 1 to optimize the generator, which contains the conditional entropy
of generated samples HQ(Y |X) that would reduce the intra-class diversity of generated samples.
In other words, the discriminative classifier does not allow the generator to remain on the desired
distribution because it still provide momentum to update the generator, resulting in a biased learning
objective for the generator.
15
Under review as a conference paper at ICLR 2022
C More Results
C.1 Synthetic Data
In this section, we report more results on experiments conducted on the one-dimensional synthetic
data and anew two-dimensional synthetic data. The one-dimensional data consists of three Gaussian
components with μo = 0,μι = 3,μ2 = 6 and σ0 = 1,σι = 2,σ2 = 3, and the similar for the two-
dimensional data. For implementing the generator, discriminator, and classifier, we use three-layer
multi-layer perceptron with hidden size of 10 and the Tanh non-linearity. The optimizer is Adam
with learning rate α = 0.002 and betas (β1, β2) = (0.5, 0.999). We train all methods for 40 epochs
with batch size of 256. Table 4 reports the quantitative maximum mean discrepancy (MMD) results
on the one-dimensional synthetic data conducted in Section 5.1. Lower MMD means better learning
results. Figure 6 and Table 5 show the qualitative and quantitative results, respectively, conducted
on the two-dimensional synthetic Gaussian data. In general, the proposed ADC-GAN consistently
replicates the data distribution under different loss function settings.
Figure 5: Distribution learning results on the one-dimensional synthetic data.
Table 4: MMD Q) results of each method on the one-dimensional synthetic data.
GAN Loss	Class	PD-GAN	AC-GAN	TAC-GAN	ADC-GAN
	Class0	-	1.83	0.05	0.06
No	Class1	-	27.32	5.99	0.06
	Class2	-	27583.31	130.89	0.19
	Marginal	-	2919.31	7.81	0.14
	Class0	0.02	0.13	0.01	0.06
Log	Class1	0.05	0.64	0.14	0.07
	Class2	0.10	639.55	0.90	0.19
	Marginal	0.11	75.58	0.66	0.14
	Class0	11328.12	2.08	0.21	0.40
Hinge	Class1	10671.01	29.96	3.04	1.22
	Class2	8420.67	32011.93	93.08	8.55
	Marginal	10218.60	3243.28	7.81	0.80
16
Under review as a conference paper at ICLR 2022
Figure 6: Distribution learning results on the two-dimensional synthetic data.
Table 5: MMD Q) results of each method on the two-dimensional synthetic data.
GAN Loss	Class	PD-GAN	AC-GAN	TAC-GAN	ADC-GAN
	Class0	-	1.28	1.55	0.07
No	Class1	-	6.64	152.49	0.09
	Class2	-	10491.93	671.89	0.11
	Marginal	-	1085.61	166.03	0.02
	Class0	0.05	0.06	0.01	0.02
Log	Class1	0.02	0.64	0.27	0.12
	Class2	0.73	119.91	0.52	0.86
	Marginal	0.01	13.12	0.04	0.08
	Class0	1625.92	1.42	0.70	0.06
Hinge	Class1	1138.02	1.43	5.75	0.23
	Class2	918.97	9440.54	33.34	0.06
	Marginal	1203.51	1019.39	7.99	0.03
Figure 7: FID curves with GAN training iterations on Tiny-ImageNet.
17