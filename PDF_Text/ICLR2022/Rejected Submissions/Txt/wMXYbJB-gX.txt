Under review as a conference paper at ICLR 2022
Towards Understanding Label Smoothing
Anonymous authors
Paper under double-blind review
Ab stract
Label smoothing regularization (LSR) is a prevalent component for training deep
neural networks and can improve the generalization of models effectively. Al-
though it achieves empirical success, the theoretical understanding about the
power of label smoothing, especially about its influence on optimization, is still
limited. In this work, we, for the first time, theoretically analyze the convergence
behaviors of stochastic gradient descent with label smoothing in deep learning.
Our analysis indicates that an appropriate LSR can speed up the convergence by
reducing the variance in gradient, which provides a theoretical interpretation on
the effectiveness of LSR. Besides, the analysis implies that LSR may slow down
the convergence at the end of optimization. Therefore, a novel algorithm, namely
Two-Stage LAbel smoothing (TSLA), is proposed to further improve the conver-
gence. With the extensive analysis and experiments on benchmark data sets, the
effectiveness of TSLA is verified both theoretically and empirically.
1	Introduction
Due to the massive number of parameters, over-fitting becomes one of the major challenges in
training effective deep neural networks (Muller et al., 2019). Various strategies such as weight
decay, dropout (Hinton et al., 2012b), batch normalization (Ioffe & Szegedy, 2015), data augmenta-
tion (Simard et al., 1998), etc. have been proposed to alleviate the problem. Unlike regularization on
input data or model weights, label smoothing regularization (LSR) (Szegedy et al., 2016) perturbs
labels of data for better generalization. Concretely, for a K-class classification problem, the one-hot
label in the prevalent cross entropy loss is replaced by a soft label as yLS = (1 - θ)y + θyb, where y
is the one-hot label and b =去 is a uniform distribution for all labels. By optimizing the soft label
instead, LSR helps to improve the generalization of deep neural networks in diverse applications
including image classification (Zoph et al., 2018; He et al., 2019), speech recognition (Chorowski &
Jaitly, 2017; Zeyer et al., 2018), and language translation (Vaswani et al., 2017; Nguyen & Salazar,
2019). Besides, it is complementary to other regularization methods and can work with others to
make learned neural networks applicable (He et al., 2019).
Despite the success of LSR, efforts toward theoretical understanding are still limited. Muller et al.
(2019) have empirically shown that LSR can help improve model calibration. However, they also
found that LSR could impair knowledge distillation when the learned model is applied as the teacher
network. Yuan et al. (2019a) have proved that LSR provides a virtual teacher model for knowledge
distillation. As a widely used trick, Lukasik et al. (2020) have shown that LSR works since it can
successfully mitigate label noise. Most of existing works try to study LSR as regularizing label
space while little work investigates the direct impact of LSR on training deep neural networks.
In this work, we analyze the benefits of LSR for the learning process from the view of optimiza-
tion. Concretely, our theory shows that an appropriate LSR can essentially reduce the variance of
stochastic gradient in the assigned class labels and thus it can speed up the convergence of stochas-
tic gradient descent, which is a standard optimizer for deep learning. Besides, the analysis implies
that LSR may incur slow convergence at the end of optimization. Inspired by the result, a novel
algorithm is developed to accelerate the convergence by a two-stage training paradigm. The effec-
tiveness of the proposed method is verified both theoretically and empirically. We summarize the
main contributions of this paper as follows.
•	To the best of our knowledge, it is the first work that establishes improved iteration com-
plexities of stochastic gradient descent (SGD) (Robbins & Monro, 1951) with LSR for
1
Under review as a conference paper at ICLR 2022
finding an -approximate stationary point (Definition 1) in solving a smooth non-convex
problem in the presence of an appropriate label smoothing. The results theoretically ex-
plain why an appropriate LSR can help speed up the convergence. (Subsection 4.1)
•	According to the analysis, we propose a simple yet effective method to accelerate the con-
vergence of LSR (Subsection 4.2). It adopts a two-stage training paradigm that applies
LSR in the first stage and then runs on one-hot labels in the second stage.
•	The theoretical analysis demonstrates that TSLA has an improved iteration complexity
(Subsection 4.3) and better generalization (Subsection 4.4) compared to the conventional
LSR. Extensive experiments on benchmark data sets confirm the effectiveness of our pro-
posed method.
2	Related Work
In this section, we introduce some related work. A closely related idea to LSR is confidence penalty
proposed by Pereyra et al. (2017), an output regularizer that penalizes confident output distribu-
tions by adding its negative entropy to the negative log-likelihood during the training process. The
authors (Pereyra et al., 2017) presented extensive experimental results in training deep neural net-
works to demonstrate better generalization comparing to baselines with only focusing on the existing
hyper-parameters. They have shown that LSR is equivalent to confidence penalty with a reversing
direction of KL divergence between uniform distributions and the output distributions.
DisturbLabel introduced by Xie et al. (2016) imposes the regularization within the loss layer, where
it randomly replaces some of the ground truth labels as incorrect values at each training iteration.
Its effect is quite similar to LSR that can help to prevent the neural network training from overfit-
ting. The authors have verified the effectiveness of DisturbLabel via several experiments on training
image classification tasks.
Recently, many works (Zhang et al., 2018; Bagherinezhad et al., 2018; Goibert & Dohmatob, 2019;
Shen et al., 2019; Li et al., 2020b) explored the idea of LSR technique. Ding et al. (2019) extended
an adaptive label regularization method, which enables the neural network to use both correctness
and incorrectness during training. Pang et al. (2018) used the reverse cross-entropy loss to smooth
the classifier’s gradients. Wang et al. (2020) proposed a graduated label smoothing method that uses
the higher smoothing penalty for high-confidence predictions than that for low-confidence predic-
tions. They found that the proposed method can improve both inference calibration and translation
performance for neural machine translation models. By contrast, we will try to understand the power
of LSR from an optimization perspective and try to study how and when to use LSR.
3	Preliminaries and Notations
We first present some notations. Let NwFS(W) denote the gradient of a function FS (w). When the
variable to be taken a gradient is obvious, We use VFS(w) for simplicity. We use k ∙ k to denote the
Euclidean norm. Let(•, •)be the inner product.
In classification problem, We aim to seek a classifier to map an example x ∈ X onto one of K
labels y ∈ Y ⊂ RK, Where y = (y1, y2, . . . , yK) is a one-hot label, meaning that yi is “1” for the
correct class and “0” for the rest. Suppose the example-label pairs are draWn from a distribution P,
i.e., (x, y)〜 P = (Px, Py). Let S = {(xι, yι),..., (xn, yn)} denotes a set of n examples drawn
from P. We denote by E(x,y)[∙] the expectation that takes over a random variable (x, y). When
the randomness is obvious, we write E[∙] for simplicity. Our goal is to learn a prediction function
f(w; x) : W × X → RK that is as close as possible to y, where w ∈ W is the parameter and W is
a closed convex set. To this end, we want to minimize the following expected loss under P:
1n
wmin FS((W) = n£'(y”f(w；Xi))	⑴
i=1
where ` : Y × RK → R+ is a cross-entropy loss function given by
'(y,f(w; x))
K
-yi log
i=1
exp(fi(w; x))
PK=I exP(fj(W；X))
(2)
2
Under review as a conference paper at ICLR 2022
The objective function FS (w) is not convex since f (w; x) is non-convex in terms of w. To solve
the problem (1), one can simply use some iterative methods such as stochastic gradient descent
(SGD). Specifically, at each training iteration t, SGD updates solutions iteratively by wt+1 = wt -
ηVw'(yt, f (wt； Xt)), where η > 0 is a learning rate.
Next, we present some notations and assumptions that will be used in the convergence analysis.
Throughout this paper, we also make the following assumptions for solving the problem (1).
Assumption 1. Assume the following conditions hold:
(i)	The stochastic gradient of FS(w) is unbiased, i.e., E(χ,y)[V'(y, f (w; x))] = VFS(w),
and the variance of stochastic gradient is bounded, i.e., there exists a constant σ2 > 0,
such that E(x,y)[∣∣V'(y,f(w; X))-VFS(W)『]=σ2.
(ii)	FS (w) is smooth with an L-Lipchitz continuous gradient, i.e., it is differentiable and there
exists a constant L > 0 such that kVFS (w) - VFS (u)k ≤ Lkw - uk, ∀w, u ∈ W.
Remark. Assumption 1 (i) and (ii) are commonly used assumptions in the literature of non-convex
optimization (Ghadimi & Lan, 2013; Yan et al., 2018; Yuan et al., 2019b; Wang et al., 2019; Li
et al., 2020a). Assumption 1 (ii) says the objective function is L-smooth, and it has an equivalent
expression (Nesterov, 2004) which is FS(W)-FS(U) ≤ NFS(u), w-ui + LL∣∣w-u∣∣2, ∀w, U ∈
W . For a classification problem, the smoothed label yLS is given by
yLS = (1 - θ)y + θyb,	(3)
where θ ∈ (0, 1) is the smoothing strength, y is the one-hot label, yb is an introduced label. For
example, one can simply use b = K (Szegedy et al., 2016) for K-class problems. Similar to label
y, we suppose the label yb is drawn from a distribution Pby . We introduce the variance of stochastic
gradient using label yb as follows.
E(χ,b) [∣∣V'(y, f(w; x)) - VFs(W)『]=b2 := δσ2.	(4)
where δ > 0 is a constant and σ2 is defined in Assumption 1 (i). We make several remarks for (4).
Remark. (a) We do not require that the stochastic gradient V'(b, f (w; x)) is unbiased, i.e., it could
be E[V'(b, f (w; x))] = VFS(w). (b) The variance b2 is defined based on the label b rather than
the smoothed label yLS . (c) We do not assume the variance σb2 is bounded since δ could be an
arbitrary value, however, we will discuss the different cases of δ in our analysis. If δ ≥ 1, then
σb2 ≥ σ2 ; while if 0 < δ < 1, then σb2 < σ2 . It is worth mentioning that δ could be small when
an appropriate label is used in the label smoothing. For example, one can smooth labels by using
a teacher model (Hinton et al., 2014) or the model’s own distribution (Reed et al., 2014). In the
first paper of label smoothing (Szegedy et al., 2016) and the following related studies (Muller et al.,
2019; Yuan et al., 2019a), researchers consider a uniform distribution over all K classes of labels
as the label b, i.e., set b = K. Due to the space limitation, We include more discussions and the
evaluations of δ in real-world applications in Appendix A. For example, we show that δ < 1 for
CIFAR-100 in practice.
We now introduce an important property regarding FS(w), i.e. Polyak-Ecjasiewicz (PL) condi-
tion (Polyak, 1963). More specifically, the following assumption holds.
Assumption 2. There exists a constant μ > 0 such that 2μ(FS(w) 一 FS(WS)) ≤
∣∣VFS(w)k2, ∀w ∈ W, where WS ∈ minw∈w FS(w) is a optimal solution.
Remark. This property has been theoretically and empirically observed in training deep neural
networks (Allen-Zhu et al., 2019; Yuan et al., 2019b). This condition is widely used to establish
convergence in the literature of non-convex optimization, please see (Yuan et al., 2019b; Wang
et al., 2019; Karimi et al., 2016; Li & Li, 2018; Charles & Papailiopoulos, 2018; Li et al., 2020a)
and references therein.
To measure the convergence of non-convex and smooth optimization problems as in (Nesterov,
1998; Ghadimi & Lan, 2013; Yan et al., 2018), we need the following definition of the first-order
stationary point.
Definition 1 (First-order stationary point). For the problem of minw∈W FS (w), a point w ∈ W is
called a first-order stationary point if∣VFS(w)∣ = 0. Moreover, if∣VFS(w)∣ ≤ , then the point
w is said to be an -stationary point, where ∈ (0, 1) is a small positive value.
3
Under review as a conference paper at ICLR 2022
Algorithm 1 SGD with Label Smoothing Regularization
1:	Initialize: w0 ∈ W, θ ∈ (0, 1), set η as the value in Theorem 3.
2:	for t = 0, 1, . . . , T - 1 do
3:	sample (xt,yt), set ytLS = (1 - θ)yt + θybt
4:	update wt+ι = Wt - ηVw'(yLs, f (wt; Xt))
5:	end for
6:	Output: wR, where R is uniformly sampled from {0, 1, . . . , T - 1}.
4 TsLA: A Two-Stage Label Smoothing Algorithm
In this section, we present our main method with its convergence analysis. As a warm-up, we first
show the convergence analysis of sGD with LsR to understand Ls from theoretical side. Then we
will introduce the proposed TsLA and study its convergence results both in training error (optimiza-
tion) and testing error (generalization).
4.1 Convergence Analysis of sGD with LsR
To understand LsR from the optimization perspective, we consider sGD with LsR in Algorithm 1
for the sake of simplicity. The only difference between Algorithm 1 and standard sGD is the use
of the output label for constructing a stochastic gradient. The following theorem shows that Al-
gorithm 1 converges to an approximate stationary point in expectation under some conditions. We
include its proof in Appendix C.
Theorem 3. Under Assumption 1, run Algorithm 1 with η = L and θ = ι+δ, then
Er[∣∣VFs (WR)Il2] ≤ 2FnwO + 2δσ2, where R is uniformly sampled from {0,1,..., T — 1}.
Furthermore, given a target accuracy level , we have the following two results.
(1) when δ ≤ 占,if we Set T = 4Fn(WO), then Algorithm 1 converges to an e-stationary point in
expectation, i.e., ER[∣∣VF⅛(WR)∣2] ≤ e* 1 2. The total Sample complexity is T = O (*).
(2) when δ > 息,if we set T = FS；wO), then Algorithm 1 does not converge to an e-stationary
point, but we have ER [IVFS (WR)I2] ≤ 4δσ2 ≤ O(δ).
Remark. We observe that the variance term is 2δσ2, instead of ηLσ2 for standard analysis of
sGD without LsR (i.e., θ = 0, please see the detailed analysis of Theorem 6 in Appendix D).
For the convergence analysis, the difference between sGD with LsR and sGD without LsR is
that V'(b, f (w; x)) is not an unbiased estimator of VFS(w) when using LSR. The convergence
behavior of Algorithm 1 heavily depends on the parameter δ. When δ is small enough, say δ ≤
O(e2) with a small positive value e ∈ (0, 1), then Algorithm 1 converges to an e-stationary point
with the total sample complexity of O (5). Recall that the total sample complexity of standard
SGD without LSR for finding an e-stationary point is O (J) ((Ghadimi & Lan, 2016; Ghadimi
et al., 2016), please also see the detailed analysis of Theorem 6 in Appendix D). The convergence
result shows that ifwe could find a label yb that has a reasonably small amount ofδ, we will be able to
reduce sample complexity for training a machine learning model from O (』)to O G). Thus, the
reduction in variance will happen when an appropriate label smoothing with δ ∈ (0, 1) is introduced.
We will find in the empirical evaluations that different label yb lead to different performances and an
appropriate selection of label yb has a better performance (see the performances of LSR and LSR-pre
in Table 2). On the other hand, when the parameter δ is large such that δ > Ω(e2), that is to say,
if an inappropriate label smoothing is used, then Algorithm 1 does not converge to an e-stationary
point, but it converges to a worse level of O(δ), indicating that it may slow down the convergence at
the end of optimization.
4.2 The TSLA Algorithm
Previous subsection have shown that LSR could not find an e-stationary point under some situa-
tions. These motivate us to investigate a strategy that combines the algorithm with and without LSR
during the training progress. Let think in this way, one possible scenario is that training one-hot
4
Under review as a conference paper at ICLR 2022
Algorithm 2 The TSLA algorithm
1:	Initialize: w° ∈ W, Ti, θ ∈ (0,1), η1,η2 › 0
// First stage: SGD with LSR
2:	for t = 0, 1, . . . , T1 - 1 do
3:	set ytLS = (1 - θ)yt + θybt
4： update wt+i = Wt - ηιV'(yLSf(Wt； Xt))
5:	end for
// Second stage: SGD without LSR
6:	for t = Ti , 1, . . . , Ti + T2 - 1 do
7:	update wt+i = Wt — ηV'(yt,f(wt； xt)).
8:	end for
9:	Output: wR, where R is uniformly sampled from
{Ti , . . . , Ti + T2 - 1}.
3.0
2.5
ω
V)
0 2.0
1.5
1.0
0	10 20 30 40 50 60 70 80 90
# epoch
Figure 1: Loss on ResNet-18 over CUB-2011
label could be “easier” than training smoothed label 1 . Taking the cross entropy loss in (2) for an
example, one need to optimize a single loss function - log exp(fk(w； x))/ PjK=i exp(fj (w； x))
when one-hot label (e.g, yk = 1 and yi = 0 for all i 6= k) is used, but need to optimize all K
loss functions - PiK=i yiLS log exp(fi(w； x))/ PjK=i exp(fj(w； x)) when smoothed label (e.g.,
yLS = (1 一 θ)y + θK so that yL = 1 一 (K 一 1)θ∕K and yL = θ∕K for all i = k) is used. NeV-
ertheless, training deep neural networks is gradually focusing on hard examples with the increase
of training epochs. It seems that training with smoothed label in the late epochs makes the learning
progress more difficult. In addition, after LSR, we focus on optimizing the oVerall distribution that
contains the minor classes, which are probably not important at the end of training progress. Please
see the blue dashed line (trained by SGD with LSR) in Figure 12, and it shows that there is almost
no change in loss after 30 epochs. One question is whether LSR helps at the early training epochs
but it has less (eVen negatiVe) effect during the later training epochs? This question encourages us to
propose and analyze a simple strategy with LSR dropping that switches a stochastic algorithm with
LSR to the algorithm without LSR.
A natural choice could be just dropping off LSR after certain epochs. For example, one can run
SGD with LSR in the first 50 epochs and then run SGD without LSR after that (see red solid line in
Figure 1). From Figure 1, we can see that the drop of LSR after 50 epochs can further decrease the
loss function.
With this in mind in this subsection, we propose a generic framework that consists of two stages,
wherein the first stage it runs SGD with LSR for Ti iterations and the second stage it runs SGD with-
out LSR up to T2 iterations. This framework is referred to as Two-Stage LAbel smoothing (TSLA)
algorithm, whose updating details are presented in Algorithm 2. Although SGD is considered as the
pipeline (Line 4 and Line 8 in Algorithm 2) in the conVergence analysis, in practice, SGD can be re-
placed by any stochastic algorithms such as momentum SGD (Polyak, 1964), Stochastic NesteroV’s
Accelerated Gradient (NesteroV, 1983), and adaptiVe algorithms including AdaGrad (Duchi et al.,
2011), RMSProp (Hinton et al., 2012a), AdaDelta (Zeiler, 2012), Adam (Kingma & Ba, 2015),
Nadam (Dozat, 2016) and AMS Grad (Reddi et al., 2018). In this paper, we will not study the the-
oretical guarantees and empirical eValuations of other optimizers, which can be considered as future
work. Please note that the algorithm can use different learning rates ηi and η2 during the two stages.
The last solution of the first stage will be used as the initial solution of the second stage. IfTi = 0,
then TSLA reduces to the baseline, i.e., SGD without LSR; while ifT2 = 0, TSLA becomes to LSR
method, i.e., SGD with LSR.
1While this is not a rigorous mathematical conclusion, we want to roughly show that optimizing loss of soft
label has to consider minimizing the combination of K functions while one-hot loss only needs to focus on
one. For soft label, each fj appears both in numerator (need to maximize) and denominator (need to minimize)
of the loss function, while for one-hot label, fj , j 6= k only appears in denominator (need to minimize) of the
loss function, indicating that one-hot label has less “constraints” than soft label.
2The blue dashed line is trained by SGD with LSR. The red solid line is trained by SGD with LSR for the
first 50 epochs and trained by SGD without LSR (i.e., using one-hot label) for the last 40 epochs.
5
Under review as a conference paper at ICLR 2022
4.3	Optimization Result of TSLA
In this subsection, we will give the convergence result of the proposed TSLA algorithm. For sim-
plicity, we use SGD as the subroutine algorithm A in the analysis. The convergence result in the
following theorem shows the power of LSR from the optimization perspective. Its proof is presented
in Appendix E. It is easy to see from the proof that by using the last output of the first stage as the
initial point of the second stage, TSLA can enjoy the advantage of LSR in the second stage with an
improved convergence.
Theorem 4. Under Assumptions 1, 2, suppose σ2δ∕μ ≤ F(w0), run Algorithm 2, θ = ι++δ, ηι =
L, T1 = log (2μ (翦2(1+δ)) /(ηιμ), η = ⅛ and T2 =繇,then Er[∣∣VFs (wr)∣∣2] ≤ e2,
where R is uniformly sampled from {T1, . . . , T1 + T2 - 1}.
Remark. It is obvious that the learning rate η2 in the second stage is roughly smaller than the
learning rate η1 in the first stage, which matches the widely used stage-wise learning rate decay
scheme in training neural networks. To explore the total sample complexity of TSLA, we consider
different conditions on δ. For a fixed the target convergence level ∈ (0, 1), let us discuss the total
sample complexities of finding -stationary points for SGD with TSLA (TSLA), SGD with LSR
(LSR), and SGD without LSR (baseline), where we only consider the orders of the complexities
but ignore all constants. When Ω(e2) < δ < 1, LSR does not converge to an e-stationary point,
while TSLA reduces sample complexity from O O to O (*)，compared to the baseline. When
δ < O(e2), the total complexity of TSLA is between log(1/e) and 1/e2, which is always better than
LSR and the baseline. In summary, TSLA achieves the best total sample complexity by enjoying
the good property ofan appropriate label smoothing (i.e., when 0 < δ < 1). However, when δ ≥ 1,
baseline has better convergence than TSLA, meaning that the selection of label yb is not appropriate.
Since T1 contains unknown parameters, it is difficulty to know its ground-truth value. However, we
can tune different values in practice.
4.4	Generalization Result of TSLA
In this subsection, we study the generalization result of TSLA. First, we give some notations, where
most of them are followed by (Hardt et al., 2015; Yuan et al., 2019b). Let wS = A(S) be a solution
that generated by a random algorithm A based on dataset S. Recall that problem (1) is called
empirical risk minimization in the literature, and the true risk minimization is given by
min F(W) := E(x,y)做y, f (W；X))].	⑸
w∈W
Then the testing error is dedined as
Ea,s[F(ws)] - ES[Fs(wS)].	(6)
We notice that there are several works (Hardt et al., 2015; Yuan et al., 2019b) study the testing error
result of SGD for non-convex setting under different conditions such as bounded stochastic gradient
∣∣Vw'(y, f (w; x))k ≤ G and decaying learning rate η ≤ C with a constant c > 0, where t is the
optimization iteration. In this paper, we are not interested in establishing fast rate under different
conditions, but we want to explore the generalization ability of TSLA with the fewest possible
modifications when building a bridge between theory and practice. For example, weight decay is a
widely used trick when training deep neural networks. With the use of weight decay, the empirical
risk minimization in practice becomes minw∈w{Fbs(w) := FS(w) + λ∣∣w∣2}. We present the
ERB of TSLA in the following Theorem 5, whose proofs can be found in the Appendix F.
Theorem 5. UnderASSumPtion 1, assume that '(y, f (w, x)) is L-smooth and B-Lipschitz, suppose
λ
FS(w) satisfies Assumption 2 and mι□w∈w FS (w) ≤ FS(WS) + 2 IlWtIl2 With λ = 2L, where Wt
is the intermediate solution in the second stage of Algorithm 2 by running with θ = ι++δ, ηι <
3L, Ti = log (2μFS2Wσ2(1+δ)) /(ηιμ), η2 = O(1∕√n) and T2 = O(δn) then the testing error
Er,a,s[F(wr)] 一 ES[Fs(WS)] ≤ O(1∕√n), where A is TSLA.
Remark. Theorem 5 shows that in order to have an bound of testing error in the order of O(1∕√n),
TSLA needs to run T = O(δn) iterations. For the standard analysis of SGD with/without LSR
(please see the remarks in Appendix F) under the same conditions, SGD without LSR needs to run
T = O(n) iterations to obtain an O(1∕√n) bound of testing error. Thus, if δ is small enough, the
6
Under review as a conference paper at ICLR 2022
Table 1: Comparison of Testing Accuracy for Different Methods (mean ± standard deviation, in %).
Algorithm*	Stanford Dogs		CUB-2011	
	Top-1 accuracy	Top-5 accuracy	Top-1 accuracy	Top-5 accuracy
baseline	82.31 ± 0.18	97.76 ± 0.06^^	75.31 ± 0.25	93.14 ± 0.31
LSR	82.80 ± 0.07	97.41 ± 0.09^^	76.97 ± 0.19	92.73 ± 0.12
TSLA(20)	83.15 ± 0.02	97.91 ± 0.08^^	76.62 ± 0.15	93.60 ± 0.18
TSLA(30)	83.89 ± 0.16	98.05 ± 0.08	77.44 ± 0.19	93.92 ± 0.16
TSLA(40)	83.93 ± 0.13	98.03 ± 0.05	77.50 ± 0.20	93.99 ± 0.11
TSLA(50)	83.91 ± 0.15	98.07 ± 0.06	77.57 ± 0.21	93.86 ± 0.14
TSLA(60)	83.51 ± 0.11	97.99 ± 0.06	77.25 ± 0.29	94.43 ± 0.18
TSLA(70)	83.38 ± 0.09	97.90 ± 0.09	77.21 ± 0.15	93.31 ± 0.12
TSLA(80)	83.14 ± 0.09	97.73 ± 0.07	77.05 ± 0.14	93.05 ± 0.08
*TSLA(s): TSLA drops offLSR after epoch s.
testing error of TSLA is better than that of SGD without LSR, otherwise, SGD without LSR has
better testing error than TSLA. For SGD with LSR, when δ > Ω(1/ √n), the bound of testing error
for LSR can not be bounded by O(1 /√n); when δ ≤ Ω(1/√n), the bound for LSR can be bounded
by O(1/√n) in T = O(√n) iterations. This shows that TSLA bas better testing error than LSR.
5	Experiments
To further evaluate the performance of the proposed TSLA method, we trained deep neural networks
on three benchmark data sets, CIFAR-100 (Krizhevsky & Hinton, 2009), Stanford Dogs (Khosla
et al., 2011) and CUB-2011 (Wah et al., 2011), for image classification tasks. CIFAR-100 3 has
50,000 training images and 10,000 testing images of 32×32 resolution with 100 classes. Stanford
Dogs data set 4 contains 20,580 images of 120 breeds of dogs, where 100 images from each breed
is used for training. CUB-2011 5 is a birds image data set with 11,788 images of 200 birds species.
The ResNet-18 model (He et al., 2016) is applied as the backbone in the experiments.
We compare the proposed TSLA incorporating with SGD (TSLA) with two baselines, SGD with
LSR (LSR) and SGD without LSR (baseline). The mini-batch size of training instances for all
methods is 256 as suggested by (He et al., 2019; 2016). The momentum parameter is fixed as 0.9.
We will include more details of experimental settings in Appendix A.
5.1	Stanford Dogs and CUB-2011
We separately train ResNet-18 (He et al., 2016) up to 90 epochs over two data sets Stanford Dogs
and CUB-2011. We use weight decay with the parameter value of 10-4. For all algorithms, the
initial learning rates for FC are set to be 0.1, while that for the pre-trained backbones are 0.001 and
0.01 for Standford Dogs and CUB-2011, respectively. The learning rates are divided by 10 every 30
epochs. For LSR, we fix the value of smoothing strength θ = 0.4 for the best performance, and the
label b used for label smoothing is set to be a uniform distribution over all K classes, i.e., b = K.
The same values of the smoothing strength θ and the same yb are used during the first stage of TSLA.
For TSLA, we drop off the LSR (i.e., let θ = 0) after s epochs during the training process, where
s ∈ {20, 30, 40, 50, 60, 70, 80}.
We first report the highest top-1 and top-5 accuracy on the testing data sets for different methods.
All top-1 and top-5 accuracy are averaged over 5 independent random trails with their standard
deviations. The results of the comparison are summarized in Table 1, where the notation “TSLA(s)”
means that the TSLA algorithm drops off LSR after epoch s. It can be seen from Table 1 that
under an appropriate hyperparameter setting the models trained using TSLA outperform that trained
using LSR and baseline, which supports the convergence result in Section 4.2. We notice that
the best top-1 accuracy of TSLA are TSLA(40) and TSLA(50) for Stanford Dogs and CUB-2011,
3https://www.cs.toronto.edu/~kriz/cifar.html
4http://vision.stanford.edu/aditya86/ImageNetDogs/
5http://www.vision.caltech.edu/visipedia/CUB-200.html
7
Under review as a conference paper at ICLR 2022
Stanford Dogs
20	30	40	50	60	70	80	90
# epoch
Stanford Dogs
8 7
9 9
Λ□2n8‹dol
20	30	40	50	60	70	80	90
# epoch
ω",σl
Stanford Dogs
2 0 8 6
■ ■ ■ ■
Iloo
0	20	40	60	80
# epoch
CUB-2011	CUB-2011
CUB-2011
76
Λ□2n84dol
5
4 3 -
9 9
Λ□2n84dol




20	30	40	50	60	70	80	90	20	30	40	50	60	70	80	90	0	20	40	60	80
# epoch	# epoch	# epoch
Figure 2: Testing Top-1, Top-5 Accuracy and Loss on ResNet-18 over Stanford Dogs and CUB-
2011. TSLA(s) means TSLA drops off LSR after epoch s.
respectively, meaning that the performance of TSLA(s) is not monotonic over the dropping epoch s.
For CUB-2011, the top-1 accuracy of TSLA(20) is smaller than that of LSR. This result matches the
convergence analysis of TSLA showing that it can not drop off LSR too early. For top-5 accuracy,
we found that TSLA(80) is slightly worse than baseline. This is because of dropping LSR too late
so that the update iterations (i.e., T2) in the second stage of TSLA is too small to converge to a good
solution. We also observe that LSR is better than baseline regarding top-1 accuracy but the result is
opposite as to top-5 accuracy.
We then plot the averaged top-1 accuracy, averaged top-5 accuracy, and averaged loss among 5 trails
of different methods in Figure 2. We remove the results for TSLA(20) since it dropped off LSR too
early as mentioned before. The figure shows TSLA improves the top-1 and top-5 testing accuracy
immediately once it drops off LSR. Although TSLA may not converge if it drops off LSR too late,
see TSLA(60), TSLA(70), and TSLA(80) from the third column of Figure 2, it still has the best
performance compared to LSR and baseline. TSLA(30), TSLA(40), and TSLA(50) can converge to
lower objective levels, comparing to LSR and baseline.
5.2	CIFAR- 1 00
ClfarlOO	ClfarlOO	clfarlOO
76
Λ□ejn84 I01
7877
60	80 100 120 140 160 180 200
# epoch
Λ3ejn84 WA-OH
60	80 100 120 140 160 180 200
# epoch
1.8
1.6
a ι.4-
O
-J
1.2
ι.o-
0.8
0 25 50 75 100 125 150 175 200
# epoch
Figure 3: Testing Top-1, Top-5 Accuracy and Loss on ResNet-18 over CIFAR-100. TSLA(s)/TSLA-
pre(s) meansTSLA/TSLA-pre drops off LSR/LSR-pre after epoch s.
The total epochs of training ResNet-18 (He et al., 2016) on CIFAR-100 is set to be 200.The weight
decay with the parameter value of 5 × 10-4 is used. We use 0.1 as the initial learning rates for all
algorithms and divide them by 10 every 60 epochs as suggested in (He et al., 2016; Zagoruyko &
Komodakis, 2016). For LSR and the first stage of TSLA, the value of smoothing strength θ is fixed
as θ = 0.1, which shows the best performance for LSR. We use two different labels yb to smooth the
8
Under review as a conference paper at ICLR 2022
one-hot label, the uniform distribution over all labels and the distribution predicted by an ImageNet
pre-trained model which is downloaded directly from PyTorch 6 (Paszke et al., 2019). For TSLA, we
try to drop off the LSR after s epochs during the training process, where s ∈ {120, 140, 160, 180}.
All top-1 and top-5 accuracy on the test-
ing data set are averaged over 5 indepen-
dent random trails with their standard
Table 2: Comparison of Testing Accuracy for Different
Methods (mean ± standard deviation, in %).
Algorithm*		CIFAR-100		
	Top-1 accuracy	Top-5 accuracy
baseline	76.87 ± 0.04^^	93.47 ± 0.15
LSR	77.77 ± 0.18^^	93.55 ± 0.11
TSLA(120)	77.92 ± 0.21 ^^	94.13 ± 0.23
TSLA(140)	77.93 ± 0.19	94.11 ± 0.22
TSLA(160)	77.96 ± 0.20	94.19 ± 0.21
TSLA(180)	78.04 ± 0.27	94.23 ± 0.15
LSR-pre	78.07 ± 0.31 ^^	94.70 ± 0.14
TSLA-pre(120)	78.34 ± 0.31 ^^	94.68 ± 0.14
TSLA-pre(140)	78.39 ± 0.25	94.73 ± 0.11
TSLA-pre(160)	78.55 ± 0.28	94.83 ± 0.08
TSLA-pre(180)	78.53 ± 003	94.96 ± 0.23
*TSLA(s)/TSLA-Pre(s):	TSLAzTSLA-Pre drops off
LSR/LSR-pre after epoch s.
deviations. We summarize the results
in Table 2, where LSR-Pre and TSLA-
Pre indicate LSR and TSLA use the la-
bel yb based on the ImageNet Pre-trained
model. The results show that LSR-
PrezTSLA-Pre has a better Performance
than LSRzTSLA. The reason might be
that the Pre-trained model-based Pre-
diction is closer to the ground truth
than the uniform Prediction and it has
lower variance (smaller δ). Then, TSLA
(LSR) with such Pre-trained model-
based Prediction converges faster than
TSLA (LSR) with uniform Prediction,
which verifies our theoretical findings in
Section 4.2 (Section 4.1). This observa-
tion also emPirically tells us the selec-
tion of the Prediction function yb used for smoothing label is the key to the success of TSLA as
well as LSR. Among all methods, the Performance of TSLA-Pre is the best. For toP-1 accuracy,
TSLA-Pre(160) outPerforms all other algorithms, while for toP-5 accuracy, TSLA-Pre(180) has the
best Performance.
Finally, we observe from Figure 3 that both TSLA and TSLA-Pre converge, while TSLA-Pre con-
verges to the lowest objective value. Similarly, the toP-1 and toP-5 accuracies show the imProve-
ments of TSLA and TSLA-Pre at the Point of droPPing off LSR.
We conduct an ablation study for the smoothing Pa-
rameter θ and the droPPing ePoch s in TSLA(s). We
follow the same settings in Subsection 5.2 but use
different values of θ in LSR and TSLA. SPecifically,
θ ∈ {0.2, 0.4, 0.9}. We use the uniform distribution
over all labels to smooth the one-hot label. The results
are summarized in Table 3, showing that the different
values of θ and s can affect the Performances of LSR
and TSLA. With the increase of θ, the Performances
become worse. However, with different values of θ,
TSLA can always outPerform LSR when an aPProPri-
ate droPPing ePoch s is selected. Besides, TSLA(180)
has the best Performance for each value of θ.
Table 3: ComParison of Testing ToP-1 Ac-
curacy for Different θ of LSR and TSLA
(in %).
θ	0.2	0.4	0.9
LSR	77.75	77.72	76.40
TSLA(120)	77.92	77.68	76.05
TSLA(140)	78.06	77.61	76.27
TSLA(160)	78.09	77.54	76.37
TSLA(180)	78.10	77.89	76.60
*TSLA(s): TSLA drops off LSR after epoch s.
6	Conclusions
In this paper, we have studied the power of LSR in training deep neural networks by analyzing
SGD with LSR in different non-convex optimization settings. The convergence results show that
an appropriate LSR with reduced label variance can help speed up the convergence. We have pro-
posed a simple yet efficient strategy so-called TSLA whose basic idea is to switch the training from
smoothed label to one-hot label. Integrating TSLA with SGD, we observe from its improved con-
vergence result that TSLA benefits from LSR in the first stage and essentially converges faster in the
second stage. Our theoretical result also shows that TSLA has better testing error than that of LSR.
Throughout extensive experiments, we have shown that TSLA improves the generalization accuracy
of deep models on several benchmark data sets.
6https:zzpytorch.orgzdocszstableztorchvisionzmodels.html
9
Under review as a conference paper at ICLR 2022
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252, 2019.
Hessam Bagherinezhad, Maxwell Horton, Mohammad Rastegari, and Ali Farhadi. Label refinery:
Improving imagenet classification through label progression. arXiv preprint arXiv:1805.02641,
2018.
Zachary Charles and Dimitris Papailiopoulos. Stability and generalization of learning algorithms
that converge to global optima. In International Conference on Machine Learning, pp. 745-754,
2018.
Jan Chorowski and Navdeep Jaitly. Towards better decoding and language model integration in
sequence to sequence models. Proc. Interspeech 2017, pp. 523-527, 2017.
Qianggang Ding, Sifan Wu, Hao Sun, Jiadong Guo, and Shu-Tao Xia. Adaptive regularization of
labels. arXiv preprint arXiv:1908.05474, 2019.
Timothy Dozat. Incorporating nesterov momentum into adam. 2016.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12:2121-2159, 2011.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and
stochastic programming. Math. Program., 156(1-2):59-99, 2016.
Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation meth-
ods for nonconvex stochastic composite optimization. Mathematical Programming, 155(1-2):
267-305, 2016.
Morgane Goibert and Elvis Dohmatob. Adversarial robustness via adversarial label-smoothing.
arXiv preprint arXiv:1906.11567, 2019.
Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of
stochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
770-778, 2016.
Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag of tricks for
image classification with convolutional neural networks. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 558-567, 2019.
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning
lecture 6a overview of mini-batch gradient descent. 2012a.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In
NeurIPS Deep Learning Workshop, 2014.
Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdi-
nov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint
arXiv:1207.0580, 2012b.
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for
mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
10
Under review as a conference paper at ICLR 2022
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the Polyak-IojasieWicz condition. In Joint European Conference on Ma-
chine Learning and Knowledge Discovery in Databases, pp. 795-811. Springer, 2016.
Aditya Khosla, Nityananda JayadevaPrakash, BangPeng Yao, and Fei-Fei Li. Novel dataset for fine-
grained image categorization: Stanford dogs. In Proc. CVPR Workshop on Fine-Grained Visual
Categorization (FGVC), volume 2, 2011.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Mas-
ter’s thesis, Technical report, University of Tronto, 2009.
Xiaoyu Li, Zhenxun Zhuang, and Francesco Orabona. Exponential step sizes for non-convex opti-
mization. arXiv preprint arXiv:2002.05273, 2020a.
Xingjian Li, Haoyi Xiong, Haozhe An, Dejing Dou, and Chengzhong Xu. Colam: Co-
learning of deep neural netWorks and soft labels via alternating minimization. arXiv preprint
arXiv:2004.12443, 2020b.
Zhize Li and Jian Li. A simple proximal stochastic gradient method for nonsmooth nonconvex
optimization. In Advances in Neural Information Processing Systems, pp. 5564-5574, 2018.
Michal Lukasik, Srinadh Bhojanapalli, Aditya Krishna Menon, and Sanjiv Kumar. Does label
smoothing mitigate label noise? arXiv preprint arXiv:2003.02819, 2020.
Rafael Muller, Simon Komblith, and Geoffrey E Hinton. When does label smoothing help? In
Advances in Neural Information Processing Systems, pp. 4696-4705, 2019.
Yurii Nesterov. A method of solving a convex programming problem With convergence rate
O(1/k2). Soviet Mathematics Doklady, 27:372-376, 1983.
Yurii Nesterov. Introductory lectures on convex programming volume i: Basic course. 1998.
Yurii Nesterov. Introductory lectures on convex optimization : a basic course. Applied optimization.
KluWer Academic Publ., 2004. ISBN 1-4020-7553-7.
Toan Q Nguyen and Julian Salazar. Transformers Without tears: Improving the normalization of
self-attention. arXiv preprint arXiv:1910.05895, 2019.
Tianyu Pang, Chao Du, Yinpeng Dong, and Jun Zhu. ToWards robust detection of adversarial exam-
ples. In Advances in Neural Information Processing Systems, pp. 4579-4589, 2018.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. In Advances in Neural Information Processing Systems,
pp. 8024-8035, 2019. URL https://pytorch.org/docs/stable/torchvision/
models.html.
Gabriel Pereyra, George Tucker, Jan ChoroWski, Lukasz Kaiser, and Geoffrey Hinton. Regularizing
neural netWorks by penalizing confident output distributions. arXiv preprint arXiv:1701.06548,
2017.
Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR Com-
putational Mathematics and Mathematical Physics, 4(5):1-17, 1964.
Boris Teodorovich Polyak. Gradient methods for minimizing functionals. Zhurnal Vychislitel’noi
Matematiki i Matematicheskoi Fiziki, 3(4):643-653, 1963.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
International Conference on Learning Representations, 2018.
11
Under review as a conference paper at ICLR 2022
Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew
Rabinovich. Training deep neural networks on noisy labels with bootstrapping. arXiv preprint
arXiv:1412.6596, 2014.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathemati-
Cal statistics, pp. 400-407, 1951.
Chaomin Shen, Yaxin Peng, Guixu Zhang, and Jinsong Fan. Defending against adversarial
attacks by suppressing the largest eigenvalue of fisher information matrix. arXiv preprint
arXiv:1909.06137, 2019.
Patrice Y Simard, Yann A LeCun, John S Denker, and Bernard Victorri. Transformation invariance
in pattern recognition—tangent distance and tangent propagation. In Neural networks: tricks of
the trade, pp. 239-274. Springer, 1998.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9, 2015.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-
ing the inception architecture for computer vision. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 2818-2826, 2016.
Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural net-
works. In International Conference on Machine Learning, pp. 6105-6114. PMLR, 2019.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems, pp. 5998-6008, 2017.
C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011
Dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.
Shuo Wang, Zhaopeng Tu, Shuming Shi, and Yang Liu. On the inference calibration of neural
machine translation. arXiv preprint arXiv:2005.00963, 2020.
Zhe Wang, Kaiyi Ji, Yi Zhou, Yingbin Liang, and Vahid Tarokh. Spiderboost and momentum:
Faster variance reduction algorithms. In Advances in Neural Information Processing Systems, pp.
2403-2413, 2019.
Lingxi Xie, Jingdong Wang, Zhen Wei, Meng Wang, and Qi Tian. Disturblabel: Regularizing
cnn on the loss layer. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 4753-4762, 2016.
Yan Yan, Tianbao Yang, Zhe Li, Qihang Lin, and Yi Yang. A unified analysis of stochastic momen-
tum methods for deep learning. In International Joint Conference on Artificial Intelligence, pp.
2955-2961, 2018.
Li Yuan, Francis EH Tay, Guilin Li, Tao Wang, and Jiashi Feng. Revisit knowledge distillation: a
teacher-free framework. arXiv preprint arXiv:1909.11723, 2019a.
Zhuoning Yuan, Yan Yan, Rong Jin, and Tianbao Yang. Stagewise training accelerates convergence
of testing error over sgd. In Advances in Neural Information Processing Systems, pp. 2604-2614,
2019b.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. British Machine Vision Confer-
ence, 2016.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
12
Under review as a conference paper at ICLR 2022
Albert Zeyer, KazUki Irie, Ralf Schluter, and Hermann Ney. Improved training of end-to-end atten-
tion models for speech recognition. Proc. Interspeech 2018, pp. 7-11, 2018.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. In International Conference on Learning Representations, 2018.
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures
for scalable image recognition. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 8697-8710, 2018.
13
Under review as a conference paper at ICLR 2022
A Additional Experiments
All experiments are implemented using PyTorch7 with version 1.7.1 on a machine with 8 NVIDIA
P100 GPUs. The code is available at
https://drive.google.com/drive/folders/1QLvSE6iB2YN7UBpRMKB_
kKQGZzK7M4Qr?usp=sharing
A. 1 THE ESTIMATION OF δ
As mentioned in previsous sections, the value of δ plays a very important role on the theoretical
results. In this section, we provide the estimation of δ for three data sets Stanford Dogs, CUB-2011
and CIFAR-100 that used in the experiments. Our results empirically show that δ < 1 for all three
data sets. Specifically, we computed the sample variances (denoted by σk2 and σbk2 , k = 10, 20, . . .
) of two variance σ2 and σb2 every 10 epochs, then based on the definitions in (4), the etimation
(denoted by δ) of δ was taken as
σbk2
δ = max T.
k σk2
All results of estimated δ are listed in Table 4, empirically showing that δ < 1. We also plot the
estimated values of δ across iterations over three different data sets in Figure 4, showing that δ < 1
for all data sets across iterations.
Table 4: The estimated value of δ for different data sets
daqset	Stanford Dogs CUB-2011 CIFAR-100 CIFAR-100 (Pretrained)
e	0.16	0.13	055	0.38
0.30
0.25
0.20
value of δ
——CUB-2011
---Stanford Dogs
0.15
0.10
0.05
0.00^----~~--——~~--~~--~~~~--~~--
10 20 30 40 50 60 70 80 90
# epoch
Figure 4: The estimated values of δ for different data sets.
A.2 More Results on Different Models and Datasets
More exPeriments on different models and datasets are included in this subsection. First, we trained
models including ResNet-50, VGG (Simonyan & Zisserman, 2014), MobileNet (Howard et al.,
7A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L.
Antiga, et al. Pytorch: An imPerative style, high-Performance deeP learning library. Advances in Neural
Information Processing Systems, 32: 8026-8037, 2019. https://pytorch.org
14
Under review as a conference paper at ICLR 2022
2017), Inception (Szegedy et al., 2015) and EfficientNet (Tan & Le, 2019) on three datasets CI-
FAR100, CUB-2011 and Stanford Dogs. We followed the same hyper-parameter setting as in the
main paper, except for the smoothing strength θ in LSR. We tuned the parameter θ for LSR and used
the one with best performance. For TSLA, the same values of smooth strength θ are used during the
first stage of TSLA. The specific values of θ for different models and datasets are listed in Table 5.
For the simplicity of presentation, we only report the best performance of TSLA among different
drop epochs s, where the values of used hyper-parameter s are listed in Table 5.
Table 5: The hyper-parameters θ and S for different models and datasets*
Model		θ			S		
	CUB	Dogs	CIFAR	CUB	Dogs	CIFAR
ResNet-50	0.4	0.5	^^04	^70^^	50	180
VGG	0.4	0.4	0.4	50	30	180
MobileNet v2	0.4	0.5	0.5	50	30	180
Inception v3	0.1	0.1	0.1	80	40	180
EfficientNet	0.1	0.1	0.3	20	30	180
*s: TSLA drops offLSR after S epochs.
All top-1 and top-5 accuracy on the testing datasets are summarized Tables 6, 7 , 8 for CUB-2011,
Stanford Dogs, and CIFAR-100, respectively. Overall, the results show that TSLA has the best
performance, comparing with baseline and LSR.
Table 6: Comparison of Testing Accuracy (%) for Different Models on CUB-2011
Model	Top-1 accuracy			Top-5 accuracy		
	baseline	LSR	TSLA	baseline	LSR	TSLA
ResNet-50	80.15	82.76	82.83	-94.87^^	95.08	95.55
VGG	80.58	81.14	81.71	95.46	95.31	95.62
MobileNet v2	78.44	79.91	80.14	94.58	94.39	94.94
Inception v3	79.08	80.12	80.19	94.63	95.24	95.19
EfficientNet	81.12	80.95	81.55	95.53	95.25	95.56
Table 7: Comparison of Testing Accuracy (%) for Different Models on Stanford Dogs
Top-1 accuracy	Top-5 accuracy
Model	baseline	LSR	TSLA	baseline	LSR	TSLA
ResNet-50	88.14	89.50	89.90	^9904^^	99.10	99.42
VGG	85.96	86.42	86.64	98.61	98.72	98.85
MobileNet v2	83.05	83.62	84.34	97.98	97.68	98.40
Inception v3	87.10	87.30	87.48	98.93	98.82	98.90
EfficientNet	85.72	85.71	86.26	98.80	98.60	98.78
Table 8: Comparison of Testing Accuracy (%) for Different Models on CIFAR-100
Model	Top-1 accuracy			Top-5 accuracy		
	baseline	LSR	TSLA	baseline	LSR	TSLA
ResNet-50	76.63	77.52	77.71	-93.99^^	93.08	94.17
VGG	72.99	74.09	74.22	90.59	90.37	91.70
MobileNet v2	74.42	76.64	76.72	93.12	93.13	94.01
Inception v3	79.95	79.71	79.87	95.23	94.76	95.19
EfficientNet	69.73	71.38	71.68	90.50	90.48	91.93
15
Under review as a conference paper at ICLR 2022
Besides, we train ResNet-50 over ImageNet with the total epochs of 180. The initial learning rate is
set to be 0.2 for all algorithm and divide them by 10 every 60 epochs as suggested in (He et al., 2016;
Zagoruyko & Komodakis, 2016). The mini-batch size of 512 is used. The momentum parameter
and weight decay are set to be 0.9 and 10-4 respectively. We fix the smoothing strength θ = 0.1
and the drop epoch s = 120 for TSLA. The top-1 accuracy are presented in Table 9, showing that
TSLA and LSR are comparable.
Table 9: Comparison of Testing Accuracy (%) for ImageNet
Top-1 accuracy	Top-5 accuracy
Model baseline LSR―TSLA―baseline LSR―TSLA
ResNet-50	76.35	76.96 77.09 ∣ 93.14	93.55 93.69
B Technical Lemma
Recall that the optimization problem is
1n
miw Fs(w) := - £'(yi,f(w； Xi))
w∈	n i=1
where the cross-entropy loss function ` is given by
'(y,f (w； x)) = X -yi log (	"(w； X))
y,	；	yi g K
i=1	j=1 exp(fj (w； X))
(7)
(8)
If we set
p(w； x) = (Pi(w； x),...,Pk (w； x)) ∈ RK, Pi(w； x) = — log [	KXMfiwX))	j , (9)
j=1 exp(fj (w； X))
the problem (7) becomes
1n
Wmin FS (w) ：= n Ehyi,p(w； Xi )).
i=1
Then the stochastic gradient with respective to w is
V'(y,f (w; x)) = hy, Vp(w; x)〉.
Lemma 1. Under Assumption 1 (i), we have
E [∣∣V'(y/,f (wt； Xt)) - VFs(Wt)『]≤ (1 - θ)σ2 + θδσ2.
(10)
(11)
Proof. By the facts of ytLS = (1 - θ)yt + θybt and the equation in (11), we have
V'(yLS, f (wt; Xt)) = (1 - θ)V'(yt, f (wt； Xt)) + θV'(yt,f (wt； Xt)).
Therefore,
E h∣∣v'(yLS,f (wt； Xt))-VFS (wt)∣∣2i
=E [k(1 — θ)V'(yt,f(wt；Xt)) — VFs(wt)] + θ[V'(bt,f(wt;Xt)) — VFS(wt)]『]
≤)(1 - θ)E [∣∣V'(yt ,f(wt; Xt))-VFS (Wt)『]+ θE [∣∣V'(yt, f (wt； Xt))-VFS (Wt)『]
(b)
≤ (1 - θ)σ2 + θδσ2,
where (a) uses the convexity of norm, i.e., k(1 - θ)a + θbk2 ≤ (1 - θ)kak2 + θkbk2; (b) uses
assumption 1 (i) and the definitions in (4), and Assumption 1 (i).	口
16
Under review as a conference paper at ICLR 2022
C Proof of Theorem 3
Proof. By the smoothness of objective function F (w) in Assumption 1 (ii) and its remark, we have
FS(wt+1) - FS(wt)
≤{VFS(wt), wt+ι - Wti + 2∣Wt+ι — Wtk2
=) — η(VF⅛(wt), V'(yLs,f(wt;Xt))) + η2L UV'(yLs,f(wt;xt))∣∣2
=- 2 kVFs(wt)k2 + 2 ∣∣VFs(Wt)- V'(yLs,f(wt; Xt))『+ ?- 1 ∣∣V'(yLs, f(wt； Xt))『
≤) — 2 kVFs(wt)k2 + 2 ∣∣VFs(Wt)- V'(yLs,f(wt; Xt))『，	(12)
where (a) is due to the update of Wt+ι; (b) is due to ha, -bi = 2 (∣∣a 一 b∣∣2 一 ∣∣ak2 一 ∣∣b∣∣2)； (C)
is due to η ≤ L. Taking the expectation over (xt, yLS) on the both sides of (12), we have
E [FS(Wt+1) - FS(Wt)]
≤-2e h∣VFs (Wt)『]+ 2e h∣∣VFs (Wt) - V'(yLS ,f (Wt； Xt))『]
≤-2E [kVFs (Wt)k2] + 2 ((1 - θ)σ2 + θδσ2) .	(13)
where the last inequality is due to Lemma 1. Then inequality (13) implies
T-1
T X E [kVFs(Wt)k2] ≤2 S(WO +(1 - θ)σ2 + θδσ2
t=0	η
=)2Fs (w0)
ηT
2δ	2
十不σ
(≤) 2Fs (wo)
_ ηT
+ 2δσ 2 ,
where (a) is due to θ = ɪ+^; (b) is due to ɪ+^ ≤ 1.
□
D CONVERGENCE ANALYSIS OF SGD WITHOUT LSR (θ = 0)
Theorem 6. Under Assumption 1, the solutions wt from Algorithm 1 with θ = 0 satisfy
1 T-1
TEE [kVFs(Wt)k2] ≤
T t=0
2Fs (wo)
ηT
+ ηLσ2 .
In order to have ER[kVFs(wr)k2] ≤ e2, it suffices to set η = min (*,2Lσ2) and T = 4FS(w0)
the total complexity is O (±).
Proof. By the smoothness of objective function Fs (W) in Assumption 1 (ii) and its remark, we have
Fs(Wt+1) - Fs(Wt)
≤hVFS (wt), Wt+1 — Wti + 2 kwt+1 — Wtk2
=) - η EFS(wt), V'(yt,f (wt; Xt))i + η2L kV'(yt, f (wt； Xt))『,
(14)
17
Under review as a conference paper at ICLR 2022
where (a) is due to the update of wt+1. Taking the expectation over (xt; yt) on the both sides of
(14), we have
E [FS(wt+1) - FS(wt)]
(a)	η2L	2
≤- ηE [kVFs(Wt)II ] +—2-E [kV'(yt, f (wt; Xt)) -VF⅛(Wt) + VFs(Wt)Ill
=) - ηE [kVFs(wt)k2] + η2LE h∣∣V'(yt,f(wt; Xt))- VFs(Wt)『]+ η2LE [∣∣VFs画)『]
≤ - 2e [kVFs(wt)k2] + η2Lσ2.	(15)
where (a) and (b) use Assumption 1 (i); (c) uses the facts that η ≤ L and Assumption 1 (i). The
inequality (15) implies
T-1
T X E [kVFs (Wt)k2] ≤ —ST0)+ ηLσ2.
t=0	η
By setting η ≤ 2Lσ2 and T = 4Fη(w0), We have T P=1 E [kVFs(Wt)k2] ≤ e2. Thus the total
complexity is in the order of O (η⅛) = O (1)∙	□
E	Proof of Theorem 4
Proof. Following the similar analysis of inequality (13) from the proof of Theorem 3, we have
E [Fs(Wt+1) - Fs (Wt)]
≤ - ηE [∣∣vfs (Wt)k2]+η ((I- θ)σ2+θδσ2).	(16)
Using the condition in Assumption 2 we can simplify the inequality from (16) as
E[Fs(Wt+ι) - Fs(WS)]
≤(1 - ηιμ)E [Fs (Wt)- FS (WS)] + -21 ((I- θ)σ2+ θδσ2)
t
≤(I- ηι μ)t+1 E [Fs (WO)- Fs (WS)] + -21 ((I- θ)σ2 + θδσ2) X(I- ηι μ)i
t
≤ (1 - ηιμ)t+1 E [Fs(wo)] + -21 ((1 - θ)σ2 + θδσ2) X (1 - nιμ)i,
where the last inequality is due to the definition of loss function that FS (WS) ≥ 0. Since -ι ≤ L <
1, then (1 - ηιμ)t+1 < exp(--ιμ(t + 1)) and P：= (1 - -ιμ)i ≤ 焉.AS a result, for any Ti,
we have
E [Fs(wti) - Fs(wS)] ≤ exp(-ηιμTι)Fs(wo) + 2μ ((1 - θ)σ2 + θδσ2) .	(17)
Let θ = ι+δ and b2 ：= (1 - θ)σ2 + θδσ2 = 12+δδσ2 then 2μ ((1 - θ)σ2 + θδσ2) = ."丁)≤
Fs(Wo) since δ is small enough. By setting
2μFs (WO)
Ti = log( —b—) /(ηιμ)
we have
σb2	2δσ 2
E [F(WTI) - Fs(ws)] ≤ — ≤ -------.	(18)
μ μ
18
Under review as a conference paper at ICLR 2022
After T1 iterations, we drop off the label smoothing, i.e. θ = 0, then we know for any t ≥ T1,
following the inequality (15) from the proof of Theorem 6, we have
E [Fs (wt+ι) - Fs (Wt)] ≤-η22 E [kVF⅛ (wt)『]+ η2∣σ2.
Therefore, we get
1	T1 +T2 -1	2
E-	E E [kvFS (Wt)k2] ≤ -^γe [FS (WTI)- FS (WTi+T2-1)]+ η2Lσ2
T2	η2T2	1	1	2
t=T1
(a)	2
≤ ʒTrE [Fs(wτι) — FS(WS)] + η2Lσ2
η2 T2	1
(18)	4δσ2	2
≤	+ η2Lσ ,
μη2T2
(19)
where (a) is due to FS(WT1+T2-1) ≥ FS(WS). By setting η2 = 2⅛ and τ2 = μ8δ⅛，We have
T Pτ=+T2-1 E [kVFs(Wt)k2] ≤ e2.	□
F Generalization Analysis
In this section, let
λ
FS (W) =FS (W) + 2 kwk2
λ
F(W) =F (w) + 2 kWk2
1n	λ
nE'(yi,f(w; Xi)) + 2 kWk2
i=1、----------{z--------}
^, ., 、、
'(yi ,f(w;Xi))
(20)
(21)
Following by Yuan et al. (2019b), we use the following decomposition of testing error.
Ea,s[F(ws)] — ES[Fs(WS)] ≤ ES[Ea[Fs(WS) - FS(WS)]] +Ea,s[F(WS) - FS(WS)].
(22)
Let define the gradient update rule Gbn as follows
_	_ 个， ~ ，	…
Gb,η(w) = w - ηVw'(y, f(w, x)).
(23)
Then we have the following lemma, which is similar to Lemma 2.5 and Lemma 4.2 in Hardt et al.
(2015).
Lemma 2. Assume that	'(y,	f (w, x))	is	L-smooth and	B -Lipschitz. Let	Wt+ι	=	G (Wt)	and
Wt0+1 = G0(Wt0 ), then
kWt+1 - W0t+1 k
(1 +ηL - ηλ)kWt - Wt0k,
(1 - ηλ)kWt - Wt0k + 2ηB,
G = G0,
G 6= G0 .
Proof. Recall that b(yi,f (w; Xi)) = '(y"(w; Xi)) + 2∣∣wk2. Then
⑴ When G = G0 and '(∙, f (w; ∙)) is L-smooth, then
I∣wt+1 - wt+ιk ≤(1 - ηλ)∣∣Wt - wtk + ηkvw'(yt,f (wt； Xt))-VW'(yt, f(wt; Xt))Il
≤(1 + ηL - ηλ)kWt - Wt0 k.
(2) When G = G0 and '(∙, f (w; ∙)) is B-Lipschitz, then
∣∣wt+ι - wt+ιk ≤(I-ηλ)∣∣wt - wtk + ηkvw'(yt,f (wt； Xt))-VW'(yt,f (Wt; Xt))Il
≤(1 - ηλ)kWt - Wt0k +2ηB.
□
19
Under review as a conference paper at ICLR 2022
F.1 Proof of Theorem 5
Proof. By Theorem 3.2 of Hardt et al. (2015), we have
∆t+1 :=E[kwt+1 -w0t+1k]
(a)
≤
(1 + ηL - ηλ)∆t + ɪ((i - ηλ)∆t + 2ηtB)
n
ι+(1-n”“十中
(b)	2ηB (c) 2ηB t0	i
≤ (1 - ηL)∆t +-≤	(1 - ηL)
nn
i=0
(d) 2B
≤ nL,
(24)
where (a) uses Lemma 2; (b) uses λ = 2L; (c) uses ∆t0 = 0; (d) uses ηL < 1. Then by Lemma
3.11 of Hardt et al. (2015) and ` is L + λ-smooth, we have
E[b(wt;Z)-b(wt;z)]≤t0 + 2B≡^) ≤ 6B+1,	(25)
n nL	n
where the last inequality holds by selecting t0 = 1 and λ = 2L. Then by Theorem 2.2 of Hardt
et al. (2015)
6B + 1
Ea,s[F(wt) - FS(wt)] ≤	,	(26)
n
which is the generalization error. Next, we will bound the optimization error. Since FbS is (L + λ)-
smooth and satisfies μ-PL condition, then follow the similar analysis of Theorem 4, We have
1 T1+T2-1	1 T1+T2-1
T X	Es[Ea[Fs(Wt)- FS(wS)]] ≤FF X E [kVFbs(Wt)Il2]
2 t=Tι	μ 2 t=Tι
2δσ2	η2(L + λ)σ2
^μ2η2T2 +	2μ
Then by (22), (26) and (27), we get
T1 +T2 -1
1
Tr	E	eA,S[F(Wt)] - ES[FS(WS)] ≤
T2	t=T1
(27)
(28)
2δσ2	η2(L + λ)σ2	6B + 1
μ2η2T2 +	2μ	n n
λ
By using the conditions that mι□w∈w FS(w) ≤ FS(WS) + 2 ∣∣wt∣∣2, definitions (20) and (21), we
get
一, . 一 ,,. ^, . ^ , ʌ ,.
F(Wt)- FS(WS) ≤ F(Wt)- FS(WS).	(29)
Therefore, we have the following inequality by (28) and (29):
1 T1 +T2 -1
五 E	ea,s[F(Wt)] - ES[FS(wS)] ≤
2	t=T1
2δσ2	η2(L + λ)σ2	6B + 1
μ2η2T2 +	2μ	+ n
(30)
By setting η2 = O(1∕√n) and T = O(δn) then the testing error Er,a,s [F(wr)]-Es [Fs(WS)] ≤
O(1∕√n), where A is TSLA.
Remark 1. (SGD without LSR) Following the similar analysis of Theorem 5 and similar analysis
of Theorem 6, we have the testing error for SGD without LSR is
Er,a,s [F(wr)] - Es [Fs (WS)] ≤ FS(W) +	+ ""2 + 6B+1.	(31)
μηl	2μ	n
By setting η = O(1∕√n) and T = O(n) then Er,a,s [F(wr)] - ES[FS(WS)] ≤ O(1∕√n), where
Ais SGD without LSR.
Remark 2. (SGD with LSR) Following the similar analysis of Theorem 5 and similar analysis of
Theorem 3, we have the testing error for SGD with LSR is
Er,a,s [F(wr)] - Es [Fs(WS)] ≤ Fs(w0) + δσ2 + 6B+1,	(32)
μηl μ n
where A is SGD with LSR. If δ > Ω(1∕√n), the testing error can not be bounded by O(1∕√n); if
δ ≤ Ω(1∕√n), the testing error can be bounded by O(1∕√n) in T = O(√n) iterations.
20