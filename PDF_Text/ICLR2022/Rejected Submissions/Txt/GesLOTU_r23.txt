Under review as a conference paper at ICLR 2022
Gradient	Explosion	and Representation
Shrinkage in Infinite Networks
Anonymous authors
Paper under double-blind review
Ab stract
We study deep fully-connected neural networks with layer normalization using the
mean field formalism, and carry out a non-perturbative analysis of signal propaga-
tion. As a result, we demonstrate that increasing the depth leads to gradient explo-
sion or to another undesirable phenomenon we call representation shrinkage. The
appearance of at least one of these problems is not restricted to a specific initial-
ization scheme or a choice of activation function, but rather is an inherent property
of the fully-connected architecture itself. Additionally, we show that many popu-
lar normalization techniques fail to mitigate these problems. Our method can also
be applied to residual networks to guide the choice of initialization variances.
1	Introduction
Deep learning is arguably the most successful modern machine learning method when it comes to
modelling complex data. Informally, this is often attributed to hidden representations not being con-
strained by the designer’s knowledge. Empirically, deep neural networks tend to outperform shallow
ones, which is explained by them learning a richer hierarchy of representations. There is a body of
works making precise a number of aspects of this idea, e.g. showing that expressivity (Montufar
et al., 2014) and disentangling ability (Poole et al., 2016) grow with the network’s depth.
Because large depth is often desired, much effort was devoted to networks with many layers. There
are three main groups of techniques to aid their training: normalization, critical initialization and
skip connections. Normalizations ensure the right scale of certain preactivation statistics, for ex-
ample magnitude or batch moments. Critical initialization is based on mean-field analysis of signal
propagation. It aims at picking an initialization scheme that brings the singular values of the input-
output Jacobian close to 1. Skip connections are used in residual blocks, which learn the difference
between the target function and the identity. Such construction guarantees that the network can rep-
resent the identity. A different approach with similar motivation is to use a parameterized activation
functions, which are linear for some value of its parameters, so that not only the identity but every
linear map is guaranteed to be representable. Our results demonstrate a limitation of normalization
methods and critical initialization, and naturally suggest initializing deep networks as linear maps.
A different line of research studied the physics of large networks. It is known that wide but shallow
neural networks at initialization simplify to Gaussian processes (GP) (Neal, 1996), and their training
is shown to be equivalent to kernel methods (Jacot et al., 2018). However, realistic finite networks
outperform their infinite limits (Ghorbani et al., 2020), which is explained by the lack of feature
learning in large networks (Chizat et al., 2019). The situation with deep networks is qualitatively
different - the GP regime breaks down when depth and width are large but comparable. Therefore, a
well-behaved infinite depth and width limit would provide a promising starting point for a theoretical
model capable of capturing feature learning. Suitably scaled infinite residual networks (ResNets)
become diffusion processes; one could hope that by appropriately combining different activation
functions and normalization methods, one could obtain a similar behaviour in MLPs. We hope
that by explaining why this is impossible, our result will help to illuminate most promising future
directions in physics of large networks.
The main advice of our work - that there are limitations to increasing the depth, even at critical
initialization - was also noted in Pennington et al. (2017) and Pennington et al. (2018). This line of
work expresses the input-output Jacobian as a product of random matrices, and uses random matrix
1
Under review as a conference paper at ICLR 2022
theory (RMT) to analyse its full spectrum. They find that the maximum and variance of its singular
values grow with depth even if the mean remains O ⑴.
1.1	Related work
Normalizations: Rescaling the preactivations to zero mean and unit variance across a mini-batch
was introduced in Ioffe & Szegedy (2015); across all the neurons in single layer was done explicitly
in (Lei Ba et al., 2016) and implicitly in (Klambauer et al., 2017). For an overview see Huang et al.
(2020). Mean-field analysis of batch normalization was carried out in Yang et al. (2019).
Critical initialization: Mean-field formalism, describing signal propagation in wide networks, was
introduced in Poole et al. (2016) based on ideas from Neal (1996). It was applied in Schoenholz et al.
(2017) to characterize Lyapunov exponents for signal and gradient propagation in fully-connected
networks. The impact of activation function on initialization was investigated in Hayou et al. (2019);
this work, as well as Xiao et al. (2019), note polynomially-quick convergence of correlation with
depth at criticality. A form of correlation degeneracy was noted in Daniely et al. (2017). Initialization
of convolutional networks was discussed in Xiao et al. (2018).
Skip connections: Residual blocks were introduced in He et al. (2016). Decreasing the variance of
residual weights with depth was shown to be necessary for a non-trivial kernel (Hayou et al., 2021)
and good signal propagation with a class of activations (Yang & Schoenholz, 2017); its utility was
observed empirically in (Wang et al., 2018). Trainable activations, brought closer to linear at the
initialization, were reported to help in training very deep networks in He et al. (2015).
Limiting behaviour of large nets: Breakdown of GP regime was demonstrated in Hanin & Nica
(2020). Physics of large networks was studied systematically in Roberts et al. (2021). The corre-
spondence between infinitely deep ResNets and diffusion processes was described in Peluchetti &
Favaro (2020), and generalized to doubly infinite ResNets in Peluchetti & Favaro (2021). A different
kind of problems with wide and deep ReLU networks was analysed in Hanin & Rolnick (2018).
1.2	Contribution
To the best of our knowledge, we present the first general non-perturbative result on signal prop-
agation in infinitely wide networks. It holds for arbitrary* 2 1 activation functions, and they can vary
between layers. Our proof does not rely on the assumption of independence between gradients
and weights. On a high level, our framework explains the origin of “signal distortion” in fully-
connected networks, and explains why they are harder to stack than residual networks. Unlike RMT,
our method can handle pairs of data points with non-infinitesimal differences.
We provide a precise trade-off between having rich hidden representations and well-behaved gradi-
ents in fully-connected and residual networks, making rigorous the intuition of competition between
these two goals. Unlike in previous work, our result is non-asymptotic in depth and avoids formu-
lation “up to a constant”. We hope that improvements to our result will serve as a guidance for
architectural choices.
2 Set-up
2.1 Notation
We denote the inner product and vector norms of u, v ∈ Rd as
d
u.v
i=1
uivi
∣∣uk = ʌ/u,u
For a matrix M ∈ Rn×m , it will be natural to induce the operator norm by the root-mean-squared
norm
∣M ∣op
max
v∈Rm
n- 2 ∣∣Mv k
m-2 ∣v∣
1Technically activations have to be square-integrable with respect to Gaussian weight
2
Under review as a conference paper at ICLR 2022
Gaussian covariance will be denoted as
C[f∣ρ] = E f(χ)f(y)
x ~N 0，1 P
To compare activation functions we will be using the Gaussian norm
kΦkN = E 1(N(0,1))2i = C[φ∣1]
We linearize functions by truncating their Hermite expansion
∞
F ∖	7 . 7	q 一	/	7 7
φ(x) = φ0 + φ1x	for	φ =	φkhk
k=0
where hk are normalized Hermite polynomials2. Let us note that they satisfy E[hk (x1)hk0 (x2)] =
δkk0 ρk for jointly normal zero-mean x1, x2 of variances 1 and covariance ρ (for a proof, see for
example O’Donnell (2021)).
2.2 Set-up
We will focus on an untrained fully-connected neural network of depth L and Nl neurons in layer l.
We compose it from pointwise non-linearities, affine transforms and layer normalizations2 3; formally
the output of the i-th neuron in layer l on data point xα is given by
Nl-1
Z(I)(Xa)= √k¾xαXα)	where	y(I)(Xa) = √n= X Wijl)MzjTI(Xa)) + b^ Q∙I)
j=1
where weights Wi(jl) ∈ Rl ×l-1 and biases bi(l) ∈ RNl have variances σ(2l),w, σ(2l),b respectively,
and φl : R J⊃ is the activation function of l-th layer. We will refer to z(l) as preactivations. At the
first layer, we set φ1(X) = X and regard it as an “embedding”.
An important role in our analysis will be played by correlation or cosine similarity of the preactiva-
tions, defined for two input data points Xa , Xβ as
l
Pl = N X z(l) (Xa) z(l)(xβ)
i=1
y(l)(xa).y(l)(xe)
ky(l)(xa)IHIy(l)(xe )k
(2.2)
When the widths are very large Nl → ∞, with Xa fixed and the parameters (Wi(jl), bi(l)) having
independent entries4 (e.g. distributed according to the Gaussian initialization scheme), one can use
the mean field approximation (Poole et al., 2016) and treat zi(l) (Xa) as a normal random variable.
Then, by the law of large numbers applied to equation 2.2, the correlation changes as
Pl
P (	) def σ2l),b + σ2l),w C[φlpl-1]
Pl(Pj) =	σ2i),b + σ2i),wC[φ∣1]
(2.3)
Letting φ = Pk=o φk hk be the expansion of the activation function φ into normalized Hermite
polynomials, we can work out the Gaussian covariance and obtain
∞
P(P) = σ2++σw,φ∣0N + σ+¾⅛N X φkρk	Q.4)
k=1
Observation 1. From expression 2.4 it is apparent that P is a power series with non-negative coef-
ficients. Therefore, it is non-decreasing and convex on [0, 1]. This implies existence and uniqueness
of an attracting (stable) fixed point 5 of P.
2First two are h0(x) = 1, h1 (x) = x
3Without centering
4Technically, we also need regularity conditions (e.g. finite fourth moment) to ensure that CLT applies.
These hold in most practical scenarios.
5A solution to P(Pfp) = pfp, such that Pn(ρ) n→∞ PfP for all P in some neighbourhood of pfp. A sufficient
condition for the latter is P0(PfP) < 1.
3
Under review as a conference paper at ICLR 2022
This is relevant when σw ,σb, φ are the same across layers; then forward propagation affects the
correlation by repeated application of P, and Pl approaches PfP as l grows. For ρ0 ≈ 1 We have
1 - ρl ≈ P0 ⑴l 2(1 - ρo), so the derivative of P at 1 determines whether the fixed point P = 1 is
stable or unstable. Depending on the ratio σσb We have three regimes of initialization, analogous to
the phase diagram from Schoenholz et al. (2017):
•	Ordered initialization for P0(1) < 1. Then PfP = 1 is stable and ρl → 1 exponentially
quickly with l. Preactivation perturbations decay, so hidden representations of all data
become strongly aligned and gradients vanish.
•	Critical initialization for P0(1) = 1. Then PfP = 1 is stable and ρl → 1 polynomially
quickly with l. Norm of preactivation perturbations changes sub-exponentially quickly so
the gradients are well-behaved.
•	Chaotic initialization for P0(1) > 1. Then, the fixed point P = 1 of P is repelling (unstable)
and there exists another fixed point PfP < 1 which is attracting. In that case P0(PfP) < 1,
so Pl → Pfp exponentially quickly with l. Hidden representations of similar data points are
pulled far apart, preactivation perturbations grow and gradients explode.
3 Main res ult
Our main result, theorem 1, quantifies the trade-off between steepness, representation ampleness,
non-linearity and depth.
Theorem 1. Let Z(L) be a Widefully-Connected network as described in equation 2.1: having L lay-
ers, using layer normalization and employing activation function φl at layer l. In the limit nl → ∞
we have thefollowing inequality
箸L ≥ * X」
where ∂ZL is the input-outputJacobian, ∣∣φ-Φ∣∣N is the error oflinearizing the activationfunction
φ and the maximal correlation Pmax is defined as
Pmax
max min
1≤l≤L xα,xβ
Za)(Xa)Za)(Xe)
Ni
We delay the complete proof to appendix A, and in this section just sketch the main ideas. Let us
start with the consequences of theorem 1, and give interpretations for each quantity. Theorem 1 tells
us that at least one of the following undesirable effects will necessarily occur:
1. The Jacobian norm ∣∣ IzL^∣∣op, which measures the steepness of z(L) as a function of z(0),
is large. This means that a small change in the input drastically alters the output and the
network becomes overly sensitive to perturbations of data. In Bayesian interpretation, the
prior hypothesis favours jagged functions. Then the gradients with respect to the first layer
parameters are necessarily large, which means gradient explosion. This happens when
1(l" are predominantly small, resembling the chaotic phase.
2. The maximal correlation is close to one Pmax ≈ 1. This means that at some layer l, the hid-
den representation z(l) (x) of any possible input data point x lies in thecone VzNx >Pmax
for some vector v. We call this scenario representation shrinkage. It may lead to a range
of problems. First, the representations do not utilize the whole available space but rather
are confined to a small region. Second, a rich representation may be necessary for some
problems. In this case getting rid of the shrinkage may be a prerequisite for learning. How-
ever, it costs time. We have observed that the alignment of preactivations tends to disappear
layer-by-layer rather than at all the layers simultaneously, suggesting that deeper networks
require more optimization steps. Third, if the angle between preactivations becomes com-
parable to the machine precision, then in practice gradient descent may run into numerical
4
Under review as a conference paper at ICLR 2022
issues. Fourth, suppose that z(l) (x) ≈ v for any data point x. If the following block
z(l+k) ◦…O z(l+1) of k layers is reasonably smooth, then for any input it ever encounters,
the block will We well approximated by its linearization at v. Therefore, z(l+k) ◦•••◦ z(l+1)
does little processing, but requires the full computational budget to implement. This sce-
nario happens when σ(l),b tend to be large (ordered-phase-like behavour).
σ(l),w
3.	The linearization error ∣∣φι — φι ∣∣n is small. This restricts the choice of activation functions
we can use to ones that are close to linear. The layers need to be close to linear maps, so
informally they will not “process the data too much”. This forces us to make the MLP into
something resembling a ResNet. At zero error the network is completely linear, so forward
propagation perfectly preserves norms, covariances and correlations of preactivations. With
no correlation distortion the gradient-shrinkage trade-off disappears, which is reflected by
theorem 1 becoming vacuous.
4.	The depth L upper-bounded. Sum of many lower-bounded terms cannot stay upper-
bounded, so we cannot stack as many layers as we want.
Sketch of proof. We examine the cosine similarity of output preactivations ρL as a function of cosine
similarity of inputs ρ0 . This functional relationship in an example network is illustrated on figure 1.
For infinitely wide networks, this functional dependence is exactly described by
PL = PL ◦•••◦ P1(ρ0)
where Pl are taken from equation 2.3. By observation 1, each Pl is convex. By adding layers
we compose more and more P’s, and we can show that the convexity “accumulates”. Then, the
“flatness near 0” and the “steepness near 1” become more pronounced and the transition between
them becomes sharper6. Therefore, the infimum7 of PL ◦…O Pi is close to 1, or its derivative at
1 is large. In the former case every possible pair of output preactivations is strongly aligned and
ρmax ≈ 1; in the latter the Jacobian norm and thus gradients are large.
(a) L = 8
Figure 1: Output correlation PL as a function of input correlation po, for the erf activation. Empirical
(Nl = 1024) in red, infinite-width in black. Blue is an upper bound on PL ◦…O Pi - for details
see remark 2 in appendix A.1.
The underlying phenomenon is the pathological behaviour of PL as a function of P0 . Every z(L)
has correlation 1 with itself, so PL (P0 = 1) = 1. However, we can find an upper-bound (blue on
figure 1) that tends to PL ◦…O Pi (0) pointwise on [0,1) as L grows.	□
Remark 1. One might ask where does gradient vanishing fit in this framework. In fact, it is a
special case of representation shrinkage. This is best illustrated on the simplified example with
σ(l),W , σ(l),b, φl independent of l. Gradient vanishing happens at the ordered initialization, when
representation shrinking “proceeds exponentially quickly” with the number of layers, i.e. 1 — Pl =
O(e-λl). One way to think about theorem 1 is that criticality is not sufficient to have a well-behaved
network: even though it avoids gradient explosion and vanishing, representation shrinkage still
happens “at a polynomial rate” (theorem Iallows to deduce the rate at least 1 — Pl =O(I- 2)).
Now let us apply our technique to residual networks. We consider residual blocks con-
sisting of an affine transform, normalization, pointwise activation and a linear map. Af-
ter adding the residual we normalize the signal. Formally, it is described by the equations
6Pictorially, this resembles the behaviour of the graph of the function y = a +(1 — a)xa as α gets large.
When a ≈ 1 these functions have infimum close to L otherwise their slope (derivative) at 1 is big.
7By observation 1, the infimum on [0, 1] is attained at P = 0
5
Under review as a conference paper at ICLR 2022
Y(l) = U (l) z (l-1)
N = z(l-1) + √mv(l)Φι (k√¾Ml))
Where z(0) = x ∈ Rn is the input. The parameters are U(l) ∈ Rml×n, V(l) ∈ Rn×ml; we assume
their entries are independent with variances 1 and σ(2l) respectively.
Theorem 2. For such residual network in the limit n, ml → ∞ we have
Il ∂z(L)Il2 ≥ (1-Pmax)2 X σ2i)kφl-布N
Il dz(O)IIop —	8 g 1 + σ2i)kφlkN
where as before
ρmax = max min
1≤l≤ xα,xβ
Za)(Xa)Za)(Xe)
Ni
In addition to the four undesired phenomena of the fully-connected variant 1, theorem 2 about
ResNets permits another alternative: initialization variances σ(l) of weights V(I) being small. This
situation is not problematic; in the extreme case σ0) = 0 the network is initialized as the identity.
4 Experiments
4.1 Applications of theorem 1
We show the trade-off between non-linearity and representation shrinkage on the example of criti-
cally initialized leaky ReLU networks. For a leaky ReLU network with negative slope a and σb = 0,
the inequality 1 predicts
1 - Pmax ≤ /S ∙之∙唔F	(4.1)
For detailed derivation of the constants see appendix B. We see that adding layers with constant
a < 1 (figure 2a) and decreasing a at constant depth (figure 2b) decrease 1 - ρmax, i.e. cause
representation shrinkage at some layer. To avoid this effect, we either need to make L bounded and
the network shallow or a ≈ 1 and keep LReLUa close to linear.
(a) a = 0 (standard ReLU), 1 ≤ L ≤ 512
Figure 2: 1 - ρL as function of depth L (fig 2a) and negative slope a (fig 2b). Empirical correlations
for sample data pairs in red; infinite-width predictions in black; smallest correlation (largest 1 - Pl)
found by gradient descent on data (with network parameters fixed) in green; bounds obtained by
rearranging inequality 4.1 in blue. Lower means smaller 1 - ρ, i.e. stronger shrinkage.
(b) L = 256, 0 ≤ a ≤ 1
We illustrate the relationships between gradient explosion and representation shrinkage on the ex-
ample of erf8 activation. With critical initialization, the operator norm of the network Jacobian (3c)
8Smooth increasing function R → [—1,1] defined as erf(x) = -√∏ R0 e-y2dy
6
Under review as a conference paper at ICLR 2022
does not grow with depth, and thus gradients (3e) are well-behaved; however, the whole input space
gets progressively more squeezed at deeper layers (3a). Theorem 1 predicts that this happens at the
rate at least
1 _ n ≤ _____________4_________ ɪ
1 -Pmax ≤ - √5 — 3)√5	√L
(4.2)
The compression of the preactivation range does not happen in chaotic initialization. As shown on
figure 3b, the correlation of preactivations on any pair of different inputs approaches ρfp < 1, i.e. the
fixed point of the function P defined in 2.3. With the approximation ρmax ≈ ρfp, theorem 1 predicts
1/(arctan ɪ — 3 ) √5	I_
≥ -----------4--------- ∙ (1 - Pfp) ∙ √L
op
(4.3)
As shown in 3d, norm of the Jacobian indeed grows, causing large gradients in low layers (figure 3f).
(b) (No) shrinkage, chaotic
largest
smallest
SrnaIlaSt
(c) Jacobian, critical
(d) Jacobian, chaotic
ILUOUr<υ一 PPJb
(e) Gradients, critical
10310210ll()θ
UJ-OUt@pP.Ib
(f) Gradients, chaotic
Figure 3: Representation shrinkage, singular values of the Jacobian and gradient propagation in deep
erf networks. Bounds in figures 3a and 3f are the predictions of inequalities 4.2 and 4.3 respectively.
4.2 Removing redundant layers
Here we compare the training of a full deep network, and a “skimmed” network obtained by replac-
ing a block of middle layers with a single linear transformation. We hypothesize that since all inputs
7
Under review as a conference paper at ICLR 2022
to the middle block lie in a small region, it can be replaced with its linearization without significantly
affecting the network outputs.
Figure 4 compares training of full and skimmed networks on MNIST. Full networks has 82 critically-
initialized layers, alternating between ReLU and erf; “skimmed” one has 67 layers (a block of 16
is linearized). At initialization, preactivations entering the middle block have correlation at least
≈ 0.97, on any possible pair of inputs; as shown on figure 4a, this does not change significantly
after training. Figure 4c shows that their performance improves at similar rates, suggesting that the
full and skimmed models follow similar trajectories. We show Pearson correlation between total
displacements of the parameters in each layer on figure 4b. It is large (up to 0.8) further from
the linearized block, suggesting that these layers learn similar features; this is smaller (≈ 0.1) but
positive in layers closer to the linearized block, suggesting some but not complete overlap.
5 Discussion
5.1	Theoretical and practical implications
We have demonstrated a problem with increasing the depth of infinitely wide fully-connected net-
works. It holds quite generally and appears to be a universal property of the fully-connected ar-
chitecture rather than a particular class of activation functions or initialization schemes. It also
demonstrates that critical initialization alone does not eliminate all the problems with signal propa-
gation.
Our result demonstrates that vanilla MLPs are not the right model for networks of large-but-
comparable depth and width. If L and Nl grow simultaneously, then the first block of log L layers
is already problematic and the function PlogL(P0) (as in figure 1) becomes pathological. We cannot
avert it by using critical initialization or mixing different layer hyperparameters. This also hints that
“weakly nonlinear networks” might make more sensible models in this setting.
Theorem 2 suggests that for ResNet initialization, breaking the symmetry between parameters is
more important than specifying a good prior. From the theory side, We can deduce that well-be-
havedness of deep ResNets requires scaling the weight variance as L-1, agreeing with Hayou et al.
(2021). From the practical side, our result suggests choosing the variance below the threshold at
which theorem 2 becomes equality for acceptable values of Il d∂Z(0)∣∣op and Pmaχ.
In our derivation we assumed that all affine layers are followed by layer normalization. This sim-
plifies the final result but does not restrict generality - ina network in which pointwise nonlinearity
follows an affine map, we can insert layer normalization between them and rescale the activation
function by the average norm of preactivations. This construction requires knowing preactivation
norm in a network without layer normalization, which was studied e.g. in Poole et al. (2016).
One can note that O(L-2) rate of decay of 1 - PL in equations 4.1 and 4.2 is worse than asymptotic
O(L-2) for ReLU and O(L-1) for erf. Moreover, the right-hand side of inequality 1 is additive
and not multiplicative. This raises the suspicion that our result can be improved. We leave the
investigation of sharper bounds for future work.
5.2	Remedying strategies
The proof of theorem 1 shows that to avoid representation shrinkage we somehow need to break the
convexity of P -functions. Mixing activation functions cannot completely cancel out each other’s un-
desirable behaviour, because each one only “adds convexity”. It also highlights a problem common
to layer normalization and self-normalizing networks - those methods only normalize the diago-
nal of the covariance matrix of a data batch. This ensures O(1)-scaling of preactivations, but off-
diagonal behaviour may still cause representation shrinkage or gradient explosion. It was showed
in Yang et al. (2019) that batch normalization is not a complete solution either - it necessarily leads
to gradient explosion (which is slowest with linear activations).
8
Under review as a conference paper at ICLR 2022
Our result demonstrates limitations of normalization methods and critical initialization in improving
the trainability of deep networks, and suggests using trainable nonlinearities, initialized as linear
functions at the start of the training (similarly to the idea from He et al. (2015)).
(a) Shrinkage in the full model before and after training. Empirical in red, infinite-width
9
Under review as a conference paper at ICLR 2022
References
Lenalc ChizaL EdoUard Oyallon, and Francis Bach. On lazy training in differen-
tiable programming. In Advances in Neural Information Processing Systems, vol-
Ume 32, 2019. URL https://proceedings.neurips.cc/paper/2019/file/
ae614c557843b1df326cb29c57225459- Paper.pdf.
Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper Understanding of neUral networks:
The power of initialization and a dUal view on expressivity, 2017. URL https://arxiv.
org/abs/1602.05897.
Alexander G. de G. Matthews, Mark Rowland, Jiri Hron, Richard E. TUrner, and ZoUbin Ghahra-
mani. GaUssian process behavioUr in wide deep neUral networks, 2018. URL https://arxiv.
org/abs/1804.11271.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neUral
networks oUtperform kernel methods? CoRR, abs/2006.13409, 2020. URL https://arxiv.
org/abs/2006.13409.
Boris Hanin. Random neUral networks in the infinite width limit as gaUssian processes, 2021. URL
https://arxiv.org/abs/2107.01562.
Boris Hanin and Mihai Nica. Finite depth and width corrections to the neUral tangent kernel. In
International Conference on Learning Representations, 2020. URL https://openreview.
net/forum?id=SJgndT4KwB.
Boris Hanin and David Rolnick. How to start training: The effect of initialization and ar-
Chitecture. In NeurIPS, pp. 569-579, 2018. URL http://papers.nips.cc/paper/
7338-how-to-start-training-the-effect-of-initialization-and-architecture.
SoUfiane HayoU, ArnaUd DoUcet, and JUdith RoUsseaU. On the impact of the activation fUnction on
deep neUral networks training. In Proceedings of the 36th International Conference on Machine
Learning, volUme 97 of Proceedings of Machine Learning Research, pp. 2672-2680, 09-15 JUn
2019. URL https://proceedings.mlr.press/v97/hayou19a.html.
SoUfiane HayoU, EUgenio Clerico, Bobby He, George Deligiannidis, ArnaUd DoUcet, and JUdith
RoUsseaU. Stable resnet. In Arindam Banerjee and Kenji FUkUmizU (eds.), Proceedings of The
24th International Conference on Artificial Intelligence and Statistics, volUme 130 of Proceedings
of Machine Learning Research, pp. 1324-1332. PMLR, 13-15 Apr 2021. URL https://
proceedings.mlr.press/v130/hayou21a.html.
Kaiming He, XiangyU Zhang, Shaoqing Ren, and Jian SUn. Delving deep into rectifiers: SUrpassing
hUman-level performance on imagenet classification. In Proceedings of the IEEE International
Conference on Computer Vision (ICCV), December 2015.
Kaiming He, XiangyU Zhang, Shaoqing Ren, and Jian SUn. Deep residUal learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), JUne 2016.
Lei HUang, Jie Qin, Yi ZhoU, Fan ZhU, Li LiU, and Ling Shao. Normalization techniqUes in training
dnns: Methodology, analysis and application. CoRR, abs/2009.12836, 2020. URL https:
//arxiv.org/abs/2009.12836.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
redUcing internal covariate shift. In ICML, pp. 448-456, 2015. URL http://proceedings.
mlr.press/v37/ioffe15.html.
Arthur Jacot, Clement Hongler, and Franck Gabriel. Neural tangent ker-
nel:	Convergence and generalization in neUral networks. In NeurIPS,
pp. 8580-8589,	2018.	URL http://papers.nips.cc/paper/
8076- neural- tangent- kernel- convergence- and- generalization- in- neural- networks.
10
Under review as a conference paper at ICLR 2022
Gunter Klambauer, Thomas Unterthiner, Andreas Mayr, and SePP Hochreiter. Self-normalizing
neural networks. In NIPS, pp. 972-981, 2017. URL http://papers.nips.cc/paper/
6698-self-normalizing-neural-networks.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer Normalization. arXiv e-prints,
art. arXiv:1607.06450, July 2016.
Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of lin-
ear regions of deep neural networks. In Advances in Neural Information Processing Systems,
volume 27. Curran Associates, Inc., 2014. URL https://proceedings.neurips.cc/
paper/2014/file/109d2dd3608f669ca17920c511c2a41e-Paper.pdf.
Radford M Neal. Priors for infinite networks. In Bayesian Learning for Neural Networks, pp. 29-53.
Springer, 1996.
Ryan O’Donnell. Analysis of boolean functions. CoRR, abs/2105.10386, 2021. URL https:
//arxiv.org/abs/2105.10386.
Stefano Peluchetti and Stefano Favaro. Infinitely deep neural networks as diffusion processes. In
Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics,
volume 108 of Proceedings of Machine Learning Research, pp. 1126-1136. PMLR, 26-28 Aug
2020.
Stefano Peluchetti and Stefano Favaro. Doubly infinite residual neural networks: a diffusion process
approach. Journal of Machine Learning Research, 22(175):1-48, 2021. URL http://jmlr.
org/papers/v22/20-706.html.
Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep learn-
ing through dynamical isometry: theory and practice. Advances in neural information processing
systems, 30, 2017.
Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. The emergence of spectral universality
in deep networks. In Amos Storkey and Fernando Perez-Cruz (eds.), Proceedings of the Twenty-
First International Conference on Artificial Intelligence and Statistics, volume 84 of Proceedings
of Machine Learning Research, pp. 1924-1932. PMLR, 09-11 Apr 2018. URL https://
proceedings.mlr.press/v84/pennington18a.html.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Gan-
guli. Exponential expressivity in deep neural networks through transient chaos.
In Advances in Neural Information Processing Systems, volume 29. Curran Asso-
ciates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/file/
148510031349642de5ca0c544f31b2ef-Paper.pdf.
Daniel A. Roberts, Sho Yaida, and Boris Hanin. The Principles of Deep Learning Theory. arXiv
e-prints, art. arXiv:2106.10165, June 2021.
Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information
propagation. In International Conference on Learning Representations, 2017.
Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Chen Change Loy, Yu Qiao,
and Xiaoou Tang. Esrgan: Enhanced super-resolution generative adversarial networks, 2018.
URL https://arxiv.org/abs/1809.00219.
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S. Schoenholz, and Jeffrey Penning-
ton. Dynamical isometry and a mean field theory of cnns: How to train 10, 000-layer vanilla con-
volutional neural networks. In ICML, pp. 5389-5398, 2018. URL http://proceedings.
mlr.press/v80/xiao18a.html.
Lechao Xiao, Jeffrey Pennington, and Samuel S. Schoenholz. Disentangling trainability and gen-
eralization in deep learning. CoRR, abs/1912.13053, 2019. URL http://arxiv.org/abs/
1912.13053.
11
Under review as a conference paper at ICLR 2022
Ge Yang and Samuel Schoenholz. Mean field residual networks: On the edge of
chaos. In Advances in Neural Information Processing Systems, volume 30. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
81c650caac28cdefce4de5ddc18befa0- Paper.pdf.
Greg Yang, Jeffrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, and Samuel S. Schoenholz. A
mean field theory of batch normalization. In International Conference on Learning Representa-
tions, 2019. URL https://openreview.net/forum?id=SyMDXnCcF7.
A Proof details
A.1 Proof of theorem 1
We will analyse the propagation of preactivations for two arbitrary data points xα , xβ . Following
equation 2.2, denote the cosine similarity of preactivations at l-th layer as ρι = z—(XaN-(xβ). Let
Pl describe the effect of l-th layer on correlation, as defined in equation 2.3. Then for layers k < l
we have
Pl = Pl ◦ Pl-1 ◦•••◦ Pk+2 ◦ Pk+l(ρk)
Here We use a “physical" argument to relate Pι with the Jacobian. For mathematically rigorous
discussion see section A.1.1. If we let one input approach the other xβ → Xα, then
limsup N2	= Il 瑞(Xa)∣∣
Xe →Xa	N0 2 kXa-Xβ k	op
On the other hand, this scenario is equivalent to ρ0 → 1, and
NL 1 k z(L)(Xa)-Z(L)(Xe )∣2 = 2 —2ρL = I-PL◦…◦ P1 (P0 )	ρ0 →1
N-1 k Xa-χβ k2	2-2p0	1-p0
Therefore the Jacobian has norm at least
ll ∂^lop
≥ (PL ◦…。Pl)0(1)
(PL。…。Pl)0(1)
(A.1)
Maximal correlation can be expressed using Pl as
ρmax = max min Pl 。 . . . P1 (ρ)
max	1≤l≤L ρ∈[-1,1]
Let us replace this quantity with something more convenient. Define pι = Pι。•…。Pι(0) and
Pmax = maxι≤ι≤L Pι. By observation 1, Pι are power series with all coefficients non-negative, so
for ρ ∈ [0, 1] we have
Pι。…。Pι(ρ) ≥ Pι。…。Pι(0) = Pι
Pι。…。Pi (-ρ) ≥ 2Pι。…。Pι(0) - Pι。…。Po(ρ) ≥ 2pι - 1
Therefore ρι ≥ 2pι — 1, and 1 一 Pmax ≤ 2(1 一 Pmax).
If we restrict our attention to P ∈ [0,1], then Pι。。…。Pi takes values in [pι, 1], so Py “encounters
J
values only from this inverval”. Therefore, let us “crop” all the P’s to functions P : [0, 1] J⊃ by
defining
Pι(t) =f Pl(PlT+11-PlT)t)-ρl	so that	Pι(Pι-i + (1 — Pι-i)t) = PI + (1 — Pι)Pι(t)
Then the Pι,Pι are related by affine transforms, domain and codomain of Pι is [0,1] and it satisfies
~ , ~ ,
Pι(0) =0,Pι(1) = 1. Then we have
Pι。…。Pk+ι	(Pk	+ (1 —	Pk)t)	=	PI	+ (1 —	PI)	∙	PI。…。Pk+i(t)	(A.2)
and as a consequence
(PL。…。Pi)0(1) = (1 — PL)(Pl。…。Pi)0(1)	(A.3)
12
Under review as a conference paper at ICLR 2022
-c-r Tl	1	.∙ .	.	, 1 Γ∙	IAC	, 1	. T-l ∙	. 1	a	F，，zɔ	∙	(~
When we substitute the formula 2.3, we see that Pl is exactly a “cropped” Gaussian covariance of
the activation function, i.e.
D (-l-∖ — C[φllpl-1+(I-Pl-1)t]-C[φlpl-1
Pl(t) =	C[φι ∖^]-C[φι∖ρi-ι]
We will now make use of the excess convexity e. It is a functional that takes convex functions on
[0, 1] and returns real numbers. Its definition and all the necessary proofs are in appendix A.3. All
we need to know in this proof is that it satisfies inequalities A.4, A.5 and A.6, which follow directly
from its properties 4, 5 and 7.
The steepness estimate (property 4) states
(PL ◦…◦ Pi)0⑴ ≥ 1 + e(pL ◦…◦ Pi)	(A.4)
The superadditivity with respect to composition (property 5) states
L
e(pL ◦ ... Pi) ≥ X e(Pι)	(A.5)
l=i
Finally, applying the “cropping” property 7 yields
e(Pl) ≥ (I-PI-lφ⅞-φlkN	(A.6)
This is all we need from e to complete the proof. Combining inequalities A.4, A.5 and A.6 gives
L
(PL ◦•••◦ Pι)0(1) ≥ e (PL ◦•••◦ Pi) ≥ 1-Pmax X ⅛⅛kN
l=i ι N
Reexpressing LHS in terms of the Jacobian norm from equation A.1 and remembering 1 - ρDmax ≥
I-Pmax and equation A.3 We arrive at the desired inequality.
Remark 2. So presented proof omits some details needed to construct the upper bound on PL ◦
∙∙∙ ◦ Pi presented on figure 1 (blue line). This bound can be derived by combining equations A.5
andA.6, and unpacking the definition of c(Pl ◦•••◦ Pi) asfollows.
We have “cropped” thefunction PL ◦•••◦ Pi according to equation A.2
_ _ , . ， . ~ ~ ,.
PL ◦…◦ Pi(t) = PL + (1 - PL)PL ◦…◦ Pi(t)
such that PL ◦•••◦ Pi (0) = 0. Then it is enough to take care of PL ◦•••◦ Pi. Equations A.5
and A.6 provide a bound on the excess convexity of PL ◦•••◦ Pi
L _
e(PL ◦…。Pi) ≥『X kφ⅛φN2N
l=i
Using the property 1of excess convexity we can bound PL ◦•••◦ Pi from above by a piecewise lin-
ear function (this g^ves exactly the bound on figure 1). The proof of this property and an intuitive
interpretation is presented in appendixA.3.
Remark 3. Having arrived at equation A.3, we could have rewritten it using chain rule and bound
each Pl0(1) straight away. There are two reasons for using the excess convexity. First, it describes
PL ◦•••◦ Pi on the whole of [0,1] and not only in the neighbourhood of 1. Knowing that e(PL。…。
Pi) is large allows us to produce the blue upper bound on figure 1, and thus explain why PL is nearly
constant for large range of P0, and rapidly jumps to 1 when the data points are very close. Second,
it rules out the situation where the pathological behaviour of PL。•一。Pi would be restricted to
some (1 - , 1) with → 0 as L grows.
13
Under review as a conference paper at ICLR 2022
A.1.1 Justification for equation A.1
To arrive at equation A.1 We the swapped the order of limits limnι→∞ and lim SuPxG →χα. Here We
give a rigorous treatment without this exchange. This will require some understanding of the excess
convexity e.
Denote E = 1-ρmax PL=I k：l/j2kN, and pick 0 < τ ≤ 2+1E. Equations A.5 and A.6 together imply
that e(pL◦ •••◦ R) ≥ E. From property 1 of excess convexity e it follows that
~ ~ . _________________ . 一
PL ◦…。Pι(1 — T) ≤ (1 + E)(1 — T) — E
= 1 - (1 + E)τ
plugging the relation between Pl and Pl from equation A.2 gives
(A.7)
PL。…。PI(I — T) ≤ ρmax + (1 — PmaX)(I -(I + E)T)
=1 — (I — Pmax)(I + E)T
Now, take any xα, xβ satisfying kxɑk2 = IlxeIl2 =	no, XaXe = 1 — T. Consider no
n-1kz(L[(χa)-z(L)(Xβ)k2 n0-1 kxα -xβ k2	1- N(L)(Xa).z(L)(Xe ) 一 一	nL	一 1 xa .xe n0
As nι,...,nL → ∞, this converges to
1-Pl。…。Pι( XoF ) x xa .χe	- 1	 n0	_ 1 — Pl o∙∙∙oPi (1-τ) T
Convergence in probability was proved in Hanin (2021), while almost sure convergence for sig-
moidal9 or ReLU activations can be deduced from Daniely et al. (2017). Remembering equation A.7,
this quantity is at least
1-PL° ,TP1(1-T) ≥ (1 — Pmax)(1 + E)
Therefore as nι,...,nL → ∞ we have
让吧:)-«俨)k2 ≥ (1 - Pmax)E
n0- kxα-xβ k2	max
(A.8)
By mean value theorem, there exists a point Xγ on the line segment joining xa, χβ satisfying
n- 1 kz(L)(xɑ)-z(L)(xβ )k
-T
n0 2 kxα-xβ k
Combining this with the estimate A.8 yields
dz(L) (T )l∣2 ≥ 1-ρmax E — (I-Pmax)2
∂z(0) (XY)∣∣op ≥	2 E =	8
L _
X kΦι-ΦιkN
乙 kΦlkN
l=1
(A.9)
Let us summarize the precise formulation. We have actually demonstrated two very similar state-
ments that differ by the exact interpretation of convergence. Using the result of Hanin (2021), we
obtain: on every line segment joining Xa ,xg (that satisfy Ilxall2 = Ilxell2 = no and 1 > Xnxe ≥
2+E), with probability tending to 1 as nι,...,nL → ∞, we can find a point xγ satisfying the
inequality A.9. Employing the result of Daniely et al. (2017), the precise statement becomes: with
sigmoidal9 or ReLU activation, on every line segment joining xa, xβ (that satisfy IxaI2 = Ixβ I2 =
no and 1 > Xanxe ≥ 21+E), almost surely, for sufficiently large n∖,...,nL,we can find a point xγ
satisfying the inequality A.9.
9Satisfying kφlk∞, kφ0lk∞, kφ0l0k∞ < CkφlkN for a (common) constant C
14
Under review as a conference paper at ICLR 2022
Remark 4. The proof is simpler if in the statement of theorem 1 we replace the square of Jacobian
norm on the LHS by its infinite-width analogue
KL (Xa ,Xa ) + KL (Xe ,Xe ) - 2KL (Xa ,Xe )
P K0(Xa,Xa)+K0(Xe,Xe)-2K0(Xa,Xe)
Xe →Xa
where Ki is the NNGP kernel (de G Matthews et al., 2018)
nl
Ki(Xa,xβ) =	lim	n X Z(I)(Xa)Z(I)(Xe)
n1 ,...,nl →∞ l
i=1
A.2 Proof of ResNet variant 2
The proof follows the same strategy as in the fully-connected case: we write down Pl , “crop” it and
recenter to [0, 1]2, and apply the excess convexity functional.
We can work out the covariances
Y (I)(Xa).Y (I)(Xe )
kY (I)(Xa)IHY (I)(Xe )k
y(l (Xa).y(l (xβ)
ρl = Pl (ρl-1)
n
Z(I)(Xa).z(I)(Xe )
n
ρl-1
Pl-1 + σ2i)C[φl |Pl-i]
Pl-ι+σ2l)C[φl∣ρl-ι ]
1+σ2l)kφlkN
(A.10)
Where Pl in equation A.10 again governs the change of correlation after l-th layer. Introduce Pl
_ _ , . . ~ ,.
Pi。∙∙∙。Pι(0) and Pi(t)
have
Pl(Pl-1 + (l-P1-1)t)-P1
∂z(L)
∂z(O)
1-Pl
2
. Similarly as in the fully-connected case, we
(1 - PL)(PL ◦…。P)⑴
(A.11)
op
Using properties 4 and 5 of excess convexity e from appendix A.3 we can get the bound
L
(PL。…。Pι)0(1) ≥ e(pL。…。Pl) ≥ X e(Pi)
l=1
(A.12)
〜
〜
Now we only need a bound on e(Pι). Substituting the equation A.10 to the definition of Pi yields
Pl(t) = 1-⅛⅛At + 1-ρlσ⅛l) A g(t)
(l)	(l)
where
A = ι∣φιkN - C[0i|Pi-i]
g(t)
C[φl∣Pi-ι + (1-Pi-ι)t]-C[φl∣Pl-ι]
A
Now we bound the excess convexity ofPi. From property 8 we can deduce
e(P) ≥ σ2l)A∙e(g)
"≥ 1-Pl-ι+σ2l)A
(l)
And property 7 states
e(g) ≥
(I-Pl-I)2 kφl-φl kN
Which gives
^2A
≥ σ2l)(1-ρl-ι)2kφl-φlkN
—2 (ι-ρl-1+σ2l) A)
Finally We use A ≤ (1 - pι-ι)kφ0∣∣N to obtain
(p )、σ2l)(I-PlT)kφi-φl kN
e' l) -	2(1 + σ2l)kφlkN)
Combining this with equations A.11, A.12 and 1 - Pmax ≤ 2(1 - Pmax) we get
得L ≥
(1-Pmax)2 X σ2l)kφl-φ7kN
—≡N
15
Under review as a conference paper at ICLR 2022
A.3 Excess convexity
Here we define the excess convexity functional and prove its properties. We will be working with
non-decreasing convex functions g : [0, 1] J⊃ satisfying g(0) = 0, g(1) = 1. Note that these condi-
tions imply continuity.
Each g satisfying these conditions must have an argument t ∈ (0, 1) for which
g(1 - t) = t. This follows from Darboux property ofg(1 - x) - x; pictorially,
the graph of g must intersect the line x + y = 1. We then define the excess
convexity of g as
e(g) = t - 2
It turns out that despite a seemingly arbitrary definition, excess convexity possesses a number of
nice properties:
!-ɪ t	if t ≤ 1 - ɪ
1+C	2+C
(1 + C )t - C if t ≥ 1 - 2 + C
Proof. The condition e(g) ≥ C means that the point 2+^ (1 + C) lies above the graph of g.
Therefore the line segments joining it with 0 and ( ； ) lie entirely above the graph of g.	□
Property 2 (Non-negativity). e(g) ≥ 0, with equality only for g = id[0,1]
Proof. By convexity g(2) ≤ 2 (g(0) + g(1)) = 2. Since g(t) +1 is increasing, we must have t ≤ 2
and hence t - 2 ≥ 0. Strict inequality for g = id[0,i] follows from property 3 below.	□
Property 3 (Coerciveness). We have the inequality ∣∣g - id[0,i] k∞ ≤ 1+^)
Proof. Essentially, this proof boils down to the graph of g lying in the shaded area.
If x ≥ 1 - t then convexity applied to arguments 0, 1 - t, x gives
t = g(1 - t) ≤ x-χ+tg(0) + 1x-tg(χ)	⇒ g(x) ≥ i-t ∙ x
So |x - g(x)∣ ≤ 1 - ι-t = 1--2t. Similarly, for X ≤ 1 -1 We apply convexity
to arguments x, 1 - t, 1 and obtain
x ≥ g(x) ≥ max {0, 2t-1+(1-t)x O
so again |x - g(x)| ≤ 1-2t for X ∈ [0,1 - t]. This means that
1-t
|x - g(x)∣ ≤ ⅛≡2t = 1+gg)	for all X ∈ [0,1]
□
Property 4 (Steepness estimate). Ifg is differentiable then g0(1) ≥ 1 + e(g)
Proof. By mean value theorem, there exists s ∈ (1 - t, 1) satisfying
0 g⑴-g(I - t
g (S) =	1-(1- t)
1 -1
t
1 + e(g)
Since g is convex, its derivative is non-decreasing so g0(1) ≥ g0(s).
□
Property 5 (Superadditivity with respect to composition). For any two functions g1, g2 satisfying
the conditions of this subsection we have e(g2 ◦ g1) ≥ e(g2) + e(g1)
16
Under review as a conference paper at ICLR 2022
Let us first present the pictorial idea behind the proof. We know we can find ti
with gi (1 - ti) = ti. We examine the points
By construction, they lie on the graph ofg2 ◦g1. We intersect the segment P1P2
with the line x + y = 1 to get Q. Since g2 ◦ g1 is convex, Q lies above the
graph of g2 ◦ g1 and hence gives a lower bound on e(g2 ◦ g1).
Proof. Now we will make this hand-wavy proof rigorous. We start by a statement about the location
of P1 . By convexity of g2 we have
g2 ◦ g1 (I - 11)=	g2(t1) ≤	1¾2	∙	g2 (I - t2)	+	11--t2	∙	g2(O) =	1-22	(A.13)
ti < 2, so the weights are non-negative. We can get an analogous inequality for P2： using concavity
of g1-1 we get
g-1 ◦ g-1(t2) = g-1(1 -12) ≥ 1⅛ ∙ g-1(tι) + ⅛τ2 ∙ g-1 (1) = 1 -隹
g2 ◦ gι(1 -线)≤ t2	(A.14)
It is straightforward, if somewhat tedious, to check that the right (i.e. giving rise to something of the
form g2 ◦ g1(1 - t) ≤ t) weights to combine inequalities A.13, A.14 are given by
W _	(I - 2t1 "2(I —12)
(1 —11-t2)(t1+t2-2t1t2)
W =	t1(I—tI)(I — 2t2)
(1 -11-t2)(t1+t2-2t1t2)
(A.15)
Adding inequalities A.13 with weight w1 and A.14 with weight w2 yields
g2 ◦ gl(1 - Wltl - W21-1) ≤ Wl ∙ g2 ◦ gl(1 - ti) + W2 ∙ g2 ◦ gι(1 - ⅜⅛1) ≤ wilt—⅜ + W2t2
After substituting the weights from A.15 this simplifies to
g2 ◦ gi (1 - t1+t21-2t1t2 )
≤ t1t2
-t1 +t2 -2t1 t2
Therefore
e(g2 ◦ gI) ≥ (t1+t21-22t1t2)	- 2 = TT + t12 - 4 = e(gI) + e(g2)
□
Property 6 (Cropping a power series). Take a power series g(x) = k∞=0 ckxk with all coefficients
non-negative ck ≥ 0. Let us “crop” it to the rectangle [a, b] × [g(a), g(b)] and “recentre”; formally,
define
g(S)=g(a+(b--g(a-g(a)	s° that	g(a+(b-	a)s)=g(a)+(g(b)-	g(a))∙	g(S)
then e(g)(2+e(g)) ≥ (b-a)2 P∞ C bk-2
then	1+e(g)	≥ g(b)-g(a) ”=2 Ckb	-
Proof. Let t = 2+1(J) so that g(1 -1) = t. By the definition of g
tg(a) + (1 - t)g(b) - g(ta + (1 - t)b) = (g(b) - g(a))(1 -1 - g(1 - t))=
=(g(b) -g(O))(I- 2t)=
=(g(b) - g(a)) 2+(gg)	(A.16)
On the other hand, expanding g yields
∞
tg(a) + (1 — t)g(b) — g(ta + (1 — t)b) = ^X Ck [tak + (1 — t)bk — (ta + (1 — t)b)k]=
k=0
∞
=X Ckbk t(b) +(1—t) —(t ∙ b + 1 -t)	(A.17)
k=2
17
Under review as a conference paper at ICLR 2022
where the constant k = 0 and linear k = 1 terms cancel. Now, consider the function k 7→ sxk -
(sx + 1 - s)k for some s, x ∈ [0, 1]. We will prove that it is non-decreasing on [1, ∞). Its derivative
with respect to k is
-Sxk log 1 + (sx + 1 - s)k log sx+ι-s ≥
≥ -Sx(Sx + 1 - s)k-1 log 1 + (sx + 1 - s)k log sx+ι-s =
=(sx + 1 — s)k-1 [s ∙ xlogx + (1 — S) ∙ 1 log 1 — (sx + 1 — S) Iog(Sx + 1 — s)]
which is non-negative by Jensen’s inequality for x log x. This allows to replace all brackets in A.17
with their values at k = 2, giving
∞∞
X Ck bk [t( a )fc + (I-1) -(t ∙ a + 1 -t)fc ] ≥ X Ck bk [t( a )2 + (I-1) -(t ∙ a + 1-tH =
k=2	k=2
∞
=t(1 - t)(1- a)2 X Ckbk
k=2
Substituting t = 2+1(§)and comparing to equation A.16 yields
∞
(g(b) - g(a)) 2+⅛⅛ ≥ (21+eg)))2 (b - a)2 X ckbk-2
k=2
which is equivalent to the desired inequality.	口
Property 7 (Cropping a Gaussian covariance). Suppose we “crop” a Gaussian Covariance C[f ∣∙]
to [r, 1] by defining
X(t) _ C[f|r+(1-r)t]-Cf |r]
g(t) = 一CfI-WH—
Then the excess convexity of g is at least
e⑼≥
(1-r)2kf -fkN
2(kfkN-Cf∣r])
(1-r)kf -fkN
2kf 0kN
≥
Proof. Applying property 6 with a = r, b = 1 gives
∞
2 (方)≥ e(g)(2+e(g)) ≥	(1-r)2	X ^2
2e(g) ≥ —ι+e(g)	≥ CwFCfM Lfk
k=2
Where f = P∞=0 fkhk is the Hermite expansion of f. Notice that the sum is simply Pk=2 fk =
kf - f kN, which completes the proof of the first inequality.
To get the second one We only need to bound the denominator. Convexity of C[f ∣∙] implies that
C[f∣1]- C[f∣r] ≤ (1 - r) ∙ dCfML=I
From Stein,s lemma we can deduce ∂ρC[f ∣ρ] = C[f0∣ρ], so dC∂,ρ]	= kf0∣∣N.	口
Property 8 (Combining with identity). For p, q ≥ 0, p + q = 1 and a function g we have
e(p ∙ id[o,i] +q ∙g) ≥ ι+p(eg(g)
Proof. Set g(1 - S) = S, p(1 - t) + qg(1 - t) = t. Write
g(1 - t) - t = q(2t - 1) ≤ 0 = g(1 - S)- S
and since g(1 -x) -x is decreasing, we must have 1 -t ≤ 1 - S. This allows deduce from convexity
of g that g(1 -1) ≤ 1-∣g(1 - s) + 1-1 g(0) = s(1-). Therefore
t - P + Pt = qg(1 -1) ≤ q ∙ s⅛≡τ
18
Under review as a conference paper at ICLR 2022
Substituting ι-s = "1⑺ and rearranging yields
t ≤	1+peS)
-2+e(g)+pe(g)
which is equivalent to
e(p ∙ id[o,i] + q ∙ g)
1 _ 2 ≥ qe(g)
t 1 1+pe(g)
□
B Hermite expansions of common activation functions
Leaky rectified linear unit with negative slope a is defined as LReLUa (X) = ax + (1 - a) ∙ ReLU(x),
and has the following
C[LReLUa∣ρ] = aρ +(1- a)2 不2+空-…P)
LReLU(X) = √⅛ + 1+a x
Therefore
IlLReLUa - LReLUallN =展(1 - a)2
IlLReLUallN = 1+a2
And the critical initialization requires σb = 0.
The error function is defined as erf(x) = √2∏ Rx e-y2 dy. It induces
C[erf∣ρ] = 2 arctan / 2ρ
LM π	√9-4ρ2
erf(X) = √π X
Therefore
kerf - erfkN = 2 "ctan √5 - 32
kerf0kN = π√5
And critical initialization happens when
2 = 2 (√5 - arctan √5).
19