Under review as a conference paper at ICLR 2022
Bandwidth-based Step-Sizes for Non-Convex
Stochastic Optimization:	Non-monotonicity,
Worst-case Convergence and Performance
Anonymous authors
Paper under double-blind review
Ab stract
Many popular learning-rate schedules for deep neural networks combine a decay-
ing trend with local perturbations that attempt to escape saddle points and bad
local minima. We derive convergence guarantees for bandwidth-based step-sizes, a
general class of learning-rates that are allowed to vary in a banded region. This
framework includes many popular cyclic and non-monotonic step-sizes for which
no theoretical guarantees were previously known. We provide worst-case guaran-
tees for SGD on smooth non-convex problems under several bandwidth-based step
sizes, including stagewise l/ʌ/t and the popular step-decay (“constant and then
drop by a constant”), which is also shown to be optimal. Moreover, we show that its
momentum variant converges as fast as SGD with the bandwidth-based step-decay
step-size. Finally, we propose novel step-size schemes in the bandwidth-based
family and verify their efficiency on several deep neural network training tasks.
1	Introduction
Stochastic gradient methods including stochastic gradient descent (SGD) (Robbins and Monro, 1951)
and its accelerated variants (e.g., SGD with momentum (Polyak, 1964; Sutskever et al., 2013)) have
become the algorithmic workhorse in much of machine learning. The step-size (learning rate) is the
most important hyper-parameter for controlling the speed at which gradient-based methods converge
to stationarity. For problems with multiple local minima, the step-size also affects which local
optimum the optimization process converges to. It therefore needs to be both well-designed and
well-tuned to make SGD and its variants effective in practice.
In the deep learning literature, cyclical step-sizes (Loshchilov and Hutter, 2017; Smith, 2017) and
non-monotonic schedules (Keskar and Saon, 2015; An et al., 2017; Seong et al., 2018; Loizou et al.,
2021) have attracted strong recent interest, with significant benefits for non-convex problems with
poor local minima or saddle points (Seong et al., 2018). Popular cyclical schedules include the cosine
step-size (cosine with restart) (Loshchilov and Hutter, 2017) and the triangular policy (Smith, 2017),
which have become the default choices in some deep learning libraries, e.g., PyTorch and TensorFlow
(cf. lr.scheduler.CyCIiCLR and CosineAnnealingLR). However, non-monotonic policies are much
more complex to analyze than decaying ones, and theoretical results for these non-monotonic policies
are scarce. This motivates us to focus on a bandwidth step-size framework, in which
mδ(t) ≤ ηt ≤ M δ(t)	(1)
for some boundary function δ(t) and positive constants m and M. This framework allows for non-
monotonic step-sizes and covers most of the situations discussed above. In particular, it includes the
cosine (Loshchilov and Hutter, 2017), triangular (Smith, 2017), sine wave (An et al., 2017) step-sizes
as special cases. The framework provides a uniform convergence rate guarantee for all step-size
policies which remain in the band (1). This gives a lot of freedom to design novel step-sizes schedules
with improved practical performance without loosing track of their theoretical convergence guarantee.
The generic bandwidth framework has recently been proposed by Wang and Yuan (2021), but they
only analyzed strongly convex problems. We believe that more significant potential lies in the
non-convex regime. For non-convex problems, non-monotonic step-sizes have distinct advantages,
helping iterates to escape local minima and producing final iterates of high quality. In the paper, we
1
Under review as a conference paper at ICLR 2022
demonstrate this point on both a simple toy example and on large-scale neural network training tasks.
Our main contribution is a sequence of non-asymptotic convergence results for the bandwidth step-
size on non-convex optimization problems, based on the popular “constant and then drop” step-size
schedules (Krizhevsky et al., 2012; He et al., 2016; Hazan and Kale, 2014; Ge et al., 2019; Wang
et al., 2021). This allows non-monotonic variations both within each (inner) stage and between stages.
1.1	Contributions
Inspired by the strong potential of non-monotonic step-size schedules demonstrated above, we extend
the bandwidth-based step-size framework to “constant and then drop” (multi-stage) profiles, where
the bands stay constant throughout each stage and drops between stages. We provide convergence
guarantees for both SGD and its momentum variant (SGDM) on non-convex problems. Specifically,
•	We establish worst-case theoretical guarantees for SGD with bandwidth step-size on smooth
nonconvex problems. We (i) derive an optimal rate for SGD under a bandwidth step-size
with δ(t) = 1/VI; (ii) and achieve optimal and near-optimal rates for step-decay (constant
and then drop by a constant), improving the results by Wang et al. (2021).
•	We provide worst-case theoretical guarantees for SGDM with bandwidth-based step-decay
step-size in the smooth non-convex setting. To the best of our knowledge, these are the
first results that provide optimal (Theorem 4.3) and near-optimal (Theorem 4.2) results
for momentum with step-decay step-sizes. Moreover, our results significantly improve the
convergence results from Liu et al. (2020) (see Remark 4.4).
•	Our analysis results also provide state-of-the-art theoretical guarantees for co-
sine (Loshchilov and Hutter, 2017) and triangular (Smith, 2017) step-sizes if their boundary
functions are within our bands. Especially, we improve the result of Li et al. (2021) for
cosine step-size and achieve a state-of-art rate (see Remark 3.4). Moreover, our results first
provide the convergence guarantees for triangular step-size (Smith, 2017).
•	We propose novel, possibly non-monotonic, step-size schedules (e.g., step-decay with linear-
mode and cosine-mode) based on the bandwidth-based framework and demonstrate their
efficacy on several large-scale neural network training tasks.
1.2	Related Work
This subsection reviews the theoretical development of the SGD algorithm and its momentum variant
in the smooth non-convex setting, with a special focus on different step-size policies.
SGD for nonconvex problems The first non-asymptotic convergence of SGD to a stationary point
of a general smooth non-convex function was established in Ghadimi and Lan (2013). The authors
proved that a constant step-size O(1/√T) attains a convergence rate of O(1/√T), where T is the
iteration budget. To the best of our knowledge, this rate is not improvable and was proven to be
tight up to a constant without additional assumptions (Drori and Shamir, 2020). For the 1∕√t decay
step-size, an O(ln T/√T) rate can be easily obtained from (Ghadimi and Lan, 2013). This rate can
be improved to the optimal by selecting a random iterate using weights proportional to the inverse of
the step-size (Wang et al., 2021). The sampling rule in (Wang et al., 2021) depends on the step-size
and is easily applicable to different step-size policies. Thus, in this paper, we choose a similar
sampling rule as (Wang et al., 2021) to favor the later iterates when selecting the output for SGD and
its momentum variant.
Step-decay step-sizes Recently, the theoretical performance of step-decay or stagewise strategies
has attracted an increasing attention due to their excellent practical performance (Yuan et al., 2019;
Ge et al., 2019; Chen et al., 2019; Li et al., 2021; Wang et al., 2021). For a class of least-squares
problems, Ge et al. (2019) established a near-optimal O(lnT/T) rate for the step-decay step-size (cut
by 2 every T/ log2 (T) iterates) and showed that step-decay can perform better than the polynomial
decay step-size. Stochastic optimization methods with stagewise step-sizes decaying as 1/t were
analyzed in Chen et al. (2019). A near-optimal rate for the continuous version of step-decay, called
exp-decay, as well as for cosine decay step-sizes under the Polyak-Lojasiewicz (PL) condition and
a general smooth assumption were established in Li et al. (2021). However, in the smooth case, to
achieve such results for exponential and cosine decay step-sizes, the initial step-size is required to
2
Under review as a conference paper at ICLR 2022
be bounded by O(1/√T). This is obviously impractical When the number of iterations T is large.
Near-optimal rates (up to lnT) of SGD with step-decay step-size in several general settings including
strongly convex, convex and smooth (non-convex) problems Were proved in Wang et al. (2021).
They also removed the restriction on the initial step-size for exponential decay step-sizes. Empirical
evidences have been given in (Wang and Yuan, 2021) that bandWidth-based strategies can improve
the performance of the step-decay step-size on some large scale neural netWork tasks. HoWever, no
theoretical guarantees for non-convex problems Were given.
SGD with momentum on nonconvex problems The momentum variant of SGD (SGDM) has been
Widely used in deep neural netWorks (Krizhevsky et al., 2012; Sutskever et al., 2013; He et al., 2016;
Zagoruyko and Komodakis, 2016). Due to its practical success on neural netWorks, its theoretical
performance is noW attracting a lot of interest, especially for nonconvex problems (Yan et al., 2018;
Gadat et al., 2018; Chen et al., 2019; Gitman et al., 2019; Mai and Johansson, 2020; Liu et al., 2020;
Defazio, 2020). Under the assumption of bounded gradients, Yan et al. (2018) proposed a unified
analysis framework for stochastic momentum methods and proved an optimal O(1/√T) rate under
constant step-sizes. A similar result for the Nesterov-accelerated variant Was established in Ghadimi
and Lan (2016). However, studies related to the multi-stage performance of SGD with momentum is
lacking and far from being complete. Reference Chen et al. (2019) considers a momentum method
with a stagewise step-size, but the method is a proximal point algorithm with extra averaging between
stages, and not the widely used momentum SGD considered here. More recently, Liu et al. (2020)
established the convergence for multi-stage SGDM and provided empirical evidence to show that
multi-stage SGDM is faster. However, their results require an inverse relationship between stage
length and step-size which limits the initial stage length or step-size. A detailed comparison with
(Liu et al., 2020) will be given in Section 4 (see Remark 4.4).
Organization: The rest of this paper is organized as follows. Notations and basic definitions are
introduced in Section 2. Our novel theoretical results for SGD and its momentum variant (SGDM)
under bandwidth-based step-sizes are introduced in Sections 3 and 4, respectively. Numerical
experiments are presented and reported in Section 5. Finally, Section 6 concludes the paper.
2	Problem Set-Up
We study the following, possibly non-convex, stochastic optimization problem
mRd f(x)= Esm[f(x； ξ)]
(2)
where ξ is a random variable drawn from some (unknown) probability distribution Ξ and f(x; ξ) is
the instantaneous loss function over the variable x ∈ Rd. We consider stochastic gradient methods
that generate iterates xt according to
xt+1 = xt - ηtdt
(3)
where ηt is the step-size and dt the search direction (e.g., dt = Vf (xt; ξ) for SGD). We assume that
there are constants m > 0 and M ≥ m, and two functions n(t) and δ(t): R → R such that such that
ηt = n(t)δ(t), ∀t ≥ 1,
where n(t) ∈ [m, M] and δ(t) is monotonically decreasing function satisfying δ(1) = 1. Note
that even though the boundary function δ(t) is monotonic, the step-size itself is not restricted to be.
Throughout the paper, we make the following assumptions:
Assumption 1. The loss function f satisfies kVf (x) - Vf (y)k ≤ Lkx-yk for every x, y ∈ dom (f).
Assumption 2. For any input vector x, the stochastic gradient oracle O returns a vector g such that
(a) E[kg - Vf(x)k2] ≤ ρ kVf (x)k2 + σ where ρ ≥ 0 andσ ≥ 0; (b) E[kgk2] ≤ G2.
3	Non-asymptotic Convergence of SGD with Bandwidth-based
Step-Size
In this section, we provide the first non-asymptotic convergence guarantees for SGD with bandwidth-
based step-sizes on smooth non-convex problems. The results consider a general family of bandwidth-
based step-sizes which includes the classical multi-stage SGD as a special case.
3
Under review as a conference paper at ICLR 2022
Algorithm 1 SGD with Bandwidth-based Step-Size
1:	Input: initial point x11, # iterations T, # stages N, stage length {St}tN=1 such that PtN=1 St = T,
the sequences {δ(t)}tN=1 and {n(t, i)}iS=t 1	∈ [m, M] with 0 < m ≤ M
2:	for t = 1 : N do	=
3:	for i = 1 : St do
4:	Query a stochastic gradient oracle O at Xt to get a vector gt such that E[gt | F↑] = Vf (Xty
5:	Update step-size ηit = n(t, i)δ(t)
6:	Xit+1 = Xit - ηitgit
7:	end for
t+1	t
8:	X1	= XSt+1
9:	end for
10:	Return: XT is uniformly chosen from {x； ,x2 , •一，XSt* }, where the integer t is chosen from
{1, 2,…，N} with probability Pt = δ-1(t)/(PN=I δ-1(l))
Algorithm 1 details our bandwidth-based version of the popular “constant and then drop” policy for
SGD. Here, the boundary function δ(t) is adjusted in an outer stage, and the length of each stage St is
allowed to vary. Similar to Wang et al. (2021), the output distribution depends on the inverse of δ(t),
hence puts more weight on the final iterates. By considering specific combinations of δ(t) and St,
this framework allows us to analyze several important multi-stage SGD algorithms, including those
with constant, polynomial-decay and step-decay step-sizes. For example, we consider the step-decay
step-size by letting n(t, i) = m and δ(t) = 1∕ɑt-1 where m denotes its initial step-size and α > 1.
Many interesting results on polynomial-decay step-size (e.g., δ(t) = 1/√t, we called it 1/√t-band)
are given in Appendix A.
3.1 Convergence Under Bandwidth Step-Decay Step-Size
Another important step-size is Step-Decay (“constant and then drop by a constant”), which is popular
and widely used in practice, e.g. for neural network training (Krizhevsky et al., 2012; He et al., 2016).
In this subsection, we analyze bandwidth step-sizes that include step-decay as a special case.
For Step-Decay, the stage length St is typically a hyper-parameter selected by experience. We first
analyze a bandwidth version of the algorithm analyzed in [Theorem 3.2](Wang et al., 2021), namely
Algorithm 1 with N = b(logα T)/2c outer loops where α > 1, each with a constant length of
St = d2T/ logα Te. The logarithmic dependence of N on T leads to a small number of stages in
practice, and was demonstrated to perform well in deep neural network tasks (Wang et al., 2021).
Theorem 3.1. Under Assumptions 1 and 2(a), and assume that there exists a constant ∆0 > 0 such
that E[f(x1) — f *] ≤ ∆o for each t ≥ 1 where f * = min f(x), ifwe runAlgorithm 1 with T > α2,
ηt ≤ 1∕((ρ +1)L), N = b(logɑ T )∕2C, St = d2T∕logɑ T] ,and δ(t) = 1∕αt-1 for 1 ≤ t ≤ N,
where α > 1 then
E[kVf(Xτ)k2] ≤
△0	aM2Lσ、(α — 1)
2αm	2m	ln α
ln T
√T — α
Theorem 3.1 establishes a near-optimal (up to lnT) rate for the step-decay bandwidth scheme which
matches the result achieved at its boundaries i.e., ηit = mδ(t) or ηit = Mδ(t) (Wang et al., 2021). As
the next theorem shows, this guarantee can be improved by appropriate tuning of the stage length St .
Remark 3.2. (Justification of uniformly bounds on the function values) In Theorem 3.1, we
require that the expectation of the function value at each outer iterate E[f (Xt1)] is uniformly upper
bounded. As shown by Shi et al. (2020), the function values at the iterates of SGD can be controlled
(bounded) by the initial state provided the step-size is bounded by 1/L. So the assumption is fair if
the initial state is settled. Nevertheless, this assumption (or its stronger version that the objective
function is bounded) is commonly used or implied in optimization (Hazan et al., 2015; Xu et al.,
2019b; 2020; 2019a) and statistic machine learning (Vapnik, 1998; Cortes et al., 2019) , and it has
never been violated in our numerical experiments.
1We use Fit to denote σ-algebra formed by all the random information before current iterate xit and xit ∈ Fit .
4
Under review as a conference paper at ICLR 2022
Theorem 3.3. Under Assumptions 1 and 2(a), and assume that there exists a constant ∆0 > 0 such
that E[f (x1) — f *] ≤ ∆o for each t ≥ 1, if we run Algorithm 1 With T > α2, ηi ≤ 1∕((ρ +1)L),
So = √T, St = dSoα(t-1)e and δ(t) = 1∕αt-1 where α > 1, then
E[kVf(Xτ)『]≤
T（ 2mo+等）√t+o（ t）.
Optimal rate for step-decay step-size The theorem shows that if the stage length St increases
exponentially, and the length of the first stage is set appropriately, then we can achieve an optimal
O(1∕√T) rate for the bandwidth step-decay step-size in the non-convex case. If M = m, which
means that the bandwidth scheme degenerates to the step-decay type step-size, Theorem 3.3 removes
the logarithmic term present in the results of Wang et al. (2021). To the best of our knowledge, this is
the first result that demonstrates that vanilla SGD with step-decay step-sizes can achieve the optimal
rate for general non-convex problems. The numerical performance of the two step-size schedules in
Theorems 3.1 and 3.3 are reported in Figure 4.
Benefits of Theorems 3.5 vs the references of Hazan and Kale (2014); Yuan et al. (2019) Another
commonly used step-decay scheme in theory which halves the step-size after each stage and then
doubles the length of each stage (e.g., (Hazan and Kale, 2014; Yuan et al., 2019)). In Hazan and
Kale (2014), which considers strongly convex problem, the initial stage is very short, S1 = 4, while
the analysis in Yuan et al. (2019) for PL functions use an inverse relation between stage length and
step-size, which means that a longer initial stage length requires a smaller stepsize. In contrast to
these references, Theorem 3.3 considers a step-decay with a long first stage, S1 = d√T], which
allows us to benefit from a large constant step-size for more iterations.
Remark 3.4. (Guarantees for cyclical step-sizes) In (Loshchilov and Hutter, 2017), the authors
decay the step-size with cosine annealing and use ηmt in < ηmt ax to control the range of the step-size.
If mδ(t) ≤ ηmt in, ηmt ax ≤ Mδ(t), then our results provide convergence guarantees for their step-size.
TO achieve a near-optimal rate, Li et al. (2021, Theorem 4) need to use an initial step-size that is
smaller than O(1∕√T) which is obviously impractical. In contrast, we allow the cosine step-size to
start from a relatively large step-size and then gradually decay (see Theorem 3.1) and also improve
the convergence rate to be optimal (Theorem 3.3).
A triangular cyclical step-size is proposed by Smith (2017) which is varied around the two boundaries
that drop by a constant after a few iterations. Our analysis first provides theoretical guarantees (e.g.,
Theorems 3.1 and 3.3) also for this step-size. The details are shown in Appendix D.2.
α 5 a 5 a
3 2 2 1 1
∙Z-SA2S
■■・,••• baseline
--一, upp«r-b«nd
-∙- cosine policy
O 50	1∞	15S	200	25β
# Iteration
0.0-
O 50 IOO 150	200	25β
# Iteration
Figure 1: Cosine step-size (left) and triangular step-size (right)

The bandwidth step-sizes we consider above are independent on the random information. In recent
years, some non-monotonic step-sizes have been proposed that are dependent on the current random
information, e.g., the trust-region-ish algorithm (Curtis et al., 2019) and stochastic Polayk step-
size (Loizou et al., 2021). We provide some interesting results for these step-sizes (see Lemma B.2 in
Appendix B).
4 Non-asymptotic Convergence of SGDM Under Bandwidth-based
Step-Size
In this section, we establish the first non-asymptotic convergence properties of SGD with momentum
(SGDM) under the bandwidth-based step-size on smooth nonconvex problems.
5
Under review as a conference paper at ICLR 2022
In this scheme, the inner iterations in Step 6 of Algorithm 1 are essentially replaced by
vit+1 = βvit + (1 - β)git	(4)
xit+1 = xit - ηitvit+1	(5)
for β ∈ (0, 1). We refer to Algorithm 2 in Appendix C for a more detailed description.
As in many studies of momentum-methods (e.g. Ghadimi et al. (2015); Yan et al. (2018); Liu et al.
(2020); Mai and Johansson (2020)), we establish an iterate relationship of the form E[Wt+1] ≤
E[Wt] - c0ηE[et] + c1η2, where Wt is a Lyapunov function, et is a performance measure (here,
et = ∣∣Vf (•)『),η is the step-size and c° and ci are constants. However, due to the time-dependent
and possibly non-monotonic bandwidth-based step-size, we cannot use the Lyapunov functions
suggested in Yan et al. (2018); Liu et al. (2020) but rely on the following non-trivial construction:
Lemma 4.1. Suppose that Assumption 1 and Assumption 2(b) hold. Let zit = (1 - β)-1(xit - β xit-1)
and assume that there exists a constant ∆° such that E[f (Xt) 一 f *] ≤ ∆0 for t,i ≥ 1 and the
step-size in each stage is monotonically decreasing. Define the function Wit+1
Wit+1 = f (Zz+「* + "匕一%2 + 2r[f (xt+ι) - f *],
ηi	ηi
where r = 2(i-&(i-β)2. Then, if ηz ≤ 1/L ,for any t and i ≥ 2, we have
E[Wit+1 | Fit] ≤ Wit + A1 f ^^t-1-) -IlVf(Xi)|『+ ηz ∙ BiG2.	⑹
ηi	ηi-1
where Ai = β∆0 + δz + rG-, Bi = r(1 - β)(2 - β) + 2(i-β)2 , and δz = 1δ0β + 2(1-β)2L.
Note that even though the step-size is assumed to be monotonically decreasing in each stage, it may
be increased between stages, leading to a globally non-monotonic step-size. The proposed bandwidth-
based step-sizes (e.g., step-decay with linear or cosine modes) in the numerical experiments and the
cosine annealing policy proposed in (Loshchilov and Hutter, 2017) all satisfy this condition. Note
that, unlike (Mai and Johansson, 2020; Liu et al., 2020), the momentum parameter β does not rely on
the step-size, but can be chosen freely in the interval (0, 1). In particular, our analysis supports the
common choice of β = 0.9 used as default in many deep learning libraries (Krizhevsky et al., 2012;
He et al., 2016). Similar to Remark 3.2, the function value of the iterates for momentum can also be
controlled (bounded) by the initial state given ηit ≤ 1/L; see (Shi, 2021). Therefore, we believe our
assumptions are reasonable.
If We restrict the analysis to a single stage, N = 1, the lemma allows to recover the optimal rate for
SGDM under the step-size ηi = ηo∕√T (Yan et al., 2018; Mai and Johansson, 2020; Liu et al., 2020;
Defazio, 2020) and to prove, for the first time, an optimal O(1∕√T) rate for SGDM under the 1∕√7
stepsize. These results are formalized in Appendix D.1.
4.1	Convergence of SGDM for Bandwidth Step-Decay Step-Size
We now show the convergence complexity of SGDM with the bandwidth step-decay step-size. Here
step-decay means that the bandwidth limits are divided by a constant after some iterations.
We first consider the total number of iterations T to be given, the stage length St to be constant, and
the number of stages N as a hyper-parameter.
Theorem 4.2.	Assume the same setting as Lemma 4.1. If given the total number of iterations T ≥ 1,
N ≥ 1, St = S = dT∕Ne, δ(t) = 1∕αt-i for each 1 ≤ t ≤ N and α > 1, then
E[∣Vf(Xτ)k2] ≤ TaNN∙
+ (aC0 + C2) ∙ N + …^ ∙ Na
Tm	T
NN
「+ MBiG2 ∙ L ⑺
m
where Co = r(GL—+ 2∆o), Ci = Ai + ∆z + 金》,and C2 = Co + A2G2, A2 = 1 + 2(i—e)2,
and Wii, Ai, Bi, ∆z, and r are defined in Lemma 4.1. Furthermore, ifN = b(logα T)∕2c and
St = d2T∕ logα Te for each 1 ≤ t ≤ N where α > 1, we have
E[∣Vf(Xτ)『]≤
a2Wi lnT (aCo + C2) lnT (∆z + Ci) lnT a2MBiG2 lnT
2 ln a T3/2 + 2ln a T + 2m ln a √T + 2 ln a √T '
6
Under review as a conference paper at ICLR 2022
When N = 1, m ≤ ηtt ≤ M and the bound (7) reduces to E[∣Nf (XT)『]≤ O(T + mT + M).
If, in particular, m and M are of order O(1/√T), then We can derive the optimal convergence for
constant bandwidth step-sizes, comparable to the literature for constant step-sizes (Yan et al., 2018;
Liu et al., 2020; Mai and Johansson, 2020; Defazio, 2020).
It is not easy to explicitly minimize the right-hand-side of (7) With respect to N. HoWever, N =
b(logα T)/2c attempts to balance the last tWo terms and appears to be a good choice in practice. The
theorem (see (8) establishes an O(ln T/ √T) rate under step-decay bandwidth step-size. If M = m,
Which means that the step-size folloWs the boundary functions, We get a near-optimal (up to lnT)
rate for stochastic momentum with a step-decay step-size on non-convex problems. We believe that
this is the first near-optimal rate for stochastic momentum with step-decay step-size. The next result
shows how an exponentially increasing stage-length allow to sharpen this guarantee even further.
Theorem 4.3.	Suppose the same setting as Lemma 4.1. Consider Algorithm 2, if the functions
δ(t) = 1∕αt-1 with α > 1, St = dSoɑt-1 ] with So = √T, we have
E[kVf(XT)『]≤O
(TW2+CT+m√τ+m√τ+
MBiG2
√T
The stage length St in Theorem 4.3 increases exponentially from Si = d√T] over N = [logα((α -
1)√T + 1)C stages, resulting in an O(1∕√T) optimal rate for SGDM under the bandwidth-based
step-decay scheme. This removes the ln T term of Theorem 8. To the best of our knowledge, this
work is the first that is able to achieve an optimal rate for stochastic momentum with step-decay
step-size in a general non-convex setting.
Remark 4.4. (Better convergence than Liu et al. (2020)) We notice that reference Liu et al. (2020)
analyzes multi-stage momentum and obtains the bound
E[kVf (X)k2] ≤ O ( f (x1N- f* + 也 P= ηt! .	(9)
Here, X is a uniformly sampled iterate (unlike our results, whichfavour later iterates) and N is the
number of stages. The result uses a time-varying momentum parameter, whose value is determined
by the step-size ηt, and also assumes an inverse relationship between the step-size and stage-length,
i.e. that ηtSt is constant. Hence, N is of O(logα T) and the convergence guarantee in (9) is of
O(1/ logα T), which is far worse than the rate of Theorem 4.2 and the optimal rate of Theorem 4.3.
5	Numerical Experiments
In this section, we design and evaluate several specific step-size policies that belong to the bandwidth-
based family. We consider SGD with and without momentum, and compare their performances on
neural network training tasks on the CIFAR10 and CIFAR100 datasets.
5.1	Baselines and Parameter Selection for the Bandwidth Step-Sizes
The bandwidth framework allows for a unified and streamlined (worst-case) analysis of all step-size
policies that lie in the corresponding band. Within this family, the band gives a lot of freedom
in crating innovative step-size policies with additional advantages. In particular, we will design a
number of step-size policies that add periodic perturbations to a baseline step-size, attempting to both
escape bad local minima and to improve the local convergence properties.
The step-decay bandwidth step-sizes divide the total number of iterations into a small number of
stages, in which the boundary functions are constant. The width of the band are determined by the
constants m and M . We will explore step-sizes that add a decreasing perturbation within each stage,
starting at the upper band at the beginning of the stage, ending at the lower bound at the end of the
stage, and decaying as 1/i, 1/√i, linearly or according to a cosine function. As baseline, we consider
the step-decay step-size that follows the lower boundary function mδ(t). To use the same maximum
value for the bandwidth step-sizes, we do not add any perturbation in the first stage; cf. Figure 5.
For the 1 / √t-band, on the other hand, stages correspond to epochs and perturbing the step-size
within a stage would be too frequent and lead to bias. Rather, we choose to add similar perturbations
7
Under review as a conference paper at ICLR 2022
Figure 2: The scatter plots (left); function value distribution
around global minima (middle)
Table 1: The percentage (%) of the final
iterates (10000 runs) close to each local
minima
	constant		step-decay	
	small	large	baseline ■	linear ■
①	29.61	0.12	0.40	0.14
②	24.66	3.45	6.89	2.95
③	25.13	3.28	7.55	2.99
④	20.60	93.15	85.16	93.92
as for the step-decay band, but adjust the perturbation between stages. In our experiments, the two
step-size policies perform roughly the same number of periods of perturbations over the training set.
As baseline, We consider the step-size ηt = m/ʌ/t. In all experiments, the hyper-parameters (e.g., m
and M) have been determined using grid search, see Section E.2 for details.
5.2	Non-monotonic Schedule Helps to Escape Local MiniMa
To demonstrate the potential benefits of bandwidth-based non-monotonic step-size schedules, we
consider the toy example (see Section E.3 for details and further results) from (Shi et al., 2020),
which is non-convex and has four local minima2; see Figure 2. We then compare the final iterates of
SGD with constant step-sizes (both large and small), step-decay, and a bandwidth-based step-decay
step size which we call linear-mode (illustrated in Figure 5). As shown in Figure 2, a large constant
step-size more easily escapes the bad local minima to approach the global minimum at (0.7, -0.7)
than a small constant step-size. However, with a large constant step-size, the final iterates are scattered
and end up far from the global minimum, which also has been observed in Figure 5 of (Shi et al.,
2020). Therefore, we have to reduce the step-size at some points to reduce the error. This is exactly
the intuition of step-decay step-size. As shown in Figure 2, the scatter plots of SGD with step-decay
(red) and step-decay with linear-mode (green) are more concentrated around the global minimum
than the constant step-sizes.
To quantify the ability of different step-sizes to escape the local minima, Table 1 reports the percentage
of the final iterates under the different step-size policies that are close to each minima. We can see
that the ability of the step-decay policy (named baseline) to escape the local minima is slightly worse
than the large constant step-size, but Figure 2 shows that the variance of the near-optimal iterates is
reduced significantly. In a similar way, we can see that linear-mode not only improves the ability to
escape the local minima, but also produces final iterates that are more concentrated around the global
optimum. Hence, it appears (at least in this example) that non-monotonic step-size schedules allow
SGD to escape local minima and produce final iterates of high quality.
5.3	Numerical Results on CIFAR10 and CIFAR100
To illustrate the practical performance of the bandwidth-based step-sizes, we choose the well-
known CIFAR10 and CIFAR100 (Krizhevsky, 2009) image classification datasets. We consider the
benchmark experiments of CIFAR10 on ResNet-18 (He et al., 2016) and CIFAR100 on a 28 × 10 wide
residual network (WRN-28-10) (Zagoruyko and Komodakis, 2016), respectively. All the experiments
are repeated 5 times to eliminate the influence of randomness.
We begin by evaluating our step-sizes for SGD. The left column of Figure 3 present the results
of the 1/%/t-band step-sizes on the two datasets. As shown in Figure 5, these stepsizes are all
non-monotonic. The sudden increase in the step-size leads to a corresponding cliff-like reduction
in accuracy followed by a recovery phase that consistently ends up at a better performance than in
the previous stage. The three 1 /ʌ/t-band step-sizes achieve significant improvements compared to
their baseline (ηt = m/ʌ/t), in terms of both test loss and test accuracy. Moreover, the linear-mode
performs the best compared to other polynomial decaying modes. Then, the results of SGD with
step-decay band (described in Section 5.1 or see Figure 5 in Appendix) on CIFAR10 and CIFAR100
2Notation:①-③ denote the local minima at top left, top right and bottom left, respectively; and ④ denotes
the global minimum at bottom right.
8
Under review as a conference paper at ICLR 2022
SGD with ISqrt-band on CIFARlO-ResNetlS
_ I I I
>UE3UU* *sk
# Epochs
SGD on CIFARlO-ResNetlB
ɪ 1 ɪ H J
>UE3UU* Nsk
20 40 60	80 100 120 140 160 180
# Epochs
5GDo∏ Cifarioo - wrn-2b-io
_ I _______ I
>UE3UU* Nsk
SGDM On CIF⅛R10∙R⅜8Net!8
86% J-----,J*∙V1L∣∙ 3" ,---------1----1-----1----1-----
0	20	40	60	80	100 120 140 160 180
# Epochs
160	170
—baseline
...l∕Λ-mcκle
—1//-mode
-i linear-mwie
-W 3sine∙mwie
I
>UE3UU* *sk
55% -..........................................
0	20	40	60	80	100 120 140 160 180
# Epochs
5Gdm on Cifarioo - wrn-2b-io
60%
>UE3UU* Nsk
0	20	40	60	80	100 120 140 160 180
# Epochs
SGD with ISqrt-band on CIFAR1M - WRN-28-10
I
>UE3UU* *sk
55% -..........................................
0	20	40	60	80 100 120 140 160 180
# Epochs
Figure 3: The test accuracy of l/ʌ/t-band (left column), Step-decay-band for SGD (middle column), step-decay
band for SGDM (right column)
are given in Figure 3 (middle column), respectively. At the final stage, the bandwidth step-sizes
improve both test loss (see Figures 6 and 7 in Appendix) and test accuracy compared to the baseline.
In particular, the cosine-mode performs the best on this problem. In the second stage, baseline
methods have a sharp boost. Our guess is that the noise accumulates quickly under a relatively large
constant step-size. But this phenomenon is only temporary. When we drop the step-size in the third
stage, the performance improves.
Next, we evaluate the performance of step-decay bandwidth step-sizes on SGDM. The results are
reported in Figure 3 (right column). The first observation from Figure 3 (right column) is that the
step-decay bandwidth step-sizes also work well for SGDM, and that again, the cosine-mode performs
better than the others. Another interesting observation is that the performance of vanilla SGD with
cosine-mode (red) in Figure 3 is comparable to (even better than) SGDM with the baseline step-decay
step-size (black) in Figure 3. A similar conclusion can also be made on CIFAR100.
6 Conclusion
We have studied a general family of bandwidth step-sizes for non-convex optimization. The family
specifies a globally decaying band in which the actual step-size is allowed to vary, and includes
both stage-wise and continuously decaying step-size policies as special cases. We have derived
convergence rate guarantees for SGD and SGDM under all step-size policies in two important classes
of bandwidth step-sizes (1∕√t and step-decay), some of which are optimal. Our results provide
theoretical guarantees for several popular “cyclical” step-sizes (Loshchilov and Hutter, 2017; Smith,
2017), as long as they are tuned to lie within our bands. We have also designed a number of novel
step-sizes that add periodic perturbations to the global trend in order to escape bad local minima and
to improve the local convergence properties. These step-sizes were shown to have superior practical
performance in neural network training tasks on the CIFAR data set.
In the analysis of SGDM, we assume that the stochastic gradient is bounded (see Assumption 2(b)).
It is interesting to see how to relax this assumption in some special cases, for example, when the
step-size is constant throughout each stage. It would also be interesting to see if the bandwidth
framework could be specialized to a more narrow class of step-sizes, for which we can provide even
stronger convergence rates.
9
Under review as a conference paper at ICLR 2022
References
W. An, H. Wang, Y. Zhang, and Q. Dai. Exponential decay sine wave learning rate for fast deep
neural network training. In 2017 IEEE Visual Communications and Image Processing (VCIP),
pages 1-4, 2017.
Z. Chen, Z. Yuan, J. Yi, B. Zhou, E. Chen, and T. Yang. Universal stagewise learning for non-convex
problems with convergence on averaged solutions. In International Conference on Learning
Representations, 2019.
C. Cortes, S. Greenberg, and M. Mohri. Relative deviation learning bounds and generalization with
unbounded loss functions. Annals of Mathematics and Artificial Intelligence, 85(1):45-70, 2019.
F. E. Curtis, K. Scheinberg, and R. Shi. A stochastic trust region algorithm based on careful step
normalization. Informs Journal on Optimization, 1(3):200-220, 2019.
A. Defazio. Understanding the role of momentum in non-convex optimization: Practical insights
from a lyapunov analysis. arXiv preprint arXiv:2010.00406, 2020.
Y. Drori and O. Shamir. The complexity of finding stationary points with stochastic gradient descent.
In International Conference on Machine Learning, pages 2658-2667. PMLR, 2020.
S. Gadat, F. Panloup, S. Saadane, et al. Stochastic heavy ball. Electronic Journal of Statistics, 12(1):
461-529, 2018.
R.	Ge, S. M. Kakade, R. Kidambi, and P. Netrapalli. The step decay schedule: A near optimal,
geometrically decaying learning rate procedure for least squares. In Advances in Neural Information
Processing Systems, pages 14977-14988, 2019.
E. Ghadimi, H. R. Feyzmahdavian, and M. Johansson. Global convergence of the heavy-ball method
for convex optimization. In 2015 European Control Conference (ECC), pages 310-315, 2015. doi:
10.1109/ECC.2015.7330562.
S.	Ghadimi and G. Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic program-
ming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
S.	Ghadimi and G. Lan. Accelerated gradient methods for nonconvex nonlinear and stochastic
programming. Mathematical Programming, 156(1-2):59-99, 2016.
I. Gitman, H. Lang, P. Zhang, and L. Xiao. Understanding the role of momentum in stochastic
gradient methods. Advances in Neural Information Processing Systems, 32:9633-9643, 2019.
E. Hazan and S. Kale. Beyond the regret minimization barrier: optimal algorithms for stochastic
strongly-convex optimization. Journal of Machine Learning Research, 15(1):2489-2512, 2014.
E. Hazan, K. Y. Levy, and S. Shalev-Shwartz. Beyond convexity: stochastic quasi-convex optimization.
In Proceedings of the 28th International Conference on Neural Information Processing Systems-
Volume 1, pages 1594-1602, 2015.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.
N. S. Keskar and G. Saon. A nonmonotone learning rate strategy for SGD training of deep neural
networks. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), pages 4974-4978. IEEE, 2015.
A. Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, Department of
Computer Science, University of Toronto, 2009.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural
networks. In Advances in Neural Information Processing Systems, pages 1106-1114, 2012.
X. Li, Z. Zhuang, and F. Orabona. A second look at exponential and cosine step sizes: Simplicity,
adaptivity, and performance. In International Conference on Machine Learning, pages 6553-6564.
PMLR, 2021.
10
Under review as a conference paper at ICLR 2022
Y. Liu, Y. Gao, and W. Yin. An improved analysis of stochastic gradient descent with momentum.
Advances in Neural Information Processing Systems, 33, 2020.
N. Loizou, S. Vaswani, I. H. Laradji, and S. Lacoste-Julien. Stochastic polyak step-size for sgd: An
adaptive learning rate for fast convergence. In International Conference on Artificial Intelligence
and Statistics, pages 1306-1314. PMLR, 2021.
I. Loshchilov and F. Hutter. SGDR: Stochastic gradient descent with warm restarts. In 5th Interna-
tional Conference on Learning Representations, ICLR 2017, Toulon, France, 2017.
V. Mai and M. Johansson. Convergence of a stochastic gradient method with momentum for non-
smooth non-convex optimization. In International Conference on Machine Learning, pages
6630-6639. PMLR, 2020.
B. T. Polyak. Some methods of speeding up the convergence of iteration methods. Ussr computational
mathematics and mathematical physics, 4(5):1-17, 1964.
H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statistics,
pages 400-407, 1951.
S. Seong, Y. Lee, Y. Kee, D. Han, and J. Kim. Towards flatter loss surface via nonmonotonic learning
rate scheduling. In UAI2018 Conference on Uncertainty in Artificial Intelligence, pages 1020-1030,
2018.
B. Shi. On the hyperparameters in stochastic gradient descent with momentum. arXiv preprint
arXiv:2108.03947, 2021.
B. Shi, W. J. Su, and M. I. Jordan. On learning rates and Schrodinger operators. arXiv preprint
arXiv:2004.06977, 2020.
L. N. Smith. Cyclical learning rates for training neural networks. In 2017 IEEE Winter Conference
on Applications of Computer Vision (WACV), pages 464-472. IEEE, 2017.
I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and momentum
in deep learning. In International Conference on Machine Learning, pages 1139-1147. PMLR,
2013.
V. Vapnik. Statistical learning theory. Wiley, New York, 1998.
X. Wang and Y.-x. Yuan. On the convergence of stochastic gradient descent with bandwidth-based
step size. arXiv preprint arXiv:2102.09031, 2021.
X. Wang, S. Magnusson, and M. Johansson. On the convergence of step decay step-size for stochastic
optimization. arXiv preprint arXiv:2102.09393, 2021.
Y	. Xu, Q. Qi, Q. Lin, R. Jin, and T. Yang. Stochastic optimization for dc functions and non-
smooth non-convex regularizers with non-asymptotic convergence. In International Conference on
Machine Learning, pages 6942-6951. PMLR, 2019a.
Y	. Xu, Z. Yuan, S. Yang, R. Jin, and T. Yang. On the convergence of (stochastic) gradient descent with
extrapolation for non-convex minimization. In Proceedings of the Twenty-Eighth International
Joint Conference on Artificial Intelligence (IJCAI-19), 2019b.
Y	. Xu, S. Zhu, S. Yang, C. Zhang, R. Jin, and T. Yang. Learning with non-convex truncated losses by
sgd. In Proceedings of The 35th Uncertainty in Artificial Intelligence Conference, volume 115,
pages 701-711. PMLR, 22-25 Jul 2020.
Y	. Yan, T. Yang, Z. Li, Q. Lin, and Y. Yang. A unified analysis of stochastic momentum methods for
deep learning. In Proceedings of the 27th International Joint Conference on Artificial Intelligence,
IJCAI’18, page 2955-2961. AAAI Press, 2018.
Z. Yuan, Y. Yan, R. Jin, and T. Yang. Stagewise training accelerates convergence of testing error over
SGD. Advances in Neural Information Processing Systems, 32:2608-2618, 2019.
S. Zagoruyko and N. Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.
11
Under review as a conference paper at ICLR 2022
A Convergence for Bandwidth Polynomial-Decay Step-Sizes
Our first more specific result considers the 1∕√7 bandwidth step-size with fixed stage length.
Theorem A.1. Under Assumptions 1 and 2(a), and assume that there exists a constant ∆0 > 0
SUCh that E[f (x1) — f *] ≤ ∆o for each t ≥ 1. Ifthe step-size ηt ≤ 1∕((ρ + 1)L) ,the stage length
St = S ≥ 1, and the boundary function δ(t) = 1∕√t for each 1 ≤ t ≤ N, we have
rιl ,	、，⑵	3∆o	1	3M 2Lσ	尻
E[kvf(XT )k ] ≤ — ∙√st + -m^r ∙ y t
(10)
Theorem A.1 shows how multi-stage SGD with polynomial-decay bandwidth step-sizes converges to
a stationary point. In the extreme case that S = 1, the step-size reduces to m/ʌ/t ≤ ηt ≤ M∕√t and
our result is comparable to the non-asymptotic optimal rate derived form= M = η0 in (Wang et al.,
2021, Theorem 3.5).
Multi-stage Vs traditional 1∕√7 step-size (S = 1) In general, during the initial iterations when the
first term of (10) dominates the error bound, the multi-stage technique can accelerate the convergence
by a larger step-size and longer inner-loop S. However, a large S will make the error bound worse
when the noise term begins to dominate the bound. The next theorem analyzes an algorithm with a
decreasing stage length.
Theorem A.2. Under Assumptions 1 and 2(a),and assume that there exists a constant ∆0 > 0
such that E[f (x1) — f *] ≤ ∆0 for each t ≥ 1. Ifthe step-size ηt ≤ 1∕((ρ + 1)L), the stage length
St = dSo∕√e with So = √T, and δ(t) = 1∕√t for each 1 ≤ t ≤ N, we have
E[k Vf(XT)『]≤ (2δ0 + 3ML√2) ∙ m√τ.
Schedule of Theorem A.2 Vs Chen et al. (2019) The theorem establishes an optimal rate for multi-
stage SGD with 1∕√t bandwidth step-size. Note that Chen et al. (2019) also analyzes a stagewise
algorithm with varying stage length, but their step-size decays as 1∕t and stage length increases with
t. An important novelty with our result is that it uses a long initial stage, S1 = √T while a large
stage length in Chen et al. (2019) requires a small initial step-size (of O(1∕√T)). Figure 4 illustrates
the performance of different step-size policies: 1) 1∕√t with St = 1; 2) 1∕√t with St = dn∕b]
where n is total sample size and b is the batch size; 3) 1∕√t with time-decreasing St = dSo∕ʌ/te and
So = √T; 4) and 1∕t step-size with St = Sot and So = 12 from Chen et al. (2019). We can see that
the step-size policies proposed in Theorem A.2 are more stable and perform the best.
For completeness, we also compare the performance of step-size schedules proposed by Theorems
3.1 and 3.3 in Figure 4 (right). Although step-decay with time-increasing stage length has a superior
theoretical convergence guarantee, constant stage length performs better in this particular example.
>UE3UUS
Different schedules for stage length
20	40	60	80	100 120 140 160 180
# Epochs
Figure 4: SGD
-∩of'∕t with St=I
... ∏o∕7t with St=∏∕b
—■- with St=
-i 1/t with St=SM(Chen et al. (2019))
ResNet18
>UE3UUS
- CIFAR10 -



..
12
Under review as a conference paper at ICLR 2022
B Proofs of Lemma and Theorems in Section 3
Lemma B.1. Suppose that Assumption 1 and Assumption 2(a) hold. If we run the Algorithm 1 with
T > 1 and ηt ≤ 1∕((ρ + 1)L), we have
E[kVf(Xτ)『]≤ -N-^——
X Stδ-1(t)
t=1
2(E[f(x1)]- E[f(x1+1)]) + M2LσT
mδ(t)2	m
(11)
Proof. (of Lemma B.1) The L-smoothness of f (see Assumption 1), i.e., kVf (x) - Vf (y)k ≤
L kx - yk for all x, y ∈ dom(f) implies that
f (x) + hVf (x), y — X- L ∣∣x - yk2 ≤ f (y) ≤ f(x) + hVf (x), y — Xi + L ∣∣x - y『.(12)
Applying the L-smoothness property of f and recalling Algorithm 1 at current iterate xit , we have
f (xt+ι) ≤ f (Xi) +〈Vf (xt),xt+ι - xi)+ L ∣∣xt+ι - xt『
≤ f(χi)-ηt (Vf (χt),gi) + 等勺刑2.
Taking conditional expectation of Fit on the above inequality and due to the unbiased estimator git
such that E[git | Fit] = Vf (Xit), we obtain that
E[f(χi+ι)Ft] ≤ f(χt) - ηt IlVf(χt)∣∣2 + (ηtpE[||gt『 F*	(⑶
By Assumption 2(a) that E[∣git - Vf (Xit)∣2 | Fit] ≤ ρ ∣Vf(Xit)∣2 + σ, we have
E[IIgitII2 |Fit]=E[IIgit-Vf(Xit)+Vf(Xit)II2]
=E[IIgit-Vf(Xit)II2]+E[IIVf(Xti)II2] ≤ (ρ + 1) IIVf (Xit)II2 + σ. (14)
Then incorporating the above inequality into (13) gives
E[f(χi+ι)Ft] ≤ f(χt) + (-ηt + (ηt)2L2ρ + 1)) IlVf(Xt)∣∣2 + (η)2Lσ.	(15)
If step-size ηi ≤ 1∕((ρ + 1)L), We have -ηi +(%)g(ρ+1) ≤ -ηt∕2. For any t ≥ 1, the inequality
(15) can be estimated as:
■ ∣∣Vf(χt)∣∣2 ≤ f(xt) - E[f(xi+ι)∣Fit] + ≡2Lσ.	(16)
Applying the assumption of step-size that ηit = n(t, i)δ(t) With m ≤ n(t, i) ≤ M for all t ∈
{1,2,…，N} to (16) gives
* ∣∣ Vf(Xt)∣∣2 ≤ f(xi)- E[f (xt+ι)Ft] + M2Lσδ(t)2,	(17)
and dividing mδ(t)2∕2 into the both sides, We have
2 V 2 (f(xt)- E[f(xi+ι)Fit])	MLσ	门8
—	mδ(t)	+ m .	()
Recalling the output XT of Algorithm 1 and then taking expectation, We have
1	N	St
E[kVf(xt)『]=PN q -W Xδ-1(t) ∙XE ∣Vf((Xi)∣2]
t=1 Stδ (t) t=1	i=1
(19)
13
Under review as a conference paper at ICLR 2022
Applying (18) recursively from i = 1 to St and using the fact that xt1+1 = xtS +1, the sum of (19) for
t ≥ 1 can be estimated as
N
N
St
Xδ-1(t)XE	Vf(xti)1 2 ≤X
t=1	i=1	t=1
Then plugging (20) into (19), we get
2(E[f(xt1)] -E[f(xt1+1)])
mδ(t)2
M 2Lσ
.
m
(20)
E[kVf(Xτ)k2] ≤
2(E[f(xt1)] - E[f(xt1+1)])
PN=1 Stδ-1(t)
mδ(t)2
+ M 2LσT
m
+
1
as desired.
□
Proof. (of Theorem A.1) In this case, the step size ηt satisfies that m/ʌ/t ≤ ηt ≤ M∕√t, where
δ(t) = 1∕√t. By the definition of δ(t), we have
S 1	CN	1	__ 2N 2
打而≥ Λ=0而dt = ~
(21)
Applying the assumption that E[f (x1) 一 f *] ≤ ∆0 where f * = minx f (x) and the definition of δ(t),
we have
N
X
t=1
2(E[f(x1)]- E[f(xS+ι)])
mδ(t)2
2N
≤ m Et(E[f(x1) — f*] - E[f (x1+1) — f*])
t=1
≤ 2 (∆0 + XXX E[f(x1)- f *]) ≤ 2nδ° .	(22)
mm
m	t=2	m
Under Assumptions 1 and 2(a) and n < 1∕((ρ + 1)L), thus Lemma B.1 holds. Incorporating these
inequalities into Lemma B.1 gives
E[kVf(Xτ)『]≤
1	a 2(E[f(x1)]- E[f(x1+1)]) + SM2LσT
S Pt=I δ-1(t)[幺	mδ(t)2	m
≤	3	J 2N ∆o + M 2LσT 1
—2SN3/2	m m _|
3∆o	1	3M 2Lσ	S
m m √ST +	2m T T
Then the proof is finished.
□
Proof. (of Theorem A.2) In this theorem, we consider SGD with the 1∕√t bandwidth step-size,
i.e., √mt ≤ ηt ≤ M for 1 ≤ t ≤ N, where the stage length St = dS0∕√t] with So = √T, and the
boundary function δ(t) = 1∕√t. By the relationship that PN=I St = T and X ≤ ∖x∖ ≤ X + 1 for all
x, we get that
(3 一 2√2)T ≤ N ≤ (ɪ + 1)2.	(23)
Under Assumptions 1 and 2(a) and ηit < 1∕((ρ + 1)L), thus Lemma B.1 holds. Following the same
process as Theorem A.1, the inequality (22) also holds, that is
N
X
t=1
2(E[f(x1)] - E[f(xS+ι)])
mδ(t)2
2N∆0
≤ ---0
m
(24)
14
Under review as a conference paper at ICLR 2022
Then incorporating the above inequalities to Lemma B.1, we have
EWf(XT)『]≤
1	Γ 2N ∆o	M 2Lσ T
PN=1 Stδ-1(t) [ m	m 一
1	Γ2N∆o + M2Lσ .T
PN=IdSo∕√e ∙√t [ m	m .
1 Γ2N∆0 + M2Lσ .T
S0 N	m	m
2∆0 +	M2Lσ 」& + M卫 \
mSo (3 - 2√2)mSo ml 0 3 - 2√2)
1
√T,
which concludes the proof.
Proof. (of Theorem 3.1) In this case, the step-size ηit exponentially decays every S iterations. By
the definition of ηit, we have δ(t) = 1∕αt-1 for all 1 ≤ t ≤ N. The sum of 1∕δ(t) can be estimated
as
NN
X ɪ = X at—1
⅛ δ(t)	L
aN - 1
α-1
Recalling the assumption that E[f (x1) - f *] ≤ ∆0 for all t ≥ 1, we have
N
X
t=1
2(E[f(x1)]- E[f(x1+1)])
mδ(t)2
2N
—Ea2(tT)(E[f (x1) - f*] - E[f (x1+1) - f*])
m
t=1
N
(a2 - 1) X a2(t-2)E[f (x1) - f *]+∆°
t=2
2α2(N-1)∆0
Under Assumptions 1 and 2(a) and ηit < 1∕((ρ +	1)L), thus Lemma B.1 holds. Then applying the
result of Lemma B.1 and incorporating the above inequalities into Lemma B.1 gives
EWf(XT)『]≤
1
S PNI δ-1(t)
2(E[f(x1)] - E[f(xS+ι)])
mδ(t)2
SM2Lσ
+------
m
≤
≤
≤
≤
≤
≤
□
2
m
m
a - 1	( 2a2(NT)∆o M2Lσ '
≤ S(ON-Iy v	m +	.
Substituting the specific values of N = b(logα T)∕2c, S = d2T∕ logα Te into the above inequality,
we have
E[∣Vf(XT)『]≤
≤
(a - 1)	(∆0a2(NT) + M2Lσ SN
S (aN — 1)( m	m
(∆0	aM2Lσ ʌ (a — 1)loga T
2am +	2m )	√T - a .
then transform the base loga to the natural logarithm ln, We can get the desired result.	□
Proof. (of Theorem 3.3) If S° = √T and St =「S。at-1], by PN=I St = T, the stage length N is
[loga((a - 1)√T + 1)C. In this case, we have δ(t) = 1∕at-1 for each t ∈ [N]. Under Assumptions
1 and 2(a) and ηit < 1∕((ρ + 1)L), thus Lemma B.1 holds. Before applying the result of Lemma
B.1,	we first give the following estimations:
N	N
X Stδ-1(t) ≥ X S0at-1 ∙ at-1 = √T ∙
t=1	t=1
1 - a2N
1 — a2
(a - 1)T3 +2T
a + 1
(25)
15
Under review as a conference paper at ICLR 2022
and also
N
X
t=1
2(E[f(x1)]- Ef(X1+1)])
mδ(t)2
≤
2
m
N
(α2 - 1) X a2(t-2)E[f(x1) - f *] +∆°
t=2
≤
2a2(NT)∆o _ 2((α - 1)√T + 1)2∆o
m	α2m
Applying these results into Lemma B.1, we have
EWf(XT)『]≤
≤
α + 1	( 2((α - 1)√T +1)2∆°
(α - 1)T3 +2T y	α2m
α+1 (竺o + MLσ ʌ 工 + O (1 )
a — 1 m m √ √T	T
M2Lσ
+----
m
Thus the proof is complete.
□
In Lemma B.1, we provide a unified analysis framework for bandwidth step-sizes which are indepen-
dent on the current random information. Recently, there are some interesting non-monotonic step-
sizes, e.g., the trust-region-ish algorithm (Curtis et al., 2019) and stochastic Polyak step-sizes (Loizou
et al., 2021), which can also be regarded to be in a band, but those are related to the current stochastic
information. We provide a unified framework for these kind of step-sizes below.
Lemma B.2. Under the same conditions as in Lemma B.1, we assume that the step-size ηit satisfies
mδ(t) ≤ ηt ≤ Mδ(t) and is dependent on the current random information. If m ≤ 工(，)and
m ≤ M ≤ M= -ρ+vPM(P+1∕+pmL, we have
E[kVf(xτ倘 ≤ ψo∑τ (市 + (M - ST + HT)	(26)
where ∑t = PN=I PS= 1 δ(t)-1 and ψo = (2 + ρ)m - ρM - LM(ρ + 1).
Proof. We consider the step-size ηit is depended on the current random information and mδ(t) ≤
ηit ≤ Mδ(t). By the L-smoothness off, that is kVf (x) - Vf(y)k ≤Lkx - yk, implies that
f (y) ≤ f (x) + hVf(x), y - Xi + 2 I∣x - y『	(27)
Let x = xit and y = xit+1, then
f(χi+ι) ≤ f(xt) -〈Vf(xt),ntgt〉+ L IIntgt『	(28)
Next we turn to estimate the product term - hVf(Xit), ηitgiti.
-{Vf (χt),nigi) = nt (∣∣gt - Vf(Xt)∣∣2 - ∣∣gt∣∣2 - ∣∣Vf(Xt)∣∣2)
≤Mδ(t)IIgit-Vf(Xit)II2-mδ(t)IIgitII2-mδ(t)IIVf(Xit)II2.
(29)
16
Under review as a conference paper at ICLR 2022
Then taking conditional expectation on Fit to (28) and applying Assumption 2(a) and (14), we have
E[f(xt+ι) I Ft] ≤ f(xt) - mδ(t) IIVf(Xt)『-mδ(t)E[∣∣gt∣∣2 | Ft] + Mδ(t)E[∣∣gt - Vf(Xi)『| Ft]
+ lm 2δ(t)2 E[∣∣gt∣2IFt]
≤f(Xit)-mδ(t)∣∣Vf(Xit)∣∣2-mδ(t)E[∣∣git-Vf(Xti)∣∣2]+∣∣Vf(Xti)∣∣2
+ Mδ(t)E[∣∣gt - Vf(Xt) ∣2∣ Fit] + LM 2δ ⑴2 (E[∣∣gt - Vf(Xi) ∣∣2] + ∣∣ Vf(Xi) ∣∣2)
≤ f (Xt) — 0mδ(t) - LM2δ(t)2 ) ∣∣ Vf(Xt)∣∣2 +(MS(t) - mδ(t) + LM2≡!) E[∣gf - Vf(Xi)∣∣2]
≤ f (Xt) - gm - LM2'(t) - ρ (M - m + LM≡)) δ(t) ∣∣ Vf(Xi)∣∣2
+ (M - m + LM≡) δ(t)σ
where by M ≥ m We have Mδ(t) — mδ(t) + LM ②,I)	> 0. Let ψ := 2m — LM；9 一
P (M 一 m + lmJ⑴).We know that δ(t) ≤ 1, then
LM2
ψ ≥ (2 + ρ)m - ρM------2-(P + 1).	(30)
Let ψo = (2 + ρ)m - ρM - lm2(ρ + 1). By solving the quadratic expression of M in ψo, if
m ≤ M < M := -P + — +2(P+1)(2 + P)mL
—	L(ρ +1)
(31)
then we have ψ ≥ ψ0 > 0. To guarantee that m ≤ M, we require that ψ0(m) > 0, then m ≤
4
L(ρ+1) ∙
Recalling the output X which is selected from {∕t} for all i ∈ [St] and t ∈ [N] with probability
Pt H 1∕δ(t), we have
1	N	St
E[kVf (X) k2]=PN^n X δ(t)-1 X E[∣∣Vf(Xt)∣∣2]
1
≤ —
ψ0ΣT
(PN=I (E[f(*)] - E[f(XSt+ι)])
I	O2
N St	LM2
+ (M - m)σ XX δ(t)-1 + LM^ T
t=1 i=1
(a)	1
≤ —
ψ0ΣT
δ0	lm 2στ
δ(N)2 +(M - m)b£T + ~2~T
(32)
where ΣT =PtN=1 Stδ(t)-1 and (a) follows from the assumption that E[f (x；) - f *] ≤ ∆0 where
f * = minx f (x) and x；+1 = xSt+；.	□
The above lemma immediately results in the following convergence results for bandwidth step-sizes
which depend on the current random information.
• If N = 1 and St = T , that is m ≤ ηit ≤ M and ηit is related to the current random
information, we have
E[kVf(X) m≤ ψδt+
(M-m)+LM2∕2
------;-----σ
ψ0
(33)
In this case, the boundary function δ(t) = 1, so the assumption on function value can be
replaced by f(X0；) - f* is bounded. We can achieve an O(1∕T) convergence rate to reach
a neighborhood of the stationary point. This error bound is comparable to the results of
(Loizou et al., 2021, Theorem 3.8) and (Curtis et al., 2019, Theorem 3.5).
17
Under review as a conference paper at ICLR 2022
Algorithm 2 SGDM with Bandwidth-based Step-Size
1:	Input: initial point x11 ∈ Rd, v11 = 0, # iterations T , # stages N, stage length {St }tN=1
such that PtN=1 St = T, momentum parameter β ∈ (0, 1), the sequences {δ(t)}tN=1 and
n{n(t, i)}iS=t 1o	∈ [m, M] with 0 < m ≤ M
2:	for t = 1 : N d=o
3:	for i = 1 : St do
4:	Query a stochastic gradient oracle O at Xt to get a vector gtt such that E[gt | Ft] = Vf (Xt)
5:	Update step-size ηit = n(t, i)δ(t), which belongs to the interval [mδ(t), M δ(t)]
6:	vit+1 = βvit + (1 - β)git
7:	Xit+1 = Xit - ηitvit+1
8:	end for
9:	v1t+ = vSt t +1 and Xt1+ = XtSt +1
10:	end for
11:	Return: XT is uniformly chosen from {x；* ,x2*, ∙∙∙ , x[* }, where the integer t* is randomly
chosen from {1,2,…，N} with probability Pt = δ-1(t)/(PN=I δ-1(l))
• If N > 1, St = S and δ(t) = 1∕αt-1, we have
E[kVf(X)k2] ≤
α-1
ΨoS(αN - 1)
(∆°α2(N-1) + LM2σ T) + (M-m
1 ( NaN-1 LM2σ (α — 1)N∖	(M — m)σ
Ψ0 I+	2	(aN - 1) ) + —Ψ0—
(34)
Compared to the case that N = 1, we observe that increasing N > 1 can improve the error
term LM /2 σ of (33). If M 一 m ≤ lm2, i.e., m ≥ M 一 LM2, then LM /2 σ turns out to
ψ0	2	2	ψ0
dominate the error bound of (33). If we increase N appropriately, then the error term of (33)
can be improved.
C Proofs of Lemma and Theorems in Section 4
We recall the momentum scheme of Algorithm 2 below
vit+1 = βvit + (1 一 β)git	(35)
Xit+1 = Xit 一 ηitvit+1	(36)
where β ∈ (0,1). Before giving the proofs, we introduce an extra variable Zt = ɪ-e 一 ɪ-gxt-1,
then
zi 一 χt = 1-β(Xt — Xt-1),	(37)
zt+ι 一 Xt = 1 一 §(Xt+1 - Xt).	(38)
However, the bandwidth-based step-size ηit in our analysis is time dependent and also possibly
non-monotonic, so the commonly used equalitiesXt+1 = Xt- ηgt + β(Xt- Xt-I)(Yan et al., 2018)
or zit+1 = zit 一 ηtgit (see lemma 3 of Liu et al. (2020)) do not hold in our analysis. This significantly
increases the level of difficulty of the analysis. The results of Lemma 4.1 is based on a sequence of
lemmas introduced below.
Lemma C.1. Suppose that the objective function f satisfies Assumption 1. At each stage t, the
step-size ηit is monotonically decreasing. Then, for i ≥ 2, we have
E[f(zit+1) | Fit] 一 f(zit) +
(/(吟一/区-1))
≤ —ntllVf(Xt)『+ 2(1βLβ^IlXt - Xt-1『+ 2^)2叫花+1 - 明21Fit].
18
Under review as a conference paper at ICLR 2022
Proof. Using the L-smoothness of f (Assumption 1) and taking conditional expectation gives
E[f(zit+1) | Fit] -f(xti)
≤ EKVf(Xi),zt+i-Xt〉∣ Ft] + 2E[∣∣
1
1-β
1
1-β
E[ Vf(xit),xti+1-xit |Fit]+
tt
zi+1 - xi
L
2 | Fit]
2(1 - β)2
E[x
E〈▽/(*), -ηt((1 - β)gt + βvt)> I Ft +
t
i+1
L
- xit ∣∣2 | Fit ]
2(1 - β)2
E[∣∣Xit+1 - Xti ∣∣2 | Fit]
-ηt ∣∣Vf(Xt)∣∣2 - 1η-ββ (Vf(Xt),vt〉+ 2(τ-1yE[∣∣χ;
=-ηt ∣∣Vf(Xt)∣∣2 - CI (Vf(Xt), Xt-I-Xt〉+
Re-using the L-smoothness property of f at zit and Xit gives
t
i+1 - x
L
it ∣∣2 | Fit]
2(1 - β)2
E[∣∣Xit+1 - Xit ∣∣2 | Fit].
f(zt) ≥ f(xt) + (Vf(xt),zt -xt〉- 2∣∣Zt
=f(xt) +占(Vf(xt),χt -xt-1〉
Then, combining the two inequalities above, we have
E[f(zit+1) | Fit] ≤f(zit)-ηit∣∣Vf(xit)∣∣2+
- xit ∣∣2
Lβ2
2(1 - β)2
∣∣xit - xit-1∣∣2 .	(39)
〈vf(Xt),xt-ι -xi)
≤
≤
≤
—
+ 2Γ⅛ E[∣∣Xi+ι - Xt∣ 门 Fit] + 2(^Lβ2βy ∣∣Xt - Xt-1∣∣2.
The step-size ηit for each stage is monotonically decreasing, i.e. ηit ≤ ηit-1 for i ≥
(1 - ηη^) ≥ 0. By the L-Smoothness of f, the inner product of (40) can be estimated as
(40)
2, so
vf(xit),xit-1
Applying (41) into (40), we find
E[f(zit+1) | Fit]
≤ f (zit) - ηit ∣∣vf (xit)∣∣2 +
-xt〉≤ f(Xli-I)- f(Xi) + 2 ∣∣xt - xi-ι∣∣ .
(41)
f(Xi-i)- f (Xt) + 2 ∣Bt - Xt
't∣ 门 Fi'] + 2⅛y∣∣Xt -Xi-ι∣∣2.
+ 2(1 -e)2 E[HXi+1 - X
(42)
Finally, we re-write the above inequality as
≤-ηt ∣Rf(χt)∣∣2 +
≤-ηt∣∣vf (χt)∣∣2 +
The proof is complete.
E[f(zit+1) |Fit]-f(zit)+
β2L
2(1 - β)
βL
2(1- β)2
画 + ⅛‰)怩-Xi-ι∣∣2 + 2⅛y E[∣∣Xt+ι -斓门 Fit]
∣∣Xt-Xt-1∣∣2 + 2(1 Le)2 E[∣∣Xi+ι - Xt∣2I Fit].
□
Lemma C.2. Suppose that the objective function satisfies Assumption 1 and the step-size ηit is
monotonically decreasing with ηt ≤ L at each stage, then
E h∣∣Xt+ι - Xt∣∣21 Fiti - β2∣∣Xt - Xt-1∣∣2 + 2(1 - β)ηt (E[f(Xt+ι) I Fit] - f(Xi))
≤ -2(ηit)2(1-β)2∣∣Vf(Xit)∣∣2+2(ηit)2(1-β)2Eh∣∣git∣∣2 | Fiti +(ηit)3β(1-β)L∣∣vit∣∣2.
19
Under review as a conference paper at ICLR 2022
Proof. First, due to the L-smoothness of the objective function f , we have
f (Xt+1) ≤ f (Xt) + (vf (χt),χt+ι - χt) + L2 I∣xt+1 - χi∣∣2
≤ f (Xt) + 3f(x»-ni((i-e)g； + βvt)> + 驾” ∣∣(1 - β)gt + βvt∣∣2
≤)f(χt) - (1 -β)ηt Wf(χ9,gQ-βηi <Vf(W),vt> + ≡-2L ((i -β)∣gt∣2 + β ∣∣vt∣∣2)
where inequality (a) follows the Cauchy-Schwarz inequality that k(1 - β)git + βvitk2 ≤ (1 -
β) kgit k2 + β kvit k2 . Then taking conditional expectation on both sides and due to that git is an
unbiased estimator of vf(Xit), i.e., E[git | Fit] = vf (Xit), we have
E[f(Xti+1) | Fit] ≤f(Xit)-ηit(1-β) ∣∣vf(Xit)∣∣2-βηitvf(Xti),vit
+ (ηi)2(1- β)LE[∣∣gt∣门 Ft] +(η彳L ∣∣vt∣∣2.	(43)
We recall the definition of vit+1 and incorporate (35) into (36), then
E[∣∣Xit+1 - Xit ∣∣2 | Fit]
=E[∣∣ηt(βvt + (1 - β)gt)∣门 Fit] = (ηt尸E[∣∣β∕ + (1 - β)gt∣门 Fit]
(=a)(ηit)2β2	∣∣vit∣∣2+(1-β)2E[∣∣git∣∣2	|Fit]+	2β(1-β)vit,vf(Xit)
Xit-1 ∣∣2 + (ηit)2(1 - β) (1-β)E[∣∣git∣∣2]+ 2βvit,vf(Xit)	(44)
(≤c)β2 ∣∣Xit - Xit-1 ∣∣2 + (ηit)2(1 - β) (1 - β)E[∣∣git∣∣2] + 2βvit,vf(Xit),	(45)
where (a) uses the fact that E[git | Fit] = vf (Xit); (b) follows the procedure that Xit = Xit-1 -ηit-1vit;
(c) applies the fact that the step-size per stage is monotonically decreasing, i.e.,ηit ≤ ηit-1 fori ≥ 2.
Then multiplying 2ηit(1 - β) into (43) and combining (45), we get that
E h∣∣xt+ι -xt∣门 Fiti - β2 ∣∣xt -Xt-1∣∣2 + 2ηt(1 - β) (E[f (xt+1) | Fit] - f (xt))
≤ -2(ηit)2(1-β)2 ∣∣vf(Xit)∣∣2
+(ηit)2(1-β)2(Lηit+1)Eh∣∣git∣∣2	|	Fiti	+ (ηit)3β(1 -	β)L	∣∣vit∣∣2
≤ -2(ηit)2(1-β)2	∣∣vf(Xit)∣∣2+2(ηit)2(1-β)2Eh∣∣git	∣∣2 |	Fiti	+	(ηit)3β(1 -	β)L	∣∣vit∣∣2
where the last inequality follows from the fact that nit ≤ 1/L.	□
Proof. (of Lemma 4.1) First we apply the result of Lemma C.1 and divided by ηit to the both side,
we have
E[f(zt+1) I	Fit]	- f (ziL	β	(f(xt)-f(xi-i))	β	(f(xi)-f(xt-i))
+	-
ηit	1 - β	ηit	1 - β	ηit-1
≤ TIVf(Xi)∣∣2 + (2(1 βL囱t) ∣∣χi -Xi-1∣∣2 + 2τ1n⅛E[∣∣vi+1∣门 Fit].	(46)
2(1 - β) ηi	2(1 - β)
Then we recall the result of Lemma C.2
E[∣∣xt+1 - xt ∣门 Fit] - ∣∣xt - Xt-1∣2 + 2nt (E[f (xt+1)] - f (xt))
≤ -(1 - β2) ∣∣Xit - Xit-1 ∣∣2-2(ηit)2(1-β)2 ∣∣Vf(Xit)∣∣2
+2(ηit)2(1-β)2Eh∣∣git∣∣2 | Fiti + (ηit)3β(1 - β)L ∣∣vit∣∣2,
20
Under review as a conference paper at ICLR 2022
multiplying a constant r
it into (46), we have
βL
2(1-β2)(1-β)2
> 0 and dividing ηit to the both side, and then incorporating
E[f(zt+ι) ∣Fit] -f(ztL	β f(xi)- f(xt-ι))	β	(f(xt)-f(W-ι))
+	-
ηit	1 - β	ηit	1 - β	ηit-1
+ η (E[∣∣xi+1 -明2 | Fit] -Ilxt - xi-1 Il2) + 2r (E[f (xt+ι) | Ft] - f (党)
≤ - ∣∣Vf(xt)∣∣2 + 2r(ηt)(1 - β)2E[∣∣gt∣门 Fit] + r(ηt)2β(1 - β)L ||斓|2
+ 2(KLβy E[∣∣vt+ι∣ 门F*	(47)
We define a function Wit+1 as follows:
Wit+1 = fzn-ɪ + r∣∣xt+1 t-xt∣∣2 + 2r[f(xt+ι) - f*].
ηi	ηi
Because of ηt ≤ ηt-1 at each stage, We have -1∕ηi ≤ -1∕ηt-1 (i ≥ 2), then
Wt	≤ f(zt+ι)- f* + ɪ (f(xt)- f*) - ɪ (f(xt)- f *)
i+1 —	ηt	1 - β	ηt	1 - β	ηt-i
+ "^t ∣∣xi+ι - xt∣∣2 + 2r (f (Xt+1)- f *) .	(48)
ηi
Taking conditional expectation on Wit+1 and applying (47) to the above inequality, We have
E[Wit+1 | Fit] ≤Wit+(f(zit)-f*)
-ɪʌ + β(f(xt-ι)- f*)
ηt-J	ι - β
a--
ηit	ηit-1
+ r ∣∣xi - Xt-1∣∣2	- ɪ) - ∣∣Vf (xt )∣∣2 + 2r(ηt )(1 - β)2E[∣∣gt ∣2∣ Fit]
ηi	ηi-1
+ r(ηt)2β(1-β)L∣∣vt∣∣2 +	ηtL	E[∣∣vt+ι∣2∣Fit].	(49)
2(1 - β)
We recall that v11 = 0, due to the assumption that E[kgit k2] ≤ G2 , and vit+1 is a convex combination
of git and vit, then by induction ifE[kvitk2] ≤ G2, then
E[∣∣vit+1∣∣2]=E∣∣βvit+(1-β)git∣∣2	≤ β ∣∣vit∣∣2 + (1 - β)E[∣∣git∣∣2] ≤G2.	(50)
Therefore, We have E[kvitk2] is bounded by G2. Then We apply E[kgitk2] ≤ G2, E[kvitk2] ≤ G2 and
ηit ≤ 1/L into (49)
E[Wi+ι IFit] - Wit ≤ (f(zt)- f *)(3
ηi
-ɪA + β(f(xt-ι)- f *)
ηti-J +	1 - β
U - 4
ηit ηit-1
+r ∣∣xt-xt-ι∣∣2 (--- tj-) -1∣Vf(Xt)『
ηi	ηi-1
+ ηtG2(r(1- β)(2 - β)+ L ) .	(51)
i	2(1 - β)2
The step-size is decreasing at each stage, then ηt ≤ ηt 1 (i ≥ 2), thus4-1— ≥ 0. Due to the
-	ηi	ηi-1
fact that vit is bounded (see (50)), i.e., E[kvit k2] ≤ G2, We have
E[∣∣xt -Xt-1∣∣2 = (ηt-i)2E[∣∣vt∣∣2]] ≤ (ηt-ι)2G2 ≤ G.
(52)
21
Under review as a conference paper at ICLR 2022
Recalling the definition of Zt, and applying the assumption that E[f (Xt) - f *] ≤ ∆o for each t,i ≥ 1
and f is L-smooth on its domain, and ηit ≤ 1/L gives
f (zt) ≤ f (Xt) +〈Vf (xt), Zt - xi〉+ 2 IH - xt『
≤ f (Xt) + 1-Γβ (Vf(XIi),x - Xt-1〉+ 2(1-β)2 IIXt - χt-i∣∣2
≤ f(Xt) + ι -β (f(Xt) - f(Xi-I) + 2 ||Xt-Xi-i『)+ 2(i -β)2 ||Xt-Xt—i||2
≤ f (Xt) + E (f (Xi) - f (Xt-ι)) + 21Lββp ||Xt - Xt-11|2
≤ 占 f (Xt)-占f(Xt-ι) + 2τ‰ ||Xt - Xt-11|2	(53)
where the inequality (a) dues to the fact that f (Xti-1) ≤ f(Xit) + Vf(Xit), Xit-1 - Xti +
LL ||x|-i - X$||2. Then We have
E[f(Zt) - f*] ≤ 占δ0 + 2(1⅜(ηt-ι)2G2 ≤ 占 + 2⅛.	(54)
Let ∆z = ι∆-0β + 2(i-Gj2l . Finally, applying (52) and (54), the bounded assumption on E[f (x∣)-f *],
and ηit ≤ 1/L, We have
E[Wi+ι | Ft] ≤ Wt + (1β-0β+δz + ^Lr) (η- ηɪ-)-1|Vf(Xt) ||2
+ ηt (r(1 - β)(2 - β) + 2(Γ⅛) G2
=Wi + AI (~t - -Γ~) - HVf(Xt)||2 + ηtBιG2	(55)
ηi	ηi-1
where AI =	f-T0	+	δz +	rG-,	BI = r(1 — β)(2	— β)+ 2(ι-β)2	and	δz	=	1-0β	+ 2(1Gβ)2 L .	□
The bandWidth step-size highly rises the difficulty of the analysis for momentum, especially When the
step-size has an increase between the stages, i.e. ηSt-1 := η0t < η1t . Before giving the results, we
consider two situations:
• η0t > η1t. We can apply Lemma 4.1 from i = 1 to St. Recalling the definition of Wit+1, we
have W1t+1 =WStt+1,then
XE|||Vf(Xt)||2] ≤ (E[Wt] - E[Wt+1]) + Al (5-")+ BiG2 Xηt.	(56)
• Otherwise if η0t ≤ η1t , the results of Lemma 4.1 only hold from i = 2 to St . Then
St
£E[||Vf(Xi)||2] ≤ (E[WJ] - E[Wt] + E[Wt] - E[Wt+i]) + E[||Vf (*)||2]
i=1
+ A1
(ηSt- ηt )+B1G2 X ηt.
(57)
For the bandwidth step-size, the initial step-size of stage t, η1t , is possibly larger than the
ending step-size of the previous stage, η0t . Thus, we can not use the simpler condition (56),
but have to rely on (57) in our derivations below.
22
Under review as a conference paper at ICLR 2022
Lemma C.3. Suppose the same setting as Lemma 4.1, we have
E[kVf(XT )k2] ≤ pδ (W1 + δ(NC+ 1) + mδ(N MN +1) + Cm X δ⅛ + CC X δ1) + BGMT
where Pδ = Pt=ι Stδ(t)-1, Co = r(GL + 2∆°), Ci = Ai + ∆% + 1-0β, C? = C° + AcG2, and
A1, B1, r, and ∆ are defined in Lemma 4.1.
Proof. Applying the result of Lemma 4.1 from i = 2 to St, the step-size ηit ∈ [mδ(t), M δ(t)], and
Wit+i = WSt +i, we have
St
XE[∣∣Vf(xt)∣∣2] ≤ (E[WS - E[Wt]+ E[Wt] - E[Wt+i]) + E[∣∣Vf(xi)∣∣2]
i=i
+ Ai (η----+ BiG2M(St- 1)δ(t).	(58)
Recalling the output of Algorithm 2, we have
1	N	St
E[kVf(Xτ)『]=PN StI*)-] X δ(t)-i XE[∣∣Vf(xt)∣∣2].	(59)
Then we divide δ(t) into the both side of (58), apply (58) from t = 1 to N and let δ
PtN=i Stδ(t)-i
E[kVf(Xτ)k2] ≤ *
X E[Wf] — E[Wt] + E[Wt] — E[Wt+i]
哈 丽
1
+ Pδ
X E[kVf(χ1)k2]
信一函一
+ BiG2MT
+A1 X δ⅛( ηSt - ηt))
(60)
First, We estimate PN=I E[kfiX1)k ]. From Lemma C.2,let i = 1, then incorporating the inequalities
(43) and (44), we have
2(ηt)2(1 - β)2E[∣∣Vf(χ])∣∣2] ≤ (η∣β) j|x] - x0/-E[∣∣χ2 - *『] + 2(η](1 - β))2E[∣∣gt『]
+ (η1 )3β(1 - β)L Ilvt『-2ηt(1 - β) f (χ]) - E[f (χ2)]]).
Then dividing 2(ηit)2(1 - β)2 to the both side and applying the fact that kxti - xt0k = (η0t)2 kvit k2,
E[kgtk2] ≤ G2 and E[kvtk2] ≤ G2 for any i,t, f(x?) - f * ≤ ∆o and η] ≥ mδ(t), we have
E[IIVf(xti)II2] ≤G2
1 + 2(1- β)2) + (1- β)ηt
≤ A2G2 +(1 - β)0mδ(t)
(61)
where A2 = (1+ 2(]-.)2). Then
N
X
t=i
E[kVf(χ])k2]
δ(t)
≤ X AGC +
≤ ⅛ δ(t)
_包_ X
(1 — β)m	δ2(t)
(62)
Next we turn to estimate PN=I E[W-)W1]. Recalling the definition of W+i, we have Wt ≥ 0.
Applying the inequalities (52), (54) and the assumption that f(xit) - f* ≤ ∆0 for any i, t, we have
E[Wt	E[Wt]	≤ E[W^	:= Ef(Ztt - f *]	+ rE[kx2	-	xik	] + 2rE[f(xt2)	-	f*]
2	i	2	ηit	ηit	2
W δz
≤ F+r
(63)
23
Under review as a conference paper at ICLR 2022
dividing δ(t) and applying (63) from t = 1 to N, we have
N
X
t=1
E[W2t] - E[W]
而
∆z N 1	G2	N 1
≤ 标 X W+r[L+2δ°) X 而.
(64)
Then we consider
N
X
t=1
E[Wt] - E[Wt+1]
而
N
X
t=1
E[W1t] E[W1t+1]
------------------
δ(t)	δ1(t+ 1)
)+X (δ⅛y
E[W1t+1]
≤ _WL
≤ δι⑴
N
+X
t=1
1
δ1 (t + I)
E[W1t+1].
(65)
—
Recalling the definition of E[W1t+1],
E[Wt+i] = Elf *]+ rE[3+1-x0+1M + 2rE[f (x1+i) - f*],
1	ηSt t	ηSt t	1
and applying the assumption that Ef (Xt) - f *] ≤ ∆0 and Ef (Zt) - f *] ≤ ∆z, and η0+1 = ηS t,
E[xt1+1 -xt0+12] = (ηStt)2E[v1t+12] ≤ (ηStt)2G2,wehave
∆	(a) ∆	G2
EWn ≤ ∆z + rηStCG +2 ≤ A r(G +
(66)
where (a) follows from ηSt ≥ mδ(t) and ηSt ≤ 1/L. Applying (66) into (65), we have
N
X
t=1
E[Wt] - E[Wt+1]
丽
≤ WL + r (G2+2δ0) +∆z X( ι
—δ(1) + δ(N + 1)	+ m 台[t+ +1)δ(t)
≤ WI +r (G2 + 2δ0) +∆z X (_1_____________________1_)
≤ δ(1) + δ(N +1)	+ m t=1 ∖t+ +1)δ(t)	δ(t)δ(t - 1) J
V W1	r (G2 + 2δo)	∆z
≤ δ(1) + δ(N + 1) — + mδ(N)δ(N + 1)
(67)
where the second inequality follows that δ(t) is decreasing, so δ(t) ≤ δ(t - 1), then -1∕δ(t) ≤
-1∕δ(t - 1). Finally, due to that ηSt ∈ [mδ(t), Mδ(t)], we have
N
t=
<X 1
_ mδ( mδ(t)2 .
(68)
Incorporate the inequalities (62), (64), (67) and (68) into (60), we have
E[kVf(Xτ)k2] ≤
W11
-Γ7+ +
r (Gr +2δoJ
δ(N +1)	+
∆z
mδ(N)δ(N + 1)
N1
+ 1 U mδ(t)2
1
+ Pδ
1
+ Pδ
∆ X ɪ
m δ(t)2
X Af+
G2
+ r∖L +
N1
X 而+BιG2Mτ
∆0 X
(1 - β)m t=1
(69)
1
Pδ [河
24
Under review as a conference paper at ICLR 2022
The above result can be re-written as (recall δ(1) = 1)
E[kVf(Xτ )『]
≤
≤
N1
X δ(t)+ BiG2MT
r I G + 2∆oJ
δ(N +1)	+
mδ(N)δ(N + 1)
+ (AI + δz + 1-0β)
N1
δ(N⅛ ++ mδ(N )δ(N + 1) + F X δ⅛ + C2 X δ⅛+ B1GGMB
m
where Co = r(GL~ + 2∆o), Ci = Ai + ∆z + 1—β, and C2 = Co + A2G2
□
Proof. (of Theorem 4.2) In this case, given the total number iteration T ≥ 1, the number of stages
N ≥ 1, St = S = \T/Ne, δ(t) = l∕αt-1 for each 1 ≤ t ≤ N, then the boundary function at the
final stage δ(N) = 1∕aNT and δ(N +1) = 1∕αN. Applying the specific value of δ(t) and N gives
N1
X丽=
N1
)而=
Then by St = dT∕Ne and (70), we easily get
N
Xδ= XSt∕δ(t)
t=i
We then plug the above results into Lemma C.3,
E[kVf(Xτ )k2]
αN - 1
α-1
α2N - 1
α2 — 1
≥ T(aN - 1)
_ N(α - 1).
N(α -I) i	N	δzα2NT	Ci(α2N -I) , C2(αN -I)
≤ T(αN - 1) (Wi+ COa +	+ m(α2 - 1) + α - 1
≤ Wiiɪ + (aCo + C2) N + (⅛±^苧 + MBiG2ɪ.
T a N i	TmT	a N i
mT
(70)
(71)
(72)
+ MB1G2T
(73)
where Co = r(GL + 2∆0), Ci = Ai + ∆z + i^e, and C2 = Co + A2G2, A2 = 1 + 2(1—βρ, and
W11, A1, B1, ∆z, and r are defined in Lemma 4.1.
Especially, we consider the number of outer-stage N = b(logα T)∕2c, the stage length St =
「2T/loga Te, and the boundary functions δ(t) = 1∕αt-1 for all t ∈ {1,2,…，N}. Let N =
b(logα T)∕2c, we have
E[kVf(Xτ)k2] ≤
α2W1 ln T	(αC0 + C2) ln T
2lnα T3/2 +
2 ln α T
Therefore, we complete the proof.
(∆z + Ci)ln T	α2MBiG2 ln T
+ 2m ln α √T +	2 ln α	√T .
(74)
□
Proof. (of Theorem 4.3) In this theorem, we consider the boundary functions δ(t) = 1∕αt-1,
and the stage length St =「Soat~1~∖ with So = TT, then we have the number of stages N =
[logα((α - 1)√T + 1)C and δ(N) = αN-1. Next we estimate P$ = PN=I Stδ-1(t) ≥ (α 一
1)2 T 3/2 + 2(α - 1)T. Then applying these results into Lemma C.3, we have
E[kVf(Xτ)『]≤O
+ Co +	+ _C^ + MBiG2
T	m√T	m√T	√T
(75)
□
25
Under review as a conference paper at ICLR 2022
D Supplementary Convergence Results
D.1 Convergence of SGDM WITH Constant and 1∕√t Bandwidth Decaying
step-size
We focus on a single stage that N = 1. In this case, We first consider the constant step-size
ηt = ηo/ √T. Recalling the result of Lemma 4.1 and letting ηt = no /√T for each i ≥ 1, we have
T X即Vf(χ1)∣∣2]≤ (W1 + T)
where BI = r(I - 3)(2 - β) + 2(i-β)2 and r = 2(1-62)(1 —β)2 .
Next we turn to analyze the 1∕√7 bandwidth step-size n1 ∈ [m/ʌ/i, M∕√z] (which is also monotonic
decreasing). Recalling the result of Lemma 4.1
E[Wi+ι | Ft] ≤ Wt + A1 (3 - ɪ) -IlVf(Xt)『+ ntiB1G2	(76)
ηi	ηi-1
and applying the result from i = 1 to T, we have
T X E[∣∣vf(χ1)∣ι2] ≤ T ((%1 - E[Wτ+ι])+AI (nl -")+BIG2 X n1).
Then applying the step-size n1 ∈ [m∕ √7,M∕√i] gives
1T
T ∑E[∣Vf (x1)
i=1
A1
+---+ +
m√T
2MB1G2 )
√T	)-
(77)
Thus, we can achieve an O(1∕√T) optimal rate for SGDM with 1∕√t bandwidth step-size on
nonconvex problems. When M = m, then the step-size reduces to n1 = m∕√7, we also provide the
convergence guarantee for the commonly used 1∕√i decaying step-size.
D.2 Convergence Guarantees for Cyclical Step-Sizes
in Loshchilov and Hutter (2017), the authors proposed a cosine annealing step-size
n = n^m in + 2 (nt∩ ax - * in) (I + Cos(Tcur FlrTy).	(78)
where ηmt in and ηmt ax are ranges of the step-size, and Tcur accounts for how many epochs since the
beginning of the current stage and Tt accounts for the current stage length (epoch). At each stage t,
the step-size is monotonically decaying within the range ηmt in and ηmt ax . in this paper, we propose a
general bandwidth framework for step-size which can cover this situation as long as mδ(t) ≤ ηmt in
and nmaχ ≤ Mδ(t). If the ranges {n2皿 nta乂} and the stage length are chosen as for example 1∕√7
in Theorems A.1 and A.2 or step-decay in Theorems 3.1 and 3.3, the theoretical convergence of
SGD under the cosine annealing step-size is guaranteed by our analysis in Section 3. Moreover,
because the cosine annealing is monotonic at each stage, so the convergence of SGD with momentum
under the cosine annealing policy is also guaranteed by the analysis of Section 4 as long as the
ranges nmt in, nmt ax are within our bands. To the best of our knowledge, Li et al. (2021) provides a
convergence guarantee for cosine step-size. However, to achieve a near-optimal rate for the general
smooth (non-convex) problems, the initial step-size is required to be bounded by O(1∕√T) which is
obviously impractical when the total number of iteration T is large (also discussed in related work).
In our framework, the cosine step-size is allowed to start from a larger step-size and gradually decay.
Besides, our results (e.g., Theorems 3.3 and 4.3) provide state-of-the-art convergence guarantees for
cosine step-size which remove the logT term ofLi et al. (2021).
Another interesting example is triangular cyclical step-size proposed by Smith (2017), which sets
minimum nmt in and maximum nmt ax boundaries and the learning rate cyclically varies (linearly
26
Under review as a conference paper at ICLR 2022
increasing then linearly decreasing) in these bounds. In each stage, the step-size is non-monotonic. In
their paper, the author also consider a variant which cuts ηmt in and ηmt ax in half after each stage. This is
exactly the step-decay boundary we discussed. Our analysis in Section 3 can provide the convergence
guarantees for such kinds of step-sizes. However, such cyclical step-size is not monotonic in each
stage, so our analysis for SGDM in Section 4 is not suitable for this situation.
E Additional Details of the Experiments on Bandwidth Step-Sizes
In this section, we provide additional details about the numerical experiments in Section 5.
E.1 How to Design the Bandwidth Step-Sizes and Select Parameters
To better understand the bandwidth step-sizes tested in the numerical experiments, we visualize the
step-size ηit (y-axis is log(ηit)) vs the number of epochs in Figure 5. We first consider the popular
“step-decay” policy as the baseline. During the first stage, the bandwidth step-size follows the lower
bound. From the second stage and on, we let the initial step-size in each stage to be equal to the
upper bound and the last step-size in the stage to reach the lower bound. Our numerical experience
has shown that the best performance is obtained when the initial step-size of each stage is larger
than the final step-size of the previous stage, which means that the step-size experiences a sudden
increase before it decreases again. For the step-decay band, we consider four decay modes: 1/i,
1 / VZi, linearly, and according to a cosine function (Loshchilov and Hutter, 2017) and update the step
size each epoch. If the training size is n and sample size per iteration is b, then one epoch is n/b
iterations.
We also adopt the polynomial l/ʌ/t step-size as the boundary function, named 1 /√t-band, and
update the step-size every epoch. We add similar perturbation as for the step-decay band, but we do
not apply the perturbation per stage. Otherwise, the perturbation is too frequent and just increases
the variance of the iterates. We tune the frequency of the perturbations (denoted as N0) to undergo a
similar number of cycles as the step-decay perturbations. In the first cycle, the bandwidth step-sizes
agree with their lower bound, just as for step-decay. From the second cycle, We begin to add the
decreasing perturbations, e.g., 1 /√7, 1 /i and linearly. As discussed above, these perturbations are
only adjusted between stages. Several different 1/Vt bandwidth step-sizes are shown in Figure 5.
1 汴band with different modes	Step-decay Zmd with different modes
So= βz∙i∙4s

0	50	100	150	200	250	0	50	100	150	200	250
# Epochs	# Epochs
Figure 5:	Bandwidth step-sizes: 1∕√t-band (left); step-decay-band (right)
We perform a grid search for the initial step-size η0 ∈ {0.01, 0.05, 0.1, 0.5, 1, 5} of the baseline step-
sizes. For step-decay step-size (baseline), we select the decay factor a from {1.5,2, 3,4,…，12}
and set the number of stages N = blogα T/2c according to Theorems 3.1 and 4.2. We choose the
lower bound parameter m = η0 to agree with the baseline. The bandwidth s = M/m = αθ where
θ ∈ {0.5, 0.8, 1, 1.2, 1.3, 1.5, 1.8}. If θ > 1, it means that the starting step-size at the current stage is
larger than the ending step-size from the previous stage. For the 1/√t-band step-sizes, we choose
ηt = no/(1 + aʌ/t) as the baseline , and select the best a > 0 to make the final step-size reach
the interval {0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1}. Moreover, we select the lower bound parameter
m = η0 to make sure that the lower bound agrees with the baseline. For the upper bound parameter
M, we do a grid search for s = M/m ∈ {2, 3, 4, 5, 6}. The number of perturbation cycles N0 for
the 1 /√t-band step-sizes is chosen from {1, 2, 3,4}. All the hyper-parameters are selected to work
best according to their performance on the test dataset.
27
Under review as a conference paper at ICLR 2022
SGD (Sqrt-band) on CIFARlO-ResNetlS
----Baseline
■■■■ Uq-MO(Ie
—∙ J#-InOete
-t- linear-mode
0	20	40 SO SO 1∞	120 UO UO ISO
# Epochs
SGD on CIFARlO-ResNetU
∙∙,■•■,
ns- 6umjx
0	20	40 SO SO 100 120 140 »0 ISO
# Epochs
SGDM on CIFARlO-ResNetlB
'0∙'γ,
WBO - 6umjx
0	20	40	60	80	100 120 140 160 180
# Epochs
SGD WIMI IaqrHWnel On ClFAlulQ∙Resl⅛tl8
Mno- *sk
0	20	40	60	80 100 120 140 160 180
5GD On ClFARI0∙R⅜8N⅜tl8
Mno- *sk
0	20	40 SO SO 100 120 140 »0 ISO
SGDM On ClFARlo∙R⅜aNetl8
0	20	40	60	80	100 120 140 160 180
# Epochs	# Epochs	# Epochs
Figure 6:	The training loss and test loss of l/ʌ/t-band (left column), step-decay band for SGD (middle column),
and step-decay band for SGDM (right column) on CIFAR10
E.2 Experiments Details on CIFAR 1 0 and CIFAR 1 00 Datasets
In this subsection, we will give the implementation details for the experiments on CIFAR10 and
CIFAR100. The benchmark datasets CIFAR10 and CIFAR100 (Krizhevsky, 2009) both consist of
60000 colour images (50000 training images and the rest 10000 images for testing). The maximum
epochs called for the two datasets is 180 and the batch size is 128. All the experiments on CIFAR
datasets are implemented in Python 3.7.4 and run on 2 x Nvidia Tesla V100 SXM2 GPUs with
32GB RAM. All experiments are repeated 5 times to eliminate the effect of the randomness. The
performance of different algorithms is evaluated in terms of their loss function value and classification
accuracy on the training and test datasets. All the results for training loss, test loss, and test accuracy
are reported in Figures 6, 7, and 3.
For CIFAR 10, we train an 18-layer Resident Network model (He et al., 2016) called ResNet-18. We
first test the vanlia SGD with a weight decay of 0.0005. The initial step-size no = 1 and a = 1.41618
for 1∕√t step-size (baseline). For 1 /√t-band step-sizes, we set: S = 2 and No = 3 for 1∕√i mode;
s = 3 and N0 = 3 for 1/i mode; and s = 4 and N0 = 3 for linear mode. For the step-decay step-size
(baseline) and also the bandwidth step-sizes, the initial step-size no = 0.5 and decay factor α = 6.
For step-decay band step-sizes, the parameter θ is 1.3 for the 1∕√7, 1/i and linear modes, and is
1.2 for cosine mode. We also implement the SGD with momentum (SGDM) algorithm, with the
momentum parameter of 0.9 and a weight decay of 0.0005. For the step-decay step-size, the initial
step-size no is 0.05 and the decay factor α is 6. We choose the same initial step-size and decay factor
for the step-decay bandwidth step-sizes. The best θ is 1.3 for the four decay modes.
In a similar way, we also detail our parameter selection for the experiments on CIFAR100. On this
data set, we train a 28 X 10 wide residual network (WRN-28-10) (Zagoruyko and Komodakis, 2016).
We first implement vanilla SGD with a weight decay of 0.0005. For the baseline of the 1∕√t band,
we set no = 0.5 and a = 3.65224. For the 1 /√t-band step-sizes we use: S = 2 and No = 3 for
1 /√7-mode; S = 4 and No = 3 for 1 ∕i-mode; S = 4 and No = 2 for linear-mode. For the step-decay
band, we choose no = 0.5 and α = 6. The parameter θ is set to 1.3 for 1/i mode and 1.2 for the other
modes. Then we also apply the step-decay band step-sizes on the SGD with momentum (SGDM)
algorithm, where the momentum parameter is 0.9 and the weight decay parameter is 0.0005. We
set the initial step-size no = 0.1 and α = 6 for the baseline and other step-decay band step-sizes;
θ = 1.2 for 1A∕i and linear modes; and θ = 1.3 for 1/i and cosine modes.
28
Under review as a conference paper at ICLR 2022

SGD WIth ISqrt-band On CIFARIOO - WRN∙Z8∙10
ns- 6um∙jx
0	20	40	60	80 100 120 140 160 180
# Epochs
5GDM On ClFARIO-R⅜aN⅜tl8
ns- 6um∙jx
0	20	40	60	80	100 120 140 160 180
# Epochs
5GDM On ClFARIOO - WRN-28∙10
w∙°^,
Se- RE-E-Eh
20	40	¢0 SO 100 UO UO UO 180
# Epochs
SGD WIMI IaqrHWnel On ClFAMΛ0 - WRIkZMQ
O 20	40	60	80 IOO 120 140 160 180
5GDM On ClFARIo-R⅜aN⅜tl8
O 20	40	60	80 IOO 120 140 160 180
SGDM on Cifarioo - wrn-2b-io
O 20	40	¢0 SO IOO UO UO UO 180
# Epochs	# Epochs	# Epochs
Figure 7:	The training loss and test loss of l/ʌ/t-band (left column), step-decay band for SGD (middle column),
and step-decay band for SGDM (right column) on CIFAR100
E.3 Experiments Results of the Toy Example on Bandwidth Step-Size
In this subsection, we describe the toy example and also report additional results using other bandwidth
step-sizes.
The loss-function is the two-dimensional (x, y ∈ R are the variables) non-convex function:
f(χ, y) = ((X + 0.7)2 + 0.1) (X - 0.7)2 + (y + 0.7)2 ((y - 0.7)2 + 0.1)
which has four local minima (denoted by ① to @ 3), one of which is global (④).We execute 10000
algorithm runs with an initial point (-0.9, 0.9). The total number of iterations is set to T = 3000.
The gradient noise is drawn from the standard normal distribution. The setting of the experiments
follows (Shi et al., 2020). The step-sizes we tested in this part are similar to Section E.1. The
difference is that here we update the step-size per iterate instead of per epoch, as we did on the CIFAR
datasets. We report the percentage (%) of the final iterate close to each local minima in Table 2. Note
that the results for constant (large and small) step-size, step-decay (baseline), and step-decay with
linear have already been presented in the introduction of the main document. The large constant
step-size is 0.1 and the small constant step-size is 0.05. As we can see, 1 /√7-band with 1 /√7, 1 /i
and linear modes more likely to escape the bad local minima and find the global solution than their
baseline. Except the result of step-decay with linear (shown in Figure 2 and Table 1), we also find
that other bandwidth step-sizes achieve good performance and work better than the baseline.
In the toy experiment, we also test the performance of step-decay (baseline) and step-decay with linear
on different initial points. We evenly select 100 initial points X0 from the region [-1, 1] × [-1, 1]. At
each initial point, we repeat the same process as previous initial point (-0.9, 0.9) and do 10000 runs.
We record the percentage of final iterate close to global minima ④ at each initial point in Figure 8.
The x-axis denotes the distance of the initial point xo and the global minima x*. We can see that
compared to its baseline, step-decay with linear is less sensitive to the selection of the initial point
and a relatively large θ > 1 works better if the initial point is far from the global minima.
3Notation:① denotes the local minima at (—0.7, 0.7);② denotes the local minima at (0.7, 0.7);③ denotes
the local minima at (—0.7, —0.7);④ denotes the global minima at (0.7, —0.7).
29
Under review as a conference paper at ICLR 2022
Table 2: The percentage (%) of the final iterate close to each local minima
step-size	type	①	②	③	④
const	small	29.61	24.66	25.13	20.60
	large	0.12	3.45	3.28	93.15
	baseline	54.93	18.65	19.48	6.94
1 / √t-band	1 / √7-mode	10.92	22.25	23.51	42.96
	1/i-mode	6.92	21.57	22.63	45.74
	linear-mode	3.75	15.86	16.37	63.97
	baseline	0.40	6.89	7.55	85.16
	1 / √7-mode	0.09	3.73	4.16	92.02
step-decay-band	1/i-mode	0.09	3.55	3.85	92.51
	linear-mode	0.14	2.95	2.99	93.92
	cosine-mode	0.18	3.36	3.48	92.98
96%-
Different initial points
94%-
▼ ▼ ▼ ▼▼ ▼
"叫沙"，尹产再,…
92%-
90%-
88%-
86%-
▲	baseline
•	linear	with θ	=	1.5
★	linear	with θ	=	1.3
▼	linear	with θ=l
A
“趣趣念
0.5
1.0	1.5	2.0	2.5
IlXO-X*∣F
Figure 8:	The toy example With different initial points for step-decay (baseline) and step-decay With linear
30