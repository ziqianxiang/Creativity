Under review as a conference paper at ICLR 2022
Federated Learning with Partial Model
Personalization
Anonymous authors
Paper under double-blind review
Ab stract
We propose and analyze a general framework of federated learning with partial model
personalization. Compared with full model personalization, partial model personalization
relies on domain knowledge to select a small portion of the model to personalize, thus im-
posing a much smaller on-device memory footprint. We propose two federated optimization
algorithms for training partially personalized models, where the shared and personal param-
eters are updated either simultaneously or alternately on each device, but only the shared
parameters are communicated and aggregated at the server. We give convergence analyses
of both algorithms for minimizing smooth nonconvex functions, providing theoretical
support of them for training deep learning models. Our experiments on real-world image
and text datasets demonstrate that (a) partial model personalization can obtain most of the
benefit of full model personalization with a small fraction of personalized parameters, and,
(b) the alternating update algorithm often outperforms the simultaneous update algorithm.
1 Introduction
Federated Learning (McMahan et al., 2017) has emerged as a powerful paradigm for distributed
and privacy-preserving machine learning over a large number of edge devices (see Kairouz et al.,
2021, and references therein). We consider a typical setting of Federated Learning (FL) with n
devices (also called clients), where each device i has a training dataset of Ni samples zi,ι,…，Zi,N.
Let w ∈ Rd represent the parameters of a (supervised) learning model and fi (w, zi,j ) be the loss
of the model on the training example zi,j . Then the loss function associated with device i is
Fi (w) = (1/Ni) PjN=i 1 fi (w, zi,j). A common objective of FL is to find model parameters that
minimize the weighted average loss across all devices (without transferring the datasets):
n
minimize	αiFi(w),
w
i=1
(1)
where weights αi are nonnegative and satisfy Pin=1 αi = 1. A common practice is to choose the
weights as αi = Ni/N where N = Pkn=1 Nk, which corresponds to minimizing the unweighted
average loss across all samples from the n devices: (1/N) Pin=1 PjN=i 1 fi(w, zi,j).
The main motivation for minimizing the average loss over all devices is to leverage their collective
statistical power for better generalization, because the amount of data on each device can be very
limited. This is especially important for training modern deep learning models with large number of
parameters. However, this argument assumes that the datasets from different devices are sampled
from the same, or at least very similar, distributions. Given the diverse characteristics of the users
and increasing trend of personalized on-device services, such an i.i.d. assumption may not hold in
practice. Thus, the one-model-fits-all formulation in (1) can be less effective and even undesirable.
Several approaches have been proposed for personalized FL, including ones based on multi-task
learning (Smith et al., 2017), meta learning (Fallah et al., 2020), and proximal methods (Dinh et al.,
2020; Li et al., 2021). A simple formulation that captures their main idea is
minimize
w0 ,{wi}in=1
n
X ai (Fi(Wi) + -2∣∣wi - wo∣∣2),
i=1
(2)
1
Under review as a conference paper at ICLR 2022
input
input
(a) Personalized input layer(s). (b) Personalized output layer(s). (c) Personalized split input layer(s).
Figure 1:	Three simple examples of partitioning deep learning models.
where wi for i = 1, . . . , n are personalized model parameters at the devices, w0 is a reference model
maintained by the server, and the λi ’s are regularization weights that control the extent of personal-
ization. A major disadvantage of the formulation (2), which we call full model personalization, is
that it requires twice the memory footprint of the model, wi and w0 at each device, which severely
limits the size of trainable models. On the other hand, the flexibility of full model personalization can
be unnecessary. Modern deep learning models are composed of many simple functional units and are
typically organized into layers or a more general interconnected architecture. Personalizing the “right”
components, selected with domain knowledge, may result in a substantial benefit with only a small
increase in memory footprint. In addition, partial model personalization can be less susceptible to
“catastrophic forgetting” (McCloskey & Cohen, 1989), where a large model finetuned on a small local
dataset forgets the original (non-personalized) task, leading to a degradation of test performance.
We propose a framework for FL with partial model personalization. Specifically, we partition the
model parameters into two groups: the shared parameters u ∈ Rd0 and the personal parameters
vi ∈ Rdi for i = 1, . . . , n. The full model on device i is denoted as wi = (u, vi), and the local loss
function is Fi(u, vi) = (1/Ni) PNiI fi((u,vi), zi,j).OUrgoalisto solve the optimization problem
n
minimize	αiFi(u, vi).	(3)
u	, {vi}in=1	i=1
Notice that the dimensions of vi can be different across the devices, allowing the personal components
of the model to have different number of parameters or even different architecture.
We investigate two FL algorithms for solving problem (3): FedSim, a simultaneous update algorithm
and FedAlt, an alternating update algorithm. Both algorithms follow the standard FL protocol.
During each round, the server randomly selects a subset of the devices for update and broadcasts the
current global version of the shared parameters to devices in the subset. Each selected device then
performs one or more steps of (stochastic) gradient descent to update both the shared parameters
and the personal parameters, and sends the updated shared parameters to the server for aggregation.
The updated personal parameters are kept local at the device to serve as the initial states when the
device is selected for another update. In FedSim, the shared and personal parameters are updated
simultaneously during each local iteration. In FedAlt, the devices first update the personal parameters
with the received shared parameters fixed and then update the shared parameters with the new personal
parameters fixed. We provide convergence analysis and empirical evaluation of both methods.
The main contributions of this paper are summarized as follows:
•	We propose a general framework of FL with partial model personalization, which relies on
domain knowledge to select a small portion of the model to personalize, thus imposing a much
smaller memory footprint on the devices than full model personalization. This framework unifies
existing work on personalized FL and allows arbitrary partitioning of deep learning models.
•	We provide convergence guarantees for the FedSim and FedAlt methods in the general (smooth)
nonconvex setting. While both methods have appeared in the literature previously, they are either
used without convergence analysis or with results on limited settings (assuming convexity or full
participation) Our analysis provides theoretical support for the general nonconvex setting with
partial participation. The analysis of FedAlt with partial participation is especially challenging
and we develop a novel technique of virtual full participation to overcome the difficulties.
2
Under review as a conference paper at ICLR 2022
adapter
(b) Generalized additive model.
(a) Transformer layer with two adapters.
Figure 2:	More structured partial model personalization. (a) The adapter has a skip connection, thus it collapses
to the identity mapping if vi = 0; in addition, it has a bottleneck in the middle (Houlsby et al., 2019). (b) The
generalized additive model can be further augmented with a shared input layer for representation learning.
•	We conduct extensive experiments on image classification and text prediction tasks, exploring
different model personalization strategies for each task, and comparing with several strong
baselines. Our results demonstrate that partial model personalization can obtain most of the
benefit of full model personalization with a small fraction of personalized parameters, and
FedAlt often outperforms FedSim.
•	Our experiments also reveal that personalization (full or partial) may lead to worse performance
for some devices, despite improving the average. Typical forms of regularization such as weight
decay and dropout do not mitigate this issue. This phenomenon has been overlooked in previous
work and calls for future research to improve both performance and fairness.
Related work. Specific forms of partial model personalization have been considered in previous
works. Liang et al. (2019) propose to personalize the input layers to learn a personalized representation
per-device (Figure 1a), while Arivazhagan et al. (2019) and Collins et al. (2021) propose to personalize
the output layer while learning a shared representation with the input layers (Figure 1b). Both FedSim
and FedAlt have appeared in the literature before, but the scope of their convergence analysis is
limited. Specifically, Liang et al. (2019), Arivazhagan et al. (2019) and Hanzely et al. (2021) use
FedSim, while Collins et al. (2021) and Singhal et al. (2021) proposed variants of FedAlt. Notably,
Hanzely et al. (2021) establish convergence of FedSim with full device participation in the convex
and non-convex cases, while Collins et al. (2021) prove the linear convergence of FedAlt for a
two-layer linear network where Fi(∙, Vi) and Fi(u, ∙) are both convex for fixed Vi and U respectively.
We analyze both FedSim and FedAlt in the general nonconvex case with partial device participation,
hence addressing a more general and practical setting.
While we primarily consider the problem (3) in the context of partial model personalization, it can
serve as a general formulation that covers many other problems. Hanzely et al. (2021) demonstrate
that various full model personalization formulations based on regularization (Dinh et al., 2020; Li
et al., 2021), including (2), as well as interpolation (Deng et al., 2020; Mansour et al., 2020) are
special cases of this problem. The rates of convergence we prove in §3 are competitive with or better
than those in previous works for full model personalization methods in the non-convex case.
2	Partially Personalized Models
Modern deep learning models all have a multi-layer architecture. While a complete understanding
of why they work so well is still out of reach, a general insight is that the lower layers (close to the
input) are mostly responsible for feature extraction and the upper layers (close to the output) focus on
complex pattern recognition. Depending on the application scenarios and domain knowledge, we
may personalize either the input layer(s) or the output layer(s) of the model; see Figure 1.
In Figure 1c, the input layers are split horizontally into two parts, one shared and the other personal.
They process different chunks of the input vector and their outputs are concatenated before feeding
3
Under review as a conference paper at ICLR 2022
Algorithm 1 Federated Learning with Partial Model Personalization (FedSim / FedAlt)
Input: initial states u(0), {vi(0)}in=1, number of rounds T, number of devices per round m
1:	for t = 0,1, ∙∙∙ ,T 一 1 do
2:	server randomly samples m devices as S(t) ⊂ {1, . . . , n}
3:	server broadcasts u(t) to each device in S(t)
4:	for each device i ∈ S(t) in parallel, do
5:	(u(t+1),v(t+1)) = LocalSim/LocalAlt(u⑴，v(t))	. v(t+1) = Vitt if i ∈ S(t)
6:	send ui(t+1) back to server
7:	server updates u(t+1) = ml Pi∈s(t) u(t+1)
to the upper layers of the model. As demonstrated in (Bui et al., 2019), this partitioning can help
protect user-specific private features (input 2 in Figure 1c) as the corresponding feature embedding
(through vi) are personalized and kept local at the device. Similar architectures have also been
proposed in context-dependent language models (e.g., Mikolov & Zweig, 2012).
A more structured partitioning is illustrated in Figure 2a, where a typical transformer layer (Vaswani
et al., 2017) is augmented with two adapters. This architecture is proposed by Houlsby et al. (2019)
for finetuning large language models. Similar residual adapter modules are proposed by Rebuffi et al.
(2017) for image classification models in the context of multi-task learning. In the context of FL, we
treat the adapter parameters as personal and the rest of the model parameters as shared.
Figure 2b shows a generalized additive model, where the outputs of two separate models, one shared
and the other personalized, are fused to generate a prediction. Suppose the shared model is h(u, ∙)
and the personal model is hi(vi, ∙). For regression tasks with samples z%,j = (xi,j ,yi,j), where Xij
is the input and yij ∈ Rp is the output, We let Fi(u, Vi) = (1/Ni) PN= ι fi((u, Vi), Zij) with
fi (u, vi), zi,j = kyi,j 一 h(u, xi,j) 一 hi(vi, xi,j)k2 .
In this special case, the personal model fits the residual of the shared model and vice-versa (Agarwal
et al., 2020). For classification tasks, h(u, ∙) and hi(vi, ∙) produce probability distributions over
multiple classes. We can use the cross-entropy loss between yi,j and a convex combination of the
two model outputs: θh(u, xi,j) + (1 一 θ)hi(Vi, xi,j), where θ ∈ (0, 1) is a learnable parameter.
Finally, we can cast the formulation (2) of full model personalization as a special case of (3) by letting
U — W0,	Vi — Wi,	Fi(U,Vi) — Fi(Vi) + (λi/2)∣∣Vi — u∣∣2.
Many other formulations of full personalization can be reduced to (3); see Hanzely et al. (2021).
3	Algorithms and Convergence Analysis
In this section, we present and analyze two FL algorithms for solving problem (3). To simplify
presentation, we denote V = (V1, . . . , Vn) ∈ Rd1+...+dn and focus on the case of αi = 1/n, i.e.,
minimize%v F(u,V):= n Pn=I Fi(u,Vi).	(4)
This is equivalent to (3) if we scale Fi by nαi , thus does not lose generality. Moreover, we consider
more general local functions Fi(u, Vi) = Ez〜Di [fi((u, Vi), z)], where Di is the local distribution.
The FedSim and FedAlt algorithms share a common outer-loop description given in Algorithm 1. They
differ only in the local update procedures LocalSim and LocalAlt, which are given in Algorithm 2
and Algorithm 3 respectively. In the two local update procedures, Nu and Nv represent stochastic
gradients with respect to w and Vi respectively. In LocalSim (Algorithm 2), the personal variables
Vi and local version of the shared parameters Ui are updated simultaneously, with their (stochastic)
partial gradients evaluated at the same point. In LocalAlt (Algorithm 3), the personal parameters
are updated first with the received shared parameters fixed, then the shared parameters are updated
with the new personal parameters fixed. They are analogous to the classical Jacobi update and
Gauss-Seidel update in numerical linear algebra (e.g., Demmel, 1997, §6.5).
In order to analyze the convergence of the two algorithms, we make the following assumptions.
4
Under review as a conference paper at ICLR 2022
Algorithm 2 LocalSim U, Vi	Algorithm 3 LocalAlt U, Vi
Input: number of steps T, step sizes γv and γu	Input: number of steps Tv , Tu, step sizes γv , γu
1: initialize Vi,0 = Vi	1: initialize Vi,0 = Vi
2: initialize Ui,0 = U	2: fθr k = 0,1,…，τv — 1 do
3: for k = 0,1,…，τ 一 1 do	3:	Vi,k+1 = Vi,k 一 Yv Vv Fi (u, Vi,k )
λ .	_	V-7 TTf ( .	∖ 4:	Vi,k+1 = Vi,k - γvVv Fi Ui,k , Vi,k	4: update Vi+ = Vi,τv and initialize Ui,0 = U
c-.	_ V7 TTt Z .	λ 5:	Ui,k+1 = Ui,k - γuVuFi Ui,k , Vi,k	5: for k = 0,1,…，τu — 1 do
6: update Vi+ = Vi,τ	+ 6:	Ui,k+1 = Ui,k 一 YuVuFi Ui,k , Vi
7: update Ui+ = Ui,τ	7: update Ui+ = Ui,τu
8: return Ui+ , Vi+	8: return Ui+ , Vi+
Assumption 1 (Smoothness). The function Fi is continuously differentiable for each i = 1, . . . , n,
and there exist constants Lu, Lv, Luv and Lvu such that for each i = 1, . . . , n, it holds that
•	VuFi(u, Vi) is Lu-Lipschitz with respect to U and Luv-Lipschitz with respect to Vi;
•	VvFi(u, Vi) is Lv-Lipschitz with respect to Vi and Lvu-Lipschitz with respect to U.
Due to the definition of F(u, V) in (4), it is easy to verify that VuF(u, V) has LiPschitz constant Lu
with respect to u, Luv/√n with respect to V, and Luv/n with respect to any Vi. We also define
X ：= max{Luv, Lvu}∕VzLuLv,	(5)
which measures the relative cross-sensitivity of VuFi with respect to Vi and Vv Fi with respect to U.
Assumption 2 (Bounded Variance). The stochastic gradients in Algorithm 2 and Algorithm 3 are
unbiased and have bounded variance. That is, for all U and Vi,
EVeuFi(U, Vi) = VuFi(U, Vi),	EVevFi(U, Vi) = VvFi(U, Vi) .
Furthermore, there exist constants σu and σv such that
E	VeuFi(U, Vi)	-	VuFi(U,	Vi)	≤	σu2	, E	VevFi(U, Vi)	- VvFi(U, Vi)	≤	σv2 .
We can view VuFi(U, Vi), when i is randomly sampled from {1, . . . , n}, as a stochastic partial
gradient of F(U, V) with respect to U. The following assumption imposes a variance bound.
Assumption 3 (Partial Gradient Diversity). There exist δ ≥ 0 and ρ ≥ 0 such that for all U and V,
n Pn=IllVuFi(υ, Vi)- VuF(u, V)∣∣2 ≤ δ2 + P2∣∣VuF(u, V)∣∣2.
With P = 0, this assumption is similar to a constant variance bound on the stochastic gradient
VuFi(u, Vi); with P > 0, it allows the variance to grow with the norm of the full gradient.
Throughout this paper, we assume F is bounded below by F? and denote ∆F0 = F u(0), V(0) -F?.
Further, we use shorthand V(t) = (V1(t) , . . . , Vn(t) ) and
∆ut) = ∣∣ VuF(U⑴,V㈤)∣∣2,	and ∆vt) = 1 Pnn=ι∣∣VvFi(U㈤,VT)) ∣∣2.
For smooth and nonconvex loss functions Fi , we obtain convergence in expectation to a stationary
point of F if the expected values of these two sequences converge to zero.
We first present our main result for FedSim (Algorithm 1 with LocalSim), proved in Appendix A.2.
Theorem 1 (Convergence of FedSim). Suppose Assumptions 1, 2 and 3 hold and the learning rates
in FedSim are chosen as γu = η∕(Luτ) and Yv = η∕(LvT) with
η ≤ min { 12(1+χ1)(1+ρ2),
m/n
196(1-τ-1)(1+χ2)(1+ρ2)

.
Then, ignoring absolute constants, we have
T PTo1 (吉E[∆ut)] + nmvE[∆vt)]) ≤ 篝 + η(i + x2) (σu⅛Jm) + 熏)
+ η2(I - TT)(I + χ2) (σ+δ + L).
(6)
5
Under review as a conference paper at ICLR 2022
Table 1: Convergence rates of FedSim and FedAlt in different regimes along with optimal learning rate via η.
We only show the dominant terms and hide the lower order terms in T for simplicity.
C	ondition	η	FedSim	FedAlt
General		T -1/2	△Fo + ι+χ2σσ +δ2 (1-n) + mσ2!	△Fo + σu + δ2 (1 — m) + σ2 m + χ2(n - m)
			√T	√T [	mLu	nLv J	√T	—mLu√T —	Lv √T	n
σu2 m=	= σv2 = 0 n, τ > 1	T -1/3	1 屋k J2(1-τ-1)(1 + χ2)∖ T2/3 (δf0 +	Lu	J	1	∕δf , δ2(1 - T-1)) T2/3 1δf0 + —Lu —J
σu2 m=	= σv2 = 0 n, τ = 1	O(1)	△Fo ~Tr	△Fo ~Tr
The left-hand side of (6) is the average over time of a weighted sum of E∆(ut) and E∆(vt). The
right-hand side contains three terms of order O(1∕(ηT)), O(η) and O(η2) respectively. We can
minimize the right-hand side by optimizing over η. By considering special cases such as σu2 = σv2 = 0
and m = n, some terms on the right-hand side disappear and we can obtain improved rates. Table 1
shows the results in several different regimes along with the optimal choices of η.
Challenge in Analyzing FedAlt. We now turn to FedAlt. Note that the personal parameters are
updated only for the m selected devices in S(t) in each round t. Specifically,
QI) "v(t) - Yv PT=0 VVFi(u(t),v(tk) if i ∈ S⑴,
vi = vi(t)	if i ∈/ S(t).
Consequently, the vector V (t+1) of personal parameters depends on the random variable S(t). This
makes it challenging to analyze the u-update steps of FedAlt because they are performed after V (t+1)
is generated (as opposed to simultaneously in FedSim). When we take expectations with respect
to the sampling of S(t) in analyzing the u-updates, V (t+1) becomes a dependent random variable,
which prevents standard proof techniques from going through (see details in Appendix A.3).
We develop a novel technique called virtual full participation to overcome this challenge. Specifically,
we define a virtual vector Ve(t+1), which is the result if every device were to perform local v-updates.
It is independent of the sampling of S(t) and we can derive a convergence rate for related quantities.
We carefully translate this rate from the virtual Ve (t+1) to the actual V (t) to get the following result.
Theorem 2 (Convergence of FedAlt). Suppose Assumptions 1, 2 and 3 hold and the learning rates
in FedAlt are chosen as Yu = η∕(LuTu) and Yv = η∕(LvTv), with
η ≤ min
{
1	_____m_____
24(1+ρ2)，128χ2(n-m)，
Then, ignoring absolute constants, we have
ɪ PT T	(ɪ E[∆(t)l	+	-m E「△(*)])	≤ δf +rι ( σ +δ2(1-n)	+	σ2 m+x2(n-m) ʌ
T 乙t=0	(LU El δu「十	nLv E Pv J )	≤ ηT 十 η 1 mLu	十	Lv n J
2	2	2	22
十 η2 σL三(i-τ-1) + 瓷(i-τ-1) + *
The proof of Theorem 2 is given in Appendix A.3. Similar to the results for FedSim, we can choose η
to minimize the above upper bound to obtain the best convergence rate, as summarized in Table 1.
Comparing FedSim and FedAlt. Table 1 shows that both FedSim and FedAlt exhibit the standard
O(1∕√T) rate in the general case. Comparing the constants in their rates, we identify two regimes in
terms of problem parameters. The regime where FedAlt dominates FedSim is characterized by
σ(1 - 2m) < σU +δ2(1-m∕n)
Lp	n	mLu	.
A practically relevant scenario where this is true is σv2 ≈ 0 and σu2 ≈ 0 from using large or full batch
on a small number of samples per device. Here, the rate of FedAlt is better than FedSim by a factor
of (1 十 χ2), indicating that the rate of FedAlt is less affected by the coupling between the personal
and shared parameters. Our experiments in §4 corroborate the practical relevance of this regime.
The rates from Table 1 also apply for full personalization schemes without convergence guarantees in
the nonconvex case (Agarwal et al., 2020; Mansour et al., 2020; Li et al., 2021). Our rates are better
than those of (Dinh et al., 2020) for their pFedMe objective.
6
Under review as a conference paper at ICLR 2022
Table 2: Summary of datasets and models. A histogram of data per device is given in Figure 5 (Appendix B).
Task	Dataset	#Classes	Model	# Model Params	#Devices	#Data per device	
						Mean	Max
Next-word prediction	StackOverflow	10000	4-layer transformer	6M	1000	4964	15520
Landmark recognition	GLDv2	2028	ResNet-18	12M	823	88	1000
Character recognition	EMNIST	63	ResNet-18	11M	1114	298	418
4 Experiments
In this section, we experimentally compare different model personalization schemes using FedAlt
and FedSim as well as no model personalization. Details about the experiments, hyperparameters and
additional results are provided in the appendices. The code to reproduce the experimental results will
be publicly released.
Datasets, Tasks and Models. We consider three learning tasks; they are summarized in Table 2.
(a)	Next-Word Prediction: We use the StackOverflow dataset, where each device corresponds to the
questions and answers of one user on stackoverflow.com. This task is representative of
mobile keyboard predictions. We use a 4-layer transformer model (Vaswani et al., 2017).
(b)	Visual Landmark Recognition: We use the GLDv2 dataset (Weyand et al., 2020; Hsu et al.,
2020), a large-scale dataset with real images of global landmarks. Each device corresponds to a
Wikipedia contributor who uploaded images. This task resembles a scenario where smartphone
users capture images of landmarks while traveling. We use a ResNet-18 (He et al., 2016) model
with group norm instead of batch norm (Hsieh et al., 2020) and images are reshaped to 224 × 224.
(c)	Character Recognition: We use the EMNIST dataset (Cohen et al., 2017), where the input is a
28 × 28 grayscale image of a handwritten character and the output is its label (0-9, a-z, A-Z).
Each device corresponds to a writer of the character. We use a ResNet-18 model, with input and
output layers modified to accommodate the smaller image size and number of classes.
All models are trained with the cross entropy loss and evaluated with top-1 accuracy of classification.
Model Partitioning for Partial Personalization. We consider three partitioning schemes.
(a)	Input layer personalization: This architecture learns a personalized representation per-device
by personalizing the input layer, while the rest of the model is shared (Figure 1a). For the
transformer, we use the first transformer layer in place of the embedding layer.
(b)	Output layer personalization: This architecture learns a shared representation but personalizes
the prediction layer (Figure 1b). For the transformer model, we use the last transformer layer
instead of the output layer.
(c)	Adapter personalization: In this architecture, each device adds lightweight personalized adapter
modules between specific layers of a shared model (Figure 2a). We use the transformer adapters
of Houlsby et al. (2019) and for ResNet-18, the residual adapters of Rebuffi et al. (2017).
Algorithms and Experimental Pipeline. For full model personalization, we consider three baselines:
(i) Finetune, where each device finetunes (using SGD locally) its personal full model starting from a
learned common model, (ii) Ditto (Li et al., 2021), which is finetuning with `2 regularization, and,
(iii) pFedMe (Dinh et al., 2020) which minimizes the objective (2). All methods, including FedSim,
FedAlt and the baselines are initialized with a global model trained with FedAvg.
4.1	Experimental Results
Partial personalization nearly matches full personalization and can sometimes outperform it.
Table 3 shows the average test accuracy across all devices of different FL algorithms. We see that on
the StackOverflow dataset, output layer personalization (25.05%) makes up nearly 90% of the gap
between the non-personalized baseline (23.82%) and full personalization (25.21%). On EMNIST,
adapter personalization exactly matches full personalization. Most surprisingly, on GLDv2, adapter
personalization outperforms full personalization by 3.5pp (percentage points).
7
Under review as a conference paper at ICLR 2022
Table 3: Comparison of partial model personalization with full model personalization in terms of the average
test accuracy % across all devices. The subscript denotes the standard deviation over 5 runs with different
random seeds. The boldfaced/highlighted numbers denote the accuracies within one standard deviation of the
maximum in each row. For partial personalization, we show the accuracy of FedAlt; see Table 4 for FedSim.
Non-pers.	Full Model Personalization	Partial Model Personalization
	FedAvg	Finetune	Ditto	pFedMe	Input Layer	Output Layer	Adapter
StackOverflow	23.82	25.200.01	25.20o.oι	25.21o.oι	24.44o.oι	25.05o.oι	24.82o.oι
GLDv2	51.43	62.85o.o2	62.85o.oι	62.92o.o2	53.94o.o7	56.64o.o5	66.41o.o6
EMNIST	93.18	94.13o.oι	94.13o.oι	94.13o.oι	93.62o.o4	93.57o.o5	94.13o.o3
StackOverflow
GLDv2
EMNIST
# Personalized Params.
# Personalized Params.
# Personalized Params.
★ Partial
♦ Full
Figure 3: Absolute change in accuracy (percentage points) due to personalization plotted against number of
personal parameters (i.e., dimensionality of vi). Note that the x-axis is in log scale.
This success of adapter personalization can be explained partly by the nature of GLDv2. On average,
the training data on each device contains 25 classes out of a possible 2028 while the testing data
contains 10 classes not seen in its own training data. These unseen classes account for nearly
23% of all testing data. Personalizing the full model is susceptible to “forgetting” the original
task (Kirkpatrick et al., 2017), making it harder to get these unseen classes right. Such catastrophic
forgetting is worse when finetuning on a very small local dataset, as we often have in FL. On the
other hand, personalizing the adapters does not suffer as much from this issue (Rebuffi et al., 2017).
Partial personalization only requires a fraction of the parameters to be personalized. Figure 3
shows that the number of personalized parameters required to compete with full model personalization
is rather small. On StackOverflow, personalizing 1.2% of the parameters with adapters captures 72%
of the accuracy boost from personalizing all 5.7M parameters; this can be improved to nearly 90%
by personalizing 14% of the parameters (output layer). Likewise, we match full personalization on
EMNIST and exceed it on GLDv2 with adapters, personalizing 11.5-12.5% of parameters.
The best personalized architecture is model and task dependent. Table 3 shows that personalizing
the final transformer layer (denoted as “Output Layer”) achieves the best performance for StackOver-
flow, while the residual adapter achieves the best performance for GLDv2 and EMNIST. This shows
that the approach of personalizing a fixed model part, as in several past works, is suboptimal. Our
framework allows for the use of domain knowledge to determine customized personalization.
Finetuning is competitive with other full personalization methods. Full finetuning matches the
performance of pFedMe and Ditto on StackOverflow and EMNIST. On GLDv2, however, pFedMe
outperforms finetuning by 0.07pp, but is still 3.5pp worse than adapter personalization.
FedAlt outperforms FedSim for partial personalization. If the optimization problem (3) were
convex, we would expect similar performance from FedAlt and FedSim. However, with nonconvex
optimization problems such as the ones considered here, the choice of the optimization algorithm often
affects the quality of the solution found. We see from Table 4 that FedAlt is almost always better than
FedSim by a small margin, e.g., 0.08pp for StackOverflow/Adapter and 0.3pp for GLDv2/Input Layer.
FedSim in turn yields a higher accuracy than simply finetuning the personalized part of the model, by
a large margin, e.g., 0.12pp for StackOverflow/Output Layer and 2.55pp for GLDv2/Adapter.
8
Under review as a conference paper at ICLR 2022
Table 4: Comparing FedAlt and FedSim for partial model personalization. “FT (part.)” means finetuning the
personal parameters vi while fixing the shared parameters u from non-personalized training. The numbers are
averaged over 5 runs with different random seeds. The standard deviations are given in Table 9 (Appendix C).
	StackOverflow			GLDv2			EMNIST		
	FT (part.)	FedAlt	FedSim	FT (part.)	FedAlt	FedSim	FT (part.)	FedAlt	FedSim
Input Layer	24.96	24.44	24.81	51.97	53.94	53.64	93.29	93.62	93.55
Output Layer	24.93	25.05	25.02	53.21	56.64	56.24	93.37	93.57	93.55
Adapter	24.71	24.82	24.74	63.86	66.41	66.35	93.66	94.13	94.07
4.2	Effects of personalization on per-device generalization
Personalization hurts the test accuracy on some devices. Figure 4 shows the change in training
and test accuracy of each device, compared with a non-personalized model trained by FedAvg. We see
that personalization leads to an improvement in training accuracy across all devices, but a reduction
in test accuracy on some of the devices over the non-personalized baseline. In particular, devices
whose testing performance is hurt by personalization are mostly on the left side of the plot, meaning
that they have relatively small number of training samples. On the other hand, many devices with
the most improved test accuracy also appear on the left side, signaling the benefit of personalization.
Therefore, there is a large variation of results for devices with few samples.
Additional experiments (see Appendix C) show that using `2 regularization, as in (2), or weight decay
does not mitigate this issue. In particular, increasing regularization strength (less personalization) can
reduce the spread of per-device accuracy, but only leads to a worse average accuracy that is close to
using a common model. Other simple strategies such as dropout also do not fix this issue.
An ideal personalized method would boost performance on most of the devices without causing a
reduction in (test) accuracy on any device. Realizing this goal calls for a sound statistical analysis for
personalized FL and may require sophisticated methods for local performance diagnosis and more
structured regularization. These are very promising directions for future research.
FUII (train)
Full (test)
PartiaI (train)
Partial (test)
Pers. helps
mean
Pers. hurts
2500 5000 7500 10000 12500 15000
# Data per device
2500 5000 7500 10000 12500 15000
# Data per device
-4 -I--1-----1----1----1----1----1-
2500 5000 7500 10000 12500 15000
# Data per device
-4 ----1-----1----1----1----1----1-
2500 5000 7500 10000 12500 15000
# Data per device
Figure 4: StackOverflow task: Scatter plot of change in training and test accuracy (percentage points) per-device
versus the number of training samples on the device for (a) Left: full personalization with finetuning, and, (b)
Right: partial personalization with the output layer.
5 Discussion
In addition to a much smaller memory footprint than full model personalization and being less
susceptible to catastrophic forgetting, partial model personalization has other advantages. For
example, it reduces the amount communication between the server and the devices because only
the shared parameters are transmitted. While the communication saving may not be significant
(especially when the personal parameters are only a small fraction of the full model), communicating
only the shared parameters may have significant implications for privacy. Intuitively, it can be harder
to infer private information from partial model information. This is especially the case if the more
sensitive features of the data are processed through personal components of the model that are kept
local at the devices. For example, we speculate that less noise needs to be added to the communicated
parameters in order to satisfy differential privacy requirements (Abadi et al., 2016). This is a very
promising direction for future research.
9
Under review as a conference paper at ICLR 2022
Reproducibility Statement
For theoretical results, we state and discuss the assumptions in Appendix A. The full proofs of all
theoretical statements are also given there.
For our numerical results, we take multiple steps for reproducibility. First, we run each numerical
experiment for five random seeds, and report both the mean and standard deviation over these runs.
Second, we only use publicly available datasets and report the preprocessing at length in Appendix B.
Third, we give the full list of hyperparameters used in our experiments in Table 8 in Appendix B.
Finally, we will publicly release the code to reproduce the our experimental results.
Ethics S tatement
The proposed framework for partial model personalization is immediately applicable for a range of
practical federated learning applications in edge devices such as text prediction and speech recognition.
One of key considerations of federated learning is privacy. Partial model personalization maintains
the all the privacy benefits of current non-personalized federated learning systems. Indeed, our
approach is compatible with techniques to enhance privacy such as differential privacy and secure
aggregation. We also speculate that partial personalization has the potential for further reducing
the privacy footprint — an investigation of this subject is beyond the scope of this work and is an
interesting direction for future work.
On the flip side, we also observed in experiments that personalization (both full or partial) leads to a
reduction in test performance on some of the devices. This has important implications for fairness,
and calls for further research into the statistical aspects of personalization, performance diagnostics
as well as more nuanced definitions of fairness in federated learning.
References
Martin Abadi, Andy Chu, Ian J. Goodfellow, H. Brendan McMahan, Ilya Mironov, KUnal Talwar,
and Li Zhang. Deep Learning with Differential Privacy. In Proc. ofACM SIGSAC, pp. 308-318.
ACM, 2016.
Alekh Agarwal, John Langford, and Chen-Yu Wei. Federated Residual Learning. arXiv Preprint,
2020.
Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Kumar Singh, and Sunav Choudhary. Feder-
ated Learning with Personalization Layers. arXiv Preprint, 2019.
Duc Bui, Kshitiz Malik, Jack Goetz, Honglei Liu, Seungwhan Moon, Anuj Kumar, and Kang G. Shin.
Federated User Representation Learning. arXiv Preprint, 2019.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre van Schaik. EMNIST: an extension of
MNIST to handwritten letters. arXiv Preprint, 2017.
Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting Shared Repre-
sentations for Personalized Federated Learning. In Proc. of ICML, volume 139, pp. 2089-2099,
2021.
James W. Demmel. Applied Numerical Linear Algebra. SIAM, Philadelphia, 1997.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. ImageNet: A large-scale
hierarchical image database. In Proc. of CVPR, pp. 248-255, 2009.
Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive Personalized Federated
Learning. arXiv Preprint, 2020.
Canh T. Dinh, Nguyen Tran, and Josh Nguyen. Personalized Federated Learning with Moreau
Envelopes. In Proc. of NeurIPS, volume 33, pp. 21394-21405, 2020.
Alireza Fallah, Aryan Mokhtari, and Asuman E. Ozdaglar. Personalized Federated Learning with
Theoretical Guarantees: A Model-Agnostic Meta-Learning Approach. In Proc. of NeurIPS, 2020.
10
Under review as a conference paper at ICLR 2022
Filip Hanzely, Boxin Zhao, and Mladen Kolar. Personalized Federated Learning: A Unified Frame-
work and Universal Optimization Techniques. arXiv Preprint, 2021.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image
Recognition. In Proc. ofCVPR, pp. 770-778, 2016.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-Efficient Transfer Learning for NLP.
In Proc. of ICML, volume 97, pp. 2790-2799, 2019.
Kevin Hsieh, Amar Phanishayee, Onur Mutlu, and Phillip B. Gibbons. The Non-IID Data Quagmire
of Decentralized Machine Learning. In Proc. of ICML, volume 119, pp. 4387-4398. PMLR, 2020.
Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Federated Visual Classification with Real-World
Data Distribution. In Proc. of ECCV, volume 12355, pp. 76-92, 2020.
Peter Kairouz, H. Brendan McMahan, Brendan Avent, AUrelien BelleL Mehdi Bennis, Arjun Nitin
Bhagoji, Kallista A. Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael
G. L. D’Oliveira, Hubert Eichner, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett,
Adria Gascθn, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Zald Harchaoui, ChaOyang
He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi,
Mikhail Khodak, Jakub Konecny, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo,
Tancrede Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer Ozgur, Rasmus
Pagh, Hang Qi, Daniel Ramage, Ramesh Raskar, Mariana Raykova, Dawn Song, Weikang Song,
Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tramer, Praneeth Vepakomma,
Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances
and Open Problems in Federated Learning. Found. Trends Mach. Learn., 14(1-2):1-210, 2021.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. SCAFFOLD: Stochastic controlled averaging for federated learning. In
Proc. of ICML, 2020.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis,
Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in
neural networks. Proceedings of the National Academy of Sciences, 114(13):3521-3526, 2017.
Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian Stich. A Unified
Theory of Decentralized SGD with Changing Topology and Local Updates. In Proc. of ICML,
2020.
Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and Robust Federated
Learning Through Personalization. In Proc. of ICML, volume 139, pp. 6357-6368, 2021.
Paul Pu Liang, Terrance Liu, Ziyin Liu, Ruslan Salakhutdinov, and Louis-Philippe Morency. Think
Locally, Act Globally: Federated Learning with Local and Global Representations. In NeurIPS
Workshop on Federated Learning, 2019.
Yishay Mansour, Mehryar Mohri, Jae Ro, and Ananda Theertha Suresh. Three Approaches for
Personalization with Applications to Federated Learning. arXiv Preprint, 2020.
Michael McCloskey and Neal J. Cohen. Catastrophic Interference in Connectionist Networks: The
Sequential Learning Problem. volume 24 of Psychology of Learning and Motivation, pp. 109-165.
Academic Press, 1989.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise AgUera y Arcas.
Communication-Efficient Learning of Deep Networks from Decentralized Data. In Proc. of
AISTATS, pp. 1273-1282, 2017.
Tomas Mikolov and Geoffrey Zweig. Context dependent recurrent neural network language model.
In IEEE SLT, pp. 234-239, 2012.
Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with
residual adapters. In Proc. of NeurIPS, pp. 506-516, 2017.
11
Under review as a conference paper at ICLR 2022
Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny,
Sanjiv Kumar, and Hugh Brendan McMahan. Adaptive Federated Optimization. In Proc. of ICLR,
2021.
Karan Singhal, Hakim Sidahmed, Zachary Garrett, Shanshan Wu, Keith Rush, and Sushant Prakash.
Federated reconstruction: Partially local federated learning. In Proc. of NeurIPS, 2021.
Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar. Federated Multi-Task
Learning. In Proc. ofNeurIPS,pp. 4424-4434, 2017.
TensorFlow Federated. https://www.tensorflow.org/federated.
Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better:
On the importance of pre-training compact models. arXiv Preprint, 2019.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is All you Need. In Proc. of NeurIPS, pp. 5998-6008,
2017.
Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim. Google Landmarks Dataset v2 - A
Large-Scale Benchmark for Instance-Level Recognition and Retrieval. In Proc. of CVPR, pp.
2572-2581, 2020.
12
Under review as a conference paper at ICLR 2022
Appendix
Table of Contents
A Convergence Analysis: Full Proofs	14
A.1 Review of Setup and Assumptions .................................. 14
A.2 Convergence Analysis of FedSim ................................... 15
A.3 Convergence Analysis of FedAlt ................................... 22
A.4 Technical Lemmas ................................................. 29
B Experiments: Detailed Setup and Hyperparameters	31
B.1	Datasets, Tasks and Models ...................................... 32
B.2	Experimental Pipeline and Baselines ............................. 34
B.3	Hyperparameters and Evaluation Details .......................... 35
C Experiments: Additional Results	36
C.1 Ablation: Final Finetuning for FedAlt and FedSim ................. 36
C.2 Effect of Personalization on Per-Device Generalization ........... 37
C.3 Partial Personalization for Stateless Devices .................... 38
13
Under review as a conference paper at ICLR 2022
A Convergence Analysis: Full Proofs
We give the full convergence proofs here. The outline of this section is:
•	§A.1: Review of setup and assumptions;
•	§A.2: Convergence analysis of FedSim and the full proof of Theorem 1;
•	§A.3: Convergence analysis of FedAlt and the full proof of Theorem 2;
•	§A.4: Technical lemmas used in the analysis.
A.1 Review of Setup and Assumptions
We consider a federated learning system with n devices. Let the loss function on device i be Fi (u, vi),
where u ∈ Rd0 denotes the shared parameters across all devices and vi ∈ Rdi denotes the personal
parameters at device i. We aim to minimize the function
1n
F(U, V) := - EFi(U, Vi),	G)
ni=1
where V = (vι, ∙∙∙ ,Vn) is a concatenation of all the personalized parameters. This is a special case
of (3) with the equal per-device weights, i.e., αi = 1/n. Recall that we assume that F is bounded
from below by F? .
For convenience, we reiterate Assumptions 1, 2 and 3 from the main-paper as Assumptions 10 , 20
and 30 below respectively, with some additional comments and discussion.
Assumption 10 (Smoothness). For each device i = 1, . . . , n, the objective Fi is smooth, i.e., it is
continuously differentiable and,
(a)	u → VuFi(u, Vi) is Lu-Lipschitzfor all Vi,
(b)	Vi → VvFi(U,Vi) is Lv-Lipschitzfor all U,
(c)	Vi 7→ VuFi(U, Vi) is Luv -Lipschitzfor all U, and,
(d)	U 7→ Vv Fi (U, Vi) is Lvu-Lipschitz for all Vi.
Further, we assume for some χ > 0 that
max{Luv,Lvu} ≤ XJLuLv .
The smoothness assumption is a standard one. We can assume without loss of generality that the
cross-Lipschitz coefficients Luv , Lvu are equal. Indeed, if Fi is twice continuously differentiable, we
can show that Luv, Lvu are both equal to the operator norm kV2uvFi(U, Vi)kop of the mixed second
derivative matrix. Further, χ denotes the extent to which U impacts the gradient of Vi and vice-versa.
Our next assumption is about the variance of the stochastic gradients, and is standard in literature.
Compared to the main paper, we adopt a more precise notation about stochastic gradients.
Assumption 20 (Bounded Variance). Let Di denote a probability distribution over the data space Z
on device i. There exist functions Gi,u and Gi,v which are unbiased estimates of VuFi and VvFi
respectively. That is, for all U, Vi :
Ez 〜Di [Gi,u(u, v , z)] = VuFi(U, Vi ), and Ez 〜Di [Gi,v (U, v, z)] = V v Fi(U, Vi) .
Furthermore, the variance of these estimators is at most σu2 and σv2 respectively. That is,
Ez 〜DikGi,u(U,V,z) - V uFi (u, vi) k ≤ σu ,
Ez 〜DikGi,v (u,v,z) - V v Fi (u, vi) k ≤ σv .
In practice, one usually has Gi,u(U, Vi, z) = Vufi((U, Vi), z), which is the gradient of the loss on
datapoint Z 〜 Di under the model (u, Vi), and similarly for Gi,v.
Finally, we make a gradient diversity assumption.
Assumption 30 (Partial Gradient Diversity). There exist δ ≥ 0 and ρ ≥ 0 such that for all U and V,
1n
—EkVuFi(u, Vi)- VuF(U, V)k2 ≤ δ2 + ρ2kVuF(U, V)k2.	(8)
n
i=1
14
Under review as a conference paper at ICLR 2022
Algorithm 4 FedSim: Simultaneous update of shared and personal parameters
Input: Initial iterates u(0), V (0), Number of communication rounds T, Number of devices per round
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
m, Number of local updates τ, Local step sizes γu , γv .
for t = 0,1, ∙∙∙ ,T 一 1 do
Sample m devices from [n] without replacement in S(t)
for each selected device i ∈ S(t) in parallel do
Initialize vi(,t0) = vi(t) and ui(,t0) = u(t)
for k = 0, ∙∙∙ ,τ 一 1 do	. Update all parameters jointly
Sample data z(t) ~ Di
v(t)	=	v(t) 一, G (u(t) v(t) z(t))
vi,k+1 = vi,k 一 γv Gi,v (ui,k, vi,k, zi,k)
u(t)	=	u(t) 一 G (u(t) v(t) z(t))
ui,k+1 = ui,k 一 γu Gi,u (ui,k, vi,k, zi,k)
Update vi(t+1) = vi( ,tτ) and ui(t+1) = ui( ,tτ)
P	α u(t+1)
Update u(t+1) = —i∈S(t) i i—— at the server with secure aggregation
i∈S(t) αi
(T )	(T )	(T )
return U(T ),v； , ∙∙∙ ,vn
This assumption is analogous to the bounded variance assumption (Assumption 20), but with the
stochasticity coming from the sampling of devices. It characterizes how much local steps on one
device help or hurt convergence globally. Similar gradient diversity assumptions are often used for
analyzing non-personalized federated learning (Koloskova et al., 2020; Karimireddy et al., 2020).
Finally, it suffices for the partial gradient diversity assumption to only hold at the iterates (u(t), V(t))
generated by either FedSim or FedAlt.
A.2 Convergence Analysis of FedSim
We give the full form of FedSim in Algorithm 4 for the general case of unequal αi ’s but focus on
αi = 1/n for the analysis. In order to simplify presentation, we denote V(t) = (v1(t) , . . . , vn(t) ) and
define the following shorthand for gradient terms
△Ut) = IlVuF (U⑴,V ㈤)『，and △£)= J XXIlVv Fi (U㈤,v(t))『.
n i=1
For convenience, we restate Theorem 1 from the main paper.
Theorem 1 (Convergence of FedSim). Suppose Assumptions 1, 2 and 3 hold and the learning rates
in FedSim are chosen as Yu = η∕(Luτ) and Yv = η∕(LvT) with
η ≤ min { 12(1+χ1)(1+ρ2), VZ 196(1-TTm(Cχ2)(1+ρ2)} .
Then, ignoring absolute constants, we have
T PT=01 (*E[∆ut)] + 田E[∆vt)]) ≤ ∆F0 + η(1 + χ2) (σuu+mL1-m) + 磨)
u	nv	η	m u	nv
+ η2(1 一 τ-1)(1 + X2) (σ⅛δ2 + Lv).	⑹
Before proving the theorem, we give the following corollary with optimized learning rates.
Corollary 3. Consider the setting of Theorem 1 and let ε > 0 be given. Suppose we set the learning
rates Yu = η∕(τLu) and Yv = η∕(τLv), where (ignoring absolute constants),
1	m/n
(1 + X2)(1+ P2) ∕∖ 1(1-TT)(1 + ρ2)(1 + χ2)
15
Under review as a conference paper at ICLR 2022
We have,
T X LuEqVuF (u(t),V㈤Μ+晨: VvFi
after T communication rounds, where, ignoring absolute constants,
≤ε
∆F0(1+χ2) (σ+解(1 - m
T≤
2	mLu
∆FoP(1 - T-1)(1+ χ2)
+	ε3/2
+
mσv2
+西
+ δF0 (1 + χ2)(1 + P2
(1 - T-1)n
m
Proof. The choice of the constant η ensures that each of the constant terms in the bound of Theorem 1
is θ(ε). The final rate is now O(∆F0∕(ηε)); plugging in the value of η completes the proof. □
We now prove Theorem 1.
Proof of Theorem 1. The proof mainly applies the smoothness upper bound to write out a descent
condition with suitably small noise terms. We start with some notation.
Notation. Let Ft denote the σ-algebra generated by (u(t), V⑴)and denote EtH = E[∙∣F(t)]∙ For
all devices, including those not selected in each round, we define virtual sequences Uitk, Vak as the
SGD updates in Algorithm 4 for all devices regardless of whether they are selected. For the selected
devices k ∈ S(t), we have (u(tk,v(tk) =(U(t), V(tk). Note now that the random variables Uitk, V(tk
are independent of the device selection S(t). The updates for the devices i ∈ S(t) are given by
τ-1
v(t+1) = v(t)-X XG∙ (U⑴ v(t)z(t))
vi	= vi - γv	Gi,v Ui,k , vi,k, zi,k ,
k=0
and the server update is given by
τ-1
u(t+i)	=	u(t)	- Yu	XX	G.	(u(t)v(t)z(t))
U	=	U m	乙 G .Gi,u	∖ui,k,vi,k,zi,k).
i∈S(t) k=0
Proof Outline. We use the smoothness ofFi, more precisely Lemma 16, to obtain
F(U(t+1),V(t+1)) - F(U(t),V(t))
≤ VuF(U(t),V(t)),U(t+1)
{z
T1,u
n
-U(t) E + 1 XDVvFi(U⑴,vitt) V(t+1) - v(t)E
------} n i=1
X--------------{--------------}
T1,v
(9)
(10)
+ Lu(1+χ2) ∣∣U(t+1) - U(t)∣∣2 + 1 X Lv(1+χ2) I
X---------------{----------/ i=1
T2,u	X	?L
T2,v
vi(t+1) -vi(t)2
}
Our goal will be to bound each of these terms to get a descent condition from each step of the form
Et [F(U(t+1),V(t+1)) - F(U(t),V(t))]
≤ -γuTlI VuF(U(t)，V(t)) ∣∣2 -筌 XII VvFi(U(t),v(t)) ∣∣2 + O(γU + Yv),
i=1
16
Under review as a conference paper at ICLR 2022
where the O(γu2 + γv2) terms are controlled using the bounded variance and gradient diversity
assumptions. Telescoping this descent condition gives the final bound.
Main Proof. Towards this end, we prove non-asymptotic bounds on each of the terms T1,v, T1,u,
T2,v and T2,u, in Claims 4 to 7 respectively. We then invoke them to get the bound
Et [F(u(t+1),V(t+1)) - F(U⑴,V㈤)]≤ - γuτ∆Ut)- γvτm∆Vt)
+ Lu(I + XYYJl T 2
+	2
2	12δ2
σu + —(I - m/n)) +
Lv (1 + X2)YvT2σVm
2n
n τ-1	2
+ n XX Et∣∣u(tk- u(t)∣∣	(LuYu + — χ2LuLvγv
i=1 k=0
(11)
n τ-1
+2 XX EtbS-V(t)∣∣
i=1 k=0
Yv + χLu Lv Yu
Note that we simplified some constants appearing on the gradient norm terms using
Yu ≤ (l2Lu(1 + χ2)(1+ ρ2)τ)-1 and YV ≤ (6Lv(1 + X)τ)-1
Our next step is to bound the last two lines of (11) with Lemma 8 and invoke the gradient diversity
assumption (Assumption 30) as
n
1 X∣∣VUFi(U(t),v(t))∣∣ ≤ δ2 + (1 + ρ2)∣∣VuF(u(t),V(t))∣∣ .
n i=1
This gives, after plugging in the learning rates and further simplifying the constants,
EtIF(u(t+1),V(t+1)) - F(u(t),V(t))]
c∆(ut)	cm∆(vt)
-
8Lu	8Lvn
+c2(1 +χ2)
2
+ -rv +
nLv
+c3(1+χ2)(1-T-1)
24δ2
l^Lτ
+
Taking full expectation, telescoping the series over t = 0,…，T 一 1 and rearranging the resulting
terms give the desired bound in Theorem 1.	□
Claim 4 (Bounding T1,v). Let T1,v be defined as in (10). We have,
n
Et[T1,v] ≤- vn2r X∣∣vv Fi (u(t),vΓ))∣∣
n	i=1
n τ-1
+ YVm XXEt	X2LuLv∣∣U(tk
i=1 k=0
-u(t)∣∣2+LV∣v(tk - v(t)∣2
Proof. Define T1,v,i to be contribution of the ith term to T1,v. For i ∈/ St, we have that T1,v,i = 0,
since vi(t+1) = vi(t). On the other hand, for i ∈ S(t), we use the unbiasedness of the gradient estimator
17
Under review as a conference paper at ICLR 2022
(t)	(t) (t)
i,v an e nepenence o zik rom uik , vik o ge
τ-1
Et [T1,v,i] =-YvXEt VvFi U(t),vi(t) ,VvFi Ui(,tk),vi(,tk)
k=0
τ-1
=-Yv X EtDVvFi (u(t),v(t)) , VvFi (Ug,碟))
k=0
- γvτVvFi (U(t),vi(t)2
—
τ-1
Yv X EtDVvFi (U㈤,v(t)) , VvFi (明,碟)-VvFi (U㈤,Wt)
k=0
τ-1
≤ -YiTllVvFi (U㈤,Y)) Il + Yv XEtIlVvFi (u(tk司)-VvFi (U⑴,v(t)) || . (12)
k=0
For the second term, We add and subtract VvFi(U(t),V(tk) and use smoothness to get
lV F- (U(t) V(t))	-	V F∙(〃⑶	v(t)) l∣2 ≤	2y2L L ∣U(t)	- U(I『+ 2L2 Il V(t)	- v(t) II2
lVv Fi Ui,k, vi,k	-	VvFi U , vi l ≤	2χ LuLv lUi,k	- U l + 2Lv lvi,k	- vi l	.
(13)
Since the right hand side of this bound is independent of St, We get,
Et[T1,v ] = m Et
n
T1,v,i
i∈S(t)
n
n X Et [Tι,v,i],
1
m
and plugging in (12) and (13) completes the proof.
□
Claim 5 (Bounding T1,u). Consider T1,u defined in (10). We have the bound,
Et[Tι,u] ≤ - y2t∣∣VuF (U(t),V⑴)ll2
n τ-1
+ Yu XXEt	LUllUitk-U叫+ X2LuLv偿
i=1 k=0
Proof. Due to the independence of S(t) from U(t), V(t), we have,
τ-1
Et hU(t+1) - U(t)i = -YuEt I m X X Vu Fi (Ui(,tk) , vi(,tk) )
m i∈S(t) k=0
二 1	τ-1
= -Yu Et I m X XVUFi (Uitk,v(tk)
m i∈S(t) k=0
n τ-1
=-Yu X X Et
18
Under review as a conference paper at ICLR 2022
where the last equality took an expectation over S(t), which is independent of u(tk, v(t). Now, using
the same sequence of arguments as Claim 4, we have,
Et DyuF(u(t),V(t)),u(t+1) - u(t)E
=-YuX Et* VuF Cu(t),V ⑴),n X VuFi (u(tk,vfk) +
k=0	i=1
τ-1
τ-1	n
γu X Et n X VuFi C
≤)- γuτ∣∣VuF W) ,v (t))∣∣2+
k=0
n τ-1
i=1
u(tk,vi(,tk)-VuF (u(t),V㈤)∣
2n xx EtIIVuFi (u(tk ,vfk) -vu Fi (u(t),v(t)
i=1 k=0
n τ-1
Yu XX Et
u(tk - u(t)
i=1 k=0
where the inequality (*) follows from Jensen's inequality as
v(,tk-v(t)∣∣2
n
2
n
n X vuFi (u(tk,vfk) -VuF
i=1
n
X∣∣VuFi (uitk,v(tk) -VuFi (u(tk,v㈤
i=1
Claim 6 (Bounding T2,v). Consider T2,v as defined in (10). We have the bound,
Et[τ2,v ] ≤3Lv(I+2x2)YvT 2m x∣∣ Vv Fi (u(t),v(t))∣∣2+
i=1
Lv (1+ χ2 )γv2τ2mσv2
2n
□
2
+也Rnr XX EJLv ∣∣
i=1 k=0
vi(,tk-v(t)∣∣2 +χ2LuLvuu(tk-u(t)∣∣2
Proof. We start with
τ-1
Et
IIvktT - v(t)∣∣ =YvEt X Gi,v C
k=0
u(tk,v(,t"tk)∣∣2
2
T-1
≤ dTX eJ∣g- Cu(t) v(t) z(t))∣∣
≤ γvτ Et ∣Gi,v ui,k, vi,k, zi,k ∣
k=0
T-1	2
≤ γvτ2σv+γVτ X Et||vv Fi Cuitk,磁)∣∣
k=0
≤ γv2τ2σv2 + 3γv2τ2∣∣VvFi u(t), vi(t)
τ-1
+3γv2τXEt Lv2II
vi(,tk-v(t)∣∣2+χ2LuLv∣∣u("u(t)∣∣2
k=0
Using (a) v(t+1) = v(t) for i ∈ S(t), and, (b) S(t) is independent from u(tk,v(tk, we get,
Et[T2,v]
Lv(1 + χ2)m
2n
Et
；1	XMT T)∣∣2
i∈S(t)
≤ Lv (12nχ2)m X EtMT-v(t)∣∣2
i=1
Plugging in the bound Et ∣∣v(? — v(t) ∣∣ completes the proof.
□
19
Under review as a conference paper at ICLR 2022
Claim 7 (Bounding T2,u). Consider T2,u as defined in (10). We have,
Et[T2,u]≤ LU(I+产(σ2 + 12δ2(1 - mn))
+ 3Lu(1 + X2)γUτ2(1 + ρ2)IlVuFi (U⑴,V㈤)||2
+ 3Lu(I+nχ2)γuT XXX EjLuM-u(t)||2+X2LuLv I∣v(,tk-v(t) Il2
i=1 k=0
Proof. We proceed with the first two inequalities as in the proof of Claim 6 to get
2
Et |||u(t+1) - u(t) |||2 ≤
m
τ-1
+ γu2τ X	Et
k=0
1.
m X VuFi (Uitk ,v(tk)
i∈S(t)
{^^^^^^^^^^^^^^^
=:T3,j
}
For T3,j, (a) We add and subtract VuF(u(t), V(t)) and VuFi(U(t),vfk), (b) invoke the squared
triangle inequality, and, (c) use smoothness to get
T3j=6Et W X VuFi (U⑴,v(t)) -VuF (U⑴,V⑴)
| m i∈S(t)
2
+ 6|||VuF (U(t), V (t))|||2
+3Et	m X	(Lu∣∣Uitk-U(t)∣∣2+χ2LuLvMk-v(t)∣∣2
i∈S(t)
For the first term, We use the fact that S(t) is obtained by sampling Without replacement to apply
Lemma 17 together With the gradient diversity assumption to get
2
Et
m X VuFi (U㈤,v(t))-VuF (U㈤,V⑴)
∣	i∈S(t)	∣
≤ ɪ (≡) 1XX ∣∣VuFi HfF SV (t))∣∣2
i=1
≤ ɪ (≡ M+P2M SV (t))∣∣2).
Therefore,
Tij=Im- (ι - Zn)+6(1+间IVuF w，V (t))『
n
+3 X Et Lu∣∣U(tk -U(t)∣∣ +χ2LuLvMk-v(t)∣∣ 「
n i=1
where We also used the independence between S(t) and (U(tk ,V(tk). Plugging this into the expression
for EtkU(t+1) 一 U(t)k2 completes the proof.	□
Lemma 8. Let Fi satisfy Assumptions 10-30, and consider the iterates
Uk+1 = Uk -γuGi,u(Uk,vk, zk) , and, vk+1 = vk -γvGi,v(Uk,vk, zk) ,
for k = 0, ∙∙∙ ,τ — 1, where Zk 〜 Di. Suppose the learning rates satisfy Yu = cu∕(τLu) and
Yv = Cv/(τLv) with cu, Cv ≤ 1/√6max{1, χ-2}. Further, define,
A = γuL2u + fχ2γvLuLv , and, B = fγv Lv2 + χ2γuLuLv ,
20
Under review as a conference paper at ICLR 2022
where f ∈ (0, 1] is given. Then, we have the bound,
τ-1
X E[Akuk-u0k2+Bkvk- u0k2] ≤ 4τ2(τ - 1) (γUσUA + γvjσvjB)
k=0
+ 12τ2(τ - 1) (γUAkVuFi(uo,vo)k2 + YvBk▽◊ Fi(U0,v0)∣∣2).
Proof. If τ = 1, there is nothing to prove, so we assume τ > 1. Let ∆k := Akuk - u0k2 + Bkvk -
vok2 and denote by Fk the sigma-algebra generated by (wk, vk). Further, let Ek[∙] = E[∙∣Fk]. We
use the inequality 2αβ ≤ α2∕δ2 + δ2β2 for reals α, β, δ to get,
Ekkuk+1 - u0 k2 ≤ (1 + T-----1 ) kuk - u0k2 + TYuEk ∣∣Gi,υ(uk, vk,Zk)II
≤ (1 + T-I) kuk - u0k2 + TYuσu + TYukνuFi(Uk,vk)k2
≤ (1 + T-1) kuk - u0k2 + TYu σU +3τYu kVuFi(U0,v0)k2
+ 3T Yu2 L2u kuk - u0k2 + 3T Yu2 Luv kvk - v0k2 ,
where the last inequality followed from the squared triangle inequality (from adding and subtracting
VuFi(u0, vk) and VuFi(u0, v0)) followed by smoothness. Together with the analogous inequality
for the v-update, we get,
Ek Ak + 1] ≤ (1 + T---1 ) ∆k + A'kuk - u0k2 + B kvk - v0k2 + C ,
where we have
A0 = 3T (Yu2 L2u A + Yv2χ2LuLvB),	and, B0 = 3T(Yv2L2vB + Yu2χ2LuLvA) and,
C0 = TYu2σu2A +TYv2σv2B + 3TYu2AkVuFi(u0,v0)k2 + 3TYv2BkVvFi(u0,v0)k2 .
Next, we apply Lemma 20 to get that A0 ≤ A/T and B0 ≤ B/T under the assumed conditions on the
learning rates; this allows us to write the right hand side completely in terms of ∆k and unroll the
recurrence. The intuition behind Lemma 20 is as follows. Ignoring the dependence on T, Lu , Lv , χ
for a moment, if Yu and Yv are both O(η), then A0, B0 are both O(η3), while A and B are O(η).
Thus, making η small enough should suffice to get A0 ≤ O(A) and B0 ≤ O(B).
Concretely, Lemma 20 gives
Ek @k+1] ≤ (1 + T-----1 ) Elʌk] + C ,
and unrolling this recurrence gives for k ≤ T - 1
E[∆k]≤ X (1+* )jC ≤ * (1+T⅛ )kC
≤LiI(1 + t⅛)	C≤e22(T- 1)C,
where We used (1 + 1∕α)α ≤ e for all α > 0. Summing over k and using the numerical bound
e2 < 8 completes the proof.	□
Remark 9. We only invoked the partial gradient diversity assumption (Assumption 3) at iterates
(u(t), V (t)); therefore, it suffices if the assumption only holds at iterates (u(t), V (t)) generated by
FedSim, rather than at all (u, V ).
21
Under review as a conference paper at ICLR 2022
Algorithm 5 FedAlt: Alternating updates of shared and personalized parameters
Input: Initial iterates u(0), V (0), Number of communication rounds T, Number of devices per round
m, Number of local updates τu , τv , Local step sizes γu , γv ,
1:	fort = 0,1,…，T 一 1 do
2:	Sample m devices from [n] without replacement in S(t)
3:	for each selected device i ∈ S(t) in parallel do
4:	Initialize vi(,t0) = vi(t)
5:	for k = 0,…，τv — 1 do	. Update personalized parameters
6:	Sample data z(t))〜Di
7:	v(t)	= v(t) G (u(t) v(t) z(t))
7:	vi,k+1 = vi,k 一 γv Gi,v (u , vi,k, zi,k)
8:	Update vi(t+1) = vk(t,)τ
9:	Initialize ui(,t0) = u(t)
10:	for k = 0, ∙∙∙ ,τu — 1 do	. Update shared parameters
11:	u(t)	= u(t) 一 G	(u(t) v(t+1) z(t))
11:	ui,k+1 = ui,k 一 γuGi,u(ui,k, vi	, zi,k)
12:	Update ui(t+1) = ui(,tτ)
P	α u(t+1)
13:	Update u(t+1) = —i∈S(t) i i—— at the server with secure aggregation
i∈S(t) αi
14:	return U(T),v(T),…，v?)
A.3 Convergence Analysis of FedAlt
We give the full form of FedAlt in Algorithms 5 for the general case of unequal αi ’s but focus on
αi = 1/n for the analysis. For convenience, we reiterate Theorem 2 below. Recall the definitions
△ut) = IlVuF (U㈤,V(t+叫2,	and,	△£) = J XXNvFi (〃％('))『.
n i=1
Theorem 2 (Convergence of FedAlt). Suppose Assumptions 1, 2 and 3 hold and the learning rates
in FedAlt are Chosen as Yu = η∕(Luτu) and Yv = η∕(LvTv), with
η ≤ min
{
1	_____m_____
24(1+ρ2) , 128χ2(n-m),
Then, ignoring absolute constants, we have
工 PT-I
T 乙t=0
(亡 E [△?)] + 法 E [Mt)])
≤ ∆F0 + η (Tɪ)
+ η2
Before proving the theorem, we have the corollary with optimized learning rates.
Corollary 10. Consider the setting ofTheorem 2 and fix some ε > 0. Suppose we set Yu = η∕(τLu)
and γv = η∕(τLv) such that, ignoring absolute constants,
σu + δ2	i Y1/2
(I-Tu 1)
χ2(n 一 m)
m
X2n
Then, we have,
1 T-1
TX
t=0
VuF (U⑴,V㈤)∣∣2 +
U XXE∣∣VvFi (υ(t),v(t))∣∣)≤ ε
m
22
Under review as a conference paper at ICLR 2022
after T communication rounds, where, ignoring absolute constants,
Proof. We get the bound by balancing terms from the bound of Theorem 2. The choice of η ensures
that all the O(η) and O(η2) terms are at most O(ε). Finally, the smallest number of communication
rounds to make the left hand side of the bound of Theorem 2 smaller than ε is ∆Fo /(ηε).	□
We are now ready to prove Theorem 2.
Proof of Theorem 2. The proof mainly applies the smoothness upper bound to write out a descent
condition with suitably small noise terms. We start with some notation.
We introduce the notation ∆e (ut) as the analogue of ∆(ut) with the virtual variable Ve (t+1):
∆Ut) = IlVuF (U㈤,V⑴)『.
Notation. Let Ft denote the σ-algebra generated by (u(t), V⑴)and denote EtH = E[∙∣F(t)]. For
all devices, including those not selected in each round, we define virtual sequences Uitk, V(tk as the
SGD updates in Algorithm 5 for all devices regardless of whether they are selected. For the selected
(t)
devices i ∈ S , we have vi,k = vi,k and Ui,k = Ui,k . Note now that the random variables Ui,k , vi,k
are independent of the device selection S(t). Finally, we have that the updates for the selected devices
i ∈ S(t) are given by
τv -1
v(t+1) = Vtt-Y X Gi,v (u(t),vfk,zfk),
k=0
and the server update is given by
τu -1
u(t+1t=Nt- m X x α,u "(,K).
i∈S(t) k=0
Proof Outline and the Challenge of Dependent Random Variables. We start with
F (U(t+1t, V (t+1t) - F (U(tt, V (tt) =F(U(tt,V(t+1t) - F (U(tt, V (tt)
+F(U(t+1t,V(t+1t) - F (U(tt, V(t+1t) .	(14)
The first line corresponds to the effect of the v-step and the second line to the U-step. The former is
easy to handle with standard techniques that rely on the smoothness of F (U⑴,∙). The latter is more
challenging. In particular, the smoothness bound for the U-step gives us
F (U(t+1t, V(t+1t) - F (U(tt, V (t+1t)
≤ DVuF (u(tt,V(t+1)) , u(t+1t - Utt)E + Lu ∣∣u(t+1t - u(tt ∣∣2 .
The standard proofs of convergence of stochastic gradient methods rely on the fact that we can take
an expectation w.r.t. the sampling S(tt of devices for the first order term. However, both V(t+1t and
23
Under review as a conference paper at ICLR 2022
u(t+1) depend on the sampling S(t) of devices. Therefore, we cannot directly take an expectation
with respect to the sampling of devices in S(t) .
Virtual Full Participation to Circumvent Dependent Random Variables. The crux of the proof
lies in replacing V (t+1) in the analysis of the u-step with the virtual iterate Ve (t+1) so as to move all
the dependence of the u-step on S(t) to the u(t+1) term. This allows us to take an expectation; it
remains to carefully bound the resulting error terms.
Finally, we will arrive at a bound of the form
ɪ X1 (γuTuE[∆Ut)] + YvTvmE[∆Vt)]) ≤ 竿 + O(Yu + Yv).
T	8	16n	T
t=0
Next, we translate this bound from gradient E[∆e(ut)] of the virtual Ve (t+1) to E[∆(ut)], which is the
gradient computed at the actual iterate V (t) . A careful analysis shows that we only incur a lower
order term of O(YuYv2) in this translation. Choosing Yu and Yv small enough will give us the final
result.
Analysis of the u-Step with Virtual Full Participation. We introduce the virtual iterates Ve (t+1)
into the analysis of the u-step as follows:
Fu(t+1),V(t+1)	- F u(t), V (t+1)
≤ DVuF (U⑴，V(t+1)) ,u(t+1) - U⑴E + Lu∣∣u(t+1) - u(t)∣∣2
=DVuF (UQV(M) ,U(M-U⑴E + Lu∣Wt+I)-U㈤∣∣2
+ DVuF (U(t),V(t+1)) -VuF (U(t),Ve(t+1)) , U(t+1) -U(t)E
≤ DVuF (U(t),Ve(t+1)) , U(t+1) -U(t)E +Lu∣∣∣U(t+1) -U(t)∣∣∣2
+ 2L- ∣∣VuF (U⑴,V(t+1)) - VuF (U⑴,V(t+1)) ∣∣2
≤ DVuF (U(t),Ve(t+1)) , U(t+1) -U(t)E+Lu∣∣∣U(t+1) - U(t)∣∣∣2 +
X------------------} X------------------------}
^{^≡
T1,u
^{^≡
T2,u
XLV
2n
n
xM+ι)-v(t+1)∣∣
i=1
{z^^
T3,u
}
The last two inequalities follow from Young’s inequality and Lipschitzness of V 7→ VuF(U, V)
respectively.
We have now successfully eliminated the dependence of the first-order term T1,u on V(t+1). The
virtual iterates Ve (t+1) are now independent of S(t). This allows us to take an expectation w.r.t. the
sampling S(t) of the devices.
We bound each of these terms in Claims 11 to 13 below to get
Et F U(t+1), V(t+1)	-F U(t),V(t+1)
≤-咛 Et[∆ 叼+亨 XXIEt忖?—"([『+iYV^LvσVχ2(I- m/n)
n	i=1 k=0
x----------{-----------}
=:T20,u
+ LUmrU (σ2 + 3δ2 (1 - T)) + 8γVτ2Lvχ2(I - m/nDVt).
Note that we used the fact that 24LuYuτu(1 + ρ2) ≤1 to simply the coefficients of some of the terms
above. The second term has also been referred to as client drift in the literature; we bound it with
24
Under review as a conference paper at ICLR 2022
Lemma 18 and invoke the assumption on gradient diversity (Assumption 30) to get
T2,u ≤ 16γuLu;(Tu- I) XXEjVuFi (U⑴,V(t+1))『+ 8γULuTu(Tu- 1)σU
n	i=1
≤ 16γuLu;(Tu- 1) }2 + ρ2Et∣∣VuF (U⑴,V(t+1))『) +8γuLuτu(Tu- 1)σu .
Plugging this back in, we get,
Et F (U(t+1), V(t+1) - F (U(t), V (t+1)
≤ - YU u Et[△ Ut] +	uYu u (σu + 2δ2(1 - m/n)) + 4γVτ2LvσVχ2(1 - m/n)
8	u	m u	vv v
+ 8γVT2LvX2(1 - m/nDVt) + 8YuLuTU(Tu- 1)(σ2 + 2δu) .
Note that we used 128γu2L2uTu(Tu - 1)ρ2 ≤ 1, which is implied by 24LuγuTu(1 + ρ2) ≤ 1.
Bound with the Virual Iterates. We plug this analysis of the U-step and Claim 14 for the v-step
into (14) next. We also simplify some coefficients using 128YvTVLvχ2(n∕m 一 1) ≤ 1. This gives Us
Et F U(t+1),V(t+1) -F U(t),V(t)
≤ - YfUEt[∆utt] - YvTnmEt[∆Vtt] +4γVLvt>V (三 + χ2(1 - m∕n))
+ y⅛2 d + 2δ2(1 - m∕n)) + RYLTsTu- 1)(σ2 + 2δ2) + 与3L2Tvi(Tv- 1)°Vm .
m u	u uu	u	n
Taking an unconditional expectation, summing it over t = 0 to T - 1 and rearranging this gives
-
ɪ X(手E[∆ut)] + ^E愁)])	(15)
T	8	16n
t=0
≤ ~t0+4τVLv T2σV (~+χ2(1- m/n))+ Yu ； u su+2δ2(1 - m/n))
+ 8YuLUtU(tu - 1)(σ2 + 2δ2) + 4y3L2t2(Tv-1)σvm .
u uu	u	n
This is a bound in terms of the virtual iterates Ve (t+1). However, we wish to show a bound in terms of
the actual iterate V(t).
Obtaining the Final Bound. It remains now to relate △e (ut) with △(ut). Using the Cauchy-Schwartz
inequality and smoothness, we have,
Et∣∣∣VuF(U(t),V(t) -VuF(U(t),Ve(t+1) ∣∣∣2
n2
≤ 1 XEt∣∣VuFi (υ(t),ν(t)) -VuFi (υ(t),v(t+1))∣∣
n i=1
≤ x2LuLv XXEt∣∣v"-v("2
i=1
≤ XLLv XX (16YvTv≡∣∣VvFi (u(t),v(t))∣∣2 +8YvTviσv)
n i=1
=8YvT2σvχ2LuLv + 16YvTviχ2LuLv∆vt),
where the last inequality followed from Lemma 19. Using
∣∣∣VuF	(U(t),V(t)∣∣∣2 ≤	2∣∣∣VuF (U(t),V(t)	- VuF	(U(t), Ve (t+1)∣∣∣2+2∣∣∣VuF	(U(t), Ve (t+1)∣∣∣2
25
Under review as a conference paper at ICLR 2022
we get,
E[∆(ut)] ≤ 2E[∆e(ut)] + 16Yv2Tv2σv2χ2LuLv + 32Yv2Tv2χ2LuLv E[∆(vt)] .
Therefore, we get,
γ⅛u E[∆Ut)] + Yfm E[△出
16 u	32n	v
≤ YuTu e∆ (t)] + YvTvm
≤ 8	[ u ] + 16n
≤ YuTu e[∆ (t)] + YvTvm
≤ 8	[ U ] + 16n
1	32η2χ2m
2 +
E[∆(vt)] + γuτuγv2τv2σv2χ2LuLv
E[∆(vt)] +γuτuγv2τv2σv2χ2LuLv ,
n
where We used 如ηnχrm ≤ 1/2, which is one of the conditions We assume on η.
Summing this up and plugging in (15) gives
T-1
T x( yutu E[∆ut)]+YvTvm eδVt)])
T t=0	16	32n
T-1
1	YuTu	(t)	YvTvm	(t)	2 2 2 2
≤ T ʌ, (-8~E[△，4+	16~~ E[δV J J + YUTUYv TV σvX LuLv
T t=0	8	16n
≤ ^TO + 4yVlvTσ (: + χ2(1 - m/n)) + YmpU gu + 2δ2(1- m/n))
+ 8yULUTU(TU - 1)(σU + 2δ2) + 4'v"vTV (TV-l)σvm + YUTUYvT2σvX2LuLv .
u uu	u	n	vvv
Plugging in YU = η∕(LuTu) and YV = η∕(LvTV) completes the proof.	□
The analysis of each of the terms in the u-step is given in the following claims.
Claim 11 (Bounding T1,U). We have,
2 n τu -1
Et[Ti,u]≤-YUTUEtIlVUF(U㈤,v(M)∣∣ + YULUXXEt同-U(R .
i=1 k=0
Proof. For i ∈ S(t), we have that U(t) = u(tk. Therefore, we have,
τu -1
Et[T1,u] = -YUEt(VUF (U⑴,V(t+1)) , - XX VUFi (u(tk,V(t+1)
m i∈S(t) k=0
Using that U(t) is independent of S(t), we get,
n τu -1
Et[T1,u] = -YUEt(VUF (U㈤,V(t+1)) , - XX VUFi (u(tk,V(t+1)
n i=1 k=0
2
-YUTUEtIIVUF(U(t),Ve(t+1)II2
τu-1
-YU X Et VUF (U(t),Ve(t+1)
k=0
n
)，n X VUFi (u(tk,v(t+1)) - VUFi (u(t),v(t+1)
i=1
x, y followed by smoothness completes the proof. □
Invoking hx, yi ≤ kxk2/2+kyk2/2 for vectors
Claim 12 (Bounding T2,U). We have,
Et [T2,u] ≤ 3LuyUtU (l + 2m2(1 - m∕n)) Et∣∣VuF (u(t),V(t+1)) ∣∣2
+ 3LU yU TU
n τu -1
XX Et∣U(tk - u(t)
i=1 k=0
2 + 6LuY⅛! (1 - m∕n).
n
m
26
Under review as a conference paper at ICLR 2022
Proof. We use Ekzk2 = kE[z]k2 + Ekz - E[z]k2 for a random vector z to get
Et [T2,u] ≤
LuYuTuσU
m
τu -1
+Luγu2τu XEt
k=0
m x vuFi (us ,v(t+1))
i∈S(t)
{^^^^^^^^^^^^^^"
=:Tk0
We break the term Tk0 as
Tk0 ≤3	X (VuFi (uitk,V(t+I))-VuFi (u(t)，V(t+1)))
i∈S(t)
+3
m x VuFi(u(t),v(t+1))
i∈S(t)
- VuF u(t), Ve (t+1)
2
+ 3VuF (u(t), Ve(t+1))2.
For the first term, we use Jensen’s inequality to take the squared norm inside the sum, then use
smoothness and take an expectation over the sampling of devices to get
Et ；1 X (VuFi (u(tk,V(t+I))-VuFi (u(t),V(t+1)))
i∈S(t)
2
n
≤ ⅛ X EMk-叫.
i=1
For the second term, we use the fact that S(t) was sampled without replacement (cf. Lemma 17) and
invoke the gradient diversity assumption (Assumption 30) to get,
mm x VuFi (u(t),v(t+1))- VuF (u(t),V(t+1)) I
i∈S(t)
≤
2
≤ —
m
VuFi (u㈤,v(t+1)) - VuF (u, V(t+1)) ∣2
δ2 + ρ2EtIVuF u(t), Ve (t+1) I2
To complete the proof, we plug these terms back into the definition of Tk0 and Et [T2,u] to complete
the proof.	□
Claim 13 (Bounding T3,u). We have,
Et [T3,u] ≤ 8γVTT2LvX (1 — -) δVt) + 4χ2γVτ2LvσV (1 - n).
Proof. Since v(t+1) = V(t+1) for i ∈ S(t), We have that
T3,u = ⅛v X||v((t+1)-v((t)||2
i∈S(t)
Since ∣∣V(t+1) - Vf) I is independent of S(t), we can take an expectation to get
Et[T3,u] = XLv XP(i ∈ S⑴)Et||v(t+1) - v(t)||2
n i=1
=χLv (ι - mn )x EtHI)-v("2∙
i=1
Plugging in Lemma 19 completes the proof.
□
27
Under review as a conference paper at ICLR 2022
The analysis of the v-step is given in the next result.
Claim 14. Consider the setting of Theorem 2 and assume that γvτvLv ≤ 1/8. We have,
EthF (u(t),V (t+1)) - F (u(t),V (t))i ≤ - Y T∖δ + *竟 L σ m I 4γ3L2 τ2(τv - 1)σ2m
Proof. From smoothness, we get,
Fi (u(t),V(t+1)) - Fi (u(t)
芯))≤(VvFi (U⑴芯)),针1)
、-----------V-----
T1,v
-*+⅞vM+1)*t)
}	'	Tv
2
}
We bound the first term as
τv -1
Et[T1,v] = -YvEt/ VvFi (u(t),v(tVvFi (U⑴,硝)
k=0
= - γv τv Vv Fi U(t),vi(t)
τv -1
-Y X EtDVvFi (u(t),v* , VvFi (U⑴,喟)-VvFi (〃⑴,。*〉
k=0
≤-Y^Vv Fi (u(t)，v*『+ YvX1Et∣∣Vv Fi (U㈤司)-Vv Fi (u(t),v*『
k=0
≤ - Y2^ VvFi (u(t),ν(t))『+ γv2L2 X1 Mki(t)『.
k=0
Next, we observe that
EzkGi,v	(U,vi, z)k2 =	kVvFi	(U,vi)k2+EzkGi,v	(U,vi, z)-VvFi	(U,vi)k2≤ kVvFi	(U,vi)k2+σv2.
We invoke this inequality to handle the second term as
Et[T2,v] ≤ Y^ X1 EtIlGi,v (u(t),Gt"k)『
k=0
≤ γ⅛2σv2 + γ2L2vτv X1Et∣∣Vv Fi (U㈤司)∣∣2
k=0
≤ γ⅛2σ2 + YvLv/∣VvFi (U(Y))Il2
τv-1	2
+ YvLvTv X Et∣∣VvFi (U⑴司)-VvFi (U(t),ν(t))∣∣
k=0
≤ YvLvTTσv + ,2L τ2∣∣v F	(〃⑶ ν(t))∣∣2 + 2LlTτ	X	E≠∣∣ν(t) -	ν(t)∣∣2
≤	2 十 Yv LvTvIIVv Fi	<U ,νi J ∣ 十 Yv Lv τv	/ J	EtIIvi,k	Ui ∣∣	.
k=0
Plugging these bounds for T1,v and T2,v into the initial smoothness bound and using YvLvTv ≤ 1/4
gives
EthFi (U(t),V(t+1)) - Fi (U(t),ν(t))i ≤
- 学 ∣∣VvFi W,ν(t)) ∣∣2 + YvL2 X1Mk - Ui(t)∣∣2 + y⅛v⅛ .
k=0
We invoke Lemma 18 to bound the Pk Etkv(tk — v(t) k2 term, which is also known as client drift.
We simplify some coefficients using 8Yv Tv Lv ≤ 1 to get
EthFi (U(t),V(t+1)) - Fi (U(t),v(t))i ≤
- 3NvFi (U(t),v(t))∣∣2 + YTL产嘘 +4Y3Lvτ2(τv - 1)σv2.
28
Under review as a conference paper at ICLR 2022
It remains to invoke that S(t) is a uniformly random sample of m devices from {1, ∙∙∙ ,n} and that
V(t+1) is independent of S(t). To this end, note that
EtiF (U⑴,V(t+1)) - F (U㈤,V⑴)]=m Et
m X Fi (u(t),v(t+1))- Fi (u(t),v*
i∈S(t)
n
≤ n XEthFi (u(t),v")- Fi (u(t),v(t))].
i=1
Plugging in the previous bound completes the proof.
□
Remark 15. We only invoked the partial gradient diversity assumption (Assumption 3) at (virtual)
iterates (U(t), Ve (t+1)); therefore, it suffices if the assumption only holds at iterates (U(t), Ve (t+1))
generated by FedAlt, rather than at all (U, V).
A.4 Technical Lemmas
The first lemma involves smoothness of two blocks of variables; we use this in the proof of FedSim.
Lemma 16 (Block Smoothness). Suppose Fi : Rd × Rdi satisfy Assumption 10. Then, it holds that
Fi(w0,v0) - Fi(W,Vi) ≤ hVwFi(w,Vi),w0 - Wi + hVvFi(w,vi),vi0 - Vii
+ Lw (1+ χ2)kw0 - Wk2 + Lv (1+χ2)kv0-Vik2.
Proof. Using the Lw-smoothness of F(∙, v0) and the Lv-smoothness of F(w, ∙), We have
Fi(w',V) - Fi(w,vi0) ≤ hVw Fi(w,vi),w0 - Wi + Lw ∣∣w0 - w∣∣2,
Fi(w, v0) - Fi(WNi) ≤ hVwFi(Wjvi),V - Vii + 勺l∣vi - Vik2.
Summing the above tWo inequalities together gives
Fi (W0, Vi0 ) - Fi (W, Vi) ≤ hVwFi(W, Vi0 ), W0 - Wi + hVvFi (W, Vi), Vi0 - Vii
+ ~wkkW0 - Wk2 + ~2kvi - vik2 .	(16)
We can bound the first inner product term on the right-hand side of the above inequality as
hVwFi(W, Vi0), W0 - Wi = hVw Fi (W, Vi), W0 - Wi + hVw Fi (W, Vi0) - VwFi(W, Vi), W0 - Wi
≤ hVwFi(W, Vi), W0 - Wi + kVwFi(W, Vi0) - VwFi(W, Vi)kkW0 - Wk
≤ hVwFi(W, Vi), W0 - Wi + LwvkVi0 - VikkW0 - Wk
≤ hVwFi(W,Vi), W0 - Wi + XVZLwLv kvi - VikkWO - Wk
≤ hVwFi(W,Vi),W0 — Wi + χ2LvIIv0 — 3∣∣2 + χ2Lw|IW0 — Wll2,
Where the first inequality is due to Cauchy-SchWarz, the second inequality is due to Lwv-Lipschitz
property of VWFi(W, ∙), the third inequality is due to the definition of X in (5), and the last inequality
is due to Young,s inequality. Substituting the above inequality into (16) yields the desired result. 口
Next, We have the variance of sampling Without replacement. Note the correction factor of (n -
m)/(n - 1) over sampling With replacement. We include the elementary proof for completeness.
Lemma 17 (Sampling Without Replacement). Let aι, ∙∙∙ ,an ∈ Rd be given. Let S be a uniformly
random sample of size m from this collection, where the sampling is without replacement. Denoting
the mean a = En=I ai/n, we have,
ES
X	ai — a
m
i∈S
2
≤
29
Under review as a conference paper at ICLR 2022
Proof. The statement is trivially true if m = 1 or m = n. Therefore, we assume now that 2 ≤ m ≤
n - 1. Further, without loss of generality, We assume that a = 0. Finally, let S denote the set of all
subsets of [n] of size m. Note that |S| = mn . We now have,
ESm X J 错 X(Xkaik2 +	X h").
i∈S	m m S∈S i∈S	i,j∈S: i6=j
For the first term, we have,
nn
XXkaik2=X X kaik2=	mn--11 Xkaik2.
S∈S i∈S	i=1 S∈S: i∈S	i=1
Likewise, for the second term, we use j 6=i aj = -ai to get,
i,j∈S: i6=j
n
hai,aji=	hai,aji
i=1 j6=i S∈S : i,j ∈S
mn--22Xn Xhai,aji=-mn--22Xn kaik2.
i=1 j6=i	i=1
Therefore, we get,
ES ~~ X
m
ai
i∈S
2	(n-1) - (n-2
m-1	m-2
m2 cm
n	(n-2λ	n
X kaik2=⅛⅛ X kaik2
n- m
mn(n - 1)
n
Xkaik2.
i=1
□
The next two lemmas are about the effect of the local updates in the local SGD literature. The first
lemma has also appeared in (Karimireddy et al., 2020); we give the proof for completeness.
Lemma 18. Consider f : Rd → R which is L-smooth and fix a w(0) ∈ Rd. Define the sequence
(w(t)) of iterates produced by stochastic gradient descent with a fixed learning rate γ starting from
w(0):
w(t+1) = w(t) - γg(t) ,
where g(t) is an unbiased (and independent of W) estimator of Vf (W) with bounded variance σ2.
Fix a number T of steps. If Y ≤ (√2τL)-1, we have the bound
τ-1
XkW(t) -
W(0)k2 ≤ 8γ2τ 2(τ - 1)kVf (W(0))k2 + 4γ2τ2(τ - 1)σ2 .
t=0
Proof. If τ = 1, we have nothing to prove. Assume now that τ ≥ 2. Let F(t) be the sigma-algebra
generated by w(t) and denote EtH = E[∙ |F(t)]. We will use the inequality
Etg(t)2	=Etg(t)	-Vf(W(t))2+	Vf(W(t))2 ≤	σ2	+	Vf (W(t))2.	(17)
We now successively deduce,
EtkW(t+1) - W(0)k2 = kW(t) - W(0) - γg(t)k2
(≤)(l + 占)kw(t)-W(O)112 + Y2τ Etkg(t)k2
≤ (l +----^ɪ) kw(t) — W(O)Il2 + 2γ2τ∣∣Vf(w(t)) — Vf(W(O))k2 + 2γ2τIlVf(W(O))k2 + γ2τσ2
≤(1+------j- + 2γ2τL2) ∣W(t) — W(O)∣∣2 + 2γ2τIlVf(W(O))∣2 + γ2τσ2
≤(1 +-----^^y) ∣W(t) — W(O)Il2 + 2γ2τIlVf(W(O))∣2 + γ2τσ2 .
30
Under review as a conference paper at ICLR 2022
Above, We used (a) the inequality 2αβ ≤ α2∕δ2 + δ2β2 for reals α,β,δ, (b) Eq. (17), (c) L-
smoothness of f, and, (d) the condition on the learning rate.
Let C = 2γ2τ∣∣Vf (w(0))k2 + γ2τσ2. Unrolling the inequality and summing up the series gives for
all t ≤ τ - 1
kw⑴-W⑼k2 ≤ CX(1 + τ41)j ≤ C2(τ -1)(1 + τ41)t
≤ C2(T- 1) (1 + 7-i)	≤ C(T- 1)e2,
where we used the bound (1 + 1∕α)α ≤ e for all α > 0. Summing over t and using the numerical
bound e2 < 8 completes the proof.	口
Lemma 19. Consider the setting of Lemma 18. If γ ≤ (2T L)-1, we have the bound
kw(τ) - w(0)k2 ≤ 16γ2T2kVf(w(0))k2 + 8γ2T 2σ2 .
Proof. Proceeding similar to the last proof (expect using δ = T) gives us
Et∣∣w(t+1) - W(O)I ≤ (l + -) ∣∣w(t) - W(O)I + 4γ2τ∣∣Vf (w(0))U + 2γ2τσ2
Unrolling and summing up the sequence completes the proof, similar to that of Lemma 18. 口
Finally, the last lemma is about constants.
Lemma 20. Let γu , γv , Lw , Lv , χ, f ∈ R+ and a natural number T be given. Denote
A := γuL2u + fγvχ2LuLv , and, B := fγvLv2 + γuχ2LuLv .
Suppose γu = cu/(TLu) and γv = cv/(TLv) with cu, cv > 0 satisfying
cu,cv ≤ √16max{1,X-2}.
Then, we have that
γv2χ2LuLvB + γu2L2uA ≤ A/(3T2) , and,	γu2χ2LuLvA + γv2L2vB ≤ B/(3T2) .
Proof. Note that it suffices to show
3T2χ2γv2LuLvB ≤ A/2 ,	and,	3T2χ2γu2LuLvA ≤ B/2 .
Plugging in γu , γv , these are equivalent to
6χ2fc3v +6χ4cv2cu ≤ χ2fcv + cu and,	6χ2c3u + 6χ4 fcv c2u ≤ fcv +χ2cu .
The assumption on cv implies that 6χ2 fcv3 ≤ χ2fcv and 6χ4 c2v cu ≤ cu. Therefore, the first condition
holds. Similarly, the second condition holds too.	口
B Experiments: Detailed S etup and Hyperparameters
We conduct our experiments on three datasets from two modalities, namely images and text. The
datasets contain a natural, non-iid split of data which is reflective of data heterogeneity encountered
in federated learning. We describe in detail the experimental setup and hyperparameters. The code to
reproduce the experimental results will be publicly released.
The outline of this section is:
•	§B.1 describes the tasks and their associated datasets and metrics.
•	§B.2 describes the experimental pipeline as well as the baselines we compare to.
•	§B.3 presents the hyperparameters of all the algorithms.
As discussed in §1, we take the weight αk to be proportional to the number of datapoints available on
the device.
31
Under review as a conference paper at ICLR 2022
Figure 5: Distribution of number of training samples per device for each of the tasks considered in the
experiments. For GLDv2, we do not show the long right tail, where the maximum number of data points per
device is 1000 (cf. Table 2).
B.1	Datasets, Tasks and Models
We consider three tasks motivated by real-world applications of federated learning. The tasks are
summarized in Table 2 of the main paper and the distribution of data across the clients is visualized
in Figure 5.
For each model, we consider three partial personalization architectures:
(a)	Input layer personalization: Motivated by Liang et al. (2019), this architecture places the first
layer on-device to learn a personalized representation per-client, while the rest of the model
is shared. For the transformer model, we use the first transformer layer in place of the word
embedding layer owing to its large size.
(b)	Output layer personalization: Motivated by Collins et al. (2021), this architecture learns a
shared global representation but personalizes the prediction layer. For the transformer model, we
use the last transformer layer in place of the last prediction layer owing to its large size.
(c)	Adapter personalization: We also consider a novel partial personalization architecture, where
the full model is shared among all clients, while each client adds personalized adapter modules,
which are lightweight modules added between layers of the shared model. We use the transformer
adapters proposed by Houlsby et al. (2019) and residual adapters proposed by Rebuffi et al.
(2017).
B.1.1	S tackOverflow for Next Word Prediction
Dataset. The StackOverflow dataset comprises of questions and answers from the programming
question-answer website stackoverflow.com. The goal of the next word prediction task is to predict
the next word given a partial sequence of words in a question or answer. This task is a good open-
source benchmark for next word predictions in mobile keyboards. We use the StackOverflow dataset
provided by TensorFlow Federated.
Client Distributions. Each client corresponds to one user on Stack Overflow; the data on the client
corresponds to the questions and answers posted by this user. We only consider clients with at least
100 training sequences and 10 testing sequences, where a sequence refers to either a question or an
answer. We use a fixed subsample of 1000 of them. Following Reddi et al. (2021), we restrict the
vocabulary to the top 10000 most frequently occurring words in the dataset. We pad and truncate each
sequence of each client to length 20 and consider at most 1000 training sequences on each client.
Model. We use a transformer model (Vaswani et al., 2017) commensurate in size with BERT
Mini (Turc et al., 2019). It has with 4 transformer blocks and 4 attention heads in each self-attention
layer with a transformer hidden dimension of 256 and a fully-connected hidden dimension of 1024.
The output layer is a causal language modeling head, i.e., a fully connected layer which assigns
32
Under review as a conference paper at ICLR 2022
Table 5: Summary of partial personalization architectures for the transformer model for next word prediction.
Personalization Type	Layer on-device	# Personalized Params.	# Shared Params.
Input Layer	1st transformer block	0.8M	4.9M
Output Layer	Last transformer block	0.8M	4.9M
Adapter	Adapter modules	0.07M	5.7M
Table 6: Summary of partial personalization architectures for the ResNet-18 model for visual landmark
recognition.
Personalization Type	Layer on-device	# Personalized Params.	# Shared Params.
Input Layer	1st conv. layer	0.01M	12.2M
Output Layer	Last fully connected layer	1M	11.2M
Adapter	Residual adapter modules	1.4M	12.2M
a score for each possible vocabulary item, including the special tokens. The model has 6 million
parameters, which require around 23 megabytes of memory.
Partial Personalization Architecture. The partial personalization architectures used are summa-
rized in Table 5.
Loss Function and Evaluation Metric. We train the model with the causal language modeling
objective. That is, for each partial sequence, we treat the prediction of the next word as a multiclass
classification problem to minimize the multinomial logistic loss, also known as cross entropy loss.
For evaluation, we use the top-1 accuracy of predicting words in the proper 10000-word vocabulary
(i.e., ignoring special tokens such as padding, out-of-vocabulary, and beginning/end of sequence).
B.1.2	GLDv2 for Visual Landmark Recognition
Dataset. GLDv2 stands for Google Landmarks Dataset v2 (Weyand et al., 2020), which is a large-
scale image dataset. It contains images of popular landmarks from around the world taken and
uploaded by Wikipedia contributors. While the images vary in size, the most common image size is
800 × 600 pixels.
The goal of the visual landmark recognition task is to identify the landmark from its image. This
task resembles a scenario where smartphone users take photos of natural and architectural landmarks
while traveling. We use the federated version of the GLDv2 dataset introduced by Hsu et al. (2020)
with 2028 landmarks and provided by TensorFlow Federated.
Client Distributions. Each client corresponds to one Wikipedia user and contains all the images
contributed by that user. We only all 823 clients with at least 50 datapoints. We do not use original
test set from GLDv2 from evaluation as it comes from different clients. Instead, we take 50% of the
data on each client as a testing set.
Model. We use a ResNet-18 (He et al., 2016) model pretrained on ImageNet (Deng et al., 2009),
with group normalization instead of batch normalization (Hsieh et al., 2020). We resize all images to
224 × 224. We use two data augmentations for training: a random crop from 256 × 256 and a random
horizontal flip. The model has 12 million parameters, which require around 49 megabytes of storage.
Partial Personalization Architecture. The partial personalization architectures used are summa-
rized in Table 6.
Loss Function and Evaluation Metric. We use the multinomial logistic loss, also known as cross
entropy loss. We evaluate the performance of the model using its classification accuracy.
33
Under review as a conference paper at ICLR 2022
Table 7: Summary of partial personalization architectures for the ResNet-18 model for character recognition.
Personalization Type	Layer on-device	# Personalized Params.	# Shared Params.
Input Layer	1st conv. layer	0.7K	11.2M
Output Layer	Last fully connected layer	0.03M	11.2M
Adapter	Residual adapter modules	1.4M	11.2M
B.1.3	EMNIST for Character Recognition
Dataset. EMNIST (Cohen et al., 2017) is a character recognition dataset. The goal is to identify
images of handwritten digits or letters; there are 62 possible options (a-z,A-Z, 0-9). The images are
grey-scaled pictures of 28 × 28 = 784 pixels. We use the EMNIST dataset provided by TensorFlow
Federated.
Client Distributions. Each client corresponds to one “writer”, i.e., the human subject who hand-
wrote the digit/letter during the data collection process. We only use those clients with at least 100
training points and 25 testing points: there are 1114 of such clients.
Model. We use a ResNet-18 (He et al., 2016) model with group normalization instead of batch
normalization (Hsieh et al., 2020). We make two modifications to handle the smaller image size
(28 × 28 × 1 as opposed to the 224 × 224 × 3 which the original ResNet was designed to accept): (a)
we use a convolutional kernel of size 3 × 3 rather than the original 7 × 7 in the first convolution layer,
and, (b) we drop the first pooling layer. The model has 11 million parameters, which require around
45 megabytes. Note that the number of parameters in this ResNet is smaller than the one for GLDv2
due to the architectural modifications we make for smaller images as well as the smaller number of
classes.
Partial Personalization Architecture. The partial personalization architectures used are summa-
rized in Table 7.
Loss Function and Evaluation Metric. We use the multinomial logistic loss, also known as cross
entropy loss. We evaluate the performance of the model using its classification accuracy.
B.2 Experimental Pipeline and Baselines
There are three components in the training pipeline for all experiments:
(a)	Non-personalized federated training: The first step involves training a global model wg using the
one-model-fits-all approach of (1) with FedAvg variants.
(b)	Personalized federated training: This optional second step involves training the shared parameters
w together with the personalized parameters vk using a personalized federated learning approach.
We warm-start w, vk from the non-personalized model wg from the previous step.
(c)	Final finetuning: The last step involves only finetuning the personalized parameters vk while the
shared parameters w remain unchanged.
For step (b), we initialize vk for each k to be the appropriate part of wg for input/output layer
personalization. On the other hand, for adapters, we initialize vk to be equal to the same set of
randomly initialized weights for each device k.
We consider the following baselines:
•	Non-personalized: This denotes the performance of step (a) of the pipeline above, i.e., non-
personalized federated training with FedAvg variants.
•	Full model personalization: We consider three baselines of personalization of the full model:
(i)	Finetune: The non-personalized model from step (a) of the pipeline above is finetuned
locally on each client (step (c) of the pipeline). Step (b) is skipped for this baseline.
34
Under review as a conference paper at ICLR 2022
Table 8: Hyperparameters for each dataset/task.
	Hyperparameter	StackOverflow	GLDv2	EMNIST
Common	Batch size Devices per round Local epochs Server Optimizer Client Optimizer Global Scheduler Warm up LR decay rounds Max. grad. norm.	64 50 1 FedAdam SGD Linear 10% of rounds N/A 0.1	64 50 1 FedAdam SGD Linear 10% of rounds N/A N/A	32 10 1 FedAvg SGD Exponential N/A 500 N/A
Non-personalized training (step (a) of the pipeline)	# Rounds Server learning rate Client learning rate	1000 5 × 10-4 1	2500 2 × 10-4 10-2	2000 1.0 0.5
Personalized training (step (b) of the pipeline)	# Rounds Server learning rate Client learning rate	500 5 × 10-5 10-1	600 2 × 10-5 10-3	500 1.0 10-2
Local finetuning (step (c) of the pipeline)	#Epochs Optimizer Client learning rate	5 SGD 10-1	5 SGD 10-3	5 SGD 10-2
(ii)	Ditto (Li et al., 2021): The non-personalized model from step (a) of the pipeline above is
finetuned locally on each client (step (c) of the pipeline) with `2 regularization kv - wgk2.
Step (b) is skipped for this baseline.
(iii)	pFedMe (Dinh et al., 2020): The non-personalized baseline model from step (a) is trained
further in step (b) to optimize (2) using the pFedMe algorithm of Dinh et al. (2020). Finally
the resulting model w is finetuned locally in step (c).
• Partial Model Personalization: We consider partial model personalization with three different
architectures, as defined in §B.1. For each personalization approach, we start with the non-
personalized model in step (a), continue personalization in step (b) using either FedAlt or FedSim
as the algorithm, and finally run step (c) for the local finetuning.
B.3 Hyperparameters and Evaluation Details
All the tuning of hyperparameters was performed on validation data, formed by holding out 20% of
the training data on each device. Once the tuning was complete, we reran the experiments on the full
training data, including those held out for validation.
Evaluation Metric. Our primary evaluation metric is the weighted average of the test accuracy on
each client, weighted by the number of test examples (the details of how the accuracy is computed
on each dataset is given in §B.1 in the paragraph on “Loss Function and Evaluation Metric”). This
corresponds to the unweighted accuracy obtained by pooling all the data locally, similar to the loss as
discussed in §1. The same metric is used for hyperparameter tuning and is reported in all the tables
and plots, unless explicitly noted otherwise.
The final hyperparameters we use are given in Table 8.
Rounds. We start with the number of communication rounds (i.e., the number of calls to secure
aggregation routine for the shared parameters), which is used to measure the progress of each
algorithm. For the non-personalized training, we use 1000 rounds for StackOverflow, 2500 rounds
for GLDv2 and 2000 rounds for EMNIST. For the personalized training, we warm-start the model
from the non-personalized one, and run the training for 500 rounds for StackOverflow and EMNIST
and 600 rounds for GLDv2.
Devices per Round. All devices are assumed to be available and selections are made uniformly at
random. Following (Reddi et al., 2021; Weyand et al., 2020), we select 50 devices per round for
35
Under review as a conference paper at ICLR 2022
Table 9: This table shows Table 4 of the main paper along with the corresponding standard deviations
as subscripts. We compare FedAlt and FedSim for partial model personalization in this table. “FT (part.)”
corresponds to finetuning the personal parameters vi locally while fixing the shared parameters u from a non-
personalized training. The numbers are averaged over 5 random seeds; the boldfaced numbers denote those
within 1 standard deviation of the highest accuracy in each row.
	StackOverflow	GLDv2	EMNIST FT (part.) FedAlt	FedSim	FT (part.) FedAlt	FedSim	FT (part.) FedAlt	FedSim
Input Layer Output Layer Adapter	24.96o.oι	24.44o.oι	24.81o.oι	51.97o.o2	53.94o.o6	53.64o.o8	93.29o.oo	93.62o.o3	93.55o.o5 24.93o.oι	25.05o.oι	25.02o.oι	53.21o.oι	56.64o.o5	56.24o.o4	93.37o.oι	93.57o.o4	93.55o.o5 24.71o.oo	24.82o.oι	24.74o.oι	63.86o.o6	66.41o.o5	66.35o.o3	93.66o.oo	94.13o.o3	94.07o.o3
StackOverflow/GLDv2 and 10 per round for EMNIST, for both the non-personalized as well as the
personalized training.
Local Updates and Minibatch Size. Each selected device locally runs 1 epoch of mini-batch
stochastic gradient descent locally for non-personalized as well as personalized federated training.
The final finetuning at the end of personalized training is performed for 5 epochs. We use a minibatch
size of 64 for StackOverflow/GLDv2 and 32 for EMNIST for all settings.
Server and Client Optimizer Details. We use FedAvg for EMNIST and FedAdam (Reddi et al.,
2021) for StackOverflow and GLDv2. We also use a global scheduler, which applies a schedule on the
client learning rates across rounds, while the client learning rate within each round is held constant.
We use either a linear scheduler or an exponential scheduler (also called “stepLR” in PyTorch). A
linear scheduler applies a linear warmup, if applicable, until the maximum learning rate followed by
a linear decay to 0. An exponential scheduler halves the client learning rate once every fixed number
of rounds. Both the client and server learning rates are tuned using the validation set.
Regularization Coefficient for pFedMe and Ditto. We tune the regularization coefficient λk = λ
for PFedMe and Ditto using the validation data from the set {10-4,10-3,…,100} of possible
values. The tuned values are:
•	StackOverflow: 10-3 for Ditto and 10-4 for pFedMe,
•	GLDv2: 10-1 for both Ditto and pFedMe,
•	EMNIST: 10-1 for both Ditto and pFedMe.
Random Seed. We report numbers averaged over 5 random seeds.
C Experiments: Additional Results
We now present the detailed experimental results.
We start with Table 9, which shows the standard deviations of Table 4. We see that the numbers are
fairly consistent across runs so the standard deviation is quite small (0.01 to 0.05 percentage points).
C.1 Ablation: Final Finetuning for FedAlt and FedSim
We now study the effect of the final finetuning (step (c) of the experimental pipeline; cf. §B.2) for
FedAlt and FedSim.
The final finetuning has a minimal impact on partial personalization. We see from Table 10 that
the effect of the final finetuning is much smaller than the improvements from personalization. For
instance, the improvements from finetuning are close to 0 for FedAlt on the StackOverflow dataset.
For GLDv2, the finetuning accounts for < 0.5pp of improvement, whereas personalization overall
accounts for 5 to 15pp.
The final finetuning is more important to FedSim than FedAlt. Table 10 also shows that the final
finetuning helps FedSim more than FedAlt. However, FedAlt still outperforms FedSim, as we saw in
36
Under review as a conference paper at ICLR 2022
Table 10: The change in accuracy (percentage points) from the final finetuning for FedAlt and FedSim with
stateful devices. The subscript denotes the standard deviation over 5 random seeds.
StackOverflow	GLDv2	EMNIST
	FedAlt	FedSim	FedAlt	FedSim	FedAlt	FedSim
Input Layer	-0.060.01	0.040.02	0.12o.o2	0.170.03	0.120.01	0.120.03
Output Layer	0.000.01	0.250.02	0.490.02	0.570.03	0.090.01	0.090.03
Adapter	0.010.01	0.400.08	0.140.02	0.170.01	0.270.02	0.330.03
Per-client Statistics (train)	Per-client Statistics (test)	Effect of Regularization (test)	Effect of Dropout (test)
>urabuu<<
>urabuu<<
5 0 5 0
2 2 11
>urabuu<<

Ditto pFedMe Partial	Ditto pFedMe Partial	No Reg. Best Reg. Large Reg.	No d/o Best d/o Large d/o
Figure 6: Left two: Distribution of change in the per-device train (left most) and test (center left) accuracy
due to personalization on the StackOverflow dataset. Right two: Distribution of change in the per-device test
accuracy of partial personalization under regularization on the StackOverflow dataset: (a) center right: adapter
personalization under `2 regularization, and, (b) rightmost: output layer personalization under dropout. Note
that the “No Reg.” and “No d/o” plots on the right two are different because they personalize different model
parts. Interpretation: The white dot in inside the violin denotes the median, while the black box enclosing this
white dot marks the interquartile range (i.e., 25th and 75th percentiles). The body of the violin is a kernel density
estimate of the distribution of accuracies. The lines extend out to the minimum and maximum accuracy in each
case.
Table 9. Overall, this shows that FedAlt is a better algorithm than FedSim. The final finetuning helps
FedSim make up some percentage points in accuracy, but not enough to make up its gap with FedAlt.
C.2 Effect of Personalization on Per-Device Generalization
Summary of all scatter plots. All the scatter plots shown in the main paper are summarized in the
violin plot of Figure 6. We see from the leftmost figure that the training accuracies on all devices
improve with personalization. From the second figure, we see that the test accuracy of some of the
devices reduces with personalization; this is true for both partial and full personalization.
From the third plot of Figure 6, we see that regularization does not mitigate this overfitting. In
fact, the regularization tuned for best average accuracy leads to a nearly identical distribution of test
accuracies. A larger regularization reduces the spread of accuracies, but does so at the expense of a
smaller median (white dot). The fourth plot of Figure 6 shows that the effect of dropout is similar.
The best dropout improves the median accuracy, but it does not mitigate the issue of some devices
being hurt by personalization.
Train Accuracy plots for devices. From Figure 7, we see that personalization leads to a reduction in
test accuracy on some of the devices beyond the initial non-personalized model. The corresponding
train accuracy plot is given in Figure 7. We observe that the personalization always leads to an
improvement in the training accuracy but not in the test accuracy. The analogous plots for GLDv2
are in Figure 8, where the trends are similar.
Whether personalization helps a device or not depends on the random seed. We see in Figure 10
that the shaded region for some of the devices intersects the dotted line at 0. In other words,
personalization sometimes helps this device and sometimes hurts it, depending on the random seed.
This indicates that the best fix in practice is to use A/B testing on the deployed model to choose
whether to use the personalized model or the non-personalized one.
37
Under review as a conference paper at ICLR 2022
>UE□UU< V
Fmetune
Ditto
Partial
108 6 4 2
>U2□UU<<
2500 5000 7500 10000 12500 15000
# Data per device
Ditto
2500 5000 7500 10000 12500 15000
# Data per device
Finetune
2500 5000 7500 10000 12500 15000
# Data per device
pFedMe
2500 5000 7500 10000 12500 15000
# Data per device
Partial
>U23U<<
Figure 7: Scatter plot of change in accuracy (pp) per-device versus the number of training samples on the
device for StackOverflow. Top: Training accuracy. Bottom: Test accuracy. This is the full version of Figure 4
from the main paper.
—4.......	......	......	......
2500 5000 7500 10000 12500 15000	2500 5000 7500 10000 12500 15000	2500 5000 7500 10000 12500 15000	2500 5000 7500 10000 12500 15000
# Data per device	# Data per device	# Data per device	# Data per device
Fmetune
40
30
20
10
Ditto
p FedMe
Partial
100
200	300	400
# Data per device
500
100	200	300	400	500
# Data per device
Ditto
Finetune
40
100	200	300	400	500
# Data per device
pFedMe
100	200	300	400	500
# Data per device
Partial
3°Mloo
>UE□UU<<
-io
100	200	300	400	500 IOO 200	300	400	500	100	200	300	400	500	100	200	300	400	500
# Data per device	# Data per device	# Data per device	# Data per device
Figure 8: Scatter plot of change in accuracy (pp) per-device versus the number of training samples on the
device for GLDv2. Top: Training accuracy. Bottom: Test accuracy.
Regularization and dropout do not mitigate this issue. From the first row of Figure 9, we see
that the weight decay with best mean accuracy exactly matches the unreguarlized case in terms
of per-device statistics. Increasing the regularization weight can reduce the spread of per-device
accuracy. However, this only leads to a worse mean accuracy and does not mitigate the issue of
personalization hurting individual devices.
From the second row of Figure 9, we see that the best dropout (0.3 in this case) leads to slight increase
in average accuracy (0.18 pp). It also reduces the number of devices hurt by personalization from 256
out of 1000 to 193, but it does not fix this issue. Increasing dropout further only leads to a degradation
of per-device statistics.
C.3 Partial Personalization for S tateles s Devices
The algorithms we considered in this paper, namely FedAlt and FedSim, require the devices to
maintain the personalized parameters vi ’s as state across rounds. In cross-device federated learning
settings, it is also interesting to consider stateless devices, which are not allowed to maintain state
between training rounds.
38
Under review as a conference paper at ICLR 2022
Reg. Param. = 0
Reg. Param. = best
Reg. Param. = large
Pers. helps
Pers. hurts
2000 4000 6000 8000 10000120001400016000 2000 4000 6000 B000 100001200014000 16000 2000 4000 6000 θ000 10000120001400016000
# Data per device	# Data per device	# Data per device
Dropout = 0
Dropout = best
Dropout = large
Figure 9: Scatter plot of change in accuracy (pp) per-device versus the number of training samples on the
device with the effect of regularization. Top: '2 regularization a.k.a. weight decay. Bottom: dropout. The “best”
values of the '2 regularization parameter and dropout are chosen to maximize the average test accuracy across
all devices.
2000 4000 6000 8000 10000120001400016000 2000 4000 6000 B000 100001200014000 16000 2000 4000 6000 θ000 10000120001400016000
# Data per device	# Data per device	# Data per device
Rnetune	Ditto
pFedMe	Partial
6 4 2 0 r
AUeJnUU4 V
O 200 400 600 800 IOOO
Device Rank
O 200 400 600 800 IOOO
Device Rank
O 200 400 600 800 IOOO O
Device Rank
200 400 600 800 IOOO
Device Rank





Figure 10: Change in per-device accuracy (PP) due to personalization. The solid line is the mean over 5 random
runs and the shaded area denotes the max/min across these runs. The devices are sorted in ascending order of
accuracy change. The points in orange depict two example devices who might either be helped or harmed by
personalization depending on the random seed.
We give preliminary experiments in this setting. We modify the FedAlt and FedSim algorithms from
the main paper so that the personalized parameters vi are reinitialized each time device i is chosen
for participation. We warm-start vi from the appropriate part of the non-personalized model trained
in step (a) of the pipeline. For adapters, we fix a random initialization once, and reuse it.
FedAlt is better than FedSim for stateless devices, although the improvement is smaller. We
see from Table 11 that all algorithms perform similarly for the stateless setting. Nevertheless, we see
that FedAlt obtains mild improvements over both FedSim and finetuning for GLDv2, e.g., 0.24pp
with adapters.
The final finetuning is crucial for stateless devices. We see from Table 12 that the final finetuning
accounts for most of improvements in the stateless case. For instance, for GLDv2, the final finetuning
accounts for 11.68 and 10.42pp out of a total of 12.67 and 11.76pp for FedAlt and FedSim respec-
tively. However, the personalized federated training (step (b) of the pipeline; cf. §B.2) still leads to
an increase in accuracy of 1 to 1.34pp.
39
Under review as a conference paper at ICLR 2022
Table 11: This is the counterpart of Table 4 to stateless devices. We compare FedAlt and FedSim for partial
model personalization with stateless devices. “FT (part.)” corresponds to finetuning the personal parameters vi
locally while fixing the shared parameters u from a non-personalized training. The numbers are averaged over 5
random seeds; the boldfaced numbers denote the highest accuracy in each row.
	StackOverflow			GLDv2			EMNIST		
	FT (part.)	FedAlt	FedSim	FT (part.)	FedAlt	FedSim	FT (part.)	FedAlt	FedSim
Input Layer	24.96o.0i	24.84o.oι	24.890.01	51.97o.02	52.76o.06	52.740.02	93.290.00	93.510.03	93.480.04
Output Layer	24.93o.0i	24.94o.0i	24.940.01	53.210.01	53.300.06	53.300.08	93.370.01	93.530.03	93.510.04
Adapter	24.71o.0θ	24.69o.oι	24.710.01	63.860.06	64.100.14	63.190.04	93.660.00	93.970.04	93.890.02
Table 12: The change in accuracy (percentage points) from the final finetuning for FedAlt and FedSim with
stateless devices. The subscript denotes the standard deviation over 5 random seeds.
StackOverflow	GLDv2	EMNIST
	FedAlt	FedSim	FedAlt	FedSim	FedAlt	FedSim
Input Layer	0.860.03	1.000.02	0.440.03	0.420.03	0.110.02	0.100.04
Output Layer	1.080.03	1.100.02	1.470.04	1.460.05	0.150.02	0.110.02
Adapter	0.840.04	0.880.02	ll.680.20	10.420.09	0.460.02	0.420.04
40