Under review as a conference paper at ICLR 2022
EqR: Equivariant Representations for
Data-Efficient Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
We study different notions of equivariance as an inductive bias in Reinforcement
Learning (RL) and propose new mechanisms for recovering representations that
are equivariant to both an agent’s action, and symmetry transformations of the
state-action pairs. Whereas prior work on exploiting symmetries in deep RL can
only incorporate predefined linear transformations, our approach allows for non-
linear symmetry transformations of state-action pairs to be learned from the data
itself. This is achieved through an equivariant Lie algebraic parameterization of
state and action encodings, equivariant latent transition models, and the use of
symmetry-based losses. We demonstrate the advantages of our learned equivari-
ant representations for Atari games, in a data-efficient setting limited to 100K
steps of interactions with the environment. Our method, which we call Equiv-
ariant representations for RL (EqR), outperforms other comparable methods on
statistically reliable evaluation metrics.
1	Introduction
The recent success of deep reinforcement learning (Francois-Lavet et al., 2018) in applications to
games such as Atari (Mnih et al., 2015), Go (Silver et al., 2016) and Poker (Brown & Sandholm,
2019), to applications in robotics (Levine et al., 2016) and autonomous navigation (Bellemare et al.,
2020) has demonstrated its promise as the framework of choice for sequential decision making.
However, the use of a reward as the only signal for representation learning with high dimensional
states and actions leads to tremendous data inefficiency. Notably, almost all success stories of RL
rely on vast amounts of data or simulations with a huge computational overload.
More data-efficient representation learning (Bengio et al., 2013) requires stronger inductive biases,
though the search for a general yet strong inductive bias is still under way. One general approach
is to place a central role on transformations of the data, where invariance and equivariance to a
set of transformations imposes strong conditions on the learned representations. This viewpoint is
particularly appealing in RL, where the agent is in control of some of these transformations through
its own actions. Fig. 1 illustrates this concept using the example of a 2D pendulum. Moreover,
(a)	(b)
Figure 1: An illustration of typical symmetries in a pendulum, and the corresponding transformations of the
state and action for a group equivariant transition model: (a) shows how reflection of the agent’s state results in
a permutation of the action, denoted by a-1 . (b) shows how rotation of the agent’s state results in invariance of
the action in the absence of gravity. The state transitions can be modeled as group actions (2D rotations in this
example), which can be captured by our symmetry transformation-based transition model. Note that rotational
symmetry can hold even when gravity is present. In this case, symmetry transformations include rotations (and
reflections) that preserve the Hamiltonian. Such non-linear energy-preserving transformations of state-actions
in the pixel space can become linear in the embedding space.
1
Under review as a conference paper at ICLR 2022
transformations naturally lead to a notion of disentanglement in the representations (Higgins et al.,
2018), potentially enabling better out-of-distribution generalization (Higgins et al., 2017; Thomas
et al., 2017). The recent success of self-supervised learning approaches that rely on a (predefined)
set of transformations (Chen et al., 2020; Zbontar et al., 2021), and also within the context of RL
(Yarats et al., 2021; Laskin et al., 2020b), further highlights the importance of transformations in
data-efficient representation learning.
Motivated by these observations, this work develops a broader perspective on the notion of equiv-
ariant representation learning within RL. In particular, we integrate equivariance under the agent’s
action and equivariance under the symmetries of the environment into a single latent variable model
that is equivariant to an a priori unknown group of non-linear transformations of state-action pairs.
In contrast to the traditional approach of using symmetric Markov Decision Processes (MDPs), we
argue for modeling the larger group of state-action symmetries (separate from reward symmetries),
and show how to parameterize the latent embeddings of states and actions to make the representa-
tions equivariant to continuous transformations of the environment resulting from agent’s action. We
benchmark our approach, which we call Equivariant representations for RL (EqR), on the 26 games
in the Atari 100K benchmark (Kaiser et al., 2019), where we outperform other comparable methods
using reliable evaluation metrics (Agarwal et al., 2021). Our approach, however, is not restricted to
this domain. It is applicable in any setting where the transformations that an agent undergoes can be
expressed using matrix Lie groups, including autonomous driving, navigation, and robotics.
2	Related work
The use of transformations, be it in data-augmentation or self-supervision, has become a common
ingredient in recent representation learning methods for deep RL. However, theoretical work on
symmetry in RL goes back to Zinkevich & Balch (2001) and Ravindran & Barto (2001), both of
which use symmetric MDPs. A more recent use of this formalism is in van der Pol et al. (2020b);
Mondal et al. (2020), where policy networks, with built-in equivariance, are shown to improve data-
efficiency. Closely related notions, that motivated the early work on symmetric MDPs, are model
minimization (Ravindran & Barto, 2002), state abstraction (Ravindran & Barto, 2003; Li et al.,
2006), MDP homomorphism (Ravindran & Barto, 2004) and lax bisimulations (Taylor, 2008). In
particular, MDP homomorphism, which requires equivariance under an agent’s action, encompasses
the general idea of model-based reinforcement learning. As examples, a latent MDP that matches
the state dynamics and the reward distribution of the environment is learned in (van der Pol et al.,
2020a; Gelada et al., 2019).
Other works in RL that are relevant to our objective are those that attempt to increase data-efficiency
using a learned model of the environment. While some methods such as SimPLe (Kaiser et al.,
2019), learn this transition model at the pixel level, the majority of methods use a latent space
model. The latent space is either learned using reconstruction (Hafner et al., 2019a;b), or through
self-supervision and contrastive methods (CURL, Laskin et al., 2020b). However, there is evidence
that the improvement in sample efficiency is largely due to image augmentation, as seen in Laskin
et al. (2020a) and DrQ (Yarats et al., 2021). Using a reconstruction-based method is also inefficient
because similar to pixel level models, one needs to learn potentially irrelevant details. The fact
that variations of model-free algorithms such as Data-Efficient Rainbow (DER) (van Hasselt et al.,
2019) and OTRainbow (Kielak, 2019) can achieve a similar performance to reconstruction-based
methods without explicit representation learning components confirms this intuition. More recently
SPR (Schwarzer et al., 2021) shows that data augmentation and improvements in Rainbow combined
with particular forms of self-supervision can significantly improve the sample efficiency, producing
state-of-the-art results in sample-efficient representation learning in RL.
3	Background
3.1	Groups and their Representations
A group G = {g} is a set, equipped with an associative binary operation, such that the set is closed
under this operation, and each element g ∈ G has a unique inverse, such that their composition
gives the identity g-1g = e. Any subset G' ≤ G that is closed under binary operation of the groups
forms a subgroup. A group G can act on a set X by transforming its elements x ∈ X through a
2
Under review as a conference paper at ICLR 2022
bijection. We use α : G X X ↦ X to denote the group action, and for brevity replace α(g,x) with g ∙X
moving forward. The action captures some of the structure of G due to two constraints - the identity
element acts trivially e ∙ X = x; and composition of actions is equal to action of the composition,
i.e., (gg′) ∙ x = g ∙ (g' ∙ x), ∀g,g' ∈ G. X is then called a G-set. Any G-action partitions X into
orbits xG = {g ∙ x ∣ g ∈ G}, and we denote the set of orbits under G-action using X/G. A G-action is
transitive iff its action results in a single orbit.
Parameterizing Lie Groups In this work, we assume G is (any sub-group of) a classical Lie group
over R. These are the groups that can be represented using invertible matrices - in other words,
we consider groups whose action is linear on real vector spaces. We use ρ(G) to denote a linear
representation of G, and ρg : RD → RD for the action (a.k.a. the representation) ofg ∈ G. Two other
greek letters τ and κ are also used for this purpose. Many such Lie groups are identifiable by their
Lie algebra g = Lie(ρ(G)).1 This connection enables a simple parameterization of ρ(G) using a set
of linear bases for their Lie algebra - that is Pg = exp(∑i βg,i E(i)), where exp (Y) = ∑∞=0 Yjj is the
matrix exponential. We refer to this parameterization later in Section 4. Such linear representations
in the form of invertible matrices can be used for both continuous transformations (e.g., 3D rotation)
and finite groups (×90° rotations).
3.2	MDP Homomorphism and Symmetric MDPs
We define an MDP as the 4-tuple M = (S, A, R, T} where S and A are respectively the set of states
and actions, R : S X A → R is the reward function, and T : S X A X S → R≥0 is the state transition
function.2 For two MDPs M = (S, A,R,T) and M = S A,R,T}, MDP homomorphism can be
defined as a tuple H = (hs,hA where hs ： S → S is the state mapping and h& ： S X A → A is the state
dependent action mapping. These two mappings satisfy the following invariance and equivariance
conditions:
1)	Invariance of the reward:
RS(hS(s), hA(s, a)) = R(s, a)	∀s, a ∈ S X A	(1)
2)	Equivariance of the deterministic transition model under the agent’s action:
TS(hS(s), hA(s, a)) = hS (T (s, a)) ∀s, a ∈ S X A	(2)
A probabilistic variation of the above equation for a stochastic MDP (Bloem-Reddy & Teh, 2020)
is:
T(hs(s')∣ hs(s),hA(s,a)) =	∑ T(s''∣ s,a)	∀s,s' ∈ S,a ∈ A,	(3)
s''∈[s']h
where [s']hs = hs-1(hs(s')) is the equivalence class of s′ under hS .
In related literature, MDP homomorphism is often used for minimization of the MDP, because the
optimal policy of MS can be lifted to obtain the optimal counterparts for M.
Symmetric MDPs The automorphism group GM = Aut(M) of an MDP identifies the set of
symmetry transformations of state-actions that preserve the reward and the transition dynamics:
R(s,a) = R(g ∙(s,a}))	∀g ∈ Gm, s ∈ S, a ∈ A (4)
T(s' ∣ s, a) = T(g ∙ s ∣ g ∙ (s, a)) and g ∙ T(s, a) = T(g ∙ (s, a))	∀g ∈ Gm, s, s ∈ S, a ∈ A (5)
We refer to a reward function R that satisfies Eq. (4) as a GM-invariant reward function and a
deterministic transition function T that satisfies Eq. (5) as a GM-equivariant transition function.
Note that this is a distinct notion from invariance and equivariance under agent’s action in the context
of MDP homomorphism. Here, the action refers to the action of a symmetry group, while in MDP
homomorphism, the equivariance is to the action of the agent. We use group action or G-action to
make this distinction clear when necessary.
For a symmetric MDP that satisfies both Eq. (4) and Eq. (5), both the optimal action-value and
optimal policy functions become invariant under GM action (Ravindran & Barto, 2001) - that is,
Q(s, a) = Q(g ∙(s,a)) and π(a, s) = π(g ∙ (a,s}) ∀g ∈ GM s, a ∈ S X A.	(6)
1This relation is bijective for ”simply connected” Lie groups.
2We ignore the discount factor for brevity.
3
Under review as a conference paper at ICLR 2022
The connection of symmetric MDPs to MDP homomorphism is due to the fact that symmetries can
be used to define a homomorphism H : M ↦ M by collapsing the state-actions that form an orbit
under Gm. Formally,the collapsed MDP M = S A, R, T) is defined by S = S/Gm, A = A/Gm,
R((s,a)GM) = R(s, a) and T(SgM ∣ (s,a)GM) = T(s' ∣ s,a). This results in symmetry-based
model minimization of symmetric MDPs.
4 Desiderata for Symmetry-Based Representation in RL
Separating Transition and Reward Symmetries One important choice is between using the
symmetry group of the MDP (GM) versus the symmetry group of state-transitions (GT), where GT is
the group of transformations of state-action pairs that leads to equivariant deterministic transitions,
as given by Equation 5. The former is a subgroup of the latter GM ≤ GT , i.e, the symmetries of a
transition model contains the symmetries of the MDP. In fact it is easy to see that GM = GT ∩ GR,
where GR is the group of transformations of state-action pairs that preserve the one step-reward and
only satisfy Equation 4. We observe that working with a larger symmetry group GT has two benefits:
1) it creates a stronger inductive bias for the model, because in many real-world settings can involve
a range of symmetries in transitions that are not present in the reward. For example, an agent’s
navigation of a 2D map often has the symmetry of the Euclidean group, while the reward (e.g., ar-
riving at a particular location) breaks this symmetry; 2) Separate modeling of transition symmetries
facilitates transfer to new tasks, where the reward is changing.
Invariance/Equivariance in model-free/model-based RL If the objective is to carry out model-
free RL, Eq. (6) motivates the need to learn action-value functions, or the policies that are invariant
to symmetries of the MDP (GM). For a deterministic policy, the invariance of Eq. (6) becomes an
equivariance constraint: g ∙ ∏(s) = ∏(g ∙ s). As it essentially leads to model minimization, van der
Pol et al. (2020b); Mondal et al. (2020) use this idea to improve sample efficiency when the groups
actions in the agent’s action space are known permutations. However, if our objective is just to learn
a symmetry-based model of the environment (i.e., transition and reward functions), Eq. (5) suggests
that we need to learn a GT -equivariant transition function.
Symmetries in a Latent Transition Model While it is possible to learn the state transition model
in the observation space that is equivariant to the agent’s action, for high-dimensional inputs this
could be quite challenging since the model has to learn details of the environment that are irrele-
vant to the RL agent. Using state and action embeddings enables learning of the transition model
in the latent space. Indeed the constraint on the model and the embedding is that of the MDP ho-
momorphism Section 3.2. Working in the latent space has an additional benefit when it comes to
symmetries: we can assume that the G action on the latent state-action pairs is linear through P(G)
despite having non-linear transformations in the observation space.
Figure 2: This figure demonstrates the relationship between
two types of equivariance in latent variable modeling for an
MDP with symmetric transition function. Green arrows (ver-
tical plane) identify a diagram for transition models in an
MDP homomorphism. A model T and state embedding func-
tion hs that are equivariant under agent,s action makes this
diagram commute. Red arrows (horizontal plane) identify the
commutativity diagram for a symmetric transition function of
an MDP in the latent space. Here the state-action embedding
{s,α} is produced through the symmetry transformation of
another state-action embedding {s,a}.
From the fact that symmetries of states GS ≤ G is a subgroup of the state-action or transition symme-
try, it follows that Pg ∈ ρ(G) can be divided into two parts: 1) Tg ∈ T(GS) the group representation
acting on the state embedding, and; 2) Kg ∈ κ(G), the group representation for state-dependent
action embedding.3
3 This is because ρ(G) can be seen as a representation that is induced by the representation τ (GS ) of its
subgroup ρ = IndGG τ .
4
Under review as a conference paper at ICLR 2022
At this point we can combine the requirement for an MDP homomorphism Eq. (2), with that of
the G-equivariant transition model Eq. (5) of a symmetric MDP. The result is the following two
constraints in our symmetric latent variable model ∀s, a ∈ S × A and g ∈ G (see Figure 2):
T(hs(s), hA(s, a)) = hs(T(s, a))
TgT (hs(s), hA(s, a)) = T (Tghs(s), KghA(s, a))
(7)
(8)
Matrix Embedding of States and Actions While the design choice of this subsection is not nec-
essary, we see that it can significantly simplify the constraints on a symmetry based model. We
propose to use group representations for our state, and state action embeddings hs : S → T(GS) and
hA ： SXA → κ(G). This choice assumes that a G action on state and state-action pairs is transitive, so
that each state, and state-action pair can be mapped to a (or at least one) group member. To empha-
size this in our notation, we use κ(s) instead of hS(s) and similarly use T(s, a) instead of hA(s, a)
for state, and state-dependent action embedding respectively. This choice of embedding has several
benefits: First, the learned embeddings are automatically equivariant to symmetry transformations
of the state, and state-actions:
τ(g ∙ s) =	Tgτ(s)	and	κ(g	∙(s,a))	=	Kgκ(s,a)	∀s, a ∈ S X	A, g	∈ G.	(9)
This means that the symmetries of the state-action pairs are preserved and now take a linear form in
the latent space. Note that while the embeddings are automatically equivariant, they may be equiv-
ariant to irrelevant non-linear transformations of the input. Word modeling constraints ensure the
relevance of these non-linear transformations that are captured by the group equivariant embeddings
above. Moreover, this embedding enables the following simple choice for the transition model
T(T(s), κ(s,a)) = κ(s,a)τ(s)	(10)
which simply transforms the state-embedding T(s) through the linear group action of state-
dependent action encoding K(s, a). Using this transition model, the action equivariance constraint
of Eq. (7), and G-equivariance constraint of Eq. (8) simplifies to: ∀s, a ∈ SXAgiven a state transition
triplet {s,a, s'}
T(s') = κ(s, a)T(s)	(11)
Tg K(s, a)T (s) = KsgK(s, a)Tg T (s)	(12)
In practice our model seeks to satisfy these two constraints via the direct minimization of appropriate
loss functions, as will be discussed in Section 5.
Decomposition of the Latent Space The decomposition G = G1 X . . . X GK into a direct product of
subgroups can disentangle the factors of variation in the dataset (Higgins et al., 2018). 4 5 This gives
us a way to represent the latent embedding space as a direct product of K subgroups of G, where
each factor varies independently by actions of a subgroup of G on the latent embedding. Intuitively
such a symmetry-based disentanglement provides an effective inductive bias particularly when there
is modularity so that temporally coherent changes in the environment are due to the change of a
(sparse) subset of factors. We impose this decomposed structure in the form of a direct sum for
state representation and the state-dependent action representation - that is T(s)= ㊉k Tk(s) and
κ(s, a)=㊉k Kk(s, a), where k ∈ {1,..., K} and g = (gι,..., gκ). Moreover the representation
of the symmetry group G acting on the state embedding and the state-dependent action embedding
is decomposed as Tg =㊉ k Tgk and Kg =㊉ k Kgk, where gk ∈ Gk for ∈ {1,..., K} and g ∈ G.
Combining this block structure with the Lie parameterization of Section 3.1 we get
Tθ(s)=㊉exp(∑βi,k,θ(S)Eci))	Kφ(Tθ(s),a)=㊉exp (∑αi,kφ(tθ(s),a)E(i)) (13)
where we use any standard neural network to implement the αφ and βθ functions above 5. As we can
backpropagate through this function, the network parameters θ, φ can be learned end to end. The
4As noted by CaSelleS-DUPre et al. (2019), simply having a product structure in the latent space does not
guarantee disentanglement, and further constraints are required. In this work, we do not impose any additional
constraints for disentanglement.
5We denote both, the neural networks which maps to group representations, and network parameters by
lowercase greek alphabets.
5
Under review as a conference paper at ICLR 2022
choice of the subgroup depends on the symmetries of the RL environment, and it only affects the
set of bases {E(i)}i in Eq. (13). For example, in Atari games, the screen often has multiple objects
undergoing 2-D translations and rotations, and one can use blocks of the 2-D Special Euclidean
(SE(2)) that comprise translation and rotations of Euclidean space (E). For more realistic 3D
environments, such as those of interest in robotics, self-driving cars and third person games, one
can use SE(3), which is the group of 3-D translations and rotations. Also, in theory, we only
need specify a group that “contains” the group of interest as a subgroup. For example, if our state-
actions only have 90。rotational symmetry, We may use a more general group for the representation
(e.g., SE(2)). The embedding function can define a homomorphism into the relevant subgroup.
5 Loss Functions
We consider a standard RL setup Where the agent interacts With its environments in episodes and We
have access to ({st, at, rt, st+1})t=1,..,T Where st is the state, at is the action taken by the agent, rt
is the reWard received and st+1 is the observed next state at timestep t. BeloW We describe three loss
functions that encode the equivariance/invariance constraints of Eqs. (1), (11) and (12).
Action EquiVariant Transition Loss - Eq. (11) Given triplets (st, at, st+。from our dataset we
simply apply a loss function ` such as square loss6 that penalizes the difference betWeen tWo sides:
LAET (θ, φ) = ` (τθ(st+1), κφ(st, at)τθ(st)) .	(14)
The choice of the embedding space and the latent transition function ensure that state embeddings
are transformed by linear group action of the action embeddings. Minimization of LAET encourages
these symmetry transformations to capture state transitions resulting from the agent’s action.
Group EquiVariant Transition Loss - Eq. (12) For this we need a st in addition to (st, a",
where t′ can be any state (at different time step in the same or a different episode.) We find the
group transformation that maps S to s' the latent space using Tg = tθ(st')τθ(st)-1. Using this we
can rewrite Equation Eq. (12) as
Tθ(st')τθ(st)-1 Kφ(st,at)τθ(St) = KgKφ(st,at) τ(st') .	(15)
、------Y-------'	、~Y~'
τg	τg τθ (st )
Since the state-dependent action encoding Kφ(st, at) for the pair (st, aQ is also produced by a neural
network, the only missing part in the equation above is κgs, the state-dependent action transformation.
We use a neural network ρω : Tg ↦ Kg to infer it from state transformation Tg.
Example 1. To get an intuition for what this network is doing consider the example of a pendulum
without gravity, with rotation and reflection symmetry O(2) as shown in Fig. 1, where inputs to the
networks (s) are image sequences and the (ideal) embeddings Tθ (s), Kφ(s, a) are x-y coordinates
plus angular velocity and torque respectively. If we rotate the pendulum using a rotation matrix Tg,
we expect state-dependent action embedding to remain the same since the effect of torque remains
similar after rotation. However, if we transform the pendulum by reflection around the vertical axis,
we expect that the effect of torque will be negated. ρω parameterizes this dependence.
A loss function ` could then measure the difference between left and the right hand side in the
equation above
Lget(Θ, φ,ω) = ` (tθ(st,)τθ(Str"(st, at)τθ(St),Pω(tθ(st"(st)-1)κφ(st, a)τθ(st')).
(16)
Action InVariant Reward Loss - Eq. (1) While LAET and LGET enforce the equivariance of the
latent transition model to an agent’s action and the symmetry group, they do not encode information
of the reward in the state representations. In order for the latent model to be homomorphic to the
underlying MDP of the environment we match the reward at every state embedding using a reward
predictor network rψ : Tθ(s) ↦ R. We measure the difference between the predicted reward and the
actual reward at time step t + 1:
LR(ψ, θ, φ) = (rψ(Kφ(Tθ(st), at)Tθ(st)) -rt+1)2 .
(17)
6In practice we use the normalized square loss '(Y, Y')=
而-i⅛ I
2
2
6
Under review as a conference paper at ICLR 2022
6 Application to Model-free RL
Following previous success of using transition models for representation learning in model-free RL
(Gelada et al., 2019; Schwarzer et al., 2021) we add the losses discussed above to the Temporal
Difference (TD) error in Deep Q-learning. In practice, we need to make three modifications to our
model/loss.
Target Network A trivial solution to both equivariance enforcing losses of Section 5 is to encode
all states and actions using an identity matrix. This problem in different contexts is known as the
problem of collapse in representation learning. While using the reward signal helps in avoiding the
collapse it is often not sufficient specially in sparse reward settings. Following Schwarzer et al.
(2021), we use a target network to encode state st+1 and st′ in Eq. (16) in which the network
parameters do not receive gradient and moreover are copied from the online network. We explicitly
drop the subscripts to differentiate the target from the online network in this section (e.g., τθ → τ).
Projection Head for Transition Losses Strict enforcement of symmetry constraints by our model
can be overly restrictive when the environment has non-symmetric components, or when the our
transition model is too simplistic. For this reason, following the previous work, we enforce the
losses on a learnable projection of the state embedding. That is before application of the loss ` in
Eqs. (14) and (16) we pass the embedding through a projection head.
M-step prediction Following the success of (Schwarzer et al., 2021) because of long-term state
embedding predictions, we predict state embeddings and rewards for M -steps.
6.1 Putting it All Together
Considering M consecutive state-actions {st：t+M, at：t+M} and Xt = Xt = tθ(st), We predict the state
embeddings and the rewards of next M steps:
xt+m = κφ (Xt+m-1, at+m-1)x^t+m-1 and rt+k = rψ (xt+k ) ∀m ∈ {1, ..., M}
Note that we are using X for M-step model prediction of the embedding to distinguish them from
the latent embedding x, and the embedding produced by the target network X = T (st+m). The same
applies to the M-step predicted reward r and observed reward r. We then project these embedding
using a projection head PZ to produce ^t+m = Pζ(Xt+m) and zt+m = p(Xt+m). Using this notation,
our final expressions for LAET and LR are:
2
2
M
LAET = ∑
m=1
zt+m	zzt+m
-------. - --------
Il zt+m Il 2 Il zt+m Il 2
M
and Lr = ∑ (^
+m - rt+m)
m=1
(18)
For LGET we need (st, at, st+ι) and another state s’. From their embedding using the notation
above we get Tg = Xt'Xt-1, the linear transformation between them, and Kg = ρω(Tg), the state-
dependent action transformation. Now for Xt', we obtain the predicted next state from Xt' as Xt'+ι =
Kgκ(Xt, at)Xt' = Pω(Xt'X-1)κ(Xt, at)Xt' and from Xt+ι as Xt'+ι = TgXt+ι. Before penalizing the
difference between these embeddings, we project them to yt'+ι = bη (Xt'+ι) and yt'+ι = b(Xt'+ι)
using projection head bη, to get the final expression for LGET:
LGET
2
yt'+ι yt'+ι
---------. - T-----------
Ilyt'+1∣∣2 IIyt'+1∣∣2 2
(19)
Q-learning We pass the representation Xt to a Q-learning head qξ to learn policies based on the
output of the Q-value estimator. The Q-value estimator is learnt by minimizing:
LDQN (ξ, θ) = (qξ(Tθ(st),at) - (rt +γmaxqξ(T(st+1),a)))2	(20)
a
We use the data efficient adaptation of Rainbow (van Hasselt et al., 2019; Hessel et al., 2018) which
combines many improvements over the original DQN(Mnih et al., 2013) such as Distributional
RL(Dabney et al., 2018), Dueling DQN (Wang et al., 2016), Double DQN (Van Hasselt et al., 2016).
The total loss optimized by our model is:
L = LDQN +λ1LR + λ2LGET + λ3LAET
(21)
7
Under review as a conference paper at ICLR 2022
where λ1, λ2 and λ3 are hyper-parameters. Motivated by the performance improvements due to
augmentation reported in recent literature (Yarats et al., 2021; Schwarzer et al., 2021), we also
augment our states by shifting and changing the pixel intensity before encoding them. Fig. 5 in
Appendix B.2 shows a detailed schematic of our model. We provide the algorithm for our model in
Appendix B.4 and details of network architectures in Appendix B.3.
7 Experiments
We test our method on a suite of 2D Atari games, which is a popular benchmark used in RL. The full
Atari suite consists of 57 games with typically 50 million environment steps. We use the sample-
efficient Atari suite introduced by Kaiser et al. (2019), which consists of 26 games with only 100,000
environment steps of training data available. In our experiments, we use three types of simple
connected Lie subgroup blocks including General Linear GL(2), Special Euclidean SE(2), and
Translation T (2), see Appendix A for more details. Unless stated otherwise, our EqR model uses
SE(2) subgroup blocks, with K = 12 blocks and M = 5 steps during training. LAET is always used
to train EqR inorder to model the non-linear transfomations in the state space resulting from agent’s
actions as linear group actions in the latent space. LGET, which makes transition model equivariant
with respect to the symmetry transformation of state-actions, and LR are optional. We build our
implementation on top of SPR’s (Schwarzer et al., 2021), which is based on rlpyt (Stooke &
Abbeel, 2019) and PyTorch (Paszke et al., 2019). We use the same underlying RL algorithm and
hyperparameters used by SPR for fair comparision.
Evaluation Metrics We compute the average episodic return (the ‘game score’) at the end of train-
ing and normalize it with respect to human scores, as is standard practice. The human-normalized
score (HNS) is given by :gent SCore -ran*m SCore . Since there is considerable variance across different
human score - random score
runs, the mean and the median are not very reliable metrics. Instead, Agarwal et al. (2021) pro-
pose using bootstrapped confidence intervals (CI) with stratified sampling which is more suitable
for small sample sizes (10 runs per game in our case). We report the Interquartile Mean (IQM),
which is the mean across the middle 50% of the runs, as well as the Optimality Gap, which is the
amount by which the algorithm fails to meet a minimum HNS of 1.0. We also provide performance
profiles showing the fraction of runs above a certain normalized score, which gives a more complete
picture of the performance.
Results We use 10 seeds for every game,
for every variation of our model. Figure 3
shows performance profiles for two vari-
ations of our model, EqR with LR and
with LR + LGET , along with other com-
parable methods. If one curve is strictly
above another, the better method is said
to “stochastically dominate” the other
(Agarwal et al., 2021). The curves for
both variations of the proposed method
are almost always above the next best
method, SPR (Schwarzer et al., 2021).
Figure 4(a) provides results for different
methods on all 26 games. The two best
Score Distributions with Non Linear Scaling
EqR, Lr	EqR, Lr + LGEr
Figure 3: Performance profiles for different methods based
on score distributions (left), and average score distributions
(right). Shaded regions show pointwise 95% confidence
bands. The higher the curve, the better the method is.
variations of the proposed method outperform previous methods, and the difference is statistically
significant considering the CI. Table 2 in Appendix B.1 shows full results on all games, and our best
model achieves super-human performance on eight games and achieves higher score than any other
previous method on 13 out of the 26 games.
In order to better understand the effect of various modeling choices, loss functions and implemen-
tation details on the performance, we now consider different variations of EqR, with the same aug-
mentation as the baseline for ablation studies.
Choice of Group To understand the role of the choice of a group in the embedding space, we use
our EqR model with LR. This variation of EqR is similar to DeepMDP (Gelada et al., 2019), except
for the group structured latent embedding space and group action-based state transition. In order to
investigate the effect of the above two group-related constructs, we remove them and use an action
8
Under review as a conference paper at ICLR 2022
IQM
EqR, LR + Lget
EqR, LR
SPR
DrQ ■
CURL ■
DER ■
SimPLe
0.15	030	0.45	0.56 0.64 0.72 0.80
Human Normalized Score
(a)
IQM
EqR, LR + I-GET
EqR, LR
SPR
DrQ	.
CURL ■
DER .
SimPLe
-0Λ—0?2—0.3 0.4	0.56 0.64 0.72 0.80
Human Normalized Score
(b)
EqR, GL(2)
EqR, SE(2)
EqR, T(2)
MLP
Rainbow w/ aug
IQM	Optimality Gap
EqR, Lr + Lget	.
EqR, Lget
EqR, Lr	・
EqR
Rainbow w/ aug ■
-0.24 0.32 0.40 0.48 ~θ3δ~~0?64~0.72
Human Normalized Score
(c)	(d)
Figure 4: Plots of Interquartile Mean (IQM) and Optimality Gap computed from human-normalized scores,
showing the point estimates along with 95% confidence intervals (over 10 runs for all methods, 5 runs for
SimPLe). A higher IQM and a lower optimality gap reflects better performance. (a) shows different methods
for all 26 games. (b) shows different methods for 17 games. (c) shows the proposed model with different group
choices for all 26 games. (d) shows the proposed model with different loss terms for all 26 games.
encoder to predict the next states directly, referring to this as MLP. We further test models with other
subgroups including T(2) and GL(2). Figure 4(c) shows that adding a symmetry-based inductive
bias in the model by making the embeddings group representations and modeling the transitions
as group actions is indeed helpful. The success of the model which uses SE (2) blocks might be
attributed to the fact that translations and rotations are the most common types of symmetry transfor-
mations present in Atari games. However, the more restrictive T (2) slightly hurts the performance,
while the more general GL(2) performs similarly to the MLP model.
Loss functions Figure 4(d) compares the performance of EqR using SE(2) subgroup blocks with
different loss components. Using EqR with the default LAET results in a considerable improvement
over Rainbow with augmentation. Adding LGET improves the performance slightly, while adding
only LR improves the performance even further. We hypothesize that the reward loss is playing a
role in both preventing representation collapse and preserving more information of the reward dis-
tribution in the latent state embeddings. Adding both LGET and LR improves the performance only
slightly. It might be that this prior of a equivariant transition model with respect to symmetry trans-
formations of state-actions is too restrictive for some games while being beneficial for others. Based
on the results in Figure 4(b), in 17 out of a total of 26 games, including this loss term leads to a sta-
tistically significant boost in performance. These 17 games are: ‘Alien’, ‘BankHeist’, ‘BattleZone’,
‘Boxing’, ‘ChopperCommand’, ‘CrazyClimber’, ‘DemonAttack’, ‘Freeway’, ‘Hero’, ‘Jamesbond’,
’MsPacman’, ‘Pong’, ‘PrivateEye’, ‘Qbert’, ‘RoadRunner’, ‘Seaquest’, ‘UpNDown’. The full list
of game-wise scores for the ablation studies are presented in Tables 3 and 4 in Appendix B.1.
8	Conclusion
This paper considers three major symmetry-related constructs within a coherent framework. First,
there is the group equivariant state and state-dependent action embedding, which we achieve through
Lie parameterization. The world modeling constraints, which are discussed next, further ensure
that the transformations captured by the equivariant embedding are relevant. Second, the action
equivariant transition, which when combined with group equivariant embeddings, ensures that the
state transitions are captured by symmetry transformations in the latent space. Third, there is the
group equivariant transition, which acts as an additional bias and ensures that the latent transition
model itself is equivariant under symmetry transformations of state-action pairs.
We provided an extensive set of experiments to evaluate the usefulness of our approach in learning
state-embeddings for model-free RL. In future work we would like to explore the application of our
approach in model-based RL, as well as its ability to generalize across tasks. We also plan to further
investigate theoretically grounded methods for combining both symmetric and asymmetric aspects
of the environment in the model.
9
Under review as a conference paper at ICLR 2022
9	Reproducibility Statement
We have made every effort to ensure that our method is reproducible. Section 5 and Section 6 provide
detailed descriptions of the loss functions and various implementation-related details. We present a
step-by-step algorithm in Appendix B.4 and have provided details of the network architecture and
the list of hyperparameters used, in Sections B.3 and B.5 respectively. Section B.1 has individual
results on each game for the proposed EqR model and its variations, to enable researchers to verify
the results. Finally, we have submitted the code as part of the submission and we plan on releasing
it to the public once the reviewing process is over.
References
Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G Bellemare.
Deep reinforcement learning at the edge of the statistical precipice. Advances in Neural Informa-
tion Processing Systems, 34, 2021.
Marc G Bellemare, Salvatore Candido, Pablo Samuel Castro, Jun Gong, Marlos C Machado, Sub-
hodeep Moitra, Sameera S Ponda, and Ziyu Wang. Autonomous navigation of stratospheric bal-
loons using reinforcement learning. Nature, 588(7836):77-82, 2020.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828,
2013.
Benjamin Bloem-Reddy and Yee Whye Teh. Probabilistic symmetries and invariant neural networks.
J. Mach. Learn. Res., 21:90-1, 2020.
Noam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science, 365(6456):
885-890, 2019.
Hugo Caselles-Dupre, Michael Garcia Ortiz, and David Filliat. Symmetry-based disentangled rep-
resentation learning requires interaction with environments. Advances in Neural Information
Processing Systems, 32:4606-4615, 2019.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning,
pp. 1597-1607. PMLR, 2020.
Will Dabney, Mark Rowland, Marc G Bellemare, and Remi Munos. Distributional reinforcement
learning with quantile regression. In Thirty-Second AAAI Conference on Artificial Intelligence,
2018.
Vincent Frangois-Lavet, Peter Henderson, Riashat Islam, Marc G Bellemare, and Joelle Pineau. An
introduction to deep reinforcement learning. arXiv preprint arXiv:1811.12560, 2018.
Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc G Bellemare. Deepmdp:
Learning continuous latent space models for representation learning. In International Conference
on Machine Learning, pp. 2170-2179. PMLR, 2019.
Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning
behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019a.
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. In International Conference on
Machine Learning, pp. 2555-2565. PMLR, 2019b.
Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan
Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in
deep reinforcement learning. In Thirty-second AAAI conference on artificial intelligence, 2018.
I.	Higgins, D. Amos, D. Pfau, SebaStien RaCaniere, Lolc Matthey, Danilo Jimenez Rezende,
and Alexander Lerchner. Towards a definition of disentangled representations.	ArXiv,
abs/1812.02230, 2018.
10
Under review as a conference paper at ICLR 2022
Irina Higgins, Arka Pal, Andrei Rusu, Loic Matthey, Christopher Burgess, Alexander Pritzel,
Matthew Botvinick, Charles Blundell, and Alexander Lerchner. Darla: Improving zero-shot trans-
fer in reinforcement learning. In International Conference on Machine Learning, pp. 1480-1490.
PMLR, 2017.
Eukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad
Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model based
reinforcement learning for atari. In International Conference on Learning Representations, 2019.
Kacper Piotr Kielak. Do recent advancements in model-based deep reinforcement learning really
improve data efficiency? 2019.
Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Re-
inforcement learning with augmented data. arXiv preprint arXiv:2004.14990, 2020a.
Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised representa-
tions for reinforcement learning. In International Conference on Machine Learning, pp. 5639-
5650. PMLR, 2020b.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuo-
motor policies. The Journal of Machine Learning Research, 17(1):1334-1373, 2016.
Lihong Li, Thomas J Walsh, and Michael L Littman. Towards a unified theory of state abstraction
for mdps. ISAIM, 4:5, 2006.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529-533, 2015.
Arnab Kumar Mondal, Pratheeksha Nair, and Kaleem Siddiqi. Group equivariant deep reinforce-
ment learning. arXiv preprint arXiv:2007.03437, 2020.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. Advances in neural information processing systems, 32:
8026-8037, 2019.
Robin Quessard, Thomas D Barrett, and William R Clements. Learning group structure and disen-
tangled representations of dynamical environments. arXiv preprint arXiv:2002.06991, 2020.
B. Ravindran and A. G. Barto. Symmetries and model minimization in markov decision processes.
Technical report, USA, 2001.
Balaraman Ravindran and Andrew G Barto. Model minimization in hierarchical reinforcement
learning. In International Symposium on Abstraction, Reformulation, and Approximation, pp.
196-211. Springer, 2002.
Balaraman Ravindran and Andrew G Barto. Smdp homomorphisms: an algebraic approach to
abstraction in semi-markov decision processes. In Proceedings of the 18th international joint
conference on Artificial intelligence, pp. 1011-1016, 2003.
Balaraman Ravindran and Andrew G. Barto. An algebraic approach to abstraction in reinforcement
learning. 2004.
Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, and
Philip Bachman. Data-efficient reinforcement learning with self-predictive represen-
tations. In International Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=uCQfPZwRaUu.
11
Under review as a conference paper at ICLR 2022
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484-489, 2016.
Adam Stooke and Pieter Abbeel. rlpyt: A research code base for deep reinforcement learning in
pytorch. arXiv preprint arXiv:1909.01500, 2019.
Jonathan Taylor. Lax probabilistic bisimulation. 2008.
Valentin Thomas, Jules Pondard, Emmanuel Bengio, Marc Sarfati, Philippe Beaudoin, Marie-Jean
Meurs, Joelle Pineau, Doina Precup, and Yoshua Bengio. Independently controllable features.
arXiv preprint arXiv:1708.01289, 2017.
Elise van der Pol, Thomas Kipf, Frans A Oliehoek, and Max Welling. Plannable approximations
to mdp homomorphisms: Equivariance under actions. In Proceedings of the 19th International
Conference on Autonomous Agents and MuItiAgent Systems, pp. 1431-1439, 2020a.
Elise van der Pol, Daniel Worrall, Herke van Hoof, Frans Oliehoek, and Max Welling. Mdp homo-
morphic networks: Group symmetries in reinforcement learning. Advances in Neural Information
Processing Systems, 33, 2020b.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In Proceedings of the AAAI conference on artificial intelligence, volume 30, 2016.
Hado P van Hasselt, Matteo Hessel, and John Aslanides. When to use parametric models in re-
inforcement learning? Advances in Neural Information Processing Systems, 32:14322-14333,
2019.
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling
network architectures for deep reinforcement learning. In International conference on machine
learning, pp. 1995-2003. PMLR, 2016.
Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing
deep reinforcement learning from pixels. In International Conference on Learning Representa-
tions, 2021. URL https://openreview.net/forum?id=GY6-6sTvGaf.
Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stephane Deny. BarloW twins: Self-supervised
learning via redundancy reduction. arXiv preprint arXiv:2103.03230, 2021.
Martin Zinkevich and Tucker Balch. Symmetry in markov decision processes and its implications
for single agent and multi agent learning. In In Proceedings of the 18th International Conference
on Machine Learning. Citeseer, 2001.
A Subgroup blocks and their parameterization
A. 1 Choice of group
Atari games differ in their style of play, their objectives, the symmetry transformations of both the
agent and other objects on the screen and associated symmetry transformation of the agent’s action.
But most of these games include symmetry transformations. For example, the screen often has mul-
tiple objects undergoing two dimensional(2-D) translations and rotations. In this case one can use
blocks of the 2-D Special Euclidean Group SE(2). Each such block can capture the transformation
of a particular object in the screen, including the agent. One can also use more restrictive subgroup
blocks like T (2), which capture only 2D translations. For more realistic 3D environments, such as
those of interest in robotics, self-driving cars and third person games, one can use SE(3), which
is the group of 3-D translations and rotations. This should capture both the transformations of the
objects in the environment and changes in viewpoint due to the agent’s actions.
A.2 Implementing parameterization
We consider three types of subgroups: GL(n) - the set of all invertible linear transformations, SE(n) -
the set of all rotations and translations and T(n) - the set of all translations in ann dimensional vector
space. We provide a general method to parameterize each of these, based on the type of group.
12
Under review as a conference paper at ICLR 2022
GL(n) As the matrix representation of GL(n) is the set of invertible matrices which has a measure
of 1 it is easy to parameterize it. We just generate n2 parameters using a network corresponding to
each element of the matrix. This gives an element from GL(n).
T(n) As T(n) just denotes translation in a n-dimensional space with group action being addition,
implementing it is straightforward. We generate n parameters using a neural network and instead
of using matrix multiplication use addition for the group action. Note that we can also use a matrix
representation for T(n) but it is unnecessary and inefficient.
SE(n) Unlike GL(n) and T(n), parameterizing SE(n) is a bit tricky because it involves pa-
rameterizing SO(n). We use a homogeneous co-ordinate based representation of SE(n) =
{(R0 1t) , R ∈ SO(n) and t ∈
.So We need n parameters for the t and another D = n(n2-1)
parameters for SO(n) from the neural network. As explained in Section 4, we can use a Lie pa-
rameterization to get the elements of SO(n) by R = exp(∑dD=1 βd E(d)) where E(d) denote D bases
of the space of skew symmetric matrices and the βi s are the parameters of the neural network. For
example, in the case of SO(2) we can use the basis E(1) = (
to SO(n) by using a basis given by D n × n matrices E(ij)
elements are Ei(,ijj) = -1 and Ej(,iij) = 1.
0	1) . Similarly, we can extend this
-1 0
∀{1 ≤ i < j ≤ n} whose only non-zero
Although Lie parameterization gives us a general recipe to output a representation of simple con-
nected Lie groups like SO(n), in our implementation We use Euler parameterization because it runs
faster in Pytorch. We provide the code for both. FolloWing Quessard et al. (2020), We parameter-
ize each rotation matrix in SO(n) using the product of rotations on D orthogonal planes in Rn:
R = ∏in=1 ∏1≤i<j≤n Rij. Here Rij ∈ Rn×n is the rotation matrix in the i - j plane, and its non-zero
elements besides the diagonal are the four values on the i, j roWs and columns, Which comprise the
2D rotation matrix that is Rii,jj
obtain from a neural network.
cos(θi,j)	sin(θi,j)]
- sin(θi,j)	cos(θi,j)
. We have D parameters θi,j which we can
The parameters in all the parameterization techniques mentioned here can be back-propagated. We
summarize the number of parameters required from a neural netWork output, representation type
and the associated group actions of different subgroups in Table 1.
Table 1: Group Properties
Subgroup block type	#Parameters	Representation type	Group action
General Linear - GL(n)	n2	Matrixn×n	Matrix multiplication
Special Euclidean - SE(n)	n(n-1) + n	Matrix(n+1)×(n+1)	Matrix multiplication
Special Orthogonal - SO(n)	n(n-1) -2-	Matrixn×n	Matrix multiplication
Translation - T (n)	n	Vectorn	Addition
13
Under review as a conference paper at ICLR 2022
B	Atari Details
B.1	Full Results
We provide individual results on the 26 Atari games after 100K training steps. Our results are aver-
aged over 10 seeds, and the network architectures and full list of hyperparameters used to produce
them are provided in Appendix B.3 and Appendix B.5.
•	Table 2 compares our two best performing EqR models using SE(2) subgroup blocks with
other methods.
•	Table 3 compares different choices of subgroup blocks with the reward loss, LR , included
for all EqR models (also see Figure 4 (a)).
•	Table 4 compares EqR using SE(2) subgroup blocks with different loss terms included in
the training objective (see Section 5 and Figure 4 (b)). The action equivariance transition
loss, LAET is always included for EqR models.
Table 2: Mean game scores on the 26 Atari games after 100K environment steps. The EqR models
use SE(2) subgroup blocks along with an action equivariant transition loss, LAET, and are averaged
over 10 seeds.
Game	Random	Human	SimPLe	DER	CURL	DrQ	SPR	EqR, LR	EqR, LR + LGET
Alien	227.8	7127.7	616.9	739.9	558.2	771.2	801.5	774.0	872.9
Amidar	5.8	1719.5	88.0	188.6	142.1	102.8	176.3	140.9	138.4
Assault	222.4	742.0	527.2	431.2	600.6	452.4	571.0	753.8	734.3
Asterix	210.0	8503.3	1128.3	470.8	734.5	603.5	977.8	923.2	902.5
Bank Heist	14.2	753.1	34.2	51.0	131.6	168.9	380.9	395.1	397.4
BattleZone	2360.0	37187.5	5184.4	10124.6	14870.0	12954.0	16651.0	13044.0	13255.0
Boxing	0.1	12.1	9.1	0.2	1.2	6.0	35.8	37.5	39.2
Breakout	1.7	30.5	16.4	1.9	4.9	16.1	17.1	17.2	16.0
ChopperCommand	811.0	7387.8	1246.9	861.8	1058.5	780.3	974.8	1073.5	1142.2
Crazy Climber	10780.5	35829.4	62583.6	16185.3	12146.5	20516.5	42923.6	49399.0	52008.1
Demon Attack	152.1	1971.0	208.1	508.0	817.6	1113.4	545.2	531.4	532.1
Freeway	0.0	29.6	20.3	27.9	26.7	9.8	24.4	24.1	25.2
Frostbite	65.2	4334.7	254.7	866.8	1181.3	331.1	1821.5	1855.6	1699.4
Gopher	257.6	2412.5	771.0	349.5	669.3	636.3	715.2	1010.0	912.1
Hero	1027.0	30826.4	2656.6	6857.0	6279.3	3736.3	7019.2	5775.2	6118.5
Jamesbond	29.0	302.8	125.3	301.6	471.0	236.0	365.4	312.8	319.7
Kangaroo	52.0	3035.0	323.1	779.3	872.5	940.6	3276.4	3569.3	3296.0
Krull	1598.0	2665.5	4539.9	2851.5	4229.6	4018.1	3688.9	5614.5	5467.7
Kung Fu Master	258.5	22736.3	17257.2	14346.1	14307.8	9111.0	13192.7	18511.0	17510.9
Ms Pacman	307.3	6951.6	1480.0	1204.1	1465.5	960.5	1313.2	1317.1	1663.5
Pong	-20.7	14.6	12.8	-19.3	-16.5	-8.5	-5.9	-6.0	-6.1
Private Eye	24.9	69571.3	58.3	97.8	218.4	-13.6	124.0	76.6	88.9
Qbert	163.9	13455.0	1288.8	1152.9	1042.4	854.4	669.1	773.8	814.9
Road Runner	11.5	7845.0	5640.6	9600.0	5661.0	8895.1	14220.5	13385.0	13708.8
Seaquest	68.4	42054.7	683.3	354.1	384.5	301.2	583.1	650.3	697.9
Up N Down	533.4	11693.2	3350.3	2877.4	2955.2	3180.8	28138.5	44295.4	52118.4
Mean Human-Norm’d	0.000	1.000	0.443	0.285	0.381	0.357	0.704	0.859	0.886
Median Human-Norm’d	0.000	1.000	0.144	0.161	0.175	0.268	0.415	0.418	0.398
# Superhuman games	0	N/A	2	2	2	2	7	8	7
14
Under review as a conference paper at ICLR 2022
Table 3: Mean game scores on the 26 Atari games after 100K environment steps for different choices
of subgroup blocks, averaged over 10 seeds. The reward loss, LR, is included in addition to the
default loss LAET.
Game	Random	Human	MLP	EqR, T2	EqR, SE2	EqR, GL2
Alien	227.8	7127.7	780.1	846.4	774.0	881.3
Amidar	5.8	1719.5	143.3	139.7	140.9	132.2
Assault	222.4	742.0	701.5	684.0	753.8	692.3
Asterix	210.0	8503.3	973.6	1004.4	923.2	889.5
Bank Heist	14.2	753.1	402.1	353.5	395.1	430.2
BattleZone	2360.0	37187.5	12722.6	11500.0	13044.0	13114.0
Boxing	0.1	12.1	38.0	28.9	37.5	33.4
Breakout	1.7	30.5	16.2	14.8	17.2	15.4
ChopperCommand	811.0	7387.8	989.5	1028.9	1073.5	1088.1
Crazy Climber	10780.5	35829.4	43705.8	50822.1	49399.0	55018.5
Demon Attack	152.1	1971.0	518.6	544.2	531.4	510.2
Freeway	0.0	29.6	20.3	18.5	24.1	21.4
Frostbite	65.2	4334.7	1702.4	1653.8	1855.6	1797.7
Gopher	257.6	2412.5	720.2	1012.5	1010.0	894.4
Hero	1027.0	30826.4	6840.0	5779.8	5775.2	5934.7
Jamesbond	29.0	302.8	337.4	313.25	312.8	334.8
Kangaroo	52.0	3035.0	2994.8	2942.5	3569.3	3186.4
Krull	1598.0	2665.5	3801.5	5293.0	5614.5	5772.6
Kung Fu Master	258.5	22736.3	13780.4	14924.2	18511.0	16002.8
Ms Pacman	307.3	6951.6	1220.8	1166.8	1317.1	1147.7
Pong	-20.7	14.6	-6.1	-11.5	-6.0	-8.2
Private Eye	24.9	69571.3	72.4	65.1	76.6	55.9
Qbert	163.9	13455.0	678.4	763.2	773.8	635.2
Road Runner	11.5	7845.0	12765.2	13654.2	13385.0	12560.4
Seaquest	68.4	42054.7	656.9	647.3	650.3	633.3
Up N Down	533.4	11693.2	23130.6	58164.4	44295.4	43767.2
Mean Human-Norm’d	0.000	1.000	0.681	0.829	0.859	0.833
Median Human-Norm’d	0.000	1.000	0.398	0.361	0.418	0.380
# Superhuman games	0	N/A	6	6	8	7
Table 4: Mean game scores on the 26 Atari games after 100K environment steps for EqR using
SE(2) subgroup blocks with different loss terms included in the training objective. The action
equivariance transition loss, LAET, is included for all EqR models and the scores are averaged over
10 seeds.
Game	Random	Human	EqR	EqR, LR	EqR, LGET	EqR, LR + LGET
Alien	227.8	7127.7	856.5	774.0	862.5	872.9
Amidar	5.8	1719.5	134.7	140.9	135.0	138.4
Assault	222.4	742.0	643.1	753.8	701.3	734.3
Asterix	210.0	8503.3	824.8	923.2	864.9	902.5
Bank Heist	14.2	753.1	407.3	395.1	335.9	397.4
BattleZone	2360.0	37187.5	12805.6	13044.0	12990.4	13255.0
Boxing	0.1	12.1	32.7	37.5	34.8	39.2
Breakout	1.7	30.5	14.6	17.2	14.8	16.0
ChopperCommand	811.0	7387.8	1015.6	1073.5	934.8	1142.2
Crazy Climber	10780.5	35829.4	38483.9	49399.0	43085.6	52008.1
Demon Attack	152.1	1971.0	523.8	531.4	504.6	532.1
Freeway	0.0	29.6	22.1	24.1	22.5	25.2
Frostbite	65.2	4334.7	1635.2	1855.6	1563.9	1699.4
Gopher	257.6	2412.5	695.3	1010.0	789.3	912.1
Hero	1027.0	30826.4	5763.9	5775.2	5603.8	6118.5
Jamesbond	29.0	302.8	388.4	312.8	344.9	319.7
Kangaroo	52.0	3035.0	2667.9	3569.3	2848.7	3296.0
Krull	1598.0	2665.5	4209.2	5614.5	4411.2	5467.7
Kung Fu Master	258.5	22736.3	12287.9	18511.0	16394.6	17510.9
Ms Pacman	307.3	6951.6	1141.3	1317.1	1514.7	1663.5
Pong	-20.7	14.6	-9.9	-6.0	-6.5	-6.1
Private Eye	24.9	69571.3	73.2	76.6	87.5	88.9
Qbert	163.9	13455.0	696.7	773.8	736.8	814.9
Road Runner	11.5	7845.0	12659.2	13385.0	13110.4	13708.8
Seaquest	68.4	42054.7	593.6	650.3	641.0	697.9
Up N Down	533.4	11693.2	29425.4	44295.4	39076.6	52118.4
Mean Human-Norm’d	0.000	1.000	0.682	0.859	0.749	0.886
Median Human-Norm’d	0.000	1.000	0.337	0.418	0.377	0.398
# Superhuman games	0	N/A	6	8	6	7
15
Under review as a conference paper at ICLR 2022
B.2	Model schematic
Q-Iearning
(Rainbow)
latent representation
function with parameter θ
Action
Equivariant
action dependent state
transition in the environment
-4¼ copy of parametric function fθ
(no gradient flow)
Iinear group action in the -^→∙ non-linear group action in
latent space	the original state space
Figure 5: A schematic of the EqR model, applied to model-free RL. Green in the framework cor-
responds to learning equivariance under the agent’s action and red corresponds to learning equiv-
ariance of the transition model with respect to symmetry transformation of the state-action. This
color scheme is consistent with Figure 2. The part of the framework that corresponds to reward
matching and Q-learning is shown in blue and brown respectively. The arrows in the schematic are
differentiated by their heads and are described in the legend.
B.3	Network Architecture
We follow the baseline RL implementation of DrQ (Yarats et al., 2021) and SPR (Schwarzer et al.,
2021) by using the 3-layer convolutional encoder from (Mnih et al., 2015) and then use a linear layer
to get the parameters for the Group Parameterization. The output size of this layer varies depending
on the group type, the number of blocks used and the size of the group. This defines our τθ . Note
that the output of our encoder is a matrix for GL(n) and SE(n). We flatten it before we feed to
other neural network like the Q-head qξ (∙).
For the action encoder κ(∙) we use a simple 1 layer MLP with batchnorm, ReLU and a hidden
size of 256. We concatenate the one-hot encodings of the actions with the state representations
coming from τθ and pass it through the action encoder to get matrix representation of the group after
parameterization.
For the reward predictor network rψ we use a 2-layered MLP with batchnorm, ReLU and a hidden
size of 256.
For the Q-head qξ(∙) We use 2-layered MLP as well.
For the projection head p《(∙) We share the first layer of Q-head whereas for projection head bη(∙)
we use a single layer MLP.
16
Under review as a conference paper at ICLR 2022
B.4	Algorithm
Algorithm 1: Equivariant Representations for RL
Denote the parameters of online networks τθ, κφ, pζ , bη as Θo
Denote the parameters of target networks τ , κ, p, b as Θc
Denote the parameters of networks ρω , qξ as Φ
Denote the dept of the prediction as M and batch size as N
Initialize the replay buffer B
while Training do
Collect {s, a, r, s'} using policy with (ΘC, Φ) and add to the buffer B
Sample a minibatch of M length sequences {s0：m , aoM, r°：M } Z B
for i in range(0, N ) do
if augmentation then
I s0M — augment(SOM )
end
xO — τθ (SO);
χ0 — xO
li — 0
for k in range(1, M + 1) do
Xk - kφ(Xk-ι,ak-I)Xk-I;	//
Xk - T(Sk);	.
Zk - PZ(Xk), zk - PZ(Xk);
I+ λ2∣∣ τz^+⅛- τ⅛ 12;
rk - rΨ (Xk);
Ii -Ii + λι∣rik - rik ∣2;
end
jZ{0,..,N-1};
// state representation
state transition by group action
// target state representation
// projections
// compute LAET at step k
// predict rewards
// compute LR at step k
// uniformly sample an index
Xo — τ(s0); // encode the state for that index from the batch
i j i -1
Tg = x%xO ;	// find the group representation
X1 — TgXI;	// next state by group action
X1 — pω(Tg)k(xO, aO)X0;	// next state by action-embedding
y1 - bη (xD, yi —b(Xjι);	// projections
li ― li + λ3∣ JyyiIi[2 - jyyij212;	// COmPute LGET
li — li + RLloSS(X0, aO,r0,X；; qξ)
end
l - Nn ∑n=0 lk;
Θo, Φ — oPtmize((Θo, Φ), l);
Θc — Θo;
// average over minibatch
// update online networks
// copy weights to target networks
end
17
Under review as a conference paper at ICLR 2022
B.5 Hyperparameters
In this section, we provide the full set of hyperparameters in our model. As mentioned earlier, our
baseline RL algorithm closely follows SPR’s (Schwarzer et al., 2021) implementation of Rainbow
and hence we use most of their hyperparameters setting in order to be able to compare to them. Note
that the weights of LR - λ1, LAET - λ2 and LGET - λ3 are set to one whenever they are used in the
model.
Table 5: Hyperparameters for ErQ (including variations) on Atari.
Parameter	Setting		
Gray-scaling Observation down-sampling Frames stacked Action repetitions Reward clipping Terminal on loss of life Max frames per episode Update Dueling Support of Q-distribution Discount factor Minibatch size Optimizer Optimizer: learning rate Optimizer: β1 Optimizer: β2 Optimizer: Max gradient norm Priority exponent Priority correction Exploration Noisy nets parameter Training steps Evaluation trajectories Min buffer size for sampling Replay period every Updates per step Multi-step return length Prediction depth, M λ1 λ2 λ3 Data Augmentation		True 84×84 4 4 [-1, 1] True 108K Distributional Q True 51 0.99 32 Adam 0.0001 0.9 0.999 0.00015 10 0.5 0.4→1 Noisy nets 0.5 100K 100 2000 1 step 2 10 5 1 1 1 Random shifts (±4 pixels) Intensity(scale=0.05)	
Parameter	Setting(T2)	Setting (SE2)	Setting (GL2)
Num Blocks (K) Group Action	32 Addition	12 MatMul	12 MatMul
18