Under review as a conference paper at ICLR 2022
Bayesian Exploration for
Lifelong Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
A central question in reinforcement learning (RL) is how to leverage prior knowl-
edge to accelerate learning in new tasks. We propose a Bayesian exploration
method for lifelong reinforcement learning (BLRL) that aims to learn a Bayesian
posterior that distills the common structure shared across different tasks. We further
derive a sample complexity analysis of BLRL in the finite MDP setting. To scale
our approach, we propose a variational Bayesian Lifelong Learning (VBLRL)
algorithm that is based on Bayesian neural networks, can be combined with recent
model-based RL methods, and exhibits backward transfer. Experimental results on
three challenging domains show that our algorithms adapt to new tasks faster than
state-of-the-art lifelong RL methods.
1	Introduction
Reinforcement-learning (RL) methods (Sutton & Barto, 1998; Kaelbling et al., 1996) have been
successfully applied to solve challenging individual tasks such as learning robotic control (Duan
et al., 2016) and playing expert-level Go (Silver et al., 2017). However, in the real world, a robot
usually experiences a collection of distinct tasks that arrive sequentially throughout its operational
lifetime; learning each new task from scratch is infeasible, but treating them all as a single task will
fail. Therefore, recent research has focused on algorithms that enable agents to learn across multiple,
sequentially posed tasks, leveraging past knowledge from previous tasks to accelerate the learning of
new tasks. This problem setting is known as lifelong reinforcement learning (Brunskill & Li, 2014;
Wilson et al., 2007b; Isele et al., 2016b). The key questions in lifelong RL research are: How can an
algorithm exploit knowledge gained from past tasks to improve performance in new tasks (forward
transfer), and how can data from new tasks help the agent to perform better on previously learned
tasks (backward transfer)?
To answer these two questions, first consider a simple problem, which is to find different items in
different houses. Here, a single task corresponds to finding items in a specific house. Although items
may be stored in different locations in different houses, there still exists some shared information
that connects all houses. For instance, a toothbrush is more likely to be found in a bathroom than
a kitchen, and a room without a window is more likely to be a bathroom than a living room. Such
information can significantly accelerate the search for items in newly encountered houses. We propose
that extracting the common structure existing in previously encountered tasks can help the agent
quickly learn the dynamics of the new tasks. Specifically, this paper considers lifelong RL problems
that can be modeled as hidden-parameter MDPs or HiP-MDPs (Doshi-Velez & Konidaris, 2016;
Killian et al., 2017), where variations among the true task dynamics can be described by a set of
hidden parameters. We model two main categories of learning across multiple tasks: the world-model
distribution, which describes the probability distribution over tasks, and the task-specific model, that
defines the (stochastic) dynamics within a single task. To enable more accurate sequential knowledge
transfer, we separate the learning process for these two quantities and maintain a hierarchical Bayesian
posterior to approximate them. The world-model posterior is designed to manage the uncertainty in
the world-model distribution, while the task-specific posterior handles the uncertainty from the data
collected from only the current task.
We propose a Bayesian exploration method for lifelong RL (BLRL) that learns a Bayesian world-
model posterior that distills the common structure of previous tasks, and then uses it as a prior to
learn a task-specific model in each subsequent task. For the discrete case, we derive an explicit
1
Under review as a conference paper at ICLR 2022
performance bound that shows that the task-specific model requires fewer samples to become accurate
as the world-model posterior approaches the true underlying world-model distribution. We further
develop VBLRL, a more scalable version of BLRL that uses variational inference to approximate the
world-model distribution and leverages Bayesian Neural Networks (BNNs) to build the hierarchical
Bayesian posterior. Our experimental results on a set of challenging domains show that our algorithms
achieve better forward and backward transfer performance than state-of-the-art lifelong RL algorithms
within limited samples for each task.
2	Background
RL is the problem of maximizing the long-term expected reward of an agent interacting with an
environment (Sutton & Barto, 1998). We usually model the environment as a Markov Decision
Process or MDP (Puterman, 1994), described by a five tuple: hS, A, R, T, γi, where S is a finite set
of states; A is a finite set of actions; R : S × A 7→ [0, 1] is a reward function, with a lower and upper
bound 0 and 1; T : S × A 7→ Pr(S) is a transition function, with T(s0|s, a) denoting the probability
of arriving in state s0 ∈ S after executing action a ∈ A in state s; and γ ∈ [0, 1) is a discount factor,
expressing the agent’s preference for delayed over immediate rewards.
An MDP is a suitable model for the task facing a single agent. In the lifelong RL setting,
the agent instead faces a series of tasks τ1 , ..., τn, each of which can be modeled as an MDP:
hS(i) , A(i) , R(i) , T(i) , γ(i)i. A key question is how these task MDPs are related; we model the
collection of tasks as a HiP-MDP (Doshi-Velez & Konidaris, 2016; Killian et al., 2017), where a
family of tasks is generated by varying a latent task parameter ω drawn for each task according to
the world-model distribution Pq. Each setting of ω specifies a unique MDP, but the agent neither
observes ω nor has access to the function that generates the task family. Formally, then, the dynamics
T(s0∣s, a; ωi) and reward function R(r∣s, a; ω/ for task i depend on ωi ∈ Ω, which is fixed for the
duration of the task. For lifelong RL problems, the performance of a specific algorithm is usually
evaluated based on both forward transfer and backward transfer results (Lopez-Paz & Ranzato, 2017):
•	Forward transfer: the influence that learning task t has on the performance in future task k	t.
•	Backward transfer: the influence that learning task t has on the performance in earlier tasks k Y t.
3 BAYESIAN EXPLORATION FOR LIFELONG REINFORCEMENT LEARNING
Figure 1: Plate representation
for the BLRL approach. Tj
denotes trajectory {s, a, r, s0}j.
There are K different tasks and
the agent samples R trajecto-
ries from each task.
The key component of our approach is a hierarchical Bayesian
posterior over task MDPs controlled by the hidden parameter ω.
Intuitively, We maintain probability distributions that separately
capture two categories of uncertainty within lifelong learning tasks:
the world-model posterior captures the epistemic uncertainty of
the world-model distribution over different tasks controlled by the
hidden parameter. As the learner is exposed to more and more
tasks, this posterior should converge to the world-model distribution
Pω. The task-specific posterior captures the epistemic uncertainty
of the current task m. As the learner is exposed to more and
more transitions within the task, this posterior should approach m,
leaving only the aleatoric uncertainty of transitions within the task,
which is independent of other tasks. By sampling from the world-
model posterior, an agent can learn new tasks faster by exploiting
knowledge common to previous tasks, thus exhibiting positive
forward transfer.
We first consider the finite MDP setting. Concretely, we use a hi-
erarchical Bayesian model to represent the distribution over MDPs.
Figure 1 shows our generative model in plate notation. Ψ is the parameter set that represents dis-
tribution PΩ. It functions as the world-model posterior that aims to capture the common structure
across different tasks. The resulting MDP mi is created based on ωi , which is one hidden parameter
sampled from Ψ. We can sample from our approximation of Ψ to create and solve possible MDPs.
2
Under review as a conference paper at ICLR 2022
The proposed BLRL approach is formalized in Algorithm 3 in the appendix. Initially, before any
MDPs are experienced, the world-model posterior qe(∙∣st, at) is initialized to an uniformed prior. For
each new task mi, We first initialize the task-specific posterior q}i (∙∣st, at) with the parameter values
from the current world-model posterior, and then, for each timestep, select actions using a Bayesian
exploration algorithm based on sampling from this posterior (Thompson, 1933; Asmuth et al., 2009).
A set of sampled MDPs drawn from qθmi is a concrete representation of the uncertainty within the
current task. BLRL samples K models from the task-specific posterior whenever the number of
transitions from a state-action pair has reached threshold B. Analogously to RMAX (Brafman &
Tennenholtz, 2003) and BOSS (Asmuth et al., 2009), we call a state-action pair known whenever
it has been observed Nst,at = B times. For each state-action pair, if it is known, we use the task-
specific posterior to sample the model. If it is unknown, we instead sample from the world-model
posterior. These models are combined into a merged MDP mi# and BLRL solves mi# to get a
policy ∏* #. This approach is adopted from BOSS (best of sampled set) to create optimism in the
mi
face of uncertainty, and thereby drive exploration. The new policy ∏* # will be used to interact
m
i
with the environment until a new state-action pair reaches the sampling threshold. The collected
transitions from this task will be used to update the task-specific posterior immediately, while the
world-model posterior will be updated using transitions from all the previous tasks at a slower pace.
For simple finite MDP problems in practice, we use the Dirichlet distribution (the conjugate for the
multinomial) to represent the Bayesian posterior. Thus, the updating process for the posterior is
straightforward to compute. Intuitively, BLRL is able to rapidly adapt to new tasks as long as the
prior of the task-specific model (that is, the world-model posterior) is close to the true underlying
model and captures the uncertainty of the common structure of a set of tasks.
3.1 Sample Complexity Analysis
We now provide a simple theoretical analysis of BLRL. First, we use the setting and results of Zhang
(2006) to describe the properties of the Bayesian prior and how it relates to the sample complexity for
the concentration of the Bayesian posterior.
Lemma 1. Let π(ω) denote the prior distribution on the parameter space Γ. We consider a set of
transition-probability densities p(∙∣ω) indexed by ω, and the true underlying density q. Define the
prior-mass radius of the transition-probability densities as:
dπ = inf {d : d ≥ - lnπ({p ∈ Γ : DKL(q||p) ≤ d})}.
(1)
Intuitively, this quantity measures the distance between the Bayesian prior we use to initialize the
posterior and the true underlying distribution. Then, ∀ρ ∈ (0, 1) and η ≥ 1, let
εn = (I + n )ηdπ + (η - P)εupper,n((η - 1)/(n - P)),	(2)
where εupper,n is the critical upper-bracketing radius (Zhang, 2006). The decay rate of εupper,n
controls the consistency of the Bayesian posterior distribution (Asmuth et al., 2009). Let P = 2, we
have for all t ≥ 0 and δ ∈ (0, 1), with probability at least 1 - δ,
πn({p ∈r: 11p-q||2/2 ≥ 2εn+δ4n- 2)t ok)≤ J.	(3)
Proof (sketch). The proof is similar to that of Corollary 5.2 of Zhang (2006) (see Appendix A.5).
Instead of using the critical prior-mass radius επ,n to describe certain characteristics of the Bayesian
prior, we define and use the prior-mass radius dπ, which is independent of the sample size n and
measures the distance between the prior and true distribution.	□
Similar to BOSS, for a new MDP m 〜M with hidden parameters ωm,, we can define the Bayesian
concentration sample complexity for the task-specific posterior: f(s, a, 0, δ0, P0), as the minimum
number c such that, if c IID transitions from (s, a) are observed, then, with probability at least 1 - δ0,
PIrm〜Posterior ( || Tm(S, a, ωm) - Tm*(s,a, ωm) ||1 < EO) ≥ 1 - p0.	(4)
3
Under review as a conference paper at ICLR 2022
Lemma 2. Assume the posterior is consistent (that is, εupper,n = o(1)) and set η = 2, then
the Bayesian concentration sample complexity for the task-specific posterior f (s, a, , δ, ρ) =
o( d∏+ιn⅛).
2δ-dπ
Proof(Sketch). This bound can be derived by directly combining Lemma 1 and Equation 4.	□
The above lemma suggests an upper bound of the Bayesian concentration sample complexity using
the prior-mass radius. We can further combine this result with PAC-MDP theory (Strehl et al., 2006)
and derive the sample complexity of the algorithm for each new task.
Theorem 1. For each new task, set the sample size K = Θ(SδA ln SA) and the parameters ∈o =
e(1 - Y)2, δo = SA, ρo = s2A2κ, then, with probability at least 1 - 4δ, VAt (St) ≥ V*(st) - 4e0
in all but O( δS2(A-Y)6) steps, where O(∙) suppresses logarithmic dependence.
Proof (sketch). The proof is based on the PAC-MDP theorem (Strehl et al., 2009) combined with the
new bound for the Bayesian concentration sample complexity we derived in Lemma 2. In general,
we use the same process in BOSS to verify the three required properties of PAC-MDP: optimism,
accuracy and learning complexity. For each new task, the main difference between BLRL and BOSS
is that we use the world-model posterior to initialize the task-specific posterior, which results in a
new sample complexity bound based on the prior-mass radius.	□
The result formalizes the intuition that, if we put a larger prior mass at a density that is close to the
true q such that dπ is small, the sample complexity of our algorithm will be lower. In the meantime,
the sample complexity is bounded by polynomial functions of the relevant quantities, showing that
our training strategy preserves the properties required by PAC-MDP algorithms (Strehl et al., 2009).
4 S caling Up: Variational Bayesian Lifelong RL
Directly computing the exact posterior is typically not possible for large scale problems. Instead,
we propose a practical approximate algorithm, VBLRL, that uses neural networks and variational
inference (Hinton & van Camp, 1993a). We model the posterior via the transition dynamics using
p(st+ι, rt∣st, at; θ),θ ∈ Θ. The posterior, given a new state-action pair, can be rewritten via Bayes'
rule:
p(∖rD	p	P(θlDt)P(st+ι, rt|Dt, at ； θ)	代、
P(θlDt, at, st+ι, rt) =-----7-------rτs——ʌ------,	(5)
p(st+1,rt|Dt, at)
where t is the agent’s history with all the experienced tasks up until time step t. As representing
the posterior p(θ∣D) is intractable, we approximate it through an alternative distribution q(θ; φ) by
minimizing DκL[q(θ; φ)∣∣p(θ∣D)], leveraging variational lower bounds (Hinton & van Camp, 1993b;
Houthooft et al., 2016).
We choose Bayesian neural networks (BNN) to approximate the posterior. The intuition is that, in the
context of stochastic outputs, BNNs naturally approximate the hierarchical Bayesian model since
they also maintain a learnable distribution over their weights and biases (Graves, 2011; Houthooft
et al., 2016). We expect the uncertainty embedded in the weights and biases of networks can capture
the epistemic uncertainty introduced by hidden parameters of different tasks, while we also set the
outputs of the neural networks to be stochastic to capture the aleatoric uncertainty within each specific
task. In our case, the BNN weights and biases distribution q(θ; φ) can be modeled as fully factorized
Gaussian distributions (Blundell et al., 2015):
©
q(θ; φ) = YN(θi∖μi,σi'),	⑹
i=1
where φ = {μ, σ}, and μ is the Gaussian,s mean vector while σ is the covariance matrix diagonal.
Then, the posterior distribution over the model parameters can be computed through:
φt = argmin [Dkl0(Θ; φ)∣∣p(θ)] - Eθ〜q(.@)[logp(st+ι,rt∣Dt,at; θ)f∣,
(7)
4
Under review as a conference paper at ICLR 2022
where p(θ) represents the fixed prior distribution of θ. The second term on the right hand side can
be approximated through R PN=I logp(st+ι,rt∣Dt, at； θi) with N samples from θi 〜q(φ). This
optimization can be performed in parallel for each s, keeping φt-1 fixed.
4
O
Pω
ωm ɪɪ›ʌ
P(s,,r|s,a;ωm)
l=>. Aleatory
/、uncertainty
EPiStemic
uncertainty
Aleatory
uncertainty
J
Epistemic
,uncertainty
Figure 2: How VBLRL estimates different kinds of uncertainties in HiP-MDP. The world-model
posterior captures the epistemic uncertainty of the general knowledge distribution (shared across all
tasks controlled by the hidden parameters) via the internal variance of world-model BNN. As the
learner is exposed to more and more tasks, the posterior should converge to Pq. The task-specific
posterior captures the epistemic uncertainty of the current task m, which comes from the alleatory
uncertainty of the world model when generating ωm for a new task, via the internal variance of
task-specific BNN. The posterior should output the highest probability for ω near the true ωm as the
agent collects enough data from the task. The aleatory uncertainty of the final prediction is measured
by the output variance of the prediction.
We provide the intuition of how our design capture the uncertainties of lifelong RL in Figure 2 and
summarize the method in Algorithm 1. The left side of the figure shows the process of how transitions
are generated from the environment’s true distribution, while the other parts show how our models
generate transition predictions and how they separately estimate different uncertainties generating
from approximating the true underlying distribution. We employ our posterior knowledge models
in the context of a model-based RL method. When encountering a new task, VBLRL first uses the
model parameters (that is, {μ,σ} of weights and biases of BNN) from the general knowledge model
to initialize the task-specific posterior network. The task-specific model outputs the predicted next
state and reward given a state-action pair. Then, We use model-predictive control (Garcia et al., 1989)
to select actions based on the generated transitions.
For planning, at each step, we begin by creating P particles from the current state sτp=t = st∀p. Then
we sample N candidate action sequences at:t+T from a learnable distribution. We propagate the
state-action pairs using the learned task-specific model Pmi (∙∣s, a) (BNN) and use the cross entropy
method (Botev et al., 2013) to update the sampling distribution to make the sampled action sequences
close to previous action sequences that achieved high reward. We further calculate the cumulative
reward estimated (via the learned model) for previously sampled sequences and use the mean of that
distribution to select the current action.
The task-specific posterior is updated using the data collected from only the current task. The
world-model posterior is updated after a few more steps with the collected transitions from all
the visited tasks. The intuition is to guide the two posteriors to separately learn two categories of
uncertainty within lifelong learning tasks. Note that other CEM-based model-based RL algorithms
like PETS (Chua et al., 2018) usually maintain a set of neural networks using the same training
data, and sample action sequences from each of the neural nets to achieve randomness in transitions.
Besides the problem of requiring special training tricks, it is unrealistic to maintain (≥ 30) models for
each task in lifelong RL settings. Our usage of BNNs avoids such problems as we only have to train
one neural network using the same data for each task, and we can sample an unlimited number of
different action sequences to cover more possibilities as needed. In PETS, the epistemic uncertainty
5
Under review as a conference paper at ICLR 2022
is estimated via the variance of the output mean of different neural networks, while in VBLRL, it
is estimated via the variance of the weights and biases distribution of BNN during training. This is
implied as the objective function We use to update φt is from minimizing DκL[q(θ; φ)∣∣p(θ∣D)].
Algorithm 1: Variational Bayesian Lifelong RL
Initialize general knowledge model Pwm(∙∣s, a; θwm), replay buffer Dmh, •…,DmM
for each task m% from i = 1, 2, 3,•…,M do
Initialize task-specific modelPmi(∙∣s, a; θmi) with general knowledge modelPwm
for each episode do
for Time t = 0 to TaskHorizon do
Sample Actions at：t+T 〜CEM(∙)
Propagate state particles sτp with Pmi (s0 |s, a)
Evaluate actions as PTtT p1 PPLI Pmi(r|s, a)
Update CEM(∙) distribution.
Execute optimal actions。；力十7
end
Add transitions to replay buffer Dmi
Update task-specific model according to Equation (7) given Dmi
Update general knowledge model according to Equation (7) given {Dm,、,…，DmJ
end
end
4.1 Backward Transfer of Variational Bayesian Lifelong RL
In our lifelong RL setting, the agent interacts with each task for only a limited number of episodes
and the task-specific model stops learning when the next task is initiated. As a result, there may
exist portions of the transition dynamics in which model uncertainty remains high. However, as the
world-model posterior continues to train on new tasks, it gathers more experience in the whole state
space and can provide improvements in its guesses concerning the “unknown” transition dynamics,
even for previously encountered tasks.
Intuitively, the performance of an agent on one task has the potential to be further improved (positive
backward transfer) if there exists a sufficiently large set of state-action transition pairs of which the
task-specific model’s predictions are not confident due to lack of data. This type of model uncertainty
is sometimes called epistemic uncertainty (Kiureghian & Ditlevsen, 2009; Ciosek et al., 2020). In
our algorithm, the aleatory variability (irreducible chance in the outcome) is measured by the output
variance of the prediction {σrτp, σsτp }, and the epistemic uncertainty (due to lack of experience)
corresponds to the uncertainty of the output mean and variance (see Definition 1 below). Thus, a
straightforward method to improve a previously learned task-specific model is to find the predictions
it needs to make that have high epistemic uncertainty, and replace them with the predictions from the
world-model posterior, which has lower epistemic uncertainty. If we only consider reward prediction,
the conditions for measuring whether a task-specific model is sufficiently confident are as follows.
Definition 1. Assume there exist known constants δ*r, δσr. For a given state-action pair (s, a), the
task-specific model (reward) is proclaimed confident when the following conditions are satisfied:
Pp=1(NrT - μrp )2
P - 1
/K	Ppp=1(σrP - σrP )2
< δ"r, ------p~~i--------
< δσr,
(8)
where P is the number of particles. Similar definition applies to the task-specific model’s next-state
prediction. Intuitively, δμr and δσr function as the threshold tojudge whether the uncertainty of the
output mean or variance for each dynamic prediction is too high to be called as a confident prediction.
The detailed backward transfer testing algorithm can be found in the appendix. In practice, it is
often hard to find specific confidence thresholds (δμs, δμr, δσs, δσr) that are effective. Instead, we
implement a simpler approach: During planning, for each prediction, we compare the uncertainty of
the output mean and variance of the world model and the task-specific model, and then choose the
one with lower values, which indicates higher confidence level.
6
Under review as a conference paper at ICLR 2022
5 Experiments
5.1	Grid-World Item Searching
We first evaluate BLRL in a simple Grid-World domain. Our testbed consists of a collection of
houses, each of which has four rooms. The goal of each task is to find a specific object (blue, green or
purple) in the current house. The type of each room is sampled based on an underlying distribution
given by the environment. Each room type has a corresponding probability distribution of which kind
of objects can be found in rooms of this type. Different tasks/houses vary in terms of which rooms
are which types and precisely where objects are located in the room (the task’s hidden parameters).
To simplify the problem, instead of modeling the whole MDP distribution, we use BLRL to model
the object distribution as the Bayesian posterior and sample MDPs from the distribution. We use
BOSS with a fixed prior (no intertask transfer) as our baseline. The average training performance of
all 300 tasks are shown in Figure 3 top right. Each task consists of 10 epochs, with 21 sample steps
for each epoch. Within the limited steps allotted for each task, BLRL is able to discover and transfer
the common knowledge and helps the agent quickly adapt to new tasks as the training goes on. In
comparison, running BOSS with a fixed prior is able to find the optimal policy eventually but needs
more sample steps and learns more slowly than BLRL.
Figure 3: Top-left: Grid-World Item Searching; Top-right: Grid-World Item Searching evaluation
results; Bottom-left: Box-jumping Task; Bottom-right: Box-jumping Task evaluation results.
5.2	Box-Jumping Task
We use a simplified version of the jumping task (des Combes et al., 2018) as a testbed for the proposed
algorithm VBLRL. As shown in Figure 3 bottom left, the goal of the agent is to reach the right side of
the screen by jumping over the obstacle. The agent can only choose from two actions: jump and right.
It will hit the obstacle unless the jump action is chosen at precisely the right time. We set different
obstacle positions as different tasks, constituting the HiP-MDP hidden parameters. The 4-element
state vector describes the (x, y) coordinates of the agent’s current position, and its velocity in the x
and y directions.
Figure 3 bottom right presents the average performance during training across all 300 tasks. Each
task is run for 30 episodes. We compare VBLRL against state-of-the-art lifelong RL methods LPG-
FTW (Mendez et al., 2020) from the multi-model category, EWC (Kirkpatrick et al., 2016) from
the single-model category, singel-task model-base RL baseline (using the same BNN structure and
planning procedures) as well as a HiP-MDP baseline (Killian et al., 2017). For a fair comparison, we
further replace the DDQN algorithm (van Hasselt et al., 2016) used in Killian et al.’s paper with CEM
planning, and let the transition model also predict the reward for each state-action pair. This modified
7
Under review as a conference paper at ICLR 2022
baseline is similar to the single-model version of VBLRL (i.e., only using the world-model posterior).
VBLRL clearly learns faster than the HiP-MDP baseline and reaches better final performance. Thus,
in the lifelong RL setting, separating the updating processes of the world-model posterior and the
task-specific posterior can lead to better learning efficiency.
5.3	OpenAI Gym MuJoCo Domains
We evaluate the performance of VBLRL on HiP-MDP versions of several continuous control tasks
from the Mujoco physics simulator (Todorov et al., 2012), HalfCheetah-gravity, HalfCheetah-
bodyparts, Hopper-gravity, Hopper-bodyparts, Walker-gravity, Walker-bodyparts, all of which are
lifelong-RL benchmarks used in prior work1 (Mendez et al., 2020). For each of six different domains,
the task-specific hidden parameters correspond to different gravity values or different sizes and
masses of the simulated body parts. More details can be found in the appendix. Compared with prior
work, we substantially reduced the number of iterations that the agent can sample and train on (100
iterations for each task and a horizon of 100/200 for each iteration). We used such settings to increase
the difficulty of lifelong learning and ensure that no learning-from-scratch algorithm could obtain a
good policy in the available training time.
	VBLRL	HiP-MDP baseline	LPG-FTW	EWC	Single-taskMBRL
	 CG -Start	160.68 ± 48.80	126.95 ± 31.41	-81.59 ± 9.18	-3426.76 ± 827.99	-83.96 ± 60.10
CG-Train	226.72 ± 26.53	170.20 ± 39.92	-29.49 ± 11.03	-3440.66 ± 1007.50	-40.47 ± 10.68
CG-Back	231.79 ± 23.49	97.84 ± 22.04	-29.95 ± 11.64	-6672.33 ± 3748.63	/
CB-Start	110.74 ± 41.96	78.95 ± 18.43	-263.94 ± 40.80	-5016.93 ± 1708.10	-101.02 ± 39.11
CB-Train	173.97 ± 78.26	87.2 ± 9.42	-217.86 ± 42.82	-5454.52 ± 2145.82	-58.93 ± 33.24
CB-Back	181.60 ± 67.50	116.03 ± 17.35	-116.41 ± 65.64	-13889.31 ± 6851.05	/
HG -Start	-149.79 ± 28.7	-130.54 ± 14.86	16.15 ± 22.83	-614.80 ± 600.14	-408.46 ± 36.10
HG-Train	-125.40 ± 9.17	-110.19 ± 22.03	22.98 ± 34.34	-749.49 ± 654.04	-386.77 ± 65.77
HG-Back	-55.95 ± 74.32	-142.82 ± 16.30	-252.36 ± 106.91	-5816.74 ± 4103.66	/
HB -Start	-119.29 ± 17.15	-73.41 ± 36.99	-15.56 ± 48.63	-4701.72 ± 1527.74	-402.03 ± 30.03
HB-Train	-99.00 ± 8.05	-96.68 ± 38.725	41.29 ± 12.19	-7384.54 ± 3232.86	-394.77 ± 25.77
HB-Back	-76.47 ± 13.27	-92.45 ± 41.50	-186.08 ± 151.82	-7921.96 ± 1147.94	/
WG-Start	-19.53 ± 4.76	-20.86 ± 5.37	-290.35 ± 70.95	-467.54 ± 249.19	-440.89 ± 59.41
WG -Train	7.77 ± 6.38	1.57 ± 2.97	-94.34 ± 61.36	-361.65 ± 260.76	-359.82 ± 45.44
WG -Back	18.56 ± 7.42	9.79 ± 6.32	-90.41 ± 64.66	-734.90 ± 413.40	/
WB-Start	-43.01 ± 10.82	-64.62 ± 28.02	-315.60 ± 31.66	-1140.62 ± 180.21	-437.96 ± 26.24
WB -Train	-2.66 ± 3.67	-31.29 ± 30.07	-187.98 ± 72.03	-1131.03 ± 451.38	-367.79 ± 74.37
WB-Back	4.04 ± 2.95	-33.75 ± 40.00	-66.97 ± 74.71	--2563.70 ± 692.70	/
Table 1: Results on OpenAI Gym Mujoco domains. CG denotes Cheetah-Gravity, CB denotes
Cheetah-Bodyparts, HG denotes Hopper-Gravity, HB denotes Hopper-Bodyparts, WG denotes
Walker-Gravity, WB denotes Walker-Bodyparts.
The results are shown in Table 1. We compare our algorithm against the three algorithms described in
Section 5.2. For all six domains, we report the average performance of all the tasks at the beginning
of training (Start) and after all training for each new task (Train), as well as the average performance
for all previous tasks after training for a given number of tasks, which is the backward transfer test
(Back). As shown in the results, our method VBLRL shows better performance on all three test stages
of the HalfCheetah domain and Walker domain, as well as better backward transfer performance on
Hopper-gravity and Hopper-bodyparts than the other three algorithms. LPG-FTW exhibits better
forward training performance in the Hopper domain, but still shows some signs of catastrophic
forgetting as there is a huge gap between its training performance and backward performance. EWC
fails in most of the tasks as the tasks are diverse and we set very limited sample steps for each task,
which means it is hard to directly learn a single shared policy that achieves good performance. The
HiP-MDP baseline shows good results on some of the tasks because it is more sample-efficient to
learn a shared model across all the tasks and easier to capture the world-model uncertainty. However,
it cannot achieve as good performance as VBLRL as it is hard to model the task-specific uncertainty
using only one model across all tasks, which also leads to the negative backward transfer performance
on Cheetah-Gravity. Comparing VBLRL’s performance on the Train stage and Back stage, we also
find that it shows positive backward transfer results on most of the tasks, without showing patterns
of catastrophic forgetting. Overall, VBLRL’s world-model posterior contributes to better forward
1We changed the environment settings for Hopper and Walker to make them amenable to model-based RL
following Wang et al. (2019).
8
Under review as a conference paper at ICLR 2022
transfer performance (Start), the learning of task-specific posterior contributes to better forward
transfer training for each new task (Train), and the combination of these two posteriors guides the
agent to achieve better backward transfer performance (Back).
6	Related work
HiP-MDPs (Doshi-Velez & Konidaris, 2016) provide a framework for studying lifelong RL. Published
HiP-MDP methods use Gaussian Processes (Doshi-Velez & Konidaris, 2016) or Bayesian neural
networks (Killian et al., 2017) to find a single model that works for all tasks, which may trigger
catastrophic forgetting. Other single-model lifelong RL algorithms encourage transfer across tasks
by modifying objective functions. EWC (Kirkpatrick et al., 2016) imposes a quadratic penalty
that pulls each weight back towards its old values by an amount proportional to its importance for
performance on previously-learned tasks to avoid forgetting. There are several extensions of this work
based on the core idea of modifying the form of the penalty (Li & Hoiem, 2017; Zenke et al., 2017;
Nguyen et al., 2018). Another category of lifelong RL methods uses multiple models with shared
parameters and task-specific parameters to avoid or alleviate the catastrophic problem (Bou-Ammar
et al., 2014; Isele et al., 2016a; Mendez et al., 2020). The drawback of this method is that it is hard to
incorporate the knowledge learned from previous tasks during initial training on a new task (Mendez
et al., 2020).Nagabandi et al. (2019) introduce a model-based continual learning framework based
on MAML, but they focus on discovering when new tasks were encountered without access to task
indicators.
Research in Meta-RL (Wang et al., 2016; Finn et al., 2017) and multi-task RL (Parisotto et al., 2016;
Teh et al., 2017) settings also attempts to find ways to accelerate learning by transferring knowledge
from different tasks. Some work employs the MAML framework with Bayesian methods to learn
a stochastic distribution over initial parameter (Yoon et al., 2018; Grant et al., 2018; Finn et al.,
2018). Other work uses the collected trajectories to infer the hidden parameter, which is taken as
an additional input when computing the policy (Rakelly et al., 2019; Zintgraf et al., 2020; Fu et al.,
2021). Our method, however, focuses on problems where the tasks arrive sequentially instead of
having a large number of tasks available at the beginning of training. This sequential setting makes
it hard to accurately infer the hidden parameters, but opens the door for algorithms that support
backward transfer. Further, our method approximates the true HiP-MDP model by learning the
Bayesian posterior over past tasks and uses it to initialize a model for each new task, encouraging the
agent to explore places where the epistemic uncertainty of the world model is high.
Some prior work uses Bayesian methods in RL to quantify uncertainty over initial MDP mod-
els (Ghavamzadeh et al., 2015; Asmuth & Littman, 2011; Guez et al., 2012). Several algorithms
start from the idea of sampling from a posterior over MDPs for Bayesian RL, maintaining Bayesian
posteriors and sampling one complete MDP (Strens, 2000; Wilson et al., 2007a) or multiple MDPs
from this distribution (Asmuth et al., 2009). Instead of focusing on single-task RL, our algorithm
aims to find a posterior over the common structure among multiple tasks. Wilson et al. (2007a) uses a
hierarchical Bayesian infinite mixture model to learn a strong prior that allows the agent to rapidly
infer the characteristics of new environment based on previous tasks. However, it only infers the
category label of a new MDP and uses that information to find parameter values. Moreover, their
method only works in discrete settings and cannot be applied to the kind of continuous problems
we included in our evaluation. Lifelong learning has also been widely studied within the supervised
learning domain with explicit performance bounds (Baxter, 2000; Pentina & Lampert, 2014). Our
work is one of the first papers to give explicit sample complexity bounds for lifelong reinforcement
learning algorithm, where data efficiency is essentially important.
7	Conclusion
To address the lifelong RL problems, our work proposed to distill the shared knowledge from similar
MDPs and maintain a Bayesian posterior to approximate the distribution derived from that knowledge.
We gave a sample complexity analysis of the algorithm in the finite MDP setting. Then, we extended
our method to use variational inference, which scales better and supports both backward and forward
transfer. Our experimental results show that the proposed algorithms enables faster training on new
tasks through collecting and transferring the knowledge learned from preceding tasks.
9
Under review as a conference paper at ICLR 2022
References
J. Asmuth and M. Littman. Learning is planning: Near Bayes-optimal reinforcement learning via
Monte-Carlo tree search. In UAI, 2011.
John Asmuth, Lihong Li, Michael L. Littman, Ali Nouri, and David Wingate. A Bayesian sampling
approach to exploration in reinforcement learning. In UAI 2009, Proceedings of the Twenty-Fifth
Conference on Uncertainty in Artificial Intelligence, 2009, pp. 19-26. AUAI Press, 2009.
Jonathan Baxter. A model of inductive bias learning. J. Artif. IntelL Res., 12:149-198, 2000.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural networks. CoRR, abs/1505.05424, 2015.
Z. Botev, Dirk P. Kroese, R. Rubinstein, and P. L’Ecuyer. Chapter 3 - the cross-entropy method for
optimization. Handbook of Statistics, 31:35-59, 2013.
Haitham Bou-Ammar, Eric Eaton, P. Ruvolo, and Matthew E. Taylor. Online multi-task learning for
policy gradient methods. In ICML, 2014.
Ronen I. Brafman and Moshe Tennenholtz. R-max - a general polynomial time algorithm for
near-optimal reinforcement learning. 3(null), 2003. ISSN 1532-4435.
Emma Brunskill and Lihong Li. Pac-inspired option discovery in lifelong reinforcement learning. In
ICML, 2014.
G. Carpenter and S. Grossberg. The art of adaptive pattern recognition by a self-organizing neural
network. Computer, 21:77-88, 1988.
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement
learning in a handful of trials using probabilistic dynamics models. In Advances in Neural
Information Processing Systems 31: Annual Conference on Neural Information Processing Systems
2018, NeurIPS 2018, pp. 4759-4770, 2018.
Kamil Ciosek, Vincent Fortuin, Ryota Tomioka, Katja Hofmann, and Richard E. Turner. Conservative
uncertainty estimation by fitting prior networks. In 8th International Conference on Learning
Representations, ICLR 2020. OpenReview.net, 2020.
Remi Tachet des Combes, Philip Bachman, and Harm van Seijen. Learning invariances for pol-
icy generalization. In 6th International Conference on Learning Representations, ICLR 2018.
OpenReview.net, 2018.
Finale Doshi-Velez and George Dimitri Konidaris. Hidden parameter markov decision processes: A
semiparametric regression approach for discovering latent task parametrizations. In Proceedings
of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016, pp.
1432-1440. IJCAI/AAAI Press, 2016.
Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and P. Abbeel. Benchmarking deep reinforce-
ment learning for continuous control. In ICML, 2016.
Chelsea Finn, P. Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In ICML, 2017.
Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. In
Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information
Processing Systems 2018, NeurIPS 2018, pp. 9537-9548, 2018.
Robert M. French. Catastrophic interference in connectionist networks: Can it be predicted, can it
be prevented? In Advances in Neural Information Processing Systems 6, [7th NIPS Conference,
1993], pp. 1176-1177. Morgan Kaufmann, 1993.
Haotian Fu, Hongyao Tang, Jianye Hao, Chen Chen, Xidong Feng, Dong Li, and Wulong Liu.
Towards effective context for meta-reinforcement learning: an approach based on contrastive
learning. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, pp. 7457-7465.
AAAI Press, 2021.
10
Under review as a conference paper at ICLR 2022
C.	E. Garcia, D. M. Prett, and M. Morari. Model predictive control: Theory and practice - a survey.
Autom., 25:335-348,1989.
Mohammed Ghavamzadeh, Shie Mannor, Joelle Pineau, and Aviv Tamar. Convex optimization:
Algorithms and complexity. Foundations and Trends® in Machine Learning, 8(5-6):359-483,
2015. ISSN 1935-8245.
Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas L. Griffiths. Recasting
gradient-based meta-learning as hierarchical bayes. In 6th International Conference on Learning
Representations, ICLR 2018. OpenReview.net, 2018.
Alex Graves. Practical variational inference for neural networks. In Advances in Neural Information
Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011,
pp. 2348-2356, 2011.
A. Guez, D. Silver, and P. Dayan. Efficient bayes-adaptive reinforcement learning using sample-based
search. In NIPS, 2012.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35th
International Conference on Machine Learning, ICML 2018, pp. 1856-1865. PMLR, 2018.
Geoffrey E. Hinton and Drew van Camp. Keeping the neural networks simple by minimizing
the description length of the weights. In Proceedings of the Sixth Annual ACM Conference on
Computational Learning Theory, COLT 1993, pp. 5-13. ACM, 1993a.
Geoffrey E. Hinton and Drew van Camp. Keeping the neural networks simple by minimizing the
description length of the weights. In COLT ’93, 1993b.
Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. VIME:
variational information maximizing exploration. In Advances in Neural Information Processing
Systems 29: Annual Conference on Neural Information Processing Systems 2016, pp. 1109-1117,
2016.
David Isele, Mohammad Rostami, and Eric Eaton. Using task features for zero-shot knowledge
transfer in lifelong learning. In Proceedings of the Twenty-Fifth International Joint Conference on
Artificial Intelligence, IJCAI 2016, pp. 1620-1626. IJCAI/AAAI Press, 2016a.
David Isele, Mohammad Rostami, and Eric Eaton. Using task features for zero-shot knowledge
transfer in lifelong learning. In IJCAI, 2016b.
Leslie Pack Kaelbling, Michael L. Littman, and Andrew W. Moore. Reinforcement learning: A
survey. J. Artif. Intell. Res., 4:237-285, 1996.
Taylor W. Killian, Samuel Daulton, Finale Doshi-Velez, and George Dimitri Konidaris. Robust
and efficient transfer learning with hidden parameter markov decision processes. In Advances in
Neural Information Processing Systems 30: Annual Conference on Neural Information Processing
Systems 2017, pp. 6250-6261, 2017.
James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, An-
drei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis
Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic
forgetting in neural networks. CoRR, abs/1612.00796, 2016.
Armen Der Kiureghian and O. Ditlevsen. Aleatory or epistemic? does it matter? Structural Safety,
31:105-112, 2009.
Zhizhong Li and Derek Hoiem. Learning without forgetting, 2017.
David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In
Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information
Processing Systems 2017, pp. 6467-6476, 2017.
11
Under review as a conference paper at ICLR 2022
Jorge Armando Mendez Mendez, Boyu Wang, and Eric Eaton. Lifelong policy gradient learning of
factored policies for faster training without forgetting. ArXiv, abs/2007.07011, 2020.
Anusha Nagabandi, Chelsea Finn, and Sergey Levine. Deep online learning via meta-learning:
Continual adaptation for model-based rl. ArXiv, abs/1812.07671, 2019.
Cuong V. Nguyen, Yingzhen Li, Thang D. Bui, and Richard E. Turner. Variational continual learning,
2018.
Emilio Parisotto, Lei Jimmy Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and
transfer reinforcement learning. In 4th International Conference on Learning Representations,
ICLR 2016, 2016.
Anastasia Pentina and Christoph H. Lampert. A pac-bayesian bound for lifelong learning. ArXiv,
abs/1311.2838, 2014.
Martin L. Puterman. Markov Decision Processes—Discrete Stochastic Dynamic Programming. John
Wiley & Sons, Inc., 1994.
Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Efficient off-
policy meta-reinforcement learning via probabilistic context variables. In Proceedings of the
36th International Conference on Machine Learning, ICML 2019, volume 97 of Proceedings of
Machine Learning Research,pp. 5331-5340. PMLR, 2019.
Anthony V. Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connect. Sci., 7:123-146,
1995.
D.	Silver, Julian Schrittwieser, K. Simonyan, Ioannis Antonoglou, Aja Huang, A. Guez, Thomas
Hubert, Lucas baker, Matthew Lai, A. Bolton, Yutian Chen, T. Lillicrap, Fan Hui, L. Sifre, G. V. D.
Driessche, T. Graepel, and D. Hassabis. Mastering the game of go without human knowledge.
Nature, 550:354-359, 2017.
A. Strehl, Lihong Li, and M. Littman. Incremental model-based learners with formal learning-time
guarantees. ArXiv, abs/1206.6870, 2006.
Alexander L. Strehl, Lihong Li, and Michael L. Littman. Reinforcement learning in finite mdps: Pac
analysis. J. Mach. Learn. Res., 10:2413-2444, 2009.
Malcolm J. A. Strens. A Bayesian framework for reinforcement learning. In Proceedings of the
Seventeenth International Conference on Machine Learning (ICML 2000), pp. 943-950, 2000.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press,
1998.
Yee Whye Teh, Victor Bapst, Wojciech M. Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell,
Nicolas Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning. In
Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information
Processing Systems 2017, pp. 4496-4506, 2017.
William R. Thompson. On the likelihood that one unknown probability exceeds another in view of
the evidence of two samples. Biometrika, 25:285-294, 1933.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2012, pp.
5026-5033. IEEE, 2012.
Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,2016, pp.
2094-2100. AAAI Press, 2016.
Jane X. Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z. Leibo, Remi Munos,
Charles Blundell, Dharshan Kumaran, and Matthew Botvinick. Learning to reinforcement learn.
CoRR, abs/1611.05763, 2016.
12
Under review as a conference paper at ICLR 2022
Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi
Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking model-based reinforcement
learning. CoRR, abs/1907.02057, 2019.
Aaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli. Multi-task reinforcement learning:
A hierarchical Bayesian approach. In Machine Learning, Proceedings of the Twenty-Fourth
International Conference (ICML 2007), 2007, volume 227 of ACM International Conference
Proceeding Series ,pp.1015-1022. ACM, 2007a.
Aaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli. Multi-task reinforcement learning: a
hierarchical bayesian approach. In ICML ’07, 2007b.
Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn.
Bayesian model-agnostic meta-learning. In Advances in Neural Information Processing Systems
31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, pp.
7343-7353, 2018.
Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence,
2017.
Tong Zhang. From -entropy to kl-entropy: Analysis of minimum information complexity density
estimation. Annals of Statistics, 34:2180-2210, 2006.
Luisa M. Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann,
and Shimon Whiteson. Varibad: A very good method for bayes-adaptive deep RL via meta-learning.
In 8th International Conference on Learning Representations, ICLR 2020. OpenReview.net, 2020.
13
Under review as a conference paper at ICLR 2022
A Appendix
A. 1 Backward Transfer
We take backward transfer to be the influence of subsequent learning on the agent’s performance of a
previous task. Negative backward transfer, which is also known as catastrophic forgetting, describes
the scenario in which the agent’s progress on a task is erased after learning new tasks (French, 1993;
Carpenter & Grossberg, 1988; Robins, 1995). Our algorithm avoids catastrophic forgetting as it
maintains a separate task-specific model for each task. In the mean time, there are circumstances in
which VBLRL exhibits positive backwards transfer as introduced in the main text.
Algorithm 2: Variational Bayesian Lifelong RL (Backward transfer)
input: Test task mi, model uncertainty thresholds δμs, δμr, δσs, δσr
for Time t = 0 to TaskHorizon do
for Trial k = 1 to K do
Sample Actions at：t+T 〜CEM(∙)
for each action do
Propagate state particles sτp and reward rτp with Tmi (s0, r|s, a)
if Tmi (s0 , r|s, a) is not confident (Definition 1) then
I Repropagate state particles sT and reward rτ With Te(s0, r|s, a)
end
end
Evaluate actions as PTtT -p PP=I ‰ (r∣s,a)
Update CEM(∙) distribution.
end
Execute optimal actions a"+T
end
The difference between the backward transfer version and the forward training version is that, when
we do predictions for the state particles, each time we will also calculate the confidence level
(Definition 1) of the task-specific model for this particular transition. If the task-specific model is not
confident enough for this state-action pair, we will use the world-model instead and recalculate the
prediction value.
In practice, it is often hard to find specific confidence thresholds (δμs, δμr, δσs, δσr) that are effective.
Instead, we implement a simpler approach: During planning, for each prediction, we compare the
uncertainty of the output mean and variance of the world model and the task-specific model, and
then choose the one with lower values, which indicates higher confidence level. Here is a link to the
learned behaviors of VBLRL https://youtu.be/I7RAT6g9v5w. The video shows the agent
trained by our algorithm learns some meaningful behaviors at the current setting within limited steps,
and performs better compared with the behaviors learned by other baseline algorithms.
A.2 BNN model
As we model the transition models as Gaussian distributions:
t 3s,a) = N (fθ(s,a),fθ(s,a))
(9)
The function fθ represented as a Bayesian neural network parameterized by θ, which is further
modeled as the posterior distribution parameterized by φ, predicts the mean μs, μr and variance
σs, σr given current state and action s, a. To make it clearer, we can view the BNN model in VBLRL
as an infinite neural network ensemble by integrating out its parameters:
T(s0
r|s, a) =
Θ
T (s0, r|s, a; θ)q(θ; φ)dθ
(10)
Compared to previous model-based algorithms that use finite number of neural network ensembles
(e.g. PETS), our choice of BNN is more suitable for lifelong RL as we only need to maintain one
neural network for each task, and we can sample an unlimited number of predictions from it which
better estimates the uncertainty and is essential in our setting where both dynamic function and
reward function are not given unlike prior model-based RL methods.
14
Under review as a conference paper at ICLR 2022
A.3 BLRL algorithm
Algorithm 3: Lifelong Bayesian Sampling Approach Algorithm
input: K, B
initialize MDP set, the world-model posterior qe(st+1, rt|st, at);
for each MDP mi do
Ns,a J 0,∀s,a
do_sample J TRUE ；
initialize the task-specific posterior qθmi (st+1, rt|st, at) J qe(st+1, rt|st, at);
for all timesteps t = 1, 2, 3, ... do
if do_sample then
Sample K models m”, m^,…,miκ from the task-specific posterior
qθmi(st+1, rt|st, at).;
Merge the models into the mixed MDP mi# ;
Solve m# to obtain π* # ;
i	mi
do_sample J FALSE
end
Use n；# for action selection: at J nm# (St) and observe reward r and next state st+ι ;
Nst,at J Nst,at + 1;
Update the task-specific posterior distribution qθmi (st+1, rt|st, at) for the current MDP;
if Nst ,at = B then
Update the world-model posterior distribution qe(st+1, rt|st, at) with the collected
transitions;
do_sample J TRUE
end
end
end
A.4 Experimental Setting
A.4. 1 Grid-World Item Searching
Our testbed consists of a collection of houses, each of which has four rooms. The goal of each
task is to find a specific object (blue, green or purple) in the current house. The type of each room
is sampled based on an underlying distribution given by the environment. Each room type has a
corresponding probability distribution of which kind of objects can be found in rooms of this type.
Different tasks/houses vary in terms of which rooms are which types and precisely where objects are
located in the room (the task’s hidden parameters). Room types are sampled from a joint distribution.
Room type probability	Room 1	Room 2	Room 3	Room 4
Top-left	0.4	0	0.4	0.2
Bottom-left	0	0.8	0	0.2
Top-right	0.1	0	0	0.9
Bottom-right	0	0	0.8	0.2
Table 2: Room type probability distribution
A.4.2 Box-jumping Task
We use a simplified version of jumping task (des Combes et al., 2018) as a simple testbed for the
proposed algorithm VBLRL. We select a random position of obstacle between 15 〜33 for each
task. The 4-element state vector describes the (x, y) coordinates of the agent’s current position, and
its velocity in the x and y directions. The agent can choose from two actions: jump and right. The
reward function for this box-jumping task is:
Rt = I{st reach the right wall} 一 I{st+ι hit the obstacle} + Xt ∙ I{st+ι not hit the obstacle} (11)
15
Under review as a conference paper at ICLR 2022
Object type probability ∣ Blue ball ∣ Green box ∣ Purple box
Room 1	0	0.3	0
Room 2	0	0.2	1
Room 3	0.6	0	0
Room 4	0	0	0
Table 3: Object type probability distribution
A.4.3 OpenAI Gym Mujoco Domains
Figure 4: Left: Walker-2D; Middle: Halfcheetah; Right: Hopper
Similar to (Mendez et al., 2020), we evaluated on the HalfCheetah, Hopper, and Walker-2D environ-
ments. For the gravity domain, we select a random gravity value between 0.5g and 1.5g for each task.
For the body-parts domain, we set the size and mass of each of the four parts of the body (head, torso,
thigh, and leg) to a random value between 0.5× and 1.5× its nominal value. As shown in Appendix
C of (Mendez et al., 2020), these changes lead to highly diverse tasks for lifelong RL. Further, as
required by model-based deep RL methods, we change the environment settings for Hopper and
Walker following (Wang et al., 2019). When implementing VBLRL, we found that in the first few
Name of Environment ∣	Reward function
HalfCheetah Hopper	Xt - 0.1llat||2 Xt — 0.1∣∣at||2 — 3.0 X (zt	- 1.3)2
Walker2D	Xt — 0.1∣∣at∣∣2 — 3.0 x (Zt	- 1.3)2
Table 4: Reward functions for OpenAI Gym domains
episodes of each new tasks, the agent hasn’t collected enough samples of the new task, which results
in overfitting problems when training the task-specific. Thus, we use the world-model posterior
instead to do the first few rounds of predictions and let the task-specific model begin training after
collecting enough samples. The world-model has lower possibility of overfitting as its training data
comes from all the previous tasks and has much larger quantity. We list the other implementation
details below. The planning horizons are selected from values suggested by previous model-based
RL papers (Chua et al., 2018; Wang et al., 2019).
For LPG-FTW and EWC, we use the original source code2 with parameters and model architectures
suggested in the original paper. Specifically, we select step size from {0.005, 0.05, 0.5}. For LPG-
FtW, e use λ = 1e - 5,μ = 1e - 5 and select k from 3,5,10. For EWC, We select λ from
{1e - 6, 1e - 7, 1e - 4}. For HiP-MDP baseline, we modify the original algorithm for a fair
comparison. We replace the DDQN algorithm used in Killian et al.’s paper with the exact same CEM
planning method we used in VBLRL as well as the same parameters. And we use the same model
architecture of Bayesian Neural network by modifying the baseline algorithm to also predict reward
for each state-action pair (the original method only considers next-state prediction).
2https://github.com/Lifelong-ML/LPG-FTW
16
Under review as a conference paper at ICLR 2022
Hyper-parameters	CG	CB	HG	HB	WG	WB
# iterations	100	100	100	100	100	100
# Steps (each iteration)	100	100	200	200	200	200
learning rate (World model)	0.001	0.001	0.0005	0.0005	0.0005	0.0005
learning rate (task-specific model)	0.0005	0.0005	0.0002	0.0002	0.0002	0.0002
planning horizon	20	20	30	30	1	1
kl-divergence Weight	0.0001	0.0001	0.0001	0.0001	0.0001	0.0001
# particles (CEM)	50	50	50	50	50	50
batch size (World-model)	8 × 64	8 × 64	8 × 64	8 × 64	8 × 64	8 × 64
batch size (task-specific)	256	256	256	256	256	256
# tasks	40	40	30	30	40	40
search population size	500	500	500	500	500	500
# elites (CEM)	50	50	50	50	50	50
Table 5: Room type probability distribution
For BOSS and BLRL, we set the number of sampled models K = 5, and γ = 0.95, ∆ = 0.01 for
value iteration.
A.5 Detailed proof for lemma 1
Following the same settings introduced in section 5 of (Zhang, 2006), we define the resolvability of
standard Bayesian posterior as:
rn(q) = inf[E∏ ω(θ)DκL(q∣∣p(∙∣θ)) + 1 DκL(ωdπ∣∣dπ)]
ωn
=-1in En e-nDKL(q1|p(^^)).
n
(12)
We refer the readers to Zhang’s paper for further explanation of the denotations. Intuitively, the
Bayesian resolvability controls the complexity of the density estimation process. Based on this
definition and our previous definitions of dπ , we can derive a simple and intuitive estimate of the
standard Bayesian resolvability.
Lemma 3. The resolvability of standard Bayesian posterior defined in (12) can be bounded as
rn(q) ≤ n +1 d∏
n
proof. For all d > 0, we have
rn(q) = - 1lnEne-nDκL⑷"忸》≤ -1 ln[e-ndπ(p ∈ Γ : DκL(q∖∖p) ≤ d)]
nn
=d + 1 X [— ln∏(p ∈ Γ : DκL(q∖∖p) ≤ d)] ≤ n+1 d∏
nn
□
This bound links the Bayesian resolvability to the number of samples n and prior-mass radius
dn which is a fixed property of the density given a specific prior and the true underlying density.
Intuitively, the Bayesian posterior is better behaved when the Bayesian prior is closer to the true
distribution (dn is smaller) and more samples are used (n is larger).
NoW We can prove the main theorem of Lemma 1. Let P = 2, Et = 2n+；：,-2", define Γι =
{p ∈ Γ : DρRe(q∖∖p) < t} and Γ2 = {p ∈: DρRe(q∖∖p) ≥ t}. We let a = e-nt and define
π0 (θ) = aπ(θ)C When θ ∈ Γ1 and π0(θ)C When θ ∈ Γ2, Where the normalization constant
C = (aπ(Γ1) + π(Γ2))-1 ∈ [1, 1/a]. Firstly,
EXπ0(Γ2∖X)Et ≤ EXE∏0∏0(θ∖x)2∖∖p - q∖∖2 ≤ EXE∏0∏0(θ∖X)DκL(q∖∖p)
17
Under review as a conference paper at ICLR 2022
according to the Markov inequality (with probability at least 1 - δ) and Pinsker’s inequality. Then
according to Theorem 5.2 and Proposition 5.2 in Zhang’s paper,
EXE∏0∏0(Θ∣X)DκL(q∣∣p) ≤ 也nEπ
O e-nDκL⑷IpGM)
ρ(ρ- 1)n
+ ρ(n÷⅛n {inf}ln X ∏0(rj )(η-1"(η-P)(I+Tub(Fj))n
Vnt — (η∕n)ln En e-nDKL(q||p(TS))
ρ(1 - ρ)
(2η — 1)t	—(η∕n) ln Ene-
ρ(1 - ρ) +
,η - ρ h(n - 1)t∣u (n -八]
+ ρ(i - P) [ FT+εuppern vη^-7 川
TbDKL ⑷ IpGIS)) + (n - ρ)εupper,n ((η - 1)∕(n - P))
ρ(1 - ρ)
Then, using the definitions of dπ, we further obtain
EX π0(Γ2∣X )et
≤ (2η - 1)t + ηinfd>0[d - 1 lnπ({p ∈ Γ : DκL(q∣∣p) ≤ d})] + (η - ρ)εupper,n((η - 1)∕(η - P))
一P(I - P)	P(I - P)
≤ (2η - 1)t + η(1 + n)d∏ + (η - P)εupper,n((η - 1)∕(η - P))
一P(I - P)	P(I - P)
=(2η - 1)t + εn
P(I - P)
We use η instead of γ which is used in the original paper to avoid confusion with the discount factor.
Then We further divide both sides by Et and obtain ∏0(Γ∣X) ≤ 0.5. Then by definition,
∏(Γ2∣X) = a∏0(Γ2∣X)/(1 - (1 - a)π0(Γ∣X))
a1
≤ a +1 = 1 + ent
Thus, We get the desired bound.
A.6 Detailed proof for Theorem 1
Theorem 2. (Full version of the bound in Theorem 1) For each neW task, set the sample size
K = Θ(SδA ln SA) and the parameters €0 = e(1 - γ)2,δ0 = SA, Po = $2；2长,then, with
probability at least 1 - 4δ, VAt (St) ≥ V*(st) - 4e in all but O(S A 1'+-^ K)ln δ ln e(i-7))
steps.
First, we would like to introduce two lemma from BOSS (Asmuth et al., 2009):
Lemma 4. The sample size K = Θ( SδA ln SA) suffices to guarantee Vmm (s) ≥ V * (s) for all S
during the entire learning process with probability at least 1 - δ.
Lemma 5. If the knownness parameter B = maxs,a f (S, a, €，SA, S2A2K), then the transition
function of all the sampled models are €-close (in the l1 sense) to the true transition function for all
the known state-action pairs during the entire learning process with probability at least 1 - δ - P.
proof. Given discrete state and action spaces, the proof that BLRL on each new task is PAC-MDP
depends on three main assumptions, following a general PAC-MDP theorem from (Strehl et al.,
2006):
1.	Learning complexity condition. In our settings, a state-action pair is claimed to be known after
being visited for B times. In the meantime, there are SA unknown state-action pairs in total at the
beginning of each task, so the bounded discoveries condition is guaranteed.
18
Under review as a conference paper at ICLR 2022
2.	Optimism. This is guaranteed by Lemma 3. We construct a new hyper-model each time a discovery
event occurs. The values for each unknown state in every hyper-model are optimistic.
3.	Accuracy. For each new task, since the prior π has bounded sample complexity of B for δ0 and
0, a known state-action pair will be locally 0-accurate with probability at least 1 - δ0. Given the
definition of Bayesian concentration sample complexity and Lemma 4, 0 = (1 - γ)2 translates into
an error bound in the value function (Asmuth et al., 2009).
Finally, given Lemma 2, we know f (s, a, 0 , δ0 , ρ0)
O (d⅛⅛)，let S = e(I - Y)2,δ0
SA,ρo = S⅛K, We Can get f(s, a, e0, δ°,ρo) = O (SA瞋+；)；)^^). We Can get the final form
of the bound by replacing B with these quantities.
□
A.7 Additional explanation of the algorithm
Here We first provide an example to help the readers better understand our plate notation. In our
Gridworld Item Searching case, Ψ represents the parameters of Pω, which is the room-type and
objeCt distribution. For eaCh task, the environment samples a hidden parameter ω, WhiCh is the aCtual
room and object layout of this house, from this distribution Pω. The sampled ω then will result in an
MDP m and let the agent interact with it.
A.7.1 planning algorithm
With the transition dynamics and reward functions, a planning algorithm like CEM is not the only
way to solve the MDP to get an optimal policy. Another option would be using other Deep RL
algorithms like Soft actor-critic (Haarnoja et al., 2018) with data generated from the model. However,
in this case, incorporating a deep RL algorithm means that we need to introduce additional neural
networks (that is, policy/value networks) for each task. The update signal from the RL loss is usually
stochastic and weak, which is even worse in this case when our model is still far from accurate. So,
here we assume applying a planning algorithm is a better way to get the policy.
A.8 Coin Example
Consider a coin-flipping environment. We want to find the sample complexity of the unbiased coin
(i.e. How many times we need to flip this coin such that our posterior samples are accurate.). Consider
a Dirichlet prior, ɑ0 = (n1,n2) and θo = (11,11). We want to find sample complexity B such that
the posterior likelihood for a coin with heads likelihood in [0.5 - , 0.5 + ] is at least 1 - δ.
Note that the Dirichlet distribution on the two-dimensional simplex is the Beta distribution. The
Multinomial distribution with two outcomes is the Binomial distribution. That is, given the process
H 〜Bin (H ∣ρ = 0.5, B),	(13)
P 〜Beta(ρ∣α = H + nι, β = B — H + n1),	(14)
choose a value B such that
P(0.5 — E ≤ P ≤ 0.5 + C) ≥ 1 — δ,	(15)
B
^X Bin (H ∣ρ = 0.5, B) ∙
H=0
0.5+
√ρ=0.5-e
Beta (ρ∖α
H + n1, β = B — H + n1 ) ≥ 1 — δ.
(16)
Here, n1 and n1 capture the prior. The smallest B that satisfies Equation 16 can be found numerically.
We set C = 0.1 and δ = 0.3. Here are the results of sample complexity B given different values of
n1, n1 :
We fix the sum of (n1, n1 ) as 10. As shown in the results, the value of sample complexity B becomes
lower as we use a more accurate prior (from (10, 0) to (5, 5)).
19
Under review as a conference paper at ICLR 2022
(n1,n2)	lowest B
(0,10)	F
(1,9)	68
(2,8)	58
(3,7)	49
(4,6)	42
(5,5)	40
(6,4)	42
(7,3)	48
(8,2)	58
(9,1)	68
(10,0)	78
In general, for the task-specific posterior, we can relate B, and δ with the following equation:
P0
Dir(Po∣ΦTrue)[	X
N:||N||1=B
Mult(N|P0,B)
UP ：||P (Φ)-Po(Φ)W≤e
Dir(P∣Φoid+N)dP]]dPo ≥ 1-δ
For the world model posterior:
(17)
Σ
N:||N||1=Bw
Mult(N ∣Pwo, Bw)[/口 ()	(	Dir(Pw ∣Φw°id
+ N)dPw i ≥ 1 - δ (18)
For each task, first we pick a true model P0 according to the true distribution and initialize the
task-specific prior Φold = Φw . Then, we make some observations from the world. Once we have the
true model and the observations, we can calculate how many models are -close to the true model,
weighted according to their posterior likelihood.
20