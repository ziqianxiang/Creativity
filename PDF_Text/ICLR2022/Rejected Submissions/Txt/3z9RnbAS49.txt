Under review as a conference paper at ICLR 2022
A Theoretical and Empirical Model
of the Generalization Error
under Time-Varying Learning Rate
Anonymous authors
Paper under double-blind review
Ab stract
Stochastic gradient descent is commonly employed as the most principled opti-
mization algorithm for deep learning, and the dependence of the generalization
error of neural networks on the given hyperparameters is crucial. However, the
case in which the batch size and learning rate vary with time has not yet been
analyzed, nor the dependence of them on the generalization error as a functional
form for both the constant and time-varying cases has been expressed. In this
study, we analyze the generalization bound for the time-varying case by applying
PAC-Bayes and experimentally show that the theoretical functional form for the
batch size and learning rate approximates the generalization error well for both
cases. We also experimentally show that hyperparameter optimization based on
the proposed model outperforms the existing libraries.
1 Introduction
While deep learning addresses a variety of practical tasks, the reason deep learning generalizes well
is not fully understood. It is hoped that the performance of deep learning methods can be improved
by understanding deep learning. Consequently, deep learning will become a cornerstone for the
next generation of technology. Stochastic gradient descent (SGD) is a principled learning algorithm
for deep learning, and because most deep learning algorithms are developed based on SGD, its
theoretical analysis is crucial. He et al. (2019) modeled the generalization bounds when both the
batch size and learning rate are constant in SGD; however, they did not model the generalization
error. Furthermore, it is crucial to decrease the learning rate as the training progresses to make
SGD converge (Robbins & Monro, 1951), which results in good performance (You et al., 2020),
but He et al. (2019) did not consider the case in which these parameters are time-varying. We
analyze the time-varying case in SGD and propose a generalization error model for both constant
and time-varying cases. Based on these models, we also propose a novel hyperparameter search
method. In this study, we utilized a PAC-Bayes-based generalization bound to generate a model of
the generalization error. To validate the proposed model, we trained approximately 300 networks on
both practical networks (VGG16 (Simonyan & Zisserman, 2015), Wide-ResNet 28-10 (Zagoruyko
& Komodakis, 2016)) and data (CIFAR10, CIFAR100 (Krizhevsky et al., 2009)). We also showed
that the proposed model can approximate the experimental results well. We also show that the
proposed model can be used for hyperparameter search via Bayesian optimization and can yield
better results, compared with the previous method for searching in the uniform distribution, and
comparable results for searching in the logarithmic uniform distribution.
The contributions of the present study are summarized as follows:
•	We analyzed the generalization bound using PAC-Bayes with the time-
varying batch size and learning rate.
•	We directly modeled the generalization error on the batch size and learning
rate and proposed its functional form.
•	We experimentally showed that it is possible to model the generalization error
with high accuracy for practical data.
•	Our experimental results demonstrated that the hyperparameter search
method built on the proposed model can outperform current libraries when
1
Under review as a conference paper at ICLR 2022
searching over uniform distributions and has comparable performance when
searching over logarithmic uniform distributions.
2	Related Work
In this section, we present related work on SGD modeling, generalization bounds, prediction of
model performance, and hyperparameter optimization.
Theoretical analysis of SGD: SGD is often treated as a continuous-time stochastic differential
equation (SDE) by modeling the noise of the gradient and the loss function and applying the Euler-
Maruyama approximation (Lin et al., 2018; Mandt et al., 2017; Li et al., 2017; Jastrzebski et al.,
2017). This approximation can be solved analytically as an Ornstein-Uhlenbeck process (Uhlenbeck
& Ornstein, 1930). In this study, we also analyzed SGD using the same process as the previously
mentioned studies. However, in most previous studies, the batch size and learning rate in SGD
were treated as constants. Hence, in this study, we treat them as time-varying and seek an analytic
solution.
Theoretical error bounds: There is a large body of research on theoretical explanations for the
generalization capability of deep neural networks (Negrea et al., 2019; Arora et al., 2018; Cao & Gu,
2019; Deng et al., 2021). He et al. (2019), building on their analysis of SGD, provided a theoretical
explanation of the effects of the batch size and learning rate on the generalization error. However,
this research only handles the case of constant batch size and learning rate and does not model the
generalization error in the function form. In this study, we derive a generalization bound based on
an analytic solution for the time-varying case and derive a model of the generalization error.
Predicting model performance: Most studies are based on the results from training networks,
assuming hyperparameters as inputs, and searching for the best parameters without training the
networks. In neural architecture search, the performance of a model was predicted for a given
model on a given dataset (Istrate et al., 2019), small data (Klein et al., 2017), and model size (Real
et al., 2019), respectively. Several studies predict the performance of a model in a functional form
by treating both the dataset and model sizes as input (Rosenfeld et al., 2020), or the pruning scale as
input (Rosenfeld et al., 2021). In this study, we exploit the findings of the above studies to predict
the performance of a model in a functional form, based on a theoretically derived generalization
bound to the batch size and learning rate.
Hyperparameter optimization: Hyperparameter optimization algorithms are mainly catego-
rized into two types: nonadaptive and adaptive. Nonadaptive algorithms search for the best hy-
perparameters without using any past search results and, thus, have the advantage of being easily
parallelizable. Grid search and random search fall into this category, and many studies have shown
that random search is more useful (Bergstra & Bengio, 2012; Li & Talwalkar, 2020). Adaptive al-
gorithms search for the best hyperparameters by exploiting past search results and can obtain better
hyperparameters with fewer searches. Adaptive algorithms include specific algorithms, such as the
evolutionary algorithm (Young et al., 2015) and Bayesian optimization (Wu et al., 2019), and prac-
tical tools, such as Hyperopt (Bergstra et al., 2015) and Optuna (Akiba et al., 2019). These methods
are flexible enough to be optimized for arbitrary functions. However, there is still room for more
efficient search when searching only for specific hyperparameters. In this study, we aim to further
improve the search efficiency of both the batch size and learning rate by employing a hyperparameter
search based on the proposed generalization error model.
In the following sections, we will analyze the influence of the batch size and learning rate on the
generalization error in SGD, based on assumptions about the distribution of the gradients and the
shape of the risk function near minima, an approximation to SDE, and the PAC-Bayes method.
3	Preliminaries
In this section, we describe the preliminary knowledge used to model the learning process of SGD.
Generalization error of stochastic algorithms. In general, machine learning algorithms are de-
signed to select a hypothesis function Fθ from a hypothesis class {Fθ∣θ ∈ Θ ⊂ Rd} such that the
expected value of the risk function R computed from the loss function l is minimized. In a proba-
2
Under review as a conference paper at ICLR 2022
bilistic algorithm such as SGD, the parameter θ after learning is considered to lie on the distribution
Q. The risk function for parameter θ is defined as follows:
R(θ) =
E(X,Y )~D [l(Fθ(X), Y)] .	(1)
Therefore, R(Q) = Eθ~q [R(θ)]. However, one cannot compute the risk function, and in practice,
the empirical risk R(θ) from the data is approximated as the risk function R(θ).
1N
R(θ) = NN ∑i(Fθ (Xi),Yi),	⑵
where (Xi,匕)is the N training data consisting of the input data and label pairs. Thus, R(Q)
Eθ~q [R(θ)].
Stochastic gradient descent. Gradient descent is an algorithm that computes the gradient of the risk
function defined in Eq.1 and updates the parameter θ based on the gradient. The gradient g(θ(t))
is defined as g(θ(t))，Vθ(t)R(θ(t)) = Vθ(t)E(χ,γ)[l(F&(t)(X), Y)]. We update the parameter
θ by θ(t + 1) = θ(t) - ηg(θ(t)), where η is the learning rate. In practice, the true gradient of
g(θ(t)) is approximated by the gradient of gs(θ(t)) computed from the empirical risk R(θ) defined
in Eq.2, and the parameter θ is updated with gs(θ(t)). Thus, the gradient gs(θ(t)) is defined as
gs(θ(t)) = Vθ(t)R(θ(t))=由 Pi∈s Vθ(t)l(Fθ(t)(Xi),Yi), and the parameter θ is updated by
θ(t + 1) = θ(t) 一 ηgs(θ(t)). We use a mini-batch S to train the model in parallel, and |S| represents
its size. Then, we treat li (θ) = l(Fθ(t) (Xi), Yi).
A ■	*	■	♦	EI 1	Γ∙	.∙	7	/∕^k∖	1 ∙ .	<A / Zi∖ ♦	F ♦	1
Analysis of the learning process. The loss function li(θ) and its mean R(θ) is an unbiased estima-
tor of the risk function R(θ), and the gradient of the loss function Vθli(θ) and its mean gs(θ) is an
unbiased estimator of the gradient g(θ) = VθR(θ). Hence, We have E[li(θ)] = E[R(θ)] = R(θ)
and E[Vθli(θ)] = E[gs(θ)] = g(θ) = VθR(θ), where the expectations are computed from the
data distribution (X, Y). Here, as in Mandt et al. (2017), we assume that {Vli (θ)} is independently
and identically sampled from a Gaussian distribution with mean g(θ) = VθR(θ). We can express
Vθli(θ)〜N(g(θ), C), where C is the covariance matrix, which is assumed to be a constant matrix
for all θ. Then the gradient of gs(θ) computed for a mini-batch follows the following Gaussian
distribution:
gs(θ) = ɪ X Vθli(θ) ~N ", ɪc).	⑶
Because the covariance matrix is symmetric and (semi) positive definite, we suppose that C can be
factorized as C = BB> . In an actual SGD algorithm, the parameters are iteratively updated using
Eq.3 to minimize the risk function R(θ):
∆θ(t) = θ(t + 1) 一 θ(t)
-ηgs (θ(t)) = -ηg(θ) + PSi bδw,
∆W ~N(0,I).
(4)
Analytical solution of the stochastic differential equation. When the learning rate η is small,
Eq.4 can be approximated by SDE, which is known as Ornstein-Uhlenbeck process (Uhlenbeck &
Ornstein, 1930). Here, we assume that the risk function is convex and second-order differentiable
in the neighborhood of the minimum (He et al., 2019). In addition, when the risk function R(θ) is
minimized at θ = θ*, it is shifted so that it is minimized at θ = 0. Thus, the risk function R(θ) can
be approximated by second-order statistic on the distribution Q as follows:
R(θ) = 2 θ>Aθ,	(5)
where A is the Hessian matrix around the minimum of the risk function and is assumed to be positive
definite. This assumption was experimentally demonstrated by Li et al. (2018), and other studies
have made similar assumptions (JaStrzebSki et al., 2017; Poggio et al., 2017). Because E[R(θ)]=
R(θ), we further assume that the empirical risk R(θ) is in the following quadratic form:
R(θ) = 2(θ - θb)> A(θ - θb)	(6)
3
Under review as a conference paper at ICLR 2022
where θb is the bias term, A is the Hessian matrix, and E[θb] = 0, E[A] = A.
It is well known that Eq.4 has an analytic solution (Gardiner, 2004),
rs /
θ(t)
e-Atθ(0) +
e-A(t-t0)BdW(t0),
(7)
where W(t0) is white noise and follows N(0, I) and Q is the distribution of θ in Eq.7 and Gaussian.
4	Theoretical Generalization B ound
In this section, we derive the generalization bounds for both constant and time-varying cases.
4.1	Constant Case
Using Eq.7 and PAC-Bayes, we obtain a generalization bound for SGD as follows (see Appendix
B.1 for the derivation).
Theorem 1 (extension ofHe et al. (2019), Theorem 2). We treat A as the Hessian of the risk function
around the local minimum, C as the covariance matrix of the gradients calculated by single sample
points, Q as the distribution of the output hypothesis function of SGD, Σ is the covariance matrix of
the distribution Q, d is the dimension of the parameter θ (network size), and R is the search radius
of the parameter θ.
When A and Σ commute, for any positive real δ ∈ (0, 1), with probability 1 - δ over a training
sample set of size N, we have the following inequality for the distribution Q:
R(Q) ≤R(Q)
2η∣ tr(CA-1) + dlog (2ηi) - log (det(CAT)) + R - d+ 2log(ɪ) + 2logN + 4
+ ∖	4N - 2	.
(8)
This generalization bound incorporates a new scale factor for the parameter search, R2, into Theo-
rem 2 of He et al. (2019). Although similar results can be derived without assuming that A and Σ
commute, we used the above assumption for simplicity. R represents the radius of the hypersphere
in the parameter search, and kθ - θ0 k2 ≤ R2. We also show that this generalization bound also has
a network initialization factor. A detailed description is provided in Appendix B.2.
4.2 Time-Varying Case
It should be noted that, when solving Eq.4, the hyperparameters in SGD such as the batch size
|S| and the learning rate η can be time-varying. While He et al. (2019) derived a generalization
bound for the case in which these are constant, we further analyze the time-varying case. In the
time-varying case, the solution takes the form of a hyperparameter term in Eq.7 entering the integral
(Gardiner, 2004).
ft
θ(t) = e-Atθ(0) +	√n(t0)e-A(t-') BdW (t0),
0
(9)
where n(t) = -∣S∣)y. Then, We treat the time-varying batch size and learning rate as a univariate
function n(t). This allows us to treat changes in the learning rate (Robbins & Monro, 1951; You
et al., 2020) and batch size (Smith et al., 2018) in a unified manner. In general, if n(t) is allowed to be
an arbitrary function, this integral cannot be solved analytically. However, because the learning rate
generally decreases and the batch size generally increases as the learning progresses, if we assume
n(t) to be a monotonically decreasing function and apply the mean value theorem for integrals
Rtt1 n(t)F (t)dt = n(ξ) Rtt1 F (t)dt, ξ ∈ [t0, t1], then we obtain a generalization bound for the time-
varying case as follows (see Appendix B.3 for the detailed derivation).
4
Under review as a conference paper at ICLR 2022
Theorem 2. We treat D = R0 e-A(t-t ) Ce-A(t-t )dt0. Using the same settings as theorem 1 and
when A and Σ commute and A and D commute, we have the following inequality for the time-
varying case:
R(Q) ≤ R(Q)
nnertr(CA-1) + dlog (-2-) - log (det(CAT)) + R - d +2log( 1) +2logN + 4
+1------------------------"iq—4N-2-------------------------------------------------，
(10)
and
ninter = (1 - λ) min (n(t)) + λmax (n(t)),	λ ∈ [0, 1].	(11)
5	Functional Approximation of the Generalization Error
Based on the theoretical analysis thus far, we consider approximating R(Q) in a functional form.
From Eq.8 and Eq.10, We have already modeled the generalization bound (R(Q) - R(Q)). We
can also design a empirical functional form based on He et al. (2019)’s experimental results on the
effect of ratio of the batch size to learning rate on the generalization error. Thus, if We can model
the training error R(Q), we can derive the functional form of the generalization error. We model the
training error by substituting Eqs.7 and 9 into Eq.6. Subsequently, We shoW the specific functional
form of the generalized error model for constant and time-varying cases.
5.1	Constant Case
First, we organize Eq.7 with respect to the batch size |S | and learning rate η and the learned param-
eter θ(t) is as follows:
θ(t)
a0 +
(12)
where a0, a1 are vectors that do not depend on |S|, η. By substituting this into Eq.6, the functional
form of the training error R(Q) is as follows. The detailed derivation is in Appendix B.4.1.
R(Q) ` Eθ〜Q 2(θ - θb)>A(θ - θb)
where a00 = a0 - θb, and a0, a1, a2 are constants. Although we can model R(Q) directly by
substituting Eq.7 into Eq.5, this cannot be treated as a practical functional form as it does not model
the experimental increase of the generalization bound with respect to the decrease of 看(He et al.,
2019). Thus, we address this problem by modeling the training error R(Q) and the generalization
bound (R(Q) - R(Q)) separately. To model the generalization bound, we propose the following
functional form based on the model derived in Eq.8 and He et al. (2019)’s experimental results. The
detailed derivation is given in Appendix B.5.1.
R(Q)-R(Q)
' a3 + a4
(14)
Here, a3, a4 are constants. By combining Eq.13 and Eq.14, we propose the following functional
form as a model of the generalization error R(Q).
R(Q) = ≡(s,η)
η
c0 网 + c1
+ c3
(15)
5
Under review as a conference paper at ICLR 2022
(a) CIFAR10
(b) CIFAR100
(c) Estimated vs. actual error.
(d) CIFAR10
(e) CIFAR100
(f) Estimated vs. actual error.
Figure 1: Experimental results of the proposed model for both (a, b, c) constant and (d, e, f) time-varying
cases. (a, b) For the constant case, we plotted points with the ratio of the learning rate to the batch
size on the horizontal axis of a log scale and the generalization error on the vertical axis. (d, e)
For the time-varying case, we plotted points with the ratio of the initial learning rate to the batch
size and the ratio of the final learning rate to the batch size on the horizontal axis of the log scale,
and the generalization error on the vertical axis. (a, b, d, e) For all cases, we also show the results
fitted by the proposed model as green lines and surfaces, respectively. (c, f) The true generalization
error and estimation error are plotted, and the mean and standard deviation of the relative error δ are
represented by μ and σ ,respectively.
where c0 〜c3 are constants. The first term models the training error R(Q), and the second term
models the generalization bound (R(Q) - R(Q)). Thus, the first term denotes that the training error
monotonically increases with respect to the hyperparameter 曲,and the second term denotes that
the generalization bound inversely decreases.
5.2	Time-Varying Case
As discussed in Section 4.2, we approximate Eq.9 to a simple form similar to Eq.12, using an ap-
proximation of the mean value theorem for integrals. Based on the functional form obtained by sub-
stituting Eq.9 into Eq.6, the model of generalization bounds derived in Eq.10, and He et al. (2019)’s
experimental results, we propose the following functional form as the model of the generalization
error (S, η). A detailed derivation is given in Appendix B.4.2 and B.5.2.
e(S, η) =(C0n1 + C1 nι) + 卜2ʌ/+。3
n1 = (1 - λ1) min (Pn(t)) + λ1 max (Pn(t)
n2 = (1 - λ2) min (n(t)) + λ2 max (n(t))
(16)
(17)
(18)
where c0 〜c3, λ1, λ2 are constants and λ1, λ2 ∈ [0,1]. Note that, for n1 in the first term, the
intermediate value is computed after taking the square root of n(t), and for n2 in the second term,
the linear interpolation is computed.
5.3	Practical Functional Form
In the discussion above, we constructed a rough model of the generalization error. However, the
models in Eqs.15 and 16 have no upper bounds, and they don’t model the fact that the loss function
6
Under review as a conference paper at ICLR 2022
(a) Constant case
(b) Time-varying case
(c) Constant case
(d) Time-varying case
Figure 2: Experimental results on the stability of the generalization error model for (a, b) CIFAR10 and (c, d)
CIFAR100, for the (a, c) constant case and (b, d) time-varying case. The horizontal axis shows the
number of networks randomly selected to estimate the generalization error, whereas the vertical axis
shows the mean value μ and standard deviation σ of the relative error δ. The shaded areas represent
one standard deviation from the mean in each direction.
hits its head at the random guess level. Therefore, it is necessary to model the transition from the
initial random guess level to the proposed model e(S, η). Thus, We propose the model ^(S, η) using
a soft-min function based on the LogSumExp function.
^(S, η) = Softmin伍(S, η), €0; c4) = -1 (- log k-c4e(S,η) + e-c4e0))	(19)
where c4 is a constant that controls the shape (sharpness) of the soft-min function (softmin(x, y) =
- log (exp (-x) + exp (-y))). €0 is the value of the loss function at the random guess level, which
is statistically determined and not explored. (e.g., for a balanced dataset in image classification, we
can treat it as €0 = 1- (1/Nclasses).)
6	Experiment
To validate the proposed model ^(S, η), we trained a practical network (VGG16 (Simonyan & ZiS-
serman, 2015), Wide-ResNet 28-10 (Zagoruyko & Komodakis, 2016)) on the image classification
tasks (CIFAR10, CIFAR100 (Krizhevsky et al., 2009)).
6.1	Experimental Settings and Result
In the constant case, the batch size and learning rate were fixed throughout the training pro-
cess. To evaluate the generalization error, we trained 60 VGG models with a batch size |S | ∈
{16, 32, 64, 128, 256, 512} and a learning rate η ∈ {1/2i|i = 3,..., 12} for CIFAR10, as well as
66 Wide-ResNet models with the same batch size |S| and learning rate η ∈ {1/2i|i = 2, ..., 12}
for CIFAR100 over a total of 200 epochs. The experimental results for the constant case are shown
in Figures.1a and 1b. We plotted the log scale of 血 on the horizontal axis and the generalization
error on the vertical axis. We found both a decreasing and an increasing property of the generaliza-
tion error before and after 血 ` 10-4, which we discussed in Eq.15. Although He et al. (2019)
claimed that increasing 向 decreases the generalization
error, the experimental results show that the
generalization error increases when 岛 exceeds a certain value. This implies that, in addition to the
generalization bound, the training error TR(Q) must be modeled.
In the time-varying case, only the learning rate is set to be time-varying, and we use an exponential
step function. We chose ηinit and ηf inal and trained over 200 epochs, starting with a learning rate
of ηinit, decaying with a decay rate of γ at 60, 120, and 160 epochs, and finally reached a learning
rate ηfinal = γ3ηinit at 160 epochs.
In addition to the constant case, we trained a case in which the batch size |S |	∈
{16, 32, 64, 128, 256}, the learning rate of ηinit ∈ {1/2i|i = 3, ..., 4} and ηf inal ∈ {0.1/2i|i =
0, ..., 4} for CIFAR10, the same batch size |S|, the learning rate of ηinit ∈ {1/2i|i = 2, ..., 4} and
the same ηf inal for CIFAR100, as well as a case in which the batch size |S| ∈ {128, 256}, the
learning rate of ηinit ∈ {1/2i|i = 5, ..., 9} and ηf inal ∈ {0.1/2i|i = 5, ..., 9} for both datasets. In
7
Under review as a conference paper at ICLR 2022
sample size
(b) CIFAR10, log
(c) CIFAR100, uniform
(d) CIFAR100, log
(a) CIFAR10, uniform
(e) CIFAR10, uniform
(f) CIFAR10, log
(g) CIFAR100, uniform
(h) CIFAR100, log
Figure 3: Hyperparameter optimization results of several methods. The number of trained models is plotted
on the horizontal axis, and the maximum accuracy achieved among the trained models is plotted on
the vertical axis for the (a, b, c, d) constant and (e, f, g, h) time-varying cases. The 95% confi-
dence intervals are shaded. The proposed method always searches for a uniform range, whereas the
comparison method searches the range written in the caption (uniform or log-uniform).
total, we measured the generalization error for the 153 models for CIFAR10 and 184 models for CI-
FAR100. More implementation details can be found in Appendix C.1. The experimental results for
the time-varying case are shown in Figures.1d and 1e. ηiSnit and ηfS∣al are plotted on the horizontal
axis, whereas the generalization error (S, η) is plotted on the vertical axis. We also plot the results
of projecting the generalization error onto ηinit and ηfSn∣al, respectively. As in the constant case, We
can observe that, when the value of n(t) = ∣ 黑 ∣ is not in the appropriate range (either quite large
or quite small), the generalization error becomes large.
6.2	Error Landscape Estimation
We estimate the obtained generalization error e(S, η) using a parametric family, ^(S, η; φ). Here,
we treat the parameter φ as φ = {c0, c1, c2, c3, c4} for the constant case, and then {λ1, λ2} is added
to the time-varying case. As in Rosenfeld et al. (2021), we search for φ such that the squared error
of δ(S, η; φ) =
^(s,ηιφ)-e(s,η)
e(s,η)
is minimized. Implementation details are provided in Appendix C.2.
Figure.1c plots our results for the constant case. The mean μ and standard deviation σ of the relative
error δ are ∣μ∣ < 2% and σ < 12%, respectively. Considering the small number of parameters used
to estimate the generalization error (∣φ∣ = 5), the proposed functional model can accurately estimate
the complex generalization error landscape (60 and 66 models, respectively). In Figure.1f, we plot
the results for the time-varying case. We achieved ∣μ∣ < 3% and σ < 17%, respectively. Although
the relative error is larger than the constant case, we can still declare that the proposed functional
model can estimate the generalization error sufficiently, considering that the generalization error of
153 or 184 models is modeled with a small number of parameters (∣φ∣ = 7). In Appendix C.2.3, we
provide a comparison between the proposed model, a model that analytically solves the time-varying
case of the step function, and a model that uses only the final learning rate.
In addition, we verified the stability of the proposed model with respect to the number of networks.
We randomly sampled a certain number of generalization errors obtained by training the networks,
estimated the parameter φ from them only, and computed the relative error δ . We performed this
procedure 100 times and computed μ and σ of the relative error. Details of the experimental setup
are presented in Appendix C.3.1.
In Figure.2 we show the experimental results. Figures.2a and 2c show that, for the constant case
of CIFAR10 and CIFAR100, we can accurately estimate the generalization error landscape by ran-
domly training about 30 models. In contrast, Figures.2b and 2d show that, for the time-varying case,
we need to train about 75 models for the estimation.
8
Under review as a conference paper at ICLR 2022
7	Hyperparameter Optimization
In this section, we show that the proposed model is useful for applications such as hyperparameter
optimization. In this study, we used Bayesian optimization.
Bayesian optimization can compute a function y = f (x) that can be fitted to any sequence of data
points {(xi, yi)}. Nogueira (2014) achieved this by using a Gaussian process with a Matern kernel,
and many Bayesian optimization methods use generic kernel functions which can be fitted to any
function. While this is practical for fitting unknown functions, ideally, the most accurate Bayesian
optimization can be achieved by employing a kernel function constructed from a feature extraction
function ψ(x) that can compute the unknown function with a linear combination of y = w>ψ(x).
Therefore, for the constant case, We propose a kernel function k(xi, Xj) for the input X =看 based
on Eq.15 for hyperparameter optimization.
k(xi, Xj) = ψ(xi)>ψ(xj) = √XiXj + XiXj +------+ 1.	(20)
√XiXj
Details are in Appendix C.4.1. In contrast, the time-varying case cannot be represented by a linear
combination ofy = w>ψ(X), because the product term of the parameters (c and λ) appears. There-
fore, We employ a hyperparameter search by sequential model-based global optimization (SMBO)
(Bergstra et al., 2011) With the proposed model in Eq.19 using Eq.16. For SMBO, We use the
Levenberg-Marquardt algorithm (Mor6, 1978) and Thompson Sampling (Thompson, 1933; RUSSo
et al., 2017). More details are provided in Appendix C.4.2.
To validate the proposed method, we explored 面 using the fitted generalization error model
^S, η; φ) as the training result of the network. The details are given in Appendix C.4.3. Figure.3
shows the experimental results. The horizontal axis plots the number of training networks, and the
vertical axis plots the maximum test accuracy among the networks trained to date. A total of 100
experiments were conducted, and the 95% confidence interval was shaded. The methods used for
comparison were Bayesian optimization using the Matern kernel and RBF kernel (Nogueira, 2014),
Hyperopt (Bergstra et al., 2015), and Optuna (Akiba et al., 2019).
As shown in the Figure.3, in almost all cases, the proposed method achieves the highest accuracy
with the least number of models in the search method with uniform distribution. This indicates that
the proposed model represents a good feature extractor, and the proposed kernel function is useful
for hyperparameter optimization. In addition, in the constant case, the proposed method achieved
the highest accuracy with the same number of models as the comparison method, which searches
for logarithmic uniform distributions. It indicates that theoretical model-based hyperparameter opti-
mization can be applied practically. In practice, it is generally believed that it is better to search for
hyperparameters S, η in the logarithmic uniform range (Rasmussen, 1997; Sundararajan & Keerthi,
2001). However, there is no theoretical foundation for this (this is confirmed by the fact that in Hy-
peropt, the results are not improved by exploring the logarithmic range). These results suggest that
the proposed model-based hyperparameter optimization is both theoretically sound and practically
useful when performing a hyperparameter search on the batch size and learning rate.
8 Discussion and Conclusion
In this paper, we presented a theoretical analysis and a model of the generalization error for the
batch size and learning rate in SGD. We analyzed the case in which these parameters are constant
during training and the more challenging and practical case when they vary with time. Through
experiments for both cases, we show that the proposed generalization error model is practical, and
we test the number of networks required to estimate the generalization error landscape (= stability
of the generalization error model). In addition, we demonstrate that a hyperparameter search based
on the proposed model outperform the conventional Bayesian optimization and hyperparameter op-
timization libraries, such as Hyperopt and Optuna. To the best of our knowledge, this is the first
study that explores the batch size and learning rate based on the generalization error model. From
the discussion, we conclude that this theoretical model is a practical and useful model for under-
standing the effects of the batch size and learning rate on the generalization error. We hope that
this work will provide further theoretical insights into deep learning and new research directions for
hyperparameter search based on theoretical models.
9
Under review as a conference paper at ICLR 2022
References
Hirotogu Akaike. Information theory and an extension of the maximum likelihood principle. In
Selected papers of hirotugu akaike, pp. 199-213. Springer, 1998.
Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna:
A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM
SIGKDD international conference on knowledge discovery & data mining, pp. 2623-2631, 2019.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252. PMLR, 2019.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. In International Conference on Machine Learning, pp.
254-263. PMLR, 2018.
James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of
machine learning research, 13(2), 2012.
James Bergstra, Remi Bardenet, Yoshua Bengio, and Balgzs KegL Algorithms for hyper-parameter
optimization. Advances in neural information processing systems, 24, 2011.
James Bergstra, Brent Komer, Chris Eliasmith, Dan Yamins, and David D Cox. Hyperopt: a python
library for model selection and hyperparameter optimization. Computational Science Discovery,
8(1):014008, 2015. URL http://stacks.iop.org/1749-4699/8/i=1/a=014008.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. SGD learns over-
parameterized networks that provably generalize on linearly separable data. In International
Conference on Learning Representations, 2018. URL https://openreview.net/forum?
id=rJ33wwxRb.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and
deep neural networks. Advances in Neural Information Processing Systems, 32:10836-10846,
2019.
Zhun Deng, Hangfeng He, and Weijie Su. Toward better generalization bounds with locally elastic
stability. In International Conference on Machine Learning, pp. 2590-2600. PMLR, 2021.
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2019. URL https://openreview.net/forum?id=S1eK3i09YQ.
C. W. Gardiner. Handbook of stochastic methods for physics, chemistry and the natural sciences,
volume 13 of Springer Series in Synergetics. Springer-Verlag, Berlin, third edition, 2004. ISBN
3-540-20882-8.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256. JMLR Workshop and Conference Proceedings, 2010.
Fengxiang He, Tongliang Liu, and Dacheng Tao. Control batch size and learning rate to generalize
well: Theoretical and empirical evidence. Advances in Neural Information Processing Systems,
32:1143-1152, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Roxana Istrate, Florian Scheidegger, Giovanni Mariani, Dimitrios Nikolopoulos, Constantine Bekas,
and Adelmo Cristiano Innocenza Malossi. Tapas: Train-less accuracy predictor for architecture
search. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 3927-
3934, 2019.
10
Under review as a conference paper at ICLR 2022
StanisIaW Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua
Bengio, and Amos Storkey. Three factors influencing minima in sgd. arXiv preprint
arXiv:1711.04623, 2017.
Bumsoo Kim. Wide-resnet.pytorch, 2018. URL https://github.com/meliketoy/
wide-resnet.pytorch.
Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, and Frank Hutter. Fast bayesian op-
timization of machine learning hyperparameters on large datasets. In Artificial Intelligence and
Statistics,pp. 528-536. PMLR, 2017.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 and cifar-100 (canadian institute for
advanced research). 2009. URL http://www.cs.toronto.edu/~kriz/cifar.html.
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss land-
scape of neural nets. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran As-
sociates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
a41b3bb3e6b050b6c9067c67f663b915- Paper.pdf.
Liam Li and Ameet TalWalkar. Random search and reproducibility for neural architecture search. In
Uncertainty in artificial intelligence, pp. 367-377. PMLR, 2020.
Qianxiao Li, Cheng Tai, and Weinan E. Stochastic modified equations and adaptive stochastic gradi-
ent algorithms. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp.
2101-2110. PMLR, 06-11 Aug 2017. URL http://proceedings.mlr.press/v70/
li17f.html.
Tao Lin, Sebastian U Stich, Kumar Kshitij Patel, and Martin Jaggi. Don’t use large mini-batches,
use local sgd. arXiv preprint arXiv:1808.07217, 2018.
Kangqiao Liu, Liu Ziyin, and Masahito Ueda. Noise and fluctuation of finite learning rate stochastic
gradient descent. In International Conference on Machine Learning, pp. 7045-7056. PMLR,
2021.
Stephan Mandt, MattheW D Hoffman, and David M Blei. Stochastic gradient descent as approximate
bayesian inference. arXiv preprint arXiv:1704.04289, 2017.
David A McAllester. Pac-bayesian model averaging. In Proceedings of the twelfth annual confer-
ence on Computational learning theory, pp. 164-170, 1999a.
David A McAllester. Some pac-bayesian theorems. Machine Learning, 37(3):355-363, 1999b.
Jorge J Morg The Ievenberg-marquardt algorithm: implementation and theory. In Numerical anal-
ysis, pp. 105-116. Springer, 1978.
Jeffrey Negrea, Mahdi Haghifam, Gintare Karolina Dziugaite, Ashish Khisti, and Daniel M Roy.
Information-theoretic generalization bounds for sgld via data-dependent estimates. arXiv preprint
arXiv:1911.02151, 2019.
Fernando Nogueira. Bayesian Optimization: Open source constrained global optimization tool for
Python, 2014. URL https://github.com/fmfn/BayesianOptimization.
Tomaso Poggio, Kenji KaWaguchi, Qianli Liao, Brando Miranda, Lorenzo Rosasco, Xavier Boix,
Jack Hidary, and Hrushikesh Mhaskar. Theory of deep learning iii: explaining the non-overfitting
puzzle. arXiv preprint arXiv:1801.00173, 2017.
Carl EdWard Rasmussen. Evaluation of Gaussian processes and other methods for non-linear re-
gression. PhD thesis, University of Toronto Toronto, Canada, 1997.
Esteban Real, Alok AggarWal, Yanping Huang, and Quoc V Le. Regularized evolution for image
classifier architecture search. In Proceedings of the aaai conference on artificial intelligence,
volume 33, pp. 4780-4789, 2019.
11
Under review as a conference paper at ICLR 2022
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathemati-
Cal statistics, pp. 400-407, 1951.
Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction
of the generalization error across scales. In International Conference on Learning Representa-
tions, 2020. URL https://openreview.net/forum?id=ryenvpEKDr.
Jonathan S Rosenfeld, Jonathan Frankle, Michael Carbin, and Nir Shavit. On the predictability of
pruning across scales. In International Conference on Machine Learning, pp. 9075-9083. PMLR,
2021.
Daniel Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen. A tutorial on
thompson sampling. arXiv preprint arXiv:1707.02038, 2017.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning
Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceed-
ings, 2015. URL http://arxiv.org/abs/1409.1556.
Samuel L. Smith, Pieter-Jan Kindermans, and Quoc V. Le. Don’t decay the learning rate, increase
the batch size. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=B1Yy1BxCZ.
Sellamanickam Sundararajan and S. Sathiya Keerthi. Predictive approaches for choosing hyperpa-
rameters in gaussian processes. Neural computation, 13(5):1103-1118, 2001.
William R Thompson. On the likelihood that one unknown probability exceeds another in view of
the evidence of two samples. Biometrika, 25(3/4):285-294, 1933.
George E Uhlenbeck and Leonard S Ornstein. On the theory of the brownian motion. Physical
review, 36(5):823, 1930.
Jia Wu, Xiu-Yun Chen, Hao Zhang, Li-Dong Xiong, Hang Lei, and Si-Hao Deng. Hyperparameter
optimization for machine learning models based on bayesian optimization. Journal of Electronic
Science and Technology, 17(1):26-40, 2019.
Kaichao You, Mingsheng Long, Jianmin Wang, and Michael I. Jordan. How does learning rate
decay help modern neural networks?, 2020. URL https://openreview.net/forum?
id=r1eOnh4YPB.
Steven R Young, Derek C Rose, Thomas P Karnowski, Seung-Hwan Lim, and Robert M Patton.
Optimizing deep learning hyper-parameters through an evolutionary algorithm. In Proceedings of
the workshop on machine learning in high-performance computing environments, pp. 1-5, 2015.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint
arXiv:1605.07146, 2016.
12
Under review as a conference paper at ICLR 2022
A Background
A. 1 Pac-Bayesian Framework
The PAC-Bayesian framework originates from McAllester (1999a;b)’s work. In the PAC-Bayesian
perspective, the hypothesis functions learned by probabilistic machine learning algorithms are drawn
at random from the hypothesis classes based on some “rules.” The generalization ability of an algo-
rithm is negatively correlated with the distance (often measured by the KUllback-Leibler (KL) diver-
gence) between the output hypothesis distribution and the a priori distribution (typically a Gaussian
or uniform distribution). The above results suggest that there is a tradeoff between minimizing the
empirical risk and exploring further regions of the hypothesis space from the beginning (a priori).
Let P be the initial priori distribution in the parameter space Θ, and let Q be the distribution in Θ
after learning. We can now define the expected value of the risk function under distribution Q as
follows:
R(Q) = Eθ 〜q[R(Θ)].
Similarly, the empirical risk function under distribution Q can be defined as follows:
R(Q) = Eθ 〜q[R(Θ)].
Based on these, we obtain the classical result in that the expected value of the risk function R(Q) is
uniformly bounded by the expected value of the empirical risk function R(Q) and KL-divergence
D(QkP).
Lemma 1 (See McAllester (1999a), Theorem1). For any positive real δ ∈ (0, 1) with a probability
of at least 1 - δ over a sample of size N, we have the following inequality for all distributions Q:
R(Q) ≤R(Q) + JQwP) + 2N-)1+logN+2,
(21)
where D(QkP ) is the KL divergence between the distributions Q and P and is defined as
D(QkP) = Eθ 〜Q
Q(θ八
P(θ)广
(22)
B S upplemental Proofs for the Proposed Method
In this section, we describe the details of the proof, which we omitted in the proposed method.
B.1 Generalization Bound for the Constant Case
To prove Theorem 1, we use an existing proof method that approximates the SGD update of the
model to a stochastic differential equation (see, e.g., (Mandt et al., 2017)). With appropriate as-
sumptions, the learning process of SGD can be approximated by the Ornstein-Uhlenbeck process
(Uhlenbeck & Ornstein, 1930). Because the Ornstein-Uhlenbeck process has an analytic solution,
we can use the distribution Q of the analytic solution θ to derive the KL-divergence, which is used
to derive a generalization error bound from Eq.21. Here, we modify and extend He et al. (2019)’s
Theorems 1 and 2 to derive the generalization error bound for the constant case.
Lemma 2 (cf. Gardiner (2004), pp.109-110). Under second-order differentiable assumption (Eq.5),
if t is sufficiently large, the Ornstein-UhIenbeckprocess (Eq.4)'s distribution
q(θ) = M exp--2 θ>Σ-1θ∣,	(23)
has the following property:
A∑ + ∑A = ɪe.
|S|
(24)
13
Under review as a conference paper at ICLR 2022
This lemma is similar to that of Gardiner (2004).
ProofofLemma 2. From the analytical solution of the Ornstein-Uhlenbeck process (Uhlenbeck &
Ornstein, 1930), the parameter θ can be expressed as follows:
θ(t) = e-Atθ(0) +
t
e-A(t-t0)BdW(t0),
(25)
where W(t0) is a white noise and follows N(0, I). From Eq.23, we obtain
Σ = Eθ〜Q [θθ>].
Thus, we obtain the following equation:
AΣ+ΣA = Ae-Athθ(0), θ>(0)ie-At + e-Athθ(0), θ>(0)ie-AtA
t
Ae-A(t-t0)Ce-A(t-t0)dt0
Ae-Athθ(0), θ>(0)ie-At + e-Athθ(0), θ>(0)ie-AtA
t
—e-A(t-t )Ce-A(t-t )dt0
dt0
= Ae-Athθ(0), θ>(0)ie-At + e-Athθ(0), θ>(0)ie-AtA
+ ɪe - ∙e-AtCe-At
+ |S| C	|S| e
〜ɪe
'∣s∣ ,
(26)
(27)
where A is the Hessian matrix and symmetric. The last transformation is guided by the assumption
that A is a positive definite matrix and all its eigenvalues are positive and by the approximation of
the term multiplied by e-At twice with zero, because t is sufficiently large.
The proof is completed.	□
Lemma 3 (extension of He et al. (2019), Appendix B.1). For any positive real δ ∈ (0, 1), with
probability 1 - δ over a training sample set of size N, we have the following inequality for the
distribution Q of the output hypothesis function of SGD:
R(Q) ≤ R(Q)+J
商tr(CA-1) - 2log(det(Σ)) + 2R2 - 2d + 4log (ɪ) +4log N + 8
8N - 4
, (28)
where A is the Hessian of the risk function around the local minimum, C is the covariance matrix of
the gradients calculated by single sample points, Σ is the covariance matrix of the distribution Q, d
is the dimension of the parameter θ(network size), and R is the search radius of the parameter θ.
Proof of Lemma 3. In the PAC-Bayesian framework ((Lemma 1), it is crucial to compute the KL-
divergence between the parameter distribution of the output hypothesis function and the prior pa-
rameter distribution in the hypothesis space. The prior distribution can be interpreted as an initial
parameter distribution, and in most cases, either a uniform distribution or a Gaussian distribution
is used. In this study, We assume that the prior distribution is Gaussian N(-θ*,I). θ* is a param-
eter for which the risk function R(θ) is minimized. The mean value —θ* of the prior distribution
corresponds to the shift in θ in Eq.5, Which actually eliminates prior knoWledge of the posterior
distribution. He et al. (2019) uses N(0, I) for the prior distribution. However, it is unnatural be-
cause the prior distribution has prior knowledge of the posterior distribution. Conversely, our setting
14
Under review as a conference paper at ICLR 2022
becomes more natural. The posterior distribution Q and prior distribution P are represented by p(θ)
and q(θ) with respect to the parameter θ, given by the following equation:
P⑻=p(2π)ddet(I) exp (- 1(θ + θ*)>I(θ + 〜
(29)
q(θ)
1
-I	— exp
√(2π)d det(Σ)
-1 θ>∑-1θ).
(30)
Here, d represents the dimension of the parameter θ, and Eq.30 computed the normalization term
M in Eq.23. Thus, we obtain
log fʌ / (2π)d det(I)~ exp (- 1(θ + θ*)>I (θ + θ*) - 1 θ>Σ-1θ
g〈V (2π)ddet(Σ) p V 2( +	)	( +	)	2
+ 1 ((θ + θ*)>I(θ + θ*) - θ>Σ-1θ).
(31)
Substituting Eq.31 into Eq.22, we can compute the KL divergence between Q and P as follows.
Here, we assume Θ = Rd .
D(QkP)
/
θ∈Θ
//log
Eθ〜Q
q(θ)dθ
+ 1 ((θ + θ*)>I(θ + θ*) - θ>Σ-1θ) q(θ)dθ
2 log
-2 Z
2 θ∈Θ
2 log
)+1/ θ>Iθq(θ)dθ +1/ _ θ*>Iθ*q(θ)dθ + / _ θ*>Iθq(θ)dθ
θ>Σ-1θq(θ)dθ
+ 2 Eθ 〜N (0,∑) [Θ>IΘ] + 1 Eθ〜N(0,∑) [θ*>Iθ*] + Eθ〜N(0,∑) [Θ*>IΘ]
-2Eθ〜N(0,Σ) [θ>ς-1θ]
= 2log
≤ 2 log
+ 1tr(∑) + 1 kθ*k2- Itr(I)
+ 2tr(*)+ 2R2 - 2tr(I).
(32)
The last transformation is obtained because ∣∣θ*k2 ≤ R2 always holds when the search range of the
parameter θ is in a hypersphere of radius R. From Eq.24, we obtain
aς + ςa = |S| C.
Therefore, we obtain
AΣA-1 + Σ=/ CAT
|S|
By computing the trace on both sides, the equation becomes
tr (AΣA-1 + ∑)
tr (WCAT
(33)
(34)
15
Under review as a conference paper at ICLR 2022
The left-hand side (LHS) can be transformed as follows:
LHS = tr (AΣA-1 + ∑)
=tr (AΣA-1) +tr(Σ)
=tr (∑A-1 A) +tr(Σ)
= tr (Σ) + tr (Σ)
=2tr(Σ).	(35)
Thus, we can derive the following:
tM^ = xtr (TnTCAT) = O τη∣tr (CA-1) .	(36)
2	|S|	2 |S|
We can also compute the following:
tr(I) = d,	(37)
because d is the dimension of parameters θ and I ∈ Rd×d. Therefore, by substituting Eq.36 and
Eq.37 into Eq.32, we obtain
D(QlIP) ≤ Z τ⅛- tr(CAT)- - IogmetN)) + -R2 - dd.	(38)
4 |S|	2	2	2
Eq.38 is	an	upper bound on the distance between the	distribution Q	learned by	SGD	and	the a
priori distribution P. Therefore, by substituting Eq.38	into Eq.21, we	can obtain	a	PAC-Bayesian
generalization bound for SGD.
The proof is completed.	口
Lemma 4 (extension ofHe et al. (2019), Appendix B.2). Assuming that A and Σ commute, the KL
divergence between the distribution Q of SGD and the prior distribution P satisfies the following
inequality:
D(QIlP) ≤	tXi tr(CAT)	+ -dlog	(ɪ^) - X log	(det(CAT))	+ -R2	- -d.	(39)
4|S |	2	η 2	2	2
Lemma 4 provides the distance between the distribution Q obtained by SGD and the prior distribu-
tion of the parameters. The assumption that A and Σ commute implies that matrices A and Σ are
simultaneously diagonalizable, and similar assumptions have been used in the work of Jastrzebski
et al. (2017) and Liu et al. (2021). Although similar results can be derived without assuming that A
and Σ commute, we used the above assumption for simplicity. Based on this assumption, we can
compute a generalization bound for a constant case.
Proof of Lemma 4. Because A and Σ commute by assumption, the following equation can be ob-
tained by transforming Eq.24:
A∑ + ∑A = ɪe
|S|
Thus,
det(Σ) = det
2ΣA
(40)
ɪe
|S I
n CAT
丽CA
2⅛ CA-I
d
det(CA-1),
(41)
Σ
16
Under review as a conference paper at ICLR 2022
and we can compute
log (det(Σ)) = log
(品)det(CAT)
-d log
+ log (det(CA-1)).
Therefore, Eq.39 can be obtained by substituting Eq.42 into Eq.38.
The proof is completed.
Thus, we can easily prove Theorem 1.
Proof of Theorem 1. It is obtained by substituting Eq.39 into Eq.21.
The proof is completed.
(42)
□
□
B.2	Additional Two Factors in the Proposed Generalization Bound
In addition to the three factors discussed in He et al. (2019)’s Theorem 2, the local geometry around
minima, gradient fluctuations, and hyperparameters, the generalization bound derived in this study
incorporates a factor for the range of parameter search and a factor for network initialization.
Parameter search space The norm of θ after learning appears as the third term R2 in Eq.32.
Hence, the parameter search range and weight decay are naturally incorporated into the generaliza-
tion bound.
Network initialization. Moreover, this generalization bound naturally incorporates the network
initialization factor. Assuming that the prior distribution is N(-θ*,λ∕),0 < λ, We obtain the
following:
p'(θ)= p(2πλ1ddet(I) exp (-2λ(θ + θ*)>"θ + θ*))，0 <λ∙	(43)
Eq.32 is as folloWs.
D(QkP) ≤ 2 log (d-λd-) + 21-tr(∑) + 2-R2 - 2tr(I).	(44)
2 det(Σ) 2λ	2λ 2
By differentiating the right-hand side (RHS) by λ, We can observe that RHS is minimized by RHS =
d log (tM'd+R ) - 1 log (det (Σ)), when λ = "("+R . Assuming that d is sufficiently large and
tr(∑) + R2 < d is valid (this property is also called overparameterization (Allen-Zhu et al., 2019;
Brutzkus et al., 2018; Du et al., 2019)), we can theoretically guarantee that the generalization bound
will be tighter when we change the covariance matrix I of the prior distribution to a diagonal matrix
with even smaller eigenvalues, such as He’s initialization (He et al., 2015) or Xavier’s initialization
(Glorot & Bengio, 2010). Therefore, the generalization bound proposed in this study, which is an
extended version of He et al. (2019), also naturally incorporates factors related to the initialization
of the network.
B.3	Generalization Bound for the Time-Varying Case
To prove Theorem 2, we only need to describe how the time-varying batch size and learning rate
change Eq.24 in Lemma 2. To do so, we solve following Lemma 5:
Lemma 5 (extension of Gardiner (2004), pp.115-116). Under second-order differentiable assump-
tion (Eq.5), if t is sufficiently large and hyperparameter 看 is time-varying, the Ornstein-Uhlenbeck
process (Eq.4)’s distribution
q(θ) = M exp--1 θ>∑-1θ}	(45)
17
Under review as a conference paper at ICLR 2022
has the following property:
AΣ+ΣA=n(ξ)C,	ξ ∈ [0, t].	(46)
Here,
n(t)
η⑴
∣s(t)∣.
(47)
ProofofLemma 5. From the analytical solution of the Ornstein-Uhlenbeck process (Uhlenbeck &
Ornstein, 1930) in the time-varying case (Gardiner, 2004), the parameter θ can be expressed as
follows:
θ(t) = e-Atθ(0) + p √n(t0)e-A(t-t0)BdW(t0),
0
where W(t0) is a white noise and follows N(0, I). From Eq.45, we obtain
∑ = Eθ〜Q [θθ>].
(48)
(49)
Thus, we obtain the following equation:
AΣ + ΣA = Ae-Athθ(0), θ>(0)ie-At + e-Athθ(0), θ>(0)ie-AtA
+ Z n(t0)Ae-A(t-t0)Ce-A(t-t0)dt0
0
+ Z n(t0)e-A(t-t0)Ce-A(t-t0)Adt0
0
(50)
where A is the Hessian matrix and symmetric. We use the mean-value theorem of integrals to obtain
the following equation: For some ξ1, ξ2 ∈ [0, t],
Z n(t0)Ae-A(t-t0)Ce-A(t-t0)dt0 = n(ξ1) Z Ae-A(t-t0)Ce-A(t-t0)dt0	(51)
Z n(t0)e-A(t-t0)Ce-A(t-t0)Adt0 = n(ξ2) Z e-A(t-t0)Ce-A(t-t0)Adt0	(52)
Here, we assume that the matrix D is
D = Z e-A(t-t0)Ce-A(t-t0)dt0
0
and that A and D commute. Then, because AD = DA,
ξ = ξ1 = ξ2
(53)
(54)
holds for Eq.51 and Eq.52. Thus, by substituting Eq.54 into Eqs.51 and 52, and then substituting
the result into Eq.50, we derive
AΣ + ΣA = Ae-Athθ(0), θ>(0)ie-At + e-Athθ(0), θ>(0)ie-AtA
+ n(ξ) Z Ae-A(t-t0)Ce-A(t-t0)dt0
0
+ n(ξ) Z e-A(t-t0)Ce-A(t-t0)Adt0
0
= Ae-Athθ(0), θ>(0)ie-At + e-Athθ(0), θ>(0)ie-AtA
+ n(ξ) Z -d7e-A(t-t0)Ce-A(t-t0)dt0
0 dt0
= Ae-Athθ(0), θ>(0)ie-At + e-Athθ(0), θ>(0)ie-AtA
+ n(ξ)C - n(ξ)e-AtCe-At
' n(ξ)C.
(55)
18
Under review as a conference paper at ICLR 2022
The last transformation is guided by the assumption that A is a positive definite matrix and all its
eigenvalues are positive and by the approximation of the term multiplied by e-At twice with zero,
because t is sufficiently large.
The proof is completed.	□
Proof of Theorem 2. Using Eq.55, rather than Eq.27 in the proof of Theorem 1, leads to Eq.10.
Furthermore, as exactly
min(n(t)) ≤ n(ξ) ≤ max(n(t)),	(56)
by applying a certain λ ∈ [0, 1], we can express
n(ξ) = (1 - λ) min(n(t)) + λ max(n(t)).
Therefore, Eq.11 was derived simultaneously.
The proof is completed.
□
B.4	Functional Model of the Training Error
B.4	. 1 Constant Case
We derive a training error model for the constant case. We approximate the training error TR(θ)
by substituting Eq.7 into Eq.6 and then approximate R(Q) by computing the expected value in the
parameter distribution Q. In the constant case, we can compute
R^(θ) ' 1(θ - θb)>A(θ - θb) = 1 (e-Atθ(0) - θb + 总(e-A(IO)BdW(t0))
A (e-Atθ(0) - θb + rɪ /t e-A(IO)BdW(t0))
= 1(θ(0)>e-At — θ>),4(e-Atθ(0) - θb)
BdW(t0)) A (∕t e-A(t-tO)BdW(t0)) ɪ
-A(IO)BdW(t0)) A(e-Atθ(0) - θb)) rɪ. (57)
Therefore, by taking the expected value with the distribution Q, we obtain
TR(Q) = Eθ〜Q [τR(θ)]
1
'Eθ〜Q 胪-θb)>A(θ - θb)
E 1(θ(0)>e-At - θ>)A(e-^tθ(0) - θb)
>a U0 e-AI)BdW ⑹)# 看
"也⑦-向)# r⅛
This allows us to model R(Q), as shown in Eq.13.
(58)
19
Under review as a conference paper at ICLR 2022
B.4.2 Time-Varying Case
To compute the training error model for the time-varying case, we only need to consider how the
time-varying batch size and learning rate change Eq.6. Thus, we use the mean value theorem of
integrals in Eq.9 to obtain the following equation:
θ(t)
θ(0)e-At + p √n(t0)e-A(t-t0)BdW(t0)
0
θ(0)e-At +
e-A(t-t0)BdW(t0).
(59)
Here, ξ ∈ [0, t] and as exactly
min(√n(t)) ≤ √n(t) ≤ max(√n(t)),	(60)
for some λ ∈ [0, 1], we obtain
√n(ξ) = (1 一 λ) min(√n(t)) + λmax(√n(t)).	(61)
Thus, we can compute the first term in Eq.16, which is the training error model for the time-varying
case, by substituting Eq.61 into Eq.59 and then performing the same transformation as in Appendix
B.4.1.
B.5 Functional Model of the Generalization Bound
B.5. 1 Constant Case
In this section, under appropriate assumptions, we show that Eq.8 monotonically decreases with
respect to 看 as a basis for deriving Eq.14 based on Eq.8. We recall He et al.(2019)'s proof to make
our paper complete. We first make the following assumptions about dimension d of the parameter
space Θ.
Assumption 1 (See He et al. (2019), Assumption 2). The network size is large enough:
d、tr(cA-1)η
2|S|
(62)
where d is the number of parameters, C is the magnitude of the individual gradient noise, A is the
Hessian matrix around the global minima, η is the learning rate, and |S | is the batch size.
This assumption can be justified by the fact that network sizes of neural networks are often extremely
large. This property is also called overparameterization (Allen-Zhu et al., 2019; Brutzkus et al.,
2018; Du et al., 2019). We can obtain the following corollary by combining Eq.8 and Assumption
1.
Corollary 1 (See He et al. (2019), Appendix B.3). When all conditions of Theorem 1 and Assump-
tion 1 hold, the generalization bound of the network is negatively correlated with the ratio of the
learning rate to the batch size.
Proof of Corollary 1. We first define
I = ɪ tr(CA-1) + d log (吧)-log (det(CA-1)) + R2 一 d + 2log(1) + 2log N + 4. (63)
2|S|	η	δ
Then, Eq.8 becomes
R(Q) ≤ R(Q) + ʌ/rɪɪ.	(64)
4N 一 2
20
Under review as a conference paper at ICLR 2022
We calculate the derivative of I with respect to the ratio 向 to check whether the generalization
bound has a negative correlation with the ratio. For brevity, we set k =看：
∂I
∂k
∂
∂k 的(CAT)- dlog(2k) - log (det (CAT)) + R2 - d + 2log(I) + 2log N + 4
tr(CA-1) d
2
k
(65)
Therefore, when Assumption 1 holds, we have
d> tr≡-⅛ = ktr(CA-1).
2|S |	2
(66)
Thus,
∂I
∂k < .
(67)
Then, I and the generalization bound have a negative correlation with the ratio of the learning rate
to the batch size.
The proof is competed.
□
B.5.2 Time-Varying Case
In this section, the basis for deriving the second term in Eq.16 based on Eq.10, we will show that
Eq.10 monotonically decreases with respect to n2 in Eq.18 under appropriate assumptions. First,
we make the following assumptions about the dimension d of the parameter space Θ.
Assumption 2 (cf. He et al. (2019), Assumption 2). The network size is large enough:
d>	n2,
(68)
where d is the number of parameters, C is the magnitude of the individual gradient noise, A is the
Hessian matrix around the global minima, and n is the intermediate value of 谭% in Eq.18, η(t)
is the learning rate, and |S (t)| is the batch size.
This assumption can also be justified as Assumption 1 When the netWork sizes of neural netWorks are
often extremely large. We can obtain the folloWing corollary by combining Eq.10 and Assumption
2.
Corollary 2 (extension of He et al. (2019), Appendix B.3). When all conditions of Theorem 2 and
Assumption 2 hold, the generalization bound of the network is negatively correlated with the ratio
of the learning rate to the batch size.
Proof of Corollary 2. Along the proof of Corollary 1, we use Eq.10, k = n2 and Assumption 2,
rather than Eq.8, k = 向 and Assumption 1. Then, We can obtain
∂I
∂k < .
(69)
The proof is competed.
□
21
Under review as a conference paper at ICLR 2022
C Implementation Detail
C.1 Datasets and Models
C.1.1 Datasets
We tested the proposed model on two popular image datasets. CIFAR10 and CIFAR100 (Krizhevsky
et al., 2009): 60 K natural RGB images of 10 classes for CIFAR10 and 100 classes for CIFAR100
with a train/test split of 50K/10 K. For both datasets, we use PyTorch version1.
C.1.2 Models
We tested the proposed model using two popular model networks. For CIFAR10, we used VGG16
(Simonyan & Zisserman, 2015), and for CIFAR100, we use WRN28-10 (Zagoruyko & Komodakis,
2016). For both model networks, we build on the code from the implementation of Kim (2018).
C.1.3 Training
We modified the implementation of Kim (2018). Both datasets are normalized by mean and standard
deviation. In the main experiments, training was performed via SGD with a momentum of 0.9, and
weight decay of 5e-4 for 200 epochs. We began training with a learning rate of ηinit , run for
200 epochs, and reduced by a multiplicative factor of γ after 60, 120, and 160 epochs to make
3
ηf inal = γ ηinit.
C.2 Error Estimation Experiment
C.2.1 Experimental Details
As described in Section 6.2, we fit the parameters of φ to minimize δ(S, η; φ), using least squares
regression. In doing so, We used scipy.optimize.curve_fit2. The optimal parameter φ*
is given by
Φ* = argminX ∣δ(S,η; φ)∣2 .	(70)
To measure the performance of the proposed model, the mean μ and standard deviation σ of the
relative error δ(S, η; φ*) are computed based on the fitted parameter φ*. To evaluate the stability of
the model, We fit a parameter φ based on a randomly sampled generalization error and compute
μ and σ from the relative error δ(S, η; φ). The values of μ and σ obtained after 100 repetitions are
shaded Within one standard deviation, as shoWn in Figure.2.
C.2.2 Found Phi Values
Table 1: The optimal value φ* is determined by the least squares regression of relative error.
(a) Constant case
	c0	ci	c2	c3	c4
CIFAR10	81.92	-7.87 ∙ 10-1	1.00 ∙ 10-4	8.20 ∙ 10-2	301
CIFAR100	8.97	6.45	7.66 ∙ 10-4	1.50 ∙ 10-i	7.69
(b) Time-varying case
	c0	c1	c2	C3	c4	λ1	λ2
CIFAR10	57.68	1.17	1.56 ∙ 10-4	5.23 ∙ 10-2	709	8.35 ∙ 10T	8.89 ∙ 10-2
CIFAR100	34.77	6.91	9.52 ∙ 10-4	1.21 ∙ 10-i	4.03	6.00 ∙ 10-i	1.63 ∙ 10-i
1https://github.com/pytorch/vision
2https://github.com/scipy/scipy
22
Under review as a conference paper at ICLR 2022
C.2.3 Comparison Models for the Time-Varying Case
For the time-varying case in this study, we used a step decay for the learning rate. Thus, rather
than using the mean value theorem of integrals to make approximations, as in Eqs.55 and59, we can
obtain a more accurate model by treating the integral value as a constant for each interval as follows:
t	t0	t
n(t0)F (t0)dt0 = n(0)	F (t0)dt0 + n(t0)	F(t0)dt0 = n(0)I0 + n(t0)I1.	(71)
0	0	t0
We call this the analytical solution model. We call the model using only the final learning rate as the
stationary model and fit these two models to the generalization error. The comparison results can be
found in Table.2.
Table 2: Fitting results of the proposed model and the comparison models for the time-varying case.
(a) CIFAR10
	lφl	μ (% J)	σ (% J)	AIC (J)
stationary model	5	-6.03	23.89	5.08
analytical solution model	13	-2.66	16.15	-98.79
proposed model	7	-2.71	16.28	-108.21
(b) CIFAR100				
	lφl	μ(%J)	σ (% J)	AIC (J)
stationary model	5	-4.88	21.88	-27.97
analytical solution model	13	-0.86	9.57	-316.23
proposed model	7	-0.88	9.69	-323.80
Here, the analytical solution model uses step decay and can divide the integration interval into four,
and then ∣φ∣ = 13. Meanwhile, the stationary model has the same number of parameters as the
constant case, ∣φ∣ = 5. As shown in these results, the analytical solution model obtains the smallest
relative error, and the proposed model has almost the same performance with a smaller number of
parameters. Assuming that the relative error δ follows a normal distribution N(μ, σ) for the mean
μ and standard deviation σ of the relative error δ in the fitted model, We can observe that Akaike's
information criterion (AIC) (Akaike, 1998) is minimized by the proposed model. Thus, the integral
in the proposed model can be approximated sufficiently using the mean value theorem.
C.3 Stability Experiment
C.3.1 Experimental Details
We use the same experimental setup as in Appendix C.2.1. However, there is instability in
scipy.optimize.curve_fit with respect to the initial value of the search parameter φ. Thus,
we sampled an initial value of φ from a uniform distribution [0, 1] and fitted it several times (10 and
5 times for the constant and time-varying cases, respectively). Then, we used the parameter that best
fit the sampled data for the stability experiment.
23
Under review as a conference paper at ICLR 2022
C.4 Hyperparameter Optimization Experiment
C.4. 1 Bayesian Optimization Model for the Constant Case
In this section, we describe the implementation details of the original kernel function proposed in
Eq.20. To construct the proposed kernel function, we implemented the following kernel function
using sklearn.gaussian_process.kernels3:
from sklearn.gaussian_process.kernels import DotProduct, ConstantKernel
sigma_0_bounds = (1e-10, 1e10)
equation:proposed_kernel_constant_case = (DotProduct(sigma_0_bounds=sigma_0_bounds) ** 0.5
+ DotProduct(sigma_0_bounds=sigma_0_bounds)
+ DotProduct(sigma_0_bounds=sigma_0_bounds) ** -0.5
+ ConstantKernel())
sigma_0_bounds decides the range of the σ, which controls the inhomogenity of the kernel.
C.4.2 Algorithm for the Time-Varying Case
The proposed model Eq.16 for the time-varying case cannot be represented by a linear combination
of y = w>ψ(x), as the product term of the parameters of c, λ appears. Thus, we employed a hy-
perparameter search by sequential model-based global optimization (Bergstra et al., 2011) combin-
ing the LeVenberg-MarqUardt algorithm (LM) (Mor6, l978) and Thompson Sampling (Thompson,
1933; Russo et al., 2017), directly using the proposed model. The LM is used to solve non-linear
least sqUares problems. It interpolates between the GaUss—Newton algorithm (GNA) and the GD
method. It is more robUst than the GNA, which implies that, in many cases, it finds a solUtion eVen
if it starts Very far from the final minimUm. It can also be regarded as the GNA Using a trUst re-
gion approach. The distribUtion of the parameter φ is obtained by minimizing the relatiVe error of
δ(S, η; φ) = "S"*/''," with respect to the parameter φ by the LM. The algorithm is as follows.
Algorithm 1 Hyperparameter search algorithm for the time-Varying case
HJ 0
sample φ0 from U (0, 1)
for t J 1 to T do
|S| max，1S| *m,n Jargmin商皿。。，尚皿. δ(S，n； φt-I)
Evaluate f (而 *	, IS *.)
| |max | |min
HJHUK |S| ；ax，1S| mιJ,f ( |S1 =aχ,而：/
μ, Σ J LM(δ, H)
. Expensive step
sample φt from trunc-norm(μ, VZdiag(Σ), blower, bupper)
end for
return H
Here, H represents the history of observations, and f (⅛ *	, ⅛ * ) represents the evaluation of
|S| max |S| min
the network for a hyperparameter (向 * ,看 * ). To keep the parameter φ in this range, Thomp-
son sampling is performed for each parameter from the following truncated normal distribution using
the lower bound blower and upper bound bupper .
1
, xσμ)
f (x； μ, σ, blower, bupper )
σ φ( bup丁μ) - φ( blowσr-μ)'
(72)
where 夕(x) is the probability density function of the standard normal distribution as follows:
Mx)=√1∏ exp (-2 x2
(73)
3https://github.com/scikit-learn/scikit-learn
24
Under review as a conference paper at ICLR 2022
and Φ(x) is its cumulative distribution function:
1
2
Φ(x)
1 + erf
(74)
C.4.3 Experimental Details
In this section, we present the implementation details of the hyperparameter optimization experi-
ments for comparison. In both constant and time-varying cases,看 was allowed to explore in the
range of [10-7, 10-2] for CIFAR10 and in the range of [10-6, 10-2] for CIFAR100. These were
determined based on the experimental results for each dataset (Figures.1a and 1b). We computed
the maximum test accuracy obtained over 30 rounds of hyperparameter search. By performing this
for a total of 100 times, we computed a 95% confidence interval of the maximum accuracy. The
proposed method uses a uniform distribution search, whereas the comparison method uses a uni-
form distribution search and log-uniform search. For comparison, we used Bayesian optimization4
(Nogueira, 2014) using the Matern kernel and RBF kernel, Hyperopt5 (Bergstra et al., 2015), and
Optuna6 (Akiba et al., 2019).
4https://github.com/fmfn/BayesianOptimization
5https://github.com/hyperopt/hyperopt
6https://github.com/optuna/optuna
25