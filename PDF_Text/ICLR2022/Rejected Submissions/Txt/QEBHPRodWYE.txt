Under review as a conference paper at ICLR 2022
InstaHide’s Sample Complexity When Mixing
Two Private Images
Anonymous authors
Paper under double-blind review
Abstract
Inspired by InstaHide challenge [Huang, Song, Li and Arora’20], [Chen, Song
and Zhuo’20] recently provides one mathematical formulation of InstaHide
attack problem under Gaussian images distribution. They show that it
suffices to use O (npkrpirviv -2/(kpriv +1)) samples to recover one private image in
npOri(vkpriv) + poly(npub) time for any integer kpriv, where npriv and npub denote
the number of images used in the private and the public dataset to generate a
mixed image sample. Under the current setup for the InstaHide challenge of
mixing two private images (kpriv = 2), this means np4r/iv3 samples are sufficient
to recover a private image. In this work, we show that npriv log(npriv) samples
are sufficient (information-theoretically) for recovering all the private images.
1	Introduction
Collaboratively training neural networks based on sensitive data is appealing in many AI
applications, such as healthcare, fraud detection, and virtual assistants. How to train neural
networks without compromising data confidentiality and prediction accuracy has become an
important and common research goal Shokri & Shmatikov (2015); Ryffel et al. (2018); Phong
et al. (2018); McMahan et al. (2017); Konecny et al. (20l6) in both academia and industry.
Huang et al. (2020b) recently proposed an approach called InstaHide for image classification.
The key idea is to train the model on a dataset where (1) each image is a mix of kpriv private
images and kpub public images, and (2) each pixel is independently sign-flipped after the
mixing. Instahide shows promising prediction accuracy on MNIST Deng (2012), CIFAR-
10 Krizhevsky (2012), CIFAR-100, and ImageNet datasets Deng et al. (2009). TextHide
Huang et al. (2020a) applies InstaHide’s idea to text datasets and achieves promising results
on natural language processing tasks.
To understand the security aspect of InstaHide in realistic deployment scenarios, InstaHide
authors present an InstaHide challenge Challenge (2020) that involves npriv = 100 private
images, ImageNet dataset as the public images, m = 5000 sample images (each image is a
combination of kpriv = 2 private images and kpub = 4 public images and the sign of each pixel
is randomly flipped). The challenge is to recover a private image given the set of sample
images.
Chen et al. (2021) is a theoretical work that formulates the InstaHide attack problem as a
recovery problem. It also provides an algorithm to recover a private image assuming each
private and public image is a random Gaussian image (i.e., each pixel is an i.i.d. draw
from N (0, 1)). The algorithm shows that O(npkrpirviv-2/(kpriv+1)) sample images are sufficient to
recover one private image. Carlini et al. (2020) provides the first heuristic-based practical
attack for the InstaHide challenge (kpriv = 2), and it can generate images that are visually
similar to the private images in the InstaHide challenge dataset. Luo et al. (2021) provides
the first heuristic-based practical attack for the InstaHide challenge (kpriv = 2) when data
augmentation is enabled.
Although the InstaHide challenge is considered broken by several researchers, the current
InstaHide challenge is itself too simple, and it is unclear whether the existing attacks Carlini
1
Under review as a conference paper at ICLR 2022
et al. (2020); Luo et al. (2021) can still work when we use InstaHide to protect large numbers
of private images (large n) Arora (2020).
This raises an important question:
What’s the minimal number of InstaHide images needed to recover a private image?
This question is worthwhile to consider because it is a quantitative measure for how secure
InstaHide is. With the same formulation in Chen et al. (2021), we achieve a better upper
bound on the number of samples needed to recover private images when kpriv = 2. Our new
algorithm can recover all the private images using only Ω(nprglog(npriv)) samples.1 This
significantly improves the state-of-the-art theoretical results Chen et al. (2021) that requires
np4r/iv3 samples to recover a single private image. However, our running time is exponential
in the number of private images (npriv) and polynomial in the number of public images
(npub), where the running time of the algorithm in Chen et al. (2021) is polynomial in npriv
and npub . In addition, we provide a four-step framework to compare our attacks with the
attacks presented in Carlini et al. (2020) and Chen et al. (2021). We hope our framework
can inspire more efficient attacks on InstaHide-like approaches and can guide the design of
future-generation deep learning algorithms on sensitive data.
1.1	Our result
Chen et al. (2021) formulates the InstaHide attack problem as a recovery problem that given
sample access to an oracle that can generate as much as InstaHide images you want, there
are two goals : 1) sample complexity, minimize the number InstaHide images being used, 2)
running time, use those InstaHide images to recover the original images as fast as possible.
Similar to Chen et al. (2021), we consider the case where private and public data vectors are
Gaussians. Let Spub denote the set of public images with |Spub | = npub, let Spriv denote the
set of private images with |Spriv| = npriv. The model that produces InstaHide image can be
described as follows:
•	Pick kpub vectors from public data set and kpriv vectors from private data set.
•	Normalize kpub vectors by l/ʌ/kpub and normalize kp^ vectors by 1 / ʌ/kpriv.
•	Add kpub + kpriv vectors together to obtain a new vector, then flip each coordinate of
that new vector independently with probability 1/2.
We state our result as follows:
Theorem 1.1 (Informal version of Theorem 3.1). Let kpriv = 2. If there are npriv private
vectors and npub public vectors, each of which is an i.i.d. draw from N (0, Idd), then as long
as
d = ω(Poly(kPUb) log(npub + npriV)),
there is some m = O(npriv log npriv) such that, given a sample of m random synthetic vectors
independently generated as above, one can exactly recover all the private vectors in time
O(dm2 + dn2pub + np2uωb+1 + mnp2ub) + d2O(m)
with high probability.
Notations. For any positive integer n, We use [n] to denote the set {1, 2,…，n}. For a set
S, we use suPP(S) to denote the support of S, i.e., the indices of its elements. We also use
suPP(w) to denote the support of vector w ∈ Rn, i.e. the indices of its non-zero coordinates.
For a vector x, We use kxk2 to denotes entry-Wise `2 norm. For tWo vectors a and b, We use
a ◦ b to denote a vector Where i-th entry is aibi. For a vector a, We use |a| to denote a vector
Where the i-th entry is |ai |. Given a vector v ∈ Rn and a subset S ⊂ [n] We use [v]S ∈ R|S|
to denote the restriction of v to the coordinates indexed by S.
1For the worst case distribution, Ω(npt∙iv) is a trivial sample complexity lower bound.
2
Under review as a conference paper at ICLR 2022
Contributions. Our contributions can be summarized into the following folds.
•	We propose an algorithm that recover all the private images using only
Ω(nprivlog(npriv)) samples in the recent theoretical framework of attacking InstaHide
Huang et al. (2020b) when mixing two private image, improving the state-of-art
result of Chen et al. (2021).
•	We summarize existing methods of attacking InstaHide into a unifying framework.
By examining the functionality of each steps we identify the connection of a key step
with problems in graph isomorphism. We also reveal the vulnerability of existing
method to recover all private images by showing hardness to recover all images.
Organizations. In Section 2 we formulate our attack problem. In Section 3 we present
our algorithm and main results. In Section 4 we conclude our paper and discuss future
directions.
2	Preliminaries
We use the same setup as Chen et al. (2021).
Definition 2.1 (Image matrix notation, Definition 2.2 in Chen et al. (2021)). Let image
matrix X ∈ Rd×n be a matrix whose columns consist of vectors x1, . . . , xn ∈ Rd corresponding
to n images each with d pixels taking values in R. It will also be convenient to refer to the
rows of X as p1, . . . , pd ∈ Rn.
We define public set, private set and synthetic images following the setup in Huang et al.
(2020b).
Definition 2.2 (Public/private notation, Definition 2.3 in Chen et al. (2021)). We will refer
to Spub ⊂ [n] and Spriv = [n]\Spub as the set of public and private images respectively, and
given a vector w ∈ Rn , we will refer to supp(w) ∩ Spub and supp(w) ∩ Spriv as the public and
private coordinates of w respectively.
Definition 2.3 (Synthetic images, Definition 2.4 in Chen et al. (2021)). Given sparsity
levels kpub ≤ |Spub |, kpriv ≤ |Spriv |, image matrix X ∈ Rd×n and a selection vector w ∈ Rn for
which [w]Spub and [w]Spriv are kpub- and kpriv-sparse respectively, the corresponding synthetic
image is the vector yX,w = |Xw| ∈ Rd where | ∙ | denotes entrywise absolute value. We say
that X ∈ Rd×n and a sequence of selection vectors w1, . . . , wm ∈ Rn give rise to a synthetic
dataset Y ∈ Rm×d consisting of the images (yX,w1, . . . , yX,wm)>.
We consider Gaussian image which is a common setting in phase retrieval Candes et al.
(2013); Netrapalli et al. (2017); Candes et al. (2015).
Definition 2.4 (Gaussian images, Definition 2.5 in Chen et al. (2021)). We say that X is a
random Gaussian image matrix if its entries are sampled i.i.d. from N(0, 1).
Distribution over selection vectors follows from variants of Mixup Zhang et al. (2017). Here
we `2 normalize all vectors for convenience of analysis. Since kpriv is a small constant, our
analysis can be easily generalized to other normalizations.
Definition 2.5 (Distribution over selection vectors, Definition 2.6 in Chen et al. (2021)).
Let D be the distribution over selection vectors defined as follows. To sample once from D,
draw random subset T1 ⊂ Spub , T2 ⊆ Spriv of size kpub and kpriv and output the unit vector
whose i-th entry is 1/ʌ/kpub if i ∈ Ti, 1/ʌ/kpriv if i ∈ T2, and zero otherwise.2
For convenience we will define pub and priv operators as follows,
Definition 2.6 (PUbliC/private operators). We define function pub(∙) and priv(∙) such that
for vector w ∈ Rn, pub(w) ∈ Rnpub will be the vector which only contains the coordinates of
w corresponding to the public subset Spub, and priv(w) ∈ Rnpriv will be the vector which only
contains the coordinates of w corresponding to the private subset Spriv.
2Note that any such vector does not specify a convex combination, but this choice of normalization
is just to make some of the analysis later on somewhat cleaner, and our results would still hold if we
chose the vectors in the support of D to have entries summing to 1.
3
Under review as a conference paper at ICLR 2022
Algorithm 1 Recovering All Private Images when kpriv = 2
procedure RecoverAll(Y)
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
. Theorem 3.1, Theorem 1.1
. InstaHide dataset Y = (yX,w1,...,yX,wm)> ∈ Rm×d
. Step 1. Retrieve Gram matrix
M — k i + kb ∙ GramExtract (Y, 2@ b；[ i))	. Algorithm 1 in Chen et al. (2021)
. Step 2. Subtract Public images from Gram matrix
for i ∈ [m] do
Si — LeARnPuBlic({(pj)spub, yX,wi )}j∈[d])	. Algorithm 2 in Chen et al. (2021)
end for
Wpub — (PUb(Vec(Sι)),…，PUb(Vec(Sm)))>	. WPub ∈ {0, l}m×npub
Mpriv — kpriv ∙ (M -焉WpubWjb)
. Step 3. Assign original images
Wpriv J AssigningORiginalIMages(MPriv,npriv)	. Algorithm 2
. Step 4. Solving system of equations.
Ypub = -‰WpubX>,b	. Xpub ∈ Rd×npub, Ypub ∈ Rm×d, Wpub ∈ {0, l}m×npub
kpub
X J SolvingSysTeMofEquaTions(Wpriv, ʌ/kprivYpub, ʌ/kprivY)	. Algorithm 3
return Xe
end procedure
For subset S ⊂ S we will refer to vec(S) ∈ Rn as the vector that vec(S)i = 1 if i ∈ S
and vec(S)i = 0 otherwise. We define the public and private components of W and Y for
convenience.
Definition 2.7 (Public and private components of image matrix and selection vectors). For
a sequence of selection vectors w1, . . . , wm ∈ Rn we will refer to
W = (w1,...,wm)> ∈ Rm×n
as the mixup matrix.
Specifically, we will refer to Wpub ∈ {0, 1}m×npub as the public component of mixup matrix
and Wpriv ∈ {0, 1}m×npriv as the private component of mixup matrix, i.e.,
Wpub
kpub ∙
‘pub(Wι, J
.
.
.
PUb(Wm,* )
-Priv(W1,*)-
∈ {0,1}m×npub, WPriv = pkpriv ∙	.	∈ {0,1}m×npriv.
Priv(Wm,*)
We will refer to Xpub ∈ Rd×npub as public component of image matrix which only contains the
columns of X ∈ Rd×n corresponding to the public subset Spub , and Xpriv ∈ Rd×npriv as private
component of image matrix which only contains the columns of X ∈ Rd×n corresponding to
the private subset Spriv .
Furthermore we define Ypub ∈ Rm×d as public contribution to InstaHide images and Ypriv ∈
Rm×d as private contribution to InstaHide images:
Ypub
W——WpubX>ub,
kpub
Ypriv
P=WPriv
priv
Xp>riv.
Instead of considering only one private image recovery as Chen et al. (2021), here we consider
a harder question which requires to recover all the private images.
Problem 1 (Exact Private image recovery). Let X ∈ Rd×n be a Gaussian image matrix.
Given access to the public images {xs}s∈Spub and the synthetic dataset (yx,w1,..., yx,wm),
where wι,..., wm.〜D are unknown selection vectors, output a set of vectors {es}s∈Spriv
for which there exists a one-to-one mapping φ from {es}s∈Spriv to {xs}s∈Spriv satisfying
Φ3s% = (xs%,Vj ∈ [d].
4
Under review as a conference paper at ICLR 2022
3	Recovering All Private Images when kpriv = 2
In this section, we prove our main algorithmic result. Our algorithm follows the high-level
procedure introduced in section A. The details are elaborated in following subsections.
Theorem 3.1 (Main result). Let Spub ⊂ [n], and let npub = |Spub| and npriv = |Spriv |.
Let kpriv = 2. Let k = kpriv + kpub∙ If d ≥ ω( poly(kpub, kpriv) log(npub + npriV)) and m ≥
Ω (kpoly(kpriv)npriv log npriv), then with high probability over X and the sequence of randomly
chosen selection vectors wι,...,Wm 〜D, there is an algorithm which takes as input the
synthetic dataset Y> = (yX,w1 , . . . , yX,wm) ∈ Rd×m and the columns of X indexed by
Spub, and outputs npriv images {xes}s∈Spriv for which there exists one-to-one mapping φ from
{xes}s∈Spriv to {xs}s∈Spriv satisfying φ(xes)j = (xs)j for all j ∈ [d]. Furthermore, the algorithm
runs in time
O(m2d + dn2ub + 片：!+1 + mn2riv + 2m ∙ mn2rivd).
Remark 3.2. Our result improves on Chen et al. (2021) on two aspects. First, we reduce the
sample complexity from npkrpirviv-2/(kpriv+1) to npriv log npriv when kpriv = 2. Note that our sample
complexity is optimal up to logarithmic factors since finding unique solutions of linear system
requires at least npriv sample complexity. Second, we can recover all private images exactly
rather than recovering a single image, which is highly desirable for real-world practitioners.
Furthermore notice that fixing all public images, multiplying any private image by -1 might
not keep InstaHide images unchanged. Thus information theoretically, we are able to recover
all private images precisely (not only absolute values) as long as we have access to sufficient
synthetic images. In fact, from the proof of Lemma 3.7 our sample complexity suffices to
achieve exact recovery.
3.1	Retrieving Gram matrix
In this section, we present the algorithm for retrieving the Gram matrix.
Lemma 3.3 (Retrieve Gram matrix, Chen et al. (2021)). Let n = npub + npriv. Suppose
d = Ω(log(m∕δ)∕η4). For a random Gaussian image matrix X ∈ Rd×n and arbitrary
wι,..., Wm ∈ S≥- , let Σ* be the output of GRAMEXtRAct when We set η = 1/2k. Then
with probability 1 — δ over the randomness of X, We have that Σ* = k ∙ WW> ∈ Rm×m.
Furthermore, GRAMEXtRAct runs in time O(m2d).
We briefly describe how this is achieved. Without loss of generality, we may assume
Spriv = [n], since once we determine the support of public images Spub , we can easily subtract
the contribution of them. Consider a matrix Y ∈ Rm×d whose rows are yX,w1 , . . . , yX,wm .
Then, it can be written as
lhPι,wιil …|hPd ,wιil
Y =	.	..	.
.	..
Jhp1,Wmi∣ …	|hPd,Wmi|._
Since X is a Gaussian matrix, we can see that each column of Y is the absolute value
of an independent draw of N (0, WW>). We define this distribution as Nfold(0, WW>),
and it can be proved that the covariance matrix of N fold (0, WW>) can be directly related
WW>. Then, the task becomes estimating the covariance matrix of N fold (0, WW>) from
d independent samples (columns of Y), which can be done by computing the empirical
estimates.
3.2 ReMove public iMAges
In this section, we present the algorithm of subtracting public images from Gram matrix.
Formally, given any synthetic image yX,w we recover the entire support of [w]Spub (essentially
supp([w]Spub)).
5
Under review as a conference paper at ICLR 2022
Lemma 3.4 (Subtract public images from Gram matrix, Chen et al. (2021)). Let n =
npriv + npub. For any δ ≥ 0, if d = Ω(poly(kpub)/log(n∕δ)), then with probability at least
1 - δ over the randomness of X, we have that the coordinates output by LearnPublic are
exactly equal to supp([w]Spub). Furthermore, LearnPublic runs in time O(dnp2ub + np2uωb+1),
where ω ≈ 2.373 is the exponent of matrix multiplication Williams (2012).
Note that this problem is closely related to the Gaussian phase retrieval problem. However,
we can only access the public subset of coordinates for any image vector pi . We denote these
ʃ-——-
partial vectors as {[pi]Spub}i∈[d] . The first step is to construct a matrix M ∈ Rnpub ×npub:
1d
f = d ∑((yX'w)2 - 1) ∙ ([pi]spuJpi]>pub -1).
i=1
It can be proved that when pi’s are i.i.d standard Gaussian vectors, the expectation of
ʃ-——-	__	ʃ-——-
M is M = 1 [w]Spub [w]>pub . However, when d n, M is not a sufficiently good spectral
ʃ-——-
approximation of M, which means we cannot directly use the top eigenvector of M. Instead,
with high probability [w]Spub can be approximated by the top eigenvector of the solution of
the following semi-definite programming (SDP):
max
Z0
npub
hMf, Zi s.t. tr[Z] = 1, X |Zi,j| ≤ kpub.
i,j=1
Hence, the time complexity of this step is O(dnp2ub + np2uωb+1), where the first term is the time
ʃ-——-
cost for constructing M and the second term is the time cost for SDP Jiang et al. (2020).
3.3	Assigning encoded images to original images
We are now at the position of recovering Wpriv ∈ Rm×npriv from private Gram matrix
Mpriv ∈ Rm×m. Recall that Mpriv = WprivWp>riv ∈ Rm×m where Wpriv ∈ {0, 1}m×npriv is the
mixup matrix with column sparsity kpriv . By recovering mixup matrix W from private Gram
matrix M the attacker maps each synthetic image yX,wi , i ∈ [m] to two original images
xi1 , . . . , xikpriv (to be recovered in the next step) in the private data set, where kpriv = 2.
On the other hand, in order to recover original image xi from private data set, the attacker
needs to know precisely the set of synthetic images yX,wi , i ∈ [m] generated by xi. Therefore
this step is crucial to recover the original private images from InstaHide images. We provide
an algorithm and certify that it outputs the private component of mixup matrix with sample
complexity m = Ω(nprivlognpriv).
As noted by Chen et al. (2021), the intricacy of this step lies in the fact that a family of
sets may not be uniquely identifiable from the cardinality of all pairwise intersections. This
problem is formally stated in the following.
Problem 2 (Recover sets from cardinality of pairwise intersections). Let Si ⊂ [n],i ∈
[m] be n sets with cardinality k. Given access to the cardinality of pairwise intersections
|Si ∩ Sj | for all i,j ∈ [m], output a family of sets Si ⊂ [n], i ∈ [m] for which there exists
a one-to-one mapping φ from Si, i ∈ [m] to Si, i ∈ [m] satisfying φ(Sj) = Sj for all
j ∈ [m].
In real world applications, attackers may not even have access to precise cardinality of
pairwise intersections |Si ∩ Sj | for all i, j ∈ [m] due to errors in retrieving Gram matrix
and public coordinates. Instead, attackers often face a harder version of the above problem,
where they only know whether |Si ∩ Sj | is an empty set for i, j ∈ [m]. However for mixing
two private images, the two problems are the same.
We now provide a solution to this problem. First we define a concept closely related to the
above problem.
6
Under review as a conference paper at ICLR 2022
Algorithm 2 Assigning Original Images
1:	procedure AssigningOriginalImages(Mpriv , npriv)
2:	. Mpriv ∈ Rm×m is Private Gram matrix, npriv is the number of private images
3:	Mg — Mpriv - I
4:	if npriv < 5 then
5:	for H ∈ {0, 1}npriv×npriv do
6:	MH — adjacency matrix of the line graph of H
7:	if MH = MG then
-	-^r-	-^r_	___ 、	............... .	.
8:	W 一∪ {Wh }	. ^VH is the incidence matrix of H
9:	end if
10:	end for
11:	return Vf
12:	end if
13:	Reconstruct G from MG	. By Theorem C.2
14:	return V	. The incidence matrix of G
15:	end procedure
Definition 3.5 (Distinguishable). For matrix M ∈ Rm×m, we say M is distinguishable if
there exists unique solution W = (w1, . . . , wm)> (up to permutation of rows) to the equation
WW> = M such that wi ∈ supp(Dpriv) for all i ∈ [m].
Lemma 3.6 (Assign InstaHide images to the original images). When m = Ω(nprjvlognprlv),
let Wpriv = (w1, . . . , wm)> where wi, i ∈ [m] are sampled from distribution Dpriv and Mpriv =
WprivWp>riv ∈ Rm×m . Then with high probability Mpriv is distinguishable and algorithm
AssigningOriginalImages inputs private Gram matrix Mpriv ∈ {0, 1, 2}m×m correctly
outputs Wpriv ∈ {0, 1}m×npriv with row sparsity kpriv = 2 such that WprivWp>riv = Mpriv.
Furthermore AssigningOriginalImages runs in time O(mnpriv).
The proof of Lemma 3.6 is deferred to Appendix C. We consider graph G = (V, E), |V | =
npriv and |E | = m where each vi ∈ V corresponds to an original image in private data set
and each e = (vi, vj) ∈ E correspond to an encrypted image generated from two original
images corresponding to vi and vj . We define the Gram matrix of graph G = (V, E), denoted
by MG ∈ {0, 1, 2}m×m where m = |E|, to be MG = WW> - I where W ∈ {0, 1}m×npriv is
the incidence matrix of G. That is 3
∣eι ∩ eι∣ …	∣eι ∩ em|
MG =	.	...	.	∈{0,1,2}m×m
|em ∩ e1 | ∙∙∙	|em ∩ em |
We can see that MG actually correspond the line graph L(G) of the graph G. We similarly call
a graph G distinguishable if there exists no other graph G0 such that G and G0 have the same
Gram matrix (up to permutations of edges), namely MG = MG0 (for some ordering of edges).
To put it into another word, if we know MG, we can recover G uniquely. Therefore, recovering
W from M can be viewed as recovering graph G from its Gram matrix MG ∈ Rm×m , and a
graph is distinguishable if and only if its Gram matrix MG is distinguishable.
This problem was studied since the 1970s and fully resolved by Whitney Whitney (1992). In
fact, from a line graph L(G) one can first identify a tree of the original graph G and then
proceed to recover the whole graph. The proof is then completed from well-known facts in
random graph theory Erdos & Renyi (1960) that G is connected with high probability when
m = Ω(npriv lognpriv). This paradigm can potentially be extended to handle k ≥ 3 case with
more information of G. Intuitively, this is achievable for a dense subgraph of G, such as the
local structure identified by Chen et al. (2021). More discussion can be found in Appendix C.
3.4	Solving a large system of equations
In this section, we solve the step 4, recovering all private images by solving an '2-regression
problem. Formally, given mixup coefficients Wpriv (for private images) and contributions to
3With high probability, W will not have multi-edge. So, most entries of M will be in {0, 1}.
7
Under review as a conference paper at ICLR 2022
InstaHide images from public images Ypub we recover all private images Xpriv (up to absolute
value).
Lemma 3.7 (Solve '2-regressi0n with hidden signs). Given WPriv ∈ Rm×npriv and Ypub, Y ∈
Rm×d. For each i ∈ [d], let Y*,i ∈ Rm denote the i-th column of Y and similarity for YPUb * i,
the following `2 regression
min
zi∈Rnpriv
k|Wprivzi + Ypub*,i| - Y*,ik2.
for all i ∈ [d] can be solve by SoLviNGSysteMOfEquAtioNs in time O(2m ∙ mn2riv ∙ d)∙
Proof. Suppose WPriv = [wι w2 •… Wm]>. Then, the '2-regressi0n we considered actu-
ally minimizes
mm
X(Iwj zi + YPUbj,i∣ - Yj,i)2 = ^X(Wj zi + YPUbj,i - σj ∙ Yji)),
j=1	j=1
where σj∈ {-1, 1} is the sign of wj zi* for the minimizer zi* .
Therefore, in Algorithm 3, we enumerate all possible σ ∈ {±1}m. Once σ is fixed, the
optimization problem becomes the usual '2-regression, which can be solved in。(n嬴 + mn2riv)
time. Since we assume m = Ω(nPriv log(nPriv)) in the previous step, the total time complexity
is O(2m ∙ mn2riv).
If sign(WPrivz + YPUb*,i) = σ holds for z = minz∈RnPriv kWPrivz + YPUb*,i - σ ◦ Y*,ik2, then
Em=ι(∣w>zi + YPUbjiI 一 Yji)2 = O and z is the unique minimizer of the signed '2-regression
problem almost surely.
Indeed, if we have for σ 6= σe, WPriv(XP>riv)*,i + YPUb*,i 一 σ ◦ Y*,i = 0 and WPriv ze + YPUb*,i 一
σe ◦ Y*,i
0 hold, then from direct calculations we come to
WPrive = σ ◦ e ◦ (WPriv (X>iv)*,i +
YPUb*,i) 一 YPUb*,i. This indicates that σ ◦ σe ◦ (WPriv(XP>riv)*,i + YPUb*,i) 一 YPUb*,i lies in a
nPriv -dimensional subspace of Rm. Noting that (XP>riv)j,i and YPUbj,i are i.i.d sampled from
Gaussian, the event above happens with probability zero since m nPriv . Thus, we can
repeat this process for all i ∈ [d] and solve all zi,s.	□
We also show that '2-regression with hidden signs is in fact a very hard problem. Although
empirical methods may bypass this issue by directly applying gradient descent, real world
practitioners taking shortcuts would certainly suffer from a lack of apriori theoretical
guarantees when facing a large private dataset.
Theorem 3.8 (Lower bound of '2-regression with hidden signs, informal version of The-
orem D.4.). There exists a constant > 0 such that it is NP-hard to (1 + )-approximate
minz∈Rn kIWzI 一 yk2, where W ∈ {0, 1}m×n is row 2-sparse and y ∈ {0, 1}m.
We will reduce the MAX-CUT problem to the '2-regression. MAX-CUT is a well-known
NP-hard problem Berman & Karpinski (1999). A MAX-CUT instance is a graph G = (V, E)
with n vertices and m edges. The goal is to find a subset of vertices S ⊆ V such that
the number of edges between S and V \S is maximized, i.e., maxS⊆V IE (S, V \S)I. We can
further assume G is 3-regular, that is, each vertex has degree 3. The full proof is deferred to
Appendix D.
3.5 PuttiNG everythiNG toGether
Now we are in the position to prove Theorem 3.1.
Proof. By Lemma 3.3 the matrix computed in Line 4 satisfies M = WW> . By Lemma 3.4,
Line 7 correctly computes the indices of all public coordinaes of wi , i ∈ [m]. Therefore from
M = WW> = WPrivWP>riv/kPriv + WPUbWP>Ub/kPUb,
8
Under review as a conference paper at ICLR 2022
Algorithm 3 Solving a large system of equations
1:	procedure SolvingSystemOfEquations(Wpriv , Ypub , Y)
2:	. Wpriv ∈ Rm×npriv , Ypub ∈ Rm×d, Y ∈ Rm×d
3:	for i = 1 → d do
4:	Xi JV
5:	for σ ∈ {-1, +1}m do
6:	z J minz∈Rnpriv k WprivZ + Ypub*,i- σ ◦ Y*,i∣∣2
7：	if Sign(Wprivz + Ypub*,j = σ then
8:	xei J xei ∪ z
9:	end if
10:	end for
11:	end for
12：	X j{Xi,…，Xd}
13:	return X
14:	end procedure
the Gram matrix computed in Line 10 satisfies Mpriv = Wpriv Wp>riv.
We can now apply Lemma 3.6 to find the private components of mixup weights. Indeed, the
output of Line 12 is exactly
-Priv(Wι,* )^
Wpriv = kpriv∙	.	∈ {0, 1}m×npriv .
Priv(Wm,*)
Based on the correctness of private weights, the output in Line 15 is exactly all private
images by Lemma 3.7. This completes the proof of correctness of Algorithm 1.
By Lemma 3.3, Line 4 takes O(m* 2d) time. By Lemma 3.4 Line 7 runs in time O(dnp2ub+np2uωb+1).
By Lemma 3.6 private weights can be computed in time mnpriv. Line 10 and Line 14 can be
computed efficiently in time O(mω). Finally Line 15 is computes in time O(2m ∙ mn^z ∙ d)
by Lemma 3.7. Combining all these steps we have that the total running time of Algorithm 1
is bounded by O(m2d + dnpub + n^+1 + mn⅛v + 2m ∙ mn^r-vd).	□
4 Conclusion and Future Work
We show that Ω(npriv log npr-v) samples suffice to recover all private images under the current
setup for InstaHide challenge of mixing two private images. We show that a key step in
attacking procedure can be formulated as a problem of graph isomorphism and prove the
uniqueness and hardness of recovery. Our approach has significantly advanced the state-of-
the-art approach Chen et al. (2021) that requires n4pr/iv3 samples to recover a single private
image. In addition, we present a theoretical framework to reason about the similarities and
differences of existing attacks Carlini et al. (2020); Chen et al. (2021) and our attack on
Instahide.
Based on our framework, there are several interesting directions for future study:
• How to generalize our result to recover all private images when mixing more than
two private images?
• How to extend this framework to analyze multi-task phase retrieval problem with
real-world data?
Real-world security is not a binary issue. We hope that our theoretical contributions shed
light on the discussion of safety for distributed training algorithms and provide inspirations
for the development of better practical privacy-preserving machine learning methods.
9
Under review as a conference paper at ICLR 2022
5	Ethics Statement
Our work can potentially create negative societal impacts because our theoretical results
can inspire new efficient practical attack methods to machine learning models protected by
InstaHide.
6	Reproducibility Statement
This is a theory paper. We explicitly stated our assumptions and we provided the complete
proofs in supplementary materials.
References
Sanjeev Arora. Instahide. http://www.offconvex.org/2020/11/11/instahide/, 2020.
Piotr Berman and Marek Karpinski. On some tighter inapproximability results. In Inter-
national Colloquium on Automata, Languages, and Programming, pp. 200-209. Springer,
1999.
J Candes, Emmanuel, Thomas Strohmer, and Vladislav Voroninski. Phaselift: Exact and
stable signal recovery from magnitude measurements via convex programming. Communi-
cations on Pure and Applied Mathematics, 66(8):1241-1274, 2013.
J Candes, Emmanuel, Xiaodong Li, and Mahdi. Soltanolkotabi. Phase retrieval via wirtinger
flow: Theory and algorithms. IEEE Transactions on Information Theory, 61(4):1985-2007,
2015.
Nicholas Carlini, Samuel Deng, Sanjam Garg, Somesh Jha, Saeed Mahloujifar, Moham-
mad Mahmoody, Shuang Song, Abhradeep Thakurta, and Florian Tramer. An attack
on instahide: Is private learning possible with instance encoding? arXiv preprint
arXiv:2011.05315, 2020.
InstaHide Challenge. Instahide challenge. https://github.com/Hazelsuko07/InstaHideChallenge,
2020.
Sitan Chen, Xiaoxiao Li, Zhao Song, and Danyang Zhuo. On instahide, phase retrieval, and
sparse matrix factorization. In ICLR, 2021.
Daniele Giorgio Degiorgi and Klaus Simon. A dynamic algorithm for line graph recognition.
In International Workshop on Graph-Theoretic Concepts in Computer Science, pp. 37-48.
Springer, 1995.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern
recognition (CVPR), pp. 248-255, 2009.
L. Deng. The mnist database of handwritten digit images for machine learning research [best
of the web]. IEEE Signal Processing Magazine, 29(6):141-142, 2012. doi: 10.1109/MSP.
2012.2211477.
Paul Erdos and Alfred Renyi. On the evolution of random graphs. Publ. Math. Inst. Hung.
Acad. Sci, 5(1):17-60, 1960.
Dimitris Fotakis, Michael Lampis, and Vangelis Th Paschos. Sub-exponential approximation
schemes for csps: From dense to almost sparse. In 33rd Symposium on Theoretical Aspects
of Computer Science (STACS 2016). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik,
2016.
Baihe Huang, Shunhua Jiang, Zhao Song, and Runzhou Tao. Solving tall dense sdps in the
current matrix multiplication time. arXiv preprint arXiv:2101.08208, 2021.
10
Under review as a conference paper at ICLR 2022
Yangsibo Huang, Zhao Song, Danqi Chen, Kai Li, and Sanjeev Arora. Texthide: Tackling
data privacy in language understanding tasks. In The Conference on Empirical Methods
in Natural Language Processing (Findings of EMNLP), 2020a.
Yangsibo Huang, Zhao Song, Kai Li, and Sanjeev Arora. Instahide: Instance-hiding schemes
for private distributed learning. In International Conference on Machine Learning (ICML),
2020b.
Russell Impagliazzo and Ramamohan Paturi. On the complexity of k-sat. Journal of
Computer and System Sciences, 62(2):367-375, 2001.
Michael S Jacobson, Andre E Kezdy, and Jeno LeheL Recognizing intersection graphs of
linear uniform hypergraphs. Graphs and Combinatorics, 13(4):359-367, 1997.
Haotian Jiang, Tarun Kathuria, Yin Tat Lee, Swati Padmanabhan, and Zhao Song. A faster
interior point method for semidefinite programming. In 61st Annual IEEE Symposium on
Foundations of Computer Science (FOCS), 2020.
Jakub Konecny, H Brendan McMahan, Felix X Yu, Peter Richtarik, Ananda Theertha
Suresh, and Dave Bacon. Federated learning: Strategies for improving communication
efficiency. arXiv preprint arXiv:1610.05492, 2016.
Alex Krizhevsky. Learning multiple layers of features from tiny images. University of Toronto,
05 2012.
Philippe GH Lehot. An optimal algorithm to detect a line graph and output its root graph.
Journal of the ACM (JACM), 21(4):569-575, 1974.
AG Levin and Regina Iosifovna Tyshkevich. Edge hypergraphs. Diskretnaya Matematika, 5
(1):112-129, 1993.
Dajie Liu, Sto jan Trajanovski, and Piet Van Mieghem. Iligra: an efficient inverse line graph
algorithm. Journal of Mathematical Modelling and Algorithms in Operations Research, 14
(1):13-33, 2015.
L Lovasz. Problem, beitrag zur graphentheorie und deren auwendungen, vorgstragen auf
dem intern. koll, 1977.
Xinjian Luo, Xiaokui Xiao, Yuncheng Wu, Juncheng Liu, and Beng Chin Ooi. A fusion-
denoising attack on instahide with data augmentation. arXiv preprint arXiv:2105.07754,
2021.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial
Intelligence and Statistics, pp. 1273-1282. PMLR, 2017.
Praneeth Netrapalli, Prateek Jain, and Sujay. Sanghavi. Phase retrieval using alternating
minimization. In Advances in Neural Information Processing Systems (NeurIPS), pp.
1273-1282, 2017.
L. T. Phong, Y. Aono, T. Hayashi, L. Wang, and S. Moriai. Privacy-preserving deep learning
via additively homomorphic encryption. IEEE Transactions on Information Forensics and
Security, 13(5):1333-1345, 2018. doi: 10.1109/TIFS.2017.2787987.
Svatopluk Poljak, Vojtech Rodl, and Daniel TURZiK. Complexity of representation of graphs
by set systems. Discrete Applied Mathematics, 3(4):301-312, 1981.
Nicholas D Roussopoulos. A max {m, n} algorithm for determining the graph h from its line
graph g. Information Processing Letters, 2(4):108-112, 1973.
Theo Ryffel, Andrew Trask, Morten Dahl, Bobby Wagner, Jason Mancuso, Daniel Rueckert,
and Jonathan Passerat-Palmbach. A generic framework for privacy preserving deep
learning. CoRR, abs/1811.04017, 2018. URL http://arxiv.org/abs/1811.04017.
11
Under review as a conference paper at ICLR 2022
Reza Shokri and Vitaly Shmatikov. Privacy-preserving deep learning. In Proceedings of
the 22nd ACM SIGSAC Conference on Computer and Communications Security, CCS
’15, pp. 1310-1321, New York, NY, USA, 2015. Association for Computing Machinery.
ISBN 9781450338325. doi: 10.1145/2810103.2813687. URL https://doi.org/10.1145/
2810103.2813687.
Maciej M Syslo. A labeling algorithm to recognize a line digraph and output its root graph.
Information Processing Letters, 15(1):28-30, 1982.
Hassler Whitney. Congruent graphs and the connectivity of graphs. In Hassler Whitney
Collected Papers, pp. 61-79. Springer, 1992.
Virginia Vassilevska Williams. Multiplying matrices faster than coppersmith-winograd. In
Proceedings of the forty-fourth annual ACM symposium on Theory of computing (STOC),
pp. 887-898. ACM, 2012.
Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond
empirical risk minimization. In ICLR. https://arxiv.org/abs/1710.09412, 2017.
IE Zverovich. An analogue of the whithey theorem for edge graphs of multigraphs, and edge
multigraphs. Discrete Mathematics and Applications, 7(3):287-294, 1997.
12
Under review as a conference paper at ICLR 2022
A A Unified Framework to Compare with Existing Attacks
Refs	Rec.	kpriv	Samples	Step 1	Step 2	Step 3	Step 4
Chen	one	≥ 2	ʃm ≥ nkpriv-2/(kpriv+1)	dm2	dr> 2	+ n2ω + 1 dnpub 十 npub	2 m2	2 kpriv
Ours	^0ii-	二2	m ≥ npriv log npriv	dm2	^d∑2^^十外 2ω + 1 dnpub + npub	mnpriv	2m ∙ n2rivd
Table 1: A summary of running times in different steps between ours and Chen et al. (2021).
This table only compares the theoretical result. Let kpriv denote the number of private images we
select in InstaHide image. Let d denote the dimension of image. Let npub denote the number of
images in public dataset. Let npriv denote the number of images in private dataset. We provide a
computational lower bound for Step 4 in Appendix D. There is no algorithm that solves Step 4
in 2o(npriv) time under Exponential Time Hypothesis (ETH) (Theorem D.4). Let Rec. denote the
Recover.
Our attack algorithm (Algorithm 1) contains four steps for kpriv = 2. We can prove
m = O(npriv log(npriv)) suffices for exact recovery. Our algorithm shares similarities as two
recent attack results : one is a practical attack Carlini et al. (2020), the other is a theoretical
attack Chen et al. (2021). In the next few paragraphs, we describe our attack algorithm in
four ma jor steps. For each step, we also give a comparison with the corresponding step in
Carlini et al. (2020) and Chen et al. (2021).
• Step 1. Section 3.1. Recover the Gram matrix M = WW> ∈ Rm×m of mixup
weights W from synthetic images Y. This Gram matrix contains all inner products
of mixup weights hwi, wji. Intuitively this measures the similarity of each pair of
two synthetic images and is a natural start of all existing attacking algorithms.
-	For this step, Carlini et al. (2020)'s attack uses a pre-trained neural network on
public dataset to construct the Gram matrix.
-	For this step, note that Y follows folded Gaussian distribution whose covariance
matrix is directly related to M. We can thus solve this step by estimating
the covariance of a folded Gaussian distribution. This is achieved by using
Algorithm 2 in Chen et al. (2021). It takes O(m2d) time.
•	Step 2. Section 3.2. Recover all public image coefficients and substract the
contribution of public coefficients from Gram matrix M to obtain Mpriv . This step
is considered as one of the main computational obstacle for private image recovery.
-	For this step, Carlini et al. (2020)’s attack: 1) they treat public images as
noise, 2) they don’t need to take care of the public images’ labels, since current
InstaHide Challenge doesn’t provide label for public images.
-	For this step, we invoke a paradigm in sparse phase retrieval via using general
SDP solver to approximate the principle components of the Gram matrix of
public coefficients. Chen et al. (2021) proved that this method exactly outputs
all public coefficients. The time of this step has two parts : 1) formulating
the matrix, which takes dnp2ub , 2) solving a SDP with np2ub × np2ub size matrix
variable and O(np2ub) constraints, which takes np2uωb+1 time Jiang et al. (2020);
Huang et al. (2021), where ω is the exponent of matrix multiplication.
• Step 3. Section 3.3. Recover private coefficients Wpriv ∈ Rm×npriv from private
Gram matrix MPriv (Mpriv = WprivW>iv), this SteP takes O(m ∙崂丸)time.
-	For this step, Carlini et al. (2020)’s attack uses K -means to figure out cliques
and then solves a min-cost max flow problem to find the correspondence between
InstaHide image and original image (see Appendix B for detailed discussions).
-	For this step Chen et al. (2021) starts by finding a local structure called
“floral matrix” in the Gram matrix. They prove the existence of this local
structure when m ≥ npkrpirviv-2/(kpriv+1) . Then Chen et al. (2021) can recover private
coefficients within that local structure using nice combinatorial properties of
the “floral matrix”.
13
Under review as a conference paper at ICLR 2022
-	For this step, We note the fact that in kpriv = 2 case the mixup matrix corresponds
to the incident matrix of a graph G and the Gram matrix corresponds to its
line graph L(G) (While kpriv ≥ 3 cases correspond to hypergraphs). We can
then leverage results in graph isomorphism theory to recover the all private
coefficients. In particular, when m ≥ Ω(nprg lognpriv) the private coefficients
are uniquely identifiable from the Gram matrix.
• Step 4. Section 3.4. Solve d independent '2-regression problems to find private
images XMv. Given WMv ∈ Rm×npriv and Y ∈ Rm×d. For each i ∈ [d], let Y*,i ∈ Rm
denote the i-th column of Y , we need to solve the following `2 regression
ZminnrivkWPrivZ + γpub*,ilTγ*,ilk2 .
The classical `2 regression can be solved in an efficient way in both theory and
practice. However, here we don’t know the random signs and we have to consider
all 2m possibilities. In fact we show that solving `2 regression with hidden signs is
NP-hard.
-	For this step, Carlini et al. (2020)’s attack is a heuristic algorithm that uses
gradient descent.
-	For this step, we enumerate all possibilities of random signs to reduce it to
standard `2 regressions. Chen et al. (2021)’s attack is doing the exact same
thing as us. However, since their goal is just recovering one private image (which
means m = O(k2)) they only need to guess 2k2 possibilities.
B Summary of the Attack by Carlini et al. Carlini et al. (2020)
This section summarizes the result of Carlini et al, which is an attack of InstaHide when
kpriv = 2. Carlini et al. (2020). We first briefly describe the current version of InstaHide
Challenge. Suppose there are npriv private images, the InstaHide authors Huang et al.
(2020b) first choose a parameter T , this can be viewed as the number of iterations in the deep
learning training process. For each t ∈ [T], Huang et al. (2020b) draws a random permutation
πt : [npriv] → [npriv]. Each InstaHide image is constructed from a private image i, another
private image ∏t(i) and also some public images. Therefore, there are T ∙ npriv InstaHide
images in total. Here is a trivial observation: each private image shown up in exactly 2T
InstaHide images (because kpriv = 2). The model in Chen et al. (2021) is a different one: each
InstaHide image is constructed from two random private images and some random public
images. Thus, the observation that each private image appears exactly 2T does not hold. In
the current version of InstaHide Challenge, the InstaHide authors create the InstaHide labels
(a vector that lies in RL where the L is the number of classes in image classification task)
in a way that the label of an InstaHide image is a linear combination of labels (i.e., one-hot
vectors) of the private images and not the public images. This is also a major difference
compared with Chen et al. (2021). Note that Carlini et al. (2020) won’t be confused about,
for the label of an InstaHide image, which coordinates of the label vector are from the private
images and which are from the public images.
• Step 1. Recover a similarity4 matrix M ∈ {0, 1, 2}m×m.
- Train a deep neural network based on all the public images, and use that neural
network to construct the similarity matrix M.
•	Step 2. Treat public image as noise.
•	Step 3. Clustering. This step is divided into 3 substeps.
The first substep uses the similarity matrix M to construct T npriv clusters of InstaHide
images based on each InstaHide image such that the images inside one cluster shares
a common original image.
The second substep runs K -means on these clusters, to group clusters into npriv
groups such that each group corresponds to one original image.
4In Carlini et al. (2020), they call it similarity matrix, in Chen et al. (2021) they call it Gram
matrix. Here, we follow Carlini et al. (2020) for convenience.
14
Under review as a conference paper at ICLR 2022
The third substep constructs a min-cost flow graph to compute the two original
images that each InstaHide image is mixed from.
-	Grow clusters. Figure 1 illustrates an example of this step. For a subset S of
InstaHide images (S ⊂ [m]), we define insert(S) as
Insert(S) = S ∪ arg max	Mi,j
i∈[m] j∈S
For each i ∈ [m], we compute set Si ⊂ [m] where Si = insert(T/2) ({i}).
-	Select cluster representatives. Figure 1 illustrates an example of this step.
Define distance between clusters as
dist(i,j)
|Si ∩ Sj |
|Si ∪ Sj |.
Run k-means using metric dist : [m] × [m] → R and k = npriv. Result is npriv
groups C1 , . . . , Cnpriv ⊆ [m]. Randomly select a representative ri ∈ Ci , for each
i ∈ [npriv].
-	Computing assignments. Construct a min-cost flow graph as Figure 2, with
ʃ-——-
weight matrix W ∈ Rm×npriv defined as follows:
Wfi,j
S x Mi
rj k∈Srj
,k.
for i ∈ [m], j ∈ [npriv]. After solving the min-cost flow (Figure 3), construct the
assignment matrix W ∈ Rm×npriv such that Wi,j = 1 if the edge from i to j has
flow, and 0 otherwise.
• Step 4. Recover original image. From Step 3, we have the unweighted assignment
matrix W ∈ {0, 1}m×npriv. Before we recover the original image, we need to first
recover the weight of mixing, which is represented by the weighted assignment matrix
U ∈ Rm×npriv . To recover weight, we first recover the label for each cluster group,
and use the recovered label and the mixed label to recover the weight.
-	First, we recover the label for each cluster, for all i ∈ [npriv]. Let L denote the
number of classes in the classification task of InstaHide application. For j ∈ [m],
let yj ∈ RL be the label of j.
label(i) =	supp(yj) ∈ [L].
j∈[m],Wj,i=1
Then, for i ∈ [m] and j ∈ [npriv] such that Wi,j = 1, define Ui,j = yi,label(j) for
| supp(yi)| = 2 and Ui,j = yi,label(j) /2 for | supp(yi)| = 1.
Here, W ∈ {0, 1}m×npriv is the unweighted assignment matrix and U ∈ Rm×npriv
is the weighted assignment matrix. For Wi,j = 0, let Ui,j = 0.
-	Second, for each pixel i ∈ [d], we run gradient descent to find the original
images. Let Y ∈ Rm×d be the matrix of all InstaHide images, Y*,i denote the
i-th column of Y.5
min
z∈Rnpriv
k∣Uz∣-∣Y*,ilk2.
C Missing proofs for Theorem 3.6
For simplicity, let W denote Wpriv and M denote Mpriv in this section.
5 The description of the attack in Carlini et al. (2020) recovers original images by using gradient
descent for minz∈Rnpriv ∣∣U∣z∣ 一 ∣Y*,i∣∣∣2, which We believe is a typo.
15
Under review as a conference paper at ICLR 2022
C.1 A graph problem (kpriv = 2)
Theorem C.1 (Whitney (1992)). Suppose G and H are connected simple graphs and
L(G) = L(H). If G and H are not K3 and K1,3, then G = H. Furthermore, if |V(G)| ≥ 5,
then an isomorphism of L(G) uniquely determines an isomorphism of G.
In other words, this theorem claims that given M = WW> , if the underlying W is not the
incident matrix of K3 or K1,3 , W can be uniquely identified up to permutation. Theorem C.1
can also be generalized to the case when G has multi-edges Zverovich (1997).
On the other hand, a series of work Roussopoulos (1973); Lehot (1974); Syslo (1982); Degiorgi
& Simon (1995); Liu et al. (2015) showed how to efficiently reconstruct the original graph
from its line graph:
Theorem C.2 (Liu et al. (2015)). Given a graph L with m vertices and t edges, there exists
an algorithm that runs in time O(m + t) to decide whether L is a line graph and output the
original graph G. Furthermore, if L is promised to be the line graph of G, then there exists
an algorithm that outputs G in time O(m).
With Theorem C.1 and Theorem C.2, Theorem 3.6 follows immediately:
Proof of Theorem 3.6. First, since m = Ω(nprivlog(npriv)), a well-known fact in random
graph theory by Erdos and Renyi Erdos & Renyi (1960) showed that the graph G with
incidence matrix W will almost surely be connected. Then, we compute MG = M - I, the
adjacency matrix of the line graph L(G). Theorem C.1 implies that G can be uniquely
recovered from MG as long as npriv is large enough. Finally, We can reconstruct G from MG
by Theorem C.2.
For the time complexity of Algorithm 2, the reconstruction step can be done in O(m)
time. Since we need to output the matrix W, we will take O(mnpriv)-time to construct the
adjacency matrix of G. Here, we do not count the time for reading the whole matrix M into
memory.
□
C.2 General case (kpriv > 2)
The characterization of M and W as the line graph and incidence graph can be generalized
to kpriv > 2 case, which corresponds to hypergraphs.
Suppose M = WW> with kpriv = k > 2. Then, W can be recognized as the incidence
matrix of a k-uniform hypergraph G, i.e., each hyperedge contains k vertices. MG = M - I
corresponds to adjacency matrix of the line graph of hypergraph G: (MG)i,j = |ei ∩ ej | for
ei , ej being two hyperedges. Now, we can see that each entry of MG is in {0, . . . , k}.
Unfortunately, the identification problem becomes very complicated for hypergraphs. Lovasz
Lovasz (1977) stated the problem of characterizing the line graphs of 3-uniform hypergraphs
and noted that Whitney’s isomorphism theorem (Theorem C.1) cannot be generalized to
hypergraphs. Hence, we may not be able to uniquely determine the underlying hypergraph
and we should just consider a more basic problem:
Problem C.3 (Line graph recognition for hypergraph). Given a simple graph L = (V, E)
and k ∈ N, decide if L is the line graph of a k-uniform hypergraph G.
Even for the recognition problem, it was proved to be NP-complete for fixed k ≥ 3 Levin &
Tyshkevich (1993); Poljak et al. (1981). However, Problem C.3 becomes tractable if we add
more constraints to the underlying hypergraph G. First, suppose G is a linear hypergraph,
i.e., the intersection of two hyperedges is at most one. If we further assume the minimum
degree of G is at least 10, i.e., each vertex are in at least 10 hyperedges, there exists a
polynomial-time algorithm for the decision problem. Similar result also holds for k > 3
Jacobson et al. (1997). Let the edge-degree of a hyperedge be the number of triangles in the
hypergraph containing that hyperedge. Jacobson et al. (1997) showed that assuming the
minimum edge-degree of G is at least 2k2 - 3k + 1, there exists a polynomial-time algorithm
16
Under review as a conference paper at ICLR 2022
private images
Grow
N=2
m = T npriv = 8
InstaHide images
Initial
m = 8 clusters
K-means
K=4
Figure 1: An example about cluster step in Carlini et al. (2020) for T = 2 and npriv = 4.
First, starting from each InstaHide image (top), the algorithm grows cluster Si with size 3
(middle). Then, we use K -means for K = 4 to compute 4 groups C1, . . . , C4 (bottom), these
groups each correspond to one original image.
to decide whether L is the line graph of a linear k-uniform hypergraph. Furthermore, in the
yes case, the algorithm can also reconstruct the underlying hypergraph. We also note that
without any constraint on minimum degree or edge-degree, the complexity of recognizing
line graphs of k-uniform linear hypergraphs is still unknown.
17
Under review as a conference paper at ICLR 2022
InstaHide images
T ∙ npriv
original images
npriv
terminal
Figure 2: The construction of the graph for min-cost max flow. c denotes the flow capacity of the
edge, and W denote the weight of the edge. The graph contains T ∙ npriv nodes for each InstaHide
images, npriv nodes for each original images, a source and a terminal. There are three types of edges:
i) (left) from the source to each InstaHide image node, with flow capacity 2 and weight 0; ii) (middle)
from each InstaHide image node i to each original image node j, with flow capacity 1 and weight
Wi,j ; iii) (right) from each original image node to the terminal, with flow capacity 2T and weight 0.
original images
npriv
InstaHide images
T ∙ npriv
Figure 3: The result of solving the min-cost flow in Figure 2. Each InstaHide image is
assigned to two clusters, which ideally correspond to two original images. In reality, a cluster
may not contain all InstaHide images that share the same original image.
D Computational Lower Bound
The goal of this section is to prove that the `2 -regression with hidden signs is actually a
very hard problem, even for approximation (Theorem D.4), which implies that Algorithm 3
cannot be significantly improved. For simplicity We consider SPUb = 0.
We first state an NP-hardness of approximation result for 3-regular MAX-CUT.
Theorem D.1 (Imapproximability of 3-regular MAX-CUT, Berman & Karpinski (1999)).
For every > 0, it is NP-hard to approximate 3-regular MAX-CUT within a factor of r + ,
where r ≈ 0.997.
18
Under review as a conference paper at ICLR 2022
If we assume the Exponential Time Hypothesis (ETH), which a plausible assumption in
theoretical computer science, we can get stronger lower bound for MAX-CUT.
Definition D.2 (Exponential Time Hypothesis (ETH), Impagliazzo & Paturi (2001)). There
exists a constant > 0 such that the time complexity of n-variable 3SAT is at least 2n .
Theorem D.3 (Fotakis et al. (2016)). Assuming ETH, there exists a constant 0 < r0 < 1
such that no 2o(n) -time algorithm can r0 -approximate the MaxCut of an n-vertex, 5-regular
graph.
With Theorem D.1 and Theorem D.3, we can prove the following inapproximability result
for the '2-regressi0n problem with hidden signs.
Theorem D.4 (Lower bound of '2-regressi0n with hidden signs). There exists a constant
> 0 such that it is NP-hard to (1 + )-approximate
min k|Wz| - yk2,	(1)
z∈Rn
where W ∈ {0, 1}m×n is row 2-sparse and y ∈ {0, 1}m.
Furthermore, assuming ETH, there exists a constant 0 such that no 2o(n)-time algorithm can
0-approximate Eq. (1).
Proof. Given a 3-regular MAX-CUT instance G, we construct an '2-regression instance (W, y)
with W ∈ {0, 1}m0×n and y ∈ {0, 1}m0 where m0 = m + cn = (1 + 3c/2)m and c = 106 as
follows.
•	For each i ∈ [m], let the i-th edge of G be ei = {u,v}. We set Wi,* to be all zeros
except the u-th and v-th coordinates being one. That is, we add a constraint |zu+zv|.
And we set yi = 0.
•	For each j ∈ [n], we set Wm+c(j-1)+1,*, . . . , Wm+cj,* to be all zero vectors except
the j-th entry being one. That is, we add c constraints of the form |zj |. And
ym+c(j-1)+1 = ∙∙∙ = ym+cj = 1.
Completeness. Let opt be the optimal value of max-cut of G and let Sopt be the optimal
subset. Then, for each u ∈ Sopt, we set zu = 1; and for u ∈/ Sopt, we set zu = -1. For the
first type constraints |zu + zv|, if u and v are cut by Sopt, then |zu + zv| = 0; otherwise
|zu + zv| = 2. For the second type constraints |zj |, all of them are satisfied by our assignment.
Thus, kWz - y k22 = 4(m - opt).
Soundness. Let η be a constant such that r < η < 1, where r is the approximation lower
bound in Theorem D.1. Let δ = 1-η. We will show that, if there exits a Z such that
kWz - yk22 ≤ δm0 , then we can recover a subset S with cut-size ηm.
It is easy to see that the optimal solution lies in [-1, 1]n. Since for z ∈/ [-1, 1]n, we can
always transform it to a new vector z0 ∈ [-1, 1]n such that kWz0 - yk2 ≤ kWz - yk2.
Suppose z ∈ {-1, 1}n is a Boolean vector. Then, we can pick S = {i ∈ [n] : zi = 1}. We
have the cut-size of S is
∣E(S,V\S)| ≥ m - δm0∕4
= m - δ(1 + 3c/2)m/4
=(1 - δ∕4 - 3cδ∕8)m
≥ ηm,
where the last step follows from δ ≤ 821-67.
For general Z ∈ [一1,1]n, we first round Z by its sign: let Zi = Sign(Zi) for i ∈ [n]. We will
show that
48
l∣Wz - yk2 TlWZ - yk2 ≤ -Cm
19
Under review as a conference paper at ICLR 2022
which implies
kWz -yk2 = IIWz- yk2 + (HWz - yk2 - kwz- yk2)
≤ δm0 H---m.
c
Then, we have the cut-size of S is
|E(S, V \S)| ≥ m - (δm0 - 48m/c)/4
=(1 — δ∕4 — 3cδ∕8 — 12∕c)m
≥ ηm,
where the last step follows from δ ≤ 8(I-+6：2/C).
Let ∆i := |zi 一 Zi| = 1 — |zi| ∈ [0,1]. We have
kWz-yk2 - kWz- yk2
mn
Σ(zui+ zvi )2 -(Zui + zvi )2 + C ∙ X(|zj |一 1)2 一(|zj |一 1)2
i=1	j=1
mn
YPui + zvi )2 一 (zui + zvi )2 一 C ∙∑(lzj ∣-1)2
i=1	j=1
mn
Σ(zui + zvi )2 一 IZui + zvi )2 一 C ∙	∆j2
i=1	j=1
mn
≤ ∑4∣∆ui + ∆ujiW∆2
i=1	j=1
n
12∆i 一 C∆i2
i=1
≤
72
——n
C
48
——m,
C
where the first step follows by the construction of W and y. The second step follows from
|zj | = 1 for all j ∈ [n]. The third step follows from the definition of ∆j. The forth step
follows from |zui + zuj | ∈ [0, 2]. The fifth step follows from the degree of the graph is 3. The
fifth step follows from the minimum of the quadratic function 12x — cx2 in [0,1] is 72. The
last step follows from m = 3n∕2.
Therefore, by the completeness and soundness of reduction, if we take ∈ (0, δ), Theorem D.1
implies that it is NP-hard to (1 + 0-approximate the '2-regression, which completes the
proof of the first part of the theorem.
For the furthermore part, we can use the same reduction for a 5-regular graph. By choosing
proper parameters (C and δ), we can use Theorem D.3 to rule out 2o(n)-time algorithm for
O(1)-factor approximation. We omit the details since they are almost the same as the first
part.	□
20