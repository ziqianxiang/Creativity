Under review as a conference paper at ICLR 2022
Near-Optimal Algorithms for Autonomous
Exploration and Muiti-Goal Stochastic
Shortest Path
Anonymous authors
Paper under double-blind review
Ab stract
We revisit the incremental autonomous exploration problem proposed by Lim and
Auer (2012). In this setting, the agent aims to learn a set of near-optimal goal-
conditioned policies to reach the L-controllable states: states that are incrementally
reachable from an initial state s0 within L steps in expectation. We introduce
three new algorithms with stronger sample complexity bounds than existing ones.
Furthermore, we also prove the first lower bound for the autonomous exploration
problem. In particular, the lower bound implies that one of our proposed algo-
rithms, Value-Aware Autonomous Exploration, is nearly minimax-optimal when
the number of L-controllable states grows polynomially with respect to L. Key
in our algorithm design is a connection between autonomous exploration and
multi-goal stochastic shortest path, a new problem that naturally generalizes the
classical stochastic shortest path problem. This new problem and its connection to
autonomous exploration can be of independent interest.
1	Introduction
Reinforcement learning (RL) with a known state space has been studied in a wide range of settings
(e.g., Schmidhuber, 1991; Oudeyer et al., 2007; Oudeyer and Kaplan, 2009; Baranes and Oudeyer,
2009). When the state space is large, it is difficult for a learning agent to discover the whole
environment. Instead, the agent can only explore a small portion of the environment. At a high level,
we hope that the agent can discover states near the initial state, expand the range of known states
by exploration, and learn near-optimal goal-conditioned policies for the known states. Because the
agent discovers its known states of the environment incrementally, this learning problem was named
Autonomous Exploration (AX) (Lim and Auer, 2012; Tarbouriech et al., 2020).
The autonomous exploration problem generalizes the Stochastic Shortest Path (SSP) problem (Bert-
sekas et al., 2000) where the agent aims to reach a predefined goal state while minimizing its total
expected cost. However, in the autonomous exploration setting, the agent aims to discover a set of
reachable states in a large environment and find the optimal policies to reach them. The autonomous
exploration formulation is applicable to an increasing number of real-world RL problems, ranging
from navigation in mazes (Devo et al., 2020) to game playing (Mnih et al., 2013). For example, in the
maze navigation problem, a robot aims to follow a predefined path in an unknown environment, and
the robot has to discover and expand the size of regions known to itself autonomously without prior
knowledge of the environment. This procedure also resembles some biological learning processes.
See Lim and Auer (2012) for more discussions.
Related Work. The setting of autonomous exploration was introduced by Lim and Auer (2012),
Who gave the first algorithm, UcbExPlore, with a sample complexity O(L3S(i+ε)LA∕ε3). Here L
denotes the distance within which we hope the learning agent to discover, S(1+ε)L denotes the total
number of states within distance (1 + ε)L from the starting state, A denotes the size of the action
space, and ε denotes the error that we can tolerate. Recent work by Tarbouriech et al. (2020) designed
the DisCo algorithm with a sample complexity bound Oe(L3S(21+ε)LA∕ε2)1, which improves the 1∕ε
1We translate their absolute error ε to the relative error εL. Their bound has a refined form
Oe(L3S(i+ε)LΓ(i+ε) LA∕ε2) where Γ(i+ε)L is the branching factor which in the worst case is S(i+ε)L-
1
Under review as a conference paper at ICLR 2022
Algorithm	Sample Complexity	AXL on S→	AX* on S→	AXL on K	AX* on K
UcbExplore (Lim and Auer, 2012)	~ .	C	. C . O(L S(i+e)la/£ )	Yes	No	Yes	No
DisCo (TarbOUrieCh et al.,2020)	0(L3S2l+ε)LA∕ε2 3)	Yes	Yes	Yes	Yes
VOISD	0(L3S(i+ε)LA∕ε2)	Yes	No	Yes	No
VOISD + Re-MG-SSP	O(L3 S(1+ε)LA/ε2')	Yes	Yes	Yes	Yes
VALAE	O(LS2LA∕ε2)	Yes	Yes	No	Yes
Lower Bound	Ω(LSlA∕e2)	Yes	Yes	Yes	Yes
Table 1: Comparisons between our results and prior results. Algorithms and results in this paper are
in grey cells. We define L, A, SL, S(1+ε)L, S2L in Sect. 2, and ε is the target accuracy. We study
four criteria, AXL on S→, AX* on S→, AXL on K, and AX* on K whose definitions are in Sect. 2.
For simplicity, for each result, we only display the leading term in terms of the scaling in 1∕ε.
dependency at the cost of a worse dependency on S(1+ε)L. In this paper, we present new algorithms
to further improve the sample complexity.
1.1	Contributions
In this paper, we take important steps toward resolving the autonomous exploration problem. We
compare our results with prior ones in Table 1.2 and we summarize our contributions below:
1.	We propose a new state discovery algorithm, Value-Optimistic Incremental State
Discovery (VOISD), which uses a value-optimistic method (Neu and Pike-Burke, 2020) to
estimate the expected cost of the optimal policy to reach these states. We prove this algorithm
enjoys an O(L3S(i+e)lA/e2) sample complexity, which improves prior results in Lim and Auer
(2012) and Tarbouriech et al. (2020).
2.	We connect the autonomous exploration problem to a new problem, multi-goal SSP and propose a
new algorithm Re-MG-SSP that 1) satisfies a stronger criterion3 and 2) serves as a burn-in step
for the next algorithm.
3.	We further propose Value-Aware Autonomous Exploration (VALAE), which uses
VOISD and Re-MG-SSP as initial steps and then uses the estimated value functions to guide
our exploration. By doing so, for each state-action pair (s, a), we derive an (s, a)-dependent
sample complexity bound, which can exploit the variance information, and yield a sharper sample
complexity bound than the bounds for VOISD and Re-MG-SSP. In particular, VALAE improves
the dependency onL from cubic to linear.
4.	We give the first lower bound of the autonomous exploration problem. This lower bound shows
VALAE is nearly minimax-optimal when the SL grows polynomially with respect toL.
1.2	Main Difficulties and Technique Overview
While our work borrows ideas from prior work on autonomous exploration (Lim and Auer, 2012;
Tarbouriech et al., 2020) and recent advances in SSP (Tarbouriech et al., 2021), we develop new
techniques to overcome additional difficulties that are unique in autonomous exploration.
Dependence between the Estimated Transition Matrix and Discovered States. Our algorithm
incrementally adds new states to the set of discovered states K. To obtain a tight dependency on
S(1+ε)L, similar to the standard RL setting, one needs to use concentration on (Ps,a - Ps,a)VK*,g
instead of kPs,a - Ps,a k1 (used by Tarbouriech et al. (2020)) where Ps,a is the estimated transition,
2In (Lim and Auer, 2012) and (Tarbouriech et al., 2020), the cost is 1 uniformly for all state-action pairs. In
this paper, we allow non-uniform costs. In Table, we consider uniform cost in Table 1 for fair comparisons.
3See Sect. 2 for different criteria.
2
Under review as a conference paper at ICLR 2022
Ps,a is the true transition, and VK,g is the value function of the optimal policy going to the state g
restricted on the discovered states K. The main challenge is that the set of discovered states K is
dependent on the samples collected, and thus VK,g is dependent on PS,a. We can use the union bound
on all the possible K, but the number of possible K is exponential in the number of states S.
Our main technique is to construct a series of sets of states {s0} = Ko ⊆ Ki ⊆ ∙∙∙ ⊆ KZ = S→,
where Kz+1 is constructed by adding all the states that are reachable from s0 by some policy on Kz
with expected cost no more than L. This series is only polynomially large so after applying the union
bound we only have a logarithmic overhead. In order to use concentrations only on this sequence of
sets, we also need to develop a modified definition of optimism. See Appendix B and Appendix C.2
for details.
Connection between Autonomous Exploration and Multi-Goal SSP. In standard RL setting, it is
known that in order to obtain a tight dependency on L, one needs to exploit the variance information
in the value function (Azar et al., 2017). However, in autonomous exploration, it is unclear how to
exploit the variance information because even which state is in SL→ is unknown.
To this end, we first consider a simpler problem, multi-goal SSP, and extend the technique for single-
goal SSP (Tarbouriech et al., 2021) to this new problem (cf. Alg. 3). We also present a reduction
from autonomous exploration to multi-goal SSP (cf. Alg. 2). These two techniques together yield the
first tight dependency on L for autonomous exploration.
2	Preliminaries
In this section, we introduce basic definitions and our problem setup.
Notations. For any two vectors X, Y ∈ RS, we write their inner product as XY := Ps∈S X(s)Y (s).
We denote kXk∞ := maxs∈S |X (S)|, and if X is a probability distribution on S, we define
V(X,Y) := Ps∈s X(S)Y(S)2 - (Ps∈s X(S)Y(s))2.
Markov Decision Process. We consider an MDP M := hS, A, P, c, s0i, where S is the state space
with size S, A is the action space with size A, and S0 ∈ S is the initial state. In state S, taking action
a has a cost drawn i.i.d. from a distribution on [cmin, 1] (where cmin > 0) with expectation c(S, a), and
transits to the next state S0 with probability P(S0|S, a). For convenience, we use Ps,a and Ps,a,s0 to
denote P(∙∣s, a) and P(s0∣s, a), respectively. A deterministic and stationary policy π : S → A is a
mapping, and the agent following the policy π will take action π(S) at state S.
For a fixed state g ∈ S we define the random variable tgπ(S) as the number of steps it takes to reach
state g starting from state S when executing policy π, i.e. tgπ(S) := inf{t ≥ 0 : St+1 = g | S1 = S, π}.
A policy π is a proper policy if for any state S ∈ S, tgπ (S) < +∞ with probability 1. Then we
define the value function of a proper policy π with respect to the goal state g and its corresponding
Q-function as follows:
tgπ(s)
Vgπ (S) = E X ct (St, π(St)) | S1 = S
t=1
tgπ(s)
, Qgπ(S, a) = E	ct(St, π(St)) | S1 = S, π(S1) = a
t=1
where ct ∈ [cmin, 1] is the instantaneous cost at step t incurred by the state-action pair (St, π(St)),
and the expectation is taken over the random sequence of states generated by executing π starting
from state S ∈ S. Here we have Vgπ(g) = 0. We use πQ to denote the greedy policy over a vector
Q ∈ RS×A, i.e. πQ(S) := arg min Q(S, a).
a∈A
For a fixed state g ∈ S, we denote Vg= as the value function of the optimal policy with respect to
goal state g, and here we list some important properties of Vg=: there exists a stationary, deterministic
and proper policy ∏*, such that its value function V= := Vgr* and its corresponding Q-function
*
Qg= := Qgπ satisfies the following Bellman optimality equations (cf. Lem. 1):
Qg= (S, a) = c(S, a) + Ps,aVg= ,
Vg=(S) = min Q=g (S, a),	∀(S, a) ∈ S × A.
Autonomous Exploration. Now we introduce the Autonomous Exploration problem. To formally
discuss the setting, we need the following assumption on our MDP M .
3
Under review as a conference paper at ICLR 2022
Assumption 1. The action space contains a RESET action s.t. P(s0|s, RESET) = 1 for any s ∈ S.
Moreover, taking RESET in any state s will incur a cost cRESET with probability 1, where cRESET is a
constant in [cmin, 1].
Given any fixed length L ≥ 1, the agent needs to learn the set of incrementally controllable states
SL→. To introduce the concept of SL→, we first give the definition of policies restricted on a subset:
Definition 1 (Policy restricted on a subset). For any S0 ⊆ S, a policy π is restricted on the set S0 if
π(s) = RESET for all s ∈/ S0.
Now we discuss the optimal policy restricted on a set of states K ⊆ S with respect to goal state g.
We denote VK,g ∈ RS as the value function of the optimal policy restricted on K with goal g, and
QK,g as the Q-function corresponding to VK,g. We consider the case that there exists at least one
proper policy restricted on K with the goal state g. Then, VK g and QKg are finite, and they satisfy
the following Bellman equations:
QK,g(s, a) = C(S, a) + Ps,aVK,g,	∀(S, a) ∈ S ×A,
VK,g(S) = m∈iAQK,g(s,a),	∀s ∈K,s = g,
VK,g(s) = QK,g(s, RESET) = CRESET + VK,g(so),	∀s ∈ K ∪ {g},
VK,g(g) = 0.
We note that when Ki ⊆ K2, for any g ∈ S, if VKl g is finite, then VK? g is also finite, and we have
VK2,g ≤ VKl ,g component-wise. And we note that for any S = g, we have min QK,g (s,a) ≤ VK,g (s)∙
Now we introduce the definition of incrementally controllable states SL→ (see Tarbouriech et al. (2020)
for more intuitions on this definition.):
Definition 2 (Incrementally L-controllable states S→). Let Y be any partial order on S. We denote
SL as the set ofstates reachablefrom s° with expected cost no more than L w.rt. Y, which is defined
as follows:
•	S0 ∈ SLL ,
•	if there is a policy π restricted on {S0 ∈ SLL : S0 Y S} such that Vsπ (S0) ≤ L, then S ∈ SLL.
The set of incrementally L-controllable states SL→ is given by SL→ =	SLL .
L
Learning Objective. In our settings, the learning agent knows the constants S, A and Cmin, but the
learning agent has no prior knowledge of the transition probability P or the cost function c(∙, ∙) of
the MDP M . We fix the length L ≥ 1 and an error parameter ε ∈ (0, 1]. A learning algorithm of the
autonomous exploration problem should output a set of states K ⊆ S that satisfies:
•	SL→ ⊆ K, i.e., the algorithm discovers all the states that we want to explore.
The algorithm also outputs a set of policies {πs}s∈K that satisfy one of the following criteria:
1.	(AXL on SL→) ∀S ∈ SL→, Vsπs (S0) ≤ (1 + ε)L;
2.	(AX* on S→) ∀s ∈S→,Vπs(so) ≤ VS→,s(s0)+ εL;
3.	(AXL onK)∀S ∈ K, Vsπs (S0) ≤ (1 + ε)L;
4.	(AX* onK)∀S ∈ K, Vsπs (S0) ≤ VK*,s(S0) +εL.
We note that AX* on K is stronger than both AXL on SL→ and AX* on SL→ , but it is not necessarily
stronger than AXL on K, because we do not necessarily have VK*,s (S0) ≤ L when S ∈/ SL→ . In the
literature, AXL on SL→ was studied in the original paper that proposed the autonomous exploration
problem (Lim and Auer, 2012) and AX* on K condition was studied in (Tarbouriech et al., 2020).
We denote T as the total number of steps the agent uses, and denote (St, at) as the state-action pair at
the t-th step. We denote by Ct(St, at) the instantaneous cost incurred at the t-th step. The performance
T
of an algorithm is measured by the cumulative cost: CT := P Ct (St, at).
t=1
Multi-goal SSP. We also study a new problem, multi-goal SSP, a natural generalization of the
classical SSP problem. In multi-goal SSP, we consider an MDP and a fixed length L ≥ 1. The MDP
M satisfies Asmp. 1, and all of its states are incrementally L-controllable, i.e. SL→ = S.
4
Under review as a conference paper at ICLR 2022
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
Algorithm 1: Value-Optimistic Incremental State Discovery (VOISD)
Input: MDP M = hS, A, P, c, s0i, confidence δ ∈ (0, 1), error parameter ε ∈ (0, 1], and L ≥ 1.
Initialize U — {}, K — {}, and Snew J so. Specify constants ci = 6, c2 = 72, c3 = 2vz2, c4 = 2vz2.
Set ε — ε∕3, δ — δ∕2, B — 10L and ∏s° ∈ Π({so}).
Set ψ — 12000(cLε)2 3 * ln(SA), and φ — 2dlog2 ψe.
For (s, a, s0) ∈ S × A × S, set
N(s, a) — 0; n(s, a) — 0; N(s, a, s0) — 0; Pbs,a,s0 — 0; θ(s, a) — 0; bc(s, a) — 0.
for round r = 1, 2,…do
\\(a) Discover Possible States in SL→
Add snew toK. Sets — snew.
for each a ∈ A do
while N(s, a) < φ do
Execute policy πs on MDP M until reaching state s.
Take action a, incur cost c and observe next state s0 〜 P(∙ | S,a).
Set N(s, a) — N(s, a) + 1, θ(s, a) — θ(s, a) + c, N(s, a, s0) — N(s, a, s0) + 1.
If s0 ∈ K, add s0 to U.
Set b(s, a) — N；Sa) and θ(s, a) — 0.
For all s0 ∈ S, set 2,…，—N(s, a, s0)∕N(s, a), n(s, a) — N(s, a).
Stop the algorithm if U is empty.
\\(b) Compute Optimistic Policy
For each g ∈ U, compute (Qg, Vg) := VISGO(S, A, K, so,g, Cminε).
Choose a state S ∈ arg min Vg (S0). Stop the algorithm if Vs (S0) > L.
g∈U
Set the policy ∏ as the greedy policy over Qs. Remove S from U, set Snew — S and set ∏ — ∏.
Output: The discovered states K and their corresponding policies {πs }s∈K.
In multi-goal SSP, a learning algorithm should output a set of policies {πs}s∈S, such that Vsπs (s0) ≤
V* (so) + εL for all S ∈ S. We observe that an algorithm that solves the autonomous exploration
problem with AX* on S→ criterion can also solve the multi-goal SSP problem.
3 Algorithms and Sample Complexity Bounds
Now we are ready to describe our algorithms. There are three key components. The first component
aims to discover a set of states K which is a superset of SL→ but it is also not too much larger than
SL→ (cf. Alg. 1), and compute a set of policies {πs}s∈K to reach each state with a small cost. The
second component reduces the autonomous exploration problem to multi-goal SSP using the set K
computed from the first component (cf. Alg. 2). Then it collects fresh samples for all state-action
pairs and computes a set of near-optimal policies {πs }s∈K on the set K. In the third component,
inspired by recent advances in stochastic shortest path Tarbouriech et al. (2021), we design a policy
evaluation step to obtain near-optimal estimates of the costs of getting to each s ∈ SL→ (cf. Alg. 3).
3.1 State Discovery via a Value-Optimistic Method
In Alg. 1, we compute a set of states K that contains SL→ . The algorithm maintains a set of "known"
states K and a set of "unknown" states U that have the potential to be recognized as members of SL→
and become "known" states in future. Initially, the set of known states is empty, and s0 is the first
state that will be added to K. Our algorithm will discover all the states s ∈ SL→ one by one, and when
the algorithm finds a policy that can reach s from s0 with expected cost less than L, the algorithm
adds s to the set of known states K. Now we discuss this process in detail.
In each round, we have two phases. The first phase is called state discovery phase, and it borrows
the idea from (Tarbouriech et al., 2020) and (Lim and Auer, 2012). We add a new state s to the set
of known states K. Then for each action a, we sample φ times, compute the empirical probability
Ps,a,s0, and use the empirical cb(s, a). The collection of these samples has two objectives. First, it
will discover new states because it adds to U all the states that have not been discovered. Second,
the collection of these samples helps us estimate the transition model Ps,a,s0 , which is crucial in
estimating the value function of optimal policies.
5
Under review as a conference paper at ICLR 2022
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Algorithm 2: Reduce Autonomous Exploration to Multi-Goal SSP (Re-MG-SSP)
Input: MDP M = hS, A, P, c, s0i, confidence δ ∈ (0, 1), error parameter ε ∈ (0, 1], and L ≥ 1.
Input: The states s in K and their corresponding policy πs .
For (s, a, s0 ) ∈ S × A × S, set
N(s,a) — 0; n(s,a) — 0; N(s,a,s0) — 0; Pg,a,s0—0; θ(s,a) — 0; b(s,a) — 0.
Set ε 一 ε∕3,δ 一 δ∕2,ψ — 12000(高)勺n(SA),and Φ — 2dlog2 ψe.
Set Kt = K∪{x} where X is defined in and construct an MDP Mt = hK*, A, P*,c*, s0 where x, K,
Pt, Ct are defined in Sect. 3.2.
for each (s, a) ∈ K × A do
while N(s, a) < φ do
Execute policy πs on MDP Mt until reaching state s.
Take action a, incur cost C and observe next state s0 〜 Pt(∙ | s,a).
Set N(s, a) - N(s, a) + 1, θ(s, a) — θ(s, a) + c, N(s, a, s0) — N(s, a, s0) + 1.
Set b(s, a) J Ns,a； and θ(s, a) J 0.
For all s0 ∈ Kt, set Ps,a,s0 J N(s, a, s0)∕N(s, a), n(s, a) J N(s, a).
0
For all a ∈ A, set N(x, a) = n(x, a) J φ, Cb(x, a) J CRESET, Px,a,s0 J 1, Px,a,s0 J 0 for all s0 ∈ S.
For g ∈ K, compute (Qg, Vg) := VISGO(Kt, A, Kt, so,g, Cmnε), and set ∏g as the greedy policy over Qg.
Output: K, {πs}s∈K, N(),n(), Pb, θ(), bC, Mt.
In the second phase, we use the estimated model Pb to estimate the value function of the optimal
policy with respect to all the goal states g ∈ U , and we denote the estimated value as Vg . We can
prove that Vg satisfies the optimism property. Hence, if g ∈ SL→, g will be eventually added to the
set of known states K, and we can obtain K ⊇ SL→ when the algorithm terminates. To this purpose,
we use a procedure called VISGO (Value Iteration with Slight Goal Optimism), which is a modified
version of the VISGO procedure in (Tarbouriech et al., 2021).
In our VISGO procedure, we use slightly different bonus function b(U, s, a). The VISGO procedure in
(Tarbouriech et al., 2021) computes the optimistic policy on the whole set S, and here we make VISGO
estimate the optimistic policy restricted on the set K. As we have collected φ = O(L2∕(cminε)2)
samples for each state-action pair (s, a) ∈ K × A, we can ensure that the estimated model Pb is close
enough to P, and in VISGO procedure, the bonus function b(U, s, a) is bounded by O(cminε), and
this is essential for our proof of K ⊆ S(→1+ε)L. The full algorithm of VISGO is deferred to Appendix
C. Now we present our upper bound of the Alg.1.
Theorem 1. Assume that L ≥ 1, 0 < ε ≤ 1 and 0 < δ < 1. For any MDP M = hS, A, P, c, s0i
satisfying Asmp. 1, with probability at least 1 - δ, our Alg. 1 will terminate and output a set of states K
such that SL→ ⊆ K ⊆ S(→1+ε)L, anda set of policies {πs}s∈K such that ∀s ∈ K, Vsπs (s0) ≤ (1 + ε)L,
and the cumulative cost CT satisfies CT = O( L s21+ε)LA).
cminε
Thm. 1 shows that our Alg. 1 meets the AXL on K criterion. When cmin = 1, which is the setting
considered in prior works (Lim and Auer, 2012; Tarbouriech et al., 2020), our bound is stronger than
previous ones either in L or 1∕ε (cf. Table 1). We remark that our algorithm VOISD follows the same
template as the DisCo algorithm in (Tarbouriech et al., 2020), which also consists of a state-discovery
phase and a compute-optimistic-policy phase. However, our sample complexity saves an S(1+ε)L
factor compared with (Tarbouriech et al., 2020), and the main reasons lie in the difference of the
methods of computing optimistic policies. The DisCo algorithm applies a model-optimistic method,
which will incur an additional S(1+ε)L factor in the bonus term bs,a. Our algorithm VOISD is based
on the value-optimistic subroutine VISGO. It uses the concentration of (PS,a - PS,a)V2 g on a
series of sets Kz and saves an S(1+ε)L term on the sample complexity.
3.2 Connection between Autonomous Exploration and Multi- Goal SSP
In Alg. 2, after we compute a set of known states K ⊇ SL→, we have discovered all the states that we
want to explore. Alg. 2 aims to compute a set of policies {∏s}s∈κ that meets the AX* on K criterion.
We will fix our set of known states K, and focus only on the policies restricted on K. Therefore, for
all the states s ∈∕ K, we can regard them as one artificial state x, and the only action at state x is
6
Under review as a conference paper at ICLR 2022
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
Algorithm 3: Value-Aware Autonomous Exploration (VALAE)
Input: MDP M = hS, A, P, c, s0i, confidence δ ∈ (0, 1), error parameter ε ∈ (0, 1], and L ≥ 1.
Specify: Trigger set N — {2j-1 : j = 1,2,...}. Constants ci = 6, c2 = 72, c3 = 2√2, c4 = 2√2.
Set δ — O(δε∕SAL).
\\We run Alg. 1 with ε = 1 and get a set K such that SL→ ⊆ K ⊆ S2→L .
Run Alg. 1 With input (M,δ∕2,1, L) and We get a set K and a set of policies {∏s }s∈κ.
Run Alg. 2 with input (M,δ/2,1, L, K, {∏s}s∈κ), and We obtain the MDP M* = hK*, A, P*, c*, s0 and
the variables N (), n(), Pb, θ(), bc.
Set initial time step t — 1 and trigger index j - 5 + log2	.
Set e 一 ε, B — 10L, λ =「2048 ln2(2∣6) ln(2δS)], and initialize U - K, and g — so.
for round r = 1, 2,…do
\\(a) Compute Optimistic Policy
Compute (Q, V ) := VISGO(K*, A,K*,s0,g,2-j/(|K*|A)).
Set the policy ∏ as the greedy policy over Q.
\\(b) Policy Evaluation
Set T . 0.
for episode k = 1, 2,…，λ do
Set st — so and reset to the initial state so. Set Tk → 0.
while st 6= g do
Take action at = arg mina∈A Q(st, a) on MDP M*, incur cost ct and observe next state
st+i 〜P*(∙ | st , at).
Set (s, a, s0, c) - (st , at , st+1 , ct) and t - t + 1.
Set N(s, a) - N(s, a) + 1, θ(s, a) - θ(s, a) + c, N(s, a, s0) - N(s, a, s0) + 1.
ifN(s, a) ∈ N then
Set j - j + 1. Set b(s, a) — [(：,：) and θ(s, a) - 0.
For all s0 ∈ K*, set Pbs,a,s0 - N (s, a, s0)/N (s, a), n(s, a) - N (s, a).
Return to line 9, start a new round (the current round has been a skipped round).
_ Set T — T+ λ, Tk - Tk + c.
if t > V (so) + eL then
L Return to line 9, start a new round (the current round has been a failure round).
Set ∏g — π. Remove g from U. The current round has been a success round.
Choose another state g ∈ U. Stop the algorithm if U is empty.
Output: The states s in K and their corresponding policy πs .
RESET. To this purpose, we will construct an MDP M* :=(K*, A, Pt, c*, s。)where we first define
the artificial state x, and we set K* = K ∪ {χ}, and K0 = |K*|. For any (s, a) ∈ K×A, we define
P* 0 as follows:
s,a,s
Ps*,a,s0 = Ps,a,s0, ∀s0∈K, and Ps*,a,x = XPs,a,s0.
s0∈K
We also define Px*,a,s0 = I[s0 = s0] for any a ∈ A, s0 ∈ K∪{x}. Finally, we define c*(s, a) = c(s, a)
for all (s, a) ∈ S × A, and c*(x, a) = cRESET for all a ∈ A. In this way, the problem reduces to
solving a multi-goal SSP problem on M* with the set of states being K*.
Next, we collect φ fresh samples for each state-action pair (s, a) again, and we will use these samples
to compute policies {∏s}s∈κ that satisfy the AX* on K criterion. We remark that using fresh samples
is essential for Alg. 2 to ensure these samples are independent of K. The following theorem states the
sample complexity for Alg. 2.
Theorem 2. Assume that L ≥ 1, 0 < ε ≤ 1 and 0 < δ < 1. For any MDP M = hS, A, P, c, s0i
satisfying Asmp. 1, running Alg. 1 and Alg. 2 successively will output a set of states K such that
with probability at least 1 - δ, SL→ ⊆ K ⊆ S(→1+ε)L, and a set of policies {πs }s∈K such that
Vs ∈ K,Vπs(s0) ≤ (1+ε)L and Vns(S0) ≤ VK,s(s0)+εL, andCT satisfies CT = O(%1+?LA).
7
Under review as a conference paper at ICLR 2022
3.3 Value-Aware Algorithms for Autonomous Exploration and Multi-Goal SSP
Finally we describe our final algorithm, Value-Aware Autonomous
Exploration (VALAE). First, VALAE uses Alg. 1 with ε = 1 as a subroutine, and Alg. 1
computes a set K such that SL→ ⊆ K. Then we discard all the samples collected in Alg. 1, in order to
ensure the independence of K and P$,a. Second, We use Alg. 2asa burn-in step to collect Ω(L2/Cmin)
samples for each of the state-action pair (s, a) so that we can use Lem. 15 to bound the expected cost
of the greedy policy ∏ in the policy-evaluation phase, and use Lem. 16 to show that we can use the
average cost in O(1∕e2) episodes to estimate the expected cost of the policy ∏.
From now on we work on the MDP Mt, and we will solve the multi-goal SSP problem on M* and
compute near-optimal policies πg for all the goal states g ∈ K. We choose the goal state g ∈ K one
by one, and we move to another goal state g if the average performance of the policy πg is close to
our estimation of the optimal policy. In each round, we have two phases. In the first phase, we use
VISGO to estimate the value function of the optimal policy with goal state g, and we set the policy
∏ as the greedy policy over Q. Since we do not know whether the policy ∏ is close enough to the
optimal policy, in the second phase, we will execute ∏ for λ = O(1∕e2) times and check whether the
average performance is close enough to our estimation of the optimal policy. In this process, we also
collect samples, and use them to help us estimate the value function of the optimal policy.
In the second phase, the current round will be classified into three cases: failure round, skipped round,
and success round. This borrows the idea from (Lim and Auer, 2012). If the average performance
of the policy ∏ is too bad, we will consider the current round as a failure round. If the number of
samples N(s, a) meets the trigger set (i.e. is a power of 2), we will consider the current round as a
skipped round, following the idea in (Jaksch et al., 2010). Otherwise, the current round is a success
round. In the case of a failure round or a skipped round, we will not change the goal state g, and in
the next round, we compute a new policy by VISGO using the samples collected in this round. In the
case of a success round, as the average performance of the policy ∏ is close to optimal, we can set the
∏ as the policy ∏g for the goal state g, and choose another goal state g.
Theorem 3. Assume that L ≥ 1, 0 < ε ≤ 1 and 0 < δ < 1. For any MDP M = hS, A, P, c, s0i
satisfying Asmp. 1, we have that with probability at least 1 - δ, our Alg. 3 will terminate and output a
set of states K such that SL→ ⊆ K ⊆ S2→L, anda set of policies {πs}s∈K such that ∀s ∈ K, Vsπs (s0) ≤
VK s(so) + εL, and the cumulative cost CT satisfies CT = O( LS2LA + LS；A + LLS/A).
,	ε	ε	cmin
Thm.3 shows that Alg.3 satisfies the AX* on K criterion. Note that in Thm. 3, the dependency on L
is tight when ε → 0, because we leverage the variance information in the policy-evaluation phase,
which is necessary in RL problems generally. Alg. 1, Alg. 2 and algorithms in prior work cannot
use the variance information because they collect equal number of samples on each state-action pair
(s, a), thus the sample collection does not use the estimated value function as the guidance.
We highlight that the leading term of CT does not have cmin . This is because the variance fundamen-
tally does not scale with cmin (cf. Lem. 17 and Lem. 18). While we discover a larger set K ⊆ S2→L
compared with Thm. 1 and Thm. 2, we note that if the number of the L-controllable states grows poly-
nomially with respect to L (which is often implicitly considered in the literature because otherwise
one may need to consider the logarithmic dependency on ∣S→∣), then we have |5五| = O(∣S→∣),
and our complexity strictly improves the existing ones and is nearly minimax optimal.
Lastly, we note that Alg. 3 enjoys a near-optimal sample complexity for multi-goal SSP:
Theorem 4. (Cumulative Cost for Multi-Goal SSP) Assume that L ≥ 1, 0 < ε ≤ 1 and 0 < δ < 1.
For any MDP M = hS, A, P, c, s0i satisfying Asmp. 1 and SL→ = S, with probability at least
1 - δ, our Alg. 3 will terminate and output a set of policies {πs}s∈S such that ∀s ∈ S, Vsπs (s0) ≤
V* (so) + εL, and the cumulative cost CT satisfies CT = O(LSA + LS2A + L3SA).
ε	ε	cmin
4 A Minimax Lower B ound for Autonomous Exploration
Here we present our lower bound for the autonomous exploration problem. To give the lower bound
formally, we introduce some basic concepts about the definition of a learning algorithm, following
the definitions in Domingues et al. (2021). We define It = (S × A)t-1 × S be the set of all possible
8
State
Action 1
Action 2
Figure 1: A graphical illustration of our construction of hard MDPs.
histories up to t steps with the form (s1,a1, s2, a2,..., St) ∈ It. We denote π，(∏t)t≥ι as a
history-dependent policy, where ∏t : It → ∆(A) describes the probability of taking action a ∈ A
after observing some history it ∈ It at step t. We consider a class of MDPs M = hS, A, P, c, s0i
such that Asmp. 1 holds, and we only consider the AXL on SL→ criterion (cf. Sect. 2). and we
note that |S | = S and |A| = A. Then we define an algorithm for the AX problem as a tuple
(π, τ, K, {πs }s∈K ), which means the algorithm executes a history-dependent policy π, and returns a
set K and a set of policies {πs}s∈K after sampling τ times.
Definition 3. Given any L ≥ 1, 0 < ε ≤ 1, 0 < δ < 1, and SL ∈ N, an algorithm
(π, τ, K, {πs }s∈K ) is (ε, δ)-PAC for the AX problem within cumulative cost C, if for any MDP
M = hS, A, P, c, s0i with at most S states and A actions such that M satisfies Asmp. 1 and
∣S→∣ ≤ SL, the algorithm always terminates after using T samples, the cumulative cost is always
not larger than C, and with probability at least 1 - δ, it returns a set K ⊇ SL→ and a set of policies
{πs}s∈K, such that ∀s ∈ SL→, Vsπs (s0) ≤ (1 + ε)L. The cumulative cost CAX(L, S, ε, δ) for the
AX problem is the least C ∈ R satisfying that there exists an algorithm (π, τ, K, {πs}s∈K) within
cumulative cost C which is (ε, δ)-PAC for AX.
The following theorem states the lower bound for the autonomous exploration problem.
Theorem 5. Assume that L > 4, A > 4, 4 ≤ SL ≤ min{( A )b L2C, S}, 0 < ε < 4 ,and δ < 专.
Then we have Cax(L, S, ε, δ) = Ω( LSL A log 1).
We note that this lower bound is for the weakest criterion AxL on SL→, so it implies lower bounds for
the other three criteria (AX* on S→, AXL on K, and AX* on K). This lower bound further implies
our upper bound (Theorem 3) is nearly minimax-optimal when SL and S2L are of the same order.
We also have a lower bound for multi-goal SSP (cf. Appendix H).
For the proof, we first observe that even for a problem two states (an initial state s0 and a goal state
g), to determine whether g is in S→, one needs Ω (LA∕ε2) samples. Next, we construct an MDP
M00 with SL - 1 states and A/2 actions, so that each state s of M00 can be reached from the initial
state s0 with less than L/2 steps. Then for each state s in M00, we add a new state gs, and each of the
remaining A/2 actions of s either transits to state gs or returns to state s. By setting the p(gs|s, a)
to ((1 + 3ε)L - VM* 0 ,s(s0))-1 , we can ensure that the expected cost to reach all the states gs from
s0 equals to (1 + 3ε)L. Then for each state s in M00, we slightly increase the probability of some
action a to reach gs , making the expected cost to reach the state gs from s0 become L. Under this
construction, we can prove the Ω(LSlA∕s2) lower bound. A graphical illustration is in Fig. 1 and
the full proof is deferred to Appendix G.
5 Conclusion
We introduced new algorithms for the autonomous exploration problems, which improve existing
ones. Along the way, we also introduced a new problem, multi-goal SSP problem, which can be of
independent interest. The natural future directions include designing an algorithm with O( LSL A)
sample complexity that satisfies all the four criteria listed in Sect. 2 and improving the lower order
terms in existing bounds.
9
Under review as a conference paper at ICLR 2022
References
Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforce-
ment learning. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pages 263-272. JMLR. org, 2017.
Adrien Baranes and Pierre-Yves Oudeyer. R-iac: Robust intrinsically motivated exploration and
active learning. IEEE Transactions on Autonomous Mental Development, 1(3):155-169, 2009.
Dimitri P Bertsekas and John N Tsitsiklis. An analysis of stochastic shortest path problems. Mathe-
matics of Operations Research, 16(3):580-595, 1991.
Dimitri P Bertsekas et al. Dynamic programming and optimal control: Vol. 1. Athena scientific
Belmont, 2000.
Alon Cohen, Haim Kaplan, Yishay Mansour, and Aviv Rosenberg. Near-optimal regret bounds for
stochastic shortest path. arXiv preprint arXiv:2002.09869, 2020.
Alessandro Devo, Gabriele Costante, and Paolo Valigi. Deep reinforcement learning for instruction
following visual navigation in 3d maze-like environments. IEEE Robotics and Automation Letters,
5(2):1175-1182, 2020.
Omar Darwiche Domingues, Pierre Menard, Emilie Kaufmann, and Michal Valko. Episodic rein-
forcement learning in finite mdps: Minimax lower bounds revisited. In Algorithmic Learning
Theory, pages 578-598. PMLR, 2021.
Aurelien Garivier, Pierre Menard, and Gilles Stoltz. Explore first, exploit next: The true shape of
regret in bandit problems. Mathematics of Operations Research, 44(2):377-399, 2019.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. Journal of Machine Learning Research, 11(Apr):1563-1600, 2010.
Shiau Hong Lim and Peter Auer. Autonomous exploration for navigating in mdps. In Conference on
Learning Theory, pages 40-1. JMLR Workshop and Conference Proceedings, 2012.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Gergely Neu and Ciara Pike-Burke. A unifying view of optimism in episodic reinforcement learning.
arXiv preprint arXiv:2007.01891, 2020.
Pierre-Yves Oudeyer and Frederic Kaplan. What is intrinsic motivation? a typology of computational
approaches. Frontiers in neurorobotics, 1:6, 2009.
Pierre-Yves Oudeyer, Frdric Kaplan, and Verena V Hafner. Intrinsic motivation systems for au-
tonomous mental development. IEEE transactions on evolutionary computation, 11(2):265-286,
2007.
Jurgen Schmidhuber. A possibility for implementing curiosity and boredom in model-building neural
controllers. In Proc. of the international conference on simulation of adaptive behavior: From
animals to animats, pages 222-227, 1991.
Jean Tarbouriech, Matteo Pirotta, Michal Valko, and Alessandro Lazaric. Improved sample complexity
for incremental autonomous exploration in mdps. arXiv preprint arXiv:2012.14755, 2020.
Jean Tarbouriech, Runlong Zhou, Simon S Du, Matteo Pirotta, Michal Valko, and Alessandro Lazaric.
Stochastic shortest path: Minimax, parameter-free and towards horizon-free regret. arXiv preprint
arXiv:2104.11186, 2021.
Huizhen Yu and Dimitri P Bertsekas. On boundedness of q-learning iterates for stochastic shortest
path problems. Mathematics of Operations Research, 38(2):209-227, 2013.
10
Under review as a conference paper at ICLR 2022
A Basic Property of the Optimal Policy
Lemma 1 (Bertsekas and Tsitsiklis, 1991;Yu and Bertsekas, 2013). Suppose that there exists a
proper policy with respect to the goal state g and that for every improper policy π0 there exists at least
one state S ∈ S such that Vgπ0 (s) = +∞. Then the optimal policy π* is stationary, deterministic,
-	_ _	-__________*	-	_	_-	-	-
and proper. Moreover, Vg= = Vgr is the unique solution of the optimality equations Vg = LVg
and V^(s) < +∞ for any S ∈ S, wherefor any vector V ∈ RS the optimal Bellman operator L is
defined as
LV (S) := min
a∈A
c(S, a) + Ps,aV	.
(1)
*
Furthermore, the optimal Q-value, denoted by Qg= = Qgπ , is related to the optimal value function as
follows
Qg=(S,a) = c(S, a) + Ps,aVg=,	Vg=(S) =minQg=(S,a),	∀(S, a) ∈S ×A.	(2)
B High-Probability Event
We focus on the samples collected in Alg. 1, and we will define the high-probability event G to do con-
centration on the variables P and bin Alg. 1. We construct a series of sets of states Ko, Ki, ∙∙∙ , KZ
in the following way.
Definition 4 (A series of sets Ko, Ki,…，KZ). The sets Ko, Ki,…，KZ are defined as follows:
•	Set Ko J {so}, and Z J 0.
•	(*) Set Kz+ι J KZ. For any S ∈ Kz+i, ifthere is a policy π restricted on KZ such that
Vsπ (So) ≤ L, then add S to KZ+i.
•	Stop ifKZ+i = KZ. Otherwise set Z J Z + 1 and return to step (*).
We note that under this definition, We have Ko ⊆ Ki ⊆ K ⊆ … ⊆ KZ = S→. And each Kz
depends only on our MDP M, and is independent of the samples collected in our algorithm.
We define the set F := {(Kz,g) : g ∈ S,z = 0,1, 2,…，Z, and VKN,g(so) ≤ L}, and we
note that |F | ≤ S2. Also, we note that as we set B = 10L in our algorithm, we can obtain
B= :
max
K,g∈F
kVK=,g k∞ ≤ B. Then we introduce the high-probability event G.
Definition 5 (High-probability event G). We define the event G := Gi ∩ G2 ∩ G3, where
Gi :
∀(S, a) ∈ S × A, ∀n(S, a) ≥ 1
2bc(S, a)ιs,a	28ιs,a
Ic(S, a) - c(S,到 ≤ 2M ms,。) + 3n(7-0y∣,
G2 :
∀(S, a, S0) ∈ S × A × S, ∀n(S, a) ≥ 1
b	2Ps,a,s0 ιs,a	ιs,a
IP…-P…| ≤ N n(S,a)	+ n^ ∫,
G3 := ∀(S, a) ∈ S × A, ∀n(S, a) ≥ 1, ∀(K, g) ∈ F : |(Pbs,a
P IV *12∣ N(Ps,a,VK,g 底a J4B*∣s,a ]
-Ps，a)VK，g i ≤ 2V	n^a+ τn^ 卜
where	:=4ln(24SA[n(s,a)]).
Lemma 2. It holds that P(G) ≥ 1 一 δ∕4.
The lemma above is a direct consequence of the concentration inequalities.
Lemma 3 (Cohen et al. (2020), Lem. B.5). Let π be a proper policy such that for some d > 0, the
expected cost Vgπ (S) ≤ dfor every non-goal state S 6= g. Then the probability that the cumulative
cost of π to reach the goal state from any state s is more than m, is at most 2e-m/(4d) for all m ≥ 0.
11
Under review as a conference paper at ICLR 2022
Lemma 4. Let τ be a random variable on [0, +∞) such that Pr(τ > m) ≤ 2e-m/4d for any m ≥ 0,
n
where d > 0 is a constant. We define the random variable T= n ETk, where each Tk has the same
k=1
2
distribution with τ. Thenfor any e > 0, we have Pr(E(T) > τ + Ed) ≤ exp(- ^8 hn25(64∕e)).
Proof. We set the constant Γ = b8d ln(64/)c. Then we define the random variables TΓ = min(T, Γ),
n
Tk = min(Tk, Γ), and T = n P Tk.
k=1
As each Tk is a random variable on [0, Γ], by Hoeffding's inequality, We have
1	nE2d2	nE2
Pr(E(Tr) > t+ 2W ≤ exp(-行) ≤ exp(-i28in2(64∕e/.
Moreover, We have
∞
E(T) ≤ E(Tr) + ^X i ∙ Pr(Γ + i — 1 < t ≤ Γ + i)
i=1
∞
= E(Tr ) + X Pr(T > m)
m=r
∞
≤ E(Tr ) + 2 X exp(-m∕4d)
m=r
≤ E(Tr) + 2Ed.
Therefore, We obtain
1	nE2
Pr(E(T) > t + Ed) ≤ Pr(E(T) >t + Ed) ≤ Pr(E(Tr) > t+ χed) ≤ exP(- IDq] 2%∕ / 、).
2	128 ln (64∕E)
□
C Analysis of a VISGO Procedure
In this section, We fix the knoWn states K and the goal state g and We analysis an execution of the
VISGO procedure in Alg. 4. We use the value iteration of the form V (i+1) = LeV (i) to estimate the
value funtion of the optimal policy. Here, We define the operator L in the folloWing Way. For any
U ∈ RS such that U ≥ 0, U(g) = 0, and kUk∞ ≤ B, We first define
LeU (s, a) := cb(s, a) + Pes,aU - b(U, s, a),
for any (s, a) ∈ K × A, Where We define
ZTT 、	S	SW(Ps,a, U)ιs,a	Bιs,α ∖	/b(s, a)ιs,a
b(U,s,a)=max< c1ι∕---------—rɪ, 6 (	、〉+。3[/ -7—C
n(s, a) n(s, a)	n(s, a)
for any (s, a) ∈K×A, and we define the transition probability Ρs,a,so =段：,3 Ps,a,so + &=+1
that slightly increases the probability to reach the goal g at each state-action pair.
Then, we set LU (s) := mina∈A LU (s, a) for s∈ K and s 6= g, and we set LU (s) := cRESET+U(s0)
for s∈ S \ (K ∪ {g}). Finally, we set LeU (g) := 0.
We note that in all of Alg. 1, Alg. 2, and Alg. 3, before we executed VISGO procedure, we have
collected φ = Oe(L2∕(c2minε2)) samples for each state-action pair (s, a) ∈ K × A. Thus we have
n(s, a) ≥ φ for each (s, a) ∈ K × A. Also, we have EVI ≤ cminε∕18. The lemmas of this section
12
Under review as a conference paper at ICLR 2022
1
2
3
4
5
6
7
8
9
10
11
12
Algorithm 4: Subroutine VISGO
Input: Set of states S, set of actions A, set of known states K, initial state s0, goal state g and VI.
Global variables: B, L, N (), n(), Pb, θ(), cb().
For all (s, a, s0) ∈ K × A × S, set
P , n(s,a P + I[s0 = g]
Ps,a,s0	n(s,a) + 1 Ps,a,s0 + n(s,a) + 1.
For all (s, a) ∈ K × A, set ∣s,a , 4 ln( 24SAn(S,a)]).
Seti , 0, V(0) , 0, V (-1) , +∞.
while kV * (i) * - V (i-1) k∞ > VI do
For all (s, a) ∈ K × A, set
√i+1)/	-	V	V(Ps,a,V (i))∣s,a	B∣s,a ∖ ,	∕c(s,α)∣s,a
b(+)(s，a) 一 max(cι√ —n(T^-, c2n(T^)+ c3∖	n(s a) ,	(3)
n s, a	n s, a	n s, a
Q(i+1) (s, a)	,	cb(s,	a)	+	Pes,aV	(i)	-	b(i+1) (s,	a),	(4)
V (i+1)(s) , min Q(i+1) (s, a).	(5)
a
For all s ∈/ K ∪ {g}, setV (i+1)(s) , cRESET +V (i)(s0).
SetV (i+1)(g) , 0, i , i+1.
ifV (i) (s0) > L then
return Q⑶,V(i).
return Q(i) ,V (i).
are based on these conditions. We note that as n(s, a) ≥ φ, and by our construction of φ, we have
b(U, s, a) ≤ cminε∕18 for any (s, a) ∈ K × A. Therefore, if U(g) = 0 and U ≥ 0 component-wise,
when n(s, a) ≥ φ for any (s, a) ∈ K × A, we have LU (s, a) ≥ 0 for any (s, a) ∈ K × A.
For convenience, we define b(U, s, a) := 0 for any s ∈∕ K and a ∈ A.
Lemma 5 (Tarbouriech et al. (2021), Lemma 12). For any non-negative vector U ∈ RS such that
U(g) = 0, for any (s, a) ∈ K × A, it holds that
PsaU ≤ PbsaU ≤ PsaU + JU“∞ .
s,a	s,a	s,a	.
n(s, a) + 1
The proof of the following Lem. 6 is similar with Lem. 16 in (Tarbouriech et al., 2021), but here we
have two distributions P and p. We give the whole proof for completeness.
Lemma6. Let Y := {v ∈ RS : V ≥ 0, v(g) = 0, ∣∣v∣∣∞ ≤ B}. Let f : ∆s×∆S× Υ×R×R×R →
R with f(p,p, v, n, B, ∣) := Pv 一 max {cι JV(Pnv)I, c? Bn }, With constants ci = 6 and c2 ≥ 2c2.
Then f sαtisfies,for all V ∈ Y, n,ι > 0, p,p ∈ ∆s s.t. P(S) — 11 p(s) ≥ 0 for all S = g,
1. f (p,p,v,n, B, ι) is non-decreasing in V(S), i.e.
∀(v,v0) ∈ Y2, V ≤ V0 =⇒ f(p,p,v,n,B,∣) ≤ f (p,p, v0, n, B, ι);
2.
f(p,p,V,n,B,ι) ≤ PV —号，吗辿—号 B ≤ PV — 2,吗辿—14B;
3. If p(g) > 0, then f (p,p, v, n, B, ι) is Pp-contractive in V(S), with Pp := 1 — p(g) < 1, i.e.
∀(V, v') ∈ γ2, lf(p,p, V,n,B,|) — f (p,p, v0,n,B, ι)1 ≤ Ppkv — v0k∞.
Proof. We use the idea in Tarbouriech et al. (2021), Lemma 14 to finish the proof.
The second claim holds by max{x, y} ≥ (x + y)∕2, ∀x, y, by the choices of c1, c2 and because both
,v(pnv)1 and B are non-negative. To verify the first and third claims, we fix all other variables but
13
Under review as a conference paper at ICLR 2022
v(s) and view f as a function in v(s). Because the derivative of f in v(s) does not exist only when
ci qV(PMI = c2 B, where the condition has at most two solutions, it suffices to prove -J-f— ≥ 0
n	n	∂ v(s)
when c1
c2 B. Direct computation gives that for any S ∈ S and S = g,
∂f)= P(S)-CII
Ci产巫≥C2B
nn
p(S)(v(S) - pv)ι
nV(p,v)ι
≥ min {p(s), P(S)——ci5P(S)(V(S) - pv)}
C2 B
≥ min {P(s), P(S) - c1 p(s)}
C2
≥ 0.
Here (i) is by v(S) - Pv ≤ v(S) ≤ B. In addition, we have
Xl -df-∣ = X U) - CiI [eiʌ /VpV)I ≥ C2 B∣∖ P(S)(V(S) - Pv)I
S=gldv(S) l	S=g[	[ Vn	n] PnV(P,v)ι _
=1 - P(g) - CiI Ci ∖/V(P，V)l ≥ C2 B J :、[pv - (1 - P(g)) ∙ Pv]
n	n nV(P, V)
≤ 1 - P(g).
Therefore, We obtain that f is ρp-contractive.
□
We note that by definition of PeS,a, We have PeS,a,s0 - iPs,as ≥ 0 for all (s, a, s0) ∈ K×A×S.
The following two lemmas follow the same proof with Lem.18, Lem.19 in (Tarbouriech et al., 2021),
respectively.
Lemma 7. The sequence (V (i))i≥0 is non-decreasing.
Lemma 8. L is a ρ-contractive operator with modulus ρ := 1 - ν < 1, where ν = min Ps,a,g.
(s,a)∈K×A
Hence, the VISGOprocedure will terminate after at most dlog(1∕tV[)/log(1∕ρ)] iterations.
C.1 The B ounded Error Property of VISGO
Now we focus on Alg. 1. We give the following lemma of the bounded error property (Lem. 9), which
indicates that the value function of the policy πs is close to our estimation. The proof Lem. 9 uses the
techniques of Lem. 2 in (Tarbouriech et al., 2020). Our Lem. 9 focuses on a more general operator
L. In our L, we involve the bonus function b(U, S, a), which is not contained in (Tarbouriech et al.,
2020).
Lemma 9. In Alg.1, under the event G, for any goal state g that is added to K, let (Q,V ) be the
output (Qg,Vg) of VISGO in that round, and let π be the greedy policy with respect to Q. Then π is
proper on the model Ps,a,s0, and for all S ∈ S, we have Vgπ (S) ≤ (1 + ε)V (S).
Proof. We define Vgπ(S) as the value function of π with goal state g on the model Ps,a,s0 . We
will first prove that Vgπ(s) ≤ (1 + ε∕3)V(s), and then prove that Vgπ(s) ≤ (1 + ε∕3)Vgr(s) using
the simulation lemma on the two models Pes,a,s0 and Ps,a,s0 . Combining them together yields
Vgπ(S) ≤ (1 + ε)V (S).
First we focus on model Ps,a,s0. We recall that for any S ∈ K and S 6= g,
≈ ，.
Lu(S)
:= min Cb(S, a) - b(u, S, a) + Pes,au
14
Under review as a conference paper at ICLR 2022
where
b(u,s, a) = max! Cit UR：",a ,c2 *⅜
I ∖	n(s,a)	n(s,a)
and we define b(u, s, a) = 0 when s ∈/ K.
As we set
φ = Y)2 ln(SA))，
and n(s, a) ≥ φ, We can obtain b(u, s, a) ≤ Cm⅛ε∕18 when ∣∣u∣∣∞ ≤ B.
In addition, under the event Gi, when n(s, a) ≥ φ, we have |c(s, a) - c(s, a)| ≤ Cm⅛ε∕18 for all
(s, a) ∈ S × A.
We denote l as the final iteration index of VISGO, and V = V (l). In VISGO, we have V (i) =
LeV(i-i) for all i = 1,2,…，l. As V(l-i) ≤ V(l) component-wise, we have for any S ∈ S,
V(s) ≤ V (l-i)(s0) + 1 ≤ L+ 1. Hence, kVk∞ ≤ L+ 1 ≤ B.
We set γ = cminε∕6. As VI ≤ cminε∕18, we have b(u, s, a) + |cb(s, a) - c(s, a)| + VI ≤ γ when
kuk∞ ≤ B.
Given the policy π restricted on K, we introduce the following operators on RS :
π
Lπ u(s) = cb(s, π(s)) - b(u, s, π(s)) + Ps,π(s)u,
π
Tγπu(s) := c(s, π(s)) - γ + Ps,π(s)u.
We can write component-wise
Tγπ V ≤ Lπ V - VI (=a)
(b)
LV-VI ≤ V,

where (a) uses that π is the greedy policy with respect to V. To prove (b), we recall that V = V(l) =
LeV(l-i). By contraction property of Le, we have kLeV - Vk∞ ≤ kV (l) - V(l-i) k∞. By stopping
condition of VISGO, we have kV (l) - V (l-i) k∞ ≤ VI, thus (b) is proved. By monotonicity of the
operator Tγπ, we have for all m > 0, (Tγπ)mV ≤ V.
As (Tγπ)mV not increases element-wise when m increase, and is non-negative element wise, when
m → ∞, it will converge to Wγπ, where Wγπ is the value function of policy π in the model P with γ
subtracted to all the costs, and we have Wγπ ≤ V component-wise. We define the random variable
居(S) as the number of steps it takes to reach g starting from S on model P when executing policy ∏.
Thus
吃(S)	一 〜
WYr(S)= EPe X C(St,π(St))-Y | Si = S = Vr(S)-YEP 图(S)].
t=i
Moreover, we have CminERn(s)] ≤ Vr(s). Therefore, we get
Vegr(S)≤
WYr (S)	≤ V(S)
1 - γ∕cmin	1 - γ∕cmin
≤ (1 + ε∕3)V (S).
As n(S, a) ≥ φ, under the event G2, we have ∀(S, a, S0) ∈ S × A × S,
Ra,'，- ps,”∕≤ 6(L+εi).
Thus by simulation lemma for SSP (Lemma 3 in Tarbouriech et al. (2020)), π is proper on true model
Ps,a,s0, and we have for all s ∈ S, Vgr (s) ≤ (1 + ε∕3)Vgr (s). The proof is completed.	□
15
Under review as a conference paper at ICLR 2022
C.2 Optimism of VISGO
Now we will give the optimism property. We still focus on Alg. 1. We note that there are exponentially
possible number of set K, thus we cannot include all the K in our high-probability event G. As the
event G3 includes the sets K0, Ki, ∙∙∙ , KZ, We choose Z ∈{0 ≤ Z ≤ Z : Kz ⊆ K}, and We will
compare the output of VISGO (denoted by V), with the value function of the optimal policy restricted
on Kz with goal state g (denoted by VK ). We recall that we define B* = max ∣∣VKK k∞. By
z,g	(K,g)∈F	,g
the definition of F , we have B* ≤ L + 1 ≤ B .
Lemma 10. Let Z ∈ {0 ≤ Z ≤ Z : Kz ⊆ K}. Assume that there exists at least one proper policy
restricted on Kz with the goal state g, and VK* ,g (s) ≤ B for all s ∈ S. In Alg.1, under the event G,
for any output (Q, V) of the VISGO procedure, it holds that
Q(s,a) ≤ QKz ,g (s,a),
V(S) ≤ VKz,g(s),
∀(s, a) ∈ K × A,
∀s ∈ S .
Proof. We prove by induction that for any inner iteration i of VISGO, Q(i) (s, a) ≤ Q*K ,g (s, a) for
any (s, a) ∈ K×A, and V(i) (s) ≤ VK* ,g(s) for any s ∈ S. By definition we have Q(0) = 0 ≤ Q*K ,g,
and V(0) = 0 ≤ VK* ,g. Assume that the property holds for iteration i, then for any (s, a) ∈ K × A,
Q(i+1)(s,a) = cb(s, a) + Pes,aV (i) -b(i+1)(s,a),
where
bc(s, a) + Pes,aV (i) -b(i+1)(s,a)
= cb(s, a) + Pes,aV(i) - max
(i)
≤ c(s, a) + Ps,aV(i) - max
n	∕v(Ps,a, V(i))∣s,a	B∣s,a o _	Sb(s, a)∣s,α
1 1 y n(s,a)	， 2 n(s, a) J 3 y n(s,a)
n	∕v(Ps,a,V(i))∣s;	B∣s,a o +	28ls,a
("V n(s,a)	，C2 n(s, a) J	3n(s,a)
=C(S, a) + f(ps,a, Pbs,a, V(i),n(s, a), B, ιs,a) +	、
3n(s, a)
≤ c(s, a) + f (Ps,a, ps,a, VKz,g, n(s, a),B, ιs,a) + 产厂工
3n(S, a)
(iii)
≤ c(s,a) + Ps,aVKz,g - 2
V(Ps,a,VKz,g )∣s,a	14B∣s,a
n(s, a)
3n(s, a)
(iv)
≤ c(s, a) + Pbs,aVKz ,g - 2
V(Ps,a,VKz,g )∣s,a	14B∣s,a
(v)
≤c(s,a)+Ps,aVK*z,g-(B-B*)
'-----------{-----------}
=(QKz,g (s，a)
n(S, a)
14ιs,a
3n(S, a)
3n(s, a)

—
s
—
≤ Q*Kz,g(S,a),
where (i) is by definition of G1 and choice of c3 , (ii) uses the first property of Lem. 6 and the
induction hypothesis that V(i) ≤ VK* ,g, (iii) uses the second property of Lem. 6 and assumption
B ≥ max{B*, 1}, (iv) uses Lem. 5, (v) is by definition of G3. Ultimately, for any (S, a) ∈ K × A,
Q(i+1)(S,a) ≤Q*Kz,g(S,a).
Then for any S ∈ K and S 6= g, we have V (i+1) (S) = m∈iAnQ(i+1)(S, a) ≤ m∈iAn Q*Kz,g (S, a) ≤
VK*z,g(S).
In addition, V (i+1)(g) = 0 = VK* ,g (g), and for any S ∈/ K ∪ {g}, as Kz ⊆ K, we have
V(i+1) (S) = cRESET + V(i) (S0) ≤ cRESET + VK*z,g(S0) = VK*z,g(S).
This completes the proof of this lemma.
□
16
Under review as a conference paper at ICLR 2022
D	Proof of Thm. 1
Proof idea. We provide the main intuition here. We denote K and {πs}s∈K as the output of Alg.1.
We fix the round index r, and We consider a state g ∈ K, g ∈ S→ and VK g (so) ≤ L. To show that
SL→ ⊆ K, we need two steps. First, we show that g will be added to U under the event G2 . Then we
will use the optimism property of VISGO (Lem. 10) to prove that our estimation of the value funtion
of the optimal policy Vg(s0) is no larger than L.
To prove that K ⊆ S→+ε)L, we need to prove that when g is added to K, we have Vn (s0) ≤ (1+ ε)L,
where ∏ is the greedy policy over Qg in that round. This is a direct consequence of Lem. 9.
Now we bound the total cumulative cost. For each state-action pair (s, a) ∈ K × A, we collected
O(L2/(Cminε2)) samples. And to reach each S ∈ K, we executed the policy ∏s, and the cost
to reach s is no larger than O(L) with high probability. Thus the total cost can be bounded by
O(L3KA/(Cminε2)), and K = |K| ≤ S(+)l.
□
Proof. We first use Lem. 10 to show that SL→ ⊆ K. Then we use Lem. 9 to show that K ⊆ S(→1+ε)L,
and each policy ∏ output by Alg.1 satisfies Vns(So) ≤ (1 + ε)VK s(so). Then we will bound the
total sample complexity of Alg.1.
To show that S→ ⊆ K, we will prove that Kz ⊆ K for all Z = 0,1,2,…，Z by induction. We
observe that {S0 } = K0 ⊆ K because the initial Snew is S0 . Then we assume that Kz ⊆ K, and we
will prove Kz+1 ⊆ K.
For any state g ∈ Kz+ι and g ∈ Kz, we have VKN g ≤ L by definition of Kz+ι. To show that g will
be added to K eventually, we need to prove the following two statements: (1) g is added to U; (2) the
output of VISGO procedure Vg(so) ≤ L when Kz ⊆ K. We observe that as VKN,g ≤ L, there must
♦ . /	、一”	A	1	.1	.	7-»	TA	1	11,1/	pʌ / 7^ 9 ∖	1 C∙	IC
exist (s, a) ∈ Kz × A such that Ps,a,g ≥ 1/L. As we have collected φ = O(L2) samples for each of
3
the state-action pair (s, a) ∈ K ×A, under event G2, we can obtain that Ps,a,g ≥ 3L > 0, which
yields that g has been discovered. Thus g has been added to U in some round, and statement (1) is
verified. The statement (2) is a direct consequence of the optimism property Lem. 10. Thus we have
proved that Kz+1 ⊆ K, which yields that SL→ ⊆ K.
Then we will prove K ⊆ S(→1+ε)L. By Lem. 9, each time when we add s to K, we have Vsπs (so) ≤
(1 + ε)L, i.e. the AXL on K criterion is met. We recall that the definition of S(→1+ε)L is a recursive
construction, adding new state s whenever there exists some policy π such that Vsπ (so) ≤ (1 + ε)L.
Therefore, each state added to K is a subset of S(→1+ε)L, and we obtain that K ⊆ S(→1+ε)L .
Now we will bound the total cumulative cost. We note that we have collected φ = O(L2/(Cminε2))
samples for each state-action pair (s, a) ∈ K × A. To collect each sample (s, a, s0, C), we executed
a policy πs to reach the state s, and the expected cost Vsπs (so) ≤ (1 + ε)L ≤ 2L. By Lem. 3, we
obtain that with probability at least 1 - δ∕4, for any state s, each time when the policy ∏s is executed,
the total cost to reach S from so is no larger than O(L log(S∕δ)). Therefore, the total cumulative cost
in Part 1 of Alg. 1 can be bounded by O(L3S(i+e)lA/(Cminε2)). Thus we obtain the bound
CT
O( l⅛a ).
Cminε
Now we bound the total failure probability. The event G fails with probability no more than δ∕4. And
in the previous para the failure probability is bounded by δ∕4, thus the total failure probability is no
more than δ∕2. The proof of Thm. 1 is completed.
□
17
Under review as a conference paper at ICLR 2022
E Proof of Thm. 2
First we define the high-probability event E to do concentration on all the samples collected in
Alg.2. We note that the definition of E also applies for Alg. 3 after it finishes the subroutine Alg. 1
and clears all the samples in Alg. 1. We note that after Alg.1, the set of known states K is fixed,
and our algorithm focuses on the new MDP Mt =(K*, A,Pt,et,so). Here for any g ∈ K, we
denote the vector Vg= ∈ RK0 as the value function of the optimal policy on MDP Mt with respect to
goal g, and when K ⊆ S→+ε)L in Alg.2, we have Bt := ∣∣Vg^k∞ ≤ B. For convenience, here we
denote Vg=(s) as the expected cost of the optimal policy to reach g from s on MDP Mt, and we have
Vg= (s) ≤ 2L + 1 ≤ B for all (s, g) ∈ Kt × K. Then we define the high-probability event Gt.
Definition 6 (High-probability event Gt). We define the event Gt := G1t ∩ G2t ∩ G3t, where
G1t :
∀(s, a) ∈ Kt × A, ∀n(s, a) ≥ 1
t	2cb(s, a)ιs,a	28ιs,a
Ib(S,a) -Ct(S,叫 ≤ 2j mm + 3n^ j,
× A × Kt , ∀n(s, a) ≥ 1
: IPs,a,s0 - Ps,a,s0 I ≤
/",s, ∣s,a +
n(S, a)
ιs,a
n(S, a)
∀(S, a, g) ∈ Kt × A × K, ∀n(S, a) ≥ 1
I风0 - Ps,a)κ∣ ≤ 2 J(PniS『,a+⅛B⅛ ∣,
n(S, a)	n(S, a)
where ∣s,a=4ln(24SA[”a)]).
Finally, we define the high-probability event E := G ∩ Gt. And we have the following lemma.
Lemma 11. It holds that P(E) ≥ 1 一 δ∕2.
We introduce the lemma of optimism and the lemma of bounded error in Alg. 2. We note that the
proof of them is the same with Lem. 10 and Lem. 9, respectively.
Lemma 12. In Alg.2, under the event E, for any output (Qg, Vg) of the VISGO procedure, it holds
that
Qg(S,a) ≤ Q=g(S,a),	∀(S, a) ∈Kt ×A,
Vg(S) ≤ Vg= (S),	∀S ∈ Kt.
Lemma 13. In Alg. 2, under the event E, let (Qg, Vg) be the output of VISGO, where g is the goal
state, and let π be the greedy policy with respect to Qg. Then π is proper on the model Ps,a,s0, and
for all S ∈ Kt, we have Vgπ (S) ≤ (1 + ε)Vg (S).
Now we give the proof of Thm. 2. First we prove the correctness. By Thm. 1, VOISD outputs a set K
such that SL→ ⊆ K ⊆ S(→1+ε)L. Now we focus on the set of policies {πs}s∈K that Alg. 2 outputs, and
we will prove that for each g ∈ K, we have Vgπ (S0) ≤ (1 + 3ε)L and Vgπ (S0) ≤ VK=,g (S0) + 3εL.
As we set ε J ε∕3 in the beginning of Alg. 1, the correctness of Alg.1+Alg. 2 can be proved.
By Lem. 13 and Lem. 12, we obtain that Vgπ(S0) ≤ (1 + ε)Vg(S0) ≤ (1 + ε)Vg=(S0). We note that
Vg= (S0) ≤ (1 + ε)L, because Alg. 1 satisfies AXL on K. Hence, we can get Vgπ(S0) ≤ (1 + ε)2L ≤
(1 + 3ε)L, and Vgπ(S0) ≤ Vg=(S0) +ε(1 +ε)L ≤ Vg=(S0) + 3εL. Thus Alg. 2 outputs a set of policies
{πs} that satisfies both AXL on K and AX= on K.
Similarly with Thm. 1, as we collect φ samples for each state-action pair (S, a) ∈ K × A, we can
obtain that with probability at least 1 一 δ, the total sample complexity is bounded by O(L s21+ε)LA).
cminε
F Proof of Thm. 3
Here we give a proof of Thm. 3 and we focus on the fixed set Kt and our constructed MDP M t
hKt, A, Pt, ct, S0i. We denote K = IKI,andK0 = IKtI = K+ 1.
18
Under review as a conference paper at ICLR 2022
Proof idea. First we prove the accuracy of Alg.3. It uses Alg. 1 with ε = 1 as a subroutine, thus we
can get a set K such that SL→ ⊆ K ⊆ S2→L. For each goal state g, when g is removed from U and the
policy πg is set, the cost to reach g from s0 under policy πg has been tested for λ = Oe(1/2) times.
Therefore, by the concentration inequality, we can prove that the average cost is close to the expected
cost of πg. Moreover, the average cost is less than our estimation of the cost of the optimal policy
Vg(s0) plus L. Hence we can prove that the expected cost of πg is close to the optimal cost using
the optimism property.
To bound the cumulative cost of Alg.3, we will focus on the number of "rounds" and the cost it takes
in each round. By multiplying them together, we can get the upper bound of the cumulative cost.
It S straightforward to show that the total cost in each round is no larger than O(L∕ε2). Then We
will bound the total number of rounds. We denote T as the total number of samples collected in
Alg. 3. The number of success rounds is at most K . And the trigger condition holds for at most
log2(2T) times for each state-action pair (s, a), thus we obtain that the number of skipped rounds
can be bounded by K0A log2(2T). Now we need only to bound the total number of failure rounds.
We borrow the idea from (Lim and Auer, 2012). We first define the regret of an episode as the total
cost in this episode minus our estimation of the optimal cost, and define the total regret as the sum of
the regret in each episode. Then we prove a regret bound of our Alg. 3, and the upper bound grows
sublinearly with the total number of episodes M . As the regret in each failure round is at least Lλ,
thus we can give a lower bound of the total regret that grows linearly with the number of episodes
M . By solving the inequality, we will obtain that the total number of failure rounds is bounded by
O(KA + εK2A). Thus the total number of rounds is bounded by O(KA + εK2A), and multiplying
it with the cost in each round, we can get the bound in Thm. 3 and complete the proof.
□
Now We give the full proof. Here We denote Vg* (S) as the expected cost of the optimal policy to reach
g from s on MDP Mt, and we have Vg= (S) ≤ 2L + 1 ≤ B for all (s, g) ∈ K X K.
We recall the definition of the high-probability event E in Appendix E. Also, we introduce the lemma
of optimism and the lemma of bounded error in Alg. 3, and the proof of them is the same with Lem. 12
and Lem. 13, respectively.
Lemma 14. In Alg.3, under the event E, for any output (Q, V ) of the VISGO procedure, it holds that
Q(s,a) ≤ Q*(s,a),	∀(s,a) ∈ Kt × A,
V(S) ≤ Vg*(s),	∀s ∈Kt,
where g is the goal state in that round.
Lemma 15. In Alg. 3, under the event E, for any output (Q, V ) of the VISGO procedure, let π be
the greedy policy with respect to Q. Then π is proper on the model Ps,a,s0, and for all S ∈ Kt, we
have Vgπ (S) ≤ 2V (S), where g is the goal state in that round.
Now we prove the correctness of Alg. 3, i.e. Alg. 3 satisfies AX* on K. The main intuition is that
each policy πs has been tested for λ times in a success round, and the average cost it takes to reach S
from S0 is less than our estimate for optimal cost V (S0) plus L. Thus by concentration inequalities,
the expected cost of πs is close to optimal.
Lemma 16. Let {πs}s∈K be the set of policies output by Alg.3. With probability at least 1 - δ,
Vsπs (S0) ≤ Vs*(S0) + εL for all S ∈ K.
Proof. We fix any state S ∈ S. In any given round where the chosen target is s, let Tk be the total
cost in the k-th episode of that round. Recall that for the algorithm to output a policy πs, its empirical
performance after λ episodes must satisfy that T ≤ V (so) + eL, where T = Pkλ1Tk.
We define the random variable τ as the total cost it takes to reach the goal state S from the start state
S0 when executing policy πs, and we have E(T) = Vsπs (S0) by definition. We note that we have
collected φ = O(L2/c2min) samples for each of the state-action pair (S, a). By Lem. 15, under event E,
the policy πs is proper, and we have E(T) ≤ 2L. Hence, we obtain d := kVsπs k∞ ≤ 2L + 1 ≤ 4L.
By Lem 3, we obtain Pr(T > m) ≤ 2 exp(-m/4d) for any m > 0.
19
Under review as a conference paper at ICLR 2022
As we set
2048 2 256	2S
λ=d 丁 ln2(—)ln( 万几
by Lem. 4, We obtain that with probability at least 1 - δ∕(2S), We have
Vns(SO)= E(T) ≤ τ + eL.
We note that T ≤ V (so) + EL ≤ V* (so) + EL by the optimism property (Lem. 14). Hence, as we set
= ε∕3 in initial, We obtain that With probability at least 1 - δ∕(2S),
Vsπs(so) ≤ Vs*(so)+εL.
Finally, as there are at most S states in total, and the event E holds with probability at least 1 - δ∕2,
by the union bound, the total success probability is at least 1 - δ.	□
Now we focus on bounding the cumulative cost of Alg.3. The key idea lies in bounding the "regret".
We will use the regret to bound the total number of rounds. We first define the regret in the k-th
episode of the j-th round. We denote Hj,k as the number of steps it takes in the k-th episode of the
j -th round. The regret in an episode k is defined as
Hj,k
(Xcjh,k) -Vj(so),
h=1
where cjh,k is the empirical cost in the h-th step in the k-th episode in the j-th round, and Vj (so) is
the value of V(so) in the j-th round. Let nj be the total number of episodes executed in the j-th
round, we define the regret in the j -th round as follows:
nj Hj,k
X((Xcjh,k) -Vj(so)).
k=1 h=1
Then we will define the total regret of Alg.3. Let r be the total number of rounds, nj be the total
number of episodes executed in the j-th round, and 0 ≤ nj ≤ λ. Then we know that the total number
r
of episodes in the whole process of Alg.3 is M = P nj . For notation convenience, we define Hm
j=1
as the number of steps it takes in the m-th episode of the whole process of Alg.3, and denote chm as
the empirical cost in the h-step of episode m. Finally we define the total regret of all the rounds as
r nj	Hj,k	M	Hm
R:=XX((Xcjh,k)-Vj(so))= X ((X chm) - V m(so)).
j=1 k=1 h=1	m=1 h=1
We will give both the upper bound and the lower bound of the regret. Here we gives the upper bound.
Lemma 17. Under event E, the total regret in M episodes is at most
R = O(L√KAM + LK 2A).
This upper bound comes from the regret bound of the EB-SSP algorithm (Tarbouriech et al., 2021),
which solves the classical SSP problem, and the proof of Lem. 17 is similar with Thm.3 in (Tar-
bouriech et al., 2021).
We observe that there are at most O(KA) skipped rounds and K success rounds. We denote by rf
the number of failure rounds, and we have the total number of episodes M = O((KA + rf)λ) =
O((KA + rf)∕E2). Thus the total regret in r rounds can be bounded by rf sublinearly:
R = O( L √KArf + LKA + LK 2A).
Then we gives the lower bound of the total regret in terms of the number of failure rounds rf .
20
Under review as a conference paper at ICLR 2022
Lemma 18. With probability 1 - δ, when r = O((SA)2), the total regret in the r rounds is at least
R = Ω
LKA)
where rf in the number of failure rounds in the r rounds.
Proof. By the criterion of our performance check, in any failure round, We have T > V (so) + eL,
nj Hj,k
and in round j, we have T= 1 P ( P Chk) by definition. Hence, in any failure round j, the regret
k=1 h=1
is λτ - njVj(so) ≥ λ(T - Vj(s0)) ≥ λeL = Ω(Lrf).
Then we focus on skipped rounds and success rounds. We denote gj as the goal state in the j -th
round, and ∏j as the policy ∏ in the j-th round, which is the greedy policy over the Q-function in the
j -th round. We observe that the regret in any round j satisfies
nj Hj,k	nj-1 Hj,k	nj-1 Hj,k
E((E Chk) - vj (so)) ≥ -L + E ((E Chk) - V,*- (so)) ≥ -l+ E ((E Chk) - Vnj (so)),
k=1 h=1
k=1	h=1
k=1	h=1
Hj,k j k
where we used the optimism property in Lem. 14. We note that	Cjh,k is the empirical cost of
h=1
policy πj in episode k, and we will use the concentration inequality to give a lower bound of the
regret in round j . As the last episode in a skipped round can terminate before reaching the goal, we
should take special considerations the last episode of each round. We directly use -L to lower bound
the regret of the last episode in round j. Then we denote n = nj - 1, and focus on the previous n
episodes in round j .
Now we fix the round index j. We denote the random variable T as the cost to reach gj from so, and
Hj,k j k	δ
we recall that Tk = E Chk. By Lem. 4 with d = 4L, with probability at least 1 - (SA尸,we have
h=1
二	S SA	S SA L L
X(Tk - E(T)) ≥ -2γy nln(ɪ) ≥ -2γy λln(ɪ) ≥ -ω(—),
k=1
where Γ = [8dln(64/6)C. Thus the regret in any round j is no less than -Ω(L). As there are at most
六 ∕T7 4、 - 1	1 —7	1	1 . . .1 . .1 . . 1	1	1
O(KA) skipped rounds and K success rounds, we obtain that the total regret R has the lower bound
R = Ω
LKA)
TL T	Λ	t .t . . 1 ∕' ∙1	FF ∙ 1 ∙ . El	FC	t	pζ∕∕c Λ ∖ 9 ∖ ♦	t	t
Now we bound the total failure probability. The number of rounds r = O((SA)2), in each round
the failure probability is at most (SA产,and the events E fails with probability δ∕2. By resetting
δ → O(δ∕(SAL)) in initial, the total failure probability is at most δ.
As the lower bound is linear in rf, and the upper bound is sublinear in rf, we can solve it and obtain
that rf = O(KA +—K2A), thus the total number of rounds can be bounded by O(KA +—K2A).
To get the cumulative cost bound in Thm. 3, we need only to bound the cost in a round. In any
round, we observe that except for the last episode, the average cost T for all the other episodes is no
larger than V(so) + ―L ≤ 2L, thus the total cost In these episodes IS no larger than 2Lλ = O(L/—2).
Also, we know that in the any episode, the expected cost of the policy ∏ to reach the goal from so
is no larger than 2L. Thus by Lem. 3, in any round, with probability at least 1 - (SA产,the cost
♦	.11.	∙	1	∙	1	.1	pʌ / T ∖ T T	.1	.	.	1	.	∙	1	1 ∙	1	.1
in the last episode is no larger than O(L). Hence, the total cost in each round is no larger than
O(L/—2). By multiplying it with O(KA +—K2A), the cumulative cost in Alg.3 can be bounded by
O(LKA∕ε2 + LK2A∕ε + L3KA∕cmn), where the term L3KA∕cmn comes from the subroutine of
Alg. 1. As we have K ≤ S2L, we obtain the bound in Thm. 3.
Now we count the total failure probability. First, Alg. 1 fails with probability δ∕2, the event E fails
with probability δ∕2, and the lower bound of the total regret R fails with probability δ. And in the
21
Under review as a conference paper at ICLR 2022
previous para, to bound the cost in the last episode of each round using Lem. 3, the failure probability
in each episode is at most (SA产.We observe that the total number of these failures is no larger than
.1 . . 1	F	Γ∙	1	1 .1 . . 1	F	Γ∙	1	FF	1 11 六 ∕T7 A	T 2 A ∖
the total number of rounds, and the total number of rounds can be bounded by O(KA + K2A),
where We omit the logarithmic factors. Thus by setting δ → O(δ∕(SAL)) in initial, We can bound
the total failure probability by the initial δ, and the proof of Thm. 3 is completed.
G Analysis of the lower b ound
Here We discuss the loWer bound of the autonomous exploration problem. We recall that We relax the
problem and consider only all the MDPs M With S states and A actions, and M satisfies Asmp. 1.
Our algorithm needs output a set K ⊇ SL→ and a set of policies {πs}s∈K, and When s ∈ SL→, the
policy πs satisfies Vsπs (s0) ≤ (1 + ε)L. Moreover, We note that here We alloW the algorithm to
output non-deterministic policies, i.e. the policy πs : S → ∆(A) is a function from the set of states
to a probability distribution over the actions.
We recall the some basic concepts about the definition of a learning algorithm, and We use the
notations in (Domingues et al., 2021). Let It = (S × A)t-1 × S be the set of all possible histories
up to t steps, that is, be the set of tuples of the form s1 , a1 , s2 , a2 , . . . , st ∈ It . We denoted
π , (πt)t≥1 as a history-dependent policy, Where πt : It → ∆(A) describes the probability of
taking action a ∈ A after observing some history it ∈ It .
Given an MDP M = hS, A, p, c, s0i, a policy π interacting With the MDP M defines a stochastic
process denoted by (St, At)t≥1, Where (St, At) is the state-action pair at time t. The Ionescu-Tulcea
theorem ensures the existence of the probability space (Ω, F, PM) such that
PM [S1 = s] = I[s = so], Pm [St+1 = S | At, 11] = p(s | St, At), and PM [At = a | 11] = πt (a | It),
Where π = (πt)t≥1 and for any t, It , S1, A1, S2, A2, . . . St is the randomvectorinIt containing
all state-action pairs observed up to step t. We denote the σ-algebra generated by It as Ft . And We
denote by PIMT the measure of IT under PM as folloWs:
T-1
PM [iT]，Pm [IT = iT] = I(s1 = so) Y ∏t(at | it')p(st+r | st,at).
t=1
Then We denote EM as the expectation under PM. Note that the dependence of PM and EM on the
policy π is denoted implicitly in the definition of PM . We Will denote them explicitly as Pπ,M and
Eπ,M respectively When We need to stress π.
Moreover, given the MDP M =(S, A,p, c, s。)，we denote VMM,g(S) as the optimal policy to reach
the goal state g from the state s.
Here we introduce the basic definitions and the technical lemmas used in our proof.
Definition 7. The Kullback-Leibler divergence between two distributions P1 and P2 on a measurable
space (Ω, G) is defined as
KL(Pι, P2)，Zo log (果(ω))dPι(ω),
if P1 P2 and +∞ otherwise. For Bernoulli distributions, we define ∀(p, q) ∈ [0, 1]2,
kl(p, q) , KL(B(p), B(q)) = p log
+ (1- p)log(⅛p
Lemma 19 (Lemma 5, Domingues et al. (2021), modified). Let M and M0 be two MDPs that are
identical except for their transition probabilities, denoted by p and p0, respectively. Assume that we
have ∀(s, a), p(∙ | s, a)《p0(∙ | s, a). Then, for any stopping time T with respect to (Ft)t≥ι that
satisfies PM [τ < ∞] = 1,
KL (PM, Pm，) = XX EM[NT,a]KL(p(∙ | s,a),p0(∙ | s,a)),
s∈S a∈A
22
Under review as a conference paper at ICLR 2022
where Nsa，ET=I !{(St, At) = (s, a)} and Iτ is the random vector representing the history of T
samples.
Lemma 20 (Lemma 1, Garivier et al. (2019)). Consider a measurable space (Ω, F) equipped with
two distributions Pi and P2. Forany F-measurablefunCtion Z : Ω → [0,1], we have
KL(P1,P2) ≥kl(E1[Z],E2[Z]),
where E1 and E2 are the expectations under P1 and P2 respectively.
Lemma 21. Forany p,q ∈ (0, ɪ], kl(p,q) ≤ 2(p-q) .
Now we construct a family of adversarial MDPs to obtain the lower bound of sample complexity.
An example of hard MDP with two states. For clarity, we first consider an MDP with two states s
and g. There are A actions in the state s, and each action a transits s to g with success probability
((1 + ε)L)-1, and transits s to s with failure probability 1 - ((1 + ε)L)-1. In this MDP, the expected
cost to reach g from s is (1 + ε)L. Then we construct A MDPs, each MDP increasing the success
probability of an action a to L-1 , and the the expected cost to reach g from s is L in all of these MDPs.
Thus to discriminate these MDPs, we need to discriminate between the two Bernoulli distributions
with p = L-1 and p = ((1 + ε)L)-1. Using the KL-divergence, we can obtain that the minimum
number of samples is O(L∕ε2). As there are A actions, We can obtain the O(LA∕ε2) lower bound.
The construction of hard MDPs with general SL. Now we fix L, A, SL, ε such that L > 4, A > 4,
4 ≤ SL ≤ min{(A)bL2c, 2}, 0 < ε < 4. It,s straightforward to verify that we can construct an
MDP M00 = hS0, A0,p00, c, s0i with |S0| = SL - 1 states and |A0| = dA/2e actions, such that
c(s, a) = cmin for each (s, a) ∈ S0 × A0, and each state s ∈ S0 is incrementally (L/2)-controllable,
i.e. its S→2 = S0. Thus for each S ∈ S0, we have VMM0 s(s。)≤ L
Then we will construct the MDP M0 = hS, A,p0, c, s0i based on M00 in the following way. First we
construct SL - 1 states and [A/21 actions, and denote them as St and At, respectively, and we set
S = S0 ∪ St, A = A0 ∪ AL For each S ∈ St and any actions a ∈ A, we havepo(so∣s, a) = 1, i.e.
every action at s ∈ St is the reset action to s0. And we set c(s, a) = cmin for each (s, a) ∈ S × A.
Moreover, for all (s, a) ∈ S0 X A0, our transition model po(∙∣s, a) is constructed based on p0(∙∣s, a),
i.e. we set p0 as follows:
p0(S0|S, a) = p00(S0|S, a),	∀(S, a, S0) ∈ S0 × A0 × S0,
p0(S0|S, a)	=	0,	∀(S, a,	S0)	∈ S0 ×	A0	×	St,
p0(S0|S, a)	=	0,	∀(S, a,	S0)	∈ S0 ×	At	×	S0,
and then we need only define the transition function p0(g|S, a) for each (S, a, g) ∈ S0 × At × St.
We note that as |St| = SL - 1, for each g ∈ St, we can pair it with a state S ∈ S0, and all pairs (g, S)
are mutually disjoint. Thus we use the notation gs ∈ St to denote a state in St that is paired with the
state S ∈ S0. Then for all (S, a) ∈ S0 × At, we define
p0(gs|S, a) =
cmin
(ι + 3ε)L-VMM 0,s(so)，
p0(S|S, a) = 1 -
p0(gs|S, a),
i.e. each new action a either transits S ∈ S0 to its pair gs ∈ St or remains at S itself.
We observe that VMo s(so) = VMM0 s(s。) for all S ∈ S0, and VMo s(so) = (1 + 3ε)L for all s ∈ St.
Then we define the adversarial examples of MDPs. Now we fix any state-action pair (S, a) ∈ S0 × At.
The adversarial MDP M(s,a) = hS, A,ps,a, c, S0i is constructed from M0, and the only difference
of M(s,a) and M0 lies on the transition probability at the fixed state-action pair (S, a), and we define
Ps,a(∙∣s, a) as follows:
ps,a(gs|S, a) =
cmin
L - VMM o,s(s0)
,ps,a(S|S, a) = 1 - ps,a(gs|S, a),
i.e. at state S, we slightly increase the probability of the action a to reach the pair state gs. And
for all the state-action pairs (s0, a0) other than (s, a), the transition probability ps,a(∙∣s0, a0) is the
23
Under review as a conference paper at ICLR 2022
same asP0(∙ls0, a0). Therefore,We have VMM(S a),gs(SO) = L, and VMM(S a),so(SO) = VMM.,s’(SO) for
all s0 ∈ S \ {gs}.
Finally, We define the family of our adversarial MDPs as {Mo} ∪ {M(s,a)}(s,a)∈sο×At. We note
that for each MDP M(s,a), its ∣S→∣ = Sl, and it satisfies Asmp.1. Also, for the MDP Mo, its
∣S→∣ = SL - 1, and it also satisfies Asmp. 1. Thus the family is valid for the relaxed autonomous
exploration problem.
NoW We give our proof of Thm. 5 through the adversarial family of MDPs. Here We use the techniques
of Thm. 7 in (Tarbouriech et al., 2020).
Proof. We denote by P(s*,a*)，P∏,M(s* a*)and E(s*,a*)，E∏,M(s* a*)the probability measure
and expectation in the MDP M(s*,a*) by following π and by Po and Eo the corresponding operators
in the MDP MO. We fix any algorithm (π, τ, K, {πs }s∈K) that solves the AX problem. We Will
prove that when working on the MDP Mo, the algorithm will cost at least Ω( LSLA log 1) samples
in expectation, i.e.
LSLA	1
Eo[τ ] = Ω(----2 log-),
cmin ε	δ
which yields that the lower bound of the total cost is Ω( LSL A log 1).
Now we fix the state-action pair (s*,a*) ∈ S0 X A*, and we denote gs* ∈ S * as the state paired with
s* ∈ S0. Also, we denote the random variable N(Ts °) as the number of samples that the algorithm
takes at the state-action pair (S, a) ∈ S × A. For any Fτ -measurable random variable Z taking
values in [0, 1], we have
EOhNii 7⅛ε2
cmin	cmin	∖
(I+ 3ε)L-VMM 0,s* (SOYL- VMM 0,s* (SO) J
=(b) KL PIoτ, P(Isτ*,a*)
(c)
≥ KL EO[Z],E(s*,a*)[Z] ,
≥(a) EO hN(τs*,a*)iKL
where (a) uses Lemma 21 and VMM, §* (so) ≤ 2; (b) uses Lemma 19; (c) uses Lemma 20.
For any policy π : S → ∆(A), we define π(a∣S) as the probability that the policy π
takes action a at state S. Then for any (S, a) ∈ S0 × A*, we define the event Zs,a =
1{The algorithm,s output satisfies gs ∈ K and ∏gs (a∣S) ≥ 2}. And we set the event Z = Zs*,α*.
We observe that as the algorithm (π, τ, K, {πs}s∈K) solves the AX problem, when working on the
MDP M(s*,a*), with probability at least 1 -δ, its output should satisfy gs* ∈ K and the expected cost
of the policy ∏gs* satisfies VsngS * (so) ≤ (1 + ε)L, which yields that ∏gs* (a*∣S*) ≥ 3. Therefore,
for any (s*, a*) ∈ S0 × A*, we have
P(s* ,a*) [Zs* ,a*] ≥ 1 - δ,
as the algorithm succeeds with probability at least 1 - δ.
Moreover, for any policy π we observe that for any two state-action pairs (S, a) and (S, a0), a 6= a0,
we cannot have both π(a∣S) ≥ 3 and π(a0∣S) ≥ 2. Therefore, We obtain that given any state S ∈ S0,
we have
X PO [Zs,a] ≤ 1,
a∈At
as any two events Zs,a and Zs,a0 (a 6= a0) are mutually exclusive.
We recall that we set Z = Zs*,a* , and we can obtain
kl(E0[Z], E(s*,a*) [Z]) = kl(PO [Zs*,a* ], P(s* ,0* ) [Zs* ,0* ])
24
Under review as a conference paper at ICLR 2022
(a)	1
≥ (I - PO [Zs*,a* ])log( ； p	f ) - log(2)
∖ 1 - P(s*,a*) [Zs*,a*] )
(b)	1
≥ (I- PO [Zs*,a* ])log (δ) - log⑵，
where (a) uses the fact that kl(p, q) ≥ (1 - p) log (ι⅛q) - log(2) for any p,q ∈ [0,1]; (b) uses that
P(s*,a*)[Zs*,a*] ≥ 1 - δ. Therefore, we have
EOhNTs*,a*J ≥ 72c . ε2((I- Po[Zs*,a*])log($) - Iog(Z))∙
We recall that Pa∈∕t Po [Zs,a] ≤ 1. ThUS we can obtain that for any s* ∈ S0,
X EOhN(S*,a*)i ≥ 72cLi2 ((lAt∣- 1) log(δ) - log(2)IAt∣).
a*∈At	mn	' /
Summing UP all the state-action pairs (s*,a*) ∈ S0 ×At, we can obtain the lower bound of the
sample complexity.
E0[τ ]=	X	Ns,a) ≥ X EθhN(S*,a*)i ≥ 7l⅞ ((AtI- 1) log( g )-log(2)IAt∣)
(s,a)∈S×A	(s*,a*)∈S0×At	min	' /
As ∣At∣ = [A/2] and ∣S0| = SL - 1, provided that L > 4, A > 4, 4 ≤ SL ≤ min{(A)bL2c, Sl},
0 < ε < 4, and δ < 焉,we can eventually prove that
LSL A	1
Eo[τ] = Ω( c ε2 log δ),
which yields the lower bound of the cumulative cost in Thm. 5.
□
H Lower Bounds for Multi-goal SSP
Here we formulize the lower bound CMSSP (L, S, ε, δ) for the multi-goal SSP problem. First we define
an algorithm for the multi-goal SSP problem as a triple (π, τ, {πs}s∈S), which means the algorithm
executes a history-dependent policy π, and returns a set of policies {πs}s∈S after sampling τ times.
Also, we allow πs to be non-deterministic policies.
Definition 8. Given any L≥ 1, 0 < ε ≤ 1, 0 < δ < 1, and S ∈ N, an algorithm (π, τ, {πs}s∈S)
is (ε, δ)-PAC for the multi-goal SSP problem within cumulative cost C, if for any MDP M =
hS, A, P, c, sOi with at most S states and A actions such that M satisfies Asmp. 1 and SL→ = S,
the algorithm always terminates after using τ samples, and the cumulative cost is always not
larger than C, and with probability at least 1 - δ, it returns a set of policies {πs}s∈S, such that
∀s ∈S,Vπs(so) ≤ V*(so)+ εL.
The cumulative cost CMSSP(L, S, ε, δ) for the multi-goal SSP problem is the least C ∈ R satisfying
that there exists an algorithm (π, τ, {πs}s∈S) within cumulative cost C which is (ε, δ)-PAC for
multi-goal SSP.
We remark that our constructed adversarial examples for the autonomous exploration problem can
also be applied to multi-goal SSP using the similar proof with Thm. 5, i.e. CMSSP(2L, 2S, ε∕2, δ) ≥
CAX(L, S, ε, δ). Thus we obtain the following lower bound for multi-goal SSP, which implies that
our Alg. 3 is also minimax for multi-goal SSP problem.
Theorem 6. Assume that L > 8, A > 4, 8 ≤ S ≤ (A)b2C, 0 < ε < 8, and δ <	. Then we have
CMSSP (L, S, ε, δ)
Ω( LSA
logδ).
25