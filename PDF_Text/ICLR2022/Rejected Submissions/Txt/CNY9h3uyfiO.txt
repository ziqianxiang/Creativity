Under review as a conference paper at ICLR 2022
Reward Shifting for Optimistic Exploration
and Conservative Exploitation
Anonymous authors
Paper under double-blind review
Ab stract
In this work, we study the simple yet universally applicable case of reward shap-
ing, the linear transformation, in value-based Deep Reinforcement Learning. We
show that reward shifting, as the simplest linear reward transformation, is equiva-
lent to changing initialization of the Q-function in function approximation. Based
on such an equivalence, we bring the key insight that a positive reward shift-
ing leads to conservative exploitation, while a negative reward shifting leads to
curiosity-driven exploration. In this case, a conservative exploitation improves
offline RL value estimation, and the optimistic value estimation benefits the ex-
ploration of online RL. We verify our insight on a range of tasks: (1) In offline
RL, the conservative exploitation leads to improved learning performance based
on off-the-shelf algorithms; (2) In online continuous control, multiple value func-
tions with different shifting constants can be used to trade-off between exploration
and exploitation thus improving learning efficiency; (3) In online RL with discrete
action space, a negative reward shifting brings an improvement over the previous
curiosity-based exploration method.
1	Introduction
While reward shaping is a well-established practice in reinforcement learning applications and has
a long-standing history (Randl0v & Alstr0m, 1998; Laud, 2004), specifying a certain reward to in-
centivize the learning agent requires domain knowledge and deep understanding of the task (Vinyals
et al., 2019; Akkaya et al., 2019; Berner et al., 2019; Elbarbari et al., 2021). Even with careful design
and tuning, learning with a shaped reward that intends to accelerate learning may on the contrary
hinder the learning performance by incurring the sub-optimal behaviors of the agent (Florensa et al.,
2017; Plappert et al., 2018). Although Ng et al. (1999) theoretically points the optimal policy will
keep unchanged under a special form of reward transformation, and in the later work of Wiewiora
et al. (2003) a framework is proposed to guide policies with prior knowledge under tabular setting,
the investigation of how it accommodates recent Deep Reinforcement Learning (DRL) algorithms
remains much less explored.
In this work, we focus on the simplest case of reward shaping, the linear transformation, in value-
based DRL (Sutton & Barto, 1998; Lillicrap et al., 2015; Mnih et al., 2015; Fujimoto et al., 2018b).
We start with understanding how such a specific kind of reward shaping works in value-based DRL
algorithms. We show that reward shifting, as the simplest reward transformation, is equivalent to
engineering initialization of the Q-function estimation, extending previous discovery of (Wiewiora
et al., 2003) to the function approximation settings. Based on such an equivalence, we bring the key
insight of this work: a positive reward shifting leads to conservative exploitation, while a negative
reward shifting leads to curiosity-driven exploration. We demonstrate the application of such an
insight to three downstream tasks: (1) for offline RL, we show that conservative exploitation can
lead to improved learning performance based on off-the-shelf algorithms; (2) for online RL setting,
we show multiple value functions with different reward shifting constants can be used as a trade-off
between exploration and exploitation, thus improving learning efficiency; (3) finally, we introduce a
simple yet crucial improvement over a prevailing curiosity-based exploration method, the Random
Network Distillation (Burda et al., 2018b), making it compatible with value-based DRL algorithms.
We evaluate our idea on various tasks, including both continuous and discrete action space control,
resulting in substantial improvements over previous baselines.
1
Under review as a conference paper at ICLR 2022
Our contributions can be summarized as follows
1.	we introduce the key insight that reward shifting is equivalent to diversified Q-value net-
work initialization, which can be used to boost both curiosity-driven exploration and con-
servative exploitation;
2.	motivated by our key insight, we present three scenarios where the reward shifting can
benefit, namely the offline conservative exploitation, the online sample-efficient RL, and
the curiosity-driven exploration;
3.	we demonstrate the effectiveness of the proposed method integrated with off-the-shelf base-
lines on both continuous and discrete control tasks.
2	Preliminaries
2.1	Online RL
We follow a standard MDP formulation in the online RL settings, i.e., M = {S, A, T, R, ρ0 , γ, T },
where S ⊂ Rd denotes the d-dim state space, A is the action space (note for discrete action space
|A| < ∞ and for continuous control |A| = ∞), T : S × A 7→ S is the transition dynamics,
R : S × A 7→ R is the reward function. ρ0 denotes the initial state distribution, i.e., ρ0 = p(s0). γ
is the discount factor and T is the episodic decision. Online RL considers the problem of learning
a policy π ∈ Π : S 7→ ∆A (or π ∈ Π : S 7→ A with a deterministic policy class), such that the
expected cumulative reward in the Markov decision process is maximized, i.e.,
T
∏ = arg max Eat 〜∏,st+ι 〜T,s° 〜ρ0 EYtrt(St,at),
π	t=0
(1)
In the online RL setting, an agent normally learns through trials and errors (Sutton & Barto, 1998),
either with an on-policy paradigm (Schulman et al., 2015; 2017; Cobbe et al., 2021) or an off-policy
manner (Mnih et al., 2015; Lillicrap et al., 2015; Wang et al., 2016; Haarnoja et al., 2018; Fujimoto
et al., 2018b). In this work, we focus on the off-policy value-based methods which are in general
more sample efficient. Specifically, our discussions assume the policy learning is based on a learned
Q-value function, that approximates the cumulative reward an agent can gain in the following part
of an episode. The Q-value function is defined as Q(st, at) = Eπ,T PτT=t γtr(sτ, aτ), and can be
approximated through the Bellman Operator BQ(s, a) = r(s, a) + γEQ(s0, a0). For value-based
methods, the (soft-)optimal policy is then produced by
πα(a|s)
(2)
where Q* is optimal Q-value function. We can also set the temperature parameter close as 0 to have
the deterministic policy class. Simplifying the notion we have π(s) = argmax。Q*(s,a). Algo-
rithms like DPG (Silver et al., 2014) can be used to address the intractable analytical argmax issue
arises in continuous action space. We develop our work on top of prevailing baseline algorithms of
DQN (Mnih et al., 2015), BCQ (Fujimoto et al., 2018a), and TD3 (Fujimoto et al., 2018b), and it
will be easy to extend to other baseline algorithms.
2.2	Exploration and the Curiosity-Driven Methods
One of the most important issues in online RL is the exploration-exploitation dilemma (Sutton &
Barto, 1998) that the agent must learn to exploit its accumulated knowledge on the task while ex-
ploring new states and actions. Plenty of previous works address the exploration problem from
various perspectives: In the tasks with discrete action space, count-based methods like Bellemare
et al. (2016); Ostrovski et al. (2017); Tang et al. (2017) are proposed to motivate the policy to ex-
plore more on under-explored states. Curiosity-driven methods are investigated by Houthooft et al.
(2016); Pathak et al. (2017); Burda et al. (2018a;b), where the intrinsic reward is designed as a
supplementary to the primal task reward for better exploration. Self-imitate approaches like Oh
et al. (2018); Ecoffet et al. (2019); Sun et al. (2019) repeat success trajectories but require extra
2
Under review as a conference paper at ICLR 2022
assumptions on the environment. The work of DIAYN and DADS (Eysenbach et al., 2018; Sharma
et al., 2019) show that various skills can be developed even without the primal extrinsic reward.
For continuous control tasks, OAC (Ciosek et al., 2019) improves the SAC (Haarnoja et al., 2018)
with informative action space noise based on the optimism in face of uncertainty (OFU) (Brafman
& Tennenholtz, 2002; Jaksch et al., 2010; Azar et al., 2017; Jin et al., 2018). GAC (Tessler et al.,
2019) addresses the exploration issue with a richer functional class for the policy.
In the recent work of Rashid et al. (2020), the problematic pessimistic initialization is addressed for
better exploration, yet the work focuses on specific settings of tabular and discrete control explo-
ration. In the work of Osband et al. (2016; 2018), ensemble models with diverse initialization and
randomized priors are used to resemble the insight of bootstrap sampling and facilitate better value
estimation, yet those methods are only applicable to discrete control tasks. Noted that although the
reward shifting can be regarded as a special case of these random priors, it can be distinguished
by not changing the optimal Q-value, and flexible to be plugged in to both continuous and discrete
control algorithms.
The Random Network Distillation (RND) (Burda et al., 2018b) propose to use the difference be-
tween a fixed neural network φ1 and a trainable network φ2 to represents the intrinsic reward, e.g.,
rint(s,a) = ∣Φ2(s,a) - Φι(s,a)∣,	(3)
when outputs of both networks are activated by a sigmoid function, and φ2 is optimized to approx-
imate φ1 for the visited (s, a) pairs. Henceforth, the value of rint (s, a) will decay to 0 when such
state-action pairs are visited frequently but remain high for seldom visited pairs.
In this work, we show that exploratory behavior can be achieved simply by shifting the reward func-
tion with a constant, thus our method is orthogonal to those previous approaches in the sense that
our intrinsic exploration behavior is motivated by function approximation error. We demonstrate
such an insight by showing that RND, with its original design, is not suitable for value-based meth-
ods in developing exploratory behaviors, but integrating RND with a shifted reward function can
remarkably improve the learning performance.
2.3	Offline RL
The offline RL, also known as batch-RL, focuses on the problems where the interaction with the
environment is impossible, and the policy can only be optimized based on the logged dataset. In
those tasks, a fixed buffer B = {si, ai, ri, s0i}i=[N] is provided. As the agent in the offline RL setting
can not correct its potentially biased knowledge through interactions, the most important issue is to
address the extrapolation error (Fujimoto et al., 2018a) induced by distributional mismatch (Kumar
et al., 2019). To address such an issue, a series of algorithms optimize the policy learning under the
constraint of distributional similarity (Kumar et al., 2019; Wu et al., 2019; Siegel et al., 2020).
Bharadhwaj et al. (2020) proposed CQL to solve the offline RL tasks with a conservative value es-
timation. Specifically, CQL learns the Q-value estimation by jointly maximizing the Q-values of
actions sampled from the behavior offline dataset and minimizing the Q-values of actions sampled
with pre-defined prior distributions (e.g., uniform distribution over the action space). As we will
show in this work, an alternative approach to have a lower bound for the optimal Q-value func-
tion is to use an appropriately shifted reward function. This idea leads to the direct application of
our proposed framework in the offline setting. In general, reward shift can be plugged in many
distribution-matching offline-RL algorithms (Fujimoto et al., 2018a; Kumar et al., 2019; Wu et al.,
2019; Siegel et al., 2020) to further improve the performance with conservative Q-value estimation.
3	A Motivating Example
We start with a motivating example that may look counterintuitive: in the prevailing contin-
uous control environment of Humanoid, a three-dimensional bipedal robot is simulated based
on the MuJoCo engine. The learning objective is to control the robot to walk forward as fast
as possible, without falling over. There are two types of positive reward signals: (1) to en-
courage forward-moving, i.e., the reward is proportional to the horizontal displacement, and
3
Under review as a conference paper at ICLR 2022
(2) to encourage the robot to avoid falling over, i.e., a binary
alive bonus whether the mass center of the robot is higher than
some certain threshold. Although the design of (2) is intended
to provide a curriculum for the agent, we show in Figure 1
that such a curriculum, on the contrary, hinders the learning
performance of such a bipedal robot to run fast.
The orange curve, TD3 w/o Alive Bonus, shows the smoothed
learning curve of a TD3 agent trained with the alive bonus set
to be 0; the blue curve, TD3 w/ Alive Bonus, shows the av-
eraged learning curve of a vanilla TD3 agent trained with the
alive bonus set to be 5 as default. Noted that for both ap-
proaches, we use identically the same primal task in policy
evaluation to make sure the curves are comparable. In gen-
eral curriculum learning improves learning efficiency (Graves
et al., 2017; Matiisen et al., 2019; Portelas et al., 2020), how-
Figure 1: Humanoid agents trained
with a linear reward shift drastically
gain asymptotic performance.
ever, in the environment of Humanoid, such a curriculum design, which aims at helping the agent
learning to run after knowing how to avoid falling over, hinders the learning efficiency and asymp-
totic performance.
Remark 1. Given an MDP M = {S, A, T, R, ρ0, γ, T}, where |A| < ∞, scaling the reward
function with linear transformation, i.e., Rk,b = k ∙R + b, ∀k > 0,b ∈ R,do not change the optimal
policy induced by
π*(s) = arg max Qk b(s, a) = arg max kQ*(s, a) + -γ~- = arg max Q*(s, a),	(4)
Remark 2. Given an MDP M = {S, A, T, R, ρ0, γ, T}, where |A| = ∞, scaling the reward
function with linear transformation, i.e., Rk,b = k ∙R + b, ∀k > 0, b ∈ R,do not change the optimal
policy induced by deterministic policy gradient (Silver et al., 2014), given proper learning rate:
RB J (μθ ) = Est [VaQ*(st, atXat=μθ(stNθ Mθ (St)] = Est ∖^aQk,b(4 st, at)∖αt=μθ(st)^ θ Mθ (St)]/k,
(5)
In the following, we will focus on the scenarios when k = 1 to avoid the trivial (though maybe
empirically important) discussions on different learning rates. We reveal the importance of selecting
the universal bias term, i.e., b in the reward function, through the lens of initialization priors in
function approximation. We further show such a bias term can be utilized not only in the online RL
settings to improve learning efficiency but also in the offline RL settings to conduct conservative
exploitation with batched data.
4 Shifted Priors for Q-Value Estimation
4.1 Reward Shift Equals to Different Initialization
We start by introducing the central idea of this work: reward shift equals different initialization.
Specifically, we illustrate the basic idea with Figure 2. The black curves denote the primal optimal
Q-value functions (e.g., Q*), and the red curves denote the optimal Q-value functions with shifted
reward r0 = r + b+, where b+ > 0 is a positive constant bias, and we will show a different
choice of such a bias leads to a different motivation in Q-value estimation. The yellow lines denote
Q-value estimators, e.g., neural network predictions of the corresponding Q-values. (a) shift the
reward function with a positive bias term b+ will lead to an uniformly increased Q-value function,
+
namely Qb+ = Q* + Ib-Y, during learning, a neural network estimator Q initialized with Qo ≈ 0 is
optimized to approximate the Q-value functions (e.g., through Temporal Difference or Monte Carlo
estimation).
(b) for any value-based RL algorithm, the value optimization step can be regarded as a function
F that minimizes the difference between the estimated Q-value function Qt and the optimal one
Qb, given the interaction experience with the environment (e.g., a replay buffer B for off-policy
methods).
4
Under review as a conference paper at ICLR 2022
(a)
Q	F: Ot → Q*| {s,a,r,s'}[M]
Figure 2: Illustrative figure for conservative exploitation, with a positive constant bias added to the
reward function.
(c)	similarly, the optimizer given the same interactive experience (e.g., replay buffer B) will learn to
minimize the difference between Q-value function Qb+,t and the optimal one Q〉, after re-labeling
the rewards in the buffer by r0 = r + b+ .
(d)	according to Remark 2, the optimization conducted in (c) is equivalent to (b) with the neural
network Q-value estimator initialized at Qo ≈ 0 - 1-+γ, rather than Qo ≈ 0. i.e., by shifting
the reward with proper positive value b+, we are able to initialize the Q-value network that lower-
bounded the optimal Q-value.
To summarize, shifting the reward function with a positive constant is equivalent to initializing the
value function network with a smaller value, hence during training, the Q-value of unseen state-
action pair is far lower than the optimal value and hence will not be selected in policy update,
leading to conservative learning behavior.
4.2	Conservative Exploitation
We begin with a natural application in the offline setting, where a policy doesn’t interact with the en-
vironment and only learns through a logged data set collected from an unknown behavior policy πβ,
which can either be an expert that generates high-quality solutions to the task (Fujimoto et al., 2018a;
Zhang et al., 2020; Fu et al., 2020) or a non-expert that provides actions that are sub-optimal (Wu
et al., 2019; Kumar et al., 2019; Agarwal et al., 2020; Fu et al., 2020; Jarrett et al., 2021) or a mixture
of both (Bharadhwaj et al., 2020).
Based on the basic idea we presented in Section 4.1, we introduce a simple yet effective approach
for conservative Q-value estimation that learns to heuristically put a lower bound on the optimal
Q-value function in batch settings. Hark back to Figure 2(a), a positive constant b+ added to the
reward function will lead to a uniformly (positively) shifted optimal Q-value function, and the gap
between the primal Q-value function and the new one is ιb+γ. Optimizing the Q-value function with
logged data (e.g., a fixed replay buffer) will minimize the difference between the predicted value
and the optimal value with observed data. For the unobserved data point (in the state-action space
S × A), the near-zero initialization guarantees the prediction is lower than the optimal Q-value, thus
conservative exploitation can be conducted with such a value function.
5
Under review as a conference paper at ICLR 2022
Q
Qo ≈ 0
IA
⅛UB,t
Qv= Q* +1-7
Figure 3: Illustrative figure for curiosity-driven exploration with a negative shifted reward
4.3	(Curiosity-Driven) Optimistic Exploration
On the other hand, if we shift the reward function to the negative side, it is equivalent to optimistic
initialization. Figure 3 (a-b) illustrate how adding a negative bias leads to curiosity-driven explo-
ration: while adding a negative constant value b- on the reward function lead to negatively shifted
optimal Q-value function Qb- (Figure 3 (a)), minimizing the difference between a Q-value approx-
imator and the optimal Q-VaIUe will enable calculating an upper-bound estimation for Qb, as shown
in Figure 3(b). With sufficiently large b- (so that b- larger than the maximal value of any s, a-pair),
such an upper bound ofQb can be used to conduct curiosity-driven exploration. Intuitively, initializ-
ing a value network that predicts value larger or equal than the true value will lead to curiosity-driven
exploration, as any visited state will be assigned a low value and the policy that learns to perform
the action with a higher value will tend to choose novel actions.
Based on the discoveries above that (1) a positive constant shift added to the reward function can be
used for conservative policy update, as shown in Figure 3(c) and (2) a negative constant shift added
to the reward function can be used for curiosity-driven exploration, as shown in Figure 3(b). We are
ready to access both the upper bound, i.e., the optimistic estimation with b-, and the lower bound,
i.e., the conservative estimation with b+ of the optimal value function. Henceforth, we are ready
to introduce our sample-efficient algorithms for both continuous control and discrete action space
respectively. We propose a practical algorithm for general continuous control in Sec. 4.3.1, while
focusing on a special class of curiosity-driven exploration method, the RND, in Sec. 4.3.2.
4.3.1	Sample-Efficient Continuous Control with Reward Shift
Based on the principle of optimism on the face of uncertainty (OFU), we introduce a exploration
bonus that manifests the uncertainty of the Q-value function. Specifically, the basic idea starts from
integrating optimistic exploration with conservative exploitation, i.e.,
Q(s, a) = QLB,t(s, a) + β[QUB,t(s, a) - QLB,t(s, a)]
b+	b-
=(I -⑶(Qb+,t(S, a) - J - Y ) + β(Qb-,t(S, a) - J - Y )	(6)
=(1 - β)Qb+,t(s, a) + βQb-,t(s, a) - (I- β)b+γ+ βb-,
where the second term with coefficient β denotes exploration bonus that is composed of uncertainty.
For those under-explored state-action pairs, i.e., extremely out-of-distribution samples for our neu-
ral network, both Qb+,t(S, a) and Qb-,t(S, a) will give near-zero predictions as a consequence of
initialization (detailed implications are provided in Appendix B). henceforth, the explorative bonus
becomes 一(1一β)b+γ+βb . Note this is equivalent to applying another constant reward shift with
value of cr = (1 - β )b+ + βb- .
For the explored state-action pairs close to the samples from the replay buffer, we have
Proposition 1. Assuming we have access to an unbiased estimator for the optimal value function
Qb, e.g., with Monte-Carlo estimation Qb = E Pt γrt, and the optimization is based on minimizing
the MSE between the unbiased estimator and the function approximator, i.e., e2 = (Qt 一 Qb)2,
6
Under review as a conference paper at ICLR 2022
Qt = Qt-ι 一 2η((Qt-ι 一 Q*), then combining the linear combination in Equation (6) is equivalent
to using a linear combination of the constants with value of cr = (1 - β)b+ + βb-.
According to Proposition 1, a grid search for trading-off between the three hyper-parameters: the
exploration bias b- , the exploitation bias b+ and the coefficient in Equation (6) is trivial as they only
lead to a linear combination as cr = (1 一 β)b+ + βb-. In principle, a meta-learner can be trained
to monitor the learning process and select a proper constant automatically (Graves et al., 2017;
Matiisen et al., 2019; Portelas et al., 2020). In this work we focus on a simple yet effective uniform
sampling strategy from multiple shift constants, which is shown as a strong baseline in Graves et al.
(2017); Matiisen et al. (2019), and leave more complicated meta-learner-based approaches in future
investigation.
Specifically, we use multiple Q-networks to learn with transition tuples (s, a, r, s0) sampled from
the identical buffer that collects the policy’s interaction history with the environment. For each
Q-network, the primal r is replaced with a new reward with shifted constant bias for temporal
difference updates. Those learned Q-networks are sampled uniformly during the training of policy
network. It is worth noting that our approach only requires post-hoc revision of the primal reward
function, rather than interacting with the environment multiple times to collect samples for each
value network.
4.3.2 Improving Value-Based Curiosity-Driven Exploration
The discussion above casts the curiosity-driven exploration method as a special case in the reward
shift equals to initialization perspective: exploration with RND (Burda et al., 2018b) is equivalent to
selecting b+ = 0, i.e., using the primal task reward for exploitation, and using b- as a fixed random
function over S× A, i.e., b- (s, a) = ∣φ1,0(s, a) 一 φ2 (s, a)∣2, ∀s, a, where φ1,0 and φ2 are two neural
networks with different random initializations. While φ1,0 is learnable, φ2 is set to be fixed during
learning as a random curiosity prior (Osband et al., 2018). Without loss of generality, we can assume
φ2 = 1 as a constant initialization, and φ1,0 = 0. Then the exploration behavior is fully controlled by
the scale of external reward, which is set to be 1.0 with external reward clipped to [一1, 1] in (Burda
etal., 2018b). The final equivalent objective in this case is touse Cr = ∖φι,t(s, a)-1∣2,t = 0,1, 2,....
Specially, φ1,0 = 0 and cr = 1 at beginning.
According to our analysis in the previous section, such a positive shift directly leads to conservative
behaviors in Q-value estimation, therefore may hinder the exploration behaviors at the beginning,
i.e., when ∖φ1,t 一 1∖	0. The exploration bonus in RND only becomes effective when visiting
seldom visited states after the predictions of φ1,t of frequently-visited states are close to 1, and
hence encouraging stepping into those novel states and discover new knowledge. To overcome
the conservative tendency induced by Q-value estimation in RND, we propose to use b-(s, a) =
∖φ1,0(s, a) 一 φ2(s, a)∖2 一 I, ∀s, a, where I is a positive constant (e.g., I = maxs,a ∖φ1,0(s, a) 一
φ2(s, a)∖2) that assures b-(s, a) is negative-initialized for optimistic exploratory behaviors.
It is worth noting that the curiosity introduced by exploratory bonuses like RND focuses on increas-
ing the temporary Q-value of rarely visited states, which still relies on the visitation behavior itself
in random exploration. i.e., without stepping into a non-frequent visited state, the agent will never
receive the novelty bonus based on the state. On the other hand, the mechanism of shifting the
reward by a constant is an universal optimistic exploration bonus as the curiosity is fused into the
initialization across the entire state-action support.
5	Experiments
5.1	Offline Reinforcement Learning with Conservative Q-value Estimation
We first demonstrate the proposed method in the offline RL setting. As has been discussed in Sec-
tion 4.2, shifting the primal reward function with a positive constant provides a natural way of
conservative exploitation. By adding a positive reward shift, any visited (s, a) saved in the given
replay buffer B (logged dataset) will be assigned a large reward, while under-explored (s, a) pairs
remain have the low values due to initialization, as illustrated in Figure 2 (d). Although in general
our proposed method can be plugged in to any off-the-shelf offline RL algorithm, in this work we
demonstrate the effectiveness of such a conservative Q-value estimation based on BCQ (Fujimoto
7
Under review as a conference paper at ICLR 2022
U-ImBU。-POSldW
Hopper Imitate BCQ
Hopper Expert BCQ
O 200000	400000	600000 8(KKKK) 10(KKKK)
# Optimization Step
Walker Medium CQL
---Vanilla
一Vanilla+Pos.(Ours)
一Vanilla+Neg.(Contrast)
O 5OO∞ IOOOOT 150000 2000∞ 250000 300000
# Optimization Step
Hopper Medium CQL
O 500000 1OOOOOO15OOOOO2OOOOOO25OOOOO3OOOOOO
# Optimization Step
0	500000 10000001500000 2000000 2500000 3000000
# Optimization Step
Figure 4: Results on offline RL settings. We verify our key insight that a positive reward shift equals
to conservative exploitation thus helps offline value estimation, while a negative reward shift leads
to worse performance.
et al., 2018a) and CQL (Bharadhwaj et al., 2020), i.e., both distribution-matching approach and
conservative value estimation approaches in offline RL.
To verify our insight, we experiment with both positive reward shift (Pos.) and negative reward shift
(Neg.), added on either BCQ or CQL. Figure 4 shows our experiment results. We experiment with
both the dataset generated in BCQ (Hopper Imitate BCQ) and the dataset used in CQL (Fu et al.,
2020) (others), and find in our experiments that learning with the CQL dataset is much more stable.
the first two figures show results with BCQ as the backbone algorithm, where the former shows
results on the vanilla BCQ dataset, while the latter shows results on the CQL dataset. The following
three experiments in Figure 4 use CQL as the backbone. In all experiments, using a positive reward
shift leads to improved learning performance, while a negative reward shift leads to performance
decay, as expected. Implementation details and ablation study can be found in Appendix C.1.
5.2	Online Reinforcement Learning with Randomized Priors
We then conduct experiments on the online RL settings. We demonstrate our proposed method in
the MuJoCo locomotion benchmarks. As our implementation is based on TD3, we use TD3-based
variants as our baselines: The TD3 is trained with default settings according to Fujimoto et al.
(2018b). We also include Ensemble TD3 and Bootstrapped TD3 as baselines due to they are
similar to our work in using multiple Q-networks in value estimation. We follow Osband et al.
(2016) but extend it to the continuous control settings. Noting that in the continuous control setting,
the argmax operator is approximated by the policy network, multiple policy networks are needed to
cooperate with the multiple bootstrapped Q-value networks. Otherwise, multiple Q-value networks
are not independent of each other thus breaking the condition of bootstrapped value estimation. The
Ensemble TD3 presents the baseline performance when multiple Q-networks are used for value
estimation in TD3, which also works as an ablation of our method when all reward shift priors are
set to be 0. As has been illustrated in Sec. 4.3.1, learning with different reward shifting values
is equivalent to learning with optimistic or conservative initialization. In our method of Random
Reward Shift (RRS), we use 3 Q-networks with different priors. In our experiments, we found
±0.5, 0 works universally as a default setting. Though, further investigation on hyper-parameter
may help to further improve the performance.
Results are shown in Figure 5. RRS outperforms the vanilla TD3 in all five environments and
outperforms all baseline methods in most tasks. In all experiments, we use 3 Q-networks for a fair
comparison. Note that there is a trade-off between computational complexity and sample efficiency,
i.e., using more Q-networks may further improve the performance at the cost of more computational
8
Under review as a conference paper at ICLR 2022
Hopper-v2	Walker2d-v2	Ant-v2
---Bootstrapped TD3
RRS (Ouns)
--- EnsembleTD3
——TO3
Figure 5: Results on continuous control tasks, the method of Random Reward Shift (RRS) outper-
forms its value-based baselines in most environments.
expensive, as reported in (Osband et al., 2016). More implementation details, pseudo code of RRS,
and ablation studies can be bound in Appendix C.2.
5.3	Optimistic Random Network Distillation
Finally, we experiment with five discrete exploration tasks, namely the MountainCar-v0, and four
navigation tasks of MiniGrid suite (Chevalier-Boisvert et al., 2018), namely the task of Empty-
Random, MultiRoom, and FourRooms, to verify our insight on improving RND for value-based
curiosity-driven exploration. More environment details are provided in the Appendix C.3.
We compare the vanilla DQN, vanilla RND, as well as improved DQN and RND according to our
proposed insight. DQN -0.5 indicates the results with a reward shifting of -0.5. Comparing DQN -
0.5 with the vanilla DQN, our insight of negative reward shift leads to curiosity-driven exploration is
again verified. RND -1.0 indicates the results when a reward shifting of -1.0 is added to RND based
on DQN. RND -1.0 improves the performance of RND and DQN in most environments, showing
the effectiveness of the RND-based exploration bonus. Note that for a fair comparison, RND -
1.0 should be compared with the vanilla DQN as the -1.0 reward shift just cancels the positive
exploration bonus introduced by RND.
Moreover, we can further improve RND by equipping it with reward-shifting-based curiosity-driven
exploration. RND -1.5 (i.e., RND with a -1.5 reward shift) can be compared with the DQN -0.5,
as both receive an -0.5 exploration bonus for unseen states. We find in all experiments that our
negative reward shift can remarkably improve exploration, not only working with RND to improve
its performance but also work effectively in isolation.
6 Conclusion
In this work, we study how reward shifting affects policy learning in value-based deep reinforce-
ment learning algorithms. Although constant reward shifting should not change the optimal policy
induced by the optimal value function, in practice such a constant shift does affect the function ap-
proximation. Our detailed analysis manifests the fact that a constant reward shift is equivalent to us-
ing different initialization in the value function approximation. Specifically, we show that a negative
reward shift leads to curiosity-driven exploration, while a positive reward shift helps conservative
exploitation. The proposed idea is then verified through a variety of application scenarios, includ-
ing offline RL, sample-efficient continuous control, and curiosity-driven exploration in value-based
methods. Thus reward shifting is a simple yet effective technique that deserves further investigation.
9
Under review as a conference paper at ICLR 2022
0	1000	2000	3000	4000	5000
# Episodes
MiniG rid-FourRooms-7x7-vO
u」n】①aypos-d山
MιnιGrιd-MultιRoom-N2-S4-vO
0	100	200	300	400	500	0	2000	4000	6000	8000	10000
# Episodes	# Episodes
MiniGrid-Fou rRooms-9x9-vO
——DQN
DQN-0.5
RND
——RND -1.0
——RND -1.5
O 2000 4000 6000 8000 IOOOO 12000 14000	O 5000 IOOOO 15000 20000 25000 30000 35000
# Episodes	# Episodes
Figure 6: Value-based RND with shifted prior: Plugging the vanilla RND into DQN is not well-
motivated according to our analysis in Section 4.3.2. The insight of equivalence between negative
reward shifting and curiosity-driven exploration motivates us to shift the vanilla RND with a con-
stant, which drastically improves the performance of RND when working with DQN.
Ethics S tatement
In this work, we study how the linear reward shaping in reinforcement learning benefits offline con-
servative estimation, online continuous control, and curiosity-driven exploration in discrete control
tasks. Although in our work we experimented on a variety of benchmark environments, there are
plenty of real-world applications: Improving the learning stability as well as the asymptotic per-
formance of offline RL empowers the application of RL in scenarios where interaction with the
environment is extremely expensive or unethical, e.g., healthcare, robotics, finance, etc. Moreover,
our discussions on continuous and discrete control tasks open up a promising direction in pursuance
of sample-efficient learning without introducing heavy extra computational burdens. Large-scale
applications of our method can help improving efficiency in exploration, which is always conducted
through computing additional curiosity networks explicitly in previous works.
Reproducibility S tatement
We include our code in the supplementary materials. More details for our experiments on offline
RL, continuous control and discrete control can be found in Appendix C.1, Appendix C.2, and
Appendix C.3, separately.
References
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline
reinforcement learning. In Intemational COnference on Machine Learning, pp. 1θ4-114. PMLR,
2020.
Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron,
Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving rubik’s cube with a
robot hand. arXiv Preprint arXiv:1910.07113, 2019.
Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforce-
ment learning. In Proceedings of the 34th InternatiOnal COnference on Machine Learning-Volume
70,pp. 263-272. JMLR. org, 2017.
10
Under review as a conference paper at ICLR 2022
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.
Unifying count-based exploration and intrinsic motivation. Advances in neural information
processing systems, 29:1471-1479, 2016.
Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy
Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large
scale deep reinforcement learning. arXiv Preprint arXiv:1912.06680, 2019.
Homanga Bharadhwaj, Aviral Kumar, Nicholas Rhinehart, Sergey Levine, Florian Shkurti, and Ani-
mesh Garg. Conservative safety critics for exploration. arXiv Preprint arXiv:2010.14497, 2020.
Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for near-
optimal reinforcement learning. JOurnaI OfMaChine Learning ReSearch, 3(OCt):213-231, 2002.
Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and Alexei A Efros.
Large-scale study of curiosity-driven learning. arXiv PrePrint arXiv:1808.04355, 2018a.
Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network
distillation. arXiv PrePrint arXiv:1810.12894, 2018b.
Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environment
for openai gym. https://github.com/maximecb/gym- minigrid, 2018.
Kamil Ciosek, Quan Vuong, Robert Loftin, and Katja Hofmann. Better exploration with optimistic
actor critic. In AdVanCeS in NeUraI InfOrmatiOn PrOCeSSing Systems, pp. 1785-1796, 2019.
Karl W Cobbe, Jacob Hilton, Oleg Klimov, and John Schulman. Phasic policy gradient. In
International COnferenCe on MaChine Learning, pp. 2020-2027. PMLR, 2021.
Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. Go-explore: a
new approach for hard-exploration problems. arXiv PrePrint arXiv:1901.10995, 2019.
Mahmoud Elbarbari, Kyriakos Efthymiadis, Bram Vanderborght, and Ann Nowe. Ltlf-based reward
shaping for reinforcement learning. In AdaPtiVe and Learning AgentS WOrkShOP 2021, 2021.
Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need:
Learning skills without a reward function. arXiv PrePrint arXiv:1802.06070, 2018.
Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, and Pieter Abbeel. Reverse cur-
riculum generation for reinforcement learning. In COnferenCe on robot Iearning, pp. 482U95.
PMLR, 2017.
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep
data-driven reinforcement learning. arXiv PrePrint arXiv:2004.07219, 2020.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. arXiv PrePrint arXiv:1812.02900, 2018a.
Scott Fujimoto, Herke Van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. arXiv PrePrint arXiv:1802.09477, 2018b.
Alex Graves, Marc G Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu. Automated
curriculum learning for neural networks. In international COnferenCe on machine Iearning, pp.
1311-1320. PMLR, 2017.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-
policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv PrePrint
arXiv:1801.01290, 2018.
Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Variational
information maximizing exploration. 2016.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. JOUmaI OfMaChine Learning ReSearch, 11(Apr):1563-1600, 2010.
11
Under review as a conference paper at ICLR 2022
Daniel Jarrett, Alihan HUyUk, and Mihaela Van Der Schaar. Inverse decision modeling: Learning
interpretable representations of behavior. In IntemationaI Conference on Machine Learning, pp.
4755-4771. PMLR, 2021.
Chi Jin, ZeyUan Allen-ZhU, Sebastien BUbeck, and Michael I Jordan. Is q-learning provably effi-
cient? In Advances in Neural Information ProceSSing Systems, pp. 4863T873, 2018.
Aviral KUmar, JUstin FU, George TUcker, and Sergey Levine. Stabilizing off-policy q-learning via
bootstrapping error reduction. arXiv PrePrint arXiv:1906.00949, 2019.
Adam Daniel Laud. TheOry and application of reward ShaPing in reinforcement learning. University
of Illinois at Urbana-Champaign, 2004.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
PrePrint arXiv:1509.02971, 2015.
Tambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman. Teacher-student curriculum learn-
ing. IEEE transactions on neural networks and Iearning systems, 31(9):3732-3740, 2019.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529-533, 2015.
Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations:
Theory and application to reward shaping. In Icml, volume 99, pp. 278-287, 1999.
Junhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee. Self-imitation learning. arXiv PrePrint
arXiv:1806.05635, 2018.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped dqn. In AdVanCeS in neural information PrOCeSSing systems, pp. 4026—4034, 2016.
Ian Osband, John Aslanides, and Albin Cassirer. Randomized prior functions for deep reinforcement
learning. In AdVanCeS in NeUraI InfOrmatiOn PrOCeSSing Systems, pp. 8617-8629, 2018.
Georg Ostrovski, Marc G Bellemare, Aaron Oord, and Remi Munos. Count-based exploration with
neural density models. In InternatiOnal COnferenCe on machine Iearning, pp. 2721-2730. PMLR,
2017.
Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration
by self-supervised prediction. In PrOCeedingS of the IEEE COnferenCe on COmPUter ViSiOn and
Pattern ReCOgnitiOn WOrkShops, pp. 16-17, 2017.
Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Pow-
ell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforce-
ment learning: Challenging robotics environments and request for research. arXiv PrePrint
arXiv:1802.09464, 2018.
Remy Portelas, CedriC Colas, Katja Hofmann, and Pierre-Yves Oudeyer. Teacher algorithms for
curriculum learning of deep rl in continuously parameterized environments. In COnference on
RobotLearning, pp. 835-853. PMLR, 2020.
Jette Randl0v and Preben Alstr0m. Learning to drive a bicycle using reinforcement learning and
shaping. In ICML, volume 98, pp. 463T71. Citeseer, 1998.
Tabish Rashid, Bei Peng, Wendelin Boehmer, and Shimon Whiteson. Optimistic exploration even
with a pessimistic initialisation. arXiv PrePrint arXiv:2002.12174, 2020.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International COnference on machine Iearning, pp. 1889-1897, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv PrePrint arXiv:1707.06347, 2017.
12
Under review as a conference paper at ICLR 2022
Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-aware
unsupervised discovery of skills. arXiv Preprint arXiv:1907.01657, 2019.
Noah Y Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Ne-
unert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing
What worked: Behavioral modelling priors for offline reinforcement learning. arXiv Preprint
arXiv:2002.08396, 2020.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In International conference on machine Iearning, pp.
387-395. PMLR, 2014.
Hao Sun, Zhizhong Li, Xiaotong Liu, Bolei Zhou, and Dahua Lin. Policy continuation with hind-
sight inverse dynamics. In AdVanceS in Neural InfOrmatiOn PrOCeSSing Systems, pp. 10265-
10275, 2019.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. 1998.
Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman,
Filip De Turck, and Pieter Abbeel. # exploration: A study of count-based exploration for deep
reinforcement learning. In 31st Conference on Neural InfOrmatiOn PrOCeSSing SyStemS (NIPS),
volume 30, pp. 1-18, 2017.
Chen Tessler, Guy Tennenholtz, and Shie Mannor. Distributional policy optimization: An alternative
approach for continuous control. arXiv PrePrint arXiv:1905.09855, 2019.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, Juny-
oung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster
level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.
Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu,
and Nando de Freitas. Sample efficient actor-critic with experience replay. arXiv PrePrint
arXiv:1611.01224, 2016.
Eric Wiewiora, Garrison W Cottrell, and Charles Elkan. Principled methods for advising reinforce-
ment learning agents. In PrOCeedingS of the 20th InternatiOnal COnference on MaChine Learning
(ICML-03), pp. 792-799, 2003.
Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.
arXiv PrePrint arXiv:1911.11361, 2019.
Chi Zhang, Sanmukh Rao Kuppannagari, and Viktor Prasanna. Brac+: Going deeper with behavior
regularized offline reinforcement learning. 2020.
A Proof of Proposition 1
Proof. the estimated Q-value Q(s, a) is composed by the two estimators with function approxima-
+
tion error, defined as q+ (s, a) = Qb+,t(s, a) - ιb-γ - Q* (s, a), and Eb- (s, a) = Qb-,t(s, a)-
1b-γ - q*(S, a).
(1 - β)EA,t + β EB,t
=2η(1 - β)Q +(1 - β)(1 - 2η)QA,t + 2ηβQ + B(1 - 2η)QB,t
=2ηQ +(1 - 2η)[(1 - β)Q A,t + βQ B,t]
=2ηQ * + (1- 2η)[(1 - β)(1 - 2η)tQ a,o + j1-f Yn)t Q* + β(1 - 2η)tQQ b,o + 1 _；：—加丫 Q *]
=2ηQ* + (1 - 2η)[(1 - 2η)t((1 - β)Qa,。+ BQbQ + 1 - ] 2疗 Q*]
EC,t
(7)
13
Under review as a conference paper at ICLR 2022
1	— /r <1∖ A . Ci-1	1,1 1 , 1 ∙	∙ K	入	入	∙ 1 , ∙ 1 ∙ ∙ , ∙ 1 ∙
where C = (1 - β)A + βB and the last line requires QA,0 = QB,0 = QC,0 are identical initializa-
tion.
With this notion, Equation (6) can be re-written as
*
Q(s, a) = Q (s, a) + (1 - β)eb+(s, a) + β6b- (s, a)
= Q* (s, a) + (1-β)b++βb- (s, a)	(8)
= Q* (s, a) + ecr (s, a)
where the second line relies on the linear assumption of the approximation error (1 - β)eb+ (s, a) +
βeb- (s, a). We further have (1 - β)Qb+ + βQb- = Q(1-β)b++βb- and Q(s, a) = Qcr (s, a),
telling us that trading-off between the constant b- used for exploration and the constant b+ used
for exploitation with the coefficient β is equivalent to use another constant with value of cr =
(1 - β)b+ + βb-.
□
B Implications of Assumption in Sec. 4.3.1
In our main text, the estimated values for extremely o.o.d. samples are assumed to be near zeros.
We provide detailed implications and explanations in this section.
On the one hand, it’s clear that such an assumption holds for the tabular settings, that un-visited
state-action pairs have the value in tabular initialization.
On the other hand, we acknowledge it as a mild assumption that there always exists o.o.d. samples
that have the Q-values near zero for function approximation settings. Interpolation between those
o.o.d. samples and other state-action pairs will clearly lead to an “in-between” value estimation,
which in practice can be achieved with properly regularized neural networks.
The key insight we want to emphasize in Sec. 4.3.1 is that for frequently visited state-action pairs,
the value discrepancy with different initialization are small, while for seldomly-visited state-action
pairs, the discrepancy are relatively large, enabling the usage of such discrepancy as exploration
bonus.
C Implementation Details And Ablation Studies
Hardware and Training Time We experiment on a server with 8 TITAN X GPUs and 32 Intel(R)
E5-2640 CPUs. In general, shifting the reward does not introduce further computation burden except
in the continuous control tasks, our method of Random Reward Shift (RRS) requires two additional
Q-value networks. In our PyTorch-based implementation, those additional networks can be easily
implemented and optimized in a parallel manner, and the extra computational burden is equivalent
to using a √3 times wider neural network during optimization. It is worth noting that RRS is
computationally much cheaper than the Bootstrapped TD3, where additional policy networks are
also needed.
Network Structure Our implementation of TD3, BCQ and CQL are based on code released by
the authors, without changing hyper-parameters. We implement DQN based on a 3-layer fully
connected neural network with 64 hidden units for the Q-value function, using ReLU and linear
activation respectively. We use the Adam optimizer with learning rate of 0.001, and use an epsilon-
greedy approach as naive exploration strategy. In our RND, we use two 4-layer fully connected
neural networks with 512 units and ReLU activation in each hidden layer, and a softmax activation
for the output layer. Adam optimizer is used for the optimization of the RND networks with learning
rate 0.0001.
Our code is provided in the supplementary materials, and will be made public available.
14
Under review as a conference paper at ICLR 2022
C.1 Offline RL
In our experiments, we use a fixed dataset with 10k offline trainsition tuples for offline RL learning.
Our implementation of BCQ and CQL are both based on the code provided by the authors. The only
change we made to verify our insight is to shift the reward by a constant. In most environments, we
find r0 = r + 8 provides good enough performance. While in Hopper Medium CQL we find using a
smaller positive reward shift r0 = r + 1 works better than r0 = r + 8, and for Walker Medium CQL,
using a larger reward shift of r0 = r + 50 further improves the result with r0 = r + 8.
Figure 7 shows different performance under different choices of the reward shift constant. We denote
a positive reward shift r0 = r + 8 as Pos.1, denote r0 = r + 20 as Pos.2 and denote r0 = r + 50 as
Pos.3 for all experiments excetp in the Hopper Medium CQL we use Pos.1 to denote r0 = r + 1.
In the experiments based on BCQ (first two figures). We can observe a uniformly performance im-
provement with all choices of reward shift constants. As the algorithm of CQL has already taken the
conservative value estimation into consideration, in the experiments based on CQL, the performance
is more closely related to the constant we use. Specifically, in Hopper Expert, while using any of
the positive reward shift constants improve the learning stability, r0 = r + 8 performs better on
preserving the learning efficiency during early learning stage. For Hopper Medium, we find using
larger positive constants hinder the performance. For Walker Medium, using a larger constant in
reward shift performs much better than using a smaller one.
O 50000 IOOOOO 150000 200000 250000 300000
# Optimization Step
Hopper Expert BCQ
——BCQ
BCQ PθS.l
BCQ Pos.2
——BCQ PθS.3
O 200000 400000 600000 800000 1000000
# Optimization Step
O 500000 10000001500000 2000000 2500000 3000000
# Optimization Step
O	500000 10000001500000 20000002500000 3000000
# Optimization Step
O 500000 10000001500000 2000000 2500000 3000000
# Optimization Step
Figure 7: Performance with different reward shift constants.
C.2 Continuous Control
Pseudo-Code for Random Reward Shift The pseudo-code of RRS is provided in Algorithm 1.
Details of RRS Although we find in the motivating example that a -5 reward shift is able to
remarkably improve the asymptotic performance of TD3, in this work we aim at proposing an uni-
formly suitable method based on the insight behind the motivating example. Therefore we propose
to use ±0.5, 0 as the reward shifting constants. We find in experiment that the sampling frequency
does not affect the performance. And in the experiments we follow BDQN Osband et al. (2016) to
use a fixed value network throughout a whole trajectory. i.e., one of the K Q-networks is sampled
uniformly after each episode with length of 1000 timesteps. Intuitively, searching for more suitable
reward randomization designs may further improve the performance, yet that is beyond the coverage
of this work.
Ablation Studies We experiment with different number of Q-value networks as well as dif-
ferent choices of the random reward shifting ranges. Results are presented in Figure 8. We
denote RRS with 7 reward shifting constants ( and therefore also 7 Q-networks) as RRS-
15
Under review as a conference paper at ICLR 2022
Algorithm 1 Sample-Efficient Continuous Control with Random Reward Shift
Require
•	the size of mini-batch N, smoothing factor τ > 0, K reward shift values rk0 = r + bk , k = 1, . . . , K.
•	Random initialized policy network ∏θ, target policy network n0，, θ0 — θ.
•	K random initialized Q networks, and corresponding target networks, parameterized by wk,wk, Wk —
wk for k = 1, . . . , K. (e.g., a ModuleList in PyTorch).
for iteration = 1, 2, ... do
Uniformly sample one of the K Q-functions, Qwk , for policy update
for t = 1, 2, ... do
#	Interaction
Run policy πθ, and collect transition tuples (st , at , s0t , rt).
Sample a mini-batch of transition tuples {(s, a, s0, r)i}iN=1.
#	Update Qw (in parallel)
Calculate the k-th target Q value yk,i = ri + bk + Qw0 (s0i, πθ0 (s0i))
Update wk with loss PiN=1(yk,i - Qwk (si, ai))2.
#	Update πθ
Update policy πθ with Qwk
end for
# Update target networks
θ0 - Te +(1 - T )θ0.
Wk ― Twk + (1 - τ)wk, k =1,...,K.
end for
7, and denote RRS with 3 reward shifting constants ( and therefore also 3 Q-networks)
as RRS-3. The constants following RRS-3/RRS-7 are the ranges of those random con-
stants. Specifically, we use [-0.5, 0, 0.5] for the RRS-3 0.5 settings, [-1.0, 0, 1.0] for the
RRS-3 1.0 settings, [-0.5, -0.33, -0.17, 0, 0.17, 0.33, 0.5] for the RRS-7 0.5 settings and
[-1.0, -0.67, -0.33, 0, 0.33, 0.67, 1.0] for the RRS-7 1.0 settings. According to the experimental
results, RRS is not sensitive to hyper-parameters, showing the robustness of the proposed method.
We believe further search for those hyper-parameters can further improve the learning efficiency, yet
this is off the main scope of this work and therefore left for the future research.
O 20000。	400000	60∞∞	8∞0∞ I(KKKKK)	O 200000	400000	600000 8(KKKK) IO(KKKK)	O 20000。	400000	60∞∞	8∞0∞ 1(KKKKK)
# Interactions	# Interactions	# Interactions
u-jnləɑypos-d山
Figure 8: Performance with different reward shift constants and different number of Q-networks.
——RRS-7 0.5
RRS-7 1.0
——RRS-3 0.5
——RRS-3 1.0
Humanoid-v2
O
200000 400000 600000 800000 1000000
# Interactions
C.3 Random Network Distillation
Environments In this work, we experiment with five discrete (sparse reward) exploration tasks ,
namely the MountainCar-v0, and four navigation tasks of MiniGrid suite (Chevalier-Boisvert et al.,
2018), namely the task of Empty-Random, MultiRoom, and FourRooms, to verify our insight on
16
Under review as a conference paper at ICLR 2022
improving RND for value-based curiosity-driven exploration. Figure 9 shows example of different
tasks.
Figure 9:	Examples of environments used in Section 5.3. The first figure shows the MountainCar-v0
environment where a car needs to accumulate potential energy to reach the flag, to receive a positive
reward. The second figure shows the maze of the Empty-Random task with size of 6, the third one
shows the MultiRoom of level S2-N4, where there are 2 rooms with size 4, the last figure shows
example of FourRoom task with size 17. In our experiments, as we use the vanilla DQN as the
baseline, which is not suitable for partial observable tasks, we use a smaller maze of size 7 and 9 to
avoid further dependency on memories. In all tasks of the MiniGrid domain, the triangular red agent
need to navigate to the green goal square, and the observable region is only a 7x7 square the agent
is facing to (i.e., the regions with shallower color in the last three figures).
Ablation Studies We experiment with different reward shifting constants in the dis-
crete control settings. We use a relatively large range in choosing constants, i.e.,
{-0.05, -0.15, -1.0, -1.5, -2.0, -2.5, -5.0, -10.0}. Results are presented in Figure 10. In all
experiments, using a moderate reward shifting constant like {-1.0, -1.5, -2.0, -2.5} remarkably
improves the learning efficiency. On the other hand, a too aggressive reward shifting will lead to too
much curiosity exploration and hinder the learning efficiency in the limited number of interactions.
17
Under review as a conference paper at ICLR 2022
MιnιGrιd-Empty-Random-6x6-vO
MiniGnd-Fou rRooms-7x7-vO
M IniGnd-MuIti Room-N2-S4-v0
o.o
RND
RND -0.05
RND -0.15
RND -1.0
RND -1.5
RND -2.0
RND -2.5
RND -5.0
RND -10.0
——RND
RND-0.05
RND -0.15
——RND -1.0
——RND -1.5
——RND -2.0
RND -2.5
RND -5.0
RND-10.0
0	100	200	300	400	500	0
# Episodes
MiniGrid-FoUrRoomS-9x9-VO
0-7
2000 4000 6000 6000 10000 12000 14000
# Episodes
MountainCar-VO
2000	4000	6000	8000	10000
# Episodes
-J βj βj βj ∙ - - _ ∙J
-------
——RND
——RND -0.05
—RND-0.15
——RND-LO
——RND -1.5
——RND-2.0
RND-2.5
——RND-5.0
RND -10.0
-200
0	5000 10000 15000 20000 25000 30000 35000
# Episodes
0	1000	2000	3000	4000	5000
# Episodes
Figure 10:	Performance with different reward shift constants in RND.
18