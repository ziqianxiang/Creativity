Under review as a conference paper at ICLR 2022
CLOOB: Modern Hopfield Networks with
InfoLOOB Outperform CLIP
Anonymous authors
Paper under double-blind review
Ab stract
Contrastive learning with the InfoNCE objective is exceptionally successful in
various self-supervised learning tasks. Recently, the CLIP model yielded im-
pressive results on zero-shot transfer learning when using InfoNCE for learning
visual representations from natural language supervision. However, InfoNCE as
a lower bound on the mutual information has been shown to perform poorly for
high mutual information. In contrast, the InfoLOOB upper bound (leave one out
bound) works well for high mutual information but suffers from large variance and
instabilities. We introduce “Contrastive Leave One Out Boost” (CLOOB), where
modern Hopfield networks boost learning with the InfoLOOB objective. Modern
Hopfield networks replace the original embeddings by retrieved embeddings in the
InfoLOOB objective. The retrieved embeddings give InfoLOOB two assets. Firstly,
the retrieved embeddings stabilize InfoLOOB, since they are less noisy and more
similar to one another than the original embeddings. Secondly, they are enriched
by correlations, since the covariance structure of embeddings is reinforced through
retrievals. We compare CLOOB to CLIP after learning on the Conceptual Captions
and the YFCC dataset with respect to their zero-shot transfer learning performance
on other datasets. CLOOB consistently outperforms CLIP at zero-shot transfer
learning across all considered architectures and datasets.
1	Introduction
With the advent of large corpora of unlabeled data in vision and language, self-supervised learning
via contrastive learning has become highly successful. Some contrastive learning objectives, such as
those of BYOL (Grill et al., 2020) and SimSiam (Chen & He, 2021), do not require negative samples.
However, the most popular objective for contrastive learning is InfoNCE (van den Oord et al., 2018),
in which for an anchor sample, a positive sample is contrasted with negative samples.
The idea to use objectives with negative samples is well known in deep learning (Gutmann & Hyvari-
nen, 2010; Chen et al., 2017; Mikolov et al., 2013). For contrastive learning, the most successful
objective is InfoNCE, which has been introduced as Contrastive Predictive Coding (CPC) (van den
Oord et al., 2018). InfONCE has been applied to transfer learning (Henaff et al., 2019), to natural
language response suggestion (Henderson et al., 2017), to learning sentence representations from
unlabelled data (Logeswaran & Lee, 2018), and to unsupervised feature learning by maximizing
distinctions between instances (Wu et al., 2018). InfoNCE has been used for learning visual repre-
sentations in Pretext-Invariant Representation Learning (PIRL) (Misra & vanDerMaaten, 2020), in
Momentum Contrast (MoCo) (He et al., 2020), and in SimCLR (Chen et al., 2020). SimCLR became
well known as is was highly effective for transfer learning. Zero-shot transfer learning (Lampert
et al., 2009) is one of the most ambitious goals in vision, since it would improve various real-world
downstream applications. Current models in natural language processing and vision perform very
well on standard benchmarks, but they fail at new data, new applications, deployments in the wild,
and stress tests (D’Amour et al., 2020; Recht et al., 2019; Taori et al., 2020; Lapuschkin et al., 2019;
Geirhos et al., 2020). A model with high zero-shot transfer learning performance will not fail on such
data, therefore will be trusted by practitioners.
Contrastive Language-Image Pre-training (CLIP) based on the InfoNCE objective yielded very
impressive results at zero-shot transfer learning (Radford et al., 2021). CLIP learns expressive image
embeddings directly from raw text, thereby leverages a much richer source of supervision than just
1
Under review as a conference paper at ICLR 2022
labels. A plethora of CLIP follow-up work has already been published (see Appendix Section A.5).
The CLIP model is considered as an important foundation model (Bommasani et al., 2021). Though
CLIP excels at zero-shot transfer learning, it can be improved.
CLIP training suffers from an “explaining away” problem (Wellman & Henrion, 1993), which leads
to “shortcut learning” (Geirhos et al., 2020) or the Clever Hans phenomenon (Lapuschkin et al., 2019).
Explaining away impedes the increase of the similarity between a text and a corresponding image,
since learning focuses on only one common aspect and does not exploit the full covariance structure
of the data. If one common aspect is sufficient for high similarity, the InfoNCE objective saturates,
since it has the form a/(a + b) with a giving the similarity of a matched pair and b giving the average
similarity of unmatched pairs. For a large similarity a, the objective saturates and increasing a has a
small effect. Contrary to InfoNCE, the leave-one-out (“InfoLOOB”) bound (Poole et al., 2019) is
of the form a/b which does not saturate. However, so far the InfoLOOB bound was not used as an
objective in contrastive learning. We justify the maximization of the InfoLOOB bound for contrastive
learning in Appendix Section A.1.3. We show that maximizing the InfoLOOB bound leads to a good
approximation of the mutual information, in particular for high mutual information. A problem of
InfoLOOB is that it has high variance for small b.
Even when InfoLOOB avoids saturation, CLIP insufficiently extracts the covariance structure in the
data. The covariance originates from co-occurrences of related words in text or from co-occurrences
of objects, textures, or colors in images. CLIP’s problem of insufficiently extracting the covariance
structure of the data is tackled by modern Hopfield networks. Hopfield networks are energy-based,
binary associative memories, which popularized artificial neural networks in the 1980s (Hopfield,
1982; 1984). Associative memory networks have been designed to store and retrieve samples. Their
storage capacity can be considerably increased by polynomial terms in the energy function (Chen
et al., 1986; Psaltis & Cheol, 1986; Baldi & Venkatesh, 1987; Gardner, 1987; Abbott & Arian, 1987;
Horn & Usher, 1988; Caputo & Niemann, 2002; Krotov & Hopfield, 2016). In contrast to these
binary memory networks, we use continuous associative memory networks with very high storage
capacity. These modern Hopfield networks for deep learning architectures have an energy function
with continuous states and can retrieve samples with only one update (Ramsauer et al., 2021; 2020).
Modern Hopfield Networks have already been successfully applied to immune repertoire classification
(Widrich et al., 2020) and chemical reaction prediction (Seidl et al., 2021). Modern Hopfield networks
reinforce the covariance structure in the data and stabilize the InfoLOOB objective by increasing b.
The covariance structure of retrieved embeddings is amplified through co-occurrences of embedding
features in the memory. Additionally, the retrieved embeddings are less noisy and more similar to
one another which leads to a larger b. We introduce “Contrastive Leave One Out Boost” (CLOOB)
which overcomes CLIP’s problems of (i) “explaining away” with saturation and (ii) insufficiently
extracting the covariance structure of the data. CLOOB uses the leave-one-out (“InfoLOOB”) bound
(Poole et al., 2019) as the objective in combination with modern Hopfield networks.
Our contributions are:
(a)	we introduce a new contrastive learning method called CLOOB,
(b)	we propose InfoLOOB as an objective for contrastive learning,
(c)	we propose to use modern Hopfield networks to reinforce covariance structures,
(d)	we show theoretical properties of the InfoLOOB objective and loss function.
2	InfoLOOB vs. InfoNCE
We discuss and analyse known bounds on the mutual information I(X ; Y ) between random variables
X and Y , which are distributed according to p(x, y):
I(X ; Y)
Ep(x,y) ln P(X) Pyy)	= Ep(x,y)
l P(X I y)'
.n p(χ).
Ep(x,y)
l p(y I χ)'
.n p(y).
(1)
We consider the multi-sample lower bound “InfoNCE” (van den Oord et al., 2018). A pair of an
anchor sample y and a positive sample X1 is drawn via the joint distributionP(X1,y). The negative
samples X = {X2,. . . ,XN} are drawn iid according to the marginal distribution P(X). Using
X = {xι,)2,..., XN}, the probabilities of the datasets are P(X) = QN=2P(Xi), P(X ∣ y) =
2
Under review as a conference paper at ICLR 2022
p(x1 | y) QiN=2 p(xi), and p(X) = QiN=1 p(xi). The InfoNCE with score function f(x, y) is
IInfoNCE(X1 ; Y ) = Ep(y)
Ep(X|y)
ln( NPfb 川
(2)
using the factor 1/N as in Poole et al. (2019); Tschannen et al. (2019); Cheng et al. (2020); Chen
et al. (2021). For f (x, y) = p(y | x), we obtain the InfoNCE with probabilities. The InfoNCE is a
lower bound on the mutual information (Poole et al., 2019), which is stated in the next theorem.
Theorem 1 (InfoNCE lower bound). InfoNCE with score function f (x, y) is a lower bound on the
mutual information:
I(X1 ; Y) ≥ Ep(y)
Ep(X |y)
ln( —f(x1, y—
N P=I f(xi, y)
IInfoNCE(X1 ; Y) .
(3)
In particular, the bound holds for InfoNCE with probabilities, i.e. for f(x,y) = p(y | x).
For a proof see Poole et al. (2019) and the proof of Theorem A1 in the Appendix.
The “Leave one out upper bound” (Poole et al., 2019) on the mutual information was called “L1Out”
in Cheng et al. (2020), while we call it “InfoLOOB” (LOOB for “Leave One Out Bound”). InfoLOOB
is the same as InfoNCE (Eq. (3)), but without the positive sample x1 in the denominator. Contrastive
Log-ratio Upper Bound (CLUB), another upper bound on the mutual information, was only used for
minimizing it (Cheng et al., 2020). Maximizing CLUB failed in experiments, because the embedding
distribution was not uniform as known for similar objectives (Wang & Liu, 2021). Uniform embedding
distributions are required for successful contrastive learning (Wang & Isola, 2020).
We use InfoLOOB as an objective, since it approximates high mutual information better than InfoNCE.
Maximizing an upper bound on the mutual information might be counter-intuitive. Therefore, we
justify the maximization of the InfoLOOB bound for contrastive learning in Appendix Section A.1.3.
We show that maximizing the InfoLOOB bound approximates the mutual information, the better
the higher it is. Recently, InfoLOOB was independently introduced for and successfully applied to
image-to-image contrastive learning (Yeh et al., 2021).
The InfoLOOB with score function f(x,y) is defined in the following, where we obtain the In-
foLOOB with probabilities for f(x, y) = p(y | x):
IInfoLOOB(X1 ; Y)
Ep⑻卜(XIy)卜(N-I PNX，y))
(4)
Before we show that InfoLOOB with a score function is an upper bound on the mutual information,
We need some definitions. P(X | y) draws the positives for y with lower probability than p(x), that
is, the positives are under-sampled. Z(y) = Ep(χ∣y) [f (x, y)] gives the average score f (x, y), if
under-sampling via P(X | y), while Z*(y) = Ep(X) [f (x, y)] average score f (x, y) if sampling
from p(x). We define the variational distribution q(x | y) = P(Z比,).OUr main assumption is
expressed by the log-ratio of the averages Z(y) and Z* (y):
Ep(y) [KL(p(x I y) k q(x I y))] 6 Ep(y) [lnZ*(y) - lnZ(y)] ,	(5)
which ensures that the positives X are sufficiently under-sampled via P(X | y). The Kullback-Leibler
divergence gives the minimal difference between averaging f (x, y) viap(x) and viaP(X ∣ y). The
next theorem shows that InfoLOOB is an upper bound on the mutual information.
Theorem 2 (InfOLOOB upper bound). If X = {x2,..., XN} are drawn Iid according to P(X ∣ y)
and if the main assumption Eq. (5) holds, then InfoLOOB with score function f(X, y) is an upper
bound on the mutual information:
I(X1 ； Y) 6 Ep(y) Ep(XIy)
f (xi, y)	ʌ
P=2 f (Xi y)j_ _
IInfoLOOB(X1 ; Y) .
(6)
The bound is valid for InfoLOOB with probabilities (without under-sampling), where the negative
samples X = {X2, . . . , XN} are drawn iid according to P(X) and f(X, y) = P(y | X).
The proof for this theorem is given as proof for Theorem A2 in the Appendix.
3
Under review as a conference paper at ICLR 2022
Loss functions and their gradients. The training set {(x1, y1), (x2, y2), . . . , (xN, yN)} consists
of N samples that are drawn iid from p(x, y). InfoNCE uses the matrix X = (x1, . . . , xN), while
InfoLOOB uses X = (x2,..., XN). The matrices differ by the positive sample xι. For the score
function f(x, y), we use f(x, y) = exp(τ -1 sim(x, y)) with the similarity sim(x, y) = yTx and
τ as the temperature. We have the InfoNCE and InfoLOOB loss functions:
1N LInfoNCE = - N E i=1 1N LInfoLO。B = - N T N i=1	exp(T-1 XTyi)	_ ɪ XX	exp(T-1 XTyj) PN=I exp(T-1 XTyj)	N i=ι	PN=I exp(T-1 XTyi)， in	NXp(TT XTy.	- ɪ X in NXp(TT XT仍)	.(8) Pj=i exp(T-1XTyj)	N 白	Pj=i exp(T-1XTyi)
In the second sum of the losses in Eq. 7 and Eq. 8, we consider only the first term. For simplicity, we
abbreviate y = y1 leading to the pair (x1 , y) and the negatives X = (x2 , . . . , xN).
L…(y) = - ln LNxpL XTyT	,	L…B(y) = - in	NxpL XT叱.
j=1 exp(τ	xj y)	j=2 exp(τ	xj y)
These loss terms can be simplified to LInfoNCE(y) = -τ -1yT X1 + τ-1lse(τ-1, XTy) and
LInfoLOOB(y) = -T-IyTXi + T-Ilse(T-1, XTy), where lse is the log-sum-exp function
(see Eq. (A103) in the Appendix). The gradient of the InfoNCE loss with respect to y is
-T-1X1 + T-1Xsoftmax(T-1XT y) and the gradient of the InfoLOOB loss is -T-1X1 +
τTXsoftmax(TTXTy). Using P = (pi,...,pn)t = softmax(τ-1XTy), the gradient of
InfoNCE with respect to y is -T-1(1 - pι)(xι - Xsoftmax(T-1XTy)) and its gradient with
respect to X1 is -T-1 (1 - p1)y (see Appendix Subsection A.1.4).
By and large, the gradient of InfoNCE is scaled by (1 - p1) compared to the gradient of InfoLOOB,
where p1 is softmax similarity between the anchor y and positive sample X1 . Consequently, InfoNCE
saturates and learning stalls when anchor and positive sample become similar to each other.
3 CLOOB: InfoLOOB with Modern Hopfield Networks
Figure 1: The CLOOB architecture for image-text pairs. The image embedding Xi and the text em-
bedding yi retrieve the embeddings Uxi and Uyi, respectively, from a modern Hopfield network that
stores image embeddings U = (u1 , . . . , uM ) (green boxes at the left). The image-retrieved image
embedding Uxi serves as anchor in order to contrast the positive text-retrieved image embedding
Uyi with the negative text-retrieved image embedding Uyj for j 6= i. Analog, for the second modern
Hopfield network that stores text embeddings V = (v1, . . . , vK) (green boxes at the right).
legend ∖
similarity to anchor
positive sample
negative sample
√
4
Under review as a conference paper at ICLR 2022
CLOOB for contrastive learning. Our novel Contrastive Leave One Out Boost (CLOOB) combines
the InfoLOOB objective with modern Hopfield networks. Modern Hopfield networks substitute
the original by retrieved embeddings, thereby reduce the variance of InfoLOOB and reinforce the
covariance structure in the data. Figure 1 sketches the CLOOB architecture for image-text pairs.
The training set consists of N pairs of embeddings {(x1, y1), . . . , (xN, yN)}, M stored embeddings
U = (u1, . . . , uM), and K stored embeddings V = (v1, . . . , vK). The state or query embeddings
xi and yi retrieve Uxi and Uyi, respectively, from U — analogous notation for retrievals from V .
All samples are normalized: kxi k = kyi k = kui k = kvi k = 1. The following vectors are retrieved
from modern Hopfield networks (Ramsauer et al., 2021):
Uxi	= U softmax(β	UTxi)	,	Uyi	= U softmax(β UTyi)	,	(9)
Vxi	= V softmax(β	VTxi)	,	Vyi	= V softmax(β V Tyi)	(10)
where Uxi denotes an image-retrieved image embedding, Uyi a text-retrieved image embedding,
Vxi an image-retrieved text embedding and Vyi a text-retrieved text embedding. The hyperparameter
β corresponds to the inverse temperature: β = 0 retrieves the average of the stored pattern, while
large β retrieves the stored pattern that is most similar to the state pattern (query).
In InfoLOOB, CLOOB substitutes the embedded samples xi and yi by the retrieved embedded
samples. In the first term, xi and yi are substituted by Uxi and Uyi, respectively, while in the second
term by Vxi and Vyi. All retrieved samples are normalized, kUxi k = kUyi k = kVxi k = kVyi k = 1.
We obtain the InfoLOOB loss function that is used by CLOOB:
LInfOLOOB = Y XXX ln LNxp(T-1 Ux Uy
N W	Pj= exp(τ-1 UXiUyj)
1N
-N X
i=1
ln	eχp(TT VxVyi)
P= exp(τ-1 Vxj Vyi).
(11)
Modern Hopfield Networks reduce high variance of InfoLOOB. CLOOB uses InfoLOOB as
objective, since it estimates the mutual information (MI) better than InfoNCE, in particular, for large
MI. Cheng et al. (2020, Fig. 1 and Fig. 2) show that InfoLOOB is a better estimator for the MI than
InfoNCE (van den Oord et al., 2018), MINE (Belghazi et al., 2018), and NWJ (Nguyen et al., 2010).
We experimentally confirmed that InfoLOOB better estimates the mutual information than InfoNCE.
noitamrofni lautuM
0	1000	2000	3000	4000
steps
noitamrofni lautuM
1000	2000	3000	4000
steps
without Hopfield
MI 14
4 2 0 8 6 4 2
noitamrofni lautuM
-----True MI
1000 2000 3000 4000 5000 6000
steps
noitamrofni lautuM
with Hopfield
MI 14
True MI
1000 2000 3000 4000 5000 6000
steps
Figure 2: Variance reduction of InfoLOOB by modern Hopfield networks. From left to right: without
Hopfield for MI 10, with Hopfield for MI 10, without Hopfield for MI 14, with Hopfield for MI 14.
Modern Hopfield networks reduce the variance of the InfoLOOB loss.
However, InfoLOOB has higher variance than lower bounds on MI like InfoNCE, which considerably
hampers learning (Cheng et al., 2020, Fig. 1 and Fig. 2), see also Appendix Section A.2. The
InfoNCE objective has the form a/(a + b) while InfoLOOB has the form a/b with a giving the
anchor-to-positive similarity and b the average anchor-to-negative similarity. For small b, we observe
high variance and instability of InfoLOOB. Modern Hopfield networks (Ramsauer et al., 2021)
are a remedy for the high variance. Modern Hopfield networks substitute the original patterns by
retrieved patterns, which are an average over the stored patterns. We tested the variance of MI
estimators/bounds on toy tasks, with samples drawn from Gaussian distributions following (Belghazi
et al., 2018; Poole et al., 2019; Cheng et al., 2020). With the InfoLOOB objective, we train deep
learning architectures with and without modern Hopfield networks on top, where the current learning
batch is stored in the modern Hopfield networks. We used training data with mutual information of
5
Under review as a conference paper at ICLR 2022
10 and 14, where the parameters were optimized for the best performance on a validation set. We
test the final model on different levels of mutual information. Figure 2 shows that modern Hopfield
networks reduce the variance of the model. The average variances are reduced from 0.67 to 0.33 for
MI 10 and from 1.00 to 0.48 for MI 14 (more details in Appendix A.2).
Modern Hopfield Networks amplify the covariance structure in the data. The covariance struc-
ture is extracted by the retrieved embeddings UxTi Uyi and VxTi Vyi . The Jacobian J of the soft-
max p = softmax(βa) is J(βa) = β diag(p) - ppT . We define the weighted covariance
Cov(U), where sample ui is drawn with probability pi, as [Cov(U)]kl = UJ(βa)UT kl =
β(PiM=1 piuikuil - PiM=1 piuik PiM=1 piuil). The formula of the weighted covariance differs from
the standard empirical covariance, since the factor 1/M is replaced by pi. Thus ui is sampled with
probability pi instead of being sampled uniformly with probability 1/M.
We apply the mean value theorem to the softmax function with mean Jacobian matrix Jm (βa) =
R01 J(λβa) dλ. The mean Jacobian Jm(βa) is a symmetric, diagonally dominant, positive semi-
definite matrix with one eigenvalue of zero for eigenvector 1 and spectral norm bounded by kJm k2 6
0.5β (see Appendix Lemma A1). We can express UxTi Uyi as (see Appendix Theorem A3):
UxTiUyi = (u + Cov(U, Xi) Xi)T (U + Cov(U, yi) yi) ,	(12)
where the mean is U= 1/M U1 and the weighted covariances are Cov(U, Xi) = U Jm(βU T Xi)UT
and Cov(U, yi) = UJm (βU T yi)U T . The weighted covariance Cov(U, .) is the covariance if the
stored pattern Ui is drawn according to an averaged pi given by Jm(.). When maximizing the dot
product UxT Uyi , the normalized vectors Xi and yi are encouraged to agree on drawing the patterns
Ui with the same probability pi in order to generate similar weighted covariance matrices Cov(U, .).
If subsets of U have a strong covariance structure, then it can be exploited to produce large weighted
covariances and, in turn, large dot products of UxT Uyi. Furthermore, for a large dot product UxT Uyi,
Xi and yi have to be similar to each other to extract the same direction from the covariance matrices.
Above considerations for UxTi Uyi analogously apply to VxTi Vyi .
We did not use a loss function that contains dot products like UxTi Vyi , because these dot products
have higher variance than the ones we have used. The dot product UxTi Vyi has higher variance, since
it uses M + K stored patterns, whereas UxTi Uyi and VxTi Vyi use M and K, respectively.
Modern Hopfield Networks can reuse training samples as stored patterns. We use the training
samples as the stored patterns in the modern Hopfield network. Hence, we set Ui = Xi and vi = yi ,
that is, U = X and V = Y . Consequently, we store the learning batch in the modern Hopfield
networks as U and V . In particular this means that Xi can retrieve itself from U = X but not from
V = Y . Analogously, yi can retrieve itself from V = Y but not from U = X.
Modern Hopfield networks allow the usage of retrieved embeddings. After learning, both the
model embeddings X and y as well as the retrieved embeddings Ux , Uy , Vx , and Vy may serve for
the downstream tasks, e.g. for zero-shot transfer learning. When using the retrieved embeddings, the
modern Hopfield networks can store random samples, prototypes, templates, or proprietary samples.
Therefore, particular embedding features can be amplified according to the task at hand.
Modern Hopfield networks is a new concept for contrastive learning. In bioinformatics the
covariance structure in a sequence is reinforced by first retrieving similar sequences from a database
and then aligning them. Conserved regions are characterized by high local covariance in the alignment
(Dickson & Gloor, 2012; Kreth & Fodor, 2014). Modern Hopfield networks detect high covariances
of embedded features, which is conveyed by the retrieved sample that corresponds to an alignment.
4	Experiments
On two pretraining datasets, we compare our new CLOOB to CLIP (Radford et al., 2021) with respect
to their capability of zero-shot transfer learning. The first dataset, Conceptual Captions (CC) (Sharma
et al., 2018), has a very rich textual description of images but only three million image-text pairs. The
second dataset, a subset of YFCC100M (Thomee et al., 2016), has 15 million image-text pairs but
the textual description is less rich than for CC and often vacuous. For both pretraining datasets, the
downstream zero-shot transfer learning performance is tested on seven image classification datasets.
6
Under review as a conference paper at ICLR 2022
Table 1: Zero-shot results for models trained on CC with ResNet-50 vision encoders for two different
checkpoints. Results are given as mean accuracy over 5 runs. Statistically significant results are
shown in bold. CLIP and CLOOB were trained for 31 epochs while CLIP* and CLOOB* were
trained for 128 epochs. In the majority of tasks CLOOB significantly outperforms CLIP.
Dataset	CLIP RN-50	CLOOB RN-50	CLIP* RN-50	CLOOB* RN-50
Birdsnap	2.26 ± 0.20	3.06 ± 0.30	2.8 ± 0.16	3.24 ± 0.31
Country211	0.67 ± 0.11	0.67 ± 0.05	0.7 ± 0.04	0.73 ± 0.05
Flowers102	12.56 ± 0.38	13.45 ± 1.19	13.32 ± 0.43	14.36 ± 1.17
GTSRB	7.66 ± 1.07	6.38 ± 2.11	8.96 ± 1.70	7.03 ± 1.22
UCF101	20.98 ± 1.55	22.26 ± 0.72	21.63 ± 0.65	23.03 ± 0.85
Stanford Cars	0.91 ± 0.10	1.23 ± 0.10	0.99 ± 0.16	1.41 ± 0.32
ImageNet	20.33 ± 0.28	23.97 ± 0.15	21.3 ± 0.42	25.67 ± 0.22
ImageNet V2	20.24 ± 0.50	23.59 ± 0.15	21.24 ± 0.22	25.49 ± 0.11
4.1	Conceptual Captions Pretraining
Pretraining dataset. The Conceptual Captions (CC) (Sharma et al., 2018) dataset consists of 2.9
million images with high-quality captions. Images and their captions have been gathered via an
automated process from the web and therefore represent a wide variety of content. Raw descriptions
of images are collected from the alt-text HTML attribute. Both images and texts are filtered for high
quality image-text pairs.
Methods compared. We compare our new CLOOB to CLIP (Radford et al., 2021). The CLOOB
implementation is based on OpenCLIP (Ilharco et al., 2021), which achieves results equivalent to
CLIP on the YFCC dataset (see Section 4.2). OpenCLIP also reports results on the CC dataset.
As CLIP does not train models on CC we report results from this reimplementation as baseline.
Analogously to Radford et al. (2021, Section 2.4), we use the modified ResNet (He et al., 2016) and
BERT (Devlin et al., 2018; 2019) architectures to encode image and text input. We use the ResNet
encoders ResNet-50, ResNet-101, and ResNet-50x4.
Hyperparameter selection and learning schedule. We use the hyperparameter values of OpenCLIP,
concretely, a learning rate of 1 × 10-3 and a weight decay of 0.1 for the Adam optimizer (Kingma
et al., 2014) with decoupled weight decay regularization (Loshchilov & Hutter, 2019). Deviating
from OpenCLIP, we use a batch size of 512 due to computational restraints, which did not change the
performance. The learning rate scheduler for all experiments is cosine annealing with warmup and
hard restarts (Loshchilov & Hutter, 2017). We report the hyperparameter τ (default 0.07) from CLIP
as τ-1 of 14.3 to be in the same regime as the hyperparameter β for the modern Hopfield networks.
The main hyperparameter search for CLOOB (also for YFCC pretraining in the next section) was
done with ResNet-50 as the vision encoder. Learnable τ-1 in combination with the InfoLOOB loss
results in undesired learning behavior (see Appendix Section A.1.4). Therefore, we set τ-1 to a fixed
value of 30, which was determined via hyperparameter search (see Appendix Section A.3.2). For
modern Hopfield networks,the hyperparameter β was set to 8. Further we scale the loss in Eq. (11)
with τ to remove the factor τ-1 from the gradients (see Appendix Section A.1.4) resulting in the loss
function τ LInfoLOOB .
Evaluation metrics: Zero-shot transfer learning. We evaluate and compare both CLIP and
CLOOB on their zero-shot transfer learning capabilities on the following downstream image classi-
fication tasks. Birdsnap (Berg et al., 2014) contains images of 500 different North American bird
species. The Country211 (Radford et al., 2021) dataset consists of photos across 211 countries and
is designed to test the geolocalization capability of visual representations. Flowers102 (Nilsback &
Zisserman, 2008) is a dataset containing images of 102 flower species. GTSRB (Stallkamp et al.,
2011) contains images for classification of German traffic signs. UCF101 (Soomro et al., 2012) is a
video dataset with short clips for action recognition. For UCF101 we follow the procedure reported
in CLIP and extract the middle frame of every video to assemble the dataset. Stanford Cars (Krause
et al., 2013) contains images of 196 types of cars. ImageNet (Deng et al., 2009) is a large scale image
classification dataset with images across 1,000 classes. ImageNetv2 (Recht et al., 2019) consists of
7
Under review as a conference paper at ICLR 2022
Table 2: Performance with InfoLOOB vs. InfoNCE objective and with vs. without Hopfield retrieval.
InfoLOOB increases the performance of CLIP in most of the tasks. Hopfield with InfoLOOB strongly
improves the performance in 7 out of 8 datasets compared to both CLIP models.
Dataset	CLIP InfoNCE InfoLOOB		Hopfield InfoNCE InfoLOOB	
Birdsnap	1.94	2.37	1.67	2.53
Country211	0.62	0.63	0.54	0.76
Flowers102	13.04	13.03	11.53	14.24
GTSRB	7.28	4.39	5.76	5.86
UCF101	21.00	19.14	20.56	22.29
Stanford Cars	0.90	1.33	1.24	1.37
ImageNet	20.31	22.13	19.04	24.21
ImageNetV2	20.63	21.65	18.97	23.80
three new test sets with 10,000 images each for the ImageNet benchmark. For further details see
Appendix Section A.3.3.
Results. We employ the same evaluation strategy and use the prompt templates as published in CLIP
(see Appendix Section A.3.3). We report zero-shot results from two checkpoints in Table 1. CLIP and
CLOOB were trained for a comparable number of epochs used in CLIP (see Appendix Section A.3.2)
while CLIP* and CLOOB* were trained until evaluation performance plateaued (epoch 128). In both
cases CLOOB significantly outperforms CLIP on the majority of tasks or matches its performance.
Statistical significance of these results was assessed by an unpaired Wilcoxon test on a 5% level.
Ablation studies. CLOOB has two new major components compared to CLIP: (1) the InfoLOOB
objective instead of the InfoNCE objective and (2) the modern Hopfield networks. To assess which of
the new major components of CLOOB has led to the performance increase over CLIP, we performed
ablation studies on CC. First, we enhanced CLIP by replacing the InfoNCE objective with InfoLOOB.
Table 2 shows that the InfoLOOB objective increases the performance of CLIP in the majority of
the datasets. The reason is that InfoLOOB suffers less than InfoNCE from the “explaining away”
problem. However, InfoLOOB is more effective for higher mutual information, that is, for a richer
covariance structure. Hopfield networks amplify the covariance structure by retrieved embeddings.
For InfoLOOB, however, this amplification is disadvantageous as the saturation effect is increased
by higher similarity between anchor and positive. Thus, combining modern Hopfield networks with
InfoNCE leads to a performance drop. Combining Hopfield and InfoLOOB into CLOOB strongly
improves the performance on 7 out of 8 zero-shot transfer learning tasks. An additional ablation
considers the learning rate scheduler. For more details see in Appendix Section A.3.1.
4.2	YFCC Pretraining
Pretraining dataset. To be comparable to the CLIP results, we use the same subset of 15 million
samples from the YFCC100M dataset (Thomee et al., 2016) as in Radford et al. (2021), which we
refer to as YFCC. YFCC was created by filtering YFCC100M for images which contain natural
language descriptions and/or titles in English. It was not filtered by quality of the captions, therefore
the textual descriptions are less rich and contain superfluous information. The dataset with 400
million samples used to train the CLIP models in Radford et al. (2021) has not been released and, thus,
is not available for comparison. Due to limited computational resources we are unable to compare
CLOOB to CLIP on other datasets of this size.
Methods compared and evaluation. In addition to the comparison of CLOOB and CLIP based on
the OpenCLIP reimplementation (Ilharco et al., 2021), we include the original CLIP results (Radford
et al., 2021, Table 12).
Hyperparameter selection. We use the hyperparameters selected at the Conceptual Captions dataset,
except learning rate, batch size, and β. For modern Hopfield networks, the hyperparameter β is set
to 14.3, which is the default parameter of τ-1 for the InfoNCE objective in Radford et al. (2021).
Furthermore, the learning rate is set to 5 × 10-4 and a batch size of 1024 as in OpenCLIP of Ilharco
et al. (2021). For further details see Appendix Section A.3.2.
8
Under review as a conference paper at ICLR 2022
Evaluation metrics. As in the previous experiment, methods are again evaluated at their zero-shot
transfer learning capabilities on downstream tasks.
Results. Table 3 provides results of the original CLIP and CLOOB trained on YFCC. The results
on zero-shot downstream tasks show that CLOOB outperforms the results of CLIP on all 7 tasks
(ImageNet V2 results have not been reported in Radford et al. (2021)). Similarly, CLOOB outperforms
CLIP on 6 out of 7 tasks for linear probing. Results of the comparison of CLOOB an the CLIP
reimplementation of OpenCLIP are given in Table 4. CLOOB exceeds the CLIP reimplementation in
7 out of 8 tasks for zero-shot classification using ResNet-50 encoders. With larger ResNet encoders,
CLOOB outperforms CLIP on all tasks. Furthermore, the experiments with larger vision encoder
networks show that CLOOB performance increases with network size. Visualizations of predictions
of CLOOB zero-shot classifiers from all datasets are shown in Appendix Section A.3.4.
Table 3: Results of CLIP and CLOOB trained on YFCC with ResNet-50 encoder. Except for one
linear probing dataset, CLOOB consistently outperforms CLIP across all tasks.
Dataset	Linear Probing		Zero-Shot	
	CLIP (OPenAD	CLOOB (ours)	CLIP (OpenAI)	CLOOB (ours)
Birdsnap	47.4	56.2	19.9	28.9
Country211	23.1	20.6	5.2	7.9
Flowers102	94.4	96.1	48.6	55.1
GTSRB	66.8	78.9	6.9	8.1
UCF101	69.2	72.3	22.9	25.3
Stanford Cars	31.4	37.7	3.8	4.1
ImageNet	62.0	65.7	31.3	35.7
ImageNet V2		-	58.7		-	34.6
Table 4: Zero-shot results for the CLIP reimplementation and CLOOB using different ResNet
architectures trained on YFCC. CLOOB outperforms CLIP in 7 out of 8 tasks using ResNet-50
encoders. With larger ResNet encoders CLOOB outperforms CLIP on all tasks. The performance of
CLOOB scales with increased encoder size.
Dataset	CLIP RN-50	CLOOB RN-50	CLIP RN-101	CLOOB RN-101	CLIP RN-50x4	CLOOB RN-50x4
Birdsnap	21.8	28.9	22.6	30.3	20.8	32.0
Country211	6.9	7.9	7.8	8.5	8.1	9.3
Flowers102	48.0	55.1	48.0	55.3	50.1	54.3
GTSRB	7.9	8.1	7.4	11.6	9.4	11.8
UCF101	27.2	25.3	28.6	28.8	31.0	31.9
Stanford Cars	3.7	4.1	3.8	5.5	3.5	6.1
ImageNet	34.6	35.7	35.3	37.1	37.7	39.0
ImageNet V2	33.4	34.6	34.1	35.6	35.9	37.3
5	Conclusion
For constrastive learning, we have introduced “Contrastive Leave One Out Boost” (CLOOB), for
which modern Hopfield networks boost learning with the InfoLOOB objective. Modern Hopfield
networks both increase the stability of InfoLOOB and reinforce the covariance structure of the data.
We have shown theoretical properties of the InfoLOOB bound and objective. Our results suggest
InfoLOOB as an alternative to InfoNCE in contrastive learning. An ablation study shows that both,
the InfoLOOB objective and modern Hopfield networks, are necessary to yield high performance. At
seven zero-shot transfer learning tasks, the novel CLOOB is compared to CLIP after pretraining on
Conceptual Captions and the YFCC dataset. CLOOB consistently outperforms CLIP at zero-shot
transfer learning across all considered architectures and datasets.
9
Under review as a conference paper at ICLR 2022
Reproducibility S tatement
We will publish the source code after the reviewing period. This will ensure that the results are
reproducible in their entirety. The datasets used for training our models as well as for the downstream
tasks are publicly available.
Ethical Considerations
Impact on ML and related scientific fields. Our research has the potential to positively impact a
wide variety of fields of life due to its general applicability. Most importantly, it has the potential to
reduce the cost for training other AI systems, which could lead to a reduction of compute costs and
carbon dioxide emissions.
However, any new development in machine learning can be applied for good or for bad. Our system
can be used for medical applications where it could save lives but might also be used for surveillance
and malevolent systems.
Impact on society. A potential danger could arise from an application of our approach in which
users rely overly on the outcomes. For example, in a medical setting, physicians might rely on the
technical system and shift the liability towards the machine. This might also happen in the domain of
self-driving cars, when drivers start paying less attention to the traffic because of an AI-based driving
system. Finally, our method may also be deployed in companies to automate various simple tasks,
which might lead to a reduced need for particular jobs in production systems.
Consequences of failures of the method. Depending on the application area, a failure of this
method might be of lesser concern, such as a failed execution of a computer program. If our method is
employed within a larger automation system, a failure could result in damages such as a car accident
or errors of a production system. However, this holds for almost all machine learning methods, and
their usage and testing depends on the application area.
Leveraging of biases in the data and potential discrimination. Our proposed method relies
on human-annotated data and thereby human decisions, which are usually strongly biased. The
undesirable biases contained in dataset are learned and may propagate to downstream applications.
Therefore, the responsible use of our method depends on a careful selection of the training data and
awareness of the potential biases within those.
References
L. F. Abbott and Y. Arian. Storage capacity of generalized networks. Phys. Rev. A, 36:5091-5094,
1987. doi: 10.1103/PhysRevA.36.5091.
S.	Agarwal, G. Krueger, J. Clark, A. Radford, J. W. Kim, and M. Brundage. Evaluating CLIP: towards
characterization of broader capabilities and downstream implications. ArXiv, 2108.02818, 2021.
P. Baldi and S. S. Venkatesh. Number of stable points for spin-glasses and neural networks of higher
orders. Phys. Rev. Lett., 58:913-916, 1987. doi: 10.1103/PhysRevLett.58.913.
D. Bau, A. Andonian, A. Cui, Y Park, A. Jahanian, A. Oliva, and A. Torralba. Paint by word. arXiv
preprint arXiv:2103.10951, 2021.
M. I. Belghazi, A. Baratin, S. Rajeswar, S. Ozair, Y. Bengio, A. Courville, and R. D. Hjelm. Mutual
information neural estimation. In J. Dy and A. Krause (eds.), Proceedings of the 35th International
Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp.
531-540. PMLR, 2018.
T.	Berg, J. Liu, S. W. Lee, M. L. Alexander, D. W. Jacobs, and P. N. Belhumeur. Birdsnap: Large-scale
fine-grained visual categorization of birds. In Proc. Conf. Computer Vision and Pattern Recognition
(CVPR), pp. 2019-2026, 2014. doi: 10.1109/CVPR.2014.259.
R. Bommasani et al. On the opportunities and risks of foundation models. ArXiv, 2108.07258, 2021.
10
Under review as a conference paper at ICLR 2022
Q. Cai, Y. Wang, Y. Pan, T. Yao, and T. Mei. Joint contrastive learning with infinite possibilities.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural
Information Processing Systems, volume 33, pp. 12638-12648. Curran Associates, Inc., 2020.
B. Caputo and H. Niemann. Storage capacity of kernel associative memories. In Proceedings of the
International Conference on Artificial Neural Networks (ICANN), pp. 51-56, Berlin, Heidelberg,
2002. Springer-Verlag.
N. Carlini and A. Terzis. Poisoning and backdooring contrastive learning. ArXiv, 2106.09667, 2021.
H. H. Chen, Y. C. Lee, G. Z. Sun, H. Y. Lee, T. Maxwell, and C. Lee Giles. High order correlation
model for associative memory. AIP Conference Proceedings, 151(1):86-99, 1986. doi: 10.1063/1.
36224.
J. Chen, Z. Gan, X. Li, Q. Guo, L. Chen, S. Gao, T. Chung, Y. Xu, B. Zeng, W. Lu, F. Li, L. Carin, and
C. Tao. Simpler, faster, stronger: Breaking the log-K curse on contrastive learners with FlatNCE.
arXiv, 2107.01152, 2021.
T. Chen, Y. Sun, Y. Shi, and L. Hong. On sampling strategies for neural network-based collaborative
filtering. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pp. 767-776, New York, NY, USA, 2017. Association for Computing
Machinery. doi: 10.1145/3097983.3098202.
T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning of
visual representations. In H. DaUme and A. Singh (eds.), Proceedings ofthe 37th International
Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp.
1597-1607. PMLR, 2020.
X. Chen and K. He. Exploring simple siamese representation learning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 15750-15758,
2021.
P. Cheng, W. Hao, S. Dai, J. Liu, Z. Gan, and L. Carin. CLUB: A contrastive log-ratio upper bound
of mutual information. In H. Daume and A. Singh (eds.), Proceedings of the 37th International
Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp.
1779-1788. PMLR, 2020.
A.	D’Amour et al. Underspecification presents challenges for credibility in modern machine learning.
ArXiv, 2011.03395, 2020.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248-255.
Ieee, 2009.
B.	Devillers, R. Bielawski, B. Choski, and R. VanRullen. Does language help generalization in vision
models? ArXiv, 2104.08313, 2021.
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: pre-training of deep bidirectional
transformers for language understanding. ArXiv, 2018.
J.	Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: pre-training of deep bidirectional trans-
formers for language understanding. In Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume
1 (Long and Short Papers), pp. 4171-4186. Association for Computational Linguistics, 2019. doi:
10.18653/v1/N19-1423.
R. J. Dickson and G. B. Gloor. Protein sequence alignment analysis by local covariation: coevolution
statistics detect benchmark alignment errors. PLoS One, 7(6):e37645, 2012. doi: 10.1371/journal.
pone.0037645.
H. Fang, P. Xiong, L. Xu, and Y. Chen. CLIP2Video: mastering video-text retrieval via image CLIP.
ArXiv, 2106.11097, 2021.
11
Under review as a conference paper at ICLR 2022
K.	Frans, L. B. Soros, and O. Witkowski. CLIPDraw: exploring text-to-drawing synthesis through
language-image encoders. ArXiv, 2106.14843, 2021.
F.	A. Galatolo, M. G. C. A. Cimino, and G. Vaglini. Generating images from caption and vice versa
via CLIP-guided generative latent space search. ArXiv, 2102.01645, 2021.
B.	Gao and L. Pavel. On the properties of the softmax function with application in game theory and
reinforcement learning. ArXiv, 2017.
T. Gao, X. Yao, and D. Chen. SimCSE: simple contrastive learning of sentence embeddings. ArXiv,
2104.08821, 2021.
E. Gardner. Multiconnected neural network models. Journal ofPhysics A, 20(11):3453-3464, 1987.
doi: 10.1088/0305-4470/20/11/046.
R. Geirhos, J.-H. Jacobsen, C. Michaelis, R. S. Zemel, W. Brendel, M. Bethge, and F. A. Wichmann.
Shortcut learning in deep neural networks. ArXiv, 2004.07780, 2020.
J.-B. Grill, F. Strub, F. AItCha C. Tallec, P. H. Richemond, E. Buchatskaya, C. Doersch, B. Avila Pires,
Z. D. Guo, M. Gheshlaghi Azar, B. Piot, K. Kavukcuoglu, R. Munos, and M. Valko. Bootstrap your
own latent - a new approach to self-supervised learning. In H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33,
pp. 21271-21284. Curran Associates, Inc., 2020.
M. Gutmann and A. Hyvarinen. Noise-contrastive estimation: A new estimation principle for unnor-
malized statistical models. In Y. W. Teh and M. Titterington (eds.), Proceedings of the Thirteenth
International Conference on Artificial Intelligence and Statistics, volume 9 of Proceedings of
Machine Learning Research, pp. 297-304. JMLR Workshop and Conference Proceedings, 2010.
T. Han, W. Xie, and A. Zisserman. Self-supervised co-training for video representation learning.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural
Information Processing Systems, volume 33, pp. 5679-5690. Curran Associates, Inc., 2020.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
K. He, H. Fan, Y. Wu, S. Xie, and R. B. Girshick. Momentum contrast for unsupervised visual
representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), 2020.
O. J. Henaff, A. Srinivas, J. DeFauw, A. Razavi, C. Doersch, S. M. A. Eslami, and A. vanDenOord.
Data-efficient image recognition with contrastive predictive coding. ArXiv, 1905.09272, 2019.
M. L. Henderson, R. Al-Rfou, B. Strope, Y.-H. Sung, L. Lukdcs, R. Guo, S. Kumar, B. Miklos, and
R. Kurzweil. Efficient natural language response suggestion for smart reply. ArXiv, 1705.00652,
2017.
J. J. Hopfield. Neural networks and physical systems with emergent collective computational abilities.
Proceedings of the National Academy of Sciences, 79(8):2554-2558, 1982.
J. J. Hopfield. Neurons with graded response have collective computational properties like those of
two-state neurons. Proceedings of the National Academy of Sciences, 81(10):3088-3092, 1984.
doi: 10.1073/pnas.81.10.3088.
D. Horn and M. Usher. Capacities of multiconnected memory models. J. Phys. France, 49(3):
389-395, 1988. doi: 10.1051/jphys:01988004903038900.
G. Ilharco, M. Wortsman, N. Carlini, R. Taori, A. Dave, V. Shankar, H. Namkoong, J. Miller,
H. Hajishirzi, A. Farhadi, and L. Schmidt. OpenCLIP, 2021.
D. P. Kingma, S. Mohamed, D .J. Rezende, and M. Welling. Semi-supervised learning with deep gen-
erative models. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger
(eds.), Advances in Neural Information Processing Systems 27, pp. 3581-3589. Curran Associates,
Inc., 2014.
12
Under review as a conference paper at ICLR 2022
J.	Krause, M. Stark, J. Deng, and L. Fei-Fei. 3D object representations for fine-grained categorization.
In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13), 2013.
K.	E. Kreth and A. A. Fodor. Covariance in protein multiple sequence alignments using groups of
columns. ArXiv, 1401.1141, 2014.
D. Krotov and J. J. Hopfield. Dense associative memory for pattern recognition. In D. D. Lee,
M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information
Processing Systems, pp. 1172-1180. Curran Associates, Inc., 2016.
C. H. Lampert, H. Nickisch, and S. Harmeling. Learning to detect unseen object classes by between-
class attribute transfer. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 951-958. IEEE, 2009.
S. Lapuschkin, S. Waldchen, A. Binder, G. Montavon, W. Samek, and K.-R. Muller. Unmasking
Clever Hans predictors and assessing what machines really learn. Nature Communications, 10,
2019. doi: 10.1038/s41467-019-08987-4.
J. Li, P. Zhou, C. Xiong, R. Socher, and S. C. H. Hoi. Prototypical contrastive learning of unsupervised
representations. In International Conference on Learning Representations (ICLR), 2021. URL
https://openreview.net/forum?id=KmykpuSrjcq. ArXiv 2005.04966.
L.	Logeswaran and H. Lee. An efficient framework for learning sentence representations. In
Sixth International Conference on Learning Representations (ICLR), 2018. URL https://
openreview.net/forum?id=rJvJXZb0W. ArXiv 1803.02893.
I.	Loshchilov and F. Hutter. SGDR: stochastic gradient descent with warm restarts. In 5th
International Conference on Learning Representations ICLR. OpenReview.net, 2017. URL
https://openreview.net/forum?id=Skq89Scxx.
I.	Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference
on Learning Representations (ICLR), 2019. URL https://openreview.net/forum?id=
Bkg6RiCqY7.
H. Luo, L. Ji, M. Zhong, Y. Chen, W. Lei, N. Duan, and T. Li. CLIP4Clip: an empirical study of
CLIP for end to end video clip retrieval. ArXiv, 2104.08860, 2021.
D. McAllester and K. Stratos. Formal limitations on the measurement of mutual information. ArXiv,
1811.04251, 2018.
D. McAllester and K. Stratos. Formal limitations on the measurement of mutual information. In Silvia
Chiappa and Roberto Calandra (eds.), Proceedings of the Twenty Third International Conference
on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research,
pp. 875-884. PMLR, 26-28 Aug 2020. URL https://proceedings.mlr.press/v108/
mcallester20a.html.
T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words
and phrases and their compositionality. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani,
and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume 26, pp.
3111-3119. Curran Associates, Inc., 2013.
T. Milbich, K. Roth, S. Sinha, L. Schmidt, M. Ghassemi, and B. Ommer. Characterizing generalization
under out-of-distribution shifts in deep metric learning. ArXiv, 2107.09562, 2021.
J.	Miller, R. Taori, A. Raghunathan, S. Sagawa, P. W. Koh, V. Shankar, P. Liang, Y. Carmon, and
L. Schmidt. Accuracy on the line: On the strong correlation between out-of-distribution and
in-distribution generalization. ArXiv, 2107.04649, 2021.
I.	Misra and L. vanDerMaaten. Self-supervised learning of pretext-invariant representations. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),
2020.
M. Narasimhan, A. Rohrbach, and T. Darrell. CLIP-It! language-guided video summarization. ArXiv,
2107.00650, 2021.
13
Under review as a conference paper at ICLR 2022
X. Nguyen, M. J. Wainwright, and M. Jordan. Estimating divergence functionals and the likelihood
ratio by penalized convex risk minimization. IEEE Transactions on Information Theory, 56(11):
5847-5861,2010. doi:10.1109/tit.2010.2068870.
M.-E. Nilsback and A. Zisserman. Automated flower classification over a large number of classes.
In Proceedings of the 2008 Sixth Indian Conference on Computer Vision, Graphics and Image
Processing, pp. 722-729. IEEE Computer Society, 2008. doi: 10.1109/ICVGIP.2008.47.
F. W. J. Olver, D. W. Lozier, R. F. Boisvert, and C. W. Clark. NIST handbook of mathematical
functions. Cambridge University Press, 1 pap/cdr edition, 2010. ISBN 9780521192255.
D. Pakhomov, S. Hira, N. Wagle, K. E. Green, and N. Navab. Segmentation in style: Unsupervised
semantic image segmentation with stylegan and CLIP. ArXiv, 2107.12518, 2021.
B. Poole, S. Ozair, A. vanDenOord, A. A. Alemi, and G. Tucker. On variational bounds of mutual
information. In K. Chaudhuri and R. Salakhutdinov (eds.), Proceedings of the 36th International
Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp.
5171-5180. PMLR, 2019.
D. Psaltis and H. P. Cheol. Nonlinear discriminant functions and associative memories. AIP
Conference Proceedings, 151(1):370-375, 1986. doi: 10.1063/1.36241.
A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin,
J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language
supervision. In Proceedings of the 38th International Conference on Machine Learning (ICML),
2021.
H. Ramsauer, B. Schafl, J. Lehner, P. Seidl, M. Widrich, L. Gruber, M. Holzleitner, M. Pavlovic, G. K.
Sandve, V. Greiff, D. Kreil, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter. Hopfield
networks is all you need. ArXiv, 2008.02217, 2020.
H. Ramsauer, B. Schafl, J. Lehner, P. Seidl, M. Widrich, L. Gruber, M. Holzleitner, M. Pavlovic, G. K.
Sandve, V. Greiff, D. Kreil, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter. Hopfield
networks is all you need. In 9th International Conference on Learning Representations (ICLR),
2021. URL https://openreview.net/forum?id=tL89RnzIiCd.
B. Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do ImageNet classifiers generalize to ImageNet?
In K. Chaudhuri and R. Salakhutdinov (eds.), Proceedings of the 36th International Conference
on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 5389-5400.
PMLR, 2019.
P. Seidl, P. Renz, N. Dyubankova, P. Neves, J. Verhoeven, J. K. Wegner, S. Hochreiter, and G. Klam-
bauer. Modern hopfield networks for few- and zero-shot reaction prediction. ArXiv, 2104.03279,
2021.
P. Sharma, N. Ding, S. Goodman, and R. Soricut. Conceptual captions: A cleaned, hypernymed,
image alt-text dataset for automatic image captioning. In Proceedings of ACL, 2018.
S. Shen, L. H. Li, H. Tan, M. Bansal, A. Rohrbach, K.-W. Chang, Z. Yao, and K. Keutzer. How much
can CLIP benefit vision-and-language tasks? ArXiv, 2107.06383, 2021.
K.	Soomro, A. R. Zamir, and M. Shah. A dataset of 101 human action classes from videos in the
wild. Center for Research in Computer Vision, 2(11), 2012.
J.	Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. The German traffic sign recognition benchmark:
A multi-class classification competition. The 2011 International Joint Conference on Neural
Networks, pp. 1453-1460, 2011.
R. Taori, A. Dave, V. Shankar, N. Carlini, B. Recht, and L. Schmidt. Measuring robustness to
natural distribution shifts in image classification. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
18583-18599. Curran Associates, Inc., 2020.
14
Under review as a conference paper at ICLR 2022
B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland, D. Borth, and L.-J. Li.
YFCC100M: The new data in multimedia research. Commun. ACM, 59(2):64-73, 2016. doi:
10.1145/2812802.
Y.-H. H. Tsai, M. Q. Ma, H. Zhao, K. Zhang, L.-P. Morency, and R. Salakhutdinov. Conditional
contrastive learning: Removing undesirable information in self-supervised representations. ArXiv,
2106.02866, 2021.
M. Tschannen, J. Djolonga, P. K. Rubenstein, S. Gelly, and M. Lucic. On mutual informa-
tion maximization for representation learning. arXiv, 1907.13625, 2019. URL https:
//openreview.net/forum?id=rkxoh24FPH. 8th International Conference on Learning
Representations (ICLR).
A. van den Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding.
ArXiv, 1807.03748, 2018.
M. J. Wainwright. Basic tail and concentration bounds, pp. 21-57. Cambridge Series in Statistical and
Probabilistic Mathematics. Cambridge University Press, 2019. doi: 10.1017/9781108627771.002.
F. Wang and H. Liu. Understanding the behaviour of contrastive loss. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2495-2504, 2021.
T. Wang and P. Isola. Understanding contrastive representation learning through alignment and
uniformity on the hypersphere. In Proceedings of the 37th International Conference on Machine
Learning (ICML), 2020.
M. P. Wellman and M. Henrion. Explaining ’explaining away’. IEEE Trans. Pattern Anal. Mach.
Intell., 15(3):287-292, 1993. doi: 10.1109/34.204911.
M. Widrich, B. Schafl, M. PavloviC, H. Ramsauer, L. Gruber, M. Holzleitner, J. Brandstetter, G. K.
Sandve, V. Greiff, S. Hochreiter, and G. Klambauer. Modern Hopfield networks and attention for
immune repertoire classification. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and
H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 18832-18845.
Curran Associates, Inc., 2020.
M. Wortsman, G. Ilharco, M. Li, J. W. Kim, H. Hajishirzi, A. Farhadi, H. Namkoong, and L. Schmidt.
Robust fine-tuning of zero-shot models. ArXiv, 2109.01903, 2021.
M. Wu, M. Mosse, C. Zhuang, D. Yamins, and N. Goodman. Conditional negative sampling for
contrastive learning of visual representations. In International Conference on Learning Represen-
tations (ICLR), 2021. URL https://openreview.net/forum?id=v8b3e5jN66j.
Z. Wu, Y. Xiong, S. X. Yu, and D. Lin. Unsupervised feature learning via non-parametric instance
discrimination. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 3733-3742, Los Alamitos, CA, USA, 2018. IEEE Computer Society. doi: 10.1109/
CVPR.2018.00393.
C.-H. Yeh, C.-Y. Hong, Y.-C. Hsu, T.-L. Liu, Y. Chen, and Y. LeCun. Decoupled contrastive learning.
ArXiv, 2110.06848, 2021.
K.	Zhou, J. Yang, C. C. Loy, and Z. Liu. Learning to prompt for vision-language models. ArXiv,
2109.01134, 2021.
15
Under review as a conference paper at ICLR 2022
A Appendix
This appendix consists of four sections (A.1-A.4). Section A.1 provides the theoretical properties
of the InfoLOOB and InfoNCE. It is shown how to derive that InfoNCE is a lower bound on
mutual information. Further it is shown how to derive that InfoLOOB is an upper bound on mutual
information. The proposed loss function LInfoLOOB and its gradients are discussed. In Section A.2
we discuss the estimation of mutual information for a toy example. Section A.3 provides details on
the experiments for Section 4. Section A.4 briefly reviews continuous modern Hopfield networks.
Section A.5 discusses further related work.
Contents of the appendix
A Appendix ........................................................................ 16
A.1	InfoLOOB vs. InfoNCE ...................................................... 17
A.1.1 InfoNCE: Lower Bound on Mutual Information ........................... 17
A.1.2 InfoLOOB: Upper Bound on Mutual Information .......................... 21
A.1.3 InfoLOOB: Analysis of the Objective .................................. 25
A.1.4 InfoNCE and InfoLOOB: Gradients ...................................... 32
A.1.5 InfoLOOB and InfoNCE: Probability Estimators ......................... 34
A.1.6 InfoLOOB and InfoNCE: Losses ......................................... 36
A.2	Mutual Information Estimation ............................................. 39
A.3	Experiments ............................................................... 39
A.3.1 Ablation studies ..................................................... 39
A.3.2 Hyperparameters ...................................................... 41
A.3.3 Datasets ............................................................. 41
A.3.4 Zero-shot evaluation ................................................. 42
A.3.5 Linear probing ....................................................... 42
A.4 Review of Modern Hopfield Networks ......................................... 43
A.5 Further Related Work ....................................................... 46
List of theorems
A1 Theorem (InfoNCE lower bound) .............................................. 19
A2 Theorem (InfoLOOB upper bound) ............................................. 23
A3 Theorem (Weighted Covariances) ............................................. 38
A4 Theorem (Modern Hopfield Networks: Retrieval with One Update) .............. 45
A5 Theorem (Modern Hopfield Networks: Exponential Storage Capacity) ........... 45
List of definitions
A1 Definition (Pattern Stored and Retrieved) .............................................. 45
List of figures
A1 Estimated mutual information of different objectives ..................................... 40
A2 Visualization of zero-shot classification of three examples from each dataset ............ 44
List of tables
A1
A2
A3
A4
Influence of loss functions and Hopfield retrieval
Influence of learning rate scheduler
Datasets used for zero-shot and linear probing .........................
Linear probing for CLIP (reimplementation) and CLOOB trained on YFCC
40414143
16
Under review as a conference paper at ICLR 2022
A.1 INFOLOOB VS. INFONCE
A.1.1 InfoNCE: Lower Bound on Mutual Information
We derive a lower bound on the mutual information between random variables X and Y distributed
according to p(x, y). The mutual information I(X ; Y ) between random variables X and Y is
I(X ; Y) = Ep(x,y) ln P(X) ,p(y)	= Ep(x,y)
(A1)
ι P(X I y)
n p(x)
Ep(x,y)
ι p(y I χ)
n p(y)
“InfoNCE” has been introduced in van den Oord et al. (2018) and is a multi-sample bound. In
the setting introduced in van den Oord et al. (2018), we have an anchor sample y given. For the
anchor sample y we draw a positive sample X1 according to p(X1 I y). Next, we draw a set
X = {X2, . . . , XN} according to p(X), which are n - 1 negative samples drawn iid according to
p(X). We have drawn a set X = {X1, X2, . . . , XN} according to p(X I y), which is one positive
sample X1 drawn by p(X1 I y) and N - 1 negative samples {X2, . . . , XN} drawn iid according to
p(X).
The InfoNCE with probabilities is
IInfoNCE(X1 ; Y)
Ep(y) [Ep(XIyjn (NPN7JPxy,IXi))
(A2)
where We inserted the factor N in contrast to the original version in van den Oord et al. (2018), where
we followed Poole et al. (2019); Tschannen et al. (2019); Cheng et al. (2020); Chen et al. (2021).
The InfoNCE with score function f (X, y) is
IInfoNCE(X1 ; Y) = Ep(y)
Ep(XIyjn (N PNfy1,y))U
(A3)
The InfoNCE with probabilities can be rewritten as:
IInfoNCE(X1 ; Y)
Ep(y JP(Tyjn (N £「Pxy,Ixi))
(A4)
Ep(y)
Ep(y)
Ep(XIy)
P(XIIy)
______P(XI)_______
1 PN	p(χi∣y)
N i=i=l P(Xi)
This is the InfoNCE withf(x, y) = P(y I x).
Set of pairs. The InfoNCE can be written in a different setting Poole et al. (2019), which is
used in most implementations. We sample N pairs independently from P(x, y), which gives Z =
{(x1, y1), (x2, y2), . . . , (xN, yN)}. The InfoNCE is then
IInfoNCE (X ; Y) = EP(XIy)
1 XI	f(xi, yi)
N ⅛ n N Pj=ιf(x, y,).
(A5)
17
Under review as a conference paper at ICLR 2022
Following van den Oord et al. (2018) we have
IInfoNCE(X1 ; Y ) = Ep(y)
= Ep(y)
= Ep(y)
Ep(X|y)
Ep(X|y)
Ep(X|y)
P(XIIy)
______P(XI)_______
1 PN	p(χi∣y)
N i=1=l	P(Xi)
(A6)
ln P(XI | y) QN2P(Xl)
PlLIP(XI I y) ∏l=ιP(Xl)
+ ln(N)
EP(y) EP(X|y) [lnP(i = 1 IX,y)] + ln(N) ,
where P(i = 1 I X, y ) is the probability that sample X1 is the positive sample if we know there exists
exactly one positive sample in X.
The InfoNCE is a lower bound on the mutual information. The following inequality is from van den
Oord et al. (2018):
S 口	口	[i	/P(Xι i y)
；Y) = Ep(y) EP(XIIy) [ln ( P(Xl)
(A7)
= EP(y)
≥ EP(y)
≈ EP(y)
= EP(y)
EP(X1 Iy)
EP(X1 Iy)
EP(XIy)
EP(X Iy)
ln I-)
ln
p(xi))
p(xi I y) )
1 + ɪ p(xi) Xp(xi I y)
N	N p(xi I y) 1=2 p(xi)
P(XIIy)
______________P(XI)
1 P(XIIy) + ɪ PN P(XiIy)
N	p(xi )	+ N 乙 1=2 P(Xi)

1
N
IInfoNCE(X1 ; Y) ,
where the "≥" is obtained by bounding ln(1/N + a) by ln(a), which gives a bound that is not very
P(XI)
P(XIIy)
tight, since a
assume
can become small. However for the "≈" van den Oord et al. (2018) have to
X P(y ∣X) ≥ I
1=2 P(y)	一 ,
(A8)
1 X P(Xi I y) = 1
N 1=2 p(xi)	N
which is unclear how to ensure.
For a proof of this bound see Poole et al. (2019).
We assumed that for the anchor sample y a positive sample X1 has been drawn according to P(X1 I y).
Aset X = {x2, ..., XN} of negative samples is drawn according to p(x). Therefore, We have a
set X = {X1, X2, . . . , XN} that is drawn with one positive sample X1 and N - 1 negative samples
^V7∙	r	1 -t-r T- 1..
X = {X2, . . . , XN}. We have
N
P(X) = YP(X1) ,	(A9)
1=2
N
P(X I y) = P(X1 I y) YP(X1) ,	(A10)
1=2
N
P(X) = YP(X1) .	(A11)
1=1
18
Under review as a conference paper at ICLR 2022
Next, we present a theorem that shows this bound, where we largely follow Poole et al. (2019) in the
proof. In contrast to Poole et al. (2019), we do not use the NWJ bound Nguyen et al. (2010). The
mutual information is
I(X1 ； Y) = EP(χι,y) [ln (pp*)
(A12)
Theorem A1 (InfoNCE lower bound). InfoNCE with score function f(x,y) according to Eq. (A3)
is a lower bound on the mutual information.
l (	f(χι, y)	ʌ
RlPfZyy)_
I(X1 ； Y) ≥ Ep(y)p(X|y)
IInfoNCE(X1 ； Y) .
(A13)
InfoNCE with probabilities according to Eq. (A2) is a lower bound on the mutual information.
I(X1 ； Y) ≥ Ep(y)p(X|y)
ln (	p(y | χι)
N PN=I p(y | χi)
IInfoNCE(X1 ； Y) .
(A14)
The second bound Eq. (A14) is a special case of the first bound Eq. (A13).
Proof. Part (I): Lower bound with score function f(x, y).
For each set X = {x2 , . . . , xN }, we define as data-dependent (depending on X) score function
g(x1, y, X) that is based on the score function f(x, y). Therefore we have for each X a different
data-dependent score function g based on f . We will derive a bound on the InfoNCE, which is the
expectation of a lower bond on the mutual information over the score functions. For score function
g(x1, y, X), we define a variational distribution q(x1 | y, X) overx1:
	q(χι I y,X);	_ p(xι) g(xι, y,X) 一	~	, Z (y,x)	(A15)
	Z(y,X);	Ep(x1 ) g(x1, y, X) ,	(A16)
which ensures	q(x1	I y,X) dχι = 1.	(A17)
We have	q(χι I y,- p(χι)	X) __ g(χι, y,x) 		~	. Z (y,x)	(A18)
For the function g , we set	g(χι, y,X)	-	f(χι, y)	(A19)
		NN PN=ι f(χi, y)，	
For the function f we use	f (x1, y) =	exp(τ -1 sim(χ1, y)) ,	(A20)
where sim(x, y) is typically the cosine similarity.
We next show that InfoNCE is a lower bound on the mutual information.
19
Under review as a conference paper at ICLR 2022
I(X1 ; Y )
=Ep(X)
=Ep(X)
≥ Ep(X)
Ep(X) [I(X1 ; Y)] = Ep(X) Ep(χι,y) ln p；(x1y，]]
Ep(x1,y)
Ep(x1,y)
Ep(x1,y)
ln ( p(χι I y) q(χι I y,X) ʌ
1q(χι I y,X)	P(XI)	)
ln q(Xp(Xy J] + Ep(y) [KL(P(X1 I y) k q(xι ∣ y,X))]
q(χι | y,X)##	"E	"l g(χι,y,X)##
ln P(χι) JJ = Ep(X) [Ep(X1,y) [ Z (y,X)]]
(A21)
Ep(X)IEp(χι,y) [lng(χι, y,X) - ln(Ep(x1) [g(χι, y,X)])□
Ep(X)IEp(y) IEp(XiIy) [lng(χι, y,X)] - ln (Epg) [g(χι, y,^X)])]]
Ep(X)IEp(y) IEp(XIIyjlng(x1, y,X)]]] - Ep(X) [Ep(y)卜n (Ep(xi) [g(x1, y,X)])]]
≥ Ep(y)p(XIy) [lng(χι, y,X)] - Ep(X)
[Ep(y) IEp(XI)Ig(X1, y,X)] - 1]]
Ep(y)p(X Iy)
Ep(y)p(X Iy)
Ep(y)p(X Iy)
Ep(y)p(X Iy)
ln	f (X1, y)
.NN ρN=ι f(χi, y)
ln	f(χι, y)
.N PN=If(Xi, y)
ln	f(χι, y)
.Nn PN=i f(xN, y)
ln	f(χι, y)
.N PN=ι f(χi, y)
Ep(y)
Ep(y)
Ep(y)
Ep(X)
f(Xι, y)
N PN=ιf (Xi, y)
-1
1 XX E ∣^ f (m y)
N h p(X) k P3f(XN, y)
Ep(X)
N PlLIf(Xi, y)
.寺 PN=ι f(XN, y)
-1
-1
	
	
	
IInfoNCE(X1 ; Y) .
For the first "≥" we used that the Kullback-Leibler divergence is non-negative. For the second "≥
we used the inequality ln a 6 a - 1 for a > 0.
Part (II): Lower bound with probabilities.
If the score function f is
f(X, y) = p(y I X) ,
(A22)
then the bound is
I(X1 ; Y) ≥ Ep(y)p(XIy)
∣ (	f(Xι, y)	Y =E	L (	Py | Xi)	Y
U pN=i f(Xi, y)n = p(y)p(XIy)I Q PN=1 P(y I Xi))
(A23)
Ep(y)p(XIy)
IInfoNCE(X1 ; Y) .
This is the bound with probabilities in the theorem.
□
20
Under review as a conference paper at ICLR 2022
A.1.2 InfoLOOB: Upper B ound on Mutual Information
We derive an upper bound on the mutual information between random variables X and Y distributed
according to p(x, y). The mutual information I(X ; Y ) between random variables X and Y is
I(X ; Y)
Ep(x,y)
ln P(X, y)-
.p(χ) p(y) 一
P(X I y)
Mx，y) ln P(X)
Ep(x,y)
l p(y I χ)一
n ^7(yΓ .
(A24)
In Poole et al. (2019) Eq. (13) introduces a variational upper bound on the mutual information,
which has been called "Leave one out upper bound" (called "L1Out" in Cheng et al. (2020)). For
simplicity, we call this bound "InfoLOOB", where LOOB is an acronym for "Leave One Out Bound".
In contrast to InfoNCE, InfoLOOB is an upper bound on the mutual information. InfoLOOB is analog
to InfoNCE except that the negative samples do not contain a positive sample. Fig. 1 and Fig. 2 in
Cheng et al. (2020) both show that InfoLOOB is a better estimator for the mutual information than
InfoNCE (van den Oord et al., 2018), MINE (Belghazi et al., 2018), and NWJ (Nguyen et al., 2010).
The InfoLOOB with score function f(X, y) is defined as
IInfoLOOB(X1 ; Y)
Ep ⑻卜(XIy)卜 L PfyLy))
(A25)
The InfoLOOB with probabilities is defined as
IInfoLOOB(X1 ; Y )
Ep(”(X|y)卜(N-1 PN2：(；1 Xi))
(A26)
This is the InfoLOOB withf(X, y) = P(y I X).
The InfoLOOB with probabilities can be written in different forms:
IInfoLOOB(X1 ; Y )
Ep(y)	Ep(X|y)
Ep(”(X1y)〕ln (N-I PNl2x1y∣Xi))
(A27)
P(XI Iy)
P(XI)_______
1 PN	p(χi∣y)
N —1 乙 i=2 P(Xi)
Set of pairs. The InfoLOOB can we written in a different setting (Poole et al., 2019), which will
be used in our implementations. We sample N pairs independently from P(X, y), which gives
X = {(X1, y1), (X2, y2), . . . , (XN, yN)}. The InfoLOOB is then
IInfoLOOB (X ; Y ) = EP(XIy) ]XX In ( , PNx y∖~7 )1 .	(A28)
_	i=1	\N—1 l^j=1,j=i f (Xj, yi)/ _|
We assume that an anchor sample y is given. For the anchor sample y we draw a positive sample
Xi according to p(xi | y). Next, We draw a set X = {x2, ..., XN} of negative samples according
to P(X ∣ y). For a given y, the X that have a largeP(X ∣ y) are drawn with a lower probability
P(X ∣ y) compared to random drawing via P(X). The negatives are indeed negatives. We have
drawn first anchor sample y and then X = {X1, . . . , XN}, where X1 is drawn according to P(X1 I y)
and X = {x2,..., XN} are drawn iid according toP(X ∣ y). We have
N
P(X I y) = YP(χi I y),	(A29)
i=2
N
P(x I y) = p(xi I y) YP(Xi I y),	(A3。)
i=2
N
P(X I y) p(xi) = p(xi) YP(Xi I y).	(A31)
i=2
21
Under review as a conference paper at ICLR 2022
We assume for score function f (x, y)
∀y∀x : 0 < f (x, y) .
We ensure this by using for score function f
f(x, y) = exp(τ -1 sim(x, y)) ,
where sim(x, y) is typically the cosine similarity.
InfoLOOB with score function f (x, y) is
(A32)
(A33)
(A34)
IInfoLOOB(X ; Y )
Ep(y[ Ep(XIJn (N-I P N1；； y))
The reference constant Z(y) gives the average score f(x,y), if the negatives for yare selected with
lower probability via P(X | y) than with random drawing according to P(X).
Z(y) = Ep(x∣y) [f(x, y)] .	(A35)
We define the variational distribution
q(χ I y) = P(X)f(X,y) ,	Z*(y) = Ep(X) [f (χ,y)] .	(A36)
Z (y)
With the variational distribution q(X | y), we express our main assumption. The main assumption
for the bound is:
Ep(y) [KL(p(x | y) k q(x I y))] 6 Ep(y) [lnZ*(y) TnZ(y)] .	(A37)
This assumption can be written as
E	∖E	N (p(y | x) Z(y)、][ 6 0	(A38)
EpQ)产 P(XIy) [ln ( p®) f(χ, y)力 J 6 0 .	(A 猫
This assumption ensures that the X with large P(X I y)) are selected with lower probability via
P(X ∣ y) than with random drawing according to p(x). The negatives are ensured to be real negatives,
that is, P(X I y) is small and so is f (X, y). Consequently, we make sure that we draw X with
sufficient small f(X,y). The Kullback-Leibler gives the minimal required gap between drawing
f (x, y) viaP(X) and drawing f (x, y) viaP(X ∣ y).
EXAMPLE. With h(y) > 0, we consider the setting
f (X, y) = Ppy PXy)h(y) ,	(A39)
— P(X) P(y)	L心—n [(p®	| X)	h(y)∖ TI ∕a4C'
P(X | y) =	h(y) p(y ∣ X) C(y)	, Cpy)	=	Ep(X) [(-)	.	. (A40)
The main assumption becomes
Ep(y) [Ep(χ∣y) [ln 黑]]6 0 ∙	(A41)
The main assumption holds since
Z(y)
EP(XIy)
ρ(y I x) h(y)
p(y).
P(X) C(y)-1 dX
P(y I χ) h(y)'
P(y)
= P	P(X) P(y)	P(y I χ) h(y) d
J h(y) P(y I χ) C(y) P(y)	"
c(y)-1 =(Ep(x)"( p⅛F )-1!-
-1∖ 一口 「P(y i x) h(y)-
=Ep(X) [ ~^y]
(A42)
/ P(y, χ) h(y)
P P(y)
h(y) ,
6
22
Under review as a conference paper at ICLR 2022
where we used for the 6 Jensen’s inequality with the function f (a) = 1/a, which is convex for
a > 0.
For score function f (x, y) and distribution P(X | y) for sampling the negative samples, We have
defined:
Z(y)	=Ep(χ∣y) [f (x, y)],	(A43)
z*(y)=	Ep(x) [f (X, y)] ,	(A44)
q(X | y)	=P(X) f (x, y) 一	Z * (y).	(A45)
Next theorem gives the upper bound of the InfoLOOB on the mutual information, Which is
I(XI; Y) = Ep(χ1,y) ιn P(P(X1y：.	(A46)
Theorem A2 (InfoLOOB upper bound). If X = {x2,..., XN} are drawn iid according to P(X | y)
and if the main assumption holds:
Ep(y) [KL(p(x | y) k q(x | y))] 6 Ep(y) [lnZ*(y) TnZ(y)] .	(A47)
Then InfoLOOB with score function f(X, y) as in Eq. (A25) is an upper bound on the mutual
information:
l(Xl ； Y) 6 Ep(y) Ep(X|y)
ln
f (χι, y)	ʌ
PN=2 f (Xi, y)J_ _
IInfoLOOB (X1 ; Y) . (A48)
Ifthe negative samples X = {X2,..., XN} are drawn iid according to P(X), then InfoLOOB with
probabilities according to Eq. (A26) is an upper bound on the mutual information:
I(X1 ; Y) 6 Ep(y) Ep(X|y)
P(y I xi)
P‰P(y I Xi)
IInfoLOOB (X1 ; Y) . (A49)
The second bound Eq. (A49) is a special case of the first bound Eq. (A48).
Proof. Part (I): Upper bound With score function f(X, y).
23
Under review as a conference paper at ICLR 2022
I(X1 ; Y) = Ep(x1,y)
l ρ(χι I y)
.n p(xι)
(A50)
Ep(x1,y)
Ep(x1,y)
6 Ep(x1,y)
Ep(x1,y)
Ep(x1,y)
Ep(x1,y)
Ep(x1,y)
ln
ln
ln
ln
ln
ln
p(x1 | y) q(x1 | y)
qx1 | y)
q(χι I y)一
p(χι) 一
q(χι I y)一
p(χι) 一
q(χι I y)
p(χι)
p(x1)
+ Ep(y) [KL(p(x1 | y) k q(x1 | y))]
+ Ep(y) ln Ep(x1) [f(x1, y)] - ln Z(y)
+ ln
Ep(x1) [f (x1, y)]
Z(y)
f(x1,y)	Ep(x1) [f(x1,y)]
Ep(x1) [f(X1, y)]
f(χι, y)一
~(y^r.
Z(y)
ln ----------r f (X1, y)----------T
Ep(XIy) [n-1 pi=2 f(Xi, y)∖
Ep(x1 ,y) [ln f(x1 , y)] - Ep(y)
6 Ep(x1 ,y) [ln f (x1 , y)] - Ep(y)
Ep(y)
Ep(XIy)
ln
f(χι, y)	ʌ
PN=2 f (Xi y))_ _
ln Ep(XIy)
Ep(XIy)
1
N - 1
ln Nj-I
N
X f(xi, y)
i=2
N
X f(xi, y)
i=2
i))
!##
= IInfoLOOB (X1 ; Y ) ,
where the first "6" uses assumption Eq. (A37), while Jensens’s inequality was used for the second
"6" by exchanging the expectation and the "ln". We also used
Ep(XIy)
1N
N-I X f(χi, y)
1
N - 1
N
EEp(XiIy) [f (xi, y)]
i=2
1N
N-I X Z ⑹
Z(y) .
(A51)
Part (II): Upper bound with probabilities.
If the score function f is
and
then
f(X, y)	= P(y I X)		(A52)
P(X I y) = p(χ),			(A53)
P(X I y)=	P(X I y) ,		(A54)
Z(y) =	Ep(x) [P(y I X)]	P(y) ,	(A55)
Z*(y)=	Ep(x) [P(y I X)]	P(y) ,	(A56)
q(X I y) =	P(X) p(y I χ)= p(y)	P(X I y) ,	(A57)
KL(P(X I y) k q(X I y)) =	KL(P(X I y) k P(	X I y)) = 0.	(A58)
24
Under review as a conference paper at ICLR 2022
Therefore, the main assumption holds, since
0 = Ep(y) [KL(p(x | y) k q(x | y))] = Ep(y) [lnZ*(y) - lnZ(y)] .	(A59)
The bound becomes
I(X1 ; Y ) 6 Ep(y)
Ep(y)
Ep(X|y)
Ep(X |y)
ln
p(y | x1)
PN=2 p(y | Xi)
p(y|xI)	∖
)))
(A60)
ln __________ p(y) ________
1 PN	p(y|Xi)
N — 1 ∕~^i=2	p(y)
IInfoLOOB(X1 ; Y ) .
An alternative proof is as follows:
I(X1 ; Y ) = I(X1 ; Y) - Ep(y)
I(X1 ; Y ) - Ep(y)
6 I(X1 ; Y ) - Ep(y)
Ep(y)
Ep(y)
Ep(x1 |y)
Ep(X |y)
ln Ep(X |y)
Ep(X|y)
N
X
i=2
N
X
i=2
- Ep(y)
一 (P(XI I y) M 一
.I P(Xi)川
(P(XIIy)
_____P(Xl)____
1 PN P(XiIy)
N —1 乙＜i=2 P(Xi)
ln (Ji X
i=2
p(y I Xi)
p(y)
p(y I Xi)
p(y)
Ep(X |y)
(A61)
)))
!##
ln N-I
XX P(Xi | y))
h P(Xi)儿
1
N - 1
ln Njn
= IInfoLOOB(X1 ; Y ) .
where we applied Jensens’s inequality for the exchanging the expectation and the "ln" to obtain the
"6" inequality.
□
Experiments that compare upper and lower bounds as mutual information estimates are provided
in Cheng et al. (2020) and in Poole et al. (2019). In Fig. 2 in Cheng et al. (2020) it is shown that
InfoLOOB is a good estimator of the mutual information.
A.1.3 InfoLOOB: Analysis of the Objective
This subsection justifies the maximization of the InfoLOOB bound for contrastive learning. Maxi-
mizing the InfoLOOB bound is not intuitive as it was introduced as an upper bound on the mutual
information in the previous subsection. Still maximizing the InfoLOOB bound leads to a good
approximation of the mutual information, in particular for high mutual information.
InfoLOOB with a neural network as a scoring function is not an upper bound on the mutual in-
formation when not under-sampling. As we use InfoLOOB on training data for which we do not
know the sampling procedure, we cannot assume under-sampling. Therefore, we elaborate more on
the rationale behind the maximization of the InfoLOOB bound. (I) We show that InfoLOOB with
neural networks as scoring function is bounded from above. Therefore, there exists a maximum
and the optimization problem is well defined. (II) We show that InfoLOOB with neural networks as
scoring function differs by two terms the mutual information. The first term is the Kullback-Leibler
divergence between the variational q(X I y) and the posterior P(X I y). This divergence is minimal
for q(X I y) = P(X I y), which implies f(y I X) = P(y I X). The second term is governed by the
difference between the mean E[f (X, y)] and the empirical mean 1/(N - 1) Pi f(X, y). Hoeffding’s
inequality bounds this difference as we demonstrate in this subsection. Therefore, the second term
25
Under review as a conference paper at ICLR 2022
is negligible for large N . In contrast, the KL term is dominant and the relevant term, therefore
maximizing InfoLOOB leads tof (y | x) ≈ p(y | x).
We assume that an anchor sample y is given. For the anchor sample y, we draw a positive sample x1
according to p(xι | y). We define the set X = {x2,..., XN} of negative samples, which are drawn
iid according to p(x). We define the set X = {x1, . . . , xN}.
We have
N
P(X) = Yp(xi) ,	(A62)
i=2
N
P(X I y) = p(χι I y) Yp(χi) = p(χι I y) p(X),	(A63)
i=2
N
p(X) = YP(Xi) = p(xι) P(X) .	(A64)
i=1
We use the score function
f(X, y) = exp(τ -1 sim(X, y)) ,	(A65)
where sim(X, y) is typically the cosine similarity.
The InfoLOOB with score function f (X, y) is defined as
IInfoLOOB(X1 ; Y ) = Ep(y)
E	∣^l( f (Xι, y)	∖
Ep(XIy) [	(N-T PN=2f(χi, y))
(A66)
We define the variational distribution
q(X I y)
Z(y)
P(X) f(χ, y)
~~ziy]-,
Ep(x) [f (X, y)] .
(A67)
(A68)
26
Under review as a conference paper at ICLR 2022
The next inequality shows the relation between I(X1 ; Y ) and IInfoLOOB(X1 ; Y ) for random
variables X1 and Y .
I(X1 ； Y) = EP(χι,y) [lnp(pX(⅛y)
(A69)
Ep(x1,y)
l (p(χι I y) q(χι I y)
.111q(χι I y) p(χι)
Ep(x1 ,y)
Ep(x1 ,y)
ln
ln
q(x1 | y)
p(x1)
f (四, y) 一
~(yyΓ.
+ Ep(y) [KL(p(x1 | y) k q(x1 | y))]
+ Ep(y) [KL(p(x1 | y) k q(x1 | y))]
Ep(x1 ,y)
ln ---------r f (x1, NN)------T + Ep(y) [KL(ρ(xι I y) k q(xι ∣ y))]
∖ep(x|y) [N-Γ Pi=2 f (Xi y)∣ / _|
Ep(x1 ,y) [ln f (x1 , y)] - Ep(y)
ln Ep(X |y)
1N
N-1 Ef (χi, y)
i=2
C)
+
Ep(y) [KL(p(x1 | y) k q(x1 | y))]
+
Ep(x1 ,y) [ln f (x1 , y)] - Ep(y)
Ep(X|y)
ln (N-IX f (M y))
Ep(y)
Ep(X|y)
ln (N-I X f(χi y))
- Ep(y)
ln Ep(X|y)
1N
N-I E f(χi, y)
i=2

+
Ep(y) [KL(p(x1 | y) k q(x1 | y))]
	
Ep(y)
Ep(X|y)
ln
f(χι, y)	ʌ
PN=2 f(χi, y)J
+ Ep(y)
Ep(X|y)
ln (N-IX f (M y))
Ep(y)
ln (Ep(X∣y) ∣N-1 X f (xi, y)
i)
Ep(y) [KL(p(x1 | y) k q(x1 | y))]
+
IInfoLOOB (X1 ； Y)
+
Ep(y)
Ep(X|y)
ln (N-I X f(χi y))
Ep(y)
ln Ep(X|y)
1N
N-I X f (M y)
	

+
Ep(y) [KL(p(x1 | y) k q(x1 | y))]
IInfoLOOB (X1 ； Y)
+
Ep(y)
Ep(X|y)
ln (N-I X f(χi y))
Ep(y) ln Ep(x1) [f(x1, y)]
	
+
Ep(y) [KL(p(x1 | y) k q(x1 | y))]
IInfoLOOB (X1 ； Y)
	
Ep(y)
Ep(X)
ln
xi)[f(χι, y)] 1
PN=2 f (χi, y) J_ _
+
Ep(y) [KL(p(x1 | y) k q(x1 | y))]
IInfoLOOB (X1 ； Y) - DE + Ep(y)
[KL(p(x1 | y) k q(x1 | y))] ,
where we used
DE=Ep"(χjn( N⅛if2(XE)M	(A70)
27
Under review as a conference paper at ICLR 2022
and
Z (y) = Ep(XI)[f(Xι, y)] = Ep(X)
N
X f(xi, y)
i=2
(A71)
1
Ep(XIy) N-I Ef(g, y)
Since both KL and DE are non-negative (for DE see below), to increase InfoLOOB we have either
to decrease KL or to increase DE.
Bounding DE. Next we bound DE. We define
N
L = zTx - β-1	zi ln zi .
i=1
(A72)
The log-sum-exponential (lse) is the maximum of L on the N -dimensional simplex D with D = {z |
Pizi = 1,0 6 zi} (Gao & Pavel, 2017):
N
lse(β, x) = maxzTx - β-1	zi lnzi .
z∈D
i=1
(A73)
For some z ∈ D we have
N
Ea [lse(β, a)] ≥ Ea zTa - β-1 Xzilnzi
i=1
N
zTEa [a] - β-1	zilnzi ,
i=1
(A74)
therefore
N
Ea [lse(β, a)] ≥ maxzTEa [a] - β-1	zilnzi = lse(β, Ea [a]) .
z∈D
i=1
(A75)
We obtain
Ep(y)
Ep(X)
ln
Ep(x1 )
exp(τ -1 sim(x1, y))
PiN=2 exp(τ -1 sim(xi, y))
Ep(y) ln Ep(x1) exp(τ -1 sim(x1, y))
Ep(y) ln Ep(x1) exp(τ -1 sim(x1, y))
N
X exp(τ -1
i=2
(A76)
Ep(xi) [sim(xi, y)])
))
- τ-1 Ep(x1) [sim(x1, y)] .
1
N - 1
)
6

We obtain via Jensen’s inequality
)
Ep(y)
Ep(X)
ln
Ep(x1 )
exp(τ -1 sim(x1, y))
PiN=2 exp(τ -1 sim(xi, y))
(A77)
≥ Ep(y)
ln Ep(x1) exp(τ -1 sim(x1, y))

N
Ep(xi) exp(τ -1 sim(x1, y))
i=2
)
0.
If we combine both previous inequalities, we obtain
0 6 DE 6	Ep(y)	lnEp(x1)	exp(τ -1	sim(x1, y))	-	τ-1	Ep(x1)	[sim(x1,y)]	.	(A78)
28
Under review as a conference paper at ICLR 2022
In particular, for bounded sim(x1, y), we get
0 6 DE 6 τ-1	max sim(x1, y) - min sim(x1, y)	,	(A79)
y,x1	y,x1
while Hoeffding’s lemma gives
0 6 DE 6 T τ-2 (maxsim(xι, y) — min sim(xι, y) ) .	(A80)
8	y,x1	y,x1
Thus, for bounded sim(x1, y), DE is bounded, therefore also InfoLOOB. For sub-exponential
distributions with variance σ2, for which Bernstein’s condition withτ > b holds (Eq. (2.16) in
Wainwright (2019)), we get (Proposition 2.3 in Wainwright (2019)):
2
0 6 DE 6	—— .	(A81)
2 (τ 2 — bτ )
Next, we show that DE is small. Hoeffding’s inequality states that if f(x, y) ∈ [a, b] then
PqEp(X1)[f(Xι, y)] — N—y X f(xi, y)
≥
2(N—1)2
e) 6 P (-	(b — a)2
(A82)
For
Ep(x1) [f(x1, y)] —
1
N — 1
N
f(xi, y) 6
i=2
(A83)
we have
l ( Ep(Xi) [f(χι, y)] ! 6 l N N-T PN=2 f(xi,y) + e
N-T PN=2 f(xi, y)	N-T P=2 f(xi, y)
6 N-T PN=2 f(xi, y) 6 Z - e，
where we used ln a 6 a — 1 for 0 < a. Analog for
(A84)
we have
1N
N-y E f(xi, y) — Ep(XI) f(χι, y)] 6 e
i=2
(A85)
Ep(XI)[f (χι, y)]	Ep(XI)[f (χι, y)]
ln U PN=2f(Xi，y)) ≥	(Ep(XI) [f (X1, y)] + e)
= — ln Ep(X1) [f(χι, y)] + e ∖ ≥__________e	= — £
ɪɪl Ep(XI) [f(xι, y)]	) —	Ep(XI) [f(xι, y)]	Z
where we used — ln a ≥ 1 — a for 0 < a.
(A86)
In summary, for
we have
1N
Ep(XI) [f(χι，y)] — N-I E f (Xi y)	6 e
(A87)
e
— Z 6 ln
i) [f(χl, y)]	6 e
LN=2 f(xi, y)	Z - e
(A88)
29
Under review as a conference paper at ICLR 2022
It follows that
	
Zr 6 DE 6 z⅛
(A89)
DE averages the ln-term over y and X, therefore it has an even smaller bound than the bound above
on the ln-term. Consequently, for small b -a and large N, the term DE is small.
KL is decreased by making the variation distribution q(x1 | y) more similar to the posterior p(x1 | y).
The value DE only depends on the marginal distributions p(y) andp(x), since P(X) = QN=2 p(xi).
The value DE can be changed by adding an offset to f(x, y). However, scaling f(x, y) by a factor
does not change DE. Consequently, DE is difficult to change.
Therefore, increasing InfoLOOB is most effective by making q(x1 | y) more similar to the posterior
p(x1 | y).
Gradient of InfoLOOB expressed by gradients of KL and DE. Assume that the similarity is
parametrized by w giving sim(x, y; w).
KL(p(x1 | y) k q(x1 | y))
/…ln IW
(A90)
1
τ

	
p(x1 | y) sim(x1, y; w) dx1 + lnZ + C ,
where C is independent of w.
Next, we compute the derivative of KL with respect to parameters w.
∂KL
∂ W
(A91)
_T-1 ∕√xι	|	M)	dsim(X1,y;W) dxι	+ ɪ ∖(x) eχp(TT Sim(X1,y;W))	dsim(X1,y;W)
-	T J P(XI	|	y)	∂W dx1	+ Z J P(XI) —∂sim(xι, y; W)	∂W dx1
-	T-1	ZP(X1 |	y)	dsim(X1，y; W)	dXi	+	T-1	Zp(xi) exp(TT Sim(X1，y;	W))为叫1, y; W) dXι
∂W	Z	∂W
-	T-1	Zp(X1 |	y)	dSim(X1,y; W)	dX1	+	T-1	Z q3 | y)痢叱，y；W)	dX1
∂W	∂W
T-1	(q(X1 | y)
- P(X1 | y))
∂sim(X1, y; W) 4x
∂w	1
The derivative is the average difference between the posterior distribution P(X1 | y) and the variational
distribution q(X1 | y) multiplied by the derivative of the similarity function. If both distribution
match, then the derivative vanishes.
30
Under review as a conference paper at ICLR 2022
Next, we compute the derivative of DE with respect to parameters w.
∂DE
∂w
(A92)
	
	
	
	
	
	
Ep(y)
Ep(y)
Ep(y)
∂ ln Z
∂w
- Ep(y)
Ep(X)
N-1 PN=2 T-1 eχρ(τ-1 sim(xi, y; W))
N-I PN=2 f(Xj,y)
∂sim(xi ,y;W)
∂w
))
τ-1	q(X1 | y)
∂sim(x1, y; w)
∂w
dX1
Ep(X)
τ- Ep(y)
τ- Ep(y)
τ- Ep(y)
τ-1 Ep(y)
τ-1 Ep(y)
τ-1 Ep(y)
τ- Ep(y)
τ- Ep(y)
τ-1 Ep(y)
τ- Ep(y)
τ- Ep(y)
τ- Ep(y)
E
N—1 pN=2 T-1 exp(τ-1 sim(xi, y； W))
N-I PN=2 f (Xj, y)
∂sim(xi,y;W)
∂w
))
q(X1 | y)
∂ sim(x1, y; w)
∂w
dX1

1
Jp(X)	N - ι
7
N
i=2 NLI PN=2 f(x, y)
f(xi, y)
∂sim(xi, y; w)
∂w
■))
p(x1) f(x1,y) ∂sim(x1, y; w)
Ep(x) [f (x, y)]
Ep(X)
Ep(x1)
Ep(X)
1
N - 1
Ep(X)
Ep(X)
Ep(X)
Ep(X)
Ep(X)
∂w
dX1
1N
E X
f(X1, y)
f(xi, y)
∂sim(xi, y; w)
NLI PN=2 f (xj, y)
∂ sim(X1, y; w)
Ep(x) [f(X, y)]
1N
二X
i=2
N
Ep(xi)
i=2
1
N - 1
1
N - 1
1
N - 1
1
N - 1
1
N - 1
N
X
i=2
N
X
i=2
N
X
i=2
N
X
i=2
N
X
i=2
∂w
))
∂w
f(xi, y)
∂sim(xi, y; w)
N-i PN=2 f(xj, y)
∂w
))
f(Xi, y)	∂sim(Xi, y; w)
Ep(x) [f (x, y)]
f(xi, y)
∂w
∂sim(xi, y; w)
N-1 PN=2 f(xj, y)
∂w
))
f(Xi, y)	∂sim(Xi, y; w)
Ep(x) [f(X, y)]
f(xi, y)
∂w
■))
∂sim(xi, y; w)
N-1 PjN=2 f (Xj, y)
∂w
))
1
1
Ep(x) [f (x, y)]
1
Z
	
	
1
The derivative is the average of 1 -
N-1 PN=2 f (Xj，y)
NLI PN=2 f(xj, y)
f(Xi, y)
1
N-1 PN=2f (xj,y)
f(xi, y)
∂sim(xi, y; w)
∂sim(Xi, y; w)
∂w
multiplied by the score function and the
derivative of the similarity function. The average is over y and X , therefore the whole derivative
becomes even smaller. Consequently, for small b - a and large N, the derivative of DE is small.
Note that for
Ep(XI) [f (X1, y)] - N - j
N
X f(Xi, y)	6
i=2

(A93)
∂w
.
■))
31
Under review as a conference paper at ICLR 2022
we have
1	1	1	1	_ E
Z	NLI PN=2 f (Xj, y)、Z Z + e	Z(Z + e)，
1	1	1	1	_ E
Z	NLI Pj=2 f(Xj, y) ≥Z Z -E	Z(Z -E),
therefore
1	1	E
Z	NLI PN=2 f(xj, y) ' Z(Z -E)
(A94)
(A95)
(A96)
If the expectation Z is well approximated by the average N—i PjN=2 f (xj, y), then both DE and its
gradient are small.
Derivative of InfoLOOB via KL and DE:
∂IlnfoLOOB(Xl ; Y) = ∂DE - ∂KL
∂w	∂w	∂w
(A97)
In this gradient, the KL term is dominating, therefore f(X, y) is pushed to approximate the conditional
probability p(y | X). Modern Hopfield networks lead to larger values of p(y | X) as the mutual
information becomes larger, therefore modern Hopfield networks help to push f(X, y) to large values.
Furthermore, modern Hopfield networks increase Z, which is in the denominator of the bound on
DE and its derivative.
A.1.4 InfoNCE and InfoLOOB: Gradients
We consider the InfoNCE and the InfoLOOB loss function. For computing the loss
function, we sample N pairs independently from p(X, y), which gives the training set
{(X1, y1), (X2, y2), . . . , (XN, yN)}. InfoNCE and InfoLOOB only differ in using the positive
example in the negatives. More precisely, InfoNCE uses for the matrix of negative samples
X = (X1, . . . , XN), while InfoLOOB uses X = (X2 , . . . , XN).
InfoNCE.
The InfoNCE loss is
LInfoNCE =- N Xln (Iftb)! = N x=x LiCE 加,
where we used
LInfoNCE(yi) = - ln 1 ι JNxi y'----------).
∖N Ej=If(X j，yi) J
For the score function f(X, y), we use
f(X, y) = exp(τL1 sim(X, y)) ,
sim(X, y) = yTX
with τ as the temperature.
The loss function for this score function is
LInfoNCE (y) = - τL1 yTX1 + τL1 lse τL1,XTy ,
where lse is the log-sum-exp function (lse):
lse(β, a) = βL1 log X exp(βai )
(A98)
(A99)
(A100)
(A101)
(A102)
(A103)
32
Under review as a conference paper at ICLR 2022
for β > 0 and vector a = (a1, . . . , aN).
The gradient with respect to y is
dLInfoNCE(y)
∂y
—T-1 xι + T-1 X Softmax (T-1XTy),
(A104)
which is the positive example x1 that fits to the anchor example y minus the Hopfield network update
with state pattern y and stored patterns X and then this difference multiplied by T-1.
This gradient can be simplified, since the positive example x1 is also in the negative examples. Using
p = (p1, . . . ,pN)T = softmax T-1XTy , we obtain
∂LlnfoNCE(y)
∂y
= — T-1 (1 — p1)
= — T-1 (1 — p1)
	
1
1 一 pi
X (Softmax (TTXTy)
一 (p1 , 0, . . . ,
where
(A105)
一 XSoftmaX (τ-1XXTy)) = (1 一 pi) dLInfoLOOBw).
------X (SoftmaX (T-IXTy) — (pi, 0,..., 0)t)
=X ((P 1, p2 ,...,pN) 一 (p1,0, . .. , O))
1 一 pi
(A106)
1	T 1N
I---X (0,p2, ...,pN ) =  -EJp Xi
1 一 pi	1 一 pi i=2
is the softmax average over the negatives xi for 2 6 i 6 N without xi. It can be easily seen that
1-宗 PN=2 Pi = 1-p1 = 1. For the derivative of the InfoLOOB see below.
The gradient with respect to xi is
dLInfoNCE(y) = _ -1	+ -1	exp(TT XTy)
dx1	P	PIN=I eχp(TTXTy) y
= 一 T-1 (1 一 p1) y .
Consequently, the learning rate is scaled by (1 一 p1).
The sum of gradients with respect to X1 and Xi is
dLInfoNCE(y) + X aLInfoNCE®) = - T-1 y + T-1 y ITSoftmaX (TTXTy)
∂X1	i=1	∂Xi
= 一 T-1 y + T-1 y = 0 ,
(A107)
(A108)
(A109)
where 1 is the vector with ones. However, the derivatives with respect to the weights are not zero
since the Xi are differently computed.
InfoLOOB.
The InfoLOOB loss is
LInfoLOOB
N
X ln
i=1
N
LInfoLOOB (yi) ,
i=1
(A110)
where we used
LInfoLOOB(yi)
一 ln
f(χi, yi)______
PN=1,j=if(xj, yi)
(A111)
	
1
N
1
N
33
Under review as a conference paper at ICLR 2022
For the score function f (x, y), we use
f(x, y) = exp(τ -1 sim(x, y)) ,
sim(x, y) = yTx
(A112)
(A113)
with τ as the temperature.
The loss function for this score function is
LinfoLOOB (y) = - T—1 yTX] + τ—1 lse (τ-1, XTy
where lse is the log-sum-exponential function.
The gradient with respect to y is
(A114)
∂LinfoLOOB(y)
∂y
—T-1 X1 + T
-1 X Softmax (T-1XTy),
(A115)
which is the positive example x1 that fits to the anchor example y minus the Hopfield network update
with state pattern y and stored patterns X and then this difference multiplied by T-1.
The gradient with respect to x1 is
∂LinfoLOOB(y)
∂x1
- T-1 y .
(A116)
The sum of gradients with respect to x1 and xi is
dLInfoLO。BQ) + X dLinfoLO。Bw) = — τ-1 y + T-1 y 1TSoftmaX (τ-1XTy)
∂x1	∂xi
i
(A117)
= — T-1 y + T-1 y = 0 ,
where 1 is the vector with ones. However, the derivatives with respect to the weights are not zero
since the xi are differently computed.
Gradients with respect to T-1.
The gradient of the InfoNCE loss Eq. (A98) using the similarity Eq. (A100) with respect to T-1 is
dLInfoNCE(y)
∂t-1
—	yT x1 + yT X Softmax T-1XTy
—	yT (x1 — X softmax (T-1XTy)),
(A118)
(A119)
which is the similarity of the anchor y with the difference of the positive example x1 and the Hopfield
network update with state pattern y and stored patterns X . The gradient of the InfoLOOB loss
Eq. (A110) using the similarity Eq. (A112) with respect to T-1 is
∂ LInfoLOOB (y)
∂t-1
—	yT x1 + yT X softmax 卜-1XTy)	(A120)
—	yT (x1 — X softmax (t-1XTy)) .	(A121)
with the difference that the Hopfield network update is done with stored patterns X instead of X .
Without the positive example x1 in the stored patterns X, the term x1 — X softmax T-1XTy
in Eq. (A120) will not decrease like the term x1 — X softmax T-1XTy in Eq. (A118) but grow
even larger with better separation of the positive and negative examples.
A.1.5 InfoLOOB and InfoNCE: Probability Estimators
In McAllester & Stratos (2018; 2020) it was shown that estimators of the mutual information by lower
bounds have problems as they come with serious statistical limitations. Statistically more justified for
34
Under review as a conference paper at ICLR 2022
representing the mutual information is a difference of entropies, which are estimated by minimizing
the cross-entropy loss. Both InfoNCE and InfoLOOB losses can be viewed as cross-entropy losses.
We sample N pairs independently from p(x, y), which gives Z =
{(x1,y1), (x2,y2), . . . , (xN,yN)}. We set X = {x1,x2, . . . ,xN} and Y = {y1,y2, . . . ,yN},
so that, Z = X × Y . The score function f (x, y) is an estimator for p(x, y). Then we obtain
estimators q for the conditional probabilities. q(yi | xi, Y \ {yi}) is an estimator for p(yi | Xi) and
q(xi | yi, X \ {xi}) an estimator for P(Xi | yi). Each estimator q uses beyond (xi, yi) additional
samples to estimate the normalizing constant. For InfoNCE these estimators are
q1(yi I χi,Y∖ {yi})
q2(χi I yi,X \ {χi})
f(χi, yi)
NN Pj=I f (Xi, yj)
f(χi, yi)
j Pj=If(Xj, yi)
f(χi, yi)
Ep(y)[f(χi, y)],
f(χi, yi)
Ep(x)[f(χ, yi)].
(A122)
(A123)


The cross-entropy losses for the InfoNCE estimators are
LInfoNCE =	1 =—— N	j X ln i=1	f(Xi， yi)	
				Pj=If (Xi yj)
LI2nfoNCE =	1	j X ln i=1	(	f(Xi，yi)
	=—— N			Pj=If (X j，yi)
For InfoLOOB these estimators are
q1(yi I Xi, Y \ {yi})
f(Xi, yi)
j-ɪ Pj=Ij= f (Xi，yj)
fM, yi)
Ep(y) [f (Xi，y)]
q2(Xi | yi,χ \{XiD = ——Vf(Xi，yi)-； ≈
j-ɪ Tj = Ij=i f (Xj, yi)
f(Xi, yi)
Ep(X) [f (X, yi)]
The cross-entropy losses for the InfoLOOB estimators are
-L XXln (________f(Xi，yi)______
N i=1	∖ j-ɪ Pj=Ij=if(Xi yj)
-1 XX ln f_______f (Xi，y_______
N i=1	∖ j-ɪ Pj=1j=i f (Xj，yi)
(A124)
(A125)
(A126)
(A127)
(A128)
(A129)

The InfoLOOB estimator uses for normalization			j f(Xj， yi) ， j=1,j=i j f(Xi，yj) ， j=1,j=i	(A130) (A131)
Ep(x) [f(X， yi)] ≈ Ep(y) [f(Xi， y)] ≈	1 N - 1 1 N - 1			
in contrast to InfoNCE, which uses			j f(Xj，yi) ， j=1	
Ep(x) [f (X， yi)]	≈	1 N		(A132)
Ep(y) [f(Xi， y)]	≈	1 N	j f(Xi， yj) . j=1	(A133)
If InfoNCE estimates the normalizing constant separately, then it would be biased. (Xi， yi) is drawn
according to p(Xi， yi) instead ofp(Xi)p(yi). In contrast, if InfoLOOB estimated the normalizing
constant separately, then it would be unbiased.
35
Under review as a conference paper at ICLR 2022
A.1.6 InfoLOOB and InfoNCE: Losses
We have N pairs drawn iid from p(x, y), where we assume that a pair (xi, yi) is already
an embedding of the original drawn pair. These build up the embedding training set Z =
{(x1, y1), (x2, y2), . . . , (xN, yN)} that allows to construct the matrices X = (x1 , x2, . . . , xN)
of N embedding samples xi and Y = (y1, y2, . . . , yN) of N embedding samples yi. We also have
M stored patterns U = (u1, . . . , uM) and K stored patterns V = (v1, . . . , vK).
The state vectors xi and yi are the queries for the Hopfield networks, which retrieve some vectors
from U or V . We normalize vectors kxi k = kyi k = kui k = kvi k = 1. The following vectors are
retrieved from modern Hopfield networks (Ramsauer et al., 2021):
Uxi = U softmax(β UTxi) , Uyi = U softmax(β UTyi) ,	(A134)
Vxi = V softmax(β VTxi) ,	Vyi = V softmax(β V Tyi)	(A135)
where Uxi denotes an image-retrieved image embedding, Uyi a text-retrieved image embedding,
Vxi an image-retrieved text embedding and Vyi a text-retrieved text embedding. The hyperparameter
β corresponds to the inverse temperature: β = 0 retrieves the average of the stored pattern, while
large β retrieve the stored pattern that is most similar to the state pattern (query).
We consider the loss functions
LInfoNCE
ɪ XX log	exp(TT Xy)
N i=ι	PN=I exp(τ-1 xTyj)
ɪ XX log	exp(TT XTy)
N i=ι	Pj=I eχp(τ-1XTy)'
(A136)

LInfoLOOB
ɪ XX log	exp(TT XTyJ
N i=ι	PN=ieχp(TT XTyj)
ɪ XX log	exp(TT XTyJ
N ⅛1	PN=iexP(TT XTyi),
(A137)
LH-UVUV _	1 X l	exp(T 1 UT Vyi)
LInfoLOOB = -N i=ι log P=IXPFIUTVyJ
ɪ XX lo°	exp(T-1 UT%)
N S	Pj=iexp(T-1 UTXi) ,
(A138)

H-UUVV
LInfoLOOB
ɪ X log	exp(T-1 UxUyi)
N ill	PM exp(T-1 UTUyj)
ɪ XX log	exP(TT VTqi)
N S	Pj=i exp(T-1 Vxj /J ,
(A139)
where for InfoLOOB the sum j=i in the denominator contains only negative examples j . We do
not consider the loss function LIHn-foULVOUOVB because of the high variance in the dot product UxTi Vyi as
elaborated in the following.
Let us consider the dot product between the anchor retrieval with the positive pattern retrieval for the
loss functions with Hopfield. In the first term of the loss function Eq. (A138), Uxi is the anchor with
Vyi as the positive sample and Vyi with Uxi as the positive sample for the second term, since the
anchor also appears in each term of the denominator. Equivalently the same is valid for Eq. (A139),
but with positive samples Vxi and Uyi respectively.	These dot products can be	written as
UxT Vyi =	softmax(β UTXi)T	UTV softmax(β VTyi)	,	(A140)
UxT Uyi =	softmax(β UTXi)T	UTU softmax(β UTyi)	,	(A141)
VxT Vyi =	softmax(β VTXi)T	VTV softmax(β VTyi)	.	(A142)
High variance of UxT Vyi. To compute the dot product UxT Vyi, M + K stored patterns are required
(M of the uj and K of the vj). In contrast, the dot products UxTi Uyi and VxTi Vyi require only M
or respectively K stored patterns. Therefore, UxT Vyi has higher variance than both UxT Uyi and
VxTiVyi.
Covariance structure extracted by UxT Uyi and VxT Vyi .
36
Under review as a conference paper at ICLR 2022
The Jacobian J of the softmax p = softmax(βa) is
J(βa) = dsθftmax(ea) = β (diag(p) - Ppr) ,	(A143)
which is a symmetric, positive semi-definite matrix with one eigenvalue of zero for eigenvector 1.
J(βa) is diagonally dominant since |pi(1 -pi)| - Pj6=i |pipj| = pi - Pj pipj = pi - pi = 0.
Next we give upper bounds on the norm of J.
Lemma A1. For a softmax p = softmax(βx) with m = maxi pi(1 - pi), the spectral norm of the
Jacobian J of the softmax is bounded:
	kJk2	6	2mβ,	(A144)
	kJk1	6	2mβ,	(A145)
	kJk∞	6	2mβ.	(A146)
In particular everywhere holds	kJk2	6	2β	(A147)
If pmax = maxi pi ≥ 1 - ≥ 0.5, then for the spectral norm of the Jacobian holds
kJk2 6 2 β - 2 2 β < 2 β .	(A148)
Proof. We consider the maximum absolute column sum norm
kAk1 = maxX |aij|	(A149)
i
and the maximum absolute row sum norm
kAk∞ = miax X |aij| .	(A150)
j
We have for A = J = β diag(p) - ppT
Elajl	= β	(Pi(I- Pi)	+ Epipj) = β Pi	(I -	2Pi	+ Epj)	(A151)
j	j,j	6=i	j
= 2 β Pi (1 - Pi ) 6 2 m β ,
X laij l = β (Pj	(1 -Pj ) + X Pj Pi ) = βPj	(1 - 2Pj	+ XPi)	(A152)
i	i,i6=j	i
= 2 β Pj (1 - Pj ) 6 2 m β .
Therefore, we have
kJk1 6 2mβ,	(A153)
kJk∞ 6 2 mβ,	(A154)
kJk2 6 qjkιMk∞ 6 2 mβ.	(A155)
The last inequality is a direct consequence of Holder,s inequality.
For 0 6 Pi 6 1, we havePi(1 - Pi) 6 0.25. Therefore, m 6 0.25 for all values ofPi.
If Pmax ≥ 1 - ≥ 0.5 ( 6 0.5), then 1 - Pmax 6 and for Pi 6= Pmax Pi 6 . The derivative
∂x(1 一 x)∕∂x = 1 一 2x > 0 for x < 0.5, therefore x(1 一 x) increases with X for X < 0.5. Using
x = 1 - Pmax and for Pi 6= Pmax x = Pi, we obtain Pi(1 - Pi) 6 (1 - ) for all i. Consequently,
we have m 6 e(1 一 e).	□
37
Under review as a conference paper at ICLR 2022
For the Softmax P = Softmax(βa) with Jacobian ∂J∕∂a = J(βa) = β (diag(p) — PpT) and for
arbitrary N -dimensional vectors b and c, we have
bτ J(βa) C = β bτ (diag(p) — PpT) C = β
X pi bi ci —
.
(A156)
Therefore, bTJ(βa)C is β times the covariance between b and C if component i is drawn with
probability pi of the multinomial distribution P. In our case the component i is sample i.
Using the mean u^ = 1/M PM=I Ui, the empirical covariance of data U is
Cov(U) = 1/M UU T — U Uu T ,
M
[Cov(U)]kl = X1/M uik uil —
i=1
The weighted covariance (samples Ui are drawn according to pi)
Cov(U) = U J(β a) UT ,
M
[Cov(U )]kl = β	pi uik uil
(A157)
(A158)
(A159)
(A160)
which replaces 1/M from equal sampling by the pi , that is, Ui is sampled with probability pi .
The next theorem states how to express the dot product UxTi Uyi by weighted covariances of the data
U.
Theorem A3 (Weighted Covariances). Using the weighted covariances
Cov(U,yi) = U Jm(β UTyi) UT , Cov(U,xi) = U Jm(β UTxi) UT ,	(A161)
Jm(β a)
Z 1J(λβa) dλ,
0
(A162)
where the mean Jacobian Jm is symmetric, diagonally dominant, and positive semi-definite with
spectral norm bounded by kJmk2 6 0.5β.
The dot product UxT Uyi can be expressed by the weighted covariances
UxiUyi = (U + Cov(U, Xi) Xi)T (U + Cov(U, yi) yi) ,	(A163)
where the mean is U = 1 /MU1.
Proof. We apply the mean value theorem to the softmax with the symmetric, diagonally dominant,
positive semi-definite Jacobian matrix Jm = R01 J(λa + (1 — λ)a0) dλ:
softmax(a) — softmax(a0) = Jm (a — a0) .
We set a0 = 0 and use βa instead of a, which gives:
softmax(β a) = 1/M 1 + Jm(β a) a ,
Jm(β a)
Z 1J(λβa)dλ,
0
which is exact. We obtain
softmax(β UTXi) = 1/M 1 + Jm(β UTXi) UTXi ,
softmax(β UTyi) = 1/M 1 + Jm(β UTyi) UTyi .
(A164)
(A165)
(A166)
(A167)
The spectral norm of Jm is bounded by kJmk2 6 0.5β, since this bound holds for every J(λβa) in
Jm(β a) = R01 J(λβa) dλ according to Lemma A1.
38
Under review as a conference paper at ICLR 2022
The dot product between the anchor retrieval and the positive sample is:
UxT Uyi = softmax(β UTxi)T UTU softmax(β UTyi)	(A168)
=(1/M 1 + Jm(β UTxi) UTxi)T UTU(1/M 1 + Jm(β UTy) UTy，
=(1/M U 1 + U Jm(β UTxi) UTxi)T(1/M U1 + U Jm(β UTm)UTm)
=(U + Cov(U, xi) Xi)T (U + Cov(U, yi) yi),
where We used the mean U= 1/M U1 and the weighted covariances
Cov(U,yi) = U Jm(β UTyi) UT ,	Cov(U,xi) = U Jm(β UTxi) UT .	(A169)
□
The Jacobian Jm is symmetric, diagonally dominant, and positive semi-definite. The weighted
covariance Cov(U, .) is the covariance if the stored pattern Ui is drawn according to an averaged
pi given by Jm(.). Analog for weighted covariance Cov(V , .). When maximizing the dot product
UxTi Uyi , the normalized vectors xi and yi are encouraged to agree on drawing the patterns Ui with
the same probability pi to generate similar weighted covariance matrices Cov(U, .). If subsets of U
have a strong covariance structure, then it can be exploited to produce large weighted covariances
and, in turn, large dot products of UxTi Uyi . Furthermore, for a large dot product UxTi Uyi, xi and
yi have to be similar to one another to extract the same direction from the covariance matrices. All
considerations are analog for VxT Vyi .
A.2 Mutual Information Estimation
We follow the toy experiment discussed in Poole et al. (2019), Belghazi et al. (2018) and Cheng
et al. (2020) and experimentally confirm the superior quality of InfoLOOB for mutual information
than InfoNCE. The dataset consists of samples (xi, yi) drawn jointly from a multivariate Gaussian
distribution with correlation ρ where the dimension of the samples x and y is set to d = 20. We
examine the performance of InfoLoob with and without Hopfield and InfoNCE at estimating mutual
information of these samples. Due to the Gaussian distribution, the true value of mutual information
can be calculated as I(x, y) = -d log(1 - ρ2). We set the mutual information true value to the
values (2.0, 4.0, 6.0, 8.0, 10.0, 14.0) by varying the value of ρ. At each MI true value, we sample
data batches 1024 times, with batch size equal to 64, for the training of variational MI estimators.
Figure 2 shows that modern Hopfield networks reduce the variance of the model. For models trained
on data with mutual information of 10 we observe an average variance of approx. 0.67 for a model
without Hopfield and an average variance of approx. 0.33 for a model with Hopfield. For models
trained on data with mutual information of 14 we observe an average variance of approx. 1.00 for a
model without Hopfield and an average variance of approx. 0.48 for a model with Hopfield.
In Figure A1 we show the performance of our method InfoLOOB with and without Hopfield at
estimating mutual information as well as InfoNCE. As expected estimates of InfoNCE have estimates
that saturate at log(batch size). InfoLOOB without Hopfield exhibits good estimates of high mutual
information while InfoLOOB with Hopfield accomplishes both - good estimates of high mutual
information with a decreased variance.
A.3 Experiments
A.3.1 Ablation studies
As mentioned in the main paper, CLOOB has two new main components compared to CLIP: (1) the
InfoLOOB objective instead of the InfoNCE objective and (2) the modern Hopfield networks. To
assess which of the new main components of CLOOB have led to the performance increase over CLIP,
we performed ablation studies on the CC dataset. The results are reported in Table A1. First, we
enhanced CLIP by replacing the InfoNCE objective with InfoLOOB (see column CLIP InfoLOOB).
Next, we added modern Hopfield networks to the CLIP architecture and used retrieved embeddings
instead of the original embeddings, while keeping the InfoNCE objective (see column Hopfield
InfoNCE). Finally, we add modern Hopfield networks to CLIP and replace the InfoNCE objective
39
Under review as a conference paper at ICLR 2022
infoNCE
8 6 4 2 0
noitamrofni lautuM
0	1000 2000 3000 4000 5000 6000 7000
steps
noitamrofni lautuM
infoLOOB without Hopfield
1412108 6
4 2 0 2
-
0	1000 2000 3000 4000 5000 6000 7000
steps
infoLOOB with Hopfield
8 6 4 2 0 2
-
noitamrofni lautuM
0	1000 2000 3000 4000 5000 6000 7000
steps
Figure A1: The estimated mutual information of the InfoNCE objective saturates at the batch size
induced bound. The InfoLOOB objective trained with the same batch size with samples from the
same correlated Gaussian distributions following (Belghazi et al., 2018; Poole et al., 2019; Cheng
et al., 2020) is not limited by that bound and better estimates higher mutual information but suffers
from higher variance. This is remedied by incorporating the modern Hopfield network.
Table A1: Influence of loss functions and Hopfield retrieval. InfoLOOB increases the performance of
CLIP in most of the tasks. The InfoNCE loss is not suited for the Hopfield approach as it saturates
leading to a worse performance. Hopfield with InfoLOOB strongly improves the performance in 7
out of 8 datasets compared to both CLIP models.
Dataset	CLIP InfoNCE InfoLOOB		Hopfield InfoNCE InfoLOOB	
Birdsnap	1.94	2.37	1.67	2.53
Country211	0.62	0.63	0.54	0.76
Flowers102	13.04	13.03	11.53	14.24
GTSRB	7.28	4.39	5.76	5.86
UCF101	21.00	19.14	20.56	22.29
Stanford Cars	0.90	1.33	1.24	1.37
ImageNet	20.31	22.13	19.04	24.21
ImageNetV2	20.63	21.65	18.97	23.80
with InfoLOOB (see column Hopfield InfoLOOB). As shown in Table A1 the InfoLOOB objective
increases the performance of CLIP in the majority of the datasets. We attribute this increase to the fact
that InfoLOOB suffers less than InfoNCE from the “explaining away” problem. However, InfoLOOB
is even more effective for higher mutual information, that is, a richer covariance structure. Hopfield
networks amplify the covariance structure in their retrieved embeddings. Though, this amplified
covariance structure is disadvantageous for InfoNCE, as the saturation effect is stronger. The stronger
saturation effect is caused by a richer covariance structure through Hopfield networks, which in turn
leads to higher similarity between anchor and positive. Therefore, we see a performance drop when
combining modern Hopfield networks with InfoNCE. Concluding, modern Hopfield networks are a
perfect match for InfoLOOB as they yield higher mutual information. Therefore, CLOOB strongly
improves the performance on 7 out of 8 zero-shot transfer learning tasks compared to CLIP.
For CLIP with InfoNCE, the hyperparameter τ-1 is a learnable parameter. For the other experiments,
we use a fixed τ-1 of 30. The value for τ-1 was determined via hyperparameter search (see
Section A.3.2).
In contrast to CLIP, we use a learning rate scheduler with restarts (Loshchilov & Hutter, 2017) to be
more flexible regarding the number of total training epochs and enable training up to a plateau. To
investigate the influence of the learning rate scheduler, we performed experiments with and without
restarts. Table A2 shows the zero-shot performance for the different downstream tasks for CLIP and
CLOOB respectively. For both CLIP and CLOOB, the performance at the majority of the tasks either
increases or remains roughly the same with restarts.
40
Under review as a conference paper at ICLR 2022
Table A2: Influence of learning rate scheduler. For most of the tasks the performance either increases
or remains roughly the same with restarts for both CLIP and CLOOB.
Dataset	CLIP		CLOOB	
	w/o restarts	w/ restarts	w/o restarts	w/ restarts
Birdsnap	2.10	1.94	2.64	2.53
Country211	0.71	0.62	0.63	0.76
Flowers102	11.00	13.04	11.50	14.24
GTSRB	6.16	7.28	5.05	5.86
UCF101	19.05	21.00	21.97	22.29
Stanford Cars	1.29	0.90	1.22	1.37
ImageNet	20.19	20.31	23.29	24.21
ImageNet V2	20.53	20.63	22.97	23.80
Table A3: Datasets used for zero-shot and linear probing. In the case of several train or test sets per
dataset we report the total number of samples. It should be noted that at the time of this work some
Birdsnap images were not accessible anymore.
Dataset	Classes	Train size	Test size	Evaluation metric
Birdsnap	500	38,411	1,855	accuracy
Country211	211	42,200	21,100	accuracy
Flowers102	102	2,040	6,149	class-weighted accuracy
GTSRB	43	26,640	12,630	accuracy
ImageNet	1,000	1,281,167	50,000	accuracy
ImageNet V2	1,000	1,281,167	30,000	accuracy
Stanford Cars	196	8,144	8,041	accuracy
UCF101	101	28,747	11,213	accuracy
A.3.2 Hyperparameters
The hyperparameter search was done on a validation split of CC with about 15,000 samples. For the
hyperparameter τ-1 several values were considered (14.3, 30, 50, 70), where 30 leads to the best
results for both YFCC and CC. Analogously to CLIP, we use the Adam optimizer (Kingma et al.,
2014) with decoupled weight decay regularization (Loshchilov & Hutter, 2019). The weight decay is
only applied to weights that are not gains or biases. As proposed in OpenCLIP (Ilharco et al., 2021)
weight decay was set to 0.1. Different choices of weight decay (0.2 or 0.05), did not lead to a relevant
performance change. We use the same learning rate of 1 × 10-3 for CC and 5 × 10-4 for YFCC as
used in OpenCLIP. For the hyperparameter β we considered values in the range of 5 to 20. A value
of 8 resulted in the best performance for CC and 14.3 for YFCC. The batch size for CC was reduced
to 512 due to computational restraints which did not result in performance losses. The batch size for
YFCC was kept at 1024 as reported by OpenCLIP since a reduction resulted in a significant drop in
performance. The learning rate scheduler for all experiments is cosine annealing with warmup and
hard restarts (Loshchilov & Hutter, 2017) with a cycle length of 7 epochs. For models trained on
YFCC the warmup was set to 10000 steps and for models trained on CC to 20000 steps.
A.3.3 Datasets
For pretraining we consider two datasets, Conceptual Captions (CC) (Sharma et al., 2018) and
YFCC100M (Thomee et al., 2016). The CC dataset consists of 2.9 million images and corresponding
high-quality captions. Images and their corresponding notations for CC have been gathered via an
automated process from the web and therefore represent a wide variety of styles. Raw descriptions
of images are collected from the alt-text HTML attribute. Both images and texts are filtered such
that only image-text pairs above a certain quality threshold are part of this dataset. The dataset we
refer to as YFCC is a subset of the Yahoo Flickr Creative Commons 100 Million (YFCC100M)
dataset. It was created by filtering for images which contain natural language descriptions and/or
titles in English resulting in 15 million image-caption pairs. The textual descriptions contain less
41
Under review as a conference paper at ICLR 2022
useful information than CC because they are not filtered by quality. Occasionally they also contain
metadata like camera settings or web addresses.
We evaluate and compare our method on several downstream classification tasks. We evaluate
on the same set of datasets as CLIP reported for a model trained on YFCC. This set contains
Birdsnap (Berg et al., 2014), Country211 (Radford et al., 2021), Flowers102 (Nilsback & Zisserman,
2008), GTSRB (Stallkamp et al., 2011), UCF101 (Soomro et al., 2012), Stanford Cars (Krause et al.,
2013) and ImageNet (Deng et al., 2009). Additionally, we include ImageNet V2 in our analysis
(Recht et al., 2019). Table A3 shows an overview of training and test set sizes, number of classes and
the applied evaluation metric. In the case of several test sets per dataset the metric is calculated for
every set individually and the average performance is reported. The set size in Table A3 corresponds
to the total number of samples across all test and training sets of a dataset respectively.
Birdsnap contains images of North American bird species, however our dataset is smaller than
reported in CLIP as some samples are no longer available. The Country211 dataset was published
in CLIP and is a small subset of the YFCC100m dataset. It consists of photos that can be assigned
to 211 countries via GPS coordinates. For each country 200 photos are sampled for the training set
and 100 for testing. For the Flowers102 images of 102 flower categories commonly occuring in the
United Kingdom were collected. Several classes are very similar and there is a large variation in scale,
pose and lighting. The German Traffic Sign Recognition Benchmark (GTSRB) was a challenge held
at the IJCNN 2011. The dataset contains images of german traffic signs from more than 40 classes.
Note that two versions of this dataset exist, one used for the challenge and an official dataset released
after the competition. For CLIP the linear probing classifiers were trained using the competition
training set but tested on the official test set. Stanford Cars contains images of 196 car models at
the level of make, model and year (e.g. Tesla Model S Sedan 2012). UCF101 (Soomro et al., 2012)
is a video dataset with short clips for action recognition consisting of three training sets and three
test sets. We follow the procedure reported in CLIP and extract the middle frame of every video to
assemble the dataset. The ImageNet Large Scale Visual Recognition Challenge was held from 2012
through 2017 and is one of the most widely used benchmarks for object detection and localization.
Several years later ImageNet V2 assembled three new test sets with images from the same 1,000
classes to test for generalization of models optimized for the original ImageNet benchmark. Every
test set comprises 10,000 samples.
A.3.4 Zero-shot evaluation
Class names for all downstream tasks were adopted from CLIP, that is, among other changes special
characters like hyphens or apostrophes were removed. Furthermore, some class names of the datasets
were slightly changed (e.g. “kite” to “kite (bird of prey)” in ImageNet). For zero-shot
evaluation, we use the same prompt templates as published in CLIP. Depending on the dataset the
number of prompts can vary from one prompt (e.g. “a photo of a {label}, a type of
bird.” for Birdsnap) up to 80 prompts for ImageNet covering various settings (e.g. “a cropped
photo of a {label}.”, “a origami {label}.”). In case of several prompts an average
embedding over all prompt embeddings is calculated. Figure A2 shows the zero-shot results for all
evaluation tasks with the ResNet-50x4 model reported in Table 4.
A.3.5 Linear probing
We try to follow the evaluation procedure in Radford et al. (2021) as closely as possible. We note one
difference with respect to the implementation: Instead of scikit-learn’s logistic regression using the
L-BFGS solver, we use cuML’s logistic regression classifier with L-BFGS algorithm to utilize GPUs
for efficiency. All hyperparameters are the same as described in Radford et al. (2021), the maximum
number of iterations was set to 1000, and the L2 regularization strength λ was determined by using a
parametric binary search.
We tried to reproduce the CLIP results with the correspondingly published models, however, failed to
produce the exact numbers. This could be due to several factors:
•	The train and validation split. Same as in Radford et al. (2021) , we use the provided
validation set to perform the hyperparameter search. When there is none provided, we use a
random half of the training dataset for validation.
42
Under review as a conference paper at ICLR 2022
Table A4: Linear probing results for the reimplementation of CLIP and CLOOB using different
ResNet architectures trained on YFCC. The performance of CLOOB scales with increased encoder
size
Dataset	CLIP RN-50	CLOOB RN-50	CLOOB RN-101	CLOOB RN-50x4
Birdsnap	50.9	56.2	58.1	62.2
Country211	19.5	20.6	21.8	24.2
Flowers102	94.8	96.1	96.1	96.2
GTSRB	82.5	78.9	77.9	80.6
UCF101	75.2	72.3	72.8	75.3
Stanford Cars	36.2	37.7	39.0	44.3
ImageNet	66.9	65.7	67.0	69.7
ImageNet V2	60.2	58.7	60.3	62.2
•	In case of a tie in the validation score, we use the maximal λ for the strongest regularization.
We note though that we came closer to reproducing the results published in CLIP when
using the mean λ over all ties when these exist.
•	For the Birdsnap dataset, the resources that we have got online at the time of this writing
could be different from the resources that CLIP’s authors obtained at the time.
Linear probing evaluation of YFCC-pretrained models is shown in Table A4. Comparing our
reimplementation of CLIP and CLOOB with ResNet-50 encoders, we observe mixed results. The
reason for this effect might be attributed to the observed task-dependence of multimodal models
(Devillers et al., 2021). Another potential reason is that the benefit of the restrictions to more reliable
patterns that occur in both modalities does not directly translate to an evaluation of just the encoding
part of one modality. Again, as expected in self-supervised training, increasing the capacity of the
CLOOB models benefits accuracy.
A.4 Review of Modern Hopfield Networks
We briefly review continuous modern Hopfield networks that are used for deep learning architectures.
They are continuous and differentiable, therefore they a work with gradient descent in deep architec-
tures. They retrieve with one update only, therefore they can be activated like other deep learning
layers. They have exponential storage capacity, therefore they can tackle large problems. Hopfield
networks are energy-based, binary associative memories, which popularized artificial neural networks
in the 1980s (Hopfield, 1982; 1984). Associative memory networks have been designed to store and
retrieve samples. Their storage capacity can be considerably increased by polynomial terms in the
energy function (Chen et al., 1986; Psaltis & Cheol, 1986; Baldi & Venkatesh, 1987; Gardner, 1987;
Abbott & Arian, 1987; Horn & Usher, 1988; Caputo & Niemann, 2002; Krotov & Hopfield, 2016).
In contrast to these binary memory networks, we use continuous associative memory networks with
very high storage capacity. These modern Hopfield networks for deep learning architectures have an
energy function with continuous states and can retrieve samples with only one update (Ramsauer
et al., 2021; 2020). Modern Hopfield Networks have been successfully applied to immune repertoire
classification (Widrich et al., 2020) and chemical reaction prediction (Seidl et al., 2021).
We assume a set of patterns {u1 , . . . , uN} ⊂ Rd that are stacked as columns to the matrix U =
(u1, . . . , uN) and a state pattern (query) ξ ∈ Rd that represents the current state. The largest norm
of a stored pattern is M = maxi kui k. Continuous modern Hopfield networks with state ξ have the
energy
E = - β-1 log X exp(βuiT ξ)
+ β-1 log N + 1 ξτξ + 2 M2 .
(A170)
For energy E and state ξ, the update rule
ξnew = f(ξ; U,β) = Up = U softmax(βUTξ)
(A171)
43
Under review as a conference paper at ICLR 2022
cottontail rabbit
Correctrank: 1/1000
cottontail rabb，
Mre
Angara rabbit
ScettIshTerrIer
“rkshlre Terrier
OO O* O.* 0Λ 0Λ XO
longhorn beetle
correct rank: 1/1000
Ignghgrn beetle
cricket Insect
weevil
tiger beetle
cockroach
oo ox o* oβ oe xo
collie
correct rank: 2/1000
Car IllIrrOr
cαllle
Barder Cnllle
Great WrelIeeS <k>g
American s⅛ff⅛rdshlrel⅛rrler
。。 0Λ 0Λ 。4 M LC
mosque
correctrank: 1/1000
mosque
BiaCk and Tan Coonhcund
rhinoceros beetle
gossamer-winged butterfly
dragonfly
oo e* o.* 0Λ 0Λ χo
correct rank: 13/1000
combine harvester
hay
corn
thatched nx>f
⅛rm MoW
M » at αβ oe 工。
Osprey
correct rank: 1/500
Osprey
BaH Eagle
SeniIpaImated Plover
Swallow tailed Kite
Rough legged Hawk
oo e* o.* 0Λ 0Λ xo
correct rank: 4/43
red and wh⅛e triangle WKh
traffic light approaching warning
empty red and wh⅛e circle
stop
⅛d circle WKh white
horho∏9∣ stripe no entry
red and wh⅛e triangle
ro⅞d Intersection warning
Great Blue Heron
correct rank: 1/500
Great Blue Heron
TrlCQlored Heron
Ltttle Blue Heron
Heddlsh Egret
Northern HaMer
M » at αβ oe 工。
Brant
correct rank: 3/500
Northern Plnlall
Long tailed Duck
SralIt
Greater Scaup
Gadwall
0Λ M M 0Λ 0Λ 10
Greece
correct rank: 1/211
Greece
Croatia
Malta
Monaco
Bermuda
M » at αβ αe 工。
Spain
correct rank: 24/211
Algeria
Palestine
Malta
Croatia
Barbados
magnolia
correctrank: 10/196
^⅜vrαlet HHRSS 2010
Dgdge Caliber Wagon 2007
Audi RS 4 Convertible 2008
Dodge Journey SUV 2012
Dadge Magnum Wagon 20Og
oo e* o.* 0Λ 0Λ xo
Horse Riding
correct rank: 3/101
MlntIty Parade
PQmmeI Horse
Horse Rldlng
ιyp∣ng
Nunchucks
OO O* O.* 0Λ 0Λ XO
correct rank: 1/102
magnolia
frangipani
gaura
cyclamen
SwEet pea
M » at αβ oe 工。
correct rank: 19/196
Ford Expedition EL SUV 2009
Chevrolet Express Van 2007
Chevrolet Sllverado 1500
Cleslc Extended Cab 2007
Fqrd Freestar Mlnhran 2Q07
Honda Odyssey Mlnlvan 2Q07
M » at αβ oe 工。
0Λ OX 0Λ 0Λ 0Λ 10
rose	correct rank: 1/102
rose
desert-rose
bougainvillea
βsteosperπιum
camellia
0Λ M M 0Λ 0Λ 10
red and white trianglewith
black curve approaching warning
Field Hockey Penalty
correct rank: 11/43
red and white triangle with
exclamation Erkwarnlng
Band wh⅛e triangle car
ding I slipping warning
red and WhItetrsngIeWith
peraoπ digging Iα>nstructigπ
red and white tr⅛πgle with
deer warning
red and white tr⅛πgle with
snowflake I ke warning
correct rank: 33/196
Honda odyssey Mlnlvan 2007
VqIvq XC90 SUV 2007
Fard Expedition EL SUV 2009
Daewno Nublra Wagon 2002
Ford Freester Mlnhran 2Q07
0Λ M M 0Λ 0Λ 10
M 02	04 M M XO
Figure A2: Visualization of zero-shot classification of three examples from each dataset. The follow-
ing datasets are used (top to bottom): ImageNet, ImageNet V2, Birdsnap, Country211, Flowers102,
GTSRB, Stanford Cars and UCF101. The ground truth label is displayed above the picture. The bar
plots show the softmax values of the top 5 classes.
has been proven to converge globally to stationary points of the energy E, which are almost always
local minima (Ramsauer et al., 2021). The update rule Eq. (A171) is also the formula of the well-
known transformer attention mechanism (Ramsauer et al., 2021), therefore Hopfield retrieval and
transformer attention coincide.
44
Under review as a conference paper at ICLR 2022
The separation ∆i of a pattern ui is defined as its minimal dot product difference to any of the
other patterns: ∆i = minj,j6=i uiT ui - uiT uj . A pattern is well-separated from the data if
△i ≥ βN + β log(2(N - 1)NβM2). If the patterns Ui are well separated, the iterate Eq. (A171)
converges to a fixed point close to a stored pattern. If some patterns are similar to one another and,
therefore, not well separated, the update rule Eq. (A171) converges to a fixed point close to the mean
of the similar patterns. This fixed point is a metastable state of the energy function and averages over
similar patterns.
The next theorem states that the update rule Eq. (A171) typically converges after one update if the
patterns are well separated. Furthermore, it states that the retrieval error is exponentially small in the
separation △i .
Theorem A4 (Modern Hopfield Networks: Retrieval with One Update). With query ξ, after one
update the distance Ofthe new point f (ξ) to the fixed point U is exponentially small in the separation
△i. The precise bounds using the Jacobian J = fξ) and its value Jm in the mean value theorem
are:
kf(ξ)-遍k 6 IIJmIl2 kξ -遍k ,	(A172)
Jmk2 6 2 βNM2 (N - 1)exp(-β (∆i - 2 max{∣ξ - uik, ∣u* - ui∣} M)) . (A173)
For given e and sufficient large ∆i, we have ∣∣f (ξ) 一 u*k < e, that is, retrieval with one update.
The retrieval error If(ξ) - uiI of pattern ui is bounded by
kf (ξ) — Uik 6 2 (N - 1) exp(- β (∆i — 2 max{∣ξ — uik, k遍—ui∣} M)) M .
(A174)
For a proof see (Ramsauer et al., 2021).
The main requirement of modern Hopfield networks to be suited for contrastive learning is that they
can store and retrieve enough embeddings if the batch size is large. We want to store a potentially
large set of embeddings. We first define what we mean by storing and retrieving patterns from a
modern Hopfield network.
Definition A1 (Pattern Stored and Retrieved). We assume that around every pattern Ui a sphere
Si is given. We say Ui is stored if there is a single fixed point Ui ∈ Si to which all points ξ ∈ Si
converge, and Si ∩ Sj = 0 for i = j. We say Ui is retrieved for a given e if iteration (update rule)
Eq. (A171) gives a point Xi that is at least e-close to the single fixed point Ui ∈ Si. The retrieval
error is |同一Ui ∣.
As with classical Hopfield networks, we consider patterns on the sphere, i.e. patterns with a fixed
norm. For randomly chosen patterns, the number of patterns that can be stored is exponential in the
dimension d of the space of the patterns (Ui ∈ Rd).
Theorem A5 (Modern Hopfield Networks: Exponential Storage Capacity). We assume a failure
probability 0 < p 6 1 and randomly chosen patterns on the sphere with radius M := K√d - 1.
We define a := £(1 + ln(2βK2p(d - 1))), b := 2Kβ, and C := Wo(exp(ba+ιn(b)), where Wo is
4
the upper branch ofthe Lambert W function (Olver et al., 2010, (4.13)), and ensure C ≥ (√ρ) & ɪ.
Then with probability 1 - p, the number of random patterns that can be stored is
d-1
N ≥ pc c "ɪ .
(A175)
Therefore it is proven for C ≥ 3.1546 with β = 1, K = 3, d = 20 andp = 0.001 (a + ln(b) > 1.27)
and proven for C ≥ 1.3718 with β = 1, K = 1, d = 75, andp = 0.001 (a + ln(b) < -0.94).
For a proof see (Ramsauer et al., 2021).
This theorem justifies to use continuous modern Hopfield networks for using retrieved embeddings
instead of the original embeddings for large batch sizes. Even for hundreds of thousands of embed-
dings, the continuous modern Hopfield network is able to retrieve the embeddings if the dimension of
the embeddings is large enough.
45
Under review as a conference paper at ICLR 2022
A.5 Further Related Work
Multiple works have proposed improvements to InfoNCE. Joint Contrastive Learning (JCL) studies
the effect of sampling multiple positives for each anchor. (Cai et al., 2020). Sampling negatives
around each positive leads to higher bias but lower variance than InfoNCE (Wu et al., 2021). InfoNCE
has been generalized to C-InfoNCE and WeaC-InfoNCE, which are conditional contrastive learning
approaches to remove undesirable information in self-supervised representations (Tsai et al., 2021).
ProtoNCE is a generalized version of the InfoNCE, which pushes representations to be closer to
their assigned prototypes (Li et al., 2021). ProtoNCE combines contrastive learning with clustering.
SimCSE employs InfoNCE for contrastive learning to learn sentence embeddings (Gao et al., 2021).
InfoNCE has been extended to video representation learning (Han et al., 2020).
Many follow up works have been based on the CLIP model. The CLIP model is used in Vision-and-
Language tasks (Shen et al., 2021). The CLIP model guided generative models via an additional
training objective (Bau et al., 2021; Galatolo et al., 2021; Frans et al., 2021) and improved clustering of
latent representations (Pakhomov et al., 2021). It is used in studies of out of distribution performance
(Devillers et al., 2021; Milbich et al., 2021; Miller et al., 2021), of fine-tuning robustness (Wortsman
et al., 2021), of zero-shot prompts (Zhou et al., 2021) and of adversarial attacks to uncurated datasets
(Carlini & Terzis, 2021). It stirred discussions about more holistic evaluation schemes in computer
vision (Agarwal et al., 2021). Multiple methods utilize the CLIP model in a straightforward way to
perform text-to-video retrieval (Fang et al., 2021; Luo et al., 2021; Narasimhan et al., 2021).
46