Under review as a conference paper at ICLR 2022
STORM: Sketch Toward Online Risk Minimiza-
TION
Anonymous authors
Paper under double-blind review
Ab stract
Empirical risk minimization is perhaps the most influential idea in statistical learn-
ing, with applications to nearly all scientific and technical domains in the form of
regression and classification models. The growing concerns about the high energy
cost of training and the increased prevalence of massive streaming datasets have
led many ML practitioners to look for approximate ERM models that can achieve
low cost on memory and latency for training. To this end, we propose STORM,
an online sketching-based method for empirical risk minimization. STORM com-
presses a data stream into a tiny array of integer counters. This sketch is sufficient
to estimate a variety of surrogate losses over the original dataset. We provide
rigorous theoretical analysis and show that STORM can estimate a carefully chosen
surrogate loss for regularized least-squares regression and a margin loss for classi-
fication. We perform an exhaustive experimental comparison for regression and
classification training on real-world datasets, achieving an approximate solution
with size even less than a data sample.
1	Introduction
Empirical risk minimization (ERM) has been independently studied by the machine learning, statistics,
and computer science communities. However, conventional ERM algorithms are becoming more
expensive in terms of latency, memory, and energy due to the need for accurate inference over rapidly
increasing data volumes (He et al., 2016; Vaswani et al., 2017). The net effect is an increase in
storage costs of large data matrices (Guhaniyogi & Dunson, 2015), network latency for personalized
inference/recommendations, and carbon emissions. There is a growing need for approximation
algorithms that can minimize the cost, memory, and energy associated with training and inference
(Lin et al., 2020; Motamedi et al., 2016). Additionally, there are distinct advantages of training on a
small memory local/personalized device, as data is generally collected on these devices. This limits
the data transfer to big servers that consumes substantial resources (Bill Allcock, 2001; Xu, 2013)
and minimizes the privacy vulnerabilities (Mendez Mena et al., 2018). Existing compression methods
generally fall into two categories: sketching and sampling.
Linear Sketches: A large family of projection-based sketches uses ideas from compressed sensing,
matrix factorization, and dimensionality reduction. A common theme is to approximate the large
N × d data matrix with a smaller representation. For example, multiplication with a random ±1
matrix yields a sketch that attains the information-theoretic lower bound for compressed linear
regression (within a constant factor) (Clarkson & Woodruff, 2009). Random Gaussian, Hadamard,
Haar, and Bernoulli sign matrices have also been used in this framework with varying degrees of
theoretical and practical success (Dobriban & Liu, 2019). The “sketch-and-solve” technique has
also been extended to regression with the `1 objective and several other linear problems (Sohler &
Woodruff, 2011). This framework cannot accommodate regularized objectives because the model is
computed directly from a set of precomputed sufficient statistics.
Recent work by Shi & Phillips (2021) has shown that a matrix sketch based on the frequent directions
method (Liberty, 2013; Ghashami et al., 2016) can be used for ridge regression. This algorithm
uses a sketch to approximate the most costly part of the ridge regression objective, XTX + λI, in a
streaming setting. The memory is O(ld), where X ∈ Rn×d and l > 2. That is, the memory is linear
in the dimensions, with a minimum size of (2d + 1) × floating precision that can be very large for
high-dimensional data.
1
Under review as a conference paper at ICLR 2022
Coresets and Sampling: A reasonable strategy is to extract a core subset (or coreset) of points that
efficiently represent the complete dataset. The coreset framework supports the approximation of
loss function sums within a (1 ± ) error margin. There are coresets for important problems such as
`p-regression (Dasgupta et al., 2009) and logistic regression (Munteanu et al., 2018). Classification
objectives also admit small coresets (Har-Peled et al., 2007; Tukan et al., 2020), as do many other
regularized ERM problems (Reddi et al., 2015). The problem is that many coresets require super-
linear construction time. For example, many of the algorithms in the previous paragraph require
k-means clustering, leverage scores, or multiple cycles through the dataset (Tukan et al., 2020;
Munteanu et al., 2018; Har-Peled et al., 2007). Coresets are also tricky to merge since it is not
straightforward to decide which points to discard. As a result, composable and streaming coresets are
an active area of research (Indyk et al., 2014; Karnin & Liberty, 2019). Although leverage scores can
be approximated online (Cohen et al., 2016), the process is computationally expensive and not well
suited to edge devices. In many practical problems, random sampling is the only feasible sampling
baseline that supports regularized ERM.
In this work, we propose a new sketching framework (STORM) that supports both regularization
and streaming algorithms. Given an N -point dataset D = {(x1, y1)...(xN, yN)} of d-dimensional
examples observed in the streaming setting (Fiat, 1998), the goal is to construct a small structure
S that can estimate a regression and classification loss over D. STORM consists of two main
components, 1) STORM loss for regression and classification, 2) A tiny array of integer count values
called STORM sketch. This array is sufficient to perform empirical risk minimization on STORM
loss. In practice, our data structure allows for optimizations that are not possible with other methods.
Specifically, our contributions are as follows:
1.	We propose an online sketching algorithm (STORM) that can estimate a special class of
loss functions using only integer count values. Our sketch is appropriate for distributed data
settings because it is small, embarrassingly parallel, and mergeable via addition.
2.	We characterize the set of STORM-approximable loss functions. The price of efficient
loss estimation is a restriction on the kinds of losses we can use. While we cannot directly
estimate popular regression and classification losses, we construct surrogate losses for `2
regression and large margin classification.
3.	We show how to optimize models over STORM sketches. The count-based nature of
our sketch makes gradient descent difficult, and black-box optimization techniques are
of limited use in high-dimensional spaces. We propose an optimization algorithm that is
custom-tailored to our sketch.
4.	We conduct a rigorous empirical evaluation with ridge regression and classification. We
show that our approach has better train/test loss performance and lower variance than
sampling baselines in the low-memory, resource-constrained computational regime. For
some datasets, we can achieve strong performance with memory smaller than even one
sample.
2	Background
2.1	Empirical Risk Minimization
In the standard statistical learning framework, we are given a training dataset D = {(xi , yi) ∈
D × Y}in=1 of examples and asked to select a function h : X → Y that can predict y given x. In
this paper, we will present our algorithms for the data space X = Rd and output space Y = R, but
most of our results hold generally for other metric spaces too. The learning problem is to select a
hypothesis hθ (a function parameterized by θ) that yields good predictions, as measured by a loss
function loss(hθ (x), y). In empirical risk minimization (ERM), we select θ to minimize the average
loss on the training set.
θ = arg min	loss(hθ(xi),yi)
θ	i=1
In the case of Linear regression, hθ (x) = hθ, xi. For most applications, θ is found using the
least-squares or '2 loss: loss(hθ(x)) = ∣∣hθ(x) - y||2. The unconstrained '2 loss is smooth and
2
Under review as a conference paper at ICLR 2022
strongly convex, with favorable convergence criteria. The parameter θ is typically found using
gradient descent or via a closed-form solution from the matrix formulation of the model y = Xθ . For
our discussion, it will be important to express the loss in terms of the concatenated vector [xi , yi]:
n
θ = arg min £((肉,yi], [θ, -1]i)2 = arg min ky - Xθ∣∣2
θ i=1	θ
2.2	Locality Sensitive Hashing
A Locality sensitive hash (LSH) function (l(x)) provides the same hash value for similar points with
high probability. The notion of similarity is usually based on the distance measure of the metric
space X. For instance, there are LSH families for the Jaccard (Broder, 1997), Euclidean (Datar
et al., 2004; Dasgupta et al., 2011) and angular distances (Charikar, 2002). If we allow asymmetric
hash constructions (i.e. l1 (x1) = l2 (x2), l1 6= l2), then x1 and x2 may collide based on other
properties such as their inner product (Shrivastava & Li, 2014). Asymmetric LSH functions use one
hash function (l1) for data and a different function (l2) for queries. To accommodate asymmetric
functions, we use the following definition of LSH, Our definition is a strict generalization of the
original definition from Indyk (1998) Indyk & Motwani (1998), which can be recovered by setting
probability thresholds.
Definition 1. We say that a hash family F is locality-sensitive with collision probability k(∙, ∙) iffor
any two points x1 and x2 in Rd, l(x1) = l(x2) with probability k(x1, x2) under a uniform random
selection of l(∙) ∈ F.
The signed random projection (SRP) family for the angular distance (Goemans & Williamson,
1995; Charikar, 2002) is an example of a symmetric LSH family. The SRP family is the set of
functions l(x) = Sign(W>x), where W 〜N(0, Id). The SRP collision probability is
1	-1 hx1, x2i
k(x1,x2) = 1 - ∏Cos lra∣⅛
The inner product hash Shrivastava & Li (2014) is an example of an asymmetric LSH. To get this
hash function, replace xι with [xι, 0, ,1 - ∣∣xι∣∣2] and x2 with [x2,，1 - ∣∣x2∣∣2,0] in the SRP
function. This procedure essentially uses different hash functions for x1 and x2 , since we augment
x1 (the data) in one way and x2 (the query) in a different way. The collision probability k(x1, x2) is
now a monotone function of the inner product hx1, x2i. This construction implicitly assumes that x1
and x2 are inside the unit sphere, but other hash functions do not require this assumption (Shrivastava
& Li, 2015; Neyshabur & Srebro, 2015; Huang et al., 2018).1
RACE Sketch: The RACE sketch is a 2D integer array S ∈ ZR×B that we index using LSH
functions (Coleman & Shrivastava, 2020; Luo & Shrivastava, 2018). Each row of S is called an
Array-of-Counts (ACE) data structure (Luo & Shrivastava, 2018), and the entire sketch is a set of R
independent ACE repetitions (RACE). To construct a RACE sketch from a dataset D, we begin with
an empty R × B integer array and a set of R independent LSH functions {l1(x)...lR(x)}. Each LSH
function has the collision probability k(θ, x). When an element x arrives from the data stream, we
hash x to get R different hash values. We increment row i at location li(x) and repeat this process
for all elements in the dataset.
To query the sketch with a query θ, we return the average value of S[i, li (θ)] over the R rows. RACE
sketches are interesting because they estimate the sum of LSH collision probabilities over the dataset
with low variance for any query θ.
Theorem 1. Unbiased RACE EstimatorColeman & Shrivastava (2020) Let X = S[i, li(θ)]. Then
E[X] =	k(x, θ)
x∈D
var(X) ≤
1Regardless, we often scale and center the dataset when using inner product hash functions in practice.
3
Under review as a conference paper at ICLR 2022
Repetition 1And 2
more
partitions
Many Repetitions
Figure 1: LSH counts are sufficient for learning regression lines and classification hyperplanes. Here,
we show the partitions that correspond to several STORM repetitions (or rows). Since the regression
line should lie inside a densely-populated region, we use sparsely-populated partitions (shown in
brown) to identify regions that contain the normal to the regression line (regression hyper-plane in
higher dimensions). Each STORM repetition prunes the space of eligible models, until we are left
with the small feasible sparse region (shown in orange). We gain the most information when the
partition boundaries are orthogonal, which is nearly always true in high dimensions.
3	STORM
While collision probabilities have been well-studied in the context of improving near-neighbor
search (Gionis et al., 1999), we study them from a new perspective. Our goal is to compose useful
loss functions from LSH collision probabilities and perform ERM on sketches. In this context, we
query the sketch with the parameter θ to estimate the empirical risk. This requires some extensions to
the RACE sketch.
Intuition To understand our approach, consider the task presented in Figure 1. We partition the
data space into B random regions using Gaussian random hyperplanes. The count of points in each
region gives an estimate of its density. We repeat this partitioning many times so that we have a good
estimate of densely-populated and sparsely-populated regions. In the regression settings, the optimal
model hyperplane passes through the dense regions. Our objective is to find a θ vector normal to the
optimal model hyperplane. This motivates to look for the sparsest region. STORM sketch (inspired
from RACE) provides counts of data points in each partition. We will see that we can optimize
over these counts to find a θ that resides in the sparsest region. The main challenge is to design an
LSH function l(θ) that has the minimal count when θ is a good model. Next, we will talk about an
appropriate hash function, STORM loss for regression and classification and, the STORM sketch for
approximating the loss.
3.1	STORM surrogate loss for Linear Regression
Since we query the STORM sketch with θ, we must be able to express loss(hθ(x), y) as loss(θ, [x, y]).
This condition is not limiting, but it does restrict our attention to hypothesis classes where θ directly
interacts with x and y in a way that can be captured by an LSH function. Linear regression
is the simplest example of such a model. Here the θ interacts with the data via the inner product
h[θ, -1], [x, y]i. The asymmetric hash function from Section 2.2 seems like a natural choice. However,
the linear regression loss function is a monotone function of the absolute value of the inner product.
To obtain a surrogate loss with the correct dependence on h[θ, -1], [x, y]i, observe that the collision
probability of SRP (Section 2.2) is monotone decreasing if we hash [-x, -y] instead of [x, y]. Thus,
We can obtain a function that is monotone in ∣h[θ, -1], [x, y])| by adding together the collision proba-
bility for [x, y] (which is monotone increasing) and for [-x, -y] (which is monotone decreasing). We
call this LSH family “paired random projections” (PRP) because the method can be implemented
by hashing both [x, y] and [-x, -y] With a given SRP function and updating both locations in the
sketch. When We query the vector θ = [θ, -1] With the same SRP function, STORM estimates the
folloWing collision probability/ surrogate loss for linear regression
g(θ, [x,y]) =1 (1 - ∏1cosT(G[χ,yD)) + 1 (1 - IcosT(Tθ,[χ,yD)),
4
Under review as a conference paper at ICLR 2022
where we are free to choose any number of random hyperplanes - p ≥ 2. The proposed loss is convex,
unbiased and consistent. More info in section 4.
Algorithm 1 STORM Sketch creation
Input: Dataset D
Result: STORM sketch S ∈ ZR×B
f for X ∈ D do
for r = 1 …R do
Increment S[r,lr (x)]
end for
end for
Figure 2: To construct a STORM sketch, we initialize a 2D array with R rows and B columns
(buckets) in each row. The asymmetric LSH function li indexes row i of the sketch. We want best
model to hash into least count buckets. See Sections 3.3 for details.
3.2	STORM Surrogate Loss for Classification
Consider linear hyperplane classifiers of the form hθ(x) = sign(hθ, [X, -1]i). The most popular
losses used to find θ are classification-calibrated margin losses. A loss function is classification-
calibrated, or Bayes consistent, if the optimal hypothesis under the loss is the same as the Bayes
optimal hypothesis. With these factors in mind, we propose Classification Random Projections
(CRP) to estimate the following loss function:
Theorem 2. Consider labels y ∈ {-1, 1}. The loss function g(θ, [X, y]) for the linear hyperplane
classifier is a classification-calibrated margin loss and is STORM-approximable.
g(θ, [χ,y]) = 2p(1 - ∏1cos-1(-yhθ,Xi))
By optimizing θ in a similar fashion to as with regression, we can train linear classifiers with STORM
sketches. The LSH function that implements g(θ, [X, y]) is the asymmetric inner product (SRP) hash,
but with the argument to the hash function multiplied by y ∈ {-1, +1}.
See the appendix for details and proof of theorems.
3.3	STORM SKETCH
We use asymmetric LSH functions to hash data points X and θ with different functions. Most LSH
functions have positive definite collision probabilities, but asymmetric LSH functions allow much
more flexibility. Our Paired Random Projections (PRP) hashing scheme premultiply the data vector
by a {±1} to estimate a surrogate regression loss. Classification Random Projections (CRP) LSH
premultiplies X with y. Figure 2 shows our sketch process for PRP. Here we insert elements into the
sketch with two SRP hashes, but only use one SRP for the query. In Section 3.1 and Section 3.2, we
show that this sketch estimates a surrogate loss for linear regression and classification.
Once we have a sketch and an appropriate hash function, we want to optimize the model parameter θ
to minimize the STORM estimate of the empirical risk. Proposed optimization method is discussed
in details in the next section.
3.4	Optimization
Gradient descent is traditionally used to find the best θ for ERM problems. Unfortunately, our sketch
consists of discrete integer counts that have neither an analytic gradient nor a subgradient. However,
following the intuition from Figure 1, we know that our surrogate loss tries to find the hyperparameter
θ in the sparsest region. This allows us to develop a specialized optimization heuristic that exploits
the hyperplanes of the proposed hash to locate the minimum-risk θ more effectively. Our surrogate
objective is: θ = arg minθ g([θ, -1], [X, y]) = arg minθ E(S[L([θ, -1])]).
5
Under review as a conference paper at ICLR 2022
Algorithm 2 Hyperplane-based stochastic gradient descent
Input: STORM sketch S ∈ ZR×B Result: Model θ
Initialise 仄 J 0d+1
for i = 1..R do
Zi = 2* binary(argmin S[i])-1
end for
Aggregate all labels Zi0s to Z and store it.
for iterations t = 1...T do
random sample i
Get random hyper-planes Wi from the given seed.
Update θj = θj-1 - ηVktanh(Wi>θ) - Zikj-2λθ
Project last dimension of θ to -1
end for
Select best Gj using a validation set.
Labels
OoIo [
()1 oci R
0011 I
Labels (z)
To store: Rp = RlogB bits
Figure 3: Left: Shows the algorithm for optimization on the extracted labels of least count bins.
Right: Figure that shows the further compression on the PRP sketch (works the same for CRP) by
storing just the labels (Z) of least count buckets.
where S is the sketch and L = {l1..lp} is the hash function. This implies that θ lies in a region where
most of the STORM partitions have lower count values (small values of S[l(θ)]). One could do a
brute-force minimization over all possible partitions and select a model θ in the winning partition,
but this is computationally expensive. Instead, we use the stochastic gradient descent approach and
transform our objective as follows: ^(θ) = PR=I S[i, Li([θ, -1])]
The gradient is Vg(θ) = PR=ι VS[i,Li([θ, -1])].
It turns out that the individual minimization problem (for each ith sample), arg minθ S[i, Li([θ, -1])]
can also be written as arg minθkLi([θ, -1]) - Zik22. Here Zi ∈ {0, 1}p represents the binary label
vector of the minimum count bucket in row i.
Because we use SRP as the hash function, Li(θG) = sign(Wi>θG). We now have the objective function
in terms of 11sign(W>θ) - Zik2. This can be made diffrentiable by using tanh instead of sign
function. Hence, the gradient update becomes
θGt+1 = θGt - ηVktanh(Wi>θG) -Zik22
This becomes the SGD over a standard single layer neural net with tanh activation. (Refer to
appendix for more details). Also, we can simply add any regularization term with this. In case of
Ridge regression we have the following gradient update
θGt+1 = θGt - ηVktanh(Wi>θG) - Zik22-2λθG
It is interesting to note that, we are learning over randomly generated hyperplanes (Wi) (these only
need a single seed value for generation) and very cheap binary vectors Z ∈ {0, 1}R×p. The sketch
size required is Rp or R log B . Although this method (Algorithm 2) finds approximate gradients, we
find that the procedure works well in practice. Since we can regenerate the hyperplanes from random
seeds, we only need to store the hash codes (Z) of the least populated buckets with this approach.
4	Theory: Sketchable Surrogate Loss
We begin with a formal description of the families of losses that STORM can approximate.
Formally, a loss g(θ, [x, y]) is STORM-approximable if there is a hash function and a STORM
sketch such that querying the STORM sketch with θ yields an unbiased estimator of g(θ, [x, y]).
Using the compositional properties of LSH, we can approximate any sum and/or product of LSH
collision probabilities using one or more STORM sketches. For example, we estimate sums by
6
Under review as a conference paper at ICLR 2022
Figure 4: (a) PRP Surrogate loss for different values of p. It is convex, but less sharp than the L2
loss. A key observation is that p = 4 produces the highest convexity among the family of PRP
surrogate losses. (b)Plot of slope surrogate loss for different values of p at h[θ, -1], [x, y]i = 0.1.
The surrogate loss has the steepest slope (and is thus easiest to optimize) near p = 4. (c) Comparison
of various classification-calibrated loss functions, including the STORM’s CRP loss.
inserting elements with more than one LSH function (or using a second sketch). We can estimate
a product of collision probabilities by combining multiple LSH outputs into a single hash code.
Theorem 3 formalizes this argument (we defer all proofs to the appendix).
Theorem 3.	The set of STORM-approximable functions SL contains all LSH collision probabilities
and is closed under addition, subtraction, and multiplication.
Given the number of known LSH families, the flexibility of asymmetric LSH and the closure of SL
under addition and multiplication, Theorem 3 suggests that SL is an expressive space of functions. It
remains for us to show that SL contains useful losses for machine learning problems.
Theorem 4.	When p ≥ 2, the PRP collision probability g([θ, -1], [x, y])is a convex surrogate loss
for the linear regression objective such that
arg min g([θ, -1], [x, y]) = arg minky - xθk
θθ
Check Figure 4 (a). A detail proof is given in the Appendix.
We analyse, how the sketch size affects the PRP/CRP loss estimation from the STORM sketch.
Consider the following Axiom
Let Z1,...ZR be R i.i.d. random variables with mean E[Z] = μ and variance ≤ σ2. Divide the
R variables into g groups so that each group contains m = R/g elements, and take the empirical
average within each group. The median-of-means estimate μ is the median of the g group means. If
g = 8 log(1∕δ) and m = R/g, then the following statement holds with probability 1 — δ.
lμ - μ∣≤ 6√σRplog ∖/
This proof is given in Alon et al. (1999) as the proof of Theorem 2.1 (which is a slightly more general
version of the statement above). From the variance result from Theorem 1, and a median of means
estimate of the PRP/CRP loss from STORM sketch, we state the following-
Theorem 5.	Let g(θ) is the median of means estimate on the STORM sketch, with R rows, then with
probability more than 1 — δ
∣g(θ) — g (θ)∣≤ 6 Pχ∈Dpk (x,θ) piθg(i∕δ)
R
Where Px∈D ʌ/k(x, θ) is the collision probability of the hash function in use.
4.1	Parameters
Number of hash functions (p): The parameter p is the number of PRP/CRP hash hyperplanes that
creates 2p partitions. Larger p is good for more resolution of the space, but it also leads to many
buckets with few/zero counts. (Figure 4(a) shows that higher p makes the loss surface flatter near
the optimal region). Figure 4 (b) shows that setting p ≈ 4 maximizes the gradient of the surrogate
loss near the minima, and we find that in practice too (Refer Appendix). It is also efficient because
for p = 4, B is only 16. Also, we need fewer rows in the sketch to optimize g(θ, [x, y]) when it is
strongly convex.
7
Under review as a conference paper at ICLR 2022
Table 1: Each dataset has N entries and d features. The datasets are taken with Open Commons
share-alike license and Creative Commons Attribution License. The dimension in bracket represent
the effective dataset dimension after random Fourier feature mapping.
Dataset	N	d	Size	TyPe	
Diabetes (Efron et al., 2004)	442	10	17KB	Patient records
Boston (Harrison & Rubinfeld, 1978)	500	13	25KB	Housing price
Meat (Jowder et al., 1997)	239	123(615)	114KB	Mid-IR spectra
Soil (Corwin, 2019)	679	127 (635)	336KB	Sensor data
Gas (Vergara et al., 2012)	13910	129	6.8MB	Sensor data
Fruit (Holland et al., 1998)	983	235(1175)	902KB	Mid-IR spectra
UJIndoorLoc (Torres-Sospedra et al., 2014)	21048	529	42MB	Localization
Australian (Dua & Graff, 2017)	690	14	37KB	Financial data
Mnist (Lecun et al., 1998)	60000	784	179MB	Image data
Number of repetitions (R): The square root of collision probability，k(x, θ) ≤ 1 always. Hence,
in the worst case the value of Pχ∈D ,k(x,θ) is N. For ∣^(θ) - g(θ)∣≤ C/log(1∕δ), We need
number of sketch rows R < O(N2) (Using the Theorem 5). The worst-case O(N2) situation happens
when every point in the dataset has maximum loss. Practical modeling situations typically have a
small fraction of points for which the training loss is small. In experiments, we vary the number of
rows and see the change in Mean Square Error (MSE).
5	Experiments
Experiment setup: We use PRP and CRP with p = 4 (most optimal, check Figure 4(b)) to create
the sketch and we vary the number of repetitions to show the memory tradeoff. We have used a
variety of datasets. Refer table 1
All models are trained and evaluated with a fixed 80/20 train/test split. We average over 100 runs
for our baselines and sketches, where each run has an independently constructed sketch or random
sample. Thus, our average is over the random LSH functions used to construct the sketch and the
stochastic gradient descent instances. We use the hyperplane optimization heuristic for regression
and derivative-free (Conn et al., 2009) gradient descent for classification. To run the code we used
our personal laptop with i7-8750H processor, 2.2 GHz base frequency and 9MB cache size.
Ridge Regression: We evaluate our method on ridge regression, which penalizes θ to avoid the
double descent and over-fitting problem (Nakkiran, 2019). To implement this model, we simply add
kθk22 to the STORM objective function. The gradient updates take place as described in Section 3.3,
but with an added regularization term. For regularized ERM in our computational setting, random
sampling and Frequent direction sketch is the only reasonable baseline. Coresets for ridge regression
exist, but cannot be constructed in the streaming setting. Also, Frequent direction based sketch takes
space much more than STORM and random samples for higher-dimensional data (near d > 100).
Hence a direct comparison is made for the low dimensional- Diabetes and Boston housing dataset.
Figure 5 shows the results of our comparison focused mostly on sub-kB region. We find that STORM
achieves a lower MSE in the low-memory regime. In particular, for the gas, soil, fruit, and indoor
datasets, STORM produces a reasonable model using less memory than even a single sample. Finally,
the STORM results have a substantially smaller variance (typically 2x), which is a critical factor in
practice. Additional plots and sanity check on a 2D synthetic data is available in the appendix.
STORM for Non-linear Objectives: STORM supports regularized linear regression and classifica-
tion. With this formulation, a non-linear model can be trained with feature mapping such as random
Fourier features (Rahimi & Recht, 2008). To achieve a non-linear boundary the mapping is done to a
higher dimension space. Practically this is 3 - 6 times the original dimensions. Since STORM does
well with high-dimensional datasets, often requiring less memory than a single sample, we expect
good performance using high-dimensional feature transformations. Figure 5 last row demonstrate
the mean square error (MSE) with the size of sketch variables required for linear regression.
Linear classification: We run the linear classification on datasets from the libsvm repository. We
compare against a linear SVM trained on a random sample of the dataset. Figure 5 shows this
comparison for the “Australian credit” (Dua & Graff, 2017) and the popular MNIST (Lecun et al.,
1998) dataset.
8
Under review as a conference paper at ICLR 2022
Size (Kilobytes)
Figure 5: First 7 plots and last row represents MSE vs memory budget for ridge regression. The shaded
region represents one standard deviation. The baselines are vanilla least-square ridge regression
(Least Sq. Ridge) using SGD, mean of the training set labels (Mean), Random sampling ridge
regression (RS - Ridge), and Frequent direction streaming sketch (FD Ridge) (Shi & Phillips, 2021).
STORM takes memory less than even a single sample and hence can perform risk minimization
where even random sampling cannot possibly help. Some of the high dimension data like Fruit shows
this predominantly. The last 2 plots of 3rd row represents classification comparing STORM with
random sampling. In last row we feature transform the data using the Random Fourier Transform
(Rahimi & Recht, 2008). For each experiment, we scale the number of features 5 times its original
size. All results are calculated over 100 runs.
6	Discussion
STORM is a scalable framework to solve regularized ERM problems. It is a memory-efficient
framework that supports regularization and streaming, as long as the loss function can be described
using LSH collision probabilities. The sketch provides compression ranging from 1/50 to 1/10000
of the original size and can take size effectively even less than a sample. Additionally, our approach
is appealing because it performs well in high dimensions, which can be a prerequisite for feature
transformations such as random Fourier features (Rahimi & Recht, 2008). We expect that STORM
can be extended to work for important tasks such as kernel classifiers and kernel ridge regression.
This will greatly expand the modeling options for applications like distributed learning via ERM on
edge devices.
9
Under review as a conference paper at ICLR 2022
References
Noga Alon, Yossi Matias, and Mario Szegedy. The space complexity of approximating the frequency
moments. Journal ofComputer and system sciences, 58(1):137-147, 1999.
John Bresnahan Ann L. Chervenak Ian Foster Carl Kesselman Sam Meder Veronika Nefedova Darcy
Quesnel Steven Tuecke Bill Allcock, Joe Bester. Secure, efficient data transport adn replica
management for high-performance data-intensive computer, 2001.
Andrei Broder. On the resemblance and containment of documents. IEEE Compression and
Complexity of Sequences, 1997.
Moses Charikar. Similarity estimation techniques from rounding algorithms. STOC, 2002.
Kenneth L Clarkson and David P Woodruff. Numerical linear algebra in the streaming model. In
Proceedings of the forty-first annual ACM symposium on Theory of computing, pp. 205-214, 2009.
Michael B Cohen, Cameron Musco, and Jakub Pachocki. Online row sampling. Approximation,
Randomization, and Combinatorial Optimization. Algorithms and Techniques, 2016.
Ben Coleman, Anshumali Shrivastava, and Richard Baraniuk. Race: Sub-linear memory sketches for
approximate near-neighbor search on streaming data. arXiv preprint arXiv:1902.06687, 2019.
Benjamin Coleman and Anshumali Shrivastava. Sub-linear race sketches for approximate kernel
density estimation on streaming data. In Proceedings of the 2020 World Wide Web Conference.
International World Wide Web Conferences Steering Committee, 2020.
Andrew R Conn, Katya Scheinberg, and Luis N Vicente. Introduction to derivative-free optimization,
volume 8. Siam, 2009.
Dennis Corwin. Compilation of literature using apparent soil electrical conductivity with geophysical
techniques to measure soil properties. Mendeley Data, 2019.
Anirban Dasgupta, Petros Drineas, Boulos Harb, Ravi Kumar, and Michael W Mahoney. Sampling
algorithms and coresets for \ell_p regression. SIAM Journal on Computing, 38(5):2060-2078,
2009.
Anirban Dasgupta, Ravi Kumar, and Tamas Sarlos. Fast locality-sensitive hashing. KDD, 2011.
Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab Mirrokni. Locality sensitive hashing scheme
based on p-stable distributions. Symposium on Computational Geometry, 2004.
Edgar Dobriban and Sifan Liu. Asymptotics for sketching in least squares regression. In Advances in
Neural Information Processing Systems, pp. 3670-3680, 2019.
Dheeru Dua and Casey Graff. Australian credit card data set (acr), UCI machine learning repository,
2017. URL http://archive.ics.uci.edu/ml.
Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. Least angle regression. Annals
of Statistics, 2004.
Amos Fiat. Online algorithms: The state of the art (lecture notes in computer science). 1998.
Mina Ghashami, Edo Liberty, Jeff M Phillips, and David P Woodruff. Frequent directions: Simple
and deterministic matrix sketching. SIAM Journal on Computing, 45(5):1762-1792, 2016.
Aristides Gionis, Piotr Indyk, Rajeev Motwani, et al. Similarity search in high dimensions via hashing.
In Vldb, volume 99, pp. 518-529, 1999.
Michael Goemans and David Williamson. Improved approximation algorithms for maximum cut and
satisfiability problems using semidefinite programming. ACM, 1995.
Rajarshi Guhaniyogi and David Dunson. Bayesian compressed regression. Journal of the American
Statistical Association, 2015.
10
Under review as a conference paper at ICLR 2022
Sariel Har-Peled, Dan Roth, and Dav Zimak. Maximum margin coresets for active and noise tolerant
learning. In IJCAI,pp. 836-841, 2007.
David Harrison and Daniel Rubinfeld. Hedonic prices and the demand for clean air. Environ.
Economics and Management, 1978.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
JK Holland, EK Kemsley, and RH Wilson. Use of fourier transform infrared spectroscopy and partial
least squares regression for the detection of adulteration of strawberry purees. Journal of the
Science of Food and Agriculture, 1998.
Qiang Huang, Guihong Ma, Jianlin Feng, Qiong Fang, and Anthony K. H. Tung. Accurate and fast
asymmetric locality-sensitive hashing scheme for maximum inner product search. In Proceedings
of the 24th ACM SIGKDD International Conference on Knowledge Discovery Data Mining,
KDD ’18, pp. 1561-1570, New York, NY, USA, 2018. Association for Computing Machinery.
ISBN 9781450355520. doi: 10.1145/3219819.3219971. URL https://doi.org/10.1145/
3219819.3219971.
Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of
dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing,
pp. 604-613, 1998.
Piotr Indyk, Sepideh Mahabadi, Mohammad Mahdian, and Vahab S Mirrokni. Composable core-sets
for diversity and coverage maximization. In Proceedings of the 33rd ACM SIGMOD-SIGACT-
SIGART symposium on Principles of database systems, pp. 100-108, 2014.
Al Jowder, EK Kemsley, and RH Wilson. Mid-infrared spectroscopy and authenticity problems in
selected meats: a feasibility study. Food Chemistry, 1997.
Zohar Karnin and Edo Liberty. Discrepancy, coresets, and sketches in machine learning. volume 99
of Proceedings of Machine Learning Research, pp. 1975-1993, Phoenix, USA, 25-28 Jun 2019.
PMLR. URL http://proceedings.mlr.press/v99/karnin19a.html.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998. doi: 10.1109/5.726791.
Edo Liberty. Simple and deterministic matrix sketching. New York, NY, USA, 2013. Association
for Computing Machinery. ISBN 9781450321747. doi: 10.1145/2487575.2487623. URL
https://doi.org/10.1145/2487575.2487623.
Ji Lin, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, and Song Han. Mcunet: Tiny deep
learning on iot devices, 2020.
Chen Luo and Anshumali Shrivastava. Arrays of (locality-sensitive) count estimators (ace): High-
speed anomaly detection via cache lookups. WWW, 2018.
Diego Mendez Mena, Ioannis Papapanagiotou, and Baijian Yang. Internet of things: Survey on
security. Information Security Journal: A Global Perspective, 27(3):162-182, Apr 2018. ISSN
1939-3547. doi: 10.1080/19393555.2018.1458258. URL http://dx.doi.org/10.1080/
19393555.2018.1458258.
Mohammad Motamedi, Daniel Fong, and Soheil Ghiasi. Fast and energy-efficient cnn inference on
iot devices, 2016.
Alexander Munteanu, Chris Schwiegelshohn, Christian Sohler, and David Woodruff. On coresets for
logistic regression. In Advances in Neural Information Processing Systems, pp. 6561-6570, 2018.
Preetum Nakkiran. More data can hurt for linear regression: Sample-wise double descent. arXiv
preprint arXiv:1912.07242, 2019.
11
Under review as a conference paper at ICLR 2022
Behnam Neyshabur and Nathan Srebro. On symmetric and asymmetric lshs for inner product search.
In Proceedings of the 32nd International Conference on International Conference on Machine
Learning - Volume 37, ICML'15, pp. 1926-1934. JMLR.org, 2015.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in
neural information processing systems, pp. 1177-1184, 2008.
Sashank J Reddi, Barnabgs P6czos, and Alexander J Smola. Communication efficient coresets for
empirical loss minimization. In UAI, pp. 752-761, 2015.
Benwei Shi and Jeff Phillips. A deterministic streaming sketch for ridge regression. In International
Conference on Artificial Intelligence and Statistics, pp. 586-594. PMLR, 2021.
Anshumali Shrivastava and Ping Li. Asymmetric lsh (alsh) for sublinear time maximum inner product
search (mips). In NIPS, 2014.
Anshumali Shrivastava and Ping Li. Improved asymmetric locality sensitive hashing (alsh) for maxi-
mum inner product search (mips). In Proceedings of the Thirty-First Conference on Uncertainty in
Artificial Intelligence, UAI’15, pp. 812-821, Arlington, Virginia, USA, 2015. AUAI Press. ISBN
9780996643108.
Christian Sohler and David P Woodruff. Subspace embeddings for the l1-norm with applications.
In Proceedings of the forty-third annual ACM symposium on Theory of computing, pp. 755-764,
2011.
Joaquin Torres-Sospedra, Raul Montoliu, Adolfo Martinez-Uso, Tomar Arnau, Joan Avariento,
Benedito-Bordonau Mauri, and Joaquin Huerta. Ujiindoorloc: A new multi-building and multi-
floor database for wlan fingerprint-based indoor localization problems. Proceedings of the Fifth
International Conference on Indoor Positioning and Indoor Navigation, 2014.
Murad Tukan, Cenk Baykal, Dan Feldman, and Daniela Rus. On coresets for support vector machines.
arXiv preprint arXiv:2002.06469, 2020.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need, 2017.
Alexander Vergara, Shankar Vembu, Tuba Ayhan, Margaret Ryan, Margie Homer, and Raman Guerta.
Chemical gas sensor drift compensation using classifier ensembles. Sensors and Actuators B:
Chemical, 2012.
Kunjie Xu. Performance modeling of bittorrent peer-to-peer file sharing networks, 2013.
12
Under review as a conference paper at ICLR 2022
A Appendix
A.1 Proof of theorems
Theorem 1. Unbiased RACE Estimator Let X = S[i, li (θ)]. Then
E[X] =	k(x, θ)
x∈D
var(X) ≤
Proof: The proof is detailed in Coleman(2019) Coleman et al. (2019) and Luo(2018) Luo & Shrivas-
tava (2018).
Theorem 3.	The set of STORM-approximable functions SL contains all LSH collision probabilities
and is closed under addition, subtraction, and multiplication.
It is straightforward to see that STORM can approximate Px∈D k(x, y) as long as there is an LSH
function with the collision probability k(x, y). To prove the theorem, it is sufficient to show that
given two LSH collision probabilities k1(x, y) and k2(x, y), STORM sketches can approximate the
following two functions
f1(y) =	k1(x, y) ± k2 (x, y)	f2(y) =	k1(x, y)k2(x, y)
x∈D	x∈D
Note that one can always write a product of (weighted) sums (Pn wnkn(x, y)) (Pm wmkm(x, y))
as the sum of (weighted) products Pn,m wnwmkn(x, y)km (x, y). Therefore, the previous two
situations ensure that the set is closed under addition, subtraction and multiplication.
Addition and Subtraction: Because of the distributive property of addition,
k1(x, y) ±k2(x,y) =	k1(x, y) ±	k2(x,y)
x∈D	x∈D	x∈D
One can then construct a STORM sketch S1 for the k1(x, y) summation and a second STORM sketch
S2 for k2(x, y) summation. We can estimate any linear combination of k1(x, y) and k2(x, y) by with
a weighted sum of S1 and S2 .
Multiplication: To approximate sums over the product k1(x, y)k2 (x, y), we rely on LSH hash
function compositions. Suppose we have an LSH function l1(x) with collision probability k1(x, y)
and l2(x) with k2(x, y). Consider the hash function l(x) = π(l1 (x), l2(x)) where π(a, b) is an
injective (or unique) mapping from Z2 → Z. An example of such a mapping is the function
π(a, b) = p1apb2 where p1 and p2 are coprime. Since the mapping is injective, this means that
l(x) = l(y) only when l1(x) = l1(y) and l2 (x) = l2(y). Therefore, Pr[l(x) = l(y)] = Pr[l1(x) =
l1 (y) ∩ l2(x) = l2(y)]
Make the choice of the LSH functions l1 and l2 independently, so that the probability factorizes
= Pr[l1(x) = l1 (y)]Pr[l2 (x) = l2(y)] = k1(x, y)k2(x, y)
Therefore, one can construct a STORM sketch for the product using the LSH function l(x) =
π(l1 (x), l2(x)).
Theorem 4.	When p ≥ 2, the PRP collision probability g([θ, -1], [x, y])is a convex surrogate loss
for the linear regression objective such that
arg min g([θ, -1], [x, y]) = arg minky - xθk
θθ
Proof: For the surrogate ERM problem to have the same solution as the linear regression ERM
problem, it is sufficient to show two things: that the surrogate loss is convex and that the global
minima of the surrogate loss and the linear regresssion loss appear in the same location. The surrogate
loss (L(θ, x, y)) is
g([θ,	-1], [x,y]) = 2 (1 - ∏CosT(Qθ, -1], [x,yD)) +2 (1 - ∏CosT(T[θ, -1], [x,yD))
13
Under review as a conference paper at ICLR 2022
L2
PRP,p=2
PRP,p=4
PRP,p=6
SUITOgate IoSS
Fruittest
-1.00 -0.75 -0.50 -0.25 0.00	0.25	0.50	0.75	1.00	0.0	0.5	1.0	1.5	2.0	2.5	0.0	0.5	1.0	1.5	2.0	2.5
Ve [X y]>	r∩em (KB)	mem (KB)
Figure 6: Left: Surrogate regression loss (STORM regression loss) for different values of p. It has the
highest convexity near p = 4. Right two: Mean square error with varying sketch size with different p
on two real world datasets. We observe that p=3,4 works the best.
For the sake of notation, we will put a = [θ, -1], b = [x, y], and
f (a，b) = (1 - ∏ CosT (h[θ,-1], [χ, yD))	L⑶ b) = 1 f ⑶ b)p + 2f (Vb)P
We will use the fact that
Va f (a, b)= Va f(a,-b) =
b
πP Tha, bil2
Location of Minima: The minimum of the surrogate loss is same as the minimum for least squares
linear regression. Using the chain rule, the gradient of the surrogate loss is
VaL(a,b)=
p(f(a, b)p-1- f(a,-b)p-1)
b
2π√1-∣(a, b〉|2
When p = 1, the gradient is always zero. When p ≥ 2, the derivative is zero when ha, bi =
h[θ, -1], [x, y]i = 0 because that is where f(a, b) = f(a, -b). Thus, the surrogate loss has the
same minimizer as the least squares loss.
Convexity: At index (i,j), the Hessian of the surrogate loss is
E L b]	_ ∂ BL b] —	P (f (a, b)p-1 - f(a,-b)p-1)
[VaL(a，b)](i，j) =西[VaL(a, b)]i =	2πp1-∣ha, b)|2
Thus, the Hessian
V2aL(a,b) = b>Va
pf (a, b)p-1 -pf(a,-b)p-1
2π√1-∣(a, b〉|2
The gradient
pf (a, b)p-1 -pf(a,-b)p-1
2π√1-∣(a, b〉|2
P(P-I)f(a, b)p-2	P(P - 1)f (a, -b)p-2v
2∏p1-∣ha, bi/ Vaf(a, b) +	2πp1-∣ha, bi|2 Vaf (a, -b)
+ (pf (a, b)p-1 - Pf (a,-b)p-1 )Va
1
2π√1-∣(a, b〉|2
Simplifying, we obtain
bTb (P(P - 1)f (a, b)p-2- P(P - 1)f (a,-b)p-2	b Pf (a, b)p-1 + Pf (a, -b)p-1
∏(1-∣ha, b〉|2)	+ ha, i (1-∣ha, b〉|2产
14
Under review as a conference paper at ICLR 2022
Figure 7: Comparison of various classification-calibrated loss functions, including the STORM
classification loss.
Which gives the following expression for the Hessian
▽2 L(a, b) = bτ bρ(a, b)
where
1	P(P - I)f(a, b)p-2 - p(p - 1)f (a, -b)p-2	Pf (a, b)p-1 + pf (a, -b)p-
Pab) = 2∏ (----------------Π(i-∣ha,b〉12)-------------+ ha,bi —(1-lha,b〉|2)3/2
for P ≥ 2 ρ(a, b) > 0 ∀ha, bi ∈ (-1, 1). Hence, the Hessian is positive semidefinite and the
function is convex in a. Also note that the function is convex in θ, since the restriction of the last
dimension of a to -1 is the restriction of a convex function to a convex set. Refer Figure 6(a).
Theorem 2. Consider labels y ∈ {-1, 1}. The loss function g(θ, [x, y]) for the linear hyperplane
classifier is a classification-calibrated margin loss and is STORM-approximable.
g(θ, [x,y]) = 2p(1 - IcosT(-y<θ, x〉))
Proof. First, we show that the loss is classification-calibrated. Then, we show that the loss can be
estimated using STORM.
Loss is Classification-Calibrated: A necessary and sufficient condition for a convex2 loss function
φ(t) to be classification-calibrated is for 亲φ(t) < 0 at t = 0. Here, t = yh(x), where h(x) is the
model. For a linear hyperplane classifier, h(x) = hθ, x〉. The loss is therefore
φ(t) = 2p(1 — co cos-1 (—1))
φ(t) is convex when P ≥ 2 for the same reasons discussed in the proof of Theorem 4. Note that
the simple asymmetric LSH for the inner product that we have used throughout the paper3 requires
t ∈ [-1, 1]. The derivative is
d	1	p-1	1
dtφ(t) = 2p p(1- ros-1 (-t))	∏√—-12
2For non-convex φ(t), the sufficient conditions are more complicated.
3There are other asymmetric inner product LSH functions without this requirement, and in practice one
usually scales the data to make the condition true.
15
Under review as a conference paper at ICLR 2022
At the origin,第 φ(t) is -1/n. Therefore, the loss is classification calibrated. Figure 7(b) compares
our STORM surrogate classification loss against popular margin losses.
Loss is STORM-Approximable: Consider the asymmetric LSH function for the inner product
where we premultiply x by -y . The collision probability under this LSH function is
k(θ, X)=(1 - 1cos-1(hθ,-yxi)
as desired.	□
Theorem 5. Let ^(θ) is the median of means estimate on the STORM sketch, with R rows, then with
probability more than 1 - δ
∣g(θ) -g(θ)l≤ 6Pχ∈Dpk(x,θ) Plog(1∕δ)
R
Proof. Consider the following axiom-
Let Zi,...Zr be R i.i.d. random variables with mean E[Z] = μ and variance ≤ σ2. Divide the
R variables into g groups so that each group contains m = R/g elements, and take the empirical
average within each group. The median-of-means estimate μ is the median ofthe g group means. If
g = 8log(1∕δ) and m = R/g, then thefollowing statement holds with probability 1 — δ.
lμ - μ∣≤ 6√R plog ∖∣6
This proof is given in Alon et al. (1999) as the proof of Theorem 2.1 (which is a slightly more general
version of the statement above).	□
Now, et Z(q) be the median-of-means estimate constructed using the R unbiased estimators of the
sketch with R rows. Then with probability 1 - δ,
∣g(θ) -g(θ)∣≤ 6 Pχ∈D√^(x,θ) Pogi洞
R
From Theorem 1, we know that σ ≤ Px∈D √kM
Substituting this variance bound into the given Lemma proves the theorem.
Required number of repetitions: The square root of collision probability，k(x, θ) ≤ 1 always.
Hence, in the worst case the value of Pχ∈D，k(x, θ) is N. For ∣^(θ) - g(θ)∣≤ C/log(1∕δ), We
need number of sketch rows R = O(N2 ).
SGD convergence and sketch size for hyperplane based method: Assuming that 1) ^(θ)
is bounded below, 2) The gradient is L-Lipschiz continuous and 3) the gradients bounded as
E [∣N^i(θ)∣∣2] ≤ σ2, with constant step size η we will get the error (g(θt) - ^(θ*)) bounded
by O(1/t) + O(η).
For a simplistic assumption that we are drawing a new sample every time the total number of steps is
equivalent to the number of repetitions R in the sketch. Hence the error at the end of the iteration
will be bounded in terms of sketch size by O(1/R) + O(η).
A.2 Experiments
A.2.1 Experiment on 2D synthetic dataset
We plot the estimated parameter θ from the STORM on 2D synthetic data (Train: 1600 points, Test:
400 points) generated from Gaussian distribution. Refer to Figure 8.
16
Under review as a conference paper at ICLR 2022
Figure 8: (a) Regression with linear model. (b) regression with linear model and Random Fourier
feature Rahimi & Recht (2008) mapping, and (c) classification STORM losses on synthetic 2D
datasets.
A.2.2 NON LINEAR REGRESSION ON SYNTHETIC DATA:
:ompression
compression
synthetic (n=1600,d=2)
----x compression
6660
synthetic (n=1600rd=2)
synthetic (n=1600,d=2)
Figure 9: The random Fourier feature maps the input dimension to a higher dimension for linear
model training. Here we map it to a 10 dimension space and project back the solution in 2D. The
figure shows the learnt model with different STORM sketch sizes. On the given synthetic data,
STORM achieves a compression of 1/256 times to 1/6660 times.
Algorithm 3 Zero order gradient descent
Input: STORM sketch S ∈ ZR×B constructed with
PRP, number of queries k to approximate the gradient,
step size η and sphere size σ)
Result: Linear regression parameter θ
Result: Initialise θ0 — 0d+1
for number of iterations n = 1, ...N do
{v1 , v2, ..vk} = random vectors on the σ-sphere
centered at θ
{δ1,δ2, ..δk} = {QueryS(v1), ...QueryS (vk)}
Approximate gradient V(θ) = mean(δ1, δ2, ..δk)
Update θn = θn-ι — η(g(θ))
C	・	r 7∖ a aΛ	a ∙ j -ι
Project last dimension of θ onto the constraint —1
end for
Size (Kilobytes)
Figure 10: Left: Shows the algorithm for zero order gradient descent. Right: The compressed PRP
sketch (which uses Hyperplane based gradient descent) is Rp bits in size. And on a experiment with
real data we found out that it can give acceptable model with a very less sketch size.
A.2.3 Comparison with zero order gradient decent
We perform derivative-free optimization Conn et al. (2009) here on the original STORM sketch.
Here, the black-box access to the loss function of interest (or its sharp approximation) is sufficient.
For the descent algorithm, we initialize θ to zeros in d + 1 dimensions. The additional dimension
17
Under review as a conference paper at ICLR 2022
is due to querying the sketch with [θ, -1] rather than [θ]. We compute the approximate gradient by
querying the sketch with equidistant points in a ball around θ and update the parameter. After each
iteration, we project the last dimension of θ back onto the constraint θd+1 = -1. This takes R × B
size, with an additional factor of sketch cell bit precision. Hence it is more memory intensive than the
hyper-plane based stochastic gradient descent algorithm. Refer the figure below.
A.2.4 Non-regularised Linear regression
Figure 11: Here, we report the mean square error for our method without any regularization, when
compared with baselines at a variety of memory budgets.
In Figure 11, we observe a double-descent phenomenon for our sampling baselines, explaining the
peak near the intrinsic dimensionality of the problem. This sample-wise double descent behavior was
recently proved for linear regression by Nakkiran (2019). STORM does not experience the double
descent curve in practice because the entire dataset (not just a subsample) is used to minimize the
loss. We perform favorably against baselines in memory regimes affected by double descent and
STORM performs competitively in other memory regimes. We also observe that the θ found using
STORM converges to the optimal θ under least-squares ERM. The datsets in use are taken from UCI
repository.
18