Under review as a conference paper at ICLR 2022
Influence-Based Reinforcement Learning for
Intrinsically-Motivated Agents
Anonymous authors
Paper under double-blind review
Ab stract
Discovering successful coordinated behaviors is a central challenge in Multi-Agent
Reinforcement Learning (MARL) since it requires exploring a joint action space
that grows exponentially with the number of agents. In this paper, we propose
a mechanism for achieving sufficient exploration and coordination in a team of
agents. Specifically, agents are rewarded for contributing to a more diversified team
behavior by employing proper intrinsic motivation functions. To learn meaningful
coordination protocols, we structure agents’ interactions by introducing a novel
framework, where at each timestep, an agent simulates counterfactual rollouts
of its policy and, through a sequence of computations, assesses the gap between
other agents’ current behaviors and their targets. Actions that minimize the gap
are considered highly influential and are rewarded. We evaluate our approach on a
set of challenging tasks with sparse rewards and partial observability that require
learning complex cooperative strategies under a proper exploration scheme, such
as the StarCraft Multi-Agent Challenge. Our methods show significantly improved
performances over different baselines across all tasks.
1	Introduction
Deep Reinforcement Learning (DRL) has been applied to solve various challenging problems, where
an agent typically learns to maximize the expected sum of extrinsic rewards gathered as a result of its
actions performed in the environment (Sutton et al., 1998). Multi-Agent Reinforcement Learning
(MARL) refers to the task of training a set of agents to maximize collective and/or individual rewards,
while existing in the same environment and interacting with each other.
Recent works have shown that agents with coordinated behaviors learn remarkably faster (Roy et al.,
2019) since coordination helps the discovery of effective policies in cooperative tasks. Nevertheless,
achieving coordination among agents still remains a central challenge in MARL (Jaques et al.,
2019). Prominent works often resort to a popular learning paradigm called Centralized Training
with Decentralized Execution (CTDE) (Lowe et al., 2017; Foerster et al., 2018), where each agent is
evaluated using a centralized critic and has access to extra information about the policies of other
learning agents during training. At the time of execution, policies’ actions are restricted to local
information only (i.e. their own observations). Our work is primarily motivated by the following
natural questions:
In multi-agent settings, how can we quantify the effect of an agent (or player) on other teammates’
behaviors (particularly on their performance)? And to what degree exploiting this quantity can lead
to coordinated behaviors and consequently better overall performances?
To that end, we propose a novel approach that aims at promoting coordination for cooperative tasks
by augmenting CTDE MARL main return-maximization objective with an additional multi-agent
objective that acts as a policy regularizer; we refer to the latter objective as the inf luence f unction.
To build intuition, a chosen agent, which we call the “influencer”, assesses the progress that other
agents are making given its current policy and consequently learns behaviors that will result in an
improved performance of its teammates. Concretely, we formulate the influence of an influencer π as
an estimation of the dissimilarity between other agents’ behaviors and their targets given the current
behavior of π . The influencer is encouraged to learn behaviors that are expected to minimize that
1
Under review as a conference paper at ICLR 2022
dissimilarity. We also propose two approaches to estimate the influence and empirically show that
they yield unbiased estimates of the true value.
Agents acting upon the proposed coordination paradigm learn to efficiently exploit the observed joint
action space using available information. However, and since the joint space grows exponentially
with the number of agents, it is highly unlikely that agents will have access to sufficient information
to learn optimal behaviors to solve the task at hand; this problem arises in many scenarios such as
sparse-reward environments, thus a proper exploration scheme is often required. However, many
existing multi-agent deep reinforcement learning algorithms still use mostly noise-based techniques
(Liu et al., 2021; Rashid et al., 2018; Yang et al., 2018). Moreover, independent exploration proved
to be inefficient in cooperative settings (Roy et al., 2019). Recently, this challenge was addressed
through Intrinsic Motivation (IM) (Jaques et al., 2019; Du et al., 2019; Zhou et al., 2020b). Many
approaches employ IM to encourage exploration of state-space (Han et al., 2020; Burda et al., 2019)
or state-action space (Fayad & Ibrahim, 2021) by identifying novel configurations and rewarding an
agent for visiting them. We provide an extension of these ideas into multi-agent settings and further
build connection between reward shaping and coordinated behavior learning, where we choose an
agent to act as an influencer (i.e. regularize its standard objective using the influence function) while
other agents learn to maximize the expected sum of both extrinsic and intrinsic rewards.
To sum up, our main contributions are threefold: 1) developing an influence function to promote
learning coordinated behaviors and improve team performance; 2) extending exploration via random
network distillation to multi-agent settings by crafting a "novelty" function that rewards under-
explored behaviors; 3) formulating a novel intrinsic incentive to promote learning diverse team
behaviors to help uncover complex behaviors in a collaborative way.
We demonstrate the effectiveness of our methods on a comprehensive set of challenging tasks which
include, but not limited to, the StarCraft Multi-Agent Challenges (SMAC) (Samvelyan et al., 2019)
and the Multi-Agent Particle Environments (MAPE) (Mordatch & Abbeel, 2018; Lowe et al., 2017).
Empirical results show a significant improvement over a wide variety of state-of-the-art MARL
approaches. We also conduct insightful ablation studies to understand the relative importance of each
component of the approach individually.
2	Background
2.1	Markov games
Markov games (Littman, 1994; Agarwal et al., 2020; Daskalakis et al., 2021) are a superset of Markov
decision process (MDPs) and matrix games, including both multiple agents and multiple states.
Formally, we consider a setting with N > 2 agents ([N] := {1, 2 . . . , N}) who repeatedly select
actions in a shared MDP. The goal of each agent is to maximize their respective action value function.
Formally, an MDP is defined as a tuple G = (S, N, {Ai, ri}i∈[N] , T, γ, ρ), where:
•	S is the finite states space, where the initial states are determined by a distribution ρ : S →
[0, 1]. Since we consider partially observable testing environments, we denote by si the
information observed by agent i about the global state s ∈ S .
•	Ai is a finite action space for agent i with generic element ai ∈ Ai . Using common
conventions, we will write A = Πj∈[N]Aj and A-i = Πj 6=iAj to denote the joint action
spaces of all agents and of all agents other than i with generic elements a = (ai)i∈[N] and
a-i = (aj)i6=j∈[N], respectively. According to this notation, we have that a = (ai, a-i) ∀i.
•	ri : S × A → R is the individual reward function of agent i, i.e., ri(s, ai , a-i) is the
instantaneous reward of agent i when agent i takes action ai and all other agents take actions
a-i at state s ∈ S.
•	T is the transition probability function, for which T(s0|s, a) is the probability of transitioning
from s to s0 when a ∈ A is the action profile chosen by the agents.
•	γ is a discount factor for future rewards of the MDP, shared by all agents.
In a Markov game, each agent is independently choosing actions and receiving rewards. Convention-
ally, an agent k aims to maximize its own total expected return Rk = PtT=0 γtrk,t where T is the
time horizon.
2
Under review as a conference paper at ICLR 2022
2.2	Multi-Agent Deep Deterministic Policy Gradient
MADDPG (Lowe et al., 2017) is a multi-agent extension of the DDPG algorithm (Lillicrap et al.,
2015). It adapts the CTDE paradigm, where each agent i possesses its own deterministic policy μ(i)
for action selection and critic Q(i) for state-action value estimation, respectively parameterized by θ(i)
and φ(i). All parametric models are trained off-policy from previous transitions ζt := (st, at, rt, st+1)
uniformly sampled from a replay buffer D. Each centralized critic is trained to estimate the expected
return for a particular agent i from the Q-learning loss:
L(i)(φ(i) ) = EZt 〜D ∣∣Q(i)(st, at； Φ(i)) — [ri,t + γQ(⅛et(st+ι,μ(st+ι))] f] ⑴
Each policy is updated to maximize the expected discounted return of the corresponding agent i :
JP(iG) (θ(i)) =ED hQ(i)(st,at)i	(2)
Notice that while optimizing an agent’s policy, all agents’ observation-action pairs are taken into
consideration. By that, the value functions of all agents are trained in a centralized, stationary
environment, despite happening in a multi-agent setting. Moreover, this procedure allows for the
learning of coordinated strategies, yet needs to be augmented with efficient exploration methods that
reward novel action configurations which may lead to the discovery of higher-return behaviors.
3 Methods
3.1 Basic Influence
Intuitively, one can define coordination in a team of agents as the behavior of each individual agent
being informed by other agents. Furthermore, agents’ behaviors can be inter-affected either directly
through communication for example or indirectly through task-specific shared goals and/or rewards
or the dynamics of the environment. We hypothesize that when agents learn in a cooperative setting,
they tend to affect each other’s exploitation processes, we confirm the hypothesis throughout the paper
and build on that to formalize a general method to foster influential interactions and learn meaningful
coordination protocols. Specifically, consider a setting where N agents with policies π1, π2, . . . , πN.
We assume that the transition dynamics are dependent on agents’ actions (i.e. T(s0|s, a) 6= T (s|s0)),
and use tasks that satisfy this assumption to validate our methods. We will write π, π-j to denote the
joint policy of all agents and of all agents other than agent j , respectively. For two fixed agents i and
j, the influence of the behavior of j’s policy on i’s performance (expected return) can be quantified by
"marginalizing" out all effects induced by other agents on the expected outcomes of i. More precisely,
if the desired quantity is qiπj, then for all (s, aj) ∈ S × Aj:
qπ (s,aj) =	E [ri(s, a)] + Y	P(SlS,aj) ∙ E	[qπj(S0,aj)]
a-j~π-j	s0	aj~nj(.|Sj)
p(s0∣s,aj) = ET(s0∣s, a) ɪɪ ∏k(ak|s)
a-j	k6=j
(3)
One can note that, analogously to the action-value function Qi , qiπj measures tuple values w.r.t i
where the learning dynamics of i conditioned only on the behavior of j . Naturally, in expectation,
qiπj represents the sum of rewards that an agent i is expected to receive when influenced by πj .
Based on above, we say that an agent is an optimal influencer when agents only influenced by its
policy learn optimal behaviors. To that end, if we fix an agent (say 1), then its influence, F (π1), on
the rest of the team is defined as follows:
F(π1) =	αi ∙ Es,aι〜∏ι [(qi i (s, a1) - maχ Qi(S, U))]
2≤i≤N	u
(4)
where (αi)2≤i≤N are positive scaling factors that sum up to 1 and Qi is the action value function
of agent i. Intuitively, since F(π1) is the difference between the expected rewards of other agents
conditioned on the current behavior of π1 and the maximum returns that can be achieved by other
agents in the given task, minimizing F will encourage π1 to take actions that lead other agents’
3
Under review as a conference paper at ICLR 2022
conditioned returns (qiπ1)2≤i≤N closer to their maximum unrestricted returns (maxu Qi(s, u)),
which matches our definition of an optimal influencer. Hence, the objective functions of agents
i ∈ [N] are: Ji(θi) = Eπ P0≤t≤T γtri,t - λF(π1)1i=1, where θi are the parameters of πi and λ
is a hyperparameter called the influence importance temperature.
Theorem 1. (Influence Gradient)
VθιF(∏ι) = E αiEs,∏ι [Vθι log∏ι(aι∣s)g(s, aι)2
2≤i≤N	(5)
+ 2g(s, aι)Es,∏ι [Vθι log∏ι(aι∣s)qπ1 (s, aι)]]
where g(T) = g(s, a1) = qiπ1 (s, a1) - maxu Qi(s, u).
It has been shown empirically that the existence of one influencer can greatly improve the overall
team performance. Thus, we only consider one influencer throughout our practical sections and upon
that, we call our method "Asymmetric Learning for Influencing a Team of Agents (ALITA)".
3.1.1	Influence with Single Estimator
Practically, we quantify the influence F(π1) by initializing a network Qcen : S × A → RN-1 with
parameters φcen; the i-th component of Qcen(.; φcen) estimates qiπ1 by minimizing the following
loss at each timestep t:
L(Φcen) = E(x,a,r,χ0)〜Dt[kQcen (x,耳 Φcen) - 丫『]	(6)
where y ∈ RN-1, yi = ri(x,a) + γQt(air)get(x0, π(x0)); Qt(air)get is the target critic of agent πi, and
Dt = {T | T = (x, a, r, x0) ⊂ B and x1 = s1,t, a1 = a1,t}, where B is a buffer storing all agents’
transitions. The reason for choosing such definition of D is that when fixing the current information
of π (i.e. at timestep t) while updating Qicen, we practically marginalize out all agents’ experience
(other than 1 and i). Later in this section, we show that this trick yields a fairly good approximation
of qiπ1 . After training Qcen, we compute the influence F(π1):
F(∏l) = E(x,.,r,χ0)〜B [kQcen(χ,∏(χ)) - z『]	(7)
where z = [maxu Q2(x0, u), . . . , maxu QN(x0, u)]T. Note that the second term in the expectation
(i.e. the target vector z) is set to be Undifferentiable with respect to ∏ι's parameters and thus does not
propagate through its network.
3.1.2	Influence with Multiple Individual Estimators
As seen earlier, agent π1 estimates the gap between each agent’s value and its target value by
employing a single network. Another desirable approach is to additively decompose the centralized
estimator Qcen, i.e. to use multiple estimators where each estimator, namely Qc(lio)ne, individually
calculates a fairly good approximation of qiπ1 . To reduce computational costs and arrive at better
estimates, each estimator’s network is initialized with the parameters of the corresponding critic
network at each episode (i.e. Qcione J Qi). The training is carried out similarly to that of the single
estimator setting,
L(φ' ^) = E(x,a,r,x0 )~Dt [||Q(ione(x, a； φ(i)) - yi||2]	⑻
The influence function could be expressed as:
1 N-1	2
F(π1) = E(x,,,x0)〜B N - 1X Q(clio)ne(χ, π(χ)) -zi2	(9)
i=1
Which approach yields better estimates of the true value of the influence? The influence of an
agent π on a team of agents T was defined as a measure of the improvement in the performance of
T given the current behavior of π. However, measuring the influence using function approximators
might result in inaccurate estimates. To resolve this concern, we plot the influence estimates of the
two prior approaches over time while they learn on the Cooperative Navigation MAPE task (Mordatch
4
Under review as a conference paper at ICLR 2022
& Abbeel, 2018), where the number of agents is N = 6. In Figure (1), we graph the average influence
estimates over 40000 episodes and compare it to the true value. The latter is averaged over 1000
episodes following the current policies of agents and is reported every 5000 episodes. The plots show
a relatively small bias of both methods during learning. However, as Figure (1) suggests, measuring
influence using multiple individual estimators yielded more accurate values after enough training
which substantiates its superiority over the shared network approach. Note that, although confirms
our prior hypothesis, this experiment does not reflect the importance of employing the influence on
the final performance of the agents as we will discuss that in Section (4).
3.2 Intrinsic Motivation for Diversified Team Behavior
In this section, we introduce a framework for achieving coop-
erative exploration by ensuring that agents are consistently
tilted towards visiting under-explored state-action configu-
rations; we start by providing a simple demonstration which
shows that the number of environment steps required for
all agents to randomly traverse all possible action configu-
rations increases at least exponentially with the number of
agents.
Proposition 1. Consider an L-action setting of n agents.
In expectation, the number of steps T needed to visit all
Ln action configurations at least once without coordinated
exploration grows at least exponentially with the number of
agents. More concretely, E[T] = Ω(nLn).
To mitigate this issue, we assign agent π1 a prediction error
as an intrinsic reward to facilitate recognizing and learning
novel behaviors:
Figure 1: Empirical evaluation of the
bias in the proposed methods of mea-
suring influence. The influence values
estimated by single and multiple esti-
mators are compared to the true values
of F . The results for the estimated
values are averaged across 8 runs.
r(π1)=rext+λ∏*i1,t)-(s1,t,a1/2	(10)
ψ(s1,t,a1,t)
Where φ is an autoencoder network regularly trained on data generated by the policy π1 and λπ is a
hyper-parameter that balances the extrinsic and intrinsic reward terms. ψ's expression stems from the
observation that when an autoencoder is trained on data from a particular distribution, it will be good
at reconstructing data from that distribution, while it will perform poorly if the data is from a different
distribution. Thus, by employing ψ as an intrinsic bonus, π1 rewards states’ observations and actions
that do not belong to the data generated by it. In practice, φ is designed to be a relatively large
network since we want it to be slightly overfitted to the training data so that it will not accidentally
generalize to behaviors that we may deem novel (Fayad & Ibrahim, 2021; Zhang et al., 2019).
Nevertheless, assigning each agent a ψ is not sufficient as it makes the case equivalent to independent
exploration approaches. Thus, we propose a coordinated exploration method that takes into account
other agents’ behaviors, encouraging agents to diversify team behavior while maintaining good
performance.
Specifically, we assign agents {πi }iN=2 intrinsic penalty defined as:
rinɪ(st, at) = - exp ( - ω,ψ(sι,t,aι,t)) ∣∣∏i(si,t) - ai,t∣∣2	(11)
This reward term aims at teaching the agents to recognize previous behaviors and synchronously
select novel configurations. To build intuition, consider a case where N = 2. Whenever (π, μ) select
an action tuple in the neighborhood ofa frequently-visited tuple (a1, a2) in a global state s, ψ will be
relatively small and the penalty, riint, will be large. Conversely, if (π, μ) encounter a novel tuple, say
(a1, a2) in s, the small penalty of μ (Eq. (11)) together with a large reward for π (Eq. (10)) will drive
both agents to further explore this encounter.
In all, Fig. (2) shows how this framework can be augmented with the basic influence introduced
earlier to reinforce learning and discovering coordinated behaviors.
5
Under review as a conference paper at ICLR 2022
Figure 2: Architecture of the general proposed method
4 Empirical Evaluation & Analysis
The goals of our experiments are to: a) verify the performance of our method on a comprehensive set
of multi-agent challenges (SMAC, MAPE, sparse-reward settings, and continuous control environ-
ments); b) perform ablations to examine which particular components of the proposed framework are
important for good performance.
4.1	Cooperative & Mixed Games
4.1.1	StarCraft Multi-Agent Challenge
StarCraft provides a rich set of heterogeneous units each with diverse actions, allowing for ex-
tremely complex cooperative behaviors among agents. We thus evaluate our method on several SC
micromanagement tasks from the SMAC1 benchmark (Samvelyan et al., 2019), where a group of
mixed-typed units controlled by decentralized agents needs to cooperate to defeat another group of
mixed-typed enemy units controlled by built-in heuristic rules with “difficult” setting; the battles
can be both symmetric (same units in both groups) or asymmetric. Each agent observes its own
status and, within its field of view, it also observes other units’ statistics such as health, location,
and unit type (partial observability); agents can only attack enemies within their shooting range.
A shared reward is received on battle victory as well as damaging or killing enemy units. Each
battle has step limits set by SMAC and may end early. We consider 5 battle maps grouped into
Easy (2s3z), Hard (5m_vs_6m, 3s_vs_5z), and Super Hard (corridor, 3s5z_vs_3s6z) against 8
baseline methods using their open-source implementations based on PyMARL (Samvelyan et al.,
2019): IQL (Tan, 1993), VDN (Sunehag et al., 2018), QMIX (Rashid et al., 2018), LIIR (Individual
Intrinsic Rewards) (Du et al., 2019), LICA (Implicit Credit Assignment) (Zhou et al., 2020b), and
MAVEN (Variational Exploration) (Mahajan et al., 2019), EDTI (Decision-Theoretic Exploration) ,
EITI (Information-Theoretic Exploration) (Wang et al., 2019).
The corridor map, in which 6 Zealots face 24 enemy Zerglings, requires agents to make effective
use of the terrain features and block enemy attacks from different directions. A properly coordinated
exploration scheme applied to this map would help the agents discover a suitable unit positioning
quickly and improve performance, while 2s3z requires agents to learn “focus fire" and interception.
For the asymmetric 5m_vs_6m, basic agent coordination alone such as “focus firing” no longer
suffices (Du et al., 2019) and consistent success requires extended exploration to uncover complex
cooperative strategies such as pulling back units with low health during combat. The 3s_vs_5z
scenario features three allied Stalkers against five enemy Zealots. Since Zealots counter Stalkers, the
only winning strategy for the allied units is to kite the enemy around the map and kill them one after
another, causing the failure of independent learning algorithms to learn good policies in this task.
The 3s5z_vs_3s6z is an extended scenario of 3s_vs_5z where 3 Stalkers and 5 Zealots battle against
3 Stalkers and 6 Zealots—The extra enemy makes this scenario far more challenging. For this task,
we randomly chose 1 S and 1 Z to act as influencers because of the heterogeneity of the team. In the
rest, agents are symmetric, hence the influencer is randomly chosen at the beginning of training.
For most of these scenarios, ALITA consistently shows the best performance with significant learning
speed which confirms the effectiveness of our proposed methods. Detailed results are reported in
Figure (3) as they present the median win rate during the training across 12 random runs.
1https://github.com/oxwhirl/smac
6
Under review as a conference paper at ICLR 2022
(b) 3s_vs_5z Hard
(a) 2s3z Easy
(d) corridor SuperHard
Figure 3: The median test win % of various methods across the SMAC scenarios.
(e) 3s5z_vs_3s6z SuperHard
4.1.2	Sparse-Reward Settings
We test on two additional tasks to show the effectiveness our method on sparse-reward settings and
compare it to famous influence-based coordinated exploration algorithms (Table (1)): EDTI, EITI
(Wang et al., 2019), and Social Influence (Jaques et al., 2019).
Sparse Push-Box: A 15 × 15 room is populated with 2 agents and 1 box. Agents need to push the
box to the wall in 300 environment steps to get a reward of 1000. Moreover, both can observe the
coordinates of their teammate and the location of the box. However, the box is so heavy that only
when two agents push it in the same direction at the same time can it be moved a grid. Agents need
to coordinate their positions and actions for multiple steps to earn a reward.
Sparse Secret Room: A 25 × 25 grid is divided into three small rooms on the right and one large
room on the left where 2 agents are initially spawned. There is one door between each small room
and the large room. A switch in the large room controls all three doors. A switch also exists in each
small room which only controls the room’s door. The agents need to navigate to one of the three small
rooms, i.e. the target room, to receive positive reward. The task is considered solved if both agents
are in the target room. The state vector contains (x, y) locations of all agents and binary variables to
indicate if doors are open.
Table 1: Results on the Push-Box and Secret Room tasks after 20000 updates across 10 runs.
Push-Box	Secret Room
Agents	Team Performance	Performance Std	Avg Success Rate	Success Rate std
ALITA	146.66	34:13	0.68	0.04
EDTI	135.84	45.20	0.34	0.02
Social Influence	86.67	65.81	0.25	0.10
EITI	75.09	78.54	0.46	0.06
A notable reason for the good performance of ALITA on the two tasks is that through the intrinsic
rewards, agents tend to explore the majority of the possible encounters at the beginning of learning
which is crucial for estimating the value of the influence. Exploiting the latter, agents tend to reinforce
learning jointly rewarding configurations.
4.1.3	Multi-Agent Particle Environments
To understand how the proposed method helps agents achieve cooperative behavior in nonstation-
ary settings, we conduct experiments on the grounded communication environment 2 proposed in
2https://github.com/openai/multiagent-particle-envs
7
Under review as a conference paper at ICLR 2022
(Mordatch & Abbeel, 2018; Lowe et al., 2017). The chosen tasks are the Cooperative Navigation,
Cooperative Communication, and Physical Deception.3 We trained with 10 random seeds and
reported results in Tables (2, 3, 4).
4.2	Continuous Environments
To confirm the scalability of our algorithm to large continuous settings, we measure the performance
of our algorithm on a suite of PyBullet (Tan et al., 2018) continuous control tasks, interfaced through
OpenAI Gym (Brockman et al., 2016). Gym environments, however, are mainly single-agent settings,
thus to evaluate our approach, we reframe the problem by introducing an additional learning agent
that acts as an auxiliary agent. Crucially, both agents work collaboratively in order to find a region of
the solution space where an agent accumulates higher rewards. We use TD3 (Fujimoto et al., 2018)
as our learning model and test it against state-of-the-art algorithms in 5 gym environments. Our
algorithm outperforms all baselines across all different environments (e.g. our method attains 131%
return of SAC final performance on Humanoid-v3). For detailed results, see Appendix C.
4.3	Ablations
Figure 4: Ablations for different compo-
nents of our framework on 2s3z scenario.
We further investigate the significance of each component
along with a symmetric extension of the proposed frame-
work. Specifically, we consider the three cases: 1) No F:
where the influence function does not contribute to the
update rule to any of the policies; 2) No IM: where a ran-
domly selected agent maximizes both the expected sum
of extrinsic rewards along with the influence function,
and other agents’ policies are learned using the DDPG; 3)
Symmetric: where all agents simultaneously play the role
of an influencer and influencee: they learn to maximize
an augmented reward function (extrinsic and intrinsic)
along with the influence function.
Results of the experiments conducted on the 2s3z SMAC
scenario show that, in the absence of the intrinsic rewards
(No IM), the agents experience a slightly decreased overall performance when compared to the
significant decline induced by detaching the influence function (No F). In Figure (4), we observe that
the agents following the Symmetric approach learn faster, and achieve a significantly higher median
win rate. This approach, however, doubles the computational costs which restricts its applicability in
larger settings.
5	Related Work
We discuss recently developed methods for exploration in RL using intrinsic motivation, coordination
in multi-agent RL, and influence-based coordinated exploration methods subsequently.
Intrinsic motivation (IM) has been increasingly used both in single-agent RL and multi-agent RL. A
core idea of IM is to encourage the agent to take new actions or visit new states, thus exploring the
environment and obtaining more diverse behaviors. One common approach is to approximate state
or state-action visitation frequency and add a reward bonus to states the agent rarely covers (Tang
et al., 2017; Bellemare et al., 2016; Martin et al., 2017). A more related IM approach is to evaluate
state visitation novelty (Klissarov et al., 2019; Han et al., 2020; Burda et al., 2019) or state-action
visitation novelty (Fayad & Ibrahim, 2021). Inspired by the latter, we provided a natural extension for
this approach to the MARL settings by the learning of a "novelty" function. Other works make use of
single-agent IM to construct their multi-agent intrinsic reward (Du et al., 2019; Iqbal & Sha, 2019).
Each agent in (Du et al., 2019) learns a distinct intrinsic reward so that the agents are stimulated
differently, even when the environment only feedbacks a team reward. This reward helps distinguish
the contributions of the agents when the environment only returns a collective reward. In (Iqbal &
Sha, 2019), each agent has a novelty function that assesses how novel an observation is to it, based
3Full description in Appendix B
8
Under review as a conference paper at ICLR 2022
on its past experience. Their multi-agent intrinsic reward is defined based on how novel all agents
consider an agent’s observation. A recent work (Liu et al., 2021) assigns agents a common goal while
exploring. The goal is selected from multiple projected state spaces via a normalized entropy-based
technique. Then, agents are trained to reach this goal in a coordinated manner.
Many works studied the cooperative settings in MARL; a straightforward approach is to use inde-
pendent learning agents (fully decentralized learning). This approach, however, is shown to perform
inadequately both with Q-learning (Matignon et al., 2012) and with policy gradient (Lowe et al.,
2017). Therefore, we considered the CTDE paradigm, where each agent’s policy takes its individual
observation as many real life applications dictate, while the centralized critic permit for sharing
of information during training. Policy gradient methods have been commonly used along with the
CTDE paradigm in MARL, either by implementing a single centralized critic for all agents (Foerster
et al., 2018), or one centralized critic for each agent (Lowe et al., 2017). Adopting the latter, we
enable agents with different reward functions to learn in competitive and mixed scenarios as well.
Some other works encouraged cooperative interactions between agents by sharing useful information
(Yang et al., 2020; Hostallero et al., 2020). In Hostallero et al. (2020), each agent broadcasts a signal
that represents an assessment of the effect of the joint actions that all agents take on its expected
reward. Different from our approach, this signal encourages agents to behave as is expected of them
and does not benefit exploration. As for Yang et al. (2020), each agent learns an incentive function
that rewards other agents based on their actions. Each agent’s function aims to alter other agents’
behavior to maximize its extrinsic rewards. To accomplish this, each agent requires access to every
other agent’s policy, incentive function, and return making this approach difficult to scale and execute.
Additionally, Roy et al. (2019) proposed two policy regularizers approaches to promote coordination
in a team of agent, one of which assumes that an agent must be able to predict the behavior of
its teammates in order to coordinate with them, while the other supposes that coordinated agents
collectively recognize different situations and synchronously switch to different sub-policies to react
to them.
Similarly to our work, (Jaques et al., 2019) proposed a similar idea of rewarding an agent for having a
casual influence on other agents’ actions. Their method showed interesting results in terms of learning
coordinated behavior. However, this casual influence is designed to reward policies for influencing
other policies’ actions without considering the "quality" of this influence. Barton et al. (2018) propose
causal influence as a way to measure coordination between agents, specifically using Convergence
Cross Mapping (CCM) to analyze the degree of dependence between two agents’ policies. Our
method also draws inspiration from the work of (Wang et al., 2019), as they define an influence-based
intrinsic exploration bonus, called Value of Interaction (VoI), by the expected difference between the
action-value function of one agent and its counterfactual action-value function without considering
the state and action of the other agent. Particularly, the latter measures the expected behavior of an
agent in a situation where it is not influenced by the other agent. Consequently, by maximizing the
VoI, agents tend to explore meaningful interaction points as the distance between their action-value
functions conditioned on other agents and their independent action-value (only conditioned on self
behavior) functions is maximized. The primary difference from ALITA lies in our definition of the
influence.
6	Conclusions & Future Work
We introduced a novel multi-agent RL algorithm for achieving coordination through assessing the
influence an agent has on other agents’ behaviors. Additionally, we proposed to learn an intrinsic
reward for each agent to promote coordinated team exploration. We tested our algorithm on a
wide variety of tasks with many challenges, such as partial observability, sparse rewards, and large
spaces; these tasks include, but not limited to, SMAC, MAPE, as well as OpenAI gym continuous
environments. Our methods achieved noticeable improvement over prominent algorithms on all tasks.
One promising extension of our algorithm is to use Graph Attention Networks (VeIickoVic et al.,
2017; Zhou et al., 2020a) to learn the importance of the influencer in determining the influencees’
policies and to establish a message-passing architecture in networked systems. The inVestigation of
the effectiVeness of these methods is left for future works.
9
Under review as a conference paper at ICLR 2022
7	Reproducibility Statement
We have provided an illustration of the proposed algorithm in Fig. (1) along with implementation
details and hyperparameters selection in Appendix D. Furthermore, code is submitted with Sup-
plementary Material and each algorithm is evaluated at least 10 times using random seeds on all
environments.
References
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. Optimality and approximation
with policy gradient methods in markov decision processes. In Conference on Learning Theory,
pp. 64-66. PMLR, 2020.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Sean L Barton, Nicholas R Waytowich, and Derrik E Asher. Coordination-driven learning in multi-
agent problem spaces. arXiv preprint arXiv:1809.04918, 2018.
Marc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom SchaUL David Saxton, and Remi
Munos. Unifying count-based exploration and intrinsic motivation. In 30th Conference on Neural
Information Processing Systems, 2016.
Greg Brockman, Vicki CheUng, LUdwig Pettersson, Jonas Schneider, John SchUlman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
YUri BUrda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network
distillation. In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=H1lJJnR5Ym.
Gang Chen and Yiming Peng. Off-policy actor-critic in an ensemble: Achieving maximUm general
entropy and effective environment exploration in deep reinforcement learning. arXiv preprint
arXiv:1902.05551, 2019.
Constantinos Daskalakis, Dylan J Foster, and Noah Golowich. Independent policy gradient methods
for competitive reinforcement learning. arXiv preprint arXiv:2101.04233, 2021.
Yali DU, Lei Han, Meng Fang, Ji LiU, Tianhong Dai, and Dacheng Tao. Liir: Learning individUal
intrinsic reward in mUlti-agent reinforcement learning. Advances in Neural Information Processing
Systems, 32:4403-4414, 2019.
Ammar Fayad and Majd Ibrahim. Behavior-gUided actor-critic: Improving exploration via learning
policy behavior representation for deep reinforcement learning. arXiv preprint arXiv:2104.04424,
2021.
Jakob Foerster, Gregory FarqUhar, Triantafyllos AfoUras, Nantas Nardelli, and Shimon Whiteson.
CoUnterfactUal mUlti-agent policy gradients. In Proceedings of the AAAI Conference on Artificial
Intelligence, volUme 32, 2018.
Scott FUjimoto, Herke Van Hoof, and David Meger. Addressing fUnction approximation error in
actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.
Xavier Glorot and YoshUa Bengio. Understanding the difficUlty of training deep feedforward neUral
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256. JMLR Workshop and Conference Proceedings, 2010.
TUomas Haarnoja, AUrick ZhoU, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximUm entropy deep reinforcement learning with a stochastic actor. In International conference
on machine learning, pp. 1861-1870. PMLR, 2018.
Gao-Jie Han, Xiao-Fang Zhang, Hao Wang, and Chen-GUang Mao. CUriosity-driven variational
aUtoencoder for deep q network. In Pacific-Asia Conference on Knowledge Discovery and Data
Mining, pp. 764-775. Springer, 2020.
10
Under review as a conference paper at ICLR 2022
Matthew Hausknecht and Peter Stone. Deep recurrent q-learning for partially observable mdps. In
2015 aaai fall symposium series, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
David Earl Hostallero, Daewoo Kim, Sangwoo Moon, Kyunghwan Son, Wan Ju Kang, and Yung
Yi. Inducing cooperation through reward reshaping based on peer evaluations in deep multi-agent
reinforcement learning. In Proceedings of the 19th International Conference on Autonomous
Agents and MultiAgent Systems, pp. 520-528, 2020.
Shariq Iqbal and Fei Sha. Coordinated exploration via intrinsic rewards for multi-agent reinforcement
learning. arXiv preprint arXiv:1905.12127, 2019.
Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro Ortega, DJ Strouse,
Joel Z Leibo, and Nando De Freitas. Social influence as intrinsic motivation for multi-agent deep
reinforcement learning. In International Conference on Machine Learning, pp. 3040-3049. PMLR,
2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Martin Klissarov, Riashat Islam, Khimya Khetarpal, and Doina Precup. Variational state encoding as
intrinsic motivation in reinforcement learning. In Task-Agnostic Reinforcement Learning Workshop
at Proceedings of the International Conference on Learning Representations, 2019.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
Machine learning proceedings 1994, pp. 157-163. Elsevier, 1994.
Iou-Jen Liu, Unnat Jain, Raymond A Yeh, and Alexander Schwing. Cooperative exploration for
multi-agent deep reinforcement learning. In International Conference on Machine Learning, pp.
6826-6836. PMLR, 2021.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-
critic for mixed cooperative-competitive environments. arXiv preprint arXiv:1706.02275, 2017.
Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. Maven: Multi-agent
variational exploration. arXiv preprint arXiv:1910.07483, 2019.
Jarryd Martin, Suraj Narayanan Sasikumar, Tom Everitt, and Marcus Hutter. Count-based exploration
in feature space for reinforcement learning. arXiv preprint arXiv:1706.08090, 2017.
Laetitia Matignon, Guillaume J Laurent, and Nadine Le Fort-Piat. Independent reinforcement learners
in cooperative markov games: a survey regarding coordination problems. Knowledge Engineering
Review, 27(1):1-31, 2012.
Igor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multi-agent
populations. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shi-
mon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement
learning. In International Conference on Machine Learning, pp. 4295-4304. PMLR, 2018.
Julien Roy, Paul Barde, Felix G Harvey, Derek Nowrouzezahrai, and Christopher PaL Promoting
coordination through policy regularization in multi-agent deep reinforcement learning. arXiv
preprint arXiv:1908.02269, 2019.
Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas Nardelli,
Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The
starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.
11
Under review as a conference paper at ICLR 2022
John Schulman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. Trust region
policy optimization. In 32nd International Conference on Machine Learning, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Peter Sunehag, Guy Lever, A. Gruslys, Wojciech Czarnecki, V. Zambaldi, Max Jaderberg, Marc
Lanctot, Nicolas Sonnerat, Joel Z. Leibo, K. Tuyls, and T. Graepel. Value-decomposition networks
for cooperative multi-agent learning. ArXiv, abs/1706.05296, 2018.
Richard S Sutton, Andrew G Barto, et al. Introduction to reinforcement learning, volume 135. MIT
press Cambridge, 1998.
Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, and
Vincent Vanhoucke. Sim-to-real: Learning agile locomotion for quadruped robots. arXiv preprint
arXiv:1804.10332, 2018.
Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In In Proceedings
of the Tenth International Conference on Machine Learning, pp. 330-337. Morgan Kaufmann,
1993.
Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman,
Filip De Turck, and Pieter Abbeel. #exploration: A study of count-based exploration for deep
reinforcement learning. In 31st Conference on Neural Information Processing Systems, 2017.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033.
IEEE, 2012.
Petar VeliCkovic, GUillem CUcUrUlL Arantxa Casanova, Adriana Romero, Pietro Lio, and YoshUa
Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.
Tonghan Wang, Jianhao Wang, Yi WU, and Chongjie Zhang. InflUence-based mUlti-agent exploration.
arXiv preprint arXiv:1910.05512, 2019.
YUhUai WU, Elman Mansimov, Roger B. Grosse, ShUn Liao, and Jimmy Ba. Scalable trUst-
region method for deep reinforcement learning Using kronecker-factored approximation. ArXiv,
abs/1708.05144, 2017.
Jiachen Yang, Ang Li, Mehrdad Farajtabar, Peter SUnehag, Edward HUghes, and HongyUan Zha.
Learning to incentivize other learning agents. arXiv preprint arXiv:2006.06051, 2020.
Yaodong Yang, RUi LUo, Minne Li, Ming ZhoU, Weinan Zhang, and JUn Wang. Mean field mUlti-
agent reinforcement learning. In International Conference on Machine Learning, pp. 5571-5580.
PMLR, 2018.
YUnbo Zhang, Wenhao YU, and Greg TUrk. Learning novel policies for tasks. In International
Conference on Machine Learning, pp. 7483-7492. PMLR, 2019.
Jie ZhoU, GanqU CUi, Zhengyan Zhang, Cheng Yang, ZhiyUan LiU, and Maosong SUn. Graph neUral
networks: A review of methods and applications. AI Open, 1:57-81, 2020a.
Meng ZhoU, ZiyU LiU, Pengwei SUi, YixUan Li, and YUk Ying ChUng. Learning implicit credit
assignment for cooperative mUlti-agent reinforcement learning. arXiv preprint arXiv:2007.02529,
2020b.
12
Under review as a conference paper at ICLR 2022
A Mathematical Details
Theorem 1. (Influence Gradient)
VθιF(∏1) = E αiEs,∏ι [Vθι log∏ι(aι∣s)g(s, aι)2 + 2g(s, aι)Es,∏ι [Vθι log∏ι(aι∣s)qπ1 (s,aι)]]
2≤i≤N
where g(T) = g(s, a1) = qiπ1 (s, a1) - maxu Qi(s, u).
Proof. We start by finding Vθ1 qiπ1 :
vθιqπ (s,aι) = Y	E	[vθι log ni(a1IsO)q『1(S0,aI) + vθιqπ1 (s0,a1)]
s0 〜p(.∣s,αι)
a；〜∏ι(.∣s1)
Let T = (s,aι) ∈ S × Aι,and φ(T) = YEsθ,a0 [Vθ; log∏ι(a1∣s0)q∏1 (s0,a1)] where the distribution
of s0 is conditional on s and a1. Thus, write Vθ1 qiπ1 (T) as:
φ(T) +YEs0,a01[Vθ1qiπ1(T0)]
=φ(T)+YEs0,a01[φ(T0)+YEs00,a010[Vθ1qiπ1(T00)]]
.
.
.
∞
=XXPr(T → T0; k)Ykφ(T0)
H X t→m Pr(St = s0∣T,γ∏ι) φ(T0)
τ0 ×→∞------------{z-----------/
dT(s0)
= X dT(s0)Es00,a010 [Vθ1 logπ1(a010Is00)qiπ1(s00,a010)]
T0
= E	[Vθ1 logπ1(a01Is0)qiπ1(s0, a01)]
S 〜dτ(.)
a； ~∏ι(.∣s0)
= E [Vθ1 logπ1(a01Is0)qiπ1 (T0)]
T0 〜ρ(.∣τ)
(T0= (s0,a01))
(new tuples T00are conditioned on previous ones)
(repeatedly unroll Vqi 1)
(normalize the series term)
(dT is the discounted visitation distribution induced by π1 )
(use T0 to simplify notation)
Since g(T) = q∏1(s, aι) — maxu Qi(s, u), the gradient of F(∏ι) can be expressed as:
Vθι F(∏ι)= X aiEτ [Vθι log∏ι(aι∣s)g(T)2 +2g(T)V©； q∏1(T)]
I--------------------------------------------}
2≤i≤N	A
X αiET [A + 2g(T)	E	[Vθ1 logπ1(a01Is0)qiπ1(T0)]]
2≤i≤N	τ'~ρ(JT)
X aiEτ [A + 2g(T) ∙ Er，[V©； log∏ι(a1∣s0)q∏1 (T0)]]
2≤i≤N
In all,
VθιF(∏1) = E αiEs,∏ι [Vθι log∏ι(aι∣s)g(s, aι)2+2g(s,aι)Es,∏ι [V©； log∏ι(aι∣s)qπ1 (s,aι)]]
2≤i≤N
□
Proposition 1. Consider an L-action setting of n agents. In expectation, the number of steps T
needed to visit all Ln action configurations at least once without coordinated exploration grows at
least exponentially with the number of agents. More concretely, E[T] = Ω(nLn).
Proof. Let M = Ln . Since agents tend to visit different action configurations with no coordinated
behavior, one can equivalently say that agents uniformly pick a configuration out of all Ln possible
13
Under review as a conference paper at ICLR 2022
configurations at each step. Let Tk be the number of steps to visit the k-th distinct configuration after
covering k - 1 distinct action tuples. Observe that:
M
E[T] = XE[Tk]
k=1
(12)
Now, Pr[Tk = i] = (kM-1 )i 1(1 - kj^) meaning that Tk follows a geometric distribution. Thus,
E[Tk]
X (胃 Γ(；)
M
M - k +1
(13)
i
Getting back to Eq. (12),
M 1	M1	M1
E[T ] = MEM-k +1 = m∑⅛ >mJ	x dx = M ln M = nLn ln L	(14)
k=1	k=1	1
And the conclusion follows.
□
B	Details of MAPE Tasks
Cooperative Navigation: In this environment, N agents must collaborate to reach a set of N
landmarks with known positions. Agents are rewarded based on how far any agent is from each
landmark, meaning that the agents learn to spread with each agent covering one landmark. The
agents, which occupy a significant physical space, are aware of their relative positions to each other
and are further penalized when colliding with each other.
Table 2: Avg # of collisions per episode and avg agent distance from a landmark in the cooperative
navigation task, after 25000 episodes. we achieved optimal performance in the N = 3 case, with
near-optimal performance in the N = 6 case, as agents focused more on not colliding with each other
(lowest collision average)
Agent π	N= Average dist.	二 3 # collisions	N= Average dist.	6 # collisions
ALITA	1.559	0185	3.349	1.294
MADDPG	1.767	0.209	3.345	1.366
DDPG	1.858	0.375	3.350	1.585
Cooperative Communication: Here, a stationary speaker must guide a listener in an environment
consisting of three landmarks of differing colors. At each episode, one landmark of a particular color
is set as a goal for the listener to be reached, however, only the speaker can observe which landmark
the listener must navigate to. Moreover, The speaker can produce a communication output at each
time step which is observed by the listener. The latter must navigate the environment to reach the
correct landmark. Agents are collectively rewarded at the end of an episode based on the listener’s
distance from the correct landmark.
Physical Deception: This environment consists of N agents and N landmarks, with one landmark
as the target of all agents. The agents are rewarded based on the distance of the closest agent to the
target landmark, making it sufficient for only one agent to reach it. An adversary agent also tries to
reach the target landmark, while the agents are penalized as it gets closer to the target. The adversary,
however, does not know which landmark is the target and must deduce it from the agents’ behavior.
For that reason, agents must cooperate to trick the adversary by learning to cover all the landmarks.
This task shows that our algorithm is applicable not only to cooperative interactions but to mixed
environments as well.
In the cooperative communication and physical deception tasks, ALITA obtained the highest success
rate across all baselines.
14
Under review as a conference paper at ICLR 2022
Table 3: Percentage of episodes where the agent reached the target landmark and average distance
from the target in the cooperative communication environment, after 25000 episodes.
Agent	Target reach %	Average distance
ALITA	903%	0.093
MADDPG	84.0%	0.133
DDPG	32.0%	0.456
DQN	24.8%	0.754
Actor-Critic	17.2%	2.071
TRPO	20.6%	1.573
REINFORCE	13.6%	3.333
Table 4: Results on the physical deception task, with N = 2 cooperative agents/landmarks. Success
(succ %) for agents (AG) and adversaries (ADV) is if they are within a small distance from the target
landmark.
Agent π	Adversary π	AG Succ %	ADV Succ %	∆ Succ %
ALITA	MADDPG	~~95.2%~~	45.1%	50.1%
MADDPG	MADDPG	94.4%	39.2%	55.2%
MADDPG	DDPG	92.2%	16.4%	75.8%
DDPG	MADDPG	68.9%	59.0%	9.9%
DDPG	DDPG	74.7%	38.6%	36.1%
C Additional Experiments on Continuous Environments
Since the formulation of F needs a shared buffer, SAC and TD3 stand as the best off-policy
candidates to be incorporated with our framework, as they have shown great performances on many
benchmarks. SAC, however, uses stochastic policies in general which makes it infeasible to combine
with the formulation of rint. Therefore, we use TD3 as our learning model to measure its performance
on a suite of PyBullet (Tan et al., 2018) continuous control tasks, interfaced through OpenAI Gym
(Brockman et al., 2016). While many previous works utilized the Mujoco (Todorov et al., 2012)
physics engine to simulate the system dynamics of these tasks, we found it better to evaluate our
method on benchmark problems powered by PyBullet simulator since it is widely reported that
PyBullet problems are harder to solve (Tan et al., 2018) when compared to Mujoco. Also, Pybullet is
license-free, unlike Mujoco that is only available to its license holders.
(a) Humanoid-v3
(c) Walker2D
(b) Ant
(d) HalfCheetah
(e) Reacher
Figure 5: Learning curves for the OpenAI gym continuous control tasks. The shaded region represents
quarter a standard deviation of the average evaluation. Curves are smoothed for visual clarity.
We compare our method to the original twin delayed deep deterministic policy gradients (TD3)
(Fujimoto et al., 2018); soft actor critic (SAC) (Haarnoja et al., 2018); proximal policy optimization
(PPO) (Schulman et al., 2017), a stable and efficient on-policy policy gradient algorithm; deep
deterministic policy gradient (DDPG); trust region policy optimization (TRPO) (Schulman et al.,
15
Under review as a conference paper at ICLR 2022
2015); Tsallis actor-critic (TAC) (Chen & Peng, 2019), a recent off-policy algorithm for learning
maximum entropy policies, where we use the implementation of the authors4 5; and Actor-Critic
using Kronecker-Factored Trust Region (ACKTR) (Wu et al., 2017), as implemented by OpenAI’s
baselines repository 6. Each task is run for at least 1 million time steps and the average return of 15
episodes is reported every 5000 time steps. To enable reproducibility, each experiment is conducted
on 10 random seeds of Gym simulator and network initialization. Results of the best performing
agent of the two across different methods are reported in Figure (5).
D Training details
D. 1 General Configurations
We use a buffer-size of 106 entries and a batch-size of 1024. We collect 100 transitions by interacting
with the environment for each learning update. For all tasks in our hyper-parameter searches, we
train the agents for 15, 000 episodes of 100 steps and then re-train the best configuration for each
algorithm-environment pair for twice as long (30, 000 episodes) to ensure full convergence for the
final evaluation. We use a discount factor γ of 0.95, an influence importance temperature λ of 0.1,
and a gradient clipping threshold of 0.5 in all experiments unless otherwise specified. Each cloned
critic is updated 4 time per step.
D.2 Sparse Push Box and Sparse Secret Room, MAPE, & Gym
We use the Adam optimizer (Kingma & Ba, 2014) to perform parameter updates. All models (actors,
critics and proxy critics) are parametrized by feedforward networks containing two hidden layers of
128 units excpet for the autoencoder network where we use 7 hidden layers with dimensions (128,
64, 12, 3, 12, 64, 128), respectively. All models’ parameters are initialized using Glorot Initialization
method (Glorot & Bengio, 2010); while the autoencoder’s parameters are initialized using Kaiming
method (He et al., 2015). We employ the Rectified Linear Unit (ReLU) as activation function and
layer normalization (Ba et al., 2016) on the pre-activations unit to stabilize the learning.
Table 5: Best found hyper-parameters for the Sparse-reward tasks, MAPE, & Gym environments
Hyper-parameter	Push Box	Secret Room	MAPE	Humanoid-v3	Gym (except for Humanoid-v3)
λπ	0.10	0.10	0.01-	0.10	0.10
{ωi}1n-1	0.10	0.10	0.01	0.10	0.10
β	0.15	0.10	0.10	0.15	0.1
D.3 SMAC
The architecture of all agent networks is a DRQN (Hausknecht & Stone, 2015) with a recurrent layer
comprised of a GRU with a 64-dimensional hidden state, with a fully-connected layer before and
after. All neural networks are trained using RMSprop (α = 0.99 with no weight decay or momentum)
with learning rate 5 × 10-4.
Table 6: Best found hyper-parameters for the SMAC environments
Hyper-parameter	Corridor	5m_vs_6m	3s_vs_5z	2s3z
λπ	0.09	0.03	0.01	0.01
{ωi}1n-1	0.03	0.03	0.01	0.01
β	0.15	0.15	0.10	0.10
4https://github.com/haarnoja/sac
5https://github.com/yimingpeng/sac-master
6https://github.com/openai/baselines
16